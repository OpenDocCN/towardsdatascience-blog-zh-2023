- en: All You Need to Know about In-Context Learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 了解上下文学习的所有信息
- en: 原文：[https://towardsdatascience.com/all-you-need-to-know-about-in-context-learning-55bde1180610](https://towardsdatascience.com/all-you-need-to-know-about-in-context-learning-55bde1180610)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/all-you-need-to-know-about-in-context-learning-55bde1180610](https://towardsdatascience.com/all-you-need-to-know-about-in-context-learning-55bde1180610)
- en: '| IN CONTEXT LEARNING | LARGE LANGUAGE MODELS| LLMs'
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '| 上下文学习 | 大型语言模型 | LLMs'
- en: What is and how does it work what makes Large Language Models so powerful
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 什么是大型语言模型（Large Language Models），它们是如何运作的，使其如此强大的原因是什么？
- en: '[](https://salvatore-raieli.medium.com/?source=post_page-----55bde1180610--------------------------------)[![Salvatore
    Raieli](../Images/6bb4520e2df40d20283e7283141b5e06.png)](https://salvatore-raieli.medium.com/?source=post_page-----55bde1180610--------------------------------)[](https://towardsdatascience.com/?source=post_page-----55bde1180610--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----55bde1180610--------------------------------)
    [Salvatore Raieli](https://salvatore-raieli.medium.com/?source=post_page-----55bde1180610--------------------------------)'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://salvatore-raieli.medium.com/?source=post_page-----55bde1180610--------------------------------)[![Salvatore
    Raieli](../Images/6bb4520e2df40d20283e7283141b5e06.png)](https://salvatore-raieli.medium.com/?source=post_page-----55bde1180610--------------------------------)[](https://towardsdatascience.com/?source=post_page-----55bde1180610--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----55bde1180610--------------------------------)
    [Salvatore Raieli](https://salvatore-raieli.medium.com/?source=post_page-----55bde1180610--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----55bde1180610--------------------------------)
    ·19 min read·Jul 25, 2023
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于[数据科学前沿](https://towardsdatascience.com/?source=post_page-----55bde1180610--------------------------------)
    ·阅读时间19分钟·2023年7月25日
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/7f1b46371aebe6ca8e4684f7f9be78fa.png)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7f1b46371aebe6ca8e4684f7f9be78fa.png)'
- en: Photo by [🇸🇮 Janko Ferlič](https://unsplash.com/ko/@itfeelslikefilm?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：[🇸🇮 Janko Ferlič](https://unsplash.com/ko/@itfeelslikefilm?utm_source=medium&utm_medium=referral)
    在 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: “For me context is the key — from that comes the understanding of everything.”
    — Kenneth Noland
  id: totrans-9
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “对我来说，上下文是关键——从中产生了对一切的理解。” — **肯尼斯·诺兰**
- en: In-context learning (ICL) is one of the most surprising model skills. Observed
    with GPT-3 it caught the authors’ attention. **Exactly what is ICL? More importantly,
    what gives rise to it?**
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 上下文学习（ICL）是最令人惊讶的模型技能之一。观察到GPT-3时引起了作者的注意。**ICL到底是什么？更重要的是，是什么促使了它的产生？**
- en: 'This article is divided into different sections, for each section we will answer
    these questions:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 本文分为不同的部分，每部分将回答这些问题：
- en: What is In-Context Learning (ICL)? Why this is interesting? Why it is useful?
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 什么是上下文学习（ICL）？为什么这很有趣？它有什么用处？
- en: 'The mystery of ICL: how does it work? Is the training data? is the prompt?
    it is the architecture?'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ICL的奥秘：它是如何工作的？是训练数据？是提示？还是架构？
- en: What is the future of ICL? What are the remaining challenges?
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ICL的未来是什么？还存在哪些挑战？
- en: Check the list of references at the end of the article, I provide also some
    suggestions to deepen the topics.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 查看文章末尾的参考文献列表，我也提供了一些建议，以便深入探讨这些主题。
- en: What is In-Context Learning (ICL)?
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是上下文学习（ICL）？
- en: '![](../Images/4b4d54bbb67cef241a1909f3d3526037.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4b4d54bbb67cef241a1909f3d3526037.png)'
- en: Photo by [Dmitry Ratushny](https://unsplash.com/@ratushny?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：[Dmitry Ratushny](https://unsplash.com/@ratushny?utm_source=medium&utm_medium=referral)
    在 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: “The limits of my language mean the limits of my world.” — Ludwig Wittgenstein
  id: totrans-19
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “我的语言的限制意味着我的世界的限制。” — **路德维希·维特根斯坦**
- en: Before [Large Language Models](https://en.wikipedia.org/wiki/Large_language_mode)
    (LLMs) were published, an artificial intelligence model was limited to the data
    it was trained on. In other words, [LLMs](https://en.wikipedia.org/wiki/Large_language_model)
    could only solve tasks for which their training was designed.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在[大型语言模型](https://en.wikipedia.org/wiki/Large_language_mode)（LLMs）发布之前，人工智能模型仅限于其训练数据。换句话说，[LLMs](https://en.wikipedia.org/wiki/Large_language_model)只能解决为其训练设计的任务。
- en: '[GPT-3](https://arxiv.org/abs/2005.14165) and today’s [LLMs](https://en.wikipedia.org/wiki/Large_language_mode),
    on the other hand, show a new capability: the ability to learn new skills and
    solve new tasks simply by providing new examples in the input (prompt). Also,
    in this case, we are not training the model; there is no [gradient update](https://en.wikipedia.org/wiki/Gradient_descent)
    or change in model parameters. This skill is called [In-Context Learning (ICL)](https://en.wikipedia.org/wiki/Prompt_engineering).'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，[GPT-3](https://arxiv.org/abs/2005.14165)和今天的[LLMs](https://en.wikipedia.org/wiki/Large_language_mode)展示了一种新能力：通过在输入（提示）中提供新的示例来学习新技能和解决新任务。此外，在这种情况下，我们并没有训练模型；没有[梯度更新](https://en.wikipedia.org/wiki/Gradient_descent)或模型参数的变化。这项技能被称为[上下文学习（ICL）](https://en.wikipedia.org/wiki/Prompt_engineering)。
- en: '![](../Images/b4e3700d502d9da8a95015beac0410c8.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b4e3700d502d9da8a95015beac0410c8.png)'
- en: 'image source: [here](https://arxiv.org/abs/2005.14165)'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：[这里](https://arxiv.org/abs/2005.14165)
- en: To be more specific, the way to interact with a model is to provide natural
    language instructions in a prompt. Although this may seem limited, different examples
    up to a certain number of [tokens](https://learn.microsoft.com/en-us/semantic-kernel/prompt-engineering/tokens)
    ([context windows](https://www.linkedin.com/pulse/whats-context-window-anyway-caitie-doogan-phd/))
    can be entered in a prompt. In addition, despite being placed in this textual
    template, it can also allow the model [to solve mathematical exercises](https://arxiv.org/abs/2212.10535).
    In fact, in the prompt, we can insert examples of word corrections, arithmetic
    exercises, translations, programming, and whatnot.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 更具体来说，与模型交互的方式是提供自然语言指令作为提示。虽然这可能看起来有限，但可以在提示中输入一定数量的[令牌](https://learn.microsoft.com/en-us/semantic-kernel/prompt-engineering/tokens)（[上下文窗口](https://www.linkedin.com/pulse/whats-context-window-anyway-caitie-doogan-phd/)）。此外，尽管处于这个文本模板中，它也可以允许模型[解决数学题](https://arxiv.org/abs/2212.10535)。事实上，在提示中，我们可以插入单词纠正、算术练习、翻译、编程等示例。
- en: '![](../Images/668ff5c0b2b29f0af73c35b6890623ca.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/668ff5c0b2b29f0af73c35b6890623ca.png)'
- en: 'image source: [here](https://arxiv.org/abs/2301.00234)'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：[这里](https://arxiv.org/abs/2301.00234)
- en: 'Now, we can give a formal definition of what is [ICL](https://en.wikipedia.org/wiki/Prompt_engineering):'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以给出[ICL](https://en.wikipedia.org/wiki/Prompt_engineering)的正式定义：
- en: In-context learning is a paradigm that allows language models to learn tasks
    given only a few examples in the form of demonstration. ([source](https://arxiv.org/abs/2301.00234))
  id: totrans-28
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 上下文学习是一种范式，它允许语言模型仅通过几个示例进行任务学习（[来源](https://arxiv.org/abs/2301.00234)）。
- en: Simply put, by giving a model a list of input-output pairs that demonstrate
    a task, the model reads the training examples to figure out the input and output
    distribution, manages to map the inputs and outputs, and generates an appropriate
    response.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 简单来说，通过给模型一组展示任务的输入输出对，模型阅读训练示例以找出输入和输出的分布，成功映射输入和输出，并生成适当的响应。
- en: '[As shown in this study](https://arxiv.org/abs/2211.09066) that this simple
    idea helps the model to perform certain tasks more easily. Explaining the model
    with unambiguous instructions on how to perform the tasks allows the model to
    better understand and perform the tasks more easily. Using these few examples
    is then competitive ([ICL](https://en.wikipedia.org/wiki/Prompt_engineering))
    is competitive against training models with many more labeled data.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '[如本研究所示](https://arxiv.org/abs/2211.09066)，这一简单的想法有助于模型更轻松地执行某些任务。通过用明确的指令解释如何执行任务，可以使模型更好地理解并更轻松地执行这些任务。使用这些少量示例相较于使用更多标记数据训练模型具有竞争力（[ICL](https://en.wikipedia.org/wiki/Prompt_engineering)）。'
- en: This has led to the emergence of various strategies to exploit [ICL](https://en.wikipedia.org/wiki/Prompt_engineering)
    (prompt engineering) since changing the prompt allows for better performance than
    [having to do fine-tuning for a specific task](https://arxiv.org/abs/2211.09066).
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 这促使了各种策略的出现，以利用[ICL](https://en.wikipedia.org/wiki/Prompt_engineering)（提示工程），因为改变提示能够比[为特定任务进行微调](https://arxiv.org/abs/2211.09066)获得更好的性能。
- en: '![](../Images/8c175a1a8e3657a5f80c1019545eaedb.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8c175a1a8e3657a5f80c1019545eaedb.png)'
- en: 'image source: [here](https://arxiv.org/abs/2211.09066)'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：[这里](https://arxiv.org/abs/2211.09066)
- en: '[](/speak-to-me-how-many-words-a-model-is-reading-331e3af86d27?source=post_page-----55bde1180610--------------------------------)
    [## Speak to me: How many words a model is reading'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/speak-to-me-how-many-words-a-model-is-reading-331e3af86d27?source=post_page-----55bde1180610--------------------------------)
    [## 告诉我：模型正在阅读多少单词]'
- en: Why and how to overcome the inner limit of a Large Language Model
  id: totrans-35
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 为什么以及如何克服大型语言模型的内在限制
- en: towardsdatascience.com](/speak-to-me-how-many-words-a-model-is-reading-331e3af86d27?source=post_page-----55bde1180610--------------------------------)
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '[在如何阅读模型的单词数量](/speak-to-me-how-many-words-a-model-is-reading-331e3af86d27?source=post_page-----55bde1180610--------------------------------)'
- en: '[This behavior also seems to emerge only at scale](https://arxiv.org/abs/2206.07682),
    i.e., it appears that [ICL](https://en.wikipedia.org/wiki/Prompt_engineering)
    emerges [only with a certain number of parameters](https://en.wikipedia.org/wiki/Neural_scaling_law).
    In fact, for some capabilities, it appears that the model has random performance
    up to a certain number of parameters, and then abruptly its performance improves.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '[这种行为似乎也仅在规模扩大时出现](https://arxiv.org/abs/2206.07682)，即，[ICL](https://en.wikipedia.org/wiki/Prompt_engineering)似乎仅在[参数数量达到一定水平](https://en.wikipedia.org/wiki/Neural_scaling_law)时出现。实际上，对于某些能力，模型的表现似乎在达到某个参数数量之前是随机的，然后突然有所改善。'
- en: '![](../Images/fd41f594bcec315652fa7c8f69a46357.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fd41f594bcec315652fa7c8f69a46357.png)'
- en: 'image source: [here](https://arxiv.org/abs/2206.07682)'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：[这里](https://arxiv.org/abs/2206.07682)
- en: '[](/emergent-abilities-in-ai-are-we-chasing-a-myth-fead754a1bf9?source=post_page-----55bde1180610--------------------------------)
    [## Emergent Abilities in AI: Are We Chasing a Myth?'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '[## AI中的新兴能力：我们在追逐一个神话吗？](/emergent-abilities-in-ai-are-we-chasing-a-myth-fead754a1bf9?source=post_page-----55bde1180610--------------------------------)'
- en: Changing Perspective on Large Language Models emerging properties
  id: totrans-41
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 从新兴属性的角度看大型语言模型
- en: towardsdatascience.com](/emergent-abilities-in-ai-are-we-chasing-a-myth-fead754a1bf9?source=post_page-----55bde1180610--------------------------------)
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '[在AI中的新兴能力：我们在追逐一个神话吗？](/emergent-abilities-in-ai-are-we-chasing-a-myth-fead754a1bf9?source=post_page-----55bde1180610--------------------------------)'
- en: 'In brief, behavior is both researched and studied because [it has definite
    advantages](https://arxiv.org/abs/2301.00234):'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，行为之所以被研究和学习，是因为[它有明显的优势](https://arxiv.org/abs/2301.00234)：
- en: The examples are written in natural language, so communication with the template
    is interpretable and understandable to humans. it is much easier to integrate
    human knowledge because you only need to change the prompt.
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 示例以自然语言编写，因此与模板的通信是可解释且易于理解的。整合人类知识要容易得多，因为你只需更改提示即可。
- en: Also [in context learnin](https://en.wikipedia.org/wiki/Prompt_engineering)g
    it remembers how humans learn because it recalls the process of learning by [analogy](https://en.wikipedia.org/wiki/Analogy).
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 另外，[上下文学习](https://en.wikipedia.org/wiki/Prompt_engineering)记忆了人类的学习方式，因为它回忆了通过[类比](https://en.wikipedia.org/wiki/Analogy)的学习过程。
- en: it is training-free, we do not have to train the model (unlike [supervised learning](https://en.wikipedia.org/wiki/Supervised_learning)).
    This means it is much cheaper as a [computational cost](https://en.wikipedia.org/wiki/Computational_complexity).
    This is really efficient since the skill can be acquired instantly.
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它是无训练的，我们不需要训练模型（与[监督学习](https://en.wikipedia.org/wiki/Supervised_learning)不同）。这意味着它在[计算成本](https://en.wikipedia.org/wiki/Computational_complexity)上要便宜得多。这非常高效，因为技能可以立即获得。
- en: It also means that the model can be used as-a-service, and can thus be deployed
    for many tasks. In fact, the tasks can be taught by everyday users.
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这也意味着模型可以作为服务使用，因此可以部署用于许多任务。实际上，这些任务可以由日常用户教授。
- en: '[ICL](https://en.wikipedia.org/wiki/Prompt_engineering) provides the model
    to [generalize](https://developers.google.com/machine-learning/crash-course/generalization/video-lecture),
    allowing the model to learn underlying patterns and rules that are present in
    the examples and then apply them to new situations. Moreover, it provides the
    model with versatility, since it can be applied to many different types of skills.'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[ICL](https://en.wikipedia.org/wiki/Prompt_engineering)使模型能够[泛化](https://developers.google.com/machine-learning/crash-course/generalization/video-lecture)，允许模型学习示例中的潜在模式和规则，然后将其应用于新情况。此外，它还为模型提供了多样性，因为它可以应用于许多不同类型的技能。'
- en: It looks amazing, but how it works?
  id: totrans-49
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 看起来很惊人，但它是如何工作的？
- en: 'The mystery of ICL: how does it work?'
  id: totrans-50
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ICL的谜团：它是如何工作的？
- en: '![](../Images/c9a184a494833d37feeb23b3928d720c.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c9a184a494833d37feeb23b3928d720c.png)'
- en: Photo by [𝓴𝓘𝓡𝓚 𝕝𝔸𝕀](https://unsplash.com/@kirklai?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由[𝓴𝓘𝓡𝓚 𝕝𝔸𝕀](https://unsplash.com/@kirklai?utm_source=medium&utm_medium=referral)提供，来源于[Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: “Usually, if you want to fine-tune these models, you need to collect domain-specific
    data and do some complex engineering. But now we can just feed it an input, five
    examples, and it accomplishes what we want. So, in-context learning is an unreasonably
    efficient learning phenomenon that needs to be understood,” Akyürek says. ([source](https://news.mit.edu/2023/large-language-models-in-context-learning-0207))
  id: totrans-53
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “通常，如果你想对这些模型进行微调，你需要收集特定领域的数据并进行一些复杂的工程工作。但现在我们只需输入五个示例，它就能完成我们想要的结果。所以，*上下文学习*是一种异常高效的学习现象，需要加以理解，”Akyürek说。
    ([source](https://news.mit.edu/2023/large-language-models-in-context-learning-0207))
- en: 'As much as [ICL](https://en.wikipedia.org/wiki/Prompt_engineering) seems almost
    magical, it also has its limitations. [GPT-3](https://en.wikipedia.org/wiki/GPT-3)
    for example had shown what seemed incredible reasoning capabilities. Yet some
    datasets that required reasoning, such as the Winograd dataset did not show [ICL](https://en.wikipedia.org/wiki/Prompt_engineering):'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管*[ICL](https://en.wikipedia.org/wiki/Prompt_engineering)*似乎几乎是神奇的，但它也有其局限性。例如，[GPT-3](https://en.wikipedia.org/wiki/GPT-3)展示了看似惊人的推理能力。然而，一些需要推理的数据集，例如
    Winograd 数据集，并没有显示*[ICL](https://en.wikipedia.org/wiki/Prompt_engineering)*的效果：
- en: A Winograd schema is a pair of sentences that differ in only one or two words
    and that contain an ambiguity that is resolved in opposite ways in the two sentences
    and requires the use of world knowledge and reasoning for its resolution. ([source](https://cs.nyu.edu/~davise/papers/WinogradSchemas/WS.html))
  id: totrans-55
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: Winograd 方案是一对仅在一两个词上有所不同的句子，这些句子中包含了一个模糊性，该模糊性在两个句子中以相反的方式得到解决，并且需要使用世界知识和推理来解决。
    ([source](https://cs.nyu.edu/~davise/papers/WinogradSchemas/WS.html))
- en: In fact, there was no improvement with the use of a few examples in the prompt.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，使用几个示例进行提示并没有带来改进。
- en: '![](../Images/77e88cac98ec8e0f298a32e423cc5387.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/77e88cac98ec8e0f298a32e423cc5387.png)'
- en: 'image source: [here](https://arxiv.org/abs/2005.14165)'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：[这里](https://arxiv.org/abs/2005.14165)
- en: 'These facts and other seemingly contradictory behaviors have led researchers
    to ask: **where does ICL originate? Why works better than fine-tuning? Can ICL
    be improved by changing the prompt?**'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 这些事实和其他看似矛盾的行为使研究人员提出了问题：**ICL的起源是什么？为什么它比微调更有效？通过改变提示，ICL能否得到改善？**
- en: Meanwhile, one must remember that most skills are learned during pre-training.
    The first step of training an [LLM](https://en.wikipedia.org/wiki/Large_language_model)
    that requires huge amounts of text and is typically conducted by simply asking
    the model to predict a word in a sequence given the previous part of the sequence.
    This step is the most expensive, time-consuming, and resource-intensive one. During
    [alignment](https://jasonwei20.github.io/files/FLAN%20talk%20external.pdf) (the
    transition from GPT 3.5 to [ChatGPT](https://openai.com/blog/chatgpt)) the model
    only improves its ability to exploit this knowledge and how to interact with humans.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 同时，必须记住大多数技能是在预训练期间学到的。训练一个*[LLM](https://en.wikipedia.org/wiki/Large_language_model)*的第一步需要大量的文本，通常通过简单地要求模型预测给定序列前部分的单词来进行。这一步是最昂贵、最耗时和最资源密集的。在*[alignment](https://jasonwei20.github.io/files/FLAN%20talk%20external.pdf)*（从
    GPT 3.5 到*[ChatGPT](https://openai.com/blog/chatgpt)）期间，模型只是提高了利用这些知识的能力和如何与人类互动。
- en: '[](/the-infinite-babel-library-of-llms-90e203b2f6b0?source=post_page-----55bde1180610--------------------------------)
    [## The Infinite Babel Library of LLMs'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/the-infinite-babel-library-of-llms-90e203b2f6b0?source=post_page-----55bde1180610--------------------------------)
    [## 大语言模型的无限巴别图书馆'
- en: 'Open-source, data, and attention: How the future of LLMs will change'
  id: totrans-62
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 开源、数据和注意力：大语言模型未来的变化
- en: towardsdatascience.com](/the-infinite-babel-library-of-llms-90e203b2f6b0?source=post_page-----55bde1180610--------------------------------)
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '[towardsdatascience.com](/the-infinite-babel-library-of-llms-90e203b2f6b0?source=post_page-----55bde1180610--------------------------------)'
- en: 'Use what you see: the pre-training impact on ICL'
  id: totrans-64
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 利用你所看到的：预训练对 ICL 的影响
- en: 'During pre-training, [LLMs](https://en.wikipedia.org/wiki/Large_language_model)
    are thus exposed to an enormous amount of text: from Wikipedia, books (fiction
    and nonfiction), scientific articles, tweets, Reddit posts, blog posts, internet
    dumps, and so on.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在预训练期间，[LLMs](https://en.wikipedia.org/wiki/Large_language_model) 因此接触到了大量的文本：来自维基百科、书籍（小说和非小说）、科学文章、推文、Reddit
    帖子、博客帖子、互联网数据等。
- en: '[In a 2022 article](https://arxiv.org/abs/2212.10559), it was proposed that
    [ICL](https://en.wikipedia.org/wiki/Prompt_engineering) can be considered a kind
    of implicit [fine-tuning](https://en.wikipedia.org/wiki/Fine-tuning_(deep_learning)).
    The main difference is that while [ICL](https://en.wikipedia.org/wiki/Prompt_engineering)
    is produced only by forward computation while fine-tuning also has a backpropagation
    step (in which parameters are updated). **This confirms that** [**ICL**](https://en.wikipedia.org/wiki/Prompt_engineering)
    **must originate during pretraining, but how does it impact pretraining?**'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '[在2022年一篇文章中](https://arxiv.org/abs/2212.10559)，提出了[ICL](https://en.wikipedia.org/wiki/Prompt_engineering)可以被视为一种隐式的[微调](https://en.wikipedia.org/wiki/Fine-tuning_(deep_learning))。主要区别在于，虽然[ICL](https://en.wikipedia.org/wiki/Prompt_engineering)仅通过前向计算产生，而微调还包含一个反向传播步骤（在该步骤中参数被更新）。**这确认了**
    [**ICL**](https://en.wikipedia.org/wiki/Prompt_engineering) **必须在预训练期间产生，但它如何影响预训练？**'
- en: '[As one article showed](https://aclanthology.org/2022.naacl-main.380/), the
    pretraining dataset is critical for a model to develop [ICL](https://en.wikipedia.org/wiki/Prompt_engineering).
    According to the authors, the source domain is more important than the size of
    the [corpus](https://en.wikipedia.org/wiki/Text_corpus). Also, putting several
    [corpora](https://en.wikipedia.org/wiki/Text_corpus) together can lead to the
    emergence of [ICL](https://en.wikipedia.org/wiki/Prompt_engineering) (if two corpora
    alone do not give ICL, joining them together can give [ICL](https://en.wikipedia.org/wiki/Prompt_engineering)).
    Another important factor is the domain relevance of the [corpus](https://en.wikipedia.org/wiki/Text_corpus):
    training only on a news [corpus](https://en.wikipedia.org/wiki/Text_corpus) allows
    relative in-context learning ability on a news-related downstream task. Finally,
    the authors note, that although [perplexity](https://en.wikipedia.org/wiki/Perplexity)
    (one of the most commonly used metrics for tracking [LLMs](https://en.wikipedia.org/wiki/Large_language_model))
    and [ICL](https://en.wikipedia.org/wiki/Prompt_engineering) generally correlate,
    perplexity alone does not reflect a model’s ability for [ICL](https://en.wikipedia.org/wiki/Prompt_engineering)
    (comparing two [LLMs](https://en.wikipedia.org/wiki/Large_language_model), the
    model with the lowest perplexity is not necessarily the one with the highest [ICL](https://en.wikipedia.org/wiki/Prompt_engineering)).'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '[正如一篇文章所示](https://aclanthology.org/2022.naacl-main.380/)，预训练数据集对模型开发[ICL](https://en.wikipedia.org/wiki/Prompt_engineering)至关重要。根据作者的说法，源领域比[语料库](https://en.wikipedia.org/wiki/Text_corpus)的大小更为重要。此外，将多个[语料库](https://en.wikipedia.org/wiki/Text_corpus)结合起来可以促使[ICL](https://en.wikipedia.org/wiki/Prompt_engineering)的出现（如果两个语料库单独无法产生ICL，将它们结合起来可以产生[ICL](https://en.wikipedia.org/wiki/Prompt_engineering)）。另一个重要因素是[语料库](https://en.wikipedia.org/wiki/Text_corpus)的领域相关性：仅在新闻[语料库](https://en.wikipedia.org/wiki/Text_corpus)上进行训练只能在与新闻相关的下游任务中实现相对的上下文学习能力。最后，作者指出，尽管[困惑度](https://en.wikipedia.org/wiki/Perplexity)（跟踪[LLMs](https://en.wikipedia.org/wiki/Large_language_model)的最常用指标之一）和[ICL](https://en.wikipedia.org/wiki/Prompt_engineering)通常存在相关性，但困惑度本身并不能反映模型在[ICL](https://en.wikipedia.org/wiki/Prompt_engineering)方面的能力（比较两个[LLMs](https://en.wikipedia.org/wiki/Large_language_model)时，困惑度最低的模型不一定是[ICL](https://en.wikipedia.org/wiki/Prompt_engineering)最高的模型）。'
- en: '[This was further confirmed](https://arxiv.org/abs/2205.05055) by the fact
    that the dataset must be several in rare classes to allow ICL. According to the
    authors, the training data examples should appear in clusters (i.e. there should
    be several for each class) and be a certain variety of classes.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '[这在进一步确认](https://arxiv.org/abs/2205.05055)了数据集必须在稀有类别中存在多个实例以允许ICL。根据作者的说法，训练数据示例应以簇的形式出现（即，每个类别应有多个示例），并且应具有一定的类别多样性。'
- en: '![](../Images/fe8746fa845491858ad4fcabf22a1b63.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fe8746fa845491858ad4fcabf22a1b63.png)'
- en: 'image source: [here](https://arxiv.org/abs/2205.05055)'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：[这里](https://arxiv.org/abs/2205.05055)
- en: '[Another study states](https://arxiv.org/abs/2303.07895) instead that in-context
    learning appears when the pretraining distribution (the training data) is an implicit
    mixture. The examples for pretraining are extracted from a mixture of tasks, and
    the association between examples and tasks is latent. So then once the model is
    trained, it manages on its own to uncover the latent task in the demonstration.
    For example, a series of tweets with positive and negative content represent a
    latent task of sentiment analysis.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '[另一项研究指出](https://arxiv.org/abs/2303.07895)在上下文学习出现时，预训练分布（训练数据）是一种隐式混合。预训练示例是从任务混合中提取的，而示例和任务之间的关联是潜在的。因此，一旦模型训练完成，它能够自行揭示示例中的潜在任务。例如，一系列包含正面和负面内容的推文代表了情感分析的潜在任务。'
- en: In short, these articles claim that [ICL](https://en.wikipedia.org/wiki/Prompt_engineering)
    appears if the dataset is diverse, they present a diverse number of class numbers
    (but simultaneously several examples per class), they cover multiple domains,
    and best if these examples represent a latent task of [NLP](https://en.wikipedia.org/wiki/Natural_language_processing).
    Since [LLMs](https://en.wikipedia.org/wiki/Large_language_model) are generally
    trained with a huge amount of text, these premises are met.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，这些文章声称，[ICL](https://en.wikipedia.org/wiki/Prompt_engineering)会出现如果数据集多样化，它们呈现了多样的类别数量（但同时每个类别有多个示例），涵盖多个领域，并且最好这些示例代表一个[自然语言处理](https://en.wikipedia.org/wiki/Natural_language_processing)的潜在任务。由于[LLMs](https://en.wikipedia.org/wiki/Large_language_model)通常使用大量文本进行训练，因此满足这些前提。
- en: '[](/say-once-repeating-words-is-not-helping-ai-58f38035f66e?source=post_page-----55bde1180610--------------------------------)
    [## Say Once! Repeating Words Is Not Helping AI'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '[## 说一次！重复词汇无助于AI](https://say-once-repeating-words-is-not-helping-ai-58f38035f66e?source=post_page-----55bde1180610--------------------------------)'
- en: How and why is repeating tokens harming LLMs? Why is this a problem?
  id: totrans-74
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 重复标记如何以及为何对LLM造成伤害？这是个问题吗？
- en: towardsdatascience.com](/say-once-repeating-words-is-not-helping-ai-58f38035f66e?source=post_page-----55bde1180610--------------------------------)
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '[towardsdatascience.com](https://say-once-repeating-words-is-not-helping-ai-58f38035f66e?source=post_page-----55bde1180610--------------------------------)'
- en: 'How you use what you learn: can you recall what you learned?'
  id: totrans-76
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 你如何使用你学到的知识：你能回忆起你学到的内容吗？
- en: 'Some researchers have attempted [to develop a framework](https://arxiv.org/abs/2111.02080)
    for understanding how [ICL](https://en.wikipedia.org/wiki/Prompt_engineering)
    emerges during pretraining. According to the authors, an [LLM](https://en.wikipedia.org/wiki/Large_language_model)
    uses [ICL](https://en.wikipedia.org/wiki/Prompt_engineering) to “locate” concepts
    that are needed to perform the task. The idea is that during training the model
    acquires latent concepts and then finds it again during [ICL](https://en.wikipedia.org/wiki/Prompt_engineering).
    To find them again, the [LLM](https://en.wikipedia.org/wiki/Large_language_model)
    can use all or some components of a prompt: format, inputs, outputs, and input-output
    mapping.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 一些研究人员尝试[开发框架](https://arxiv.org/abs/2111.02080)以理解[ICL](https://en.wikipedia.org/wiki/Prompt_engineering)在预训练过程中如何出现。根据作者的说法，[LLM](https://en.wikipedia.org/wiki/Large_language_model)使用[ICL](https://en.wikipedia.org/wiki/Prompt_engineering)来“定位”执行任务所需的概念。这个想法是，在训练过程中模型获得了潜在概念，然后在[ICL](https://en.wikipedia.org/wiki/Prompt_engineering)过程中再次找到它们。为了再次找到这些概念，[LLM](https://en.wikipedia.org/wiki/Large_language_model)可以使用提示的全部或部分组件：格式、输入、输出和输入-输出映射。
- en: 'As explained, [in a blog post by the authors](https://ai.stanford.edu/blog/understanding-incontext/),
    the model learns several concepts during training, after which the model uses
    the training examples to understand that the task in the prompt required either
    [sentiment analysis](https://en.wikipedia.org/wiki/Sentiment_analysis) or [topic
    classification](https://developers.google.com/machine-learning/guides/text-classification?hl=it)
    and at this point applies the mapping to the test input:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 正如[作者在博客文章中解释的](https://ai.stanford.edu/blog/understanding-incontext/)，模型在训练过程中学习了多个概念，然后模型使用训练示例来理解提示中的任务需要进行[情感分析](https://en.wikipedia.org/wiki/Sentiment_analysis)或[主题分类](https://developers.google.com/machine-learning/guides/text-classification?hl=it)，此时将映射应用于测试输入：
- en: In this paper, we study how in-context learning can emerge when pretraining
    documents have long-range coherence. Here, the LM must infer a latent document-level
    concept to generate coherent next tokens during pretraining. At test time, in-context
    learning occurs when the LM also infers a shared latent concept between examples
    in a prompt. ([source](https://arxiv.org/abs/2111.02080))
  id: totrans-79
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 在这篇论文中，我们研究了当预训练文档具有长程连贯性时，如何出现上下文学习。在这种情况下，语言模型必须推断出潜在的文档级概念，以在预训练期间生成连贯的下一个标记。在测试时，上下文学习发生在语言模型还推断出提示中示例之间的共享潜在概念时。（[来源](https://arxiv.org/abs/2111.02080)）
- en: '![](../Images/39f467017195bea30d100863084d5721.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/39f467017195bea30d100863084d5721.png)'
- en: 'image source: [here](https://arxiv.org/abs/2111.02080)'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：[这里](https://arxiv.org/abs/2111.02080)
- en: '**Yes but what is a concept?** for the authors is “*a latent variable that
    contains various document-level statistics.*” So a concept for a topic (for example,
    news) is the distribution of words (what words are used), format (how they are
    written), relationships between articles and topics, and so on.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '**是的，但什么是概念？** 对于作者来说，概念是“*一个包含各种文档级统计信息的潜在变量*”。因此，某个话题的概念（例如新闻）是单词的分布（使用了哪些单词）、格式（如何书写）、文章与话题之间的关系等。'
- en: The body of texts that are provided to the model are not random words, but the
    texts have their own [internal coherence](https://en.wikipedia.org/wiki/Coherence_(linguistics)).
    In other words, similar texts have similar semantic information (the same topic)
    and formatting (alternate programming documentation explanations and code snippets).
    By learning to predict a word given those precedences, the [LLM](https://en.wikipedia.org/wiki/Large_language_model)
    also models internal consistency and allows it to infer latent concepts that are
    in the prompt
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 提供给模型的文本不是随机的单词，而是文本具有其自身的[内部连贯性](https://en.wikipedia.org/wiki/Coherence_(linguistics))。换句话说，类似的文本具有相似的语义信息（相同的话题）和格式（不同的编程文档解释和代码片段）。通过学习根据这些前例预测一个单词，[LLM](https://en.wikipedia.org/wiki/Large_language_model)也建模内部一致性，并使其能够推断出提示中的潜在概念。
- en: 'In the authors'' words:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 作者的话：
- en: '**1\. Pretrain**: To predict the next token during pretraining, the LM must
    infer (“locate”) the latent concept for the document using evidence from the previous
    sentences.'
  id: totrans-85
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**1\. 预训练**：为了在预训练期间预测下一个标记，语言模型必须利用前面的句子中的证据来推断（“定位”）文档的潜在概念。'
- en: ''
  id: totrans-86
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**2\. In-context learning**: If the LM also infers the *prompt concept* (the
    latent concept shared by examples in the prompt) using in-context examples in
    the prompt, then in-context learning occurs! ([source](https://ai.stanford.edu/blog/understanding-incontext/))'
  id: totrans-87
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**2\. 上下文学习**：如果语言模型还通过提示中的上下文示例推断出*提示概念*（提示中示例共享的潜在概念），那么就会发生上下文学习！（[来源](https://ai.stanford.edu/blog/understanding-incontext/)）'
- en: So for the authors, this process of “locating” can be seen as [Bayesian inference](https://en.wikipedia.org/wiki/Bayesian_inference),
    in which the [LLMs](https://en.wikipedia.org/wiki/Large_language_model) infer
    concepts in the prompt (a concept that is shared by all the examples presented
    to it in the input prompt). Once he has inferred the concept he can then produce
    the correct answer
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 对于作者来说，这种“定位”过程可以视为[贝叶斯推断](https://en.wikipedia.org/wiki/Bayesian_inference)，其中[LLMs](https://en.wikipedia.org/wiki/Large_language_model)推断提示中的概念（所有示例在输入提示中共享的概念）。一旦他推断出概念，他就可以产生正确的答案。
- en: 'In formula:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 公式中：
- en: '![](../Images/8718900b9ea981fa05ea2b6b7117f0bf.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8718900b9ea981fa05ea2b6b7117f0bf.png)'
- en: 'image source: [here](https://arxiv.org/abs/2111.02080)'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：[这里](https://arxiv.org/abs/2111.02080)
- en: 'Ask nicely: effect of the prompt'
  id: totrans-92
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 请礼貌提问：提示的效果
- en: In recent work, [Min et al.](https://arxiv.org/abs/2202.12837) defined the characteristics
    of a prompt for ICL and how the various components of the prompt affect the performance
    of the model in doing ICL
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在近期的工作中，[Min等人](https://arxiv.org/abs/2202.12837)定义了ICL提示的特征以及提示的各种组件如何影响模型在ICL中的表现。
- en: '![](../Images/3dda25ecd69fc81d7b1be593488a2917.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3dda25ecd69fc81d7b1be593488a2917.png)'
- en: 'image source: [here](https://arxiv.org/abs/2202.12837)'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：[这里](https://arxiv.org/abs/2202.12837)
- en: 'Considering a demonstration as input-output pairs ( (x1, y1)…(xk, yk)) there
    are four formal aspects:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 认为演示作为输入输出对（(x1, y1)…(xk, yk)）有四个正式方面：
- en: '**The input-label mapping**. an input x is correctly paired with its label
    y.'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**输入-标签映射**。一个输入x与其标签y正确配对。'
- en: '**The distribution of the input text**, the distribution from where input x
    is extracted.'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**输入文本的分布**，即从中提取输入x的分布。'
- en: '**The label space** is the space of the y outputs.'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**标签空间**是y输出的空间。'
- en: '**The format**, specifically the pairing of the input-output pairs'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**格式**，特别是输入-输出对的配对'
- en: '[For the authors](https://arxiv.org/abs/2202.12837), the format, the distribution
    of the input, and label spaces are important. In contrast, input-label mapping
    matters little to [ICL](https://en.wikipedia.org/wiki/Prompt_engineering). [According
    to Stanford AI Lab](https://ai.stanford.edu/blog/understanding-incontext/), this
    would stem from the fact, that the model is already exposed to input-output matching
    during pretraining so it would not need the input-label mapping in the demonstration.
    Instead, the other elements are needed to be able to locate the concepts it has
    learned (in short perform [Bayesian inference](https://en.wikipedia.org/wiki/Bayesian_inference)).'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 对于[作者们](https://arxiv.org/abs/2202.12837)而言，格式、输入和标签空间的分布非常重要。相比之下，输入-标签映射对[ICL](https://en.wikipedia.org/wiki/Prompt_engineering)几乎没有影响。[根据斯坦福人工智能实验室](https://ai.stanford.edu/blog/understanding-incontext/)的说法，这源于模型在预训练期间已经接触过输入-输出匹配，因此在演示时不需要输入-标签映射。相反，其他元素是必需的，以便能够定位其已学习的概念（简而言之，执行[贝叶斯推断](https://en.wikipedia.org/wiki/Bayesian_inference)）。
- en: '![](../Images/6b897bdf4114db6d6f1cb57c2d7ca69d.png)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6b897bdf4114db6d6f1cb57c2d7ca69d.png)'
- en: 'image source: [here](https://arxiv.org/abs/2202.12837)'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：[这里](https://arxiv.org/abs/2202.12837)
- en: '[Another paper states](https://arxiv.org/abs/2205.12685) that actually input-label
    mapping, while [according to another](https://arxiv.org/abs/2303.03846) it is
    true that it is important but if the model is large enough it can learn the mapping
    on its own.'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '[另一篇论文指出](https://arxiv.org/abs/2205.12685)，实际上输入-标签映射是重要的，[根据另一篇论文](https://arxiv.org/abs/2303.03846)，虽然确实重要，但如果模型足够大，它可以自行学习映射。'
- en: For other authors, it is important that the demonstrations are different, simple,
    and similar anyway (at least in terms of structure). For another paper, the [order
    of the demonstrations is important](https://aclanthology.org/2022.acl-long.556/).
    Whereas, [Liu et al](https://aclanthology.org/2022.deelio-1.10.pdf), show that
    the choice of examples strongly impacts [ICL](https://en.wikipedia.org/wiki/Prompt_engineering).
    So one should choose examples that are close to an [embedding space](https://developers.google.com/machine-learning/crash-course/embeddings/video-lecture).
    In fact, one technique that shows results is when one provides a question to embed
    it and looks for examples that are close in distance in the [embedding](https://en.wikipedia.org/wiki/Embedding).
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 对于其他作者而言，演示的多样性、简洁性以及结构上的相似性都是重要的。对于另一篇论文，[演示的顺序很重要](https://aclanthology.org/2022.acl-long.556/)。然而，[刘等人](https://aclanthology.org/2022.deelio-1.10.pdf)显示，示例的选择对[ICL](https://en.wikipedia.org/wiki/Prompt_engineering)有很大影响。因此，应选择接近[嵌入空间](https://developers.google.com/machine-learning/crash-course/embeddings/video-lecture)的示例。事实上，一种有效的技术是提供一个问题以进行嵌入，并寻找在[嵌入](https://en.wikipedia.org/wiki/Embedding)中距离较近的示例。
- en: '![](../Images/1192be99f7221b72a5ae571369b3d2a8.png)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1192be99f7221b72a5ae571369b3d2a8.png)'
- en: 'image source: [here](https://aclanthology.org/2022.deelio-1.10.pdf)'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：[这里](https://aclanthology.org/2022.deelio-1.10.pdf)
- en: '[](https://levelup.gitconnected.com/the-ai-college-student-goes-back-to-the-bench-daa6d9bdfb14?source=post_page-----55bde1180610--------------------------------)
    [## The AI college student goes back to the bench'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://levelup.gitconnected.com/the-ai-college-student-goes-back-to-the-bench-daa6d9bdfb14?source=post_page-----55bde1180610--------------------------------)
    [## 人工智能大学生回到基础]'
- en: How LLM can solve college exams and why this is important
  id: totrans-109
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 大型语言模型如何解决大学考试及其重要性
- en: levelup.gitconnected.com](https://levelup.gitconnected.com/the-ai-college-student-goes-back-to-the-bench-daa6d9bdfb14?source=post_page-----55bde1180610--------------------------------)
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '[levelup.gitconnected.com](https://levelup.gitconnected.com/the-ai-college-student-goes-back-to-the-bench-daa6d9bdfb14?source=post_page-----55bde1180610--------------------------------)'
- en: A closed look to attention
  id: totrans-111
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 对注意力机制的深入探讨
- en: 'We have seen so far the role of the training dataset and the prompt, now it
    is time to closer look at the effect of architecture. An [LLM](https://en.wikipedia.org/wiki/Large_language_model)
    is a transformer, and the [transformer](https://en.wikipedia.org/wiki/Transformer_(machine_learning_model))
    is mainly based on [multi-head self-attention](https://en.wikipedia.org/wiki/Attention_(machine_learning)).
    Because [ICL](https://en.wikipedia.org/wiki/Prompt_engineering) is one of the
    most interesting behaviors of [LLMs](https://en.wikipedia.org/wiki/Large_language_model),
    many authors have focused on trying to find a mechanistic answer to how [ICL](https://en.wikipedia.org/wiki/Prompt_engineering)
    occurs:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 我们到目前为止已经看到训练数据集和提示的作用，现在是时候更仔细地研究架构的效果了。一个[LLM](https://en.wikipedia.org/wiki/Large_language_model)是一个变换器，而[变换器](https://en.wikipedia.org/wiki/Transformer_(machine_learning_model))主要基于[多头自注意力](https://en.wikipedia.org/wiki/Attention_(machine_learning))。由于[ICL](https://en.wikipedia.org/wiki/Prompt_engineering)是[LLMs](https://en.wikipedia.org/wiki/Large_language_model)最有趣的行为之一，许多作者专注于尝试找到[ICL](https://en.wikipedia.org/wiki/Prompt_engineering)发生的机制性答案：
- en: If we can understand the internal structures that cause Transformer models to
    produce the outputs they do, then we may be able to address current safety problems
    more systematically, as well as anticipating safety problems in future more powerful
    models. ([source](https://arxiv.org/abs/2209.11895))
  id: totrans-113
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 如果我们可以理解导致变换器模型产生其输出的内部结构，那么我们可能能够更系统地解决当前的安全问题，并预测未来更强大的模型中的安全问题。 ([source](https://arxiv.org/abs/2209.11895))
- en: '![](../Images/470625884f8f80cbbb190a076b5eeb81.png)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/470625884f8f80cbbb190a076b5eeb81.png)'
- en: 'image source: [here](https://arxiv.org/abs/1706.03762)'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：[这里](https://arxiv.org/abs/1706.03762)
- en: '[Researchers at Anthropic identified circuits](https://arxiv.org/abs/2209.11895)
    they called **induction heads**. An induction head is a circuit consisting of
    [two attention heads](https://en.wikipedia.org/wiki/Attention_(machine_learning))
    in different layers cooperating with each other to copy or complete patterns.
    Basically, the first attention head copies information from the previous token
    to the next one. The second attention head then has information about what happened
    previous to the present token. Then this mechanism can search the sequence where
    the present token A and sees the next token B, so the pattern once it sees A is
    more likely to produce output B.'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '[Anthropic 的研究人员识别了他们称为**归纳头**的电路](https://arxiv.org/abs/2209.11895)。归纳头是一个由[两个注意力头](https://en.wikipedia.org/wiki/Attention_(machine_learning))在不同层中协作以复制或完成模式的电路。基本上，第一个注意力头将信息从前一个标记复制到下一个标记。第二个注意力头则拥有关于当前标记之前发生的事情的信息。然后，这个机制可以搜索当前标记A所在的序列，并查看下一个标记B，因此一旦看到A，模式更有可能生成输出B。'
- en: '![](../Images/1c3b8ddbce7d102d33057ae158f67070.png)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1c3b8ddbce7d102d33057ae158f67070.png)'
- en: 'image source: [here](https://arxiv.org/abs/2209.11895)'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：[这里](https://arxiv.org/abs/2209.11895)
- en: '[For the authors](https://arxiv.org/abs/2209.11895), however, it is not a simple
    copying mechanism. In fact, in [inductive reasoning](https://en.wikipedia.org/wiki/Inductive_reasoning),
    we can infer that A is followed by B, if previously in context we saw that A most
    likely followed by B. For the authors then these induction heads crystallize this
    inference mechanism, which is not based on the training data but on the context:
    [A][B]…[A]→[B]'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '[对于作者们](https://arxiv.org/abs/2209.11895)来说，这并不是一种简单的复制机制。实际上，在[归纳推理](https://en.wikipedia.org/wiki/Inductive_reasoning)中，我们可以推断出A后面跟着B，如果在上下文中我们看到A最有可能跟着B。对作者们而言，这些归纳头使这种推理机制得以具体化，这种机制不是基于训练数据，而是基于上下文：[A][B]…[A]→[B]'
- en: '[For Anthropic](https://arxiv.org/abs/2209.11895) these induction heads play
    an important role in [ICL](https://en.wikipedia.org/wiki/Prompt_engineering).
    In fact, the fact that they can learn and repeat arbitrary sequences can be thought
    of as a simplified form of few-shot learning. In [a large model](https://en.wikipedia.org/wiki/Large_language_model),
    this effect is amplified, since they can work on abstract representations. Thus:
    “, t*he very same heads that do this sequence copying also take on a more expanded
    role of analogical sequence copying or in-context nearest neighbors*”.'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '[对于 Anthropic](https://arxiv.org/abs/2209.11895)，这些归纳头在[ICL](https://en.wikipedia.org/wiki/Prompt_engineering)中扮演了重要角色。事实上，它们能够学习和重复任意序列，可以被视为一种简化的少样本学习形式。在[大型模型](https://en.wikipedia.org/wiki/Large_language_model)中，这种效果会被放大，因为它们可以处理抽象表示。因此：“*进行序列复制的相同头部也承担了类比序列复制或上下文最近邻的更广泛角色*”。'
- en: 'Now, this mechanism is also interesting because it also promotes another kind
    of sequence completion: [A*][B*] … [A] → [B]. In this case A* and B* are not the
    same tokens A and B but tokens that are similar in [embedding space](https://en.wikipedia.org/wiki/Embedding)
    (for example, the same word in different languages).'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，这一机制也很有趣，因为它还促进了另一种序列完成：[A*][B*] … [A] → [B]。在这种情况下，A*和B*不是相同的令牌A和B，而是在[嵌入空间](https://en.wikipedia.org/wiki/Embedding)中相似的令牌（例如，不同语言中的相同单词）。
- en: These induction heads seem to appear as the [LLM](https://en.wikipedia.org/wiki/Large_language_model)
    improves its skill in ICL. Also, [for Anthrophic](https://arxiv.org/abs/2209.11895)
    in small [LMs](https://en.wikipedia.org/wiki/Large_language_model), one can observe
    this relationship with [ICL](https://en.wikipedia.org/wiki/Prompt_engineering)
    (for them the induction heads are the driver of [ICL](https://en.wikipedia.org/wiki/Prompt_engineering)).
    In addition, reverse engineering of these induction heads can be done for them,
    and this seems like a promising line of research to understand how they are formed
    and how they impact [ICL](https://en.wikipedia.org/wiki/Prompt_engineering).
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 这些引导头似乎在[LLM](https://en.wikipedia.org/wiki/Large_language_model)提高其ICL技能时出现。同时，在小型[LMs](https://en.wikipedia.org/wiki/Large_language_model)中，可以观察到这种关系与[ICL](https://en.wikipedia.org/wiki/Prompt_engineering)（对它们来说，引导头是[ICL](https://en.wikipedia.org/wiki/Prompt_engineering)的驱动因素）。此外，这些引导头的逆向工程可以完成，这似乎是理解它们如何形成以及如何影响[ICL](https://en.wikipedia.org/wiki/Prompt_engineering)的一个有前景的研究方向。
- en: '![](../Images/89d1bccf2e12244a91bab946d0f5db4d.png)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/89d1bccf2e12244a91bab946d0f5db4d.png)'
- en: 'image source: [here](https://arxiv.org/abs/2209.11895)'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：[这里](https://arxiv.org/abs/2209.11895)
- en: '[ICL](https://en.wikipedia.org/wiki/Prompt_engineering) in each case is linked
    and emerges through attention. This has a quadratic (in computational terms) cost,
    though. Several models with simplified forms of [attention](https://en.wikipedia.org/wiki/Attention_(machine_learning))
    (linear or logarithmic) were tested; however, this led to a decrease in expressiveness
    and impacted the [ICL](https://en.wikipedia.org/wiki/Prompt_engineering) ability
    of the model. Therefore, although an alternative to [multi-head self-attention](https://en.wikipedia.org/wiki/Attention_(machine_learning))
    is sought, the authors take care that their proposed model is capable of [ICL](https://en.wikipedia.org/wiki/Prompt_engineering).'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 每个案例中的[ICL](https://en.wikipedia.org/wiki/Prompt_engineering)都是通过注意力机制关联并产生的。不过，这有一个二次（计算术语中）的成本。一些具有简化形式的[注意力](https://en.wikipedia.org/wiki/Attention_(machine_learning))（线性或对数）模型被测试过；然而，这导致了表现力的下降，并影响了模型的[ICL](https://en.wikipedia.org/wiki/Prompt_engineering)能力。因此，虽然寻求[多头自注意力](https://en.wikipedia.org/wiki/Attention_(machine_learning))的替代方案，但作者们确保他们提出的模型能够进行[ICL](https://en.wikipedia.org/wiki/Prompt_engineering)。
- en: '[](https://levelup.gitconnected.com/welcome-back-80s-transformers-could-be-blown-away-by-convolution-21ff15f6d1cc?source=post_page-----55bde1180610--------------------------------)
    [## Welcome Back 80s: Transformers Could Be Blown Away by Convolution'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://levelup.gitconnected.com/welcome-back-80s-transformers-could-be-blown-away-by-convolution-21ff15f6d1cc?source=post_page-----55bde1180610--------------------------------)
    [## 欢迎回到80年代：卷积可能会超越变换器'
- en: The Hyena model shows how convolution could be faster than self-attention
  id: totrans-127
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Hyena模型展示了卷积如何比自注意力更快
- en: levelup.gitconnected.com](https://levelup.gitconnected.com/welcome-back-80s-transformers-could-be-blown-away-by-convolution-21ff15f6d1cc?source=post_page-----55bde1180610--------------------------------)
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: levelup.gitconnected.com](https://levelup.gitconnected.com/welcome-back-80s-transformers-could-be-blown-away-by-convolution-21ff15f6d1cc?source=post_page-----55bde1180610--------------------------------)
- en: Learning to learn the context
  id: totrans-129
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 学会学习上下文
- en: Clearly, since the [transformer](https://en.wikipedia.org/wiki/Transformer_(machine_learning_model))
    is trained through [gradient descent](https://en.wikipedia.org/wiki/Gradient_descent)
    there is a relationship between the latter and [ICL](https://en.wikipedia.org/wiki/Prompt_engineering).
    Using [linear regression](https://en.wikipedia.org/wiki/Linear_regression) as
    a starting point, [Akyürek suggests](https://arxiv.org/abs/2211.15661) that transformers
    implicitly treat [ICL](https://en.wikipedia.org/wiki/Prompt_engineering) as an
    optimization problem.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，由于[变换器](https://en.wikipedia.org/wiki/Transformer_(machine_learning_model))是通过[梯度下降](https://en.wikipedia.org/wiki/Gradient_descent)训练的，因此后者与[ICL](https://en.wikipedia.org/wiki/Prompt_engineering)之间存在关系。以[线性回归](https://en.wikipedia.org/wiki/Linear_regression)为起点，[Akyürek建议](https://arxiv.org/abs/2211.15661)变换器隐式地将[ICL](https://en.wikipedia.org/wiki/Prompt_engineering)视为一个优化问题。
- en: '[Oswald](https://arxiv.org/abs/2212.07677) showed that [transformer layers](https://en.wikipedia.org/wiki/Transformer_(machine_learning_model))
    can theoretically implement [gradient descent](https://en.wikipedia.org/wiki/Gradient_descent)
    on the in-context data. According to [Oswald](https://arxiv.org/abs/2212.07677),
    in-context learning mimics [gradient descent](https://en.wikipedia.org/wiki/Gradient_descent)
    in certain cases. [This paper](https://arxiv.org/abs/2212.07677) showed that there
    is an [ICL](https://en.wikipedia.org/wiki/Prompt_engineering) and gradient descent
    relationship.'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '[Oswald](https://arxiv.org/abs/2212.07677)展示了[变换器层](https://en.wikipedia.org/wiki/Transformer_(machine_learning_model))理论上可以在上下文数据上实现[梯度下降](https://en.wikipedia.org/wiki/Gradient_descent)。根据[Oswald](https://arxiv.org/abs/2212.07677)的说法，上下文学习在某些情况下模仿[梯度下降](https://en.wikipedia.org/wiki/Gradient_descent)。[这篇论文](https://arxiv.org/abs/2212.07677)展示了[ICL](https://en.wikipedia.org/wiki/Prompt_engineering)和梯度下降之间的关系。'
- en: '![](../Images/128ac013ae9e45af341a022694e50030.png)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/128ac013ae9e45af341a022694e50030.png)'
- en: 'Image source: [here](https://arxiv.org/abs/2212.07677)'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：[这里](https://arxiv.org/abs/2212.07677)
- en: The results above, and [Akyürek’s results](https://arxiv.org/abs/2211.15661),
    mean that models doing in-context learning are not just matching previous patterns,
    but instead are also learning how to perform other tasks (an extension of what
    was said with induction heads). In fact, [Akyürek](https://arxiv.org/abs/2211.15661)
    provided prompts containing synthetic data to prevent the model from having already
    seen the data.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 上述结果，以及[Akyürek的结果](https://arxiv.org/abs/2211.15661)，意味着进行上下文学习的模型不仅仅是匹配先前的模式，而是还学会了如何执行其他任务（这是对诱导头所说内容的扩展）。实际上，[Akyürek](https://arxiv.org/abs/2211.15661)提供了包含合成数据的提示，以防模型已经见过这些数据。
- en: '[Akyürek’s hypothesis](https://arxiv.org/abs/2211.15661) is then the models
    internally perform sort of machine learning algorithms (which in part echoes but
    extends the idea that the model does Bayesian inference). In the article, they
    state that the model implements in its hidden states a linear model, and this
    is learned during training.'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '[Akyürek的假设](https://arxiv.org/abs/2211.15661)是模型在内部执行某种机器学习算法（这在某种程度上呼应但扩展了模型进行贝叶斯推断的想法）。在文章中，他们表示模型在其隐藏状态中实现了一个线性模型，并且这一点在训练过程中得到了学习。'
- en: “In this case, we tried to recover the actual solution to the linear model,
    and we could show that the parameter is written in the hidden states. This means
    the linear model is in there somewhere,” Akyürek says. ([source](https://news.mit.edu/2023/large-language-models-in-context-learning-0207))
  id: totrans-136
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “在这种情况下，我们尝试恢复线性模型的实际解决方案，我们能够证明参数被写在隐藏状态中。这意味着线性模型某处确实存在，”Akyürek说。（[来源](https://news.mit.edu/2023/large-language-models-in-context-learning-0207)）
- en: '[In an intriguing experiment](https://ai.googleblog.com/2023/05/larger-language-models-do-in-context.html),
    Google tested whether models via [ICL](https://en.wikipedia.org/wiki/Prompt_engineering)
    can override previous [prior knowledge](https://en.wikipedia.org/wiki/Prior_knowledge_for_pattern_recognition),
    for the authors this is also an example of the emergent property of broad [LLMs](https://en.wikipedia.org/wiki/Large_language_model).'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '[在一个有趣的实验中](https://ai.googleblog.com/2023/05/larger-language-models-do-in-context.html)，Google测试了通过[ICL](https://en.wikipedia.org/wiki/Prompt_engineering)模型是否能够覆盖先前的[先验知识](https://en.wikipedia.org/wiki/Prior_knowledge_for_pattern_recognition)，对作者来说，这也是广泛的[LLMs](https://en.wikipedia.org/wiki/Large_language_model)的一个新兴特性例子。'
- en: In one of the experiments, the authors performed regular [ICL](https://en.wikipedia.org/wiki/Prompt_engineering),
    flipped ICL (where labels are flipped), and semantically-unrelated label ICL (SUL-ICL)
    where labels are words that are not semantically related.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 在其中一个实验中，作者进行了常规的[ICL](https://en.wikipedia.org/wiki/Prompt_engineering)、翻转的ICL（标签被翻转）和语义不相关的标签ICL（SUL-ICL），其中标签是没有语义相关性的词汇。
- en: '![](../Images/2807e4985bd1bae2dd2215dc62c8d9df.png)'
  id: totrans-139
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2807e4985bd1bae2dd2215dc62c8d9df.png)'
- en: 'image source: [here](https://arxiv.org/abs/2303.03846)'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：[这里](https://arxiv.org/abs/2303.03846)
- en: This article shows some interesting things. When the labels are flipped (but
    the ground-truth evaluation is kept the same) if the model is able to override
    its prior knowledge it should have a decrease. The result is that the performance
    of small models stays flat, while there is a drop for [large models](https://en.wikipedia.org/wiki/Large_language_model).
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 这篇文章展示了一些有趣的现象。当标签被翻转（但真实评价保持不变）时，如果模型能够覆盖其先验知识，则应出现性能下降。结果是小模型的性能保持平稳，而[大模型](https://en.wikipedia.org/wiki/Large_language_model)则出现了下降。
- en: These results indicate that large models can override prior knowledge from pre-training
    when contradicting input-label mappings are presented in-context. Small models
    can’t do this, making this ability an emergent phenomena of model scale. ([source](https://ai.googleblog.com/2023/05/larger-language-models-do-in-context.html))
  id: totrans-142
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 这些结果表明，当矛盾的输入-标签映射在语境中出现时，大型模型可以覆盖预训练中的先验知识。小模型无法做到这一点，使这一能力成为模型规模的突现现象。([source](https://ai.googleblog.com/2023/05/larger-language-models-do-in-context.html))
- en: '![](../Images/15b8229c8301c802ce1b0c2a46c4b578.png)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/15b8229c8301c802ce1b0c2a46c4b578.png)'
- en: 'image source: [here](https://arxiv.org/abs/2303.03846)'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：[这里](https://arxiv.org/abs/2303.03846)
- en: Second, the model can also learn from input-label mappings when provided in
    the demonstration of semantically-irrelevant labels (“foo/bar” instead of “negative/positive”
    for sentiment analysis). A model that relies only on [prior knowledge](https://en.wikipedia.org/wiki/Prior_knowledge_for_pattern_recognition)
    should have a performance drop because it cannot exploit the semantic meaning
    of labels for predictions. In fact, small models have a drop in prediction, while
    [LLMs](https://en.wikipedia.org/wiki/Large_language_model) do not. For the authors,
    this means that while small models rely on [prior knowledge](https://en.wikipedia.org/wiki/Prior_knowledge_for_pattern_recognition),
    “*large models, on the other hand, have the ability to learn input-label mappings
    in context when the semantic nature of labels is removed.*”
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，当在语义无关的标签（例如“foo/bar”而不是“负面/正面”用于情感分析）的演示中提供输入-标签映射时，模型也可以学习。从[先验知识](https://en.wikipedia.org/wiki/Prior_knowledge_for_pattern_recognition)中仅依赖的模型应该会有性能下降，因为它无法利用标签的语义意义进行预测。事实上，小模型的预测能力下降，而[LLM](https://en.wikipedia.org/wiki/Large_language_model)则没有。对作者而言，这意味着虽然小模型依赖于[先验知识](https://en.wikipedia.org/wiki/Prior_knowledge_for_pattern_recognition)，但“*大型模型在语境中有能力学习输入-标签映射，当标签的语义性质被移除时。*”
- en: '![](../Images/d166a16fd271cee5b30b5c7c55da4a41.png)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d166a16fd271cee5b30b5c7c55da4a41.png)'
- en: 'image source: [here](https://arxiv.org/abs/2303.03846)'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：[这里](https://arxiv.org/abs/2303.03846)
- en: '[The authors also took a look](https://arxiv.org/abs/2303.03846) at what is
    the effect of instruction tuning on ICL. During instruction tuning, instructions
    are given to the model that often contains questions and answers. So this process
    involves natural language labels, and the authors wondered whether it impacts
    an [LLM’s](https://en.wikipedia.org/wiki/Large_language_model) ability to learn
    input-label mappings or exploit semantic prior knowledge. The experiments show
    that: “instruction tuning improves the ability to learn input-label mappings,
    it strengthens the usage of semantic prior knowledge more.”'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '[作者们还研究了](https://arxiv.org/abs/2303.03846)指令调整对 ICL 的影响。在指令调整过程中，模型会接收到包含问题和答案的指令。因此，这个过程涉及自然语言标签，作者们想知道这是否会影响[LLM](https://en.wikipedia.org/wiki/Large_language_model)学习输入-标签映射或利用语义先验知识的能力。实验表明：“指令调整提高了学习输入-标签映射的能力，它增强了对语义先验知识的使用。”'
- en: 'So these results show that it is not only the architecture, the amount of data,
    and the prompt that influence ICL, but [for Google also the number of parameters
    themselves](https://ai.googleblog.com/2023/05/larger-language-models-do-in-context.html):'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，这些结果表明，不仅是架构、数据量和提示影响 ICL，[对于 Google 来说，参数的数量本身](https://ai.googleblog.com/2023/05/larger-language-models-do-in-context.html)也很重要：
- en: These results underscore how the in-context learning behavior of language models
    can change depending on the scale of the language model, and that larger language
    models have an emergent ability to map inputs to many types of labels, a form
    of true symbolic reasoning in which input–label mappings can be learned for arbitrary
    symbols. ([source](https://arxiv.org/pdf/2303.03846.pdf))
  id: totrans-150
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 这些结果强调了语言模型的语境学习行为如何根据语言模型的规模而变化，以及更大的语言模型具有将输入映射到多种类型标签的突现能力，这是一种真正的符号推理形式，其中输入-标签映射可以为任意符号学习。
    ([source](https://arxiv.org/pdf/2303.03846.pdf))
- en: '[](https://salvatore-raieli.medium.com/scaling-isnt-everything-how-bigger-models-fail-harder-d64589be4f04?source=post_page-----55bde1180610--------------------------------)
    [## Scaling Isn’t Everything: How Bigger Models Fail Harder'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://salvatore-raieli.medium.com/scaling-isnt-everything-how-bigger-models-fail-harder-d64589be4f04?source=post_page-----55bde1180610--------------------------------)
    [## 扩展并非一切：更大模型如何更容易失败'
- en: Are Large Language Models really understanding programming languages?
  id: totrans-152
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 大型语言模型是否真的理解编程语言？
- en: salvatore-raieli.medium.com](https://salvatore-raieli.medium.com/scaling-isnt-everything-how-bigger-models-fail-harder-d64589be4f04?source=post_page-----55bde1180610--------------------------------)
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: '[salvatore-raieli.medium.com](https://salvatore-raieli.medium.com/scaling-isnt-everything-how-bigger-models-fail-harder-d64589be4f04?source=post_page-----55bde1180610--------------------------------)'
- en: Conclusions, challenges, and perspective
  id: totrans-154
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论、挑战和前景
- en: '![](../Images/986b2920355032f6fc74247aeef7d0a9.png)'
  id: totrans-155
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/986b2920355032f6fc74247aeef7d0a9.png)'
- en: Photo by [Nadine Shaabana](https://unsplash.com/@nadineshaabana?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: '[Nadine Shaabana](https://unsplash.com/@nadineshaabana?utm_source=medium&utm_medium=referral)
    在 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral) 上传的照片'
- en: “Separate text from context and all that remains is a con.” ― **Stewart Stafford**
  id: totrans-157
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “将文本与上下文分开，剩下的只是骗局。” ― **斯图尔特·斯塔福德**
- en: ''
  id: totrans-158
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: “Words are never good or bad on their own, context makes them so.” ― **Abhijit
    Naskar**
  id: totrans-159
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “单词本身从来没有好坏之分，是上下文让它们如此。” ― **阿比吉特·纳斯卡尔**
- en: In-context learning is one of the most interesting and elusive behaviors of
    [LLMs](https://en.wikipedia.org/wiki/Large_language_model). First admired with
    the publication of GPT-3, it has excited the community about its possible applications.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 在-context 学习是 [LLMs](https://en.wikipedia.org/wiki/Large_language_model) 最有趣且难以捉摸的行为之一。GPT-3
    发布时首次受到赞赏，它让社区对其潜在应用充满兴奋。
- en: ICL in simple terms is the ability to learn from analogy. It only takes a few
    examples in a demonstration for the model to make a prediction. Which allows the
    model unprecedented versatility and the possibility of developing endless applications.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: ICL 简单来说就是从类比中学习的能力。只需在演示中提供几个例子，模型就能做出预测。这使模型具备前所未有的多功能性和无限应用的发展可能。
- en: Despite this, we still do not understand precisely how it originates during
    training. We have seen the importance of training data, the prompt, or attention
    itself. Today, with the idea of wanting to replace attention-based models with
    new architectures we need to understand how to preserve ICL.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管如此，我们仍然不完全理解它在训练过程中如何产生。我们已经看到了训练数据、提示或注意力本身的重要性。今天，随着希望用新架构取代基于注意力的模型的想法，我们需要了解如何保留
    ICL。
- en: 'Research in ICL is very active, some of the lines of research are:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: ICL 研究非常活跃，一些研究方向包括：
- en: '**New Pretraining Strategie**s, as mentioned if a training strategy increases
    performance classically (decrease in perplexity) it does not mean that it increases
    the ICL skills of the model. Therefore, focused strategies are sought to increase
    a model’s ICL skills.'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**新预训练策略**，如前所述，如果一种训练策略传统上提高了性能（困惑度下降），并不意味着它提高了模型的 ICL 技能。因此，正在寻找专注于提高模型
    ICL 技能的策略。'
- en: '**ICL Ability Distillation**, ICL seems to emerge with the scale of the model,
    but if one were able to distill these skills into smaller models we would have
    savings in computational cost, memory, and infrastructure. Therefore, distillation
    seems promising for smaller models with ICL. [Preliminary studies look promising](https://arxiv.org/abs/2212.08410).'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ICL 能力蒸馏**，ICL 似乎随着模型的规模而出现，但如果能够将这些技能蒸馏到较小的模型中，我们将节省计算成本、内存和基础设施。因此，蒸馏对于具有
    ICL 的较小模型似乎很有前景。[初步研究显示前景光明](https://arxiv.org/abs/2212.08410)。'
- en: '**ICL Robustness.** As we have seen ICL skills are not stable, permutations
    and changes in the format of the demonstration impact ICL. [In one study it is
    shown](https://arxiv.org/abs/2209.07661) that increasing robustness comes at the
    cost of decreasing accuracy, so we need studies that delve into how ICL works.
    A better theoretical understanding can help develop a more robust ICL.'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ICL 鲁棒性**。正如我们所见，ICL 技能并不稳定，演示格式的排列和变化会影响 ICL。[有一项研究表明](https://arxiv.org/abs/2209.07661)，提高鲁棒性会以降低准确度为代价，因此我们需要深入研究
    ICL 的工作机制。更好的理论理解可以帮助开发更鲁棒的 ICL。'
- en: '**ICL Efficiency and Scalability.** ICL requires different examples in the
    demonstration. In theory, more examples improve ICL. Increasing the number of
    examples has a computational cost, which comes from calculating attention (efficiency).
    The other challenge is that you cannot add more examples than the context window
    allows (scalability). [As we saw in a previous article](/speak-to-me-how-many-words-a-model-is-reading-331e3af86d27),
    research has been very active in how to extend the context window (and what strategies
    have been used), although it is unclear whether the model can then exploit it.
    Also, in some cases, inverse scaling was seen, where the model instead of following
    in-context instruction regurgitated memorized data.'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ICL的效率和可扩展性。** ICL需要在演示中使用不同的示例。理论上，更多的示例可以改善ICL。增加示例的数量有计算成本，这来自于计算注意力（效率）。另一个挑战是，你不能添加超过上下文窗口允许的示例（可扩展性）。[正如我们在之前的文章中看到的](/speak-to-me-how-many-words-a-model-is-reading-331e3af86d27)，研究在如何扩展上下文窗口（以及使用了哪些策略）方面非常活跃，尽管尚不清楚模型是否能够利用它。此外，在某些情况下，出现了逆向扩展，其中模型不是遵循上下文中的指令，而是重复了记忆的数据。'
- en: Another line of research is the development of techniques that can improve ICL
    by acting on the format of the prompt. Several interesting approaches have been
    proposed over time (including Chain of thought (COT), Self-consistency COT, Tree
    of Thoughts, and so on). These approaches have shown success in being able to
    improve model performance for mathematical exercises and other reasoning problems.
    All this is done simply through modifications of the prompt. In this article,
    I have focused on more mechanistic aspects of the model, training data, the prompt,
    and how ICL emerges. In the next article, I will discuss these approaches in detail.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个研究方向是开发可以通过调整提示格式来提高ICL（In-context Learning，情境学习）的技术。随着时间的推移，已经提出了几种有趣的方法（包括思维链（COT）、自一致性COT、思维树等）。这些方法在改善数学练习和其他推理问题的模型性能方面已显示出成功。所有这些都是通过简单地修改提示实现的。在本文中，我关注了模型的更机械化的方面、训练数据、提示以及ICL是如何出现的。在下一篇文章中，我将详细讨论这些方法。
- en: What do guys think? Let me know in the comments
  id: totrans-169
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 大家怎么看？在评论中告诉我吧。
- en: 'If you have found this interesting:'
  id: totrans-170
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如果你觉得这很有趣：
- en: '*You can look for my other articles, you can also* [***subscribe***](https://salvatore-raieli.medium.com/subscribe)
    *to get notified when I publish articles, you can* [***become a Medium member***](https://medium.com/@salvatore-raieli/membership)
    *to access all its stories (affiliate links of the platform for which I get small
    revenues without cost to you) and you can also connect or reach me on*[***LinkedIn***](https://www.linkedin.com/in/salvatore-raieli/)***.***'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: '*你可以查看我的其他文章，也可以* [***订阅***](https://salvatore-raieli.medium.com/subscribe)
    *以便在我发布文章时收到通知，你也可以* [***成为Medium会员***](https://medium.com/@salvatore-raieli/membership)
    *来访问所有故事（这是平台的会员链接，我从中获得少量收入，但对你没有费用），你还可以在*[***LinkedIn***](https://www.linkedin.com/in/salvatore-raieli/)***上与我联系。***'
- en: '*Here is the link to my GitHub repository, where I am planning to collect code
    and many resources related to machine learning, artificial intelligence, and more.*'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: '*这里是我的GitHub仓库的链接，我计划在这里收集与机器学习、人工智能等相关的代码和许多资源。*'
- en: '[](https://github.com/SalvatoreRa/tutorial?source=post_page-----55bde1180610--------------------------------)
    [## GitHub - SalvatoreRa/tutorial: Tutorials on machine learning, artificial intelligence,
    data science…'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: '[## GitHub - SalvatoreRa/tutorial: 机器学习、人工智能、数据科学的教程…](https://github.com/SalvatoreRa/tutorial?source=post_page-----55bde1180610--------------------------------)'
- en: Tutorials on machine learning, artificial intelligence, data science with math
    explanation and reusable code (in python…
  id: totrans-174
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 关于机器学习、人工智能、数据科学的教程，包括数学解释和可重复使用的代码（用Python编写…）。
- en: github.com](https://github.com/SalvatoreRa/tutorial?source=post_page-----55bde1180610--------------------------------)
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: '[github.com](https://github.com/SalvatoreRa/tutorial?source=post_page-----55bde1180610--------------------------------)'
- en: Reference
  id: totrans-176
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: Here is the list of the principal references I consulted to write this article,
    only the first name for an article is cited. I suggest also them if you want to
    deepen on the topic.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我撰写本文时参考的主要文献列表，仅引用了文章的第一个名字。如果你想深入了解这个话题，我也推荐这些文献。
- en: Brown, 2020, Language Models are Few-Shot Learners, [link](https://arxiv.org/abs/2005.14165)
  id: totrans-178
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Brown, 2020, 语言模型是少样本学习者，[链接](https://arxiv.org/abs/2005.14165)
- en: Dong, 2022, A Survey on In-context Learning, [link](https://arxiv.org/abs/2301.00234)
  id: totrans-179
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Dong, 2022, 关于情境学习的调查，[链接](https://arxiv.org/abs/2301.00234)
- en: Zhao, A Survey of Large Language Models, [link](https://arxiv.org/abs/2303.18223)
  id: totrans-180
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 赵，《大语言模型调查》，[链接](https://arxiv.org/abs/2303.18223)
- en: Xie, 2022, How does in-context learning work? A framework for understanding
    the differences from traditional supervised learning. [link](https://ai.stanford.edu/blog/understanding-incontext/)
  id: totrans-181
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 谢，2022年，《上下文学习是如何工作的？理解与传统监督学习的差异的框架》，[链接](https://ai.stanford.edu/blog/understanding-incontext/)
- en: Wei, 2022, Emergent Abilities of Large Language Models, [link](https://arxiv.org/abs/2206.07682)
  id: totrans-182
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 魏，2022年，《大语言模型的突现能力》，[链接](https://arxiv.org/abs/2206.07682)
- en: Zhou, 2022, Teaching Algorithmic Reasoning via In-context Learning, [link](https://arxiv.org/abs/2211.09066)
  id: totrans-183
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 周，2022年，《通过上下文学习教授算法推理》，[链接](https://arxiv.org/abs/2211.09066)
- en: '[Vinita Silaparasetty](https://medium.com/u/8f22c49c614?source=post_page-----55bde1180610--------------------------------),
    What is Prompt Engineering?, [link](https://medium.com/@vinitasilaparasetty/what-is-prompt-engineering-8221e0aa619d)'
  id: totrans-184
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[Vinita Silaparasetty](https://medium.com/u/8f22c49c614?source=post_page-----55bde1180610--------------------------------)，《什么是提示工程？》，[链接](https://medium.com/@vinitasilaparasetty/what-is-prompt-engineering-8221e0aa619d)'
- en: '[Fareed Khan](https://medium.com/u/b856005e5ecd?source=post_page-----55bde1180610--------------------------------),
    Prompt Engineering Complete Guide, [link](https://medium.com/@fareedkhandev/prompt-engineering-complete-guide-2968776f0431)'
  id: totrans-185
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[Fareed Khan](https://medium.com/u/b856005e5ecd?source=post_page-----55bde1180610--------------------------------)，《提示工程完全指南》，[链接](https://medium.com/@fareedkhandev/prompt-engineering-complete-guide-2968776f0431)'
- en: '[Paul DelSignore](https://medium.com/u/6202cb40e768?source=post_page-----55bde1180610--------------------------------),
    The Dark Side Of Prompt Engineering, [link](https://medium.com/the-generator/the-dark-side-of-prompt-engineering-33b8087ffd59)'
  id: totrans-186
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[Paul DelSignore](https://medium.com/u/6202cb40e768?source=post_page-----55bde1180610--------------------------------)，《提示工程的黑暗面》，[链接](https://medium.com/the-generator/the-dark-side-of-prompt-engineering-33b8087ffd59)'
- en: '[Babar M Bhatti](https://medium.com/u/10dee34829b?source=post_page-----55bde1180610--------------------------------),
    The Art and Science of Crafting Effective Prompts for LLMs, [link](https://thebabar.medium.com/the-art-and-science-of-crafting-effective-prompts-for-llms-e04447e8f96a)'
  id: totrans-187
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[Babar M Bhatti](https://medium.com/u/10dee34829b?source=post_page-----55bde1180610--------------------------------)，《为LLMs制作有效提示的艺术与科学》，[链接](https://thebabar.medium.com/the-art-and-science-of-crafting-effective-prompts-for-llms-e04447e8f96a)'
- en: Dai, 2022, Why Can GPT Learn In-Context? Language Models Implicitly Perform
    Gradient Descent as Meta-Optimizers, [link](https://arxiv.org/abs/2212.10559)
  id: totrans-188
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 戴，2022年，《为什么GPT能在上下文中学习？语言模型隐式地执行梯度下降作为元优化器》，[链接](https://arxiv.org/abs/2212.10559)
- en: Shin, 2022, On the Effect of Pretraining Corpora on In-context Learning by a
    Large-scale Language Model, [link](https://aclanthology.org/2022.naacl-main.380/)
  id: totrans-189
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 辛，2022年，《大规模语言模型预训练语料对上下文学习的影响》，[链接](https://aclanthology.org/2022.naacl-main.380/)
- en: '[Cameron R. Wolfe, Ph.D.](https://medium.com/u/28aa6026c553?source=post_page-----55bde1180610--------------------------------),
    Language Model Scaling Laws and GPT-3, [link](/language-model-scaling-laws-and-gpt-3-5cdc034e67bb)'
  id: totrans-190
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[Cameron R. Wolfe, Ph.D.](https://medium.com/u/28aa6026c553?source=post_page-----55bde1180610--------------------------------)，《语言模型的扩展法则与GPT-3》，[链接](/language-model-scaling-laws-and-gpt-3-5cdc034e67bb)'
- en: Xie, 2021, An Explanation of In-context Learning as Implicit Bayesian Inference
    , [link](https://arxiv.org/abs/2111.02080)
  id: totrans-191
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 谢，2021年，《将上下文学习解释为隐式贝叶斯推断》，[链接](https://arxiv.org/abs/2111.02080)
- en: Huszár, 2022, Implicit Bayesian Inference in Large Language Models, [link](https://www.inference.vc/implicit-bayesian-inference-in-sequence-models/)
  id: totrans-192
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Huszár，2022年，《大语言模型中的隐式贝叶斯推断》，[链接](https://www.inference.vc/implicit-bayesian-inference-in-sequence-models/)
- en: 'Min, 2022, Rethinking the Role of Demonstrations: What Makes In-Context Learning
    Work?, [link](https://arxiv.org/abs/2202.12837)'
  id: totrans-193
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 闵，2022年，《重新思考示范的作用：什么使得上下文学习有效？》，[链接](https://arxiv.org/abs/2202.12837)
- en: Chan, 2022, Data Distributional Properties Drive Emergent In-Context Learning
    in Transformers, [link](https://arxiv.org/abs/2205.05055)
  id: totrans-194
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 陈，2022年，《数据分布属性驱动变压器中的突现上下文学习》，[链接](https://arxiv.org/abs/2205.05055)
- en: Liu, 2023, What Makes Good In-Context Examples for GPT-3?, [link](https://aclanthology.org/2022.deelio-1.10/)
  id: totrans-195
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 刘，2023年，《什么样的上下文示例对GPT-3有效？》，[链接](https://aclanthology.org/2022.deelio-1.10/)
- en: '[Priyanka](https://medium.com/u/29db232e8826?source=post_page-----55bde1180610--------------------------------),
    Perplexity of Language Models, [link](https://medium.com/@priyankads/perplexity-of-language-models-41160427ed72)'
  id: totrans-196
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[Priyanka](https://medium.com/u/29db232e8826?source=post_page-----55bde1180610--------------------------------)，《语言模型的困惑度》，[链接](https://medium.com/@priyankads/perplexity-of-language-models-41160427ed72)'
- en: Olsson, 2022, In-context Learning and Induction Heads, link
  id: totrans-197
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 奥尔松，2022年，《上下文学习与归纳头》，[链接]
- en: Wies, 2023, The Learnability of In-Context Learning, [link](https://arxiv.org/abs/2303.07895)
  id: totrans-198
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Wies, 2023，《上下文学习的可学习性》，[链接](https://arxiv.org/abs/2303.07895)
- en: Akyürek, 2022, What learning algorithm is in-context learning? Investigations
    with linear models, [link](https://arxiv.org/abs/2211.15661)
  id: totrans-199
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Akyürek, 2022，《什么是上下文学习的学习算法？线性模型的研究》，[链接](https://arxiv.org/abs/2211.15661)
- en: Oswald, 2022, Transformers learn in-context by gradient descent, [link](https://arxiv.org/abs/2212.07677)
  id: totrans-200
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Oswald, 2022，《变压器通过梯度下降在上下文中学习》，[链接](https://arxiv.org/abs/2212.07677)
- en: Wei, 2023, Larger language models do in-context learning differently, [link](https://arxiv.org/abs/2303.03846)
  id: totrans-201
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Wei, 2023，《更大的语言模型在上下文学习中的表现不同》，[链接](https://arxiv.org/abs/2303.03846)
- en: Google blog, Larger language models do in-context learning differently, [link](https://ai.googleblog.com/2023/05/larger-language-models-do-in-context.html)
  id: totrans-202
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Google 博客，《更大的语言模型在上下文学习中的表现不同》，[链接](https://ai.googleblog.com/2023/05/larger-language-models-do-in-context.html)
- en: Zewe, Solving a machine-learning mystery, [link](https://news.mit.edu/2023/large-language-models-in-context-learning-0207)
  id: totrans-203
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Zewe，《解决机器学习的谜团》，[链接](https://news.mit.edu/2023/large-language-models-in-context-learning-0207)
- en: Kaddour, 2023, Challenges and Applications of Large Language Models, [link](https://arxiv.org/abs/2307.10169)
  id: totrans-204
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Kaddour, 2023，《大型语言模型的挑战与应用》，[链接](https://arxiv.org/abs/2307.10169)
- en: Magister, 2022, Teaching Small Language Models to Reason, [link](https://arxiv.org/abs/2212.08410)
  id: totrans-205
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Magister, 2022，《教小型语言模型推理》，[链接](https://arxiv.org/abs/2212.08410)
- en: Chen, 2022, On the Relation between Sensitivity and Accuracy in In-context Learning,
    [link](https://arxiv.org/abs/2209.07661)
  id: totrans-206
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Chen, 2022，《上下文学习中敏感性与准确性之间的关系》，[链接](https://arxiv.org/abs/2209.07661)
