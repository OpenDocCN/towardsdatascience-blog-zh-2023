- en: 3 Use-Cases for Gaussian Mixture Models (GMM)
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: é«˜æ–¯æ··åˆæ¨¡å‹ï¼ˆGMMï¼‰çš„3ä¸ªåº”ç”¨åœºæ™¯
- en: åŸæ–‡ï¼š[https://towardsdatascience.com/3-use-cases-for-gaussian-mixture-model-gmm-72951fcf8363?source=collection_archive---------1-----------------------#2023-07-27](https://towardsdatascience.com/3-use-cases-for-gaussian-mixture-model-gmm-72951fcf8363?source=collection_archive---------1-----------------------#2023-07-27)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡ï¼š[https://towardsdatascience.com/3-use-cases-for-gaussian-mixture-model-gmm-72951fcf8363?source=collection_archive---------1-----------------------#2023-07-27](https://towardsdatascience.com/3-use-cases-for-gaussian-mixture-model-gmm-72951fcf8363?source=collection_archive---------1-----------------------#2023-07-27)
- en: Feature engineering, unsupervised classification, and anomaly detection with
    the versatility of the GMM algorithm
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ç‰¹å¾å·¥ç¨‹ã€æ— ç›‘ç£åˆ†ç±»ä»¥åŠä½¿ç”¨GMMç®—æ³•çš„å¼‚å¸¸æ£€æµ‹
- en: '[](https://viyaleta.medium.com/?source=post_page-----72951fcf8363--------------------------------)[![Viyaleta
    Apgar](../Images/8d8fd8e4817bc4d1dbeb16a2ec1ae1f1.png)](https://viyaleta.medium.com/?source=post_page-----72951fcf8363--------------------------------)[](https://towardsdatascience.com/?source=post_page-----72951fcf8363--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----72951fcf8363--------------------------------)
    [Viyaleta Apgar](https://viyaleta.medium.com/?source=post_page-----72951fcf8363--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://viyaleta.medium.com/?source=post_page-----72951fcf8363--------------------------------)[![Viyaleta
    Apgar](../Images/8d8fd8e4817bc4d1dbeb16a2ec1ae1f1.png)](https://viyaleta.medium.com/?source=post_page-----72951fcf8363--------------------------------)[](https://towardsdatascience.com/?source=post_page-----72951fcf8363--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----72951fcf8363--------------------------------)
    [Viyaleta Apgar](https://viyaleta.medium.com/?source=post_page-----72951fcf8363--------------------------------)'
- en: Â·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: Â·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fccae8864d5a4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F3-use-cases-for-gaussian-mixture-model-gmm-72951fcf8363&user=Viyaleta+Apgar&userId=ccae8864d5a4&source=post_page-ccae8864d5a4----72951fcf8363---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----72951fcf8363--------------------------------)
    Â·10 min readÂ·Jul 27, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F72951fcf8363&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F3-use-cases-for-gaussian-mixture-model-gmm-72951fcf8363&user=Viyaleta+Apgar&userId=ccae8864d5a4&source=-----72951fcf8363---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[å…³æ³¨](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fccae8864d5a4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F3-use-cases-for-gaussian-mixture-model-gmm-72951fcf8363&user=Viyaleta+Apgar&userId=ccae8864d5a4&source=post_page-ccae8864d5a4----72951fcf8363---------------------post_header-----------)
    å‘å¸ƒäº [Towards Data Science](https://towardsdatascience.com/?source=post_page-----72951fcf8363--------------------------------)
    Â·10åˆ†é’Ÿé˜…è¯»Â·2023å¹´7æœˆ27æ—¥[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F72951fcf8363&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F3-use-cases-for-gaussian-mixture-model-gmm-72951fcf8363&user=Viyaleta+Apgar&userId=ccae8864d5a4&source=-----72951fcf8363---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F72951fcf8363&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F3-use-cases-for-gaussian-mixture-model-gmm-72951fcf8363&source=-----72951fcf8363---------------------bookmark_footer-----------)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F72951fcf8363&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F3-use-cases-for-gaussian-mixture-model-gmm-72951fcf8363&source=-----72951fcf8363---------------------bookmark_footer-----------)'
- en: Gaussian Mixture Model (GMM) is a simple, yet powerful unsupervised classification
    algorithm which builds upon K-means instructions in order to predict the probability
    of classification for each instance. This property of GMM makes it versatile for
    many applications. In this article, I will discuss how GMM can be used in feature
    engineering, unsupervised classification, and anomaly detection.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: é«˜æ–¯æ··åˆæ¨¡å‹ï¼ˆGMMï¼‰æ˜¯ä¸€ç§ç®€å•è€Œå¼ºå¤§çš„æ— ç›‘ç£åˆ†ç±»ç®—æ³•ï¼ŒåŸºäºK-meansç®—æ³•æ¥é¢„æµ‹æ¯ä¸ªå®ä¾‹çš„åˆ†ç±»æ¦‚ç‡ã€‚GMMçš„è¿™ä¸€ç‰¹æ€§ä½¿å…¶åœ¨è®¸å¤šåº”ç”¨ä¸­éƒ½å…·æœ‰å¾ˆå¤§çš„çµæ´»æ€§ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘å°†è®¨è®ºGMMå¦‚ä½•ç”¨äºç‰¹å¾å·¥ç¨‹ã€æ— ç›‘ç£åˆ†ç±»å’Œå¼‚å¸¸æ£€æµ‹ã€‚
- en: '![](../Images/668cbd936d2f6728d0a0fef57658e670.png)'
  id: totrans-9
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/668cbd936d2f6728d0a0fef57658e670.png)'
- en: What are Gaussian Mixture Models (GMM)?
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: é«˜æ–¯æ··åˆæ¨¡å‹ï¼ˆGMMï¼‰æ˜¯ä»€ä¹ˆï¼Ÿ
- en: Model Description
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ¨¡å‹æè¿°
- en: While the Gaussian distribution of a single or multiple variables of a dataset
    attempts to represent the entire population probabilistically, GMM makes an assumption
    that there exist subpopulations in the dataset and each follows its own normal
    distribution. In an unsupervised fashion, GMM attempts to learn the subpopulations
    within the data and its probabilistic representation of each data point [1]. This
    property of GMM allows us to use the model to find points that have low probability
    of belonging to any subpopulation and, therefore, categorize such points as outliers.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: å°½ç®¡å•ä¸€æˆ–å¤šä¸ªå˜é‡çš„æ•°æ®é›†çš„é«˜æ–¯åˆ†å¸ƒè¯•å›¾ä»¥æ¦‚ç‡æ–¹å¼è¡¨ç¤ºæ•´ä¸ªæ•°æ®é›†ï¼ŒGMMå‡è®¾æ•°æ®é›†ä¸­å­˜åœ¨å­ç¾¤ä½“ï¼Œå¹¶ä¸”æ¯ä¸ªå­ç¾¤ä½“éµå¾ªå…¶è‡ªå·±çš„æ­£æ€åˆ†å¸ƒã€‚ä»¥æ— ç›‘ç£çš„æ–¹å¼ï¼ŒGMMè¯•å›¾å­¦ä¹ æ•°æ®ä¸­çš„å­ç¾¤ä½“åŠå…¶å¯¹æ¯ä¸ªæ•°æ®ç‚¹çš„æ¦‚ç‡è¡¨ç¤º[1]ã€‚GMMçš„è¿™ä¸€ç‰¹æ€§ä½¿æˆ‘ä»¬èƒ½å¤Ÿä½¿ç”¨æ¨¡å‹æ‰¾åˆ°å±äºä»»ä½•å­ç¾¤ä½“çš„æ¦‚ç‡è¾ƒä½çš„ç‚¹ï¼Œä»è€Œå°†è¿™äº›ç‚¹åˆ†ç±»ä¸ºå¼‚å¸¸å€¼ã€‚
- en: 'GMM essentially extends the multivariate Gaussian distribution to fit the subpopulation
    case by utilizing components to represent these subpopulations and alters the
    multivariate probability distribution function to fit the components. As a gentle
    reminder, the probability density function of the multivariate Gaussian looks
    like this:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: GMMæœ¬è´¨ä¸Šæ˜¯é€šè¿‡åˆ©ç”¨ç»„ä»¶æ¥è¡¨ç¤ºè¿™äº›å­ç¾¤ä½“ï¼Œå¹¶ä¿®æ”¹å¤šå˜é‡æ¦‚ç‡åˆ†å¸ƒå‡½æ•°ä»¥é€‚åº”ç»„ä»¶ï¼Œä»è€Œæ‰©å±•äº†å¤šå˜é‡é«˜æ–¯åˆ†å¸ƒä»¥é€‚åº”å­ç¾¤ä½“æƒ…å†µã€‚æ¸©é¦¨æé†’ï¼Œå¤šå˜é‡é«˜æ–¯åˆ†å¸ƒçš„æ¦‚ç‡å¯†åº¦å‡½æ•°å¦‚ä¸‹ï¼š
- en: '![](../Images/4cd3b3619fa973e582fa30b30c9a6578.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4cd3b3619fa973e582fa30b30c9a6578.png)'
- en: Multivariate Gaussian distribution probability density function
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: å¤šå˜é‡é«˜æ–¯åˆ†å¸ƒçš„æ¦‚ç‡å¯†åº¦å‡½æ•°
- en: 'In GMM, the probability of each instance is modified to be the sum of probabilities
    across all components and component weights are parameterized as ğœ™. GMM requires
    that the sum of all components weights is 1 so it can treat each component as
    a ratio of the whole. GMM also incorporates feature means and variances for each
    component. The model looks like this:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨GMMä¸­ï¼Œæ¯ä¸ªå®ä¾‹çš„æ¦‚ç‡è¢«ä¿®æ”¹ä¸ºæ‰€æœ‰ç»„ä»¶çš„æ¦‚ç‡å’Œï¼Œç»„ä»¶æƒé‡è¢«å‚æ•°åŒ–ä¸ºğœ™ã€‚GMMè¦æ±‚æ‰€æœ‰ç»„ä»¶æƒé‡çš„æ€»å’Œä¸º1ï¼Œä»¥ä¾¿å°†æ¯ä¸ªç»„ä»¶è§†ä¸ºæ•´ä½“çš„ä¸€ä¸ªæ¯”ç‡ã€‚GMMè¿˜ç»“åˆäº†æ¯ä¸ªç»„ä»¶çš„ç‰¹å¾å‡å€¼å’Œæ–¹å·®ã€‚æ¨¡å‹å¦‚ä¸‹ï¼š
- en: '![](../Images/dfcb4ede0d9a1f6b290d56d28e38f85e.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/dfcb4ede0d9a1f6b290d56d28e38f85e.png)'
- en: GMM model formulation
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: GMMæ¨¡å‹çš„å…¬å¼
- en: Notice the parallels between multivariate distribution and GMM. In essence,
    the GMM algorithm finds the correct weight for each component is represented as
    a multivariate Gaussian distribution. In [his post](/gaussian-mixture-models-explained-6986aaf5a95),
    [Oscar Contreras Carrasco](https://medium.com/u/91a848e356c8?source=post_page-----72951fcf8363--------------------------------)
    does a fantastic derivation of GMM [2].
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: æ³¨æ„å¤šå˜é‡åˆ†å¸ƒä¸GMMä¹‹é—´çš„ç›¸ä¼¼æ€§ã€‚å®è´¨ä¸Šï¼ŒGMMç®—æ³•ä¸ºæ¯ä¸ªç»„ä»¶æ‰¾åˆ°æ­£ç¡®çš„æƒé‡ï¼Œè¿™äº›ç»„ä»¶è¢«è¡¨ç¤ºä¸ºå¤šå˜é‡é«˜æ–¯åˆ†å¸ƒã€‚åœ¨[ä»–çš„æ–‡ç« ](/gaussian-mixture-models-explained-6986aaf5a95)ä¸­ï¼Œ[Oscar
    Contreras Carrasco](https://medium.com/u/91a848e356c8?source=post_page-----72951fcf8363--------------------------------)å¯¹GMMåšäº†ç²¾å½©çš„æ¨å¯¼[2]ã€‚
- en: The parameters of the model can be initiated randomly or by using a specific
    strategy and the component weights ğœ™ of the model are determined using repeated
    Expectation Maximization (EM) steps [1].
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: æ¨¡å‹çš„å‚æ•°å¯ä»¥é€šè¿‡éšæœºåˆå§‹åŒ–æˆ–ä½¿ç”¨ç‰¹å®šç­–ç•¥è¿›è¡Œåˆå§‹åŒ–ï¼Œæ¨¡å‹çš„ç»„ä»¶æƒé‡ğœ™é€šè¿‡é‡å¤çš„æœŸæœ›æœ€å¤§åŒ–ï¼ˆEMï¼‰æ­¥éª¤æ¥ç¡®å®š[1]ã€‚
- en: Model Algorithm
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ¨¡å‹ç®—æ³•
- en: 'The first part of implementation of GMM is **initialization of the components**.
    GMM implementation consists of initialization step followed by iterative **Expectation
    Maximization** (EM) process which repeats until convergence:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: GMMçš„å®æ–½çš„ç¬¬ä¸€éƒ¨åˆ†æ˜¯**ç»„ä»¶çš„åˆå§‹åŒ–**ã€‚GMMçš„å®æ–½åŒ…æ‹¬åˆå§‹åŒ–æ­¥éª¤ï¼Œéšåæ˜¯è¿­ä»£çš„**æœŸæœ›æœ€å¤§åŒ–**ï¼ˆEMï¼‰è¿‡ç¨‹ï¼Œç›´åˆ°æ”¶æ•›ï¼š
- en: '**Step 1:** In the ***initialization******step***, model parameters are initialized:
    *K* values from the dataset are randomly assigned as component means; component
    variances are calculated based on the randomly assigned means; all component weights
    are assigned a value of 1/*K.*'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '**ç¬¬1æ­¥ï¼š** åœ¨***åˆå§‹åŒ–***æ­¥éª¤ä¸­ï¼Œæ¨¡å‹å‚æ•°è¢«åˆå§‹åŒ–ï¼š*K*å€¼ä»æ•°æ®é›†ä¸­éšæœºåˆ†é…ä¸ºç»„ä»¶å‡å€¼ï¼›ç»„ä»¶æ–¹å·®æ ¹æ®éšæœºåˆ†é…çš„å‡å€¼è®¡ç®—ï¼›æ‰€æœ‰ç»„ä»¶æƒé‡è¢«èµ‹å€¼ä¸º1/*K*ã€‚'
- en: '**Step 2:** In the ***expectation******step***, we calculate the probability
    that each data point is generated by each component. The expectation for each
    datapoint-component pair is the weight of this specific component times the probability
    that our datapoint belongs to this component (given the component mean and variance)
    as a fraction of all other component probabilities, parameterized with their respective
    component weights. Essentially, expectation attempts to find the likelihood that
    each point belongs to each component and will use this value to slowly adjust
    model parameters until convergence.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '**æ­¥éª¤ 2ï¼š** åœ¨***æœŸæœ›æ­¥éª¤***ä¸­ï¼Œæˆ‘ä»¬è®¡ç®—æ¯ä¸ªæ•°æ®ç‚¹ç”±æ¯ä¸ªç»„ä»¶ç”Ÿæˆçš„æ¦‚ç‡ã€‚æ¯ä¸ªæ•°æ®ç‚¹-ç»„ä»¶å¯¹çš„æœŸæœ›æ˜¯è¯¥ç‰¹å®šç»„ä»¶çš„æƒé‡ä¹˜ä»¥æˆ‘ä»¬æ•°æ®ç‚¹å±äºè¯¥ç»„ä»¶çš„æ¦‚ç‡ï¼ˆç»™å®šç»„ä»¶å‡å€¼å’Œæ–¹å·®ï¼‰ï¼Œä½œä¸ºæ‰€æœ‰å…¶ä»–ç»„ä»¶æ¦‚ç‡çš„ä¸€ä¸ªåˆ†æ•°ï¼Œå‚æ•°åŒ–ä¸ºå„è‡ªçš„ç»„ä»¶æƒé‡ã€‚åŸºæœ¬ä¸Šï¼ŒæœŸæœ›æ­¥éª¤å°è¯•æ‰¾å‡ºæ¯ä¸ªç‚¹å±äºæ¯ä¸ªç»„ä»¶çš„å¯èƒ½æ€§ï¼Œå¹¶åˆ©ç”¨è¿™ä¸ªå€¼æ¥é€æ­¥è°ƒæ•´æ¨¡å‹å‚æ•°ç›´åˆ°æ”¶æ•›ã€‚'
- en: '![](../Images/c1305a6cae954acf60a4a26af782f866.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c1305a6cae954acf60a4a26af782f866.png)'
- en: Formula for the expectation step
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: æœŸæœ›æ­¥éª¤çš„å…¬å¼
- en: '**Step 3:** In the ***maximization step***, we reset the component weights
    and means and recalculate the variances based on the Î³ value from expectation
    step. The new component weights are set as sum of expectation values across all
    datapoints for this component. The new mean value for each component is the average
    of all data points, weighted by the expectation value.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '**æ­¥éª¤ 3ï¼š** åœ¨***æœ€å¤§åŒ–æ­¥éª¤***ä¸­ï¼Œæˆ‘ä»¬é‡ç½®ç»„ä»¶çš„æƒé‡å’Œå‡å€¼ï¼Œå¹¶æ ¹æ®æœŸæœ›æ­¥éª¤ä¸­çš„Î³å€¼é‡æ–°è®¡ç®—æ–¹å·®ã€‚æ–°çš„ç»„ä»¶æƒé‡è®¾ç½®ä¸ºè¯¥ç»„ä»¶æ‰€æœ‰æ•°æ®ç‚¹æœŸæœ›å€¼çš„æ€»å’Œã€‚æ¯ä¸ªç»„ä»¶çš„æ–°å‡å€¼æ˜¯æ‰€æœ‰æ•°æ®ç‚¹çš„å¹³å‡å€¼ï¼ŒåŠ æƒç”±æœŸæœ›å€¼å†³å®šã€‚'
- en: '![](../Images/fac778a796f4fa457553840394b4112f.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fac778a796f4fa457553840394b4112f.png)'
- en: Formulas for the maximization step
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€å¤§åŒ–æ­¥éª¤çš„å…¬å¼
- en: Just like in a k-means algorithm, it is appropriate to guess the number of components
    *K*, if the number of components is not previously available.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: å°±åƒåœ¨k-meansç®—æ³•ä¸­ä¸€æ ·ï¼Œå¦‚æœç»„ä»¶çš„æ•°é‡äº‹å…ˆä¸å¯ç”¨ï¼Œé‚£ä¹ˆçŒœæµ‹ç»„ä»¶çš„æ•°é‡*K*æ˜¯åˆé€‚çš„ã€‚
- en: Below is a visual example of GMM convergence. Here, GMM demonstrates convergence
    on a two-dimensional dataset with two clusters. The algorithm behaves similarly
    to k-means but differs in that it estimates the probability density (as opposed
    to pure categorization of sample data points).
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸‹é¢æ˜¯GMMæ”¶æ•›çš„è§†è§‰ç¤ºä¾‹ã€‚åœ¨è¿™é‡Œï¼ŒGMMå±•ç¤ºäº†åœ¨å…·æœ‰ä¸¤ä¸ªç°‡çš„äºŒç»´æ•°æ®é›†ä¸Šçš„æ”¶æ•›ã€‚è¯¥ç®—æ³•çš„è¡Œä¸ºç±»ä¼¼äºk-meansï¼Œä½†ä¸åŒä¹‹å¤„åœ¨äºå®ƒä¼°è®¡æ¦‚ç‡å¯†åº¦ï¼ˆè€Œä¸æ˜¯çº¯ç²¹å¯¹æ ·æœ¬æ•°æ®ç‚¹çš„åˆ†ç±»ï¼‰ã€‚
- en: '![](../Images/f00ea4a65606926a97d1de7570082347.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f00ea4a65606926a97d1de7570082347.png)'
- en: EM Clustering of Old Faithful data [[3](https://commons.wikimedia.org/wiki/File:EM_Clustering_of_Old_Faithful_data.gif)]
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: æ—§å¿ å®æ•°æ®çš„EMèšç±» [[3](https://commons.wikimedia.org/wiki/File:EM_Clustering_of_Old_Faithful_data.gif)]
- en: Letâ€™s see how this algorithm can be applied to our 3 use-cases.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬çœ‹çœ‹è¿™ä¸ªç®—æ³•å¦‚ä½•åº”ç”¨äºæˆ‘ä»¬çš„ä¸‰ä¸ªç”¨ä¾‹ã€‚
- en: GMM for Feature Engineering
  id: totrans-35
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç‰¹å¾å·¥ç¨‹ä¸­çš„GMM
- en: While some machine learning models (like the infamous XGBoost) can learn a variety
    of input feature distributions, others are more strict in their requirements.
    Linear and logistic regression, linear discriminant analysis (LDA), and multivariate
    Gaussian typically expect features to be normally distributed and may not function
    so well if the data is multimodal. There are other analytical and visual reasons
    we may want to deal with multimodality and GMM can help us do just that.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: å°½ç®¡ä¸€äº›æœºå™¨å­¦ä¹ æ¨¡å‹ï¼ˆå¦‚è‡­åæ˜­è‘—çš„XGBoostï¼‰å¯ä»¥å­¦ä¹ å„ç§è¾“å…¥ç‰¹å¾åˆ†å¸ƒï¼Œä½†å…¶ä»–æ¨¡å‹å¯¹å…¶è¦æ±‚æ›´ä¸ºä¸¥æ ¼ã€‚çº¿æ€§å›å½’ã€é€»è¾‘å›å½’ã€çº¿æ€§åˆ¤åˆ«åˆ†æï¼ˆLDAï¼‰å’Œå¤šå˜é‡é«˜æ–¯é€šå¸¸æœŸæœ›ç‰¹å¾å‘ˆæ­£æ€åˆ†å¸ƒï¼Œå¦‚æœæ•°æ®æ˜¯å¤šæ¨¡æ€çš„ï¼Œå¯èƒ½æ•ˆæœä¸ä½³ã€‚æˆ‘ä»¬å¯èƒ½è¿˜æœ‰å…¶ä»–åˆ†æå’Œè§†è§‰ä¸Šçš„åŸå› éœ€è¦å¤„ç†å¤šæ¨¡æ€ï¼Œè€ŒGMMå¯ä»¥å¸®åŠ©æˆ‘ä»¬å®ç°è¿™ä¸€ç‚¹ã€‚
- en: Letâ€™s take a look at an example of bimodal feature from a fictional bookstore
    dataset. I pulled this data from a Kaggle repository ([found here](https://www.kaggle.com/datasets/sbonelondhlazi/bookstore-dataset))
    which contains data scraped from [books.toscrape.com](http://books.toscrape.com/)
    [7]. This dataset features typical bookstore information like book title, genre,
    price, and rating. It also contains book quantity, which determines the total
    number of books the fictional book store has in stock. As luck would have it,
    book has a bimodal distribution. Letâ€™s see if we can use GMM as a feature engineering
    technique in order to create two separate features from the book quantity data.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬æ¥çœ‹ä¸€ä¸ªè™šæ„ä¹¦åº—æ•°æ®é›†ä¸­åŒå³°ç‰¹å¾çš„ç¤ºä¾‹ã€‚æˆ‘ä» Kaggle æ•°æ®åº“ä¸­æå–äº†è¿™äº›æ•°æ®ï¼ˆ[é“¾æ¥åœ¨æ­¤](https://www.kaggle.com/datasets/sbonelondhlazi/bookstore-dataset)ï¼‰ï¼Œå…¶ä¸­åŒ…å«ä»
    [books.toscrape.com](http://books.toscrape.com/) [7] çˆ¬å–çš„æ•°æ®ã€‚è¯¥æ•°æ®é›†åŒ…å«å…¸å‹çš„ä¹¦åº—ä¿¡æ¯ï¼Œå¦‚ä¹¦åã€ç±»åˆ«ã€ä»·æ ¼å’Œè¯„åˆ†ã€‚å®ƒè¿˜åŒ…å«ä¹¦ç±æ•°é‡ï¼Œè¿™å†³å®šäº†è™šæ„ä¹¦åº—åº“å­˜ä¸­çš„ä¹¦ç±æ€»æ•°ã€‚å·§åˆçš„æ˜¯ï¼Œè¿™äº›ä¹¦ç±å…·æœ‰åŒå³°åˆ†å¸ƒã€‚æˆ‘ä»¬æ¥çœ‹çœ‹æ˜¯å¦å¯ä»¥ä½¿ç”¨
    GMM ä½œä¸ºç‰¹å¾å·¥ç¨‹æŠ€æœ¯ï¼Œä»ä¹¦ç±æ•°é‡æ•°æ®ä¸­åˆ›å»ºä¸¤ä¸ªç‹¬ç«‹çš„ç‰¹å¾ã€‚
- en: 'I used the following code to implement this task in Python:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä½¿ç”¨ä»¥ä¸‹ä»£ç åœ¨ Python ä¸­å®ç°äº†è¿™ä¸€ä»»åŠ¡ï¼š
- en: Letâ€™s take a look at the results. The left hand-side chart shows the original
    distribution of book quantity. The right hand-side chart shows the distribution
    of each predicted component, after the GMM transformation. Notice that the shape
    of the complete distribution is exactly the same but the two components split
    the original distribution at a specific point, creating two (mostly) normal histograms.
    If I was not satisfied with the point which segments two components, I can use
    the GMM predicted probability to adjust where component 1 ends and component 2
    begins.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬æŸ¥çœ‹ç»“æœã€‚å·¦ä¾§çš„å›¾è¡¨æ˜¾ç¤ºäº†ä¹¦ç±æ•°é‡çš„åŸå§‹åˆ†å¸ƒã€‚å³ä¾§çš„å›¾è¡¨æ˜¾ç¤ºäº†æ¯ä¸ªé¢„æµ‹ç»„ä»¶çš„åˆ†å¸ƒï¼Œåœ¨ GMM è½¬æ¢ä¹‹åã€‚è¯·æ³¨æ„ï¼Œå®Œæ•´åˆ†å¸ƒçš„å½¢çŠ¶å®Œå…¨ç›¸åŒï¼Œä½†ä¸¤ä¸ªç»„ä»¶åœ¨ç‰¹å®šç‚¹å¤„æ‹†åˆ†äº†åŸå§‹åˆ†å¸ƒï¼Œåˆ›å»ºäº†ä¸¤ä¸ªï¼ˆå¤§å¤šï¼‰æ­£å¸¸çš„ç›´æ–¹å›¾ã€‚å¦‚æœæˆ‘å¯¹åˆ†å‰²ä¸¤ä¸ªç»„ä»¶çš„ç‚¹ä¸æ»¡æ„ï¼Œæˆ‘å¯ä»¥ä½¿ç”¨
    GMM é¢„æµ‹çš„æ¦‚ç‡æ¥è°ƒæ•´ç»„ä»¶ 1 ç»“æŸå’Œç»„ä»¶ 2 å¼€å§‹çš„ä½ç½®ã€‚
- en: '![](../Images/0edf34e4c22611332b3030d577ca1c7c.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0edf34e4c22611332b3030d577ca1c7c.png)'
- en: Distribution of Quantity before and after GMM [[9](https://github.com/viyaleta/Medium-Code-Examples/blob/main/GMM/3%20Use-Cases%20for%20GMM.ipynb)]
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: æ•°é‡åœ¨ GMM ä¹‹å‰å’Œä¹‹åçš„åˆ†å¸ƒ [[9](https://github.com/viyaleta/Medium-Code-Examples/blob/main/GMM/3%20Use-Cases%20for%20GMM.ipynb)]
- en: GMM for Unsupervised Classification
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: GMM ç”¨äºæ— ç›‘ç£åˆ†ç±»
- en: Another use-case for GMM is unsupervised classification. In this sense, GMM
    works similarly to K-means algorithm but allows probabilistic determination of
    class belonging (unlike the K-means, where the output is a binary metric). This
    can be especially beneficial to use-cases that require custom thresholds for categorization
    or simply call for a probabilistic output.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: GMM çš„å¦ä¸€ä¸ªåº”ç”¨åœºæ™¯æ˜¯æ— ç›‘ç£åˆ†ç±»ã€‚åœ¨è¿™æ–¹é¢ï¼ŒGMM çš„å·¥ä½œæ–¹å¼ç±»ä¼¼äº K-means ç®—æ³•ï¼Œä½†å…è®¸å¯¹ç±»åˆ«å½’å±è¿›è¡Œæ¦‚ç‡æ€§åˆ¤æ–­ï¼ˆä¸åŒäº K-meansï¼Œå…¶ä¸­è¾“å‡ºæ˜¯ä¸€ä¸ªäºŒå…ƒåº¦é‡ï¼‰ã€‚è¿™å¯¹äºéœ€è¦è‡ªå®šä¹‰åˆ†ç±»é˜ˆå€¼æˆ–ç®€å•è¦æ±‚æ¦‚ç‡è¾“å‡ºçš„ç”¨ä¾‹ç‰¹åˆ«æœ‰ç›Šã€‚
- en: 'For this example, I downloaded the penguins dataset ([available on Kaggle](https://www.kaggle.com/datasets/parulpandey/palmer-archipelago-antarctica-penguin-data))
    and selected two features for the purpose of visual demonstration: penguin culmen
    length and depth (culmen is the top ridge of penguinâ€™s bill) [[8](https://www.kaggle.com/datasets/parulpandey/palmer-archipelago-antarctica-penguin-data)].
    I removed null data points and created a scatterplot to depict the data.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºè¿™ä¸ªä¾‹å­ï¼Œæˆ‘ä¸‹è½½äº†ä¼é¹…æ•°æ®é›†ï¼ˆ[åœ¨ Kaggle ä¸Šæä¾›](https://www.kaggle.com/datasets/parulpandey/palmer-archipelago-antarctica-penguin-data)ï¼‰ï¼Œå¹¶é€‰æ‹©äº†ä¸¤ä¸ªç‰¹å¾ä»¥è¿›è¡Œå¯è§†åŒ–æ¼”ç¤ºï¼šä¼é¹…çš„å–™é•¿åº¦å’Œæ·±åº¦ï¼ˆå–™æ˜¯ä¼é¹…å–™çš„é¡¶éƒ¨è„Šï¼‰[[8](https://www.kaggle.com/datasets/parulpandey/palmer-archipelago-antarctica-penguin-data)]ã€‚æˆ‘åˆ é™¤äº†ç©ºå€¼æ•°æ®ç‚¹ï¼Œå¹¶åˆ›å»ºäº†æ•£ç‚¹å›¾ä»¥æç»˜æ•°æ®ã€‚
- en: When we take a look at the scatterplot, 3 potential groups stand out. If we
    color the ground truth categories, we see that, in fact, the three species of
    penguin do align with the 3 groups of data points. This example is quite rudimentary
    since in real world, we usually work with multidimensional data and there is no
    easy way to determine how many subpopulations exist in our dataset. Nonetheless,
    letâ€™s see how GMM can help us segment our data.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: å½“æˆ‘ä»¬æŸ¥çœ‹æ•£ç‚¹å›¾æ—¶ï¼Œ3 ä¸ªæ½œåœ¨çš„ç»„è„±é¢–è€Œå‡ºã€‚å¦‚æœæˆ‘ä»¬ä¸ºå®é™…ç±»åˆ«ä¸Šè‰²ï¼Œæˆ‘ä»¬ä¼šå‘ç°ï¼Œäº‹å®ä¸Šï¼Œä¸‰ç§ä¼é¹…ä¸ä¸‰ç»„æ•°æ®ç‚¹å¯¹é½ã€‚è¿™ä¸ªä¾‹å­éå¸¸åŸºç¡€ï¼Œå› ä¸ºåœ¨ç°å®ä¸–ç•Œä¸­ï¼Œæˆ‘ä»¬é€šå¸¸å¤„ç†çš„æ˜¯å¤šç»´æ•°æ®ï¼Œä¸”æ²¡æœ‰ç®€å•çš„æ–¹æ³•æ¥ç¡®å®šæ•°æ®é›†ä¸­å­˜åœ¨å¤šå°‘ä¸ªå­ç¾¤ä½“ã€‚å°½ç®¡å¦‚æ­¤ï¼Œæˆ‘ä»¬è¿˜æ˜¯æ¥çœ‹çœ‹
    GMM å¦‚ä½•å¸®åŠ©æˆ‘ä»¬å¯¹æ•°æ®è¿›è¡Œåˆ†å‰²ã€‚
- en: '![](../Images/4243f106ece15ba8650d32dda59a0820.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4243f106ece15ba8650d32dda59a0820.png)'
- en: Scatterplot depicting penguin culmen length vs. depth [[9](https://github.com/viyaleta/Medium-Code-Examples/blob/main/GMM/3%20Use-Cases%20for%20GMM.ipynb)]
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: æè¿°ä¼é¹…å–™é•¿ä¸æ·±åº¦çš„æ•£ç‚¹å›¾ [[9](https://github.com/viyaleta/Medium-Code-Examples/blob/main/GMM/3%20Use-Cases%20for%20GMM.ipynb)]
- en: In the Python code snippet below, I downloaded the dataset, removed null values,
    selected the two features of interest, and fitted the GMM model to them. `sklearn`
    offers two options for predictions â€” predicting the class and predicting the probability
    of class belonging. Sum of probabilities for each data point is equal to 1 (per
    GMM algorithm constraint).
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä¸‹é¢çš„Pythonä»£ç ç‰‡æ®µä¸­ï¼Œæˆ‘ä¸‹è½½äº†æ•°æ®é›†ï¼Œåˆ é™¤äº†ç©ºå€¼ï¼Œé€‰æ‹©äº†ä¸¤ä¸ªæ„Ÿå…´è¶£çš„ç‰¹å¾ï¼Œå¹¶å°†GMMæ¨¡å‹æ‹Ÿåˆåˆ°å®ƒä»¬ä¸Šé¢ã€‚`sklearn`æä¾›äº†ä¸¤ç§é¢„æµ‹é€‰é¡¹â€”â€”é¢„æµ‹ç±»åˆ«å’Œé¢„æµ‹ç±»åˆ«å½’å±çš„æ¦‚ç‡ã€‚æ¯ä¸ªæ•°æ®ç‚¹çš„æ¦‚ç‡æ€»å’Œç­‰äº1ï¼ˆæ ¹æ®GMMç®—æ³•çº¦æŸï¼‰ã€‚
- en: Letâ€™s take a look at the probability of each data point belonging to each component.
    The three scatterplots below show probabilities of instances per component. Points
    with higher transparency have lower probabilities and points depicted in brighter
    color have higher probability. In the chart below, we can see that GMM extrapolated
    greater uncertainty for points located between different components, as expected.
    We can use the `gmm.predict_proba()` function to retain control of class attribution.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬çœ‹çœ‹æ¯ä¸ªæ•°æ®ç‚¹å±äºæ¯ä¸ªç»„ä»¶çš„æ¦‚ç‡ã€‚ä¸‹é¢çš„ä¸‰ä¸ªæ•£ç‚¹å›¾æ˜¾ç¤ºäº†æ¯ä¸ªç»„ä»¶å®ä¾‹çš„æ¦‚ç‡ã€‚é€æ˜åº¦è¾ƒé«˜çš„ç‚¹å…·æœ‰è¾ƒä½çš„æ¦‚ç‡ï¼Œè€Œé¢œè‰²è¾ƒäº®çš„ç‚¹å…·æœ‰è¾ƒé«˜çš„æ¦‚ç‡ã€‚åœ¨ä¸‹å›¾ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼ŒGMMå¯¹ä½äºä¸åŒç»„ä»¶ä¹‹é—´çš„ç‚¹é¢„æµ‹äº†æ›´å¤§çš„ä¸ç¡®å®šæ€§ï¼Œè¿™åœ¨é¢„æœŸä¹‹ä¸­ã€‚æˆ‘ä»¬å¯ä»¥ä½¿ç”¨`gmm.predict_proba()`å‡½æ•°æ¥æ§åˆ¶ç±»åˆ«å½’å±ã€‚
- en: '![](../Images/b4c0098171b267e3e92e23a7a4217bb2.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b4c0098171b267e3e92e23a7a4217bb2.png)'
- en: Scatterplots depicting probability of class belonging for each predicted class
    [[9](https://github.com/viyaleta/Medium-Code-Examples/blob/main/GMM/3%20Use-Cases%20for%20GMM.ipynb)]
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: æè¿°æ¯ä¸ªé¢„æµ‹ç±»åˆ«çš„ç±»å½’å±æ¦‚ç‡çš„æ•£ç‚¹å›¾ [[9](https://github.com/viyaleta/Medium-Code-Examples/blob/main/GMM/3%20Use-Cases%20for%20GMM.ipynb)]
- en: GMM for Anomaly Detection
  id: totrans-52
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç”¨äºå¼‚å¸¸æ£€æµ‹çš„GMM
- en: In my [previous story](/the-basics-of-anomaly-detection-65aff59949b7), I shared
    the basics of anomaly detection and an application of a statistical approach to
    detect anomalies [6]. Namely, I used the mutlivariate Gaussian distribution to
    identify data points that have low probability of following the Normal distribution.
    The problem with this approach, however, is that our data is often more complex.
    Gaussian Mixture Model (GMM) attempts to solve the multimodality problem and is
    useful in cases where features form subpopulations of normally distributed feature
    relationships.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æˆ‘çš„[ä¹‹å‰çš„æ•…äº‹](/the-basics-of-anomaly-detection-65aff59949b7)ä¸­ï¼Œæˆ‘åˆ†äº«äº†å¼‚å¸¸æ£€æµ‹çš„åŸºç¡€çŸ¥è¯†ä»¥åŠç»Ÿè®¡æ–¹æ³•åœ¨æ£€æµ‹å¼‚å¸¸ä¸­çš„åº”ç”¨[6]ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œæˆ‘ä½¿ç”¨å¤šå˜é‡é«˜æ–¯åˆ†å¸ƒæ¥è¯†åˆ«ä½æ¦‚ç‡ç¬¦åˆæ­£æ€åˆ†å¸ƒçš„æ•°æ®ç‚¹ã€‚ç„¶è€Œï¼Œè¿™ç§æ–¹æ³•çš„é—®é¢˜æ˜¯æˆ‘ä»¬çš„æ•°æ®å¾€å¾€æ›´å¤æ‚ã€‚é«˜æ–¯æ··åˆæ¨¡å‹ï¼ˆGMMï¼‰å°è¯•è§£å†³å¤šæ¨¡æ€é—®é¢˜ï¼Œå¹¶ä¸”åœ¨ç‰¹å¾å½¢æˆæ­£æ€åˆ†å¸ƒç‰¹å¾å…³ç³»çš„å­ç¾¤ä½“çš„æƒ…å†µä¸‹éå¸¸æœ‰ç”¨ã€‚
- en: For this last example, I will use a wine dataset from the [ODDS library](http://odds.cs.stonybrook.edu/wine-dataset/)
    [10]. This is the same dataset I used in [my post](/the-basics-of-anomaly-detection-65aff59949b7)
    where I applied multivariate Gaussian distribution to detect outliers. Letâ€™s see
    if I can improve on my [previous result](/the-basics-of-anomaly-detection-65aff59949b7),
    where 41 instances where misclassified, 40 of which were falsely identified anomalies.
    The first advantage over the multivariate Gaussian distribution method is that
    we can use all of our features and we are not restricted to only normally distributed
    features. The second advantage is that we can account for data subpopulations.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ä¸ªæœ€åçš„ä¾‹å­ä¸­ï¼Œæˆ‘å°†ä½¿ç”¨æ¥è‡ª[ODDS library](http://odds.cs.stonybrook.edu/wine-dataset/)
    [10]çš„è‘¡è„é…’æ•°æ®é›†ã€‚è¿™æ˜¯æˆ‘åœ¨[æˆ‘çš„å¸–å­](/the-basics-of-anomaly-detection-65aff59949b7)ä¸­ä½¿ç”¨çš„æ•°æ®é›†ï¼Œåœ¨é‚£é‡Œæˆ‘åº”ç”¨äº†å¤šå˜é‡é«˜æ–¯åˆ†å¸ƒæ¥æ£€æµ‹ç¦»ç¾¤ç‚¹ã€‚è®©æˆ‘ä»¬çœ‹çœ‹æˆ‘æ˜¯å¦å¯ä»¥æ”¹è¿›[ä¹‹å‰çš„ç»“æœ](/the-basics-of-anomaly-detection-65aff59949b7)ï¼Œåœ¨é‚£ä¸ªç»“æœä¸­ï¼Œ41ä¸ªå®ä¾‹è¢«é”™è¯¯åˆ†ç±»ï¼Œå…¶ä¸­40ä¸ªè¢«é”™è¯¯è¯†åˆ«ä¸ºå¼‚å¸¸ç‚¹ã€‚ç›¸æ¯”å¤šå˜é‡é«˜æ–¯åˆ†å¸ƒæ–¹æ³•ï¼Œç¬¬ä¸€ä¸ªä¼˜ç‚¹æ˜¯æˆ‘ä»¬å¯ä»¥ä½¿ç”¨æ‰€æœ‰ç‰¹å¾ï¼Œè€Œä¸é™äºä»…æ­£æ€åˆ†å¸ƒçš„ç‰¹å¾ã€‚ç¬¬äºŒä¸ªä¼˜ç‚¹æ˜¯æˆ‘ä»¬å¯ä»¥è€ƒè™‘æ•°æ®å­ç¾¤ä½“ã€‚
- en: 'Perhaps the most prominent advantage of GMM is that this model can help in
    finding two types of anomalous data: anomalies which are outliers of a population
    (e.g. mistake in data entry) and anomalies which form their own group (e.g. credit
    card fraud behavior). The positive instances in the wine dataset are constructed
    as samples of two types of wine while anomalies in the dataset are constructed
    as a subsample of a third type of wine [10]. Therefore, we will likely find that
    our outliers make up their own group in this instance.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: GMMæœ€æ˜¾è‘—çš„ä¼˜åŠ¿å¯èƒ½æ˜¯è¯¥æ¨¡å‹å¯ä»¥å¸®åŠ©å‘ç°ä¸¤ç§ç±»å‹çš„å¼‚å¸¸æ•°æ®ï¼šä¸€ç§æ˜¯äººå£ä¸­çš„ç¦»ç¾¤ç‚¹ï¼ˆä¾‹å¦‚æ•°æ®å½•å…¥é”™è¯¯ï¼‰ï¼Œå¦ä¸€ç§æ˜¯å½¢æˆè‡ªå·±ç»„çš„å¼‚å¸¸ï¼ˆä¾‹å¦‚ä¿¡ç”¨å¡æ¬ºè¯ˆè¡Œä¸ºï¼‰ã€‚åœ¨è‘¡è„é…’æ•°æ®é›†ä¸­ï¼Œæ­£å®ä¾‹è¢«æ„å»ºä¸ºä¸¤ç§ç±»å‹çš„è‘¡è„é…’æ ·æœ¬ï¼Œè€Œæ•°æ®é›†ä¸­çš„å¼‚å¸¸è¢«æ„å»ºä¸ºç¬¬ä¸‰ç§è‘¡è„é…’çš„å­æ ·æœ¬
    [10]ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å¾ˆå¯èƒ½ä¼šå‘ç°æˆ‘ä»¬çš„ç¦»ç¾¤ç‚¹åœ¨è¿™ä¸ªå®ä¾‹ä¸­æ„æˆäº†è‡ªå·±çš„ç»„ã€‚
- en: Since we have 13 features in our dataset, letâ€™s summarize the data into first
    two PCA components and plot them. In the chart below, we will see that the scatterplot
    features one dense area and about two dozen points scattered throughout. No distinct
    subpopulations appear in the data.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: ç”±äºæˆ‘ä»¬çš„æ•°æ®é›†ä¸­æœ‰13ä¸ªç‰¹å¾ï¼Œè®©æˆ‘ä»¬å°†æ•°æ®æ±‡æ€»åˆ°å‰ä¸¤ä¸ªPCAç»„ä»¶ä¸­å¹¶ç»˜åˆ¶å®ƒä»¬ã€‚åœ¨ä¸‹é¢çš„å›¾è¡¨ä¸­ï¼Œæˆ‘ä»¬å°†çœ‹åˆ°æ•£ç‚¹å›¾åŒ…å«ä¸€ä¸ªå¯†é›†åŒºåŸŸå’Œå¤§çº¦äºŒåä¸ªåˆ†æ•£çš„ç‚¹ã€‚æ•°æ®ä¸­æ²¡æœ‰æ˜æ˜¾çš„å­ç¾¤ä½“ã€‚
- en: '![](../Images/b05535bb4405c809eb09efc549e88069.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b05535bb4405c809eb09efc549e88069.png)'
- en: Scatterplot of first 2 PCA components, showing ground truth anomalies [[9](https://github.com/viyaleta/Medium-Code-Examples/blob/main/GMM/3%20Use-Cases%20for%20GMM.ipynb)]
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: å‰ä¸¤ä¸ªPCAç»„ä»¶çš„æ•£ç‚¹å›¾ï¼Œæ˜¾ç¤ºçœŸå®çš„å¼‚å¸¸æƒ…å†µ [[9](https://github.com/viyaleta/Medium-Code-Examples/blob/main/GMM/3%20Use-Cases%20for%20GMM.ipynb)]
- en: The code snippet below fits the GMM model with 2 components and standard input
    fields from `sklearn` library to our dataset. Hopefully, one component will identify
    positive instances while the other component identifies anomalies.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸‹é¢çš„ä»£ç ç‰‡æ®µä½¿ç”¨`sklearn`åº“ä¸­çš„2ä¸ªç»„ä»¶å’Œæ ‡å‡†è¾“å…¥å­—æ®µæ¥æ‹ŸåˆGMMæ¨¡å‹åˆ°æˆ‘ä»¬çš„æ•°æ®é›†ã€‚å¸Œæœ›ä¸€ä¸ªç»„ä»¶èƒ½å¤Ÿè¯†åˆ«æ­£å®ä¾‹ï¼Œè€Œå¦ä¸€ä¸ªç»„ä»¶èƒ½å¤Ÿè¯†åˆ«å¼‚å¸¸ã€‚
- en: The predicted components are numbered randomly so I did some background magic
    to relabel the predictions. You can check out my [Jupyter notebook](https://github.com/viyaleta/Medium-Code-Examples/blob/main/GMM/3%20Use-Cases%20for%20GMM.ipynb)
    for more guidance on how to do that.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: é¢„æµ‹ç»„ä»¶çš„ç¼–å·æ˜¯éšæœºçš„ï¼Œå› æ­¤æˆ‘åšäº†ä¸€äº›èƒŒæ™¯å¤„ç†æ¥é‡æ–°æ ‡è®°é¢„æµ‹ã€‚ä½ å¯ä»¥æŸ¥çœ‹æˆ‘çš„ [Jupyter notebook](https://github.com/viyaleta/Medium-Code-Examples/blob/main/GMM/3%20Use-Cases%20for%20GMM.ipynb)
    ä»¥è·å–æ›´å¤šå…³äºå¦‚ä½•åšåˆ°è¿™ä¸€ç‚¹çš„æŒ‡å¯¼ã€‚
- en: The results look good! We have 0 false negatives and we have 11 falsely identified
    anomalies. If this was the real world, we would only have to manually check some
    15% of data after running this model. This is already a significant improvement
    upon the multivariate Gaussian distribution approach.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: ç»“æœçœ‹èµ·æ¥å¾ˆå¥½ï¼æˆ‘ä»¬æ²¡æœ‰å‡é˜´æ€§ï¼Œå¹¶ä¸”æœ‰11ä¸ªé”™è¯¯è¯†åˆ«çš„å¼‚å¸¸ã€‚å¦‚æœè¿™æ˜¯ç°å®ä¸–ç•Œï¼Œæˆ‘ä»¬åœ¨è¿è¡Œæ­¤æ¨¡å‹ååªéœ€äººå·¥æ£€æŸ¥å¤§çº¦15%çš„æ•°æ®ã€‚è¿™å·²ç»æ˜¯å¯¹å¤šå˜é‡é«˜æ–¯åˆ†å¸ƒæ–¹æ³•çš„æ˜¾è‘—æ”¹è¿›ã€‚
- en: '![](../Images/0a0149bc5a3f51631c72088af7d686eb.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0a0149bc5a3f51631c72088af7d686eb.png)'
- en: Confusion matrix of GMM results [[9](https://github.com/viyaleta/Medium-Code-Examples/blob/main/GMM/3%20Use-Cases%20for%20GMM.ipynb)]
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: GMMç»“æœçš„æ··æ·†çŸ©é˜µ [[9](https://github.com/viyaleta/Medium-Code-Examples/blob/main/GMM/3%20Use-Cases%20for%20GMM.ipynb)]
- en: If we plot our results, we can see that the falsely identified anomalies happen
    to be closer to the most dense area. Again, we are using the first two components
    of PCA for visualization purposes only and GMM is doing a lot more to categorize
    our data points.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæˆ‘ä»¬ç»˜åˆ¶ç»“æœï¼Œå¯ä»¥çœ‹åˆ°è¢«é”™è¯¯è¯†åˆ«çš„å¼‚å¸¸æƒ…å†µå®é™…ä¸Šæ›´æ¥è¿‘æœ€å¯†é›†çš„åŒºåŸŸã€‚å†æ¬¡å¼ºè°ƒï¼Œæˆ‘ä»¬ä»…ä½¿ç”¨å‰ä¸¤ä¸ªPCAç»„ä»¶è¿›è¡Œå¯è§†åŒ–ï¼ŒGMMåœ¨å¯¹æ•°æ®ç‚¹è¿›è¡Œåˆ†ç±»æ—¶åšäº†æ›´å¤šå·¥ä½œã€‚
- en: '![](../Images/706cf41f59a0847713126bce99a11db9.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/706cf41f59a0847713126bce99a11db9.png)'
- en: Scatterplot of first 2 PCA components, showing predicted anomalies [[9](https://github.com/viyaleta/Medium-Code-Examples/blob/main/GMM/3%20Use-Cases%20for%20GMM.ipynb)]
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: å‰ä¸¤ä¸ªPCAç»„ä»¶çš„æ•£ç‚¹å›¾ï¼Œæ˜¾ç¤ºé¢„æµ‹çš„å¼‚å¸¸æƒ…å†µ [[9](https://github.com/viyaleta/Medium-Code-Examples/blob/main/GMM/3%20Use-Cases%20for%20GMM.ipynb)]
- en: We can probably improve upon this result by analyzing our predicted anomaly
    probabilities. Below, I have charted those probabilities for falsely identified
    anomaly instances and true anomaly instances (there were no predicted false negatives).
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯èƒ½é€šè¿‡åˆ†æé¢„æµ‹çš„å¼‚å¸¸æ¦‚ç‡æ¥æ”¹è¿›è¿™ä¸ªç»“æœã€‚ä¸‹é¢ï¼Œæˆ‘å·²ç»ä¸ºé”™è¯¯è¯†åˆ«çš„å¼‚å¸¸å®ä¾‹å’ŒçœŸå®å¼‚å¸¸å®ä¾‹ç»˜åˆ¶äº†è¿™äº›æ¦‚ç‡ï¼ˆæ²¡æœ‰é¢„æµ‹çš„å‡é˜´æ€§ï¼‰ã€‚
- en: '![](../Images/04a51ec35f2cbfcf0c692cf1e5b2ad2f.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/04a51ec35f2cbfcf0c692cf1e5b2ad2f.png)'
- en: Bar chart showing probabilities of anomaly class belonging among falsely identified
    anomalies and true anomalies [[9](https://github.com/viyaleta/Medium-Code-Examples/blob/main/GMM/3%20Use-Cases%20for%20GMM.ipynb)]
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: æ¡å½¢å›¾æ˜¾ç¤ºäº†åœ¨é”™è¯¯è¯†åˆ«çš„å¼‚å¸¸å’ŒçœŸå®å¼‚å¸¸ä¹‹é—´çš„å¼‚å¸¸ç±»åˆ«å½’å±æ¦‚ç‡ [[9](https://github.com/viyaleta/Medium-Code-Examples/blob/main/GMM/3%20Use-Cases%20for%20GMM.ipynb)]
- en: 'We can see that most of our falsely identified anomaly probabilities are lower
    than 0.9999\. So, instead of using default classification, we can use the predicted
    anomaly probability and set a new threshold. If we set the threshold anomaly detection
    to be greater 0.9999, we will only have 3 falsely identified anomalies. This can
    be done with a single line of code: `components = np.where(proba>0.9999, 1, 0)`
    . In some instances, changing the threshold may add more false negatives to our
    final results. Lucky us, this is not the case.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥çœ‹åˆ°å¤§å¤šæ•°é”™è¯¯è¯†åˆ«çš„å¼‚å¸¸æ¦‚ç‡ä½äº 0.9999ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨é¢„æµ‹çš„å¼‚å¸¸æ¦‚ç‡å¹¶è®¾ç½®æ–°çš„é˜ˆå€¼ï¼Œè€Œä¸æ˜¯ä½¿ç”¨é»˜è®¤åˆ†ç±»ã€‚å¦‚æœæˆ‘ä»¬å°†é˜ˆå€¼è®¾ç½®ä¸ºå¤§äº 0.9999
    çš„å¼‚å¸¸æ£€æµ‹ï¼Œæˆ‘ä»¬å°†åªæœ‰ 3 ä¸ªé”™è¯¯è¯†åˆ«çš„å¼‚å¸¸ã€‚è¿™å¯ä»¥ç”¨ä¸€è¡Œä»£ç å®Œæˆï¼š`components = np.where(proba>0.9999, 1, 0)`ã€‚åœ¨æŸäº›æƒ…å†µä¸‹ï¼Œæ”¹å˜é˜ˆå€¼å¯èƒ½ä¼šå¢åŠ æˆ‘ä»¬æœ€ç»ˆç»“æœä¸­çš„å‡é˜´æ€§ã€‚å¹¸è¿çš„æ˜¯ï¼Œè¿™ç§æƒ…å†µå¹¶ä¸å­˜åœ¨ã€‚
- en: '![](../Images/4747a4b5145bbd2020e95a3fa57ed7dd.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4747a4b5145bbd2020e95a3fa57ed7dd.png)'
- en: Confusion matrix of GMM results, after decreasing increasing probability threshold
    [[9](https://github.com/viyaleta/Medium-Code-Examples/blob/main/GMM/3%20Use-Cases%20for%20GMM.ipynb)]
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: GMM ç»“æœçš„æ··æ·†çŸ©é˜µï¼Œåœ¨é™ä½æé«˜æ¦‚ç‡é˜ˆå€¼ä¹‹å [[9](https://github.com/viyaleta/Medium-Code-Examples/blob/main/GMM/3%20Use-Cases%20for%20GMM.ipynb)]
- en: In the following PCA scatterplot, we can see that our model still predicts 3
    falsely identified anomalies but with the new improvement, we would only need
    to manually check 10% of data instances for presence of anomalies. Given that
    8% of data are actually anomalous, this is great!
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä»¥ä¸‹ PCA æ•£ç‚¹å›¾ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°æˆ‘ä»¬çš„æ¨¡å‹ä»ç„¶é¢„æµ‹äº† 3 ä¸ªé”™è¯¯è¯†åˆ«çš„å¼‚å¸¸ï¼Œä½†é€šè¿‡æ–°çš„æ”¹è¿›ï¼Œæˆ‘ä»¬åªéœ€æ‰‹åŠ¨æ£€æŸ¥ 10% çš„æ•°æ®å®ä¾‹ä»¥æ£€æµ‹å¼‚å¸¸ã€‚è€ƒè™‘åˆ°
    8% çš„æ•°æ®å®é™…ä¸Šæ˜¯å¼‚å¸¸çš„ï¼Œè¿™çœŸæ˜¯å¤ªæ£’äº†ï¼
- en: '![](../Images/cdc939fc37c9e0326251483df54cac8d.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/cdc939fc37c9e0326251483df54cac8d.png)'
- en: Scatterplot of first 2 PCA components, after increasing anomaly probability
    threshold [[9](https://github.com/viyaleta/Medium-Code-Examples/blob/main/GMM/3%20Use-Cases%20for%20GMM.ipynb)]
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: æé«˜å¼‚å¸¸æ¦‚ç‡é˜ˆå€¼åçš„å‰ 2 ä¸ª PCA ç»„ä»¶çš„æ•£ç‚¹å›¾ [[9](https://github.com/viyaleta/Medium-Code-Examples/blob/main/GMM/3%20Use-Cases%20for%20GMM.ipynb)]
- en: The power to predict class probabilities makes GMM is powerful and versatile
    tool in data science. Although it has the same limitations as its K-means cousin,
    GMM is helpful in feature engineering, more flexible unsupervised classification,
    and anomaly detection.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: é¢„æµ‹ç±»åˆ«æ¦‚ç‡çš„èƒ½åŠ›ä½¿å¾— GMM æˆä¸ºæ•°æ®ç§‘å­¦ä¸­ä¸€ä¸ªå¼ºå¤§è€Œå¤šç”¨é€”çš„å·¥å…·ã€‚å°½ç®¡å®ƒæœ‰ä¸ K-means åŒç±»çš„ç›¸åŒé™åˆ¶ï¼ŒGMM åœ¨ç‰¹å¾å·¥ç¨‹ã€æ›´çµæ´»çš„æ— ç›‘ç£åˆ†ç±»å’Œå¼‚å¸¸æ£€æµ‹ä¸­éå¸¸æœ‰ç”¨ã€‚
- en: I had a lot of fun writing this article and I had just as much fun reading it.
    I am always open to feedback and questions so make sure to make use of that comment
    section. If you would prefer to provide direct feedback, you can always [find
    me on LinkedIn](https://www.linkedin.com/in/viyaleta/).
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘åœ¨å†™è¿™ç¯‡æ–‡ç« æ—¶éå¸¸å¼€å¿ƒï¼Œè¯»å®ƒçš„æ—¶å€™ä¹ŸåŒæ ·å¼€å¿ƒã€‚æˆ‘æ€»æ˜¯æ¬¢è¿åé¦ˆå’Œé—®é¢˜ï¼Œæ‰€ä»¥ä¸€å®šè¦åˆ©ç”¨è¯„è®ºåŒºã€‚å¦‚æœä½ å¸Œæœ›ç›´æ¥åé¦ˆï¼Œå¯ä»¥éšæ—¶[åœ¨ LinkedIn ä¸Šæ‰¾åˆ°æˆ‘](https://www.linkedin.com/in/viyaleta/)ã€‚
- en: 'Sources:'
  id: totrans-78
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ¥æºï¼š
- en: '[https://brilliant.org/wiki/gaussian-mixture-model/](https://brilliant.org/wiki/gaussian-mixture-model/)'
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[https://brilliant.org/wiki/gaussian-mixture-model/](https://brilliant.org/wiki/gaussian-mixture-model/)'
- en: '[https://towardsdatascience.com/gaussian-mixture-models-explained-6986aaf5a95](/gaussian-mixture-models-explained-6986aaf5a95)'
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[https://towardsdatascience.com/gaussian-mixture-models-explained-6986aaf5a95](/gaussian-mixture-models-explained-6986aaf5a95)'
- en: '[https://commons.wikimedia.org/wiki/File:EM_Clustering_of_Old_Faithful_data.gif](https://commons.wikimedia.org/wiki/File:EM_Clustering_of_Old_Faithful_data.gif)'
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[https://commons.wikimedia.org/wiki/File:EM_Clustering_of_Old_Faithful_data.gif](https://commons.wikimedia.org/wiki/File:EM_Clustering_of_Old_Faithful_data.gif)'
- en: '[https://towardsdatascience.com/understanding-anomaly-detection-in-python-using-gaussian-mixture-model-e26e5d06094b](/understanding-anomaly-detection-in-python-using-gaussian-mixture-model-e26e5d06094b)'
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[https://towardsdatascience.com/understanding-anomaly-detection-in-python-using-gaussian-mixture-model-e26e5d06094b](/understanding-anomaly-detection-in-python-using-gaussian-mixture-model-e26e5d06094b)'
- en: '[https://scikit-learn.org/stable/modules/generated/sklearn.mixture.GaussianMixture.html#sklearn.mixture.GaussianMixture.fit](https://scikit-learn.org/stable/modules/generated/sklearn.mixture.GaussianMixture.html#sklearn.mixture.GaussianMixture.fit)'
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[https://scikit-learn.org/stable/modules/generated/sklearn.mixture.GaussianMixture.html#sklearn.mixture.GaussianMixture.fit](https://scikit-learn.org/stable/modules/generated/sklearn.mixture.GaussianMixture.html#sklearn.mixture.GaussianMixture.fit)'
- en: '[https://towardsdatascience.com/the-basics-of-anomaly-detection-65aff59949b7](/the-basics-of-anomaly-detection-65aff59949b7)'
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[https://towardsdatascience.com/the-basics-of-anomaly-detection-65aff59949b7](/the-basics-of-anomaly-detection-65aff59949b7)'
- en: '[https://www.kaggle.com/datasets/sbonelondhlazi/bookstore-dataset](https://www.kaggle.com/datasets/sbonelondhlazi/bookstore-dataset)'
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[https://www.kaggle.com/datasets/sbonelondhlazi/bookstore-dataset](https://www.kaggle.com/datasets/sbonelondhlazi/bookstore-dataset)'
- en: '[https://medium.com/r/?url=https%3A%2F%2Fwww.kaggle.com%2Fdatasets%2Fparulpandey%2Fpalmer-archipelago-antarctica-penguin-data](https://www.kaggle.com/datasets/parulpandey/palmer-archipelago-antarctica-penguin-data)'
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[https://medium.com/r/?url=https%3A%2F%2Fwww.kaggle.com%2Fdatasets%2Fparulpandey%2Fpalmer-archipelago-antarctica-penguin-data](https://www.kaggle.com/datasets/parulpandey/palmer-archipelago-antarctica-penguin-data)'
- en: '[https://github.com/viyaleta/Medium-Code-Examples/blob/main/GMM/3%20Use-Cases%20for%20GMM.ipynb](https://github.com/viyaleta/Medium-Code-Examples/blob/main/GMM/3%20Use-Cases%20for%20GMM.ipynb)'
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[https://github.com/viyaleta/Medium-Code-Examples/blob/main/GMM/3%20Use-Cases%20for%20GMM.ipynb](https://github.com/viyaleta/Medium-Code-Examples/blob/main/GMM/3%20Use-Cases%20for%20GMM.ipynb)'
- en: 'Saket Sathe and Charu C. Aggarwal. LODES: Local Density meets Spectral Outlier
    Detection. SIAM Conference on Data Mining, 2016\. [http://odds.cs.stonybrook.edu/wine-dataset/](http://odds.cs.stonybrook.edu/wine-dataset/)'
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Saket Sathe å’Œ Charu C. Aggarwal. LODES: å±€éƒ¨å¯†åº¦ä¸è°±å¼‚å¸¸æ£€æµ‹çš„ç»“åˆã€‚SIAM æ•°æ®æŒ–æ˜ä¼šè®®ï¼Œ2016\. [http://odds.cs.stonybrook.edu/wine-dataset/](http://odds.cs.stonybrook.edu/wine-dataset/)'
