- en: 'Understanding TF-IDF: A Traditional Approach to Feature Extraction in NLP'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 了解TF-IDF：NLP中的一种传统特征提取方法
- en: 原文：[https://towardsdatascience.com/understanding-tf-idf-a-traditional-approach-to-feature-extraction-in-nlp-a5bfbe04723f](https://towardsdatascience.com/understanding-tf-idf-a-traditional-approach-to-feature-extraction-in-nlp-a5bfbe04723f)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/understanding-tf-idf-a-traditional-approach-to-feature-extraction-in-nlp-a5bfbe04723f](https://towardsdatascience.com/understanding-tf-idf-a-traditional-approach-to-feature-extraction-in-nlp-a5bfbe04723f)
- en: Learn the fundamentals of TF-IDF and how to implement it from scratch in Python
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 学习TF-IDF的基础知识以及如何在Python中从零实现它
- en: '[](https://itsuncheng.medium.com/?source=post_page-----a5bfbe04723f--------------------------------)[![Raymond
    Cheng](../Images/09f3d83726274188c58f8eebc79abb0e.png)](https://itsuncheng.medium.com/?source=post_page-----a5bfbe04723f--------------------------------)[](https://towardsdatascience.com/?source=post_page-----a5bfbe04723f--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----a5bfbe04723f--------------------------------)
    [Raymond Cheng](https://itsuncheng.medium.com/?source=post_page-----a5bfbe04723f--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://itsuncheng.medium.com/?source=post_page-----a5bfbe04723f--------------------------------)[![Raymond
    Cheng](../Images/09f3d83726274188c58f8eebc79abb0e.png)](https://itsuncheng.medium.com/?source=post_page-----a5bfbe04723f--------------------------------)[](https://towardsdatascience.com/?source=post_page-----a5bfbe04723f--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----a5bfbe04723f--------------------------------)
    [Raymond Cheng](https://itsuncheng.medium.com/?source=post_page-----a5bfbe04723f--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----a5bfbe04723f--------------------------------)
    ·9 min read·Mar 30, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page-----a5bfbe04723f--------------------------------)
    ·阅读时间9分钟·2023年3月30日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/93fb9b8062226265d6274ca8a0717cfe.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/93fb9b8062226265d6274ca8a0717cfe.png)'
- en: Photo by [Aaron Burden](https://unsplash.com/@aaronburden?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 照片由[Aaron Burden](https://unsplash.com/@aaronburden?utm_source=medium&utm_medium=referral)拍摄，来源于[Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: Introduction
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: Feature extraction is an important initial step in [NLP](https://en.m.wikipedia.org/wiki/Natural_language_processing),
    which involves transforming textual data into a mathematical representation, often
    in the form of vectors, known as [word embeddings](https://en.wikipedia.org/wiki/Word_embedding).
    Various word embedding approaches exist, ranging from classical approaches like
    [word2vec](https://proceedings.neurips.cc/paper/2013/file/9aa42b31882ec039965f3c4923ce901b-Paper.pdf)
    and [GloVe](https://nlp.stanford.edu/pubs/glove.pdf) to more modern ones like
    [BERT](https://arxiv.org/pdf/1810.04805.pdf) embeddings. While [transformer](https://arxiv.org/pdf/1706.03762.pdf)-based
    embeddings dominate the field of NLP today, understanding the evolution of previous
    approaches can be helpful.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 特征提取是[自然语言处理](https://en.m.wikipedia.org/wiki/Natural_language_processing)（NLP）中的一个重要初始步骤，它涉及将文本数据转换为数学表示，通常是向量形式，这被称为[词嵌入](https://en.wikipedia.org/wiki/Word_embedding)。存在各种词嵌入方法，从经典的方法如[word2vec](https://proceedings.neurips.cc/paper/2013/file/9aa42b31882ec039965f3c4923ce901b-Paper.pdf)和[GloVe](https://nlp.stanford.edu/pubs/glove.pdf)到更现代的[BERT](https://arxiv.org/pdf/1810.04805.pdf)嵌入。虽然基于[transformer](https://arxiv.org/pdf/1706.03762.pdf)的嵌入今天主导了NLP领域，但了解以前方法的演变仍然是有帮助的。
- en: In this article, we will explore a more traditional approach to feature extraction
    known as [TF-IDF](https://en.wikipedia.org/wiki/Tf%E2%80%93idf), which is based
    on statistical analysis. We will delve into the details of TF-IDF and its implementation,
    as well as provide a bonus application to help solidify your understanding. So,
    stay with us until the end as we uncover the ins and outs of TF-IDF!
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们将探索一种被称为[TF-IDF](https://en.wikipedia.org/wiki/Tf%E2%80%93idf)的传统特征提取方法，该方法基于统计分析。我们将深入探讨TF-IDF及其实现，并提供一个额外的应用以帮助巩固您的理解。因此，请跟随我们，直到最后，揭开TF-IDF的方方面面！
- en: What is TF-IDF?
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是TF-IDF？
- en: TF-IDF, short for Term Frequency-Inverse Document Frequency, is a commonly used
    technique in NLP to determine the significance of words in a document or corpus.
    To give some background context, a [survey](http://nbn-resolving.de/urn:nbn:de:bsz:352-0-311312)
    conducted in 2015 showed that 83% of text-based recommender systems in digital
    libraries use TF-IDF for extracting textual features. That’s how popular the technique
    is. Essentially, it measures the importance of a word by comparing its frequency
    within a specific document with the frequency to its frequency in the entire corpus.
    The underlying assumption is that a word that occurs more frequently within a
    document but rarely in the corpus is particularly important in that document.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: TF-IDF，全称为词频-逆文档频率，是自然语言处理（NLP）中常用的技术，用于确定文档或语料库中单词的重要性。为了提供一些背景信息，一项[调查](http://nbn-resolving.de/urn:nbn:de:bsz:352-0-311312)显示，2015年83%的基于文本的推荐系统在数字图书馆中使用TF-IDF来提取文本特征。这表明了这一技术的流行程度。本质上，它通过比较词在特定文档中的频率与其在整个语料库中的频率来衡量词的重要性。其基本假设是，频繁出现在文档中但在语料库中很少出现的词在该文档中尤为重要。
- en: 'Now, let’s take a look at the mathematical formula for calculating TF-IDF:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看计算TF-IDF的数学公式：
- en: TF (Term Frequency) is determined by calculating the frequency of a word in
    a document and dividing it by the total number of words in the document.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: TF（词频）通过计算词在文档中的出现频率并将其除以文档中的总词数来确定。
- en: '`TF = (Number of times the word appears in the document) / (Total number of
    words in the document)`'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '`TF = (词在文档中出现的次数) / (文档中的总词数)`'
- en: 'IDF (Inverse Document Frequency), on the other hand, measures the importance
    of a word within the corpus as a whole. It is calculated as:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，IDF（逆文档频率）测量词在整个语料库中的重要性。其计算公式为：
- en: '`IDF = log((Total number of documents in the corpus) / (Number of documents
    containing the word))`'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '`IDF = log((语料库中的文档总数) / (包含该词的文档数))`'
- en: Finally, the TF-IDF score for a word in a given document is the product of the
    word’s TF and IDF scores. The higher the resulting TF-IDF score, the more significant
    the word is considered to be in the context of that document compared to other
    words.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，某词在给定文档中的TF-IDF分数是该词TF和IDF分数的乘积。结果的TF-IDF分数越高，表明该词在文档中的相对重要性越高。
- en: Implementing TF-IDF in Python
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 用Python实现TF-IDF
- en: Now that we’ve covered how TF-IDF is calculated mathematically, let’s implement
    it using Python. While there are libraries available to compute the TF-IDF features
    faster, this article will focus on building it from scratch.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了TF-IDF的数学计算方法，让我们用Python实现它。虽然有库可以更快地计算TF-IDF特征，但本文将专注于从头开始构建它。
- en: Setting up and Preprocessing
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 设置和预处理
- en: To begin, let’s import the necessary packages that we’ll need later on, such
    as the [Counter](https://docs.python.org/3/library/collections.html#collections.Counter)
    class from the [collections](https://docs.python.org/3/library/collections.html)
    module.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们导入后续需要的必要包，例如来自[collections](https://docs.python.org/3/library/collections.html)模块的[Counter](https://docs.python.org/3/library/collections.html#collections.Counter)类。
- en: '[PRE0]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Next, we’ll define a list of documents/corpus that we’ll use as an example.
    Let’s borrow from the recent craze around [ChatGPT](https://openai.com/blog/chatgpt)
    and Generative AI.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将定义一个文档/语料库的列表作为示例。我们借用最近围绕[ChatGPT](https://openai.com/blog/chatgpt)和生成性AI的热潮。
- en: '[PRE1]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Before calculating text features, it’s standard practice to first preprocess
    the documents in any NLP task, such as lowercase, lemmatization, stemming, stopword
    removal, etc. In this example, we’ll lowercase the documents and remove any punctuation.
    However, more preprocessing can be done depending on the task at hand, and these
    steps can be performed using NLP libraries such as [NLTK](https://www.nltk.org/)
    or [SpaCy](https://spacy.io/). We’ll also keep track of the unique set of tokens
    in the corpus.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在计算文本特征之前，标准做法是首先对文档进行预处理，如转换为小写、词形还原、词干提取、去除停用词等。在本示例中，我们将文档转换为小写并去除标点符号。然而，根据任务的不同，可以进行更多的预处理，这些步骤可以使用[NLTK](https://www.nltk.org/)或[SpaCy](https://spacy.io/)等NLP库来完成。我们还将跟踪语料库中的唯一词汇集。
- en: '[PRE2]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '![](../Images/82a5e2cdafdbfa1b583368c6e9736eb8.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/82a5e2cdafdbfa1b583368c6e9736eb8.png)'
- en: Calculating IDF of unique tokens in the corpus
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 计算语料库中唯一词汇的IDF
- en: After we have the token set, we can calculate the IDF of each token in the corpus
    using the formula given above.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在获得词汇集之后，我们可以使用上述公式计算语料库中每个词的IDF。
- en: '[PRE3]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '![](../Images/bd703ad0671c2cd98139794ca09e3091.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bd703ad0671c2cd98139794ca09e3091.png)'
- en: Notice that words like “openai” and “gpt” have a higher IDF than words like
    “chatgpt” or “ai”, since the former terms occur less frequently in the corpus.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，像“openai”和“gpt”这样的词的 IDF 高于“chatgpt”或“ai”，因为前者在语料库中的出现频率较低。
- en: Calculating TF of each token for each document
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 计算每个文档中每个词的 TF
- en: While IDF is calculated per token on the entire corpus, TF is calculated for
    each token on a per-document basis. Using the formula for TF, we can quickly get
    the count of a token in a document and calculate its relative frequency using
    the Counter class.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然 IDF 是在整个语料库中对每个词计算的，但 TF 是在每个文档中对每个词计算的。使用 TF 的公式，我们可以快速获取文档中某个词的计数，并使用 Counter
    类计算其相对频率。
- en: '[PRE4]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Here’s an example of the term frequency of “chatgpt” in each document.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 这是“chatgpt”在每个文档中的词频示例。
- en: '[PRE5]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '![](../Images/b57febf6079ecb51e0ab71ec749a0f95.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b57febf6079ecb51e0ab71ec749a0f95.png)'
- en: Finally, onto TF-IDF
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 最后，进入 TF-IDF
- en: Now, it’s time to implement the TF-IDF function. We can first wrap the previous
    preprocessing code into a function so we can call it when calculating the TF-IDF.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，是时候实现 TF-IDF 函数了。我们可以先将之前的预处理代码包装成一个函数，以便在计算 TF-IDF 时调用它。
- en: '[PRE6]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Let’s try some examples.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试一些示例。
- en: '[PRE7]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '![](../Images/f04b85ff7921b0677fbda4afcadaa67f.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f04b85ff7921b0677fbda4afcadaa67f.png)'
- en: '[PRE8]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '![](../Images/1a1e5a26a1bca510ff676fbefe9ff924.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1a1e5a26a1bca510ff676fbefe9ff924.png)'
- en: '[PRE9]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '![](../Images/d76775abd9825e4224e50159eb6ace36.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d76775abd9825e4224e50159eb6ace36.png)'
- en: '[PRE10]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '![](../Images/29d49678ff01770241089eac059a6ffc.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/29d49678ff01770241089eac059a6ffc.png)'
- en: '[PRE11]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '![](../Images/aa99415bc1f7bf4988cca71b4089bba7.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/aa99415bc1f7bf4988cca71b4089bba7.png)'
- en: From the above examples, we can see that “chatgpt” has a higher TF-IDF in documents
    1 and 2 than document 3 since the term doesn’t appear in document 3\. Although
    “chatgpt” occurs in document 1 and 2 only once, the former has a slightly higher
    TF since it has fewer tokens, leading to a higher TF-IDF.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 从上面的例子中，我们可以看到“chatgpt”在文档 1 和 2 中的 TF-IDF 高于文档 3，因为该词没有出现在文档 3 中。尽管“chatgpt”在文档
    1 和 2 中只出现过一次，但前者的 TF 稍高，因为它的词数更少，从而导致更高的 TF-IDF。
- en: The “gpt” TF-IDF is 0 in documents 1 and 3, since neither of them contains the
    word. “gpt” in document 2 is present; however, the TF-IDF is higher than the TF-IDF
    of “chatgpt” in document 1 since its rare occurrence in the corpus outweighs the
    longer length of the second document.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: “gpt”在文档 1 和 3 中的 TF-IDF 为 0，因为它们都不包含这个词。文档 2 中有“gpt”，然而，其 TF-IDF 高于文档 1 中“chatgpt”的
    TF-IDF，因为它在语料库中的稀有出现超越了第二个文档的较长长度。
- en: “generative” has a TF-IDF of 0 in documents 1 and 2 due to its non-occurrence.
    “is” has a 0 TF-IDF for all documents since it is present in all of them. Out-of-vocabulary
    tokens like “gpt4” also have a TF-IDF of 0.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: “generative”在文档 1 和 2 中的 TF-IDF 为 0，因为没有出现。“is”在所有文档中的 TF-IDF 都为 0，因为它出现在所有文档中。像“gpt4”这样的未登录词也有
    TF-IDF 为 0。
- en: And here you have it, how you could implement TF-IDF in Python.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是如何在 Python 中实现 TF-IDF 的方法。
- en: 'Bonus: Building a search engine MVP with TF-IDF'
  id: totrans-58
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 奖励：用 TF-IDF 构建搜索引擎 MVP
- en: TF-IDF has many potential applications, and one of them is building a search
    engine. In this section, we will explore how we can use TF-IDF to build a simple
    search engine MVP. The approach we will take is to rank documents against a query
    by the sum of their TF-IDF terms, with a higher sum resulting in a higher document
    ranking.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: TF-IDF 有许多潜在的应用，其中之一是构建搜索引擎。在本节中，我们将探讨如何使用 TF-IDF 构建一个简单的搜索引擎 MVP。我们采取的方法是通过
    TF-IDF 词汇的总和来排名文档，总和越高，文档排名越高。
- en: To get started, let’s modify the `tf_idf` function to append the TF-IDF results
    to a list and set the TF-IDF of out-of-vocabulary words to 0.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 要开始，先修改 `tf_idf` 函数，将 TF-IDF 结果附加到列表中，并将未登录词的 TF-IDF 设置为 0。
- en: '[PRE12]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: With this modification, we can now implement the search engine. The code below
    computes the TF-IDF values of each query term against the corpus and ranks the
    documents in order of their summation.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这个修改，我们现在可以实现搜索引擎。下面的代码计算每个查询词在语料库中的 TF-IDF 值，并按其总和对文档进行排名。
- en: '[PRE13]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Let’s try searching for the query “ChatGPT AI”:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试搜索查询“ChatGPT AI”：
- en: '[PRE14]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '![](../Images/1891b13cbef3b6ac4bfd5f8c46090114.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1891b13cbef3b6ac4bfd5f8c46090114.png)'
- en: We can see that “ChatGPT AI” is most relevant to document 1 because of its highest
    TF-IDF. This makes sense since “ChatGPT” and “AI” are both included in the document
    while each of the other two documents contains only one of the terms.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，“ChatGPT AI”与文档 1 的相关性最高，因为它的 TF-IDF 最高。这是有道理的，因为“ChatGPT”和“AI”都包含在该文档中，而另外两个文档各自只包含一个术语。
- en: What will happen if we try “AI language models”?
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们尝试“AI 语言模型”会发生什么？
- en: '[PRE15]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '![](../Images/c2357be84d8e9a89adeefa10949e0f70.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c2357be84d8e9a89adeefa10949e0f70.png)'
- en: Document 2 is ranked the highest, since it contains two tokens “language” and
    “models”, while the other two only contain “AI”. Document 1 is ranked higher than
    3 in this case since it is shorter, resulting in slightly higher term frequency.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 文档 2 排名最高，因为它包含了两个词项“language”和“models”，而另外两个文档只包含“AI”。在这种情况下，文档 1 排在文档 3 之前，因为它较短，导致词频略高。
- en: And here you have it, an extremely simple search engine based on TF-IDF!
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 就这样，你拥有了一个基于 TF-IDF 的极其简单的搜索引擎！
- en: Conclusion
  id: totrans-73
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: 'In this article, we learned about TF-IDF, what it is, how it works, and most
    importantly, why it’s needed in modern NLP applications. We also implemented TF-IDF
    and showed that we can even build a minimalistic search engine out of it. If you
    don’t want to implement it from scratch, you can use `[sklearn.feature_extraction.text.TfidfVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html)`
    to implement it faster. Here are two great blog articles that cover how to use
    it: [1](https://www.analyticsvidhya.com/blog/2021/11/how-sklearns-tfidfvectorizer-calculates-tf-idf-values/)
    and [2](https://okan.cloud/posts/2022-01-16-text-vectorization-using-python-tf-idf/).'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们学习了 TF-IDF，它是什么，它如何工作，以及最重要的，为什么它在现代 NLP 应用中是必需的。我们还实现了 TF-IDF 并展示了如何用它构建一个极简的搜索引擎。如果你不想从头实现它，可以使用
    `[sklearn.feature_extraction.text.TfidfVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html)`
    更快地实现它。这里有两个很好的博客文章介绍了如何使用它：[1](https://www.analyticsvidhya.com/blog/2021/11/how-sklearns-tfidfvectorizer-calculates-tf-idf-values/)
    和 [2](https://okan.cloud/posts/2022-01-16-text-vectorization-using-python-tf-idf/)。
- en: TF-IDF is not the only feature extraction method for text. There are many more
    methods that might perform better than it, and it’s worth understanding them.
    Nonetheless, it’s still a popular text feature approach, and we might cover the
    more recent ones in the future.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: TF-IDF 并不是文本特征提取的唯一方法。还有许多其他方法可能表现更好，值得了解它们。尽管如此，它仍然是一种流行的文本特征方法，我们可能会在未来涵盖更近期的方法。
- en: Thanks for reading. If this helps, feel free to follow me and subscribe for
    more articles to come.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 感谢阅读。如果这对你有帮助，随时关注我并订阅更多即将发布的文章。
- en: Hope you had a great time.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 希望你度过了一段愉快的时光。
- en: Cheers.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 干杯。
- en: References
  id: totrans-79
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] Mikolov, Tomas, et al. “Distributed representations of words and phrases
    and their compositionality.” *Advances in neural information processing systems*
    26 (2013).'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] Mikolov, Tomas 等. “词语和短语的分布式表示及其组合性。” *神经信息处理系统进展* 26 (2013)。'
- en: '[2] Pennington, Jeffrey, Richard Socher, and Christopher D. Manning. “Glove:
    Global vectors for word representation.” *Proceedings of the 2014 conference on
    empirical methods in natural language processing (EMNLP)*. 2014.'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] Pennington, Jeffrey, Richard Socher 和 Christopher D. Manning. “Glove：词表示的全局向量。”
    *2014 年自然语言处理经验方法会议（EMNLP）论文集*。2014。'
- en: '[3] Devlin, Jacob, et al. “Bert: Pre-training of deep bidirectional transformers
    for language understanding.” *arXiv preprint arXiv:1810.04805* (2018).'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] Devlin, Jacob 等. “Bert：用于语言理解的深度双向变换器预训练。” *arXiv 预印本 arXiv:1810.04805*
    (2018)。'
- en: '[4] Vaswani, Ashish, et al. “Attention is all you need.” *Advances in neural
    information processing systems* 30 (2017).'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] Vaswani, Ashish 等. “注意力即一切。” *神经信息处理系统进展* 30 (2017)。'
- en: '[5] Beel, Joeran, et al. “Paper recommender systems: a literature survey.”
    *International Journal on Digital Libraries* 17 (2016): 305–338.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '[5] Beel, Joeran 等. “论文推荐系统：文献综述。” *国际数字图书馆期刊* 17 (2016): 305–338。'
- en: '[6] “How sklearn’s Tfidfvectorizer Calculates tf-idf Values”, Analytics Vidhya,
    [https://www.analyticsvidhya.com/blog/2021/11/how-sklearns-tfidfvectorizer-calculates-tf-idf-values/](https://www.analyticsvidhya.com/blog/2021/11/how-sklearns-tfidfvectorizer-calculates-tf-idf-values/)'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '[6] “如何 sklearn 的 Tfidfvectorizer 计算 tf-idf 值”，Analytics Vidhya，[https://www.analyticsvidhya.com/blog/2021/11/how-sklearns-tfidfvectorizer-calculates-tf-idf-values/](https://www.analyticsvidhya.com/blog/2021/11/how-sklearns-tfidfvectorizer-calculates-tf-idf-values/)'
- en: '[7] “Text Vectorization Using Python: TF-IDF”, Okan Bulut, [https://okan.cloud/posts/2022-01-16-text-vectorization-using-python-tf-idf/](https://okan.cloud/posts/2022-01-16-text-vectorization-using-python-tf-idf/)'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '[7] “使用 Python 进行文本向量化：TF-IDF”，Okan Bulut，[https://okan.cloud/posts/2022-01-16-text-vectorization-using-python-tf-idf/](https://okan.cloud/posts/2022-01-16-text-vectorization-using-python-tf-idf/)'
- en: '[8] “Understanding TF-IDF For Machine Learning”, Capital One, [https://www.capitalone.com/tech/machine-learning/understanding-tf-idf/](https://www.capitalone.com/tech/machine-learning/understanding-tf-idf/)'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '[8] “理解机器学习中的 TF-IDF”，Capital One，[https://www.capitalone.com/tech/machine-learning/understanding-tf-idf/](https://www.capitalone.com/tech/machine-learning/understanding-tf-idf/)'
- en: '[9] “Understanding TF-ID: A Simple Introduction”, MonkeyLearn, [https://monkeylearn.com/blog/what-is-tf-idf/](https://monkeylearn.com/blog/what-is-tf-idf/)'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '[9] “理解 TF-IDF：一个简单的介绍”，MonkeyLearn，[https://monkeylearn.com/blog/what-is-tf-idf/](https://monkeylearn.com/blog/what-is-tf-idf/)'
