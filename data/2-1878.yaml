- en: 'Sklearn Tutorial: Module 4'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Sklearn 教程：模块 4
- en: 原文：[https://towardsdatascience.com/sklearn-tutorial-module-4-1e1a50e5247d](https://towardsdatascience.com/sklearn-tutorial-module-4-1e1a50e5247d)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/sklearn-tutorial-module-4-1e1a50e5247d](https://towardsdatascience.com/sklearn-tutorial-module-4-1e1a50e5247d)
- en: Linear models, handling non-linearity, and regularization
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 线性模型、处理非线性和正则化
- en: '[](https://mocquin.medium.com/?source=post_page-----1e1a50e5247d--------------------------------)[![Yoann
    Mocquin](../Images/b30a0f70c56972aabd2bc0a74baa90bb.png)](https://mocquin.medium.com/?source=post_page-----1e1a50e5247d--------------------------------)[](https://towardsdatascience.com/?source=post_page-----1e1a50e5247d--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----1e1a50e5247d--------------------------------)
    [Yoann Mocquin](https://mocquin.medium.com/?source=post_page-----1e1a50e5247d--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://mocquin.medium.com/?source=post_page-----1e1a50e5247d--------------------------------)[![Yoann
    Mocquin](../Images/b30a0f70c56972aabd2bc0a74baa90bb.png)](https://mocquin.medium.com/?source=post_page-----1e1a50e5247d--------------------------------)[](https://towardsdatascience.com/?source=post_page-----1e1a50e5247d--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----1e1a50e5247d--------------------------------)
    [Yoann Mocquin](https://mocquin.medium.com/?source=post_page-----1e1a50e5247d--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----1e1a50e5247d--------------------------------)
    ·14 min read·Dec 22, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----1e1a50e5247d--------------------------------)
    ·14 分钟阅读·2023 年 12 月 22 日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: 'This is the **fourth** post in my scikit-learn tutorial series. If you didn’t
    catch them, I strongly recommend my first three posts — it’ll be way easier to
    follow along:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我 scikit-learn 教程系列的**第四**篇文章。如果你错过了前几篇，我强烈推荐你先阅读前三篇——这样会更容易跟上：
- en: '![Yoann Mocquin](../Images/234a99f243ff3c70fd90170ddde8659d.png)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
  zh: '![Yoann Mocquin](../Images/234a99f243ff3c70fd90170ddde8659d.png)'
- en: '[Yoann Mocquin](https://mocquin.medium.com/?source=post_page-----1e1a50e5247d--------------------------------)'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '[Yoann Mocquin](https://mocquin.medium.com/?source=post_page-----1e1a50e5247d--------------------------------)'
- en: Sklearn tutorial
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Sklearn 教程
- en: '[View list](https://mocquin.medium.com/list/sklearn-tutorial-2e46a0e06b39?source=post_page-----1e1a50e5247d--------------------------------)9
    stories![](../Images/4ffe6868fb22c241a959bd5d5a9fd5d7.png)![](../Images/8aa32b00faa0ef7376e121ba9c9ffdb7.png)![](../Images/9f986423d7983bc08fc2073534603c35.png)'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '[查看列表](https://mocquin.medium.com/list/sklearn-tutorial-2e46a0e06b39?source=post_page-----1e1a50e5247d--------------------------------)9
    个故事![](../Images/4ffe6868fb22c241a959bd5d5a9fd5d7.png)![](../Images/8aa32b00faa0ef7376e121ba9c9ffdb7.png)![](../Images/9f986423d7983bc08fc2073534603c35.png)'
- en: This 4th module introduces the concept of **linear models**, using the infamous
    **linear regression** and **logistic regression** models as working examples.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 第 4 模块介绍了**线性模型**的概念，使用臭名昭著的**线性回归**和**逻辑回归**模型作为实际示例。
- en: In addition to these basic linear models, we show how to use feature engineering
    to **handle nonlinear problems using only linear models,** as well as the concept
    of **regularization** in order to prevent overfitting.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 除了这些基本的线性模型外，我们展示了如何利用特征工程来**仅使用线性模型处理非线性问题**，以及**正则化**的概念以防止过拟合。
- en: Altogether, these concepts enable us to create very simple yet powerful models,
    capable of handling a lot of ML problems with fine-tuned hyperparameters, without
    overfitting, while handling nonlinear problems.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，这些概念使我们能够创建非常简单但强大的模型，能够处理许多机器学习问题，通过微调超参数来避免过拟合，同时处理非线性问题。
- en: '![](../Images/1fc42d52ce3838258d306c1ef042f157.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1fc42d52ce3838258d306c1ef042f157.png)'
- en: Photo by [Roman Synkevych](https://unsplash.com/@synkevych?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由 [Roman Synkevych](https://unsplash.com/@synkevych?utm_source=medium&utm_medium=referral)
    拍摄，来自 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: '*All graphs and images are made by the author.*'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '*所有图表和图像均由作者制作。*'
- en: Linear models
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 线性模型
- en: '**Linear models are models that “fit” or “learn” by setting coefficients such
    that they eventually only rely on a linear combination of the input features.**
    In other words, if the input data is made of N features f_1 to f_N, the model
    at some point is based on the linear combination:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '**线性模型是通过设置系数来“拟合”或“学习”的模型，使其最终仅依赖于输入特征的线性组合。** 换句话说，如果输入数据由 N 个特征 f_1 到 f_N
    组成，则模型在某些时候基于线性组合：'
- en: '![](../Images/8be0af90148c270f2a0b0d1479b1059d.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8be0af90148c270f2a0b0d1479b1059d.png)'
- en: The coefficients the model learns are the N+1 coefficients beta. The coefficient
    beta_0 represent an offset, a constant value in the output whatever the values
    in the input. The idea behind such models is that the “truth” can be approximated
    with a linear relationship between the inputs and the output.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 模型学习到的系数是 N+1 个系数 beta。系数 beta_0 代表一个偏移量，无论输入值如何，输出中都存在一个常量值。这种模型的思想是“真实情况”可以通过输入和输出之间的线性关系来近似。
- en: In the case of regression problems where we want to predict a numerical value
    from the inputs, one of the simplest and well known linear model is the linear
    regression. You most likely have done hundreds of linear regression already (by
    hand, in excel or python).
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在回归问题中，我们想从输入中预测一个数值，其中一个最简单且广为人知的线性模型是线性回归。你很可能已经进行过数百次线性回归（通过手动操作、在 Excel
    中或使用 Python）。
- en: In the case of classification problem, where we want to predit a category from
    the inputs, the simplest and well known linear model is the logistic regression
    (don’t get fooled by the “regression” in “logistic regression”, it really deals
    with classification).
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在分类问题中，我们想要从输入中预测一个类别，最简单且广为人知的线性模型是逻辑回归（不要被“逻辑回归”中的“回归”欺骗，它实际上处理的是分类问题）。
- en: Many other linear models exists, for example Support Vector Regression and Support
    Vector Classification, as well as lots of variants of linear regression and logistic
    regression. They could all be the subject of a series of posts. The idea here
    is not to review them in depth, but to show their basic usage and limitations
    (although my completeness syndrom will make me detail them a bit).
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 还有许多其他线性模型，例如支持向量回归和支持向量分类，以及许多线性回归和逻辑回归的变体。这些都可以成为一系列文章的主题。这里的想法不是深入审查它们，而是展示它们的基本使用和限制（尽管我的完备症会让我详细讲解一些）。
- en: '**Important note:** one of the committed stances of sklearn is to provide models
    that work out of the box, so newcomers can have running code quickly (and not
    spend too much time setting up framework gimmicks or have many errors to deal
    with when setting up new models)—I’d still recommend reading the documentation
    extensively as it is well written and you’ll learn a lot both about the python
    API as well as the mathematics and good practices.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '**重要说明：** sklearn 的一个承诺立场是提供开箱即用的模型，以便新手可以快速运行代码（而不需要花费太多时间设置框架的花招或在设置新模型时处理很多错误）——我仍然建议广泛阅读文档，因为它写得很好，你将会学到很多关于
    Python API 以及数学和良好实践的知识。'
- en: 'Linear model for regression: linear regression'
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 回归的线性模型：线性回归
- en: 'Linear regression is the most well known linear regression model. As stated
    above, the idea is to approximate as best as possible the output y from a linear
    combination of the inputs f_i:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 线性回归是最著名的线性回归模型。如上所述，其思想是通过输入 f_i 的线性组合尽可能准确地近似输出 y：
- en: '![](../Images/8be0af90148c270f2a0b0d1479b1059d.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8be0af90148c270f2a0b0d1479b1059d.png)'
- en: One of the reasons of the popularity of linear regression and linear models
    in general is that they can be handle with matrices, since matrices are representations
    of linear operations. In particular, one the possible ways (and the most used
    in science in general, not particularly in ML) to learn the coefficients beta
    is to use the Ordinary Least Squares method.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 线性回归和线性模型广泛流行的原因之一是它们可以用矩阵来处理，因为矩阵是线性操作的表示。特别是，学习系数 beta 的一种可能方法（在科学中普遍使用，尤其是在机器学习中）是使用普通最小二乘法。
- en: '**The Ordinary Least Squares method** consists in selecting the one and best
    vector of beta such that the sum of squared errors is minimal: this method has
    the advantage to be easily interpretable (the model “minimizes” the squared distance
    between the data and its predictions) and it has a closed-form solution (so no
    numerical optimization method is needed, it basically is just matrix multiplication
    and inversion). If you check the documention of sklearn LinearRegression, you’ll
    see that its this method that is implemented.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '**普通最小二乘法** 旨在选择一个最佳的 beta 向量，使得平方误差之和最小：这种方法的优点在于易于解释（模型“最小化”数据与预测之间的平方距离），并且它有一个封闭形式的解（因此不需要数值优化方法，它基本上只是矩阵乘法和逆运算）。如果你查看
    sklearn 的 LinearRegression 文档，你会看到正是这个方法被实现了。'
- en: Many other methods exists in order to fit a linear regression, which all lead
    to “variants” of that simplest model. As we’ll see below, ridge regression and
    lasso regression are part of such variants.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 还有许多其他方法可以用来拟合线性回归，这些方法都导致了该最简单模型的“变体”。正如我们下面将看到的，岭回归和套索回归是这些变体的一部分。
- en: 'Note that linear regression allows to do polynomial regression if a small preprocessing
    step is applied. Indeed a polynomial is just like a linear combination of monomial
    of inputs:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，线性回归允许进行多项式回归，只需应用一个小的预处理步骤。事实上，多项式就像是输入的单项式的线性组合：
- en: '![](../Images/8be0af90148c270f2a0b0d1479b1059d.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8be0af90148c270f2a0b0d1479b1059d.png)'
- en: so a polynomial can be written
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 所以一个多项式可以写成
- en: '![](../Images/48ad8a447622c97a20ae21c581f4a645.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/48ad8a447622c97a20ae21c581f4a645.png)'
- en: in the case of an univariate (a single input feature). To do so we simply generate
    a new input matrix made of all the polynomial variables we want (the X¹, X², etc
    for univariate problem, and even cross-variables in the case of multivariate polynomial
    like X_1 X_2, X_1 X_3, and so on).
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在单变量（单一输入特征）的情况下。为此，我们只需生成一个新的输入矩阵，该矩阵由我们想要的所有多项式变量组成（对于单变量问题，是 X¹、X² 等，甚至在多变量多项式情况下如
    X_1 X_2、X_1 X_3 等的交叉变量）。
- en: 'Here is a simple example of a linear regression for a 1d input feature, so
    the model actually learn only beta0 and beta1:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个关于 1d 输入特征的线性回归简单示例，因此模型实际上只学习 beta0 和 beta1：
- en: '[PRE0]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '![](../Images/9bafb0f6b3d9bcf0fae2a03c627e5488.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9bafb0f6b3d9bcf0fae2a03c627e5488.png)'
- en: 'To give you a bit more information about the linear regression model in sklearn:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 让你多了解一些关于 sklearn 中线性回归模型的信息：
- en: you can tune the linear regression to handle the offset beta0 or not by setting
    the `fit_intercept` hyperparameter `LinearRegression(fit_intercept=True)`. If
    using False, the model expect target y to be centered, that is to have mean 0.
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你可以通过设置 `fit_intercept` 超参数 `LinearRegression(fit_intercept=True)` 来调整线性回归是否处理偏置
    beta0。如果使用 False，则模型期望目标 y 被中心化，即均值为 0。
- en: once the model has been fitted, it learned the coefficients betas. You can inspect
    them using `model.intercept_` and `model.coefficients_`. Remember that in the
    sklearn API, the learned parameters are suffixed with an underscore “_”
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一旦模型拟合完成，它就会学习到系数 beta。你可以使用 `model.intercept_` 和 `model.coefficients_` 来检查这些系数。请记住，在
    sklearn API 中，学习到的参数后缀带有下划线“_”。
- en: the default score for linear regression is the R² coefficients which translates
    how well the fitted model “explains” the variability of the dataset. Of course,
    you can also import any score function from the metrics module and compute other
    scores using for example `from sklearn.metric import mean_absolute_error; mean_absolute_error(y_true,
    y_pred)`
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 线性回归的默认评分是 R² 系数，它表明拟合模型“解释”数据集变异性的程度。当然，你也可以从 metrics 模块中导入任何评分函数，并使用例如 `from
    sklearn.metric import mean_absolute_error; mean_absolute_error(y_true, y_pred)`
    来计算其他评分。
- en: 'Linear model for classification: logistic regression'
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分类的线性模型：逻辑回归
- en: The equivalent of linear regression for classification problem is logistic regression.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 线性回归在分类问题中的等价物是逻辑回归。
- en: 'The idea is pretty simple: create a linear combination y that when feeded of
    the logistic function best separates the target classes. Like the linear regression,
    the linear combination y can have any value — but to meet our context of classification
    it is feeded to the logistic function which is an S-shaped function that takes
    any real input and maps it to the [0, 1] interval. This interval is then associated
    with the target classes, where 0 corresponds to a class and +1 to the other.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 这个想法很简单：创建一个线性组合 y，当其输入到逻辑函数时，能够最好地分隔目标类别。像线性回归一样，线性组合 y 可以具有任何值——但为了满足分类的背景，它被输入到逻辑函数中，该函数是一个
    S 形函数，接受任何实数输入，并将其映射到 [0, 1] 区间。这个区间然后与目标类别相关联，其中 0 对应于一个类别，+1 对应于另一个类别。
- en: In other words, if a sample is mapped to a very negative value from the linear
    combination, it’ll be heavily associated with class 0\. As the y value increases
    and approaches 0.5, the target class becomes “uncertain”. Then if the linear combination
    y keeps increasing above 0.5, it will be mapped to class +1\. In this case, we
    say that 0.5 is the classifcation threshold. Note that some other similar algorithms
    use rather the [-1,1] mapping interval with a threshold value of 0\. Those are
    basicaly jsut conventions and won’t change the model performance.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，如果一个样本从线性组合中映射到一个非常负的值，它将与类别0密切相关。随着y值的增加并接近0.5，目标类别变得“不确定”。然后，如果线性组合y继续增加超过0.5，它将被映射到类别+1。在这种情况下，我们称0.5为分类阈值。请注意，一些其他类似的算法使用的是[-1,1]的映射区间，阈值为0。这些基本上只是惯例，不会改变模型的性能。
- en: 'So we could write the model as such:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们可以这样写模型：
- en: '![](../Images/6d7ca4d291f7b3aa0436ea76e0654385.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6d7ca4d291f7b3aa0436ea76e0654385.png)'
- en: where x represents a sample vector of length N with features f_1 to f_N, and
    y is the linear combination of that sample with the model’s coefficients which
    can have any value, and that value is mapped to the [0–1] interval thanks to the
    logistic function.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 其中x代表一个长度为N的样本向量，特征为f_1到f_N，y是该样本与模型系数的线性组合，系数可以有任意值，该值通过逻辑函数映射到[0-1]区间。
- en: To express it again differently, the probability of a sample to belong to a
    certain class is linked to its corresponding linear combination value y. The final
    corresponding class is simply the closest -or most probable- class, based on its
    position relative to the threshold.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，一个样本属于某一类别的概率与其对应的线性组合值y有关。最终的类别是基于其相对于阈值的位置，最接近的 - 或最可能的 - 类别。
- en: In sklearn terms, the probability is computed with the `.predict_proba` which
    returns an array of floats that sum to 1 to represent probabilities to belong
    to a class. On the other hand, the `.predict` returns a class, which corresponds
    to the most probable class of the `.predict_proba`.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在sklearn术语中，概率是通过`.predict_proba`计算的，该方法返回一个浮点数组，数组的和为1，以表示属于某个类别的概率。另一方面，`.predict`返回一个类别，对应于`.predict_proba`的最可能类别。
- en: 'Let’s see a simple 1D example: again, the linear model only has a single input
    to work on, so the X-axis can be used to plot either the feature value, or the
    y linear combination (beta1X+beta0):'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一个简单的一维例子：同样，线性模型只有一个输入，因此X轴可以用来绘制特征值或y线性组合（beta1X+beta0）：
- en: '[PRE1]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '![](../Images/cf36894f9cb19d485d8f10f700a62652.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/cf36894f9cb19d485d8f10f700a62652.png)'
- en: Here the green line corresponds to the linear combination of the input features.
    It corresponds to the logistic value of the linear combination for a given sample
    with value x. By tunning the linear coefficients, this green lines shape and position
    moves to better match the train samples. It is then used to predict the class
    and probability of new samples.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，绿色线条对应于输入特征的线性组合。它对应于给定样本的线性组合的逻辑值。通过调整线性系数，这条绿色线的形状和位置会移动，以更好地匹配训练样本。然后它被用来预测新样本的类别和概率。
- en: 'To better understand, let’s see a 2d example, which is kinda more suited for
    visual linear classification example:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地理解，我们来看看一个二维例子，这个例子更适合用于视觉线性分类。
- en: '[PRE2]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '![](../Images/75acb6e47fa77181a95a9ff429cf6d93.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/75acb6e47fa77181a95a9ff429cf6d93.png)'
- en: This example show how a 2d input feature is splitted by the model by a 1D line.
    This “line” represents the logistic function, so that above the threshold the
    samples belong to one class, and on the other side they belong to the other. The
    important idea here is to expand the reasoning from the previous example to higher
    dimension.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 这个例子展示了模型如何通过一条一维线将二维输入特征划分开来。这条“线”代表逻辑函数，因此在阈值以上样本属于一个类别，而在另一侧则属于另一个类别。这里的重要思想是将前一个例子的推理扩展到更高维度。
- en: 'Like before, here are additionnal info about the logistic regression model
    in sklearn:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 像之前一样，这里是有关sklearn中的逻辑回归模型的附加信息：
- en: LogisticRegression takes more hyperparameters, including `fit_intercept` like
    linear regression, but also additional parameters that allow to tune regularization
    — we’ll see about those further below
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LogisticRegression接受更多的超参数，包括类似线性回归的`fit_intercept`，但也有其他参数允许调整正则化——我们将在下面进一步讨论这些。
- en: also like linear regression, the learned coefficients can be accessed with `model.coef_`
    and `model.intercept_`. Additionnaly, you can get a list of the encountered classes
    with `model.classes_`.
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与线性回归一样，学习到的系数可以通过`model.coef_`和`model.intercept_`访问。此外，你还可以通过`model.classes_`获取遇到的类列表。
- en: 'the default score is the accuracy, which is simply a percentage of good classification:
    0 corresponds to no good prediction and 1 corresponds to all good predictions.'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 默认的评分是准确度，这只是正确分类的百分比：0表示没有正确预测，1表示所有预测都正确。
- en: Again, there are a lot of things to say about linear models for classication,
    but the point here is just to provide simple example. To learn more about LogisticRegression,
    I strongly encourage you to go check the user guide of sklearn.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 再次强调，关于分类的线性模型还有很多要说的，但这里的重点只是提供一个简单的例子。要了解更多关于LogisticRegression的内容，我强烈建议你去查看sklearn的用户指南。
- en: Handling non-linear data
  id: totrans-65
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 处理非线性数据
- en: So far we have seen examples of linear regression and logistic for synthetic
    data that were indeed linera. In other words, the truth we try to approximate
    with a linear model was indeed linear. But that almost never happens with real
    data, where the systems we try to model and reproduce are pretty non-linear.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经看到线性回归和逻辑回归在合成数据上的例子，这些数据确实是线性的。换句话说，我们尝试用线性模型来逼近的真实情况确实是线性的。但在实际数据中，这几乎从未发生过，我们尝试建模和复制的系统通常是相当非线性的。
- en: So does that means that the linear models fall short ? Actually no, there is
    a workaround.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 那么这是否意味着线性模型不够好？实际上并非如此，有解决方法。
- en: In addition to using natively non-linear models (models that handle non-linear
    data by design), we can still use our linear models by creating new features in
    the input data that hold some non-linearity.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 除了使用天生非线性的模型（通过设计处理非线性数据的模型）外，我们还可以通过在输入数据中创建具有一定非线性的特征来使用线性模型。
- en: In other words, we are going to use the same models, but with “bigger” input
    data matrix, where we add “ourselves” new features that contains non-linear relations
    between the input features.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，我们将使用相同的模型，但使用“更大”的输入数据矩阵，在其中我们“自己”添加包含输入特征之间非线性关系的新特征。
- en: 'A good simple example is a polynomial regression as introduced above. Say we
    want to fit a target y that is non-linear with respect to a single feature x.
    With standard linear model, this polynomial regression is simply a beta1 x + beta0
    regression. If instead we create new features, say x² and x³, the input matrix
    has now 3 features, and the linear regression can use the relation y=beta3 x³
    + beta2 x² + beta_1x + beta0 to fit the target. See the following example :'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 一个好的简单例子是上面介绍的多项式回归。假设我们想要拟合一个相对于单个特征x非线性的目标y。对于标准线性模型，这个多项式回归只是beta1 x + beta0回归。如果我们创建新的特征，比如x²和x³，输入矩阵现在有3个特征，线性回归可以使用关系y=beta3
    x³ + beta2 x² + beta1x + beta0来拟合目标。见下例：
- en: '[PRE3]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '![](../Images/1b4f46459e0f400a89fd1c852e897c02.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1b4f46459e0f400a89fd1c852e897c02.png)'
- en: 'In particular, let’s inspect the coefficients for both the linear regression
    and the polynomial regression:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 特别是，让我们检查线性回归和多项式回归的系数：
- en: '[PRE4]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: For the simple linear regression we only get the beta1 value and beta0 intercept,
    but for the polynomial regressor, we get 3 coefficients corresponding to beta3,
    beta2 and beta1, as well as the beta0 intercept.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 对于简单的线性回归，我们只得到beta1值和beta0截距，但对于多项式回归，我们得到3个系数，分别对应beta3、beta2和beta1，以及beta0截距。
- en: Using PolynomialFeatures is just one of many possibilities to create new feature
    that contain non-linearity. Other options are using KBinsDiscretizer (especially
    with `encode=’onehot`), the SplineTransformer or kernel approaches with Nystroem
    or the kernel trick implemented in some models (like Support Vector Machines models
    with SVR for regression and SVC for classification).
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 使用PolynomialFeatures只是创建包含非线性的新特征的众多可能性之一。其他选项包括使用KBinsDiscretizer（特别是`encode='onehot'`）、SplineTransformer或使用Nystroem的核方法，或者某些模型中实现的核技巧（例如，使用SVR进行回归和SVC进行分类的支持向量机模型）。
- en: 'The approach is always the same: create new features that are non-linear and
    add them to the input data so the linear models can use them to fit complex y
    target. And the good news is that in sklearn all the approaches are implemented
    either as preprocessing steps in a pipeline, or are built-in the estimator models.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 方法总是一样的：创建非线性的新特征并将其添加到输入数据中，以便线性模型可以利用这些特征来拟合复杂的y目标。好消息是，在sklearn中，所有这些方法都实现为管道中的预处理步骤，或内置于估计模型中。
- en: Regularization
  id: totrans-78
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 正则化
- en: So far we have seen how to use basic linear model, both on linear problems and
    non-linear problems by adding new non-linear features.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经看到如何使用基本的线性模型，既用于线性问题也用于通过添加新的非线性特征来处理非线性问题。
- en: Regularization consists in changing or tweaking the way models learn, usually
    in changing the objective/cost function, in order to keep their complexity not
    to high.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 正则化的目的是改变或调整模型的学习方式，通常是通过改变目标/代价函数，以保持模型复杂度不过高。
- en: 'Mathematically, it is often implemented by adding a term in the cost function
    of the problem. For example one of the simplest example of regularization is that
    of linear regression, in which case it is called a “Ridge regression”. The classic
    linear regression cost function is given by the mean (or sum) of the squared errors:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 从数学上讲，它通常是通过在问题的代价函数中添加一个项来实现的。例如，正则化的一个最简单的例子是线性回归，在这种情况下称为“岭回归”。经典的线性回归代价函数由平方误差的均值（或总和）给出：
- en: '![](../Images/f720666faaedd814ba8f2448d761adf8.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f720666faaedd814ba8f2448d761adf8.png)'
- en: 'With regularization, the cost function includes an additional term, that consists
    in the L2 norm of the vector beta:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 使用正则化时，代价函数包含一个附加项，即向量 beta 的 L2 范数：
- en: '![](../Images/c1537eaa63ad21eaca224d146114108f.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c1537eaa63ad21eaca224d146114108f.png)'
- en: The norm of the coefficient vector is weighted by the alpha hyperparameter,
    so that we can modify by how much its norm should impute of the final solution.
    This way, in the optimization/learning process, the coefficients of beta won’t
    go arbitrary large and instead a good balance between its norm and the errors
    will be found.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 系数向量的范数由 alpha 超参数加权，以便我们可以修改其范数在最终解决方案中应有的影响程度。这样，在优化/学习过程中，beta 的系数不会变得过于庞大，而是会找到范数和误差之间的良好平衡。
- en: This concept can be applied to pretty much any other models, including logistic
    regression of course.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 这个概念可以应用于几乎任何其他模型，包括逻辑回归。
- en: 'But let’s go a bit further regarding regularization: just like we saw how it
    is important to tune hyperparameters in a pipeline/model, the alpha parameter
    of the ridge regression should be optimized (and this applies to any regularization
    parameter).'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 关于正则化，我们可以进一步讨论：就像我们看到的在管道/模型中调整超参数的重要性一样，岭回归的 alpha 参数也应该被优化（这适用于任何正则化参数）。
- en: To do so we can a GridSearchCV or RandomSearchCV as seen in the previous module,
    but since it is so common to optimize the alpha parameter of a Ridge regression,
    sklearn provide a RidgeCV model that take a list of alpha values to test and select
    using cross-validation.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 为此，我们可以使用 GridSearchCV 或 RandomSearchCV，正如在之前的模块中所见，但由于优化岭回归的 alpha 参数非常常见，sklearn
    提供了一个 RidgeCV 模型，它接受一个 alpha 值列表进行测试，并使用交叉验证进行选择。
- en: 'So let’s sum up the 4 approaches to handle regularization for a linear regression:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，让我们总结一下处理线性回归正则化的 4 种方法：
- en: No regularization, using LinearRegression()
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 不使用正则化，使用 LinearRegression()
- en: Standard, non-optimized ridge regression, using Ridge(), equivalent to alpha=1
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 标准的未优化岭回归，使用 Ridge()，相当于 alpha=1
- en: Optimized ridge regression using GridSearch or RandomSearch
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 GridSearch 或 RandomSearch 优化的岭回归
- en: Ridge regression with builtin optimization with RidgeCV
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 带有内置优化的岭回归，使用 RidgeCV
- en: Let’s see visually how regularization influences the coefficients of linear
    regression, hence using a ridge model. In the following example, we use a 5 degree
    polynomial feature expansion to do a linear regression, with a regularization
    term through the Ridge model and its alpha hyperparameter to control the strengh
    of regularization.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过视觉化的方式来看正则化如何影响线性回归的系数，因此使用岭回归模型。在以下示例中，我们使用 5 次多项式特征扩展进行线性回归，通过 Ridge
    模型的正则化项及其 alpha 超参数来控制正则化的强度。
- en: '**With alpha=0**, there is no regularization, we get the classic linear regression
    results. Since we use up to degree 5 features to regress a noisy linear relation,
    the model tends to overfit a bit, and the linear coefficients span a great interval
    (here between -1500 to +1500).'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '**使用 alpha=0** 时，没有正则化，我们得到经典的线性回归结果。由于我们使用高达 5 次特征来回归一个有噪声的线性关系，模型往往会有些过拟合，线性系数的范围很大（这里在
    -1500 到 +1500 之间）。'
- en: '**With alpha=1**, we get “mild” regularization. The model overfits way less
    and the coefficients amplitudes are quite smaller.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '**使用 alpha=1** 时，我们得到“轻微”正则化。模型过拟合的程度明显减少，系数的幅度也小了很多。'
- en: '**With alpha=100**, we get very hard regularization, so the coefficients are
    not allowed to grow a lot and the model tends to underfit.'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '**使用 alpha=100** 时，我们得到非常强的正则化，因此系数不会增长很多，模型往往会出现欠拟合。'
- en: '[PRE5]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '![](../Images/7bf1d1f00b3e02d1817902ba37ae5923.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7bf1d1f00b3e02d1817902ba37ae5923.png)'
- en: 'Let’s go a bit further and inspect how the train score and test score evolve
    with the value of alpha:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们进一步探讨alpha值如何影响训练分数和测试分数的变化：
- en: '[PRE6]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '![](../Images/7c4f8a6d9dbb0f9f923e42481fe67548.png)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7c4f8a6d9dbb0f9f923e42481fe67548.png)'
- en: For a degree 10 polynomial, the optimum regularization coefficient alpha seems
    to be between 0.01 and 1.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 对于一个10次多项式，最佳正则化系数alpha似乎在0.01到1之间。
- en: Finally, remember that just like creating new feature to handle non-linearity
    can be applied to pretty much any model, regularization also can be included in
    most models (including logistic regression with its C parameter).
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，请记住，就像创建新特征来处理非线性可以应用于几乎所有模型一样，正则化也可以包含在大多数模型中（包括带有其C参数的逻辑回归）。
- en: Wrapup
  id: totrans-105
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: 'In this 4th post, we saw:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 在这第4篇文章中，我们看到了：
- en: the most important linear models namely linear regression and logistic regression
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最重要的线性模型，即线性回归和逻辑回归
- en: how we can handle non-linear problems while using such linear models by creating
    new features. This can be done for example by creating polynomial feature
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何通过创建新特征来处理非线性问题，例如创建多项式特征
- en: how regularization can help control the complexity of models by adding a regularization
    term in the objective function, such that the linear coefficients cannot go arbitrarily
    large
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何通过在目标函数中添加正则化项来控制模型的复杂度，以使线性系数不能任意增大
- en: 'You might like some of my other posts, make sure to check them out:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会喜欢我的一些其他文章，确保查看一下：
- en: '![Yoann Mocquin](../Images/234a99f243ff3c70fd90170ddde8659d.png)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![Yoann Mocquin](../Images/234a99f243ff3c70fd90170ddde8659d.png)'
- en: '[Yoann Mocquin](https://mocquin.medium.com/?source=post_page-----1e1a50e5247d--------------------------------)'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '[Yoann Mocquin](https://mocquin.medium.com/?source=post_page-----1e1a50e5247d--------------------------------)'
- en: Fourier-transforms for time-series
  id: totrans-113
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 时序的傅里叶变换
- en: '[View list](https://mocquin.medium.com/list/fouriertransforms-for-timeseries-ed423e3f38ad?source=post_page-----1e1a50e5247d--------------------------------)4
    stories![](../Images/86efd63d329650eb9b6d7c33625d6884.png)![](../Images/c693e4e596df5c1a8ef1b0fb3777d7ac.png)![](../Images/b6bc5330fb2d92bc3aad36f5bbc950da.png)![Yoann
    Mocquin](../Images/234a99f243ff3c70fd90170ddde8659d.png)'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '[查看列表](https://mocquin.medium.com/list/fouriertransforms-for-timeseries-ed423e3f38ad?source=post_page-----1e1a50e5247d--------------------------------)4个故事![](../Images/86efd63d329650eb9b6d7c33625d6884.png)![](../Images/c693e4e596df5c1a8ef1b0fb3777d7ac.png)![](../Images/b6bc5330fb2d92bc3aad36f5bbc950da.png)![Yoann
    Mocquin](../Images/234a99f243ff3c70fd90170ddde8659d.png)'
- en: '[Yoann Mocquin](https://mocquin.medium.com/?source=post_page-----1e1a50e5247d--------------------------------)'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '[Yoann Mocquin](https://mocquin.medium.com/?source=post_page-----1e1a50e5247d--------------------------------)'
- en: Scientific/numerical python
  id: totrans-116
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 科学/数值Python
- en: '[View list](https://mocquin.medium.com/list/scientificnumerical-python-9ce115122ab6?source=post_page-----1e1a50e5247d--------------------------------)3
    stories![Ironicaly, an array of containers](../Images/4ecd0326a3efdda93947f60872018d41.png)![](../Images/f11076a724463f7b11d819d95bcf0ea4.png)![](../Images/e340b22f444d2bd311537341cf1a105a.png)![Yoann
    Mocquin](../Images/234a99f243ff3c70fd90170ddde8659d.png)'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '[查看列表](https://mocquin.medium.com/list/scientificnumerical-python-9ce115122ab6?source=post_page-----1e1a50e5247d--------------------------------)3个故事![具有讽刺意味的容器数组](../Images/4ecd0326a3efdda93947f60872018d41.png)![](../Images/f11076a724463f7b11d819d95bcf0ea4.png)![](../Images/e340b22f444d2bd311537341cf1a105a.png)![Yoann
    Mocquin](../Images/234a99f243ff3c70fd90170ddde8659d.png)'
- en: '[Yoann Mocquin](https://mocquin.medium.com/?source=post_page-----1e1a50e5247d--------------------------------)'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '[Yoann Mocquin](https://mocquin.medium.com/?source=post_page-----1e1a50e5247d--------------------------------)'
- en: Sklearn tutorial
  id: totrans-119
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Sklearn教程
- en: '[View list](https://mocquin.medium.com/list/sklearn-tutorial-2e46a0e06b39?source=post_page-----1e1a50e5247d--------------------------------)9
    stories![](../Images/4ffe6868fb22c241a959bd5d5a9fd5d7.png)![](../Images/8aa32b00faa0ef7376e121ba9c9ffdb7.png)![](../Images/9f986423d7983bc08fc2073534603c35.png)![Yoann
    Mocquin](../Images/234a99f243ff3c70fd90170ddde8659d.png)'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '[查看列表](https://mocquin.medium.com/list/sklearn-tutorial-2e46a0e06b39?source=post_page-----1e1a50e5247d--------------------------------)9个故事![](../Images/4ffe6868fb22c241a959bd5d5a9fd5d7.png)![](../Images/8aa32b00faa0ef7376e121ba9c9ffdb7.png)![](../Images/9f986423d7983bc08fc2073534603c35.png)![Yoann
    Mocquin](../Images/234a99f243ff3c70fd90170ddde8659d.png)'
- en: '[Yoann Mocquin](https://mocquin.medium.com/?source=post_page-----1e1a50e5247d--------------------------------)'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '[Yoann Mocquin](https://mocquin.medium.com/?source=post_page-----1e1a50e5247d--------------------------------)'
- en: Data science and Machine Learning
  id: totrans-122
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据科学与机器学习
- en: '[View list](https://mocquin.medium.com/list/data-science-and-machine-learning-ba3fb2206051?source=post_page-----1e1a50e5247d--------------------------------)3
    stories![](../Images/c078e74fd67e0141c2b54b82823c78d4.png)![](../Images/79988eda04a078da9373f03d7db51c51.png)![](../Images/6a5966e529bf4ba9b16c592fec7b591a.png)'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '[查看列表](https://mocquin.medium.com/list/data-science-and-machine-learning-ba3fb2206051?source=post_page-----1e1a50e5247d--------------------------------)3
    个故事！[](../Images/c078e74fd67e0141c2b54b82823c78d4.png)![](../Images/79988eda04a078da9373f03d7db51c51.png)![](../Images/6a5966e529bf4ba9b16c592fec7b591a.png)'
