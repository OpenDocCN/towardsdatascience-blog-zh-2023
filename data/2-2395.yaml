- en: Word2Vec, GloVe, and FastText, Explained
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 解释 Word2Vec、GloVe 和 FastText
- en: 原文：[https://towardsdatascience.com/word2vec-glove-and-fasttext-explained-215a5cd4c06f](https://towardsdatascience.com/word2vec-glove-and-fasttext-explained-215a5cd4c06f)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/word2vec-glove-and-fasttext-explained-215a5cd4c06f](https://towardsdatascience.com/word2vec-glove-and-fasttext-explained-215a5cd4c06f)
- en: How computers understand words
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 计算机如何理解单词
- en: '[](https://medium.com/@dataemporium?source=post_page-----215a5cd4c06f--------------------------------)[![Ajay
    Halthor](../Images/1be821c8d8ed336b9ecedcf94f960ede.png)](https://medium.com/@dataemporium?source=post_page-----215a5cd4c06f--------------------------------)[](https://towardsdatascience.com/?source=post_page-----215a5cd4c06f--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----215a5cd4c06f--------------------------------)
    [Ajay Halthor](https://medium.com/@dataemporium?source=post_page-----215a5cd4c06f--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@dataemporium?source=post_page-----215a5cd4c06f--------------------------------)[![Ajay
    Halthor](../Images/1be821c8d8ed336b9ecedcf94f960ede.png)](https://medium.com/@dataemporium?source=post_page-----215a5cd4c06f--------------------------------)[](https://towardsdatascience.com/?source=post_page-----215a5cd4c06f--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----215a5cd4c06f--------------------------------)
    [Ajay Halthor](https://medium.com/@dataemporium?source=post_page-----215a5cd4c06f--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----215a5cd4c06f--------------------------------)
    ·10 min read·Jun 20, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----215a5cd4c06f--------------------------------)
    ·10 min 阅读·2023年6月20日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/9c0e7688b42e45235291764ce07ad479.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9c0e7688b42e45235291764ce07ad479.png)'
- en: Photo by [Growtika](https://unsplash.com/@growtika?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[Growtika](https://unsplash.com/@growtika?utm_source=medium&utm_medium=referral)
    在 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral) 上的照片'
- en: Computers don’t understand words like we do. They prefer to work with numbers.
    So, to help computers understand words and their meanings, we use something called
    embeddings. These embeddings numerically represent words as mathematical vectors.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 计算机不像我们那样理解单词。它们更喜欢处理数字。因此，为了帮助计算机理解单词及其含义，我们使用一种叫做嵌入的技术。这些嵌入以数学向量的形式数值化地表示单词。
- en: The cool thing about these embeddings is that if we learn them properly, words
    that have similar meanings will have similar numeric values. In other words, their
    numbers will be closer to each other. This allows computers to grasp the connections
    and similarities between different words based on their numeric representations.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 这些嵌入的酷炫之处在于，如果我们正确学习它们，那么具有相似含义的单词将具有相似的数值。换句话说，它们的数值会更接近。这使得计算机能够基于数值表示把握不同单词之间的联系和相似性。
- en: One prominent method for learning word embeddings is Word2Vec. In this article,
    we will delve into the intricacies of Word2Vec and explore its various architectures
    and variants.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 一种学习单词嵌入的显著方法是 Word2Vec。在这篇文章中，我们将深入探讨 Word2Vec 的复杂性，并探索其各种架构和变体。
- en: Word2Vec
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Word2Vec
- en: '![](../Images/729bbfa94374af6bfcda2475c9a71f4f.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/729bbfa94374af6bfcda2475c9a71f4f.png)'
- en: 'Figure 1: Word2Vec architectures ([Source](https://arxiv.org/abs/1301.3781))'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '图 1: Word2Vec 架构 ([来源](https://arxiv.org/abs/1301.3781))'
- en: In the early days, sentences were represented with n-gram vectors. These vectors
    aimed to capture the essence of a sentence by considering sequences of words.
    However, they had some limitations. N-gram vectors were often large and sparse,
    which made them computationally challenging to create. This created a problem
    known as the [curse of dimensionality](https://youtu.be/L9eNxU-9jBQ). Essentially,
    it meant that in high-dimensional spaces, the vectors representing words were
    so far apart that it became difficult to determine which words were truly similar.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在早期，句子通过 n-gram 向量进行表示。这些向量旨在通过考虑单词序列来捕捉句子的本质。然而，它们存在一些局限性。n-gram 向量通常非常大且稀疏，这使得它们在计算上很具挑战性。这造成了一个被称为[维度诅咒](https://youtu.be/L9eNxU-9jBQ)的问题。本质上，这意味着在高维空间中，表示单词的向量距离过远，从而很难确定哪些单词真正相似。
- en: Then, in 2003, a remarkable breakthrough occurred with the introduction of a
    [neural probabilistic language model](https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf).
    This model completely changed how we represent words by using something called
    continuous dense vectors. Unlike n-gram vectors, which were discrete and sparse,
    these dense vectors offered a continuous representation. Even small changes to
    these vectors resulted in meaningful representations, although they might not
    directly correspond to specific English words.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，在2003年，随着[神经概率语言模型](https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf)的引入，出现了一个显著的突破。这个模型通过使用所谓的连续密集向量完全改变了我们对词语的表示方式。与离散且稀疏的n-gram向量不同，这些密集向量提供了连续的表示。即使这些向量发生小的变化，也会产生有意义的表示，尽管它们可能不直接对应特定的英语单词。
- en: 'Building upon this exciting progress, the [Word2Vec](https://arxiv.org/abs/1301.3781)
    framework emerged in 2013\. It presented a powerful method for encoding word meanings
    into continuous dense vectors. Within Word2Vec, two primary architectures were
    introduced: Continuous Bag of Words (CBoW) and Skip-gram.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一激动人心的进展基础上，[Word2Vec](https://arxiv.org/abs/1301.3781)框架于2013年出现。它提供了一种将词义编码为连续密集向量的强大方法。在Word2Vec中，引入了两种主要架构：连续词袋模型（CBoW）和Skip-gram。
- en: These architectures opened doors to efficient training models capable of generating
    high-quality word embeddings. By leveraging vast amounts of text data, Word2Vec
    brought words to life in the numeric world. This enabled computers to understand
    the contextual meanings and relationships between words, offering a transformative
    approach to natural language processing.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 这些架构为生成高质量的词嵌入的高效训练模型打开了大门。通过利用大量的文本数据，Word2Vec使单词在数字世界中栩栩如生。这使计算机能够理解单词之间的上下文含义和关系，为自然语言处理提供了一种变革性的方法。
- en: Continuous Bag-of-Words (CBoW)
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 连续词袋模型（CBoW）
- en: '![](../Images/3a5759208ec2037fc5226c4fc57089fa.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3a5759208ec2037fc5226c4fc57089fa.png)'
- en: 'Figure 2: CBoW training illustration (image by author)'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：CBoW训练示意图（作者提供的图片）
- en: 'In this section and the next, let’s understand how CBoW and skip-gram models
    are trained using a small vocabulary of five words: biggest, ever, lie, told,
    and the. And we have an example sentence “The biggest lie ever told”. How would
    we pass this into the CBoW architecture? This is shown in *Figure 2* above, but
    we will describe the process as well.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节及下一节中，让我们了解如何使用一个包含五个单词的小词汇表训练CBoW和skip-gram模型：biggest、ever、lie、told和the。我们有一个示例句子“The
    biggest lie ever told”。我们将如何将其传递给CBoW架构？这在*图2*中显示，但我们也会描述这个过程。
- en: Suppose we set the context window size to 2\. We take the words “The,” “biggest,”
    “ever,” and “told” and convert them into 5x1 one-hot vectors.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们将上下文窗口大小设置为2。我们取“the”、“biggest”、“ever”和“told”这几个词，并将它们转换为5x1的独热向量。
- en: These vectors are then passed as input to the model and mapped to a projection
    layer. Let’s say this projection layer has a size of 3\. Each word’s vector is
    multiplied by a 5x3 weight matrix (shared across inputs), resulting in four 3x1
    vectors. Taking the average of these vectors gives us a single 3x1 vector. This
    vector is then projected back to a 5x1 vector using another 3x5 weight matrix.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 然后将这些向量作为输入传递到模型，并映射到一个投影层。假设这个投影层的大小为3。每个词的向量乘以一个5x3的权重矩阵（在输入间共享），得到四个3x1的向量。取这些向量的平均值，得到一个3x1的向量。然后使用另一个3x5的权重矩阵将这个向量投影回5x1的向量。
- en: This final vector represents the middle word “lie.” By calculating the true
    one hot vector and the actual output vector, we get a loss that is used to update
    the network’s weights through backpropagation.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 这个最终向量代表了中间单词“lie”。通过计算真实的独热向量和实际的输出向量，我们得到一个损失，用于通过反向传播更新网络的权重。
- en: We repeat this process by sliding the context window and then applying it to
    thousands of sentences. After training is complete, the first layer of the model,
    with dimensions 5x3 (vocabulary size x projection size), contains the learned
    parameters. These parameters are used as a lookup table to map each word to its
    corresponding vector representation.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过滑动上下文窗口并将其应用于成千上万的句子来重复这个过程。训练完成后，模型的第一层，尺寸为5x3（词汇表大小x投影大小），包含了学习到的参数。这些参数用作查找表，将每个单词映射到其对应的向量表示。
- en: Skip-gram
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Skip-gram
- en: '![](../Images/ab60b70cd449ef5a0fe26769c85284ef.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ab60b70cd449ef5a0fe26769c85284ef.png)'
- en: 'Figure 3: Skip-gram training illustration (image by author)'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：Skip-gram训练示意图（作者提供的图片）
- en: In the skip-gram model, we use a similar architecture as the continuous bag-of-words
    (CBoW) case. However, instead of predicting the target word based on its surrounding
    words, we flip the scenario as shwon in *Figure 3*. Now, the word “lie” becomes
    the input, and we aim to predict its context words. The name “skip-gram” reflects
    this approach, as we predict context words that may “skip” over a few words.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在skip-gram模型中，我们使用了类似于连续词袋（CBoW）模型的架构。然而，与其根据周围词预测目标词相反，我们将场景反转，如*图3*所示。现在，词“lie”成为输入，我们的目标是预测其上下文词。名称“skip-gram”反映了这种方法，因为我们预测的上下文词可能会“跳过”几个词。
- en: 'To illustrate this, let’s consider some examples:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明这一点，我们来看一些例子：
- en: The input word “lie” is paired with the output word “the.”
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输入词“lie”与输出词“the”配对。
- en: The input word “lie” is paired with the output word “biggest.”
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输入词“lie”与输出词“biggest”配对。
- en: The input word “lie” is paired with the output word “ever.”
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输入词“lie”与输出词“ever”配对。
- en: The input word “lie” is paired with the output word “told.”
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输入词“lie”与输出词“told”配对。
- en: We repeat this process for all the words in the training data. Once the training
    is complete, the parameters of the first layer, with dimensions of vocabulary
    size x projection size, capture the relationships between input words and their
    corresponding vector representations. These learned parameters allow us to map
    an input word to its respective vector representation in the skip-gram model.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对训练数据中的所有词汇重复这一过程。一旦训练完成，第一层的参数，其维度为词汇大小 x 投影大小，将捕捉输入词与其对应向量表示之间的关系。这些学习到的参数使我们能够在skip-gram模型中将输入词映射到其相应的向量表示。
- en: Pros
  id: totrans-36
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 优点
- en: '**Overcomes the curse of dimensionality with simplicity**: Word2Vec provides
    a straightforward and efficient solution to the curse of dimensionality. By representing
    words as dense vectors, it reduces the sparsity and computational complexity associated
    with traditional methods like n-gram vectors.'
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**以简洁的方式克服维度灾难**：Word2Vec提供了一个直接而高效的解决方案来应对维度灾难。通过将词表示为密集向量，它减少了与传统方法（如n-gram向量）相关的稀疏性和计算复杂度。'
- en: '**Generates vectors such that words closer in meaning have closer vector values**:
    Word2Vec’s embeddings exhibit a valuable property where words with similar meanings
    are represented by vectors that are closer in numerical value. This allows for
    capturing semantic relationships and performing tasks like word similarity and
    analogy detection.'
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**生成向量，使得意义相近的词具有更接近的向量值**：Word2Vec的嵌入展现了一个有价值的特性，即具有相似意义的词由数值更接近的向量表示。这允许捕捉语义关系并执行诸如词语相似性和类比检测等任务。'
- en: '**Pretrained embeddings for various NLP applications**: Word2Vec’s pretrained
    embeddings are widely available and can be utilized in a range of natural language
    processing (NLP) applications. These embeddings, trained on large corpora, provide
    a valuable resource for tasks like sentiment analysis, named entity recognition,
    machine translation, and more.'
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**预训练的嵌入用于各种NLP应用**：Word2Vec的预训练嵌入广泛可用，并可以用于各种自然语言处理（NLP）应用。这些嵌入经过大规模语料库的训练，为情感分析、命名实体识别、机器翻译等任务提供了宝贵的资源。'
- en: '**Self-supervised framework for data augmentation and training**: Word2Vec
    operates in a self-supervised manner, leveraging the existing data to learn word
    representations. This makes it easy to gather more data and train the model, as
    it does not require extensive labeled datasets. The framework can be applied to
    large amounts of unlabeled text, enhancing the training process.'
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**自监督框架用于数据增强和训练**：Word2Vec以自监督的方式运行，利用现有数据来学习词表示。这使得收集更多数据和训练模型变得容易，因为它不需要大量标记数据集。该框架可以应用于大量未标记的文本，从而增强训练过程。'
- en: Cons
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 缺点
- en: '**Limited preservation of global information**: Word2Vec’s embeddings focus
    primarily on capturing local context information and may not preserve global relationships
    between words. This limitation can impact tasks that require a broader understanding
    of text, such as document classification or sentiment analysis at the document
    level.'
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**对全局信息的有限保留**：Word2Vec的嵌入主要关注捕捉局部上下文信息，可能无法保留词之间的全局关系。这一限制可能会影响需要更广泛理解文本的任务，例如文档分类或文档级别的情感分析。'
- en: '**Less suitable for morphologically rich languages**: Morphologically rich
    languages, characterized by complex word forms and inflections, may pose challenges
    for Word2Vec. Since Word2Vec treats each word as an atomic unit, it may struggle
    to capture the rich morphology and semantic nuances present in such languages.'
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**不太适合形态丰富的语言**：形态丰富的语言特征是复杂的单词形式和词形变化，这可能对Word2Vec造成挑战。由于Word2Vec将每个单词视为原子单位，它可能难以捕捉此类语言中的丰富形态学和语义细微差别。'
- en: '**Lack of broad context awareness**: Word2Vec models consider only a local
    context window of words surrounding the target word during training. This limited
    context awareness may result in incomplete understanding of word meanings in certain
    contexts. It may struggle to capture long-range dependencies and intricate semantic
    relationships present in certain language phenomena.'
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**缺乏广泛的上下文意识**：Word2Vec模型在训练过程中只考虑了目标单词周围的局部上下文窗口。这种有限的上下文意识可能导致在某些语境中对单词意义的理解不完整。它可能难以捕捉某些语言现象中存在的长期依赖关系和复杂的语义关系。'
- en: In the following sections, we will see some word embedding architectures that
    help address these cons.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的部分中，我们将看到一些能够解决这些缺点的词嵌入架构。
- en: 'GloVe: Global Vectors'
  id: totrans-46
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 'GloVe: 全球向量'
- en: Word2Vec methods have been successful in capturing local context to a certain
    extent, but they do not take full advantage of the global context available in
    the corpus. Global context refers to using multiple sentences across the corpus
    to gather information. This is where [GloVe](https://nlp.stanford.edu/pubs/glove.pdf)
    comes in, as it leverages word-word co-occurrence for learning word embeddings.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: Word2Vec方法在一定程度上成功地捕捉了局部上下文，但没有充分利用语料库中可用的全局上下文。全局上下文指的是使用语料库中的多个句子来收集信息。这就是[GloVe](https://nlp.stanford.edu/pubs/glove.pdf)发挥作用的地方，因为它利用词-词共现来学习词嵌入。
- en: The concept of a word-word co-occurrence matrix is key to Glove. It is a matrix
    that captures the occurrences of each word in the context of every other word
    in the corpus. Each cell in the matrix represents the count of occurrences of
    one word in the context of another word.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 词-词共现矩阵的概念是GloVe的关键。它是一个矩阵，捕捉语料库中每个单词在其他每个单词上下文中的出现情况。矩阵中的每个单元格表示一个单词在另一个单词上下文中的出现次数。
- en: '![](../Images/6d83d27ac3a39f885d89711cc6b09add.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6d83d27ac3a39f885d89711cc6b09add.png)'
- en: 'Figure 4: Example of word co-occurrence probability ratio ([Source](https://nlp.stanford.edu/pubs/glove.pdf))'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：词共现概率比率的示例（[来源](https://nlp.stanford.edu/pubs/glove.pdf)）
- en: Instead of working directly with the probabilities of co-occurrence as in Word2Vec,
    Glove starts with the ratios of co-occurrence probabilities. In the context of
    *Figure 4*, P(*k* | *ice*) represents the probability of word k occurring in the
    context of the word “ice,” and P(*k* | *steam*) represents the probability of
    word k occurring in the context of the word “steam.” By comparing the ratio P(*k*
    | *ice*) / P(*k* | *steam*), we can determine the association of word k with either
    ice or steam. If the ratio is much greater than 1, it indicates a stronger association
    with ice. Conversely, if it is closer to 0, it suggests a stronger association
    with steam. A ratio closer to 1 implies no clear association with either ice or
    steam.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 与Word2Vec直接处理共现概率不同，GloVe从共现概率的比率开始。在*图4*的背景下，P(*k* | *ice*)表示单词k在“ice”一词上下文中的出现概率，而P(*k*
    | *steam*)表示单词k在“steam”一词上下文中的出现概率。通过比较比率P(*k* | *ice*) / P(*k* | *steam*)，我们可以确定单词k与冰或蒸汽的关联。如果比率远大于1，则表明与冰的关联更强。相反，如果接近0，则表明与蒸汽的关联更强。比率接近1则表明与冰或蒸汽没有明确的关联。
- en: For example, when k = “solid,” the probability ratio is much greater than 1,
    indicating a strong association with ice. On the other hand, when k = “gas,” the
    probability ratio is much closer to 0, suggesting a stronger association with
    steam. As for the words “water” and “fashion,” they do not exhibit a clear association
    with either ice or steam.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，当k = “solid”时，概率比率远大于1，表明与冰的关联很强。另一方面，当k = “gas”时，概率比率接近0，表明与蒸汽的关联更强。至于“water”和“fashion”这两个词，它们与冰或蒸汽都没有明确的关联。
- en: This association of words based on probability ratios is precisely what we aim
    to achieve. And this is optimized when learning embeddings with GloVe.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 基于概率比率的词语关联正是我们希望实现的目标。而在学习GloVe的嵌入时，这一点得到了优化。
- en: FastText
  id: totrans-54
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: FastText
- en: The traditional word2vec architectures, besides lacking the utilization of global
    information, do not effectively handle languages that are morphologically rich.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 传统的 word2vec 架构除了缺乏全球信息的利用外，也无法有效处理形态丰富的语言。
- en: So, what does it mean for a language to be morphologically rich? In such languages,
    a word can change its form based on the context in which it is used. Let’s take
    the example of a South Indian language called “Kannada.”
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，语言形态丰富意味着什么呢？在这种语言中，单词的形式可以根据其使用的上下文而变化。我们以一种名为“卡纳达语”的南印度语言为例。
- en: In Kannada, the word for “house” is written as ಮನೆ (mane). However, when we
    say “in the house,” it becomes ಮನೆಯಲ್ಲಿ (maneyalli), and when we say “from the
    house,” it changes to ಮನೆಯಿಂದ (maneyinda). As you can see, only the preposition
    changes, but the translated words have different forms. In English, they are all
    simply “house.” Consequently, traditional word2vec architectures would map all
    of these variations to the same vector. However, if we were to create a word2vec
    model for Kannada, which is morphologically rich, each of these three cases would
    be assigned different vectors. Moreover, the word “house” in Kannada can take
    on many more forms than just these three examples. Since our corpus may not contain
    all of these variations, the traditional word2vec training might not capture all
    the diverse word representations.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在卡纳达语中，“house”的单词是写作 ಮನೆ (mane)。然而，当我们说“在房子里”时，它变成了 ಮನೆಯಲ್ಲಿ (maneyalli)，而当我们说“从房子里”时，它则变为
    ಮನೆಯಿಂದ (maneyinda)。如你所见，只有介词发生变化，但翻译的词具有不同的形式。在英语中，这些都是简单的“house”。因此，传统的 word2vec
    架构会将所有这些变体映射到相同的向量。然而，如果我们为形态丰富的卡纳达语创建一个 word2vec 模型，那么这三种情况中的每一种将被分配不同的向量。此外，卡纳达语中的“house”可以有比这三个例子更多的形式。由于我们的语料库可能不包含所有这些变体，传统的
    word2vec 训练可能无法捕捉所有不同的词表示。
- en: To address this issue, [FastText](https://arxiv.org/pdf/1607.04606.pdf) introduces
    a solution by considering subword information when generating word vectors. Instead
    of treating each word as a whole, FastText breaks down words into character n-grams,
    ranging from tri-grams to 6-grams. These n-grams are then mapped to vectors, which
    are subsequently aggregated to represent the entire word. These aggregated vectors
    are then fed into a skip-gram architecture.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这个问题，[FastText](https://arxiv.org/pdf/1607.04606.pdf) 通过在生成词向量时考虑子词信息引入了一个解决方案。FastText
    不将每个单词作为一个整体，而是将单词分解为从三元组到六元组的字符 n-grams。这些 n-grams 被映射到向量上，然后这些向量被聚合以代表整个单词。聚合后的向量接着被输入到
    skip-gram 架构中。
- en: This approach allows for the recognition of shared characteristics among different
    word forms within a language. Even though we may not have seen every single form
    of a word in the corpus, the learned vectors capture the commonalities and similarities
    among these forms. Morphologically rich languages, such as Arabic, Turkish, Finnish,
    and various Indian languages, can benefit from FastText’s ability to generate
    word vectors that account for different forms and variations.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法能够识别语言中不同单词形式的共同特征。即使我们可能没有在语料库中见过单词的每一种形式，所学到的向量仍能捕捉这些形式之间的共性和相似性。形态丰富的语言，如阿拉伯语、土耳其语、芬兰语以及各种印度语言，可以受益于
    FastText 能生成考虑不同形式和变体的词向量。
- en: Context Awareness
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 上下文意识
- en: '![](../Images/f80a5224e891ff2e20d6e72f63d5af84.png)![](../Images/08a4fc5adbfe78f077365b4f26740dfa.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f80a5224e891ff2e20d6e72f63d5af84.png)![](../Images/08a4fc5adbfe78f077365b4f26740dfa.png)'
- en: 'Figure 5: ELMo and BERT ([Source](https://arxiv.org/abs/1810.04805))'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 图5：ELMo 和 BERT ([来源](https://arxiv.org/abs/1810.04805))
- en: 'Despite their advantages, the aforementioned word2vec architectures suffer
    from a limitation: they generate the same vector representation for a given word,
    regardless of its context.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管有其优点，前述的 word2vec 架构存在一个局限性：它们为给定的单词生成相同的向量表示，而不考虑其上下文。
- en: 'To illustrate this point, let’s consider the following two sentences:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明这一点，我们考虑以下两个句子：
- en: “That drag queen slays.”
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: “那个变装皇后真棒。”
- en: “She has an ace and queen for a perfect hand.”
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: “她有一张王牌和一张皇后，手牌完美。”
- en: In these sentences, the word “queen” has different meanings. However, in word2vec
    architectures, the vectors for “queen” in both cases would be the same. This is
    not ideal because we want word vectors to capture and represent different meanings
    based on their contexts.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些句子中，单词“queen”有不同的含义。然而，在 word2vec 架构中，这两种情况下“queen”的向量是相同的。这并不理想，因为我们希望词向量能够根据上下文捕捉和表示不同的含义。
- en: To address this issue, more advanced architectures such as [LSTM](https://youtu.be/QciIcRxJvsM)
    cells were introduced. These architectures were designed to incorporate contextual
    information into word representations. Over time, transformer-based models like
    [BERT](https://youtu.be/xI0HHN5XKDo) and [GPT](https://youtu.be/3IweGfgytgY) emerged,
    leading to the development of large-scale language models we see today. These
    models excel at considering context and generating word representations that are
    sensitive to the surrounding words and sentences.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这一问题，引入了更先进的架构，如[LSTM](https://youtu.be/QciIcRxJvsM)单元。这些架构被设计为将上下文信息纳入词语表示中。随着时间的推移，基于Transformer的模型如[BERT](https://youtu.be/xI0HHN5XKDo)和[GPT](https://youtu.be/3IweGfgytgY)的出现，推动了我们今天看到的大规模语言模型的发展。这些模型在考虑上下文和生成对周围词语和句子敏感的词语表示方面表现出色。
- en: By taking context into account, these advanced architectures enable the creation
    of more nuanced and meaningful word vectors, ensuring that the same word can have
    different vector representations depending on its specific context.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 通过考虑上下文，这些先进的架构使得生成更加细致和有意义的词向量成为可能，确保相同的词在不同的具体上下文中可以有不同的向量表示。
- en: Conclusion
  id: totrans-70
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: In conclusion, this blog post provided insights into the word2vec architecture
    and its ability to represent words using continuous dense vectors. Subsequent
    implementations such as GloVe capitalized on global context, while FastText enabled
    efficient learning of vectors for morphologically rich languages like Arabic,
    Finnish, and various Indian languages. However, a common drawback among these
    approaches is that they assign the same vector to a word regardless of its context
    during inference, which can hinder accurate representation of words with multiple
    meanings.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，本文提供了关于word2vec架构及其使用连续稠密向量表示词语的能力的见解。后续实现如GloVe利用了全局上下文，而FastText使得对形态丰富的语言如阿拉伯语、芬兰语和各种印度语言的向量学习变得高效。然而，这些方法的一个共同缺点是，在推理过程中，它们对一个词分配相同的向量，无论其上下文如何，这可能会阻碍对具有多重含义的词的准确表示。
- en: To address this limitation, subsequent advancements in NLP introduced LSTM cells
    and transformer architectures, which excel at capturing specific context and have
    become the foundation for modern large language models. These models have the
    capability to understand and generate word representations that vary based on
    their surrounding context, accommodating the nuanced meanings of words in different
    scenarios.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这一局限性，后续在自然语言处理（NLP）领域的进展引入了LSTM单元和Transformer架构，这些技术在捕捉特定上下文方面表现出色，并成为现代大型语言模型的基础。这些模型能够理解并生成根据周围上下文变化的词语表示，适应了不同场景中词语的细微含义。
- en: Nevertheless, it is important to acknowledge that the word2vec framework remains
    significant, as it continues to power numerous applications in the field of natural
    language processing. Its simplicity and ability to generate meaningful word embeddings
    have proven valuable, despite the challenges posed by contextual variations in
    word meanings.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，重要的是要承认word2vec框架依然具有重要意义，因为它继续推动着自然语言处理领域中的众多应用。尽管词义的上下文变化带来了挑战，其简单性和生成有意义的词嵌入的能力仍然被证明是有价值的。
- en: For more information on language models, [check out this YouTube playlist](https://www.youtube.com/playlist?list=PLTl9hO2Oobd_bzXUpzKMKA3liq2kj6LfE).
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 欲了解更多关于语言模型的信息，[查看这个YouTube播放列表](https://www.youtube.com/playlist?list=PLTl9hO2Oobd_bzXUpzKMKA3liq2kj6LfE)。
- en: '*Happy Learning!*'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '*快乐学习！*'
