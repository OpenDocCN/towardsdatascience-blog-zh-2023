- en: What are the fairness implications of encoding categorical protected attributes?
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 对编码分类受保护属性的公平性影响是什么？
- en: 原文：[https://towardsdatascience.com/what-are-the-fairness-implications-of-encoding-categorical-protected-attributes-2cc1dd0f5229?source=collection_archive---------5-----------------------#2023-05-06](https://towardsdatascience.com/what-are-the-fairness-implications-of-encoding-categorical-protected-attributes-2cc1dd0f5229?source=collection_archive---------5-----------------------#2023-05-06)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/what-are-the-fairness-implications-of-encoding-categorical-protected-attributes-2cc1dd0f5229?source=collection_archive---------5-----------------------#2023-05-06](https://towardsdatascience.com/what-are-the-fairness-implications-of-encoding-categorical-protected-attributes-2cc1dd0f5229?source=collection_archive---------5-----------------------#2023-05-06)
- en: Exploring the Impact of Encoding Protected Attributes on Fairness in ML
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 探讨编码受保护属性对机器学习公平性的影响
- en: '[](https://medium.com/@carmougan?source=post_page-----2cc1dd0f5229--------------------------------)[![Carlos
    Mougan](../Images/7a56269362fc48c7a6179474b106857a.png)](https://medium.com/@carmougan?source=post_page-----2cc1dd0f5229--------------------------------)[](https://towardsdatascience.com/?source=post_page-----2cc1dd0f5229--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----2cc1dd0f5229--------------------------------)
    [Carlos Mougan](https://medium.com/@carmougan?source=post_page-----2cc1dd0f5229--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@carmougan?source=post_page-----2cc1dd0f5229--------------------------------)[![Carlos
    Mougan](../Images/7a56269362fc48c7a6179474b106857a.png)](https://medium.com/@carmougan?source=post_page-----2cc1dd0f5229--------------------------------)[](https://towardsdatascience.com/?source=post_page-----2cc1dd0f5229--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----2cc1dd0f5229--------------------------------)
    [Carlos Mougan](https://medium.com/@carmougan?source=post_page-----2cc1dd0f5229--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fd5344df58d03&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-are-the-fairness-implications-of-encoding-categorical-protected-attributes-2cc1dd0f5229&user=Carlos+Mougan&userId=d5344df58d03&source=post_page-d5344df58d03----2cc1dd0f5229---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----2cc1dd0f5229--------------------------------)
    ·7 min read·May 6, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F2cc1dd0f5229&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-are-the-fairness-implications-of-encoding-categorical-protected-attributes-2cc1dd0f5229&user=Carlos+Mougan&userId=d5344df58d03&source=-----2cc1dd0f5229---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fd5344df58d03&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-are-the-fairness-implications-of-encoding-categorical-protected-attributes-2cc1dd0f5229&user=Carlos+Mougan&userId=d5344df58d03&source=post_page-d5344df58d03----2cc1dd0f5229---------------------post_header-----------)
    发表在[Towards Data Science](https://towardsdatascience.com/?source=post_page-----2cc1dd0f5229--------------------------------)
    ·7分钟阅读·2023年5月6日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F2cc1dd0f5229&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-are-the-fairness-implications-of-encoding-categorical-protected-attributes-2cc1dd0f5229&user=Carlos+Mougan&userId=d5344df58d03&source=-----2cc1dd0f5229---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F2cc1dd0f5229&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-are-the-fairness-implications-of-encoding-categorical-protected-attributes-2cc1dd0f5229&source=-----2cc1dd0f5229---------------------bookmark_footer-----------)![](../Images/b3e4278cc0dbc42f6be7eb77d99a9e0c.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F2cc1dd0f5229&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-are-the-fairness-implications-of-encoding-categorical-protected-attributes-2cc1dd0f5229&source=-----2cc1dd0f5229---------------------bookmark_footer-----------)![](../Images/b3e4278cc0dbc42f6be7eb77d99a9e0c.png)'
- en: Lady Justice at dawn from [Unsplash](https://www.istockphoto.com/es/foto/lady-justice-al-amanecer-gm1318486133-405603913)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 曙光中的公正女神来自[Unsplash](https://www.istockphoto.com/es/foto/lady-justice-al-amanecer-gm1318486133-405603913)
- en: We will explore the world of categorical attribute encoding and its implications
    for machine learning models in terms of accuracy and fairness. Categorical attributes,
    such as country of birth or ethnicity, play a crucial role in determining the
    presence of sensitive information in data. However, many machine learning algorithms
    struggle to directly process categorical attributes, necessitating the use of
    encoding methods to transform them into numerical features that the models can
    utilize. Furthermore, we study the implication of intersectional fairness engineering.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将探讨分类属性编码的世界及其对机器学习模型在准确性和公平性方面的影响。分类属性，如出生国家或族裔，在确定数据中敏感信息的存在方面起着关键作用。然而，许多机器学习算法难以直接处理分类属性，因此需要使用编码方法将其转换为模型可以利用的数值特征。此外，我们还研究了交叉公平性工程的影响。
- en: This blog contains a summary of our **AI, Ethics, and Society (AIES’23)** conference
    paper, **Fairness implications of encoding protected categorical attributes**,
    [[link](https://arxiv.org/abs/2201.11358)].
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 本博客包含了我们**AI、伦理和社会（AIES'23）**会议论文**编码保护分类属性的公平性影响**的总结，[[link](https://arxiv.org/abs/2201.11358)]。
- en: '***TLDR;*** *Encoding protected attributes improved model performance but also
    increases fairness violation. They can be reconciled by target encoding regularization.
    Engineering intersectional protected attribute (Gender-Race) increases performance
    but heavily increases fairness violations.*'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '***总结：*** *编码保护属性提高了模型性能，但也增加了公平性违背。这可以通过目标编码正则化来调和。工程化交叉保护属性（性别-种族）提高了性能，但大幅增加了公平性违背。*'
- en: Introduction
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 引言
- en: Sensitive attributes are central to fairness, as are their handling throughout
    the machine learning pipeline. Many machine learning algorithms require categorical
    attributes to be suitably encoded as numerical data before being fed to algorithms.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 敏感属性在公平性中至关重要，其处理贯穿整个机器学习流程。许多机器学习算法要求将分类属性适当地编码为数值数据，然后再输入到算法中。
- en: '***What are the implications of encoding categorical protected attributes?***'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '***编码分类保护属性的影响是什么？***'
- en: '**Types of Induced Bias**'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '**诱导偏差的类型**'
- en: 'We investigate two types of biases that can arise from these encoding methods:
    irreducible bias and reducible bias:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 我们研究了这些编码方法可能引发的两种偏差：不可减少的偏差和可减少的偏差：
- en: '**Irreducible bias:** It refers to (direct) group discrimination arising from
    the categorization of groups into labels: more data about the compared groups
    do not reduce this type of bias. In the COMPAS dataset, criminal ethnicity was
    paramount when determining recidivism scores; the numerical encoding of large
    ethnicity groups such as *African-Americans* or *Caucasians* may lead to discrimination,
    which is an unfair effect coming from the irreducible bias.'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**不可减少的偏差：** 这是指由于将群体分类到标签中而产生的（直接）群体歧视：关于比较群体的更多数据无法减少这种类型的偏差。在 COMPAS 数据集中，犯罪分子的族裔在确定再犯评分时至关重要；对像*非洲裔美国人*或*白人*这样的大族裔群体的数值编码可能导致歧视，这是一种源于不可减少偏差的不公平效应。'
- en: '**Reducible bias:** arises due to the variance when encoding groups with a
    small statistical representation, sometimes evenvery few instances of the group.
    Reducible bias can be found and introduced when encoding the ethnicity category
    *Arabic*, which is rarely represented in the data, provoking a large sampling
    variance that ends in an almost random and unrealistic encoding.'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可减少的偏差：** 由于对具有小统计代表性的群体进行编码时的方差而产生，有时甚至只有非常少的实例。当对族裔类别*阿拉伯裔*进行编码时，常常会引发较大的采样方差，最终导致几乎是随机和不现实的编码，从而发现和引入可减少的偏差。'
- en: '**Encoding methods**'
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**编码方法**'
- en: '**One hot encoding:** This is the most established encoding method for categorical
    features and is also the default method within the fairness literature.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '**独热编码：** 这是分类特征中最成熟的编码方法，也是公平性文献中的默认方法。'
- en: '![](../Images/bab53dcac4ac9357a9736db6654c298c.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bab53dcac4ac9357a9736db6654c298c.png)'
- en: One Hot Encoding over illustrative example. Image by author
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: One Hot Encoding 的示例。图片来源：作者
- en: '**Target Encoding:** categorical features are replaced with the mean target
    value of each respective category. This technique handles high cardinality categorical
    data, and categories are ordered. The main drawback of target encoding appears
    when categories with few samples are replaced by values close to the desired target.
    This introduces bias to the model as it over-trusts the target encoded feature
    and makes it prone to overfitting and reducible bias.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**目标编码：** 分类特征被替换为各自类别的均值目标值。这种技术处理高基数分类数据，并对类别进行排序。目标编码的主要缺点出现在样本较少的类别被替换为接近期望目标的值时。这会引入模型偏差，因为它过度信任目标编码特征，并使其容易过拟合和减少偏差。'
- en: Furthermore, this type of encoding allows for regularization. In this paper,
    we study two types of regularization (in the blog, we only study Gaussian)
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，这种类型的编码允许正则化。在本文中，我们研究了两种类型的正则化（在博客中我们只研究了高斯正则化）
- en: '![](../Images/a0d53d7dcb08e053827801e160c5d4c5.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a0d53d7dcb08e053827801e160c5d4c5.png)'
- en: Target Encoding over illustrative example. Image by author
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 目标编码示例说明。作者提供的图像
- en: Experiments
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实验
- en: For predictive performance, we use AUC and fairness metrics. Between the reference
    group (r) and the group, we want to compare (i). Z is the protected attribute,
    and \hat{Y} is the model prediction.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 对于预测性能，我们使用AUC和公平性指标。参考组（r）与我们想要比较的组（i）之间。Z是受保护的属性，\hat{Y}是模型预测。
- en: Demographic Parity
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 人口统计学平等
- en: '![](../Images/641e5d5a310c41356bac49d93c2bbb2c.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/641e5d5a310c41356bac49d93c2bbb2c.png)'
- en: The difference between favorable outcomes received by the unprivileged group
    and privileged group
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 不受特权组与受特权组获得的有利结果之间的差异
- en: Equal Opportunity Fairness
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 平等机会公平性
- en: '![](../Images/ac4c65829d26fc3c5bd1c57fc0cf8f3a.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ac4c65829d26fc3c5bd1c57fc0cf8f3a.png)'
- en: Ensuring fair opportunity instead of raw outcomes. The value is the difference
    in the
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 确保公平机会而非原始结果。该值是
- en: True Positive Rate (TPR) between the protected group and
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 受保护组与
- en: the reference group
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 参考组
- en: Average Absolute Odds
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 平均绝对赔率
- en: '![](../Images/4e06f1f4f8272733447eae1d2ca29488.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4e06f1f4f8272733447eae1d2ca29488.png)'
- en: The sum of the absolute differences between the True Positive Rates and the
    False Positive Rates of the unprivileged group plus the same ratio for the privileged
    group
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 不受特权组与受特权组的真实正例率和假正例率之间绝对差异的总和
- en: '**Experiment Hypothesis**'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '**实验假设**'
- en: Encoding the protected attribute
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 对受保护属性的编码
- en: Improves performance
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 提高性能
- en: Increases fairness violations
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 增加了公平性违反
- en: Performance and fairness can be reconciled with target encoding regularization.
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 性能和公平性可以通过目标编码正则化来调和。
- en: '![](../Images/b618cc20c6b8c700b2e9dac280d95de9.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b618cc20c6b8c700b2e9dac280d95de9.png)'
- en: 'Figure: Comparing OHE and target encoding regularization (Gaussian noise) for
    the Logistic Regression over COMPAS dataset. Red dots regard different regularization
    parameters: the darker the red the higher the regularization. The blue dot regards
    the one-hot encoding. Image by author.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 图：比较OHE和目标编码正则化（高斯噪声）在COMPAS数据集上的逻辑回归。红点表示不同的正则化参数：红色越深，正则化越高。蓝点表示独热编码。作者提供的图像。
- en: '![](../Images/a7f34b2884ac148042ee7ae252702788.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a7f34b2884ac148042ee7ae252702788.png)'
- en: Impact of the Gaussian noise regularization parameter $\lambda$ on performance
    and fairness metrics over the test set of the COMPAS dataset using a Logistic
    Regression with L1 penalty. In the left image, the AUC of all the protected groups
    is over the regularization hyperparameter. On the right, the equal opportunity
    fairness, demographic parity, and average absolute oods variation throughout the
    regularization hyperparameter. Image by author
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 高斯噪声正则化参数$\lambda$对COMPAS数据集测试集上的性能和公平性指标的影响，使用L1惩罚的逻辑回归。在左侧图像中，所有受保护组的AUC都超过了正则化超参数。在右侧，平等机会公平性、人口统计学平等和平均绝对赔率在正则化超参数中的变化。作者提供的图像
- en: In the experiment, we showed that the most used categorical encoding method
    in the fair machine learning literature, one-hot encoding, discriminates more
    regarding equal opportunity fairness than target encoding. However, target encoding
    shows promising results. Target encoding using Gaussian regularization shows improvements
    under the presence of both types of biases, with the risk of a noticeable loss
    of model performance in the case of over-parametrization.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在实验中，我们展示了公平机器学习文献中最常用的分类编码方法——独热编码，在平等机会公平性方面比目标编码更具歧视性。然而，目标编码表现出了良好的结果。使用高斯正则化的目标编码在存在两种偏见时显示了改善，但在过度参数化的情况下风险会显著丧失模型性能。
- en: Intersectional Fairness
  id: totrans-50
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 交叉公正性
- en: In the quest for fairness in machine learning, it is essential to recognize
    the complex interplay of attributes and their impact on discrimination. This section
    delves into the effects of encoding categorical attributes on intersectional fairness,
    focusing on insights gained from the COMPAS dataset. We put forth hypotheses concerning
    the potential degradation of fairness through attribute engineering, the propensity
    for categorical encoding to increase discrimination, and the efficacy of regularization
    techniques in mitigating intersectional biases.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习中追求公平时，必须认识到属性的复杂相互作用及其对歧视的影响。本节将**深入探讨**分类属性编码对交叉公正性的影响，重点关注从COMPAS数据集中获得的见解。我们提出了有关通过属性工程可能导致公正性下降的假设、分类编码可能增加歧视的倾向以及正则化技术在缓解交叉偏见中的有效性。
- en: To explore the effects of encodings on intersectional fairness, we analyze the
    concatenated “Ethnic” and “Marital Status” attributes in the COMPAS dataset. By
    selecting the “Caucasian Married” group as the reference, we compare the maximum
    fairness violation across all groups. To facilitate comprehension, we utilize
    the generalized linear model from the previous section and emphasize the Equal
    Opportunity Fairness metric, which aligns with the behavior of other fairness
    metrics.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 为了探讨编码对交叉公正性的影响，我们分析了COMPAS数据集中连接的“民族”和“婚姻状况”属性。通过选择“白人已婚”组作为参考，我们比较了所有组的最大公平性违规。为了便于理解，我们利用了上一节的广义线性模型，并强调平等机会公平性指标，这与其他公平性指标的行为一致。
- en: '![](../Images/8d1f54272e82a0c62bbef0e8edcdd335.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8d1f54272e82a0c62bbef0e8edcdd335.png)'
- en: Equal opportunity fairness implications of encoding categorical protected attributes
    and their regularization effects on the Compas Dataset. Horizontal lines are the
    baselines where the protected attribute is not included in the training data.
    Regularized target encoding does not harm fairness metrics but can improve this
    dataset's predictive performance. Image by the author.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 编码分类保护属性的平等机会公平性及其在Compas数据集上的正则化效果。水平线是保护属性未包含在训练数据中的基线。正则化目标编码不会损害公平性指标，但可以提高该数据集的预测性能。图片由作者提供。
- en: The above figure visually demonstrates how attribute concatenation creates intersectional
    attributes and exacerbates fairness violations, providing empirical support for
    our first hypothesis. Remarkably, even when the protected attributes are not encoded
    (represented by horizontal lines), the maximum fairness violation increases substantially
    from 0.015 for “Ethnic” or 0.08 for “Marital Status” to 0.16 for the intersectional
    attribute. This finding substantiates Kimberle Crenshaw’s seminal work in 1958,
    which shed light on how different forms of oppression intersect and compound discrimination
    for marginalized groups.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 上图直观地展示了属性连接如何创建交叉属性并加剧公平性违规，为我们的第一个假设提供了实证支持。值得注意的是，即使在未编码保护属性的情况下（由水平线表示），最大公平性违规也显著增加，从“民族”0.015或“婚姻状况”0.08增加到交叉属性的0.16。这一发现证实了金伯莉·克伦肖1958年的开创性工作，揭示了不同形式的压迫如何交织在一起并加剧对边缘化群体的歧视。
- en: 'Moreover, our second hypothesis is corroborated by observing that both encoding
    techniques result in higher equal opportunity violations compared to not encoding
    the protected attribute. This highlights the role of encoding in amplifying discrimination.
    However, there is a glimmer of hope: through the regularization of target encoding,
    fairness can be enhanced. This result aligns with our theoretical understanding,
    as attribute concatenation can worsen fairness by increasing both irreducible
    and reducible biases.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们的第二个假设得到了验证，即观察到这两种编码技术导致的平等机会违反率高于不编码受保护属性的情况。这突显了编码在放大歧视方面的作用。然而，仍有一线希望：通过目标编码的正则化，可以提升公平性。这个结果与我们的理论理解一致，因为属性连接可能会通过增加不可减少偏差和可减少偏差来恶化公平性。
- en: Conclusions
  id: totrans-57
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: Our research highlights the significant role of categorical attribute encoding
    in balancing model accuracy and fairness in machine learning. We have identified
    two types of biases, irreducible and reducible, that can arise from encoding categorical
    attributes.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的研究强调了类别属性编码在平衡机器学习模型的准确性和公平性方面的重要作用。我们已识别出编码类别属性时可能出现的两种偏差：不可减少偏差和可减少偏差。
- en: Through theoretical analysis and empirical experiments, we find that one-hot
    encoding exhibits more discrimination than target encoding. However, promising
    results are observed with regularized target encoding, which shows potential for
    improving fairness while maintaining acceptable model performance.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 通过理论分析和实证实验，我们发现独热编码（one-hot encoding）表现出比目标编码（target encoding）更多的歧视。然而，经过正则化的目标编码表现出有希望的结果，它在保持可接受的模型性能的同时，展现了提高公平性的潜力。
- en: We emphasize the importance of considering the implications of encoding categorical
    protected attributes, as slight modifications in the encoding approach can lead
    to fairness improvements without significant sacrifices in predictive accuracy.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 我们强调了考虑编码类别受保护属性的影响的重要性，因为对编码方法进行轻微修改可以在不显著牺牲预测准确性的情况下实现公平性改善。
- en: '**Between the lines**'
  id: totrans-61
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**潜台词**'
- en: 'In recent years we have seen algorithmic methods aiming to improve fairness
    in data-driven systems from many perspectives: data collection, pre-processing,
    in-processing, and post-processing steps. In this work, we have focused on how
    encoding categorical attributes (a common pre-processing step) can reconcile model
    quality and fairness.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，我们看到许多算法方法从数据收集、预处理、处理过程中以及后处理步骤等多个角度来提高数据驱动系统中的公平性。在这项工作中，我们集中研究了如何通过编码类别属性（一个常见的预处理步骤）来调和模型质量和公平性。
- en: A common underpinning of much of the work in fair ML is the assumption that
    trade-offs between equity and accuracy may necessitate complex methods or difficult
    policy choices [[Rodolfa et al.](https://www.nature.com/articles/s42256-021-00396-x)]
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 许多公平机器学习工作的一个共同基础假设是，公平性与准确性之间的权衡可能需要复杂的方法或困难的政策选择 [[Rodolfa et al.](https://www.nature.com/articles/s42256-021-00396-x)]
- en: Since target encoding with regularization is easy to perform and does not require
    significant changes to the machine learning models, it can be explored in the
    future as a suitable complementary for in-processing methods in fair machine learning.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 由于正则化的目标编码易于执行且不需要对机器学习模型进行重大更改，因此它可以作为公平机器学习中处理方法的合适补充，值得在未来进一步探索。
- en: We encourage industry practitioners to consider the implications of encoding
    categorical protected attributes. With slight changes in the encoding approach,
    improvements in fairness can be achieved without significant detriment to predictive
    performance. However, it is essential to understand that using fair AI methods
    does not guarantee the fairness of complex socio-technical systems.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 我们鼓励行业从业者考虑编码类别受保护属性的影响。通过对编码方法进行轻微调整，可以在不显著影响预测性能的情况下实现公平性改善。然而，必须理解的是，使用公平人工智能方法并不保证复杂社会技术系统的公平性。
- en: '**Limitations**: This work aims to show what are some of the implications of
    encoding categorical protected attributes at any moment. It should be understood
    as if, in any situation encoding categorical protected attributes won’t increase
    fairness metrics; we advocate considering the effects of encoding regularization
    along the fairness axis, too, not only on the predictive performance axis. Fair-AI
    methods do not necessarily guarantee the fairness of AI-based complex socio-technical
    systems.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '**限制**：本工作旨在展示在任何时刻对分类保护属性进行编码的一些影响。应理解为，在任何情况下对分类保护属性的编码不会增加公平性指标；我们提倡在公平性轴上考虑编码正则化的效果，而不仅仅是在预测性能轴上。Fair-AI
    方法并不一定保证基于 AI 的复杂社会技术系统的公平性。'
- en: '**Acknowledgments**'
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**致谢**'
- en: '*This work has received funding from the European Union’s Horizon 2020 research
    and innovation program under Marie Sklodowska-Curie Actions (grant agreement number
    860630) for the project ‘’NoBIAS — Artificial Intelligence without Bias’’*'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '*本工作得到了欧洲联盟 Horizon 2020 研究与创新计划资助，资助项目为 Marie Sklodowska-Curie Actions（资助协议编号
    860630），项目名称为‘’NoBIAS — Artificial Intelligence without Bias’’*'
- en: '**Disclaimer**'
  id: totrans-69
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**免责声明**'
- en: '*This work reflects only the authors’ views, and the European Research Executive
    Agency (REA) is not responsible for any use that may be made of the information
    it contains.*'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '*本工作仅反映作者的观点，欧洲研究执行局（REA）对其包含的信息的任何使用不承担责任。*'
- en: Cite
  id: totrans-71
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 引用
- en: '[PRE0]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
