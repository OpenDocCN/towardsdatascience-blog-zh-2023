- en: GPT and Human Psychology
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: GPT 与人类心理学
- en: 原文：[https://towardsdatascience.com/gpt-and-human-psychology-94a21ba6d20e](https://towardsdatascience.com/gpt-and-human-psychology-94a21ba6d20e)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/gpt-and-human-psychology-94a21ba6d20e](https://towardsdatascience.com/gpt-and-human-psychology-94a21ba6d20e)
- en: Analogies with Human Thinking and Reasoning.
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 人类思维和推理的类比。
- en: '[](https://medium.com/@maartengrootendorst?source=post_page-----94a21ba6d20e--------------------------------)[![Maarten
    Grootendorst](../Images/58e24b9cf7e10ff1cd5ffd75a32d1a26.png)](https://medium.com/@maartengrootendorst?source=post_page-----94a21ba6d20e--------------------------------)[](https://towardsdatascience.com/?source=post_page-----94a21ba6d20e--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----94a21ba6d20e--------------------------------)
    [Maarten Grootendorst](https://medium.com/@maartengrootendorst?source=post_page-----94a21ba6d20e--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@maartengrootendorst?source=post_page-----94a21ba6d20e--------------------------------)[![Maarten
    Grootendorst](../Images/58e24b9cf7e10ff1cd5ffd75a32d1a26.png)](https://medium.com/@maartengrootendorst?source=post_page-----94a21ba6d20e--------------------------------)[](https://towardsdatascience.com/?source=post_page-----94a21ba6d20e--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----94a21ba6d20e--------------------------------)
    [Maarten Grootendorst](https://medium.com/@maartengrootendorst?source=post_page-----94a21ba6d20e--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----94a21ba6d20e--------------------------------)
    ·13 min read·Jun 29, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----94a21ba6d20e--------------------------------)
    ·13 分钟阅读·2023 年 6 月 29 日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: The state of AI has changed drastically with generative text models, such as
    ChatGPT, GPT-4, and many others.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 随着生成文本模型（如 ChatGPT、GPT-4 等）的出现，人工智能的状态发生了巨大的变化。
- en: These **GPT** (Generative Pretrained Transformer) models seemingly removed the
    threshold for diving into Artificial intelligence for those without a technical
    background. Anyone can just start asking the models a bunch of stuff and get scarily
    accurate answers.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 这些**GPT**（生成式预训练变换器）模型似乎降低了那些没有技术背景的人进入人工智能领域的门槛。任何人只需开始向模型提问，就能得到令人惊讶的准确回答。
- en: At least, most of the time…
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 至少，大多数时候是这样……
- en: When it fails to reproduce the right output, it does not mean it is incapable
    of doing so. Often, we simply need to change what we ask, the **prompt**, in a
    way to guide the model toward the right answer.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 当它无法生成正确的输出时，并不意味着它无法做到这一点。我们通常只需改变我们提问的方式，即**提示**，以引导模型得到正确的答案。
- en: This is often referred to as **prompt engineering**.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 这通常被称为**提示工程**。
- en: Many of the techniques in prompt engineering try to mimic the way humans think.
    Asking the model to *“think aloud”* or *“let’s think step by step”* are great
    examples of having the model mimic how we think.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 许多提示工程中的技术试图模仿人类思考的方式。要求模型*“大声思考”*或*“一步步思考”*是让模型模仿我们思维方式的很好的例子。
- en: These analogies between GPT models and human psychology are important since
    they help us understand how we can improve the output of GPT models. It shows
    us capabilities they might be missing.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: GPT 模型与人类心理学之间的这些类比很重要，因为它们帮助我们理解如何改进 GPT 模型的输出。这显示了它们可能缺失的能力。
- en: This does not mean that I am advocating for any GPT model as general intelligence
    but it is interesting to see how and why we are trying to make GPT models “think”
    like humans**.**
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 这并不意味着我在倡导任何 GPT 模型作为通用智能，但有趣的是，看看我们如何以及为什么试图让 GPT 模型像人类一样**“思考”**。
- en: Many of the analogies that you will see here are also discussed in the video
    below. Andrej Karpathy shares amazing insights into Large Language Models from
    a psychological perspective and is definitely worth watching!
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 你在这里看到的许多类比在下面的视频中也有讨论。Andrej Karpathy 从心理学的角度分享了对大型语言模型的惊人见解，绝对值得观看！
- en: An excellent video describing the state of GPT using interesting psychological
    analogies.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 一部优秀的视频，描述了 GPT 的状态，并使用有趣的心理学类比。
- en: As a data scientist and psychologist myself, this is a subject that is close
    to my heart. It is incredibly interesting to see how these models behave, how
    we would like them to behave, and how we are nudging these models to behave like
    us.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 作为数据科学家和心理学家，这个话题对我来说非常重要。看到这些模型的行为、我们希望它们如何表现，以及我们如何推动这些模型像我们一样表现，真的很有趣。
- en: 'There are a number of subjects where analogies between GPT models and human
    psychology give interesting insights that will be discussed in this article:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多主题中GPT模型和人类心理学的类比提供了有趣的见解，这将在本文中讨论：
- en: '![](../Images/929894205929dc3715d028f98052f80e.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/929894205929dc3715d028f98052f80e.png)'
- en: An overview of what will be discussed in this article.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 本文将讨论的内容概述。
- en: '**DISCLAIMER**: When talking about analogies of GPT models with human psychology,
    there is a risk involved, namely the anthropomorphism of Artificial Intelligence.
    In other words, humanizing these GPT models. This is definitely **not** my intention.
    This post is not about existential risks or general intelligence but merely a
    fun exercise drawing similarities between us and GPT models. If anything, feel
    free to take this with a grain of salt!'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '**免责声明**：在讨论GPT模型与人类心理学的类比时，存在一定的风险，即人工智能的人性化。换句话说，就是将这些GPT模型人性化。这绝对**不是**我的意图。此帖子不是关于存在风险或一般智能，而仅仅是一个有趣的练习，指出我们与GPT模型之间的相似之处。如果有的话，请随意带着一定的怀疑态度看待！'
- en: Prompting
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 提示
- en: 'A prompt is what we ask of a GPT model, for example: “Create a list of 10 book
    titles”.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 提示就是我们对GPT模型的要求，例如：“创建一个10本书名的列表”。
- en: When we try different questions in the hopes of improving the performance of
    the model, then we apply **prompt engineering**.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们尝试不同的问题以期提高模型的表现时，我们就应用了**提示工程**。
- en: In psychology, there are many different forms of prompting individuals to exhibit
    certain behavior, which is typically used in [applied behavior applications](https://www.researchgate.net/profile/Traci-Sitzmann/publication/41087516_Sometimes_You_Need_a_Reminder_The_Effects_of_Prompting_Self-Regulation_on_Regulatory_Processes_Learning_and_Attrition/links/6065bd8892851c91b194ea94/Sometimes-You-Need-a-Reminder-The-Effects-of-Prompting-Self-Regulation-on-Regulatory-Processes-Learning-and-Attrition.pdf)
    (ABA) to learn new behavior.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在心理学中，有许多不同形式的提示个体表现出某些行为，这通常用于[应用行为分析](https://www.researchgate.net/profile/Traci-Sitzmann/publication/41087516_Sometimes_You_Need_a_Reminder_The_Effects_of_Prompting_Self-Regulation_on_Regulatory_Processes_Learning_and_Attrition/links/6065bd8892851c91b194ea94/Sometimes-You-Need-a-Reminder-The-Effects-of-Prompting-Self-Regulation-on-Regulatory-Processes-Learning-and-Attrition.pdf)
    (ABA)中学习新行为。
- en: '![](../Images/1a71ff8d3399351f75c033ba0422c5e4.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1a71ff8d3399351f75c033ba0422c5e4.png)'
- en: There is a distinct difference between how this works in GPT models versus Psychology.
    In Psychology, prompting is about learning new behavior. Something the individual
    could not do before. For a GPT model, it is about demonstrating previously unseen
    behavior.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: GPT模型和心理学中提示的工作方式有明显的不同。在心理学中，提示是为了学习新的行为，即个体之前无法做到的事。而对于GPT模型来说，则是展示以前未见过的行为。
- en: The main distinction lies in that an individual learns something entirely new
    and, to a certain degree, changes as an individual. In contrast, the GPT model
    was already capable of showing that behavior but did not due to its circumstances,
    namely the prompts. Even when you successfully elicit “appropriate” behavior from
    the model, the model itself did not change.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 主要的区别在于个体学到的是全新的东西，并且在某种程度上作为个体发生了变化。相比之下，GPT模型已经能够展示这种行为，只是由于其情况，即提示，没有展示这种行为。即使你成功地从模型中引导出“适当”的行为，模型本身并没有发生变化。
- en: '![](../Images/29138489ded82ec80a2d8fd1e616762a.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/29138489ded82ec80a2d8fd1e616762a.png)'
- en: Prompting in GPT models is also a lot less subtle. Many of the techniques in
    prompting are as explicit as they can be (e.g., *“You are a scientist. Summarize
    this article.”*).
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在GPT模型中，提示也明显简单了很多。许多提示技巧都非常直白（例如，*“你是一个科学家。总结一下这篇文章。”*）。
- en: Mimicking Behavior
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模仿行为
- en: GPT models are copycats. It, and comparable models, are trained on mountains
    of textual data and try to replicate that as best as they can.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: GPT模型是模仿者。它和类似的模型在大量文本数据上进行训练，并尽可能地复制这些数据。
- en: '![](../Images/bd2b22ccc5cca8ae8f6b2a741d2cc1c5.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bd2b22ccc5cca8ae8f6b2a741d2cc1c5.png)'
- en: This means that when you ask the model a question, it tries to generate a sequence
    of words that fits best with what it has seen during training. With enough training
    data, this sequence of words becomes more and more coherent.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着，当你问模型一个问题时，它会尝试生成一系列与训练中看到的内容最匹配的单词。随着训练数据的增加，这些单词序列变得越来越连贯。
- en: '![](../Images/3b7cd57db52de629400359f56f7f2aa7.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3b7cd57db52de629400359f56f7f2aa7.png)'
- en: However, such a model has no inherent capabilities of truly understanding the
    behavior it is mimicking. As with many things in this article, whether a GPT model
    truly is capable of reasoning is definitely open for discussion and often elicits
    [passionate discussions](https://twitter.com/Grady_Booch/status/1673797840605433856).
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这样的模型没有真正理解其模仿行为的固有能力。正如本文中的许多内容一样，GPT 模型是否真正具备推理能力无疑是一个讨论的话题，并且常常引发[热烈的讨论](https://twitter.com/Grady_Booch/status/1673797840605433856)。
- en: Although we have inherent capabilities for mimicking behavior, it is much more
    involved and has a grounding in both [social constructs](http://ndl.ethernet.edu.et/bitstream/123456789/17733/1/18.pdf.pdf#page=70)
    and [biology](https://www.sciencedirect.com/science/article/pii/S0960982210002332).
    We tend to, to some degree, understand mimicked behavior and can easily generalize
    it.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们具备模仿行为的固有能力，但这要复杂得多，并且有基于[社会构建](http://ndl.ethernet.edu.et/bitstream/123456789/17733/1/18.pdf.pdf#page=70)和[生物学](https://www.sciencedirect.com/science/article/pii/S0960982210002332)的基础。我们往往在某种程度上理解模仿的行为，并且可以很容易地概括它。
- en: Identity
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 身份
- en: We have a preconceived notion of who we are, how our experiences have shaped
    us, and the views that we have of the world. We have an **identity**.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对自己有预设的观念，了解我们的经历如何塑造了我们，以及我们对世界的看法。我们有**身份**。
- en: GPT models do not have an identity. It has a lot of knowledge about the world
    we live in and it knows what kind of answers we might prefer, but it has no sense
    of “self”.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: GPT 模型没有身份。它对我们生活的世界有很多知识，并且知道我们可能偏好的答案，但它没有“自我”意识。
- en: '![](../Images/caddbd7b77e5ec18fbfc9b681601f0b2.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/caddbd7b77e5ec18fbfc9b681601f0b2.png)'
- en: It is not necessarily guided toward certain views like we are. From an identity
    perspective, it is a [**blank slate**](https://en.wikipedia.org/wiki/Tabula_rasa).
    This means that since a GPT model has a lot of knowledge about the world, it has
    some capabilities to mimic the identity you ask of it.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 它不一定像我们一样被引导向某些观点。从身份的角度来看，它是一个[**白板**](https://en.wikipedia.org/wiki/Tabula_rasa)。这意味着，由于
    GPT 模型对世界有很多知识，它具备模仿你要求的身份的能力。
- en: But as always, it is just mimicked behavior.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 但一如既往，这只是模仿行为。
- en: It does have a major advantage. We can ask the model to take on the role of
    a scientist, writer, editor, etc. and it will try to follow suit. By priming it
    towards mimicking certain identities, its output will be more tuned toward the
    task.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 它确实有一个主要的优势。我们可以要求模型扮演科学家、作家、编辑等角色，它会尽力配合。通过使其模仿特定身份，它的输出将更适应任务。
- en: '![](../Images/c595f18483ba15eb217cea33288537de.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c595f18483ba15eb217cea33288537de.png)'
- en: Competencies
  id: totrans-45
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 能力
- en: This is an interesting subject. There are many sources for evaluating Large
    Language Models on a wide variety of tests, such as the [Hugging Face Leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard)
    or using [Elo ratings](https://lmsys.org/blog/2023-05-25-leaderboard/) to challenge
    Large Language Models.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个有趣的主题。评估大语言模型的测试有很多来源，如[Hugging Face 排行榜](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard)或使用[Elo
    评分](https://lmsys.org/blog/2023-05-25-leaderboard/)来挑战大语言模型。
- en: These are important tests to evaluate the capabilities of these models. However,
    what I consider to be a strength of a certain model, you might not agree with.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是评估这些模型能力的重要测试。然而，我认为某个模型的优点，你可能不一定同意。
- en: '![](../Images/5243fcd374aa42d1e5b8a90ff05d9623.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5243fcd374aa42d1e5b8a90ff05d9623.png)'
- en: This relates to the model itself. Even if we tell it the scores of these tests,
    it still does not know where its strengths and weaknesses **comparatively** lie.
    For example, GPT-4 passed the [bar exam](https://www.iit.edu/news/gpt-4-passes-bar-exam)
    which we generally consider a big strength. However, the model might then not
    realize that only passing the bar is not a strength it was when in a room full
    of experienced lawyers.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 这与模型本身有关。即使我们告诉它这些测试的得分，它仍然不知道自己的优点和**相对**缺点在哪里。例如，GPT-4 通过了我们通常认为是一个大优点的[律师资格考试](https://www.iit.edu/news/gpt-4-passes-bar-exam)。然而，该模型可能并未意识到，仅仅通过律师资格考试并不是它在充满经验丰富的律师的房间里所拥有的优点。
- en: In other words, it highly depends on the context of the situation when one’s
    capabilities are considered strengths or weaknesses. The same applies to our own
    capabilities. I might think myself to be proficient in Large Language Models but
    if you surround me with people like Andrew Ng, Sebastian Raschka, etc. my knowledge
    about Large Language Models is suddenly not the strength it was before.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，一个人的能力被认为是优点还是缺点很大程度上依赖于情况的背景。这同样适用于我们自己的能力。我可能认为自己在大型语言模型方面很熟练，但如果你把我放在Andrew
    Ng、Sebastian Raschka等人周围，我对大型语言模型的知识突然不再是之前的强项。
- en: This is important because the model does not **instinctively** know when something
    is a strength or weakness, **so you should tell it**.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 这很重要，因为模型并不**本能**地知道什么是优点或缺点，**所以你需要告诉它**。
- en: '![](../Images/6756c8a666c6d0a16d0508443a047107.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6756c8a666c6d0a16d0508443a047107.png)'
- en: For example, if you feel like the model is poor when solving mathematical equations,
    you can tell it to never perform any calculations itself but use the [Wolfram
    Plugin](https://www.wolfram.com/wolfram-plugin-chatgpt/) instead.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果你觉得模型在解决数学方程时表现不佳，你可以告诉它不要自己进行任何计算，而是使用[Wolfram插件](https://www.wolfram.com/wolfram-plugin-chatgpt/)。
- en: In contrast, although we claim to have some notion of our own strengths and
    weaknesses, these are often subjective and tend to be heavily biased.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，尽管我们声称对自己的优点和缺点有一定的认识，但这些往往是主观的，并且容易受到严重偏见的影响。
- en: Tools
  id: totrans-55
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 工具
- en: As mentioned previously, a GPT model does not know what it is good at or not
    in specific situations. You can help it make sense of the situation by adding
    an explanation of the situation to the prompt. By describing the situation, the
    model is primed towards generating more accurate answers.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，GPT模型不知道它在特定情况下擅长或不擅长什么。你可以通过在提示中添加对情况的解释来帮助它理解情况。通过描述情况，模型会更倾向于生成更准确的回答。
- en: This will not always make it capable across tasks. Like humans, explaining the
    situation helps but does not overcome all their weaknesses.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 这并不总是能使它在各种任务中都具备能力。就像人类一样，解释情况有帮助，但并不能克服所有的弱点。
- en: '![](../Images/7ec23d327dc1b836617fcd4782fcee8d.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7ec23d327dc1b836617fcd4782fcee8d.png)'
- en: Instead, when we face something that we are not currently capable of we often
    rely on **tools** to overcome them. We use a calculator when doing complex equations
    or use a car for faster transportation.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，当我们面对当前无法完成的事情时，我们通常依靠**工具**来克服这些困难。我们在做复杂的方程时使用计算器，或者用汽车进行更快的交通。
- en: This reliance on external tools is not something a GPT model automatically does.
    You will have to tell the model to use a specific external tool when you are convinced
    it is not capable of a certain task.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 这种对外部工具的依赖不是GPT模型自动进行的。你需要告诉模型在你确信它无法完成某个任务时使用特定的外部工具。
- en: '![](../Images/60801c7cd87bdc6cf5c4407f06f31e34.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/60801c7cd87bdc6cf5c4407f06f31e34.png)'
- en: What is important here is that we rely on an enormous amount of tools on a daily
    basis, your phone, keys, glasses, etc. Giving a GPT model the same capabilities
    can be a tremendous help to its performance. These external tools are similar
    to the [plugins](https://openai.com/blog/chatgpt-plugins) that OpenAI has available.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 这里重要的是，我们每天依赖大量的工具，比如手机、钥匙、眼镜等。给GPT模型相同的能力可以极大地帮助它的表现。这些外部工具类似于OpenAI提供的[插件](https://openai.com/blog/chatgpt-plugins)。
- en: A major disadvantage of this is that these models do not automatically use tools.
    It will only access plugins if you tell the model that it is a possibility.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 一个主要的缺点是这些模型不会自动使用工具。只有在你告诉模型这是一个可能性时，它才会访问插件。
- en: Internal Monologue
  id: totrans-64
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 内部对话
- en: We typically have an inner voice that we converse with when solving difficult
    problems. “If I do this, then that will be results but if I do that, then that
    might give me a better solution”.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通常有一个内心的声音，在解决困难问题时与之对话。“如果我这样做，那么结果会是这样，但如果我那样做，可能会给我更好的解决方案”。
- en: GPT models do not exhibit this behavior automatically. When you ask it a question
    it simply generates a number of words that most logically would follow that question.
    Sure, it does compute those words but it does not leverage those words to create
    this internal monologue.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: GPT模型不会自动表现出这种行为。当你问它一个问题时，它只是生成一系列最符合逻辑的词汇。确实，它会计算这些词汇，但它不会利用这些词汇来创建内部对话。
- en: '![](../Images/38255518f654e653abaf59bb454e5956.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/38255518f654e653abaf59bb454e5956.png)'
- en: As it turns out, asking the model “think aloud” by saying, “Let’s think step
    by step” tends to improve the answers it gives quite a bit. This is called [chain-of-thoughts](https://arxiv.org/pdf/2201.11903.pdf?trk=public_post_comment-text)
    and tries to emulate the thought processes of human reasoners. This does not necessarily
    mean that the model is “reasoning” but it is interesting to see how much this
    improves its performance.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 事实证明，通过让模型“思考大声”即说“让我们一步步来思考”，往往可以显著提高它给出的答案。这被称为 [链式思维](https://arxiv.org/pdf/2201.11903.pdf?trk=public_post_comment-text)，试图模拟人类推理者的思维过程。这不一定意味着模型在“推理”，但看到这如何改善其表现还是很有趣的。
- en: As a nice little bonus, the model does not perform this monologue internally,
    so following along with what the model is thinking gives amazing insights into
    its behavior.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一个小小的附加好处，模型不会在内部进行这个独白，因此跟随模型的思考过程可以提供对其行为的惊人洞察。
- en: '![](../Images/9deb38a8b47760da0645c6e3123a7f99.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9deb38a8b47760da0645c6e3123a7f99.png)'
- en: This “inner voice” is quite a bit simplified compared to how ours works. We
    are much more dynamic in the “conversations” we have with ourselves as well as
    the way we have those “conversations”. It can be symbolic, motoric, or even emotional
    in nature. For example, many athletes picture themselves performing the sport
    they excel in as a way to train for the actual thing. This is called [**mental
    imagery**](https://cnbc.cmu.edu/~tai/readings/nature/kosslyn_imagery.pdf)**.**
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 这种“内心声音”相比于我们实际的运作方式要简单得多。我们在与自己进行“对话”时更加动态，这些“对话”方式可以是象征性的、运动性的，甚至是情感性的。例如，许多运动员会在脑海中想象自己在擅长的运动中表现的情景，以此来训练。这被称为
    [**心理意象**](https://cnbc.cmu.edu/~tai/readings/nature/kosslyn_imagery.pdf)**。**
- en: These conversations allow us to brainstorm. We use this to come up with new
    ideas, solve problems, and understand the context in which a problem appears.
    A GPT model, in contrast, will have to be told explicitly to brainstorm a solution
    through very specific instructions.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 这些对话允许我们进行头脑风暴。我们用这个来提出新想法、解决问题以及理解问题出现的背景。相比之下，GPT 模型则必须通过非常具体的指令来明确地进行头脑风暴。
- en: We can further relate this to our [system 1 and system 2](https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=94a9488e9a647841ff237d1d44803ba23c40ee5e)
    thinking processes. **System 1** thinking is an automatic, intuitive, and near-instantaneous
    process. We have very little control here. In contrast, **system 2** is a conscious,
    slow, logical, and effortful process.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以进一步将这与我们的 [系统 1 和系统 2](https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=94a9488e9a647841ff237d1d44803ba23c40ee5e)
    思维过程相关联。**系统 1** 思维是一个自动的、直观的、几乎瞬时的过程。我们在这里几乎没有控制权。相反，**系统 2** 是一个有意识的、缓慢的、逻辑的、费力的过程。
- en: By giving a GPT model the ability of self-reflection, we are essentially trying
    to mimic this system 2 way of thinking. The model takes more time to generate
    an answer and looks over it carefully instead of quickly generating a response.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 通过赋予 GPT 模型自我反思的能力，我们实际上是在尝试模仿这种系统 2 的思维方式。模型需要更多时间来生成答案，并仔细审查，而不是快速生成回应。
- en: Roughly, you could say that without any prompt engineering, we enable its system
    1 thinking process whilst without specific instructions and chain-of-thought-like
    processes, we enable its system 2 way of thinking.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 粗略地说，你可以认为在没有任何提示工程的情况下，我们启用了它的系统 1 思维过程，而在没有具体指令和类似链式思维的过程的情况下，我们启用了它的系统 2
    思维方式。
- en: If you want to know more about our system 1 and system 2 thinking, there is
    an amazing book called [Thinking, Fast and Slow](https://www.goodreads.com/book/show/11468377-thinking-fast-and-slow)
    that is worth reading!
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想了解更多关于我们系统 1 和系统 2 思维的内容，有一本很棒的书叫做 [《思考，快与慢》](https://www.goodreads.com/book/show/11468377-thinking-fast-and-slow)
    非常值得阅读！
- en: Memory
  id: totrans-77
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 记忆
- en: Andrej Karpathy, in his video mentioned at the beginning of the article, makes
    a great comparison of a human’s memory capabilities versus that of a GPT model.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: Andrej Karpathy 在他在文章开头提到的视频中，对人类记忆能力与 GPT 模型的记忆能力进行了很好的比较。
- en: Our memory is quite complex, we have long-term memory, working memory, short-term
    memory, sensory memory, and more.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的记忆非常复杂，包括长期记忆、工作记忆、短期记忆、感觉记忆等等。
- en: '![](../Images/495deb1447672a27bb9b440ff9633357.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/495deb1447672a27bb9b440ff9633357.png)'
- en: 'We can, very roughly, view the memory of a GPT model as four components and
    compare that to our own memory systems:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以非常粗略地将 GPT 模型的记忆视为四个组件，并将其与我们自己的记忆系统进行比较：
- en: Long-term memory
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 长期记忆
- en: Working memory
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 工作记忆
- en: Sensory memory
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 感觉记忆
- en: External memory
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 外部记忆
- en: The **long-term memory** of a GPT model can be viewed as the things it has learned
    whilst training on billions of data. That information is, to a certain degree,
    represented within the model which it can perfectly reproduce whenever it wants.
    This long-term memory will stick with the model throughout its existence. In contrast,
    our long-term memory can decay over time, often referred to as the decay theory.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: GPT模型的**长期记忆**可以被视为它在训练过程中学到的内容。这些信息在模型中以一定程度上表示，它可以在任何时候完美地再现。这种长期记忆会伴随模型的整个存在。相比之下，我们的长期记忆可能会随着时间的推移而衰退，通常被称为衰退理论。
- en: A GPT model’s long-term memory is perfect and does not decay over time
  id: totrans-87
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: GPT模型的长期记忆是完美的，不会随时间衰退。
- en: The **working memory** of a GPT model is everything that fits within the prompt
    you give it. The model can use all of that information perfectly to perform its
    calculation and give back a response. This is a great analogy with our working
    memory since it is a type of memory that has a limited capacity to temporarily
    hold information. A GPT model for instance will “forget” its prompt after it has
    given its response. The reason why it seems to remember the conversation is that
    alongside the prompt, the conversation history is added to the prompt.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: GPT模型的**工作记忆**是你给它的提示中所有适合的内容。模型可以完美地使用所有这些信息来进行计算并给出响应。这与我们的工作记忆是一个很好的类比，因为它是一种具有有限容量的记忆，用于暂时保存信息。例如，GPT模型在给出响应后会“忘记”它的提示。之所以似乎记住对话，是因为除了提示外，对话历史也被添加到提示中。
- en: '![](../Images/137be71d91ee8e6c2289146814a41e41.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/137be71d91ee8e6c2289146814a41e41.png)'
- en: A GPT model is forgetful when it comes to new information
  id: totrans-90
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: GPT模型在处理新信息时是健忘的。
- en: '**Sensory memory** relates to how we hold information derived from our senses,
    like visual, auditory, and haptic information. We use this information and pass
    it to our short-term or working memory for processing. This is similar to multi-modal
    GPT models, models that work on text, images, and even sound.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '**感觉记忆**与我们如何持有来自感官的信息有关，如视觉、听觉和触觉信息。我们使用这些信息并将其传递到我们的短期或工作记忆中进行处理。这类似于多模态GPT模型，这些模型处理文本、图像甚至声音。'
- en: However, it might be more appropriate to say that GPT models have multi-modal
    working and long-term memory rather than sensory memory. These models tightly
    couple multi-modal data, with their different forms of “memory”. So as we have
    seen before, it rather seems to **mimic** sensory memory.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，更合适的说法可能是GPT模型具有多模态的工作记忆和长期记忆，而非感觉记忆。这些模型将多模态数据与其不同形式的“记忆”紧密结合。因此，正如我们之前所见，它似乎更像是**模仿**感觉记忆。
- en: A GPT model mimics sensory memory with a multi-modal training procedure
  id: totrans-93
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: GPT模型通过多模态训练程序模仿感觉记忆。
- en: Lastly, GPT models become quite a bit stronger when you give them **external
    memory**. This refers to a database of information that it can access whenever
    it wants, like several books about physics. In contrast, our external memory uses
    cues from the environment to help us remember certain ideas and sensations. In
    a way, it is about accessing external information versus remembering internal
    information.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，当你给GPT模型**外部记忆**时，它会变得更强。这指的是一个信息数据库，它可以随时访问，比如几本关于物理的书。相比之下，我们的外部记忆使用来自环境的线索来帮助我们记住某些想法和感觉。从某种程度上说，它是访问外部信息与记住内部信息的区别。
- en: '**NOTE:** I did not mention short-term memory. There is much discussion between
    short-term and working memory and whether they are not actually the same thing.
    A difference often mentioned is that working memory does more than just the short-term
    storage of information but also has the ability to manipulate it. Also, it has
    a better analogy with a GPT model, so let’s cherry-pick for a bit here.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意：** 我没有提到短期记忆。关于短期记忆和工作记忆之间是否真的相同，有很多讨论。一个常提到的区别是，工作记忆不仅仅是短期储存信息，还具有操控信息的能力。此外，它与GPT模型有更好的类比，所以这里我们稍微挑选一下。'
- en: Autonomy
  id: totrans-96
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自主性
- en: As we have seen throughout this article, if we want a GPT model to do something,
    **we should tell it**.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在本文中看到的，如果我们希望GPT模型做某事，**我们应该告诉它**。
- en: This is important to note as it relates to a sense of autonomy. By default,
    we have a certain degree of autonomy. If I decide to grab a drink, I can.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 这点很重要，因为它涉及自主感。默认情况下，我们有一定程度的自主性。如果我决定去拿一杯饮料，我可以。
- en: '![](../Images/f777ff6330aacc674a08abb39d2826e5.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f777ff6330aacc674a08abb39d2826e5.png)'
- en: This is different for a GPT model as it has no autonomy by default. It cannot
    operate independently without giving it the necessary tools and environment to
    do so.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 对于GPT模型而言，这有所不同，因为它默认没有自主性。它无法在没有必要的工具和环境的情况下独立操作。
- en: We can give a GPT model autonomy by having it create a number of tasks to execute
    in order to reach a certain end goal. For each task, it writes down the steps
    for completing it, reflects on them, and executes them if it has the tools to
    do so.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过让GPT模型创建多个任务以执行，从而赋予它自主性，以实现某个最终目标。对于每个任务，它会写下完成步骤，进行反思，如果有工具，它会执行这些步骤。
- en: '[AutoGPT](https://autogpt.net/) is a great example of giving a GPT model autonomy'
  id: totrans-102
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '[AutoGPT](https://autogpt.net/)是赋予GPT模型自主性的一个很好的例子。'
- en: As a result, whatever the model is capable of is very much dependent on its
    environment to an arguably larger degree than our environment impacts us. Which
    is quite impactful considering the effect our environment has on us.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，模型的能力在很大程度上依赖于其环境，甚至可能比我们的环境对我们的影响还要大。这是相当有影响力的，考虑到环境对我们的影响。
- en: This also means that although a GPT model can show impressively complex autonomous
    behavior, it is fixed. It cannot decide to use a tool we never told it existed.
    For us, we are more adaptable to new and previously unknown tools.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 这也意味着，尽管GPT模型可以展示令人印象深刻的复杂自主行为，但它是固定的。它不能决定使用我们从未告诉它存在的工具。对于我们来说，我们对新工具和未知工具的适应性更强。
- en: Hallucination
  id: totrans-105
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 幻觉
- en: A common problem with GPT models is their ability to confidently say something
    that is simply not true nor supported by their training data.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: GPT模型的一个常见问题是它们能够自信地说出一些根本不真实也不受其训练数据支持的内容。
- en: For example, when you ask a GPT model to generate factual information, like
    the revenue of Apple in 2019, it might generate completely false information.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，当你要求GPT模型生成事实信息，如2019年苹果公司的收入时，它可能生成完全错误的信息。
- en: This is called [**hallucination**](https://arxiv.org/pdf/2202.03629.pdf).
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 这被称为[**幻觉**](https://arxiv.org/pdf/2202.03629.pdf)。
- en: The term stems from hallucination in human psychology, where we believe something
    that we see to be true whilst in reality, it is not. The main difference here
    is that human hallucination is based on perception, whilst a model “hallucinates”
    incorrect facts.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 这个术语源自人类心理学中的**幻觉**，即我们相信我们看到的东西是真实的，而实际上并非如此。这里的主要区别在于，人类的幻觉基于感知，而模型则“幻觉”出错误的事实。
- en: '![](../Images/4ae754138139f02584f22a4dbfab3148.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4ae754138139f02584f22a4dbfab3148.png)'
- en: It might be more appropriate to compare it with [**false memories**](https://blogs.brown.edu/recoveredmemory/files/2015/05/Loftus_Pickrell_PA_95.pdf).
    The tendency of humans to recall something differently from how it actually happened.
    This is similar to a GPT model that tries to reproduce things that actually never
    happened.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 将其与[**虚假记忆**](https://blogs.brown.edu/recoveredmemory/files/2015/05/Loftus_Pickrell_PA_95.pdf)进行比较可能更为恰当。人类记忆与实际发生情况不同的倾向。这类似于GPT模型试图重现实际上从未发生过的事情。
- en: '![](../Images/6e14407eee4c4f117cfc854bb92a2161.png)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6e14407eee4c4f117cfc854bb92a2161.png)'
- en: Interestingly, we can more easily generate false memories with suggestibility,
    priming, framing, etc. This seems to more closely match how a GPT model “hallucinates”
    as the prompt it receives is highly influential.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是，我们更容易通过暗示、预设、框架等生成虚假记忆。这似乎更接近于GPT模型如何“幻觉”，因为它收到的提示具有很大的影响力。
- en: Our memories can also be influenced by prompts/phrases that we receive from
    others. For example, by asking a person “What shade of red was this car?” we are
    implicitly providing a person with a supposed “fact”, namely that the car was
    red even when it was not. This can generate false memories and is referred to
    as a [**presupposition**](https://plato.stanford.edu/entries/presupposition/).
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的记忆也可能受到来自他人的提示/短语的影响。例如，通过问一个人“这辆车是什么红色的？”我们实际上是在隐含地提供一个所谓的“事实”，即这辆车是红色的，即使它实际上并不是。这可能会产生虚假的记忆，这被称为[**预设**](https://plato.stanford.edu/entries/presupposition/)。
- en: Thank you for reading!
  id: totrans-115
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 谢谢阅读！
- en: If you are, like me, passionate about AI, Data Science, or Psychology, please
    feel free to add me on [**LinkedIn**](https://www.linkedin.com/in/mgrootendorst/),
    follow me on [**Twitter**](https://twitter.com/MaartenGr), or subscribe to my
    [**Newsletter**](http://maartengrootendorst.substack.com/). You can also find
    some of my content on my [**Personal Website**](https://maartengrootendorst.com/).
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你和我一样，对 AI、数据科学或心理学充满热情，请随时在[**LinkedIn**](https://www.linkedin.com/in/mgrootendorst/)上添加我，在[**Twitter**](https://twitter.com/MaartenGr)上关注我，或订阅我的[**Newsletter**](http://maartengrootendorst.substack.com/)。你还可以在我的[**个人网站**](https://maartengrootendorst.com/)上找到一些我的内容。
- en: '*All images without a source credit were created by the author*'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '*所有未注明来源的图片均由作者创作*'
