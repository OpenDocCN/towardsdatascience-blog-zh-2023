- en: Correct Sampling Bias for Recommender Systems
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 正确采样偏差的推荐系统
- en: 原文：[https://towardsdatascience.com/correct-sampling-bias-for-recommender-systems-d2f6d9fdddec](https://towardsdatascience.com/correct-sampling-bias-for-recommender-systems-d2f6d9fdddec)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/correct-sampling-bias-for-recommender-systems-d2f6d9fdddec](https://towardsdatascience.com/correct-sampling-bias-for-recommender-systems-d2f6d9fdddec)
- en: What is sampling bias in recommendation, and how to correct them
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 推荐中的采样偏差是什么？如何纠正？
- en: '[](https://medium.com/@vuphuongthao9611?source=post_page-----d2f6d9fdddec--------------------------------)[![Thao
    Vu](../Images/9d44a2f199cdc9c29da72d9dc4971561.png)](https://medium.com/@vuphuongthao9611?source=post_page-----d2f6d9fdddec--------------------------------)[](https://towardsdatascience.com/?source=post_page-----d2f6d9fdddec--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----d2f6d9fdddec--------------------------------)
    [Thao Vu](https://medium.com/@vuphuongthao9611?source=post_page-----d2f6d9fdddec--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@vuphuongthao9611?source=post_page-----d2f6d9fdddec--------------------------------)[![Thao
    Vu](../Images/9d44a2f199cdc9c29da72d9dc4971561.png)](https://medium.com/@vuphuongthao9611?source=post_page-----d2f6d9fdddec--------------------------------)[](https://towardsdatascience.com/?source=post_page-----d2f6d9fdddec--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----d2f6d9fdddec--------------------------------)
    [Thao Vu](https://medium.com/@vuphuongthao9611?source=post_page-----d2f6d9fdddec--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----d2f6d9fdddec--------------------------------)
    ·8 min read·Oct 1, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----d2f6d9fdddec--------------------------------)
    ·阅读时间8分钟·2023年10月1日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/c5bbbc4ae0707318b5df926332f23574.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c5bbbc4ae0707318b5df926332f23574.png)'
- en: Photo by [NordWood Themes](https://unsplash.com/@nordwood?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由 [NordWood Themes](https://unsplash.com/@nordwood?utm_source=medium&utm_medium=referral)
    提供，[Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)上的照片
- en: '**Introduction**'
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**介绍**'
- en: Recommendations are ubiquitous in our digital lives, ranging from e-commerce
    giants to streaming services. However, hidden beneath every large recommender
    system lies a challenge that can significantly impact their effectiveness — sampling
    bias.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 推荐系统在我们的数字生活中无处不在，从电子商务巨头到流媒体服务。然而，每个大型推荐系统背后都隐藏着一个挑战，这可能显著影响其有效性——采样偏差。
- en: In this article, I will introduce how sampling bias occurs during training recommendation
    models and how we can solve this issue in practice.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我将介绍采样偏差如何在训练推荐模型期间发生，以及我们如何在实践中解决这个问题。
- en: Let’s dive in!
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们深入了解！
- en: Two-tower Recommendation systems
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 双塔推荐系统
- en: 'In general, we can formulate the recommendation problem as follows: given **query
    x** (which can contain user information, context, previously clicked items, etc.***)***,
    find the set of items ***{y1,.., yk}*** that the userwill likely be interested
    in.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，我们可以将推荐问题表述为：给定**查询 x**（可能包含用户信息、上下文、之前点击的项目等），找到用户可能感兴趣的项目集合***{y1,.., yk}***。
- en: 'One of the main challenges for large-scale recommender systems is low-latency
    requirements. However, user and item pools are vast and dynamic, so scoring every
    candidate and greedily finding the best one is impossible. Therefore, to meet
    the latency requirement, recommender systems are generally broken down into 2
    main stages: retrieval and ranking.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 大规模推荐系统的主要挑战之一是低延迟要求。然而，用户和项目池庞大且动态，因此对每个候选项进行评分并贪婪地找到最佳项是不可能的。因此，为了满足延迟要求，推荐系统通常被分解为两个主要阶段：检索和排序。
- en: '![](../Images/b05b10e53d68e6a6fb6593cfb9be96a4.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b05b10e53d68e6a6fb6593cfb9be96a4.png)'
- en: Multi-stage recommender systems (Image by the author)
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 多阶段推荐系统（作者提供的图像）
- en: 'Retrieval is a cheap and efficient way to quickly capture the top item candidates
    (a few hundred) from the vast candidate pool (millions or billions). Retrieval
    optimization is mainly about 2 objectives:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 检索是一种廉价且高效的方式，可以从庞大的候选池（数百万或数十亿）中快速捕捉到顶级候选项（几百个）。检索优化主要涉及两个目标：
- en: During the training phase, we want to encode users and items into embeddings
    that capture the user's behaviour and preferences.
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在训练阶段，我们希望将用户和项目编码为嵌入，以捕捉用户的行为和偏好。
- en: During the inference, we want to quickly retrieve relevant items through Approximate
    Nearest Neighbors (ANN).
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在推理过程中，我们希望通过近似最近邻（ANN）快速检索相关项目。
- en: For the first objective, one of the common approaches is the two-tower neural
    networks. The model gained its popularity for tackling the cold-start problems
    by incorporating item content features.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 对于第一个目标，常用的方法之一是双塔神经网络。该模型因通过结合物品内容特征来解决冷启动问题而获得了广泛的关注。
- en: In detail, queries and items are encoded by corresponding DNN towers so that
    the relevant (query, item) embeddings stay close to each other.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，查询和物品通过相应的DNN塔进行编码，以便相关的（查询，物品）嵌入保持彼此接近。
- en: '![](../Images/89574700e83ee66109c508f826ee5fe1.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/89574700e83ee66109c508f826ee5fe1.png)'
- en: Two-tower neural networks (Image by the author)
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 双塔神经网络（图片来源于作者）
- en: It can be reformated as a multi-class classification problem as follows.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 它可以被重新格式化为如下的多分类问题。
- en: 'Given query **x** and a list of items **(y1, y2,..)**, we output a probability
    vector with a size equal to the number of items predicting the probability **pi**
    that the user will interact with item **yi** (~1: yes/~0: no).'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '给定查询**x**和一系列物品**(y1, y2,..)**，我们输出一个概率向量，大小等于物品的数量，用于预测用户与物品**yi**互动的概率**pi**（~1:
    是/~0: 否）。'
- en: For training data, the positive label means the user has clicked/viewed/bought
    item **yi**. The negative label means that item **yi** has been shown to the user,
    but the user had no engagement.
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于训练数据，正标签表示用户已经点击/查看/购买了物品**yi**。负标签表示该物品**yi**已经展示给用户，但用户没有任何互动。
- en: The pair (x, yi) is passed through their corresponding tower and encoded into
    **(emb_x, emb_yi)**.
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于对（x，yi）对的处理，通过其相应的塔进行编码，得到**(emb_x, emb_yi)**。
- en: The similarity of **emb_x** and **emb_yi** can be measured by the dot product
    or cosine similarity, named **sim_i.**
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**emb_x**和**emb_yi**的相似度可以通过点积或余弦相似度来测量，称为**sim_i**。'
- en: The probability of interaction is measured by softmax probability as follows,
    with Z as the normalized term.
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 互动的概率通过如下的softmax概率进行测量，其中Z为标准化项。
- en: '![](../Images/b111de69d55254ab5eee1369c463e8ba.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b111de69d55254ab5eee1369c463e8ba.png)'
- en: Softmax probability (Image by the author)
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: Softmax概率（图片来源于作者）
- en: Folding problem
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 折叠问题
- en: 'The model goal is to generalize well on our dataset: it should be able to predict
    the score for (**x*, y***) pairs which don''t appear in the training data.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 模型的目标是对我们的数据集进行良好的泛化：它应该能够预测那些在训练数据中未出现的(**x*, y***)对的得分。
- en: To do so, we need to teach the model that, for each query embedding, good item
    embeddings should be pulled closer, while irrelevant items should be pushed far
    away. How can the model know about the unrelated items? Based on negative samples.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 为此，我们需要教会模型，对于每个查询嵌入，好的物品嵌入应该被拉得更近，而无关的物品则应被推得更远。模型如何知道这些无关的物品？基于负样本。
- en: One of the initial ways to build negative training data is to use the items
    that users viewed but did not click/view/purchase. However, each user only views
    and engages with a few items, so the feedback is relatively sparse and skewed
    toward a specific subgroup.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 构建负训练数据的最初方法之一是使用用户查看但未点击/查看/购买的物品。然而，每个用户仅查看和互动的物品很少，因此反馈相对稀疏，并且偏向特定的子群体。
- en: If we only train the model on (x, y) pairs that appear in our log, the model
    may learn well to keep queries and matching items close, but it may coincidentally
    put queries and completely irrelevant items near each other, too. This problem
    is known as "folding" [4].
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们仅在日志中出现的（x, y）对上训练模型，模型可能会很好地学习保持查询和匹配物品接近，但也可能偶然将查询和完全无关的物品放在一起。这个问题被称为“折叠”[4]。
- en: Therefore, we need to sample some extra negative data for training to teach
    the model to keep irrelevant embeddings far away from each other.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们需要采样一些额外的负数据进行训练，以教会模型将无关的嵌入保持远离彼此。
- en: But how can we collect these negative data effectively?
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 但我们如何有效地收集这些负数据？
- en: In-batch negative and the bias problem
  id: totrans-39
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 批次内负样本和偏差问题
- en: The simplest solution is to use in-batch negative, where we use other items
    in the same batch as negative samples.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 最简单的解决方案是使用批次内负样本，我们使用同一批次中的其他物品作为负样本。
- en: Given the whole batch of data **(x1, y1), (x2, y2)**,…, **(x_bn, y_bn)**, for
    each sample **(xi, yi)**, we will use **(xi, yj)** as in-batch negative samples.
    In the image below, blue lines mean positive samples and red lines mean the in-batch
    negative samples.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 给定整个数据批次**(x1, y1), (x2, y2)**,…, **(x_bn, y_bn)**，对于每个样本**(xi, yi)**，我们将使用**(xi,
    yj)**作为批次内负样本。在下图中，蓝色线条表示正样本，红色线条表示批次内负样本。
- en: '![](../Images/c32eeed54aeb149b5d9ba37f8592942e.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c32eeed54aeb149b5d9ba37f8592942e.png)'
- en: In-batch negative sampling. (Image by the author)
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 批量负采样。（作者提供的图像）
- en: In-batch negative training is an easy and memory-efficient way to reuse the
    item samples already in the batch rather than sampling and computing new samples
    from the whole item corpus.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 批量负训练是一种简单且节省内存的方式，可以重用批量中已经存在的项目样本，而无需从整个项目语料库中采样和计算新的样本。
- en: However, it tends to suffer from sampling bias, especially for skewed data distribution.
    For example, think of the YouTube platform; a few top videos can have billions
    of views, while most others only have a few hundred views. Those top videos will
    appear more frequently in training data than others. Therefore, they will be more
    likely to be used as "in-batch negative" and, hence, be overly penalized.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，它往往会受到采样偏差的影响，尤其是对于数据分布不均的情况。例如，考虑 YouTube 平台；一些热门视频可能有数十亿的观看次数，而大多数其他视频只有几百次观看。这些热门视频会在训练数据中比其他视频出现得更频繁。因此，它们更有可能被用作“批量负样本”，从而被过度惩罚。
- en: How can we fix this bias sample problem?
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何解决这个偏差样本问题？
- en: Importance sampling
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 重要性采样
- en: First, we need to define what is an ideal and unbiased sampling for our problem.
    Given the whole item corpus (y1, y2,…., yN), for each positive sample (x, y),
    we want to tell the model that all other items in the corpus are negative samples.
    Namely, each item y should be sampled with equal probability p(y) = 1/N.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要定义什么是针对我们问题的理想和无偏采样。给定整个项目语料库 (y1, y2,…., yN)，对于每个正样本 (x, y)，我们希望告知模型语料库中的所有其他项目都是负样本。即，每个项目
    y 应以相等的概率 p(y) = 1/N 进行采样。
- en: 'So ideally, the normalized term Z should be:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 因此理想情况下，归一化项 Z 应为：
- en: '![](../Images/d74bc2caf808d32b6ee4217883a98978.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d74bc2caf808d32b6ee4217883a98978.png)'
- en: Softmax sampling from the item pool (Image by the author)
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 从项目池中进行 Softmax 采样（作者提供的图像）
- en: However, that is impossible due to the substantial computation resources required
    to perform gradient updates on every item. A solution is to use "**importance
    sampling**" [2]. We still use the in-batch samples but will "correct" its distribution
    so it mirrors the unbiased sampling distribution above.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，由于对每个项目进行梯度更新所需的计算资源非常庞大，这几乎是不可能的。解决方案是使用 "**重要性采样**" [2]。我们仍然使用批量中的样本，但会“修正”其分布，使其反映上述无偏采样分布。
- en: Given an in-batch item set (y1, y2,..,yn) and **q(yi)** is the in-batch sampling
    probability of **yi**, we will correct **q** as follows.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个批量项目集 (y1, y2,..,yn) 和 **q(yi)** 是 **yi** 的批量采样概率，我们将按如下方式修正 **q**。
- en: '![](../Images/6e2acd53666de63d7df82e9cc4e5ed20.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6e2acd53666de63d7df82e9cc4e5ed20.png)'
- en: Bias sampling correction (Image by the author)
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 偏差采样校正（作者提供的图像）
- en: In the formula, **ri** acts as a "correction" weight that lets us utilize the
    in-batch items but still follow the correct uniform distribution.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在公式中，**ri** 充当“修正”权重，使我们能够利用批量中的项目，但仍遵循正确的均匀分布。
- en: That is the core idea of the bias correction. Next, let's learn how a big tech
    company, Google, corrects sampling bias for their recommender systems.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是偏差校正的核心思想。接下来，让我们了解一下大型科技公司 Google 如何校正其推荐系统的采样偏差。
- en: Negative sampling correction in practices
  id: totrans-58
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实践中的负采样校正
- en: Sampling-Bias-Corrected for Streaming data
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 针对流数据的采样偏差校正
- en: In this work [1], the authors introduce an algorithm to correct the bias sampling
    for streaming data. Since recommender systems are moving closer to real-time updates,
    online training with streaming data is necessary.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在这项工作 [1] 中，作者介绍了一种算法，用于校正流数据的偏差采样。由于推荐系统越来越接近实时更新，因此在线训练与流数据的结合是必要的。
- en: '![](../Images/dd630a0f47e0eccef475a0dd35fd0975.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/dd630a0f47e0eccef475a0dd35fd0975.png)'
- en: Online training for recommender system [1]
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 推荐系统的在线训练 [1]
- en: As mentioned above, we may need to access the whole training dataset to correct
    the sampling bias to estimate the distribution of item y, **p(y)**. However, measuring
    the distribution of streaming data is almost impossible.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 如上所述，我们可能需要访问整个训练数据集来校正采样偏差，以估计项目 y 的分布，**p(y)**。然而，测量流数据的分布几乎是不可能的。
- en: The paper proposed to estimate **p(yi)** based on the gap between two consecutive
    batches that contain item_i **B[h(yi)]**. Intuitively, the smaller the gap, the
    more popular y is.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 论文建议根据包含 item_i 的两个连续批次之间的差距来估计 **p(yi)**。直观地说，差距越小，y 的受欢迎程度越高。
- en: '![](../Images/8b4a28741b4cdeb37ea4c76e3cf11abf.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8b4a28741b4cdeb37ea4c76e3cf11abf.png)'
- en: Sample frequency estimation [1]
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 样本频率估计 [1]
- en: Based on the estimation of its popularity, we can decide how much y should be
    penalized as an in-batch negative. The more popular **y** is, the less impact
    it should have on the batch loss. Hence, it will be penalized less.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 基于其受欢迎程度的估计，我们可以决定**y**应被惩罚多少作为批内负样本。**y**越受欢迎，对批量损失的影响应该越小。因此，它将受到更少的惩罚。
- en: '![](../Images/f4e3b593b6b340b4f94cf1ac6f8c28f9.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f4e3b593b6b340b4f94cf1ac6f8c28f9.png)'
- en: Sampling correction for streaming data (Image by the author)
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 流数据的采样校正（图片由作者提供）
- en: This approach does not require fixed item vocabulary and can still produce unbiased
    estimations. Moreover, it can adapt to online item distribution changes over time.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法不需要固定的项词汇表，仍然可以产生无偏估计。此外，它可以适应在线项目分布随时间的变化。
- en: To study the strategy impact, the team has experimented with offline evaluation
    on YouTube video recommendation, where the index of approximately 10M videos is
    periodically built every few hours. During the 7-day evaluation period, 10% of
    the data log is held out for evaluation. The approach with sampling correction
    has achieved significant gains in offline recall metrics compared to plain negative
    sampling.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 为了研究策略的影响，团队在YouTube视频推荐上进行了离线评估实验，其中大约10M个视频的索引每隔几个小时周期性构建。在7天的评估期间，10%的数据日志被保留用于评估。与普通负采样相比，采样校正的方法在离线召回指标上取得了显著提升。
- en: '![](../Images/73eb3ef0842f03a02236f54ad1ff0589.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/73eb3ef0842f03a02236f54ad1ff0589.png)'
- en: Recall@K for retrieving clicked video pages from a 10M video corpus on YouTube.
    [1]
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 从YouTube上的10M视频语料库中检索点击视频页面的Recall@K。[1]
- en: Encouraged by the positive offline result, the team continued to try the new
    retrieval methods online. The engagement metric is also remarkably improved, proving
    the effectiveness of sampling correction.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 受到积极离线结果的鼓励，团队继续在线尝试新的检索方法。参与度指标也显著提高，证明了采样校正的有效性。
- en: '![](../Images/30d245a8e348ea893fe622f70a8a93fa.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/30d245a8e348ea893fe622f70a8a93fa.png)'
- en: YouTube live experiment results [1]
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: YouTube直播实验结果[1]
- en: Mixed negative sampling
  id: totrans-77
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 混合负采样
- en: This work [3] takes one step further to solve the bias issue. The paper mentioned
    another type of bias, "**selection bias, "** where items with no user feedback
    will never be included in training data. Therefore, the model lacks generalizability
    to differentiate the long-tail items from others.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 这项工作[3]更进一步解决了偏差问题。论文提到了一种类型的偏差，“**选择性偏差**”，即没有用户反馈的项目将永远不会被纳入训练数据。因此，模型缺乏区分长尾项和其他项的通用性。
- en: The authors proposed that, besides in-batch negative samples, we also sample
    negative data from the entire item corpus. Therefore, all items have a chance
    to serve as negative samples. This can be especially helpful for the model to
    learn about fresh and long-tail items.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提出，除了批内负样本外，我们还从整个项目语料库中抽取负样本。因此，所有项目都有机会作为负样本。这对于模型了解新鲜和长尾项尤其有帮助。
- en: Besides the in-batch items B, we uniformly sample items from the whole item
    corpus as an extra negative sample, named B'. The sampling will then be "corrected"
    similarly to the above approach.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 除了批内项目B外，我们还从整个项目语料库中均匀地抽取项目作为额外的负样本，命名为B'。然后，采样将“校正”到上述方法类似。
- en: '![](../Images/1d981da885c4fcea41b485ad493cd9da.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1d981da885c4fcea41b485ad493cd9da.png)'
- en: Mixed negative sampling [3]
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 混合负采样[3]
- en: 'To prove the impact of mixed negative sampling, the team conducted experiments
    in the Google Play app recommendation. The baseline is compared with 2 negative
    sampling strategies: in-batch negative only and mixed negative.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 为了证明混合负采样的影响，团队在Google Play应用推荐中进行了实验。基线与两种负采样策略进行了比较：仅批内负采样和混合负采样。
- en: Compared to the baseline, both negative-sampling approaches incorporated item
    contents as features to improve the generability. However, the in-batch negative
    sampling group performed worse than the baseline in both offline and online evaluations.
    For this group, the team observed a few irrelevant long-tail apps in the model's
    recommendations. This supports the hypothesis that in-batch negative suffers from
    selection bias. Long-tail items were not frequently included in training data
    and, subsequently, not demoted as they should be.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 与基线相比，两种负采样方法都将项目内容作为特征以改善通用性。然而，批内负采样组在离线和在线评估中表现均不如基线。对于这一组，团队观察到模型推荐中出现了一些无关的长尾应用。这支持了批内负采样存在选择性偏差的假设。长尾项没有频繁地被纳入训练数据，因此没有被应有地降级。
- en: In contrast, mixed negative sampling was superior in both offline and online
    evaluation. Namely, for online abtest, the team focuses on high-quality instalments
    where users install and use the apps afterwards. The mixed negative sampling group
    won by a large margin of 1.54%.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，混合负采样在离线和在线评估中均表现优越。即，对于在线AB测试，团队关注高质量的安装用户，即那些安装并使用应用程序的用户。混合负采样组以1.54%的显著差距获胜。
- en: '![](../Images/d2330ecb94e7cb867eb991bc040f81e1.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d2330ecb94e7cb867eb991bc040f81e1.png)'
- en: '*Recall@K of several models in the Google Play dataset [3]*'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '*Google Play数据集中几个模型的Recall@K [3]*'
- en: '![](../Images/90e7a249b14c92940defc634ab4ea47c.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/90e7a249b14c92940defc634ab4ea47c.png)'
- en: '*Live experiment results of 3 models in Google Play. [3]*'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '*Google Play上3个模型的实时实验结果。[3]*'
- en: References
  id: totrans-90
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] Yi, Xinyang, et al. "Sampling-bias-corrected neural modeling for large
    corpus item recommendations." *Proceedings of the 13th ACM Conference on Recommender
    Systems*. 2019.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] 易欣阳, 等. "用于大规模语料库项目推荐的采样偏差修正神经建模。" *第十三届ACM推荐系统会议论文集*。2019年。'
- en: '[2] [https://en.wikipedia.org/wiki/Importance_sampling](https://en.wikipedia.org/wiki/Importance_sampling)'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] [https://en.wikipedia.org/wiki/Importance_sampling](https://en.wikipedia.org/wiki/Importance_sampling)'
- en: '[3] Yang, Ji, et al. "Mixed negative sampling for learning two-tower neural
    networks in recommendations." *Companion Proceedings of the Web Conference 2020*.
    2020.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] 杨继, 等. "用于推荐的双塔神经网络的混合负采样。" *2020年Web会议论文集*。2020年。'
- en: '[4] Xin, Doris, et al. "Folding: Why good models sometimes make spurious recommendations."
    *Proceedings of the Eleventh ACM Conference on Recommender Systems*. 2017.'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] 辛多丽丝, 等. "折叠：为什么好的模型有时会做出虚假的推荐。" *第十一届ACM推荐系统会议论文集*。2017年。'
