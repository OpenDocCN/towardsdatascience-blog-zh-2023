- en: 'Reinforcement Learning for Inventory Optimization Series III: Sim-to-Real Transfer
    for the RL Model'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 强化学习在库存优化中的应用系列 III：从模拟到现实的 RL 模型转移
- en: 原文：[https://towardsdatascience.com/reinforcement-learning-for-inventory-optimization-series-iii-sim-to-real-transfer-for-the-rl-model-d260c3b8277d?source=collection_archive---------10-----------------------#2023-01-30](https://towardsdatascience.com/reinforcement-learning-for-inventory-optimization-series-iii-sim-to-real-transfer-for-the-rl-model-d260c3b8277d?source=collection_archive---------10-----------------------#2023-01-30)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/reinforcement-learning-for-inventory-optimization-series-iii-sim-to-real-transfer-for-the-rl-model-d260c3b8277d?source=collection_archive---------10-----------------------#2023-01-30](https://towardsdatascience.com/reinforcement-learning-for-inventory-optimization-series-iii-sim-to-real-transfer-for-the-rl-model-d260c3b8277d?source=collection_archive---------10-----------------------#2023-01-30)
- en: Bridge the gap between the simulator and real world
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 弥合模拟器与现实世界之间的差距
- en: '[](https://medium.com/@guanx92?source=post_page-----d260c3b8277d--------------------------------)[![Guangrui
    Xie](../Images/def9aa637424a88d75a6a3bb103350bc.png)](https://medium.com/@guanx92?source=post_page-----d260c3b8277d--------------------------------)[](https://towardsdatascience.com/?source=post_page-----d260c3b8277d--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----d260c3b8277d--------------------------------)
    [Guangrui Xie](https://medium.com/@guanx92?source=post_page-----d260c3b8277d--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@guanx92?source=post_page-----d260c3b8277d--------------------------------)[![Guangrui
    Xie](../Images/def9aa637424a88d75a6a3bb103350bc.png)](https://medium.com/@guanx92?source=post_page-----d260c3b8277d--------------------------------)[](https://towardsdatascience.com/?source=post_page-----d260c3b8277d--------------------------------)[![数据科学前沿](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----d260c3b8277d--------------------------------)
    [Guangrui Xie](https://medium.com/@guanx92?source=post_page-----d260c3b8277d--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F495b92f0c66d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-for-inventory-optimization-series-iii-sim-to-real-transfer-for-the-rl-model-d260c3b8277d&user=Guangrui+Xie&userId=495b92f0c66d&source=post_page-495b92f0c66d----d260c3b8277d---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----d260c3b8277d--------------------------------)
    ·8 min read·Jan 30, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fd260c3b8277d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-for-inventory-optimization-series-iii-sim-to-real-transfer-for-the-rl-model-d260c3b8277d&user=Guangrui+Xie&userId=495b92f0c66d&source=-----d260c3b8277d---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F495b92f0c66d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-for-inventory-optimization-series-iii-sim-to-real-transfer-for-the-rl-model-d260c3b8277d&user=Guangrui+Xie&userId=495b92f0c66d&source=post_page-495b92f0c66d----d260c3b8277d---------------------post_header-----------)
    发表在 [数据科学前沿](https://towardsdatascience.com/?source=post_page-----d260c3b8277d--------------------------------)
    · 8 分钟阅读 · 2023年1月30日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fd260c3b8277d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-for-inventory-optimization-series-iii-sim-to-real-transfer-for-the-rl-model-d260c3b8277d&user=Guangrui+Xie&userId=495b92f0c66d&source=-----d260c3b8277d---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fd260c3b8277d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-for-inventory-optimization-series-iii-sim-to-real-transfer-for-the-rl-model-d260c3b8277d&source=-----d260c3b8277d---------------------bookmark_footer-----------)![](../Images/57bb3512be81d81faddf8f3fd7b5317c.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fd260c3b8277d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-for-inventory-optimization-series-iii-sim-to-real-transfer-for-the-rl-model-d260c3b8277d&source=-----d260c3b8277d---------------------bookmark_footer-----------)![](../Images/57bb3512be81d81faddf8f3fd7b5317c.png)'
- en: Photo by [Suad Kamardeen](https://unsplash.com/@suadkamardeen?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 照片由 [Suad Kamardeen](https://unsplash.com/@suadkamardeen?utm_source=medium&utm_medium=referral)
    提供，来源于 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: 'Update: this article is the third article in my blog series *Reinforcement
    Learning for Inventory Optimization*.Below are the links to the other articles
    in the same series. Please check them out if you are interested.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 更新：这篇文章是我博客系列*库存优化中的强化学习*的第三篇文章。以下是同一系列其他文章的链接。如果您感兴趣，请查看。
- en: '[*Reinforcement Learning for Inventory Optimization Series I: An RL Model for
    Single Retailers*](https://medium.com/towards-data-science/a-reinforcement-learning-based-inventory-control-policy-for-retailers-ac35bc592278)'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '[*库存优化中的强化学习系列 I：单一零售商的 RL 模型*](https://medium.com/towards-data-science/a-reinforcement-learning-based-inventory-control-policy-for-retailers-ac35bc592278)'
- en: '[*Reinforcement Learning for Inventory Optimization Series II: An RL Model
    for A Multi-Echelon Network*](https://medium.com/towards-data-science/reinforcement-learning-for-inventory-optimization-series-ii-an-rl-model-for-a-multi-echelon-921857acdb00)'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '[*库存优化中的强化学习系列 II：多级网络的 RL 模型*](https://medium.com/towards-data-science/reinforcement-learning-for-inventory-optimization-series-ii-an-rl-model-for-a-multi-echelon-921857acdb00)'
- en: In previous articles, I built two RL models for a single retailer and a multi-echelon
    supply chain network. In these two articles, I trained the RL model using a simulator
    of the inventory system based on a historical demand dataset following a mixture
    of different normal distributions whose parameters depend on days of week. Then
    I tested the RL models on a test set in which the demand data follow the same
    distribution as the historical training data. This is essentially assuming the
    simulator perfectly matches the real world where we would apply the RL models
    to, and historical demand data perfectly represent the future demand pattern.
    However, this assumption is rarely true in real-world applications.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的文章中，我为单一零售商和多级供应链网络构建了两个 RL 模型。在这两篇文章中，我使用了基于历史需求数据的库存系统模拟器来训练 RL 模型，这些数据遵循不同正态分布的混合，其参数依赖于星期几。然后，我在一个测试集上测试了
    RL 模型，其中的需求数据与历史训练数据的分布相同。这实际上假设模拟器与我们应用 RL 模型的现实世界完全匹配，且历史需求数据完全代表未来需求模式。然而，这一假设在实际应用中很少成立。
- en: How to deal with the gap between the distributions in the training set and test
    set is a well-known problem in the general context of machine learning. The essence
    of this problem is the bias vs variance trade-off. If we rely too much on the
    training set to train our model, then the model is prone to overfitting, meaning
    it will only well on the training set but not on the test set. The same issue
    applies here in the RL field.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 如何处理训练集与测试集之间分布的差距是机器学习领域中的一个著名问题。这个问题的核心是偏差与方差的权衡。如果我们过于依赖训练集来训练模型，那么模型容易过拟合，意味着它只会在训练集上表现良好，而在测试集上表现不佳。在
    RL 领域也是如此。
- en: Performance Deterioration due to the Gap between Simulator and Real World
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 由于模拟器与现实世界之间的差距导致的性能退化
- en: Taking the RL models I built previously as an example, the models were trained
    through a large number of episodes on the same historical demand data, hence it
    is highly likely that the inventory policies learned from the trained RL models
    are overfitted to the demand pattern represented by the historical demand data.
    The inventory policies will perform well if the same demand pattern continues
    in the future, but the performance will deteriorate if the future demand pattern
    deviates from the historical one.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 以我之前构建的 RL 模型为例，这些模型通过大量的历史需求数据进行训练，因此很可能从训练的 RL 模型中学习到的库存政策过度拟合了历史需求数据所代表的需求模式。如果未来的需求模式继续保持相同，库存政策将表现良好，但如果未来的需求模式偏离历史模式，性能将会退化。
- en: As a numerical illustration, let’s assume that the future demand distribution
    in the real world deviates from the historical demand data in the simulator used
    for training the RL models. I took the RL model trained in [my first article](https://medium.com/towards-data-science/a-reinforcement-learning-based-inventory-control-policy-for-retailers-ac35bc592278)
    and applied it to two future demand scenarios in real world. In one of the scenarios,
    we assume the means of the mixture of the normal distributions will increase by
    1, while in the other scenario, the means will decrease by 1\. The demand distribution
    structure is given by the table below.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一个数值示例，假设未来需求分布在现实世界中偏离了用于训练RL模型的模拟器中的历史需求数据。我将[我第一篇文章](https://medium.com/towards-data-science/a-reinforcement-learning-based-inventory-control-policy-for-retailers-ac35bc592278)中训练的RL模型应用于现实世界的两个未来需求场景。在一个场景中，我们假设正态分布的均值增加1，而在另一个场景中，均值减少1。需求分布结构如下表所示。
- en: '![](../Images/ae64463d93bb6ab33b286a83fb4967d3.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ae64463d93bb6ab33b286a83fb4967d3.png)'
- en: Two future/real-world demand distribution scenarios (Image by author)
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 两个未来/现实世界需求分布场景（图片由作者提供）
- en: For both of the demand increase scenario and demand decrease scenario (second
    and third columns in the table above), I generated 100 demand datasets which consist
    of 52 weeks of data using their own demand distribution setting. Then I applied
    the DQN policy trained on the historical/simulator demand data (first column in
    the table above) to both scenarios, the corresponding average profits obtained
    are given in the table below.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 对于需求增加场景和需求减少场景（如上表的第二列和第三列），我生成了100个需求数据集，每个数据集包含52周的数据，使用其自身的需求分布设置。然后，我将基于历史/模拟器需求数据（如上表的第一列）训练的DQN策略应用于这两个场景，得到的平均利润如下表所示。
- en: '![](../Images/875d73b680def5abba7790abdb6faf55.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/875d73b680def5abba7790abdb6faf55.png)'
- en: Average profits obtained by applying the DQN policy trained on historical/simulator
    data to each demand scenario (Image by author)
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 应用训练于历史/模拟器数据的DQN策略到每个需求场景中获得的平均利润（图片由作者提供）
- en: To assess how well the trained DQN policy performs, for each of the demand scenarios,
    let’s assume we know what the future/real-world demand distribution will deviate
    ahead of time, and generate a training set consisting of 52 weeks of demand data
    using its own future/real-world demand distribution setting. Then we train a brand
    new RL model using its own training set for each scenario and apply the new RL
    model to each scenario. The corresponding average profits obtained are given in
    the table below.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 为了评估训练的DQN策略表现如何，对于每个需求场景，假设我们知道未来/现实世界需求分布将如何偏离，并生成一个由52周需求数据组成的训练集，使用其自身的未来/现实世界需求分布设置。然后，我们为每个场景训练一个全新的RL模型，并将新RL模型应用于每个场景。得到的平均利润如下表所示。
- en: '![](../Images/46b9f9db2d6275890dd36142ebd0b90a.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/46b9f9db2d6275890dd36142ebd0b90a.png)'
- en: Average profits obtained by applying the DQN policy trained on each scenario’s
    training set to each demand scenario (Image by author)
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 应用训练于每个场景的训练集的DQN策略到每个需求场景中获得的平均利润（图片由作者提供）
- en: As we can see from the comparison in the two tables above ($25077.61 vs $27399.79
    and $13890.99 vs $14707.44), there is a performance deterioration in the RL model
    if there is a gap between the simulator and real world.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 从上面两张表的比较（$25077.61 对比 $27399.79 和 $13890.99 对比 $14707.44）可以看出，如果模拟器与现实世界之间存在差距，RL模型的性能会下降。
- en: Bridging the Gap Using Domain Randomization
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用领域随机化弥合差距
- en: To bridge the gap between the simulator and real world, in real applications,
    one may choose to select a shorter period of most recent historical demand data,
    and assume this shorter period represents the future demand pattern or trend better.
    Then we frequently retrain the RL model using the recently updated shorter period
    of historical demand data. However, we can also try to address this problem during
    the training process in the first place using the concept of domain randomization.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 为了弥合模拟器和现实世界之间的差距，在实际应用中，可以选择最近一段时间的历史需求数据，并假设这段较短的时间更好地代表了未来需求模式或趋势。然后，我们可以通过使用最近更新的较短历史需求数据频繁重新训练RL模型。然而，我们也可以尝试在训练过程中使用领域随机化的概念来解决这个问题。
- en: Domain randomization is a technique typically used for sim-to-real transfer
    when applying RL to robotics. The applications of RL in robotics also face the
    issue of reality gap, since the gap between the simulation and real world degrades
    the performance of the policies once the RL models are transferred into real robots[1].
    In robotics context, the core idea of domain randomization is by randomizing the
    the physical parameters of the simulated environment (e.g., friction coefficients
    and vision properties such as objects’ appearance) when training the RL model,
    the RL model will experience situations more like the real environment, hence
    the learned policies will generalize better to the real environment. The figure
    below illustrates the intuition behind domain randomization.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 领域随机化是一种通常用于将模拟转移到实际的技术，尤其是在将强化学习应用于机器人时。机器人中的强化学习应用也面临现实差距的问题，因为模拟与现实世界之间的差距会降低将强化学习模型转移到实际机器人中的策略的性能[1]。在机器人领域，领域随机化的核心思想是通过在训练强化学习模型时随机化模拟环境的物理参数（例如摩擦系数和视觉属性如物体外观），强化学习模型将体验到更接近真实环境的情况，从而学习到的策略会更好地推广到实际环境中。下图展示了领域随机化的直觉。
- en: '![](../Images/7c7a3d1976c0b74a9a3e3e6a0e95d41b.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7c7a3d1976c0b74a9a3e3e6a0e95d41b.png)'
- en: Intuition behind domain randomization (Image from reference [1])
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 领域随机化的直觉（图片来源于参考文献 [1]）
- en: In the context of inventory optimization, to implement domain randomization,
    we can add randomization to the historical demand data. To be specific, we extend
    the historical demand dataset by adding normally distributed noise to the demand
    observations. The implementation of this idea is given in the code block below.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在库存优化的背景下，为了实现领域随机化，我们可以对历史需求数据进行随机化。具体而言，我们通过向需求观察值中添加正态分布噪声来扩展历史需求数据集。该想法的实现见下方代码块。
- en: '[PRE0]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Here I sort of created 10 historical demand scenario by adding a random noise
    to the demand observation in each scenario. Then I appended the scenarios together
    as the entire historical demand data used for training, pretending that we have
    a 10 years’ of demand data. Note that the essence of this idea also aligns with
    the data augmentation technique in computer vision tasks where we manipulate the
    images to enrich the training set to avoid overfitting.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我通过向每个场景的需求观察值中添加随机噪声来创建了10个历史需求场景。然后我将这些场景组合在一起，作为用于训练的完整历史需求数据，假装我们有10年的需求数据。请注意，这个想法的本质也与计算机视觉任务中的数据增强技术相一致，我们通过操控图像来丰富训练集，以避免过拟合。
- en: I tried different values for the mean of the normal distribution used for generating
    the random noise. Initially I thought it makes sense to use a 0 mean, since it
    will more likely generate equal numbers of demand increase and decrease scenarios.
    Interestingly, after trying out different values, I found making the mean above
    0 (more demand increase scenarios) gave better test results for this particular
    example. It might be due to the fact that the demand data are truncated at 0,
    hence although we generate noise with mean 0, there is not too much room for the
    demand distribution to shift left. So generating noise with a mean of a positive
    number will let the RL model learn more useful knowledge.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 我尝试了不同的正态分布均值来生成随机噪声。最初，我认为使用均值为0是有意义的，因为它更有可能生成相等数量的需求增加和减少场景。有趣的是，在尝试了不同的值之后，我发现将均值设置为0以上（更多的需求增加场景）在这个特定的例子中得到了更好的测试结果。这可能是因为需求数据在0处被截断，因此尽管我们生成均值为0的噪声，但需求分布向左移动的空间不大。所以生成均值为正数的噪声会使强化学习模型学到更多有用的知识。
- en: Now we train the DQN model using the new 10 years’ historical demand data obtained
    by domain randomization and then apply the learned DQN policy to the demand increase
    and decrease scenarios (detailed setting in the first table) described in the
    previous section. The following table shows the average profits obtained in each
    scenario.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们使用通过领域随机化获得的新10年历史需求数据来训练DQN模型，然后将学习到的DQN策略应用于上一节中描述的需求增加和减少场景（详见第一个表）。下表展示了每个场景中获得的平均利润。
- en: '![](../Images/360f435930f658b9dada5c27e199242f.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/360f435930f658b9dada5c27e199242f.png)'
- en: Average profits obtained in each scenario using the DQN policy trained after
    domain randomization (Image by author)
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 使用领域随机化后训练的DQN策略在每个场景中获得的平均利润（图片由作者提供）
- en: We see that there is a clear improvement as compared to the results without
    domain randomization ($26432.61 vs $25077.61 in increase scenario, $14311.52 vs
    $13890.99 in decrease scenario).
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到，与没有领域随机化的结果相比，明显有所改善（增加场景中为 $26432.61 对 $25077.61，减少场景中为 $14311.52 对 $13890.99）。
- en: To show the performance improvement is not obtained by chance, I further created
    more randomized test demand scenarios by adding a random noise to the means and
    variances of the mixture of demand distributions. See the code below.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 为了表明性能提升不是偶然获得的，我进一步创建了更多随机化的测试需求场景，通过对需求分布的均值和方差添加随机噪声。见下面的`代码`。
- en: '[PRE1]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The DQN policy before domain randomization gives an average profit of $20163.84
    on this test set, and the DQN policy after domain randomization gives an average
    profit of $21887.77, which still shows a performance improvement.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在领域随机化之前，DQN 策略在该测试集上的平均利润为 $20163.84，而领域随机化之后的 DQN 策略平均利润为 $21887.77，这仍然显示出性能的提升。
- en: '**Conclusion**'
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**结论**'
- en: In this article, I focus on pointing out and addressing the gap between training/simulation
    and test/real world environments for the previous RL model I built for inventory
    optimization. It’s good to see that techniques such as domain randomization in
    the robotics field can also be effective in the inventory optimization area. Experimental
    results suggest that it should be a good practice to manipulate historical demand
    data using techniques like domain randomization to enrich the training set, so
    that the RL model trained in a lab can better generalize to the real world.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我重点指出并解决了我为库存优化构建的 RL 模型在训练/仿真和测试/现实世界环境之间的差距。很高兴看到，像领域随机化这样的技术在机器人领域中也能在库存优化领域发挥作用。实验结果表明，使用领域随机化等技术来操控历史需求数据以丰富训练集应该是一种良好的实践，这样实验室中训练的
    RL 模型可以更好地推广到现实世界。
- en: References
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] [Sim-to-Real Transfer in Deep Reinforcement Learning for Robotics: a Survey](https://arxiv.org/abs/2009.13303)'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] [深度强化学习在机器人领域的仿真到现实迁移：综述](https://arxiv.org/abs/2009.13303)'
