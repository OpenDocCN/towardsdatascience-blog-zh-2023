- en: Defining Interpretable Features
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 定义可解释的特征
- en: 原文：[https://towardsdatascience.com/defining-interpretable-features-ebd7ed94897?source=collection_archive---------4-----------------------#2023-01-14](https://towardsdatascience.com/defining-interpretable-features-ebd7ed94897?source=collection_archive---------4-----------------------#2023-01-14)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/defining-interpretable-features-ebd7ed94897?source=collection_archive---------4-----------------------#2023-01-14](https://towardsdatascience.com/defining-interpretable-features-ebd7ed94897?source=collection_archive---------4-----------------------#2023-01-14)
- en: '![](../Images/3c249d3744da050be86ce0f32278375a.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3c249d3744da050be86ce0f32278375a.png)'
- en: Photo by [Kevin Ku](https://unsplash.com/@ikukevk?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 照片由 [Kevin Ku](https://unsplash.com/@ikukevk?utm_source=medium&utm_medium=referral)
    提供，发布在 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: A summary of the findings and developed taxonomy developed by MIT researchers.
  id: totrans-4
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 这是MIT研究人员总结的发现和开发的分类法。
- en: '[](https://medium.com/@upadhyan?source=post_page-----ebd7ed94897--------------------------------)[![Nakul
    Upadhya](../Images/336cb21272e9b1f098177adbde50e92e.png)](https://medium.com/@upadhyan?source=post_page-----ebd7ed94897--------------------------------)[](https://towardsdatascience.com/?source=post_page-----ebd7ed94897--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----ebd7ed94897--------------------------------)
    [Nakul Upadhya](https://medium.com/@upadhyan?source=post_page-----ebd7ed94897--------------------------------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@upadhyan?source=post_page-----ebd7ed94897--------------------------------)[![Nakul
    Upadhya](../Images/336cb21272e9b1f098177adbde50e92e.png)](https://medium.com/@upadhyan?source=post_page-----ebd7ed94897--------------------------------)[](https://towardsdatascience.com/?source=post_page-----ebd7ed94897--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----ebd7ed94897--------------------------------)
    [Nakul Upadhya](https://medium.com/@upadhyan?source=post_page-----ebd7ed94897--------------------------------)'
- en: ·
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F4d9dddc62a80&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdefining-interpretable-features-ebd7ed94897&user=Nakul+Upadhya&userId=4d9dddc62a80&source=post_page-4d9dddc62a80----ebd7ed94897---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----ebd7ed94897--------------------------------)
    ·9 min read·Jan 14, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Febd7ed94897&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdefining-interpretable-features-ebd7ed94897&user=Nakul+Upadhya&userId=4d9dddc62a80&source=-----ebd7ed94897---------------------clap_footer-----------)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F4d9dddc62a80&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdefining-interpretable-features-ebd7ed94897&user=Nakul+Upadhya&userId=4d9dddc62a80&source=post_page-4d9dddc62a80----ebd7ed94897---------------------post_header-----------)
    发表在 [数据科学前沿](https://towardsdatascience.com/?source=post_page-----ebd7ed94897--------------------------------)
    ·9分钟阅读·2023年1月14日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Febd7ed94897&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdefining-interpretable-features-ebd7ed94897&user=Nakul+Upadhya&userId=4d9dddc62a80&source=-----ebd7ed94897---------------------clap_footer-----------)'
- en: --
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Febd7ed94897&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdefining-interpretable-features-ebd7ed94897&source=-----ebd7ed94897---------------------bookmark_footer-----------)'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Febd7ed94897&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdefining-interpretable-features-ebd7ed94897&source=-----ebd7ed94897---------------------bookmark_footer-----------)'
- en: 'In February 2022, researchers at the [Data to AI (DAI) group at MIT](https://dai.lids.mit.edu/)
    released a paper called “The Need for Interpretable Features: Motivation and Taxonomy”
    [1]. In this post, I aim to summarize some of the main points and contributions
    of these authors and discuss some of the potential implications and critiques
    of their work. I highly recommend reading the [original paper](https://arxiv.org/abs/2202.11748)
    if you find any of this intriguing. Additionally, if you’re new to Interpretable
    Machine Learning, I highly recommend [Christopher Molnar’s free book](https://christophm.github.io/interpretable-ml-book/)
    [2]. While the definition of interpretability/explainability often changes in
    different publications [1], this book provides a strong foothold in understanding
    the field.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在2022年2月，麻省理工学院[数据到人工智能（DAI）小组](https://dai.lids.mit.edu/)的研究人员发布了一篇名为《可解释特征的必要性：动机和分类法》的论文[1]。在这篇文章中，我旨在总结这些作者的一些主要观点和贡献，并讨论他们工作的潜在影响和批评。如果你对这些内容感兴趣，我强烈推荐阅读[原始论文](https://arxiv.org/abs/2202.11748)。此外，如果你对可解释机器学习不太熟悉，我强烈推荐[Christopher
    Molnar的免费书籍](https://christophm.github.io/interpretable-ml-book/) [2]。虽然可解释性/解释性的定义在不同的出版物中常常变化[1]，但这本书提供了理解该领域的坚实基础。
- en: The core finding of the paper is that **even with highly interpretable models
    like Linear Regression, non-interpretable features can result in impossible-to-understand
    explanations** (ex. a weight of 4 on the feature x12 means nothing to most people).
    With this in mind, the paper contributes a categorization of stakeholders, real-world
    use cases for interpretable features, classification of various feature qualities,
    and possible interpretable feature transformations that help data scientists develop
    understandable features.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 论文的核心发现是**即使是像线性回归这样高度可解释的模型，非可解释的特征也可能导致难以理解的解释**（例如，特征x12的权重为4对大多数人来说毫无意义）。鉴于此，论文提供了利益相关者的分类、可解释特征的现实世界应用场景、各种特征质量的分类，以及可能的可解释特征转换，这些都帮助数据科学家开发易于理解的特征。
- en: Definition of Stakeholders
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 利益相关者的定义
- en: 'The first contribution of this paper is expanding the main user types who may
    benefit from ML explanations proposed by Preece et al. [3] as well as defining
    some of their interests. While Preece et al. proposed 4 main types of stakeholders,
    the authors of this paper expand that list to 5:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 论文的第一个贡献是扩展了Preece等人提出的可能受益于机器学习解释的主要用户类型，并定义了一些他们的兴趣。虽然Preece等人提出了4种主要的利益相关者类型，但本文的作者将该列表扩展到5种：
- en: 'Developers: The ones who train, test, and deploy ML models and are interested
    in features to improve model performance.'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 开发者：那些训练、测试和部署机器学习模型的人，他们关注特征以提高模型性能。
- en: 'Theorists: The ones who are interested in advancing ML theory and are interested
    in features to understand their impact on models’ inner workings.'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理论家：那些对推进机器学习理论感兴趣的人，他们关注特征，以了解其对模型内部机制的影响。
- en: 'Ethicists: The ones who are interested in the fairness of the models and are
    interested in features to ensure ethical uses of models.'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 伦理学家：那些对模型的公平性感兴趣的人，他们关注特征，以确保模型的伦理使用。
- en: 'Decision Makers: The ones who intake the result of models to complete tasks
    and decisions. They are not explicitly interested in features but need explanations
    to ensure their decisions are made with sound info.'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 决策者：那些获取模型结果以完成任务和决策的人。他们对特征本身不感兴趣，但需要解释以确保他们的决策基于可靠的信息。
- en: 'Impacted Users: These are individuals impacted by the models and their use,
    but do not directly interact with the models unless it’s to understand the impact
    on themselves.'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 受影响的用户：这些是受模型及其使用影响的个人，但不会直接与模型互动，除非是为了理解对他们自身的影响。
- en: Each of the various users has different needs when it comes to feature engineering,
    and these needs often conflict with each other. While a decision maker may want
    the simplest features in the model for better interpretability, a developer may
    opt for complicated transformations that engineer a feature to be ultra-predictive.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 各种用户在特征工程方面有不同的需求，这些需求往往相互冲突。决策者可能希望模型中的特征越简单越好，以便更好地解释，而开发者可能会选择复杂的转换，以使特征具有极高的预测能力。
- en: Real-World Use Cases
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 现实世界应用场景
- en: Along with presenting stakeholders, the authors present 5 real-world domains
    in which they ran into roadblocks when attempting to explain their developed models.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 除了介绍利益相关者之外，作者还介绍了5个实际领域，在这些领域中，他们在试图解释自己开发的模型时遇到了障碍。
- en: Case Studies
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 案例研究
- en: '**Child Welfare**'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '**儿童福利**'
- en: In this case study, the DAI team collaborated with social workers and scientists
    (serving as decision-makers and ethicists) to develop an explainable LASSO model
    with over 400 features that outputted a risk score for potential child abuse cases.
    During this process, the DAI team found that most of the distrust surrounding
    the model stemmed from the features rather than the ML algorithm. One prominent
    point of confusion was around the wording surrounding one-hot encoded categorical
    features (ex. `role of child is sibling == False`). Additionally, many of the
    social workers and scientists had concerns about features that they deemed to
    be unrelated to the predictive task at hand based on their subject matter expertise.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在这项案例研究中，DAI团队与社会工作者和科学家（担任决策者和伦理学家）合作，开发了一个解释性LASSO模型，该模型拥有超过400个特征，并输出潜在儿童虐待案件的风险评分。在此过程中，DAI团队发现模型的大多数不信任来源于特征而非机器学习算法。一个突出的问题是关于一热编码分类特征（例如`role
    of child is sibling == False`）的措辞。此外，许多社会工作者和科学家对他们认为与预测任务无关的特征表示担忧，基于他们的主题领域专业知识。
- en: '**Education**'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '**教育**'
- en: In the domain of online education, the authors worked on adding interpretability
    to various decision tasks related to massively open online courses (ex. free courses
    on Coursera, edX, etc.). While working with various course developers and instructors,
    the authors found that the most useful features were ones that combined data to
    abstract concepts that have meaning for the user (such as combining work completion
    and interaction into a `participation`feature). Along with this, the researchers
    found that stakeholders responded better when the data sources of these abstract
    concepts were easily traceable.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在在线教育领域，作者致力于为大规模开放在线课程（例如Coursera、edX等免费课程）相关的各种决策任务添加可解释性。在与各种课程开发者和讲师合作时，作者发现，最有用的特征是那些将数据组合成对用户有意义的抽象概念（例如将工作完成情况和互动结合成`参与`特征）的特征。此外，研究人员发现，当这些抽象概念的数据来源易于追溯时，利益相关者的反应更好。
- en: '**Cybersecurity**'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '**网络安全**'
- en: In the third domain, researchers worked to develop models to detect Domain Generation
    Algorithms to help security analysts respond to potential attacks. While many
    features were engineered to identify these attacks, the raw DNS logs that these
    features were built from were much more useful to users and the challenge the
    authors faced was how to trace feature values back to the relevant logs.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在第三个领域，研究人员致力于开发检测领域生成算法的模型，以帮助安全分析师应对潜在的攻击。虽然为识别这些攻击工程了许多特征，但构建这些特征的原始DNS日志对用户的帮助更大，作者面临的挑战是如何将特征值追溯到相关日志。
- en: '**Medical Records**'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '**医疗记录**'
- en: In the domain of healthcare, researchers worked with six clinicians to develop
    a model to predict complications after surgery. In this case study, the authors
    used SHAP values to explain feature contributions but quickly found that SHAP
    explanations alone were not enough. Continuing the trend from the cybersecurity
    domain, the authors found that features based on aggregation functions are not
    as interpretable as the original signal data.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在医疗保健领域，研究人员与六位临床医生合作开发了一个预测手术后并发症的模型。在这项案例研究中，作者使用了SHAP值来解释特征贡献，但很快发现仅靠SHAP解释是不够的。延续网络安全领域的趋势，作者发现基于聚合函数的特征不如原始信号数据那样易于解释。
- en: '**Satellite Monitoring**'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '**卫星监测**'
- en: In this case study, the authors aimed to visualize the results of time-series
    anomaly detection solutions and developed a tool along with six domain experts.
    The authors then ran two user studies to evaluate the tool both with domain experts
    and with general end-users using stock price data. In this exercise, the authors
    discovered that more transparency is needed around the imputation process and
    most questions were about which of the values were imputed versus real.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在这项案例研究中，作者旨在可视化时间序列异常检测解决方案的结果，并与六位领域专家一起开发了一个工具。作者随后进行了两项用户研究，分别使用领域专家和普通终端用户的股票价格数据来评估该工具。在这项工作中，作者发现对插补过程的透明度需求更高，大多数问题集中在哪些值是插补的而非真实的。
- en: Lessons Learned
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 经验教训
- en: 'There were three key lessons from all of the cases:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 从所有案例中总结了三个关键经验：
- en: Most attention in the literature is placed on selecting and engineering features
    to maximize model performance, but models that interface with human users and
    decision-makers need an interpretable feature space to be useful.
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 文献中大多数关注点在于选择和工程化特性以最大化模型性能，但与人类用户和决策者接口的模型需要一个可解释的特性空间才有用。
- en: To be interpretable, a feature needs to have various properties (discussed later
    in the taxonomy).
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了具有可解释性，特性需要具备多种属性（稍后将在分类法中讨论）。
- en: While transformations that bring features to a model-ready state are important,
    there also needs to be a way to undo these transformations for interpretability.
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 虽然将特性转换为模型准备状态的重要性不言而喻，但也需要有方法来撤销这些转换以保持可解释性。
- en: Feature Taxonomy
  id: totrans-38
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 特性分类法
- en: The authors used the domains they worked in along with a large literature search
    to then develop a taxonomy of feature qualities that the identified users. The
    authors organize these qualities across 2 main qualities — model-readiness and
    interpretability — with some features sharing both qualities.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 作者结合他们工作的领域和大量文献搜索，开发了一种特性质量的分类法，识别了用户。作者将这些特性组织为两个主要质量——模型准备性和可解释性——其中一些特性同时具备这两个质量。
- en: Model-ready properties make a feature work well in a model and are what **developers**,
    **theorists**, and **ethicists** focus on.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 模型准备属性使特性在模型中表现良好，是**开发者**、**理论家**和**伦理学家**关注的重点。
- en: Interpretable properties are the ones that make a feature more understandable
    for users. These properties primarily benefit **decision-makers**, **users**,
    and **ethicists**.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 可解释属性是使特性对用户更易理解的属性。这些属性主要惠及**决策者**、**用户**和**伦理学家**。
- en: '**Model-Ready Feature Properties**'
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**模型准备特性属性**'
- en: '*Predictive*: The feature correlations with the prediction target. This does
    not imply a direct causal link however as a feature can be a confounding variable
    or a [spurious correlation](https://www.tylervigen.com/spurious-correlations).'
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*预测性*：特性与预测目标的相关性。这并不意味着直接的因果关系，因为特性可能是混淆变量或[虚假相关性](https://www.tylervigen.com/spurious-correlations)。'
- en: '*Model-Compatible*: The feature is supported by the model architecture, but
    may not be predictive or useful.'
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*模型兼容性*：特性由模型架构支持，但可能不具备预测性或实用性。'
- en: '*Model-Ready*: The feature is model-compatible and can help generate an accurate
    prediction. Model-ready features also include ones that have been transformed
    through methods like normalization and standardization.'
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*模型准备性*：特性与模型兼容，能够帮助生成准确预测。模型准备特性还包括通过标准化和归一化等方法转换过的特性。'
- en: '**Interpretable Feature Properties**'
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**可解释特性属性**'
- en: '*Readable*: The feature is written in plain text and users can understand what
    is referred to without looking at any code.'
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*可读性*：特性以普通文本书写，用户无需查看代码即可理解所指内容。'
- en: '*Human-Worded*: The feature is both readable and described in a natural, human-friendly
    way. The authors found that stakeholders in the child welfare space particularly
    benefitted from this property.'
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*人性化*：该特性既可读又以自然、友好的方式描述。作者发现，儿童福利领域的利益相关者特别受益于这一特性。'
- en: '*Understandable*: The feature refers to real-world metrics that the users understand.
    This property is heavily dependent on the users’ expertise but is usually features
    that have not undergone complex mathematical operations (ex. age is understandable,
    but `log(humidity)` may not be).'
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*可理解性*：特性指代用户理解的现实世界指标。这个属性在很大程度上依赖于用户的专业知识，但通常是那些没有经过复杂数学操作的特性（例如，年龄是可理解的，但`log(humidity)`可能不是）。'
- en: '**Both Model-Ready and Interpretable Properties**'
  id: totrans-50
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**模型准备性和可解释性属性兼具**'
- en: '*Meaningful*: The feature is one that subject matter experts believe is related
    to the target variable. Some features may be predictive, but not meaningful due
    to spurious correlations. Similarly, some features may be meaningful, but not
    very predictive. However, it is good practice to try to mostly use meaningful
    features.'
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*有意义*：特性是学科专家认为与目标变量相关的特性。一些特性可能具有预测性，但由于虚假相关性而不具意义。类似地，一些特性可能有意义，但预测性不强。然而，最好还是尽量使用有意义的特性。'
- en: '*Abstract Concepts*: The feature is calculated through some domain-expert-defined
    combination of original features and is often generic concepts (ex. participation
    and achievement).'
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*抽象概念*：特性通过某些领域专家定义的原始特性的组合计算而得，通常是通用概念（例如，参与和成就）。'
- en: '*Trackable*: The feature can be associated accurately with the raw data they
    were calculated from.'
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*可追踪*：该特征可以与其计算原始数据准确关联。'
- en: '*Simulatable*: The feature can be accurately recalculated from raw data if
    needed. All simulatable features are trackable, but not all trackable features
    are simulatable. For example, `test grade over time`` may be trackable (it came
    from raw test grades), but not simulatable as this could refer to average grades
    per month or year, or grade change.'
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*可模拟*：如果需要，可以从原始数据中准确重新计算该特征。所有可模拟的特征都是可追踪的，但并非所有可追踪的特征都是可模拟的。例如，`test grade
    over time` 可能是可追踪的（它来自原始测试成绩），但不能模拟，因为这可能指的是每月或每年的平均成绩或成绩变化。'
- en: Interpretable Transforms
  id: totrans-55
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 可解释变换
- en: Along with various properties of interpretable features, the authors also presented
    a few feature engineering methods and how they could potentially contribute to
    feature interpretability. While some data transformations to make features model-ready
    can also help with interpretability, this is not often the case. Interpretability
    transforms aim to help bridge this gap, but can often undo model-ready transforms.
    This may reduce the predictive ability of the model, but will introduce interpretable
    feature properties making it more trusted by decision-makers, users, and ethicists.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 除了可解释特征的各种属性，作者还提出了一些特征工程方法及其可能对特征可解释性的贡献。虽然有些数据变换可以帮助特征准备模型，但这并不常见。可解释性变换旨在帮助弥补这一差距，但往往会撤销模型准备变换。这可能会降低模型的预测能力，但会引入可解释的特征属性，使其更受决策者、用户和伦理学家的信任。
- en: '*Converting to Categorical*: When aiming to explain features, convert one-hot
    encoded variables back to their categorical form.'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*转换为分类变量*：在解释特征时，将独热编码变量转换回其分类形式。'
- en: '*Semantic Binning*: When binning numerical data, attempt to bin based on real-world
    distinctions instead of statistical distinctions. For example, it is more interpretable
    to bin `age` by `child`, `young-adult`, `adult`, and `senior` categories instead
    of binning by quartiles.'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*语义分箱*：在对数值数据进行分箱时，尽量基于现实世界的区别而不是统计区别进行分箱。例如，将 `age` 按 `child`、`young-adult`、`adult`
    和 `senior` 分类，比按四分位数进行分箱更具可解释性。'
- en: '*Flagged Imputation*: If data imputation is used, an extra feature identifying
    the points containing imputed data can greatly increase trust in your models.'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*标记插补*：如果使用数据插补，添加一个额外的特征来识别包含插补数据的点，可以大大增加对模型的信任。'
- en: '*Aggregate Numeric Features*: When many closely-related metrics are present
    in the data, it may be beneficial to aggregate them into a single feature to prevent
    data overload. For example, the authors found that summing up various physical
    and emotional abuse referrals into a single referral count metric helped decision-makers.'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*汇总数值特征*：当数据中存在许多紧密相关的指标时，将它们汇总为一个特征可能有利于防止数据过载。例如，作者发现将各种身体和情感虐待的推荐汇总为一个单一的推荐计量，有助于决策者。'
- en: '*Modify Categorical Granularity*: When many categories are related to each
    other, interpretability and performance can be improved by selecting the appropriate
    summarization of the variable (ex. summarizing the soil zones in the [forest covertype](https://archive.ics.uci.edu/ml/datasets/covertype)
    dataset to the main 8 geological soil zones)'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*修改分类粒度*：当许多类别彼此相关时，通过选择变量的适当总结（例如，将 [森林覆盖类型](https://archive.ics.uci.edu/ml/datasets/covertype)
    数据集中的土壤区划汇总为主要的8种地质土壤区）可以提高可解释性和性能。'
- en: '*Converting to Abstract Concepts*: Apply numerical aggregation and categorical
    granularity transformers to develop a hand-crafted formula to generate an abstract
    concept that subject matter experts can understand.'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*转换为抽象概念*：应用数值汇总和分类粒度变换器，开发一个手工制作的公式，以生成主题专家可以理解的抽象概念。'
- en: '*Reverse Scaling and Feature Engineering*: If standardization, normalization,
    or mathematical transforms are applied, interpretability can be increased if these
    transforms are reversed before analyzing the features. For example, reporting
    the feature weight on `age` is more helpful than reporting the weight of `sqrt(age)`.'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*反向缩放和特征工程*：如果应用了标准化、归一化或数学变换，在分析特征之前反转这些变换可以提高可解释性。例如，报告 `age` 的特征权重比报告 `sqrt(age)`
    的权重更有帮助。'
- en: '*Link to Raw Data*: This transform extends reversing scaling and feature engineering.
    If possible, explicitly display how the engineered feature is calculated from
    raw data.'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*原始数据链接*：此转换扩展了反向缩放和特征工程。如果可能，请明确展示如何从原始数据计算工程特征。'
- en: While this is not an exhaustive list of all the possible transforms, this does
    provide a great starting point for data scientists out there on some simple steps
    they can take to ensure that they have an interpretable feature space.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这不是所有可能转换的详尽列表，但它确实为数据科学家提供了一个很好的起点，介绍了一些简单的步骤，以确保他们拥有一个可解释的特征空间。
- en: Discussion and Conclusion
  id: totrans-66
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 讨论与结论
- en: '![](../Images/42d9acf92415c7189fe583020175ed3b.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/42d9acf92415c7189fe583020175ed3b.png)'
- en: 'Figure 1: Summary of the feature taxonomy proposed by Zytek et al. [1] (Figure
    from Paper)'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：Zytek 等人提出的特征分类总结 [1]（图源自论文）
- en: Reading this paper I did have some critiques. For one, while the authors developed
    various stakeholders, they never provided any examples of when **impacted users**
    would be different than **decision-makers.** While we can make some educated guesses
    (ex. students could be impacted users in the education case, and patients could
    be impacted users in the healthcare case), there is not a presented reason for
    how interpretable features help this group.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 阅读这篇论文时，我确实有一些批评。首先，尽管作者开发了各种利益相关者，但他们从未提供过 **受影响用户** 和 **决策者** 不同的示例。虽然我们可以做出一些有根据的猜测（例如，学生在教育案例中可能是受影响的用户，而患者在医疗案例中可能是受影响的用户），但并没有提出解释性特征如何帮助这个群体的理由。
- en: The authors themselves also presented some risks of interpretable features as
    well. In their example, a developer could maliciously include the race feature
    into the abstract concept of socioeconomic factors, effectively hiding that race
    was used as a predictor in their model. Additionally, the authors concede that
    many of the interpretability transformations proposed may reduce model performance.
    Some interpretable feature properties (like readability) are also not appropriate
    when data privacy is important.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 作者们自己也提出了一些可解释特征的风险。在他们的例子中，开发者可能会恶意地将种族特征包含到社会经济因素的抽象概念中，从而有效地掩盖了种族在模型中的预测作用。此外，作者承认，许多提出的可解释性转换可能会降低模型性能。一些可解释特征属性（如可读性）在数据隐私重要时也不适用。
- en: Despite these criticisms, it is undeniable that Zytek et al.[1] provided a lot
    of information about what makes features interpretable, how to achieve interpretability,
    and why it is important in the first place. Additionally, the proposed transforms
    are relatively simple to implement, making them much more friendly to beginner
    data scientists. Their taxonomy is summarized in Figure 1 above and is probably
    an image most data scientists need to keep handy on their desks.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管有这些批评，但不可否认的是，Zytek 等人 [1] 提供了很多关于特征如何变得可解释、如何实现可解释性以及为何这很重要的信息。此外，提出的转换相对容易实现，使其对初学者数据科学家更为友好。他们的分类在上面的图
    1 中总结，可能是大多数数据科学家需要随时备份在桌上的图像。
- en: Resources and References
  id: totrans-72
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 资源与参考文献
- en: '[1] A. Zytek, I. Arnaldo, D. Liu, L. Berti-Equille, K. Veeramachaneni. [The
    Need for Interpretable Features: Motivation and Taxonomy](https://arxiv.org/abs/2202.11748)
    (2022). SIGKDD Explorations.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] A. Zytek, I. Arnaldo, D. Liu, L. Berti-Equille, K. Veeramachaneni. [对可解释特征的需求：动机与分类](https://arxiv.org/abs/2202.11748)（2022）。SIGKDD
    Explorations。'
- en: '[2] C. Molnar. [Interpretable Machine Learning](https://christophm.github.io/interpretable-ml-book/)
    (2020). LeanPub'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] C. Molnar. [可解释的机器学习](https://christophm.github.io/interpretable-ml-book/)（2020）。LeanPub'
- en: '[3] A. Preece, D. Harborne, D. Braines, R. Tomsett, S. Chakraborty. [Stakeholders
    in Explainable AI](https://arxiv.org/abs/1810.00184) (2018). Artificial Intelligence
    in Government and Public Sector page 6.'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] A. Preece, D. Harborne, D. Braines, R. Tomsett, S. Chakraborty. [可解释 AI
    中的利益相关者](https://arxiv.org/abs/1810.00184)（2018）。《政府与公共部门的人工智能》第6页。'
- en: '[3] S. Lundberg, S.I. Lee. [A Unified Approach to Interpreting Model Predictions](https://arxiv.org/abs/1705.07874)
    (2017). Advances in Neural Information Processing volume 31 page 10.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] S. Lundberg, S.I. Lee. [对模型预测的统一解释方法](https://arxiv.org/abs/1705.07874)（2017）。《神经信息处理进展》第31卷第10页。'
