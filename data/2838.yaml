- en: 'Reinforcement Learning: an Easy Introduction to Value Iteration'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 强化学习：价值迭代的简单介绍
- en: 原文：[https://towardsdatascience.com/reinforcement-learning-an-easy-introduction-to-value-iteration-e4cfe0731fd5?source=collection_archive---------0-----------------------#2023-09-10](https://towardsdatascience.com/reinforcement-learning-an-easy-introduction-to-value-iteration-e4cfe0731fd5?source=collection_archive---------0-----------------------#2023-09-10)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/reinforcement-learning-an-easy-introduction-to-value-iteration-e4cfe0731fd5?source=collection_archive---------0-----------------------#2023-09-10](https://towardsdatascience.com/reinforcement-learning-an-easy-introduction-to-value-iteration-e4cfe0731fd5?source=collection_archive---------0-----------------------#2023-09-10)
- en: Learn the fundamentals of RL and how to apply Value Iteration to a simple example
    problem
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 学习强化学习（RL）的基础知识以及如何将价值迭代应用于一个简单的示例问题。
- en: '[](https://medium.com/@carlbettosi?source=post_page-----e4cfe0731fd5--------------------------------)[![Carl
    Bettosi](../Images/19c640d8e96fa39583a3c998764aa2b8.png)](https://medium.com/@carlbettosi?source=post_page-----e4cfe0731fd5--------------------------------)[](https://towardsdatascience.com/?source=post_page-----e4cfe0731fd5--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----e4cfe0731fd5--------------------------------)
    [Carl Bettosi](https://medium.com/@carlbettosi?source=post_page-----e4cfe0731fd5--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@carlbettosi?source=post_page-----e4cfe0731fd5--------------------------------)[![Carl
    Bettosi](../Images/19c640d8e96fa39583a3c998764aa2b8.png)](https://medium.com/@carlbettosi?source=post_page-----e4cfe0731fd5--------------------------------)[](https://towardsdatascience.com/?source=post_page-----e4cfe0731fd5--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----e4cfe0731fd5--------------------------------)
    [Carl Bettosi](https://medium.com/@carlbettosi?source=post_page-----e4cfe0731fd5--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fabe6f5e189c8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-an-easy-introduction-to-value-iteration-e4cfe0731fd5&user=Carl+Bettosi&userId=abe6f5e189c8&source=post_page-abe6f5e189c8----e4cfe0731fd5---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----e4cfe0731fd5--------------------------------)
    ·15 min read·Sep 10, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fe4cfe0731fd5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-an-easy-introduction-to-value-iteration-e4cfe0731fd5&user=Carl+Bettosi&userId=abe6f5e189c8&source=-----e4cfe0731fd5---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fabe6f5e189c8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-an-easy-introduction-to-value-iteration-e4cfe0731fd5&user=Carl+Bettosi&userId=abe6f5e189c8&source=post_page-abe6f5e189c8----e4cfe0731fd5---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----e4cfe0731fd5--------------------------------)
    · 15 分钟阅读 · 2023年9月10日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fe4cfe0731fd5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-an-easy-introduction-to-value-iteration-e4cfe0731fd5&user=Carl+Bettosi&userId=abe6f5e189c8&source=-----e4cfe0731fd5---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fe4cfe0731fd5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-an-easy-introduction-to-value-iteration-e4cfe0731fd5&source=-----e4cfe0731fd5---------------------bookmark_footer-----------)![](../Images/291df786927604981f62437fda3c5b4f.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fe4cfe0731fd5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-an-easy-introduction-to-value-iteration-e4cfe0731fd5&source=-----e4cfe0731fd5---------------------bookmark_footer-----------)![](../Images/291df786927604981f62437fda3c5b4f.png)'
- en: Value Iteration (VI) is typically one of the first algorithms introduced on
    the Reinforcement Learning (RL) learning pathway. The underlying specifics of
    the algorithm introduce some of the most fundamental aspects of RL and, hence,
    it is important to master VI before progressing to more complex RL algorithms.
    Yet, this can be tricky to get your head around.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 价值迭代（VI）通常是强化学习（RL）学习路径中首先引入的算法之一。该算法的基本细节介绍了RL的一些最基本的方面，因此，在进阶到更复杂的RL算法之前，掌握VI是很重要的。然而，这可能有些难以理解。
- en: This article is designed to be an easy-to-understand introduction to VI, which
    will assume the reader to be new to the field of RL. Let’s get started.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 本文旨在成为一个易于理解的价值迭代（VI）介绍，假设读者对强化学习领域是新的。让我们开始吧。
- en: Already know the basics of RL? → [**Skip to how to use Value Iteration**](#dab6).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 已经了解了RL的基础知识？→ [**跳到如何使用价值迭代**](#dab6)。
- en: The basics of RL
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 强化学习的基础
- en: Let’s start with a textbook definition, then I’ll break it down using an easy
    example.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从教科书定义开始，然后用一个简单的例子来解释。
- en: '![](../Images/bdfd099792f5ee2310c43662522084b7.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bdfd099792f5ee2310c43662522084b7.png)'
- en: Overview of the reinforcement learning training process
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习训练过程概述
- en: RL is one of the three key machine learning paradigms beside supervised and
    unsupervised learning. RL is not a singular algorithm, but rather a framework
    which encompasses a range of techniques and approaches for teaching agents to
    learn and make decisions in their environments.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习（RL）是除了监督学习和无监督学习之外的三大主要机器学习范式之一。RL不是单一的算法，而是一个框架，涵盖了一系列技术和方法，用于教导代理在其环境中学习和做出决策。
- en: In RL, an agent interacts with an environment by taking various actions. The
    agent is rewarded when those actions lead to desired states and punished when
    they don’t. The agent’s objective is to learn a strategy, called a policy, that
    guides its actions to maximize the reward it accumulates over time. This trial-and-error
    process refines the agent's behavioral policy, allowing it to take optimal or
    near-optimal behaviors in its environment.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在RL中，代理通过采取各种动作与环境互动。当这些动作导致期望的状态时，代理会获得奖励；当动作未达到期望的状态时，代理会受到惩罚。代理的目标是学习一个策略，称为策略（policy），该策略指导其行动以最大化其随着时间积累的奖励。这个试错过程精炼了代理的行为策略，使其在环境中采取最优或接近最优的行为。
- en: '*The book An “Introduction to Reinforcement Learning” by Richard S. Sutton
    and Andrew G. Barto is considered the best in the field for those wishing to gain
    a solid understanding of RL.. and it’s* [*available for free*](http://incompleteideas.net/book/the-book-2nd.html)*!*'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '*理查德·S·萨顿和安德鲁·G·巴托的《强化学习导论》一书被认为是该领域的最佳读物之一，对于那些希望深入理解RL的人来说..而且它是* [*免费提供的*](http://incompleteideas.net/book/the-book-2nd.html)*！*'
- en: Let’s define an example problem
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 让我们定义一个示例问题
- en: '![](../Images/25fc75950f08dcf8fbb9ceba748c9fa4.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/25fc75950f08dcf8fbb9ceba748c9fa4.png)'
- en: Possible states for the game of golf
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 高尔夫游戏中的可能状态
- en: This image depicts the game of golf in its simplest form. We will use this as
    golf has a clearly defined goal - get the ball in the hole.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 这张图片以最简单的形式描绘了高尔夫游戏。我们将使用这个例子，因为高尔夫有一个明确的目标——把球打进洞里。
- en: 'In our example, the golf ball can be in one of three positions: *on the fairway*;
    *on the green*; or *in the hole*. We start on the fairway and aim to get closer
    to the hole with each shot, with the hole sitting on the green.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的示例中，高尔夫球可以处于三种位置之一：*在球道上*；*在果岭上*；或*在洞里*。我们从球道开始，目标是每次击球都靠近洞，洞位于果岭上。
- en: In RL, each of these positions is referred to as a ***state*** of the ***environment.***
    You can think of the state as being a snapshot of the current environment (the
    golf course), which also records the ball position.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在强化学习（RL）中，这些位置中的每一个被称为***状态***或***环境状态***。你可以把状态看作是当前环境（高尔夫球场）的快照，同时记录球的位置。
- en: '![](../Images/2938c430e74f76f0d14cc0001e1ae266.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2938c430e74f76f0d14cc0001e1ae266.png)'
- en: Possible actions in the game of golf
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 高尔夫球游戏中的可能动作
- en: 'In our game, the ***agent*** takes shots of hitting the ball, starting at the
    *ball on fairway* state. The ***agent*** simply refers to the entity that is in
    control of taking ***actions***. Our game has three available actions: *hit to
    fairway*; *hit to green*; and *hit in hole*.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的游戏中，***代理***进行击球动作，开始时处于*球在球道上*状态。***代理***仅指控制进行***动作***的实体。我们的游戏有三种可用的动作：*击球到球道*；*击球到果岭*；以及*击球进洞*。
- en: '![](../Images/110c6f6d02f7fb59ee2b805e836a368b.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/110c6f6d02f7fb59ee2b805e836a368b.png)'
- en: Transition probabilities in the game of golf
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 高尔夫游戏中的转移概率
- en: Of course, when you take a shot, the ball may not go where you want it to. Therefore,
    we introduce a ***transition function*** linking actions to states with some probability
    weighting.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，当你击球时，球可能不会落在你想要的位置。因此，我们引入了一个***转移函数***，通过一些概率权重将动作与状态关联起来。
- en: For example, we may miss the green when we take a shot from the fairway and
    end up still on the fairway. Written in an RL context, if we are in the *ball
    on fairway* state and take the action *hit to green*, there is a 90% probability
    we will enter the *ball on green* state but a 10% probability we will re-enter
    the *ball on fairway* state.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，当我们从球道上击球时，可能会偏离果岭，仍然停留在球道上。用RL的术语来说，如果我们处于*球在球道上*状态，并采取*击球到果岭*的动作，则有90%的概率进入*球在果岭上*状态，但也有10%的概率重新进入*球在球道上*状态。
- en: '![](../Images/d9a6e396a04c36b4d5f972989603693b.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d9a6e396a04c36b4d5f972989603693b.png)'
- en: A reward of 10 for getting the ball in the hole
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 将球击入洞中的奖励为 10
- en: Every time the agent takes an action, we refer to this as a ***step*** through
    the environment. Based upon the action just taken, the agent observes the new
    state it ends up in as well as a ***reward***. A reward function is an incentive
    mechanism for pushing the agent in the right direction. In other words, we design
    the reward function to shape the desired behavior of our agent. In our simplified
    golf example, we provide a reward of 10 for getting the ball in the hole.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 每当代理执行一个动作时，我们称之为在环境中的 ***步***。根据刚刚采取的动作，代理观察到它所处的新状态以及一个 ***奖励***。奖励函数是一个激励机制，用于引导代理朝正确方向前进。换句话说，我们设计奖励函数以塑造代理的期望行为。在我们简化的高尔夫示例中，我们为将球击入洞中的行为提供
    10 的奖励。
- en: '*The design of environment dynamics (the transition and reward functions) is
    not a trivial task. If the environment does not represent the problem you are
    attempting to solve, the agent will learn a policy reflecting a correct solution
    for an incorrect problem. We will not cover such design elements here, but it’s
    worth noting.*'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '*环境动态（转换和奖励函数）的设计不是一项简单的任务。如果环境未能代表你试图解决的问题，代理将学习到一个反映不正确问题的正确解决方案的策略。我们在这里不讨论这些设计元素，但值得注意。*'
- en: Markov Decision Processes
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 马尔可夫决策过程
- en: To represent a problem in a way the agent understands, we must formalise it
    as a Markov Decision Process (MDP).
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 为了以代理能够理解的方式表示问题，我们必须将其形式化为马尔可夫决策过程（MDP）。
- en: A MDP is a mathematical model which describes our problem in a structured way.
    It represents the agent's interactions with the environment as a sequential decision-making
    process (i.e., one action after another).
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: MDP 是一种数学模型，它以结构化的方式描述了我们的问题。它将代理与环境的互动表示为一个顺序决策过程（即，一个接一个的动作）。
- en: 'It consists of the environment dynamics (I’m going to add some math notation
    here to shorten things):'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 它由环境动态组成（我将添加一些数学符号以简化说明）：
- en: a finite set of states **s ∈ S**.
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个有限的状态集合 **s ∈ S**。
- en: a finite set of actions ***a* ∈ *A***.
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个有限的动作集合 ***a* ∈ *A***。
- en: a transition function ***T*(*s*′∣*s*,*a*)** returning the probability of reaching
    state ***s*′**, given the current state ***s***, the current action ***a***.
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个转换函数 ***T*(*s*′∣*s*,*a*)** 返回在当前状态 ***s*** 和当前动作 ***a*** 下达到状态 ***s*′** 的概率。
- en: a reward function ***R*(*s*,*a*,*s*′)** returning a scalar reward based on reaching
    next state ***s*′**, after being in state ***s***, and taking action ***a***.
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个奖励函数 ***R*(*s*,*a*,*s*′)** 基于从状态 ***s*** 到达下一个状态 ***s*′** 的情况返回一个标量奖励，并考虑采取动作
    ***a***。
- en: '*Note that if there is some uncertainty or randomness involved in the transitions
    between states (i.e. taking the same action in the same state twice may lead to
    different outcomes), we refer to this as a* ***stochastic*** *MDP. We could also
    create a* ***deterministic*** *MDP, where transitions and rewards are entirely
    predictable. This means when an agent takes an action in a specific state, there
    is a one-to-one correspondence between the action and the resulting next state
    and the reward.*'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '*注意，如果状态之间的转换涉及某些不确定性或随机性（即在相同状态下两次采取相同动作可能导致不同结果），我们称之为* ***随机*** *MDP。我们也可以创建一个*
    ***确定性*** *MDP，其中转换和奖励是完全可预测的。这意味着当代理在特定状态下采取一个动作时，动作与结果状态及奖励之间是一一对应的。*'
- en: Visualised as an MDP, our golf problem looks pretty much the same as the images
    depicted earlier. We will use S = {s1, s2, s3} for shorthand.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 以 MDP 形式可视化，我们的高尔夫问题看起来与之前描述的图像几乎相同。我们将使用 S = {s1, s2, s3} 作为简写。
- en: '![](../Images/d2778219029b8237cd1d52e14e59fb54.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d2778219029b8237cd1d52e14e59fb54.png)'
- en: Our golf example problem written as an MDP
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的高尔夫示例问题以 MDP 形式呈现
- en: '*The use of an MDP assumes that what’s going to happen next in the environment
    only depends on what’s happening right now — the current state and action — and
    not on what happened before. This is called the* **Markov property***, and it’s
    important in RL as it reduces computational complexity. I’ll explain this more
    later.*'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '*MDP 的使用假设环境中接下来会发生的事情只依赖于现在的状态和动作，而不依赖于之前发生的事情。这被称为* **马尔可夫性质** *，它在强化学习中很重要，因为它降低了计算复杂性。我稍后会详细解释这一点。*'
- en: What is Value Iteration?
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 什么是价值迭代？
- en: Value Iteration (VI) is an algorithm used to solve RL problems like the golf
    example mentioned above, where we have full knowledge of all components of the
    MDP. It works by iteratively improving its estimate of the ‘value’ of being in
    each state. It does this by considering the immediate rewards and expected future
    rewards when taking different available actions. These values are tracked using
    a value table, which updates at each step. Eventually, this sequence of improvements
    converges, yielding an optimal policy of state → action mappings that the agent
    can follow to make the best decisions in the given environment.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 值迭代（VI）是一种用于解决类似于上述高尔夫球例子中的强化学习（RL）问题的算法，其中我们对MDP的所有组件有完全的了解。它通过迭代改进对每个状态“价值”的估计来实现。它通过考虑不同可用动作时的即时奖励和期望的未来奖励来完成这项工作。这些值通过一个值表进行跟踪，该表在每一步都会更新。最终，这一系列的改进将会收敛，产生一个状态
    → 动作映射的最优策略，使代理可以在给定环境中做出最佳决策。
- en: VI leverages the concept of *dynamic programming*, where solving a big problem
    is broken down into smaller subproblems. To achieve this in VI, the *Bellman equation*
    is used to guide the process of iteratively updating value estimates for each
    state, providing a recursive relationship that expresses the value of a state
    in terms of the values of its neighbouring states.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: VI 利用 *动态规划* 的概念，其中将一个大问题分解成较小的子问题来解决。为了在VI中实现这一点，使用 *贝尔曼方程* 来指导迭代更新每个状态的价值估计的过程，提供一个递归关系，用于表示状态的价值与其邻近状态的价值之间的关系。
- en: This won’t make much sense now. The easiest way to learn VI is to break it down
    step-by-step, so let’s do that.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 现在这可能不太容易理解。学习VI的最简单方法是逐步分解，我们就这样做吧。
- en: How does the Value Iteration algorithm work?
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 值迭代算法是如何工作的？
- en: The image below depicts the steps of the algorithm. Don’t be put off, it's easier
    than it appears.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 下图展示了算法的步骤。不要被吓到，它比看起来更简单。
- en: '![](../Images/b6f032fc68d508cb4d115e760c583e68.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b6f032fc68d508cb4d115e760c583e68.png)'
- en: The Value Iteration algorithm
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 值迭代算法
- en: Firstly, we need to define some parameters for our training.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要为我们的训练定义一些参数。
- en: Theta **θ** represents a threshold for convergence. Once we reach **θ**, we
    can terminate the training loop and generate the policy. It’s essentially just
    a way to ensure the policy we create is accurate enough. If we stop training too
    early, we may not learn the best actions to take.
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Theta **θ** 代表收敛的阈值。一旦我们达到 **θ**，就可以终止训练循环并生成策略。它本质上只是一种确保我们创建的策略足够准确的方法。如果我们过早停止训练，可能无法学习到最佳的行动。
- en: Gamma **γ** represents the *discount factor*. This is a value which determines
    how much our agent values future rewards compared to immediate rewards. A higher
    discount factor (closer to 1) indicates that the agent values long-term rewards
    more, while a lower discount factor (closer to 0) places greater emphasis on immediate
    rewards.
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gamma **γ** 代表 *折扣因子*。这是一个值，决定了我们的代理对未来奖励的重视程度与对即时奖励的重视程度相比。较高的折扣因子（接近1）表示代理更重视长期奖励，而较低的折扣因子（接近0）则更强调即时奖励。
- en: '*To understand the discount factor better, consider an RL agent playing chess.
    Let’s say you have the opportunity to capture your opponent’s queen in the next
    move, which would yield a significant immediate reward. However, you also notice
    that by sacrificing a less valuable piece now, you could set up a future advantage
    that might lead to checkmate and an even bigger reward later. The discount factor
    helps you balance this decision.*'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '*为了更好地理解折扣因子，考虑一个玩国际象棋的RL代理。假设你有机会在下一步捕获对手的皇后，这将带来显著的即时奖励。然而，你还注意到，通过现在牺牲一个不太重要的棋子，你可以为未来的优势铺平道路，可能会导致将死并获得更大的奖励。折扣因子帮助你平衡这个决定。*'
- en: '**(1) Initialisation:** Now we have the parameters defined, we want to initialise
    our *value function* ***V(s)*** for all states in ***S***. This typically means
    we set all values to 0 (or some other arbitrary constant) for every state. Think
    of the value function as a table that tracks a value for each state, updating
    frequently.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '**(1) 初始化：** 现在我们已经定义了参数，我们想要初始化我们所有状态的 *价值函数* ***V(s)***。这通常意味着我们将每个状态的所有值设置为0（或其他任意常数）。可以把价值函数想象成一个表格，跟踪每个状态的值，并频繁更新。'
- en: '![](../Images/fb25e69630ea42956f30577387e322b3.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fb25e69630ea42956f30577387e322b3.png)'
- en: An initialised value table
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 初始化的值表
- en: '**(2) Outer loop:** Now everything is set up, we can start the iterative process
    of updating our values. We begin in the outer loop, which repeats until the convergence
    criteria are met (until Δ < θ).'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '**(2) 外层循环：** 现在一切都准备好了，我们可以开始更新值的迭代过程。我们从外层循环开始，它会重复直到收敛标准满足（直到 Δ < θ）。'
- en: At each pass of the outer loop, we begin by setting Δ = 0\. Delta **Δ** is used
    to represent the change in value estimates across all states, and the algorithm
    continues iterating until this change Δfalls below the specified threshold θ.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在每次外层循环中，我们首先设置Δ = 0。**Δ** 用来表示所有状态的价值估计变化，算法继续迭代，直到这一变化Δ低于指定的阈值θ。
- en: '**(3) Inner loop:** For every state *s* in *S,* we:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '**(3) 内层循环：** 对于每个状态 *s* 在 *S* 中，我们：'
- en: set a variable ***v*** to the current value of that state ***V(s)***, remember
    - this is fetched from our value table (so on the first pass, ***v*** = ***V(s)***
    = 0)
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将变量 ***v*** 设置为当前状态的价值 ***V(s)***，记住 - 这是从我们的价值表中获取的（所以在第一次遍历时，***v*** = ***V(s)***
    = 0）
- en: perform the bellman equation to update ***V(s)***
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 执行贝尔曼方程来更新 ***V(s)***
- en: update **Δ** (we’ll come back to this)
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更新 **Δ**（我们稍后会回来讨论）
- en: '**The Bellman Equation**'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '**贝尔曼方程**'
- en: '![](../Images/ac783b3215bf86884f63526f4edb24b5.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ac783b3215bf86884f63526f4edb24b5.png)'
- en: This line of the algorithm is the most important. It requires that we update
    the value of the current state we are looking at in the loop. This value is calculated
    by considering all available actions from that specific state (a 1-step look ahead).
    When we take each of those possible actions it will present us with a set of possible
    next states ***s*′** and respective rewards ***r.***
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 算法的这一行是最重要的。它要求我们更新当前循环中所看的状态的价值。这个价值是通过考虑从那个特定状态所有可用的动作（前瞻1步）来计算的。当我们采取这些可能的动作时，它会给我们一组可能的下一个状态
    ***s*′** 和相应的奖励 ***r.***
- en: '![](../Images/8076df201e89c49f337967581486f369.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8076df201e89c49f337967581486f369.png)'
- en: 'So, for each of those next states ***s*′** and respective rewards ***r***,
    we perform ***p(s′, r|s, a)[r + γV(s′)].*** Let''s break this up:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，对于每一个下一个状态 ***s*′** 和相应的奖励 ***r***，我们执行 ***p(s′, r|s, a)[r + γV(s′)].***
    让我们分解一下：
- en: '***p(s′, r|s, a)*** the probability of being in state ***s***, taking action
    ***a***, and ending up in next state ***s*′**(this is just our transition function)'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***p(s′, r|s, a)*** 在状态 ***s*** 中，采取动作 ***a*** 并最终到达下一个状态 ***s*′** 的概率（这只是我们的转移函数）'
- en: '***[r + γV(s′)]*** the reward ***r*** of ending up in next state ***s*′** (we
    get that from our reward function) *+* our discount **γ** * by the value of that
    next state ***s*′** (we get that from our value table)'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***[r + γV(s′)]*** 下一个状态 ***s*′** 的奖励 ***r***（我们从奖励函数中得到这个值） *+* 我们的折扣 **γ**
    * 乘以下一个状态的价值 ***s*′**（我们从价值表中得到这个值）'
- en: We then multiply these two parts ***p(s′, r|s, a) * [r + γV(s′)]***
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然后我们将这两个部分 ***p(s′, r|s, a) * [r + γV(s′)]*** 相乘
- en: Remember, this calculation is just for **one** next state ***s′*** (the third
    level of the tree), we need to repeat this for each possible next state ***s′***
    after taking ***a.***
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，这个计算只是针对**一个**下一个状态 ***s′***（树的第三层），我们需要对每一个可能的下一个状态 ***s′*** 在采取 ***a.***
    后重复这个过程。
- en: Once we have done this, we sum all the results we just got ***Σₛ′, ᵣ p(s′, r|s,
    a) * [r + γV(s′)].*** We then repeat this for each action ***a*** (the second
    level in the tree).
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦完成这一步，我们将所有刚获得的结果 ***Σₛ′, ᵣ p(s′, r|s, a) * [r + γV(s′)].*** 汇总起来。然后我们对每个动作
    ***a*** 进行重复（树中的第二层）。
- en: Once these steps are complete, we will have a value associated with each possible
    action ***a*** from the current state we are looking at in the inner loop ***s.***
    Wechoose the highest using ***maxₐ*** and set this equal to our new value for
    that state ***V(s)←maxₐ Σₛ′, ᵣ p(s′, r|s, a) * [r + γV(s′)].***
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦完成这些步骤，我们将为当前状态 ***s*** 在内层循环中看到的每个可能的动作 ***a*** 关联一个值。我们使用 ***maxₐ*** 选择最高的，并将其设置为该状态的新值
    ***V(s)←maxₐ Σₛ′, ᵣ p(s′, r|s, a) * [r + γV(s′)].***
- en: Remember, this process covered only one state ***s*** (the first level in the
    tree)
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，这个过程只涵盖了一个状态 ***s***（树的第一层）
- en: '*If we were programming this, it would be 3* ***for*** *loops for each level
    in the tree:*'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '*如果我们在编程，这将是树中每一层的3* ***for*** *循环：*'
- en: '![](../Images/a949262be65f616b0d6fbd68d7ef07d3.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a949262be65f616b0d6fbd68d7ef07d3.png)'
- en: '**(3) Inner loop (continued):** Before moving to the next pass in the inner
    loop, we perform a comparison between the current value of ***Δ*** and the difference
    between the previous value of this state ***v*** and the new value for this state
    we just calculated ***V(s)***. We update ***Δ*** to the larger of these two: ***Δ
    ← max(Δ,| v - V(s)|)***. This helps us track how close we are to convergence.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '**(3) 内循环（继续）：** 在移动到内循环的下一次迭代之前，我们对当前***Δ***的值和该状态***v***的前一个值与刚计算的状态新值***V(s)***之间的差异进行比较。我们将***Δ***更新为这两者中的较大者：***Δ
    ← max(Δ,| v - V(s)|)***。这有助于我们跟踪离收敛的距离。'
- en: Ok, this process completes one pass of the inner loop. **We perform step (3)
    for each s in S** before breaking out of the inner loop and performing a check
    on the convergence condition ***Δ < θ.*** If this condition is met, we break out
    the outer loop, if not, we go back to step (2).
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，这个过程完成了一次内循环的操作。 **我们对S中的每个s执行第（3）步**，然后再退出内循环并对收敛条件***Δ < θ***进行检查。如果满足此条件，我们退出外循环，否则返回第（2）步。
- en: '**(4) Policy extraction:** By this time, we have likely performed multiple
    passes through the outer loop until we have converged. This means our value table
    will be updated to represent the final value of each state (in other words, ‘how
    good it is to be in each state’). We can now extract a policy from this.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '**(4) 策略提取：** 到目前为止，我们可能已经进行了多次外循环，直到收敛。这意味着我们的值表将被更新为表示每个状态的最终值（换句话说，‘处于每个状态的好处’）。我们现在可以从中提取策略。'
- en: '![](../Images/ea3f5ebe2aa38c2fc3d57beabedaeae4.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ea3f5ebe2aa38c2fc3d57beabedaeae4.png)'
- en: Remember, the policy **π** is essentially a mapping from **states → actions**,
    and for each state, it selects the action that maximizes the expected return.
    To calculate this, we perform the exact same process as before, but instead of
    getting a value for state ***s*** using ***maxₐ***, we get the action ***a***
    that gives us the best value using ***argmaxₐ***.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，策略**π**本质上是**状态 → 动作**的映射，对于每个状态，它选择最大化期望回报的动作。要计算这一点，我们执行与之前完全相同的过程，但不是使用***maxₐ***获得状态***s***的值，而是使用***argmaxₐ***获得给我们最佳值的动作***a***。
- en: And that’s it!
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 就这样！
- en: '***Policy Iteration*** *is another dynamic programming algorithm. It is similar
    to VI except it alternates between improving the policy by making it greedy with
    respect to the current value function and evaluating the policy’s performance
    until convergence, often requiring fewer iterations but more computation per iteration.*'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '***策略迭代*** *是另一种动态规划算法。它类似于VI，只是它在通过使策略相对于当前值函数变得贪婪来改进策略和评估策略的表现直到收敛之间交替进行，通常需要较少的迭代但每次迭代需要更多计算。*'
- en: Solving the example using Value Iteration
  id: totrans-90
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用值迭代解决示例
- en: VI should make even more sense once we complete an example problem, so let’s
    get back to our golf MDP. We have formalised this as an MDP but currently, the
    agent doesn’t know the best strategy when playing golf, so let’s solve the golf
    MDP using VI.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦完成示例问题，VI应该更有意义，所以让我们回到我们的高尔夫MDP。我们已将其形式化为MDP，但目前，代理不知道打高尔夫的最佳策略，因此让我们使用VI解决高尔夫MDP。
- en: '![](../Images/3bc4524300a1072607e002076d7f28bd.png)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3bc4524300a1072607e002076d7f28bd.png)'
- en: 'We’ll start by defining our model parameters using fairly standard values:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将开始使用相当标准的值来定义我们的模型参数：
- en: '[PRE0]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'We will then initialise our value table to 0 for states in ***S***:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们将对***S***中的状态初始化我们的值表为0：
- en: '[PRE1]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'We can now start in the outer loop:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以开始外循环：
- en: '[PRE2]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'And three passes of the inner loop for each state in ***S***:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 对于***S***中的每个状态，进行三次内循环：
- en: '[PRE3]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'This gives us the following update to our value table:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 这为我们的值表提供了以下更新：
- en: '[PRE4]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '*We don’t need to worry about* ***s2*** *as this is a terminal state, meaning
    no actions are possible here.*'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '*我们不需要担心* ***s2*** *，因为这是一个终止状态，意味着这里不可能进行任何操作。*'
- en: 'We now break out the inner loop and continue the outer loop, performing a convergence
    check on:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在分离内循环，继续外循环，对以下内容进行收敛检查：
- en: '[PRE5]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Since we have not converged, we do a second iteration of the outer loop:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 由于尚未收敛，我们进行外循环的第二次迭代：
- en: '[PRE6]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'And another 3 passes of the inner loop, using the updated value table:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 另进行3次内循环，使用更新后的值表：
- en: '[PRE7]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'At the end of the second iteration, our values are:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 在第二次迭代结束时，我们的值是：
- en: '[PRE8]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Check convergence once again:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 再次检查收敛：
- en: '[PRE9]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Still no convergence, so we continue the same process as above until Δ < θ.
    I won’t show all the calculations, the above two are enough to understand the
    process.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 仍未收敛，所以我们继续上述过程，直到 Δ < θ。我不会展示所有计算，以上两个步骤足以理解过程。
- en: 'After 6 iterations, our policy converges. This is our values and convergence
    rate as they change over each iteration:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 经过6次迭代，我们的策略已经收敛。这是我们的价值和收敛率在每次迭代中的变化情况：
- en: '[PRE10]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Now we can extract our policy:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以提取我们的策略：
- en: '[PRE11]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Our final policy is:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的最终策略是：
- en: '[PRE12]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: So, when our agent is in the ***Ball on fairway*** state (s0), the best action
    is to ***hit to green***. This seems pretty obvious since that is the only available
    action. However, in ***s1***, where there are two possible actions, our policy
    has learned to ***hit in hole***. We can now give this learned policy to other
    agents who want to play golf!
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，当我们的智能体处于 ***球在球道上*** 状态（s0）时，最佳行动是 ***击球到果岭***。这似乎很明显，因为这是唯一可用的行动。然而，在 ***s1***
    中，那里有两个可能的行动，我们的策略已经学会了 ***打入洞中***。我们现在可以将这个学到的策略提供给其他想打高尔夫的智能体！
- en: And there you have it! We have just solved a very simple RL problem using Value
    Iteration.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 就是这样！我们刚刚使用价值迭代解决了一个非常简单的 RL 问题。
- en: Limitations to dynamic programming
  id: totrans-123
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 动态编程的局限性
- en: It’s important to note that Value Iteration, along with other Dynamic Programming
    algorithms, has its limitations. Firstly, it assumes that we have complete knowledge
    of the dynamics of the MDP (we call this ***model-based RL***). However, this
    is rarely the case in real-world problems, for example, we may not know the transition
    probabilities. For problems where this is the case, we need to use other approaches
    such as *Q-learning* (***model-free RL***).
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的是，价值迭代以及其他动态编程算法都有其局限性。首先，它假设我们对 MDP 的动态有完全的了解（我们称之为***基于模型的 RL***）。然而，在实际问题中这种情况很少见，例如，我们可能不知道转移概率。对于这种情况，我们需要使用其他方法，如*Q-learning*（***无模型
    RL***）。
- en: '![](../Images/f60ad4bf1fac52b752748a38f8f6dc1f.png)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f60ad4bf1fac52b752748a38f8f6dc1f.png)'
- en: Curse of dimensionality in chess
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 国际象棋中的维度诅咒
- en: Secondly, for bigger problems, as the number of states and actions increases,
    the size of the value table grows exponentially (think about trying to define
    all the possible states of chess). This results in the ‘*curse of dimensionality*’
    problem, where the computational and memory requirements escalate rapidly, making
    it challenging to apply DP to high-dimensional problems.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，对于更大的问题，随着状态和动作数量的增加，价值表的大小会呈指数增长（想象一下尝试定义所有可能的国际象棋状态）。这导致了‘*维度诅咒*’问题，其中计算和内存需求迅速增加，使得将
    DP 应用于高维问题变得具有挑战性。
- en: Nevertheless, VI is great to learn as it introduces some of the key foundational
    concepts of RL which form the basis of more complex algorithms that you may go
    on to learn.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，VI 学习是很棒的，因为它介绍了一些 RL 的关键基础概念，这些概念构成了你可能会继续学习的更复杂算法的基础。
- en: Thanks for reading!
  id: totrans-129
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 感谢阅读！
- en: I hope this article has provided an easy-to-understand introduction to reinforcement
    learning, and Value Iteration specifically.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 我希望这篇文章提供了一个易于理解的强化学习介绍，特别是价值迭代。
- en: '*If you learnt something new here, please give this a 👏 and follow!*'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '*如果你在这里学到了新的知识，请点赞👏并关注！*'
- en: Unless otherwise stated, all images are created by the author.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 除非另有说明，所有图片均由作者创作。
