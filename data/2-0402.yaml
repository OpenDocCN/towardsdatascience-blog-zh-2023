- en: 'Boosting Spark Union Operator Performance: Optimization Tips for Improved Query
    Speed'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 提升 Spark 联合运算符性能：改进查询速度的优化技巧
- en: 原文：[https://towardsdatascience.com/boosting-spark-union-operator-performance-optimization-tips-for-improved-query-speed-9123ae6ada80](https://towardsdatascience.com/boosting-spark-union-operator-performance-optimization-tips-for-improved-query-speed-9123ae6ada80)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/boosting-spark-union-operator-performance-optimization-tips-for-improved-query-speed-9123ae6ada80](https://towardsdatascience.com/boosting-spark-union-operator-performance-optimization-tips-for-improved-query-speed-9123ae6ada80)
- en: Demystify Spark Performance in Union Operator
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解密 Spark 联合运算符的性能
- en: '[](https://chengzhizhao.medium.com/?source=post_page-----9123ae6ada80--------------------------------)[![Chengzhi
    Zhao](../Images/186bba91822dbcc0f926426e56faf543.png)](https://chengzhizhao.medium.com/?source=post_page-----9123ae6ada80--------------------------------)[](https://towardsdatascience.com/?source=post_page-----9123ae6ada80--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----9123ae6ada80--------------------------------)
    [Chengzhi Zhao](https://chengzhizhao.medium.com/?source=post_page-----9123ae6ada80--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://chengzhizhao.medium.com/?source=post_page-----9123ae6ada80--------------------------------)[![Chengzhi
    Zhao](../Images/186bba91822dbcc0f926426e56faf543.png)](https://chengzhizhao.medium.com/?source=post_page-----9123ae6ada80--------------------------------)[](https://towardsdatascience.com/?source=post_page-----9123ae6ada80--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----9123ae6ada80--------------------------------)
    [Chengzhi Zhao](https://chengzhizhao.medium.com/?source=post_page-----9123ae6ada80--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----9123ae6ada80--------------------------------)
    ·6 min read·Apr 20, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----9123ae6ada80--------------------------------)
    ·阅读时间6分钟·2023年4月20日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/bd00c06827eb22a6b64bcc2112e3d7d8.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bd00c06827eb22a6b64bcc2112e3d7d8.png)'
- en: Photo by [Fahrul Azmi](https://unsplash.com/@fahrulazmi?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
    on [Unsplash](https://unsplash.com/photos/zN4mtLHkHn4?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由 [Fahrul Azmi](https://unsplash.com/@fahrulazmi?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
    提供，来源于 [Unsplash](https://unsplash.com/photos/zN4mtLHkHn4?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
- en: The union operator is one of the set operators to merge two input data frames
    into one. Union is a convenient operation in Apache Spark for combining rows with
    the same order of columns. One frequently used case is applying different transformations
    and then unioning them together.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 联合运算符是将两个输入数据框合并为一个的集合运算符之一。联合操作在 Apache Spark 中是一个方便的操作，用于合并具有相同列顺序的行。一个常见的用例是应用不同的转换，然后将它们联合在一起。
- en: The ways of using the union operation in Spark are often discussed widely. However,
    a hidden fact that has been less discussed is the performance caveat associated
    with the union operator. If we didn’t understand the caveat of the union operator
    in Spark, we might fall into the trap of doubling the execution time to get the
    result.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Spark 中使用联合操作的方法常常被广泛讨论。然而，一个较少被讨论的隐藏事实是与联合运算符相关的性能陷阱。如果我们不理解 Spark 中联合运算符的陷阱，可能会陷入将执行时间翻倍的陷阱。
- en: We will focus on the Apache Spark DataFrameunion operator in this story with
    examples, show you the physical query plan, and share techniques for optimization
    in this story.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将重点讲解 Apache Spark DataFrame 的联合运算符，提供示例，展示物理查询计划，并分享优化技巧。
- en: Union Operator 101 in Spark
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Spark 中的 Union Operator 101
- en: 'Like Relational Database (RDBMS) SQL, the union is a direct way to combine
    rows. One important thing to note when dealing with a union operator is to ensure
    rows follow the same structure:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 像关系型数据库（RDBMS）SQL 一样，联合是一种直接合并行的方式。处理联合运算符时要注意的一点是确保行遵循相同的结构：
- en: '**The number of columns should be identical**. The union operation won’t silently
    work or fill with NULL when the number of columns differs on data frames.'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**列的数量应该相同**。当数据框的列数量不同，联合操作不会静默地工作或用 NULL 填充。'
- en: '**The column data type should match and resolves columns by position**. The
    column name should follow the same sequence for each data frame. Nevertheless,
    that’s not mandatory. The first data frame will be chosen as the default for the
    column name. So mixing order can potentially cause an undesired result. Spark
    `unionByName` is intended to resolve this issue.'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**列的数据类型应匹配，并按位置解析列**。列名在每个数据框中应遵循相同的顺序。然而，这不是强制性的。第一个数据框将被选择为列名的默认值。因此，混合顺序可能会导致意外结果。Spark的`unionByName`旨在解决这个问题。'
- en: In Spark, the operation `unionAll` is an alias to `union` that doesn’t remove
    duplication. We’d need to add distinct after performing union to perform SQL-like
    union operations without duplication.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在Spark中，操作`unionAll`是`union`的别名，不会去除重复项。我们需要在执行联合后添加distinct，以进行无重复项的SQL类似联合操作。
- en: We can also combine multiple data frames to produce a final data frame.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也可以将多个数据框合并成一个最终的数据框。
- en: '[PRE0]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Performance Bottleneck of Union Operator
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 联合操作符的性能瓶颈
- en: One typical pattern of using the union operator is splitting a single data frame
    into multiple, then applying different transformations, and eventually combining
    them into the final one.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 使用联合操作符的一个典型模式是将单个数据框拆分为多个数据框，然后应用不同的转换，最后将它们合并成最终的数据框。
- en: 'Here is an example: we have two big tables (fact table) that need to join,
    and the best way to join is the SortMerged join in Spark. Once we got the SortMerged
    data frame, we split it into four subsets. Each subset uses different transformations,
    and eventually, we combine those 4 data frames into the final one.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一个示例：我们有两个需要连接的大表（事实表），最好的连接方式是在Spark中使用SortMerged连接。一旦得到SortMerged数据框，我们将其拆分成四个子集。每个子集使用不同的转换，最终将这四个数据框合并成一个最终的数据框。
- en: '![](../Images/aedfcc3042c197c4580de6e8324241f9.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/aedfcc3042c197c4580de6e8324241f9.png)'
- en: Union Operation in Spark | Image By Author
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: Spark中的联合操作 | 图片由作者提供
- en: Spark data frame leverages Catalyst optimizer, which takes the data frame code
    you had, then performs code analysis, logical optimization, physical planning,
    and code generation. Catalyst tries to create an optimal plan that executes your
    Spark job efficiently.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: Spark数据框利用Catalyst优化器，它会对你编写的数据框代码进行代码分析、逻辑优化、物理规划和代码生成。Catalyst尝试创建一个执行Spark作业的**最优**计划。
- en: In recent years, Spark has extensively accomplished a lot of optimization on
    Catalyst to improve performance on Spark join operations. The join operation has
    more scenarios to use than the union operation, leading to less effort put into
    the union operation.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，Spark在Catalyst上进行了大量优化，以提升Spark连接操作的性能。连接操作的应用场景比联合操作更多，因此对联合操作的优化投入较少。
- en: If users don’t use union on entirely different data sources, union operators
    will face a potential performance bottleneck — Catalyst isn’t “smart” to identify
    the shared data frames to reuse.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 如果用户不在完全不同的数据源上使用联合操作，联合操作符将面临潜在的性能瓶颈——Catalyst并不“聪明”到能识别共享的数据框以进行重用。
- en: In this case, Spark will take each data frame as separate branches, then perform
    everything from the root multiple times. In our example, we will perform the two
    big table join four times! It is a huge bottleneck.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，Spark会将每个数据框视为独立的分支，然后从根节点多次执行所有操作。在我们的示例中，我们将对两个大表进行四次连接！这是一个巨大的瓶颈。
- en: Set up an Example with Union Operator in Spark
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在Spark中设置一个使用联合操作符的示例
- en: It’s straightforward to reproduce a non-optimized physical query plan for the
    union operator in Spark. We will do the following
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在Spark中重现一个未优化的物理查询计划对于联合操作符来说非常简单。我们将执行以下操作
- en: Create two data frames from 1 to 1000000\. Let’s call them `df1` and `df2`
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建两个数据框，从1到1000000。我们称它们为`df1`和`df2`
- en: Perform inner join on `df1` and `df2`
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对`df1`和`df2`进行内连接
- en: 'Split the joined result into two data frames: one only contains the odd numbers,
    another one for the even numbers'
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将连接结果拆分成两个数据框：一个仅包含奇数，另一个包含偶数。
- en: Add a transformation with a field called `magic_value` , which is generated
    by two dummy transformations.
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 添加一个名为`magic_value`的转换字段，该字段由两个虚拟转换生成。
- en: Union the odd and even number data frames
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将奇数和偶数数据框进行联合
- en: '[PRE1]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Here is a high-level view of what the DAG looks like. If we look at the DAG
    bottom-up, one thing that stands out is the join happened twice, and the upstream
    almost looks identical.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 这是DAG的高级视图。如果从底部向上查看，显著的一个点是连接操作发生了两次，而且上游几乎看起来一模一样。
- en: We have seen where Spark needs to optimize the union operator extensively, and
    much time is wasted performing unnecessary recomputing if the data source can
    be reused.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到Spark需要广泛优化联合操作符，如果数据源可以重用，大量时间将被浪费在不必要的重新计算上。
- en: '![](../Images/add8c79c2672760e195efa5889b57e5f.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/add8c79c2672760e195efa5889b57e5f.png)'
- en: The DAG for non-optimized query plan for Union Operation | Image By Author
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 未优化查询计划的DAG | 图片由作者提供
- en: Here is the physical plan that has 50 stages scheduled with AQE enabled. We
    can see ids 13 and 27\. Spark did perform join twice on each branch and recomputed
    its branch.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个物理计划，其中有50个阶段，启用了AQE。我们可以看到id 13和27。Spark确实在每个分支上执行了两次连接并重新计算了它的分支。
- en: '![](../Images/f22a40b040403b4d2deb7056b6503f78.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f22a40b040403b4d2deb7056b6503f78.png)'
- en: The non-optimized Physical query plan for Union Operation | Image By Author
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 未优化的联合操作物理查询计划 | 图片由作者提供
- en: How to Improve the Performance of Union Operation
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何提高联合操作的性能
- en: Now we can see this potential bottleneck. How could we resolve this? One option
    is to double the number of executors to run more concurrent tasks. But there is
    a better way to hint to Catalyst and let it reuse the joined data frame from memory.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以看到这个潜在的瓶颈。我们该如何解决这个问题？一个选项是将执行器的数量翻倍，以运行更多的并发任务。但有一个更好的方法是提示Catalyst并让它重用内存中的连接数据框。
- en: To resolve the issue of the Spark performance of union operation, **we can explicitly
    call a** `**cache**` **to persist the joined data frame in memory.** So Catalyst
    knows the shortcut to fetch the data instead of returning it to the source.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决Spark联合操作的性能问题，**我们可以显式调用** `**cache**` **来将连接的数据框保留在内存中。** 这样Catalyst就知道获取数据的快捷方式，而不是返回数据源。
- en: Where to add the `cache()` ? The recommended place would be the data frame before
    the filtering and after the join is completed.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 应该在哪里添加 `cache()`？推荐的位置是在过滤之前和连接完成之后的数据框。
- en: 'Let’s see it in action:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看它的实际效果：
- en: '[PRE2]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Here is the query plan: **InMemoryTableScan** is present, so we can reuse the
    data frame to save other computing.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 这是查询计划：**InMemoryTableScan** 存在，因此我们可以重用数据框以节省其他计算。
- en: '![](../Images/321a65c28576e4c8594712fac1b6d0ba.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/321a65c28576e4c8594712fac1b6d0ba.png)'
- en: The DAG for optimized query plan for Union Operation | Image By Author
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 优化查询计划的DAG | 图片由作者提供
- en: Now the physical plan is reduced to have only 32 stages, and if we check, ids
    1 and 15 both leverage the **InMemoryTableScan.** This could save much more time
    if we split the original data frames into smaller datasets and then union them.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 现在物理计划减少到仅32个阶段，如果我们检查id 1和15都利用了**InMemoryTableScan。** 如果我们将原始数据框分割成较小的数据集，然后将它们联合，这可以节省更多时间。
- en: '![](../Images/95230850a6a7f3d0904d3f591ab9bb8a.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/95230850a6a7f3d0904d3f591ab9bb8a.png)'
- en: Optimized Physical query plan for Union Operation | Image By Author
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 优化的联合操作物理查询计划 | 图片由作者提供
- en: Final Thoughts
  id: totrans-54
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 最终思考
- en: I hope this story helps provide some insights into why sometimes the union operation
    becomes a bottleneck for your Spark performance. Due to the lack of optimization
    in Catalyst for the union operator in Spark, users need to be aware of such caveats
    to develop Spark code more effectively.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 我希望这个故事能够提供一些见解，解释为什么有时联合操作会成为Spark性能的瓶颈。由于Catalyst对Spark中联合操作符缺乏优化，用户需要了解这些警示，以更有效地开发Spark代码。
- en: Adding cache can save time in our example, but it won’t help if the union is
    performed on two completely different data sources and there is no shared place
    to perform cache.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 添加缓存可以节省我们例子中的时间，但如果联合操作是在两个完全不同的数据源上进行，并且没有共享的地方来执行缓存，这将无济于事。
- en: Kazuaki Ishizaki's talk inspires this story — [Goodbye Hell of Unions in Spark
    SQL](https://www.youtube.com/watch?v=c25eT-2dwAg), and my experience handling
    a similar issue for my projects.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 石崎和明的演讲激发了这个故事——[告别Spark SQL中的联合地狱](https://www.youtube.com/watch?v=c25eT-2dwAg)，以及我在项目中处理类似问题的经验。
- en: '[Goodbye Hell of Unions in Spark SQL](https://www.youtube.com/watch?v=c25eT-2dwAg)'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '[告别Spark SQL中的联合地狱](https://www.youtube.com/watch?v=c25eT-2dwAg)'
- en: 'ps: If you are interest in how to handle data skew part of Spark performance,
    I have another story on TDS for it.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: ps：如果你对如何处理Spark性能中的数据倾斜感兴趣，我在TDS上有另一个相关的故事。
- en: '[](/deep-dive-into-handling-apache-spark-data-skew-57ce0d94ee38?source=post_page-----9123ae6ada80--------------------------------)
    [## Deep Dive into Handling Apache Spark Data Skew'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/deep-dive-into-handling-apache-spark-data-skew-57ce0d94ee38?source=post_page-----9123ae6ada80--------------------------------)
    [## 深入了解处理Apache Spark数据倾斜'
- en: The Ultimate Guide To Handle Data Skew In Distributed Compute
  id: totrans-61
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 处理分布式计算中的数据倾斜的终极指南
- en: towardsdatascience.com](/deep-dive-into-handling-apache-spark-data-skew-57ce0d94ee38?source=post_page-----9123ae6ada80--------------------------------)
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '[深入探讨处理 Apache Spark 数据倾斜](https://towardsdatascience.com/deep-dive-into-handling-apache-spark-data-skew-57ce0d94ee38?source=post_page-----9123ae6ada80--------------------------------)'
