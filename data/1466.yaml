- en: OCR-Free Document Data Extraction with Transformers (1/2)
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 无需 OCR 的文档数据提取与变换器 (1/2)
- en: 原文：[https://towardsdatascience.com/ocr-free-document-data-extraction-with-transformers-1-2-b5a826bc2ac3?source=collection_archive---------3-----------------------#2023-04-28](https://towardsdatascience.com/ocr-free-document-data-extraction-with-transformers-1-2-b5a826bc2ac3?source=collection_archive---------3-----------------------#2023-04-28)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/ocr-free-document-data-extraction-with-transformers-1-2-b5a826bc2ac3?source=collection_archive---------3-----------------------#2023-04-28](https://towardsdatascience.com/ocr-free-document-data-extraction-with-transformers-1-2-b5a826bc2ac3?source=collection_archive---------3-----------------------#2023-04-28)
- en: Donut versus Pix2Struct *on custom data*
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Donut 与 Pix2Struct *在自定义数据上的对比*
- en: '[](https://toon-beerten.medium.com/?source=post_page-----b5a826bc2ac3--------------------------------)[![Toon
    Beerten](../Images/f169eaa8cefa00f17176955596972d57.png)](https://toon-beerten.medium.com/?source=post_page-----b5a826bc2ac3--------------------------------)[](https://towardsdatascience.com/?source=post_page-----b5a826bc2ac3--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----b5a826bc2ac3--------------------------------)
    [Toon Beerten](https://toon-beerten.medium.com/?source=post_page-----b5a826bc2ac3--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://toon-beerten.medium.com/?source=post_page-----b5a826bc2ac3--------------------------------)[![图标：Toon
    Beerten](../Images/f169eaa8cefa00f17176955596972d57.png)](https://toon-beerten.medium.com/?source=post_page-----b5a826bc2ac3--------------------------------)[](https://towardsdatascience.com/?source=post_page-----b5a826bc2ac3--------------------------------)[![图标：Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----b5a826bc2ac3--------------------------------)
    [Toon Beerten](https://toon-beerten.medium.com/?source=post_page-----b5a826bc2ac3--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F3aef462e13b5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Focr-free-document-data-extraction-with-transformers-1-2-b5a826bc2ac3&user=Toon+Beerten&userId=3aef462e13b5&source=post_page-3aef462e13b5----b5a826bc2ac3---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----b5a826bc2ac3--------------------------------)
    ·10 min read·Apr 28, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fb5a826bc2ac3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Focr-free-document-data-extraction-with-transformers-1-2-b5a826bc2ac3&user=Toon+Beerten&userId=3aef462e13b5&source=-----b5a826bc2ac3---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F3aef462e13b5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Focr-free-document-data-extraction-with-transformers-1-2-b5a826bc2ac3&user=Toon+Beerten&userId=3aef462e13b5&source=post_page-3aef462e13b5----b5a826bc2ac3---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----b5a826bc2ac3--------------------------------)
    ·10分钟阅读·2023年4月28日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fb5a826bc2ac3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Focr-free-document-data-extraction-with-transformers-1-2-b5a826bc2ac3&user=Toon+Beerten&userId=3aef462e13b5&source=-----b5a826bc2ac3---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '-- '
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fb5a826bc2ac3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Focr-free-document-data-extraction-with-transformers-1-2-b5a826bc2ac3&source=-----b5a826bc2ac3---------------------bookmark_footer-----------)![](../Images/45dc7196c8f321f51a04bce1054c5709.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fb5a826bc2ac3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Focr-free-document-data-extraction-with-transformers-1-2-b5a826bc2ac3&source=-----b5a826bc2ac3---------------------bookmark_footer-----------)![](../Images/45dc7196c8f321f51a04bce1054c5709.png)'
- en: Image by author ([with](https://huggingface.co/spaces/albarji/mixture-of-diffusers))
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图像 ([与](https://huggingface.co/spaces/albarji/mixture-of-diffusers))
- en: '[Donut](https://arxiv.org/abs/2111.15664) and [Pix2Struct](https://arxiv.org/abs/2210.03347)
    are image-to-text models that combine the simplicity of pure pixel inputs with
    visual language understanding tasks. Simply put: an image goes in and extracted
    indexes come out as JSON.'
  id: totrans-9
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '[Donut](https://arxiv.org/abs/2111.15664) 和 [Pix2Struct](https://arxiv.org/abs/2210.03347)
    是图像到文本模型，将纯像素输入的简单性与视觉语言理解任务相结合。简单来说：输入一张图像，提取的索引以 JSON 格式输出。'
- en: 'Recently I [released](https://huggingface.co/spaces/to-be/invoice_document_headers_extraction_with_donut)
    a Donut model finetuned on invoices. Ever so often I get the question how to train
    with a custom dataset. Also, a similar model was released: Pix2Struct, it claims
    to be significantly better. But is that so?'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 最近我[发布了](https://huggingface.co/spaces/to-be/invoice_document_headers_extraction_with_donut)一个在发票上微调的Donut模型。我经常收到如何使用自定义数据集进行训练的问题。此外，还发布了一个类似的模型：Pix2Struct，它声称性能显著更好。但真的是这样吗？
- en: 'Time to roll up my sleeves. I will show you:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 该是卷起袖子的时候了。我将展示给你：
- en: how to prepare your data for finetuning Donut and Pix2Struct
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何为Donut和Pix2Struct微调准备数据
- en: the training procedure for both models
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 两种模型的训练过程
- en: comparative results on an actual dataset
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实际数据集上的比较结果
- en: Of course I’ll provide the colab notebooks as well, for easy experimentation
    and/or replication from your end.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，我也会提供colab笔记本，以便于你的实验和/或复制。
- en: '**Dataset**'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '**数据集**'
- en: 'To do this comparison, I need a public dataset. I wanted to avoid the usual
    ones for document understanding tasks such as [CORD](https://github.com/clovaai/cord),
    had a look around and found the [Ghega dataset](https://machinelearning.inginf.units.it/data-and-tools/ghega-dataset).
    It’s quite small (~250 documents) and consists out of 2 types of documents: patent
    applications and datasheets. With the different types we can simulate a classification
    problem. Per type we have multiple indexes to extract. These indexes are unique
    for the type. Exactly what I need. Prof. [Medvet](https://medvet.inginf.units.it/)
    from the machine learning lab at the university of Trieste graciously approved
    the usage for these articles.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 要进行此比较，我需要一个公开的数据集。我想避免使用通常用于文档理解任务的数据集，例如[CORD](https://github.com/clovaai/cord)，浏览了一下，发现了[Ghega数据集](https://machinelearning.inginf.units.it/data-and-tools/ghega-dataset)。它相当小（约250个文档），由2种类型的文档组成：专利申请和数据表。通过不同类型，我们可以模拟一个分类问题。每种类型我们都有多个索引需要提取。这些索引对于每种类型都是唯一的。正是我所需要的。来自的Trieste大学机器学习实验室的[Medvet](https://medvet.inginf.units.it/)教授慷慨批准了这些文章的使用。
- en: The dataset seems to be quite old so it needs to be investigated if it still
    suits our goal.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集似乎比较旧，所以需要调查它是否仍然适合我们的目标。
- en: '**First exploration**'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '**初步探索**'
- en: 'When you get a new set of data, you first need to get acquainted with how it
    is structured. Luckily the website’s detailed description aides us. This is the
    dataset file structure:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 当你获得一组新的数据时，你首先需要熟悉其结构。幸运的是，网站的详细描述对我们很有帮助。这是数据集的文件结构：
- en: '[PRE0]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'We can see two main subfolders for the two doctypes: *datasheets* and *patents*.
    One level lower we have subfolders that are not important by themselves, but they
    contain files that start with a certain prefix. We can see a unique identifier,
    e.g. *document-000–123542* . For each of these identifiers we have 4 kinds of
    data:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到两个主要的子文件夹对应两个文档类型：*数据表*和*专利*。在更下一级，我们有一些子文件夹，这些子文件夹本身不重要，但它们包含以某个前缀开头的文件。我们可以看到一个唯一的标识符，例如*document-000–123542*。对于每个这些标识符，我们有4种数据：
- en: The *blocks.csv* file contains info about bounding boxes. As Donut or Pix2Struct
    don’t use this info, we can ignore these files.
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*blocks.csv* 文件包含有关边界框的信息。由于Donut或Pix2Struct不使用这些信息，我们可以忽略这些文件。'
- en: The *out.000.png* file is the postprocessed (deskewed) image file. As I would
    rather test on unprocessed files, I will ignore these as well.
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*out.000.png* 文件是后处理（去倾斜）的图像文件。由于我更愿意测试未处理的文件，我也会忽略这些。'
- en: The raw, unprocessed document image has the *in.000.png* suffix. That’s what
    we are interested in.
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 原始的、未处理的文档图像有一个 *in.000.png* 后缀。这是我们感兴趣的。
- en: And finally the corresponding *groundtruth.csv* file. This contains indexes
    for this image that we consider the ground truth.
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后是相应的*groundtruth.csv*文件。这包含我们认为是实际标注的图像索引。
- en: 'Here is a sample groundtruth csv along with the column description:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是一个示例groundtruth csv文件以及列描述：
- en: '[PRE1]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '[PRE2]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'So that means we are only interested in the first and last column. The first
    being the *key* and the last being the *value*. In this case:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着我们只对第一列和最后一列感兴趣。第一列是*键*，最后一列是*值*。在这种情况下：
- en: '[PRE3]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: So that means that for this document we will finetune the models to look for
    a ‘*Case*’ with value ‘*MELF CASE*’ and also to extract a ‘*StorageTemperature*’
    that is ‘*-65 to +200*’.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着对于该文档，我们将微调模型以查找‘*Case*’的值为‘*MELF CASE*’，并且提取一个‘*StorageTemperature*’，其值为‘*-65
    to +200*’。
- en: '**Indexes**'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '**索引**'
- en: 'The following indexes exist in the groundtruth metadata:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在groundtruth元数据中存在以下索引：
- en: '**data-sheets**: Model, Type, Case, Power Dissipation, Storage Temperature,
    Voltage, Weight, Thermal Resistance'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据表**：型号、类型、外壳、功耗、储存温度、电压、重量、热阻'
- en: '**patents**: Title, Applicant, Inventor, Representative, Filing Date, Publication
    Date, Application Number, Publication Number, Priority, Classification, Abstract
    1st line'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**专利**：标题、申请人、发明人、代表、申请日期、出版日期、申请编号、出版编号、优先权、分类、摘要第一行'
- en: 'Looking at the quality of the ground truth and feasibility I choose to retain
    the following indexes:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 观察到地面真实值的质量和可行性，我选择保留以下索引：
- en: '[PRE4]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '**Quality**'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '**质量**'
- en: For the image conversion to text, [ocropus](http://code.google.com/p/ocropus/)
    version 0.2 was used. Which means it dates to about the end of 2014\. This is
    ancient in terms of data science, so does the groundtruth quality live up to our
    task?
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 对于图像转换为文本，使用了 [ocropus](http://code.google.com/p/ocropus/) 版本 0.2。这意味着它大约在 2014
    年底发布。在数据科学领域这已经很古老了，那么地面真实度的质量是否符合我们的任务要求呢？
- en: 'For this I had a look at random images and compared the groundtruth with was
    actually written on the document. Here are two examples where the OCR was incorrect:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 为此，我查看了一些随机图像，并将地面真实值与实际在文档上写的内容进行了比较。以下是两个 OCR 不正确的示例：
- en: '![](../Images/798c47d8cb3079a86d6cc8af74e49659.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/798c47d8cb3079a86d6cc8af74e49659.png)'
- en: document-001–109381.in.000.png from Ghega dataset
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 来自 Ghega 数据集的 document-001–109381.in.000.png
- en: The key *Classification* is set as *BGSD 81/00* as ground truth. And it should
    be *B65D 81/100.*
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 键 *Classification* 被设置为 *BGSD 81/00* 作为地面真实值。它应该是 *B65D 81/100*。
- en: '![](../Images/da972563105ade20c85a3167784b56a5.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/da972563105ade20c85a3167784b56a5.png)'
- en: document-003–112107.in.000.png from Ghega dataset
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 来自 Ghega 数据集的 document-003–112107.in.000.png
- en: The key *StorageTemperature* says *I -65 {O + 150* as ground truth, while we
    can see it should be *-65 to + 150.*
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 键 *StorageTemperature* 显示 *I -65 {O + 150* 作为地面真实值，而我们可以看到它应该是 *-65 to + 150*。
- en: There are many such errors in the dataset. One approach is to correct these.
    Another to ignore. Since I will use the same data just for comparing both models,
    I choose the latter. Shall the data be used for production, you may want to choose
    the former option to get the best results.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集中有许多此类错误。一种方法是纠正这些错误。另一种是忽略这些错误。由于我将使用相同的数据来比较两个模型，我选择了后者。如果数据用于生产，你可能需要选择前一种方法以获得最佳结果。
- en: (also note that these special characters could mess up the JSON format, I will
    come back to that topic later)
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: （还要注意，这些特殊字符可能会搞乱 JSON 格式，稍后我会回到这个话题）
- en: '**Donut dataset structure**'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '**Donut 数据集结构**'
- en: What does the format of the data we need it to be in look like?
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要的数据格式是什么样的？
- en: For finetuning the Donut model we need to have the data organized in one folder
    with all the documents as separate image files and one metadata file, structured
    as a JSON lines file.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 对于微调 Donut 模型，我们需要将数据组织在一个文件夹中，所有文档作为单独的图像文件和一个元数据文件，结构为 JSON lines 文件。
- en: '[PRE5]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The JSONL file contains per image file a line like this:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: JSONL 文件包含每个图像文件一行，格式如下：
- en: '[PRE6]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Let’s break down this JSON line. On the upper level we have a dict with two
    elements: *file_name* and *ground_truth*. Under the *ground_truth* key, we have
    a dict with key *gt_parse*. The value is in itself a dict with the key value pairs
    that we know on the document. Or even better: *assign*. Remember that the doctype
    is not necessarily present as text in the document. The term *datasheet* is not
    present as text on those documents.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们分解这行 JSON。在上层我们有一个包含两个元素的字典：*file_name* 和 *ground_truth*。在 *ground_truth*
    键下，我们有一个包含 *gt_parse* 键的字典。其值本身是一个字典，包含我们在文档中知道的键值对。或者更好：*assign*。记住，文档中不一定会出现文档类型。术语
    *datasheet* 并没有作为文本出现在这些文档中。
- en: Luckily pix2struct uses the same format for finetuning, so we can kill two birds
    with one stone. Once we have converted it in this structure, we can use it for
    finetuning Pix2Struct as well.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，pix2struct 使用相同的格式进行微调，因此我们可以一举两得。一旦我们将其转换为这种结构，我们还可以用来微调 Pix2Struct。
- en: '**Conversion**'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '**转换**'
- en: For the conversion itself, I created a Jupyter notebook on colab. I decided
    to create a split into a train and validation set at this stage, as opposed to
    just before finetuning. This way, the same validation images will be used for
    both models and the results will be better comparable. One out of 5 documents
    will be used for validation.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 对于转换本身，我在 colab 上创建了一个 Jupyter notebook。我决定在这个阶段将数据拆分为训练集和验证集，而不是在微调之前。这种方式，两个模型将使用相同的验证图像，结果会更具可比性。五个文档中会有一个用于验证。
- en: 'With the above knowledge of the structure of the Ghega dataset, we can construe
    the conversion procedure as follows:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 利用上述 Ghega 数据集的结构知识，我们可以将转换过程概括如下：
- en: For every filename ending in in.000.png we take the corresponding groundtruth
    file and create a temporary dataframe object.
  id: totrans-61
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 对于每个以 in.000.png 结尾的文件名，我们取对应的 groundtruth 文件并创建一个临时的数据框对象。
- en: Beware that the groundtruth could be empty or doesn’t exist entirely. (e.g.
    for *datasheets/taiwan-switching*)
  id: totrans-62
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 注意，groundtruth 可能为空或完全不存在。（例如，对于 *datasheets/taiwan-switching*）
- en: 'Next, we deduct the class from the subfolder: *patent* or *datasheet .* Now
    we have to build the JSON line. For each element/index we want to extract, we
    check if it is in that dataframe and collect it. Then copy the image itself.'
  id: totrans-63
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 接下来，我们从子文件夹中扣除类：*patent* 或 *datasheet* 。现在我们需要构建 JSON 行。对于每个我们想提取的元素/索引，我们检查它是否在数据框中并进行收集。然后复制图像本身。
- en: Do this for all images and at the end we have a JSONL file to write out.
  id: totrans-64
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 对所有图像执行此操作，最后我们就有一个 JSONL 文件可以写出。
- en: 'In python it looks like this:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Python 中，它看起来是这样的：
- en: '[PRE7]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: The ghega_df is a dataframe to do some sanity checks or statistical analysis
    on if wanted. I used it to check random samples if my converted data was actually
    correct.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: ghega_df 是一个数据框，用于进行一些合理性检查或统计分析（如有需要）。我用它来检查随机样本，验证我的转换数据是否正确。
- en: '**Hiccups**'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题**'
- en: Once converted, it looks like everything is all copacetic. But I want to get
    rid of the idea that everything usually runs from the first try. There are always
    small unexpected hiccups happening. Talking about the errors I encountered and
    showing the remedies is useful for anybody mimicking this whole process with their
    own dataset.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 转换完成后，一切看起来都很顺利。但我想摆脱那种通常第一次尝试就能成功的想法。总是会有一些小的意外问题发生。谈论我遇到的错误并展示解决方案对任何模拟整个过程并使用自己数据集的人都是有用的。
- en: 'For example, after converting the dataset, I wanted to train the Donut model.
    Before I can do that I need to create a train dataset, like so:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在转换数据集后，我想训练 Donut 模型。在此之前，我需要创建一个训练数据集，如下所示：
- en: '[PRE8]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'And got this error:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 并且出现了这个错误：
- en: '[PRE9]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'So it seems there is a problem with the JSON format in row 7\. I copied that
    line and pasted it in an [online JSON validator](https://jsonformatter.curiousconcept.com/#):'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来第 7 行的 JSON 格式有问题。我复制了那一行并将其粘贴到一个 [在线 JSON 验证器](https://jsonformatter.curiousconcept.com/#)
    中：
- en: '![](../Images/fcc1e25cb0180b26712b1496194d27d4.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fcc1e25cb0180b26712b1496194d27d4.png)'
- en: Image by author
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图像
- en: '![](../Images/732f1846570edeab4c0b07c4006f595b.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/732f1846570edeab4c0b07c4006f595b.png)'
- en: Image by author
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图像
- en: '![](../Images/732f1846570edeab4c0b07c4006f595b.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/732f1846570edeab4c0b07c4006f595b.png)'
- en: Image by author
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图像
- en: 'It however says it’s a valid JSON line. So let’s have a deeper look:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，它表示这是一个有效的 JSON 行。让我们更深入地看看：
- en: '[PRE10]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Did you spot the error? After some time I noticed there’s a missing comma between
    the *DocType* and *FilingDate*. It was however missing on all lines, so it’s unclear
    to me why it says line 7 has a problem. When I fixed this issue, I tried again
    and now it claims there’s a problem on line 17:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 你发现错误了吗？经过一段时间，我注意到 *DocType* 和 *FilingDate* 之间缺少逗号。然而，这在所有行中都是缺失的，所以我不清楚为什么第
    7 行会出现问题。当我修复了这个问题后，我再次尝试，现在它声称第 17 行有问题：
- en: '[PRE11]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Here is line 17, do you spot the problem?
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 这是第 17 行，你发现了问题吗？
- en: '[PRE12]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'It’s the unescaped quotation marks for the *Classification* element. To remedy
    this, I made a decision that all values will only be allowed to contain alphanumeric
    and a few special characters with this regex:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 这是*Classification* 元素的未转义引号。为了解决这个问题，我决定所有值只能包含字母数字字符和一些特殊字符，并使用了这个正则表达式：
- en: '[PRE13]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: This may affect the true performance badly, but from what I could see, any other
    characters were caused by OCR errors anyway. I suppose for the relative comparison
    between the models leaving them out doesn’t matter much.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 这可能会严重影响真实性能，但从我所见，其他字符都是由于 OCR 错误引起的。我认为，对于模型之间的相对比较，忽略这些字符影响不大。
- en: '**Data preparation: done**'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '**数据准备：完成**'
- en: It’s often overlooked and certainly underestimated, the importance of preparing
    data before training. With the steps above I have shown you how you can adapt
    your own data to be used by both Donut and Pix2Struct for key index extraction
    on documents. Common pitfalls were also remedied. The Jupyter notebook with all
    steps can be found [here](https://github.com/Toon-nooT/notebooks/blob/main/Donut_vs_pix2struct_1_Ghega_data_prep.ipynb).
    We’re halfway there. The next step is to train both models on this dataset. I’m
    very curious how well they fare, but the comparison and training will be for a
    next article.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 数据准备的重要性常被忽视且被低估。通过上述步骤，我展示了如何调整自己的数据，以便Donut和Pix2Struct用于文档的关键索引提取。常见的陷阱也得到了修正。包含所有步骤的Jupyter笔记本可以在[这里](https://github.com/Toon-nooT/notebooks/blob/main/Donut_vs_pix2struct_1_Ghega_data_prep.ipynb)找到。我们已经完成了一半。下一步是用这个数据集训练这两个模型。我非常好奇它们的表现如何，但比较和训练将留到下一篇文章中。
- en: 'You may also like:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能还喜欢：
- en: '[](https://toon-beerten.medium.com/hands-on-document-data-extraction-with-transformer-7130df3b6132?source=post_page-----b5a826bc2ac3--------------------------------)
    [## Hands-on: document data extraction with 🍩 transformer'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://toon-beerten.medium.com/hands-on-document-data-extraction-with-transformer-7130df3b6132?source=post_page-----b5a826bc2ac3--------------------------------)
    [## 实战：使用🍩变换器进行文档数据提取'
- en: My experience using donut transformers model to extract invoice indexes.
  id: totrans-94
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 我使用Donut变换器模型提取发票索引的经验。
- en: toon-beerten.medium.com](https://toon-beerten.medium.com/hands-on-document-data-extraction-with-transformer-7130df3b6132?source=post_page-----b5a826bc2ac3--------------------------------)
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: toon-beerten.medium.com](https://toon-beerten.medium.com/hands-on-document-data-extraction-with-transformer-7130df3b6132?source=post_page-----b5a826bc2ac3--------------------------------)
- en: 'References:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 参考文献：
- en: '[](https://arxiv.org/abs/2111.15664?source=post_page-----b5a826bc2ac3--------------------------------)
    [## OCR-free Document Understanding Transformer'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://arxiv.org/abs/2111.15664?source=post_page-----b5a826bc2ac3--------------------------------)
    [## 无OCR文档理解变换器'
- en: Understanding document images (e.g., invoices) is a core but challenging task
    since it requires complex functions such…
  id: totrans-98
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 理解文档图像（例如，发票）是一项核心但具有挑战性的任务，因为它需要复杂的功能…
- en: 'arxiv.org](https://arxiv.org/abs/2111.15664?source=post_page-----b5a826bc2ac3--------------------------------)
    [](https://arxiv.org/abs/2210.03347?source=post_page-----b5a826bc2ac3--------------------------------)
    [## Pix2Struct: Screenshot Parsing as Pretraining for Visual Language Understanding'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: arxiv.org](https://arxiv.org/abs/2111.15664?source=post_page-----b5a826bc2ac3--------------------------------)
    [](https://arxiv.org/abs/2210.03347?source=post_page-----b5a826bc2ac3--------------------------------)
    [## Pix2Struct：作为视觉语言理解预训练的截图解析
- en: Visually-situated language is ubiquitous -- sources range from textbooks with
    diagrams to web pages with images and…
  id: totrans-100
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 视觉位置语言无处不在——来源包括带有图表的教科书到包含图像的网页…
- en: arxiv.org](https://arxiv.org/abs/2210.03347?source=post_page-----b5a826bc2ac3--------------------------------)
    [](https://machinelearning.inginf.units.it/data-and-tools/ghega-dataset?source=post_page-----b5a826bc2ac3--------------------------------)
    [## Machine Learning Lab - Ghega dataset
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: arxiv.org](https://arxiv.org/abs/2210.03347?source=post_page-----b5a826bc2ac3--------------------------------)
    [](https://machinelearning.inginf.units.it/data-and-tools/ghega-dataset?source=post_page-----b5a826bc2ac3--------------------------------)
    [## 机器学习实验室 - Ghega数据集
- en: 'Ghega-dataset: a dataset for document understanding and classification We provide
    here a labeled dataset which can be…'
  id: totrans-102
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Ghega数据集：用于文档理解和分类的数据集，我们提供了一个标注数据集，可以…
- en: machinelearning.inginf.units.it](https://machinelearning.inginf.units.it/data-and-tools/ghega-dataset?source=post_page-----b5a826bc2ac3--------------------------------)
    [](https://huggingface.co/to-be/donut-base-finetuned-invoices?source=post_page-----b5a826bc2ac3--------------------------------)
    [## to-be/donut-base-finetuned-invoices · Hugging Face
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: machinelearning.inginf.units.it](https://machinelearning.inginf.units.it/data-and-tools/ghega-dataset?source=post_page-----b5a826bc2ac3--------------------------------)
    [](https://huggingface.co/to-be/donut-base-finetuned-invoices?source=post_page-----b5a826bc2ac3--------------------------------)
    [## to-be/donut-base-finetuned-invoices · Hugging Face
- en: Edit model card Based on Donut base model (introduced in the paper OCR-free
    Document Understanding Transformer by…
  id: totrans-104
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 编辑模型卡 基于Donut基础模型（在论文《无OCR文档理解变换器》中介绍）…
- en: huggingface.co](https://huggingface.co/to-be/donut-base-finetuned-invoices?source=post_page-----b5a826bc2ac3--------------------------------)
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: huggingface.co](https://huggingface.co/to-be/donut-base-finetuned-invoices?source=post_page-----b5a826bc2ac3--------------------------------)
