- en: OCR-Free Document Data Extraction with Transformers (1/2)
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ— éœ€ OCR çš„æ–‡æ¡£æ•°æ®æå–ä¸å˜æ¢å™¨ (1/2)
- en: åŸæ–‡ï¼š[https://towardsdatascience.com/ocr-free-document-data-extraction-with-transformers-1-2-b5a826bc2ac3?source=collection_archive---------3-----------------------#2023-04-28](https://towardsdatascience.com/ocr-free-document-data-extraction-with-transformers-1-2-b5a826bc2ac3?source=collection_archive---------3-----------------------#2023-04-28)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡ï¼š[https://towardsdatascience.com/ocr-free-document-data-extraction-with-transformers-1-2-b5a826bc2ac3?source=collection_archive---------3-----------------------#2023-04-28](https://towardsdatascience.com/ocr-free-document-data-extraction-with-transformers-1-2-b5a826bc2ac3?source=collection_archive---------3-----------------------#2023-04-28)
- en: Donut versus Pix2Struct *on custom data*
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Donut ä¸ Pix2Struct *åœ¨è‡ªå®šä¹‰æ•°æ®ä¸Šçš„å¯¹æ¯”*
- en: '[](https://toon-beerten.medium.com/?source=post_page-----b5a826bc2ac3--------------------------------)[![Toon
    Beerten](../Images/f169eaa8cefa00f17176955596972d57.png)](https://toon-beerten.medium.com/?source=post_page-----b5a826bc2ac3--------------------------------)[](https://towardsdatascience.com/?source=post_page-----b5a826bc2ac3--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----b5a826bc2ac3--------------------------------)
    [Toon Beerten](https://toon-beerten.medium.com/?source=post_page-----b5a826bc2ac3--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://toon-beerten.medium.com/?source=post_page-----b5a826bc2ac3--------------------------------)[![å›¾æ ‡ï¼šToon
    Beerten](../Images/f169eaa8cefa00f17176955596972d57.png)](https://toon-beerten.medium.com/?source=post_page-----b5a826bc2ac3--------------------------------)[](https://towardsdatascience.com/?source=post_page-----b5a826bc2ac3--------------------------------)[![å›¾æ ‡ï¼šTowards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----b5a826bc2ac3--------------------------------)
    [Toon Beerten](https://toon-beerten.medium.com/?source=post_page-----b5a826bc2ac3--------------------------------)'
- en: Â·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: Â·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F3aef462e13b5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Focr-free-document-data-extraction-with-transformers-1-2-b5a826bc2ac3&user=Toon+Beerten&userId=3aef462e13b5&source=post_page-3aef462e13b5----b5a826bc2ac3---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----b5a826bc2ac3--------------------------------)
    Â·10 min readÂ·Apr 28, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fb5a826bc2ac3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Focr-free-document-data-extraction-with-transformers-1-2-b5a826bc2ac3&user=Toon+Beerten&userId=3aef462e13b5&source=-----b5a826bc2ac3---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[å…³æ³¨](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F3aef462e13b5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Focr-free-document-data-extraction-with-transformers-1-2-b5a826bc2ac3&user=Toon+Beerten&userId=3aef462e13b5&source=post_page-3aef462e13b5----b5a826bc2ac3---------------------post_header-----------)
    å‘è¡¨åœ¨ [Towards Data Science](https://towardsdatascience.com/?source=post_page-----b5a826bc2ac3--------------------------------)
    Â·10åˆ†é’Ÿé˜…è¯»Â·2023å¹´4æœˆ28æ—¥[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fb5a826bc2ac3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Focr-free-document-data-extraction-with-transformers-1-2-b5a826bc2ac3&user=Toon+Beerten&userId=3aef462e13b5&source=-----b5a826bc2ac3---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '-- '
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fb5a826bc2ac3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Focr-free-document-data-extraction-with-transformers-1-2-b5a826bc2ac3&source=-----b5a826bc2ac3---------------------bookmark_footer-----------)![](../Images/45dc7196c8f321f51a04bce1054c5709.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fb5a826bc2ac3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Focr-free-document-data-extraction-with-transformers-1-2-b5a826bc2ac3&source=-----b5a826bc2ac3---------------------bookmark_footer-----------)![](../Images/45dc7196c8f321f51a04bce1054c5709.png)'
- en: Image by author ([with](https://huggingface.co/spaces/albarji/mixture-of-diffusers))
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: ä½œè€…æä¾›çš„å›¾åƒ ([ä¸](https://huggingface.co/spaces/albarji/mixture-of-diffusers))
- en: '[Donut](https://arxiv.org/abs/2111.15664) and [Pix2Struct](https://arxiv.org/abs/2210.03347)
    are image-to-text models that combine the simplicity of pure pixel inputs with
    visual language understanding tasks. Simply put: an image goes in and extracted
    indexes come out as JSON.'
  id: totrans-9
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '[Donut](https://arxiv.org/abs/2111.15664) å’Œ [Pix2Struct](https://arxiv.org/abs/2210.03347)
    æ˜¯å›¾åƒåˆ°æ–‡æœ¬æ¨¡å‹ï¼Œå°†çº¯åƒç´ è¾“å…¥çš„ç®€å•æ€§ä¸è§†è§‰è¯­è¨€ç†è§£ä»»åŠ¡ç›¸ç»“åˆã€‚ç®€å•æ¥è¯´ï¼šè¾“å…¥ä¸€å¼ å›¾åƒï¼Œæå–çš„ç´¢å¼•ä»¥ JSON æ ¼å¼è¾“å‡ºã€‚'
- en: 'Recently I [released](https://huggingface.co/spaces/to-be/invoice_document_headers_extraction_with_donut)
    a Donut model finetuned on invoices. Ever so often I get the question how to train
    with a custom dataset. Also, a similar model was released: Pix2Struct, it claims
    to be significantly better. But is that so?'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€è¿‘æˆ‘[å‘å¸ƒäº†](https://huggingface.co/spaces/to-be/invoice_document_headers_extraction_with_donut)ä¸€ä¸ªåœ¨å‘ç¥¨ä¸Šå¾®è°ƒçš„Donutæ¨¡å‹ã€‚æˆ‘ç»å¸¸æ”¶åˆ°å¦‚ä½•ä½¿ç”¨è‡ªå®šä¹‰æ•°æ®é›†è¿›è¡Œè®­ç»ƒçš„é—®é¢˜ã€‚æ­¤å¤–ï¼Œè¿˜å‘å¸ƒäº†ä¸€ä¸ªç±»ä¼¼çš„æ¨¡å‹ï¼šPix2Structï¼Œå®ƒå£°ç§°æ€§èƒ½æ˜¾è‘—æ›´å¥½ã€‚ä½†çœŸçš„æ˜¯è¿™æ ·å—ï¼Ÿ
- en: 'Time to roll up my sleeves. I will show you:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: è¯¥æ˜¯å·èµ·è¢–å­çš„æ—¶å€™äº†ã€‚æˆ‘å°†å±•ç¤ºç»™ä½ ï¼š
- en: how to prepare your data for finetuning Donut and Pix2Struct
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¦‚ä½•ä¸ºDonutå’ŒPix2Structå¾®è°ƒå‡†å¤‡æ•°æ®
- en: the training procedure for both models
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¸¤ç§æ¨¡å‹çš„è®­ç»ƒè¿‡ç¨‹
- en: comparative results on an actual dataset
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å®é™…æ•°æ®é›†ä¸Šçš„æ¯”è¾ƒç»“æœ
- en: Of course Iâ€™ll provide the colab notebooks as well, for easy experimentation
    and/or replication from your end.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: å½“ç„¶ï¼Œæˆ‘ä¹Ÿä¼šæä¾›colabç¬”è®°æœ¬ï¼Œä»¥ä¾¿äºä½ çš„å®éªŒå’Œ/æˆ–å¤åˆ¶ã€‚
- en: '**Dataset**'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '**æ•°æ®é›†**'
- en: 'To do this comparison, I need a public dataset. I wanted to avoid the usual
    ones for document understanding tasks such as [CORD](https://github.com/clovaai/cord),
    had a look around and found the [Ghega dataset](https://machinelearning.inginf.units.it/data-and-tools/ghega-dataset).
    Itâ€™s quite small (~250 documents) and consists out of 2 types of documents: patent
    applications and datasheets. With the different types we can simulate a classification
    problem. Per type we have multiple indexes to extract. These indexes are unique
    for the type. Exactly what I need. Prof. [Medvet](https://medvet.inginf.units.it/)
    from the machine learning lab at the university of Trieste graciously approved
    the usage for these articles.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: è¦è¿›è¡Œæ­¤æ¯”è¾ƒï¼Œæˆ‘éœ€è¦ä¸€ä¸ªå…¬å¼€çš„æ•°æ®é›†ã€‚æˆ‘æƒ³é¿å…ä½¿ç”¨é€šå¸¸ç”¨äºæ–‡æ¡£ç†è§£ä»»åŠ¡çš„æ•°æ®é›†ï¼Œä¾‹å¦‚[CORD](https://github.com/clovaai/cord)ï¼Œæµè§ˆäº†ä¸€ä¸‹ï¼Œå‘ç°äº†[Ghegaæ•°æ®é›†](https://machinelearning.inginf.units.it/data-and-tools/ghega-dataset)ã€‚å®ƒç›¸å½“å°ï¼ˆçº¦250ä¸ªæ–‡æ¡£ï¼‰ï¼Œç”±2ç§ç±»å‹çš„æ–‡æ¡£ç»„æˆï¼šä¸“åˆ©ç”³è¯·å’Œæ•°æ®è¡¨ã€‚é€šè¿‡ä¸åŒç±»å‹ï¼Œæˆ‘ä»¬å¯ä»¥æ¨¡æ‹Ÿä¸€ä¸ªåˆ†ç±»é—®é¢˜ã€‚æ¯ç§ç±»å‹æˆ‘ä»¬éƒ½æœ‰å¤šä¸ªç´¢å¼•éœ€è¦æå–ã€‚è¿™äº›ç´¢å¼•å¯¹äºæ¯ç§ç±»å‹éƒ½æ˜¯å”¯ä¸€çš„ã€‚æ­£æ˜¯æˆ‘æ‰€éœ€è¦çš„ã€‚æ¥è‡ªçš„Triesteå¤§å­¦æœºå™¨å­¦ä¹ å®éªŒå®¤çš„[Medvet](https://medvet.inginf.units.it/)æ•™æˆæ…·æ…¨æ‰¹å‡†äº†è¿™äº›æ–‡ç« çš„ä½¿ç”¨ã€‚
- en: The dataset seems to be quite old so it needs to be investigated if it still
    suits our goal.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: æ•°æ®é›†ä¼¼ä¹æ¯”è¾ƒæ—§ï¼Œæ‰€ä»¥éœ€è¦è°ƒæŸ¥å®ƒæ˜¯å¦ä»ç„¶é€‚åˆæˆ‘ä»¬çš„ç›®æ ‡ã€‚
- en: '**First exploration**'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '**åˆæ­¥æ¢ç´¢**'
- en: 'When you get a new set of data, you first need to get acquainted with how it
    is structured. Luckily the websiteâ€™s detailed description aides us. This is the
    dataset file structure:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: å½“ä½ è·å¾—ä¸€ç»„æ–°çš„æ•°æ®æ—¶ï¼Œä½ é¦–å…ˆéœ€è¦ç†Ÿæ‚‰å…¶ç»“æ„ã€‚å¹¸è¿çš„æ˜¯ï¼Œç½‘ç«™çš„è¯¦ç»†æè¿°å¯¹æˆ‘ä»¬å¾ˆæœ‰å¸®åŠ©ã€‚è¿™æ˜¯æ•°æ®é›†çš„æ–‡ä»¶ç»“æ„ï¼š
- en: '[PRE0]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'We can see two main subfolders for the two doctypes: *datasheets* and *patents*.
    One level lower we have subfolders that are not important by themselves, but they
    contain files that start with a certain prefix. We can see a unique identifier,
    e.g. *document-000â€“123542* . For each of these identifiers we have 4 kinds of
    data:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ä¸¤ä¸ªä¸»è¦çš„å­æ–‡ä»¶å¤¹å¯¹åº”ä¸¤ä¸ªæ–‡æ¡£ç±»å‹ï¼š*æ•°æ®è¡¨*å’Œ*ä¸“åˆ©*ã€‚åœ¨æ›´ä¸‹ä¸€çº§ï¼Œæˆ‘ä»¬æœ‰ä¸€äº›å­æ–‡ä»¶å¤¹ï¼Œè¿™äº›å­æ–‡ä»¶å¤¹æœ¬èº«ä¸é‡è¦ï¼Œä½†å®ƒä»¬åŒ…å«ä»¥æŸä¸ªå‰ç¼€å¼€å¤´çš„æ–‡ä»¶ã€‚æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ä¸€ä¸ªå”¯ä¸€çš„æ ‡è¯†ç¬¦ï¼Œä¾‹å¦‚*document-000â€“123542*ã€‚å¯¹äºæ¯ä¸ªè¿™äº›æ ‡è¯†ç¬¦ï¼Œæˆ‘ä»¬æœ‰4ç§æ•°æ®ï¼š
- en: The *blocks.csv* file contains info about bounding boxes. As Donut or Pix2Struct
    donâ€™t use this info, we can ignore these files.
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*blocks.csv* æ–‡ä»¶åŒ…å«æœ‰å…³è¾¹ç•Œæ¡†çš„ä¿¡æ¯ã€‚ç”±äºDonutæˆ–Pix2Structä¸ä½¿ç”¨è¿™äº›ä¿¡æ¯ï¼Œæˆ‘ä»¬å¯ä»¥å¿½ç•¥è¿™äº›æ–‡ä»¶ã€‚'
- en: The *out.000.png* file is the postprocessed (deskewed) image file. As I would
    rather test on unprocessed files, I will ignore these as well.
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*out.000.png* æ–‡ä»¶æ˜¯åå¤„ç†ï¼ˆå»å€¾æ–œï¼‰çš„å›¾åƒæ–‡ä»¶ã€‚ç”±äºæˆ‘æ›´æ„¿æ„æµ‹è¯•æœªå¤„ç†çš„æ–‡ä»¶ï¼Œæˆ‘ä¹Ÿä¼šå¿½ç•¥è¿™äº›ã€‚'
- en: The raw, unprocessed document image has the *in.000.png* suffix. Thatâ€™s what
    we are interested in.
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åŸå§‹çš„ã€æœªå¤„ç†çš„æ–‡æ¡£å›¾åƒæœ‰ä¸€ä¸ª *in.000.png* åç¼€ã€‚è¿™æ˜¯æˆ‘ä»¬æ„Ÿå…´è¶£çš„ã€‚
- en: And finally the corresponding *groundtruth.csv* file. This contains indexes
    for this image that we consider the ground truth.
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æœ€åæ˜¯ç›¸åº”çš„*groundtruth.csv*æ–‡ä»¶ã€‚è¿™åŒ…å«æˆ‘ä»¬è®¤ä¸ºæ˜¯å®é™…æ ‡æ³¨çš„å›¾åƒç´¢å¼•ã€‚
- en: 'Here is a sample groundtruth csv along with the column description:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é‡Œæ˜¯ä¸€ä¸ªç¤ºä¾‹groundtruth csvæ–‡ä»¶ä»¥åŠåˆ—æè¿°ï¼š
- en: '[PRE1]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '[PRE2]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'So that means we are only interested in the first and last column. The first
    being the *key* and the last being the *value*. In this case:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ„å‘³ç€æˆ‘ä»¬åªå¯¹ç¬¬ä¸€åˆ—å’Œæœ€åä¸€åˆ—æ„Ÿå…´è¶£ã€‚ç¬¬ä¸€åˆ—æ˜¯*é”®*ï¼Œæœ€åä¸€åˆ—æ˜¯*å€¼*ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼š
- en: '[PRE3]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: So that means that for this document we will finetune the models to look for
    a â€˜*Case*â€™ with value â€˜*MELF CASE*â€™ and also to extract a â€˜*StorageTemperature*â€™
    that is â€˜*-65 to +200*â€™.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ„å‘³ç€å¯¹äºè¯¥æ–‡æ¡£ï¼Œæˆ‘ä»¬å°†å¾®è°ƒæ¨¡å‹ä»¥æŸ¥æ‰¾â€˜*Case*â€™çš„å€¼ä¸ºâ€˜*MELF CASE*â€™ï¼Œå¹¶ä¸”æå–ä¸€ä¸ªâ€˜*StorageTemperature*â€™ï¼Œå…¶å€¼ä¸ºâ€˜*-65
    to +200*â€™ã€‚
- en: '**Indexes**'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '**ç´¢å¼•**'
- en: 'The following indexes exist in the groundtruth metadata:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨groundtruthå…ƒæ•°æ®ä¸­å­˜åœ¨ä»¥ä¸‹ç´¢å¼•ï¼š
- en: '**data-sheets**: Model, Type, Case, Power Dissipation, Storage Temperature,
    Voltage, Weight, Thermal Resistance'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**æ•°æ®è¡¨**ï¼šå‹å·ã€ç±»å‹ã€å¤–å£³ã€åŠŸè€—ã€å‚¨å­˜æ¸©åº¦ã€ç”µå‹ã€é‡é‡ã€çƒ­é˜»'
- en: '**patents**: Title, Applicant, Inventor, Representative, Filing Date, Publication
    Date, Application Number, Publication Number, Priority, Classification, Abstract
    1st line'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ä¸“åˆ©**ï¼šæ ‡é¢˜ã€ç”³è¯·äººã€å‘æ˜äººã€ä»£è¡¨ã€ç”³è¯·æ—¥æœŸã€å‡ºç‰ˆæ—¥æœŸã€ç”³è¯·ç¼–å·ã€å‡ºç‰ˆç¼–å·ã€ä¼˜å…ˆæƒã€åˆ†ç±»ã€æ‘˜è¦ç¬¬ä¸€è¡Œ'
- en: 'Looking at the quality of the ground truth and feasibility I choose to retain
    the following indexes:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: è§‚å¯Ÿåˆ°åœ°é¢çœŸå®å€¼çš„è´¨é‡å’Œå¯è¡Œæ€§ï¼Œæˆ‘é€‰æ‹©ä¿ç•™ä»¥ä¸‹ç´¢å¼•ï¼š
- en: '[PRE4]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '**Quality**'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '**è´¨é‡**'
- en: For the image conversion to text, [ocropus](http://code.google.com/p/ocropus/)
    version 0.2 was used. Which means it dates to about the end of 2014\. This is
    ancient in terms of data science, so does the groundtruth quality live up to our
    task?
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºå›¾åƒè½¬æ¢ä¸ºæ–‡æœ¬ï¼Œä½¿ç”¨äº† [ocropus](http://code.google.com/p/ocropus/) ç‰ˆæœ¬ 0.2ã€‚è¿™æ„å‘³ç€å®ƒå¤§çº¦åœ¨ 2014
    å¹´åº•å‘å¸ƒã€‚åœ¨æ•°æ®ç§‘å­¦é¢†åŸŸè¿™å·²ç»å¾ˆå¤è€äº†ï¼Œé‚£ä¹ˆåœ°é¢çœŸå®åº¦çš„è´¨é‡æ˜¯å¦ç¬¦åˆæˆ‘ä»¬çš„ä»»åŠ¡è¦æ±‚å‘¢ï¼Ÿ
- en: 'For this I had a look at random images and compared the groundtruth with was
    actually written on the document. Here are two examples where the OCR was incorrect:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºæ­¤ï¼Œæˆ‘æŸ¥çœ‹äº†ä¸€äº›éšæœºå›¾åƒï¼Œå¹¶å°†åœ°é¢çœŸå®å€¼ä¸å®é™…åœ¨æ–‡æ¡£ä¸Šå†™çš„å†…å®¹è¿›è¡Œäº†æ¯”è¾ƒã€‚ä»¥ä¸‹æ˜¯ä¸¤ä¸ª OCR ä¸æ­£ç¡®çš„ç¤ºä¾‹ï¼š
- en: '![](../Images/798c47d8cb3079a86d6cc8af74e49659.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/798c47d8cb3079a86d6cc8af74e49659.png)'
- en: document-001â€“109381.in.000.png from Ghega dataset
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥è‡ª Ghega æ•°æ®é›†çš„ document-001â€“109381.in.000.png
- en: The key *Classification* is set as *BGSD 81/00* as ground truth. And it should
    be *B65D 81/100.*
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: é”® *Classification* è¢«è®¾ç½®ä¸º *BGSD 81/00* ä½œä¸ºåœ°é¢çœŸå®å€¼ã€‚å®ƒåº”è¯¥æ˜¯ *B65D 81/100*ã€‚
- en: '![](../Images/da972563105ade20c85a3167784b56a5.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/da972563105ade20c85a3167784b56a5.png)'
- en: document-003â€“112107.in.000.png from Ghega dataset
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥è‡ª Ghega æ•°æ®é›†çš„ document-003â€“112107.in.000.png
- en: The key *StorageTemperature* says *I -65 {O + 150* as ground truth, while we
    can see it should be *-65 to + 150.*
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: é”® *StorageTemperature* æ˜¾ç¤º *I -65 {O + 150* ä½œä¸ºåœ°é¢çœŸå®å€¼ï¼Œè€Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°å®ƒåº”è¯¥æ˜¯ *-65 to + 150*ã€‚
- en: There are many such errors in the dataset. One approach is to correct these.
    Another to ignore. Since I will use the same data just for comparing both models,
    I choose the latter. Shall the data be used for production, you may want to choose
    the former option to get the best results.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: æ•°æ®é›†ä¸­æœ‰è®¸å¤šæ­¤ç±»é”™è¯¯ã€‚ä¸€ç§æ–¹æ³•æ˜¯çº æ­£è¿™äº›é”™è¯¯ã€‚å¦ä¸€ç§æ˜¯å¿½ç•¥è¿™äº›é”™è¯¯ã€‚ç”±äºæˆ‘å°†ä½¿ç”¨ç›¸åŒçš„æ•°æ®æ¥æ¯”è¾ƒä¸¤ä¸ªæ¨¡å‹ï¼Œæˆ‘é€‰æ‹©äº†åè€…ã€‚å¦‚æœæ•°æ®ç”¨äºç”Ÿäº§ï¼Œä½ å¯èƒ½éœ€è¦é€‰æ‹©å‰ä¸€ç§æ–¹æ³•ä»¥è·å¾—æœ€ä½³ç»“æœã€‚
- en: (also note that these special characters could mess up the JSON format, I will
    come back to that topic later)
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: ï¼ˆè¿˜è¦æ³¨æ„ï¼Œè¿™äº›ç‰¹æ®Šå­—ç¬¦å¯èƒ½ä¼šæä¹± JSON æ ¼å¼ï¼Œç¨åæˆ‘ä¼šå›åˆ°è¿™ä¸ªè¯é¢˜ï¼‰
- en: '**Donut dataset structure**'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '**Donut æ•°æ®é›†ç»“æ„**'
- en: What does the format of the data we need it to be in look like?
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬éœ€è¦çš„æ•°æ®æ ¼å¼æ˜¯ä»€ä¹ˆæ ·çš„ï¼Ÿ
- en: For finetuning the Donut model we need to have the data organized in one folder
    with all the documents as separate image files and one metadata file, structured
    as a JSON lines file.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºå¾®è°ƒ Donut æ¨¡å‹ï¼Œæˆ‘ä»¬éœ€è¦å°†æ•°æ®ç»„ç»‡åœ¨ä¸€ä¸ªæ–‡ä»¶å¤¹ä¸­ï¼Œæ‰€æœ‰æ–‡æ¡£ä½œä¸ºå•ç‹¬çš„å›¾åƒæ–‡ä»¶å’Œä¸€ä¸ªå…ƒæ•°æ®æ–‡ä»¶ï¼Œç»“æ„ä¸º JSON lines æ–‡ä»¶ã€‚
- en: '[PRE5]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The JSONL file contains per image file a line like this:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: JSONL æ–‡ä»¶åŒ…å«æ¯ä¸ªå›¾åƒæ–‡ä»¶ä¸€è¡Œï¼Œæ ¼å¼å¦‚ä¸‹ï¼š
- en: '[PRE6]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Letâ€™s break down this JSON line. On the upper level we have a dict with two
    elements: *file_name* and *ground_truth*. Under the *ground_truth* key, we have
    a dict with key *gt_parse*. The value is in itself a dict with the key value pairs
    that we know on the document. Or even better: *assign*. Remember that the doctype
    is not necessarily present as text in the document. The term *datasheet* is not
    present as text on those documents.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬åˆ†è§£è¿™è¡Œ JSONã€‚åœ¨ä¸Šå±‚æˆ‘ä»¬æœ‰ä¸€ä¸ªåŒ…å«ä¸¤ä¸ªå…ƒç´ çš„å­—å…¸ï¼š*file_name* å’Œ *ground_truth*ã€‚åœ¨ *ground_truth*
    é”®ä¸‹ï¼Œæˆ‘ä»¬æœ‰ä¸€ä¸ªåŒ…å« *gt_parse* é”®çš„å­—å…¸ã€‚å…¶å€¼æœ¬èº«æ˜¯ä¸€ä¸ªå­—å…¸ï¼ŒåŒ…å«æˆ‘ä»¬åœ¨æ–‡æ¡£ä¸­çŸ¥é“çš„é”®å€¼å¯¹ã€‚æˆ–è€…æ›´å¥½ï¼š*assign*ã€‚è®°ä½ï¼Œæ–‡æ¡£ä¸­ä¸ä¸€å®šä¼šå‡ºç°æ–‡æ¡£ç±»å‹ã€‚æœ¯è¯­
    *datasheet* å¹¶æ²¡æœ‰ä½œä¸ºæ–‡æœ¬å‡ºç°åœ¨è¿™äº›æ–‡æ¡£ä¸­ã€‚
- en: Luckily pix2struct uses the same format for finetuning, so we can kill two birds
    with one stone. Once we have converted it in this structure, we can use it for
    finetuning Pix2Struct as well.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: å¹¸è¿çš„æ˜¯ï¼Œpix2struct ä½¿ç”¨ç›¸åŒçš„æ ¼å¼è¿›è¡Œå¾®è°ƒï¼Œå› æ­¤æˆ‘ä»¬å¯ä»¥ä¸€ä¸¾ä¸¤å¾—ã€‚ä¸€æ—¦æˆ‘ä»¬å°†å…¶è½¬æ¢ä¸ºè¿™ç§ç»“æ„ï¼Œæˆ‘ä»¬è¿˜å¯ä»¥ç”¨æ¥å¾®è°ƒ Pix2Structã€‚
- en: '**Conversion**'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '**è½¬æ¢**'
- en: For the conversion itself, I created a Jupyter notebook on colab. I decided
    to create a split into a train and validation set at this stage, as opposed to
    just before finetuning. This way, the same validation images will be used for
    both models and the results will be better comparable. One out of 5 documents
    will be used for validation.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºè½¬æ¢æœ¬èº«ï¼Œæˆ‘åœ¨ colab ä¸Šåˆ›å»ºäº†ä¸€ä¸ª Jupyter notebookã€‚æˆ‘å†³å®šåœ¨è¿™ä¸ªé˜¶æ®µå°†æ•°æ®æ‹†åˆ†ä¸ºè®­ç»ƒé›†å’ŒéªŒè¯é›†ï¼Œè€Œä¸æ˜¯åœ¨å¾®è°ƒä¹‹å‰ã€‚è¿™ç§æ–¹å¼ï¼Œä¸¤ä¸ªæ¨¡å‹å°†ä½¿ç”¨ç›¸åŒçš„éªŒè¯å›¾åƒï¼Œç»“æœä¼šæ›´å…·å¯æ¯”æ€§ã€‚äº”ä¸ªæ–‡æ¡£ä¸­ä¼šæœ‰ä¸€ä¸ªç”¨äºéªŒè¯ã€‚
- en: 'With the above knowledge of the structure of the Ghega dataset, we can construe
    the conversion procedure as follows:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: åˆ©ç”¨ä¸Šè¿° Ghega æ•°æ®é›†çš„ç»“æ„çŸ¥è¯†ï¼Œæˆ‘ä»¬å¯ä»¥å°†è½¬æ¢è¿‡ç¨‹æ¦‚æ‹¬å¦‚ä¸‹ï¼š
- en: For every filename ending in in.000.png we take the corresponding groundtruth
    file and create a temporary dataframe object.
  id: totrans-61
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: å¯¹äºæ¯ä¸ªä»¥ in.000.png ç»“å°¾çš„æ–‡ä»¶åï¼Œæˆ‘ä»¬å–å¯¹åº”çš„ groundtruth æ–‡ä»¶å¹¶åˆ›å»ºä¸€ä¸ªä¸´æ—¶çš„æ•°æ®æ¡†å¯¹è±¡ã€‚
- en: Beware that the groundtruth could be empty or doesnâ€™t exist entirely. (e.g.
    for *datasheets/taiwan-switching*)
  id: totrans-62
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: æ³¨æ„ï¼Œgroundtruth å¯èƒ½ä¸ºç©ºæˆ–å®Œå…¨ä¸å­˜åœ¨ã€‚ï¼ˆä¾‹å¦‚ï¼Œå¯¹äº *datasheets/taiwan-switching*ï¼‰
- en: 'Next, we deduct the class from the subfolder: *patent* or *datasheet .* Now
    we have to build the JSON line. For each element/index we want to extract, we
    check if it is in that dataframe and collect it. Then copy the image itself.'
  id: totrans-63
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬ä»å­æ–‡ä»¶å¤¹ä¸­æ‰£é™¤ç±»ï¼š*patent* æˆ– *datasheet* ã€‚ç°åœ¨æˆ‘ä»¬éœ€è¦æ„å»º JSON è¡Œã€‚å¯¹äºæ¯ä¸ªæˆ‘ä»¬æƒ³æå–çš„å…ƒç´ /ç´¢å¼•ï¼Œæˆ‘ä»¬æ£€æŸ¥å®ƒæ˜¯å¦åœ¨æ•°æ®æ¡†ä¸­å¹¶è¿›è¡Œæ”¶é›†ã€‚ç„¶åå¤åˆ¶å›¾åƒæœ¬èº«ã€‚
- en: Do this for all images and at the end we have a JSONL file to write out.
  id: totrans-64
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: å¯¹æ‰€æœ‰å›¾åƒæ‰§è¡Œæ­¤æ“ä½œï¼Œæœ€åæˆ‘ä»¬å°±æœ‰ä¸€ä¸ª JSONL æ–‡ä»¶å¯ä»¥å†™å‡ºã€‚
- en: 'In python it looks like this:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ Python ä¸­ï¼Œå®ƒçœ‹èµ·æ¥æ˜¯è¿™æ ·çš„ï¼š
- en: '[PRE7]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: The ghega_df is a dataframe to do some sanity checks or statistical analysis
    on if wanted. I used it to check random samples if my converted data was actually
    correct.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: ghega_df æ˜¯ä¸€ä¸ªæ•°æ®æ¡†ï¼Œç”¨äºè¿›è¡Œä¸€äº›åˆç†æ€§æ£€æŸ¥æˆ–ç»Ÿè®¡åˆ†æï¼ˆå¦‚æœ‰éœ€è¦ï¼‰ã€‚æˆ‘ç”¨å®ƒæ¥æ£€æŸ¥éšæœºæ ·æœ¬ï¼ŒéªŒè¯æˆ‘çš„è½¬æ¢æ•°æ®æ˜¯å¦æ­£ç¡®ã€‚
- en: '**Hiccups**'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '**é—®é¢˜**'
- en: Once converted, it looks like everything is all copacetic. But I want to get
    rid of the idea that everything usually runs from the first try. There are always
    small unexpected hiccups happening. Talking about the errors I encountered and
    showing the remedies is useful for anybody mimicking this whole process with their
    own dataset.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: è½¬æ¢å®Œæˆåï¼Œä¸€åˆ‡çœ‹èµ·æ¥éƒ½å¾ˆé¡ºåˆ©ã€‚ä½†æˆ‘æƒ³æ‘†è„±é‚£ç§é€šå¸¸ç¬¬ä¸€æ¬¡å°è¯•å°±èƒ½æˆåŠŸçš„æƒ³æ³•ã€‚æ€»æ˜¯ä¼šæœ‰ä¸€äº›å°çš„æ„å¤–é—®é¢˜å‘ç”Ÿã€‚è°ˆè®ºæˆ‘é‡åˆ°çš„é”™è¯¯å¹¶å±•ç¤ºè§£å†³æ–¹æ¡ˆå¯¹ä»»ä½•æ¨¡æ‹Ÿæ•´ä¸ªè¿‡ç¨‹å¹¶ä½¿ç”¨è‡ªå·±æ•°æ®é›†çš„äººéƒ½æ˜¯æœ‰ç”¨çš„ã€‚
- en: 'For example, after converting the dataset, I wanted to train the Donut model.
    Before I can do that I need to create a train dataset, like so:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: ä¾‹å¦‚ï¼Œåœ¨è½¬æ¢æ•°æ®é›†åï¼Œæˆ‘æƒ³è®­ç»ƒ Donut æ¨¡å‹ã€‚åœ¨æ­¤ä¹‹å‰ï¼Œæˆ‘éœ€è¦åˆ›å»ºä¸€ä¸ªè®­ç»ƒæ•°æ®é›†ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š
- en: '[PRE8]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'And got this error:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: å¹¶ä¸”å‡ºç°äº†è¿™ä¸ªé”™è¯¯ï¼š
- en: '[PRE9]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'So it seems there is a problem with the JSON format in row 7\. I copied that
    line and pasted it in an [online JSON validator](https://jsonformatter.curiousconcept.com/#):'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: çœ‹èµ·æ¥ç¬¬ 7 è¡Œçš„ JSON æ ¼å¼æœ‰é—®é¢˜ã€‚æˆ‘å¤åˆ¶äº†é‚£ä¸€è¡Œå¹¶å°†å…¶ç²˜è´´åˆ°ä¸€ä¸ª [åœ¨çº¿ JSON éªŒè¯å™¨](https://jsonformatter.curiousconcept.com/#)
    ä¸­ï¼š
- en: '![](../Images/fcc1e25cb0180b26712b1496194d27d4.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fcc1e25cb0180b26712b1496194d27d4.png)'
- en: Image by author
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: ä½œè€…æä¾›çš„å›¾åƒ
- en: '![](../Images/732f1846570edeab4c0b07c4006f595b.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/732f1846570edeab4c0b07c4006f595b.png)'
- en: Image by author
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: ä½œè€…æä¾›çš„å›¾åƒ
- en: '![](../Images/732f1846570edeab4c0b07c4006f595b.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/732f1846570edeab4c0b07c4006f595b.png)'
- en: Image by author
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: ä½œè€…æä¾›çš„å›¾åƒ
- en: 'It however says itâ€™s a valid JSON line. So letâ€™s have a deeper look:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶è€Œï¼Œå®ƒè¡¨ç¤ºè¿™æ˜¯ä¸€ä¸ªæœ‰æ•ˆçš„ JSON è¡Œã€‚è®©æˆ‘ä»¬æ›´æ·±å…¥åœ°çœ‹çœ‹ï¼š
- en: '[PRE10]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Did you spot the error? After some time I noticed thereâ€™s a missing comma between
    the *DocType* and *FilingDate*. It was however missing on all lines, so itâ€™s unclear
    to me why it says line 7 has a problem. When I fixed this issue, I tried again
    and now it claims thereâ€™s a problem on line 17:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ å‘ç°é”™è¯¯äº†å—ï¼Ÿç»è¿‡ä¸€æ®µæ—¶é—´ï¼Œæˆ‘æ³¨æ„åˆ° *DocType* å’Œ *FilingDate* ä¹‹é—´ç¼ºå°‘é€—å·ã€‚ç„¶è€Œï¼Œè¿™åœ¨æ‰€æœ‰è¡Œä¸­éƒ½æ˜¯ç¼ºå¤±çš„ï¼Œæ‰€ä»¥æˆ‘ä¸æ¸…æ¥šä¸ºä»€ä¹ˆç¬¬
    7 è¡Œä¼šå‡ºç°é—®é¢˜ã€‚å½“æˆ‘ä¿®å¤äº†è¿™ä¸ªé—®é¢˜åï¼Œæˆ‘å†æ¬¡å°è¯•ï¼Œç°åœ¨å®ƒå£°ç§°ç¬¬ 17 è¡Œæœ‰é—®é¢˜ï¼š
- en: '[PRE11]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Here is line 17, do you spot the problem?
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯ç¬¬ 17 è¡Œï¼Œä½ å‘ç°äº†é—®é¢˜å—ï¼Ÿ
- en: '[PRE12]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Itâ€™s the unescaped quotation marks for the *Classification* element. To remedy
    this, I made a decision that all values will only be allowed to contain alphanumeric
    and a few special characters with this regex:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯*Classification* å…ƒç´ çš„æœªè½¬ä¹‰å¼•å·ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘å†³å®šæ‰€æœ‰å€¼åªèƒ½åŒ…å«å­—æ¯æ•°å­—å­—ç¬¦å’Œä¸€äº›ç‰¹æ®Šå­—ç¬¦ï¼Œå¹¶ä½¿ç”¨äº†è¿™ä¸ªæ­£åˆ™è¡¨è¾¾å¼ï¼š
- en: '[PRE13]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: This may affect the true performance badly, but from what I could see, any other
    characters were caused by OCR errors anyway. I suppose for the relative comparison
    between the models leaving them out doesnâ€™t matter much.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å¯èƒ½ä¼šä¸¥é‡å½±å“çœŸå®æ€§èƒ½ï¼Œä½†ä»æˆ‘æ‰€è§ï¼Œå…¶ä»–å­—ç¬¦éƒ½æ˜¯ç”±äº OCR é”™è¯¯å¼•èµ·çš„ã€‚æˆ‘è®¤ä¸ºï¼Œå¯¹äºæ¨¡å‹ä¹‹é—´çš„ç›¸å¯¹æ¯”è¾ƒï¼Œå¿½ç•¥è¿™äº›å­—ç¬¦å½±å“ä¸å¤§ã€‚
- en: '**Data preparation: done**'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '**æ•°æ®å‡†å¤‡ï¼šå®Œæˆ**'
- en: Itâ€™s often overlooked and certainly underestimated, the importance of preparing
    data before training. With the steps above I have shown you how you can adapt
    your own data to be used by both Donut and Pix2Struct for key index extraction
    on documents. Common pitfalls were also remedied. The Jupyter notebook with all
    steps can be found [here](https://github.com/Toon-nooT/notebooks/blob/main/Donut_vs_pix2struct_1_Ghega_data_prep.ipynb).
    Weâ€™re halfway there. The next step is to train both models on this dataset. Iâ€™m
    very curious how well they fare, but the comparison and training will be for a
    next article.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: æ•°æ®å‡†å¤‡çš„é‡è¦æ€§å¸¸è¢«å¿½è§†ä¸”è¢«ä½ä¼°ã€‚é€šè¿‡ä¸Šè¿°æ­¥éª¤ï¼Œæˆ‘å±•ç¤ºäº†å¦‚ä½•è°ƒæ•´è‡ªå·±çš„æ•°æ®ï¼Œä»¥ä¾¿Donutå’ŒPix2Structç”¨äºæ–‡æ¡£çš„å…³é”®ç´¢å¼•æå–ã€‚å¸¸è§çš„é™·é˜±ä¹Ÿå¾—åˆ°äº†ä¿®æ­£ã€‚åŒ…å«æ‰€æœ‰æ­¥éª¤çš„Jupyterç¬”è®°æœ¬å¯ä»¥åœ¨[è¿™é‡Œ](https://github.com/Toon-nooT/notebooks/blob/main/Donut_vs_pix2struct_1_Ghega_data_prep.ipynb)æ‰¾åˆ°ã€‚æˆ‘ä»¬å·²ç»å®Œæˆäº†ä¸€åŠã€‚ä¸‹ä¸€æ­¥æ˜¯ç”¨è¿™ä¸ªæ•°æ®é›†è®­ç»ƒè¿™ä¸¤ä¸ªæ¨¡å‹ã€‚æˆ‘éå¸¸å¥½å¥‡å®ƒä»¬çš„è¡¨ç°å¦‚ä½•ï¼Œä½†æ¯”è¾ƒå’Œè®­ç»ƒå°†ç•™åˆ°ä¸‹ä¸€ç¯‡æ–‡ç« ä¸­ã€‚
- en: 'You may also like:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ å¯èƒ½è¿˜å–œæ¬¢ï¼š
- en: '[](https://toon-beerten.medium.com/hands-on-document-data-extraction-with-transformer-7130df3b6132?source=post_page-----b5a826bc2ac3--------------------------------)
    [## Hands-on: document data extraction with ğŸ© transformer'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://toon-beerten.medium.com/hands-on-document-data-extraction-with-transformer-7130df3b6132?source=post_page-----b5a826bc2ac3--------------------------------)
    [## å®æˆ˜ï¼šä½¿ç”¨ğŸ©å˜æ¢å™¨è¿›è¡Œæ–‡æ¡£æ•°æ®æå–'
- en: My experience using donut transformers model to extract invoice indexes.
  id: totrans-94
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: æˆ‘ä½¿ç”¨Donutå˜æ¢å™¨æ¨¡å‹æå–å‘ç¥¨ç´¢å¼•çš„ç»éªŒã€‚
- en: toon-beerten.medium.com](https://toon-beerten.medium.com/hands-on-document-data-extraction-with-transformer-7130df3b6132?source=post_page-----b5a826bc2ac3--------------------------------)
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: toon-beerten.medium.com](https://toon-beerten.medium.com/hands-on-document-data-extraction-with-transformer-7130df3b6132?source=post_page-----b5a826bc2ac3--------------------------------)
- en: 'References:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: å‚è€ƒæ–‡çŒ®ï¼š
- en: '[](https://arxiv.org/abs/2111.15664?source=post_page-----b5a826bc2ac3--------------------------------)
    [## OCR-free Document Understanding Transformer'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://arxiv.org/abs/2111.15664?source=post_page-----b5a826bc2ac3--------------------------------)
    [## æ— OCRæ–‡æ¡£ç†è§£å˜æ¢å™¨'
- en: Understanding document images (e.g., invoices) is a core but challenging task
    since it requires complex functions suchâ€¦
  id: totrans-98
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ç†è§£æ–‡æ¡£å›¾åƒï¼ˆä¾‹å¦‚ï¼Œå‘ç¥¨ï¼‰æ˜¯ä¸€é¡¹æ ¸å¿ƒä½†å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ï¼Œå› ä¸ºå®ƒéœ€è¦å¤æ‚çš„åŠŸèƒ½â€¦
- en: 'arxiv.org](https://arxiv.org/abs/2111.15664?source=post_page-----b5a826bc2ac3--------------------------------)
    [](https://arxiv.org/abs/2210.03347?source=post_page-----b5a826bc2ac3--------------------------------)
    [## Pix2Struct: Screenshot Parsing as Pretraining for Visual Language Understanding'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: arxiv.org](https://arxiv.org/abs/2111.15664?source=post_page-----b5a826bc2ac3--------------------------------)
    [](https://arxiv.org/abs/2210.03347?source=post_page-----b5a826bc2ac3--------------------------------)
    [## Pix2Structï¼šä½œä¸ºè§†è§‰è¯­è¨€ç†è§£é¢„è®­ç»ƒçš„æˆªå›¾è§£æ
- en: Visually-situated language is ubiquitous -- sources range from textbooks with
    diagrams to web pages with images andâ€¦
  id: totrans-100
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: è§†è§‰ä½ç½®è¯­è¨€æ— å¤„ä¸åœ¨â€”â€”æ¥æºåŒ…æ‹¬å¸¦æœ‰å›¾è¡¨çš„æ•™ç§‘ä¹¦åˆ°åŒ…å«å›¾åƒçš„ç½‘é¡µâ€¦
- en: arxiv.org](https://arxiv.org/abs/2210.03347?source=post_page-----b5a826bc2ac3--------------------------------)
    [](https://machinelearning.inginf.units.it/data-and-tools/ghega-dataset?source=post_page-----b5a826bc2ac3--------------------------------)
    [## Machine Learning Lab - Ghega dataset
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: arxiv.org](https://arxiv.org/abs/2210.03347?source=post_page-----b5a826bc2ac3--------------------------------)
    [](https://machinelearning.inginf.units.it/data-and-tools/ghega-dataset?source=post_page-----b5a826bc2ac3--------------------------------)
    [## æœºå™¨å­¦ä¹ å®éªŒå®¤ - Ghegaæ•°æ®é›†
- en: 'Ghega-dataset: a dataset for document understanding and classification We provide
    here a labeled dataset which can beâ€¦'
  id: totrans-102
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Ghegaæ•°æ®é›†ï¼šç”¨äºæ–‡æ¡£ç†è§£å’Œåˆ†ç±»çš„æ•°æ®é›†ï¼Œæˆ‘ä»¬æä¾›äº†ä¸€ä¸ªæ ‡æ³¨æ•°æ®é›†ï¼Œå¯ä»¥â€¦
- en: machinelearning.inginf.units.it](https://machinelearning.inginf.units.it/data-and-tools/ghega-dataset?source=post_page-----b5a826bc2ac3--------------------------------)
    [](https://huggingface.co/to-be/donut-base-finetuned-invoices?source=post_page-----b5a826bc2ac3--------------------------------)
    [## to-be/donut-base-finetuned-invoices Â· Hugging Face
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: machinelearning.inginf.units.it](https://machinelearning.inginf.units.it/data-and-tools/ghega-dataset?source=post_page-----b5a826bc2ac3--------------------------------)
    [](https://huggingface.co/to-be/donut-base-finetuned-invoices?source=post_page-----b5a826bc2ac3--------------------------------)
    [## to-be/donut-base-finetuned-invoices Â· Hugging Face
- en: Edit model card Based on Donut base model (introduced in the paper OCR-free
    Document Understanding Transformer byâ€¦
  id: totrans-104
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ç¼–è¾‘æ¨¡å‹å¡ åŸºäºDonutåŸºç¡€æ¨¡å‹ï¼ˆåœ¨è®ºæ–‡ã€Šæ— OCRæ–‡æ¡£ç†è§£å˜æ¢å™¨ã€‹ä¸­ä»‹ç»ï¼‰â€¦
- en: huggingface.co](https://huggingface.co/to-be/donut-base-finetuned-invoices?source=post_page-----b5a826bc2ac3--------------------------------)
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: huggingface.co](https://huggingface.co/to-be/donut-base-finetuned-invoices?source=post_page-----b5a826bc2ac3--------------------------------)
