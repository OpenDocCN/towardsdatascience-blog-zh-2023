- en: 'Empowering Fairness: Recognizing and Addressing Bias in Generative Models'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 赋能公平：识别与解决生成模型中的偏见
- en: 原文：[https://towardsdatascience.com/empowering-fairness-recognizing-and-addressing-bias-in-generative-models-1723ce3973aa](https://towardsdatascience.com/empowering-fairness-recognizing-and-addressing-bias-in-generative-models-1723ce3973aa)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/empowering-fairness-recognizing-and-addressing-bias-in-generative-models-1723ce3973aa](https://towardsdatascience.com/empowering-fairness-recognizing-and-addressing-bias-in-generative-models-1723ce3973aa)
- en: With the integration of AI into our everyday lives, a biased model can have
    drastic consequences on users
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 随着人工智能融入我们的日常生活，一个有偏见的模型可能对用户产生严重后果
- en: '[](https://medium.com/@kevin.berlemont?source=post_page-----1723ce3973aa--------------------------------)[![Kevin
    Berlemont, PhD](../Images/18697f38b76f1fb04870f565cfb04b4c.png)](https://medium.com/@kevin.berlemont?source=post_page-----1723ce3973aa--------------------------------)[](https://towardsdatascience.com/?source=post_page-----1723ce3973aa--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----1723ce3973aa--------------------------------)
    [Kevin Berlemont, PhD](https://medium.com/@kevin.berlemont?source=post_page-----1723ce3973aa--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@kevin.berlemont?source=post_page-----1723ce3973aa--------------------------------)[![Kevin
    Berlemont, PhD](../Images/18697f38b76f1fb04870f565cfb04b4c.png)](https://medium.com/@kevin.berlemont?source=post_page-----1723ce3973aa--------------------------------)[](https://towardsdatascience.com/?source=post_page-----1723ce3973aa--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----1723ce3973aa--------------------------------)
    [Kevin Berlemont, PhD](https://medium.com/@kevin.berlemont?source=post_page-----1723ce3973aa--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----1723ce3973aa--------------------------------)
    ·6 min read·Jul 6, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page-----1723ce3973aa--------------------------------)
    ·阅读时间6分钟·2023年7月6日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/49c317d74bb76b8a63d74a410f64cb29.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/49c317d74bb76b8a63d74a410f64cb29.png)'
- en: Photo by [Dainis Graveris](https://unsplash.com/@dainisgraveris?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由[Dainis Graveris](https://unsplash.com/@dainisgraveris?utm_source=medium&utm_medium=referral)拍摄，来源于[Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: In 2021, Princeton University’s Center for Information Technology Policy released
    a report where they found that machine learning algorithms can pick up biases
    similar to those of humans from their training data. One striking example of this
    effect is a study about the AI hiring tool from Amazon **[1]**. The tool was trained
    on resumes submitted to Amazon during the previous year and was ranking the different
    candidates. Due to the huge gender imbalance in tech positions over the past decade,
    the algorithm had learned language that would associate to women, such as women’s
    sport teams and would downgrade the rank of such resumes. This example highlights
    the necessity of not only fair and accurate models but datasets too, to remove
    bias during training. In the current context of the fast development of generative
    models such as ChatGPT and the integration of AI into our everyday lives, a biased
    model can have drastic consequences and erode the trust of users and global acceptance.
    Addressing these biases is thus necessary from a business perspective and Data
    Scientists (in a broad definition) have to be aware of them to mitigate them and
    make sure they are aligned with their principles.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 2021年，普林斯顿大学信息技术政策中心发布了一份报告，发现机器学习算法可能会从训练数据中学习到类似于人类的偏见。其中一个引人注目的例子是关于亚马逊**[1]**的AI招聘工具的研究。该工具在之前一年提交给亚马逊的简历上进行了训练，并对不同的候选人进行了排名。由于过去十年科技职位中性别比例严重失衡，该算法学会了将语言与女性相关联，比如女性运动队，并降低了这些简历的排名。这个例子突显了不仅需要公平和准确的模型，还需要公平的数据集，以消除训练中的偏见。在ChatGPT等生成模型快速发展的背景下，以及人工智能日益融入我们的日常生活中，一个有偏见的模型可能会带来严重后果，侵蚀用户信任和全球接受度。因此，从商业角度来看，解决这些偏见是必要的，数据科学家（广义上的定义）必须意识到这些偏见，以减轻其影响，并确保其与原则一致。
- en: Examples of Biases in Generative Models
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 生成模型中的偏见示例
- en: The first type of task where generative models are widely used that comes to
    mind is a translation task. Users input a text in language A and expect a translation
    in language B. Different languages don’t necessarily use the same type of gendered
    pronouns, for example *“The senator”* in English could be either feminine or masculine,
    as in French it would be *“La senatrice”* or *“Le senateur”*. Even in the case
    of the gender being specified in the sentence (example below), it is not uncommon
    for generative model to reinforce gender stereotype roles during the translation.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个想到的生成模型广泛使用的任务类型是翻译任务。用户输入一种语言A的文本，期望得到语言B的翻译。例如，不同语言不一定使用相同类型的性别代词，例如英文中的*“The
    senator”*可以是女性或男性，而法语中则是*“La senatrice”*或*“Le senateur”*。即使在句子中指定了性别（见下例），生成模型在翻译过程中强化性别刻板印象角色也并不罕见。
- en: '![](../Images/1f139699f5d40454bf4c3d0c442551be.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1f139699f5d40454bf4c3d0c442551be.png)'
- en: Example of a translation task with ChatGPT
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 使用ChatGPT的翻译任务示例
- en: Similar to translation tasks, caption generation tasks require the model to
    generate a new text based on some input, i.e a translation from an image to a
    text. A recent study **[2]** analyzed the performances of a generative transformer
    model on a caption generation task (figure below) on the Common Objects in Context
    dataset.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于翻译任务，字幕生成任务要求模型基于某些输入生成新的文本，即将图像翻译成文本。一项最近的研究**[2]**分析了生成型变换器模型在Common Objects
    in Context数据集上进行字幕生成任务（见下图）的表现。
- en: '![](../Images/7e7a67c133b7be3514c7710863c91b9e.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7e7a67c133b7be3514c7710863c91b9e.png)'
- en: Figure from “Understanding and Evaluating Racial Biases” [https://arxiv.org/pdf/2106.08503.pdf](https://arxiv.org/pdf/2106.08503.pdf)
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 图源：“理解与评估种族偏见” [https://arxiv.org/pdf/2106.08503.pdf](https://arxiv.org/pdf/2106.08503.pdf)
- en: The generative model assigned various racial and cultural descriptions to the
    captions, despite not being applicable in all the images. These descriptors were
    only learned by the newer generative models and show an increase in bias for these
    models. It is worth noting that transformer models exhibit gender bias too for
    this dataset, exacerbating the men/women imbalance ratio by, for example, identifying
    a person as a woman depending on the background of a house/room.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这些描述符在所有图像中并不适用，但生成模型为字幕分配了各种种族和文化描述。这些描述符仅由更新的生成模型学习，并显示出这些模型的偏见增加。值得注意的是，变换器模型对于这个数据集也表现出性别偏见，例如，通过房屋/房间的背景将某人识别为女性，从而加剧了男女不平衡的比例。
- en: '**Why does Bias occur?**'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '**偏差为何会发生？**'
- en: The conception phase of a generative leaves plenty of space for bias to develop
    within a model. They can arise from the data itself, labels and annotations, internal
    representations or even the model (see [https://huggingface.co/blog/ethics-soc-4](https://huggingface.co/blog/ethics-soc-4)
    for an extensive list focused on Text-to-Image models).
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 生成模型的构思阶段为模型中的偏差发展留出了大量空间。偏差可能来自数据本身、标签和注释、内部表示甚至模型（请参见 [https://huggingface.co/blog/ethics-soc-4](https://huggingface.co/blog/ethics-soc-4)
    以获取针对Text-to-Image模型的详细列表）。
- en: The necessary data for the training of a generative model comes from a multitude
    of sources, usually online. To ensure the integrity of the training data, AI companies
    often use well-known news websites and similar to construct their database. The
    models trained on this dataset are going to perpetuate biased associations due
    to the restrictive demographics being considered (usually white, middle-aged,
    upper-middle-class).
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 生成模型训练所需的数据来自众多来源，通常是在线的。为了确保训练数据的完整性，人工智能公司通常使用知名新闻网站等类似资源来构建其数据库。在这个数据集上训练的模型将由于考虑的群体人口特征限制（通常是白人、中年、上层中产阶级）而延续偏见的联想。
- en: The Label Bias ([https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3994857/](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3994857/))
    is maybe the more explicit as it results in biases being introduced, usually inadvertently,
    in the labeled data. Generative models are trained to reproduce/approximate their
    training dataset so a bias in the labels will have a drastic impact on the output
    representations of the model. Thankfully, using multiple version of the label
    and cross-checking them allow to mitigate biases.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 标签偏差 ([https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3994857/](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3994857/))
    可能是更明显的，因为它会导致标签数据中引入偏差，这通常是无意的。生成模型被训练以重现/近似其训练数据集，因此标签中的偏差将对模型的输出表示产生严重影响。幸运的是，使用多个版本的标签并交叉检查它们可以缓解这些偏差。
- en: The last two types of biases, internal representations and model biases, both
    come from a specific step in the modeling. The first one is introduced at the
    pre-processing stage, manual or algorithmic. This stage is prone to incorporate
    biases and a loss of cultural nuances, especially if the original dataset lacks
    diversity. The model bias arises simply from an objective function based on discriminatory
    features and an amplification of the biases to improve accuracy of the model.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 最后两种偏见类型，即内部表示和模型偏见，都来自建模过程中的特定步骤。第一种在预处理阶段引入，无论是手动还是算法性。这一阶段容易引入偏见和文化细微差别的丧失，特别是当原始数据集缺乏多样性时。模型偏见则简单地来源于基于歧视特征的目标函数以及为提高模型准确性而放大的偏见。
- en: Detecting Biases in Generative Models
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 生成模型中的偏见检测
- en: As highlighted through this article, bias in generative models is observed in
    various forms and under various conditions. Methods to detect it have to be as
    diverse as the biases they are trying to detect.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 正如本文所强调的，生成模型中的偏见以各种形式和不同条件下的方式存在。检测偏见的方法必须像它们试图检测的偏见一样多样化。
- en: One of the main measure of biases in language models consists in the Word Embedding
    Association Test. This score measures the similarity, within an embedding space
    (internal representation), between two set of words. A high score indicates a
    strong association. More specifically, it computes the difference of similarity
    between a target set of words and two inputs sets, for example *[home, family]*
    as target and *[he, man]/[she, woman]* as inputs. A score of 0 would indicate
    a perfectly balanced model. This metrics was used to demonstrate that RoBERTa
    is one of the most biased generative model ([https://arxiv.org/pdf/2106.13219.pdf](https://arxiv.org/pdf/2106.13219.pdf)).
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 语言模型中偏见的主要衡量方法之一是词嵌入关联测试。该分数测量的是在嵌入空间（内部表示）中，两个词集之间的相似性。高分数表示强关联。更具体地说，它计算的是目标词集与两个输入词集之间相似性的差异，例如将*[home,
    family]*作为目标，将*[he, man]/[she, woman]*作为输入。如果分数为0，则表示模型完全平衡。该指标被用来证明RoBERTa是最偏见的生成模型之一
    ([https://arxiv.org/pdf/2106.13219.pdf](https://arxiv.org/pdf/2106.13219.pdf))。
- en: An innovative way of measuring biases in generative models (counterfactual evaluation),
    and more specifically gender bias, consists in swapping the gender of the words
    and to observe the change of accuracy in the predictions. If the modified accuracy
    and the original accuracy are different it highlights the presence of biases in
    the model as an unbiased generative model should be as accurate independently
    of the gender of the inputs. The main caveat of this measure is that it only captures
    gender bias and has thus to be completed by other measure to fully evaluate bias
    sources. On a similar idea, one can use the Bilingual Evaluation Understudy (a
    classical translation measure) to compare the similarity between the output resulting
    from the gender swapped input and the original one.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 测量生成模型（反事实评估）偏见的一个创新方法，尤其是性别偏见，是通过交换词汇的性别并观察预测准确性的变化。如果修改后的准确性与原始准确性不同，则表明模型中存在偏见，因为一个公正的生成模型应该在输入的性别独立的情况下具有相同的准确性。这一措施的主要缺陷是它只捕捉到性别偏见，因此需要其他测量方法来全面评估偏见来源。在类似的思路下，可以使用双语评估替代（经典翻译测量）来比较性别交换输入和原始输入产生的输出之间的相似性。
- en: Current generative models are based on transformer models that used a feature
    called *attention* to predict the output based on the output. Studies have investigated
    the relationship between gender and roles using the attention score directly from
    the model ([https://arxiv.org/abs/2110.15733](https://arxiv.org/abs/2110.15733)).
    This allows to compare different part of the model between each other to detect
    which module contributes more to bias. If it has been shown with this measure
    that generative models introduce a gender bias on the Wikipedia dataset, one caveat
    of this measure is that attention values don’t represent direct effect and similarity
    between concepts and require an in depth analysis to draw any conclusions.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 目前的生成模型基于使用一种叫做*注意力*的特征的变换器模型，以根据输出预测结果。研究已经直接从模型中调查了性别与角色之间的关系 ([https://arxiv.org/abs/2110.15733](https://arxiv.org/abs/2110.15733))。这允许比较模型中的不同部分，以检测哪个模块对偏见的贡献更大。如果通过这种措施表明生成模型在维基百科数据集上引入了性别偏见，这一措施的一个缺陷是注意力值不代表概念之间的直接效应和相似性，需要深入分析才能得出结论。
- en: '**How to overcome biases in generative models?**'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '**如何克服生成模型中的偏见？**'
- en: Various techniques have been developed by researchers to provide less biased
    generative systems. Most of the time these technics consist in additional steps
    in the modeling, such as setting a control variable that will fix the gender based
    on previous information or adding another model to provide contextual information.
    However, all these steps don’t necessarily addresses the use of inherently biased
    datasets. In addition, most of the generative models are based on English training
    data, drastically limiting the cultural and societal diversity of these models.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 研究人员开发了各种技术以提供更少偏见的生成系统。这些技术大多包括在建模中的额外步骤，如设置一个控制变量以基于先前的信息确定性别，或添加另一个模型以提供上下文信息。然而，所有这些步骤不一定解决固有偏见数据集的问题。此外，大多数生成模型基于英文训练数据，极大地限制了这些模型的文化和社会多样性。
- en: Fully overcoming biases in generative models would require the establishment
    of a formal framework and benchmarks to test and evaluate models across multiple
    languages. This would allow the detection of bias that is present in nuanced ways
    in diverse AI models.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 完全克服生成模型中的偏见需要建立一个正式的框架和基准，以测试和评估跨多种语言的模型。这将允许检测到在多样化的人工智能模型中以细微方式存在的偏见。
- en: References
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: '*Amazon scraps secret AI recruiting tool that showed bias against women,* Jeffrey
    Dastin*,* [https://www.reuters.com/article/us-amazon-com-jobs-automation-insight-idUSKCN1MK08G](https://www.reuters.com/article/us-amazon-com-jobs-automation-insight-idUSKCN1MK08G)'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*亚马逊废弃了一个显示出对女性有偏见的秘密 AI 招聘工具，* Jeffrey Dastin*,* [https://www.reuters.com/article/us-amazon-com-jobs-automation-insight-idUSKCN1MK08G](https://www.reuters.com/article/us-amazon-com-jobs-automation-insight-idUSKCN1MK08G)'
- en: '*Understanding and Evaluating Racial Biases in Image Captioning,* Dora Zhao,
    Angelina Wang, Olga Russakovsky, [https://arxiv.org/pdf/2106.08503.pdf](https://arxiv.org/pdf/2106.08503.pdf)'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*理解和评估图像描述中的种族偏见，* Dora Zhao, Angelina Wang, Olga Russakovsky, [https://arxiv.org/pdf/2106.08503.pdf](https://arxiv.org/pdf/2106.08503.pdf)'
- en: Follow me on [Medium](https://medium.com/@kevin.berlemont) for more content
    about Data Science!
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 关注我在 [Medium](https://medium.com/@kevin.berlemont) 上，获取更多关于数据科学的内容！
- en: If you enjoy reading stories like these and want to support me as a writer,
    consider [signing up to become a Medium member](https://medium.com/@kevin.berlemont/membership).
    It’s $5 a month, giving you unlimited access to stories on Medium. If you [sign
    up using my link](https://medium.com/@kevin.berlemont/membership), I’ll earn a
    small commission.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你喜欢阅读这样的故事并想支持我作为作者，可以考虑 [注册成为 Medium 会员](https://medium.com/@kevin.berlemont/membership)。每月
    $5，你将无限制访问 Medium 上的所有故事。如果你 [使用我的链接注册](https://medium.com/@kevin.berlemont/membership)，我将获得少量佣金。
- en: '[](https://medium.com/@kevin.berlemont/membership?source=post_page-----1723ce3973aa--------------------------------)
    [## Join Medium with my referral link - Kevin Berlemont, PhD'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@kevin.berlemont/membership?source=post_page-----1723ce3973aa--------------------------------)
    [## 使用我的推荐链接加入 Medium - Kevin Berlemont, PhD'
- en: As a Medium member, a portion of your membership fee goes to writers you read,
    and you get full access to every story…
  id: totrans-36
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 作为 Medium 的会员，你的部分会费将用于支持你阅读的作者，并且你可以完全访问每一个故事……
- en: medium.com](https://medium.com/@kevin.berlemont/membership?source=post_page-----1723ce3973aa--------------------------------)
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: medium.com](https://medium.com/@kevin.berlemont/membership?source=post_page-----1723ce3973aa--------------------------------)
