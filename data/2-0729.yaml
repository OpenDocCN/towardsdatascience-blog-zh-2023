- en: Deterministic vs. Probabilistic Deep Learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 确定性与概率性深度学习
- en: 原文：[https://towardsdatascience.com/deterministic-vs-probabilistic-deep-learning-5325769dc758](https://towardsdatascience.com/deterministic-vs-probabilistic-deep-learning-5325769dc758)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/deterministic-vs-probabilistic-deep-learning-5325769dc758](https://towardsdatascience.com/deterministic-vs-probabilistic-deep-learning-5325769dc758)
- en: Probabilistic Deep Learning
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 概率性深度学习
- en: '[](https://medium.com/@luisroque?source=post_page-----5325769dc758--------------------------------)[![Luís
    Roque](../Images/e281d470b403375ba3c6f521b1ccf915.png)](https://medium.com/@luisroque?source=post_page-----5325769dc758--------------------------------)[](https://towardsdatascience.com/?source=post_page-----5325769dc758--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----5325769dc758--------------------------------)
    [Luís Roque](https://medium.com/@luisroque?source=post_page-----5325769dc758--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@luisroque?source=post_page-----5325769dc758--------------------------------)[![路易斯·罗克](../Images/e281d470b403375ba3c6f521b1ccf915.png)](https://medium.com/@luisroque?source=post_page-----5325769dc758--------------------------------)[](https://towardsdatascience.com/?source=post_page-----5325769dc758--------------------------------)[![数据科学之路](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----5325769dc758--------------------------------)
    [路易斯·罗克](https://medium.com/@luisroque?source=post_page-----5325769dc758--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----5325769dc758--------------------------------)
    ·9 min read·Jan 11, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [数据科学之路](https://towardsdatascience.com/?source=post_page-----5325769dc758--------------------------------)
    ·9分钟阅读·2023年1月11日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: Introduction
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: This article belongs to the series “Probabilistic Deep Learning”. This weekly
    series covers probabilistic approaches to deep learning. The main goal is to extend
    deep learning models to quantify uncertainty, i.e., know what they do not know.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 本文属于“概率性深度学习”系列。该系列每周覆盖概率性深度学习的方法。主要目标是扩展深度学习模型以量化不确定性，即了解它们所不知道的内容。
- en: This article covers the main differences between Deterministic and Probabilistic
    deep learning. Deterministic deep learning models are trained to optimize a scalar-valued
    loss function, while probabilistic deep learning models are trained to optimize
    a probabilistic objective function. Deterministic models provide a single prediction
    for each input, while probabilistic models provide a probabilistic characterization
    of the uncertainty in their predictions, as well as the ability to generate new
    samples from the model.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 本文涵盖了确定性与概率性深度学习之间的主要区别。确定性深度学习模型旨在优化标量值损失函数，而概率性深度学习模型则旨在优化概率目标函数。确定性模型为每个输入提供一个单一预测，而概率性模型则提供对其预测不确定性的概率性描述，并具备从模型生成新样本的能力。
- en: 'Articles published so far:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 目前已发布的文章：
- en: '[Gentle Introduction to TensorFlow Probability: Distribution Objects](https://medium.com/towards-data-science/gentle-introduction-to-tensorflow-probability-distribution-objects-1bb6165abee1)'
  id: totrans-10
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[TensorFlow Probability 简介：分布对象](https://medium.com/towards-data-science/gentle-introduction-to-tensorflow-probability-distribution-objects-1bb6165abee1)'
- en: '[Gentle Introduction to TensorFlow Probability: Trainable Parameters](https://medium.com/towards-data-science/gentle-introduction-to-tensorflow-probability-trainable-parameters-5098ea4fed15)'
  id: totrans-11
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[TensorFlow Probability 简介：可训练参数](https://medium.com/towards-data-science/gentle-introduction-to-tensorflow-probability-trainable-parameters-5098ea4fed15)'
- en: '[Maximum Likelihood Estimation from scratch in TensorFlow Probability](/maximum-likelihood-estimation-from-scratch-in-tensorflow-probability-2fc0eefdbfc2)'
  id: totrans-12
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[从头开始的最大似然估计（TensorFlow Probability）](/maximum-likelihood-estimation-from-scratch-in-tensorflow-probability-2fc0eefdbfc2)'
- en: '[Probabilistic Linear Regression from scratch in TensorFlow](/probabilistic-linear-regression-from-scratch-in-tensorflow-2eb633fffc00)'
  id: totrans-13
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[从头开始的概率性线性回归（TensorFlow）](/probabilistic-linear-regression-from-scratch-in-tensorflow-2eb633fffc00)'
- en: '[Probabilistic vs. Deterministic Regression with Tensorflow](https://medium.com/towards-data-science/probabilistic-vs-deterministic-regression-with-tensorflow-85ef791beeef)'
  id: totrans-14
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[概率性与确定性回归在 TensorFlow 中的比较](https://medium.com/towards-data-science/probabilistic-vs-deterministic-regression-with-tensorflow-85ef791beeef)'
- en: '[Frequentist vs. Bayesian Statistics with Tensorflow](https://medium.com/towards-data-science/frequentist-vs-bayesian-statistics-with-tensorflow-fbba2c6c9ae5)'
  id: totrans-15
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[频率派与贝叶斯统计（Tensorflow）](https://medium.com/towards-data-science/frequentist-vs-bayesian-statistics-with-tensorflow-fbba2c6c9ae5)'
- en: Deterministic vs. Probabilistic Deep Learning
  id: totrans-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 确定性与概率性深度学习
- en: '![](../Images/71c0028b4a604a1f23ee10072d79519e.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/71c0028b4a604a1f23ee10072d79519e.png)'
- en: 'Figure 1: The motto for today: we are always uncertain about things; why should
    our models be any different? ([source](https://unsplash.com/photos/Vkp9wg-VAsQ))'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：今天的座右铭：我们对事物总是充满不确定性；我们的模型为什么应该有所不同？ ([source](https://unsplash.com/photos/Vkp9wg-VAsQ))
- en: We develop our models using TensorFlow and TensorFlow Probability. TensorFlow
    Probability is a Python library built on top of TensorFlow. We will start with
    the basic objects we can find in TensorFlow Probability and understand how we
    can manipulate them. We will increase complexity incrementally over the following
    weeks and combine our probabilistic models with deep learning on modern hardware
    (e.g. GPU).
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用 TensorFlow 和 TensorFlow Probability 开发我们的模型。TensorFlow Probability 是一个建立在
    TensorFlow 之上的 Python 库。我们将从 TensorFlow Probability 中找到的基本对象开始，并了解如何操作它们。在接下来的几周中，我们将逐步增加复杂性，并将我们的概率模型与现代硬件（例如
    GPU）上的深度学习相结合。
- en: As usual, the code is available on my [GitHub](https://github.com/luisroque/probabilistic_deep_learning_with_TFP).
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 和往常一样，代码可以在我的 [GitHub](https://github.com/luisroque/probabilistic_deep_learning_with_TFP)
    上找到。
- en: Deterministic vs. Probabilistic Deep Learning
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 确定性与概率深度学习
- en: Deep learning has become the dominant approach for a wide range of machine learning
    tasks, such as image and speech recognition, natural language processing, and
    reinforcement learning. A key feature of deep learning is the ability to learn
    complex, non-linear functions from large amounts of data. However, traditional
    deep learning models are deterministic, meaning they make the same predictions
    given the same inputs.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习已经成为广泛机器学习任务（如图像和语音识别、自然语言处理和强化学习）的主导方法。深度学习的一个关键特性是能够从大量数据中学习复杂的非线性函数。然而，传统的深度学习模型是确定性的，意味着它们在给定相同输入时会做出相同的预测。
- en: Deterministic deep learning models, such as feedforward neural networks, are
    trained to optimize a scalar-valued loss function, such as mean squared error
    or cross-entropy. The trained model produces a single prediction for each input,
    but does not provide any information about the uncertainty of the prediction.
    In contrast, probabilistic deep learning models, such as variational autoencoders
    (VAEs) and generative adversarial networks (GANs), are trained to optimize a probabilistic
    objective function, such as the negative log-likelihood of the data or an approximate
    posterior distribution. These models provide a probabilistic characterization
    of the uncertainty in their predictions.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 确定性深度学习模型，例如前馈神经网络，旨在优化标量值的损失函数，如均方误差或交叉熵。训练后的模型为每个输入生成一个单一的预测，但不提供关于预测不确定性的任何信息。相比之下，概率深度学习模型，如变分自编码器（VAEs）和生成对抗网络（GANs），旨在优化概率目标函数，如数据的负对数似然或近似后验分布。这些模型提供了对其预测不确定性的概率特征。
- en: One of the main advantages of probabilistic deep learning models is the ability
    to generate new samples from the model. For example, a VAE can be used to generate
    new images that are similar to the training data, while a GAN can be used to generate
    new images that are different from the training data. Additionally, probabilistic
    deep learning models are often used for unsupervised learning, where the goal
    is to learn a compact representation of the data without any labels.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 概率深度学习模型的主要优势之一是能够从模型中生成新样本。例如，VAE 可以用来生成与训练数据相似的新图像，而 GAN 可以用来生成与训练数据不同的新图像。此外，概率深度学习模型通常用于无监督学习，其目标是学习数据的紧凑表示而无需任何标签。
- en: Data and Approach
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据与方法
- en: In this article, we employ the MNIST dataset and its corrupted counterpart to
    evaluate our approach. The corrupted version of the MNIST dataset introduces an
    added level of complexity through grey spatters superimposed on the original images,
    making classifying the handwritten digits more challenging.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们使用 MNIST 数据集及其被破坏的对应数据集来评估我们的方法。MNIST 数据集的破坏版本通过在原始图像上叠加灰色斑点，增加了额外的复杂性，使手写数字分类更具挑战性。
- en: Our objective is to construct a convolutional neural network (CNN) that effectively
    classifies the images of handwritten digits into 10 distinct classes. To this
    end, we make use of the aforementioned datasets, in order to provide a comprehensive
    evaluation of the performance of both the deterministic and the probabilistic
    CNNs.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的目标是构建一个卷积神经网络（CNN），有效地将手写数字的图像分类为 10 个不同的类别。为此，我们使用前述数据集，以提供对确定性和概率 CNN 性能的全面评估。
- en: '[PRE0]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '![](../Images/98fbc67bcdc61423c1180bbbb62c181e.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/98fbc67bcdc61423c1180bbbb62c181e.png)'
- en: 'Figure 2: Random examples from the MNIST dataset.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：来自 MNIST 数据集的随机示例。
- en: '[PRE1]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '![](../Images/216a793a81c0b023de312311daadf6cb.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/216a793a81c0b023de312311daadf6cb.png)'
- en: 'Figure 3: Random examples from the corrupted version of the MNIST dataset.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：来自损坏版本的 MNIST 数据集的随机示例。
- en: Deterministic Architecture
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 确定性架构
- en: 'We begin by formally introducing the deterministic model, which is a convolutional
    neural network (CNN) classifier comprised of several key architectural components.
    Specifically, this model consists of:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先正式介绍确定性模型，它是一个由多个关键架构组件组成的卷积神经网络（CNN）分类器。具体来说，这个模型包括：
- en: A convolutional layer, in which the convolutional operation is performed by
    a set of 8 filters with a kernel size of 5x5 and ‘VALID’ padding, and the output
    is passed through a rectified linear unit (ReLU) activation function.
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个卷积层，其中卷积操作由一组 8 个滤波器进行，滤波器的大小为 5x5，采用‘VALID’填充，输出通过一个修正线性单元（ReLU）激活函数。
- en: A max-pooling layer, in which the maximum value within non-overlapping windows
    of size 6x6 is taken, reducing the spatial dimensionality of the feature maps.
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个最大池化层，其中在大小为 6x6 的非重叠窗口内取最大值，减少特征图的空间维度。
- en: A flatten layer, in which the pooled feature maps are collapsed into a single
    vector, allowing for the final dense layer to have fully connected computations.
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个 flatten 层，将池化后的特征图压缩成一个单一的向量，使得最终的全连接层可以进行完全连接的计算。
- en: A dense layer, which is also known as fully-connected layer, has 10 units and
    applies a softmax activation function to obtain the final probability distribution
    over the class labels.
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个全连接层，也称为全连接层，有 10 个单元，并应用 softmax 激活函数以获得类别标签的最终概率分布。
- en: This CNN classifier architecture is designed to efficiently extract discriminative
    features and perform classification on high-dimensional image data.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 这个 CNN 分类器架构旨在高效提取辨别特征并对高维图像数据进行分类。
- en: '[PRE2]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: We have implemented the architecture discussed above, so now we are ready to
    start the training procedure.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经实现了上述讨论的架构，因此现在我们准备开始训练过程。
- en: '[PRE3]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Finally, we can check the accuracy on both datasets.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以在两个数据集上检查准确性。
- en: '[PRE4]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Probabilistic Architecture
  id: totrans-46
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 概率架构
- en: In contrast to the deterministic model previously discussed, our probabilistic
    model introduces a new approach to outputting a distribution object, specifically
    a One-Hot Categorical distribution. This enables the modeling of aleatoric uncertainty
    on the image labels, allowing for a more comprehensive characterization of the
    model’s predictions.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 与之前讨论的确定性模型相比，我们的概率模型引入了一种新的输出分布对象的方法，即 One-Hot 分类分布。这使得可以对图像标签的随机不确定性进行建模，从而更全面地表征模型的预测。
- en: The One-Hot Categorical distribution is a discrete probability distribution
    over one-hot bit vectors, with the event dimension equal to K, the number of classes.
    It is mathematically equivalent to the Categorical distribution, which is a discrete
    probability distribution over positive integers, with the key distinction being
    that Categorical has an empty event dimension, whereas One-Hot Categorical has
    event dimension equal to K.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: One-Hot 分类分布是一种离散概率分布，作用于 one-hot 位向量，其中事件维度等于 K，即类别数量。它在数学上等同于分类分布，分类分布是一种离散概率分布，作用于正整数，其关键区别在于分类分布具有空事件维度，而
    One-Hot 分类分布的事件维度等于 K。
- en: 'Our proposed probabilistic CNN architecture consists of:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提出的概率卷积神经网络（CNN）架构包括：
- en: A convolutional layer, in which the convolutional operation is performed by
    a set of 8 filters with a kernel size of 5x5 and ‘VALID’ padding, and the output
    is passed through a rectified linear unit (ReLU) activation function.
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个卷积层，其中卷积操作由一组 8 个滤波器进行，滤波器的大小为 5x5，采用‘VALID’填充，输出通过一个修正线性单元（ReLU）激活函数。
- en: A max-pooling layer, in which the maximum value within non-overlapping windows
    of size 6x6 is taken, reducing the spatial dimensionality of the feature maps.
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个最大池化层，其中在大小为 6x6 的非重叠窗口内取最大值，减少特征图的空间维度。
- en: A flatten layer, in which the pooled feature maps are collapsed into a single
    vector, allowing for the final dense layer to have fully connected computations.
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个 flatten 层，将池化后的特征图压缩成一个单一的向量，使得最终的全连接层可以进行完全连接的计算。
- en: A dense layer, which is also known as fully-connected layer, with the number
    of units required to parameterize the probabilistic layer that follows.
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个密集层，也称为全连接层，具有参数化随后的概率层所需的单元数量。
- en: A OneHotCategorical distribution layer, with an event shape equal to 10, corresponding
    to the 10 classes.
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个 OneHotCategorical 分布层，其事件形状为 10，对应于 10 个类别。
- en: This novel architecture, incorporating One-Hot Categorical output distribution,
    enables the modeling of aleatoric uncertainty on the image labels, and allows
    for a more comprehensive characterization of the model’s predictions.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 这一新颖的架构，结合了 One-Hot Categorical 输出分布，使得能够对图像标签上的随机不确定性进行建模，并允许对模型预测进行更全面的表征。
- en: '[PRE5]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Our implementation of the model is done. Take a moment to compare the implementation
    above with the one that we did earlier that implements the corresponding deterministic
    architecture. If you have any doubts about specific components that we defined
    for the probabilistic version of the model, e.g. the loss function, please refer
    to the earlier articles in this series. Once again, we can now start the training
    procedure.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的模型实现完成了。请花一点时间将上述实现与我们之前进行的实现（实现相应的确定性架构）进行比较。如果您对我们为概率模型版本定义的特定组件（例如损失函数）有任何疑问，请参考本系列早期的文章。我们现在可以开始训练过程。
- en: '[PRE6]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Finally, we can check the accuracy of this version of the model. Note that the
    test accuracy of the probabilistic model is identical to the deterministic one.
    This is because the model architectures for both are equivalent; the only difference
    being that the probabilistic model returns a distribution object.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以检查这个版本模型的准确性。请注意，概率模型的测试准确性与确定性模型相同。这是因为两者的模型架构是等效的，唯一的区别是概率模型返回一个分布对象。
- en: '[PRE7]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Results and Discussion
  id: totrans-61
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结果与讨论
- en: Accuracy is an important metric to assess the performance of the model. Nonetheless,
    it is sometimes shallow since it does not provide information about the uncertainty
    of the predictions.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 准确率是评估模型性能的重要指标。然而，它有时较为浅显，因为它未提供有关预测不确定性的信息。
- en: In this section, we go beyond the predicted label and provide a visual representation
    of the uncertainty in the predictions. To accomplish this, we sample the predictive
    distribution of the model and calculate the percentiles of the resulting samples.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们超越预测标签，提供了对预测不确定性的可视化表示。为此，我们对模型的预测分布进行采样，并计算结果样本的百分位数。
- en: '[PRE8]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The model is very confident that the first image is a 7, which is correct. For
    the second image, the model struggles, assigning nonzero probabilities to many
    different classes.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 模型对第一张图像是 7 的预测非常有信心，这是正确的。对于第二张图像，模型遇到困难，为许多不同类别分配了非零概率。
- en: '[PRE9]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '![](../Images/b04c8d0904a01b294ab8603b1e14abcf.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b04c8d0904a01b294ab8603b1e14abcf.png)'
- en: 'Figure 4: Predictions from the probabilistic model for the MNIST dataset.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：概率模型对 MNIST 数据集的预测。
- en: Once again, the model is confident about its prediction of the first image.
    Despite the spatters, the number is still easy to identify. The second number
    is significantly harder to identify. The model still does a good job of predicting
    the right number and showing uncertainty about that choice.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，该模型对第一张图像的预测非常有信心。尽管有些污点，但这个数字仍然容易识别。第二个数字则显著更难识别。模型仍然能够很好地预测正确的数字，并展示了对这个选择的不确定性。
- en: '[PRE10]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '![](../Images/20f19ec19e798f6683ece0420dd5a761.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/20f19ec19e798f6683ece0420dd5a761.png)'
- en: 'Figure 5: Predictions from the probabilistic model for the corrupted version
    of the MNIST dataset.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：概率模型对 MNIST 数据集损坏版本的预测。
- en: Conclusion
  id: totrans-73
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: In this article, we have discussed the key differences between deterministic
    and probabilistic deep learning models, focusing on using these models for image
    classification tasks. By analyzing the performance of a CNN on the MNIST dataset
    and on the corrupted version of the same dataset, we have shown that probabilistic
    deep learning models can achieve similar accuracy levels to deterministic models
    and provide a probabilistic characterization of the uncertainty in their predictions.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们讨论了确定性和概率深度学习模型之间的关键差异，重点介绍了这些模型在图像分类任务中的应用。通过分析 CNN 在 MNIST 数据集及其损坏版本上的表现，我们展示了概率深度学习模型可以实现与确定性模型相似的准确度水平，并提供了对其预测不确定性的概率描述。
- en: One of the main advantages of probabilistic deep learning models is the ability
    to generate new samples from the model. This can be useful for tasks such as image
    synthesis or data augmentation, where the goal is to create new, realistic images
    from a given dataset. Additionally, probabilistic deep learning models can be
    used for unsupervised learning, where the goal is to learn a compact representation
    of the data without any labels.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 概率深度学习模型的主要优势之一是能够从模型中生成新样本。这在图像合成或数据增强等任务中非常有用，目标是从给定的数据集中创建新的、真实的图像。此外，概率深度学习模型还可以用于无监督学习，其目标是学习数据的紧凑表示而无需任何标签。
- en: 'Keep in touch: [LinkedIn](https://www.linkedin.com/in/luisbrasroque/)'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '保持联系: [LinkedIn](https://www.linkedin.com/in/luisbrasroque/)'
- en: References and Materials
  id: totrans-77
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考资料和材料
- en: '[1] — [Coursera: Deep Learning Specialization](https://www.coursera.org/specializations/deep-learning)'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] — [Coursera: 深度学习专业化](https://www.coursera.org/specializations/deep-learning)'
- en: '[2] — [Coursera: TensorFlow 2 for Deep Learning](https://www.coursera.org/specializations/tensorflow2-deeplearning)
    Specialization'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] — [Coursera: TensorFlow 2 深度学习](https://www.coursera.org/specializations/tensorflow2-deeplearning)
    专业化'
- en: '[3] — [TensorFlow Probability Guides and Tutorials](https://www.tensorflow.org/probability/overview)'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] — [TensorFlow 概率指南与教程](https://www.tensorflow.org/probability/overview)'
- en: '[4] — [TensorFlow Probability Posts in TensorFlow Blog](https://blog.tensorflow.org/search?label=TensorFlow+Probability&max-results=20)'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] — [TensorFlow 博客中的 TensorFlow 概率文章](https://blog.tensorflow.org/search?label=TensorFlow+Probability&max-results=20)'
