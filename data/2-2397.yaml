- en: World History Through the Lens of AI
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从人工智能的视角看世界历史
- en: 原文：[https://towardsdatascience.com/world-history-through-the-lens-of-ai-340df6241fbf](https://towardsdatascience.com/world-history-through-the-lens-of-ai-340df6241fbf)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/world-history-through-the-lens-of-ai-340df6241fbf](https://towardsdatascience.com/world-history-through-the-lens-of-ai-340df6241fbf)
- en: What historical knowledge do language models encode?
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 语言模型编码了什么历史知识？
- en: '[](https://medium.com/@artfish?source=post_page-----340df6241fbf--------------------------------)[![Yennie
    Jun](../Images/b635e965f21c3d55833269e12e861322.png)](https://medium.com/@artfish?source=post_page-----340df6241fbf--------------------------------)[](https://towardsdatascience.com/?source=post_page-----340df6241fbf--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----340df6241fbf--------------------------------)
    [Yennie Jun](https://medium.com/@artfish?source=post_page-----340df6241fbf--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@artfish?source=post_page-----340df6241fbf--------------------------------)[![Yennie
    Jun](../Images/b635e965f21c3d55833269e12e861322.png)](https://medium.com/@artfish?source=post_page-----340df6241fbf--------------------------------)[](https://towardsdatascience.com/?source=post_page-----340df6241fbf--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----340df6241fbf--------------------------------)
    [Yennie Jun](https://medium.com/@artfish?source=post_page-----340df6241fbf--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----340df6241fbf--------------------------------)
    ·11 min read·Jul 7, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----340df6241fbf--------------------------------)
    ·11分钟阅读·2023年7月7日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/d14490fb1ce7bc05910c5287b7e27954.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d14490fb1ce7bc05910c5287b7e27954.png)'
- en: Probing OpenAI’s GPT-4, Anthropic’s Claude, and TII’s Falcon 40B Instruct on
    top historical events from 1910 (prompted in 6 different languages). Created by
    the author.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 探讨OpenAI的GPT-4、Anthropic的Claude和TII的Falcon 40B Instruct对1910年重大历史事件的分析（以6种不同语言提示）。由作者创作。
- en: '*This article was* [*originally published on my blog*](https://www.artfish.ai/p/world-history-through-ai)'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '*本文* [*最初发布在我的博客上*](https://www.artfish.ai/p/world-history-through-ai)'
- en: Advancements in artificial intelligence, particularly large language models,
    open up exciting possibilities for [historical research](https://ts2.space/en/chatgpt-4-a-valuable-tool-for-historical-research-and-analysis/#:~:text=ChatGPT%2D4%20works%20by%20taking,information%20available%20in%20its%20database.)
    and [education](https://www.history4humans.com/blogs/history/7-ways-history-teachers-can-use-chat-gpt-in-the-classroom).
    However, it is important to scrutinize the ways these models interpret and recall
    the past. Do they reflect any inherent biases in their understanding of history?
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 人工智能的进步，尤其是大型语言模型，为 [历史研究](https://ts2.space/en/chatgpt-4-a-valuable-tool-for-historical-research-and-analysis/#:~:text=ChatGPT%2D4%20works%20by%20taking,information%20available%20in%20its%20database.)
    和 [教育](https://www.history4humans.com/blogs/history/7-ways-history-teachers-can-use-chat-gpt-in-the-classroom)
    开辟了令人兴奋的可能性。然而，重要的是审视这些模型如何解读和回忆过去。它们是否反映了它们对历史理解中的任何固有偏见？
- en: I am well aware of the subjectivity of history (I majored in history in my undergrad!).
    The events we remember and the narratives we form about the past are heavily influenced
    by the historians who penned them and the society we inhabit. Take, for instance,
    my high school world history course, which devoted over 75% of the curriculum
    to European history, skewing my understanding of world events.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 我非常清楚历史的主观性（我在本科时主修历史！）。我们记住的事件和我们对过去形成的叙事受到撰写这些叙事的历史学家和我们所处社会的强烈影响。例如，我的高中世界历史课程将超过75%的课程内容集中在欧洲历史上，从而偏颇了我对世界事件的理解。
- en: 'In this article, I explore how human history gets remembered and interpreted
    through the lens of AI. I examine the interpretations of key historical events
    by several large language models to uncover:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我探讨了人类历史如何通过人工智能的视角被记忆和解释。我考察了多个大型语言模型对关键历史事件的解释，以揭示：
- en: Do these models display a Western or American bias towards events?
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这些模型是否对事件表现出西方或美国的偏见？
- en: Do the models’ historical interpretations differ based on the language used
    for prompts, such as Korean or French prompts emphasizing more Korean or French
    events, respectively?
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型的历史解释是否根据提示的语言（如韩语或法语提示）有所不同，例如韩语提示可能更强调韩国事件，而法语提示则更强调法国事件？
- en: With these questions in mind, let’s dive in!
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 带着这些问题，让我们深入探讨吧！
- en: 'Example: 1910'
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 示例：1910
- en: As an example, I asked three different large language models (LLMs) what the
    major historical events in the year 1910 were. (More details on each LLM in the
    next section.)
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一个例子，我询问了三个不同的大型语言模型（LLMs）1910年的主要历史事件是什么。（每个LLM的更多细节在下一节。）
- en: '![](../Images/ca42277b8edf1b656d5bfed3b5482c56.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ca42277b8edf1b656d5bfed3b5482c56.png)'
- en: OpenAI’s GPT-4, Anthropic’s Claude, and Technology Innovation Institute’s Falcon
    40B Instruct respond to a prompt in English about top historical events in 1910\.
    Created by the author.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI的GPT-4、Anthropic的Claude和Technology Innovation Institute的Falcon 40B Instruct在关于1910年主要历史事件的英文提示下作出回应。由作者创建。
- en: The question I posed was deliberately loaded with no objective answer. The significance
    of the year 1910 varies greatly depending on one’s cultural perspective. In Korean
    history, it marks the start of the Japanese occupation, a turning point that significantly
    influenced the country’s trajectory (see [Japan-Korea Treaty of 1910](https://en.wikipedia.org/wiki/Japan%E2%80%93Korea_Treaty_of_1910)).
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 我提出的问题故意没有客观答案。1910年的重要性因文化视角的不同而大相径庭。在韩国历史中，它标志着日本占领的开始，这是一个显著影响该国发展轨迹的转折点（见[1910年日韩条约](https://en.wikipedia.org/wiki/Japan%E2%80%93Korea_Treaty_of_1910)）。
- en: Yet, the Japanese annexation of Korea did not feature in any of the responses.
    I wondered if the same models would interpret the question differently if prompted
    in a different language — say, in Korean.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，日本对韩国的吞并在任何回应中都没有出现。我想知道如果用不同的语言——比如韩语——来提示这些模型，它们是否会有不同的解读。
- en: '![](../Images/2b4c8297c482e090423cf84e68eaefea.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2b4c8297c482e090423cf84e68eaefea.png)'
- en: OpenAI’s GPT-4, Anthropic’s Claude, and Technology Innovation Institute’s Falcon
    40B Instruct respond to a prompt in Korean about top historical events in 1910\.
    Korean responses have been translated by me into English (in red). Created by
    the author.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI的GPT-4、Anthropic的Claude和Technology Innovation Institute的Falcon 40B Instruct在关于1910年主要历史事件的韩语提示下作出回应。韩语回答已由我翻译成英文（红色）。由作者创建。
- en: Prompted in Korean, one of the top events noted by Claude is indeed the Japanese
    Annexation of Korea. However, I found it interesting that two out of five of GPT-4’s
    important events were US-centric (Boy Scouts and Mann-Elkins Act) while neglecting
    to mention the Annexation of Korea. Not to mention that Falcon, even when prompted
    in Korean, responded in English.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在韩语提示下，Claude确实将日本对韩国的吞并列为主要事件之一。然而，我发现有趣的是，GPT-4提到的五个重要事件中有两个是以美国为中心（童子军和曼-埃尔金斯法案），而忽略了提到韩国吞并。更不用说，即使在韩语提示下，Falcon也用英语回应。
- en: The experiments
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实验
- en: 'The experiment setup was as follows:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 实验设置如下：
- en: '3 models: [OpenAI’s GPT-4](https://openai.com/gpt-4), [Anthropic’s Claude](https://www.anthropic.com/index/introducing-claude),
    and TII’s [Falcon-40B-Instruct](https://huggingface.co/tiiuae/falcon-40b-instruct#:~:text=Falcon%2D40B%2DInstruct%20is%20a,under%20the%20Apache%202.0%20license.)'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 3个模型：[OpenAI的GPT-4](https://openai.com/gpt-4)、[Anthropic的Claude](https://www.anthropic.com/index/introducing-claude)和TII的[Falcon-40B-Instruct](https://huggingface.co/tiiuae/falcon-40b-instruct#:~:text=Falcon%2D40B%2DInstruct%20is%20a,under%20the%20Apache%202.0%20license.)
- en: '6 languages: English, French, Spanish, Korean, Japanese, Chinese'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 6种语言：英语、法语、西班牙语、韩语、日语、中文
- en: 3 years (610, 1848, 1910)
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 3年（610年，1848年，1910年）
- en: 5 historical events per run
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每次运行5个历史事件
- en: 10 runs
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 10次运行
- en: = 2700 total events
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: = 2700个总事件
- en: Languages and Prompts
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 语言和提示
- en: 'The languages I chose were mostly arbitrary, based on the languages that I
    was the most familiar with (English, Korean) and those that a few of my closest
    friends spoke and could translate for me (Chinese, Japanese, French, Spanish).
    Translations can be found at the end of the article. I asked them to translate
    the English for me:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 我选择的语言大多是随意的，基于我最熟悉的语言（英语、韩语）以及一些我最亲近的朋友讲的语言，这些朋友可以为我翻译（中文、日语、法语、西班牙语）。翻译可以在文章末尾找到。我请他们为我翻译英文内容：
- en: '`“Top five historical events in the year {}, ranked by importance. Be brief
    and only give the name of the event.”`'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '`“列出1910年五大历史事件，按重要性排序。简要说明，仅提供事件名称。”`'
- en: Models
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型
- en: '[OpenAI’s GPT-4](https://openai.com/gpt-4) is the newer generation of ChatGPT,
    which is one of the most popular AI chatbot (with [over 100 million monthly active
    users](https://explodingtopics.com/blog/chatgpt-users))'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[OpenAI的GPT-4](https://openai.com/gpt-4)是ChatGPT的更新版本，是最受欢迎的AI聊天机器人之一（[每月活跃用户超过1亿](https://explodingtopics.com/blog/chatgpt-users)）'
- en: '[Anthropic’s Claude](https://www.anthropic.com/index/introducing-claude) is
    a ChatGPT competitor trained to be harmless and helpful using a method called
    [Constitutional AI](https://www.anthropic.com/index/claudes-constitution)'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Anthropic 的 Claude](https://www.anthropic.com/index/introducing-claude) 是一个与
    ChatGPT 竞争的模型，采用称为 [Constitutional AI](https://www.anthropic.com/index/claudes-constitution)
    的方法进行训练，以实现无害和有帮助。'
- en: '[Technical Innovation Institute](https://www.tii.ae/)’s [Falcon-40B-Instruct](https://huggingface.co/tiiuae/falcon-40b-instruct#:~:text=Falcon%2D40B%2DInstruct%20is%20a,under%20the%20Apache%202.0%20license.)
    is the best open-source language model, according to [HuggingFace’s Open LLM Leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard)'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[技术创新研究所](https://www.tii.ae/)的 [Falcon-40B-Instruct](https://huggingface.co/tiiuae/falcon-40b-instruct#:~:text=Falcon%2D40B%2DInstruct%20is%20a,under%20the%20Apache%202.0%20license.)
    是最好的开源语言模型，根据 [HuggingFace的开放LLM排行榜](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard)。'
- en: Normalizing the events
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 规范化事件
- en: Even if a model generated the same event with each run, there was a lot of diversity
    in the way it described the same event.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 即使一个模型在每次运行时生成相同的事件，它描述同一事件的方式也存在很大差异。
- en: 'For example, the following all refer to the same event:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，以下所有表述都指代同一事件：
- en: “Japan annexation of Korea”
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “日本对朝鲜的吞并”
- en: “Japan’s Annexation of Korea”
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “日本对朝鲜的吞并”
- en: “Japan annexes Korea”
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “日本吞并朝鲜”
- en: “Japan-Korea Annexation Treaty”
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “日韩吞并条约”
- en: I needed a way to refer to a single event (the Japanese annexation of Korea)
    using the same vocabulary (a process known as ***normalization***). Not to mention
    that the same event could be described in six different languages!
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 我需要一种方法来使用相同的词汇（称为 ***规范化***）来指代单一事件（日本对朝鲜的吞并）。更不用说同一事件可以用六种不同的语言来描述！
- en: I used a combination of manual rules, Google Translate, and GPT-4 to assist
    with the normalization. Initially I had hoped to use one LLM to normalize the
    events of another LLM (e.g. use GPT-4 to normalize Claude’s events; Claude to
    normalize Falcon’s events, etc) to reduce bias. However, Claude and Falcon were
    not very good at following directions to normalize and GPT-4 emerged as the best
    model for the job.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 我使用了手动规则、Google 翻译和 GPT-4 的组合来协助规范化。起初我希望使用一个 LLM 来规范化另一个 LLM 的事件（例如使用 GPT-4
    来规范化 Claude 的事件；Claude 来规范化 Falcon 的事件等），以减少偏见。然而，Claude 和 Falcon 在规范化方面并不擅长，GPT-4
    成为了最适合的模型。
- en: I acknowledge the biases that come with using a model to normalize its own events.
    However, as I used different sessions of GPT-4 to generate historical events and
    to normalize the events, there was no overlap in context. In the future, normalization
    can be done using a more objective method.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 我承认使用模型来规范化其自身事件会带来偏见。然而，由于我使用不同的 GPT-4 会话来生成历史事件并规范化这些事件，因此在上下文上没有重叠。未来，可以使用更客观的方法来进行规范化。
- en: Results
  id: totrans-49
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结果
- en: Overall, I was surprised by the different models’ understanding of history.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，我对不同模型对历史的理解感到惊讶。
- en: GPT-4 was more likely to generate the same events regardless of the language
    it was prompted with
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GPT-4 更可能生成相同的事件，无论用什么语言提示。
- en: Anthropic was more likely to generate historical events relevant to the language
    it was prompted with
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Anthropic 更可能生成与其提示语言相关的历史事件。
- en: Falcon (unfortunately) was more likely to make up fake events
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Falcon（不幸的是）更可能编造虚假的事件。
- en: All three models displayed a bias for Western or American events, but not in
    the way I expected. When prompted in a non-English language, the model would generate
    an American or British historical event (even when the model would not generate
    that event when prompted in English). This happened across all three models.
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所有三个模型都表现出对西方或美国事件的偏见，但方式并不是我所预期的。当用非英语语言提示时，模型会生成美国或英国的历史事件（即使在用英语提示时模型不会生成那个事件）。这种情况发生在所有三个模型中。
- en: 1\. Comparing languages for each model (1910)
  id: totrans-55
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 1\. 比较每个模型的语言（1910年）
- en: Each model x language combination generated “top 5 historical events” 10 times
    (= 50 events total). I took the subset of events which at least one language generated
    5 times or more. This was because models sometimes predicted a one-off event that
    it never predicted again. The cells with values 10 mean that the model predicted
    that event every single time I prompted it.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 每个模型 x 语言组合生成了“前 5 个历史事件” 10 次（= 总共 50 个事件）。我选择了至少有一种语言生成了 5 次或更多次的事件。这是因为模型有时预测了一个一次性事件，而之后再也没有预测过。值为
    10 的单元表示模型每次我提示时都预测了那个事件。
- en: In this section, I show the top events predicted by each of the 3 models, broken
    down by languages, for the year 1910\. Similar charts for the years 610 and 1848
    can be found on the [GitHub page](https://github.com/yenniejun/world-history-ai),
    where I shared all of the code and analyses.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我展示了1910年每个模型预测的主要事件，按语言分类。有关610年和1848年的类似图表可以在[GitHub页面](https://github.com/yenniejun/world-history-ai)找到，在那里我分享了所有代码和分析。
- en: '**GPT-4 (OpenAI)**'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '**GPT-4 (OpenAI)**'
- en: 'Mexican Revolution: across all languages, the Mexican Revolution was consistently
    an important world event — even in languages I didn’t expect, such as Korean or
    Japanese'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 墨西哥革命：在所有语言中，墨西哥革命始终是一个重要的世界事件——即使在我没预料到的语言中，如韩语或日语
- en: 'Japanese Annexation of Korea: Not mentioned when asked in Spanish or French.
    When prompted in Japanese, was more likely to mention this event (9 times) than
    when prompted in Korean (6 times), which I found strange and interesting'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 日本对韩国的吞并：在用西班牙语或法语提问时没有提到。用日语提问时，比用韩语提问时更可能提到这一事件（9次对比6次），这让我觉得既奇怪又有趣
- en: 'Boy Scouts of America founded: GPT-4 predicted this event when prompted in
    Japanese (7 times) nearly twice as often as when prompted in English (4 times).
    It seems like a random tidbits of American information was encoded into the Japanese
    understanding of 1910'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 美国童子军成立：GPT-4在用日语提问时（7次）几乎是用英语提问时（4次）的两倍。这似乎是一些美国信息被编码进了对1910年的日语理解中
- en: 'Establishment of Glacier National Park: Even stranger, GPT-4 predicted this
    event when prompted in Spanish and French, but not in English'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 冰川国家公园的建立：更奇怪的是，GPT-4在用西班牙语和法语提问时预测了这一事件，但在英语中却没有
- en: '![](../Images/26492702d682d965de75c7656ac8b1d3.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/26492702d682d965de75c7656ac8b1d3.png)'
- en: Top events generated by GPT-4 for the year 1910, compared across language it
    was prompted in. Created by the author.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-4为1910年生成的主要事件，按语言对比。由作者创建。
- en: '**Claude (Anthropic)**'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '**Claude (Anthropic)**'
- en: 'Overall: Unlike GPT-4, there was no single event that was deemed “important
    historical event” by *all languages.*'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言：与GPT-4不同，没有任何单一事件被*所有语言*认为是“重要历史事件”。
- en: 'Mexican Revolution: While generated often when asked in French, Spanish, and
    (inexplicably) Korean, not as important in English as was with GPT-4'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 墨西哥革命：虽然在法语、西班牙语和（莫名其妙的）韩语中经常生成，但在英语中并不像GPT-4中那样重要
- en: 'Japanese Annexation of Korea: More important for Korean and Japanese than for
    other languages (the two countries involved in the event)'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 日本对韩国的吞并：对韩语和日语比其他语言更重要（这两个国家参与了这一事件）
- en: 'Death of Edward VII: More important for English and French (and not for other
    languages). Edward VII was the King of the United Kingdom and apparently had good
    relations with France.'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 爱德华七世的去世：对英语和法语更重要（对其他语言则不然）。爱德华七世是英国国王，显然与法国关系良好。
- en: 'Exploration of Antarctica: This event was actually the [*British* Antarctic
    expedition](https://en.wikipedia.org/wiki/Terra_Nova_Expedition), in which a British
    man reached Antarctica for the first time. However, for some unknown reason, Claude
    generates this event only when prompted in Chinese or Japanese (but not in English).'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 南极洲探险：这一事件实际上是[*英国*南极探险](https://en.wikipedia.org/wiki/Terra_Nova_Expedition)，一位英国人首次到达南极。然而，由于某些未知原因，Claude仅在用中文或日语提问时生成这一事件（而不是英语）。
- en: '![](../Images/7cd7adb0af014824f174598f637ef8ca.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7cd7adb0af014824f174598f637ef8ca.png)'
- en: Top events generated by Claude for the year 1910, compared across language it
    was prompted in. Created by the author.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: Claude为1910年生成的主要事件，按语言对比。由作者创建。
- en: '**Falcon 40B Instruct (Open Source; TII)**'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '**Falcon 40B Instruct (Open Source; TII)**'
- en: Overall, Falcon was not as consistent or accurate as the other two models. The
    reason fewer events are shown in the chart is because there were no other events
    that Falcon predicted 5 times or more! Meaning that Falcon was a bit inconsistent
    in its predictions.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，Falcon在一致性和准确性方面不如其他两个模型。图表中显示的事件较少，是因为没有其他事件被Falcon预测5次或更多次！这意味着Falcon的预测有些不稳定。
- en: 'The Titanic sinks: This actually happened in 1912'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 泰坦尼克号沉没：这实际上发生在1912年
- en: 'Outbreak of World War I: This actually happened in 1914'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第一次世界大战爆发：这实际上发生在1914年
- en: Falcon is historically inaccurate in its predictions. But at least it got the
    decade right?
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Falcon在预测方面历史上不准确。但至少它对年代是正确的？
- en: '![](../Images/e2629980c71a61b226602c9a5cfee059.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e2629980c71a61b226602c9a5cfee059.png)'
- en: Top events generated by Falcon for the year 1910, compared across language it
    was prompted in. Created by the author.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 按语言对比的Falcon生成的1910年主要事件。由作者创建。
- en: 2\. Comparing model correlations for each language (1910)
  id: totrans-80
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 2\. 对每种语言的模型相关性进行比较（1910年）
- en: Next, I quantified how similar the *overall* predictions of one model compared
    to the others. I used a mathematical method ([cosine similarity](https://en.wikipedia.org/wiki/Cosine_similarity))
    to determine how similar two prediction distributions were. Values closer to 1
    signified that predictions were identical; values closer to 0 signified that two
    sets of predictions shared nothing in common.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我量化了一个模型的*整体*预测与其他模型的相似程度。我使用了一种数学方法（[余弦相似度](https://en.wikipedia.org/wiki/Cosine_similarity)）来确定两个预测分布的相似性。值越接近1表示预测越相同；值越接近0表示两个预测集没有任何共同点。
- en: Again, I show this example for the year 1910\. The other years can be found
    on the [GitHub page](https://github.com/yenniejun/world-history-ai).
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 再次展示1910年的例子。其他年份可以在[GitHub页面](https://github.com/yenniejun/world-history-ai)找到。
- en: Across most of the languages, GPT-4 and Claude had a higher correlation value
    — meaning that despite all of the languages, the two models predicted a high percentage
    of similar events.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在大多数语言中，GPT-4和Claude的相关性值较高——这意味着尽管使用了不同的语言，这两个模型预测了大量相似的事件。
- en: Falcon, on the other hand, tended to be less correlated, meaning that its understanding
    of history veered away from that of GPT-4 and Claude.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，Falcon的相关性较低，这意味着它对历史的理解与GPT-4和Claude有所不同。
- en: '![](../Images/2e4eeeb160aa502e7c0af8a60d745f3d.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2e4eeeb160aa502e7c0af8a60d745f3d.png)'
- en: Model correlations for events predicted for the year 1910\. Created by the author.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 针对1910年预测的事件模型相关性。由作者创建。
- en: 3\. Comparing models for each year
  id: totrans-87
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 3\. 对每一年的模型进行比较
- en: Next, I compared the different language models for each year. I combined all
    events predicted for all languages and considered the overall events predicted
    by a model, regardless of the language. I took the subset of events for which
    at least one model generated **10 times or more**.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我对每一年进行了不同语言模型的比较。我将所有语言预测的事件进行了汇总，并考虑了模型预测的整体事件，不论语言如何。我选取了至少有一个模型预测**10次或更多**的事件。
- en: Similar to the trends found in the section above, GPT-4 and Claude tended to
    predict similar major historical events for each year — The First Revelations
    of Muhammad and the Ascension of Emperor Heraclius to the Byzantine Throne in
    610; the European Revolutions of 1848; and the Mexican Revolution in 1910.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 与上述部分发现的趋势类似，GPT-4和Claude倾向于预测每年的主要历史事件——610年的穆罕默德的首次启示和赫拉克勒斯皇帝登基拜占庭王朝；1848年的欧洲革命；以及1910年的墨西哥革命。
- en: There were certain events that one model disproportionately predicted compared
    to the others. For example, for the year 1848, GPT-4 predicted “Publication of
    the Communist Manifesto” 42 times, compared to Claude’s 15 times. For the year
    1910, Claude predicted “Death of Edward VII” 26 times, compared to GPT-4’s 1 time.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 有些事件是某个模型相比其他模型预测得不成比例的。例如，在1848年，GPT-4预测了“《共产党宣言》的发表”42次，而Claude预测了15次。在1910年，Claude预测了“爱德华七世的去世”26次，而GPT-4仅预测了1次。
- en: Falcon tended to have the least understanding of historical events. Falcon missed
    major events for all three years. For the year 610, Falcon failed to predict the
    event of the Ascension of Emperor Heraclius. For the year 1910, it failed to predict
    events such as Japan’s Annexation of Korea, Formation of Union of South Africa,
    and Portuguese Revolution (all non-American global events), while instead predicting
    America-centric events such as the [Triangle Shirtwaist Factory Fire](https://en.wikipedia.org/wiki/Triangle_Shirtwaist_Factory_fire)
    (which happened in 1911, not 1910). Interestingly, Falcon was able to predict
    most of the 1848 events similar to the other two models — perhaps because the
    1848 events were more Western-centric (e.g. European revolutions)?
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: Falcon在理解历史事件方面通常最差。Falcon错过了所有三个年份的主要事件。在610年，Falcon未能预测赫拉克勒斯皇帝的登基。在1910年，它未能预测诸如日本吞并朝鲜、南非联盟的形成和葡萄牙革命（这些都是非美洲的全球事件），而是预测了一些美国中心的事件，如[三角工厂火灾](https://en.wikipedia.org/wiki/Triangle_Shirtwaist_Factory_fire)（发生在1911年，而非1910年）。有趣的是，Falcon能够预测大部分1848年的事件，这与其他两个模型相似——也许是因为1848年的事件更具西方中心（例如欧洲革命）？
- en: Events from longer ago (e.g. year 610) meant that history is a bit more fuzzy.
    The [Tang Dynasty was established in 618, not 610](https://en.wikipedia.org/wiki/Tang_dynasty)
    and the [Construction of the Grand Canal under Emperor Yang of Sui](https://en.wikipedia.org/wiki/Grand_Canal_(China)#:~:text=The%20Grand%20Canal%20was%20fully%20completed%20under%20the%20second%20Sui%20emperor%2C%20from%20the%20years%20604%20to%20609%2C%5B14%5D%20first%20by%20linking%20Luoyang%20to%20the%20Yangzhou%20(and%20the%20Yangtze%20valley)%2C)
    was actually completed under a longer period of time (604 to 609).
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 较久远的事件（例如610年）使得历史稍显模糊。[唐朝成立于618年，而非610年](https://en.wikipedia.org/wiki/Tang_dynasty)，而[隋炀帝下的运河建设](https://en.wikipedia.org/wiki/Grand_Canal_(China)#:~:text=The%20Grand%20Canal%20was%20fully%20completed%20under%20the%20second%20Sui%20emperor%2C%20from%20the%20years%20604%20to%20609%2C%5B14%5D%20first%20by%20linking%20Luoyang%20to%20the%20Yangzhou%20(and%20the%20Yangtze%20valley)%2C)
    实际上是在较长的时间段内完成的（604到609年）。
- en: '**610**'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '**610**'
- en: '![](../Images/26e0e21d0d4f52af2ce1408fdac782b5.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/26e0e21d0d4f52af2ce1408fdac782b5.png)'
- en: Comparison of top events generated by each of the models for the year 610, combined
    for all languages. Created by the author.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 比较各个模型生成的610年的主要事件，涵盖所有语言。由作者创建。
- en: '**1848**'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '**1848**'
- en: '![](../Images/b3164ce280f81262f74494130fae7cd0.png)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b3164ce280f81262f74494130fae7cd0.png)'
- en: Comparison of top events generated by each of the models for the year 1848,
    combined for all languages. Created by the author.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 比较各个模型生成的1848年的主要事件，涵盖所有语言。由作者创建。
- en: '**1910**'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '**1910**'
- en: '![](../Images/14fbcb84e3d551e09c22d190ba74fede.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/14fbcb84e3d551e09c22d190ba74fede.png)'
- en: Comparison of top events generated by each of the models for the year 1910,
    combined for all languages. Created by the author.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 比较各个模型生成的1910年的主要事件，涵盖所有语言。由作者创建。
- en: Discussion
  id: totrans-102
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 讨论
- en: So why does this all matter?
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，这一切为什么重要呢？
- en: As educational companies increasingly incorporate Large Language Models (LLMs)
    into their products — Duolingo leveraging GPT-4 for language learning, [Khan Academy
    introducing AI teaching assistant ‘Khanmigo’](https://support.khanacademy.org/hc/en-us/articles/14394953976333--Update-Introducing-Khanmigo-Khan-Academy-s-AI-Tool),
    and [Harvard University planning to integrate AI into their computer science curriculum](https://www.thecrimson.com/article/2023/6/21/cs50-artificial-intelligence/)
    — understanding the underlying biases of these models becomes crucial. If a student
    uses an LLM to learn history, what biases might they inadvertently absorb?
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 随着教育公司越来越多地将大型语言模型（LLM）纳入其产品——例如，Duolingo利用GPT-4进行语言学习，[Khan Academy推出AI教学助手‘Khanmigo’](https://support.khanacademy.org/hc/en-us/articles/14394953976333--Update-Introducing-Khanmigo-Khan-Academy-s-AI-Tool)，以及[哈佛大学计划将AI整合到计算机科学课程中](https://www.thecrimson.com/article/2023/6/21/cs50-artificial-intelligence/)——了解这些模型的潜在偏见变得至关重要。如果学生使用LLM学习历史，他们可能会无意中吸收哪些偏见？
- en: In this article, I showed that some popular language models, such as GPT-4,
    consistently predict “important events” regardless of the prompt language. Other
    models, like Claude, showed more language-specific predictions. Closed-source
    models generally exhibited greater consistency and accuracy than the leading open-source
    alternative. Across all of the models tested in this article, there was a tendency
    to predict Western or American events (even arcane events) at the expense of other
    global events.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我展示了一些流行的语言模型，例如GPT-4，无论提示语言是什么，都会一致地预测“重要事件”。其他模型，如Claude，显示出更多语言特定的预测。封闭源模型通常表现出比领先的开源替代品更高的一致性和准确性。在本文测试的所有模型中，普遍存在倾向于预测西方或美国事件（即使是冷僻事件），而忽视其他全球事件。
- en: 'Future work could include:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 未来的工作可能包括：
- en: Expanding the analysis to encompass more languages and years
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 扩展分析以涵盖更多语言和年份
- en: Doing a deeper analysis into the *historical accuracy* of model outputs
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对*模型输出的历史准确性*进行更深入的分析
- en: Doing a deeper analysis into the *ranking* of top historical events
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对*历史事件排名*进行更深入的分析
- en: Developing a more objective method for event normalization
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 发展更客观的事件标准化方法
- en: The aim of this article was not to discredit LLMs or suggest their removal from
    educational settings. Rather, I would like to urge a critical and cautious approach,
    one that recognizes and mitigates their biases. LLMs, when used responsibly, can
    be valuable resources for both students and teachers across disciplines. However,
    we must also comprehend the biases they may carry, such as Western-centrism, and
    tailor their use accordingly.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 这篇文章的目的是不是为了抨击大型语言模型（LLMs）或建议将其从教育环境中移除。相反，我希望提出一种批判性和谨慎的方法，认识到并减轻这些模型的偏见。大型语言模型在负责任的使用下，可以成为学生和教师在各学科中的宝贵资源。然而，我们也必须理解它们可能存在的偏见，例如西方中心主义，并相应地调整它们的使用。
- en: Replacing your history professor or textbooks with an LLM risks yielding a distorted,
    one-sided interpretation of history. Ultimately, we must utilize these tools thoughtfully,
    cognizant of their inherent biases, ensuring they augment rather than dictate
    our understanding of the world.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 用大型语言模型替代你的历史教授或教科书，可能会导致对历史的扭曲和片面解读。最终，我们必须审慎地利用这些工具，意识到它们固有的偏见，确保它们是补充而非主导我们对世界的理解。
- en: '*Thank you for reading!*'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '*感谢阅读！*'
- en: '*This article was o*[*riginally published on my blog*](https://blog.yenniejun.com/p/world-history-through-ai)*:
    feel free to follow to stay up to date with other writing!*'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '*这篇文章*[*最初发布在我的博客上*](https://blog.yenniejun.com/p/world-history-through-ai)*：欢迎关注以获取最新的其他写作内容！*'
- en: Bloopers
  id: totrans-115
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 失误
- en: I tried out a few different open source models. Below are a few bloopers (all
    in Korean) of the strange outputs I found the models generating!
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 我尝试了几个不同的开源模型。以下是一些我发现模型生成的奇怪输出（全部是韩语）中的几处失误！
- en: Falcon 40B Instruct
  id: totrans-117
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Falcon 40B 指令
- en: '![](../Images/34e3ecb6d56ba6461c47c4dce3f4174c.png)![](../Images/d4d4c0db55789a6dfbe98a4977731f0e.png)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/34e3ecb6d56ba6461c47c4dce3f4174c.png)![](../Images/d4d4c0db55789a6dfbe98a4977731f0e.png)'
- en: Pythia 12B
  id: totrans-119
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Pythia 12B
- en: The model seems to have gotten stuck in a loop consisting of Kangaroo, Air Mail,
    and variations of торговать (which means trade in Russian).
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型似乎陷入了一个由袋鼠、航空邮件以及 торговать（俄语中意为贸易）的变体组成的循环中。
- en: '![](../Images/d20809c74c58effffd72f27e68317db1.png)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d20809c74c58effffd72f27e68317db1.png)'
- en: Translations
  id: totrans-122
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 翻译
- en: '[PRE0]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
