- en: Serve Large Language Models from Your Computer with Text Generation Inference
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从你的计算机上使用文本生成推理来服务大型语言模型
- en: 原文：[https://towardsdatascience.com/serve-large-language-models-from-your-computer-with-text-generation-inference-54f4dd8783a7?source=collection_archive---------7-----------------------#2023-07-13](https://towardsdatascience.com/serve-large-language-models-from-your-computer-with-text-generation-inference-54f4dd8783a7?source=collection_archive---------7-----------------------#2023-07-13)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/serve-large-language-models-from-your-computer-with-text-generation-inference-54f4dd8783a7?source=collection_archive---------7-----------------------#2023-07-13](https://towardsdatascience.com/serve-large-language-models-from-your-computer-with-text-generation-inference-54f4dd8783a7?source=collection_archive---------7-----------------------#2023-07-13)
- en: Examples with the instruct version of Falcon-7B
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 Falcon-7B 的指令版本的示例
- en: '[](https://medium.com/@bnjmn_marie?source=post_page-----54f4dd8783a7--------------------------------)[![Benjamin
    Marie](../Images/3ea1ad230cb1e67610418a8e36a5e5dd.png)](https://medium.com/@bnjmn_marie?source=post_page-----54f4dd8783a7--------------------------------)[](https://towardsdatascience.com/?source=post_page-----54f4dd8783a7--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----54f4dd8783a7--------------------------------)
    [Benjamin Marie](https://medium.com/@bnjmn_marie?source=post_page-----54f4dd8783a7--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@bnjmn_marie?source=post_page-----54f4dd8783a7--------------------------------)[![Benjamin
    Marie](../Images/3ea1ad230cb1e67610418a8e36a5e5dd.png)](https://medium.com/@bnjmn_marie?source=post_page-----54f4dd8783a7--------------------------------)[](https://towardsdatascience.com/?source=post_page-----54f4dd8783a7--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----54f4dd8783a7--------------------------------)
    [Benjamin Marie](https://medium.com/@bnjmn_marie?source=post_page-----54f4dd8783a7--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fad2a414578b3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fserve-large-language-models-from-your-computer-with-text-generation-inference-54f4dd8783a7&user=Benjamin+Marie&userId=ad2a414578b3&source=post_page-ad2a414578b3----54f4dd8783a7---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----54f4dd8783a7--------------------------------)
    ·6 min read·Jul 13, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F54f4dd8783a7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fserve-large-language-models-from-your-computer-with-text-generation-inference-54f4dd8783a7&user=Benjamin+Marie&userId=ad2a414578b3&source=-----54f4dd8783a7---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fad2a414578b3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fserve-large-language-models-from-your-computer-with-text-generation-inference-54f4dd8783a7&user=Benjamin+Marie&userId=ad2a414578b3&source=post_page-ad2a414578b3----54f4dd8783a7---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----54f4dd8783a7--------------------------------)
    ·6 分钟阅读·2023年7月13日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F54f4dd8783a7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fserve-large-language-models-from-your-computer-with-text-generation-inference-54f4dd8783a7&user=Benjamin+Marie&userId=ad2a414578b3&source=-----54f4dd8783a7---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F54f4dd8783a7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fserve-large-language-models-from-your-computer-with-text-generation-inference-54f4dd8783a7&source=-----54f4dd8783a7---------------------bookmark_footer-----------)![](../Images/41e6b63543c42e19cc0b1e4ee23f95bf.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F54f4dd8783a7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fserve-large-language-models-from-your-computer-with-text-generation-inference-54f4dd8783a7&source=-----54f4dd8783a7---------------------bookmark_footer-----------)![](../Images/41e6b63543c42e19cc0b1e4ee23f95bf.png)'
- en: Photo by [Nana Dua](https://unsplash.com/@nanadua11?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 照片由 [Nana Dua](https://unsplash.com/@nanadua11?utm_source=medium&utm_medium=referral)
    提供，来源于 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: Running very large language models (LLM) locally, on consumer hardware, is now
    possible thanks to quantization methods such as [QLoRa](/qlora-fine-tune-a-large-language-model-on-your-gpu-27bed5a03e2b)
    and [GPTQ](https://github.com/IST-DASLab/gptq).
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 由于量化方法如[QLoRa](/qlora-fine-tune-a-large-language-model-on-your-gpu-27bed5a03e2b)和[GPTQ](https://github.com/IST-DASLab/gptq)，现在可以在消费者硬件上本地运行非常大的语言模型（LLM）。
- en: Considering how long it takes to load an LLM, we may also want to keep the LLM
    in memory to query it and have the results instantly. If you use LLMs with a standard
    inference pipeline, you must reload the model each time. If the model is very
    large, you may have to wait several minutes for the model to generate an output.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到加载LLM所需的时间，我们可能还希望将LLM保存在内存中，以便即时查询并获得结果。如果你使用的是标准推理管道，你必须每次都重新加载模型。如果模型非常大，你可能需要等待几分钟才能生成输出。
- en: There are various frameworks that can host LLMs on a server (locally or remotely).
    On my blog, I have already presented the [Triton Inference Server which is a very
    optimized framework](https://medium.com/towards-data-science/deploy-your-local-gpt-server-with-triton-a825d528aa5d),
    developed by NVIDIA, to serve multiple LLMs and balance the load across GPUs.
    But if you have only one GPU and if you want to host your model on your computer,
    using a Triton inference may seem unsuitable.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 有多种框架可以在服务器上托管LLMs（本地或远程）。在我的博客上，我已经介绍了由NVIDIA开发的[极为优化的Triton推理服务器](https://medium.com/towards-data-science/deploy-your-local-gpt-server-with-triton-a825d528aa5d)，它能够服务多个LLMs并在GPU之间平衡负载。但如果你只有一张GPU，并且想要在你的计算机上托管模型，使用Triton推理可能显得不太合适。
- en: In this article, I present an alternative called Text Generation Inference.
    A more straightforward framework that implements all the minimum features to run
    and serve LLMs on consumer hardware.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我介绍了一种名为Text Generation Inference的替代方案。一个更为简洁的框架，实现了运行和服务LLMs所需的所有基本功能，适用于消费级硬件。
- en: After reading this article, you will have on your computer a chat model/LLM
    deployed locally and…
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 阅读完这篇文章后，你将会在你的计算机上本地部署一个聊天模型/LLM，并且……
