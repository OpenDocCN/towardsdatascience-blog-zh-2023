- en: How Does a Decision Tree Know the Next Best Question to Ask from the Data?
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å†³ç­–æ ‘å¦‚ä½•çŸ¥é“ä»æ•°æ®ä¸­è¯¢é—®ä¸‹ä¸€ä¸ªæœ€ä½³é—®é¢˜ï¼Ÿ
- en: åŸæ–‡ï¼š[https://towardsdatascience.com/how-does-a-decision-tree-know-the-next-best-question-to-ask-from-the-data-0d44c9433b06](https://towardsdatascience.com/how-does-a-decision-tree-know-the-next-best-question-to-ask-from-the-data-0d44c9433b06)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡ï¼š[https://towardsdatascience.com/how-does-a-decision-tree-know-the-next-best-question-to-ask-from-the-data-0d44c9433b06](https://towardsdatascience.com/how-does-a-decision-tree-know-the-next-best-question-to-ask-from-the-data-0d44c9433b06)
- en: Build your own decision tree classifier (from scratch in Python) and understand
    how it uses entropy to split a node
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä»é›¶å¼€å§‹æ„å»ºä½ è‡ªå·±çš„å†³ç­–æ ‘åˆ†ç±»å™¨ï¼ˆä½¿ç”¨Pythonï¼‰ï¼Œå¹¶ç†è§£å®ƒå¦‚ä½•ä½¿ç”¨ç†µæ¥åˆ†è£‚èŠ‚ç‚¹
- en: '[](https://medium.com/@gurjinderkaur95?source=post_page-----0d44c9433b06--------------------------------)[![Gurjinder
    Kaur](../Images/d5c6746466025dad06077b1a89a789d1.png)](https://medium.com/@gurjinderkaur95?source=post_page-----0d44c9433b06--------------------------------)[](https://towardsdatascience.com/?source=post_page-----0d44c9433b06--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----0d44c9433b06--------------------------------)
    [Gurjinder Kaur](https://medium.com/@gurjinderkaur95?source=post_page-----0d44c9433b06--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@gurjinderkaur95?source=post_page-----0d44c9433b06--------------------------------)[![Gurjinder
    Kaur](../Images/d5c6746466025dad06077b1a89a789d1.png)](https://medium.com/@gurjinderkaur95?source=post_page-----0d44c9433b06--------------------------------)[](https://towardsdatascience.com/?source=post_page-----0d44c9433b06--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----0d44c9433b06--------------------------------)
    [Gurjinder Kaur](https://medium.com/@gurjinderkaur95?source=post_page-----0d44c9433b06--------------------------------)'
- en: Â·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----0d44c9433b06--------------------------------)
    Â·14 min readÂ·Nov 17, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: Â·å‘å¸ƒäº[Towards Data Science](https://towardsdatascience.com/?source=post_page-----0d44c9433b06--------------------------------)
    Â·14åˆ†é’Ÿé˜…è¯»Â·2023å¹´11æœˆ17æ—¥
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/4f9f5b9174a45c2bfe56d92a8282fcbc.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4f9f5b9174a45c2bfe56d92a8282fcbc.png)'
- en: Photo by [Daniele Levis Pelusi](https://unsplash.com/@yogidan2012?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ç‰‡ç”±[Daniele Levis Pelusi](https://unsplash.com/@yogidan2012?utm_source=medium&utm_medium=referral)æ‹æ‘„ï¼Œæ¥è‡ª[Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: Introduction
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä»‹ç»
- en: Decision trees are versatile machine learning algorithms that can perform both
    classification and regression tasks. They make decisions by asking questions about
    the data based on its features, using an IF-ELSE structure to follow a path, that
    ultimately leads to the final prediction. The challenge is to find out what question
    to ask at each step of the decision-making process, which is also equivalent to
    asking how to determine the best split at each decision node.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: å†³ç­–æ ‘æ˜¯å¤šç”¨é€”çš„æœºå™¨å­¦ä¹ ç®—æ³•ï¼Œèƒ½å¤Ÿæ‰§è¡Œåˆ†ç±»å’Œå›å½’ä»»åŠ¡ã€‚å®ƒä»¬é€šè¿‡è¯¢é—®æœ‰å…³æ•°æ®çš„ç‰¹å¾çš„é—®é¢˜ï¼Œä½¿ç”¨IF-ELSEç»“æ„æ¥è·Ÿéšè·¯å¾„ï¼Œæœ€ç»ˆå¾—å‡ºé¢„æµ‹ç»“æœã€‚æŒ‘æˆ˜åœ¨äºç¡®å®šåœ¨æ¯ä¸€æ­¥å†³ç­–è¿‡ç¨‹ä¸­éœ€è¦è¯¢é—®ä»€ä¹ˆé—®é¢˜ï¼Œè¿™ä¹Ÿç­‰åŒäºç¡®å®šåœ¨æ¯ä¸ªå†³ç­–èŠ‚ç‚¹ä¸Šæœ€ä½³åˆ†è£‚çš„æ–¹å¼ã€‚
- en: In this article, we will attempt to build a decision tree for a simple binary
    classification task. The objective of this article is to understand how an impurity
    measure (*e.g. entropy*) is used at each node to determine the best split, eventually
    constructing a tree-like structure that uses a rule-based approach to get to the
    final prediction.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘ä»¬å°†å°è¯•æ„å»ºä¸€ä¸ªç”¨äºç®€å•äºŒåˆ†ç±»ä»»åŠ¡çš„å†³ç­–æ ‘ã€‚æœ¬æ–‡çš„ç›®çš„æ˜¯ç†è§£åœ¨æ¯ä¸ªèŠ‚ç‚¹ä¸Šå¦‚ä½•ä½¿ç”¨ä¸çº¯åº¦åº¦é‡ï¼ˆ*ä¾‹å¦‚ç†µ*ï¼‰æ¥ç¡®å®šæœ€ä½³åˆ†è£‚ï¼Œæœ€ç»ˆæ„å»ºä¸€ä¸ªåŸºäºè§„åˆ™çš„æ ‘çŠ¶ç»“æ„ä»¥è·å¾—æœ€ç»ˆé¢„æµ‹ç»“æœã€‚
- en: To gain intuition behind entropy and gini impurity (another metric used to measure
    randomness and determine the quality of split in decision trees), quickly check
    out this [article](https://medium.com/@gurjinderkaur95/entropy-and-gini-index-c04b7452efbe).
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†è·å¾—å…³äºç†µå’ŒåŸºå°¼ä¸çº¯åº¦ï¼ˆç”¨äºæµ‹é‡éšæœºæ€§å¹¶ç¡®å®šå†³ç­–æ ‘ä¸­åˆ†è£‚è´¨é‡çš„å¦ä¸€ç§åº¦é‡ï¼‰çš„ç›´è§‰ï¼Œå¯ä»¥å¿«é€ŸæŸ¥çœ‹è¿™ç¯‡[æ–‡ç« ](https://medium.com/@gurjinderkaur95/entropy-and-gini-index-c04b7452efbe)ã€‚
- en: Problem definition and data
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: é—®é¢˜å®šä¹‰å’Œæ•°æ®
- en: '**Problem:** Given its length and weight measurements, predict whether a fish
    is tuna or salmon.'
  id: totrans-13
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**é—®é¢˜ï¼š** æ ¹æ®é±¼çš„é•¿åº¦å’Œé‡é‡é¢„æµ‹å®ƒæ˜¯é‡‘æªé±¼è¿˜æ˜¯é²‘é±¼ã€‚'
- en: The challenge is to predict the *type* (target variable) of fish given its *weight*
    and *length*. This is an example of a binary classification task since there are
    two possible values of our target variable *type* i.e., *tuna* and *salmon.*
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: æŒ‘æˆ˜åœ¨äºæ ¹æ®é±¼çš„*é‡é‡*å’Œ*é•¿åº¦*é¢„æµ‹é±¼çš„*ç±»å‹*ï¼ˆç›®æ ‡å˜é‡ï¼‰ã€‚è¿™æ˜¯ä¸€ä¸ªäºŒåˆ†ç±»ä»»åŠ¡çš„ç¤ºä¾‹ï¼Œå› ä¸ºæˆ‘ä»¬çš„ç›®æ ‡å˜é‡*ç±»å‹*æœ‰ä¸¤ä¸ªå¯èƒ½çš„å€¼ï¼Œå³*tuna*å’Œ*salmon*ã€‚
- en: You can download the dataset from [here](https://github.com/gurjinderbassi/Machine-Learning/blob/main/fish.csv).
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ å¯ä»¥ä»[è¿™é‡Œ](https://github.com/gurjinderbassi/Machine-Learning/blob/main/fish.csv)ä¸‹è½½æ•°æ®é›†ã€‚
- en: Itâ€™s highly encouraged to code along as youâ€™re reading this article to get the
    maximum understanding :)
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: å¼ºçƒˆå»ºè®®ä½ åœ¨é˜…è¯»æœ¬æ–‡æ—¶è¿›è¡Œç¼–ç ï¼Œä»¥è·å¾—æœ€å¤§ç†è§£ :)
- en: Code-along prerequisites
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä»£ç è·Ÿéšå‰ææ¡ä»¶
- en: Letâ€™s make sure you have everything to get started (I bet you already do, but
    just in case).
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬ç¡®ä¿ä½ æœ‰ä¸€åˆ‡å‡†å¤‡å¥½å¼€å§‹ï¼ˆæˆ‘æ•¢æ‰“èµŒä½ å·²ç»å‡†å¤‡å¥½äº†ï¼Œä½†ä»¥é˜²ä¸‡ä¸€ï¼‰ã€‚
- en: '[***Python***](https://www.python.org/downloads/)'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[***Python***](https://www.python.org/downloads/)'
- en: Any ***code editor*** that lets you work with Python (.ipynb extension) notebooks,
    [Visual Studio Code](https://code.visualstudio.com/), [Jupyter Notebook](https://jupyter.org/install),
    and [Google Colab](https://colab.research.google.com/?utm_source=scs-index) to
    name a few.
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä»»ä½•å…è®¸ä½ ä½¿ç”¨ Python (.ipynb æ‰©å±•å) ç¬”è®°æœ¬çš„***ä»£ç ç¼–è¾‘å™¨***ï¼Œä¾‹å¦‚ [Visual Studio Code](https://code.visualstudio.com/)ã€[Jupyter
    Notebook](https://jupyter.org/install) å’Œ [Google Colab](https://colab.research.google.com/?utm_source=scs-index)
    ç­‰ç­‰ã€‚
- en: '***Libraries*:** [pandas](https://pandas.pydata.org/docs/getting_started/install.html)
    and [numpy](https://numpy.org/install/) for data manipulation; [plotly](https://plotly.com/python/getting-started/)
    for visualization. (Feel free to use any other data viz library of your choice
    if you want).'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***åº“*:** [pandas](https://pandas.pydata.org/docs/getting_started/install.html)
    å’Œ [numpy](https://numpy.org/install/) ç”¨äºæ•°æ®å¤„ç†ï¼›[plotly](https://plotly.com/python/getting-started/)
    ç”¨äºå¯è§†åŒ–ã€‚ï¼ˆå¦‚æœä½ æ„¿æ„ï¼Œå¯ä»¥ä½¿ç”¨ä»»ä½•å…¶ä»–çš„æ•°æ®å¯è§†åŒ–åº“ã€‚ï¼‰'
- en: '[***Data***](https://github.com/gurjinderbassi/Machine-Learning/blob/main/fish.csv),
    of course.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[***æ•°æ®***](https://github.com/gurjinderbassi/Machine-Learning/blob/main/fish.csv)ï¼Œå½“ç„¶ã€‚'
- en: Thatâ€™s all we need, and most probably you are already good to go. Now letâ€™s
    start coding!
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å°±æ˜¯æˆ‘ä»¬éœ€è¦çš„ä¸€åˆ‡ï¼Œå¾ˆå¯èƒ½ä½ å·²ç»å‡†å¤‡å¥½äº†ã€‚ç°åœ¨è®©æˆ‘ä»¬å¼€å§‹ç¼–ç å§ï¼
- en: A step-by-step solution
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä¸€æ­¥æ­¥è§£å†³æ–¹æ¡ˆ
- en: '**Create a new .ipynb file and import the libraries first.**'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '**åˆ›å»ºä¸€ä¸ªæ–°çš„ .ipynb æ–‡ä»¶å¹¶é¦–å…ˆå¯¼å…¥åº“ã€‚**'
- en: '[PRE0]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '**Read the data into a pandas data frame.**'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '**å°†æ•°æ®è¯»å…¥ pandas æ•°æ®æ¡†ã€‚**'
- en: '[PRE1]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '`Cell Output:`'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '`å•å…ƒè¾“å‡ºï¼š`'
- en: '![](../Images/0e8d1c7256af0d8151f6de75bdcf5596.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0e8d1c7256af0d8151f6de75bdcf5596.png)'
- en: There are 1000 rows and 3 columns in our data frame. â€˜lengthâ€™ and â€˜weightâ€™ are
    the features and â€˜typeâ€™ is the target variable. Since the â€˜typeâ€™ column has values
    â€” â€˜tunaâ€™ and â€˜salmonâ€™, letâ€™s label encode them.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬çš„æ•°æ®æ¡†ä¸­æœ‰ 1000 è¡Œå’Œ 3 åˆ—ã€‚â€˜lengthâ€™ å’Œ â€˜weightâ€™ æ˜¯ç‰¹å¾ï¼Œâ€˜typeâ€™ æ˜¯ç›®æ ‡å˜é‡ã€‚ç”±äºâ€˜typeâ€™ åˆ—æœ‰å€¼â€”â€”â€˜tunaâ€™
    å’Œ â€˜salmonâ€™ï¼Œè®©æˆ‘ä»¬å¯¹å®ƒä»¬è¿›è¡Œæ ‡ç­¾ç¼–ç ã€‚
- en: '**Label encoding our target column:**'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '**å¯¹ç›®æ ‡åˆ—è¿›è¡Œæ ‡ç­¾ç¼–ç ï¼š**'
- en: '[PRE2]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'We have labeled our classes as: `{salmon: 0 and tuna: 1}`'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 'æˆ‘ä»¬å°†ç±»åˆ«æ ‡è®°ä¸ºï¼š`{salmon: 0 å’Œ tuna: 1}`'
- en: Now, letâ€™s **plot a scatter plot to visualize our classes.**
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ï¼Œè®©æˆ‘ä»¬**ç»˜åˆ¶ä¸€ä¸ªæ•£ç‚¹å›¾æ¥å¯è§†åŒ–æˆ‘ä»¬çš„ç±»åˆ«ã€‚**
- en: '[PRE3]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '`Cell Output:`'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '`å•å…ƒè¾“å‡ºï¼š`'
- en: '![](../Images/de8ddf4c7f4f68e80ce4cfa22beea6a4.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/de8ddf4c7f4f68e80ce4cfa22beea6a4.png)'
- en: Image by Author
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ç‰‡ç”±ä½œè€…æä¾›
- en: We can now clearly see our two classes marked in red and blue colors. This was
    all about data preparation, letâ€™s get into the decision tree now. Assign the feature
    column names to a list that we will use later.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ç°åœ¨å¯ä»¥æ¸…æ¥šåœ°çœ‹åˆ°ç”¨çº¢è‰²å’Œè“è‰²æ ‡è®°çš„ä¸¤ä¸ªç±»åˆ«ã€‚è¿™ä¸€åˆ‡éƒ½æ˜¯å…³äºæ•°æ®å‡†å¤‡çš„ï¼Œè®©æˆ‘ä»¬è¿›å…¥å†³ç­–æ ‘çš„éƒ¨åˆ†ã€‚å°†ç‰¹å¾åˆ—åç§°åˆ†é…ç»™ä¸€ä¸ªåˆ—è¡¨ï¼Œæˆ‘ä»¬ç¨åä¼šä½¿ç”¨åˆ°ã€‚
- en: '[PRE4]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '**Finding our first split**'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '**æ‰¾åˆ°æˆ‘ä»¬çš„ç¬¬ä¸€ä¸ªåˆ’åˆ†**'
- en: A **split** at a node in decision tree refers to the (feature, value) pair that
    divides the data into two (or more) partitions.
  id: totrans-43
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åœ¨å†³ç­–æ ‘ä¸­çš„**åˆ’åˆ†**æŒ‡çš„æ˜¯å°†æ•°æ®åˆ†ä¸ºä¸¤ä¸ªï¼ˆæˆ–æ›´å¤šï¼‰åˆ†åŒºçš„ (ç‰¹å¾, å€¼) å¯¹ã€‚
- en: ''
  id: totrans-44
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: In case of numerical feature, the split will cause two partitions of data â€”
    one with **feature â‰¤ value** and the other with **feature > value**.
  id: totrans-45
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åœ¨æ•°å€¼ç‰¹å¾çš„æƒ…å†µä¸‹ï¼Œåˆ’åˆ†å°†å¯¼è‡´æ•°æ®çš„ä¸¤ä¸ªåˆ†åŒºâ€”â€”ä¸€ä¸ªæ˜¯**ç‰¹å¾ â‰¤ å€¼**ï¼Œå¦ä¸€ä¸ªæ˜¯**ç‰¹å¾ > å€¼**ã€‚
- en: ''
  id: totrans-46
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: In case of categorical feature, the split will cause two partitions of data
    â€” one with **feature=value** and the other with **feature not equal value**.
  id: totrans-47
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åœ¨åˆ†ç±»ç‰¹å¾çš„æƒ…å†µä¸‹ï¼Œåˆ’åˆ†å°†å¯¼è‡´æ•°æ®çš„ä¸¤ä¸ªåˆ†åŒºâ€”â€”ä¸€ä¸ªæ˜¯**ç‰¹å¾=å€¼**ï¼Œå¦ä¸€ä¸ªæ˜¯**ç‰¹å¾ä¸ç­‰äºå€¼**ã€‚
- en: '[PRE5]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Running this cell will produce an error because we havenâ€™t defined the function
    `compute_impurity` yet. We need this function to compare the impurity in data
    before making the split and after making the split. The (feature, value) pair
    that results in the lowest impurity after splitting will be chosen as the best
    split at the current node and we will update the *best_params* accordingly.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: è¿è¡Œæ­¤å•å…ƒå°†äº§ç”Ÿé”™è¯¯ï¼Œå› ä¸ºæˆ‘ä»¬è¿˜æ²¡æœ‰å®šä¹‰å‡½æ•° `compute_impurity`ã€‚æˆ‘ä»¬éœ€è¦è¿™ä¸ªå‡½æ•°æ¥æ¯”è¾ƒåœ¨åˆ’åˆ†ä¹‹å‰å’Œä¹‹åæ•°æ®çš„æ‚è´¨ã€‚ç»“æœåœ¨åˆ’åˆ†åæ‚è´¨æœ€ä½çš„
    (ç‰¹å¾, å€¼) å¯¹å°†è¢«é€‰ä¸ºå½“å‰èŠ‚ç‚¹çš„æœ€ä½³åˆ’åˆ†ï¼Œæˆ‘ä»¬å°†ç›¸åº”åœ°æ›´æ–°*best_params*ã€‚
- en: 'Define the function as follows:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: å®šä¹‰å‡½æ•°å¦‚ä¸‹ï¼š
- en: '[PRE6]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'This function uses another helper function `compute_entropy` that gives us
    the entropy of a given list of classes. Letâ€™s define this as well:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤å‡½æ•°ä½¿ç”¨å¦ä¸€ä¸ªè¾…åŠ©å‡½æ•° `compute_entropy`ï¼Œå®ƒä¸ºç»™å®šç±»åˆ«åˆ—è¡¨æä¾›ç†µã€‚è®©æˆ‘ä»¬ä¹Ÿå®šä¹‰è¿™ä¸ªå‡½æ•°ï¼š
- en: '[PRE7]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Now as both of our helper functions are defined, run the cell again which previously
    led to an error, it should run successfully now. Print the *best_params* dictionary
    to check if it got updated or not.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬å·²ç»å®šä¹‰äº†ä¸¤ä¸ªè¾…åŠ©å‡½æ•°ï¼Œå†æ¬¡è¿è¡Œä¹‹å‰å¯¼è‡´é”™è¯¯çš„å•å…ƒæ ¼ï¼Œç°åœ¨åº”è¯¥å¯ä»¥æˆåŠŸè¿è¡Œäº†ã€‚æ‰“å° *best_params* å­—å…¸ä»¥æ£€æŸ¥å®ƒæ˜¯å¦å·²æ›´æ–°ã€‚
- en: '[PRE8]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '`Cell Output:`'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '`å•å…ƒæ ¼è¾“å‡ºï¼š`'
- en: '![](../Images/989ee647c08e6b66d676165d687d6ea0.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/989ee647c08e6b66d676165d687d6ea0.png)'
- en: Voila! We got our first best split at `length = 3` which means we will split
    our data into two partitions now â€” `data['length']<=3` and `data['length']>3.`
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: å®Œç¾ï¼æˆ‘ä»¬åœ¨ `length = 3` å¤„å¾—åˆ°äº†ç¬¬ä¸€ä¸ªæœ€ä½³åˆ†å‰²ï¼Œè¿™æ„å‘³ç€æˆ‘ä»¬ç°åœ¨å°†æ•°æ®åˆ†ä¸ºä¸¤ä¸ªåˆ†åŒºâ€”â€”`data['length']<=3` å’Œ `data['length']>3`ã€‚
- en: '***Note:*** *Here we are going with the split that results in the minimum entropy.
    Decision trees can vary in terms of this split criterion, for example,* ***ID3
    uses******Information Gain*** *(which is the difference in entropy of data before
    split and the weighted sum of entropy of branches after split), and* ***CART uses
    the Gini index*** *as their respective split criteria.*'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '***æ³¨æ„ï¼š*** *åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬é€‰æ‹©ç»“æœç†µæœ€å°çš„åˆ†å‰²ã€‚å†³ç­–æ ‘å¯ä»¥åœ¨æ­¤åˆ†å‰²æ ‡å‡†ä¸Šæœ‰æ‰€ä¸åŒï¼Œä¾‹å¦‚ï¼Œ* ***ID3 ä½¿ç”¨******ä¿¡æ¯å¢ç›Š*** *(å³åˆ†å‰²å‰æ•°æ®çš„ç†µä¸åˆ†å‰²ååˆ†æ”¯ç†µåŠ æƒå’Œçš„å·®å€¼)ï¼Œè€Œ*
    ***CART ä½¿ç”¨åŸºå°¼æŒ‡æ•°*** *ä½œä¸ºå®ƒä»¬å„è‡ªçš„åˆ†å‰²æ ‡å‡†ã€‚*'
- en: Following is how our decision tree looks right now. Since the data at the left
    branch consists of a single class, we will make it a leaf node; so any data point
    with `length<=3` will be predicted as *tuna.*
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸‹é¢æ˜¯æˆ‘ä»¬å½“å‰å†³ç­–æ ‘çš„æ ·å­ã€‚ç”±äºå·¦ä¾§åˆ†æ”¯çš„æ•°æ®åªåŒ…å«ä¸€ä¸ªç±»åˆ«ï¼Œæˆ‘ä»¬å°†å…¶è®¾ä¸ºå¶èŠ‚ç‚¹ï¼›å› æ­¤ï¼Œä»»ä½• `length<=3` çš„æ•°æ®ç‚¹éƒ½å°†è¢«é¢„æµ‹ä¸º *é‡‘æªé±¼*ã€‚
- en: '(***Reminder:*** according to our color_map, we have assigned blue color to
    tuna and red color to salmon):'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: (***æé†’ï¼š*** æ ¹æ®æˆ‘ä»¬çš„ color_mapï¼Œæˆ‘ä»¬å°†è“è‰²åˆ†é…ç»™é‡‘æªé±¼ï¼Œå°†çº¢è‰²åˆ†é…ç»™é²‘é±¼)ï¼š
- en: '[PRE9]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: For the data at the right branch, we can recursively follow the same process
    and find the best split, repeat for subsequent branches until we reach a maximum
    depth or no further splitting is possible.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºå³ä¾§åˆ†æ”¯çš„æ•°æ®ï¼Œæˆ‘ä»¬å¯ä»¥é€’å½’åœ°éµå¾ªç›¸åŒçš„è¿‡ç¨‹ï¼Œæ‰¾åˆ°æœ€ä½³åˆ†å‰²ï¼Œå¹¶å¯¹åç»­åˆ†æ”¯é‡å¤æ­¤è¿‡ç¨‹ï¼Œç›´åˆ°è¾¾åˆ°æœ€å¤§æ·±åº¦æˆ–ä¸å†å¯èƒ½è¿›è¡Œè¿›ä¸€æ­¥åˆ†å‰²ã€‚
- en: '![](../Images/358412cd61839e3cf75d0089b273a6d1.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/358412cd61839e3cf75d0089b273a6d1.png)'
- en: Visualizing decision tree after making first split (Image by Author)
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ç¬¬ä¸€æ¬¡åˆ†å‰²åå¯è§†åŒ–å†³ç­–æ ‘ï¼ˆå›¾åƒä½œè€…æä¾›ï¼‰
- en: This process then gets repeated for each of the partitions until a stopping
    condition is met (such as the maximum depth of the tree is reached, or the number
    of samples in a leaf node is below a threshold, etc.). This is what *hyperparameters*
    allow us to define when we use classifiers or regressors from packages such as
    scikit-learn.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªè¿‡ç¨‹ä¼šå¯¹æ¯ä¸ªåˆ†åŒºé‡å¤ï¼Œç›´åˆ°æ»¡è¶³åœæ­¢æ¡ä»¶ï¼ˆä¾‹å¦‚ï¼Œæ ‘çš„æœ€å¤§æ·±åº¦è¾¾åˆ°ï¼Œæˆ–è€…å¶èŠ‚ç‚¹ä¸­çš„æ ·æœ¬æ•°é‡ä½äºé˜ˆå€¼ç­‰ï¼‰ã€‚è¿™å°±æ˜¯å½“æˆ‘ä»¬ä½¿ç”¨å¦‚ scikit-learn ç­‰åŒ…ä¸­çš„åˆ†ç±»å™¨æˆ–å›å½’å™¨æ—¶ï¼Œ*è¶…å‚æ•°*
    å…è®¸æˆ‘ä»¬å®šä¹‰çš„å†…å®¹ã€‚
- en: '**Generalize the code using recursion**'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '**ä½¿ç”¨é€’å½’æ¥æ³›åŒ–ä»£ç **'
- en: We will wrap the above code in a function `build_tree`that can be called recursively
    to build the decision tree.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†æŠŠä¸Šè¿°ä»£ç å°è£…åˆ°ä¸€ä¸ªå‡½æ•° `build_tree` ä¸­ï¼Œè¯¥å‡½æ•°å¯ä»¥é€’å½’è°ƒç”¨æ¥æ„å»ºå†³ç­–æ ‘ã€‚
- en: '**Helper functions:**'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '**è¾…åŠ©å‡½æ•°ï¼š**'
- en: '`compute_entropy:` Returns the entropy of a dataset with two classes. Defined
    above.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '`compute_entropy:` è¿”å›å…·æœ‰ä¸¤ä¸ªç±»åˆ«çš„æ•°æ®é›†çš„ç†µã€‚ä¸Šè¿°å®šä¹‰ã€‚'
- en: '`compute_gini:` Returns the gini index of a dataset with two classes. We can
    choose between entropy and gini index as our impurity measures.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '`compute_gini:` è¿”å›å…·æœ‰ä¸¤ä¸ªç±»åˆ«çš„æ•°æ®é›†çš„åŸºå°¼æŒ‡æ•°ã€‚æˆ‘ä»¬å¯ä»¥é€‰æ‹©ä½¿ç”¨ç†µæˆ–åŸºå°¼æŒ‡æ•°ä½œä¸ºæˆ‘ä»¬çš„ impurity è¡¡é‡æ ‡å‡†ã€‚'
- en: '[PRE10]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '`compute_impurity:` An extension of compute_impurity function defined above,
    it returns the *impurity* of the dataset. It uses `compute_entropy` and `compute_gini`
    functions to calculate the entropy and the Gini index at the given split point,
    according to the criterion specified.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '`compute_impurity:` æ˜¯ä¸Šè¿° compute_impurity å‡½æ•°çš„æ‰©å±•ï¼Œå®ƒè¿”å›æ•°æ®é›†çš„ *impurity*ã€‚å®ƒä½¿ç”¨ `compute_entropy`
    å’Œ `compute_gini` å‡½æ•°æ ¹æ®æŒ‡å®šçš„æ ‡å‡†è®¡ç®—ç»™å®šåˆ†å‰²ç‚¹çš„ç†µå’ŒåŸºå°¼æŒ‡æ•°ã€‚'
- en: '[PRE11]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '`get_best_params:` Returns the *best_params* dictionary containing the feature
    and value to use for split at the current node.'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '`get_best_params:` è¿”å›åŒ…å«å½“å‰èŠ‚ç‚¹åˆ†å‰²æ‰€ç”¨ç‰¹å¾å’Œå€¼çš„ *best_params* å­—å…¸ã€‚'
- en: '[PRE12]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '**Main function**'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '**ä¸»å‡½æ•°**'
- en: '`build_tree:` It is the main driver function that makes use of helper functions
    to build the decision tree recursively for the given data. I have also added additional
    statements in an attempt to print the decision tree in an interpretable format
    as it is created.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '`build_tree:` è¿™æ˜¯ä¸»è¦çš„é©±åŠ¨å‡½æ•°ï¼Œå®ƒåˆ©ç”¨è¾…åŠ©å‡½æ•°é€’å½’åœ°ä¸ºç»™å®šçš„æ•°æ®æ„å»ºå†³ç­–æ ‘ã€‚æˆ‘è¿˜æ·»åŠ äº†ä¸€äº›é¢å¤–çš„è¯­å¥ï¼Œè¯•å›¾åœ¨åˆ›å»ºè¿‡ç¨‹ä¸­ä»¥å¯è§£é‡Šçš„æ ¼å¼æ‰“å°å†³ç­–æ ‘ã€‚'
- en: '[PRE13]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Now, we can pass our fish dataset and test the output of our code as follows.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ï¼Œæˆ‘ä»¬å¯ä»¥ä¼ é€’æˆ‘ä»¬çš„é±¼æ•°æ®é›†å¹¶æµ‹è¯•ä»£ç çš„è¾“å‡ºï¼Œå¦‚ä¸‹æ‰€ç¤ºã€‚
- en: '[PRE14]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '`Cell Output:`'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '`å•å…ƒæ ¼è¾“å‡ºï¼š`'
- en: '![](../Images/1a6cdb5c5b8311c3319f4849f90e4075.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1a6cdb5c5b8311c3319f4849f90e4075.png)'
- en: 'Following is what our final decision tree looks like:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: ä»¥ä¸‹æ˜¯æˆ‘ä»¬æœ€ç»ˆå†³ç­–æ ‘çš„æ ·å­ï¼š
- en: '![](../Images/7f2c327a26b137d97f75628fa60f6327.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7f2c327a26b137d97f75628fa60f6327.png)'
- en: Image by Author
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ç‰‡æ¥æºï¼šä½œè€…
- en: 'This corresponds to the following decision boundaries:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å¯¹åº”äºä»¥ä¸‹å†³ç­–è¾¹ç•Œï¼š
- en: '![](../Images/868c6d0d1b4dbae53e9e139286993b98.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/868c6d0d1b4dbae53e9e139286993b98.png)'
- en: Image by Author
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ç‰‡æ¥æºï¼šä½œè€…
- en: '**Note:** What happens if the leaf node is not pure i.e., there is more than
    one class in a leaf node partition? *Simply go for the majority class.*'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '**æ³¨æ„ï¼š** å¦‚æœå¶èŠ‚ç‚¹ä¸æ˜¯çº¯å‡€çš„ï¼Œå³å¶èŠ‚ç‚¹åˆ†åŒºä¸­æœ‰å¤šä¸ªç±»åˆ«ï¼Œä¼šå‘ç”Ÿä»€ä¹ˆï¼Ÿ*åªéœ€é€‰æ‹©å¤šæ•°ç±»åˆ«ã€‚*'
- en: Link to Code
  id: totrans-91
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä»£ç é“¾æ¥
- en: You can get the final code notebook from [here.](https://github.com/gurjinderbassi/Machine-Learning/blob/main/Decision%20Tree%20-%20Fish%20dataset.ipynb)
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ å¯ä»¥ä»[è¿™é‡Œ](https://github.com/gurjinderbassi/Machine-Learning/blob/main/Decision%20Tree%20-%20Fish%20dataset.ipynb)è·å–æœ€ç»ˆçš„ä»£ç ç¬”è®°æœ¬ã€‚
- en: Takeaways
  id: totrans-93
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ä¸»è¦æ”¶è·
- en: If youâ€™ve come so far, you will now be much more comfortable making sense out
    of a lot of important things related to decision trees such as *interpretability*
    being an advantage, and *overfitting* being a top disadvantage â€” that you might
    already come across during your previous encounter with decision trees. And it
    will be unfair if we leave out these topics here, so touching briefly on them
    below.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœä½ å·²ç»è¯»åˆ°è¿™é‡Œï¼Œä½ ä¼šå¯¹è®¸å¤šä¸å†³ç­–æ ‘ç›¸å…³çš„é‡è¦é—®é¢˜æœ‰æ›´æ¸…æ™°çš„è®¤è¯†ï¼Œæ¯”å¦‚*å¯è§£é‡Šæ€§*ä½œä¸ºä¼˜ç‚¹ï¼Œä»¥åŠ*è¿‡æ‹Ÿåˆ*ä½œä¸ºä¸»è¦ç¼ºç‚¹â€”â€”ä½ å¯èƒ½åœ¨ä¹‹å‰æ¥è§¦å†³ç­–æ ‘æ—¶å·²ç»é‡åˆ°è¿‡ã€‚æˆ‘ä»¬ä¸èƒ½åœ¨è¿™é‡Œå¿½ç•¥è¿™äº›è¯é¢˜ï¼Œæ‰€ä»¥ç®€å•æåŠä¸€ä¸‹ã€‚
- en: Letâ€™s first look at the advantages. There are more, but we are sticking with
    the most important ones.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬å…ˆæ¥çœ‹çœ‹ä¼˜ç‚¹ã€‚è¿˜æœ‰æ›´å¤šï¼Œä½†æˆ‘ä»¬åªåˆ—å‡ºæœ€é‡è¦çš„å‡ ä¸ªã€‚
- en: What are the (top) advantages of a decision tree?
  id: totrans-96
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å†³ç­–æ ‘çš„ï¼ˆä¸»è¦ï¼‰ä¼˜ç‚¹æ˜¯ä»€ä¹ˆï¼Ÿ
- en: '***Interpretability****:* A decision tree prediction is easier to interpret
    as compared to other machine learning models since we can take a visual look at
    the path that was followed to get to the final prediction.'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***å¯è§£é‡Šæ€§ï¼š*** å†³ç­–æ ‘çš„é¢„æµ‹ç›¸è¾ƒäºå…¶ä»–æœºå™¨å­¦ä¹ æ¨¡å‹æ›´æ˜“äºè§£é‡Šï¼Œå› ä¸ºæˆ‘ä»¬å¯ä»¥ç›´è§‚åœ°æŸ¥çœ‹è¾¾åˆ°æœ€ç»ˆé¢„æµ‹æ‰€éµå¾ªçš„è·¯å¾„ã€‚'
- en: A decision tree is intuitive, and can be explained easily, even to a non-technical
    person.
  id: totrans-98
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: å†³ç­–æ ‘ç›´è§‚ä¸”æ˜“äºè§£é‡Šï¼Œå³ä½¿å¯¹éæŠ€æœ¯äººå‘˜ä¹Ÿæ˜¯å¦‚æ­¤ã€‚
- en: For example, letâ€™s say a bank is using a decision tree to predict whether it
    should grant a loan to a customer based on their attributes such as income, bank
    balance, age, profession, etc. If the classification system suggests that the
    bank should not grant a loan to a customer, then the bank needs to draft a proper
    response stating the reasons for rejection. Otherwise, it can harm their business
    and reputation.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: ä¾‹å¦‚ï¼Œå‡è®¾ä¸€å®¶é“¶è¡Œä½¿ç”¨å†³ç­–æ ‘é¢„æµ‹æ˜¯å¦åº”æ ¹æ®å®¢æˆ·çš„å±æ€§ï¼ˆå¦‚æ”¶å…¥ã€é“¶è¡Œä½™é¢ã€å¹´é¾„ã€èŒä¸šç­‰ï¼‰å‘å®¢æˆ·æˆäºˆè´·æ¬¾ã€‚å¦‚æœåˆ†ç±»ç³»ç»Ÿå»ºè®®é“¶è¡Œä¸åº”å‘å®¢æˆ·æä¾›è´·æ¬¾ï¼Œé‚£ä¹ˆé“¶è¡Œéœ€è¦åˆ¶å®šé€‚å½“çš„å›åº”ï¼Œè¯´æ˜æ‹’ç»çš„ç†ç”±ã€‚å¦åˆ™ï¼Œè¿™å¯èƒ½ä¼šæŸå®³ä»–ä»¬çš„ä¸šåŠ¡å’Œå£°èª‰ã€‚
- en: '***No need for heavy-duty preprocessing:*** Decision trees donâ€™t expect the
    data to be normalized (or standardized) as opposed to some other machine learning
    models.'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***æ— éœ€é‡åº¦é¢„å¤„ç†ï¼š*** ä¸å…¶ä»–ä¸€äº›æœºå™¨å­¦ä¹ æ¨¡å‹ä¸åŒï¼Œå†³ç­–æ ‘ä¸è¦æ±‚æ•°æ®è¿›è¡Œå½’ä¸€åŒ–ï¼ˆæˆ–æ ‡å‡†åŒ–ï¼‰ã€‚'
- en: You do minimal data pre-processing and the decision tree wonâ€™t mind much.
  id: totrans-101
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ä½ åªéœ€è¿›è¡Œæœ€å°‘çš„æ•°æ®é¢„å¤„ç†ï¼Œå†³ç­–æ ‘ä¹Ÿä¸ä¼šå¤ªåœ¨æ„ã€‚
- en: Moreover, it can intrinsically handle categorical features and we donâ€™t need
    to worry about one-hot encoding (or other solutions) as distinct categories will
    simply be considered as different branches during a split.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤å¤–ï¼Œå®ƒå¯ä»¥è‡ªç„¶åœ°å¤„ç†åˆ†ç±»ç‰¹å¾ï¼Œæˆ‘ä»¬ä¸éœ€è¦æ‹…å¿ƒç‹¬çƒ­ç¼–ç ï¼ˆæˆ–å…¶ä»–è§£å†³æ–¹æ¡ˆï¼‰ï¼Œå› ä¸ºä¸åŒçš„ç±»åˆ«åœ¨åˆ†è£‚æ—¶ä¼šè¢«è§†ä¸ºä¸åŒçš„åˆ†æ”¯ã€‚
- en: And that major drawback â€”> Overfitting
  id: totrans-103
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä¸»è¦ç¼ºç‚¹â€”â€”> è¿‡æ‹Ÿåˆ
- en: Overfitting is when our model is toooo good to be real i.e., the model adapts
    to the training data so closely that it loses its ability to generalize and fails
    to show similar level of accuracy when presented with test data.
  id: totrans-104
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: è¿‡æ‹Ÿåˆæ˜¯æŒ‡æˆ‘ä»¬çš„æ¨¡å‹å¥½å¾—ä¸çœŸå®ï¼Œå³æ¨¡å‹å¯¹è®­ç»ƒæ•°æ®çš„é€‚åº”è¿‡äºç´§å¯†ï¼Œä»¥è‡³äºä¸§å¤±äº†æ³›åŒ–èƒ½åŠ›ï¼Œæ— æ³•åœ¨é¢å¯¹æµ‹è¯•æ•°æ®æ—¶è¡¨ç°å‡ºç±»ä¼¼çš„å‡†ç¡®åº¦ã€‚
- en: '**Decision trees when not controlled properly are highly prone to overfitting.**
    Notice in our example above that if we keep splitting the training data without
    defining any limit then the decision tree would keep creating more decision boundaries,
    learning the noise in training data.'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '**å†³ç­–æ ‘å¦‚æœæ§åˆ¶ä¸å½“ï¼Œéå¸¸å®¹æ˜“è¿‡æ‹Ÿåˆã€‚** æ³¨æ„åœ¨æˆ‘ä»¬ä¸Šè¿°çš„ä¾‹å­ä¸­ï¼Œå¦‚æœæˆ‘ä»¬ä¸æ–­åœ°åˆ†è£‚è®­ç»ƒæ•°æ®è€Œä¸è®¾å®šä»»ä½•é™åˆ¶ï¼Œé‚£ä¹ˆå†³ç­–æ ‘å°†ä¸æ–­åˆ›å»ºæ›´å¤šçš„å†³ç­–è¾¹ç•Œï¼Œå­¦ä¹ è®­ç»ƒæ•°æ®ä¸­çš„å™ªå£°ã€‚'
- en: In order to retain the modelâ€™s generalizability property, itâ€™s important to
    follow measures to avoid overfitting. In the case of decision trees, we can take
    the following **steps to prevent overfitting:**
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†ä¿æŒæ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œé‡‡å–æªæ–½é¿å…è¿‡æ‹Ÿåˆæ˜¯éå¸¸é‡è¦çš„ã€‚åœ¨å†³ç­–æ ‘çš„æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬å¯ä»¥é‡‡å–ä»¥ä¸‹**é˜²æ­¢è¿‡æ‹Ÿåˆçš„æ­¥éª¤ï¼š**
- en: '*Pre-pruning:* Pruning refers to preventing the decision tree from growing
    at its full capacity. It can be done proactively by:'
  id: totrans-107
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*é¢„å‰ªæï¼š* å‰ªææŒ‡çš„æ˜¯é˜²æ­¢å†³ç­–æ ‘è¾¾åˆ°å…¶æœ€å¤§å®¹é‡ã€‚å¯ä»¥é€šè¿‡ä»¥ä¸‹æ–¹å¼ä¸»åŠ¨è¿›è¡Œï¼š'
- en: Setting *max_depth:* Donâ€™t allow the tree to grow beyond a pre-defined depth.
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è®¾ç½®*max_depth:* ä¸å…è®¸æ ‘çš„æ·±åº¦è¶…è¿‡é¢„å®šä¹‰çš„æ·±åº¦ã€‚
- en: Setting *min_samples_split:* Donâ€™t allow the split to happen if the number of
    samples is below this value at a decision node
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è®¾ç½®*min_samples_split:* å¦‚æœå†³ç­–èŠ‚ç‚¹å¤„çš„æ ·æœ¬æ•°é‡ä½äºæ­¤å€¼ï¼Œåˆ™ä¸å…è®¸åˆ†è£‚å‘ç”Ÿã€‚
- en: Setting *min_samples_leaf:* Donâ€™t allow the split to happen if the number of
    samples at any of the resulting leaf nodes is lower than this value.
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è®¾ç½®*min_samples_leaf:* å¦‚æœä»»ä½•ç»“æœå¶èŠ‚ç‚¹å¤„çš„æ ·æœ¬æ•°é‡ä½äºæ­¤å€¼ï¼Œåˆ™ä¸å…è®¸åˆ†è£‚å‘ç”Ÿã€‚
- en: These (plus many others) are the ***hyperparameters*** that we can tune as per
    our needs when we implement decision trees via packages such as scikit-learn.
    You can find all the hyperparameters and their definitions in this [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html).
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™äº›ï¼ˆä»¥åŠè®¸å¤šå…¶ä»–çš„ï¼‰æ˜¯æˆ‘ä»¬åœ¨é€šè¿‡è¯¸å¦‚ scikit-learn è¿™æ ·çš„åŒ…å®ç°å†³ç­–æ ‘æ—¶å¯ä»¥æ ¹æ®éœ€è¦è°ƒæ•´çš„***è¶…å‚æ•°***ã€‚ä½ å¯ä»¥åœ¨è¿™ä¸ª[æ–‡æ¡£](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html)ä¸­æ‰¾åˆ°æ‰€æœ‰è¶…å‚æ•°åŠå…¶å®šä¹‰ã€‚
- en: '*2\. Post-pruning:* It refers to letting the decision tree grow at its full
    capacity and then discarding some sections/branches afterward that seem unnecessary
    or are leading to high variance.'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '*2\. åå‰ªæï¼š* æŒ‡çš„æ˜¯è®©å†³ç­–æ ‘åœ¨å…¶æœ€å¤§å®¹é‡ä¸‹ç”Ÿé•¿ï¼Œç„¶åä¸¢å¼ƒä¸€äº›çœ‹èµ·æ¥ä¸å¿…è¦æˆ–å¯¼è‡´é«˜æ–¹å·®çš„éƒ¨åˆ†/åˆ†æ”¯ã€‚'
- en: '3\. Another possible solution is: *donâ€™t use decision trees!* But rather opt
    for their advanced versions â€” such as *random forests or gradient-boosted trees
    :)* which still requires you to have basic knowledge of their predecessors, so
    reading this article is not a waste of time at all!'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 3\. å¦ä¸€ç§å¯èƒ½çš„è§£å†³æ–¹æ¡ˆæ˜¯ï¼š*ä¸è¦ä½¿ç”¨å†³ç­–æ ‘ï¼* è€Œæ˜¯é€‰æ‹©å®ƒä»¬çš„é«˜çº§ç‰ˆæœ¬â€”â€”æ¯”å¦‚*éšæœºæ£®æ—æˆ–æ¢¯åº¦æå‡æ ‘ :)* è¿™ä»ç„¶éœ€è¦ä½ å¯¹å‰è€…æœ‰åŸºæœ¬äº†è§£ï¼Œå› æ­¤é˜…è¯»è¿™ç¯‡æ–‡ç« ç»å¯¹ä¸ä¼šæµªè´¹æ—¶é—´ï¼
- en: Bonus Points
  id: totrans-114
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å¥–åŠ±ç§¯åˆ†
- en: 'There are some other properties of decision trees that are worth noting:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: è¿˜æœ‰ä¸€äº›å†³ç­–æ ‘çš„å…¶ä»–ç‰¹æ€§å€¼å¾—æ³¨æ„ï¼š
- en: '**Non-parametric**: Decision trees are non-parametric machine learning models,
    which means that they do not make assumptions about the training data related
    to its distribution, independence of features, etc.'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**éå‚æ•°**ï¼šå†³ç­–æ ‘æ˜¯éå‚æ•°æœºå™¨å­¦ä¹ æ¨¡å‹ï¼Œè¿™æ„å‘³ç€å®ƒä»¬å¯¹è®­ç»ƒæ•°æ®çš„åˆ†å¸ƒã€ç‰¹å¾çš„ç‹¬ç«‹æ€§ç­‰ä¸åšå‡è®¾ã€‚'
- en: '**Greedy approach**: Decision trees follow a greedy approach, which means that
    they opt for the split that they think is the best at the given node (*i.e.*,
    locally optimal solution) and cannot backtrack, leading to a sub-optimal solution.'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**è´ªå¿ƒç®—æ³•**ï¼šå†³ç­–æ ‘éµå¾ªè´ªå¿ƒç®—æ³•ï¼Œè¿™æ„å‘³ç€å®ƒä»¬é€‰æ‹©åœ¨ç»™å®šèŠ‚ç‚¹å¤„è®¤ä¸ºæœ€å¥½çš„åˆ†è£‚ï¼ˆ*å³*ï¼Œå±€éƒ¨æœ€ä¼˜è§£ï¼‰ï¼Œä¸”æ— æ³•å›æº¯ï¼Œä»è€Œå¯èƒ½å¯¼è‡´æ¬¡ä¼˜è§£ã€‚'
- en: Conclusion
  id: totrans-118
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç»“è®º
- en: In this article, we learned to build a decision tree for a binary classification
    task without making use of any packages to get strong at the conceptual level.
    We went through a step-by-step process to understand how a decision rule is generated
    at each node using data impurity measures such as entropy, and then later implemented
    a recursive algorithm in Python to output the final decision tree. The goal of
    this article was to get the fundamentals of the decision tree by looking under
    the hood.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘ä»¬å­¦ä¹ äº†å¦‚ä½•åœ¨æ²¡æœ‰ä½¿ç”¨ä»»ä½•åŒ…çš„æƒ…å†µä¸‹ä¸ºäºŒåˆ†ç±»ä»»åŠ¡æ„å»ºå†³ç­–æ ‘ï¼Œä»¥åœ¨æ¦‚å¿µå±‚é¢ä¸Šæ‰“ä¸‹åšå®çš„åŸºç¡€ã€‚æˆ‘ä»¬é€šè¿‡é€æ­¥è¿‡ç¨‹ç†è§£äº†å¦‚ä½•ä½¿ç”¨æ•°æ®ä¸çº¯åº¦åº¦é‡ï¼ˆå¦‚ç†µï¼‰åœ¨æ¯ä¸ªèŠ‚ç‚¹ç”Ÿæˆå†³ç­–è§„åˆ™ï¼Œç„¶ååœ¨
    Python ä¸­å®ç°äº†é€’å½’ç®—æ³•ä»¥è¾“å‡ºæœ€ç»ˆçš„å†³ç­–æ ‘ã€‚è¿™ç¯‡æ–‡ç« çš„ç›®çš„æ˜¯é€šè¿‡æ·±å…¥äº†è§£å†³ç­–æ ‘çš„åŸºæœ¬åŸç†ã€‚
- en: In practice, however, when dealing with real-life data and challenges at hand,
    we will never have the need to do this from scratch as there are numerous packages
    available that make things far more convenient and robust for us. But having a
    strong background is going to help us better utilize those packages, and we will
    be more confident while leveraging them.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: å®é™…ä¸Šï¼Œå½“å¤„ç†ç°å®ä¸–ç•Œçš„æ•°æ®å’Œé¢ä¸´æŒ‘æˆ˜æ—¶ï¼Œæˆ‘ä»¬é€šå¸¸ä¸éœ€è¦ä»é›¶å¼€å§‹åšè¿™äº›å·¥ä½œï¼Œå› ä¸ºæœ‰è®¸å¤šç°æˆçš„åŒ…å¯ä»¥ä½¿äº‹æƒ…å˜å¾—æ›´æ–¹ä¾¿å’Œç¨³å¥ã€‚ä½†æ‹¥æœ‰æ‰å®çš„åŸºç¡€ä¼šå¸®åŠ©æˆ‘ä»¬æ›´å¥½åœ°åˆ©ç”¨è¿™äº›åŒ…ï¼ŒåŒæ—¶åœ¨ä½¿ç”¨æ—¶ä¹Ÿä¼šæ›´æœ‰ä¿¡å¿ƒã€‚
- en: Hopefully, next time we go on to build our next Decision Tree or Random Forest
    (which is an ensemble of multiple decision trees), we will be more thoughtful
    while configuring our models (and there will be less struggle to understand what
    a hyperparameter really means ğŸ˜º).
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: å¸Œæœ›ä¸‹æ¬¡å½“æˆ‘ä»¬æ„å»ºä¸‹ä¸€ä¸ªå†³ç­–æ ‘æˆ–éšæœºæ£®æ—ï¼ˆè¿™æ˜¯ä¸€ç»„å†³ç­–æ ‘çš„é›†æˆï¼‰æ—¶ï¼Œæˆ‘ä»¬èƒ½åœ¨é…ç½®æ¨¡å‹æ—¶æ›´åŠ å‘¨åˆ°ï¼ˆå¹¶ä¸”æ›´å®¹æ˜“ç†è§£è¶…å‚æ•°çš„çœŸæ­£å«ä¹‰ ğŸ˜ºï¼‰ã€‚
- en: I hope this was helpful. Open to any feedback or suggestions.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: å¸Œæœ›è¿™äº›å†…å®¹å¯¹ä½ æœ‰æ‰€å¸®åŠ©ã€‚æ¬¢è¿æä¾›ä»»ä½•åé¦ˆæˆ–å»ºè®®ã€‚
- en: Iâ€™d like to acknowledge [Ritvik Kharkar](https://medium.com/u/ddca178f703?source=post_page-----0d44c9433b06--------------------------------)
    for his amazing YouTube video that helped me better understand decision trees
    conceptually. Iâ€™ve taken inspiration from his video to write this article, using
    the same example he used, and taking the solution a step ahead by adding recursive
    implementation and logic to print the decision tree. The link to his video is
    in the references below.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘æƒ³æ„Ÿè°¢ [Ritvik Kharkar](https://medium.com/u/ddca178f703?source=post_page-----0d44c9433b06--------------------------------)
    çš„ç²¾å½© YouTube è§†é¢‘ï¼Œå¸®åŠ©æˆ‘æ›´å¥½åœ°ç†è§£äº†å†³ç­–æ ‘çš„æ¦‚å¿µã€‚æˆ‘ä»ä»–çš„å½•åƒä¸­è·å¾—äº†çµæ„Ÿï¼Œå†™äº†è¿™ç¯‡æ–‡ç« ï¼Œä½¿ç”¨äº†ä»–ç”¨çš„ç›¸åŒä¾‹å­ï¼Œå¹¶é€šè¿‡æ·»åŠ é€’å½’å®ç°å’Œæ‰“å°å†³ç­–æ ‘çš„é€»è¾‘å°†è§£å†³æ–¹æ¡ˆæ¨è¿›äº†ä¸€æ­¥ã€‚ä»–çš„è§†é¢‘é“¾æ¥åœ¨ä¸‹é¢çš„å‚è€ƒæ–‡çŒ®ä¸­ã€‚
- en: '**Related Reading**'
  id: totrans-124
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**ç›¸å…³é˜…è¯»**'
- en: '**Want to get the intuition behind impurity measures?** Check out this article
    related to Entropy and Gini index:'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**æƒ³è¦ç†è§£ impurity measures èƒŒåçš„ç›´è§‰ï¼Ÿ** è¯·æŸ¥çœ‹è¿™ç¯‡å…³äºç†µå’ŒåŸºå°¼æŒ‡æ•°çš„æ–‡ç« ï¼š'
- en: '[](https://medium.com/@gurjinderkaur95/entropy-and-gini-index-c04b7452efbe?source=post_page-----0d44c9433b06--------------------------------)
    [## Entropy and Gini Index'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@gurjinderkaur95/entropy-and-gini-index-c04b7452efbe?source=post_page-----0d44c9433b06--------------------------------)
    [## ç†µå’ŒåŸºå°¼æŒ‡æ•°'
- en: Understanding how these measures help us quantify uncertainty in a dataset
  id: totrans-127
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ç†è§£è¿™äº›æŒ‡æ ‡å¦‚ä½•å¸®åŠ©æˆ‘ä»¬é‡åŒ–æ•°æ®é›†ä¸­çš„ä¸ç¡®å®šæ€§
- en: medium.com](https://medium.com/@gurjinderkaur95/entropy-and-gini-index-c04b7452efbe?source=post_page-----0d44c9433b06--------------------------------)
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: medium.com](https://medium.com/@gurjinderkaur95/entropy-and-gini-index-c04b7452efbe?source=post_page-----0d44c9433b06--------------------------------)
- en: '**How to evaluate a decision tree classifier?** Check out the article below
    to learn about different evaluation metrics for classification models:'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**å¦‚ä½•è¯„ä¼°å†³ç­–æ ‘åˆ†ç±»å™¨ï¼Ÿ** è¯·æŸ¥çœ‹ä¸‹é¢çš„æ–‡ç« ï¼Œäº†è§£ä¸åŒçš„åˆ†ç±»æ¨¡å‹è¯„ä»·æŒ‡æ ‡ï¼š'
- en: '[](https://medium.com/@gurjinderkaur95/evaluation-metrics-for-classification-beyond-accuracy-1e20d8c76ba0?source=post_page-----0d44c9433b06--------------------------------)
    [## Evaluation Metrics for Classification- beyond Accuracy'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@gurjinderkaur95/evaluation-metrics-for-classification-beyond-accuracy-1e20d8c76ba0?source=post_page-----0d44c9433b06--------------------------------)
    [## åˆ†ç±»çš„è¯„ä»·æŒ‡æ ‡ - è¶…è¶Šå‡†ç¡®ç‡'
- en: Unfolding Confusion Matrix, Precision, Recall, F1 Score, and ROC Curve
  id: totrans-131
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: å±•å¼€æ··æ·†çŸ©é˜µã€ç²¾å‡†ç‡ã€å¬å›ç‡ã€F1 åˆ†æ•°å’Œ ROC æ›²çº¿
- en: medium.com](https://medium.com/@gurjinderkaur95/evaluation-metrics-for-classification-beyond-accuracy-1e20d8c76ba0?source=post_page-----0d44c9433b06--------------------------------)
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: medium.com](https://medium.com/@gurjinderkaur95/evaluation-metrics-for-classification-beyond-accuracy-1e20d8c76ba0?source=post_page-----0d44c9433b06--------------------------------)
- en: References
  id: totrans-133
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å‚è€ƒæ–‡çŒ®
- en: '[1] AurÃ©lien GÃ©ron, (2019). *Hands-on machine learning with Scikit-Learn, Keras
    and TensorFlow: concepts, tools, and techniques to build intelligent systems*
    (2nd ed.). Oâ€™Reilly.'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] AurÃ©lien GÃ©ron, (2019). *Hands-on machine learning with Scikit-Learn, Keras
    and TensorFlow: concepts, tools, and techniques to build intelligent systems*
    (ç¬¬2ç‰ˆ). Oâ€™Reilly.'
- en: '[2] ritvikmathâ€™s YouTube [video](https://youtu.be/dCez6oGZilY?si=slDWXQG5ZJgSv36W)'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] ritvikmath çš„ YouTube [è§†é¢‘](https://youtu.be/dCez6oGZilY?si=slDWXQG5ZJgSv36W)'
- en: '[3] [StatQuest](https://www.youtube.com/watch?v=_L39rN6gz7Y&ab_channel=StatQuestwithJoshStarmer)'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] [StatQuest](https://www.youtube.com/watch?v=_L39rN6gz7Y&ab_channel=StatQuestwithJoshStarmer)'
