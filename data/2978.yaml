- en: 'Making the Right Decisions: AI Advice, Decision Aids, and the Promise of LLMs'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 做出正确决策：AI 建议、决策辅助工具以及大语言模型的前景
- en: 原文：[https://towardsdatascience.com/making-the-right-decisions-ai-advice-decision-aids-and-the-promise-of-llms-65b70682ee08?source=collection_archive---------9-----------------------#2023-09-28](https://towardsdatascience.com/making-the-right-decisions-ai-advice-decision-aids-and-the-promise-of-llms-65b70682ee08?source=collection_archive---------9-----------------------#2023-09-28)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/making-the-right-decisions-ai-advice-decision-aids-and-the-promise-of-llms-65b70682ee08?source=collection_archive---------9-----------------------#2023-09-28](https://towardsdatascience.com/making-the-right-decisions-ai-advice-decision-aids-and-the-promise-of-llms-65b70682ee08?source=collection_archive---------9-----------------------#2023-09-28)
- en: Exploring the new dawn of decision-making with LLMs
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 探索大语言模型带来的决策新时代
- en: '[](https://medium.com/@ujwal07?source=post_page-----65b70682ee08--------------------------------)[![Ujwal
    Gadiraju](../Images/ee7345cf2e7fbf6ee29866659b60b18e.png)](https://medium.com/@ujwal07?source=post_page-----65b70682ee08--------------------------------)[](https://towardsdatascience.com/?source=post_page-----65b70682ee08--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----65b70682ee08--------------------------------)
    [Ujwal Gadiraju](https://medium.com/@ujwal07?source=post_page-----65b70682ee08--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@ujwal07?source=post_page-----65b70682ee08--------------------------------)[![乌贾瓦尔·加迪拉朱](../Images/ee7345cf2e7fbf6ee29866659b60b18e.png)](https://medium.com/@ujwal07?source=post_page-----65b70682ee08--------------------------------)[](https://towardsdatascience.com/?source=post_page-----65b70682ee08--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----65b70682ee08--------------------------------)
    [乌贾瓦尔·加迪拉朱](https://medium.com/@ujwal07?source=post_page-----65b70682ee08--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fc529f92bfe0d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmaking-the-right-decisions-ai-advice-decision-aids-and-the-promise-of-llms-65b70682ee08&user=Ujwal+Gadiraju&userId=c529f92bfe0d&source=post_page-c529f92bfe0d----65b70682ee08---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----65b70682ee08--------------------------------)
    ·10 min read·Sep 28, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F65b70682ee08&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmaking-the-right-decisions-ai-advice-decision-aids-and-the-promise-of-llms-65b70682ee08&user=Ujwal+Gadiraju&userId=c529f92bfe0d&source=-----65b70682ee08---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fc529f92bfe0d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmaking-the-right-decisions-ai-advice-decision-aids-and-the-promise-of-llms-65b70682ee08&user=Ujwal+Gadiraju&userId=c529f92bfe0d&source=post_page-c529f92bfe0d----65b70682ee08---------------------post_header-----------)
    发表在[Towards Data Science](https://towardsdatascience.com/?source=post_page-----65b70682ee08--------------------------------)
    · 10 分钟阅读 · 2023年9月28日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F65b70682ee08&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmaking-the-right-decisions-ai-advice-decision-aids-and-the-promise-of-llms-65b70682ee08&user=Ujwal+Gadiraju&userId=c529f92bfe0d&source=-----65b70682ee08---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F65b70682ee08&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmaking-the-right-decisions-ai-advice-decision-aids-and-the-promise-of-llms-65b70682ee08&source=-----65b70682ee08---------------------bookmark_footer-----------)![](../Images/087d06c143f7cd0a5b5f62c7bc6176c7.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F65b70682ee08&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmaking-the-right-decisions-ai-advice-decision-aids-and-the-promise-of-llms-65b70682ee08&source=-----65b70682ee08---------------------bookmark_footer-----------)![](../Images/087d06c143f7cd0a5b5f62c7bc6176c7.png)'
- en: Photo by [Robert Ruggiero](https://unsplash.com/@robert2301?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
    on [Unsplash](https://unsplash.com/photos/X2bcQMMhaow?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 照片由[罗伯特·鲁吉耶罗](https://unsplash.com/@robert2301?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)拍摄，发布在[Unsplash](https://unsplash.com/photos/X2bcQMMhaow?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
- en: Introduction
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: The democratization of AI has led to the adoption of AI systems in a variety
    of domains. The recent wave of generative models, such as pre-trained large language
    models (LLMs), has led to their widespread use in different activities in our
    daily lives — from increasing productivity by aiding the framing of emails to
    helping solve the dreaded “blank page” obstacle for novice and expert writers
    alike. Due to the increasing reliance on LLMs to aid decision-making, this article
    presents a synthesis of human decision-making and the evolution of human-AI decision-making.
    Finally, the article reflects on the opportunities that LLMs offer for aiding
    decision-making tasks and the concomitant threats of reliance on LLMs for decision-making.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 人工智能的民主化导致了AI系统在各种领域的采用。最近一波生成模型，例如预训练的大型语言模型（LLMs），已广泛应用于我们日常生活中的不同活动——从通过帮助撰写邮件提高生产力，到帮助解决令新手和专家作家都感到困扰的“空白页”难题。由于对LLMs在决策中的依赖日益增加，本文提供了人类决策和人类与AI决策演变的综合分析。最后，文章反思了LLMs在辅助决策任务中提供的机会以及对LLMs决策依赖的相关威胁。
- en: Human Decision-Making
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 人类决策
- en: In a world that is characterized by a growing spectrum of choices with respect
    to nearly every single decision we encounter in our daily lives (*e.g.*, food
    to buy or clothes to wear, books to read, music to listen to, or movies to watch,
    from lifestyle choices to travel destinations), the quality of decision-making
    has received renewed interest. In his influential work exposing the “*paradox
    of choice,*” Barry Schwartz articulated this growing difficulty in decision-making
    on the heels of technological advances at the stroke of the millennium [12]. Schwarz
    elucidates this with an example of a ​​doctor offering a patient an array of treatments,
    conveying the potential risks and weighing them against the benefits for each.
    In such a situation, the burden of a high-stakes decision is shifted from the
    expert doctor to the non-expert patient. Among other factors, an abundance of
    choices often tends to impede effective human decision-making.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在一个几乎每个日常决策（*例如*，购买食品或穿着衣物、阅读书籍、听音乐或观看电影，从生活方式选择到旅行目的地）的选择范围不断扩大的世界中，决策质量受到了重新关注。在他有影响力的作品中揭示“*选择悖论*”的巴里·施瓦茨（Barry
    Schwartz）阐述了这种随着千年技术进步而逐渐增加的决策困难。施瓦茨通过一个例子来解释这一点：医生向患者提供一系列治疗方案，传达潜在风险并将其与每种治疗的收益进行权衡。在这种情况下，高风险决策的负担从专家医生转移到非专家患者。除了其他因素外，选择的过多往往会阻碍有效的人类决策。
- en: Different research communities, ranging from evolutionary psychology to cognitive
    sciences and neuroscience, have explored the nature of human decision-making and
    various factors that shape decision-making processes among humans [2,10]. It is
    no secret that human decision-making is plagued by cognitive biases and punctuated
    with irrationality. This was most famously documented by Nobel prize-winning behavioral
    economist Daniel Kahneman in *Thinking, Fast, and Slow* [6]*.*
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 不同的研究社区，从进化心理学到认知科学和神经科学，已经探索了人类决策的性质以及塑造人类决策过程的各种因素。人类决策受认知偏差困扰，充满了非理性，这一点并不鲜见。这一点在诺贝尔奖获奖行为经济学家丹尼尔·卡尼曼的*《思考，快与慢》*一书中得到了最著名的记录。
- en: '![](../Images/b1c22ad1ee60a9ba1c68c1dc31d44dc6.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b1c22ad1ee60a9ba1c68c1dc31d44dc6.png)'
- en: Photo by [Robynne Hu](https://unsplash.com/@robynnexy?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
    on [Unsplash](https://unsplash.com/photos/HOrhCnQsxnQ?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 由 [Robynne Hu](https://unsplash.com/@robynnexy?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
    提供的照片，来源于 [Unsplash](https://unsplash.com/photos/HOrhCnQsxnQ?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
- en: Human-AI Decision Making
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 人类与人工智能的决策
- en: The advent of technologies has ushered in growth in decision-support systems
    that can support humans in overcoming obstacles in their decision-making processes.
    Decision-support systems take various shapes and forms in broader socio-technical
    contexts– from algorithms that power user interactions to complex machine-learning
    models that aid users with predictions and forecasting. For example, recommender
    systems can help users by presenting them with content or products likely to best
    satisfy their needs. Other algorithmic systems can mine through large volumes
    of data to offer advice to users in a plethora of decision-making tasks.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 技术的出现带来了决策支持系统的增长，这些系统可以帮助人类克服决策过程中遇到的障碍。决策支持系统在更广泛的社会技术背景下有各种形态和形式——从驱动用户交互的算法到帮助用户进行预测和预报的复杂机器学习模型。例如，推荐系统可以通过向用户展示可能最能满足其需求的内容或产品来提供帮助。其他算法系统则可以挖掘大量数据，以在各种决策任务中向用户提供建议。
- en: A central objective common to all human-AI decision-making contexts is the potential
    to improve decision-making effectiveness by combining human intelligence with
    the computational power of algorithmic systems. This, however, is far from how
    many collaborative human-AI decision-making processes unfold in the real world.
    Humans fail to rely appropriately on AI systems in decision-making tasks, leading
    to sub-optimal team performance. *Appropriate reliance* has been conceptualized
    as the reliance of humans on AI advice when it is correct and self-reliance when
    the AI is incorrect [11]. There are a number of factors that play a role in shaping
    such outcomes — human factors (e.g., domain knowledge, affinity to technology
    interaction, prior experience); system factors (e.g., accuracy or confidence of
    the AI system); task factors (e.g., task complexity, task uncertainty, stakes).
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 所有的人类与AI决策制定背景中一个共同的核心目标是通过将人类智能与算法系统的计算能力相结合，提升决策效果。然而，这与许多协作的人类与AI决策过程在现实世界中的展开方式相去甚远。人类在决策任务中未能适当地依赖AI系统，从而导致团队表现次优。*适当的依赖*被概念化为在人类依赖AI建议时AI正确时的依赖，以及AI不正确时的人类自我依赖[11]。有许多因素在塑造这些结果中发挥作用——人类因素（例如，领域知识、对技术交互的亲和力、先前经验）；系统因素（例如，AI系统的准确性或信心）；任务因素（例如，任务复杂性、任务不确定性、风险）。
- en: Empirical explorations of human-AI decision-making in various contexts, including
    loan application decisions and medical diagnosis, have revealed that humans either
    under-rely on AI advice and lose out on the opportunity to improve the outcomes
    of their decisions or over-rely on AI advice and achieve sub-optimal outcomes.
    To tackle the problem of over-reliance and under-reliance and foster appropriate
    reliance on AI advice, prior works have proposed the use of explanations [13],
    *cognitive forcing functions* (i.e., interventions that force critical consideration
    and reflection during the decision-making process) [4], tutorials or training
    sessions that communicate the strengths and weaknesses of AI systems, and initiatives
    to increase the general AI literacy of populations. Recent work has proposed an
    alternative framework called “*evaluative AI”* to promote appropriate reliance
    on AI advice. This framework suggests that decision support tools should provide
    evidence for and against decisions made by people rather than provide recommendations
    to accept or reject [7].
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 对人类与AI在各种背景下的决策制定的实证探索，包括贷款申请决策和医学诊断，已揭示出人类要么对AI建议的依赖不足，错失了改善决策结果的机会，要么对AI建议的依赖过度，从而导致次优结果。为了应对过度依赖和不足依赖的问题，并促进对AI建议的适当依赖，先前的研究提出了使用解释[13]、*认知强制功能*（即，在决策过程中强制进行关键考虑和反思的干预措施）[4]、传达AI系统优缺点的教程或培训课程，以及提高人群一般AI素养的倡议。最近的研究提出了一种名为“*评估性AI*”的替代框架，以促进对AI建议的适当依赖。该框架建议决策支持工具应提供支持和反对人们所做决策的证据，而不是提供接受或拒绝的推荐[7]。
- en: Cognitive biases have also influenced human-AI decision-making [1, 3]. Rastogi
    et al. [9] argued that our general perception and understanding of decision-making
    tasks can be distorted by cognitive biases, such as confirmation bias, anchoring
    bias, and availability bias. They explored the role of anchoring bias and proposed
    methods to mitigate their negative effects on collaborative decision-making performance.
    He et al. [22] showed that the Dunning-Kruger effect, a metacognitive bias, can
    influence how people rely on advice from AI systems. They revealed that users
    who overestimate their ability or performance tend to exhibit under-reliance on
    AI systems, hindering optimal team performance in decision-making tasks. Other
    factors, such as algorithmic aversion and appreciation, have also been shown to
    be influential in determining the fruitfulness of human-AI decision-making [17].
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 认知偏差也影响了人机决策[1, 3]。Rastogi等人[9]认为，我们对决策任务的整体感知和理解可能会受到认知偏差的扭曲，例如确认偏差、锚定偏差和可得性偏差。他们探讨了锚定偏差的作用，并提出了减少其对协作决策表现负面影响的方法。He等人[22]展示了厄尔斯效应（一种元认知偏差）如何影响人们对AI系统建议的依赖。他们揭示了那些高估自己能力或表现的用户倾向于对AI系统表现出过少的依赖，从而阻碍了决策任务中的最佳团队表现。其他因素，如算法厌恶和欣赏，也被证明对人机决策的成果有影响[17]。
- en: Despite the ongoing work in the broad realm of human-AI collaboration, fostering
    appropriate reliance on AI systems in decision-making tasks remains an unsolved
    problem. Different research communities at the intersection of Artificial Intelligence,
    Machine Learning, and Human-Computer Interaction are actively working on advancing
    our understanding of this area and developing methods, tools, and frameworks that
    can help us benefit from the potential of human-AI collaboration.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管在人机协作的广泛领域中正在进行持续的工作，但在决策任务中如何适当地依赖AI系统仍然是一个未解的问题。人工智能、机器学习和人机交互交汇的不同研究社区正积极推进我们对这一领域的理解，并开发方法、工具和框架，帮助我们从人机协作的潜力中受益。
- en: At this moment, large language models (LLMs) have found widespread application
    and adoption across domains. In the remainder of this article, we will explore
    the opportunities that LLMs provide in aiding human decision-making and intertwined
    with potential benefits.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，大型语言模型（LLMs）在各个领域得到了广泛的应用和采纳。在本文的余下部分，我们将深入探讨LLMs在辅助人类决策中提供的机会以及潜在的好处。
- en: LLMs for Decision-Making Tasks
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LLMs用于决策任务
- en: LLMs are being increasingly used in a variety of sociotechnical systems despite
    demonstrating biases and the potential to cause harm. Having said that, they have
    also shown promise to have a positive impact at scale — for instance, through
    supporting auditing processes as demonstrated by Rostagi et al. [8] via an auditing
    tool that is powered by a generative LLM. The authors proposed to leverage the
    complementary strengths of humans and generative models in the collaborative auditing
    of commercial language models. Wu et al. [14] proposed *AutoGen*, a framework
    that enables complex LLM-based workflows using multi-agent conversations. AutoGen
    can support online decision-making tasks such as game-playing or web interactions.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管大型语言模型（LLMs）表现出偏见和可能造成伤害的潜在风险，它们在各种社会技术系统中的使用仍在不断增加。话虽如此，它们也显示出了在大规模上产生积极影响的潜力——例如，通过支持审计过程，如Rostagi等人[8]所展示的，通过一个由生成型LLM驱动的审计工具。作者提议在商业语言模型的协同审计中利用人类和生成模型的互补优势。Wu等人[14]提出了*AutoGen*，一个通过多代理对话支持复杂LLM-based工作流程的框架。AutoGen可以支持在线决策任务，如游戏或网络互动。
- en: On the one hand, there is evidence that LLMs like GPT-3 exhibit behavior that
    strikingly resembles human-like intuition — and the cognitive errors that come
    with it [16]. Recent research has highlighted the feasibility of using ChatGPT
    for radiologic decision-making, potentially improving clinical workflows and the
    responsible use of radiology services [18]. To enhance AI safety in decision-making
    processes, Jin et al. [15] have aimed to replicate and empower LLMs with the ability
    to determine when a rule should be broken, especially in novel or unusual situations.
    On the other hand, LLMs can inadvertently perpetuate stereotypes toward marginalized
    groups [20] and exhibit bias involving race, gender, religion, and political orientation.
    Similar to the challenges in fostering appropriate trust and reliance among users
    in decision-support systems, if humans are to rely on LLMs for decision-making,
    we will need to better understand the benefits and pitfalls of such interactions.
    A particularly magnified risk in LLM-powered interactions is the apparent ease
    with which conversational interactions can be facilitated. Prior work has already
    uncovered the illusionary role of explanatory depth created by using explanations
    in decision-making tasks, resulting in overreliance on AI systems. If human interactions
    with decision-support systems are made even more *seamless* (for example, through
    interactive or conversational interfaces), we can expect to unearth more instances
    of inappropriate reliance.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 一方面，有证据表明，像GPT-3这样的LLMs表现出与人类直觉惊人相似的行为，以及随之而来的认知误差[16]。最近的研究强调了使用ChatGPT进行放射学决策的可行性，潜在地改善临床工作流程和放射学服务的负责任使用[18]。为了增强决策过程中的AI安全性，Jin等人[15]旨在复制并赋予LLMs在新颖或异常情况下决定是否打破规则的能力。另一方面，LLMs可能无意中加剧对边缘化群体的刻板印象[20]，并表现出涉及种族、性别、宗教和政治取向的偏见。与决策支持系统中培养适当信任和依赖的挑战类似，如果人类依赖LLMs做出决策，我们将需要更好地理解这种互动的利与弊。在LLM驱动的互动中，尤其突出的风险是通过使用解释在决策任务中创造的解释深度的假象，导致对AI系统的过度依赖。如果人类与决策支持系统的互动变得更加“无缝”（例如通过交互式或对话界面），我们可以预期会发现更多不适当依赖的实例。
- en: It is increasingly difficult to study LLMs through the lenses of their architecture
    and hyperparameters. There is sufficient evidence, at this point, to understand
    that generative AI can produce high-quality written and visual content that may
    be used for the greater good or misused to cause harm. Potsdam Mann et al. [19]
    argue that a credit–blame asymmetry arises for assigning responsibility for LLMs
    outputs and ethical and policy implications focused on LLMs.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 通过它们的架构和超参数的视角研究LLMs变得越来越困难。此时有足够的证据表明，生成式AI可以产生高质量的书面和视觉内容，这些内容可以为社会福祉服务，也可能被误用以造成伤害。Potsdam
    Mann等人[19]认为，为LLMs输出分配责任存在信誉-责备不对称性，并且伦理和政策影响集中在LLMs上。
- en: What Needs To Be Done Next?
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**接下来需要做什么？**'
- en: 'It is evident that more research and empirical work is needed to inform the
    safe and robust use of LLMs in decision-making tasks. This is particularly evident
    when considering the current limitations with respect to multimodal and multilingual
    LLMs. Here is a compilation of some questions that remain critical in determining
    the extent to which we can consistently benefit from marrying LLMs into everyday
    decision-making:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，在决策任务中安全且健壮地使用LLMs需要更多的研究和实证工作。特别是考虑到目前在多模态和多语言LLMs方面的限制。以下是一些仍然至关重要的问题汇编，用于确定我们可以从将LLMs嫁接到日常决策中中持续获益的程度：
- en: '*How can we facilitate appropriate reliance on LLMs or LLM-infused systems
    for effective decision-making?*'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*如何促进对LLMs或LLM注入系统进行有效决策支持的适当依赖？*'
- en: '*How can we increase the robustness, reliability, and trustworthiness of LLM-infused
    decision-support systems?*'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*我们如何提高LLM注入的决策支持系统的稳健性、可靠性和信任度？*'
- en: '*How can we foster appropriate trust and reliance on LLMs in multimodal and
    multilingual decision-making contexts?*'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*如何在多模态和多语言决策环境中培养对LLMs的适当信任和依赖？*'
- en: '*How can people of varying abilities, individual traits, prior knowledge, education
    and qualifications, and other demographics be equally supported by LLMs in decision-making
    tasks?*'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*如何支持不同能力、个人特征、先验知识、教育和资格以及其他人口统计学特征的人在LLMs决策任务中得到平等的支持？*'
- en: '**So, if you have an LLM at your fingertips, do not be in a hurry to rely on
    it as a black-box decision-making aid just yet!**'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '**因此，如果你手边有一个大语言模型，还是不要急于把它当作黑箱决策辅助工具来依赖！**'
- en: Dr. ir. Ujwal Gadiraju is a tenured Assistant Professor at [*Delft University
    of Technology*](https://www.tudelft.nl/en/). He co-directs the [Delft “Design@Scale”
    AI Lab](https://www.tudelft.nl/en/ai/design-at-scale-lab) and co-leads a Human-Centered
    AI and Crowd Computing research line. He is a Distinguished Speaker of the ACM
    and a board member of [*CHI Netherlands*](https://chinederland.nl/). Ujwal spends
    a part of his time working at [Toloka AI](https://toloka.ai/) with their AI, data,
    and research teams and is also an advisory board member for [Deeploy](https://www.deeploy.ml/),
    a growing MLOps company.
  id: totrans-34
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: Dr. ir. Ujwal Gadiraju 是[*代尔夫特理工大学*](https://www.tudelft.nl/en/)的终身副教授。他共同指导了[Delft
    “Design@Scale” AI实验室](https://www.tudelft.nl/en/ai/design-at-scale-lab)并领导了一个以人为本的AI与众包计算研究方向。他是ACM的杰出演讲者，也是[*CHI
    Netherlands*](https://chinederland.nl/)的董事会成员。Ujwal在[Toloka AI](https://toloka.ai/)的AI、数据和研究团队中花费部分时间工作，同时也是[Deeploy](https://www.deeploy.ml/)的顾问委员会成员，Deeploy是一家正在成长的MLOps公司。
- en: References
  id: totrans-35
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Bertrand, A., Belloum, R., Eagan, J. R., & Maxwell, W. (2022, July). *How cognitive
    biases affect XAI-assisted decision-making: A systematic review.* In Proceedings
    of the 2022 AAAI/ACM Conference on AI, Ethics, and Society (pp. 78–91).'
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Bertrand, A., Belloum, R., Eagan, J. R., & Maxwell, W. (2022年7月). *认知偏差如何影响XAI辅助决策：系统评估.*
    见于2022 AAAI/ACM AI、伦理与社会会议论文集（第78–91页）。
- en: Bossaerts, P., & Murawski, C. (2017). *Computational complexity and human decision-making.*
    Trends in cognitive sciences, 21(12), 917–929.
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Bossaerts, P., & Murawski, C. (2017). *计算复杂性与人类决策.* 认知科学趋势, 21(12), 917–929。
- en: Boonprakong, N., He, G., Gadiraju, U., van Berkel, N., Wang, D., Chen, S., Liu,
    J., Tag, B., Goncalves, J. and Dingler, T., 2023\. *Workshop on Understanding
    and Mitigating Cognitive Biases in Human-AI Collaboration.*
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Boonprakong, N., He, G., Gadiraju, U., van Berkel, N., Wang, D., Chen, S., Liu,
    J., Tag, B., Goncalves, J. 和 Dingler, T., 2023\. *关于理解和缓解人类与AI协作中的认知偏差的研讨会。*
- en: 'Buçinca, Z., Malaya, M. B., & Gajos, K. Z. (2021). *To trust or to think: cognitive
    forcing functions can reduce overreliance on AI in AI-assisted decision-making.*
    Proceedings of the ACM on Human-Computer Interaction, 5(CSCW1), 1–21.'
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Buçinca, Z., Malaya, M. B., & Gajos, K. Z. (2021). *信任还是思考：认知强制函数可以减少对AI的过度依赖.*
    ACM人机交互学报, 5(CSCW1), 1–21。
- en: Haupt, C. E., & Marks, M. (2023). *AI-generated medical advice — GPT and beyond.*
    Jama, 329(16), 1349–1350.
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Haupt, C. E., & Marks, M. (2023). *AI生成的医学建议——GPT及其超越。* Jama, 329(16), 1349–1350。
- en: Kahneman, D. (2011). *Thinking, Fast and Slow.* Macmillan.
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Kahneman, D. (2011). *思考，快与慢。* 麦克米伦。
- en: Miller, T. (2023, June). *Explainable AI is Dead, Long Live Explainable AI!
    Hypothesis-driven Decision Support using Evaluative AI.* In Proceedings of the
    2023 ACM Conference on Fairness, Accountability, and Transparency (pp. 333–342).
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Miller, T. (2023年6月). *可解释AI已死，万岁可解释AI！使用评估性AI的假设驱动决策支持.* 见于2023 ACM公平性、问责性与透明度会议论文集（第333–342页）。
- en: Rastogi, C., Tulio Ribeiro, M., King, N., Nori, H., & Amershi, S. (2023, August).
    *Supporting human-ai collaboration in auditing llms with llms.* In Proceedings
    of the 2023 AAAI/ACM Conference on AI, Ethics, and Society (pp. 913–926).
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Rastogi, C., Tulio Ribeiro, M., King, N., Nori, H., & Amershi, S. (2023年8月).
    *支持人类与AI协作的审计LLMs.* 见于2023 AAAI/ACM AI、伦理与社会会议论文集（第913–926页）。
- en: 'Rastogi, C., Zhang, Y., Wei, D., Varshney, K. R., Dhurandhar, A., & Tomsett,
    R. (2022). *Deciding fast and slow: The role of cognitive biases in ai-assisted
    decision-making.* Proceedings of the ACM on Human-Computer Interaction, 6(CSCW1),
    1–22.'
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Rastogi, C., Zhang, Y., Wei, D., Varshney, K. R., Dhurandhar, A., & Tomsett,
    R. (2022). *快速与缓慢决策：认知偏差在AI辅助决策中的作用.* ACM人机交互学报, 6(CSCW1), 1–22。
- en: Santos, L. R., & Rosati, A. G. (2015). *The evolutionary roots of human decision
    making.* Annual review of psychology, 66, 321–347.
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Santos, L. R., & Rosati, A. G. (2015). *人类决策的进化根源.* 心理学年鉴, 66, 321–347。
- en: Schemmer, M., Hemmer, P., Kühl, N., Benz, C., & Satzger, G. (2022). *Should
    I follow AI-based advice? Measuring appropriate reliance in human-AI decision-making.*
    arXiv preprint arXiv:2204.06916.
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Schemmer, M., Hemmer, P., Kühl, N., Benz, C., & Satzger, G. (2022). *我是否应该听从基于AI的建议？衡量人类与AI决策中的适当依赖性.*
    arXiv预印本 arXiv:2204.06916。
- en: 'Schwartz, B. (2004). *The paradox of choice: Why more is less.* New York.'
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Schwartz, B. (2004). *选择的悖论：为何更多更少。* 纽约。
- en: Vasconcelos, H., Jörke, M., Grunde-McLaughlin, M., Gerstenberg, T., Bernstein,
    M. S., & Krishna, R. (2023). *Explanations can reduce overreliance on ai systems
    during decision-making.* Proceedings of the ACM on Human-Computer Interaction,
    7(CSCW1), 1–38.
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Vasconcelos, H., Jörke, M., Grunde-McLaughlin, M., Gerstenberg, T., Bernstein,
    M. S., & Krishna, R. (2023). *解释可以减少在决策过程中对AI系统的过度依赖*。ACM人机交互学报，7(CSCW1)，1–38。
- en: 'Wu, Qingyun, Gagan Bansal, Jieyu Zhang, Yiran Wu, Shaokun Zhang, Erkang Zhu,
    Beibin Li, Li Jiang, Xiaoyun Zhang, and Chi Wang. *AutoGen: Enabling next-gen
    LLM applications via multi-agent conversation framework.* arXiv preprint arXiv:2308.08155
    (2023).'
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Wu, Qingyun, Gagan Bansal, Jieyu Zhang, Yiran Wu, Shaokun Zhang, Erkang Zhu,
    Beibin Li, Li Jiang, Xiaoyun Zhang, 和 Chi Wang. *AutoGen：通过多代理对话框架启用下一代LLM应用*。arXiv预印本
    arXiv:2308.08155 (2023)。
- en: 'Jin, Z., Levine, S., Gonzalez Adauto, F., Kamal, O., Sap, M., Sachan, M., Mihalcea,
    R., Tenenbaum, J. and Schölkopf, B., 2022\. *When to make exceptions: Exploring
    language models as accounts of human moral judgment.* Advances in neural information
    processing systems, 35, pp.28458–28473.'
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Jin, Z., Levine, S., Gonzalez Adauto, F., Kamal, O., Sap, M., Sachan, M., Mihalcea,
    R., Tenenbaum, J. 和 Schölkopf, B., 2022. *何时例外：将语言模型作为人类道德判断的解释*。神经信息处理系统进展，35，
    第28458–28473页。
- en: 'Hagendorff, T., Fabi, S., & Kosinski, M. (2022). *Machine intuition: Uncovering
    human-like intuitive decision-making in GPT-3.5.* arXiv preprint arXiv:2212.05206.'
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Hagendorff, T., Fabi, S., & Kosinski, M. (2022). *机器直觉：揭示GPT-3.5中的类人直观决策*。arXiv预印本
    arXiv:2212.05206。
- en: 'Erlei, A., Das, R., Meub, L., Anand, A., & Gadiraju, U. (2022, April). *For
    what it’s worth: Humans overwrite their economic self-interest to avoid bargaining
    with AI systems.* In Proceedings of the 2022 CHI Conference on Human Factors in
    Computing Systems (pp. 1–18).'
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Erlei, A., Das, R., Meub, L., Anand, A., & Gadiraju, U. (2022年4月). *值得关注的是：人类会忽略其经济自利以避免与AI系统讨价还价*。见于2022年CHI计算机系统人因会议论文集（第1–18页）。
- en: Rao, A., Kim, J., Kamineni, M., Pang, M., Lie, W., & Succi, M. D. (2023). *Evaluating
    ChatGPT as an adjunct for radiologic decision-making.* medRxiv, 2023–02.
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Rao, A., Kim, J., Kamineni, M., Pang, M., Lie, W., & Succi, M. D. (2023). *评估ChatGPT作为放射科决策的辅助工具*。medRxiv,
    2023–02。
- en: Porsdam Mann, S., Earp, B. D., Nyholm, S., Danaher, J., Møller, N., Bowman-Smart,
    H., … & Savulescu, J. (2023). *Generative AI entails a credit–blame asymmetry.*
    Nature Machine Intelligence, 1–4.
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Porsdam Mann, S., Earp, B. D., Nyholm, S., Danaher, J., Møller, N., Bowman-Smart,
    H., … & Savulescu, J. (2023). *生成式AI涉及信贷–责备不对称*。自然机器智能，1–4。
- en: 'Dhingra, H., Jayashanker, P., Moghe, S., & Strubell, E. (2023). *Queer people
    are people first: Deconstructing sexual identity stereotypes in large language
    models.* arXiv preprint arXiv:2307.00101.'
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Dhingra, H., Jayashanker, P., Moghe, S., & Strubell, E. (2023). *酷儿首先是人：解构大语言模型中的性别身份刻板印象*。arXiv预印本
    arXiv:2307.00101。
- en: 'He, G., Kuiper, L., & Gadiraju, U. (2023, April). Knowing About Knowing: An
    Illusion of Human Competence Can Hinder Appropriate Reliance on AI Systems. In
    *Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems*
    (pp. 1–18).'
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: He, G., Kuiper, L., & Gadiraju, U. (2023年4月). *了解认知：人类能力的幻觉可能阻碍对AI系统的适当依赖*。见于*2023年CHI计算机系统人因会议论文集*（第1–18页）。
