- en: Efficient Model Fine-Tuning with Bottleneck Adapter
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用瓶颈适配器进行高效模型微调
- en: 原文：[https://towardsdatascience.com/efficient-model-fine-tuning-with-bottleneck-adapter-5162fcec3909](https://towardsdatascience.com/efficient-model-fine-tuning-with-bottleneck-adapter-5162fcec3909)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/efficient-model-fine-tuning-with-bottleneck-adapter-5162fcec3909](https://towardsdatascience.com/efficient-model-fine-tuning-with-bottleneck-adapter-5162fcec3909)
- en: How to fine-tune Transformer-based models with bottleneck adapters
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何使用瓶颈适配器微调基于Transformer的模型
- en: '[](https://medium.com/@marcellusruben?source=post_page-----5162fcec3909--------------------------------)[![Ruben
    Winastwan](../Images/15ad0dd03bf5892510abdf166a1e91e1.png)](https://medium.com/@marcellusruben?source=post_page-----5162fcec3909--------------------------------)[](https://towardsdatascience.com/?source=post_page-----5162fcec3909--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----5162fcec3909--------------------------------)
    [Ruben Winastwan](https://medium.com/@marcellusruben?source=post_page-----5162fcec3909--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@marcellusruben?source=post_page-----5162fcec3909--------------------------------)[![Ruben
    Winastwan](../Images/15ad0dd03bf5892510abdf166a1e91e1.png)](https://medium.com/@marcellusruben?source=post_page-----5162fcec3909--------------------------------)[](https://towardsdatascience.com/?source=post_page-----5162fcec3909--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----5162fcec3909--------------------------------)
    [Ruben Winastwan](https://medium.com/@marcellusruben?source=post_page-----5162fcec3909--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----5162fcec3909--------------------------------)
    ·14 min read·Nov 22, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----5162fcec3909--------------------------------)
    ·阅读时间 14 分钟·2023年11月22日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/b79802cb5528fbfe5bb1a2f50166d41b.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b79802cb5528fbfe5bb1a2f50166d41b.png)'
- en: 'Photo by Karolina Grabowska: [https://www.pexels.com/photo/set-of-modern-port-adapters-on-black-surface-4219861/](https://www.pexels.com/photo/set-of-modern-port-adapters-on-black-surface-4219861/)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '照片来源：Karolina Grabowska: [https://www.pexels.com/photo/set-of-modern-port-adapters-on-black-surface-4219861/](https://www.pexels.com/photo/set-of-modern-port-adapters-on-black-surface-4219861/)'
- en: 'Fine-tuning is one of the most common things that we can do to gain better
    performance from a deep learning model on our specific task. The time we need
    to fine-tune a model normally corresponds to its size: the bigger the size of
    the model, the longer the time needed to fine-tune it.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 微调是我们为了在特定任务中从深度学习模型中获得更好性能时可以做的最常见的事情之一。我们需要微调模型的时间通常与模型的大小成正比：模型越大，微调所需的时间就越长。
- en: 'I think we can agree that nowadays, deep learning models such as Transformer-based
    models are becoming increasingly sophisticated. Overall, this is a good thing
    to see but it comes with a caveat: they tend to have huge number of parameters.
    Thus, fine-tuning large models is becoming more challenging to manage and we need
    a more efficient way to do it.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以达成一致的是，如今，基于Transformer的深度学习模型正变得越来越复杂。总体来说，这是一个值得关注的好现象，但它有一个警告：它们往往拥有庞大的参数量。因此，微调大型模型变得越来越难以管理，我们需要一种更高效的方法来进行微调。
- en: In this article, we’re going to discuss one of several efficient fine-tuning
    methods called bottleneck adapter. Although you can apply this method to any deep
    learning model, we’ll only focus our attention to its application on Transformer-based
    models.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们将讨论一种称为瓶颈适配器的高效微调方法。虽然您可以将这种方法应用于任何深度学习模型，但我们将重点关注其在基于Transformer的模型中的应用。
- en: 'The structure of this article is as follows: first, we’re going to do a normal
    fine-tuning of a BERT model on a specific dataset. Then, we will insert some bottleneck
    adapters into our BERT model with the help of `adapter-transformers` library to
    see how they can help us to make fine-tuning process more efficient.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 本文的结构如下：首先，我们将对一个特定数据集进行正常的BERT模型微调。然后，我们将借助`adapter-transformers`库将一些瓶颈适配器插入到BERT模型中，以查看它们如何帮助我们使微调过程更高效。
- en: Before we fine-tune the model, let’s start with the dataset that we’re going
    to use.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们微调模型之前，让我们先介绍一下我们将使用的数据集。
- en: About the Dataset
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 关于数据集
- en: The dataset we about to use contains [**different types of text related to mental
    health collected from Reddit**](https://huggingface.co/datasets/mrjunos/depression-reddit-cleaned)
    (licensed under CC-BY-4.0). The dataset itself is suitable for text classification
    tasks, where we can predict whether any given text has a depressive sentiment
    in it or not. Let’s take a look at a sample of it.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 我们即将使用的数据集包含[**从Reddit收集的与心理健康相关的不同类型文本**](https://huggingface.co/datasets/mrjunos/depression-reddit-cleaned)（许可协议为CC-BY-4.0）。该数据集适用于文本分类任务，我们可以预测给定文本是否包含抑郁情绪。让我们看一下它的样本。
- en: '[PRE0]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'As you can see, the dataset is very straightforward as we only have two fields:
    one for the text and another for the label. The label itself contains only two
    possible values: 1 if the text has a depressive sentiment and 0 otherwise. Our
    task is to fine-tune a pretrained BERT model to predict the sentiment of each
    text.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，数据集非常简单，因为我们只有两个字段：一个是文本，另一个是标签。标签本身只有两个可能的值：如果文本包含抑郁情绪则为1，否则为0。我们的任务是微调一个预训练的BERT模型，以预测每个文本的情感。
- en: In total, there are around 7731 texts, and we’re going to use 6500 of them for
    training and the rest 1231 for validation during fine-tuning process.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 总共有大约7731个文本，我们将使用其中6500个进行训练，其余1231个用于微调过程中的验证。
- en: 'Let’s create a dataloader to load the dataset in batch during fine-tuning process
    that we’ll see in the next section:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们创建一个数据加载器，在微调过程中批量加载数据集，我们将在下一节中看到：
- en: '[PRE1]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Now that we have our data, we can start to talk about the main topic of this
    article. However, the concept behind bottleneck adapters will be easier to understand
    if we already familiar with the standard process of a normal fine-tuning.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了数据，可以开始讨论本文的主要话题。然而，如果我们已经熟悉普通微调的标准过程，那么理解瓶颈适配器的概念会更容易。
- en: Thus, in the next section, we‘ll start with the concept of a normal fine-tuning
    process before expanding the concept to the application of bottleneck adapters.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在下一节中，我们将从普通微调过程的概念开始，然后扩展到瓶颈适配器的应用。
- en: We’ll be using `adapter-transformers` library to do both normal fine-tuning
    and adapter-based fine-tuning. This library is a direct fork from the famous `transformers`
    library from HuggingFace, which means that it contains all of the functionality
    of `transformers` with several addition of model classes and methods so that we
    can apply adapters into models easily.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用`adapter-transformers`库来进行普通微调和基于适配器的微调。这个库是著名的HuggingFace的`transformers`库的直接分支，这意味着它包含了`transformers`的所有功能，并增加了几个模型类和方法，以便我们可以轻松地将适配器应用到模型中。
- en: 'You can install `adapter-transformers` with the following command:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用以下命令安装`adapter-transformers`：
- en: '[PRE2]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Now let’s start with the common procedure of a normal fine-tuning.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们开始讨论普通微调的常规过程。
- en: Normal BERT Fine-Tuning
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 普通BERT微调
- en: 'Fine-tuning is a common technique in deep learning in order to gain better
    performance from a pretrained model on a specific data and/or task. The main idea
    is simple: we take the weights of a pretrained model, and then update those weights
    based on a new domain-specific data.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 微调是深度学习中的一种常见技术，旨在从预训练模型中获得在特定数据和/或任务上的更好性能。其主要思想很简单：我们获取预训练模型的权重，然后基于新的领域特定数据更新这些权重。
- en: '![](../Images/0e2d0faa76ec52acef09f4a5e5226d0c.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0e2d0faa76ec52acef09f4a5e5226d0c.png)'
- en: Normal fine-tuning process. Image by author.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 普通微调过程。图片由作者提供。
- en: The common procedure of a normal fine-tuning is as follows.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 普通微调的常规过程如下。
- en: 'First, we pick a pretrained model, which in our case would be a BERT-base model.
    As a side note, we’re not going to focus our attention on BERT in this article,
    but if you’re new to BERT and want to find out more about it, you can check my
    article that talks about BERT here:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们选择一个预训练模型，在我们的情况下是BERT-base模型。顺便提一下，我们在这篇文章中不会专注于BERT，但如果你对BERT不熟悉并想了解更多，可以查看我关于BERT的文章：
- en: '[](/text-classification-with-bert-in-pytorch-887965e5820f?source=post_page-----5162fcec3909--------------------------------)
    [## Text Classification with BERT in PyTorch'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/text-classification-with-bert-in-pytorch-887965e5820f?source=post_page-----5162fcec3909--------------------------------)
    [## 使用BERT进行PyTorch中的文本分类'
- en: How to leverage a pre-trained BERT model from Hugging Face to classify text
    of news articles
  id: totrans-33
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 如何利用Hugging Face的预训练BERT模型来分类新闻文章的文本
- en: towardsdatascience.com](/text-classification-with-bert-in-pytorch-887965e5820f?source=post_page-----5162fcec3909--------------------------------)
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '[towardsdatascience.com](/text-classification-with-bert-in-pytorch-887965e5820f?source=post_page-----5162fcec3909--------------------------------)'
- en: In a nutshell, BERT-base contains 12 stacks of Transformer-encoder layer. During
    fine-tuning process, we need to add a linear layer on top of the last stack that
    will act as a classifier. Since the label in our dataset only consists of two
    possible values, then the output of our linear layer will also be two.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，BERT-base 包含 12 层 Transformer 编码器。在微调过程中，我们需要在最后一层上添加一个线性层，作为分类器。由于我们数据集中的标签仅包含两个可能的值，因此我们的线性层的输出也将是两个。
- en: '[PRE3]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '![](../Images/87b1418a06ca1585d96c7a2b9555b198.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/87b1418a06ca1585d96c7a2b9555b198.png)'
- en: BERT architecture. Image by author.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: BERT 架构。图片由作者提供。
- en: Now that we have defined our model, we need to create the fine-tuning script.
    Below is the code snippet to fine-tune the model on our dataset.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经定义了我们的模型，我们需要创建微调脚本。以下是对模型进行微调的代码片段。
- en: '[PRE4]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: We will fine-tune our BERT model for around 10 epochs and the learning rate
    is set to 10e-7\. I fine-tuned the model on a T4 GPU with batch size of 2\. Below
    is a snapshot of how the training and validation accuracy should look like.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将对我们的 BERT 模型进行大约 10 个周期的微调，学习率设置为 10e-7。 我在 T4 GPU 上使用批量大小为 2 进行了模型微调。以下是训练和验证准确度的快照。
- en: '[PRE5]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: And that’s it! We achieved a validation accuracy of 97.3% with BERT on our dataset.
    We can then proceed to use the fine-tuned model to make a prediction on unseen
    data.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 就这样！我们在数据集上使用 BERT 达到了 97.3% 的验证准确率。然后我们可以继续使用微调后的模型对未见数据进行预测。
- en: Overall, normal fine-tuning of a pretrained model wouldnt’be a problem if our
    model has ‘small*’* number of parameters, as we can see with BERT model above.
    Let’s check the total number of parameters that our BERT-base model has.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 总体来说，如果我们的模型具有“小”数量的参数，正常的微调不会成为问题，如上所示的 BERT 模型。让我们检查一下我们的 BERT-base 模型的总参数数量。
- en: '[PRE6]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: This model has in total close to 110 million parameters. Although it looks like
    a lot, but it’s still nothing compared to most Large Language Models nowadays,
    as they can have billions of paramaters. If you notice as well, the number of
    trainable parameters is the same as the total number of parameters of our BERT
    model. This means that during a normal fine-tuning process, we update the weight
    of all of the parameters of our BERT model.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型总共有接近 1.1 亿个参数。虽然看起来很多，但与现在的大多数大型语言模型相比，这仍然不算什么，因为它们可能有数十亿个参数。如果你也注意到，可训练参数的数量与我们
    BERT 模型的总参数数量相同。这意味着在正常的微调过程中，我们会更新 BERT 模型的所有参数的权重。
- en: With the help of a T4 GPU and the fact that our training dataset contains only
    6500 entries, we fortunately only need around 12 minutes per epoch to update all
    of the weights. Now imagine if we use larger models and larger dataset, the computation
    time to do normal fine-tuning would be costly.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 借助 T4 GPU 和我们的训练数据集仅包含 6500 条数据，我们幸运地只需大约 12 分钟每个周期来更新所有权重。现在想象一下，如果我们使用更大的模型和数据集，进行正常微调的计算时间将会非常昂贵。
- en: Also, normal fine-tuning is commonly associated with a risk of the so-called
    catastrophic forgetting if we’re not careful with how we choose a learning rate,
    or when we try to fine-tune a pretrained model on several tasks/datasets. Catastrophic
    forgetting refers to a condition when a pretrained model ‘*forgets*’ the task
    it was trained on when we fine-tune it on a new task.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，正常的微调通常与所谓的灾难性遗忘风险相关，如果我们在选择学习率时不小心，或者当我们尝试在多个任务/数据集上微调预训练模型时。灾难性遗忘指的是当我们在新任务上微调预训练模型时，它会“遗忘”其训练过的任务。
- en: Thus, we definitely need a more efficient procedure to do a fine-tuning process.
    And this is where we can use different types of efficient fine tuning method,
    with bottleneck adapter being one of them.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们确实需要一种更高效的程序来进行微调过程。这就是我们可以使用不同类型的高效微调方法的地方，其中瓶颈适配器就是其中之一。
- en: How Bottleneck Adapter Works
  id: totrans-50
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**瓶颈适配器**的工作原理'
- en: The main idea behind an adapter is that we introduce small subset of layers
    and place them inside of the original architecture of pretrained models. During
    the fine-tuning process, we freeze all of the parameters of original pretrained
    models and thus, only the weights of these additional subset of layers will be
    updated.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 适配器的主要思想是，我们引入一小部分层，并将其放置在预训练模型的原始架构中。在微调过程中，我们冻结预训练模型的所有参数，因此，只有这些附加子集层的权重会被更新。
- en: Bottleneck adapter specifically is an adapter that consists of two normal feed-forward
    layers, with an optional normalization layer before and/or after them. The functionality
    of one feed-forward layer is to downscale the output, while the other is to upscale
    the output. This is why this adapter is called bottleneck adapter.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 瓶颈适配器特指一种由两个普通前馈层组成的适配器，前后可选地添加归一化层。一个前馈层的功能是缩小输出，而另一个是放大输出。这就是为什么它被称为瓶颈适配器的原因。
- en: '![](../Images/1fd52a6f95d490e9e5b4240441622681.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1fd52a6f95d490e9e5b4240441622681.png)'
- en: Common bottleneck adapters. Image by author.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 常见的瓶颈适配器。图片由作者提供。
- en: You can apply this adapter on any deep learning model, but as mentioned earlier,
    we’ll focus our attention to its application on Transfomer-based models.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以将这个适配器应用于任何深度学习模型，但如前所述，我们将重点关注其在基于Transformer的模型上的应用。
- en: 'Transformer-based models normally consist of several stacks of Transformer
    layers. BERT-based model that we use in this article, for example, has 12 stacks
    of Transformer-encoder layer. Each stack consists of the following components:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 基于Transformer的模型通常由多个Transformer层堆叠组成。例如，本文使用的基于BERT的模型有12个Transformer编码器层堆叠。每个堆叠包括以下组件：
- en: '![](../Images/c91ba9179360f97f8dd2a6368efdcab5.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c91ba9179360f97f8dd2a6368efdcab5.png)'
- en: Transformer encoder stack. Image by author.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer编码器堆叠。图片由作者提供。
- en: 'There are several different ways we can put a bottleneck adapter into this
    stack. However, there are two common configurations in which the adapter can be
    inserted: one is proposed by Pfeiffer and another is proposed by Houlsby.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将瓶颈适配器放入这个堆叠的几种不同方式。然而，有两种常见的配置：一种是Pfeiffer提出的，另一种是Houlsby提出的。
- en: 'The bottleneck adapter proposed by Pfeiffer is inserted after the final norm
    layer, while the bottleneck adapter proposed by Houlsby is inserted in two different
    places: one after multi-head attention layer and another after the feed-forward
    layer, as you can see from the visualization below:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: Pfeiffer提出的瓶颈适配器插入在最后的规范层之后，而Houlsby提出的瓶颈适配器插入在两个不同的位置：一个在多头注意力层之后，另一个在前馈层之后，如下图所示：
- en: '![](../Images/a783afeb3413b95c3f3f9c8cfc4dc457.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a783afeb3413b95c3f3f9c8cfc4dc457.png)'
- en: Difference between Pfeiffer and Houlsby adapter configuration. Image by author.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: Pfeiffer和Houlsby适配器配置的区别。图片由作者提供。
- en: 'Since our BERT-base model has 12 stacks of Transformer-encoder layer, then
    we will have a total of 12 bottleneck adapters if we use Pfeiffer configuration:
    one adapter in each stack. Meanwhile, we’ll have a total of 24 bottleneck adapters
    with Houlsby configuration: two adapters in each stack.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们的BERT-base模型有12个Transformer编码器层堆叠，因此如果使用Pfeiffer配置，我们将有12个瓶颈适配器：每个堆叠一个适配器。同时，如果使用Houlsby配置，我们将有24个瓶颈适配器：每个堆叠两个适配器。
- en: Although Pfeiffer configuration leads to fewer parameters compared to Houlsby,
    their performances have shown to be on par with each other across 8 different
    tasks.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管Pfeiffer配置相比于Houlsby配置参数更少，但在8个不同任务中的表现相当。
- en: 'Now the question is: how does this bottleneck adapter make fine-tuning process
    more efficient?'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 现在的问题是：这个瓶颈适配器是如何让微调过程更高效的？
- en: As mentioned previously, we freeze the weights of our pretrained model and only
    update the weights of our adapter during fine-tuning process. This means that
    we can speed up the fine-tuning process by considerable margin and we will see
    this in the next section. The experiments have also shown that the performance
    of fine-tuning with adapters are mostly comparable with normal fine-tuning.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，我们在微调过程中冻结预训练模型的权重，只更新适配器的权重。这意味着我们可以显著加快微调过程，接下来的部分会展示这一点。实验也表明，使用适配器进行微调的性能通常与普通微调相当。
- en: Also, imagine the situation where we want to use the same pretrained model for
    two different datasets. Instead of having two copies of the same model fine-tuned
    on different datasets to avoid the risk of catastrophic forgetting, we can have
    just one model with two different adapters fine-tuned on different datasets.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 同时，假设我们想用相同的预训练模型处理两个不同的数据集。我们可以使用一个模型，通过两个不同的适配器在不同的数据集上进行微调，从而避免灾难性遗忘的风险，而不是拥有两个在不同数据集上微调的模型。
- en: '![](../Images/62629350bae3af2e6b01722fad762b61.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/62629350bae3af2e6b01722fad762b61.png)'
- en: Image by author.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供。
- en: With this approach, we save a lot of storage space. As an illustration, the
    size of a single BERT-base model is 440 MB, which translates to 880 MB if we have
    two models. Meanwhile, if we have one BERT-base model with two adapters, the size
    would only be roughly around 450 MB, since bottleneck adapters only take small
    size of memory.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这种方法，我们节省了大量存储空间。例如，一个单独的BERT-base模型的大小是440 MB，如果我们有两个模型，则为880 MB。与此同时，如果我们有一个带有两个适配器的BERT-base模型，大小大约为450
    MB，因为瓶颈适配器只占用少量内存。
- en: Bottleneck Adapter Implementation
  id: totrans-71
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 瓶颈适配器实现
- en: In this section, we’ll implement Pfeiffer’s version of bottleneck adapter. To
    do this, we only need to change the script of model architecture, while the scripts
    related to fine-tuning process and dataloading remain the same.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将实现Pfeiffer版本的瓶颈适配器。为此，我们只需要更改模型架构的脚本，而与微调过程和数据加载相关的脚本保持不变。
- en: Let’s define the model architecture with Pfeiffer’s adapter.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用Pfeiffer的适配器定义模型架构。
- en: '[PRE7]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'As you can see, it’s pretty straightforward to implement the adapter version
    of a model:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，实现适配器版本的模型非常简单：
- en: Define the adapter configuration we want to apply with `AdapterConfig.load('pfeiffer')`
    . If you want to use Houlsby configuration, just change it to `'houlsby'` .
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用`AdapterConfig.load('pfeiffer')`定义我们想要应用的适配器配置。如果你想使用Houlsby配置，只需将其更改为`'houlsby'`。
- en: Insert adapters into our BERT model with `add_adapter()` method. Common practice
    is to give adapters the name according to task or dataset that we want our model
    to be fine-tuned on.
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用`add_adapter()`方法将适配器插入到我们的BERT模型中。常见做法是根据任务或数据集为适配器命名，以便我们希望模型进行微调。
- en: Freeze all the weights of pretrained model with `train_adapter()` method.
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用`train_adapter()`方法冻结预训练模型的所有权重。
- en: Add a linear layer that will act as a prediction head on top of our BERT model
    with `add_classification_head()` method. Common practice is to give prediction
    head the same name as our adapters.
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用`add_classification_head()`方法在BERT模型上添加一个线性层，作为预测头。常见做法是为预测头取与适配器相同的名称。
- en: Activate our adapters and prediction head to make sure that they’re used in
    every forward pass with `set_active_adapters()` method.
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 激活我们的适配器和预测头，以确保它们在每次前向传递中都被使用，使用`set_active_adapters()`方法。
- en: 'Now let’s check the total number of parameters and the proportion of trainable
    parameters after the inclusion of adapters:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们检查在添加适配器后参数的总数和可训练参数的比例：
- en: '[PRE8]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The model with adapters have more parameters than our original BERT-base model,
    but only 1.35% of them are trainable, since we’ll only update the weights of our
    adapters.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 带有适配器的模型参数比我们原始的BERT-base模型多，但只有1.35%是可训练的，因为我们只会更新适配器的权重。
- en: 'Now it’s time to train the model. Since the weights of our adapter are initialized
    randomly, then we’ll go with a slightly higher learning rate this time. We’ll
    also train this model for 10 epochs. If everything goes well, you’ll get more
    or less similar outputs like below:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是训练模型的时候了。由于适配器的权重是随机初始化的，因此这次我们将使用略高的学习率。我们还将训练该模型10个时期。如果一切顺利，你将获得类似如下的输出：
- en: '[PRE9]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: As you can see, the performance of our model with adapters is comparable with
    the full fine-tuning version of the model. Also, the time it needs to complete
    one epoch is 4.5 minutes faster than full fine-tuning.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，带有适配器的模型性能与完全微调版本的模型相当。而且，完成一个时期所需的时间比完全微调快4.5分钟。
- en: Now that we have trained it, we can save the adapter.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们已经训练好了它，我们可以保存适配器。
- en: '[PRE10]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'And then we can load the adapter and use it for inference as follows:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们可以加载适配器并进行推理，如下所示：
- en: '[PRE11]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Conclusion
  id: totrans-91
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: In this article, we’ve seen how bottleneck adapters can be helpful in fine-tuning
    process of a large model. With bottleneck adapters, we’re able to speed up fine-tuning
    while still maintaining the end performance of the model. These adapters are also
    useful to avoid the risk of catastrophic forgetting commonly associated to a fine-tuned
    model. Moreover, these adapters don’t take a lot of space in the memory.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们已经看到瓶颈适配器在大型模型的微调过程中是如何有帮助的。使用瓶颈适配器，我们能够加快微调速度，同时保持模型的最终性能。这些适配器也有助于避免通常与微调模型相关的灾难性遗忘风险。此外，这些适配器不会占用大量内存空间。
- en: I hope this article is helpful for you to get your hands dirty with bottleneck
    adapters. If you want to access all of the code implemented in this article, you
    can check it out via [**this notebook**](https://github.com/marcellusruben/medium-resources/blob/main/Bottleneck_Adapters/Bottleneck_Adapters_Medium.ipynb).
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 我希望这篇文章对你在使用瓶颈适配器时有所帮助。如果你想查看本文中实现的所有代码，可以通过[**这个笔记本**](https://github.com/marcellusruben/medium-resources/blob/main/Bottleneck_Adapters/Bottleneck_Adapters_Medium.ipynb)访问。
- en: Dataset Reference
  id: totrans-94
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据集参考
- en: '[https://huggingface.co/datasets/mrjunos/depression-reddit-cleaned](https://huggingface.co/datasets/mrjunos/depression-reddit-cleaned)'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://huggingface.co/datasets/mrjunos/depression-reddit-cleaned](https://huggingface.co/datasets/mrjunos/depression-reddit-cleaned)'
