- en: 'ReLoRa: Pre-train a Large Language Model on Your GPU'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 'ReLoRa: 在你的GPU上预训练大型语言模型'
- en: 原文：[https://towardsdatascience.com/relora-pre-train-a-large-language-model-on-your-gpu-d104756f9ddf?source=collection_archive---------3-----------------------#2023-07-20](https://towardsdatascience.com/relora-pre-train-a-large-language-model-on-your-gpu-d104756f9ddf?source=collection_archive---------3-----------------------#2023-07-20)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/relora-pre-train-a-large-language-model-on-your-gpu-d104756f9ddf?source=collection_archive---------3-----------------------#2023-07-20](https://towardsdatascience.com/relora-pre-train-a-large-language-model-on-your-gpu-d104756f9ddf?source=collection_archive---------3-----------------------#2023-07-20)
- en: LoRa but with multiple resets in a row
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: LoRa，但有多个连续重置
- en: '[](https://medium.com/@bnjmn_marie?source=post_page-----d104756f9ddf--------------------------------)[![Benjamin
    Marie](../Images/3ea1ad230cb1e67610418a8e36a5e5dd.png)](https://medium.com/@bnjmn_marie?source=post_page-----d104756f9ddf--------------------------------)[](https://towardsdatascience.com/?source=post_page-----d104756f9ddf--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----d104756f9ddf--------------------------------)
    [Benjamin Marie](https://medium.com/@bnjmn_marie?source=post_page-----d104756f9ddf--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@bnjmn_marie?source=post_page-----d104756f9ddf--------------------------------)[![Benjamin
    Marie](../Images/3ea1ad230cb1e67610418a8e36a5e5dd.png)](https://medium.com/@bnjmn_marie?source=post_page-----d104756f9ddf--------------------------------)[](https://towardsdatascience.com/?source=post_page-----d104756f9ddf--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----d104756f9ddf--------------------------------)
    [Benjamin Marie](https://medium.com/@bnjmn_marie?source=post_page-----d104756f9ddf--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fad2a414578b3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frelora-pre-train-a-large-language-model-on-your-gpu-d104756f9ddf&user=Benjamin+Marie&userId=ad2a414578b3&source=post_page-ad2a414578b3----d104756f9ddf---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----d104756f9ddf--------------------------------)
    ·8 min read·Jul 20, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fd104756f9ddf&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frelora-pre-train-a-large-language-model-on-your-gpu-d104756f9ddf&user=Benjamin+Marie&userId=ad2a414578b3&source=-----d104756f9ddf---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[阅读](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fad2a414578b3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frelora-pre-train-a-large-language-model-on-your-gpu-d104756f9ddf&user=Benjamin+Marie&userId=ad2a414578b3&source=post_page-ad2a414578b3----d104756f9ddf---------------------post_header-----------)
    发布在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----d104756f9ddf--------------------------------)
    · 8分钟阅读 · 2023年7月20日'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fd104756f9ddf&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frelora-pre-train-a-large-language-model-on-your-gpu-d104756f9ddf&source=-----d104756f9ddf---------------------bookmark_footer-----------)![](../Images/a84220fa206495cacbda94863cf4dfe4.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fd104756f9ddf&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frelora-pre-train-a-large-language-model-on-your-gpu-d104756f9ddf&source=-----d104756f9ddf---------------------bookmark_footer-----------)![](../Images/a84220fa206495cacbda94863cf4dfe4.png)'
- en: The ReLoRa framework — Image by the author
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: ReLoRa框架 — 图片由作者提供
- en: In 2021, [Hu et al.](https://arxiv.org/abs/2106.09685) proposed low-rank adapters
    (LoRa) for LLMs. This method significantly reduces the cost of fine-tuning large
    language models (LLMs) by only training a few added parameters (low-rank networks)
    while keeping the LLM’s original parameters (high-rank networks) frozen.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 2021年，[胡等人](https://arxiv.org/abs/2106.09685)提出了用于LLMs的低秩适配器（LoRa）。该方法通过仅训练几个附加参数（低秩网络），同时保持LLM的原始参数（高秩网络）冻结，从而显著降低了对大型语言模型（LLMs）进行微调的成本。
- en: With LoRa, we still need an existing pre-trained model to fine-tune, i.e., it
    can’t pre-train a good LLM from scratch due to the low-rank restrictions. It leaves
    pre-training unaffordable for most individuals and organizations.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 使用LoRa，我们仍然需要一个现有的预训练模型进行微调，即由于低秩限制，它不能从头开始预训练一个好的LLM。这使得大多数个人和组织难以承担预训练的成本。
- en: To reduce this cost, [Lialin et al. (2023)](https://arxiv.org/pdf/2307.05695.pdf)
    propose ReLoRa. This is a modification of LoRa that allows pre-training LLMs from
    scratch.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 为了降低这个成本，[Lialin 等（2023）](https://arxiv.org/pdf/2307.05695.pdf) 提出了 ReLoRa。这是对
    LoRa 的一种修改，允许从头开始预训练 LLM。
- en: In this article, I first explain how ReLoRa works. Then, I analyze and comment
    on the results presented in the scientific paper describing ReLoRa. In the last
    section, I show how to set up and run ReLoRa on your computer.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我首先解释 ReLoRa 的工作原理。接着，我分析并评论描述 ReLoRa 的科学论文中的结果。在最后一部分，我展示了如何在你的计算机上设置和运行
    ReLoRa。
- en: '*Note about the licenses:* [*The scientific paper published on arXiv*](https://arxiv.org/abs/2307.05695)
    *and describing ReLoRa is distributed under a CC BY 4.0 license.* [*The source
    code of ReLoRa is published on GitHub*](https://github.com/guitaricet/peft_pretraining)
    *and distributed under an Apache 2.0 license allowing commercial use.*'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '*关于许可证的说明：* [*描述 ReLoRa 的科学论文*](https://arxiv.org/abs/2307.05695) *以 CC BY
    4.0 许可证发布。* [*ReLoRa 的源代码已发布在 GitHub 上*](https://github.com/guitaricet/peft_pretraining)
    *并以 Apache 2.0 许可证发布，允许商业使用。*'
- en: 'ReLoRa: from low-rank to high-rank networks'
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ReLoRa：从低秩到高秩网络
