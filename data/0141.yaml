- en: 'Scalable Serving with Kubernetes and Seldon Core: A tutorial'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Kubernetes 和 Seldon Core 进行可扩展服务：教程
- en: 原文：[https://towardsdatascience.com/scalable-serving-with-kubernetes-and-seldon-core-a-tutorial-37aec914c4d5?source=collection_archive---------20-----------------------#2023-01-09](https://towardsdatascience.com/scalable-serving-with-kubernetes-and-seldon-core-a-tutorial-37aec914c4d5?source=collection_archive---------20-----------------------#2023-01-09)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/scalable-serving-with-kubernetes-and-seldon-core-a-tutorial-37aec914c4d5?source=collection_archive---------20-----------------------#2023-01-09](https://towardsdatascience.com/scalable-serving-with-kubernetes-and-seldon-core-a-tutorial-37aec914c4d5?source=collection_archive---------20-----------------------#2023-01-09)
- en: Learn how to deploy ML models in Kubernetes clusters and to implement autoscaling
    for your deployment with HPA and KEDA
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 学习如何在 Kubernetes 集群中部署机器学习模型，并使用 HPA 和 KEDA 实现自动扩展
- en: '[](https://medium.com/@tintn03?source=post_page-----37aec914c4d5--------------------------------)[![Tin
    Nguyen](../Images/f5a69125e3d42be7906c8cd51f827854.png)](https://medium.com/@tintn03?source=post_page-----37aec914c4d5--------------------------------)[](https://towardsdatascience.com/?source=post_page-----37aec914c4d5--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----37aec914c4d5--------------------------------)
    [Tin Nguyen](https://medium.com/@tintn03?source=post_page-----37aec914c4d5--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@tintn03?source=post_page-----37aec914c4d5--------------------------------)[![Tin
    Nguyen](../Images/f5a69125e3d42be7906c8cd51f827854.png)](https://medium.com/@tintn03?source=post_page-----37aec914c4d5--------------------------------)[](https://towardsdatascience.com/?source=post_page-----37aec914c4d5--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----37aec914c4d5--------------------------------)
    [Tin Nguyen](https://medium.com/@tintn03?source=post_page-----37aec914c4d5--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F78d51d946a3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fscalable-serving-with-kubernetes-and-seldon-core-a-tutorial-37aec914c4d5&user=Tin+Nguyen&userId=78d51d946a3&source=post_page-78d51d946a3----37aec914c4d5---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----37aec914c4d5--------------------------------)
    ·11 min read·Jan 9, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F37aec914c4d5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fscalable-serving-with-kubernetes-and-seldon-core-a-tutorial-37aec914c4d5&user=Tin+Nguyen&userId=78d51d946a3&source=-----37aec914c4d5---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F78d51d946a3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fscalable-serving-with-kubernetes-and-seldon-core-a-tutorial-37aec914c4d5&user=Tin+Nguyen&userId=78d51d946a3&source=post_page-78d51d946a3----37aec914c4d5---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----37aec914c4d5--------------------------------)
    ·11 分钟阅读·2023年1月9日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F37aec914c4d5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fscalable-serving-with-kubernetes-and-seldon-core-a-tutorial-37aec914c4d5&user=Tin+Nguyen&userId=78d51d946a3&source=-----37aec914c4d5---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F37aec914c4d5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fscalable-serving-with-kubernetes-and-seldon-core-a-tutorial-37aec914c4d5&source=-----37aec914c4d5---------------------bookmark_footer-----------)![](../Images/b9678f7f5d613266724cc19254ea38e4.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F37aec914c4d5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fscalable-serving-with-kubernetes-and-seldon-core-a-tutorial-37aec914c4d5&source=-----37aec914c4d5---------------------bookmark_footer-----------)![](../Images/b9678f7f5d613266724cc19254ea38e4.png)'
- en: Photo by [Adam Kool](https://unsplash.com/@adamkool) on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由 [Adam Kool](https://unsplash.com/@adamkool) 拍摄，来源于 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: In most ML applications, deploying trained models to production is a crucial
    stage. It’s where the models demonstrate their values by giving predictions for
    customers or other systems.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在大多数机器学习应用中，将训练好的模型部署到生产环境是一个关键阶段。在这一阶段，模型通过为客户或其他系统提供预测来展示其价值。
- en: Deploying a model can be as simple as implementing a Flask server and then exporting
    its endpoints for users to call. Yet, It’s not easy to build a system that can
    robustly and reliably serve a large number of requests with strict response time
    or throughput requirements.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 部署模型可以像实现一个 Flask 服务器然后导出其端点供用户调用一样简单。然而，构建一个能够稳健且可靠地处理大量请求且有严格响应时间或吞吐量要求的系统并不容易。
- en: For medium and large businesses, the systems must be able to scale to process
    heavier workloads without significantly changing the codebase. Perhaps the corporation
    is expanding and needs a scalable system to handle the growing number of requests
    (this characteristic is scalability). The business needs the system to be able
    to adapt to traffic fluctuations (this characteristic is elasticity). These characteristics
    can be achieved if the systems are capable of autoscaling based on traffic volume.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 对于中型和大型企业，系统必须能够扩展以处理更重的工作负载，而无需显著改变代码库。也许公司正在扩展，需要一个可扩展的系统来处理不断增长的请求数量（这种特性叫做可扩展性）。业务需要系统能够适应流量波动（这种特性叫做弹性）。如果系统能够根据流量量进行自动扩展，这些特性是可以实现的。
- en: In this tutorial, we’re going to learn how to deploy ML models in Kubernetes
    clusters with Seldon Core. We’ll also learn to implement autoscaling for our deployment
    with HPA and KEDA. The code for this tutorial can be found in this [repo](https://github.com/tintn/ml-model-deployment-tutorials).
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个教程中，我们将学习如何使用 Seldon Core 在 Kubernetes 集群中部署 ML 模型。我们还将学习如何通过 HPA 和 KEDA
    实现自动扩展。这个教程的代码可以在这个[仓库](https://github.com/tintn/ml-model-deployment-tutorials)中找到。
- en: Train a PyTorch model
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练一个 PyTorch 模型
- en: To go through the deployment process, we’ll need a model. We use the model from
    this [tutorial](https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html)
    from the official PyTorch website. It’s a simple image classification model that
    can run with CPU easily, so we can test the whole deployment process on local
    machines like your laptop.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 要完成部署过程，我们需要一个模型。我们使用来自官方 PyTorch 网站的这个[教程](https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html)中的模型。它是一个简单的图像分类模型，能够轻松运行于
    CPU，因此我们可以在像你的笔记本电脑这样的本地机器上测试整个部署过程。
- en: 'Assume you’re in the `toy-model` folder of this [repo](https://github.com/tintn/ml-model-deployment-tutorials).
    You can train the model on the CIFAR10 dataset with:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你在这个[仓库](https://github.com/tintn/ml-model-deployment-tutorials)的 `toy-model`
    文件夹中。你可以使用以下命令在 CIFAR10 数据集上训练模型：
- en: '[PRE0]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Seldon Core uses [Triton Inference Server](https://github.com/triton-inference-server/server)
    to serve PyTorch models, so we need to prepare the model in a format that it can
    be served with Triton. First, we need to export the model to TorchScript (it’s
    also possible to serve PyTorch models with Triton’s [python backend](https://github.com/triton-inference-server/python_backend),
    but it’s generally less efficient and more complicated to deploy).
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: Seldon Core 使用[Triton Inference Server](https://github.com/triton-inference-server/server)来服务
    PyTorch 模型，因此我们需要将模型准备成可以用 Triton 服务的格式。首先，我们需要将模型导出为 TorchScript（也可以通过 Triton
    的[python 后端](https://github.com/triton-inference-server/python_backend)服务 PyTorch
    模型，但通常效率较低且部署更复杂）。
- en: 'Tracing and scripting are the two methods for exporting a model to TorchScript.
    The choice between them is still debatable, this [article](https://ppwwyyxx.com/blog/2022/TorchScript-Tracing-vs-Scripting/)
    explores the benefits and drawbacks of both methods. We’ll use the tracing method
    to export the model:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 跟踪和脚本是将模型导出到 TorchScript 的两种方法。两者的选择仍然存在争议，这篇[文章](https://ppwwyyxx.com/blog/2022/TorchScript-Tracing-vs-Scripting/)探讨了这两种方法的优缺点。我们将使用跟踪方法来导出模型：
- en: '[PRE1]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Triton loads models from a model repository. It must contain information that
    the server needs to serve a model such as the model’s input/output information,
    backend to use… A model repository must follow the following structure:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: Triton 从模型仓库加载模型。它必须包含服务器需要的服务模型的信息，如模型的输入/输出信息、使用的后端等。模型仓库必须遵循以下结构：
- en: '[PRE2]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'In our case, we only have one model. Let’s call the model `cifar10-pytorch`,
    our model repo should have the following structure:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的例子中，我们只有一个模型。我们称这个模型为 `cifar10-pytorch`，我们的模型仓库应该具有以下结构：
- en: '[PRE3]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '`cifar10-model` is the repository''s name, `cifar10-pytorch` is the model name
    and `model.ts` is the TorchScript model we just exported. `config.pdtxt` defines
    how the model should be served with Triton:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '`cifar10-model` 是仓库的名称，`cifar10-pytorch` 是模型名称，而 `model.ts` 是我们刚刚导出的 TorchScript
    模型。`config.pdtxt` 定义了如何使用 Triton 服务模型：'
- en: '[PRE4]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: You can find the final repository [here](https://github.com/tintn/ml-model-deployment-tutorials/tree/main/toy-model/cifar10-model).
    Triton supports several features that may be used to tune the model’s performance.
    You can also group multiple steps or multiple models into an inference pipeline
    to implement your business logic. However, I deliberately keep the model config
    simple to illustrate the entire process of model deployment instead of focusing
    on performance.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在[这里](https://github.com/tintn/ml-model-deployment-tutorials/tree/main/toy-model/cifar10-model)找到最终的代码库。Triton支持几种可能用于调整模型性能的特性。你还可以将多个步骤或多个模型组合成一个推理管道，以实现你的业务逻辑。然而，我故意保持模型配置简单，以展示模型部署的整个过程，而不是专注于性能。
- en: If you want to see a more realistic example of how to export and serve a PyTorch
    model with Triton, have a look at this [post](https://tintn.github.io/deploy-detectron2-with-triton/).
    It demonstrates how to use Triton to serve the MaskRCNN model from Detectron2,
    a popular model for instance segmentation and used in many real-world computer
    vision systems.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想查看如何使用Triton导出和服务PyTorch模型的更实际示例，可以查看这个[帖子](https://tintn.github.io/deploy-detectron2-with-triton/)。它展示了如何使用Triton服务来自Detectron2的MaskRCNN模型，这是一个用于实例分割的流行模型，并且在许多实际的计算机视觉系统中使用。
- en: 'Triton can access models from local filesystem or cloud storage services (e.g.
    S3, Google Storage, or Azure Storage). As we’re going to deploy the model in Kubernetes,
    using a cloud storage service is more convenient because all nodes in the Kubernetes
    cluster can access the same models. We’ll use AWS S3 as the model repository in
    this tutorial. Assume that you already have an AWS account, let’s create an S3
    bucket and upload the folder we have prepared:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: Triton可以访问本地文件系统或云存储服务（如S3、Google Storage或Azure Storage）中的模型。由于我们将要在Kubernetes中部署模型，使用云存储服务更为方便，因为Kubernetes集群中的所有节点都可以访问相同的模型。在本教程中，我们将使用AWS
    S3作为模型库。假设你已经有了AWS账户，让我们创建一个S3存储桶并上传我们准备好的文件夹：
- en: '[PRE5]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Replace `<YOUR_BUCKET>` to the name of your bucket. We now have the model repository
    on AWS S3, we can start to deploy the model.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 将`<YOUR_BUCKET>`替换为你的存储桶名称。我们现在已经在AWS S3上有了模型库，可以开始部署模型了。
- en: Deploy models with Seldon Core
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Seldon Core部署模型
- en: We’ll deploy the model to a Kubernetes cluster with Seldon Core, a framework
    specializing in ML model deployment and monitoring. Let’s create a local Kubernetes
    cluster, so we can test the deployment process using our local machine.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用Seldon Core将模型部署到Kubernetes集群中，Seldon Core是一个专注于ML模型部署和监控的框架。让我们创建一个本地Kubernetes集群，以便使用我们的本地计算机测试部署过程。
- en: '[Kind](https://kind.sigs.k8s.io/docs/user/quick-start/#installation) can be
    used to create local clusters. At the time of writing, Seldon Core has an [issue](https://github.com/SeldonIO/seldon-core/issues/4339)
    with k8s ≥ 1.25, so we have to use version 1.24 or older. To specify the k8s version
    with Kind, just choose the image with the corresponding version to start a cluster.
    The following command creates a local cluster named `kind-seldon` with `k8s==1.24.7`:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '[Kind](https://kind.sigs.k8s.io/docs/user/quick-start/#installation)可以用于创建本地集群。在撰写本文时，Seldon
    Core在k8s ≥ 1.25上有一个[问题](https://github.com/SeldonIO/seldon-core/issues/4339)，所以我们必须使用1.24或更旧版本。要使用Kind指定k8s版本，只需选择带有相应版本的镜像来启动集群。以下命令创建一个名为`kind-seldon`的本地集群，使用`k8s==1.24.7`：'
- en: '[PRE6]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Also, make sure you have `docker`, `kubectl`, `helm` installed on your local
    machine. Switching the context of `kubectl` to `kind-seldon` instructs `kubectl`
    to connect to the newly created cluster by default:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 同时，请确保你在本地计算机上安装了`docker`、`kubectl`和`helm`。将`kubectl`的上下文切换到`kind-seldon`指示`kubectl`默认连接到新创建的集群：
- en: '[PRE7]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Install Seldon Core
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 安装Seldon Core
- en: 'We’ll use [Istio](https://istio.io/) as the cluster’s Ingress and Seldon Core
    as the serving platform. You can find the installation instruction [here](https://docs.seldon.io/projects/seldon-core/en/latest/install/kind.html).
    After installing Istio and Seldon Core, run these commands to check if they are
    all correctly installed:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用[Istio](https://istio.io/)作为集群的Ingress，Seldon Core作为服务平台。你可以在[这里](https://docs.seldon.io/projects/seldon-core/en/latest/install/kind.html)找到安装说明。安装了Istio和Seldon
    Core后，运行这些命令检查它们是否都已正确安装：
- en: '[PRE8]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Check if the Istio gateway is running:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 检查Istio网关是否正在运行：
- en: '[PRE9]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Check if the Seldon controller is running:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 检查Seldon控制器是否正在运行：
- en: '[PRE10]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'If you haven’t done it, make sure the label `istio-injection` is enabled:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你还没做过，请确保启用了标签`istio-injection`：
- en: '[PRE11]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The Istio gateway is running on port 80 in the cluster, we need to forward
    a port from your local machine to that port so we can access it externally:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: Istio 网关在集群中的端口 80 上运行，我们需要将本地机器的端口转发到该端口，以便我们可以从外部访问：
- en: '[PRE12]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Serving with Seldon Core
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 Seldon Core 提供服务
- en: 'If your model repository is stored in a private bucket, you need to grant permission
    to access your bucket from within the cluster. It can be done by creating a secret
    and then referring to it when creating a deployment. This is a template to create
    secrets for S3 buckets:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你的模型库存储在私有存储桶中，你需要授予集群内部访问存储桶的权限。这可以通过创建一个秘密并在创建部署时引用它来完成。这是为 S3 存储桶创建秘密的模板：
- en: '[PRE13]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Replace `AWS_ACCESS_KEY_ID` and `AWS_SECRET_ACCESS_KEY` with your actual AWS
    access key ID and secret access key. Create the secret:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 将 `AWS_ACCESS_KEY_ID` 和 `AWS_SECRET_ACCESS_KEY` 替换为你的实际 AWS 访问密钥 ID 和秘密访问密钥。创建秘密：
- en: '[PRE14]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'We can deploy the model with this [manifest](https://github.com/tintn/ml-model-deployment-tutorials/blob/main/scalable-serving/cifar10-deploy.yaml),
    notice the created secret is referred to in the manifest with the `envSecretRefName`
    key. Make sure that `spec.predictors[].graph.name` matches the model name you
    uploaded to your model repository. Apply the manifest to create a deployment:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用这个 [清单](https://github.com/tintn/ml-model-deployment-tutorials/blob/main/scalable-serving/cifar10-deploy.yaml)
    部署模型，注意清单中创建的秘密通过 `envSecretRefName` 键进行引用。确保 `spec.predictors[].graph.name` 与你上传到模型库中的模型名称匹配。应用清单以创建部署：
- en: '[PRE15]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'If this is your first deployment in this cluster, it’ll take a while to download
    the necessary docker images. Check if the model is successfully deployed:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 如果这是你在该集群中的第一次部署，那么下载必要的 Docker 镜像将需要一些时间。检查模型是否成功部署：
- en: '[PRE16]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'I created a [script](https://github.com/tintn/ml-model-deployment-tutorials/blob/main/testing/test.py)
    using Locust to test the deployed models. You need to install the [requirements](https://github.com/tintn/ml-model-deployment-tutorials/blob/main/testing/requirements.txt)
    needed for the script to run first:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 我创建了一个 [脚本](https://github.com/tintn/ml-model-deployment-tutorials/blob/main/testing/test.py)，使用
    Locust 来测试已部署的模型。你需要首先安装脚本运行所需的 [依赖](https://github.com/tintn/ml-model-deployment-tutorials/blob/main/testing/requirements.txt)：
- en: '[PRE17]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Given that `localhost:8080` has been port-forwarded to the cluster''s gateway,
    run the following command to send requests to models deployed with Seldon:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 给定`localhost:8080`已经被端口转发到集群的网关，运行以下命令以向部署了 Seldon 的模型发送请求：
- en: '[PRE18]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'If your deployment name or model names are different, you can adjust the deployment’s
    URL accordingly in the script. The URL for a deployed model follows Seldon Core’s
    [inference protocol](https://docs.seldon.io/projects/seldon-core/en/latest/reference/apis/v2-protocol.html):'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你的部署名称或模型名称不同，你可以在脚本中相应地调整部署的 URL。已部署模型的 URL 遵循 Seldon Core 的 [推理协议](https://docs.seldon.io/projects/seldon-core/en/latest/reference/apis/v2-protocol.html)：
- en: '[PRE19]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: We’ve deployed our custom model with Seldon Core and tested it by sending inference
    requests to the model. In the next section, we’ll explore how to scale the deployment
    to handle more users or higher traffic.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经使用 Seldon Core 部署了自定义模型，并通过向模型发送推理请求进行了测试。在下一部分中，我们将深入探讨如何扩展部署，以处理更多用户或更高的流量。
- en: Pod Autoscaling with HPA
  id: totrans-64
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 HPA 进行 Pod 自动扩缩
- en: When it comes to scalability, Kubernetes offers HPA (Horizontal Pod Autoscaling).
    When certain metrics reach their thresholds for a resource (e.g. CPU or memory),
    HPA can add more pods to process heavier workloads.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 说到可扩展性，Kubernetes 提供了 HPA（水平 Pod 自动扩缩）。当某些指标达到资源的阈值（例如 CPU 或内存）时，HPA 可以添加更多的
    Pods 来处理更重的负载。
- en: Install Metrics Server
  id: totrans-66
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 安装指标服务器
- en: 'HPA needs to fetch metrics from an aggregated API, which is usually provided
    through a [Metrics Server](https://github.com/kubernetes-sigs/metrics-server).
    You can install a metrics server for your cluster with:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: HPA 需要从聚合的 API 中获取指标，这通常通过一个 [Metrics Server](https://github.com/kubernetes-sigs/metrics-server)
    提供。你可以使用以下命令为你的集群安装一个指标服务器：
- en: '[PRE20]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'If your cluster is local, you also need to disable certificate validation by
    passing the argument `-kubelet-insecure-tls` to the server:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你的集群是本地的，你还需要通过将参数 `-kubelet-insecure-tls` 传递给服务器来禁用证书验证：
- en: '[PRE21]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Deploy models with HPA
  id: totrans-71
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 HPA 部署模型
- en: 'We can enable HPA in the deployment manifest by adding `hpaSpec` for the corresponding
    component:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过在部署清单中添加 `hpaSpec` 来启用 HPA 以适应相应的组件：
- en: '[PRE22]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: The HPA spec tells the deployment to scale up when the current metric value
    (CPU usage in this case) is higher than 50% of the desired value, and the maximum
    replicas that the deployment can possibly have are 2.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: HPA 规范告诉部署当当前指标值（在本例中为 CPU 使用率）高于期望值的 50% 时进行扩展，并且部署可能具有的最大副本数为 2。
- en: 'Apply this [manifest](https://github.com/tintn/ml-model-deployment-tutorials/blob/main/scalable-serving/cifar10-deploy-hpa.yaml)
    to create a deployment with HPA, make sure you replace `<YOUR_BUCKET>` with your
    bucket name and the secret for accessing the bucket (as mentioned in the previous
    section) has been created:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 应用此[清单](https://github.com/tintn/ml-model-deployment-tutorials/blob/main/scalable-serving/cifar10-deploy-hpa.yaml)以创建一个具有
    HPA 的部署，确保您将 `<YOUR_BUCKET>` 替换为您的存储桶名称，并且已经创建了访问存储桶的密钥（如前一节中所述）：
- en: '[PRE23]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'You can see the current metric value with:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过以下命令查看当前指标值：
- en: '[PRE24]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Let’s check the running pods. You should see a running pod for the deployed
    model:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们检查运行中的 Pod。您应该看到部署模型的正在运行的 Pod：
- en: '[PRE25]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Now we can test the deployed model with our [test script](https://github.com/tintn/ml-model-deployment-tutorials/blob/main/testing/test.py)
    (which was mentioned in the previous section):'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以使用我们的[测试脚本](https://github.com/tintn/ml-model-deployment-tutorials/blob/main/testing/test.py)（在前一节中提到过）测试已部署的模型：
- en: '[PRE26]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Monitoring the current metric value with `kubectl get hpa -w`, you can see
    after a while the metric value exceeds the threshold, and HPA will trigger the
    creation of a new pod:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 通过 `kubectl get hpa -w` 监控当前指标值，一段时间后可以看到指标值超过阈值，HPA 将触发新 Pod 的创建：
- en: '[PRE27]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'If the current metric value is lower than the threshold for a certain period
    (it’s 5 minutes by default), HPA will scale down the deployment. The period can
    be configured with the argument `--horizontal-pod-autoscaler-downscale-stabilization`
    flag to `kube-controller-manager`:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 如果当前指标值在一定时期内低于阈值（默认为 5 分钟），HPA 将缩减部署。可以使用参数 `--horizontal-pod-autoscaler-downscale-stabilization`
    标志配置时期为 `kube-controller-manager`：
- en: '[PRE28]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: In this section, we’ve learned how to scale the number of pods up and down based
    on CPU usage. In the next section, we’ll use KEDA to scale our deployment more
    flexibly based on custom metrics.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们已经学会了如何根据 CPU 使用率上下扩展 Pod 的数量。在下一节中，我们将使用 KEDA 根据自定义指标更灵活地扩展我们的部署。
- en: Pod Autoscaling with KEDA
  id: totrans-88
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 KEDA 进行 Pod 自动扩展
- en: KEDA can fetch metrics from many sources (they are called scalers), see the
    list of supported scalers [here](https://keda.sh/docs/2.9/scalers/). We’ll set
    up KEDA to fetch metrics from a Prometheus server, and monitor the metrics to
    trigger pod scaling. The Prometheus server collects metrics from Seldon deployments
    in the cluster.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: KEDA 可以从许多来源获取指标（称为扩展器），请参见支持的扩展器列表[这里](https://keda.sh/docs/2.9/scalers/)。我们将设置
    KEDA 从 Prometheus 服务器获取指标，并监视指标以触发 Pod 的扩展。Prometheus 服务器收集集群中 Seldon 部署的指标。
- en: Install Seldon Monitoring and KEDA
  id: totrans-90
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 安装 Seldon 监控和 KEDA
- en: 'Follow this [instruction](https://docs.seldon.io/projects/seldon-core/en/latest/analytics/analytics.html)
    to install Seldon’s stack for monitoring, which includes a Prometheus server.
    The following pods should now be present in the `seldon-monitoring` namespace:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 按照[说明](https://docs.seldon.io/projects/seldon-core/en/latest/analytics/analytics.html)安装
    Seldon 的监控堆栈，其中包括 Prometheus 服务器。现在 `seldon-monitoring` 命名空间中应该存在以下 Pod：
- en: '[PRE29]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Check if the pod monitor for Seldon Core has been created:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 检查 Seldon Core 的 Pod 监视器是否已创建：
- en: '[PRE30]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Run the following command to enable KEDA in Seldon Core:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 运行以下命令在 Seldon Core 中启用 KEDA：
- en: '[PRE31]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Install KEDA to the cluster, and make sure that the previously installed KEDA
    (if any) is completely uninstalled:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 将 KEDA 安装到集群中，并确保先前安装的 KEDA（如果有）已完全卸载：
- en: '[PRE32]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Deploy models with KEDA
  id: totrans-99
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 KEDA 部署模型
- en: 'We’ve had everything set up. Let’s create a Seldon deployment with KEDA. Similar
    to HPA, to enable KEDA in a deployment, we only need to include `kedaSpec` in
    the deployment''s manifest. Consider the following spec:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经设置好了一切。让我们使用 KEDA 创建一个 Seldon 部署。与 HPA 类似，在部署中启用 KEDA，我们只需要在部署清单中包含 `kedaSpec`。考虑以下规范：
- en: '[PRE33]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: '`serverAddress` is the address of the Prometheus server within the cluster,
    it should be the URL of the Prometheus service, we can check the service with
    `kubectl get svc -n seldon-monitoring`. When the metric value surpasses `threshold`,
    the scaling will be triggered. The `query` is the average number of requests per
    second across running replicas, which is the metric we want to monitor.'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '`serverAddress` 是集群中 Prometheus 服务器的地址，它应该是 Prometheus 服务的 URL，我们可以通过 `kubectl
    get svc -n seldon-monitoring` 来检查服务。当指标值超过 `threshold` 时，将触发缩放。`query` 是运行中副本的每秒平均请求数，这是我们要监控的指标。'
- en: 'Apply this [manifest](https://github.com/tintn/ml-model-deployment-tutorials/blob/main/scalable-serving/cifar10-deploy-keda.yaml)
    to deploy the model:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 应用这个 [清单](https://github.com/tintn/ml-model-deployment-tutorials/blob/main/scalable-serving/cifar10-deploy-keda.yaml)
    来部署模型：
- en: '[PRE34]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Let’s trigger the autoscaling by sending requests to the deployment:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 通过向部署发送请求来触发自动缩放：
- en: '[PRE35]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'After a few seconds, you can see a new pod created:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 几秒钟后，你可以看到一个新的 pod 被创建：
- en: '[PRE36]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: Similar to HPA, downscaling will be triggered after a certain period (5 minutes
    by default) of low traffic.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于 HPA，在一段低流量时间（默认5分钟）后，将触发缩放。
- en: '[PRE37]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: Conclusion
  id: totrans-111
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: We’ve learned how to deploy machine learning models to Kubernetes clusters with
    Seldon Core. Although we mainly focused on deploying PyTorch models, the procedures
    shown in this guide may be used to deploy models from other frameworks.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经学习了如何使用 Seldon Core 将机器学习模型部署到 Kubernetes 集群中。虽然我们主要关注于部署 PyTorch 模型，但本指南中展示的程序也可以用于部署其他框架的模型。
- en: We’ve also made the deployments scalable using HPA and KEDA. Compared to HPA,
    KEDA provides more flexible ways to scale the system based on Prometheus metrics
    (or other supported scalers from KEDA). Technically, we can implement any scaling
    rules from metrics that can be fetched from the Prometheus server.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还使用 HPA 和 KEDA 使部署具有可扩展性。与 HPA 相比，KEDA 提供了更多基于 Prometheus 指标（或 KEDA 支持的其他扩展器）灵活的缩放方式。从技术上讲，我们可以实现从
    Prometheus 服务器获取的任何指标的缩放规则。
- en: '*Originally published at* [*https://tintn.github.io*](https://tintn.github.io/Scalable-Serving-with-Kubernetes-and-Seldon-Core/)
    *on January 9, 2023.*'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '*原文发表于* [*https://tintn.github.io*](https://tintn.github.io/Scalable-Serving-with-Kubernetes-and-Seldon-Core/)
    *2023年1月9日。*'
