- en: 'QLoRa: Fine-Tune a Large Language Model on Your GPU'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: QLoRa：在你的GPU上微调大型语言模型
- en: 原文：[https://towardsdatascience.com/qlora-fine-tune-a-large-language-model-on-your-gpu-27bed5a03e2b?source=collection_archive---------1-----------------------#2023-05-30](https://towardsdatascience.com/qlora-fine-tune-a-large-language-model-on-your-gpu-27bed5a03e2b?source=collection_archive---------1-----------------------#2023-05-30)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/qlora-fine-tune-a-large-language-model-on-your-gpu-27bed5a03e2b?source=collection_archive---------1-----------------------#2023-05-30](https://towardsdatascience.com/qlora-fine-tune-a-large-language-model-on-your-gpu-27bed5a03e2b?source=collection_archive---------1-----------------------#2023-05-30)
- en: Fine-tuning models with billions of parameters is now possible on consumer hardware
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在消费级硬件上微调拥有数十亿参数的模型现在已经成为可能
- en: '[](https://medium.com/@bnjmn_marie?source=post_page-----27bed5a03e2b--------------------------------)[![Benjamin
    Marie](../Images/3ea1ad230cb1e67610418a8e36a5e5dd.png)](https://medium.com/@bnjmn_marie?source=post_page-----27bed5a03e2b--------------------------------)[](https://towardsdatascience.com/?source=post_page-----27bed5a03e2b--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----27bed5a03e2b--------------------------------)
    [Benjamin Marie](https://medium.com/@bnjmn_marie?source=post_page-----27bed5a03e2b--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@bnjmn_marie?source=post_page-----27bed5a03e2b--------------------------------)[![本杰明·玛丽](../Images/3ea1ad230cb1e67610418a8e36a5e5dd.png)](https://medium.com/@bnjmn_marie?source=post_page-----27bed5a03e2b--------------------------------)[](https://towardsdatascience.com/?source=post_page-----27bed5a03e2b--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----27bed5a03e2b--------------------------------)
    [本杰明·玛丽](https://medium.com/@bnjmn_marie?source=post_page-----27bed5a03e2b--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fad2a414578b3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fqlora-fine-tune-a-large-language-model-on-your-gpu-27bed5a03e2b&user=Benjamin+Marie&userId=ad2a414578b3&source=post_page-ad2a414578b3----27bed5a03e2b---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----27bed5a03e2b--------------------------------)
    ·6 min read·May 30, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F27bed5a03e2b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fqlora-fine-tune-a-large-language-model-on-your-gpu-27bed5a03e2b&user=Benjamin+Marie&userId=ad2a414578b3&source=-----27bed5a03e2b---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fad2a414578b3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fqlora-fine-tune-a-large-language-model-on-your-gpu-27bed5a03e2b&user=Benjamin+Marie&userId=ad2a414578b3&source=post_page-ad2a414578b3----27bed5a03e2b---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----27bed5a03e2b--------------------------------)
    ·6 min read·2023年5月30日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F27bed5a03e2b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fqlora-fine-tune-a-large-language-model-on-your-gpu-27bed5a03e2b&user=Benjamin+Marie&userId=ad2a414578b3&source=-----27bed5a03e2b---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F27bed5a03e2b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fqlora-fine-tune-a-large-language-model-on-your-gpu-27bed5a03e2b&source=-----27bed5a03e2b---------------------bookmark_footer-----------)![](../Images/2bf17a5fcbca8e98c743ab818939b6ab.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F27bed5a03e2b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fqlora-fine-tune-a-large-language-model-on-your-gpu-27bed5a03e2b&source=-----27bed5a03e2b---------------------bookmark_footer-----------)![](../Images/2bf17a5fcbca8e98c743ab818939b6ab.png)'
- en: Illustration by the author.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 插图由作者提供。
- en: Most large language models (LLM) are too big to be fine-tuned on consumer hardware.
    For instance, to fine-tune a 65 billion parameters model we need more than 780
    Gb of GPU memory. This is equivalent to ten A100 80 Gb GPUs. In other words, you
    would need cloud computing to fine-tune your models.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数大型语言模型（LLM）过于庞大，无法在消费级硬件上进行微调。例如，微调一个拥有650亿参数的模型需要超过780GB的GPU内存。这相当于十个A100
    80GB的GPU。换句话说，你需要云计算才能微调你的模型。
- en: Now, with QLoRa ([Dettmers et al., 2023](https://arxiv.org/pdf/2305.14314.pdf)),
    you could do it with only one A100.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，使用QLoRa（[Dettmers等人，2023](https://arxiv.org/pdf/2305.14314.pdf)），你只需一个A100就可以做到这一点。
- en: In this blog post, I will introduce QLoRa. I will briefly describe how it works
    and we will see how to use it to fine-tune a GPT model with 20 billion parameters,
    on your GPU.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇博客文章中，我将介绍QLoRa。我将简要描述它的工作原理，并展示如何在你的GPU上使用它来微调一个拥有200亿参数的GPT模型。
- en: '*Note: I used my own nVidia RTX 3060 12 Gb to run all the commands in this
    post. You can also use a free instance of Google Colab to achieve the same results.
    If you want to use a GPU with a smaller memory, you would have to use a smaller
    LLM.*'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '*注意：我使用了自己的nVidia RTX 3060 12 Gb运行了本文中的所有命令。你也可以使用Google Colab的免费实例获得相同的结果。如果你想使用内存较小的GPU，你需要使用更小的大语言模型。*'
- en: 'I provide all the necessary code to run QLoRa for fine-tuning in this article.
    If you don’t want to code it by yourself, [I also created a Google Colab notebook
    on The Kaitchup (my substack newsletter). This is notebook #2.](https://newsletter.kaitchup.com/p/notebooks)'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '我在这篇文章中提供了运行QLoRa进行微调所需的所有代码。如果你不想自己编写代码，[我还在The Kaitchup（我的Substack通讯）上创建了一个Google
    Colab笔记本。这是笔记本 #2。](https://newsletter.kaitchup.com/p/notebooks)'
- en: 'QLoRa: Quantized LLMs with Low-Rank Adapters'
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: QLoRa：低秩适配器的量化大语言模型
- en: In June 2021, [Hu et al. (2021)](https://arxiv.org/abs/2106.09685) introduced
    low-rank adapters (LoRa) for LLMs.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在2021年6月，[Hu等人（2021）](https://arxiv.org/abs/2106.09685)介绍了针对大语言模型的低秩适配器（LoRa）。
- en: LoRa adds a tiny amount of trainable parameters, i.e., adapters, for each layer
    of the LLM and freezes all the original parameters. For fine-tuning, we only have
    to update the adapter weights which significantly reduces the memory footprint.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: LoRa为LLM的每一层添加了一小部分可训练的参数，即适配器，并冻结了所有原始参数。对于微调，我们只需更新适配器权重，这大大减少了内存占用。
- en: 'QLoRa goes three steps further by introducing: 4-bit quantization, double quantization,
    and the exploitation of nVidia unified memory for paging.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: QLoRa通过引入：4-bit量化、双重量化以及利用nVidia统一内存进行分页，进一步向前迈进了三步。
- en: 'In a few words, each one of these steps works as follows:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，每个步骤的工作原理如下：
- en: '**4-bit NormalFloat quantization**: This is a method that improves upon quantile
    quantization. It ensures an equal number of values in each quantization bin. This
    avoids computational issues and errors for outlier values.'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**4-bit NormalFloat 量化**：这是一种改进量化分位数的量化方法。它确保每个量化箱中有相等数量的值。这可以避免计算问题和对异常值的错误。'
- en: '**Double quantization**: The authors of QLoRa define it as follows: “*the process
    of quantizing the quantization constants for additional memory savings.*”'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**双重量化**：QLoRa的作者将其定义为：“*对量化常数进行量化以节省额外内存的过程。*”'
- en: '**Paging with unified memory**: It relies on the NVIDIA Unified Memory feature
    and automatically handles page-to-page transfers between the CPU and GPU. It ensures
    error-free GPU processing, especially in situations where the GPU may run out
    of memory.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**统一内存分页**：它依赖于NVIDIA统一内存功能，自动处理CPU和GPU之间的页到页传输。它确保了无错误的GPU处理，尤其是在GPU可能内存不足的情况下。'
- en: All of these steps drastically reduce the memory requirements for fine-tuning,
    while performing almost on par with standard fine-tuning.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些步骤都大幅减少了微调的内存要求，同时性能几乎与标准微调相当。
- en: Fine-tuning a GPT model with QLoRa
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用QLoRa对GPT模型进行微调
- en: 'Hardware requirements for QLoRa:'
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: QLoRa的硬件要求：
- en: '**GPU**: The following demo works on a GPU with 12 Gb of VRAM, for a model
    with less than 20 billion parameters, e.g., GPT-J. For instance, I ran it with
    my RTX 3060 12 Gb. If you have a bigger card with 24 Gb of VRAM, you can do it
    with a 20 billion parameter model, e.g., GPT-NeoX-20b.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**GPU**：以下演示在具有12 Gb VRAM的GPU上运行，适用于参数少于20亿的模型，例如GPT-J。例如，我在我的RTX 3060 12 Gb上运行了它。如果你有一张具有24
    Gb VRAM的更大显卡，你可以使用20亿参数的模型，例如GPT-NeoX-20b。'
- en: '**RAM**: I recommend a minimum of 6 Gb. Most recent computers have enough RAM.'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**RAM**：我建议至少6 Gb。大多数现代计算机都有足够的RAM。'
- en: '**Hard drive**: GPT-J and GPT-NeoX-20b are both very big models. I recommend
    at least 80 Gb of free space.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**硬盘**：GPT-J 和 GPT-NeoX-20b 都是非常大的模型。我建议至少有 80 Gb 的可用空间。'
- en: If your machine doesn’t meet these requirements, the free instance of Google
    Colab would be enough instead.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你的机器不满足这些要求，Google Colab 的免费实例也足够了。
- en: 'Software requirements for QLoRa:'
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: QLoRa的软硬件要求：
- en: We need CUDA. Make sure it is installed on your machine.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要CUDA。确保它已安装在你的机器上。
- en: 'We will also need to install all the dependencies:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还需要安装所有依赖项：
- en: '**bitsandbytes**: A library that contains all we need to quantize an LLM.'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**bitsandbytes**：一个包含量化LLM所需所有工具的库。'
- en: '**Hugging Face Transformers and Accelerate**: These are standard libraries
    that are used to efficiently train models from Hugging Face Hub.'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Hugging Face Transformers 和 Accelerate**：这些是用于高效训练来自 Hugging Face Hub 模型的标准库。'
- en: '**PEFT**: A library that provides the implementations for various methods to
    only fine-tune a small number of (extra) model parameters. We need it for LoRa.'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**PEFT**：一个提供各种方法实现的库，用于只微调少量（额外）模型参数。我们需要它用于LoRa。'
- en: '**Datasets**: This one is not a requirement. We will only use it to get a dataset
    for fine-tuning. Of course, you can provide instead your own dataset.'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据集**：这不是必需的。我们仅用它来获取一个用于微调的数据集。当然，你可以提供你自己的数据集。'
- en: 'We can get all of them with PIP:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过PIP获取它们全部：
- en: '[PRE0]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Next, we can start writing the Python script.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们可以开始编写Python脚本。
- en: Loading and Quantization of a GPT model
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: GPT模型的加载和量化
- en: We need the following imports to load and quantize an LLM.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要以下导入来加载和量化LLM。
- en: '[PRE1]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'For this demo, we will fine-tune the [GPT NeoX](https://huggingface.co/EleutherAI/gpt-neox-20b)
    model pre-trained by [EleutherAI](https://www.eleuther.ai/). This is a model with
    20 billion parameters. *Note: GPT NeoX has a permissive license (Apache 2.0) that
    allows commercial use.*'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个演示，我们将微调 [GPT NeoX](https://huggingface.co/EleutherAI/gpt-neox-20b) 模型，这个模型由
    [EleutherAI](https://www.eleuther.ai/) 预训练。这个模型拥有200亿个参数。 *注意：GPT NeoX具有允许商业使用的宽松许可（Apache
    2.0）。*
- en: 'We can get this model and the associated tokenizer from Hugging Face Hub:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以从Hugging Face Hub获取这个模型和相关的分词器：
- en: '[PRE2]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Then, we need to detail the configuration of the quantizer, as follows:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们需要详细说明量化器的配置，如下：
- en: '[PRE3]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'load_in_4bit: The model will be loaded in the memory with 4-bit precision.'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'load_in_4bit: 模型将以4-bit精度加载到内存中。'
- en: 'bnb_4bit_use_double_quant: We will do the double quantization proposed by QLoRa.'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'bnb_4bit_use_double_quant: 我们将进行QLoRa提议的双量化。'
- en: 'bnb_4bit_quant_type: This is the type of quantization. “nf4” stands for 4-bit
    NormalFloat.'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'bnb_4bit_quant_type: 这是量化类型。“nf4”表示4-bit NormalFloat。'
- en: 'bnb_4bit_compute_dtype: While we load and store the model in 4-bit, we will
    partially dequantize it when needed and do all the computations with a 16-bit
    precision (bfloat16).'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'bnb_4bit_compute_dtype: 当我们以4-bit加载和存储模型时，我们会在需要时部分去量化，并以16-bit精度（bfloat16）进行所有计算。'
- en: 'So now we can load the model in 4-bit:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 所以现在我们可以以4-bit加载模型：
- en: '[PRE4]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Then, we enable gradient checkpointing:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们启用梯度检查点：
- en: '[PRE5]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Preprocessing the GPT model for LoRa
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 为LoRa预处理GPT模型
- en: This is where we use PEFT. We prepare the model for LoRa, adding trainable adapters
    for each layer.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 这里我们使用PEFT。我们为LoRa准备模型，为每一层添加可训练的适配器。
- en: '[PRE6]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: In LoraConfig, you can play with r, alpha, and dropout to obtain better results
    on your task. You can find more options and details in the [PEFT repository](https://github.com/huggingface/peft/tree/main).
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在LoraConfig中，你可以调整r、alpha和dropout以获得更好的任务结果。你可以在 [PEFT仓库](https://github.com/huggingface/peft/tree/main)
    找到更多选项和细节。
- en: With LoRa, we add only 8 million parameters. We will only train these parameters
    and freeze everything else. Fine-tuning should be fast.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 使用LoRa，我们仅增加了800万参数。我们只训练这些参数，冻结其他所有参数。微调应该会很快。
- en: Get your dataset ready
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备好你的数据集
- en: For this demo, I use the “english_quotes” dataset. [This is a dataset made of
    famous quotes](https://huggingface.co/datasets/Abirate/english_quotes) distributed
    under a [CC BY 4.0 license](https://creativecommons.org/licenses/by/4.0/).
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个演示，我使用“english_quotes”数据集。 [这是一个由著名名言组成的数据集](https://huggingface.co/datasets/Abirate/english_quotes)
    以 [CC BY 4.0 许可](https://creativecommons.org/licenses/by/4.0/) 进行分发。
- en: '[PRE7]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Fine-tuning GPT-NeoX-20B with QLoRa
  id: totrans-63
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用QLoRa微调GPT-NeoX-20B
- en: Finally, the fine-tuning with Hugging Face Transformers is very standard.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，使用Hugging Face Transformers进行微调是非常标准的。
- en: '[PRE8]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Don’t forget optim=”paged_adamw_8bit”. It activates the paging for better memory
    management. Without it, we get out-of-memory errors.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 别忘了optim=”paged_adamw_8bit”。它激活了分页以更好地管理内存。没有它，我们会遇到内存不足的错误。
- en: Running this fine-tuning should only take 5 minutes on Google Colab.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在Google Colab上运行这个微调只需5分钟。
- en: The VRAM consumption should peak at 15 Gb.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: VRAM消耗应达到15Gb峰值。
- en: And that’s it, we fine-tuned an LLM for free!
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 就这样，我们免费微调了一个LLM！
- en: Does it work? Let’s try inference.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 它有效吗？让我们尝试推理。
- en: GPT Inference with QLoRa
  id: totrans-71
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: GPT推理与QLoRa
- en: 'The QLoRa model we fine-tuned can be directly used with the standard Hugging
    Face Transformers’ inference, as follows:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 我们微调的QLoRa模型可以直接用于标准的Hugging Face Transformers推理，具体如下：
- en: '[PRE9]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'You should get this quote as output:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该得到以下输出：
- en: '[PRE10]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: We got the expected quote. Not bad for 5 minutes of fine-tuning!
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到了预期的报价。对于5分钟的微调来说还不错！
- en: Conclusion
  id: totrans-77
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: Large language models got bigger but, at the same time, we finally got the tools
    to do fine-tuning and inference on consumer hardware.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型变得更大，但同时，我们终于拥有了在消费者硬件上进行微调和推理的工具。
- en: Thanks to LoRa, and now QLoRa, we can fine-tune models with billion parameters
    without relying on cloud computing and without a significant drop in performance
    according to the [QLoRa paper](https://arxiv.org/abs/2106.09685).
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 多亏了LoRa，现在QLoRa的出现使我们能够在不依赖云计算的情况下微调具有亿级参数的模型，并且根据[QLoRa论文](https://arxiv.org/abs/2106.09685)性能下降并不显著。
- en: If you have any problem running the code, please drop a comment, and I’ll try
    to help. You can also find more information about [QLoRa implementation in the
    official GitHub repository](https://github.com/artidoro/qlora).
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你在运行代码时遇到任何问题，请留言，我会尽量帮助你。你还可以在[官方GitHub仓库中找到关于QLoRa实现的更多信息](https://github.com/artidoro/qlora)。
- en: 'If you want to deploy an LLM, have a look at my tutorial using nVidia Triton
    Inference Server:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想部署LLM，可以查看我使用nVidia Triton推理服务器的教程：
- en: '[](/deploy-your-local-gpt-server-with-triton-a825d528aa5d?source=post_page-----27bed5a03e2b--------------------------------)
    [## Deploy Your Local GPT Server With Triton'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/deploy-your-local-gpt-server-with-triton-a825d528aa5d?source=post_page-----27bed5a03e2b--------------------------------)
    [## 使用Triton部署本地GPT服务器'
- en: How to run large language models on your local server
  id: totrans-83
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 如何在本地服务器上运行大型语言模型
- en: towardsdatascience.com](/deploy-your-local-gpt-server-with-triton-a825d528aa5d?source=post_page-----27bed5a03e2b--------------------------------)
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/deploy-your-local-gpt-server-with-triton-a825d528aa5d?source=post_page-----27bed5a03e2b--------------------------------)
- en: '*If you like this article and would be interested in reading the next ones,
    the best way to support my work is to subscribe to The Kaitchup:*'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '*如果你喜欢这篇文章并且对阅读下一篇感兴趣，支持我工作的最佳方式是订阅Kaitchup：*'
- en: '[](https://newsletter.kaitchup.com/?source=post_page-----27bed5a03e2b--------------------------------)
    [## The Kaitchup - AI on a Budget | Benjamin Marie | Substack'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://newsletter.kaitchup.com/?source=post_page-----27bed5a03e2b--------------------------------)
    [## Kaitchup - AI预算低 | Benjamin Marie | Substack'
- en: Weekly tutorials, tips, and news on fine-tuning, running, and serving large
    language models on your computer. The…
  id: totrans-87
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 每周提供有关微调、运行和服务大型语言模型的教程、技巧和新闻。…
- en: newsletter.kaitchup.com](https://newsletter.kaitchup.com/?source=post_page-----27bed5a03e2b--------------------------------)
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: newsletter.kaitchup.com](https://newsletter.kaitchup.com/?source=post_page-----27bed5a03e2b--------------------------------)
