- en: Monte Carlo Methods
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 蒙特卡洛方法
- en: 原文：[https://towardsdatascience.com/monte-carlo-methods-b2504976c415](https://towardsdatascience.com/monte-carlo-methods-b2504976c415)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/monte-carlo-methods-b2504976c415](https://towardsdatascience.com/monte-carlo-methods-b2504976c415)
- en: '[A Baby Robot’s Guide To Reinforcement Learning](https://towardsdatascience.com/tagged/baby-robot-guide)'
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '[《婴儿机器人强化学习指南》](https://towardsdatascience.com/tagged/baby-robot-guide)'
- en: 'An Introduction to Reinforcement Learning: Part 4'
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 强化学习简介：第4部分
- en: '[](https://medium.com/@tinkertytonk?source=post_page-----b2504976c415--------------------------------)[![Steve
    Roberts](../Images/14384b0516dfd3dc792972b221d787ec.png)](https://medium.com/@tinkertytonk?source=post_page-----b2504976c415--------------------------------)[](https://towardsdatascience.com/?source=post_page-----b2504976c415--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----b2504976c415--------------------------------)
    [Steve Roberts](https://medium.com/@tinkertytonk?source=post_page-----b2504976c415--------------------------------)'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@tinkertytonk?source=post_page-----b2504976c415--------------------------------)[![Steve
    Roberts](../Images/14384b0516dfd3dc792972b221d787ec.png)](https://medium.com/@tinkertytonk?source=post_page-----b2504976c415--------------------------------)[](https://towardsdatascience.com/?source=post_page-----b2504976c415--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----b2504976c415--------------------------------)
    [Steve Roberts](https://medium.com/@tinkertytonk?source=post_page-----b2504976c415--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----b2504976c415--------------------------------)
    ·26 min read·Aug 26, 2023
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page-----b2504976c415--------------------------------)
    ·阅读时间26分钟·2023年8月26日
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/2bce2525d4bd8752cf2f67f728bf7e67.png)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2bce2525d4bd8752cf2f67f728bf7e67.png)'
- en: All images by author
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 所有图片由作者提供
- en: Introduction
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: 'Once again we’re off to the casino, and this time it’s situated in sunny Monte
    Carlo, made famous by its appearance in the classic movie [*Madagascar 3: Europe’s
    Most Wanted*](https://en.wikipedia.org/wiki/Madagascar_3:_Europe%27s_Most_Wanted)(although
    there’s a slight chance that it was already famous).'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 我们再次前往赌场，这次它位于阳光明媚的蒙特卡洛，这里因在经典电影[*马达加斯加3：欧洲大追捕*](https://en.wikipedia.org/wiki/Madagascar_3:_Europe%27s_Most_Wanted)中的出现而闻名（虽然它可能早已出名）。
- en: In our last visit to a casino we looked at the [***multi-armed bandit***](https://medium.com/towards-data-science/multi-armed-bandits-part-1-b8d33ab80697)and
    used this as a way to visualise the problem of how to choose the best action when
    confronted with many possible actions.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们上次访问赌场时，我们研究了[***多臂赌博机***](https://medium.com/towards-data-science/multi-armed-bandits-part-1-b8d33ab80697)，并将其作为一个视觉化的方式来解决如何在面对多种可能行动时选择最佳行动的问题。
- en: In terms of ***Reinforcement Learning*** the bandit problem can be thought of
    as representing a single state and the actions available within that state. *Monte
    Carlo* methods extend this idea to cover multiple, interrelated, states.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 从***强化学习***的角度来看，赌博机问题可以视为代表一个单一状态以及该状态下可用的行动。*蒙特卡洛*方法将这一概念扩展到覆盖多个相互关联的状态。
- en: Additionally, in the previous problems we’ve looked at, we’ve always been given
    a full model of the environment. This model defines both the transition probabilities,
    that describe the chances of moving from one state to the next, and the reward
    received for making this transition.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，在我们之前研究的问题中，我们总是得到了环境的完整模型。这个模型定义了转移概率，即描述从一个状态转移到下一个状态的机会，以及为此转移所获得的奖励。
- en: In *Monte Carlo* methods this isn’t the case. No model is given and instead
    the agent must discover the properties of the environment through exploration,
    gathering information as it moves from one state to the next. In other words,
    *Monte Carlo methods learn from experience.*
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在*蒙特卡洛*方法中情况并非如此。没有给定模型，代理必须通过探索来发现环境的属性，随着从一个状态转移到下一个状态而收集信息。换句话说，*蒙特卡洛方法从经验中学习*。
- en: '![](../Images/9cdf6c8fcb0d1b9c006591725d35fa90.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9cdf6c8fcb0d1b9c006591725d35fa90.png)'
- en: The examples in this article make use of the custom [**Baby Robot Gym Environment**](https://github.com/WhatIThinkAbout/BabyRobotGym)and
    all of the related code for this article can be found on [Github](https://github.com/WhatIThinkAbout/BabyRobot/tree/master/Reinforcement_Learning).
  id: totrans-16
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 本文中的例子使用了自定义的[**Baby Robot Gym 环境**](https://github.com/WhatIThinkAbout/BabyRobotGym)，与本文相关的所有代码可以在[Github](https://github.com/WhatIThinkAbout/BabyRobot/tree/master/Reinforcement_Learning)上找到。
- en: ''
  id: totrans-17
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Additionally, an interactive version of this article can be found in [**notebook**](https://github.com/WhatIThinkAbout/BabyRobot/blob/master/Reinforcement_Learning/Part%204%20-%20Monte%20Carlo%20Methods.ipynb)
    form, where you can actually run all of the code snippets described below.
  id: totrans-18
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 此外，您可以在[**notebook**](https://github.com/WhatIThinkAbout/BabyRobot/blob/master/Reinforcement_Learning/Part%204%20-%20Monte%20Carlo%20Methods.ipynb)格式中找到这篇文章的互动版本，在那里您可以实际运行下面描述的所有代码片段。
- en: ''
  id: totrans-19
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'All of the previous articles in this series can be found here: [**A Baby Robot’s
    Guide To Reinforcement Learning**](https://towardsdatascience.com/tagged/baby-robot-guide)**.**'
  id: totrans-20
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 本系列之前的所有文章可以在这里找到：[**小机器人的强化学习指南**](https://towardsdatascience.com/tagged/baby-robot-guide)**.**
- en: ''
  id: totrans-21
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: And, for a quick recap of the theory and terminology used in this article, check
    out [S**tate Values and Policy Evaluation in 5 minutes**](https://medium.com/@tinkertytonk/state-values-and-policy-evaluation-in-5-minutes-f3e00f3c1a50).
  id: totrans-22
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 并且，为了快速回顾本文中使用的理论和术语，请查看[**状态值和策略评估5分钟**](https://medium.com/@tinkertytonk/state-values-and-policy-evaluation-in-5-minutes-f3e00f3c1a50)。
- en: Monte Carlo Prediction
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 蒙特卡洛预测
- en: In the prediction problem we want to find how good it is to be in a particular
    state of the environment. This “*goodness*” is represented by the state value,
    which is defined as the expected reward that can be obtained when starting in
    that state and then following the current policy for all subsequent states.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在预测问题中，我们想要找到在环境中某一特定状态下的好坏。这种“*好坏*”由状态值表示，状态值定义为在该状态下开始，然后按照当前策略继续进行所有后续状态时可以获得的期望奖励。
- en: When we have full knowledge about the environment, and know the transition probabilities
    and rewards, we can simply use [***Dynamic Programming***](https://medium.com/towards-data-science/state-values-and-policy-evaluation-ceefdd8c2369#e996)
    to iteratively calculate the value for each state.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们对环境有充分了解，知道转移概率和奖励时，我们可以简单地使用[***动态规划***](https://medium.com/towards-data-science/state-values-and-policy-evaluation-ceefdd8c2369#e996)来迭代计算每个状态的值。
- en: In practice, it’s unlikely that a system’s transition probabilities are known
    in advance. Therefore, to estimate how likely it is to move from one state to
    another, it’s possible to observe multiple episodes and then take the average.
    This approach, of taking random samples to calculate estimates, is known as ***Monte
    Carlo Sampling****.*
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，系统的转移概率通常是未知的。因此，为了估计从一个状态转移到另一个状态的可能性，可以观察多个回合，然后取平均值。这种通过随机样本计算估计值的方法被称为***蒙特卡洛采样***。
- en: 'Consider the level, shown in *figure 1* below, where Baby Robot currently finds
    himself:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 请考虑下面的*图1*中显示的关卡，小机器人目前正处于其中：
- en: '![](../Images/45fdc24b65b5e6244f7afd5d5161e53a.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/45fdc24b65b5e6244f7afd5d5161e53a.png)'
- en: 'Figure 1: A level containing a glass wall and the co-ordinates on this level.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：包含玻璃墙的关卡以及该关卡上的坐标。
- en: 'At first glance this level appears to be rather simple, with a short path from
    the start of the level to the exit. However, there are 2 obstacles of note:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 初看之下，这个关卡似乎相当简单，从关卡的开始到出口有一条短路径。然而，有两个值得注意的障碍：
- en: In the top-middle square (coordinate [1,0]) there is a large puddle. As we’ve
    seen before, Baby Robot doesn’t like puddles. They take longer to move through,
    incurring a negative reward of -4, and can cause him to skid.
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在顶部中间的方格（坐标[1,0]）有一个大水坑。如我们之前所见，小机器人不喜欢水坑。它们移动起来耗时更长，会带来-4的负面奖励，并可能导致小机器人打滑。
- en: When a skid occurs Baby Robot won’t reach the target state. Normally this would
    result in him moving to one of the other possible states, but in this case there
    are no other possible states, so he’ll stay exactly where he is and receive another
    -4 penalty.
  id: totrans-32
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 当发生打滑时，小机器人不会到达目标状态。通常这将导致他移动到其他可能的状态之一，但在这种情况下没有其他可能的状态，因此他将停留在原地并再次收到-4的惩罚。
- en: If Baby Robot moves into this puddle there’s a good chance he’ll become stuck
    for several time periods and receive a large negative reward. It would be best
    to avoid this puddle!
  id: totrans-33
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果小机器人进入这个水坑，有很大的可能性他会被困住几个时间段，并且会收到一个大的负面奖励。最好避免这个水坑！
- en: The thick blue line, between the cells [1,1] and [1,2], represents a glass wall.
    This is a new type of challenge that Baby Robot hasn’t encountered before.
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在[1,1]和[1,2]之间的粗蓝线表示一面玻璃墙。这是一种小机器人以前未遇到过的新类型挑战。
- en: Unlike standard walls, Baby Robot can’t see glass walls and may therefore select
    an action that causes him to walk into the wall. When this happens he’ll bounce
    off the wall and, rather than reaching the target state, end up in the opposite
    state. Also, he’ll be given a negative reward penalty of -1 for the additional
    time required to make the move.
  id: totrans-35
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 与标准墙不同，Baby Robot 看不见玻璃墙，因此可能会选择一个使他撞上墙的动作。当这种情况发生时，他会从墙上弹开，而不是到达目标状态，反而会到达相对的状态。此外，他还会因为需要额外的时间来移动而受到-1的负奖励惩罚。
- en: 'In this level there are 2 possible opportunities for walking into the glass
    wall:'
  id: totrans-36
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在这个关卡中，有2个可能会碰到玻璃墙的机会：
- en: '- if he moves South from cell [1,1] he’ll instead end up in the puddle at [1,0]
    and receive a reward of -5 (-4 for moving into a puddle and -1 for hitting the
    glass wall).'
  id: totrans-37
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '- 如果他从单元[1,1]向南移动，他将最终到达[1,0]的水坑，并获得-5的奖励（-4因进入水坑而受到的惩罚和-1因撞上玻璃墙而受到的惩罚）。'
- en: '- if he moves North from [1,2], instead of arriving at [1,1] he’ll actually
    bounce off the wall and end up at the exit. In this case he’ll be given a reward
    of -2 (-1 wall penalty and -1 for moving to a dry square).'
  id: totrans-38
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '- 如果他从[1,2]向北移动，而不是到达[1,1]，他实际上会撞上墙并最终到达出口。在这种情况下，他将获得-2的奖励（-1墙壁惩罚和-1移动到干燥区域的奖励）。'
- en: As mentioned above, when we have complete information about a system, and know
    all of its transition probabilities and rewards, we can use [***Policy Evaluation***](/state-values-and-policy-evaluation-ceefdd8c2369)
    to calculate the state values.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 如上所述，当我们对系统拥有完整信息，并且知道所有的转移概率和奖励时，我们可以使用[***策略评估***](/state-values-and-policy-evaluation-ceefdd8c2369)来计算状态值。
- en: 'For this environment with a stochastic policy, that just chooses randomly between
    the available actions in each state, *Policy Evaluation* gives the following state
    values:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个随机选择每个状态下可用动作的环境，*策略评估*给出了以下状态值：
- en: '![](../Images/ca1595f14690cd7f1587371a059551b1.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ca1595f14690cd7f1587371a059551b1.png)'
- en: 'Figure 2: State values calculated using Policy Evaluation for a stochastic
    policy.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：使用策略评估计算的状态值，针对一个随机策略。
- en: 'Each of these state values represents the expected return from that state.
    So, at time ‘***t***’ if we start in state ‘***s***’ the value of a state under
    policy ‘**π’** is given by:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 每个状态值代表了从该状态开始的期望回报。因此，在时间‘***t***’时，如果我们从状态‘***s***’开始，策略‘**π**’下的状态值由下式给出：
- en: '![](../Images/c43934abe43cc9ce7b816ff139210ddf.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c43934abe43cc9ce7b816ff139210ddf.png)'
- en: 'Equation 1: The state value function under policy **π**'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 方程1：策略**π**下的状态值函数
- en: 'Where the return ***Gₜ,*** the total amount of reward accumulated over an episode,
    starting at time ‘*t’*, is given by:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 其中回报***Gₜ***，即从时间‘*t*’开始在一个回合中累积的总奖励，由下式给出：
- en: '![](../Images/824be1d71bcf7e19981eb46b7d055bb6.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/824be1d71bcf7e19981eb46b7d055bb6.png)'
- en: 'Equation 2: The discount return at time ‘t’'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 方程2：时间‘t’的折扣回报
- en: With a stochastic policy, for this simple level, we know that ultimately Baby
    Robot will reach the exit and the episode will terminate. Therefore we can set
    the discount factor ‘*γ*’ to 1\. Under these conditions the state value gives
    the average future total reward that can be expected when starting in that state.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在随机策略下，对于这个简单的关卡，我们知道最终Baby Robot会到达出口，回合将结束。因此，我们可以将折扣因子‘*γ*’设置为1。在这些条件下，状态值表示从该状态开始时可以预期的未来总奖励的平均值。
- en: In other words, the value of a state is the total of all future rewards obtained
    from that state, until completion of an episode, averaged over infinite episodes.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，状态值是从该状态开始，到回合结束时所有未来奖励的总和，平均过无限回合。
- en: Therefore, to get a simple estimate of the state value, we can simply take the
    average return of multiple episodes that commence in that state. The more episodes
    we run the better our estimate will be. This is the exact approach taken by ***Monte
    Carlo*** ***methods***.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，为了得到状态值的简单估计，我们可以简单地取从该状态开始的多个回合的平均回报。运行的回合越多，我们的估计就会越好。这正是***蒙特卡洛*** ***方法***所采用的方式。
- en: '![](../Images/0036f1a191810e2eadbdc93112b14a26.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0036f1a191810e2eadbdc93112b14a26.png)'
- en: 'Figure 3: A sample episode with a trajectory that goes directly from the start
    state to the exit.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：一个从起始状态直接到达出口的回合示例。
- en: 'For example, from the start state, if Baby Robot was lucky enough to follow
    a path that took him directly to the exit (as shown in *figure 3* above), his
    trajectory would have the following states and rewards:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，从起始状态开始，如果Baby Robot足够幸运地沿着一条直接到达出口的路径（如上面*图3*所示），他的轨迹将具有以下状态和奖励：
- en: '[PRE0]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: So, for this episode, the return value is -5\. If we then ran the experiment
    again, with a stochastic policy that produces a different, random, path through
    the level, we’d get a different return value. By adding the returns from many
    such episodes and taking the average we’d get an estimate of the state value for
    the start state [0,1]. The more episodes we run, the closer this estimate would
    be to the true state value.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，对于这个回合，回报值为 -5。如果我们再运行一次实验，使用产生不同随机路径的随机策略，我们会得到不同的回报值。通过将许多这样的回合的回报相加并取平均，我们将得到起始状态
    [0,1] 的状态值估计。我们运行的回合越多，这个估计就越接近真实的状态值。
- en: For the initial state on this level, if we run multiple episodes and take the
    average return, we get an estimated state value of approximately -102, which is
    identical to the state value calculated using Policy Evaluation.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个层级的初始状态，如果我们运行多个回合并取平均回报，我们得到的估计状态值大约为 -102，这与使用策略评估计算的状态值相同。
- en: However, unlike Policy Evaluation, we’ve now calculated this state value purely
    by exploration, without knowing the model of the environment. Additionally, by
    following a random trajectory until termination of the episode, we’ve actually
    just performed our first Monte Carlo search.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，与策略评估不同，我们现在完全通过探索计算了这个状态的值，而不知道环境的模型。此外，通过跟随随机轨迹直到回合结束，我们实际上进行了第一次蒙特卡洛搜索。
- en: 'To see this in action, check out the accompanying Jupyter notebook: [**Part
    4 — Monte Carlo Methods.ipynb**](https://github.com/WhatIThinkAbout/BabyRobot/blob/master/Reinforcement_Learning/Part%204%20-%20Monte%20Carlo%20Methods.ipynb)'
  id: totrans-59
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '要查看实际效果，请查看附带的 Jupyter notebook: [**第4部分 — 蒙特卡洛方法.ipynb**](https://github.com/WhatIThinkAbout/BabyRobot/blob/master/Reinforcement_Learning/Part%204%20-%20Monte%20Carlo%20Methods.ipynb)'
- en: Calculating the return values for other states on the trajectory
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 计算轨迹上其他状态的回报值
- en: In the above example we saw that, by repeatedly running episodes from the start
    of the level, we could produce an estimate of the state value for that initial
    state. However, each episode passes through many states before finally reaching
    the terminal state, but we only considered the return values obtained from the
    start state and ignored all the other information gathered during the episode.
    This is very inefficient.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述示例中，我们看到，通过反复从层级的起始状态运行回合，我们可以生成该初始状态的状态值估计。然而，每个回合在最终到达终端状态之前会经过许多状态，但我们只考虑了从起始状态获得的回报值，而忽略了回合中收集的所有其他信息。这非常低效。
- en: A better approach is to calculate the return value for every state that’s seen
    during the episode. Then, over multiple random episodes, we can create an estimate
    for every state value that’s been visited.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 更好的方法是计算回合中看到的每个状态的回报值。然后，通过多次随机回合，我们可以为每个已访问状态创建一个估计。
- en: As you can see for the sample trajectory given above (*figure 3*), on his way
    to the exit, Baby Robot also visits the states [1,1], [2,1], [2,2] and [1,2].
    So, during this episode, we can also gather information for all of these states.
    For each state visited, we can consider the future rewards obtained from each
    state to be the return for that state. This then gives us an estimated return
    value for all states seen during the trajectory, not just the start state.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你在上面提供的样本轨迹中看到的（*图3*），在前往出口的过程中，Baby Robot 还访问了状态 [1,1]、[2,1]、[2,2] 和 [1,2]。因此，在这个回合中，我们还可以收集这些状态的信息。对于每个访问过的状态，我们可以将从该状态获得的未来奖励视为该状态的回报。这就给出了轨迹中所有看到的状态的估计回报值，而不仅仅是起始状态。
- en: 'For this sample trajectory we get the following return values for the states
    that have been visited:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个样本轨迹，我们得到以下已访问状态的回报值：
- en: '[PRE1]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: If we then average these values over many episodes we’ll get an increasingly
    accurate estimate of the state value for all states that have been visited.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们在多个回合中平均这些值，我们将得到一个越来越准确的已访问状态的状态值估计。
- en: First-Visit Monte Carlo
  id: totrans-67
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 首次访问蒙特卡洛
- en: When each state is visited only once during an episode, as in our example above,
    then it’s easy to calculate the return for each state — you just add up all future
    rewards from that state. But what happens when a state is seen multiple times?
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 当每个状态在一个回合中只访问一次时，如上例所示，那么计算每个状态的回报很简单——你只需将该状态的所有未来奖励相加。但如果一个状态被多次看到会发生什么呢？
- en: The simplest approach to calculate the value, for a state that can be visited
    repeatedly in a single episode, is to simply take the rewards obtained from the
    first time the state is visited until the end of the episode. When these returns
    are averaged over multiple episodes we can build up an estimate of the state value
    for every state that’s been visited. Unsurprisingly, this approach is known as
    ***First-Visit Monte Carlo***.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 对于一个在单个剧集中可以重复访问的状态，计算其价值的最简单方法是从第一次访问该状态开始，一直到剧集结束。将这些回报在多个剧集中平均，我们可以建立每个已访问状态的价值估计。不出所料，这种方法被称为***首次访问蒙特卡洛***。
- en: 'Example:'
  id: totrans-70
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 示例：
- en: 'One possible path from the entrance to the exit of the sample level is shown
    in Figure 4 below:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 从样本级别的入口到出口的一个可能路径如图4所示：
- en: '![](../Images/012cf48c2914afc0f580b31b78ab0e76.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/012cf48c2914afc0f580b31b78ab0e76.png)'
- en: 'Figure 4: A trajectory with repeated state visits'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：具有重复状态访问的轨迹
- en: 'This path has the following steps and rewards:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 这条路径包含以下步骤和奖励：
- en: '[PRE2]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: In this trajectory, rather than moving straight to the exit, on step 4 Baby
    Robot takes a step backwards to state [2,1]. After this he follows a direct path
    to the exit. As a result of this backwards step he visits states [2,1] and [2,2]
    twice.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在这条轨迹中，Baby Robot没有直接前往出口，而是在第4步向后移动到状态[2,1]。之后，他沿着直接路径前往出口。由于这一步向后移动，他访问了状态[2,1]和[2,2]两次。
- en: With *First-Visit Monte Carlo* we sum the rewards obtained from the first time
    a state is seen until the end of the episode, to give an estimate of that state’s
    value. When a state that’s been seen before is re-visited, its reward is still
    used as part of the return, but we don’t consider this to be a new trajectory
    from that state.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 使用*首次访问蒙特卡洛*方法，我们从第一次看到状态到剧集结束时累加获得的奖励，以给出该状态的价值估计。当一个已被见过的状态再次被访问时，其奖励仍然作为回报的一部分被使用，但我们不将此视为从该状态的新轨迹。
- en: 'For this trajectory the first-visit return values are shown below. Note that
    we don’t calculate returns from steps 5 and 6, for states that have already been
    seen:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个轨迹，首次访问回报值如下所示。请注意，我们不会计算步骤5和6的回报，对于那些已经被见过的状态：
- en: '[PRE3]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The steps of the method we’ve described above are detailed in the following
    Python Pseudo-Code, for the First-Visit Monte Carlo algorithm:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 我们上面描述的方法的步骤在以下Python伪代码中详细列出，用于首次访问蒙特卡洛算法：
- en: '[PRE4]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The points to note in the code above, for *First-Visit Monte Carlo*, are as
    follows:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 上述*首次访问蒙特卡洛*代码中的要点如下：
- en: We want to keep track of the episode returns and number of visits to each state,
    therefore we start by creating zero’d Numpy arrays for each of these. The array
    dimensions are set to the width and height of the environment.
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们想要跟踪剧集回报和每个状态的访问次数，因此我们开始为每个状态创建零初始化的Numpy数组。这些数组的维度设置为环境的宽度和高度。
- en: Run for multiple episodes to get the returns generated over multiple trajectories.
    Here the number of runs is defined by ‘*max_episodes*’.
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行多个剧集以获取在多个轨迹中生成的回报。在这里，运行次数由‘*max_episodes*’定义。
- en: Get the rewards for a single episode. This returns a list of all the states
    visited and the rewards received between the start and end of the episode.
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 获取单个剧集的奖励。这将返回一个从剧集开始到结束之间访问的所有状态及其获得的奖励的列表。
- en: Convert the rewards into returns. This is done by simply summing the rewards
    in reverse direction.
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将奖励转换为回报。这是通过简单地反向累加奖励来完成的。
- en: Take the first visit return value and add this value to the ‘*total_state_returns*’
    array. Additionally, for every state visited, we increment the count of the total
    number of visits to that state (i.e. we count only the number of first-visits).
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 获取首次访问回报值，并将该值添加到‘*total_state_returns*’数组中。此外，对于每个访问过的状态，我们增加该状态的访问总次数（即我们仅统计首次访问的次数）。
- en: Finally, after all episodes have completed, we divide the total first-visit
    rewards by the total number of visits, to get the average return for each state.
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最终，在所有剧集完成后，我们将总的首次访问奖励除以总的访问次数，以得到每个状态的平均回报。
- en: '![](../Images/ca5d0840fd7372352193d5e2010c4519.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ca5d0840fd7372352193d5e2010c4519.png)'
- en: 'Figure 5: First-Visit MC calculated state values and state first-visit count
    for 10000 episodes.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 图5：10000次剧集计算的首次访问蒙特卡洛状态值和状态首次访问计数。
- en: The state values, calculated using *First-Visit Monte Carlo*, running for 10000
    episodes on this level, are shown above. Comparing these values to the ones we
    calculated previously using Policy Evaluation (see *Figure 2*), we can see that,
    for this very simple level, the values are identical.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 使用*首次访问蒙特卡洛*计算的状态值，在这个级别上进行了 10000 次实验，如上所示。将这些值与我们之前使用策略评估计算的值（见*图 2*）进行比较，我们可以看到，对于这个非常简单的级别，值是相同的。
- en: This shows the power of Monte Carlo methods. By simply observing the results
    from a set of random episodes we’ve been able to calculate very good estimates
    of the state values and all without any knowledge of the underlying environment
    properties.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 这显示了蒙特卡洛方法的威力。通过简单地观察一组随机实验的结果，我们能够计算出状态值的非常好的估计，并且完全不需要了解基础环境属性。
- en: Sample Average Estimates
  id: totrans-93
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 样本平均估计
- en: In the example above, to calculate the mean state values, we kept track of the
    total return and total number of first visits to each state. When all the episodes
    were complete the total return was divided by the number of visits to get the
    estimated state values. However, since the total return is getting progressively
    larger this has the potential to cause problems.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述示例中，为了计算均值状态值，我们跟踪了每个状态的总回报和首次访问总次数。当所有实验完成后，将总回报除以访问次数，以获得估计的状态值。然而，由于总回报不断增大，这可能会导致问题。
- en: A better approach is to use a running mean where, as each new return value is
    obtained, we update the estimated state value based on the last estimate. Not
    only does this help avoid problems with storage and compute time, but it also
    lets us have an estimate of the state value at each step. We don’t need to wait
    until the process has finished to compute the average state value.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 更好的方法是使用运行均值，在获得每个新的回报值时，我们基于最后的估计来更新估计的状态值。这不仅有助于避免存储和计算时间的问题，还使我们能够在每一步都获得状态值的估计。我们不需要等到过程结束后才能计算平均状态值。
- en: 'The formula for calculating the new state value, in terms of the previous estimate
    and the return obtained from the latest episode, is shown below (where ’*n*’ is
    the number of visits to the state):'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 计算新状态值的公式，基于之前的估计和最新实验中获得的回报，如下所示（其中‘*n*’是访问状态的次数）：
- en: '![](../Images/9d836d45c403a2fad7d9be0624334936.png)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9d836d45c403a2fad7d9be0624334936.png)'
- en: 'Equation 3: The estimated value of state ‘s’ calculated in terms of the previous
    value and new return.'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 方程 3：计算状态‘s’的估计值，基于之前的值和新的回报。
- en: For a proof of this equation check out the [***Bandits article***](/multi-armed-bandits-part-1-b8d33ab80697),
    where we used a similar method to calculate a sample average estimate for the
    action value.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 要验证这个方程式，可以查看[***Bandits article***](/multi-armed-bandits-part-1-b8d33ab80697)，我们使用类似的方法来计算行动值的样本平均估计。
- en: Exploring Starts
  id: totrans-100
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索开始
- en: The second image in *Figure 5* above shows the state first-visit count. This
    number represents the number of episodes in which the state was visited at least
    once, as opposed to the total number of visits to that state. It is also equal
    to the number of trajectories that passed through this state on the way to the
    exit.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 5* 上的第二张图片显示了状态首次访问计数。这个数字代表了状态至少被访问过一次的实验次数，而不是该状态的总访问次数。它也等于经过这个状态的所有轨迹的数量。'
- en: In this simple environment there’s only a single route from the entrance to
    the exit (this is the direct path that we saw back in *Figure 2)*. To reach the
    exit Baby Robot has to pass through each of the states on this path. Therefore,
    all of the states on this trajectory have to be visited at least once during an
    episode and, as a result, all have 10,000 first-visits, equal to the total number
    of episodes.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个简单的环境中，从入口到出口只有一条单一的路线（这就是我们在*图 2*中看到的直接路径）。要到达出口，Baby Robot 必须经过这条路径上的每一个状态。因此，这条轨迹上的所有状态在一次实验中都必须被访问至少一次，因此，所有状态都有
    10,000 次首次访问，等于总实验次数。
- en: In contrast, the grid square containing the puddle (at grid location [1,0])
    doesn’t lie on the direct path to the exit. It’s not necessary to visit this state
    while moving to the exit and therefore this state only gets visited when an action,
    chosen at random by the policy, causes Baby Robot to move there. Consequently,
    it was only visited in 8,737 of the 10,000 episodes.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，包含水坑的网格方块（在网格位置 [1,0]）并不位于到出口的直接路径上。在移动到出口的过程中不需要访问这个状态，因此只有当策略随机选择的动作使
    Baby Robot 移动到这里时，这个状态才会被访问。因此，它仅在 10,000 次实验中的 8,737 次中被访问。
- en: From Baby Robot’s point of view this was a good thing, as it stopped him getting
    wet, but from the point of view of the accuracy of our state estimate it’s not
    so good, since this state was sampled fewer times than the other states. In general,
    the most frequently visited states will have more accurate estimates of the states
    values than the seldom seen states.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 从婴儿机器人的角度来看，这是件好事，因为它阻止了他变湿，但从我们状态估计的准确度来看，这并不理想，因为这个状态被采样的次数少于其他状态。一般来说，访问最频繁的状态会比不常见的状态具有更准确的状态值估计。
- en: To compensate for this we can instead choose to begin each episode in a different
    state and thereby get a more balanced accuracy for the state value estimates.
    Additionally, if our policy resulted in some states never being visited we could
    start some episodes in these states and, in this way, estimate a value for these
    states.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 为了弥补这一点，我们可以选择在不同的状态下开始每一集，从而获得更均衡的状态值估计的准确性。此外，如果我们的策略导致一些状态从未被访问，我们可以在这些状态下开始一些集，从而估计这些状态的值。
- en: This technique, of beginning each episode in a different state chosen from the
    total state space, is referred to as ***Exploring Starts***.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 这种从总状态空间中选择不同状态开始每集的技术被称为***探索性起始***。
- en: '![](../Images/641e5d964be68066d3ffce4a574f4c0a.png)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/641e5d964be68066d3ffce4a574f4c0a.png)'
- en: 'Figure 6: First-Visit MC with Exploring Starts. Calculated state values and
    state first-visit count for 10000 episodes.'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 图6：带有探索性起始的首次访问MC。计算的状态值和状态首次访问计数，针对10,000集。
- en: The state values and first-visit counts when using *Exploring Starts* for 10,000
    episodes is shown in *Figure 6* above. You can see that, in this case, the number
    of first-visits to most states has actually decreased. This is due to the average
    trajectory being shorter since, in most cases, the starting state will be closer
    to the exit. For a simple environment, such as this one, *exploring starts* doesn’t
    really give any benefit. Indeed, while *exploring starts* can help balance the
    visits to each state it may not always be possible or even desirable.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 使用*探索性起始*进行10,000集时的状态值和首次访问计数如*图6*所示。你可以看到，在这种情况下，大多数状态的首次访问次数实际上减少了。这是因为平均轨迹更短，因为在大多数情况下，起始状态会更接近出口。对于像这样的简单环境，*探索性起始*并不会带来实际的好处。事实上，虽然*探索性起始*可以帮助平衡对每个状态的访问，但这可能并不总是可行或甚至可取。
- en: When, as in this case, we’re simply running a simulation of an environment then
    its very easy to select where each episode will begin. If, on the other hand,
    this was an actual, real-life, maze that *Baby Robot* was navigating then selecting
    the start state would be considerably more difficult and, for certain environments,
    might actually be impossible.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们仅仅是在运行环境的模拟时，选择每集的开始位置非常简单。另一方面，如果这是一个实际的、真实生活中的迷宫，而*婴儿机器人*正在导航，则选择起始状态将变得相当困难，并且对于某些环境，可能实际上是不可能的。
- en: Additionally, although the values calculated for each state should be of a similar
    level of accuracy, this may not always be beneficial. Without *exploring starts*
    the states that are most frequently visited will have a higher level of accuracy
    than those that are rarely visited. This can be a good thing, since we’re concentrating
    our efforts on finding the state values for the most often seen states and not
    wasting time on those that would be seen infrequently or never visited. However,
    by not using *exploring starts* we run the risk of missing states that would potentially
    give high levels of return. As is always the case with *Reinforcement Learning*,
    its a balancing act between exploration and exploitation.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，尽管计算出的每个状态的值应该具有类似的准确度，但这不一定总是有益的。没有*探索性起始*的情况下，访问最频繁的状态将具有比那些不常访问的状态更高的准确度。这可能是件好事，因为我们集中精力寻找最常见状态的状态值，而不是浪费时间在那些不常见或从未访问的状态上。然而，不使用*探索性起始*则有可能错过那些可能会带来高回报的状态。正如*强化学习*中总是要面对的情况一样，这是探索与开发之间的平衡。
- en: Every-Visit Monte Carlo
  id: totrans-112
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 每次访问的蒙特卡洛
- en: So far, with both our experiments that began at the level’s start point and
    with *exploring starts*, we were using *First-Visit Monte Carlo*, in which the
    returns were taken from the first time a state was visited until the end of the
    episode.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，无论是我们从关卡起点开始的实验，还是使用*探索性起始*，我们都在使用*首次访问蒙特卡洛*，其中返回值是从状态第一次被访问起计算到本集结束。
- en: However, during any episode, a state may be visited multiple times (for example,
    see the sample trajectory given in *figure 4*, where the states [2,1] and [2,2]
    are both visited twice). So, another approach to calculating an estimate of the
    state value is to treat each visit to a state as the start of a separate trajectory
    and record the return from that visit until the episode terminates. This approach
    to estimating state values is known as ***Every-Visit Monte Carlo***.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在任何一集游戏中，一个状态可能会被多次访问（例如，参见*图4*中给出的样本轨迹，其中状态[2,1]和[2,2]都被访问了两次）。因此，另一种计算状态值估计的方法是将每次访问状态视为一个单独轨迹的开始，并记录该访问的回报，直到本集游戏终止。这种估计状态值的方法被称为***每次访问蒙特卡洛***。
- en: The results obtained for our sample level, when recording every visit to a state,
    are shown in *Figure 7* below. Although the test was run for the standard 10,000
    episodes, by recording every visit to a state the actual number of visits recorded
    to each state are in most cases much higher.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 对我们样本关卡的结果，在记录每次访问状态时，如下*图7*所示。尽管测试进行了标准的10,000次试验，但通过记录每次访问状态，实际上记录到每个状态的访问次数在大多数情况下要高得多。
- en: Both *First-Visit* and *Every-Visit Monte Carlo* are, given enough time, guaranteed
    to converge on the true state value. However, although *Every-Visit MC* records
    more visits to each state, it’s not clear that it actually gives better results
    than *First-Visit MC*. This is probably due to the information from the extra
    trajectories that are recorded by *Every-Visit MC* already being included in the
    return obtained from the *First-Visit* trajectory.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 无论是*首次访问*还是*每次访问蒙特卡洛*，只要时间足够，最终都能收敛到真实的状态值。然而，尽管*每次访问蒙特卡洛*记录了更多对每个状态的访问，但并不清楚它是否比*首次访问蒙特卡洛*实际能提供更好的结果。这可能是因为*每次访问蒙特卡洛*记录的额外轨迹信息已经包含在*首次访问*轨迹的回报中。
- en: '![](../Images/6745a7f9e9cb92c0b615b648bbaa5237.png)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6745a7f9e9cb92c0b615b648bbaa5237.png)'
- en: 'Figure 7: Every-Visit MC, calculated state values and state visit count for
    10,000 episodes.'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 图7：每次访问蒙特卡洛，计算出的状态值和10,000次试验的状态访问计数。
- en: Monte Carlo Control
  id: totrans-119
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 蒙特卡洛控制
- en: Using any of the Monte Carlo methods described above, we are able to generate
    a pretty good estimate for the state values for any supplied policy. However,
    although this lets us see the relative goodness of any state with respect to the
    other states, this doesn’t actually help us to navigate the level.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 使用上述任何蒙特卡洛方法，我们能够为任何给定的策略生成相当准确的状态值估计。然而，虽然这使我们能够看到相对于其他状态的相对优劣，但这实际上并不能帮助我们导航关卡。
- en: When we have full information about state transitions and rewards, we can use
    [*Dynamic Programming*](https://medium.com/towards-data-science/state-values-and-policy-evaluation-ceefdd8c2369#e996)
    to turn the [*Bellman Equations*](https://medium.com/towards-data-science/markov-decision-processes-and-bellman-equations-45234cce9d25#9799)
    into a set of update rules that can calculate the state values under the current
    policy. From these state values we can then select the action that gives the maximum
    expected return.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们掌握了有关状态转移和奖励的完整信息时，可以使用[*动态规划*](https://medium.com/towards-data-science/state-values-and-policy-evaluation-ceefdd8c2369#e996)将[*贝尔曼方程*](https://medium.com/towards-data-science/markov-decision-processes-and-bellman-equations-45234cce9d25#9799)转化为一组更新规则，以计算当前策略下的状态值。然后，我们可以从这些状态值中选择出能够带来最大预期回报的动作。
- en: 'This is summarised by the following equation for greedy policy ***π*** in state
    ***s*** :'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 这由以下方程总结，针对状态***s***的贪婪策略***π***：
- en: '![](../Images/195198deb83da4843bdf2853101b045c.png)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/195198deb83da4843bdf2853101b045c.png)'
- en: 'Equation 4: Greedy policy update w.r.t. the state values.'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 方程4：关于状态值的贪婪策略更新。
- en: 'where:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 其中：
- en: '***p(s’,r|s,a)*** is the probability of moving to the next state ***s’*** and
    getting reward ***r*** when starting in state **s** and taking action***a***.'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***p(s’,r|s,a)*** 是在状态**s**下采取动作***a***后，转移到下一个状态***s’***并获得奖励***r***的概率。'
- en: '***r*** is the reward received after taking this action.'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***r*** 是采取该动作后获得的奖励。'
- en: '***γ*** is the discount factor.'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***γ*** 是折扣因子。'
- en: '***V(s’)*** is the value of the next state.'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***V(s’)*** 是下一个状态的值。'
- en: Unfortunately, since we’re not given the model of the environment, we don’t
    know ***p(s’,r|s,a)*** and therefore can’t make use of the next state value ***V(s’).***
    Since we’re not given a model of the environment we don’t know where we’ll end
    up if we take a specific action, nor the reward we’ll get for taking that action.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，由于我们没有提供环境模型，我们不知道***p(s’,r|s,a)***，因此无法利用下一个状态值***V(s’）。*** 由于没有环境模型，我们不知道采取特定动作后会到达何处，也不知道采取该动作会获得什么奖励。
- en: 'As an illustration, consider the level that Baby Robot has been exploring,
    along with the first-visit state values that we calculated, shown again in *Figure
    8* below:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 作为说明，考虑一下小机器人正在探索的区域，以及我们计算的首次访问状态值，再次显示在*图8*下方：
- en: '![](../Images/6e44396cf4832f0b3dc140c36e2162e1.png)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6e44396cf4832f0b3dc140c36e2162e1.png)'
- en: 'Figure 8: State values and coordinates.'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 图8：状态值和坐标。
- en: When Baby Robot reaches the grid square [1,1], as shown above, if we simply
    act greedily with respect to the calculated state values, then the optimal action
    appears to be a move south, to grid square [1,2], since this is the adjacent state
    with the highest state value. However, since we don’t have a model of the environment,
    we know nothing about the transition probabilities or rewards that would be received
    for taking this action.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 当小机器人到达网格方格[1,1]时，如上所示，如果我们仅仅依据计算出的状态值贪婪地行动，那么最优动作似乎是向南移动到网格方格[1,2]，因为这是与当前状态具有最高状态值的相邻状态。然而，由于我们没有环境模型，我们对采取该动作所得到的转移概率或奖励一无所知。
- en: If Baby Robot was to take this action his probability of reaching the target
    state [1,2] is actually zero and instead he’d bounce off the glass wall and end
    up in the puddle at [1,0]. Additionally, he’d receive a large negative reward
    of -5 for taking this action. So, in reality, this wouldn’t be a good action to
    choose.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 如果小机器人采取这个动作，他到达目标状态[1,2]的概率实际上为零，而是会撞到玻璃墙，最终落入[1,0]的水坑中。此外，他还会因采取这个动作而收到-5的大负奖励。因此，实际上，这并不是一个好的选择。
- en: Therefore, for *Monte Carlo* *control*, rather than calculating the values of
    each state, we need to calculate the action values. As a reminder, these values
    are known as [***Q-Values***](/markov-decision-processes-and-bellman-equations-45234cce9d25#0bb8)
    and represent the returns that can be expected from taking an individual action.
    We can then use these to find the optimal policy and navigate the best path through
    the maze.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，对于*蒙特卡洛* *控制*，我们需要计算的是动作值，而不是每个状态的值。作为提醒，这些值被称为[***Q-值***](/markov-decision-processes-and-bellman-equations-45234cce9d25#0bb8)，表示从采取单个动作中可以期待的回报。然后，我们可以利用这些值来找到最优策略，并在迷宫中找到最佳路径。
- en: Once we know the state-action values (or approximations of them) then we can
    do policy improvement by acting greedily with respect to these values, simply
    choosing the action with the maximum value in each state. We don’t need the model
    of the environment and we don’t *bootstrap* (i.e. no use is made of the value
    of the potential next state).
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们知道状态-动作值（或其近似值），我们就可以通过在这些值上贪婪地行动来进行策略改进，即在每个状态中简单地选择具有最大值的动作。我们不需要环境模型，也不需要*引导*（即不使用潜在下一个状态的值）。
- en: This policy update is described by *equation 5*, where, for state ‘***s’***,
    the greedy policy ‘***π’*** simply chooses the action ***‘a’*** with the maximum
    state-action value ***‘q’.***
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 该策略更新由*方程5*描述，对于状态‘***s’***，贪婪策略‘***π’***简单地选择具有最大状态-动作值***‘q’***的动作***‘a’***。
- en: '![](../Images/c7f21a9da19c0c0e7fcf7f77121bed71.png)'
  id: totrans-139
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c7f21a9da19c0c0e7fcf7f77121bed71.png)'
- en: 'Equation 5: Greedy policy update w.r.t. the maximum state-action value'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 方程5：关于最大状态-动作值的贪婪策略更新
- en: The code to calculate the state-action values using Monte Carlo is almost identical
    to the code that was used to calculate the state values, except now, rather than
    considering visits to individual states, we’re analysing the visits to the actions
    within those states.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 使用蒙特卡洛方法计算状态-动作值的代码与计算状态值的代码几乎相同，只不过现在我们分析的是这些状态中的动作访问情况，而不是单个状态的访问情况。
- en: 'The Python pseudo-code to calculate the state-action values using First-Visit
    Monte Carlo is shown below:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 使用首次访问蒙特卡洛方法计算状态-动作值的Python伪代码如下所示：
- en: '[PRE5]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: If you compare this code with the *Monte Carlo First-Visit* code that we used
    to calculate the state values, you’ll see that it’s practically identical. The
    only real difference is that now, instead of using a 2-dimensional array of the
    states in the environment, we use a third dimension to allow the *Q-values* to
    be calculated.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你将这段代码与我们用来计算状态值的*蒙特卡罗首次访问*代码进行比较，你会发现它们几乎完全相同。唯一真正的区别是，现在我们使用一个第三维度来计算*Q值*，而不是使用环境中的二维状态数组。
- en: '**The points to note in the code above, for First-Visit Monte Carlo, are as
    follows:**'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '**上面代码中的重点，对于首次访问蒙特卡罗方法，如下：**'
- en: Over the course of all episodes, we want to keep track of the returns that are
    received after the first visit to each action and the number of episodes in which
    each action was visited (for *First-Visit MC*). Therefore we start by creating
    zero’d Numpy arrays for each of these. As before, the array dimensions are set
    to the width and height of the environment, but now a third dimension is added
    for the number of possible actions in each state.
  id: totrans-146
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在所有集的过程中，我们希望跟踪在每个动作首次访问后的回报和每个动作被访问的集数（对于*首次访问蒙特卡罗*）。因此，我们首先为每个动作创建零初始化的Numpy数组。与之前一样，数组维度设置为环境的宽度和高度，但现在增加了一个维度，用于每个状态中的可能动作数量。
- en: As in all Monte Carlo methods we want to sample the rewards obtained over multiple
    episodes.
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 正如所有蒙特卡罗方法中一样，我们希望对多个集获得的奖励进行采样。
- en: For each episode get the list of the state actions visited and the rewards received
    for each of these.
  id: totrans-148
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于每一集，获取访问的状态动作列表以及每个状态动作收到的奖励。
- en: Sum the rewards in reverse direction to calculate the returns for each action.
  id: totrans-149
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 反向求和奖励以计算每个动作的回报。
- en: Calculate the first-visit return value for the actions taken during the episode
    and then add this to the total returns. This updates the total return of every
    action seen during the episode and increments the count of the number of first
    visits for these actions.
  id: totrans-150
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算在集期间采取的动作的首次访问回报值，然后将其添加到总回报中。这更新了每个在集期间出现的动作的总回报，并增加了这些动作的首次访问计数。
- en: Finally, after all episodes have completed, divide the total first-visit rewards
    by the total number of visits, to get the estimated Q-Value for each state-action.
  id: totrans-151
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，在所有集完成后，将总首次访问奖励除以访问总数，以获得每个状态-动作的估计Q值。
- en: 'Running this code for 1000 episodes gives the following state-action values
    and action visit counts:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 运行这段代码1000集后，得到如下状态-动作值和动作访问计数：
- en: '![](../Images/1d49e9a6a64e8510b0953a9d8435e322.png)'
  id: totrans-153
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1d49e9a6a64e8510b0953a9d8435e322.png)'
- en: 'Figure 9: Action values and counts calculated using Monte Carlo.'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 图9：使用蒙特卡罗计算的动作值和计数。
- en: In *Figure 9,* above, we can see that the actions that must be taken to move
    from the entrance to the exit of the level all have been sampled 1000 times, equal
    to the number of episodes. Similarly, since Baby Robot is smart enough to not
    walk into standard walls, the actions that would result in hitting a wall are
    never taken and so have a zero visit count. Due to a stochastic policy being used,
    all other possible actions have been visited across the set of episodes.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 在*图9*中，我们可以看到，从入口到关卡出口必须采取的所有动作都被采样了1000次，等于集的数量。同样，由于宝宝机器人足够聪明，不会走进标准墙，因此会导致撞墙的动作从未被采取，因此访问计数为零。由于使用了随机策略，所有其他可能的动作在所有集上都被访问过。
- en: 'If we now act greedily with respect to these action values we get the following
    policy which, for this very simple level, is also the optimal policy. By following
    this, Baby Robot will reach the exit in the shortest possible time:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们现在根据这些动作值采取贪婪行动，我们得到的政策如下，对于这个非常简单的关卡，这也是*最优政策*。按照这个政策，宝宝机器人将以最短时间到达出口：
- en: '![](../Images/66f05b748231109e9c89cc97c945e17a.png)'
  id: totrans-157
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/66f05b748231109e9c89cc97c945e17a.png)'
- en: 'Figure 10: The optimal policy, calculated by acting greedily with respect to
    the estimated action values.'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 图10：通过对估计的动作值采取贪婪行动计算出的最优政策。
- en: Generalised Policy Iteration (GPI) with Monte Carlo
  id: totrans-159
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用蒙特卡罗方法的广义策略迭代（GPI）
- en: For the simple level that we’ve been using as an example, with a stochastic
    policy where, in any state, each action was equally likely to be chosen, we evaluated
    the policy using *Monte Carlo* to estimate the action values and then improved
    the policy by selecting the action with the maximum value in each state (i.e.
    we acted greedily with respect to the action values).
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们用作示例的简单关卡，在任何状态下，每个动作被选择的概率相等，我们使用*蒙特卡洛*评估了策略以估计动作值，然后通过在每个状态中选择具有最大值的动作来改进策略（即，我们对动作值采取了贪婪行为）。
- en: This represents a single iteration of [*Generalised Policy Iteration*](/policy-and-value-iteration-78501afb41d2)
    (GPI) in which repeated evaluation and improvement steps move the policy ever
    closer to the optimal policy, as shown in *figure 11\.* But now, rather than using
    [*Dynamic Programming*](https://medium.com/@tinkertytonk/state-values-and-policy-evaluation-in-5-minutes-f3e00f3c1a50#5f4c)
    to evaluate each policy, we’ve approximated the value function using Monte Carlo.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 这代表了[*广义策略迭代*](/policy-and-value-iteration-78501afb41d2)（GPI）的单次迭代，其中重复的评估和改进步骤使策略越来越接近最优策略，如*图11*所示。但现在，我们没有使用[*动态规划*](https://medium.com/@tinkertytonk/state-values-and-policy-evaluation-in-5-minutes-f3e00f3c1a50#5f4c)来评估每个策略，而是使用蒙特卡洛方法来逼近价值函数。
- en: '![](../Images/056a7c05a8cc1c1ea361b03c78c1058e.png)'
  id: totrans-162
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/056a7c05a8cc1c1ea361b03c78c1058e.png)'
- en: 'Figure 11: Generalised Policy Iteration (GPI)'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 图11：广义策略迭代（GPI）
- en: In the case of our simple level we reached the optimal policy after only a single
    iteration but, in practice, for more complex environments many iterations may
    be required.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们简单的关卡中，我们仅经过一次迭代就达到了最优策略，但在实践中，对于更复杂的环境，可能需要多次迭代。
- en: 'As an example, consider the more complex level and an associated deterministic
    policy shown below:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，考虑下面显示的更复杂的关卡及其相关的确定性策略：
- en: '![](../Images/ed3de0326672cd05fe31823651b975b5.png)![](../Images/b7b9e6a3b730296eb6f16478714828ef.png)'
  id: totrans-166
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ed3de0326672cd05fe31823651b975b5.png)![](../Images/b7b9e6a3b730296eb6f16478714828ef.png)'
- en: 'Figure 12: A slightly more complex level and a deterministic policy for this
    level'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 图12：稍微复杂一点的关卡及其对应的确定性策略
- en: 'We can run a single iteration of Monte Carlo policy evaluation for this deterministic
    policy, which gives us the following state-action values:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以对该确定性策略进行单次蒙特卡洛策略评估，这会给我们以下状态-动作值：
- en: '![](../Images/300e476ba9ad738bda44e469f33dfa12.png)'
  id: totrans-169
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/300e476ba9ad738bda44e469f33dfa12.png)'
- en: 'Figure 13: state-action values after a single iteration of MC policy evaluation
    for a deterministic policy'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 图13：确定性策略下MC策略评估单次迭代后的状态-动作值
- en: 'And, if we act greedily with respect to these values, we get the following
    updated policy:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们对这些值采取贪婪行为，我们将得到以下更新的策略：
- en: '![](../Images/7c44e7b5447505321a5d72d08a2c0ae0.png)'
  id: totrans-172
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7c44e7b5447505321a5d72d08a2c0ae0.png)'
- en: 'Figure 14: Updated deterministic policy after acting greedily with respect
    to the estimated action values'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 图14：在对估计的动作值采取贪婪行为后的更新确定性策略
- en: We appear to have a problem! Not only has the majority of the level not been
    visited, but Baby Robot is stuck running in circles, never reaching the exit (and
    this would have continued for ever is we hadn’t set a limit on the allowed length
    of an episode).
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 我们似乎遇到了问题！不仅关卡的大部分未被访问，而且宝宝机器人还被困在循环中，永远无法到达出口（如果我们没有设置回合长度限制，这将永远继续下去）。
- en: Clearly, when using a deterministic policy, we also need to add some form of
    exploration, otherwise we’ll never be able to estimate the state-action values
    for actions that haven’t been visited and, as a result, we’d never be able to
    improve the policy.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 很明显，当使用确定性策略时，我们还需要添加一些形式的探索，否则我们将无法估计未访问过的动作的状态-动作值，因此，我们将无法改进策略。
- en: We could achieve this exploration by using the *exploring starts* idea that
    we saw for state values. In this case, rather than beginning each episode in a
    new state, we’d extend this to also cover all actions. However, outside of a simple
    simulation of the environment, this would be impractical. A better approach is
    to consider policies that have a non-zero chance of selecting all actions in each
    state.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过使用我们在状态值中看到的*探索起始*思想来实现这种探索。在这种情况下，我们不仅在每个回合开始时进入一个新的状态，还将其扩展到覆盖所有动作。然而，除非在简单的环境模拟中，这将是不可行的。更好的方法是考虑在每个状态中有非零选择所有动作的机会的策略。
- en: The Epsilon-Greedy Algorithm (ε-Greedy)
  id: totrans-177
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Epsilon-贪婪算法（ε-贪婪）
- en: When following a deterministic policy only a single action will be taken in
    each state. However, to allow us to choose the best actions we need to form estimates
    of the values for all actions, so need to add some form of exploration.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 遵循确定性策略时，每个状态只会采取一个动作。然而，为了让我们选择最佳动作，我们需要对所有动作的值进行估计，因此需要加入某种形式的探索。
- en: One of the simplest, but highly effective, methods for achieving this is the
    [***Epsilon-Greedy***](https://medium.com/towards-data-science/bandit-algorithms-34fd7890cb18#0145)algorithm(which
    we saw previously when looking at Bandit problems). In this method, in any state,
    we normally choose the action with the maximum estimated action value, but with
    probability ‘ε’ (epsilon) an action is selected at random.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 实现这一目标的最简单但非常有效的方法之一是[***Epsilon-Greedy***](https://medium.com/towards-data-science/bandit-algorithms-34fd7890cb18#0145)算法（我们在研究Bandit问题时见过）。在这种方法中，在任何状态下，我们通常选择具有最大估计动作值的动作，但以概率‘ε’（epsilon）随机选择一个动作。
- en: 'Therefore, for the current state, we can select an action as shown below:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，对于当前状态，我们可以选择如下所示的动作：
- en: '[PRE6]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Now, when following an *epsilon-greedy* policy, we can discover other states
    and actions that wouldn’t be found under a purely deterministic policy. As a result,
    we gather a lot more information during an episode than we would if simply following
    the deterministic policy.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，当遵循*epsilon-greedy*策略时，我们可以发现其他状态和动作，这些状态和动作在纯确定性策略下是不会被发现的。因此，在一个回合中，我们会收集到比仅遵循确定性策略时更多的信息。
- en: 'For example, for the same deterministic policy that was used above, but with
    the addition of random actions, taken with probability 0.1 (so approximately every
    10th action will be chosen at random), for a single episode, we get action values
    like the one shown below:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，对于上述使用的相同确定性策略，但增加了以0.1的概率选择的随机动作（即大约每第10个动作会随机选择），在一个回合中，我们得到如下所示的动作值：
- en: '![](../Images/15db55a7819c18cc0ce141f289f22e63.png)'
  id: totrans-184
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/15db55a7819c18cc0ce141f289f22e63.png)'
- en: 'Figure 15: Action values for a single episode of Epsilon-Greedy (with the allowed
    number of steps in an episode increased to 2000)'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15：一个回合的Epsilon-Greedy动作值（回合中的允许步数增加到2000）
- en: 'When we act greedily with respect to these action values, we get the following
    policy which, as you can see, has defined actions based on a much larger set of
    states:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们对这些动作值贪婪地采取行动时，会得到如下策略，正如你所看到的，这些策略基于一个更大的状态集合定义了动作：
- en: '![](../Images/d91efd2effd14299251500a85e466ba9.png)'
  id: totrans-187
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d91efd2effd14299251500a85e466ba9.png)'
- en: 'Figure 16: Policy formed from acting greedily with respect to action values
    calculated using an Epsilon-Greedy policy.'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 图 16：从贪婪地执行基于动作值的策略中形成的策略。
- en: Now that we’ve got a means by which we can explore the environment, we can return
    to our initial problem of using GPI to find the optimal policy.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了一种可以探索环境的方法，我们可以回到最初使用GPI来寻找最优策略的问题。
- en: In the example above, we only ran a single episode of *Monte Carlo* to evaluate
    the state-action values and then improved the policy based on these values. Instead,
    before the policy improvement step, we could have evaluated the policy over multiple
    episodes, to form a more accurate approximation of the action values.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述示例中，我们只运行了一个*蒙特卡洛*回合来评估状态-动作值，然后基于这些值改进策略。相反，在策略改进步骤之前，我们本可以在多个回合中评估策略，以形成对动作值的更准确的估计。
- en: These two approaches are similar to [*Value Iteration*](https://medium.com/towards-data-science/policy-and-value-iteration-78501afb41d2#29d6),
    were only a single step of policy evaluation is done before the policy is improved,
    and [*Policy Iteration*](https://medium.com/towards-data-science/policy-and-value-iteration-78501afb41d2#bad9),
    in which the policy is evaluated to convergence before the policy improvement
    step. In either case, for *Monte Carlo* methods, we need to run complete episodes
    in the policy evaluation step, before policy improvement takes place.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 这两种方法类似于[*价值迭代*](https://medium.com/towards-data-science/policy-and-value-iteration-78501afb41d2#29d6)，只在策略改进前完成了一步策略评估，以及[*策略迭代*](https://medium.com/towards-data-science/policy-and-value-iteration-78501afb41d2#bad9)，其中策略在策略改进步骤之前进行收敛评估。在任何情况下，对于*蒙特卡洛*方法，我们需要在策略评估步骤中运行完整的回合，然后再进行策略改进。
- en: '![](../Images/d33c2a7777de6ca316893fdca89b076e.png)'
  id: totrans-192
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d33c2a7777de6ca316893fdca89b076e.png)'
- en: 'Figure 17: an initial deterministic policy that does reach the exit'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 图 17：一个初始的确定性策略，该策略确实可以达到出口。
- en: The deterministic policy, shown in *Figure 17* above, defines a trajectory that
    would take Baby Robot from the start to the exit of the level. But, as you’ve
    probably noticed, it’s less than optimal.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 确定性策略，如上图*图17*所示，定义了一条从起点到达关卡出口的轨迹。但正如你可能已经注意到的，它并不是最优的。
- en: 'Therefore, we’d like to improve this policy by running repeated rounds of MC
    policy evaluation and policy improvement to move us progressively closer to the
    optimal policy. With epsilon set to 0.5, to give us a high degree of exploration,
    and running a single episode of MC policy evaluation before each policy improvement,
    we get the action value estimates and associated policies shown below:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们希望通过运行多轮MC策略评估和策略改进来改进这个策略，使我们逐步接近最优策略。设置ε为0.5，以提供较高的探索度，并在每次策略改进之前运行一次MC策略评估，我们得到了下面显示的动作价值估计和相关策略：
- en: '![](../Images/5d0a36f4787a0cce53c1ca62f8a15739.png)![](../Images/f8f04fc7ed35b1e81d70c1cd4cf2e1bc.png)'
  id: totrans-196
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5d0a36f4787a0cce53c1ca62f8a15739.png)![](../Images/f8f04fc7ed35b1e81d70c1cd4cf2e1bc.png)'
- en: 'Figure 18: Action value estimates and associated policy improvements when running
    GPI with epsilon = 0.5'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 图18：运行GPI时，ε=0.5的动作价值估计和相关策略改进
- en: The delta value, shown at each iteration of GPI in *figure 18,* and in the graph
    below, represents the average difference between the state action values at the
    start and end of the iteration. As you can see, it initially begins with a high
    value and converges towards zero as the estimated action values become better
    estimates of the true action values.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: delta值，如*图18*所示，每次GPI迭代中都有显示，并且在下面的图表中，代表了迭代开始和结束时状态动作值之间的平均差异。正如你所看到的，它最初以一个较高的值开始，并在估计的动作值变得更接近真实动作值时趋近于零。
- en: '![](../Images/a679211b1e8284170a80163d63662e67.png)'
  id: totrans-199
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a679211b1e8284170a80163d63662e67.png)'
- en: 'Figure 19: The change in the delta value, representing the mean difference
    of all action values between the start and end of each iteration'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 图19：delta值的变化，表示每次迭代中所有动作值的均值差异
- en: 'In this run we’ve actually used the delta value to control the stopping of
    the GPI process (considering convergence to have been reached when delta is less
    than 0.04). The final policy is shown in figure 20 below:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 在这次运行中，我们实际上使用了delta值来控制GPI过程的停止（当delta小于0.04时认为已经收敛）。最终策略如下面的*图20*所示：
- en: '![](../Images/caec1e7a248b3e1ef2275eb2c8e12326.png)'
  id: totrans-202
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/caec1e7a248b3e1ef2275eb2c8e12326.png)'
- en: 'Figure 20: The policy formed at the final iteration of GPI, with a minimum
    delta of 0.04 used as the stopping criteria.'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 图20：GPI最终迭代中形成的策略，使用最小delta值0.04作为停止标准。
- en: This final policy defines a much shorter route from the entrance to the exit
    of the level. Additionally, the single action value of the start state has reduced
    from -45 in the initial policy down to -27 in the final policy. Since this value
    represents the return that can be expected when taking this action and, in the
    case of this environment where every action incurs a penalty of -1 for taking
    that action, you can see that we’ve dramatically reduced the amount of time that
    it will take Baby Robot to reach the exit.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 这个最终策略定义了从入口到出口的更短路线。此外，起始状态的单一动作价值从初始策略中的-45降低到最终策略中的-27。由于这个值代表了在采取此动作时可以预期的回报，并且在这个环境中，每个动作都会因为采取该动作而产生-1的惩罚，你可以看到我们大大减少了宝宝机器人到达出口所需的时间。
- en: However, things aren’t perfect and this isn’t the optimal policy for this level.
    For example, if you consider the bottom left-hand state, you can see that the
    final policy defines an action that would take Baby Robot North, when a shorter
    route to the exit would be to move East.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，情况并不完美，这并不是这个关卡的最优策略。例如，如果你考虑左下角的状态，你会发现最终的策略定义了一个将宝宝机器人向北移动的动作，而更短的通向出口的路线是向东移动。
- en: The best action hasn’t been found due to a lack of sampling of the actions in
    this state. If you look at the number of first-visits to each action, as shown
    in *Figure 21* below, you can see that the North action was only seen 3 times
    across the 23 episodes used for policy evaluation.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 由于在此状态下动作采样不足，最佳动作尚未找到。如果你查看每个动作的首次访问次数，如下*图21*所示，你可以看到北方动作在用于策略评估的23个回合中仅出现了3次。
- en: '![](../Images/cdce3cf974c00a1defbe3dfae14a28f2.png)'
  id: totrans-207
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/cdce3cf974c00a1defbe3dfae14a28f2.png)'
- en: 'Figure 21: The first-visit count of the actions visited during the 23 iterations
    of GPI.'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 图21：在23轮GPI迭代中访问的动作的首次访问计数。
- en: 'There are a few reasons why this action was hardly ever visited:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 这个动作几乎从未被访问的原因有几个：
- en: We defined a minimum delta value that stopped GPI before the optimal policy
    was found.
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们定义了一个最小的delta值，在找到最优策略之前停止了GPI。
- en: We only ran a single episode for each policy evaluation step. Increasing the
    number of episodes would increase the accuracy of the action value estimates and
    increase the number of visits to each action.
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们每次策略评估步骤仅运行一个回合。增加回合数将提高动作值估计的准确性，并增加每个动作的访问次数。
- en: By default, Epsilon-Greedy follows the current deterministic policy. Although
    other actions are chosen at random to increase the exploration of the environment,
    remote states and actions are less likely to be visited.
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 默认情况下，Epsilon-贪婪算法遵循当前的确定性策略。虽然其他动作是随机选择的，以增加对环境的探索，但远离的状态和动作较少被访问。
- en: As mentioned previously, the fact that this state and its actions have been
    visited infrequently may not be a bad thing. Rather than visiting all states and
    actions an equal number of times, by concentrating on those that lead from the
    start state to the exit, we’ve reduced the time required to locate the most optimal
    trajectory through the maze. As usual it’s the old exploration\exploitation trade-off.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，这种状态及其动作被访问得不频繁可能并不是一件坏事。与其均匀地访问所有状态和动作，不如集中在从初始状态到出口的路径上，这样我们就减少了寻找迷宫中最优轨迹所需的时间。像往常一样，这涉及到探索\利用的权衡。
- en: Summary
  id: totrans-214
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: '*Monte Carlo* methods use sampling to learn from experience. Over the course
    of multiple episodes they gradually build up an increasingly accurate picture
    of the environment. As a result, they can estimate state and action values without
    being given a model of the environment.'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: '*蒙特卡洛*方法使用采样从经验中学习。在多个回合中，它们逐渐建立对环境的越来越准确的描述。因此，它们可以在没有环境模型的情况下估计状态和动作值。'
- en: During the policy evaluation step of *Generalized Policy Iteration* (GPI), by
    running complete episodes using the current policy, Monte Carlo methods can be
    used to form an average of the visited state-action values.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 在*广义策略迭代*（GPI）的策略评估步骤中，通过使用当前策略运行完整的回合，可以使用蒙特卡洛方法形成访问的状态-动作值的平均值。
- en: To ensure that all states and actions are visited, exploration can be introduced
    in a number of ways, such as *exploring starts*, in which each new episode begins
    in a different state or action, or *epsilon-greedy* policies where, by default,
    the action specified by the policy is taken, but with a probability ‘*epsilon*’
    a different, random, action is chosen.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确保所有状态和动作都被访问，可以通过多种方式引入探索，例如*探索开始*，在这种情况下，每个新回合从不同的状态或动作开始，或*epsilon-贪婪*策略，其中，默认情况下，采取由策略指定的动作，但以‘*epsilon*’的概率选择一个不同的随机动作。
- en: If the policy is then improved by acting greedily with respect to these estimated
    values, the policy improvement theorem guarantees that the new policy will be
    as good or better than the previous policy. By repeating these steps of policy
    evaluation and improvement, Monte Carlo methods can be used to find the optimal
    policy.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 如果策略通过对这些估计值采取贪婪行动来改进，那么策略改进定理保证新策略将与之前的策略一样好或更好。通过重复这些策略评估和改进步骤，蒙特卡洛方法可以用来找到最优策略。
- en: What’s Next
  id: totrans-219
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 下一步
- en: So far we’ve only considered ‘***on-policy***’ methods, in which a single policy
    is evaluated and improved. Going forwards we’ll take a look at ‘***off-policy***’
    methods which employ more than one policy. For example, one policy can be used
    to explore and gather information, and a second policy, which is the policy that’s
    actually used to negotiate the environment, is then created from this information.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们只考虑了‘***策略内***’方法，其中评估和改进的是单一策略。接下来，我们将查看‘***策略外***’方法，这些方法使用多个策略。例如，一种策略可以用于探索和收集信息，而第二种策略，即实际用于与环境互动的策略，则根据这些信息创建。
- en: We’ve also seen that unlike *Dynamic Programming*, Monte Carlo methods don’t
    use the values from other states when updating their value estimates. In other
    words they don’t ‘*bootstrap*’. In the next article we’ll look at ***Temporal-Difference
    (TD)*** learning which, like Monte Carlo, learns from experience but also employs
    bootstrapping when calculating its value estimates.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还看到，与*动态规划*不同，蒙特卡洛方法在更新其值估计时不会使用其他状态的值。换句话说，它们不进行‘*自举*’。在下一篇文章中，我们将探讨***时序差分
    (TD)*** 学习，这种方法像蒙特卡洛一样从经验中学习，但在计算其值估计时也采用自举方法。
- en: 'References:'
  id: totrans-222
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献：
- en: '“[*Reinforcement Learning: An Introduction*](http://www.incompleteideas.net/book/RLbook2020.pdf)”,
    Sutton & Barto (2018)'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: '“[*强化学习: 介绍*](http://www.incompleteideas.net/book/RLbook2020.pdf)”，Sutton &
    Barto (2018)'
- en: '“[*Lecture 4: Model-free Prediction*](https://www.youtube.com/watch?v=PnHCvfgC_ZA)”,
    David Silver'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: '“[*讲座 4: 无模型预测*](https://www.youtube.com/watch?v=PnHCvfgC_ZA)”，David Silver'
- en: '![](../Images/9cdf6c8fcb0d1b9c006591725d35fa90.png)'
  id: totrans-225
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9cdf6c8fcb0d1b9c006591725d35fa90.png)'
- en: '[PRE7]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
