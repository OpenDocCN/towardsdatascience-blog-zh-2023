- en: Introducing KeyLLM — Keyword Extraction with LLMs
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍KeyLLM — 使用LLM进行关键词提取
- en: 原文：[https://towardsdatascience.com/introducing-keyllm-keyword-extraction-with-llms-39924b504813](https://towardsdatascience.com/introducing-keyllm-keyword-extraction-with-llms-39924b504813)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/introducing-keyllm-keyword-extraction-with-llms-39924b504813](https://towardsdatascience.com/introducing-keyllm-keyword-extraction-with-llms-39924b504813)
- en: Use KeyLLM, KeyBERT, and Mistral 7B to extract keywords
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用KeyLLM、KeyBERT和Mistral 7B来提取关键词
- en: '[](https://medium.com/@maartengrootendorst?source=post_page-----39924b504813--------------------------------)[![Maarten
    Grootendorst](../Images/58e24b9cf7e10ff1cd5ffd75a32d1a26.png)](https://medium.com/@maartengrootendorst?source=post_page-----39924b504813--------------------------------)[](https://towardsdatascience.com/?source=post_page-----39924b504813--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----39924b504813--------------------------------)
    [Maarten Grootendorst](https://medium.com/@maartengrootendorst?source=post_page-----39924b504813--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@maartengrootendorst?source=post_page-----39924b504813--------------------------------)[![Maarten
    Grootendorst](../Images/58e24b9cf7e10ff1cd5ffd75a32d1a26.png)](https://medium.com/@maartengrootendorst?source=post_page-----39924b504813--------------------------------)[](https://towardsdatascience.com/?source=post_page-----39924b504813--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----39924b504813--------------------------------)
    [Maarten Grootendorst](https://medium.com/@maartengrootendorst?source=post_page-----39924b504813--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----39924b504813--------------------------------)
    ·9 min read·Oct 5, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page-----39924b504813--------------------------------)
    ·阅读时间9分钟·2023年10月5日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/47514c37ae551ce79a535285a4d4488a.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/47514c37ae551ce79a535285a4d4488a.png)'
- en: Large Language Models (LLMs) are becoming smaller, faster, and more efficient.
    Up to the point where I started to consider them for iterative tasks, like keyword
    extraction.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）变得越来越小、快速且高效。直到我开始考虑它们用于迭代任务，如关键词提取。
- en: Having created [KeyBERT](https://github.com/MaartenGr/KeyBERT), I felt that
    it was time to extend the package to also include LLMs. They are quite powerful
    and I wanted to prepare the package for when these models can be run on smaller
    GPUs.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在创建了[KeyBERT](https://github.com/MaartenGr/KeyBERT)之后，我觉得是时候扩展该包以包括LLMs了。它们非常强大，我希望为将来这些模型可以在更小的GPU上运行做好准备。
- en: As such, introducing `[KeyLLM](https://maartengr.github.io/KeyBERT/guides/keyllm.html)`,
    an extension to KeyBERT that allows you to use any LLM to extract, create, or
    even fine-tune the keywords!
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 介绍`[KeyLLM](https://maartengr.github.io/KeyBERT/guides/keyllm.html)`，这是KeyBERT的一个扩展，允许你使用任何LLM来提取、创建甚至微调关键词！
- en: In this tutorial, we will go through keyword extraction with `[KeyLLM](https://maartengr.github.io/KeyBERT/guides/keyllm.html)`
    using the recently released Mistral 7B model.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在本教程中，我们将通过使用最近发布的Mistral 7B模型的`[KeyLLM](https://maartengr.github.io/KeyBERT/guides/keyllm.html)`进行关键词提取。
- en: 'We will start by installing a number of packages that we are going to use throughout
    this example:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从安装一系列在本示例中将要使用的包开始：
- en: '[PRE0]'
  id: totrans-12
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: We are installing `sentence-transformers` from its main branch since it has
    a fix for community detection which we will use in the last few use cases. We
    do the same for `transformers` since it does not yet support the Mistral architecture.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 我们正在从主分支安装`sentence-transformers`，因为它修复了社区检测的功能，我们将在最后几个用例中使用。我们对`transformers`也做了同样的处理，因为它尚不支持Mistral架构。
- en: You can also follow along with the [**Google Colab Notebook**](https://colab.research.google.com/drive/1A1lbPnBhtxL9jR7vFcm7Z0F0aJdEl-zj?usp=sharing)
    to make sure everything works as intended.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以跟随[**Google Colab Notebook**](https://colab.research.google.com/drive/1A1lbPnBhtxL9jR7vFcm7Z0F0aJdEl-zj?usp=sharing)以确保一切按预期工作。
- en: 🤖 Loading the Model
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 🤖 加载模型
- en: In previous tutorials, we demonstrated how we could quantize the original model’s
    weight to make it run without running into memory problems.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的教程中，我们展示了如何对原始模型的权重进行量化，以便在不遇到内存问题的情况下运行。
- en: Over the course of the last few months, [TheBloke](https://huggingface.co/TheBloke)
    has been working hard on doing the quantization for hundreds of models for us.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去几个月里，[TheBloke](https://huggingface.co/TheBloke)为我们辛勤工作，对数百个模型进行了量化。
- en: This way, we can download the model directly which will speed things up quite
    a bit.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 这样，我们可以直接下载模型，这将大大加快速度。
- en: We’ll start with loading the model itself. We will offload 50 layers to the
    GPU. This will reduce RAM usage and use VRAM instead. If you are running into
    memory errors, reducing this parameter (`gpu_layers`) might help!
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从加载模型本身开始。我们将把50层卸载到GPU上。这将减少RAM使用量，而使用VRAM。如果你遇到内存错误，减少这个参数（`gpu_layers`）可能会有所帮助！
- en: '[PRE1]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: After having loaded the model itself, we want to create a 🤗 Transformers pipeline.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在加载了模型本身后，我们想创建一个🤗 Transformers管道。
- en: The main benefit of doing so is that these pipelines are found in many tutorials
    and are often used in packages as a backend. Thus far, `ctransformers` is not
    yet natively supported as much as `transformers`.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 这样做的主要好处是，这些管道在许多教程中都有出现，通常作为后台在软件包中使用。到目前为止，`ctransformers`的原生支持程度还没有`transformers`那么高。
- en: Loading the Mistral tokenizer with `ctransformers` is not yet possible as the
    model is quite new. Instead, we use the tokenizer from the original repository
    instead.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 由于Mistral的分词器相对较新，尚无法使用`ctransformers`加载。因此，我们使用原始仓库中的分词器。
- en: '[PRE2]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 📄 Prompt Engineering
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 📄 提示工程
- en: 'Let’s see if this works with a very basic example:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们用一个非常基础的示例来看看这是否有效：
- en: '[PRE3]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Perfect! It can handle a very basic question. For the purpose of keyword extraction,
    let’s explore whether it can handle a bit more complexity.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 完美！它可以处理一个非常基本的问题。为了关键词提取的目的，让我们探索一下它是否能处理更多的复杂性。
- en: '[PRE4]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'We get the following output:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到如下输出：
- en: '[PRE5]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: It does great! However, if we want the structure of the output to stay consistent
    regardless of the input text we will have to give the LLM an example.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 它表现得非常好！然而，如果我们希望输出的结构在输入文本不同的情况下保持一致，我们需要给LLM一个示例。
- en: This is where more advanced prompt engineering comes in. As with most Large
    Language Models, Mistral 7B expects a certain prompt format. This is tremendously
    helpful when we want to show what a “correct” interaction looks like.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是更高级提示工程的用武之地。与大多数大型语言模型一样，Mistral 7B期望特定的提示格式。当我们想展示“正确”交互的样子时，这非常有帮助。
- en: 'The prompt template is as follows:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 提示模板如下：
- en: '![](../Images/ed736ed8bd3f7f90ec3b71a5fe87d324.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ed736ed8bd3f7f90ec3b71a5fe87d324.png)'
- en: Based on that template, let’s create a template for keyword extraction.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 基于该模板，让我们创建一个关键词提取的模板。
- en: 'It needs to have two components:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 它需要有两个组件：
- en: '`Example prompt` - This will be used to show the LLM what a “good” output looks
    like'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Example prompt` - 这将用于向LLM展示“良好”输出的样子'
- en: '`Keyword prompt` - This will be used to ask the LLM to extract the keywords'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Keyword prompt` - 这将用于请求LLM提取关键词'
- en: The first component, the `example_prompt`, will simply be an example of correctly
    extracting the keywords in the format that we are interested in.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个组件，`example_prompt`，将仅仅是一个正确提取关键词的示例，符合我们感兴趣的格式。
- en: 'The **format** is a key component since it will make sure that the LLM will
    always output keywords the way we want:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '**格式**是一个关键组件，因为它确保LLM始终以我们希望的方式输出关键词：'
- en: '[PRE6]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The second component, the `keyword_prompt`, will essentially be a repeat of
    the `example_prompt` but with two changes:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个组件，`keyword_prompt`，实际上是`example_prompt`的重复，只不过有两个变化：
- en: It will not have an output yet. That will be generated by the LLM.
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它还不会有输出。那将由LLM生成。
- en: We make use of `KeyBERT`’s **[DOCUMENT]** tag for indicating where the input
    document will go.
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们使用`KeyBERT`的**[DOCUMENT]**标签来指示输入文档的位置。
- en: We can use the **[DOCUMENT]** to insert a document at a location of your choice.
    Having this option helps us to change the structure of the prompt if needed without
    being set on having the prompt at a specific location.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用**[DOCUMENT]**将文档插入到您选择的位置。这个选项有助于我们在需要时更改提示的结构，而不必将提示设置在特定位置。
- en: '[PRE7]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Lastly, we combine the two prompts to create our final template:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将两个提示合并以创建我们的最终模板：
- en: '[PRE8]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Now that we have our final prompt template, we can start exploring a couple
    of interesting new features in `KeyBERT` with `KeyLLM`. We will start by exploring
    `KeyLLM` only using Mistral’s 7B model
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了最终的提示模板，我们可以开始探索`KeyBERT`与`KeyLLM`的一些有趣的新功能。我们将首先探索仅使用Mistral的7B模型的`KeyLLM`
- en: 🗝️ Keyword Extraction with `KeyLLM`
  id: totrans-51
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 🗝️ 使用`KeyLLM`进行关键词提取
- en: Keyword extraction with vanilla `KeyLLM` couldn’t be more straightforward; we
    simply ask it to extract keywords from a document.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 使用原生`KeyLLM`进行关键词提取是极其简单的；我们只需让它从文档中提取关键词即可。
- en: '![](../Images/80116740c612fd002adc807f474e5863.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/80116740c612fd002adc807f474e5863.png)'
- en: This idea of extracting keywords from documents through an LLM is straightforward
    and allows for easily testing your LLM and its capabilities.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 通过 LLM 从文档中提取关键词的想法很简单，可以轻松测试你的 LLM 及其功能。
- en: Using `KeyLLM` is straightforward, we start by loading our LLM through `keybert.llm.TextGeneration`
    and give it the prompt template that we created before.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`KeyLLM`很简单，我们首先通过`keybert.llm.TextGeneration`加载我们的 LLM，并给它之前创建的提示模板。
- en: '🔥 **Tip** 🔥: If you want to use a different LLM, like ChatGPT, you can find
    a full overview of implemented algorithms [here:](https://maartengr.github.io/KeyBERT/guides/llms.html)'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 🔥 **提示** 🔥：如果你想使用不同的 LLM，如 ChatGPT，你可以在[这里](https://maartengr.github.io/KeyBERT/guides/llms.html)查看已实现算法的完整概述。
- en: '[PRE9]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'After preparing our `KeyLLM` instance, it is as simple as running `.extract_keywords`
    over your documents:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在准备好我们的`KeyLLM`实例后，只需对你的文档运行`.extract_keywords`即可：
- en: '[PRE10]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'We get the following keywords:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到以下关键词：
- en: '[PRE11]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: These seem like a great set of keywords!
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 这些关键词看起来很棒！
- en: You can play around with the prompt to specify the kind of keywords you want
    extracted, how long they can be, and even in which language they should be returned
    if your LLM is multi-lingual.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以调整提示以指定你想提取的关键词类型，它们可以有多长，甚至在 LLM 是多语言的情况下返回哪种语言。
- en: 🚀 Efficient Keyword Extraction with `KeyLLM`
  id: totrans-64
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 🚀 使用`KeyLLM`进行高效关键词提取
- en: Iterating your LLM over thousands of documents is not the most efficient approach!
    Instead, we can leverage embedding models to make the keyword extraction a bit
    more efficient.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 对成千上万的文档进行 LLM 迭代并不是最有效的方法！相反，我们可以利用嵌入模型使关键词提取更高效。
- en: This works as follows. First, we embed all of our documents and convert them
    to numerical representations. Second, we find out which documents are most similar
    to one another. We assume that documents that are highly similar will have the
    same keywords, so there would be no need to extract keywords for all documents.
    Third, we only extract keywords from 1 document in each cluster and assign the
    keywords to all documents in the same cluster.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 这个过程如下。首先，我们将所有文档嵌入并转换为数值表示。其次，我们找出哪些文档彼此最相似。我们假设高度相似的文档将具有相同的关键词，因此不需要为所有文档提取关键词。第三，我们只从每个簇中的一个文档中提取关键词，并将这些关键词分配给同一簇中的所有文档。
- en: This is much more efficient and also quite flexible. The clusters are generated
    purely based on the similarity between documents, without taking cluster structures
    into account. In other words, it is essentially finding near-duplicate documents
    that we expect to have the same set of keywords.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 这更高效，而且相当灵活。簇是纯粹基于文档之间的相似性生成的，而不考虑簇结构。换句话说，它本质上是在寻找我们预期具有相同关键词集合的近重复文档。
- en: '![](../Images/eaa53dc36d4b4e0a0d725a0870665841.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/eaa53dc36d4b4e0a0d725a0870665841.png)'
- en: To do this with `KeyLLM`, we embed our documents beforehand and pass them to
    `.extract_keywords`. The threshold indicates how similar documents will minimally
    need to be in order to be assigned to the same cluster.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用`KeyLLM`实现这一点，我们提前嵌入文档并将它们传递给`.extract_keywords`。阈值指示文档需要多相似才能分配到同一个簇。
- en: Increasing this value to something like .95 will identify near-identical documents
    whereas setting it to something like .5 will identify documents about the same
    topic.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 将此值增加到像.95这样的数字将识别近乎相同的文档，而将其设置为像.5这样的值将识别关于相同主题的文档。
- en: '[PRE12]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'We get the following keywords:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到以下关键词：
- en: '[PRE13]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: In this example, we can see that the first two documents were clustered together
    and received the same keywords. Instead of passing all three documents to the
    LLM, we only pass two documents. This can speed things up significantly if you
    have thousands of documents.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们可以看到前两个文档被聚类在一起并获得了相同的关键词。我们只传递两个文档，而不是传递所有三个文档。如果你有成千上万的文档，这可以显著加快速度。
- en: 🏆 Efficient Keyword Extraction with `KeyBERT` & `KeyLLM`
  id: totrans-75
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 🏆 使用`KeyBERT`和`KeyLLM`进行高效关键词提取
- en: Before, we manually passed the embeddings to `KeyLLM` to essentially do a zero-shot
    extraction of keywords. We can further extend this example by leveraging `KeyBERT`.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 之前，我们手动将嵌入传递给`KeyLLM`以进行零-shot 关键词提取。我们可以通过利用`KeyBERT`进一步扩展这个例子。
- en: Since `KeyBERT` generates keywords and embeds the documents, we can leverage
    that to not only simplify the pipeline but suggest a number of keywords to the
    LLM.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 由于`KeyBERT`生成关键词并嵌入文档，我们可以利用这一点不仅简化流程，还可以向 LLM 建议一些关键词。
- en: These suggested keywords can help the LLM decide on the keywords to use. Moreover,
    it allows for everything within `KeyBERT` to be used with `KeyLLM`!
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 这些建议的关键字可以帮助LLM决定使用哪些关键字。此外，这允许`KeyBERT`中的所有内容与`KeyLLM`一起使用！
- en: '![](../Images/fbe7f8a1b581d44e8cc90b0aed10af87.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fbe7f8a1b581d44e8cc90b0aed10af87.png)'
- en: 'This efficient keyword extraction with both `KeyBERT` and `KeyLLM` only requires
    three lines of code! We create a KeyBERT model and assign it the LLM with the
    embedding model we previously created:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 这种高效的关键字提取方法，使用`KeyBERT`和`KeyLLM`只需要三行代码！我们创建了一个KeyBERT模型，并将之前创建的嵌入模型分配给LLM：
- en: '[PRE14]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'We get the following keywords:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到以下关键字：
- en: '[PRE15]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: And that is it! With `KeyLLM` you are able to use Large Language Models to help
    create better keywords. We can choose to extract keywords from the text itself
    or ask the LLM to come up with keywords.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 就这样！使用`KeyLLM`，你可以利用大型语言模型来帮助创建更好的关键字。我们可以选择从文本本身提取关键字，或者让LLM提出关键字。
- en: By combining `KeyLLM` with `KeyBERT`, we increase its potential by doing some
    computation and suggestions beforehand.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将`KeyLLM`与`KeyBERT`结合使用，我们可以通过一些计算和建议来增加其潜力。
- en: '**Update**: I uploaded a video version to YouTube that goes more in-depth into
    how to use KeyLLM:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '**更新**：我在YouTube上上传了一个视频版本，更深入地讲解了如何使用KeyLLM：'
- en: Thank you for reading!
  id: totrans-87
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 感谢阅读！
- en: If you are, like me, passionate about AI and/or Psychology, please feel free
    to add me on [**LinkedIn**](https://www.linkedin.com/in/mgrootendorst/), follow
    me on [**Twitter**](https://twitter.com/MaartenGr), or subscribe to my [**Newsletter**](http://maartengrootendorst.substack.com/).
    You can also find some of my content on my [**Personal Website**](https://maartengrootendorst.com/).
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你和我一样，对AI和/或心理学充满热情，请随时在[**LinkedIn**](https://www.linkedin.com/in/mgrootendorst/)上添加我，在[**Twitter**](https://twitter.com/MaartenGr)上关注我，或者订阅我的[**Newsletter**](http://maartengrootendorst.substack.com/)。你还可以在我的[**个人网站**](https://maartengrootendorst.com/)上找到一些我的内容。
- en: '*All images without a source credit were created by the author — Which means
    all of them, I like creating my own images ;)*'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '*所有没有来源信用的图像都是作者创建的——也就是说，所有图像都是我自己制作的 ;)*'
