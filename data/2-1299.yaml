- en: 'Illuminating Insights: GPT Extracts Meaning from Charts and Tables'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 闪耀的洞察：GPT 从图表和表格中提取意义
- en: 原文：[https://towardsdatascience.com/illuminating-insights-gpt-extracts-meaning-from-charts-and-tables-a0b71c991d34](https://towardsdatascience.com/illuminating-insights-gpt-extracts-meaning-from-charts-and-tables-a0b71c991d34)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/illuminating-insights-gpt-extracts-meaning-from-charts-and-tables-a0b71c991d34](https://towardsdatascience.com/illuminating-insights-gpt-extracts-meaning-from-charts-and-tables-a0b71c991d34)
- en: Using GPT Vision to interpret and aggregate image data.
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 GPT 视觉来解释和汇总图像数据。
- en: '[](https://medium.com/@ilia.teimouri?source=post_page-----a0b71c991d34--------------------------------)[![Ilia
    Teimouri PhD](../Images/0eb948c4d3f81c116cd16fa4d5016629.png)](https://medium.com/@ilia.teimouri?source=post_page-----a0b71c991d34--------------------------------)[](https://towardsdatascience.com/?source=post_page-----a0b71c991d34--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----a0b71c991d34--------------------------------)
    [Ilia Teimouri PhD](https://medium.com/@ilia.teimouri?source=post_page-----a0b71c991d34--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@ilia.teimouri?source=post_page-----a0b71c991d34--------------------------------)[![Ilia
    Teimouri PhD](../Images/0eb948c4d3f81c116cd16fa4d5016629.png)](https://medium.com/@ilia.teimouri?source=post_page-----a0b71c991d34--------------------------------)[](https://towardsdatascience.com/?source=post_page-----a0b71c991d34--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----a0b71c991d34--------------------------------)
    [Ilia Teimouri PhD](https://medium.com/@ilia.teimouri?source=post_page-----a0b71c991d34--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----a0b71c991d34--------------------------------)
    ·7 min read·Dec 24, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----a0b71c991d34--------------------------------)
    ·阅读时间 7 分钟·2023年12月24日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/6f4c9df91feb7c242dca20cf31c52358.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6f4c9df91feb7c242dca20cf31c52358.png)'
- en: Photo by [David Travis](https://unsplash.com/@dtravisphd) on [Unsplash](https://unsplash.com).
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 照片由 [David Travis](https://unsplash.com/@dtravisphd) 拍摄，发布在 [Unsplash](https://unsplash.com)。
- en: Integrating visual inputs like images alongside text and speech into large language
    models (LLMs) is considered an important new direction in AI research by many
    experts in the field. By augmenting these models to handle multiple modes of data
    beyond just language, there is potential to significantly broaden the scope of
    applications they can be utilised for as well as enhance their overall intelligence
    and performance on existing NLP tasks.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 许多领域的专家认为，将图像等视觉输入与文本和语音整合到大型语言模型（LLMs）中，被视为 AI 研究中的一个重要新方向。通过增强这些模型处理除语言之外的多种数据模式，有可能显著拓宽它们的应用范围，同时提高它们在现有自然语言处理任务中的整体智能和性能。
- en: The promise of multimodal AI spans from more engaging user experiences like
    conversational agents that can see their surroundings and refer to objects around
    them, to robots that can fluidly translate commands into physical actions using
    combined knowledge of language and vision. By uniting historically separate areas
    of AI around a unified model architecture, multimodality may accelerate progress
    in tasks relying on multiple skills like visual question answering or image captioning.
    The synergies between learning algorithms, data types, and model designs across
    fields could lead to rapid advancement.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 多模态 AI 的前景从更具吸引力的用户体验，如能够感知其周围环境并提及周围物体的对话代理，到能够通过结合语言和视觉知识流畅地将指令转化为物理动作的机器人。通过将历史上分离的
    AI 领域统一到一个模型架构中，多模态技术可能会加速依赖多种技能的任务的进展，如视觉问答或图像描述。不同领域的学习算法、数据类型和模型设计之间的协同作用可能会导致快速进步。
- en: 'Many companies have already embraced multimodality in various forms: [OpenAI](https://chat.openai.com),
    [Anthropic](http://claude.ai), Google ([Bard](https://bard.google.com) and [Gemini](https://deepmind.google/technologies/gemini/#introduction))
    allow you to upload your own image or text data and chat with them.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 许多公司已经以各种形式采纳了多模态技术：[OpenAI](https://chat.openai.com)，[Anthropic](http://claude.ai)，谷歌的
    [Bard](https://bard.google.com) 和 [Gemini](https://deepmind.google/technologies/gemini/#introduction)
    允许用户上传自己的图像或文本数据并进行聊天。
- en: In this article, I hope to demonstrate a straightforward yet powerful application
    of large language models with computer vision in finance. Equity researchers and
    investment banking analysts may find this especially useful, as you likely spend
    considerable time reading reports and statements containing various tables and
    graphs. Reading lengthly tables and graphs and interpreting them correctly requires
    a great amount of time, knowledge in the field as well as adequate focus to avoid
    mistakes. More tediously, analysts occasionally need to manually enter tabular
    data from PDFs simply to create new charts. An automated solution could alleviate
    these pains by extracting and interpreting key information without the capacity
    for human oversight or fatigue.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我希望展示大语言模型与计算机视觉在金融领域的一种简单而强大的应用。股票研究员和投资银行分析师可能会发现这特别有用，因为你们可能会花费大量时间阅读包含各种表格和图表的报告和声明。阅读冗长的表格和图表并正确解释它们需要大量时间、领域知识以及足够的专注以避免错误。更繁琐的是，分析师偶尔需要手动从PDF中输入表格数据，以便创建新的图表。一个自动化的解决方案可以通过提取和解释关键信息来减轻这些痛苦，而无需人工监督或疲劳。
- en: In fact, by combining NLP with computer vision, we can create an assistant to
    handle many repetitive analytical tasks, freeing analysts to focus on higher-level
    strategy and decision making.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，通过将自然语言处理与计算机视觉相结合，我们可以创建一个助手来处理许多重复的分析任务，从而让分析师专注于更高级的战略和决策制定。
- en: In recent years, there were a lot of advances in using [Optical Character Recognition](https://en.wikipedia.org/wiki/Optical_character_recognition)
    or Visual Document Understanding ([image to text](https://huggingface.co/models?pipeline_tag=image-to-text))
    to extract text from image / PDF data. However, due to the nature of currently
    available training data, the existing methods still struggle with complex layouts
    and formatting found in many financial statements, research reports, and regulatory
    filings.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，在使用 [光学字符识别](https://en.wikipedia.org/wiki/Optical_character_recognition)
    或视觉文档理解（[图像转文本](https://huggingface.co/models?pipeline_tag=image-to-text)）从图像/PDF数据中提取文本方面取得了很多进展。然而，由于当前可用的训练数据的性质，现有方法仍然难以处理许多财务报表、研究报告和监管文件中的复杂布局和格式。
- en: GPT-4V(ision) for tables and graphs
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: GPT-4V(ision)用于表格和图表
- en: 'Back in Sept 2023, OpenAI released [GPT-4 Vision](https://openai.com/research/gpt-4v-system-card).
    According to OpenAI:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在2023年9月，OpenAI 发布了 [GPT-4 Vision](https://openai.com/research/gpt-4v-system-card)。根据OpenAI的说法：
- en: GPT-4 with vision (GPT-4V) enables users to instruct GPT-4 to analyze image
    inputs provided by the user.
  id: totrans-16
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: GPT-4带有视觉功能（GPT-4V）使用户能够指示GPT-4分析用户提供的图像输入。
- en: GPT-4V’s visual skills come from GPT-4, so both models were trained in a similar
    way. First, researchers fed the system huge amounts of text to teach it the fundamentals
    of language. Its goal was to predict the next word in a document. Then came the
    finesse training using an approach called reinforcement learning from human feedback,
    or RLHF for short. This involves fine-tuning the model further based on positive
    reactions from human trainers to produce outputs we find truly helpful.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-4V的视觉能力来自于GPT-4，因此两个模型的训练方式相似。首先，研究人员向系统输入了大量文本，以教会它语言的基本知识。目标是预测文档中的下一个词。然后是使用一种称为人类反馈强化学习（RLHF）的精细训练方法。这涉及根据人类训练者的积极反应进一步微调模型，以产生我们认为真正有用的输出。
- en: In this article, I’m going to make a Steamlit application where the user can
    upload an image and ask various questions about the image. The images that I am
    going to use are screenshots of a financial PDF document. In fact, the document
    is a publicly available [Fund Fact Sheet](https://www.ubs.com/2/e/files/RET/FS_RET_LU2408467723_GB_EN.pdf).
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我将创建一个Steamlit应用程序，用户可以上传图像并询问关于图像的各种问题。我将使用的图像是金融PDF文档的截图。实际上，该文档是公开的
    [基金事实表](https://www.ubs.com/2/e/files/RET/FS_RET_LU2408467723_GB_EN.pdf)。
- en: 'There are two main parts to this code, first is a function to encode the image
    from a given file path:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 代码的主要部分有两个，第一个是一个将图像从给定文件路径编码的函数：
- en: '[PRE0]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'you need this function as the model expect your input image to be in base 64
    encoded format. The next main part of the code would be the way you send your
    request to the OpenAI’s API:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 你需要这个功能，因为模型期望你的输入图像是base 64编码格式。接下来的主要代码部分将是你如何将请求发送到OpenAI的API：
- en: '[PRE1]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: where we set the model name to be `gpt-4-vision-preview`. As you can see this
    is quite different than the usual text to text OpenAI’s API calls. In this case,
    we define a json object called **payload** that contains your text, as well as
    your image data.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将模型名称设置为`gpt-4-vision-preview`。如你所见，这与通常的文本到文本的OpenAI API调用非常不同。在这种情况下，我们定义了一个名为**payload**的json对象，其中包含你的文本以及图像数据。
- en: You can expand the `get_image_analysis` method to send multiple images, or control
    how the model should process the images via `detail` parameter. See more [here](https://platform.openai.com/docs/guides/vision).
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以扩展`get_image_analysis`方法，以发送多个图像，或通过`detail`参数控制模型如何处理图像。详细信息请参见[这里](https://platform.openai.com/docs/guides/vision)。
- en: The rest of the code is mainly the Streamlit method, where we allow users to
    upload their image and interact with the image by asking questions.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 剩余的代码主要是Streamlit方法，我们允许用户上传他们的图像，并通过提问与图像互动。
- en: 'Full code: (also available on [Github](https://github.com/iteimouri/GPT-Vision-for-Finance/tree/main))'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 完整代码：（也可以在[Github](https://github.com/iteimouri/GPT-Vision-for-Finance/tree/main)上找到）
- en: '[PRE2]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Output and summary
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 输出和总结
- en: 'Now let us look at a few examples:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们来看几个例子：
- en: '![](../Images/29792265352740d8b2e97c75cf4f081d.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/29792265352740d8b2e97c75cf4f081d.png)'
- en: Image is generated by the author. The graph is from the publicly available [UBS
    Fund Fact Sheet](https://www.ubs.com/2/e/files/RET/FS_RET_LU2408467723_GB_EN.pdf).
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 图像由作者生成。图表来自公开的[UBS基金事实说明书](https://www.ubs.com/2/e/files/RET/FS_RET_LU2408467723_GB_EN.pdf)。
- en: In this example the question is asked about the peak of the performance. We
    can see that the peak is identified correctly by the model. The model could also
    understand that the dotted line is the index performance from the plot’s legend.
    Understanding dashed and dotted lines are generally difficult in computer vision
    but **given a good quality screenshot** (with enough details), GPT Vision could
    pass the task easily.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，问题是关于性能的峰值。我们可以看到模型正确地识别了峰值。模型还能够理解虚线是图例中的指数表现。在计算机视觉中，理解虚线和点线通常比较困难，但**只要截图质量好**（细节足够），GPT
    Vision 就可以轻松完成任务。
- en: 'Another example:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个例子：
- en: '![](../Images/69cdb2a7eb6d28965195d9f43637b0da.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/69cdb2a7eb6d28965195d9f43637b0da.png)'
- en: Image is generated by the author. The material is from the publicly available
    [UBS Fund Fact Sheet](https://www.ubs.com/2/e/files/RET/FS_RET_LU2408467723_GB_EN.pdf).
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 图像由作者生成。资料来自公开的[UBS基金事实说明书](https://www.ubs.com/2/e/files/RET/FS_RET_LU2408467723_GB_EN.pdf)。
- en: 'In this example, I have tried to examine how good is the model in: 1) extracting
    the relevant table among other data 2) extracting relevant part of the table 3)
    some basic mathematical operations.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我尝试检验模型在以下方面的表现：1）从其他数据中提取相关表格 2）提取表格的相关部分 3）进行一些基本的数学运算。
- en: As demonstrated, the model successfully fulfilled all three requirements for
    this task — no small feat given the complexity involved traditionally. Manually,
    an analyst would have struggled extracting the two-column table locked within
    a PDF, even using optical character recognition (OCR) tools. Additional coding
    would be needed to parse figures into a structured dataframe amenable to aggregation.
    This could consume substantial time before answering the original question. However
    here, one achieves the desired result with only a prompt. Avoiding the cumbersome
    workflow of deciphering images, scraping data, wrangling spreadsheets, and writing
    scripts unlocks tremendous efficiency.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 如所示，模型成功满足了此任务的所有三个要求——鉴于传统上涉及的复杂性，这并非易事。手动操作时，分析师即使使用光学字符识别（OCR）工具，也很难提取锁定在PDF中的双栏表格。还需要额外的编码来将图表解析为结构化的数据框，以便于汇总。这可能会在回答原始问题之前消耗大量时间。然而，在这里，只需一个提示就能实现预期结果。避免了解码图像、抓取数据、处理电子表格和编写脚本的繁琐工作，极大提高了效率。
- en: 'Lastly:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 最后：
- en: '![](../Images/edf565aa98095582b29a80e8090fadd2.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/edf565aa98095582b29a80e8090fadd2.png)'
- en: Image is generated by the author. The material is from the publicly available
    [UBS Fund Fact Sheet](https://www.ubs.com/2/e/files/RET/FS_RET_LU2408467723_GB_EN.pdf).
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 图像由作者生成。资料来自公开的[UBS基金事实说明书](https://www.ubs.com/2/e/files/RET/FS_RET_LU2408467723_GB_EN.pdf)。
- en: Sorting algorithms systematically reorder the elements of a list or array based
    on a specified ordering rule. However, unlike traditional code, LLMs like GPT
    do not have predefined sorting routines hardcoded within.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 排序算法根据指定的排序规则系统地重新排列列表或数组的元素。然而，与传统代码不同，像GPT这样的LLM没有预定义的排序例程。
- en: Instead, GPT is trained to predict the next word in a sequence given prior context.
    With sufficient data and model capacity, the ability to sort emerges from learning
    textual patterns.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，GPT 被训练成根据先前的上下文预测序列中的下一个词。通过足够的数据和模型能力，排序能力从学习文本模式中显现出来。
- en: The example above demonstrates this — GPT correctly sorts two columns in a table
    extracted from a PDF screenshot, a non-trivial feat requiring optical character
    recognition, data extraction, and manipulation skills. Even in Excel, multi-column
    sorting requires some expertise. But by simply providing the goal in a prompt,
    GPT handles these complex steps automatically behind the scenes.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 上面的例子说明了这一点——GPT 正确地对从 PDF 截图中提取的表格中的两列进行排序，这是一项复杂的工作，涉及光学字符识别、数据提取和处理技能。即便在
    Excel 中，多列排序也需要一定的专业知识。但只需在提示中提供目标，GPT 就能自动处理这些复杂的步骤。
- en: Rather than following rigid, step-by-step instructions like traditional algorithms,
    LLMs like GPT develop sorting capabilities through recognizing relationships in
    text during training. This allows them to absorb a variety of abilities from their
    diverse exposure, as opposed to being limited by predefined programming.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 与传统算法遵循严格的逐步指令不同，像 GPT 这样的语言模型通过在训练过程中识别文本中的关系来发展排序能力。这使得它们能够从多样的曝光中吸收各种能力，而不是被预定义的编程所限制。
- en: '**Why is this a big deal?**'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '**为什么这很重要？**'
- en: By harnessing this flexibility into specialised tasks like we see here, prompts
    can unlock efficient problem solving that would otherwise demand significant manual
    effort and technical knowledge.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将这种灵活性应用于我们在这里看到的专业任务，提示可以解锁高效的问题解决方案，这些方案否则将需要大量的手动工作和技术知识。
