- en: Vision-centric Semantic Occupancy Prediction for Autonomous Driving
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 面向视觉的语义占用预测用于自动驾驶
- en: 原文：[https://towardsdatascience.com/vision-centric-semantic-occupancy-prediction-for-autonomous-driving-16a46dbd6f65](https://towardsdatascience.com/vision-centric-semantic-occupancy-prediction-for-autonomous-driving-16a46dbd6f65)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/vision-centric-semantic-occupancy-prediction-for-autonomous-driving-16a46dbd6f65](https://towardsdatascience.com/vision-centric-semantic-occupancy-prediction-for-autonomous-driving-16a46dbd6f65)
- en: A review of the academic “Occupancy Networks” as of 2023H1
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 关于2023年上半年学术“占用网络”的综述
- en: '[](https://medium.com/@patrickllgc?source=post_page-----16a46dbd6f65--------------------------------)[![Patrick
    Langechuan Liu](../Images/fecbf85146a9bde21e6b2251538ddd65.png)](https://medium.com/@patrickllgc?source=post_page-----16a46dbd6f65--------------------------------)[](https://towardsdatascience.com/?source=post_page-----16a46dbd6f65--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----16a46dbd6f65--------------------------------)
    [Patrick Langechuan Liu](https://medium.com/@patrickllgc?source=post_page-----16a46dbd6f65--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@patrickllgc?source=post_page-----16a46dbd6f65--------------------------------)[![Patrick
    Langechuan Liu](../Images/fecbf85146a9bde21e6b2251538ddd65.png)](https://medium.com/@patrickllgc?source=post_page-----16a46dbd6f65--------------------------------)[](https://towardsdatascience.com/?source=post_page-----16a46dbd6f65--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----16a46dbd6f65--------------------------------)
    [Patrick Langechuan Liu](https://medium.com/@patrickllgc?source=post_page-----16a46dbd6f65--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----16a46dbd6f65--------------------------------)
    ·11 min read·May 29, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布在[Towards Data Science](https://towardsdatascience.com/?source=post_page-----16a46dbd6f65--------------------------------)
    ·阅读时间11分钟·2023年5月29日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: One critical pain point of existing 3D object detection methods in autonomous
    driving is that they typically output concise 3D bounding boxes, neglecting finer
    geometric details and struggling to handle general, out-of-vocabulary objects.
    This pain point exists for both [monocular 3D object detection](/monocular-3d-object-detection-in-autonomous-driving-2476a3c7f57e)
    and [BEV multicamera object detection](/monocular-bev-perception-with-transformers-in-autonomous-driving-c41e4a893944).
    To address this problem, **Occupancy Network,** a vision-centric solution for
    general obstacle detection was first introduced in a [keynote speech by Tesla
    in CVPR 2022](https://www.youtube.com/watch?v=jPCV4GKX9Dw) and later popularized
    by [AI Day 2022](https://www.youtube.com/watch?v=ODSJsviD_SU). For more details,
    please refer to the previous blog post series on drivable space.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 现有的3D物体检测方法在自动驾驶中的一个关键痛点是它们通常输出简洁的3D边界框，忽略了更精细的几何细节，并且在处理一般的、超出词汇表的物体时表现不佳。这一痛点存在于[单目3D物体检测](/monocular-3d-object-detection-in-autonomous-driving-2476a3c7f57e)和[BEV多摄像头物体检测](/monocular-bev-perception-with-transformers-in-autonomous-driving-c41e4a893944)中。为了解决这个问题，**占用网络**作为一种面向视觉的通用障碍检测解决方案，首次在[特斯拉在CVPR
    2022的主题演讲](https://www.youtube.com/watch?v=jPCV4GKX9Dw)中介绍，并在[AI Day 2022](https://www.youtube.com/watch?v=ODSJsviD_SU)中推广。更多细节请参见之前关于可行驶空间的博客系列。
- en: '[](https://medium.com/@patrickllgc/drivable-space-in-autonomous-driving-the-industry-7a4624b94d41?source=post_page-----16a46dbd6f65--------------------------------)
    [## Drivable Space in Autonomous Driving — The Industry'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@patrickllgc/drivable-space-in-autonomous-driving-the-industry-7a4624b94d41?source=post_page-----16a46dbd6f65--------------------------------)
    [## 自动驾驶中的可行驶空间——行业概况'
- en: Recent trends in industry application as of 2023
  id: totrans-8
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 截至2023年的行业应用最新趋势
- en: medium.com](https://medium.com/@patrickllgc/drivable-space-in-autonomous-driving-the-industry-7a4624b94d41?source=post_page-----16a46dbd6f65--------------------------------)
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: medium.com](https://medium.com/@patrickllgc/drivable-space-in-autonomous-driving-the-industry-7a4624b94d41?source=post_page-----16a46dbd6f65--------------------------------)
- en: In academia, the equivalent perception track to Occupancy Network is known as
    **Semantic Occupancy Prediction (SOP)** and is sometimes also referred to as **Semantic
    Scene Completion (SSC)**, with some subtle differences between the two explained
    below. Semantic occupancy prediction assigns the occupancy state and semantic
    label to each voxel in the scene. This is a representation general and expressive
    enough to describe objects of known classes but with an irregular shape or objects
    out of the known white list. This blog post will review the state-of-the-art methods
    of semantic occupancy prediction as of early 2023\. This area has received intense
    attention in academia, evidenced by an explosion of papers submitted to top conferences,
    and there is also an [occupancy predicting challenge at this year’s CVPR](https://opendrivelab.com/AD23Challenge.html#rules).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在学术界，与占用网络相对应的感知轨道被称为 **语义占用预测（SOP）**，有时也称为 **语义场景完成（SSC）**，两者之间有一些细微的区别。语义占用预测为场景中的每个体素分配占用状态和语义标签。这是一种通用且足够表达的表示方式，可以描述已知类别但形状不规则或不在已知白名单中的物体。本文将回顾截至
    2023 年初的语义占用预测的最新方法。这个领域在学术界受到广泛关注，顶级会议上提交了大量论文，今年的 CVPR 也有一个[占用预测挑战](https://opendrivelab.com/AD23Challenge.html#rules)。
- en: Semantic Scene Completion vs Semantic Occupancy Prediction
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 语义场景完成 vs 语义占用预测
- en: The concept of semantic scene completion (SSC) was first proposed in the SSCNet
    paper (CVPR 2017) and later popularized by SemanticKITTI (ICCV 2019), which provided
    an official dataset and competition track. More recently, a slightly different
    task semantic occupancy prediction (SOP) emerged. Both SSC and SOP aim to predict
    the occupancy status and semantic class of a voxel in a given spatial location,
    but there are several subtle differences. First, the input modality for SSC is
    partial 3D data collected by LiDAR or other active depth sensors, hence the name
    “completion” of a 3D semantic scene. SOP uses 2D images, possibly multicamera
    and multiframe, as input. In addition, SSC typically focuses on static scenes
    while SOP can handle dynamic objects as well. In summary, **SOP seems to be a
    more general and preferred terminology**, and in this article, we will use Semantic
    Occupancy Prediction, Semantic Scene Completion, and Occupancy Network interchangeably.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 语义场景完成（SSC）的概念最初在 SSCNet 论文（CVPR 2017）中提出，后来由 SemanticKITTI（ICCV 2019）普及，该数据集提供了官方数据集和比赛赛道。最近，出现了一种略有不同的任务语义占用预测（SOP）。SSC
    和 SOP 都旨在预测给定空间位置处体素的占用状态和语义类别，但存在一些细微差别。首先，SSC 的输入模式是由 LiDAR 或其他主动深度传感器收集的部分
    3D 数据，因此称为 3D 语义场景的“完成”。SOP 使用 2D 图像，可能是多摄像头和多帧的。除此之外，SSC 通常关注静态场景，而 SOP 也可以处理动态物体。总之，**SOP
    似乎是更通用和更受欢迎的术语**，在本文中，我们将交替使用语义占用预测、语义场景完成和占用网络。
- en: The pioneering work of monoScene first uses monocular images to conduct the
    semantic occupacny prediction task on SemanticKITTI. It still refers to the task
    as SSC, perhaps due to the fact that SemanticKITTI contains mostly static scenes.
    Later studies prefer the term SOP which expands the task to other dataset such
    as NuScenes and Waymo and handles dynamic objects as well.
  id: totrans-13
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: MonoScene 的开创性工作首次使用单目图像在 SemanticKITTI 上进行语义占用预测任务。它仍然将任务称为 SSC，这可能是因为 SemanticKITTI
    主要包含静态场景。后续研究更倾向于使用 SOP 这一术语，将任务扩展到 NuScenes 和 Waymo 等其他数据集，并处理动态物体。
- en: High-level summary of the pioneering works
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 开创性工作的高级总结
- en: Here I will first summarize the explosion of research studies during the past
    year on a high level, and then follow up with a summary of the various technical
    details. Below is a diagram summarizing the overall development thread of the
    work to be reviewed. It is worth noting that the field is still rapidly evolving,
    and has yet to converge to a universally accepted dataset and evaluation metric.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 我将首先总结过去一年研究的爆炸性增长，然后跟进各种技术细节的总结。下面是一个总结待审阅工作的整体发展脉络的图示。值得注意的是，该领域仍在快速发展，并且尚未趋向于一个普遍接受的数据集和评估指标。
- en: '![](../Images/e3f6874a73649486a923b5f0b416c9e9.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e3f6874a73649486a923b5f0b416c9e9.png)'
- en: 'The development timeline for the field of semantic occupancy prediction (source:
    created by the author)'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 语义占用预测领域的发展时间线（来源：作者创作）
- en: MonoScene (CVPR 2022), first vision-input attempt
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: MonoScene（CVPR 2022），首次尝试视觉输入
- en: '[MonoScene](https://arxiv.org/abs/2112.00726) is the first work to reconstruct
    outdoor scenes using only **RGB images as inputs**, as opposed to lidar point
    clouds that previous studies used. It is a single-camera solution, focusing on
    the front-camera-only SemanticKITTI dataset.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '[MonoScene](https://arxiv.org/abs/2112.00726)是首个仅使用**RGB图像作为输入**来重建户外场景的工作，相较于以往研究中使用的激光雷达点云。它是一个单摄像头解决方案，专注于前摄像头的SemanticKITTI数据集。'
- en: '![](../Images/549c23217f96c5d843e53be4a3cae95a.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/549c23217f96c5d843e53be4a3cae95a.png)'
- en: 'The architecture of MonoScene (source: [MonoScene](https://arxiv.org/abs/2112.00726))'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: MonoScene的架构（来源：[MonoScene](https://arxiv.org/abs/2112.00726)）
- en: The paper proposes many ideas, but only one design choice seems critical — FLoSP
    (Feature Line of Sight Projection). This idea is similar to the idea of feature
    propagation along the line of sight, also adopted by [OFT](https://arxiv.org/abs/1811.08188)
    (BMVC 2019) or [Lift-Splat-Shoot](https://arxiv.org/abs/2008.05711) (ECCV 2020).
    Other novelties such as Context Relation Prior and unique losses inspired by directly
    optimizing the metrics seem not that useful according to the ablation study.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 论文提出了许多想法，但似乎只有一个设计选择至关重要——FLoSP（Feature Line of Sight Projection）。这个想法类似于沿视线传播特征的思想，也被[OFT](https://arxiv.org/abs/1811.08188)（BMVC
    2019）或[Lift-Splat-Shoot](https://arxiv.org/abs/2008.05711)（ECCV 2020）所采用。其他新颖性如Context
    Relation Prior和直接优化度量的独特损失，根据消融研究看来似乎不那么有用。
- en: VoxFormer (CVPR 2023), significantly improved monoScene
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: VoxFormer (CVPR 2023)，显著改进了monoScene
- en: 'The key insight of [VoxFormer](https://arxiv.org/abs/2302.12251) is that SOP/SSC
    has to address two issues simultaneously: scene reconstruction for visible areas
    and scene hallucination for occluded regions. VoxFormer proposes a **reconstruct-and-densify**
    approach. In the first reconstruction stage, the paper lifts RGB pixels to a pseudo-LiDAR
    point cloud with monodepth methods, and then voxelize it into initial query proposals.
    In the second densification stage, these sparse queries are enhanced with image
    features and use self-attention for label propagation to generate a dense prediction.
    VoxFormer **significantly outperformed MonoScene** on SemanticKITTI and is still
    a single-camera solution. The image feature enhancement architecture heavily borrows
    the deformable attention idea from [BEVFormer](https://arxiv.org/abs/2203.17270).'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '[VoxFormer](https://arxiv.org/abs/2302.12251)的关键见解在于SOP/SSC必须同时解决两个问题：可见区域的场景重建和遮挡区域的场景幻觉。VoxFormer提出了一种**重建和密集化**的方法。在第一次重建阶段，论文利用单目深度方法将RGB像素提升为伪LiDAR点云，然后将其体素化为初始查询提案。在第二次密集化阶段，这些稀疏的查询通过图像特征增强，并利用自注意力进行标签传播以生成密集预测。VoxFormer在SemanticKITTI上的性能**显著优于MonoScene**，且仍为单摄像头解决方案。图像特征增强架构大量借鉴了[BEVFormer](https://arxiv.org/abs/2203.17270)的变形注意力思想。'
- en: '![](../Images/241311f033f0444223ba7a9b8c62f2bc.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/241311f033f0444223ba7a9b8c62f2bc.png)'
- en: 'The architecture of VoxFormer (source: [VoxFormer](https://arxiv.org/abs/2302.12251))'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: VoxFormer的架构（来源：[VoxFormer](https://arxiv.org/abs/2302.12251)）
- en: TPVFormer (CVPR 2023), the first multi-camera attempt
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: TPVFormer (CVPR 2023)，首次多摄像头尝试
- en: '[TPVFormer](https://arxiv.org/abs/2302.07817) is the first work to generalize
    3D semantic occupancy prediction to a **multi-camera** setup and extends the idea
    of SOP/SSC from semanticKITTI to NuScenes.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '[TPVFormer](https://arxiv.org/abs/2302.07817)是首个将3D语义占用预测推广到**多摄像头**设置的工作，并将SOP/SSC的理念从SemanticKITTI扩展到NuScenes。'
- en: '![](../Images/ea3ac7032f34f3f359611c10fac160ea.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ea3ac7032f34f3f359611c10fac160ea.png)'
- en: 'The architecture of TPVFormer (source: [TPVFormer](https://arxiv.org/abs/2302.07817))'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: TPVFormer的架构（来源：[TPVFormer](https://arxiv.org/abs/2302.07817)）
- en: TPVFormer extends the idea of BEV to three orthogonal axes. This allows the
    modeling of 3D without suppressing any axes and avoids cubic complexity. Concretely
    TPVFormer proposes two steps of attention to generating TPV features. First, it
    uses image cross-attention (ICA) to get TPV features. This essentially borrows
    the idea of [BEVFormer](https://arxiv.org/abs/2203.17270) and extends to the other
    two orthogonal directions to form a TriPlane View feature. Then it uses cross-view
    hybrid attention (CVHA) to enhance each TPV feature by attending to the other
    two.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: TPVFormer将BEV的理念扩展到三个正交轴。这允许在不压制任何轴的情况下建模3D，避免了立方复杂度。具体来说，TPVFormer提出了两个步骤的注意力来生成TPV特征。首先，它使用图像交叉注意力（ICA）来获取TPV特征。这本质上借鉴了[BEVFormer](https://arxiv.org/abs/2203.17270)的思想，并扩展到其他两个正交方向以形成TriPlane
    View特征。然后，它使用交叉视图混合注意力（CVHA）通过关注其他两个方向来增强每个TPV特征。
- en: '![](../Images/77b49024951fabb39719cd3265b80d78.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/77b49024951fabb39719cd3265b80d78.png)'
- en: 'The prediction is denser than supervision in TPVFormer, but still has gaps
    and holes (source: [TPVFormer](https://arxiv.org/abs/2302.07817))'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 预测比TPVFormer中的监督更密集，但仍然存在间隙和孔洞（来源：[TPVFormer](https://arxiv.org/abs/2302.07817)）
- en: TPVFormer uses supervision from sparse lidar points from the vanilla NuScenes
    dataset, without any multiframe densification or reconstruction. It claimed that
    the model can predict denser and more consistent volume occupancy for all voxels
    at inference time, despite the sparse supervision at training time. However, the
    *denser* prediction is still not as dense as compared to later studies such as
    SurroundOcc which uses densified NuScenes dataset.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: TPVFormer使用了来自普通NuScenes数据集的稀疏激光雷达点进行监督，没有任何多帧密集化或重建。尽管在训练时进行的监督是稀疏的，但它声称模型能够在推理时预测所有体素的更密集和一致的体积占据。然而，这种*密集*的预测仍然不如后来的研究，如使用密集NuScenes数据集的SurroundOcc那样密集。
- en: SurroundOcc (Arxiv 2023/03) and OpenOccupancy (Arxiv 2023/03), the first attempts
    at dense label supervision
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: SurroundOcc（Arxiv 2023/03）和OpenOccupancy（Arxiv 2023/03），是首次尝试密集标签监督
- en: '[**SurroundOcc**](https://arxiv.org/abs/2303.09551) argues that dense prediction
    requires **dense labels**. The paper successfully demonstrated that denser labels
    can significantly improve the performance of previous methods, such as TPVFormer,
    by almost 3x. Its most significant contribution is a pipeline for generating dense
    occupancy ground truth without the need for costly human annotation.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '[**SurroundOcc**](https://arxiv.org/abs/2303.09551)认为密集预测需要**密集标签**。论文成功证明了更密集的标签可以显著提高之前方法（如TPVFormer）的性能，提升近3倍。其最大贡献是生成密集占据真实数据的管道，无需昂贵的人为标注。'
- en: '![](../Images/695612dd57a7890ced70ae9a393163e0.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/695612dd57a7890ced70ae9a393163e0.png)'
- en: 'GT generation pipeline of SurroundOcc (source: [SurroundOcc](https://arxiv.org/abs/2303.09551))'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: SurroundOcc的GT生成管道（来源：[SurroundOcc](https://arxiv.org/abs/2303.09551)）
- en: 'The generation of dense occupancy labels involves two steps: multiframe data
    aggregation and densification. First, multi-frame lidar points of dynamic objects
    and static scenes are stitched separately. The accumulated data is denser than
    a single frame measurement, but it still has many holes and requires further densification.
    The densification is performed by Poisson Surface Reconstruction of a triangular
    mesh, and Nearest Neighbor (NN) to propagate the labels to newly filled voxels.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 密集占据标签的生成涉及两个步骤：多帧数据汇总和密集化。首先，将动态物体和静态场景的多帧激光雷达点分别拼接在一起。累计的数据比单帧测量更密集，但仍然存在许多孔洞，需要进一步密集化。密集化是通过三角网格的泊松表面重建和最近邻（NN）将标签传播到新填充的体素上来完成的。
- en: '[**OpenOccupancy**](https://arxiv.org/abs/2303.03991) is contemporary to and
    similar in spirit to SurroundOcc. Like SurroundOcc, OpenOccupancy also uses a
    pipeline that first aggregates multiframe lidar measurements for dynamic objects
    and static scenes separately. For further densification, instead of Poisson Reconstruction
    adopted by [SurroundOcc](https://arxiv.org/abs/2303.09551), OpenOccupancy uses
    an **Augment-and-Purify** (AAP) approach. Concretely, a baseline model is trained
    with the aggregated raw label, and its prediction result is used to fuse with
    the original label to generate a denser label (aka “augment”). The denser label
    is roughly 2x denser, and manually refined by human labelers (aka “purify”). A
    total of 4000 human hours were invested to refine the label for nuScenes, roughly
    4 human hours per 20-second clip.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '[**OpenOccupancy**](https://arxiv.org/abs/2303.03991)与SurroundOcc同时期且在精神上相似。与SurroundOcc一样，OpenOccupancy也使用了一个管道，该管道首先分别汇总动态物体和静态场景的多帧激光雷达测量数据。为了进一步密集化，OpenOccupancy采用了**Augment-and-Purify**（AAP）方法，而不是[SurroundOcc](https://arxiv.org/abs/2303.09551)所采用的泊松重建方法。具体来说，一个基线模型使用汇总的原始标签进行训练，然后用其预测结果与原始标签融合以生成更密集的标签（即“增强”）。这种更密集的标签大约密集2倍，并由人工标注者进行手动精炼（即“净化”）。总共投入了4000小时的人工时间来精炼nuScenes的标签，大约每20秒的片段需要4小时的人工时间。'
- en: '![](../Images/fd166ca4f667d6af763d73edaff06fc2.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fd166ca4f667d6af763d73edaff06fc2.png)'
- en: 'The architecture of SurroundOcc (source: [SurroundOcc](https://arxiv.org/abs/2303.09551))'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: SurroundOcc的架构（来源：[SurroundOcc](https://arxiv.org/abs/2303.09551)）
- en: '![](../Images/8f0cb1a6bd81845c18d98a51d7a5c656.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8f0cb1a6bd81845c18d98a51d7a5c656.png)'
- en: 'The architecture of CONet (source: [OpenOccupancy](https://arxiv.org/abs/2303.03991))'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: CONet的架构（来源：[OpenOccupancy](https://arxiv.org/abs/2303.03991)）
- en: Compared to the contribution in the creation of the dense label generation pipeline,
    the network architecture of SurroundOcc and OpenOccupancy are not as innovative.
    SurroundOcc is largely based on [BEVFormer](https://arxiv.org/abs/2203.17270),
    with a coarse-to-fine step to enhance 3D features. OpenOccupancy proposes CONet
    (cascaded occupancy network) which uses an approach similar to that of [Lift-Splat-Shoot](https://arxiv.org/abs/2008.05711)
    to lift 2D features to 3D and then enhances 3D features through a cascaded scheme.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 与密集标签生成管道的贡献相比，SurroundOcc和OpenOccupancy的网络架构不如创新。SurroundOcc主要基于[BEVFormer](https://arxiv.org/abs/2203.17270)，通过粗到细的步骤来增强3D特征。OpenOccupancy提出了CONet（级联占用网络），其方法类似于[Lift-Splat-Shoot](https://arxiv.org/abs/2008.05711)，将2D特征提升到3D，然后通过级联方案增强3D特征。
- en: Occ3D (Arxiv 2023/04), the first attempt at occlusion reasoning
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Occ3D（Arxiv 2023/04），首次尝试遮挡推理
- en: '[Occ3D](https://arxiv.org/abs/2304.14365) also proposed a pipeline to generate
    dense occupancy labels, which includes point cloud aggregation, point labeling,
    and occlusion handling. It is the first paper that **explicitly handles the visibility
    and occlusion reasoning** of the dense label. Visibility and occlusion reasoning
    are critically important for the onboard deployment of SOP models. Special treatment
    on occlusion and visibility is necessary during training to avoid false positives
    from over-hallucination about the unobservable scene.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '[Occ3D](https://arxiv.org/abs/2304.14365)还提出了一种生成密集占用标签的管道，包括点云聚合、点标注和遮挡处理。这是首篇**明确处理密集标签的可见性和遮挡推理**的论文。可见性和遮挡推理对于SOP模型的车载部署至关重要。在训练过程中，必须对遮挡和可见性进行特殊处理，以避免对不可观察场景的过度幻想导致假阳性。'
- en: It is noteworthy that lidar visibility is different from camera visibility.
    Lidar visibility describes the **completeness** of the dense label, as some voxels
    are not observable even after multiframe data aggregation. It is consistent across
    the whole sequence. Meanwhile, camera visibility focuses on the **possibility
    of detection** of onboard sensors without hallucination and differs at each timestamp.
    Eval is only performed on the “visible” voxels in both the LiDAR and camera views.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，激光雷达的可见性与相机的可见性不同。激光雷达的可见性描述了**完整性**，因为有些体素在多帧数据聚合后仍不可观察。这在整个序列中是一致的。同时，相机的可见性关注于**检测的可能性**，而且在每个时间戳上不同。评估只在激光雷达和相机视图中的“可见”体素上进行。
- en: In the preparation of dense labels, Occ3D only relies on the multiframe data
    aggregation and does not have the second densification stage as in SurroundOcc
    and OpenOccupancy. The authors claimed that for the Waymo dataset, the label is
    already quite dense without densification. For nuScenes, although the annotation
    still does have holes after point cloud aggregation, Poisson Reconstruction leads
    to inaccurate results, therefore no densification step is performed. Maybe the
    Augment-and-Purify approach by OpenOccupancy is more practical in this setting.
  id: totrans-49
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 在密集标签的准备过程中，Occ3D仅依赖于多帧数据聚合，没有像SurroundOcc和OpenOccupancy那样的第二阶段密集化。作者声称，对于Waymo数据集，标签在没有密集化的情况下已经相当密集。对于nuScenes，尽管点云聚合后标注仍有孔洞，但Poisson重建导致结果不准确，因此没有执行密集化步骤。也许OpenOccupancy的Augment-and-Purify方法在这种情况下更为实用。
- en: '![](../Images/2031886728b6199cd9a503511fb64bb1.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2031886728b6199cd9a503511fb64bb1.png)'
- en: 'The architecture of CTF-Occ in Occ3D (source: [Occ3D](https://arxiv.org/abs/2304.14365))'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: CTF-Occ在Occ3D中的架构（来源：[Occ3D](https://arxiv.org/abs/2304.14365)）
- en: Occ3D also proposed a neural network architecture Coarse-to-Fine Occupancy (CTF-Occ).
    The coarse-to-fine idea is largely the same as that in OpenOccupancy and SurroundOcc.
    CTF-Occ proposed incremental token selection to reduce the computation burden.
    It also proposed an implicit decoder to output the semantic label of any given
    point, similar to the idea of Occupancy Networks.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: Occ3D还提出了一种神经网络架构：粗到细占用（CTF-Occ）。粗到细的思想与OpenOccupancy和SurroundOcc中的基本相同。CTF-Occ提出了增量标记选择以减少计算负担。它还提出了一种隐式解码器，用于输出任何给定点的语义标签，类似于占用网络的思想。
- en: Comparisons of Technical Details
  id: totrans-53
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术细节比较
- en: The Semantic Occupancy Prediction studies reviewed above are summarized in the
    following table, in terms of network architecture, training losses, evaluation
    metrics, and detection range and resolution.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 上述关于语义占用预测的研究总结在下表中，包括网络架构、训练损失、评估指标以及检测范围和分辨率。
- en: '![](../Images/5e313303ee3b7e8967d9b0a7c98a90fa.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5e313303ee3b7e8967d9b0a7c98a90fa.png)'
- en: 'Comparison of technical details of recent Semantic Occupancy Prediction papers
    as of 2023/04 (source: created by the author, [text version](https://github.com/patrick-llgc/Learning-Deep-Learning/blob/master/topics/topic_occupancy_network.md))'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 最近的语义占用预测论文技术细节的比较截至2023年4月（来源：作者创建，[文本版](https://github.com/patrick-llgc/Learning-Deep-Learning/blob/master/topics/topic_occupancy_network.md)）
- en: '**Network Architecture**'
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**网络架构**'
- en: Most of the studies are based on proven state-of-the-art methods for BEV perception,
    such as [BEVFormer](https://arxiv.org/abs/2203.17270) and [Lift, Splat, Shoot](https://arxiv.org/abs/2008.05711).
    The architecture can be largely divided into two stages, 2D-to-3D feature lifting,
    and 3D feature enhancement. See the above table for a more detailed summary. The
    architecture seems to have largely converged. **What matters most is the** **dense
    occupancy annotation generation pipeline, and dense supervision during training**.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数研究基于已验证的BEV感知最先进方法，如[BEVFormer](https://arxiv.org/abs/2203.17270)和[Lift,
    Splat, Shoot](https://arxiv.org/abs/2008.05711)。架构主要分为两个阶段：2D到3D特征提升和3D特征增强。有关更详细的总结，请参见上表。架构似乎已基本趋于一致。**最重要的是密集占用注释生成管道，以及训练过程中的密集监督**。
- en: Below is a summary of the autolabel pipeline to generate dense occupancy labels
    in [SurroundOcc](https://arxiv.org/abs/2303.09551), [OpenOccupancy](https://arxiv.org/abs/2303.03991),
    and [Occ3D](https://arxiv.org/abs/2304.14365).
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是[SurroundOcc](https://arxiv.org/abs/2303.09551)、[OpenOccupancy](https://arxiv.org/abs/2303.03991)和[Occ3D](https://arxiv.org/abs/2304.14365)中生成密集占用标签的自动标记管道的总结。
- en: '![](../Images/10fc1b52198a8832209a2000adc928cd.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/10fc1b52198a8832209a2000adc928cd.png)'
- en: 'Summary of dense label pipeline in [SurroundOcc](https://arxiv.org/abs/2303.09551),
    [OpenOccupancy](https://arxiv.org/abs/2303.03991), and [Occ3D](https://arxiv.org/abs/2304.14365)
    (source: created by the author)'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '[SurroundOcc](https://arxiv.org/abs/2303.09551)、[OpenOccupancy](https://arxiv.org/abs/2303.03991)和[Occ3D](https://arxiv.org/abs/2304.14365)中密集标签管道的总结（来源：作者创建）'
- en: Training Loss
  id: totrans-62
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练损失
- en: The semantic occupancy prediction task is very similar to semantic segmentation,
    in that SOP has to predict one semantic label for every voxel in 3D space, while
    semantic segmentation has to predict one semantic label for every measurement
    point, be it a pixel per image, or a 3D point per lidar scan. The main loss for
    semantic segmentation has been cross-entropy loss and [Lovasz loss](https://openaccess.thecvf.com/content_cvpr_2018/papers/Berman_The_LovaSz-Softmax_Loss_CVPR_2018_paper.pdf).
    The Lovasz extension enables direct optimization of the mean intersection-over-union
    (IoU) metric in neural networks.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 语义占用预测任务与语义分割非常相似，SOP需要对3D空间中的每个体素预测一个语义标签，而语义分割需要对每个测量点（无论是图像中的像素，还是激光雷达扫描中的3D点）进行预测。语义分割的主要损失函数是交叉熵损失和[Lovasz损失](https://openaccess.thecvf.com/content_cvpr_2018/papers/Berman_The_LovaSz-Softmax_Loss_CVPR_2018_paper.pdf)。Lovasz扩展使得神经网络可以直接优化平均交集-并集（IoU）指标。
- en: Perhaps inspired by Lovasz, monoScene proposed several other losses that can
    directly optimize evaluation metrics. However, they seem esoteric and not fully
    supported by ablation studies.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 可能受到Lovasz的启发，monoScene提出了几种其他损失函数，可以直接优化评估指标。然而，它们似乎较为深奥，并未完全通过消融研究得到支持。
- en: Evaluation Metrics
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 评估指标
- en: The primary metric is IoU for geometry occupancy prediction (whether a voxel
    is occupied) and mIoU (mean IoU) for semantic classification (which class an occupied
    voxel belongs to). These metrics perhaps would be **inadequate for industrial
    applications.**
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 主要指标是几何占用预测的IoU（一个体素是否被占用）和语义分类的mIoU（平均IoU）。这些指标可能**不适合工业应用**。
- en: Vision-based SOP task needs to mature for industrial use and to replace lidar.
    Although both precision and recall matter as is calculated in the IoU metric,
    precision is always more important for ADAS (Advanced Driver Assistance Systems)
    applications to avoid [phantom braking](https://electrek.co/2022/06/03/tesla-respond-increase-phantom-braking-complaints/),
    as long as we still have a driver behind the wheel.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 基于视觉的SOP任务需要成熟以满足工业使用并取代激光雷达。虽然IoU指标中计算了精度和召回率，但精度在ADAS（高级驾驶员辅助系统）应用中始终更为重要，以避免[虚假刹车](https://electrek.co/2022/06/03/tesla-respond-increase-phantom-braking-complaints/)，只要我们还有司机在驾驶。
- en: Detection Range and Resolution
  id: totrans-68
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 检测范围和分辨率
- en: All current tracks predict 50 meters around the ego vehicle. Voxel resolution
    varies from 0.2 m for SemanticKITTI to 0.4 m or 0.5 m for NuScenes and Waymo datasets.
    This is a good starting point, but perhaps still **inadequate for industrial applications**.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 目前所有跟踪算法在自车周围预测50米的范围。体素分辨率从SemanticKITTI的0.2米到NuScenes和Waymo数据集的0.4米或0.5米不等。这是一个良好的起点，但可能仍然**不适用于工业应用**。
- en: A more reasonable resolution and range may be 0.2 m for the range within 50
    m, and 0.4 m for the range between 50 m to 100 m.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 一个更合理的分辨率和范围可能是50米范围内的0.2米，以及50米到100米范围内的0.4米。
- en: Related tasks
  id: totrans-71
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 相关任务
- en: There are two related tasks to SOP, Surrounding Depth Maps and Lidar Semantic
    Segmentation, which we will briefly review below.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 有两个与SOP相关的任务，即周围深度图和激光雷达语义分割，我们将在下面简要回顾。
- en: Surround depth map prediction task (such as [FSM](https://arxiv.org/abs/2104.00152)
    and [SurroundDepth](https://arxiv.org/abs/2204.03636)) expands monocular depth
    prediction and leverages the consistency in overlapped camera field-of-view to
    further improve the performance. It focuses more on the measurement source by
    giving each pixel in the images a depth value (bottom-up), while SOP focuses more
    on the application target in the BEV space (top-down). The same analogy is between
    Lift-Splat-Shoot and BEVFormer for BEV perception, where the former is a bottom-up
    approach and the latter is top-down.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 周围深度图预测任务（例如 [FSM](https://arxiv.org/abs/2104.00152) 和 [SurroundDepth](https://arxiv.org/abs/2204.03636)）扩展了单目深度预测，并利用重叠摄像头视场中的一致性来进一步提升性能。它更多关注测量源，通过为图像中的每个像素赋予深度值（自下而上），而SOP则更多关注BEV空间中的应用目标（自上而下）。Lift-Splat-Shoot与BEVFormer在BEV感知中的类似情况也是如此，前者是自下而上的方法，后者是自上而下的。
- en: Lidar Semantic Segmentation focuses on assigning each lidar point cloud in a
    lidar scan a semantic class label. Real-world sensing in 3D is inherently sparse
    and incomplete. For holistic semantic understanding, it is insufficient to solely
    parse the sparse measurements while ignoring the unobserved scene structures.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 激光雷达语义分割专注于为激光雷达扫描中的每个点云分配语义类别标签。现实世界中的3D感知本质上是稀疏和不完整的。为了全面的语义理解，单纯解析稀疏测量而忽略未观察到的场景结构是不够的。
- en: Takeaways
  id: totrans-75
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 主要收获
- en: The neural network architecture in semantic occupancy prediction seems to have
    largely converged. What matters the most is the autolabel pipeline to generate
    dense occupancy labels and dense supervision during training.
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 语义占用预测中的神经网络架构似乎已经基本收敛。最重要的是自动标签管道，以生成密集的占用标签和训练过程中的密集监督。
- en: The current detection range and voxel resolution adopted by common datasets
    would be inadequate for industrial applications. We need more detection range
    (e.g., 100 m) at a finer resolution (e.g., 0.2 m).
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当前常用数据集采用的检测范围和体素分辨率对于工业应用来说是不够的。我们需要更大的检测范围（例如100米）和更细的分辨率（例如0.2米）。
- en: The current evaluation metrics would be inadequate for industrial applications
    as well. Precision is more important than recall for ADAS applications to avoid
    frequent phantom braking.
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 目前的评估指标对于工业应用也显得不够。对于ADAS应用，精确度比召回率更为重要，以避免频繁的虚假刹车。
- en: Future directions of semantic occupancy prediction may include scene flow estimation.
    This will help the prediction of future trajectories of unknown obstacles and
    collision avoidance during trajectory planning for the ego vehicle.
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 语义占用预测的未来方向可能包括场景流估计。这将有助于预测未知障碍物的未来轨迹，并在自车轨迹规划过程中进行碰撞避免。
- en: 'Note: All images in this blog post are either created by the author, or from
    academic papers publicly available. See captions for details.'
  id: totrans-80
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 注：本博客文章中的所有图片均由作者创作，或来自公开的学术论文。详情请参见图注。
- en: Reference
  id: totrans-81
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[**SSCNet**: Semantic Scene Completion from a Single Depth Image](https://arxiv.org/abs/1611.08974),
    CVPR 2017'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**SSCNet**: 从单幅深度图像进行语义场景补全](https://arxiv.org/abs/1611.08974)，CVPR 2017'
- en: '[**SemanticKITTI**: A Dataset for Semantic Scene Understanding of LiDAR Sequences](https://arxiv.org/abs/1904.01416),
    ICCV 2019'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**SemanticKITTI**: 用于激光雷达序列语义场景理解的数据集](https://arxiv.org/abs/1904.01416)，ICCV
    2019'
- en: '[**MonoScene**: Monocular 3D Semantic Scene Completion](https://arxiv.org/abs/2112.00726),
    CVPR 2022'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**MonoScene**: 单目3D语义场景补全](https://arxiv.org/abs/2112.00726)，CVPR 2022'
- en: '[**VoxFormer**: Sparse Voxel Transformer for Camera-based 3D Semantic Scene
    Completion](https://arxiv.org/abs/2302.12251), CVPR 2023'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**VoxFormer**: 基于相机的稀疏体素变换器用于3D语义场景补全](https://arxiv.org/abs/2302.12251)，CVPR
    2023'
- en: '[**TPVFormer**: Tri-Perspective View for Vision-Based 3D Semantic Occupancy
    Prediction](https://arxiv.org/abs/2302.07817), CVPR 2023'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**TPVFormer**: 三视角视图用于基于视觉的 3D 语义占用预测](https://arxiv.org/abs/2302.07817)，CVPR
    2023'
- en: '[**Occ3D**: A Large-Scale 3D Occupancy Prediction Benchmark for Autonomous
    Driving](https://arxiv.org/abs/2304.14365), Arxiv 2023/04'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**Occ3D**: 自主驾驶的大规模 3D 占用预测基准](https://arxiv.org/abs/2304.14365)，Arxiv 2023/04'
- en: '[**SurroundOcc**: Multi-Camera 3D Occupancy Prediction for Autonomous Driving](https://arxiv.org/abs/2303.09551),
    Arxiv 2023/03'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**SurroundOcc**: 自主驾驶的多摄像头 3D 占用预测](https://arxiv.org/abs/2303.09551)，Arxiv
    2023/03'
- en: '[**OpenOccupancy**: A Large Scale Benchmark for Surrounding Semantic Occupancy
    Perception](https://arxiv.org/abs/2303.03991), Arxiv 2023/03'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**OpenOccupancy**: 周围语义占用感知的大规模基准](https://arxiv.org/abs/2303.03991)，Arxiv
    2023/03'
- en: '[**SimpleOccupancy**: A Simple Attempt for 3D Occupancy Estimation in Autonomous
    Driving](https://arxiv.org/abs/2303.10076), Arxiv 2023/03'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**SimpleOccupancy**: 自主驾驶中 3D 占用估计的简单尝试](https://arxiv.org/abs/2303.10076)，Arxiv
    2023/03'
- en: '[**OccFormer**: Dual-path Transformer for Vision-based 3D Semantic Occupancy
    Prediction](https://arxiv.org/abs/2304.05316), Arxiv 2023/04'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**OccFormer**: 基于视觉的 3D 语义占用预测的双路径变换器](https://arxiv.org/abs/2304.05316)，Arxiv
    2023/04'
- en: '[**BEVFormer**: Learning Bird’s-Eye-View Representation from Multi-Camera Images
    via Spatiotemporal Transformers](https://arxiv.org/abs/2203.17270), ECCV 2022'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**BEVFormer**: 通过时空变换器从多摄像头图像学习鸟瞰视图表示](https://arxiv.org/abs/2203.17270)，ECCV
    2022'
- en: '[**FSM**: Full Surround Monodepth from Multiple Cameras](https://arxiv.org/abs/2104.00152),
    ICRA 2021'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**FSM**: 来自多摄像头的完整周围单目深度](https://arxiv.org/abs/2104.00152)，ICRA 2021'
- en: '[**SurroundDepth**: Entangling Surrounding Views for Self-Supervised Multi-Camera
    Depth Estimation](https://arxiv.org/abs/2204.03636), CoRL 2022'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**SurroundDepth**: 通过周围视图纠缠进行自监督多摄像头深度估计](https://arxiv.org/abs/2204.03636)，CoRL
    2022'
- en: '[**Lift, Splat, Shoot (LSS)**: Encoding Images From Arbitrary Camera Rigs by
    Implicitly Unprojecting to 3D](https://arxiv.org/abs/2008.05711), ECCV 2020'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**Lift, Splat, Shoot (LSS)**: 通过隐式反投影到 3D 来编码来自任意相机设备的图像](https://arxiv.org/abs/2008.05711)，ECCV
    2020'
- en: '[**OFT**: Orthographic Feature Transform for Monocular 3D Object Detection](https://arxiv.org/abs/1811.08188),
    BMVC 2019'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**OFT**: 单目 3D 物体检测的正射特征变换](https://arxiv.org/abs/1811.08188)，BMVC 2019'
