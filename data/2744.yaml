- en: A Beginnerâ€™s Guide to LLM Fine-Tuning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LLMå¾®è°ƒåˆå­¦è€…æŒ‡å—
- en: åŸæ–‡ï¼š[https://towardsdatascience.com/a-beginners-guide-to-llm-fine-tuning-4bae7d4da672?source=collection_archive---------1-----------------------#2023-08-30](https://towardsdatascience.com/a-beginners-guide-to-llm-fine-tuning-4bae7d4da672?source=collection_archive---------1-----------------------#2023-08-30)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡ï¼š[https://towardsdatascience.com/a-beginners-guide-to-llm-fine-tuning-4bae7d4da672?source=collection_archive---------1-----------------------#2023-08-30](https://towardsdatascience.com/a-beginners-guide-to-llm-fine-tuning-4bae7d4da672?source=collection_archive---------1-----------------------#2023-08-30)
- en: How to fine-tune Llama and other LLMs with one tool
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å¦‚ä½•ä½¿ç”¨ä¸€ç§å·¥å…·å¾®è°ƒLlamaå’Œå…¶ä»–LLMs
- en: '[](https://medium.com/@mlabonne?source=post_page-----4bae7d4da672--------------------------------)[![Maxime
    Labonne](../Images/a7efdd305e3cc77d5509bbb1076d57d8.png)](https://medium.com/@mlabonne?source=post_page-----4bae7d4da672--------------------------------)[](https://towardsdatascience.com/?source=post_page-----4bae7d4da672--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----4bae7d4da672--------------------------------)
    [Maxime Labonne](https://medium.com/@mlabonne?source=post_page-----4bae7d4da672--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@mlabonne?source=post_page-----4bae7d4da672--------------------------------)[![Maxime
    Labonne](../Images/a7efdd305e3cc77d5509bbb1076d57d8.png)](https://medium.com/@mlabonne?source=post_page-----4bae7d4da672--------------------------------)[](https://towardsdatascience.com/?source=post_page-----4bae7d4da672--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----4bae7d4da672--------------------------------)
    [Maxime Labonne](https://medium.com/@mlabonne?source=post_page-----4bae7d4da672--------------------------------)'
- en: Â·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: Â·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fdc89da634938&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-beginners-guide-to-llm-fine-tuning-4bae7d4da672&user=Maxime+Labonne&userId=dc89da634938&source=post_page-dc89da634938----4bae7d4da672---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----4bae7d4da672--------------------------------)
    Â·8 min readÂ·Aug 30, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F4bae7d4da672&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-beginners-guide-to-llm-fine-tuning-4bae7d4da672&user=Maxime+Labonne&userId=dc89da634938&source=-----4bae7d4da672---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[å…³æ³¨](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fdc89da634938&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-beginners-guide-to-llm-fine-tuning-4bae7d4da672&user=Maxime+Labonne&userId=dc89da634938&source=post_page-dc89da634938----4bae7d4da672---------------------post_header-----------)
    å‘è¡¨åœ¨ [Towards Data Science](https://towardsdatascience.com/?source=post_page-----4bae7d4da672--------------------------------)
    Â·8åˆ†é’Ÿé˜…è¯»Â·2023å¹´8æœˆ30æ—¥[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F4bae7d4da672&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-beginners-guide-to-llm-fine-tuning-4bae7d4da672&user=Maxime+Labonne&userId=dc89da634938&source=-----4bae7d4da672---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4bae7d4da672&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-beginners-guide-to-llm-fine-tuning-4bae7d4da672&source=-----4bae7d4da672---------------------bookmark_footer-----------)![](../Images/3cd56f68c14e07ab9ae3eb624bd064ed.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[æ— ](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4bae7d4da672&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-beginners-guide-to-llm-fine-tuning-4bae7d4da672&source=-----4bae7d4da672---------------------bookmark_footer-----------)![](../Images/3cd56f68c14e07ab9ae3eb624bd064ed.png)'
- en: Image by author
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ç‰‡ç”±ä½œè€…æä¾›
- en: The growing interest in Large Language Models (LLMs) has led to a surge in **tools
    and wrappers designed to streamline their training process**.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å…´è¶£æ—¥ç›Šå¢é•¿ï¼Œå¯¼è‡´äº†**æ—¨åœ¨ç®€åŒ–å…¶è®­ç»ƒè¿‡ç¨‹çš„å·¥å…·å’Œå°è£…çš„æ¿€å¢**ã€‚
- en: Popular options include [FastChat](https://github.com/lm-sys/FastChat) from
    LMSYS (used to train [Vicuna](https://huggingface.co/lmsys/vicuna-13b-v1.5)) and
    Hugging Faceâ€™s [transformers](https://github.com/huggingface/transformers)/[trl](https://github.com/huggingface/trl)
    libraries (used in [my previous article](/fine-tune-your-own-llama-2-model-in-a-colab-notebook-df9823a04a32)).
    In addition, each big LLM project, like [WizardLM](https://github.com/nlpxucan/WizardLM/tree/main),
    tends to have its own training script, inspired by the original [Alpaca](https://github.com/tatsu-lab/stanford_alpaca)
    implementation.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: æµè¡Œçš„é€‰æ‹©åŒ…æ‹¬ LMSYS çš„[FastChat](https://github.com/lm-sys/FastChat)ï¼ˆç”¨äºè®­ç»ƒ[Vicuna](https://huggingface.co/lmsys/vicuna-13b-v1.5)ï¼‰å’Œ
    Hugging Face çš„[transformers](https://github.com/huggingface/transformers)/[trl](https://github.com/huggingface/trl)åº“ï¼ˆç”¨äº[æˆ‘ä¹‹å‰çš„æ–‡ç« ](/fine-tune-your-own-llama-2-model-in-a-colab-notebook-df9823a04a32)ï¼‰ã€‚æ­¤å¤–ï¼Œæ¯ä¸ªå¤§å‹
    LLM é¡¹ç›®ï¼Œå¦‚[WizardLM](https://github.com/nlpxucan/WizardLM/tree/main)ï¼Œå¾€å¾€éƒ½æœ‰è‡ªå·±çš„è®­ç»ƒè„šæœ¬ï¼Œçµæ„Ÿæ¥è‡ªåŸå§‹çš„[Alpaca](https://github.com/tatsu-lab/stanford_alpaca)å®ç°ã€‚
- en: In this article, we will use [**Axolotl**](https://github.com/OpenAccess-AI-Collective/axolotl),
    a tool created by the OpenAccess AI Collective. We will use it to fine-tune a
    [**Code Llama 7b**](https://github.com/OpenAccess-AI-Collective/axolotl/blob/main/examples/llama-2/qlora.yml)
    model on an evol-instruct dataset comprised of 1,000 samples of Python code.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ç”± OpenAccess AI Collective åˆ›å»ºçš„[**Axolotl**](https://github.com/OpenAccess-AI-Collective/axolotl)å·¥å…·ã€‚æˆ‘ä»¬å°†åˆ©ç”¨å®ƒåœ¨ä¸€ä¸ªåŒ…å«
    1,000 ä¸ª Python ä»£ç æ ·æœ¬çš„ evol-instruct æ•°æ®é›†ä¸Šå¾®è°ƒä¸€ä¸ª[**Code Llama 7b**](https://github.com/OpenAccess-AI-Collective/axolotl/blob/main/examples/llama-2/qlora.yml)æ¨¡å‹ã€‚
- en: ğŸ¤” Why Axolotl?
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ğŸ¤” ä¸ºä»€ä¹ˆé€‰æ‹© Axolotlï¼Ÿ
- en: 'The main appeal of Axolotl is that it provides a one-stop solution, which includes
    numerous features, model architectures, and an active community. Hereâ€™s a quick
    list of my favorite things about it:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: Axolotl çš„ä¸»è¦å¸å¼•åŠ›åœ¨äºå®ƒæä¾›äº†ä¸€ç«™å¼è§£å†³æ–¹æ¡ˆï¼ŒåŒ…æ‹¬ä¼—å¤šåŠŸèƒ½ã€æ¨¡å‹æ¶æ„å’Œä¸€ä¸ªæ´»è·ƒçš„ç¤¾åŒºã€‚ä»¥ä¸‹æ˜¯æˆ‘æœ€å–œæ¬¢çš„ä¸€äº›ç‰¹ç‚¹ï¼š
- en: '**Configuration**: All parameters used to train an LLM are neatly stored in
    a yaml config file. This makes it convenient for sharing and reproducing models.
    You can see an example for Llama 2 [here](https://github.com/OpenAccess-AI-Collective/axolotl/tree/main/examples/llama-2).'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**é…ç½®**ï¼šç”¨äºè®­ç»ƒ LLM çš„æ‰€æœ‰å‚æ•°éƒ½æ•´é½åœ°å­˜å‚¨åœ¨ä¸€ä¸ª yaml é…ç½®æ–‡ä»¶ä¸­ã€‚è¿™ä½¿å¾—å…±äº«å’Œé‡ç°æ¨¡å‹å˜å¾—éå¸¸æ–¹ä¾¿ã€‚ä½ å¯ä»¥æŸ¥çœ‹ Llama 2 çš„ç¤ºä¾‹[åœ¨è¿™é‡Œ](https://github.com/OpenAccess-AI-Collective/axolotl/tree/main/examples/llama-2)ã€‚'
- en: '**Dataset Flexibility**: Axolotl allows the specification of multiple datasets
    with varied prompt formats such asâ€¦'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**æ•°æ®é›†çµæ´»æ€§**ï¼šAxolotl å…è®¸æŒ‡å®šå¤šä¸ªæ•°æ®é›†ï¼Œå¹¶æ”¯æŒå„ç§æç¤ºæ ¼å¼ï¼Œä¾‹å¦‚â€¦â€¦'
