- en: Large Language Models as Zero-shot Labelers
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 大型语言模型作为零-shot 标注者
- en: 原文：[https://towardsdatascience.com/large-language-models-as-zero-shot-labelers-d26aa2642c88](https://towardsdatascience.com/large-language-models-as-zero-shot-labelers-d26aa2642c88)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/large-language-models-as-zero-shot-labelers-d26aa2642c88](https://towardsdatascience.com/large-language-models-as-zero-shot-labelers-d26aa2642c88)
- en: Using LLMs to obtain labels for supervised models
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用LLMs为监督模型获取标签
- en: '[](https://medium.com/@devins?source=post_page-----d26aa2642c88--------------------------------)[![Devin
    Soni](../Images/ad0e4be26a861f2584abc643daf52203.png)](https://medium.com/@devins?source=post_page-----d26aa2642c88--------------------------------)[](https://towardsdatascience.com/?source=post_page-----d26aa2642c88--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----d26aa2642c88--------------------------------)
    [Devin Soni](https://medium.com/@devins?source=post_page-----d26aa2642c88--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@devins?source=post_page-----d26aa2642c88--------------------------------)[![Devin
    Soni](../Images/ad0e4be26a861f2584abc643daf52203.png)](https://medium.com/@devins?source=post_page-----d26aa2642c88--------------------------------)[](https://towardsdatascience.com/?source=post_page-----d26aa2642c88--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----d26aa2642c88--------------------------------)
    [Devin Soni](https://medium.com/@devins?source=post_page-----d26aa2642c88--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----d26aa2642c88--------------------------------)
    ·5 min read·Mar 20, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----d26aa2642c88--------------------------------)
    ·5分钟阅读·2023年3月20日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/4517fd2ff5ae2bea5a595bff399475b2.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4517fd2ff5ae2bea5a595bff399475b2.png)'
- en: Photo by [h heyerlein](https://unsplash.com/es/@heyerlein?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由 [h heyerlein](https://unsplash.com/es/@heyerlein?utm_source=medium&utm_medium=referral)
    提供，来源于 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: Introduction
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: Labeling data is a critical step in building supervised machine learning models,
    as the quantity and quality of labels is often the main factor that determines
    model performance.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 数据标注是构建监督式机器学习模型中的一个关键步骤，因为标签的数量和质量通常是决定模型性能的主要因素。
- en: However, labeling data can be very time-consuming and expensive, especially
    for complex tasks that involve domain knowledge or reading large amounts of data.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，数据标注可能非常耗时且昂贵，特别是对于涉及领域知识或阅读大量数据的复杂任务。
- en: In recent years, large language models (LLMs) have emerged as a powerful solution
    for obtaining labels on text data. Through zero-shot learning, we can obtain labels
    on unlabeled data using only the output of the LLM, rather than having to ask
    a human to obtain the labels. This can significantly lower the cost of obtaining
    labels, and makes the process far more salable.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，大型语言模型（LLMs）作为获取文本数据标签的强大解决方案出现。通过零-shot 学习，我们可以仅使用LLM的输出来获取未标记数据的标签，而不必让人类来获取这些标签。这可以显著降低获取标签的成本，并使过程变得更加可扩展。
- en: In this article, we will further explore the concept of zero-shot learning and
    how LLMs can be used for this purpose.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我们将进一步探讨零-shot 学习的概念以及LLMs如何用于这一目的。
- en: What is Zero-Shot Learning?
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是零-shot 学习？
- en: '![](../Images/e6b0b7b78d31241482486c84fdd7e723.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e6b0b7b78d31241482486c84fdd7e723.png)'
- en: Photo by [Alex Knight](https://unsplash.com/@agk42?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由 [Alex Knight](https://unsplash.com/@agk42?utm_source=medium&utm_medium=referral)
    提供，来源于 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: Zero-shot learning (ZSL) is a a problem setup in machine learning in which the
    model is asked to solve a prediction task that it was not trained on. This often
    involves recognizing or classifying data into concepts it had not explicitly seen
    during training.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 零-shot 学习（ZSL）是机器学习中的一种问题设置，其中模型被要求解决一个它没有经过训练的预测任务。这通常涉及将数据识别或分类为训练时未明确见过的概念。
- en: In traditional supervised learning, this is not possible, as the model can only
    output predictions for tasks it was trained on (i.e. had labels for). However,
    in the ZSL paradigm, models can generalize to an arbitrary unseen task, and perform
    at a reasonable level. Note that in most cases, a supervised model trained on
    a given task will still outperform a model using ZSL, so ZSL is more often used
    before supervised labels are readily available.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在传统的监督学习中，这是不可能的，因为模型只能对其训练过的任务（即有标签的任务）进行预测。然而，在ZSL范式中，模型可以推广到任意未见的任务，并在合理的水平上表现。请注意，在大多数情况下，针对特定任务训练的监督模型仍将优于使用ZSL的模型，因此ZSL更常在监督标签尚不可用时使用。
- en: One of the most promising applications of ZSL is in data labeling, where it
    can significantly reduce the cost of obtaining labels. If a model is able to automatically
    classify data into categories without having been trained on that task, it can
    be used to generate labels for a downstream supervised model. These labels can
    be used to bootstrap a supervised model, in a paradigm similar to active learning
    or human-in-the-loop machine learning.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: ZSL最有前景的应用之一是在数据标记中，它可以显著降低获取标签的成本。如果一个模型能够自动将数据分类到类别中，而无需在该任务上进行训练，它可以用于为下游监督模型生成标签。这些标签可以用于引导一个监督模型，这种方式类似于主动学习或人机交互式机器学习。
- en: Zero-Shot Learning with Large Language Models
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 大语言模型的零样本学习
- en: '![](../Images/a125bdc1b8fe07f54ee9edc4871db98a.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a125bdc1b8fe07f54ee9edc4871db98a.png)'
- en: Photo by [Arseny Togulev](https://unsplash.com/@tetrakiss?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 由[Arseny Togulev](https://unsplash.com/@tetrakiss?utm_source=medium&utm_medium=referral)拍摄，图片来源于[Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: LLMs like GPT-3 are powerful tools for ZSL because their robust pre-training
    process allows them to have a holistic understanding of natural language that
    is not based on a certain supervised task’s labels.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 像 GPT-3 这样的LLM是ZSL的强大工具，因为它们的稳健预训练过程使它们对自然语言有全面的理解，这种理解并不依赖于某个特定监督任务的标签。
- en: Embedding search
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 嵌入搜索
- en: The contextual embeddings of a LLM are able to capture the semantic concepts
    in a given piece of text, which make them very useful for ZSL.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: LLM的上下文嵌入能够捕捉给定文本中的语义概念，这使得它们在ZSL中非常有用。
- en: Libraries such as [sentence-transformers](https://github.com/UKPLab/sentence-transformers)
    offer LLMs that have been trained in such a way that semantically similar pieces
    of text will have embeddings that have a small distance from each other.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 像[句子转换器](https://github.com/UKPLab/sentence-transformers)这样的库提供了经过训练的LLM，这些LLM使得语义相似的文本具有彼此距离很小的嵌入。
- en: '[](https://github.com/UKPLab/sentence-transformers?source=post_page-----d26aa2642c88--------------------------------)
    [## GitHub - UKPLab/sentence-transformers: Multilingual Sentence & Image Embeddings
    with BERT'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://github.com/UKPLab/sentence-transformers?source=post_page-----d26aa2642c88--------------------------------)
    [## GitHub - UKPLab/sentence-transformers: 多语言句子与图像嵌入与BERT'
- en: This framework provides an easy method to compute dense vector representations
    for sentences, paragraphs, and images…
  id: totrans-27
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 这个框架提供了一种计算句子、段落和图像的稠密向量表示的简单方法……
- en: github.com](https://github.com/UKPLab/sentence-transformers?source=post_page-----d26aa2642c88--------------------------------)
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: github.com](https://github.com/UKPLab/sentence-transformers?source=post_page-----d26aa2642c88--------------------------------)
- en: If we have the embeddings for a few labeled pieces of data, we can use a nearest-neighbor
    search to find pieces of unlabeled data with similar embeddings.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们有一些标记数据的嵌入，我们可以使用最近邻搜索来找到具有相似嵌入的未标记数据。
- en: If two pieces of text are very close to each other in the embedding space, then
    they likely have the same label.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 如果两个文本在嵌入空间中非常接近，那么它们很可能具有相同的标签。
- en: In-context learning
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 上下文学习
- en: '[In-context learning](http://ai.stanford.edu/blog/understanding-incontext/)
    is an emergent ability of LLMs that allows them to learn to solve new tasks simply
    by seeing input-output pairs. No parameter updates of the model are needed for
    it to be able to learn arbitrary new tasks.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '[上下文学习](http://ai.stanford.edu/blog/understanding-incontext/)是LLM的一种新兴能力，它使它们能够仅通过查看输入-输出对来学习解决新任务。模型无需更新参数即可学习任意新任务。'
- en: '[](http://ai.stanford.edu/blog/understanding-incontext/?source=post_page-----d26aa2642c88--------------------------------)
    [## How does in-context learning work? A framework for understanding the differences
    from traditional…'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '[](http://ai.stanford.edu/blog/understanding-incontext/?source=post_page-----d26aa2642c88--------------------------------)
    [## 上下文学习如何运作？理解与传统学习的差异框架…'
- en: In this post, we provide a Bayesian inference framework for in-context learning
    in large language models like GPT-3 and…
  id: totrans-34
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 在这篇文章中，我们提供了一个关于大语言模型如GPT-3的上下文学习的贝叶斯推理框架。
- en: ai.stanford.edu](http://ai.stanford.edu/blog/understanding-incontext/?source=post_page-----d26aa2642c88--------------------------------)
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: ai.stanford.edu](http://ai.stanford.edu/blog/understanding-incontext/?source=post_page-----d26aa2642c88--------------------------------)
- en: We can use this ability to obtain labels by simply providing a few input-output
    pairs for our downstream task, and allow the model to provide labels for unlabeled
    data points.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以利用这种能力，通过仅提供几个输入-输出对来获得标签，并让模型为未标记的数据点提供标签。
- en: In the context of ZSL, this means that we can provide a few handcrafted examples
    of text with their associated supervised labels, and have the model learn the
    labeling function on-the-fly.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在零样本学习的背景下，这意味着我们可以提供一些手工制作的文本示例及其相关的监督标签，让模型实时学习标注功能。
- en: '![](../Images/ad50a16d50b0f93dee71ca00397fbfb1.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ad50a16d50b0f93dee71ca00397fbfb1.png)'
- en: In this trivial case, we train ChatGPT to classify whether or not a sentence
    is about frogs via in-context learning.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个简单的案例中，我们训练ChatGPT通过上下文学习来分类一个句子是否与青蛙相关。
- en: Generative models
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 生成模型
- en: Recent advances in alignment methods such as [RLHF (Reinforcement Learning from
    Human Feedback)](https://huggingface.co/blog/rlhf) in generative LLMs have made
    it possible to simply ask the model to label data for you.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，生成LLMs中的对齐方法，如[RLHF（来自人类反馈的强化学习）](https://huggingface.co/blog/rlhf)的进展，使得只需要求模型为你标注数据成为可能。
- en: '[](https://huggingface.co/blog/rlhf?source=post_page-----d26aa2642c88--------------------------------)
    [## Illustrating Reinforcement Learning from Human Feedback (RLHF)'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://huggingface.co/blog/rlhf?source=post_page-----d26aa2642c88--------------------------------)
    [## 说明来自人类反馈的强化学习（RLHF）'
- en: This article has been translated to Chinese 简体中文. Interested in translating
    to another language? Contact nathan at…
  id: totrans-43
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 本文已翻译成中文简体。对翻译成其他语言感兴趣？请联系nathan…
- en: huggingface.co](https://huggingface.co/blog/rlhf?source=post_page-----d26aa2642c88--------------------------------)
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: huggingface.co](https://huggingface.co/blog/rlhf?source=post_page-----d26aa2642c88--------------------------------)
- en: Models such as ChatGPT are able to provide labels for input data by simply replying
    (in language) with the desired label. Their vast knowledge of the world obtained
    through pre-training on such large amounts of data have endowed these models with
    the ability to solve novel tasks using only their semantic understanding of the
    question being asked.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 如ChatGPT这样的模型能够通过简单地（用语言）回复所需标签来为输入数据提供标签。它们通过在如此大量的数据上进行预训练获得的丰富世界知识，使这些模型能够仅凭语义理解来解决新任务。
- en: This process can be automated using open-sourced models such as [FLAN-T5](https://huggingface.co/google/flan-t5-xxl)
    by asking the model to respond with only items in your label set (e.g. “Respond
    with ‘Yes’ or ‘No’”), and checking which option has the highest output probability
    after asking the model for labels.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 这个过程可以通过使用开源模型如[FLAN-T5](https://huggingface.co/google/flan-t5-xxl)来自动化，只需要求模型仅用你的标签集中的项回应（例如，“回应‘是’或‘否’”），然后在询问模型标签后检查哪个选项的输出概率最高。
- en: '![](../Images/1548eb1c4ec958355a2c58b1d51dd1d7.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1548eb1c4ec958355a2c58b1d51dd1d7.png)'
- en: ChatGPT is able to not only provide a label, but also explain its logic for
    obtaining said label.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: ChatGPT 不仅能提供标签，还能解释其获得该标签的逻辑。
- en: Conclusion
  id: totrans-49
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: Labeling data is a critical step in supervised machine learning, but it can
    be costly to obtain large amounts of labeled data.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 数据标注是监督机器学习中的关键步骤，但获取大量标注数据可能很昂贵。
- en: With zero-shot learning and LLMs, we can significantly reduce the cost of label
    acquisition.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 使用零样本学习和LLMs，我们可以显著降低标签获取的成本。
- en: LLMs pre-trained on huge amounts of data encode a semantic understanding of
    the world’s information that allow them to have high performance on arbitrary,
    unseen tasks. These models can automatically label data for us with high accuracy,
    allowing us to bootstrap supervised models at a low cost.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: LLMs 在大量数据上进行预训练，编码了对世界信息的语义理解，使它们在任意未见任务上表现出色。这些模型可以高准确度地自动标注数据，使我们可以以低成本启动监督模型。
