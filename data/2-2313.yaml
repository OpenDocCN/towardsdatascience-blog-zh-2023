- en: Watch Out For Your Beam Search Hyperparameters
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 注意你的束搜索超参数
- en: 原文：[https://towardsdatascience.com/watch-out-for-your-beam-search-hyperparameters-9c4daf6668d6](https://towardsdatascience.com/watch-out-for-your-beam-search-hyperparameters-9c4daf6668d6)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/watch-out-for-your-beam-search-hyperparameters-9c4daf6668d6](https://towardsdatascience.com/watch-out-for-your-beam-search-hyperparameters-9c4daf6668d6)
- en: The default values are never the best
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 默认值永远不是最佳的
- en: '[](https://medium.com/@bnjmn_marie?source=post_page-----9c4daf6668d6--------------------------------)[![Benjamin
    Marie](../Images/3ea1ad230cb1e67610418a8e36a5e5dd.png)](https://medium.com/@bnjmn_marie?source=post_page-----9c4daf6668d6--------------------------------)[](https://towardsdatascience.com/?source=post_page-----9c4daf6668d6--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----9c4daf6668d6--------------------------------)
    [Benjamin Marie](https://medium.com/@bnjmn_marie?source=post_page-----9c4daf6668d6--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@bnjmn_marie?source=post_page-----9c4daf6668d6--------------------------------)[![Benjamin
    Marie](../Images/3ea1ad230cb1e67610418a8e36a5e5dd.png)](https://medium.com/@bnjmn_marie?source=post_page-----9c4daf6668d6--------------------------------)[](https://towardsdatascience.com/?source=post_page-----9c4daf6668d6--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----9c4daf6668d6--------------------------------)
    [Benjamin Marie](https://medium.com/@bnjmn_marie?source=post_page-----9c4daf6668d6--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----9c4daf6668d6--------------------------------)
    ·6 min read·Jan 11, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page-----9c4daf6668d6--------------------------------)
    ·阅读时间6分钟·2023年1月11日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/0fab5a914d706a814394dc57392b6d05.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0fab5a914d706a814394dc57392b6d05.png)'
- en: Photo by [Paulius Dragunas](https://unsplash.com/@paulius005?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由[Paulius Dragunas](https://unsplash.com/@paulius005?utm_source=medium&utm_medium=referral)提供，来源于[Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: When developing applications using neural models, it is common to try different
    hyperparameters for training the models.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用神经模型开发应用时，尝试不同的超参数来训练模型是很常见的。
- en: For instance, the learning rate, the learning schedule, and the dropout rates
    are important hyperparameters that have a significant impact on the learning curve
    of your models.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，学习率、学习计划和丢弃率是对模型学习曲线产生重要影响的超参数。
- en: What is much less common is the search for the **best decoding hyperparameters**.
    If you read a deep learning tutorial or a scientific paper tackling natural language
    processing applications, there is a high chance that **the hyperparameters used
    for inference are not even mentioned**.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 更少见的是**最佳解码超参数**的搜索。如果你阅读深度学习教程或处理自然语言处理应用的科学论文，很可能**用于推理的超参数甚至未被提及**。
- en: Most authors, including myself, do not bother searching for the best decoding
    hyperparameters and use default ones.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数作者，包括我自己，不会费心寻找最佳解码超参数，而是使用默认值。
- en: Yet, these hyperparameters can actually have a significant impact on the results,
    and whatever is the decoding algorithm you are using there are always some hyperparameters
    that **should be fine-tuned to obtain better results**.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这些超参数实际上可能对结果产生显著影响，无论你使用什么解码算法，总会有一些**需要微调以获得更好结果的超参数**。
- en: In this blog article, I show the impact of decoding hyperparameters with simple
    Python examples, and a machine translation application. I focus on beam search,
    since this is by far the most popular decoding algorithm, and two particular hyperparameters.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇博客文章中，我展示了使用简单Python示例和机器翻译应用程序的解码超参数的影响。我专注于束搜索，因为这是迄今为止最受欢迎的解码算法，以及两个特定的超参数。
- en: Framework and Requirements
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 框架和要求
- en: To demonstrate the effect and importance of each hyperparameter, I will show
    some examples produced using the [Hugging Face Transformers package](https://huggingface.co/docs/transformers/index),
    in Python.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 为了展示每个超参数的效果和重要性，我将展示一些使用[Hugging Face Transformers包](https://huggingface.co/docs/transformers/index)在Python中生成的示例。
- en: 'To install this package, run in your terminal (I recommend to do it in a separate
    conda environment) the following command:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 要安装此包，请在终端中运行以下命令（我建议在单独的conda环境中执行）：
- en: '[PRE0]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: I will use GPT-2 (MIT licence) to generate simple sentences.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 我将使用GPT-2（MIT许可证）生成简单句子。
- en: I will also run other examples in machine translation using Marian (MIT licence).
    I installed it on Ubuntu 20.04, following the [official instructions](https://marian-nmt.github.io/docs/).
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 我还将使用 Marian (MIT 许可证) 运行其他机器翻译示例。我在 Ubuntu 20.04 上安装了它，按照[官方说明](https://marian-nmt.github.io/docs/)。
- en: Beam Size and Length Penalty
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Beam Size 和 Length Penalty
- en: Beam search is probably the most popular decoding algorithm for language generation
    tasks.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: Beam search 可能是语言生成任务中最受欢迎的解码算法。
- en: It keeps at each time step, i.e., for each new token generated, the *k* most
    probable hypotheses, according to the model used for inference, and the remaining
    ones are discarded.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 它在每一步，即为每个新生成的标记，保持模型推断中*最可能的*k个假设，其余的假设被丢弃。
- en: Finally, at the end of the decoding, the hypothesis with the highest probability
    will be the output.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，在解码结束时，概率最高的假设将作为输出。
- en: '*k,* usually called the “beam size”, is a very important hyperparameter.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '*k*，通常称为“beam size”，是一个非常重要的超参数。'
- en: '**With a higher *k* you get a more probable hypothesis**. Note that when *k*=1,
    we talk about “greedy search” since we only keep the most probable hypothesis
    at each time step.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '**更高的*k*会得到更可能的假设**。注意，当*k*=1时，我们称之为“贪心搜索”，因为我们只保留每一步中最可能的假设。'
- en: By default, in most applications, *k* is **arbitrarily** set between 1 and 10\.
    Values that may seem very low.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，在大多数应用中，*k* 是**任意**设置在 1 到 10 之间。这个值可能看起来非常低。
- en: 'There are two main reasons for this:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 这有两个主要原因：
- en: Increasing *k* **increases the decoding time and the memory** requirements.
    In other words, it gets more costly.
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 增加 *k* **会增加解码时间和内存**需求。换句话说，它变得更加昂贵。
- en: Higher *k* **may yield more probable but worse results**. This is mainly, but
    not only, due to the length of the hypotheses. **Longer hypotheses tend to have
    lower probability**, so beam search will tend to promote shorter hypotheses that
    may be more unlikely for some applications.
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更高的 *k* **可能会产生更可能但更差的结果**。这主要是由于假设的长度，但不仅仅是由于长度。**较长的假设往往概率较低**，因此 beam search
    会倾向于促进较短的假设，这对于某些应用可能不太可能。
- en: The first point can be straightforwardly fixed by performing better batch decoding
    and investing in better hardware.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 第一点可以通过进行更好的批量解码和投资更好的硬件来直接解决。
- en: 'The length bias can be controlled through another hyperparameter that normalizes
    the probability of an hypothesis by its length (number of tokens) at each time
    step. There are numerous ways to perform this normalization. One of the most used
    equation was proposed by [Wu et al. (2016)](https://arxiv.org/pdf/1609.08144.pdf):'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 长度偏差可以通过另一个超参数来控制，该超参数通过每一步的长度（标记数）来标准化假设的概率。执行这种标准化的方法有很多。最常用的方程之一是由[Wu et
    al. (2016)](https://arxiv.org/pdf/1609.08144.pdf) 提出的：
- en: '[PRE1]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Where |Y| is the length of the hypothesis and *α* an hyperparameter usually
    set between 0.5 and 1.0.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 |Y| 是假设的长度，*α* 是一个通常设置在 0.5 和 1.0 之间的超参数。
- en: Then, the score lp(Y) is used to modify the probability of the hypothesis to
    bias the decoding and produce longer or shorter hypotheses given *α*.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，分数 lp(Y) 用来修改假设的概率，以便在给定 *α* 的情况下，偏向于解码并生成较长或较短的假设。
- en: 'The implementation in Hugging Face transformers might be slightly different,
    but there is such an *α* that you can pass as “lengh_penalty” to the *generate*
    function, as in the following example (adapted from the [Transformers’ documentation](https://huggingface.co/docs/transformers/main_classes/text_generation)):'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: Hugging Face transformers 中的实现可能略有不同，但有一个*α*可以作为“length_penalty”传递给*generate*函数，如以下示例所示（改编自[Transformers’
    documentation](https://huggingface.co/docs/transformers/main_classes/text_generation)）：
- en: '[PRE2]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: “num_beams” in this code sample is our other hyperparameter *k.*
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 这个代码示例中的“num_beams”是我们的另一个超参数*k*。
- en: 'With this code sample, the prompt “Today I believe we can finally”, k=4, and
    α=0.5, we get:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这个代码示例，提示“Today I believe we can finally”，k=4 和 α=0.5 时，我们得到：
- en: '[PRE3]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '[PRE4]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'With k=50 and α=1.0, we get:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 当 k=50 和 α=1.0 时，我们得到：
- en: '[PRE5]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '[PRE6]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: You can see that the results are not quite the same.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到结果并不完全相同。
- en: '***k* and *α* should be fine-tuned independently on your target task**, using
    some development dataset.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '***k*和*α*应在你的目标任务上独立微调**，使用一些开发数据集。'
- en: Let’s take a concrete example in machine translation to see how to do a simple
    *grid search* to find the best hyperparameters and their impact in a real use
    case.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在机器翻译中举一个具体的例子，看看如何进行简单的*网格搜索*来找到最佳超参数及其在实际应用中的影响。
- en: Experiments with Machine Translation
  id: totrans-47
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器翻译实验
- en: For these experiments, I use Marian with a machine translation model trained
    on the [TILDE RAPID corpus](https://tilde-model.s3-eu-west-1.amazonaws.com/Tilde_MODEL_Corpus.html)
    (CC-BY 4.0) to do French-to-English translation.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些实验中，我使用了 Marian 和训练在 [TILDE RAPID 语料库](https://tilde-model.s3-eu-west-1.amazonaws.com/Tilde_MODEL_Corpus.html)（CC-BY
    4.0）上的机器翻译模型进行法语到英语的翻译。
- en: 'I used only the first 100k lines of the dataset for training and the last 6k
    lines as devtest. I split the devtest into two parts of 3k lines each: the first
    part is used for validation and the second part is used for evaluation. Note:
    *the RAPID corpus has its sentences ordered alphabetically. My train/devtest split
    is thus not ideal for a realistic use case. I recommend shuffling the lines of
    the corpus, preserving the sentence pairs, before splitting the corpus. In this
    article, I kept the alphabetical order, and didn’t shuffle, to make the following
    experiments more reproducible.*'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 我仅使用了数据集的前 100k 行进行训练，最后的 6k 行作为开发测试集。我将开发测试集分成两部分，每部分 3k 行：第一部分用于验证，第二部分用于评估。注意：*RAPID
    语料库的句子按字母顺序排列。因此，我的训练/开发测试集划分在实际使用案例中并不理想。我建议在划分语料库之前，先打乱语料库的行顺序，同时保留句子对。在本文中，我保持了字母顺序，并没有打乱，以使以下实验更具可重复性。*
- en: I evaluate the translation quality with the metric [COMET](https://unbabel.github.io/COMET/html/index.html)
    (Apache License 2.0).
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 我使用 [COMET](https://unbabel.github.io/COMET/html/index.html)（Apache 许可证 2.0）来评估翻译质量。
- en: To search for the best pair of values for *k* and *α* with grid search, we have
    to first define a set of values for each hyperparameter and then try all the possible
    combinations.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 要通过网格搜索来寻找 *k* 和 *α* 的最佳值对，我们首先必须定义每个超参数的一组值，然后尝试所有可能的组合。
- en: Since here we are searching for decoding hyperparameters, this search is quite
    fast and straightforward in constrat to searching for training hyperparameters.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们在这里搜索的是解码超参数，因此这一搜索相对快速且直接，与搜索训练超参数相比。
- en: 'The sets of values I chose for this task are as follows:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 我为此任务选择的值集如下：
- en: '*k*: {1,2,**4**,10,20,50,100}'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*k*: {1,2,**4**,10,20,50,100}'
- en: '*α*: {0.5,**0.6**,0.7,0.8,1.0,1.1,1.2}'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*α*: {0.5,**0.6**,0.7,0.8,1.0,1.1,1.2}'
- en: I put in bold the most common values used in machine translation by default.
    For most natural language generation tasks, these sets of values should be tried,
    except maybe k=100 which is often unlikely to yield the best results while it
    is a costly decoding.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 我将机器翻译中默认使用的最常见值加粗。对于大多数自然语言生成任务，应该尝试这些值集，除了 k=100，通常这个值不容易产生最佳结果，同时解码代价较高。
- en: We have 7 values for *k* and 7 values for *α*. We want to try all the combinations
    so we have 7*7=49 decodings of the evaluation dataset to do.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有 7 个 *k* 的值和 7 个 *α* 的值。我们想尝试所有组合，所以我们需要对评估数据集进行 7*7=49 次解码。
- en: 'We can do that with a simple bash script:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过一个简单的 bash 脚本来实现：
- en: '[PRE7]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Then for each decoding output we run COMET to evaluate the translation quality.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，对每个解码输出运行 COMET 以评估翻译质量。
- en: 'With all the results we can draw the following table of COMET scores for each
    pair of values:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 根据所有结果，我们可以绘制出每对值的 COMET 分数表：
- en: '![](../Images/5f1a5d8baf0cc152990ea15e2cafa6e6.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5f1a5d8baf0cc152990ea15e2cafa6e6.png)'
- en: Table by the author
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的表格
- en: As you can see, the result obtained with the default hyperparameter (underline)
    is lower than 26 of the other results obtained with other hyparameter values.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，使用默认超参数（下划线）获得的结果低于 26 个其他超参数值获得的结果。
- en: 'Actually, all the results in bold are statistically **significantly better**
    than the default one. Note: *In this experiments I am using the test set to compute
    the results I showed in the table. In a realistic scenario, these results should
    be computed on another development/validation set to decide on the pair of values
    that will be used on the test set, or for a real-world applications.*'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，所有加粗的结果在统计上都**显著优于**默认结果。注意：*在这些实验中，我使用了测试集来计算表中展示的结果。在实际情况中，这些结果应该在另一个开发/验证集上计算，以决定在测试集上使用的值对，或用于真实世界应用。*
- en: Hence, for your applications, it is definitely worth fine-tuning the decoding
    hyperparameters to obtain better results at the cost of a very small engineering
    effort.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，对于你的应用程序，确实值得微调解码超参数，以在付出非常小的工程努力的情况下获得更好的结果。
- en: Conclusion
  id: totrans-67
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: In this article, we only played with two hyperparameters of beam search. Many
    more should be fine-tuned.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我们只调整了 beam search 的两个超参数。还应微调更多参数。
- en: Other decoding algorithms such as temperature and nucleus sampling have hyperparameters
    that you may want to look at instead of using default ones.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 其他解码算法，如温度采样和核采样，具有一些超参数，你可能想要查看这些参数，而不是使用默认值。
- en: Obviously, as we increase the number of hyperparameters to fine-tune, the grid
    search becomes more costly. Only your **experience and experiments with your application**
    will tell you whether it is worth fine-tuning a particular hyperparameter, and
    which values are more likely to yield satisfying results.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，随着我们增加超参数的数量进行调整，网格搜索的成本会更高。只有你**在应用中的经验和实验**才能告诉你是否值得对特定的超参数进行调整，以及哪些值更有可能产生令人满意的结果。
