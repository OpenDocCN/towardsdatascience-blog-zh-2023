- en: An Intro to Hugging Face With Implementation of 6 NLP Tasks
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Hugging Face 简介及 6 种 NLP 任务实现
- en: 原文：[https://towardsdatascience.com/implement-nlp-tasks-using-hugging-face-77dfdcad65fd](https://towardsdatascience.com/implement-nlp-tasks-using-hugging-face-77dfdcad65fd)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/implement-nlp-tasks-using-hugging-face-77dfdcad65fd](https://towardsdatascience.com/implement-nlp-tasks-using-hugging-face-77dfdcad65fd)
- en: An introductory tutorial to use Hugging Face for NLP tasks
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Hugging Face 用于 NLP 任务的入门教程
- en: '[](https://medium.com/@fmnobar?source=post_page-----77dfdcad65fd--------------------------------)[![Farzad
    Mahmoodinobar](../Images/2d75209693b712300e6f0796bd2487d0.png)](https://medium.com/@fmnobar?source=post_page-----77dfdcad65fd--------------------------------)[](https://towardsdatascience.com/?source=post_page-----77dfdcad65fd--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----77dfdcad65fd--------------------------------)
    [Farzad Mahmoodinobar](https://medium.com/@fmnobar?source=post_page-----77dfdcad65fd--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@fmnobar?source=post_page-----77dfdcad65fd--------------------------------)[![Farzad
    Mahmoodinobar](../Images/2d75209693b712300e6f0796bd2487d0.png)](https://medium.com/@fmnobar?source=post_page-----77dfdcad65fd--------------------------------)[](https://towardsdatascience.com/?source=post_page-----77dfdcad65fd--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----77dfdcad65fd--------------------------------)
    [Farzad Mahmoodinobar](https://medium.com/@fmnobar?source=post_page-----77dfdcad65fd--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----77dfdcad65fd--------------------------------)
    ·12 min read·Apr 18, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----77dfdcad65fd--------------------------------)
    ·阅读时间 12 分钟·2023年4月18日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/8a227cd780125742369e775c5e6ccf84.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8a227cd780125742369e775c5e6ccf84.png)'
- en: Photo by [Duy Pham](https://unsplash.com/@miinyuii?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
    on [Unsplash](https://unsplash.com/photos/Cecb0_8Hx-o?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 照片由 [Duy Pham](https://unsplash.com/@miinyuii?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
    提供，来源于 [Unsplash](https://unsplash.com/photos/Cecb0_8Hx-o?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
- en: '[Hugging Face](https://huggingface.co/) is an open-source AI community for
    and by machine learning practitioners with a focus on Natural Language Processing
    (NLP), computer vision and audio/speech processing tasks. Whether you already
    work in one of these areas or aspire to enter this realm in the future, you will
    benefit from learning how to use Hugging Face tools and models.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '[Hugging Face](https://huggingface.co/) 是一个开源 AI 社区，由机器学习从业者创建，专注于自然语言处理（NLP）、计算机视觉以及音频/语音处理任务。无论你是已经在这些领域工作还是希望未来进入这些领域，你都会从学习如何使用
    Hugging Face 的工具和模型中受益。'
- en: 'In this post we are going to go over six of the most frequently used NLP tasks
    by leveraging pre-trained models available on Hugging Face, as follows:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我们将介绍利用 Hugging Face 上的预训练模型完成的六种最常用的 NLP 任务，如下所示：
- en: Text Generation (a.k.a. Language Modeling)
  id: totrans-10
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 文本生成（即语言建模）
- en: Question Answering
  id: totrans-11
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 问答系统
- en: Sentiment Analysis
  id: totrans-12
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 情感分析
- en: Text Classification
  id: totrans-13
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 文本分类
- en: Text Summarization
  id: totrans-14
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 文本摘要
- en: Machine Translation
  id: totrans-15
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 机器翻译
- en: Before jumping into the tasks, let’s take a minute to talk about the distinction
    between “Training” and “Inference”, which are two important concepts in machine
    learning, in order to clarify what we will be working on today.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始任务之前，让我们花一分钟时间讨论“训练”和“推断”这两个机器学习中的重要概念，以澄清我们今天将要做的工作。
- en: Let’s get started!
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始吧！
- en: '[](https://medium.com/@fmnobar/membership?source=post_page-----77dfdcad65fd--------------------------------)
    [## Join Medium with my referral link'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@fmnobar/membership?source=post_page-----77dfdcad65fd--------------------------------)
    [## 使用我的推荐链接加入 Medium'
- en: Read every story from Farzad (and other writers on Medium). Your membership
    fee directly supports Farzad and other…
  id: totrans-19
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 阅读 Farzad 的每一个故事（以及 Medium 上其他作者的故事）。你的会员费直接支持 Farzad 和其他…
- en: medium.com](https://medium.com/@fmnobar/membership?source=post_page-----77dfdcad65fd--------------------------------)
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: medium.com](https://medium.com/@fmnobar/membership?source=post_page-----77dfdcad65fd--------------------------------)
- en: Training vs. Inference in Machine Learning
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习中的训练与推断
- en: Training is the process of feeding a machine learning model with large amounts
    of data. During this process the model “learns” from the provided data (by optimizing
    an objective function) and hence this process is called “Training”. Once we have
    a trained model, we can use it to make predictions in new data that model has
    not seen before. This process is called “Inference”. In short, training is the
    learning process for the model, while inference is the model making predictions
    (i.e. when we actually use the model).
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 训练是将大量数据输入机器学习模型的过程。在这个过程中，模型通过优化目标函数从提供的数据中“学习”，因此这个过程被称为“训练”。一旦我们有了训练好的模型，我们可以使用它来对模型之前未见过的新数据进行预测。这个过程被称为“推理”。简而言之，训练是模型的学习过程，而推理是模型进行预测的过程（即我们实际使用模型时）。
- en: Now that we understand the distinction between training and inference, we can
    more concretely define what we will be working on today. In this post, we will
    be using various pre-trained models for inference. In other words, we would not
    be going through the expensive process of training any new models here. On the
    other hand, we are going to leverage the myriad of existing pre-trained models
    in the Hugging Face Hub and use those for inference (i.e. to make predictions).
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们理解了训练和推理之间的区别后，我们可以更具体地定义我们今天要做的工作。在这篇文章中，我们将使用各种预训练模型进行推理。换句话说，我们不会在这里经历训练任何新模型的昂贵过程。另一方面，我们将利用
    Hugging Face Hub 中众多现有的预训练模型，并使用这些模型进行推理（即进行预测）。
- en: 1\. Text Generation (a.k.a. Language Modeling)
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 1\. 文本生成（又称语言建模）
- en: I decided to start with this task, given the recent hiked interest about Generative
    AI such as ChatGPT. This task is usually called language modeling and the task
    that the models perform is to predict missing parts of text (this can be a word,
    token or larger strings of text). What has attracted a lot of interest recently
    is that the models can generate text without necessarily having seen such prompts
    before.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 我决定从这个任务开始，考虑到最近对生成性人工智能（如 ChatGPT）的兴趣激增。这个任务通常被称为语言建模，模型执行的任务是预测文本中缺失的部分（这可以是一个词、token
    或更大的文本字符串）。最近引起了很多兴趣的是，模型可以生成文本，而不一定需要之前见过这样的提示。
- en: Let’s see how it works in practice!
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看它在实践中的效果！
- en: 1.1\. Text Generation — Implementation
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.1\. 文本生成 — 实现
- en: 'In order to implement text generation, we will import `pipeline` from `transformers`
    library, use one of the GPT models and take the steps below. I have also added
    comments in the code so that you can more easily follow the steps:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现文本生成，我们将从 `transformers` 库中导入 `pipeline`，使用其中一个 GPT 模型，并按照以下步骤进行。我还在代码中添加了注释，以便您更容易地跟随这些步骤：
- en: Import libraries
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入库
- en: Specify the name of the pre-trained model to be used for this specific task
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 指定要用于此特定任务的预训练模型的名称
- en: Specify the sentence, which will be completed by the model
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 指定将由模型完成的句子
- en: Create an instance of `pipeline` as `generator`
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个 `pipeline` 的实例作为 `generator`
- en: Perform the text generation and store the results as `output`
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行文本生成并将结果存储为`output`
- en: Return the results
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 返回结果
- en: Code block below follows these steps.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的代码块按照这些步骤进行。
- en: '[PRE0]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Results:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 结果：
- en: '![](../Images/5de8984992318dd87aacdbbaf11cd913.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5de8984992318dd87aacdbbaf11cd913.png)'
- en: Text Generation Results
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 文本生成结果
- en: We can see in the results that the model took our provided input text and generated
    additional text, given the data it has been trained on and the sentence that we
    provided. Note that I limited the length of the output using the `max_new_tokens`
    to 30 tokens to prevent a lengthy response. The generated text sounds reasonable
    and relevant to context.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以从结果中看到，模型根据我们提供的输入文本生成了额外的文本，考虑到它所训练的数据和我们提供的句子。请注意，我将输出的长度限制为 `max_new_tokens`
    的 30 个 token，以防生成过长的响应。生成的文本听起来合理且与上下文相关。
- en: But what about a case where we would like to ask a question from the model?
    Can the model answer a question, instead of just completing an incomplete sentence?
    Let’s explora that next.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 但如果我们想向模型提出一个问题呢？模型能否回答问题，而不仅仅是完成一个不完整的句子？让我们接下来探索一下。
- en: 2\. Question Answering
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 2\. 问答
- en: 'Question answering, as the name suggests, is a task where the model answers
    a question provided by the user. There are generally two types of question answering
    tasks:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 问答，如其名所示，是一个模型回答用户提出的问题的任务。通常有两种类型的问答任务：
- en: '**Extractive (i.e. context-dependent):** Where the user describes a situation
    to the model in the question/prompt and ask the model to generate a response,
    given that provided information. In this scenario, the model picks the relevant
    parts of the information from the prompt and returns the results'
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**抽象式（即上下文相关）：** 用户在问题/提示中向模型描述一个情况，并要求模型根据提供的信息生成回应。在这种情况下，模型从提示中挑选相关的信息部分，并返回结果。'
- en: '**Abstractive (i.e. context-independent):** Where the user asks a question
    from the model, without providing any context'
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**抽取式（即上下文无关）：** 用户向模型提问，而不提供任何上下文。'
- en: Let’s look at how question answering can be implemented.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何实现问答系统。
- en: 2.1\. Question Answering — Implementation
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.1\. 问答系统 — 实现
- en: Implementation process is similar to the language modeling task. We will use
    two different models to be able to compare the results.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 实现过程与语言建模任务类似。我们将使用两个不同的模型来比较结果。
- en: Let’s start with the `distilbert-base-cased-distilled-squad` ([link](https://huggingface.co/distilbert-base-cased-distilled-squad)).
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从`distilbert-base-cased-distilled-squad`开始 ([link](https://huggingface.co/distilbert-base-cased-distilled-squad))。
- en: '[PRE1]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Results:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 结果：
- en: '![](../Images/df755d3f89fe5dac99a188cf302b4bcb.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/df755d3f89fe5dac99a188cf302b4bcb.png)'
- en: Question Answering Results (distilbert-base-cased-distilled-squad `Model)`
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 问答结果 (distilbert-base-cased-distilled-squad `Model`)
- en: We can see the summary and the model was able to determine which part of the
    context was relevant to answer the question.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到总结，模型能够确定哪个部分的上下文与回答问题相关。
- en: let’s implement the same problem using a different model, named `deepset/roberta-base-squad2`
    ([link](https://huggingface.co/deepset/roberta-base-squad2)).
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用另一个模型`deepset/roberta-base-squad2` ([link](https://huggingface.co/deepset/roberta-base-squad2))来实现相同的问题。
- en: '[PRE2]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Results:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 结果：
- en: '![](../Images/0e158182b62c0f846aae0b86dd9e56a8.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0e158182b62c0f846aae0b86dd9e56a8.png)'
- en: Question Answering Results (`deepset/roberta-base-squad2 Model`)
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 问答结果 (`deepset/roberta-base-squad2 Model`)
- en: As we see in the above example, the second model was also able to identify that
    NLP stands for natural language processing, given the context that we provided.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在上面的例子中看到的，第二个模型也能够识别出NLP代表自然语言处理，鉴于我们提供的上下文。
- en: Let’s continue our journey in the NLP tasks by looking at sentiment analysis
    next.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们继续我们的NLP任务之旅，接下来看看情感分析。
- en: 3\. Sentiment Analysis
  id: totrans-62
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 3\. 情感分析
- en: 'Sentiment analysis is the process of categorizing the sentiment of a text into
    positive, negative or neutral. There is a wide range of applications for sentiment
    analysis in different industries, such as monitoring customers’ sentiment from
    product reviews or even in politics, such as gauging public interest in a given
    topic during an election year. Focus of this post is to use Hugging Face for various
    tasks so we will not dive deeper into each topic but if you are interested in
    learning more about sentiment analysis in depth, you can refer to this post:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 情感分析是将文本的情感分类为积极、消极或中性的过程。情感分析在不同的行业中有广泛的应用，比如通过产品评论监控客户情绪，甚至在政治中，比如在选举年评估公众对某一主题的兴趣。本文的重点是使用Hugging
    Face完成各种任务，因此我们不会深入探讨每个主题，但如果您有兴趣深入了解情感分析，可以参考这篇文章：
- en: '[](/sentiment-analysis-intro-and-implementation-ddf648f79327?source=post_page-----77dfdcad65fd--------------------------------)
    [## Sentiment Analysis — Intro and Implementation'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/sentiment-analysis-intro-and-implementation-ddf648f79327?source=post_page-----77dfdcad65fd--------------------------------)
    [## 情感分析 — 介绍与实现'
- en: Sentiment Analysis using NLTK, scikit-learn and TextBlob
  id: totrans-65
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用NLTK、scikit-learn和TextBlob进行情感分析
- en: towardsdatascience.com](/sentiment-analysis-intro-and-implementation-ddf648f79327?source=post_page-----77dfdcad65fd--------------------------------)
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/sentiment-analysis-intro-and-implementation-ddf648f79327?source=post_page-----77dfdcad65fd--------------------------------)
- en: 3.1\. Sentiment Analysis — Implementation
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.1\. 情感分析 — 实现
- en: In order to implement sentiment analysis, we will again rely on`pipeline` from
    `transformers` library and take the steps below. I have also added comments in
    the code so that you can more easily follow the steps.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现情感分析，我们将再次依赖`transformers`库中的`pipeline`，并按照以下步骤操作。我还在代码中添加了注释，以便您更容易跟随步骤。
- en: Import libraries
  id: totrans-69
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入库
- en: Specify the name of the pre-trained model to be used for this specific task
    (i.e. sentiment analysis)
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 指定用于特定任务（例如情感分析）的预训练模型名称。
- en: Specify the task (i.e. sentiment analysis)
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 指定任务（即情感分析）
- en: Specify the sentence, which will be sentiment analyzed
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 指定将进行情感分析的句子
- en: Create an instance of `pipeline` as `analyzer`
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个`pipeline`实例作为`analyzer`
- en: Perform the sentiment analysis and save the results as `output`
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行情感分析并将结果保存为`output`
- en: Return the results
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 返回结果
- en: '[PRE3]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Results:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 结果：
- en: '![](../Images/0e5bf1b80072f1bb437635989b5c8451.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0e5bf1b80072f1bb437635989b5c8451.png)'
- en: Sentiment Analysis Results
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 情感分析结果
- en: The results indicate that the sentiment of the sentence is a positive one with
    a score of ~85%. The sentence sounds pretty positive to me so I like the results
    so far. Feel free to replicate the process for other sentences and test it out!
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 结果表明，句子的情感是积极的，得分约为85%。这句句子听起来相当积极，所以到目前为止我喜欢这些结果。可以自由地重复这一过程，测试其他句子！
- en: Let’s move on to a different type of text classification.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们转向另一种文本分类类型。
- en: 4\. Text Classification
  id: totrans-82
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 4\. 文本分类
- en: Sentiment analysis, which we just covered, can be considered a special case
    of text classification, where the categories (or classes) are only positive, negative
    or neutral. Text classification is more generic in that it can classify (or categorize)
    the incoming text (e.g. sentence, paragraph or document) into pre-defined classes.
    Let’s see what this means in practice.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 情感分析，正如我们刚刚讨论的，可以被视为文本分类的一个特殊案例，其中类别（或类）仅为正面、负面或中性。文本分类则更为通用，它可以将传入的文本（例如句子、段落或文档）分类到预定义的类别中。让我们在实践中深入了解一下这意味着什么。
- en: 4.1\. Text Classification — Implementation
  id: totrans-84
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.1\. 文本分类 — 实现
- en: 'We will use the same `pipeline` package and take steps very similar to what
    we did for sentiment analysis, as follows:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用相同的`pipeline`包，并采取与情感分析非常相似的步骤，如下所示：
- en: '[PRE4]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Results:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 结果：
- en: '![](../Images/bd38a8780d752b47257237e4dea5e5c7.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bd38a8780d752b47257237e4dea5e5c7.png)'
- en: Text Classification Results
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 文本分类结果
- en: Results are quite interesting! The scores correspond to each label, sorted from
    the largest to the smallest for ease of reading. For example, the results indicate
    that our sentence is labeled as “education” with a score of ~40%, followed by
    “business” by ~22%, while labels for “music”, “sports” and “politics” have very
    low scores, which makes sense to me overall.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 结果相当有趣！分数对应于每个标签，按从大到小排序以便阅读。例如，结果表明我们的句子被标记为“教育”，得分约为40%，其次是“商业”，得分约为22%，而“音乐”、“体育”和“政治”标签的分数都很低，这总体上对我来说是合理的。
- en: Let’s move on to our next task, which is summarization.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们继续进行下一个任务，即摘要。
- en: 5\. Text Summarization
  id: totrans-92
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 5\. 文本摘要
- en: Text summarization is the task of automatically summarizing textual input, while
    still conveying the main points/gist of the incoming text. One example of the
    business intuition behind the need for such summarization models is the situations
    where humans read incoming text communications (e.g. customer emails) and using
    a summarization model can save human time. For example, these human representatives
    can read the summary of the customer emails instead of the entire emails, resulting
    in improved operational efficiency by saving human time and cost.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 文本摘要的任务是自动总结文本输入，同时仍然传达传入文本的主要点或要旨。需要这种摘要模型的商业直觉示例是那些人类阅读传入的文本通信（例如客户邮件）的情况，使用摘要模型可以节省人力时间。例如，这些人工代表可以阅读客户邮件的摘要，而不是全部邮件，从而通过节省人力时间和成本提高操作效率。
- en: Let’s look at how we can implement text summarization.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何实现文本摘要。
- en: 5.1\. Text Summarization — Implementation
  id: totrans-95
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.1\. 文本摘要 — 实现
- en: Similar to other tasks, we will use the `pipeline` for a summarization task.
    For this specific task, we will first use a text-to-text pre-rained model from
    Google named [T5](https://github.com/google-research/text-to-text-transfer-transformer)
    to summarize the description that we just read about “Text Summarization” in the
    section above. We will then repeat the same exercise using a different model from
    Google to see how the results vary. Let’s see how we can implement this.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 与其他任务类似，我们将使用`pipeline`进行摘要任务。对于这个具体任务，我们将首先使用来自Google的名为[T5](https://github.com/google-research/text-to-text-transfer-transformer)的文本到文本预训练模型，来总结我们刚刚阅读的关于“文本摘要”的描述。然后，我们将使用Google的不同模型重复相同的练习，看看结果的变化。让我们看看如何实现这一点。
- en: '[PRE5]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Results:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 结果：
- en: '![](../Images/732cdbd9d24b13790eec08fb126dbb1c.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/732cdbd9d24b13790eec08fb126dbb1c.png)'
- en: Text Summarization Results (T5 Model)
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 文本摘要结果（T5模型）
- en: As you see in the results, the T5 model took the input text, which was rather
    long, and returned a brief summary of what it considered the main points of the
    input text. I like the summary since it explains what text summarization is and
    what benefits it can provide — that’s a good summary!
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你在结果中看到的，T5模型接受了相对较长的输入文本，并返回了一个简洁的摘要，概述了它认为输入文本的主要点。我喜欢这个摘要，因为它解释了什么是文本摘要以及它能提供什么好处——这是一个很好的摘要！
- en: Let’s try another model from Google named [Pegasus](https://huggingface.co/google/pegasus-cnn_dailymail)
    to see if and how the results change, when we use a different model.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试另一个来自Google的模型，[Pegasus](https://huggingface.co/google/pegasus-cnn_dailymail)，看看使用不同模型时结果如何变化。
- en: '[PRE6]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Results:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 结果：
- en: '![](../Images/8aebfbfd0c88e203b79cc25583fce843.png)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8aebfbfd0c88e203b79cc25583fce843.png)'
- en: Text Summarization Results (Pegasus Model)
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 文本摘要结果（Pegasus模型）
- en: As expected, resulting outputs of the two models defer, since they are each
    trained using specific data and training objectives but both somehow accomplished
    the task. I personally prefer the outcome of the T5 model, since it more succinctly
    states the point of the input text.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 正如预期的那样，两种模型的结果有所不同，因为它们各自使用了特定的数据和训练目标，但都在某种程度上完成了任务。我个人更喜欢T5模型的结果，因为它更简洁地陈述了输入文本的要点。
- en: Last, but not the least task that we will be looking at is machine translation.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将要讨论的任务是机器翻译。
- en: 6\. Machine Translation
  id: totrans-109
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 6\. 机器翻译
- en: Machine translation is the task of generating the translation of an input text
    in a target language. This is similar to what Google Translate or other similar
    translation engines provide. One of the benefits of using Hugging Face for machine
    translation is that we get to choose what models to use for our translation, which
    can potentially provide a more accurate translation for the specific language
    that we are looking for.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 机器翻译是生成输入文本在目标语言中翻译的任务。这类似于Google Translate或其他类似翻译引擎提供的服务。使用Hugging Face进行机器翻译的好处之一是我们可以选择用于翻译的模型，这可能为我们寻找的特定语言提供更准确的翻译。
- en: Let’s look at the implementation of machine translation in Hugging Face.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看看Hugging Face中机器翻译的实现。
- en: 6.1\. Machine Translation — Implementation
  id: totrans-112
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.1\. 机器翻译 — 实现
- en: In order to generate translations, we will use two of the most common pre-trained
    models to translate the same sentence from English to French. Implementation of
    each slightly varies but the overall process is the same as other tasks that we
    have implemented so far.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 为了生成翻译，我们将使用两个最常见的预训练模型将同一句话从英语翻译成法语。每个模型的实现略有不同，但整体过程与我们到目前为止实现的其他任务相同。
- en: 6.1.1\. T5
  id: totrans-114
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.1.1\. T5
- en: '[T5](https://huggingface.co/docs/transformers/model_doc/t5) is an encoder-decoder
    pre-trained model developed by Google, which works well on multiple tasks, including
    machine translation. In order to prompt T5 to perform a tasks such as translation
    from language X to language Y, we will add a string (called a “prefix”) to the
    sentence to the input of each task follows: `"translate X to Y: sentence_to_be_translated"`.'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '[T5](https://huggingface.co/docs/transformers/model_doc/t5)是Google开发的一个编码器-解码器预训练模型，在包括机器翻译在内的多个任务上表现良好。为了提示T5执行诸如从语言X到语言Y的翻译等任务，我们会在每个任务的输入句子前添加一个字符串（称为“前缀”）：`"translate
    X to Y: sentence_to_be_translated"`。'
- en: This is actually easier in practice to understand so let’s just translate a
    sentence from English to French using T5 and see how it works.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，这在实践中更容易理解，所以我们直接使用T5将一个句子从英语翻译成法语，看看效果如何。
- en: '[PRE7]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Results:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 结果：
- en: '![](../Images/ff09beef413b5d75dcf118b24cfc6259.png)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ff09beef413b5d75dcf118b24cfc6259.png)'
- en: Machine Translation Results (T5 Model)
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 机器翻译结果（T5模型）
- en: I looked up this translation on Google Translate and this looks like a good
    translation! I was concerned about this verification methodology, since T5 is
    also developed by Google. I do not know what exact model Google Translate uses
    for translation but we will see how much the results vary when we run the same
    translation task using mBART in the next section.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 我在Google Translate上查找了这个翻译，看起来这是一个不错的翻译！我对这种验证方法有些担忧，因为T5也是由Google开发的。我不知道Google
    Translate使用的是哪个具体模型，但我们将在下一节中使用mBART运行相同的翻译任务，看看结果有多大差异。
- en: 6.1.2\. mBART
  id: totrans-122
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.1.2\. mBART
- en: '[mBART](https://huggingface.co/docs/transformers/model_doc/mbart) is a multilingual
    encoder-decoder pre-trained model developed by Meta, which is primarily intended
    for machine translation tasks. mBART, unlike T5, does not require the prefix in
    the prompt but we need to identify the original and target languges to the model.'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '[mBART](https://huggingface.co/docs/transformers/model_doc/mbart)是由 Meta 开发的多语言编码器-解码器预训练模型，主要用于机器翻译任务。mBART
    与 T5 不同，它不需要提示中的前缀，但我们需要向模型提供原始语言和目标语言的识别。'
- en: Let’s implement the same task in mBART.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在 mBART 中实现相同的任务。
- en: '[PRE8]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '![](../Images/01ceddbf75c2b303d82c706352e78920.png)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/01ceddbf75c2b303d82c706352e78920.png)'
- en: Machine Translation Results (mBART Model)
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 机器翻译结果（mBART 模型）
- en: Results seem very similar to what T5 generated, with the exception of “poste”
    having been replaced by “post”. Regardless of the difference between the two outcomes,
    the main point of the exercise was to demonstrate how these pre-trained models
    can generate machine translation, which we have accomplished using both models.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 结果与 T5 生成的结果非常相似，唯一的区别是“poste”被替换为“post”。尽管两者之间存在差异，但这次练习的主要目的是展示这些预训练模型如何生成机器翻译，我们已经使用这两个模型完成了这一目标。
- en: Conclusion
  id: totrans-129
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: In this post we introduced Hugging Face, an open-source AI community used by
    and for many machine learning practitioners in NLP, computer vision and audio/speech
    processing tasks. We then walked through the implementation of such pre-trained
    models within the Hugging Face platform to accomplish downstream NLP tasks, such
    as text generation, question answering, sentiment analysis, text classification,
    text summarization and machine translation.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我们介绍了 Hugging Face，这是一个开源 AI 社区，被许多从事自然语言处理、计算机视觉和音频/语音处理任务的机器学习从业者使用。然后，我们演示了如何在
    Hugging Face 平台上实现这些预训练模型，以完成下游 NLP 任务，如文本生成、问答、情感分析、文本分类、文本摘要和机器翻译。
- en: Thanks for Reading!
  id: totrans-131
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 感谢阅读！
- en: If you found this post helpful, please [follow me on Medium](https://medium.com/@fmnobar)
    and subscribe to receive my latest posts!
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你觉得这篇文章对你有帮助，请[关注我在 Medium](https://medium.com/@fmnobar)并订阅以接收我最新的帖子！
- en: '*(All images, unless otherwise noted, are by the author.)*'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '*(除非另有说明，所有图片均由作者提供。)*'
