- en: How to Train the LILT Model on Invoices and Run Inference
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何训练 LILT 模型并在发票上进行推断
- en: 原文：[https://towardsdatascience.com/how-to-train-the-lilt-model-on-invoices-and-run-inference-8fd6b3cfae1b](https://towardsdatascience.com/how-to-train-the-lilt-model-on-invoices-and-run-inference-8fd6b3cfae1b)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/how-to-train-the-lilt-model-on-invoices-and-run-inference-8fd6b3cfae1b](https://towardsdatascience.com/how-to-train-the-lilt-model-on-invoices-and-run-inference-8fd6b3cfae1b)
- en: A Step-by-Step Tutorial
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分步教程
- en: '[](https://walidamamou.medium.com/?source=post_page-----8fd6b3cfae1b--------------------------------)[![Walid
    Amamou](../Images/c5ae089c59a5ff070f0f90ad63ee3817.png)](https://walidamamou.medium.com/?source=post_page-----8fd6b3cfae1b--------------------------------)[](https://towardsdatascience.com/?source=post_page-----8fd6b3cfae1b--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----8fd6b3cfae1b--------------------------------)
    [Walid Amamou](https://walidamamou.medium.com/?source=post_page-----8fd6b3cfae1b--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://walidamamou.medium.com/?source=post_page-----8fd6b3cfae1b--------------------------------)[![Walid
    Amamou](../Images/c5ae089c59a5ff070f0f90ad63ee3817.png)](https://walidamamou.medium.com/?source=post_page-----8fd6b3cfae1b--------------------------------)[](https://towardsdatascience.com/?source=post_page-----8fd6b3cfae1b--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----8fd6b3cfae1b--------------------------------)
    [Walid Amamou](https://walidamamou.medium.com/?source=post_page-----8fd6b3cfae1b--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----8fd6b3cfae1b--------------------------------)
    ·5 min read·Jan 3, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于[Towards Data Science](https://towardsdatascience.com/?source=post_page-----8fd6b3cfae1b--------------------------------)
    ·5分钟阅读·2023年1月3日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/f78a852d92471c06d30e5522584f4505.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f78a852d92471c06d30e5522584f4505.png)'
- en: Image by [**Zinkevych_D**](https://elements.envato.com/user/Zinkevych_D)from
    [Envanto](https://elements.envato.com/close-up-of-an-invoice-being-signed-HL7W2QD)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：[**Zinkevych_D**](https://elements.envato.com/user/Zinkevych_D)来自[Envato](https://elements.envato.com/close-up-of-an-invoice-being-signed-HL7W2QD)
- en: In the realm of document understanding, deep learning models have played a significant
    role. These models are able to accurately interpret the content and structure
    of documents, making them valuable tools for tasks such as invoice processing,
    resume parsing, and contract analysis. Another important benefit of deep learning
    models for document understanding is their ability to learn and adapt over time.
    As new types of documents are encountered, these models can continue to learn
    and improve their performance, making them highly scalable and efficient for tasks
    such as document classification and information extraction.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在文档理解领域，深度学习模型发挥了重要作用。这些模型能够准确地解读文档的内容和结构，使其成为处理发票、简历解析和合同分析等任务的宝贵工具。深度学习模型在文档理解中的另一个重要好处是它们能够随着时间的推移不断学习和适应。随着新类型文档的出现，这些模型可以继续学习并提升性能，使其在文档分类和信息提取等任务中具有高度的可扩展性和效率。
- en: One of these models is the LILT model (Language-Independent Layout Transformer),
    a deep learning model developed for the task of document layout analysis. Unlike
    it’s layoutLM predecessor, LILT is originally designed to be language-independent,
    meaning it can analyze documents in any language while achieving superior performance
    compared to other existing models in many downstream tasks application. Furthermore,
    the model has the MIT license, which means it can be used commercially unlike
    the latest layoutLM v3 and layoutXLM. Therefore, it is worthwhile to create a
    tutorial on how to fine-tune this model as it has the potential to be widely used
    for a wide range of document understanding tasks.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 其中一个模型是 LILT 模型（Language-Independent Layout Transformer），这是一个为文档布局分析任务开发的深度学习模型。与其前身
    LayoutLM 不同，LILT 最初设计为语言无关，这意味着它可以分析任何语言的文档，同时在许多下游任务应用中相较于其他现有模型表现优越。此外，该模型拥有
    MIT 许可证，这意味着它可以用于商业用途，与最新的 LayoutLM v3 和 LayoutXLM 不同。因此，创建一个关于如何微调该模型的教程是值得的，因为它有潜力被广泛用于各种文档理解任务。
- en: In this tutorial, we will discuss this novel model architecture and show how
    to fine-tune it on invoice extraction. We will then use it to run inference on
    a new set of invoices.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在本教程中，我们将讨论这一新型模型架构，并展示如何在发票提取上进行微调。然后，我们将使用该模型对一组新的发票进行推断。
- en: 'LILT Model Architecture:'
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LILT 模型架构：
- en: One of the key advantages of using the LILT model is its ability to handle multi-language
    document understanding with state-of-the-art performance. The authors achieved
    this by separating the text and layout embedding into their corresponding transformer
    architecture and using a bi-directional attention complementation mechanism (BiACM)
    to enable cross-modality interaction between the two types of data. The encoded
    text and layout features are then concatenated and additional heads are added,
    allowing the model to be used for either self-supervised pre-training or downstream
    fine-tuning. This approach is different from the layoutXLM model, which involves
    collecting and pre-processing a large dataset of multilingual documents.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 LILT 模型的一个关键优势是其处理多语言文档理解的能力，表现出色。作者通过将文本和布局嵌入分离到各自的变压器架构中，并使用双向注意力补充机制（BiACM）实现两种数据类型之间的跨模态交互，从而达到了这一点。编码后的文本和布局特征被连接在一起，并添加了额外的头部，允许模型用于自监督预训练或下游微调。这种方法不同于
    layoutXLM 模型，该模型涉及收集和预处理大量多语言文档数据集。
- en: '![](../Images/80a1dfef3ac7ef69893a1ca01e382149.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/80a1dfef3ac7ef69893a1ca01e382149.png)'
- en: LILT Model Architecture. [Source](https://arxiv.org/pdf/2202.13669.pdf)
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: LILT 模型架构。 [来源](https://arxiv.org/pdf/2202.13669.pdf)
- en: The key novelty in this model is the use of the BiACM to capture the cross-interaction
    between the text and layout features during the encoding process. Simply concatenating
    the text and layout model output results in worse performance, suggesting that
    cross-interaction during the encoding pipeline is key to the success of this model.
    For more in-depth details, read the [original article](https://arxiv.org/pdf/2202.13669.pdf).
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型的关键创新在于使用 BiACM 来捕捉文本和布局特征在编码过程中的交互。仅仅连接文本和布局模型输出会导致性能下降，这表明编码流程中的交互对于模型的成功至关重要。有关更深入的细节，请阅读
    [原始文章](https://arxiv.org/pdf/2202.13669.pdf)。
- en: 'Model Fine-tuning:'
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型微调：
- en: 'Similar to my previous articles on how to [fine-tune the layoutLM model](https://medium.com/towards-data-science/fine-tuning-layoutlm-v3-for-invoice-processing-e64f8d2c87cf),
    we will use the same dataset to fine-tune the LILT model. The data was obtained
    by manually labeling 220 invoices using [UBIAI](https://ubiai.tools) text annotation
    tool. More details about the labeling process can be found in this [link](https://medium.com/towards-data-science/how-to-annotate-pdfs-and-scanned-images-for-nlp-applications-f7b7b1db5c4a).
    For an in-depth video tutorial, checkout the link below:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于我之前关于如何 [微调 layoutLM 模型](https://medium.com/towards-data-science/fine-tuning-layoutlm-v3-for-invoice-processing-e64f8d2c87cf)
    的文章，我们将使用相同的数据集来微调 LILT 模型。数据通过使用 [UBIAI](https://ubiai.tools) 文本注释工具手动标记 220
    张发票获得。关于标记过程的更多细节可以在这个 [链接](https://medium.com/towards-data-science/how-to-annotate-pdfs-and-scanned-images-for-nlp-applications-f7b7b1db5c4a)
    中找到。有关深入的视频教程，请查看以下链接：
- en: LILT Tutorial
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: LILT 教程
- en: 'To train the model, we first pre-pre-process the data output from UBIAI to
    get it ready for model training. These steps are the same as in the previous notebook
    training the layoutLM model, here is the notebook:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 要训练模型，我们首先预处理来自 UBIAI 的数据输出，以准备模型训练。这些步骤与之前训练 layoutLM 模型的笔记本中的步骤相同，这里是笔记本：
- en: '[](https://colab.research.google.com/drive/1RQwdD86PUfrsL2GAJkl2o-zAb0CjT1N1?usp=sharing&authuser=2&source=post_page-----8fd6b3cfae1b--------------------------------#scrollTo=iInYrU6DpD5p)
    [## Google Colaboratory'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '[Google Colaboratory](https://colab.research.google.com/drive/1RQwdD86PUfrsL2GAJkl2o-zAb0CjT1N1?usp=sharing&authuser=2&source=post_page-----8fd6b3cfae1b--------------------------------#scrollTo=iInYrU6DpD5p)'
- en: Edit description
  id: totrans-21
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 编辑描述
- en: colab.research.google.com](https://colab.research.google.com/drive/1RQwdD86PUfrsL2GAJkl2o-zAb0CjT1N1?usp=sharing&authuser=2&source=post_page-----8fd6b3cfae1b--------------------------------#scrollTo=iInYrU6DpD5p)
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '[colab.research.google.com](https://colab.research.google.com/drive/1RQwdD86PUfrsL2GAJkl2o-zAb0CjT1N1?usp=sharing&authuser=2&source=post_page-----8fd6b3cfae1b--------------------------------#scrollTo=iInYrU6DpD5p)'
- en: 'We download the LILT model from Huggingface:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从 Huggingface 下载 LILT 模型：
- en: '[PRE0]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'For this model training, we use the following hyperparameters:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 对于此模型训练，我们使用以下超参数：
- en: '[PRE1]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'To train the model, simply run trainer.train() command:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 要训练模型，只需运行 trainer.train() 命令：
- en: '![](../Images/cb180a75e6c077e3640da1a3545a721d.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/cb180a75e6c077e3640da1a3545a721d.png)'
- en: 'Image by Author: Model Training In Progress.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片：模型训练进行中。
- en: 'On GPU, training takes approximately 1h. After training, we evaluate the model
    by running trainer.evaluate():'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在 GPU 上，训练大约需要 1 小时。训练完成后，我们通过运行 trainer.evaluate() 来评估模型：
- en: '[PRE2]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: We get a precision, recall and F-1 score of 0.63, 0.74 and 0.68 respectively.
    The LILT model evaluation F-1 score of 0.68 indicates that the model is performing
    well in terms of its ability to accurately classify and predict outcomes with
    a moderate to good accuracy. It is worth noting, however, that there is always
    room for improvement, and it is beneficial to continue labeling more data in order
    to further increase its performance. Overall, the LILT model evaluation F-1 score
    of 0.68 is a positive result and suggests that the model is performing well in
    its intended task.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的精度、召回率和F-1分数分别为0.63、0.74和0.68。LILT模型的F-1分数为0.68，表明模型在准确分类和预测结果方面表现良好，准确率中等到良好。然而，值得注意的是，仍有改进的空间，继续标注更多数据将有助于进一步提高模型性能。总体来看，LILT模型的F-1分数为0.68是一个积极的结果，表明模型在其预期任务中表现良好。
- en: In order to assess the model performance on unseen data, we run inference on
    a new invoice.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 为了评估模型在未见数据上的表现，我们在新发票上运行了推理。
- en: 'We make sure to save the model so we can use it for inference later on using
    this command:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 我们确保保存模型，以便稍后使用以下命令进行推理：
- en: '[PRE3]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Model Inference:'
  id: totrans-36
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型推理：
- en: 'To test the model on a new invoice, we run the inference script below:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 要在新的发票上测试模型，我们运行下面的推理脚本：
- en: '[](https://colab.research.google.com/drive/1om_xsTuuwOXzrldhkQj76HXSQkLYftRz?usp=sharing&source=post_page-----8fd6b3cfae1b--------------------------------)
    [## Google Colaboratory'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://colab.research.google.com/drive/1om_xsTuuwOXzrldhkQj76HXSQkLYftRz?usp=sharing&source=post_page-----8fd6b3cfae1b--------------------------------)
    [## Google Colaboratory'
- en: Edit description
  id: totrans-39
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 编辑描述
- en: colab.research.google.com](https://colab.research.google.com/drive/1om_xsTuuwOXzrldhkQj76HXSQkLYftRz?usp=sharing&source=post_page-----8fd6b3cfae1b--------------------------------)
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: colab.research.google.com](https://colab.research.google.com/drive/1om_xsTuuwOXzrldhkQj76HXSQkLYftRz?usp=sharing&source=post_page-----8fd6b3cfae1b--------------------------------)
- en: 'Below is the result:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是结果：
- en: '![](../Images/b13a558feb2c6205e1b2c1356f2557a7.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b13a558feb2c6205e1b2c1356f2557a7.png)'
- en: 'Image by Author: LILT output on invoice 1'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片：LILT在发票1上的输出
- en: 'The LILT model correctly identified a wide range of entities, including seller
    names, invoice numbers, and total amounts. Let’s try out a couple more invoices:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: LILT模型正确识别了广泛的实体，包括卖方名称、发票号码和总金额。让我们再尝试几张发票：
- en: '![](../Images/50a46c6abf6d0520ca68f0e058cbaeb0.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/50a46c6abf6d0520ca68f0e058cbaeb0.png)'
- en: 'Image by Author: LILT output on invoice 2'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片：LILT在发票2上的输出
- en: '![](../Images/9c92e307369f4cd2a32d2f48003c33f7.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9c92e307369f4cd2a32d2f48003c33f7.png)'
- en: 'Image by Author: LILT output on invoice 3'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片：LILT在发票3上的输出
- en: As we can see, the LILT model was able to handle a variety of different formats
    with different context with a relatively good accuracy although it made few mistakes.
    Overall, the LILT model performed well and its predictions were similar to those
    produced by layoutlm v3 highlighting its effectiveness for document understanding
    tasks.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，LILT模型能够处理各种不同格式和不同上下文的内容，尽管有一些错误，但准确性相对较好。总体而言，LILT模型表现良好，其预测结果与layoutLM
    v3产生的结果相似，突显了其在文档理解任务中的有效性。
- en: Conclusion
  id: totrans-50
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: In conclusion, the LILT model has proven to be effective for document understanding
    tasks. Unlike the layoutLM v3 model, the LILT model is MIT licensed which allows
    for widespread commercial adoption and use by researchers and developers, making
    it a desirable choice for many projects. As a next step, we can improve the model
    performance by labeling and improving the training dataset.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，LILT模型已经证明在文档理解任务中是有效的。与layoutLM v3模型不同，LILT模型采用MIT许可证，这允许研究人员和开发者广泛商业使用，使其成为许多项目的理想选择。下一步，我们可以通过标注和改进训练数据集来提升模型性能。
- en: If you want to efficiently and easily create your own training dataset, checkout
    [UBIAI’s OCR annotation feature](https://ubiai.tools/Signup) for free.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想高效便捷地创建自己的训练数据集，可以免费体验[UBIAI的OCR标注功能](https://ubiai.tools/Signup)。
- en: Follow us on Twitter [@UBIAI5](https://twitter.com/UBIAI5) or [subscribe here](https://walidamamou.medium.com/subscribe)!
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在Twitter上关注我们[@UBIAI5](https://twitter.com/UBIAI5)或[订阅这里](https://walidamamou.medium.com/subscribe)!
