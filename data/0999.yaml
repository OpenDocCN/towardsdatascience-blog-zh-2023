- en: Improving Hebrew Q&A Models via Intelligent Prompting
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通过智能提示改进希伯来语问答模型
- en: 原文：[https://towardsdatascience.com/improving-hebrew-q-a-models-via-prompting-20f5fb5a2f36?source=collection_archive---------8-----------------------#2023-03-17](https://towardsdatascience.com/improving-hebrew-q-a-models-via-prompting-20f5fb5a2f36?source=collection_archive---------8-----------------------#2023-03-17)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/improving-hebrew-q-a-models-via-prompting-20f5fb5a2f36?source=collection_archive---------8-----------------------#2023-03-17](https://towardsdatascience.com/improving-hebrew-q-a-models-via-prompting-20f5fb5a2f36?source=collection_archive---------8-----------------------#2023-03-17)
- en: '[](https://medium.com/@erap129?source=post_page-----20f5fb5a2f36--------------------------------)[![Elad
    Rapaport](../Images/2ad958f92cd8b5735da900ae8f5559f3.png)](https://medium.com/@erap129?source=post_page-----20f5fb5a2f36--------------------------------)[](https://towardsdatascience.com/?source=post_page-----20f5fb5a2f36--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----20f5fb5a2f36--------------------------------)
    [Elad Rapaport](https://medium.com/@erap129?source=post_page-----20f5fb5a2f36--------------------------------)'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@erap129?source=post_page-----20f5fb5a2f36--------------------------------)[![Elad
    Rapaport](../Images/2ad958f92cd8b5735da900ae8f5559f3.png)](https://medium.com/@erap129?source=post_page-----20f5fb5a2f36--------------------------------)[](https://towardsdatascience.com/?source=post_page-----20f5fb5a2f36--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----20f5fb5a2f36--------------------------------)
    [Elad Rapaport](https://medium.com/@erap129?source=post_page-----20f5fb5a2f36--------------------------------)'
- en: ·
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fd2d1ff8f0490&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fimproving-hebrew-q-a-models-via-prompting-20f5fb5a2f36&user=Elad+Rapaport&userId=d2d1ff8f0490&source=post_page-d2d1ff8f0490----20f5fb5a2f36---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----20f5fb5a2f36--------------------------------)
    ·13 min read·Mar 17, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F20f5fb5a2f36&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fimproving-hebrew-q-a-models-via-prompting-20f5fb5a2f36&user=Elad+Rapaport&userId=d2d1ff8f0490&source=-----20f5fb5a2f36---------------------clap_footer-----------)'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fd2d1ff8f0490&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fimproving-hebrew-q-a-models-via-prompting-20f5fb5a2f36&user=Elad+Rapaport&userId=d2d1ff8f0490&source=post_page-d2d1ff8f0490----20f5fb5a2f36---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----20f5fb5a2f36--------------------------------)
    ·13分钟阅读·2023年3月17日'
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F20f5fb5a2f36&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fimproving-hebrew-q-a-models-via-prompting-20f5fb5a2f36&source=-----20f5fb5a2f36---------------------bookmark_footer-----------)![](../Images/460a2dc30eb62d6e636e117e21cf5c2f.png)'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F20f5fb5a2f36&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fimproving-hebrew-q-a-models-via-prompting-20f5fb5a2f36&source=-----20f5fb5a2f36---------------------bookmark_footer-----------)![](../Images/460a2dc30eb62d6e636e117e21cf5c2f.png)'
- en: 'DALL-E 2: “A robot dressed in a robe writing biblical sentences in Hebrew.
    Digital art.”'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 'DALL-E 2: “一个穿着长袍的机器人用希伯来语书写圣经句子。数字艺术。”'
- en: Hi all,
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 大家好，
- en: I am sharing a short project I worked on that involves improving the performance
    of the `text-davinci-003` model by OpenAI using intelligent prompting. I will
    start by saying that this work is inspired by the excellent video tutorials by
    [James Briggs](https://www.youtube.com/@jamesbriggs/featured) — (specifically
    this one — [https://youtu.be/dRUIGgNBvVk](https://youtu.be/dRUIGgNBvVk)) and much
    of the code I used is taken from his examples as well.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 我分享了一个我参与的短期项目，涉及通过智能提示提高 `text-davinci-003` 模型的性能。我首先要说的是，这项工作受到 [James Briggs](https://www.youtube.com/@jamesbriggs/featured)
    的优秀视频教程的启发——特别是这一个 — [https://youtu.be/dRUIGgNBvVk](https://youtu.be/dRUIGgNBvVk)，我使用的许多代码也取自他的示例。
- en: 'The outline of this article is as follows:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 本文的提纲如下：
- en: Intro — Unique and Proprietary data in Large Language Models (LLMs)
  id: totrans-11
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 简介 — 大型语言模型（LLMs）中的独特和专有数据
- en: Problem Statement — Answering Questions in Hebrew
  id: totrans-12
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 问题陈述 — 用希伯来语回答问题
- en: Solution Suggestion — Prompting & Querying
  id: totrans-13
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 解决方案建议 — 提示与查询
- en: Experiment — Question Answering in Hebrew
  id: totrans-14
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实验 — 希伯来语问答
- en: Conclusions
  id: totrans-15
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 结论
- en: A Google Colab notebook with the complete code is here — [https://colab.research.google.com/drive/1_UqPHGPW1yLf3O_BOySRjf3bWqMe6A4H#scrollTo=4DY7XgilIr-H](https://colab.research.google.com/drive/1_UqPHGPW1yLf3O_BOySRjf3bWqMe6A4H#scrollTo=4DY7XgilIr-H)
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 完整代码的 Google Colab 笔记本在这里 — [https://colab.research.google.com/drive/1_UqPHGPW1yLf3O_BOySRjf3bWqMe6A4H#scrollTo=4DY7XgilIr-H](https://colab.research.google.com/drive/1_UqPHGPW1yLf3O_BOySRjf3bWqMe6A4H#scrollTo=4DY7XgilIr-H)
- en: Part 1\. Problem Statement — Unique and Proprietary data in Large Language Models
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第1部分 问题陈述 — 大型语言模型中的独特和专有数据
- en: The main official language in Israel is Hebrew, and as an Israeli, I am interested
    in improving the performance of large language models in Hebrew. It is a widely
    known fact that the performance of ChatGPT in Hebrew could be better relative
    to its performance in English, despite recent improvements. In this article, though,
    Hebrew is only an allegory to a source of data that is under-represented in a
    large language model and this article is going to deal with an attempt to assist
    an LLM in yielding useful results which are based on such data.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 以色列的主要官方语言是希伯来语，作为以色列人，我有兴趣提高大型语言模型在希伯来语中的表现。虽然近期有所改善，但众所周知，ChatGPT 在希伯来语中的表现相较于其在英语中的表现仍有待提高。本文中，希伯来语只是代表那些在大型语言模型中被低估的数据来源，本文将探讨如何尝试帮助
    LLM 提供基于这些数据的有用结果。
- en: One option, which was common before the era of very large language models, was
    to perform additional training on the model with the new data/task. When the models
    were at a scale of millions of parameters this option was a viable approach for
    many people and use cases and was achievable using common consumer hardware.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在非常大型语言模型出现之前，常见的一个选择是对模型进行额外的训练，加入新的数据/任务。当模型的参数规模达到百万级时，这种方法对许多人和用例来说是可行的，并且可以使用常见的消费级硬件来实现。
- en: In the past several years there has been an explosion in model sizes for large
    language models, which nowadays reach hundreds of billions of parameters. On the
    other hand, consumer hardware has not seen such a massive efficiency explosion.
    Thus, the option for fine-tuning these models has been taken off the table for
    most users and organizations.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去几年中，大型语言模型的规模爆炸性增长，如今已经达到数百亿个参数。另一方面，消费级硬件并没有出现如此大规模的效率提升。因此，针对这些模型的微调选项已经被大多数用户和组织排除在外。
- en: ChatGPT is widely known already and it achieves remarkable performance on many
    tasks and is proving to be an invaluable AI assistant. Yet still, if I want to
    use this technology to assist a person speaking in a rare language, or to assist
    a company with proprietary data which is not available online — chances are that
    ChatGPT will fail for obvious reasons of not seeing the data beforehand. So how
    do we make use of special/proprietary data in these models, without requiring
    millions of dollars for training using special hardware? One option is prompting
    via querying, which we will explore today.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: ChatGPT 已经广泛为人所知，在许多任务上表现出色，证明了它是一个不可或缺的 AI 助手。然而，如果我希望利用这项技术来帮助说稀有语言的人，或帮助处理公司内部的专有数据（这些数据在线上不可用），那么
    ChatGPT 很可能因为之前未见过这些数据而失败。那么我们如何在这些模型中利用特殊/专有数据，而不需要花费数百万美元来使用特殊硬件进行训练呢？一个可行的选择是通过查询提示，这一点我们今天将进行探索。
- en: '**Part 2\.** Problem Statement — Answering Questions in Hebrew'
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**第2部分** 问题陈述 — 希伯来语中的问答'
- en: We will use the OpenAI API to answer three questions in Hebrew, which were inspired
    by the [SQuAD](https://rajpurkar.github.io/SQuAD-explorer/explore/v2.0/dev/) dataset
    and translated into Hebrew. The following code snippet shows the questions, expected
    answers, and English translations.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用 OpenAI API 来回答三个希伯来语问题，这些问题的灵感来自于[SQuAD](https://rajpurkar.github.io/SQuAD-explorer/explore/v2.0/dev/)数据集，并翻译成希伯来语。以下代码片段展示了这些问题、预期的答案和英文翻译。
- en: '[PRE0]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '**Disclaimer — the recent version of ChatGPT proves to yield very good answers
    for these queries (in Hebrew). The point of this article is not to “compete” with
    it, but rather to show a solution to a problem that might arise with other models/data.**'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '**免责声明 — 最新版本的 ChatGPT 对这些查询（希伯来语）的回答表现非常好。本文的重点不是“竞争”，而是展示一个可能在其他模型/数据中出现的问题的解决方案。**'
- en: 'We will use the `text-davinci-003` model to answer the questions using this
    code, which will call the OpenAI API:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用 `text-davinci-003` 模型来回答这些问题，使用的代码将调用 OpenAI API：
- en: '[PRE1]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Output:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：
- en: '[PRE2]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: We see that `text-davinci-003` is very bad at answering these questions out
    of the box. It gets all three questions wrong and produces some ridiculous answers
    for the second and third questions.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到 `text-davinci-003` 在默认情况下回答这些问题的表现非常差。它把所有三个问题都答错了，并且对第二和第三个问题产生了一些荒谬的答案。
- en: So the question is asked — how do we solve this? How can we get good answers
    from `text-davinci-003` without having to fine-tune it on a bunch of Hebrew data?
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 所以问题来了——我们如何解决这个问题？我们如何从 `text-davinci-003` 获得良好的答案，而不必在一堆希伯来数据上对其进行微调？
- en: Part 3\. Solution Suggestion — Prompting & Querying
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第三部分\. 解决方案建议——提示与查询
- en: As I wrote in the introduction, one way to add information to the LLM without
    fine-tuning is prompting, which is simply including relevant information in the
    input to the model. But including relevant information for each query can be a
    very tedious and time-consuming task, can we automate this?
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我在介绍中写的那样，向 LLM 添加信息的一种方法是提示，即在模型的输入中包含相关信息。但为每个查询包含相关信息可能是一个非常繁琐且耗时的任务，我们能否对此进行自动化？
- en: Of course, we can! Enter prompting via querying. We will take a fairly small
    dump of Wikipedia in Hebrew, which definitely contains answers to our questions.
    We will split this dump into lines, embed each of these lines using an embedding
    model by OpenAI, and insert these embeddings into a vector database called Pinecone
    DB. Then, for each question we ask the model, we will search the vector database
    for bits of information that are semantically similar to the question and append
    them to the model input as additional information. This procedure is shown in
    figure 1\. Hopefully, the model will be able to make use of the provided context
    and yield correct answers to our questions.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，我们可以！通过查询来进行提示。我们将使用希伯来语的维基百科的一个较小的快照，这个快照中肯定包含了我们问题的答案。我们将把这个快照分割成多行，使用
    OpenAI 的嵌入模型对每一行进行嵌入，并将这些嵌入插入到一个名为 Pinecone DB 的向量数据库中。然后，对于我们提出的每一个问题，我们将搜索向量数据库中与问题语义相似的信息片段，并将这些片段附加到模型输入中作为附加信息。这个过程如图
    1 所示。希望模型能够利用提供的上下文，并给出我们问题的正确答案。
- en: '![](../Images/37163f1f594e22773dcc5dfc74fb4106.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/37163f1f594e22773dcc5dfc74fb4106.png)'
- en: Figure 1\. Prompting with querying
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1\. 通过查询进行提示
- en: Inspired by Jason Briggs, I will show you a prompt template that will help us
    generate intelligible prompts to `text-davinci-003` and hopefully, make it answer
    our three previous questions correctly.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 受到 Jason Briggs 的启发，我将展示一个提示模板，这将帮助我们生成可理解的提示给 `text-davinci-003`，并希望使其正确回答我们之前的三个问题。
- en: Part 4\. Experiment — Question Answering in Hebrew
  id: totrans-38
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第四部分\. 实验——希伯来语问答
- en: First things first — we need to populate our vector database with relevant information.
    For this purpose we are going to use a small Hebrew Wikipedia dump which I downloaded
    from here — [https://u.cs.biu.ac.il/~yogo/hebwiki/](https://u.cs.biu.ac.il/~yogo/hebwiki/)
    (available under a Creative Commons Attribution-ShareAlike 4.0 International License).
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 首先——我们需要用相关信息填充我们的向量数据库。为此，我们将使用一个从这里下载的小型希伯来语维基百科快照——[https://u.cs.biu.ac.il/~yogo/hebwiki/](https://u.cs.biu.ac.il/~yogo/hebwiki/)（在
    Creative Commons Attribution-ShareAlike 4.0 International License 下提供）。
- en: '[PRE3]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '[PRE4]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'For general information, I calculated how much it will cost to embed the whole
    Wikipedia dump using the OpenAI API — not too bad! But still, for our purposes
    we only need specific subjects so we can save some money by choosing more particular
    sentences to embed:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 对于一般信息，我计算了一下使用 OpenAI API 嵌入整个维基百科快照的费用——还不错！但为了我们的目的，我们只需要特定的主题，所以我们可以通过选择更具体的句子来节省一些费用：
- en: '[PRE5]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '[PRE6]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'That’s better. We are now ready to populate the vector database:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 好一些了。我们现在准备填充向量数据库：
- en: '[PRE7]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'We generated a new Pinecone DB index (if it didn’t exist yet). Let’s populate
    it with our data:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 我们生成了一个新的 Pinecone DB 索引（如果它尚不存在的话）。让我们用我们的数据填充它：
- en: '[PRE8]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Great! We have a populated vector DB. Let’s get the top-2 contexts for the
    first question (Which city in Victoria is considered the sporting capital of Australia?):'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 很好！我们已经填充了向量数据库。让我们为第一个问题（维多利亚州哪个城市被认为是澳大利亚的体育之都？）获取前两个最相关的上下文：
- en: '[PRE9]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '[PRE10]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: We see that the top 2 lines in our vector DB (in terms of cosine similarity
    to the question’s embedding) are not so helpful. The first line is a very general
    sentence about Australia and the second indeed talks about sports in Australia,
    but there is no mention of Melbourne which is the answer we are expecting.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到，在我们的向量数据库中（以与问题嵌入的余弦相似度为准），前两行并不太有用。第一行是关于澳大利亚的非常一般的句子，第二行确实谈论了澳大利亚的体育，但没有提到我们期望的答案——墨尔本。
- en: But still, let’s challenge our model and see how it fares now that it has access
    to additional information in Hebrew. Notice that we instructed our model to answer
    “I don’t know” if the answer cannot be deduced from the given context.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 但仍然，让我们挑战我们的模型，看看在它现在可以访问额外的希伯来语信息的情况下表现如何。请注意，我们指示模型在无法从给定上下文中推导出答案时回答“I don’t
    know”。
- en: '[PRE11]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '[PRE12]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: OK, getting a bit better. The first answer is still wrong. In the second and
    third answers, we got “I don’t know” which I perceive as a major improvement over
    getting wrong answers. But still, we want to give current answers, how can we
    improve on this..?
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，稍微有所改善。第一个答案仍然错误。在第二个和第三个答案中，我们得到了“I don’t know”，我认为这比错误答案要好得多。但我们仍然希望提供当前的答案，我们如何改进呢？
- en: Let’s add some additional subjects to the vector DB, maybe it didn’t have enough
    relevant information from which to pull a relevant context.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们向向量数据库中添加一些额外的主题，也许它没有足够的相关信息来提取相关上下文。
- en: '[PRE13]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '[PRE14]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Great, we added richer context (you might even say we are cheating a bit — notice
    that we added all sentences that contain the words “Melbourne” and “Charles” amongst
    others, which are the answers to our questions. But it’s just a demonstration!)
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 很好，我们添加了更丰富的上下文（你甚至可以说我们有点作弊 — 请注意，我们添加了所有包含“墨尔本”和“查尔斯”等词汇的句子，这些句子是我们问题的答案。但这只是一个演示！）
- en: '[PRE15]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '[PRE16]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Nice! We have our answers! Although the third answer is still not 100% correct,
    but way better than “I don’t know” and way better than the initial hallucination.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 不错！我们得到了答案！虽然第三个答案仍然不是 100% 正确，但比“I don’t know”好得多，也比最初的幻觉要好。
- en: Part 5\. Conclusions
  id: totrans-64
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第五部分：结论
- en: In this article, we have seen how to automatically provide relevant context
    to an LLM using a vector database in order to improve the model outputs. In this
    specific case, it might not have been necessary because the performance of ChatGPT
    (which is available as an API) on Hebrew has improved tremendously and it is able
    to answer these questions very well out-of-the-box.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们已经看到如何使用向量数据库自动提供相关上下文给 LLM，以改善模型输出。在这个具体的案例中，这可能不是必需的，因为 ChatGPT（作为
    API 提供）在希伯来语上的表现已经大幅提升，能够很好地回答这些问题。
- en: 'Yet still, the point of this article was to introduce this concept, which can
    be generalized for other purposes such as:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，本文的重点是介绍这一概念，这一概念可以推广到其他目的，例如：
- en: Using an LLM to analyze proprietary data which is not available online
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 LLM 分析未在网上公开的专有数据
- en: Adding a form of explainability to the model, by appending the context which
    was used to the output of the user (to prove the credibility of the answer).
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过将用于生成输出的上下文附加到用户输出中，为模型添加一种可解释性（以证明答案的可靠性）。
- en: I hope you enjoyed this article and were motivated to try it for yourself. See
    you next time!
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 希望你喜欢这篇文章，并激发了你自己尝试的动力。下次见！
- en: Elad
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: Elad
- en: References
  id: totrans-71
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: Hebrew Wikipedia dump — [https://u.cs.biu.ac.il/~yogo/hebwiki/](https://u.cs.biu.ac.il/~yogo/hebwiki/)
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 希伯来语维基百科数据 — [https://u.cs.biu.ac.il/~yogo/hebwiki/](https://u.cs.biu.ac.il/~yogo/hebwiki/)
- en: SQuAD dataset — [https://rajpurkar.github.io/SQuAD-explorer/](https://rajpurkar.github.io/SQuAD-explorer/)
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SQuAD 数据集 — [https://rajpurkar.github.io/SQuAD-explorer/](https://rajpurkar.github.io/SQuAD-explorer/)
- en: Video tutorial by James Briggs — [https://youtu.be/dRUIGgNBvVk](https://youtu.be/dRUIGgNBvVk)
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: James Briggs 的视频教程 — [https://youtu.be/dRUIGgNBvVk](https://youtu.be/dRUIGgNBvVk)
- en: OpenAI API — [https://openai.com/blog/openai-api](https://openai.com/blog/openai-api)
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAI API — [https://openai.com/blog/openai-api](https://openai.com/blog/openai-api)
- en: Pinecone DB — [https://www.pinecone.io/](https://www.pinecone.io/)
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pinecone 数据库 — [https://www.pinecone.io/](https://www.pinecone.io/)
