- en: Use Delta Lake as the Master Data Management (MDM) Source for Downstream Applications
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将 Delta Lake 用作下游应用程序的主数据管理（MDM）源
- en: 原文：[https://towardsdatascience.com/use-delta-lake-as-the-master-data-management-mdm-source-for-downstream-applications-882250dc0b4a](https://towardsdatascience.com/use-delta-lake-as-the-master-data-management-mdm-source-for-downstream-applications-882250dc0b4a)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/use-delta-lake-as-the-master-data-management-mdm-source-for-downstream-applications-882250dc0b4a](https://towardsdatascience.com/use-delta-lake-as-the-master-data-management-mdm-source-for-downstream-applications-882250dc0b4a)
- en: In this article, we will try to understand how the output from Delta Lake change
    feed can be used to feed downstream applications
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在本文中，我们将尝试了解如何利用 Delta Lake 更改提要的输出来为下游应用程序提供数据
- en: '[](https://mkukreja1.medium.com/?source=post_page-----882250dc0b4a--------------------------------)[![Manoj
    Kukreja](../Images/de7c954b505e9749bad47975a3bee80b.png)](https://mkukreja1.medium.com/?source=post_page-----882250dc0b4a--------------------------------)[](https://towardsdatascience.com/?source=post_page-----882250dc0b4a--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----882250dc0b4a--------------------------------)
    [Manoj Kukreja](https://mkukreja1.medium.com/?source=post_page-----882250dc0b4a--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://mkukreja1.medium.com/?source=post_page-----882250dc0b4a--------------------------------)[![Manoj
    Kukreja](../Images/de7c954b505e9749bad47975a3bee80b.png)](https://mkukreja1.medium.com/?source=post_page-----882250dc0b4a--------------------------------)[](https://towardsdatascience.com/?source=post_page-----882250dc0b4a--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----882250dc0b4a--------------------------------)
    [Manoj Kukreja](https://mkukreja1.medium.com/?source=post_page-----882250dc0b4a--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----882250dc0b4a--------------------------------)
    ·9 min read·Feb 5, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----882250dc0b4a--------------------------------)
    ·9 min read·2023年2月5日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/9929918cfc996b208d995d6122853179.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9929918cfc996b208d995d6122853179.png)'
- en: Image by [Satheesh Sankaran](https://pixabay.com/users/satheeshsankaran-11196627/?utm_source=link-attribution&utm_medium=referral&utm_campaign=image&utm_content=7206402)
    from [Pixabay](https://pixabay.com//?utm_source=link-attribution&utm_medium=referral&utm_campaign=image&utm_content=7206402)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由 [Satheesh Sankaran](https://pixabay.com/users/satheeshsankaran-11196627/?utm_source=link-attribution&utm_medium=referral&utm_campaign=image&utm_content=7206402)
    提供，来自 [Pixabay](https://pixabay.com//?utm_source=link-attribution&utm_medium=referral&utm_campaign=image&utm_content=7206402)
- en: As per ACID rules, the theory of isolation states that “***the intermediate
    state of any transaction should not affect other transactions***”. Almost every
    modern database has been built to follow this rule. Unfortunately, until recently
    the same rule could not be effectively implemented in the big data world. What
    was the reason?
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 根据 ACID 规则，隔离理论指出“***任何事务的中间状态不应影响其他事务***”。几乎每个现代数据库都是为了遵循这一规则而构建的。不幸的是，直到最近，同样的规则在大数据领域还未能有效实施。这是什么原因呢？
- en: 'Modern distributed processing frameworks such as **Hadoop MapReduce** and **Apache
    Spark** perform computations in batches. After the computations are completed
    a set of output files is generated, and each file stores a collection of records.
    Usually, the number of partitions and reducers influences how many output files
    will be generated. But there are a few problems:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 现代分布式处理框架，如 **Hadoop MapReduce** 和 **Apache Spark**，以批处理的方式进行计算。计算完成后，会生成一组输出文件，每个文件存储一系列记录。通常，分区和减少器的数量会影响生成的输出文件数量。但也存在一些问题：
- en: A minor record level change or new records addition (CDC) forces you to recompute
    the entire batch every time — a huge wastage of compute cycles impacting both
    cost and time.
  id: totrans-10
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 轻微的记录级更改或新增记录（CDC）会迫使你每次都重新计算整个批处理——这是一种巨大的计算周期浪费，影响了成本和时间。
- en: Downstream consumers may see inconsistent data during the time the batch is
    being recomputed
  id: totrans-11
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下游消费者在批处理重新计算期间可能会看到不一致的数据
- en: '![](../Images/a8eae1a9aa06950f68a96fd2fc91d9a5.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a8eae1a9aa06950f68a96fd2fc91d9a5.png)'
- en: Image by author
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片
- en: The Delta Lake framework adds the notion of transaction management to Spark
    computing. By adding support for ACID transactions, schema enforcement, indexing,
    versioning and data pruning, Delta Lake aims at improving the reliability, quality,
    and performance of data.
  id: totrans-14
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: Delta Lake 框架为 Spark 计算添加了事务管理的概念。通过支持 ACID 事务、模式强制、索引、版本控制和数据修剪，Delta Lake
    旨在提高数据的可靠性、质量和性能。
- en: In simplistic terms, using Delta Lake the entire batch does not need to be **recomputed**
    even though a few CDC records may have been added or changed. Instead, it provides
    functionality to INSERT, UPDATE, DELETE or MERGE data. Delta Lake works by selecting
    the files containing data that has changed, reading them in memory, and writing
    results in a new file.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 简单来说，即使添加或更改了一些 CDC 记录，也无需**重新计算**整个批次。相反，它提供了插入、更新、删除或合并数据的功能。Delta Lake 通过选择包含已更改数据的文件，将其读入内存，然后将结果写入新文件来工作。
- en: '![](../Images/40cf70e795a0f0eafc5b62b0884b6496.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/40cf70e795a0f0eafc5b62b0884b6496.png)'
- en: Image by author
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片
- en: '**Delta Lake** is widely used as the foundation for implementing the modern
    data Lakehouse architecture. Its core capabilities make it extremely suitable
    for merging data sets from diverse sources and varying schemas into a common data
    layer often referred to as the “***single source of truth***”. The “***single
    source of truth***” aka MDM layer is used to serve all analytical workloads including
    BI, data science, machine learning, and artificial intelligence.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '**Delta Lake** 被广泛用作实现现代数据湖屋架构的基础。它的核心功能使其非常适合将来自不同来源和不同模式的数据集合并到一个通常被称为“***单一真实来源***”的公共数据层中。这个“***单一真实来源***”即
    MDM 层，用于支持所有的分析工作负载，包括 BI、数据科学、机器学习和人工智能。'
- en: In this article, we will try to extend our understanding of Delta Lake one step
    further. If Delta Lake can act as a **sink** for merging data from varying sources,
    why can’t we use it as a **source** for capturing change data (CDC) for downstream
    consumers? Even better if we can use the [medallion architecture](https://databricks.com/notebooks/delta-lake-cdf.html)
    to achieve this. The medallion architecture can be used to merge CDC data from
    source systems into the bronze, silver, and gold layers of the data lakehouse.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们将尝试进一步拓展对 Delta Lake 的理解。如果 Delta Lake 可以作为**汇**来合并来自不同来源的数据，那我们为什么不能将其用作**源**来捕获下游消费者的变更数据（CDC）呢？如果我们能使用[奖章架构](https://databricks.com/notebooks/delta-lake-cdf.html)来实现这一点，那就更好了。奖章架构可以用来将来源系统中的
    CDC 数据合并到数据湖屋的青铜层、白银层和黄金层。
- en: 'Even better, we can capture changes and publish them to downstream consumers
    in a streaming fashion. Let us explore a use-case as under:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 更好的是，我们可以以流的方式捕获变更并将其发布到下游消费者。让我们探讨一个用例：
- en: Hotel prices change several times during the day
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 酒店价格在一天内会发生多次变化
- en: An e-commerce company is in the business of tracking the latest hotel prices
    around the world and displaying them on its web portal so that customers can book
    them based on real-time data.
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一家电子商务公司专注于追踪全球最新的酒店价格，并在其网站门户上展示这些信息，以便客户可以根据实时数据进行预订。
- en: '![](../Images/8e359a11e19a788e6012927421e43396.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8e359a11e19a788e6012927421e43396.png)'
- en: Image by author
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片
- en: In this example, we will read real-time price information from three API sources
    using a producer program. The producer will send data as events in JSON format
    to Amazon Kinesis. We will then read these events in a Databricks notebook using
    structured streaming. Finally, the CDC events are transmitted to a relational
    database. The idea is that the relational database is used by the e-commerce portal
    to display the ever-evolving hotel prices on their e-commerce portal.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在此示例中，我们将使用生产者程序从三个 API 来源读取实时价格信息。生产者将以 JSON 格式将数据作为事件发送到 Amazon Kinesis。然后，我们将在
    Databricks notebook 中使用结构化流来读取这些事件。最后，CDC 事件被传输到关系数据库。这个关系数据库被电子商务门户用来展示不断变化的酒店价格。
- en: '**Performing Prerequisite Steps for Running CDF Notebook**'
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**执行运行 CDF Notebook 的前置步骤**'
- en: 'The code for the example above is available at:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 上述示例的代码可以在以下位置找到：
- en: '[](https://github.com/mkukreja1/blogs/tree/master/cdc-source?source=post_page-----882250dc0b4a--------------------------------)
    [## blogs/cdc-source at master · mkukreja1/blogs'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://github.com/mkukreja1/blogs/tree/master/cdc-source?source=post_page-----882250dc0b4a--------------------------------)
    [## blogs/cdc-source at master · mkukreja1/blogs'
- en: You can't perform that action at this time. You signed in with another tab or
    window. You signed out in another tab or…
  id: totrans-29
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 目前无法执行该操作。您已在另一个标签或窗口中登录。您已在另一个标签或窗口中注销…
- en: github.com](https://github.com/mkukreja1/blogs/tree/master/cdc-source?source=post_page-----882250dc0b4a--------------------------------)
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: github.com](https://github.com/mkukreja1/blogs/tree/master/cdc-source?source=post_page-----882250dc0b4a--------------------------------)
- en: 'To run this code, you will need operational AWS and Databricks accounts. Before
    running the notebook in Databricks there are a few prerequisite steps that need
    t be performed on AWS:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 要运行此代码，你需要拥有有效的 AWS 和 Databricks 帐户。在 Databricks 中运行笔记本之前，需要在 AWS 上完成一些前提步骤：
- en: Get access to the AWS access key using the link below. The access key (access
    key ID and secret access key) will be used as the credentials for the Databricks
    notebook to access AWS services like Amazon Kinesis. [https://console.aws.amazon.com/iam/home?region=us-east-1#/security_credentials](https://console.aws.amazon.com/iam/home?region=us-east-1#/security_credentials)
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用下面的链接获取 AWS 访问密钥。访问密钥（访问密钥 ID 和秘密访问密钥）将作为 Databricks 笔记本访问 AWS 服务（如 Amazon
    Kinesis）的凭证。[https://console.aws.amazon.com/iam/home?region=us-east-1#/security_credentials](https://console.aws.amazon.com/iam/home?region=us-east-1#/security_credentials)
- en: 'While connected to the AWS portal click on the AWS cloud shell menu. Then run
    the commands below to create the prerequisite AWS resource required by this article:'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在连接到 AWS 门户时，点击 AWS 云终端菜单。然后运行下面的命令以创建本文所需的前提 AWS 资源：
- en: '[PRE0]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '![](../Images/5ea424ceafdc4d3a4a269172a3cb61be.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5ea424ceafdc4d3a4a269172a3cb61be.png)'
- en: Image by author
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片
- en: Start the Producer that will read events from APIs and sends them to Amazon
    Kinesis.
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 启动将从 API 读取事件并将其发送到 Amazon Kinesis 的生产者。
- en: '[PRE1]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '![](../Images/79856d2c58334dbc1f39b9d60f96e9dd.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/79856d2c58334dbc1f39b9d60f96e9dd.png)'
- en: Image by author
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片
- en: Keep the AWS cloud shell session running. From here onward the producer will
    send events to Amazon Kinesis every 5 minutes.
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 保持 AWS 云终端会话运行。从这里开始，生产者将每 5 分钟将事件发送到 Amazon Kinesis。
- en: '**Delta Lake change feed in Action!**'
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**Delta Lake 更改馈送正在运行中！**'
- en: Now that we have prerequisite resources created on AWS, we are ready run the
    CDC as a source notebook. Code in the Databricks notebook will read events from
    Amazon Kinesis, merge changes to the bronze layer, then perform cleanup and merge
    results to the silver layer. All of this will be accomplished in a streaming fashion,
    finally, the results (the change data feed) will be synced to an external relational
    database table. At this point, you need to be logged in to your Databricks account.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经在 AWS 上创建了前提资源，我们准备好将 CDC 作为源笔记本进行运行。Databricks 笔记本中的代码将从 Amazon Kinesis
    读取事件，将更改合并到铜层，然后进行清理并将结果合并到银层。所有这些都将在流式处理的方式中完成，最后，结果（更改数据馈送）将同步到外部关系数据库表。在此时，你需要登录到你的
    Databricks 帐户。
- en: '**Preparing the Delta Lake as a Change Data Feed Source Notebook Environment**'
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**准备 Delta Lake 作为更改数据馈送源笔记本环境**'
- en: Import the [delta-as-cdc-source-notebook.ipynb](https://github.com/mkukreja1/blogs/blob/master/cdc-source/delta-as-cdc-source-notebook.ipynb)
    notebook in you Databricks workspace. To run the notebook, you will need to replace
    three variables (*awsAccessKeyId*, *awsSecretKey* and *rdsConnectString*) with
    values fetched from the previous section.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 将 [delta-as-cdc-source-notebook.ipynb](https://github.com/mkukreja1/blogs/blob/master/cdc-source/delta-as-cdc-source-notebook.ipynb)
    笔记本导入到你的 Databricks 工作区。要运行笔记本，你需要用前一节中获取的值替换三个变量（*awsAccessKeyId*、*awsSecretKey*
    和 *rdsConnectString*）。
- en: '![](../Images/000d646cf2c1eb759fdbd6761e072297.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/000d646cf2c1eb759fdbd6761e072297.png)'
- en: Image by author
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片
- en: Creating the Delta Tables in the Bronze Layer
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在铜层中创建 Delta 表
- en: We will start by reading events from Amazon Kinesis. By default, the Kinesis
    connector for structured streaming is included in Databricks runtime. You may
    have noticed previously that we are sending JSON events in the payload of the
    stream. In the example below we are reading events using structured streaming,
    applying the schema to JSON, extracting values from it, and finally saving results
    as a Delta table in the bronze layer. We have chosen Amazon S3 as our storage
    location for all Delta Tables.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从 Amazon Kinesis 读取事件。默认情况下，Databricks 运行时包括结构化流的 Kinesis 连接器。你可能已经注意到，我们在流的有效载荷中发送了
    JSON 事件。在下面的示例中，我们使用结构化流读取事件，将架构应用于 JSON，提取其中的值，最后将结果保存为铜层中的 Delta 表。我们选择 Amazon
    S3 作为所有 Delta 表的存储位置。
- en: '![](../Images/6bc9eedd1c6d5f7e64442b01ec9d6329.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6bc9eedd1c6d5f7e64442b01ec9d6329.png)'
- en: Image by author
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片
- en: Notice that data in the bronze layer is the raw representation of events data,
    therefore we are adhering to a schema that matches the stream in its original
    form.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 注意到铜层中的数据是事件数据的原始表示，因此我们遵循匹配原始流形式的架构。
- en: '**Curating Data in the Change Data Stream**'
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**在更改数据流中策划数据**'
- en: From here onward the bronze layer table will keep adding **new partitions**
    based on data read from the Kinesis stream. It is a good practice to choose the
    timestamp as the partition column in the bronze layer. This helps easily identify
    the chronology of events as they are read from the source and play an important
    role if we need to replay events in the future.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 从这里开始，铜层表将根据从 Kinesis 流读取的数据不断添加**新分区**。在铜层中选择时间戳作为分区列是一个好习惯。这有助于轻松识别从源读取的事件的时间顺序，并在未来需要重放事件时发挥重要作用。
- en: In the next step, we are performing a few transformations to curate data such
    as changing Unix epoch time to date, changing data types, and splitting a field.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一步中，我们将进行一些转换以整理数据，例如将 Unix 纪元时间转换为日期、更改数据类型和拆分字段。
- en: '![](../Images/d0b401dacd7cccaf3bec2704779beac1.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d0b401dacd7cccaf3bec2704779beac1.png)'
- en: Image by author
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片
- en: We are ready to merge data into the silver layer now. But before that, we need
    to understand how CDC works in structured streaming. More importantly, how does
    the change log stream from the Delta layer gets published to downstream consumers?
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在准备将数据合并到银层中。但是在此之前，我们需要了解 CDC 在结构化流中的工作原理。更重要的是，Delta 层的变更日志流如何发布到下游消费者？
- en: '**Understanding the Flow of Delta as a Change Feed**'
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**理解 Delta 作为变更流的流程**'
- en: 'Reference to the example below lets us understand the flow of change feed data
    with Delta Lake being the source. In structured streaming data gets processed
    in **micro-batches**. The implementation involves writing the change data feed
    concurrently to multiple tables also known as Idempotent writes as follows:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 参考下面的示例，我们可以了解使用 Delta Lake 作为数据源的变更数据流动。在结构化流处理数据时，数据会被处理成**微批次**。实施过程中包括将变更数据同时写入多个表，这也称为幂等写入，如下所示：
- en: The silver layer table (**hotels_silver**) where records from each micro-batch
    are either inserted as a new record or merged into existing ones. Every change
    creates a new version of the delta table.
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 银层表（**hotels_silver**），其中每个微批次中的记录要么作为新记录插入，要么合并到现有记录中。每次变更都会创建 Delta 表的新版本。
- en: A change log table (**change_log)** that stores the ***Key*** and ***batchId***.
    View data in this table as an immutable log of changes over time.
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一个变更日志表（**change_log**）存储***键***和***batchId***。将该表中的数据视为时间上的不可变更日志。
- en: In the example below, the bronze layer stream shows two records (highlighted
    in the image below) for the **Mariott** hotel in **New York**. Notice the variation
    of price between the two records over time. Time chronology wise when the first
    record was read from Kinesis at **timestamp=022–02–16T21:06:57** itwas assigned
    to **batchId=2**. Now if we join the record using the key from the *change_log*
    to the record in the *hotels_silver* table we can reconstruct the row back and
    send it as a CDC record for downstream consumers. In the example below, the same
    record was sent twice to the downstream consumer at different time intervals.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的示例中，铜层流显示了**Marriott** 酒店在**纽约**的两条记录（在下图中突出显示）。注意这两条记录随时间变化的价格差异。时间上，当第一条记录从
    Kinesis 读取时，其**时间戳=022–02–16T21:06:57** 被分配为**batchId=2**。现在，如果我们使用 *change_log*
    表中的键将记录与 *hotels_silver* 表中的记录连接起来，我们可以重建该行并将其作为 CDC 记录发送给下游消费者。在下面的示例中，相同的记录在不同的时间间隔内被发送了两次。
- en: '![](../Images/b44e78927b82118d57895bace966a42a.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b44e78927b82118d57895bace966a42a.png)'
- en: Image by author
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片
- en: Second time a change record at **timestamp=022–02–16T21:07:41 it** was assigned
    to **batchId=3** and sent downstream. The downstream consumers can receive the
    CDC and keep its state up to date with the ongoing changes.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 第二次变更记录的**时间戳=022–02–16T21:07:41** 被分配为**batchId=3**并发送到下游。下游消费者可以接收 CDC 并保持其状态与持续变化保持同步。
- en: '**Implementing Delta as a Change Data Feed**'
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**将 Delta 实现为变更数据流**'
- en: With the understanding of the flow of data, let us deep dive into the actual
    implementation. The function below runs at the micro-batch level. For each micro-batch,
    this function performs **Idempotent** writes to the silver layer as well as the
    change record table.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 了解数据流动后，让我们深入探讨实际实现。下面的函数在微批次级别运行。对于每个微批次，该函数会对银层以及变更记录表执行**幂等**写入。
- en: '![](../Images/e2beb731664ad488f7cae842c0dd87c3.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e2beb731664ad488f7cae842c0dd87c3.png)'
- en: This function is invoked using *foreachBatch()* operation that allows arbitrary
    operations and writing logic on the output of a streaming query. In the code below
    we are performing an idempotent write of the curated data stream to two tables
    concurrently.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 该函数通过 *foreachBatch()* 操作调用，该操作允许对流查询的输出进行任意操作和写入逻辑。在下面的代码中，我们同时对两个表执行了经过整理的数据流的幂等写入。
- en: '![](../Images/eec89ad623cd577edf18100b686e5359.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/eec89ad623cd577edf18100b686e5359.png)'
- en: Image by author
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片
- en: While the idempotent writes are going on, for every new micro-batch the change
    data is joined to the silver table to reconstruct the CDC record.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在幂等写入进行的同时，对于每个新的微批次，变更数据会与银层表连接，以重建 CDC 记录。
- en: '![](../Images/a3f9f818a5cde5abe58b36c07058d67a.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a3f9f818a5cde5abe58b36c07058d67a.png)'
- en: Image by author
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片
- en: The reconstructed CDC records can then be synced downstream. In the example
    below we are sending the CDC records to a relational data store.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，可以将重建的 CDC 记录同步到下游。在下面的例子中，我们将 CDC 记录发送到关系型数据存储。
- en: '![](../Images/9d28d331d8c437821faa38b69fbcd26d.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9d28d331d8c437821faa38b69fbcd26d.png)'
- en: Image by author
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片
- en: The relational data store receives the **immutable** CDC record stream and performs
    **deduplication** logic to show the latest record equivalent on their applications.
    Let’s check how that happens in the section below.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 关系型数据存储接收**不可变**的 CDC 记录流，并执行**去重**逻辑，以便在其应用程序中显示最新的记录等效项。让我们在下面的部分检查这如何发生。
- en: '**Checking Hotel Prices in Downstream Consumer**'
  id: totrans-80
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**在下游消费者中检查酒店价格**'
- en: Now that we have the CDC stream pushed to a downstream consumer (a relational
    MySQL database in our case), let’s query a few records to see how the records
    are evolving. The CDC record stream from the Databricks notebook is being continuously
    pushed to the *hotelcdcprices* table. But this table holds all records including
    changes over time. Therefore, a view is created over the CDC table that ranks
    the change rows based on the timestamp.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经将 CDC 流推送到下游消费者（在我们的例子中是关系型 MySQL 数据库），让我们查询几个记录以查看记录如何演变。来自 Databricks
    笔记本的 CDC 记录流正在不断推送到*hotelcdcprices*表。但该表包含所有记录，包括随时间变化的记录。因此，创建了一个视图来根据时间戳对变更行进行排序。
- en: '![](../Images/b3c8e8dcabb9193db5900216db2c811e.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b3c8e8dcabb9193db5900216db2c811e.png)'
- en: Image by author
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片
- en: This view shows the equivalent of the latest prices for any hotel at any given
    time. This view can be used by the web application to display the latest prices
    on the portal.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 此视图显示了任何酒店在任何给定时间的最新价格等效项。此视图可供 Web 应用程序使用，以在门户上显示最新价格。
- en: '![](../Images/03263bad98ceb54d2171dbae6b580368.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/03263bad98ceb54d2171dbae6b580368.png)'
- en: Image by author
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片
- en: What are the typical use cases for Change Data Feed?
  id: totrans-87
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 变更数据馈送的典型用例是什么？
- en: 'Here are some common use cases that can benefit from using Delta tables as
    a sink for merging CDC data from varying sources and sending it downstream to
    consumers for their use:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是一些常见的用例，可以从使用 Delta 表作为合并来自不同来源的 CDC 数据并将其下传给消费者中获益：
- en: '**Read Change Data Feed and Merge to Silver Layer in a Streaming Fashion**'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '**以流式方式读取变更数据馈送并合并到银层**'
- en: Capture CDC from streaming data sources and merge micro-batches into the silver
    layer in a continuous fashion.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 从流数据源捕获 CDC 并将微批次连续合并到银层。
- en: '**Perform Aggregations in Gold Layer without Recomputing the Entire Batch**'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '**在黄金层中执行聚合而无需重新计算整个批次**'
- en: Using only the change data from the silver layer aggregate corresponding rows
    in the gold layer without recomputing the entire batch.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 仅使用银层中的变更数据，在黄金层中汇总相应的行，无需重新计算整个批次。
- en: '**Transparently Transmit Changes in Delta Tables to Downstream Consumers**'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '**透明地将 Delta 表中的变更传输给下游消费者**'
- en: Easily transmit changes to delta tables downstream to consumers such as relational
    databases and applications.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 轻松将变更传输到下游消费者，如关系型数据库和应用程序。
- en: To conclude, using the change data feed feature in Delta tables you can not
    only make the process of CDC data collection and merging easier, but extend its
    usage to **transmit** change data downstream to relational databases, No-SQL databases,
    and other applications. These downstream applications can effectively use this
    CDC data for any purpose deemed necessary.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，使用 Delta 表中的变更数据馈送功能，不仅可以简化 CDC 数据的收集和合并过程，还可以将变更数据下传到关系型数据库、No-SQL 数据库以及其他应用程序。这些下游应用可以有效地利用这些
    CDC 数据来满足任何必要的目的。
- en: I hope this article was helpful. Delta Lake and Change Data Feed is covered
    as part of the AWS Big Data Analytics course offered by [Datafence Cloud Academy](http://www.datafence.com).
    The course is taught online by myself on weekends.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 我希望这篇文章对你有所帮助。Delta Lake 和 Change Data Feed 是由 [Datafence Cloud Academy](http://www.datafence.com)
    提供的 AWS 大数据分析课程的一部分。课程由我本人在周末在线授课。
