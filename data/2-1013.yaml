- en: 'Gradient Descent vs. Gradient Boosting: A Side-by-Side Comparison'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 梯度下降与梯度提升：逐一对比
- en: 原文：[https://towardsdatascience.com/gradient-descent-vs-gradient-boosting-a-side-by-side-comparison-7067bb3c5712](https://towardsdatascience.com/gradient-descent-vs-gradient-boosting-a-side-by-side-comparison-7067bb3c5712)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/gradient-descent-vs-gradient-boosting-a-side-by-side-comparison-7067bb3c5712](https://towardsdatascience.com/gradient-descent-vs-gradient-boosting-a-side-by-side-comparison-7067bb3c5712)
- en: From Initialization to Convergence in simple English
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从初始化到收敛的简单英语
- en: '[](https://medium.com/@angela.shi?source=post_page-----7067bb3c5712--------------------------------)[![Angela
    and Kezhan Shi](../Images/a89d678f2f3887c0c2ff3928f9d767b4.png)](https://medium.com/@angela.shi?source=post_page-----7067bb3c5712--------------------------------)[](https://towardsdatascience.com/?source=post_page-----7067bb3c5712--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----7067bb3c5712--------------------------------)
    [Angela and Kezhan Shi](https://medium.com/@angela.shi?source=post_page-----7067bb3c5712--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@angela.shi?source=post_page-----7067bb3c5712--------------------------------)[![Angela
    and Kezhan Shi](../Images/a89d678f2f3887c0c2ff3928f9d767b4.png)](https://medium.com/@angela.shi?source=post_page-----7067bb3c5712--------------------------------)[](https://towardsdatascience.com/?source=post_page-----7067bb3c5712--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----7067bb3c5712--------------------------------)
    [Angela and Kezhan Shi](https://medium.com/@angela.shi?source=post_page-----7067bb3c5712--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----7067bb3c5712--------------------------------)
    ·5 min read·Feb 28, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page-----7067bb3c5712--------------------------------)
    ·阅读时长5分钟·2023年2月28日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: Introduction
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 引言
- en: Gradient descent and gradient boosting are two popular machine learning algorithms.
    Despite their different approaches and applications, both gradient descent and
    gradient boosting algorithms are founded on gradient calculations and share several
    common steps. The main aim of this article is to provide a detailed comparison
    of these two algorithms to help readers gain a better understanding of their similarities
    and differences.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度下降和梯度提升是两种流行的机器学习算法。尽管它们的处理方法和应用不同，但这两种算法都基于梯度计算，并且共享若干相似步骤。本文的主要目的是详细比较这两种算法，帮助读者更好地理解它们的相似之处和不同之处。
- en: '![](../Images/8c776904768c4cbd3516306acf0a29c3.png)'
  id: totrans-8
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8c776904768c4cbd3516306acf0a29c3.png)'
- en: Photo by [Gregoire Jeanneau](https://unsplash.com/es/@gregjeanneau?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由[Gregoire Jeanneau](https://unsplash.com/es/@gregjeanneau?utm_source=medium&utm_medium=referral)提供，来源于[Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: Gradient Descent
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 梯度下降
- en: Gradient descent is a common optimization algorithm used in machine learning
    to minimize a cost function. The goal is to find the best set of parameters that
    minimize the error between the predicted and actual values. The process starts
    by randomly initializing the weights or coefficients of the model. Then, it iteratively
    updates the weights in the direction of the steepest descent of the cost function
    by calculating the gradient of the cost function with respect to each parameter.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度下降是机器学习中常用的优化算法，用于最小化成本函数。目标是找到一组最佳参数，以最小化预测值与实际值之间的误差。该过程开始时随机初始化模型的权重或系数。然后，它通过计算成本函数相对于每个参数的梯度，迭代地更新权重，沿着成本函数的最大下降方向进行。
- en: Gradient Boosting
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 梯度提升
- en: Gradient boosting is an ensemble method that combines multiple weak models to
    create a stronger predictive model. It works by iteratively fitting a new model
    to the residual errors of the previous model. The final prediction is the sum
    of the predictions of all the models. In gradient boosting, the focus is on the
    errors made by the previous models.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度提升是一种集成方法，通过结合多个弱模型来创建一个更强的预测模型。它通过迭代地将新模型拟合到前一个模型的残差错误来进行工作。最终的预测是所有模型预测值的总和。在梯度提升中，重点是前一个模型所犯的错误。
- en: Different Yet Similar
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 不同却相似
- en: To provide a comprehensive comparison of gradient descent and gradient boosting,
    we will first explain the algorithms separately and then provide a step-by-step
    comparison of each algorithm’s approach to optimization. This approach will help
    readers gain a better understanding of the similarities and differences between
    the two algorithms.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 为了提供梯度下降和梯度提升的全面比较，我们将首先分别解释这两种算法，然后逐步比较每种算法的优化方法。这种方法将帮助读者更好地理解这两种算法的相似性和差异。
- en: Gradient descent algorithm in simple English
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 梯度下降算法的简单英语解释
- en: 'Here are a few steps that explain gradient descent in simple English:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一些用简单英语解释梯度下降的步骤：
- en: 'Choose a starting point: Gradient descent starts with a random or predefined
    initial set of weights or coefficients for the model.'
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择起点：梯度下降从随机或预定义的模型权重或系数开始。
- en: 'Calculate the gradient: The gradient is the direction of the steepest ascent
    or descent of a function. In gradient descent, we calculate the gradient of the
    cost function with respect to each parameter. The cost function measures how well
    the model fits the training data.'
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算梯度：梯度是函数的最陡上升或下降方向。在梯度下降中，我们计算成本函数相对于每个参数的梯度。成本函数衡量模型对训练数据的拟合程度。
- en: 'Update the weights: Once we have the gradient, we update the weights of the
    model in the opposite direction of the gradient. The size of the update is determined
    by a learning rate, which controls how much the weights are adjusted in each iteration.'
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更新权重：一旦我们获得梯度，就更新模型的权重，方向与梯度相反。更新的大小由学习率决定，学习率控制每次迭代中权重的调整幅度。
- en: 'Repeat until convergence: We repeat steps 2 and 3 until we reach the minimum
    of the cost function, which corresponds to the best set of weights for the model.
    The convergence criteria may vary, such as reaching a certain number of iterations
    or when the change in the cost function becomes small enough.'
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 直到收敛：我们重复步骤2和3，直到达到成本函数的最小值，这对应于模型的最佳权重集。收敛标准可能有所不同，例如达到一定的迭代次数或当成本函数的变化变得足够小时。
- en: By iteratively adjusting the weights in the direction of the steepest descent
    of the cost function, gradient descent aims to find the best set of parameters
    that minimize the error between the predicted and actual values.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 通过迭代地调整权重，朝着成本函数的最陡下降方向，梯度下降旨在找到最佳的参数集，以最小化预测值和实际值之间的误差。
- en: Gradient boosting algorithm in simple English
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 梯度提升算法的简单英语解释
- en: 'Here are a few steps that explain gradient boosting in simple English:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一些用简单英语解释梯度提升的步骤：
- en: 'Train a weak model: We start by training a weak model, such as a decision tree
    or a regression model, on the training data. The weak model may not perform well
    on its own but can make some predictions.'
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练一个弱模型：我们从训练一个弱模型开始，比如决策树或回归模型，使用训练数据。弱模型可能单独表现不佳，但可以进行一些预测。
- en: 'Calculate the error: We calculate the error between the predicted and actual
    values of the weak model. This error becomes the target for the next model.'
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算误差：我们计算弱模型的预测值和实际值之间的误差。这个误差成为下一个模型的目标。
- en: 'Train a new model: We train a new model to predict the error made by the previous
    model. This new model is fitted on the residuals or errors of the previous model.'
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练新模型：我们训练一个新模型来预测前一个模型的错误。这个新模型在前一个模型的残差或错误上进行拟合。
- en: 'Combine the models: We combine the predictions of all the models to make the
    final prediction. The final prediction is the sum of the predictions of all the
    models.'
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 合并模型：我们将所有模型的预测结果合并以进行最终预测。最终预测是所有模型预测结果的总和。
- en: 'Repeat until convergence: We repeat steps 2 to 4, adding new models to the
    ensemble, until we reach a predefined number of models or until the performance
    on a validation set stops improving.'
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 直到收敛：我们重复步骤2到4，向集成中添加新模型，直到达到预定义的模型数量或在验证集上的性能停止提升。
- en: By iteratively fitting new models to the residual errors of the previous model,
    gradient boosting aims to improve the accuracy of the model. The final prediction
    is a combination of the predictions of all the models, each correcting the errors
    of the previous model. Gradient boosting can handle non-linear relationships,
    missing values, and outliers effectively.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 通过迭代地将新模型拟合到前一个模型的残差中，梯度提升旨在提高模型的准确性。最终预测是所有模型预测的组合，每个模型都纠正前一个模型的错误。梯度提升可以有效地处理非线性关系、缺失值和异常值。
- en: Side-by-side comparison
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 并排比较
- en: 'Here is a side-by-side comparison of gradient descent and gradient boosting
    for each step:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是梯度下降和梯度提升每一步的并排比较：
- en: '1\. Initialization:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 1\. 初始化：
- en: 'Gradient Descent: Random or predefined initialization of the weights or coefficients
    of the model.'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 梯度下降：随机或预定义初始化模型的权重或系数。
- en: 'Gradient Boosting: Training of a weak model, such as a decision tree or a regression
    model, on the training data.'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 梯度提升：在训练数据上训练一个弱模型，例如决策树或回归模型。
- en: '2\. Calculation of Error:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 2\. 错误计算：
- en: 'Gradient Descent: Calculation of the error or loss between the predicted and
    actual values of the model on the entire training set.'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 梯度下降：计算模型在整个训练集上的预测值与实际值之间的误差或损失。
- en: 'Gradient Boosting: Calculation of the error or residuals between the predicted
    and actual values of the weak model on the training set.'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 梯度提升：计算弱模型在训练集上的预测值与实际值之间的误差或残差。
- en: '3\. Update or Fitting:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 3\. 更新或拟合：
- en: 'Gradient Descent: Update the weights of the model in the opposite direction
    of the gradient, based on the learning rate and gradient of the cost function.'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 梯度下降：根据学习率和成本函数的梯度，沿着梯度的反方向更新模型的权重。
- en: 'Gradient Boosting: Fit a new model to predict the residual errors of the previous
    model, based on the error of the weak model and the training data.'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 梯度提升：拟合一个新模型以预测前一个模型的残差错误，基于弱模型的错误和训练数据。
- en: '4\. Combination:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 4\. 组合：
- en: 'Gradient Descent: No combination is needed as the goal is to optimize the parameters
    of a single model.'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 梯度下降：不需要组合，因为目标是优化单个模型的参数。
- en: 'Gradient Boosting: Combine the predictions of all the models to make the final
    prediction. The final prediction is the sum of the predictions of all the models.'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 梯度提升：结合所有模型的预测结果以做出最终预测。最终预测是所有模型预测结果的总和。
- en: '5\. Convergence:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 5\. 收敛：
- en: 'Gradient Descent: Repeat steps 2 to 4 until convergence is reached, which may
    vary depending on the criteria such as the number of iterations or the change
    in the cost function.'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 梯度下降：重复步骤2到4，直到达到收敛，这可能取决于如迭代次数或成本函数的变化等标准。
- en: 'Gradient Boosting: Repeat steps 2 to 4, adding new models to the ensemble until
    a predefined number of models or until the performance on a validation set stops
    improving.'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 梯度提升：重复步骤2到4，向集成中添加新模型，直到达到预定数量的模型或在验证集上的性能停止改善。
- en: Conclusion
  id: totrans-48
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: Both gradient descent and gradient boosting rely on gradient calculations to
    optimize models, but they differ in their approach and purpose. Gradient descent
    focuses on minimizing the cost function of a single model, while gradient boosting
    aims to improve the accuracy of an ensemble of models.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度下降和梯度提升都依赖梯度计算来优化模型，但它们在方法和目的上有所不同。梯度下降专注于最小化单一模型的成本函数，而梯度提升则旨在提高模型集成的准确性。
- en: While gradient descent and gradient boosting have different optimization goals,
    they share a common algorithmic foundation based on gradient descent. In gradient
    descent, the algorithm optimizes the parameters of a single model to minimize
    a cost function. In contrast, gradient boosting aims to optimize an ensemble of
    models by iteratively adding new models to minimize the cost function of the ensemble.
    However, both algorithms use gradient descent as the fundamental optimization
    technique.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管梯度下降和梯度提升具有不同的优化目标，但它们共享基于梯度下降的共同算法基础。在梯度下降中，算法优化单个模型的参数以最小化成本函数。相比之下，梯度提升旨在通过迭代添加新模型来优化模型集成，以最小化集成的成本函数。然而，这两种算法都使用梯度下降作为基本优化技术。
