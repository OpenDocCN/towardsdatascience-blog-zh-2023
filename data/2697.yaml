- en: Dive Into LoRA Adapters
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深入了解LoRA适配器
- en: 原文：[https://towardsdatascience.com/dive-into-lora-adapters-38f4da488ede?source=collection_archive---------0-----------------------#2023-08-25](https://towardsdatascience.com/dive-into-lora-adapters-38f4da488ede?source=collection_archive---------0-----------------------#2023-08-25)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/dive-into-lora-adapters-38f4da488ede?source=collection_archive---------0-----------------------#2023-08-25](https://towardsdatascience.com/dive-into-lora-adapters-38f4da488ede?source=collection_archive---------0-----------------------#2023-08-25)
- en: 'Exploring Parameter Efficient Finetuning (PEFT): Intuitively Understanding
    Finetuning Using LoRA'
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 探索参数高效微调（PEFT）：直观理解使用LoRA的微调
- en: '[](https://medium.com/@mkamp?source=post_page-----38f4da488ede--------------------------------)[![Mariano
    Kamp](../Images/d58d3321564409fba27c7c644fe5d813.png)](https://medium.com/@mkamp?source=post_page-----38f4da488ede--------------------------------)[](https://towardsdatascience.com/?source=post_page-----38f4da488ede--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----38f4da488ede--------------------------------)
    [Mariano Kamp](https://medium.com/@mkamp?source=post_page-----38f4da488ede--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@mkamp?source=post_page-----38f4da488ede--------------------------------)[![Mariano
    Kamp](../Images/d58d3321564409fba27c7c644fe5d813.png)](https://medium.com/@mkamp?source=post_page-----38f4da488ede--------------------------------)[](https://towardsdatascience.com/?source=post_page-----38f4da488ede--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----38f4da488ede--------------------------------)
    [Mariano Kamp](https://medium.com/@mkamp?source=post_page-----38f4da488ede--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F1ed8ca6eb79f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdive-into-lora-adapters-38f4da488ede&user=Mariano+Kamp&userId=1ed8ca6eb79f&source=post_page-1ed8ca6eb79f----38f4da488ede---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----38f4da488ede--------------------------------)
    ·14 min read·Aug 25, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F38f4da488ede&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdive-into-lora-adapters-38f4da488ede&user=Mariano+Kamp&userId=1ed8ca6eb79f&source=-----38f4da488ede---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F1ed8ca6eb79f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdive-into-lora-adapters-38f4da488ede&user=Mariano+Kamp&userId=1ed8ca6eb79f&source=post_page-1ed8ca6eb79f----38f4da488ede---------------------post_header-----------)
    发布于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----38f4da488ede--------------------------------)
    · 14分钟阅读 · 2023年8月25日 [](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F38f4da488ede&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdive-into-lora-adapters-38f4da488ede&user=Mariano+Kamp&userId=1ed8ca6eb79f&source=-----38f4da488ede---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F38f4da488ede&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdive-into-lora-adapters-38f4da488ede&source=-----38f4da488ede---------------------bookmark_footer-----------)![](../Images/7ff47e06d3e0b926b8f949e6efa3e6e8.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F38f4da488ede&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdive-into-lora-adapters-38f4da488ede&source=-----38f4da488ede---------------------bookmark_footer-----------)![](../Images/7ff47e06d3e0b926b8f949e6efa3e6e8.png)'
- en: Large Language Models (LLMs) have taken the world by storm. Over the last year
    we have witnessed a massive leap in what they can do, going from quite narrow
    and restricted applications to now engaging in fluent, multi-turn conversations.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）已经在全球掀起了风暴。在过去的一年里，我们见证了它们的巨大进步，从非常狭窄和受限的应用，到现在能够进行流畅的多轮对话。
- en: Isn’t it amazing how these models have shifted from extractive summarization—copying
    the source verbatim—to now providing abstractive summarizations? They are now
    completely re-writing the summary to match the reader’s style preference and the
    reader’s existing knowledge. What’s even more astonishing is that these new models
    can not only generate new code, but explain your existing code. Fascinating.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 难道这些模型从提取性摘要——逐字复制源文本——到现在提供抽象性摘要的转变不令人惊叹吗？它们现在完全重写摘要，以匹配读者的风格偏好和现有知识。更令人惊奇的是，这些新模型不仅能够生成新代码，还能解释你现有的代码。真是令人着迷。
- en: Frequently these large models are so powerful that they even yield impressive
    results when queried in a **zero-shot** or **few-shot** manner. Although this
    allows for rapid experimentation and seeing results immediately, for many tasks
    this is often followed by **finetuning** a model to achieve the best performance
    and efficiency. However, **finetuning every single one of their billions of parameters**
    **becomes impractical** inefficient. Moreover, given the size of the models, do
    we even have enough labeled data to train such a massive model without overfitting?
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 这些大型模型通常非常强大，即使在**零样本**或**少样本**的情况下查询，也能产生令人印象深刻的结果。尽管这允许快速实验并立即看到结果，但对于许多任务，通常之后会**微调**模型以实现最佳性能和效率。然而，**微调每一个参数**变得**不切实际**且效率低下。此外，考虑到模型的规模，我们是否有足够的标注数据来训练如此庞大的模型而不导致过拟合？
- en: '**Parameter Efficient Finetuning (PEFT)** to the rescue: You can now achieve
    great performance while **only tuning a small fraction of the weights**. Not having
    to tune billions of parameters across multiple machines, makes the whole process
    of finetuning more practical and economically viable again. Using PEFT and quantization
    allows large models with billions of parameters to be finetuned on a single GPU.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '**参数高效微调（PEFT）**来拯救你：你现在可以在**仅调整少量权重**的情况下实现出色的性能。不必在多个机器上调整数十亿个参数，使得微调过程变得更加实际和经济可行。使用PEFT和量化可以让具有数十亿参数的大型模型在单个GPU上进行微调。'
- en: 'This mini-series is for experienced ML practitioners who want to explore PEFT
    and specifically LoRA [2]:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 这个迷你系列适合那些希望探索PEFT和具体LoRA的经验丰富的机器学习从业者[2]：
- en: In **Article One** we explore the motivation for parameter efficient finetuning
    (PEFT). We review why and how finetuning works, what aspects of our existing practices
    can be retained, generalized and applied in a refined fashion. We’ll get **hands-on**
    and implement the essentials from scratch to create an **intuitive understanding**
    and to illustrate the simplicity of the method we chose to explore, LoRA.
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在**第一篇文章**中，我们探讨了参数高效微调（PEFT）的动机。我们回顾了微调的原理及其作用，以及我们现有实践中可以保留、概括并以改进的方式应用的方面。我们将**亲自动手**，从零开始实现必要的内容，以创造一个**直观的理解**并展示我们选择探索的方法LoRA的简单性。
- en: In [**Article Two**](/a-winding-road-to-parameter-efficiency-12448e64524d)we
    now dive into finding good hyperparameter values, i.e. we review the relevant
    design decisions when applying LoRA. Along the way we establish baselines for
    performance comparisons and then review the components that we can adapt using
    LoRA, what their impact is and how we can appropriately size them.
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在[**第二篇文章**](/a-winding-road-to-parameter-efficiency-12448e64524d)中，我们现在深入寻找合适的超参数值，即我们回顾应用LoRA时相关的设计决策。在此过程中，我们建立了性能比较的基线，然后回顾可以使用LoRA调整的组件、它们的影响以及如何适当调整它们。
- en: Based on a trained and tuned model for a single task, in **Article Three** we
    now extend our view to tuning multiple tasks. Also, what about deployment? How
    can we use the relative small footprint of the adapters we trained for a single
    task and implement a hot-swapping mechanism to use a single model endpoint to
    do inference for multiple tasks.
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于对单一任务进行训练和调整的模型，在**第三篇文章**中，我们将视角扩展到多个任务的调优。此外，部署方面呢？我们如何利用为单一任务训练的适配器相对小的占用空间，并实现热插拔机制，以便使用单一模型端点进行多个任务的推理。
- en: Over the course of the first three articles, we have developed an intuitive
    grasp of training, tuning and deploying using PEFT. Transitioning into **Article
    Four** we’ll become very practical. We’ll step away from our educational model,
    asking “What have we learned so far and how do we apply this to a real world scenario?”
    We then use the established implementation by Hugging Face to meet our goals.
    This will include using QLoRA, that marries LoRA and quantization for efficient
    GPU memory usage.
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在前三篇文章中，我们对使用PEFT进行训练、调优和部署有了直观的把握。过渡到**第四篇文章**时，我们将变得非常实际。我们将离开我们的教育模型，问道“到目前为止我们学到了什么，如何将其应用于实际场景？”然后使用Hugging
    Face提供的实现来实现我们的目标。这将包括使用QLoRA，它结合了LoRA和量化，以实现高效的GPU内存使用。
- en: Ready to dive in? For today, let’s start with why this all works.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 准备好深入了解了吗？今天，让我们从为什么这些方法有效开始。
- en: On The Effectiveness Of Pre-Training and Finetuning
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 关于预训练和微调的有效性
- en: In their work Aghajanyan et al [1] showed two interesting observations about
    how the neural network layers change during pre-training, making it easier to
    do finetuning. This is broadly applicable, not just for a specific finetuning
    task.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在他们的研究中，Aghajanyan等人[1]展示了神经网络层在预训练期间如何变化的两个有趣观察，使得微调更容易。这是广泛适用的，而不仅仅是针对特定的微调任务。
- en: 'Specifically they show that pre-training minimizes the intrinsic dimensionality
    (ID) of the representations. The following two figures — taken from their work
    — illustrate the effect:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 他们具体展示了预训练如何最小化表示的内在维度（ID）。以下两个图——取自他们的工作——说明了这一效果：
- en: '![](../Images/0db30d959f0e4397ebc0f3a3e01f4154.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0db30d959f0e4397ebc0f3a3e01f4154.png)'
- en: Intrinsic Dimensionality decreases over the duration of the pre-training (image
    by Aghajanyan et al.)
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 内在维度在预训练期间逐渐减少（图像由Aghajanyan等人提供）
- en: '![](../Images/5761bc5ad66fc6757fc596b2c53efe85.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5761bc5ad66fc6757fc596b2c53efe85.png)'
- en: Intrinsic Dimensionality decreases with an increase in model capacity (image
    by Aghajanyan et al.)
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 内在维度随着模型容量的增加而减少（图像由Aghajanyan等人提供）
- en: Rather than finetuning all parameters, the authors trained the respective models
    with a smaller, randomly selected, subset of parameters. The number of parameters
    was chosen to match 90% of the performance of full finetuning. This dimensionality,
    required to achieve 90% performance, is denoted as `d90` on the two y-axes in
    the figure above.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 作者没有对所有参数进行微调，而是用一个较小的、随机选择的参数子集来训练相应的模型。参数数量被选择以匹配完整微调的90%性能。这个实现90%性能所需的维度在上面的图中用两个y轴表示为`d90`。
- en: The first figure shows that with an **increase in pre-training duration**, on
    the x-axis, `d90` goes down, i.e. the number of parameters needed in the following
    finetuning to achieve 90% of the full finetuning performance decreases. Which
    in itself shows the effectiveness of pre-training as a way to **compress knowledge**.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 第一张图显示，随着**预训练时长的增加**（x轴），`d90`下降，即在随后的微调中实现90%完整微调性能所需的参数数量减少。这本身就显示了预训练作为一种**压缩知识**的方法的有效性。
- en: In the second figure we can also see that with **increased capacity** the number
    of parameters needed to achieve `d90` in the finetuned model goes down as well.
    How interesting. It indicates that a larger model can learn a better representation
    of the training data—the world the model sees—and creates hierarchical features
    that are easy to use in **any** downstream task.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在第二张图中，我们还可以看到，随着**容量的增加**，达到`d90`所需的参数数量也下降了。这很有趣。这表明，更大的模型可以学习训练数据的更好表示——模型所见的世界——并创建在**任何**下游任务中易于使用的层次特征。
- en: One specific example the authors point out is that `d90` for RoBERTa Large (354M)
    is about 207 parameters. Bam!
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 作者指出的一个具体例子是，RoBERTa Large（354M）的`d90`大约是207个参数。太棒了！
- en: Please find that example in the diagram above and then also check that the **smaller**
    RoBERTa Base (123M) needs **more** parameters to achieve 90% performance, here
    896\. Interesting.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 请在上图中找到这个例子，然后也查看一下**较小的**RoBERTa Base（123M）需要**更多**的参数才能达到90%的性能，这里是896。很有趣。
- en: 'From discussions I had on this topic I learned that there are a few things
    worth pointing out explicitly:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 从我在这个话题上的讨论中，我了解到有几点值得明确指出：
- en: We leverage the effect of ID during finetuning, but the graphs and numbers above
    are all about the pre-training. We just use the finetuning numbers to make the
    resulting downstream impact more tangible.
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们在微调过程中利用了ID的效果，但上面的图表和数字都是关于预训练的。我们只是使用微调的数据来使最终的下游影响更具可感知性。
- en: Using a larger model not only results in a lower ID **relative** to their size,
    but also **absolutely**. We will see a similar effect when moving to PEFT.
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用更大的模型不仅相对其大小具有更低的ID，而且**绝对**也是如此。当我们转向PEFT时，会看到类似的效果。
- en: In [1] you will find the above illustrations as figure 2, figure 3 and the cited
    results are taken from table 1.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在[1]中，你会找到上述图示作为图2、图3，并且引用的结果取自表1。
- en: In conclusion, we can see that the learned representations during pre-training
    compress the knowledge that the model learned and make it easier to finetune a
    downstream model using these more semantic representations. We’ll built on top
    of that with PEFT. Only, instead of selecting the parameters to tune randomly
    and to aim for 90% performance, we will use a more directed approach to select
    which parameters to train and aim to almost match the performance of the full
    finetuning. Exciting!
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，我们可以看到，在预训练过程中学到的表示压缩了模型学习的知识，使得使用这些更具语义的表示来微调下游模型变得更加容易。我们将在此基础上使用PEFT。只不过，我们不会随机选择要调整的参数并目标为90%的性能，而是使用更有针对性的方法来选择要训练的参数，并力求几乎匹配全微调的性能。令人兴奋！
- en: What to Tune?
  id: totrans-35
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么需要调整？
- en: We have established that we can work with a very small number of parameters.
    But which ones? Where in the model?
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经确定可以使用非常少量的参数。但是哪一些？在模型的哪个位置？
- en: 'We’ll go into much more details in the next article. But to get our thinking
    started and to frame our problem, let’s reflect on two general approaches for
    now:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在下一篇文章中深入讨论更多细节。但为了开始我们的思考并框定问题，让我们现在反思两种一般方法：
- en: '![](../Images/e25a07dbccdde5316c7573a6b1923001.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e25a07dbccdde5316c7573a6b1923001.png)'
- en: 'Allocation based on **task**: Tuning which parameters is most impactful based
    on our understanding of the task?'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 基于**任务**的分配：基于对任务的理解，调整哪些参数最具影响力？
- en: '**Based on the task:** When using finetuning, we want to retain the knowledge
    from the pre-training and to avoid “[catastrophic forgetting](https://en.wikipedia.org/wiki/Catastrophic_interference)“.
    We recognize that the downstream task-specific learnings should happen in the
    task head, here the classifier, of the finetuned model and in the immediate layers
    below the head (depicted in green), while in the lower layers and embeddings we
    want to retain the general knowledge we learned about the use of language (in
    red). Frequently we guide the model then with per-layer learning rates, or even
    completely freezing the lower layers.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '**基于任务：** 使用微调时，我们希望保留预训练中的知识，并避免“[灾难性遗忘](https://en.wikipedia.org/wiki/Catastrophic_interference)”。我们认识到，下游任务特定的学习应发生在微调模型的任务头部（这里是分类器）及其下方的直接层（如图中绿色所示），而在较低层和嵌入中我们希望保留我们关于语言使用的一般知识（如图中红色所示）。通常我们通过每层学习率来引导模型，或完全冻结下层。'
- en: This is all based on our understanding on **where** we expect the model to learn
    the essential knowledge for our downstream task, and **where** existing knowledge
    from the pre-training should be retained.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 这都基于我们对**模型学习**下游任务所需的关键知识的位置的理解，以及**预训练**中现有知识应保留的位置。
- en: '![](../Images/c44955b30610f89d94e3978ddd89335f.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c44955b30610f89d94e3978ddd89335f.png)'
- en: 'Allocation based on **architectural** elements: Which parameters are best placed,
    most powerful for finetuning?'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 基于**架构**元素的分配：哪些参数在微调中最有效、最有力？
- en: '**Based on the architecture:** In contrast, we can also review the components
    of our architecture, their parameters and their possible impact. In the illustration
    above you see for example **LayerNorm** and **Biases**, which are of low capacity,
    but spread out all over the model. These are in central positions to impact the
    model, but have relatively few parameters.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '**基于架构：** 相对而言，我们也可以审视我们架构的组件、它们的参数及其可能的影响。在上面的插图中，例如可以看到**LayerNorm**和**Biases**，这些容量较小，但遍布整个模型。这些位于中央位置以影响模型，但参数相对较少。'
- en: On the other hand, we have the parameters from the **embeddings**. These are
    not close to the task, but close to the inputs however. And we have a lot of parameters
    in the embeddings. So if we want to be efficient they would not be our first choice
    for any kind of finetuning, including PEFT.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，我们还有**嵌入**的参数。这些参数虽然离任务较远，但靠近输入。而且嵌入中有大量参数。因此，如果我们想要高效，这些参数不会是我们进行任何形式的微调，包括PEFT的首选。
- en: And last, not least, we have the large linear modules that come with the transformers
    architecture, namely the **attention vectors** and the **feed forward layers**.
    These have a large number of parameters and we can decide on which layer to adapt
    them.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 最后但同样重要的是，我们还有与transformers架构一起出现的大型线性模块，即**attention vectors**和**feed forward
    layers**。这些模块具有大量参数，我们可以决定在哪一层调整它们。
- en: We’ll revisit selecting the right parameters in the next article in more detail.
    For this article, no matter how we want to slice and dice the problem, we will
    end up with groups of parameters that we want to tune. For the rest of this article
    it will be some linear modules.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在下一篇文章中更详细地回顾选择正确参数的过程。在本文中，无论我们如何切割和拆分问题，我们最终都会得到一组我们想要调整的参数。本文的其余部分将涉及一些线性模块。
- en: Using Adapters to Become More Efficient
  id: totrans-48
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用适配器提高效率
- en: Instead of tuning a whole linear module with all of its parameters we want to
    be more efficient. The approach we use is injecting adapters. These new modules
    are relatively small and will be placed after the module we want to adapt. The
    adapters can modify the output of the linear modules, i.e. they can refine the
    pre-trained outputs in a way that is beneficial to the downstream-task.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望更高效地调整线性模块，而不是调整所有参数。我们使用的方法是注入适配器。这些新模块相对较小，将放置在我们想要适配的模块之后。适配器可以修改线性模块的输出，即它们可以以有利于下游任务的方式细化预训练输出。
- en: '![](../Images/114fff28022f5f20e8f14430b66f0a8b.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/114fff28022f5f20e8f14430b66f0a8b.png)'
- en: Trainable adapters and frozen modules
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 可训练适配器和冻结模块
- en: But there is a problem with this approach. Can you spot it? It’s about the relative
    sizes of the module to be adapted and the adapter. If you look at the illustration
    below, you see the GPU memory. For efficiency we size our model so that it can
    **fit as tightly as possible into the available GPU memory**. This is particularly
    easy with the Transformer architecture due to each layer being of the same width,
    and even the down projected heads add up to the full width again. Hence we can
    pick a batch-size based on the uniform width of the Transformer’s components.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 但这种方法存在一个问题。你能发现吗？这与待适配模块和适配器的相对大小有关。如果你看下面的插图，你会看到GPU内存。为了提高效率，我们将模型大小调整到**尽可能紧密地适配可用的GPU内存**。由于每一层宽度相同，Transformer架构特别容易实现这种调整，即使是降维后的头部也会再次加起来达到整个宽度。因此，我们可以根据Transformer组件的统一宽度选择批次大小。
- en: But if we now inject very small adapters after larger linear layers, we have
    a problem. Our memory use becomes inefficient as can be seen in the illustration
    below.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 但如果我们现在在较大的线性层之后注入非常小的适配器，就会出现问题。正如下图所示，我们的内存使用变得低效。
- en: The batch-size fits the width of the Linear layer, but now we have a much smaller
    adapter. Therefore most of the GPU has to wait for the small adapter to be executed.
    This lowers the GPU utilization. And it’s worse than it looks in the illustration,
    keeping in mind that the area of the adapter in the diagram is supposed to be
    around 1%, while in the illustration it looks closer to 20%.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 批次大小适合线性层的宽度，但现在我们有一个更小的适配器。因此，大部分GPU必须等待小适配器执行。这降低了GPU的利用率。而且，这比插图中显示的情况更糟，考虑到插图中的适配器区域应该约为1%，而插图中看起来接近20%。
- en: '![](../Images/26332bbca0928aa57bc861180739c3a3.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/26332bbca0928aa57bc861180739c3a3.png)'
- en: Inefficient use of GPU
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: GPU利用效率低
- en: One way to deal with this is to parallelize the adaptation and just connect
    them with an addition, allowing both paths to contribute to the output. This way
    we don’t have a memory bottleneck anymore and can execute both the original linear
    module and the adapter in parallel, avoiding the gaps we saw before.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 解决这个问题的一种方法是并行化适配，并通过加法将它们连接起来，使两个路径都能贡献输出。这样，我们就没有了内存瓶颈，可以并行执行原始线性模块和适配器，避免了之前看到的间隙。
- en: '![](../Images/343ef4c27f22a382619945d0f4736aee.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/343ef4c27f22a382619945d0f4736aee.png)'
- en: Much better
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 更好
- en: But even the execution in parallel is an additional burden compared to having
    no adapter at all. This is true for training, but would even be true for inference.
    That’s not ideal.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 但即使并行执行也是一种额外的负担，与完全没有适配器相比。这对于训练来说是正确的，对于推理也是如此。这并不理想。
- en: Also, how big should such an adapter be?
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，这种适配器应该有多大？
- en: '![](../Images/1e8a2c58cea3b04859df80c5b4324ca5.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1e8a2c58cea3b04859df80c5b4324ca5.png)'
- en: What’s missing?
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 缺少什么？
- en: 'We will deal with the inefficiency during inference in the third article. Sneak
    peek: It’s going to be fine — we will merge the module’s weights with the product
    of the low rank matrices.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在第三篇文章中处理推理过程中的低效率问题。抢先看：一切都会好起来的——我们将把模块的权重与低秩矩阵的乘积合并。
- en: Back to this article — let’s tackle the adapter size.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 回到这篇文章——让我们解决适配器的大小问题。
- en: Low Ranking Matrices As Adapters
  id: totrans-66
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 低秩矩阵作为适配器
- en: Let’s zoom in.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们放大来看。
- en: Below, you see the original linear module in grey on the left and the adapter
    in orange on the right. To make them compatible, the inputs and outputs must match,
    so that we can call them in parallel with the same input and then add up the outputs,
    similar to using a residual connection. Hence the input and output dimensions
    on both sides must match.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 下面，你可以看到左侧灰色的原始线性模块和右侧橙色的适配器。为了使它们兼容，输入和输出必须匹配，以便我们可以使用相同的输入并行调用它们，然后将输出相加，类似于使用残差连接。因此，两个侧面的输入和输出维度必须匹配。
- en: '![](../Images/fc42e774c09626f35b3190062df1504f.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fc42e774c09626f35b3190062df1504f.png)'
- en: Adaptee vs Adapter, full rank each
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: Adaptee与Adapter，每个都是全秩的
- en: The linear module and the adapter translate to two matrices. And given their
    matching dimensions, **mechanically**, we have compatibility now. But as the adapter
    is as big as the the module we are adapting, we did not become more efficient.
    We need to have an adapter, that is **small and compatible**.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 线性模块和适配器转换为两个矩阵。由于它们的维度匹配，**机械地**，我们现在有了兼容性。但由于适配器的大小与我们正在调整的模块一样大，我们没有变得更高效。我们需要一个**小而兼容**的适配器。
- en: 'The product of two low rank matrices fits our requirements:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 两个低秩矩阵的乘积符合我们的要求：
- en: '![](../Images/c757a4fd74076016954bc5b0fb3ec2a4.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c757a4fd74076016954bc5b0fb3ec2a4.png)'
- en: Adapter is decomposed into two much lower rank matrices
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 适配器被分解为两个更低秩的矩阵
- en: The large matrix is decomposed in two low rank matrices. But the matrices themselves
    are much smaller, `d_in x r` and `r x d_out`, especially as `r` is much smaller
    as `d_in` and `d_out`. We typically look at numbers like 1, 2, 4, 16 for `r`,
    while `d_in` and `d_out` are like 768, 1024, 3072, 4096.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 大矩阵被分解为两个低秩矩阵。但这些矩阵本身要小得多，`d_in x r`和`r x d_out`，特别是`r`远小于`d_in`和`d_out`。我们通常会看到`r`的值如1、2、4、16，而`d_in`和`d_out`则如768、1024、3072、4096。
- en: 'Let’s put this all together:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们把这些全部结合起来：
- en: '![](../Images/bb0ce9840e7ca7a2f29d96c9b0f7a555.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bb0ce9840e7ca7a2f29d96c9b0f7a555.png)'
- en: Applying LoRA during the forward pass
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在前向传播过程中应用LoRA
- en: We can see that we have a single`x` as input. `x` is then multiplied with the
    original weights, `W0`. `W0` are the pre-trained weights. And `x` is multiplied
    with `A` and `B`, and eventually both results are added and form the adjusted
    output, here named `x'`.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到输入是一个单一的`x`。`x`随后与原始权重`W0`相乘。`W0`是预训练的权重。然后`x`与`A`和`B`相乘，最终将两个结果相加形成调整后的输出，这里称为`x'`。
- en: There are different adapter implementations, but in LoRA we make this an optimization
    problem and both low rank matrices `A` and `B` are learned for the specific downstream
    task. Learning those fewer parameters is then more efficient than learning all
    parameters in `W0`.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 存在不同的适配器实现，但在LoRA中，我们将其视为一个优化问题，并为特定的下游任务学习两个低秩矩阵`A`和`B`。学习这些较少的参数比学习`W0`中的所有参数更有效。
- en: Initialization
  id: totrans-81
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 初始化
- en: Let’s go on a quick tangent. How would you initialize `A` and `B?` If you initialize
    it randomly, consider what would happen in the beginning of the training?
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们快速转到一个相关的话题。你会如何初始化`A`和`B`？如果你随机初始化，考虑一下训练开始时会发生什么？
- en: In each forward pass we would add random noise to the output of an adapted module
    and we would have to wait for the optimizer to step-by-step correct the wrong
    initialization, leading to instabilities at the beginning of the finetuning.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在每次前向传播中，我们会向适配模块的输出添加随机噪声，我们将不得不等待优化器一步步修正错误的初始化，这会导致微调开始时的不稳定。
- en: 'To mitigate we typically use lower learning rates, smaller initialization values
    or warm up periods where we limit the effect that these wrong parameters can have,
    so that we do not destabilize the weights too much. In the LLAMA adapter [3] paper
    the authors introduce zero gating: They start the value of an adapter’s gate—to
    be multiplied with the actual weights—with 0 and increase its value over the course
    of the training.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 为了减轻问题，我们通常使用较低的学习率、更小的初始化值或加热期，以限制这些错误参数的影响，从而避免过度不稳定权重。在LLAMA适配器[3]的论文中，作者引入了零门控：他们将适配器的门的值（与实际权重相乘的值）初始化为0，并在训练过程中逐渐增加其值。
- en: An alternative approach would be to initialize `A`and `B` with 0\. But then
    you would not be able to break symmetry and in the learning process all parameters
    may be treated as one parameter.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 一种替代方法是将`A`和`B`初始化为0。但这样你将无法打破对称性，在学习过程中所有参数可能会被视为一个参数。
- en: What LoRA actually does is quite elegant. One matrix, `A`, is initialized randomly,
    while the other matrix, `B`, is initialized with 0\. Hence the product of the
    two matrices is 0, but still each parameter can be differentiated individually
    during backpropagation. Starting with 0 means that the inductive bias is to do
    nothing, unless changing the weights will lead to a loss reduction. So there will
    be no instabilities at the beginning of the training. Nice!
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: LoRA实际做的事情非常优雅。一个矩阵`A`是随机初始化的，而另一个矩阵`B`是用0初始化的。因此，两个矩阵的乘积为0，但每个参数在反向传播过程中仍然可以单独求导。从0开始意味着归纳偏差是不做任何事情，除非改变权重会导致损失减少。因此，在训练开始时不会有不稳定性。不错！
- en: '![](../Images/021d30bc91f37166f876e6efd001eb3d.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/021d30bc91f37166f876e6efd001eb3d.png)'
- en: Initialization of LoRA adapters — Do Nothing
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: LoRA适配器的初始化——什么都不做
- en: How Could That Look like in Code?
  id: totrans-89
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 代码中可能会是什么样子？
- en: Let’s check out some code excerpts for our small, illustrative example. You
    find the full code in the [accompanying notebook](https://github.com/marianokamp/peft_lora/blob/main/1_lora_from_scratch.ipynb)
    and a [more complete implementation](https://github.com/marianokamp/peft_lora/blob/fd46890711dc9edd6219018186d8d2211dda43ee/src/lora.py#L33C2-L33C2)
    that we use in the following articles is in the same repository.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们查看一些代码摘录，以了解我们的小示例。你可以在[随附的笔记本](https://github.com/marianokamp/peft_lora/blob/main/1_lora_from_scratch.ipynb)中找到完整的代码，而[更完整的实现](https://github.com/marianokamp/peft_lora/blob/fd46890711dc9edd6219018186d8d2211dda43ee/src/lora.py#L33C2-L33C2)则在同一个仓库中，供后续文章使用。
- en: Let’s start with how we setup an adapter. We pass in a reference to the module
    to be adapted, which we now call the `adaptee`. We store a reference to its original
    `forward` method and let the `adaptee`’s forward method now point to the adapter's
    `forward` method’s implementation.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先设置一个适配器。我们传入一个对要调整的模块的引用，现在我们称之为`adaptee`。我们存储了对其原始`forward`方法的引用，并让`adaptee`的`forward`方法现在指向适配器的`forward`方法的实现。
- en: '[PRE0]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Now that we have setup the mechanics of the integration, we also initialize
    the parameters of our low rank matrices. Recognize that we initialize one matrix
    with 0 and one randomly:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经设置好了集成的机制，我们还初始化了低秩矩阵的参数。注意，我们初始化了一个矩阵为0，另一个矩阵为随机值：
- en: '[PRE1]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: And finally, still part of the `LoRAAdapter` class, we have our `forward` method
    that first calls the `adaptee`’s `forward` method with our input `x`. That is
    the original path executed in the original module. But we then also add that result
    to that from our adapted branch, where we matrix multiply the input `x` with `A`
    and `B`.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，在`LoRAAdapter`类中，我们有一个`forward`方法，它首先用输入`x`调用`adaptee`的`forward`方法。这是原始模块中执行的原始路径。但我们还将这个结果与我们调整过的分支中的结果相加，在那里我们将输入`x`与`A`和`B`进行矩阵乘法。
- en: '[PRE2]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: This simplicity looks elegant to my eye.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 这种简洁对我来说看起来很优雅。
- en: 'There are more details that could be interesting, but are best explained alongside
    code. You find these in the [accompanying notebook](https://github.com/marianokamp/peft_lora/blob/main/1_lora_from_scratch.ipynb):'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 还有更多可能有趣的细节，但最好是和代码一起解释。你可以在[随附的笔记本](https://github.com/marianokamp/peft_lora/blob/main/1_lora_from_scratch.ipynb)中找到这些：
- en: How to first freeze the whole model
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何首先冻结整个模型
- en: How to then unfreeze the classifier. As it is specific to our downstream task
    and we completely train it.
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何解冻分类器。因为它是特定于我们的下游任务的，我们需要对其进行完全训练。
- en: How to add adapters; which are all active, not frozen.
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何添加适配器；这些适配器都是活跃的，未被冻结。
- en: Reviewing how the dimensions of the module’s matrix relate to the two lower
    rank matrices `A` and `B`.
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 审查模块矩阵的维度如何与两个低秩矩阵`A`和`B`相关。
- en: How much smaller is the number of parameters when using a small value for `r`?
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当使用小的`r`值时，参数数量会减少多少？
- en: 'A small excerpt below shows how the parameters of the original module `output.dense`
    are not trained (marked with a `0` ), but its LoRA matrices are trainable (marked
    with a `1`) and, of course, the overall classifier of the model (also marked as
    trainable with a `1`):'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的小摘录展示了原始模块`output.dense`的参数没有被训练（标记为`0`），但其LoRA矩阵是可训练的（标记为`1`），当然，模型的整体分类器（也标记为可训练的`1`）：
- en: '[PRE3]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Check out the [notebook](https://github.com/marianokamp/peft_lora/blob/main/1_lora_from_scratch.ipynb)
    for more.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 查看更多内容，请查看[笔记本](https://github.com/marianokamp/peft_lora/blob/main/1_lora_from_scratch.ipynb)。
- en: Take It for a Spin?
  id: totrans-107
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 来试试吧？
- en: Further, you will see some tests in the [notebook](https://github.com/marianokamp/peft_lora/blob/main/1_lora_from_scratch.ipynb)
    that show that the whole setup works mechanically.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，你将看到一些在[笔记本](https://github.com/marianokamp/peft_lora/blob/main/1_lora_from_scratch.ipynb)中进行的测试，这些测试显示整个设置在机械层面上是有效的。
- en: But then we run our first experiment and submit the Training Jobs to SageMaker.
    We do a full finetuning on the original model and then a training with LoRA enabled
    as described here.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们进行第一次实验并提交训练作业到SageMaker。我们对原始模型进行完整微调，然后如这里所述启用LoRA进行训练。
- en: For our test, we train RoBERTa Large [4] on the [sst-2 dataset](https://huggingface.co/datasets/sst2)
    [5] with `r`=2 adapting the `query` and `output` parameters on all layers. We
    use `5e-5` and `4e-4` as learning rates for the full-finetuning and the LoRA finetuning.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 对我们的测试，我们在[sst-2数据集](https://huggingface.co/datasets/sst2) [5] 上训练RoBERTa Large
    [4]，`r`=2，调整所有层的`query`和`output`参数。我们使用`5e-5`和`4e-4`作为全微调和LoRA微调的学习率。
- en: 'That’s the result (more in the [notebook](https://github.com/marianokamp/peft_lora/blob/main/1_lora_from_scratch.ipynb)):'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 这是结果（更多内容请见[笔记本](https://github.com/marianokamp/peft_lora/blob/main/1_lora_from_scratch.ipynb)）：
- en: '[PRE4]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: So that’s … great, not so great? What is it? First, it clearly shows that the
    whole setup works on a mechanical level — that’s great. And an accuracy over 90%
    shows that it is working well.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 所以这是……好，还是不好？是什么？首先，这清楚地表明整个设置在机械层面上是有效的——这很好。90%以上的准确度表明它工作得很好。
- en: But how well? What do we compare these numbers to? And how representative are
    these two individual training runs? Were we just lucky or unlucky? The LoRA numbers
    are better than the traditional approach? Isn’t that strange. How well did we
    tune the traditional approach?
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 但效果如何？我们将这些数字与什么进行比较？这两个单独训练运行的代表性如何？我们只是运气好还是不好？LoRA的结果比传统方法更好吗？这不是很奇怪吗？我们调优传统方法的效果如何？
- en: None of the above results are reliable. We don’t know if using our hyperparameters
    on a second run would produce similar results. Also, we used hyperparameters selected
    with a semi-educated guess.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 上述结果都不可靠。我们不知道在第二次运行时使用我们的超参数是否会产生类似的结果。此外，我们使用了通过半教育猜测选择的超参数。
- en: 'There is, of course, a better way. And so in the [next article](/a-winding-road-to-parameter-efficiency-12448e64524d)
    we will apply a more serious approach to selecting hyperparameters and will be
    evaluating the performance more systematically:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，还有更好的方法。在[下一篇文章](/a-winding-road-to-parameter-efficiency-12448e64524d)中，我们将采用更严谨的方法来选择超参数，并将更系统地评估性能。
- en: Establish baselines for comparisons
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 建立比较基准。
- en: Search good hyperparameters for both the baselines and the experiments
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 搜索基准和实验的良好超参数。
- en: 'Most importantly: Deepen our understanding of the LoRA method and the impact
    of design decisions, aligning our intuitions in a data-driven fashion'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最重要的是：加深我们对LoRA方法和设计决策影响的理解，使我们的直觉以数据驱动的方式对齐。
- en: Until then, I hope you had fun reading this article.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 在那之前，希望你阅读这篇文章时感到愉快。
- en: Thanks to [Constantin Gonzalez](https://www.linkedin.com/in/constantingonzalez/),
    [Ümit Yoldas](https://www.linkedin.com/in/%C3%BCmit-yoldas-23a908177/), [Valerio
    Perrone](https://www.linkedin.com/in/valerio-perrone/) and [Elina Lesyk](https://www.linkedin.com/in/elina-lesyk/)
    for providing invaluable feedback during the writing of this article.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 感谢 [Constantin Gonzalez](https://www.linkedin.com/in/constantingonzalez/), [Ümit
    Yoldas](https://www.linkedin.com/in/%C3%BCmit-yoldas-23a908177/), [Valerio Perrone](https://www.linkedin.com/in/valerio-perrone/)
    和 [Elina Lesyk](https://www.linkedin.com/in/elina-lesyk/) 在撰写本文期间提供的宝贵反馈。
- en: All images by the author unless otherwise noted.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 所有图像均由作者提供，除非另有说明。
- en: '[1] [Armen Aghajanyan, Luke Zettlemoyer, Sonal Gupta. Intrinsic Dimensionality
    Explains the Effectiveness of Language Model Fine-Tuning, 2020](https://aclanthology.org/2021.acl-long.568/)'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] [Armen Aghajanyan, Luke Zettlemoyer, Sonal Gupta. 内在维度解释了语言模型微调的有效性, 2020](https://aclanthology.org/2021.acl-long.568/)'
- en: '[2] [Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li,
    Shean Wang, Lu Wang, Weizhu Chen. LoRA: Low-Rank Adaptation of Large Language
    Models, 2021](https://arxiv.org/abs/2106.09685)'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] [Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li,
    Shean Wang, Lu Wang, Weizhu Chen. LoRA: 大型语言模型的低秩适应, 2021](https://arxiv.org/abs/2106.09685)'
- en: '[3] [Renrui Zhang, Jiaming Han, Chris Liu, Peng Gao, Aojun Zhou, Xiangfei Hu,
    Shilin Yan, Pan Lu, Hongsheng Li, Yu Qiao. LLaMA-Adapter: Efficient Fine-tuning
    of Language Models with Zero-init Attention, 2023](https://arxiv.org/abs/2303.16199)'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] [Renrui Zhang, Jiaming Han, Chris Liu, Peng Gao, Aojun Zhou, Xiangfei Hu,
    Shilin Yan, Pan Lu, Hongsheng Li, Yu Qiao. LLaMA-Adapter: 高效微调语言模型的零初始化注意力, 2023](https://arxiv.org/abs/2303.16199)'
- en: '[4] [Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen,
    Omer Levy, Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov. RoBERTa: A Robustly
    Optimized BERT Pretraining Approach, 2019](https://arxiv.org/abs/1907.11692)'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] [尹汉·刘、迈尔·奥特、纳曼·戈亚尔、景飞·杜、曼达尔·乔希、丹琪·陈、奥梅尔·列维、迈克·刘易斯、卢克·泽特尔莫耶、维塞林·斯托扬诺夫。《RoBERTa：一种强健优化的BERT预训练方法》，2019](https://arxiv.org/abs/1907.11692)'
- en: '[5] [Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D.
    Manning, Andrew Ng, and Christopher Potts. Recursive Deep Models for Semantic
    Compositionality Over a Sentiment Treebank, 2013](https://aclanthology.org/D13-1170/)'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '[5] [理查德·索彻、亚历克斯·佩雷尔金、简·吴、杰森·庄、克里斯托弗·D·曼宁、安德鲁·吴和克里斯托弗·波茨。《用于情感树库的递归深度模型的语义组合性研究》，2013](https://aclanthology.org/D13-1170/)'
