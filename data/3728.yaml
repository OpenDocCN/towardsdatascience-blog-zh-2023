- en: Understanding LoRA — Low Rank Adaptation For Finetuning Large Models
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 了解 LoRA — 低秩适配用于微调大型模型
- en: 原文：[https://towardsdatascience.com/understanding-lora-low-rank-adaptation-for-finetuning-large-models-936bce1a07c6?source=collection_archive---------1-----------------------#2023-12-22](https://towardsdatascience.com/understanding-lora-low-rank-adaptation-for-finetuning-large-models-936bce1a07c6?source=collection_archive---------1-----------------------#2023-12-22)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/understanding-lora-low-rank-adaptation-for-finetuning-large-models-936bce1a07c6?source=collection_archive---------1-----------------------#2023-12-22](https://towardsdatascience.com/understanding-lora-low-rank-adaptation-for-finetuning-large-models-936bce1a07c6?source=collection_archive---------1-----------------------#2023-12-22)
- en: Math behind this parameter efficient finetuning method
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 这一参数高效微调方法背后的数学原理
- en: '[](https://bhavinjawade.medium.com/?source=post_page-----936bce1a07c6--------------------------------)[![Bhavin
    Jawade](../Images/d10f204b2d0a8a3fd6887048ecaa307d.png)](https://bhavinjawade.medium.com/?source=post_page-----936bce1a07c6--------------------------------)[](https://towardsdatascience.com/?source=post_page-----936bce1a07c6--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----936bce1a07c6--------------------------------)
    [Bhavin Jawade](https://bhavinjawade.medium.com/?source=post_page-----936bce1a07c6--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://bhavinjawade.medium.com/?source=post_page-----936bce1a07c6--------------------------------)[![Bhavin
    Jawade](../Images/d10f204b2d0a8a3fd6887048ecaa307d.png)](https://bhavinjawade.medium.com/?source=post_page-----936bce1a07c6--------------------------------)[](https://towardsdatascience.com/?source=post_page-----936bce1a07c6--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----936bce1a07c6--------------------------------)
    [Bhavin Jawade](https://bhavinjawade.medium.com/?source=post_page-----936bce1a07c6--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F11a205eeb0d3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-lora-low-rank-adaptation-for-finetuning-large-models-936bce1a07c6&user=Bhavin+Jawade&userId=11a205eeb0d3&source=post_page-11a205eeb0d3----936bce1a07c6---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----936bce1a07c6--------------------------------)
    ·4 min read·Dec 22, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F936bce1a07c6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-lora-low-rank-adaptation-for-finetuning-large-models-936bce1a07c6&user=Bhavin+Jawade&userId=11a205eeb0d3&source=-----936bce1a07c6---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F11a205eeb0d3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-lora-low-rank-adaptation-for-finetuning-large-models-936bce1a07c6&user=Bhavin+Jawade&userId=11a205eeb0d3&source=post_page-11a205eeb0d3----936bce1a07c6---------------------post_header-----------)
    发布于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----936bce1a07c6--------------------------------)
    · 4 分钟阅读 · 2023年12月22日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F936bce1a07c6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-lora-low-rank-adaptation-for-finetuning-large-models-936bce1a07c6&user=Bhavin+Jawade&userId=11a205eeb0d3&source=-----936bce1a07c6---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F936bce1a07c6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-lora-low-rank-adaptation-for-finetuning-large-models-936bce1a07c6&source=-----936bce1a07c6---------------------bookmark_footer-----------)![](../Images/5703f134a85ea9f30a74e384e46bba18.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F936bce1a07c6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-lora-low-rank-adaptation-for-finetuning-large-models-936bce1a07c6&source=-----936bce1a07c6---------------------bookmark_footer-----------)![](../Images/5703f134a85ea9f30a74e384e46bba18.png)'
- en: 'Source — Image generated using DALLE-3\. Prompt: a smaller robot shaking hands
    with a bigger robot. The smaller robot is purple, lightning, active and energetic,
    while the bigger robot is frozen in ice and gray.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 来源 — 图片由 DALLE-3 生成。提示：一个较小的机器人与一个较大的机器人握手。较小的机器人是紫色的，充满电，活跃而有活力，而较大的机器人则被冻结在冰中，呈灰色。
- en: Fine-tuning large pre-trained models is computationally challenging, often involving
    adjustment of millions of parameters. This traditional fine-tuning approach, while
    effective, demands substantial computational resources and time, posing a bottleneck
    for adapting these models to specific tasks. LoRA presented an effective solution
    to this problem by decomposing the update matrix during finetuing. To study LoRA,
    let us start by first revisiting traditional finetuing.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 微调大型预训练模型在计算上具有挑战性，通常涉及调整数百万个参数。虽然这种传统的微调方法有效，但需要大量的计算资源和时间，这为将这些模型适应特定任务带来了瓶颈。LoRA通过在微调过程中分解更新矩阵，提出了一个有效的解决方案。为了研究LoRA，我们首先回顾一下传统的微调。
- en: Decomposition of ( Δ W )
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: （ Δ W ）的分解
- en: In traditional fine-tuning, we modify a pre-trained neural network’s weights
    to adapt to a new task. This adjustment involves altering the original weight
    matrix ( W ) of the network. The changes made to ( W ) during fine-tuning are
    collectively represented by ( Δ W ), such that the updated weights can be expressed
    as ( W + Δ W ).
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在传统的微调中，我们会修改预训练神经网络的权重以适应新任务。这种调整涉及改变网络的原始权重矩阵（ W ）。微调过程中对（ W ）所做的更改统称为（ Δ
    W ），以便更新后的权重可以表示为（ W + Δ W ）。
- en: Now, rather than modifying ( W ) directly, the LoRA approach seeks to decompose
    ( Δ W ). This decomposition is a crucial step in reducing the computational overhead
    associated with fine-tuning large models.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，LoRA方法并不是直接修改（ W ），而是寻求分解（ Δ W ）。这种分解是减少与微调大模型相关的计算开销的关键步骤。
- en: '![](../Images/f82bfcfa1f214d4914baa83392d0f3e5.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f82bfcfa1f214d4914baa83392d0f3e5.png)'
- en: Traditional finetuning can be reimagined us above. Here W is frozen where as
    ΔW is trainable (Image by the blog author)
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 传统的微调可以根据上述方法重新构想。在这里，W 是固定的，而 ΔW 是可训练的（图像来源于博客作者）
- en: The Intrinsic Rank Hypothesis
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 内在秩假设
- en: The intrinsic rank hypothesis suggests that significant changes to the neural
    network can be captured using a lower-dimensional representation. Essentially,
    it posits that not all elements of ( Δ W ) are equally important; instead, a smaller
    subset of these changes can effectively encapsulate the necessary adjustments.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 内在秩假设表明，通过较低维度的表示可以捕捉到神经网络的显著变化。基本上，它认为（ Δ W ）的所有元素并不是同样重要的；相反，这些变化的一个较小子集可以有效地封装所需的调整。
- en: Introducing Matrices ( A ) and ( B )
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 引入矩阵（ A ）和（ B ）
- en: 'Building on this hypothesis, LoRA proposes representing ( Δ W ) as the product
    of two smaller matrices, ( A ) and ( B ), with a lower rank. The updated weight
    matrix ( W’ ) thus becomes:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在此假设基础上，LoRA建议将（ Δ W ）表示为两个较小矩阵（ A ）和（ B ）的乘积，且秩较低。更新后的权重矩阵（ W’ ）因此变为：
- en: '[ W’ = W + BA ]'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '[ W’ = W + BA ]'
- en: In this equation, ( W ) remains frozen (i.e., it is not updated during training).
    The matrices ( B ) and ( A ) are of lower dimensionality, with their product (
    BA ) representing a low-rank approximation of ( Δ W ).
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个方程中，（ W ）保持不变（即，在训练过程中不更新）。矩阵（ B ）和（ A ）的维度较低，其乘积（ BA ）表示（ Δ W ）的低秩近似。
- en: '![](../Images/a57eb454cfa654b203ca8adb8fe7c233.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a57eb454cfa654b203ca8adb8fe7c233.png)'
- en: ΔW is decomposed into two matrices A and B where both have lower dimensionality
    then d x d. (Image by the blog author)
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: ΔW 被分解成两个矩阵 A 和 B，其中两个矩阵的维度都低于 d x d。（图像来源于博客作者）
- en: Impact of Lower Rank on Trainable Parameters
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 低秩对可训练参数的影响
- en: By choosing matrices ( A ) and ( B ) to have a lower rank ( r ), the number
    of trainable parameters is significantly reduced. For example, if ( W ) is a (
    d x d ) matrix, traditionally, updating ( W ) would involve ( d² ) parameters.
    However, with ( B ) and ( A ) of sizes ( d x r ) and ( r x d ) respectively, the
    total number of parameters reduces to ( 2dr ), which is much smaller when ( r
    << d ).
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 通过选择具有较低秩（ r ）的矩阵（ A ）和（ B ），可训练参数的数量显著减少。例如，如果（ W ）是一个（ d x d ）矩阵，传统上，更新（ W
    ）将涉及（ d² ）个参数。然而，使用大小为（ d x r ）和（ r x d ）的（ B ）和（ A ），总参数数量减少到（ 2dr ），当（ r <<
    d ）时，这个数量要小得多。
- en: 'The reduction in the number of trainable parameters, as achieved through the
    Low-Rank Adaptation (LoRA) method, offers several significant benefits, particularly
    when fine-tuning large-scale neural networks:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 通过低秩自适应（LoRA）方法实现的可训练参数数量的减少提供了几个重要的好处，特别是在微调大规模神经网络时：
- en: 'Reduced Memory Footprint: LoRA decreases memory needs by lowering the number
    of parameters to update, aiding in the management of large-scale models.'
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 减少内存占用：LoRA通过降低需要更新的参数数量来减少内存需求，有助于管理大规模模型。
- en: 'Faster Training and Adaptation: By simplifying computational demands, LoRA
    accelerates the training and fine-tuning of large models for new tasks.'
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更快的训练和适应：通过简化计算需求，LoRA加快了大型模型对新任务的训练和微调。
- en: 'Feasibility for Smaller Hardware: LoRA’s lower parameter count enables the
    fine-tuning of substantial models on less powerful hardware, like modest GPUs
    or CPUs.'
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对较小硬件的可行性：LoRA的较低参数数量使得可以在较不强大的硬件上，如普通GPU或CPU，进行大型模型的微调。
- en: 'Scaling to Larger Models: LoRA facilitates the expansion of AI models without
    a corresponding increase in computational resources, making the management of
    growing model sizes more practical.'
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 扩展到更大的模型：LoRA使得AI模型的扩展无需相应增加计算资源，使得管理不断增长的模型规模变得更为实际。
- en: In the context of LoRA, the concept of rank plays a pivotal role in determining
    the efficiency and effectiveness of the adaptation process. Remarkably, the paper
    highlights that the rank of the matrices *A* and *B* can be astonishingly low,
    sometimes as low as one.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在LoRA的背景下，秩的概念在确定适应过程的效率和有效性方面起着关键作用。值得注意的是，论文指出矩阵*A*和*B*的秩可以非常低，有时低至1。
- en: Although the LoRA paper predominantly showcases experiments within the realm
    of Natural Language Processing (NLP), the underlying approach of low-rank adaptation
    holds broad applicability and could be effectively employed in training various
    types of neural networks across different domains.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管LoRA论文主要展示了在自然语言处理（NLP）领域的实验，但低秩适应的基本方法具有广泛的适用性，可能在训练不同领域的各种神经网络时有效。
- en: Conclusion
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: LoRA’s approach to decomposing ( Δ W ) into a product of lower rank matrices
    effectively balances the need to adapt large pre-trained models to new tasks while
    maintaining computational efficiency. The intrinsic rank concept is key to this
    balance, ensuring that the essence of the model’s learning capability is preserved
    with significantly fewer parameters.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: LoRA通过将（Δ W）分解为低秩矩阵的乘积，有效地平衡了将大型预训练模型适应新任务与保持计算效率的需求。内在的秩概念对这一平衡至关重要，确保了模型学习能力的本质在参数大大减少的情况下得以保留。
- en: 'References:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 参考文献：
- en: '[[1] Hu, Edward J., et al. “Lora: Low-rank adaptation of large language models.”
    *arXiv preprint arXiv:2106.09685* (2021).](https://arxiv.org/abs/2106.09685)'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '[[1] Hu, Edward J., et al. “Lora: Low-rank adaptation of large language models.”
    *arXiv预印本arXiv:2106.09685* (2021).](https://arxiv.org/abs/2106.09685)'
