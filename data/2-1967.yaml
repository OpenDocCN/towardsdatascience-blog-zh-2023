- en: 'TensorFlow Decision Forests: A Comprehensive Introduction'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: TensorFlow Decision Forests：全面介绍
- en: 原文：[https://towardsdatascience.com/tensorflow-decision-forests-a-comprehensive-introduction-3b6056a6d6b0](https://towardsdatascience.com/tensorflow-decision-forests-a-comprehensive-introduction-3b6056a6d6b0)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/tensorflow-decision-forests-a-comprehensive-introduction-3b6056a6d6b0](https://towardsdatascience.com/tensorflow-decision-forests-a-comprehensive-introduction-3b6056a6d6b0)
- en: Train, tune, evaluate, interpret and serve the tree-based models using TensorFlow
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用TensorFlow训练、调整、评估、解释和服务基于树的模型
- en: '[](https://medium.com/@antonsruberts?source=post_page-----3b6056a6d6b0--------------------------------)[![Antons
    Tocilins-Ruberts](../Images/363a4f32aa793cca7a67dea68e76e3cf.png)](https://medium.com/@antonsruberts?source=post_page-----3b6056a6d6b0--------------------------------)[](https://towardsdatascience.com/?source=post_page-----3b6056a6d6b0--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----3b6056a6d6b0--------------------------------)
    [Antons Tocilins-Ruberts](https://medium.com/@antonsruberts?source=post_page-----3b6056a6d6b0--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@antonsruberts?source=post_page-----3b6056a6d6b0--------------------------------)[![Antons
    Tocilins-Ruberts](../Images/363a4f32aa793cca7a67dea68e76e3cf.png)](https://medium.com/@antonsruberts?source=post_page-----3b6056a6d6b0--------------------------------)[](https://towardsdatascience.com/?source=post_page-----3b6056a6d6b0--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----3b6056a6d6b0--------------------------------)
    [Antons Tocilins-Ruberts](https://medium.com/@antonsruberts?source=post_page-----3b6056a6d6b0--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----3b6056a6d6b0--------------------------------)
    ·11 min read·Apr 14, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page-----3b6056a6d6b0--------------------------------)
    ·阅读时长11分钟·2023年4月14日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/5366f2a6d39c66852d1ebbf295e77b6b.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5366f2a6d39c66852d1ebbf295e77b6b.png)'
- en: Photo by [Javier Allegue Barros](https://unsplash.com/pt-br/@soymeraki?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由[Javier Allegue Barros](https://unsplash.com/pt-br/@soymeraki?utm_source=medium&utm_medium=referral)提供，来源于[Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: Introduction
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: Two years ago, TensorFlow (TF) team has open-sourced a library to train tree-based
    models called [TensorFlow Decision Forests (TFDF)](https://github-com.translate.goog/tensorflow/decision-forests?_x_tr_sl=en&_x_tr_tl=ru&_x_tr_hl=ru&_x_tr_pto=sc).
    Just last month they’ve finally [announced](https://blog.tensorflow.org/2023/02/updates-tensorflow-decision-forests-is-production-ready.html)
    that the package is production ready, so I’ve decided that it’s time to take a
    closer look. The aim of this post is to give you a better idea about the package
    and show you how to (effectively) use it. Below you can see the structure of this
    post, feel free to skip to any part that interests you the most.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 两年前，TensorFlow (TF) 团队开源了一个用于训练基于树的模型的库，[TensorFlow Decision Forests (TFDF)](https://github-com.translate.goog/tensorflow/decision-forests?_x_tr_sl=en&_x_tr_tl=ru&_x_tr_hl=ru&_x_tr_pto=sc)。就在上个月，他们终于[宣布](https://blog.tensorflow.org/2023/02/updates-tensorflow-decision-forests-is-production-ready.html)该软件包已准备好投入生产，因此我决定更深入地了解一下。本文的目的是让你对这个软件包有更好的了解，并展示如何（有效地）使用它。下面你可以看到本文的结构，随意跳过你最感兴趣的部分。
- en: What is TFDF and why use it?
  id: totrans-10
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 什么是TFDF以及为什么使用它？
- en: Train Random Forest (RF) and Gradient Boosted Trees (GBT) models using TFDF
  id: totrans-11
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用TFDF训练随机森林（RF）和梯度提升树（GBT）模型
- en: Hyper-parameter tuning with TFDF and Optuna
  id: totrans-12
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用TFDF和Optuna进行超参数调整
- en: Model inspection
  id: totrans-13
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 模型检查
- en: Serving GBT model using TF Serving
  id: totrans-14
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用TF Serving服务GBT模型
- en: Setup
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设置
- en: You can find all the code in my [repo](https://github.com/aruberts/tutorials/tree/main/tfdf/notebooks),
    so make sure to star it if you haven’t already. In this post we’ll be training
    a few models for loan default prediction using the [U.S. Small Business Administration
    dataset](https://www.kaggle.com/datasets/mirbektoktogaraev/should-this-loan-be-approved-or-denied)
    (CC BY-SA 4.0 license) dataset. Models will be trained using already pre-processed
    data but you can find a [notebook](https://github.com/aruberts/tutorials/blob/main/tfdf/notebooks/data_preprocessing.ipynb)
    in the repo that describes the processing and feature engineering steps. Make
    sure to follow them if you want to directly replicate my code here. Alternatively,
    use this code as a starting point and adapt it to your dataset (my recommended
    approach).
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在我的[仓库](https://github.com/aruberts/tutorials/tree/main/tfdf/notebooks)中找到所有代码，如果还没有，请务必给它加个星。在这篇文章中，我们将使用[美国小企业管理局数据集](https://www.kaggle.com/datasets/mirbektoktogaraev/should-this-loan-be-approved-or-denied)（CC
    BY-SA 4.0许可）来训练几个贷款违约预测模型。模型将使用已经预处理的数据进行训练，但你可以在仓库中找到一个[笔记本](https://github.com/aruberts/tutorials/blob/main/tfdf/notebooks/data_preprocessing.ipynb)，描述了处理和特征工程步骤。如果你想直接复制我的代码，请确保遵循这些步骤。或者，使用这些代码作为起点，并根据你的数据集进行调整（这是我推荐的方法）。
- en: Installing TensorFlow Decision Forests is quite straightforward, just run `pip
    install tensorflow_decision_forests` and most of the time this should work. There
    are some [issues](https://github.com/tensorflow/decision-forests/issues/152) reported
    with M1 and M2 Macs but it worked fine for me personally with the latest version
    of TFDF.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 安装TensorFlow Decision Forests非常简单，只需运行`pip install tensorflow_decision_forests`，通常这就可以了。虽然有一些在M1和M2
    Mac上报告的[问题](https://github.com/tensorflow/decision-forests/issues/152)，但我个人在最新版本的TFDF上没有遇到问题。
- en: TensorFlow Decision Forest
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: TensorFlow Decision Forest
- en: What is TFDF?
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 什么是TFDF？
- en: TensorFlow Decision Forest is actually built on top of the C++ library called
    Yggdrasil Decision Forests which also developed by Google. The original C++ algorithms
    are designed to build scalable decision tree models that can handle large datasets
    and high-dimensional feature spaces. By integrating this library into the wider
    TF ecosystem, users are able now to easily build scalable RF and GBT models without
    having to learn another language.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow Decision Forest实际上是基于Google开发的C++库Yggdrasil Decision Forests。原始C++算法旨在构建可扩展的决策树模型，以处理大数据集和高维特征空间。通过将这个库整合到更广泛的TF生态系统中，用户现在可以轻松地构建可扩展的RF和GBT模型，而无需学习另一种语言。
- en: Why use it?
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 为什么使用它？
- en: The main advantage of this library over e.g. XGBoost or LightGBM is its tight
    integration with the other TF ecosystem components. It might be particularly interesting
    for teams who already have other TensorFlow models as part of their pipeline or
    use TFX. TFDF can be quite easily integrated with e.g. NLP models making multi-modal
    pipelines easier. Also, if you are serving models using TF Serving you might also
    want to consider this library due to its native support (no need for ONNX or other
    cross-package serialisation methods). Finally, this library gives you truly a
    ton parameters that you can adjust to approximate models from XGBoost, LightGBM,
    and many other Gradient Boosted Machine (GBM) methods. This means that you don’t
    need to switch between different GBM libraries in the training process which can
    be quite nice from code maintainability perspective.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 这个库相对于例如XGBoost或LightGBM的主要优势在于它与其他TF生态系统组件的紧密集成。对于已经在管道中使用其他TensorFlow模型或使用TFX的团队，它可能特别有趣。TFDF可以很容易地与例如NLP模型集成，使多模态管道更加简单。此外，如果你使用TF
    Serving来服务模型，你也可能会考虑这个库，因为它原生支持（无需ONNX或其他跨包序列化方法）。最后，这个库提供了大量参数，你可以调整以接近XGBoost、LightGBM和许多其他梯度提升机（GBM）方法的模型。这意味着你在训练过程中无需在不同的GBM库之间切换，这对于代码维护来说是非常有利的。
- en: Model Training
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型训练
- en: Make sure to pull [this notebook](https://github.com/aruberts/tutorials/blob/main/tfdf/notebooks/model_training.ipynb)
    and follow along as below you can only see parts of the code.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 确保拉取[这个笔记本](https://github.com/aruberts/tutorials/blob/main/tfdf/notebooks/model_training.ipynb)，并按照下面的步骤操作，因为你在这里只能看到部分代码。
- en: Data
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据
- en: As said in the setup section, I’m going to be using a pre-processed version
    of this dataset. To prepare it for TFDF, we first need to read it in as usual
    with pandas and decide which columns we’re going to treat as categorical and which
    are going to be numerical.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 正如设置部分所述，我将使用这个数据集的预处理版本。为了准备TFDF，我们首先需要像往常一样用pandas读取数据，并决定哪些列将作为分类变量，哪些作为数值变量。
- en: Feature Usage
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 特征使用情况
- en: 'To ensure a well-structured project and avoid unexpected behaviour, it is considered
    a good practice to specify a `FeatureUsage` for each feature, although it’s not
    mandatory. Fortunately, it’s an easy task: you simply need to decide which feature
    types to assign to each one from the six supported types — `BOOLEAN`, `CATEGORICAL`,
    `CATEGORICAL_SET`, `DISCRETIZED_NUMERICAL`, `HASH`, and `NUMERICAL`. Some of these
    types come with additional parameters, so make sure to read more about them [here](https://www.tensorflow.org/decision_forests/api_docs/python/tfdf/keras/FeatureSemantic).'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确保项目结构良好并避免意外行为，通常建议为每个特征指定一个`FeatureUsage`，虽然这不是强制性的。幸运的是，这是一项简单的任务：你只需决定将每个特征分配给六种支持类型中的一种——`BOOLEAN`、`CATEGORICAL`、`CATEGORICAL_SET`、`DISCRETIZED_NUMERICAL`、`HASH`和`NUMERICAL`。其中一些类型还有额外的参数，因此确保了解更多信息
    [这里](https://www.tensorflow.org/decision_forests/api_docs/python/tfdf/keras/FeatureSemantic)。
- en: While we’ll keep things simple in this example and stick to only numerical and
    categorical data types, don’t hesitate to experiment with the other options, especially
    `DISCRETIZED_NUMERICAL`, as they can significantly speed up your training process
    (similar to LightGBM). As you can see below, you need to provide the chosen data
    type to `semantic` parameter and for the categorical features we also want to
    specify the `min_vocab_frequency` parameter to get rid of rare values.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们将保持简单，只使用数值型和类别型数据类型，但不要犹豫尝试其他选项，特别是`DISCRETIZED_NUMERICAL`，因为它们可以显著加快训练过程（类似于
    LightGBM）。正如下方所示，你需要将所选的数据类型提供给`semantic`参数，对于类别特征，我们还需要指定`min_vocab_frequency`参数以去除稀有值。
- en: Reading Data Using TF Dataset
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 TF 数据集读取数据
- en: To simplest way to read in the dataset is by using TF Dataset. TFDF has a very
    nice utility function called`pd_dataframe_to_tf_dataset` which makes this step
    a piece of cake.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 读取数据集的最简单方法是使用 TF 数据集。TFDF 提供了一个非常好的实用函数`pd_dataframe_to_tf_dataset`，使这一过程变得非常简单。
- en: 'In the code above we pass our DataFrame objects into the function and provide
    the following parameters:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在上面的代码中，我们将 DataFrame 对象传递给函数，并提供以下参数：
- en: Name of the label column
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 标签列的名称
- en: Name of the weight column (None in this case)
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 权重列的名称（在此情况下为 None）
- en: Batch size (helps to speed up reading of the data)
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 批量大小（有助于加快数据读取速度）
- en: The resulting datasets are in the correct format of TF Dataset (batched and
    pre-fetched) and are ready to be used for training/evaluation. You can of course
    create your own method for reading in the datasets but you must pay a special
    attention to the outputted format.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 生成的数据集已按照 TF 数据集的正确格式（批处理和预取）进行准备，可以用于训练/评估。当然，你也可以创建自己的读取数据集的方法，但必须特别注意输出格式。
- en: TFDF Default Parameters
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: TFDF 默认参数
- en: Training the models is quite straight-forward if you’ve followed all the previous
    data preparation instructions.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你按照所有先前的数据准备说明进行操作，训练模型非常简单。
- en: As you can see from the code above, it takes just a few lines to build and train
    GBT and RF models with default paramaters. All you need to specify is the features
    used, training and validation datasets, and you’re good to go. When evaluating
    both of these models using ROC and PR AUCs we can see that the performance is
    already quite good.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 从上面的代码可以看出，只需几行代码即可使用默认参数构建和训练 GBT 和 RF 模型。你只需指定所使用的特征、训练和验证数据集，然后就可以开始了。在使用
    ROC 和 PR AUC 评估这两个模型时，我们可以看到其性能已经相当不错。
- en: '[PRE0]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Let’s see if these results can be further improved using hyper-parameter tuning.
    For simplicity, I’m going to focus solely on optimising the GBT model but everything
    can be as easily applied to the RF models as well.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看这些结果是否可以通过超参数调整进一步改善。为了简化，我将专注于优化 GBT 模型，但这些方法也可以很容易地应用于 RF 模型。
- en: Hyper-parameter Tuning
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 超参数调整
- en: There are a ton of parameters to tune, very good explanation of every one of
    the can be found in the official Yggdrasil [documentation](https://ydf.readthedocs.io/en/latest/hyper_parameters.html).
    TFDF gives you a few in-built options to tune parameters but you can also use
    more standard libraries like [Optuna](https://github.com/optuna/optuna) or [Hyperpot](https://github.com/hyperopt/hyperopt).
    Here’s a list of the approaches ordered from the least involved to the most involved
    approaches.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多参数需要调整，每一个的详细解释可以在官方 Yggdrasil [文档](https://ydf.readthedocs.io/en/latest/hyper_parameters.html)中找到。TFDF
    为你提供了一些内置的参数调整选项，但你也可以使用更标准的库，如[Optuna](https://github.com/optuna/optuna)或[Hyperpot](https://github.com/hyperopt/hyperopt)。以下是按从最少参与到最多参与的方式排列的方法列表。
- en: Hyper-parameter templates
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 超参数模板
- en: Hyper-parameter search using pre-defined space
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用预定义空间的超参数搜索
- en: Hyper-parameter search using custom space
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用自定义空间的超参数搜索
- en: Hyper-parameter Templates
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 超参数模板
- en: Very cool feature that TFDF provides is the availability of hyper-parameter
    templates. These are the parameters that in the [paper](https://arxiv.org/abs/2212.02934)
    were shown to perform the best across a wide range of the datasets. Two available
    templates are — `better_default` and `benchmark_rank1` . If you’re short on time
    or are not familiar with machine learning that well, this might be a good option
    for you. Specifying these parameters is literally just 1 line of code.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: TFDF提供了一个非常酷的功能，就是超参数模板的可用性。这些是[论文](https://arxiv.org/abs/2212.02934)中显示在各种数据集上表现最好的参数。两个可用的模板是——`better_default`和`benchmark_rank1`。如果你时间紧迫或对机器学习不太熟悉，这可能是一个不错的选择。指定这些参数只需一行代码。
- en: Looking at the results we can see that with `better_default` parameters we were
    able to get a slight uplift in both ROC and PR AUCs. `benchmark_rank1` parameter,
    on the other hand, perform much worse. This is why it’s important to properly
    evaluate the resulting models before deploying them.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 从结果来看，我们可以看到使用`better_default`参数在ROC和PR AUC上都有了轻微的提升。而`benchmark_rank1`参数则表现较差。这就是为什么在部署模型之前正确评估结果模型很重要的原因。
- en: '[PRE1]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Pre-defined Search Space
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 预定义搜索空间
- en: TFDF comes with a nice utility called `RandomSearch` which performs randomised
    grid search (similar to `sklearn`) across many of the available parameters. There’s
    an option to specify these parameters manually (see example [here](https://www.tensorflow.org/decision_forests/tutorials/automatic_tuning_colab#training_a_model_with_automated_hyper-parameter_tuning_and_manual_definition_of_the_hyper-parameters))
    but it’s also possible to use a pre-defined search space. Again, if you’re not
    that familiar with ML this might be a good option for you because it removes the
    need to set these parameters manually.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: TFDF附带了一个名为`RandomSearch`的实用工具，它在许多可用参数之间执行随机网格搜索（类似于`sklearn`）。有一个选项可以手动指定这些参数（参见[示例](https://www.tensorflow.org/decision_forests/tutorials/automatic_tuning_colab#training_a_model_with_automated_hyper-parameter_tuning_and_manual_definition_of_the_hyper-parameters)），但也可以使用预定义的搜索空间。再次说明，如果你对机器学习不太熟悉，这可能是一个不错的选择，因为它不需要你手动设置这些参数。
- en: 'WARNING: this search took me ages, so I had to stop it after 12 iterations.
    Some parameters that get tested (e.g. oblique splits) take a long time to fit.'
  id: totrans-53
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 警告：这个搜索花了我很长时间，因此我不得不在12次迭代后停止。一些被测试的参数（例如斜切分裂）需要较长时间来拟合。
- en: You can access all of the tried combinations using the following command.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用以下命令访问所有尝试过的组合。
- en: '[PRE2]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '![](../Images/fa0359d4cbc57af5bb77fa4ede8dc10c.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fa0359d4cbc57af5bb77fa4ede8dc10c.png)'
- en: Hyper-parameter table. Screenshot by author.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 超参数表。截图由作者提供。
- en: After 12 iterations, the best model performed a bit worse than the baseline,
    so use this tuning method cautiously. You can try to alter the search space, but
    at this point you might as well use another library.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在12次迭代后，最佳模型的表现稍逊于基线，因此使用这种调优方法时要谨慎。你可以尝试更改搜索空间，但此时你不妨使用另一个库。
- en: '[PRE3]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Custom Search Space (with custom loss)
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 自定义搜索空间（带自定义损失）
- en: 'There are a few notable disadvantages to using `RandomSearch` approach:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`RandomSearch`方法有一些显著的缺点：
- en: Only randomised grid search algorithm is available
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 仅提供随机网格搜索算法
- en: No option to define your own loss to optimise
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 没有选项可以定义你自己的损失函数进行优化
- en: Full parameter grid needs to be provided if you don’t use `use_predefined_hps`
    flag
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果不使用`use_predefined_hps`标志，则需要提供完整的参数网格
- en: Because of these reasons I highly recommend using external optimisation libraries
    if you have enough knowledge to set a sensible search space yourself. Below you
    can see how to do the tuning using `optuna` .
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这些原因，我强烈建议如果你有足够的知识来自己设置合理的搜索空间，最好使用外部优化库。下面你可以看到如何使用`optuna`进行调优。
- en: Most of these parameters are quite standard for GBTs, but there are a few note-worthy
    parameters. First, we can change the `growing_strategy` to `BEST_FIRST_GLOBAL`
    (a.k.a leaf-wise growth) which is the strategy used by LightGBM. Second, we can
    use `BINARY_FOCAL_LOSS` which is supposed to perform better with the imbalanced
    datasets ([source](https://paperswithcode.com/method/focal-loss)). Third, there’s
    an option to change the `split_axis` parameter to use sparse oblique splits which
    was shown to be quite effective in [this paper](https://arxiv.org/abs/1506.03410).
    Finally, there’s also an opportunity to build “[honest trees](https://arxiv.org/abs/1610.01271)”
    using the `honest` parameter.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 这些参数中的大多数对于 GBT 来说都是相当标准的，但也有一些值得注意的参数。首先，我们可以将 `growing_strategy` 更改为 `BEST_FIRST_GLOBAL`（即叶子优先生长），这是
    LightGBM 使用的策略。其次，我们可以使用 `BINARY_FOCAL_LOSS`，这种方法对于不平衡数据集表现更佳 ([source](https://paperswithcode.com/method/focal-loss))。第三，可以选择将
    `split_axis` 参数更改为使用稀疏斜切分，这在 [这篇论文](https://arxiv.org/abs/1506.03410) 中显示效果相当好。最后，还可以使用
    `honest` 参数构建 “[诚实树](https://arxiv.org/abs/1610.01271)”。
- en: Below you can see the results achieved with the best parameters. As you can
    see, tuning with custom search space has yielded the best results so far.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是使用最佳参数取得的结果。正如你所见，自定义搜索空间的调优迄今为止取得了最佳结果。
- en: '[PRE4]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Now that we’ve settled on the hyper-parameters, let’s re-train the model and
    proceed with its inspection.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经确定了超参数，让我们重新训练模型并继续进行检查。
- en: Model Inspection
  id: totrans-70
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型检查
- en: 'TFDF offers a nice utility object to inspect the trained model called `Inspector`
    . There are 3 main uses for this object that I’ll explore below:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: TFDF 提供了一个名为 `Inspector` 的实用工具来检查训练后的模型。这个对象有 3 个主要的用途，下面我将进行详细探讨：
- en: Inspecting the model’s attributes like type, number of trees or features used
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检查模型的属性，例如类型、树的数量或使用的特征
- en: Obtaining feature importances
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 获取特征重要性
- en: Extracting trees structures
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 提取树结构
- en: Inspect Model Attributes
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 检查模型属性
- en: The inspector class stores various attributes that you might want to explore
    if, for example, you’ve loaded somebody else’s model or you haven’t used it in
    a while. You can print out the model type (GBT or RF), the number of trees your
    model has, the training objective, and the features that were used to train the
    model. Inspecting number of trees is especially useful since if the early-stopping
    has kicked in, this parameter is going to be smaller than what you’ve set it up
    to be.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: inspector 类存储了各种属性，如果例如你加载了其他人的模型或有一段时间没有使用它，你可能想要探索这些属性。你可以打印出模型类型（GBT 或 RF）、模型的树的数量、训练目标以及用于训练模型的特征。检查树的数量特别有用，因为如果早期停止已触发，这个参数会比你设置的值小。
- en: Another option is to simply run `manual_tuned.summary()` to examine the model
    in more detail.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种选择是简单地运行 `manual_tuned.summary()` 以更详细地检查模型。
- en: Feature Importances
  id: totrans-78
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 特征重要性
- en: Just like all the other libraries, TFDF comes with in-built feature importance
    scores. For GBTs, you get access to `NUM_NODES` , `SUM_SCORE` , `INV_MEAN_MIN_DEPTH`
    , `NUM_AS_ROOT` methods of explaining. Note that you can also set `compute_permutation_variable_importance`
    parameter to `True` during training which will add a few additional methods. The
    downside of this is that the model training will take significantly longer, so
    use it with care (perhaps on a sample of data).
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 就像所有其他库一样，TFDF 提供了内置的特征重要性评分。对于 GBT，你可以访问 `NUM_NODES`、`SUM_SCORE`、`INV_MEAN_MIN_DEPTH`、`NUM_AS_ROOT`
    方法进行解释。请注意，你还可以在训练过程中将 `compute_permutation_variable_importance` 参数设置为 `True`，这将添加一些额外的方法。缺点是模型训练时间会显著增加，因此请谨慎使用（最好是在数据样本上）。
- en: '![](../Images/39dc4b3dfd659352d963afba439a677d.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/39dc4b3dfd659352d963afba439a677d.png)'
- en: Imprtances bar plot. Screenshot by author.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 重要性条形图。截图由作者提供。
- en: For the model that I’ve built, Term variable has consistently come up as the
    most important feature with categorical variables like Bank, State, and Bank State
    following it. I’d say that one of the largest disadvantages of TFDF library is
    the inability to use SHAP with it. Hopefully, the support will come in the future
    versions.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我构建的模型，Term 变量一直被认为是最重要的特征，类别变量如 Bank、State 和 Bank State 紧随其后。我认为 TFDF 库最大的缺点之一是无法与
    SHAP 配合使用。希望未来的版本能提供支持。
- en: Inspect Individual Trees
  id: totrans-83
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 检查单独的树
- en: Sometimes we want to take a look at the individual trees for the sake of explainability
    or model validation. TFDF gives an easy access to all the trees constructed during
    training in the inspector object. For now, let’s inspect the first tree of our
    GBT model since usually it’s the most informative one.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 有时我们需要查看单独的树，以便于解释性或模型验证。 TFDF 在 inspector 对象中提供了对训练过程中构建的所有树的简单访问。目前，让我们检查一下我们的
    GBT 模型的第一棵树，因为它通常是最具信息性的。
- en: '![](../Images/e9feb8c5a671742898251233d8c3058e.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e9feb8c5a671742898251233d8c3058e.png)'
- en: Tree structure. Screenshot by author.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 树结构。作者截图。
- en: As you can see, when we’re dealing with large trees it may not be that convenient
    to inspect them using the print out statement. That’s why TFDF also has a tree
    plotting utility — `tfdf.model_plotter.plot_model` .
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所见，当我们处理大型树时，使用打印语句检查它们可能不是很方便。这就是为什么 TFDF 还有一个树绘图工具——`tfdf.model_plotter.plot_model`。
- en: '![](../Images/8bb950e7d917da036be2a1e0c0ddee8b.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8bb950e7d917da036be2a1e0c0ddee8b.png)'
- en: First GBT tree (depth=4). Screenshot by author.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 第一棵 GBT 树（深度=4）。作者截图。
- en: Also note that for Random Forest models, you can use `dtreeviz` package which
    gives you more visually appealing results ([here’s how to use it](https://www.tensorflow.org/decision_forests/tutorials/dtreeviz_colab#feature_space_partitioning_2)).
    GBT models with this package are not yet supported.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 另外请注意，对于随机森林模型，你可以使用`dtreeviz`包，它会给你更具视觉吸引力的结果（[这里是如何使用它](https://www.tensorflow.org/decision_forests/tutorials/dtreeviz_colab#feature_space_partitioning_2)）。目前，这个包对
    GBT 模型尚不支持。
- en: TF Serving
  id: totrans-91
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: TF Serving
- en: So far, we’ve trained, tuned and evaluated the model. What else is there? Serving
    the model of course! Lucky for us, TFDF is natively supported by TF Serving (since
    the latest version), so this part is also quite easy. If you already have an up-to-date
    TF Serving instance, all you need to do is to point to your saved model in the
    `model_base_path` parameter. You can save the TFDF model with the `save` method.
    Notice that you should save it to a folder `1` since it’s the first version of
    your model.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经训练、调整和评估了模型。还有什么其他的呢？当然是服务模型！幸运的是，TFDF 得到了 TF Serving 的原生支持（从最新版本开始），所以这一部分也很简单。如果你已经有了最新的
    TF Serving 实例，你只需在`model_base_path`参数中指向你保存的模型即可。你可以使用`save`方法保存 TFDF 模型。请注意，你应该将其保存到文件夹`1`，因为这是你模型的第一个版本。
- en: '[PRE5]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: For those who haven’t used TF Serving model you can find a good tutorial by
    the TF team [here](https://www.tensorflow.org/decision_forests/tensorflow_serving)
    and I’ve also written this [Colab notebook](https://colab.research.google.com/drive/13fSg7LXdewamRh-nRVRh1Dw_Fq_bo8C-?usp=sharing)
    just in case you’re working on M1 or M2 Macs (there’s currently no support for
    TF Serving).
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 对于那些没有使用 TF Serving 模型的人，你可以在 TF 团队的[这里](https://www.tensorflow.org/decision_forests/tensorflow_serving)找到一个很好的教程，我也写了这个[Colab
    笔记本](https://colab.research.google.com/drive/13fSg7LXdewamRh-nRVRh1Dw_Fq_bo8C-?usp=sharing)，以防你在
    M1 或 M2 Mac 上工作（目前不支持 TF Serving）。
- en: 'Essentially, all you need to do is to install the TF Serving locally and launch
    it with the right parameters. Once you have the binary downloaded, here’s the
    command to launch the server:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 本质上，你需要做的就是在本地安装 TF Serving 并使用正确的参数启动它。一旦下载了二进制文件，这里是启动服务器的命令：
- en: '[PRE6]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Please note that `model_base_path` should be absolute path and not relative.
    After the TF Serving server starts, you can start sending requests to it. There
    are two expected formats — `instances` and `inputs` . Below you can see an example
    of the later format but you can see examples of both in this [tutorial](https://www.tensorflow.org/decision_forests/tensorflow_serving#:~:text=Finally%2C%20you%20can%20send%20a%20request%20to%20TF%20Serving%20using%20the%20Rest%20API.%20Two%20formats%20are%20available%3A%20predict%2Binstances%20API%20and%20predict%2Binputs%20API.%20Here%20is%20an%20example%20of%20each%20of%20them%3A).
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，`model_base_path` 应该是绝对路径，而不是相对路径。TF Serving 服务器启动后，你可以开始向其发送请求。有两种预期的格式——`instances`和`inputs`。下面你可以看到后者格式的示例，但你可以在这个[教程](https://www.tensorflow.org/decision_forests/tensorflow_serving#:~:text=Finally%2C%20you%20can%20send%20a%20request%20to%20TF%20Serving%20using%20the%20Rest%20API.%20Two%20formats%20are%20available%3A%20predict%2Binstances%20API%20and%20predict%2Binputs%20API.%20Here%20is%20an%20example%20of%20each%20of%20them%3A)中查看到两者的示例。
- en: '[PRE7]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: If you’ve managed to get the response (it can be different from mine) — congratulations!
    You’ve made it to the last step of this post. Now, let’s look back at what you’ve
    accomplished if you have followed all these chapter.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你成功获得了响应（可能与我的不同）——恭喜你！你已经完成了本文的最后一步。现在，让我们回顾一下如果你遵循了所有这些章节，你完成了什么。
- en: Conclusion
  id: totrans-100
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: To summarise, TFDF is a powerful and scalable library for training tree based
    models in TensorFlow. TFDF models are well integrated with the rest of the TensorFlow
    ecosystem, so if you’re using TFX, have other TF models in production, or are
    using TF Serving, you’ll find this library quite useful.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，TFDF 是一个强大且可扩展的库，用于在 TensorFlow 中训练基于树的模型。TFDF 模型与 TensorFlow 生态系统的其他部分集成良好，因此如果你在使用
    TFX，有其他 TF 模型在生产中，或在使用 TF Serving，你会发现这个库非常有用。
- en: If you’ve followed through the notebooks, you should now know how to train,
    tune, inspect, and server the TFDF models. As you saw, TFDF models are highly
    customisable, so if you’re in need of a high-performance library for tree-based
    models, give it a shot and let me know how it goes!
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你已经完成了这些笔记本，你现在应该知道如何训练、调优、检查和服务 TFDF 模型。正如你所看到的，TFDF 模型高度可定制，因此如果你需要一个高性能的树模型库，可以试试看，并告诉我效果如何！
- en: Not a Medium Member yet?
  id: totrans-103
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还不是 Medium 会员？
- en: '[](https://medium.com/@antonsruberts/membership?source=post_page-----3b6056a6d6b0--------------------------------)
    [## Join Medium with my referral link - Antons Tocilins-Ruberts'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@antonsruberts/membership?source=post_page-----3b6056a6d6b0--------------------------------)
    [## 使用我的推荐链接加入 Medium - Antons Tocilins-Ruberts'
- en: Read every story from Antons Tocilins-Ruberts (and thousands of other writers
    on Medium). Your membership fee directly…
  id: totrans-105
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 阅读 Antons Tocilins-Ruberts（以及 Medium 上成千上万的其他作者）撰写的每一个故事。您的会员费直接……
- en: medium.com](https://medium.com/@antonsruberts/membership?source=post_page-----3b6056a6d6b0--------------------------------)
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: medium.com](https://medium.com/@antonsruberts/membership?source=post_page-----3b6056a6d6b0--------------------------------)
