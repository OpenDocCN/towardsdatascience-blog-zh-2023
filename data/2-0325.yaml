- en: 'Araucana XAI: Local Explainability With Decision Trees for Healthcare'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Araucana XAI：利用决策树在医疗保健中的局部解释性
- en: 原文：[https://towardsdatascience.com/araucana-xai-why-did-ai-get-this-one-wrong-8ee79dabdb1a](https://towardsdatascience.com/araucana-xai-why-did-ai-get-this-one-wrong-8ee79dabdb1a)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/araucana-xai-why-did-ai-get-this-one-wrong-8ee79dabdb1a](https://towardsdatascience.com/araucana-xai-why-did-ai-get-this-one-wrong-8ee79dabdb1a)
- en: Introducing a new model-agnostic, post hoc XAI approach based on CART to provide
    local explanations improving the transparency of AI-assisted decision making in
    healthcare
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 引入一种基于CART的模型无关、后期XAI方法，以提供局部解释，提升医疗保健中AI辅助决策的透明度
- en: '[](https://medium.com/@detsutut?source=post_page-----8ee79dabdb1a--------------------------------)[![Tommaso
    Buonocore](../Images/c842f7d670434ba2315c63adf6fc385f.png)](https://medium.com/@detsutut?source=post_page-----8ee79dabdb1a--------------------------------)[](https://towardsdatascience.com/?source=post_page-----8ee79dabdb1a--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----8ee79dabdb1a--------------------------------)
    [Tommaso Buonocore](https://medium.com/@detsutut?source=post_page-----8ee79dabdb1a--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@detsutut?source=post_page-----8ee79dabdb1a--------------------------------)[![Tommaso
    Buonocore](../Images/c842f7d670434ba2315c63adf6fc385f.png)](https://medium.com/@detsutut?source=post_page-----8ee79dabdb1a--------------------------------)[](https://towardsdatascience.com/?source=post_page-----8ee79dabdb1a--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----8ee79dabdb1a--------------------------------)
    [Tommaso Buonocore](https://medium.com/@detsutut?source=post_page-----8ee79dabdb1a--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----8ee79dabdb1a--------------------------------)
    ·7 min read·Jul 14, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----8ee79dabdb1a--------------------------------)
    ·阅读时间 7 分钟·2023年7月14日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/22aec8c9f8f98b143838f8990ff864f5.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/22aec8c9f8f98b143838f8990ff864f5.png)'
- en: The term ‘Araucana’ comes from the monkey puzzle tree pine from Chile, but is
    also the name of a beautiful breed of domestic chicken. © [MelaniMarfeld](https://pixabay.com/photos/araucana-hen-chicken-poultry-4097906/)
    from Pixabay
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: “Araucana”一词来源于智利的猴面包树松，但也是一种美丽的家禽鸡的名字。© [MelaniMarfeld](https://pixabay.com/photos/araucana-hen-chicken-poultry-4097906/)
    来源于Pixabay
- en: Why did AI get this one wrong?
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为什么AI会在这一点上出错？
- en: In the realm of artificial intelligence, there is a growing concern regarding
    the lack of transparency and understandability of complex AI systems. Recent research
    has been dedicated to addressing this issue by developing explanatory models that
    shed light on the inner workings of opaque systems like boosting, bagging, and
    deep learning techniques.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在人工智能领域，对复杂AI系统缺乏透明度和可理解性的担忧日益增加。近期研究致力于通过开发解释性模型来解决这个问题，这些模型揭示了像提升、集成和深度学习技术等不透明系统的内部工作原理。
- en: Local and Global Explainability
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 局部和全球解释性
- en: 'Explanatory models can shed light on the behavior of AI systems in two distinct
    ways:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 解释性模型可以以两种不同的方式揭示AI系统的行为：
- en: '**Global explainability**. Global explainers provide a *comprehensive understanding*
    of how the AI classifier behaves as a whole. They aim to uncover overarching patterns,
    trends, biases, and other characteristics that remain consistent across various
    inputs and scenarios.'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**全球解释性**。全球解释器提供对AI分类器整体行为的*全面理解*。他们旨在揭示在各种输入和场景中保持一致的宏观模式、趋势、偏差和其他特征。'
- en: '**Local explainability**. On the other hand, local explainers focus on providing
    *insights into the decision-making process* of the AI system for a *single instance*.
    By highlighting the features or inputs that significantly influenced the model’s
    prediction, a local explainer offers a glimpse into how a specific decision was
    reached. However, it’s important to note that these explanations may not be applicable
    to other instances or provide a complete understanding of the model’s overall
    behavior.'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**局部解释性**。另一方面，局部解释器专注于提供对AI系统在*单个实例*中的决策过程的*见解*。通过突出对模型预测有重大影响的特征或输入，局部解释器提供了对特定决策如何得出的一个
    glimpse。然而，需要注意的是，这些解释可能不适用于其他实例，也不能全面理解模型的整体行为。'
- en: The increasing demand for **trustworthy and transparent AI systems** is not
    only fueled by the widespread adoption of complex **black box models**, known
    for their accuracy but also for their limited interpretability. It is also motivated
    by the need to comply with **new regulations** aimed at safeguarding individuals
    against the misuse of data and data-driven applications, such as the Artificial
    Intelligence Act, the General Data Protection Regulation (GDPR), or the U.S. Department
    of Defense’s Ethical Principles for Artificial Intelligence.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 对**可信且透明的AI系统**的需求不断增加，这不仅受到以准确性著称但解释性有限的复杂**黑箱模型**广泛采用的推动，也受到遵守**新法规**的需求的激励，这些法规旨在保护个人免受数据及数据驱动应用滥用的影响，如人工智能法案、通用数据保护条例（GDPR）或美国国防部的人工智能伦理原则。
- en: By delving into the inner workings of AI systems and providing explanations
    for their outputs, researchers strive to demystify the black box, fostering a
    greater understanding and trust in the technology that is reshaping our world.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 通过深入了解AI系统的内部工作机制并提供输出解释，研究人员努力揭开黑箱的神秘面纱，促进对重新塑造我们世界的技术的更大理解和信任。
- en: '![](../Images/ce4c84c2f4a8d872c1cfbf3047e4cae3.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ce4c84c2f4a8d872c1cfbf3047e4cae3.png)'
- en: 'Black-box model vs. white-box explainable model. In the first pipeline, the
    butterfly is correctly classified as an insect, but the model does not provide
    an explanation for its prediction. In the second pipeline, instead, the inner
    mechanisms of the model are transparent and we can explain why the butterfly is
    classified as an insect. For intrinsically opaque models, a common strategy is
    to provide explanations through a surrogate white-box model trained to mimic the
    black-box model. In our example, this would be formulated as: “an interpretable
    model that behaves exactly as the black-box model says that the butterfly is an
    insect because it has six legs”.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 黑箱模型与白箱可解释模型。在第一个流程中，蝴蝶被正确分类为昆虫，但模型没有提供其预测的解释。在第二个流程中，模型的内部机制是透明的，我们可以解释为什么蝴蝶被分类为昆虫。对于本质上不透明的模型，一种常见策略是通过训练来模仿黑箱模型的代理白箱模型提供解释。在我们的例子中，这可以表述为：“一个可解释的模型，表现得与黑箱模型完全一致，认为蝴蝶是昆虫，因为它有六条腿。”
- en: Model-agnostic, Post-hoc, Local Explainers
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型不可知、事后、局部解释器
- en: In this blog post, our primary focus will be solely on local explainability,
    in particular on **model-agnostic**, **post-hoc**, **local** explainers. Model-agnostic
    explainers can be applied to any machine learning model, regardless of the underlying
    algorithm or architecture. The term “post-hoc” instead indicates that the explanations
    are generated after the model has made a prediction for a specific instance. In
    short, explainers with these properties can analyze any model’s decision-making
    process for a particular instance, highlighting the features or inputs that had
    the most significant influence on the prediction, without requiring to modify
    or retrain the model.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇博客文章中，我们将主要关注局部可解释性，特别是**模型不可知**、**事后**、**局部**解释器。模型不可知解释器可以应用于任何机器学习模型，无论其底层算法或架构如何。术语“事后”指的是解释在模型对特定实例做出预测后生成的。简而言之，具有这些属性的解释器可以分析任何模型在特定实例上的决策过程，突出对预测影响最大的特征或输入，而无需修改或重新训练模型。
- en: At the time of writing, SHAP [1] and LIME [2] are arguably the two most widely
    adopted model-agnostic techniques used for explaining the predictions of machine
    learning models.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在撰写时，SHAP [1] 和 LIME [2] 可以说是解释机器学习模型预测的两种最广泛采用的模型不可知技术。
- en: '**SHAP** (SHapley Additive exPlanations) is based on game theory and the concept
    of Shapley values. It provides explanations by assigning importance scores to
    each feature in a prediction, considering all possible combinations of features
    and their contributions to the prediction. SHAP values capture the average marginal
    contribution of a feature across all possible feature combinations, resulting
    in a more accurate and consistent explanation.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**SHAP**（SHapley Additive exPlanations）基于博弈论和Shapley值的概念。它通过为每个预测中的特征分配重要性分数来提供解释，考虑所有可能的特征组合及其对预测的贡献。SHAP值捕捉了特征在所有可能特征组合中的平均边际贡献，从而提供更准确和一致的解释。'
- en: '**LIME** (Local Interpretable Model-Agnostic Explanations) approximates the
    behavior of the underlying model around the prediction of interest by creating
    a simpler interpretable model, such as a linear model, in the local neighborhood
    of the instance. LIME explains the model’s prediction by weighting the importance
    of each feature based on its impact on the local model’s output.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**LIME**（局部可解释模型无关解释）通过在实例的局部邻域创建一个更简单的可解释模型，如线性模型，来近似基础模型在感兴趣预测周围的行为。LIME通过基于每个特征对局部模型输出的影响来加权特征的重要性，从而解释模型的预测。'
- en: Both SHAP and LIME have their particular strengths and limitations, but one
    main limitation that is common to both approaches is that they deliver explainability
    through **feature importance** and feature ranking. The importance of a feature
    represents just one aspect of the broader and more complex concept of explainable
    AI. In the clinical domain, for instance, physicians dealing with AI-driven tools
    often complain about the impossibility of checking and navigating the reasoning
    process that led the model to land on a specific decision, as they would with
    a medical guideline.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: SHAP和LIME都有各自的优点和局限性，但两者共有的一个主要局限性是它们通过**特征重要性**和特征排名来提供解释。特征的重要性只是可解释AI这一更广泛和复杂概念的一个方面。例如，在临床领域，处理AI驱动工具的医生常常抱怨无法检查和导航模型得出特定决策的推理过程，就像他们处理医疗指南一样。
- en: AraucanaXAI
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: AraucanaXAI
- en: AraucanaXAI [3] was born to address clinicians’ complaints about traditionally
    used XAI approaches but can be extended to any other scenario where having decision
    rules is preferable. The AraucanaXAI framework proposes a novel methodological
    approach for generating explanations of the predictions of a generic ML model
    for a single instance using **decision trees** to provide **explanations in the
    form of a decisional process**. Advantages of the proposed XAI approach include
    improved fidelity to the original model, the ability to deal with non-linear decision
    boundaries, and native support for both classification and regression problems.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: AraucanaXAI [3] 诞生于解决临床医生对传统XAI方法的投诉，但也可以扩展到任何其他需要决策规则的场景。AraucanaXAI框架提出了一种新颖的方法论，用于生成通用ML模型对单个实例的预测解释，使用**决策树**提供**决策过程形式的解释**。所提出的XAI方法的优点包括对原始模型的改进忠实度、处理非线性决策边界的能力，以及对分类和回归问题的原生支持。
- en: As for SHAP and LIME, also AraucanaXAI is available as a [Python package](https://pypi.org/project/araucanaxai/)
    that can be easily installed via PyPI.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 至于SHAP和LIME，AraucanaXAI同样可以作为一个[Python包](https://pypi.org/project/araucanaxai/)轻松通过PyPI安装。
- en: '[](https://github.com/detsutut/AraucanaXAI?source=post_page-----8ee79dabdb1a--------------------------------)
    [## GitHub - detsutut/AraucanaXAI: Tree-based local explanations of machine learning
    model predictions'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '[## GitHub - detsutut/AraucanaXAI: 基于树的机器学习模型预测的局部解释](https://github.com/detsutut/AraucanaXAI?source=post_page-----8ee79dabdb1a--------------------------------)'
- en: Increasingly complex learning methods such as boosting, bagging and deep learning
    have made ML models more accurate…
  id: totrans-28
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 日益复杂的学习方法，如提升（boosting）、集成（bagging）和深度学习，已经使ML模型更加准确……
- en: github.com](https://github.com/detsutut/AraucanaXAI?source=post_page-----8ee79dabdb1a--------------------------------)
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '[github.com](https://github.com/detsutut/AraucanaXAI?source=post_page-----8ee79dabdb1a--------------------------------)'
- en: How does it work?
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的？
- en: The algorithm is relatively simple. Given a single instance *x:*
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 算法相对简单。给定单个实例 *x:*
- en: Compute *D* = dist(*x*,*z*) for each element *z* of the training set. The default
    distance metric is the Gower distance, compatible with mixed-type variables.
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算 *D* = dist(*x*,*z*) 对于训练集的每个元素 *z*。默认的距离度量是Gower距离，兼容混合类型变量。
- en: Define subset *T_n* as the closest *N* elements to *x* (i.e., the neighborhood
    of *x*).
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义子集 *T_n* 为最接近 *x* 的 *N* 个元素（即 *x* 的邻域）。
- en: Augment the neighborhood *T_n* with SMOTE oversampling (optional). This makes
    the local region we want to inspect more dense and balanced.
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用SMOTE过采样（可选）来扩展邻域 *T_n*。这使得我们要检查的局部区域更加密集和平衡。
- en: Re-label the samples of *T_n* (or *T_n* ∪ *S*, the samples generated with oversampling)
    with the class *y_hat* predicted by the predictive function *f* of the black-box
    classifier. Define the explainer set *E* as *T_n* ∪ *S* ∪ *x*. Remember that the
    target of a surrogate model is not to maximize the predictive performance, but
    to have the same predictive behavior as the original model. That’s why we are
    interested in *y_hat* and not *y*.
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 用黑箱分类器的预测函数*f*预测的类别*y_hat*重新标记*T_n*（或*T_n* ∪ *S*，即通过过采样生成的样本）的样本。将解释集*E*定义为*T_n*
    ∪ *S* ∪ *x*。请记住，代理模型的目标不是最大化预测性能，而是具有与原始模型相同的预测行为。这就是我们关注*y_hat*而非*y*的原因。
- en: Train the decision tree *e* on *E*. Optionally prune it.
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在*E*上训练决策树*e*。可选地，进行剪枝。
- en: Navigate *e* from the root node to the leaf node corresponding to *x* to get
    the rule set.
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从根节点到对应于*x*的叶节点导航*e*以获取规则集。
- en: '![](../Images/15c5089e4d6215a228845a7bcf5f7806.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/15c5089e4d6215a228845a7bcf5f7806.png)'
- en: 'Step-by-step visualization of the AraucanaXAI algorithm. (note: re-labeling
    step not displayed)'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: AraucanaXAI算法的逐步可视化。（注：重新标记步骤未显示）
- en: 'Use case example: ALS mortality prediction'
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 用例示例：ALS死亡率预测
- en: Now that we know more about this new XAI approach, when shall we use it? As
    stated at the beginning of this post, **AraucanaXAI is born to address clinicians’
    needs**, providing explanations in the form of a navigable tree or a set of hierarchical
    rules that are easy to compare with guidelines and established medical knowledge.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们对这种新的XAI方法了解更多了，我们什么时候使用它？正如本文开头所述，**AraucanaXAI旨在满足临床医生的需求**，以可导航的树或一组与指南和既定医学知识易于比较的层次化规则的形式提供解释。
- en: AraucanaXAI has been recently employed to enhance complex prediction models
    designed to predict mortality for **Amyotrophic Lateral Sclerosis** (ALS) patients
    based on observations carried out over a period of 6 months. Predicting ALS progression
    is a challenging problem requiring complex models and many features, including
    the administration of questionnaires to stratify the severity of ALS. AraucanaXAI
    can help clinicians to break down the model’s reasoning into simpler-yet-truthful
    rules, usually to better understand why the model disagrees with them.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: AraucanaXAI最近被用来提升复杂预测模型，这些模型旨在基于6个月的观察预测**肌萎缩侧索硬化症**（ALS）患者的死亡率。预测ALS进展是一个具有挑战性的问题，需要复杂的模型和许多特征，包括通过问卷来分层ALS的严重程度。AraucanaXAI可以帮助临床医生将模型的推理分解为更简单但真实的规则，通常是为了更好地理解模型为何与他们的观点不一致。
- en: 'In the example below, for instance, the ground truth says that the ALS patient
    will die within six months, while a neural network predicts otherwise. The set
    of rules highlighted by AraucanaXAI can help in understanding the model’s point
    of view: the onset date happened 3 years before the first ALSFRS-R questionnaire
    submission (T0), the progression slope is less than 0.35 (i.e., according to the
    questionnaire, the patient is worsening slowly), the diagnosis date is more than
    8 months before T0, and the normalized score for “turning in bed and adjusting
    bed clothes” is low. Overall, this clinical picture is not that bad and this led
    the model to think that the patient would still be alive six months after.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在下面的例子中，实际情况表明ALS患者将在六个月内死亡，而神经网络则预测相反。AraucanaXAI突出的规则集可以帮助理解模型的观点：发病日期发生在第一次ALSFRS-R问卷提交（T0）前3年，进展斜率小于0.35（即，根据问卷，患者恶化缓慢），诊断日期在T0之前超过8个月，“翻身和调整床单”
    的归一化评分较低。总体而言，这一临床图景并没有那么糟糕，这使得模型认为患者在六个月后仍然会活着。
- en: '![](../Images/a62d055bb111044b058347f0d9528628.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a62d055bb111044b058347f0d9528628.png)'
- en: Explanations for mortality prediction of a specific ALS patient, provided by
    SHAP (left), LIME (center), and AraucanaXAI (right)
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 针对特定ALS患者的死亡预测解释，由SHAP（左）、LIME（中）、和AraucanaXAI（右）提供
- en: Conclusion and Future Work
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结论与未来工作
- en: AraucanaXAI has proven to be a **promising approach for XAI for individual patients
    in healthcare**, although the same strategy can be generalized to any other field
    where breaking down explanations as hierarchical rules constitutes an added value
    for the decision maker.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管同样的策略可以推广到其他领域，但AraucanaXAI已经被证明是**在医疗保健中针对个体患者的XAI的有前途的方法**，其中将解释分解为层次化规则为决策者提供了额外的价值。
- en: However, several limitations are still untackled. Firstly, the healthcare field
    produces an ever-increasing amount of unstructured data, but AraucanaXAI currently
    works on tabular data. To be employed in the clinical practice, AraucanaXAI should
    be upgraded to deal also with text and images, which are essential assets for
    medical institutions. Secondly, the evaluation of **what constitutes a “good”
    explanation for a user cannot be thoroughly assessed without clearly defined metrics**
    (which is currently an acknowledged gap in the XAI literature) and direct involvement
    of the physician users themselves in a properly designed evaluation study. Such
    studies constitute future work worth pursuing, with the potential to benefit the
    explainable AIM community at large. Finally, AraucanaXAI’s way of presenting the
    generated explanations relies on scikit-learn facilities for decision tree visualization,
    which is limited and should be improved.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '-   然而，仍然存在一些未解决的局限性。首先，医疗领域产生了越来越多的非结构化数据，但AraucanaXAI目前仅处理表格数据。为了在临床实践中使用，AraucanaXAI需要升级以处理文本和图像，这对医疗机构至关重要。其次，**什么构成对用户的“良好”解释目前没有明确的度量标准**（这是目前XAI文献中已知的空白）以及医生用户本身的直接参与，这需要在设计良好的评估研究中进行。此类研究构成了未来值得追求的工作，具有潜在的好处可以造福可解释的AI社区。最后，AraucanaXAI展示生成的解释的方式依赖于scikit-learn的决策树可视化功能，该功能有限，需要改进。'
- en: If you are interested in helping with AraucanaXAI, check the G[itHub repository](https://github.com/bmi-labmedinfo/araucana-xai)
    and **become a contributor**! Extension/improvement will be properly credited.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '-   如果你有兴趣帮助AraucanaXAI，查看[G[itHub 仓库](https://github.com/bmi-labmedinfo/araucana-xai)
    并**成为贡献者**！扩展/改进将得到适当的认可。'
- en: References
  id: totrans-50
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] [S. M. Lundberg and S.-I. Lee — A Unified Approach to Interpreting Model
    Predictions](https://arxiv.org/abs/1705.07874)'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] [S. M. Lundberg 和 S.-I. Lee — 统一模型预测解释方法](https://arxiv.org/abs/1705.07874)'
- en: '[2] [M. T. Ribeiro, S. Singh, and C. Guestrin — “Why Should I Trust You?”:
    Explaining the Predictions of Any Classifier](https://arxiv.org/abs/1602.04938)'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] [M. T. Ribeiro, S. Singh 和 C. Guestrin — “我为什么要相信你？”：解释任何分类器的预测](https://arxiv.org/abs/1602.04938)'
- en: '[3] [E. Parimbelli, T.M. Buonocore, G. Nicora, W. Michalowski, S. Wilk, R.
    Bellazzi — Why did AI get this one wrong? Tree-based explanations of machine learning
    model predictions](https://www.sciencedirect.com/science/article/pii/S0933365722002238)'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] [E. Parimbelli, T.M. Buonocore, G. Nicora, W. Michalowski, S. Wilk, R.
    Bellazzi — 为什么人工智能在这个问题上出错了？基于树的机器学习模型预测解释](https://www.sciencedirect.com/science/article/pii/S0933365722002238)'
- en: '[4] [T.M. Buonocore, G. Nicora, A. Dagliati, E. Parimbelli— Evaluation of XAI
    on ALS 6-months mortality prediction](https://www.researchgate.net/profile/Tommaso-Buonocore/publication/362761796_Evaluation_of_XAI_on_ALS_6-months_mortality_prediction/links/62fe1002eb7b135a0e422dfd/Evaluation-of-XAI-on-ALS-6-months-mortality-prediction.pdf)'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] [T.M. Buonocore, G. Nicora, A. Dagliati, E. Parimbelli— XAI在ALS 6个月死亡率预测中的评估](https://www.researchgate.net/profile/Tommaso-Buonocore/publication/362761796_Evaluation_of_XAI_on_ALS_6-months_mortality_prediction/links/62fe1002eb7b135a0e422dfd/Evaluation-of-XAI-on-ALS-6-months-mortality-prediction.pdf)'
- en: '*If not mentioned otherwise, images are original contributions of the author.*'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '*如果未另有说明，图片均为作者原创贡献。*'
- en: Please leave your thoughts in the comments section and share if you find this
    helpful! And if you like what I do, you can now show your support by buying me
    a coffee for a [few extra hours of autonomy](https://www.buymeacoffee.com/detsutut)
    ☕
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '-   请在评论区留下你的想法，并分享如果你觉得这有帮助！如果你喜欢我的工作，你现在可以通过为我买杯咖啡来表示支持，[几小时的自主时间](https://www.buymeacoffee.com/detsutut)
    ☕'
- en: '[](https://www.linkedin.com/in/tbuonocore?source=post_page-----8ee79dabdb1a--------------------------------)
    [## Tommaso Buonocore — Ph.D. Student — Big Data & Biomedical Informatics — ICS
    Maugeri SpA Società…'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://www.linkedin.com/in/tbuonocore?source=post_page-----8ee79dabdb1a--------------------------------)
    [## Tommaso Buonocore — 博士生 — 大数据与生物医学信息学 — ICS Maugeri SpA Società…'
- en: Biomedical Engineer and AI enthusiast, currently researching NLP solutions to
    improve healthcare-based prediction tasks…
  id: totrans-58
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '-   生物医学工程师和人工智能爱好者，目前正在研究自然语言处理解决方案，以改善基于医疗的预测任务…'
- en: www.linkedin.com](https://www.linkedin.com/in/tbuonocore?source=post_page-----8ee79dabdb1a--------------------------------)
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: www.linkedin.com](https://www.linkedin.com/in/tbuonocore?source=post_page-----8ee79dabdb1a--------------------------------)
