- en: Unveiling the Secrets of Log-Loss
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 揭示对数损失的秘密
- en: 原文：[https://towardsdatascience.com/secrets-of-log-loss-84c668f4024a](https://towardsdatascience.com/secrets-of-log-loss-84c668f4024a)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/secrets-of-log-loss-84c668f4024a](https://towardsdatascience.com/secrets-of-log-loss-84c668f4024a)
- en: Math, theory, and intuition for machine learning engineers
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数学、理论和直观理解，专为机器学习工程师准备
- en: '[](https://jvision.medium.com/?source=post_page-----84c668f4024a--------------------------------)[![Joseph
    Robinson, Ph.D.](../Images/3117b65a4e10752724585d3457343695.png)](https://jvision.medium.com/?source=post_page-----84c668f4024a--------------------------------)[](https://towardsdatascience.com/?source=post_page-----84c668f4024a--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----84c668f4024a--------------------------------)
    [Joseph Robinson, Ph.D.](https://jvision.medium.com/?source=post_page-----84c668f4024a--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://jvision.medium.com/?source=post_page-----84c668f4024a--------------------------------)[![Joseph
    Robinson, Ph.D.](../Images/3117b65a4e10752724585d3457343695.png)](https://jvision.medium.com/?source=post_page-----84c668f4024a--------------------------------)[](https://towardsdatascience.com/?source=post_page-----84c668f4024a--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----84c668f4024a--------------------------------)
    [Joseph Robinson, Ph.D.](https://jvision.medium.com/?source=post_page-----84c668f4024a--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----84c668f4024a--------------------------------)
    ·12 min read·Nov 23, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: · 发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----84c668f4024a--------------------------------)
    · 12 分钟阅读 · 2023年11月23日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: 'Let’s examine log-loss and demystify this critical machine learning objective:
    its mathematical rigor, theoretical foundations, and intuitive aspects. This blog
    will provide deep insights to optimize your models more effectively and understand
    log-loss for real-world applications!'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们深入探讨对数损失，并揭开这个关键机器学习目标的神秘面纱：它的数学严谨性、理论基础和直观方面。这个博客将提供深入见解，以更有效地优化你的模型，并理解对数损失在现实应用中的意义！
- en: '![](../Images/c2e29cce0d924ea863ce6b7ee283dc68.png)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c2e29cce0d924ea863ce6b7ee283dc68.png)'
- en: '**Log-Loss Curves**: Demonstrating the increasing penalty as predicted probabilities
    diverge from true labels. The steeper the curve, the higher the cost of being
    wrong. Plot generated by the author.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '**对数损失曲线**：展示了预测概率与真实标签偏离时惩罚的增加。曲线越陡峭，错误的代价越高。图表由作者生成。'
- en: Table of Contents
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 目录
- en: · [Introduction](#0f73)
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: · [介绍](#0f73)
- en: · [The Basics of Log-Loss](#0d97)
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: · [对数损失的基础](#0d97)
- en: · [The Mathematics Behind Log-Loss](#9460)
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: · [对数损失背后的数学](#9460)
- en: · [The Theory Underpinning Log-Loss](#58b4)
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: · [支撑对数损失的理论](#58b4)
- en: · [Intuitive Understanding of Log-Loss](#0731)
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: · [对数损失的直观理解](#0731)
- en: · [Practical Implications for Machine Learning](#3ac3)
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: · [机器学习的实际应用](#3ac3)
- en: · [Optimizing Models](#6399)
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: · [优化模型](#6399)
- en: · [Common Pitfalls and How to Avoid Them](#e72f)
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: · [常见陷阱及其避免方法](#e72f)
- en: · [Conclusion](#e400)
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: · [结论](#e400)
- en: Introduction
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: The enigmatic log loss is as intriguing as it is pivotal. It stands at the crux
    of machine learning, bathed in mathematical elegance. Besides, log-loss is at
    the heart of probabilistic classifiers; it entices us with promises of more powerful,
    accurate models.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 神秘的对数损失既引人入胜又至关重要。它处于机器学习的核心，沐浴在数学的优雅之中。此外，对数损失是概率分类器的核心；它以更强大、更准确的模型为承诺吸引我们。
- en: But let’s not dally in awe and wonder. We have work to do!
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 但让我们不要在惊叹和惊奇中耽搁。我们还有工作要做！
- en: Why should you, as a machine learning engineer, work through the mathematical
    and conceptual whirlpool that is log-loss? Simple. **Log-loss is a Swiss Army
    knife.** A deeper understanding allows you to scrutinize the nuances of a classifier’s
    performance beyond mere accuracy. Hence, the log loss isn’t just another number—it's
    a litmus test for the robustness of your machine-learning model, allowing you
    to fine-tune and optimize with a level of nuance that other metrics can only aspire
    to.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么作为机器学习工程师的你应该深入研究对数损失这一数学和概念的漩涡？很简单。**对数损失是一把瑞士军刀。** 更深刻的理解使你能够超越简单的准确度，细致审视分类器性能的细微差别。因此，对数损失不仅仅是一个数字——它是你机器学习模型稳健性的试金石，让你可以以其他指标难以达到的细腻程度进行微调和优化。
- en: Our goal in this blog is to fathom the intricate layers of log loss. Our itinerary
    spans the rigor of mathematical derivations, untangling the theoretical foundations
    knotted deep within, and venturing into intuition, finding the relatable in the
    abstract. We’ll explore log-loss basics, break down its mathematical components,
    and unravel its connection to information theory. Real-world applications and
    case studies are used to highlight the practical power standing this metric deeply
    under. We’ll discuss some pitfalls—those tricky nuances that can trip you up—and
    how to sidestep them gracefully. Finally, we will use visualizations to better
    understand this mathematical construct in theory and practice.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这个博客中的目标是深入理解对数损失的复杂层次。我们的行程包括数学推导的严谨性，解开深藏其间的理论基础，探索直观，找出抽象中的可关联性。我们将探讨对数损失的基础，分解其数学成分，并揭示它与信息理论的关系。通过现实世界的应用和案例研究，强调这个指标的实际力量。我们将讨论一些陷阱——那些容易让你绊倒的细微差别——以及如何优雅地避免它们。最后，我们将使用可视化来更好地理解这一数学构造的理论和实践。
- en: Ready? Let’s plunge in.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 准备好了吗？让我们深入探讨。
- en: The Basics of Log-Loss
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 对数损失基础
- en: 'Let’s delve into the nitty-gritty: what is log loss, and when is it the knight
    in shining armor for your machine-learning escapades?'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们深入探讨：什么是对数损失，何时它是你机器学习冒险中的骑士？
- en: Definition and Formula
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 定义与公式
- en: 'Log loss, formally known as logistic loss or logarithmic loss, is a performance
    metric for classification models that output probabilities: smaller means better,
    with a perfect model having a log loss of zero. It is a favorite in scenarios
    requiring probabilistic outcomes rather than hard classifications. Log loss quantifies
    how far off your predictions are from the actual outcomes in a way that is more
    telling than mere accuracy.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 对数损失，正式称为逻辑损失或对数损失，是一个用于分类模型的性能指标，模型输出概率：数值越小越好，完美模型的对数损失为零。它在需要概率结果而非硬分类的场景中很受欢迎。对数损失量化了你的预测与实际结果之间的偏差，比单纯的准确率更具说明性。
- en: 'Mathematically, the log-loss for a binary classifier is often expressed as:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在数学上，二分类器的对数损失通常表示为：
- en: '![](../Images/c727533fec1cf30605a4a9283c3dd2ac.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c727533fec1cf30605a4a9283c3dd2ac.png)'
- en: Here, *N* is the number of samples, y_i is the true label of the i^{th} sample
    and p_i is the predicted probability of the i^{th} sample being in class 1.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*N* 是样本的数量，y_i 是第 i^{th} 样本的真实标签，p_i 是第 i^{th} 样本被预测为类别 1 的概率。
- en: 'Here’s a simple Python code snippet using NumPy to calculate log loss:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个简单的 Python 代码片段，使用 NumPy 计算对数损失：
- en: '[PRE0]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Output:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：
- en: '[PRE1]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: When to Use Log-Loss
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 何时使用对数损失
- en: 'Log loss is particularly helpful in problems demanding a nuanced interpretation
    of outcomes. Think medical diagnosis, where it is not just the label (i.e., sick
    or healthy), but the probability of being sick, which carries immense importance.
    Or recommendation systems: you are not just categorizing likes or dislikes but
    deciding whether a user might click on a recommended item.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 对数损失在需要细致解读结果的问题中特别有用。比如医学诊断，不仅仅是标签（即生病或健康），而是生病的概率，这具有巨大的重要性。或者推荐系统：你不仅仅是对喜欢或不喜欢进行分类，还要决定用户是否可能点击推荐的项目。
- en: Log loss differs from other metrics, which paint broad strokes but miss the
    nuances (e.g., accuracy). Accuracy, for example, counts the number of correctly
    predicted events but says nothing about confidence. Then there is the F1-score,
    a harmonic mean of precision and recall, which is exceptional for imbalanced datasets.
    But, alas, it too offers no peek into the graded confidence of classifications.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 对数损失与其他指标不同，后者往往笼统却忽略了细微差别（例如准确率）。准确率例如统计正确预测的事件数量，但并未说明置信度。还有 F1 分数，它是精度和召回率的调和均值，适用于不平衡数据集。然而，它也无法窥见分类的分级置信度。
- en: Log loss, on the other hand, penalizes both being wrong and the model’s confidence
    in your wrongness. Its sensitivity to probabilities makes it an invaluable metric
    for situations demanding a more discerning evaluation.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，对数损失惩罚错误的同时也惩罚模型对错误的置信度。它对概率的敏感性使它成为在需要更为精细评估的情况下的宝贵指标。
- en: '![](../Images/a02a3de431edcde790419205fbb624c5.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a02a3de431edcde790419205fbb624c5.png)'
- en: '**Log-Loss Landscape:** This 3D plot unravels the intricacies of the log-loss
    function. Observe how it dramatically peaks when predictions stray far from true
    labels, visually emphasizing the steeper penalties for being confidently wrong.
    Also notice that log-loss penalizes false positives and negatives differently.
    Plot generated by the author.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '**对数损失景观**：该3D图揭示了对数损失函数的复杂性。观察当预测偏离真实标签时，对数损失如何显著增加，直观地强调了自信错误预测的更陡罚款。同时请注意，对数损失对假阳性和假阴性有不同的惩罚。图表由作者生成。'
- en: Hence, log-loss is more than a metric; it is a storytelling tool for the performance
    of your model, one that plays well when stakes are high and the nuances are abundant.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，对数损失不仅仅是一个指标；它是衡量模型表现的叙事工具，尤其在风险高、细节丰富时效果显著。
- en: The Mathematics Behind Log-Loss
  id: totrans-43
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 对数损失背后的数学
- en: Mathematics is the language of the universe and the bedrock on which the enigma
    of log loss exists.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 数学是宇宙的语言，也是对数损失之谜存在的基础。
- en: Derivation of the Formula
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 公式的推导
- en: In the realm of statistics, we often begin with the concept of likelihood, a
    measure of how well a model explains the observed data. For a binary classification,
    the likelihood *L* can be expressed as follows.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在统计学领域，我们通常从似然性这一概念开始，它是衡量模型如何解释观察数据的指标。对于二分类问题，似然性*L*可以表达如下。
- en: '![](../Images/67eaa2c97afc098d2722b3b69c0b8002.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/67eaa2c97afc098d2722b3b69c0b8002.png)'
- en: This formulation is intuitive yet computationally cumbersome. A product of probabilities
    can result in an underflow, especially as *N* grows large.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 这个公式直观但计算起来繁琐。概率的乘积可能会导致下溢，特别是当*N*很大时。
- en: '![](../Images/2589721838b8d6dc5073e473dc7415b8.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2589721838b8d6dc5073e473dc7415b8.png)'
- en: '**Likelihood Surface Visualization**: This 3D plot captures the relationship
    between the number of successes and the estimated probability of success. Peaks
    in the surface represent higher likelihoods, emphasizing how our confidence in
    predicting success varies across different probability estimates and observed
    successes. Plot generated by the author.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '**似然面可视化**：该3D图捕捉了成功次数与估计成功概率之间的关系。表面上的峰值表示较高的似然性，强调了在不同概率估计和观察到的成功中，我们对预测成功的信心如何变化。图表由作者生成。'
- en: 'Enter the natural logarithm; we use the natural log to transform our likelihood
    into a sum, which we call the log-likelihood:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 引入自然对数；我们使用自然对数将似然转化为和，我们称之为对数似然：
- en: '![](../Images/c4c83e221a57675f6d0644a0b849b49d.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c4c83e221a57675f6d0644a0b849b49d.png)'
- en: 'Here lies the core of the log-loss formula: the negated average of this log-likelihood:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是对数损失公式的核心：该对数似然的负平均：
- en: '![](../Images/6ecb87c6150b69611f812f2d8206aadd.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6ecb87c6150b69611f812f2d8206aadd.png)'
- en: Here, we use the negative to convert our maximization problem (i.e., maximizing
    log-likelihood) to a minimization one—a more familiar terrain in optimization—a
    concept conceptualized in the following figure.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们使用负号将最大化问题（即最大化对数似然）转化为最小化问题——这是优化中更为熟悉的领域——这一概念在下图中得以阐释。
- en: '![](../Images/b06b825ee5c895a625b1e531fa7e4221.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b06b825ee5c895a625b1e531fa7e4221.png)'
- en: 'The Behavior of Logarithm in Log-Loss: a dual-axis plot showing how both the
    logarithm and negative logarithm behave, key components in log-loss. Plot generated
    by the author.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 对数在对数损失中的表现：一个双轴图展示了对数和负对数的行为，这两个是对数损失中的关键组成部分。图表由作者生成。
- en: Deep Dive into the Math
  id: totrans-58
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数学深度探讨
- en: '*So, why logarithms?* You may wonder. Logarithms serve a deeper purpose than
    merely a mathematical convenience: the logarithm function is monotonic, preserving
    the order between probabilities. Plus, it **amplifies the penalty for wrong but
    confident predictions**. For instance, if you predict a probability of 0.01 when
    the valid label is 1, the logarithm will boost your loss, urging you to rethink
    your misplaced confidence.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '*那么，为何使用对数？* 你可能会问。对数不仅仅是数学上的方便工具：对数函数是单调的，保持了概率之间的顺序。而且，它**放大了错误但自信预测的惩罚**。例如，如果你预测的概率为0.01，而真实标签为1，那么对数将增加你的损失，促使你重新思考你错误的自信。'
- en: Log-loss is **sensitive and resistant to outliers**, a paradox that intrigues
    it. Predict an extreme probability near 0 or 1; if you’re wrong, the log loss
    becomes punitive, pulling no punches. On the other hand, it is less prone to outlier
    skewing than other metrics (e.g., mean squared error, giving disproportionate
    weight to extreme values).
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 对数损失**对异常值敏感且具有抗干扰性**，这是一个让人着迷的悖论。预测一个接近0或1的极端概率；如果你错了，对数损失会变得惩罚性十足，不留情面。另一方面，它比其他度量标准（例如，均方误差，对极端值给予不成比例的权重）更不容易受到异常值的影响。
- en: 'Here’s a quick Python code snippet demonstrating the impact of an outlier:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个快速的 Python 代码片段，演示了异常值的影响：
- en: '[PRE2]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Output:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：
- en: '[PRE3]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Notice the log loss with an outlier doesn’t veer off dramatically from the one
    without, showcasing its relative resistance to extreme values.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，带有异常值的对数损失与没有异常值的情况相比，并没有显著偏离，展示了它对极端值的相对抗干扰性。
- en: The mathematics of log-loss is more than just a series of abstract symbols;
    it is a narrative. It speaks of likelihoods and logarithms, balancing confidence
    and penalty.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 对数损失的数学不仅仅是一系列抽象符号；它是一种叙事。它讲述了可能性和对数，平衡了信心和惩罚。
- en: The Theory Underpinning Log-Loss
  id: totrans-67
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 对数损失的理论基础
- en: And now, let’s step beyond the mathematics to peer into the theoretical realm.
    It is one thing to wrestle with equations; it’s another to appreciate the intellectual
    underpinnings and ask, “Why does this metric even exist?” Are you ready for this
    excursion into the theoretical depths?
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们超越数学，深入理论领域。与方程式搏斗是一回事，理解其知识基础并问道：“这个度量标准为什么存在？”这又是另一回事。你准备好进入理论的深度探讨了吗？
- en: Probabilistic Foundations
  id: totrans-69
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 概率基础
- en: In its essence, log-loss is inextricably connected to information theory—a field
    that quantifies information. Information theory says, “Tell me something I don’t
    know, and you give me information.” The log loss is a measure of surprise (i.e.,
    [uncertainty](https://en.wikipedia.org/wiki/Entropy_(information_theory))). The
    further your model’s predictions deviate from the actual outcomes, the more “surprised”
    one would be (i.e., the more information that is conveyed).
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 从本质上讲，对数损失与信息理论紧密相连——一个量化信息的领域。信息理论说，“告诉我一些我不知道的事情，你就给了我信息。”对数损失是惊讶的度量（即，[不确定性](https://en.wikipedia.org/wiki/Entropy_(information_theory))）。你的模型的预测越是偏离实际结果，人们会越“惊讶”（即传达的信息越多）。
- en: 'Thus, the concept of entropy quantifies this information content. For a single
    event with probability *p*, entropy *H* is:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，熵的概念量化了信息内容。对于一个概率为 *p* 的单一事件，熵 *H* 为：
- en: '![](../Images/4325db69bc592bd1dbcdd3fbbebf9ade.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4325db69bc592bd1dbcdd3fbbebf9ade.png)'
- en: 'Taking it a step further, let us look at cross-entropy, which measures the
    distance between the true distribution *y* and the predicted distribution *p*.
    For binary classification, the cross-entropy is:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 更进一步，让我们看看交叉熵，它衡量真实分布 *y* 和预测分布 *p* 之间的距离。对于二分类，交叉熵为：
- en: '![](../Images/0b982ad410002371de3922481f454e17.png)![](../Images/df0a0f577e532249d1232670f5a8e9ff.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0b982ad410002371de3922481f454e17.png)![](../Images/df0a0f577e532249d1232670f5a8e9ff.png)'
- en: This graph visualizes how entropy and cross-entropy values change with varying
    true probabilities (p). The entropy decreases as the probability approaches 0
    or 1, indicating more certainty in the event outcome. Cross-entropy is displayed
    as a consistent line, emphasizing its role in measuring the difference between
    two probability distributions. Plot generated by the author.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 这张图展示了熵和交叉熵值如何随着真实概率 (p) 的变化而变化。熵随着概率接近0或1而减少，表示事件结果的确定性增加。交叉熵显示为一条稳定的线，强调它在测量两个概率分布之间差异中的作用。图表由作者生成。
- en: And lo, we find ourselves back at our familiar friend, the log loss, which is
    the average cross-entropy over all instances.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 于是，我们发现自己又回到了熟悉的对数损失，这是所有实例上交叉熵的平均值。
- en: '![](../Images/84670d3c0d528e7dc10855b7df9bcf79.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/84670d3c0d528e7dc10855b7df9bcf79.png)'
- en: 'Distribution comparisons between Entropy and Log-Loss: The histograms highlight
    the frequency distribution of both metrics with overlaying curves representing
    their expected distributions. While entropy shows a concentration towards higher
    values, log-loss showcases a more spread-out distribution with a peak around lower
    values, illustrating their inherent differences and how data behaves under each
    metric. Plot generated by the author.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 熵与对数损失的分布比较：直方图突出显示了这两种指标的频率分布，并叠加了表示其期望分布的曲线。熵显示了向高值集中，而对数损失则展示了更分散的分布，峰值在低值附近，体现了它们的固有差异以及数据在每种指标下的表现。图由作者生成。
- en: 'In Python, cross-entropy can be calculated using the following:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在Python中，可以使用以下方式计算交叉熵：
- en: '[PRE4]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Output:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：
- en: '[PRE5]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Philosophical Aspects
  id: totrans-83
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 哲学方面
- en: Using log loss is not without its assumptions. It **assumes that your predictions
    are probabilities**, falling between 0 and 1, and that **the labels are genuinely
    binary**. If you veer off from these conditions, the metric might lead you astray,
    spouting numbers that are difficult to interpret as meaningful.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 使用对数损失并非没有假设。它**假设你的预测是概率**，在0和1之间，并且**标签是真正的二元**。如果偏离这些条件，指标可能会误导你，产生难以解读的数字。
- en: At its core, log-loss measures the uncertainty in your model’s predictions.
    The philosophical idea it embodies is that of calibrated probability. In an ideal
    world, a prediction made with 90% confidence should be correct 90% of the time.
    Hence, log-loss keeps your model honest, penalizing overconfident wrong answers
    and underconfident right ones.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 从本质上讲，对数损失衡量的是模型预测中的不确定性。它体现的哲学思想是校准概率。在理想的世界中，一个90%信心的预测应该在90%的时间内是正确的。因此，对数损失保持你的模型诚实，惩罚过于自信的错误答案和过于不自信的正确答案。
- en: '![](../Images/ce8e2741b3fb7450a4c1102681140fa5.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ce8e2741b3fb7450a4c1102681140fa5.png)'
- en: 'By understanding the zones where log-loss is sensitive and the nature of the
    curve, machine learning engineers can better grasp how their models might behave
    in practical, real-world conditions. **Primary Curve (Blue solid)**: shows how
    log-loss varies with the true probability of the positive class. **Perfect Calibration
    Curve (Red dashed)**: a hypothetical line indicating what log-loss would look
    like for a perfectly calibrated model. **Yellow Shaded Regions**: highlight where
    the log-loss function is susceptible to changes in the true probability. Minor
    variations in this area lead to significant spikes in log-loss. **Annotations
    and Text**: these provide additional insights, calling out specific points on
    the curve and making it easier to understand log-loss behavior. Plot generated
    by the author.'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 通过了解对数损失敏感的区域及曲线的性质，机器学习工程师可以更好地理解模型在实际环境中的表现。**主要曲线（蓝色实线）**：显示对数损失如何随正类的真实概率变化。**完美校准曲线（红色虚线）**：一个假设的线条，表示完美校准模型的对数损失应有的样子。**黄色阴影区域**：突出显示对数损失函数对真实概率变化的敏感性。在这个区域内的微小变化会导致对数损失的显著波动。**注释和文本**：提供了额外的见解，指出曲线上的特定点，使理解对数损失行为更加容易。图由作者生成。
- en: Intuitive Understanding of Log-Loss
  id: totrans-88
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 对数损失的直观理解
- en: It’s time to unshackle ourselves from the rigidity of formalism and explore
    the landscape of log loss with fresh eyes.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 是时候摆脱形式主义的束缚，以全新的视角探索对数损失的领域了。
- en: Analogies and Real-world Examples
  id: totrans-90
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 类比与现实世界的例子
- en: 'Think of log loss as the cost of a lie. Imagine you are betting on a horse
    race. The stronger your bet—the higher the probability you assign to a particular
    outcome—the more you stand to lose if wrong. What about a minor lie where you
    weren’t too sure to begin with? The contrary: less loss. A whopper of a lie when
    you were supremely confident. That loses more!'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 将对数损失视为谎言的代价。假设你在赛马中下注。你对某一结果赋予的概率越高，你如果判断错误损失的就越大。那如果是一个轻微的谎言，你本来就不太确定呢？相反，损失较少。如果是一个极大的谎言，而你非常有信心，那么损失就更大了！
- en: Returning to using it in recommendation systems, use the log loss as the annoyance
    level of your users. Recommend a rarely watched movie with high certainty, and
    you annoy your user; get it right, and you are a hero. In healthcare, consider
    a diagnostic test. Predict a disease the patient has with a low likelihood, and
    the consequences could be dire.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 回到推荐系统的应用中，使用对数损失作为用户的烦恼程度。以高度确定性推荐一部鲜有人观看的电影，你会让用户感到烦恼；若推荐正确，你则是英雄。在医疗保健中，考虑一下诊断测试。以低概率预测患者患有某种疾病，可能会导致严重后果。
- en: 'Here’s a Python example mimicking a basic healthcare diagnosis model:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是一个模仿基本医疗诊断模型的Python示例：
- en: '[PRE6]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Output:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：
- en: '[PRE7]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Visualizing Log-Loss
  id: totrans-97
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 视觉化对数损失
- en: One of the most effective ways to visualize log loss is through a graph that
    plots true labels against predicted probabilities. As you fall off the ideal diagonal
    line (where predicted probabilities match the proper labels), your log loss increases,
    visually signifying your model’s imperfections. On this graph, a perfect model
    would be a straight diagonal line from the bottom left to the top right corner.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 可视化对数损失的最有效方法之一是通过绘制真实标签与预测概率的图表。当你偏离理想的对角线（预测概率与真实标签匹配的地方）时，你的对数损失会增加，直观地显示出模型的缺陷。在这个图表上，完美的模型将是一条从左下角到右上角的直对角线。
- en: '![](../Images/0f71105a87f6f3e35b85d439787de160.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0f71105a87f6f3e35b85d439787de160.png)'
- en: '**Dissecting Log-Loss:** The dotted line perfectly matches true labels and
    predicted probabilities. The blue dots highlight the deviations of actual predictions
    from this ideal, illustrating the concept of log-loss, where greater distances
    from the line indicate higher prediction errors. Plot generated by the author.'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '**剖析对数损失：** 虚线完美地匹配真实标签和预测概率。蓝点突出了实际预测与这一理想之间的偏差，说明了对数损失的概念，其中离线距离越大表示预测误差越高。图表由作者生成。'
- en: Practical Implications for Machine Learning
  id: totrans-101
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习的实际影响
- en: It’s time for the rubber to meet the road. We’ve traversed the mathematical
    terrain and waded through intuitive lakes. But what of the practical soil under
    our feet? How does the understanding of log loss fertilize the gardens of machine
    learning projects?
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是时候将理论付诸实践了。我们已经穿越了数学领域，涉足了直观的湖泊。那么实际的土壤如何？对数损失的理解如何为机器学习项目的花园施肥？
- en: Optimizing Models
  id: totrans-103
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 优化模型
- en: '***How minimizing log loss can lead to a more robust model***'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '***如何通过最小化对数损失来获得更强健的模型***'
- en: 'Let’s get down to brass tacks. Minimizing log-loss is a process of model calibration.
    Think of it as tuning a musical instrument: the closer you get to the perfect
    note (i.e., the true label), the better your performance. When you minimize log
    loss, you’re telling your model to be less ‘surprised’ by the true outcomes, thus
    making more calibrated, reliable predictions.'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们切入正题。最小化对数损失是一个模型校准的过程。把它看作是调音：你越接近完美音符（即真实标签），你的表现就越好。当你最小化对数损失时，你是在告诉你的模型对真实结果“惊讶”得更少，从而做出更准确、可靠的预测。
- en: '***Techniques and strategies for optimization***'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '***优化的技术和策略***'
- en: 'Now, how do you water this plant? There are many approaches: gradient descent
    methods, hyperparameter tuning, ensemble techniques, etc. One widely used method
    is Grid Search in conjunction with cross-validation.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你如何给这个植物浇水？有很多方法：梯度下降法、超参数调整、集成技术等。一种广泛使用的方法是结合交叉验证的网格搜索。
- en: '[PRE8]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: By fine-tuning these dials, you can optimize the log loss, thereby creating
    a more robust model.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 通过微调这些拨轮，你可以优化对数损失，从而创建一个更强健的模型。
- en: Case Studies
  id: totrans-110
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 案例研究
- en: One groundbreaking application of log-loss optimization can be found in the
    healthcare industry, specifically in early cancer detection. By lowering the log-loss,
    the models were better at finding cancerous cells, which was essential for starting
    treatments early.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 对数损失优化的一个突破性应用可以在医疗行业找到，特别是在早期癌症检测中。通过降低对数损失，模型能更好地发现癌细胞，这对于早期开始治疗至关重要。
- en: Another case comes from the finance sector, where credit scoring models have
    been fine-tuned using log-loss as a performance metric. The result? More accurate
    risk profiles and smarter lending decisions.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个案例来自金融领域，信用评分模型已经通过使用对数损失作为性能指标进行了微调。结果？更准确的风险评估和更聪明的借贷决策。
- en: So, here we are—standing on the fertile soil where theory takes root as practice.
    Log-loss isn’t just a mathematical abstraction or a subject for intellectual debate;
    it’s a powerful, actionable lever that can shift the trajectory of machine learning
    projects from the ordinary to the extraordinary.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，我们现在站在理论与实践结合的沃土上。对数损失不仅仅是一个数学抽象或知识辩论的主题；它是一个强大、可操作的杠杆，可以将机器学习项目的轨迹从普通转向非凡。
- en: Common Pitfalls and How to Avoid Them
  id: totrans-114
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 常见陷阱及如何避免它们
- en: Numerical Stability
  id: totrans-115
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数值稳定性
- en: Numerical stability—or lack thereof—is the trap door many have fallen through.
    When calculating the logarithm of a probability, pushing that number closer and
    closer to zero can lead to numerical instability, causing havoc in the calculations.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 数值稳定性——或缺乏稳定性——是许多人跌入的陷阱。在计算概率的对数时，将那个数字不断推向零会导致数值不稳定，从而在计算中引发混乱。
- en: 'To mitigate this, one often applies a small epsilon ϵ to the predicted probabilities:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 为了缓解这个问题，通常会对预测的概率应用一个小的epsilon ϵ：
- en: '![](../Images/b919c2c650c56b2993f458f1c2003fef.png)![](../Images/f391ac87897862c2f74779df99a750aa.png)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b919c2c650c56b2993f458f1c2003fef.png)![](../Images/f391ac87897862c2f74779df99a750aa.png)'
- en: This plot offers a comprehensive look at how epsilon values can affect the stability
    of log-loss calculations, an important concept, especially when dealing with probabilities
    close to 0 or 1\. Plot generated by the author.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 这个图表全面展示了epsilon值如何影响对数损失计算的稳定性，这一点在处理接近0或1的概率时尤为重要。图表由作者生成。
- en: 'Here’s how you could modify a Python log-loss function:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以这样修改一个Python对数损失函数：
- en: '[PRE9]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Output:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：
- en: '[PRE10]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: When Not to Use Log-Loss
  id: totrans-124
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 什么时候不使用对数损失
- en: Now, even the sharpest knife isn’t helpful for every task. The same goes for
    log loss. In classification problems where the class imbalance is severe or when
    the cost of false positives and negatives varies dramatically, metrics like F1-score,
    precision, or recall may serve you better.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，即使是最锋利的刀子也不适用于每个任务。对数损失也是如此。在类别不平衡严重或假阳性和假阴性的成本差异很大的分类问题中，像F1-score、精度或召回率这样的指标可能更适合。
- en: '![](../Images/65409908c738acf662ae0015786407d7.png)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/65409908c738acf662ae0015786407d7.png)'
- en: 'Another valuable angle is how log-loss changes for balanced vs. imbalanced
    datasets. This is a crucial insight for machine learning engineers working on
    real-world problems with common class imbalances. In these histograms: Log-Loss
    Distribution for Balanced Dataset: The first plot shows how the log-loss values
    are distributed when the classes are balanced. The values tend to spread across
    a wider range, reflecting the model’s different levels of certainty. Log-Loss
    Distribution for Imbalanced Dataset: The second plot shows the distribution of
    log-loss values in an imbalanced dataset. Notice how the range is generally narrower,
    reflecting a model that might be overconfident in its predictions due to the imbalance.
    Understanding the nuances between balanced and imbalanced datasets in a log-loss
    context can help machine learning engineers tailor their model evaluation and
    adjustment strategies more effectively. Plot generated by the author.'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个有价值的角度是对比平衡数据集与不平衡数据集中的对数损失变化。这是一个关键洞察，尤其是对处理常见类别不平衡的实际问题的机器学习工程师来说。在这些直方图中：
    平衡数据集的对数损失分布：第一个图展示了类别平衡时对数损失值的分布。值倾向于分布在更广的范围内，反映出模型的不同确定性水平。不平衡数据集的对数损失分布：第二个图展示了不平衡数据集中对数损失值的分布。注意范围通常较窄，反映出由于不平衡，模型可能对其预测过于自信。理解平衡和不平衡数据集中对数损失的细微差别可以帮助机器学习工程师更有效地调整模型评估和调整策略。图表由作者生成。
- en: Another case? For multi-class problems with more than two labels, while one
    could generalize log-loss to such scenarios, it must often be more straightforward
    and interpretable. A metric like categorical cross-entropy or simple accuracy
    might be more effective in these situations.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种情况？对于具有两个以上标签的多类别问题，虽然可以将对数损失推广到这些情境中，但通常需要更加直接和易于解释。在这些情况下，像分类交叉熵或简单准确率这样的指标可能更有效。
- en: Conclusion
  id: totrans-129
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: Here, we stand at the summit of our intellectual trek through the intricate
    terrain of log loss. With theoretical and practical wisdom under our belts, we
    can revisit the trail we’ve blazed. Let us recap.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们站在对数损失复杂领域智力之旅的顶峰。掌握了理论和实践智慧后，我们可以重温我们所开辟的路径。让我们回顾一下。
- en: Understanding log loss is akin to mastering the art of tuning a complex instrument.
    It equips you with the discernment to calibrate your probabilistic models, generating
    reliable and interpretable predictions. In the era of data, where models influence
    everything from healthcare to finance, such mastery isn’t just a nicety—it's a
    necessity.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 理解对数损失就像掌握调节复杂仪器的艺术。它让你具备了调整概率模型的能力，生成可靠且易于解释的预测。在数据时代，模型影响着从医疗保健到金融的方方面面，这种掌握不仅是美好之事，而是必需的。
- en: Knowledge is most fruitful when it’s applied. It’s time to roll up those sleeves
    and dig your hands into the rich soil of your projects. Tweak hyperparameters,
    try different optimization techniques, and never shy away from taking calculated
    risks. In the forges of experimentation, the metal of theory is shaped into the
    sword of application.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 知识在应用时最为有益。是时候卷起袖子，把手深入你项目的肥沃土壤中了。调整超参数，尝试不同的优化技术，不要畏惧采取计算过的风险。在实验的熔炉中，理论的金属被锻造成应用的利剑。
- en: As we close this blog, I trust that the quest for deeper understanding doesn’t
    end here. What is the pursuit of knowledge, if not a never-ending journey? Shall
    you venture forth into your projects armed with this newly acquired wisdom? The
    horizon beckons.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们结束这个博客时，我相信对更深层次理解的追求不会止步于此。如果知识的追求不是一段无尽的旅程，那它是什么呢？你是否将带着这份新获得的智慧，勇敢迈向你的项目？地平线在召唤。
- en: Contact
  id: totrans-134
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 联系
- en: Want to connect? Follow Dr. Robinson on [LinkedIn](https://www.linkedin.com/in/jrobby/),
    [Twitter](https://twitter.com/jrobvision), [Facebook](https://www.facebook.com/joe.robinson.39750),
    and [Instagram](https://www.instagram.com/doctor__jjj/). Visit my homepage for
    papers, blogs, email signups, and more!
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 想要联系？关注罗宾逊博士的[LinkedIn](https://www.linkedin.com/in/jrobby/)、[Twitter](https://twitter.com/jrobvision)、[Facebook](https://www.facebook.com/joe.robinson.39750)和[Instagram](https://www.instagram.com/doctor__jjj/)。访问我的主页，获取论文、博客、邮件注册及更多内容！
- en: '[](https://www.jrobs-vision.com/?source=post_page-----84c668f4024a--------------------------------)
    [## HOME | Joe Robinson''s site | Research Engineer | Entrepreneur'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://www.jrobs-vision.com/?source=post_page-----84c668f4024a--------------------------------)
    [## 主页 | 乔·罗宾逊的网站 | 研究工程师 | 企业家'
- en: Dr. Robinson has 35+ papers on computer vision, pattern recognition, MM, and
    multimodal. Worked in various industries…
  id: totrans-137
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 罗宾逊博士在计算机视觉、模式识别、MM和多模态方面有超过35篇论文。曾在各个行业工作过……
- en: www.jrobs-vision.com](https://www.jrobs-vision.com/?source=post_page-----84c668f4024a--------------------------------)
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: www.jrobs-vision.com](https://www.jrobs-vision.com/?source=post_page-----84c668f4024a--------------------------------)
