- en: 'Unbox the Cox: A Hidden Dark Secret of Cox Regression'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 解密Cox回归：Cox回归的隐藏黑暗秘密
- en: 原文：[https://towardsdatascience.com/unbox-the-cox-a-hidden-dark-secret-of-cox-regression-fb432137fd6?source=collection_archive---------9-----------------------#2023-06-27](https://towardsdatascience.com/unbox-the-cox-a-hidden-dark-secret-of-cox-regression-fb432137fd6?source=collection_archive---------9-----------------------#2023-06-27)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/unbox-the-cox-a-hidden-dark-secret-of-cox-regression-fb432137fd6?source=collection_archive---------9-----------------------#2023-06-27](https://towardsdatascience.com/unbox-the-cox-a-hidden-dark-secret-of-cox-regression-fb432137fd6?source=collection_archive---------9-----------------------#2023-06-27)
- en: Why do perfect predictors result in a p-value of 0.93?
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 为什么完美预测因子的p值会为0.93？
- en: '[](https://medium.com/@igor-s?source=post_page-----fb432137fd6--------------------------------)[![Igor
    Šegota](../Images/17c592b71fef9526a0679d47937837f6.png)](https://medium.com/@igor-s?source=post_page-----fb432137fd6--------------------------------)[](https://towardsdatascience.com/?source=post_page-----fb432137fd6--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----fb432137fd6--------------------------------)
    [Igor Šegota](https://medium.com/@igor-s?source=post_page-----fb432137fd6--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@igor-s?source=post_page-----fb432137fd6--------------------------------)[![Igor
    Šegota](../Images/17c592b71fef9526a0679d47937837f6.png)](https://medium.com/@igor-s?source=post_page-----fb432137fd6--------------------------------)[](https://towardsdatascience.com/?source=post_page-----fb432137fd6--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----fb432137fd6--------------------------------)
    [Igor Šegota](https://medium.com/@igor-s?source=post_page-----fb432137fd6--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fe5f8ebca4ad8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funbox-the-cox-a-hidden-dark-secret-of-cox-regression-fb432137fd6&user=Igor+%C5%A0egota&userId=e5f8ebca4ad8&source=post_page-e5f8ebca4ad8----fb432137fd6---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----fb432137fd6--------------------------------)
    ·8 min read·Jun 27, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Ffb432137fd6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funbox-the-cox-a-hidden-dark-secret-of-cox-regression-fb432137fd6&user=Igor+%C5%A0egota&userId=e5f8ebca4ad8&source=-----fb432137fd6---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fe5f8ebca4ad8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funbox-the-cox-a-hidden-dark-secret-of-cox-regression-fb432137fd6&user=Igor+%C5%A0egota&userId=e5f8ebca4ad8&source=post_page-e5f8ebca4ad8----fb432137fd6---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----fb432137fd6--------------------------------)
    · 8 min 阅读 · 2023年6月27日 [](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Ffb432137fd6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funbox-the-cox-a-hidden-dark-secret-of-cox-regression-fb432137fd6&user=Igor+%C5%A0egota&userId=e5f8ebca4ad8&source=-----fb432137fd6---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ffb432137fd6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funbox-the-cox-a-hidden-dark-secret-of-cox-regression-fb432137fd6&source=-----fb432137fd6---------------------bookmark_footer-----------)![](../Images/5854f8dc3addbe438ea7037fcb639fe1.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ffb432137fd6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funbox-the-cox-a-hidden-dark-secret-of-cox-regression-fb432137fd6&source=-----fb432137fd6---------------------bookmark_footer-----------)![](../Images/5854f8dc3addbe438ea7037fcb639fe1.png)'
- en: Photo by [Dima Pechurin](https://unsplash.com/pt-br/@pechka?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由 [Dima Pechurin](https://unsplash.com/pt-br/@pechka?utm_source=medium&utm_medium=referral)
    提供，来源于 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: Diving into the perfect predictors
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索完美预测因子
- en: If you have been following my previous blog posts, you might recall that logistic
    regression encounters a problem when trying to fit perfectly separated data, [leading
    to an infinite odds ratio](https://medium.com/towards-data-science/logistic-regression-faceoff-67560de4f492)*.*
    In Cox regression, where hazard replaces odds, you might wonder if a similar issue
    arises with perfect predictors. It does occur, but unlike logistic regression,
    it is much less apparent how this occurs here and even what constitutes “perfect
    predictors”. As will become more clear later, perfect predictors are defined as
    predictors *x* whose ranks exactly match the ranks of event times (their Spearman
    correlation is one).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你一直关注我的之前的博客帖子，你可能会记得逻辑回归在尝试完美分离数据时会遇到问题，[导致无限的赔率比](https://medium.com/towards-data-science/logistic-regression-faceoff-67560de4f492)*。在
    Cox 回归中，风险替代了赔率，你可能会想知道完美预测变量是否会出现类似的问题。确实会出现，但是与逻辑回归不同，这里的问题不那么明显，也不容易界定什么是“完美预测变量”。如后面会更加明确，完美预测变量被定义为预测变量*x*，其排名恰好与事件时间的排名一致（它们的斯皮尔曼相关系数为
    1）。
- en: 'Previously, on “Unbox the Cox”:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 之前，在“Unbox the Cox”中：
- en: '[](/unbox-the-cox-intuitive-guide-to-cox-regressions-c485408ae15d?source=post_page-----fb432137fd6--------------------------------)
    [## Unbox the Cox: Intuitive Guide to Cox Regressions'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/unbox-the-cox-intuitive-guide-to-cox-regressions-c485408ae15d?source=post_page-----fb432137fd6--------------------------------)
    [## Unbox the Cox: 直观指南到 Cox 回归'
- en: How do hazards and maximum likelihood estimates predict event rankings?
  id: totrans-13
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 风险和最大似然估计如何预测事件排名？
- en: towardsdatascience.com](/unbox-the-cox-intuitive-guide-to-cox-regressions-c485408ae15d?source=post_page-----fb432137fd6--------------------------------)
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '[towardsdatascience.com](/unbox-the-cox-intuitive-guide-to-cox-regressions-c485408ae15d?source=post_page-----fb432137fd6--------------------------------)'
- en: 'we explained maximum likelihood estimation and introduced a made-up dataset
    with five subjects, where a single predictor, *x*, represented the dosage of a
    life-extending drug. To make *x* a perfect predictor of event times, here we swapped
    the event times for subjects C and D:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 我们解释了最大似然估计，并介绍了一个虚构的数据集，其中有五个主体，一个预测变量*x*，代表了一种延长生命的药物的剂量。为了使*x*成为事件时间的完美预测变量，我们在这里交换了主体
    C 和 D 的事件时间：
- en: '[PRE0]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '![](../Images/15880abec14a17404267728d9a3afd82.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/15880abec14a17404267728d9a3afd82.png)'
- en: Image by the author.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供。
- en: 'To understand why these “perfect predictors” can be problematic, let’s pick
    up right where we left off and check the negative log-likelihood cost plotted
    against *β*:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解为什么这些“完美预测变量”可能会有问题，让我们从上次的内容继续，查看负对数似然图与*β*的关系：
- en: '[PRE1]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '![](../Images/84e4aa14eea51ed69bc51e9c23ff5840.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/84e4aa14eea51ed69bc51e9c23ff5840.png)'
- en: Image by the author.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供。
- en: 'You can see right away that there is no minimum value of *β* any longer: if
    we use very large negative values of *β*, we end up with log-likelihood fits that
    are near-perfect for all events.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以立即看到*β*没有最小值：如果我们使用非常大的负值的*β*，我们最终会得到几乎完美的对数似然拟合结果。
- en: 'Now, let’s dive into the math behind this and take a look at the likelihood
    of event A. We’ll dig into how the numerator and denominator change as we tweak
    *β*:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们深入探讨一下背后的数学原理，看看事件 A 的可能性。我们将研究分子和分母在调整*β*时的变化情况：
- en: '![](../Images/f5509f0ba94030c965b8c45e143af6d3.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f5509f0ba94030c965b8c45e143af6d3.png)'
- en: 'When *β* is high or a large positive number, the last term in the denominator
    (with the largest *x* of 1.2), representing the hazard of subject E, dominates
    the entire denominator and becomes exceedingly large. So, the likelihood becomes
    small and approaches zero:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 当*β*很高或是一个很大的正数时，分母中的最后一个项（具有最大* x* 为 1.2），代表了主体 E 的风险，主导了整个分母，并变得极其巨大。因此，似然变得很小，接近于零：
- en: '![](../Images/4a33d3c071776416288bc2bd87eed8bb.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4a33d3c071776416288bc2bd87eed8bb.png)'
- en: This results in a big negative log-likelihood. The same thing happens for each
    individual likelihood because the last hazard of subject E, will always exceed
    any hazard in the numerator. As a result, the negative log-likelihood increases
    for subjects A to D. In this case, when we have high *β*, it brings down all the
    likelihoods, resulting in poor fits for all events.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 这会导致很大的负对数似然。每个单独的似然情况也是如此，因为主体 E 的最后一个风险总是会超过分子中的任何风险。因此，主体 A 到 D 的负对数似然增加。在这种情况下，当我们有较高的*β*时，它会降低所有的似然，导致所有事件的拟合效果较差。
- en: 'Now, when *β* is low or a big negative number, the first term in the denominator,
    representing the hazard of subject A, dominates since it has the lowest *x* value.
    As the same hazard of subject A also appears in the numerator, the likelihood
    L(A) can be arbitrarily close to 1 by making β increasingly negative, thereby
    creating an almost perfect fit:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，当*β*值较低或为一个很大的负数时，分母中的第一个项，代表了对象A的风险，因为它的*x*值最低，主导了分母。由于对象A的风险也出现在分子中，通过使*β*越来越负，似乎能使L(A)接近1，从而创造出几乎完美的拟合：
- en: '![](../Images/606a470b8a080c47cafdcf39a47ac2d8.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/606a470b8a080c47cafdcf39a47ac2d8.png)'
- en: 'The same deal goes for all the other individual likelihoods: negative *βs*
    now boost the likelihoods of all events at the same time. Basically, having a
    negativeβ doesn’t come with any downsides. At the same time, certain individual
    hazards increase (subjects A and B with negativex), some stay the same (subject
    C with *x =* 0), and others decrease (subject D with positivex). But remember,
    what really matters here are ratios of hazards. We can verify this hand-waving
    math by plotting individual hazards:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 对所有其他单个可能性来说也是一样的：负*β*现在同时提升所有事件的可能性。基本上，负*β*不会带来任何缺点。同时，某些个体风险增加（对象A和B的负*x*），有些保持不变（对象C的*x
    =* 0），其他则减少（对象D的正*x*）。但请记住，真正重要的是风险的比率。我们可以通过绘制单个风险来验证这一点：
- en: '[PRE2]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '![](../Images/e3607c24c61922b16d990450fa47d835.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e3607c24c61922b16d990450fa47d835.png)'
- en: Image by the author.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供。
- en: 'The way likelihoods are put together, as a ratio of a hazard to the sum of
    hazards of all subjects still at risk, means that negative *β* values make a **perfect
    fit for the likelihood of each subject whose event time rank is greater than or
    equal to the predictor rank**! As a side note, if *x* had a perfect *negative*
    Spearman correlation with event times, things would be flipped around: arbitrarily
    positive *β*s would give us arbitrarily good fits.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 可能性的组合方式，即风险与所有仍然处于风险中的对象的风险总和的比率，意味着负*β*值为每个事件时间排名大于或等于预测因子排名的对象的**可能性提供了一个完美的拟合**！作为附带说明，如果*x*与事件时间有完美的*负*斯皮尔曼相关性，情况将会反转：任意正的*β*会给我们带来任意好的拟合。
- en: Misaligned predictor and time ranks
  id: totrans-36
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 不匹配的预测因子和时间排名
- en: 'We can actually see this and show you what goes down when event time ranks
    and predictor ranks do not line up using a another made-up example:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 我们实际上可以看到这一点，并向你展示当事件时间排名和预测因子排名不匹配时会发生什么，使用另一个虚构的例子：
- en: '[PRE3]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '![](../Images/3c639010a4db2b20b224393ffed42ef7.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3c639010a4db2b20b224393ffed42ef7.png)'
- en: 'In this particular example, the `time` column ranges from 1 to 8, where each
    value represents its own rank. We also have an `x_rank` column, which ranks the
    predictors *x*. Now, here''s the key observation: for subjects D and G, their
    `x_rank` is actually higher than their corresponding `time` rank. As a result,
    the likelihoods of D and G won''t experience the cancellation effect between the
    numerator and denominator when we have large negative values of *β*:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个特定的例子中，`time`列的范围从1到8，每个值代表其自己的排名。我们还有一个`x_rank`列，用于排名预测因子*x*。现在，关键观察点是：对于D和G对象，它们的`x_rank`实际上高于其对应的`time`排名。因此，当我们有大负值的*β*时，D和G的可能性不会在分子和分母之间经历抵消效应：
- en: '![](../Images/a59a9dda57fca6a1bf85aa40863c81c7.png)![](../Images/f6fb2510e40f11c7ed08408509ac2da5.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a59a9dda57fca6a1bf85aa40863c81c7.png)![](../Images/f6fb2510e40f11c7ed08408509ac2da5.png)'
- en: 'Their likelihoods are now maximal at some intermediate finite values of *β*.
    Let’s take a look at a plot of individual likelihoods to see this:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 他们的可能性现在在某些中间有限的*β*值处达到最大。我们来看看单个可能性的图表：
- en: '[PRE4]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '![](../Images/963e381927c49fa06858b34c9ac5f2dc.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/963e381927c49fa06858b34c9ac5f2dc.png)'
- en: Image by the author.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供。
- en: 'These “misaligned” ranks between a time and a predictor play a crucial role:
    they stop all likelihoods from essentially collapsing into one when we have significantly
    negative *β*s.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 时间和预测因子之间的这些“错位”排名起着至关重要的作用：它们阻止所有可能性在我们有显著负*β*值时基本上坍缩成一个。
- en: In conclusion, in Cox regression, in order to obtain a finite coefficient *β*
    for a predictor *x*, we require at least one instance where the rank of the predictor
    *x* is lower than the rank of the event time.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，在Cox回归中，为了获得预测因子*x*的有限系数*β*，我们需要至少有一个实例，其中预测因子*x*的排名低于事件时间的排名。
- en: Perfect is indeed the enemy of the good (p-value)
  id: totrans-48
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 完美确实是良好的敌人（p值）
- en: 'So, how do these perfect predictors actually behave in real-life scenarios?
    Well, to find out, let’s once again turn to the lifelines library for some investigation:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，这些完美的预测因子在现实场景中实际上表现如何？为了找出答案，让我们再次转向lifelines库进行一些调查：
- en: '[PRE5]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '[PRE6]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '![](../Images/f2e4431769f8c082ba5d63a695e27887.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f2e4431769f8c082ba5d63a695e27887.png)'
- en: Just like in logistic regression, we’re encountering convergence warnings and
    getting extremely wide confidence intervals for our predictor coefficient *β*.
    **As a result, we end up with a p-value of 0.93**!
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 就像在逻辑回归中一样，我们遇到了收敛警告，并且得到了极宽的预测因子系数*β*的置信区间**因此，我们得到的p值为0.93**！
- en: If we simply filter models based on p-values without taking this issue into
    account or conducting further investigation, we would overlook these perfect predictors.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们仅仅基于p值过滤模型而不考虑这个问题或进行进一步调查，我们可能会忽视这些完美的预测因子。
- en: 'To tackle this convergence problem, the lifelines library documentation and
    some helpful StackOverflow threads suggest a potential solution: incorporating
    a regularization term into the cost function. This term effectively increases
    the cost for large coefficient values, and you can activate L2 regularization
    by setting the `penalizer` argument to a value greater than zero:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 为了应对这个收敛问题，lifelines库文档和一些有用的StackOverflow线程建议了一个潜在的解决方案：将正则化项纳入成本函数。这个项有效地增加了大系数值的成本，你可以通过将`penalizer`参数设置为大于零的值来激活L2正则化：
- en: '[PRE7]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '![](../Images/6b85fb942524535ad537bab989e4d1a7.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6b85fb942524535ad537bab989e4d1a7.png)'
- en: This approach fixes the convergence warning, but it doesn’t really make a huge
    dent in shrinking that pesky p-value. Even with this regularization trick, the
    p-value for a perfect predictor still hangs around at a somewhat large value 0.11.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法修复了收敛警告，但并没有在缩小那个讨厌的p值上取得巨大进展。即使使用了这种正则化技巧，完美预测因子的p值仍然维持在一个相对较大的值0.11。
- en: 'Time is relative: only ranks matter'
  id: totrans-59
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 时间是相对的：只有排名才重要
- en: 'Lastly, we’ll verify that the absolute values of event times have no impact
    on a Cox regression fit, using our previous example. To do this, we’ll introduce
    a new column called `time2`, which will contain random numbers in the same order
    as the `time` column:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将验证事件时间的绝对值对Cox回归拟合没有影响，使用我们之前的例子。为此，我们将引入一个名为`time2`的新列，其中包含与`time`列相同顺序的随机数字：
- en: '[PRE8]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '![](../Images/c640ed3ed5010c5ab0d8da500b31c714.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c640ed3ed5010c5ab0d8da500b31c714.png)'
- en: 'Their fits are indeed identical:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 它们的拟合确实是相同的：
- en: '[PRE9]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '![](../Images/99c7b05224f1a308f298c14adcce3a87.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/99c7b05224f1a308f298c14adcce3a87.png)'
- en: '[PRE10]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '![](../Images/c49f79ac66e6621a31ffc051e8188e87.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c49f79ac66e6621a31ffc051e8188e87.png)'
- en: Conclusion
  id: totrans-68
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: What did we learn from all of this?
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从中学到了什么？
- en: Perfect predictors in survival models are those predictors whose ranks perfectly
    match the ranks of event times.
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生存模型中的完美预测因子是那些排名与事件时间的排名完全匹配的预测因子。
- en: Cox regression cannot fit these perfect predictors with a finite coefficient
    *β*, leading to wide confidence intervals and big p-values.
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cox回归无法用有限的系数*β*来拟合这些完美预测因子，导致了宽置信区间和大的p值。
- en: The actual values of event times don’t really matter — it’s all about their
    ranks.
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 事件时间的实际值并不重要——关键在于它们的排名。
- en: When the ranks of event times and predictors don’t align, we don’t get that
    handy cancellation effect for large *β* values in likelihoods. So, we need at
    least one case where the ranks don’t match to have a fit with a finite coefficient.
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当事件时间和预测因子的排名不一致时，我们无法在似然中获得大*β*值的便捷取消效应。因此，我们至少需要一个排名不匹配的案例，以获得一个有限系数的拟合。
- en: Even if we try some fancy regularization techniques, perfect predictors can
    still give us those annoyingly wide confidence intervals and high p-values in
    real-life situations.
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 即使我们尝试一些高级的正则化技术，完美预测因子在现实情况下仍然会给我们那些恼人的宽置信区间和高p值。
- en: Just like in logistic regression, if we don’t really care about those p-values,
    using a regularization method can still provide us with a handy model fit that
    gets the prediction right!
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 就像在逻辑回归中一样，如果我们不太关心这些p值，使用正则化方法仍然可以提供一个方便的模型拟合，准确进行预测！
- en: 'If you would like to run the code yourself, feel free to use IPython notebooks
    from my Github: [https://github.com/igor-sb/blog/blob/main/posts/cox_perfect.ipynb](https://github.com/igor-sb/blog/blob/main/posts/cox_perfect.ipynb)'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想自己运行代码，可以使用我Github上的IPython笔记本：[https://github.com/igor-sb/blog/blob/main/posts/cox_perfect.ipynb](https://github.com/igor-sb/blog/blob/main/posts/cox_perfect.ipynb)
- en: Farewell until the next post! 👋
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 告别，直到下一篇文章！ 👋
