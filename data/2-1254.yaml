- en: How to Train BERT for Masked Language Modeling Tasks
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何训练 BERT 进行掩码语言建模任务
- en: 原文：[https://towardsdatascience.com/how-to-train-bert-for-masked-language-modeling-tasks-3ccce07c6fdc](https://towardsdatascience.com/how-to-train-bert-for-masked-language-modeling-tasks-3ccce07c6fdc)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/how-to-train-bert-for-masked-language-modeling-tasks-3ccce07c6fdc](https://towardsdatascience.com/how-to-train-bert-for-masked-language-modeling-tasks-3ccce07c6fdc)
- en: Hands-on guide to building language model for MLM tasks from scratch using Python
    and Transformers library
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从零开始使用 Python 和 Transformers 库构建 MLM 任务语言模型的实用指南
- en: '[](https://ransakaravihara.medium.com/?source=post_page-----3ccce07c6fdc--------------------------------)[![Ransaka
    Ravihara](../Images/ac09746938c10ad8f157d46ea0de27ca.png)](https://ransakaravihara.medium.com/?source=post_page-----3ccce07c6fdc--------------------------------)[](https://towardsdatascience.com/?source=post_page-----3ccce07c6fdc--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----3ccce07c6fdc--------------------------------)
    [Ransaka Ravihara](https://ransakaravihara.medium.com/?source=post_page-----3ccce07c6fdc--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[Medium](https://ransakaravihara.medium.com/?source=post_page-----3ccce07c6fdc--------------------------------)
    [![Ransaka Ravihara](../Images/ac09746938c10ad8f157d46ea0de27ca.png)](https://ransakaravihara.medium.com/?source=post_page-----3ccce07c6fdc--------------------------------)
    [Towards Data Science](https://towardsdatascience.com/?source=post_page-----3ccce07c6fdc--------------------------------)
    [![Towards Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----3ccce07c6fdc--------------------------------)
    [Ransaka Ravihara](https://ransakaravihara.medium.com/?source=post_page-----3ccce07c6fdc--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----3ccce07c6fdc--------------------------------)
    ·7 min read·Oct 17, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----3ccce07c6fdc--------------------------------)
    ·阅读时长 7 分钟·2023年10月17日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/f2df84bc5e99738d8d9c3264785f2390.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f2df84bc5e99738d8d9c3264785f2390.png)'
- en: Introduction
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: In recent years, large language models(LLMs) have taken all the attention from
    the machine learning community. Before LLMs came in, we had a crucial research
    phase on various language modeling techniques, including masked language modeling,
    causal language modeling, and sequence-to-sequence language modeling.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，大型语言模型（LLMs）吸引了整个机器学习社区的关注。在 LLMs 出现之前，我们经历了一个重要的研究阶段，研究了各种语言建模技术，包括掩码语言建模、因果语言建模和序列到序列语言建模。
- en: From the above list, masked language models such as BERT became more usable
    in downstream NLP tasks such as classification and clustering. Thanks to libraries
    such as Hugging Face Transformers, adapting these models for downstream tasks
    became more accessible and manageable. Also thanks to the open-source community,
    we have plenty of language models to choose from covering widely used languages
    and domains.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 从上述列表来看，诸如 BERT 这样的掩码语言模型在分类和聚类等下游 NLP 任务中变得更加实用。由于 Hugging Face Transformers
    等库的帮助，调整这些模型以适应下游任务变得更加可及和可管理。感谢开源社区，我们有大量的语言模型可以选择，涵盖了广泛使用的语言和领域。
- en: GitHub repository for this tutorial
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 本教程的 GitHub 仓库
- en: '[](https://github.com/Ransaka/Train-BERT-for-Masked-Language-Modeling-Tasks?source=post_page-----3ccce07c6fdc--------------------------------)
    [## GitHub - Ransaka/Train-BERT-for-Masked-Language-Modeling-Tasks: GitHub repo
    for TDS article "How to…'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '[GitHub](https://github.com/Ransaka/Train-BERT-for-Masked-Language-Modeling-Tasks?source=post_page-----3ccce07c6fdc--------------------------------)
    [## GitHub - Ransaka/Train-BERT-for-Masked-Language-Modeling-Tasks: GitHub repo
    for TDS article "How to…'
- en: GitHub repo for TDS article "How to Train BERT for Masked Language Modeling
    Tasks" - GitHub …
  id: totrans-12
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: GitHub 仓库用于 TDS 文章 "如何训练 BERT 进行掩码语言建模任务" - GitHub …
- en: github.com](https://github.com/Ransaka/Train-BERT-for-Masked-Language-Modeling-Tasks?source=post_page-----3ccce07c6fdc--------------------------------)
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '[GitHub](https://github.com/Ransaka/Train-BERT-for-Masked-Language-Modeling-Tasks?source=post_page-----3ccce07c6fdc--------------------------------)'
- en: Fine-tune or build one from scratch?
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 微调还是从头开始构建？
- en: When adapting existing language models to your specific use cases, sometimes
    we can use existing models without further tuning (so-called fine-tuning). For
    example, if you want an English sentiment/intent detection model, you can go into
    HuggingFace.co and find a suitable model for your use case.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在将现有语言模型适应于特定用例时，有时我们可以在不进一步调整（即所谓的微调）的情况下使用现有模型。例如，如果你需要一个英语情感/意图检测模型，你可以进入
    HuggingFace.co 并找到适合你用例的模型。
- en: '![](../Images/b7b3c524c332f97ac2b6e822de2a7ac0.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b7b3c524c332f97ac2b6e822de2a7ac0.png)'
- en: However, you can only expect this for some of the tasks encountered in the real
    world. That's where we need an additional technique called fine-tuning. First,
    you must choose a base model that will be fine-tuned. Here, you must be careful
    about the selected model and your target language's lexical similarity.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，您只能对现实世界中遇到的一些任务抱有这种期望。这就是我们需要一种称为微调的额外技术的地方。首先，您必须选择一个将要微调的基础模型。在这里，您必须小心选择的模型和目标语言的词汇相似性。
- en: However, if you can't find a suitable model retrained on the desired language,
    consider building one from scratch. In this tutorial, we will implement the BERT
    model for the masked language model.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，如果您找不到适用于所需语言的合适模型，请考虑从头开始构建一个。在本教程中，我们将实现用于掩码语言模型的 BERT 模型。
- en: BERT Architecture
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: BERT 架构
- en: Even though describing BERT architecture is out of the scope of this tutorial,
    for the sake of clarity, let's go through it very narrowly. BERT, or **B**idirectional
    **E**ncoder **R**epresentations from **T**ransformers, belongs to the encoder-only
    transformer family. It was introduced in the year 2018 by researchers at Google.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管描述 BERT 架构超出了本教程的范围，但为了清晰起见，我们还是非常简略地介绍一下。BERT，或**双向编码器表示**来自**变换器**，属于仅编码器的变换器家族。它于
    2018 年由 Google 的研究人员引入。
- en: 'Paper abstract:'
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 论文摘要：
- en: We introduce a new language representation model called BERT, which stands for
    Bidirectional Encoder Representations from Transformers. Unlike recent language
    representation models, BERT is designed to pre-train deep bidirectional representations
    from unlabeled text by jointly conditioning on both left and right context in
    all layers. As a result, the pre-trained BERT model can be fine-tuned with just
    one additional output layer to create state-of-the-art models for a wide range
    of tasks, such as question answering and language inference, without substantial
    task-specific architecture modifications.
  id: totrans-22
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 我们介绍了一种新的语言表示模型，称为 BERT，代表**双向编码器表示**来自**变换器**。与最近的语言表示模型不同，BERT 旨在通过在所有层中共同考虑左右上下文来从未标记文本中预训练深度双向表示。因此，预训练的
    BERT 模型可以通过添加一个额外的输出层来进行微调，从而创建广泛任务的最先进模型，如问答和语言推理，而无需对任务特定的架构进行 substantial 修改。
- en: BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art
    results on eleven natural language processing tasks, including pushing the GLUE
    score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6%
    absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point
    absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).
  id: totrans-23
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: BERT 从概念上讲很简单，从经验上讲却非常强大。它在十一项自然语言处理任务中获得了新的最先进结果，包括将 GLUE 分数推高至 80.5%（绝对提高
    7.7 个百分点），MultiNLI 准确率提升至 86.7%（绝对提高 4.6 个百分点），SQuAD v1.1 问答测试 F1 提升至 93.2（绝对提高
    1.5 个点），以及 SQuAD v2.0 测试 F1 提升至 83.1（绝对提高 5.1 个点）。
- en: 'Paper: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805)'
  id: totrans-24
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 论文：[https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805)
- en: In the above, we can see an interesting keyword, Bidirectional. Bidirectional
    nature gives human-like power to BERT. Assume you have to fill in a blank like
    the below one,
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在上面，我们可以看到一个有趣的关键词——双向。双向特性赋予了 BERT 类似于人类的能力。假设您需要填入一个像下面这样的空白，
- en: “War may sometimes be a necessary evil. But no matter how necessary, it is always
    an _____, never a good.”
  id: totrans-26
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “战争有时可能是一种必要的恶行。但无论多么必要，它始终是一种_____，从来不是一种好事。”
- en: 'To guess the word comes into a blank position, you should be aware of a few
    things: words before empty, words after blank, and the overall context of the
    sentence. Adapting this human nature, BERT works the same way. During training,
    we hide some words and ask BERT to try predicting those. When training is finished,
    BERT can predict masked tokens based on their before and after words. To do this,
    the model should allocate different attention to words presented in the input
    sequence, which may significantly impact predicting masked tokens.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 要猜测单词填入空白位置，您需要注意以下几点：空白前的单词、空白后的单词，以及整个句子的上下文。根据这种人类特性，BERT 也以相同的方式工作。在训练过程中，我们隐藏一些单词，并要求
    BERT 尝试预测这些单词。训练完成后，BERT 可以根据前后单词来预测被掩码的标记。为此，模型应对输入序列中的单词分配不同的注意力，这可能会显著影响对掩码标记的预测。
- en: '![](../Images/b479a89ad5558c20c076ccbff0a93ddc.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b479a89ad5558c20c076ccbff0a93ddc.png)'
- en: Image by Author via [https://huggingface.co/spaces/exbert-project/exbert](https://huggingface.co/spaces/exbert-project/exbert)
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：作者，[https://huggingface.co/spaces/exbert-project/exbert](https://huggingface.co/spaces/exbert-project/exbert)
- en: As you can see here, the model sees a suitable word for the hidden position
    as *evil* and the first sentence's *evil* as necessary to make this prediction.
    This is a noticeable point and implies that the model understands the context
    of the input sequence. This context awareness allows BERT to generate meaningful
    sentence embeddings for given tasks. Further, these embeddings can be used in
    downstream tasks such as clustering and classification. Enough about BERT; let's
    build one from scratch.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，模型将适合隐藏位置的词视为 *evil*，并且将第一个句子的 *evil* 视为进行此预测所必需的。这是一个显著的点，表明模型理解输入序列的上下文。这种上下文感知使得
    BERT 能够为给定任务生成有意义的句子嵌入。进一步，这些嵌入可以用于下游任务，如聚类和分类。说够了 BERT；让我们从头开始构建一个。
- en: Defining BERT model
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 定义 BERT 模型
- en: We generally have BERT*(base)* and BERT*(large).* Both have 64 dimensions per
    head. The *large* variant contains 24 encoder layers, while the *base* variant
    only has 12\. However, we are not limited to these configurations. Surprisingly,
    we have complete control over defining the model using Hugging Face Transformers
    library. All we have to do is define desired model configurations using the *BertConfig*
    class.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通常有 BERT*(base)* 和 BERT*(large)*。两者每个头部都有 64 维度。*large* 变体包含 24 个编码器层，而 *base*
    变体只有 12 层。不过，我们并不局限于这些配置。令人惊讶的是，我们可以完全控制使用 Hugging Face Transformers 库定义模型。我们需要做的就是使用
    *BertConfig* 类定义所需的模型配置。
- en: I chose 6 heads and 384 total model dimensions to comply with the original implementation.
    In this way, each head has 64 dimensions similar to the original implementation.
    Let's initialize our BERT model.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 我选择了 6 个头和 384 个总模型维度，以符合原始实现。这样，每个头都有 64 个维度，类似于原始实现。让我们初始化我们的 BERT 模型。
- en: '[PRE0]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Training a tokenizer
  id: totrans-35
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练标记器
- en: Here, I won't describe how tokenization works under the hood. Instead, let's
    train one from scratch using Hugging Face tokenizers library. Please note that
    the tokenizer used in the original BERT implementation is WordPiece tokenizer,
    yet another subword-based tokenization method. You can learn more about this tokenization
    using the neat HuggingFace resource below.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我不会描述标记化的底层工作原理。相反，让我们使用 Hugging Face tokenizers 库从零开始训练一个标记器。请注意，原始 BERT
    实现中使用的标记器是 WordPiece 标记器，这是一种基于子词的标记化方法。您可以通过下面的 HuggingFace 资源深入了解这种标记化。
- en: '[](https://huggingface.co/learn/nlp-course/chapter6/6?fw=pt&source=post_page-----3ccce07c6fdc--------------------------------)
    [## WordPiece tokenization — Hugging Face NLP Course'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '[## WordPiece tokenization — Hugging Face NLP Course](https://huggingface.co/learn/nlp-course/chapter6/6?fw=pt&source=post_page-----3ccce07c6fdc--------------------------------)'
- en: We're on a journey to advance and democratize artificial intelligence through
    open source and open science.
  id: totrans-38
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 我们正在通过开源和开放科学来推进和普及人工智能。
- en: huggingface.co](https://huggingface.co/learn/nlp-course/chapter6/6?fw=pt&source=post_page-----3ccce07c6fdc--------------------------------)
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '[huggingface.co](https://huggingface.co/learn/nlp-course/chapter6/6?fw=pt&source=post_page-----3ccce07c6fdc--------------------------------)'
- en: The dataset used here is the Sinhala-400M dataset *(under apache-2.0)*. You
    can follow the same with any dataset you have.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 这里使用的数据集是 Sinhala-400M 数据集 *(under apache-2.0)*。您可以使用任何您拥有的数据集来进行相同的操作。
- en: '![](../Images/377b2ebfccb2a7f8d25df41b71c6627c.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/377b2ebfccb2a7f8d25df41b71c6627c.png)'
- en: As you may notice, some Sinhalese words have been typed using English as well.
    Let's train a tokenizer for these corpora.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您可能注意到的，有些僧伽罗语单词也用英语键入。让我们为这些语料库训练一个标记器。
- en: Let's import the necessary modules first. The good thing about training tokenizers
    using Hugging Face Tokenizers library is that we can use existing tokenizers and
    replace only vocabulary (and merge where applicable) per our training corpus.
    This means tokenization steps such as pre-tokenization and post-tokenization will
    be preserved. For this, we can use a method, *train_new_from_iterator* BertTokenizer
    class*.*
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 首先让我们导入必要的模块。使用 Hugging Face Tokenizers 库训练标记器的好处是，我们可以使用现有的标记器并仅根据我们的训练语料库替换词汇表（并在适用时合并）。这意味着标记化步骤，如预标记化和后标记化，将被保留。为此，我们可以使用
    *train_new_from_iterator* 方法来自 *BertTokenizer* 类。
- en: '[PRE1]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '![](../Images/c66a51e3ac4dad4f8e2c5244908f2549.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c66a51e3ac4dad4f8e2c5244908f2549.png)'
- en: '[PRE2]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '![](../Images/a9a95cf24c4b15bff37110a33f2e4635.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a9a95cf24c4b15bff37110a33f2e4635.png)'
- en: You can see words like 'cricketer' decomposed into *cricket* and *##er,* indicating
    that the tokenizer has been adequately trained. However, try out different vocab
    sizes; mine is 5000, which is relatively small but suitable for this toy example.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到像“cricketer”这样的词被分解为*cricket*和*##er*，这表明标记器已经经过充分训练。然而，可以尝试不同的词汇大小；我的词汇量是5000，相对较小，但适合这个玩具示例。
- en: Finally, we can save the trained tokenizer into our directory.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，我们可以将训练好的标记器保存到我们的目录中。
- en: '[PRE3]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Define data collator and tokenize dataset.
  id: totrans-51
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 定义数据整理器并标记化数据集。
- en: Let's define a collator for MLM tasks. Here, we will mask 15% of tokens. Anyway,
    we can set different masking probabilities.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们定义一个用于MLM任务的整理器。在这里，我们将掩盖15%的标记。无论如何，我们可以设置不同的掩盖概率。
- en: '[PRE4]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Let's tokenize the dataset using a previously created tokenizer. I'm replacing
    the original *LineByLineTextDataset* using my custom class utilizing Hugging Face
    accelerate.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用之前创建的标记器对数据集进行标记化。我用自定义类替代了原始的*LineByLineTextDataset*，利用了Hugging Face accelerate。
- en: '[PRE5]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Let's tokenize the dataset.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们对数据集进行标记化。
- en: '[PRE6]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Alright, let's code our training loop.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，让我们编写训练循环代码。
- en: '[PRE7]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: We can invoke the trainer using its *train()* method.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用其*train()*方法调用训练器。
- en: '[PRE8]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '![](../Images/05feb8f3954884ffa738ba0f551bdaee.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/05feb8f3954884ffa738ba0f551bdaee.png)'
- en: After sufficient training, our model can be used for downstream tasks such as
    zero-shot classification and clustering. You may find the example using this Hugging
    Face space for more details.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在充分训练后，我们的模型可以用于下游任务，如零样本分类和聚类。你可以通过使用这个Hugging Face空间的示例获得更多详细信息。
- en: '[](https://huggingface.co/spaces/Ransaka/sinhala-embedding-space?source=post_page-----3ccce07c6fdc--------------------------------)
    [## Sinhala Embedding Space - a Hugging Face Space by Ransaka'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://huggingface.co/spaces/Ransaka/sinhala-embedding-space?source=post_page-----3ccce07c6fdc--------------------------------)
    [## Sinhala Embedding Space - Ransaka的Hugging Face空间'
- en: Discover amazing ML apps made by the community
  id: totrans-65
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 发现由社区制作的惊人ML应用
- en: huggingface.co](https://huggingface.co/spaces/Ransaka/sinhala-embedding-space?source=post_page-----3ccce07c6fdc--------------------------------)
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: huggingface.co](https://huggingface.co/spaces/Ransaka/sinhala-embedding-space?source=post_page-----3ccce07c6fdc--------------------------------)
- en: Conclusion
  id: totrans-67
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: With limited resources, pre-trained models may only recognize specific linguistic
    patterns, but they can still be helpful for particular use cases. It is highly
    recommended to fine-tune when possible.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 在资源有限的情况下，预训练模型可能只识别特定的语言模式，但它们在特定的使用案例中仍然很有用。强烈建议在可能的情况下进行微调。
- en: '*In this article, all images, unless otherwise noted, are by the author.*'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '*在本文中，除非另有说明，否则所有图片均为作者提供。*'
- en: '**References**'
  id: totrans-70
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**参考文献**'
- en: An Explorable BERT — [https://huggingface.co/spaces/exbert-project/exbert](https://huggingface.co/spaces/exbert-project/exbert)
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一个可探究的BERT — [https://huggingface.co/spaces/exbert-project/exbert](https://huggingface.co/spaces/exbert-project/exbert)
- en: BERT Paper — [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805)
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: BERT论文 — [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805)
- en: Dataset — [https://huggingface.co/datasets/Ransaka/Sinhala-400M](https://huggingface.co/datasets/Ransaka/Sinhala-400M)
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 数据集 — [https://huggingface.co/datasets/Ransaka/Sinhala-400M](https://huggingface.co/datasets/Ransaka/Sinhala-400M)
