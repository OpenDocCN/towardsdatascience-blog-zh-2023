- en: Generating Images Using VAEs, GANs, and Diffusion Models
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 VAEs、GANs 和扩散模型生成图像
- en: 原文：[https://towardsdatascience.com/generating-images-using-vaes-gans-and-diffusion-models-48963ddeb2b2?source=collection_archive---------1-----------------------#2023-05-06](https://towardsdatascience.com/generating-images-using-vaes-gans-and-diffusion-models-48963ddeb2b2?source=collection_archive---------1-----------------------#2023-05-06)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/generating-images-using-vaes-gans-and-diffusion-models-48963ddeb2b2?source=collection_archive---------1-----------------------#2023-05-06](https://towardsdatascience.com/generating-images-using-vaes-gans-and-diffusion-models-48963ddeb2b2?source=collection_archive---------1-----------------------#2023-05-06)
- en: Learn how to generate images using VAEs, DCGANs, and DDPMs
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 了解如何使用 VAEs、DCGANs 和 DDPMs 生成图像
- en: '[](https://medium.com/@jcheigh?source=post_page-----48963ddeb2b2--------------------------------)[![Justin
    Cheigh](../Images/0bafdd733fe57267074a937b4777418c.png)](https://medium.com/@jcheigh?source=post_page-----48963ddeb2b2--------------------------------)[](https://towardsdatascience.com/?source=post_page-----48963ddeb2b2--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----48963ddeb2b2--------------------------------)
    [Justin Cheigh](https://medium.com/@jcheigh?source=post_page-----48963ddeb2b2--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@jcheigh?source=post_page-----48963ddeb2b2--------------------------------)[![贾斯廷·切赫](../Images/0bafdd733fe57267074a937b4777418c.png)](https://medium.com/@jcheigh?source=post_page-----48963ddeb2b2--------------------------------)[](https://towardsdatascience.com/?source=post_page-----48963ddeb2b2--------------------------------)[![面向数据科学](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----48963ddeb2b2--------------------------------)
    [贾斯廷·切赫](https://medium.com/@jcheigh?source=post_page-----48963ddeb2b2--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F24cd781f1018&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgenerating-images-using-vaes-gans-and-diffusion-models-48963ddeb2b2&user=Justin+Cheigh&userId=24cd781f1018&source=post_page-24cd781f1018----48963ddeb2b2---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----48963ddeb2b2--------------------------------)
    ·21 min read·May 6, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F48963ddeb2b2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgenerating-images-using-vaes-gans-and-diffusion-models-48963ddeb2b2&user=Justin+Cheigh&userId=24cd781f1018&source=-----48963ddeb2b2---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F24cd781f1018&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgenerating-images-using-vaes-gans-and-diffusion-models-48963ddeb2b2&user=Justin+Cheigh&userId=24cd781f1018&source=post_page-24cd781f1018----48963ddeb2b2---------------------post_header-----------)
    发表在 [面向数据科学](https://towardsdatascience.com/?source=post_page-----48963ddeb2b2--------------------------------)
    · 21 分钟阅读 · 2023 年 5 月 6 日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F48963ddeb2b2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgenerating-images-using-vaes-gans-and-diffusion-models-48963ddeb2b2&user=Justin+Cheigh&userId=24cd781f1018&source=-----48963ddeb2b2---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F48963ddeb2b2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgenerating-images-using-vaes-gans-and-diffusion-models-48963ddeb2b2&source=-----48963ddeb2b2---------------------bookmark_footer-----------)![](../Images/1cf4312fbcc564f4c6172c34651fe21e.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F48963ddeb2b2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgenerating-images-using-vaes-gans-and-diffusion-models-48963ddeb2b2&source=-----48963ddeb2b2---------------------bookmark_footer-----------)![](../Images/1cf4312fbcc564f4c6172c34651fe21e.png)'
- en: Image by [CatBird AI](https://www.catbird.ai/), Image Prompt by [ChatGPT](https://chat.openai.com/),
    ChatGPT Prompted by Justin Cheigh
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由 [CatBird AI](https://www.catbird.ai/) 提供，图片提示由 [ChatGPT](https://chat.openai.com/)
    提供，ChatGPT 由贾斯廷·切赫提供提示
- en: 'Introduction:'
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 引言：
- en: We’re currently in the midst of a generative AI boom. In November 2022, Open
    AI’s generative language model ChatGPT shook up the world, and in March 2023 we
    even got GPT-4!
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 我们目前正处于生成式 AI 繁荣期。2022 年 11 月，Open AI 的生成语言模型 ChatGPT 震撼了世界，而在 2023 年 3 月，我们甚至迎来了
    GPT-4！
- en: 'Even though the future of these LLMs is extremely exciting, today we will be
    focusing on image generation. With the rise of diffusion models, image generation
    took a giant leap forward. Now we’re surrounded by models like DALL-E 2, Stable
    Diffusion, and Midjourney. For example, see the above image. Just to show the
    power of these LLMs, I gave ChatGPT a very simple prompt, which I then fed into
    the free [CatbirdAI.](https://www.catbird.ai/) CatbirdAI uses different models,
    including Openjourney, Dreamlike Diffusion, and more:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这些LLM的未来非常令人兴奋，但今天我们将专注于图像生成。随着扩散模型的兴起，图像生成取得了巨大的飞跃。现在我们被像DALL-E 2、Stable
    Diffusion和Midjourney这样的模型包围。例如，查看上面的图像。为了展示这些LLM的强大，我给ChatGPT一个非常简单的提示，然后将其输入到免费的[CatbirdAI](https://www.catbird.ai/)中。CatbirdAI使用了不同的模型，包括Openjourney、Dreamlike
    Diffusion等：
- en: In this article [**Daisuke Yamada**](https://www.linkedin.com/in/daisukeyamada1999/)
    (my co-author) and I will work towards diffusion. We’ll use 3 different models
    and generate images in the style of MNIST handwritten digits using each of them.
    The first model will be a traditional Variational Autoencoder (VAE). We’ll then
    discuss GANs and implement a [Deep Convolution GAN (DCGAN).](https://arxiv.org/pdf/1511.06434v2.pdf)
    Finally, we’ll turn to diffusion models and implement the model described in the
    paper [Denoising Diffusion Probabilistic Models](https://arxiv.org/pdf/2006.11239.pdf).
    For each model we’ll go through the theory working behind the scenes before implementing
    in Tensorflow/Keras.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，[**Daisuke Yamada**](https://www.linkedin.com/in/daisukeyamada1999/)（我的合著者）和我将研究扩散模型。我们将使用3种不同的模型，并使用每种模型生成MNIST手写数字风格的图像。第一个模型将是传统的变分自编码器（VAE）。然后我们将讨论GAN，并实现一个[深度卷积GAN
    (DCGAN)](https://arxiv.org/pdf/1511.06434v2.pdf)。最后，我们将转向扩散模型，并实现论文中描述的模型[去噪扩散概率模型](https://arxiv.org/pdf/2006.11239.pdf)。对于每个模型，我们将讨论其背后的理论，然后在Tensorflow/Keras中实现。
- en: A quick note on notation. We will use try to use subscripts like ***x₀,*** but
    there may be times where instead we will have to use ***x_T*** to denote subscript.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 快速说明一下符号。我们会尽量使用下标如***x₀***，但有时可能需要使用***x_T***来表示下标。
- en: 'Let’s briefly discuss prerequisites. It’s important to be familiar with deep
    learning and comfortable with Tensorflow/Keras. Further, you should be familiar
    with VAEs and GANs; we will go over the main theory but prior experience will
    be helpful. If you’ve never seen these models, check out these helpful sources:
    [MIT S6.191 Lecture](https://www.youtube.com/watch?v=3G5hWM6jqPk&t=285s), [Stanford
    Generative Model Lecture](https://www.youtube.com/watch?v=5WoItGTWV54&t=516s),
    [VAE Blog](/understanding-variational-autoencoders-vaes-f70510919f73). Finally,
    there’s no need to be familiar with DCGANs or diffusion. Great! Let’s get started.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们简要讨论一下先决条件。熟悉深度学习和使用Tensorflow/Keras是很重要的。此外，你应该对VAE和GAN有一定了解；我们会讲解主要理论，但有经验会更有帮助。如果你从未见过这些模型，可以查看这些有用的资源：[MIT
    S6.191 讲座](https://www.youtube.com/watch?v=3G5hWM6jqPk&t=285s)，[斯坦福生成模型讲座](https://www.youtube.com/watch?v=5WoItGTWV54&t=516s)，[VAE
    博客](/understanding-variational-autoencoders-vaes-f70510919f73)。最后，无需了解DCGANs或扩散模型。很好！我们开始吧。
- en: 'Generative Model Trilemma:'
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 生成模型三难问题：
- en: As an unsupervised process, generative AI often lacks well defined metrics to
    track progress. But before we approach any methods to evaluate generative models,
    we need to understand what generative AI is actually trying to accomplish! The
    goal of generative AI is to take training samples from some unknown**,** complex
    data distribution(e.g., the distribution of human faces)and learn a model that
    can “capture this distribution”. So, what factors are relevant in evaluating such
    a model?
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一种无监督过程，生成型AI通常缺乏明确的指标来跟踪进展。但在我们讨论任何评估生成模型的方法之前，我们需要理解生成型AI实际上试图实现什么！生成型AI的目标是从某些未知**复杂数据分布**（例如，人脸的分布）中获取训练样本，并学习一个能够“捕捉这种分布”的模型。那么，评估这样的模型时相关的因素是什么呢？
- en: We certainly want high quality samples, i.e. the generated data should be realistic
    and accurate compared to the actual data distribution. Intuitively we can just
    subjectively evaluate this by looking at the outputs. This is formalized and standardized
    in a benchmark known as [HYPE (Human eYe Perceptual Evaluation).](https://arxiv.org/pdf/1904.01121.pdf)
    Although there are other [quantitative methods](https://deepgenerativemodels.github.io/assets/slides/cs236_lecture15.pdf),
    today we will just rely on our own subjective evaluation.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 我们当然希望样本质量高，即生成的数据应该与实际数据分布相比，真实且准确。直观上，我们可以通过查看输出结果来主观评估这一点。这在一个称为[HYPE（Human
    eYe Perceptual Evaluation）](https://arxiv.org/pdf/1904.01121.pdf)的基准中得到了形式化和标准化。虽然还有其他的[定量方法](https://deepgenerativemodels.github.io/assets/slides/cs236_lecture15.pdf)，但今天我们将仅依靠我们自己的主观评估。
- en: It’s also important to have fast sampling (i.e., the speed of generation, or
    scalability)**.** One particular aspect we will look at is the number of network
    passes required to generate a new sample. For example, we will see that GANs will
    require just one pass of the generator network to turn noise into a (hopefully)
    realistic data sample, while DDPMs require sequential generation,which ends up
    making it much slower.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 还有一个重要因素是快速采样（即生成速度或可扩展性）**。** 我们将关注的一个特定方面是生成新样本所需的网络传递次数。例如，我们将看到GAN只需对生成器网络进行一次传递即可将噪声转换为（希望是）现实的数据样本，而DDPM则需要顺序生成，这使得速度大大降低。
- en: A final important quality is known as mode coverage. We don’t just want to learn
    a specific part of the unknown distribution, but rather we want to capture the
    entire distribution to ensure sample diversity. For example, we don’t want a model
    that just outputs images of 0s and 1s, but rather all possible digit classes.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 最终一个重要的质量标准称为模式覆盖。我们不仅仅希望学习未知分布的特定部分，而是希望捕捉整个分布以确保样本的多样性。例如，我们不希望一个仅输出0和1图像的模型，而是希望输出所有可能的数字类别。
- en: Each of these three important factors (quality of samples, speed of sampling,
    and mode coverage), are covered in the **“**[**Generative Model Trilemma**](https://arxiv.org/pdf/2112.07804.pdf)**”.**
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 这三个重要因素（样本质量、采样速度和模式覆盖）都涵盖在**“**[**生成模型三难困境**](https://arxiv.org/pdf/2112.07804.pdf)**”**中。
- en: '![](../Images/b262f9578a1ad0179b8afa78c1c545ba.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b262f9578a1ad0179b8afa78c1c545ba.png)'
- en: Image Created by Daisuke Yamada, Inspired by [Figure 1 in DDGANs Paper](https://arxiv.org/pdf/2112.07804.pdf)
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由 Daisuke Yamada 创建，灵感来源于[DDGANs 论文中的图1](https://arxiv.org/pdf/2112.07804.pdf)
- en: Now that we understand how we will compare and contrast these models, let’s
    dive into VAEs!
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们了解了如何比较和对比这些模型，让我们深入研究VAE吧！
- en: 'Variational Autoencoder:'
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 变分自编码器：
- en: One of the first generative models that you will encounter is the Variational
    Autoencoder (VAE). Since VAEs are just traditional autoencoders with a probabilistic
    spin, let’s remind ourselves of autoencoders.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 你将遇到的第一个生成模型是变分自编码器（VAE）。由于VAE只是具有概率性变换的传统自编码器，我们来回顾一下自编码器。
- en: 'Autoencoders are dimensionality reduction models that learn to compress data
    into some latent representation:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 自编码器是学习将数据压缩成某种潜在表示的降维模型：
- en: '![](../Images/f6e569a1baedf817f06b3bdeb69241e3.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f6e569a1baedf817f06b3bdeb69241e3.png)'
- en: Image Created by Justin Cheigh
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由 Justin Cheigh 创建
- en: The encoder compresses the input into a latent representation called the bottleneck,
    and then the decoder reconstructs the input. The decoder reconstructing the input
    means we can train with L2 loss between input/output.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 编码器将输入压缩为称为瓶颈的潜在表示，然后解码器重建输入。解码器重建输入意味着我们可以用输入/输出之间的L2损失进行训练。
- en: 'Autoencoders cannot be used for image generation since they overfit, which
    leads to a sparse latent space that is discontinuous and disconnected (non-regularizable).
    VAEs fix this by encoding the input ***x*** as a distribution over the latent
    space:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 自编码器不能用于图像生成，因为它们会过拟合，导致稀疏的潜在空间是不连续且断开的（不可正则化的）。VAE通过将输入***x***编码为潜在空间上的一个分布来解决这个问题：
- en: The input ***x*** gets fed into the encoder ***E.*** The output ***E(x)*** isa
    vector of means and vector of standard deviations which parameterize a distribution
    ***P(z | x).*** The common choice is a multivariate standard Gaussian.From here
    we sample ***z ~ P(z | x),*** and finally the decoder attempts to reconstruct
    ***x*** from ***z (***just like with the autoencoder).
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 输入***x***传递到编码器***E.*** 输出***E(x)***是均值向量和标准差向量，这些向量参数化了分布***P(z | x)***。常见的选择是多变量标准高斯。从这里我们采样***z
    ~ P(z | x)***，最终解码器尝试从***z***重建***x***（就像自编码器一样）。
- en: 'Notice this sampling process is non-differentiable, so we need to change something
    to allow backpropagation to be possible. To do so we use the reparameterization
    trick, where we move sampling to an input layerby first sampling ***ϵ ~ N(0,1).***
    Then we can perform a fixed sampling step: ***z* = *μ* + *σ* ⊙ *ϵ.*** Notice we
    get the same sampling, but now we have a clear path to backpropagate error since
    the only stochastic node is an input!'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 注意这个采样过程是非可微的，因此我们需要做一些改变以允许反向传播实现。为此，我们使用重参数化技巧，即首先采样***ϵ ~ N(0,1)***，然后将采样移动到输入层。接着，我们可以进行固定的采样步骤：***z*
    = *μ* + *σ* ⊙ *ϵ.*** 注意我们获得了相同的采样，但现在我们有了一个清晰的路径来反向传播误差，因为唯一的随机节点是输入！
- en: 'Recall training for autoencoders is L2 loss, which constitutes a reconstruction
    term. For VAEs, we also add a regularization term, which is used to make the latent
    space “well-behaved”:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，自编码器的训练是L2损失，它构成了重建项。对于VAE，我们还添加了一个正则化项，用于使潜在空间“表现良好”：
- en: Here, the first term is a reconstruction term, whereas the second term is a
    regularization term. Specifically here we are using the [Kullback-Leibler (KL)
    divergence](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence)
    between the learned distribution over the latent space and a prior distribution.
    This measures the similarity between 2 distributions and helps prevent overfitting.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，第一个项是重建项，而第二个项是正则化项。具体来说，我们使用的是[Kullback-Leibler (KL) 散度](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence)，它衡量了学习到的潜在空间分布与先验分布之间的相似性。这有助于防止过拟合。
- en: 'Great! We’ve recapped the theory and intuition behind VAEs, and we will now
    discuss implementation details. After importing relevant libraries, we define
    a few hyperparameter values:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 很好！我们已经回顾了VAE的理论和直觉，现在将讨论实现细节。在导入相关库后，我们定义了一些超参数值：
- en: '[PRE0]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'After downloading the MNIST dataset and doing some basic preprocessing, we
    define our loss function and network:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 下载MNIST数据集并进行一些基本预处理后，我们定义了我们的损失函数和网络：
- en: '[PRE1]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Ok, let’s break this down. The get_default_loss(model, x) function takes in
    a VAE model and some input ***x*** and returns the VAE loss we defined before
    (with ***C* = 1**). We defined a convolutional VAE, where the encoder uses Conv2D
    layers to downsample, and the decoder uses [Conv2DTranspose](https://www.geeksforgeeks.org/what-is-transposed-convolutional-layer/)
    layers (deconvolution layers) to upsample. We optimized with Adam.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，我们来分析一下。函数get_default_loss(model, x)接收一个VAE模型和一些输入***x***，并返回我们之前定义的VAE损失（其中***C*
    = 1**）。我们定义了一个卷积VAE，其中编码器使用Conv2D层进行下采样，解码器使用[Conv2DTranspose](https://www.geeksforgeeks.org/what-is-transposed-convolutional-layer/)层（反卷积层）进行上采样。我们使用Adam进行了优化。
- en: 'Since the other two generative models begin with random noise, rather than
    use some input image we simply sampled from the latent space and used the decoder
    to generate new images. We tested latent_dim = 2 **and** latent_dim = 100 and
    obtained the following results:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 由于其他两个生成模型从随机噪声开始，我们没有使用某些输入图像，而是从潜在空间中采样并使用解码器生成新图像。我们测试了latent_dim = 2 **和**
    latent_dim = 100，并获得了以下结果：
- en: '![](../Images/3c8e2bbf8dd68b0e589114f533a04b6e.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3c8e2bbf8dd68b0e589114f533a04b6e.png)'
- en: Generated Images for VAE. Image Created by Daisuke Yamada
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: VAE生成的图像。图像由Daisuke Yamada创建
- en: Since we just do one forward pass for generating new samples, our sampling is
    fast. Further, this is comparatively a simple model, so our training is fast.
    The results in dimension 2 (meaning the dimension of our bottleneck latent representation
    is 2) are good but a bit blurry. However, our results in dimension 100 are not
    that good. We think either we lacked computing power or maybe the posterior began
    to spread over non-existent modes. In other words, we begin to learn unmeaningful
    latent features.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们只进行一次前向传递来生成新样本，因此我们的采样速度很快。此外，这是一个相对简单的模型，所以我们的训练速度也很快。维度为 2（意味着瓶颈潜在表示的维度为
    2）的结果很好，但有点模糊。然而，维度为 100 的结果则不是很好。我们认为可能是计算能力不足，或者后验分布开始扩展到不存在的模式。换句话说，我们开始学习没有意义的潜在特征。
- en: So, how could one theoretically choose the “optimal” latent dimension? Clearly
    100 is not good, but perhaps there’s something in between 2 and 100 that is ideal.
    There’s a tradeoff here betweensample quality and computational efficiency. So,
    you could determine how important each of these factors is for you and do something
    like a grid searchto correctly choose this hyperparameter.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，理论上如何选择“最佳”的潜在维度呢？显然，100 不是一个好选择，但也许在 2 和 100 之间的某个值是理想的。这里存在样本质量和计算效率之间的权衡。因此，你可以确定这些因素对你的重要性，并进行类似网格搜索的操作来正确选择这个超参数。
- en: We also plotted the latent space in dimension 2\. Basically, the following tells
    us what the decoder outputs based on where in the latent space we begin with.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还绘制了维度为 2 的潜在空间。基本上，以下内容告诉我们解码器根据我们在潜在空间中的起点输出什么。
- en: '![](../Images/be81739e30ee3c519f7d08772ede4918.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/be81739e30ee3c519f7d08772ede4918.png)'
- en: Our VAE Latent Space
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的 VAE 潜在空间
- en: 'As you can see the latent space is decently diverse and is pretty complete
    and continuous! So, reflecting on the Generative Model Trilemma we get the following:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所见，潜在空间相当多样化，而且相当完整和连续！因此，反思生成模型三难问题，我们得到以下结论：
- en: '![](../Images/adc868c8d930283d3fce1d2bf9bb0c4f.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/adc868c8d930283d3fce1d2bf9bb0c4f.png)'
- en: Trilemma for VAE (Red is Good, Blue is Bad). Image Created by Daisuke Yamada
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: VAE 的三难问题（红色为好，蓝色为坏）。图像由大辅·山田创建
- en: We’ll now shift gears and discuss DCGANs, and we’ll begin with an accelerated
    explanation of GANs.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将转到 DCGANs，并从加速解释 GANs 开始。
- en: 'Deep Convolutional GANs:'
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 深度卷积 GANs：
- en: In GANs, there is a **generator G** and a **discriminator D**. The generator
    creates new data, and the discriminator differentiates (or discriminates) between
    real and fake data. The two are trained against each other in a mini-max game
    fashion, hence the term adversarial.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在 GANs 中，有一个 **生成器 G** 和一个 **鉴别器 D**。生成器创建新数据，而鉴别器区分（或辨别）真实数据和虚假数据。两者在一个极小化-极大化游戏中对抗训练，因此有了“对抗性”一词。
- en: We are given some training data, and we begin by sampling random noise ***z***
    using either a standard normal or uniform distribution. This noise is the latent
    representation of the data to be generated. We start with noise to allow for more
    diverse data samples and to avoid overfitting.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 我们获得一些训练数据，并开始通过标准正态分布或均匀分布来采样随机噪声 ***z***。这些噪声是待生成数据的潜在表示。我们从噪声开始，以允许更多样化的数据样本，并避免过拟合。
- en: The noise is fed into the generator, which outputs the generated data ***x =
    G(z).*** The discriminator then takes ***x*** and outputs ***P[x = real] = D(x),***
    i.e. the probability that the generated image ***x*** is a real image. Additionally,
    we feed the discriminator a real image from the training set.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 噪声被输入到生成器中，生成器输出生成的数据 ***x = G(z)***。然后，鉴别器接受 ***x*** 并输出 ***P[x = real] = D(x)***，即生成的图像
    ***x*** 是真实图像的概率。此外，我们还将训练集中真实图像输入鉴别器。
- en: 'We typically define the loss as a mini-max game:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通常将损失定义为一个极小化-极大化游戏：
- en: GANs Loss
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: GANs 损失
- en: Notice for the discriminator this looks like binary cross entropy, which makes
    sense since it’s a binary classifier. The fancy looking expected value over points
    sampled from each distribution really corresponds to what you would expect to
    get if you take a data point from (a) the data distribution *(****E_{x ~ p(data)})***
    and (b) random noise ***(E_{z ~ p(z)).*** The first term expresses that the discriminator
    wants to maximize the likelihood of classifying real data as 1, whereas the second
    term expresses that the discriminator wants to maximize the likelihood of classifying
    fake data as 0\. The discriminator also acts under the mini-max assumption that
    the generator will act optimally.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 对于判别器，这看起来像是二元交叉熵，这很合理，因为它是一个二元分类器。对每个分布中采样的点的期望值实际上对应于你从（a）数据分布中*(****E_{x
    ~ p(data)})***和（b）随机噪声***(E_{z ~ p(z)).***中获得的数据点。第一个项表示判别器想要最大化将真实数据分类为1的可能性，而第二个项表示判别器想要最大化将假数据分类为0的可能性。判别器还在生成器会以最优方式行动的最小-最大假设下运行。
- en: 'Ok, we’ll now transition to DCGANs. DCGANs are like GANs, with a few notable
    changes in architecture; the main one is that DCGANs don’t use any multilayer
    perceptrons and instead utilizes convolutions/deconvolutions. Below are the architectural
    guidelines for stable DCGANs (from the original paper):'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，我们现在将过渡到 DCGANs。DCGANs 与 GANs 类似，但架构上有几个显著变化；主要的是 DCGANs 不使用任何多层感知机，而是使用卷积/反卷积。以下是稳定
    DCGANs 的架构指南（来自原始论文）：
- en: Replace any pooling layers with strided convolutions (discriminator) and fractional-strided
    convolutions (i.e. deconvolutions) (generator)
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用步幅卷积（判别器）和分数步幅卷积（即反卷积）（生成器）替换任何池化层
- en: Use batchnorm in both the generator and the discriminator
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在生成器和判别器中使用批量归一化
- en: Remove fully connected hidden layers for deeper architectures
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为了更深的架构，移除全连接的隐藏层
- en: Use ReLU activation in generator for all layers except for the output, which
    uses Tanh
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在生成器中，对所有层使用 ReLU 激活，除了输出层使用 Tanh
- en: Use LeakyReLU activation in the discriminator for all layers.
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在判别器中，对所有层使用 LeakyReLU 激活。
- en: '![](../Images/e32fb044eea8a87bcc9946c9b35c269c.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e32fb044eea8a87bcc9946c9b35c269c.png)'
- en: DCCGAN Architecture- Image Created by Justin Cheigh
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: DCCGAN 架构 - 图像由 Justin Cheigh 创建
- en: We often use GANs for image generation, so intuitively it makes sense to use
    convolutional layers. We use standard convolutional layers in the discriminator,
    as we want to down-sample the image into hierarchical features, while for generators
    we use deconvolutional layers to up-sample the image from noise (latent representation)
    to the generated image.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 我们经常使用GANs进行图像生成，因此直观上使用卷积层是有意义的。我们在判别器中使用标准卷积层，因为我们希望将图像降采样成分层特征，而对于生成器，我们使用反卷积层将图像从噪声（潜在表示）上采样到生成的图像。
- en: Batch normalization is used to stabilize the training process, improve convergence,
    and enable faster learning. Leaky ReLU prevents the zero learning problem of ReLu.
    Finally, Tanh is used to prevent saturation of smaller inputs and avoid the vanishing
    gradient problem (since it’s symmetric around the origin).
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 批量归一化用于稳定训练过程、提高收敛性并加快学习速度。Leaky ReLU 防止了 ReLU 的零学习问题。最后，使用 Tanh 来防止较小输入的饱和，并避免梯度消失问题（因为它在原点周围是对称的）。
- en: 'Great! Now let’s see how to implement DCGANs. After importing libraries we
    set hyperparameter values:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 太棒了！现在让我们看看如何实现 DCGANs。导入库后，我们设置超参数值：
- en: '[PRE2]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'After some data preprocessing and splitting into batches for computational
    efficiency, we are ready to define the generator and discriminator:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 经过一些数据预处理和为计算效率进行批量拆分后，我们准备定义生成器和判别器：
- en: '[PRE3]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Remember all of the architectural guidelines for DCGANs! We are using Conv2DTranspose
    for the generator, and regular Conv2D with stride for the discriminator. Notice
    we compile the discriminator with the loss Binary cross entropy, yet specify the
    discriminator as trainable = False. This is because we will implement the training
    loop ourselves:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 记住所有关于 DCGANs 的架构指南！我们对生成器使用 Conv2DTranspose，对判别器使用普通的 Conv2D 和步幅。注意我们用 Binary
    cross entropy 编译判别器，但将判别器指定为 trainable = False。这是因为我们将自己实现训练循环：
- en: '[PRE4]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'First we train the discriminator with both real data and fake data created
    from the generator, and then we train the generator as well. Great! Let’s see
    what the results look like:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们用真实数据和生成器生成的假数据训练判别器，然后也训练生成器。太棒了！让我们看看结果：
- en: '![](../Images/0fa24f563a86ba2252c53f82b859d186.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0fa24f563a86ba2252c53f82b859d186.png)'
- en: Image Created by Daisuke Yamada
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 图像由大辅·山田创作
- en: 'Our sample quality is better (less blurry)! We still have fast sampling, as
    inference is just inputting random noise to the generator. Below is our latent
    space:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的样本质量更好（不那么模糊）！我们仍然有快速采样，因为推理只需将随机噪声输入生成器。下面是我们的潜在空间：
- en: '![](../Images/137d0d8879e32e6c0774ce7c15a5e88c.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/137d0d8879e32e6c0774ce7c15a5e88c.png)'
- en: Our DCGAN Latent Space
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的DCGAN潜在空间
- en: 'Unfortunately, our latent space is not very diverse (this is especially evident
    from the samples in Epoch 1 with latent dimension 2). We are likely experiencing
    a common issue of mode collapse. Formally, this means the generator only learns
    to create a subset specialized in fooling discriminator. In other words, if the
    discriminator doesn’t do well when the generator creates images of 1s, there’s
    no reason for the generator to do anything else. So, for the generative model
    trilemma, we get the following:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，我们的潜在空间不是很多样化（特别是在潜在维度为2的第1代样本中尤为明显）。我们很可能遇到了模式崩溃的常见问题。正式地说，这意味着生成器只学会创建一个专门用来欺骗判别器的子集。换句话说，如果生成器在生成1的图像时判别器表现不佳，那么生成器没有理由去做其他事情。因此，对于生成模型三难问题，我们得到如下：
- en: '![](../Images/b0fa9d8b81a5445c2defc1776dea1c93.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b0fa9d8b81a5445c2defc1776dea1c93.png)'
- en: Trilemma for DCGANs (Red is Good, Blue is Bad). Image Created by Daisuke Yamada
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: DCGANs的三难困境（红色为好，蓝色为坏）。图像由大辅·山田创作
- en: Now that we explored GANs and DCGANs, it’s time to transition to diffusion models!
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经探索了GANs和DCGANs，是时候过渡到扩散模型了！
- en: 'Diffusion Models:'
  id: totrans-85
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 扩散模型：
- en: Diffusion probabilistic models (or just diffusion models) are currently a part
    of every top image generation model. We will be discussing **Denoising Diffusion
    Probabilistic Models (DDPMs)**.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 扩散概率模型（或简称扩散模型）目前是每个顶级图像生成模型的一部分。我们将讨论**去噪扩散概率模型（DDPMs）**。
- en: 'For both VAEs and GANs, sample generation involves going from noise to a generated
    image in one step.GANs perform inference by taking noise and doing a forward pass
    through the generator, and VAEs perform inference by sampling noise and passing
    it through the decoder. The main idea of diffusion is to generate a sequence of
    images,where every subsequent image is slight less noisy, with the final image
    ideally being realistic! There are two aspects of DDPMs. In the forward process
    we take real images and iteratively add noise. In the reverse process we learn
    how to undo the noise added in the forward process:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 对于VAEs和GANs，样本生成涉及从噪声到生成图像的一个步骤。GANs通过将噪声输入生成器并进行正向传播来执行推理，而VAEs通过采样噪声并将其传递通过解码器来执行推理。扩散模型的主要思想是生成一系列图像，其中每个后续图像稍微少一些噪声，最终图像理想情况下是现实的！DDPM有两个方面。在正向过程中，我们使用真实图像并迭代地添加噪声。在反向过程中，我们学习如何撤销在正向过程中添加的噪声：
- en: Let’s explain the forward and reverse process at an intuitive level. We are
    given a set of training data ***X***₀***,*** where each data point ***x₀* ∈ *X***₀
    is sampled from a data distribution ***x₀ ~ q(x₀).*** Recall ***q(x₀)*** is the
    unknown distribution we want to represent.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从直观的层面解释正向和反向过程。我们给定一组训练数据***X***₀***，***其中每个数据点***x₀*** ∈ ***X***₀从数据分布***x₀
    ~ q(x₀)***中采样。回忆一下***q(x₀)***是我们想要表示的未知分布。
- en: From right to left is the hardcoded forward process where we take some training
    sample ***x₀ ~ q(x₀)*** and iteratively add Gaussian noise. Namely we will generate
    a sequence of images ***x***₁, ***x***₂, …, ***x_T,*** with each subsequent image
    in the sequence being more and more noisy. In the end, we will end up with something
    that can be thought of as pure noise! From left to right we have the reverse process,
    where we learn how to denoise**,** i.e. predict how to get from ***x_{t+1} → xₜ.***
    Great! Now that we understand the basics of the forward and reverse process, let’s
    dive into the theory!
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 从右到左是硬编码的正向过程，我们从一些训练样本***x₀ ~ q(x₀)***开始，逐步添加高斯噪声。也就是说，我们将生成一系列图像***x***₁,
    ***x***₂, …, ***x_T***，其中每个后续图像都越来越嘈杂。最终，我们会得到可以被视为纯噪声的东西！从左到右是反向过程，我们学习如何去噪，即预测如何从***x_{t+1}
    → xₜ***。很好！现在我们理解了正向和反向过程的基本概念，让我们深入了解理论吧！
- en: Formally, the forward process is described by a Markov chain that iteratively
    adds Gaussian noise to the data according to a pre-determined variance schedule
    ***β₁, …, β_T.*** The term Markov chain just means that ***x_{t+1}*** *only* depends
    on ***xₜ.*** So, ***x_{t+1}*** is conditionally independent of ***x***₁, …, ***x_ₜ₋₁***
    given ***xₜ.*** which means ***q(xₜ | x₀, …, xₜ₋₁) = q(xₜ | xₜ₋₁).*** The other
    important concept is a variance schedule. We define some values ***β₁, …, β_T***
    which are used to parameterize the Gaussian noise we add at each time step. Typically
    ***0 ≤ β ≤ 1*** with ***β₁*** small and ***β_T*** large. All of this is put in
    our definition of ***q(xₜ | xₜ₋₁):***
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 正式地，前向过程由一个马尔可夫链描述，该链根据预定的方差调度***β₁, …, β_T*** 迭代地向数据中添加高斯噪声。马尔可夫链这个术语意味着***x_{t+1}***
    *仅* 依赖于***xₜ***。因此，***x_{t+1}*** 在给定***xₜ***的条件下，与***x***₁, …, ***x_ₜ₋₁*** 条件独立。这意味着***q(xₜ
    | x₀, …, xₜ₋₁) = q(xₜ | xₜ₋₁)***。另一个重要的概念是方差调度。我们定义一些值***β₁, …, β_T***，这些值用于参数化我们在每个时间步添加的高斯噪声。通常，***0
    ≤ β ≤ 1***，其中***β₁*** 较小，***β_T*** 较大。所有这些都包含在我们对***q(xₜ | xₜ₋₁)***的定义中：
- en: Forward Diffusion Process
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 前向扩散过程
- en: 'So, we start with ***x₀.*** Then, for **T** timesteps, we follow the above
    equation to get to the next image in the sequence: ***xₜ ~ q(xₜ | xₜ₋₁)***. One
    can prove in the limit **T →** ∞ that ***x_T*** is equivalent to an isotropic
    Gaussian distribution.'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，我们从***x₀***开始。然后，经过**T**个时间步，我们遵循上述方程来获取序列中的下一个图像：***xₜ ~ q(xₜ | xₜ₋₁)***。可以证明在极限**T
    →** ∞ 时，***x_T*** 等同于各向同性的高斯分布。
- en: 'Except we’re not done yet. We intuitively should be able to get from ***x₀***
    to any ***x_t*** in one step by expanding recursively. We first use a reparameterization
    trick (like with VAEs):'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 不过我们还没完成。我们直观上应该能够通过递归展开一步步从***x₀*** 到任何***x_t***。我们首先使用重参数化技巧（类似于变分自编码器）：
- en: Reparameterization Trick
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 重参数化技巧
- en: 'This allows us to do the following:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 这使我们能够做到以下几点：
- en: Sampling Arbitrary Timestep t
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 抽样任意时间步 t
- en: By following the above equation we can get from ***x₀*** to any ***xₜ*** in
    one step! For those curious the [derivation](https://medium.com/@steinsfu/diffusion-model-clearly-explained-cd331bd41166)
    involves expanding and using the [addition property of Gaussians](https://en.wikipedia.org/wiki/Sum_of_normally_distributed_random_variables).Let’s
    move on to the reverse process.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 通过遵循上述方程，我们可以在一步内从***x₀***到任何***xₜ***！对那些好奇的人，[推导过程](https://medium.com/@steinsfu/diffusion-model-clearly-explained-cd331bd41166)涉及展开和使用[高斯的加法性质](https://en.wikipedia.org/wiki/Sum_of_normally_distributed_random_variables)。让我们继续讨论逆向过程。
- en: In the reverse process our goal is to know ***q(xₜ₋₁| xₜ)*** since we can just
    take random noise and iteratively sample from ***q(xₜ₋₁ | xₜ)*** to generate a
    realistic image. One may think we can easily obtain ***q(xₜ₋₁ | xₜ)*** using Bayes
    rule,but it turns out this is computationally intractable. This intuitively makes
    sense; to reverse the forward step we need to look at ***xₜ*** and consider all
    the possible ways we could have gotten there.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 在逆向过程中，我们的目标是了解***q(xₜ₋₁| xₜ)***，因为我们可以随机取噪声并从***q(xₜ₋₁ | xₜ)*** 中迭代抽样以生成逼真的图像。我们可能会认为可以使用贝叶斯规则轻松获得***q(xₜ₋₁
    | xₜ)***，但事实证明这是计算上不可行的。这在直观上是合理的；要逆转前向步骤，我们需要查看***xₜ*** 并考虑我们可能到达那里的所有方式。
- en: 'So, rather than directly computing ***q(xₜ₋₁ | xₜ)***, we will learn a model
    ***p*** with weights ***θ*** that approximates these conditional probabilities.
    Luckily, we can successfully estimate ***q(xₜ₋₁| xₜ)*** as a Gaussian if ***βₜ***
    is sufficiently small. This insight is due to some incredibly difficult theory
    involving stochastic differential equations.So we can define ***p*** as the following:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们将学习一个具有权重***θ***的模型***p***，而不是直接计算***q(xₜ₋₁ | xₜ)***，该模型近似这些条件概率。幸运的是，如果***βₜ***
    足够小，我们可以成功地将***q(xₜ₋₁| xₜ)*** 估计为高斯分布。这一见解来自涉及随机微分方程的一些极其困难的理论。因此，我们可以定义***p***为以下内容：
- en: Model Definition
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 模型定义
- en: 'So, what is our loss? Well, if we want to undo the noise added, intuitively
    it should suffice to just predict the added noise. To see a more complete derivation
    please check out [this great blog](https://lilianweng.github.io/posts/2021-07-11-diffusion-models/#nice)
    by Lilian Weng**.** But it turns out our intuition is true, and rather than have
    a model ***p***, instead we can just have a network ***ϵ_θ*** that predicts noise
    added. With this we can train using MSE between the actual and predicted noise:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，我们的损失是什么呢？如果我们想要撤销添加的噪声，直观上只需预测添加的噪声就足够了。要查看更完整的推导，请查阅[Lilian Weng**的这篇优秀博客](https://lilianweng.github.io/posts/2021-07-11-diffusion-models/#nice)**。但事实证明，我们的直觉是正确的，我们不需要一个模型***p***，而是可以使用一个预测添加噪声的网络***ϵ_θ***。通过这种方法，我们可以使用实际噪声和预测噪声之间的均方误差（MSE）进行训练：
- en: Final Loss
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 最终损失
- en: Here, ***ϵ*** is the actual error, whereas the other term is the predicted error.
    You may notice the expectation is taken over ***x_0***; this is because usually
    the error is written in terms of the reparameterization trick (from above), which
    allows you to obtain ***x_t*** directly from ***x_0*.** Thus our network inputs
    are the time ***t*** and the current image ***xₜ***.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，***ϵ*** 是实际误差，而另一个术语是预测误差。你可能会注意到期望值是对***x_0***取的；这是因为通常误差是以重参数化技巧（如上所述）的形式写出的，这使你可以直接从***x_0*获得***x_t***。因此，我们的网络输入是时间***t***和当前图像***xₜ***。
- en: 'Let’s do a full recap. We train a network ***ϵ_θ*** using MSE to learn how
    to predict noise added. Once trained, we can use our neural network ***ϵ_θ***
    to predict the noise added at any timestep. Using this noise and some of the above
    equations we complete the reverse process and effectively can “denoise”. We therefore
    can perform inference by taking noise and continuously denoise. Both this sampling
    process and train process are described by the following pieces of pseudocode:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们做一个全面回顾。我们使用 MSE 训练网络***ϵ_θ***以学习如何预测添加的噪声。一旦训练完成，我们可以使用我们的神经网络***ϵ_θ***来预测任何时间步的添加噪声。利用这些噪声和一些上述方程，我们完成逆过程并有效地“去噪”。因此，我们可以通过获取噪声并持续去噪来进行推断。这两个过程都由以下伪代码描述：
- en: '![](../Images/cf0de3a60f43f86b4d104047722d3568.png)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/cf0de3a60f43f86b4d104047722d3568.png)'
- en: '[Pseudocode for Training/Sampling](https://hojonathanho.github.io/diffusion/)'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '[训练/采样伪代码](https://hojonathanho.github.io/diffusion/)'
- en: In training we take a real image, sample ***t* ~ Uniform({*1,2,…,T*})** (we
    do this since it’s computationally inefficient to do every step), then take a
    gradient descent step on the MSE of target/predicted noise. In sampling we take
    random noise then continuously sample using our predicted noise and our derived
    equations, until we get to some generated image ***x₀.***
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练过程中，我们获取一张真实图像，抽样***t* ~ Uniform({*1,2,…,T*})**（我们这样做是因为逐步计算效率低），然后对目标/预测噪声的
    MSE 进行梯度下降。在采样过程中，我们取随机噪声，然后使用我们预测的噪声和推导的方程连续采样，直到我们得到一些生成的图像***x₀.***
- en: Great! We can now move on to implementation details. For our underlying architecture
    we will use a [**U-Net**](https://lmb.informatik.uni-freiburg.de/people/ronneber/u-net/)**:**
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 很好！我们现在可以进入实施细节。对于我们的基础架构，我们将使用一个[**U-Net**](https://lmb.informatik.uni-freiburg.de/people/ronneber/u-net/)**：**
- en: '![](../Images/805102a6a9bbde32efe879e54048736e.png)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/805102a6a9bbde32efe879e54048736e.png)'
- en: Image Created by Justin Cheigh; [Inspiration](https://lmb.informatik.uni-freiburg.de/people/ronneber/u-net/)
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 图像由 Justin Cheigh 创建；[灵感](https://lmb.informatik.uni-freiburg.de/people/ronneber/u-net/)
- en: From the architecture it’s pretty clear why this is called a U-Net! U-Nets were
    initially used in biomedical image segmentation, but they also work very well
    with diffusion models! Intuitively, this is true because (a) the input and output
    shape are the same, which is exactly what we need, and (b) we will see that U-Nets
    (due to the encoder-decoder structure paired with the skip connections) are good
    at preserving both local/global information, which helps retain our image but
    still add noise effectively.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 从架构来看，为什么这被称为 U-Net 很明显！U-Net 最初用于生物医学图像分割，但它们在扩散模型中也表现得非常好！直观地说，这是因为（a）输入和输出的形状是相同的，这正是我们需要的，以及（b）我们将看到
    U-Net（由于编码器-解码器结构配合跳跃连接）在保留局部/全局信息方面表现良好，这有助于保留图像但仍有效地添加噪声。
- en: The U-Net has a similar encoder-decoder structure as past generative models.
    Specifically if you look at the image following the shape of the “U”, you can
    see on the way down we have a sequence of downsampling layers, each of which are
    part of the encoder structure. On the way up, we have a sequence of upsampling
    layers, which are part of the decoder structure. The input and output have the
    same shape, which is ideal given our input is a noisy image ***xₜ*** and our output
    is some predicted noise.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: U-Net 具有类似于过去生成模型的编码器-解码器结构。具体来说，如果你查看形状为“U”的图像，你会发现下行过程中有一系列下采样层，这些层是编码器结构的一部分。上行过程中，我们有一系列上采样层，这些层是解码器结构的一部分。输入和输出具有相同的形状，这对于我们的输入是带噪声的图像
    ***xₜ*** 和我们的输出是一些预测噪声的情况来说是理想的。
- en: However, you may notice there is one important difference between a U-Net and
    a standard autoencoder, which are the skip connections. At each level we have
    a downsampling block, which connects to another downsampling block (following
    the shape of the “U”), and a skip connection to an upsampling block. Remember
    these downsampling blocks basically are looking at the image at different resolutions
    (learning different levels of hierarchical features). By having these skip connections
    we ensure that we account for each of these features at each resolution! Another
    way to think of a U-Net is as a sequence of stacked autoencoders.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，你可能会注意到 U-Net 和标准自编码器之间有一个重要区别，那就是跳跃连接。在每个层级中，我们有一个下采样模块，该模块连接到另一个下采样模块（遵循“U”的形状），并且有一个跳跃连接到上采样模块。记住，这些下采样模块基本上是在以不同分辨率查看图像（学习不同层次的特征）。通过这些跳跃连接，我们确保在每个分辨率上都考虑到这些特征！另一种思考
    U-Net 的方式是将其视为一系列堆叠的自编码器。
- en: 'Ok, now let’s look at our specific implementation. First of all, I lied… I
    said that our input is just the noisy image ***xₜ.*** However, we also input the
    actual timestep ***t*** in order to give us a notion of time.The way we do so
    is using a time step embedding, where we take the time ***t***and use a sinusoidal
    positional embedding:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，现在让我们看一下我们的具体实现。首先，我撒了个谎……我说我们的输入只是带噪声的图像 ***xₜ.*** 然而，我们还输入了实际的时间步 ***t***
    以提供时间概念。我们的方法是使用时间步嵌入，其中我们取时间 ***t*** 并使用正弦位置嵌入：
- en: '![](../Images/ba068bfe9ba3050bc3ea39b46c6088ed.png)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ba068bfe9ba3050bc3ea39b46c6088ed.png)'
- en: Sinusoidal Position Embedding
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 正弦位置嵌入
- en: For those unfamiliar, a high level overview of sinusoidal position embedding
    is that we encode elements in some sequence (here just the timesteps) using sinusoidal
    functions, with the intuition being the smooth structure of these functions will
    be easier for neural networks to learn from. So, our actual input is our noisy
    image ***xₜ*** and our time step ***t***, which initially goes through this time
    embedding layer.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 对于不熟悉的人，正弦位置嵌入的高级概述是我们使用正弦函数对某些序列中的元素（这里是时间步）进行编码，直觉是这些函数的平滑结构将更容易被神经网络学习。因此，我们的实际输入是带噪声的图像
    ***xₜ*** 和时间步 ***t***，它们首先经过这个时间嵌入层。
- en: 'We then have our downsampling/upsampling blocks: each downsampling (upsampling)
    block contains 2 ResNet blocks, 1 Attention layer, and 1 Convolution (deconvolution)
    layer. Let’s quickly go over each of these.'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来是我们的下采样/上采样模块：每个下采样（上采样）模块包含 2 个 ResNet 模块、1 个 Attention 层和 1 个卷积（反卷积）层。我们快速了解一下这些模块。
- en: Residual Networks, or ResNet, are basically a sequence of convolutional layers
    with large skip connections, which allow information flow across very deep neural
    networks. Attention, a revolutionary idea crucial in understanding fundamental
    architectures like the Transformer, tells the neural network what to focus on.
    For example, here we have 2 ResNet blocks. After these blocks we will have the
    input image as a vector of latent features, and the attention layer will tell
    the neural network which of these features are most important to focus on. Finally,
    the standard convolution/deconvolution allows for down/upsampling, respectively.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 残差网络（ResNet）基本上是一系列具有大跳跃连接的卷积层，这些跳跃连接允许信息在非常深的神经网络中流动。Attention 是一种革命性的想法，对于理解像
    Transformer 这样的基础架构至关重要，它告诉神经网络该关注什么。例如，这里我们有 2 个 ResNet 模块。在这些模块之后，我们将得到输入图像的潜在特征向量，Attention
    层将告诉神经网络这些特征中哪些是最重要的。最后，标准的卷积/反卷积分别用于下采样/上采样。
- en: 'We use 4 of these stacked autoencoders in our implementation:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的实现中，我们使用了 4 个这样的堆叠自编码器：
- en: '[PRE5]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Great! Now that we have defined our U-Net class, we can move on to using the
    U-Net for our specific problem. We first define relevant hyperparameters:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 很好！现在我们已经定义了我们的U-Net类，我们可以继续将U-Net应用于我们的具体问题。我们首先定义相关的超参数：
- en: '[PRE6]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Due to lack of computing power, we use **timesteps = *T* = 200,** even though
    the original paper used ***T* = 1000\.** After data preprocessing, we define the
    forward process
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 由于计算能力不足，我们使用**timesteps = *T* = 200,** 尽管原始论文使用的是***T* = 1000\.** 数据预处理后，我们定义前向过程
- en: '[PRE7]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'So, here we define our variance schedule in a pretty standard way. In the forward
    function we use the reparameterization trick that allows us to sample arbitrary
    ***xₜ*** from ***x₀.*** Below is a visualization of the forward process:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，在这里我们以一种相当标准的方式定义我们的方差调度。在前向函数中，我们使用重新参数化技巧，允许我们从***x₀***中抽样任意的***xₜ***。下面是前向过程的可视化：
- en: '![](../Images/253ffb16376a1b1b00563453e87fead4.png)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/253ffb16376a1b1b00563453e87fead4.png)'
- en: Visualization of Our Forward Process
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的前向过程可视化
- en: 'We then instantiate our U-Net, define our loss function, and define the training
    process:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们实例化我们的U-Net，定义我们的损失函数，并定义训练过程：
- en: '[PRE8]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Remember, our loss (after lots of work) is just MSE! The rest is a fairly standard
    training loop. After training, we can think aboutinference. Recalling our Sampling
    Algorithm 2, we implement as follows:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，我们的损失（经过大量工作之后）只是均方误差（MSE）！其余部分是一个相当标准的训练循环。训练完成后，我们可以考虑推断。回顾我们的采样算法2，我们按如下方式实现：
- en: '[PRE9]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Here we define how to take an image at a certain time step and denoise it.
    With this we can fully define our inference process:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们定义如何在特定时间步采集图像并进行去噪。通过这些，我们可以完全定义我们的推断过程：
- en: '[PRE10]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'We take random noise, then continuously use our backward function to denoise,
    until we get to a realistic looking image! And here are some of our results:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从随机噪声开始，然后不断使用我们的反向函数去噪，直到得到一个现实的图像！以下是我们的一些结果：
- en: '![](../Images/9f01eafdba4b3a6a42b442652dc303ec.png)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9f01eafdba4b3a6a42b442652dc303ec.png)'
- en: Examples of Our DDPMs Generated Samples
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的DDPM生成样本示例
- en: 'The samples are decently high quality. Further, we were able to get a diverse
    range of samples. Presumably our sample quality would improve with more computing
    power; diffusion is very computationally expensive, which impacted our ability
    to train this model. One can also “reverse engineer”. We take a training image,
    noise it, and then denoise it to see our ability to reconstruct the image. We
    get the following:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 样本质量相当高。此外，我们能够获得多样化的样本。推测我们的样本质量会随着计算能力的提高而改善；扩散过程计算开销非常大，这影响了我们训练这个模型的能力。还可以进行“逆向工程”。我们取一个训练图像，添加噪声，然后去噪，以观察我们重建图像的能力。我们得到以下结果：
- en: '![](../Images/c4d744658be612e0c5066b84b8734d01.png)'
  id: totrans-139
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c4d744658be612e0c5066b84b8734d01.png)'
- en: Reverse Engineering. Image Created by Daisuke Yamada
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 逆向工程。图像由大辅·山田创建
- en: It is important to note that the reverse process is probabilistic, meaning we
    don’t always end up with even a similar image as our input image.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的是，逆向过程是概率性的，这意味着我们并不总是得到与输入图像相似的图像。
- en: 'Great! Let’s go back to the Generative Model Trilemma. We have high quality
    samples, a diverse range of samples, and more stable training (this happens as
    a byproduct of doing these iterative steps). However, we have slow training and
    slow sampling, as we need to sample over and over again during inference. So,
    we get the following:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 很好！让我们回到生成模型三难题。我们有高质量的样本、多样化的样本范围，以及更稳定的训练（这是这些迭代步骤的副产品）。然而，我们有缓慢的训练和采样，因为在推断过程中我们需要一次又一次地采样。因此，我们得到以下结果：
- en: '![](../Images/2664a1495ec2701f253da9d716883b6e.png)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2664a1495ec2701f253da9d716883b6e.png)'
- en: Trilemma for DDPMs (Red is Good, Blue is Bad). Image Created by Daisuke Yamada
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: DDPMs的三难题（红色代表好，蓝色代表不好）。图像由大辅·山田创建
- en: 'Conclusion:'
  id: totrans-145
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结论：
- en: 'Wow! We covered 3 image generation models, going all the way from standard
    VAEs to DDPMs. For each we looked at the Generative Model Trilemma and obtained
    the following results:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 哇！我们覆盖了三种图像生成模型，从标准的变分自编码器（VAE）到扩散模型（DDPM）。对于每种模型，我们查看了生成模型三难题，并获得了以下结果：
- en: '![](../Images/f849303d137db4aedaebd3c75b648cf6.png)'
  id: totrans-147
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f849303d137db4aedaebd3c75b648cf6.png)'
- en: Comparing Models. Image Created by Daisuke Yamada
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 比较模型。图像由大辅·山田创建
- en: 'The natural question is: can we get all 3 parts of the Generative Model Trilemma?
    Well it seems like diffusion is almost there, as we just need to figure out a
    way to increase the speed of sampling. Intuitively this is difficult because we
    relied on the assumption that we can model the reverse process as a Gaussian,
    which only works if we do the reverse process at nearly all timesteps.'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 自然的问题是：我们能否获得生成模型三难问题的所有3个部分？看起来扩散模型几乎达到了这一点，因为我们只需找出提高采样速度的方法。从直观上讲，这很困难，因为我们依赖于将逆过程建模为高斯过程的假设，这只有在我们在几乎所有时间步长中执行逆过程时才有效。
- en: However, it turns out getting all 3 factors of the Trilemma is possible!Models
    like DDIMs or DDGANs build on top of DDPMs, but they have figured out ways to
    increase the speed of sampling (one way is to use a strided sampling schedule**).**
    With this and different other optimizations, we can obtain all 3 facets of the
    Generative Model Trilemma!
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，实际上，获得三难问题的所有3个因素是可能的！像DDIMs或DDGANs这样的模型建立在DDPMs之上，但它们已经找到了提高采样速度的方法（其中一种方法是使用**跨步采样计划**）。通过这些和其他不同的优化，我们可以获得生成模型三难问题的所有3个方面！
- en: So, what’s next? One particular interesting avenue is conditional generation.Conditional
    generation allows you to generate new samples conditioned on some class labels
    or descriptive text. For example, in all of the image generation models initially
    listed you can input something like “Penguin bench pressing 1000 pounds” and get
    a reasonable output. Although we didn’t have time to explore this avenue of conditional
    generation, it seems like a very interesting next step!
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，接下来是什么？一个特别有趣的方向是条件生成。条件生成允许你在一些类别标签或描述性文本的条件下生成新样本。例如，在所有最初列出的图像生成模型中，你可以输入类似“企鹅卧推1000磅”的内容，并获得合理的输出。虽然我们没有时间探索这一条件生成的方向，但它似乎是一个非常有趣的下一步！
- en: '**Well, that’s all from us. Thank you for reading!**'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '**好吧，这就是我们的全部内容。感谢阅读！**'
- en: '*Unless otherwise stated, all images are created by the author(s).*'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: '*除非另有说明，所有图片均由作者（们）创建。*'
