- en: How to Chunk Text Data — A Comparative Analysis
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何分块文本数据 — 一项比较分析
- en: 原文：[https://towardsdatascience.com/how-to-chunk-text-data-a-comparative-analysis-3858c4a0997a?source=collection_archive---------0-----------------------#2023-07-20](https://towardsdatascience.com/how-to-chunk-text-data-a-comparative-analysis-3858c4a0997a?source=collection_archive---------0-----------------------#2023-07-20)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/how-to-chunk-text-data-a-comparative-analysis-3858c4a0997a?source=collection_archive---------0-----------------------#2023-07-20](https://towardsdatascience.com/how-to-chunk-text-data-a-comparative-analysis-3858c4a0997a?source=collection_archive---------0-----------------------#2023-07-20)
- en: Exploring distinct approaches to text chunking.
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 探索文本分块的不同方法。
- en: '[](https://solano-todeschini.medium.com/?source=post_page-----3858c4a0997a--------------------------------)[![Solano
    Todeschini](../Images/75e871340659c8df37f558b74c9d73c5.png)](https://solano-todeschini.medium.com/?source=post_page-----3858c4a0997a--------------------------------)[](https://towardsdatascience.com/?source=post_page-----3858c4a0997a--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----3858c4a0997a--------------------------------)
    [Solano Todeschini](https://solano-todeschini.medium.com/?source=post_page-----3858c4a0997a--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://solano-todeschini.medium.com/?source=post_page-----3858c4a0997a--------------------------------)[![Solano
    Todeschini](../Images/75e871340659c8df37f558b74c9d73c5.png)](https://solano-todeschini.medium.com/?source=post_page-----3858c4a0997a--------------------------------)[](https://towardsdatascience.com/?source=post_page-----3858c4a0997a--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----3858c4a0997a--------------------------------)
    [Solano Todeschini](https://solano-todeschini.medium.com/?source=post_page-----3858c4a0997a--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F618a52c38c0c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-chunk-text-data-a-comparative-analysis-3858c4a0997a&user=Solano+Todeschini&userId=618a52c38c0c&source=post_page-618a52c38c0c----3858c4a0997a---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----3858c4a0997a--------------------------------)
    ·17 min read·Jul 20, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F3858c4a0997a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-chunk-text-data-a-comparative-analysis-3858c4a0997a&user=Solano+Todeschini&userId=618a52c38c0c&source=-----3858c4a0997a---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F618a52c38c0c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-chunk-text-data-a-comparative-analysis-3858c4a0997a&user=Solano+Todeschini&userId=618a52c38c0c&source=post_page-618a52c38c0c----3858c4a0997a---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----3858c4a0997a--------------------------------)
    ·17分钟阅读·2023年7月20日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F3858c4a0997a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-chunk-text-data-a-comparative-analysis-3858c4a0997a&user=Solano+Todeschini&userId=618a52c38c0c&source=-----3858c4a0997a---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F3858c4a0997a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-chunk-text-data-a-comparative-analysis-3858c4a0997a&source=-----3858c4a0997a---------------------bookmark_footer-----------)![](../Images/dfd6dd3b7af559b26e33e94122605cf6.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F3858c4a0997a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-chunk-text-data-a-comparative-analysis-3858c4a0997a&source=-----3858c4a0997a---------------------bookmark_footer-----------)![](../Images/dfd6dd3b7af559b26e33e94122605cf6.png)'
- en: Image compiled by the author. Pineapple image from Canva.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者整理。菠萝图片来自Canva。
- en: Introduction
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: The ‘Text chunking’ process in Natural Language Processing (NLP) involves the
    conversion of unstructured text data into meaningful units. This seemingly simple
    task belies the complexity of the various methods employed to achieve it, each
    with its strengths and weaknesses.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 自然语言处理（NLP）中的“文本分块”过程涉及将非结构化文本数据转换为有意义的单位。这一看似简单的任务掩盖了实现这一目标的各种方法的复杂性，每种方法都有其优缺点。
- en: At a high level, these methods typically fall into one of two categories. The
    first, rule-based methods, hinge on the use of explicit separators such as punctuation
    or space characters, or the application of sophisticated systems like regular
    expressions, to partition text into chunks. The second category, semantic clustering
    methods, leverages the inherent meaning embedded in the text to guide the chunking
    process. These might utilize machine learning algorithms to discern context and
    infer natural divisions within the text.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 从高层次来看，这些方法通常分为两类。第一类是基于规则的方法，依赖于使用明确的分隔符，如标点符号或空格字符，或应用复杂的系统如正则表达式，将文本分成块。第二类是语义聚类方法，利用文本中固有的意义来指导分块过程。这些方法可能利用机器学习算法来辨别上下文，并推断文本中的自然分界。
- en: 'In this article, we’ll explore and compare these two distinct approaches to
    text chunking. We’ll represent rule-based methods with NLTK, Spacy, and Langchain,
    and contrast this with two different semantic clustering techniques: KMeans and
    a custom technique for Adjacent Sentence Clustering.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们将探讨并比较这两种不同的文本分块方法。我们将用NLTK、Spacy和Langchain来表示基于规则的方法，并将其与两种不同的语义聚类技术进行对比：KMeans和一种自定义的邻近句子聚类技术。
- en: The goal is to equip practitioners with a clear understanding of each method’s
    pros, cons, and ideal use cases to enable better decision-making in their NLP
    projects.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 目标是让从业者清楚理解每种方法的优缺点和理想的使用场景，以便在他们的NLP项目中做出更好的决策。
- en: In Brazilian slang, “abacaxi,” which translates to “pineapple,” signifies “something
    that doesn’t yield a good outcome, a tangled mess, or something that is no good.”
  id: totrans-14
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 在巴西俚语中，“abacaxi”翻译为“菠萝”，意指“某事未能产生良好结果、混乱的局面或毫无价值的事物。”
- en: Use Cases for Text Chunking
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 文本分块的使用案例
- en: 'Text chunking can be used by several different applications:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 文本分块可以被多种不同的应用程序使用：
- en: '**Text Summarization**: By breaking down large bodies of text into manageable
    chunks, we can summarize each section individually, leading to a more accurate
    overall summary.'
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**文本摘要**：通过将大段文本拆分为可管理的块，我们可以对每个部分进行单独总结，从而得到更准确的总体摘要。'
- en: '**Sentiment Analysis**: Analyzing the sentiment of shorter, coherent chunks
    can often yield more precise results than analyzing an entire document.'
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**情感分析**：分析较短且连贯的文本块通常能比分析整个文档更精确地得出结果。'
- en: '**Information Extraction**: Chunking helps in locating specific entities or
    phrases within text, enhancing the process of information retrieval.'
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**信息提取**：分块有助于定位文本中的特定实体或短语，增强信息检索的过程。'
- en: '**Text Classification**: Breaking down text into chunks allows classifiers
    to focus on smaller, contextually meaningful units rather than entire documents,
    which can improve performance.'
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**文本分类**：将文本分解成块允许分类器专注于较小且有上下文意义的单元，而不是整个文档，这可以提高性能。'
- en: '**Machine Translation**: Translation systems often operate on chunks of text
    rather than on individual words or whole documents. Chunking can aid in maintaining
    the coherence of the translated text.'
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**机器翻译**：翻译系统通常在文本块上操作，而不是单个单词或整个文档。分块有助于保持翻译文本的一致性。'
- en: Understanding these use cases can help in choosing the most suitable chunking
    technique for your specific project.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 理解这些使用场景可以帮助选择最适合您特定项目的分块技术。
- en: Comparing Different Methods for Semantic Chunking
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 语义分块的不同方法比较
- en: 'In this part of the article, we will compare popular methods for semantic chunking
    of unstructured text: NLTK Sentence Tokenizer, Langchain Text Splitter, KMeans
    Clustering, and Clustering Adjacent Sentences based on similarity.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文的这一部分，我们将比较流行的非结构化文本语义分块方法：NLTK句子分割器、Langchain文本分割器、KMeans聚类以及基于相似性的邻近句子聚类。
- en: In the following example, we’re gonna evaluate this technique using a text extracted
    from a PDF, processing it into sentences and their clusters.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的例子中，我们将使用从PDF中提取的文本评估这一技术，将其处理为句子及其聚类。
- en: The data we used was a PDF exported from [Brazil’s Wikipedia page](https://en.wikipedia.org/wiki/Brazil).
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用的数据是从[巴西的维基百科页面](https://en.wikipedia.org/wiki/Brazil)导出的PDF。
- en: 'For extracting text from PDF and split into sentences with NLTK,we use the
    following functions:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 为了从PDF中提取文本并用NLTK将其分割成句子，我们使用以下函数：
- en: '[PRE0]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Like that, we end with a string `text` with 210964 characters of length.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 如此，我们得到了一个长度为210964个字符的字符串`text`。
- en: 'Here''s a sample of the Wiki text:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 这是维基文本的一个样本：
- en: '[PRE1]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: NLTK Sentence Tokenizer
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: NLTK句子分割器
- en: The Natural Language Toolkit (NLTK) provides a useful function for splitting
    text into sentences. This sentence tokenizer divides a given block of text into
    its component sentences, which can then be used for further processing.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 自然语言工具包 (NLTK) 提供了一个将文本划分为句子的有用功能。这个句子分词器将给定的文本块划分为其组成句子，然后可以用于进一步处理。
- en: Implementation
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实现
- en: 'Here’s an example of using the NLTK sentence tokenizer:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 这是使用 NLTK 句子分词器的一个示例：
- en: '[PRE2]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: This returns a list of 2670 `sentences` extracted from the input text with a
    mean of 78 characters per sentence.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 这会返回从输入文本中提取的 2670 个 `句子`，每个句子的平均字符数为 78。
- en: Evaluating NLTK Sentence Tokenizer
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 评估 NLTK 句子分词器
- en: 'While the NLTK Sentence Tokenizer is a straightforward and efficient way to
    divide a large body of text into individual sentences, it does come with certain
    limitations:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然 NLTK 句子分词器是一种将大量文本划分为单独句子的直接有效方法，但它确实存在某些局限性：
- en: '**Language Dependency**: The NLTK Sentence Tokenizer relies heavily on the
    language of the text. It performs well with English but may not provide accurate
    results with other languages without additional configuration.'
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**语言依赖性**：NLTK 句子分词器在很大程度上依赖于文本的语言。它在英语中表现良好，但对于其他语言，可能需要额外配置才能提供准确结果。'
- en: '**Abbreviations and Punctuation**: The tokenizer can occasionally misinterpret
    abbreviations or other punctuation at the end of a sentence. This can lead to
    fragments of sentences being treated as independent sentences.'
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**缩写和标点**：分词器有时会误解句子末尾的缩写或其他标点符号。这可能导致句子片段被当作独立的句子处理。'
- en: '**Lack of Semantic Understanding**: Like most tokenizers, the NLTK Sentence
    Tokenizer does not consider the semantic relationship between sentences. Therefore,
    a context that spans multiple sentences might be lost in the tokenization process.'
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**语义理解的缺乏**：像大多数分词器一样，NLTK 句子分词器不考虑句子之间的语义关系。因此，跨越多个句子的上下文可能在分词过程中丢失。'
- en: Spacy Sentence Splitter
  id: totrans-43
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Spacy 句子分割器
- en: Spacy, another powerful NLP library, provides a sentence tokenization function
    that relies heavily on linguistic rules. It is a similar approach to NLTK.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: Spacy 另一个强大的 NLP 库，提供了一种依赖语言规则的句子分词功能。这与 NLTK 的方法类似。
- en: Implementation
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实现
- en: 'Implementing Spacy’s sentence splitter is quite straightforward. Here’s how
    to do it in Python:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 实现 Spacy 的句子分割器相当简单。以下是在 Python 中的实现方法：
- en: '[PRE3]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: This returns a list of 2336 `sentences` extracted from the input text with a
    mean of 89 characters per sentence.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 这会返回从输入文本中提取的 2336 个 `句子`，每个句子的平均字符数为 89。
- en: Evaluating Spacy Sentence Splitter
  id: totrans-49
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 评估 Spacy 句子分割器
- en: Spacy’s sentence splitter tends to create smaller chunks compared to the Langchain
    Character Text Splitter, as it strictly adheres to sentence boundaries. This can
    be advantageous when smaller text units are necessary for analysis.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 与 Langchain 字符文本分割器相比，Spacy 的句子分割器倾向于创建较小的块，因为它严格遵守句子边界。当分析需要较小的文本单元时，这可能是一个优势。
- en: Like NLTK, however, Spacy’s performance depends on the quality of the input
    text. For poorly punctuated or structured text, the identified sentence boundaries
    might not always be accurate.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，与 NLTK 一样，Spacy 的性能取决于输入文本的质量。对于标点或结构较差的文本，识别的句子边界可能并不总是准确。
- en: Now, we’ll see how Langchain provides a framework for chunking text data and
    further compare it with NLTK and Spacy.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将查看 Langchain 如何提供文本数据分块的框架，并进一步与 NLTK 和 Spacy 进行比较。
- en: Langchain Character Text Splitter
  id: totrans-53
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Langchain 字符文本分割器
- en: The Langchain Character Text Splitter works by recursively dividing the text
    at specific characters. It is especially useful for generic text.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: Langchain 字符文本分割器通过在特定字符处递归地划分文本来工作。它对通用文本特别有用。
- en: The splitter is defined by a list of characters. It attempts to split the text
    based on these characters until the generated chunks meet the desired size criterion.
    The default list is [“\n\n”, “\n”, “ ”, “”], aiming to keep paragraphs, sentences,
    and words together as much as possible to maintain semantic coherence.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 该分割器由一系列字符定义。它尝试根据这些字符划分文本，直到生成的块符合所需的大小标准。默认列表是[“\n\n”， “\n”， “ ”， “”]，旨在尽可能保持段落、句子和词汇的连贯，以维持语义一致性。
- en: Implementation
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实现
- en: Consider the following example, where we split the sample text extracted from
    our PDF using this method.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑以下示例，我们使用这种方法分割了从 PDF 中提取的示例文本。
- en: '[PRE4]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Finally, we end up with 3205 chunks of text, represented by the `texts` list.
    65.8 charactersis the mean for each chunk here — a bit less thank NLTK's mean
    (79 characters).
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们得到 3205 个文本块，由 `texts` 列表表示。每个块的平均长度为 65.8 个字符——比 NLTK 的平均长度（79 个字符）稍少。
- en: '**Changing Parameters and Using ''\n'' Separator:**'
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**更改参数和使用 ''\n'' 分隔符：**'
- en: For a more customized approach on the Langchain Splitter, we can alter the `chunk_size`
    and `chunk_overlap` parameters according to our needs. Additionally, we can specify
    only one character (or set of characters) for the splitting operation, such as
    `\n`. This will guide the splitter to separate the text into chunks only at the
    new line characters.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 Langchain Splitter 的更自定义方法，我们可以根据需要更改 `chunk_size` 和 `chunk_overlap` 参数。此外，我们还可以仅指定一个字符（或字符集）作为分隔操作，例如
    `\n`。这将指导分隔器仅在新行字符处将文本拆分成块。
- en: Let’s consider an example where we set `chunk_size` to 300, `chunk_overlap`
    to 30, and only use `\n` as the separator.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑一个例子，其中我们将 `chunk_size` 设置为 300，`chunk_overlap` 设置为 30，并且只使用 `\n` 作为分隔符。
- en: '[PRE5]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Now, let''s compare some outputs from the standard set of parameters with the
    custom parameters:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们比较一些标准参数集与自定义参数的输出：
- en: '[PRE6]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: We can already see that these custom parameters yield much bigger chunks and
    therefore keep more content than the default set of parameters.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经可以看到，这些自定义参数产生了更大的块，从而保留了比默认参数集更多的内容。
- en: Evaluating the Langchain Character Text Splitter
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 评估 Langchain 字符文本拆分器
- en: 'After splitting the text into chunks using different parameters, we obtain
    two lists of chunks: `texts` and `custom_texts`, containing 3205 and 1404 text
    chunks, respectively. Now, let''s plot the distribution of chunk lengths for these
    two scenarios to better understand the impact of changing the parameters.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用不同参数将文本拆分成块后，我们获得了两个块列表：`texts` 和 `custom_texts`，分别包含 3205 和 1404 个文本块。现在，让我们绘制这两种情况下块长度的分布图，以更好地理解更改参数的影响。
- en: '![](../Images/6bc0fe338ed1534c308f134cf80fa36b.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6bc0fe338ed1534c308f134cf80fa36b.png)'
- en: '**Figure 1**: Distribution plot of chunk lengths for Langchain splitter with
    different parameters (Image by Author)'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '**图1**：Langchain 分隔器不同参数的块长度分布图（图像由作者提供）'
- en: In this histogram, the x-axis represents the chunk lengths, while the y-axis
    represents the frequency of each length. The blue bars represent the distribution
    of chunk lengths for the original parameters, and the orange bars represent the
    distribution of the custom parameters. By comparing these two distributions, we
    can see how the changes in parameters affected the resulting chunk lengths.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个直方图中，x 轴表示块长度，而 y 轴表示每个长度的频率。蓝色条表示原始参数的块长度分布，橙色条表示自定义参数的分布。通过比较这两种分布，我们可以看到参数变化如何影响生成的块长度。
- en: '*Remember, the ideal distribution depends on the specific requirements of your
    text-processing task. You might want smaller, more numerous chunks if you’re dealing
    with fine-grained analysis or larger, fewer chunks for broader semantic analysis.*'
  id: totrans-72
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*请记住，理想的分布取决于您文本处理任务的具体要求。如果您处理的是精细的分析，可能需要更小、更众多的块；如果是更广泛的语义分析，则可能需要更大、更少的块。*'
- en: Langchain Character Text Splitter vs. NLTK and Spacy
  id: totrans-73
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Langchain 字符文本拆分器与 NLTK 和 Spacy
- en: Earlier, we generated 3205 chunks using the Langchain splitter with its default
    parameters. The NLTK Sentence Tokenizer, on the other hand, split the same text
    into a total of 2670 sentences.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 早些时候，我们使用 Langchain 分隔器及其默认参数生成了 3205 个块。而 NLTK 句子分词器将相同的文本拆分成总共 2670 个句子。
- en: To get a more intuitive understanding of the difference between these methods,
    we can visualize the distribution of chunk lengths. The following plot shows the
    densities of chunk lengths for each method, allowing us to see how the lengths
    are distributed and where most of the lengths lie.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更直观地理解这些方法之间的差异，我们可以可视化块长度的分布。下面的图显示了每种方法的块长度密度，让我们可以看到长度的分布情况及其大多数长度的位置。
- en: '![](../Images/55d688776a0dcc8cd5b779ba04b3d185.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/55d688776a0dcc8cd5b779ba04b3d185.png)'
- en: '**Figure 2**: Distribution plot of chunk lengths resulting from Langchain Splitter
    with custom parameters vs. NLTK and Spacy (Image by Author)'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '**图2**：使用自定义参数的 Langchain Splitter 与 NLTK 和 Spacy 的块长度分布图（图像由作者提供）'
- en: From Figure 1, we can see that the Langchain splitter results in a much more
    concise density of cluster lengths and has a tendency to have more of longer clusters
    whereas NLTK and Spacy seem to produce very similar outputs in terms of cluster
    length, preferring smaller sentences while having lots of outliers with lengths
    that can reach up to 1400 characters — and a tendency of decreasing length.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 从图 1 中，我们可以看到 Langchain 分割器结果具有更简洁的聚类长度密度，并且更倾向于生成较长的聚类，而 NLTK 和 Spacy 似乎在聚类长度方面产生非常相似的输出，倾向于较小的句子，同时存在许多长度可达
    1400 个字符的离群点，并且长度有减少的趋势。
- en: KMeans Clustering
  id: totrans-79
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: KMeans 聚类
- en: Sentence Clustering is a technique that involves grouping sentences based on
    their semantic similarity. By using sentence embeddings and a clustering algorithm
    such as K-means, we can implement Sentence Clustering.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 句子聚类是一种根据句子的语义相似性进行分组的技术。通过使用句子嵌入和诸如 K-means 的聚类算法，我们可以实现句子聚类。
- en: Implementation
  id: totrans-81
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实现
- en: 'Here is a simple example code snippet using the Python library `sentence-transformers`
    for generating sentence embeddings and `scikit-learn` for K-means clustering:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个使用 Python 库 `sentence-transformers` 生成句子嵌入和 `scikit-learn` 进行 K-means 聚类的简单示例代码片段：
- en: '[PRE7]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'You can see here that the steps for clustering a list of sentences are:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在这里看到对句子列表进行聚类的步骤：
- en: Load a Sentence Transform model. In this case, we're using `all-MiniLM-L6-v2`
    from sentence-transformers/all-MiniLM-L6-v2 in [HuggingFace](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2).
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载一个句子转换模型。在这种情况下，我们使用的是来自 [HuggingFace](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2)
    的`all-MiniLM-L6-v2`模型。
- en: Define your sentences and generate their embeddings with the `encode()` method
    from the model.
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用模型的`encode()`方法定义你的句子并生成它们的嵌入。
- en: Then you define your clustering technique and number of clusters (we're using
    KMeans with 3 clusters here) and finally fit it into the dataset.
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后你定义你的聚类技术和聚类数量（这里我们使用的是 3 个聚类的 KMeans），并最终将其适配到数据集。
- en: Evaluating KMeans Clustering
  id: totrans-88
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 评估 KMeans 聚类
- en: And finally we plot a WordCloud for each cluster.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们为每个聚类绘制一个词云。
- en: '[PRE8]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Below we have the WordCloud plots for the generated clusters:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是生成的聚类的词云图：
- en: '![](../Images/d315eac1d84c9a0a389376dedeab4441.png)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d315eac1d84c9a0a389376dedeab4441.png)'
- en: '**Figure 3**: Word Cloud plot for KMeans clustering — cluster 0 (Image by Author)'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 3**：KMeans 聚类的词云图 — 聚类 0（图片由作者提供）'
- en: '![](../Images/2e9fcb7f2635eb6512d863ef5f8aa009.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2e9fcb7f2635eb6512d863ef5f8aa009.png)'
- en: '**Figure 4**: Word Cloud plot for KMeans clustering — cluster 1 (Image by Author)'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 4**：KMeans 聚类的词云图 — 聚类 1（图片由作者提供）'
- en: '![](../Images/aa981bfdb20ca0356275a6cbc557f8a6.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/aa981bfdb20ca0356275a6cbc557f8a6.png)'
- en: '**Figure 5**: Word Cloud plot for KMeans clustering — cluster 2 (Image by Author)'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 5**：KMeans 聚类的词云图 — 聚类 2（图片由作者提供）'
- en: In our analysis of the word cloud for the KMeans clustering, it’s evident that
    each cluster distinctively differentiates based on the semantics of its most frequent
    words. This demonstrates a strong semantic differentiation amongst clusters. Moreover,
    a noticeable variation in cluster sizes is observed, indicating a significant
    disparity in the number of sequences each cluster comprises.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们对 KMeans 聚类的词云分析中，很明显每个聚类在其最常见词汇的语义上具有显著的差异。这显示了聚类之间的强语义区分。此外，观察到聚类大小的明显变化，表明每个聚类所包含的序列数量存在显著差异。
- en: Limitations of KMeans Clustering
  id: totrans-99
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: KMeans 聚类的局限性
- en: 'Sentence clustering, although beneficial, does have a few notable drawbacks.
    The primary limitations include:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管句子聚类有益，但也存在一些显著的缺点。主要的局限性包括：
- en: '**Loss of Sentence Order**: Sentence clustering doesn’t retain the original
    sequence of sentences, which could distort the natural flow of the narrative.
    ** This is very important**'
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**句子顺序丧失**：句子聚类不会保留原始句子的顺序，这可能会扭曲叙事的自然流动。**这点非常重要**'
- en: '**Computational Efficiency**: KMeans can be computationally intensive and slow,
    especially with large text corpora or when working with a larger number of clusters.
    This can be a significant drawback for real-time applications or when handling
    big data.'
  id: totrans-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**计算效率**：KMeans 可能会计算密集且较慢，特别是在处理大型文本语料库或较多聚类时。这对于实时应用或处理大数据来说可能是一个显著的缺点。'
- en: Clustering Adjacent Sentences
  id: totrans-103
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 聚类相邻句子
- en: To overcome some of the limitations of KMeans clustering, especially the loss
    of sentence order, an alternative approach could be clustering adjacent sentences
    based on their semantic similarity. The fundamental premise of this approach is
    that two sentences that appear consecutively in a text are more likely to be semantically
    related than two sentences that are farther apart.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 为了克服 KMeans 聚类的一些局限性，尤其是句子顺序的丢失，另一种方法可以是基于语义相似性对相邻句子进行聚类。该方法的基本前提是，文本中连续出现的两个句子比相距较远的两个句子更有可能在语义上相关。
- en: Implementation
  id: totrans-105
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实施
- en: 'Here’s an expanded implementation of this heuristics using Spacy sentences
    as inputs:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是使用 Spacy 句子作为输入的启发式扩展实现：
- en: '[PRE9]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '*Key takeaways from this code:*'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '*代码中的主要收获：*'
- en: '**Text Processing**: Each text chunk is passed to the `process` function. This
    function uses the SpaCy library to create sentence embeddings, which are used
    to represent the semantic meaning of each sentence in the text chunk.'
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**文本处理**：每个文本片段都传递给`process`函数。该函数使用 SpaCy 库创建句子嵌入，这些嵌入用于表示文本片段中每个句子的语义意义。'
- en: '**Cluster Creation**: The `cluster_text` function forms clusters of sentences
    based on the cosine similarity of their embeddings. If the cosine similarity is
    less than a specified threshold, a new cluster begins.'
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**簇创建**：`cluster_text`函数根据句子嵌入的余弦相似性形成句子簇。如果余弦相似性低于指定的阈值，则开始一个新的簇。'
- en: '**Length Check**: The code then checks the length of each cluster. If a cluster
    is too short (less than 60 characters) or too long (more than 3000 characters),
    the threshold is adjusted and the process repeats for that particular cluster
    until an acceptable length is achieved.'
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**长度检查**：代码接着检查每个簇的长度。如果一个簇太短（少于 60 个字符）或太长（超过 3000 个字符），则调整阈值，并对该簇重复处理，直到达到可接受的长度。'
- en: 'Let’s take a look at some of the output chunks from this approach and compare
    them to Langchain Splitter:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 我们来看一下这种方法的一些输出片段，并将它们与 Langchain 分割器进行比较：
- en: '[PRE10]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Great, now let’s compare the distribution of chunk lengths of the `final_texts`
    (from the adjacent sequence clustering approach) with the distributions from the
    Langchain Character Text Splitter and NLTK Sentence Tokenizer. To do this, we''ll
    first need to calculate the lengths of the chunks in `final_texts`:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 很好，现在让我们将`final_texts`（来自相邻序列聚类方法）的片段长度分布与 Langchain 字符文本分割器和 NLTK 句子分词器的分布进行比较。为此，我们首先需要计算`final_texts`中片段的长度：
- en: '[PRE11]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'We can now plot the distributions of all three methods:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以绘制三种方法的分布图：
- en: '![](../Images/296e3efe4e40f3ca545e546ceb143638.png)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/296e3efe4e40f3ca545e546ceb143638.png)'
- en: '**Figure 3**: Distribution plot of chunk lengths resulting from all the different
    methods tested (Image by Author)'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 3**：测试的所有不同方法生成的片段长度分布图（图像由作者提供）'
- en: From Figure 6, we can derive that the Langchain splitter, using its predefined
    chunk size, creates a uniform distribution, implying consistent chunk lengths.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 从图 6 中，我们可以推导出 Langchain 分割器使用其预定义的片段大小，创建了一个均匀的分布，意味着片段长度一致。
- en: The Spacy Sentence Splitter and the NLTK Sentence Tokenizer, on the other hand,
    seem to prefer smaller sentences, though with many larger outliers, indicating
    their reliance on linguistic cues to determine splits and potentially produce
    irregularly sized chunks.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，Spacy 句子分割器和 NLTK 句子分词器似乎更倾向于较短的句子，虽然存在许多较大的离群值，表明它们依赖语言线索来确定分割，可能会产生不规则大小的片段。
- en: Lastly, the custom Adjacent Sequence Clustering approach, which clusters based
    on semantic similarity, exhibits a more varied distribution. This could be indicative
    of a more context-sensitive approach, maintaining the coherence of content within
    chunks while allowing for more flexibility in size.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，自定义的相邻序列聚类方法，根据语义相似性进行聚类，展现出更为多样化的分布。这可能表明一种更具上下文敏感的方法，在保持片段内容连贯的同时，允许更大的灵活性。
- en: Evaluating Adjacent Sequence Clustering Approach
  id: totrans-122
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 评估相邻序列聚类方法
- en: 'The Adjacent Sequence Clustering Approach brings **unique benefits**:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 相邻序列聚类方法带来了**独特的好处**：
- en: '**Contextual Coherence**: Generates thematically consistent chunks by considering
    semantic and contextual coherence.'
  id: totrans-124
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**上下文一致性**：通过考虑语义和上下文一致性生成主题一致的片段。'
- en: '**Flexibility**: Balances context preservation and computational efficiency,
    providing adjustable chunk sizes.'
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**灵活性**：平衡上下文保持和计算效率，提供可调的片段大小。'
- en: '**Threshold Tuning**: Allows users to fine-tune the chunking process according
    to their needs, by adjusting the similarity threshold.'
  id: totrans-126
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**阈值调整**：允许用户根据需求微调分块过程，通过调整相似度阈值。'
- en: '**Sequence Preservation**: Retains the original order of sentences in the text,
    essential for sequential language models and tasks where text order matters.'
  id: totrans-127
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**序列保留**：保留文本中句子的原始顺序，对于需要文本顺序的序列语言模型和任务至关重要。'
- en: 'Comparing Text Chunking Methods: Summary of Insights'
  id: totrans-128
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 文本分块方法比较：见解总结
- en: Langchain Character Text Splitter
  id: totrans-129
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Langchain 字符文本分割器
- en: This method provides consistent chunk lengths, yielding a uniform distribution.
    This could be beneficial when a standard size is necessary for downstream processing
    or analysis. The approach is less sensitive to the specific linguistic structure
    of the text, focusing more on producing chunks of a predefined character length.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法提供了均匀的块长度，产生均匀的分布。当下游处理或分析需要标准大小时，这可能是有益的。这种方法对文本的具体语言结构不那么敏感，更侧重于产生预定义字符长度的块。
- en: NLTK Sentence Tokenizer and Spacy Sentence Splitter
  id: totrans-131
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: NLTK 句子分词器和 Spacy 句子分割器
- en: These approaches exhibit a preference for smaller sentences but include many
    larger outliers. While this can result in more linguistically coherent chunks,
    it can also lead to high variability in chunk size.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 这些方法表现出对较小句子的偏好，但包括许多较大的离群点。虽然这可以导致更具语言一致性的块，但也可能导致块大小的高度可变性。
- en: These methods can yield good results that can serve as inputs to downstream
    tasks too.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 这些方法也能产生良好的结果，可以作为下游任务的输入。
- en: Adjacent Sequence Clustering
  id: totrans-134
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 相邻序列聚类
- en: This method generates a more varied distribution, indicative of its context-sensitive
    approach. By clustering based on semantic similarity, it ensures that the content
    within each chunk is coherent while allowing for flexibility in chunk size. This
    method may be advantageous when it is important to preserve the semantic continuity
    of text data.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法生成了更多样的分布，表明了其上下文敏感的方法。通过基于语义相似性进行聚类，确保每个块中的内容是连贯的，同时允许块大小的灵活性。当保持文本数据的语义连续性很重要时，这种方法可能是有利的。
- en: 'For a more visual and abstract (or silly) representation, let''s look at Figure
    7 below and try to figure out which kind of pineapple "cut" would better represent
    the approaches discussed:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更直观和抽象（或有趣）的表示，让我们看看下面的图7，试着找出哪种菠萝“切块”更好地代表所讨论的方法：
- en: '![](../Images/89004ea38220a7f04c73cac9d5616f0f.png)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/89004ea38220a7f04c73cac9d5616f0f.png)'
- en: '**Figure 7**: Different methods of text chunking shown as pineapple cuts (Image
    compiled by the author. Pineapple image from Canva)'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '**图7**：不同的文本分块方法以菠萝切块形式展示（图像由作者整理，菠萝图像来自 Canva）'
- en: 'Listing them in order:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 按顺序列出：
- en: Cut number 1 would represent a rule-based approach, in which you can just "peel
    off" the "junk" text you want based on filters or regular expressions. *Lot's
    of work* to do the whole pineapple tho, since it also retains a lot of outliers
    with a much bigger context size.
  id: totrans-140
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 切割方法1 代表了基于规则的方法，你可以根据过滤器或正则表达式“剥离”你想要的“垃圾”文本。不过要处理整个菠萝可要花不少功夫，因为它还保留了许多上下文较大的离群点。*这确实需要很多工作*。
- en: Langchain would be like cut number 2\. Very similar pieces in size but not holding
    the entire desired context (it's a triangle, so it could be a watermelon as well).
  id: totrans-141
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Langchain 类似于切割方法2。大小非常相似，但未能包含整个所需的上下文（它是一个三角形，所以也可能是一个西瓜）。
- en: Cut number 3 is definitely KMeans. You may even group only what makes sense
    for you — the juiciest part — but you won't get its core. Without it, the chunks
    lose all the structure and meaning. I think it takes a lot of work to do that
    as well… *especially for bigger pineapples*.
  id: totrans-142
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 切割方法3 绝对是 KMeans。你甚至可以只分组那些对你有意义的部分——最有汁的部分——但你无法得到其核心。没有它，块会失去所有结构和意义。我认为这也需要很多工作……
    *特别是对于更大的菠萝*。
- en: Lastly, cut number 4 illustrates the Adjacent Sentence Clustering method. The
    size of the chunks can vary but they often maintain contextual information, similar
    to uneven pineapple pieces that still indicate the fruit’s overall structure.
  id: totrans-143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，切割方法4展示了相邻句子聚类方法。块的大小可能有所不同，但通常保持上下文信息，类似于不规则的菠萝块，仍然能显示出水果的整体结构。
- en: '***TL;DR****: In this article, we’ve compared three text chunking methods and
    their unique benefits. Langchain offers consistent chunk sizes, but the linguistic
    structure takes a back seat. NLTK and Spacy give linguistically coherent chunks,
    yet the size varies considerably. Adjacent Sequence Clustering clusters based
    on semantic similarity, providing content coherence with flexible chunk sizes.
    Ultimately, the optimal choice hinges on your specific needs, including linguistic
    coherence, uniformity in chunk size, and available computational power.*'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '***总结***：在这篇文章中，我们比较了三种文本分块方法及其独特的好处。Langchain 提供了一致的分块大小，但语言结构却被忽视。NLTK 和
    Spacy 提供了语言上连贯的分块，但大小差异较大。相邻序列聚类基于语义相似性进行聚类，提供内容连贯性和灵活的分块大小。*最终，最佳选择取决于你的具体需求，包括语言连贯性、分块大小的一致性和可用的计算能力。*'
- en: Thank you for reading!
  id: totrans-145
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 感谢阅读！
- en: Follow me on [Linkedin](https://www.linkedin.com/in/solano-todeschini/)!
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 关注我 [Linkedin](https://www.linkedin.com/in/solano-todeschini/)!
