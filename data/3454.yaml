- en: A Gentle Introduction to Deep Reinforcement Learning in JAX
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ã€Šæ·±å…¥æµ…å‡º JAX ä¸­çš„æ·±åº¦å¼ºåŒ–å­¦ä¹ ã€‹
- en: åŸæ–‡ï¼š[https://towardsdatascience.com/a-gentle-introduction-to-deep-reinforcement-learning-in-jax-c1e45a179b92?source=collection_archive---------4-----------------------#2023-11-21](https://towardsdatascience.com/a-gentle-introduction-to-deep-reinforcement-learning-in-jax-c1e45a179b92?source=collection_archive---------4-----------------------#2023-11-21)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡ï¼š[https://towardsdatascience.com/a-gentle-introduction-to-deep-reinforcement-learning-in-jax-c1e45a179b92?source=collection_archive---------4-----------------------#2023-11-21](https://towardsdatascience.com/a-gentle-introduction-to-deep-reinforcement-learning-in-jax-c1e45a179b92?source=collection_archive---------4-----------------------#2023-11-21)
- en: Solving the CartPole environment with DQN in under a second
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: åœ¨ä¸€ç§’é’Ÿå†…ç”¨ DQN è§£å†³ CartPole ç¯å¢ƒ
- en: '[](https://medium.com/@ryanpegoud?source=post_page-----c1e45a179b92--------------------------------)[![Ryan
    PÃ©goud](../Images/9314b76c2be56bda8b73b4badf9e3e4d.png)](https://medium.com/@ryanpegoud?source=post_page-----c1e45a179b92--------------------------------)[](https://towardsdatascience.com/?source=post_page-----c1e45a179b92--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----c1e45a179b92--------------------------------)
    [Ryan PÃ©goud](https://medium.com/@ryanpegoud?source=post_page-----c1e45a179b92--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@ryanpegoud?source=post_page-----c1e45a179b92--------------------------------)[![Ryan
    PÃ©goud](../Images/9314b76c2be56bda8b73b4badf9e3e4d.png)](https://medium.com/@ryanpegoud?source=post_page-----c1e45a179b92--------------------------------)[](https://towardsdatascience.com/?source=post_page-----c1e45a179b92--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----c1e45a179b92--------------------------------)
    [Ryan PÃ©goud](https://medium.com/@ryanpegoud?source=post_page-----c1e45a179b92--------------------------------)'
- en: Â·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: Â·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F27fba63b402e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-gentle-introduction-to-deep-reinforcement-learning-in-jax-c1e45a179b92&user=Ryan+P%C3%A9goud&userId=27fba63b402e&source=post_page-27fba63b402e----c1e45a179b92---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----c1e45a179b92--------------------------------)
    Â·10 min readÂ·Nov 21, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fc1e45a179b92&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-gentle-introduction-to-deep-reinforcement-learning-in-jax-c1e45a179b92&user=Ryan+P%C3%A9goud&userId=27fba63b402e&source=-----c1e45a179b92---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[å…³æ³¨](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F27fba63b402e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-gentle-introduction-to-deep-reinforcement-learning-in-jax-c1e45a179b92&user=Ryan+P%C3%A9goud&userId=27fba63b402e&source=post_page-27fba63b402e----c1e45a179b92---------------------post_header-----------)
    å‘è¡¨åœ¨[Towards Data Science](https://towardsdatascience.com/?source=post_page-----c1e45a179b92--------------------------------)
    Â· 10åˆ†é’Ÿé˜…è¯»Â·2023å¹´11æœˆ21æ—¥[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fc1e45a179b92&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-gentle-introduction-to-deep-reinforcement-learning-in-jax-c1e45a179b92&user=Ryan+P%C3%A9goud&userId=27fba63b402e&source=-----c1e45a179b92---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc1e45a179b92&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-gentle-introduction-to-deep-reinforcement-learning-in-jax-c1e45a179b92&source=-----c1e45a179b92---------------------bookmark_footer-----------)![](../Images/af8f701ac536881e802dd7507bdc46f2.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc1e45a179b92&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-gentle-introduction-to-deep-reinforcement-learning-in-jax-c1e45a179b92&source=-----c1e45a179b92---------------------bookmark_footer-----------)![](../Images/af8f701ac536881e802dd7507bdc46f2.png)'
- en: Photo by [Thomas Despeyroux](https://unsplash.com/@thomasdes?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: ç”±[Thomas Despeyroux](https://unsplash.com/@thomasdes?utm_source=medium&utm_medium=referral)æ‹æ‘„ï¼Œå‘å¸ƒäº[Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: Recent progress in Reinforcement Learning (RL), such as Waymoâ€™s autonomous taxis
    or DeepMindâ€™s superhuman chess-playing agents, complement **classical RL** with
    **Deep Learning** components such as **Neural Networks** and **Gradient Optimization**
    methods.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€è¿‘åœ¨å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ–¹é¢çš„è¿›å±•ï¼Œä¾‹å¦‚ Waymo çš„è‡ªåŠ¨é©¾é©¶å‡ºç§Ÿè½¦æˆ– DeepMind çš„è¶…äººç±»æ£‹ç±»ä»£ç†ï¼Œç»“åˆäº†**ç»å…¸ RL**å’Œ**æ·±åº¦å­¦ä¹ **ç»„ä»¶ï¼Œå¦‚**ç¥ç»ç½‘ç»œ**å’Œ**æ¢¯åº¦ä¼˜åŒ–**æ–¹æ³•ã€‚
- en: Building on the foundations and coding principles introduced in one of my previous
    stories, weâ€™ll discover and learn to implement **Deep Q-Networks** (**DQN**) and
    **replay buffers** to solve OpenAIâ€™s **CartPole** environment. All of that **in
    under a second** using JAX!
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä¹‹å‰ä»‹ç»çš„åŸºç¡€å’Œç¼–ç åŸåˆ™ä¸Šæ„å»ºï¼Œæˆ‘ä»¬å°†æ¢ç´¢å¹¶å­¦ä¹ å¦‚ä½•ä½¿ç”¨JAXå®ç°**æ·±åº¦Qç½‘ç»œ**ï¼ˆ**DQN**ï¼‰å’Œ**å›æ”¾ç¼“å†²åŒº**æ¥è§£å†³OpenAIçš„**CartPole**ç¯å¢ƒã€‚æ‰€æœ‰è¿™äº›æ“ä½œéƒ½åœ¨**ä¸åˆ°ä¸€ç§’**çš„æ—¶é—´å†…å®Œæˆï¼
- en: 'For an introduction to **JAX**, **vectorized environments**, and **Q-learning**,
    please refer to the content of this story:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äº**JAX**ã€**å‘é‡åŒ–ç¯å¢ƒ**å’Œ**Q-learning**çš„ä»‹ç»ï¼Œè¯·å‚é˜…ä»¥ä¸‹å†…å®¹ï¼š
- en: '[](/vectorize-and-parallelize-rl-environments-with-jax-q-learning-at-the-speed-of-light-49d07373adf5?source=post_page-----c1e45a179b92--------------------------------)
    [## Vectorize and Parallelize RL Environments with JAX: Q-learning at the Speed
    of Lightâš¡'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/vectorize-and-parallelize-rl-environments-with-jax-q-learning-at-the-speed-of-light-49d07373adf5?source=post_page-----c1e45a179b92--------------------------------)
    [## ä½¿ç”¨JAXå‘é‡åŒ–å’Œå¹¶è¡ŒåŒ–RLç¯å¢ƒï¼šQ-learningçš„å…‰é€Ÿâš¡'
- en: Learn to vectorize a GridWorld environment and train 30 Q-learning agents in
    parallel on a CPU, at 1.8 million step perâ€¦
  id: totrans-13
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: å­¦ä¹ å¦‚ä½•åœ¨CPUä¸Šå‘é‡åŒ–GridWorldç¯å¢ƒï¼Œå¹¶åŒæ—¶è®­ç»ƒ30ä¸ªQ-learningä»£ç†ï¼Œæ¯ä¸ªä»£ç†è¿›è¡Œ180ä¸‡æ­¥â€¦
- en: towardsdatascience.com](/vectorize-and-parallelize-rl-environments-with-jax-q-learning-at-the-speed-of-light-49d07373adf5?source=post_page-----c1e45a179b92--------------------------------)
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/vectorize-and-parallelize-rl-environments-with-jax-q-learning-at-the-speed-of-light-49d07373adf5?source=post_page-----c1e45a179b92--------------------------------)
- en: 'Our framework of choice for deep learning will be DeepMindâ€™s **Haiku** library,
    which I recently introduced in the context of Transformers:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬é€‰æ‹©çš„æ·±åº¦å­¦ä¹ æ¡†æ¶æ˜¯DeepMindçš„**Haiku**åº“ï¼Œæˆ‘æœ€è¿‘åœ¨Transformerçš„ä¸Šä¸‹æ–‡ä¸­ä»‹ç»è¿‡ï¼š
- en: '[](/implementing-a-transformer-encoder-from-scratch-with-jax-and-haiku-791d31b4f0dd?source=post_page-----c1e45a179b92--------------------------------)
    [## Implementing a Transformer Encoder from Scratch with JAX and Haiku ğŸ¤–'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/implementing-a-transformer-encoder-from-scratch-with-jax-and-haiku-791d31b4f0dd?source=post_page-----c1e45a179b92--------------------------------)
    [## ä½¿ç”¨JAXå’ŒHaikuä»å¤´å¼€å§‹å®ç°Transformerç¼–ç å™¨ ğŸ¤–'
- en: Understanding the fundamental building blocks of Transformers.
  id: totrans-17
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ç†è§£Transformerçš„åŸºç¡€æ„å»ºæ¨¡å—ã€‚
- en: towardsdatascience.com](/implementing-a-transformer-encoder-from-scratch-with-jax-and-haiku-791d31b4f0dd?source=post_page-----c1e45a179b92--------------------------------)
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/implementing-a-transformer-encoder-from-scratch-with-jax-and-haiku-791d31b4f0dd?source=post_page-----c1e45a179b92--------------------------------)
- en: 'This article will cover the following sections:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬æ–‡å°†æ¶µç›–ä»¥ä¸‹å‡ ä¸ªéƒ¨åˆ†ï¼š
- en: '**Why** do we need Deep RL?'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ä¸ºä»€ä¹ˆ**æˆ‘ä»¬éœ€è¦æ·±åº¦RLï¼Ÿ'
- en: '**Deep Q-Networks,** *theory and practice*'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**æ·±åº¦Qç½‘ç»œ**çš„**ç†è®ºå’Œå®è·µ**'
- en: '**Replay Buffers**'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**å›æ”¾ç¼“å†²åŒº**'
- en: Translating the **CartPole** environment to **JAX**
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å°†**CartPole**ç¯å¢ƒè½¬æ¢ä¸º**JAX**
- en: The **JAX** way to write **efficient training loops**
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**JAX**ç¼–å†™**é«˜æ•ˆè®­ç»ƒå¾ªç¯**çš„æ–¹å¼'
- en: '*As always, all the code presented in this article is available on GitHub:*'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '*å¦‚å¾€å¸¸ä¸€æ ·ï¼Œæœ¬æ–‡ä¸­æä¾›çš„æ‰€æœ‰ä»£ç éƒ½å¯åœ¨GitHubä¸Šæ‰¾åˆ°ï¼š*'
- en: '[](https://github.com/RPegoud/jym?source=post_page-----c1e45a179b92--------------------------------)
    [## GitHub - RPegoud/jym: JAX implementation of RL algorithms and vectorized environments'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://github.com/RPegoud/jym?source=post_page-----c1e45a179b92--------------------------------)
    [## GitHub - RPegoud/jymï¼šJAXå®ç°çš„RLç®—æ³•å’Œå‘é‡åŒ–ç¯å¢ƒ'
- en: 'JAX implementation of RL algorithms and vectorized environments - GitHub -
    RPegoud/jym: JAX implementation of RLâ€¦'
  id: totrans-27
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 'JAXå®ç°çš„RLç®—æ³•å’Œå‘é‡åŒ–ç¯å¢ƒ - GitHub - RPegoud/jym: JAXå®ç°çš„RLâ€¦'
- en: github.com](https://github.com/RPegoud/jym?source=post_page-----c1e45a179b92--------------------------------)
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: github.com](https://github.com/RPegoud/jym?source=post_page-----c1e45a179b92--------------------------------)
- en: '**Why** do we need Deep RL?'
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**ä¸ºä»€ä¹ˆ**æˆ‘ä»¬éœ€è¦æ·±åº¦RLï¼Ÿ'
- en: In previous articles, we introduced [Temporal Difference Learning](https://medium.com/towards-data-science/temporal-difference-learning-and-the-importance-of-exploration-an-illustrated-guide-5f9c3371413a)
    algorithms and in particular [Q-learning](/vectorize-and-parallelize-rl-environments-with-jax-q-learning-at-the-speed-of-light-49d07373adf5).
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä¹‹å‰çš„æ–‡ç« ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†[æ—¶é—´å·®åˆ†å­¦ä¹ ](https://medium.com/towards-data-science/temporal-difference-learning-and-the-importance-of-exploration-an-illustrated-guide-5f9c3371413a)ç®—æ³•ï¼Œç‰¹åˆ«æ˜¯[Q-learning](/vectorize-and-parallelize-rl-environments-with-jax-q-learning-at-the-speed-of-light-49d07373adf5)ã€‚
- en: Simply put, Q-learning is an **off-policy** algorithm *(the target policy is
    not the policy used for decision-making)* maintaining and updating a **Q-table**,
    an explicit **mapping** of **states** to corresponding **action values**.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: ç®€å•æ¥è¯´ï¼ŒQ-learningæ˜¯ä¸€ç§**ç¦»ç­–ç•¥**ç®—æ³•ï¼ˆ*ç›®æ ‡ç­–ç•¥ä¸ç”¨äºå†³ç­–çš„ç­–ç•¥ä¸åŒ*ï¼‰ï¼Œç”¨äºç»´æŠ¤å’Œæ›´æ–°**Qè¡¨**ï¼Œä¸€ä¸ªæ˜ç¡®çš„**çŠ¶æ€**åˆ°ç›¸åº”**åŠ¨ä½œå€¼**çš„**æ˜ å°„**ã€‚
- en: While Q-learning is a practical solution for environments with discrete action
    spaces and restricted observation spaces, it struggles to scale well to more complex
    environments. Indeed, creating a Q-table requires **defining** the **action**
    and **observation spaces**.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: å°½ç®¡Qå­¦ä¹ æ˜¯ç¦»æ•£è¡ŒåŠ¨ç©ºé—´å’Œå—é™è§‚å¯Ÿç©ºé—´ç¯å¢ƒçš„å®é™…è§£å†³æ–¹æ¡ˆï¼Œä½†åœ¨æ›´å¤æ‚çš„ç¯å¢ƒä¸­å¾ˆéš¾æ‰©å±•ã€‚äº‹å®ä¸Šï¼Œåˆ›å»ºQè¡¨éœ€è¦å®šä¹‰**è¡ŒåŠ¨**å’Œ**è§‚å¯Ÿç©ºé—´**ã€‚
- en: Consider the example of **autonomous driving**, the **observation space** is
    composed of an *infinity of potential configurations* derived from camera feeds
    and other sensory inputs. On the other hand, the **action space** includes a *wide
    spectrum of steering wheel positions* and varying levels of force applied to the
    brake and accelerator.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: è€ƒè™‘**è‡ªåŠ¨é©¾é©¶**çš„ä¾‹å­ï¼Œ**è§‚å¯Ÿç©ºé—´**ç”±æ¥è‡ªæ‘„åƒå¤´å’Œå…¶ä»–æ„ŸçŸ¥è¾“å…¥çš„*æ— é™æ½œåœ¨é…ç½®*ç»„æˆã€‚å¦ä¸€æ–¹é¢ï¼Œ**è¡ŒåŠ¨ç©ºé—´**åŒ…æ‹¬*å¹¿æ³›çš„æ–¹å‘ç›˜ä½ç½®*ä»¥åŠæ–½åŠ åˆ°åˆ¹è½¦å’Œæ²¹é—¨çš„ä¸åŒåŠ›åº¦ã€‚
- en: Even though we could theoretically discretize the action space, the sheer volume
    of possible states and actions leads to an **impractical Q-table** in **real-world
    applications**.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: å°½ç®¡ç†è®ºä¸Šæˆ‘ä»¬å¯ä»¥ç¦»æ•£åŒ–è¡ŒåŠ¨ç©ºé—´ï¼Œä½†å®é™…åº”ç”¨ä¸­å¯èƒ½ä¼šå¯¼è‡´**ä¸åˆ‡å®é™…çš„Qè¡¨**ï¼Œå› ä¸ºå¯èƒ½çš„çŠ¶æ€å’Œè¡ŒåŠ¨æ•°é‡åºå¤§ã€‚
- en: '![](../Images/69029c0c7e9ad43e30b3e37045cbfb99.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/69029c0c7e9ad43e30b3e37045cbfb99.png)'
- en: Photo by [Kirill Tonkikh](https://unsplash.com/@photophotostock?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '[Kirill Tonkikh](https://unsplash.com/@photophotostock?utm_source=medium&utm_medium=referral)çš„ç…§ç‰‡æ¥è‡ª[Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)'
- en: Finding optimal actions in large and complex state-action spaces thus requires
    **powerful function approximation algorithms**, which is precisely what **Neural
    Networks** are. In the case of Deep Reinforcement Learning, neural nets are used
    as a **replacement for the Q-table** and provide an efficient solution to the
    *curse of dimensionality* introduced by large state spaces. Furthermore, we do
    not need to explicitly define the observation space.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å¤§å‹å¤æ‚çŠ¶æ€-åŠ¨ä½œç©ºé—´ä¸­å¯»æ‰¾æœ€ä¼˜è¡ŒåŠ¨å› æ­¤éœ€è¦**å¼ºå¤§çš„å‡½æ•°é€¼è¿‘ç®—æ³•**ï¼Œè¿™æ­£æ˜¯**ç¥ç»ç½‘ç»œ**æ‰€æ“…é•¿çš„ã€‚åœ¨æ·±åº¦å¼ºåŒ–å­¦ä¹ ä¸­ï¼Œç¥ç»ç½‘ç»œç”¨ä½œ**Qè¡¨çš„æ›¿ä»£å“**ï¼Œå¹¶ä¸ºå¤§çŠ¶æ€ç©ºé—´å¼•å…¥çš„*ç»´åº¦ç¾éš¾*æä¾›äº†é«˜æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬ä¸éœ€è¦æ˜¾å¼å®šä¹‰è§‚å¯Ÿç©ºé—´ã€‚
- en: Deep Q-Networks & Replay Buffers
  id: totrans-38
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ·±åº¦Qç½‘ç»œä¸é‡æ’­ç¼“å†²åŒº
- en: DQN uses two types of neural networks in parallel, starting with the â€œ***online***â€
    network which is used for **Q-value prediction** and **decision-making**. On the
    other hand, the â€œ***target***â€ network is used to **create stable Q-targets**
    to assess the performance of the online net via the loss function.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: DQNåŒæ—¶ä½¿ç”¨ä¸¤ç§ç±»å‹çš„ç¥ç»ç½‘ç»œï¼Œå¹¶è¡Œè¿›è¡Œï¼Œé¦–å…ˆæ˜¯ç”¨äº**Qå€¼é¢„æµ‹**å’Œ**å†³ç­–**çš„â€œ***åœ¨çº¿***â€ç½‘ç»œã€‚å¦ä¸€æ–¹é¢ï¼Œâ€œ***ç›®æ ‡***â€ç½‘ç»œç”¨äºé€šè¿‡æŸå¤±å‡½æ•°è¯„ä¼°åœ¨çº¿ç½‘ç»œçš„æ€§èƒ½ï¼Œä»¥ç”Ÿæˆ**ç¨³å®šçš„Qç›®æ ‡**ã€‚
- en: 'Similarly to Q-learning, DQN agents are defined by two functions: `act` and
    `update`.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸Qå­¦ä¹ ç±»ä¼¼ï¼ŒDQNä»£ç†ç”±ä¸¤ä¸ªå‡½æ•°å®šä¹‰ï¼š`act`å’Œ`update`ã€‚
- en: Act
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è¡ŒåŠ¨
- en: The `act` function implements an epsilon-greedy policy with respect to Q-values,
    which are estimated by the online neural network. In other words, the agent selects
    the action corresponding to the **maximum predicted Q-value** for a given state,
    with a set probability of acting randomly.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '`act`å‡½æ•°å®ç°äº†å…³äºQå€¼çš„Îµ-è´ªå¿ƒç­–ç•¥ï¼ŒQå€¼ç”±åœ¨çº¿ç¥ç»ç½‘ç»œä¼°è®¡ã€‚æ¢å¥è¯è¯´ï¼Œä»£ç†æ ¹æ®ç»™å®šçŠ¶æ€çš„**æœ€å¤§é¢„æµ‹Qå€¼**é€‰æ‹©åŠ¨ä½œï¼ŒåŒæ—¶ä»¥ä¸€å®šæ¦‚ç‡éšæœºæ‰§è¡ŒåŠ¨ä½œã€‚'
- en: You might remember that Q-learning updates its Q-table **after *every* step**,
    however, in Deep Learning it is common practice to compute updates using **gradient
    descent** on a **batch of inputs**.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: æ‚¨å¯èƒ½è¿˜è®°å¾—Qå­¦ä¹ åœ¨æ¯ä¸€æ­¥ä¹‹åæ›´æ–°å…¶Qè¡¨ï¼Œä½†åœ¨æ·±åº¦å­¦ä¹ ä¸­ï¼Œé€šå¸¸ä½¿ç”¨æ¢¯åº¦ä¸‹é™åœ¨**è¾“å…¥æ‰¹æ¬¡**ä¸Šè®¡ç®—æ›´æ–°ã€‚
- en: For this reason, DQN stores experiences (tuples containing `state, action, reward,
    next_state, done_flag`) in a **replay buffer**. To train the network, weâ€™ll sample
    a batch of experiences from this buffer instead of using only the last experience
    *(more details in the Replay Buffer section)*.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: å› æ­¤ï¼ŒDQNå°†ç»éªŒï¼ˆåŒ…å«`state, action, reward, next_state, done_flag`çš„å…ƒç»„ï¼‰å­˜å‚¨åœ¨**é‡æ’­ç¼“å†²åŒº**ä¸­ã€‚ä¸ºäº†è®­ç»ƒç½‘ç»œï¼Œæˆ‘ä»¬å°†ä»æ­¤ç¼“å†²åŒºä¸­éšæœºæŠ½å–ä¸€æ‰¹ç»éªŒï¼Œè€Œä¸ä»…ä»…ä½¿ç”¨æœ€åä¸€æ¬¡ç»éªŒï¼ˆæœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚è§é‡æ’­ç¼“å†²åŒºéƒ¨åˆ†ï¼‰ã€‚
- en: '![](../Images/7650171ae97fe0479b46e1490c1fdca0.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7650171ae97fe0479b46e1490c1fdca0.png)'
- en: Visual representation of **DQNâ€™s action selection** process (Made by the author)
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: å±•ç¤ºäº†**DQNè¡ŒåŠ¨é€‰æ‹©**è¿‡ç¨‹çš„è§†è§‰è¡¨ç¤ºï¼ˆä½œè€…åˆ¶ä½œï¼‰
- en: 'Hereâ€™s a JAX implementation of the action-selection part of DQN:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é‡Œæ˜¯DQNè¡ŒåŠ¨é€‰æ‹©éƒ¨åˆ†çš„JAXå®ç°ï¼š
- en: The only subtlety of this snippet is that the `model` attribute doesnâ€™t contain
    any internal parameters as is usually the case in frameworks such as PyTorch or
    TensorFlow.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªä»£ç ç‰‡æ®µçš„å”¯ä¸€ç»†å¾®ä¹‹å¤„åœ¨äº `model` å±æ€§ä¸åŒ…å«ä»»ä½•å†…éƒ¨å‚æ•°ï¼Œè¿™ä¸ PyTorch æˆ– TensorFlow ç­‰æ¡†æ¶ä¸­é€šå¸¸çš„æƒ…å†µä¸åŒã€‚
- en: Here, the model is a **function** representing a **forward pass** through our
    architecture, but the ***mutable* weights are stored externally** and passed as
    **arguments**. This explains why we can use `jit` while passing the `self` argument
    as **static *(****the model being stateless as other class attributes)*.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™é‡Œï¼Œæ¨¡å‹æ˜¯ä¸€ä¸ª **å‡½æ•°**ï¼Œè¡¨ç¤ºæˆ‘ä»¬æ¶æ„ä¸­çš„ **å‰å‘ä¼ é€’**ï¼Œä½† ***å¯å˜çš„* æƒé‡æ˜¯å¤–éƒ¨å­˜å‚¨** å¹¶ä½œä¸º **å‚æ•°** ä¼ é€’ã€‚è¿™è§£é‡Šäº†ä¸ºä»€ä¹ˆæˆ‘ä»¬å¯ä»¥åœ¨ä¼ é€’
    `self` å‚æ•°æ—¶ä½¿ç”¨ `jit`ï¼Œä½œä¸º **é™æ€*ï¼ˆ**æ¨¡å‹åœ¨å…¶ä»–ç±»å±æ€§ä¸­æ˜¯æ— çŠ¶æ€çš„ï¼‰*ã€‚
- en: Update
  id: totrans-50
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ›´æ–°
- en: 'The `update` function is responsible for training the network. It computes
    a **mean squared error** (MSE) loss based on the **temporal-difference** (TD)
    **error**:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '`update` å‡½æ•°è´Ÿè´£è®­ç»ƒç½‘ç»œã€‚å®ƒæ ¹æ® **æ—¶é—´å·®**ï¼ˆTDï¼‰ **è¯¯å·®** è®¡ç®— **å‡æ–¹è¯¯å·®**ï¼ˆMSEï¼‰æŸå¤±ï¼š'
- en: '![](../Images/449285430b8e3bdfe71e785562543881.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/449285430b8e3bdfe71e785562543881.png)'
- en: Mean Squared Error used in DQN
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: DQNä¸­ä½¿ç”¨çš„å‡æ–¹è¯¯å·®
- en: In this loss function, ***Î¸*** denotes the **parameters of the online network**,
    and ***Î¸*âˆ’** represents the **parameters of the target network**. The parameters
    of the target network are set on the online networkâ€™s parameters every *N* steps*,*
    similar to a *checkpoint* (*N is a hyperparameter).*
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ä¸ªæŸå¤±å‡½æ•°ä¸­ï¼Œ***Î¸*** è¡¨ç¤º **åœ¨çº¿ç½‘ç»œçš„å‚æ•°**ï¼Œè€Œ ***Î¸*âˆ’** ä»£è¡¨ **ç›®æ ‡ç½‘ç»œçš„å‚æ•°**ã€‚ç›®æ ‡ç½‘ç»œçš„å‚æ•°æ¯éš” *N* æ­¥è¢«è®¾ç½®ä¸ºåœ¨çº¿ç½‘ç»œçš„å‚æ•°ï¼Œç±»ä¼¼äºä¸€ä¸ª
    *æ£€æŸ¥ç‚¹* ï¼ˆ*N æ˜¯ä¸€ä¸ªè¶…å‚æ•°*ï¼‰ã€‚
- en: This separation of parameters (with *Î¸* for the current Q-values and *Î¸*âˆ’ for
    the target Q-values) is crucial to stabilize training.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°çš„åˆ†ç¦»ï¼ˆå½“å‰Qå€¼çš„ *Î¸* å’Œç›®æ ‡Qå€¼çš„ *Î¸*âˆ’ï¼‰å¯¹äºç¨³å®šè®­ç»ƒè‡³å…³é‡è¦ã€‚
- en: Using the same parameters for both would be similar to aiming at a moving target,
    as **updates to the network** would i**mmediately shift the target values**. By
    **periodically updating** ***Î¸*âˆ’** (i.e. freezing these parameters for a set number
    of steps), we ensure **stable Q-targets** while the online network continues to
    learn.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœå¯¹ä¸¤ä¸ªç½‘ç»œä½¿ç”¨ç›¸åŒçš„å‚æ•°ï¼Œå°±ç±»ä¼¼äºç„å‡†ä¸€ä¸ªç§»åŠ¨çš„ç›®æ ‡ï¼Œå› ä¸º **å¯¹ç½‘ç»œçš„æ›´æ–°** ä¼š **ç«‹å³æ”¹å˜ç›®æ ‡å€¼**ã€‚é€šè¿‡ **å®šæœŸæ›´æ–°** ***Î¸*âˆ’**ï¼ˆå³åœ¨è®¾å®šæ­¥æ•°å†…å†»ç»“è¿™äº›å‚æ•°ï¼‰ï¼Œæˆ‘ä»¬ç¡®ä¿äº†
    **ç¨³å®šçš„Qç›®æ ‡**ï¼ŒåŒæ—¶åœ¨çº¿ç½‘ç»œç»§ç»­å­¦ä¹ ã€‚
- en: Finally, the *(1-done)* term **adjusts the target** for **terminal states**.
    Indeed, when an episode ends (i.e. â€˜doneâ€™ is equal to 1), there is no next state.
    Therefore, the Q-value for the next state is set to 0.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åï¼Œ*(1-done)* é¡¹ **è°ƒæ•´ç›®æ ‡** ç”¨äº **ç»ˆæ­¢çŠ¶æ€**ã€‚å®é™…ä¸Šï¼Œå½“ä¸€ä¸ªå›åˆç»“æŸæ—¶ï¼ˆå³ â€˜doneâ€™ ç­‰äº 1ï¼‰ï¼Œæ²¡æœ‰ä¸‹ä¸€ä¸ªçŠ¶æ€ã€‚å› æ­¤ï¼Œä¸‹ä¸€çŠ¶æ€çš„Qå€¼è¢«è®¾ä¸º0ã€‚
- en: '![](../Images/6cc23c97e8034ed5aa9e046e8ca97139.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6cc23c97e8034ed5aa9e046e8ca97139.png)'
- en: Visual representation of **DQNâ€™s parameter update** process (Made by the author)
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '**DQNå‚æ•°æ›´æ–°** è¿‡ç¨‹çš„å¯è§†åŒ–è¡¨ç¤ºï¼ˆä½œè€…åˆ¶ä½œï¼‰'
- en: 'Implementing the update function for DQN is slightly more complex, letâ€™s break
    it down:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: å®ç°DQNçš„æ›´æ–°å‡½æ•°ç¨å¾®å¤æ‚ä¸€äº›ï¼Œæˆ‘ä»¬åˆ†è§£ä¸€ä¸‹ï¼š
- en: First, the `_loss_fn` function implements the squared error described previously
    for a **single experience**.
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: é¦–å…ˆï¼Œ`_loss_fn` å‡½æ•°å®ç°äº†ä¹‹å‰æè¿°çš„ç”¨äº **å•ä¸€ç»éªŒ** çš„å¹³æ–¹è¯¯å·®ã€‚
- en: Then, `_batch_loss_fn` acts as a wrapper for `_loss_fn` and decorates it with
    `vmap`, applying the loss function to a **batch of experiences**. We then return
    the average error for this batch.
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç„¶åï¼Œ`_batch_loss_fn` ä½œä¸º `_loss_fn` çš„åŒ…è£…å™¨ï¼Œå¹¶é€šè¿‡ `vmap` è£…é¥°å®ƒï¼Œå°†æŸå¤±å‡½æ•°åº”ç”¨äº **ä¸€æ‰¹ç»éªŒ**ã€‚ç„¶åæˆ‘ä»¬è¿”å›è¿™ä¸€æ‰¹çš„å¹³å‡è¯¯å·®ã€‚
- en: Finally, `update` acts as a final layer to our loss function, computing its
    **gradient** with respect to the online network parameters, the target network
    parameters, and a batch of experiences. We then use **Optax** *(a JAX library
    commonly used for optimization)* to perform an optimizer step and update the online
    parameters.
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æœ€åï¼Œ`update` ä½œä¸ºæˆ‘ä»¬æŸå¤±å‡½æ•°çš„æœ€ç»ˆå±‚ï¼Œè®¡ç®—å…¶ **æ¢¯åº¦** ç›¸å¯¹äºåœ¨çº¿ç½‘ç»œå‚æ•°ã€ç›®æ ‡ç½‘ç»œå‚æ•°ä»¥åŠä¸€æ‰¹ç»éªŒã€‚ç„¶åæˆ‘ä»¬ä½¿ç”¨ **Optax**
    *ï¼ˆä¸€ä¸ªå¸¸ç”¨äºä¼˜åŒ–çš„JAXåº“ï¼‰* æ‰§è¡Œä¼˜åŒ–æ­¥éª¤å¹¶æ›´æ–°åœ¨çº¿å‚æ•°ã€‚
- en: 'Notice that, similarly to the replay buffer, the model and optimizer are **pure
    functions** modifying an **external state**. The following line serves as a good
    illustration of this principle:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: æ³¨æ„ï¼Œä¸å›æ”¾ç¼“å†²åŒºç±»ä¼¼ï¼Œæ¨¡å‹å’Œä¼˜åŒ–å™¨æ˜¯ **çº¯å‡½æ•°**ï¼Œä¿®æ”¹ **å¤–éƒ¨çŠ¶æ€**ã€‚ä»¥ä¸‹ä¸€è¡Œå¾ˆå¥½åœ°è¯´æ˜äº†è¿™ä¸€åŸåˆ™ï¼š
- en: '[PRE0]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: This also explains why we can use a single model for both the online and target
    networks, as the parameters are stored and updated externally.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¹Ÿè§£é‡Šäº†ä¸ºä»€ä¹ˆæˆ‘ä»¬å¯ä»¥å¯¹åœ¨çº¿ç½‘ç»œå’Œç›®æ ‡ç½‘ç»œä½¿ç”¨ä¸€ä¸ªæ¨¡å‹ï¼Œå› ä¸ºå‚æ•°æ˜¯å¤–éƒ¨å­˜å‚¨å’Œæ›´æ–°çš„ã€‚
- en: '[PRE1]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'For context, the model we use in this article is a *multi-layer perceptron*
    defined as follows:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†æä¾›èƒŒæ™¯ï¼Œæˆ‘ä»¬åœ¨æœ¬æ–‡ä¸­ä½¿ç”¨çš„æ¨¡å‹æ˜¯ä¸€ä¸ª*å¤šå±‚æ„ŸçŸ¥æœº*ï¼Œå®šä¹‰å¦‚ä¸‹ï¼š
- en: '[PRE2]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Replay Buffer
  id: totrans-70
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: é‡æ”¾ç¼“å†²åŒº
- en: 'Now let us take a step back and look closer at replay buffers. They are widely
    used in reinforcement learning for a variety of reasons:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ï¼Œè®©æˆ‘ä»¬é€€ä¸€æ­¥ï¼Œæ›´è¯¦ç»†åœ°çœ‹ä¸€ä¸‹é‡æ”¾ç¼“å†²åŒºã€‚å®ƒä»¬åœ¨å¼ºåŒ–å­¦ä¹ ä¸­è¢«å¹¿æ³›ä½¿ç”¨ï¼ŒåŸå› æœ‰å¾ˆå¤šï¼š
- en: '**Generalization**: By sampling from the replay buffer, we break the correlation
    between consecutive experiences by mixing up their order. This way, we avoid overfitting
    to specific sequences of experiences.'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**æ³›åŒ–ï¼š** é€šè¿‡ä»é‡æ”¾ç¼“å†²åŒºé‡‡æ ·ï¼Œæˆ‘ä»¬æ‰“ç ´äº†è¿ç»­ç»éªŒä¹‹é—´çš„ç›¸å…³æ€§ï¼Œé€šè¿‡æ··åˆå®ƒä»¬çš„é¡ºåºã€‚è¿™ç§æ–¹å¼é¿å…äº†å¯¹ç‰¹å®šç»éªŒåºåˆ—çš„è¿‡æ‹Ÿåˆã€‚'
- en: '**Diversity**: As the sampling is not limited to recent experiences, we generally
    observe a lower variance in updates and prevent overfitting to the latest experiences.'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**å¤šæ ·æ€§ï¼š** ç”±äºé‡‡æ ·ä¸é™äºæœ€è¿‘çš„ç»éªŒï¼Œæˆ‘ä»¬é€šå¸¸ä¼šè§‚å¯Ÿåˆ°æ›´æ–°çš„æ–¹å·®è¾ƒä½ï¼Œå¹¶ä¸”é¿å…å¯¹æœ€æ–°ç»éªŒçš„è¿‡æ‹Ÿåˆã€‚'
- en: '**Increased sample efficiency**: Each experience can be sampled multiple times
    from the buffer, enabling the model to learn more from individual experiences.'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**å¢åŠ æ ·æœ¬æ•ˆç‡ï¼š** æ¯ä¸ªç»éªŒå¯ä»¥ä»ç¼“å†²åŒºä¸­è¢«å¤šæ¬¡é‡‡æ ·ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿä»ä¸ªä½“ç»éªŒä¸­å­¦ä¹ æ›´å¤šã€‚'
- en: 'Finally, we can use several sampling schemes for our replay buffer:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨å‡ ç§é‡‡æ ·æ–¹æ¡ˆæ¥ç®¡ç†æˆ‘ä»¬çš„é‡æ”¾ç¼“å†²åŒºï¼š
- en: '**Uniform sampling:** Experiences are sampled uniformly at random. This type
    of sampling is straightforward to implement and allows the model to learn from
    experiences independently from the timestep they were collected.'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**å‡åŒ€é‡‡æ ·ï¼š** ç»éªŒä»¥å‡åŒ€çš„éšæœºæ–¹å¼è¿›è¡Œé‡‡æ ·ã€‚è¿™ç§é‡‡æ ·ç±»å‹æ˜“äºå®ç°ï¼Œå¹¶å…è®¸æ¨¡å‹ä»ç»éªŒä¸­ç‹¬ç«‹äºå®ƒä»¬è¢«æ”¶é›†çš„æ—¶é—´æ­¥é•¿ä¸­å­¦ä¹ ã€‚'
- en: '**Prioritized sampling:** This category includes different algorithms such
    as **Prioritized Experience Replay** (â€œPERâ€, [*Schaul et al. 2015*](https://arxiv.org/abs/1511.05952)*)*
    or **Gradient Experience Replay** (â€œGERâ€, [*Lahire et al., 2022*](https://arxiv.org/abs/2110.01528)*).*
    These methods attempt to prioritize the selection of experiences according to
    some metric related to their â€œ*learning potentialâ€* (the amplitude of the TD error
    for PER and the norm of the experienceâ€™s gradient for GER).'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ä¼˜å…ˆé‡‡æ ·ï¼š** è¿™ä¸ªç±»åˆ«åŒ…æ‹¬ä¸åŒçš„ç®—æ³•ï¼Œå¦‚**ä¼˜å…ˆç»éªŒé‡æ”¾**ï¼ˆâ€œPERâ€ï¼Œ[*Schaulç­‰ï¼Œ2015*](https://arxiv.org/abs/1511.05952)ï¼‰æˆ–**æ¢¯åº¦ç»éªŒé‡æ”¾**ï¼ˆâ€œGERâ€ï¼Œ[*Lahireç­‰ï¼Œ2022*](https://arxiv.org/abs/2110.01528)ï¼‰ã€‚è¿™äº›æ–¹æ³•è¯•å›¾æ ¹æ®ä¸å…¶â€œ*å­¦ä¹ æ½œåŠ›*â€ï¼ˆPERçš„TDè¯¯å·®å¹…åº¦å’ŒGERçš„ç»éªŒæ¢¯åº¦çš„èŒƒæ•°ï¼‰ç›¸å…³çš„æŸäº›æŒ‡æ ‡ä¼˜å…ˆé€‰æ‹©ç»éªŒã€‚'
- en: For the sake of simplicity, weâ€™ll implement a uniform replay buffer in this
    article. However, I plan to cover prioritized sampling extensively in the future.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†ç®€å•èµ·è§ï¼Œæˆ‘ä»¬å°†åœ¨æœ¬æ–‡ä¸­å®ç°ä¸€ä¸ªå‡åŒ€é‡æ”¾ç¼“å†²åŒºã€‚ç„¶è€Œï¼Œæˆ‘è®¡åˆ’åœ¨æœªæ¥è¯¦ç»†è®¨è®ºä¼˜å…ˆé‡‡æ ·ã€‚
- en: As promised, the uniform replay buffer is quite easy to implement, however,
    there are a few complexities related to the use of JAX and functional programming.
    As always, we have to work with **pure functions** that are **devoid of side effects**.
    In other words, we are not allowed to define the buffer as a class instance with
    a variable internal state.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: æ­£å¦‚æ‰¿è¯ºçš„é‚£æ ·ï¼Œå‡åŒ€é‡æ”¾ç¼“å†²åŒºçš„å®ç°éå¸¸ç®€å•ï¼Œä½†ä¸JAXå’Œå‡½æ•°å¼ç¼–ç¨‹çš„ä½¿ç”¨ç›¸å…³çš„ä¸€äº›å¤æ‚æ€§éœ€è¦è§£å†³ã€‚ä¸å¾€å¸¸ä¸€æ ·ï¼Œæˆ‘ä»¬å¿…é¡»ä½¿ç”¨**çº¯å‡½æ•°**ï¼Œè¿™äº›å‡½æ•°**æ²¡æœ‰å‰¯ä½œç”¨**ã€‚æ¢å¥è¯è¯´ï¼Œæˆ‘ä»¬ä¸èƒ½å°†ç¼“å†²åŒºå®šä¹‰ä¸ºå…·æœ‰å˜é‡å†…éƒ¨çŠ¶æ€çš„ç±»å®ä¾‹ã€‚
- en: Instead, we initialize a `buffer_state` dictionary that maps keys to empty arrays
    with predefined shapes, as JAX requires constant-sized arrays when jit-compiling
    code to XLA.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: ç›¸åï¼Œæˆ‘ä»¬åˆå§‹åŒ–ä¸€ä¸ª`buffer_state`å­—å…¸ï¼Œè¯¥å­—å…¸å°†é”®æ˜ å°„åˆ°å…·æœ‰é¢„å®šä¹‰å½¢çŠ¶çš„ç©ºæ•°ç»„ï¼Œå› ä¸ºJAXåœ¨å¯¹XLAè¿›è¡ŒJITç¼–è¯‘æ—¶è¦æ±‚å›ºå®šå¤§å°çš„æ•°ç»„ã€‚
- en: '[PRE3]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'We will use a `UniformReplayBuffer` class to interact with the buffer state.
    This class has two methods:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†ä½¿ç”¨`UniformReplayBuffer`ç±»ä¸ç¼“å†²åŒºçŠ¶æ€è¿›è¡Œäº¤äº’ã€‚è¿™ä¸ªç±»æœ‰ä¸¤ä¸ªæ–¹æ³•ï¼š
- en: '`add`: Unwraps an experience tuple and maps its components to a specific index.
    `idx = idx % self.buffer_size` ensures that when the buffer is full, adding new
    experiences overwrites older ones.'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`add`ï¼šè§£å¼€ä¸€ä¸ªç»éªŒå…ƒç»„ï¼Œå¹¶å°†å…¶ç»„ä»¶æ˜ å°„åˆ°ç‰¹å®šç´¢å¼•ã€‚`idx = idx % self.buffer_size`ç¡®ä¿å½“ç¼“å†²åŒºæ»¡æ—¶ï¼Œæ·»åŠ çš„æ–°ç»éªŒä¼šè¦†ç›–æ—§çš„ç»éªŒã€‚'
- en: '`sample`: Samples a sequence of random indexes from the uniform random distribution.
    The sequence length is set by `batch_size` while the range of the indexes is `[0,
    current_buffer_size-1]`. This ensures that we do not sample empty arrays while
    the buffer is not yet full. Finally, we use JAXâ€™s `vmap` in combination with `tree_map`
    to return a batch of experiences.'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sample`ï¼šä»å‡åŒ€éšæœºåˆ†å¸ƒä¸­é‡‡æ ·ä¸€ç³»åˆ—éšæœºç´¢å¼•ã€‚åºåˆ—é•¿åº¦ç”±`batch_size`è®¾å®šï¼Œè€Œç´¢å¼•çš„èŒƒå›´ä¸º`[0, current_buffer_size-1]`ã€‚è¿™ç¡®ä¿äº†åœ¨ç¼“å†²åŒºå°šæœªæ»¡æ—¶ä¸ä¼šé‡‡æ ·åˆ°ç©ºæ•°ç»„ã€‚æœ€åï¼Œæˆ‘ä»¬ä½¿ç”¨JAXçš„`vmap`ç»“åˆ`tree_map`è¿”å›ä¸€æ‰¹ç»éªŒã€‚'
- en: Translating the **CartPole** environment to **JAX**
  id: totrans-85
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å°†**CartPole**ç¯å¢ƒè½¬æ¢ä¸º**JAX**
- en: Now that our DQN agent is ready for training, weâ€™ll quickly implement a vectorized
    CartPole environment using the same framework as introduced in an [earlier article](/vectorize-and-parallelize-rl-environments-with-jax-q-learning-at-the-speed-of-light-49d07373adf5).
    CartPole is a control environment having a **large continuous observation space,**
    which makes it relevant to test our DQN.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬çš„DQNä»£ç†å·²å‡†å¤‡å¥½è¿›è¡Œè®­ç»ƒï¼Œæˆ‘ä»¬å°†å¿«é€Ÿå®ç°ä¸€ä¸ªä½¿ç”¨ä¸[æ—©æœŸæ–‡ç« ](/vectorize-and-parallelize-rl-environments-with-jax-q-learning-at-the-speed-of-light-49d07373adf5)ä»‹ç»çš„ç›¸åŒæ¡†æ¶çš„çŸ¢é‡åŒ–CartPoleç¯å¢ƒã€‚
    CartPoleæ˜¯ä¸€ä¸ªå…·æœ‰**å¤§å‹è¿ç»­è§‚å¯Ÿç©ºé—´**çš„æ§åˆ¶ç¯å¢ƒï¼Œè¿™ä½¿å¾—æµ‹è¯•æˆ‘ä»¬çš„DQNå˜å¾—ç›¸å…³ã€‚
- en: '![](../Images/6c7f16ff97e5c92150e3a4ece0e1f843.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6c7f16ff97e5c92150e3a4ece0e1f843.png)'
- en: 'Visual representation of the CartPole Environment (credits and documentation:
    [OpenAI Gymnasium](https://gymnasium.farama.org/environments/classic_control/cart_pole/),
    MIT license)'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: CartPoleç¯å¢ƒçš„å¯è§†åŒ–è¡¨ç¤ºï¼ˆé¸£è°¢å’Œæ–‡æ¡£ï¼š[OpenAI Gymnasium](https://gymnasium.farama.org/environments/classic_control/cart_pole/)ï¼ŒMITè®¸å¯è¯ï¼‰
- en: 'The process is quite straightforward, we reuse most of [OpenAIâ€™s Gymnasium
    implementation](https://github.com/Farama-Foundation/Gymnasium/blob/main/gymnasium/envs/classic_control/cartpole.py)
    while making sure we use JAX arrays and lax control flow instead of Python or
    Numpy alternatives, for instance:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªè¿‡ç¨‹éå¸¸ç®€å•ï¼Œæˆ‘ä»¬å¤§éƒ¨åˆ†éƒ½é‡ç”¨äº†[OpenAIçš„Gymnasiumå®ç°](https://github.com/Farama-Foundation/Gymnasium/blob/main/gymnasium/envs/classic_control/cartpole.py)ï¼ŒåŒæ—¶ç¡®ä¿æˆ‘ä»¬ä½¿ç”¨JAXæ•°ç»„å’Œlaxæ§åˆ¶æµï¼Œè€Œä¸æ˜¯Pythonæˆ–Numpyçš„æ›¿ä»£æ–¹æ¡ˆï¼Œä¾‹å¦‚ï¼š
- en: '[PRE4]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'For the sake of brevity, the full environment code is available here:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†ç®€æ´èµ·è§ï¼Œå®Œæ•´çš„ç¯å¢ƒä»£ç åœ¨æ­¤å¤„å¯ç”¨ï¼š
- en: '[](https://github.com/RPegoud/jym/blob/main/src/envs/control/cartpole.py?source=post_page-----c1e45a179b92--------------------------------)
    [## jym/src/envs/control/cartpole.py at main Â· RPegoud/jym'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://github.com/RPegoud/jym/blob/main/src/envs/control/cartpole.py?source=post_page-----c1e45a179b92--------------------------------)
    [## jym/src/envs/control/cartpole.py at main Â· RPegoud/jym'
- en: JAX implementation of RL algorithms and vectorized environments - jym/src/envs/control/cartpole.py
    at main Â·â€¦
  id: totrans-93
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: JAXå®ç°çš„RLç®—æ³•å’ŒçŸ¢é‡åŒ–ç¯å¢ƒ - jym/src/envs/control/cartpole.py at main Â·â€¦
- en: github.com](https://github.com/RPegoud/jym/blob/main/src/envs/control/cartpole.py?source=post_page-----c1e45a179b92--------------------------------)
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '[github.com](https://github.com/RPegoud/jym/blob/main/src/envs/control/cartpole.py?source=post_page-----c1e45a179b92--------------------------------)'
- en: The **JAX** way to write **efficient training loops**
  id: totrans-95
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ä»¥**JAX**çš„æ–¹å¼ç¼–å†™**é«˜æ•ˆçš„è®­ç»ƒå¾ªç¯**
- en: The last part of our implementation of DQN is the training loop *(also called
    rollout).* As mentioned in previous articles, we have to respect a specific format
    in order to take advantage of JAXâ€™s speed.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: DQNå®ç°çš„æœ€åéƒ¨åˆ†æ˜¯è®­ç»ƒå¾ªç¯ï¼ˆä¹Ÿç§°ä¸ºæ¨æ¼”ï¼‰ã€‚ æ­£å¦‚å‰æ–‡æ‰€è¿°ï¼Œä¸ºäº†åˆ©ç”¨JAXçš„é€Ÿåº¦ï¼Œæˆ‘ä»¬å¿…é¡»éµå®ˆç‰¹å®šçš„æ ¼å¼ã€‚
- en: 'The rollout function might appear daunting at first, but most of its complexity
    is purely syntactic as weâ€™ve already covered most of the building blocks. Hereâ€™s
    a pseudo-code walkthrough:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: æ¨æ¼”å‡½æ•°å¯èƒ½ä¸€å¼€å§‹çœ‹èµ·æ¥ä»¤äººç”Ÿç•ï¼Œä½†å®ƒå¤§éƒ¨åˆ†çš„å¤æ‚æ€§çº¯ç²¹æ˜¯è¯­æ³•ä¸Šçš„ï¼Œå› ä¸ºæˆ‘ä»¬å·²ç»æ¶µç›–äº†å¤§å¤šæ•°æ„å»ºå—ã€‚ è¿™æ˜¯ä¸€ä¸ªä¼ªä»£ç æ¼”ç¤ºï¼š
- en: '[PRE5]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: We can now run DQN for **20,000 steps** and observe the performances. After
    around 45 episodes, the agent manages to obtain decent performances, balancing
    the pole for more than 100 steps consistently.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬å¯ä»¥è¿è¡ŒDQNè¿›è¡Œ**20,000æ­¥**å¹¶è§‚å¯Ÿå…¶è¡¨ç°ã€‚ å¤§çº¦åœ¨45é›†åï¼Œä»£ç†æˆåŠŸåœ°ä¿æŒäº†è¶…è¿‡100æ­¥çš„ç¨³å®šå¹³è¡¡ã€‚
- en: The **green bars** indicate that the agent managed to balance the pole for **more
    than 200 steps**, **solving the environment**. Notably, the agent set its record
    on the **51st episode**, with **393 steps**.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '**ç»¿è‰²æ¡**è¡¨ç¤ºä»£ç†æˆåŠŸåœ°åœ¨**200å¤šæ­¥**å†…å¹³è¡¡äº†æ†ï¼Œ**è§£å†³äº†ç¯å¢ƒ**ã€‚ å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œä»£ç†åœ¨**ç¬¬51é›†**ä¸Šåˆ›ä¸‹äº†**393æ­¥**çš„è®°å½•ã€‚'
- en: Performance report for DQN (made by the author)
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: DQNçš„æ€§èƒ½æŠ¥å‘Šï¼ˆä½œè€…åˆ¶ä½œï¼‰
- en: The **20.000 training steps** were executed in **just over a second**, at a
    rate of **15.807 steps per second** *(on a* ***single CPU****)*!
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '**20,000è®­ç»ƒæ­¥éª¤**åœ¨**ä¸€ç§’å¤šä¸€ç‚¹**å†…æ‰§è¡Œå®Œæ¯•ï¼Œé€Ÿåº¦ä¸º**æ¯ç§’15,807æ­¥**ï¼ˆ*åœ¨å•ä¸ªCPUä¸Š*ï¼‰ï¼'
- en: These performances hint at JAXâ€™s impressive scaling capabilities, allowing practitioners
    to run large-scale parallelized experiments with minimal hardware requirements.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™äº›è¡¨ç°æç¤ºäº†JAXä»¤äººå°è±¡æ·±åˆ»çš„æ‰©å±•èƒ½åŠ›ï¼Œå…è®¸ä»ä¸šè€…ä½¿ç”¨æœ€å°çš„ç¡¬ä»¶è¦æ±‚è¿›è¡Œå¤§è§„æ¨¡å¹¶è¡Œå®éªŒã€‚
- en: '[PRE6]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Weâ€™ll take a closer look at **parallelized rollout procedures** to run **statistically
    significant** experiments and **hyperparameter searches** in a future article!
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†æ›´è¯¦ç»†åœ°çœ‹çœ‹**å¹¶è¡ŒåŒ–çš„æ¨æ¼”è¿‡ç¨‹**ï¼Œä»¥è¿è¡Œ**å…·æœ‰ç»Ÿè®¡æ˜¾è‘—æ€§**çš„å®éªŒå’Œ**è¶…å‚æ•°æœç´¢**åœ¨æœªæ¥çš„æ–‡ç« ä¸­ï¼
- en: 'In the meantime, feel free to reproduce the experiment and dabble with hyperparameters
    using this notebook:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸æ­¤åŒæ—¶ï¼Œéšæ—¶å¯ä»¥ä½¿ç”¨è¿™æœ¬ç¬”è®°æœ¬é‡ç°å®éªŒå¹¶å°è¯•ä¸åŒçš„è¶…å‚æ•°ï¼š
- en: '[](https://github.com/RPegoud/jym/blob/main/notebooks/control/cartpole/dqn_cartpole.ipynb?source=post_page-----c1e45a179b92--------------------------------)
    [## jym/notebooks/control/cartpole/dqn_cartpole.ipynb at main Â· RPegoud/jym'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://github.com/RPegoud/jym/blob/main/notebooks/control/cartpole/dqn_cartpole.ipynb?source=post_page-----c1e45a179b92--------------------------------)
    [## jym/notebooks/control/cartpole/dqn_cartpole.ipynb at main Â· RPegoud/jym'
- en: JAX implementation of RL algorithms and vectorized environments - jym/notebooks/control/cartpole/dqn_cartpole.ipynb
    atâ€¦
  id: totrans-108
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: JAX å¼ºåŒ–å­¦ä¹ ç®—æ³•å’Œå‘é‡åŒ–ç¯å¢ƒçš„å®ç° - jym/notebooks/control/cartpole/dqn_cartpole.ipynb atâ€¦
- en: github.com](https://github.com/RPegoud/jym/blob/main/notebooks/control/cartpole/dqn_cartpole.ipynb?source=post_page-----c1e45a179b92--------------------------------)
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: github.com](https://github.com/RPegoud/jym/blob/main/notebooks/control/cartpole/dqn_cartpole.ipynb?source=post_page-----c1e45a179b92--------------------------------)
- en: Conclusion
  id: totrans-110
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç»“è®º
- en: As always, **thanks for reading this far!** I hope this article provided a decent
    introduction to Deep RL in JAX. Should you have any questions or feedback related
    to the content of this article, make sure to let me know, Iâ€™m always happy to
    have a little chat ;)
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚å¾€å¸¸ä¸€æ ·ï¼Œ**æ„Ÿè°¢æ‚¨è¯»åˆ°è¿™é‡Œï¼**å¸Œæœ›æœ¬æ–‡ä¸ºæ‚¨åœ¨ JAX ä¸­çš„æ·±åº¦å¼ºåŒ–å­¦ä¹ æä¾›äº†ä¸€ä¸ªä¸é”™çš„ä»‹ç»ã€‚å¦‚æœæ‚¨å¯¹æœ¬æ–‡å†…å®¹æœ‰ä»»ä½•é—®é¢˜æˆ–åé¦ˆï¼Œè¯·åŠ¡å¿…å‘Šè¯‰æˆ‘ï¼Œæˆ‘æ€»æ˜¯ä¹æ„èŠä¸€èŠ
    ;)
- en: Until next time ğŸ‘‹
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: ç›´åˆ°ä¸‹æ¬¡è§é¢ ğŸ‘‹
- en: 'Credits:'
  id: totrans-113
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: è‡´è°¢ï¼š
- en: '[Cartpole Gif](https://gymnasium.farama.org/environments/classic_control/cart_pole/),
    OpenAI Gymnasium library, (MIT license)'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Cartpole Gif](https://gymnasium.farama.org/environments/classic_control/cart_pole/)ï¼ŒOpenAI
    Gymnasium åº“ï¼Œï¼ˆMIT è®¸å¯è¯ï¼‰'
