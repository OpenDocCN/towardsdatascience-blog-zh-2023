- en: 'Falcon 180B: Can It Run on Your Computer?'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Falcon 180B：它能在你的计算机上运行吗？
- en: 原文：[https://towardsdatascience.com/falcon-180b-can-it-run-on-your-computer-c3f3fb1611a9](https://towardsdatascience.com/falcon-180b-can-it-run-on-your-computer-c3f3fb1611a9)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/falcon-180b-can-it-run-on-your-computer-c3f3fb1611a9](https://towardsdatascience.com/falcon-180b-can-it-run-on-your-computer-c3f3fb1611a9)
- en: Yes, if you have enough CPU RAM
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 是的，如果你有足够的 CPU 内存
- en: '[](https://medium.com/@bnjmn_marie?source=post_page-----c3f3fb1611a9--------------------------------)[![Benjamin
    Marie](../Images/3ea1ad230cb1e67610418a8e36a5e5dd.png)](https://medium.com/@bnjmn_marie?source=post_page-----c3f3fb1611a9--------------------------------)[](https://towardsdatascience.com/?source=post_page-----c3f3fb1611a9--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----c3f3fb1611a9--------------------------------)
    [Benjamin Marie](https://medium.com/@bnjmn_marie?source=post_page-----c3f3fb1611a9--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@bnjmn_marie?source=post_page-----c3f3fb1611a9--------------------------------)[![Benjamin
    Marie](../Images/3ea1ad230cb1e67610418a8e36a5e5dd.png)](https://medium.com/@bnjmn_marie?source=post_page-----c3f3fb1611a9--------------------------------)[](https://towardsdatascience.com/?source=post_page-----c3f3fb1611a9--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----c3f3fb1611a9--------------------------------)
    [Benjamin Marie](https://medium.com/@bnjmn_marie?source=post_page-----c3f3fb1611a9--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----c3f3fb1611a9--------------------------------)
    ·7 min read·Sep 12, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----c3f3fb1611a9--------------------------------)
    ·阅读时间 7 分钟·2023年9月12日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/8f3997b4586e11465fe44235b7675fb0.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8f3997b4586e11465fe44235b7675fb0.png)'
- en: Image made by the author with illustrations from Pixabay ([1](https://pixabay.com/vectors/computer-technology-internet-web-312476/),[2](https://pixabay.com/illustrations/falcon-birds-of-prey-bird-2928724/))
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者制作，插图来源于 Pixabay ([1](https://pixabay.com/vectors/computer-technology-internet-web-312476/),[2](https://pixabay.com/illustrations/falcon-birds-of-prey-bird-2928724/))
- en: 'In May 2023, The Technology Innovation Institute (TII) of Abu-Dhabi released
    two pre-trained LLMs: Falcon-7B and Falcon-40B, and their chat versions. These
    two models demonstrated very good performance and were ranked first on the [OpenLLM
    leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard).'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 2023年5月，阿布扎比技术创新研究所（TII）发布了两个预训练的 LLM：Falcon-7B 和 Falcon-40B 及其聊天版本。这两个模型表现非常出色，并在
    [OpenLLM 排行榜](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard)
    上排名第一。
- en: 'A third model released by TII just joined the Falcon family: Falcon 180B, a
    180 billion parameter model. It has 2.5 more parameters than Llama 2 70B and 4.5
    more than Falcon-40B.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: TII 发布的第三个模型刚刚加入了 Falcon 家族：**Falcon 180B**，一个具有 1800 亿参数的模型。它比 Llama 2 70B
    多了 2.5 倍的参数，比 Falcon-40B 多了 4.5 倍。
- en: 'Here are some facts about Falcon 180B (source: [Falcon 180B model card](https://huggingface.co/tiiuae/falcon-180B)):'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是关于 Falcon 180B 的一些事实（来源：[Falcon 180B 模型卡](https://huggingface.co/tiiuae/falcon-180B)）：
- en: Pre-trained on 3.5 trillion tokens ([RefinedWeb](https://huggingface.co/datasets/tiiuae/falcon-refinedweb))
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 经过 3.5 万亿个标记的预训练 ([RefinedWeb](https://huggingface.co/datasets/tiiuae/falcon-refinedweb))
- en: Distributed with an Apache 2.0 license
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 以 Apache 2.0 许可证分发
- en: Has a size of 360 GB
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 大小为 360 GB
- en: 'Is ranked first (as of 11th September 2023) on the [OpenLLM leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard):'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 [OpenLLM 排行榜](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard)
    上排名第一（截至2023年9月11日）：
- en: '![](../Images/2ae19cc41090ae37059620b860053c96.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2ae19cc41090ae37059620b860053c96.png)'
- en: Screenshot of the OpenLLM leaderboard (11th September 2023) — Image by the author
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: OpenLLM 排行榜截图（2023年9月11日） — 作者提供的图片
- en: 'There is also a chat version. The models are available on the Hugging Face
    hub:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 还有一个聊天版本。模型可以在 Hugging Face hub 上获取：
- en: '[Falcon 180B](https://huggingface.co/tiiuae/falcon-180B)'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Falcon 180B](https://huggingface.co/tiiuae/falcon-180B)'
- en: '[Falcon 180B Chat](https://huggingface.co/tiiuae/falcon-180B-chat)'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Falcon 180B Chat](https://huggingface.co/tiiuae/falcon-180B-chat)'
- en: Falcon 180B is completely free and state-of-the-art. But it’s also a huge model.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '**Falcon 180B** 完全免费且技术先进。但它也是一个庞大的模型。'
- en: '*Can it run on your computer?*'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '*它能在你的计算机上运行吗？*'
- en: Unless your computer is ready for very intensive computing, it can’t run Falcon
    180B out-of-the-box. You will need to upgrade your computer and use a quantized
    version of the model.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 除非你的计算机准备好进行非常高强度的计算，否则无法开箱即用运行 Falcon 180B。你需要升级计算机并使用该模型的量化版本。
- en: In this article, I explain how you can run Falcon-180B on consumer hardware.
    We will see that it can be reasonably affordable to run a 180 billion parameter
    model on a modern computer. I also discuss several techniques that help reduce
    the hardware requirements.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我解释了如何在消费级硬件上运行 Falcon-180B。我们将看到在现代计算机上运行一个 1800 亿参数的模型是相对负担得起的。我还讨论了几种有助于减少硬件要求的技术。
- en: 'Loading Falcon 180B on your computer: What do you need?'
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在你的计算机上加载 Falcon 180B：你需要什么？
- en: The first thing you need to know is that Falcon 180B has 180 billion parameters
    stored as bfloat16\. A (b)float16 parameter is 2 bytes in memory.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 首先你需要知道的是，Falcon 180B 具有 1800 亿个参数，存储为 bfloat16。一个 (b)float16 参数在内存中占用 2 字节。
- en: 'When you load a model, the standard Pytorch pipeline works like this:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 当你加载一个模型时，标准的 Pytorch 流程如下：
- en: 'An empty model is created: 180B parameters * 2 bytes = 360 GB'
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一个空模型被创建：180B 参数 * 2 字节 = 360 GB
- en: 'Load in memory its weights: 180B parameters * 2 bytes = 360 GB'
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将权重加载到内存中：180B 参数 * 2 字节 = 360 GB
- en: Load the weights loaded at step 2 in the empty model created at step 1
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将第 2 步加载的权重加载到第 1 步创建的空模型中。
- en: Move the model obtained at step 3 on the device for inference, e.g., a GPU
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将第 3 步获得的模型移动到用于推理的设备上，例如 GPU。
- en: Step 1 and step 2 are the ones that consume memory. In total, you would need
    720 GB of memory available. This can be CPU RAM, but for fast inference, you may
    want to use GPUs, e.g., 9 A100 with 80 GB of VRAM.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 第 1 步和第 2 步是消耗内存的步骤。总的来说，你需要 720 GB 的可用内存。这可以是 CPU RAM，但为了快速推理，你可能想使用 GPU，例如
    9 个带有 80 GB VRAM 的 A100。
- en: With either CPU RAM or VRAM, that’s a lot of memory. Fortunately, these requirements
    can be easily reduced.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 无论是 CPU RAM 还是 VRAM，这都是大量的内存。幸运的是，这些要求可以很容易地减少。
- en: On the Hugging Face Hub, Falcon 180B is distributed using the safetensors format.
    This format has several advantages over the standard Pytorch format. It (almost)
    does zero copy so the model is directly loaded into the empty model created at
    step 1\. It saves a lot of memory.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Hugging Face Hub 上，Falcon 180B 采用 safetensors 格式分发。这个格式相较于标准的 Pytorch 格式有几个优势。它（几乎）没有复制，因此模型直接加载到第
    1 步创建的空模型中。这节省了大量内存。
- en: '***About safetensors***'
  id: totrans-34
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '***关于 safetensors***'
- en: ''
  id: totrans-35
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[*safetensors*](https://huggingface.co/docs/safetensors/index) *saves memory
    but it also makes the model safer to run since no arbitrary code can be stored
    in this format. safetensors models are also much faster to load. Use this format
    instead of the “.bin” format when you download models from the hub for faster,
    safe, and memory-efficient loading.*'
  id: totrans-36
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '[*safetensors*](https://huggingface.co/docs/safetensors/index) *节省内存，但它还使模型运行更安全，因为在这种格式中无法存储任意代码。safetensors
    模型的加载速度也更快。当你从 hub 下载模型时，使用这种格式代替“.bin”格式，可以实现更快、更安全且节省内存的加载。*'
- en: Even though it looks like we skip step 2, there is still some memory overhead
    to expect. TII wrote on [the model card](https://huggingface.co/tiiuae/falcon-180B)
    that 400 GB of memory would work. That’s still a lot but 220 GB less than using
    the standard Pytorch format.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管看起来我们跳过了第 2 步，但仍然会有一些内存开销需要预期。TII 在[模型卡](https://huggingface.co/tiiuae/falcon-180B)上写道，400
    GB 的内存是可行的。这仍然很多，但比使用标准的 Pytorch 格式少 220 GB。
- en: We need a device with 400 GB, e.g., 5 A100 GPUs with 80 GB of VRAM. We are still
    far from a “consumer” configuration.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要一个具有 400 GB 存储空间的设备，例如，5 个带有 80 GB VRAM 的 A100 GPU。我们距离“消费级”配置还有很大差距。
- en: Split Falcon 180B over multiple memory devices
  id: totrans-39
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将 Falcon 180B 分割到多个内存设备上。
- en: 'You may not have a single memory device with 400 GB, but your computer has
    probably more than 400 GB of memory if you combine the memory of all the following
    devices:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能没有一个 400 GB 的单一内存设备，但如果你结合所有以下设备的内存，你的计算机可能有超过 400 GB 的内存：
- en: 'The GPUs VRAM: If you have an NVIDIA RTX 3090 or 4090, that’s already 24 GB.'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GPU 的 VRAM：如果你有一块 NVIDIA RTX 3090 或 4090，那已经有 24 GB。
- en: 'The CPU RAM: Most modern computers have at least 12 GB of CPU RAM. CPU RAM
    is also very cheap to extend.'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CPU RAM：大多数现代计算机至少有 12 GB 的 CPU RAM。扩展 CPU RAM 也非常便宜。
- en: 'The hard drive (or SSD): That can be several terabytes of free memory. Note
    that an SSD (type NVMe M2) would be much faster than a typical hard drive if you
    plan to use it to run an LLM.'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 硬盘（或 SSD）：这可以是几个 TB 的空闲内存。请注意，如果你计划使用 SSD（NVMe M2 类型）来运行 LLM，它将比典型的硬盘快得多。
- en: 'To take advantage of the devices available, we can split Falcon 180B so that
    it uses the maximum memory available of a device in this order of priority: GPU,
    CPU RAM, and hard drive.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 为了利用可用设备，我们可以将 Falcon 180B 切分，以便它按照优先顺序使用设备的最大内存：GPU、CPU RAM 和硬盘。
- en: This is easily achievable with [Accelerate](https://huggingface.co/docs/accelerate/index)
    device_map.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 通过[Accelerate](https://huggingface.co/docs/accelerate/index)的device_map可以轻松实现这一点。
- en: device_map puts entire layers of the model on the different devices that you
    have.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: device_map将模型的整个层分配到你拥有的不同设备上。
- en: '![](../Images/346ada05e45868b2a6a42513bf97173a.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/346ada05e45868b2a6a42513bf97173a.png)'
- en: device_map — Image by the author
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: device_map — 作者提供的图片
- en: If you want to see some examples of device_map usage, [check my notebooks](https://kaitchup.substack.com/p/notebooks).
    I use device_map in most of them.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想查看一些device_map使用的例子，[查看我的笔记本](https://kaitchup.substack.com/p/notebooks)。我在大多数笔记本中使用了device_map。
- en: '[device_map is very convenient to avoid CUDA out-of-memory errors](https://kaitchup.substack.com/p/device-map-avoid-out-of-memory-errors-when-running-large-language-models-af7de5076f9d).
    But still far from ideal if you plan to use Falcon 180B on consumer hardware.
    Even with a high-end configuration with 24 GB of VRAM and 32 GB of CPU RAM, it
    would leave several hundred GBs on the hard drive.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '[device_map非常方便可以避免CUDA内存不足错误](https://kaitchup.substack.com/p/device-map-avoid-out-of-memory-errors-when-running-large-language-models-af7de5076f9d)。但如果你打算在消费级硬件上使用Falcon
    180B，这仍然远非理想。即使配置高端，配备24 GB的VRAM和32 GB的CPU RAM，也会在硬盘上留下几百GB的空间。'
- en: 'This is an issue for two reasons:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个问题，原因有两个：
- en: hard drives and SSDs are much **slower than VRAM and CPU RAM**. Loading and
    running Falcon 180B from a hard drive would take a very long time.
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 硬盘和SSD的**速度比VRAM和CPU RAM**慢得多。从硬盘加载和运行Falcon 180B会花费非常长的时间。
- en: consumer hard drives and SSDs were **not designed and tested for this kind of
    intensive use**. If many parts of the model are offloaded on the hard drive, the
    system would have to access and read the huge splits of the models many times
    during inference. It’s a huge number of reading operations for a prolonged time.
    If you do inference for days, e.g., to generate some synthetic datasets, that
    may break your hard drive or at least significantly reduce its life expectancy.
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 消费级硬盘和SSD**并未设计和测试用于这种密集使用**。如果模型的许多部分被卸载到硬盘上，系统将不得不在推理过程中多次访问和读取这些巨大的模型分片。这是一个长期的巨大读写操作数量。如果你进行几天的推理，例如生成一些合成数据集，这可能会损坏你的硬盘，或者至少显著减少其寿命。
- en: 'To avoid overusing the hard drive, we don’t have many solutions:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 为了避免过度使用硬盘，我们没有太多解决方案：
- en: 'Add one more GPU: Most high-end motherboards can hold two RTX 3090/4090\. It
    would give you 48 GB of VRAM.'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 添加一张GPU：大多数高端主板可以容纳两张RTX 3090/4090。这将为你提供48 GB的VRAM。
- en: 'Extend the CPU RAM: Most motherboards have 4 slots available for CPU RAM kits.
    Kits of 4*128GB CPU RAM are sold but not easy to find and are still expensive.
    *Note: There is also a limit on the total amount of CPU RAM that your OS can support.
    For Windows 10, it’s 2 TB. If you have an older OS you should have a look at its
    documentation before buying more RAM.*'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 扩展CPU RAM：大多数主板有4个可用于CPU RAM套件的插槽。虽然有4*128GB的CPU RAM套件出售，但不容易找到，而且仍然很昂贵。*注意：操作系统对CPU
    RAM的总支持量也有限制。对于Windows 10，它是2 TB。如果你使用的是旧版本操作系统，在购买更多RAM之前应查看其文档。*
- en: Quantize Falcon 180B and extend the CPU RAM.
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 量化Falcon 180B并扩展CPU RAM。
- en: The quantization of Falcon 180B is one of the best options to reduce its memory
    consumption.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: Falcon 180B的量化是减少内存消耗的最佳选择之一。
- en: Reduce the size of Falcon 180B with quantization
  id: totrans-59
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通过量化减少Falcon 180B的大小
- en: It’s now common practice to quantize very large language models to a lower precision.
    [GPTQ](https://arxiv.org/abs/2210.17323) and [bitsandbytes nf4](https://arxiv.org/abs/2305.14314)
    are two popular ways to quantize LLM to 4-bit precision.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 量化非常大的语言模型到较低精度现在已成为一种常见做法。[GPTQ](https://arxiv.org/abs/2210.17323)和[bitsandbytes
    nf4](https://arxiv.org/abs/2305.14314)是将LLM量化到4位精度的两种流行方法。
- en: Falcon 180B uses bfloat16\. We saw that it’s 360 GB.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: Falcon 180B使用bfloat16。我们看到它是360 GB。
- en: Once quantized to 4-bit precision, it’s only 90 GB (180 billion parameters *
    0.5 Bytes). We can load the 4-bit Falcon 180B with 100 GB of memory (90GB + some
    memory overhead).
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦量化到4位精度，它仅为90 GB（1800亿参数 * 0.5 字节）。我们可以使用100 GB的内存（90GB + 一些内存开销）来加载4位Falcon
    180B。
- en: 'If you have 24 GB of VRAM, you “only” need 75 GB of CPU RAM. It is still a
    lot but much more affordable than loading the original model, and it wouldn’t
    offload layers of the models on the hard drive during inference. *Note: You still
    need 100 GB of free space on the hard drive to store the model.*'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你有24 GB的VRAM，你“仅需”75 GB的CPU RAM。这仍然很多，但比加载原始模型要实惠得多，而且在推理过程中不会将模型的层卸载到硬盘上。*注意：你仍然需要在硬盘上留出100
    GB的自由空间以存储模型。*
- en: You don’t even need a GPU. With 128 GB of CPU RAM, you could do inference using
    only your CPU.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 你甚至不需要GPU。拥有128GB的CPU内存，你可以仅使用CPU进行推理。
- en: 'The quantization itself is extremely costly. Thankfully, we can already find
    quantized versions online. [TheBloke](https://huggingface.co/TheBloke) released
    4-bit versions made with GPTQ:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 量化本身是非常昂贵的。幸运的是，我们已经可以在网上找到量化版本。[TheBloke](https://huggingface.co/TheBloke)发布了使用GPTQ制作的4-bit版本：
- en: '[4-bit Falcon 180B](https://huggingface.co/TheBloke/Falcon-180B-GPTQ)'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4-bit Falcon 180B](https://huggingface.co/TheBloke/Falcon-180B-GPTQ)'
- en: '[4-bit Faclon 180B Chat](https://huggingface.co/TheBloke/Falcon-180B-Chat-GPTQ)'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4-bit Falcon 180B Chat](https://huggingface.co/TheBloke/Falcon-180B-Chat-GPTQ)'
- en: '*Note: There are also 3-bit models available as “branches” of the models. Follow
    the instructions from the 4-bit model cards to get them.*'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '*注意：还有3-bit模型作为这些模型的“分支”提供。请遵循4-bit模型卡上的说明来获取这些模型。*'
- en: While the precision of the model is reduced, the performance of the model remains
    similar according to [Hugging Face’s experiments](https://huggingface.co/blog/falcon-180b).
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管模型的精度有所降低，但根据[Hugging Face的实验](https://huggingface.co/blog/falcon-180b)，模型的性能保持相似。
- en: 'GPTQ models are fast for inference and you can fine-tune them with LoRA adapters.
    For instance, I show how to fine-tune Llama 2 quantized with GPTQ in this article:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: GPTQ模型推理速度快，你可以使用LoRA适配器进行微调。例如，我在这篇文章中展示了如何微调使用GPTQ量化的Llama 2。
- en: '[](https://kaitchup.substack.com/p/quantize-and-fine-tune-llms-with?source=post_page-----c3f3fb1611a9--------------------------------)
    [## Quantize and Fine-tune LLMs with GPTQ Using Transformers and TRL'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '[## 使用Transformers和TRL进行量化和微调LLMs](https://kaitchup.substack.com/p/quantize-and-fine-tune-llms-with?source=post_page-----c3f3fb1611a9--------------------------------)'
- en: GPTQ is now much easier to use
  id: totrans-72
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: GPTQ现在使用起来容易多了
- en: kaitchup.substack.com](https://kaitchup.substack.com/p/quantize-and-fine-tune-llms-with?source=post_page-----c3f3fb1611a9--------------------------------)
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '[kaitchup.substack.com](https://kaitchup.substack.com/p/quantize-and-fine-tune-llms-with?source=post_page-----c3f3fb1611a9--------------------------------)'
- en: While it’s possible to fine-tune GPTQ models, I don’t recommend it. Fine-tuning
    with QLoRA has a similar memory consumption but yields better models thanks to
    the better quantization with nf4 as shown in the [QLoRA paper](https://arxiv.org/abs/2305.14314).
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然可以对GPTQ模型进行微调，但我不推荐这样做。使用QLoRA进行微调虽然内存消耗相似，但由于nf4量化的改进，能获得更好的模型，正如[QLoRA论文](https://arxiv.org/abs/2305.14314)中所示。
- en: Conclusion
  id: totrans-75
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: To sum up, you need quantization and 100 GB of memory to run Falcon 180B on
    a reasonably affordable computer.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 总结一下，你需要量化和100GB内存来在一个相对实惠的电脑上运行Falcon 180B。
- en: For fast inference or fine-tuning, you will need a GPU. The RTX 4090 (or the
    RTX 3090 24GB, which is more affordable but slower) would be enough to load 1/4
    of the quantized model. If you have enough space in your computer case, you could
    even put two RTX cards.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 对于快速推理或微调，你需要一个GPU。RTX 4090（或更便宜但较慢的RTX 3090 24GB）足以加载量化模型的1/4。如果你的电脑机箱有足够的空间，你甚至可以放入两张RTX显卡。
- en: If you want to run Falcon-180B on a CPU-only configuration, i.e., without a
    GPU, forget about fine-tuning, it would be too slow. Inference would also be slow
    but with a recent high-end CPU and software optimized for faster inference, such
    as llama.cpp, running Falcon 180B is possible.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想在仅CPU配置上运行Falcon-180B，即没有GPU，忘掉微调吧，这会太慢。推理也会很慢，但使用近期的高端CPU和优化过的软件，如llama.cpp，运行Falcon
    180B还是可能的。
- en: 'If you are interested in using [llama.cpp](https://github.com/ggerganov/llama.cpp)
    have a look at my article explaining how to run Vicuna using only the CPU:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你对使用[llama.cpp](https://github.com/ggerganov/llama.cpp)感兴趣，可以看看我关于如何仅使用CPU运行Vicuna的文章：
- en: '[](https://kaitchup.substack.com/p/high-speed-inference-with-llama-cpp-and-vicuna-on-cpu-136d28e7887b?source=post_page-----c3f3fb1611a9--------------------------------)
    [## High-Speed Inference with llama.cpp and Vicuna on CPU'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '[## 高速推理：使用llama.cpp和Vicuna在CPU上](https://kaitchup.substack.com/p/high-speed-inference-with-llama-cpp-and-vicuna-on-cpu-136d28e7887b?source=post_page-----c3f3fb1611a9--------------------------------)'
- en: You don't need a GPU for fast inference
  id: totrans-81
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 快速推理不需要GPU
- en: kaitchup.substack.com](https://kaitchup.substack.com/p/high-speed-inference-with-llama-cpp-and-vicuna-on-cpu-136d28e7887b?source=post_page-----c3f3fb1611a9--------------------------------)
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '[kaitchup.substack.com](https://kaitchup.substack.com/p/high-speed-inference-with-llama-cpp-and-vicuna-on-cpu-136d28e7887b?source=post_page-----c3f3fb1611a9--------------------------------)'
