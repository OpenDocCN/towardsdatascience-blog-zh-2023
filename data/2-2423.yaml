- en: 'Zephyr 7B Beta: A Good Teacher Is All You Need'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Zephyr 7B Beta：一个好的老师就是你所需要的一切
- en: 原文：[https://towardsdatascience.com/zephyr-7b-beta-a-good-teacher-is-all-you-need-c931fcd0bfe7](https://towardsdatascience.com/zephyr-7b-beta-a-good-teacher-is-all-you-need-c931fcd0bfe7)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/zephyr-7b-beta-a-good-teacher-is-all-you-need-c931fcd0bfe7](https://towardsdatascience.com/zephyr-7b-beta-a-good-teacher-is-all-you-need-c931fcd0bfe7)
- en: Knowledge distillation for Mistral 7B
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Mistral 7B 的知识蒸馏
- en: '[](https://medium.com/@bnjmn_marie?source=post_page-----c931fcd0bfe7--------------------------------)[![Benjamin
    Marie](../Images/3ea1ad230cb1e67610418a8e36a5e5dd.png)](https://medium.com/@bnjmn_marie?source=post_page-----c931fcd0bfe7--------------------------------)[](https://towardsdatascience.com/?source=post_page-----c931fcd0bfe7--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----c931fcd0bfe7--------------------------------)
    [Benjamin Marie](https://medium.com/@bnjmn_marie?source=post_page-----c931fcd0bfe7--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@bnjmn_marie?source=post_page-----c931fcd0bfe7--------------------------------)[![本杰明·玛丽](../Images/3ea1ad230cb1e67610418a8e36a5e5dd.png)](https://medium.com/@bnjmn_marie?source=post_page-----c931fcd0bfe7--------------------------------)[](https://towardsdatascience.com/?source=post_page-----c931fcd0bfe7--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----c931fcd0bfe7--------------------------------)
    [本杰明·玛丽](https://medium.com/@bnjmn_marie?source=post_page-----c931fcd0bfe7--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----c931fcd0bfe7--------------------------------)
    ·8 min read·Nov 11, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----c931fcd0bfe7--------------------------------)
    ·阅读时间 8 分钟·2023年11月11日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/c9f381fbf16adcf625a9df3101552363.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c9f381fbf16adcf625a9df3101552363.png)'
- en: Image from [Pixabay](https://pixabay.com/illustrations/man-drinking-booze-drinker-5334659/)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源于 [Pixabay](https://pixabay.com/illustrations/man-drinking-booze-drinker-5334659/)
- en: Mistral 7B is one of the [best pre-trained large language modes (LLMs)](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard).
    By releasing [Zephyr 7B Alpha](https://huggingface.co/HuggingFaceH4/zephyr-7b-alpha),
    Hugging Face has demonstrated that Mistral 7B fine-tuned with DPO can outperform
    chat models that are 10 times bigger and even match the performance of GPT-4 for
    some tasks.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: Mistral 7B 是其中一个 [最佳预训练大型语言模型（LLM）](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard)。通过发布
    [Zephyr 7B Alpha](https://huggingface.co/HuggingFaceH4/zephyr-7b-alpha)，Hugging
    Face 已证明 Mistral 7B 经过 DPO 微调后可以超越比其大 10 倍的聊天模型，并且在某些任务上与 GPT-4 的性能相匹配。
- en: 'With the “Alpha” in the name of the model, Hugging Face was obviously planning
    to release better versions of Zephyr 7B. And they indeed released [Zephyr 7B Beta](https://huggingface.co/HuggingFaceH4/zephyr-7b-beta)
    only 2 weeks later. There is a technical report on arXiv describing the model
    and its evaluation:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 模型名称中的“Alpha”显然表明 Hugging Face 计划发布更好的 Zephyr 7B 版本。他们确实在仅仅两周后发布了 [Zephyr 7B
    Beta](https://huggingface.co/HuggingFaceH4/zephyr-7b-beta)。arXiv 上有一份技术报告描述了该模型及其评估：
- en: '[Zephyr: Direct Distillation of LM Alignment](https://arxiv.org/abs/2310.16944)
    (Tunstall et al., 2023)'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '[Zephyr: 语言模型对齐的直接蒸馏](https://arxiv.org/abs/2310.16944) (Tunstall 等, 2023)'
- en: In this article, we will see what makes Zephyr 7B Beta better than larger LLMs.
    More particularly, we will see how Hugging Face leveraged larger LLMs, such as
    GPT-4, to teach Mistral 7B to answer instructions and align the answers with human
    preferences.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我们将看到是什么让 Zephyr 7B Beta 比更大的 LLM 更好。更具体地说，我们将了解 Hugging Face 如何利用更大的
    LLM，如 GPT-4，来教导 Mistral 7B 以响应指令并将回答与人类偏好对齐。
- en: 'Distillation: When Smaller LLMs Learn from Larger Ones'
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 蒸馏：当较小的 LLM 向较大的 LLM 学习
- en: Since Hugging Face relied on knowledge distillation (KD) to train Zephyr, let’s
    have a brief reminder of what KD is in the context of LLMs.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 Hugging Face 依赖于知识蒸馏（KD）来训练 Zephyr，让我们简单回顾一下 KD 在 LLM 上的应用。
- en: Most LLMs are trained on texts written by humans. Human texts present a high
    diversity of sequences of tokens and vocabulary that is difficult to model. Because
    of this difficulty, we need a lot of data to train an LLM to properly model language.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数 LLM 是在由人类撰写的文本上进行训练的。人类文本具有高多样性的标记序列和词汇，这使得建模变得困难。由于这种困难，我们需要大量的数据来训练 LLM
    以正确建模语言。
- en: 'There is a shortcut to reduce the training cost and difficulty: knowledge **distillation**
    (KD). There are many ways to do KD. In this section, I’ll only discuss the method
    used by Hugging Face.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 有一个减少训练成本和难度的捷径：知识 **蒸馏**（KD）。有多种方法可以进行 KD。在这一部分，我将仅讨论 Hugging Face 使用的方法。
- en: 'Once trained on human texts, even though LLMs can be very good at generating
    language, they only approximate the true probability distribution of language.
    LLMs generate by default much less diverse sequences of tokens than humans. *Note:
    That’s why random sampling is often introduced during inference, for instance
    via* [*nucleus sampling*](https://arxiv.org/abs/1904.09751)*, to improve the diversity
    in the generated text.*'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦在人工文本上训练，即使LLMs在生成语言方面表现非常好，它们也只能近似语言的真实概率分布。LLMs生成的token序列比人类的要少得多。*注意：这就是为什么在推理过程中经常引入随机抽样，例如通过*[*核采样*](https://arxiv.org/abs/1904.09751)*，以提高生成文本的多样性。*
- en: Since sequences of tokens generated by LLMs are less diverse than human text,
    learning to model these generated sequences is a much easier task.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 由于LLMs生成的token序列比人类文本的多样性要低，因此学习建模这些生成的序列是一项容易得多的任务。
- en: In practice, this is achieved by using a state-of-the-art model, often called
    the **teacher** model, to generate a large amount of synthetic text that will
    be used to train a smaller model, often called the **student** model. The student
    distills the knowledge of its teacher.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，这是通过使用最先进的模型（通常称为**教师**模型）来生成大量合成文本，这些文本将用于训练一个较小的模型，通常称为**学生**模型。学生模型从其教师那里提取知识。
- en: The student model’s training converges much faster on the generated text and
    can achieve a performance close to the teacher*.*
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 学生模型在生成文本上的训练收敛速度要快得多，并且可以达到接近教师模型的性能。*。
- en: 'This strategy works well for training LLMs. One of the best examples of success
    that we have is Microsoft’s phi-1.5: A 1.3 billion parameter model matching the
    performance of much larger models. Microsoft’s phi-1.5 has been exclusively trained
    on synthetic data generated by other models, i.e., Microsoft’s phi-1.5 is a student
    model. *Note: Microsoft didn’t disclose what were the teacher models.*'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 这种策略在训练LLMs时效果很好。我们成功的最佳例子之一是微软的phi-1.5：一个拥有13亿参数的模型，其表现与更大模型相匹配。微软的phi-1.5完全在由其他模型生成的合成数据上进行训练，即，微软的phi-1.5是一个学生模型。*注意：微软没有披露教师模型是什么。*
- en: '[](https://kaitchup.substack.com/p/how-to-fine-tune-quantize-and-run?source=post_page-----c931fcd0bfe7--------------------------------)
    [## How to Fine-tune, Quantize, and Run Microsoft phi-1.5'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '[## 如何微调、量化和运行微软phi-1.5](https://kaitchup.substack.com/p/how-to-fine-tune-quantize-and-run?source=post_page-----c931fcd0bfe7--------------------------------)'
- en: A model pre-trained for many tasks
  id: totrans-22
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 一个为多任务预训练的模型
- en: kaitchup.substack.com](https://kaitchup.substack.com/p/how-to-fine-tune-quantize-and-run?source=post_page-----c931fcd0bfe7--------------------------------)
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '[kaitchup.substack.com](https://kaitchup.substack.com/p/how-to-fine-tune-quantize-and-run?source=post_page-----c931fcd0bfe7--------------------------------)'
- en: Hugging Face’s Zephyr 7B Beta is also a student model. All its training data
    have been generated by much larger models, hence a much better performance than
    other LLMs of similar size trained on human texts (e.g., Llama 2).
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: Hugging Face的Zephyr 7B Beta也是一个学生模型。它的所有训练数据都是由更大模型生成的，因此相较于其他在人工文本上训练的类似大小的LLMs（例如Llama
    2），其表现更好。
- en: In the case of Zephyr 7B Beta, Hugging Face pushed knowledge distillation much
    further into the process of training and aligning an LLM with human preferences,
    as we will see in the next sections.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在Zephyr 7B Beta的情况下，Hugging Face将知识蒸馏推进到训练和对齐LLM与人类偏好的过程中的更深层次，正如我们在接下来的部分中将看到的那样。
- en: 'dDPO: Distilled Direct Preference Optimization with Mistral 7B'
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: dDPO：使用Mistral 7B的蒸馏直接偏好优化
- en: 'Making Zephyr 7B Beta from Mistral 7B is a three-step process:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 从Mistral 7B生成Zephyr 7B Beta是一个三步过程：
- en: Supervised fine-tuning (SFT) on instruction datasets generated by other larger
    models
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在由其他更大模型生成的指令数据集上进行监督微调（SFT）
- en: Scoring/ranking LLMs’ outputs using a state-of-the-art LLM
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用最先进的LLM对LLMs的输出进行评分/排名
- en: Training DPO with the model obtained in Step 1 on the data obtained in Step
    2
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用步骤1中获得的模型对步骤2中获得的数据进行DPO训练
- en: Distilled Supervised Fine-Tuning (dSFT)
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 蒸馏监督微调（dSFT）
- en: 'SFT is the standard first step for training an instruct/chat model. It requires
    an instruction dataset: instructions/questions paired with answers given by humans.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: SFT是训练指令/聊天模型的标准第一步。它需要一个指令数据集：指令/问题与人类给出的答案配对。
- en: The main issue here is that collecting such a dataset is extremely expensive
    since it involves human labor. A more and more common and cheaper alternative
    is to use instruction datasets generated by other LLMs.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 主要问题在于，收集这样的数据集非常昂贵，因为它涉及人力劳动。一个越来越常见且更便宜的替代方案是使用其他LLMs生成的指令数据集。
- en: 'We can find many such instruction datasets on the Hugging Face Hub that we
    can use for SFT, for instance:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在 Hugging Face Hub 上找到许多这样的指令数据集，例如：
- en: '[OpenAssistant Conversations Dataset (OASST1)](https://huggingface.co/datasets/OpenAssistant/oasst1)
    (84.4k training examples)'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[OpenAssistant Conversations Dataset (OASST1)](https://huggingface.co/datasets/OpenAssistant/oasst1)（84.4k
    训练示例）'
- en: '[OpenOrca](https://huggingface.co/datasets/Open-Orca/OpenOrca) (4.2M training
    examples)'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[OpenOrca](https://huggingface.co/datasets/Open-Orca/OpenOrca)（4.2M 训练示例）'
- en: '[openassistant-guanaco](https://huggingface.co/datasets/timdettmers/openassistant-guanaco)
    (9.8k training examples)'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[openassistant-guanaco](https://huggingface.co/datasets/timdettmers/openassistant-guanaco)（9.8k
    训练示例）'
- en: 'For Zephyr 7B Beta, Hugging Face fine-tuned Mistral 7B on a custom version
    of [Ultrachat](https://huggingface.co/datasets/stingning/ultrachat) that they
    aggressively filtered:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 Zephyr 7B Beta，Hugging Face 在自定义版本的 [Ultrachat](https://huggingface.co/datasets/stingning/ultrachat)
    上微调了 Mistral 7B，该版本经过了严格筛选：
- en: '[HuggingFaceH4/ultrachat_200k](https://huggingface.co/datasets/HuggingFaceH4/ultrachat_200k)
    (MIT license), use the “sft” splits'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[HuggingFaceH4/ultrachat_200k](https://huggingface.co/datasets/HuggingFaceH4/ultrachat_200k)（MIT
    许可证），使用“sft”拆分'
- en: '*we applied truecasing heuristics to fix the grammatical errors (approximately
    5% of the dataset), as well as several filters to focus on helpfulness and remove
    the undesired model responses.*'
  id: totrans-40
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*我们应用了真正的大小写规则来修复语法错误（大约 5% 的数据集），以及几个过滤器来关注有用性并删除不需要的模型回应。*'
- en: Hugging Face denotes this SFT “Distilled Supervised Fine-Tuning” since the fine-tuning
    is done on datasets generated by “teacher” models.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: Hugging Face 将这种 SFT 称为“蒸馏监督微调”，因为微调是在“教师”模型生成的数据集上进行的。
- en: AI Feedback through Preferences (AIF)
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 基于偏好的 AI 反馈（AIF）
- en: For alignment with humans, we need a dataset of prompts paired with ranked answers.
    We can then use DPO, or RLHF, to train the model to generate preferred answers.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 为了与人类对齐，我们需要一个包含排名答案的提示数据集。然后我们可以使用 DPO 或 RLHF 来训练模型生成首选答案。
- en: Ranking models’ answers is an expensive task requiring human labor. But again,
    we already have aligned LLMs that are good enough to make this ranking.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 对模型答案进行排名是一项昂贵的任务，需要人工劳动。但我们已经有对齐的 LLM，足以进行这种排名。
- en: We can take an existing dataset of prompts paired with answers generated by
    different models and use a state-of-the-art LLM to rank these answers.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用现有的提示数据集与由不同模型生成的答案配对，并使用最先进的 LLM 来对这些答案进行排名。
- en: For this step, Hugging Face directly used the dataset [UltraFeedback](https://huggingface.co/datasets/openbmb/UltraFeedback).
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这一步，Hugging Face 直接使用了数据集 [UltraFeedback](https://huggingface.co/datasets/openbmb/UltraFeedback)。
- en: 'UltraFeedback contains 74k prompts paired with responses generated by the following
    models:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: UltraFeedback 包含 74k 个提示，配对以下模型生成的回应：
- en: LLaMA-2–7B-chat, LLaMA-2–13B-chat, LLaMA-2–70B-chat
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LLaMA-2–7B-chat, LLaMA-2–13B-chat, LLaMA-2–70B-chat
- en: UltraLM-13B, UltraLM-65B
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: UltraLM-13B, UltraLM-65B
- en: WizardLM-7B, WizardLM-13B, WizardLM-70B
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: WizardLM-7B, WizardLM-13B, WizardLM-70B
- en: Vicuna-33B
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vicuna-33B
- en: Alpaca-7B
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Alpaca-7B
- en: Falcon-40B-instruct
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Falcon-40B-instruct
- en: MPT-30B-chat
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MPT-30B-chat
- en: StarChat-Beta
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: StarChat-Beta
- en: Pythia-12B
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pythia-12B
- en: 'Each LLM’s output is rated by GPT-4 with a score from 1 to 5 (higher is better)
    for various criteria:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 每个 LLM 的输出由 GPT-4 按 1 到 5 的评分（分数越高越好）进行评估，评分标准如下：
- en: instruction following
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 指令跟随
- en: helpfulness
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有用性
- en: honesty
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 诚实性
- en: truthfulness
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 真实性
- en: For DPO, we need a “chosen” output, i.e., the output that we prefer, and a “rejected”
    output, an output that we don’t want the model to generate.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 DPO，我们需要一个“选择”的输出，即我们偏好的输出，以及一个“拒绝”的输出，即我们不希望模型生成的输出。
- en: For the chosen output, the output with the highest mean score (using all criteria
    to compute this mean) is selected. For the rejected output, they randomly selected
    an output among the remaining ones.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 对于选择的输出，选择了得分最高的输出（使用所有标准计算这个平均值）。对于被拒绝的输出，他们在剩余的输出中随机选择一个。
- en: 'They justify this random selection as follows:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 他们将这种随机选择的理由说明如下：
- en: '*We opted for random selection instead of selecting the lowest-scored response
    to encourage diversity and make the DPO objective more challenging*'
  id: totrans-65
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*我们选择了随机选择而不是选择最低得分的回应，以鼓励多样性并使 DPO 目标更具挑战性*'
- en: 'The version of the dataset they have made and used for training DPO is here:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 他们制作并用于 DPO 训练的数据集版本在这里：
- en: '[HuggingFaceH4/ultrafeedback_binarized](https://huggingface.co/datasets/HuggingFaceH4/ultrafeedback_binarized)
    (MIT license), use the “prefs” splits'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[HuggingFaceH4/ultrafeedback_binarized](https://huggingface.co/datasets/HuggingFaceH4/ultrafeedback_binarized)（MIT
    许可证），使用“prefs”拆分'
- en: Distilled Direct Preference Optimization (dDPO)
  id: totrans-68
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 蒸馏直接偏好优化（dDPO）
- en: 'Instruct LLMs, e.g., chat models, are usually trained with reinforcement learning
    with human feedback (RLHF) using a Proximal Policy Optimization (PPO). It works
    well to align LLMs with human preferences but RLHF is also unstable and complicated.
    Indeed, before running RLHF, we need to train two models:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 指令 LLM，例如聊天模型，通常使用人类反馈的强化学习（RLHF）进行训练，采用邻近策略优化（PPO）。它能够很好地使 LLM 与人类偏好对齐，但 RLHF
    也不稳定且复杂。实际上，在运行 RLHF 之前，我们需要训练两个模型：
- en: A reference model simply trained with supervised fine-tuning (SFT) on an instruction
    dataset
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个通过在指令数据集上进行有监督微调（SFT）简单训练的参考模型
- en: A reward model trained to predict human preferences. The training data for this
    model are usually rankings by humans of models’ outputs for a given prompt. The
    reward model is trained to predict this ranking.
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个训练用于预测人类偏好的奖励模型。该模型的训练数据通常是人类对给定提示的模型输出进行排名的结果。奖励模型被训练以预测这些排名。
- en: 'Then, RLHF uses 4 different models:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，RLHF 使用 4 个不同的模型：
- en: The reference model trained with SFT
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 SFT 训练的参考模型
- en: The reward model
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 奖励模型
- en: A value model that is usually initialized by the reward model
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个通常由奖励模型初始化的价值模型
- en: The model (policy) that we want to train with RLHF which is usually initialized
    by the reference model
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们希望使用 RLHF 训练的模型（策略），通常由参考模型初始化
- en: Using all these models, RLHF uses RL to optimize a language model policy to
    produce responses with a high reward (according to the reward model) without drifting
    excessively far from the original reference model.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 使用所有这些模型，RLHF 使用 RL 来优化语言模型策略，以生成高奖励的响应（根据奖励模型），而不会过度偏离原始参考模型。
- en: Several frameworks implement RLHF to make it computationally more efficient.
    Nonetheless, it remains a complicated and unstable process involving many models.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 几个框架实现了 RLHF，使其计算上更高效。然而，它仍然是一个复杂且不稳定的过程，涉及多个模型。
- en: DPO is a simple alternative to RLHF. It implicitly optimizes the same objective
    as existing RLHF algorithms (reward maximization with a KL-divergence constraint).
    The authors of DPO demonstrate that the constrained reward maximization problem
    can be exactly optimized by solving a much simpler classification problem on human
    preferences.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: DPO 是 RLHF 的一个简单替代方案。它隐式优化与现有 RLHF 算法相同的目标（具有 KL 散度约束的奖励最大化）。DPO 的作者证明，受约束的奖励最大化问题可以通过解决一个更简单的人类偏好分类问题来精确优化。
- en: Since it can be reduced to a classification problem, DPO trains the model using
    a simple binary cross-entropy objective. DPO completely eliminates the need for
    reinforcement learning.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 由于它可以被简化为分类问题，DPO 使用简单的二元交叉熵目标来训练模型。DPO 完全消除了强化学习的需求。
- en: Given a prompt and several LLMs’ outputs ranked by humans according to their
    quality, DPO trains the model to assign a higher reward to the best outputs.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个提示和几个根据质量由人类排名的 LLM 输出，DPO 训练模型为最佳输出分配更高的奖励。
- en: 'DPO only requires two models:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: DPO 只需要两个模型：
- en: The reference model fine-tuned with SFT on instruct datasets
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 SFT 在指令数据集上微调的参考模型
- en: The base model that we want to train with DPO
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们希望使用 DPO 训练的基础模型
- en: '![](../Images/70fa71fef1c7d146088eeb5a5bf9d2e4.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/70fa71fef1c7d146088eeb5a5bf9d2e4.png)'
- en: Illustration by [Rafailov et al. (2023)](https://arxiv.org/abs/2305.18290) —
    CC-BY 4.0
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 插图由 [Rafailov et al. (2023)](https://arxiv.org/abs/2305.18290) — CC-BY 4.0
- en: 'DPO is presented by Stanford in this arXiv paper:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: DPO 在这篇 arXiv 论文中由斯坦福大学提出：
- en: '[Direct Preference Optimization: Your Language Model is Secretly a Reward Model](https://arxiv.org/abs/2305.18290)'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '[直接偏好优化：你的语言模型在秘密中是一个奖励模型](https://arxiv.org/abs/2305.18290)'
- en: Note that for Zephyr, Hugging Face calls it “Distilled Direct Preference Optimization”
    only because the SFT and preferences are generated by other LLMs. The DPO process
    itself remains standard.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，对于 Zephyr，Hugging Face 称之为“蒸馏直接偏好优化”，仅仅是因为 SFT 和偏好是由其他 LLM 生成的。DPO 过程本身仍然是标准的。
- en: 'If you are interested in using DPO to fine-tune Mistral 7B, have a look at
    my tutorial:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你对使用 DPO 微调 Mistral 7B 感兴趣，可以看看我的教程：
- en: '[](https://kaitchup.substack.com/p/fine-tune-your-own-instruct-version?source=post_page-----c931fcd0bfe7--------------------------------)
    [## Fine-tune Your Own Instruct Version of Mistral 7B with Direct Preference Optimization
    (DPO)'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://kaitchup.substack.com/p/fine-tune-your-own-instruct-version?source=post_page-----c931fcd0bfe7--------------------------------)
    [## 使用直接偏好优化（DPO）微调你自己的 Mistral 7B 指令版本'
- en: Making a cheap Zephyr 7B
  id: totrans-92
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 制作便宜的 Zephyr 7B
- en: kaitchup.substack.com](https://kaitchup.substack.com/p/fine-tune-your-own-instruct-version?source=post_page-----c931fcd0bfe7--------------------------------)
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '[Kaitchup - Fine-tune Your Own Instruct Version](https://kaitchup.substack.com/p/fine-tune-your-own-instruct-version?source=post_page-----c931fcd0bfe7--------------------------------)'
- en: The Evaluation of Zephyr 7B Beta
  id: totrans-94
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Zephyr 7B Beta 的评估
- en: 'Hugging Face has evaluated Zephyr along the following axes on MT Bench: writing,
    roleplay, reasoning, math, coding, extraction, STEM, and humanities'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: Hugging Face 在 MT Bench 上从以下几个方面评估了 Zephyr：写作、角色扮演、推理、数学、编程、提取、STEM 和人文学科
- en: '![](../Images/3ab6a054156bce278d22218d5f342cef.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3ab6a054156bce278d22218d5f342cef.png)'
- en: Illustration by [Tunstall et al. (2023)](https://arxiv.org/abs/2310.16944) —
    CC-BY 4.0
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 插图由 [Tunstall et al. (2023)](https://arxiv.org/abs/2310.16944) 提供 — CC-BY 4.0
- en: It is clear that Zephyr outperforms Llama 2 70B while performing closely to
    other state-of-the-art commercial LLMs. GPT-4, Zephyr’s main teacher, remains
    much better at reasoning, math, coding, and extraction.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，Zephyr 在性能上优于 Llama 2 70B，同时在其他最先进的商业 LLM 中表现接近。GPT-4，作为 Zephyr 的主要老师，在推理、数学、编程和提取方面仍然表现更好。
- en: They have also performed an ablation study to demonstrate the importance of
    DPO.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 他们还进行了一项消融研究，以展示 DPO 的重要性。
- en: '![](../Images/2e2c781f71bc5c30bea85f74fa86313a.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2e2c781f71bc5c30bea85f74fa86313a.png)'
- en: Table by [Tunstall et al. (2023)](https://arxiv.org/abs/2310.16944) —CC-BY 4.0
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 表格由 [Tunstall et al. (2023)](https://arxiv.org/abs/2310.16944) 提供 — CC-BY 4.0
- en: DPO alone (first row) poorly performs. However, the combination of DPO and SFT
    clearly outperforms SFT alone.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 单独使用 DPO（第一行）表现较差。然而，DPO 和 SFT 的组合明显优于单独使用 SFT。
- en: Conclusion
  id: totrans-103
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: By relying on knowledge distillation, Hugging Face has demonstrated that it
    is possible to train and align a state-of-the-art LLM without using any human
    annotations.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 通过依赖知识蒸馏，Hugging Face 展示了在不使用任何人工标注的情况下训练和对齐最先进的大型语言模型（LLM）是可能的。
- en: Zephyr 7B Beta is a rather affordable model to make, especially compared to
    other larger models such as Llama 2 Chat 70B. However, given the size of the training
    batch (2) per GPU, and the fact that they fully fine-tuned Mistral 7B, they had
    to use 16 A100 80 GB GPUs (for up to 4 hours according to the technical report).
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: Zephyr 7B Beta 是一种相对便宜的模型，尤其是与其他更大模型如 Llama 2 Chat 70B 相比。然而，鉴于每个 GPU 的训练批量（2）以及他们完全微调了
    Mistral 7B，他们不得不使用 16 个 A100 80 GB GPU（根据技术报告，训练时间长达 4 小时）。
- en: 'Note that you can use LoRA to train DPO with Hugging Face’s TRL library. It
    significantly reduces the memory consumption. Hugging Face didn’t use parameter-efficient
    fine-tuning methods such as LoRA, but they are rather optimistic that it would
    work as well as full fine-tuning:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，你可以使用 LoRA 来训练 DPO，配合 Hugging Face 的 TRL 库。这可以显著减少内存消耗。Hugging Face 没有使用像
    LoRA 这样的参数高效微调方法，但他们对这些方法能与完全微调一样有效持乐观态度：
- en: We did not experiment with parameter-efficient techniques such as LoRA (Hu et
    al., 2021), but expect similar results to hold with these methods.
  id: totrans-107
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 我们没有尝试像 LoRA（Hu et al., 2021）这样的参数高效技术，但预计这些方法也会有类似的结果。
- en: 'To support my work, consider subscribing to my newsletter:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 为了支持我的工作，请考虑订阅我的新闻通讯：
- en: '[](https://kaitchup.substack.com/?source=post_page-----c931fcd0bfe7--------------------------------)
    [## The Kaitchup - AI on a Budget | Benjamin Marie, PhD | Substack'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://kaitchup.substack.com/?source=post_page-----c931fcd0bfe7--------------------------------)
    [## Kaitchup - AI on a Budget | Benjamin Marie, PhD | Substack'
- en: Weekly news, tips, and tutorials on fine-tuning, running, and serving large
    language models on your computer. Each…
  id: totrans-110
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 每周提供有关微调、运行和在计算机上服务大型语言模型的新闻、提示和教程。每期…
- en: kaitchup.substack.com](https://kaitchup.substack.com/?source=post_page-----c931fcd0bfe7--------------------------------)
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '[Kaitchup - AI on a Budget | Benjamin Marie, PhD | Substack](https://kaitchup.substack.com/?source=post_page-----c931fcd0bfe7--------------------------------)'
