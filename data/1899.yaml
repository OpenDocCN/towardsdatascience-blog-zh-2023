- en: 'Mastering the Art of Machine Learning Workflows: A Comprehensive Guide to Transformer,
    Estimator, and Pipeline'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æŒæ¡æœºå™¨å­¦ä¹ å·¥ä½œæµçš„è‰ºæœ¯ï¼šå˜å‹å™¨ã€ä¼°ç®—å™¨å’Œç®¡é“çš„å…¨é¢æŒ‡å—
- en: åŸæ–‡ï¼š[https://towardsdatascience.com/mastering-the-art-of-machine-learning-workflows-a-comprehensive-guide-to-transformer-estimator-6254f4e2d2f8?source=collection_archive---------7-----------------------#2023-06-09](https://towardsdatascience.com/mastering-the-art-of-machine-learning-workflows-a-comprehensive-guide-to-transformer-estimator-6254f4e2d2f8?source=collection_archive---------7-----------------------#2023-06-09)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡ï¼š[https://towardsdatascience.com/mastering-the-art-of-machine-learning-workflows-a-comprehensive-guide-to-transformer-estimator-6254f4e2d2f8?source=collection_archive---------7-----------------------#2023-06-09](https://towardsdatascience.com/mastering-the-art-of-machine-learning-workflows-a-comprehensive-guide-to-transformer-estimator-6254f4e2d2f8?source=collection_archive---------7-----------------------#2023-06-09)
- en: Write seamless code with optimal results
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ç¼–å†™æ— ç¼ä»£ç ä»¥è·å¾—æœ€ä½³ç»“æœ
- en: '[](https://medium.com/@andreas030503?source=post_page-----6254f4e2d2f8--------------------------------)[![Andreas
    Lukita](../Images/8660ca1fea5da34ce3475281c1f52152.png)](https://medium.com/@andreas030503?source=post_page-----6254f4e2d2f8--------------------------------)[](https://towardsdatascience.com/?source=post_page-----6254f4e2d2f8--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----6254f4e2d2f8--------------------------------)
    [Andreas Lukita](https://medium.com/@andreas030503?source=post_page-----6254f4e2d2f8--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@andreas030503?source=post_page-----6254f4e2d2f8--------------------------------)[![Andreas
    Lukita](../Images/8660ca1fea5da34ce3475281c1f52152.png)](https://medium.com/@andreas030503?source=post_page-----6254f4e2d2f8--------------------------------)[](https://towardsdatascience.com/?source=post_page-----6254f4e2d2f8--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----6254f4e2d2f8--------------------------------)
    [Andreas Lukita](https://medium.com/@andreas030503?source=post_page-----6254f4e2d2f8--------------------------------)'
- en: Â·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: Â·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F955ef38ea7b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmastering-the-art-of-machine-learning-workflows-a-comprehensive-guide-to-transformer-estimator-6254f4e2d2f8&user=Andreas+Lukita&userId=955ef38ea7b&source=post_page-955ef38ea7b----6254f4e2d2f8---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----6254f4e2d2f8--------------------------------)
    Â·14 min readÂ·Jun 9, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F6254f4e2d2f8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmastering-the-art-of-machine-learning-workflows-a-comprehensive-guide-to-transformer-estimator-6254f4e2d2f8&user=Andreas+Lukita&userId=955ef38ea7b&source=-----6254f4e2d2f8---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[å…³æ³¨](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F955ef38ea7b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmastering-the-art-of-machine-learning-workflows-a-comprehensive-guide-to-transformer-estimator-6254f4e2d2f8&user=Andreas+Lukita&userId=955ef38ea7b&source=post_page-955ef38ea7b----6254f4e2d2f8---------------------post_header-----------)
    å‘å¸ƒäº [Towards Data Science](https://towardsdatascience.com/?source=post_page-----6254f4e2d2f8--------------------------------)
    Â·14 min readÂ·2023å¹´6æœˆ9æ—¥[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F6254f4e2d2f8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmastering-the-art-of-machine-learning-workflows-a-comprehensive-guide-to-transformer-estimator-6254f4e2d2f8&user=Andreas+Lukita&userId=955ef38ea7b&source=-----6254f4e2d2f8---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F6254f4e2d2f8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmastering-the-art-of-machine-learning-workflows-a-comprehensive-guide-to-transformer-estimator-6254f4e2d2f8&source=-----6254f4e2d2f8---------------------bookmark_footer-----------)![](../Images/750da8ba509c1ea6c28de2648e625fa5.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F6254f4e2d2f8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmastering-the-art-of-machine-learning-workflows-a-comprehensive-guide-to-transformer-estimator-6254f4e2d2f8&source=-----6254f4e2d2f8---------------------bookmark_footer-----------)![](../Images/750da8ba509c1ea6c28de2648e625fa5.png)'
- en: Photo by [Rick Hyne](https://unsplash.com/de/@quinley1770?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ç‰‡ç”± [Rick Hyne](https://unsplash.com/de/@quinley1770?utm_source=medium&utm_medium=referral)
    è´¡çŒ®ï¼Œæ¥æºäº [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: '*â€œItâ€™s okay to write it this way as long as I understand it now, and the good
    thing is, it works! I manage to magically churns out a pretty good result with
    my model, what a good one to end off the day.â€*'
  id: totrans-9
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*â€œåªè¦æˆ‘ç°åœ¨èƒ½ç†è§£å®ƒï¼Œå†™æˆè¿™æ ·ä¹Ÿæ²¡å…³ç³»ï¼Œè€Œå¥½çš„ä¸€ç‚¹æ˜¯ï¼Œå®ƒç¡®å®æœ‰æ•ˆï¼æˆ‘æˆåŠŸåœ°ç”¨æˆ‘çš„æ¨¡å‹å¥‡è¿¹èˆ¬åœ°å¾—åˆ°äº†ä¸€ä¸ªç›¸å½“ä¸é”™çš„ç»“æœï¼ŒçœŸæ˜¯ä¸€ä¸ªä¸é”™çš„æ”¶å°¾ã€‚â€*'
- en: No, Iâ€™m here to tell you thatâ€™s not good enough. Indeed, upon starting a Machine
    Learning project, many novices and intermediate-level analysts alike rush to produce
    mediocre-level models with a lack of proper workflow. While at times the problem
    at hand is simple, failure to follow proper workflow often leads to insidious
    problems that can be hard to detect â€” data leakage, for example.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ï¼Œæˆ‘æ¥å‘Šè¯‰ä½ ï¼Œè¿™è¿˜ä¸å¤Ÿå¥½ã€‚ç¡®å®ï¼Œå½“ä½ å¼€å§‹ä¸€ä¸ªæœºå™¨å­¦ä¹ é¡¹ç›®æ—¶ï¼Œè®¸å¤šæ–°æ‰‹å’Œä¸­çº§åˆ†æå¸ˆéƒ½æ€¥äºåˆ¶ä½œå‡ºä¸­ç­‰æ°´å¹³çš„æ¨¡å‹ï¼Œå´ç¼ºä¹é€‚å½“çš„å·¥ä½œæµç¨‹ã€‚è™½ç„¶æœ‰æ—¶å€™é—®é¢˜æœ¬èº«å¾ˆç®€å•ï¼Œä½†å¦‚æœä¸éµå¾ªé€‚å½“çš„å·¥ä½œæµç¨‹ï¼Œå¸¸å¸¸ä¼šå¯¼è‡´ä¸€äº›éš¾ä»¥å¯Ÿè§‰çš„æ½œåœ¨é—®é¢˜ï¼Œæ¯”å¦‚æ•°æ®æ³„æ¼ã€‚
- en: â€œAs long as it works, itâ€™s good enough.â€ Let me tell you itâ€™s not. Letâ€™s role-play
    a quick scenario where you have to explain your work to the Senior Analyst. Here
    are some questions. If it works today, is it guaranteed that it will work tomorrow
    and will be reproducible easily? Could you explain the preprocessing steps of
    your model workflow in a Notebook consisting of more than 200 cells? If you perform
    cross-validation this way, wouldnâ€™t that expose the testing dataset and bloat
    up the model performance? Tough questions, arenâ€™t they?
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: â€œåªè¦æœ‰æ•ˆï¼Œå°±è¶³å¤Ÿå¥½äº†ã€‚â€ è®©æˆ‘å‘Šè¯‰ä½ ï¼Œè¿™å¹¶ä¸æ˜¯ã€‚è®©æˆ‘ä»¬å¿«é€Ÿæ¨¡æ‹Ÿä¸€ä¸ªåœºæ™¯ï¼Œä½ éœ€è¦å‘é«˜çº§åˆ†æå¸ˆè§£é‡Šä½ çš„å·¥ä½œã€‚è¿™é‡Œæœ‰ä¸€äº›é—®é¢˜ã€‚å¦‚æœä»Šå¤©æœ‰æ•ˆï¼Œæ˜¯å¦èƒ½ä¿è¯æ˜å¤©ä¹Ÿæœ‰æ•ˆï¼Œå¹¶ä¸”å®¹æ˜“é‡å¤ï¼Ÿä½ èƒ½åœ¨åŒ…å«200å¤šä¸ªå•å…ƒæ ¼çš„Notebookä¸­è§£é‡Šä½ çš„æ¨¡å‹å·¥ä½œæµç¨‹çš„é¢„å¤„ç†æ­¥éª¤å—ï¼Ÿå¦‚æœä½ ä»¥è¿™ç§æ–¹å¼è¿›è¡Œäº¤å‰éªŒè¯ï¼Œæ˜¯å¦ä¼šæš´éœ²æµ‹è¯•æ•°æ®é›†å¹¶ä½¿æ¨¡å‹æ€§èƒ½è†¨èƒ€ï¼Ÿè¿™äº›é—®é¢˜å¾ˆæ£˜æ‰‹ï¼Œä¸æ˜¯å—ï¼Ÿ
- en: Let me tell you, actually, you are not alone and you are not that far. Even
    after attending multiple Business Analytics and Machine Learning courses, none
    of my instructors have shared the tools and tips that I am sharing below. I would
    say they are not the spotlight classes that everyone is paying attention to when
    first introduced to Scikit-Learn. Yet, they produce a consistent result that improves
    your code-writing dramatically. Imagine effortlessly wrangling your data, seamlessly
    transforming features, and training sophisticated models, all while maintaining
    the elegance and simplicity of your code. Yes, that is our objective by the end
    of this comprehensive guide, and hopefully, you will be convinced to adopt the
    practice below. Letâ€™s get started.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘å‘Šè¯‰ä½ ï¼Œå®é™…ä¸Šï¼Œä½ å¹¶ä¸å­¤å•ï¼Œä¹Ÿå¹¶æ²¡æœ‰é‚£ä¹ˆè¿œã€‚å³ä½¿åœ¨å‚åŠ äº†å¤šä¸ªå•†ä¸šåˆ†æå’Œæœºå™¨å­¦ä¹ è¯¾ç¨‹åï¼Œæˆ‘çš„ä»»ä½•ä¸€ä¸ªè®²å¸ˆéƒ½æ²¡æœ‰åˆ†äº«æˆ‘ä¸‹é¢æ‰€è¦åˆ†äº«çš„å·¥å…·å’ŒæŠ€å·§ã€‚æˆ‘ä¼šè¯´ï¼Œè¿™äº›ä¸æ˜¯æ¯ä¸ªäººåœ¨ç¬¬ä¸€æ¬¡æ¥è§¦Scikit-Learnæ—¶éƒ½å…³æ³¨çš„äº®ç‚¹è¯¾ç¨‹ã€‚ç„¶è€Œï¼Œå®ƒä»¬ä¼šäº§ç”Ÿä¸€è‡´çš„ç»“æœï¼Œæ˜¾è‘—æå‡ä½ çš„ä»£ç ç¼–å†™æ°´å¹³ã€‚æƒ³è±¡ä¸€ä¸‹ï¼Œè½»æ¾å¤„ç†æ•°æ®ï¼Œæµç•…åœ°è½¬æ¢ç‰¹å¾ï¼Œå¹¶è®­ç»ƒå¤æ‚çš„æ¨¡å‹ï¼ŒåŒæ—¶ä¿æŒä»£ç çš„ä¼˜é›…å’Œç®€æ´ã€‚è¿™å°±æ˜¯æˆ‘ä»¬åœ¨æœ¬ç»¼åˆæŒ‡å—ç»“æŸæ—¶çš„ç›®æ ‡ï¼Œå¸Œæœ›ä½ èƒ½è¢«ä¸‹é¢çš„å®è·µæ‰€è¯´æœã€‚è®©æˆ‘ä»¬å¼€å§‹å§ã€‚
- en: '**Table of Contents**'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '**ç›®å½•**'
- en: '[Reasons to Adopt Pipeline](#8d5b)'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[é‡‡ç”¨æµæ°´çº¿çš„ç†ç”±](#8d5b)'
- en: '[Estimators](#29de)'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[ä¼°ç®—å™¨](#29de)'
- en: '[Transformers](#4358)'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[å˜æ¢å™¨](#4358)'
- en: '[Pipeline](#fd4a)'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[æµæ°´çº¿](#fd4a)'
- en: '[Custom Estimator](#0457)'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[è‡ªå®šä¹‰ä¼°ç®—å™¨](#0457)'
- en: '[FeatureUnion](#5660)'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[ç‰¹å¾è”åˆ](#5660)'
- en: '[Real-world Dataset Examples: Bank Marketing with Grid Search CV](#547b)'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[çœŸå®ä¸–ç•Œæ•°æ®é›†ç¤ºä¾‹ï¼šé“¶è¡Œè¥é”€ä¸ç½‘æ ¼æœç´¢äº¤å‰éªŒè¯](#547b)'
- en: Reasons to Adopt Pipeline
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: é‡‡ç”¨æµæ°´çº¿çš„ç†ç”±
- en: '**1\. Streamlined Workflow.** Leveraging Pipeline allows seamless integration
    of multiple steps in your data preprocessing and modeling journey. It enables
    you to chain together various transformers and estimators, ensuring a clear, concise,
    and automated flow from data preprocessing to model training and evaluation. By
    encapsulating your preprocessing and modeling steps in a Pipeline, your code becomes
    more organized, modular, and easier to understand. It improves the way your code
    looks and can be maintained because each step is clearly defined. Treat each step
    in the Pipeline as independent, you can change or add steps without worrying about
    how one preprocessing step will affect the other!'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '**1\. ç²¾ç®€çš„å·¥ä½œæµç¨‹ã€‚** åˆ©ç”¨æµæ°´çº¿å¯ä»¥å®ç°æ•°æ®é¢„å¤„ç†å’Œå»ºæ¨¡è¿‡ç¨‹ä¸­çš„å¤šä¸ªæ­¥éª¤çš„æ— ç¼é›†æˆã€‚å®ƒä½¿ä½ èƒ½å¤Ÿå°†å„ç§å˜æ¢å™¨å’Œä¼°ç®—å™¨ä¸²è”èµ·æ¥ï¼Œç¡®ä¿ä»æ•°æ®é¢„å¤„ç†åˆ°æ¨¡å‹è®­ç»ƒå’Œè¯„ä¼°çš„æµç¨‹æ¸…æ™°ã€ç®€æ´ä¸”è‡ªåŠ¨åŒ–ã€‚é€šè¿‡å°†ä½ çš„é¢„å¤„ç†å’Œå»ºæ¨¡æ­¥éª¤å°è£…åœ¨æµæ°´çº¿ä¸­ï¼Œä½ çš„ä»£ç å˜å¾—æ›´åŠ æœ‰ç»„ç»‡ã€æ¨¡å—åŒ–ï¼Œå¹¶ä¸”æ›´æ˜“äºç†è§£ã€‚å®ƒæ”¹å–„äº†ä»£ç çš„å¤–è§‚å’Œå¯ç»´æŠ¤æ€§ï¼Œå› ä¸ºæ¯ä¸€æ­¥éƒ½è¢«æ¸…æ¥šåœ°å®šä¹‰ã€‚å°†æµæ°´çº¿ä¸­çš„æ¯ä¸€æ­¥è§†ä¸ºç‹¬ç«‹çš„ï¼Œä½ å¯ä»¥åœ¨ä¸æ‹…å¿ƒä¸€ä¸ªé¢„å¤„ç†æ­¥éª¤å¦‚ä½•å½±å“å…¶ä»–æ­¥éª¤çš„æƒ…å†µä¸‹è¿›è¡Œæ›´æ”¹æˆ–æ·»åŠ æ­¥éª¤ï¼'
- en: '![](../Images/3a976ef31dac95508e385c36c6278c77.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3a976ef31dac95508e385c36c6278c77.png)'
- en: Image by Author
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: ä½œè€…æä¾›çš„å›¾ç‰‡
- en: '**2\. Prevent Data Leakage.** The dreaded antagonist, the nemesis of every
    analyst. Data Leakage may occur when information from the test dataset unintentionally
    influences the preprocessing steps or model training, leading to overly optimistic
    performance estimates. In a way, you are leaking information about what is going
    to be tested and making your learning model see what is going to be tested in
    advance. Obviously, *â€œhe tryna gas it upâ€.* Generally, the rule of thumb is to
    fit the training dataset only, then transform both the training and testing dataset.
    ***The code below shows where some people went wrong.*** Also, often you would
    have multiple preprocessing steps that typically involve transformers such as
    `StandardScaler()`, `MinMaxScaler()`, `OneHotEncoder()`, etc. Imagine having to
    do the fit and transform process multiple times throughout your workflow, wouldnâ€™t
    that be confusing and inconvenient?'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '**2\. é˜²æ­¢æ•°æ®æ³„éœ²ã€‚** è¿™æ˜¯æ¯ä¸ªåˆ†æå¸ˆéƒ½å®³æ€•çš„å¯¹æ‰‹ã€‚æ•°æ®æ³„éœ²å¯èƒ½å‘ç”Ÿåœ¨æµ‹è¯•æ•°æ®é›†çš„ä¿¡æ¯æ— æ„ä¸­å½±å“äº†é¢„å¤„ç†æ­¥éª¤æˆ–æ¨¡å‹è®­ç»ƒï¼Œä»è€Œå¯¼è‡´è¿‡äºä¹è§‚çš„æ€§èƒ½ä¼°è®¡ã€‚ä»æŸç§ç¨‹åº¦ä¸Šæ¥è¯´ï¼Œä½ æ˜¯åœ¨æ³„éœ²å…³äºå°†è¦æµ‹è¯•çš„å†…å®¹çš„ä¿¡æ¯ï¼Œä½¿ä½ çš„å­¦ä¹ æ¨¡å‹æå‰çœ‹åˆ°å°†è¦æµ‹è¯•çš„å†…å®¹ã€‚æ˜¾ç„¶ï¼Œ*â€œä»–åœ¨è¯•å›¾å¹å˜˜â€ã€‚*
    é€šå¸¸çš„ç»éªŒæ³•åˆ™æ˜¯åªæ‹Ÿåˆè®­ç»ƒæ•°æ®é›†ï¼Œç„¶ååŒæ—¶è½¬æ¢è®­ç»ƒå’Œæµ‹è¯•æ•°æ®é›†ã€‚***ä¸‹é¢çš„ä»£ç å±•ç¤ºäº†æŸäº›äººé”™è¯¯çš„åœ°æ–¹ã€‚*** æ­¤å¤–ï¼Œä½ é€šå¸¸ä¼šæœ‰å¤šä¸ªé¢„å¤„ç†æ­¥éª¤ï¼Œè¿™äº›æ­¥éª¤é€šå¸¸æ¶‰åŠå˜æ¢å™¨ï¼Œä¾‹å¦‚
    `StandardScaler()`ã€`MinMaxScaler()`ã€`OneHotEncoder()` ç­‰ã€‚æƒ³è±¡ä¸€ä¸‹åœ¨æ•´ä¸ªå·¥ä½œæµç¨‹ä¸­å¤šæ¬¡è¿›è¡Œæ‹Ÿåˆå’Œè½¬æ¢è¿‡ç¨‹ï¼Œéš¾é“è¿™ä¸ä¼šè®©äººå›°æƒ‘å’Œä¸ä¾¿å—ï¼Ÿ'
- en: '[PRE0]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '**3\. Hyperparameter Tuning and Cross-Validation.** Easily tune hyperparameters
    across all the steps in your pipeline using techniques such as GridSearchCV. Error
    often goes unnoticed in this particular step, however. Letâ€™s look at a simple
    illustration.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '**3\. è¶…å‚æ•°è°ƒæ•´å’Œäº¤å‰éªŒè¯ã€‚** ä½¿ç”¨å¦‚ GridSearchCV ä¹‹ç±»çš„æŠ€æœ¯è½»æ¾è°ƒæ•´ç®¡é“ä¸­æ‰€æœ‰æ­¥éª¤çš„è¶…å‚æ•°ã€‚ç„¶è€Œï¼Œè¿™ä¸€æ­¥éª¤ä¸­çš„é”™è¯¯å¾€å¾€è¢«å¿½è§†ã€‚è®©æˆ‘ä»¬çœ‹ä¸€ä¸ªç®€å•çš„ä¾‹å­ã€‚'
- en: '[PRE1]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Try running both examples: although the cross-validation score is not far off,
    the part without the Pipeline leaks information since the feature selection step
    is performed on the entire dataset. When we reach the cross-validation step in
    which the dataset is separated into training and validation sets, they are essentially
    from the same source (the training set has information learned previously from
    the validation set when we perform feature selection). If you find it hard to
    understand this part, try rereading the paragraph and code it out yourself to
    internalize.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: å°è¯•è¿è¡Œè¿™ä¸¤ä¸ªç¤ºä¾‹ï¼šè™½ç„¶äº¤å‰éªŒè¯åˆ†æ•°ç›¸å·®ä¸è¿œï¼Œä½†æ²¡æœ‰ä½¿ç”¨ Pipeline çš„éƒ¨åˆ†æ³„éœ²äº†ä¿¡æ¯ï¼Œå› ä¸ºç‰¹å¾é€‰æ‹©æ­¥éª¤æ˜¯åœ¨æ•´ä¸ªæ•°æ®é›†ä¸Šè¿›è¡Œçš„ã€‚å½“æˆ‘ä»¬åˆ°è¾¾äº¤å‰éªŒè¯æ­¥éª¤æ—¶ï¼Œæ•°æ®é›†è¢«åˆ†ä¸ºè®­ç»ƒé›†å’ŒéªŒè¯é›†ï¼Œå®ƒä»¬æœ¬è´¨ä¸Šæ¥è‡ªåŒä¸€æ¥æºï¼ˆè®­ç»ƒé›†åœ¨è¿›è¡Œç‰¹å¾é€‰æ‹©æ—¶å·²ä»éªŒè¯é›†ä¸­å­¦ä¹ åˆ°ä¿¡æ¯ï¼‰ã€‚å¦‚æœä½ å‘ç°è¿™ä¸€éƒ¨åˆ†éš¾ä»¥ç†è§£ï¼Œè¯·å°è¯•é‡æ–°é˜…è¯»è¿™ä¸€æ®µå¹¶è‡ªå·±ç¼–ç ä»¥åŠ æ·±ç†è§£ã€‚
- en: Estimators
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ä¼°ç®—å™¨
- en: Before we dive deeper into what Pipeline can do, letâ€™s digress towards the components
    that form a Pipeline â€” Estimators. We will touch on the other componentsâ€”Transformers,
    Predictors, and Models in the next section.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æˆ‘ä»¬æ·±å…¥äº†è§£ Pipeline èƒ½åšä»€ä¹ˆä¹‹å‰ï¼Œè®©æˆ‘ä»¬æš‚æ—¶ç¦»å¼€ï¼Œäº†è§£ç»„æˆ Pipeline çš„ç»„ä»¶â€”â€”ä¼°ç®—å™¨ã€‚æˆ‘ä»¬å°†åœ¨ä¸‹ä¸€èŠ‚ä¸­è®¨è®ºå…¶ä»–ç»„ä»¶â€”â€”å˜æ¢å™¨ã€é¢„æµ‹å™¨å’Œæ¨¡å‹ã€‚
- en: A lot of people often get confused with the term Estimator in Scikit-learn.
    People tend to associate estimators with the ability to predictâ€”that is, with
    the `predict` method in particular. While there are some truths in that statement,
    it is unfortunately only a half-truth at best. Estimators are basically the building
    block of the Scikit-learn library. An estimator is a tool that can learn from
    your training set to create a model that can make predictions or inferences about
    new data. Since all estimators have the `fit` method to learn from the training
    set, they inherit from `BaseEstimator`
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: è®¸å¤šäººåœ¨ä½¿ç”¨ Scikit-learn æ—¶å¸¸å¸¸å¯¹ä¼°ç®—å™¨è¿™ä¸ªæœ¯è¯­æ„Ÿåˆ°å›°æƒ‘ã€‚äººä»¬å€¾å‘äºå°†ä¼°ç®—å™¨ä¸é¢„æµ‹èƒ½åŠ›è”ç³»èµ·æ¥â€”â€”ä¹Ÿå°±æ˜¯ç‰¹åˆ«æŒ‡ `predict` æ–¹æ³•ã€‚è™½ç„¶è¿™ç§è¯´æ³•æœ‰ä¸€å®šçš„çœŸå®æ€§ï¼Œä½†é—æ†¾çš„æ˜¯ï¼Œæœ€å¤šåªæ˜¯éƒ¨åˆ†çœŸå®ã€‚ä¼°ç®—å™¨åŸºæœ¬ä¸Šæ˜¯
    Scikit-learn åº“çš„æ„å»ºå—ã€‚ä¼°ç®—å™¨æ˜¯ä¸€ç§å·¥å…·ï¼Œå®ƒå¯ä»¥ä»ä½ çš„è®­ç»ƒé›†å­¦ä¹ ï¼Œåˆ›å»ºä¸€ä¸ªå¯ä»¥å¯¹æ–°æ•°æ®è¿›è¡Œé¢„æµ‹æˆ–æ¨æ–­çš„æ¨¡å‹ã€‚ç”±äºæ‰€æœ‰ä¼°ç®—å™¨éƒ½æœ‰ `fit` æ–¹æ³•æ¥ä»è®­ç»ƒé›†å­¦ä¹ ï¼Œå®ƒä»¬ç»§æ‰¿è‡ª
    `BaseEstimator`ã€‚
- en: From `BaseEstimator` itself, there is no `predict` method, only `fit`. An estimator
    does not necessarily need to have a `predict` method, although some do. An estimator
    with predict method attempts to make predictions on new, unseen data based on
    the learned model. For example, regressors and classifiers such as Linear Regression,
    Random Forest Classifier, Gradient Boosting Classifier, etc. are estimators with
    the`predict` method.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: ä»`BaseEstimator`æœ¬èº«æ¥çœ‹ï¼Œæ²¡æœ‰`predict`æ–¹æ³•ï¼Œåªæœ‰`fit`ã€‚ä¸€ä¸ªä¼°è®¡å™¨å¹¶ä¸ä¸€å®šéœ€è¦æœ‰`predict`æ–¹æ³•ï¼Œè™½ç„¶æœ‰äº›æœ‰ã€‚ä¸€ä¸ªå…·æœ‰`predict`æ–¹æ³•çš„ä¼°è®¡å™¨è¯•å›¾åŸºäºå­¦ä¹ åˆ°çš„æ¨¡å‹å¯¹æ–°çš„ã€æœªè§è¿‡çš„æ•°æ®è¿›è¡Œé¢„æµ‹ã€‚ä¾‹å¦‚ï¼Œåƒçº¿æ€§å›å½’ã€éšæœºæ£®æ—åˆ†ç±»å™¨ã€æ¢¯åº¦æå‡åˆ†ç±»å™¨ç­‰å›å½’å™¨å’Œåˆ†ç±»å™¨éƒ½æ˜¯å…·æœ‰`predict`æ–¹æ³•çš„ä¼°è®¡å™¨ã€‚
- en: Going one step further, letâ€™s peek into the original documentation for `LogisticRegression`
    class[Â²](#11e5). In the snippet below, we observe that the class inherits from
    `BaseEstimator` for the `fit` method, and `LinearClassifierMixin` for the `predict`
    method.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: æ›´è¿›ä¸€æ­¥ï¼Œæˆ‘ä»¬æ¥çœ‹çœ‹`LogisticRegression`ç±»çš„åŸå§‹æ–‡æ¡£[Â²](#11e5)ã€‚åœ¨ä¸‹é¢çš„ä»£ç ç‰‡æ®µä¸­ï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°è¯¥ç±»ç»§æ‰¿äº†`BaseEstimator`ä»¥è·å¾—`fit`æ–¹æ³•ï¼Œå¹¶ç»§æ‰¿äº†`LinearClassifierMixin`ä»¥è·å¾—`predict`æ–¹æ³•ã€‚
- en: '![](../Images/040dc7cbdee785137308b46706d97c64.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/040dc7cbdee785137308b46706d97c64.png)'
- en: '[Scikit-learn GitHub](https://github.com/scikit-learn/scikit-learn/blob/364c77e047ca08a95862becf40a04fe9d4cd2c98/sklearn/linear_model/_logistic.py)
    (BSD-3)'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '[Scikit-learn GitHub](https://github.com/scikit-learn/scikit-learn/blob/364c77e047ca08a95862becf40a04fe9d4cd2c98/sklearn/linear_model/_logistic.py)ï¼ˆBSD-3ï¼‰'
- en: Transformers
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å˜æ¢å™¨
- en: Transformer is a type of estimator with a `transform` method. Note that the
    word â€œtransformerâ€ here refers to the Scikit-learn context specifically. It should
    not be confused or mistaken with the transformer in neural network architecture,
    which has gained more attention and prominence in recent years.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: å˜æ¢å™¨æ˜¯ä¸€ç§å…·æœ‰`transform`æ–¹æ³•çš„ä¼°è®¡å™¨ã€‚è¯·æ³¨æ„ï¼Œè¿™é‡Œçš„â€œå˜æ¢å™¨â€ç‰¹æŒ‡Scikit-learnä¸Šä¸‹æ–‡ã€‚å®ƒä¸åº”ä¸è¿‘å¹´æ¥å¤‡å—å…³æ³¨çš„ç¥ç»ç½‘ç»œæ¶æ„ä¸­çš„å˜æ¢å™¨æ··æ·†ã€‚
- en: In short, what a transformer does is transform/manipulate the predictors (`X`)
    in some ways such that it is ready to be consumed by machine learning algorithms.
    This could be scaling of continuous predictors using prominent tools such as `StandardScaler`
    and `MinMaxScaler`, or encoding categorical predictors using `OneHotEncoder` or
    `OrdinalEncoder`.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: ç®€è€Œè¨€ä¹‹ï¼Œå˜æ¢å™¨çš„ä½œç”¨æ˜¯ä»¥æŸç§æ–¹å¼è½¬æ¢/å¤„ç†é¢„æµ‹å˜é‡ï¼ˆ`X`ï¼‰ï¼Œä½¿å…¶å¯ä»¥è¢«æœºå™¨å­¦ä¹ ç®—æ³•ä½¿ç”¨ã€‚è¿™å¯èƒ½æ˜¯ä½¿ç”¨åƒ`StandardScaler`å’Œ`MinMaxScaler`è¿™æ ·çš„æ˜¾è‘—å·¥å…·å¯¹è¿ç»­é¢„æµ‹å˜é‡è¿›è¡Œç¼©æ”¾ï¼Œæˆ–è€…ä½¿ç”¨`OneHotEncoder`æˆ–`OrdinalEncoder`å¯¹åˆ†ç±»é¢„æµ‹å˜é‡è¿›è¡Œç¼–ç ã€‚
- en: Going a step further, a transformer has a fit-transform mechanism, where it
    learns from the training data using the `fit` method and then applies the learned
    transformations to both the training and test data using the `transform` method.
    This ensures that the same transformations are consistently applied throughout.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: æ›´è¿›ä¸€æ­¥ï¼Œå˜æ¢å™¨å…·æœ‰fit-transformæœºåˆ¶ï¼Œå…¶ä¸­å®ƒä½¿ç”¨`fit`æ–¹æ³•ä»è®­ç»ƒæ•°æ®ä¸­å­¦ä¹ ï¼Œç„¶åä½¿ç”¨`transform`æ–¹æ³•å°†å­¦ä¹ åˆ°çš„è½¬æ¢åº”ç”¨äºè®­ç»ƒæ•°æ®å’Œæµ‹è¯•æ•°æ®ã€‚è¿™ç¡®ä¿äº†æ•´ä¸ªè¿‡ç¨‹ä¸­çš„è½¬æ¢ä¸€è‡´åº”ç”¨ã€‚
- en: Going two steps further, to follow the Scikit-learn API implementation rule,
    a transformer usually inherits from `BaseEstimator` for its `fit` method, and
    `TransformerMixin` for its `transform` method. Letâ€™s peek into the original documentation
    for `StandardScaler` library[Â³](#2e21).
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: å†è¿›ä¸€æ­¥ï¼Œä¸ºäº†éµå¾ªScikit-learn APIå®ç°è§„åˆ™ï¼Œå˜æ¢å™¨é€šå¸¸ä»`BaseEstimator`ç»§æ‰¿å…¶`fit`æ–¹æ³•ï¼Œä»`TransformerMixin`ç»§æ‰¿å…¶`transform`æ–¹æ³•ã€‚æˆ‘ä»¬æ¥çœ‹çœ‹`StandardScaler`åº“çš„åŸå§‹æ–‡æ¡£[Â³](#2e21)ã€‚
- en: '![](../Images/4ade96aaf2b16470dcdafba3f3fa35a3.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4ade96aaf2b16470dcdafba3f3fa35a3.png)'
- en: '[Scikit-learn GitHub](https://github.com/scikit-learn/scikit-learn/blob/364c77e04/sklearn/preprocessing/_data.py#L644)'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '[Scikit-learn GitHub](https://github.com/scikit-learn/scikit-learn/blob/364c77e04/sklearn/preprocessing/_data.py#L644)'
- en: ColumnTransformer[âµ](#ea73)
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ColumnTransformer[âµ](#ea73)
- en: At times, you would need to apply transformations specific to certain columns
    only depending on your needs. For example, applying `OneHotEncoder` to categorical
    features with no specific hierarchy, and `OrdinalEncoder` to categorical features
    with specific hierarchy and ordering (i.e. for t-shirt sizes, we usually have
    sizes ordering to follow such as XS<S<M<L<XL). We can achieve this separation
    using `ColumnTransformer`.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: æœ‰æ—¶ï¼Œä½ å¯èƒ½éœ€è¦æ ¹æ®éœ€æ±‚ä»…å¯¹æŸäº›åˆ—åº”ç”¨ç‰¹å®šçš„å˜æ¢ã€‚ä¾‹å¦‚ï¼Œå¯¹æ²¡æœ‰ç‰¹å®šå±‚æ¬¡ç»“æ„çš„åˆ†ç±»ç‰¹å¾åº”ç”¨`OneHotEncoder`ï¼Œå¯¹å…·æœ‰ç‰¹å®šå±‚æ¬¡ç»“æ„å’Œæ’åºçš„åˆ†ç±»ç‰¹å¾ï¼ˆå³Tæ¤å°ºå¯¸ï¼Œæˆ‘ä»¬é€šå¸¸æœ‰XS<S<M<L<XLçš„æ’åºï¼‰åº”ç”¨`OrdinalEncoder`ã€‚æˆ‘ä»¬å¯ä»¥ä½¿ç”¨`ColumnTransformer`æ¥å®ç°è¿™ç§åˆ†ç¦»ã€‚
- en: '[PRE2]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: As you might expect, we are going to put the variable `col_trans` above as part
    of our big overall Pipeline later on in the code. Simple and elegant.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: æ­£å¦‚ä½ å¯èƒ½é¢„æ–™çš„é‚£æ ·ï¼Œæˆ‘ä»¬å°†æŠŠå˜é‡`col_trans`ä½œä¸ºä»£ç åç»­å¤§æ•´ä½“ç®¡é“çš„ä¸€éƒ¨åˆ†æ”¾åœ¨ä¸Šé¢ã€‚ç®€å•è€Œä¼˜é›…ã€‚
- en: '**Pipeline**'
  id: totrans-48
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**ç®¡é“**'
- en: The `Pipeline[â¶](#8591)` class executes the estimators in the pipe in a sequential
    manner, passing the output of one step as the input to the next. This essentially
    allows the concept of chaining to take place. From the [Scikit-learn documentation](https://scikit-learn.org/stable/developers/develop.html)[â´](#74fc)
    itself, here are the criteria for an estimator to be eligible to be incorporated
    as part of a Pipeline.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '`Pipeline[â¶](#8591)`ç±»ä»¥é¡ºåºæ–¹å¼æ‰§è¡Œç®¡é“ä¸­çš„ä¼°ç®—å™¨ï¼Œå°†ä¸€ä¸ªæ­¥éª¤çš„è¾“å‡ºä½œä¸ºä¸‹ä¸€ä¸ªæ­¥éª¤çš„è¾“å…¥ã€‚è¿™æœ¬è´¨ä¸Šå®ç°äº†é“¾å¼æ“ä½œçš„æ¦‚å¿µã€‚æ ¹æ®[Scikit-learn
    æ–‡æ¡£](https://scikit-learn.org/stable/developers/develop.html)[â´](#74fc)çš„è¯´æ˜ï¼Œä»¥ä¸‹æ˜¯ä¼°ç®—å™¨æœ‰èµ„æ ¼ä½œä¸ºç®¡é“çš„ä¸€éƒ¨åˆ†çš„æ ‡å‡†ã€‚'
- en: For an estimator to be usable together with `pipeline.Pipeline` in any but the
    last step, it needs to provide a `fit` or `fit_transform` function. To be able
    to evaluate the pipeline on any data but the training set, it also needs to provide
    a `transform` function. There are no special requirements for the last step in
    a pipeline, except that it has a `fit` function.
  id: totrans-50
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: è¦ä½¿ä¼°ç®—å™¨èƒ½å¤Ÿä¸`pipeline.Pipeline`ä¸€èµ·ä½¿ç”¨ï¼Œé™¤äº†æœ€åä¸€æ­¥ä¹‹å¤–ï¼Œéœ€è¦æä¾›`fit`æˆ–`fit_transform`å‡½æ•°ã€‚ä¸ºäº†èƒ½å¤Ÿåœ¨è®­ç»ƒé›†ä¹‹å¤–çš„æ•°æ®ä¸Šè¯„ä¼°ç®¡é“ï¼Œå®ƒè¿˜éœ€è¦æä¾›`transform`å‡½æ•°ã€‚ç®¡é“ä¸­çš„æœ€åä¸€æ­¥æ²¡æœ‰ç‰¹åˆ«è¦æ±‚ï¼Œåªéœ€å…·æœ‰`fit`å‡½æ•°ã€‚
- en: Using `Pipeline`, we remove the redundant steps of having to call the method
    `fit` and `transform` on every estimator and/or transformer. Calling the method
    `fit` once directly from the pipeline suffices. How this works behind the scene
    is that it calls `fit` on the first estimator, then`transform` the input and pass
    it on to the next estimator. Indeed, the pipeline is as best as what the last
    estimator can do (it has all the methods of the last estimator in the pipe). If
    the last estimator is a regressor, then the Pipeline can be used as a regressor.
    If the last estimator is a transformer, so is the pipeline.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨`Pipeline`ï¼Œæˆ‘ä»¬å»é™¤äº†åœ¨æ¯ä¸ªä¼°ç®—å™¨å’Œ/æˆ–è½¬æ¢å™¨ä¸Šè°ƒç”¨`fit`å’Œ`transform`æ–¹æ³•çš„å†—ä½™æ­¥éª¤ã€‚ç›´æ¥ä»ç®¡é“è°ƒç”¨ä¸€æ¬¡`fit`æ–¹æ³•å°±è¶³å¤Ÿäº†ã€‚å…¶èƒŒåçš„å·¥ä½œåŸç†æ˜¯ï¼Œé¦–å…ˆåœ¨ç¬¬ä¸€ä¸ªä¼°ç®—å™¨ä¸Šè°ƒç”¨`fit`ï¼Œç„¶å`transform`è¾“å…¥å¹¶ä¼ é€’ç»™ä¸‹ä¸€ä¸ªä¼°ç®—å™¨ã€‚å®é™…ä¸Šï¼Œç®¡é“çš„æ•ˆæœå–å†³äºæœ€åä¸€ä¸ªä¼°ç®—å™¨ï¼ˆå®ƒåŒ…å«äº†ç®¡é“ä¸­æœ€åä¸€ä¸ªä¼°ç®—å™¨çš„æ‰€æœ‰æ–¹æ³•ï¼‰ã€‚å¦‚æœæœ€åä¸€ä¸ªä¼°ç®—å™¨æ˜¯å›å½’å™¨ï¼Œé‚£ä¹ˆç®¡é“ä¹Ÿå¯ä»¥ä½œä¸ºå›å½’å™¨ä½¿ç”¨ã€‚å¦‚æœæœ€åä¸€ä¸ªä¼°ç®—å™¨æ˜¯è½¬æ¢å™¨ï¼Œç®¡é“ä¹Ÿæ˜¯å¦‚æ­¤ã€‚
- en: Below is an illustration of how to use the `Pipeline` class.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: ä»¥ä¸‹æ˜¯å¦‚ä½•ä½¿ç”¨`Pipeline`ç±»çš„ç¤ºä¾‹ã€‚
- en: '[PRE3]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: In short, the argument to `Pipeline` is a list of tuples executed sequentially.
    The first element of the tuple is the arbitrary name you set as per your wish
    to identify the estimator, sort of like the ID. Meanwhile, the second element
    is the estimator object. Simple isnâ€™t it? If you are not good with names, Scikit-learn
    provides the shorthand `make_pipeline` method that removes the headache of having
    to come up with names.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: ç®€è€Œè¨€ä¹‹ï¼Œ`Pipeline`çš„å‚æ•°æ˜¯ä¸€ä¸ªé¡ºåºæ‰§è¡Œçš„å…ƒç»„åˆ—è¡¨ã€‚å…ƒç»„çš„ç¬¬ä¸€ä¸ªå…ƒç´ æ˜¯ä½ ä»»æ„è®¾å®šçš„åç§°ï¼Œç”¨æ¥æ ‡è¯†ä¼°ç®—å™¨ï¼Œæœ‰ç‚¹åƒIDã€‚è€Œç¬¬äºŒä¸ªå…ƒç´ æ˜¯ä¼°ç®—å™¨å¯¹è±¡ã€‚ç®€å•å§ï¼Ÿå¦‚æœä½ ä¸æ“…é•¿èµ·åå­—ï¼ŒScikit-learn
    æä¾›äº†ç®€å†™çš„`make_pipeline`æ–¹æ³•ï¼Œçœå»äº†èµ·åå­—çš„éº»çƒ¦ã€‚
- en: '[PRE4]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Custom Estimator
  id: totrans-56
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: è‡ªå®šä¹‰ä¼°ç®—å™¨
- en: So far, methods such as `StandardScaler` and `MinMaxScaler` look good and work
    for many cases. The question is, what if you have your own customized method to
    manipulate and preprocess your dataset for example? Can you still incorporate
    it neatly into the `Pipeline` class? The answer is a resounding yes! There are
    two ways of achieving thisâ€”leveraging on `FunctionTransformer` or writing your
    own custom class.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: åˆ°ç›®å‰ä¸ºæ­¢ï¼Œåƒ`StandardScaler`å’Œ`MinMaxScaler`è¿™æ ·çš„æ–¹æ³•çœ‹èµ·æ¥å¾ˆå¥½ï¼Œå¹¶ä¸”é€‚ç”¨äºè®¸å¤šæƒ…å†µã€‚é—®é¢˜æ˜¯ï¼Œå¦‚æœä½ æœ‰è‡ªå·±å®šåˆ¶çš„æ–¹æ³•æ¥å¤„ç†å’Œé¢„å¤„ç†æ•°æ®é›†ï¼Œå¯ä»¥å°†å…¶æ•´æ´åœ°æ•´åˆåˆ°`Pipeline`ç±»ä¸­å—ï¼Ÿç­”æ¡ˆæ˜¯è‚¯å®šçš„ï¼æœ‰ä¸¤ç§æ–¹æ³•å¯ä»¥å®ç°è¿™ä¸€ç‚¹â€”â€”åˆ©ç”¨`FunctionTransformer`æˆ–ç¼–å†™ä½ è‡ªå·±çš„è‡ªå®šä¹‰ç±»ã€‚
- en: Letâ€™s say you want to do a Box-Cox transformation on part of your dataset.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: æ¯”å¦‚ä½ æƒ³å¯¹æ•°æ®é›†çš„ä¸€éƒ¨åˆ†è¿›è¡ŒBox-Coxå˜æ¢ã€‚
- en: '[PRE5]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The second method is to write your own custom class that inherits from `BaseEstimator`
    and `TransformerMixin` if you are writing a transformer estimator. If you are
    writing an estimator with a classification task for example, then inherit from
    `ClassifierMixin` instead.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: ç¬¬äºŒç§æ–¹æ³•æ˜¯ç¼–å†™ä¸€ä¸ªè‡ªå®šä¹‰ç±»ï¼Œç»§æ‰¿è‡ª`BaseEstimator`å’Œ`TransformerMixin`ï¼Œå¦‚æœä½ ç¼–å†™çš„æ˜¯è½¬æ¢å™¨ä¼°ç®—å™¨ã€‚ä¾‹å¦‚ï¼Œå¦‚æœä½ ç¼–å†™ä¸€ä¸ªåˆ†ç±»ä»»åŠ¡çš„ä¼°ç®—å™¨ï¼Œé‚£ä¹ˆåº”ç»§æ‰¿è‡ª`ClassifierMixin`ã€‚
- en: Letâ€™s say you want to write a class that removes outliers and incorporates it
    into your Pipeline.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: æ¯”å¦‚ä½ æƒ³ç¼–å†™ä¸€ä¸ªç§»é™¤å¼‚å¸¸å€¼çš„ç±»ï¼Œå¹¶å°†å…¶æ•´åˆåˆ°ä½ çš„ç®¡é“ä¸­ã€‚
- en: '[PRE6]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: I want to bring your focus, particularly on the `OutlierRemove` class. Here,
    we have the `fit` method that returns `self` to allow us to continue chaining
    and the`transform` method that does the removal of the outliers. After this, we
    can simply incorporate the class into our `Pipeline` like the following
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ç‰¹åˆ«å¸Œæœ›å°†æ‚¨çš„æ³¨æ„åŠ›é›†ä¸­åœ¨`OutlierRemove`ç±»ä¸Šã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬æœ‰`fit`æ–¹æ³•è¿”å›`self`ä»¥ä¾¿ç»§ç»­é“¾å¼è°ƒç”¨ï¼Œè¿˜æœ‰`transform`æ–¹æ³•è¿›è¡Œå¼‚å¸¸å€¼çš„åˆ é™¤ã€‚ä¹‹åï¼Œæˆ‘ä»¬å¯ä»¥å°†è¯¥ç±»ç®€å•åœ°åˆå¹¶åˆ°æˆ‘ä»¬çš„`Pipeline`ä¸­ï¼Œå¦‚ä¸‹æ‰€ç¤ºã€‚
- en: '[PRE7]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: FeatureUnion
  id: totrans-65
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: FeatureUnion
- en: Here comes the confusing partâ€”`FeatureUnion` serves the same purpose as `Pipeline`,
    but they work quite differently. In `FeatureUnion`, the `fit` and `transform`
    methods are not done sequentially one after the other. Each transformer estimator
    is `fit` independently to the data, and then the `transform` methods are applied
    in parallel. The end results are then combined together. Picture the code below.
    Here, we can run the preprocessing for numerical and categorical predictors in
    parallel using `FeatureUnion[â·](#2758)` as they are independent of one another.
    This results in faster and more efficient operation.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é‡Œæ˜¯ä»¤äººå›°æƒ‘çš„éƒ¨åˆ†â€”â€”`FeatureUnion`çš„ä½œç”¨ä¸`Pipeline`ç›¸åŒï¼Œä½†å®ƒä»¬çš„å·¥ä½œæ–¹å¼å´å¤§ç›¸å¾„åº­ã€‚åœ¨`FeatureUnion`ä¸­ï¼Œ`fit`å’Œ`transform`æ–¹æ³•ä¸æ˜¯ä¸€ä¸ªæ¥ä¸€ä¸ªåœ°æ‰§è¡Œã€‚æ¯ä¸ªè½¬æ¢å™¨ä¼°ç®—å™¨ç‹¬ç«‹åœ°`fit`æ•°æ®ï¼Œç„¶åå¹¶è¡Œåœ°åº”ç”¨`transform`æ–¹æ³•ã€‚æœ€ç»ˆç»“æœè¢«ç»„åˆåœ¨ä¸€èµ·ã€‚æƒ³è±¡ä¸€ä¸‹ä¸‹é¢çš„ä»£ç ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨`FeatureUnion[â·](#2758)`å¹¶è¡Œè¿è¡Œæ•°å€¼å’Œåˆ†ç±»é¢„æµ‹çš„é¢„å¤„ç†ï¼Œå› ä¸ºå®ƒä»¬ç›¸äº’ç‹¬ç«‹ã€‚è¿™å¸¦æ¥äº†æ›´å¿«å’Œæ›´é«˜æ•ˆçš„æ“ä½œã€‚
- en: '[PRE8]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Real-world Dataset Examples: Bank Marketing with Grid Search CV'
  id: totrans-68
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: çœŸå®ä¸–ç•Œæ•°æ®é›†ç¤ºä¾‹ï¼šé“¶è¡Œè¥é”€ä¸ç½‘æ ¼æœç´¢CV
- en: Here, I wish to end off by illustrating the contents above using a real-world
    dataset inspired by a Portuguese Financial Institution. The dataset is available
    on [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/Bank+Marketing)[Â¹](#ed3c)
    for public use with citation.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™é‡Œï¼Œæˆ‘å¸Œæœ›é€šè¿‡ä½¿ç”¨å—è‘¡è„ç‰™é‡‘èæœºæ„å¯å‘çš„çœŸå®æ•°æ®é›†æ¥è¯´æ˜ä¸Šè¿°å†…å®¹ã€‚è¯¥æ•°æ®é›†å¯ä»¥åœ¨[UCIæœºå™¨å­¦ä¹ åº“](https://archive.ics.uci.edu/ml/datasets/Bank+Marketing)[Â¹](#ed3c)ä¸Šè·å–ï¼Œä¾›å…¬ä¼—ä½¿ç”¨å¹¶å¼•ç”¨ã€‚
- en: Allow me to skip all the exploratory data analysis and visualization, and zoom
    straight into the modeling of the Pipeline.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: è¯·å…è®¸æˆ‘è·³è¿‡æ‰€æœ‰çš„æ¢ç´¢æ€§æ•°æ®åˆ†æå’Œå¯è§†åŒ–ï¼Œç›´æ¥è¿›å…¥ç®¡é“å»ºæ¨¡éƒ¨åˆ†ã€‚
- en: '**1\. Importing the dataset**'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '**1\. å¯¼å…¥æ•°æ®é›†**'
- en: '[PRE9]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'In short, what the code above does are the followings:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: ç®€è€Œè¨€ä¹‹ï¼Œä¸Šé¢çš„ä»£ç å®ç°äº†ä»¥ä¸‹å‡ ä¸ªåŠŸèƒ½ï¼š
- en: Import the dataset with a comma separator
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ä½¿ç”¨é€—å·åˆ†éš”ç¬¦å¯¼å…¥æ•°æ®é›†
- en: Rename the column â€˜yâ€™ to â€˜depositâ€™
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å°†åˆ—â€˜yâ€™é‡å‘½åä¸ºâ€˜depositâ€™
- en: Encode the column deposit from no and yes to 0 and 1
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å°†åˆ—ä¸­çš„â€œnoâ€å’Œâ€œyesâ€ç¼–ç ä¸º0å’Œ1
- en: '**2\. Train-Test Split**'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '**2\. è®­ç»ƒ-æµ‹è¯•æ‹†åˆ†**'
- en: '[PRE10]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '**3\. Writing additional 3 custom classes**'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '**3\. ç¼–å†™å¦å¤–3ä¸ªè‡ªå®šä¹‰ç±»**'
- en: '[PRE11]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'In short, what the code above does are the followings:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: ç®€è€Œè¨€ä¹‹ï¼Œä¸Šé¢çš„ä»£ç å®ç°äº†ä»¥ä¸‹å‡ ä¸ªåŠŸèƒ½ï¼š
- en: The class `ClfSwitcher` inherits from `BaseEstimator`. This class serves the
    purpose of switching between classifiers easily. We set the default classifier
    to be XGBoost Classifier.
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ç±»`ClfSwitcher`ç»§æ‰¿è‡ª`BaseEstimator`ã€‚æ­¤ç±»çš„ç›®çš„æ˜¯æ–¹ä¾¿åœ°åœ¨åˆ†ç±»å™¨ä¹‹é—´åˆ‡æ¢ã€‚æˆ‘ä»¬å°†é»˜è®¤åˆ†ç±»å™¨è®¾ç½®ä¸ºXGBooståˆ†ç±»å™¨ã€‚
- en: The method `outlier_thresholds` and `delete_potential_outlier_list` identify
    outliers in each column and set them to `NaN`. The class `OutlierTrans` is a transformer
    that inherits from both `BaseEstimator` and `TransformerMixin`. The `transform`
    method returns the previously mentioned 2 methods.
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æ–¹æ³•`outlier_thresholds`å’Œ`delete_potential_outlier_list`è¯†åˆ«æ¯åˆ—ä¸­çš„å¼‚å¸¸å€¼å¹¶å°†å…¶è®¾ç½®ä¸º`NaN`ã€‚ç±»`OutlierTrans`æ˜¯ä¸€ä¸ªç»§æ‰¿è‡ª`BaseEstimator`å’Œ`TransformerMixin`çš„è½¬æ¢å™¨ã€‚`transform`æ–¹æ³•è¿”å›ä¹‹å‰æåˆ°çš„ä¸¤ä¸ªæ–¹æ³•ã€‚
- en: The class `TweakBankMarketing` is a custom class to do custom transformations
    such as creating new columns, dropping undesirable columns, and changing data
    types accordingly.
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ç±»`TweakBankMarketing`æ˜¯ä¸€ä¸ªè‡ªå®šä¹‰ç±»ï¼Œç”¨äºæ‰§è¡Œè‡ªå®šä¹‰è½¬æ¢ï¼Œä¾‹å¦‚åˆ›å»ºæ–°åˆ—ã€åˆ é™¤ä¸éœ€è¦çš„åˆ—ä»¥åŠç›¸åº”åœ°æ›´æ”¹æ•°æ®ç±»å‹ã€‚
- en: '**4\. Preparing Pipeline**'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '**4\. å‡†å¤‡ç®¡é“**'
- en: '[PRE12]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'In short, what the code above does are the followings:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: ç®€è€Œè¨€ä¹‹ï¼Œä¸Šé¢çš„ä»£ç å®ç°äº†ä»¥ä¸‹å‡ ä¸ªåŠŸèƒ½ï¼š
- en: Scale numerical columns using `StandardScaler` and `MinMaxScaler`
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ä½¿ç”¨`StandardScaler`å’Œ`MinMaxScaler`å¯¹æ•°å€¼åˆ—è¿›è¡Œç¼©æ”¾
- en: Encode categorical columns using `OneHotEncoder` and `OrdinalEncoder`
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ä½¿ç”¨`OneHotEncoder`å’Œ`OrdinalEncoder`å¯¹åˆ†ç±»åˆ—è¿›è¡Œç¼–ç 
- en: Use `ColumnTransformer` to do the transformations for different columns of the
    datasets separately.
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ä½¿ç”¨`ColumnTransformer`å¯¹æ•°æ®é›†ä¸­çš„ä¸åŒåˆ—åˆ†åˆ«è¿›è¡Œè½¬æ¢ã€‚
- en: Finally, `Pipeline` encapsulates everything seamlessly.
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æœ€ç»ˆï¼Œ`Pipeline`æ— ç¼åœ°å°è£…äº†æ‰€æœ‰å†…å®¹ã€‚
- en: At this stage, this is our constructed Pipeline.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ä¸ªé˜¶æ®µï¼Œè¿™æ˜¯æˆ‘ä»¬æ„å»ºçš„ç®¡é“ã€‚
- en: '![](../Images/b0bd308ca51fa6e1ae7316665e165d05.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b0bd308ca51fa6e1ae7316665e165d05.png)'
- en: Image by Author
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: ä½œè€…å›¾ç‰‡
- en: '**5\. Define hyperparameters for Grid Search CV**'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '**5\. ä¸ºç½‘æ ¼æœç´¢CVå®šä¹‰è¶…å‚æ•°**'
- en: '[PRE13]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'In short, what the code above does are the followings:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: ç®€è€Œè¨€ä¹‹ï¼Œä¸Šè¿°ä»£ç æ‰§è¡Œçš„æ“ä½œå¦‚ä¸‹ï¼š
- en: Define parameter grids for 4 different classifiers, namely, `SGDClassifier`,
    `LogisticRegression`, `RandomForestClassifier`, `XGBClassifier`.
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ä¸º4ç§ä¸åŒçš„åˆ†ç±»å™¨å®šä¹‰å‚æ•°ç½‘æ ¼ï¼Œå³`SGDClassifier`ï¼Œ`LogisticRegression`ï¼Œ`RandomForestClassifier`ï¼Œ`XGBClassifier`ã€‚
- en: '**6\. Perform Grid Search CV**'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '**6\. æ‰§è¡Œç½‘æ ¼æœç´¢CV**'
- en: '[PRE14]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'In short, what the code above does are the followings:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: ç®€è€Œè¨€ä¹‹ï¼Œä¸Šè¿°ä»£ç æ‰§è¡Œçš„æ“ä½œå¦‚ä¸‹ï¼š
- en: Placing our pipeline object as the first argument to the `GridSearchCV` parameter.
  id: totrans-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å°†æˆ‘ä»¬çš„ç®¡é“å¯¹è±¡ä½œä¸º`GridSearchCV`å‚æ•°çš„ç¬¬ä¸€ä¸ªå‚æ•°ã€‚
- en: '**7\. Printing best estimator**'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '**7\. æ‰“å°æœ€ä½³ä¼°è®¡å™¨**'
- en: '[PRE15]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Here, we obtain a validation score of 0.74, with an AUC score of 0.74 as well.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬è·å¾—äº†0.74çš„éªŒè¯åˆ†æ•°ï¼Œä»¥åŠ0.74çš„AUCåˆ†æ•°ã€‚
- en: '**8\. Plot the ROC-AUC curve**'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '**8\. ç»˜åˆ¶ROC-AUCæ›²çº¿**'
- en: '[PRE16]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '![](../Images/704e0e23207a8a15c9b496876cad3d55.png)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/704e0e23207a8a15c9b496876cad3d55.png)'
- en: Image by Author
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ç‰‡æ¥æºï¼šä½œè€…
- en: Afterword
  id: totrans-110
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: åè®°
- en: There you have it! Pipeline with Estimators and Transformers. Next time when
    you approach an ML project, consider using this technique. It may seem difficult
    to adopt at first, but keep practicing and soon you will create robust and efficient
    Machine Learning pipelines.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: å°±æ˜¯è¿™æ ·ï¼ä½¿ç”¨ä¼°è®¡å™¨å’Œè½¬æ¢å™¨çš„ç®¡é“ã€‚ä¸‹æ¬¡å½“ä½ å¤„ç†MLé¡¹ç›®æ—¶ï¼Œè€ƒè™‘ä½¿ç”¨è¿™ä¸ªæŠ€æœ¯ã€‚èµ·åˆå¯èƒ½è§‰å¾—éš¾ä»¥é‡‡ç”¨ï¼Œä½†æŒç»­ç»ƒä¹ ï¼Œå¾ˆå¿«ä½ å°±èƒ½åˆ›å»ºå‡ºç¨³å¥è€Œé«˜æ•ˆçš„æœºå™¨å­¦ä¹ ç®¡é“ã€‚
- en: If you pick up something useful from this article, do consider giving me a [***Follow***](https://medium.com/@andreas030503)
    on Medium. Easy, 1 article a week to keep yourself updated and stay ahead of the
    curve!
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœä½ ä»è¿™ç¯‡æ–‡ç« ä¸­è·å¾—äº†æœ‰ç”¨çš„ä¿¡æ¯ï¼Œè¯·è€ƒè™‘åœ¨Mediumä¸Šç»™æˆ‘ä¸€ä¸ª[***å…³æ³¨***](https://medium.com/@andreas030503)ã€‚ç®€å•ï¼Œæ¯å‘¨ä¸€ç¯‡æ–‡ç« ï¼Œä¿æŒæ›´æ–°å¹¶èµ°åœ¨å‰æ²¿ï¼
- en: Connect With Me!
  id: totrans-113
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å…³æ³¨æˆ‘ï¼
- en: '[*LinkedIn*](https://www.linkedin.com/in/andreaslukita7/)ğŸ‘”'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*LinkedIn*](https://www.linkedin.com/in/andreaslukita7/)ğŸ‘”'
- en: '[*Twitter*](https://twitter.com/andreaslukita7)ğŸ–Š'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*Twitter*](https://twitter.com/andreaslukita7)ğŸ–Š'
- en: References
  id: totrans-116
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å‚è€ƒæ–‡çŒ®
- en: 'Bank Marketing Data Set [Moro et al., 2014] S. Moro, P. Cortez, and P. Rita.
    A Data-Driven Approach to Predict the Success of Bank Telemarketing. Decision
    Support Systems, Elsevier, 62:22â€“31, June 2014: [https://archive.ics.uci.edu/ml/datasets/Bank+Marketing](https://archive.ics.uci.edu/ml/datasets/Bank+Marketing)
    (CC BY 4.0)'
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: é“¶è¡Œè¥é”€æ•°æ®é›† [Moro et al., 2014] S. Moro, P. Cortez, å’Œ P. Ritaã€‚åŸºäºæ•°æ®çš„æ–¹æ³•æ¥é¢„æµ‹é“¶è¡Œç”µè¯è¥é”€çš„æˆåŠŸã€‚å†³ç­–æ”¯æŒç³»ç»Ÿï¼ŒElsevierï¼Œ62:22â€“31ï¼Œ2014å¹´6æœˆï¼š[https://archive.ics.uci.edu/ml/datasets/Bank+Marketing](https://archive.ics.uci.edu/ml/datasets/Bank+Marketing)
    (CC BY 4.0)
- en: 'Scikit-learn Linear Model Logistic: [https://github.com/scikit-learn/scikit-learn/blob/364c77e047ca08a95862becf40a04fe9d4cd2c98/sklearn/linear_model/_logistic.py](https://github.com/scikit-learn/scikit-learn/blob/364c77e047ca08a95862becf40a04fe9d4cd2c98/sklearn/linear_model/_logistic.py)'
  id: totrans-118
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Scikit-learnçº¿æ€§æ¨¡å‹é€»è¾‘å›å½’ï¼š[https://github.com/scikit-learn/scikit-learn/blob/364c77e047ca08a95862becf40a04fe9d4cd2c98/sklearn/linear_model/_logistic.py](https://github.com/scikit-learn/scikit-learn/blob/364c77e047ca08a95862becf40a04fe9d4cd2c98/sklearn/linear_model/_logistic.py)
- en: 'Scikit-learn Preprocessing: [https://github.com/scikit-learn/scikit-learn/blob/364c77e04/sklearn/preprocessing/_data.py#L644](https://github.com/scikit-learn/scikit-learn/blob/364c77e04/sklearn/preprocessing/_data.py#L644)'
  id: totrans-119
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Scikit-learné¢„å¤„ç†ï¼š[https://github.com/scikit-learn/scikit-learn/blob/364c77e04/sklearn/preprocessing/_data.py#L644](https://github.com/scikit-learn/scikit-learn/blob/364c77e04/sklearn/preprocessing/_data.py#L644)
- en: 'Developing Scikit-learn estimators: [https://scikit-learn.org/stable/developers/develop.html](https://scikit-learn.org/stable/developers/develop.html)'
  id: totrans-120
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å¼€å‘Scikit-learnä¼°è®¡å™¨ï¼š[https://scikit-learn.org/stable/developers/develop.html](https://scikit-learn.org/stable/developers/develop.html)
- en: 'Scikit-learn ColumnTransformer: [https://scikit-learn.org/stable/modules/generated/sklearn.compose.ColumnTransformer.html](https://scikit-learn.org/stable/modules/generated/sklearn.compose.ColumnTransformer.html)'
  id: totrans-121
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Scikit-learn ColumnTransformerï¼š[https://scikit-learn.org/stable/modules/generated/sklearn.compose.ColumnTransformer.html](https://scikit-learn.org/stable/modules/generated/sklearn.compose.ColumnTransformer.html)
- en: 'Scikit-learn Pipeline: [https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html)'
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Scikit-learnç®¡é“ï¼š[https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html)
- en: 'Scikit-learn FeatureUnion: [https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.FeatureUnion.html](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.FeatureUnion.html)'
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Scikit-learn FeatureUnionï¼š[https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.FeatureUnion.html](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.FeatureUnion.html)
