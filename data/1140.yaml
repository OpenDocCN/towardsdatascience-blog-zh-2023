- en: 'Behind the Millions: Estimating the Scale of Large Language Models'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 'Behind the Millions: Estimating the Scale of Large Language Models'
- en: 原文：[https://towardsdatascience.com/behind-the-millions-estimating-the-scale-of-large-language-models-97bd7287fb6b?source=collection_archive---------0-----------------------#2023-03-31](https://towardsdatascience.com/behind-the-millions-estimating-the-scale-of-large-language-models-97bd7287fb6b?source=collection_archive---------0-----------------------#2023-03-31)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/behind-the-millions-estimating-the-scale-of-large-language-models-97bd7287fb6b?source=collection_archive---------0-----------------------#2023-03-31](https://towardsdatascience.com/behind-the-millions-estimating-the-scale-of-large-language-models-97bd7287fb6b?source=collection_archive---------0-----------------------#2023-03-31)
- en: Discussing LLMs like ChatGPT, the underlying costs, and inference optimization
    approaches
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Discussing LLMs like ChatGPT, the underlying costs, and inference optimization
    approaches
- en: '[](https://medium.com/@andimid?source=post_page-----97bd7287fb6b--------------------------------)[![Dmytro
    Nikolaiev (Dimid)](../Images/4121156b9c08ed20e7aa620712a391d9.png)](https://medium.com/@andimid?source=post_page-----97bd7287fb6b--------------------------------)[](https://towardsdatascience.com/?source=post_page-----97bd7287fb6b--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----97bd7287fb6b--------------------------------)
    [Dmytro Nikolaiev (Dimid)](https://medium.com/@andimid?source=post_page-----97bd7287fb6b--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@andimid?source=post_page-----97bd7287fb6b--------------------------------)[![Dmytro
    Nikolaiev (Dimid)](../Images/4121156b9c08ed20e7aa620712a391d9.png)](https://medium.com/@andimid?source=post_page-----97bd7287fb6b--------------------------------)[](https://towardsdatascience.com/?source=post_page-----97bd7287fb6b--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----97bd7287fb6b--------------------------------)
    [Dmytro Nikolaiev (Dimid)](https://medium.com/@andimid?source=post_page-----97bd7287fb6b--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F97b5279dad26&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbehind-the-millions-estimating-the-scale-of-large-language-models-97bd7287fb6b&user=Dmytro+Nikolaiev+%28Dimid%29&userId=97b5279dad26&source=post_page-97b5279dad26----97bd7287fb6b---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----97bd7287fb6b--------------------------------)
    ·13 min read·Mar 31, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F97bd7287fb6b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbehind-the-millions-estimating-the-scale-of-large-language-models-97bd7287fb6b&user=Dmytro+Nikolaiev+%28Dimid%29&userId=97b5279dad26&source=-----97bd7287fb6b---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F97b5279dad26&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbehind-the-millions-estimating-the-scale-of-large-language-models-97bd7287fb6b&user=Dmytro+Nikolaiev+%28Dimid%29&userId=97b5279dad26&source=post_page-97b5279dad26----97bd7287fb6b---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----97bd7287fb6b--------------------------------)
    ·13 min read·Mar 31, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F97bd7287fb6b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbehind-the-millions-estimating-the-scale-of-large-language-models-97bd7287fb6b&user=Dmytro+Nikolaiev+%28Dimid%29&userId=97b5279dad26&source=-----97bd7287fb6b---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F97bd7287fb6b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbehind-the-millions-estimating-the-scale-of-large-language-models-97bd7287fb6b&source=-----97bd7287fb6b---------------------bookmark_footer-----------)![](../Images/216ddba9f024c656c8b7a89b5e2e8a50.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F97bd7287fb6b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbehind-the-millions-estimating-the-scale-of-large-language-models-97bd7287fb6b&source=-----97bd7287fb6b---------------------bookmark_footer-----------)![](../Images/216ddba9f024c656c8b7a89b5e2e8a50.png)'
- en: Photo by [Pixabay](https://www.pexels.com/photo/hard-cash-on-a-briefcase-259027/)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: Photo by [Pixabay](https://www.pexels.com/photo/hard-cash-on-a-briefcase-259027/)
- en: Thanks to [Regan Yue](https://medium.com/u/84ea27feb7de?source=post_page-----97bd7287fb6b--------------------------------),
    you can read the Chinese version of this article at [mp.weixin.qq.com](https://mp.weixin.qq.com/s/ek9Z9E-T04BBP7g7rl2Eqw),
    [juejin.cn](https://juejin.cn/post/7225058326575693884), [segmentfault.com](https://segmentfault.com/a/1190000043715456)
    and [xie.infoq.cn](https://xie.infoq.cn/article/e6d832cc0db3203e661647960)!
  id: totrans-9
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 感谢[Regan Yue](https://medium.com/u/84ea27feb7de?source=post_page-----97bd7287fb6b--------------------------------)，你可以在[mp.weixin.qq.com](https://mp.weixin.qq.com/s/ek9Z9E-T04BBP7g7rl2Eqw)、[juejin.cn](https://juejin.cn/post/7225058326575693884)、[segmentfault.com](https://segmentfault.com/a/1190000043715456)和[xie.infoq.cn](https://xie.infoq.cn/article/e6d832cc0db3203e661647960)阅读这篇文章的中文版本！
- en: In the recent past, machine learning was considered a complex, niche technology
    that only a select few could comprehend. However, as ML applications become more
    powerful, the public’s interest has surged, leading to a vast amount of content
    surrounding Artificial Intelligence. The culmination of this happened in [November
    2022, when we saw ChatGPT](https://openai.com/blog/chatgpt), and continued in
    [March 2023 with the release of GPT-4](https://openai.com/research/gpt-4), when
    even the most skeptical person was surprised at what modern neural networks can
    do.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在近期过去，机器学习被视为一种复杂的、小众的技术，只有少数人能够理解。然而，随着机器学习应用变得越来越强大，公众的兴趣激增，围绕人工智能的内容也变得极其丰富。这一高潮发生在[2022年11月，当我们看到ChatGPT](https://openai.com/blog/chatgpt)，并在[2023年3月GPT-4发布时](https://openai.com/research/gpt-4)，即使是最怀疑的人也对现代神经网络的能力感到惊讶。
- en: '![](../Images/c89106d2f2cccd701ed6952812eb5b11.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c89106d2f2cccd701ed6952812eb5b11.png)'
- en: Asking ChatGPT about its capabilities. Image by Author created using [ChatGPT](https://chat.openai.com/chat)
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 询问ChatGPT关于其能力的问题。图片由作者使用[ChatGPT](https://chat.openai.com/chat)创建
- en: While some of this content is undoubtedly valuable, a significant portion of
    it perpetuates **fear** and **misinformation**, such as the *spread of the idea
    that robots will replace all human jobs* or *discovering secret ways to make huge
    sums of money on neural networks*. As a result, it has become increasingly important
    to dispel misconceptions about machine learning and **large language models**
    and provide informative content to help people understand these technologies better.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这些内容中有些无疑是有价值的，但其中大量内容传播了**恐惧**和**错误信息**，例如*传播机器人将取代所有人类工作*或*发现神秘的方法通过神经网络赚取巨额财富*。因此，澄清有关机器学习和**大型语言模型**的误解，并提供有用的信息以帮助人们更好地理解这些技术，变得越来越重要。
- en: This article aims to discuss the crucial aspect of modern machine learning that
    is often overlooked or misunderstood — the cost of training large language models.
    At the same time, we will briefly take a look at what LLM is and some possible
    techniques to optimize its inference. By providing comprehensive examples, I hope
    to convince you that these technologies *do not come out of the air.* By getting
    an idea about the **scale of the data and the underlying calculations** you will
    better understand these powerful tools.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 本文旨在讨论现代机器学习中一个常被忽视或误解的关键方面——训练大型语言模型的成本。同时，我们将简要了解什么是LLM以及一些可能的优化推理技术。通过提供详细的示例，我希望能让你相信这些技术*并非凭空而来*。通过了解**数据的规模和底层计算**，你将更好地理解这些强大的工具。
- en: 'Mostly, I will rely on the recent [LLaMA paper by Meta AI](https://ai.facebook.com/blog/large-language-model-llama-meta-ai/)
    because of its clarity in the sense of the amount of data and compute the team
    used to train these models. The post will be divided into the following sections:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 我主要依赖于最近的[LLaMA论文，由Meta AI发布](https://ai.facebook.com/blog/large-language-model-llama-meta-ai/)，因为它清楚地说明了团队用于训练这些模型的数据量和计算资源。本文将分为以下几个部分：
- en: First, we’ll briefly look at **what modern LLMs are**;
  id: totrans-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们将简要了解**现代大型语言模型是什么**；
- en: Then, we discuss **how much it costs to train such models**;
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接着，我们讨论**训练这些模型的成本**；
- en: In the end, we briefly consider some **popular techniques to optimize language
    models for inference**.
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们将简要考虑一些**优化语言模型推理的流行技术**。
- en: Stay tuned as we delve deeper into the world of large language models and you
    will see that everything is *very simple* and *very complicated* at the same time.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 请继续关注，我们将深入探讨大型语言模型的世界，你会发现一切既*非常简单*又*非常复杂*。
- en: Introduction to Large Language Models
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 大型语言模型简介
- en: Before we explore the costs associated with training Large Language Models (LLMs),
    let’s first briefly define what a language model is.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们深入探讨训练大型语言模型（LLMs）相关的成本之前，让我们先简要定义一下什么是语言模型。
- en: '![](../Images/30e42d2c00e8ab3c7dd6aade7470950b.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/30e42d2c00e8ab3c7dd6aade7470950b.png)'
- en: Parameter counts of several language models released in 2018–2019\. Modern LLMs
    usually have tens to hundreds of billions of parameters. Figure 1 from [DistilBERT
    paper](https://arxiv.org/pdf/1910.01108.pdf)
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 2018-2019年发布的几种语言模型的参数计数。现代LLM通常有数十亿到数百亿的参数。图1来自[DistilBERT论文](https://arxiv.org/pdf/1910.01108.pdf)
- en: 'In simple terms, a language model is a type of machine learning algorithm designed
    to understand or generate human language. Recently, exactly **generative models**
    have become more and more popular — the **GPT model family**developed by OpenAI:
    ChatGPT, GPT-4, etc (stands for *Generative Pre-trained Transformer*, honoring
    the [*Transformer* architecture](https://huggingface.co/course/chapter1/4) on
    which it is based).'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 简单来说，语言模型是一种旨在理解或生成自然语言的机器学习算法。最近，**生成模型**变得越来越受欢迎 —— 由OpenAI开发的**GPT模型系列**：ChatGPT、GPT-4等（代表*生成预训练变换器*，基于[*变换器*架构](https://huggingface.co/course/chapter1/4)）。
- en: Less popular, but still important examples include [GPT-3 (175B)](https://en.wikipedia.org/wiki/GPT-3),
    [BLOOM (176B)](https://bigscience.huggingface.co/blog/bloom), [Gopher (280B)](https://www.deepmind.com/blog/language-modelling-at-scale-gopher-ethical-considerations-and-retrieval),
    [Chinchilla (70B)](https://arxiv.org/abs/2203.15556), and [LLaMA (65B)](https://ai.facebook.com/blog/large-language-model-llama-meta-ai/),
    where *B* refers to *billions* of parameters, although many of these models also
    have smaller versions.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然不太流行，但仍然重要的例子包括[GPT-3 (175B)](https://en.wikipedia.org/wiki/GPT-3)、[BLOOM
    (176B)](https://bigscience.huggingface.co/blog/bloom)、[Gopher (280B)](https://www.deepmind.com/blog/language-modelling-at-scale-gopher-ethical-considerations-and-retrieval)、[Chinchilla
    (70B)](https://arxiv.org/abs/2203.15556)和[LLaMA (65B)](https://ai.facebook.com/blog/large-language-model-llama-meta-ai/)，其中*B*指*十亿*个参数，尽管这些模型中许多也有较小的版本。
- en: Nothing is known about the number of ChatGPT and especially GPT-4 parameters,
    but it looks like these are about the same numbers.
  id: totrans-26
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 关于ChatGPT特别是GPT-4的参数数量一无所知，但看起来这些数量大致相同。
- en: '![](../Images/a2016210338f77ae9153992dbf03dd4d.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a2016210338f77ae9153992dbf03dd4d.png)'
- en: Some of the popular LLMs architectures. Image by Author
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 一些流行的LLM架构。图像由作者提供
- en: 'These models are “trained” using vast amounts of text data, enabling them to
    learn the complex patterns and structures of natural language. However, the task
    they solve during training is very simple: they just predict the next word (or
    *token)* in a sequence.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 这些模型通过使用大量的文本数据进行“训练”，使它们能够学习自然语言的复杂模式和结构。然而，它们在训练过程中解决的任务非常简单：它们只是预测序列中的下一个词（或*标记*）。
- en: 'You may have heard such a model called **autoregressive,** which means it *uses
    its past outputs as input for future predictions* and *generate output step by
    step*. This can be seen, among other things, in the example of ChatGPT:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能听说过这样一种模型叫做**自回归**，这意味着它*使用过去的输出作为未来预测的输入*，并*一步一步生成输出*。这可以在ChatGPT的示例中看到：
- en: '![](../Images/66aef20a3c0122577e3ba5851f4df307.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/66aef20a3c0122577e3ba5851f4df307.png)'
- en: GhatGPT generates a response. Gif by Author created using [ChatGPT](https://chat.openai.com/chat)
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: GhatGPT生成了一个响应。Gif由作者使用[ChatGPT](https://chat.openai.com/chat)创建
- en: You can notice that the model generates the answer *gradually* and *in chunks*
    that are sometimes less than one word. These chunks are called *tokens* and [they
    are very useful in NLP](https://neptune.ai/blog/tokenization-in-nlp), although
    not so important for us now.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以注意到模型生成答案是*逐渐的*和*分块的*，这些块有时少于一个词。这些块称为*标记*，[它们在NLP中非常有用](https://neptune.ai/blog/tokenization-in-nlp)，尽管现在对我们来说不那么重要。
- en: At each time step, the model concatenates the previous output to the current
    input and keeps generating. It does so until it reaches the special *End of Sequence
    (EOS)* token. Omitting the prompt task and taking words as tokens for simplicity,
    the process can be illustrated as follows.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在每个时间步骤，模型将之前的输出连接到当前输入，并继续生成。它一直这样做，直到达到特殊的*序列结束（EOS）*标记。省略提示任务并将单词作为标记，为简便起见，过程可以如下所示。
- en: '![](../Images/daabb0414aba3ccf9732c3402f6831b7.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/daabb0414aba3ccf9732c3402f6831b7.png)'
- en: Illustrating text generation for autoregressive models. Image by Author
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 为自回归模型生成文本的示意图。图像由作者提供
- en: This simple mechanism together with a **hugeamount** of data (*more than any
    person could read in several lifetimes*) allows the model to generate coherent
    and contextually appropriate text, mimicking human-like writing.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 这个简单的机制加上**大量**的数据（*比任何人几辈子能读的还多*）使得模型能够生成连贯且上下文适宜的文本，模拟人类的写作。
- en: Note, that here we are talking about generative models only. Why if there are
    other model families?
  id: totrans-38
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 注意，这里我们仅讨论生成模型。如果还有其他模型家族呢？
- en: ''
  id: totrans-39
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The reason is quite simple — the text generation task is one of the most **difficult**
    to solve and at the same time one of the most **impressive**. [ChatGPT gained
    1 million users in just 5 days](https://twitter.com/gdb/status/1599683104142430208)
    — faster than any other application before, and [continues in the same spirit](https://www.reuters.com/technology/chatgpt-sets-record-fastest-growing-user-base-analyst-note-2023-02-01/).
  id: totrans-40
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原因很简单——文本生成任务是最**困难**的任务之一，同时也是最**令人印象深刻**的任务之一。[ChatGPT在仅仅5天内获得了100万用户](https://twitter.com/gdb/status/1599683104142430208)——比之前任何其他应用都要快，并且[以相同的势头继续发展](https://www.reuters.com/technology/chatgpt-sets-record-fastest-growing-user-base-analyst-note-2023-02-01/)。
- en: ''
  id: totrans-41
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: So-called [**encoders**](https://huggingface.co/course/chapter1/5)(**BERT**
    model family) can be much less exciting, but they can also solve various problems
    with human-level performance and help you with tasks like [text classification](https://paperswithcode.com/task/text-classification)
    or [Named Entity Recognition (NER)](https://paperswithcode.com/task/named-entity-recognition-ner).
  id: totrans-42
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 所谓的[**编码器**](https://huggingface.co/course/chapter1/5)（**BERT**模型家族）可能不那么令人兴奋，但它们也能以人类水平的表现解决各种问题，并帮助你完成诸如[文本分类](https://paperswithcode.com/task/text-classification)或[命名实体识别（NER）](https://paperswithcode.com/task/named-entity-recognition-ner)等任务。
- en: 'I will not provide particular examples of what LLMs can do — the Internet is
    already full of them. The best way to get an idea is to [try ChatGPT yourself](https://chat.openai.com/chat),
    but you can also find plenty of exciting resources like the [Awesome ChatGPT prompts
    repo](https://github.com/f/awesome-chatgpt-prompts). Despite their impressive
    capabilities, current large language models have some limitations. The most popular
    and significant of them include:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 我不会提供LLMs可以做的具体例子——互联网已经充满了这些例子。最好的方法是[亲自尝试ChatGPT](https://chat.openai.com/chat)，但你也可以找到很多令人兴奋的资源，比如[Awesome
    ChatGPT prompts repo](https://github.com/f/awesome-chatgpt-prompts)。尽管其能力令人印象深刻，但当前的大型语言模型仍有一些限制。最流行且显著的包括：
- en: '**Bias and staticity**: Since LLMs are trained on data from various sources,
    they inadvertently learn and reproduce biases present in those sources. They are
    also static in the sense that they cannot adapt to new data or update their knowledge
    in real time without re-training.'
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**偏见和静态性**：由于LLMs是在来自各种来源的数据上训练的，它们不可避免地学习并复制了这些来源中的偏见。它们在某种意义上也具有静态性，即无法适应新数据或实时更新其知识，除非重新训练。'
- en: '**Comprehension and disinformation**: Although LLMs can generate human-like
    text, they may not always fully understand the context of the input. Also, the
    autoregressive way of generating output text does not prohibit the model from
    generating lies or nonsense.'
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**理解和虚假信息**：尽管LLMs可以生成类似人类的文本，但它们可能并不总是完全理解输入的上下文。此外，生成输出文本的自回归方式并不禁止模型生成虚假或无意义的信息。'
- en: '**Resource-intensive**: Training LLMs requires substantial computing resources,
    which translates to high costs and energy consumption. This factor can limit the
    accessibility of LLMs for smaller organizations or individual researchers.'
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**资源密集型**：训练LLMs需要大量的计算资源，这转化为高成本和能源消耗。这个因素可能限制了LLMs对较小组织或个人研究人员的可及性。'
- en: These and other drawbacks are active topics for the research community. It is
    worthwhile to note that the field is growing so fast that it is impossible to
    predict what limitations will be overcome in just a few months — but without a
    doubt, new ones will arise.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 这些及其他缺陷是研究社区的活跃话题。值得注意的是，该领域发展如此之快，以至于几个月内无法预测哪些限制会被克服——但毫无疑问，新的限制将会出现。
- en: One possible example is the fact that earlier models simply grew in the number
    of parameters, but now it is considered that it is better to train smaller models
    for a longer time and give them more data. This reduces the model size and the
    cost of the model’s further use during inference.
  id: totrans-48
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 一个可能的例子是早期模型简单地增加了参数数量，但现在认为训练较小的模型更长时间，并提供更多的数据更为有效。这减少了模型的大小及其在推理过程中进一步使用的成本。
- en: ''
  id: totrans-49
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: In that way, the LLaMA release freed the hands of enthusiasts and these models
    were already run locally on [computers](https://mobile.twitter.com/ggerganov/status/1640022482307502085),
    [Raspberry Pi](https://twitter.com/miolini/status/1634982361757790209), and even
    [phones](https://twitter.com/thiteanish/status/1635188333705043969)!
  id: totrans-50
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 通过这种方式，LLaMA的发布解放了爱好者，这些模型已经可以在[计算机](https://mobile.twitter.com/ggerganov/status/1640022482307502085)、[树莓派](https://twitter.com/miolini/status/1634982361757790209)甚至[手机](https://twitter.com/thiteanish/status/1635188333705043969)上本地运行！
- en: Having a big picture of what LLM is, let’s move on to the main section of this
    article — estimating the cost of training large language models.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在了解了LLM的整体概况后，让我们进入本文的主要部分——估算训练大型语言模型的成本。
- en: Estimating the Cost of Machine Learning Models in General and LLMs in Particular
  id: totrans-52
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 估算机器学习模型的一般成本及LLM的特殊成本
- en: 'To estimate the cost of training large language models, it is essential to
    consider three key factors that any machine learning algorithm consists of:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 要估算训练大型语言模型的成本，必须考虑任何机器学习算法包含的三个关键因素：
- en: '**Data**,'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据**，'
- en: '**Compute resources**, and'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**计算资源**，以及'
- en: '**Architecture** (or the algorithm itself).'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**架构**（或算法本身）。'
- en: Let’s delve deeper into each of these aspects to better understand their impact
    on training costs.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们深入探讨这些方面，以更好地理解它们对训练成本的影响。
- en: Data
  id: totrans-58
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据
- en: LLMs require massive amounts of data to learn the patterns and structures of
    natural language. Estimating the cost of data can be challenging since companies
    often use data accumulated over time through their business operations together
    with open-sourced datasets.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: LLM需要大量的数据来学习自然语言的模式和结构。估算数据的成本可能具有挑战性，因为公司通常会使用通过业务操作积累的数据以及开源数据集。
- en: Additionally, data needs to be *cleaned*, *labeled*, *organized*, and *stored*
    efficiently, considering the scale of LLMs. Data management and processing costs
    can add up quickly, especially when factoring in the infrastructure, tools, and
    data engineers required for these tasks.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，数据需要被*清洗*、*标注*、*组织*和*高效存储*，考虑到LLM的规模。数据管理和处理成本可以迅速累积，特别是当考虑到这些任务所需的基础设施、工具和数据工程师时。
- en: To make a particular example, it is known that LLaMA used a training dataset
    containing **1.4 trillion tokens** with a total size of **4.6 terabytes**!
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 以一个具体例子为例，已知LLaMA使用了包含**1.4万亿个标记**的训练数据集，总大小为**4.6TB**！
- en: '![](../Images/76fbf5879560b4e3d2a8131bb616e1eb.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/76fbf5879560b4e3d2a8131bb616e1eb.png)'
- en: Training dataset of LLaMA models. Table 1 from [LLaMA paper](https://arxiv.org/pdf/2302.13971.pdf)
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: LLaMA模型的训练数据集，来源于[LLaMA论文](https://arxiv.org/pdf/2302.13971.pdf)
- en: Smaller models (7B and 13B) were trained on 1T tokens, while larger ones (33B
    and 65B) used the full dataset of 1.4T tokens.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 较小的模型（7B和13B）在1T标记上进行训练，而较大的模型（33B和65B）则使用了1.4T标记的完整数据集。
- en: '![](../Images/343af4446e0e34340c69378c96587c8b.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/343af4446e0e34340c69378c96587c8b.png)'
- en: Training loss over tokens for LLaMA models. Figure 1 from [LLaMA paper](https://arxiv.org/pdf/2302.13971.pdf)
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: LLaMA模型的训练损失图，来源于[LLaMA论文](https://arxiv.org/pdf/2302.13971.pdf)
- en: I think now you understand that no one is overstating when calling these datasets
    **huge** and why it wasn’t technically possible ten years ago. But things are
    even more interesting with computing resources.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 我认为现在你明白了，当人们称这些数据集为**庞大**时，并没有夸大其词，也明白了为什么在十年前技术上无法实现这些。但计算资源的情况更为有趣。
- en: Compute
  id: totrans-68
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 计算
- en: The actual training process accounts for a significant portion of the LLM budget.
    Training large language models is resource-intensive and is done on powerful Graphics
    Processing Units (GPUs), due to significant parallel processing capabilities.
    [NVIDIA releases new GPUs every year](https://www.crn.com/news/components-peripherals/8-big-announcements-at-nvidia-s-gtc-2023-from-generative-ai-services-to-new-gpus),
    the cost of which hits hundreds of thousands of dollars.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 实际的训练过程占据了LLM预算的一大部分。训练大型语言模型是资源密集型的，需要在强大的图形处理单元（GPU）上进行，因为GPU具有显著的并行处理能力。[NVIDIA每年发布新GPU](https://www.crn.com/news/components-peripherals/8-big-announcements-at-nvidia-s-gtc-2023-from-generative-ai-services-to-new-gpus)，其成本高达数十万美元。
- en: The cost of cloud computing services for training these models can be huge and
    reach **several million dollars**, especially considering iterating through various
    configurations.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 训练这些模型的云计算服务成本可能非常高，达到**数百万美元**，尤其是考虑到需要通过各种配置进行迭代。
- en: Returning to the LLaMA paper, the authors report that they train the biggest
    65B model for **21 days on two thousand GPUs with 80 GB of RAM each**.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 回到 LLaMA 论文，作者报告他们用**两千个每个 80 GB RAM 的 GPU 训练最大的 65B 模型 21 天**。
- en: '![](../Images/9fc18370b8aa038ee24375c5a4ac1612.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9fc18370b8aa038ee24375c5a4ac1612.png)'
- en: Amount of computing resources for training the LLaMA model. Image from [LLaMA
    paper](https://arxiv.org/pdf/2302.13971.pdf)
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 训练 LLaMA 模型所需的计算资源。图片来源于 [LLaMA 论文](https://arxiv.org/pdf/2302.13971.pdf)
- en: '[NVIDIA A100 GPU](https://www.nvidia.com/en-us/data-center/a100/) authors used
    is a popular choice for modern neural network training. [Google Could Platform
    offers such GPUs](https://cloud.google.com/compute/gpus-pricing) for **$3.93 per
    hour**.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '[NVIDIA A100 GPU](https://www.nvidia.com/en-us/data-center/a100/) 是现代神经网络训练的热门选择。[Google
    Cloud Platform 提供这种 GPU](https://cloud.google.com/compute/gpus-pricing)，**每小时
    $3.93**。'
- en: '![](../Images/3ff8ab50ec05b9fa428a6f46aad9f892.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3ff8ab50ec05b9fa428a6f46aad9f892.png)'
- en: Price of NVIDIA A100 GPU. Screenshot of a [public GCP pricing page](https://cloud.google.com/compute/gpus-pricing)
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: NVIDIA A100 GPU 的价格。截图来源于 [公共 GCP 定价页面](https://cloud.google.com/compute/gpus-pricing)
- en: 'So let’s do some quick calculations:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 那么让我们做一些快速计算：
- en: 2048 GPUs x $3.93 GPU per hour x 24 hours x 21 days =
  id: totrans-78
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 2048 GPUs x $3.93 GPU 每小时 x 24 小时 x 21 天 =
- en: ''
  id: totrans-79
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 4.05 million dollars
  id: totrans-80
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 405 万美元
- en: Four million dollars is a budget that not every researcher can afford, huh?
    And it is a single run! To give you another example, [this article estimates the
    cost of training GPT-3](https://lambdalabs.com/blog/demystifying-gpt-3), and the
    authors got **355 GPU-years and 4.6 million dollars**.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 四百万美元是并非每个研究人员都能承担的预算，对吧？而且这是单次运行！再举一个例子，[这篇文章估算了训练 GPT-3 的成本](https://lambdalabs.com/blog/demystifying-gpt-3)，作者得出了**355
    GPU 年和 460 万美元**。
- en: You may have heard that “neural networks train very quickly on GPU”, but no
    one says relative to what.
  id: totrans-82
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 你可能听说过“神经网络在 GPU 上训练得非常快”，但没有人说明相对于什么。
- en: ''
  id: totrans-83
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: They’re really training fast taking into account the **enormous** amount of
    calculations, and without these GPUs, they would have been training for decades.
    So yeah, 21 days is pretty fast for LLMs.
  id: totrans-84
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 考虑到**巨大的**计算量，它们的训练速度确实很快，没有这些 GPU，它们可能要训练几十年。因此，21 天对 LLM 来说确实很快。
- en: Architecture (and Infrastructure)
  id: totrans-85
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 架构（和基础设施）
- en: The development of state-of-the-art LLMs also depends on the work of skilled
    researchers and engineers to develop the architecture and configure the training
    process properly. The architecture is the foundation of the model, dictating how
    it learns and generates text.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 最先进的 LLM 的开发还依赖于技术娴熟的研究人员和工程师来开发架构并正确配置训练过程。架构是模型的基础，决定了它如何学习和生成文本。
- en: Expertise in various computer science areas is required for designing, implementing,
    and controlling these architectures. Engineers and researchers responsible for
    publishing and delivering cutting-edge results can command salaries reaching **hundreds
    of thousands of dollars**. It is worth noting that the skill set required for
    LLM development may differ significantly from the skill set of a “classic” machine
    learning engineer.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 设计、实施和控制这些架构需要计算机科学各个领域的专业知识。负责发布和交付前沿成果的工程师和研究人员的薪资可以达到**数十万美元**。值得注意的是，LLM
    开发所需的技能集可能与“经典”机器学习工程师的技能集差异很大。
- en: '![](../Images/dd4a832be476af47f2a09f4ac0602d69.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/dd4a832be476af47f2a09f4ac0602d69.png)'
- en: Machine learning system infrastructure. Figure 1 from [Hidden Technical Debt
    in Machine Learning Systems paper](https://proceedings.neurips.cc/paper_files/paper/2015/file/86df7dcfd896fcaf2674f757a2463eba-Paper.pdf)
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习系统基础设施。图 1 来源于 [Hidden Technical Debt in Machine Learning Systems paper](https://proceedings.neurips.cc/paper_files/paper/2015/file/86df7dcfd896fcaf2674f757a2463eba-Paper.pdf)
- en: I think now you do not doubt that training LLMs is a *very hard* and *resource-intensive*
    engineering problem.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 我认为现在你不会怀疑训练 LLM 是一个*非常困难*和*资源密集型*的工程问题了。
- en: Now let’s briefly discuss some methods for making the process of LLM inference
    more efficient and cost-effective.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们简要讨论一些使 LLM 推理过程更高效、成本更低的方法。
- en: Optimizing Language Models for Inference
  id: totrans-92
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 优化语言模型以进行推理
- en: Do we actually need optimization?
  id: totrans-93
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们真的需要优化吗？
- en: Inference refers to the process of using a trained language model to generate
    predictions or responses, usually as an API or web service. Given the resource-intensive
    nature of LLMs, it is essential to optimize them for efficient inference.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 推理是指使用训练好的语言模型生成预测或回应的过程，通常作为 API 或 web 服务。鉴于 LLM 的资源密集型特性，优化它们以实现高效推理至关重要。
- en: For example, GPT-3 model has 175 billion parameters, which is **700 GB** of
    float32 numbers. Approximately the same amount of memory will be taken up by activations,
    and remember that we are talking about RAM.
  id: totrans-95
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 例如，GPT-3模型有1750亿个参数，占**700 GB**的float32数字。大致相同量的内存也会被激活占用，请记住我们讨论的是RAM。
- en: ''
  id: totrans-96
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: To serve predictions without any optimization technique, we will need 16 A100
    GPUs with 80 GB of memory each!
  id: totrans-97
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 为了进行预测而不使用任何优化技术，我们将需要16个每个有80 GB内存的A100 GPU！
- en: Several popular techniques can help reduce memory requirements and model latency,
    including *model parallelism*, *quantization*, and others.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 一些流行的技术可以帮助减少内存需求和模型延迟，包括*模型并行*、*量化*等。
- en: Model Parallelism
  id: totrans-99
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型并行
- en: '[Parallelism](https://colossalai.org/docs/concepts/paradigms_of_parallelism/)
    is a technique that distributes the computation of a single model across multiple
    GPUs and can be used both during training and inference.'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '[并行性](https://colossalai.org/docs/concepts/paradigms_of_parallelism/)是一种将单个模型的计算分布到多个GPU上的技术，可以在训练和推理过程中使用。'
- en: Splitting the model’s layers or parameters across multiple devices can dramatically
    improve the overall inference speed and is very often used in practice.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 将模型的层或参数拆分到多个设备上可以显著提高整体推理速度，并且在实践中非常常见。
- en: Quantization
  id: totrans-102
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 量化
- en: '[Quantization](https://huggingface.co/docs/optimum/concept_guides/quantization)
    involves reducing the precision of the model’s numerical values (such as weights).
    By converting floating-point numbers to lower-precision integers, quantization
    can result in significant memory savings and faster computation without a substantial
    loss in model performance.'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '[量化](https://huggingface.co/docs/optimum/concept_guides/quantization)涉及减少模型数值值（如权重）的精度。通过将浮点数转换为低精度整数，量化可以在不显著降低模型性能的情况下显著节省内存和加快计算速度。'
- en: The simple idea that arises quite quickly is to use *float16* numbers instead
    of *float32* and reduce the amount of memory by half*.* It turns out that it is
    possible to convert model weights even to *int8* almost without accuracy loss
    due to the fact that they are located close to each other on the number line.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 产生的简单想法是使用*float16*数字代替*float32*，将内存减少一半。事实证明，模型权重甚至可以几乎无准确性损失地转换为*int8*，因为它们在数轴上相互接近。
- en: Other Techniques
  id: totrans-105
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 其他技术
- en: 'Finding ways to optimize LLMs is an active area of research, and other techniques
    include:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 寻找优化LLM的方法是一个活跃的研究领域，其他技术包括：
- en: '[Knowledge distillation](https://neptune.ai/blog/knowledge-distillation) —
    training a smaller *student* model to mimic the behavior of a larger *teacher;*'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[知识蒸馏](https://neptune.ai/blog/knowledge-distillation)——训练一个较小的*学生*模型来模仿较大*教师*模型的行为。'
- en: '[Parameter pruning](https://analyticsindiamag.com/a-beginners-guide-to-neural-network-pruning/)
    — removing redundant or less important parameters from the model to reduce its
    size and computational requirements;'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[参数剪枝](https://analyticsindiamag.com/a-beginners-guide-to-neural-network-pruning/)——从模型中移除冗余或不重要的参数，以减少模型的大小和计算需求；'
- en: And using frameworks like [ORT (ONNX Runtime)](https://onnxruntime.ai/) to optimize
    calculation graphs with techniques like *operator fusion* and *constant folding.*
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 并使用像[ORT (ONNX Runtime)](https://onnxruntime.ai/)这样的框架，通过*算子融合*和*常量折叠*等技术优化计算图。
- en: Overall, optimizing large language models for inference is a critical aspect
    of their deployment. By applying various optimization techniques, developers can
    ensure that their LLMs are not only powerful and accurate but also cost-effective
    and scalable.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，优化大型语言模型以进行推理是其部署的关键方面。通过应用各种优化技术，开发人员可以确保他们的LLM不仅强大和准确，而且具有成本效益和可扩展性。
- en: Why did OpenAI Open Access to ChatGPT?
  id: totrans-111
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为什么OpenAI开放ChatGPT访问？
- en: After all the above, one might wonder why OpenAI decided to open access to ChatGPT,
    given the high costs associated with training and inference. While we cannot be
    certain of the company’s exact motivations, we can analyze the benefits and potential
    strategic reasons behind this decision.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到上述所有因素，人们可能会想知道为什么OpenAI决定开放ChatGPT的访问权，考虑到训练和推理相关的高成本。虽然我们无法确定公司的确切动机，但我们可以分析这个决定背后的好处和潜在的战略原因。
- en: First and foremost, OpenAI has gained *significant popularity* by making state-of-the-art
    LLMs more accessible to the broader public (see [AI revolution is more of a UX
    revolution](https://medium.com/@kozyrkov/whats-different-about-today-s-ai-380569e3b0cd)).
    By demonstrating the practical applications of large language models, the company
    has captured the attention of investors, customers, and the tech community at
    large. Moreover, this allowed OpenAI to **collect huge amounts of feedback and
    data** to improve their models.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，OpenAI通过让最先进的LLM更广泛地为公众所用，从而获得了*显著的知名度*（参见[AI革命更多是用户体验革命](https://medium.com/@kozyrkov/whats-different-about-today-s-ai-380569e3b0cd)）。通过展示大型语言模型的实际应用，该公司吸引了投资者、客户和科技界的广泛关注。此外，这使得OpenAI能够**收集大量反馈和数据**以改进他们的模型。
- en: Secondly, OpenAI’s mission revolves around the creation and advancement of AI.
    By opening access to ChatGPT, the company is arguably moving closer to fulfilling
    its mission and preparing society for unavoidable changes. Providing access to
    powerful AI tools encourages innovation, driving the field of AI research forward.
    This progress can lead to the development of more efficient models, more extensive
    applications, and novel solutions to various challenges. It’s worth noting here
    that the **architecture of ChatGPT and GPT-4 is closed**, but that’s another discussion.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，OpenAI的使命围绕着AI的创建和进步。通过开放ChatGPT的访问，公司可以说在更接近实现其使命和为不可避免的变化做准备。提供强大AI工具的访问促进了创新，推动了AI研究领域的发展。这一进展可能会导致更高效的模型、更广泛的应用和各种挑战的创新解决方案。值得注意的是，**ChatGPT和GPT-4的架构是封闭的**，但这是另一个话题。
- en: While the costs associated with training and maintaining large language models
    are undoubtedly significant, the benefits and strategic advantages that come with
    opening access to these tools can outweigh the expenses for *some* organizations.
    In the case of OpenAI, opening access to ChatGPT has not only increased their
    popularity and proved to be a leader in the AI field, but also allowed them to
    collect more data to train more powerful models. This strategy has allowed them
    to advance their mission and contribute (*in some sense*) to the broader development
    of AI and LLM technologies.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然训练和维护大型语言模型的成本无疑是巨大的，但对某些组织来说，开放这些工具的好处和战略优势可能会超过费用。以OpenAI为例，开放ChatGPT的访问不仅提高了他们的知名度，证明了他们在AI领域的领先地位，还使他们能够收集更多数据来训练更强大的模型。这一战略使他们能够推进使命，并在某种程度上对AI和LLM技术的发展做出贡献。
- en: '![](../Images/80511d030bfdd16042515b6d604833f2.png)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/80511d030bfdd16042515b6d604833f2.png)'
- en: Asking ChatGPT why OpenAI is giving free access to ChatGPT. Image by Author
    created using [ChatGPT](https://chat.openai.com/chat)
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 询问ChatGPT为何OpenAI提供免费访问ChatGPT。图像由作者使用[ChatGPT](https://chat.openai.com/chat)创建
- en: Conclusion
  id: totrans-118
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: As we have seen, the cost of training large language models is influenced by
    various factors, including *not only* expensive computing resources but also big
    data management and the expertise required to develop cutting-edge architectures.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，训练大型语言模型的成本受到各种因素的影响，包括*不仅*是昂贵的计算资源，还有大数据管理以及开发前沿架构所需的专业知识。
- en: Modern LLMs have **billions** of parameters, are trained on **trillions** of
    tokens, and cost **millions** of dollars.
  id: totrans-120
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 现代的LLM拥有**数十亿**个参数，训练数据量达到**万亿**个标记，且花费**数百万**美元。
- en: I hope you now better understand the scale of training and inferencing large
    language models, as well as their limitations and pitfalls.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 我希望你现在能更好地理解训练和推理大型语言模型的规模，以及它们的局限性和陷阱。
- en: The field of NLP has been experiencing its [ImageNet moment](https://thegradient.pub/nlp-imagenet/)
    for several years, and now it’s the turn of **generative models**. The widespread
    application and adoption of generative language models have the potential to *revolutionize
    various industries and aspects of our lives*. While it is difficult to predict
    exactly how these changes will unfold, we can be certain that LLMs will have some
    impact on the world.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 自几年前以来，自然语言处理领域一直在经历其[ImageNet时刻](https://thegradient.pub/nlp-imagenet/)，现在轮到**生成模型**了。生成语言模型的广泛应用和采纳有可能*彻底改变各个行业和我们生活的各个方面*。虽然很难预测这些变化会如何展开，但我们可以确定LLM将对世界产生一些影响。
- en: Personally, I like the recent tendency of training “smarter”, not just “larger”
    models. By exploring more elegant ways to develop and deploy LLMs, we can push
    the boundaries of AI and NLP, opening the door to innovative solutions and a brighter
    future for the field.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 就个人而言，我喜欢最近训练“更聪明”的模型，而不仅仅是“更大”的模型的趋势。通过探索更优雅的方式来开发和部署大型语言模型，我们可以推动人工智能和自然语言处理的边界，为该领域开辟创新解决方案和更加光明的未来。
- en: Resources
  id: totrans-124
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 资源
- en: 'Here are my other articles about LLMs that may be useful to you. I have already
    covered:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是我关于大型语言模型的其他文章，它们可能对你有用。我已经涵盖了：
- en: '[Best Practices for Prompt Engineering](/summarising-best-practices-for-prompt-engineering-c5e86c483af4):
    how to apply prompt engineering techniques to interact with LLMs effectively and
    how to build local LLM-based applications with OpenAI API and Streamlit;'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[提示工程最佳实践](/summarising-best-practices-for-prompt-engineering-c5e86c483af4)：如何应用提示工程技术与大型语言模型有效互动，以及如何使用
    OpenAI API 和 Streamlit 构建本地大型语言模型应用程序；'
- en: '[Using ChatGPT for Debugging](/using-chatgpt-for-efficient-debugging-fc9e065b7856#da94-27cac6b3f550):
    how to use LLMs for debugging and code generation.'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用 ChatGPT 进行调试](/using-chatgpt-for-efficient-debugging-fc9e065b7856#da94-27cac6b3f550)：如何使用大型语言模型进行调试和代码生成。'
- en: 'If you become interested in LLMs and want to learn more about them, here are
    some resources that can help you with that:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你对大型语言模型感兴趣并想了解更多，这里有一些可以帮助你的资源：
- en: '[Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/)
    is a great introduction to Transformer architecture which started the NLP big
    bang by Jay Alammar;'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[插图化 Transformer](https://jalammar.github.io/illustrated-transformer/) 是对 Transformer
    架构的极好介绍，该架构由 Jay Alammar 引发了自然语言处理领域的重大变革；'
- en: '[How GPT-3 Works — Visualizations and Animations](https://jalammar.github.io/how-gpt3-works-visualizations-animations/)
    is a visualization of an autoregressive decoding process by the same author;'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[GPT-3的工作原理——可视化和动画](https://jalammar.github.io/how-gpt3-works-visualizations-animations/)
    是同一作者对自回归解码过程的可视化展示；'
- en: '[GPT in 60 Lines of NumPy](https://jaykmody.com/blog/gpt-from-scratch/) is
    a great article by Jay Mody where the author builds his own simple GPT;'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[用 60 行 NumPy 实现 GPT](https://jaykmody.com/blog/gpt-from-scratch/) 是 Jay Mody
    撰写的一篇精彩文章，作者在其中构建了自己的简单 GPT；'
- en: '[Let’s build GPT: from scratch, in code, spelled out](https://www.youtube.com/watch?v=kCc8FmEb1nY)
    is an awesome video by famous Andrej Karpathy, who worked on [Tesla Autopilot](https://www.tesla.com/autopilot)
    as Sr. Director of AI at Tesla;'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[让我们构建 GPT：从头开始，用代码详细说明](https://www.youtube.com/watch?v=kCc8FmEb1nY) 是著名的
    Andrej Karpathy 制作的精彩视频，他曾在 [特斯拉自动驾驶](https://www.tesla.com/autopilot)担任 AI 高级总监；'
- en: To get a more in-depth idea of the field, check out the [Awesome-LLM GitHub
    repo](https://github.com/Hannibal046/Awesome-LLM) for a more detailed list of
    resources. Take a look at [Chinchilla](https://arxiv.org/abs/2203.15556) and [LLaMA](https://arxiv.org/abs/2302.13971)
    as one of the most influential papers of recent times.
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 要深入了解该领域，请查看 [Awesome-LLM GitHub 仓库](https://github.com/Hannibal046/Awesome-LLM)
    以获取更详细的资源列表。可以查看 [Chinchilla](https://arxiv.org/abs/2203.15556) 和 [LLaMA](https://arxiv.org/abs/2302.13971)
    作为近年来最具影响力的论文之一。
- en: Thank you for reading!
  id: totrans-134
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 感谢阅读！
- en: I hope these materials were useful to you. [Follow me on Medium](https://medium.com/@andimid)
    to get more articles like this.
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 希望这些资料对你有帮助。[在 Medium 上关注我](https://medium.com/@andimid)以获取更多类似的文章。
- en: If you have any questions or comments, I will be glad to get any feedback. Ask
    me in the comments, or connect via [LinkedIn](https://www.linkedin.com/in/andimid/)
    or [Twitter](https://twitter.com/dimid_ml).
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果你有任何问题或评论，我很乐意收到任何反馈。可以在评论中问我，或通过 [LinkedIn](https://www.linkedin.com/in/andimid/)
    或 [Twitter](https://twitter.com/dimid_ml) 与我联系。
- en: To support me as a writer and to get access to thousands of other Medium articles,
    get Medium membership using [my referral link](https://medium.com/@andimid/membership)
    (no extra charge for you).
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 支持我作为作者并访问成千上万的其他 Medium 文章，请通过 [我的推荐链接](https://medium.com/@andimid/membership)
    获取 Medium 会员（对你没有额外费用）。
