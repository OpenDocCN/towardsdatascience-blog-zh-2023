- en: 'How GenAI Solutions Revolutionize Business Automation: Unpacking LLM Applications
    for Executives'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何通过GenAI解决方案彻底改变商业自动化：解读LLM应用的高级管理层
- en: 原文：[https://towardsdatascience.com/how-genai-solutions-revolutionize-business-automation-57747b0f11ce?source=collection_archive---------3-----------------------#2023-09-06](https://towardsdatascience.com/how-genai-solutions-revolutionize-business-automation-57747b0f11ce?source=collection_archive---------3-----------------------#2023-09-06)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/how-genai-solutions-revolutionize-business-automation-57747b0f11ce?source=collection_archive---------3-----------------------#2023-09-06](https://towardsdatascience.com/how-genai-solutions-revolutionize-business-automation-57747b0f11ce?source=collection_archive---------3-----------------------#2023-09-06)
- en: How companies can apply the power of LLMs to automate workflows and gain cost
    efficiencies
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 公司如何利用大型语言模型（LLMs）的力量来自动化工作流程并提高成本效率
- en: '[](https://medium.com/@ninadsohoni?source=post_page-----57747b0f11ce--------------------------------)[![Ninad
    Sohoni](../Images/8d6ec40665bb85fb7b4ece99e6a40913.png)](https://medium.com/@ninadsohoni?source=post_page-----57747b0f11ce--------------------------------)[](https://towardsdatascience.com/?source=post_page-----57747b0f11ce--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----57747b0f11ce--------------------------------)
    [Ninad Sohoni](https://medium.com/@ninadsohoni?source=post_page-----57747b0f11ce--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@ninadsohoni?source=post_page-----57747b0f11ce--------------------------------)[![Ninad
    Sohoni](../Images/8d6ec40665bb85fb7b4ece99e6a40913.png)](https://medium.com/@ninadsohoni?source=post_page-----57747b0f11ce--------------------------------)[](https://towardsdatascience.com/?source=post_page-----57747b0f11ce--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----57747b0f11ce--------------------------------)
    [Ninad Sohoni](https://medium.com/@ninadsohoni?source=post_page-----57747b0f11ce--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F5ee93978501b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-genai-solutions-revolutionize-business-automation-57747b0f11ce&user=Ninad+Sohoni&userId=5ee93978501b&source=post_page-5ee93978501b----57747b0f11ce---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----57747b0f11ce--------------------------------)
    ·11 min read·Sep 6, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F57747b0f11ce&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-genai-solutions-revolutionize-business-automation-57747b0f11ce&user=Ninad+Sohoni&userId=5ee93978501b&source=-----57747b0f11ce---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F5ee93978501b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-genai-solutions-revolutionize-business-automation-57747b0f11ce&user=Ninad+Sohoni&userId=5ee93978501b&source=post_page-5ee93978501b----57747b0f11ce---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----57747b0f11ce--------------------------------)
    · 11 分钟阅读 · 2023年9月6日'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F57747b0f11ce&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-genai-solutions-revolutionize-business-automation-57747b0f11ce&source=-----57747b0f11ce---------------------bookmark_footer-----------)![](../Images/4477eaceddb851489dddad62b6f73539.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F57747b0f11ce&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-genai-solutions-revolutionize-business-automation-57747b0f11ce&source=-----57747b0f11ce---------------------bookmark_footer-----------)![](../Images/4477eaceddb851489dddad62b6f73539.png)'
- en: Photo by [Gerard Siderius](https://unsplash.com/@siderius_creativ?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 照片由 [Gerard Siderius](https://unsplash.com/@siderius_creativ?utm_source=medium&utm_medium=referral)
    提供，来源于 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: Introduction
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: In a recent collaboration with manufacturing executives at a biopharma company,
    we delved into the world of generative AI, specifically large language models
    (LLMs), to explore how they may be used to expedite quality investigations. Quality
    investigations are triggered when deviations are identified in manufacturing or
    testing of products. Due to potential risks to patient health and regulatory requirements,
    the batch is put on hold, and depending on impact, even production might be suspended.
    Accelerating investigations to carry out a root cause analysis, and implement
    a corrective and preventive action plan as quickly as possible is paramount. Our
    objective was to accelerate this process using GenAI, wherever possible.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在最近与一家生物制药公司制造高管的合作中，我们深入探讨了生成性 AI 的世界，特别是大型语言模型（LLMs），以探索它们如何用于加速质量调查。质量调查在产品制造或测试中发现偏差时触发。由于潜在的患者健康风险和监管要求，批次被暂停，根据影响，生产甚至可能会被暂停。加速调查以进行根本原因分析，并尽快实施纠正和预防措施计划至关重要。我们的目标是尽可能利用
    GenAI 加速这一过程。
- en: As we started thinking about the minimum viable product (MVP), we faced several
    options regarding how GenAI could automate different stages of the process to
    improve cycle time and unfreeze batches as quickly as possible. The executives
    were experts in their field and had undergone GenAI trainings. However, it was
    necessary to delve deeper into LLM capabilities and various GenAI solution patterns
    to determine what stage of the quality investigation process to prioritize for
    the MVP, balancing short-term solution feasibility and expected improvement in
    cycle time.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们开始思考最低可行产品（MVP）时，面临了几个选项，关于 GenAI 如何自动化流程的不同阶段，以提高周期时间并尽快解冻批次。高管们是各自领域的专家，并且接受了
    GenAI 培训。然而，有必要深入了解 LLM 的能力和各种 GenAI 解决方案模式，以确定在质量调查过程中优先考虑哪个阶段作为 MVP，在短期解决方案可行性和预期周期时间改进之间取得平衡。
- en: While in our case, the discussion focused on a specific process, the same solution
    pattern is being leveraged across industries and functions to extract cost efficiencies
    and accelerate outcomes. So, how can GenAI solutions help with such a process?
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管在我们的案例中，讨论集中在一个特定的过程上，但相同的解决方案模式正被各行业和职能利用，以提取成本效率并加速成果。那么，GenAI 解决方案如何帮助这样的过程呢？
- en: The Unique Capabilities of LLMs
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LLMs 的独特能力
- en: 'Before the recent surge in GenAI’s popularity, automation solutions in the
    corporate world primarily targeted routine, rule-based tasks or relied on Robotic
    Process Automation (RPA). Machine learning applications primarily revolved around
    analytics, such as using regression models to predict outcomes like sales volumes.
    However, the latest LLMs stand out due to their remarkable properties:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在最近 GenAI 人气激增之前，企业界的自动化解决方案主要针对常规的、基于规则的任务，或依赖于机器人流程自动化（RPA）。机器学习应用主要围绕分析展开，例如使用回归模型预测销售量等结果。然而，最新的
    LLMs 由于其卓越的特性而脱颖而出：
- en: '**Content Understanding:** LLMs are able to “understand” the meaning of text'
  id: totrans-15
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**内容理解：** LLMs 能够“理解”文本的含义'
- en: '**On-the-fly Training:** LLMs are able to perform new tasks that were not a
    part of their original training (aka zero-shot learning) guided by natural language
    instructions and optionally a few examples (few-shot learning)'
  id: totrans-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**即时训练：** LLMs 能够执行其原始训练中未涉及的新任务（即零样本学习），通过自然语言指令和可选的少量示例（少样本学习）'
- en: '**Reasoning:** LLMs are able to “think” and “reason out” potential actions
    to a degree (though with some limitations and risks)'
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**推理：** LLMs 能够在一定程度上“思考”和“推理”潜在的行动（尽管存在一些限制和风险）'
- en: In “conventional” machine learning, the process to build and use a model generally
    involved gathering data, defining a ‘target’ manually, and training the model
    to predict the ‘target’ given other properties. So, the models could perform one
    specific task or answer one specific type of question. On the contrary, you could
    just ask a pre-trained LLM to evaluate a customer review on specific aspects important
    to your business that neither the LLM has seen before nor are explicitly mentioned
    in the review.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在“传统”机器学习中，构建和使用模型的过程通常涉及收集数据、手动定义“目标”，并训练模型以预测给定其他属性的“目标”。因此，模型可以执行一个特定的任务或回答一个特定类型的问题。相反，你可以要求一个经过预训练的
    LLM 评估客户评论中对你的业务重要的特定方面，这些方面是 LLM 从未见过的，也没有在评论中明确提到。
- en: The Mechanics of LLM-based Solutions
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LLM-based 解决方案的机制
- en: Many LLM solutions in the industry center around designing and feeding detailed
    instructions to have LLMs perform specific tasks (this is known as prompt-engineering).
    A potent way to amplify LLM impact is by allowing them access to a company’s proprietary
    information in an automated manner. Retrieval Augmented Generation (RAG) has emerged
    as one of the most common solution patterns to achieve this.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 行业内的许多LLM解决方案集中于设计和提供详细的指令，以使LLM执行特定任务（这被称为提示工程）。一种增强LLM影响力的有效方式是通过自动化方式使其访问公司的专有信息。检索增强生成（RAG）已成为实现这一目标的最常见解决方案模式之一。
- en: Overview — A 10,000 ft view
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 概述 — 10,000 英尺视角
- en: '![](../Images/952e9174e519007bdbe8bec637105a73.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/952e9174e519007bdbe8bec637105a73.png)'
- en: 'A High-Level Overview of GenAI Solution Workflow (source: Image by the author)'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: GenAI 解决方案工作流的高级概览（来源：作者提供的图像）
- en: 'In simple terms, the solution has two stages:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，解决方案有两个阶段：
- en: '**Search:** Retrieve company data relevant to the user’s request. For example,
    if the ask is to write a report in a particular format or style, a previous report’s
    text is fetched and sent to the LLM as an example'
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**搜索：** 检索与用户请求相关的公司数据。例如，如果要求以特定格式或风格撰写报告，则会提取并将以前报告的文本发送给LLM作为示例。'
- en: '**Generate:** Compile instructions and examples (or any other relevant information)
    retrieved in the previous stage into a text prompt and send it to the LLM to generate
    the desired output. In the case of the report, the prompt could be,'
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**生成：** 将在先前阶段检索到的指令和示例（或任何其他相关信息）编译成文本提示，并将其发送给LLM以生成所需的输出。以报告为例，提示可以是，'
- en: Please compile the following information into a report, using the format and
    style of the provided example.
  id: totrans-27
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 请将以下信息编写成报告，使用提供示例的格式和风格。
- en: 'Here is the content: [report content…. ].'
  id: totrans-28
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 这里是内容：[报告内容…. ]。
- en: 'Here is the example: [ Previous Report Title'
  id: totrans-29
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 这是示例：[之前的报告标题
- en: Section 1 …
  id: totrans-30
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 第 1 部分 …
- en: Section 2 …
  id: totrans-31
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 第 2 部分 …
- en: Conclusion ]
  id: totrans-32
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 结论
- en: The RAG Workflow — 1,000 ft view
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: RAG 工作流 — 1,000 英尺视角
- en: Let’s dive a little deeper into both the Search and Generate stage of the solution
    pattern.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们深入了解解决方案模式中的搜索和生成两个阶段。
- en: '![](../Images/a7f6437d4036bbda8828c4ddc05ce62e.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a7f6437d4036bbda8828c4ddc05ce62e.png)'
- en: 'A Closer Look at the Retrieval-Augmented Generation (RAG) Solution Workflow
    (source: Image by the author)'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 检索增强生成（RAG）解决方案工作流的详细观察（来源：作者提供的图像）
- en: '**1\. Word Embeddings — the basis for language “understanding”:** To facilitate
    natural language understanding, text is run through algorithms or through LLMs
    to get a numerical representation (known as vector embeddings) that captures the
    meaning and context of data. The length is determined by the model used to create
    this representation — some models such as word2vec have vector lengths of up to
    300, whereas GPT-3 uses vectors of up to a length of 12,288\. For example,'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '**1\. 词嵌入 — 语言“理解”的基础：** 为了促进自然语言理解，文本通过算法或LLM运行，以获得捕捉数据含义和上下文的数值表示（称为向量嵌入）。表示的长度由用于创建此表示的模型决定——一些模型如
    word2vec 的向量长度可达 300，而 GPT-3 使用的向量长度可达 12,288。例如，'
- en: '![](../Images/038a551a1fdbd0091d7776977a517b61.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/038a551a1fdbd0091d7776977a517b61.png)'
- en: 'Vector Embeddings (i.e.numerical representations) of sample sentences using
    [Cohere’s embed-english-v2.0 model](https://docs.cohere.com/docs/models) (source:
    Image by the author)'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 使用[Cohere 的 embed-english-v2.0 模型](https://docs.cohere.com/docs/models)的示例句子的向量嵌入（即数值表示）（来源：作者提供的图像）
- en: Here’s how these numerical representations map to a two-dimensional space. It
    is interesting to see that sentences about similar topics are mapped close to
    each other.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 这些数值表示如何映射到二维空间中是很有趣的。可以看到，相似主题的句子被映射得很近。
- en: '![](../Images/ccfd9af9efabd2f81631abce4f794222.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ccfd9af9efabd2f81631abce4f794222.png)'
- en: 'Sample Sentences Vector Embeddings Plotted in a Two-Dimensional Space (source:
    Image by the author, inspired by a demonstration in the course [Large Language
    Models With Semantic Search](https://www.deeplearning.ai/short-courses/large-language-models-semantic-search/)
    at [Deeplearning.ai](http://deeplearning.ai/))'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 示例句子的向量嵌入绘制在二维空间中（来源：作者提供的图像，灵感来自课程[具有语义搜索的大型语言模型](https://www.deeplearning.ai/short-courses/large-language-models-semantic-search/)中的演示，课程由[Deeplearning.ai](http://deeplearning.ai/)提供）
- en: '**2\. Creating** **the Knowledge Corpus:** The search component of the solution
    takes an incoming question and runs a semantic search to find the most similar
    pieces of information available in the knowledge corpus. So, how is this knowledge
    corpus created? Files that need to be in the knowledge corpus are processed by
    an embeddings model, which creates numerical representations. These numerical
    representations are loaded into a specialized database — generally a vector database
    purpose-built for storing this type of information efficiently and retrieving
    it quickly when needed.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '**2\. 创建** **知识库：** 解决方案的搜索组件接收一个问题，并进行语义搜索以寻找知识库中最相似的信息。那么，这个知识库是如何创建的呢？需要放入知识库的文件会由嵌入模型处理，从而创建数值表示。这些数值表示会被加载到一个专门的数据库中——通常是为高效存储和快速检索这种信息而特别设计的向量数据库。'
- en: '**3\. Searching Similar Information in Knowledge Corpus (i.e. Retrieval):**
    When a user submits a question or a task to the solution, the solution uses an
    embeddings model to convert the question text into a numerical representation.
    This question vector is run against the knowledge corpus to find the most similar
    pieces of information. There can be one or more search results returned, which
    can be passed on to the next stage to generate a response or output.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '**3\. 在知识库中检索相似信息（即检索）：** 当用户提交问题或任务到解决方案时，解决方案使用嵌入模型将问题文本转换为数值表示。这个问题向量会与知识库进行匹配，以寻找最相似的信息。可能会返回一个或多个搜索结果，这些结果可以传递到下一阶段以生成回应或输出。'
- en: '**4\. Generating Output Using LLM (i.e. Generation):** Now that the solution
    has managed to find relevant pieces of information that can help the LLM generate
    a meaningful output, the entire package, known as a “prompt”, can be sent to the
    LLM. This prompt includes one or more standard sets of instructions that guide
    the LLM, the actual user question, and finally the pieces of information retrieved
    in the Search stage. The resulting output from the LLM can be processed, if necessary
    (for example, load outputs into a specific format in a word document), before
    being delivered back to the user.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '**4\. 使用LLM生成输出（即生成）：** 现在，解决方案已经成功找到可以帮助LLM生成有意义输出的相关信息，整个包，即“提示”，可以发送到LLM。这个提示包括一个或多个标准的指令集，这些指令引导LLM，还有实际的用户问题，最后是检索阶段得到的信息。LLM生成的结果可以在必要时进行处理（例如，将输出加载到特定格式的Word文档中），然后再交付给用户。'
- en: A Deeper Dive into Solution Components
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 深入了解解决方案组件
- en: Let’s dig one step deeper into the components of the solution
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更深入地探讨解决方案的组件
- en: '![](../Images/c294b5492ed5564bacb0da27b11aef34.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c294b5492ed5564bacb0da27b11aef34.png)'
- en: 'Deep Dive into the Components of the RAG Solution Workflow (purple blocks display
    optional components). (source: Image by the author)'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 深入探讨RAG解决方案工作流的组件（紫色块显示可选组件）。(来源：作者提供的图片)
- en: '**1\. Create Knowledge Corpus**: Loading relevant documents into a Knowledge
    Corpus has some nuances and involves several considerations.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '**1\. 创建知识库：** 将相关文档加载到知识库中有一些细节和考虑因素。'
- en: '**Document Loading:** Different relevant documents (pdfs, word, online sources,
    etc.) might need to be imported into a data repository. Depending on the use case,
    only select portions of some documents might be relevant. For example, in a solution
    designed for financial analysts to query company 10-K reports, sections like the
    title page, table of contents, boilerplate compliance information, and some exhibits
    may be irrelevant for financial analysis. Hence, these sections can be omitted
    from the knowledge bank. It’s crucial to avoid redundant information in the knowledge
    bank to ensure diverse and high-quality responses from the LLM model.'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**文档加载：** 可能需要将不同的相关文档（pdf、word、在线资源等）导入到数据存储库中。根据用例，某些文档的只有特定部分可能是相关的。例如，对于为金融分析师设计的查询公司10-K报告的解决方案，标题页、目录、标准合规信息和一些附录可能与财务分析无关。因此，这些部分可以从知识库中省略。避免知识库中的冗余信息至关重要，以确保LLM模型提供多样化和高质量的回应。'
- en: '**Document Splitting:** Once the relevant document sections are identified
    for inclusion in the knowledge bank, the next step is to determine how to divide
    this information and load it into the vector database. The choice can vary depending
    on the use case. An effective approach is to split by paragraphs with some overlap.
    This entails setting a word (or ‘token,’ the units LLMs use for text processing)
    limit for retaining a paragraph as a whole. If a paragraph exceeds this limit,
    it should be divided into multiple records for the vector database. Typically,
    some word overlap is deliberately maintained to preserve context. For instance,
    using a limit of 1,000 words per vector with a 40-word overlap.'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**文档拆分：** 一旦确定了需要包含在知识库中的相关文档部分，下一步是确定如何拆分这些信息并将其加载到向量数据库中。选择方式可能因使用场景而异。一种有效的方法是按段落拆分并留有一定重叠。这涉及到为保留完整段落设置字数（或“令牌”，LLM用于文本处理的单位）限制。如果段落超过此限制，则应将其拆分为多个记录以存储在向量数据库中。通常会故意保留一些词汇重叠以保持上下文。例如，使用每个向量1,000字的限制，并有40字的重叠。'
- en: '**Additional Metadata:** Enhancing the information in the knowledge bank involves
    tagging each record with meaningful metadata. Basic metadata examples include
    the original document title from which the information was extracted and section
    hierarchy. Additional metadata can further enhance the quality of search and retrieval.
    For instance, balance sheet data extracted from a 10-K report could be tagged
    with metadata like:'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**附加元数据：** 增强知识库中的信息涉及用有意义的元数据标记每条记录。基本的元数据示例包括提取信息的原始文档标题和章节层级。附加元数据可以进一步提升搜索和检索的质量。例如，从
    10-K 报告中提取的资产负债表数据可以用以下元数据进行标记：'
- en: 'Original Document Title: Company XYZ 10-K'
  id: totrans-54
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原始文档标题：公司 XYZ 10-K
- en: 'Year: 2022'
  id: totrans-55
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 年份：2022
- en: 'Section: Financial Statements & Supplementary Data | Balance Sheet'
  id: totrans-56
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 部分：财务报表及附加数据 | 资产负债表
- en: '**Storage:** There are various available options for storing information. A
    vector database solution such as chroma, or Faiss on top of Postgres / MySQL can
    be used. However, SQL databases, NoSQL databases or Document stores, Graph databases
    can also be used. Additionally, there are possibilities for in-memory storage
    to reduce latency, as well as horizontal scaling to improve scalability, availability,
    load balancing, etc.'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**存储：** 存储信息的选项有很多。可以使用如 Chroma 或 Faiss 之类的向量数据库解决方案，或在 Postgres / MySQL 上使用这些解决方案。然而，SQL
    数据库、NoSQL 数据库、文档存储、图数据库也可以使用。此外，还可以考虑使用内存存储以减少延迟，以及横向扩展以提高可伸缩性、可用性、负载均衡等。'
- en: '**2\. Search Similar Information from Knowledge Corpus (Retrieval):** For straightforward
    use cases, a retrieval approach based on searching for similar vectors within
    the knowledge bank, as discussed in the previous section, should suffice. A common
    two-stage approach balances search speed with accuracy:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '**2\. 从知识库中检索相似信息：** 对于简单的使用案例，基于在知识库中搜索相似向量的检索方法，如前一部分所述，应该足够。一种常见的两阶段方法可以平衡搜索速度与准确性：'
- en: '**Dense Retrieval:** Initially, a rapid scan of the extensive knowledge corpus
    is conducted using fast approximate nearest neighbor searches for a search query.
    This yields tens or hundreds of results for further evaluation.'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**密集检索：** 最初，通过快速近似邻居搜索对广泛的知识库进行快速扫描，以处理搜索查询。这将产生几十个或几百个结果供进一步评估。'
- en: '**Reranking:** Among the candidates fetched, more compute-intensive algorithms
    can be used to discern between more and less relevant results. Additional relevance
    scores can be calculated either by taking a second pass over the candidates fetched
    by Dense Retrieval stage or by using additional features such as number of links
    pointing to each search result (indicating credibility or topical authority),
    TF-IDF scores, or just asking a LLM to review all candidates and rank them for
    relevance.'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**重新排序：** 在获取的候选项中，可以使用更计算密集的算法来区分更相关和不太相关的结果。可以通过对密集检索阶段获取的候选项进行二次处理，或使用其他特征（如指向每个搜索结果的链接数量（表示可信度或主题权威）、TF-IDF
    分数，或直接请求 LLM 审查所有候选项并对其相关性进行排名）来计算额外的相关性分数。'
- en: For advanced capabilities, such as improving the quality of search results by
    selecting diverse information, applying filters based on plain-language user prompts,
    and more, a more sophisticated approach may be necessary. For example, in a financial
    query where a user asks, “What was the net profit for Company XYZ in 2020?” the
    solution must filter documents for Company XYZ and the year 2020\. One possible
    solution involves using an LLM to split the request into a filtering component
    that narrows down the semantic search targets by filtering on the year 2020 using
    metadata. Then, a semantic search is performed to locate “net profit for Company
    XYZ” within the knowledge bank.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 对于高级功能，例如通过选择多样化的信息、基于自然语言用户提示应用过滤器等来提高搜索结果的质量，可能需要更复杂的方法。例如，在财务查询中，用户问：“XYZ
    公司在 2020 年的净利润是多少？”解决方案必须过滤出关于 XYZ 公司的文件以及 2020 年的数据。一个可能的解决方案是使用 LLM 将请求拆分为过滤组件，通过使用元数据按年份
    2020 进行过滤，从而缩小语义搜索的目标范围。然后，执行语义搜索以在知识库中定位“XYZ 公司的净利润”。
- en: '**3\. Generate Output using LLM (Generation) :** The final step in the process
    involves generating output using an LLM'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '**3\. 使用 LLM 生成输出（生成）：** 过程的最后一步涉及使用 LLM 生成输出。'
- en: '**Direct Approach:** The straightforward approach is to pass all the information
    retrieved from the Search Stage to the LLM, along with the human prompt and instructions.
    However, there is a limitation on how much information can be passed to an LLM.
    For instance, the Azure OpenAI base GPT-4 model has a context size of 1024 tokens,
    approximately equivalent to 2 pages of text. Depending on the use case, workarounds
    for this limitation may be necessary.'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**直接方法：** 直接方法是将从搜索阶段检索到的所有信息连同人工提示和指令传递给 LLM。然而，对于 LLM 能处理的信息量存在限制。例如，Azure
    OpenAI 基础 GPT-4 模型的上下文大小为 1024 个令牌，大约相当于 2 页文本。根据使用情况，可能需要对这一限制进行变通处理。'
- en: '**Chains:** To circumvent the context size limit, one approach is to successively
    provide pieces of information to the language model and instruct it to build and
    refine its answer with each iteration. The LangChain framework offers methods
    like “refine,” “map_reduce,” and “map_rerank” to facilitate the generation of
    multiple parts of answers using a language model and ultimately combine them using
    another LLM call.'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**链式方法：** 为了绕过上下文大小限制，一种方法是逐步向语言模型提供信息，并指示它在每次迭代中构建和完善答案。LangChain 框架提供了如“refine”、“map_reduce”和“map_rerank”等方法，以帮助生成多个答案部分，并最终通过另一个
    LLM 调用将它们组合起来。'
- en: Conclusion
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结论
- en: In an era of escalating data generation, harnessing the power of GenAI, our
    context-aware and trainable assistant, has never been more impactful. This solution
    pattern, as outlined in the article, seamlessly tackles the challenge of automating
    data processing and freeing up human resources for more complex tasks. With the
    increasing commoditization of Large Language Models (LLMs) and the standardization
    of solution components, it’s foreseeable that such solutions will soon become
    ubiquitous.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据生成不断增加的时代，利用 GenAI，我们的上下文感知和可训练助手，比以往任何时候都更具影响力。正如文章中所述，这一解决方案模式无缝地解决了自动化数据处理的挑战，并释放出人力资源以处理更复杂的任务。随着大型语言模型（LLM）的日益商品化和解决方案组件的标准化，可以预见，这些解决方案将很快变得普遍。
- en: This article touched on foundational RAG concepts. The next article is a hands-on
    exploration of these concepts through building a chatbot capable of answering
    questions based on information from any specified website. As they say, the best
    way to learn is by doing!
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 本文涉及了基础 RAG 概念。下一篇文章将通过构建一个能够回答基于任何指定网站信息的问题的聊天机器人来对这些概念进行实际探索。正如所说，最好的学习方式就是实践！
- en: '[](/hands-on-genai-for-product-engineering-leaders-6ee6ad94e058?source=post_page-----57747b0f11ce--------------------------------)
    [## Hands-On GenAI for Product & Engineering Leaders'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/hands-on-genai-for-product-engineering-leaders-6ee6ad94e058?source=post_page-----57747b0f11ce--------------------------------)
    [## GenAI 实操指南：面向产品与工程领导者'
- en: Make better product decisions by taking a peek under the hood of LLM-based products
  id: totrans-69
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 通过了解 LLM 基于产品的内部机制来做出更好的产品决策。
- en: towardsdatascience.com](/hands-on-genai-for-product-engineering-leaders-6ee6ad94e058?source=post_page-----57747b0f11ce--------------------------------)
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/hands-on-genai-for-product-engineering-leaders-6ee6ad94e058?source=post_page-----57747b0f11ce--------------------------------)
- en: Frequently Asked Questions (FAQs)
  id: totrans-71
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 常见问题解答（FAQs）
- en: '*Will the generated content become a part of the LLM’s memory and affect future
    output? For example, will bad outputs generated by less-experienced users affect
    other users’ output quality?*'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*生成的内容会成为 LLM 的记忆并影响未来的输出吗？例如，经验不足的用户生成的糟糕输出会影响其他用户的输出质量吗？*'
- en: No. In this solution approach, the LLM has no “memory” of what it generates
    — each request starts with a clean slate. Unless the LLM is fine-tuned (trained
    further) or the generated output is also added to the knowledge base, the future
    outputs will not be impacted.
  id: totrans-73
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 不会。在这种解决方案方法中，LLM 对其生成的内容没有“记忆”——每个请求都从头开始。除非 LLM 进行进一步的微调（训练）或生成的输出也被添加到知识库中，否则未来的输出不会受到影响。
- en: '*Is the LLM learning and becoming better with use?* Not automatically. The
    RAG solution pattern is not a reinforcement learning system. However, the solution
    can be designed so that users are able to provide feedback on the quality of output,
    which can then be used to fine-tune the model. Updates to the knowledge base or
    using upgraded LLMs can also improve the quality of the solution’s output.'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*LLM 是否会随着使用而学习并变得更好？* 不会自动如此。RAG 解决方案模式并非一种强化学习系统。然而，可以设计解决方案，使用户能够对输出质量提供反馈，从而用以微调模型。更新知识库或使用升级的
    LLM 也可以提高解决方案输出的质量。'
- en: '*Will the vector embeddings be saved in the source data warehouse?* Generally,
    no. While the vectors of document chunks can technically be stored in the source
    data warehouse, the purpose of the source data warehouse and the vector db (or
    for that matter a SQL db explicitly used to store vectors for the solution) are
    different. Adding vectors to the source database will likely create operational
    dependencies and overheads, which may not be necessary or result in any reward.'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*向量嵌入会保存在源数据仓库中吗？* 一般不会。虽然文档切块的向量可以在技术上存储在源数据仓库中，但源数据仓库和向量数据库（或者为了这个解决方案专门用于存储向量的
    SQL 数据库）的目的不同。将向量添加到源数据库可能会创建操作依赖性和额外开销，这可能是不必要的或没有任何奖励的。'
- en: '*How will the solution be updated with new data?* The data loading pipeline
    (identifying documents to load, processing, chunking, vectorization, loading to
    vector db) will need to be run on new data as it becomes available. This can be
    a periodic batch process. The frequency of updates can be tailored to the use
    case.'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*解决方案如何用新数据进行更新？* 数据加载流程（识别文档、处理、切块、向量化、加载到向量数据库）需要在新数据可用时运行。这可以是一个定期批处理过程。更新频率可以根据使用案例进行调整。'
- en: '*How can we ensure that the sensitive information in documents stored in the
    knowledge base isn’t accessible to the public or to LLM vendors?*'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*我们如何确保知识库中存储的文档中的敏感信息不会被公众或 LLM 供应商访问？*'
- en: Enterprises can use Azure OpenAI service as a single-tenant solution with a
    private instance of OpenAI’s LLMs. This can ensure data privacy and security.
    Another solution is to deploy Hugging Face LLMs on the company’s private infrastructure
    so that no data leaves the company’s security perimeter (unlike when using a publicly
    hosted LLM)
  id: totrans-78
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 企业可以使用 Azure OpenAI 服务作为单租户解决方案，配备私有实例的 OpenAI LLM。这可以确保数据隐私和安全。另一种解决方案是将 Hugging
    Face LLM 部署到公司的私有基础设施上，以确保数据不会离开公司的安全边界（与使用公开托管的 LLM 不同）。
- en: Recommended Resources
  id: totrans-79
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 推荐资源
- en: 'Explore these resources to deepen your understanding of LLMs and their applications:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 探索这些资源以深入了解 LLM 及其应用：
- en: '[Generative AI Defined: How It Works, Benefits and Dangers (techrepublic.com)](https://www.techrepublic.com/article/what-is-generative-ai/)'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[生成 AI 定义：如何运作、优势和风险 (techrepublic.com)](https://www.techrepublic.com/article/what-is-generative-ai/)'
- en: '[What Business Leaders Should Know About Using LLMs Like ChatGPT (forbes.com)](https://www.forbes.com/sites/forbesbusinesscouncil/2023/02/07/what-business-leaders-should-know-about-using-llms-like-chatgpt/?sh=4dc731bd514a)'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[商业领袖应了解的使用 LLMs 如 ChatGPT 的信息 (forbes.com)](https://www.forbes.com/sites/forbesbusinesscouncil/2023/02/07/what-business-leaders-should-know-about-using-llms-like-chatgpt/?sh=4dc731bd514a)'
- en: 'For a closer look at the RAG solution pattern:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 想更深入了解 RAG 解决方案模式：
- en: '[Question answering using Retrieval Augmented Generation with foundation models
    in Amazon SageMaker JumpStart | AWS Machine Learning Blog](https://aws.amazon.com/blogs/machine-learning/question-answering-using-retrieval-augmented-generation-with-foundation-models-in-amazon-sagemaker-jumpstart/)'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用检索增强生成的问答与 Amazon SageMaker JumpStart 中的基础模型 | AWS 机器学习博客](https://aws.amazon.com/blogs/machine-learning/question-answering-using-retrieval-augmented-generation-with-foundation-models-in-amazon-sagemaker-jumpstart/)'
- en: '[Deeplearning.ai](http://deeplearning.ai/) course: [Large Language Models With
    Semantic Search](https://www.deeplearning.ai/short-courses/large-language-models-semantic-search/)'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Deeplearning.ai](http://deeplearning.ai/) 课程: [大型语言模型与语义搜索](https://www.deeplearning.ai/short-courses/large-language-models-semantic-search/)'
- en: '[Deeplearning.ai](http://deeplearning.ai/) course: [LangChain: Chat with Your
    Data](https://www.deeplearning.ai/short-courses/langchain-chat-with-your-data/)'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Deeplearning.ai](http://deeplearning.ai/) 课程: [LangChain: 与你的数据对话](https://www.deeplearning.ai/short-courses/langchain-chat-with-your-data/)'
