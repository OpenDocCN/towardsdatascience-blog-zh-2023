- en: Implement interpretable neural models in PyTorch!
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在 PyTorch 中实现可解释的神经模型！
- en: 原文：[https://towardsdatascience.com/implement-interpretable-neural-models-in-pytorch-6a5932bdb078?source=collection_archive---------4-----------------------#2023-05-29](https://towardsdatascience.com/implement-interpretable-neural-models-in-pytorch-6a5932bdb078?source=collection_archive---------4-----------------------#2023-05-29)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/implement-interpretable-neural-models-in-pytorch-6a5932bdb078?source=collection_archive---------4-----------------------#2023-05-29](https://towardsdatascience.com/implement-interpretable-neural-models-in-pytorch-6a5932bdb078?source=collection_archive---------4-----------------------#2023-05-29)
- en: '[](https://medium.com/@pb737?source=post_page-----6a5932bdb078--------------------------------)[![Pietro
    Barbiero](../Images/28a6cfc89e32de0401ec3d732c085410.png)](https://medium.com/@pb737?source=post_page-----6a5932bdb078--------------------------------)[](https://towardsdatascience.com/?source=post_page-----6a5932bdb078--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----6a5932bdb078--------------------------------)
    [Pietro Barbiero](https://medium.com/@pb737?source=post_page-----6a5932bdb078--------------------------------)'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@pb737?source=post_page-----6a5932bdb078--------------------------------)[![Pietro
    Barbiero](../Images/28a6cfc89e32de0401ec3d732c085410.png)](https://medium.com/@pb737?source=post_page-----6a5932bdb078--------------------------------)[](https://towardsdatascience.com/?source=post_page-----6a5932bdb078--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----6a5932bdb078--------------------------------)
    [Pietro Barbiero](https://medium.com/@pb737?source=post_page-----6a5932bdb078--------------------------------)'
- en: ·
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fc3b867b869ca&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fimplement-interpretable-neural-models-in-pytorch-6a5932bdb078&user=Pietro+Barbiero&userId=c3b867b869ca&source=post_page-c3b867b869ca----6a5932bdb078---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----6a5932bdb078--------------------------------)
    ·11 min read·May 29, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F6a5932bdb078&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fimplement-interpretable-neural-models-in-pytorch-6a5932bdb078&user=Pietro+Barbiero&userId=c3b867b869ca&source=-----6a5932bdb078---------------------clap_footer-----------)'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fc3b867b869ca&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fimplement-interpretable-neural-models-in-pytorch-6a5932bdb078&user=Pietro+Barbiero&userId=c3b867b869ca&source=post_page-c3b867b869ca----6a5932bdb078---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----6a5932bdb078--------------------------------)
    · 11 分钟阅读 · 2023年5月29日 [](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F6a5932bdb078&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fimplement-interpretable-neural-models-in-pytorch-6a5932bdb078&user=Pietro+Barbiero&userId=c3b867b869ca&source=-----6a5932bdb078---------------------clap_footer-----------)'
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F6a5932bdb078&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fimplement-interpretable-neural-models-in-pytorch-6a5932bdb078&source=-----6a5932bdb078---------------------bookmark_footer-----------)'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F6a5932bdb078&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fimplement-interpretable-neural-models-in-pytorch-6a5932bdb078&source=-----6a5932bdb078---------------------bookmark_footer-----------)'
- en: '**TL;DR —** Experience the power of interpretability with “[PyTorch, Explain!](https://github.com/pietrobarbiero/pytorch_explain)”
    — a Python library that empowers you to implement state-of-the-art and interpretable
    concept-based models! [[GitHub](https://github.com/pietrobarbiero/pytorch_explain)]'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '**总结 —** 体验可解释性的强大力量，通过 “[PyTorch, Explain!](https://github.com/pietrobarbiero/pytorch_explain)”
    —— 一个能够帮助你实现最先进且可解释的基于概念的模型的 Python 库！ [[GitHub](https://github.com/pietrobarbiero/pytorch_explain)]'
- en: '![](../Images/424bbb13c1b438e116c9cc6b35e31bd1.png)'
  id: totrans-8
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/424bbb13c1b438e116c9cc6b35e31bd1.png)'
- en: Interpretable AI models make predictions for reasons humans can understand.
    Image by the authors.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 可解释的 AI 模型会给出人类能够理解的预测理由。图片由作者提供。
- en: 'Tutorials inspired by methods presented in:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 灵感来源于以下方法的教程：
- en: '***ICML*** 2020 paper “[Concept Bottleneck Models](https://arxiv.org/abs/2007.04612)”;'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***ICML*** 2020 论文 “[概念瓶颈模型](https://arxiv.org/abs/2007.04612)”；'
- en: '***NeurIPS*** 2022 paper “[Concept Embedding Models: Beyond the accuracy-explainability
    trade-off](https://arxiv.org/abs/2209.09056)”;'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***NeurIPS*** 2022 论文 “[概念嵌入模型：超越准确性与可解释性的权衡](https://arxiv.org/abs/2209.09056)”；'
- en: '***ICML*** 2023 paper “[Interpretable Neural-Symbolic Concept Reasoning](https://arxiv.org/abs/2304.14068)”.'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***ICML*** 2023 论文 “[可解释的神经符号概念推理](https://arxiv.org/abs/2304.14068)”。'
- en: Motivation
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 动机
- en: The lack of interpretability in deep learning systems poses a significant challenge
    to establishing human trust. The complexity of these models makes it nearly impossible
    for humans to understand the underlying reasons behind their decisions.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习系统缺乏可解释性对建立人类信任构成了重大挑战。这些模型的复杂性使得人类几乎无法理解其决策背后的原因。
- en: The lack of interpretability in deep learning systems hinders human trust.
  id: totrans-16
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 深度学习系统缺乏可解释性阻碍了人类信任。
- en: To address this issue, researchers have been actively investigating novel solutions,
    leading to significant innovations such as concept-based models. These models
    not only enhance model transparency but also foster a renewed sense of trust in
    the system’s decision-making by incorporating high-level human-interpretable concepts
    (like “colour” or “shape”) in the training process. As a result, these models
    can provide simple and intuitive explanations for their predictions in terms of
    the learnt concepts, allowing humans to **check the reasoning behind their decisions**.
    And that’s not all! They even allow humans to interact with the learnt concepts,
    giving us **control over the final decisions**.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这个问题，研究人员一直在积极探索新颖的解决方案，导致了诸如基于概念的模型这样的重大创新。这些模型不仅提高了模型的透明性，还通过在训练过程中融入高层次的人类可解释概念（如“颜色”或“形状”）来培养对系统决策的新信任感。因此，这些模型可以提供简单直观的预测解释，基于学习到的概念，允许人类**检查决策背后的推理**。而且不仅如此！它们甚至允许人类与学习到的概念互动，赋予我们**对最终决策的控制**。
- en: Concept-based models allow humans to **check the reasoning behind deep learning
    predictions** and give us back **control over the final decision.**
  id: totrans-18
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 基于概念的模型允许人们**检查深度学习预测背后的推理**，并重新获得**对最终决策的控制**。
- en: In this blog post, we will delve into these techniques and provide you with
    the tools to implement state-of-the-art concept-based models using simple PyTorch
    interfaces. Through hands-on experience, you will learn how to leverage these
    powerful models to enhance interpretability and ultimately calibrate human trust
    in your deep learning systems.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇博客文章中，我们将深入探讨这些技术，并为你提供使用简单 PyTorch 接口实现最先进的基于概念的模型的工具。通过实践经验，你将学会如何利用这些强大的模型来增强可解释性，并最终校准人类对深度学习系统的信任。
- en: 'Tutorial #1: Implement your first concept bottleneck model'
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '教程 #1：实现你的第一个概念瓶颈模型'
- en: To showcase the power of PyTorch Explain, let’s dive into our first tutorial!
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 为了展示 PyTorch Explain 的强大功能，让我们深入我们的第一个教程！
- en: A primer on concept bottleneck models
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 概念瓶颈模型简介
- en: 'In this introductory session, we’ll dive into concept bottleneck models. These
    models, introduced in a paper [1] presented at the International Conference on
    Machine Learning in 2020, are designed to first learn and predict a set of concepts,
    such as “colour” or “shape,” and then utilize these concepts to solve a downstream
    classification task:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个介绍性会议中，我们将深入探讨概念瓶颈模型。这些模型在2020年国际机器学习会议上发表的论文 [1] 中首次提出，旨在首先学习和预测一组概念，例如“颜色”或“形状”，然后利用这些概念来解决下游分类任务：
- en: '![](../Images/3266d868c1f5d009d12463ed3c44186d.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3266d868c1f5d009d12463ed3c44186d.png)'
- en: Concept Bottleneck Models learn tasks (**Y**) as a function of concepts (**C**).
    Image by the authors.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 概念瓶颈模型将任务（**Y**）学习为概念（**C**）的函数。图像由作者提供。
- en: By following this approach, we can trace predictions back to concepts providing
    explanations like “The input object is an {apple} because it is {spherical} and
    {red}.”
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 通过遵循这种方法，我们可以将预测追溯到概念，从而提供类似于“输入对象是一个{苹果}，因为它是{球形}和{红色}”的解释。
- en: Concept bottleneck models first learn a set of concepts, such as “colour” or
    “shape,” and then utilize these concepts to solve a downstream classification
    task.
  id: totrans-27
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 概念瓶颈模型首先学习一组概念，例如“颜色”或“形状”，然后利用这些概念来解决下游分类任务。
- en: Hands-on concept bottlenecks
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实践概念瓶颈
- en: To illustrate concept bottleneck models, we will revisit the well-known XOR
    problem, but with a twist. Our input will consist of two continuous features.
    To capture the essence of these features, we will employ a concept encoder that
    maps them into two meaningful concepts, denoted as “A” and “B”. The objective
    of our task is to predict the exclusive OR (XOR) of “A” and “B”. By working through
    this example, you’ll gain a better understanding of how concept bottlenecks can
    be applied in practice and witness their effectiveness in tackling a concrete
    problem.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明概念瓶颈模型，我们将重新审视众所周知的异或（XOR）问题，但有了一点变化。我们的输入将由两个连续的特征组成。为了捕捉这些特征的本质，我们将使用一个概念编码器将它们映射到两个有意义的概念，标记为“A”和“B”。我们的任务目标是预测“A”和“B”的异或（XOR）。通过解决这个例子，你将更好地理解概念瓶颈如何在实践中应用，并见证它们在解决具体问题时的有效性。
- en: 'We can start by importing the necessary libraries and loading this simple dataset:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以先导入必要的库并加载这个简单的数据集：
- en: '[PRE0]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Next, we instantiate a concept encoder to map the input features to the concept
    space and a task predictor to map concepts to task predictions:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们实例化一个概念编码器，将输入特征映射到概念空间，并一个任务预测器，将概念映射到任务预测中：
- en: '[PRE1]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'We then train the network by optimizing the cross-entropy loss on both concepts
    and tasks:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们通过优化交叉熵损失来训练网络，既包括概念又包括任务：
- en: '[PRE2]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'After training the model, we evaluate its performance on the test set:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在对模型进行训练后，我们在测试集上评估其性能：
- en: '[PRE3]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Now, after just a few epochs, we can observe that both the concept and the task
    accuracy are quite good on the test set (~98% accuracy)!
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，经过仅仅几个epochs之后，我们可以观察到概念和任务的准确率在测试集上相当不错（~98% 准确率）！
- en: 'Thanks to this architecture we can provide explanations for a model prediction
    by looking at the response of the task predictor in terms of the input concepts,
    as follows:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 多亏了这种架构，我们可以通过观察任务预测器对输入概念的响应来为模型的预测提供解释，方法如下：
- en: '[PRE4]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: which yields e.g., `f([0,1])=1` and `f([1,1])=0` , as expected. This allows
    us to understand a bit more about the behaviour of the model and check that it
    behaves as expected for any relevant set of concepts e.g., for mutually exclusive
    input concepts `[0,1]`or `[1,0]` it returns a prediction of `y=1`.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 这导致例如，`f([0,1])=1`和`f([1,1])=0`，如预期的那样。这使我们能够更多地理解模型的行为，并检查它是否对任何相关的概念集合（例如，对于互斥的输入概念`[0,1]`或`[1,0]`）都返回`y=1`的预测。
- en: Concept bottleneck models provide **intuitive** explanations by tracing predictions
    back to concepts.
  id: totrans-42
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 概念瓶颈模型通过将预测追溯到概念来提供**直观**的解释。
- en: Drowning in the accuracy-explainability trade-off
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在准确性-可解释性的权衡中陷入困境
- en: One of the key advantages of concept bottleneck models is their ability to provide
    explanations for their predictions by revealing concept-prediction patterns allowing
    humans to assess whether the model’s reasoning aligns with their expectations.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 概念瓶颈模型的一个关键优势之一是它们能够通过揭示概念-预测模式来为他们的预测提供解释，从而使人类能够评估模型的推理是否符合他们的期望。
- en: However, the main issue with standard concept bottleneck models is that they
    struggle in solving complex problems! More generally, they **suffer from a well-known
    issue in explainable AI, referred to as the accuracy-explainability trade-off**.
    Practically, we desire models that not only achieve high task performance but
    also offer high-quality explanations. Unfortunately, in many cases, as we strive
    for higher accuracy, the explanations provided by the models tend to deteriorate
    in quality and faithfulness, and vice versa.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，标准概念瓶颈模型的主要问题在于它们在解决复杂问题时很困难！更一般地说，它们**在可解释人工智能中遇到了一个众所周知的问题，被称为准确性-可解释性的权衡**。实际上，我们希望模型不仅在任务表现上取得高准确性，而且能够提供高质量的解释。然而，很不幸的是，在许多情况下，随着我们追求更高的准确性，模型提供的解释往往会质量下降，且反之亦然。
- en: 'Visually, this trade-off can be represented as follows:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在视觉上，这种权衡可以表示为下图所示：
- en: '![](../Images/ba3b17e1ca59b6c3cabc0f81395f1077.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ba3b17e1ca59b6c3cabc0f81395f1077.png)'
- en: Visual representation of the accuracy-explainability trade-off. The picture
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 准确性-可解释性权衡的视觉表示。图片
- en: shows the difference between interpretable and “black-box” (non-interpretable)
    models
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 展示了可解释和“黑盒”（不可解释）模型之间的差异
- en: 'in terms of two axes: task performance and explanation quality. Image by the
    authors.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 从两个角度来看：任务表现和解释质量。图片作者提供。
- en: where interpretable models excel at providing high-quality explanations but
    struggle with solving challenging tasks, while black-box models achieve high task
    accuracy at the expense of providing brittle and poor explanations.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 可解释模型擅长提供高质量解释，但在解决具有挑战性的任务时表现不佳，而黑盒模型则通过提供脆弱和贫乏的解释来实现高任务准确性。
- en: 'To illustrate this trade-off in a concrete setting, let’s consider a concept
    bottleneck model applied to a slightly more demanding benchmark, the “trigonometry”
    dataset:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在一个具体的情景中说明这种折衷，让我们考虑一个应用于略微更具挑战性基准数据集“三角函数”的概念瓶颈模型：
- en: '[PRE5]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Upon training the same network architecture on this dataset, we observe significantly
    diminished task accuracy, reaching only around 80%.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个数据集上训练相同的网络架构后，我们观察到明显降低的任务准确性，仅达到约80%。
- en: Concept bottleneck models fail to strike a balance between task accuracy and
    explanation quality.
  id: totrans-55
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 概念瓶颈模型未能在任务准确性和解释质量之间取得平衡。
- en: 'This begs the question: are we perpetually forced to choose between accuracy
    and the quality of explanations, or is there a way to strike a better balance?'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 这引出了一个问题：我们是否被永远迫使在准确性和解释质量之间进行选择，还是有更好的平衡方式？
- en: 'Tutorial #2: Beyond the accuracy-explainability trade-off with concept embedding
    models'
  id: totrans-57
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '教程 #2：超越准确性和可解释性的折衷之道：概念嵌入模型'
- en: The answer is “yes!”, a solution does exist!
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 答案是“是的！”，方案确实存在！
- en: A primer on concept embedding models
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 概念嵌入模型简介
- en: 'A recent solution to address this challenge was introduced at the *Advances
    in Neural Information Processing Systems* conference in a paper called “Concept
    Embedding Models: Beyond the accuracy-explainability trade-off” [2] (I discuss
    this method more extensively in [this](/concept-embedding-models-beyond-the-accuracy-explainability-trade-off-f7ba02f28fad)
    blog post if you want to know more!). The key innovation of this paper was to
    design supervised high-dimensional concept representations. Unlike standard concept
    bottleneck models that represent each concept with a single neuron’s activation:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 最近提出的解决这一挑战的方法是在 *神经信息处理系统的进展* 会议上介绍的，一篇名为“概念嵌入模型：超越准确性和可解释性之间的折衷”的论文 [2]（如果你想了解更多，我在[这篇](/concept-embedding-models-beyond-the-accuracy-explainability-trade-off-f7ba02f28fad)博客文章中更详细地讨论了这种方法！）。
    这篇论文的关键创新是设计了受监督的高维概念表示。与用单个神经元激活表示每个概念的标准概念瓶颈模型不同：
- en: '![](../Images/15b64307f135319265cae5970401a4c4.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/15b64307f135319265cae5970401a4c4.png)'
- en: Concept Bottleneck Models learn tasks (**Y**) as a function of concepts (**C**).
    Image by the authors.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 概念瓶颈模型将任务（**Y**）作为概念（**C**）的函数进行学习。图片由作者提供。
- en: '… a concept embedding model represents each concept with a set of neurons,
    effectively overcoming the information bottleneck associated with the concept
    layer:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: … 一个概念嵌入模型用一组神经元表示每个概念，有效地克服了与概念层相关的信息瓶颈：
- en: '![](../Images/eec3e417a7fcbc25c6982010803deb6a.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/eec3e417a7fcbc25c6982010803deb6a.png)'
- en: Concept Embedding Models represent each concept as a supervised vector. Image
    by the authors.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 概念嵌入模型将每个概念表示为一个受监督的向量。图片由作者提供。
- en: 'As a result, concept embedding models enable us to achieve both high accuracy
    and high-quality explanations simultaneously:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，概念嵌入模型使我们能够同时实现高准确性和高质量解释：
- en: '![](../Images/8cc20b7417da10e603aa681a9e8d3cbe.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8cc20b7417da10e603aa681a9e8d3cbe.png)'
- en: Concept Embedding Models go beyond the accuracy-explainability trade-off in
    Concept Bottleneck Models with nearly optimal task accuracy and concept alignment.
    The optimal trade-off is represented by the red star (top-right). The task is
    to learn the sign (+/-) of the dot product between two vectors. Image by the authors.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 概念嵌入模型在概念瓶颈模型中超越了准确性和可解释性的折衷，几乎实现了最佳的任务准确性和概念对齐。 最佳折衷由红色星星（右上方）表示。 任务是学习两个向量之间点积的符号（+/-）。图片由作者提供。
- en: Concept embedding models succeed in striking a balance between task accuracy
    and explanation quality.
  id: totrans-69
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 概念嵌入模型成功地在任务准确性和解释质量之间取得平衡。
- en: Hands-on concept embedding models
  id: totrans-70
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 亲身体验概念嵌入模型
- en: Implementing these models in pytorch is as easy as it was with standard concept
    bottleneck models!
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在 pytorch 中实现这些模型就像实现标准概念瓶颈模型那样简单！
- en: 'We start by loading our data:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从加载数据开始：
- en: '[PRE6]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Next, we instantiate a concept encoder to map the input features to the concept
    space and a task predictor to map concepts to task predictions:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们实例化一个概念编码器，将输入特征映射到概念空间，并实例化一个任务预测器，将概念映射到任务预测：
- en: '[PRE7]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'We then train the network by optimizing the cross-entropy loss on both concepts
    and tasks:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们通过优化概念和任务上的交叉熵损失来训练网络：
- en: '[PRE8]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'After training the model, we evaluate its performance on the test set:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练模型后，我们在测试集上评估其性能：
- en: '[PRE9]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Now, after just a few epochs, we can observe that both the concept and the task
    accuracy are quite good on the test set (~96% accuracy), almost ~15% higher than
    with a standard concept bottleneck model!
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，仅经过几轮训练，我们可以观察到概念和任务的准确性在测试集上都相当好（约96%的准确率），比标准概念瓶颈模型高出近15%！
- en: Why interpretability > explainability?
  id: totrans-81
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 为什么可解释性>可解释性？
- en: 'Despite the simplicity and intuitiveness of the explanations provided by the
    techniques discussed thus far, there is still an inherent limitation: the precise
    logical reasoning behind the model’s predictions remains unclear.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管到目前为止所讨论的技术提供了简单直观的解释，但仍存在一个固有的限制：模型预测背后的精确逻辑推理仍然不清楚。
- en: In fact, even if we were to employ a transparent machine learning model like
    a decision tree or logistic regression, it wouldn’t necessarily alleviate the
    issue when using concept embeddings. This is because the individual dimensions
    of concept vectors lack a clear semantic interpretation for humans. For instance,
    a logic sentence in a decision tree stating*“if {yellow[2]>0.3} and {yellow[3]<-1.9}
    and {round[1]>4.2} then {banana}”* does not hold much semantic meaning as terms
    like “*{yellow[2]>0.3}”* (referring to the second dimension of the concept vector
    “yellow” being greater than “0.3”) do not carry significant relevance to us.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，即使我们使用像决策树或逻辑回归这样的透明机器学习模型，它也不一定能缓解使用概念嵌入时的问题。这是因为概念向量的个别维度对人类缺乏明确的语义解释。例如，决策树中的逻辑句子*“如果
    {yellow[2]>0.3} 和 {yellow[3]<-1.9} 和 {round[1]>4.2} 那么 {banana}”* 的术语如*“{yellow[2]>0.3}”*（指概念向量“yellow”的第二维大于“0.3”）对我们来说并没有太大语义意义。
- en: '![](../Images/c634d0f0dd1554ef247f98676266e0e7.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c634d0f0dd1554ef247f98676266e0e7.png)'
- en: Standard interpretable classifiers fail to provide interpretable predictions
    using concept embeddings as individual embedding dimensions lack a clear semantic
    meaning. Image by the authors.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 标准的可解释分类器无法使用概念嵌入提供可解释的预测，因为个别嵌入维度缺乏明确的语义意义。图片由作者提供。
- en: Even transparent models cannot provide interpretable predictions when applied
    on concept embeddings.
  id: totrans-86
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 即使是透明模型在应用于概念嵌入时也无法提供可解释的预测。
- en: How can we overcome this challenge this time?!
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 我们这次如何克服这个挑战？！
- en: 'Step #3: Interpretability without compromises'
  id: totrans-88
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第3步：无妥协的可解释性
- en: Again, a solution does exist!
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，解决方案确实存在！
- en: A primer on deep concept reasoning
  id: totrans-90
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 深度概念推理简介
- en: 'Deep Concept Reasoners [3] (a recent paper accepted at the 2023 *International
    Conference on Machine Learning*) address the limitations of concept embedding
    models by achieving full interpretability using concept embeddings. The key innovation
    of this method was to design a task predictor which processes concept embeddings
    and concept truth degrees separately. While a standard machine learning model
    would process concept embeddings and concept truth degrees simultaneously:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 深度概念推理器 [3]（一篇被2023年*国际机器学习大会*接受的最新论文）通过实现完全的可解释性来解决概念嵌入模型的局限性。该方法的关键创新在于设计了一个任务预测器，分别处理概念嵌入和概念真实性度。标准的机器学习模型则会同时处理概念嵌入和概念真实性度：
- en: '![](../Images/c634d0f0dd1554ef247f98676266e0e7.png)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c634d0f0dd1554ef247f98676266e0e7.png)'
- en: Standard interpretable classifiers fail to provide interpretable predictions
    using concept embeddings as individual embedding dimensions lack a clear semantic
    meaning. Image by the authors.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 标准的可解释分类器无法使用概念嵌入提供可解释的预测，因为个别嵌入维度缺乏明确的语义意义。图片由作者提供。
- en: 'a deep concept reasoner generates (***interpretable***!) logic rules using
    concept embeddings and then executes rules symbolically assigning to concept symbols
    their corresponding truth value:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 深度概念推理器生成（***可解释的***！）逻辑规则，使用概念嵌入，然后以符号化方式执行规则，将相应的真实性值分配给概念符号：
- en: '![](../Images/32b9923c40fcb6be424da5d743252162.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/32b9923c40fcb6be424da5d743252162.png)'
- en: A Deep Concept Reasoner generates fuzzy logic rules using neural models on concept
    embeddings, and then
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 深度概念推理器使用神经模型在概念嵌入上生成模糊逻辑规则，然后
- en: executes the rule using the concept truth degrees to evaluate the rule symbolically.
    Image by the authors.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 使用概念真实性度执行规则，以符号化方式评估规则。图片由作者提供。
- en: Deep concept reasoners provide interpretable predictions when applied on concept
    embeddings as each prediction is generated using a logic rule of concept truth
    degrees.
  id: totrans-98
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 深度概念推理器在应用于概念嵌入时提供可解释的预测，因为每个预测都是基于逻辑规则的概念真值生成的。
- en: 'This unique technique allows us to implement models that are **perfectly interpretable,
    as they make predictions based on logic rules as a decision tree!** What sets
    them apart is their remarkable performance on challenging tasks, surpassing that
    of traditional interpretable models like decision trees or logistic regression:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 这种独特的技术使我们能够实现**完全可解释的模型，因为它们基于逻辑规则进行预测，如决策树一样！** 使它们与众不同的是在挑战性任务中的卓越表现，超越了传统的可解释模型，如决策树或逻辑回归：
- en: '![](../Images/3ea093fae88bfa1d3f630952ae078f92.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3ea093fae88bfa1d3f630952ae078f92.png)'
- en: Deep concept reasoners outperform interpretable concept-based models and match
    black-box models’ accuracy. CE stands for concept embeddings and CT for concept
    truth values. Image by the authors.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 深度概念推理器超越了可解释的基于概念的模型，并且与黑箱模型的准确性相匹配。CE代表概念嵌入，CT代表概念真值。图片由作者提供。
- en: By leveraging deep concept reasoning, we can unlock the potential for highly
    interpretable models that offer superior performance on complex tasks.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 通过利用深度概念推理，我们可以释放出具有高解释性的模型的潜力，这些模型在复杂任务上表现出色。
- en: Deep concept reasoners provide interpretable predictions while outperforming
    interpretable models in terms of task accuracy.
  id: totrans-103
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 深度概念推理器提供可解释的预测，同时在任务准确性方面超越了可解释模型。
- en: Hands-on deep concept reasoning
  id: totrans-104
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实践深度概念推理
- en: Implementing deep concept reasoning is again quite easy using the `pytorch_explain`
    library!
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`pytorch_explain`库实现深度概念推理同样非常简单！
- en: 'As in previous examples, we instantiate a concept encoder to map the input
    features to the concept space and a deep concept reasoner to map concepts to task
    predictions:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 如前面的示例所示，我们实例化一个概念编码器，将输入特征映射到概念空间，并实例化一个深度概念推理器，将概念映射到任务预测：
- en: '[PRE10]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'We then train the network by optimizing the cross-entropy loss on both concepts
    and tasks:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们通过优化概念和任务的交叉熵损失来训练网络：
- en: '[PRE11]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'After training the model, we can evaluate its performance on the test set and
    check that it matches the accuracy of a concept embedding model (~99%):'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 训练模型后，我们可以在测试集上评估其性能，并检查其是否与概念嵌入模型的准确性相匹配（约99%）：
- en: '[PRE12]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'However, this time we can get the precise and exact reasoning behind each prediction
    by reading the corresponding logical rule:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，这次我们可以通过阅读相应的逻辑规则，精确获取每个预测背后的推理：
- en: '[PRE13]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'where each element in `local_explanations` has the following structure:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 每个`local_explanations`中的元素具有以下结构：
- en: '[PRE14]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Similarly, we can extract global explanations using the command:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，我们可以使用命令提取全局解释：
- en: '[PRE15]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'which returns the full set of rules found by the model, enabling humans to
    double check that the reasoning of the deep learning system matches the expected
    behaviour:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 该方法返回模型找到的完整规则集，使人类能够再次检查深度学习系统的推理是否与预期行为一致：
- en: '[PRE16]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Key takeaways
  id: totrans-120
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 主要收获
- en: In this post, we have explored the key features of the `pytorch_explain` library,
    highlighting state-of-the-art concept-based architectures and demonstrating their
    implementation with just a few lines of code.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我们探索了`pytorch_explain`库的关键功能，突出了最先进的基于概念的架构，并用几行代码演示了它们的实现。
- en: 'Here’s a recap of what we covered:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是我们涵盖内容的总结：
- en: 'Concept bottleneck models: These models provide intuitive explanations by tracing
    predictions back to a set of human-interpretable concepts;'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 概念瓶颈模型：这些模型通过追溯预测到一组人类可解释的概念，提供直观的解释；
- en: 'Concept embedding models: By overcoming the information bottleneck associated
    with concepts, these models achieve high prediction accuracy without compromising
    the quality of explanations;'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 概念嵌入模型：通过克服与概念相关的信息瓶颈，这些模型在不影响解释质量的情况下实现了高预测准确性；
- en: 'Deep concept reasoners: The predictions of these models are fully interpretable
    as deep concept reasoners make predictions using logical rules composing concepts’
    truth values.'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深度概念推理器：这些模型的预测完全可解释，因为深度概念推理器使用逻辑规则来组合概念的真值进行预测。
- en: By utilizing the powerful capabilities of the “*Pytorch, Explain!”* library
    and implementing the techniques discussed, you have the opportunity to significantly
    enhance the interpretability of your models while maintaining high prediction
    accuracy. This not only empowers you to gain deeper insights into the reasoning
    behind model predictions but also fosters and calibrates users’ trust in the system.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 通过利用“*Pytorch, Explain!*”库的强大功能并实施讨论中的技术，你有机会显著提升模型的可解释性，同时保持高预测准确性。这不仅使你能够深入了解模型预测背后的推理，还培养和校准用户对系统的信任。
- en: References
  id: totrans-127
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] Koh, Pang Wei, et al. “Concept bottleneck models.” *International Conference
    on Machine Learning*. PMLR, 2020.'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] Koh, Pang Wei, 等人. “概念瓶颈模型。” *国际机器学习大会*。PMLR，2020年。'
- en: '[2] Zarlenga, Mateo Espinosa, et al. “Concept embedding models: Beyond the
    accuracy-explainability trade-off.” *Advances in Neural Information Processing
    Systems*. Vol. 35\. Curran Associates, Inc., 2022\. 21400–21413.'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] Zarlenga, Mateo Espinosa, 等人. “概念嵌入模型：超越准确性与可解释性权衡。” *神经信息处理系统进展*。第35卷。Curran
    Associates, Inc.，2022年。21400–21413。'
- en: '[3] Barbiero, Pietro, et al. “Interpretable Neural-Symbolic Concept Reasoning.”
    *arXiv preprint arXiv:2304.14068* (2023).'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] Barbiero, Pietro, 等人. “可解释的神经符号概念推理。” *arXiv预印本 arXiv:2304.14068*（2023年）。'
