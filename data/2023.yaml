- en: Is Your LLM Application Ready for the Public?
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 您的LLM应用程序准备好公开了吗？
- en: 原文：[https://towardsdatascience.com/is-your-llm-application-ready-for-the-public-55fdcce99888?source=collection_archive---------17-----------------------#2023-06-20](https://towardsdatascience.com/is-your-llm-application-ready-for-the-public-55fdcce99888?source=collection_archive---------17-----------------------#2023-06-20)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/is-your-llm-application-ready-for-the-public-55fdcce99888?source=collection_archive---------17-----------------------#2023-06-20](https://towardsdatascience.com/is-your-llm-application-ready-for-the-public-55fdcce99888?source=collection_archive---------17-----------------------#2023-06-20)
- en: Key concerns when productionizing LLM-based applications
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将基于LLM的应用程序投入生产时的关键关注点
- en: '[](https://medium.com/@itai_88363?source=post_page-----55fdcce99888--------------------------------)[![Itai
    Bar Sinai](../Images/f5639687f014cd240832a01414f27d8b.png)](https://medium.com/@itai_88363?source=post_page-----55fdcce99888--------------------------------)[](https://towardsdatascience.com/?source=post_page-----55fdcce99888--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----55fdcce99888--------------------------------)
    [Itai Bar Sinai](https://medium.com/@itai_88363?source=post_page-----55fdcce99888--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@itai_88363?source=post_page-----55fdcce99888--------------------------------)[![Itai
    Bar Sinai](../Images/f5639687f014cd240832a01414f27d8b.png)](https://medium.com/@itai_88363?source=post_page-----55fdcce99888--------------------------------)[](https://towardsdatascience.com/?source=post_page-----55fdcce99888--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----55fdcce99888--------------------------------)
    [Itai Bar Sinai](https://medium.com/@itai_88363?source=post_page-----55fdcce99888--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fdae6c2441260&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fis-your-llm-application-ready-for-the-public-55fdcce99888&user=Itai+Bar+Sinai&userId=dae6c2441260&source=post_page-dae6c2441260----55fdcce99888---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----55fdcce99888--------------------------------)
    ·6 min read·Jun 20, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F55fdcce99888&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fis-your-llm-application-ready-for-the-public-55fdcce99888&user=Itai+Bar+Sinai&userId=dae6c2441260&source=-----55fdcce99888---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fdae6c2441260&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fis-your-llm-application-ready-for-the-public-55fdcce99888&user=Itai+Bar+Sinai&userId=dae6c2441260&source=post_page-dae6c2441260----55fdcce99888---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----55fdcce99888--------------------------------)
    ·6分钟阅读·2023年6月20日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F55fdcce99888&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fis-your-llm-application-ready-for-the-public-55fdcce99888&user=Itai+Bar+Sinai&userId=dae6c2441260&source=-----55fdcce99888---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F55fdcce99888&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fis-your-llm-application-ready-for-the-public-55fdcce99888&source=-----55fdcce99888---------------------bookmark_footer-----------)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F55fdcce99888&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fis-your-llm-application-ready-for-the-public-55fdcce99888&source=-----55fdcce99888---------------------bookmark_footer-----------)'
- en: Large language models (LLMs) are becoming the bread and butter of modern NLP
    applications and have, in many ways, replaced a variety of more specialized tools
    such as named entity recognition models, question-answering models, and text classifiers.
    As such, it’s difficult to imagine an NLP product that doesn’t use an LLM in at
    least some fashion. While LLMs bring a host of benefits such as increased personalization
    and creative dialogue generation, it’s important to understand their pitfalls
    and how to address them when integrating these models into a software product
    that serves end users. As it turns out, monitoring is well-posed to address many
    of these challenges and is an essential part of the toolbox for any business working
    with LLMs.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）正成为现代自然语言处理（NLP）应用的基本组成部分，并且在许多方面取代了各种更为专业的工具，例如命名实体识别模型、问答模型和文本分类器。因此，很难想象一个不以某种方式使用LLM的NLP产品。虽然LLM带来了诸如个性化和创意对话生成等诸多好处，但在将这些模型集成到服务最终用户的软件产品中时，了解其陷阱以及如何应对这些问题是很重要的。事实证明，监控能够很好地解决这些挑战，并且是任何与LLM合作的企业工具箱中的一个重要部分。
- en: Data, Privacy, and Prompt Injection
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据、隐私和提示注入
- en: '![](../Images/f33bf41296052bcbdf1e05d77807782d.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f33bf41296052bcbdf1e05d77807782d.png)'
- en: Image by TheDigitalArtist via[Pixabay](https://pixabay.com/photos/computer-business-gdpr-legislation-3233754/)
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由 TheDigitalArtist 提供，来源于 [Pixabay](https://pixabay.com/photos/computer-business-gdpr-legislation-3233754/)
- en: Data and privacy
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据与隐私
- en: Privacy and data usage are among the primary concerns of the modern day consumer,
    and in the wake of well-known data sharing scandals such as Cambridge Analytica
    consumers are becoming less and less likely to use services and products that
    put their personal privacy at risk. While LLMs provide users with an incredible
    degree of personalization, it’s important to understand the risks they pose. As
    with all machine learning models, LLMs are vulnerable to targeted attacks designed
    to reveal training data and they are particularly at risk due to their generative
    nature and can even leak data accidentally while performing free-form generation.
    For example, in a [2020 blog post](https://ai.googleblog.com/2020/12/privacy-considerations-in-large.html),
    Nicholas Carlini, a research scientist at Google Brain, discussed how LLMs such
    as GPT can be prompted in a way that leads them to reveal personally identifiable
    information such as name, address, and email address that are contained in the
    model’s training data. This suggests that businesses that fine-tune LLMs on their
    customer’s data are likely to engender these same sorts of privacy risks. Similarly,
    [a paper](https://ppml-workshop.github.io/ppml21/pdfs/ppml21-final58.pdf) from
    researchers at Microsoft corroborates these claims as well as suggests specific
    mitigation strategies which utilize techniques from differential privacy in order
    to train LLMs while reducing data leakage concerns. Unfortunately, many businesses
    cannot leverage these techniques due to using LLM APIs that do not give them control
    over the fine-tuning process. The solution for these companies lies in inserting
    a monitoring step that validates and constrains a model’s outputs prior to returning
    the results to an end user. In this way, businesses can identify and flag potential
    instances of training data leakage prior to the actual occurrence of a privacy
    violation. For example, a monitoring tool can apply techniques such as Named Entity
    Recognition and Regex filtering to identify names of persons, addresses, emails,
    and other sensitive information generated by a model before it gets into the wrong
    hands. This is particularly essential for organizations working in a privacy-restricted
    space such as healthcare or finance where strict regulations such as HIPAA, and
    FTC/FDIC come into play. Even businesses who simply work internationally are at
    risk of violating complex location-specific regulations such as the EU’s GDPR.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 隐私和数据使用是现代消费者的主要关注点之一，尤其是在剑桥分析等著名数据共享丑闻的影响下，消费者越来越不愿使用那些可能危及个人隐私的服务和产品。虽然LLM（大型语言模型）为用户提供了极高的个性化，但了解它们所带来的风险也很重要。与所有机器学习模型一样，LLM容易受到针对性的攻击，这些攻击旨在揭示训练数据，由于其生成性质，LLM尤其容易受到风险，甚至在进行自由形式生成时可能会意外泄露数据。例如，在[2020年博客文章](https://ai.googleblog.com/2020/12/privacy-considerations-in-large.html)中，谷歌大脑的研究科学家尼古拉斯·卡尔尼讨论了如何通过提示LLM（如GPT）来泄露个人身份信息，如姓名、地址和电子邮件，这些信息包含在模型的训练数据中。这表明，针对客户数据对LLM进行微调的企业可能会引发类似的隐私风险。同样，来自微软研究人员的[论文](https://ppml-workshop.github.io/ppml21/pdfs/ppml21-final58.pdf)也证实了这些说法，并建议了一些具体的缓解策略，这些策略利用差分隐私技术来训练LLM，同时减少数据泄露的担忧。不幸的是，由于许多企业使用的LLM
    API不允许他们控制微调过程，因此无法利用这些技术。这些公司的解决方案在于插入一个监控步骤，在将结果返回给最终用户之前验证和限制模型的输出。通过这种方式，企业可以在隐私违规实际发生之前识别并标记潜在的训练数据泄露实例。例如，监控工具可以应用诸如命名实体识别和正则表达式过滤等技术来识别模型生成的姓名、地址、电子邮件及其他敏感信息，防止这些信息落入不良分子之手。这对于在隐私受限领域如医疗或金融工作的组织尤其重要，这些领域涉及HIPAA和FTC/FDIC等严格法规。即使是仅仅在国际上运营的企业也面临违反复杂的地域特定法规，如欧盟的GDPR。
- en: Prompt injection
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 提示注入
- en: Prompt injection refers to the (often malicious) process of designing LLM prompts
    that somehow “trick” or confuse the system into providing harmful outputs. For
    example, a recent article showed how well-designed prompt injection attacks make
    it possible to subvert OpenAI’s GPT-4 model and have it provide factually false
    information and even promote conspiracy theories. One can imagine even more nefarious
    scenarios in which a user prompts an LLM to provide advice on how to build a bomb,
    to give details on how to best commit suicide, or to generate code that can be
    used to infect other computers. Vulnerability to prompt injection attacks is an
    unfortunate side effect of how LLMs are trained, and it’s difficult to do anything
    on the front-end that will prevent every possible prompt injection attack. Even
    the most robust and recent LLMs, such as OpenAI’s ChatGPT — which was aligned
    specifically for safety — have proven vulnerable to prompt injections.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 提示注入（Prompt injection）指的是设计 LLM 提示的过程，通常是恶意的，通过某种方式“欺骗”或混淆系统，从而产生有害的输出。例如，最近的一篇文章展示了精心设计的提示注入攻击如何使得
    OpenAI 的 GPT-4 模型被颠覆，从而提供事实错误的信息甚至推广阴谋论。可以想象到更多邪恶的场景，例如用户提示 LLM 提供如何制造炸弹的建议、如何最佳自杀的细节，或生成可用于感染其他计算机的代码。提示注入攻击的脆弱性是不幸的副作用，由于
    LLM 的训练方式，很难在前端做任何事情来防止所有可能的提示注入攻击。即使是最强大和最新的 LLM，如 OpenAI 的 ChatGPT——专门为安全而调整——也证明了对提示注入的脆弱性。
- en: Due to the myriad ways in which prompt injection can manifest, it’s nearly impossible
    to guard against all possibilities. As such, monitoring of LLM generated outputs
    is crucial as it provides a mechanism for identifying and flagging specious information
    as well as outright harmful generations. Monitoring can use simple NLP heuristics
    or additional ML classifiers to flag responses from the model that contain harmful
    content and intercept them before they are returned to the user. Similarly, monitoring
    of the prompts themselves can catch some of the harmful ones prior to their being
    passed to the model.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 由于提示注入可能表现出的方式千差万别，因此几乎不可能防范所有可能性。因此，监控 LLM 生成的输出至关重要，因为它提供了一种识别和标记虚假信息以及明显有害生成的机制。监控可以使用简单的
    NLP 启发式方法或额外的 ML 分类器来标记包含有害内容的模型响应，并在返回给用户之前拦截它们。类似地，对提示本身的监控可以在提示被传递给模型之前捕捉到一些有害的提示。
- en: Hallucinations
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 幻觉
- en: The term hallucination refers to the propensity of an LLM to occasionally “dream
    up” outputs that are not actually grounded in reality. Prompt injection and hallucinations
    can manifest as two sides of the same coin, although with prompt injection the
    generation of falsities is a deliberate intention of the user, whereas hallucinations
    are an unintended side effect of an LLM’s training objective. Because LLMs are
    trained to, at each time step, predict the next most likely word in a sequence,
    they are able to generate highly realistic text. As a result, hallucinations are
    a simple consequence of the fact that what is most likely is not always true.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 幻觉（hallucination）指的是 LLM 偶尔“编造”那些实际上不符合现实的输出的倾向。提示注入和幻觉可以表现为同一枚硬币的两面，尽管提示注入中的虚假生成是用户的故意行为，而幻觉则是
    LLM 训练目标的无意副作用。由于 LLM 被训练为在每个时间步骤中预测序列中下一个最可能的词，因此它们能够生成高度逼真的文本。因此，幻觉是最可能的情况并不总是真的简单结果。
- en: '![](../Images/4ad9a20e80f4114ca5a3042e78f5b19a.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4ad9a20e80f4114ca5a3042e78f5b19a.png)'
- en: Image by Matheus Bertelli via [Pexels](https://www.pexels.com/photo/woman-laptop-working-internet-16094040/)
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：Matheus Bertelli via [Pexels](https://www.pexels.com/photo/woman-laptop-working-internet-16094040/)
- en: The latest generation of LLMs, such as GPT-3 and GPT-4, are optimized using
    an algorithm called Reinforcement Learning from Human Feedback (RLHF) in order
    to match a human’s subjective opinion of what makes a good response to a prompt.
    While this has allowed LLMs to reach higher levels of conversational fluency,
    it also sometimes leads them to speak too confidently when issuing their responses.
    For example, it is not uncommon to ask ChatGPT a question and have it confidently
    give a reply that seems plausible at first glance, yet which upon further examination
    turns out to be objectively false. Infusing LLMs with the ability to provide quantifications
    of uncertainty is still very much an active research problem and is not likely
    to be solved anytime soon. Thus, developers of LLM-based products should consider
    monitoring and analyzing outputs in an attempt to detect hallucinations and yield
    more nuanced responses than what LLM models provide out-of-the-box. This is especially
    vital in contexts where outputs of an LLM might be guiding some downstream process.
    For example, if an LLM chatbot is assisting a user by providing product recommendations
    and helping to place an order on a retailer’s website, monitoring procedures should
    be in effect to ensure that the model does not suggest purchasing a product that
    is not actually sold on that retailer’s website.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 最新一代的大型语言模型（LLM），例如 GPT-3 和 GPT-4，采用了一种叫做**人类反馈强化学习**（RLHF）的算法进行优化，以匹配人类对什么构成良好响应的主观判断。虽然这使得
    LLM 能够达到更高的对话流畅度，但有时也会导致它们在回应时表现得过于自信。例如，询问 ChatGPT 一个问题时，它可能会自信地给出一个初看似乎合理的回答，但经过进一步检查，结果却是客观上错误的。赋予
    LLM 提供不确定性量化的能力仍然是一个活跃的研究问题，并且不太可能很快得到解决。因此，LLM 基于的产品的开发者应该考虑监控和分析输出，以试图检测幻觉，并提供比
    LLM 模型开箱即用的更细致的回应。这在 LLM 的输出可能指导某些下游过程的情况下尤为重要。例如，如果一个 LLM 聊天机器人在帮助用户提供产品推荐并协助在零售商网站上下订单时，应该实施监控程序，以确保模型不会建议购买在该零售商网站上实际上并未销售的产品。
- en: Uncontrolled costs
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**不受控制的成本**'
- en: Because LLMs are becoming increasingly commoditized via APIs, it’s important
    that businesses integrating these models into their products have a plan in place
    to prevent unbounded increases in costs. Without safeguards in place, it can be
    easy for users of a product to generate thousands of API calls and issue prompts
    with thousands of tokens (think of the case where a user copy-pastes an extremely
    long document into the input and asks the LLM to analyze it). Because LLM APIs
    are usually metered on the basis of number of calls and token counts (both in
    the prompt and the model’s response), it’s not difficult to see how costs can
    rapidly spiral out of control. Therefore, businesses need to be mindful in how
    they create their pricing structures in order to offset these costs. Furthermore,
    businesses should have monitoring procedures in place that allow them to understand
    how surges in usage impact costs and allow them to mitigate these surges by imposing
    usage caps or taking other remediative measures.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 LLM 通过 API 正变得越来越商品化，企业在将这些模型集成到其产品中时，制定防止成本无限增加的计划非常重要。如果没有保护措施，用户生成数千个
    API 调用并发出数千个令牌的提示（例如，用户将一份极长的文档粘贴到输入中，并要求 LLM 分析它）是很容易的。由于 LLM API 通常根据调用次数和令牌计数（包括提示和模型的回应）进行计量，因此成本迅速失控并不困难。因此，企业在制定定价结构时需要小心，以抵消这些成本。此外，企业应该有监控程序，以了解使用激增如何影响成本，并通过施加使用上限或采取其他补救措施来缓解这些激增。
- en: Conclusion
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**结论**'
- en: Every business that uses LLMs in their products should be sure to incorporate
    monitoring into their systems in order to avoid and address the many pitfalls
    of LLMs. In addition, the monitoring solutions used should be specifically geared
    towards LLM applications and allow users to identify potential privacy violations,
    prevent and/or remediate prompt injections, flag hallucinations, and diagnose
    rising costs. The best monitoring solutions will address all of these concerns
    and provide a framework for businesses to ensure that their LLM-based applications
    are ready to be deployed to the public. Have confidence your LLM application is
    fully optimized and performing as intended by [booking a demo](https://www.monalabs.io/request-demo)
    to see Mona’s comprehensive monitoring capabilities.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 每个将大型语言模型（LLMs）应用于其产品的企业，都应确保将监控功能整合到其系统中，以避免并解决LLMs的众多陷阱。此外，使用的监控解决方案应专门针对LLMs应用，允许用户识别潜在的隐私侵犯，防止和/或修复提示注入，标记幻觉，并诊断不断上升的成本。最佳的监控解决方案将解决所有这些问题，并为企业提供一个框架，以确保其基于LLM的应用准备好对公众发布。通过[预约演示](https://www.monalabs.io/request-demo)来查看Mona的全面监控能力，确保你的LLM应用经过充分优化并按预期运行。
