- en: Language Models for Sentence Completion
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 用于句子补全的语言模型
- en: 原文：[https://towardsdatascience.com/language-models-for-sentence-completion-6a5298a85e43?source=collection_archive---------5-----------------------#2023-09-15](https://towardsdatascience.com/language-models-for-sentence-completion-6a5298a85e43?source=collection_archive---------5-----------------------#2023-09-15)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/language-models-for-sentence-completion-6a5298a85e43?source=collection_archive---------5-----------------------#2023-09-15](https://towardsdatascience.com/language-models-for-sentence-completion-6a5298a85e43?source=collection_archive---------5-----------------------#2023-09-15)
- en: A practical application of a language model that picks the most likely candidate
    word that extends an English sentence by a single word
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 语言模型的实际应用，选择最可能的单词来扩展一个英文句子
- en: '[](https://medium.com/@dhruvbird?source=post_page-----6a5298a85e43--------------------------------)[![Dhruv
    Matani](../Images/d63bf7776c28a29c02b985b1f64abdd3.png)](https://medium.com/@dhruvbird?source=post_page-----6a5298a85e43--------------------------------)[](https://towardsdatascience.com/?source=post_page-----6a5298a85e43--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----6a5298a85e43--------------------------------)
    [Dhruv Matani](https://medium.com/@dhruvbird?source=post_page-----6a5298a85e43--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@dhruvbird?source=post_page-----6a5298a85e43--------------------------------)[![Dhruv
    Matani](../Images/d63bf7776c28a29c02b985b1f64abdd3.png)](https://medium.com/@dhruvbird?source=post_page-----6a5298a85e43--------------------------------)[](https://towardsdatascience.com/?source=post_page-----6a5298a85e43--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----6a5298a85e43--------------------------------)
    [Dhruv Matani](https://medium.com/@dhruvbird?source=post_page-----6a5298a85e43--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F63f5d5495279&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flanguage-models-for-sentence-completion-6a5298a85e43&user=Dhruv+Matani&userId=63f5d5495279&source=post_page-63f5d5495279----6a5298a85e43---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----6a5298a85e43--------------------------------)
    ·13 min read·Sep 15, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F6a5298a85e43&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flanguage-models-for-sentence-completion-6a5298a85e43&user=Dhruv+Matani&userId=63f5d5495279&source=-----6a5298a85e43---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F63f5d5495279&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flanguage-models-for-sentence-completion-6a5298a85e43&user=Dhruv+Matani&userId=63f5d5495279&source=post_page-63f5d5495279----6a5298a85e43---------------------post_header-----------)
    发表在[Towards Data Science](https://towardsdatascience.com/?source=post_page-----6a5298a85e43--------------------------------)
    ·13分钟阅读·2023年9月15日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F6a5298a85e43&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flanguage-models-for-sentence-completion-6a5298a85e43&user=Dhruv+Matani&userId=63f5d5495279&source=-----6a5298a85e43---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F6a5298a85e43&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flanguage-models-for-sentence-completion-6a5298a85e43&source=-----6a5298a85e43---------------------bookmark_footer-----------)![](../Images/e52731d3997a812b3de2cf04fc95c135.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F6a5298a85e43&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flanguage-models-for-sentence-completion-6a5298a85e43&source=-----6a5298a85e43---------------------bookmark_footer-----------)![](../Images/e52731d3997a812b3de2cf04fc95c135.png)'
- en: Photo by [Brett Jordan](https://unsplash.com/@brett_jordan?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由[Brett Jordan](https://unsplash.com/@brett_jordan?utm_source=medium&utm_medium=referral)拍摄，来源于[Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: Co-authored with [Naresh Singh](https://medium.com/@brocolishbroxoli).
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 与[Naresh Singh](https://medium.com/@brocolishbroxoli)共同撰写。
- en: Table of contents
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 目录
- en: '[**Introduction**](#6d2d)'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '[**介绍**](#6d2d)'
- en: '[**Problem Statement**](#93ee)'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '[**问题陈述**](#93ee)'
- en: '[**Brainstorming a solution**](#7fa3)'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '[**头脑风暴解决方案**](#7fa3)'
- en: '[Algorithms and Data Structures](#9890)'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[算法与数据结构](#9890)'
- en: '[NLP (Natural Language Processing)](#d107)'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[NLP（自然语言处理）](#d107)'
- en: '[Deep Learning (Neural Networks)](#e846)'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[深度学习（神经网络）](#e846)'
- en: '[**An LSTM model**](#2ecd)'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '[**LSTM模型**](#2ecd)'
- en: '[Tokenization](#4a4c)'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[分词](#4a4c)'
- en: '[The PyTorch Model](#75ba)'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[PyTorch模型](#75ba)'
- en: '[Using the model to prune invalid suggestions](#7ad8)'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用模型修剪无效建议](#7ad8)'
- en: '[Computing the next word probability](#1049)'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[计算下一个单词的概率](#1049)'
- en: '[**A Transformer model**](#9241)'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '[**变换器模型**](#9241)'
- en: '[**Conclusion**](#597e)'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '[**结论**](#597e)'
- en: Introduction
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: Language models such as GPT have become very popular recently and are being
    used for a variety of text generation tasks, such as in ChatGPT or other conversational
    AI systems. These language models are huge, often exceeding tens of billions of
    parameters, and need a lot of computing resources and money to run.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 语言模型如GPT最近变得非常流行，并被用于各种文本生成任务，例如在ChatGPT或其他对话式AI系统中。这些语言模型非常庞大，通常超过数十亿个参数，需要大量计算资源和金钱来运行。
- en: In the context of English language models, these massive models are [over-parameterized](https://www.reddit.com/r/MachineLearning/comments/ib4rth/d_why_does_models_like_gpt3_or_bert_dont_have/)
    since they use the model’s parameters to memorize and learn aspects of our world
    instead of just modeling the English language. We can likely use a much smaller
    model if we have an application that requires the model to understand just the
    language and its constructs.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在英语语言模型的背景下，这些大型模型是[过度参数化的](https://www.reddit.com/r/MachineLearning/comments/ib4rth/d_why_does_models_like_gpt3_or_bert_dont_have/)，因为它们使用模型的参数来记忆和学习我们世界的各个方面，而不仅仅是建模英语语言。如果我们有一个仅需理解语言及其结构的应用，我们很可能可以使用一个更小的模型。
- en: The complete code for running inference on the trained model can be found [in
    this notebook](https://github.com/dhruvbird/ml-notebooks/blob/main/next_word_probability/inference-next-word-probability.ipynb).
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 运行训练模型推理的完整代码可以在[这个笔记本中找到](https://github.com/dhruvbird/ml-notebooks/blob/main/next_word_probability/inference-next-word-probability.ipynb)。
- en: Problem Statement
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题陈述
- en: Let’s assume we’re building a swipe keyboard system that tries to predict the
    word you type in next on your mobile phone. Based on the pattern traced by the
    swipe pattern, there are many possibilities for the user’s intended word. However,
    many of these possible words aren’t actual words in English and can be eliminated.
    Even after this initial pruning and elimination step, many candidates remain,
    and we need to pick one as a suggestion for the user.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们正在构建一个滑动键盘系统，尝试预测你在手机上输入的下一个词。根据滑动模式的轨迹，用户的意图词有很多可能性。然而，这些可能的词中许多不是英语中的实际单词，可以被排除。即使在这个初步修剪和排除步骤之后，仍然有许多候选词，我们需要选择一个作为对用户的建议。
- en: To further prune this list of candidates, we can use a deep-learning-based language
    model that looks at the provided context and tells us which candidate is most
    likely to complete the sentence.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 为进一步修剪候选列表，我们可以使用基于深度学习的语言模型，查看提供的上下文，并告诉我们哪个候选词最有可能完成这个句子。
- en: For example, if the user has typed the sentence *“I’ve scheduled this”* and
    then swipes a pattern as shown below
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果用户输入了句子*“我已经安排好了”*，然后划出如下面所示的模式
- en: '![](../Images/1d165ee855a1952f47030d274ae208b7.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1d165ee855a1952f47030d274ae208b7.png)'
- en: 'Then, some possible English language words that the user could have meant are:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，用户可能意图的英语单词包括：
- en: messing
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 混乱
- en: meeting
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 会议
- en: However, if we think about it, it’s probably more likely that the user meant
    “meeting” and not “messing” because of the word “scheduled” in the earlier part
    of the sentence.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，如果我们考虑一下，用户可能更可能意图“会议”而不是“混乱”，因为句子前面的单词是“安排”。
- en: Given everything we know so far, what options do we have for doing this pruning
    programmatically? Let’s brainstorm some solutions in the section below.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 根据我们目前所知道的一切，我们有哪些选项可以通过编程方式进行修剪？让我们在下面的章节中头脑风暴一些解决方案。
- en: Brainstorming a solution
  id: totrans-38
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 头脑风暴解决方案
- en: Algorithms and Data Structures
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 算法与数据结构
- en: Using first principles, it seems reasonable to start with a corpus of data,
    find pairs of words that come together, and train a [Markov model](https://en.wikipedia.org/wiki/Markov_model)
    that predicts the probability of the pair occurring in a sentence. You’ll notice
    two significant issues with this approach.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 基于第一原则，似乎合理从数据语料库开始，找到一对一对出现的单词，并训练一个[马尔可夫模型](https://en.wikipedia.org/wiki/Markov_model)来预测这一对在句子中出现的概率。你会发现这种方法有两个显著的问题。
- en: '**Space utilization**: There are anywhere [between 250k to 1 million words
    in the English language](https://www.merriam-webster.com/help/faq-how-many-english-words),
    which don’t include the numerous proper nouns that are constantly growing in volume.
    Hence, any traditional software solution modeling the probability of a pair of
    words occurring together must maintain a lookup table with 250k*250k = 62.5 billion
    word pairs, which is somewhat excessive. It seems likely that many pairs don’t
    occur very often and can be pruned. Even after pruning, there are a lot of pairs
    to worry about.'
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**空间利用**：英语中有[25万到100万词](https://www.merriam-webster.com/help/faq-how-many-english-words)，这还不包括不断增长的众多专有名词。因此，任何传统的软件解决方案在建模词对同时出现的概率时，必须维护一个250k*250k
    = 625亿词对的查找表，这显得有些过于庞大。许多词对可能出现的频率并不高，可以被修剪。即使在修剪之后，也有很多词对需要关注。'
- en: '**Completeness**: Encoding the probability of a pair of words doesn’t do justice
    to the problem at hand. For example, the earlier sentence context is completely
    lost when you’re looking at just the most recent pair of words. In the sentence
    *“How is your day coming”* if you want to check the word after “coming”, you’d
    have a lot of pairs starting with “coming”. This misses the entire sentence context
    before that word. One can imagine using word triplets, etc.… but this exacerbates
    the problem of space utilization mentioned above.'
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**完整性**：仅仅编码一对词的概率并不能解决眼前的问题。例如，当你只查看最近的一对词时，之前的句子上下文完全丧失。在句子*“你今天怎么样”*中，如果你想查看“coming”之后的词，你会有很多以“coming”开头的词对。这会忽略该词之前的整个句子上下文。可以想象使用词三元组等，但这会加剧上述提到的空间利用问题。'
- en: Let’s shift our focus to a solution that leverages the nature of the English
    language and see if that can help us here.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将注意力转移到一种利用英语语言特性的解决方案上，看看这是否能帮到我们。
- en: NLP (Natural Language Processing)
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: NLP（自然语言处理）
- en: Historically, the area of [NLP (natural language processing)](https://en.wikipedia.org/wiki/Natural_language_processing)
    involved understanding the [parts of speech (POS)](https://www.englishclub.com/grammar/parts-of-speech.php)
    of a sentence and using that information to perform such pruning and prediction
    decisions. One can imagine using a POS tag associated with each word to determine
    if the following word in a sentence is valid.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 历史上，[NLP（自然语言处理）](https://en.wikipedia.org/wiki/Natural_language_processing)领域涉及理解句子的[词性（POS）](https://www.englishclub.com/grammar/parts-of-speech.php)，并利用这些信息来进行修剪和预测决策。可以想象使用与每个词相关的POS标签来确定句子中的下一个词是否有效。
- en: However, the process of computing the parts of speech for a sentence is a complex
    process in itself, and requires specialized understanding of language as evidenced
    in [this page on NLTK’s parts of speech tagging](https://www.nltk.org/book/ch05.html).
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，计算句子的词性是一个复杂的过程，需要对语言有专门的理解，这在[NLTK词性标注的页面](https://www.nltk.org/book/ch05.html)中有所体现。
- en: Next, let’s take a look at a deep-learning-based approach that requires a lot
    more tagged data, but not as much language expertise to build.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们来看看一种基于深度学习的方法，这种方法需要大量标记数据，但构建时不需要太多语言专业知识。
- en: Deep Learning (Neural Networks)
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 深度学习（神经网络）
- en: The area of NLP has been upended by the advent of deep learning. With the invention
    of LSTM and Transformer based language models, the solution more often than not
    involves throwing some high-quality data at a model and training it to predict
    the next word.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 随着深度学习的出现，NLP（自然语言处理）领域发生了翻天覆地的变化。随着LSTM和基于Transformer的语言模型的发明，解决方案往往涉及将一些高质量的数据投入到模型中，并训练其预测下一个词。
- en: In essence, this is what the GPT model is doing. GPT (Generative Pre-Trained
    Transformer) models are trained to predict the next word (token) given a prefix
    of a sentence.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 本质上，这就是GPT模型正在做的。GPT（生成预训练变换器）模型被训练来预测给定句子前缀后的下一个词（标记）。
- en: Given the sentence prefix *“It is such a wonderful”*, it’s likely for the model
    to provide the following as high-probability predictions for the word following
    the sentence.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 给定句子前缀*“这真是太棒了”*，模型很可能会提供以下作为该句子后续词的高概率预测。
- en: day
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 天
- en: experience
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 经验
- en: world
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 世界
- en: life
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 生活
- en: It’s also likely that the following words will have a lower probability of completing
    the sentence prefix.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，后续词完成句子前缀的概率可能较低。
- en: red
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 红色
- en: mouse
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 鼠标
- en: line
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 行
- en: The [Transformer model architecture](https://en.wikipedia.org/wiki/Transformer_(machine_learning_model))
    is at the heart of systems such as ChatGPT. However, for the more restricted use
    case of learning English language semantics, we can use a cheaper-to-run model
    architecture such as an [LSTM (long short-term memory)](https://en.wikipedia.org/wiki/Long_short-term_memory)
    model.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '[Transformer模型架构](https://en.wikipedia.org/wiki/Transformer_(machine_learning_model))是像ChatGPT这样的系统的核心。然而，对于学习英语语言语义的更受限的用例，我们可以使用一种更便宜的模型架构，如[LSTM（长短期记忆）](https://en.wikipedia.org/wiki/Long_short_term_memory)模型。'
- en: An LSTM model
  id: totrans-61
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LSTM模型
- en: Let’s build a simple LSTM model and train it to predict the next token given
    a prefix of tokens. Now, you might ask what a token is.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们构建一个简单的LSTM模型，并训练它根据标记的前缀预测下一个标记。现在，你可能会问什么是标记。
- en: Tokenization
  id: totrans-63
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 标记化
- en: Typically for language models, a token can mean
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 对于语言模型，标记通常可以意味着
- en: A single character (or a single byte)
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 单个字符（或单个字节）
- en: An entire word in the target language
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 目标语言中的整个单词
- en: Something in between 1 and 2\. This is usually called a sub-word
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 介于1和2之间。这通常被称为子词
- en: Mapping a single character (or byte) to a token is very restrictive since we’re
    overloading that token to hold a lot of context about where it occurs. This is
    because the character “c” for example, occurs in many different words, and to
    predict the next character after we see the character “c” requires us to really
    look hard at the leading context.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 将单个字符（或字节）映射到一个标记上是非常有限制的，因为我们把这个标记过度负荷到包含大量关于它出现的上下文的信息。这是因为字符“c”例如，出现在许多不同的单词中，要预测在看到字符“c”后下一个字符，需要仔细查看前面的上下文。
- en: Mapping a single word to a token is also problematic since English itself has
    anywhere between 250k and 1 million words. In addition, what happens when a new
    word is added to the language? Do we need to go back and re-train the entire model
    to account for this new word?
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 将单个单词映射到一个标记上也是有问题的，因为英语本身有大约25万到100万个单词。此外，当语言中添加了一个新单词时会发生什么呢？我们是否需要回去重新训练整个模型以考虑这个新单词？
- en: Sub-word tokenization is considered the industry standard in the year 2023\.
    It assigns substrings of bytes frequently occurring together to unique tokens.
    Typically, language models have anywhere from a few thousand (say 4,000) to tens
    of thousands (say 60,000) of unique tokens. The algorithm to determine what constitutes
    a token is determined by the [BPE (Byte pair encoding) algorith](https://huggingface.co/learn/nlp-course/chapter6/5?fw=pt)m.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 子词标记化被认为是2023年的行业标准。它将经常一起出现的字节子串分配给唯一的标记。通常，语言模型有几千（比如4,000）到几十万（比如60,000）个唯一标记。确定什么构成标记的算法由[BPE（字节对编码）算法](https://huggingface.co/learn/nlp-course/chapter6/5?fw=pt)m决定。
- en: 'To choose the number of unique tokens in our vocabulary (called the vocabulary
    size), we need to be mindful of a few things:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 选择我们词汇表中的唯一标记数量（称为词汇大小）时，我们需要注意以下几点：
- en: If we choose too few tokens, we’re back in the regime of a token per character,
    and it’s hard for the model to learn anything useful.
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果我们选择的标记太少，我们就回到了每个字符一个标记的情况，模型很难学到有用的东西。
- en: If we choose too many tokens, we end up in a situation where the model’s embedding
    tables over-shadow the rest of the model’s weight and it becomes hard to deploy
    the model in a constrained environment. The size of the embedding table will depend
    on the number of dimensions we use for each token. It’s not uncommon to use a
    size of 256, 512, 786, etc… If we use a token embedding dimension of 512, and
    we have 100k tokens, we end up with an embedding table that uses 200MiB in memory.
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果我们选择的标记太多，我们会遇到模型的嵌入表超越模型其他部分的权重，从而使模型在受限环境中部署变得困难。嵌入表的大小将取决于我们为每个标记使用的维度。使用256、512、786等大小并不罕见。如果我们使用512的标记嵌入维度，并且有100k个标记，我们会得到一个在内存中使用200MiB的嵌入表。
- en: Hence, we need to strike a balance when choosing the vocabulary size. In this
    example, we pick 6600 tokens and train our tokenizer with a vocabulary size of
    6600\. Next, let’s take a look at the model definition itself.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在选择词汇大小时我们需要找到一个平衡点。在这个例子中，我们选择了6600个标记，并用6600的词汇大小训练了我们的标记器。接下来，让我们看看模型定义本身。
- en: The PyTorch Model
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: PyTorch模型
- en: 'The model itself is pretty straightforward. We have the following layers:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 模型本身是相当简单的。我们有以下层：
- en: Token Embedding (vocab size=6600, embedding dim=512), for a total size of about
    15MiB (assuming 4 byte float32 as the embedding table’s data type)
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 词汇嵌入（词汇表大小=6600，嵌入维度=512），总大小约为15MiB（假设嵌入表的数据类型为4字节float32）
- en: LSTM (num layers=1, hidden dimension=786) for a total size of about 16MiB
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: LSTM（层数=1，隐藏维度=786）总大小约为16MiB
- en: Multi-Layer Perceptron (786 to 3144 to 6600 dimensions) for a total size of
    about 93MiB
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 多层感知器（786到3144到6600维）总大小约为93MiB
- en: The complete model has about 31M trainable parameters for a total size of about
    120MiB.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 完整的模型大约有31M可训练参数，总大小约为120MiB。
- en: '![](../Images/202d59c74834875e7046d636f26a5ce4.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/202d59c74834875e7046d636f26a5ce4.png)'
- en: Here’s the PyTorch code for the model.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 这是模型的PyTorch代码。
- en: '[PRE0]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Here’s the model summary using torchinfo.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 这是使用torchinfo的模型摘要。
- en: LSTM Model Summary
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: LSTM模型摘要
- en: '[PRE1]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '**Interpreting the accuracy**: After training this model on 12M English language
    sentences for about 8 hours on a P100 GPU, we achieved a loss of 4.03, a top-1
    accuracy of 29% and a top-5 accuracy of 49%. This means that 29% of the time,
    the model was able to correctly predict the next token, and 49% of the time, the
    next token in the training set was one of the top 5 predictions by the model.'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '**解读准确率**：在P100 GPU上对1200万句英语句子进行大约8小时训练后，我们达到了4.03的损失值，top-1准确率为29%，top-5准确率为49%。这意味着模型在29%的情况下能够正确预测下一个词汇，在49%的情况下，训练集中下一个词汇是模型前五个预测中的一个。'
- en: '**What should our success metric be?** While the top-1 and top-5 accuracy numbers
    for our model aren’t impressive, they aren’t as important for our problem. Our
    candidate words are a small set of possible words that fit the swipe pattern.
    What we want from our model is to be able to select an ideal candidate to complete
    the sentence such that it is syntactically and semantically coherent. Since our
    model learns the *nature* of language through the training data, we expect it
    to assign a higher probability to coherent sentences. For example, if we have
    the sentence *“The baseball player”* and possible completion candidates (“ran”,
    “swam”, “hid”), then the word “ran” is a better follow-up word than the other
    two. So, if our model predicts the word *ran* with a higher probability than the
    rest, it works for us.'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '**我们的成功指标应该是什么？** 虽然我们模型的top-1和top-5准确率数字并不令人印象深刻，但对于我们的问题，它们并不那么重要。我们的候选词是一个适合滑动模式的小词汇集合。我们希望模型能够选择一个理想的候选词来完成句子，使其在语法和语义上都连贯。由于我们的模型通过训练数据学习语言的*本质*，我们期望它对连贯的句子赋予更高的概率。例如，如果我们有句子
    *“棒球运动员”* 和可能的补全候选词（“跑了”、“游泳”、“躲藏”），那么“跑了”是比其他两个更好的后续词。因此，如果我们的模型以比其他词更高的概率预测“跑了”，那么它就能满足我们的需求。'
- en: '**Interpreting the loss**: A loss of 4.03 means that the negative log-likelihood
    of the prediction is 4.03, which means that the probability of predicting the
    next token correctly is e^-4.03 = 0.0178 or 1/56\. A randomly initialized model
    typically has a loss of about 8.8 which is -log_e(1/6600), since the model randomly
    predicts 1/6600 tokens (6600 being the vocabulary size). While a loss of 4.03
    may not seem great, it’s important to remember that the trained model is about
    120x better than an untrained (or randomly initialized) model.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '**解读损失值**：4.03的损失值意味着预测的负对数似然值为4.03，这意味着正确预测下一个词汇的概率为e^-4.03 = 0.0178或1/56。一个随机初始化的模型通常有大约8.8的损失值，这是-log_e(1/6600)，因为模型随机预测1/6600个词汇（6600是词汇表的大小）。虽然4.03的损失值可能看起来并不理想，但重要的是要记住，训练后的模型比未训练（或随机初始化）的模型要好约120倍。'
- en: Next, let’s take a look at how we can use this model to improve suggestions
    from our swipe keyboard.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们看看如何利用这个模型来改进我们的滑动键盘的建议功能。
- en: Using the model to prune invalid suggestions
  id: totrans-91
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用模型来修剪无效的建议
- en: Let’s take a look at a real example. Suppose we have a partial sentence *“I
    think”*, and the user makes the swipe pattern shown in blue below, starting at
    “o”, going between the letters “c” and “v”, and ending between the letters “e”
    and “v”.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 我们来看一个实际的例子。假设我们有一个部分句子 *“我认为”*，用户做出如下图中蓝色的滑动模式，从“o”开始，经过“c”和“v”之间，结束于“e”和“v”之间。
- en: '![](../Images/b8aa04ccb0955e8ed0c860ae8421faae.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b8aa04ccb0955e8ed0c860ae8421faae.png)'
- en: Some possible words that could be represented by this swipe pattern are
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 这个滑动模式可能表示的一些词汇是
- en: Over
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 结束
- en: Oct (short for October)
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 十月（October的缩写）
- en: Ice
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 冰
- en: I’ve (with the apostrophe implied)
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我已经（省略了撇号）
- en: Of these suggestions, the most likely one is probably going to be *“I’ve”*.
    Let’s feed these suggestions into our model and see what it spits out.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些建议中，最可能的一个可能是*“I’ve”*。让我们将这些建议输入到我们的模型中，看看它输出什么。
- en: '[PRE2]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The value after the = sign is the probability of the word being a valid completion
    of the sentence prefix. In this case, we see that the word “I’ve” has been assigned
    the highest probability. Hence, it is the most likely word to follow the sentence
    prefix “I think”.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 等号后的值是词作为句子前缀有效完成的概率。在这种情况下，我们看到词“*I’ve*”被分配了最高的概率。因此，它是最可能跟随句子前缀“I think”的词。
- en: The next question you might have is how we can compute these next-word probabilities.
    Let’s take a look.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来你可能会问，我们如何计算这些下一个词的概率。让我们来看看。
- en: Computing the next word probability
  id: totrans-103
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 计算下一个词的概率
- en: To compute the probability that a word is a valid completion of a sentence prefix,
    we run the model in eval (inference) mode and feed in the tokenized sentence prefix.
    We also tokenize the word after adding a whitespace prefix to the word. This is
    done because the HuggingFace pre-tokenizer splits words with spaces at the beginning
    of the word, so we want to make sure that our inputs are consistent with the tokenization
    strategy used by HuggingFace Tokenizers.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 为了计算一个词是否为句子前缀的有效完成，我们以评估（推理）模式运行模型，并输入标记化的句子前缀。我们还在词前添加一个空格前缀来标记化词。这是因为HuggingFace预标记器在词的开头用空格拆分词，所以我们希望确保我们的输入与HuggingFace标记器使用的标记化策略一致。
- en: Let’s assume that the candidate word is made up of 3 tokens T0, T1, and T2.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 假设候选词由3个标记T0、T1和T2组成。
- en: We first run the model with the original tokenized sentence prefix. For the
    last token, we check the probability of predicting token T0\. We add this to the
    “probs” list.
  id: totrans-106
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们首先用原始标记化的句子前缀运行模型。对于最后一个标记，我们检查预测T0标记的概率。我们将其添加到“probs”列表中。
- en: Next, we run a prediction on the prefix+T0 and check the probability of token
    T1\. We add this probability to the “probs” list.
  id: totrans-107
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们对前缀+T0进行预测，并检查T1的概率。我们将此概率添加到“probs”列表中。
- en: Next, we run a prediction on the prefix+T0+T1 and check the probability of token
    T2\. We add this probability to the “probs” list.
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们对前缀+T0+T1进行预测，并检查T2的概率。我们将此概率添加到“probs”列表中。
- en: The “probs” list contains the individual probabilities of generating the tokens
    T0, T1, and T2 in sequence. Since these tokens correspond to the tokenization
    of the candidate word, we can multiply these probabilities to get the combined
    probability of the candidate being a completion of the sentence prefix.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: “probs”列表包含生成T0、T1和T2标记的单独概率。由于这些标记对应于候选词的标记化，我们可以将这些概率相乘，以获得候选词作为句子前缀的完成的组合概率。
- en: The code for computing the completion probabilities is shown below.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 计算完成概率的代码如下所示。
- en: '[PRE3]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: We can see some more examples below.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 下面我们可以看到更多示例。
- en: '[PRE4]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: These examples show the probability of the word completing the sentence before
    it. The candidates are sorted in decreasing order of probability.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 这些示例展示了词在句子中的前面完成的概率。候选词按概率降序排列。
- en: Since Transformers are slowly replacing LSTM and RNN models for sequence-based
    tasks, let’s take a look at what a Transformer model for the same objective would
    look like.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 由于Transformer模型正逐渐取代LSTM和RNN模型用于基于序列的任务，让我们看看一个用于相同目标的Transformer模型会是什么样的。
- en: A Transformer model
  id: totrans-116
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Transformer模型
- en: Transformer-based models are a very popular architecture for training language
    models to predict the next word in a sentence. The specific technique we’ll use
    is the causal attention mechanism. We’ll train just the [transformer encoder layer
    in PyTorch](https://pytorch.org/docs/stable/generated/torch.nn.TransformerEncoder.html#torch.nn.TransformerEncoder)
    using causal attention. Causal attention means we’ll allow every token in the
    sequence to only look at the tokens before it. This resembles the information
    that a unidirectional LSTM layer uses when trained only in the forward direction.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 基于Transformer的模型是一种非常流行的架构，用于训练语言模型以预测句子中的下一个词。我们将使用的具体技术是因果注意力机制。我们将仅使用因果注意力训练[PyTorch中的Transformer编码器层](https://pytorch.org/docs/stable/generated/torch.nn.TransformerEncoder.html#torch.nn.TransformerEncoder)。因果注意力意味着我们允许序列中的每个标记仅查看它之前的标记。这类似于单向LSTM层在仅向前训练时所使用的信息。
- en: '![](../Images/533424e318bb7df7c8884e5dca7a45db.png)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/533424e318bb7df7c8884e5dca7a45db.png)'
- en: The Transformer model we’ll see here is based directly on the [nn.TransformerEncoder](https://pytorch.org/docs/stable/generated/torch.nn.TransformerEncoder.html#torch.nn.TransformerEncoder)
    and [nn.TransformerEncoderLayer](https://pytorch.org/docs/stable/generated/torch.nn.TransformerEncoderLayer.html#torch.nn.TransformerEncoderLayer)
    in PyTorch.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将看到的 Transformer 模型直接基于 PyTorch 中的 [nn.TransformerEncoder](https://pytorch.org/docs/stable/generated/torch.nn.TransformerEncoder.html#torch.nn.TransformerEncoder)
    和 [nn.TransformerEncoderLayer](https://pytorch.org/docs/stable/generated/torch.nn.TransformerEncoderLayer.html#torch.nn.TransformerEncoderLayer)。
- en: '[PRE5]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: We can plug this model in place of the LSTM model that we used before since
    it’s API is compatible. This model takes longer to train for the same amount of
    training data and has comparable performance.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以用这个模型代替之前使用的 LSTM 模型，因为它的 API 是兼容的。这个模型在训练相同数量的数据时需要更长的时间，但性能相当。
- en: Transformer models are better for long sequences. In our case, we have sequences
    of length 256\. Most of the context needed to perform next-word completion tends
    to be local, so we don’t really need the power of Transformers here.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer 模型更适合长序列。在我们的例子中，序列长度为 256。完成下一个单词所需的大部分上下文通常是局部的，因此我们这里并不需要 Transformers
    的强大能力。
- en: Conclusion
  id: totrans-123
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: We saw how we can solve very practical NLP problems using deep learning techniques
    based on LSTM (RNN) and Transformer models. Not every language task requires the
    use of models with billions of parameters. Specialized applications that require
    modeling language itself, and not memorizing large volumes of information can
    be handled using much smaller models that can be deployed easily and more efficiently
    than the massive language models that we are used to seeing these days.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到如何使用基于 LSTM（RNN）和 Transformer 模型的深度学习技术解决非常实际的 NLP 问题。并非所有语言任务都需要使用拥有数十亿参数的模型。对于需要建模语言本身而不是记忆大量信息的专业应用，可以使用更小的模型，这些模型比我们现在常见的大型语言模型更容易部署和更高效。
- en: All the image(s) except for the first one were created by the author(s).
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 除了第一张图片外，所有的图片都是由作者创建的。
