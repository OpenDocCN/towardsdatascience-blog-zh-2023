- en: 'The History of Open-Source LLMs: Imitation and Alignment (Part Three)'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 开源LLMs的历史：模仿与对齐（三）
- en: 原文：[https://towardsdatascience.com/the-history-of-open-source-llms-imitation-and-alignment-part-three-603d709c7aa5](https://towardsdatascience.com/the-history-of-open-source-llms-imitation-and-alignment-part-three-603d709c7aa5)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/the-history-of-open-source-llms-imitation-and-alignment-part-three-603d709c7aa5](https://towardsdatascience.com/the-history-of-open-source-llms-imitation-and-alignment-part-three-603d709c7aa5)
- en: Open-source LLMs need alignment to become truly remarkable…
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 开源LLMs需要对齐才能真正出色……
- en: '[](https://wolfecameron.medium.com/?source=post_page-----603d709c7aa5--------------------------------)[![Cameron
    R. Wolfe, Ph.D.](../Images/52bb88d7cf1105501be2fae5ccbe7a03.png)](https://wolfecameron.medium.com/?source=post_page-----603d709c7aa5--------------------------------)[](https://towardsdatascience.com/?source=post_page-----603d709c7aa5--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----603d709c7aa5--------------------------------)
    [Cameron R. Wolfe, Ph.D.](https://wolfecameron.medium.com/?source=post_page-----603d709c7aa5--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://wolfecameron.medium.com/?source=post_page-----603d709c7aa5--------------------------------)[![Cameron
    R. Wolfe, Ph.D.](../Images/52bb88d7cf1105501be2fae5ccbe7a03.png)](https://wolfecameron.medium.com/?source=post_page-----603d709c7aa5--------------------------------)[](https://towardsdatascience.com/?source=post_page-----603d709c7aa5--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----603d709c7aa5--------------------------------)
    [Cameron R. Wolfe, Ph.D.](https://wolfecameron.medium.com/?source=post_page-----603d709c7aa5--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----603d709c7aa5--------------------------------)
    ·20 min read·Nov 28, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page-----603d709c7aa5--------------------------------)
    ·阅读时间20分钟·2023年11月28日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/09529330477dcea0f682d4764a7fe0fc.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/09529330477dcea0f682d4764a7fe0fc.png)'
- en: (Photo by [Joanna Kosinska](https://unsplash.com/@joannakosinska?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)
    on [Unsplash](https://unsplash.com/photos/brown-paper-and-black-pen-B6yDtYs2IgY?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash))
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: （照片由[Joanna Kosinska](https://unsplash.com/@joannakosinska?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)提供，发布在[Unsplash](https://unsplash.com/photos/brown-paper-and-black-pen-B6yDtYs2IgY?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)）
- en: A majority of prior research on open-source large language models (LLMs) focused
    heavily upon creating pre-trained base models. However, these models have not
    undergone any fine-tuning, so they fail to match the quality of top closed-source
    LLMs (e.g., ChatGPT or Claude) due to their lack of alignment. Paid models are
    aligned extensively using techniques like SFT and RLHF, which greatly enhances
    their usability. In comparison, open-source models are typically fine-tuned to
    a lesser extent using smaller, public datasets. Within this overview, however,
    we will take a look at recent research that aims to improve the quality of open-source
    LLMs via more extensive fine-tuning and alignment.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 之前关于开源大型语言模型（LLMs）的研究大多数集中在创建预训练基础模型上。然而，这些模型并未经过任何微调，因此由于缺乏对齐，它们无法与顶级闭源LLMs（例如，ChatGPT
    或 Claude）的质量相匹配。付费模型通过使用诸如SFT和RLHF等技术进行广泛对齐，这大大提高了其可用性。相比之下，开源模型通常使用较小的公开数据集进行较少的微调。然而，在这篇概述中，我们将审视近期研究，旨在通过更广泛的微调和对齐来提升开源LLMs的质量。
- en: '![](../Images/1a77e11ba9dd423a60dcfd943e2a772e.png)'
  id: totrans-9
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1a77e11ba9dd423a60dcfd943e2a772e.png)'
- en: (from [1, 2, 12])
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: （来自[1, 2, 12]）
- en: This overview is the third (and final) part of my series on the history of open-source
    LLMs. In the [first part](/the-history-of-open-source-llms-early-days-part-one-d782bcd8f7e8)
    of the series, we looked at initial attempts at creating open-source language
    models. Although these initial pre-trained LLMs performed poorly, they were quickly
    followed up by much better open-source base models, which we covered in [part
    two](https://medium.com/towards-data-science/the-history-of-open-source-llms-better-base-models-part-two-6ca51ae74ebe)
    of this series. Now, we will cover how these better open-source models can be
    fine-tuned/aligned to improve their quality and close the gap in performance between
    open-source and proprietary LLMs, completing the journey from initial models like
    OPT to the incredibly high-performing open-source LLMs that we have today (e.g.,
    LLaMA-2-Chat).
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 本概述是我关于开源LLMs历史系列的第三部分（也是最后一部分）。在系列的[第一部分](/the-history-of-open-source-llms-early-days-part-one-d782bcd8f7e8)中，我们回顾了创建开源语言模型的初步尝试。虽然这些初步预训练的LLMs表现不佳，但它们很快被更好的开源基础模型所取代，我们在系列的[第二部分](https://medium.com/towards-data-science/the-history-of-open-source-llms-better-base-models-part-two-6ca51ae74ebe)中进行了介绍。现在，我们将探讨如何对这些更好的开源模型进行微调/对齐，以提高其质量并缩小开源LLMs与专有LLMs之间的性能差距，完成从最初的模型如OPT到我们今天拥有的高性能开源LLMs（例如LLaMA-2-Chat）的历程。
- en: '![](../Images/83a43fe06b6c5fd20d6c2670a174813a.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/83a43fe06b6c5fd20d6c2670a174813a.png)'
- en: (from [17, 18])
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: (来自 [17, 18])
- en: '**The alignment process.** This overview will study the fine-tuning and alignment
    process for open-source LLMs. Prior to studying research in this area, however,
    we need to understand what alignment is and how it is accomplished. We should
    recall that the training process for language models proceeds in several parts.
    As shown above, we begin with pre-training, which is followed by several fine-tuning
    steps. After pre-training, the LLM can accurately perform next token prediction,
    but its output may be repetitive and uninteresting. Thus, the model needs to be
    fine-tuned to improve its *alignment*, or its ability to generate text that aligns
    with the desires of a human user (e.g., follow instructions, avoid harmful output,
    avoid lying, produce interesting or creative output, etc.).'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '**对齐过程。** 本概述将研究开源LLMs的微调和对齐过程。然而，在研究该领域的研究之前，我们需要了解对齐是什么以及如何实现。我们应该记住，语言模型的训练过程分为几个部分。如上所示，我们从预训练开始，随后进行几个微调步骤。在预训练之后，LLM可以准确地进行下一个令牌预测，但其输出可能会重复且不有趣。因此，模型需要进行微调以改善其*对齐*，即生成与人类用户的期望一致的文本的能力（例如，遵循指令、避免有害输出、避免撒谎、产生有趣或创造性的输出等）。'
- en: '![](../Images/ac091974fab99dc79bc6c2c9734df861.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ac091974fab99dc79bc6c2c9734df861.png)'
- en: (from [17])
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: (来自 [17])
- en: '**SFT.** Alignment is accomplished via two fine-tuning techniques: supervised
    fine-tuning (SFT) and reinforcement learning from human feedback (RLHF); see above
    for a depiction. SFT simply fine-tunes the model, using a standard language modeling
    objective, over examples of high-quality prompt and response pairs. The LLM is
    allowed to see examples of how it should respond and learn from these responses!
    SFT is incredibly simple and effective, but it requires carefully curating a dataset
    that captures “correct” behavior.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '**SFT**。对齐是通过两种微调技术实现的：监督微调（SFT）和从人类反馈中进行的强化学习（RLHF）；见上图。SFT只是利用标准的语言建模目标，对高质量提示和响应对的示例进行微调。LLM可以看到应如何响应的示例，并从这些响应中学习！SFT非常简单且有效，但需要仔细策划一个能够捕捉“正确”行为的数据集。'
- en: '**RLHF** trains the LLM directly on feedback from human annotators — *humans
    identify outputs that they like, and the LLM learns how to produce more outputs
    like this*. To do this, we first obtain a set of prompts and generate several
    different outputs from the LLM on each prompt. Using a group of human annotators,
    we score each of these responses based on their quality. These scores can then
    be used to train a reward model (i.e., just a fine-tuned version of our LLM with
    an added regression head) to predict the score of a response. From here, RLHF
    fine-tunes the model to maximize this score using a reinforcement learning algorithm
    called PPO. Typically, the highest-performing LLMs are aligned by performing both
    SFT and RLHF (with lots of human feedback) in sequence.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '**RLHF** 直接根据人类注释者的反馈训练大型语言模型（LLM）—— *人们识别他们喜欢的输出，LLM 学习如何产生更多类似的输出*。为此，我们首先获得一组提示，并从LLM生成多个不同的输出。通过一组人类注释者，我们根据这些响应的质量对每个响应进行评分。这些评分可以用来训练一个奖励模型（即，带有附加回归头的微调版LLM），以预测响应的分数。然后，RLHF
    使用名为PPO的强化学习算法来微调模型，以最大化该分数。通常，最高性能的LLM是通过按顺序执行SFT和RLHF（并获得大量人类反馈）来对齐的。'
- en: Imitation Learning
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模仿学习
- en: '![](../Images/e8b7bce991020cec584a6ebd5e46eaec.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e8b7bce991020cec584a6ebd5e46eaec.png)'
- en: (from [16])
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: （引自[16]）
- en: With the release of LLaMA [3], the open-source research community finally had
    access to powerful base LLMs that could be fine-tuned or aligned for a variety
    of different applications. As such, LLaMa catalyzed an explosion of open-source
    LLM research, as practitioners rushed to fine-tune LLaMA models on their task
    of choice. Interestingly, one of the most common directions of research during
    this time was *imitation learning*. Imitation learning, which is (arguably) a
    form of alignment, fine-tunes an LLM over outputs from another, more powerful
    LLM. Such an approach is inspired by the idea of knowledge distillation; see above.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 随着LLaMA [3] 的发布，开源研究社区终于可以访问强大的基础LLM，这些LLM可以被微调或对齐以适应各种不同的应用。因此，LLaMA催化了开源LLM研究的爆炸性增长，实践者们纷纷急于在自己选择的任务上微调LLaMA模型。有趣的是，在此期间最常见的研究方向之一是*模仿学习*。模仿学习（可以说）是一种对齐形式，通过另一个更强大的LLM的输出来微调LLM。这种方法的灵感来源于知识蒸馏的理念；见上文。
- en: “The premise of model imitation is that once a proprietary LM is made available
    via API, one can collect a dataset of API outputs and use it to fine-tune an open-source
    LM.” *— from [6]*
  id: totrans-23
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “模型模仿的前提是，一旦通过API提供了专有语言模型（LM），就可以收集API输出的数据集，并使用这些数据来微调开源语言模型。” *— 引自[6]*
- en: 'The question posed by open-source imitation learning research was simple: *can
    we create a model that is as powerful as ChatGPT or GPT-4 by just fine-tuning
    on responses from these models?* To test this out, we can follow a simple approach:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 开源模仿学习研究提出的问题很简单：*我们是否可以仅通过对这些模型的响应进行微调，创建一个与ChatGPT或GPT-4一样强大的模型？* 为了测试这一点，我们可以采取一种简单的方法：
- en: Collect dialogue examples from these models (e.g., using the OpenAI API).
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 收集这些模型的对话示例（例如，使用OpenAI API）。
- en: Perform (supervised) fine-tuning on this data (i.e., using a normal language
    modeling objective).
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对这些数据进行（监督）微调（即，使用正常的语言建模目标）。
- en: As we will see, the research community hotly debated whether imitation learning
    was a valuable approach for quite some time! In the end, we found out that the
    approach is practically useful, but it only works well under certain conditions.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们将看到的，研究界对模仿学习是否是一种有价值的方法进行了激烈的讨论！最后，我们发现这种方法在实践中确实有用，但仅在某些条件下效果良好。
- en: Initial Efforts in Imitation Learning
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模仿学习的初步努力
- en: '![](../Images/97cfd5f7a37fc8e420426e833bd4c0ae.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/97cfd5f7a37fc8e420426e833bd4c0ae.png)'
- en: LLaMA catalyzed the creation of numerous imitation models (from [7, 8, 9, 10])
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: LLaMA催化了众多模仿模型的创建（引自[7, 8, 9, 10]）
- en: After the release of LLaMA, researchers quickly began to release a variety of
    imitation models using dialogue derived from ChatGPT. Typically, the data used
    for training — *which prohibits the resulting model from being used commercially*
    — is obtained either from the OpenAI API or sources like [ShareGPT](https://sharegpt.com/).
    A few of the most widely-known imitation models are outlined below (in chronological
    order).
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在LLaMA发布后，研究人员迅速开始发布各种使用ChatGPT对话派生的模仿模型。通常，用于训练的数据——*这会禁止结果模型用于商业用途*——来自OpenAI
    API或类似[ShareGPT](https://sharegpt.com/)的来源。以下是一些最广为人知的模仿模型（按时间顺序列出）。
- en: '**Alpaca [7]** fine-tunes LLaMA-7B by using the self-instruct [11] framework
    to automatically collect a fine-tuning dataset from [GPT-3.5](https://platform.openai.com/docs/models/gpt-3-5)
    (i.e., `text-davinci-003`). Collecting data and fine-tuning Alpaca costs only
    $600.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '**Alpaca [7]** 使用 self-instruct [11] 框架对 LLaMA-7B 进行微调，从 [GPT-3.5](https://platform.openai.com/docs/models/gpt-3-5)（即，`text-davinci-003`）自动收集微调数据集。收集数据和微调
    Alpaca 仅花费了 $600。'
- en: '**Vicuna [8]** fine-tunes LLaMA-13B over 70K dialogue examples from ChatGPT
    (i.e., derived from ShareGPT). Interestingly, the entire fine-tuning process for
    Vicuna costs only $300.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '**Vicuna [8]** 对来自 ChatGPT（即，源自 ShareGPT）的 70K 对话示例进行微调。有趣的是，Vicuna 的整个微调过程仅花费了
    $300。'
- en: '**Koala [9]** fine-tunes LLaMA-13B on a large dataset of dialogue examples
    from both the Alpaca fine-tuning set and a variety of other sources like [ShareGPT](https://sharegpt.com/),
    [HC3](https://huggingface.co/datasets/Hello-SimpleAI/HC3), [OIG](https://laion.ai/blog/oig-dataset/),
    [Anthropic HH](https://huggingface.co/datasets/Anthropic/hh-rlhf), and OpenAI
    [WebGPT](https://huggingface.co/datasets/openai/webgpt_comparisons)/[Summarization](https://huggingface.co/datasets/openai/summarize_from_feedback).
    Compared to prior imitation models, Koala is fine-tuned over a larger dataset
    and evaluated more extensively.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '**Koala [9]** 在来自 Alpaca 微调集和各种其他来源（如 [ShareGPT](https://sharegpt.com/)、[HC3](https://huggingface.co/datasets/Hello-SimpleAI/HC3)、[OIG](https://laion.ai/blog/oig-dataset/)、[Anthropic
    HH](https://huggingface.co/datasets/Anthropic/hh-rlhf) 和 OpenAI [WebGPT](https://huggingface.co/datasets/openai/webgpt_comparisons)/[Summarization](https://huggingface.co/datasets/openai/summarize_from_feedback)）的大量对话示例数据集上对
    LLaMA-13B 进行微调。与之前的模仿模型相比，Koala 在更大的数据集上进行微调，并进行了更广泛的评估。'
- en: '**GPT4ALL [16]** fine-tunes LLaMA-7B on over 800K chat completions from `GPT-3.5-turbo`.
    Along with the model, authors release both training/inference code and quantized
    model weights that can be used to perform inference with minimal compute resources
    (e.g., a laptop).'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '**GPT4ALL [16]** 在 `GPT-3.5-turbo` 的 80 万个聊天完成数据上对 LLaMA-7B 进行微调。除了模型，作者还发布了训练/推理代码和量化的模型权重，可以用来在最小计算资源（例如，笔记本电脑）下进行推理。'
- en: '![](../Images/b4be15e264d09efec80cf2c456aac282.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b4be15e264d09efec80cf2c456aac282.png)'
- en: (from [8\. 9])
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: （来自 [8\. 9]）
- en: '**The impact of imitation.** These models were published in close succession
    and claimed to achieve comparably quality to top proprietary models like ChatGPT
    and GPT-4\. For example, Vicuna is found to maintain 92% of the quality of GPT-4,
    while Koala is found to match or exceed the quality of ChatGPT in many cases;
    see above. Such findings seemed to indicate that model imitation could be used
    to distill the capabilities of any proprietary model into a smaller, open-source
    LLM. If this were true, the quality of even the best proprietary LLMs could be
    easily replicated and these models would be left with [no true advantage](https://www.semianalysis.com/p/google-we-have-no-moat-and-neither).'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '**模仿的影响。** 这些模型在短时间内发布，并声称能够达到类似于顶级专有模型如 ChatGPT 和 GPT-4 的质量。例如，Vicuna 被发现保持了
    GPT-4 92% 的质量，而 Koala 在许多情况下与 ChatGPT 的质量相当或超过；详见上文。这些发现似乎表明，模型模仿可以用于将任何专有模型的能力提炼到一个更小的开源
    LLM 中。如果这是真的，即使是最好的专有 LLM 的质量也可以轻松复制，这些模型将没有 [真正的优势](https://www.semianalysis.com/p/google-we-have-no-moat-and-neither)。'
- en: “Open-source models are faster, more customizable, more private, and … more
    capable. They are doing things with $100 and 13B params that [Google] struggles
    with at $10M and 540B. And they are doing so in weeks, not months.” *— from [9]*
  id: totrans-39
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “开源模型更快、更可定制、更私密，并且……更有能力。它们用 $100 和 13B 参数做的事情， [Google] 在 $10M 和 540B 的条件下都难以做到。而且它们是在几周内完成的，不是几个月。”
    *— 来自 [9]*
- en: The explosion of imitation models was one of the first instances in which open-source
    models were truly seen as a potential alternative to the closed-source LLMs that
    had dominated the LLM landscape [since the proposal of GPT-3](https://openai.com/blog/openai-api).
    Despite the use of paid APIs becoming standard, the impressive performance of
    imitation models fostered a feeling of promise for open-source LLMs.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 模仿模型的爆炸性增长是开源模型首次真正被视为对主导 LLM 领域的闭源 LLM 的潜在替代品的实例之一 [自 GPT-3 提议以来](https://openai.com/blog/openai-api)。尽管付费
    API 的使用已成为标准，但模仿模型的惊人表现激发了对开源 LLM 的承诺感。
- en: Are imitation models a false promise?
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模仿模型是否是虚假的承诺？
- en: '![](../Images/6061f398d45ebc75a6b1a2fb047f72bc.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6061f398d45ebc75a6b1a2fb047f72bc.png)'
- en: (from [6])
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: （来自 [6]）
- en: Despite the promise of imitation models’ impressive performance, we see in [6]
    that we are missing something important. Namely, more targeted evaluations of
    these models reveal that they do not perform nearly as well as top proprietary
    LLMs like ChatGPT and GPT-4\. In fact, we see that fine-tuning a base model via
    imitation actually does very little to close the gap in performance between open-source
    and proprietary models in most cases. Rather, the resulting model tends to only
    improve in performance on tasks that are heavily represented in the fine-tuning
    set and may even have a more pronounced tendency for hallucination.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管模仿模型表现出色的前景，但我们在[6]中看到我们遗漏了一些重要的东西。即，更具针对性的评估表明，这些模型的表现远不如顶级专有 LLM，如 ChatGPT
    和 GPT-4。事实上，我们看到，通过模仿微调基础模型实际上几乎没有缩小开源模型和专有模型之间的性能差距。相反，结果模型往往只在微调集大量表示的任务上性能有所提升，甚至可能有更明显的幻想倾向。
- en: '![](../Images/206639a640feaaf12da8f2a229df06bd.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/206639a640feaaf12da8f2a229df06bd.png)'
- en: (from [6])
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: （来自 [6]）
- en: '**Experimental setup.** To determine the utility of imitation learning, authors
    in [6] curate a dataset of ~130K diverse dialogue examples from ChatGPT. Then,
    several different sizes of language models are fine-tuned over various amounts
    of imitation data before having their performance measured. As shown above, there
    are a few interesting observations that we can make from these experiments:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '**实验设置。** 为了确定模仿学习的实用性，[6]中的作者从 ChatGPT 中策划了约 130K 个多样化对话示例的数据集。然后，对各种规模的语言模型进行不同数量的模仿数据微调，然后测量它们的性能。如上所示，我们可以从这些实验中得出一些有趣的观察结果：'
- en: The amount of imitation data used for fine-tuning does not improve model quality
    in human evaluation trials.
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于微调的模仿数据量不会在人工评估试验中改善模型质量。
- en: Imitation models’ performance on standardized benchmarks is often worse than
    that of the base model (and deteriorates as more imitation data is used).
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模仿模型在标准化基准上的表现通常比基础模型差（并且随着更多模仿数据的使用而恶化）。
- en: Increasing the size of the base model consistently improves the quality of the
    resulting imitation models.
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 增加基础模型的规模会一致地提高结果模仿模型的质量。
- en: '**What is going on here?** When imitation models are evaluated across a wider
    variety of natural language benchmarks, we see that their performance is comparable
    to or below that of the corresponding base LLM. In other words, *imitation models
    do not actually match the quality of models like ChatGPT*. Compared to proprietary
    LLMs, these models have a less extensive knowledge base, as revealed by the performance
    improvement observed with larger base models.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '**这里发生了什么？** 当对模仿模型进行更广泛的自然语言基准评估时，我们发现它们的表现与相应的基础 LLM 相当或更差。换句话说，*模仿模型实际上无法匹敌像
    ChatGPT 这样的模型质量*。与专有 LLM 相比，这些模型的知识基础不够广泛，正如通过更大基础模型观察到的性能提升所揭示的那样。'
- en: “We argue that the highest leverage action for improving open-source models
    is to tackle the difficult challenge of developing better base LMs, rather than
    taking the shortcut of imitating proprietary systems.” *— from [6]*
  id: totrans-52
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “我们认为，提高开源模型的最高杠杆作用是解决开发更好的基础语言模型的困难挑战，而不是走捷径模仿专有系统。” *— 来自 [6]*
- en: 'With this in mind, the first question we might have is: *why did it seem like
    these models performed so well?* We see in [6] that imitation models learn to
    mimic the style of a model like ChatGPT. As such, human workers can be tricked
    into perceiving the model as high-quality even if it generates factually incorrect
    information more frequently (i.e., this is harder to easily check or verify).'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于此，我们可能第一个问题是：*为什么这些模型看起来表现如此出色？* 我们在[6]中看到，模仿模型学习模仿像 ChatGPT 这样的模型的风格。因此，人类工作者可能会被误导，认为该模型质量很高，即使它生成的信息更频繁地事实不准确（即，这更难以轻易检查或验证）。
- en: Is imitation learning actually useful?
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模仿学习真的有用吗？
- en: “Our research indicates that learning from step-by-step explanations, whether
    these are generated by humans or more advanced AI models, is a promising direction
    to improve model capabilities and skills.” *— from [1]*
  id: totrans-55
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “我们的研究表明，无论这些解释是由人类还是更先进的 AI 模型生成，从逐步解释中学习是提高模型能力和技能的一个有前途的方向。” *— 来自 [1]*
- en: 'After work in [6] revealed that imitation models did not perform nearly as
    well as initially thought, the research community was unclear whether imitation
    models actually had any value. Notably, analysis in [6] indicates that local imitation
    — *or learning to imitate a model’s behavior on a specific task, instead of imitating
    its behavior as a whole —* is quite effective. However, this does not mean the
    imitation model matches the quality of proprietary models more generally. To make
    imitation models better in general, authors in [6] pose two paths forward:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [6] 的研究揭示模仿模型的表现远不如最初预期之后，研究界对模仿模型是否真正有价值感到不确定。值得注意的是，[6] 的分析表明，局部模仿 —— *即在特定任务上学习模仿模型的行为，而不是整体模仿其行为*
    —— 是相当有效的。然而，这并不意味着模仿模型在整体上能够匹配专有模型的质量。为了使模仿模型在整体上更好，[6] 的作者提出了两个前进方向：
- en: Generating a much bigger and more comprehensive imitation dataset
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生成一个更大、更全面的模仿数据集
- en: Creating a better base model to use for imitation learning
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建一个更好的基础模型以用于模仿学习
- en: Interestingly, both of these recommendations were explored extensively by subsequent
    research and found to yield positive results.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是，这些建议都被后续的研究广泛探讨，并发现其效果积极。
- en: '![](../Images/47c1dc31d49b7f02ce1e25a3703ec4d6.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/47c1dc31d49b7f02ce1e25a3703ec4d6.png)'
- en: (from [12])
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: （来自 [12]）
- en: '**Orca [12]** is an imitation model based upon LLaMA-13B. Compared to prior
    work on imitation learning, however, Orca is trained over a higher-quality, more
    detailed, and more comprehensive dataset collected from ChatGPT and GPT-4\. In
    particular, prior datasets collected for imitation learning can be considered
    “shallow” — they are simply examples of prompt and response pairs generated by
    a model like ChatGPT; see above.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '**Orca [12]** 是一个基于 LLaMA-13B 的模仿模型。然而，与先前的模仿学习工作相比，Orca 是在从 ChatGPT 和 GPT-4
    收集的更高质量、更详细、更全面的数据集上训练的。特别是，之前为模仿学习收集的数据集可以被认为是“浅层”的 —— 它们只是由类似 ChatGPT 的模型生成的提示和响应对的示例；见上文。'
- en: “We conclude that broadly matching ChatGPT using purely imitation would require
    a concerted effort to collect enormous imitation datasets and far more diverse
    and higher quality imitation data than is currently available.” *— from [6]*
  id: totrans-63
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “我们得出结论，要通过纯粹的模仿广泛匹配 ChatGPT，需要大力收集巨大的模仿数据集，并且需要比目前可用的更多样化和更高质量的模仿数据。” *— 来自
    [6]*
- en: 'Improving upon shallow imitation, Orca attempts to augment imitation datasets
    generated by models like ChatGPT or GPT-4 with:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在浅层模仿的基础上，Orca 试图通过以下方式增强由 ChatGPT 或 GPT-4 生成的模仿数据集：
- en: Explanation traces
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 解释追踪
- en: Step-by-step thought processes
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 步骤思考过程
- en: Complex instructions
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 复杂的指令
- en: To do this, the model being imitated is prompted to provide detailed explanations
    of its response via an instruction or system message. Such an approach goes beyond
    simple prompt-response pairs by adding extra, useful information to the data seen
    by an imitation model. When learning from powerful LLMs like ChatGPT, Orca sees
    more than just the model’s response. Namely, it can learn from detailed explanations
    and thought processes generated along with the model’s response on complex prompts!
    See below for an illustration.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 为此，被模仿的模型会通过指令或系统消息被提示提供其响应的详细解释。这种方法超越了简单的提示-响应对，通过为模仿模型所见的数据添加额外的有用信息。当从像
    ChatGPT 这样强大的 LLM 学习时，Orca 不仅仅看到模型的响应。即，它可以从与模型响应一起生成的详细解释和思考过程中学习复杂的提示！见下文的插图。
- en: '![](../Images/fcfe4674c54960734d383ec64f2eaf87.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fcfe4674c54960734d383ec64f2eaf87.png)'
- en: (from [12])
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: （来自 [12]）
- en: After being fine-tuned over a massive dataset of such detailed imitation data
    (i.e., 5M examples from ChatGPT and 1M examples from GPT-4), we see that Orca
    performs incredibly well compared to prior imitation models; see below.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在经过大量这样的详细模仿数据（即来自 ChatGPT 的 500 万个示例和来自 GPT-4 的 100 万个示例）进行微调之后，我们看到 Orca 相比于先前的模仿模型表现极为出色；见下文。
- en: '![](../Images/84a8f7eec9fe68743ed2e4a23f5dfc24.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/84a8f7eec9fe68743ed2e4a23f5dfc24.png)'
- en: (from [12])
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: （来自 [12]）
- en: Although Orca significantly narrows the gap between open-source imitation models
    and proprietary LLMs, we still see in the table below that the model is outperformed
    consistently by GPT-4\. Unfortunately, even an improved imitation approach is
    not enough to fully match the quality of top proprietary models.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管 Orca 显著缩小了开源模仿模型与专有 LLM 之间的差距，但我们仍然看到下表中该模型始终被 GPT-4 超越。不幸的是，即使改进的模仿方法也不足以完全匹配顶级专有模型的质量。
- en: '![](../Images/b0baf2a72f3ebbbe331a4f92c4a2c430.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b0baf2a72f3ebbbe331a4f92c4a2c430.png)'
- en: (from [12])
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: （来自 [12]）
- en: 'Nonetheless, Orca’s impressive performance reveals that imitation learning
    is a valuable fine-tuning strategy that can drastically improve the performance
    of any high-quality base LLM. Going further, we learn in [12] that leveraging
    imitation learning successfully has two main requirements:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管如此，Orca的出色表现表明，模仿学习是一种有价值的微调策略，可以大幅提升任何高质量基础LLM的性能。进一步来看，我们在 [12] 中了解到，成功利用模仿学习有两个主要要求：
- en: A large, comprehensive imitation dataset
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个大型、全面的模仿数据集
- en: Detailed explanation traces within each response
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个响应中的详细解释跟踪
- en: '**Better base LLMs.** Although authors in [6] argue that collecting a sufficiently
    large and diverse imitation learning dataset is incredibly difficult, we see with
    Orca that such a feat is at least possible. Additionally, later work extensively
    explores the alternative suggestion in [6]: *creating more powerful (open-source)
    base models*. Although open-source pre-trained LLMs performed poorly at first,
    we have recently seen the proposal of a variety of powerful pre-trained LLMs;
    e.g., LLaMA [3], MPT [14, 15], and Falcon [13]. Given that model pre-training
    is a starting point for any fine-tuning that follows (e.g., imitation learning,
    SFT, RLHF, etc.), starting with a better base model improves the downstream imitation
    model as well! Luckily, we covered all of the best open-source, pre-trained language
    models in part two of this series. See [here](https://medium.com/towards-data-science/the-history-of-open-source-llms-better-base-models-part-two-6ca51ae74ebe)
    for more details.'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '**更好的基础LLM。** 尽管 [6] 中的作者认为收集足够大且多样的模仿学习数据集极为困难，但我们从Orca中看到，这样的成就至少是可能的。此外，后续工作广泛探索了
    [6] 中的替代建议：*创建更强大的（开源）基础模型*。虽然开源预训练LLM最初表现不佳，但我们最近看到了一些强大的预训练LLM的提议；例如，LLaMA [3]、MPT
    [14, 15] 和 Falcon [13]。鉴于模型预训练是后续任何微调（例如，模仿学习、SFT、RLHF等）的起点，从更好的基础模型开始也能提升下游模仿模型的质量！幸运的是，我们在本系列的第二部分中涵盖了所有最佳的开源预训练语言模型。更多详细信息，请见
    [这里](https://medium.com/towards-data-science/the-history-of-open-source-llms-better-base-models-part-two-6ca51ae74ebe)。'
- en: Aligning Open-Source LLMs
  id: totrans-81
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 对齐开源LLM
- en: '![](../Images/aa181a9101c5231ac41a5af68680489a.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/aa181a9101c5231ac41a5af68680489a.png)'
- en: (from [5])
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: （摘自 [5]）
- en: Imitation learning attempted to improve the quality of open-source base models
    by training over the responses (and explanation traces) of proprietary LLMs. Although
    this approach is successful in some cases, this is (obviously) not the manner
    in which the top proprietary models are trained — *imitation is a short cut for
    creating powerful open-source models*. If we want open-source LLMs that rival
    the quality of proprietary models, we need to invest significantly into alignment.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 模仿学习试图通过训练专有LLM的响应（和解释跟踪）来提高开源基础模型的质量。虽然这种方法在某些情况下是成功的，但（显然）这并不是顶级专有模型的训练方式——*模仿是创建强大开源模型的捷径*。如果我们想要与专有模型质量相媲美的开源LLM，我们需要在对齐上投入大量资源。
- en: “These closed product LLMs are heavily fine-tuned to align with human preferences,
    which greatly enhances their usability and safety. This step can require significant
    costs in compute and human annotation, and is often not transparent or easily
    reproducible.” *— from [1]*
  id: totrans-85
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “这些封闭产品LLM被大量微调以与人类偏好对齐，这大大提高了它们的可用性和安全性。这一步骤可能需要大量的计算和人工注释成本，而且通常不透明或不容易复制。”
    *— 摘自 [1]*
- en: '**What’s the hold up?** The idea of aligning open-source imitation models seems
    easy enough. We have really great base models, *why not just replicate the alignment
    process used by models like GPT-4?* The alignment process requires extensive compute
    and human annotation resources. Plus, it is heavily dependent upon proprietary
    data, which limits transparency and makes reproducing results quite difficult.
    As such, open-source models have lagged behind their proprietary counterparts
    in alignment research for quite some time. Within this section, however, we will
    explore two recent works — LIMA [2] and LLaMA-2 [1] — that drastically improve
    the quality of open-source LLMs via better alignment.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题出在哪里？** 对齐开源模仿模型的想法似乎很简单。我们有很棒的基础模型，*为什么不直接复制像GPT-4这样的模型使用的对齐过程？* 对齐过程需要大量的计算和人工注释资源。此外，它严重依赖于专有数据，这限制了透明度，使得结果复制变得非常困难。因此，开源模型在对齐研究方面长期以来落后于专有模型。然而，在本节中，我们将探讨两项最新的工作——LIMA
    [2] 和 LLaMA-2 [1]——它们通过更好的对齐极大地提高了开源LLM的质量。'
- en: Prior Work on Open-Source Alignment
  id: totrans-87
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 先前的开源对齐工作
- en: Before covering LIMA and LLaMA-2, it is important to note that the open-source
    research community has not avoided aligning pre-trained models altogether. For
    example, Falcon-40B-Instruct [13] undergoes SFT over 150M token of data from [Baize](https://github.com/project-baize/baize-chatbot).
    Similarly, numerous fine-tuned variants of MPT-7B [14] and MPT-30B [15] have been
    released, including both chat/instruct variants that undergo SFT on public datasets
    and a StoryWriter variant that is fine-tuned over data with a much longer context
    length.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在讨论 LIMA 和 LLaMA-2 之前，需要注意的是，开源研究社区并没有完全避免对预训练模型的对齐。例如，Falcon-40B-Instruct [13]
    在 [Baize](https://github.com/project-baize/baize-chatbot) 上进行超过 150M token 数据的
    SFT。同样，许多经过微调的 MPT-7B [14] 和 MPT-30B [15] 变体也已发布，包括那些在公共数据集上进行 SFT 的聊天/指令变体和一个在具有更长上下文长度的数据上进行微调的
    StoryWriter 变体。
- en: '![](../Images/97dde27c493c4444d9b4f753e8b9adb5.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/97dde27c493c4444d9b4f753e8b9adb5.png)'
- en: (from the OpenLLM Leaderboard)
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: （来自 OpenLLM 排行榜）
- en: Plus, if we take a simple look at the Open LLM Leaderboard (see above), we see
    a variety of different models that have underwent fine-tuning via SFT on all types
    of different datasets. Open-source LLMs have not avoided alignment altogether.
    However, top proprietary models undergo both SFT and RLHF over massive datasets
    of high-quality dialogue and human feedback. In comparison, most open-source models
    have been aligned using solely SFT over public datasets that lack in quality and
    diversity. To truly match the quality of proprietary models, *open-source LLMs
    needed to make an attempt at replicating their alignment process.*
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，如果我们简单查看 Open LLM 排行榜（见上文），我们会看到各种不同的模型经历了在各种不同数据集上通过 SFT 进行的微调。开源 LLMs 并没有完全避免对齐。然而，顶级专有模型经历了
    SFT 和 RLHF 在大量高质量对话和人类反馈的数据集上进行的训练。相比之下，大多数开源模型仅通过 SFT 在质量和多样性不足的公共数据集上进行了对齐。为了真正匹配专有模型的质量，*开源
    LLMs 需要尝试复制其对齐过程。*
- en: 'LIMA: Data-Efficient Alignment [2]'
  id: totrans-92
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 'LIMA: 数据高效对齐 [2]'
- en: “A model’s knowledge and capabilities are learnt almost entirely during pretraining,
    while alignment teaches it which subdistribution of formats should be used when
    interacting with users.” *— from [2]*
  id: totrans-93
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “模型的知识和能力几乎完全在预训练期间获得，而对齐则教会模型在与用户交互时应使用哪种格式子分布。” *— 来自 [2]*
- en: As mentioned above, open-source LLMs — for quite some time — mostly performed
    alignment via SFT on public datasets. Given this heavy emphasis upon SFT, authors
    in [2] studied extensively the impact impact of SFT on pre-trained LLMs. The goal
    of this analysis was to uncover the relative importance of pre-training and alignment
    via SFT in creating a high-performing LLM, as well as to reveal best practices
    for maximizing a model’s performance after undergoing SFT.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 如上所述，开源 LLMs — 很长一段时间以来 — 主要通过 SFT 在公共数据集上进行对齐。鉴于对 SFT 的高度重视，[2] 中的作者广泛研究了 SFT
    对预训练 LLMs 的影响。这项分析的目的是揭示预训练和通过 SFT 对齐在创建高性能 LLM 中的相对重要性，并揭示在经过 SFT 后最大化模型性能的最佳实践。
- en: '**The dataset.** To do this, authors in [2] construct a small dataset of 1,000
    dialogue examples to use for SFT. Although this might not seem like enough data,
    the examples included in this dataset are curated to ensure quality by using diverse
    prompts and a uniform output style or tone; see below.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '**数据集。** 为此，[2] 中的作者构建了一个包含 1,000 个对话示例的小数据集以用于 SFT。尽管这可能看起来数据量不足，但该数据集中的示例经过精心策划，以确保质量，使用了多样化的提示和统一的输出风格或语气；见下文。'
- en: '![](../Images/710bc1acada4be78166c5200050186ec.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/710bc1acada4be78166c5200050186ec.png)'
- en: (from [2])
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: （来自 [2]）
- en: The SFT dataset used to train LIMA is small but of incredibly high quality.
    Interestingly, we see in [2] that LIMA performs surprisingly well when fine-tuned
    over this dataset, even approaching the performance of state-of-the-art LLMs like
    GPT-4 or Claude; see below.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 用于训练 LIMA 的 SFT 数据集虽然小，但质量极高。有趣的是，我们在 [2] 中看到 LIMA 在这个数据集上的微调表现惊人地好，甚至接近于如 GPT-4
    或 Claude 等最先进 LLMs 的性能；见下文。
- en: '![](../Images/77b7023d4c4ad933838db14d34dac813.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/77b7023d4c4ad933838db14d34dac813.png)'
- en: (from [2])
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: （来自 [2]）
- en: Such a result reveals that language models can be effectively aligned via a
    small number of carefully chosen examples. Although LIMA still falls short of
    GPT-4’s performance, the ability to perform such high-quality alignment with such
    little data is both unexpected and impressive. Such a result shows us that data
    quality is seemingly the most important factor in performing alignment via SFT.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 这样的结果表明，语言模型可以通过少量精心挑选的示例有效对齐。尽管 LIMA 的表现仍未达到 GPT-4，但能够以如此少的数据进行高质量对齐既意外又令人印象深刻。这样的结果告诉我们，数据质量似乎是通过
    SFT 进行对齐时最重要的因素。
- en: '![](../Images/da2ad328c87f2649eb949f91e5887c8e.png)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/da2ad328c87f2649eb949f91e5887c8e.png)'
- en: (from [2])
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: （来自 [2]）
- en: '**What do we learn?** We learn a variety of useful lessons from LIMA. First,
    the quality of data is incredibly important for SFT. Just using more data is not
    enough — *the data also needs to be of high quality*; see above. Additionally,
    the results in [2] lead to the proposal of the “Superficial Alignment Hypothesis”,
    which offers a new and unique perspective of alignment. Put simply, this hypothesis
    posits that most of an LLM’s core knowledge is [learned during pre-training](https://twitter.com/cwolferesearch/status/1660744247123890179?s=20),
    while alignment searches for the proper format or style for surfacing this knowledge.
    As such, alignment can be learned in a data efficient manner.'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '**我们学到了什么？** 我们从 LIMA 中学到了一些有用的经验。首先，数据的质量对于 SFT 至关重要。仅仅使用更多的数据是不够的 —— *数据也需要具有高质量*；见上文。此外，[2]
    中的结果导致了“表面对齐假说”的提出，该假说提供了对齐的新颖独特视角。简单来说，该假说认为大多数 LLM 的核心知识是在预训练期间 [学习的](https://twitter.com/cwolferesearch/status/1660744247123890179?s=20)，而对齐则寻求为呈现这些知识找到适当的格式或风格。因此，对齐可以以数据高效的方式学习。'
- en: 'LLaMA-2: Improving Transparency in Alignment Research [1]'
  id: totrans-105
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: LLaMA-2：提升对齐研究的透明度 [1]
- en: “Llama 2-Chat is the result of several months of research and iterative applications
    of alignment techniques, including both instruction tuning and RLHF, requiring
    significant computational and annotation resources.” *— from [1]*
  id: totrans-106
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “Llama 2-Chat 是几个月研究和迭代应用对齐技术的结果，包括指令调整和 RLHF，涉及大量计算和注释资源。” *— 来自 [1]*
- en: The recently-released LLaMA-2 [1] suite of LLMs is comprised of several open-source
    models with sizes ranging from 7–70 billion parameters. Compared to their predecessors
    (i.e., LLaMA-1 [3]), LLaMA-2 models differentiate themselves by pre-training over
    40% more data (i.e., 2 trillion tokens instead of 1.4 trillion), having a longer
    context length, and using an architecture that is optimized for fast inference
    (i.e., by using [grouped query attention](https://twitter.com/_philschmid/status/1673335690912825347?s=20)
    [4]).
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 最近发布的 LLaMA-2 [1] LLM 套件由多个开源模型组成，规模范围从 70 亿到 700 亿参数。与其前身（即 LLaMA-1 [3]）相比，LLaMA-2
    模型通过在超过 40% 更多的数据（即 2 万亿个标记而不是 1.4 万亿）上进行预训练、具有更长的上下文长度，并使用优化为快速推断的架构（即通过使用 [分组查询注意力](https://twitter.com/_philschmid/status/1673335690912825347?s=20)
    [4]）来区分自己。
- en: LLaMA-2 achieves state-of-the-art performance among open-source models. However,
    the LLaMA-2 suite contains more than just pre-trained LLMs. Authors invest heavily
    into the alignment process by fine-tuning each model — using both SFT and RLHF
    — over a massive amount of dialogue data and human feedback; see below. The resulting
    models are referred to as the LLaMA-2-Chat models.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: LLaMA-2 在开源模型中实现了最先进的性能。然而，LLaMA-2 套件不仅仅包含预训练的 LLMs。作者在对齐过程中投入了大量精力，通过在大量对话数据和人类反馈上对每个模型进行微调——使用
    SFT 和 RLHF——来实现这一点；见下文。最终模型被称为 LLaMA-2-Chat 模型。
- en: '![](../Images/a6512d59a0d2d6d6409ddfaf5eacbc20.png)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a6512d59a0d2d6d6409ddfaf5eacbc20.png)'
- en: (from [5])
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: （来自 [5]）
- en: 'These refined versions of LLaMA-2 perform incredibly well and take a major
    step towards closing the gap in alignment between open-source and proprietary
    LLMs. LLaMA-2’s alignment process emphasizes two key behavioral properties:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 这些精细化版本的 LLaMA-2 表现出色，并在弥合开源与专有 LLM 之间的对齐差距方面迈出了重要一步。LLaMA-2 的对齐过程强调两个关键行为特性：
- en: '*Helpfulness*: the model fulfills users’ requests and provides requested information.'
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*有用性*：模型满足用户的请求并提供所请求的信息。'
- en: 'Safety: the model avoids responses that are “unsafe”'
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 安全性：模型避免产生“安全性差”的回应
- en: To ensure that the aligned model is both helpful and safe, data curated for
    both SFT and RLHF is filtered, collected, and annotated according to these principles.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 为确保对齐后的模型既有用又安全，用于 SFT 和 RLHF 的数据会根据这些原则进行过滤、收集和注释。
- en: '![](../Images/759625144742a35309e36d163de6e316.png)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/759625144742a35309e36d163de6e316.png)'
- en: (from [1])
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: （来自 [1]）
- en: '**SFT.** The first step in LLaMA-2’s alignment process is fine-tuning with
    SFT. Similar to other open-source LLMs, LLaMA-2 is first fine-tuned over publicly-available
    instruction tuning data. However, such data tends to lack in diversity and quality,
    which — *as demonstrated by LIMA [2]* — massively impacts performance. As such,
    authors in [1] focus upon collecting a smaller set of high-quality data for SFT.
    This data comes from a variety of sources, including both manually created or
    annotated examples and data from public sources that is filtered for quality.
    Ultimately, LLaMA-2 undergoes a second stage of fine-tuning with 27,540 high-quality
    dialogue examples; see above for samples.'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '**SFT.** LLaMA-2对齐过程的第一步是使用SFT进行微调。与其他开源LLM类似，LLaMA-2首先在公开可用的指令调优数据上进行微调。然而，这类数据往往缺乏多样性和质量，*正如LIMA
    [2]所示*，这会大幅影响性能。因此，[1]中的作者专注于收集一小部分高质量数据进行SFT。这些数据来自各种来源，包括手动创建或注释的示例以及经过质量筛选的公共来源数据。最终，LLaMA-2在27,540个高质量对话示例上进行第二阶段的微调；参见上文的示例。'
- en: “Surprisingly, we found that the outputs sampled from the resulting SFT model
    were often competitive with SFT data handwritten by human annotators, suggesting
    that we could reprioritize and devote more annotation effort to preference-based
    annotation for RLHF.” *— from [1]*
  id: totrans-118
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “令人惊讶的是，我们发现从结果SFT模型中采样的输出通常与人类注释员手写的SFT数据相当具有竞争力，这表明我们可以重新调整优先级，将更多的注释工作投入到基于偏好的注释中以便于RLHF。”
    *— 来自 [1]*
- en: Interestingly, authors in [1] observe that collecting more data (i.e., beyond
    the 27K high-quality examples) for SFT provides diminishing benefits. These findings
    align with the empirical analysis from LIMA [2]. We don’t need a ton of data for
    SFT, but the data should be of high-quality! Interestingly, authors in [1] also
    note that LLaMA-2 models that have underwent SFT seem to be capable of generating
    their own data for SFT anyways.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是，[1]中的作者观察到，收集更多数据（即超出27K高质量示例）对SFT的收益递减。这些发现与LIMA [2] 的实证分析一致。我们不需要大量数据进行SFT，但数据应当具有高质量！有趣的是，[1]中的作者还指出，经过SFT的LLaMA-2模型似乎能够生成自己的SFT数据。
- en: '**RLHF.** LLaMA-2 is further fine-tuned using RLHF over a dataset of >1M examples
    of human feedback. To collect this feedback, a binary protocol is adopted, in
    which human annotators are asked to write a prompt and choose the better of two
    generated responses from the LLM. Here, human preference data is collected according
    to both helpfulness and safety standards. For example, human preference annotations
    focused upon safety may encourage the annotator to craft an adversarial prompt
    that is likely to elicit an unsafe response. Then, the human annotator can label
    which of the responses — if any — is preferable and safe.'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '**RLHF.** LLaMA-2进一步通过RLHF在超过100万例人类反馈的数据集上进行微调。为了收集这些反馈，采用了二元协议，其中人类注释员被要求写出一个提示，并选择两个LLM生成的响应中较好的一个。在这里，人类偏好数据根据有用性和安全性标准进行收集。例如，专注于安全性的人工偏好注释可能会鼓励注释员设计出可能引发不安全响应的对抗性提示。然后，人工注释员可以标记哪些响应——如果有的话——是可取的且安全的。'
- en: “Everything else being equal, an improvement of the reward model can be directly
    translated into an improvement for Llama 2-Chat.” *— from [1]*
  id: totrans-121
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “在其他条件相等的情况下，奖励模型的改进可以直接转化为Llama 2-Chat的改进。” *— 来自 [1]*
- en: Human feedback data is collected in batches, and LLaMA-2 is fine-tuned via RLHF
    between each batch. As such, several versions of each LLaMA-2-Chat model — five
    in total — are iteratively created after each trial of RLHF. In [1], we see that
    a new reward model is trained for use in RLHF each time fresh human preference
    data is collected, ensuring the reward model accurately captures human preferences
    of the latest model. Additionally, we see that the quality of the resulting reward
    model is surprisingly predictive of LLaMA-2-Chat model quality overall. In total,
    LLaMA-2 is fine-tuned on over 1M instances of human feedback throughout the entirety
    of the iterative RLHF process.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 人类反馈数据是批量收集的，LLaMA-2在每批数据之间通过RLHF进行微调。因此，每次RLHF试验后，都会迭代创建几个版本的每个LLaMA-2-Chat模型，共五个版本。在
    [1] 中，我们看到每次收集到新的人工偏好数据时，都会为RLHF训练一个新的奖励模型，确保奖励模型准确捕捉到最新模型的人类偏好。此外，我们看到生成的奖励模型的质量出奇地能够预测LLaMA-2-Chat模型的整体质量。总体来说，LLaMA-2在整个迭代RLHF过程中经过了超过100万实例的人类反馈微调。
- en: '![](../Images/e1be3b210426fae3e8054e32371fcc3f.png)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e1be3b210426fae3e8054e32371fcc3f.png)'
- en: (from [1])
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: （来自 [1]）
- en: As shown in the figure above, the quality of LLaMA-2-Chat — in terms of both
    helpfulness and safety — improves smoothly throughout the several iterations of
    alignment with both SFT and RLHF. This visualization clearly depicts the level
    of impact of each technique on the resulting model’s quality. Namely, performing
    SFT alone only gets us so far! The model’s alignment improves drastically with
    each phase of RLHF that is performed even after SFT is applied.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 如上图所示，LLaMA-2-Chat 的质量——无论是**有用性**还是**安全性**——在与 SFT 和 RLHF 多次对齐的过程中平稳提升。这一可视化清晰地描绘了每种技术对模型质量的影响程度。也就是说，仅仅进行
    SFT 不足以达到目标！即便在应用 SFT 后，模型的对齐在每个 RLHF 阶段都会显著改善。
- en: '![](../Images/1165788d2c200dd51e5a9e35f00c25ec.png)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1165788d2c200dd51e5a9e35f00c25ec.png)'
- en: All top-5 models on the Open LLM leaderboard are based upon LLaMA-2 (from Open
    LLM leaderboard)
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: Open LLM 排行榜上的前五名模型均基于 LLaMA-2（来自 Open LLM 排行榜）
- en: '**Performance.** The LLaMA-2-Chat models are currently state-of-the-art for
    open-source LLMs, as shown by the [Open LLM leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard)
    above. When LLaMA-2-Chat models are compared to other popular LLMs in [1], we
    see that they far exceed other open-source models in terms of helpfulness and
    safety; see below.'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '**性能。** 如上所示，LLaMA-2-Chat 模型目前是开源 LLM 的最先进水平，如 [Open LLM 排行榜](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard)
    所示。当将 LLaMA-2-Chat 模型与 [1] 中的其他流行 LLM 进行比较时，我们看到它们在**有用性**和**安全性**方面远远超过其他开源模型；见下文。'
- en: '![](../Images/6018d1404960abc1e8cab15a06fbc5e6.png)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6018d1404960abc1e8cab15a06fbc5e6.png)'
- en: (from [1])
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: （来自 [1]）
- en: Furthermore, LLaMA-2 is even found to perform comparably to top proprietary
    models like ChatGPT when evaluated in terms of helpfulness and safety. Put simply,
    these results heavily indicate that the quality of alignment performed for the
    LLaMA-2-Chat models is high. The resulting models tend to accurately capture and
    adhere to desired helpfulness and safety standards.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，LLaMA-2 的表现甚至与如 ChatGPT 等顶级专有模型在**有用性**和**安全性**方面相当。简单来说，这些结果强烈表明 LLaMA-2-Chat
    模型的对齐质量很高。生成的模型往往能准确地捕捉并遵循期望的**有用性**和**安全性**标准。
- en: “[Alignment] can require significant costs in compute and human annotation,
    and is often not transparent or easily reproducible, limiting progress within
    the community to advance AI alignment research.” *— from [1]*
  id: totrans-132
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “[对齐] 可能需要显著的计算和人工标注成本，并且通常不透明或不易复制，限制了社区内在推进 AI 对齐研究的进展。” *— 来自 [1]*
- en: '**The importance of LLaMA-2.** The impact of LLaMA-2 on open-source LLM research
    goes beyond simply setting a new state-of-the-art in terms of performance. *Why?*
    We see in [2] that LLaMA-2 adopts a fundamentally different approached compared
    to prior work. Due to the fact that closed-source LLMs are typically aligned with
    extensive amount of proprietary, human-annotated data, this process has been more
    difficult to replicate within open-source research. Although prior open-source
    models mostly leverage SFT and public sources of dialogue data, LLaMA-2 is one
    of the first open-source LLMs to invest extensively into the alignment process,
    curating a great deal of high-quality dialogues and human preferences for both
    SFT and RLHF.'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '**LLaMA-2 的重要性。** LLaMA-2 对开源 LLM 研究的影响不仅仅体现在性能上设立了新的最先进水平。*为什么？* 我们在 [2] 中看到，LLaMA-2
    采用了一种与以往工作 fundamentally 不同的方法。由于闭源 LLM 通常通过大量专有的人工标注数据进行对齐，这一过程在开源研究中更难以复制。尽管以往的开源模型主要利用
    SFT 和公共对话数据来源，但 LLaMA-2 是首批大量投入对齐过程的开源 LLM 之一，精心策划了大量高质量的对话和人工偏好数据用于 SFT 和 RLHF。'
- en: Closing Remarks
  id: totrans-134
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结束语
- en: We have now studied the entire journey of open-source language models from OPT
    to LLAMA-2\. Despite the incredible amount of research that occurred between these
    two models, their proposal was only a year apart! The open-source AI research
    community moves very quickly, and keeping up with research in this area is incredibly
    exciting, interesting, and rewarding. Having access to powerful models like LLaMA-2-Chat
    is humbling. As both practitioners and researchers, we have the ability to use
    these models, learn from them, and truly gain a deeper understanding of how they
    work. Such an opportunity is unique and should not be taken for granted. Especially
    for LLMs, open-source is pretty cool!
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在已经深入研究了从OPT到LLAMA-2的整个开源语言模型的发展历程。尽管这两个模型之间发生了大量研究，但它们的提议间隔仅一年！开源AI研究社区发展非常迅速，跟上这一领域的研究非常令人兴奋、有趣和回报丰厚。能够访问像LLaMA-2-Chat这样强大的模型令人谦卑。作为实践者和研究者，我们有能力使用这些模型、从中学习，并真正深入理解它们的工作原理。这样的机会是独特的，不应被视为理所当然。尤其是对于LLM来说，开源真是非常酷！
- en: Connect with me!
  id: totrans-136
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 与我联系！
- en: Thanks so much for reading this article. I am [Cameron R. Wolfe](https://cameronrwolfe.me/),
    Director of AI at [Rebuy](https://www.rebuyengine.com/). I study the empirical
    and theoretical foundations of deep learning. If you liked this overview, subscribe
    to my [Deep (Learning) Focus newsletter](https://cameronrwolfe.substack.com/),
    where I help readers understand AI research via overviews of relevant topics from
    the ground up. You can also follow me on [X](https://twitter.com/cwolferesearch)
    and [LinkedIn](https://www.linkedin.com/in/cameron-r-wolfe-ph-d-04744a238/), or
    check out my [other writings](https://medium.com/@wolfecameron) on medium!
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 非常感谢你阅读这篇文章。我是[Cameron R. Wolfe](https://cameronrwolfe.me/)，[Rebuy](https://www.rebuyengine.com/)的AI总监。我研究深度学习的经验和理论基础。如果你喜欢这个概述，订阅我的[Deep
    (Learning) Focus新闻通讯](https://cameronrwolfe.substack.com/)，我通过从基础到高级的相关话题概述来帮助读者理解AI研究。你也可以在[X](https://twitter.com/cwolferesearch)和[LinkedIn](https://www.linkedin.com/in/cameron-r-wolfe-ph-d-04744a238/)上关注我，或者查看我在medium上的[其他写作](https://medium.com/@wolfecameron)！
- en: Bibliography
  id: totrans-138
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] Touvron, Hugo, et al. “Llama 2: Open Foundation and Fine-Tuned Chat Models.”
    *arXiv preprint arXiv:2307.09288* (2023).'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] Touvron, Hugo等。“Llama 2：开放基础和微调的聊天模型。”*arXiv预印本arXiv:2307.09288*（2023年）。'
- en: '[2] Zhou, Chunting, et al. “Lima: Less is more for alignment.” *arXiv preprint
    arXiv:2305.11206* (2023).'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] Zhou, Chunting等。“Lima：对齐的少即是多。”*arXiv预印本arXiv:2305.11206*（2023年）。'
- en: '[3] Touvron, Hugo, et al. “Llama: Open and efficient foundation language models.”
    *arXiv preprint arXiv:2302.13971* (2023).'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] Touvron, Hugo等。“Llama：开放而高效的基础语言模型。”*arXiv预印本arXiv:2302.13971*（2023年）。'
- en: '[4] Ainslie, Joshua, et al. “GQA: Training Generalized Multi-Query Transformer
    Models from Multi-Head Checkpoints.” *arXiv preprint arXiv:2305.13245* (2023).'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] Ainslie, Joshua等。“GQA：从多头检查点训练广义多查询变换器模型。”*arXiv预印本arXiv:2305.13245*（2023年）。'
- en: '[5] “Introducing Llama2: The next generation of our open source large language
    model”, *Meta*, [https://ai.meta.com/llama/.](https://ai.meta.com/llama/.)'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '[5] “介绍Llama2：我们开源大型语言模型的下一代”，*Meta*，[https://ai.meta.com/llama/.](https://ai.meta.com/llama/.)'
- en: '[6] Gudibande, Arnav, et al. “The false promise of imitating proprietary llms.”
    *arXiv preprint arXiv:2305.15717* (2023).'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '[6] Gudibande, Arnav等。“模仿专有LLM的虚假承诺。”*arXiv预印本arXiv:2305.15717*（2023年）。'
- en: '[7] Taori, Rohan et al. “Stanford Alpaca: An Instruction-following LLaMA model.”
    (2023).'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '[7] Taori, Rohan等。“斯坦福阿尔帕卡：一种跟随指令的LLaMA模型。”（2023年）。'
- en: '[8] Chiang, Wei-Lin et al. “Vicuna: An Open-Source Chatbot Impressing GPT-4
    with 90%* ChatGPT Quality.” (2023).'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '[8] Chiang, Wei-Lin等。“Vicuna：一个开源聊天机器人，凭借90%* ChatGPT质量给GPT-4留下深刻印象。”（2023年）。'
- en: '[9] Geng, Xinyang et al. “Koala: A Dialogue Model for Academic Research.” (2023).'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '[9] Geng, Xinyang等。“Koala：一个用于学术研究的对话模型。”（2023年）。'
- en: '[10] Yuvanesh Anand, Zach Nussbaum, Brandon Duderstadt, Benjamin Schmidt, and
    Andriy Mulyar. GPT4All: Training an assistant-style chatbot with large scale data
    distillation from GPT-3.5-Turbo, 2023.'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '[10] Yuvanesh Anand, Zach Nussbaum, Brandon Duderstadt, Benjamin Schmidt和Andriy
    Mulyar。GPT4All：通过大规模数据提炼从GPT-3.5-Turbo训练助手风格的聊天机器人，2023年。'
- en: '[11] Wang, Yizhong, et al. “Self-instruct: Aligning language model with self
    generated instructions.” *arXiv preprint arXiv:2212.10560* (2022).'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '[11] Wang, Yizhong等。“Self-instruct：使语言模型与自生成指令对齐。”*arXiv预印本arXiv:2212.10560*（2022年）。'
- en: '[12] Mukherjee, Subhabrata, et al. “Orca: Progressive Learning from Complex
    Explanation Traces of GPT-4.” *arXiv preprint arXiv:2306.02707* (2023).'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '[12] Mukherjee, Subhabrata等。“Orca：从GPT-4的复杂解释痕迹中逐步学习。”*arXiv预印本arXiv:2306.02707*（2023年）。'
- en: '[13] “Introducing Falcon LLM”, *Technology Innovation Institute*, [https://falconllm.tii.ae/.](https://falconllm.tii.ae/.)'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '[13] “介绍 Falcon LLM”， *技术创新研究所*，[https://falconllm.tii.ae/.](https://falconllm.tii.ae/.)'
- en: '[14] “Introducing MPT-7B: A New Standard for Open-Source, Commercially Usable
    Llms.” *MosaicML*, [www.mosaicml.com/blog/mpt-7b.](http://www.mosaicml.com/blog/mpt-7b.)'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '[14] “介绍 MPT-7B：开源、商业可用 Llms 的新标准。” *MosaicML*，[www.mosaicml.com/blog/mpt-7b.](http://www.mosaicml.com/blog/mpt-7b.)'
- en: '[15] “MPT-30B: Raising the Bar for Open-Source Foundation Models.” *MosaicML*,
    [www.mosaicml.com/blog/mpt-30b.](http://www.mosaicml.com/blog/mpt-30b.)'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: '[15] “MPT-30B：提升开源基础模型的标准。” *MosaicML*，[www.mosaicml.com/blog/mpt-30b.](http://www.mosaicml.com/blog/mpt-30b.)'
- en: '[16] Gou, Jianping, et al. “Knowledge distillation: A survey.” *International
    Journal of Computer Vision* 129 (2021): 1789–1819.'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '[16] 苟建平等。“知识蒸馏：综述。” *计算机视觉国际期刊* 129 (2021)：1789–1819。'
- en: '[17] Ouyang, Long, et al. “Training language models to follow instructions
    with human feedback.” *Advances in Neural Information Processing Systems* 35 (2022):
    27730–27744.'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '[17] 欧阳龙等。“训练语言模型以遵循人类反馈的指令。” *神经信息处理系统进展* 35 (2022)：27730–27744。'
- en: '[18] Glaese, Amelia, et al. “Improving alignment of dialogue agents via targeted
    human judgements.” *arXiv preprint arXiv:2209.14375* (2022).'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: '[18] 格拉斯，阿梅利亚等。“通过针对性人类判断改善对话代理的对齐。” *arXiv 预印本 arXiv:2209.14375* (2022)。'
