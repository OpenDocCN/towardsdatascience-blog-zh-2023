- en: Time Series Forecasting with Deep Learning in PyTorch (LSTM-RNN)
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于深度学习的时间序列预测（LSTM-RNN）在 PyTorch 中的应用
- en: 原文：[https://towardsdatascience.com/time-series-forecasting-with-deep-learning-in-pytorch-lstm-rnn-1ba339885f0c](https://towardsdatascience.com/time-series-forecasting-with-deep-learning-in-pytorch-lstm-rnn-1ba339885f0c)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/time-series-forecasting-with-deep-learning-in-pytorch-lstm-rnn-1ba339885f0c](https://towardsdatascience.com/time-series-forecasting-with-deep-learning-in-pytorch-lstm-rnn-1ba339885f0c)
- en: An in depth tutorial on forecasting a univariate time series using deep learning
    with PyTorch
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 一个关于使用 PyTorch 进行单变量时间序列深度学习预测的深入教程
- en: '[](https://zainbaq.medium.com/?source=post_page-----1ba339885f0c--------------------------------)[![Zain
    Baquar](../Images/27c7941771179fe5731641930c403ff2.png)](https://zainbaq.medium.com/?source=post_page-----1ba339885f0c--------------------------------)[](https://towardsdatascience.com/?source=post_page-----1ba339885f0c--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----1ba339885f0c--------------------------------)
    [Zain Baquar](https://zainbaq.medium.com/?source=post_page-----1ba339885f0c--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://zainbaq.medium.com/?source=post_page-----1ba339885f0c--------------------------------)[![Zain
    Baquar](../Images/27c7941771179fe5731641930c403ff2.png)](https://zainbaq.medium.com/?source=post_page-----1ba339885f0c--------------------------------)[](https://towardsdatascience.com/?source=post_page-----1ba339885f0c--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----1ba339885f0c--------------------------------)
    [Zain Baquar](https://zainbaq.medium.com/?source=post_page-----1ba339885f0c--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----1ba339885f0c--------------------------------)
    ·12 min read·Feb 9, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----1ba339885f0c--------------------------------)
    ·阅读时间 12 分钟·2023年2月9日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/d2d7047e533c03e443b61973f43ce21a.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d2d7047e533c03e443b61973f43ce21a.png)'
- en: '[Unsplash: Maxim Hopman](https://unsplash.com/@nampoh)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[Unsplash: Maxim Hopman](https://unsplash.com/@nampoh)'
- en: '**Introduction**'
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**介绍**'
- en: Believe it or not, humans are constantly predicting things passively — even
    the most minuscule or seemingly trivial things. When crossing the road, we forecast
    where the cars will be to cross the road safely, or we try to predict exactly
    where a ball will be when we try to catch it. We don’t need to know the exact
    velocity of the car or the precise wind direction affecting the ball in order
    to perform these tasks — they come more or less naturally and obviously to us.
    These abilities are tuned by a handful of events, which over years of experience
    and practice allow us to navigate the unpredictable reality we live in. Where
    we fail in this regard, is when there are simply too many factors to take into
    consideration when we are actively predicting a large scale phenomenon, like the
    weather or how the economy will perform one year down the line.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 信不信由你，人类在不断被动地预测事物——即使是最微小或看似琐碎的事情。过马路时，我们预测汽车会在哪里以安全地过马路，或者我们尝试预测球在我们试图接住它时会在哪里。我们不需要知道汽车的确切速度或影响球的风向才能完成这些任务——这些能力对我们来说或多或少是自然而然的。这些能力通过一系列事件得到调整，通过多年的经验和实践使我们能够应对我们生活中不可预测的现实。而当需要主动预测大规模现象，如天气或一年后的经济表现时，我们往往因为需要考虑的因素过多而失败。
- en: This is where the power of computing comes into focus — to fill the gap of our
    inability to take even the most seemingly random of occurrences and relate them
    to a future event. As we all know, computers are extremely good at doing a specific
    task over numerous iterations — which we can leverage in order to predict the
    future.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是计算的力量所在——弥补我们无法将即使是最看似随机的事件与未来事件联系起来的不足。众所周知，计算机在执行特定任务时非常擅长——我们可以利用这一点来预测未来。
- en: '**What is a ‘time series’?**'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '**什么是“时间序列”？**'
- en: A time series is any quantifiable metric or event that takes place over a period
    of time. As trivial as this sounds, almost anything can be thought of as a time
    series. Your average heart rate per hour over a month or the daily closing value
    of a stock over a year or the number vehicle accidents in a certain city per week
    over a year. Recording this information over any uniform period of time is considered
    as a time series. The astute would note that for each of these examples, there
    is a **frequency** (daily, weekly, hourly etc) of the event and a **length** of
    time (a month, year, day etc) over which the event takes place.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 时间序列是指在一段时间内发生的任何可量化的指标或事件。尽管这听起来很琐碎，但几乎任何东西都可以被认为是时间序列。比如一个月内每小时的平均心率，或者一年内每日的股票收盘价，或者某城市一年内每周的交通事故数量。记录这些信息在任何统一的时间段内被认为是时间序列。聪明的人会注意到，这些例子中都有一个**频率**（每天、每周、每小时等）和一个**时间长度**（一个月、一年、一天等）来描述事件发生的周期。
- en: For a time series, the metric is recorded with a uniform frequency throughout
    the length of time over which we are observing the metric. In other words, the
    time in between each record should be the same.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 对于时间序列，指标是在我们观察指标的时间长度内以统一频率记录的。换句话说，每个记录之间的时间应该是相同的。
- en: In this tutorial, we will explore how to use past data in the form of a time
    series to forecast what may happen in the future.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在本教程中，我们将探讨如何使用时间序列中的过去数据来预测未来可能发生的情况。
- en: '**Objective**'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '**目标**'
- en: The objective of the algorithm is to be able to take in a sequence of values,
    and predict the next value in the sequence. The simplest way to do this is to
    use an **Auto-Regressive** model, however, this has been covered extensively by
    other authors, and so we will focus on a more deep learning approach to this problem,
    using **recurrent neural networks**. I’ve linked the implementation notebook [here](https://drive.google.com/file/d/1ZzxQISX0519T347j3Sx71iHKsdYDOzcl/view?usp=sharing).
    The dataset used in this tutorial was used in a Kaggle competition, and is found
    [here](https://www.kaggle.com/competitions/store-sales-time-series-forecasting/data?select=oil.csv).
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 算法的目标是能够接受一系列值，并预测序列中的下一个值。最简单的方法是使用**自回归**模型，然而，这已经被其他作者广泛讨论过，因此我们将专注于一种更深入的学习方法，即使用**递归神经网络**。我已将实现笔记本链接[在此](https://drive.google.com/file/d/1ZzxQISX0519T347j3Sx71iHKsdYDOzcl/view?usp=sharing)。本教程中使用的数据集曾在Kaggle比赛中使用，可以在[这里](https://www.kaggle.com/competitions/store-sales-time-series-forecasting/data?select=oil.csv)找到。
- en: '**Data Preparation**'
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**数据准备**'
- en: Let’s have a look at a sample time series. The plot below shows some data on
    the price of oil from 2013 to 2018.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看一个示例时间序列。下面的图表显示了2013年至2018年间油价的一些数据。
- en: '![](../Images/48c4e48fcbff37198877292c230c971f.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/48c4e48fcbff37198877292c230c971f.png)'
- en: Image by author
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图像
- en: This is simply a plot of a single sequence of numbers on a date axis. The table
    below shows the first 10 entries of this time series. Just looking at the date
    column, it is apparent that we have price data at a daily frequency.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 这只是一个在日期轴上绘制单一数值序列的图表。下表显示了该时间序列的前10个条目。从日期列来看，很明显我们有每天频率的价格数据。
- en: '[PRE0]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Many machine learning models perform much better on normalized data. The standard
    way to normalize data is to transform it such that for each column, the mean is
    0 and the standard deviation is 1\. The code below provides a way to do this using
    the **scikit-learn** library.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 许多机器学习模型在标准化数据上表现更好。标准化数据的方式是将数据转换为每列的均值为0，标准差为1。下面的代码提供了一种使用**scikit-learn**库进行标准化的方法。
- en: '[PRE1]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: We also want to ensure that our data has a uniform frequency — in this example,
    we have the price of oil on each day across these 5 years, so this works out nicely.
    If, for your data, this is not the case, Pandas has a few different ways to resample
    your data to fit a uniform frequency.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还希望确保我们的数据具有统一的频率——在这个例子中，我们在这5年中每天都有油价数据，所以这很好地解决了问题。如果你的数据中不是这种情况，Pandas提供了几种不同的方法来重新采样数据以适应统一的频率。
- en: '**Sequencing**'
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**序列化**'
- en: 'Once this is achieved, we are going to use the time series and generate clips,
    or **sequences** of fixed length. While recording these sequences, we will also
    record the value that occurred right after that sequence. For example: let’s say
    we have a sequence: [1, 2, 3, 4, 5, 6].'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦实现了这一点，我们将使用时间序列生成固定长度的片段或**序列**。在记录这些序列时，我们还会记录序列之后发生的值。例如：假设我们有一个序列：[1,
    2, 3, 4, 5, 6]。
- en: 'By choosing a sequence length of 3, we can generate the following sequences,
    and their associated targets:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 通过选择序列长度为 3，我们可以生成以下序列及其相关目标：
- en: '**[Sequence]: Target**'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '**[Sequence]: Target**'
- en: '[1, 2, 3] → 4'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '[1, 2, 3] → 4'
- en: '[2, 3, 4] → 5'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '[2, 3, 4] → 5'
- en: '[3, 4, 5] → 6'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '[3, 4, 5] → 6'
- en: Another way to look at this is that we are defining how many steps back to look
    in order to predict the next value. We will call this value the **training window**
    and the number of values to predict, the **prediction window**. In this example,
    these are 3 and 1 respectively. The function below details how this is accomplished.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种看待这个问题的方法是，我们定义了回顾多少步以预测下一个值。我们将这个值称为 **训练窗口**，预测的值数目称为 **预测窗口**。在这个例子中，这些分别是
    3 和 1。下面的函数详细说明了如何实现这一点。
- en: '[PRE2]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '*PyTorch* requires us to store our data in a Dataset class in the following
    way:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '*PyTorch* 要求我们以以下方式将数据存储在 Dataset 类中：'
- en: '[PRE3]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: We can then use a *PyTorch* DataLoader to iterate through the data. The benefit
    of using a DataLoader is that it handles batching and shuffling internally, so
    we don’t have to worry about implementing it for ourselves.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以使用 *PyTorch* 的 DataLoader 来迭代数据。使用 DataLoader 的好处是它内部处理批次和洗牌，因此我们不必担心自己实现这些功能。
- en: 'The training batches are finally ready after the following code:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 训练批次在以下代码执行后就准备好了：
- en: '[PRE4]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: At each iteration the DataLoader will yield 16 (batch size) sequences with their
    associated targets which we will pass into the model.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在每次迭代中，DataLoader 将输出 16（批次大小）个序列及其相关目标，我们将这些序列传递给模型。
- en: '**Model Architecture**'
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**模型架构**'
- en: The class below defines this architecture in *PyTorch*. We’ll be using a single
    LSTM layer, followed by some dense layers for the regressive part of the model
    with dropout layers in between them. The model will output a single value for
    each training input.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 以下类在 *PyTorch* 中定义了这一架构。我们将使用一个 LSTM 层，后面跟着一些密集层用于模型的回归部分，并且它们之间有丢弃层。模型将为每个训练输入输出一个单一值。
- en: '[PRE5]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: This class is a plug n’ play Python class that I built to be able to dynamically
    build a neural network (of this type) of any size, based on the parameters we
    choose — so feel free to tune the parameters n_hidden and n_deep_players to add
    or remove parameters from your model. More parameters means more model complexity
    and longer training times, so be sure to refer to your use-case for what’s best
    for your data.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 这个类是一个即插即用的 Python 类，我构建它是为了能够动态构建任何大小的神经网络（此类型），基于我们选择的参数——因此请随意调整参数 n_hidden
    和 n_deep_players，以添加或删除模型中的参数。更多参数意味着更多模型复杂性和更长的训练时间，所以一定要参考你的使用场景，以确定对数据最合适的参数设置。
- en: As an arbitrary selection, lets create a Long Short-Term Memory model with 5
    fully connected layers which 50 neurons each, ending with a single output value
    for each training example in each batch. Here, **sequence_len** refers to the
    training window and **nout** defines how many steps to predict; setting **sequence_len**
    as 180 and **nout** as 1, means that the model will look at 180 days (half a year)
    back to predict what will happen tomorrow.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一个任意选择，我们创建一个具有 5 层全连接层的长短期记忆（LSTM）模型，每层 50 个神经元，最终每个训练样本在每个批次中以单一输出值结束。在这里，**sequence_len**
    指的是训练窗口，而 **nout** 定义了预测的步数；将 **sequence_len** 设置为 180 和 **nout** 设置为 1，意味着模型将回顾过去
    180 天（半年），以预测明天会发生什么。
- en: '[PRE6]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '**Model Training**'
  id: totrans-47
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**模型训练**'
- en: 'With our model defined, we can choose our loss function and optimizer, set
    our learning rate and number of epochs, and begin our training loop. Since this
    is regressive problem (i.e. we are trying to predict a continuous value), a safe
    choice is Mean Squared Error for the loss function. This provides a robust way
    to calculate the error between the actual values and what the model predicts.
    This is given by:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 定义了模型后，我们可以选择损失函数和优化器，设置学习率和训练轮数，并开始训练循环。由于这是一个回归问题（即我们尝试预测一个连续值），一个安全的选择是均方误差作为损失函数。这提供了一种稳健的方法来计算实际值与模型预测值之间的误差。计算公式如下：
- en: '![](../Images/13a66e22dc6bd9018198abeea20afb76.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/13a66e22dc6bd9018198abeea20afb76.png)'
- en: Image snipped from Google.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 图片摘自 Google。
- en: The optimizer object stores and calculates all the gradients needed for back
    propagation.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 优化器对象存储和计算了反向传播所需的所有梯度。
- en: '[PRE7]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Here’s the training loop. In each training iteration, we will calculate the
    loss on both the training and validation sets we created earlier:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 这是训练循环。在每次训练迭代中，我们将计算之前创建的训练集和验证集上的损失：
- en: '[PRE8]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '![](../Images/17028744e646c14abaed45ad0b4bd290.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/17028744e646c14abaed45ad0b4bd290.png)'
- en: Sample output for training loop showing the training and validation loss at
    each epoch.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 训练循环的示例输出，显示每个时代的训练和验证损失。
- en: Now that the model is trained, we can evaluate our predictions.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 现在模型已经训练完成，我们可以评估我们的预测。
- en: '**Inference**'
  id: totrans-58
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**推断**'
- en: Here we will simply call our trained model to predict on our un-shuffled data
    and see how different the predictions are from the true observations.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将简单地调用我们训练好的模型来预测未打乱的数据，并查看预测与真实观察值的差异。
- en: '[PRE9]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '![](../Images/9239b1333d89793b8eabc6441c36b78b.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9239b1333d89793b8eabc6441c36b78b.png)'
- en: Normalized Predicted vs Actual price of oil historically. Image by author.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 历史上的归一化预测与实际油价。图片由作者提供。
- en: For a first try, our predictions don’t look too bad! And it helps that our validation
    loss is as low as our training loss, showing that we did not overfit the model
    and thus, the model can be considered to generalize well — which is important
    for any predictive system.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 初次尝试，我们的预测看起来还不错！而且我们验证损失与训练损失一样低，说明我们没有过拟合模型，因此模型可以被认为具有良好的泛化能力——这对任何预测系统都很重要。
- en: With a somewhat decent estimator for the price of oil with respect to time in
    this time period, let’s see if we can use it to forecast what lies ahead.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 有了对该时间段油价的相对合理估计，让我们看看能否用它来预测未来的情况。
- en: '**Forecasting**'
  id: totrans-65
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**预测**'
- en: 'If we define history as the series until the moment of the forecast, the algorithm
    is simple:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们将历史定义为预测时刻之前的系列，算法则很简单：
- en: Get the latest valid sequence from the history (of training window length).
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从历史记录中获取最新有效序列（训练窗口长度）。
- en: Input that latest sequence to the model and predict the next value.
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将最新的序列输入模型，并预测下一个值。
- en: Append the predicted value on to the history.
  id: totrans-69
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将预测值附加到历史记录中。
- en: Repeat from step 1 for any number of iterations.
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从第1步重复进行任意次数的迭代。
- en: One caveat here is that depending on the parameters chosen upon training the
    model, the further out you forecast, the more the model succumbs to it’s own biases
    and starts to predict the mean value. So we don’t want to always predict too far
    ahead if unnecessary, as it takes away from the accuracy of the forecast.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 一个警告是，根据训练模型时选择的参数，预测得越远，模型越容易受到自身偏差的影响，开始预测均值。因此，如果不必要，我们不希望总是预测过远，因为这会降低预测的准确性。
- en: 'This is implemented in the functions below:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 这在下面的函数中实现：
- en: '[PRE10]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Let’s try a few cases.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试几个案例。
- en: Lets forecast from different places in the middle of the series so we can compare
    the forecast to what actually happened. The way we have coded the forecaster,
    we can forecast from anywhere and for any reasonable number of steps. The red
    line shows the forecast. Keep in mind, the plots show the normalized prices on
    the y-axis.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 从系列中间的不同位置进行预测，以便将预测与实际发生的情况进行比较。由于我们编码的预测器可以从任何地方进行预测，并且可以预测任意合理的步数。红线表示预测。请注意，图中的y轴显示的是归一化价格。
- en: '![](../Images/e2028b2f4fb4dfc5eaf331e7ed03fc01.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e2028b2f4fb4dfc5eaf331e7ed03fc01.png)'
- en: Forecasting 200 days from Q3 2013\. Image by author.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 从2013年第三季度预测200天。图片由作者提供。
- en: '![](../Images/7f4df4fdfb8c12661b24deb6cfec9f99.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7f4df4fdfb8c12661b24deb6cfec9f99.png)'
- en: Forecasting 200 days from EOY 2014/15\. Image by author.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 从2014/15年年末预测200天。图片由作者提供。
- en: '![](../Images/1a75676534e172e00b151e99b305c259.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1a75676534e172e00b151e99b305c259.png)'
- en: Forecasting 200 days from Q1 2016\. Image by author.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 从2016年第一季度预测200天。图片由作者提供。
- en: '![](../Images/8258d44173c3fc71c54deaa3865a56f9.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8258d44173c3fc71c54deaa3865a56f9.png)'
- en: Forecasting 200 days from the last day of the data. Image by author.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 从数据的最后一天预测200天。图片由作者提供。
- en: And this was just the first model configuration we tried! Experimenting more
    with the architecture and implementation would definitely allow your model to
    train better and forecast more accurately.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 这只是我们尝试的第一个模型配置！更多地实验架构和实现将使您的模型训练得更好，并更准确地进行预测。
- en: Conclusion
  id: totrans-85
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: There we have it! A model that can predict what will happen next in a **univariate**
    time series. It’s pretty cool when thinking about all the ways and places in which
    this can be applied. Yes, this article only handled **univariate** timeseries,
    in which there is a single sequence of values. However there are ways to use multiple
    series measuring different things together to make predictions. This is called
    **multivariate** time series forecasting, it mainly just needs a few tweaks to
    the model architecture which I will cover in a future article.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 就这样！一个可以预测**单变量**时间序列中接下来会发生什么的模型。考虑到这种模型可以应用的各种方式和场景，真的很酷。是的，这篇文章仅处理了**单变量**时间序列，其中只有一个值序列。然而，也有方法可以使用多个测量不同事物的序列来进行预测。这被称为**多变量**时间序列预测，它主要只需要对模型架构进行一些调整，这些我将在未来的文章中进行介绍。
- en: The true magic of this kind of forecasting model is in the LSTM layer of the
    model, and how it handles and remembers sequences as a **recurrent** layer of
    the neural network. For more information on Neural Networks of different kinds,
    I would highly recommend [3blue1browns video](https://www.youtube.com/watch?v=aircAruvnKk)
    on the topic. He has a great series detailing how these algorithms work internally,
    which is very visually intuitive.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 这种预测模型的真正魔力在于模型的LSTM层，它如何处理和记忆序列作为神经网络的**递归**层。有关不同类型神经网络的更多信息，我强烈推荐[3blue1brown的视频](https://www.youtube.com/watch?v=aircAruvnKk)。他有一个很棒的系列，详细介绍了这些算法如何在内部工作，非常直观。
- en: Thanks for reading, and be sure to check out my other articles!
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 感谢阅读，确保查看我的其他文章！
- en: '**References:**'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '**参考资料：**'
- en: Time series data — [https://www.kaggle.com/competitions/store-sales-time-series-forecasting/data?select=oil.csv](https://www.kaggle.com/competitions/store-sales-time-series-forecasting/data?select=oil.csv)
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 时间序列数据 — [https://www.kaggle.com/competitions/store-sales-time-series-forecasting/data?select=oil.csv](https://www.kaggle.com/competitions/store-sales-time-series-forecasting/data?select=oil.csv)
