- en: 'The Ultimate Guide to Training BERT from Scratch: Final Act'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从零开始训练BERT的终极指南：最终篇
- en: 原文：[https://towardsdatascience.com/the-ultimate-guide-to-training-bert-from-scratch-final-act-eab78b0657bb](https://towardsdatascience.com/the-ultimate-guide-to-training-bert-from-scratch-final-act-eab78b0657bb)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 译文：[https://towardsdatascience.com/the-ultimate-guide-to-training-bert-from-scratch-final-act-eab78b0657bb](https://towardsdatascience.com/the-ultimate-guide-to-training-bert-from-scratch-final-act-eab78b0657bb)
- en: 'The Final Frontier: Building and Training Your BERT Model'
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 最终前沿：构建和训练你的BERT模型
- en: '[](https://dpoulopoulos.medium.com/?source=post_page-----eab78b0657bb--------------------------------)[![Dimitris
    Poulopoulos](../Images/ce535a1679779f5a2ec8b024e6691e50.png)](https://dpoulopoulos.medium.com/?source=post_page-----eab78b0657bb--------------------------------)[](https://towardsdatascience.com/?source=post_page-----eab78b0657bb--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----eab78b0657bb--------------------------------)
    [Dimitris Poulopoulos](https://dpoulopoulos.medium.com/?source=post_page-----eab78b0657bb--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://dpoulopoulos.medium.com/?source=post_page-----eab78b0657bb--------------------------------)[![Dimitris
    Poulopoulos](../Images/ce535a1679779f5a2ec8b024e6691e50.png)](https://dpoulopoulos.medium.com/?source=post_page-----eab78b0657bb--------------------------------)[](https://towardsdatascience.com/?source=post_page-----eab78b0657bb--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----eab78b0657bb--------------------------------)
    [Dimitris Poulopoulos](https://dpoulopoulos.medium.com/?source=post_page-----eab78b0657bb--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----eab78b0657bb--------------------------------)
    ·7 min read·Dec 18, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page-----eab78b0657bb--------------------------------)·阅读时间7分钟·2023年12月18日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/548c617f9d3d11635cc30ccc45e1d106.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/548c617f9d3d11635cc30ccc45e1d106.png)'
- en: Photo by [Rob Laughter](https://unsplash.com/@roblaughter?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：[Rob Laughter](https://unsplash.com/@roblaughter?utm_source=medium&utm_medium=referral)在[Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: This blog post concludes our series on training BERT from scratch. For context
    and a complete understanding, please refer to [Part I](/the-ultimate-guide-to-training-bert-from-scratch-introduction-b048682c795f),
    [Part II](/the-ultimate-guide-to-training-bert-from-scratch-the-tokenizer-ddf30f124822),
    and [Part III](/the-ultimate-guide-to-training-bert-from-scratch-prepare-the-dataset-beaae6febfd5)
    of the series.
  id: totrans-8
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 本博客文章总结了我们关于从零开始训练BERT的系列内容。有关背景和完整理解，请参阅系列的[第一部分](/the-ultimate-guide-to-training-bert-from-scratch-introduction-b048682c795f)、[第二部分](/the-ultimate-guide-to-training-bert-from-scratch-the-tokenizer-ddf30f124822)和[第三部分](/the-ultimate-guide-to-training-bert-from-scratch-prepare-the-dataset-beaae6febfd5)。
- en: When BERT burst onto the scene in 2018, it triggered a tsunami in the world
    of Natural Language Processing (NLP). Many consider this as the NLP’s own ImageNet
    moment, drawing parallels to the shift deep neural networks brought to computer
    vision and the broader field of machine learning back in 2012.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 当BERT在2018年登场时，它在自然语言处理（NLP）领域掀起了一场巨浪。许多人将其视为NLP的ImageNet时刻，类似于2012年深度神经网络对计算机视觉及更广泛的机器学习领域带来的变化。
- en: Five years down the line, the prophecy holds true. Transformer-based Large Language
    Models (LLMs) aren’t just the shiny new toy; they’re reshaping the landscape.
    From transforming how we work to revolutionizing how we access information, these
    models are core technology behind countless emerging startups aiming to harness
    their untapped potential.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 五年过去了，预言依然准确。基于Transformer的大型语言模型（LLMs）不仅仅是新玩具；它们正在重塑整个格局。从改变我们的工作方式到革命性地获取信息，这些模型是无数新兴初创公司背后的核心技术，旨在挖掘它们尚未被充分利用的潜力。
- en: This is the reason I decided to write this series of blog posts, diving into
    the world of BERT and how can you train your own model from scratch. The point
    isn’t just to get the job done — after all, you can easily find pre-trained BERT
    models on the Hugging Face Hub. The real magic lies in understanding the inner
    workings of this groundbreaking model and applying that knowledge to the current
    environment.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是我决定撰写这一系列博客文章的原因，深入探讨BERT的世界以及如何从零开始训练自己的模型。重点不仅仅是完成任务——毕竟，你可以轻松找到在Hugging
    Face Hub上预训练的BERT模型。真正的魔力在于理解这个突破性模型的内部工作原理，并将这些知识应用于当前环境。
- en: 'The first post served as your entry ticket, introducing BERT’s core concepts,
    objectives, and potential applications. We even went through the fine-tuning process
    together, creating a question-answering system:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 第一篇文章作为你的入门票，引入了BERT的核心概念、目标和潜在应用。我们甚至一起完成了微调过程，创建了一个问答系统：
- en: '[](/the-ultimate-guide-to-training-bert-from-scratch-introduction-b048682c795f?source=post_page-----eab78b0657bb--------------------------------)
    [## The Ultimate Guide to Training BERT from Scratch: Introduction'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '[BERT训练的终极指南：介绍](https://towardsdatascience.com/the-ultimate-guide-to-training-bert-from-scratch-introduction-b048682c795f?source=post_page-----eab78b0657bb--------------------------------)
    [## BERT训练的终极指南：介绍'
- en: 'Demystifying BERT: The definition and various applications of the model that
    changed the NLP landscape.'
  id: totrans-14
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 解密BERT：改变NLP领域的模型的定义和各种应用。
- en: towardsdatascience.com](/the-ultimate-guide-to-training-bert-from-scratch-introduction-b048682c795f?source=post_page-----eab78b0657bb--------------------------------)
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '[BERT训练的终极指南：介绍](https://towardsdatascience.com/the-ultimate-guide-to-training-bert-from-scratch-introduction-b048682c795f?source=post_page-----eab78b0657bb--------------------------------)'
- en: 'The second installment acted as your insider’s guide to the often-overlooked
    realm of tokenizers — unpacking their role, showing how they convert words into
    numerical values, and guiding you through the process of training your own:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 第二部分作为你了解分词器这一经常被忽视领域的内部指南——解释它们的作用，展示它们如何将单词转换为数值，并指导你训练自己的分词器：
- en: '[](/the-ultimate-guide-to-training-bert-from-scratch-the-tokenizer-ddf30f124822?source=post_page-----eab78b0657bb--------------------------------)
    [## The Ultimate Guide to Training BERT from Scratch: The Tokenizer'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '[BERT训练的终极指南：分词器](https://towardsdatascience.com/the-ultimate-guide-to-training-bert-from-scratch-the-tokenizer-ddf30f124822?source=post_page-----eab78b0657bb--------------------------------)
    [## BERT训练的终极指南：分词器'
- en: 'From Text to Tokens: Your Step-by-Step Guide to BERT Tokenization'
  id: totrans-18
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 从文本到标记：你的BERT分词指南
- en: towardsdatascience.com](/the-ultimate-guide-to-training-bert-from-scratch-the-tokenizer-ddf30f124822?source=post_page-----eab78b0657bb--------------------------------)
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '[BERT训练的终极指南：分词器](https://towardsdatascience.com/the-ultimate-guide-to-training-bert-from-scratch-the-tokenizer-ddf30f124822?source=post_page-----eab78b0657bb--------------------------------)'
- en: 'In the third chapter, we tackled what I consider the most difficult step of
    the entire training pipeline: dataset preparation. We delved into the details
    of preparing your dataset for BERT’s specialized Masked ML and NSP tasks, a proxy
    it uses to understand language and context.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在第三章中，我们讨论了我认为整个训练流程中最困难的步骤：数据集准备。我们*深入探讨*了为BERT的专用Masked ML和NSP任务准备数据集的细节，这些任务是它用来理解语言和上下文的代理。
- en: '[](/the-ultimate-guide-to-training-bert-from-scratch-prepare-the-dataset-beaae6febfd5?source=post_page-----eab78b0657bb--------------------------------)
    [## The Ultimate Guide to Training BERT from Scratch: Prepare the Dataset'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '[BERT训练的终极指南：准备数据集](https://towardsdatascience.com/the-ultimate-guide-to-training-bert-from-scratch-prepare-the-dataset-beaae6febfd5?source=post_page-----eab78b0657bb--------------------------------)
    [## BERT训练的终极指南：准备数据集'
- en: 'Data Preparation: Dive Deeper, Optimize your Process, and Discover How to Attack
    the Most Crucial Step'
  id: totrans-22
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据准备：*深入探讨*，优化流程，并发现如何应对最关键的步骤
- en: towardsdatascience.com](/the-ultimate-guide-to-training-bert-from-scratch-prepare-the-dataset-beaae6febfd5?source=post_page-----eab78b0657bb--------------------------------)
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '[BERT训练的终极指南：准备数据集](https://towardsdatascience.com/the-ultimate-guide-to-training-bert-from-scratch-prepare-the-dataset-beaae6febfd5?source=post_page-----eab78b0657bb--------------------------------)'
- en: 'And now, the curtain rises and drops on our final act: training. In this post,
    we’ll roll up our sleeves and train a new BERT model — one you can later fine-tune
    for your specific NLP needs. If you’ve already dipped your toes into deep learning
    model training, you should be able to follow this one without any issues. So,
    without further ado, let’s dive in!'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，最终的序幕拉开并落下：训练。在这篇文章中，我们将*挽起袖子*，训练一个新的BERT模型——你可以稍后针对特定的NLP需求进行微调。如果你已经涉足深度学习模型训练，这篇文章应该能让你毫无问题地跟上。所以，事不宜迟，让我们*深入探讨*吧！
- en: '[Learning Rate](https://www.dimpo.me/newsletter?utm_source=medium&utm_medium=article&utm_campaign=bert-training)
    is a newsletter for those who are curious about the world of ML and MLOps. If
    you want to learn more about topics like this subscribe [here](https://www.dimpo.me/newsletter?utm_source=medium&utm_medium=article&utm_campaign=bert-training).
    You’ll hear from me on the last Sunday of every month with updates and thoughts
    on the latest MLOps news and articles!'
  id: totrans-25
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '[学习速率](https://www.dimpo.me/newsletter?utm_source=medium&utm_medium=article&utm_campaign=bert-training)是一个面向对ML和MLOps世界感兴趣的人的通讯。如果你想了解更多类似的主题，请[点击这里](https://www.dimpo.me/newsletter?utm_source=medium&utm_medium=article&utm_campaign=bert-training)订阅。每个月的最后一个星期天，我会与你分享最新的MLOps新闻和文章的更新和看法！'
- en: The Training Loop
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练循环
- en: Modern frameworks like PyTorch and TensorFlow have streamlined the process of
    building and training a deep neural network. The training loop is like choreography
    that passes the dataset features through the model to get a prediction, compares
    the model’s prediction to the labels (the ground truth) to calculate the loss,
    and finally uses that loss to update the weights of the model by propagating the
    error backward through the layers of the model.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 现代框架如PyTorch和TensorFlow简化了构建和训练深度神经网络的过程。训练循环就像编排，将数据集特征传递通过模型以获得预测，将模型的预测与标签（真实值）进行比较以计算损失，最后使用损失通过模型的层反向传播误差以更新模型的权重。
- en: 'This is what this flow looks like in PyTorch:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是在PyTorch中流程的样子：
- en: '[PRE0]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: In theory, it looks simple, wouldn’t you agree? Well, in practice, it gets simpler
    yet more powerful. We will use the Hugging Face `transformers` library to train
    our BERT encoder. The `Trainer` object handles the loop implementation for us,
    adding a lot of other features on top of that. Thus, we can turn our attention
    to other pieces of the code. So, let’s define our metric first.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 理论上，这看起来很简单，不是吗？实际上，它变得更简单而且更强大。我们将使用Hugging Face `transformers` 库来训练我们的BERT编码器。`Trainer`
    对象为我们处理循环实现，并添加了许多其他功能。因此，我们可以将注意力转向代码的其他部分。现在，让我们首先定义我们的指标。
- en: 'On paper, the process seems really simple, don’t you think? The reality is
    even better: it’s not just simple but also incredibly robust, thanks to tools
    like the Hugging Face `transformers` library.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在纸面上，这个过程看起来真的很简单，你不觉得吗？现实更好：它不仅简单，而且非常强大，这要归功于像Hugging Face `transformers`
    库这样的工具。
- en: Their `Trainer` object takes over the heavy lifting while tossing in a lot of
    additional features. Thus, we can turn our attention to other pieces of the code.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 他们的 `Trainer` 对象承担了繁重的工作，同时增加了许多额外的功能。因此，我们可以将注意力转向代码的其他部分。
- en: So, let’s define our metric next.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，接下来让我们定义我们的指标。
- en: Accuracy
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准确率
- en: 'To evaluate our model’s performance during training, we need to keep our eyes
    on two key numbers: the loss and a metric that’s meaningful to us. The loss is
    what algorithms like gradient descent use to improve the model’s performance during
    back-propagation, while the metric is something that is more human-readable.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 为了评估我们模型在训练期间的表现，我们需要关注两个关键数字：损失和对我们有意义的指标。损失是像梯度下降这样的算法在反向传播期间用来改进模型性能的，而指标则是更易于理解的东西。
- en: 'In the spirit of simplicity, we’ll opt for a straightforward measure: the model’s
    accuracy on both the Masked Language Modeling (MLM) and Next Sentence Prediction
    (NSP) tasks. Essentially, we’re interested in two things: how accurately our model
    fills in those masked words and how good is to recognize that two sentences logically
    follow one another.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 本着简洁的精神，我们将选择一个直接的衡量标准：模型在掩码语言建模（MLM）和下一句预测（NSP）任务上的准确率。实质上，我们关心的是两个方面：我们的模型在填充那些掩码词时的准确度，以及它在识别两句逻辑上是否相互衔接方面的表现。
- en: 'However, we must say that this is not the best way to validate our model’s
    performance. For instance, picture the following sentence:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我们必须说这不是验证我们模型性能的最佳方式。例如，考虑以下句子：
- en: I am [MASK] at math.
  id: totrans-38
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 我在数学方面是 [MASK]。
- en: How would you fill in the masked word? I am *great* at math, or I am *terrible*
    at math? Both of these predictions seem contextually valid, yet they completely
    change the meaning of the sentence.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 你会如何填充掩码词？我是*擅长*数学，还是我是*糟糕*的数学？这两种预测在语境上似乎都有效，但它们完全改变了句子的意思。
- en: So, evaluating your model against a validation dataset that offers only one
    correct answer is not the best idea. This may not really test your model’s ability
    to understand language but to memorize. To mitigate this issue a bit, we can calculate
    another metric, like the perplexity of our model after training.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，用只有一个正确答案的验证数据集来评估模型不是最好的主意。这可能并不能真正测试模型理解语言的能力，而是记忆能力。为了解决这个问题，我们可以计算另一个指标，比如训练后模型的困惑度。
- en: 'For now, let’s evaluate the accuracy of the model by loading the corresponding
    module from the `transformers` library:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们通过从 `transformers` 库中加载相应模块来评估模型的准确性：
- en: '[PRE1]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Training
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练
- en: 'Assuming you’ve been with me through this tutorial series, you’re now at a
    pivotal moment where you have prepared your dataset. There’s a common saying among
    data scientists: once your dataset is sculpted for the model, the bulk of your
    work is nearly complete. Thanks to the advancements in Deep Learning frameworks,
    what follows often feels like mere boilerplate code.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你在整个教程系列中一直跟随我，现在你正处于一个关键时刻，你已经准备好了数据集。数据科学家中有一句常见的说法：一旦你的数据集为模型雕琢完成，你的大部分工作就差不多完成了。由于深度学习框架的进步，接下来的步骤往往感觉像是简单的模板代码。
- en: So, with anticipation, let’s instantiate the model.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，带着期待，让我们开始实例化模型吧。
- en: '[PRE2]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Now comes the grand finale: launching the training script. First, we’ll define
    the training arguments, and then, with everything in place, we’ll launch the training
    loop:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 现在迎来了高潮：启动训练脚本。首先，我们将定义训练参数，然后，在一切准备就绪后，我们将启动训练循环：
- en: '[PRE3]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Two hours and 20 epochs later, in the limited world of a Colab Notebook, I’ve
    hit 80% accuracy on the NSP task and 15% accuracy on the Masked ML task. For a
    model we’re training from scratch, these numbers are okay.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 两小时和20个周期后，在有限的Colab Notebook环境中，我在NSP任务上达到了80%的准确率，在Masked ML任务上达到了15%的准确率。对于一个从零开始训练的模型来说，这些数字还不错。
- en: '![](../Images/686ea6f168d6cb72d37ecbb6983d1ae2.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/686ea6f168d6cb72d37ecbb6983d1ae2.png)'
- en: Training results — Image by Author
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 训练结果 — 作者提供的图片
- en: However, it’s important to note that in most real-world scenarios, your starting
    line isn’t from scratch. Typically, you’d begin with a pre-trained model, which
    often leads to better results.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，值得注意的是，在大多数实际场景中，你的起点并不是从零开始。通常，你会从一个预训练的模型开始，这往往能带来更好的结果。
- en: Also, there’s ample room to experiment and enhance performance. Tinkering with
    the hyperparameters of the training process, particularly the learning rate, can
    significantly boost your results, even when you’re building a model from the ground
    up.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 同时，还有大量的空间进行实验和提升性能。调整训练过程中的超参数，特别是学习率，可以显著提升你的结果，即使你是从头开始构建模型。
- en: Conclusion
  id: totrans-54
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: We’ve reached the final destination in our quest to train a BERT model from
    scratch. Throughout this journey, we saw why BERT is considered to be a seminal
    model in the area of NLP, and we delved into the intricacies of tokenizers, not
    only understanding their mechanisms but also mastering the art of training our
    own.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经达到了从零开始训练BERT模型的最终目的地。在这段旅程中，我们了解了为什么BERT被认为是NLP领域的开创性模型，并深入探讨了分词器的复杂性，不仅理解了它们的机制，还掌握了训练我们自己分词器的技巧。
- en: Next, our journey took us through the meticulous process of preparing a dataset
    for two distinct tasks running in tandem, the NSP and Masked ML tasks. Finally,
    we landed on the crucial step of launching a training script, using the `transformers`
    library.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们的旅程带我们经历了为两个并行运行的任务——NSP和Masked ML任务——准备数据集的细致过程。最终，我们来到了启动训练脚本的关键步骤，使用了`transformers`库。
- en: It’s been an long but fulfilling expedition, and I hope you also enjoyed it.
    You should be ready to apply this knowledge to your own activities and expand
    it to cover more ground. Until next time, happy coding!
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一段漫长但充实的旅程，我希望你也喜欢它。你应该准备好将这些知识应用到自己的活动中，并扩展到更多领域。下次见，祝编程愉快！
- en: About the Author
  id: totrans-58
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 关于作者
- en: My name is [Dimitris Poulopoulos](https://www.dimpo.me/?utm_source=medium&utm_medium=article&utm_campaign=bert-intro),
    and I’m a machine learning engineer working for [HPE](https://www.hpe.com/us/en/home.html).
    I have designed and implemented AI and software solutions for major clients such
    as the European Commission, IMF, the European Central Bank, IKEA, Roblox and others.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 我的名字是 [Dimitris Poulopoulos](https://www.dimpo.me/?utm_source=medium&utm_medium=article&utm_campaign=bert-intro)，我是一个为
    [HPE](https://www.hpe.com/us/en/home.html)工作的机器学习工程师。我为欧洲委员会、国际货币基金组织、欧洲中央银行、宜家、Roblox等主要客户设计和实施了AI和软件解决方案。
- en: If you are interested in reading more posts about Machine Learning, Deep Learning,
    Data Science, and DataOps, follow me on [Medium](https://towardsdatascience.com/medium.com/@dpoulopoulos/follow),
    [LinkedIn](https://www.linkedin.com/in/dpoulopoulos/), or [@james2pl](https://twitter.com/james2pl)
    on Twitter.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你有兴趣阅读更多关于机器学习、深度学习、数据科学和数据运维的文章，可以在 [Medium](https://towardsdatascience.com/medium.com/@dpoulopoulos/follow)、[LinkedIn](https://www.linkedin.com/in/dpoulopoulos/)
    或在Twitter上的 [@james2pl](https://twitter.com/james2pl) 关注我。
- en: Opinions expressed are solely my own and do not express the views or opinions
    of my employer.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 所表达的意见仅代表我个人，并不代表我的雇主的观点或意见。
