- en: Data Collators in HuggingFace
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: HuggingFace中的数据整理器
- en: 原文：[https://towardsdatascience.com/data-collators-in-huggingface-a0c76db798d2](https://towardsdatascience.com/data-collators-in-huggingface-a0c76db798d2)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/data-collators-in-huggingface-a0c76db798d2](https://towardsdatascience.com/data-collators-in-huggingface-a0c76db798d2)
- en: What they are and what they do
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它们是什么以及它们的功能
- en: '[](https://medium.com/@mina.ghashami?source=post_page-----a0c76db798d2--------------------------------)[![Mina
    Ghashami](../Images/745f53b94f5667a485299b49913c7a21.png)](https://medium.com/@mina.ghashami?source=post_page-----a0c76db798d2--------------------------------)[](https://towardsdatascience.com/?source=post_page-----a0c76db798d2--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----a0c76db798d2--------------------------------)
    [Mina Ghashami](https://medium.com/@mina.ghashami?source=post_page-----a0c76db798d2--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@mina.ghashami?source=post_page-----a0c76db798d2--------------------------------)[![Mina
    Ghashami](../Images/745f53b94f5667a485299b49913c7a21.png)](https://medium.com/@mina.ghashami?source=post_page-----a0c76db798d2--------------------------------)[](https://towardsdatascience.com/?source=post_page-----a0c76db798d2--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----a0c76db798d2--------------------------------)
    [Mina Ghashami](https://medium.com/@mina.ghashami?source=post_page-----a0c76db798d2--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----a0c76db798d2--------------------------------)
    ·9 min read·Nov 8, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [数据科学前沿](https://towardsdatascience.com/?source=post_page-----a0c76db798d2--------------------------------)
    ·9分钟阅读·2023年11月8日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/2ddfbf36b4be5c6b3f2d3216d22460d6.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2ddfbf36b4be5c6b3f2d3216d22460d6.png)'
- en: Image from [unsplash.com](https://unsplash.com/photos/assorted-source-codes-sp1BZ1atp7M)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来自 [unsplash.com](https://unsplash.com/photos/assorted-source-codes-sp1BZ1atp7M)
- en: When I started learning HuggingFace, data collators were one of the least intuitive
    components for me. I had a hard time understanding them, and I did not find good
    enough resources that explain them intuitively.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 当我开始学习HuggingFace时，数据整理器是我最不直观的组件之一。我很难理解它们，而且没有找到足够好的资源来直观地解释它们。
- en: In this post, we take a look at what data collators are, how they differ, and
    how to write a customized data collator.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我们将探讨数据整理器是什么，它们有何不同，以及如何编写自定义的数据整理器。
- en: 'Data Collators: High Level'
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据整理器：高级
- en: Data collators are an essential part of data processing in HuggingFace. We all
    have used them after tokenizing the data, and before passing the data to the Trainer
    object to train the model.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 数据整理器是HuggingFace中数据处理的关键部分。我们在对数据进行标记化后，都会使用它们，然后将数据传递给Trainer对象来训练模型。
- en: In a nutshell, they put together a list of samples into a mini training batch.
    What they do depends on the task they are defined for, but at the very least they
    pad or truncate input samples to make sure all samples in a mini batch are of
    same length. Typical mini-batch sizes range from 16 to 256 samples, depending
    on the model size, data, and hardware constraints.
  id: totrans-12
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 简而言之，它们将样本列表整理成一个小型训练批次。它们的功能取决于它们所定义的任务，但至少会对输入样本进行填充或截断，以确保小批次中的所有样本长度相同。典型的小批量大小范围从16到256个样本，具体取决于模型大小、数据和硬件限制。
- en: 'Data collators are **task-specific**. There is a data collator for each of
    the following tasks:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 数据整理器是**任务特定的**。每个以下任务都有一个数据整理器：
- en: Causal language modeling (CLM)
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 因果语言建模（CLM）
- en: Masking language modeling (MLM)
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 掩码语言建模（MLM）
- en: Sequence classification
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 序列分类
- en: Seq2Seq
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Seq2Seq
- en: Token classification
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 标记分类
- en: '*Some data collators are simple.* For example for the “sequence classification”
    task, the data collator just needs to pad all sequences in a mini batch to ensure
    they are of the same length. It would then concatenate them into one tensor.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '*一些数据整理器很简单*。例如，对于“序列分类”任务，数据整理器只需填充小批量中的所有序列以确保它们长度相同。然后将它们连接成一个张量。'
- en: '*Some data collators are quite complex*, as they need to handle the data processing
    for that task.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '*一些数据整理器非常复杂*，因为它们需要处理该任务的数据处理。'
- en: Basic Data Collators
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基本数据整理器
- en: 'Two of most basic data collators are as following:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 最基本的两个数据整理器如下：
- en: '[**1)DefaultDataCollator**](https://github.com/huggingface/transformers/blob/v4.35.0/src/transformers/data/data_collator.py#L78)**:**
    This does not do any padding or truncation. It assumes all input samples are of
    the same length. If your input samples are not of the same length, this would
    throw errors.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '[**1)DefaultDataCollator**](https://github.com/huggingface/transformers/blob/v4.35.0/src/transformers/data/data_collator.py#L78)**:**
    这不进行任何填充或截断。它假设所有输入样本的长度相同。如果你的输入样本长度不一致，这会引发错误。'
- en: '[PRE0]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'After tokenizing about the output is :'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 令牌化后的输出是：
- en: '[PRE1]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: As you see the two sequences have different lengths. The first sequence is 4
    tokens, the second sequence is 6 tokens. This is the cause of error when you try
    to call the data loader !
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，两个序列的长度不同。第一个序列有4个标记，第二个序列有6个标记。这是在调用数据加载器时出现错误的原因！
- en: 'Now if we change the input to two sequences of the same length, for example:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 现在如果我们将输入更改为两个相同长度的序列，例如：
- en: '[PRE2]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Then the code works fine and the output will be:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 然后代码运行正常，输出将是：
- en: '[PRE3]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Notice that it does not return `labels`.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，它不返回`labels`。
- en: '[**2) DataCollatorWithPadding**](https://github.com/huggingface/transformers/blob/v4.35.0/src/transformers/data/data_collator.py#L215)**:**
    This collator pads the input samples so that they are all of the same length.
    For padding,'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '[**2) DataCollatorWithPadding**](https://github.com/huggingface/transformers/blob/v4.35.0/src/transformers/data/data_collator.py#L215)**:**
    这个整理器填充输入样本，使它们都具有相同长度。对于填充，'
- en: either it pads to the `max_length` argument provided
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它要么填充到提供的`max_length`参数。
- en: or it pads to the largest sequence in the batch.
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 或者它填充到批次中最长的序列。
- en: See the [documentation](https://github.com/huggingface/transformers/blob/v4.35.0/src/transformers/data/data_collator.py#L215)
    for more details.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 查看 [文档](https://github.com/huggingface/transformers/blob/v4.35.0/src/transformers/data/data_collator.py#L215)
    获取更多详细信息。
- en: In addition, this collator accepts *tokenizer* as many tokenizers have different
    padding token and so **DataCollatorWithPadding** accepts tokenizer to figure out
    the padding token while padding the sequence.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，这个整理器接受*tokenizer*，因为许多标记器有不同的填充标记，因此**DataCollatorWithPadding**接受标记器来确定填充序列时的填充标记。
- en: '[PRE4]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The output of above code is:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码的输出是：
- en: '[PRE5]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Note the first sequence which was 4 tokens is now padded to be 6 tokens. The
    padding token id is 0 in `bert-base-uncase` tokenizer. Let’s see that in code:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 注意第一个序列的4个标记现在填充为6个标记。在`bert-base-uncase`标记器中，填充标记的ID是0。让我们在代码中查看：
- en: '[PRE6]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: This code outputs
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码输出：
- en: '[PRE7]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: and `pad_token = [PAD]` is what is used in padding. Let’s check its token_id.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '`pad_token = [PAD]` 是用于填充的标记。让我们检查它的token_id。'
- en: '[PRE8]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: and this prints `0` .
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 这输出为`0`。
- en: Pay attention that DataCollatorWithPadding similar to DefaultDataCollator does
    not create `labels`. If you have labels in your data already, they will return
    it but otherwise they won’t create it !!
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，与DefaultDataCollator类似，DataCollatorWithPadding不创建`labels`。如果你的数据中已经有标签，它们会返回，但否则不会创建！！
- en: In a nutshell, use these two collators if your labels is straightforward and
    your data does not need any special processing before feeding it to the model
    for training.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，如果你的标签简单明了，且数据在喂给模型进行训练之前不需要任何特殊处理，则使用这两个整理器。
- en: Language Modeling Data Collators
  id: totrans-50
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 语言建模数据整理器
- en: 'Language modeling data collator comes in two modes:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 语言建模数据整理器有两种模式：
- en: 'MLM data collator: this is for masked language modeling, where we mask 15%
    of tokens and the model predict them'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MLM 数据整理器：这是用于掩码语言建模的，我们掩盖15%的标记，模型进行预测。
- en: 'CLM data collator: this is for causal language modeling where we mask all tokens
    to the right side of the current token, and expect the model to predict the next
    token at each step.'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CLM 数据整理器：这是用于因果语言建模的，我们掩盖当前标记右侧的所有标记，并期望模型在每一步预测下一个标记。
- en: 'In code, the MLM collator is:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在代码中，MLM整理器是：
- en: '[PRE9]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'and the CLM collator is:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: CLM 整理器是：
- en: '[PRE10]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Let’s see an example of MLM collator:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看MLM整理器的例子：
- en: '[PRE11]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'It prints the following:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 它输出如下内容：
- en: '[PRE12]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Few points to pay attention to:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 注意几点：
- en: '**1)**The `input_ids` for both example starts with `101` and end with `102`
    . In the second example, after `102` , there is a `0` which we know already is
    a padding token. Let’s see what is `101` and `102` ?'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '**1)** 两个示例的`input_ids`都以`101`开头，以`102`结尾。在第二个示例中，`102`之后有一个`0`，我们已经知道这是填充标记。让我们看看`101`和`102`是什么？'
- en: '[PRE13]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: It prints `[CLS]` and `[SEP]` tokens, respectively.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 它分别打印`[CLS]`和`[SEP]`标记。
- en: '**2)** It returns `labels` , unlike the basic data collators. We see there
    are many `-100` in labels. Note the length of the `labels` is the same as length
    of `input_ids` for each sample! The places that `label=-100` it means the corresponding
    token was not masked, and therefore we have to ignore this when computing the
    loss. MLM uses cross entropy loss function and if you check the [documentation](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html)
    the default `ignore_index=-100` ; that means ignore if a label is set to `-100`
    .'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '**2)** 它返回`labels`，不同于基本的数据整理器。我们看到标签中有很多`-100`。请注意，每个样本的`labels`长度与`input_ids`的长度相同！标签为`-100`的地方意味着对应的标记没有被屏蔽，因此在计算损失时我们需要忽略这些位置。MLM使用交叉熵损失函数，如果你查看[文档](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html)，默认的`ignore_index=-100`；这意味着如果标签设置为`-100`则忽略。'
- en: 3) Third point to pay attention to is that for the first example, the lable
    is `[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]`
    which means none of its tokens were masked. However for the second input, the
    labels are `[-100, -100, -100, 4083, -100, -100, -100, -100, -100, -100, -100,
    -100]` and we see that one token was masked and its corresponding label is `4083`.
    The corresponding token in the `input_ids` is `103` which is the token_id for
    `[MASK]` token.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 第三点需要注意的是，对于第一个示例，标签是`[-100, -100, -100, -100, -100, -100, -100, -100, -100,
    -100, -100, -100]`，这意味着没有任何标记被屏蔽。然而，对于第二个输入，标签是`[-100, -100, -100, 4083, -100,
    -100, -100, -100, -100, -100, -100, -100]`，我们看到有一个标记被屏蔽，其对应的标签是`4083`。`input_ids`中的对应标记是`103`，这是`[MASK]`标记的token_id。
- en: 'The CLM collator is a lot easier as it is for causal language modeling i.e
    to predict the next token. Let’s take a look at an example:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: CLM数据整理器要简单得多，因为它用于因果语言建模，即预测下一个标记。我们来看一个示例：
- en: '[PRE14]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'And the output is as following:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE15]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: you see for the two examples, the labels is a copy of input_ids. It is because
    in causal language modeling, the task is to predict the token given all previous
    tokens and the label for a position is the token itself.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 你会看到在这两个示例中，标签是`input_ids`的副本。这是因为在因果语言建模中，任务是预测给定所有先前标记的下一个标记，而某个位置的标签就是该标记本身。
- en: Customize A Data Collator
  id: totrans-73
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自定义数据整理器
- en: 'Let’s assume you have a dataset that contain two columns: *instruction* and
    *response*.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你有一个数据集，包含两列：*指令*和*响应*。
- en: '![](../Images/374b3f866e6a56054bcdf2e8a7f648f9.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/374b3f866e6a56054bcdf2e8a7f648f9.png)'
- en: image by author
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图像
- en: And you want to do *instruction tuning* on a pre-trained model. Without getting
    into too much details of training, we notice that we should have a customized
    data collator to only mask response and not the instruction.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 你想对一个预训练模型进行*指令调优*。不深入细节，我们注意到我们需要一个自定义数据整理器来仅屏蔽响应而不是指令。
- en: 'Let’s say we write a function that combines the two columns into the following
    format:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们写一个函数，将两列合并成以下格式：
- en: '`Below is an instruction that describes a task. Write a response that appropriately
    completes the request. ### Instruction: {instruction} ### Response: {response}
    ### End`'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '`下面是一个描述任务的指令。写一个适当完成请求的响应。 ### 指令: {instruction} ### 响应: {response} ### 结束`'
- en: 'The data now looks like this:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 数据现在看起来是这样的：
- en: '![](../Images/5a007260bacde6a05127c40c16b51934.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5a007260bacde6a05127c40c16b51934.png)'
- en: Image by author
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图像
- en: 'We see that there are special tokens:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到有特殊标记：
- en: Instruction
  id: totrans-84
  prefs:
  - PREF_UL
  - PREF_H3
  type: TYPE_NORMAL
  zh: 指令
- en: Response
  id: totrans-85
  prefs:
  - PREF_UL
  - PREF_H3
  type: TYPE_NORMAL
  zh: 响应
- en: End
  id: totrans-86
  prefs:
  - PREF_UL
  - PREF_H3
  type: TYPE_NORMAL
  zh: 结束
- en: So let’s get started by writing customized data collator. We want to have a
    collator that only masks response and not instruction. Why? because we want the
    model to generate the response not the instruction.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 那么让我们开始编写自定义数据整理器。我们希望有一个仅屏蔽响应而不屏蔽指令的数据整理器。为什么？因为我们希望模型生成响应而不是指令。
- en: '[PRE16]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'This data collator finds the location of `###Response: \n` and changes the
    label to any token before that to `-100` .This way the loss function is going
    to ignore those tokens.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '这个数据整理器找到`###Response: \n`的位置，并将该位置之前的任何标记的标签更改为`-100`。这样，损失函数将忽略这些标记。'
- en: 'And when we call the collator, we use it as :'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们调用数据整理器时，我们使用它如下：
- en: '[PRE17]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: As usual, we use this data_collator inside Training object before doing the
    model training.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 像往常一样，我们在进行模型训练之前，在训练对象中使用这个数据整理器。
- en: Conclusion
  id: totrans-93
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: In this post, we looked at data collators in HuggingFace. We learned that data
    collators are responsible for padding the sequences so that all samples in a batch
    are of same length. We also saw four different examples of data collators. One
    important data collator is `DataCollatorForLanguageModeling` which is used for
    both MLM and CLM training. We also saw an example of how to modify a data collator
    for instruction tuning.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我们深入探讨了 HuggingFace 中的数据收集器。我们了解到数据收集器负责填充序列，使得批次中的所有样本长度相同。我们还看到了四种不同的数据收集器示例。其中一个重要的数据收集器是
    `DataCollatorForLanguageModeling`，它用于 MLM 和 CLM 训练。我们还看到了如何修改数据收集器以进行指令调优的示例。
- en: 'If you have any questions or suggestions, feel free to reach out to me:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你有任何问题或建议，请随时联系我：
- en: 'Email: mina.ghashami@gmail.com'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '电子邮件: mina.ghashami@gmail.com'
- en: 'LinkedIn: [https://www.linkedin.com/in/minaghashami/](https://www.linkedin.com/in/minaghashami/)'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 'LinkedIn: [https://www.linkedin.com/in/minaghashami/](https://www.linkedin.com/in/minaghashami/)'
