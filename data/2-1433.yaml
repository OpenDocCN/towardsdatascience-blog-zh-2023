- en: 'LLM Economics: ChatGPT vs Open-Source'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LLM 经济学：ChatGPT 与开源
- en: 原文：[https://towardsdatascience.com/llm-economics-chatgpt-vs-open-source-dfc29f69fec1](https://towardsdatascience.com/llm-economics-chatgpt-vs-open-source-dfc29f69fec1)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/llm-economics-chatgpt-vs-open-source-dfc29f69fec1](https://towardsdatascience.com/llm-economics-chatgpt-vs-open-source-dfc29f69fec1)
- en: How much does it cost to deploy LLMs like ChatGPT? Are open-source LLMs cheaper
    to deploy? What are the tradeoffs?
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 部署像 ChatGPT 这样的 LLM 成本是多少？开源 LLM 部署是否更便宜？有哪些权衡？
- en: '[](https://skanda-vivek.medium.com/?source=post_page-----dfc29f69fec1--------------------------------)[![Skanda
    Vivek](../Images/9d25bee2fb75176ca7f7ea6eff7d7ab5.png)](https://skanda-vivek.medium.com/?source=post_page-----dfc29f69fec1--------------------------------)[](https://towardsdatascience.com/?source=post_page-----dfc29f69fec1--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----dfc29f69fec1--------------------------------)
    [Skanda Vivek](https://skanda-vivek.medium.com/?source=post_page-----dfc29f69fec1--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://skanda-vivek.medium.com/?source=post_page-----dfc29f69fec1--------------------------------)[![Skanda
    Vivek](../Images/9d25bee2fb75176ca7f7ea6eff7d7ab5.png)](https://skanda-vivek.medium.com/?source=post_page-----dfc29f69fec1--------------------------------)[](https://towardsdatascience.com/?source=post_page-----dfc29f69fec1--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----dfc29f69fec1--------------------------------)
    [Skanda Vivek](https://skanda-vivek.medium.com/?source=post_page-----dfc29f69fec1--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----dfc29f69fec1--------------------------------)
    ·6 min read·Apr 26, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----dfc29f69fec1--------------------------------)
    ·阅读时间 6 分钟·2023 年 4 月 26 日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/fa975d94faf37f22c807696c80da0324.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fa975d94faf37f22c807696c80da0324.png)'
- en: Cartoon schematic for comparing LLM costs | Skanda Vivek
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 比较 LLM 成本的卡通示意图 | Skanda Vivek
- en: 'TLDR: For lower usage in the 1000’s of requests per day range ChatGPT works
    out cheaper than using open-sourced LLMs deployed to AWS. For millions of requests
    per day, open-sourced models deployed in AWS work out cheaper. (As of writing
    this article on April 24th, 2023.)'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 'TLDR: 对于每天请求数在 1000 个左右的低使用量，ChatGPT 的成本低于使用部署在 AWS 上的开源 LLM。对于每天请求数在百万级别的高使用量，部署在
    AWS 上的开源模型成本更低。（截至撰写本文日期 2023 年 4 月 24 日。）'
- en: Large Language Models are taking the world by storm. Transformers were introduced
    in 2017, followed by breakthrough models like BERT, GPT, and BART — 100’s of millions
    of parameters; and capable to performing multiple Language tasks like sentiment
    analysis, Q&A, classification, etc.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 大语言模型正在席卷全球。变压器模型于 2017 年推出，随后出现了突破性的模型如 BERT、GPT 和 BART——拥有数亿参数；并能够执行多种语言任务，如情感分析、问答、分类等。
- en: A couple of years ago — researchers from OpenAI and Google documented multiple
    papers showing that large language models with more than 10’s of Billions of parameters
    started showing emergent capabilities where they seemingly understand complex
    aspects of language and are almost human-like in their responses.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 几年前——来自 OpenAI 和 Google 的研究人员记录了多篇论文，显示具有超过 10 亿参数的大语言模型开始展示出突现能力，它们似乎能够理解语言的复杂方面，并且在回答时几乎像人类一样。
- en: '![](../Images/f39567219981d5f1f495ab8bd9f9174f.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f39567219981d5f1f495ab8bd9f9174f.png)'
- en: '[GPT-3 paper](https://arxiv.org/abs/2005.14165) showing the impressive learning
    capabilities of Large Language Models'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '[GPT-3 论文](https://arxiv.org/abs/2005.14165)展示了大语言模型令人印象深刻的学习能力。'
- en: The GPT-3 paper showed that models > 10-100 Billion parameters show impressive
    learning capabilities with just a few tens of prompts.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-3 论文展示了参数超过 10-100 亿的模型在仅有几十个提示的情况下展现了令人印象深刻的学习能力。
- en: However, these LLMs are so resource intensive that they were economically challenging
    to deploy at scale. That is, until the recent arrival of ChatGPT. Soon after the
    ChatGPT interface was released, OpenAI made the ChatGPT API accessible, so developers
    could use ChatGPT in their applications. Let’s see how much these cost at scale
    and the economic viability.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这些 LLM 的资源消耗非常大，以至于在大规模部署时经济上具有挑战性。直到最近 ChatGPT 的出现才改变了这种状况。ChatGPT 界面发布后不久，OpenAI
    使 ChatGPT API 可用，以便开发者可以在他们的应用中使用 ChatGPT。让我们看看这些大规模部署的成本以及经济可行性。
- en: ChatGPT API costs
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ChatGPT API 成本
- en: The ChatGPT API is priced by usage. It costs $0.002/1K tokens. Each token is
    3/4th a word roughly — and the number of tokens in a single request is the sum
    of prompt + generated output tokens. Let’s say you process 1000 small chunks of
    text per day, each chunk being a page of text — so 500 words or 667 tokens. **This
    comes to $0.002/1000x667*1000= ~$1.3 a day.** Not too bad!
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: ChatGPT API 按使用量计费。每 1K 个 token 的费用为 $0.002。每个 token 大约是一个单词的 3/4 —— 单次请求中的
    token 数量是提示 + 生成的输出 token 的总和。假设你每天处理 1000 个小文本块，每个文本块为一页 —— 即 500 个单词或 667 个
    tokens。**这将花费 $0.002/1000x667*1000= ~$1.3 一天。** 还不错！
- en: But what happens if you are processing a million such documents a day? **Then
    it is $1,300 per day or ~0.5 Million$ per year!** ChatGPT goes from being a cool
    toy to being a major expense (and consequently one hopes — a major source of revenue)
    in a multi-million dollar business!
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，如果你每天处理一百万个这样的文档会发生什么？**那么，每天的费用为 $1,300 或每年 ~0.5 百万美元！** ChatGPT 从一个有趣的玩具变成了一个主要的开支（因此希望
    —— 也是一个主要的收入来源）在一个数百万美元的业务中！
- en: Opensource Generative Models
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 开源生成模型
- en: After ChatGPT came out, there were a bunch of open source initiatives. Meta
    came out with [LLaMA — a 10’s of billions of parameters LLM model](https://ai.facebook.com/blog/large-language-model-llama-meta-ai/)
    that outperforms GPT-3\. Stanford then fine-tuned the 7B version of LLaMA on 52K
    instruction-following demos and found that [their Alpaca model](https://crfm.stanford.edu/2023/03/13/alpaca.html)
    outperformed GPT-3.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在 ChatGPT 发布之后，出现了一些开源的倡议。Meta 发布了 [LLaMA——一个拥有数十亿参数的 LLM 模型](https://ai.facebook.com/blog/large-language-model-llama-meta-ai/)，其性能优于
    GPT-3。斯坦福大学随后对 LLaMA 的 7B 版本进行了 52K 指令跟随演示的微调，发现 [他们的 Alpaca 模型](https://crfm.stanford.edu/2023/03/13/alpaca.html)
    优于 GPT-3。
- en: '[A team of researchers also recently showed a 13B parameter fine-tuned LLaMA
    model called Vicuna acheived >90% of ChatGPT quality](https://vicuna.lmsys.org/).
    There are multiple reasons why companies might choose to use opensource generative
    models instead of the GPT family of models from OpenAI. These include less susceptibility
    to OpenAI outages, easier to customize, and possibly cheaper.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '[最近有研究团队展示了一种 13B 参数的微调 LLaMA 模型，称为 Vicuna，其质量达到了 ChatGPT 的 >90%](https://vicuna.lmsys.org/)。公司可能选择使用开源生成模型而非
    OpenAI 的 GPT 系列模型的原因有多个，包括对 OpenAI 故障的较低敏感性、定制更为容易，以及可能更便宜。'
- en: While open source models are free to use, the infrastructure to host and deploy
    them is not. And while earlier transformer models like BERT could easily be run
    and fine-tuned on personal computers with a good CPU and basic GPUs, LLMs are
    more resource intensive. A common solution is to use cloud providers like AWS
    to host and deploy such models.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然开源模型免费使用，但托管和部署它们的基础设施却不是。而且，早期的 transformer 模型如 BERT 可以在配备良好的 CPU 和基础 GPU
    的个人计算机上轻松运行和微调，但 LLMs 更为资源密集。一个常见的解决方案是使用云服务提供商如 AWS 来托管和部署这些模型。
- en: Let’s dive into AWS costs for hosting open source models.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们深入了解托管开源模型的 AWS 成本。
- en: AWS costs
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: AWS 成本
- en: 'First, let’s discuss the standard architecture for deploying models in AWS
    and serving them as APIs. Usually there are three steps:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们讨论在 AWS 上部署模型并将其作为 API 提供服务的标准架构。通常有三个步骤：
- en: Deploying the model as an endpoint usingAWS Sagemaker.
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 AWS Sagemaker 将模型部署为端点。
- en: Connecting the Sagemaker endpoint to AWS Lambda.
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将 Sagemaker 端点连接到 AWS Lambda。
- en: Serving the Lambda function as an API through API Gateway
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过 API 网关将 Lambda 函数作为 API 提供服务
- en: '![](../Images/f6023397d6b436e188b548f74e4fc97a.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f6023397d6b436e188b548f74e4fc97a.png)'
- en: '[Invoking a Sagemaker model endpoint using API Gateway and Lambda](https://aws.amazon.com/blogs/machine-learning/call-an-amazon-sagemaker-model-endpoint-using-amazon-api-gateway-and-aws-lambda/)'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '[使用 API 网关和 Lambda 调用 Sagemaker 模型端点](https://aws.amazon.com/blogs/machine-learning/call-an-amazon-sagemaker-model-endpoint-using-amazon-api-gateway-and-aws-lambda/)'
- en: When the client makes an API call to the API Gateway, it triggers the lambda
    function, that parses the function and sends it to the Sagemaker endpoint. The
    model endpoint then does the prediction, and sends the information to Lambda.
    Lambda parses this and sends it to the API, and eventually back to the client.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 当客户端向 API 网关发起 API 调用时，会触发 Lambda 函数，该函数解析请求并将其发送到 Sagemaker 端点。模型端点随后进行预测，并将信息发送到
    Lambda。Lambda 解析这些信息并将其发送到 API，最终返回给客户端。
- en: The Sagemaker costs are sensitive to the type of computing instance for hosting
    the model. LLMs use rather large computing instance.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: Sagemaker 的成本对托管模型的计算实例类型非常敏感。LLMs 使用相当大的计算实例。
- en: '![](../Images/b32a9a0d869317c427aa9307917cd4f7.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b32a9a0d869317c427aa9307917cd4f7.png)'
- en: '[Sagemaker instance pricing](https://aws.amazon.com/sagemaker/pricing/?p=pm&c=sm&z=2)'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '[Sagemaker 实例定价](https://aws.amazon.com/sagemaker/pricing/?p=pm&c=sm&z=2)'
- en: 'For example, this article written by someone from AWS details how to deploy
    Flan UL2 — a 20 Billion parameter model, on AWS:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，这篇由 AWS 的某人撰写的文章详细说明了如何在 AWS 上部署 Flan UL2 —— 一个 200 亿参数的模型：
- en: '[](https://betterprogramming.pub/deploy-flan-ul2-on-a-single-gpu-1778dac605f3?source=post_page-----dfc29f69fec1--------------------------------)
    [## Deploy Flan-UL2 on a Single GPU With Amazon SageMaker'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://betterprogramming.pub/deploy-flan-ul2-on-a-single-gpu-1778dac605f3?source=post_page-----dfc29f69fec1--------------------------------)
    [## 使用 Amazon SageMaker 在单个 GPU 上部署 Flan-UL2'
- en: The Hugging Face + AWS partnership makes it easier than ever to experiment with
    open-source state-of-the-art language…
  id: totrans-36
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Hugging Face + AWS 的合作使得实验开源最先进语言模型变得比以往更加容易……
- en: betterprogramming.pub](https://betterprogramming.pub/deploy-flan-ul2-on-a-single-gpu-1778dac605f3?source=post_page-----dfc29f69fec1--------------------------------)
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '[betterprogramming.pub](https://betterprogramming.pub/deploy-flan-ul2-on-a-single-gpu-1778dac605f3?source=post_page-----dfc29f69fec1--------------------------------)'
- en: The article used the ml.g5.4xlarge instance. While the Sagemaker pricing above
    didn’t list the cost for this specific instance price, it seems like it would
    be in the ballpark of ~5$ per hour. Which comes to 150$ per day! And this is just
    for instance hosting, we haven’t yet come to the Lambda and API Gateway costs.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 文章使用了 ml.g5.4xlarge 实例。虽然上面的 Sagemaker 定价没有列出这个特定实例的价格，但看起来大约在每小时 ~5$ 的范围。这就意味着每天
    150$！这只是实例托管的费用，我们还没有计算 Lambda 和 API 网关的费用。
- en: Below details the AWS lambda pricing — which is a function of memory usage and
    frequency of requests.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是 AWS Lambda 定价的详细信息——这取决于内存使用量和请求频率。
- en: '![](../Images/76c3c94e61983612142278210fbaa61c.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/76c3c94e61983612142278210fbaa61c.png)'
- en: '[AWS Lambda Pricing](https://aws.amazon.com/lambda/pricing/)'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '[AWS Lambda 定价](https://aws.amazon.com/lambda/pricing/)'
- en: Let’s say it takes 5s for obtaining a response. 128 MB is plenty given we are
    routing the data to the AWS Sagemaker endpoint. So this would cost 5*.128*1000*$0.0000166667=
    0.01$ for 1000 requests, or 10$ for 1M requests.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 假设获取响应需要 5 秒。128 MB 足够了，因为我们将数据路由到 AWS Sagemaker 端点。所以这将花费 5 * .128 * 1000 *
    $0.0000166667 = 0.01$ 每 1000 次请求，或 10$ 每 1M 次请求。
- en: 'And the final cost is for the API gateway:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 最终成本是 API 网关的费用：
- en: '![](../Images/bae2fc46e6a3b3c1ae8ba8e38e0c60e5.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bae2fc46e6a3b3c1ae8ba8e38e0c60e5.png)'
- en: '[AWS API Gateway Pricing](https://aws.amazon.com/api-gateway/pricing/)'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '[AWS API 网关定价](https://aws.amazon.com/api-gateway/pricing/)'
- en: As you can see, the API Gateway is pretty cheap — 1$ per million requests.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，API 网关相当便宜——每百万次请求 1$。
- en: So ultimately **the cost of hosting an open-source LLM like Flan-UL2 on AWS
    is 150$ for 1000 requests a day, and 160$ for 1 M requests a day.**
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，**在 AWS 上托管开源 LLM，如 Flan-UL2 的成本是每天 1000 次请求 150$，每天 1M 次请求 160$。**
- en: But do we always need such costly computing instances? For smaller language
    models like BERT that are 100’s of millions of parameters — you can get away with
    using cheaper instances like ml.m5.xlarge that is **$0.23/hour and ~5$ a day.**
    These models are also pretty powerful, and are more task and training data specific
    as compared to LLMs that seem to understand the complex nuances of language.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 但我们是否总是需要如此昂贵的计算实例？对于像 BERT 这样的较小语言模型（参数数量为亿级）——你可以使用更便宜的实例，比如 **$0.23/小时和 ~5$
    每天**。这些模型也相当强大，并且比那些似乎能够理解复杂语言细微差别的 LLM 更加针对任务和训练数据。
- en: Closing Thoughts
  id: totrans-49
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结思考
- en: 'So which is better? Using paid service LLMs like OpenAI’s GPT series? Or open-access
    LLMs? It depends on the use case:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 那么哪个更好？使用像 OpenAI 的 GPT 系列这样的付费服务 LLM？还是开放访问 LLM？这取决于使用场景：
- en: '![](../Images/8da5d346bdd3bdd5e95c61253fdf48cf.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8da5d346bdd3bdd5e95c61253fdf48cf.png)'
- en: Pros and Cons of Paid Service LLMs | Skanda Vivek
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 付费服务 LLM 的利与弊 | Skanda Vivek
- en: '![](../Images/edbb5072fea5e8b9d77b42a82d1b4cb9.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/edbb5072fea5e8b9d77b42a82d1b4cb9.png)'
- en: Pros and Cons of Open-Access LLMs | Skanda Vivek
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 开放访问 LLM 的利与弊 | Skanda Vivek
- en: 'Note: Since this is such a rapidly advancing field, it is quite possible that
    due to the large-scale demands, in the relatively near future, deployment costs
    drastically reduce. (Keep in mind that while hosting open-source LLMs is a challenge,
    smaller language models like BERT that have 100’s of millions of parameters are
    still a great option for specific tasks. I’ve written articles on how fine-tuning
    BERT based models on tasks like [question answering](/fine-tune-transformer-models-for-question-answering-on-custom-data-513eaac37a80)
    and [spam detection](/transformer-models-for-custom-text-classification-through-fine-tuning-3b065cc08da1)
    can yield near human performance.)'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：由于这是一个迅速发展的领域，由于大规模的需求，未来相对较近的时期，部署成本大幅降低的可能性很大。（请记住，虽然托管开源 LLM 是一个挑战，但像
    BERT 这样具有数亿参数的小型语言模型在特定任务中仍然是一个很好的选择。我已撰写有关如何通过对 BERT 基础模型进行微调来实现类似人类表现的任务，如 [问答系统](/fine-tune-transformer-models-for-question-answering-on-custom-data-513eaac37a80)
    和 [垃圾邮件检测](/transformer-models-for-custom-text-classification-through-fine-tuning-3b065cc08da1)
    的文章。）
- en: But which model is better? The responses from ChatGPT and GPT-4 are more relevant
    than those from open-source LLMs. However, open-source models are catching up
    quickly. And there might be very good reasons for using open source models rather
    than closed APIs.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 但哪个模型更好呢？ChatGPT 和 GPT-4 的回应比开源 LLM 的回应更具相关性。然而，开源模型正在快速赶上。而且，使用开源模型而不是封闭 API
    可能有非常好的理由。
- en: Companies want to fine-tune open source models on their specific data sources.
    ChatGPT and subsequent OpenAI models might not perform as well as open sourced
    models fine-tuned on domain specific data; due to the generic nature of such models.
    Already, we are seeing domain specific models like BloombergGPT making powerful
    moves in Generative AI.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 公司希望在其特定数据源上对开源模型进行微调。ChatGPT 和后续的 OpenAI 模型可能无法像在领域特定数据上微调的开源模型那样表现良好；由于这些模型的通用性质。我们已经看到像
    BloombergGPT 这样的领域特定模型在生成式 AI 方面取得了强大的进展。
- en: Oh — and let’s all pray that OpenAI does not hike the price of the ChatGPT API.
    When the ChatGPT API came out it was pleasantly surprising that the API was priced
    10X cheaper than the earlier GPT-3 API.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 哦——我们都祈祷 OpenAI 不会提高 ChatGPT API 的价格。当 ChatGPT API 刚推出时，令人惊讶的是它的定价比早期的 GPT-3
    API 便宜了 10 倍。
- en: We live in exciting times!
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 我们生活在激动人心的时代！
- en: '*If you like this post, follow me — I write on topics related to applying state-of-the-art
    NLP in real-world applications and, more generally, on the intersections between
    data and society.*'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '*如果你喜欢这篇文章，请关注我——我撰写关于将最先进的 NLP 应用于现实世界应用的主题，更一般地说，也涉及数据与社会的交集。*'
- en: '*Feel free to connect with me on* [*LinkedIn*](https://www.linkedin.com/in/skanda-vivek-01619311b/)*!*'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '*欢迎通过* [*LinkedIn*](https://www.linkedin.com/in/skanda-vivek-01619311b/)*与我联系！*'
- en: '*If you are not yet a Medium member and want to support writers like me, feel
    free to sign-up through my referral link:* [*https://skanda-vivek.medium.com/membership*](https://skanda-vivek.medium.com/membership)'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '*如果你还不是 Medium 的会员，并且想要支持像我这样的作者，欢迎通过我的推荐链接注册：* [*https://skanda-vivek.medium.com/membership*](https://skanda-vivek.medium.com/membership)'
