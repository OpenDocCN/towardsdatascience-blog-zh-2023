- en: Increase Llama 2's Latency and Throughput Performance by Up to 4X
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将 Llama 2 的延迟和吞吐量性能提高多达 4 倍
- en: 原文：[https://towardsdatascience.com/increase-llama-2s-latency-and-throughput-performance-by-up-to-4x-23034d781b8c?source=collection_archive---------1-----------------------#2023-08-09](https://towardsdatascience.com/increase-llama-2s-latency-and-throughput-performance-by-up-to-4x-23034d781b8c?source=collection_archive---------1-----------------------#2023-08-09)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/increase-llama-2s-latency-and-throughput-performance-by-up-to-4x-23034d781b8c?source=collection_archive---------1-----------------------#2023-08-09](https://towardsdatascience.com/increase-llama-2s-latency-and-throughput-performance-by-up-to-4x-23034d781b8c?source=collection_archive---------1-----------------------#2023-08-09)
- en: Real-world benchmarks for Llama-2 13B
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Llama-2 13B 的现实世界基准
- en: '[](https://medium.com/@het.trivedi05?source=post_page-----23034d781b8c--------------------------------)[![Het
    Trivedi](../Images/f6f11a66f60cacc6b553c7d1682b2fc6.png)](https://medium.com/@het.trivedi05?source=post_page-----23034d781b8c--------------------------------)[](https://towardsdatascience.com/?source=post_page-----23034d781b8c--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----23034d781b8c--------------------------------)
    [Het Trivedi](https://medium.com/@het.trivedi05?source=post_page-----23034d781b8c--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[![Het Trivedi](../Images/f6f11a66f60cacc6b553c7d1682b2fc6.png)](https://medium.com/@het.trivedi05?source=post_page-----23034d781b8c--------------------------------)
    [![Towards Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----23034d781b8c--------------------------------)
    [Het Trivedi](https://medium.com/@het.trivedi05?source=post_page-----23034d781b8c--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fce8ebd0c262c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fincrease-llama-2s-latency-and-throughput-performance-by-up-to-4x-23034d781b8c&user=Het+Trivedi&userId=ce8ebd0c262c&source=post_page-ce8ebd0c262c----23034d781b8c---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----23034d781b8c--------------------------------)
    ·7 min read·Aug 9, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F23034d781b8c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fincrease-llama-2s-latency-and-throughput-performance-by-up-to-4x-23034d781b8c&user=Het+Trivedi&userId=ce8ebd0c262c&source=-----23034d781b8c---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fce8ebd0c262c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fincrease-llama-2s-latency-and-throughput-performance-by-up-to-4x-23034d781b8c&user=Het+Trivedi&userId=ce8ebd0c262c&source=post_page-ce8ebd0c262c----23034d781b8c---------------------post_header-----------)
    发布在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----23034d781b8c--------------------------------)
    · 7分钟阅读 · 2023年8月9日'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F23034d781b8c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fincrease-llama-2s-latency-and-throughput-performance-by-up-to-4x-23034d781b8c&source=-----23034d781b8c---------------------bookmark_footer-----------)![](../Images/26cdb97ee3584c0c46666f945c041218.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/26cdb97ee3584c0c46666f945c041218.png)'
- en: Image By Author — Created using Stable Diffusion
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图像 — 使用 Stable Diffusion 创建
- en: Introduction
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 介绍
- en: In the realm of large language models (LLMs), integrating these advanced systems
    into real-world enterprise applications is a pressing need. However, the pace
    at which generative AI is evolving is so quick that most can’t keep up with the
    advancements.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在大型语言模型（LLMs）的领域，将这些先进系统集成到实际的企业应用中是一个迫切的需求。然而，生成式 AI 发展的速度如此之快，以至于大多数人无法跟上这些进展。
- en: 'One solution is to use managed services like the ones provided by OpenAI. These
    managed services offer a streamlined solution, yet for those who either lack access
    to such services or prioritize factors like security and privacy, an alternative
    avenue emerges: open-source tools.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 一种解决方案是使用如 OpenAI 提供的托管服务。这些托管服务提供了精简的解决方案，但对于那些无法访问这些服务或优先考虑安全和隐私等因素的人，另一个途径是：开源工具。
- en: Open-source generative AI tools are extremely popular right now and companies
    are scrambling to get their AI-powered apps out the door. While trying to build
    quickly, companies oftentimes forget that in order to truly gain value from generative
    AI they need to build “production”-ready apps, not just prototypes.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 目前开源生成 AI 工具非常流行，许多公司争相推出其 AI 驱动的应用程序。在快速构建的过程中，公司们常常忘记，要真正从生成 AI 中获得价值，他们需要构建“生产”就绪的应用程序，而不仅仅是原型。
- en: In this article, I want to show you the performance difference for Llama 2 using
    two different inference methods. The first method of inference will be a containerized
    Llama 2 model served via Fast API, a popular choice among developers for serving
    models as REST API endpoints. The second method will be the same containerized
    model served via [Text Generation Inference](https://github.com/huggingface/text-generation-inference),
    an open-source library developed by hugging face to easily deploy LLMs.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我想向你展示 Llama 2 使用两种不同推理方法的性能差异。第一种推理方法是通过 Fast API 服务的容器化 Llama 2 模型，这是一种在开发者中非常受欢迎的选择，用于将模型作为
    REST API 端点进行服务。第二种方法是通过 [文本生成推理](https://github.com/huggingface/text-generation-inference)
    服务的相同容器化模型，这是 Hugging Face 开发的开源库，用于轻松部署 LLM。
- en: Both methods we’re looking at are meant to work well for real-world use, like
    in businesses or apps. But it’s important to realize that they don’t scale the
    same way. We’ll dive into this comparison to see how they each perform and understand
    the differences better.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 我们正在查看的这两种方法都旨在适用于实际使用，例如在商业或应用程序中。但重要的是要认识到，它们的扩展方式不同。我们将深入比较这两种方法，看看它们各自的表现，并更好地理解差异。
- en: What powers LLM inference at OpenAI and Cohere
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 支撑 OpenAI 和 Cohere 的 LLM 推理
- en: Have you ever wondered why ChatGPT is so fast?
  id: totrans-16
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 你是否曾经好奇为什么 ChatGPT 这么快？
- en: Large language models require a ton of computing power and due to their sheer
    size, they oftentimes need multiple GPUs. When working with large GPU clusters,
    companies have to be very mindful of how their computing is being utilized.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型需要大量计算能力，由于其庞大的规模，它们往往需要多个 GPU。在处理大型 GPU 集群时，公司必须非常注意计算资源的使用情况。
- en: LLM providers like OpenAI run large GPU clusters to power inferencing for their
    models. In order to squeeze as much performance from their GPUs they use tools
    like the [**Nvidia Triton inference server**](https://docs.nvidia.com/deeplearning/triton-inference-server/user-guide/docs/index.html)to
    increase throughput and reduce latency.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 像 OpenAI 这样的 LLM 提供商运行大型 GPU 集群来支撑其模型的推理。为了最大限度地提高 GPU 性能，他们使用像 [**Nvidia Triton
    推理服务器**](https://docs.nvidia.com/deeplearning/triton-inference-server/user-guide/docs/index.html)
    这样的工具来提高吞吐量并减少延迟。
- en: '![](../Images/ccdba02d89abddcc6f0400743e72c945.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ccdba02d89abddcc6f0400743e72c945.png)'
- en: Illustration inspired by — [Triton Inference Server Architecture](https://developer.nvidia.com/triton-inference-server)
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 插图灵感来源 — [Triton 推理服务器架构](https://developer.nvidia.com/triton-inference-server)
- en: While Triton is highly performant and has many benefits, it’s quite difficult
    to use for developers. A lot of the newer models on Hugging Face are not supported
    on Triton and the process to add support for these models is not trivial.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管 Triton 性能卓越且有许多优点，但对于开发者来说使用起来非常困难。许多 Hugging Face 上的较新模型在 Triton 上不受支持，添加对这些模型的支持的过程也并不简单。
- en: This is where [**Text Generation Inference**](https://github.com/huggingface/text-generation-inference)
    **(TGI)** comes in. This tool offers many of the same performance improvements
    as Triton, but it’s user-friendly and works well with Hugging Face models.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 这时，[**文本生成推理**](https://github.com/huggingface/text-generation-inference) **（TGI）**
    就派上用场了。这个工具提供了与 Triton 相同的性能提升，但它更用户友好，并且与 Hugging Face 模型兼容性好。
- en: LLM inference optimizations
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: LLM 推理优化
- en: Before we jump into the benchmarks, I want to cover a few of the optimization
    techniques used by modern inference servers such as TGI to speed up LLMs.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们深入基准测试之前，我想介绍一些现代推理服务器如 TGI 用于加速 LLM 的优化技术。
- en: '**Tensor Parallelism**'
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**张量并行**'
- en: LLMs are often too large to fit on a single GPU. Using a concept called **Model
    Parallelism**, a model can be split across multiple GPUs. **Tensor Parallelism**
    is a type of model parallelism that splits the model into multiple shards that
    are processed independently by different GPUs.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: LLM 通常太大而无法容纳在单个 GPU 上。通过一种称为 **模型并行** 的概念，可以将模型分割到多个 GPU 上。**张量并行** 是一种模型并行，它将模型分割成多个由不同
    GPU 独立处理的分片。
- en: '![](../Images/45eec79a2e1998a5ffa7a1b793b302ed.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/45eec79a2e1998a5ffa7a1b793b302ed.png)'
- en: Illustration inspired by [source](https://aws.amazon.com/blogs/machine-learning/deploy-large-models-at-high-performance-using-fastertransformer-on-amazon-sagemaker/)
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 插图灵感来自于 [来源](https://aws.amazon.com/blogs/machine-learning/deploy-large-models-at-high-performance-using-fastertransformer-on-amazon-sagemaker/)
- en: To put it simply, imagine you’re working on a big jigsaw puzzle, but it’s so
    huge that you can’t put all the pieces on one table. So, you decide to work with
    your friend. You split the puzzle into sections, and each of you works on your
    own section at the same time. This way, you can solve the puzzle faster.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，想象你在拼一个大拼图，但它太大了，你无法把所有的拼图片放在一张桌子上。所以，你决定和你的朋友一起工作。你把拼图分成几个部分，每个人同时处理自己的一部分。这样，你可以更快地完成拼图。
- en: '**2\. Continuous batching**'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '**2\. 连续批处理**'
- en: When you make an API call to an LLM it processes it in one go and returns the
    output. If you make 5 API calls, it will process each one sequentially. This essentially
    means we have a batch size of 1, meaning only 1 request can be processed at a
    given time. As you might guess, this design isn’t ideal because each new request
    has to wait for the one before it to complete.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 当你向LLM发起API调用时，它会一次性处理并返回结果。如果你发起5个API调用，它会顺序处理每一个。这实际上意味着我们有一个批处理大小为1，即每次只能处理1个请求。正如你所猜测的，这种设计并不理想，因为每个新请求必须等待前一个请求完成。
- en: '![](../Images/747627ad956e1a700b820f5671378f18.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/747627ad956e1a700b820f5671378f18.png)'
- en: Illustration inspired by [Static Batching](https://www.anyscale.com/blog/continuous-batching-llm-inference)
    — Need to wait for all processes to finish before handling more requests
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 插图灵感来自于 [静态批处理](https://www.anyscale.com/blog/continuous-batching-llm-inference)
    — 需要等所有进程完成后才能处理更多请求
- en: By increasing the batch size you can handle more requests in parallel. With
    a batch size of 4, you would be able to handle 4 of the 5 API calls in parallel.
    You would have to wait until all 4 requests have finished before handling the
    5th request.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 通过增加批处理大小，你可以并行处理更多请求。批处理大小为4时，你可以并行处理5个API调用中的4个。你必须等到这4个请求全部完成后，才能处理第5个请求。
- en: '![](../Images/7b94fc126972c1f54510bdb069b0f160.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7b94fc126972c1f54510bdb069b0f160.png)'
- en: Illustration inspired by [Continuous Batching](https://www.anyscale.com/blog/continuous-batching-llm-inference)
    — You can handle new requests immediately without waiting for all processes to
    finish
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 插图灵感来自于 [连续批处理](https://www.anyscale.com/blog/continuous-batching-llm-inference)
    — 你可以立即处理新请求，而不必等待所有进程完成
- en: '**Continuous batching** builds on the idea of using a bigger batch size and
    goes a step further by immediately tackling new tasks as they come in. For example,
    let’s say that your GPU has a batch size of 4 meaning it can handle 4 requests
    in parallel. If you make 5 requests, 4 of them will get processed in parallel
    and whichever process finishes first will immediately get to work on the 5th request.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '**连续批处理**基于使用更大批处理大小的想法，并进一步通过立即处理新任务来提升效率。例如，假设你的GPU的批处理大小为4，这意味着它可以并行处理4个请求。如果你发起5个请求，其中4个会被并行处理，而第一个完成的进程会立即处理第5个请求。'
- en: Llama 2 Benchmarks
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Llama 2 基准测试
- en: Now that we have a basic understanding of the optimizations that allow for faster
    LLM inferencing, let’s take a look at some practical benchmarks for the **Llama-2
    13B** model.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们对优化方案有了基本的理解，能够实现更快的LLM推理，让我们来看一下**Llama-2 13B**模型的一些实际基准。
- en: 'There are 2 main metrics I wanted to test for this model:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 我想测试这个模型的2个主要指标：
- en: Throughput (tokens/second)
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 吞吐量（tokens/second）
- en: Latency (time it takes to complete one full inference)
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 延迟（完成一次完整推理所需的时间）
- en: I wanted to compare the performance of Llama inference using two different instances.
    One instance runs via FastAPI, while the other operates through TGI. Both setups
    utilize GPUs for computation.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 我想比较Llama推理在两种不同实例上的性能。一种实例通过FastAPI运行，而另一种通过TGI操作。两个设置都利用了GPU进行计算。
- en: 'Note: Neither of the two instances quantizes the weights of the model.'
  id: totrans-44
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 注意：这两个实例都没有量化模型的权重。
- en: The TGI setup employs the power of two GPUs*(NVIDIA RTX A4000)* to leverage
    parallelism, whereas FastAPI relies on a single*(NVIDIA A100)*, albeit more powerful
    GPU.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: TGI 设置利用了两个GPU*(NVIDIA RTX A4000)*的强大性能来实现并行，而FastAPI依赖于一个*(NVIDIA A100)*，尽管更强大的GPU。
- en: Making a direct comparison between these two approaches is a bit tricky since
    FastAPI doesn’t allow the model to be distributed across two GPUs. To level the
    playing field, I opted to equip the FastAPI instance with a more powerful GPU.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 直接比较这两种方法有点棘手，因为FastAPI不允许模型分布在两个GPU上。为了公平起见，我选择为FastAPI实例配备了更强大的GPU。
- en: Each API request made to the model had the exact **same prompt** and the generated
    output token limit was set to **128 tokens**.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 对模型发出的每个API请求都使用**相同的提示**，生成的输出token限制设置为**128 tokens**。
- en: '**Throughput Results:**'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '**吞吐量结果：**'
- en: '![](../Images/8a515b42238c85ff8d70cf1b0e267765.png)![](../Images/f1b2dde2c315b7fd66019670592d9e1a.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8a515b42238c85ff8d70cf1b0e267765.png)![](../Images/f1b2dde2c315b7fd66019670592d9e1a.png)'
- en: 'Left: Fast API vs TGI throughput | Right: Average throughput increase — Illustration
    by Author'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 左：Fast API与TGI吞吐量 | 右：平均吞吐量提升 — 作者插图
- en: 'Analysis:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 分析：
- en: In both cases, throughput decreases as the number of inference requests increases
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在这两种情况下，随着推理请求数量的增加，吞吐量都会下降。
- en: Batching significantly improves the throughput for an LLM, hence why TGI has
    better throughput
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 批处理显著提高了LLM的吞吐量，这就是为什么TGI的吞吐量更好的原因。
- en: Although the Fast API instance has a larger GPU memory (VRAM) available to manage
    requests as larger batches, it doesn’t handle this process efficiently
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 尽管Fast API实例具有更多的GPU内存（VRAM）用于处理更大的批次请求，但它处理这个过程并不高效。
- en: '**Latency Results:**'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '**延迟结果：**'
- en: '![](../Images/d91f5d18b02d21d95e6679353030e562.png)![](../Images/c2599ec06642198c04ef397ea1a30377.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d91f5d18b02d21d95e6679353030e562.png)![](../Images/c2599ec06642198c04ef397ea1a30377.png)'
- en: 'Left: Fast API vs TGI latency | Right: Average latency performance increase
    — Illustration by Author'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 左：Fast API与TGI延迟 | 右：平均延迟性能提升 — 作者插图
- en: 'Analysis:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 分析：
- en: Tensor parallelism results in a nearly 5X reduction in latency for TGI!
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 张量并行使TGI的延迟减少了近5倍！
- en: As the number of requests increases, the latency for the Fast API-based instance
    surpasses 100 seconds
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随着请求数量的增加，基于Fast API的实例的延迟超过100秒。
- en: As evident from the results, optimized inference servers are highly performant
    compared to readily available API wrappers. As a final test, I wanted to evaluate
    the performance of TGI when the generated output token limit is increased to **256
    tokens** as opposed to **128 tokens**.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 从结果中可以看出，与现成的API包装器相比，优化过的推理服务器具有很高的性能。作为最终测试，我想评估当生成的输出token限制增加到**256 tokens**时TGI的性能，与**128
    tokens**相比。
- en: '**Throughput TGI 128 tokens vs 256 tokens:**'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '**吞吐量 TGI 128 tokens 与 256 tokens：**'
- en: '![](../Images/a0dc8d9f8996937171d30a949de765c9.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a0dc8d9f8996937171d30a949de765c9.png)'
- en: 128 token vs 256 token TGI throughput test — Illustration by Author
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 128 token 与 256 token TGI吞吐量测试 — 作者插图
- en: As you can see, the throughput is quite similar despite doubling the number
    of generated tokens. One thing to note that’s not on this chart is that at 300
    concurrent requests, the throughput dwindled to approximately 2 tokens/sec while
    producing a 256-token output. At this throughput, the latency was over 100 seconds
    per request and there were multiple request timeouts. Due to these substantial
    performance limitations, the results from this scenario were excluded from the
    chart.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所见，尽管生成的token数量翻倍，但吞吐量非常相似。需要注意的是，这张图表没有显示，在300个并发请求时，吞吐量降到大约2 token/秒，同时生成256
    token的输出。此时，延迟超过每个请求100秒，并且出现了多次请求超时。由于这些显著的性能限制，这种情况的结果被排除在图表之外。
- en: '**Latency TGI 128 tokens vs 256 tokens:**'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '**延迟 TGI 128 tokens 与 256 tokens：**'
- en: '![](../Images/13370bf7f28cb05d33a5817d47669502.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/13370bf7f28cb05d33a5817d47669502.png)'
- en: 128 token vs 256 token TGI latency test — Illustration by Author
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 128 token 与 256 token TGI延迟测试 — 作者插图
- en: Unlike the throughput, the latency clearly spikes when generating longer sequences
    of text. Adding more GPUs would help decrease the latency, but it would come at
    a financial cost.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 与吞吐量不同，生成更长文本序列时，延迟明显增加。增加更多的GPU可以帮助减少延迟，但会带来财务成本。
- en: Conclusion
  id: totrans-70
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结论
- en: My goal for this blog post was to compare the real-world performance of LLMs
    at scale*(hundreds of requests per second).* Often times it’s easy to deploy models
    behind an API wrapper such as Fast API, but in the case of LLMs, you’d be leaving
    a significant amount of performance on the table.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 我写这篇博客的目标是比较大规模LLM的实际性能*(每秒数百个请求)*。通常情况下，部署模型在像Fast API这样的API包装器后是很简单的，但对于LLM而言，你可能会错过相当多的性能。
- en: Even with the optimization techniques used by modern inference servers, the
    performance cannot match that of a managed service like ChatGPT. OpenAI surely
    runs several large GPU clusters to power inference for their models along with
    their own in-house optimization techniques.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 即使使用现代推理服务器的优化技术，其性能也无法与像 ChatGPT 这样的托管服务相比。OpenAI 当然运行了几个大型 GPU 集群来为他们的模型提供推理支持，并结合了他们自己内部的优化技术。
- en: However, for generative AI use cases enterprises will likely have to adopt inference
    servers as they are far more scalable and reliable compared to traditional model
    deployment techniques.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，对于生成式 AI 的使用场景，企业可能需要采用推理服务器，因为它们在可扩展性和可靠性方面远超传统的模型部署技术。
- en: Thanks for reading!
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 感谢阅读！
- en: Peace.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 平静。
