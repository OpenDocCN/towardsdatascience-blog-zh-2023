- en: 'Dynamic Pricing with Contextual Bandits: Learning by Doing'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用上下文强盗进行动态定价：通过实践学习
- en: 原文：[https://towardsdatascience.com/dynamic-pricing-with-contextual-bandits-learning-by-doing-b88e49f55894](https://towardsdatascience.com/dynamic-pricing-with-contextual-bandits-learning-by-doing-b88e49f55894)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/dynamic-pricing-with-contextual-bandits-learning-by-doing-b88e49f55894](https://towardsdatascience.com/dynamic-pricing-with-contextual-bandits-learning-by-doing-b88e49f55894)
- en: Adding context to your dynamic pricing problem can increase opportunities as
    well as challenges
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将上下文添加到你的动态定价问题中可以增加机会，同时也带来挑战
- en: '[](https://medium.com/@massi.costacurta?source=post_page-----b88e49f55894--------------------------------)[![Massimiliano
    Costacurta](../Images/599c3469021c53f116cc67c390db6695.png)](https://medium.com/@massi.costacurta?source=post_page-----b88e49f55894--------------------------------)[](https://towardsdatascience.com/?source=post_page-----b88e49f55894--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----b88e49f55894--------------------------------)
    [Massimiliano Costacurta](https://medium.com/@massi.costacurta?source=post_page-----b88e49f55894--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@massi.costacurta?source=post_page-----b88e49f55894--------------------------------)[![Massimiliano
    Costacurta](../Images/599c3469021c53f116cc67c390db6695.png)](https://medium.com/@massi.costacurta?source=post_page-----b88e49f55894--------------------------------)[](https://towardsdatascience.com/?source=post_page-----b88e49f55894--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----b88e49f55894--------------------------------)
    [Massimiliano Costacurta](https://medium.com/@massi.costacurta?source=post_page-----b88e49f55894--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----b88e49f55894--------------------------------)
    ·17 min read·Oct 5, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----b88e49f55894--------------------------------)
    ·阅读时间 17 分钟·2023年10月5日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/6e3e13d9902455d3cb57bfb4f8ba38a1.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6e3e13d9902455d3cb57bfb4f8ba38a1.png)'
- en: Photo by [Artem Beliaikin](https://unsplash.com/@belart84?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 照片由 [Artem Beliaikin](https://unsplash.com/@belart84?utm_source=medium&utm_medium=referral)
    在 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral) 上提供
- en: From Multi-armed to Contextual Bandits
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从多臂强盗到上下文强盗
- en: In my [previous article](https://medium.com/towards-data-science/dynamic-pricing-with-multi-armed-bandit-learning-by-doing-3e4550ed02ac),
    I conducted a thorough analysis of the most popular strategies for tackling the
    dynamic pricing problem using simple Multi-armed Bandits. If you’ve come here
    from that piece, firstly, thank you. It’s by no means an easy read, and I truly
    appreciate your enthusiasm for the subject. Secondly, get ready, as this new article
    promises to be even more demanding. However, if this is your introduction to the
    topic, I strongly advise beginning with the previous article. There, I present
    foundational concepts, which I’ll assume readers are familiar with in this discussion.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在我的 [上一篇文章](https://medium.com/towards-data-science/dynamic-pricing-with-multi-armed-bandit-learning-by-doing-3e4550ed02ac)
    中，我对使用简单的多臂强盗应对动态定价问题的最流行策略进行了详细分析。如果你是从那篇文章过来的，首先，感谢你。那绝不是一篇容易阅读的文章，我非常感激你对这个话题的热情。其次，做好准备，因为这篇新文章会更加具有挑战性。然而，如果这是你首次接触这个话题，我强烈建议你先从上一篇文章开始阅读。在那篇文章中，我介绍了基础概念，这些概念在本讨论中我会假设读者已经熟悉。
- en: 'Anyway, a brief recap: the prior analysis aimed to simulate a dynamic pricing
    scenario. The main goal was to assess as quickly as possible various price points
    to find the one yielding the highest cumulated reward. We explored four distinct
    algorithms: greedy, ε-greedy, Thompson Sampling, and UCB1, detailing the strengths
    and weaknesses of each. Although the methodology employed in that article is theoretically
    sound, it bears oversimplifications that don’t hold up in more complex, real-world
    situations. The most problematic of these simplifications is the assumption that
    the underlying process is stationary — meaning the optimal price remains constant
    irrespective of the external environment. This is clearly not the case. Consider,
    for example, fluctuations in demand during holiday seasons, sudden shifts in competitor
    pricing, or changes in raw material costs.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 不管怎样，简单回顾一下：之前的分析旨在模拟一个动态定价场景。主要目标是尽快评估各种价格点，找到能够产生最高累计奖励的价格。我们探索了四种不同的算法：贪婪算法、ε-贪婪算法、汤普森采样和UCB1，详细描述了每种算法的优缺点。尽管那篇文章中采用的方法在理论上是可靠的，但它存在一些简化，这些简化在更复杂的现实世界情况下并不成立。其中最具问题的是假设基本过程是稳定的——即最优价格在外部环境无论如何都保持不变。这显然不符合实际。例如，节假日期间需求波动、竞争对手价格的突然变化或原材料成本的变化。
- en: To solve this issue, Contextual Bandits come into play. Contextual Bandits are
    an extension of the Multi-armed Bandit problem where the decision-making agent
    not only receives a reward for each action (or “arm”) but also has access to context
    or environment-related information before choosing an arm. The context can be
    any piece of information that might influence the outcome, such as customer demographics
    or external market conditions.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这个问题，上下文赌博机应运而生。上下文赌博机是多臂赌博机问题的扩展，其中决策代理不仅为每个行动（或“臂”）获得奖励，而且在选择臂之前还可以访问上下文或环境相关信息。上下文可以是任何可能影响结果的信息，例如客户的人口统计数据或外部市场条件。
- en: 'Here’s how they work: before deciding which arm to pull (or, in our case, which
    price to set), the agent observes the current context and uses it to make a more
    informed decision. The agent then learns over time which actions work best for
    which contexts, adapting its strategies based on both the rewards received and
    the contexts in which those rewards were obtained. This continuous learning and
    adapting mechanism can be used by businesses to dynamically adjust their pricing
    strategies in response to ever-changing external factors, leading to potentially
    better performance and increased profits.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 它们的工作方式如下：在决定拉哪个臂（或者在我们的例子中，设置哪个价格）之前，代理会观察当前的上下文，并利用它做出更明智的决策。代理随后随着时间的推移学习哪些行动在特定上下文中效果最佳，并根据所获得的奖励以及这些奖励获得的上下文来调整其策略。这种持续学习和适应机制可以帮助企业动态调整其定价策略，以应对不断变化的外部因素，从而可能提高表现和增加利润。
- en: 'Contextual Bandits: Architecture, Modeling, and Challenges'
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 上下文赌博机：架构、建模与挑战
- en: 'The architecture of the Contextual Bandit problem can be thought of as a generalization
    of the Multi-armed Bandit problem. Both approaches have the overarching goal of
    maximizing rewards over time, that is finding the best balance between exploring
    new actions and exploiting those already known to yield significant rewards. Furthermore,
    they both learn from their history: decisions made and the corresponding rewards
    they receive.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 上下文赌博机问题的架构可以被视为多臂赌博机问题的一个推广。这两种方法的最终目标都是在时间上最大化奖励，即找到探索新行动和利用已知行动之间的最佳平衡。此外，它们都从历史中学习：所做的决策以及所获得的相应奖励。
- en: However, the ways in which they make decisions and learn exhibit fundamental
    differences. The most striking distinction lies in the concept of context. While
    decisions in the Multi-armed Bandit problem are made based solely on the historical
    record of rewards associated with each arm, the contextual bandit incorporates
    additional external information, or context, into its decision-making framework.
    This context often provides crucial insights that can significantly influence
    the outcomes of chosen actions.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，它们做出决策和学习的方式存在根本的不同。最显著的区别在于上下文的概念。在多臂赌博机问题中，决策完全基于与每个臂相关的历史奖励记录，而上下文赌博机则将额外的外部信息或上下文纳入其决策框架。这些上下文通常提供了关键见解，可以显著影响所选择行动的结果。
- en: '![](../Images/8bae350617de71771a11606ae983f5de.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8bae350617de71771a11606ae983f5de.png)'
- en: Architecture of the Contextual Bandits feedback loop. Notice that the information
    coming from the customer and/or the environment is now an input to the bandits.
    Image by author.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 上下文赌博机反馈循环的架构。注意到来自客户和/或环境的信息现在是赌博机的输入。图片由作者提供。
- en: The presence of context in the Contextual Bandit problem, however, necessitates
    a more sophisticated modeling approach. Here, a predictive model (often referred
    to as “oracle”) for each arm is needed to help identify the best action based
    on the given context. This involves leveraging techniques such as linear or logistic
    regression, neural networks, or other predictive algorithms that can effectively
    weave in the context to anticipate rewards.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，上下文赌博机问题中的上下文存在，需要更复杂的建模方法。在这里，需要一个针对每个臂的预测模型（通常称为“预言机”），以帮助基于给定的上下文识别最佳动作。这涉及利用线性回归、逻辑回归、神经网络或其他预测算法等技术，这些技术能够有效地融入上下文以预测奖励。
- en: Given this added dimension of context, it’s evident that Contextual Bandits
    present a level of complexity surpassing that of the Multi-armed Bandits. Instead
    of merely tracking rewards, it requires a more sophisticated analysis to learn
    how varying contexts correlate with rewards for different actions. In essence,
    while Multi-armed Bandits offer a simpler, history-focused view, Contextual Bandits
    provide a richer, more adaptive perspective, attuned to changing environments
    and their implications on potential rewards.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于这种额外的上下文维度，很明显上下文赌博机呈现出一种超越多臂赌博机的复杂性。它不仅仅是跟踪奖励，还需要更复杂的分析来学习不同上下文如何与不同动作的奖励相关联。从本质上讲，多臂赌博机提供了一个较简单、以历史为中心的视角，而上下文赌博机则提供了一个更丰富、更具适应性的视角，适应于变化的环境及其对潜在奖励的影响。
- en: 'And now the bad news. As mentioned earlier, oracles are generally predictive
    models. They can be any Machine Learning model capable of generating predictions
    and associated probabilities given a specific context. Unfortunately, while most
    Machine Learning algorithms excel at making predictions and estimating probabilities,
    many fall short in providing a measure of uncertainty for their predictions. Uncertainty
    is crucial for leveraging approaches such as Thompson Sampling or Upper Confidence
    Bound. With Contextual Bandits, implementing these becomes especially challenging,
    since creating an uncertainty measure (perhaps through bootstrapping or ensemble
    methods) would complicate an already non-trivial architecture. Although there
    are [frameworks](https://contextual-bandits.readthedocs.io/en/latest/) that incorporate
    these features, I’ll set them aside for this discussion and focus solely on the
    two primary uncertainty-free algorithms: greedy and ε-greedy.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是坏消息。正如前面提到的，预言机通常是预测模型。它们可以是任何能够在特定上下文下生成预测和相关概率的机器学习模型。不幸的是，虽然大多数机器学习算法在生成预测和估计概率方面表现优异，但许多算法在提供预测的不确定性度量方面存在不足。不确定性对利用诸如汤普森采样或上置信界等方法至关重要。对于上下文赌博机来说，实施这些方法特别具有挑战性，因为创建一个不确定性度量（也许通过自助法或集成方法）将使本已复杂的架构更加复杂。虽然有[框架](https://contextual-bandits.readthedocs.io/en/latest/)融入了这些特性，但在这次讨论中我将把它们搁置，专注于两个主要的无不确定性算法：贪婪算法和ε-贪婪算法。
- en: Modeling The Context
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 上下文建模
- en: Having gained a clearer understanding of Contextual Bandits, it’s now time to
    set up the environment to test them. In the previous article, I designed a simple
    demand curve using the logistic function. The purpose of this function is to provide
    an expected probability of purchase for a hypothetical customer at any given price
    point.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在对上下文赌博机有了更清晰的理解后，现在是时候建立环境来测试它们了。在上一篇文章中，我使用逻辑函数设计了一个简单的需求曲线。这个函数的目的是提供在任何给定价格点的假设客户的购买预期概率。
- en: 'As previously discussed, the parameter *b* determines the steepness of the
    demand curve, indicating how quickly the curve converges to 0\. This, in turn,
    impacts the identification of the optimal price. In the Multi-armed Bandit scenario,
    we assigned a single value to both *a* and *b*. However, for the contextual setup,
    while we still are going to keep *a* fixed (equal to 2), we intend to make *b*
    contingent on the values of our context. Specifically, for our simulation, we’ll
    define a context made up of two features: geographical area and age. Essentially,
    we are postulating that our demand curve — represented by the *b* value — varies
    based on these two parameters.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，参数*b*决定需求曲线的陡峭程度，指示曲线收敛到0的速度。这反过来会影响最优价格的确定。在多臂老虎机场景中，我们为*a*和*b*分配了单一值。然而，在上下文设置中，虽然我们仍将保持*a*不变（等于2），但我们打算使*b*取决于我们上下文的值。具体来说，在我们的模拟中，我们将定义一个由两个特征组成的上下文：地理区域和年龄。本质上，我们假设我们的需求曲线——由*b*值表示——会根据这两个参数变化。
- en: 'To streamline our analysis, we’ll limit the input space. For the geographical
    area, we’ll consider two regions: ‘EU’ and ‘US’. For age, instead of using a continuous
    range, we’ll segment it into four distinct buckets. As a result, we’ll have a
    total of eight unique contexts to work with. Naturally, this is a simplified model,
    and it serves as a foundation upon which more complicated scenarios can be developed
    by introducing additional contextual features. Another relevant assumption we’re
    making is that the context solely dictates the non-stationarity of the demand
    curve. In other words, while different contexts lead to different demand curves,
    these demand curves are time-independent, meaning their parameters do not vary
    based on the time step.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简化我们的分析，我们将限制输入空间。对于地理区域，我们将考虑两个区域：‘EU’和‘US’。对于年龄，我们将其分成四个不同的桶，而不是使用连续范围。因此，我们将有总共八个独特的上下文进行分析。显然，这是一个简化模型，它作为基础可以通过引入额外的上下文特征来发展更复杂的场景。另一个相关的假设是，上下文仅决定需求曲线的非平稳性。换句话说，虽然不同的上下文导致不同的需求曲线，但这些需求曲线是时间独立的，这意味着其参数不随时间步骤变化。
- en: With our contexts clearly defined, the next step is to allocate a specific *b*
    value for each one. This will enable us to determine the optimal price point tailored
    to each context (see the previous article for more details about finding the optimal
    price for a given demand curve).
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们明确了上下文之后，下一步是为每个上下文分配一个特定的*b*值。这将使我们能够确定针对每个上下文的**最优价格点**（有关如何找到给定需求曲线的最优价格的更多细节，请参见之前的文章）。
- en: '![](../Images/d463874ebab888d0a260349fdbd61b6a.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d463874ebab888d0a260349fdbd61b6a.png)'
- en: Context mapping with the values related to the demand parameter ‘b’ and the
    corresponding optimal prices. Image by author.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 上下文映射与需求参数‘b’相关的值以及相应的最优价格。图片由作者提供。
- en: The table presented above delineates the combination of context and *b* values,
    along with the corresponding optimal prices. I’ve selected the *b* values deliberately
    so as to easily generate a set of candidate prices to test our models. These prices
    are (15, 20, 25, 30, 35, 40, 45, 50, 55, 60) and they represent our ‘arms’, the
    possible actions we can choose from to maximize the cumulated reward.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 上表阐明了上下文和*b*值的组合，以及相应的最优价格。我故意选择了*b*值，以便轻松生成一组候选价格来测试我们的模型。这些价格是（15, 20, 25,
    30, 35, 40, 45, 50, 55, 60），它们代表了我们的‘臂’，即我们可以选择的可能行动，以最大化累积奖励。
- en: 'To provide you with a clearer understanding of our bandits’ objective, let’s
    focus on the arm/price set at 30\. The oracle ‘managing’ this arm needs to determine
    the purchase probabilities for all 8 contexts. Even though these probabilities
    are not known in practice, in our simulation they can be calculated using the
    demand curve function provided earlier by setting price=30, *a*=2, and adjusting
    *b* based on the specific context. As a reference, here is the set of probabilities
    that the oracle associated with this arm is tasked with learning:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 为了让您更清楚我们老虎机的目标，我们来关注定价为30的臂/价格集合。管理这个臂的神谕需要确定所有8个上下文的购买概率。尽管这些概率在实际中是未知的，但在我们的模拟中，可以通过将价格=30，*a*=2，并根据特定上下文调整*b*，使用前面提供的需求曲线函数进行计算。作为参考，这里是神谕需要学习的与此臂相关的概率集合：
- en: '![](../Images/a389cf98c839bd12b00940d030db10fc.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a389cf98c839bd12b00940d030db10fc.png)'
- en: Purchase probabilities per context for price=30\. Image by author.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 每个上下文的购买概率，价格=30。图片由作者提供。
- en: It goes without saying that all the other arms will cope with their specific
    set of probabilities.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 毋庸置疑，其他所有臂部将处理各自特定的概率集。
- en: 'To put this into practice, here’s the code that dynamically generates the context
    at runtime. Additionally, within this code, we’re also constructing the dictionary
    that maps the contexts with the related optimal prices. These dictionaries will
    be particularly useful later when calculating the regret:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实践这一点，这里是动态生成运行时上下文的代码。此外，在这段代码中，我们还构建了一个字典，将上下文与相关的最佳价格进行映射。这些字典在稍后计算遗憾时会特别有用：
- en: 'Setting up the Oracles: Logistic Regression'
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设置预言机：逻辑回归
- en: 'Just as in the simple Multi-armed Bandit scenario, our reward function will
    return either a 1 or a 0, reflecting whether a customer makes a purchase or not.
    In essence, for every possible price point, we’ll progressively compile datasets
    in the format: (geographical area, age bucket, reward). At a glance, this really
    looks like a typical binary classification problem, which leads us to consider
    testing one of the most prevalent binary classification models: logistic regression.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 就像在简单的多臂赌博机场景中一样，我们的奖励函数将返回1或0，反映客户是否进行了购买。实质上，对于每个可能的价格点，我们将逐步编译数据集，格式为：（地理区域，年龄范围，奖励）。乍一看，这确实像是一个典型的二分类问题，这使我们考虑测试最常见的二分类模型之一：逻辑回归。
- en: However, when venturing into the domain of reinforcement learning, the learning
    methodology differs from the traditional supervised learning paradigm. Specifically,
    in reinforcement learning the model is updated continuously, every time a new
    record is introduced. This shift requires the adoption of ‘online’ algorithms,
    which can be incrementally updated as new records populate the dataset. Such algorithms
    are crucial not only for optimizing computational efficiency but also for conserving
    memory resources. Due to this requirement, the conventional implementation of
    logistic regression provided by `sklearn`— which doesn’t support online training
    or “partial fit” — becomes less ideal.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，当涉足强化学习领域时，学习方法与传统的监督学习范式不同。具体来说，在强化学习中，模型会不断更新，每次引入新记录时都会进行更新。这种转变要求采用‘在线’算法，这些算法可以随着新记录的引入逐步更新。这样的算法对于优化计算效率和节省内存资源至关重要。由于这一要求，`sklearn`提供的传统逻辑回归实现——不支持在线训练或“部分拟合”——变得不太理想。
- en: Enter the `SGDClassifier`. The Stochastic Gradient Descent (SGD) Classifier
    is a linear classifier optimized by SGD. Essentially, instead of processing the
    entire dataset to compute the gradient of the objective function, SGD approximates
    it using just a single or a few training samples. This makes it highly efficient
    and suitable for online learning scenarios. To configure `SGDClassifier` to emulate
    logistic regression, we need to set its loss function to 'log_loss'. By doing
    this, we essentially employ the logistic regression model, but with the added
    advantage of SGD's ability to incrementally update the model with each new data
    point, fitting perfectly into our reinforcement learning framework.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 进入`SGDClassifier`。随机梯度下降（SGD）分类器是一个通过SGD优化的线性分类器。实质上，SGD不是处理整个数据集来计算目标函数的梯度，而是使用一个或几个训练样本来近似计算。这使得它在在线学习场景中高度高效。为了将`SGDClassifier`配置为模拟逻辑回归，我们需要将其损失函数设置为'log_loss'。这样，我们实际上采用了逻辑回归模型，但增加了SGD能够随着每个新数据点逐步更新模型的优势，完美契合我们的强化学习框架。
- en: First, we’ll create a `Model` helper class. This class will encapsulate not
    just the `SGDClassifier` instance, but also all the related methods to be utilized
    during our simulation. Additionally, this setup lays the foundation for swiftly
    integrating other models into the simulation. And, spoiler alert, we’ll need to
    do that later.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将创建一个`Model`辅助类。这个类不仅会封装`SGDClassifier`实例，还会包含所有在模拟过程中需要使用的相关方法。此外，这个设置为快速集成其他模型到模拟中奠定了基础。还有，剧透警告，我们稍后会需要这样做。
- en: 'Here is a quick description of the class methods:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 这是类方法的简要描述：
- en: '`add_sample`: Takes in new data points and labels and appends them to the existing
    datasets.'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`add_sample`：接收新的数据点和标签，并将它们附加到现有数据集中。'
- en: '`can_predict`: Determines if the model is capable of making predictions. It
    ensures that there are both positive and negative examples in the dataset to avoid
    the “cold start” problem where the model can’t make meaningful predictions due
    to a lack of diverse data.'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`can_predict`：确定模型是否能够进行预测。它确保数据集中既有正例也有负例，以避免由于缺乏多样化数据导致模型无法做出有意义预测的“冷启动”问题。'
- en: '`get_prob`: Given a data point, this method returns the probability that it
    belongs to the class labeled ‘1’. This probability represents our estimate of
    the likelihood of a purchase occurring for the specified context at the price
    the model is referencing.'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`get_prob`：给定一个数据点，此方法返回它属于标记为‘1’的类别的概率。这个概率表示我们对在模型参考的价格下发生购买的可能性的估计。'
- en: '`fit`: A partial fitting method specific to the logistic regression model.
    It incrementally trains the model on the latest data point without having to retrain
    the entire dataset.'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`fit`：一种特定于逻辑回归模型的部分拟合方法。它在最新数据点上增量训练模型，而无需重新训练整个数据集。'
- en: Setting Up the Simulation
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设置模拟
- en: 'With the `Model` class ready, we can then create a function that simulates
    the decision-making process:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 准备好`Model`类后，我们可以创建一个模拟决策过程的函数：
- en: 'The `run_simulation` code simulates the process of choosing optimal prices
    in order to maximize the reward. It takes in the following arguments:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '`run_simulation`代码模拟了选择最佳价格以最大化奖励的过程。它接受以下参数：'
- en: '`prices`: A list of potential prices to offer.'
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`prices`：要提供的潜在价格列表。'
- en: '`nstep`: The number of steps or iterations for the simulation.'
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`nstep`：模拟的步骤或迭代次数。'
- en: '`strategy`: The decision-making strategy to use, which can be ''random'', ''greedy''
    or ''epsgreedy''.'
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`strategy`：要使用的决策策略，可以是''random''、''greedy''或''epsgreedy''。'
- en: '`model_type`: The type of predictive model to use, with the default being ''logistic''.'
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`model_type`：要使用的预测模型类型，默认为''logistic''。'
- en: Given these parameters, the function chooses a price (referred to as `arm`)
    for each iteration based on the selected strategy. At each step, a customer context
    is generated using the `generate_context()` function. Depending on the strategy,
    the arm is chosen either randomly (that is our evaluation baseline) or based on
    the greedy or ε-greedy algorithm. The customer’s response to the chosen price
    is then simulated using the `get_reward()` function, and the regret (that is the
    difference between the reward obtained adopting the optimal and the chosen price)
    is computed and accumulated. The model corresponding to the selected arm is then
    updated with the new data and retrained. The function also manages the ‘cold start’
    problem, where, in the early iterations, the predictive models might not have
    enough data to make a prediction. In such cases, the first arm whose model cannot
    predict is selected.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些参数的基础上，函数根据选择的策略为每次迭代选择一个价格（称为`arm`）。在每一步，使用`generate_context()`函数生成一个客户上下文。根据策略，`arm`要么随机选择（这是我们的评估基线），要么基于贪婪或ε-贪婪算法选择。然后使用`get_reward()`函数模拟客户对选择价格的反馈，计算并累计遗憾（即采用最优价格与选择价格之间的奖励差异）。然后使用新数据更新与所选`arm`对应的模型，并重新训练。该函数还处理“冷启动”问题，在早期迭代中，预测模型可能没有足够的数据来进行预测。在这种情况下，选择第一个无法预测的`arm`。
- en: The greedy and ε-greedy algorithms were described in detail in the previous
    article. In this section, we will present only the code that implements them in
    the new contextual scenario. It is largely similar to the one used in the Multi-Armed
    Bandit case, the main distinction is that here we use model probabilities to calculate
    the arms’ expected return (that is the product between the probability and the
    related price).
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 贪婪和ε-贪婪算法在之前的文章中有详细描述。在本节中，我们将仅展示在新的上下文场景中实现这些算法的代码。它与多臂赌博机中的使用方式大致相同，主要区别在于这里我们使用模型概率来计算`arms`的期望回报（即概率与相关价格的乘积）。
- en: 'The output of the simulation function is inherently stochastic. To comprehensively
    evaluate the models and the strategies, we will run this simulation 1,000 times.
    Each simulation will last for 10,000 time steps, collecting relevant intermediate
    data to produce aggregated and statistically more reliable results. The two primary
    metrics of our interest are:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 模拟函数的输出本质上是随机的。为了全面评估模型和策略，我们将运行此模拟1,000次。每次模拟将持续10,000个时间步骤，收集相关的中间数据以产生汇总的和统计上更可靠的结果。我们关注的两个主要指标是：
- en: '**Average cumulative regret**: Just like in the Multi-armed Bandit case, this
    is a metric that gives an aggregated sense of this missed opportunity over each
    simulation run. We are going to accumulate the regret curves computed at each
    run and then average them. We can compute this metric since in this simulation
    we know which is the optimal price for each context. Obviously, this is not the
    case in any real-world scenario.'
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**平均累计遗憾**：就像在多臂赌博机的情况一样，这是一个在每次模拟运行中汇总这一错失机会的指标。我们将累计每次运行计算出的遗憾曲线，然后对它们进行平均。我们可以计算这个指标，因为在这次模拟中我们知道每个上下文的最优价格。显然，这在任何真实场景中都不适用。'
- en: '**Average optimal price estimate per context**: This represents the price with
    the highest probability among all predictions made by the models, tailored to
    each context value. In simpler terms, after each simulation run and for every
    possible context, we “ask” the models to offer their probabilities. We then select
    the model — or more accurately, the price linked to that model — with the highest
    expected return. The price estimates generated by each run are then aggregated
    and averaged. By comparing these estimated prices with the optimal ones, we can
    assess the overall ability of the models to “understand” the underlying demand
    curves.'
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**每个上下文的平均最优价格估计**：这代表了在所有模型预测中，每个上下文值的最高概率价格。简单来说，每次模拟运行后，对于每个可能的上下文，我们“询问”模型提供它们的概率。然后，我们选择具有最高预期回报的模型——更准确地说，是与该模型相关的价格。每次运行生成的价格估计随后被汇总并平均。通过将这些估计价格与最优价格进行比较，我们可以评估模型“理解”基本需求曲线的整体能力。'
- en: Below is the code that runs the simulation 1,000 times, collects the required
    data, and computes the aggregated metrics.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是运行模拟1,000次、收集所需数据并计算汇总指标的代码。
- en: 'Before we delve into the simulation results, let me make a brief comment about
    the volume of data in each simulation. Stating that every simulation will run
    for 10,000 steps indicates that we are collecting 10,000 data points in each run.
    In the realm of Machine Learning, a dataset of 10,000 entries is often considered
    the minimum threshold to derive meaningful insights. However, our situation is
    even more challenging. The 10,000 records are actually “distributed” among the
    10 models corresponding to each price candidate (our actions). Consequently, each
    model may have no more than a thousand records for training, which is often insufficient.
    This brings us to a key insight regarding Contextual Bandits: if you have multiple
    actions, you may require a substantial amount of data to glean useful information.
    And by substantial, I mean a lot.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们深入探讨模拟结果之前，让我简要评论一下每次模拟中的数据量。说明每次模拟将运行10,000步，表明我们在每次运行中收集了10,000个数据点。在机器学习领域，10,000条数据通常被视为获取有意义见解的最小阈值。然而，我们的情况更具挑战性。这10,000条记录实际上是“分布”在对应于每个价格候选（我们的行动）的10个模型中。因此，每个模型用于训练的记录可能不超过一千条，这通常是不够的。这带来了一个关于上下文赌博的关键见解：如果你有多个行动，可能需要大量数据才能获取有用的信息。而这个大量，就是很多。
- en: Results
  id: totrans-58
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结果
- en: 'Without further ado, let’s see the plot of the cumulative regret:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 话不多说，来看一下累计遗憾的图表：
- en: '![](../Images/2bb9c2bbe9f612aad16891cc86a22c39.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2bb9c2bbe9f612aad16891cc86a22c39.png)'
- en: Image by author
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片
- en: 'By looking at this plot, your initial reaction might be: “This guy messed up
    the legend labels! the greedy strategy can’t possibly outperform the ε-greedy!”
    At least, that was my thought the first time I viewed this output. The surprise
    is that the plot is accurate, even if it’s highly counterintuitive. Let’s get
    deeper into this and grasp what’s happening by considering the average price estimate.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 看着这个图表，你的初步反应可能是：“这个家伙搞错了图例标签！贪婪策略不可能优于ε-贪婪策略！”至少，这就是我第一次看到这个输出时的想法。惊讶的是，这个图表是准确的，尽管它非常违反直觉。让我们深入探讨，了解发生了什么，通过考虑平均价格估计来理解。
- en: '![](../Images/238475879305b05c76d88b3ff6b22a88.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/238475879305b05c76d88b3ff6b22a88.png)'
- en: Image by author
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片
- en: 'The price estimate plot is essentially “revealing the culprit”. If you observe
    the first four contexts and compare them to the second one, you’ll easily notice
    that the prices predicted by the logistic regression are decreasing (as indicated
    by the two arrows). The root of the issue is straightforward: the logistic regression
    is essentially a linear model, even if we subject it to a nonlinear transformation.
    Regrettably, our data is intrinsically nonlinear, as evidenced by the red dots.
    Hence, the logistic regression is merely striving to accommodate the data, but
    it’s evidently not the appropriate model for this scenario.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 价格估算图基本上是“揭示罪魁祸首”。如果你观察前四个上下文并将它们与第二个上下文进行比较，你会很容易发现逻辑回归预测的价格在下降（如两个箭头所示）。问题的根源很简单：逻辑回归本质上是一个线性模型，即使我们对其进行非线性转换。遗憾的是，我们的数据本质上是非线性的，如红点所示。因此，逻辑回归仅仅是在努力适应数据，但显然不是这个场景下合适的模型。
- en: 'What we’re witnessing is this: adhering to some initial positive outcomes in
    a ‘greedy’ way is less detrimental than depending on the model probabilities.
    This is because the more the models learn from the data, the more they **consistently**
    err in their predictions. At the end of the day, the ‘ε’ exploration merely ensures
    that some data are utilized to refine the models, but the more these models learn,
    the more they lead us astray.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 我们所见的是：以某些初始的积极结果‘贪婪’地进行调整，比依赖模型的概率更少有害。这是因为模型从数据中学习得越多，它们在预测中的**一致性**误差就越大。归根结底，‘ε’探索仅仅确保了一些数据被用来完善模型，但这些模型学习得越多，就越会把我们引入歧途。
- en: 'For those of you not persuaded by this explanation, consider a simple one-dimensional
    scenario: only four points (0, 1, 2, 3) with these respective probabilities of
    yielding a 1: 0.01, 0.08, 0.95, 0.97\. In this case, everybody is happy because
    the logistic regression functions flawlessly and you can even get a nice “by-the-book”
    plot:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 对于那些对这个解释不信服的人，考虑一个简单的一维场景：只有四个点（0，1，2，3），其分别获得1的概率为：0.01，0.08，0.95，0.97。在这种情况下，大家都很满意，因为逻辑回归效果很好，甚至可以得到一个漂亮的“按部就班”的图：
- en: '![](../Images/a277e9d74063d3c715f84df52a9b72a6.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a277e9d74063d3c715f84df52a9b72a6.png)'
- en: Image by author
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片
- en: 'The probabilities output by the logistic regression represent the values of
    the curve at specific points, and here they are quite close to the actual values
    (the green dots). However, let’s see what happens if we modify the probabilities
    to (0.1, 0.8, 0.5, 0.3), which more closely mirrors our pricing scenario:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑回归输出的概率表示曲线在特定点的值，在这里它们非常接近实际值（绿色点）。然而，让我们看看如果我们将概率修改为（0.1，0.8，0.5，0.3），更接近我们的定价场景，会发生什么：
- en: '![](../Images/7352a2a65c4c5e273a1d275db3124243.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7352a2a65c4c5e273a1d275db3124243.png)'
- en: Image by author
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片
- en: In this case, things get nasty because three out of four probability predictions
    will be completely off (those related to inputs 0, 1 and 3). Now, at each iteration,
    we choose the best arm based on the expected returns, that is the products of
    the models’ probabilities and the corresponding arms’ prices. If the probabilities
    are completely off, it should be evident that we’re going to have a big problem.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，事情变得棘手，因为四个概率预测中的三个将完全错误（那些与输入0，1和3相关的）。现在，在每次迭代中，我们根据期望回报选择最佳的臂，即模型概率与相应臂价格的乘积。如果概率完全错误，那么我们将面临一个大问题。
- en: So, what do we do now? The immediate temptation might be to opt for a complex
    nonlinear model like neural networks, or even better, go for a sophisticated architecture
    that ventures into the realm of deep learning. This is certainly an option, and
    there are indeed companies that are exploring this direction for pricing. However,
    we first want to consider if there’s a simpler solution for our situation. If
    only there were an algorithm that could split the input space based on the probability
    of getting a ‘one’ in any given area…
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，我们现在该怎么办？直接的诱惑可能是选择一个复杂的非线性模型，比如神经网络，甚至更好的是，选择一个深入学习领域的复杂架构。这当然是一个选择，确实有公司在探索这种定价方向。然而，我们首先想要考虑是否有更简单的解决方案。如果有一个算法可以根据在任何给定区域获得‘1’的概率来划分输入空间就好了……
- en: 'Adjusting the Oracles: Decision Trees'
  id: totrans-75
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 调整预言机：决策树
- en: 'The last sentence should scream into your ears two simple words: decision trees.
    Why? Because that is precisely the essence of what decision trees do. Decision
    trees partition the input space by recursively splitting it based on specific
    feature thresholds, resulting in a hierarchical structure of nodes and branches.
    Each split, or decision, is made in a manner that best segregates the data according
    to the target variable — in our context, where the probability of getting a ‘one’
    is relatively uniform within a certain region.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一段应该向你耳边呐喊两个简单的词：决策树。为什么？因为这正是决策树的本质。决策树通过根据特定特征阈值递归地划分输入空间，从而形成了一个节点和分支的层次结构。每一次分裂或决策，都是以最佳方式将数据根据目标变量进行区分——在我们的上下文中，‘一’的概率在某个区域内相对均匀。
- en: Once the tree is constructed, each terminal node (or leaf) represents a specific
    segment of the input space where the data exhibits similar characteristics. The
    probability prediction for a given input is then derived from the fraction of
    ‘ones’ within the data samples that fall into the corresponding leaf. For instance,
    if a leaf node has 80 samples, and 40 of them are labeled as ‘one’, the predicted
    probability for an input that reaches this leaf would be 40/80=0.5.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦树构建完成，每个终端节点（或叶子）代表输入空间中的一个特定区域，其中数据表现出类似的特征。对于给定输入的概率预测，然后是从落入相应叶子中的数据样本的‘一’的比例中得出的。例如，如果一个叶子节点有80个样本，其中40个被标记为‘一’，那么对于到达该叶子的输入，预测的概率将是40/80=0.5。
- en: So, they seem to be exactly what we need for our problem. Unfortunately, decision
    trees don’t have an online implementation, which means that at every step we need
    to retrain the model from scratch. On the bright side, decision trees are quite
    fast at training, but you might want to keep that in mind if you plan to retrain
    your models in real time for your real-world application.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，它们似乎正是我们问题所需的。遗憾的是，决策树没有在线实现，这意味着在每一步我们需要从头开始重新训练模型。好的一面是，决策树训练非常迅速，但如果你计划在实际应用中实时重新训练模型，可能需要记住这一点。
- en: 'Alright, first we need to update our `Model` class to incorporate decision
    trees as an option. It’s straightforward — just a series of if-then statements
    to ensure we invoke the appropriate functions based on the initial configuration:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，首先我们需要更新我们的`Model`类以将决策树作为一个选项。很简单——只需一系列的if-then语句以确保我们根据初始配置调用适当的函数：
- en: 'Next, we can re-execute the simulation by simply adding ‘dectree’ to the list
    that the outer loop iterates over. And that’s it, after some time we have our
    final results:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们可以通过简单地将‘dectree’添加到外部循环迭代的列表中来重新执行模拟。就这样，经过一段时间，我们得到了最终结果：
- en: '![](../Images/89f2bb77d30bf2f3246601ba0b2640e3.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/89f2bb77d30bf2f3246601ba0b2640e3.png)'
- en: Image by author
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：作者
- en: 'Now we’re talking. First, decision trees are the top performers, confirming
    the correctness of our initial intuition. Furthermore, the ε-greedy algorithm
    significantly outperforms the simple greedy one, as one would expect. This suggests
    that learning from the context indeed helps the models in making improved predictions
    and, ultimately, facilitates better decision-making. The plot comparing predicted
    prices to optimal prices also looks more promising:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以深入探讨了。首先，决策树表现最佳，确认了我们最初直觉的正确性。此外，ε-贪婪算法显著优于简单的贪婪算法，正如预期的那样。这表明，从上下文中学习确实有助于模型做出更好的预测，并且*最终*促进了更好的决策。比较预测价格与最优价格的图表看起来也更有希望：
- en: '![](../Images/fbe308c2d1428cef9dd8a38421ea095f.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fbe308c2d1428cef9dd8a38421ea095f.png)'
- en: Image by author
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：作者
- en: The ε-greedy algorithm essentially identifies all the optimal prices, outperforming
    the greedy approach in every context.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: ε-贪婪算法本质上识别了所有的最优价格，在每种上下文中都优于贪婪方法。
- en: Lessons learned and next steps
  id: totrans-87
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 经验教训与下一步
- en: '**The More Data, the Better (?):** You might think that if you have data, you
    should always use it. Generally speaking, that’s true. In the realm of Contextual
    Bandits, adding data (the context) to your algorithms can lead to more informed
    decisions and tailored actions. However, there’s a catch. The primary intent behind
    ‘Bandits’ algorithms is to expedite the process of making optimal decisions. Adding
    context introduces complexity, and potentially slows down the attainment of desired
    outcomes. Transitioning to Contextual Bandits might also impede the utilization
    of effective strategies such as Thompson sampling or UCB. As with many choices
    in data science, there’s a tradeoff between objectives and constraints.'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据越多越好（？）：** 你可能认为只要有数据就应该使用它。一般来说，这是对的。在 Contextual Bandits 领域，将数据（背景）添加到你的算法中可以导致更明智的决策和量身定制的行动。然而，有一个陷阱。‘Bandits’
    算法的主要目的是加速做出最佳决策的过程。添加背景会增加复杂性，可能会减缓达成预期结果的速度。过渡到 Contextual Bandits 也可能妨碍有效策略的利用，如
    Thompson 采样或 UCB。正如数据科学中的许多选择一样，在目标和限制之间存在权衡。'
- en: '**Know Your Data and Your Algorithms**: It’s imperative to be cautious about
    the assumptions made regarding your data. The common sentiment is that logistic
    regression is often apt for machine learning tasks because the “linearity assumption
    frequently holds.” While this might often be the case, it’s not a given. Making
    false assumptions can lead to significant pitfalls. This underscores the importance
    of leveraging metrics, ensuring we can quickly gauge the accuracy of our preliminary
    assumptions and adjust our strategies if needed.'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**了解你的数据和算法**：必须小心对数据做出的假设。常见的观点是，逻辑回归通常适用于机器学习任务，因为“线性假设经常成立”。虽然这可能经常是这样，但并非绝对。错误的假设可能导致重大陷阱。这强调了利用指标的重要性，以确保我们能够快速评估初步假设的准确性，并在需要时调整我们的策略。'
- en: '**Keep It Simple (for as long as you can)**: Even in data science, it’s easy
    to get caught up in the latest trends. At the time of this writing, the AI world
    is abuzz with Deep Learning methodologies. Although powerful, it’s essential to
    remember that myriad simpler and robust strategies could be perfectly suited to
    your problem, no more and no less.'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**保持简单（尽可能长时间）**：即便在数据科学领域，也很容易被最新的趋势所吸引。在撰写本文时，人工智能领域正热衷于深度学习方法。虽然强大，但必须记住，有许多更简单且稳健的策略可能完全适合你的问题，不多也不少。'
- en: In conclusion, Contextual Bandits is an exciting research area that certainly
    deserves more attention. In this article, we tackled a relatively simple context,
    but contexts can become incredibly intricate in both data dimensionality and type
    (like text or images). Even if this seems to contradict the previous point about
    simplicity, the logical progression for this analysis is to explore more sophisticated
    algorithms like Deep Learning. While I don’t anticipate adding more chapters to
    this series, I’m eager to hear about your experiences in this field. If you’ve
    delved into this area, please drop a comment or reach out to me directly!
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，Contextual Bandits 是一个令人兴奋的研究领域，确实值得更多关注。本文探讨了一个相对简单的背景，但背景可以在数据维度和类型（如文本或图像）上变得非常复杂。即使这似乎与关于简单性的前一点相矛盾，分析的逻辑进程是探索更复杂的算法，如深度学习。虽然我不打算在这一系列中添加更多章节，但我很想听听你在这个领域的经验。如果你已经深入研究过这个领域，请留下评论或直接联系我！
- en: '**Code Repository**'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '**代码库**'
- en: '[https://github.com/massi82/contextual_bandits](https://github.com/massi82/contextual_bandits)'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/massi82/contextual_bandits](https://github.com/massi82/contextual_bandits)'
- en: '**References**'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '**参考文献**'
- en: '[https://contextual-bandits.readthedocs.io/en/latest/](https://contextual-bandits.readthedocs.io/en/latest/)'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://contextual-bandits.readthedocs.io/en/latest/](https://contextual-bandits.readthedocs.io/en/latest/)'
- en: '[https://www.youtube.com/watch?v=sVpwtj4nSfI](https://www.youtube.com/watch?v=sVpwtj4nSfI)'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://www.youtube.com/watch?v=sVpwtj4nSfI](https://www.youtube.com/watch?v=sVpwtj4nSfI)'
- en: '[](/contextual-bandits-and-reinforcement-learning-6bdfeaece72a?source=post_page-----b88e49f55894--------------------------------)
    [## Contextual Bandits and Reinforcement Learning'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/contextual-bandits-and-reinforcement-learning-6bdfeaece72a?source=post_page-----b88e49f55894--------------------------------)
    [## Contextual Bandits 和 强化学习'
- en: If you develop personalization of user experience for your website or an app,
    contextual bandits can help you. Using…
  id: totrans-98
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 如果你为网站或应用开发个性化用户体验，Contextual Bandits 可以帮助你。使用……
- en: towardsdatascience.com](/contextual-bandits-and-reinforcement-learning-6bdfeaece72a?source=post_page-----b88e49f55894--------------------------------)
    [](https://cloud.google.com/blog/products/ai-machine-learning/how-to-build-better-contextual-bandits-machine-learning-models?source=post_page-----b88e49f55894--------------------------------)
    [## How to build better contextual bandits machine learning models | Google Cloud
    Blog
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '[towardsdatascience.com](/contextual-bandits-and-reinforcement-learning-6bdfeaece72a?source=post_page-----b88e49f55894--------------------------------)
    [](https://cloud.google.com/blog/products/ai-machine-learning/how-to-build-better-contextual-bandits-machine-learning-models?source=post_page-----b88e49f55894--------------------------------)
    [## 如何构建更好的上下文策略带机器学习模型 | Google Cloud Blog'
- en: 'Contextual bandits: what is it, how businesses can apply it, and how to use
    AutoML to implement it'
  id: totrans-100
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 上下文策略带：它是什么，企业如何应用它，以及如何使用 AutoML 实现它
- en: cloud.google.com](https://cloud.google.com/blog/products/ai-machine-learning/how-to-build-better-contextual-bandits-machine-learning-models?source=post_page-----b88e49f55894--------------------------------)
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '[cloud.google.com](https://cloud.google.com/blog/products/ai-machine-learning/how-to-build-better-contextual-bandits-machine-learning-models?source=post_page-----b88e49f55894--------------------------------)'
- en: '[https://courses.cs.washington.edu/courses/cse599i/18wi/resources/lecture10/lecture10.pdf](https://courses.cs.washington.edu/courses/cse599i/18wi/resources/lecture10/lecture10.pdf)'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://courses.cs.washington.edu/courses/cse599i/18wi/resources/lecture10/lecture10.pdf](https://courses.cs.washington.edu/courses/cse599i/18wi/resources/lecture10/lecture10.pdf)'
- en: Did you enjoy this article? If you’re looking for applications of AI, NLP, Machine
    Learning and Data Analytics in solving real-world problems, you’ll likely enjoy
    my other work as well. My goal is to craft actionable articles that show these
    transformative technologies in practical scenarios. If this is also you, follow
    me on Medium to stay informed about my latest pieces!
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 你喜欢这篇文章吗？如果你对人工智能、自然语言处理、机器学习和数据分析在解决现实世界问题中的应用感兴趣，你可能也会喜欢我的其他作品。我的目标是撰写出展示这些变革性技术在实际场景中应用的可操作性文章。如果这也是你的兴趣所在，请在
    Medium 上关注我，以获取最新文章！
