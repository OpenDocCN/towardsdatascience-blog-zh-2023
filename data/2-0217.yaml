- en: A Practical Introduction to LLMs
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实用介绍LLMs
- en: 原文：[https://towardsdatascience.com/a-practical-introduction-to-llms-65194dda1148](https://towardsdatascience.com/a-practical-introduction-to-llms-65194dda1148)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/a-practical-introduction-to-llms-65194dda1148](https://towardsdatascience.com/a-practical-introduction-to-llms-65194dda1148)
- en: 3 levels of using LLMs in practice
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实践中使用LLM的3个层次
- en: '[](https://shawhin.medium.com/?source=post_page-----65194dda1148--------------------------------)[![Shaw
    Talebi](../Images/1449cc7c08890e2078f9e5d07897e3df.png)](https://shawhin.medium.com/?source=post_page-----65194dda1148--------------------------------)[](https://towardsdatascience.com/?source=post_page-----65194dda1148--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----65194dda1148--------------------------------)
    [Shaw Talebi](https://shawhin.medium.com/?source=post_page-----65194dda1148--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://shawhin.medium.com/?source=post_page-----65194dda1148--------------------------------)[![Shaw
    Talebi](../Images/1449cc7c08890e2078f9e5d07897e3df.png)](https://shawhin.medium.com/?source=post_page-----65194dda1148--------------------------------)[](https://towardsdatascience.com/?source=post_page-----65194dda1148--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----65194dda1148--------------------------------)
    [Shaw Talebi](https://shawhin.medium.com/?source=post_page-----65194dda1148--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----65194dda1148--------------------------------)
    ·7 min read·Jul 13, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page-----65194dda1148--------------------------------)
    ·阅读时间7分钟·2023年7月13日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: This is the first article in a [series](https://shawhin.medium.com/list/large-language-models-llms-8e009ae3054c)
    on using Large Language Models (LLMs) in practice. Here I will give an introduction
    to LLMs and present 3 levels of working with them. Future articles will explore
    practical aspects of LLMs, such as how to use [OpenAI’s public API](https://medium.com/towards-data-science/cracking-open-the-openai-python-api-230e4cae7971),
    the [Hugging Face Transformers](https://medium.com/towards-data-science/cracking-open-the-hugging-face-transformers-library-350aa0ef0161)
    Python library, how to [fine-tune LLMs](https://medium.com/towards-data-science/fine-tuning-large-language-models-llms-23473d763b91),
    and [how to build an LLM from scratch](https://medium.com/towards-data-science/how-to-build-an-llm-from-scratch-8c477768f1f9).
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 这是关于实践中使用大型语言模型（LLMs）的[系列文章](https://shawhin.medium.com/list/large-language-models-llms-8e009ae3054c)的第一篇。在这里，我将介绍LLM并展示3个层次的使用方式。未来的文章将深入探讨LLM的实际应用，例如如何使用[OpenAI的公共API](https://medium.com/towards-data-science/cracking-open-the-openai-python-api-230e4cae7971)、[Hugging
    Face Transformers](https://medium.com/towards-data-science/cracking-open-the-hugging-face-transformers-library-350aa0ef0161)
    Python库、如何[微调LLM](https://medium.com/towards-data-science/fine-tuning-large-language-models-llms-23473d763b91)以及[如何从零开始构建LLM](https://medium.com/towards-data-science/how-to-build-an-llm-from-scratch-8c477768f1f9)。
- en: '![](../Images/a92d8deb44c331cde299747e18da5c7f.png)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a92d8deb44c331cde299747e18da5c7f.png)'
- en: Photo by [Patrick Tomasso](https://unsplash.com/@impatrickt?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由[Patrick Tomasso](https://unsplash.com/@impatrickt?utm_source=medium&utm_medium=referral)提供，来源于[Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: '**What is an LLM?**'
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**什么是LLM？**'
- en: '**LLM** is short for **Large Language Model**, which is a recent innovation
    in AI and machine learning. This powerful new type of AI went viral in Dec 2022
    with the release of ChatGPT.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '**LLM** 是**大型语言模型**的缩写，这是一种最近在AI和机器学习领域的创新。这个强大的新型AI在2022年12月通过ChatGPT的发布而迅速传播。'
- en: For those enlightened enough to live outside the world of AI buzz and tech news
    cycles, **ChatGPT** is a chat interface that ran on an LLM called GPT-3 (now upgraded
    to either GPT-3.5 or GPT-4 at the time of writing this).
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 对于那些足够开明、生活在AI热点和技术新闻周期之外的人来说，**ChatGPT** 是一个基于名为GPT-3（在撰写时已升级为GPT-3.5或GPT-4）的LLM的聊天界面。
- en: If you’ve used ChatGPT, it’s obvious that this is not your traditional chatbot
    from [AOL Instant Messenger](https://en.wikipedia.org/wiki/AIM_(software)) or
    your credit card’s customer care.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你使用过ChatGPT，很明显这不是你传统的来自[AOL即时消息](https://en.wikipedia.org/wiki/AIM_(software))或你的信用卡客户服务的聊天机器人。
- en: This one *feels* different.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 这篇文章*感觉*有所不同。
- en: '**What makes an LLM “large”?**'
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**什么使LLM“巨大”？**'
- en: When I heard the term “Large Language Model,” one of my first questions was,
    *how is this different from a “regular” language model?*
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 当我听到“大型语言模型”这个术语时，我第一个问题是，*这与“常规”语言模型有何不同？*
- en: A language model is more generic than a large language model. Just like all
    squares are rectangles but not all rectangles are squares. **All LLMs are language
    models, but not all language models are LLMs**.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 语言模型比大语言模型更为通用。就像所有的正方形都是矩形，但并非所有的矩形都是正方形。**所有的LLM都是语言模型，但并非所有的语言模型都是LLM**。
- en: '![](../Images/f1538bc098dd64cea39915cbb331d339.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f1538bc098dd64cea39915cbb331d339.png)'
- en: Large Language Models are a special type of Language Model. Image by author.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 大语言模型是一种特殊类型的语言模型。图片由作者提供。
- en: Okay, so LLMs are a special type of language model, **but what makes them special?**
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 好吧，LLM是一种特殊类型的语言模型，**但是什么让它们与众不同呢？**
- en: There are **2 key properties** that distinguish LLMs from other language models.
    One is quantitative, and the other is qualitative.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 有**2个关键特性**将LLM与其他语言模型区分开来。其中一个是定量的，另一个是定性的。
- en: '**Quantitatively**, what distinguishes an LLM is the number of parameters used
    in the model. Current LLMs have on the order of **10–100 billion parameters**
    [1].'
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**定量上**，LLM的区别在于模型中使用的参数数量。当前LLM的参数数量在**100亿到1000亿之间** [1]。'
- en: '**Qualitatively**, something remarkable happens when a language model becomes
    “large.” It exhibits so-called ***emergent properties***e.g. zero-shot learning
    [1]. These are **properties that seem to suddenly appear** when a language model
    reaches a sufficiently large size.'
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**定性上**，当语言模型变得“庞大”时，会发生一些显著的变化。它表现出所谓的***突现特性***例如零 shot 学习 [1]。这些是**在语言模型达到足够大的规模时似乎突然出现的特性**。'
- en: '**Zero-shot Learning**'
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**零 shot 学习**'
- en: The major innovation of GPT-3 (and other LLMs) is that it is capable of **zero-shot
    learning** in a wide variety of contexts [2]. This means ChatGPT can **perform
    a task even if it has not been explicitly trained to do it**.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-3（及其他LLM）的主要创新在于它能够在各种上下文中进行**零 shot 学习** [2]。这意味着ChatGPT可以**执行即使未被明确训练过的任务**。
- en: While this might be no big deal to us highly evolved humans, this zero-shot
    learning ability starkly contrasts the prior machine learning paradigm.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这对我们高度进化的人工来说可能没什么大不了，但这种零-shot 学习能力与之前的机器学习范式形成了鲜明对比。
- en: Previously, a model needed to be **explicitly trained on the task it aimed to
    do** in order to have good performance. This could require anywhere from 1k-1M
    pre-labeled training examples.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 以前，模型需要**明确地在其旨在完成的任务上进行训练**才能有良好的表现。这可能需要从1千到1百万个预标记的训练样本。
- en: For instance, if you wanted a computer to do language translation, sentiment
    analysis, and identify grammatical errors. Each of these tasks would require a
    specialized model trained on a large set of labeled examples. Now, however, **LLMs
    can do all these things without explicit training**.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果你想让计算机进行语言翻译、情感分析和识别语法错误。这些任务中的每一个都需要一个在大量标记样本上进行训练的专门模型。然而，现在**LLM可以在没有明确训练的情况下完成所有这些任务**。
- en: '**How do LLMs work?**'
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**LLM是如何工作的？**'
- en: The core task used to train most state-of-the-art LLMs is **word prediction**.
    In other words, given a sequence of words, **what is the probability distribution
    of the next word**?
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 训练大多数最先进LLM的核心任务是**词预测**。换句话说，给定一系列单词，**下一个词的概率分布是什么**？
- en: 'For example, given the sequence “Listen to your ____,” the most likely next
    words might be: heart, gut, body, parents, grandma, etc. This might look like
    the probability distribution shown below.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，对于序列“听从你的____”，最可能的下一个词可能是：心、直觉、身体、父母、奶奶等。这可能看起来像下图所示的概率分布。
- en: '![](../Images/5012798faee287a80d6de31d5d742bbd.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5012798faee287a80d6de31d5d742bbd.png)'
- en: Toy probability distribution of next word in sequence “Listen to your ___.”
    Image by author.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: “听从你的___”序列中的下一个词的玩具概率分布。图片由作者提供。
- en: Interestingly, this is the same way many (non-large) language models have been
    trained in the past (e.g. GPT-1) [3]. However, for some reason, when language
    models get beyond a certain size (say ~10B parameters), these (emergent) abilities,
    such as zero-shot learning, can start to pop up [1].
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是，过去许多（非大规模）语言模型也是以这种方式进行训练的（例如GPT-1）[3]。然而，由于某种原因，当语言模型超过一定规模（比如~10B参数）时，这些（突现的）能力，如零
    shot 学习，可能会开始出现[1]。
- en: Although there is no clear answer as to *why* this occurs (only speculations
    for now), it is clear that LLMs are a powerful technology with countless potential
    use cases.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管目前对*为什么*会发生这种情况没有明确的答案（目前只是猜测），但显然LLM是一项强大的技术，具有无数的潜在应用。
- en: '**3 Levels of Using LLMs**'
  id: totrans-35
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**使用LLM的3个层次**'
- en: Now we turn to how to use this powerful technology in practice. While there
    are countless potential LLM use cases, here I categorize them into 3 levels **ordered
    by required technical knowledge and computational resources**. We start with the
    most accessible.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来探讨如何在实践中使用这一强大技术。虽然 LLM 的潜在用例无数，但我在这里将其分为 3 个级别，**按所需的技术知识和计算资源排序**。我们从最容易的开始。
- en: '**Level 1: Prompt Engineering**'
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**第 1 级：提示工程**'
- en: The first level of using LLMs in practice is **prompt engineering**, which I
    define as **any use of an LLM out-of-the-box** i.e. not changing any model parameters.
    While many technically-inclined individuals seem to scoff at the idea of prompt
    engineering, this is the most accessible way to use LLMs (both technically and
    economically) in practice.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 实践中使用 LLMs 的第一个级别是**提示工程**，我将其定义为**开箱即用的 LLM 任何用法**，即不更改任何模型参数。虽然许多技术倾向的个人似乎对提示工程嗤之以鼻，但这是在实践中使用
    LLMs（无论是技术上还是经济上）最可及的方式。
- en: '[](/prompt-engineering-how-to-trick-ai-into-solving-your-problems-7ce1ed3b553f?source=post_page-----65194dda1148--------------------------------)
    [## Prompt Engineering — How to trick AI into solving your problems'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '[## 提示工程 — 如何让 AI 解决你的问题](https://example.org/prompt-engineering-how-to-trick-ai-into-solving-your-problems-7ce1ed3b553f?source=post_page-----65194dda1148--------------------------------)'
- en: 7 prompting tricks, Langchain, and Python example code
  id: totrans-40
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7 个提示技巧、Langchain 和 Python 示例代码
- en: towardsdatascience.com](/prompt-engineering-how-to-trick-ai-into-solving-your-problems-7ce1ed3b553f?source=post_page-----65194dda1148--------------------------------)
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/prompt-engineering-how-to-trick-ai-into-solving-your-problems-7ce1ed3b553f?source=post_page-----65194dda1148--------------------------------)
- en: 'There are 2 main ways to do prompt engineering: the **Easy Way** and the **Less
    Easy Way**.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 提示工程有两种主要方式：**简单方式**和**不那么简单的方式**。
- en: '**The Easy Way: ChatGPT (or another convenient LLM UI) —** The key benefit
    of this method is convenience. Tools like ChatGPT provide an intuitive, no-cost,
    and no-code way to use an LLM (it doesn’t get much easier than that).'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '**简单方式：ChatGPT（或其他方便的 LLM 用户界面） —** 这种方法的关键好处是便利。像 ChatGPT 这样的工具提供了一种直观、无成本且无需编码的使用
    LLM 的方式（没有比这更简单的了）。'
- en: However, convenience often comes at a cost. In this case, there are **2 key
    drawbacks** to this approach. The **first** is a lack of functionality. For example,
    ChatGPT does not readily enable users to customize model input parameters (e.g.
    temperature or max response length), which are values that modulate LLM outputs.
    **Second**, interactions with the ChatGPT UI cannot be readily automated and thus
    applied to large-scale use cases.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，便利通常伴随代价。在这种情况下，这种方法有**两个主要缺点**。**第一个**是功能性不足。例如，ChatGPT 并不容易让用户自定义模型输入参数（如温度或最大响应长度），这些参数会调节
    LLM 输出。**第二**，与 ChatGPT 用户界面的交互无法轻松自动化，因此不能应用于大规模用例。
- en: While these drawbacks may be dealbreakers for some use cases, both can be ameliorated
    if we take prompt engineering one step further.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这些缺点可能对某些用例来说是致命的，但如果我们将提示工程更进一步，这些缺点都可以得到改善。
- en: '**The Less Easy Way: Interact with LLM directly —** We can overcome some of
    the drawbacks of ChatGPT by interacting directly with an LLM via programmatic
    interfaces. This could be via public APIs (e.g. OpenAI’s API) or running an LLM
    locally (using libraries like [Transformers](https://medium.com/towards-data-science/cracking-open-the-hugging-face-transformers-library-350aa0ef0161)).'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '**不那么简单的方式：直接与 LLM 互动 —** 我们可以通过程序化接口直接与 LLM 互动，从而克服 ChatGPT 的一些缺点。这可以通过公共
    API（例如 OpenAI 的 API）或本地运行 LLM（使用如 [Transformers](https://medium.com/towards-data-science/cracking-open-the-hugging-face-transformers-library-350aa0ef0161)
    的库）来实现。'
- en: While this way of doing prompt engineering is less convenient (since it requires
    programming knowledge and potential API costs), it provides a customizable, flexible,
    and scalable way to use LLMs in practice. Future articles in this series will
    discuss [paid](/cracking-open-the-openai-python-api-230e4cae7971) and [cost-free](/cracking-open-the-hugging-face-transformers-library-350aa0ef0161)
    ways to do this type of prompt engineering.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这种提示工程方式不那么方便（因为需要编程知识和可能的 API 成本），但它提供了一种可定制、灵活和可扩展的方式来实际使用 LLMs。本系列未来的文章将讨论
    [付费的](/cracking-open-the-openai-python-api-230e4cae7971) 和 [无成本的](/cracking-open-the-hugging-face-transformers-library-350aa0ef0161)
    这种类型的提示工程方式。
- en: Although prompt engineering (as defined here) can handle most potential LLM
    applications, relying on a generic model, out-of-the-box may result in sub-optimal
    performance for specific use cases. For these situations, we can go to the next
    level of using LLMs.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管（在这里定义的）提示工程可以处理大多数潜在的 LLM 应用，但依赖于通用模型和现成的解决方案可能会导致特定用例的性能不佳。在这些情况下，我们可以进入使用
    LLM 的下一个级别。
- en: '**Level 2: Model Fine-tuning**'
  id: totrans-49
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**第二级：模型微调**'
- en: The second level of using an LLM is **model fine-tuning**, which I’ll define
    as taking an existing LLM and tweaking it for a particular use case by **training
    at least 1 (internal) model parameter** i.e. weights and biases. For the aficionados
    out there, this is an example of *transfer learning* i.e. using some part of an
    existing model to develop another model.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 LLM 的第二级是**模型微调**，我将其定义为对现有 LLM 进行调整，以通过**训练至少 1 个（内部）模型参数**（即权重和偏差）来适应特定用例。对于那些喜欢的人来说，这是*迁移学习*的一个例子，即使用现有模型的一部分来开发另一个模型。
- en: 'Fine-tuning typically consists of 2 steps. **Step 1**: Obtain a pre-trained
    LLM. **Step 2**: Update model parameters for a specific task given (typically
    1000s of) high-quality labeled examples.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 微调通常包括 2 个步骤。**步骤 1**：获取一个预训练的 LLM。**步骤 2**：根据特定任务更新模型参数（通常是成千上万的）高质量标记示例。
- en: The model parameters define the LLM’s internal representation of the input text.
    Thus, by tweaking these parameters for a particular task, the internal representations
    become optimized for the fine-tuning task (or at least that’s the idea).
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 模型参数定义了 LLM 对输入文本的内部表示。因此，通过调整这些参数以适应特定任务，内部表示会针对微调任务进行优化（或者至少这是其理念）。
- en: This is a powerful approach to model development because a relatively **small
    number of examples** and computational resources **can produce exceptional model
    performance**.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一种强大的模型开发方法，因为相对**少量的示例**和计算资源**可以产生卓越的模型性能**。
- en: The downside, however, is it requires significantly more technical expertise
    and computational resources than prompt engineering. In a [future article](https://medium.com/towards-data-science/fine-tuning-large-language-models-llms-23473d763b91),
    I will attempt to curb this downside by reviewing fine-tuning techniques and sharing
    example Python code.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，缺点是它需要比提示工程显著更多的技术专长和计算资源。在[未来的文章](https://medium.com/towards-data-science/fine-tuning-large-language-models-llms-23473d763b91)中，我将尝试通过回顾微调技术并分享示例
    Python 代码来减少这个缺点。
- en: '[](/fine-tuning-large-language-models-llms-23473d763b91?source=post_page-----65194dda1148--------------------------------)
    [## Fine-Tuning Large Language Models (LLMs)'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/fine-tuning-large-language-models-llms-23473d763b91?source=post_page-----65194dda1148--------------------------------)
    [## 微调大型语言模型（LLMs）'
- en: A conceptual overview with example Python code
  id: totrans-56
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 一个概念概述，并附有示例 Python 代码
- en: towardsdatascience.com](/fine-tuning-large-language-models-llms-23473d763b91?source=post_page-----65194dda1148--------------------------------)
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/fine-tuning-large-language-models-llms-23473d763b91?source=post_page-----65194dda1148--------------------------------)
- en: While prompt engineering and model fine-tuning can likely handle 99% of LLM
    applications, there are cases where one must go even further.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然提示工程和模型微调可能能够处理 99% 的 LLM 应用，但在某些情况下，必须进一步深入。
- en: '**Level 3: Build your own LLM**'
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**第三级：构建你自己的 LLM**'
- en: The third and final way to use an LLM in practice is to [**build your own**](https://medium.com/towards-data-science/how-to-build-an-llm-from-scratch-8c477768f1f9).
    In terms of model parameters, this is where you **come up with all the model parameters**
    from scratch.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 LLM 的第三种也是最后一种实际方法是[**构建你自己的**](https://medium.com/towards-data-science/how-to-build-an-llm-from-scratch-8c477768f1f9)。在模型参数方面，这意味着你**从头开始制定所有模型参数**。
- en: An LLM is primarily a product of its training data. Thus, for some applications,
    it may be necessary to curate custom, high-quality text corpora for model training—for
    example, a medical research corpus for the development of a clinical application.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: LLM 主要是其训练数据的产物。因此，对于某些应用，可能需要为模型训练策划定制的高质量文本语料库——例如，为开发临床应用而制定的医学研究语料库。
- en: The biggest upside to this approach is you can **fully customize the LLM for
    your particular use case**. This is the ultimate flexibility. However, as is often
    the case, flexibility comes at the cost of convenience.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法最大的好处是你可以**完全定制 LLM 以适应你的特定用例**。这是**最终的灵活性**。然而，正如常见的情况一样，灵活性往往以便利性为代价。
- en: Since the **key to LLM performance is scale**, building an LLM from scratch
    requires tremendous computational resources and technical expertise. In other
    words, this isn’t going to be a solo weekend project but a full team working for
    months, if not years, with a 7–8F budget.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 由于**LLM性能的关键在于规模**，从零构建LLM需要巨大的计算资源和技术专长。换句话说，这不仅仅是一个周末项目，而是一个团队要工作数月甚至数年的工程，预算达到7–8F。
- en: Nevertheless, in a [future article](https://medium.com/towards-data-science/how-to-build-an-llm-from-scratch-8c477768f1f9)
    in this series, we will explore popular techniques for developing LLMs from scratch.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在这系列的[未来文章](https://medium.com/towards-data-science/how-to-build-an-llm-from-scratch-8c477768f1f9)中，我们将深入探讨从零开发LLM的流行技术。
- en: '[](/how-to-build-an-llm-from-scratch-8c477768f1f9?source=post_page-----65194dda1148--------------------------------)
    [## How to Build an LLM from Scratch'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/how-to-build-an-llm-from-scratch-8c477768f1f9?source=post_page-----65194dda1148--------------------------------)
    [## 如何从零构建LLM'
- en: Data Curation, Transformers, Training at Scale, and Model Evaluation
  id: totrans-66
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据整理、变换器、规模化训练和模型评估
- en: towardsdatascience.com](/how-to-build-an-llm-from-scratch-8c477768f1f9?source=post_page-----65194dda1148--------------------------------)
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/how-to-build-an-llm-from-scratch-8c477768f1f9?source=post_page-----65194dda1148--------------------------------)
- en: Conclusion
  id: totrans-68
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: While there is more than enough hype about LLMs, they are a powerful innovation
    in AI. Here, I provided a primer on what LLMs are and framed how they can be used
    in practice. The [next article](https://medium.com/towards-data-science/cracking-open-the-openai-python-api-230e4cae7971)
    in this series will give a beginner’s guide to OpenAI’s Python API to help jumpstart
    your next LLM use case.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然关于LLM的炒作过多，但它们是AI中的一种强大创新。在这里，我提供了LLM是什么的入门知识，并框定了它们如何在实践中使用。[系列中的下一篇文章](https://medium.com/towards-data-science/cracking-open-the-openai-python-api-230e4cae7971)将为OpenAI的Python
    API提供初学者指南，帮助启动你的下一个LLM用例。
- en: '👉 **More on LLMs**: [OpenAI API](https://medium.com/towards-data-science/cracking-open-the-openai-python-api-230e4cae7971)
    | [Hugging Face Transformers](https://medium.com/towards-data-science/cracking-open-the-hugging-face-transformers-library-350aa0ef0161)
    | [Prompt Engineering](https://medium.com/towards-data-science/prompt-engineering-how-to-trick-ai-into-solving-your-problems-7ce1ed3b553f)
    | [Fine-tuning](https://medium.com/towards-data-science/fine-tuning-large-language-models-llms-23473d763b91)
    | [Build an LLM](/how-to-build-an-llm-from-scratch-8c477768f1f9) | [QLoRA](/qlora-how-to-fine-tune-an-llm-on-a-single-gpu-4e44d6b5be32)
    | [RAG](https://medium.com/towards-data-science/how-to-improve-llms-with-rag-abdc132f76ac)
    | [Text Embeddings](/text-embeddings-classification-and-semantic-search-8291746220be)'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '👉 **更多LLM信息**: [OpenAI API](https://medium.com/towards-data-science/cracking-open-the-openai-python-api-230e4cae7971)
    | [Hugging Face Transformers](https://medium.com/towards-data-science/cracking-open-the-hugging-face-transformers-library-350aa0ef0161)
    | [Prompt Engineering](https://medium.com/towards-data-science/prompt-engineering-how-to-trick-ai-into-solving-your-problems-7ce1ed3b553f)
    | [Fine-tuning](https://medium.com/towards-data-science/fine-tuning-large-language-models-llms-23473d763b91)
    | [构建LLM](/how-to-build-an-llm-from-scratch-8c477768f1f9) | [QLoRA](/qlora-how-to-fine-tune-an-llm-on-a-single-gpu-4e44d6b5be32)
    | [RAG](https://medium.com/towards-data-science/how-to-improve-llms-with-rag-abdc132f76ac)
    | [文本嵌入](/text-embeddings-classification-and-semantic-search-8291746220be)'
- en: '![Shaw Talebi](../Images/02eefb458c6eeff7cd29d40c212e3b22.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![Shaw Talebi](../Images/02eefb458c6eeff7cd29d40c212e3b22.png)'
- en: '[Shaw Talebi](https://shawhin.medium.com/?source=post_page-----65194dda1148--------------------------------)'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '[Shaw Talebi](https://shawhin.medium.com/?source=post_page-----65194dda1148--------------------------------)'
- en: Large Language Models (LLMs)
  id: totrans-73
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）
- en: '[View list](https://shawhin.medium.com/list/large-language-models-llms-8e009ae3054c?source=post_page-----65194dda1148--------------------------------)13
    stories![](../Images/82e865594c68f5307e75665842d197bb.png)![](../Images/b9436354721f807e0390b5e301be2119.png)![](../Images/59c8db581de77a908457dec8981f3c37.png)'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '[查看列表](https://shawhin.medium.com/list/large-language-models-llms-8e009ae3054c?source=post_page-----65194dda1148--------------------------------)13篇故事![](../Images/82e865594c68f5307e75665842d197bb.png)![](../Images/b9436354721f807e0390b5e301be2119.png)![](../Images/59c8db581de77a908457dec8981f3c37.png)'
- en: Resources
  id: totrans-75
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 资源
- en: '**Connect**: [My website](https://shawhintalebi.com/) | [Book a call](https://calendly.com/shawhintalebi)'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '**联系**: [我的网站](https://shawhintalebi.com/) | [预约电话](https://calendly.com/shawhintalebi)'
- en: '**Socials**: [YouTube 🎥](https://www.youtube.com/channel/UCa9gErQ9AE5jT2DZLjXBIdA)
    | [LinkedIn](https://www.linkedin.com/in/shawhintalebi/) | [Twitter](https://twitter.com/ShawhinT)'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '**社交媒体**: [YouTube 🎥](https://www.youtube.com/channel/UCa9gErQ9AE5jT2DZLjXBIdA)
    | [LinkedIn](https://www.linkedin.com/in/shawhintalebi/) | [Twitter](https://twitter.com/ShawhinT)'
- en: '**Support**: [Buy me a coffee](https://www.buymeacoffee.com/shawhint) ☕️'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '**支持**: [请我喝咖啡](https://www.buymeacoffee.com/shawhint) ☕️'
- en: '[](https://shawhin.medium.com/subscribe?source=post_page-----65194dda1148--------------------------------)
    [## Get FREE access to every new story I write'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://shawhin.medium.com/subscribe?source=post_page-----65194dda1148--------------------------------)
    [## 免费获取我撰写的每一篇新故事'
- en: Get FREE access to every new story I write P.S. I do not share your email with
    anyone By signing up, you will create a…
  id: totrans-80
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 免费获取我撰写的每一篇新故事 P.S. 我不会与任何人分享您的电子邮件 注册后，您将创建一个...
- en: shawhin.medium.com](https://shawhin.medium.com/subscribe?source=post_page-----65194dda1148--------------------------------)
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: shawhin.medium.com](https://shawhin.medium.com/subscribe?source=post_page-----65194dda1148--------------------------------)
- en: '[1] Survey of Large Language Models. [arXiv:2303.18223](https://arxiv.org/abs/2303.18223)
    **[cs.CL]**'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] 大型语言模型调查。 [arXiv:2303.18223](https://arxiv.org/abs/2303.18223) **[cs.CL]**'
- en: '[2] GPT-3 Paper. [arXiv:2005.14165](https://arxiv.org/abs/2005.14165) **[cs.CL]**'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] GPT-3 论文。 [arXiv:2005.14165](https://arxiv.org/abs/2005.14165) **[cs.CL]**'
- en: '[3] Radford, A., & Narasimhan, K. (2018). Improving Language Understanding
    by Generative Pre-Training. ([GPT-1 Paper](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf))'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] Radford, A., & Narasimhan, K. (2018). 通过生成预训练提高语言理解。 ([GPT-1 论文](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf))'
