- en: Implementing a Transformer Encoder from Scratch with JAX and Haiku ğŸ¤–
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ä½¿ç”¨ JAX å’Œ Haiku ä»å¤´å®ç° Transformer ç¼–ç å™¨ ğŸ¤–
- en: åŸæ–‡ï¼š[https://towardsdatascience.com/implementing-a-transformer-encoder-from-scratch-with-jax-and-haiku-791d31b4f0dd?source=collection_archive---------7-----------------------#2023-11-07](https://towardsdatascience.com/implementing-a-transformer-encoder-from-scratch-with-jax-and-haiku-791d31b4f0dd?source=collection_archive---------7-----------------------#2023-11-07)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡ï¼š[https://towardsdatascience.com/implementing-a-transformer-encoder-from-scratch-with-jax-and-haiku-791d31b4f0dd?source=collection_archive---------7-----------------------#2023-11-07](https://towardsdatascience.com/implementing-a-transformer-encoder-from-scratch-with-jax-and-haiku-791d31b4f0dd?source=collection_archive---------7-----------------------#2023-11-07)
- en: Understanding the fundamental building blocks of Transformers.
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ç†è§£ Transformers çš„åŸºæœ¬æ„å»ºæ¨¡å—ã€‚
- en: '[](https://medium.com/@ryanpegoud?source=post_page-----791d31b4f0dd--------------------------------)[![Ryan
    PÃ©goud](../Images/9314b76c2be56bda8b73b4badf9e3e4d.png)](https://medium.com/@ryanpegoud?source=post_page-----791d31b4f0dd--------------------------------)[](https://towardsdatascience.com/?source=post_page-----791d31b4f0dd--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----791d31b4f0dd--------------------------------)
    [Ryan PÃ©goud](https://medium.com/@ryanpegoud?source=post_page-----791d31b4f0dd--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@ryanpegoud?source=post_page-----791d31b4f0dd--------------------------------)[![Ryan
    PÃ©goud](../Images/9314b76c2be56bda8b73b4badf9e3e4d.png)](https://medium.com/@ryanpegoud?source=post_page-----791d31b4f0dd--------------------------------)[](https://towardsdatascience.com/?source=post_page-----791d31b4f0dd--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----791d31b4f0dd--------------------------------)
    [Ryan PÃ©goud](https://medium.com/@ryanpegoud?source=post_page-----791d31b4f0dd--------------------------------)'
- en: Â·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: Â·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F27fba63b402e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fimplementing-a-transformer-encoder-from-scratch-with-jax-and-haiku-791d31b4f0dd&user=Ryan+P%C3%A9goud&userId=27fba63b402e&source=post_page-27fba63b402e----791d31b4f0dd---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----791d31b4f0dd--------------------------------)
    Â·12 min readÂ·Nov 7, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F791d31b4f0dd&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fimplementing-a-transformer-encoder-from-scratch-with-jax-and-haiku-791d31b4f0dd&user=Ryan+P%C3%A9goud&userId=27fba63b402e&source=-----791d31b4f0dd---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[å…³æ³¨](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F27fba63b402e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fimplementing-a-transformer-encoder-from-scratch-with-jax-and-haiku-791d31b4f0dd&user=Ryan+P%C3%A9goud&userId=27fba63b402e&source=post_page-27fba63b402e----791d31b4f0dd---------------------post_header-----------)
    å‘è¡¨åœ¨ [Towards Data Science](https://towardsdatascience.com/?source=post_page-----791d31b4f0dd--------------------------------)
    Â·12åˆ†é’Ÿé˜…è¯»Â·2023å¹´11æœˆ7æ—¥[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F791d31b4f0dd&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fimplementing-a-transformer-encoder-from-scratch-with-jax-and-haiku-791d31b4f0dd&user=Ryan+P%C3%A9goud&userId=27fba63b402e&source=-----791d31b4f0dd---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F791d31b4f0dd&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fimplementing-a-transformer-encoder-from-scratch-with-jax-and-haiku-791d31b4f0dd&source=-----791d31b4f0dd---------------------bookmark_footer-----------)![](../Images/bdeff665e8c9cb3e1a1bdb370c5de52f.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F791d31b4f0dd&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fimplementing-a-transformer-encoder-from-scratch-with-jax-and-haiku-791d31b4f0dd&source=-----791d31b4f0dd---------------------bookmark_footer-----------)![](../Images/bdeff665e8c9cb3e1a1bdb370c5de52f.png)'
- en: Transformers, in the style of Edward Hopper (generated by Dall.E 3)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: Transformersï¼Œä»¥çˆ±å¾·åÂ·éœç€ï¼ˆç”± Dall.E 3 ç”Ÿæˆï¼‰çš„é£æ ¼
- en: Introduced in 2017 in the seminal paper â€œ[***Attention is all you need***](https://arxiv.org/pdf/1706.03762.pdf)***â€***[0],
    the Transformer architecture is arguably one of the most impactful breakthroughs
    in recent Deep Learning history, enabling the rise of large language models and
    even finding use in fields such as computer vision.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨2017å¹´çš„å¼€åˆ›æ€§è®ºæ–‡â€œ[***æ³¨æ„åŠ›æœºåˆ¶å°±æ˜¯ä½ éœ€è¦çš„***](https://arxiv.org/pdf/1706.03762.pdf)***â€***[0]ä¸­ä»‹ç»çš„
    Transformer æ¶æ„å¯ä»¥è¯´æ˜¯è¿‘æœŸæ·±åº¦å­¦ä¹ å†å²ä¸Šæœ€å…·å½±å“åŠ›çš„çªç ´ä¹‹ä¸€ï¼Œå®ƒä½¿å¤§å‹è¯­è¨€æ¨¡å‹çš„å…´èµ·æˆä¸ºå¯èƒ½ï¼Œå¹¶ä¸”åœ¨è®¡ç®—æœºè§†è§‰ç­‰é¢†åŸŸä¹Ÿæ‰¾åˆ°äº†åº”ç”¨ã€‚
- en: Succeeding to former state-of-the-art architectures relying on **recurrence**
    such as Long Short-Term Memory (**LSTM**) networks or Gated Recurrent Units (**GRU**),
    **Transformers** introduce the concept of **self-attention**, coupled with an
    **encoder/decoder** architecture.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: ç»§æ‰¿äº†ä¾èµ–äº**é€’å½’**çš„å‰æ²¿æ¶æ„ï¼Œå¦‚é•¿çŸ­æœŸè®°å¿†ï¼ˆ**LSTM**ï¼‰ç½‘ç»œæˆ–é—¨æ§å¾ªç¯å•å…ƒï¼ˆ**GRU**ï¼‰ï¼Œ**Transformer**å¼•å…¥äº†**è‡ªæ³¨æ„åŠ›**çš„æ¦‚å¿µï¼Œå¹¶ç»“åˆäº†**ç¼–ç å™¨/è§£ç å™¨**æ¶æ„ã€‚
- en: In this article, weâ€™ll implement the first half of a Transformer, the **Encoder**,
    from scratch and step by step. Weâ€™ll use **JAX** as our main framework along with
    **Haiku,** one of DeepMindâ€™s deep learning libraries.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å°†ä»é›¶å¼€å§‹ä¸€æ­¥ä¸€æ­¥å®ç°Transformerçš„å‰åŠéƒ¨åˆ†ï¼Œå³**ç¼–ç å™¨**ã€‚æˆ‘ä»¬å°†ä½¿ç”¨**JAX**ä½œä¸ºä¸»è¦æ¡†æ¶ï¼Œå¹¶ç»“åˆ**Haiku**ï¼Œè¿™æ˜¯DeepMindçš„æ·±åº¦å­¦ä¹ åº“ä¹‹ä¸€ã€‚
- en: 'In case you are unfamiliar with JAX or need a fresh reminder about its amazing
    functionalities, Iâ€™ve already covered the topic in the context of Reinforcement
    Learning in my **previous article**:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœä½ å¯¹JAXä¸ç†Ÿæ‚‰æˆ–éœ€è¦å¯¹å…¶æƒŠäººåŠŸèƒ½æœ‰ä¸€ä¸ªæ–°çš„æé†’ï¼Œæˆ‘åœ¨æˆ‘çš„**ä¸Šä¸€ç¯‡æ–‡ç« **ä¸­å·²ç»æ¶µç›–äº†è¿™ä¸ªè¯é¢˜ï¼š
- en: '[](/vectorize-and-parallelize-rl-environments-with-jax-q-learning-at-the-speed-of-light-49d07373adf5?source=post_page-----791d31b4f0dd--------------------------------)
    [## Vectorize and Parallelize RL Environments with JAX: Q-learning at the Speed
    of Lightâš¡'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '[## ä½¿ç”¨JAXå‘é‡åŒ–å’Œå¹¶è¡ŒåŒ–å¼ºåŒ–å­¦ä¹ ç¯å¢ƒï¼šå…‰é€ŸQ-learningâš¡](/vectorize-and-parallelize-rl-environments-with-jax-q-learning-at-the-speed-of-light-49d07373adf5?source=post_page-----791d31b4f0dd--------------------------------)'
- en: Learn to vectorize a GridWorld environment and train 30 Q-learning agents in
    parallel on a CPU, at 1.8 million step perâ€¦
  id: totrans-14
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: å­¦ä¹ å¦‚ä½•å‘é‡åŒ–ä¸€ä¸ªGridWorldç¯å¢ƒï¼Œå¹¶åœ¨CPUä¸Šå¹¶è¡Œè®­ç»ƒ30ä¸ªQ-learningä»£ç†ï¼Œæ­¥æ•°è¾¾åˆ°180ä¸‡â€¦â€¦
- en: towardsdatascience.com](/vectorize-and-parallelize-rl-environments-with-jax-q-learning-at-the-speed-of-light-49d07373adf5?source=post_page-----791d31b4f0dd--------------------------------)
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '[towardsdatascience.com](/vectorize-and-parallelize-rl-environments-with-jax-q-learning-at-the-speed-of-light-49d07373adf5?source=post_page-----791d31b4f0dd--------------------------------)'
- en: 'Weâ€™ll go over each of the blocks that make up the encoder and learn to implement
    them efficiently. In particular, the outline of this article contains:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†é€ä¸€è®²è§£æ„æˆç¼–ç å™¨çš„æ¯ä¸ªæ¨¡å—ï¼Œå¹¶å­¦ä¹ å¦‚ä½•é«˜æ•ˆåœ°å®ç°å®ƒä»¬ã€‚ç‰¹åˆ«æ˜¯ï¼Œæœ¬æ–‡çš„çº²è¦åŒ…æ‹¬ï¼š
- en: The **Embedding Layer** and **Positional Encodings**
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**åµŒå…¥å±‚** å’Œ **ä½ç½®ç¼–ç **'
- en: '**Multi-Head Attention**'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**å¤šå¤´æ³¨æ„åŠ›**'
- en: '**Residual Connections** and **Layer Normalization**'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**æ®‹å·®è¿æ¥** å’Œ **å±‚å½’ä¸€åŒ–**'
- en: '**Position-wise Feed-Forward Networks**'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ä½ç½®-wiseå‰é¦ˆç½‘ç»œ**'
- en: '*Disclaimer: this article is not intended to be a complete introduction to
    these notions as weâ€™ll focus on implementation first. If needed, please refer
    to the resources at the end of this post.*'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '*å…è´£å£°æ˜ï¼šæœ¬æ–‡å¹¶éè¿™äº›æ¦‚å¿µçš„å®Œæ•´ä»‹ç»ï¼Œæˆ‘ä»¬å°†é¦–å…ˆå…³æ³¨å®ç°éƒ¨åˆ†ã€‚å¦‚æœ‰éœ€è¦ï¼Œè¯·å‚é˜…æœ¬æ–‡æœ«å°¾çš„èµ„æºã€‚*'
- en: '***As always, the fully commented code for this article as well as illustrated
    notebooks are available on*** [***GitHub***](https://github.com/RPegoud/jab)***,
    feel free to star the repository if you enjoyed the article!***'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '***ä¸€å¦‚æ—¢å¾€ï¼Œæœ¬æ–‡çš„å®Œæ•´æ³¨é‡Šä»£ç ä»¥åŠæ’å›¾ç¬”è®°æœ¬å¯åœ¨*** [***GitHub***](https://github.com/RPegoud/jab)***ä¸Šè·å¾—ï¼Œå¦‚æœä½ å–œæ¬¢è¿™ç¯‡æ–‡ç« ï¼Œæ¬¢è¿ç»™ä»“åº“åŠ æ˜Ÿï¼***'
- en: '[](https://github.com/RPegoud/jab?source=post_page-----791d31b4f0dd--------------------------------)
    [## GitHub â€” RPegoud/jab: A collection of foundational Deep Learning models implemented
    in JAX'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '[## GitHub â€” RPegoud/jab: ä¸€ç³»åˆ—åœ¨JAXä¸­å®ç°çš„åŸºç¡€æ·±åº¦å­¦ä¹ æ¨¡å‹](https://github.com/RPegoud/jab?source=post_page-----791d31b4f0dd--------------------------------)'
- en: 'A collection of foundational Deep Learning models implemented in JAX â€” GitHub
    â€” RPegoud/jab: A collection ofâ€¦'
  id: totrans-24
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 'åœ¨JAXä¸­å®ç°çš„ä¸€ç³»åˆ—åŸºç¡€æ·±åº¦å­¦ä¹ æ¨¡å‹ â€” GitHub â€” RPegoud/jab: ä¸€ç³»åˆ—â€¦'
- en: github.com](https://github.com/RPegoud/jab?source=post_page-----791d31b4f0dd--------------------------------)
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '[github.com](https://github.com/RPegoud/jab?source=post_page-----791d31b4f0dd--------------------------------)'
- en: Main parameters
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä¸»è¦å‚æ•°
- en: 'Before we get started, we need to define a few parameters that will play a
    crucial role in the encoder block:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æˆ‘ä»¬å¼€å§‹ä¹‹å‰ï¼Œæˆ‘ä»¬éœ€è¦å®šä¹‰å‡ ä¸ªåœ¨ç¼–ç å™¨æ¨¡å—ä¸­å‘æŒ¥é‡è¦ä½œç”¨çš„å‚æ•°ï¼š
- en: '**Sequence Length** (`seq_len`): The number of tokens or words in a sequence.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**åºåˆ—é•¿åº¦** (`seq_len`): åºåˆ—ä¸­çš„æ ‡è®°æˆ–è¯çš„æ•°é‡ã€‚'
- en: '**Embedding Dimension** (`embed_dim`): The dimension of the embeddings, in
    other words, the number of numerical values used to describe a single token or
    word.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**åµŒå…¥ç»´åº¦** (`embed_dim`): åµŒå…¥çš„ç»´åº¦ï¼Œæ¢å¥è¯è¯´ï¼Œå°±æ˜¯ç”¨æ¥æè¿°å•ä¸ªæ ‡è®°æˆ–è¯çš„æ•°å€¼æ•°é‡ã€‚'
- en: '**Batch Size (**`batch_size`**):** The size of a batch of inputs, i.e. the
    number of sequences processed at the same time.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**æ‰¹é‡å¤§å° (**`batch_size`**):** è¾“å…¥æ‰¹é‡çš„å¤§å°ï¼Œå³åŒæ—¶å¤„ç†çš„åºåˆ—æ•°é‡ã€‚'
- en: The input sequences to our encoder model will typically be of shape **(**`batch_size`**,**
    `seq_len`**)**. In this article, weâ€™ll use `batch_size=32` and `seq_len=10`, which
    means that our encoder will simultaneously process 32 sequences of 10 words.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬çš„ç¼–ç å™¨æ¨¡å‹çš„è¾“å…¥åºåˆ—é€šå¸¸æ˜¯**ï¼ˆ**`batch_size`**ï¼Œ** `seq_len`**ï¼‰**çš„å½¢çŠ¶ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨`batch_size=32`å’Œ`seq_len=10`ï¼Œè¿™æ„å‘³ç€æˆ‘ä»¬çš„ç¼–ç å™¨å°†åŒæ—¶å¤„ç†32ä¸ª10è¯çš„åºåˆ—ã€‚
- en: 'Paying attention to the shape of our data at each step of the processing will
    enable us to better visualize and understand how the data flows in the encoder
    block. Hereâ€™s a high-level overview of our encoder, weâ€™ll start from the bottom
    with the **embedding layer** and **positional encodings**:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: å…³æ³¨æ¯ä¸€æ­¥å¤„ç†ä¸­çš„æ•°æ®å½¢çŠ¶å°†ä½¿æˆ‘ä»¬æ›´å¥½åœ°å¯è§†åŒ–å’Œç†è§£æ•°æ®åœ¨ç¼–ç å™¨å—ä¸­çš„æµåŠ¨ã€‚ä»¥ä¸‹æ˜¯æˆ‘ä»¬ç¼–ç å™¨çš„é«˜çº§æ¦‚è¿°ï¼Œæˆ‘ä»¬å°†ä»åº•éƒ¨å¼€å§‹ï¼Œä»‹ç»**åµŒå…¥å±‚**å’Œ**ä½ç½®ç¼–ç **ï¼š
- en: '![](../Images/818dc653dc223239b2d0b4bd7739222f.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/818dc653dc223239b2d0b4bd7739222f.png)'
- en: Representation of the **Transformer Encoder block** (made by the author)
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '**Transformerç¼–ç å™¨å—**çš„è¡¨ç¤ºï¼ˆç”±ä½œè€…åˆ¶ä½œï¼‰'
- en: Embedding Layer and Positional Encodings
  id: totrans-35
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: åµŒå…¥å±‚å’Œä½ç½®ç¼–ç 
- en: As mentioned previously, our model takes batched sequences of tokens as inputs.
    Generating those tokens could be as simple as collecting a set of unique words
    in our dataset, and assigning an index to each of them. Then we would sample **32**
    **sequences** of **10 words** and replace each word with its index in the vocabulary.
    This procedure would provide us with an array of shape **(**`batch_size`**,**
    `seq_len`**)**, as expected.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚å‰æ‰€è¿°ï¼Œæˆ‘ä»¬çš„æ¨¡å‹æ¥å—æ‰¹å¤„ç†çš„ä»¤ç‰Œåºåˆ—ä½œä¸ºè¾“å…¥ã€‚ç”Ÿæˆè¿™äº›ä»¤ç‰Œå¯èƒ½åƒæ”¶é›†æ•°æ®é›†ä¸­ä¸€ç»„å”¯ä¸€è¯æ±‡å¹¶ä¸ºæ¯ä¸ªè¯æ±‡åˆ†é…ä¸€ä¸ªç´¢å¼•ä¸€æ ·ç®€å•ã€‚ç„¶åï¼Œæˆ‘ä»¬å°†é‡‡æ ·**32**ä¸ª**10è¯**çš„**åºåˆ—**ï¼Œå¹¶ç”¨è¯æ±‡ä¸­çš„ç´¢å¼•æ›¿æ¢æ¯ä¸ªè¯ã€‚è¿™ä¸€è¿‡ç¨‹å°†ä¸ºæˆ‘ä»¬æä¾›ä¸€ä¸ªå½¢çŠ¶ä¸º**ï¼ˆ**`batch_size`**ï¼Œ**
    `seq_len`**ï¼‰**çš„æ•°ç»„ï¼Œæ­£å¦‚é¢„æœŸçš„é‚£æ ·ã€‚
- en: We are now ready to get started with our Encoder. The first step is to create
    â€œ***positional embeddings***â€ for our sequences. Positional embeddings are the
    **sum** of **word embeddings** and **positional encodings**.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ç°åœ¨å‡†å¤‡å¼€å§‹ä½¿ç”¨ç¼–ç å™¨ã€‚ç¬¬ä¸€æ­¥æ˜¯ä¸ºæˆ‘ä»¬çš„åºåˆ—åˆ›å»ºâ€œ***ä½ç½®åµŒå…¥***â€ã€‚ä½ç½®åµŒå…¥æ˜¯**è¯åµŒå…¥**å’Œ**ä½ç½®ç¼–ç **çš„**å’Œ**ã€‚
- en: Word Embeddings
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è¯åµŒå…¥
- en: Word embeddings allow us to encode the **meaning** and **semantic relations**
    **between words** in our vocabulary. In this article, the embedding dimension
    is fixed to **64**. This means that each word is represented by a **64-dimensional
    vector** so that words with similar meanings have similar coordinates. Moreover,
    we can manipulate these vectors to **extract relations between words**, as depicted
    below.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: è¯åµŒå…¥ä½¿æˆ‘ä»¬èƒ½å¤Ÿç¼–ç **è¯æ±‡ä¸­**çš„**æ„ä¹‰**å’Œ**è¯­ä¹‰å…³ç³»**ã€‚åœ¨æœ¬æ–‡ä¸­ï¼ŒåµŒå…¥ç»´åº¦å›ºå®šä¸º**64**ã€‚è¿™æ„å‘³ç€æ¯ä¸ªè¯ç”±ä¸€ä¸ª**64ç»´å‘é‡**è¡¨ç¤ºï¼Œä»è€Œå…·æœ‰ç›¸ä¼¼æ„ä¹‰çš„è¯å…·æœ‰ç›¸ä¼¼çš„åæ ‡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¯ä»¥æ“ä½œè¿™äº›å‘é‡æ¥**æå–è¯ä¹‹é—´çš„å…³ç³»**ï¼Œå¦‚ä¸‹æ‰€ç¤ºã€‚
- en: '![](../Images/55b06988518a9554add4ad341f107320.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/55b06988518a9554add4ad341f107320.png)'
- en: Example of analogies derived from word embeddings (image from developers.google.com)
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: ä»è¯åµŒå…¥æ´¾ç”Ÿçš„ç±»æ¯”ç¤ºä¾‹ï¼ˆå›¾ç‰‡æ¥è‡ª developers.google.comï¼‰
- en: 'Using Haiku, generating learnable embeddings is as simple as calling:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨ Haikuï¼Œç”Ÿæˆå¯å­¦ä¹ çš„åµŒå…¥å°±åƒè°ƒç”¨ä¸€æ ·ç®€å•ï¼š
- en: '[PRE0]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: These embeddings will be updated along with other learnable parameters during
    model training *(more on that in a second)*.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™äº›åµŒå…¥å°†åœ¨æ¨¡å‹è®­ç»ƒæœŸé—´ä¸å…¶ä»–å¯å­¦ä¹ çš„å‚æ•°ä¸€èµ·æ›´æ–°ï¼ˆ*ç¨åä¼šè¯¦ç»†ä»‹ç»*ï¼‰ã€‚
- en: Positional Encodings
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä½ç½®ç¼–ç 
- en: As opposed to recurrent neural nets, Transformers canâ€™t infer the position of
    a token given a shared hidden state as they **lack recurrent** or **convolutional
    structures**. Hence the introduction of **positional encodings, vectors** that
    convey a **tokenâ€™s position** in the **input sequence**.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸é€’å½’ç¥ç»ç½‘ç»œä¸åŒï¼ŒTransformersæ— æ³•æ ¹æ®å…±äº«çš„éšè—çŠ¶æ€æ¨æ–­ä»¤ç‰Œçš„ä½ç½®ï¼Œå› ä¸ºå®ƒä»¬**ç¼ºä¹é€’å½’**æˆ–**å·ç§¯ç»“æ„**ã€‚å› æ­¤ï¼Œå¼•å…¥äº†**ä½ç½®ç¼–ç **ï¼Œè¿™äº›å‘é‡ä¼ è¾¾äº†**ä»¤ç‰Œåœ¨è¾“å…¥åºåˆ—ä¸­çš„ä½ç½®**ã€‚
- en: Essentially, each token is assigned a **positional vector** composed of **alternating
    sine and cosine values**. Those vectors match the dimensionality of word embeddings
    so that both can be summed.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: ä»æœ¬è´¨ä¸Šè®²ï¼Œæ¯ä¸ªä»¤ç‰Œè¢«åˆ†é…ä¸€ä¸ªç”±**äº¤æ›¿çš„æ­£å¼¦å’Œä½™å¼¦å€¼**ç»„æˆçš„**ä½ç½®å‘é‡**ã€‚è¿™äº›å‘é‡çš„ç»´åº¦ä¸è¯åµŒå…¥ç›¸åŒ¹é…ï¼Œä»¥ä¾¿ä¸¤è€…å¯ä»¥ç›¸åŠ ã€‚
- en: 'In particular, the original Transformer paper uses the following functions:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: ç‰¹åˆ«æ˜¯ï¼ŒåŸå§‹çš„ Transformer è®ºæ–‡ä½¿ç”¨äº†ä»¥ä¸‹å‡½æ•°ï¼š
- en: '![](../Images/683d82eb429e0126a88dbded90881ceb.png)![](../Images/c29c91a5570ead1acef50cc6df4c3e59.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/683d82eb429e0126a88dbded90881ceb.png)![](../Images/c29c91a5570ead1acef50cc6df4c3e59.png)'
- en: Positional Encoding functions (reproduced from â€œAttention is all you needâ€,
    Vaswani et al. 2017)
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ç½®ç¼–ç å‡½æ•°ï¼ˆè½¬è½½è‡ªâ€œAttention is all you needâ€ï¼ŒVaswaniç­‰ï¼Œ2017ï¼‰
- en: The below figures enable us to further understand the functioning of positional
    encodings. Letâ€™s take a look at the first row of the uppermost plot, we can see
    **alternating sequences of zeros and ones.** Indeed, rows represent the position
    of a token in the sequence (the `pos` variable) while columns represent the embedding
    dimension (the `i` variable).
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸‹é¢çš„å›¾ä½¿æˆ‘ä»¬èƒ½å¤Ÿè¿›ä¸€æ­¥ç†è§£ä½ç½®ç¼–ç çš„åŠŸèƒ½ã€‚è®©æˆ‘ä»¬æ¥çœ‹ä¸€ä¸‹æœ€ä¸Šé¢å›¾çš„ç¬¬ä¸€è¡Œï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°**äº¤æ›¿çš„é›¶å’Œä¸€åºåˆ—**ã€‚å®é™…ä¸Šï¼Œè¡Œè¡¨ç¤ºåºåˆ—ä¸­ä¸€ä¸ª token
    çš„ä½ç½®ï¼ˆ`pos` å˜é‡ï¼‰ï¼Œè€Œåˆ—è¡¨ç¤ºåµŒå…¥ç»´åº¦ï¼ˆ`i` å˜é‡ï¼‰ã€‚
- en: Therefore, when `pos=0`, the previous equations return `sin(0)=0` for even embedding
    dimensions and `cos(0)=1` for odd dimensions.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: å› æ­¤ï¼Œå½“ `pos=0` æ—¶ï¼Œä¹‹å‰çš„æ–¹ç¨‹å¯¹å¶æ•°åµŒå…¥ç»´åº¦è¿”å› `sin(0)=0`ï¼Œå¯¹å¥‡æ•°ç»´åº¦è¿”å› `cos(0)=1`ã€‚
- en: Moreover, we see that adjacent rows share similar values, whereas the first
    and last rows are wildly different. This property is helpful for the model to
    assess the **distance between words** in the sequence as well as their **ordering**.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤å¤–ï¼Œæˆ‘ä»¬çœ‹åˆ°ç›¸é‚»çš„è¡Œå…·æœ‰ç›¸ä¼¼çš„å€¼ï¼Œè€Œç¬¬ä¸€è¡Œå’Œæœ€åä¸€è¡Œåˆ™å·®å¼‚å¾ˆå¤§ã€‚è¿™ä¸€ç‰¹æ€§æœ‰åŠ©äºæ¨¡å‹è¯„ä¼°åºåˆ—ä¸­**å•è¯ä¹‹é—´çš„è·ç¦»**ä»¥åŠå®ƒä»¬çš„**é¡ºåº**ã€‚
- en: Finally, the third plot represents the sum of positional encodings and embeddings,
    which is the output of the embedding block.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åï¼Œç¬¬ä¸‰ä¸ªå›¾è¡¨ç¤ºä½ç½®ç¼–ç å’ŒåµŒå…¥çš„æ€»å’Œï¼Œè¿™å°±æ˜¯åµŒå…¥å—çš„è¾“å‡ºã€‚
- en: Representation of word embeddings and positional encodings, with seq_len=16
    and embed_dim=64 (made by the author)
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: å•è¯åµŒå…¥å’Œä½ç½®ç¼–ç çš„è¡¨ç¤ºï¼Œseq_len=16 å’Œ embed_dim=64ï¼ˆç”±ä½œè€…åˆ¶ä½œï¼‰
- en: Using Haiku, we define the embedding layer as follows. Similarly to other deep
    learning frameworks, Haiku allows us to define **custom modules** (here `hk.Module`)
    to **store learnable parameters** and **define the behavior** of our modelâ€™s components.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨ Haikuï¼Œæˆ‘ä»¬å°†åµŒå…¥å±‚å®šä¹‰å¦‚ä¸‹ã€‚ä¸å…¶ä»–æ·±åº¦å­¦ä¹ æ¡†æ¶ç±»ä¼¼ï¼ŒHaiku å…è®¸æˆ‘ä»¬å®šä¹‰**è‡ªå®šä¹‰æ¨¡å—**ï¼ˆæ­¤å¤„ä¸º `hk.Module`ï¼‰ï¼Œä»¥**å­˜å‚¨å¯å­¦ä¹ çš„å‚æ•°**å’Œ**å®šä¹‰æ¨¡å‹ç»„ä»¶çš„è¡Œä¸º**ã€‚
- en: Each Haiku module needs to have an `__init__`and `__call__`function. Here, the
    call function simply computes the embeddings using the `hk.Embed` function and
    the positional encodings, before summing them.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: æ¯ä¸ª Haiku æ¨¡å—éœ€è¦æœ‰ä¸€ä¸ª `__init__` å’Œ `__call__` å‡½æ•°ã€‚åœ¨è¿™é‡Œï¼Œcall å‡½æ•°ç®€å•åœ°ä½¿ç”¨ `hk.Embed` å‡½æ•°å’Œä½ç½®ç¼–ç è®¡ç®—åµŒå…¥ï¼Œç„¶åå¯¹å…¶è¿›è¡Œæ±‚å’Œã€‚
- en: The positional encoding function uses JAX functionalities such as `vmap`and`lax.cond`for
    performance. If you are unfamiliar with those functions, feel free to check out
    my [previous post](https://medium.com/towards-data-science/vectorize-and-parallelize-rl-environments-with-jax-q-learning-at-the-speed-of-light-49d07373adf5)
    where they are presented more in-depth.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ç½®ç¼–ç å‡½æ•°ä½¿ç”¨ JAX åŠŸèƒ½ï¼Œå¦‚ `vmap` å’Œ `lax.cond` æ¥æé«˜æ€§èƒ½ã€‚å¦‚æœä½ å¯¹è¿™äº›å‡½æ•°ä¸ç†Ÿæ‚‰ï¼Œå¯ä»¥æŸ¥çœ‹æˆ‘çš„[ä¸Šä¸€ç¯‡æ–‡ç« ](https://medium.com/towards-data-science/vectorize-and-parallelize-rl-environments-with-jax-q-learning-at-the-speed-of-light-49d07373adf5)ï¼Œé‚£é‡Œå¯¹è¿™äº›å‡½æ•°è¿›è¡Œäº†æ›´æ·±å…¥çš„ä»‹ç»ã€‚
- en: Put simply, `vmap`allows us to define a function for a **single sample** and
    **vectorize it** so that it can be applied to **batches** of data. The `in_axes`parameter
    is used to specify that we want to iterate over the first axis of the `dim`input,
    which is the embedding dimension. On the other hand, `lax.cond`is an XLA-compatible
    version of a Python if/else statement.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: ç®€è€Œè¨€ä¹‹ï¼Œ`vmap` å…è®¸æˆ‘ä»¬ä¸º**å•ä¸ªæ ·æœ¬**å®šä¹‰ä¸€ä¸ªå‡½æ•°å¹¶**å°†å…¶å‘é‡åŒ–**ï¼Œä»¥ä¾¿å®ƒå¯ä»¥åº”ç”¨äº**æ•°æ®æ‰¹æ¬¡**ã€‚`in_axes` å‚æ•°ç”¨äºæŒ‡å®šæˆ‘ä»¬è¦éå†
    `dim` è¾“å…¥çš„ç¬¬ä¸€ä¸ªè½´ï¼Œå³åµŒå…¥ç»´åº¦ã€‚å¦ä¸€æ–¹é¢ï¼Œ`lax.cond` æ˜¯ XLA å…¼å®¹ç‰ˆæœ¬çš„ Python if/else è¯­å¥ã€‚
- en: Self-attention and MultiHead-Attention
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: è‡ªæ³¨æ„åŠ›å’Œå¤šå¤´æ³¨æ„åŠ›
- en: 'Attention aims to compute the **importance of each word in a sequence**, **relative
    to an input word**. For example, in the sentence:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: æ³¨æ„åŠ›æ—¨åœ¨è®¡ç®—**åºåˆ—ä¸­æ¯ä¸ªå•è¯çš„é‡è¦æ€§**ï¼Œ**ç›¸å¯¹äºè¾“å…¥å•è¯**ã€‚ä¾‹å¦‚ï¼Œåœ¨å¥å­ä¸­ï¼š
- en: â€œThe black cat jumped on the sofa, lied down and fell asleep, as it was tiredâ€.
  id: totrans-62
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: â€œé‚£åªé»‘çŒ«è·³ä¸Šæ²™å‘ï¼Œèººä¸‹å¹¶å…¥ç¡ï¼Œå› ä¸ºå®ƒç´¯äº†ã€‚â€
- en: The word â€œ**it**â€ could be quite ambiguous for the model, as *technically*,
    it could refer to both â€œ**cat**â€ and â€œ**sofa**â€. A well-trained attention model
    would be able to understand that â€œ**it**â€ refers to â€œ**cat**â€ and therefore assign
    attention values to the rest of the sentence accordingly.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: è¯è¯­â€œ**å®ƒ**â€å¯¹äºæ¨¡å‹æ¥è¯´å¯èƒ½ä¼šç›¸å½“æ¨¡ç³Šï¼Œå› ä¸º*ä»æŠ€æœ¯ä¸Šè®²*ï¼Œå®ƒå¯ä»¥æŒ‡ä»£â€œ**çŒ«**â€æˆ–â€œ**æ²™å‘**â€ã€‚ä¸€ä¸ªç»è¿‡è‰¯å¥½è®­ç»ƒçš„æ³¨æ„åŠ›æ¨¡å‹èƒ½å¤Ÿç†è§£â€œ**å®ƒ**â€æŒ‡çš„æ˜¯â€œ**çŒ«**â€ï¼Œå¹¶å› æ­¤ä¸ºå¥å­çš„å…¶ä½™éƒ¨åˆ†åˆ†é…ç›¸åº”çš„æ³¨æ„åŠ›å€¼ã€‚
- en: Essentially, **attention values** could be seen as **weights** that describe
    the **importance** of a certain word **given the context of the input** word.
    For instance, the attention vector for the word â€œ**jumped**â€ would have high values
    for words like â€œ**cat**â€ (*what* jumped?), â€œ**on**â€, and â€œ**sofa**â€ (*where* did
    it jump?) as these words are **relevant to its context**.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬è´¨ä¸Šï¼Œ**æ³¨æ„åŠ›å€¼**å¯ä»¥è§†ä¸º**æƒé‡**ï¼Œæè¿°äº†æŸä¸ªå•è¯**åœ¨ç»™å®šè¾“å…¥ä¸Šä¸‹æ–‡ä¸­çš„é‡è¦æ€§**ã€‚ä¾‹å¦‚ï¼Œâ€œ**è·³è·ƒ**â€ä¸€è¯çš„æ³¨æ„åŠ›å‘é‡ä¼šå¯¹â€œ**çŒ«**â€ï¼ˆ*è·³è·ƒäº†ä»€ä¹ˆï¼Ÿ*ï¼‰ã€â€œ**åœ¨**â€å’Œâ€œ**æ²™å‘**â€ï¼ˆ*è·³è·ƒåˆ°å“ªé‡Œï¼Ÿ*ï¼‰ç­‰è¯å…·æœ‰è¾ƒé«˜çš„å€¼ï¼Œå› ä¸ºè¿™äº›è¯**ä¸å…¶ä¸Šä¸‹æ–‡ç›¸å…³**ã€‚
- en: '![](../Images/31016a82455c64d4f840c4bbe4861bad.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/31016a82455c64d4f840c4bbe4861bad.png)'
- en: Visual representation of an **attention vector** (made by the author)
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '**æ³¨æ„åŠ›å‘é‡**çš„å¯è§†åŒ–è¡¨ç¤ºï¼ˆç”±ä½œè€…åˆ¶ä½œï¼‰'
- en: 'In the Transformer paper, attention is computed using ***Scaled Dot-Product
    Attention***. Which is summarized by the formula:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ Transformer è®ºæ–‡ä¸­ï¼Œæ³¨æ„åŠ›æ˜¯ä½¿ç”¨***ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›***è®¡ç®—çš„ã€‚å…¶å…¬å¼æ€»ç»“å¦‚ä¸‹ï¼š
- en: '![](../Images/252c2d3a224b203a09510ff5ba3d43a9.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/252c2d3a224b203a09510ff5ba3d43a9.png)'
- en: Scaled Dot-Product Attention (reproduced from â€œAttention is all you needâ€, *Vaswani
    et al. 2017*)
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›ï¼ˆé‡ç°è‡ªâ€œAttention is all you needâ€ï¼Œ*Vaswani et al. 2017*ï¼‰
- en: Here, Q,K and V stand for ***Queries, Keys*** and ***Values****.* These matrices
    are obtained by multiplying learned weight vectors WQ, WK and WV with positional
    embeddings.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™é‡Œï¼ŒQã€K å’Œ V åˆ†åˆ«ä»£è¡¨***æŸ¥è¯¢ã€é”®***å’Œ***å€¼***ã€‚è¿™äº›çŸ©é˜µæ˜¯é€šè¿‡å°†å­¦ä¹ åˆ°çš„æƒé‡å‘é‡ WQã€WK å’Œ WV ä¸ä½ç½®åµŒå…¥ç›¸ä¹˜å¾—åˆ°çš„ã€‚
- en: These names are mainly **abstractions** used to help understand how the information
    is processed and weighted in the attention block. They are an allusion to **retrieval
    systems** vocabulary[2] (e.g. searching a video on YouTube for instance).
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™äº›åç§°ä¸»è¦æ˜¯**æŠ½è±¡æ¦‚å¿µ**ï¼Œç”¨äºå¸®åŠ©ç†è§£ä¿¡æ¯åœ¨æ³¨æ„åŠ›å—ä¸­çš„å¤„ç†å’ŒåŠ æƒæ–¹å¼ã€‚å®ƒä»¬æš—æŒ‡**æ£€ç´¢ç³»ç»Ÿ**çš„è¯æ±‡[2]ï¼ˆä¾‹å¦‚ï¼Œåœ¨ YouTube ä¸Šæœç´¢è§†é¢‘ï¼‰ã€‚
- en: 'Hereâ€™s an **intuitive** explanation:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é‡Œæ˜¯ä¸€ä¸ª**ç›´è§‚çš„**è§£é‡Šï¼š
- en: '**Queries**: They can be interpreted as a â€œ*set of questions*â€ about all the
    positions in a sequence. For instance, interrogating the context of a word and
    trying to identify the most relevant parts of the sequence.'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**æŸ¥è¯¢**ï¼šå®ƒä»¬å¯ä»¥è¢«ç†è§£ä¸ºå…³äºåºåˆ—ä¸­æ‰€æœ‰ä½ç½®çš„â€œ*ä¸€ç»„é—®é¢˜*â€ã€‚ä¾‹å¦‚ï¼Œè¯¢é—®ä¸€ä¸ªå•è¯çš„ä¸Šä¸‹æ–‡å¹¶è¯•å›¾è¯†åˆ«åºåˆ—ä¸­æœ€ç›¸å…³çš„éƒ¨åˆ†ã€‚'
- en: '**Keys**: They can be seen as holding information that the queries interact
    with, the compatibility between a query and a key determines how much attention
    the query should pay to the corresponding value.'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**é”®**ï¼šå®ƒä»¬å¯ä»¥è¢«è§†ä¸ºåŒ…å«æŸ¥è¯¢äº¤äº’çš„ä¿¡æ¯ï¼ŒæŸ¥è¯¢ä¸é”®ä¹‹é—´çš„å…¼å®¹æ€§å†³å®šäº†æŸ¥è¯¢åº”è¯¥ç»™äºˆå¯¹åº”å€¼å¤šå°‘æ³¨æ„åŠ›ã€‚'
- en: '**Values**: Matching keys and queries allows us to decide which keys are relevant,
    values are the actual content paired with the keys.'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**å€¼**ï¼šåŒ¹é…çš„é”®å’ŒæŸ¥è¯¢ä½¿æˆ‘ä»¬èƒ½å¤Ÿå†³å®šå“ªäº›é”®æ˜¯ç›¸å…³çš„ï¼Œå€¼æ˜¯ä¸é”®é…å¯¹çš„å®é™…å†…å®¹ã€‚'
- en: In the following figure, the query is a YouTube search, the keys are the video
    descriptions and metadata, while the value are the associated videos.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä¸‹å›¾ä¸­ï¼ŒæŸ¥è¯¢æ˜¯ YouTube æœç´¢ï¼Œé”®æ˜¯è§†é¢‘æè¿°å’Œå…ƒæ•°æ®ï¼Œè€Œå€¼æ˜¯ç›¸å…³è”çš„è§†é¢‘ã€‚
- en: '![](../Images/483f936afe83dcd9d81810153894e7b2.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/483f936afe83dcd9d81810153894e7b2.png)'
- en: Intuitive representation of the Queries, Keys, Values concept (made by the author)
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: æŸ¥è¯¢ã€é”®ã€å€¼æ¦‚å¿µçš„ç›´è§‚è¡¨ç¤ºï¼ˆç”±ä½œè€…åˆ¶ä½œï¼‰
- en: In our case, queries, keys, and values come from the **same source** (as theyâ€™re
    derived from the input sequences), hence the name **self-attention**.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æˆ‘ä»¬çš„æƒ…å†µä¸‹ï¼ŒæŸ¥è¯¢ã€é”®å’Œå€¼æ¥è‡ªäº**åŒä¸€æ¥æº**ï¼ˆå› ä¸ºå®ƒä»¬æ˜¯ä»è¾“å…¥åºåˆ—æ´¾ç”Ÿçš„ï¼‰ï¼Œå› æ­¤è¢«ç§°ä¸º**è‡ªæ³¨æ„åŠ›**ã€‚
- en: The computation of attention scores is usually executed **multiple times in
    parallel**, each time with a **fraction of the embeddings**. This mechanism is
    called â€œ**Multi-Head Attention**â€ and enables each head to learn several different
    representations of the data in parallel, leading to a more **robust** model.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: æ³¨æ„åŠ›åˆ†æ•°çš„è®¡ç®—é€šå¸¸æ˜¯**å¤šæ¬¡å¹¶è¡Œæ‰§è¡Œ**çš„ï¼Œæ¯æ¬¡ä½¿ç”¨**éƒ¨åˆ†åµŒå…¥**ã€‚è¿™ä¸€æœºåˆ¶è¢«ç§°ä¸ºâ€œ**å¤šå¤´æ³¨æ„åŠ›**â€ï¼Œä½¿æ¯ä¸ªå¤´å¯ä»¥å¹¶è¡Œåœ°å­¦ä¹ æ•°æ®çš„å‡ ç§ä¸åŒè¡¨ç¤ºï¼Œä»è€Œå½¢æˆæ›´**å¼ºå¥**çš„æ¨¡å‹ã€‚
- en: A single attention head would generally process arrays with shape (`batch_size,
    seq_len, d_k`**)**where `d_k`can be set as the ratio between the number of heads
    and the dimension of the embeddings (`d_k = n_heads/embed_dim`). This way, concatenating
    the outputs of each head conveniently gives an array with shape **(**`batch_size,
    seq_len, embed_dim`**)**, as the input.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: å•ä¸ªæ³¨æ„åŠ›å¤´é€šå¸¸å¤„ç†å½¢çŠ¶ä¸º (`batch_size, seq_len, d_k`**)** çš„æ•°ç»„ï¼Œå…¶ä¸­ `d_k` å¯ä»¥è®¾ç½®ä¸ºå¤´æ•°ä¸åµŒå…¥ç»´åº¦çš„æ¯”ç‡ï¼ˆ`d_k
    = n_heads/embed_dim`ï¼‰ã€‚è¿™æ ·ï¼Œè¿æ¥æ¯ä¸ªå¤´çš„è¾“å‡ºå°±èƒ½æ–¹ä¾¿åœ°å¾—åˆ°å½¢çŠ¶ä¸º**ï¼ˆ`batch_size, seq_len, embed_dim`**ï¼‰çš„æ•°ç»„ï¼Œä½œä¸ºè¾“å…¥ã€‚
- en: 'The computation of attention matrices can be broken down into several steps:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: æ³¨æ„åŠ›çŸ©é˜µçš„è®¡ç®—å¯ä»¥åˆ†è§£ä¸ºå‡ ä¸ªæ­¥éª¤ï¼š
- en: First, we define **learnable weight vectors** WQ, WK, and WV. These vectors
    have shapes **(**`n_heads, embed_dim, d_k`**)**.
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: é¦–å…ˆï¼Œæˆ‘ä»¬å®šä¹‰**å¯å­¦ä¹ çš„æƒé‡å‘é‡** WQã€WK å’Œ WVã€‚è¿™äº›å‘é‡çš„å½¢çŠ¶ä¸ºï¼ˆ`n_heads, embed_dim, d_k`ï¼‰ã€‚
- en: In parallel, we **multiply** the **positional embeddings** with the **weight
    vectors**. We obtain Q, K, and V matrices with shapes **(**`batch_size, seq_len,
    d_k`**)**.
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åŒæ—¶ï¼Œæˆ‘ä»¬å°†**ä½ç½®åµŒå…¥**ä¸**æƒé‡å‘é‡**ç›¸ä¹˜ã€‚æˆ‘ä»¬å¾—åˆ°å½¢çŠ¶ä¸ºï¼ˆ`batch_size, seq_len, d_k`ï¼‰çš„ Qã€K å’Œ V çŸ©é˜µã€‚
- en: We then **scale** the **dot-product** of Q and K (transposed). This scaling
    involves dividing the result of the dot-product by the square root of `d_k`and
    applying the softmax function on the matrices rows. Therefore, attention scores
    for an input token (i.e. a row) sum up to one, this helps prevent values from
    becoming too large and slowing down computation. The output has shape (`batch_size,
    seq_len, seq_len`)
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç„¶åï¼Œæˆ‘ä»¬å¯¹ Q å’Œ Kï¼ˆè½¬ç½®ï¼‰çš„ç‚¹ç§¯è¿›è¡Œ**ç¼©æ”¾**ã€‚è¿™ä¸ªç¼©æ”¾æ“ä½œåŒ…æ‹¬å°†ç‚¹ç§¯çš„ç»“æœé™¤ä»¥ `d_k` çš„å¹³æ–¹æ ¹ï¼Œå¹¶åœ¨çŸ©é˜µçš„è¡Œä¸Šåº”ç”¨ softmax å‡½æ•°ã€‚å› æ­¤ï¼Œå¯¹äºè¾“å…¥çš„ä»¤ç‰Œï¼ˆå³ä¸€è¡Œï¼‰ï¼Œæ³¨æ„åŠ›åˆ†æ•°æ€»å’Œä¸ºä¸€ï¼Œè¿™æœ‰åŠ©äºé˜²æ­¢å€¼å˜å¾—è¿‡å¤§è€Œå‡æ…¢è®¡ç®—é€Ÿåº¦ã€‚è¾“å‡ºçš„å½¢çŠ¶ä¸º
    (`batch_size, seq_len, seq_len`)ã€‚
- en: Finally, we dot the result of the previous operation with V, making the shape
    of the output (`batch_size, seq_len, d_k`).
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æœ€åï¼Œæˆ‘ä»¬å°†ä¸Šä¸€æ“ä½œçš„ç»“æœä¸ V è¿›è¡Œç‚¹ä¹˜ï¼Œè¾“å‡ºçš„å½¢çŠ¶ä¸º (`batch_size, seq_len, d_k`)ã€‚
- en: '![](../Images/53a16f4b352d266f793268e64d9cd1e6.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/53a16f4b352d266f793268e64d9cd1e6.png)'
- en: Visual representation of matrix operations inside **an attention block** (made
    by the author)
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '**æ³¨æ„å—**å†…éƒ¨çš„çŸ©é˜µæ“ä½œçš„å¯è§†åŒ–è¡¨ç¤ºï¼ˆä½œè€…åˆ¶ä½œï¼‰'
- en: The outputs of each attention head can then be **concatenated** to form a matrix
    with shape (`batch_size, seq_len, embed_dim`). The Transformer paper also adds
    a **linear layer** at the end of the multi-head attention module, to **aggregate**
    and **combine** the learned representations from **all the attention heads**.
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç„¶åï¼Œæ¯ä¸ªæ³¨æ„åŠ›å¤´çš„è¾“å‡ºå¯ä»¥**ä¸²è”**èµ·æ¥å½¢æˆä¸€ä¸ªå½¢çŠ¶ä¸ºï¼ˆ`batch_size, seq_len, embed_dim`ï¼‰çš„çŸ©é˜µã€‚Transformerè®ºæ–‡è¿˜åœ¨å¤šå¤´æ³¨æ„åŠ›æ¨¡å—çš„æœ€åæ·»åŠ äº†ä¸€ä¸ª**çº¿æ€§å±‚**ï¼Œç”¨äº**æ±‡èš**å’Œ**ç»„åˆ**æ¥è‡ª**æ‰€æœ‰æ³¨æ„åŠ›å¤´**çš„å­¦ä¹ è¡¨ç¤ºã€‚
- en: '![](../Images/2b83a1bf528562df5c6794e0dffe2819.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2b83a1bf528562df5c6794e0dffe2819.png)'
- en: Concatenation of multi-head attention matrices and linear layer (made by the
    author)
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: å¤šå¤´æ³¨æ„åŠ›çŸ©é˜µçš„ä¸²è”å’Œçº¿æ€§å±‚ï¼ˆä½œè€…åˆ¶ä½œï¼‰
- en: In Haiku, the Multi-Head Attention module can be implemented as follows. The
    `__call__`function follows the same logic as the above graph while the class methods
    take advantage of JAX utilities such as `vmap`(to vectorize our operations over
    the different attention heads and matrices) and `tree_map`(to map matrix dot-products
    over weight vectors).
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨Haikuä¸­ï¼Œå¤šå¤´æ³¨æ„åŠ›æ¨¡å—å¯ä»¥å¦‚ä¸‹å®ç°ã€‚`__call__`å‡½æ•°éµå¾ªä¸ä¸Šè¿°å›¾è¡¨ç›¸åŒçš„é€»è¾‘ï¼Œè€Œç±»æ–¹æ³•åˆ©ç”¨JAXå®ç”¨ç¨‹åºï¼Œå¦‚`vmap`ï¼ˆåœ¨ä¸åŒæ³¨æ„åŠ›å¤´å’ŒçŸ©é˜µä¸Šå‘é‡åŒ–æˆ‘ä»¬çš„æ“ä½œï¼‰å’Œ`tree_map`ï¼ˆåœ¨æƒé‡å‘é‡ä¸Šæ˜ å°„çŸ©é˜µç‚¹ç§¯ï¼‰ã€‚
- en: Residual Connections and Layer Normalization
  id: totrans-93
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ®‹å·®è¿æ¥å’Œå±‚å½’ä¸€åŒ–
- en: As you might have noticed on the Transformer graph, the multi-head attention
    block and the feed-forward net are followed by **residual connections** and **layer
    normalization**.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: æ­£å¦‚ä½ åœ¨Transformerå›¾ä¸­æ‰€æ³¨æ„åˆ°çš„ï¼Œå¤šå¤´æ³¨æ„åŠ›å—å’Œå‰é¦ˆç½‘ç»œä¹‹åè·Ÿç€**æ®‹å·®è¿æ¥**å’Œ**å±‚å½’ä¸€åŒ–**ã€‚
- en: Residual or skip connections
  id: totrans-95
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ®‹å·®è¿æ¥æˆ–è·³è·ƒè¿æ¥
- en: Residual connections are a standard solution to **solve** the **vanishing gradient
    problem**, which occurs when gradients become too small to effectively update
    the modelâ€™s parameters.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: æ®‹å·®è¿æ¥æ˜¯è§£å†³**æ¢¯åº¦æ¶ˆå¤±é—®é¢˜**çš„æ ‡å‡†è§£å†³æ–¹æ¡ˆï¼Œå³å½“æ¢¯åº¦å˜å¾—å¤ªå°ä»¥è‡³äºæ— æ³•æœ‰æ•ˆæ›´æ–°æ¨¡å‹å‚æ•°æ—¶ã€‚
- en: As this issue naturally arises in particularly deep architectures, residual
    connections are used in a variety of complex models such as **ResNet** *(*[*Kaiming
    et al*](https://arxiv.org/abs/1512.03385v1)*, 2015)* in computer vision, **AlphaZero**
    ([*Silver et al*](https://arxiv.org/abs/1712.01815v1)*, 2017*) in reinforcement
    learning, and of course, **Transformers**.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: ç”±äºè¿™ä¸ªé—®é¢˜åœ¨ç‰¹åˆ«æ·±çš„æ¶æ„ä¸­è‡ªç„¶è€Œç„¶åœ°å‡ºç°ï¼Œæ‰€ä»¥æ®‹å·®è¿æ¥è¢«ç”¨åœ¨å„ç§å¤æ‚æ¨¡å‹ä¸­ï¼Œæ¯”å¦‚åœ¨è®¡ç®—æœºè§†è§‰ä¸­çš„**ResNet**ï¼ˆ[*Kaiming et al*](https://arxiv.org/abs/1512.03385v1)ï¼Œ2015å¹´ï¼‰ï¼Œåœ¨å¼ºåŒ–å­¦ä¹ ä¸­çš„**AlphaZero**ï¼ˆ[*Silver
    et al*](https://arxiv.org/abs/1712.01815v1)ï¼Œ2017å¹´ï¼‰ï¼Œå½“ç„¶è¿˜æœ‰**Transformers**ã€‚
- en: In practice, residual connections simply forward the output of a specific layer
    to a following one, **skipping one or more layers** on the way. For instance,
    the residual connection around the multi-head attention is equivalent to summing
    the output of multi-head attention with positional embeddings.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å®è·µä¸­ï¼Œæ®‹å·®è¿æ¥ç®€å•åœ°å°†ç‰¹å®šå±‚çš„è¾“å‡ºç›´æ¥è½¬å‘åˆ°ä¸‹ä¸€å±‚ï¼Œ**è·³è¿‡ä¸€ä¸ªæˆ–å¤šä¸ªå±‚**ã€‚ä¾‹å¦‚ï¼Œå›´ç»•å¤šå¤´æ³¨æ„åŠ›çš„æ®‹å·®è¿æ¥ç›¸å½“äºå°†å¤šå¤´æ³¨æ„åŠ›çš„è¾“å‡ºä¸ä½ç½®åµŒå…¥æ±‚å’Œã€‚
- en: This enables gradients to flow more efficiently through the architecture during
    backpropagation and can usually lead to **faster convergence** and more **stable
    training**.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä½¿å¾—æ¢¯åº¦åœ¨åå‘ä¼ æ’­è¿‡ç¨‹ä¸­æ›´æœ‰æ•ˆåœ°æµåŠ¨ï¼Œé€šå¸¸å¯ä»¥å¯¼è‡´**æ›´å¿«çš„æ”¶æ•›**å’Œæ›´**ç¨³å®šçš„è®­ç»ƒ**ã€‚
- en: '![](../Images/abba4a948e3ceaf4187f0f3469983f17.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/abba4a948e3ceaf4187f0f3469983f17.png)'
- en: Representation of **residual connections** in Transformers (made by the author)
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer ä¸­**æ®‹å·®è¿æ¥**çš„è¡¨ç¤ºï¼ˆç”±ä½œè€…åˆ¶ä½œï¼‰
- en: Layer Normalization
  id: totrans-102
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å±‚å½’ä¸€åŒ–
- en: Layer normalization helps ensure that the values propagated through the model
    do not â€œ***explode***â€ (tend toward infinity), which could easily happen in attention
    blocks, where several matrices are multiplied during each forward pass.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: å±‚å½’ä¸€åŒ–æœ‰åŠ©äºç¡®ä¿é€šè¿‡æ¨¡å‹ä¼ æ’­çš„å€¼ä¸ä¼šâ€œ***çˆ†ç‚¸***â€ï¼ˆè¶‹å‘æ— ç©·å¤§ï¼‰ï¼Œè¿™ç§æƒ…å†µåœ¨æ³¨æ„åŠ›æ¨¡å—ä¸­å¾ˆå®¹æ˜“å‘ç”Ÿï¼Œå› ä¸ºåœ¨æ¯æ¬¡å‰å‘ä¼ é€’ä¸­ä¼šæœ‰å¤šä¸ªçŸ©é˜µç›¸ä¹˜ã€‚
- en: Unlike batch normalization, which normalizes across the batch dimension assuming
    a uniform distribution, **layer normalization operates** **across the features**.
    This approach is suitable for sentence batches where each may have **unique distributions**
    due to **varying meanings** and **vocabularies**.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸åœ¨æ‰¹æ¬¡ç»´åº¦ä¸Šè¿›è¡Œå½’ä¸€åŒ–å¹¶å‡è®¾å‡åŒ€åˆ†å¸ƒçš„æ‰¹é‡å½’ä¸€åŒ–ä¸åŒï¼Œ**å±‚å½’ä¸€åŒ–åœ¨ç‰¹å¾**ä¸Šè¿›è¡Œå½’ä¸€åŒ–ã€‚æ­¤æ–¹æ³•é€‚ç”¨äºå¥å­æ‰¹æ¬¡ï¼Œå› ä¸ºæ¯ä¸ªå¥å­å¯èƒ½ç”±äº**ä¸åŒçš„æ„ä¹‰**å’Œ**è¯æ±‡**è€Œå…·æœ‰**ç‹¬ç‰¹çš„åˆ†å¸ƒ**ã€‚
- en: By normalizing across **features**, such as **embeddings** or **attention values**,
    layer normalization **standardizes data** to a consistent scale **without conflating
    distinct sentence characteristics**, maintaining the unique distribution of each.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: é€šè¿‡åœ¨**ç‰¹å¾**ä¸Šè¿›è¡Œå½’ä¸€åŒ–ï¼Œä¾‹å¦‚**åµŒå…¥**æˆ–**æ³¨æ„åŠ›å€¼**ï¼Œå±‚å½’ä¸€åŒ–**å°†æ•°æ®æ ‡å‡†åŒ–**ä¸ºä¸€è‡´çš„å°ºåº¦ï¼Œ**è€Œä¸ä¼šæ··æ·†ä¸åŒå¥å­çš„ç‰¹å¾**ï¼Œä¿æŒæ¯ä¸ªå¥å­çš„ç‹¬ç‰¹åˆ†å¸ƒã€‚
- en: '![](../Images/ac49f30b74e842229d2043091b9ef894.png)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ac49f30b74e842229d2043091b9ef894.png)'
- en: Representation of **Layer Normalization** in the context of Transformers (made
    by the author)
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer ä¸­**å±‚å½’ä¸€åŒ–**çš„è¡¨ç¤ºï¼ˆç”±ä½œè€…åˆ¶ä½œï¼‰
- en: The implementation of layer normalization is pretty straightforward, we initialize
    the learnable parameters alpha and beta and normalize along the desired feature
    axis.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: å±‚å½’ä¸€åŒ–çš„å®ç°éå¸¸ç®€å•ï¼Œæˆ‘ä»¬åˆå§‹åŒ–å¯å­¦ä¹ çš„å‚æ•° alpha å’Œ betaï¼Œå¹¶åœ¨æ‰€éœ€çš„ç‰¹å¾è½´ä¸Šè¿›è¡Œå½’ä¸€åŒ–ã€‚
- en: '**Position-wise Feed-Forward Network**'
  id: totrans-109
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**ä½ç½®-wise å‰é¦ˆç½‘ç»œ**'
- en: The last component of the encoder that we need to cover is the **position-wise
    feed-forward network**. This fully connected network takes the normalized outputs
    of the attention block as inputs and is used to introduce **non-linearity** and
    increase the **modelâ€™s capacity** to learn complex functions.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: ç¼–ç å™¨ä¸­æˆ‘ä»¬éœ€è¦è¦†ç›–çš„æœ€åä¸€ä¸ªç»„ä»¶æ˜¯**ä½ç½®-wise å‰é¦ˆç½‘ç»œ**ã€‚è¿™ä¸ªå…¨è¿æ¥ç½‘ç»œä»¥æ³¨æ„åŠ›å—çš„å½’ä¸€åŒ–è¾“å‡ºä½œä¸ºè¾“å…¥ï¼Œç”¨äºå¼•å…¥**éçº¿æ€§**å¹¶å¢åŠ **æ¨¡å‹çš„å®¹é‡**ä»¥å­¦ä¹ å¤æ‚çš„å‡½æ•°ã€‚
- en: 'It is composed of two dense layers separated by a [gelu activation](https://paperswithcode.com/method/gelu):'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: å®ƒç”±ä¸¤ä¸ªå¯†é›†å±‚ç»„æˆï¼Œä¸­é—´éš”ç€ä¸€ä¸ª [gelu æ¿€æ´»å‡½æ•°](https://paperswithcode.com/method/gelu)ã€‚
- en: After this block, we have another residual connection and layer normalization
    to complete the encoder.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ä¸ªæ¨¡å—ä¹‹åï¼Œæˆ‘ä»¬æœ‰å¦ä¸€ä¸ªæ®‹å·®è¿æ¥å’Œå±‚å½’ä¸€åŒ–ï¼Œä»¥å®Œæˆç¼–ç å™¨ã€‚
- en: Wrapping up
  id: totrans-113
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ€»ç»“
- en: 'There we have it! By now you should be familiar with the main concepts of the
    Transformer encoder. Hereâ€™s the full encoder class, notice that in Haiku, we assign
    a name to each layer, so that learnable parameters are separated and easy to access.
    The `__call__`function provides a good summary of the different steps of our encoder:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: å°±è¿™æ ·ï¼åˆ°ç°åœ¨ä½ åº”è¯¥å¯¹ Transformer ç¼–ç å™¨çš„ä¸»è¦æ¦‚å¿µå¾ˆç†Ÿæ‚‰äº†ã€‚è¿™æ˜¯å®Œæ•´çš„ç¼–ç å™¨ç±»ï¼Œè¯·æ³¨æ„ï¼Œåœ¨ Haiku ä¸­ï¼Œæˆ‘ä»¬ä¸ºæ¯ä¸€å±‚åˆ†é…äº†ä¸€ä¸ªåç§°ï¼Œä»¥ä¾¿å­¦ä¹ å‚æ•°åˆ†å¼€ä¸”æ˜“äºè®¿é—®ã€‚`__call__`å‡½æ•°å¾ˆå¥½åœ°æ€»ç»“äº†æˆ‘ä»¬ç¼–ç å™¨çš„ä¸åŒæ­¥éª¤ï¼š
- en: To use this module on actual data, we have to apply `hk.transform` to a function
    encapsulating the encoder class. Indeed, you might remember that JAX embraces
    the **functional programming** paradigm, therefore, Haiku follows the same principles.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: è¦åœ¨å®é™…æ•°æ®ä¸Šä½¿ç”¨æ­¤æ¨¡å—ï¼Œæˆ‘ä»¬å¿…é¡»å°† `hk.transform` åº”ç”¨äºå°è£…ç¼–ç å™¨ç±»çš„å‡½æ•°ã€‚ç¡®å®ï¼Œä½ å¯èƒ½ä¼šè®°å¾— JAX é‡‡ç”¨äº†**å‡½æ•°å¼ç¼–ç¨‹**èŒƒå¼ï¼Œå› æ­¤ï¼ŒHaiku
    éµå¾ªç›¸åŒçš„åŸåˆ™ã€‚
- en: 'We define a function containing an instance of the encoder class and return
    the output of a forward pass. Applying `hk.transform` returns a transformed object
    having access to two functions: `init` and `apply`.'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å®šä¹‰äº†ä¸€ä¸ªåŒ…å«ç¼–ç å™¨ç±»å®ä¾‹çš„å‡½æ•°ï¼Œå¹¶è¿”å›å‰å‘ä¼ é€’çš„è¾“å‡ºã€‚åº”ç”¨ `hk.transform` è¿”å›ä¸€ä¸ªè½¬æ¢å¯¹è±¡ï¼Œå…·æœ‰ä¸¤ä¸ªå‡½æ•°ï¼š`init` å’Œ `apply`ã€‚
- en: The former enables us to initialize the module with a random key as well as
    some dummy data (notice that here we pass an array of zeros with shape `batch_size,
    seq_len`) while the latter allows us to process real data.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: å‰è€…ä½¿æˆ‘ä»¬èƒ½å¤Ÿç”¨ä¸€ä¸ªéšæœºé”®å’Œä¸€äº›è™šæ‹Ÿæ•°æ®åˆå§‹åŒ–æ¨¡å—ï¼ˆè¯·æ³¨æ„è¿™é‡Œæˆ‘ä»¬ä¼ é€’çš„æ˜¯ä¸€ä¸ªå½¢çŠ¶ä¸º`batch_size, seq_len`çš„é›¶æ•°ç»„ï¼‰ï¼Œè€Œåè€…åˆ™å…è®¸æˆ‘ä»¬å¤„ç†çœŸå®æ•°æ®ã€‚
- en: '[PRE1]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: In the next article, weâ€™ll **complete the transformer** architecture by adding
    a **decoder**, which reuses most of the blocks we introduced so far, and learn
    how to **train a model** on a specific task using Optax!
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä¸‹ä¸€ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘ä»¬å°†**å®Œæˆ Transformer** æ¶æ„ï¼Œé€šè¿‡æ·»åŠ ä¸€ä¸ª **è§£ç å™¨**ï¼Œå®ƒé‡ç”¨äº†æˆ‘ä»¬è¿„ä»Šä¸ºæ­¢ä»‹ç»çš„å¤§éƒ¨åˆ†æ¨¡å—ï¼Œå¹¶å­¦ä¹ å¦‚ä½•ä½¿ç”¨ Optax
    **è®­ç»ƒæ¨¡å‹** ä»¥å®Œæˆç‰¹å®šä»»åŠ¡ï¼
- en: '**Conclusion**'
  id: totrans-120
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**ç»“è®º**'
- en: '**Thank you for reading this far**, if you are interested in dabbling with
    the code, you can find it fully commented on GitHub, along with additional details
    and a walkthrough using a toy dataset.'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '**æ„Ÿè°¢ä½ è¯»åˆ°è¿™é‡Œ**ï¼Œå¦‚æœä½ å¯¹ä»£ç æ„Ÿå…´è¶£ï¼Œå¯ä»¥åœ¨ GitHub ä¸Šæ‰¾åˆ°å®Œæ•´çš„æ³¨é‡Šä»¥åŠé¢å¤–çš„ç»†èŠ‚å’Œä½¿ç”¨ç©å…·æ•°æ®é›†çš„ç¤ºä¾‹ã€‚'
- en: '[](https://github.com/RPegoud/jab?source=post_page-----791d31b4f0dd--------------------------------)
    [## GitHub - RPegoud/jab: A collection of foundational Deep Learning models implemented
    in JAX'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://github.com/RPegoud/jab?source=post_page-----791d31b4f0dd--------------------------------)
    [## GitHub - RPegoud/jab: ä¸€ç»„ç”¨ JAX å®ç°çš„åŸºç¡€æ·±åº¦å­¦ä¹ æ¨¡å‹'
- en: 'A collection of foundational Deep Learning models implemented in JAX - GitHub
    - RPegoud/jab: A collection ofâ€¦'
  id: totrans-123
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 'ä¸€ç»„ç”¨ JAX å®ç°çš„åŸºç¡€æ·±åº¦å­¦ä¹ æ¨¡å‹ - GitHub - RPegoud/jab: ä¸€ç»„â€¦'
- en: github.com](https://github.com/RPegoud/jab?source=post_page-----791d31b4f0dd--------------------------------)
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: github.com](https://github.com/RPegoud/jab?source=post_page-----791d31b4f0dd--------------------------------)
- en: If youâ€™d like to dig deeper into Transformers, the following section contains
    some articles that helped me redact this article.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœä½ æƒ³æ›´æ·±å…¥äº†è§£ Transformersï¼Œä»¥ä¸‹éƒ¨åˆ†åŒ…å«äº†ä¸€äº›å¸®åŠ©æˆ‘æ’°å†™æœ¬æ–‡çš„æ–‡ç« ã€‚
- en: Until next time ğŸ‘‹
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸‹æ¬¡è§ ğŸ‘‹
- en: 'References and Resources:'
  id: totrans-127
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å‚è€ƒæ–‡çŒ®å’Œèµ„æºï¼š
- en: '[1] [***Attention is all you need***](https://arxiv.org/pdf/1706.03762.pdf)
    (2017), Vaswani et al, Google'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] [***Attention is all you need***](https://arxiv.org/pdf/1706.03762.pdf)
    (2017), Vaswani ç­‰ï¼Œè°·æ­Œ'
- en: '[2] [***What exactly are keys, queries, and values in attention mechanisms?***](https://stats.stackexchange.com/questions/421935/what-exactly-are-keys-queries-and-values-in-attention-mechanisms)*(2019)*Stack
    Exchange'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] [***æ³¨æ„æœºåˆ¶ä¸­çš„é”®ã€æŸ¥è¯¢å’Œå€¼åˆ°åº•æ˜¯ä»€ä¹ˆï¼Ÿ***](https://stats.stackexchange.com/questions/421935/what-exactly-are-keys-queries-and-values-in-attention-mechanisms)*(2019)*Stack
    Exchange'
- en: '[3] [***The Illustrated Transformer***](http://jalammar.github.io/illustrated-transformer/)*(2018),*
    [Jay Alammar](http://jalammar.github.io/)'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] [***å›¾è§£ Transformer***](http://jalammar.github.io/illustrated-transformer/)*(2018)ï¼Œ*
    [Jay Alammar](http://jalammar.github.io/)'
- en: '[4] [***A Gentle Introduction to Positional Encoding in Transformer Models***](https://machinelearningmastery.com/a-gentle-introduction-to-positional-encoding-in-transformer-models-part-1/)*(2023),*
    [**Mehreen Saeed**](https://machinelearningmastery.com/author/msaeed/), Machine
    Learning Mastery'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] [***Transformer æ¨¡å‹ä¸­ä½ç½®ç¼–ç çš„æ¸©å’Œä»‹ç»***](https://machinelearningmastery.com/a-gentle-introduction-to-positional-encoding-in-transformer-models-part-1/)*(2023)ï¼Œ*
    [**Mehreen Saeed**](https://machinelearningmastery.com/author/msaeed/)ï¼ŒMachine
    Learning Mastery'
- en: '[***JAX documentation***](https://jax.readthedocs.io/en/latest/index.html)'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[***JAX æ–‡æ¡£***](https://jax.readthedocs.io/en/latest/index.html)'
- en: '[***Haiku documentation***](https://dm-haiku.readthedocs.io/en/latest/notebooks/basics.html)'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[***Haiku æ–‡æ¡£***](https://dm-haiku.readthedocs.io/en/latest/notebooks/basics.html)'
- en: Image Credits
  id: totrans-134
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å›¾ç‰‡æ¥æº
- en: '[Word embeddings](https://developers.google.com/machine-learning/crash-course/embeddings/translating-to-a-lower-dimensional-space?hl=fr),
    developers.google.com'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[è¯åµŒå…¥](https://developers.google.com/machine-learning/crash-course/embeddings/translating-to-a-lower-dimensional-space?hl=fr)ï¼Œdevelopers.google.com'
- en: Cat picture, [Karsten Winegeart](https://unsplash.com/fr/photos/bulldog-francese-marrone-che-indossa-una-camicia-gialla-5PVXkqt2s9k),
    Unsplash
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: çŒ«çš„ç…§ç‰‡ï¼Œ[Karsten Winegeart](https://unsplash.com/fr/photos/bulldog-francese-marrone-che-indossa-una-camicia-gialla-5PVXkqt2s9k)ï¼ŒUnsplash
- en: Norway landscape, [Pascal Debrunner](https://unsplash.com/fr/photos/corpo-de-agua-perto-da-montanha-LKOuYT5_dyw),
    Unsplash
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æŒªå¨é£æ™¯ï¼Œ[Pascal Debrunner](https://unsplash.com/fr/photos/corpo-de-agua-perto-da-montanha-LKOuYT5_dyw)ï¼ŒUnsplash
- en: Dog picture, [Loan](https://unsplash.com/fr/photos/chaton-tabby-argente-sur-marbre-7AIDE8PrvA0),
    Unsplash
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç‹—çš„ç…§ç‰‡ï¼Œ[Loan](https://unsplash.com/fr/photos/chaton-tabby-argente-sur-marbre-7AIDE8PrvA0)ï¼ŒUnsplash
