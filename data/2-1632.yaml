- en: Paper Explained — High-Resolution Image Synthesis with Latent Diffusion Models
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 论文解读——《基于潜在扩散模型的高分辨率图像合成》
- en: 原文：[https://towardsdatascience.com/paper-explained-high-resolution-image-synthesis-with-latent-diffusion-models-f372f7636d42](https://towardsdatascience.com/paper-explained-high-resolution-image-synthesis-with-latent-diffusion-models-f372f7636d42)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/paper-explained-high-resolution-image-synthesis-with-latent-diffusion-models-f372f7636d42](https://towardsdatascience.com/paper-explained-high-resolution-image-synthesis-with-latent-diffusion-models-f372f7636d42)
- en: While OpenAI has dominated the field of natural language processing with their
    generative text models, their image generation counterpart, DALL·E, now faces
    a viable open-source competitor in Stable Diffusion. This article delves into
    the Latent Diffusion paper, upon which Stable Diffusion is based
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 虽然 OpenAI 在自然语言处理领域通过其生成文本模型占据了主导地位，但其图像生成模型 DALL·E 现在面临一个有效的开源竞争者——Stable Diffusion。本文将深入探讨基于
    Stable Diffusion 的潜在扩散论文。
- en: '[](https://mnslarcher.medium.com/?source=post_page-----f372f7636d42--------------------------------)[![Mario
    Larcher](../Images/b5b443807fe06f096ed4fc5139b3cb42.png)](https://mnslarcher.medium.com/?source=post_page-----f372f7636d42--------------------------------)[](https://towardsdatascience.com/?source=post_page-----f372f7636d42--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----f372f7636d42--------------------------------)
    [Mario Larcher](https://mnslarcher.medium.com/?source=post_page-----f372f7636d42--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://mnslarcher.medium.com/?source=post_page-----f372f7636d42--------------------------------)[![Mario
    Larcher](../Images/b5b443807fe06f096ed4fc5139b3cb42.png)](https://mnslarcher.medium.com/?source=post_page-----f372f7636d42--------------------------------)[](https://towardsdatascience.com/?source=post_page-----f372f7636d42--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----f372f7636d42--------------------------------)
    [Mario Larcher](https://mnslarcher.medium.com/?source=post_page-----f372f7636d42--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----f372f7636d42--------------------------------)
    ·10 min read·Mar 30, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表在[《数据科学前沿》](https://towardsdatascience.com/?source=post_page-----f372f7636d42--------------------------------)
    ·10 分钟阅读·2023年3月30日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/d785dcaef1afb14a4814109f8991004c.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d785dcaef1afb14a4814109f8991004c.png)'
- en: Part of Fig. 13\. from [High-Resolution Image Synthesis with Latent Diffusion
    Models](https://arxiv.org/abs/2112.10752), generated with the prompt “An oil painting
    of a latent space”.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13 的一部分来自于[《基于潜在扩散模型的高分辨率图像合成》](https://arxiv.org/abs/2112.10752)，生成的提示是“潜在空间的油画”。
- en: Introduction
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 引言
- en: As I write this article, OpenAI’s chatbot, ChatGPT, continues to gain traction
    with its integration into Microsoft products used by over a billion people. While
    Google has recently launched its own AI assistant, Bard, and other companies are
    also making advancements in the field, OpenAI continues to remain at the forefront
    with no clear contender in sight. One might assume that OpenAI’s DALL·E**, t**he
    **generative model for images**, would be similarly dominant in the field of conditional
    and non-conditional image generation. However, it’s actually an open-source alternative,
    **Stable Diffusion**, that’**s** **taking the lead in popularity and innovation**.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在我撰写这篇文章时，OpenAI 的聊天机器人 ChatGPT 正在通过与微软产品的整合继续获得关注，这些产品被超过十亿人使用。尽管谷歌最近推出了自己的
    AI 助手 Bard，其他公司也在该领域取得了进展，但 OpenAI 依然处于前沿，没有明显的竞争对手。有人可能认为，OpenAI 的 **DALL·E**，即**图像生成模型**，在条件和非条件图像生成领域也会同样主导。然而，实际上是一个开源替代品——**Stable
    Diffusion**，在**受欢迎度和创新性方面领先**。
- en: '**This article delves deep into the scientific paper behind Stable Diffusion**,
    aiming to provide a clear and comprehensive understanding of the model that’s
    revolutionizing the world of image generation. While other articles provide high-level
    explanations of the technology, this piece goes beyond the surface to explore
    often overlooked details.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '**本文深入探讨了 Stable Diffusion 背后的科学论文**，旨在提供对这一正在革新图像生成领域的模型的清晰而全面的理解。虽然其他文章提供了该技术的高层次解释，但本文超越了表面，探索了经常被忽视的细节。'
- en: Limitations of Previous Approaches in Image Generation
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 之前图像生成方法的局限性
- en: Before delving into the methodology presented in the scientific paper [High-Resolution
    Image Synthesis with Latent Diffusion Models](https://arxiv.org/abs/2112.10752),
    it’s essential to understand the key issues this work addresses.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入分析科学论文[《基于潜在扩散模型的高分辨率图像合成》](https://arxiv.org/abs/2112.10752)中的方法之前，了解该工作所解决的关键问题是至关重要的。
- en: 'Over the years, image generation has been tackled mainly through **four families
    of models**: Generative Adversarial Networks (GANs), Variational Autoencoders
    (VAEs), Autoregressive Models (ARMs), and more recently, Diffusion Probabilistic
    Models (DMs).'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 多年来，图像生成主要通过**四类模型**来解决：生成对抗网络（GANs）、变分自编码器（VAEs）、自回归模型（ARMs），以及最近的扩散概率模型（DMs）。
- en: Generative Adversarial Networks (GANs)
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 生成对抗网络（GANs）
- en: Since their first appearance in 2014, Generative Adversarial Networks (GANs)
    have been one of the dominant approaches for image generation. While GANs show
    promising results for **data with limited variability**, they come with several
    issues. The most well-known issue is **mode-collapse**, where the generator produces
    a limited range of outputs instead of a diverse set of images
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 自2014年首次出现以来，生成对抗网络（GANs）一直是图像生成领域的主流方法之一。尽管GANs在**数据变异性有限**的情况下显示出有前景的结果，但它们也存在一些问题。最著名的问题是**模式崩溃**，在这种情况下，生成器生成的输出范围有限，而不是多样化的图像集。
- en: '**Mode collapse**: this phenomenon occurs when the generator can alternately
    generate a limited number of outputs that fool the discriminator. In general,
    GANs struggle capturing the full data distribution.'
  id: totrans-16
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**模式崩溃**：这种现象发生在生成器可以交替生成有限数量的输出，来欺骗判别器。通常，GANs在捕捉完整数据分布方面存在困难。'
- en: and, in general, their **training** **is** often **unstable**.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 一般而言，它们的**训练****常常**是**不稳定的**。
- en: Variational Autoencoders (VAEs)
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 变分自编码器（VAEs）
- en: Variational Autoencoders (VAEs) are an alternative to GANs that offer several
    advantages. They do not suffer from mode-collapse and can efficiently generate
    high-resolution images. However, their **sample quality** is **not** always **comparable
    to** that of **GANs**.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 变分自编码器（VAEs）是GANs的替代方案，提供了几个优势。它们不会遭遇模式崩溃，并且可以高效地生成高分辨率图像。然而，它们的**样本质量****不**总是**可比于**
    **GANs**。
- en: Autoregressive Models (ARMs)
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 自回归模型（ARMs）
- en: Autoregressive Models (ARMs) are excellent at density estimation and have achieved
    remarkable performance in this area. However, their **computationally demanding**
    architectures and sequential sampling process **limit** them **to** generating
    **low-resolution images**.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 自回归模型（ARMs）在密度估计方面表现出色，并在这一领域取得了显著成果。然而，它们**计算需求高**的架构和顺序采样过程**限制**了它们**仅能生成**
    **低分辨率图像**。
- en: Diffusion Probabilistic Models (DMs)
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 扩散概率模型（DMs）
- en: Diffusion models have made significant progress in density estimation and sample
    quality, but their operation in **pixel space** by adding or removing noise to
    a tensor of the same size as the original image results in **slow inference speed**
    and **high computational cost**. For instance, even a relatively small image such
    as an RGB image of size 512x512 corresponds to a tensor of around 800,000 values,
    which makes the generation of larger images computationally demanding during both
    training for gradient propagation and inference for the iterative approach used
    in generation.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 扩散模型在密度估计和样本质量方面取得了显著进展，但它们在**像素空间**中通过向与原始图像大小相同的张量添加或去除噪声来操作，导致了**推断速度缓慢**和**计算成本高**。例如，即使是相对较小的图像，如尺寸为512x512的RGB图像，也对应于大约80万个值的张量，这使得在训练过程中进行梯度传播和在生成过程中进行迭代的方法时，生成更大图像的计算需求也非常高。
- en: Conditioning Mechanism
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 条件机制
- en: Generating images based on textual description or the style of another image
    is often desirable, but **conditioning** the result on one or more inputs **has
    been a challenge in previous approaches**.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 基于文本描述或其他图像风格生成图像通常是期望的，但**将结果条件化到一个或多个输入** **在之前的方法中一直是一个挑战**。
- en: High-Level Overview of Latent Diffusion
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 潜在扩散的高级概述
- en: '![](../Images/baf080474153942cd07ba5c75fce0a5a.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/baf080474153942cd07ba5c75fce0a5a.png)'
- en: Fig. 3\. from [High-Resolution Image Synthesis with Latent Diffusion Models](https://arxiv.org/abs/2112.10752).
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 图3. 来自 [高分辨率图像合成与潜在扩散模型](https://arxiv.org/abs/2112.10752)。
- en: 'To summarize the approach proposed by the scientific paper High-Resolution
    Image Synthesis with Latent Diffusion Models, we can break it down into **four
    main steps**:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 总结科学论文《高分辨率图像合成与潜在扩散模型》中提出的方法，我们可以将其分解为**四个主要步骤**：
- en: The first step is to **extract** **a** more **compact representation of the
    image** using the encoder *E* located in the upper left corner of the figure above.
    Unlike other methods, **latent diffusion works in the latent space** defined by
    the encoder, **rather than in pixel space**.
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 第一步是使用位于图上左上角的编码器 *E* **提取** **图像的** 更**紧凑的表示**。与其他方法不同，**潜在扩散在编码器定义的潜在空间中工作**，**而不是在像素空间中**。
- en: Next, **Gaussian noise is added** to the image in the upper middle part of the
    figure, as part of the **diffusion process** that goes from *z* to *zT* (in case
    *T* steps of noise addition are applied).
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，将**高斯噪声添加**到图中上中部的图像中，作为从 *z* 到 *zT* 的**扩散过程**的一部分（假设应用了 *T* 步噪声添加）。
- en: The *zT* representation is then passed through a **U-Net** located in the middle
    part at the bottom of the figure. The U-Net has the role of **predicting *zT-1***,
    and this process is repeated ***T-1* times until we arrive at *z***, which is
    **then returned from latent space to pixel space** via the decoder *D*.
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，将 *zT* 表示通过位于图中下部中央的**U-Net**。U-Net 的作用是**预测 *zT-1***，这个过程重复 ***T-1* 次，直到我们得到
    *z***，然后通过解码器 *D* **将其从潜在空间返回到像素空间**。
- en: Finally, the approach allows for **arbitrary conditioning** by mapping various
    input modalities such as semantic maps or text. This is achieved by first **transforming
    the input** *y* **with a dedicated encoder** *τθ* and **then** **mapping it to
    the intermediate layers of the U-Net with** the same **cross-attention layer**
    used by the Transformer architecture.
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，该方法允许通过映射各种输入模态（如语义图或文本）进行**任意条件**。这通过首先**使用专用编码器** *τθ* **转换输入** *y* **，然后**
    **将其映射到 U-Net 的中间层，使用与 Transformer 架构相同的** **交叉注意力层** 实现。
- en: With this general overview, we can now take a closer look at each of these steps
    in more detail.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这个总体概述，我们现在可以更详细地深入了解这些步骤。
- en: Perceptual Image Compression
  id: totrans-35
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 感知图像压缩
- en: Latent Diffusion explicitly **separates the image compression** phase to remove
    high frequency details (**perceptual compression**), **from the generation phase**
    where the model learns a semantic and conceptual composition of the data (**semantic
    compression**).
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 潜在扩散明确**将图像压缩**阶段分离，以去除高频细节（**感知压缩**），**与生成阶段**分开，在生成阶段模型学习数据的语义和概念组成（**语义压缩**）。
- en: Objective Function
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 目标函数
- en: To train the **autoencoder** used **for image compression** the authors follow
    the same approach used by **VQGAN** presented in [Taming Transformers for High-Resolution
    Image Synthesis](https://arxiv.org/abs/2012.09841).
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 为了训练用于**图像压缩**的**自编码器**，作者采用了 [《驯化变压器用于高分辨率图像合成》](https://arxiv.org/abs/2012.09841)中**VQGAN**使用的方法。
- en: '![](../Images/36087abbfb238a8f1cf0bb53fd6aac3d.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/36087abbfb238a8f1cf0bb53fd6aac3d.png)'
- en: Fig. 2\. from [Taming Transformers for High-Resolution Image Synthesis](https://arxiv.org/abs/2012.09841).
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 图2\. 来源于 [《驯化变压器用于高分辨率图像合成》](https://arxiv.org/abs/2012.09841)。
- en: 'In particular, the **objective function** to train the autoencoding model (*E*,
    *D*) is:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 特别是，用于训练自编码模型 (*E*, *D*) 的**目标函数**是：
- en: '![](../Images/65ba9d0867e19f835d8a4156a6f42189.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/65ba9d0867e19f835d8a4156a6f42189.png)'
- en: Eq. 25\. from [High-Resolution Image Synthesis with Latent Diffusion Models](https://arxiv.org/abs/2112.10752).
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 方程25\. 来源于 [《基于潜在扩散模型的高分辨率图像合成》](https://arxiv.org/abs/2112.10752)。
- en: Let’s define *x^* as the **reconstructed image** *D(E(x)), Lrec* is a **reconstruction
    loss** (squared error between *x* and *x^*), *Ladv* is the **adversarial loss**
    defined as log*(1 - Dψ(x^)), Dψ* is a **patch-based discriminator** optimized
    to differentiate original images from reconstructions *x^* (so *Dψ(x)* try to
    output 1 for the real image *x* and 0 for the reconstructed “fake” image *x^),*
    and *Lreg* is a **regularization loss**.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 设 *x^* 为 **重建图像** *D(E(x))，Lrec* 是**重建损失**（*x* 和 *x^* 之间的平方误差），*Ladv* 是**对抗损失**，定义为
    log*(1 - Dψ(x^))，Dψ* 是一个**基于补丁的判别器**，优化以区分原始图像和重建的“伪造”图像 *x^*（所以 *Dψ(x)* 试图对真实图像
    *x* 输出 1，对重建的“伪造”图像 *x^* 输出 0），*，*Lreg* 是**正则化损失**。
- en: Regularization
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 正则化
- en: The authors experiment with two different **methods of regularization**.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 作者实验了两种不同的**正则化方法**。
- en: The first approach involves a low-weighted **Kullback-Leibler** term, similar
    to standard VAEs.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 第一种方法涉及低权重的**Kullback-Leibler**项，类似于标准的VAE。
- en: '**Kullback-Leibler (KL) Penality**: Kullback-Leibler divergence is a type of
    statistical distance between two distributions. In this case, the idea is to make
    the distribution of the latent variable z ~ N(Eµ , Eσ^2) close to that of a standard
    normal distribution N(0, 1). Imposing this constraint regularizes the latent space
    by concentrating it more, so, for example, if z lies close to z1 and z2 then D(z)
    will itself have something in common both with D(z1) and D(z2).'
  id: totrans-48
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**Kullback-Leibler (KL) 惩罚**：Kullback-Leibler 散度是两种分布之间的一种统计距离。在这种情况下，目标是使潜在变量
    z ~ N(Eµ , Eσ^2) 的分布接近标准正态分布 N(0, 1)。施加这个约束可以通过使潜在空间更加集中来正则化它，因此，例如，如果 z 接近于 z1
    和 z2，则 D(z) 将与 D(z1) 和 D(z2) 有某种共同之处。'
- en: In the second approach, the latent space is regularized with a **vector quantization**
    layer.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在第二种方法中，潜在空间通过 **向量量化** 层进行正则化。
- en: '**Vector Quantization (VQ)**: VQ is the approach used by VQVAE presented in
    the scientific paper [Neural Discrete Representation Learning](https://arxiv.org/abs/1711.00937)
    and the already mentioned VQGAN. As visible from the above image, for each spatial
    position of the encoder output z^, the corresponding vector (whose size depends
    on the number of channels of z^) is replaced with the vector closest to it in
    a learnable “codebook”. This then limits the decoder’s possible inputs during
    inference, which can only be combinations of codebook vectors (a discretization
    or quantization of the latent space).'
  id: totrans-50
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**向量量化 (VQ)**：VQ 是科学论文 [Neural Discrete Representation Learning](https://arxiv.org/abs/1711.00937)
    中介绍的 VQVAE 以及之前提到的 VQGAN 所使用的方法。从上面的图像可以看出，对于编码器输出 z^ 的每个空间位置，对应的向量（其大小取决于 z^
    的通道数量）会被替换为在可学习的“代码本”中最接近的向量。这会限制解码器在推理过程中可能的输入，只能是代码本向量的组合（即潜在空间的离散化或量化）。'
- en: In the case of VQ-regularized latent space, *z* is extracted before the quantization
    layer and absorbs the quantization operation in the decoder, i.e., it can be interpreted
    as the first layer of *D*.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在 VQ 正则化的潜在空间中，*z* 在量化层之前被提取，并在解码器中吸收量化操作，即它可以被解释为 *D* 的第一层。
- en: Diffusion Models
  id: totrans-52
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 扩散模型
- en: '![](../Images/faa0d6d6a757ff4e8a25b32a180e67b4.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/faa0d6d6a757ff4e8a25b32a180e67b4.png)'
- en: Fig. 2\. from [Denoising Diffusion Probabilistic Models](https://arxiv.org/abs/2006.11239).
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2\. 来自 [Denoising Diffusion Probabilistic Models](https://arxiv.org/abs/2006.11239)。
- en: 'Since this article deals with Latent Diffusion and not diffusion models in
    general, I will only describe the most important aspects of them. First, let us
    distinguish between **two processes**: **forward** and **reverse**.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这篇文章讨论的是潜在扩散模型而不是一般的扩散模型，我将只描述它们最重要的方面。首先，让我们区分 **两个过程**：**正向**和**反向**。
- en: Forward Process
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 正向过程
- en: 'The **forward** or **diffusion process**, the one going from right to left
    in the figure, is a **Markov chain**, that is, the image at time *t* depends only
    on the one at time *t-1* and not on all the previous ones. At each step, *xt*
    is sampled according to the following **transition probability**:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '**正向**或 **扩散过程**，即图中从右向左的过程，是一个 **马尔可夫链**，即时间 *t* 的图像仅依赖于时间 *t-1* 的图像，而不是所有之前的图像。每一步，*xt*
    都根据以下 **转移概率** 进行采样：'
- en: '![](../Images/7c4ed9dc267be368695b8a20596787cd.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7c4ed9dc267be368695b8a20596787cd.png)'
- en: Eq. 2\. from [Denoising Diffusion Probabilistic Models](https://arxiv.org/abs/2006.11239).
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: Eq. 2\. 来自 [Denoising Diffusion Probabilistic Models](https://arxiv.org/abs/2006.11239)。
- en: In the formula above the *βt* define a **variance schedule** and can be either
    learned or held constant by treating them as hyperparameters. An interesting property
    of forward processing is also that it is possible to sample *xt* in closed form
    for an arbitrary timestep *t.* Using the notation
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在上面的公式中，*βt* 定义了一个 **方差调度**，可以通过将其视为超参数来学习或保持不变。正向处理的一个有趣特性是可以在任意时间步 *t* 以封闭形式对
    *xt* 进行采样。使用符号
- en: '![](../Images/d59f98809d5643e369d02bfab256820d.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d59f98809d5643e369d02bfab256820d.png)'
- en: and
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 和
- en: '![](../Images/7b383c89f0b932d281641461f593429f.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7b383c89f0b932d281641461f593429f.png)'
- en: we have
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有
- en: '![](../Images/cff2dd68f67cfa79d8d3a283d4632b67.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/cff2dd68f67cfa79d8d3a283d4632b67.png)'
- en: Eq. 4\. from [Denoising Diffusion Probabilistic Models](https://arxiv.org/abs/2006.11239).
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: Eq. 4\. 来自 [Denoising Diffusion Probabilistic Models](https://arxiv.org/abs/2006.11239)。
- en: So, to recap, in the forward process we can get the image at any time *t* by
    sampling from a Gaussian distribution with mean and variance defined by the formula
    above.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，总结一下，在正向过程中，我们可以通过从均值和方差由上述公式定义的高斯分布中采样来获取任意时间 *t* 的图像。
- en: Reverse Process
  id: totrans-68
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 反向过程
- en: 'Given the forward process, the **reverse** **process** also follows a Gaussian
    distribution:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 给定正向过程，**逆向** **过程**也遵循高斯分布：
- en: '![](../Images/8ef1c0a4ffdb90e997663083cd14a0ea.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8ef1c0a4ffdb90e997663083cd14a0ea.png)'
- en: As for the variance, the authors set it at
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 至于方差，作者将其设置为
- en: '![](../Images/ffc4ad852c04f2e52e197f02bae6328a.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ffc4ad852c04f2e52e197f02bae6328a.png)'
- en: where they note experimentally that both
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 他们通过实验指出，两者
- en: '![](../Images/24d37d5b83df5cfdaa3d5a43d18c987f.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/24d37d5b83df5cfdaa3d5a43d18c987f.png)'
- en: and
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 和
- en: '![](../Images/04d5b148bea15a9c17552e150c2eecda.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/04d5b148bea15a9c17552e150c2eecda.png)'
- en: bring equivalent results.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 产生等效的结果。
- en: 'Before seeing how the mean is parameterized, let us re-parameterize eq. 4\.
    of the forward process:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在看到均值的参数化之前，让我们重新参数化等式 4 的正向过程：
- en: '![](../Images/216defb18e4a7ceeb54cc9d48a43d58a.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/216defb18e4a7ceeb54cc9d48a43d58a.png)'
- en: At this point we parameterize the mean as
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 此时我们将均值参数化为
- en: '![](../Images/b86ba58acb8f64f60a036bc3c6d0cc5e.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b86ba58acb8f64f60a036bc3c6d0cc5e.png)'
- en: where *ϵθ* is an estimator of *ϵ* from *xt*, specifically it is a variant of
    a **time-conditional U-Net**.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 其中*ϵθ*是*ϵ*的估计量，来自*xt*，具体来说，它是**时间条件 U-Net**的一个变体。
- en: At this point we have all the elements to sample *xt-1* conditioned to *xt,*
    considering that we know all the parameters of the Gaussian distribution introduced
    at the beginning of the description of the reverse process.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，我们已经具备了采样*xt-1*条件于*xt*所需的所有元素，考虑到我们知道逆过程描述开始时引入的高斯分布的所有参数。
- en: 'Without entering in the mathematical details, the (simplified) objective to
    be minimized is:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 不进入数学细节，目标（简化版）是：
- en: '![](../Images/04ebc69032e1a051a4bee9700eb3efea.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/04ebc69032e1a051a4bee9700eb3efea.png)'
- en: Eq. 1\. from [High-Resolution Image Synthesis with Latent Diffusion Models](https://arxiv.org/abs/2112.10752).
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 等式 1 来自于 [高分辨率图像合成与潜在扩散模型](https://arxiv.org/abs/2112.10752)。
- en: with *t* uniformly sampled from {1, . . . , *T*}.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 其中*t* 从 {1, . . . , *T*} 中均匀采样。
- en: Generative Modeling of Latent Representations
  id: totrans-88
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 潜在表示的生成建模
- en: As already noted, Latent Diffusion works like a diffusion model, similar to
    the one explained earlier. However, it differs in that it **starts from the latent
    representation** *z* of the image obtained through an encoder (**latent space**),
    rather than from the image *x* (**pixel space**). This detail greatly **reduces
    the computational burden**, as the latent space is more compact than pixel space.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，Latent Diffusion 的工作方式类似于前面解释的扩散模型。然而，它的不同之处在于它**从通过编码器获得的图像的潜在表示** *z*
    开始（**潜在空间**），而不是从图像 *x*（**像素空间**）开始。这一细节大大**减少了计算负担**，因为潜在空间比像素空间更紧凑。
- en: 'Given this, replacing *xt* with *zt* in the diffusion model objective, we have
    the new objective:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 基于此，将*xt*替换为扩散模型目标中的*zt*，我们得到新的目标：
- en: '![](../Images/128c6ae0e204212c088d863721225ea1.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/128c6ae0e204212c088d863721225ea1.png)'
- en: Eq. 2\. from [High-Resolution Image Synthesis with Latent Diffusion Models](https://arxiv.org/abs/2112.10752).
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 等式 2 来自于 [高分辨率图像合成与潜在扩散模型](https://arxiv.org/abs/2112.10752)。
- en: Conditioning Mechanisms
  id: totrans-93
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 条件机制
- en: Before this study, there was limited exploration on **how to condition diffusion
    models** with inputs beyond a class label or a blurred version of the input image.
    The proposed approach by Latent Diffusion is highly versatile and involves **integrating**
    **additional information directly into the intermediate layers of the U-Net model
    using cross-attention**, which is similar to the Transformer architecture.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在这项研究之前，对于**如何通过输入条件扩散模型**的探索是有限的，输入包括类别标签或模糊版本的输入图像。Latent Diffusion 提出的方案具有很高的通用性，涉及**将**
    **附加信息直接集成到 U-Net 模型的中间层中，使用交叉注意力**，这类似于 Transformer 架构。
- en: 'To be more specific, the input information (such as text) is first converted
    into an intermediate representation through a **domain-specific encoder** called
    *τθ* (an example will be provided later). This representation is then passed through
    a **cross-attention layer** and added to intermediate layers of the U-Net:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 更具体地说，输入信息（如文本）首先通过一个**领域特定的编码器** *τθ* 转换为中间表示（稍后会提供示例）。然后，将该表示传递通过**交叉注意力层**并添加到
    U-Net 的中间层：
- en: '![](../Images/1f1ee9ede933ad9b89abd8cfa94f6f5e.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1f1ee9ede933ad9b89abd8cfa94f6f5e.png)'
- en: with
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 与
- en: '![](../Images/4bb4e864963d2be962e19c62919f7178.png)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4bb4e864963d2be962e19c62919f7178.png)'
- en: 'In the equation, *ϕi(zt)* represents the flattened intermediate representation
    of the U-Net, and the *W*s are trainable projection matrices. Although the paper
    does not elaborate on this, the code implementation reveals that **the output
    of the cross-attention layer is added to the original U-Net layer**. This can
    be seen in the following code snippet:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 在方程中，*ϕi(zt)*表示U-Net的扁平化中间表示，*W*s是可训练的投影矩阵。尽管论文没有详细说明，代码实现却揭示了**交叉注意力层的输出被加到原始U-Net层**中。这可以在以下代码片段中看到：
- en: x = self.attn2(self.norm2(x), context=context) + x
  id: totrans-100
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: x = self.attn2(self.norm2(x), context=context) + x
- en: Here, attn2 denotes the cross-attention layer, while the context refers to *τθ(y)*.
    Although the full implementation of this process is more complex, this is the
    crucial conceptual element. For a more in-depth understanding of this mechanism,
    please refer to the [BasicTransformerBlock](https://github.com/CompVis/latent-diffusion/blob/e66308c7f2e64cb581c6d27ab6fbeb846828253b/ldm/modules/attention.py#L196)
    that is utilized in the model.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，attn2 表示交叉注意力层，而上下文指的是*τθ(y)*。虽然这一过程的完整实现更为复杂，但这是关键的概念元素。欲深入了解这一机制，请参考[BasicTransformerBlock](https://github.com/CompVis/latent-diffusion/blob/e66308c7f2e64cb581c6d27ab6fbeb846828253b/ldm/modules/attention.py#L196)模型中使用的模块。
- en: Experiments
  id: totrans-102
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实验
- en: '![](../Images/67fe792598f9aa7768199863e966a618.png)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/67fe792598f9aa7768199863e966a618.png)'
- en: Fig. 5\. from [High-Resolution Image Synthesis with Latent Diffusion Models](https://arxiv.org/abs/2112.10752).
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5\. 来自 [High-Resolution Image Synthesis with Latent Diffusion Models](https://arxiv.org/abs/2112.10752)。
- en: The paper conducts numerous experiments to explore various methods of image
    generation, including unconditional generation, layout-to-image synthesis, spatial
    conditioning, super-resolution, inpainting, and more. To further highlight two
    important aspects of Latent Diffusion, we will focus on the well-known task of
    **text-to-image**.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 论文进行大量实验探讨了图像生成的各种方法，包括无条件生成、布局到图像合成、空间条件、超分辨率、修补等。为了进一步突出 Latent Diffusion
    的两个重要方面，我们将重点关注**文本到图像**这一广为人知的任务。
- en: The first crucial aspect to consider is **how to convert text** into a representation
    that can be passed to the cross-attention layer. The authors use the **BERT-tokenizer**
    and implement *τθ* as a **Transformer** to achieve this.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 首要考虑的关键方面是**如何将文本转换**为可以传递给交叉注意力层的表示。作者使用**BERT-tokenizer**并将*τθ*实现为**Transformer**以达到这一目标。
- en: The second important aspect is how much the input image should be compressed
    through the encoder. The authors experiment with different **downsampling factors**
    *f* ∈ {1, 2, 4, 8, 16, 32}, and they conclude that 4 and 8 offer the best conditions
    for achieving high-quality synthesis results. The results shown above were obtained
    using LDM-8 (KL).
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个重要方面是通过编码器对输入图像进行压缩的程度。作者尝试了不同的**下采样因子** *f* ∈ {1, 2, 4, 8, 16, 32}，并得出结论认为
    4 和 8 提供了最佳的高质量合成结果条件。上述结果是使用 LDM-8 (KL) 获得的。
- en: Conclusion
  id: totrans-108
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: Latent Diffusion and the successive works inspired by this paper have produced
    astonishing results that were once considered unimaginable. Today, **these models**
    are no longer confined to research labs but **are being integrated into popular
    products** such as Adobe Photoshop. This development marks a significant milestone
    in the field of artificial intelligence and demonstrates its potential to impact
    various aspects of our lives.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: Latent Diffusion 及其后续工作的灵感来源于这篇论文，已取得令人惊讶的成果，这些成果曾被认为难以想象。如今，**这些模型**不再局限于研究实验室，而是**被集成到流行产品中**，如
    Adobe Photoshop。这一发展标志着人工智能领域的一个重要里程碑，并展示了它对我们生活各个方面的潜在影响。
- en: However, despite the remarkable progress made in this area, there are still
    several challenges that need to be addressed. These challenges encompass **copyright
    issues** pertaining to the usage of images for training AI models and the innate
    **biases** that surface when large datasets are crawled from the internet. However,
    notwithstanding these limitations, the potential of AI to **democratize creativity**
    and enable individuals to express themselves in novel and captivating ways is
    too significant to ignore.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，尽管在这一领域取得了显著进展，但仍然存在一些需要解决的挑战。这些挑战包括**版权问题**，涉及使用图像训练 AI 模型，以及在从互联网爬取大型数据集时出现的固有**偏见**。然而，尽管存在这些限制，AI
    有能力**民主化创造力**并使个人能够以新颖和引人入胜的方式表达自己，这一潜力不容忽视。
- en: Thank you for taking the time to read this article, and please feel free to
    leave a comment or connect with me to share your thoughts or ask any questions.
    To stay updated on my latest articles, you can follow me on [Medium](https://medium.com/@mnslarcher),
    [LinkedIn](https://www.linkedin.com/in/mnslarcher/) or [Twitter](https://twitter.com/mnslarcher).
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 感谢你花时间阅读这篇文章，欢迎留下评论或与我联系，分享你的想法或提出任何问题。要及时了解我最新的文章，你可以在[Medium](https://medium.com/@mnslarcher)、[LinkedIn](https://www.linkedin.com/in/mnslarcher/)或[Twitter](https://twitter.com/mnslarcher)上关注我。
- en: '[](https://medium.com/@mnslarcher/membership?source=post_page-----f372f7636d42--------------------------------)
    [## Join Medium with my referral link - Mario Namtao Shianti Larcher'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@mnslarcher/membership?source=post_page-----f372f7636d42--------------------------------)
    [## 通过我的推荐链接加入Medium - Mario Namtao Shianti Larcher'
- en: Read every story from Mario Namtao Shianti Larcher (and thousands of other writers
    on Medium). Your membership fee…
  id: totrans-113
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 阅读Mario Namtao Shianti Larcher（以及Medium上其他成千上万的作者）的每一个故事。你的会员费…
- en: medium.com](https://medium.com/@mnslarcher/membership?source=post_page-----f372f7636d42--------------------------------)
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '[medium.com](https://medium.com/@mnslarcher/membership?source=post_page-----f372f7636d42--------------------------------)'
