- en: 'Transformer Models 101: Getting Started — Part 1'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Transformer 模型 101：入门 — 第 1 部分
- en: 原文：[https://towardsdatascience.com/transformer-models-101-getting-started-part-1-b3a77ccfa14d?source=collection_archive---------1-----------------------#2023-02-18](https://towardsdatascience.com/transformer-models-101-getting-started-part-1-b3a77ccfa14d?source=collection_archive---------1-----------------------#2023-02-18)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/transformer-models-101-getting-started-part-1-b3a77ccfa14d?source=collection_archive---------1-----------------------#2023-02-18](https://towardsdatascience.com/transformer-models-101-getting-started-part-1-b3a77ccfa14d?source=collection_archive---------1-----------------------#2023-02-18)
- en: The complex math behind transformer models, in simple words
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 用简单的语言解释 Transformer 模型背后的复杂数学
- en: '[](https://nandinibansal1811.medium.com/?source=post_page-----b3a77ccfa14d--------------------------------)[![Nandini
    Bansal](../Images/a813ac8be6f89b2e856651fda66ab078.png)](https://nandinibansal1811.medium.com/?source=post_page-----b3a77ccfa14d--------------------------------)[](https://towardsdatascience.com/?source=post_page-----b3a77ccfa14d--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----b3a77ccfa14d--------------------------------)
    [Nandini Bansal](https://nandinibansal1811.medium.com/?source=post_page-----b3a77ccfa14d--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://nandinibansal1811.medium.com/?source=post_page-----b3a77ccfa14d--------------------------------)[![Nandini
    Bansal](../Images/a813ac8be6f89b2e856651fda66ab078.png)](https://nandinibansal1811.medium.com/?source=post_page-----b3a77ccfa14d--------------------------------)[](https://towardsdatascience.com/?source=post_page-----b3a77ccfa14d--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----b3a77ccfa14d--------------------------------)
    [Nandini Bansal](https://nandinibansal1811.medium.com/?source=post_page-----b3a77ccfa14d--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fc41c26af0465&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransformer-models-101-getting-started-part-1-b3a77ccfa14d&user=Nandini+Bansal&userId=c41c26af0465&source=post_page-c41c26af0465----b3a77ccfa14d---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----b3a77ccfa14d--------------------------------)
    ·11 min read·Feb 18, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fb3a77ccfa14d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransformer-models-101-getting-started-part-1-b3a77ccfa14d&user=Nandini+Bansal&userId=c41c26af0465&source=-----b3a77ccfa14d---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fc41c26af0465&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransformer-models-101-getting-started-part-1-b3a77ccfa14d&user=Nandini+Bansal&userId=c41c26af0465&source=post_page-c41c26af0465----b3a77ccfa14d---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----b3a77ccfa14d--------------------------------)
    ·11分钟阅读·2023年2月18日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fb3a77ccfa14d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransformer-models-101-getting-started-part-1-b3a77ccfa14d&user=Nandini+Bansal&userId=c41c26af0465&source=-----b3a77ccfa14d---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fb3a77ccfa14d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransformer-models-101-getting-started-part-1-b3a77ccfa14d&source=-----b3a77ccfa14d---------------------bookmark_footer-----------)![](../Images/75e375b7793cc2763bbaa654fb731228.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fb3a77ccfa14d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransformer-models-101-getting-started-part-1-b3a77ccfa14d&source=-----b3a77ccfa14d---------------------bookmark_footer-----------)![](../Images/75e375b7793cc2763bbaa654fb731228.png)'
- en: Image by [Kerttu](https://pixabay.com/users/kerttu-569708/?utm_source=link-attribution&amp%3Butm_medium=referral&amp%3Butm_campaign=image&amp%3Butm_content=1151405)
    from [Pixabay](https://pixabay.com//?utm_source=link-attribution&amp%3Butm_medium=referral&amp%3Butm_campaign=image&amp%3Butm_content=1151405)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：[Kerttu](https://pixabay.com/users/kerttu-569708/?utm_source=link-attribution&amp%3Butm_medium=referral&amp%3Butm_campaign=image&amp%3Butm_content=1151405)
    来自 [Pixabay](https://pixabay.com//?utm_source=link-attribution&amp%3Butm_medium=referral&amp%3Butm_campaign=image&amp%3Butm_content=1151405)
- en: It is no secret that transformer architecture was a breakthrough in the field
    of Natural Language Processing (NLP). It overcame the limitation of seq-to-seq
    models like RNNs, etc for being incapable of capturing long-term dependencies
    in text. The transformer architecture turned out to be the foundation stone of
    revolutionary architectures like BERT, GPT, and T5 and their variants. As many
    say, NLP is in the midst of a golden era and it wouldn’t be wrong to say that
    the transformer model is where it all started.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 变换器架构在自然语言处理（NLP）领域的突破并不是什么秘密。它克服了诸如 RNN 等序列到序列（seq-to-seq）模型在捕捉文本中的长期依赖性方面的限制。变换器架构成为了
    BERT、GPT 和 T5 及其变体等革命性架构的基石。正如许多人所说，NLP 正处于一个黄金时代，毫不夸张地说，变换器模型正是这一切的起点。
- en: Need for Transformer Architecture
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 变换器架构的必要性
- en: As said, necessity is the mother of invention. The traditional seq-to-seq models
    were no good when it came to working with long texts. ***Meaning the model tends
    to forget the learnings from the earlier parts of the input sequence as it moves
    to process the latter part of the input sequence***. This loss of information
    is undesirable.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 正如所说的，需求是发明的母亲。传统的序列到序列模型在处理长文本时表现不佳。***这意味着模型在处理输入序列的后半部分时，往往会忘记前半部分的学习内容***。这种信息丧失是不可取的。
- en: Although gated architectures like LSTMs and GRUs showed some improvement in
    performance for handling long-term dependencies by ***discarding information that
    was useless along the way to remember important information,*** it still wasn’t
    enough. The world needed something more powerful and in 2015, “attentionmechanisms”
    were introduced by [**Bahdanau et al**](https://arxiv.org/abs/1409.0473)**.**
    They were used in combination with RNN/LSTM to mimic human behaviour to focus
    on selective things while ignoring the rest. Bahdanau suggested assigning relative
    importance to each word in a sentence so that model focuses on important words
    and ignores the rest. It emerged to be a massive improvement over encoder-decoder
    models for neural machine translation tasks and soon enough, the application of
    the attention mechanism was rolled out in other tasks as well.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管像 LSTM 和 GRU 这样的门控架构通过 ***丢弃沿途无用的信息以记住重要信息***，在处理长期依赖性方面表现出了一些改进，但仍然不够。世界需要更强大的东西，于是，在2015年，[**Bahdanau
    等人**](https://arxiv.org/abs/1409.0473)**引入了“注意力机制”**。它们与 RNN/LSTM 结合使用，模仿人类行为，专注于选择性事物，同时忽略其余部分。Bahdanau
    提议为句子中的每个单词分配相对重要性，以便模型关注重要单词并忽略其他部分。这被证明是对神经机器翻译任务中的编码器-解码器模型的巨大改进，很快，注意力机制的应用也扩展到了其他任务中。
- en: The Era of Transformer Models
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**变换器模型的时代**'
- en: The transformer models are entirely based on an attention mechanism which is
    also known as ***“self-attention”***. This architecture was introduced to the
    world in the paper “[**Attention is All You Need**](https://arxiv.org/abs/1706.03762)”
    in 2017\. It consisted of an Encoder-Decoder Architecture.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 变换器模型完全基于注意力机制，也称为 ***“自注意力”***。这一架构在2017年通过论文 “[**Attention is All You Need**](https://arxiv.org/abs/1706.03762)”
    介绍给世界。它由编码器-解码器架构组成。
- en: '![](../Images/e1adf6bb56d8cb4d42606b7e2236a8d0.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e1adf6bb56d8cb4d42606b7e2236a8d0.png)'
- en: 'Fig. Transformer Model Architecture on high-level (Source: Author)'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 图. 高层次的变换器模型架构（来源：作者）
- en: On a high level,
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 从高层次来看，
- en: The ***encoder*** is responsible for accepting the input sentence and converting
    it into a hidden representation with all useless information discarded.
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***编码器*** 负责接受输入句子，并将其转换为一个隐藏表示，其中所有无用信息都被丢弃。'
- en: The ***decoder*** accepts this hidden representation and tries to generate the
    target sentence.
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***解码器*** 接受这一隐藏表示，并尝试生成目标句子。'
- en: In this article, we will delve into a detailed breakdown of the Encoder component
    of the Transformer model. In the next article, we shall look at the Decoder component
    in detail. Let’s start!
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们将详细探讨变换器模型的编码器组件。在下一篇文章中，我们将详细讨论解码器组件。让我们开始吧！
- en: Encoder of Transformer
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 变换器的编码器
- en: The encoder block of the transformer consists of a stack of N encoders that
    work sequentially. The output of one encoder is the input for the next encoder
    and so on. The output of the last encoder is the final representation of the input
    sentence that is fed to the decoder block.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 变换器的编码器块由 N 个编码器的堆叠组成，这些编码器顺序工作。一个编码器的输出是下一个编码器的输入，依此类推。最后一个编码器的输出是输入句子的最终表示，这一表示被传递给解码器块。
- en: '![](../Images/e82a5f3835bb6428ad06e633f02301be.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e82a5f3835bb6428ad06e633f02301be.png)'
- en: 'Fig. Enoder block with stacked encoders (Source: Author)'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 图：具有堆叠编码器的编码器块（来源：作者）
- en: Each encoder block can be further split into two components as shown in the
    figure below.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 每个编码器块可以进一步分为下图所示的两个组件。
- en: '![](../Images/701ef09d3f7d1158f0d332eafcbcedb2.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/701ef09d3f7d1158f0d332eafcbcedb2.png)'
- en: 'Fig. Components of Encoder Layer (Source: Author)'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 图：编码器层的组件（来源：作者）
- en: 'Let us look into each of these components one by one in detail to understand
    how the encoder block is working. The first component in the encoder block is
    ***multi-head attention*** but before we hop into the details, let us first understand
    an underlying concept: ***self-attention***.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们详细查看这些组件，以理解编码器块是如何工作的。编码器块中的第一个组件是***多头注意力***，但在深入细节之前，我们先理解一个基本概念：***自注意力***。
- en: Self-Attention Mechanism
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 自注意力机制
- en: 'The first question that might pop up in everyone’s mind: *Are attention and
    self-attention different concepts?* Yes, they are. (Duh!)'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 每个人可能会产生的第一个问题是：*注意力和自注意力是不同的概念吗？* 是的，它们不同。（显而易见！）
- en: Traditionally, the ***attention mechanisms*** came into existence for the task
    of neural machine translation as discussed in the previous section. So essentially
    the attention mechanism was applied to map the source and target sentence. As
    the seq-to-seq models perform the translation task token by token, the attention
    mechanism helps us to identify which token(s) from the source sentence to ***focus
    more on*** while generating token x for the target sentence. For this, it makes
    use of hidden state representations from encoders and decoders to calculate the
    attention scores and generate context vectors based on these scores as input for
    the decoder. If you wish to learn more about the Attention Mechanism, please hop
    on to [this article](/intuitive-understanding-of-attention-mechanism-in-deep-learning-6c9482aecf4f)
    (Brilliantly explained!).
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 传统上，***注意力机制***在前一节中讨论的神经机器翻译任务中产生。因此，本质上，注意力机制用于将源句和目标句进行映射。由于 seq-to-seq 模型逐词执行翻译任务，注意力机制帮助我们识别在为目标句生成令牌
    x 时应该***更多关注***源句中的哪些令牌。为此，它利用来自编码器和解码器的隐藏状态表示来计算注意力分数，并基于这些分数生成上下文向量作为解码器的输入。如果你想了解更多关于注意力机制的内容，请查看[这篇文章](/intuitive-understanding-of-attention-mechanism-in-deep-learning-6c9482aecf4f)（解释得非常精彩！）。
- en: Coming back to ***self-attention***, the main idea is to calculate the attention
    scores while mapping the source sentence to itself. If you have a sentence like,
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 回到***自注意力***，主要思想是在将源句映射到自身时计算注意力分数。如果你有一句话，如，
- en: '*“The boy did not cross the* ***road*** *because* ***it*** *was too wide.”*'
  id: totrans-33
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*“那个男孩没有过马路，因为* ***它*** *太宽了。”*'
- en: It is easy for us humans to understand that word “it” refers to “road” in the
    above sentence but how do we make our language model understand this relationship
    as well? That's where ***self-attention*** comes into the picture!
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 对我们人类来说，理解句子中的“it”指的是“road”很简单，但我们如何让语言模型也理解这种关系呢？这就是***自注意力***的作用！
- en: On a high level, every word in the sentence is compared against every other
    word in the sentence to quantify the relationships and understand the context.
    For representational purposes, you can refer to the figure below.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 从高层次来看，句子中的每个单词都会与句子中的其他单词进行比较，以量化关系并理解上下文。为了表示目的，你可以参考下图。
- en: Let us see in detail how this self-attention is calculated (in real).
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们详细了解一下自注意力是如何计算的（实际上）。
- en: '*Generate embeddings for the input sentence*'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*为输入句子生成嵌入*'
- en: Find embeddings of all the words and convert them into an input matrix. These
    embeddings can be generated via simple tokenisation and one-hot encoding or could
    be generated by embedding algorithms like BERT, etc. The ***dimension of the input
    matrix*** will be equal to the ***sentence length x embedding dimension***. Let
    us call this ***input matrix X*** for future reference.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 查找所有单词的嵌入并将其转换为输入矩阵。这些嵌入可以通过简单的标记化和独热编码生成，也可以通过像 BERT 等嵌入算法生成。***输入矩阵的维度***将等于***句子长度
    x 嵌入维度***。我们将其称为***输入矩阵 X***以便于将来参考。
- en: '*Transform input matrix into Q, K & V*'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*将输入矩阵转换为 Q、K 和 V*'
- en: 'For calculating self-attention, we need to transform X (input matrix) into
    three new matrices:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 为了计算自注意力，我们需要将 X（输入矩阵）转换为三个新矩阵：
- en: '- Query (Q)'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '- 查询 (Q)'
- en: '- Key (K)'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '- 关键 (K)'
- en: '- Value (V)'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '- 值 (V)'
- en: To calculate these three matrices, we will randomly initialise three weight
    matrices namely ***Wq, Wk, & Wv***. The input matrix X will be multiplied with
    these weight matrices Wq, Wk, & Wv to obtain values for Q, K & V respectively.
    The optimal values for weight matrices will be learned during the process to obtain
    more accurate values for Q, K & V.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 为了计算这三个矩阵，我们将随机初始化三个权重矩阵，即 ***Wq、Wk 和 Wv***。输入矩阵 X 将与这些权重矩阵 Wq、Wk 和 Wv 相乘，以分别获得
    Q、K 和 V 的值。权重矩阵的最优值将在过程中学习，以获得更准确的 Q、K 和 V 值。
- en: '*Calculate the dot product of Q and K-transpose*'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*计算 Q 和 K 转置的点积*'
- en: From the figure above, we can imply that qi, ki, and vi represent the values
    of Q, K, and V for the i-th word in the sentence.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 从上图中，我们可以推断 qi、ki 和 vi 代表句子中第 i 个词的 Q、K 和 V 的值。
- en: '![](../Images/67fc6dc5181747aa4895072c410b31a6.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/67fc6dc5181747aa4895072c410b31a6.png)'
- en: 'Fig. Example of dot product of Q and K-transpose (Source: Author)'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 图例：Q 和 K 转置的点积示例（来源：作者）
- en: The first row of the output matrix will tell you how word1 represented by q1
    is related to the rest of the words in the sentence using dot-product. The higher
    the value of the dot-product, the more related the words are. For intuition of
    why this dot product was calculated, you can understand Q (query) and K (key)
    matrices in terms of information retrieval. So here,
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 输出矩阵的第一行会告诉你词汇 word1 由 q1 表示的与句子中其余词汇的关系，使用点积来计算。点积值越高，词汇之间的相关性越大。为了理解为何计算这个点积，你可以从信息检索的角度理解
    Q（查询）和 K（键）矩阵。因此，
- en: '- Q or Query = Term you are searching for'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '- Q 或 Query = 你正在搜索的术语'
- en: '- K or Key = a set of keywords in your search engine against which Q is compared
    and matched.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '- K 或 Key = 搜索引擎中的一组关键词，与 Q 进行比较和匹配。'
- en: '*Scale the dot-product*'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*缩放点积*'
- en: As in the previous step, we are calculating the dot-product of two matrices
    i.e. performing a multiplication operation, there are chances that the value might
    explode. To make sure this does not happen and gradients are stabilised, we divide
    the dot product of Q and K-transpose by the square root of the embedding dimension
    (dk).
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 与之前的步骤一样，我们正在计算两个矩阵的点积，即执行乘法操作，这可能导致值的爆炸。为了确保不会发生这种情况并稳定梯度，我们将 Q 和 K 转置的点积除以嵌入维度的平方根（dk）。
- en: '*Normalise the values using softmax*'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*使用 softmax 归一化值*'
- en: Normalisation using the softmax function will result in values between 0 and
    1\. The cells with high-scaled dot-product will be heightened furthermore whereas
    low values will be reduced making the distinction between matched word pairs clearer.
    The resultant output matrix can be regarded as a *score matrix S*.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 softmax 函数的归一化将导致值在 0 和 1 之间。具有高点积的单元将被进一步提升，而低值将被减少，使匹配词对之间的区别更清晰。结果输出矩阵可以视为
    *得分矩阵 S*。
- en: '*Calculate the attention matrix Z*'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*计算注意力矩阵 Z*'
- en: The values matrix or V is multiplied by the score matrix S obtained from the
    previous step to calculate the attention matrix Z.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 值矩阵或 V 会与上一步骤中获得的得分矩阵 S 相乘，以计算注意力矩阵 Z。
- en: '*But wait, why multiply?*'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '*但等等，为什么要相乘？*'
- en: Suppose, Si = [0.9, 0.07, 0.03] is the score matrix value for i-th word from
    a sentence. This vector is multiplied with the V matrix to calculate Zi (attention
    matrix for i-th word).
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 假设 Si = [0.9, 0.07, 0.03] 是句子中第 i 个词的得分矩阵值。这个向量会与 V 矩阵相乘，以计算 Zi（第 i 个词的注意力矩阵）。
- en: '***Zi = [0.9 * V1 + 0.07 * V2 + 0.03 * V3]***'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '***Zi = [0.9 * V1 + 0.07 * V2 + 0.03 * V3]***'
- en: Can we say that for understanding the context of i-th word, we should only focus
    on word1 (i.e. V1) as 90% of the value of attention score is coming from V1? We
    could clearly define the important words where ***more attention*** should be
    paid to understand the context of i-th word.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以说，为了理解第 i 个词的上下文，我们应该只关注 word1（即 V1），因为注意力分数的 90% 来自 V1 吗？我们可以明确地定义出应该***更多关注***的重要词汇，以理解第
    i 个词的上下文。
- en: Hence, we can conclude that the higher the contribution of a word in the Zi
    representation, the more critical and related the words are to one another.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们可以得出结论：词汇在 Zi 表示中的贡献越高，词汇之间的相关性和重要性就越大。
- en: Now that we know how to calculate the self-attention matrix, let us understand
    the concept of the ***multi-head attention mechanism***.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们知道如何计算自注意力矩阵，让我们深入了解 ***多头注意力机制*** 的概念。
- en: '***Multi-head attention Mechanism***'
  id: totrans-64
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '***多头注意力机制***'
- en: What will happen if your score matrix is biased toward a specific word representation?
    It will mislead your model and the results will not be as accurate as we expect.
    Let us see an example to understand this better.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你的分数矩阵偏向于特定的词表示会发生什么？这会误导你的模型，结果可能不如预期。让我们看一个例子来更好地理解这一点。
- en: 'S1: “*All is well*”'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 'S1: “*一切都好*”'
- en: Z(well) = 0.6 * V(all) + 0.0 * v(is) + 0.4 * V(well)
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: Z(well) = 0.6 * V(all) + 0.0 * v(is) + 0.4 * V(well)
- en: 'S2: “*The dog ate the food because it was hungry*”'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 'S2: “*狗吃了食物，因为它饿了*”'
- en: Z(it) = 0.0 * V(the) + 1.0 * V(dog) + 0.0 * V(ate) + …… + 0.0 * V(hungry)
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: Z(it) = 0.0 * V(the) + 1.0 * V(dog) + 0.0 * V(ate) + …… + 0.0 * V(hungry)
- en: In S1 case, while calculating Z(well), more importance is given to V(all). It
    is even more than V(well) itself. There is no guarantee how accurate this will
    be.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在 S1 的情况下，计算 Z(well) 时，更重要的是 V(all)。这甚至比 V(well) 自身还要多。没有保证这个结果的准确性。
- en: In the S2 case, while calculating Z(it), all the importance is given to V(dog)
    whereas the scores for the rest of the words are 0.0 including V(it) as well.
    This looks acceptable as the “it” word is ambiguous. It makes sense to relate
    it more to another word than the word itself. That was the whole purpose of this
    exercise of calculating self-attention. To handle the context of ambiguous words
    in the input sentences.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在 S2 的情况下，计算 Z(it) 时，所有的重点都放在 V(dog) 上，而其他单词的分数，包括 V(it) 本身，都为 0.0。这看起来是可以接受的，因为“it”这个词是模糊的。将其与其他词相关联比与词本身相关联更合理。这就是计算自注意力的整个目的，即处理输入句子中模糊词的上下文。
- en: In other words, we can say that if the current word is ambiguous then it is
    okay to give more importance to some other word while calculating self-attention
    but in other cases, it can be misleading for the model. So, what do we do now?
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，我们可以说，如果当前词是模糊的，那么在计算自注意力时给其他单词更多的关注是可以的，但在其他情况下，这可能会误导模型。那么，我们现在该怎么办？
- en: '*What if we calculate multiple attention matrices instead of calculating one
    attention matrix and derive the final attention matrix from these?*'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '*如果我们计算多个注意力矩阵，而不是计算一个注意力矩阵并从中推导最终的注意力矩阵，会怎么样？*'
- en: That is precisely what ***multi-head attention*** is all about! We calculate
    multiple versions of attention matrices z1, z2, z3, ….., zm and concatenate them
    to derive the final attention matrix. That way we can be more confident about
    our attention matrix.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 这正是 ***多头注意力*** 的意义所在！我们计算多个版本的注意力矩阵 z1, z2, z3, ….., zm 并将它们连接起来以推导最终的注意力矩阵。这样我们可以对我们的注意力矩阵更有信心。
- en: Moving on to the next important concept,
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来要讨论的另一个重要概念是，
- en: Positional Encoding
  id: totrans-76
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 位置编码
- en: In seq-to-seq models, the input sentence is fed word by word to the network
    which allows the model to track the positions of words relative to other words.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在 seq-to-seq 模型中，输入句子逐词输入网络，这使得模型能够跟踪单词相对于其他单词的位置。
- en: But in transformer models, we follow a different approach. Instead of giving
    inputs word by word, they are fed parallel-y which helps in reducing the training
    time and learning long-term dependency. But with this approach, the word order
    is lost. However, to understand the meaning of a sentence correctly, word order
    is extremely important. To overcome this problem, a new matrix called “***positional
    encoding***” (P) is introduced.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 但在变压器模型中，我们采取不同的方法。不是逐词输入，而是并行输入，这有助于减少训练时间并学习长期依赖。然而，这种方法会丧失词序。为了正确理解句子的意义，词序是极其重要的。为了解决这个问题，引入了一个新的矩阵称为“***位置编码***”（P）。
- en: This matrix P is sent along with input matrix X to include the information related
    to the word order. For obvious reasons, the dimensions of X and P matrices are
    the same.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 这个矩阵 P 与输入矩阵 X 一起发送，以包含与词序相关的信息。出于明显的原因，X 和 P 矩阵的维度是相同的。
- en: To calculate positional encoding, the formula given below is used.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 计算位置编码时，使用下面给出的公式。
- en: '![](../Images/cd3612f39271e47035921e579b010bfb.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/cd3612f39271e47035921e579b010bfb.png)'
- en: 'Fig. Formula to calculate positional encoding (Source: Author)'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 图。计算位置编码的公式（来源：作者）
- en: In the above formula,
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述公式中，
- en: '**pos** = position of the word in the sentence'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**pos** = 单词在句子中的位置'
- en: '**d** = dimension of the word/token embedding'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**d** = 单词/标记嵌入的维度'
- en: '**i** = represents each dimension in the embedding'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**i** = 表示嵌入中的每一个维度'
- en: In calculations, d is fixed but pos and i vary. If d=512, then i ∈ [0, 255]
    as we take 2i.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 在计算中，d 是固定的，但 pos 和 i 变化。如果 d=512，那么 i ∈ [0, 255]，因为我们取 2^i。
- en: This video covers positional encoding in-depth if you wish to know more about
    it.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想深入了解位置编码，这个视频涵盖了详细内容。
- en: '[Visual Guide to Transformer Neural Networks — (Part 1) Position Embeddings](https://www.youtube.com/watch?v=dichIcUZfOw)'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '[变换器神经网络视觉指南 — （第1部分）位置嵌入](https://www.youtube.com/watch?v=dichIcUZfOw)'
- en: I am using some visuals from the above video to explain this concept in my words.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 我正在使用上述视频中的一些视觉内容来用我的话解释这一概念。
- en: '![](../Images/ea8f94415a5cf0c2c2f9df3d73b615ab.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ea8f94415a5cf0c2c2f9df3d73b615ab.png)'
- en: 'Fig. Positional Encoding Vector Representation (Source: Author)'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 图。位置编码向量表示（来源：作者）
- en: The above figure shows an example of a positional encoding vector along with
    different variable values.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 上图展示了一个位置编码向量及其不同的变量值示例。
- en: '![](../Images/ab2f78bca8ce05315cae5718b316e410.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ab2f78bca8ce05315cae5718b316e410.png)'
- en: 'Fig. Positional Encoding Vectors with constant i and d (Source: Author)'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 图。位置编码向量，i和d恒定（来源：作者）
- en: '![](../Images/396c07028afbefbe8e9c4d543febaa9e.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/396c07028afbefbe8e9c4d543febaa9e.png)'
- en: 'Fig. Positional Encoding Vectors with constant i and d (Source: Author)'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 图。位置编码向量，i和d恒定（来源：作者）
- en: The above figure shows how the values of ***PE(pos, 2i)*** will vary *if i is
    constant and only pos varies*. As we know the ***sinusoidal wave*** is a periodic
    function that tends to repeat itself after a fixed interval. We can see that the
    encoding vectors for pos = 0 and pos = 6 are identical. This is not desirable
    as we would want *different positional encoding vectors for different values of
    pos*.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 上图展示了如何*在i恒定而只有pos变化的情况下，***PE(pos, 2i)*** 的值会变化*。我们知道***正弦波***是一个周期性函数，倾向于在固定的间隔后重复自己。我们可以看到pos
    = 0和pos = 6的编码向量是相同的。这并不可取，因为我们希望*对不同的pos值有不同的位置编码向量*。
- en: This can be achieved by *varying the frequency of the sinusoidal wave.*
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 这可以通过*改变正弦波的频率*来实现。
- en: '![](../Images/8510db69b50d4b69bb7d67905eb17508.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8510db69b50d4b69bb7d67905eb17508.png)'
- en: 'Fig. Positional Encoding Vectors with varying pos and i (Source: Author)'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 图。位置编码向量，pos和i变化（来源：作者）
- en: As the value of i varies, the frequency of sinusoidal waves also varies resulting
    in different waves and hence, resulting in different values for every positional
    encoding vector. This is exactly what we wanted to achieve.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 随着i的变化，正弦波的频率也会变化，导致不同的波形，因此，每个位置编码向量的值也不同。这正是我们想要实现的。
- en: The positional encoding matrix (P) is added to the input matrix (X) and fed
    to the encoder.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 位置编码矩阵（P）被添加到输入矩阵（X）中，并送入编码器。
- en: '![](../Images/85aa9a8bf6e14434b531874306780d2e.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/85aa9a8bf6e14434b531874306780d2e.png)'
- en: 'Fig. Adding positional encoding to the input embedding (Source: Author)'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 图。将位置编码添加到输入嵌入中（来源：作者）
- en: The next component of the encoder is the **feedforward network**.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 编码器的下一个组件是**前馈网络**。
- en: Feedforward Network
  id: totrans-107
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 前馈网络
- en: This sublayer in the encoder block is the classic neural network with two dense
    layers and ReLU activations. It accepts input from the multi-head attention layer,
    performs some non-linear transformations on the same and finally generates contextualised
    vectors. The fully-connected layer is responsible for considering each attention
    head and learning relevant information from them. Since the attention vectors
    are independent of each other, they can be passed to the transformer network in
    a parallelised way.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 编码器块中的这一子层是经典的神经网络，包含两个全连接层和ReLU激活。它接受来自多头注意力层的输入，对其进行一些非线性变换，最终生成上下文化的向量。全连接层负责考虑每个注意力头，并从中学习相关信息。由于注意力向量彼此独立，它们可以以并行方式传递给变换器网络。
- en: The last and final component of the Encoder block is ***Add & Norm component***.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 编码器块的最后一个组件是***加法和规范化组件***。
- en: '**Add & Norm component**'
  id: totrans-110
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**加法和规范化组件**'
- en: This is a *residual layer* followed by *layer normalisation*. The residual layer
    ensures that no important information related to the input of sub-layers is lost
    in the processing. While the normalisation layer promotes faster model training
    and prevents the values from changing heavily.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个*残差层*，后跟*层归一化*。残差层确保在处理过程中不会丢失与子层输入相关的重要信息，而归一化层则促进模型训练的速度，并防止值的大幅变化。
- en: '![](../Images/8445cba56bcc14616fbd25b49285cc98.png)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8445cba56bcc14616fbd25b49285cc98.png)'
- en: 'Fig. Encoder components with Add & Norm layers included (Source: Author)'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 图。包含加法和规范化层的编码器组件（来源：作者）
- en: 'Within the encoder, there are two add & norm layers:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 在编码器内部，有两个加法和规范化层：
- en: connects the input of the multi-head attention sub-layer to its output
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 连接多头注意力子层的输入和输出
- en: connects the input of the feedforward network sub-layer to its output
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将前馈网络子层的输入连接到其输出。
- en: 'With this, we conclude the internal working of the Encoders. To summarize the
    article, let us quickly go over the steps that the encoder uses:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 到此为止，我们总结了编码器的内部工作原理。为了总结文章，让我们快速回顾编码器使用的步骤：
- en: Generate embeddings or tokenized representations of the input sentence. This
    will be our input matrix X.
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生成输入句子的嵌入或标记化表示。这将是我们的输入矩阵 X。
- en: Generate the positional embeddings to preserve the information related to the
    word order of the input sentence and add it to the input matrix X.
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生成位置嵌入，以保留与输入句子单词顺序相关的信息，并将其添加到输入矩阵 X 中。
- en: 'Randomly initialize three matrices: Wq, Wk, & Wvi.e. weights of query, key
    & value. These weights will be updated during the training of the transformer
    model.'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随机初始化三个矩阵：Wq、Wk 和 Wv，即查询、键和值的权重。这些权重将在 Transformer 模型的训练过程中更新。
- en: Multiply the input matrix X with each of Wq, Wk, & Wv to generate Q (query),
    K (key) and V (value) matrices.
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将输入矩阵 X 与每个 Wq、Wk 和 Wv 相乘，生成 Q（查询）、K（键）和 V（值）矩阵。
- en: Calculate the dot product of Q and K-transpose, scale the product by dividing
    it with the square root of dk or embedding dimension and finally normalize it
    using the softmax function.
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算 Q 和 K 转置的点积，将该乘积除以 dk 或嵌入维度的平方根进行缩放，最后使用 softmax 函数进行归一化。
- en: Calculate the attention matrix Z by multiplying the V or value matrix with the
    output of the softmax function.
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过将 V 或值矩阵与 softmax 函数的输出相乘来计算注意力矩阵 Z。
- en: Pass this attention matrix to the feedforward network to perform non-linear
    transformations and generate contextualized embeddings.
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将此注意力矩阵传递给前馈网络，以执行非线性变换并生成上下文化的嵌入。
- en: In the next article, we will understand how the Decoder component of the Transformer
    model works.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一篇文章中，我们将了解 Transformer 模型的解码器组件是如何工作的。
- en: This would be all for this article. I hope you found it useful. If you did,
    please don’t forget to clap and share it with your friends.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 这篇文章就到这里。我希望你觉得它有用。如果有的话，请不要忘记点赞并与朋友分享。
