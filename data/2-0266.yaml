- en: 'Advances in Deep Learning for Time Series Forecasting and Classification: Winter
    2023 Edition'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 时间序列预测与分类中的深度学习进展：2023年冬季版
- en: 原文：[https://towardsdatascience.com/advances-in-deep-learning-for-time-series-forecasting-and-classification-winter-2023-edition-6617c203c1d1](https://towardsdatascience.com/advances-in-deep-learning-for-time-series-forecasting-and-classification-winter-2023-edition-6617c203c1d1)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/advances-in-deep-learning-for-time-series-forecasting-and-classification-winter-2023-edition-6617c203c1d1](https://towardsdatascience.com/advances-in-deep-learning-for-time-series-forecasting-and-classification-winter-2023-edition-6617c203c1d1)
- en: The downfall of transformers for time series forecasting and the rise of time
    series embedding methods. Plus advances in anomaly detection, classification,
    and optimal (t) interventions.
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 时间序列预测中变压器的衰退与时间序列嵌入方法的兴起。此外，还有异常检测、分类和最优(t)干预的进展。
- en: '[](https://igodfried.medium.com/?source=post_page-----6617c203c1d1--------------------------------)[![Isaac
    Godfried](../Images/302c9b76bc1e293698ffa1479825e326.png)](https://igodfried.medium.com/?source=post_page-----6617c203c1d1--------------------------------)[](https://towardsdatascience.com/?source=post_page-----6617c203c1d1--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----6617c203c1d1--------------------------------)
    [Isaac Godfried](https://igodfried.medium.com/?source=post_page-----6617c203c1d1--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://igodfried.medium.com/?source=post_page-----6617c203c1d1--------------------------------)[![Isaac
    Godfried](../Images/302c9b76bc1e293698ffa1479825e326.png)](https://igodfried.medium.com/?source=post_page-----6617c203c1d1--------------------------------)[](https://towardsdatascience.com/?source=post_page-----6617c203c1d1--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----6617c203c1d1--------------------------------)
    [Isaac Godfried](https://igodfried.medium.com/?source=post_page-----6617c203c1d1--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----6617c203c1d1--------------------------------)
    ·16 min read·Jan 10, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于[Towards Data Science](https://towardsdatascience.com/?source=post_page-----6617c203c1d1--------------------------------)
    ·阅读时间16分钟·2023年1月10日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/7af7bc73ed4069181cec96d796eced4a.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7af7bc73ed4069181cec96d796eced4a.png)'
- en: Photo from myself (Great Sand Dunes NP at sunset)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 我自己拍摄的照片（大沙丘国家公园日落时分）
- en: '*Note you can find an [updated 2024 issue of this article in DDS](https://medium.com/deep-data-science/advances-in-deep-learning-for-time-series-forecasting-classification-winter-2024-a3fd31b875b0).'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '*注意，你可以在DDS中找到这篇文章的[2024年更新版](https://medium.com/deep-data-science/advances-in-deep-learning-for-time-series-forecasting-classification-winter-2024-a3fd31b875b0)。*'
- en: It has been quite sometime since I’ve written an update on the state of deep
    learning for time series. Several conferences have come and gone and the field
    as a whole has advanced in several different ways. Here I will attempt to cover
    some of the more promising as well as critical papers that have come out in the
    last year or so as well as updates to the [Flow Forecast](https://github.com/AIStream-Peelout/flow-forecast)
    framework [FF].
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 自从我上次更新有关时间序列深度学习的状态以来已经有一段时间了。几个会议已经过去，整个领域在多个方面都有所进展。在这里，我将尝试涵盖过去一年左右出现的一些更有前途以及关键的论文，以及对[Flow
    Forecast](https://github.com/AIStream-Peelout/flow-forecast)框架[FF]的更新。
- en: '[Flow Forecast Framework](https://github.com/AIStream-Peelout/flow-forecast)
    updates:'
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '[Flow Forecast Framework](https://github.com/AIStream-Peelout/flow-forecast)
    更新：'
- en: Over the last year we have made major strides in the architecture and documentation
    of FF. Just recently we rolled out full support for time series classification
    and supervised anomaly detection. Additionally, we have added several more [tutorial
    notebooks](https://github.com/AIStream-Peelout/flow_tutorials) and expanded our
    unit-test coverage to more than 77%.
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在过去的一年里，我们在FF的架构和文档方面取得了重大进展。最近，我们推出了对时间序列分类和监督异常检测的全面支持。此外，我们还增加了更多的[教程笔记](https://github.com/AIStream-Peelout/flow_tutorials)并将单元测试覆盖率扩大到超过77%。
- en: We also added a vanilla GRU model that you can use for time series forecasting,
    classification and anomaly detection.
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们还添加了一个原始的GRU模型，您可以用它进行时间序列预测、分类和异常检测。
- en: I presented some of my recent research at PyData NYC this past November (they
    still have not posted videos online unfortunately). I also coded up a [tutorial
    on Avocado Price Forecasting.](https://medium.com/towards-data-science/deep-time-series-forecasting-with-flow-forecast-part-1-avocado-prices-276a59eb454f)
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我在去年的11月在PyData NYC展示了我最近的一些研究（遗憾的是，他们尚未在线发布视频）。我还编写了一个[关于鳄梨价格预测的教程。](https://medium.com/towards-data-science/deep-time-series-forecasting-with-flow-forecast-part-1-avocado-prices-276a59eb454f)
- en: We are using GitHub discussions! Be sure to check out our [discussions](https://github.com/AIStream-Peelout/flow-forecast/discussions)
    or start a new one.
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们正在使用GitHub讨论！务必查看我们的[讨论](https://github.com/AIStream-Peelout/flow-forecast/discussions)或开始一个新的讨论。
- en: Now let’s jump into some field updates.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们跳转到一些领域更新。
- en: '**Transformer Related Research: Autoformer, Pyraformer, Fedformer, etc, their
    effectiveness and problems**'
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**变换器相关研究：Autoformer、Pyraformer、Fedformer等，它们的有效性和问题**'
- en: '![](../Images/58fe2668eb77396951ea8062579fe752.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/58fe2668eb77396951ea8062579fe752.png)'
- en: Figure from page 3 of [Are Transformers Really Effective for Time Series Forecasting?](https://arxiv.org/pdf/2205.13504.pdf)
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 来源于[变换器在时间序列预测中的有效性？](https://arxiv.org/pdf/2205.13504.pdf) 第3页的图
- en: '***Summary***: The transformer family of time series forecasting architectures
    has continued to grow with models such as the Autoformer (Neurips 2021), [Pyraformer](https://openreview.net/forum?id=0EXmFzUn5I)(ICLR
    2022), [Fedformer (ICML 2022)](https://arxiv.org/abs/2201.12740), [EarthFormer](https://arxiv.org/abs/2207.05833)
    (Neurips 2022), and [Non-Stationary Transformer](https://openreview.net/forum?id=ucNDIDRNjjv)
    (Neurips 2022). However, the ability of these models to accurately forecast data
    and outperform existing methods remains in question particularly in light of new
    research (which we will discuss on its in a bit).'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '***总结***：变换器系列的时间序列预测架构持续增长，如Autoformer（Neurips 2021）、[Pyraformer](https://openreview.net/forum?id=0EXmFzUn5I)（ICLR
    2022）、[Fedformer（ICML 2022）](https://arxiv.org/abs/2201.12740)、[EarthFormer](https://arxiv.org/abs/2207.05833)（Neurips
    2022）和[非平稳变换器](https://openreview.net/forum?id=ucNDIDRNjjv)（Neurips 2022）。然而，这些模型是否能准确预测数据并超越现有方法仍然存在疑问，特别是考虑到新研究（我们将稍后讨论）。'
- en: '[Autoformer](https://arxiv.org/abs/2106.13008):'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '[Autoformer](https://arxiv.org/abs/2106.13008):'
- en: '![](../Images/eace1323199ab2c9f6c2840be036809d.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/eace1323199ab2c9f6c2840be036809d.png)'
- en: Architecture of the Autoformer model (Neurips 2021). The model features a seasonal
    decomposition mechanism which aims to create seasonal and cyclical representations
    of the temporal data. The decoder ingests three items . The decoder outputs a
    seasonal part and trend part that then get added together into a prediction. Figure
    from page 2 of Autoformer paper.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: Autoformer模型的架构（Neurips 2021）。该模型具有一种季节性分解机制，旨在创建时间数据的季节性和周期性表示。解码器接收三个项目。解码器输出一个季节性部分和一个趋势部分，然后将它们相加得到预测。来自Autoformer论文第2页的图。
- en: Autoformer expands and improves the performance of the Informer model. Autoformer
    features an auto-correlation mechanism which enables the model to better learn
    temporal dependencies than standard attention. It aims to accurately decompose
    the the trend and seasonal components of temporal data. You can find full code
    for the paper [here.](https://github.com/thuml/Autoformer)
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: Autoformer扩展和改进了Informer模型的性能。Autoformer具有一种自相关机制，使模型能够比标准注意力机制更好地学习时间依赖性。它旨在准确分解时间数据的趋势和季节性成分。您可以在[这里](https://github.com/thuml/Autoformer)找到该论文的完整代码。
- en: 'Pyraformer: In this paper the authors introduce “pyramidal attention module
    (PAM) in which the inter-scale tree structure summarizes features at different
    resolutions and the intra-scale neighboring connections model the temporal dependencies
    of different ranges.”'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: Pyraformer：在这篇论文中，作者介绍了“金字塔注意力模块（PAM），其中跨尺度树结构总结了不同分辨率下的特征，而同尺度邻接连接则建模不同范围的时间依赖性。”
- en: '[Fedformer:](https://arxiv.org/abs/2201.12740) This model focuses on capturing
    the global trend in the time series data. The authors, propose a seasonal trend
    decomposition module that aims to capture the global character of the time series.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '[Fedformer:](https://arxiv.org/abs/2201.12740) 该模型专注于捕捉时间序列数据中的全球趋势。作者提出了一种季节性趋势分解模块，旨在捕捉时间序列的全球特征。'
- en: 'Earthformer: Perhaps the most unique of this “set” of papers, the earthformer
    specifically focuses on forecasting earth systems such as weather, climate, and
    agriculture. This paper features a new earth cuboid attention mechanism. I’m hopeful
    of the potential this paper has for my research on stream and flash flood forecasting,
    where many of the classic transformers have failed.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: Earthformer：也许是这一“系列”论文中最独特的，Earthformer专注于预测地球系统，如天气、气候和农业。该论文展示了一种新的地球立方体注意力机制。我对这篇论文在我的流域和暴雨洪水预测研究中的潜力感到希望，因为许多经典的变换器在这方面表现不佳。
- en: '[Non-Stationary Transformer](https://openreview.net/forum?id=ucNDIDRNjjv):
    This is the most recent publication in the group of transformer for forecasting
    papers. The authors aim to better adapt transformers for handling non-stationary
    time series. They employ two mechanisms: de-stationary attention and a series
    stationarization mechanism. These mechanisms can be plugged into any existing
    transformer model and the authors test plugging them into the Informer, Autoformer,
    and the Vanilla Transformer where they all boost performance (in the appendix
    they also show it boosts Fedformer performance).'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '[非平稳变换器](https://openreview.net/forum?id=ucNDIDRNjjv)：这是变换器用于预测领域中的最新出版物。作者旨在更好地调整变换器以处理非平稳时间序列。他们采用了两种机制：去平稳注意力和系列平稳化机制。这些机制可以插入到任何现有的变换器模型中，作者测试了它们在Informer、Autoformer和Vanilla
    Transformer中的效果，并且都提升了性能（在附录中，他们还显示它提升了Fedformer的性能）。'
- en: '***Discussion/Evaluation***: Similar to the Informer all these models (with
    the exception of Earthformer) were evaluated on the electrical (ETTh), traffic,
    exchange and weather datasets. These models primarily evaluate based on Mean Squared
    Error (MSE) and the Mean Absolute Error (MAE) metrics:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '***讨论/评估***：与Informer类似，所有这些模型（地球变换器除外）都在电力（ETTh）、交通、交换和天气数据集上进行了评估。这些模型主要基于均方误差（MSE）和平均绝对误差（MAE）指标进行评估：'
- en: '![](../Images/061d793a5ba3b5ef1a31597a09fc38ed.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/061d793a5ba3b5ef1a31597a09fc38ed.png)'
- en: Forecasting results from Non-Stationary Transformers compared to other models.
    From page 7 of [Non-Stationary Transformers](https://openreview.net/pdf?id=ucNDIDRNjjv).
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 非平稳变换器与其他模型的预测结果。见[非平稳变换器](https://openreview.net/pdf?id=ucNDIDRNjjv)第7页。
- en: In the future I hope that all authors of transformer papers compare their model
    to simpler methods like D-Linear (which will talk about later) and even a basic
    LSTM/GRU. Also they should go beyond some of these standard datasets as I haven’t
    seen good performance on other time series related datasets. For instance, I had
    immense problems getting the Informer to accurately forecast river flows and it
    generally performed poorly compared to a LSTM or a even a vanilla transformer
    model. Since unlike with computer vision where image dimensions at least stay
    constant, time series data can differ immensely in terms of length, periodicity,
    trend, and seasonality a larger range of datasets is needed.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 我希望未来所有变换器论文的作者能将他们的模型与更简单的方法如D-Linear（稍后将讨论）甚至基本的LSTM/GRU进行比较。他们还应该超越一些标准数据集，因为我没有看到其他时间序列相关数据集上的良好表现。例如，我在让Informer准确预测河流流量时遇到了极大的问题，与LSTM或即使是基本的变换器模型相比，其表现普遍较差。由于与计算机视觉不同，图像尺寸至少保持不变，时间序列数据在长度、周期性、趋势和季节性方面可能差异巨大，因此需要更广泛的数据集。
- en: 'In the [comments on OpenReview for the Non-Stationary Transformer one of the
    reviewers echoed these concerns](https://openreview.net/forum?id=ucNDIDRNjjv),
    however it was unfortunately shot-down in the final meta-review:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在[OpenReview上关于非平稳变换器的评论中，一位评审表达了这些担忧](https://openreview.net/forum?id=ucNDIDRNjjv)，然而在最终的元评审中不幸被驳回：
- en: “As the model is in the Transformer space, and transformers have previously
    been shown to be state of the art on a number of tasks, I do not find it necessary
    to compare against other ‘families’ of methods.”
  id: totrans-33
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “由于该模型在变换器领域，且变换器在多个任务上已被证明是最先进的，因此我认为没有必要与其他‘家族’的方法进行比较。”
- en: I personally think this is an extremely problematic argument and has led to
    the lack of applicability of research in the real-world. If certain models perform
    well in NLP we are supposed to just assume that they will perform well in time
    series? Also, if there is an incorrect evaluation protocol but it was the standard
    in previous publications then it should be repeated? As someone who has valued
    state of the art approaches and innovative models in practice, this is the exact
    type of thing that will make me look like a complete idiot when I spend months
    trying to get a supposedly “good” model to work only to be out performed by linear
    regression.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 我个人认为这是一个极具问题的论点，并导致了研究在实际应用中的缺乏。如果某些模型在NLP中表现良好，我们是否就应该假设它们在时间序列中也会表现良好？另外，如果评估协议不正确但在以往的出版物中是标准的，那么它是否应该被重复？作为一个在实践中重视最先进方法和创新模型的人，这正是那种会让我看起来像个完全的傻瓜的情况，我花了几个月时间试图让一个所谓的“好”模型工作，结果却被线性回归超越。
- en: That said I don’t necessarily think this paper should’ve been rejected or singled
    out as all the transformer papers are equally guilty of limited evaluation. Rather
    we should from the start require more rigorous comparisons and clear illuminations
    of shortcomings. A complex model “family” might not always outperform simple models
    initially but that needs to be clearly noted in the paper and not glossed over
    or simply assumed not to be case because it has previously performed well in other
    domains.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 也就是说，我并不认为这篇论文应该被拒绝或特别指出，因为所有的变换器论文都同样存在评估有限的问题。相反，我们应该从一开始就要求更严格的比较和清晰的不足之处说明。一个复杂的模型“家族”可能不会总是优于简单模型，但这一点需要在论文中明确指出，而不是被掩盖或简单地假设它在其他领域表现良好。
- en: On another happier note I was somewhat impressed with the evaluation of Earthformer.
    Earthformer was evaluated on the moving the “MovingMNIST dataset and a newly proposed
    chaotic N-body MNIST dataset” which the authors used to verify the effectiveness
    of the cuboid attention. They then evaluated it for precipitation now-casting
    and el/nino cycle forecasting. I think that this is a good example integrating
    physical knowledge into a model architecture with the cuboid attention and then
    designing good sub-tests.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在另一个更令人愉快的消息中，我对 Earthformer 的评估感到有些印象深刻。Earthformer 在“移动 MNIST 数据集和一个新提出的混沌
    N-body MNIST 数据集”上进行了评估，作者们使用这些数据集来验证立方体注意力的有效性。然后，他们对降水现在casting 和厄尔尼诺循环预测进行了评估。我认为这是一个将物理知识融入模型架构（结合立方体注意力）并设计良好子测试的好例子。
- en: '***Flow Forecast integration***: Since many of these models follow the same
    basic format of Informer the work to port them to FF is not that extensive. However,
    at certain point we have to wonder how much better these newer transformer models
    are on real-world data. Code consolidation is another area for myself and other
    maintainers to think about. Previously we have copied large swathes of code from
    authors implementations and tried to preserve that as much as possible (so as
    not to introduce new errors). That said we will likely add several of the models
    over the next couple of months (Fedformer, Non-Stationary Transformer).'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '***流量预测集成***：由于许多这些模型遵循相同的基本格式，因此将它们移植到 FF 的工作量不是很大。然而，在某些情况下，我们不得不考虑这些更新的变换器模型在现实世界数据上的表现如何。代码整合是我和其他维护人员需要考虑的另一个领域。之前，我们从作者的实现中复制了大量代码，并尽可能保留（以避免引入新错误）。也就是说，我们可能会在接下来的几个月中添加几个模型（Fedformer，非平稳变换器）。'
- en: '**Are Transformers Effective for Time Series Forecasting (2022)?**'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '**变换器在时间序列预测中的有效性（2022）？**'
- en: '![](../Images/ecd3a72e2c9d6405caad0a4b48cebe3f.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ecd3a72e2c9d6405caad0a4b48cebe3f.png)'
- en: Image from [Are Transformers Effective for Time Series Forecasting](https://arxiv.org/pdf/2205.13504.pdf)
    page. 2\. TLDR is basically simple models outperform pretty much every transformer
    model up-to the Fedformer model (Non-stationary transformer was a later work though
    simple model would’ve likely out performed it as well. Possibly Non-Stationary
    Transformer + Fedformer might beat simple models in certain cases but that is
    a very big model compared to a simple one).
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源于[变换器在时间序列预测中的有效性](https://arxiv.org/pdf/2205.13504.pdf)第2页。TLDR 基本上是简单模型在几乎所有变换器模型中表现优越，直到
    Fedformer 模型（尽管非平稳变换器是后来的工作，但简单模型很可能也会超越它。可能非平稳变换器 + Fedformer 在某些情况下会超过简单模型，但与简单模型相比，这是一个非常大的模型）。
- en: 'This paper explores the ability of transformer to forecast data versus baseline
    methods. The results somewhat reaffirm what I have seen in many of my own experiments
    that transformers often perform worse than simpler models and are difficult to
    tune. A couple interesting points within the paper include:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 本文探讨了变换器预测数据的能力与基线方法的对比。结果在一定程度上证实了我在许多实验中看到的情况，即变换器通常表现不如简单模型，并且难以调整。文中几个有趣的点包括：
- en: 'The authors gradually replace self-attention with basic linear layers and find:
    “Surprisingly, the performance of Informer grows with the gradual simplification,
    indicating the unnecessary of the self-attention scheme and other complex modules
    at least for existing LTSF benchmarks”'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 作者逐渐用基本的线性层替代自注意力，并发现：“令人惊讶的是，Informer 的性能随着逐步简化而提高，表明自注意力机制和其他复杂模块在现有 LTSF
    基准测试中并非必要。”
- en: 'The authors also investigate whether increasing the look-back window improves
    the transformer performance and find that: “the performance of the SOTA Transformers
    drops slightly, indicating these models only capture similar temporal information
    from the adjacent time series sequence.”'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 作者们还研究了是否增加回顾窗口会提高变压器的性能，发现：“SOTA 变压器的性能略微下降，这表明这些模型仅捕捉到来自相邻时间序列的相似时间信息。”
- en: Authors also explored whether positional embedding really capture the temporal
    order of the time series well. They do this by randomly shuffling the input sequence
    into the transformer. They found on several datasets this shuffling did not impact
    results (which is obviously quite troubling).
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 作者们还探讨了位置嵌入是否真的很好地捕捉时间序列的时间顺序。他们通过将输入序列随机打乱后输入变压器来进行这项研究。他们发现，在多个数据集上，这种打乱并没有影响结果（这显然是相当令人担忧的）。
- en: '***Discussion:*** Over the last several years I have run countless time-series
    experiments with transformer models and in the vast majority of cases the results
    were not great. For the longest time I assumed I must be doing something wrong
    or missing some small implementation detail. After-all these were supposed the
    next SOTA model just like in NLP. So it is nice to see some research that shows
    my experiments weren’t flawed (at least not entirely). However, it still leaves
    a load of lasting questions such as where to head next? If a simple model outperforms
    transformers should we continue to use them? Are all transformers inherently flawed
    or is it just the current mechanism? Should we return back to architectures like
    LSTMs, GRUs or simple feed forward models? These are questions I do not know the
    answer to and it remains to be seen the overall impact of the paper. As of now
    I think that the answer might be to take a step back and focus on learning effective
    time series representations. After-all initially BERT in the NLP context succeeded
    by forming good representations.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '***讨论：*** 在过去的几年里，我进行了无数次的时间序列实验，使用变压器模型，在绝大多数情况下，结果都不是很好。很长时间我认为我一定是在做错什么或者遗漏了一些小的实现细节。毕竟，这些模型本应成为下一个
    SOTA 模型，就像在 NLP 中一样。因此，看到一些研究表明我的实验并非完全有缺陷（至少不是完全的）是很好的。然而，这仍然留下了许多持续的问题，比如下一步该如何走？如果简单模型的性能优于变压器，我们是否应该继续使用变压器？所有变压器是否本质上都有缺陷，还是仅仅是当前机制的问题？我们是否应该回到像
    LSTM、GRU 或简单的前馈模型这样的架构？这些问题我不知道答案是什么，而且论文的总体影响尚待观察。现在，我认为答案可能是退一步，专注于学习有效的时间序列表示。毕竟，最初
    BERT 在 NLP 上的成功就是通过形成良好的表示来实现的。'
- en: That said I don’t think we should view tranformers for time-series as entirely
    dead. The Fedformer did perform quite close to the simple model benchmarks and
    did better with the various ablation shuffling tasks. I’ve also seen anecdotally
    that while transformers often struggle with forecasting in many cases their internal
    representations of the data can be quite good. I think more is needed to see the
    disconnect between the internal representation and the actual forecasting output.
    Also, as the authors suggest improving the positional embeddings could play a
    key role in improving overall performance. Finally, as we will see below there
    was a recent transformer based model that performed very well on a wide array
    of anomaly detection datasets.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管如此，我认为我们不应该完全认为时间序列的变压器已经死了。Fedformer 在简单模型基准测试中表现得非常接近，并且在各种消融打乱任务中表现更好。我也从经验上看到，虽然变压器在预测中经常表现不佳，但它们对数据的内部表示往往相当好。我认为需要更多的研究来了解内部表示和实际预测输出之间的脱节。此外，正如作者所建议的，改进位置嵌入可能在提高整体性能方面发挥关键作用。最后，正如我们将看到的，最近有一种基于变压器的模型在各种异常检测数据集上表现非常好。
- en: '***Flow Forecast integration:*** The paper did introduce a number of simpler
    models that serve as good benchmarks against complicated transformer methods.
    Since the models are simple they shouldn’t require that much effort to add to
    our framework. We will likely add these to FF over the next couple months. In
    the meantime you can find full code for the paper here.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '***流量预测整合：*** 论文确实介绍了一些较简单的模型，这些模型作为复杂变压器方法的良好基准。由于这些模型很简单，所以将它们添加到我们的框架中不会需要太多努力。我们可能会在接下来的几个月内将这些模型添加到
    FF 中。与此同时，你可以在这里找到论文的完整代码。'
- en: '[**Anomaly Transformer (ICLR Spolight 2022)**](https://arxiv.org/abs/2110.02642)**:**
    As evidenced above quite a bit of research has focused on applying the transformers
    to forecasting, however there has been comparatively little research anomaly detection.
    This paper introduces a (unsupervised) transformer to detect anomalies. The model
    utilizes specially constructed anomaly attention mechanism in conjunction with
    a minmax strategy.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '[**异常变压器 (ICLR Spolight 2022)**](https://arxiv.org/abs/2110.02642)**:** 如上所示，已有相当多的研究专注于将变压器应用于预测，但在异常检测方面的研究相对较少。本文介绍了一种（无监督的）变压器用于检测异常。该模型利用了特别构造的异常注意力机制和最小最大策略。'
- en: '![](../Images/838ba8e2f228d04a6f09a13de071eb6c.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/838ba8e2f228d04a6f09a13de071eb6c.png)'
- en: The authors develop a special form of attention specifically for anomaly detection.
    From Anomaly Transformer paper page 4.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 作者开发了一种专门用于异常检测的特殊注意力形式。来自异常变压器论文第4页。
- en: '*Evaluation*: This paper evaluates the performance of the model on five real
    world datasets including Server Machine Dataset, Pooled Server Metrics, Soil Moisture
    Active Passive, and NeurIPS-TS (which itself consists of five different datasets).
    While one might be tempted to view this model skeptically particularly with respect
    to the above mentioned transformers, this evaluation was fairly rigorous. Neurips-TS
    was a recent dataset that was specifically created to provide a more rigorous
    evaluation of anomaly detection models (see more in the datasets section below).
    Therefore it does seem that this model actually improves performance versus simpler
    anomaly detection models.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '*评估*: 本文评估了模型在五个现实世界数据集上的表现，包括服务器机器数据集、汇总服务器指标、土壤湿度主动被动，以及NeurIPS-TS（它本身包含五个不同的数据集）。尽管人们可能会倾向于对该模型持怀疑态度，特别是关于上述提到的变压器，但这次评估相当严格。Neurips-TS是一个最近的数据集，专门创建用于提供更严格的异常检测模型评估（更多内容见下面的数据集部分）。因此，这个模型实际上似乎在性能上优于更简单的异常检测模型。'
- en: '*Discussion*: Here the authors present a unique unsupervised transformer that
    performs well on a plethora of anomaly detection datasets. To me it was one of
    the more promising papers in the time series transformer space over the last couple
    years.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '*讨论*: 在这里，作者展示了一种在众多异常检测数据集上表现良好的独特无监督变压器。对我来说，这是过去几年时间序列变压器领域中最有前途的论文之一。'
- en: In many ways it seems to make sense to first create models to effectively classify
    and detect anomalies in the temporal space and only later to focus on forecasting
    data. In my general experience forecasting is more challenging to than classification
    and even anomaly detection as you are trying to predict a huge possible range
    of values multiple time steps into the future. I’m kind of surprised so much research
    has focused on forecasts and ignored classification or anomaly detection given
    they seem to be a more natural first step with transformers.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多方面，首先创建有效分类和检测时间序列异常的模型似乎更有意义，然后再专注于数据预测。根据我的一般经验，预测比分类甚至异常检测更具挑战性，因为你需要预测未来多个时间步长中的巨大可能值范围。我有点惊讶这么多研究专注于预测而忽视了分类或异常检测，考虑到它们似乎是变压器的更自然的第一步。
- en: '*FF Integration:* Definitely in the future I hope to add the model to FF as
    right now we have very limited anomaly detection models. However, adding the paper
    will likely entail writing a separate data-loader as the model is unsupervised
    and possibly adding additional checks to our main training loop (FF training loop
    assume both an X, Y value will be returned by the data-loader). You can however
    see the full code implementation for the model here.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '*FF集成:* 未来我希望将该模型添加到FF中，因为目前我们只有非常有限的异常检测模型。然而，添加这篇论文可能需要编写一个单独的数据加载器，因为该模型是无监督的，并且可能需要对我们的主要训练循环进行额外检查（FF训练循环假设数据加载器将返回X和Y值）。不过，你可以在这里查看模型的完整代码实现。'
- en: '[**WaveBound: Dynamic Error Bounds for Stable Time Series Forecasting**](https://openreview.net/forum?id=vsNQkquutZk)
    **(Neurips 2022):**'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '[**WaveBound: 稳定时间序列预测的动态误差界限**](https://openreview.net/forum?id=vsNQkquutZk)
    **(Neurips 2022):**'
- en: 'Summary: This paper introduces a new form of regularization that aims to improve
    training of deep time series forecasting models (particularly the above mentioned
    transformers).'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 总结：本文介绍了一种新的正则化形式，旨在改善深度时间序列预测模型（特别是上述提到的变压器）的训练。
- en: 'Evaluation: The authors evaluate their model in conjunction by plugging into
    existing transformer models + LSTNet. They find that it significantly improves
    performance in most cases. Though they only test models through the Autoformer
    and not more recent models like Fedformer.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 评估：作者通过将模型插入现有的变换器模型 + LSTNet 来评估其模型。他们发现，在大多数情况下，这显著提高了性能。虽然他们仅通过 Autoformer
    测试了模型，而没有测试更近期的模型如 Fedformer。
- en: 'Discussion: New forms of regularization or loss functions are always useful
    as they can often plug-in to any existing time-series model to improve performance.
    Also I’m beginning to think that maybe if you combined Fedformer + Non-Stationary
    Mechanism + Wavebound you might be able to beat the simple D-Linear in performance
    :). Not great but it is a start but hey it might mean transformers aren’t completely
    dead with sufficient boosting.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 讨论：新的正则化或损失函数形式总是有用的，因为它们可以经常插入任何现有的时间序列模型中以提高性能。此外，我开始认为如果将 Fedformer + 非平稳机制
    + Wavebound 结合起来，可能会在性能上超越简单的 D-Linear。虽然不算完美，但这是一个开始，也许这意味着只要足够的提升，变换器并未完全死去。
- en: 'FF Integration: The authors do provide an implementation of the code. I like
    the fact that it can work with both RNNs and Transformers (since our code-base
    contains both). However, we probably won’t get around to adding it for awhile
    as a number of the other models are higher priority. But we will add it to our
    roadmap. If you have time You also can always open a PR yourself!'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: FF 集成：作者确实提供了代码实现。我喜欢它可以与 RNN 和变换器兼容（因为我们的代码库包含这两者）。不过，我们可能不会很快添加它，因为其他模型优先级更高。但我们会将其加入我们的路线图。如果有时间，你也可以自己提交
    PR！
- en: Time Series Representations
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 时间序列表示
- en: While the news might seem somewhat bleak with respect to transformers and forecasting,
    there have been a number of advances in creating useful time series representations.
    Some of these developments overlap and parallel the transformer related research
    but they have the added benefit of being primarily focused on the representations
    and not the final forecasting result. Altogether I think this is an impressive
    new area in the field of deep learning for time series that should be explored
    in more depth.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管关于变换器和预测的新闻可能显得有些悲观，但在创建有用的时间序列表示方面已经取得了一些进展。这些发展中的一些与变换器相关的研究有所重叠和并行，但它们主要专注于表示而非最终预测结果。总的来说，我认为这是深度学习时间序列领域中的一个令人印象深刻的新领域，值得更深入地探索。
- en: '[**TS2Vec: Towards Universal Representation of Time Series (AAAI 2022)**](https://arxiv.org/abs/2106.10466)'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '[**TS2Vec: 朝着时间序列的通用表示（AAAI 2022）**](https://arxiv.org/abs/2106.10466)'
- en: 'Summary: TS2Vec is a universal framework for learning time series representations/embeddings.
    The paper itself is already somewhat dated, however it really started this trend
    of time series representation learning papers.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 总结：TS2Vec 是一个用于学习时间序列表示/嵌入的通用框架。尽管论文本身已经有些过时，但它真正开启了时间序列表示学习论文的趋势。
- en: 'Discussion/Evaluation: Evaluation is done both for using the representations
    for forecasting and anomaly detection. The model outperforms many models such
    as the Informer and Log Transformer.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 讨论/评估：评估既包括使用这些表示进行预测，也包括异常检测。该模型在许多模型中表现优于 Informer 和 Log Transformer。
- en: 'FF Integration: We do plan on adding this paper as a baseline time-series embedding
    method likely within the next two months. Although it has been out performed by
    more recent papers its simplicity and adaptability is still nice.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: FF 集成：我们计划在接下来的两个月内将这篇论文作为基准时间序列嵌入方法添加。尽管它已被更新的论文超越，但其简洁性和适应性仍然令人满意。
- en: '[**Learning Latent Seasonal-Trend Representations for Time Series Forecasting**](https://openreview.net/forum?id=C9yUwd72yy)
    **(Neurips 2022):**'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '[**学习时间序列预测的潜在季节性-趋势表示**](https://openreview.net/forum?id=C9yUwd72yy) **（Neurips
    2022）：**'
- en: '![](../Images/dc231c643dc095d958ce202c213e4db2.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/dc231c643dc095d958ce202c213e4db2.png)'
- en: Image of the proposed LaST architecture (page 3). The model utilizes both a
    trend and seasonal encoder + decoder which create two separate representations
    that then get fed into a simple MLP predictors for the forecasting tasks.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 提议的 LaST 架构的图像（第3页）。该模型利用了趋势和季节编码器 + 解码器，生成两个独立的表示，然后输入到简单的 MLP 预测器中用于预测任务。
- en: '*Summary*: The authors create a model (LAST) to create disentangled representations
    of both the seasonality and trends using variational inference.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '*总结*：作者创建了一个模型（LAST），通过变分推断来创建季节性和趋势的解耦表示。'
- en: 'Evaluation: The authors evaluate their model on down-stream forecasting tasks
    similar to the Informer, Autformer, and other models etc. They do this by adding
    a predictor (see B in figure above) on to the representations. They also provide
    interesting plots that show visualizations of the representations. The model outperforms
    the Autoformer at several forecasting tasks as well as TS2Vec and CoST at pretty
    much all of them. It also looks like on some of the forecasting tasks it may out-perform
    the D-Linear model mentioned above.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 评价：作者在类似于 Informer、Autformer 和其他模型的下游预测任务上评估他们的模型。他们通过在表示上添加一个预测器（见上图 B）来实现这一点。他们还提供了有趣的图表，展示了表示的可视化。该模型在多个预测任务中优于
    Autoformer 以及 TS2Vec 和 CoST 几乎所有任务。在一些预测任务中，它似乎也可能超越了上述提到的 D-Linear 模型。
- en: '![](../Images/51a8117521e4588c208d9fd21a9b5c92.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/51a8117521e4588c208d9fd21a9b5c92.png)'
- en: An interesting diagram from the paper pg 9\. We can see the differences in learned
    visualized representations of the seasonality/trend.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 论文第 9 页的一个有趣图示。我们可以看到季节性/趋势的学习可视化表示的差异。
- en: '*Discussion*: Though at the moment I remain somewhat skeptical of models that
    only evaluate on the standard forecasting tasks, I do like that this model focuses
    on the representations rather than the forecasting task itself. If we look at
    some of the diagrams shown in the paper we can see that the model does seem to
    learn to distinguish between the seasonality and trend. It would be interesting
    to see the visualizations of representations of different datasets also embedded
    into the same space and if they show substantial differences.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '*讨论*：虽然目前我对仅在标准预测任务上评估的模型仍持有一定怀疑态度，但我喜欢这个模型更关注表示而不是预测任务本身。如果我们查看论文中展示的一些图表，我们可以看到模型确实似乎学会了区分季节性和趋势。如果能看到不同数据集的表示嵌入到相同空间中的可视化，并且它们展示了实质性的差异，那将会很有趣。'
- en: '*FF implementation*: In all likelihood we will add TS2Vec and before adding
    this model as that model is simpler. However, I hope to add this model at some
    point down the line as it does provide two good separate representations of the
    different temporal components. I’d guess we will probably add the model within
    the next two months.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '*FF 实现*：我们很可能会在添加 TS2Vec 之前添加这个模型，因为那个模型更简单。然而，我希望在未来的某个时点添加这个模型，因为它确实提供了对不同时间组件的两个良好的独立表示。我猜我们可能会在接下来的两个月内添加这个模型。'
- en: '[**CoST: Contrastive Learning of Disentangled Seasonal-Trend Representations
    for Time Series Forecasting**](https://openreview.net/forum?id=PilZY3omXV2) **(ICLR
    2022):**'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '[**CoST: Contrastive Learning of Disentangled Seasonal-Trend Representations
    for Time Series Forecasting**](https://openreview.net/forum?id=PilZY3omXV2) **（ICLR
    2022）：**'
- en: This was a paper that appeared earlier in 2022 at ICLR that is quite similar
    to LaST in learning seasonal and trend representations. As LaST for the most part
    has already superseded it in performance I will not go into that much description.
    But the link is above for those that want to read it.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一篇在 2022 年早些时候于 ICLR 上发表的论文，与 LaST 在学习季节性和趋势表示方面非常相似。由于 LaST 在性能上已经大部分超越了它，我不会过多描述。但对于那些想阅读的，可以查看上面的链接。
- en: Other interesting papers
  id: totrans-77
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 其他有趣的论文
- en: '[**Domain Adaptation for Time Series Forecasting via Attention Sharing**](https://arxiv.org/abs/2102.06828)
    **(ICML 2022):**'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '[**Domain Adaptation for Time Series Forecasting via Attention Sharing**](https://arxiv.org/abs/2102.06828)
    **（ICML 2022）：**'
- en: '![](../Images/c8a86171077203a5b061e73efe88f6d4.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c8a86171077203a5b061e73efe88f6d4.png)'
- en: 'Figure from: Domain Adaptation for Time Series Forecasting Via Attention Sharing
    paper page 3.'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 来源于：Domain Adaptation for Time Series Forecasting Via Attention Sharing 论文第
    3 页。
- en: 'Summary: Forecasting is challenging for DNNs when there is a lack of training
    data. I remember when I worked on COVID-19 forecasting the lack of temporal history
    initially made forecasting very difficult. Therefore I’m hopeful to see more papers
    start to address time series transfer learning scenarios. This paper utilizes
    shared attention layers for domains with rich data and then individual modules
    for the target domain(s).'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 总结：当训练数据不足时，DNN 的预测是具有挑战性的。我记得在我研究 COVID-19 预测时，最初缺乏时间历史使得预测非常困难。因此，我希望看到更多论文开始解决时间序列迁移学习场景。本文利用共享注意力层用于数据丰富的领域，然后使用单独的模块用于目标领域。
- en: 'Evaluation: The proposed model is evaluated with both synthetic and real datasets.
    In the synthetic setting they test both cold-start learning and few-shot learning
    and find their model outperforms a vanilla transformer and DeepAR. For the real
    word datasets they take a subset of the Kaggle retail dataset and electric datasets.
    The model greatly outperforms the baselines in these experiments.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 评估：所提出的模型在合成和真实数据集上都进行了评估。在合成设置中，他们测试了冷启动学习和少量样本学习，并发现他们的模型优于普通的 transformer
    和 DeepAR。对于真实世界数据集，他们使用了 Kaggle 零售数据集和电力数据集的一个子集。在这些实验中，模型显著优于基线。
- en: 'Discussion: Cold start, few shot and limited learning are extremely important
    topics yet few papers address with respect to time series. This model provides
    an important step in addressing some of these problems. That said I think they
    could have evaluated on more different limited real world datasets and compared
    against more benchmark models. Also it would be nice if the model was more easily
    to “plug-in” into existing architectures. Something nice about fine-tuning or
    regularization is that you can do it with any architecture.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 讨论：冷启动、少量样本学习和有限学习是非常重要的主题，但针对时间序列的相关论文却不多。该模型在解决这些问题上迈出了重要的一步。也就是说，我认为他们本可以在更多不同的有限真实世界数据集上进行评估，并与更多基准模型进行比较。此外，如果模型能够更容易地“插入”到现有架构中，那就更好了。微调或正则化的一个好处是你可以在任何架构上进行。
- en: 'FF Implementation: We already have some functionality in FF for transfer learning
    which greatly aided us when providing early insights into COVID. Adding this model
    could help provide a more, however the overhead looks high and the model can not
    easily plug into existing models in our ecosystem.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: FF 实现：我们在 FF 中已经有一些迁移学习的功能，这在提供 COVID 的早期见解时对我们帮助很大。添加这个模型可能会提供更多帮助，但开销似乎很高，而且该模型不能轻松插入到我们生态系统中的现有模型中。
- en: '[**When to Intervene: Learning Optimal Intervention Policies for Critical Events**](https://openreview.net/pdf?id=rP9xfRSF4F)
    **(Neurips 2022):**'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '[**何时干预：学习关键事件的最佳干预策略**](https://openreview.net/pdf?id=rP9xfRSF4F) **(Neurips
    2022)：**'
- en: '*Summary*: While not a “typical” time series paper I choose to include it on
    this list because at the of end of the day most companies are not looking to just
    forecast values or detect anomalies but to “respond” in someway. This article
    focuses on finding the optimal time to intervene before a machine fails. This
    is referred to as OTI or optimally timed intervention. The author'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '*总结*：虽然这不是一篇“典型”的时间序列论文，我选择将其纳入此列表，因为最终大多数公司不仅仅是希望预测值或检测异常，而是希望以某种方式“响应”。这篇文章专注于在机器故障之前找到最佳干预时间。这被称为
    OTI 或最佳时间干预。作者'
- en: 'Eval: Of course one of the problems with evaluating OTI is the problem is the
    accuracy of the underlying survival analysis (if it is incorrect the eval will
    also be incorrect). The authors evaluate their model against two static thresholds
    and find that it performs well. They plot the expected performance of different
    policies and the hit to miss ratio.'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 评估：当然，评估 OTI 的问题之一是基础生存分析的准确性问题（如果它不正确，评估也将不正确）。作者将他们的模型与两个静态阈值进行比较，发现表现良好。他们绘制了不同策略的预期性能以及命中与遗漏的比率。
- en: 'Discussion: This is an interesting problem and the authors propose a novel
    solution, however to me the evaluation was a little lacking. One of the reviwers
    notes that “I think that the experiments could be a lot more compelling if there
    were a plot showing the trade-off between the probability of failure and the expected
    time to intervene so that one could see visually what shape this trade-off curve
    takes”'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 讨论：这是一个有趣的问题，作者提出了一个新颖的解决方案，但在我看来，评估略显不足。一位审稿人指出，“我认为如果有一个图表显示失败概率与预期干预时间之间的权衡，这样可以直观地看到这种权衡曲线的形状，实验会更有说服力。”
- en: 'FF Integration: Both OTI and reinforcement learning on temporal data are interesting
    future potential directions to support in Flow Forecast. Unfortunately at the
    moment they are not a high priority as we are trying to make the framework rock-solid
    at forecasting, anomaly detection, and classification first. However, I definitely
    think in the future we could look into creating a more “actionable” framework
    to better support decision making.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: FF 集成：在 Flow Forecast 中，OTI 和时间数据上的强化学习都是有趣的未来潜在方向。不幸的是，目前它们不是优先级很高的方向，因为我们首先要让框架在预测、异常检测和分类上非常稳固。不过，我确实认为将来我们可以考虑创建一个更“可操作”的框架来更好地支持决策。
- en: '[**FiLM: Frequency improved Legendre Memory Model for Long-term Time Series
    Forecasting**](https://openreview.net/forum?id=zTQdHSQUQWc) **(Neurips 2022):**
    C[ode](https://github.com/tianzhou2011/FiLM/).'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '[**FiLM: 改进的频率勒让德记忆模型用于长期时间序列预测**](https://openreview.net/forum?id=zTQdHSQUQWc)
    **（Neurips 2022）：** C[ode](https://github.com/tianzhou2011/FiLM/)。'
- en: '**Adjusting for Autocorrelated Errors in Neural Networks for Time Series (Neurips
    2021)**: Code here.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '**调整神经网络中的自相关误差以用于时间序列（Neurips 2021）**: 代码在这里。'
- en: '[**Dynamic Sparse Network for Time Series Classification: Learning What to
    “See”**](https://openreview.net/forum?id=ZxOO5jfqSYw) **(Neurips 2022):**'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '[**动态稀疏网络用于时间序列分类：学习“看到”什么**](https://openreview.net/forum?id=ZxOO5jfqSYw)
    **（Neurips 2022）：**'
- en: (Fairly) Recent Datasets/Benchmarks
  id: totrans-93
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: （相对）近期的数据集/基准
- en: '[**Monash Time Series Forecasting Archive**](https://forecastingdata.org/)
    **(Neurips 2021):** Lately we have seen many deep time series all evaluated on
    the same datasets. While this is okay for basic benchmarking they often do not
    hold up on differing temporal tasks. This archive aims to form a “master list”
    of different time series datasets and provide a more authoritative benchmark.
    The repository contains over 20 different datasets spanning across a wide variety
    of industries including health, retail, ride-share, demographic, and many-more.'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**莫纳什时间序列预测档案**](https://forecastingdata.org/) **（Neurips 2021）：** 最近我们看到许多深度时间序列模型在相同的数据集上进行评估。虽然这对于基本基准测试是可以的，但它们在不同的时间任务上往往表现不佳。该档案旨在形成一个“主列表”来汇总不同的时间序列数据集，并提供更权威的基准。该库包含超过20个不同的数据集，涵盖了包括健康、零售、拼车、人口统计等多个行业。'
- en: '[**Subseasonal Forecasting Microsoft**](https://www.microsoft.com/en-us/research/project/subseasonal-climate-forecasting/)
    **(2021):** This is a publicly released dataset by Microsoft that aims to facilitate
    the use of machine learning to improve subseasonal forecasting (e.g. two to six
    weeks in the future). Subseasonal forecasting helps government agencies better
    prepare for weather events as well as farmer’s decisions. Microsoft included several
    benchmark models for the task and in general deep learning models performed quite
    poorly compared to other methods. The best DL model turned out to be a simple
    feed-forward model and the Informer performed awfully.'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**季节性预测微软**](https://www.microsoft.com/en-us/research/project/subseasonal-climate-forecasting/)
    **（2021）：** 这是微软公开发布的数据集，旨在促进机器学习在季节性预测（例如未来两到六周）中的应用。季节性预测帮助政府机构更好地准备天气事件以及农民的决策。微软为这一任务提供了几个基准模型，并且总体上，深度学习模型的表现相比其他方法相当差。最佳的深度学习模型是一个简单的前馈模型，而Informer表现非常糟糕。'
- en: '[**Revisiting Time Series Outlier Detection: Definitions and Benchmarks**](https://openreview.net/forum?id=r8IvOsnHchr)This
    paper critiques many existing anomaly/outlier detection datasets and proposes
    35 new synthetic datasets and 4 real world datasets for bench-marking purposes.'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**重新审视时间序列异常检测：定义与基准**](https://openreview.net/forum?id=r8IvOsnHchr) 这篇论文批评了许多现有的异常/离群点检测数据集，并提出了35个新的合成数据集和4个真实世界数据集用于基准测试。'
- en: '**Conclusion**'
  id: totrans-97
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**结论**'
- en: A lot has happened in the past two years in the deep-learning for time series
    space. We have seen the rise and possibly the fall of transformers for time series
    forecasting. We have seen the rise of the time series embedding methods and additional
    breakthroughs in anomaly detection as well as classification. Flow Forecast has
    continued to grow as a framework and we hope to continue to incorporate the latest
    ground-breaking research.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去两年里，深度学习在时间序列领域发生了许多变化。我们见证了变压器在时间序列预测中的兴起和可能的衰退。我们也看到了时间序列嵌入方法的兴起以及在异常检测和分类方面的进一步突破。Flow
    Forecast 继续作为一个框架增长，我们希望继续整合最新的突破性研究。
- en: We hope to add more interpretability, visualization, and benchmark methods so
    researchers and industry data-scientists alike can see where their model performs
    and where exactly the model breaks-down in performance. Additionally, we hope
    to add more forms of regularization, preprocessing, and transfer learning to boost
    performance. Maybe transformers are good for time series forecasting or maybe
    they aren’t but we will continue to support both them and their alternatives!
    As always feel free to leave any questions or insights below. Thank you for reading
    until the end.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望增加更多的可解释性、可视化和基准测试方法，以便研究人员和行业数据科学家都能清楚地了解他们的模型表现如何，以及模型性能具体在哪些方面出现了问题。此外，我们希望增加更多的正则化、预处理和迁移学习形式，以提升性能。也许变换器（transformers）对时间序列预测效果很好，或者可能不适用，但我们将继续支持它们及其替代方案！如往常一样，欢迎在下方留下任何问题或见解。感谢阅读到最后。
