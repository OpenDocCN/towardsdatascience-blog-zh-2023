- en: 'GLIP: Introducing Language-Image Pre-Training to Object Detection'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 'GLIP: å°†è¯­è¨€-å›¾åƒé¢„è®­ç»ƒå¼•å…¥ç›®æ ‡æ£€æµ‹'
- en: åŸæ–‡ï¼š[https://towardsdatascience.com/glip-introducing-language-image-pre-training-to-object-detection-5ddb601873aa?source=collection_archive---------1-----------------------#2023-09-01](https://towardsdatascience.com/glip-introducing-language-image-pre-training-to-object-detection-5ddb601873aa?source=collection_archive---------1-----------------------#2023-09-01)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡ï¼š[https://towardsdatascience.com/glip-introducing-language-image-pre-training-to-object-detection-5ddb601873aa?source=collection_archive---------1-----------------------#2023-09-01](https://towardsdatascience.com/glip-introducing-language-image-pre-training-to-object-detection-5ddb601873aa?source=collection_archive---------1-----------------------#2023-09-01)
- en: '[ğŸš€Saschaâ€™s Paper Club](https://towardsdatascience.com/tagged/saschas-paper-club)'
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '[ğŸš€Saschaâ€™s Paper Club](https://towardsdatascience.com/tagged/saschas-paper-club)'
- en: Grounded Language-Image Pre-training by L. H. Li et. al.
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Grounded Language-Image Pre-training by L. H. Li et. al.
- en: '[](https://medium.com/@SaschaKirch?source=post_page-----5ddb601873aa--------------------------------)[![Sascha
    Kirch](../Images/a0d45da9dc9c602075b2810786c660c9.png)](https://medium.com/@SaschaKirch?source=post_page-----5ddb601873aa--------------------------------)[](https://towardsdatascience.com/?source=post_page-----5ddb601873aa--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----5ddb601873aa--------------------------------)
    [Sascha Kirch](https://medium.com/@SaschaKirch?source=post_page-----5ddb601873aa--------------------------------)'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@SaschaKirch?source=post_page-----5ddb601873aa--------------------------------)[![Sascha
    Kirch](../Images/a0d45da9dc9c602075b2810786c660c9.png)](https://medium.com/@SaschaKirch?source=post_page-----5ddb601873aa--------------------------------)[](https://towardsdatascience.com/?source=post_page-----5ddb601873aa--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----5ddb601873aa--------------------------------)
    [Sascha Kirch](https://medium.com/@SaschaKirch?source=post_page-----5ddb601873aa--------------------------------)'
- en: Â·
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: Â·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F5c38dace9d5e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fglip-introducing-language-image-pre-training-to-object-detection-5ddb601873aa&user=Sascha+Kirch&userId=5c38dace9d5e&source=post_page-5c38dace9d5e----5ddb601873aa---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----5ddb601873aa--------------------------------)
    Â·9 min readÂ·Sep 1, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F5ddb601873aa&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fglip-introducing-language-image-pre-training-to-object-detection-5ddb601873aa&user=Sascha+Kirch&userId=5c38dace9d5e&source=-----5ddb601873aa---------------------clap_footer-----------)'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F5c38dace9d5e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fglip-introducing-language-image-pre-training-to-object-detection-5ddb601873aa&user=Sascha+Kirch&userId=5c38dace9d5e&source=post_page-5c38dace9d5e----5ddb601873aa---------------------post_header-----------)
    å‘è¡¨åœ¨ [Towards Data Science](https://towardsdatascience.com/?source=post_page-----5ddb601873aa--------------------------------)
    Â·9 min readÂ·Sep 1, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F5ddb601873aa&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fglip-introducing-language-image-pre-training-to-object-detection-5ddb601873aa&user=Sascha+Kirch&userId=5c38dace9d5e&source=-----5ddb601873aa---------------------clap_footer-----------)'
- en: --
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F5ddb601873aa&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fglip-introducing-language-image-pre-training-to-object-detection-5ddb601873aa&source=-----5ddb601873aa---------------------bookmark_footer-----------)'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F5ddb601873aa&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fglip-introducing-language-image-pre-training-to-object-detection-5ddb601873aa&source=-----5ddb601873aa---------------------bookmark_footer-----------)'
- en: 'Today we will dive into a paper that builds upon the great success of [CLIP](/the-clip-foundation-model-7770858b487d?sk=a7b10ba1d0c3a20ecd4adb8200a48500)
    in language-image pre-training and extends it to the task of object detection:
    GLIP â€” **G**rounded **L**anguage-**I**mage **P**re-training. We will cover the
    key concepts and findings of the paper and make them easy to understand by providing
    further context and adding annotations to images and experiment results. Letâ€™s
    go!'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: ä»Šå¤©æˆ‘ä»¬å°†æ·±å…¥æ¢è®¨ä¸€ç¯‡å»ºç«‹åœ¨[CLIP](/the-clip-foundation-model-7770858b487d?sk=a7b10ba1d0c3a20ecd4adb8200a48500)åœ¨è¯­è¨€-å›¾åƒé¢„è®­ç»ƒé¢†åŸŸå·¨å¤§æˆåŠŸçš„åŸºç¡€ä¸Šï¼Œå¹¶å°†å…¶æ‰©å±•åˆ°ç›®æ ‡æ£€æµ‹ä»»åŠ¡çš„è®ºæ–‡ï¼šGLIPâ€”â€”**G**rounded
    **L**anguage-**I**mage **P**re-trainingã€‚æˆ‘ä»¬å°†ä»‹ç»è®ºæ–‡çš„å…³é”®æ¦‚å¿µå’Œå‘ç°ï¼Œå¹¶é€šè¿‡æä¾›æ›´å¤šçš„èƒŒæ™¯ä¿¡æ¯å’Œå¯¹å›¾åƒåŠå®éªŒç»“æœçš„æ³¨é‡Šï¼Œä½¿å…¶æ˜“äºç†è§£ã€‚è®©æˆ‘ä»¬å¼€å§‹å§ï¼
- en: '![](../Images/d7ef9f457ada5a99c0936ce420e24274.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d7ef9f457ada5a99c0936ce420e24274.png)'
- en: Image created from [publication](https://arxiv.org/abs/2112.03857) by [Sascha
    Kirch](https://medium.com/@SaschaKirch)
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ç‰‡æ¥è‡ª [å‡ºç‰ˆç‰©](https://arxiv.org/abs/2112.03857) ç”± [Sascha Kirch](https://medium.com/@SaschaKirch)
    åˆ›ä½œ
- en: '**Paper:** [Grounded Language-Image Pre-training](https://arxiv.org/abs/2112.03857)
    by'
  id: totrans-12
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**è®ºæ–‡ï¼š** [Grounded Language-Image Pre-training](https://arxiv.org/abs/2112.03857)
    ä½œè€…'
- en: Liunian Harold Li et. al., 7\. Dec. 2021
  id: totrans-13
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: Liunian Harold Li ç­‰ï¼Œ2021 å¹´ 12 æœˆ 7 æ—¥
- en: ''
  id: totrans-14
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**Resources:** [GitHub](https://github.com/microsoft/GLIP)'
  id: totrans-15
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**èµ„æºï¼š** [GitHub](https://github.com/microsoft/GLIP)'
- en: ''
  id: totrans-16
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**Category:** representation learning, object detection, phrase-grounding,
    foundation models'
  id: totrans-17
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**ç±»åˆ«ï¼š** è¡¨ç¤ºå­¦ä¹ ã€ç‰©ä½“æ£€æµ‹ã€çŸ­è¯­å®šä½ã€åŸºç¡€æ¨¡å‹'
- en: ''
  id: totrans-18
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[**Other Walkthroughs**](https://medium.com/@SaschaKirch/list/paper-walkthroughs-by-sascha-kirch-89c7847da8e2)**:**'
  id: totrans-19
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '[**å…¶ä»–æ•™ç¨‹**](https://medium.com/@SaschaKirch/list/paper-walkthroughs-by-sascha-kirch-89c7847da8e2)**ï¼š**'
- en: '[[BYOL](/byol-the-alternative-to-contrastive-self-supervised-learning-5d0a26983d7c?sk=fc5a3b3a556088181d8726226862252c)]
    â€” [[CLIP](/the-clip-foundation-model-7770858b487d?sk=a7b10ba1d0c3a20ecd4adb8200a48500)]
    â€” [[Depth Anything](https://medium.com/towards-data-science/depth-anything-a-foundation-model-for-monocular-depth-estimation-8a7920b5c9cc?sk=fc6197edd68e6137c3396c83e50f65cb)]
    â€” [[Segment Anything](/segment-anything-promptable-segmentation-of-arbitrary-objects-f28958c5612d?sk=bd1311a6d8b1e0e6d3369d536dba0700)]
    â€” [[DINO](/segment-anything-promptable-segmentation-of-arbitrary-objects-f28958c5612d?sk=bd1311a6d8b1e0e6d3369d536dba0700)]
    â€” [[DDPM](/the-rise-of-diffusion-models-a-new-era-of-generative-deep-learning-3ef4779f6e1b?sk=8c178422a977c6f49ec24b13502be4fd)]'
  id: totrans-20
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '[[BYOL](/byol-the-alternative-to-contrastive-self-supervised-learning-5d0a26983d7c?sk=fc5a3b3a556088181d8726226862252c)]
    â€” [[CLIP](/the-clip-foundation-model-7770858b487d?sk=a7b10ba1d0c3a20ecd4adb8200a48500)]
    â€” [[Depth Anything](https://medium.com/towards-data-science/depth-anything-a-foundation-model-for-monocular-depth-estimation-8a7920b5c9cc?sk=fc6197edd68e6137c3396c83e50f65cb)]
    â€” [[Segment Anything](/segment-anything-promptable-segmentation-of-arbitrary-objects-f28958c5612d?sk=bd1311a6d8b1e0e6d3369d536dba0700)]
    â€” [[DINO](/segment-anything-promptable-segmentation-of-arbitrary-objects-f28958c5612d?sk=bd1311a6d8b1e0e6d3369d536dba0700)]
    â€” [[DDPM](/the-rise-of-diffusion-models-a-new-era-of-generative-deep-learning-3ef4779f6e1b?sk=8c178422a977c6f49ec24b13502be4fd)]'
- en: Outline
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å¤§çº²
- en: Context & Background
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: èƒŒæ™¯ä¸ä¸Šä¸‹æ–‡
- en: Claimed Contributions
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å£°ç§°çš„è´¡çŒ®
- en: Method
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æ–¹æ³•
- en: Experiments
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å®éªŒ
- en: Further Readings & Resources
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: è¿›ä¸€æ­¥é˜…è¯»ä¸èµ„æº
- en: Context & Background
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: èƒŒæ™¯ä¸ä¸Šä¸‹æ–‡
