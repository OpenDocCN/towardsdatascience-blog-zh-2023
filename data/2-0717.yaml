- en: Deploying LLMs On Amazon SageMaker With DJL Serving
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在 Amazon SageMaker 上使用 DJL Serving 部署 LLMs
- en: 原文：[https://towardsdatascience.com/deploying-llms-on-amazon-sagemaker-with-djl-serving-8220e3cfad0c](https://towardsdatascience.com/deploying-llms-on-amazon-sagemaker-with-djl-serving-8220e3cfad0c)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/deploying-llms-on-amazon-sagemaker-with-djl-serving-8220e3cfad0c](https://towardsdatascience.com/deploying-llms-on-amazon-sagemaker-with-djl-serving-8220e3cfad0c)
- en: Deploy BART on Amazon SageMaker Real-Time Inference
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在 Amazon SageMaker 实时推理中部署 BART
- en: '[](https://ram-vegiraju.medium.com/?source=post_page-----8220e3cfad0c--------------------------------)[![Ram
    Vegiraju](../Images/07d9334e905f710d9f3c6187cf69a1a5.png)](https://ram-vegiraju.medium.com/?source=post_page-----8220e3cfad0c--------------------------------)[](https://towardsdatascience.com/?source=post_page-----8220e3cfad0c--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----8220e3cfad0c--------------------------------)
    [Ram Vegiraju](https://ram-vegiraju.medium.com/?source=post_page-----8220e3cfad0c--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://ram-vegiraju.medium.com/?source=post_page-----8220e3cfad0c--------------------------------)[![Ram
    Vegiraju](../Images/07d9334e905f710d9f3c6187cf69a1a5.png)](https://ram-vegiraju.medium.com/?source=post_page-----8220e3cfad0c--------------------------------)[](https://towardsdatascience.com/?source=post_page-----8220e3cfad0c--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----8220e3cfad0c--------------------------------)
    [Ram Vegiraju](https://ram-vegiraju.medium.com/?source=post_page-----8220e3cfad0c--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----8220e3cfad0c--------------------------------)
    ·8 min read·Jun 7, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----8220e3cfad0c--------------------------------)
    ·8 分钟阅读·2023 年 6 月 7 日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/27d3d2701b592cc5346c24eb2c67afe6.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/27d3d2701b592cc5346c24eb2c67afe6.png)'
- en: Image from [Unsplash](https://unsplash.com/photos/CejqWHRRXUQ)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：[Unsplash](https://unsplash.com/photos/CejqWHRRXUQ)
- en: Large Language Models (LLMs) and Generative AI continue to take over the Machine
    Learning and general tech space in 2023\. With the LLM expansion has come an influx
    of new models that continue to improve at a stunning rate.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在 2023 年，大语言模型（LLMs）和生成式 AI 继续在机器学习和一般技术领域中占据主导地位。随着 LLM 的扩展，新模型不断涌现，并以惊人的速度不断改进。
- en: While the accuracy and performance of these models are incredible, they have
    their own set of challenges in terms of hosting these models. Without model hosting,
    it is hard to recognize the value that these LLMs provide in real-world applications.
    What are the specific challenges with LLM hosting and performance tuning?
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这些模型的准确性和性能令人难以置信，但在托管这些模型时仍面临着一系列挑战。如果没有模型托管，很难识别这些大语言模型（LLMs）在实际应用中所提供的价值。LLM
    托管和性能调优的具体挑战是什么？
- en: How can we load these larger models that are scaling up to past 100s of GBs
    in size?
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们如何加载这些体积达到数百 GB 的大型模型？
- en: How can we properly apply model partitioning techniques to efficiently utilize
    hardware while not compromising on model accuracy?
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们如何正确应用模型分区技术，以高效利用硬件而不影响模型的准确性？
- en: How can we fit these models on a singular GPU or multiple?
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们如何将这些模型适配到单个 GPU 或多个 GPU 上？
- en: 'These are all challenging questions that are addressed and abstracted out through
    a model server known as [DJL Serving](http://djl.ai/serving/). DJL Serving is
    a high performance universal solution that integrates directly with various model
    partitioning frameworks such as the following: [HuggingFace Accelerate](https://huggingface.co/docs/accelerate/index),
    [DeepSpeed](https://github.com/microsoft/DeepSpeed), and [FasterTransformers](https://github.com/NVIDIA/FasterTransformer).
    With DJL Serving you can configure your serving stack to utilize these partitioning
    frameworks to optimize inference at scale across multiple GPUs with these larger
    models.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 这些都是具有挑战性的问题，通过一个被称为 [DJL Serving](http://djl.ai/serving/) 的模型服务器进行解决和抽象。DJL
    Serving 是一个高性能的通用解决方案，可以直接与各种模型分区框架集成，例如：[HuggingFace Accelerate](https://huggingface.co/docs/accelerate/index)、[DeepSpeed](https://github.com/microsoft/DeepSpeed)
    和 [FasterTransformers](https://github.com/NVIDIA/FasterTransformer)。使用 DJL Serving，你可以配置你的服务堆栈，利用这些分区框架在多个
    GPU 上优化大模型的推理。
- en: In today’s article in specific we explore one of the smaller language models
    in [BART](https://huggingface.co/facebook/bart-large) for Feature Extraction.
    We will showcase how you can use DJL Serving to configure your serving stack and
    host a [HuggingFace Model](https://huggingface.co/) of your choice. This example
    can serve as a template to build upon and utilize the model partitioning frameworks
    aforementioned. We will then take our DJL specific code and integrate it with
    SageMaker to create a [Real-Time Endpoint](https://docs.aws.amazon.com/sagemaker/latest/dg/realtime-endpoints.html)
    that you can use for inference.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在今天的文章中，我们特别探讨了 [BART](https://huggingface.co/facebook/bart-large) 中的一个较小的语言模型用于特征提取。我们将展示如何使用
    DJL Serving 配置你的服务堆栈并托管你选择的[HuggingFace 模型](https://huggingface.co/)。这个示例可以作为一个模板，构建和利用上述的模型分区框架。然后我们将把
    DJL 特定代码与 SageMaker 集成，创建一个[实时端点](https://docs.aws.amazon.com/sagemaker/latest/dg/realtime-endpoints.html)，供你进行推理使用。
- en: '**NOTE**: For those of you new to AWS, make sure you make an account at the
    following [link](https://aws.amazon.com/console/) if you want to follow along.
    The article also assumes an intermediate understanding of SageMaker Deployment,
    I would suggest following this [article](https://aws.amazon.com/blogs/machine-learning/part-2-model-hosting-patterns-in-amazon-sagemaker-getting-started-with-deploying-real-time-models-on-sagemaker/)
    for understanding Deployment/Inference more in depth.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意**：对于那些新接触 AWS 的朋友，如果你想跟随本文操作，请确保在以下[链接](https://aws.amazon.com/console/)上创建一个账户。本文假设你对
    SageMaker 部署有中级理解，建议你阅读这篇[文章](https://aws.amazon.com/blogs/machine-learning/part-2-model-hosting-patterns-in-amazon-sagemaker-getting-started-with-deploying-real-time-models-on-sagemaker/)以更深入理解部署/推理。'
- en: What is a Model Server? What Model Servers Does Amazon SageMaker Support?
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 什么是模型服务器？Amazon SageMaker 支持哪些模型服务器？
- en: '[Model Servers](https://thenewstack.io/model-server-the-critical-building-block-of-mlops/)
    at a very basic premise are “inference as a service”. We need an easy way to expose
    our models via an API, but these model servers take care of the grunt work behind
    the scenes. These model servers load and unload our model artifacts and provide
    the runtime environment for your ML models that you are hosting. These model servers
    can also be tuned depending on what they expose to the user. For example, TensorFlow
    Serving gives the choice of [gRPC vs REST](https://medium.com/@avidaneran/tensorflow-serving-rest-vs-grpc-e8cef9d4ff62)
    for your API calls.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '[模型服务器](https://thenewstack.io/model-server-the-critical-building-block-of-mlops/)的基本概念是“作为服务的推理”。我们需要一种简便的方法通过
    API 暴露我们的模型，而这些模型服务器则处理背后的繁重工作。这些模型服务器加载和卸载我们的模型工件，并提供托管 ML 模型所需的运行时环境。这些模型服务器也可以根据它们向用户暴露的内容进行调优。例如，TensorFlow
    Serving 允许你选择[gRPC 还是 REST](https://medium.com/@avidaneran/tensorflow-serving-rest-vs-grpc-e8cef9d4ff62)进行
    API 调用。'
- en: 'Amazon SageMaker integrates with a variety of these different model servers
    that are also exposed via the different [Deep Learning Containers](https://github.com/aws/deep-learning-containers/blob/master/available_images.md)
    that AWS provides. Some of these model servers include the following:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: Amazon SageMaker 与各种不同的模型服务器集成，这些服务器也通过 AWS 提供的不同[深度学习容器](https://github.com/aws/deep-learning-containers/blob/master/available_images.md)进行暴露。这些模型服务器包括以下几种：
- en: '[TensorFlow Serving](https://www.tensorflow.org/tfx/guide/serving)'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[TensorFlow Serving](https://www.tensorflow.org/tfx/guide/serving)'
- en: '[TorchServe](https://pytorch.org/serve/)'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[TorchServe](https://pytorch.org/serve/)'
- en: '[Multi-Model Server (MMS)](https://github.com/awslabs/multi-model-server)'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[多模型服务器 (MMS)](https://github.com/awslabs/multi-model-server)'
- en: '[Triton Inference Server](https://github.com/triton-inference-server/server)'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Triton Inference Server](https://github.com/triton-inference-server/server)'
- en: '[DJL Serving](https://github.com/deepjavalibrary/djl-serving)'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[DJL Serving](https://github.com/deepjavalibrary/djl-serving)'
- en: For this specific example we will utilize DJL Serving as it is tailored for
    Large Language Model Hosting with it’s different model partitioning frameworks
    it has enabled. That does not mean the server is limited to LLMs, you can also
    utilize it for other models as long as you are properly configuring the environment
    to install and load up any other dependencies.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个具体示例中，我们将使用 DJL Serving，因为它针对大语言模型托管进行了不同的模型分区框架。这并不意味着服务器仅限于 LLM，你也可以将其用于其他模型，只要你正确配置环境以安装和加载其他依赖项。
- en: At a very high level overview depending on the model server that you are using
    the way you bake and shape your artifacts that you provide the server is the only
    difference along with whatever model frameworks and environments they support
    as well.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 从很高的层次来看，具体使用的模型服务器会影响你提供给服务器的工件的构建和形状，这是唯一的区别，还有它们支持的模型框架和环境。
- en: DJL Serving vs JumpStart
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: DJL Serving 与 JumpStart
- en: In my previous [article](/deploying-cohere-language-models-on-amazon-sagemaker-23a3f79639b1)
    we explored how we could deploy [Cohere’s Language Models](https://cohere.com/)
    via [SageMaker JumpStart](https://awstip.com/automl-beyond-with-sagemaker-jumpstart-9962ffc4bcd1).
    Why not use SageMaker JumpStart in this case? At the moment not all LLMs are supported
    by SageMaker JumpStart. In the case that there’s a specific LLM that JumpStart
    does not support it makes sense to use DJL Serving.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在我之前的[文章](/deploying-cohere-language-models-on-amazon-sagemaker-23a3f79639b1)中，我们探讨了如何通过
    [SageMaker JumpStart](https://awstip.com/automl-beyond-with-sagemaker-jumpstart-9962ffc4bcd1)
    部署 [Cohere 的语言模型](https://cohere.com/)。为什么这次不使用 SageMaker JumpStart？目前并非所有 LLM
    都得到 SageMaker JumpStart 的支持。如果有特定的 LLM JumpStart 不支持，那么使用 DJL Serving 就是明智的选择。
- en: The other major use case for DJL Serving is when it comes to customization and
    performance optimization. With JumpStart you are constrained to the model offering
    and whatever limitations exist with the container that’s already been pre-baked
    for you. With DJL there is more code work at a container level but you can apply
    performance optimization techniques of your choice with the different partitioning
    frameworks that exist.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: DJL Serving 的另一个主要用例是当涉及到自定义和性能优化时。使用 JumpStart，你受限于模型提供以及已为你预先准备的容器的限制。使用 DJL，虽然在容器级别需要更多的代码工作，但你可以使用不同的分区框架应用你选择的性能优化技术。
- en: DJL Serving Setup
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: DJL Serving 设置
- en: For this code example we will be utilizing a ml.c5.9xlarge [SageMaker Classic
    Notebook Instance](https://docs.aws.amazon.com/sagemaker/latest/dg/nbi.html) with
    a conda_amazonei_pytorch_latest_p37 kernel for development.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个代码示例，我们将使用一个 ml.c5.9xlarge [SageMaker Classic Notebook 实例](https://docs.aws.amazon.com/sagemaker/latest/dg/nbi.html)
    和一个 conda_amazonei_pytorch_latest_p37 内核进行开发。
- en: Before we can get to DJL Serving Setup we can quickly explore the BART model
    itself. This model can be found in the HuggingFace Model Hub and can be utilized
    for a variety of tasks such as Feature Extraction and Summarization. The following
    code snippet is how you can utilize the BART Tokenizer and Model for a sample
    inference locally.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们进入 DJL Serving 设置之前，我们可以快速探索 BART 模型。这个模型可以在 HuggingFace Model Hub 中找到，并可以用于各种任务，如特征提取和摘要。以下代码片段展示了如何在本地使用
    BART Tokenizer 和 Model 进行示例推理。
- en: '[PRE0]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Now we can map this model to DJL Serving with a few specific files. First we
    define a serving.properties file which essentially defines the configuration for
    your model deployment. In this case we specify a few parameters.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以通过几个特定的文件将此模型映射到 DJL Serving。首先，我们定义一个 serving.properties 文件，该文件本质上定义了模型部署的配置。在这种情况下，我们指定了一些参数。
- en: '**Engine**: We are utilizing Python for the DJL Engine, the other options here
    are also DeepSpeed, FasterTransformers, and Accelerate.'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**引擎**：我们为 DJL 引擎使用 Python，其他选项还包括 DeepSpeed、FasterTransformers 和 Accelerate。'
- en: '**Model_ID**: For the HuggingFace Hub each model has a model_id that can be
    used as an identifier, we can feed this into our model script for model loading.'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型_ID**：对于 HuggingFace Hub，每个模型都有一个可以用作标识符的 model_id，我们可以将其传递到我们的模型脚本中用于模型加载。'
- en: '**Task**: For HuggingFace specific models you can include a task as many of
    these models can support various language tasks, in this case we specify Feature
    Extraction.'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**任务**：对于 HuggingFace 特定的模型，你可以包含一个任务，因为这些模型可以支持各种语言任务，在这种情况下，我们指定了特征提取。'
- en: '[PRE1]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Other configurations you can specify for DJL include: tensor_parallel degree,
    minimum and maximum workers on a per model basis. For an extensive list of properties
    you can configure please refer to the following [documentation](https://docs.aws.amazon.com/sagemaker/latest/dg/large-model-inference-configuration.html).'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以为 DJL 指定的其他配置包括：tensor_parallel degree，每个模型的最小和最大工作线程。有关可以配置的属性的详细列表，请参阅以下[文档](https://docs.aws.amazon.com/sagemaker/latest/dg/large-model-inference-configuration.html)。
- en: The next files we provide are our actual model artifact and a requirements.txt
    for any additional libraries you will utilize in your inference script.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来我们提供的是实际的模型工件和一个 requirements.txt 文件，用于你在推理脚本中使用的任何额外库。
- en: '[PRE2]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: In this case we have no model artifacts as we will directly load the model from
    the HuggingFace Hub in our inference script.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们没有模型工件，因为我们将在推断脚本中直接从 HuggingFace Hub 加载模型。
- en: In our Inference Script (model.py) we can create a class that captures both
    model loading and inference.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的推断脚本（model.py）中，我们可以创建一个类来捕获模型加载和推断。
- en: '[PRE3]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Our initialize method will parse our serving.properties file and load the BART
    Model and Tokenizer from the HuggingFace Model Hub. The properties object essentially
    contains everything you have defined in the serving.properties file.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的初始化方法将解析我们的 serving.properties 文件，并从 HuggingFace 模型库加载 BART 模型和分词器。属性对象本质上包含了你在
    serving.properties 文件中定义的所有内容。
- en: '[PRE4]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: We then define an inference method which accepts a string input and tokenizes
    the text for the BART Model inference that we can copy from the local inference
    example above.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们定义一个推断方法，该方法接受一个字符串输入，并对文本进行分词，以进行 BART 模型的推断，我们可以从上面的本地推断示例中复制。
- en: '[PRE5]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: We then instantiate this class and capture all of this in the “handle” method.
    By default for DJL Serving this is the method that the handler parses for in the
    inference script.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 我们然后实例化这个类，并在“handle”方法中捕获所有这些。默认情况下，对于 DJL Serving，这是处理程序在推断脚本中解析的方法。
- en: '[PRE6]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: We now have all the necessary artifacts on the DJL Serving side and can configure
    these files to fit the SageMaker constructs to create a Real-Time Endpoint.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们在 DJL Serving 端已经拥有所有必要的工件，并可以配置这些文件以适应 SageMaker 构造，以创建实时终端节点。
- en: SageMaker Endpoint Creation & Inference
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: SageMaker 终端节点创建与推断
- en: 'For creating a SageMaker Endpoint the process is very similar to that of other
    Model Servers such as MMS. We need two artifacts to create a SageMaker Model Entity:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 创建 SageMaker 终端节点的过程与其他模型服务器（如 MMS）非常相似。我们需要两个工件来创建一个 SageMaker 模型实体。
- en: 'model.tar.gz: This will contain our DJL specific files and we organize these
    in a format that the model server expects.'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'model.tar.gz: 这将包含我们特定于 DJL 的文件，我们将这些文件组织成模型服务器所期望的格式。'
- en: '[Container Image](https://aws.plainenglish.io/how-to-retrieve-amazon-sagemaker-deep-learning-images-ff4a5866299e):
    SageMaker Inference always expects a container, in this case we use the DJL Deepseed
    image provided and maintained by AWS.'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[容器镜像](https://aws.plainenglish.io/how-to-retrieve-amazon-sagemaker-deep-learning-images-ff4a5866299e):
    SageMaker 推断始终期望一个容器，在这种情况下我们使用 AWS 提供和维护的 DJL Deepseed 镜像。'
- en: We can create our model tarball, upload it to S3 and then retrieve our image
    to get the artifacts ready for Inference.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以创建模型 tarball，将其上传到 S3，然后检索镜像以准备推断所需的工件。
- en: '[PRE7]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: We can then utilize the [Boto3 SDK](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/sagemaker.html)
    to conduct our [Model](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/sagemaker/client/create_model.html),
    [Endpoint Configuration](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/sagemaker/client/create_endpoint_config.html),
    and [Endpoint](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/sagemaker/client/create_endpoint.html)
    creation. The only change from the usual three API calls is that in the Endpoint
    Configuration API call we specify [Model Download Timeout](https://docs.aws.amazon.com/sagemaker/latest/dg/large-model-inference-hosting.html)
    and [Container Health Check Timeout](https://docs.aws.amazon.com/sagemaker/latest/dg/large-model-inference-hosting.html)
    parameters to higher numbers as we are dealing with a larger model in this case.
    We also utilize a g5 family instance for the additional GPU compute power. For
    most LLMs, GPUs are mandatory to be able to host models at this size and scale.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以利用[Boto3 SDK](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/sagemaker.html)来进行[模型](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/sagemaker/client/create_model.html)、[终端节点配置](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/sagemaker/client/create_endpoint_config.html)和[终端节点](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/sagemaker/client/create_endpoint.html)的创建。唯一不同于通常的三个
    API 调用的是，在终端节点配置 API 调用中，我们将[模型下载超时](https://docs.aws.amazon.com/sagemaker/latest/dg/large-model-inference-hosting.html)和[容器健康检查超时](https://docs.aws.amazon.com/sagemaker/latest/dg/large-model-inference-hosting.html)参数设置为更高的值，因为我们在此情况下处理的是一个更大的模型。我们还利用了
    g5 系列实例以获得额外的 GPU 计算能力。对于大多数 LLM 来说，GPU 是必需的，以便能够在这种规模下托管模型。
- en: '[PRE8]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Once the endpoint has been created we can perform a sample inference utilizing
    the [invoke_endpoint](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/sagemaker-runtime/client/invoke_endpoint.html)
    API call and you should see a numpy array returned.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦创建了端点，我们可以利用 [invoke_endpoint](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/sagemaker-runtime/client/invoke_endpoint.html)
    API 调用进行样本推理，你应该会看到返回一个 numpy 数组。
- en: '[PRE9]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Additional Resources & Conclusion
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附加资源与结论
- en: '[](https://github.com/RamVegiraju/SageMaker-Deployment/tree/master/LLM-Hosting/DJL-Serving/BART?source=post_page-----8220e3cfad0c--------------------------------)
    [## SageMaker-Deployment/LLM-Hosting/DJL-Serving/BART at master · RamVegiraju/SageMaker-Deployment'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '[## SageMaker-Deployment/LLM-Hosting/DJL-Serving/BART at master · RamVegiraju/SageMaker-Deployment](https://github.com/RamVegiraju/SageMaker-Deployment/tree/master/LLM-Hosting/DJL-Serving/BART?source=post_page-----8220e3cfad0c--------------------------------)'
- en: You can't perform that action at this time. You signed in with another tab or
    window. You signed out in another tab or…
  id: totrans-63
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 目前无法执行该操作。你在另一个标签页或窗口中登录了。你在另一个标签页或窗口中注销了……
- en: github.com](https://github.com/RamVegiraju/SageMaker-Deployment/tree/master/LLM-Hosting/DJL-Serving/BART?source=post_page-----8220e3cfad0c--------------------------------)
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '[github.com](https://github.com/RamVegiraju/SageMaker-Deployment/tree/master/LLM-Hosting/DJL-Serving/BART?source=post_page-----8220e3cfad0c--------------------------------)'
- en: You can find the code for the entire example at the link above. LLM Hosting
    is still a growing space with many challenges that DJL Serving can help simplify.
    Paired with the hardware and optimizations SageMaker provides this can help enhance
    your inference performance for LLMs.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在上面的链接找到整个示例的代码。LLM Hosting 仍然是一个不断发展的领域，面临许多挑战，DJL Serving 可以帮助简化这些挑战。结合
    SageMaker 提供的硬件和优化，这可以帮助提升你在 LLMs 上的推理性能。
- en: As always feel free to leave any feedback or questions around the article. Thank
    you for reading and stay tuned for more content in the LLM space.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 一如既往，欢迎随时对文章提出反馈或问题。感谢阅读，敬请关注更多关于 LLM 领域的内容。
- en: '*If you enjoyed this article feel free to connect with me on* [*LinkedIn*](https://www.linkedin.com/in/ram-vegiraju-81272b162/)
    *and subscribe to my Medium* [*Newsletter*](https://ram-vegiraju.medium.com/subscribe)*.
    If you’re new to Medium, sign up using my* [*Membership Referral*](https://ram-vegiraju.medium.com/membership)*.*'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '*如果你喜欢这篇文章，可以通过* [*LinkedIn*](https://www.linkedin.com/in/ram-vegiraju-81272b162/)
    *与我联系，并订阅我的 Medium* [*Newsletter*](https://ram-vegiraju.medium.com/subscribe)*。如果你是
    Medium 新手，可以使用我的* [*Membership Referral*](https://ram-vegiraju.medium.com/membership)*进行注册。*'
