- en: Hierarchical Transformers — part 1
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分层 Transformer — 第1部分
- en: 原文：[https://towardsdatascience.com/hierarchical-transformers-54f6d59fa8fc](https://towardsdatascience.com/hierarchical-transformers-54f6d59fa8fc)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/hierarchical-transformers-54f6d59fa8fc](https://towardsdatascience.com/hierarchical-transformers-54f6d59fa8fc)
- en: More efficient language models
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 更高效的语言模型
- en: '[](https://medium.com/@mina.ghashami?source=post_page-----54f6d59fa8fc--------------------------------)[![Mina
    Ghashami](../Images/745f53b94f5667a485299b49913c7a21.png)](https://medium.com/@mina.ghashami?source=post_page-----54f6d59fa8fc--------------------------------)[](https://towardsdatascience.com/?source=post_page-----54f6d59fa8fc--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----54f6d59fa8fc--------------------------------)
    [Mina Ghashami](https://medium.com/@mina.ghashami?source=post_page-----54f6d59fa8fc--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@mina.ghashami?source=post_page-----54f6d59fa8fc--------------------------------)[![Mina
    Ghashami](../Images/745f53b94f5667a485299b49913c7a21.png)](https://medium.com/@mina.ghashami?source=post_page-----54f6d59fa8fc--------------------------------)[](https://towardsdatascience.com/?source=post_page-----54f6d59fa8fc--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----54f6d59fa8fc--------------------------------)
    [Mina Ghashami](https://medium.com/@mina.ghashami?source=post_page-----54f6d59fa8fc--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----54f6d59fa8fc--------------------------------)
    ·9 min read·Oct 4, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----54f6d59fa8fc--------------------------------)
    ·阅读时间 9 分钟·2023年10月4日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/1b3d6e9ce3c3e6c0dec3c8326bd0faf9.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1b3d6e9ce3c3e6c0dec3c8326bd0faf9.png)'
- en: Image from [unsplash.com](https://unsplash.com/photos/x22UAIdif_k)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源于 [unsplash.com](https://unsplash.com/photos/x22UAIdif_k)
- en: 'In this article, we look at hierarchical transformers: what they are, how they
    work, how they differ from standard Transformers and what are their benefits.
    Let’s get started.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们将探讨分层 Transformer：它们是什么、如何工作、与标准 Transformer 的不同之处以及它们的好处。让我们开始吧。
- en: What Are Hierarchical Transformers
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是分层 Transformer
- en: The “hierarchical Transformer” refers to a transformer architecture that operates
    on multiple scales or resolutions of the input sequence.
  id: totrans-10
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “分层 Transformer”指的是在输入序列的多个尺度或分辨率上运行的 Transformer 架构。
- en: '**Why do we need hierarchical transformers?**'
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**我们为什么需要分层 Transformer？**'
- en: Standard Transformers as amazing that they are are very time consuming. The
    attention mechanism inside Transformers takes O(n²) to run on an input sequence
    of n tokens. This means Transformers are not practical for long sequences. One
    way to address this inefficiencies, is to have hierarchical transformers. Is it
    the only way? no! another way is to have improve efficiency of attention mechanism.
    But this is a topic for another day.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 标准 Transformer 尽管非常出色，但在时间上非常消耗。Transformer 内部的注意力机制在处理 n 个标记的输入序列时需要 O(n²)
    的时间。这意味着 Transformer 对于长序列不够实用。解决这种低效的一个方法是使用分层 Transformer。它是唯一的解决方案吗？不！另一个方法是提高注意力机制的效率。但这是另一个话题。
- en: How does hierarchy in Transformers help?
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分层结构如何帮助 Transformer？
- en: Hierarchical Transformers enable the model to operate on different levels of
    the input e.g. words, sentences, paragraphs etc. This matches how humans process
    text too. This forces attention over different hierarchies to model relationships
    between entities at different granularities.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 分层 Transformer 使模型能够在不同级别的输入上进行操作，例如单词、句子、段落等。这与人类处理文本的方式相匹配。这迫使注意力机制跨越不同的层次，以建模不同粒度的实体之间的关系。
- en: There are many methods for hierarchical transformers; in this article, we strive
    to intuitively explain one of these methods.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 分层 Transformer 有很多方法；在本文中，我们力图直观地解释其中一种方法。
- en: The Hourglass Transformer
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Hourglass Transformer
- en: The *Hourglass* [1] network is a joint work by OpenAI, Google Research and University
    of Warsaw. It is a hierarchical autoregressive Transformer that takes an input
    sequence, and forms a hierarchy of sequences from full resolution to smaller and
    smaller scales; at each scale it processes the sequence within that resolution,
    and finally it expands the sequence back to the full size. This makes the model
    more efficient as the shorter sequences are cheaper to process.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '*Hourglass* [1] 网络是 OpenAI、Google Research 和华沙大学的共同研究成果。它是一种分层自回归 Transformer，接收一个输入序列，并形成从全分辨率到越来越小尺度的序列层次；在每个尺度上，它处理该分辨率内的序列，最后将序列扩展回全尺寸。这使得模型更加高效，因为较短的序列处理成本较低。'
- en: Note that in autoregressive transformers the first and last layer at the very
    least have to operate on full-scale of the input. This is because the first layer
    is processing the input so it has to operate at full-scale, and the last layer
    (since model is autoregressive) is producing an output so has to operate on full-scale
    again.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在自回归 Transformer 中，第一层和最后一层至少必须在输入的全尺度上操作。这是因为第一层处理输入，所以必须在全尺度上操作，最后一层（由于模型是自回归的）生成输出，因此必须再次在全尺度上操作。
- en: Let’s take a look at this architecture. The image below, demonstrates *Hourglass:*
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看这个架构。下图展示了*沙漏：*
- en: '![](../Images/5c2850b54eb73ade9bd07ff1bfca40be.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5c2850b54eb73ade9bd07ff1bfca40be.png)'
- en: “Hourglass” architecture — image taken from [[1](https://arxiv.org/pdf/2110.13711.pdf)]
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: “沙漏”架构——图片来自[[1](https://arxiv.org/pdf/2110.13711.pdf)]
- en: We describe it step by step. We start from the left of above image where the
    input tokens are depicted as a grey box.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 我们一步一步描述。我们从上面图片的左侧开始，其中输入令牌被表示为灰色框。
- en: The model first processes the full input sequence using standard Transformer
    layers. So if the input sequence (shown as grey box titled “input tokens”) has
    L tokens, then they are all passed through standard Transformer layers (depicted
    in blue and called as *pre-vanilla layers).* These layers output L embedding vectors
    one for each token.
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 模型首先使用标准的 Transformer 层处理完整的输入序列。因此，如果输入序列（显示为标题为“输入令牌”的灰色框）有 L 个令牌，则它们都通过标准
    Transformer 层（用蓝色表示，称为*预处理层*）。这些层输出每个令牌的 L 个嵌入向量。
- en: The pre-vanilla layers are Transformer layers operating on the full token-level
    sequence before shortening.
  id: totrans-24
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 预先处理的层是对缩短前的完整令牌级序列进行操作的 Transformer 层。
- en: So if the task is “language modeling on text”, the input to pre-vanilla layers
    is a sequence of subword tokens representing the text. If the task is “image generation”,
    the input would be pixel values flattened into a sequence.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，如果任务是“文本语言建模”，则输入到预处理层的是一个表示文本的子词令牌序列。如果任务是“图像生成”，则输入将是展平的像素值序列。
- en: '![](../Images/87f94c52be0937da9a9f537ff96842c0.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/87f94c52be0937da9a9f537ff96842c0.png)'
- en: Step 1 — hourglass architecture — Image from [[1](https://arxiv.org/pdf/2110.13711.pdf)]
    modified by the author
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 第 1 步——沙漏架构——图片来自[[1](https://arxiv.org/pdf/2110.13711.pdf)]，由作者修改
- en: 2\. In the second step, the model shortens the sequence from *L* tokens into
    fewer tokens. This step is shown as an orange trapezius and the “shortening factor”
    is stated as *sf = k₁*. Note *sf* stands for “shortening factor”, and if it is
    set to *k₁* it means every *k₁* tokens is merged into 1 token. The shortening
    happens via using some sort of pooling operations such as *average pooling*, *linear
    pooling,* or *attention pooling*. We will talk about them soon. The output of
    this step is *L/k₁* tokens. This step is also known as down *down-sampling step.*
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 2\. 在第二步中，模型将序列从*L* 个令牌缩短为更少的令牌。这一步显示为橙色梯形，并且“缩短因子”表示为*sf = k₁*。注意*sf* 代表“缩短因子”，如果设置为*k₁*，则意味着每*k₁*
    个令牌合并为 1 个令牌。缩短通过使用某种池化操作来实现，例如*平均池化*、*线性池化* 或 *注意力池化*。我们很快会讨论这些。此步骤的输出为*L/k₁*
    个令牌。此步骤也称为下*下采样步骤*。
- en: '![](../Images/831300273cf5532bcb62ddf33f13a74e.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/831300273cf5532bcb62ddf33f13a74e.png)'
- en: Step 2— hourglass architecture — Image from [[1](https://arxiv.org/pdf/2110.13711.pdf)]
    modified by the author
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 第 2 步——沙漏架构——图片来自[[1](https://arxiv.org/pdf/2110.13711.pdf)]，由作者修改
- en: 3\. The shortened sequence is processed by more Transformer layers called *shortened
    layers*. In the image they are shown by yellow boxes. These layers output updated
    embeddings for the tokens.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 3\. 缩短的序列经过更多的 Transformer 层，称为*缩短层*。在图中，它们由黄色框表示。这些层输出更新后的令牌嵌入。
- en: '![](../Images/8e94cfdc3920c0eb7e86066cce58595e.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8e94cfdc3920c0eb7e86066cce58595e.png)'
- en: Step 3— hourglass architecture — Image from [[1](https://arxiv.org/pdf/2110.13711.pdf)]
    modified by the author
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 第 3 步——沙漏架构——图片来自[[1](https://arxiv.org/pdf/2110.13711.pdf)]，由作者修改
- en: 4\. If there is more shortening left to do, we simply repeat the process. In
    the image below, in the second orange trapezius, we are shortening the input sequence
    by a factor of *sf = k₂.* This will merge every *k₂* tokens into 1 token, and
    so will output *L/(k₁.k₂)* tokens. The outputted tokens get passed through more
    *shortened layers* shown in light yellow.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 4\. 如果还有更多的缩短需要完成，我们简单地重复这个过程。在下图中，在第二个橙色梯形中，我们将输入序列缩短一个*sf = k₂*的因子。这将把每个*k₂*
    个令牌合并为 1 个令牌，因此将输出*L/(k₁.k₂)* 个令牌。输出的令牌经过更多的*缩短层*，这些层用淡黄色表示。
- en: '![](../Images/3bbd5aae06806bc0f99deb46551b8d7d.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3bbd5aae06806bc0f99deb46551b8d7d.png)'
- en: Step 4— hourglass architecture — Image from [[1](https://arxiv.org/pdf/2110.13711.pdf)]
    modified by the author
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 第4步——沙漏结构——图像来自[[1](https://arxiv.org/pdf/2110.13711.pdf)]，由作者修改
- en: By now, we are in the middle of the architecture…
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 到现在，我们正处于架构的中间…
- en: From here, the up-sampling starts!
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 从这里，上采样开始了！
- en: '5\. Up-sampling layers are used to expand the shortest sequence back to the
    original full resolution. Since we had two down-samplings (one from L tokens to
    *L/k₁* and the second from *L/k₁*tokens to *L/(k₁.k₂)* tokens), we will perform
    two up-sampling to bring back the number of tokens to *L* tokens. The first up-sampling
    is the following:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 5. 上采样层用于将最短的序列扩展回原始的全分辨率。由于我们进行了两次下采样（一次从L tokens到*L/k₁*，第二次从*L/k₁* tokens到*L/(k₁.k₂)*
    tokens），我们将执行两次上采样，将token数量恢复到*L* tokens。第一次上采样如下：
- en: '![](../Images/6b74b538cdd7848eed4a1a56dd91e693.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6b74b538cdd7848eed4a1a56dd91e693.png)'
- en: Step 5 (first upsampling) — hourglass architecture — Image from [[1](https://arxiv.org/pdf/2110.13711.pdf)]
    modified by the author
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 第5步（第一次上采样）——沙漏结构——图像来自[[1](https://arxiv.org/pdf/2110.13711.pdf)]，由作者修改
- en: 'And the second upsampling is the following:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 第二次上采样如下：
- en: '![](../Images/eec60403b7b7f0fd713258a57efe3248.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/eec60403b7b7f0fd713258a57efe3248.png)'
- en: Step 5 (second upsampling)— hourglass architecture — Image from [[1](https://arxiv.org/pdf/2110.13711.pdf)]
    modified by the author
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 第5步（第二次上采样）——沙漏结构——图像来自[[1](https://arxiv.org/pdf/2110.13711.pdf)]，由作者修改
- en: After every up-sampling operation, we pass the token embeddings through Transformer
    layers. In the image they are called either *shortened layers* or *post-vanilla
    layers.*
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 每次上采样操作后，我们将token嵌入通过Transformer层。在图像中，它们被称为 *缩短层* 或 *后置vanilla层*。
- en: '![](../Images/dc461c041498f9df0afb5b8234fe83c6.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/dc461c041498f9df0afb5b8234fe83c6.png)'
- en: step 5 — upsampling involves Transformer layers either as shortened layers or
    as post vanilla. — Image from [[1](https://arxiv.org/pdf/2110.13711.pdf)] modified
    by the author
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 第5步——上采样涉及到Transformer层，无论是作为缩短层还是作为后置vanilla层——图像来自[[1](https://arxiv.org/pdf/2110.13711.pdf)]，由作者修改
- en: The last upsampling passes embeddings through *post vanilla layers* and that
    outputs the embedding of the next predicted token.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 最后的上采样通过*后置vanilla层*传递嵌入，从而输出下一个预测token的嵌入。
- en: Down-sampling Step
  id: totrans-49
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 下采样步骤
- en: 'The downsampling step (also known as shortening step in the paper) shortens
    the input sequence to fewer tokens. This step is done by merging tokens into groups
    using various pooling operations such as: 1) average pooling, 2) linear pooling
    and 3) attention pooling.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 下采样步骤（也称为论文中的缩短步骤）将输入序列缩短为更少的token。此步骤通过使用各种池化操作将tokens合并为组来完成，例如：1）平均池化，2）线性池化和3）注意力池化。
- en: '**1)Average pooling:** On a high level, average pooling merges k adjacent token
    embeddings into a single embedding by taking the average. This method has two
    hyper- parameters: “pool size” and “stride”.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '**1) 平均池化：** 从高层次来看，平均池化通过取平均值将k个相邻的token嵌入合并为单一的嵌入。这种方法有两个超参数：“池化大小”和“步幅”。'
- en: '“Pool size” is the size of the window, and “stride” is how many steps the window
    size would move forward each time. For example in a sequence of “ABCDEF” and with
    pool size = stride = 2, first two tokens make up the first window and the window
    moves forward 2 tokens at a time. So the windows will be: [AB], [CD], [EF].'
  id: totrans-52
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “池化大小”是窗口的大小，“步幅”是窗口每次移动的步数。例如，在一个序列“ABCDEF”中，池化大小 = 步幅 = 2，前两个token组成第一个窗口，窗口每次移动2个token。所以窗口将是：[AB]，[CD]，[EF]。
- en: 'The paper sets both “pool size” and “stride” to a same number and call it “shortening
    factor (sf)”. Let’s see this in an example:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 论文将“池化大小”和“步幅”设为相同的数字，并称之为“缩短因子 (sf)”。我们通过一个例子来看看：
- en: 'If the input sequence is *[x1, x2, x3, x4, x5, x6, x7, x8, x9, x10]* and hyper-parameters
    *pool size=stride=3*, then **average pooling** divides the sequence into chunks
    of size 3 i.e. *[x1, x2, x3], [x4, x5, x6], [x7, x8, x9], [x10]* and averages
    token embeddings in each window to get a single embedding i.e.:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 如果输入序列是 *[x1, x2, x3, x4, x5, x6, x7, x8, x9, x10]*，且超参数为 *pool size=stride=3*，则**平均池化**将序列划分为大小为3的块，即
    *[x1, x2, x3]，[x4, x5, x6]，[x7, x8, x9]，[x10]*，并对每个窗口中的token嵌入取平均值，以获得单一嵌入，如下：
- en: '*e1 = mean(x1, x2, x3)*'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '*e1 = mean(x1, x2, x3)*'
- en: '*e2 = mean(x4, x5, x6)*'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '*e2 = mean(x4, x5, x6)*'
- en: '*e3 = mean(x7, x8, x9)*'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '*e3 = mean(x7, x8, x9)*'
- en: '*e4 = x10*'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '*e4 = x10*'
- en: Therefore the shortened sequence will be *[e1, e2, e3, e4]* . Note the length
    of the shortened sequence is input length/sf = 10/3 = 3.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，缩短后的序列将是 *[e1, e2, e3, e4]*。注意缩短后的序列长度是输入长度/sf = 10/3 = 3。
- en: '**2) Linear pooling:**'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '**2) 线性池化：**'
- en: 'This method sets a stride = k and divides the input sequence of length L tokens
    to L/k windows. Each window has k tokens where each token has an embedding vector
    of dimension let’s say d. The method then flattens each window into k*d dimensional
    vector, and forms the following matrix:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法设置步幅 = k，并将长度为 L 的输入序列划分为 L/k 个窗口。每个窗口包含 k 个 token，每个 token 具有一个维度为 d 的嵌入向量。然后，该方法将每个窗口展平为
    k*d 维的向量，并形成以下矩阵：
- en: '![](../Images/95774df342131fe20dc9d812f3fa8429.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/95774df342131fe20dc9d812f3fa8429.png)'
- en: linear pooling — part 1 — image by the author
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 线性池化 — 第 1 部分 — 图片由作者提供
- en: 'Let’s say the input sequence is *[x1, x2, x3, x4, x5, x6, x7, x8, x9, x10]*
    and stride=3, then we have the following windows: *[x1, x2, x3] , [x4, x5, x6],
    [x7, x8, x9], [x10],* and if every token has a 100-dimensional embedding vector,
    then the above matrix becomes:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 假设输入序列是 *[x1, x2, x3, x4, x5, x6, x7, x8, x9, x10]*，且步幅=3，则我们有以下窗口： *[x1, x2,
    x3] , [x4, x5, x6], [x7, x8, x9], [x10]*，如果每个 token 具有一个 100 维的嵌入向量，则上述矩阵变为：
- en: '![](../Images/53a1c77052346acea2c58fbdc166a2f9.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/53a1c77052346acea2c58fbdc166a2f9.png)'
- en: linear pooling — part 2 — image by the author
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 线性池化 — 第 2 部分 — 图片由作者提供
- en: Note that we have now reduced length of sequence from 10 to 4, but now each
    new token has dimensionality of 300 instead of 100! To bring it back to the original
    dimensionality, linear pooling projects them to a 100-dim space using a learned
    linear transformation. A linear transformation is a 300*100 matrix that is learned
    from the data.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，现在我们已经将序列的长度从 10 缩短到 4，但现在每个新 token 的维度是 300 而不是 100！为了恢复到原始维度，线性池化将它们通过学习到的线性变换投影到
    100 维空间中。线性变换是一个 300*100 的矩阵，由数据学习得出。
- en: '**3) Attention pooling:**'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '**3) 注意力池化：**'
- en: 'This method starts similar to above two methods: the input sequence is divided
    into windows of size k, then an attention is applied within each window, this
    allows tokens inside each window to attend to each other. At the end the embeddings
    produced by attention of tokens in each window are summed together. After this
    step, a feedforward layer is applied on the chunk embeddings.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法与上述两种方法类似开始：输入序列被划分为大小为 k 的窗口，然后在每个窗口内应用注意力，这使得窗口中的 token 可以相互关注。最后，由注意力产生的每个窗口的嵌入被加在一起。经过这一步后，在块嵌入上应用前馈层。
- en: '![](../Images/7c0865f8ea2e8dffe7d87b2115617ac5.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7c0865f8ea2e8dffe7d87b2115617ac5.png)'
- en: Attention pooling — image by the author
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 注意力池化 — 图片由作者提供
- en: Up-sampling Step
  id: totrans-72
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 上采样步骤
- en: 'The up-sampling step expands the shortened sequence back to the original full
    length. There are two simple ways to do upsampling:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 上采样步骤将缩短的序列扩展回原始的完整长度。有两种简单的上采样方法：
- en: '*repeat expansion:* The *repeat expansion* just simply copies each embedding
    multiple times. This is computationally very efficient.'
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*重复扩展：* *重复扩展* 只是简单地多次复制每个嵌入。这在计算上非常高效。'
- en: '*linear expansion*. The *linear expansion* projects into the higher dimension
    then expands it. For example if the shortened sequence is [e1, e2, e3, e4] and
    the *sf=k=3*, then each embedding is linearly projected to a vector of size *k
    * d*, where *d* is the original embedding dimension. The projection weight matrix
    is learnable and is trained end-to-end along with the full model.'
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*线性扩展*。*线性扩展*将其投影到更高的维度，然后进行扩展。例如，如果缩短后的序列是 [e1, e2, e3, e4] 并且 *sf=k=3*，那么每个嵌入会线性投影到一个大小为
    *k * d* 的向量，其中 *d* 是原始嵌入维度。投影权重矩阵是可学习的，并且与完整模型一起端到端地训练。'
- en: To maintain fidelity to the original input sequence, a residual connection (shown
    as red dotted line) adds the input sequence from before shortening to the upsampled
    sequence. Think of this as a way to acoustic context through the multiple shorten-expand
    cycles.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 为了保持对原始输入序列的保真度，残差连接（如红色虚线所示）将缩短之前的输入序列添加到上采样序列中。可以将其视为通过多次缩短-扩展周期来获取声学上下文的一种方式。
- en: '![](../Images/af7582679c40f4b36b7f9f20603184f9.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/af7582679c40f4b36b7f9f20603184f9.png)'
- en: residual connections added for maintaining fidelity — image from [[1](https://arxiv.org/pdf/2110.13711.pdf)]
    modified by the author
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 为了保持保真度添加的残差连接 — 图片来自 [[1](https://arxiv.org/pdf/2110.13711.pdf)]，由作者修改
- en: 'There is a more advanced upsampling method too that is called *attention upsampling*
    and it works as following:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 还有一种更高级的上采样方法，称为 *注意力上采样*，其工作原理如下：
- en: If the shortened sequence is [e1, e2, e3, e4] and *sf=k=3,* then first a linear
    or repeat upsampling is applied to expand this to the original length. This gives
    [u1, u2, …, u12].
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 如果缩短的序列是[ e1, e2, e3, e4 ]，且*sf=k=3*，则首先应用线性或重复上采样将其扩展到原始长度。这将得到[ u1, u2, …,
    u12 ]。
- en: 'Let the embeddings before shortening be [x1, x2, …, x12]; these are added to
    the upsampled embeddings via residual connection (the red dotted lines) and form
    [u1+x1, …, u12+x12]. Now the self-attention mechanism is applied over this sequence,
    where:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 设缩短前的嵌入为[ x1, x2, …, x12 ]；这些通过残差连接（红色虚线）添加到上采样的嵌入中，形成[ u1+x1, …, u12+x12 ]。现在，自注意力机制应用于这个序列，其中：
- en: Queries (Q) come from the summed embeddings [u1+x1, …, u12+x12].
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 查询（Q）来自求和的嵌入[ u1+x1, …, u12+x12 ]。
- en: Keys (K) and Values (V) come from the upsampled embeddings [u1, u2, …, u12].
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 键（K）和值（V）来自上采样的嵌入[ u1, u2, …, u12 ]。
- en: This updates the summed embeddings and that would be the final output. The attention
    over the upsampled sequence helps amplify relevant parts and combine it with the
    pre-shortened context.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 这更新了求和的嵌入，这将是最终输出。对上采样序列的注意力有助于放大相关部分，并与预缩短的上下文进行结合。
- en: Experiments
  id: totrans-85
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实验
- en: They[1] evaluated their model on language modeling using enwik8, and image generation
    using ImageNet-32/64\. They showed perplexity was improved on Enwik8 dataset by
    10–15% over Transformer-XL baseline; and they achieved new State-of-the-art for
    autoregressive Transformer models on ImageNet-32 image generation.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 他们[1]在语言建模（使用enwik8）和图像生成（使用ImageNet-32/64）上评估了他们的模型。他们显示，相比于Transformer-XL基线，Enwik8数据集上的困惑度提高了10-15%；并且他们在ImageNet-32图像生成任务上实现了自回归变换器模型的新最先进水平。
- en: '![](../Images/d80a600921bf4e70847f3cdad6668b2e.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d80a600921bf4e70847f3cdad6668b2e.png)'
- en: parameters of the experiment — image by the author
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 实验参数 — 作者提供的图片
- en: This concludes the hourglass network. In next article, we will look into other
    hierarchical transformer models.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 这部分结束了沙漏网络的讨论。在下一篇文章中，我们将深入探讨其他层次化的变换器模型。
- en: Summary
  id: totrans-90
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: 'In this article we reviewed a hierarchical architecture for transformers which
    improves efficiency and reduces memory usage in processing long sequences. The
    architecture is called Hourglass [1] and it consists of two major components:
    1) shortening or downsampling, and 2) upsampling. The shortening is done by merging
    tokens into groups using pooling operations like average pooling or linear pooling.
    The sequence length is reduced by a shorten factor k in the middle layers of the
    network. The upsampling component uses methods like linear upsampling or attention
    upsampling to expand the shortened sequences back to the original length. Hourglass
    models improve perplexity compared to baseline Transformers like Transformer-XL
    [1]. In fact, it achieves new SOTA for Transformer models on ImageNet32 image
    generation task.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我们回顾了一种改进效率并减少处理长序列时内存使用的变换器层次化架构。这种架构称为Hourglass [1]，由两个主要组件组成：1）缩短或下采样，2）上采样。缩短是通过使用池化操作（如平均池化或线性池化）将令牌合并成组来完成的。网络中间层的序列长度由缩短因子k减少。上采样组件使用线性上采样或注意力上采样等方法将缩短的序列扩展回原始长度。Hourglass模型相比于基线变换器如Transformer-XL
    [1]，改善了困惑度。实际上，它在ImageNet32图像生成任务上达到了变换器模型的新最先进水平。
- en: 'If you have any questions or suggestions, feel free to reach out to me:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你有任何问题或建议，欢迎随时联系我：
- en: 'Email: mina.ghashami@gmail.com'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '邮箱: mina.ghashami@gmail.com'
- en: 'LinkedIn: [https://www.linkedin.com/in/minaghashami/](https://www.linkedin.com/in/minaghashami/)'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 'LinkedIn: [https://www.linkedin.com/in/minaghashami/](https://www.linkedin.com/in/minaghashami/)'
- en: References
  id: totrans-95
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[Hierarchical Transformers Are More Efficient Language Models](https://arxiv.org/pdf/2110.13711.pdf)'
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[层次化变换器是更高效的语言模型](https://arxiv.org/pdf/2110.13711.pdf)'
- en: '[An Exploration of Hierarchical Attention Transformers for Efficient Long Document
    Classification](https://arxiv.org/abs/2210.05529)'
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[探索层次化注意力变换器在长文档分类中的高效性](https://arxiv.org/abs/2210.05529)'
