- en: 'Gaussian Mixture Models (GMMs): from Theory to Implementation'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 高斯混合模型（GMMs）：从理论到实现
- en: 原文：[https://towardsdatascience.com/gaussian-mixture-models-gmms-from-theory-to-implementation-4406c7fe9847](https://towardsdatascience.com/gaussian-mixture-models-gmms-from-theory-to-implementation-4406c7fe9847)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/gaussian-mixture-models-gmms-from-theory-to-implementation-4406c7fe9847](https://towardsdatascience.com/gaussian-mixture-models-gmms-from-theory-to-implementation-4406c7fe9847)
- en: In-depth explanation of GMMs and the Expectation-Maximization algorithm used
    to train them
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 对 GMMs 和用于训练它们的期望最大化算法的深入解释
- en: '[](https://medium.com/@roiyeho?source=post_page-----4406c7fe9847--------------------------------)[![Dr.
    Roi Yehoshua](../Images/905a512ffc8879069403a87dbcbeb4db.png)](https://medium.com/@roiyeho?source=post_page-----4406c7fe9847--------------------------------)[](https://towardsdatascience.com/?source=post_page-----4406c7fe9847--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----4406c7fe9847--------------------------------)
    [Dr. Roi Yehoshua](https://medium.com/@roiyeho?source=post_page-----4406c7fe9847--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@roiyeho?source=post_page-----4406c7fe9847--------------------------------)[![Dr.
    Roi Yehoshua](../Images/905a512ffc8879069403a87dbcbeb4db.png)](https://medium.com/@roiyeho?source=post_page-----4406c7fe9847--------------------------------)[](https://towardsdatascience.com/?source=post_page-----4406c7fe9847--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----4406c7fe9847--------------------------------)
    [Dr. Roi Yehoshua](https://medium.com/@roiyeho?source=post_page-----4406c7fe9847--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----4406c7fe9847--------------------------------)
    ·17 min read·Nov 28, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----4406c7fe9847--------------------------------)
    ·17 分钟阅读·2023年11月28日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: Gaussian Mixture Models (GMMs) are statistical models that represent the data
    as a mixture of Gaussian (normal) distributions. These models can be used to identify
    groups within the dataset, and to capture the complex, multi-modal structure of
    data distributions.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 高斯混合模型（GMMs）是统计模型，将数据表示为高斯（正态）分布的混合。这些模型可以用于识别数据集中的群体，并捕捉数据分布的复杂、多模态结构。
- en: GMMs are used in a variety of machine learning applications, including [clustering](https://medium.com/ai-made-simple/introduction-to-clustering-2ffc22673b5a),
    density estimation, and pattern recognition.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: GMMs 在各种机器学习应用中得到使用，包括 [聚类](https://medium.com/ai-made-simple/introduction-to-clustering-2ffc22673b5a)、密度估计和模式识别。
- en: In this article we will first explore mixture models, focusing on Gaussian mixture
    models and their underlying principles. Then, we will examine how to estimate
    the parameters of these models using a powerful technique known as Expectation-Maximization
    (EM), and provide a step-by-step guide to implementing it from scratch in Python.
    Finally, we will demonstrate how to perform clustering with GMM using the Scikit-Learn
    library.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们将首先探讨混合模型，重点关注高斯混合模型及其基本原理。然后，我们将介绍如何使用一种强大的技术——期望最大化（EM）——来估计这些模型的参数，并提供一个从头开始在
    Python 中实现的逐步指南。最后，我们将展示如何使用 Scikit-Learn 库通过 GMM 进行聚类。
- en: '![](../Images/e3bd2c1d7f6076240b8c9b005b168df4.png)'
  id: totrans-9
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e3bd2c1d7f6076240b8c9b005b168df4.png)'
- en: Image by [Markéta Klimešová](https://pixabay.com/users/maky_orel-436253/?utm_source=link-attribution&utm_medium=referral&utm_campaign=image&utm_content=5029714)
    from [Pixabay](https://pixabay.com//?utm_source=link-attribution&utm_medium=referral&utm_campaign=image&utm_content=5029714)
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：[Markéta Klimešová](https://pixabay.com/users/maky_orel-436253/?utm_source=link-attribution&utm_medium=referral&utm_campaign=image&utm_content=5029714)
    来自 [Pixabay](https://pixabay.com//?utm_source=link-attribution&utm_medium=referral&utm_campaign=image&utm_content=5029714)
- en: Mixture Models
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 混合模型
- en: A **mixture model** is a probability model for representing data that may arise
    from several different sources or categories, each of which is modeled by a separate
    probability distribution. For example, financial returns typically behave differently
    under normal market conditions and during periods of crisis, and thus can be modeled
    as a mixture of two distinct distributions.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '**混合模型** 是一种概率模型，用于表示可能来源于多个不同来源或类别的数据，每个来源或类别都由单独的概率分布建模。例如，金融回报在正常市场条件下和危机期间的表现通常不同，因此可以建模为两个不同分布的混合。'
- en: 'Formally, if *X* is a random variable whose distribution is a mixture of *K*
    component distributions, the probability density function (PDF) or probability
    mass function (PMF) of *X* can be written as:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 正式地，如果 *X* 是一个随机变量，其分布是 *K* 个组件分布的混合，那么 *X* 的概率密度函数（PDF）或概率质量函数（PMF）可以写为：
- en: '![](../Images/c7cd249072205eae2a81a2f4a16b960a.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c7cd249072205eae2a81a2f4a16b960a.png)'
- en: A mixture model
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 一个混合模型
- en: 'where:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 其中：
- en: '*p*(*x*) is the overall density or mass function of the mixture model.'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*p*(*x*) 是混合模型的整体密度或质量函数。'
- en: '*K* is the number of component distributions in the mixture.'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*K* 是混合中组件分布的数量。'
- en: '*fₖ*(*x*; *θₖ*) is the density or mass function of the *k*-th component distribution,
    parametrized by *θₖ*.'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*fₖ*(*x*; *θₖ*) 是第 *k* 个组件分布的密度或质量函数，由 *θₖ* 参数化。'
- en: '*wₖ* is the mixing weight of the *k*-th component, with 0 ≤ *wₖ* ≤ 1 and the
    sum of the weights being 1\. *wₖ* is also known as the prior probability of component
    *k*.'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*wₖ* 是第 *k* 个组件的混合权重，0 ≤ *wₖ* ≤ 1，权重的总和为 1。*wₖ* 也被称为组件 *k* 的先验概率。'
- en: '*θₖ* represents the parameters of the *k*-th component, such as the mean and
    standard deviation in the case of Gaussian distributions.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*θₖ* 代表第 *k* 个组件的参数，例如高斯分布中的均值和标准差。'
- en: The mixture model assumes that each data point comes from one of the *K* component
    distributions, with the specific distribution being selected according to the
    mixing weights *wₖ*. The model does not require knowing which component each data
    point belongs to.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 混合模型假设每个数据点来自 *K* 个组件分布之一，具体的分布是根据混合权重 *wₖ* 选择的。模型不需要知道每个数据点属于哪个组件。
- en: 'A **Gaussian mixture model** (GMM) is a common mixture model, where the probability
    density is given by a mixture of Gaussian distributions:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '**高斯混合模型**（GMM）是常见的混合模型，其中概率密度由高斯分布的混合给出：'
- en: '![](../Images/5c7d05610e42ddb1f4d5d9a32980fe4c.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5c7d05610e42ddb1f4d5d9a32980fe4c.png)'
- en: A Gaussian mixture model
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 一个高斯混合模型
- en: 'where:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 其中：
- en: '**x** is a *d*-dimensional vector.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**x** 是一个 *d* 维向量。'
- en: '*μₖ* is the mean vector of the *k*-th Gaussian component.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*μₖ* 是第 *k* 个高斯组件的均值向量。'
- en: Σ*ₖ* is the covariance matrix of the *k*-th Gaussian component.
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Σ*ₖ* 是第 *k* 个高斯组件的协方差矩阵。
- en: '*N*(**x**; *μₖ*, Σ*ₖ*) is the multivariate normal density function for the
    *k*-th component:'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*N*(**x**; *μₖ*, Σ*ₖ*) 是第 *k* 个组件的多元正态密度函数：'
- en: '![](../Images/b52166ac55f9654920a211e60e2f7de4.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b52166ac55f9654920a211e60e2f7de4.png)'
- en: 'In the case of univariate Gaussian distributions, the probability density can
    be simplified to:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在单变量高斯分布的情况下，概率密度可以简化为：
- en: '![](../Images/b6cf79b16461597a21f8883f63854f56.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b6cf79b16461597a21f8883f63854f56.png)'
- en: A mixture model of univariate Gaussian distributions
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 单变量高斯分布的混合模型
- en: 'where:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 其中：
- en: '*μₖ* is the mean of the *k*-th Gaussian component.'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*μₖ* 是第 *k* 个高斯组件的均值。'
- en: '*σₖ* is the covariance matrix of the *k*-th Gaussian component.'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*σₖ* 是第 *k* 个高斯组件的协方差矩阵。'
- en: '*N*(*x*; *μₖ*, *σₖ*) is the univariate normal density function for the *k*-th
    component:'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*N*(*x*; *μₖ*, *σₖ*) 是第 *k* 个组件的单变量正态密度函数：'
- en: '![](../Images/dd2755ff599b04845f69172865e74fa2.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/dd2755ff599b04845f69172865e74fa2.png)'
- en: 'For example, the following Python function plots a mixture distribution of
    two univariate Gaussian distributions:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，下面的 Python 函数绘制了两个单变量高斯分布的混合分布：
- en: '[PRE0]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Let’s use this function to plot a mixture of two Gaussian distributions with
    parameters *μ*₁ = -1, *σ*₁ = 1, *μ*₂ = 4, *σ*₂ = 1.5, and mixture weights of *w*₁
    = 0.7 and *w*₂ = 0.3:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用这个函数来绘制两个高斯分布的混合，其中参数为 *μ*₁ = -1, *σ*₁ = 1, *μ*₂ = 4, *σ*₂ = 1.5，以及混合权重
    *w*₁ = 0.7 和 *w*₂ = 0.3。
- en: '[PRE1]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '![](../Images/24c30ab9796f6acc90b3b8211c2caf4f.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/24c30ab9796f6acc90b3b8211c2caf4f.png)'
- en: A mixture model of two univariate Gaussian distributions
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 两个单变量高斯分布的混合模型
- en: The dashed lines represent the individual normal distributions, and the solid
    black line shows the resulting mixture. This plot illustrates how the mixture
    model combines the two distributions, each with its own mean, standard deviation,
    and weight in the overall mixture. ​
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 虚线表示各个正态分布，实线黑色线条展示了最终的混合。这张图展示了混合模型如何结合这两个分布，每个分布都有其自身的均值、标准差和在整体混合中的权重。​
- en: Learning the GMM Parameters
  id: totrans-47
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 学习 GMM 参数
- en: Our goal is to find the parameters of the GMM (means, covariances, and mixing
    coefficients) that will best explain the observed data. To that end, we first
    define the likelihood of the model given the input data.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的目标是找到 GMM（均值、协方差和混合系数）的参数，这些参数能最好地解释观察到的数据。为此，我们首先定义给定输入数据的模型的似然性。
- en: 'For a GMM with *K* components and a dataset *X* = {**x**₁, …, **x***ₙ*} of
    *n* data points, the likelihood function *L* is given by the product of the probability
    densities of each data point, as defined by the GMM:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 对于一个具有 *K* 个成分的 GMM 和一个数据集 *X* = {**x**₁, …, **x**ₙ}，其中有 *n* 个数据点，似然函数 *L* 由每个数据点的概率密度的乘积给出，如
    GMM 所定义：
- en: '![](../Images/d535098186869a806272b8e77b8dc0b0.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d535098186869a806272b8e77b8dc0b0.png)'
- en: The likelihood of the GMM model
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: GMM 模型的似然性
- en: where *θ* represents all the parameters of the model (means, variances, and
    mixture weights).
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *θ* 表示模型的所有参数（均值、方差和混合权重）。
- en: 'In practice, it is easier to work with the log-likelihood, since the product
    of probabilities can to lead to numerical underflow for large datasets. The log-
    likelihood is given by:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在实际应用中，处理对数似然更为方便，因为对于大型数据集，概率的乘积可能会导致数值下溢。对数似然由以下公式给出：
- en: '![](../Images/d33dd4e600a81b27aa9e63056e6d3585.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d33dd4e600a81b27aa9e63056e6d3585.png)'
- en: 'The parameters of the GMM can be estimated by maximizing this log-likelihood
    function with respect to *θ*.However, we cannot directly apply [Maximum Likelihood
    Estimation](https://medium.com/@roiyeho/maximum-likelihood-855b6df92c43) (MLE)
    to estimate the parameters of a GMM due to the following reasons:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: GMM 的参数可以通过最大化相对于 *θ* 的对数似然函数来估计。然而，由于以下原因，我们不能直接应用 [最大似然估计](https://medium.com/@roiyeho/maximum-likelihood-855b6df92c43)（MLE）来估计
    GMM 的参数：
- en: The log-likelihood function is highly non-linear and complex to maximize analytically.
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对数似然函数高度非线性，且在分析上很复杂。
- en: The model has latent variables (the mixture weights), which are not directly
    observable in the data.
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 模型具有潜在变量（混合权重），这些变量在数据中不可直接观察。
- en: To overcome these issues, the Expectation-Maximization (EM) algorithm is commonly
    used instead. This algorithm is described in the next section.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 为了克服这些问题，通常使用期望最大化（EM）算法。该算法将在下一节中描述。
- en: Expectation-Maximization (EM)
  id: totrans-59
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 期望最大化（EM）
- en: The EM algorithm is a powerful method for finding maximum likelihood estimates
    of parameters in statistical models that depend on unobserved latent variables.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: EM 算法是一个强大的方法，用于在依赖于未观察的潜在变量的统计模型中寻找最大似然估计。
- en: 'The algorithm begins by randomly initializing the model parameters. Then it
    iterates between two steps:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 该算法从随机初始化模型参数开始。然后在两个步骤之间迭代：
- en: '**Expectation step (E-step)**: Compute the expected log-likelihood of the model
    with respect to the distribution of the latent variables, given the observed data
    and the current estimates of the model parameters. This step involves an estimation
    of the probabilities of the latent variables.'
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**期望步骤（E 步骤）**：计算模型的期望对数似然，考虑到观测数据和当前模型参数的估计。此步骤涉及对潜在变量概率的估计。'
- en: '**Maximization step (M-step)**: Update the parameters of the model to maximize
    the log-likelihood of the observed data, given the estimated latent variables
    from the E-step.'
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**最大化步骤（M 步骤）**：更新模型的参数，以最大化观察数据的对数似然，给定 E 步骤中估计的潜在变量。'
- en: These two steps are repeated until convergence, typically determined by a threshold
    on the change in the log-likelihood or a maximum number of iterations.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个步骤会重复进行，直到收敛，通常通过对数似然的变化阈值或最大迭代次数来确定。
- en: Let’s formulate the update equations used in the EM steps for estimating the
    parameters of a Gaussian Mixture Model. In GMMs, the latent variables represent
    the unknown component memberships of each data point. Let *Zᵢ* be the random variable
    indicating the component from which data point **x***ᵢ* was generated. *Zᵢ* can
    takeone of the values {1, …, *K*}, corresponding to the *K* components.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们制定用于估计高斯混合模型参数的 EM 步骤中的更新方程。在 GMM 中，潜在变量表示每个数据点的未知成分隶属关系。设 *Zᵢ* 为随机变量，表示数据点
    **x**ᵢ 是从哪个成分生成的。*Zᵢ* 可以取 {1, …, *K*} 中的一个值，对应于 *K* 个成分。
- en: '**E-Step**'
  id: totrans-66
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**E 步骤**'
- en: In the E-step, we compute the probability distributions of the latent variables
    *Zᵢ* given the current estimates of the model parameters. In other words, we calculate
    the membership probabilities for each data point in each Gaussian component.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在 E 步骤中，我们计算给定当前模型参数估计的潜在变量 *Zᵢ* 的概率分布。换句话说，我们计算每个数据点在每个高斯成分中的隶属概率。
- en: 'The probability that *Zᵢ* = *k*, i.e., that **x***ᵢ* belongs to the *k*-th
    component, can be computed using Bayes’ rule:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '*Zᵢ* = *k* 的概率，即 **x**ᵢ 属于 *k* -th 成分的概率，可以使用贝叶斯规则计算：'
- en: '![](../Images/9353f697ea94e2f8820260d98a41ffad.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9353f697ea94e2f8820260d98a41ffad.png)'
- en: 'Let’s denote this probability by the variable *γ*(*zᵢₖ*). Thus, we can write:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 设该概率为变量*γ*(*zᵢₖ*)。因此，我们可以写成：
- en: '![](../Images/0726d9e0a17379ffa1c58df10606f4f0.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0726d9e0a17379ffa1c58df10606f4f0.png)'
- en: The variables *γ*(*zᵢₖ*) are often referred to as *responsibilities*,since they
    describe how responsible is each component for each observation. These responsibilities
    serve as proxies for the missing information about the latent variables.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 变量*γ*(*zᵢₖ*)通常被称为*责任值*，因为它们描述了每个成分对每个观测值的责任程度。这些责任值作为关于潜在变量缺失信息的代理。
- en: 'The expected log-likelihood with respect to the distribution of the latent
    variables can now be written as follows:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 相对于潜在变量分布的期望对数似然现在可以写为：
- en: '![](../Images/f1981d13ae914c4fbc57b2459df48b0a.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f1981d13ae914c4fbc57b2459df48b0a.png)'
- en: The function *Q* is a weighted sum of the log-likelihoods of all the data points
    under each Gaussian component, with the weights being the responsibilities. Note
    that *Q* is different from the log-likelihood function *l*(*θ|X*) shown earlier.
    The log-likelihood *l*(*θ|X*) expresses the likelihood of the observed data under
    the mixture model as a whole, without explicitly accounting for the latent variables,
    whereas *Q* represents an expected log-likelihood over both the observed data
    and the estimated latent variable distributions.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 函数*Q*是所有数据点在每个高斯成分下对数似然的加权和，其中权重是责任值。请注意，*Q*与前面显示的对数似然函数*l*(*θ|X*)不同。对数似然*l*(*θ|X*)表示在混合模型下观测数据的可能性，没有明确考虑潜在变量，而*Q*表示对观测数据和估计的潜在变量分布的期望对数似然。
- en: M-Step
  id: totrans-76
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: M步
- en: In the M-step, we update the parameters *θ* of the GMM (means, covariances,
    and mixing weights) so as to maximize the expected likelihood *Q*(*θ*) using the
    responsibilities calculated in the E-step.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在M步中，我们更新GMM（均值、协方差和混合权重）的参数*θ*，以最大化使用E步计算的责任值的期望似然*Q*(*θ*)。
- en: 'The parameter updates are as follows:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 参数更新如下：
- en: 'Update the means for each component:'
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更新每个成分的均值：
- en: '![](../Images/41c74ec7627348b2991314b742bf53eb.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/41c74ec7627348b2991314b742bf53eb.png)'
- en: That is, the new mean of the *k*-th component is a weighted average of all the
    data points, with the weights being the probabilities that these points belong
    to component *k*.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 即，第*k*个成分的新均值是所有数据点的加权平均，其中权重是这些点属于成分*k*的概率。
- en: This update formula can be derived from maximizing the expected log-likelihood
    function *Q* with respect to the means *μₖ*. I will show here the proof for the
    univariate Gaussian distributions case.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 此更新公式可以通过最大化期望对数似然函数*Q*相对于均值*μₖ*来推导。我将在这里展示单变量高斯分布情况的证明。
- en: '*Proof:*'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '*证明：*'
- en: 'The expected log-likelihood in the case of univariate Gaussian distributions
    is:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 单变量高斯分布的期望对数似然为：
- en: '![](../Images/a0e8ff7a09be9551b9c5364f201bb426.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a0e8ff7a09be9551b9c5364f201bb426.png)'
- en: 'Taking the derivative of this function with respect to *μₖ* and setting it
    to 0 gives us:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 对该函数关于*μₖ*求导并设其为0可得：
- en: '![](../Images/03339871eea729abae42f648fb1a1c0c.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/03339871eea729abae42f648fb1a1c0c.png)'
- en: '2\. Update the covariances for each component:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 2\. 更新每个成分的协方差：
- en: '![](../Images/c56b50366d4f2301351bb3cea30cb8f4.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c56b50366d4f2301351bb3cea30cb8f4.png)'
- en: That is, the new covariance of the *k*-th component is a weighted average of
    the squared deviations of each data point from the component’s mean, where the
    weights are the probabilities of the points assigned to that component.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 即，第*k*个成分的新协方差是每个数据点与该成分均值的平方偏差的加权平均，其中权重是分配给该成分的点的概率。
- en: 'In the case of univariate normal distributions, this update is simplified to:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 对于单变量正态分布，此更新简化为：
- en: '![](../Images/bd84db349f8a5b6fb88c9810b4a012ec.png)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bd84db349f8a5b6fb88c9810b4a012ec.png)'
- en: '3\. Update the mixing weights:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 3\. 更新混合权重：
- en: '![](../Images/dc23e3bcd3535ad48505ed812b90c405.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/dc23e3bcd3535ad48505ed812b90c405.png)'
- en: That is, the new weight of the *k*-th component is the total probability of
    the points belonging to this component, normalized by the number of points *n*.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 即，第*k*个成分的新权重是点属于该成分的总概率，经过点数*n*的归一化。
- en: Repeating these two steps is guaranteed to convergence to a local maximum of
    the likelihood function. Since the final optimum reached depends on the initial
    random parameter values, it is a common practice to run the EM algorithm several
    times with varied random initializations and keep the model that obtains the highest
    likelihood.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 重复这两个步骤可以确保收敛到似然函数的局部最大值。由于最终达到的最优值依赖于初始的随机参数值，因此通常的做法是多次运行 EM 算法，使用不同的随机初始化，并保留获得最高似然的模型。
- en: Implementation in Python
  id: totrans-97
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Python 中的实现
- en: We will now implement the EM algorithm for estimating the parameters of a GMM
    of two univariate Gaussian distributions from a given dataset.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将实现 EM 算法，以从给定数据集中估计两个单变量高斯分布的 GMM 参数。
- en: 'We start by importing the required libraries:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从导入所需的库开始：
- en: '[PRE2]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Next, let’s write a function to initialize the parameters of the GMM:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们编写一个函数来初始化 GMM 的参数：
- en: '[PRE3]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The means are initialized from random data points in the dataset, the standard
    deviations are set to 1, and the mixing weights are set uniformly to be 0.5.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 均值从数据集中随机数据点初始化，标准差设置为 1，混合权重均匀设置为 0.5。
- en: 'We now implement the E-step, in which we compute the responsibilities (probabilities)
    for each data point belonging to each Gaussian component:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们实现 E 步，其中我们计算每个数据点属于每个高斯组件的责任（概率）：
- en: '[PRE4]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'In the M-step, we update the model parameters based on the responsibilities
    calculated in the E-step:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 在 M 步中，我们根据 E 步计算的责任来更新模型参数：
- en: '[PRE5]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Finally, we write the main function that runs the EM algorithm, iterating between
    the E-step and M-step for a specified number of iterations:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们编写主函数来运行 EM 算法，在 E 步和 M 步之间进行迭代，直到指定的迭代次数：
- en: '[PRE6]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: To test our implementation, we will create a synthetic dataset by sampling data
    from a known mixture distribution with predefined parameters. Then, we will use
    the EM algorithm to estimate the parameters of the distribution, and compare the
    estimated parameters with the original ones.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 为了测试我们的实现，我们将创建一个合成数据集，通过从已知混合分布中采样数据，并使用 EM 算法估计分布的参数，然后将估计的参数与原始参数进行比较。
- en: 'First, let’s write a function to sample data from a mixture of two univariate
    normal distributions:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们编写一个函数，从两个单变量正态分布的混合中采样数据：
- en: '[PRE7]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'We will now use this function to sample 1,000 data points from the mixture
    distribution we have defined earlier:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将使用这个函数从之前定义的混合分布中采样 1,000 个数据点：
- en: '[PRE8]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'We can now run the EM algorithm on this dataset:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以在这个数据集上运行 EM 算法：
- en: '[PRE9]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'We get the following output:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到如下输出：
- en: '[PRE10]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The algorithm has converged to parameters that are close to the original parameters
    of the mixture: *μ*₁ = -1.031, *σ*₁ = 1.033, *μ*₂ = 4.181, *σ*₂ = 1.370, and mixture
    weights of *w*₁ = 0.675 and *w*₂ = 0.325.'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 算法已收敛到接近原始混合参数的参数：*μ*₁ = -1.031，*σ*₁ = 1.033，*μ*₂ = 4.181，*σ*₂ = 1.370，以及混合权重
    *w*₁ = 0.675 和 *w*₂ = 0.325。
- en: 'Let’s use the `plot_mixture()` function we have written earlier to plot the
    final distribution. We will update the function to plot an histogram of the sampled
    data as well:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用之前编写的 `plot_mixture()` 函数绘制最终分布。我们还将更新该函数，以绘制采样数据的直方图：
- en: '[PRE11]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '[PRE12]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The result is shown in the following graph:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 结果显示在下图中：
- en: '![](../Images/31dd3082554c4b0797212fbd2eab46ef.png)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/31dd3082554c4b0797212fbd2eab46ef.png)'
- en: The mixture distribution estimated from the dataset using the EM algorithm
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 EM 算法从数据集中估计的混合分布
- en: As can be seen, the estimated distribution closely aligns with the histogram
    of the data points.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 如所示，估计的分布与数据点的直方图紧密对齐。
- en: 'Exercise: Extend the code above to handle multivariate normal distributions
    and any number of distributions K.'
  id: totrans-127
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 练习：扩展上述代码以处理多变量正态分布和任意数量的分布 K。
- en: 'Hint: You can use the function `scipy.stats.multivariate_normal`to compute
    the PDF of a multivariate normal distribution.'
  id: totrans-128
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 提示：你可以使用函数 `scipy.stats.multivariate_normal` 来计算多变量正态分布的 PDF。
- en: GMM in Scikit-Learn
  id: totrans-129
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Scikit-Learn 中的 GMM
- en: 'Scikit-Learn provides an implementation of Gaussian mixture model in the class
    `[sklearn.mixture.GaussianMixture](https://scikit-learn.org/stable/modules/generated/sklearn.mixture.GaussianMixture.html)`.
    Important parameters of this class include:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: Scikit-Learn 在类 `[sklearn.mixture.GaussianMixture](https://scikit-learn.org/stable/modules/generated/sklearn.mixture.GaussianMixture.html)`
    中提供了高斯混合模型的实现。该类的重要参数包括：
- en: '`n_components`: The number of mixture components (defaults to 1).'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`n_components`：混合组件的数量（默认为 1）。'
- en: '`covariance_type`: The type of covariance parameters to use. Can be one of
    the following options:'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`covariance_type`：要使用的协方差参数类型。可以是以下选项之一：'
- en: '- `''full''`: Each component has its own covariance matrix.'
  id: totrans-133
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '- `''full''`：每个组件有自己的协方差矩阵。'
- en: '- `''tied''`: All components share the same covariance matrix.'
  id: totrans-134
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '- `''tied''`：所有组件共享相同的协方差矩阵。'
- en: '- `''diag''`: Each component has its own covariance matrix, which must be diagonal.'
  id: totrans-135
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '- `''diag''`：每个组件都有自己的协方差矩阵，该矩阵必须是对角的。'
- en: '- `''spherical''`: Each component has its own single variance.'
  id: totrans-136
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '- `''spherical''`：每个组件有自己的单一方差。'
- en: '`tol`: The convergence threshold. The EM algorithm will stop when the average
    improvement of the log-likelihood falls below this threshold (defaults to 0.001).'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tol`：收敛阈值。当对数似然的平均改善低于此阈值时，EM 算法将停止（默认为0.001）。'
- en: '`max_iter`: The number of EM iterations to perform (defaults to 100).'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_iter`：执行的 EM 迭代次数（默认为100）。'
- en: '`n_init`: The number of random initializations to perform (defaults to 1).'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`n_init`：执行的随机初始化次数（默认为1）。'
- en: '`init_params`: The method used to initialize the parameters of the model. Can
    take one of the following options:'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`init_params`：用于初始化模型参数的方法。可以选择以下选项之一：'
- en: '`''kmeans''`: The parameters are initialized using *k*-means (the default).'
  id: totrans-141
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`''kmeans''`：参数使用 *k*-均值初始化（默认）。'
- en: '`''k-means++''`: The parameters are initialized using *k*-means++.'
  id: totrans-142
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`''k-means++''`：参数使用 *k*-均值++ 初始化。'
- en: '`''random''`: The parameters are randomly initialized.'
  id: totrans-143
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`''random''`：参数被随机初始化。'
- en: '`''random_from_data''`: The initial means are randomly selected from the given
    data points.'
  id: totrans-144
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`''random_from_data''`：初始均值从给定的数据点中随机选择。'
- en: 'In addition, this class provides the following attributes:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，此类提供了以下属性：
- en: '`weights_`: The mixture weights.'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`weights_`：混合权重。'
- en: '`means_`: The means of each component.'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`means_`：每个组件的均值。'
- en: '`covariances_`: The covariance of each component.'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`covariances_`：每个组件的协方差。'
- en: '`converged_`: A Boolean indicating whether a convergence has been reached by
    the EM algorithm.'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`converged_`：一个布尔值，指示 EM 算法是否已达到收敛。'
- en: '`n_iter_`: The number of steps used by the EM to reach convergence.'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`n_iter_`：EM 算法达到收敛所用的步骤数。'
- en: Note that unlike other clustering algorithms in Scikit-Learn, this class does
    not provide a `labels_` attribute. Therefore, to get the cluster assignments of
    the data points, you need to call the `predict()`method on the fitted model (or
    call `fit_predict()`).
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，与 Scikit-Learn 中的其他聚类算法不同，此类不提供 `labels_` 属性。因此，要获取数据点的簇分配，您需要在拟合模型上调用 `predict()`
    方法（或调用 `fit_predict()`）。
- en: 'For example, let’s use this class to perform clustering on the following dataset,
    which consists of two elliptical blobs and a spherical one:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，使用此类对以下数据集进行聚类，该数据集包含两个椭圆形簇和一个球形簇：
- en: '[PRE13]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Let’s plot the dataset:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们绘制数据集：
- en: '[PRE14]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Next, we instantiate the `GMM`class with `n_components=3`, and call its `fit_predict()`
    method to get the cluster assignments:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们用 `n_components=3` 实例化 `GMM` 类，并调用其 `fit_predict()` 方法以获取簇分配：
- en: '[PRE15]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'We can check how many iterations it took for the EM algorithm to converge:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以检查 EM 算法收敛所需的迭代次数：
- en: '[PRE16]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '[PRE17]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: It took only two iterations for the EM algorithm to converge in this case.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，EM 算法只需两次迭代即可收敛。
- en: 'We can also examine the estimated GMM parameters:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以检查估计的 GMM 参数：
- en: '[PRE18]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '[PRE19]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: We can see that the estimated weights are very close to the original proportions
    of the three blobs, and the mean and variance of the spherical blob are very close
    to its original parameters.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，估计的权重非常接近三个簇的原始比例，球形簇的均值和方差也非常接近其原始参数。
- en: 'Let’s plot the clusters:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们绘制簇：
- en: '[PRE20]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '![](../Images/fae51f47bd7ba85a02aa535d0f8a59ac.png)'
  id: totrans-168
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fae51f47bd7ba85a02aa535d0f8a59ac.png)'
- en: The results of GMM clustering
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: GMM 聚类的结果
- en: GMM has correctly identified all three clusters.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: GMM 已正确识别出所有三个簇。
- en: In addition, we can use the method `predict_proba()` to get the membership probabilities
    for each data point in each cluster.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们可以使用方法 `predict_proba()` 获取每个数据点在每个簇中的归属概率。
- en: '[PRE21]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'For example, the first point in the dataset has a very high probability of
    belonging to the green cluster:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，数据集中的第一个点非常可能属于绿色簇：
- en: '[PRE22]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '[PRE23]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'We can visualize these probabilities by making the size of each point proportional
    to its probability of belonging to the cluster it was assigned to:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过将每个点的大小与其归属簇的概率成比例来可视化这些概率：
- en: '[PRE24]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '![](../Images/44fa0489ac3958bb4155bac43d4e4c30.png)'
  id: totrans-178
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/44fa0489ac3958bb4155bac43d4e4c30.png)'
- en: Probabilities of cluster assignments
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 簇分配的概率
- en: We can see that the points that lie in the border between the two elliptical
    clusters have lower probability. Data points that have a significantly low probability
    density (e.g., falling below a predefined threshold) can be identified as anomalies
    or outliers.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，位于两个椭圆簇边界上的点的概率较低。概率显著低的数据点（例如，低于预定义的阈值）可以被识别为异常值或离群点。
- en: 'For comparison, the following figure shows the results of other clustering
    algorithms applied to the same dataset:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进行比较，下图展示了应用于相同数据集的其他聚类算法的结果：
- en: '![](../Images/6913fb1711177424cd27c0a72da01db7.png)'
  id: totrans-182
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6913fb1711177424cd27c0a72da01db7.png)'
- en: As can be seen, other clustering algorithms fail to identify correctly the elliptical
    clusters.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 可以看出，其他聚类算法未能正确识别椭圆形簇。
- en: Model Evaluation
  id: totrans-184
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型评估
- en: The log-likelihood is a primary measure used to evaluate GMMs. It is also monitored
    during training to check for convergence of the EM algorithm. However, sometimes
    we need to compare models with different number of components or different covariance
    structures.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 对数似然度是评估 GMM 的主要度量。它在训练过程中也被监控以检查 EM 算法的收敛性。然而，有时我们需要比较具有不同组件数量或不同协方差结构的模型。
- en: 'To that end, we have two additional measures, which balance the model complexity
    (number of parameters) against its goodness of fit (represented by the log-likelihood):'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 为此，我们有两个额外的度量，它们在模型复杂性（参数数量）与拟合优度（由对数似然度表示）之间进行平衡：
- en: 'Akaike Information Criterion (AIC):'
  id: totrans-187
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 赤池信息准则 (AIC)：
- en: '![](../Images/38a2bbaaecc2ab3d69202d6003eff6eb.png)'
  id: totrans-188
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/38a2bbaaecc2ab3d69202d6003eff6eb.png)'
- en: 'where:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 其中：
- en: '*p* is the number of parameters in the model (including all the means, covariances,
    and mixing weights).'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*p* 是模型中的参数数量（包括所有均值、协方差和混合权重）。'
- en: '*L* is the maximum likelihood of the model (the likelihood of the model with
    the optimal parameter values).'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*L* 是模型的最大似然估计（具有最佳参数值的模型的似然度）。'
- en: Lower values of AIC indicate a better model. AIC rewards models that fit the
    data well, but also penalizes models with more parameters.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 较低的 AIC 值表示更好的模型。AIC 奖励那些对数据拟合良好的模型，但也惩罚参数更多的模型。
- en: '2\. Bayesian Information Criterion (BIC):'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 2\. 贝叶斯信息准则 (BIC)：
- en: '![](../Images/fe9769c0f223252eed7f79a71f4433c7.png)'
  id: totrans-194
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fe9769c0f223252eed7f79a71f4433c7.png)'
- en: where *p* and *L* are defined as before, and *n* is the number of data points.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *p* 和 *L* 的定义如前所述，*n* 是数据点的数量。
- en: Similar to AIC, BIC balances model fit and complexity, but places a greater
    penalty on models with more parameters, as *p* is multiplied by log(*n*) instead
    of 2.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于 AIC，BIC 平衡模型拟合和复杂性，但对参数更多的模型处以更大的惩罚，因为 *p* 被乘以 log(*n*) 而不是 2。
- en: 'In Scikit-Learn, you can compute these measures using the methods `aic()` and
    `bic()` of the `GMM`class. For example, the AIC and BIC values of the GMM clustering
    of the blobs dataset are:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Scikit-Learn 中，你可以使用 `GMM` 类的 `aic()` 和 `bic()` 方法计算这些度量。例如，blobs 数据集的 GMM
    聚类的 AIC 和 BIC 值是：
- en: '[PRE25]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '[PRE26]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: These measures can be used to find the optimal number of components by fitting
    GMMs with different numbers of components to the dataset and then selecting the
    model with the lowest AIC or BIC value.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 这些度量可以用来通过将不同组件数的 GMM 拟合到数据集中，然后选择具有最低 AIC 或 BIC 值的模型来寻找最优的组件数。
- en: Summary
  id: totrans-201
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: 'Let’s summarize the pros and cons of GMMs as compared to other clustering algorithms:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们总结一下 GMM 相对于其他聚类算法的优缺点：
- en: '**Pros**:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: '**优点**：'
- en: Unlike *k*-means, which assumes spherical clusters, GMMs can adapt to ellipsoidal
    shapes thanks to the covariance component. This allows GMMs to capture a wider
    variety of cluster shapes.
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与假设球形簇的 *k* 均值不同，GMM 由于协方差成分，可以适应椭圆形的形状。这使得 GMM 能够捕捉更广泛的簇形状。
- en: Can deal with clusters with varying sizes due to their use of covariance matrices
    and mixing coefficients, which account for the spread and proportion of each cluster.
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可以处理具有不同大小的簇，因为它们使用协方差矩阵和混合系数，考虑了每个簇的扩展和比例。
- en: GMMs provide probabilities (soft assignments) of each point belonging to each
    cluster, which can be more informative in understanding the data.
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GMM 提供了每个点属于每个簇的概率（软分配），这在理解数据方面可能更具信息性。
- en: Can deal with overlapping clusters, since it assigns data points to clusters
    based on probabilities rather than hard boundaries.
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可以处理重叠簇，因为它根据概率而不是硬性边界将数据点分配到簇中。
- en: Easy to explain the clustering results, because each cluster is represented
    by a Gaussian distribution with specific parameters.
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 聚类结果易于解释，因为每个簇由具有特定参数的高斯分布表示。
- en: In addition to clustering, GMMs can also be used for density estimation and
    anomaly detection.
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 除了聚类，GMM还可以用于密度估计和异常检测。
- en: '**Cons**:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: '**缺点**：'
- en: Require specifying the number of components (clusters) in advance.
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 需要提前指定组件（簇）的数量。
- en: Assume that the data in each cluster follows a Gaussian distribution, which
    might not always be a valid assumption for real-world data.
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 假设每个簇中的数据遵循高斯分布，这对实际数据可能并不总是有效的假设。
- en: May not work well when clusters contain only a few data points, as the model
    relies on sufficient data to accurately estimate the parameters of each component.
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当簇中仅包含少量数据点时，模型可能效果不好，因为模型依赖于足够的数据来准确估计每个组件的参数。
- en: The clustering results can be sensitive to the initial choice of parameters.
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 聚类结果可能对初始参数选择非常敏感。
- en: The EM algorithm used in GMMs can get stuck in a local optimum, and its convergence
    can be slow.
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GMM中使用的EM算法可能会陷入局部最优，并且收敛速度可能较慢。
- en: Badly-conditioned covariance matrices (i.e., matrices that are near singular
    or have a very high condition number) can lead to numerical instabilities during
    the EM computations.
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 条件不良的协方差矩阵（即，接近奇异或具有非常高条件数的矩阵）可能导致EM计算过程中的数值不稳定。
- en: Computationally more intensive than simpler algorithms like *k*-means, especially
    for large datasets or when the number of components is high.
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算上比简单算法如*k*-均值更为复杂，尤其是在数据集较大或组件数量较高时。
- en: Thanks for reading!
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 感谢阅读！
- en: All the images are by the author unless stated otherwise.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 所有图像均由作者提供，除非另有说明。
- en: 'You can find the code examples of this article on my github: [https://github.com/roiyeho/medium/tree/main/gmm](https://github.com/roiyeho/medium/tree/main/gmm)'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在我的GitHub上找到本文的代码示例：[https://github.com/roiyeho/medium/tree/main/gmm](https://github.com/roiyeho/medium/tree/main/gmm)
