- en: Nonlinear Dimension Reduction, Kernel PCA (kPCA), and Multidimensional Scaling
    â€” An Easy Tutorial with Python
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: éçº¿æ€§ç»´åº¦é™ä½ã€æ ¸PCAï¼ˆkPCAï¼‰å’Œå¤šç»´å°ºåº¦åˆ†æâ€” Pythonç®€å•æ•™ç¨‹
- en: åŸæ–‡ï¼š[https://towardsdatascience.com/nonlinear-dimension-reduction-kernel-pca-kpca-and-multidimensional-scaling-an-easy-tutorial-63429ee9d0ae](https://towardsdatascience.com/nonlinear-dimension-reduction-kernel-pca-kpca-and-multidimensional-scaling-an-easy-tutorial-63429ee9d0ae)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡ï¼š[https://towardsdatascience.com/nonlinear-dimension-reduction-kernel-pca-kpca-and-multidimensional-scaling-an-easy-tutorial-63429ee9d0ae](https://towardsdatascience.com/nonlinear-dimension-reduction-kernel-pca-kpca-and-multidimensional-scaling-an-easy-tutorial-63429ee9d0ae)
- en: How to Flatten your Swiss-Roll without Destroying It!!
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å¦‚ä½•åœ¨ä¸ç ´åç‘å£«å·çš„æƒ…å†µä¸‹å°†å…¶å±•å¹³!!
- en: '[](https://medium.com/@biman.pph?source=post_page-----63429ee9d0ae--------------------------------)[![Biman
    Chakraborty](../Images/c0bd6ee0a1b09456bd9e6aae0969da18.png)](https://medium.com/@biman.pph?source=post_page-----63429ee9d0ae--------------------------------)[](https://towardsdatascience.com/?source=post_page-----63429ee9d0ae--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----63429ee9d0ae--------------------------------)
    [Biman Chakraborty](https://medium.com/@biman.pph?source=post_page-----63429ee9d0ae--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@biman.pph?source=post_page-----63429ee9d0ae--------------------------------)[![Biman
    Chakraborty](../Images/c0bd6ee0a1b09456bd9e6aae0969da18.png)](https://medium.com/@biman.pph?source=post_page-----63429ee9d0ae--------------------------------)[](https://towardsdatascience.com/?source=post_page-----63429ee9d0ae--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----63429ee9d0ae--------------------------------)
    [Biman Chakraborty](https://medium.com/@biman.pph?source=post_page-----63429ee9d0ae--------------------------------)'
- en: Â·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----63429ee9d0ae--------------------------------)
    Â·11 min readÂ·Dec 11, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: Â·å‘è¡¨äº [Towards Data Science](https://towardsdatascience.com/?source=post_page-----63429ee9d0ae--------------------------------)
    Â·é˜…è¯»æ—¶é•¿11åˆ†é’ŸÂ·2023å¹´12æœˆ11æ—¥
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/3d4589e1299bc55b786feb6867522a9f.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3d4589e1299bc55b786feb6867522a9f.png)'
- en: Swiss Roll Data (Image by Author)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: ç‘å£«å·æ•°æ®ï¼ˆä½œè€…æä¾›çš„å›¾ç‰‡ï¼‰
- en: In my article on [**Principal Component Analysis (PCA) â€” An Easy Tutorial with
    Python**](https://medium.com/@biman.pph/principal-component-analysis-pca-an-easy-tutorial-with-python-c623b583cf29),
    I have discussed how PCA can be used to reduce the dimensionality of the data
    while reserving the distance between pairs of points as much as possible. I illustrated
    some examples with MNIST hand-written data sets and how PCA can reduce the dimensionality
    of the data from 784 to 35 and still being able to use supervised learning techniques
    with high degree of accuracy.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æˆ‘çš„æ–‡ç«  [**ä¸»æˆåˆ†åˆ†æï¼ˆPCAï¼‰â€” Pythonç®€å•æ•™ç¨‹**](https://medium.com/@biman.pph/principal-component-analysis-pca-an-easy-tutorial-with-python-c623b583cf29)
    ä¸­ï¼Œæˆ‘è®¨è®ºäº†å¦‚ä½•ä½¿ç”¨PCAæ¥å‡å°‘æ•°æ®çš„ç»´åº¦ï¼ŒåŒæ—¶å°½å¯èƒ½ä¿ç•™ç‚¹å¯¹ç‚¹ä¹‹é—´çš„è·ç¦»ã€‚æˆ‘ç”¨MNISTæ‰‹å†™æ•°æ®é›†ä¸¾äº†ä¸€äº›ä¾‹å­ï¼Œè¯´æ˜PCAå¦‚ä½•å°†æ•°æ®çš„ç»´åº¦ä»784é™åˆ°35ï¼Œå¹¶ä¸”ä»ç„¶èƒ½å¤Ÿä½¿ç”¨é«˜å‡†ç¡®åº¦çš„ç›‘ç£å­¦ä¹ æŠ€æœ¯ã€‚
- en: In this article, we start with an example of a simple **Swiss Roll** data in
    three dimension where the true manifold of the data has a dimension 2 and we will
    start with PCA.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘ä»¬ä»¥ä¸€ä¸ªç®€å•çš„**ç‘å£«å·**æ•°æ®çš„ä¸‰ç»´ç¤ºä¾‹å¼€å§‹ï¼Œå…¶ä¸­æ•°æ®çš„çœŸå®æµå½¢å…·æœ‰2ç»´ï¼Œæˆ‘ä»¬å°†ä»PCAå¼€å§‹ã€‚
- en: 'Example: Swiss Roll Dataset'
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç¤ºä¾‹ï¼šç‘å£«å·æ•°æ®é›†
- en: Figure 1 shows a simulated Swiss Roll data with ğ‘›=2000 points using `sklearn`
    library. The scatter plot shows points with different colors lying in different
    parts of the spiral.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾1æ˜¾ç¤ºäº†ä½¿ç”¨`sklearn`åº“æ¨¡æ‹Ÿçš„ç‘å£«å·æ•°æ®ï¼ŒåŒ…å«ğ‘›=2000ä¸ªç‚¹ã€‚æ•£ç‚¹å›¾æ˜¾ç¤ºäº†ä¸åŒé¢œè‰²çš„ç‚¹åˆ†å¸ƒåœ¨èºæ—‹çš„ä¸åŒéƒ¨åˆ†ã€‚
- en: '[PRE0]'
  id: totrans-12
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '![](../Images/c0f92a0c3c6453b87bc572c07eb7af4b.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c0f92a0c3c6453b87bc572c07eb7af4b.png)'
- en: 'Figure 1: Three dimensional view of the Swiss Roll Data (Image by Author)'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾1ï¼šç‘å£«å·æ•°æ®çš„ä¸‰ç»´è§†å›¾ï¼ˆä½œè€…æä¾›çš„å›¾ç‰‡ï¼‰
- en: We first apply PCA to this dataset and visualise the first two components in
    Figure 2\. We observe that it still retains the spiral shape of the data. The
    points in different sections of the spiral are not separable using linear boundaries
    and most of the classification methods will fail with the reduced data.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬é¦–å…ˆå¯¹è¿™ä¸ªæ•°æ®é›†åº”ç”¨PCAï¼Œå¹¶åœ¨å›¾2ä¸­å¯è§†åŒ–å‰ä¸¤ä¸ªä¸»æˆåˆ†ã€‚æˆ‘ä»¬è§‚å¯Ÿåˆ°å®ƒä»ç„¶ä¿ç•™äº†æ•°æ®çš„èºæ—‹å½¢çŠ¶ã€‚èºæ—‹çš„ä¸åŒéƒ¨åˆ†çš„ç‚¹æ— æ³•ä½¿ç”¨çº¿æ€§è¾¹ç•Œåˆ†ç¦»ï¼Œå¤§å¤šæ•°åˆ†ç±»æ–¹æ³•åœ¨é™ç»´åçš„æ•°æ®ä¸Šå°†ä¼šå¤±è´¥ã€‚
- en: '[PRE1]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '![](../Images/b485c9205012915ae3182f5198d4ff66.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b485c9205012915ae3182f5198d4ff66.png)'
- en: 'Figure 2: First two Principal Component Dimensions for Swiss Roll Data (Image
    by Author)'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾2ï¼šç‘å£«å·æ•°æ®çš„å‰ä¸¤ä¸ªä¸»æˆåˆ†ç»´åº¦ï¼ˆä½œè€…æä¾›çš„å›¾ç‰‡ï¼‰
- en: It does not unroll the underlying 2 dimensions. Why is it so? To understand,
    let us look into Figure 3, where the Euclidean distance between two points A and
    B are shown in blue dashed line. Though these two points are in completely different
    parts of the spiral, they are close to each other in Euclidean distance.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: å®ƒæ²¡æœ‰å±•å¼€æ½œåœ¨çš„äºŒç»´ç©ºé—´ã€‚ä¸ºä»€ä¹ˆä¼šè¿™æ ·ï¼Ÿä¸ºäº†ç†è§£è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬æ¥çœ‹ä¸€ä¸‹å›¾ 3ï¼Œå…¶ä¸­ä¸¤ç‚¹ A å’Œ B ä¹‹é—´çš„æ¬§å‡ é‡Œå¾—è·ç¦»ç”¨è“è‰²è™šçº¿è¡¨ç¤ºã€‚å°½ç®¡è¿™ä¸¤ç‚¹ä½äºèºæ—‹çš„å®Œå…¨ä¸åŒéƒ¨åˆ†ï¼Œå®ƒä»¬åœ¨æ¬§å‡ é‡Œå¾—è·ç¦»ä¸Šå´å¾ˆæ¥è¿‘ã€‚
- en: '[PRE2]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '![](../Images/5b02b9c1235fb1f2217eaf0ef5aa89b2.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5b02b9c1235fb1f2217eaf0ef5aa89b2.png)'
- en: 'Figure 3: Geodesic distance versus Euclidean distance for the Swiss Roll Data
    (Image by Author)'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 3ï¼šç‘å£«å·æ•°æ®çš„æµ‹åœ°è·ç¦»ä¸æ¬§å‡ é‡Œå¾—è·ç¦»ï¼ˆå›¾ç‰‡ç”±ä½œè€…æä¾›ï¼‰
- en: In PCA, the Euclidean distances are preserved. However, the distance between
    these two points A and B along the spiral manifold is shown by the red line, which
    shows that these two points are far away in the manifold. The key difference is
    here that the manifold is not linear. Eucldean distance or PCA works quite well
    when we have linear manifolds. But quite often, the data is not on linear manifolds
    as is evident on this example dataset Other image data like even hand-written
    digits data are some good examples of non-linear manifolds of high dimensional
    data.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ PCA ä¸­ï¼Œæ¬§å‡ é‡Œå¾—è·ç¦»è¢«ä¿ç•™ã€‚ç„¶è€Œï¼Œè¿™ä¸¤ç‚¹ A å’Œ B æ²¿èºæ—‹æµå½¢çš„è·ç¦»ç”±çº¢çº¿æ˜¾ç¤ºï¼Œè¡¨æ˜è¿™ä¸¤ç‚¹åœ¨æµå½¢ä¸Šç›¸è·è¾ƒè¿œã€‚å…³é”®åŒºåˆ«åœ¨äºæµå½¢ä¸æ˜¯çº¿æ€§çš„ã€‚å½“æˆ‘ä»¬å¤„ç†çº¿æ€§æµå½¢æ—¶ï¼Œæ¬§å‡ é‡Œå¾—è·ç¦»æˆ–
    PCA çš„æ•ˆæœéå¸¸å¥½ã€‚ä½†æ•°æ®å¾€å¾€ä¸åœ¨ç›´çº¿æµå½¢ä¸Šï¼Œå¦‚è¿™ä¸ªç¤ºä¾‹æ•°æ®é›†æ‰€ç¤ºã€‚å…¶ä»–å›¾åƒæ•°æ®ï¼Œå¦‚æ‰‹å†™æ•°å­—æ•°æ®ï¼Œæ˜¯é«˜ç»´æ•°æ®ä¸­éçº¿æ€§æµå½¢çš„å¥½ä¾‹å­ã€‚
- en: We need to define the distances differently to capture such differences. But
    before that let us first discuss how one can construct the principal components
    using the distances.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬éœ€è¦ä»¥ä¸åŒçš„æ–¹å¼å®šä¹‰è·ç¦»ä»¥æ•æ‰è¿™ç§å·®å¼‚ã€‚ä½†åœ¨æ­¤ä¹‹å‰ï¼Œæˆ‘ä»¬å…ˆè®¨è®ºä¸€ä¸‹å¦‚ä½•åˆ©ç”¨è·ç¦»æ„å»ºä¸»æˆåˆ†ã€‚
- en: 'Principal Components: Mathematical Formulation'
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ä¸»æˆåˆ†ï¼šæ•°å­¦å…¬å¼
- en: Given a ğ‘›Ã—ğ‘ data matrix ğ—, the *principal component directions* are defined
    to be the ğ‘-dimensional orthonormal vectors along which the sample variance of
    ğ— is successively maximized. For centered ğ—, that is the sum of each column of
    ğ— is 0, the ğ‘˜-th principal component direction is
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: ç»™å®šä¸€ä¸ª ğ‘›Ã—ğ‘ æ•°æ®çŸ©é˜µ ğ—ï¼Œ*ä¸»æˆåˆ†æ–¹å‘* è¢«å®šä¹‰ä¸ºåœ¨è¿™äº›æ–¹å‘ä¸Šï¼Œğ— çš„æ ·æœ¬æ–¹å·®ä¾æ¬¡è¢«æœ€å¤§åŒ–çš„ ğ‘ ç»´æ­£äº¤å‘é‡ã€‚å¯¹äºä¸­å¿ƒåŒ–çš„ ğ—ï¼Œå³ ğ— çš„æ¯ä¸€åˆ—ä¹‹å’Œä¸º
    0ï¼Œç¬¬ ğ‘˜ ä¸ªä¸»æˆåˆ†æ–¹å‘æ˜¯
- en: '![](../Images/b02776b66494e2f6bf72d88f9f2aee28.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b02776b66494e2f6bf72d88f9f2aee28.png)'
- en: The ğ‘›-dimensional vector ğ—ğ‘£_ğ‘˜ is called the ğ‘˜-th *principal component score*
    of ğ— and ğ‘¢_ğ‘˜=(ğ—ğ‘£_ğ‘˜)/ğ‘‘_k is the normalized ğ‘˜-th principal component score, with
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: ğ‘› ç»´å‘é‡ ğ—ğ‘£_ğ‘˜ ç§°ä¸º ğ— çš„ç¬¬ ğ‘˜ ä¸ª *ä¸»æˆåˆ†å¾—åˆ†*ï¼Œä¸” ğ‘¢_ğ‘˜=(ğ—ğ‘£_ğ‘˜)/ğ‘‘_k æ˜¯å½’ä¸€åŒ–çš„ç¬¬ ğ‘˜ ä¸ªä¸»æˆåˆ†å¾—åˆ†ï¼Œå…¶å…¬å¼ä¸º
- en: '![](../Images/8b1838ca409f0be310882ec2147e1a82.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8b1838ca409f0be310882ec2147e1a82.png)'
- en: The quantity *dÂ²_k/n* is the amount of variance explained by ğ‘£_ğ‘˜.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: æ•°é‡ *dÂ²_k/n* æ˜¯ ğ‘£_ğ‘˜ è§£é‡Šçš„æ–¹å·®é‡ã€‚
- en: The *singular value decomposition* of ğ— as ğ— = ğ‘ˆğ·ğ‘‰^âŠ¤ describes all the prinicipal
    component scores and variances with ğ‘ˆ being a ğ‘›Ã—ğ‘ dimensional matrix with columns
    ğ‘¢_1,ğ‘¢_2,â€¦,ğ‘¢_ğ‘, ğ‘‰ being a ğ‘Ã—*p* dimensional matrix with columns ğ‘£_1,ğ‘£_2,â€¦,ğ‘£_ğ‘ and
    ğ· is ğ‘Ã—ğ‘ diagonal matrix with diagonal elements given by ğ‘‘_1,ğ‘‘_2,â€¦,ğ‘‘_ğ‘.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: ğ— çš„ *å¥‡å¼‚å€¼åˆ†è§£* ğ— = ğ‘ˆğ·ğ‘‰^âŠ¤ æè¿°äº†æ‰€æœ‰ä¸»æˆåˆ†å¾—åˆ†å’Œæ–¹å·®ï¼Œå…¶ä¸­ ğ‘ˆ æ˜¯ä¸€ä¸ª ğ‘›Ã—ğ‘ ç»´çŸ©é˜µï¼Œåˆ—ä¸º ğ‘¢_1,ğ‘¢_2,â€¦,ğ‘¢_ğ‘ï¼Œğ‘‰ æ˜¯ä¸€ä¸ª
    ğ‘Ã—*p* ç»´çŸ©é˜µï¼Œåˆ—ä¸º ğ‘£_1,ğ‘£_2,â€¦,ğ‘£_ğ‘ï¼Œğ· æ˜¯ä¸€ä¸ª ğ‘Ã—ğ‘ çš„å¯¹è§’çŸ©é˜µï¼Œå¯¹è§’å…ƒç´ ä¸º ğ‘‘_1,ğ‘‘_2,â€¦,ğ‘‘_ğ‘ã€‚
- en: Let us consider the first ğ‘˜ principal components scores ğ—ğ‘£_1=ğ‘‘_1ğ‘¢_1, â€¦, ğ—ğ‘£_ğ‘˜=ğ‘‘_ğ‘˜ğ‘¢_ğ‘˜
    as the new feature vectors. Then we can write this as ğ™=ğ—ğ‘‰_ğ‘˜=(ğ‘ˆğ·)_ğ‘˜, that is,
    the first ğ‘˜ columns of the matrix (ğ‘ˆğ·) and think of **Z** as are new low-dimensional
    representation for ğ—.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬è€ƒè™‘å‰ ğ‘˜ ä¸ªä¸»æˆåˆ†å¾—åˆ† ğ—ğ‘£_1=ğ‘‘_1ğ‘¢_1, â€¦, ğ—ğ‘£_ğ‘˜=ğ‘‘_ğ‘˜ğ‘¢_ğ‘˜ ä½œä¸ºæ–°çš„ç‰¹å¾å‘é‡ã€‚ç„¶åæˆ‘ä»¬å¯ä»¥å°†å…¶å†™æˆ ğ™=ğ—ğ‘‰_ğ‘˜=(ğ‘ˆğ·)_ğ‘˜ï¼Œä¹Ÿå°±æ˜¯çŸ©é˜µ
    (ğ‘ˆğ·) çš„å‰ ğ‘˜ åˆ—ï¼Œå¹¶å°† **Z** è§†ä¸º ğ— çš„æ–°ä½ç»´è¡¨ç¤ºã€‚
- en: The rows ğ‘§_1,â€¦,ğ‘§_ğ‘› of ğ™ are the data points in this new low-dimensional representation.
    We have argued earlier that
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: ğ™ çš„è¡Œ ğ‘§_1,â€¦,ğ‘§_ğ‘› æ˜¯è¿™ä¸ªæ–°ä½ç»´è¡¨ç¤ºä¸­çš„æ•°æ®ç‚¹ã€‚æˆ‘ä»¬ä¹‹å‰è®¨è®ºè¿‡
- en: '![](../Images/2def003874b70a1bf1da09cf5ff20bab.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2def003874b70a1bf1da09cf5ff20bab.png)'
- en: The Euclidean distance between the ğ‘– and ğ‘— points in the lower dimensional represnetation
    is approximately equal to the original Euclidean distance between these two points.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä½ç»´è¡¨ç¤ºä¸­ï¼Œğ‘– å’Œ ğ‘— ç‚¹ä¹‹é—´çš„æ¬§å‡ é‡Œå¾—è·ç¦»å¤§è‡´ç­‰äºè¿™ä¸¤ç‚¹ä¹‹é—´çš„åŸå§‹æ¬§å‡ é‡Œå¾—è·ç¦»ã€‚
- en: The Inner-Product Matrix
  id: totrans-36
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å†…ç§¯çŸ©é˜µ
- en: The ğ‘›Ã—ğ‘› dimensional matrix ğ—ğ—^âŠ¤ is known as the *inner-product matrix* whose
    (ğ‘–,ğ‘—)-th element is given by *ğ‘¥_i*^âŠ¤*x_j*, the inner product between the ğ‘–-th
    and the ğ‘—-th rows of the matrix ğ—. From above, we have
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: ğ‘›Ã—ğ‘› ç»´çŸ©é˜µ ğ—ğ—^âŠ¤ è¢«ç§°ä¸º *å†…ç§¯çŸ©é˜µ*ï¼Œå…¶ (ğ‘–,ğ‘—) å…ƒç´ ç”± *ğ‘¥_i*^âŠ¤*x_j* ç»™å‡ºï¼Œå³çŸ©é˜µ ğ— çš„ç¬¬ ğ‘– è¡Œå’Œç¬¬ ğ‘— è¡Œä¹‹é—´çš„å†…ç§¯ã€‚ä»ä¸Šé¢æˆ‘ä»¬å¯ä»¥å¾—å‡º
- en: '![](../Images/41e964d69a8d8b80a9a803a87cf9788f.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/41e964d69a8d8b80a9a803a87cf9788f.png)'
- en: Thus we can write,
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: å› æ­¤æˆ‘ä»¬å¯ä»¥å†™é“ï¼Œ
- en: '![](../Images/348ef807fdd5f1fcd20c9b1cac3d53cd.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/348ef807fdd5f1fcd20c9b1cac3d53cd.png)'
- en: This is called an *eigendecomposition* of ğ—ğ—^âŠ¤ because the columns of ğ‘ˆ are
    eigenvectors of ğ—ğ—^âŠ¤. From this representation we can simply compute the eigendecomposition
    or *factorize* the inner product matrix ğ—ğ—^âŠ¤, and then the principal component
    scores are given by the columns of ğ‘ˆğ·, that is, ğ‘‘_ğ‘—ğ‘¢_ğ‘—, ğ‘—=1,â€¦,ğ‘. This shows that
    principal components scores can be computed if only the inner product matrix is
    given instead of the original data points.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ç§°ä¸º ğ—ğ—^âŠ¤ çš„ *ç‰¹å¾åˆ†è§£*ï¼Œå› ä¸º ğ‘ˆ çš„åˆ—æ˜¯ ğ—ğ—^âŠ¤ çš„ç‰¹å¾å‘é‡ã€‚ä»è¿™ä¸ªè¡¨ç¤ºä¸­ï¼Œæˆ‘ä»¬å¯ä»¥ç®€å•åœ°è®¡ç®—ç‰¹å¾åˆ†è§£æˆ– *åˆ†è§£* å†…ç§¯çŸ©é˜µ ğ—ğ—^âŠ¤ï¼Œç„¶åä¸»æˆåˆ†å¾—åˆ†ç”±
    ğ‘ˆğ· çš„åˆ—ç»™å‡ºï¼Œå³ ğ‘‘_ğ‘—ğ‘¢_ğ‘—ï¼Œğ‘—=1,â€¦,ğ‘ã€‚è¿™è¡¨æ˜ï¼Œå¦‚æœä»…ç»™å‡ºå†…ç§¯çŸ©é˜µè€Œä¸æ˜¯åŸå§‹æ•°æ®ç‚¹ï¼Œåˆ™å¯ä»¥è®¡ç®—ä¸»æˆåˆ†å¾—åˆ†ã€‚
- en: Low Dimensional Representation from Distances Only
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ä»…ä»è·ç¦»ä¸­è·å¾—çš„ä½ç»´è¡¨ç¤º
- en: Suppose that we only have the distances between the data points instead of the
    original data. That is, we have the Euclidean distances
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: å‡è®¾æˆ‘ä»¬åªæœ‰æ•°æ®ç‚¹ä¹‹é—´çš„è·ç¦»ï¼Œè€Œæ²¡æœ‰åŸå§‹æ•°æ®ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œæˆ‘ä»¬æœ‰æ¬§å‡ é‡Œå¾—è·ç¦»ã€‚
- en: '![](../Images/2ff5e219a62a1444a5dbe10678df0de8.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2ff5e219a62a1444a5dbe10678df0de8.png)'
- en: or all ğ‘– and ğ‘—. Can we still recover the principal component directions from
    these distances?
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ–è€…æ‰€æœ‰ ğ‘– å’Œ ğ‘—ã€‚æˆ‘ä»¬è¿˜èƒ½ä»è¿™äº›è·ç¦»ä¸­æ¢å¤ä¸»æˆåˆ†æ–¹å‘å—ï¼Ÿ
- en: Let us first define a ğ‘›Ã—ğ‘› dimensional distance matrix Î” with (ğ‘–,ğ‘—)-th element
    given by Î”_ğ‘–ğ‘—. We can recover the inner product matrix ğµ=ğ—ğ—^âŠ¤ from the distance
    matrix Î”.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: é¦–å…ˆå®šä¹‰ä¸€ä¸ª ğ‘›Ã—ğ‘› ç»´çš„è·ç¦»çŸ©é˜µ Î”ï¼Œå…¶ä¸­ (ğ‘–,ğ‘—) å…ƒç´ ç”± Î”_ğ‘–ğ‘— ç»™å‡ºã€‚æˆ‘ä»¬å¯ä»¥ä»è·ç¦»çŸ©é˜µ Î” æ¢å¤å†…ç§¯çŸ©é˜µ ğµ=ğ—ğ—^âŠ¤ã€‚
- en: Create the ğ‘›Ã—ğ‘› matrix ğ´ with its (ğ‘–,ğ‘—)-th element given by
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: åˆ›å»º ğ‘›Ã—ğ‘› çŸ©é˜µ ğ´ï¼Œå…¶ (ğ‘–,ğ‘—) å…ƒç´ ç”±
- en: '![](../Images/94081d5c34cd40576399f4fbb621f06f.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/94081d5c34cd40576399f4fbb621f06f.png)'
- en: 2\. Double center ğ´, that is, center both the columns and rows of ğ´ to recover
    the matrix ğµ by using the transformation *B* = (*I* â€” *M*)*A*(*I* â€” *M*) where
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 2\. å¯¹ ğ´ è¿›è¡ŒåŒé‡ä¸­å¿ƒåŒ–ï¼Œå³åŒæ—¶ä¸­å¿ƒåŒ– ğ´ çš„åˆ—å’Œè¡Œï¼Œé€šè¿‡ä½¿ç”¨å˜æ¢ *B* = (*I* â€” *M*)*A*(*I* â€” *M*) æ¥æ¢å¤çŸ©é˜µ ğµï¼Œå…¶ä¸­
- en: '![](../Images/245f14fda5cd995dadec21369c009103.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/245f14fda5cd995dadec21369c009103.png)'
- en: Kernel PCA
  id: totrans-51
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ ¸ä¸»æˆåˆ†åˆ†æ
- en: Kernel PCA simply mimics this procedure by replacing the inner product matrix
    ğµ by the kernel matrix ğ¾=((ğ¾_ğ‘–ğ‘—)), where ğ¾_ğ‘–ğ‘—=<ğœ™(ğ‘¥_ğ‘–),ğœ™(ğ‘¥_ğ‘—)>, the inner-product
    between the feature vectors ğœ™(ğ‘¥_ğ‘–) and ğœ™(ğ‘¥_ğ‘—). Here ğœ™ is a nonlinear map from
    â„^ğ‘ â†’ ğ¹, a feature space of arbitrary dimension. The idea is similar to the kernels
    in support vector machines (SVM) for classification problems. We are projecting
    the observations to a higher-dimensional space and then obtaining the principal
    components in that space. We can simply define, ğ¾_ğ‘–ğ‘—=Î¦(ğ‘¥_ğ‘–,ğ‘¥_ğ‘—), where for the
    radial kernel,
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: æ ¸ä¸»æˆåˆ†åˆ†æé€šè¿‡ç”¨æ ¸çŸ©é˜µ ğ¾=((ğ¾_ğ‘–ğ‘—)) æ›¿æ¢å†…ç§¯çŸ©é˜µ ğµ æ¥ç®€å•æ¨¡æ‹Ÿæ­¤è¿‡ç¨‹ï¼Œå…¶ä¸­ ğ¾_ğ‘–ğ‘—=<ğœ™(ğ‘¥_ğ‘–),ğœ™(ğ‘¥_ğ‘—)>ï¼Œå³ç‰¹å¾å‘é‡ ğœ™(ğ‘¥_ğ‘–)
    å’Œ ğœ™(ğ‘¥_ğ‘—) ä¹‹é—´çš„å†…ç§¯ã€‚è¿™é‡Œ ğœ™ æ˜¯ä» â„^ğ‘ â†’ ğ¹ çš„éçº¿æ€§æ˜ å°„ï¼Œğ¹ æ˜¯ä»»æ„ç»´åº¦çš„ç‰¹å¾ç©ºé—´ã€‚è¿™ä¸ªæƒ³æ³•ç±»ä¼¼äºç”¨äºåˆ†ç±»é—®é¢˜çš„æ”¯æŒå‘é‡æœºï¼ˆSVMï¼‰ä¸­çš„æ ¸å‡½æ•°ã€‚æˆ‘ä»¬å°†è§‚å¯Ÿå€¼æŠ•å½±åˆ°ä¸€ä¸ªæ›´é«˜ç»´çš„ç©ºé—´ä¸­ï¼Œç„¶ååœ¨è¯¥ç©ºé—´ä¸­è·å¾—ä¸»æˆåˆ†ã€‚æˆ‘ä»¬å¯ä»¥ç®€å•åœ°å®šä¹‰
    ğ¾_ğ‘–ğ‘—=Î¦(ğ‘¥_ğ‘–,ğ‘¥_ğ‘—)ï¼Œå¯¹äºå¾„å‘åŸºæ ¸ï¼Œ
- en: '![](../Images/dc02b10e3fbe8bb019f55ca574647ae4.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/dc02b10e3fbe8bb019f55ca574647ae4.png)'
- en: and for the polynomial kernel,
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºå¤šé¡¹å¼æ ¸ï¼Œ
- en: '![](../Images/4649f21b400fb488eb04b8c50bbe8c98.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4649f21b400fb488eb04b8c50bbe8c98.png)'
- en: where ğ›¾, ğ‘ and ğ‘‘ are the parameters of the respective kernel functions.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­ ğ›¾ã€ğ‘ å’Œ ğ‘‘ æ˜¯ç›¸åº”æ ¸å‡½æ•°çš„å‚æ•°ã€‚
- en: 'The algorithm can be described as follows:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: ç®—æ³•å¯ä»¥æè¿°å¦‚ä¸‹ï¼š
- en: Define the ğ‘›Ã—ğ‘› kernel inner product matrix ğ¾ as ğ¾=((Î¦(ğ‘¥_ğ‘–,ğ‘¥_ğ‘—)).
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å°† ğ‘›Ã—ğ‘› æ ¸å†…ç§¯çŸ©é˜µ ğ¾ å®šä¹‰ä¸º ğ¾=((Î¦(ğ‘¥_ğ‘–,ğ‘¥_ğ‘—))ã€‚
- en: Use eigendecomposition of ğ¾ to extract the eigenvalues and the eigenvectors
    of ğ¾.
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ä½¿ç”¨ ğ¾ çš„ç‰¹å¾åˆ†è§£æ¥æå– ğ¾ çš„ç‰¹å¾å€¼å’Œç‰¹å¾å‘é‡ã€‚
- en: The eigenvectors of ğ¾ will give the principal component scores.
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ğ¾ çš„ç‰¹å¾å‘é‡å°†ç»™å‡ºä¸»æˆåˆ†å¾—åˆ†ã€‚
- en: This is a nonlinear dimension reduction and we can illustrate the use of kernel
    PCA for our **Swiss Roll** data discussed in the previous example.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯ä¸€ç§éçº¿æ€§é™ç»´ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡åœ¨å‰é¢ç¤ºä¾‹ä¸­è®¨è®ºçš„ **ç‘å£«å·** æ•°æ®æ¥è¯´æ˜æ ¸ä¸»æˆåˆ†åˆ†æçš„ä½¿ç”¨ã€‚
- en: '[PRE3]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '![](../Images/92d3b95533f338cfa745aa90c02763ee.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/92d3b95533f338cfa745aa90c02763ee.png)'
- en: 'Figure 4: Kernel PCA Dimensions for the Swiss Roll Data (Image by Author)'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 4ï¼šç‘å£«å·æ•°æ®çš„æ ¸ä¸»æˆåˆ†åˆ†æç»´åº¦ï¼ˆä½œè€…æä¾›çš„å›¾ç‰‡ï¼‰
- en: In the above, we have used an `rbf` kernel with ğ›¾ = 0.002\. Though the results
    improved from PCA, it still does not unroll the swiss roll, but picks the manifold
    very well.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä¸Šè¿°å†…å®¹ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨äº†ä¸€ä¸ª`rbf`æ ¸ï¼Œğ›¾ = 0.002ã€‚å°½ç®¡ç»“æœç›¸æ¯”ä¸»æˆåˆ†åˆ†ææœ‰æ‰€æ”¹è¿›ï¼Œä½†å®ƒä»ç„¶æ²¡æœ‰å±•å¼€ç‘å£«å·æ•°æ®ï¼Œè€Œæ˜¯å¾ˆå¥½åœ°æ•æ‰äº†æµå½¢ã€‚
- en: We illustrate the kernel PCA with a different simulated data set below.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬åœ¨ä¸‹é¢ç”¨ä¸åŒçš„æ¨¡æ‹Ÿæ•°æ®é›†å±•ç¤ºäº†æ ¸ä¸»æˆåˆ†åˆ†æã€‚
- en: '![](../Images/afb45d342afc007dc080d3b4c5129506.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/afb45d342afc007dc080d3b4c5129506.png)'
- en: 'Figure 5: Kernel PCA dimensions for the simulated data on the left. (Image
    by Author)'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾5ï¼šå·¦ä¾§æ¨¡æ‹Ÿæ•°æ®çš„æ ¸PCAç»´åº¦ã€‚ï¼ˆä½œè€…æä¾›çš„å›¾ç‰‡ï¼‰
- en: In the left, we have the simulated data with 3 concentric circles having uniform
    distribution with radius 1.0, 2.8 and 5.0 respectively. On the right hand side,
    we plot the kernel PCA components with `rbf` kernel and ğ›¾=0.3\. We observe a nice
    separation of the three clusters of the data.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å·¦ä¾§ï¼Œæˆ‘ä»¬æœ‰ä¸€ä¸ªåŒ…å«åŠå¾„åˆ†åˆ«ä¸º1.0ã€2.8å’Œ5.0çš„3ä¸ªåŒå¿ƒåœ†çš„æ¨¡æ‹Ÿæ•°æ®ï¼Œåˆ†å¸ƒå‡åŒ€ã€‚åœ¨å³ä¾§ï¼Œæˆ‘ä»¬ç»˜åˆ¶äº†ä½¿ç”¨`rbf`æ ¸å’Œğ›¾=0.3çš„æ ¸PCAç»„ä»¶ã€‚æˆ‘ä»¬è§‚å¯Ÿåˆ°æ•°æ®çš„ä¸‰ä¸ªç°‡ä¹‹é—´æœ‰å¾ˆå¥½çš„åˆ†ç¦»ã€‚
- en: Multi-Dimensional Scaling
  id: totrans-70
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å¤šç»´ç¼©æ”¾
- en: We have discussed in our [article on PCA](https://medium.com/@biman.pph/principal-component-analysis-pca-an-easy-tutorial-with-python-c623b583cf29)
    that it tries to preserve the distance between the observations in a lower dimensional
    representation. In other words, if ğ‘§_1,ğ‘§_2,â€¦,ğ‘§_ğ‘› are the lower dimensional representation
    of ğ‘¥_1,ğ‘¥_2,â€¦,ğ‘¥_ğ‘›, then PCA minimizes
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬åœ¨[å…³äºPCAçš„æ–‡ç« ](https://medium.com/@biman.pph/principal-component-analysis-pca-an-easy-tutorial-with-python-c623b583cf29)ä¸­è®¨è®ºè¿‡ï¼Œå®ƒè¯•å›¾åœ¨ä½ç»´è¡¨ç¤ºä¸­ä¿ç•™è§‚å¯Ÿä¹‹é—´çš„è·ç¦»ã€‚æ¢å¥è¯è¯´ï¼Œå¦‚æœ
    ğ‘§_1,ğ‘§_2,â€¦,ğ‘§_ğ‘› æ˜¯ ğ‘¥_1,ğ‘¥_2,â€¦,ğ‘¥_ğ‘› çš„ä½ç»´è¡¨ç¤ºï¼Œé‚£ä¹ˆPCAæœ€å°åŒ–
- en: '![](../Images/406a8f6936d372b3d893200d8eceb320.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/406a8f6936d372b3d893200d8eceb320.png)'
- en: 'We now generalize this idea by defining a *stress* function as follows:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ç°åœ¨é€šè¿‡å®šä¹‰ä¸€ä¸ª*å‹åŠ›*å‡½æ•°æ¥æ¨å¹¿è¿™ä¸ªæƒ³æ³•ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š
- en: '![](../Images/9275d4a9db046d34836840fed3fe2499.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9275d4a9db046d34836840fed3fe2499.png)'
- en: where ğ‘‘_ğ‘–ğ‘— is a distance between ğ‘¥_ğ‘– and ğ‘¥_ğ‘—. Usually, we chose Euclidean distances,
    but other distances can be used as well.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­ ğ‘‘_ğ‘–ğ‘— æ˜¯ ğ‘¥_ğ‘– å’Œ ğ‘¥_ğ‘— ä¹‹é—´çš„è·ç¦»ã€‚é€šå¸¸ï¼Œæˆ‘ä»¬é€‰æ‹©æ¬§å‡ é‡Œå¾—è·ç¦»ï¼Œä½†ä¹Ÿå¯ä»¥ä½¿ç”¨å…¶ä»–è·ç¦»ã€‚
- en: '**Multidimensional scaling** seeks values ğ‘§_1,ğ‘§_2,â€¦,ğ‘§_ğ‘›âˆˆâ„^ğ‘˜ to minimize the
    *stress function*, ğ‘†_ğ‘€(ğ‘§_1,ğ‘§_2,â€¦,ğ‘§_ğ‘›).'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '**å¤šç»´ç¼©æ”¾**å¯»æ±‚å€¼ ğ‘§_1,ğ‘§_2,â€¦,ğ‘§_ğ‘›âˆˆâ„^ğ‘˜ï¼Œä»¥æœ€å°åŒ–*å‹åŠ›å‡½æ•°* ğ‘†_ğ‘€(ğ‘§_1,ğ‘§_2,â€¦,ğ‘§_ğ‘›)ã€‚'
- en: This is known as *least squares* or *Kruskalâ€“Shephard scaling*. The idea is
    to find a lower-dimensional representation of the data that preserves the pairwise
    distances as well as possible. Notice that the approximation is in terms of the
    distances rather than squared distances.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™è¢«ç§°ä¸º*æœ€å°äºŒä¹˜*æˆ–*Kruskalâ€“Shephardç¼©æ”¾*ã€‚è¿™ä¸ªæƒ³æ³•æ˜¯æ‰¾åˆ°ä¸€ä¸ªä½ç»´çš„æ•°æ®è¡¨ç¤ºï¼Œå°½å¯èƒ½ä¿ç•™æˆå¯¹çš„è·ç¦»ã€‚è¯·æ³¨æ„ï¼Œè¿™ç§è¿‘ä¼¼æ˜¯åŸºäºè·ç¦»è€Œä¸æ˜¯å¹³æ–¹è·ç¦»çš„ã€‚
- en: Let us look into its implementation for the **Swiss Roll** data.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬çœ‹çœ‹å®ƒåœ¨**ç‘å£«å·**æ•°æ®ä¸Šçš„å®ç°ã€‚
- en: '[PRE4]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '![](../Images/eeb6759c19e5ca3e289ddd9a0f4e497c.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/eeb6759c19e5ca3e289ddd9a0f4e497c.png)'
- en: 'Figure 6: First 2 dimensions of the classical multidimensional scaling. (Image
    by Author)'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾6ï¼šç»å…¸å¤šç»´ç¼©æ”¾çš„å‰ä¸¤ä¸ªç»´åº¦ã€‚ï¼ˆä½œè€…æä¾›çš„å›¾ç‰‡ï¼‰
- en: We observe that the results are very similar to the kernel PCA.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬è§‚å¯Ÿåˆ°ç»“æœä¸æ ¸ä¸»æˆåˆ†åˆ†æï¼ˆkernel PCAï¼‰éå¸¸ç›¸ä¼¼ã€‚
- en: So far, we have not gone beyond Euclidian distances. But we mentioned earlier
    that in the Swiss roll data, the Euclidian distances are not ideal.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: åˆ°ç›®å‰ä¸ºæ­¢ï¼Œæˆ‘ä»¬è¿˜æ²¡æœ‰è¶…è¶Šæ¬§å‡ é‡Œå¾—è·ç¦»ã€‚ä½†æˆ‘ä»¬ä¹‹å‰æåˆ°ï¼Œåœ¨ç‘å£«å·æ•°æ®ä¸­ï¼Œæ¬§å‡ é‡Œå¾—è·ç¦»å¹¶ä¸ç†æƒ³ã€‚
- en: There is a class of methods which construct a fancier distance ğ‘‘_ğ‘–ğ‘— between
    high-dimensional points ğ‘¥_1,â€¦,ğ‘¥_ğ‘›âˆˆâ„^ğ‘, and then they feed these ğ‘‘_ğ‘–ğ‘— through multidimensional
    scaling to get a low-dimensional representation ğ‘§_1,â€¦,ğ‘§_ğ‘›âˆˆâ„^ğ‘˜. In this case, we
    donâ€™t just get principal component scores, and our low-dimensional representation
    can end up being a *nonlinear function* of the data.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: æœ‰ä¸€ç±»æ–¹æ³•æ„é€ ä¸€ä¸ªæ›´å¤æ‚çš„è·ç¦» ğ‘‘_ğ‘–ğ‘— æ¥åº¦é‡é«˜ç»´ç‚¹ ğ‘¥_1,â€¦,ğ‘¥_ğ‘›âˆˆâ„^ğ‘ ä¹‹é—´çš„è·ç¦»ï¼Œç„¶åå°†è¿™äº› ğ‘‘_ğ‘–ğ‘— é€šè¿‡å¤šç»´ç¼©æ”¾å¤„ç†ï¼Œä»¥è·å¾—ä½ç»´è¡¨ç¤º
    ğ‘§_1,â€¦,ğ‘§_ğ‘›âˆˆâ„^ğ‘˜ã€‚è¿™æ ·ï¼Œæˆ‘ä»¬ä¸ä»…å¾—åˆ°ä¸»æˆåˆ†å¾—åˆ†ï¼Œæˆ‘ä»¬çš„ä½ç»´è¡¨ç¤ºå¯èƒ½æœ€ç»ˆæˆä¸ºæ•°æ®çš„*éçº¿æ€§å‡½æ•°*ã€‚
- en: Tangent distance
  id: totrans-85
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: åˆ‡å‘è·ç¦»
- en: '*Tangent distance* is an example of a fancier metric that we can run through
    multidimensional scaling (though used elsewhere too).'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '*åˆ‡å‘è·ç¦»*æ˜¯ä¸€ä¸ªæˆ‘ä»¬å¯ä»¥é€šè¿‡å¤šç»´ç¼©æ”¾ï¼ˆè™½ç„¶ä¹Ÿç”¨äºå…¶ä»–åœ°æ–¹ï¼‰è¿è¡Œçš„æ›´å¤æ‚çš„åº¦é‡ã€‚'
- en: A motivating example is the *handwritten digits data* we used earlier. Here,
    we have *16 \times 16* images, treated as points ğ‘¥_ğ‘–âˆˆâ„Â²âµâ¶ (i.e., they are unraveled
    into vectors). If we take, e.g., a â€œ3â€ and *rotate* it through a small angle,
    we would like for the rotated image to be considered close to the original image.
    This is not neccessarily true of Euclidean distance.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªæ¿€åŠ±ç¤ºä¾‹æ˜¯æˆ‘ä»¬ä¹‹å‰ä½¿ç”¨çš„ *æ‰‹å†™æ•°å­—æ•°æ®*ã€‚è¿™é‡Œï¼Œæˆ‘ä»¬æœ‰ *16 \times 16* çš„å›¾åƒï¼Œå°†å…¶è§†ä¸ºç‚¹ ğ‘¥_ğ‘–âˆˆâ„Â²âµâ¶ï¼ˆå³ï¼Œå®ƒä»¬è¢«å±•å¼€æˆå‘é‡ï¼‰ã€‚ä¾‹å¦‚ï¼Œå¦‚æœæˆ‘ä»¬å–ä¸€ä¸ªâ€œ3â€å¹¶
    *æ—‹è½¬* å®ƒä¸€ä¸ªå°è§’åº¦ï¼Œæˆ‘ä»¬å¸Œæœ›æ—‹è½¬åçš„å›¾åƒè¢«è®¤ä¸ºæ¥è¿‘åŸå§‹å›¾åƒã€‚è¿™åœ¨æ¬§å‡ é‡Œå¾—è·ç¦»ä¸­ä¸ä¸€å®šæˆç«‹ã€‚
- en: '![](../Images/18d00ffbd85d32f0aed9fe8fc806839b.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/18d00ffbd85d32f0aed9fe8fc806839b.png)'
- en: 'Figure 7: Original images of â€œ3â€ and a rotated image of â€œ3â€ (Image by Author)'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾7ï¼šåŸå§‹â€œ3â€å’Œæ—‹è½¬åçš„â€œ3â€å›¾åƒï¼ˆå›¾ç‰‡ç”±ä½œè€…æä¾›ï¼‰
- en: We could define Î”_ğ‘–ğ‘—^rotation to be the shortest Euclidean distance between
    a rotated version of ğ‘¥_ğ‘– and rotated version of ğ‘¥_ğ‘—. However, you can immediately
    see that there is a problem in rotating the digits â€œ6â€ and â€œ9â€.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥å®šä¹‰ Î”_ğ‘–ğ‘—^rotation ä¸ºæ—‹è½¬åçš„ ğ‘¥_ğ‘– å’Œæ—‹è½¬åçš„ ğ‘¥_ğ‘— ä¹‹é—´çš„æœ€çŸ­æ¬§å‡ é‡Œå¾—è·ç¦»ã€‚ç„¶è€Œï¼Œä½ å¯ä»¥ç«‹å³å‘ç°æ—‹è½¬æ•°å­—â€œ6â€å’Œâ€œ9â€å­˜åœ¨é—®é¢˜ã€‚
- en: We need something easier to calculate, and that restricts attention to *small
    rotations*. It helps to think of a set of rotations of an image as defining a
    curve in â„^ğ‘ â€” an image ğ‘¥_ğ‘– is a point in â„^ğ‘, and as we rotate it in either directions,
    we get a curve.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬éœ€è¦ä¸€äº›æ›´å®¹æ˜“è®¡ç®—çš„ä¸œè¥¿ï¼Œå¹¶ä¸”å°†æ³¨æ„åŠ›é™åˆ¶åœ¨ *å°æ—‹è½¬* ä¸Šã€‚å¯ä»¥å°†å›¾åƒçš„æ—‹è½¬é›†è§†ä¸ºå®šä¹‰äº† â„^ğ‘ ä¸­çš„æ›²çº¿â€”â€”ä¸€ä¸ªå›¾åƒ ğ‘¥_ğ‘– æ˜¯ â„^ğ‘ ä¸­çš„ä¸€ä¸ªç‚¹ï¼Œå½“æˆ‘ä»¬åœ¨ä»»æ„æ–¹å‘ä¸Šæ—‹è½¬å®ƒæ—¶ï¼Œæˆ‘ä»¬å¾—åˆ°ä¸€æ¡æ›²çº¿ã€‚
- en: '**Tangent distance** Î”_ğ‘–ğ‘—^tangent is defined by first computing the tangent
    line to each curve at the observed image, and then using the shortest Euclidean
    distance between tangent lines.'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '**åˆ‡çº¿è·ç¦»** Î”_ğ‘–ğ‘—^tangent é€šè¿‡é¦–å…ˆè®¡ç®—æ¯æ¡æ›²çº¿åœ¨è§‚å¯Ÿåˆ°çš„å›¾åƒå¤„çš„åˆ‡çº¿ï¼Œç„¶åä½¿ç”¨åˆ‡çº¿ä¹‹é—´çš„æœ€çŸ­æ¬§å‡ é‡Œå¾—è·ç¦»æ¥å®šä¹‰ã€‚'
- en: Isometric Feature Mapping (Isomap)
  id: totrans-93
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç­‰è·ç‰¹å¾æ˜ å°„ï¼ˆIsomapï¼‰
- en: '*Isometric feature mapping* (Isomap) learns structure in a more general setting
    to define distances. The basic idea is to construct a graph ğº=(ğ‘‰,ğ¸), i.e., construct
    edges ğ¸ between vertices ğ‘‰={1,â€¦,ğ‘›}, based on the structure between ğ‘¥_1,â€¦,ğ‘¥_ğ‘›âˆˆâ„^ğ‘
    . Then we define a graph distance Î”_ğ‘–ğ‘—^Isomap between ğ‘¥_ğ‘– and ğ‘¥_ğ‘—, and use multidimensional
    scaling for our low-dimensional representation'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '*ç­‰è·ç‰¹å¾æ˜ å°„*ï¼ˆIsomapï¼‰åœ¨æ›´ä¸€èˆ¬çš„è®¾ç½®ä¸­å­¦ä¹ ç»“æ„ä»¥å®šä¹‰è·ç¦»ã€‚åŸºæœ¬æ€æƒ³æ˜¯æ„é€ ä¸€ä¸ªå›¾ ğº=(ğ‘‰,ğ¸)ï¼Œå³åœ¨é¡¶ç‚¹ ğ‘‰={1,â€¦,ğ‘›} ä¹‹é—´æ„é€ è¾¹ ğ¸ï¼ŒåŸºäº
    ğ‘¥_1,â€¦,ğ‘¥_ğ‘›âˆˆâ„^ğ‘ ä¹‹é—´çš„ç»“æ„ã€‚ç„¶åæˆ‘ä»¬å®šä¹‰ ğ‘¥_ğ‘– å’Œ ğ‘¥_ğ‘— ä¹‹é—´çš„å›¾è·ç¦» Î”_ğ‘–ğ‘—^Isomapï¼Œå¹¶ä½¿ç”¨å¤šç»´ç¼©æ”¾è¿›è¡Œä½ç»´è¡¨ç¤ºã€‚'
- en: '**Constructing the graph**: for each pair ğ‘–,ğ‘—, we connect ğ‘–,ğ‘— with an edge
    if either:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '**æ„é€ å›¾**ï¼šå¯¹äºæ¯å¯¹ ğ‘–,ğ‘—ï¼Œå¦‚æœæ»¡è¶³ä»¥ä¸‹ä»»ä¸€æ¡ä»¶ï¼Œæˆ‘ä»¬å°† ğ‘–,ğ‘— ç”¨è¾¹è¿æ¥ï¼š'
- en: ğ‘¥_ğ‘– is one of ğ‘¥_ğ‘—â€™s ğ‘š nearest neighbors, or
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ğ‘¥_ğ‘– æ˜¯ ğ‘¥_ğ‘— çš„ ğ‘š ä¸ªæœ€è¿‘é‚»ä¹‹ä¸€ï¼Œæˆ–è€…
- en: ğ‘¥_ğ‘— is one of ğ‘¥_ğ‘–â€™s ğ‘š nearest neighbors The weight of this edge ğ‘’ = {ğ‘–,ğ‘—} is
    then ğ‘¤_ğ‘’=â€–ğ‘¥_ğ‘–âˆ’ğ‘¥_ğ‘—â€–.
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ğ‘¥_ğ‘— æ˜¯ ğ‘¥_ğ‘– çš„ ğ‘š ä¸ªæœ€è¿‘é‚»ä¹‹ä¸€ã€‚è¿™æ¡è¾¹ ğ‘’ = {ğ‘–,ğ‘—} çš„æƒé‡ä¸º ğ‘¤_ğ‘’=â€–ğ‘¥_ğ‘–âˆ’ğ‘¥_ğ‘—â€–ã€‚
- en: '**Defining graph distances**: now that we have built a graph, i.e., we have
    built an edge set ğ¸, we define the graph distance Î”_ğ‘–ğ‘—^Isomap between ğ‘¥_ğ‘– and
    ğ‘¥_ğ‘— to be the shortest path in our graph from ğ‘– to ğ‘—:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '**å®šä¹‰å›¾è·ç¦»**ï¼šç°åœ¨æˆ‘ä»¬å·²ç»æ„å»ºäº†å›¾ï¼Œå³æˆ‘ä»¬å·²ç»æ„å»ºäº†è¾¹é›† ğ¸ï¼Œæˆ‘ä»¬å®šä¹‰å›¾è·ç¦» Î”_ğ‘–ğ‘—^Isomap ä¸ºä» ğ‘– åˆ° ğ‘— çš„æœ€çŸ­è·¯å¾„ï¼š'
- en: '![](../Images/cc23df2584e0c951fd57658a7f031969.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/cc23df2584e0c951fd57658a7f031969.png)'
- en: (This can be computed by, e.g., *Dijkstraâ€™s algorithm* or *Floydâ€™s algorithm*)
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: ï¼ˆè¿™å¯ä»¥é€šè¿‡ä¾‹å¦‚ *è¿ªæ°æ–¯ç‰¹æ‹‰ç®—æ³•* æˆ– *å¼—æ´›ä¼Šå¾·ç®—æ³•* è®¡ç®—ï¼‰
- en: Let us now look into its implementation for the **Swiss roll** data.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬ç°åœ¨æ·±å…¥ç ”ç©¶å…¶åœ¨ **ç‘å£«å·** æ•°æ®ä¸Šçš„å®ç°ã€‚
- en: '[PRE5]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '![](../Images/561bb0d2fdfe53172bb154e1baa0330b.png)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/561bb0d2fdfe53172bb154e1baa0330b.png)'
- en: 'Figure 8: Two dimensional representation of the isomap of the Swiss Roll data.
    (Image by Author)'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾8ï¼šç‘å£«å·æ•°æ®çš„äºŒç»´è¡¨ç¤ºã€‚ï¼ˆå›¾ç‰‡ç”±ä½œè€…æä¾›ï¼‰
- en: With the number of neighbours ğ‘š=7, the multodimemsional scaling with the **isomap**
    distances now unrolls the **Swiss roll** data.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨é‚»å±…æ•°é‡ ğ‘š=7 çš„æƒ…å†µä¸‹ï¼Œå¤šç»´ç¼©æ”¾ä¸ **isomap** è·ç¦»ç°åœ¨å±•å¼€äº† **ç‘å£«å·** æ•°æ®ã€‚
- en: Local Linear Embedding
  id: totrans-106
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å±€éƒ¨çº¿æ€§åµŒå…¥
- en: Another nonlinear dimension reduction method is **Local linear embedding** (LLE),
    which is a similar method in spirit but its details are very different. It doesnâ€™t
    use multidimensional scaling.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: å¦ä¸€ç§éçº¿æ€§ç»´åº¦çº¦å‡æ–¹æ³•æ˜¯ **å±€éƒ¨çº¿æ€§åµŒå…¥**ï¼ˆLLEï¼‰ï¼Œå®ƒåœ¨ç²¾ç¥ä¸Šç±»ä¼¼ä½†ç»†èŠ‚å´å¤§ç›¸å¾„åº­ã€‚å®ƒä¸ä½¿ç”¨å¤šç»´ç¼©æ”¾ã€‚
- en: 'The basic idea has two steps:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: åŸºæœ¬æ€æƒ³åˆ†ä¸ºä¸¤ä¸ªæ­¥éª¤ï¼š
- en: Learn a bunch of local approximations to the structure between ğ‘¥_1,â€¦,ğ‘¥_ğ‘›âˆˆâ„^ğ‘
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å­¦ä¹ ä¸€ç»„å±€éƒ¨è¿‘ä¼¼æ¥æè¿° ğ‘¥_1,â€¦,ğ‘¥_ğ‘›âˆˆâ„^ğ‘ ä¹‹é—´çš„ç»“æ„
- en: Learn a low-dimensional representation ğ‘§_1,â€¦,ğ‘§_ğ‘›âˆˆâ„^ğ‘˜ that best matches these
    local approximations
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å­¦ä¹ ä¸€ä¸ªä½ç»´è¡¨ç¤º ğ‘§_1,â€¦,ğ‘§_ğ‘›âˆˆâ„^ğ‘˜ï¼Œæœ€å¥½ä¸è¿™äº›å±€éƒ¨è¿‘ä¼¼åŒ¹é…
- en: What is meant by such local approximations? We simply try to predict each ğ‘¥_ğ‘–
    by a linear function of nearby points ğ‘¥_ğ‘— (hence the name local linear embedding).
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: ä»€ä¹ˆæ˜¯å±€éƒ¨è¿‘ä¼¼ï¼Ÿæˆ‘ä»¬åªæ˜¯å°è¯•ç”¨é™„è¿‘ç‚¹ğ‘¥_ğ‘—çš„çº¿æ€§å‡½æ•°æ¥é¢„æµ‹æ¯ä¸ªğ‘¥_ğ‘–ï¼ˆå› æ­¤å¾—åå±€éƒ¨çº¿æ€§åµŒå…¥ï¼‰ã€‚
- en: For each ğ‘¥_ğ‘–, we first find its ğ‘š nearest neighbours, and collect their indices
    as N(ğ‘–). Then we build a weight vector ğ‘¤_ğ‘–âˆˆâ„^ğ‘›, setting ğ‘¤_ğ‘–ğ‘—=0 for ğ‘—âˆ‰N(ğ‘–) and
    setting ğ‘¤_ğ‘–ğ‘— for ğ‘—âˆˆN(ğ‘–) by minimizing
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºæ¯ä¸ªğ‘¥_ğ‘–ï¼Œæˆ‘ä»¬é¦–å…ˆæ‰¾åˆ°å®ƒçš„ğ‘šä¸ªæœ€è¿‘é‚»ï¼Œå¹¶å°†å®ƒä»¬çš„ç´¢å¼•æ”¶é›†ä¸ºN(ğ‘–)ã€‚ç„¶åæˆ‘ä»¬æ„å»ºä¸€ä¸ªæƒé‡å‘é‡ğ‘¤_ğ‘–âˆˆâ„^ğ‘›ï¼Œè®¾ç½®ğ‘¤_ğ‘–ğ‘—=0ï¼ˆå½“ğ‘—âˆ‰N(ğ‘–)æ—¶ï¼‰ï¼Œå¹¶é€šè¿‡æœ€å°åŒ–æ¥è®¾ç½®ğ‘¤_ğ‘–ğ‘—ï¼ˆå½“ğ‘—âˆˆN(ğ‘–)æ—¶ï¼‰ã€‚
- en: '![](../Images/a4923ea91402950b401d3d06a3d26653.png)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a4923ea91402950b401d3d06a3d26653.png)'
- en: Finally, we take these weights ğ‘¤_1,â€¦,ğ‘¤_ğ‘›âˆˆâ„^ğ‘› and we fit the low-dimensional
    representation ğ‘§_1,â€¦,ğ‘§_ğ‘›âˆˆâ„^ğ‘˜, by minimizing
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åï¼Œæˆ‘ä»¬å–è¿™äº›æƒé‡ğ‘¤_1,â€¦,ğ‘¤_ğ‘›âˆˆâ„^ğ‘›ï¼Œå¹¶é€šè¿‡æœ€å°åŒ–æ¥æ‹Ÿåˆä½ç»´è¡¨ç¤ºğ‘§_1,â€¦,ğ‘§_ğ‘›âˆˆâ„^ğ‘˜ã€‚
- en: '![](../Images/9eefa5853f41c28e0d40e8a4b2f49bd6.png)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9eefa5853f41c28e0d40e8a4b2f49bd6.png)'
- en: We again illustrate the use of *Local Linear Embedding* using the Swiss roll
    data.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å†æ¬¡ä½¿ç”¨*å±€éƒ¨çº¿æ€§åµŒå…¥*ï¼ˆLocal Linear Embeddingï¼‰æ¥è¯´æ˜å¦‚ä½•å¤„ç†ç‘å£«å·æ•°æ®ã€‚
- en: '[PRE6]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '![](../Images/72c3d9c2a1d5d397d5364b7eed5e0827.png)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/72c3d9c2a1d5d397d5364b7eed5e0827.png)'
- en: 'Figure 9: Local Linear Embedding dimensions for the Swiss Roll data. (Image
    by Author)'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 9ï¼šç‘å£«å·æ•°æ®çš„å±€éƒ¨çº¿æ€§åµŒå…¥ç»´åº¦ã€‚ï¼ˆä½œè€…æä¾›çš„å›¾åƒï¼‰
- en: '*Local Linear Embedding* also yielded better dimension reduction than kernel
    PCA or classical MDS, though it is not as good as *Isomap*.'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '*å±€éƒ¨çº¿æ€§åµŒå…¥*çš„é™ç»´æ•ˆæœä¼˜äºæ ¸PCAæˆ–ç»å…¸MDSï¼Œå°½ç®¡ä¸å¦‚*Isomap*ã€‚'
- en: In this article, we have learnt a few nonlinear dimension reduction technique
    by generalizing the concepts from PCA. However, there is no single method which
    works best for all sort of data for dimension reduction. Depending on the nature
    of the data, we should decide about the dimension reduction technique to be used.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘ä»¬é€šè¿‡å¯¹PCAæ¦‚å¿µçš„æ¨å¹¿å­¦ä¹ äº†ä¸€äº›éçº¿æ€§é™ç»´æŠ€æœ¯ã€‚ç„¶è€Œï¼Œæ²¡æœ‰ä¸€ç§å•ä¸€çš„æ–¹æ³•å¯ä»¥é€‚ç”¨äºæ‰€æœ‰ç±»å‹çš„æ•°æ®é™ç»´ã€‚æ ¹æ®æ•°æ®çš„æ€§è´¨ï¼Œæˆ‘ä»¬åº”é€‰æ‹©åˆé€‚çš„é™ç»´æŠ€æœ¯ã€‚
- en: Hope, you have enjoyed the article!!
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: å¸Œæœ›ä½ å–œæ¬¢è¿™ç¯‡æ–‡ç« ï¼ï¼
- en: For consulting on any data science problems, contact [biman.pph@gmail.com](mailto:biman.pph@gmail.com)
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: æœ‰å…³æ•°æ®ç§‘å­¦é—®é¢˜çš„å’¨è¯¢ï¼Œè¯·è”ç³» [biman.pph@gmail.com](mailto:biman.pph@gmail.com)
