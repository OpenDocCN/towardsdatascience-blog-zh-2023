- en: Multi-GPU Training on a single GPU System in 3 Minutes
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: åœ¨ 3 åˆ†é’Ÿå†…åœ¨å•ä¸ª GPU ç³»ç»Ÿä¸Šè¿›è¡Œå¤š GPU è®­ç»ƒ
- en: åŸæ–‡ï¼š[https://towardsdatascience.com/implement-multi-gpu-training-on-a-single-gpu-e9b6b775456a](https://towardsdatascience.com/implement-multi-gpu-training-on-a-single-gpu-e9b6b775456a)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡ï¼š[https://towardsdatascience.com/implement-multi-gpu-training-on-a-single-gpu-e9b6b775456a](https://towardsdatascience.com/implement-multi-gpu-training-on-a-single-gpu-e9b6b775456a)
- en: An Advanced Guide for TensorFlow
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: TensorFlow é«˜çº§æŒ‡å—
- en: '[](https://medium.com/@SaschaKirch?source=post_page-----e9b6b775456a--------------------------------)[![Sascha
    Kirch](../Images/a0d45da9dc9c602075b2810786c660c9.png)](https://medium.com/@SaschaKirch?source=post_page-----e9b6b775456a--------------------------------)[](https://towardsdatascience.com/?source=post_page-----e9b6b775456a--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----e9b6b775456a--------------------------------)
    [Sascha Kirch](https://medium.com/@SaschaKirch?source=post_page-----e9b6b775456a--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@SaschaKirch?source=post_page-----e9b6b775456a--------------------------------)[![Sascha
    Kirch](../Images/a0d45da9dc9c602075b2810786c660c9.png)](https://medium.com/@SaschaKirch?source=post_page-----e9b6b775456a--------------------------------)[](https://towardsdatascience.com/?source=post_page-----e9b6b775456a--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----e9b6b775456a--------------------------------)
    [Sascha Kirch](https://medium.com/@SaschaKirch?source=post_page-----e9b6b775456a--------------------------------)'
- en: Â·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----e9b6b775456a--------------------------------)
    Â·3 min readÂ·May 15, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: Â·å‘è¡¨äº[Towards Data Science](https://towardsdatascience.com/?source=post_page-----e9b6b775456a--------------------------------)
    Â·é˜…è¯»æ—¶é—´ 3 åˆ†é’ŸÂ·2023å¹´5æœˆ15æ—¥
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/dbabbc57ed819e80bd93ba392243de25.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/dbabbc57ed819e80bd93ba392243de25.png)'
- en: Photo by [Chris Liverani](https://unsplash.com/@chrisliverani?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ç‰‡ç”±[Chris Liverani](https://unsplash.com/@chrisliverani?utm_source=medium&utm_medium=referral)æä¾›ï¼Œ[Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: I want to share with you a neat little trick on how I test my multi GPU training
    code on a single GPU.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘æƒ³å’Œä½ åˆ†äº«ä¸€ä¸ªæœ‰è¶£çš„å°æŠ€å·§ï¼Œå…³äºå¦‚ä½•åœ¨å•ä¸ª GPU ä¸Šæµ‹è¯•æˆ‘çš„å¤š GPU è®­ç»ƒä»£ç ã€‚
- en: Motivation
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: åŠ¨æœº
- en: I guess the problem is obvious and you probably experienced it yourself. You
    want to train a deep learning model and you want to take advantage of multiple
    GPUs, a TPU or even multiple workers for some extra speed or larger batch size.
    But of course you cannot (letâ€™s say should not because Iâ€™ve seen it quite often
    ğŸ˜…) block the usually shared hardware for debugging or even spend a ton of money
    on a paid cloud instance.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘æƒ³é—®é¢˜å¾ˆæ˜æ˜¾ï¼Œä½ å¯èƒ½è‡ªå·±ä¹Ÿç»å†è¿‡ã€‚ä½ æƒ³è®­ç»ƒä¸€ä¸ªæ·±åº¦å­¦ä¹ æ¨¡å‹ï¼Œå¹¶å¸Œæœ›åˆ©ç”¨å¤šä¸ª GPUã€TPU æˆ–ç”šè‡³å¤šä¸ªå·¥ä½œèŠ‚ç‚¹æ¥è·å¾—é¢å¤–çš„é€Ÿåº¦æˆ–æ›´å¤§çš„æ‰¹é‡å¤§å°ã€‚ä½†å½“ç„¶ï¼Œä½ ä¸èƒ½ï¼ˆæˆ–è€…è¯´ä¸åº”è¯¥ï¼Œå› ä¸ºæˆ‘è§è¿‡å¾ˆå¤š
    ğŸ˜…ï¼‰å ç”¨é€šå¸¸å…±äº«çš„ç¡¬ä»¶è¿›è¡Œè°ƒè¯•ï¼Œç”šè‡³åœ¨ä»˜è´¹äº‘å®ä¾‹ä¸ŠèŠ±è´¹å¤§é‡é‡‘é’±ã€‚
- en: 'Let me tell you, it is not important how many physical GPUs your system has
    but rather how many your software thinks it does have. The keyword is: ***(device)
    virtualization***.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘å‘Šè¯‰ä½ ï¼Œé‡è¦çš„ä¸æ˜¯ä½ çš„ç³»ç»Ÿæœ‰å¤šå°‘ä¸ªç‰©ç† GPUï¼Œè€Œæ˜¯ä½ çš„è½¯ä»¶è®¤ä¸ºå®ƒæœ‰å¤šå°‘ä¸ªã€‚å…³é”®è¯æ˜¯ï¼š***ï¼ˆè®¾å¤‡ï¼‰è™šæ‹ŸåŒ–***ã€‚
- en: Letâ€™s implement it
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬å®ç°å®ƒ
- en: 'First lets have a look on how you would usually **detect and connect to your
    GPU**:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: é¦–å…ˆï¼Œæˆ‘ä»¬æ¥çœ‹çœ‹ä½ é€šå¸¸å¦‚ä½•**æ£€æµ‹å’Œè¿æ¥åˆ°ä½ çš„ GPU**ï¼š
- en: 'Code 1: Detect all available GPUs, initialize the respective scope and initialize
    your model, optimizer and checkpoints within the scope of the strategy.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: ä»£ç  1ï¼šæ£€æµ‹æ‰€æœ‰å¯ç”¨çš„ GPUï¼Œåˆå§‹åŒ–ç›¸åº”çš„ä½œç”¨åŸŸï¼Œå¹¶åœ¨ç­–ç•¥ä½œç”¨åŸŸå†…åˆå§‹åŒ–ä½ çš„æ¨¡å‹ã€ä¼˜åŒ–å™¨å’Œæ£€æŸ¥ç‚¹ã€‚
- en: You would first list all devices available, then select a suitable strategy
    and the initialize your model, optimizer and checkpoint within the scope of the
    strategy. If you would use a standard training loop with ***model.fit()*** you
    would be done. If you would use a custom training loop you would need to implement
    some extra steps.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ é¦–å…ˆéœ€è¦åˆ—å‡ºæ‰€æœ‰å¯ç”¨çš„è®¾å¤‡ï¼Œç„¶åé€‰æ‹©åˆé€‚çš„ç­–ç•¥ï¼Œå¹¶åœ¨ç­–ç•¥ä½œç”¨åŸŸå†…åˆå§‹åŒ–ä½ çš„æ¨¡å‹ã€ä¼˜åŒ–å™¨å’Œæ£€æŸ¥ç‚¹ã€‚å¦‚æœä½ ä½¿ç”¨æ ‡å‡†è®­ç»ƒå¾ªç¯ä¸­çš„***model.fit()***ï¼Œä½ å°±å®Œæˆäº†ã€‚å¦‚æœä½ ä½¿ç”¨è‡ªå®šä¹‰è®­ç»ƒå¾ªç¯ï¼Œä½ éœ€è¦å®ç°ä¸€äº›é¢å¤–çš„æ­¥éª¤ã€‚
- en: Check out my tutorial on [Accelerated Distributed Training with TensorFlow on
    Googleâ€™s TPU](/accelerated-distributed-training-with-tensorflow-on-googles-tpu-52f1fe21da33?sk=b713cd3cf705bae60c523b26cfe25b3f)
    for more details an distributed training with custom training loops.
  id: totrans-16
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: æŸ¥çœ‹æˆ‘å…³äº[ä½¿ç”¨ Google çš„ TPU è¿›è¡ŒåŠ é€Ÿåˆ†å¸ƒå¼è®­ç»ƒçš„æ•™ç¨‹](/accelerated-distributed-training-with-tensorflow-on-googles-tpu-52f1fe21da33?sk=b713cd3cf705bae60c523b26cfe25b3f)ï¼Œäº†è§£æ›´å¤šå…³äºè‡ªå®šä¹‰è®­ç»ƒå¾ªç¯çš„åˆ†å¸ƒå¼è®­ç»ƒç»†èŠ‚ã€‚
- en: 'There is one important detail in the code above. Did you noticed I used the
    function ***list_logical_devices(â€œGPUâ€)*** rather then ***list_physical_devices(â€œGPUâ€)***?
    Logical devices are all devices visible to the software but these are not always
    associated with an actual physical device. If we run the code block right now
    this could be an output you would see:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸Šè¿°ä»£ç ä¸­æœ‰ä¸€ä¸ªé‡è¦ç»†èŠ‚ã€‚ä½ æ³¨æ„åˆ°æˆ‘ç”¨äº†å‡½æ•° ***list_logical_devices(â€œGPUâ€)*** è€Œä¸æ˜¯ ***list_physical_devices(â€œGPUâ€)***
    å—ï¼Ÿé€»è¾‘è®¾å¤‡æ˜¯æ‰€æœ‰å¯¹è½¯ä»¶å¯è§çš„è®¾å¤‡ï¼Œä½†è¿™äº›è®¾å¤‡ä¸ä¸€å®šä¸å®é™…çš„ç‰©ç†è®¾å¤‡å…³è”ã€‚å¦‚æœæˆ‘ä»¬ç°åœ¨è¿è¡Œä»£ç å—ï¼Œè¿™å¯èƒ½æ˜¯ä½ ä¼šçœ‹åˆ°çš„è¾“å‡ºï¼š
- en: '![](../Images/4d0a65689a23f310b1f78f3fce32ab08.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4d0a65689a23f310b1f78f3fce32ab08.png)'
- en: 'Figure 1: Screenshot of output after running Code 1\. and connecting to a single
    logical GPU with one associated physical GPU. Taken by author.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 1ï¼šè¿è¡Œ Code 1 ä¹‹åçš„è¾“å‡ºæˆªå›¾ï¼Œå¹¶è¿æ¥åˆ°ä¸€ä¸ªé€»è¾‘ GPU å’Œä¸€ä¸ªç›¸å…³çš„ç‰©ç† GPUã€‚ç”±ä½œè€…æ‹æ‘„ã€‚
- en: 'We will use the logical device definition to our advantage and define some
    logical devices, before we list all logical devices and connect to them. To be
    precise, we will define 4 logical GPUs associated with a single physical GPU.
    This is how it is done:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†åˆ©ç”¨é€»è¾‘è®¾å¤‡å®šä¹‰æ¥å®šä¹‰ä¸€äº›é€»è¾‘è®¾å¤‡ï¼Œç„¶ååˆ—å‡ºæ‰€æœ‰é€»è¾‘è®¾å¤‡å¹¶è¿æ¥åˆ°å®ƒä»¬ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å°†å®šä¹‰ 4 ä¸ªé€»è¾‘ GPU ä¸ä¸€ä¸ªç‰©ç† GPU ç›¸å…³è”ã€‚è¿™æ˜¯æ“ä½œæ­¥éª¤ï¼š
- en: 'Code 2: Create multiple logical GPU devices associated with a single physical
    GPU.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: Code 2ï¼šåˆ›å»ºä¸å•ä¸ªç‰©ç† GPU ç›¸å…³è”çš„å¤šä¸ªé€»è¾‘ GPU è®¾å¤‡ã€‚
- en: 'If we would again print the number of logical vs. physical devices youâ€™ll see:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæˆ‘ä»¬å†æ¬¡æ‰“å°é€»è¾‘è®¾å¤‡ä¸ç‰©ç†è®¾å¤‡çš„æ•°é‡ï¼Œä½ ä¼šçœ‹åˆ°ï¼š
- en: '![](../Images/e1724aa8e47fc229bf375fbce66810d9.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e1724aa8e47fc229bf375fbce66810d9.png)'
- en: 'Figure 2: Screenshot of output after running Code 2\. before Code 1\. and connecting
    to four logical GPU with one associated physical GPU. Taken by author.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 2ï¼šè¿è¡Œ Code 2 ä¹‹åçš„è¾“å‡ºæˆªå›¾ã€‚åœ¨ Code 1 ä¹‹å‰ï¼Œå¹¶è¿æ¥åˆ°å››ä¸ªé€»è¾‘ GPU å’Œä¸€ä¸ªç›¸å…³çš„ç‰©ç† GPUã€‚ç”±ä½œè€…æ‹æ‘„ã€‚
- en: And voilÃ , you can now test your code on a single GPU as if you would be performing
    distributed training on 4 GPUs.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: å°±è¿™æ ·ï¼Œä½ ç°åœ¨å¯ä»¥åƒåœ¨ 4 ä¸ª GPU ä¸Šè¿›è¡Œåˆ†å¸ƒå¼è®­ç»ƒä¸€æ ·ï¼Œåœ¨å•ä¸ª GPU ä¸Šæµ‹è¯•ä½ çš„ä»£ç ã€‚
- en: 'There are several things to keep in mind:'
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æœ‰å‡ ç‚¹éœ€è¦æ³¨æ„ï¼š
- en: You are not actually performing distributed training, hence there is now performance
    gain through parallelization
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ä½ å®é™…ä¸Šå¹¶æ²¡æœ‰è¿›è¡Œåˆ†å¸ƒå¼è®­ç»ƒï¼Œå› æ­¤æ²¡æœ‰é€šè¿‡å¹¶è¡ŒåŒ–è·å¾—æ€§èƒ½æå‡ã€‚
- en: You need to assign the logical devices, before you connect to your hardware,
    otherwise an exception is raised.
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ä½ éœ€è¦åœ¨è¿æ¥ç¡¬ä»¶ä¹‹å‰åˆ†é…é€»è¾‘è®¾å¤‡ï¼Œå¦åˆ™ä¼šå¼•å‘å¼‚å¸¸ã€‚
- en: It only tests the correct implementation of your algorithm and you can check
    if the output shapes and values are as expected. It will not guarantee that all
    drivers and hardware in a multi-GPU setup is correct.
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å®ƒåªæµ‹è¯•ç®—æ³•çš„æ­£ç¡®å®ç°ï¼Œä½ å¯ä»¥æ£€æŸ¥è¾“å‡ºçš„å½¢çŠ¶å’Œå€¼æ˜¯å¦ç¬¦åˆé¢„æœŸã€‚å®ƒä¸èƒ½ä¿è¯å¤š GPU é…ç½®ä¸­çš„æ‰€æœ‰é©±åŠ¨ç¨‹åºå’Œç¡¬ä»¶éƒ½æ˜¯æ­£ç¡®çš„ã€‚
- en: Let me know in the comments if this trick is useful for you and if you already
    knew about this feature! For me it was a game changer.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœè¿™ä¸ªæŠ€å·§å¯¹ä½ æœ‰ç”¨ï¼Œæˆ–è€…ä½ å·²ç»çŸ¥é“è¿™ä¸ªåŠŸèƒ½ï¼Œè¯·åœ¨è¯„è®ºä¸­å‘Šè¯‰æˆ‘ï¼å¯¹æˆ‘æ¥è¯´ï¼Œè¿™çœŸæ˜¯ä¸€ä¸ªæ¸¸æˆæ”¹å˜è€…ã€‚
- en: Happy testing!ğŸ’ª
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ç¥ä½ æµ‹è¯•æ„‰å¿«ï¼ğŸ’ª
