- en: Speculative Sampling — Intuitively and Exhaustively Explained
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 投机采样 — 直观且全面的解释
- en: 原文：[https://towardsdatascience.com/speculative-sampling-intuitively-and-exhaustively-explained-2daca347dbb9](https://towardsdatascience.com/speculative-sampling-intuitively-and-exhaustively-explained-2daca347dbb9)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/speculative-sampling-intuitively-and-exhaustively-explained-2daca347dbb9](https://towardsdatascience.com/speculative-sampling-intuitively-and-exhaustively-explained-2daca347dbb9)
- en: Machine Learning | Natural Language Processing | Data Science
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 机器学习 | 自然语言处理 | 数据科学
- en: Exploring the drop-in strategy that’s speeding up language models by 3x
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 探索加速语言模型3倍的替代策略
- en: '[](https://medium.com/@danielwarfield1?source=post_page-----2daca347dbb9--------------------------------)[![Daniel
    Warfield](../Images/c1c8b4dd514f6813e08e401401324bca.png)](https://medium.com/@danielwarfield1?source=post_page-----2daca347dbb9--------------------------------)[](https://towardsdatascience.com/?source=post_page-----2daca347dbb9--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----2daca347dbb9--------------------------------)
    [Daniel Warfield](https://medium.com/@danielwarfield1?source=post_page-----2daca347dbb9--------------------------------)'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@danielwarfield1?source=post_page-----2daca347dbb9--------------------------------)[![Daniel
    Warfield](../Images/c1c8b4dd514f6813e08e401401324bca.png)](https://medium.com/@danielwarfield1?source=post_page-----2daca347dbb9--------------------------------)[](https://towardsdatascience.com/?source=post_page-----2daca347dbb9--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----2daca347dbb9--------------------------------)
    [Daniel Warfield](https://medium.com/@danielwarfield1?source=post_page-----2daca347dbb9--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----2daca347dbb9--------------------------------)
    ·12 min read·Dec 15, 2023
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----2daca347dbb9--------------------------------)
    ·阅读时间12分钟·2023年12月15日
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/c21285038373258e14b9f0b7ea4033a1.png)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c21285038373258e14b9f0b7ea4033a1.png)'
- en: “Speculators” by Daniel Warfield using MidJourney and Affinity Design 2\. All
    images by the author unless otherwise specified.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: “投机者”由Daniel Warfield使用MidJourney和Affinity Design 2制作。所有图像均由作者提供，除非另有说明。
- en: In this article we’ll discuss “Speculative Sampling”, a strategy that makes
    text generation faster and more affordable without compromising on performance.
    In doing so, we’ll take a thorough look at some of the more subtle aspects of
    language models.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们将讨论“投机采样”，这是一种使文本生成更快、更经济的策略，同时不妨碍性能。为此，我们将深入探讨语言模型的一些更微妙的方面。
- en: '![](../Images/37be7902d8b7476df84c826007418377.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/37be7902d8b7476df84c826007418377.png)'
- en: Empirical results of using speculative sampling on a variety of text generation
    tasks. Notice how, in all cases, generation time is significantly faster. [Source](https://arxiv.org/pdf/2211.17192.pdf)
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 使用投机采样在各种文本生成任务中的实证结果。请注意，在所有情况下，生成时间都显著更快。 [来源](https://arxiv.org/pdf/2211.17192.pdf)
- en: First we’ll discuss a major problem that’s slowing down modern language models,
    then we’ll build an intuitive understanding of how speculative sampling elegantly speeds them up,
    then we’ll implement speculative sampling from scratch in Python.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将讨论一个拖慢现代语言模型的主要问题，然后建立对投机采样如何优雅加速它们的直观理解，最后我们将用Python从头实现投机采样。
- en: '**Who is this useful for?** Anyone interested in natural language processing
    (NLP), or cutting edge AI advancements.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '**这对谁有用？** 对自然语言处理（NLP）或前沿人工智能进展感兴趣的任何人。'
- en: '**How advanced is this post?** The concepts in this article are accessible
    to machine learning enthusiasts, and are cutting edge enough to interest seasoned
    data scientists. The code at the end may be useful to developers.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '**这篇文章的难度如何？** 本文中的概念对机器学习爱好者来说是可以接受的，同时足够前沿，能引起经验丰富的数据科学家的兴趣。文末的代码可能对开发者有用。'
- en: '**Pre-requisites:** It might be useful to have a cursory understanding of Transformers,
    OpenAI’s GPT models, or both. If you find yourself confused, you can refer to
    either of these articles:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '**前提条件：** 了解变换器、OpenAI的GPT模型或两者之一可能会有帮助。如果你感到困惑，可以参考这两篇文章：'
- en: '[](/gpt-intuitively-and-exhaustively-explained-c70c38e87491?source=post_page-----2daca347dbb9--------------------------------)
    [## GPT — Intuitively and Exhaustively Explained'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/gpt-intuitively-and-exhaustively-explained-c70c38e87491?source=post_page-----2daca347dbb9--------------------------------)
    [## GPT — 直观且全面的解释'
- en: Exploring the architecture of OpenAI’s Generative Pre-trained Transformers.
  id: totrans-17
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 探索OpenAI生成预训练变换器的架构。
- en: towardsdatascience.com](/gpt-intuitively-and-exhaustively-explained-c70c38e87491?source=post_page-----2daca347dbb9--------------------------------)
    [](/transformers-intuitively-and-exhaustively-explained-58a5c5df8dbb?source=post_page-----2daca347dbb9--------------------------------)
    [## Transformers — Intuitively and Exhaustively Explained
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/gpt-intuitively-and-exhaustively-explained-c70c38e87491?source=post_page-----2daca347dbb9--------------------------------)
    [](/transformers-intuitively-and-exhaustively-explained-58a5c5df8dbb?source=post_page-----2daca347dbb9--------------------------------)
    [## 变换器 — 直观且详尽的解释
- en: 'Exploring the modern wave of machine learning: taking apart the transformer
    step by step'
  id: totrans-19
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 探索现代机器学习的潮流：一步步拆解变换器
- en: towardsdatascience.com](/transformers-intuitively-and-exhaustively-explained-58a5c5df8dbb?source=post_page-----2daca347dbb9--------------------------------)
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/transformers-intuitively-and-exhaustively-explained-58a5c5df8dbb?source=post_page-----2daca347dbb9--------------------------------)
- en: Language Models Are Getting Too Big
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 语言模型变得过于庞大
- en: Over the last four years OpenAI’s GPT models have grown from 117 million parameters
    in 2018 to an estimated 1.8 Trillion parameters in 2023\. This rapid growth can
    largely be attributed to the fact that, in language modeling, bigger is better.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去四年中，OpenAI 的 GPT 模型从 2018 年的 1.17 亿参数增长到 2023 年估计的 1.8 万亿参数。这一快速增长在很大程度上归因于在语言建模中，更大的模型效果更好。
- en: '![](../Images/dc29d1fea21cc0d5f25b71a079c856dd.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/dc29d1fea21cc0d5f25b71a079c856dd.png)'
- en: A graph of model size vs performance, showing that bigger is better. From [my
    article on GPT](https://medium.com/towards-data-science/gpt-intuitively-and-exhaustively-explained-c70c38e87491).
    [Original source](https://arxiv.org/pdf/2005.14165.pdf)
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 模型规模与性能的图表，显示出更大的模型效果更好。来自 [我关于 GPT 的文章](https://medium.com/towards-data-science/gpt-intuitively-and-exhaustively-explained-c70c38e87491)。
    [原始来源](https://arxiv.org/pdf/2005.14165.pdf)
- en: As a result, the last few years have been an arms race. Numerous companies have
    been dropping billions of dollars on fancy graphics cards to the schagrin of Fortnight
    players everywhere.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，过去几年中，行业变成了一场军备竞赛。众多公司在炫酷的显卡上投入了数十亿美元，这让 Fortnite 玩家感到不满。
- en: The issue is, the models are simply getting too big. Language models, like the
    ones used in ChatGPT, have to generate their responses one word at a time through
    a process called “autoregressive generation”. The bigger the model gets, the more
    money and time it takes to generate output word by word.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 问题在于，这些模型变得过于庞大。像 ChatGPT 使用的语言模型，需要通过一种称为“自回归生成”的过程逐字生成响应。模型越大，生成逐字输出所需的金钱和时间就越多。
- en: '![](../Images/14ad4313662eaa0d11b1eb52f8a26036.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/14ad4313662eaa0d11b1eb52f8a26036.png)'
- en: 'A decoder only style model, like GPT, iteratively constructing an output. The
    model takes an input “Translate to French: I am a manager”, and generates the
    response word by word by using the previous outputs as part of the input. This
    style of text generation is called “autoregressive generation”. [From my article
    on GPT](https://medium.com/towards-data-science/gpt-intuitively-and-exhaustively-explained-c70c38e87491)'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 像 GPT 这样的解码器单模型，通过逐步构建输出。模型接受输入“翻译成法语：我是一个经理”，并通过将之前的输出作为输入的一部分，逐字生成响应。这种文本生成方式称为“自回归生成”。
    [来自我关于 GPT 的文章](https://medium.com/towards-data-science/gpt-intuitively-and-exhaustively-explained-c70c38e87491)
- en: OpenAI’s GPT-4, [based on a leak by some guy on twitter](https://archive.is/2RQ8X#selection-833.1-873.202),
    uses a bunch of technologies to get around this problem. One of them, the topic
    of this article, is speculative sampling.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI 的 GPT-4，[基于某人在 Twitter 上的泄露信息](https://archive.is/2RQ8X#selection-833.1-873.202)，使用了一些技术来绕过这个问题。其中之一，就是本文的主题——投机采样。
- en: Speculative Sampling in a Nutshell
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 投机采样简要概述
- en: 'Speculative sampling (also known as “Speculative Decoding” or “Speculative
    Generation”) was simultaneously proposed in two papers, both of which suggest
    a speedup in text generation by around 3x:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 投机采样（也称为“投机解码”或“投机生成”）在两篇论文中同时被提出，两者都建议通过投机采样将文本生成速度提高约 3 倍：
- en: “[Accelerating Large Language Model Decoding with Speculative Sampling](https://arxiv.org/pdf/2302.01318.pdf)”,
    a paper by DeepMind,
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “[加速大型语言模型解码的投机采样](https://arxiv.org/pdf/2302.01318.pdf)”，这是 DeepMind 发表的一篇论文，
- en: “[Fast Inference from Transformers via Speculative Decoding](https://arxiv.org/pdf/2211.17192.pdf)”,
    a paper by Google.
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “[通过投机解码实现变换器的快速推断](https://arxiv.org/pdf/2211.17192.pdf)”，这是 Google 发表的一篇论文。
- en: Despite being published independently, both approaches are functionally identical,
    so we’ll treat them synonymously.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管两种方法是独立发布的，但功能上是相同的，因此我们将它们视为同义词。
- en: The fundamental idea of speculative sampling is that bigger language models
    are better because *some* examples of text generation are difficult, but not *all*
    examples. For instance, suppose you ask a language model about the geological
    composition of the moon. To formulate a coherent response the model has to understand
    fancy sciency stuff, and also has to put words like “a”, “and”, and “of” in the
    right spots. Knowing the moon consists of something called “Breccias” is more
    difficult than knowing the word “are” might come after the word “which”.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 推测采样的基本思想是，较大的语言模型更好，因为*有些*文本生成的例子是困难的，但不是*所有*例子。例如，假设你问一个语言模型关于月球的地质组成。为了形成一个连贯的回答，模型不仅需要理解复杂的科学内容，还需要将“a”、“and”和“of”等词放在正确的位置。知道月球由一种叫“Breccias”的物质组成比知道“are”可能会跟在“which”之后要困难。
- en: '![](../Images/4890a3bd9357e7925b6175a5e7610f95.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4890a3bd9357e7925b6175a5e7610f95.png)'
- en: A conceptual demonstration of generative difficulty. When a model predicts a
    response sequence, word by word, some of the words are difficult to predict because
    they require in depth knowledge, but some of them are easy because they can be
    inferred by simple grammar or context clues. In this example the text in red might
    be more difficult to predict than the text in blue.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 生成难度的概念性演示。当模型逐字预测响应序列时，一些词难以预测，因为它们需要深入的知识，而一些词则容易预测，因为可以通过简单的语法或上下文线索推断。在这个例子中，红色文本可能比蓝色文本更难预测。
- en: 'Speculative sampling exploits the idea of varying degrees of difficulty by
    using two language models; a target model and a draft model:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 推测采样利用了通过使用两个语言模型的不同难度程度的概念；一个目标模型和一个草稿模型：
- en: The target model is the super big, super smart model we’re trying to speed up.
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 目标模型是我们试图加速的超大、超智能模型。
- en: The draft model is a smaller, dumber, and faster model.
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 草稿模型是一个较小、较简单且更快的模型。
- en: The idea is to use the draft model to predict numerous words in the sequence,
    then ask the target model to confirm that all the generated words are good. We
    can throw away all disagreements, resulting in an output which is identical to
    what the target model would output if it was working alone.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 这个想法是使用草稿模型来预测序列中的多个词，然后让目标模型确认所有生成的词都是好的。我们可以丢弃所有不一致的部分，从而得到一个与目标模型独立工作时输出的结果相同的输出。
- en: '![](../Images/d100f94cb722db75aa4581543a8bd90d.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d100f94cb722db75aa4581543a8bd90d.png)'
- en: An example of speculative generation in action. In the first row, the draft
    model output “japan’s benchmark bond”, but the target model disagreed with “bond”
    in favor of “n”. The word “bond” was replaced with “n”, and anything the draft
    model might have predicted after the word “bond” is thrown out. In effect, this
    allows a speculative generation system to output multiple words for each pass
    of the target model. [source](https://arxiv.org/pdf/2211.17192.pdf)
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 一个具有推测生成的实际例子。在第一行中，草稿模型输出了“日本的基准债券”，但目标模型不同意“债券”，选择了“n”。词语“债券”被替换为“n”，草稿模型在“债券”之后可能预测的任何内容都被丢弃。实际上，这使得推测生成系统可以为目标模型的每次通过输出多个词。[source](https://arxiv.org/pdf/2211.17192.pdf)
- en: A Natural Question
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 一个自然的问题
- en: If you’re anything like me, you might be a bit confused. The common intuition,
    and the intuition that I communicated in both my [article on transformers](https://medium.com/towards-data-science/transformers-intuitively-and-exhaustively-explained-58a5c5df8dbb)
    and my [article on GPT](https://medium.com/towards-data-science/gpt-intuitively-and-exhaustively-explained-c70c38e87491),
    is that language models predict output word by word. Under that intuition it’s
    not exactly obvious how a target model might efficiently “double check” the output
    of the draft model; if the target model has to check predictions one by one, then
    what’s the point of going through the trouble of using the draft model in the
    first place?
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你和我一样，可能会感到有些困惑。常见的直觉，以及我在我的[变压器文章](https://medium.com/towards-data-science/transformers-intuitively-and-exhaustively-explained-58a5c5df8dbb)和我的[GPT文章](https://medium.com/towards-data-science/gpt-intuitively-and-exhaustively-explained-c70c38e87491)中传达的直觉，是语言模型逐字预测输出。在这种直觉下，目标模型如何高效地“二次检查”草稿模型的输出并不明显；如果目标模型必须逐个检查预测，那么最初使用草稿模型有什么意义呢？
- en: '![](../Images/14ad4313662eaa0d11b1eb52f8a26036.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/14ad4313662eaa0d11b1eb52f8a26036.png)'
- en: If a model like GPT outputs text word by word, wouldn’t it have to check the
    output of the draft model word by word, thus taking the same amount of time? No,
    and we’ll talk about why in the next section.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 如果像 GPT 这样的模型逐字输出文本，难道它不能像检查草稿模型的输出那样逐字检查吗？不会，我们将在下一节讨论原因。
- en: The idea of speculative sampling requires a thorough understanding of the exact
    output of transformers. There are some quirks which normally aren’t relevant,
    but are very relevant for speculative sampling.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 推测性采样的思想需要对 Transformer 的确切输出有透彻的理解。有一些通常不相关的细节，但对于推测性采样来说非常相关。
- en: The Secret Outputs of Transformers, and How Speculative Sampling Uses Them
  id: totrans-49
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Transformer 的秘密输出，以及推测性采样如何使用它们
- en: As I discussed in [my article on the original transformer architecture](https://medium.com/towards-data-science/transformers-intuitively-and-exhaustively-explained-58a5c5df8dbb),
    the thing that made transformers so special was their ability to parallelize training.
    Before transformers, models like LSTMs had to be trained word by word, which was
    a slow and expensive process.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我在[我关于原始 Transformer 架构的文章](https://medium.com/towards-data-science/transformers-intuitively-and-exhaustively-explained-58a5c5df8dbb)中讨论的那样，Transformer
    的特别之处在于它们能够并行化训练。在 Transformer 之前，像 LSTM 这样的模型必须逐字训练，这是一种缓慢且昂贵的过程。
- en: When a model like GPT is trained, an entire input sequence is provided at input,
    and the model is asked to predict that same sequence, just shifted by one word.
    The model is then trained to minimize the flaws of its predictions.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 当像 GPT 这样的模型进行训练时，会将整个输入序列提供给模型，模型被要求预测相同的序列，只是偏移了一个单词。然后，模型被训练以最小化其预测的缺陷。
- en: '![](../Images/8132621588836de8f5299cda729d66a6.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8132621588836de8f5299cda729d66a6.png)'
- en: An example of the training process for a language model, like GPT. The model
    is given an input sequence, shifted to the right by a token which designates the
    start of a sequence, and the model is asked to predict that same sequence, in
    a single pass. Any errors are used to train the model. Essentially, the model
    is trained to predict *all next words simultaneously.*
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 语言模型（如 GPT）训练过程的一个例子。模型被给定一个输入序列，该序列向右移动一个标记以指示序列的开始，然后模型被要求在一次传递中预测相同的序列。任何错误都用于训练模型。本质上，模型被训练来预测*所有下一个单词同时出现*。
- en: So, if the model has access to the entire input sequence, wouldn’t it cheat
    by just moving each word over by one space? No, and that’s because of masking.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，如果模型可以访问整个输入序列，它是否会通过将每个单词移到一个空间来作弊？不会，这要归功于掩码。
- en: 'Transformers use “masked” self attention, which essentially blocks out information
    about future words from reaching the information for a given word. I’ll probably
    cover masking in it’s own dedicated article, it’s definitely worthy of a deeper
    dive, but the intuition is this: By setting certain values in the self attention
    mechanism to zero, the prediction of a given word is not influenced by future
    words.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer 使用“掩码”自注意力，这本质上是阻止未来单词的信息到达给定单词的信息。我可能会在专门的文章中详细介绍掩码，这绝对值得深入探讨，但直观理解是：通过将自注意力机制中的某些值设为零，给定单词的预测不会受到未来单词的影响。
- en: '![](../Images/3f049bc3556dfd4a7bda8ac8ffaa8fcb.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3f049bc3556dfd4a7bda8ac8ffaa8fcb.png)'
- en: A conceptual diagram of training with masking. In effect, with masking, a language
    model is asked to predict all next words simultaneously.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 使用掩码的训练概念图。实际上，使用掩码时，语言模型被要求同时预测所有下一个单词。
- en: Typically, when using a transformer, we only care about a prediction of the
    next word in a sequence; that’s how we get text to generate and cause venture
    capitalists to empty their pockets. However, technically, the model has outputs
    for the entire sequence as if the next words in the sequence did not exist, because
    of the way the model is trained.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，当使用 Transformer 时，我们只关心序列中下一个单词的预测；这就是我们生成文本并促使风险投资家掏钱的方式。然而，技术上讲，模型的输出对于整个序列来说，就好像序列中的下一个单词不存在一样，这是由于模型的训练方式。
- en: '![](../Images/543b80612b86895b208df82bfd101d91.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/543b80612b86895b208df82bfd101d91.png)'
- en: The true output of a transformer based language model like GPT. While we usually
    only care about the final last word prediction, technically it predicts all next
    words in the sequence.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 基于 Transformer 的语言模型（如 GPT）的真实输出。虽然我们通常只关心最终的最后一个单词预测，但技术上它预测了序列中的所有下一个单词。
- en: And that’s how the target model can quickly check numerous predictions from
    the draft model. If we give the draft models output to the target model as input,
    and ask the target model to predict the next word, we can compare the predicted
    values for every word in the sequence. If there’s a discrepancy we can stop there
    and use the target model’s output.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是目标模型如何快速检查草稿模型的众多预测的方法。如果我们将草稿模型的输出作为输入提供给目标模型，并要求目标模型预测下一个词，我们可以比较序列中每个词的预测值。如果存在差异，我们可以停在那里并使用目标模型的输出。
- en: '![](../Images/95ee7441708d4b960e90910d2a1a247f.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/95ee7441708d4b960e90910d2a1a247f.png)'
- en: Suppose the text in blue was generated by the draft model, and the target model
    disagreed with some word in the sequence as highlighted in red and underlined.
    All draft generated before the disagreement can be accepted, and all drafted text
    after the disagreement must be rejected. At the point of first disagreement we
    use the output from the target model. In effect, we just generated “primarily
    made up of rocks and regolith” with a single pass of the target model.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 假设蓝色的文本是由草稿模型生成的，而目标模型不同意序列中某些用红色高亮和下划线标记的词。所有在分歧前生成的草稿都可以接受，而分歧后生成的所有文本必须被拒绝。在第一次分歧时我们使用目标模型的输出。实际上，我们通过目标模型的一次运行生成了“主要由岩石和月壤组成”。
- en: A cool note about this process generally. Every single time we run the target
    model, it predicts the next word in the sequence. The target model might confirm
    all of the predictions of the draft model, or it disagree with all of them. Regardless,
    the target model will always predict a new word. As a result, in a worst case
    scenario where the draft model consistently outputs incorrect information, the
    entire system is as fast as if we were only using the target model. In other words,
    **speculative sampling can’t slow down generation, it can only make generation
    faster** (at least, when it’s implemented correctly)**.**
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 关于这个过程的一点有趣说明。每次我们运行目标模型时，它都会预测序列中的下一个词。目标模型可能会确认草稿模型的所有预测，或者与所有预测不一致。无论如何，目标模型总是会预测一个新词。因此，在草稿模型持续输出错误信息的最坏情况下，整个系统的速度与仅使用目标模型时一样。换句话说，**推测采样不会减慢生成速度，它只能使生成速度更快**（至少，当它正确实现时）。
- en: Sequences, Tokens, TokenIds, Logits, and Probabilities
  id: totrans-65
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 序列、标记、标记ID、逻辑值和概率
- en: That was the theory. Before we dive into the code we should discuss some technical
    details about how transformers function.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是理论。在我们深入代码之前，我们应该讨论一些关于变换器如何工作的技术细节。
- en: Text, from a language modeling perspective, is conceptualized as a **sequence**;a
    list of “things” that come one after another. Typically these “things” can be
    conceptualized as words, but in reality they’re a bit more abstract than that.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 从语言建模的角度来看，文本被概念化为**序列**；一个一个接着的“元素”列表。通常这些“元素”可以被概念化为词，但实际上它们比这更抽象。
- en: A machine learning model first breaks the input sequence into **tokens**, which
    are the “things” that make up a sequence. This can be done using one of many algorithms,
    but the end result is that the input sequence is divided into atomic chunks. These
    might be individual words, portions of words, multiple words, punctuation, numbers,
    or spaces.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习模型首先将输入序列拆分为**标记**，这些标记是构成序列的“元素”。这可以通过多种算法完成，但最终结果是输入序列被分割成原子级的块。这些块可能是单个词、词的一部分、多个词、标点符号、数字或空格。
- en: '![](../Images/53e25c56aef9c03dd4a57a7c57faf0bd.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/53e25c56aef9c03dd4a57a7c57faf0bd.png)'
- en: An example of tokenization, using a tokenizer called “sentencepiece”
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 一个使用名为“sentencepiece”的分词器进行标记化的示例
- en: Each of the tokens extracted from a tokenizer has a unique number, called the
    **TokenId**. Typically, a transformer style model learns a representative vector
    for each TokenId, which then becomes the input to the model. There’s one vector
    associated with each TokenId, which the model optimizes throughout training.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 从分词器提取的每个标记都有一个独特的编号，称为**TokenId**。通常，变换器风格的模型会为每个TokenId学习一个代表向量，该向量随后成为模型的输入。每个TokenId都有一个与之关联的向量，模型在训练过程中对该向量进行优化。
- en: '![](../Images/239fbb45a0fe9c7980082b18f4c988a0.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/239fbb45a0fe9c7980082b18f4c988a0.png)'
- en: The same tokens, with their associated Ids
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 相同的标记及其关联的ID
- en: After the data goes through numerous rounds of self attention within the model,
    the data becomes an abstract sequence of vectors, one for each output. This is
    sometimes referred to as the “final hidden state”.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 数据经过模型内部多个自注意力轮次后，数据变成了每个输出一个的抽象向量序列。这有时被称为“最终隐藏状态”。
- en: '![](../Images/4a971ff4cf2d8a6bbb79f047d2b768a5.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4a971ff4cf2d8a6bbb79f047d2b768a5.png)'
- en: The input, which has vectors which cleanly correspond with each word, get passed
    through numerous layers of self attention. This process creates highly abstract
    representations. [From my article on transformers.](https://medium.com/towards-data-science/transformers-intuitively-and-exhaustively-explained-58a5c5df8dbb)
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 输入的向量与每个单词干净地对应，通过多个自注意力层。这一过程创建了高度抽象的表示。[来自我关于变换器的文章。](https://medium.com/towards-data-science/transformers-intuitively-and-exhaustively-explained-58a5c5df8dbb)
- en: This is passed through a language modeling head, which converts the model’s
    abstract representation into a representation that corresponds directly to the
    tokenizer. There’s a set number of TokenIds for a given tokenizer, and the language
    modeling head converts the output of the model into vectors which contain the
    same number of values.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 这通过一个语言建模头传递，该头将模型的抽象表示转换为与分词器直接对应的表示。对于给定的分词器，有一个固定数量的TokenIds，语言建模头将模型的输出转换为包含相同数量值的向量。
- en: '![](../Images/d4653ff0c7be9283288f5316e8c98972.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d4653ff0c7be9283288f5316e8c98972.png)'
- en: After the transformer does its thing, the final hidden state of the model is
    passed through a language modeling head, which re-structures the data into a format
    which directly corresponds to whatever tokenizer the model is being trained with.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在变换器完成其任务后，模型的最终隐藏状态通过语言建模头，该头将数据重构为直接对应于模型所训练的分词器的格式。
- en: These outputs are called **logits**. Typically, the term “logit” is used to
    refer to the unfiltered, unprocessed, true output of the model. This is the thing
    that usually gets optimized. logits are typically compared to each other using
    a softmax function, which converts the logits into **probabilities**. Big logit
    values become big probabilities, small logit values become small probabilities.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 这些输出被称为**logits**。通常，“logit”这个术语用来指代模型的未过滤、未处理的真实输出。这通常是被优化的内容。logits 通常通过 softmax
    函数进行比较，将 logits 转换为**概率**。大 logit 值变成大概率，小 logit 值变成小概率。
- en: '![](../Images/0b85b223d7731d4fb9471a97d9e33d7c.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0b85b223d7731d4fb9471a97d9e33d7c.png)'
- en: A conceptual diagram of logits getting converted to probabilities
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: logits 转换为概率的概念图。
- en: These probabilities can then be converted into tokens, which then can be used
    to construct the output sequence. There are a few ways to go about doing that
    though.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 这些概率可以被转换为令牌，然后可以用来构建输出序列。不过，有几种方法可以做到这一点。
- en: You can simply always choose to use the highest probability token
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你可以简单地总是选择使用最高概率的令牌。
- en: You could randomly select an output in a manner which is weighted by probability
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你可以以加权概率的方式随机选择输出。
- en: You could do a more complex strategy like “top K sampling”.
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你可以采用更复杂的策略，例如“top K 采样”。
- en: Regardless, the probabilities become a tokenId, that tokenId becomes the token
    itself, and from the tokens, the output can be constructed.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 无论如何，概率变成了一个tokenId，这个tokenId变成了令牌本身，从这些令牌中可以构建输出。
- en: 'So, to recap:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，总结一下：
- en: '**Sequence:** This is typically used in reference to the input and output text,
    but can also be conceptualized as a sequence of tokens, sequence of tokenIds,
    sequence of logits, sequence of probabilities, whatever. “The sequence” can mean
    a few things, depending on the context of the discussion'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Sequence：** 这通常用来指代输入和输出文本，但也可以概念化为令牌序列、TokenIds 序列、logits 序列、概率序列等。“序列”可以根据讨论的上下文有几种含义。'
- en: '**Token:** Text can be divided into atomic tokens with a tokenizer. These are
    used to break text up into atomic, predefined chunks. Sometimes these cleanly
    correspond to words, and sometimes they don’t.'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Token：** 文本可以通过分词器分割成原子令牌。这些用于将文本拆分成原子预定义的块。有时这些块与单词干净地对应，有时则不然。'
- en: '**TokenId:** Each token has a corresponding TokenId, which is simply a number.
    The model uses this number to retrieve a learned vector for that token, thus constructing
    the input to the model'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**TokenId：** 每个令牌都有一个对应的 TokenId，这只是一个数字。模型使用这个数字来检索该令牌的学习向量，从而构建模型的输入。'
- en: '**Logits and Probabilities:** After the model does its thing, it outputs a
    series of values. These are typically softmaxed, and thus turned into probabilities.
    The probabilities are used to select output tokens.'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Logits 和概率：** 在模型完成其操作后，它会输出一系列值。这些值通常会经过softmax处理，从而转化为概率。这些概率用于选择输出令牌。'
- en: Speculative Sampling in PyTorch
  id: totrans-93
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: PyTorch中的猜测性抽样
- en: Now that we understand logits, probabilities, and tokens, we can start diving
    into a practical example of Speculative Sampling.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们理解了logits、概率和令牌，我们可以开始深入了解猜测性抽样的实际例子。
- en: 'Let’s keep it simple: We’ll use the maximum logit to decide which token gets
    generated on each step. If both the draft and target models output the same max
    value, we’ll say they agree.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们保持简单：我们将使用最大logit值来决定每一步生成哪个令牌。如果草稿模型和目标模型都输出相同的最大值，我们将说它们达成了一致。
- en: 'Full code can be found here:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 完整的代码可以在这里找到：
- en: '[](https://github.com/DanielWarfield1/MLWritingAndResearch/blob/main/SpeculativeSampling.ipynb?source=post_page-----2daca347dbb9--------------------------------)
    [## MLWritingAndResearch/SpeculativeSampling.ipynb at main · DanielWarfield1/MLWritingAndResearch'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '[## MLWritingAndResearch/SpeculativeSampling.ipynb at main · DanielWarfield1/MLWritingAndResearch'
- en: Notebook Examples used in machine learning writing and research - MLWritingAndResearch/SpeculativeSampling.ipynb
    at…
  id: totrans-98
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 机器学习写作和研究中使用的笔记本示例 - MLWritingAndResearch/SpeculativeSampling.ipynb at…
- en: github.com](https://github.com/DanielWarfield1/MLWritingAndResearch/blob/main/SpeculativeSampling.ipynb?source=post_page-----2daca347dbb9--------------------------------)
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '[github.com](https://github.com/DanielWarfield1/MLWritingAndResearch/blob/main/SpeculativeSampling.ipynb?source=post_page-----2daca347dbb9--------------------------------)'
- en: Loading the Models
  id: totrans-100
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 加载模型
- en: First, we need a draft and a target model. I’m using T5 in this example, which
    stands for “Text to Text Transfer Transformer”. It’s an encoder-decoder style
    transformer ([like the one I talk about in this article](/transformers-intuitively-and-exhaustively-explained-58a5c5df8dbb)),
    which differs from a decoder only model ([like the one I talk about in this article](https://medium.com/p/c70c38e87491)).
    Regardless, it has a decoder, so it will work for our purposes. Also, conveniently,
    T5 comes in a variety of sizes, pre-traind, and easily available on huggingface.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要一个草稿模型和一个目标模型。在这个例子中，我使用的是T5，它代表“Text to Text Transfer Transformer”。它是一种编码器-解码器风格的变换器（[就像我在这篇文章中讨论的那样](/transformers-intuitively-and-exhaustively-explained-58a5c5df8dbb)），与仅解码器模型（[就像我在这篇文章中讨论的那样](https://medium.com/p/c70c38e87491)）不同。不过，它有一个解码器，所以它可以满足我们的需求。此外，方便的是，T5有多种尺寸，经过预训练，并且在huggingface上很容易获得。
- en: '[PRE0]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '[PRE1]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The whole idea of speculative decoding relies on the draft and target model
    having the same tokens. So, just to double check, I confirmed that the tokenizers
    for both models behaved similarly.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 猜测性解码的整个概念依赖于草稿模型和目标模型具有相同的令牌。因此，为了双重确认，我确认了两个模型的分词器行为相似。
- en: '[PRE2]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '![](../Images/48c587278e6d57843be23d46838fe591.png)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/48c587278e6d57843be23d46838fe591.png)'
- en: The “0”, in this case, means both tokenizers behave similarly
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，“0”表示两个分词器的行为相似
- en: Building Speculative Sampling
  id: totrans-108
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 构建猜测性抽样
- en: 'Once you have the models, you just, kinda.. do speculative sampling. Of course,
    as previously mentioned, to do speculative sampling productively you need a whole
    architecture that can handle parallelized cues of information. In this example
    I’m simply doing drafting and checking within the same loop on a single machine.
    It’s not a very complicated process, but there are some loops and logic that need
    to happen to get it all working. Here’s the code:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你拥有了模型，你只需要进行一些…猜测性抽样。正如之前提到的，要有效地进行猜测性抽样，你需要一个可以处理并行信息提示的完整架构。在这个例子中，我只是简单地在同一台机器上进行草稿和检查。这并不是一个非常复杂的过程，但确实有一些循环和逻辑需要实现才能使其正常工作。以下是代码：
- en: '[PRE3]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Once concluded, we can observe how many tokens were generated in each loop.
    In this example we’re asking the model to translate a famous quote from English
    to German:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦得出结论，我们可以观察每个循环生成了多少个令牌。在这个例子中，我们要求模型将一句名言从英语翻译成德语：
- en: '![](../Images/e12aec97563102b5ee5917ad119cbfe8.png)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e12aec97563102b5ee5917ad119cbfe8.png)'
- en: Every iteration of speculative sampling.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 每次猜测性抽样的迭代。
- en: As you can see, with the chosen task and models, most iterations did not have
    useful draft output. In some examples however, like 8 and 11, the draft model
    allowed the system to effectively generate five tokens in one run of the target
    model. The models used in this example are fairly small. I imagine, when dealing
    with larger models, the draft model would be more useful more often.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，使用所选择的任务和模型，大多数迭代并没有产生有用的草稿输出。然而在某些例子中，例如第8和第11，草稿模型允许系统在一次目标模型运行中有效地生成五个标记。这个例子中使用的模型相当小。我想象，当处理更大的模型时，草稿模型会更经常地发挥作用。
- en: '**Conclusion**'
  id: totrans-115
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**结论**'
- en: And that’s it. Speculative sampling is an incredibly elegant way to drastically
    speed up text generation. We use a small language model to quickly generate output,
    then (by using a quirk of masked attention during training) we can use a large
    language model to double check that work essentially for free. We only keep generated
    text that the larger model agrees with, so at the end we get the same output,
    only faster.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 就这样。推测性采样是一种极其优雅的方法，可以大幅度加快文本生成速度。我们使用一个小型语言模型快速生成输出，然后（通过利用训练期间掩蔽注意力的一个特性）我们可以使用大型语言模型来几乎免费地对这些工作进行双重检查。我们只保留大型模型认同的生成文本，因此最后得到的输出是相同的，只是更快。
- en: Follow For More!
  id: totrans-117
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 关注以获取更多更新！
- en: I describe papers and concepts in the ML space, with an emphasis on practical
    and intuitive explanations.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 我描述了 ML 领域的论文和概念，重点是实用和直观的解释。
- en: '[](https://medium.com/@danielwarfield1/subscribe?source=post_page-----2daca347dbb9--------------------------------)
    [## Get an email whenever Daniel Warfield publishes'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@danielwarfield1/subscribe?source=post_page-----2daca347dbb9--------------------------------)
    [## 订阅 Daniel Warfield 的最新邮件'
- en: Get an email whenever Daniel Warfield publishes By signing up, you will create
    a Medium account if you don't already…
  id: totrans-120
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 订阅 Daniel Warfield 的最新邮件，注册后，如果你还没有 Medium 账户，你将创建一个…
- en: medium.com](https://medium.com/@danielwarfield1/subscribe?source=post_page-----2daca347dbb9--------------------------------)
    [![](../Images/1f6f4c8a07d69cf53e055e0130a85b03.png)](https://www.buymeacoffee.com/danielwarfield)
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: medium.com](https://medium.com/@danielwarfield1/subscribe?source=post_page-----2daca347dbb9--------------------------------)
    [![](../Images/1f6f4c8a07d69cf53e055e0130a85b03.png)](https://www.buymeacoffee.com/danielwarfield)
- en: Never expected, always appreciated. By donating you allow me to allocate more
    time and resources towards more frequent and higher quality articles. [Learn More](https://www.buymeacoffee.com/danielwarfield)
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 从未预期，总是感激。通过捐赠，你使我能够投入更多时间和资源来创作更频繁和更高质量的文章。[了解更多](https://www.buymeacoffee.com/danielwarfield)
- en: '**Attribution:** All of the images in this document were created by Daniel
    Warfield, unless a source is otherwise provided. You can use any images in this
    post for your own non-commercial purposes, so long as you reference this article,
    [https://danielwarfield.dev](https://danielwarfield.dev/), or both.'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '**版权声明：** 本文档中的所有图片均由 Daniel Warfield 创建，除非另有说明。你可以将本帖子中的任何图片用于自己的非商业用途，只要你引用了这篇文章，[https://danielwarfield.dev](https://danielwarfield.dev/)，或两者都引用。'
