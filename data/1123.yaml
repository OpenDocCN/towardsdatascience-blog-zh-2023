- en: Serving ML Models with TorchServe
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用TorchServe服务ML模型
- en: 原文：[https://towardsdatascience.com/serving-ml-models-with-torchserve-1578eca5aa20?source=collection_archive---------9-----------------------#2023-03-29](https://towardsdatascience.com/serving-ml-models-with-torchserve-1578eca5aa20?source=collection_archive---------9-----------------------#2023-03-29)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/serving-ml-models-with-torchserve-1578eca5aa20?source=collection_archive---------9-----------------------#2023-03-29](https://towardsdatascience.com/serving-ml-models-with-torchserve-1578eca5aa20?source=collection_archive---------9-----------------------#2023-03-29)
- en: A complete end-to-end example of serving an ML model for image classification
    task
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 一个完整的端到端图像分类任务ML模型服务示例
- en: '[](https://medium.com/@summit.mnr?source=post_page-----1578eca5aa20--------------------------------)[![Andrey
    Golovin](../Images/3afbee89a80374b346e57c8f317c9b3a.png)](https://medium.com/@summit.mnr?source=post_page-----1578eca5aa20--------------------------------)[](https://towardsdatascience.com/?source=post_page-----1578eca5aa20--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----1578eca5aa20--------------------------------)
    [Andrey Golovin](https://medium.com/@summit.mnr?source=post_page-----1578eca5aa20--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@summit.mnr?source=post_page-----1578eca5aa20--------------------------------)[![Andrey
    Golovin](../Images/3afbee89a80374b346e57c8f317c9b3a.png)](https://medium.com/@summit.mnr?source=post_page-----1578eca5aa20--------------------------------)[](https://towardsdatascience.com/?source=post_page-----1578eca5aa20--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----1578eca5aa20--------------------------------)
    [Andrey Golovin](https://medium.com/@summit.mnr?source=post_page-----1578eca5aa20--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fc18c39659707&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fserving-ml-models-with-torchserve-1578eca5aa20&user=Andrey+Golovin&userId=c18c39659707&source=post_page-c18c39659707----1578eca5aa20---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----1578eca5aa20--------------------------------)
    ·8 min read·Mar 29, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F1578eca5aa20&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fserving-ml-models-with-torchserve-1578eca5aa20&user=Andrey+Golovin&userId=c18c39659707&source=-----1578eca5aa20---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fc18c39659707&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fserving-ml-models-with-torchserve-1578eca5aa20&user=Andrey+Golovin&userId=c18c39659707&source=post_page-c18c39659707----1578eca5aa20---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----1578eca5aa20--------------------------------)
    ·8 min read·2023年3月29日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F1578eca5aa20&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fserving-ml-models-with-torchserve-1578eca5aa20&user=Andrey+Golovin&userId=c18c39659707&source=-----1578eca5aa20---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F1578eca5aa20&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fserving-ml-models-with-torchserve-1578eca5aa20&source=-----1578eca5aa20---------------------bookmark_footer-----------)![](../Images/d444be3fa94d97d5bc6f951f3e33825c.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F1578eca5aa20&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fserving-ml-models-with-torchserve-1578eca5aa20&source=-----1578eca5aa20---------------------bookmark_footer-----------)![](../Images/d444be3fa94d97d5bc6f951f3e33825c.png)'
- en: Image by author
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：作者
- en: Motivation
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 动机
- en: This post will walk you through a process of serving your deep learning Torch
    model with the TorchServe framework.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 本文将引导你通过使用TorchServe框架服务你的深度学习Torch模型的过程。
- en: There are quite a bit of articles about this topic. However, typically they
    are focused either on deploying TorchServe itself or on writing custom handlers
    and getting the end results. That was a motivation for me to write this post.
    It covers both parts and gives end-to-end example.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 关于这个主题有很多文章。然而，通常这些文章要么专注于部署TorchServe本身，要么专注于编写自定义处理程序并获得最终结果。这是我写这篇文章的动机。本文涵盖了这两个部分，并提供了端到端的示例。
- en: The image classification challenge was taken as an example. At the end of the
    day you will be able to deploy TorchServe server, serve a model, send any random
    picture of a clothes and finally get the predicted label of a clothes class. I
    believe this is what people may expect from an ML model served as API endpoint
    for classification.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 以图像分类挑战为例。最终你将能够部署 TorchServe 服务器，提供模型服务，发送任何随机的衣物图片，并最终获得预测的衣物类别标签。我相信这就是人们对作为
    API 端点提供分类服务的 ML 模型的期望。
- en: Brief intro
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 简介
- en: Say, your data science team designed a wonderful DL model. It’s a great accomplishment
    with no doubts. However, to make a value out of it the model needs to be somehow
    exposed to the outside world (if it’s not a Kaggle competition). This is called
    model serving. In this post I’ll not touch serving patterns for batch operations
    as well as streaming patterns purely based on streaming frameworks. I’ll focus
    on one option of serving a model as API (never mind if this API is called by a
    streaming framework or by any custom service). More precisely, this option is
    the TorchServe framework.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 比如，你的数据科学团队设计了一个很棒的深度学习模型。这无疑是一个伟大的成就。然而，为了让它发挥价值，模型需要以某种方式向外界开放（如果不是 Kaggle
    竞赛的话）。这就是所谓的模型服务。在这篇文章中，我不会涉及批处理操作的服务模式以及基于流处理框架的流处理模式。我将专注于将模型作为 API 提供服务的一个选项（无论这个
    API 是由流处理框架还是任何自定义服务调用）。更准确地说，这个选项是 TorchServe 框架。
- en: 'So, when you decide to serve your model as API you have at least the following
    options:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，当你决定将模型作为 API 提供服务时，你至少有以下几种选择：
- en: web frameworks such as Flask, Django, FastAPI etc
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 网络框架如 Flask、Django、FastAPI 等等
- en: cloud services like AWS Sagemaker endpoints
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 云服务如 AWS Sagemaker 端点
- en: dedicated serving frameworks like Tensorflow Serving, Nvidia Triton and TorchServe
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 专用服务框架如 Tensorflow Serving、Nvidia Triton 和 TorchServe
- en: All have its pros and cons and the choice might be not always straightforward.
    Let’s practically explore the TorchServe option.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 每种方法都有其优缺点，选择可能并不总是简单明了的。让我们实际探索一下 TorchServe 选项。
- en: Structure
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结构
- en: The first part will briefly describe how a model was trained. It’s not important
    for the TorchServe however I believe it helps to follow the end-to-end process.
    Then a custom handler will be explained.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 第一部分将简要描述模型是如何训练的。这对 TorchServe 来说并不重要，但我认为这有助于跟踪端到端的过程。接下来，将解释自定义处理器。
- en: The second part will focus on deployment of the TorchServe framework.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 第二部分将重点讨论 TorchServe 框架的部署。
- en: 'Source code for this post is located here: [git repo](https://github.com/quasi-researcher/torchserve_example)'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 本文的源代码位于这里：[git repo](https://github.com/quasi-researcher/torchserve_example)
- en: Preparing a DL model
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备一个深度学习模型
- en: 'For this toy example I selected the image classification task based on FashionMNIST
    dataset. In case you’re not familiar with the dataset it’s 70k of grayscale 28x28
    images of different clothes. There are 10 classes of the clothes. So, a DL classification
    model will return 10 logit values. For the sake of simplicity a model is based
    on the TinyVGG architecture (in case you want to visualize it with CNN explainer):
    simply few convolution and max pooling layers with RELU activation. The notebook
    [*model_creation_notebook*](https://github.com/quasi-researcher/torchserve_example/blob/master/model_creation_notebook.ipynb)
    in the repo shows all the process of training and saving the model.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个示例，我选择了基于 FashionMNIST 数据集的图像分类任务。如果你不熟悉这个数据集，它包含 70k 张 28x28 像素的灰度图像，展示了不同的衣物。共有
    10 个衣物类别。因此，一个深度学习分类模型将返回 10 个 logit 值。为了简化，模型基于 TinyVGG 架构（如果你想用 CNN 解释器可视化它）：仅有少量卷积层和最大池化层，带有
    RELU 激活。仓库中的笔记本 [*model_creation_notebook*](https://github.com/quasi-researcher/torchserve_example/blob/master/model_creation_notebook.ipynb)
    展示了训练和保存模型的全部过程。
- en: 'In brief the notebook just downloads the data, defines the model architecture,
    trains the model and saves state dict with torch save. There are two artifacts
    relevant to TorchServe: a class with definition of the model architecture and
    the saved model (.pth file).'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，笔记本只是下载数据，定义模型架构，训练模型并用 torch 保存状态字典。有两个与 TorchServe 相关的文档：一个包含模型架构定义的类和保存的模型（.pth
    文件）。
- en: Preparing artifacts for TorchServe to serve the model
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为 TorchServe 准备文档以提供模型服务
- en: 'Two modules need to be prepared: model file and custom handler.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 需要准备两个模块：模型文件和自定义处理器。
- en: '**Model file**'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '**模型文件**'
- en: As per documentation “*A model file should contain the model architecture. This
    file is mandatory in case of eager mode models.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 根据文档，“*模型文件应包含模型架构。对于急切模式的模型，这个文件是必须的。
- en: This file should contain a single class that inherits from torch.nn.Module.*”
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 该文件应包含一个继承自 torch.nn.Module 的单个类。
- en: 'So, let’s just copy the class definition from the model training notebook and
    save it as *model.py* (any name you prefer):'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，让我们从模型训练笔记本中复制类定义，并将其保存为 *model.py*（你可以选择任何名称）：
- en: '**Handler**'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '**处理程序**'
- en: TorchServe offers some default handlers (e.g. image_classifier) but I doubt
    it can be used as is for real cases. So, most likely you will need to create a
    custom handler for your task. The handler actually defines how to preprocess data
    from http request, how to feed it into the model, how to postprocess the model’s
    output and what to return as the final result in the response.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: TorchServe 提供了一些默认处理程序（例如 image_classifier），但我怀疑它是否可以直接用于实际案例。因此，你很可能需要为你的任务创建一个自定义处理程序。处理程序实际上定义了如何从
    http 请求中预处理数据，如何将其输入模型，如何后处理模型的输出以及在响应中返回最终结果。
- en: There are two options — module level entry point and class level entry point.
    See the official documentation [here](https://github.com/pytorch/serve/blob/master/docs/custom_service.md).
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 有两个选项——模块级入口点和类级入口点。请查看官方文档[这里](https://github.com/pytorch/serve/blob/master/docs/custom_service.md)。
- en: 'I’ll implement the class level option. It basically means that I need to create
    a custom Python class and define two mandatory functions: *initialize* and *handle*.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 我将实现类级选项。这基本上意味着我需要创建一个自定义 Python 类并定义两个强制函数：*initialize* 和 *handle*。
- en: First of all, to make it easier let’s inherit from the *BaseHandler* class.
    The *initialize* function defines how to load the model. Since we don’t have any
    specific requirements here let’s just use the definition from the super class.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，为了简化操作，让我们继承 *BaseHandler* 类。*initialize* 函数定义了如何加载模型。由于我们这里没有任何具体要求，因此可以直接使用超类中的定义。
- en: 'The *handle* function basically defines how to process the data. In the simplest
    case the flow is: *preprocess >> inference >> postprocess*. In real applications
    likely you’ll need to define your custom preprocess and postprocess functions.
    For the inference function for this example I’ll use the default definition in
    the super class:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '*handle* 函数基本上定义了如何处理数据。在最简单的情况下，流程是：*预处理 >> 推理 >> 后处理*。在实际应用中，你可能需要定义自定义的预处理和后处理函数。对于这个示例的推理函数，我将使用超类中的默认定义：'
- en: '**Preprocess function**'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '**预处理函数**'
- en: 'Say, you built an app for image classification. The app sends the request to
    TorchServe with an image as payload. It’s probably unlikely that the image always
    complies with the image format used for model training. Also you’d probably train
    your model on batches of samples and tensor dimensions must be adjusted. So, let’s
    make a simple preprocess function: resize image to the required shape, make it
    grayscale, transform to Torch tensor and make it as one-sample batch.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 比如，你构建了一个图像分类应用。该应用将图像作为负载发送给 TorchServe。图像可能不会始终符合用于模型训练的图像格式。此外，你可能会在样本批次上训练模型，张量维度必须进行调整。因此，让我们创建一个简单的预处理函数：将图像调整为所需的形状，转换为灰度图像，转换为
    Torch 张量并将其作为单样本批次。
- en: '**Postprocess function**'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '**后处理函数**'
- en: A multiclass classification model will return a list of logit or softmax probabilities.
    But in real scenario you’d rather need a predicted class or a predicted class
    with the probability value or maybe top N predicted labels. Of course, you can
    do it somewhere in the main app/other service but it means you bind the logic
    of your app with the ML training process. So, let’s return the predicted class
    directly in the response.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 多类别分类模型将返回一个 logit 或 softmax 概率列表。但在实际场景中，你更可能需要一个预测类别或带有概率值的预测类别，或者可能是前 N 个预测标签。当然，你可以在主应用程序/其他服务中完成这些操作，但这意味着将应用程序逻辑与
    ML 训练过程绑定在一起。因此，让我们直接在响应中返回预测类别。
- en: (for the sake of simplicity the list of labels is hardcoded here. In github
    version the handler reads is from config)
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: （为了简单起见，这里的标签列表是硬编码的。在 github 版本中，处理程序从配置中读取标签）
- en: Start Torch server
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 启动 Torch 服务器
- en: Ok, the model file and the handler are ready. Now let’s deploy TorchServe server.
    Code above assumes that you have already installed pytorch. Another prerequisite
    is JDK 11 installed (note, just JRE is not enough, you need JDK).
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 好了，模型文件和处理程序都准备好了。现在让我们部署 TorchServe 服务器。上面的代码假设你已经安装了 pytorch。另一个前提条件是安装了 JDK
    11（注意，仅 JRE 不够，你需要 JDK）。
- en: 'For TorchServe you need to install two packages: *torchserve* and *torch-model-archiver*.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 TorchServe，你需要安装两个包：*torchserve* 和 *torch-model-archiver*。
- en: 'After successful installation the first step is to prepare a .mar file — archive
    with the model artifacts. CLI interface of torch-model-archiver is aimed to do
    it. Type in terminal:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 成功安装后，第一步是准备一个 .mar 文件——包含模型工件的档案。torch-model-archiver 的 CLI 接口旨在实现这一点。终端中输入：
- en: '[PRE0]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Arguments are the following:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 参数如下：
- en: '-*model name*: a name you want to give to the model'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: -*模型名称*：你想给模型起的名称
- en: '-*version*: semantic version for versioning'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: -*版本*：用于版本控制的语义版本
- en: '-*model file*: file with class definition of the model architecture'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: -*模型文件*：包含模型架构类定义的文件
- en: '-*serialized file*: .pth file from torch.save()'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: -*序列化文件*：来自 torch.save() 的 .pth 文件
- en: '-*handler*: Python module with handler'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: -*处理器*：包含处理器的 Python 模块
- en: As a result the .mar file called as model name (in this example *fashion_mnist.mar*)
    will be generated in the directory where CLI command is executed. So, better to
    cd to your project directory before calling the command.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 结果是一个名为模型名称的 .mar 文件（在这个例子中是 *fashion_mnist.mar*）将在执行 CLI 命令的目录中生成。因此，最好在调用命令之前
    cd 到你的项目目录。
- en: 'Next step finally is to start the server. Type in terminal:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是启动服务器。终端中输入：
- en: '[PRE1]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Arguments:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 参数：
- en: '-*model store*: directory where the mar files are located'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: -*模型存储*：存放 mar 文件的目录
- en: '-*models*: name(s) of the model(s) and path to the corresponding mar file.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: -*模型*：模型名称和对应 mar 文件的路径。
- en: Note, that model name in archiver defines how your .mar file will be named.
    The model name in torchserve defines the API endpoint name to invoke the model.
    So, those names can be the same or different, it’s up to you.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，归档器中的模型名称定义了你的 .mar 文件将如何命名。torchserve 中的模型名称定义了调用模型的 API 端点名称。所以，这些名称可以相同也可以不同，取决于你。
- en: 'After those two command the server shall be up and running. By default TorchServe
    uses three ports: 8080, 8081 and 8082 for inference, management and metrics correspondingly.
    Go to your browser/curl/Postman and send a request to'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 执行这两个命令后，服务器应该启动并运行。默认情况下，TorchServe 使用三个端口：8080、8081 和 8082 分别用于推理、管理和指标。打开浏览器/curl/Postman
    发送请求到
- en: '*http://localhost:8080/ping*'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '*http://localhost:8080/ping*'
- en: 'If TorchServe works correctly you should see *{‘status’: ‘Healthy’}*'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '如果 TorchServe 正常工作，你应该看到 *{‘status’: ‘Healthy’}*'
- en: '![](../Images/0df4ccbecb4662d7a41d15f337d6c058.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0df4ccbecb4662d7a41d15f337d6c058.png)'
- en: Image by author
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源于作者
- en: '**A couple of hints for possible issues**:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '**一些可能问题的提示**：'
- en: 1\. If after *torchserve -start* command you see errors in the log with mentioning
    “*..no module named captum*” then install it manually. I encountered this error
    with the *torchserve 0.7.1*
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 1\. 如果在 *torchserve -start* 命令后你在日志中看到提到“*..no module named captum*”的错误，那么请手动安装它。我在
    *torchserve 0.7.1* 中遇到了这个错误
- en: 2\. It may happen that some port is already busy with another process. Then
    likely you will see ‘*Partially healthy*’ status and some errors in log.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 2\. 可能会有某些端口已经被另一个进程占用。那你可能会看到 ‘*Partially healthy*’ 状态和日志中的一些错误。
- en: 'To check which process uses the port on Mac type (for example for 8081):'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 要检查哪个进程使用了 Mac 上的端口（例如 8081），输入：
- en: '[PRE2]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: One option can be to kill the process to free the port. But it might be not
    always a good idea if the process is somehow important.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 一种选择是终止进程以释放端口。但如果该进程很重要，这可能并不是一个好主意。
- en: 'Instead it’s possible to specify any new port for TorchServe in a simple config
    file. Say, you have some application which is already working on 8081 port. Let’s
    change the default port for TorchServe management API by creating torch_config
    file with just one line:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 另外，可以在简单的配置文件中为 TorchServe 指定任何新的端口。假设你有一个已经在 8081 端口上运行的应用程序。通过创建包含一行的 torch_config
    文件来更改默认的 TorchServe 管理 API 端口：
- en: '[PRE3]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '*(you can choose any free port)*'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '*(你可以选择任何空闲端口)*'
- en: Next we need to let TorchServe know about the config. First, stop the unhealthy
    server by
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来我们需要让 TorchServe 知道配置。首先，通过以下命令停止不健康的服务器
- en: '[PRE4]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Then restart it as
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 然后重新启动它：
- en: '[PRE5]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Request the predictions from the model
  id: totrans-80
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 请求模型的预测
- en: At this step it’s assumed the server is up and running correctly. Let’s pass
    a random clothes image to the inference API and get the predicted label.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一步假设服务器已正确启动。让我们将随机的衣物图像传递给推理 API 并获取预测标签。
- en: The endpoint for inference is
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 推理的端点是
- en: '*http://localhost:8080/predictions/model_name*'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '*http://localhost:8080/predictions/model_name*'
- en: In this example it’s *http://localhost:8080/predictions/fmnist*
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中是 *http://localhost:8080/predictions/fmnist*
- en: Let’s curl it and pass an image as
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用 curl 传递一个图像：
- en: '[PRE6]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'for example with the sample image from the repo:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，使用 repo 中的示例图像：
- en: '[PRE7]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '*(X flag is to specify the method /POST/, -T flag is to transfer a file)*'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '*(X 标志用于指定方法 /POST/，-T 标志用于传输文件)*'
- en: 'In the response we should see the predicted label:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 在响应中，我们应该看到预测的标签：
- en: '![](../Images/7ba3d410efe561befca4dbac6a11f21a.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7ba3d410efe561befca4dbac6a11f21a.png)'
- en: Image by author
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供
- en: Conclusion
  id: totrans-93
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: 'Well, by following along this blog post we were able to create a REST API endpoint
    to which we can send an image and get the predicted label of the image. By repeating
    the same procedure on a server instead of local machine one can leverage it to
    create an endpoint for user-facing app, for other services or for instance endpoint
    for streaming ML application (see this interesting paper for a reason why you
    likely should not do that: [*https://sites.bu.edu/casp/files/2022/05/Horchidan22Evaluating.pdf*](https://sites.bu.edu/casp/files/2022/05/Horchidan22Evaluating.pdf))'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 好吧，通过跟随这篇博客文章，我们能够创建一个REST API端点，我们可以向其发送图像并获取图像的预测标签。通过在服务器上重复相同的过程，而不是本地机器，一个人可以利用它来为面向用户的应用程序、其他服务，或者例如流式机器学习应用程序创建一个端点（参见这篇有趣的论文了解为什么你可能不应该这样做：[*https://sites.bu.edu/casp/files/2022/05/Horchidan22Evaluating.pdf*](https://sites.bu.edu/casp/files/2022/05/Horchidan22Evaluating.pdf)）
- en: 'Stay tuned, in the next part I’ll expand the example: let’s make a mock of
    Flask app for business logic and invoke an ML model served via TorchServe (and
    deploy everything with Kubernetes).'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 敬请关注，在下一部分中我将扩展示例：让我们为业务逻辑创建一个Flask应用程序的模拟，并通过TorchServe调用一个机器学习模型（并使用Kubernetes部署所有内容）。
- en: 'A simple use case: user-facing app with tons of business logic and with many
    different features. Say, one feature is uploading an image to apply a desired
    style to it with a style transfer ML model. The ML model can be served with TorchServe
    and thus the ML part will be completely decoupled from business logic and other
    features in the main app.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 一个简单的用例：面向用户的应用程序，具有大量业务逻辑和许多不同的功能。比如，一个功能是上传图像，将所需的样式应用于图像，使用样式迁移机器学习模型。机器学习模型可以通过TorchServe提供，因此机器学习部分将完全与主应用程序中的业务逻辑和其他功能解耦。
