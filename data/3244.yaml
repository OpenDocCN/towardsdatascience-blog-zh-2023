- en: A comprehensive guide of Distributed Data Parallel (DDP)
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 《分布式数据并行（DDP）综合指南》
- en: 原文：[https://towardsdatascience.com/a-comprehensive-guide-of-distributed-data-parallel-ddp-2bb1d8b5edfb?source=collection_archive---------4-----------------------#2023-10-30](https://towardsdatascience.com/a-comprehensive-guide-of-distributed-data-parallel-ddp-2bb1d8b5edfb?source=collection_archive---------4-----------------------#2023-10-30)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/a-comprehensive-guide-of-distributed-data-parallel-ddp-2bb1d8b5edfb?source=collection_archive---------4-----------------------#2023-10-30](https://towardsdatascience.com/a-comprehensive-guide-of-distributed-data-parallel-ddp-2bb1d8b5edfb?source=collection_archive---------4-----------------------#2023-10-30)
- en: A comprehensive guide on how to speed up the training of your models with Distributed
    Data Parallel (DDP)
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 关于如何使用分布式数据并行（DDP）加速模型训练的综合指南
- en: '[](https://medium.com/@francoisporcher?source=post_page-----2bb1d8b5edfb--------------------------------)[![François
    Porcher](../Images/9ddb233f8cadbd69026bd79e2bd62dea.png)](https://medium.com/@francoisporcher?source=post_page-----2bb1d8b5edfb--------------------------------)[](https://towardsdatascience.com/?source=post_page-----2bb1d8b5edfb--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----2bb1d8b5edfb--------------------------------)
    [François Porcher](https://medium.com/@francoisporcher?source=post_page-----2bb1d8b5edfb--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@francoisporcher?source=post_page-----2bb1d8b5edfb--------------------------------)[![François
    Porcher](../Images/9ddb233f8cadbd69026bd79e2bd62dea.png)](https://medium.com/@francoisporcher?source=post_page-----2bb1d8b5edfb--------------------------------)[](https://towardsdatascience.com/?source=post_page-----2bb1d8b5edfb--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----2bb1d8b5edfb--------------------------------)
    [François Porcher](https://medium.com/@francoisporcher?source=post_page-----2bb1d8b5edfb--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F8e8e73046f53&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-comprehensive-guide-of-distributed-data-parallel-ddp-2bb1d8b5edfb&user=Fran%C3%A7ois+Porcher&userId=8e8e73046f53&source=post_page-8e8e73046f53----2bb1d8b5edfb---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----2bb1d8b5edfb--------------------------------)
    ·12 min read·Oct 30, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F2bb1d8b5edfb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-comprehensive-guide-of-distributed-data-parallel-ddp-2bb1d8b5edfb&user=Fran%C3%A7ois+Porcher&userId=8e8e73046f53&source=-----2bb1d8b5edfb---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F8e8e73046f53&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-comprehensive-guide-of-distributed-data-parallel-ddp-2bb1d8b5edfb&user=Fran%C3%A7ois+Porcher&userId=8e8e73046f53&source=post_page-8e8e73046f53----2bb1d8b5edfb---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----2bb1d8b5edfb--------------------------------)
    ·12分钟阅读·2023年10月30日'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F2bb1d8b5edfb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-comprehensive-guide-of-distributed-data-parallel-ddp-2bb1d8b5edfb&source=-----2bb1d8b5edfb---------------------bookmark_footer-----------)![](../Images/f23ad409732c360931dcb34f473cc0a5.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F2bb1d8b5edfb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-comprehensive-guide-of-distributed-data-parallel-ddp-2bb1d8b5edfb&source=-----2bb1d8b5edfb---------------------bookmark_footer-----------)![](../Images/f23ad409732c360931dcb34f473cc0a5.png)'
- en: Image by Author
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供
- en: Introduction
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 简介
- en: Hi everyone! I am Francois, Research Scientist at Meta. Welcome to this new
    tutorial part of the series [Awesome AI Tutorials](https://github.com/FrancoisPorcher/awesome-ai-tutorials).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 大家好！我是Francois，Meta的研究科学家。欢迎来到这个新教程，它是系列教程 [Awesome AI Tutorials](https://github.com/FrancoisPorcher/awesome-ai-tutorials)
    的一部分。
- en: In this tutorial we are going to demistify a well known technique called DDP
    to train models on several GPUs at the same time.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在本教程中，我们将揭示一种广为人知的技术——DDP，用于同时在多个GPU上训练模型。
- en: During my days at engineering school, I recall leveraging Google Colab’s GPUs
    for training. However, in the corporate realm, the landscape is different. If
    you’re part of an organization that’s heavily invested in AI — particularly if
    you’re within a tech giant — you likely have a wealth of GPU clusters at your
    disposal.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在我读工程学校的时候，我记得使用Google Colab的GPU进行训练。然而，在企业界，情况有所不同。如果你所在的组织大力投资于AI——尤其是如果你在一家科技巨头公司——你很可能有大量的GPU集群可供使用。
- en: his session aims to equip you with the knowledge to harness the power of multiple
    GPUs, enabling swift and efficient training. And guess what? It’s simpler than
    you might think! Before we proceed, I recommend having a good grasp of PyTorch,
    including its core components like Datasets, DataLoaders, Optimizers, CUDA, and
    the training loop.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 本节旨在为你提供利用多个GPU的知识，从而实现快速高效的训练。猜猜看？这比你想象的要简单！在我们继续之前，我建议你对PyTorch有一个良好的掌握，包括其核心组件，如Datasets、DataLoaders、Optimizers、CUDA以及训练循环。
- en: Initially, I viewed DDP as a complex, nearly unattainable tool, thinking it
    would require a large team to set up the necessary…
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 起初，我把DDP视为一个复杂的、几乎难以实现的工具，认为需要一个大型团队来设置所需的…
