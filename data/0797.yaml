- en: Introduction to PyTorch
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: PyTorch 介绍
- en: 原文：[https://towardsdatascience.com/introduction-to-pytorch-e0235570a080?source=collection_archive---------11-----------------------#2023-03-01](https://towardsdatascience.com/introduction-to-pytorch-e0235570a080?source=collection_archive---------11-----------------------#2023-03-01)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/introduction-to-pytorch-e0235570a080?source=collection_archive---------11-----------------------#2023-03-01](https://towardsdatascience.com/introduction-to-pytorch-e0235570a080?source=collection_archive---------11-----------------------#2023-03-01)
- en: Going through the Workflow of a PyTorch Project
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 了解 PyTorch 项目的工作流程
- en: '[](https://medium.com/@pumaline?source=post_page-----e0235570a080--------------------------------)[![Datamapu](../Images/63b0c7f9a3d160c5bb039bbebd791f7e.png)](https://medium.com/@pumaline?source=post_page-----e0235570a080--------------------------------)[](https://towardsdatascience.com/?source=post_page-----e0235570a080--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----e0235570a080--------------------------------)
    [Datamapu](https://medium.com/@pumaline?source=post_page-----e0235570a080--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@pumaline?source=post_page-----e0235570a080--------------------------------)[![Datamapu](../Images/63b0c7f9a3d160c5bb039bbebd791f7e.png)](https://medium.com/@pumaline?source=post_page-----e0235570a080--------------------------------)[](https://towardsdatascience.com/?source=post_page-----e0235570a080--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----e0235570a080--------------------------------)
    [Datamapu](https://medium.com/@pumaline?source=post_page-----e0235570a080--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ffcd72d75ae6e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintroduction-to-pytorch-e0235570a080&user=Datamapu&userId=fcd72d75ae6e&source=post_page-fcd72d75ae6e----e0235570a080---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----e0235570a080--------------------------------)
    ·13 min read·Mar 1, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fe0235570a080&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintroduction-to-pytorch-e0235570a080&user=Datamapu&userId=fcd72d75ae6e&source=-----e0235570a080---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ffcd72d75ae6e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintroduction-to-pytorch-e0235570a080&user=Datamapu&userId=fcd72d75ae6e&source=post_page-fcd72d75ae6e----e0235570a080---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----e0235570a080--------------------------------)
    · 13 分钟阅读 · 2023年3月1日 [](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fe0235570a080&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintroduction-to-pytorch-e0235570a080&user=Datamapu&userId=fcd72d75ae6e&source=-----e0235570a080---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fe0235570a080&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintroduction-to-pytorch-e0235570a080&source=-----e0235570a080---------------------bookmark_footer-----------)![](../Images/1e2d7992b69d15a3657c1c6969a38233.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fe0235570a080&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintroduction-to-pytorch-e0235570a080&source=-----e0235570a080---------------------bookmark_footer-----------)![](../Images/1e2d7992b69d15a3657c1c6969a38233.png)'
- en: Photo by Igor Lepilin on Unsplash
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：Igor Lepilin 在 Unsplash
- en: In this article, we will go through the lifecycle of a Deep Learning project
    using PyTorch. We assume that you are already familiar with Neural Networks and
    will not explain them in detail, but only consider the PyTorch-specific aspects.
    We will mainly follow the steps shown in the [official documentation](https://pytorch.org/tutorials/beginner/basics/intro.html)
    of PyTorch, but consider a different example. In the documentation an example
    of image classification is presented, here we will consider tabular data stored
    in a .csv file. This means that some changes will be necessary, especially in
    preparing the dataset. Having two different examples should help to understand
    the general workflow of a PyTorch project better. Additionally to this post, you
    can follow the [colab notebook](https://drive.google.com/file/d/1p31TH09BExMYyo-cm2DfcacoTdxONgwe/view?usp=sharing)
    including the entire code and workflow structure, or find the notebook on [GitHub](https://github.com/froukje/articles/blob/main/06_pytorch_introduction.ipynb).
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们将通过使用PyTorch的深度学习项目生命周期。我们假设你已经对神经网络有一定了解，因此不会详细解释它们，只会关注PyTorch特有的方面。我们将主要遵循[官方文档](https://pytorch.org/tutorials/beginner/basics/intro.html)中展示的步骤，但考虑一个不同的例子。在文档中展示的是图像分类的例子，而这里我们将考虑存储在.csv文件中的表格数据。这意味着一些更改是必要的，特别是在准备数据集时。拥有两个不同的例子应该有助于更好地理解PyTorch项目的一般工作流程。除了这篇文章，你还可以查看包含完整代码和工作流程结构的[colab笔记本](https://drive.google.com/file/d/1p31TH09BExMYyo-cm2DfcacoTdxONgwe/view?usp=sharing)，或者在[GitHub](https://github.com/froukje/articles/blob/main/06_pytorch_introduction.ipynb)上找到该笔记本。
- en: Data
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据
- en: The data used is downloaded from [kaggle](https://www.kaggle.com/datasets/adityakadiwal/water-potability)
    [1] and is freely available. The data describes different features that are needed
    to determine the water quality in an urban environment. The objective is to predict
    whether the water quality is good or bad. That is we are considering a binary
    classification problem. In total 9 features, which are all numerical, and the
    label are given.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 使用的数据从[kaggle](https://www.kaggle.com/datasets/adityakadiwal/water-potability)
    [1]下载，且免费提供。这些数据描述了在城市环境中确定水质所需的不同特征。目标是预测水质是好还是坏。也就是说，我们正在考虑一个二分类问题。总共有9个特征（均为数值型）和标签。
- en: '![](../Images/0657b63353cb716606c32939eca8dcdb.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0657b63353cb716606c32939eca8dcdb.png)'
- en: The features and the label of the given dataset.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 给定数据集的特征和标签。
- en: Preprocessing
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 预处理
- en: As in every Data Science project, first, some preprocessing needs to be done.
    This is independent of the deep learning framework we use. Therefore we won´t
    go into detail here. For further information, please refer to the [colab notebook](https://colab.research.google.com/drive/1p31TH09BExMYyo-cm2DfcacoTdxONgwe)
    or [GitHub](https://github.com/froukje/articles/blob/main/06_pytorch_introduction.ipynb).
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 与每个数据科学项目一样，首先需要进行一些预处理。这与我们使用的深度学习框架无关。因此，我们在这里不详细讨论。有关更多信息，请参见[colab笔记本](https://colab.research.google.com/drive/1p31TH09BExMYyo-cm2DfcacoTdxONgwe)或[GitHub](https://github.com/froukje/articles/blob/main/06_pytorch_introduction.ipynb)。
- en: We are lucky and the data does not need very much preparation. All features
    are numerical and float type. There are however some missing values, which we
    imputed with the mean of the corresponding feature. Also to consider is that the
    target variable is not equally distributed, but there are much more 0s (bad water
    quality) than 1s (good water quality). For the sake of simplicity, we upsampled
    the data, such that the number of 0s is equal to the number of 1s by randomly
    drawing samples from the subset of 1s until the same number of samples is reached.
    Finally, we divided the dataset into a training (80%), validation (10%), and test
    (10%) set and scaled the data.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 我们很幸运，数据不需要太多准备。所有特征都是数值型和浮点型。然而，有一些缺失值，我们用相应特征的均值进行了填补。还需考虑的是，目标变量并不均匀分布，0（差的水质）的数量远多于1（良好的水质）。为了简化，我们对数据进行了上采样，使得0的数量与1的数量相等，通过从1的子集随机抽样直到达到相同数量的样本。最后，我们将数据集划分为训练集（80%）、验证集（10%）和测试集（10%），并对数据进行了缩放。
- en: Create a PyTorch Dataset
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建一个PyTorch数据集
- en: 'In order to use our data in a PyTorch model, we need to bring it into a specific
    form: a *PyTorch Dataset.* The construction of this dataset is decoupled from
    the model. The dataset object stores the samples and their corresponding labels.
    At this point, this example deviates slightly from the PyTorch documentation page.
    The example data used in the documentation is [FashionMNIST](https://www.kaggle.com/datasets/zalando-research/fashionmnist).
    For this (and several other datasets) PyTorch offers pre-loaded datasets. To see
    how to load these datasets, you can check their [PyTorch tutorial](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html).
    However, if you want to use PyTorch for your own data you most likely have to
    write your own customized Dataset class.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在 PyTorch 模型中使用我们的数据，我们需要将其转化为特定的形式：*PyTorch 数据集*。该数据集的构建与模型解耦。数据集对象存储样本及其对应的标签。此时，这个例子略微偏离了
    PyTorch 文档页面。文档中使用的示例数据是 [FashionMNIST](https://www.kaggle.com/datasets/zalando-research/fashionmnist)。对于这个（和其他几个）数据集，PyTorch
    提供了预加载的数据集。要了解如何加载这些数据集，可以查看他们的 [PyTorch 教程](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html)。然而，如果你想用
    PyTorch 处理自己的数据，你很可能需要编写自己的自定义数据集类。
- en: 'To create a customized Dataset class, we can inherit from the Dataset class
    provided by PyTorch. We have to adjust the three following main methods:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 要创建自定义的数据集类，我们可以从 PyTorch 提供的 Dataset 类继承。我们需要调整以下三个主要方法：
- en: The **__init__** method is run once when instantiating the dataset object. In
    this simple example, only the input and labels are stored as tensors
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '**__init__** 方法在实例化数据集对象时运行一次。在这个简单的例子中，仅将输入和标签作为张量存储。'
- en: The **__len__** method returns the number of samples in our dataset.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '**__len__** 方法返回数据集中样本的数量。'
- en: The **__getitem__** method loads and returns a sample from the dataset at the
    given index.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '**__getitem__** 方法加载并返回数据集中给定索引处的样本。'
- en: 'The dataset is also the place for transformations when working e.g. with image
    data. In our tabular data, this is not relevant and therefore not covered here.
    For the water quality problem considered here, the customized Dataset class looks
    as follows:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集也是进行变换的地方，例如处理图像数据时。在我们的表格数据中，这一点不相关，因此在这里不涉及。对于这里考虑的水质问题，自定义数据集类如下所示：
- en: '[PRE0]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Using this class, we define the datasets for the training, validation, and test
    data.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这个类，我们定义训练、验证和测试数据集。
- en: '[PRE1]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Define the DataLoader
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 定义 DataLoader
- en: After creating a Dataset, we use the PyTorch DataLoader to wrap an iterable
    around it that permits to easy access the data during training and validation.
    The Dataset retrieves our dataset’s features and labels one sample at a time.
    When training a model, we usually want to pass samples of batches and reshuffle
    the data at every epoch. In this example, when iterating through the DataLoader,
    each iteration returns a minibatch of 32 samples. It is possible to further configure
    the DataLoader. For all possible configurations please refer to the [documentation](https://pytorch.org/docs/stable/data.html).
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 创建数据集后，我们使用 PyTorch 的 DataLoader 将可迭代对象包装起来，以便在训练和验证期间轻松访问数据。数据集一次获取一个样本的特征和标签。在训练模型时，我们通常希望以批次的形式传递样本，并在每个
    epoch 重新洗牌数据。在这个例子中，迭代 DataLoader 时，每次迭代返回一个包含 32 个样本的迷你批次。可以进一步配置 DataLoader。有关所有可能的配置，请参阅
    [文档](https://pytorch.org/docs/stable/data.html)。
- en: '[PRE2]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Define a Model
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 定义一个模型
- en: Now, the data is prepared and we are ready to define a model. We assume that
    you are familiar with the general structure of a Neural Net. In PyTorch the **torch.nn**
    namespace provides all the building blocks to create a Neural Net. The model we
    use in this example is very simple and only consists of [linear layers](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear),
    the [ReLu activation function](https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html#torch.nn.ReLU),
    and a [Dropout layer](https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html).
    For an overview of all pre-defined layers in PyTorch, please refer to the [documentation](https://pytorch.org/docs/stable/nn.html).
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，数据已经准备好，我们可以定义模型了。我们假设你对神经网络的基本结构有所了解。在 PyTorch 中，**torch.nn** 命名空间提供了创建神经网络的所有构建模块。我们在这个例子中使用的模型非常简单，仅包含
    [线性层](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear)、[ReLu
    激活函数](https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html#torch.nn.ReLU)
    和 [Dropout 层](https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html)。有关
    PyTorch 中所有预定义层的概述，请参阅 [文档](https://pytorch.org/docs/stable/nn.html)。
- en: We can build our own model by inheriting from the **nn.Module***.* A PyTorch
    model contains at least two methods. The **__init__** method, where all needed
    layers are instantiated, and the **forward** method, where the final model is
    defined. Here is an example model, that gives good enough results for our example
    data.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过继承**nn.Module**来构建自己的模型。一个 PyTorch 模型至少包含两个方法。**__init__** 方法，其中实例化了所有需要的层，和**forward**
    方法，其中定义了最终模型。以下是一个示例模型，能够为我们的示例数据提供足够好的结果。
- en: '[PRE3]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: This model consists of four linear layers. The number of input and output features
    are defined, which set the size of the input and output sample. Our data consists
    of 9 features, so the number of input features in the first layer is 9\. The output
    feature size can be changed but must fit the next input feature size. Since we
    finally want a 1-dimensional output (0 or 1) the last output feature size is equal
    to 1\. Note, that the final sigmoid-layer is not applied here. We will explain
    this in the next section.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型由四个线性层组成。输入和输出特征的数量已定义，这决定了输入和输出样本的大小。我们的数据包含 9 个特征，因此第一层的输入特征数量为 9。输出特征的大小可以更改，但必须与下一个输入特征的大小匹配。由于我们最终需要一个
    1 维的输出（0 或 1），因此最后的输出特征大小为 1。注意，这里没有应用最终的 sigmoid 层。我们将在下一节中解释这一点。
- en: Train the Model
  id: totrans-35
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练模型
- en: 'Next, we need to train the model. Training a model in PyTorch consists of four
    main steps:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要训练模型。在 PyTorch 中训练模型包括四个主要步骤：
- en: Apply the model
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 应用模型
- en: Calculate the loss
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算损失
- en: Backpropagation
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 反向传播
- en: Update the weights
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更新权重
- en: 'To train one epoch, these steps need to be done for all batches in the *train_dataloader*.
    Another loop then needs to go over the desired number of epochs. In pseudocode
    the training of one epoch looks as follows:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 要训练一个轮次，这些步骤需要在所有批次的*train_dataloader*上完成。然后需要另一个循环遍历所需的轮次数。伪代码中一个轮次的训练如下：
- en: '[PRE4]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The optimizer and the loss function still need to be defined. We will do this
    in the next section. Below is a function that includes this training loop. Additionally,
    some metrics (accuracy, recall, and precision) are calculated. Note, that we set
    the model to training mode (`model.train()`) in contrast to the evaluation mode
    (`model.eval()`). This affects dropout or batch normalization layers, which are
    treated differently for training and validation. The inputs of this function are
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 优化器和损失函数仍需定义。我们将在下一节中完成这部分。以下是包含这个训练循环的函数。此外，还计算了一些指标（准确率、召回率和精度）。注意，我们将模型设置为训练模式（`model.train()`），而不是评估模式（`model.eval()`）。这会影响
    dropout 或批归一化层，这些层在训练和验证时的处理方式不同。该函数的输入是
- en: 1\. The **model**. This is the above-defined model.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 1\. **模型**。这是上面定义的模型。
- en: 2\. The **device**. This can be either GPU or CPU and will be set in the next
    section.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 2\. **设备**。这可以是 GPU 或 CPU，将在下一节中设置。
- en: 3\. The **train_dataloader**. The above-defined dataloader for the training
    data.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 3\. **train_dataloader**。上述定义的用于训练数据的数据加载器。
- en: 4\. The **optimizer**. The optimizer is used to minimize the error. We will
    also specify this in the next section.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 4\. **优化器**。优化器用于最小化误差。我们将在下一节中具体说明。
- en: 5\. The **loss function (criterion)**. We will specify this in the next section.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 5\. **损失函数（标准）**。我们将在下一节中具体说明。
- en: 6\. The **epoch**. The current epoch.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 6\. **训练轮次**。当前的训练轮次。
- en: '[PRE5]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The validation of the model is similar, but without the backpropagation and
    without updating the weights. In pseudocode, the validation for one epoch looks
    like this.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 模型的验证类似，但没有反向传播，也没有更新权重。伪代码中一个轮次的验证如下。
- en: '[PRE6]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: A function for validating one epoch is given below. It is very similar to the
    previous function for training. Note, that the model is set to evaluation mode
    (`model.eval()`) and since no gradients need to be calculated during the validation
    we set `with torch.no_grad()`. This will reduce memory consumption for computations.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一个用于验证一个轮次的函数。它与前面用于训练的函数非常相似。注意，模型设置为评估模式（`model.eval()`），并且由于在验证过程中不需要计算梯度，我们设置了
    `with torch.no_grad()`。这将减少计算时的内存消耗。
- en: '[PRE7]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Putting it all Together
  id: totrans-55
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 综合起来
- en: 'To finally train a neural network using PyTorch, we need to do the following:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用 PyTorch 训练神经网络，我们需要做以下几步：
- en: '**Generate Datasets** from the data and wrap them into a **Dataloader**'
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从数据中**生成数据集**并将其包装成**数据加载器**
- en: We already did this in the previous sections, however, for the sake of completeness
    we will generate them again.
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们在前面的部分已经做过这些，然而为了完整性，我们将再次生成它们。
- en: 2\. **Define the model**
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 2\. **定义模型**
- en: We use our above-defined model `WaterNet.`
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用上述定义的模型`WaterNet`。
- en: 3\. **Define the optimizer**
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 3. **定义优化器**
- en: There are different [optimizers available in PyTorch](https://pytorch.org/docs/stable/optim.html).
    We use the Adam optimizer, which is a very common one. You can however try different
    ones. The Adam optimizer is an extension of the Stochastic Gradient Descent. Said
    in a simplified way the difference is that Stochastic Gradient Descent keeps the
    learning rate constant during training, while in Adam it is adapted. An introduction
    to the Adam optimizer can be found [here](https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/).
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在PyTorch中有不同的[优化器](https://pytorch.org/docs/stable/optim.html)。我们使用Adam优化器，这是一种非常常见的优化器。不过你也可以尝试不同的优化器。Adam优化器是随机梯度下降的扩展。简单来说，区别在于随机梯度下降在训练过程中保持学习率不变，而Adam则对其进行调整。关于Adam优化器的介绍可以在[这里](https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/)找到。
- en: 4\. **Define the loss function**
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 4. **定义损失函数**
- en: We are considering a binary classification problem with a 1-dimensional output,
    the default choice for this type of problem is the [binary cross entropy](/understanding-binary-cross-entropy-log-loss-a-visual-explanation-a3ac6025181a),
    which we will use.
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们正在考虑一个1维输出的二分类问题，这种问题的默认选择是[二元交叉熵](/understanding-binary-cross-entropy-log-loss-a-visual-explanation-a3ac6025181a)，我们将使用它。
- en: Note that we didn’t apply the final [sigmoid-layer](https://pytorch.org/docs/stable/generated/torch.nn.Sigmoid.html)
    in our model. This was done on purpose since PyTorch offers the `[nn.BCEWithLogitsLoss()](https://pytorch.org/docs/stable/generated/torch.nn.BCEWithLogitsLoss.html)`
    method, which combines the final sigmoid layer and the binary cross entropy. We
    could also apply these two methods separately, i.e. the `[nn.Sigmoid](https://pytorch.org/docs/stable/generated/torch.nn.Sigmoid.html)`
    layer as a final step of the model and then the `[nn.BCE()](https://pytorch.org/docs/stable/generated/torch.nn.BCELoss.html)`
    loss. However, using `nn.BCEWithLogitsLoss()` is the recommended way of dealing
    with binary classification problems, as it is numerically more stable.
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 请注意，我们没有在模型中应用最终的[sigmoid层](https://pytorch.org/docs/stable/generated/torch.nn.Sigmoid.html)。这是故意的，因为PyTorch提供了`[nn.BCEWithLogitsLoss()](https://pytorch.org/docs/stable/generated/torch.nn.BCEWithLogitsLoss.html)`方法，它将最终的sigmoid层和二元交叉熵结合起来。我们也可以单独应用这两种方法，即将`[nn.Sigmoid](https://pytorch.org/docs/stable/generated/torch.nn.Sigmoid.html)`层作为模型的最后一步，然后使用`[nn.BCE()](https://pytorch.org/docs/stable/generated/torch.nn.BCELoss.html)`损失。不过，使用`nn.BCEWithLogitsLoss()`是处理二分类问题的推荐方式，因为它在数值上更稳定。
- en: Before we start to train the model, we set the hyperparameters. Hyperparameters
    are adjustable parameters that let you control the model optimization process.
    Different hyperparameter values can impact model training and convergence rates.
    In our case, we have three hyperparameters, that we have to set. Note, that also
    the in- and output features of the model layers are also hyperparameters. We set
    them to fix values in the model, you can however try different values.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始训练模型之前，我们设置超参数。超参数是可调节的参数，允许你控制模型优化过程。不同的超参数值可以影响模型的训练和收敛速度。在我们的例子中，我们有三个超参数需要设置。请注意，模型层的输入和输出特征也是超参数。我们将它们设置为固定值，不过你可以尝试不同的值。
- en: '`batch_size`: Training and validation batch size'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`batch_size`：训练和验证批次大小'
- en: '`epochs`: Number of epochs to train'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`epochs`：训练的周期数'
- en: '`learning_rate`: The learning rate'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`learning_rate`：学习率'
- en: We also set the variable `print_every`. This is not a hyperparameter but just
    determines how often the loss is printed during training and validation. Note,
    that we further have to manually set the device to “cuda” if a GPU is available.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还设置了变量`print_every`。这不是一个超参数，而只是决定在训练和验证过程中多频繁打印损失。请注意，如果有GPU可用，我们还需要手动将设备设置为“cuda”。
- en: '[PRE8]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Now, we are ready for training. Below is the final training loop. Additionally,
    the calculated metrics are saved for each epoch.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们准备开始训练。下面是最终的训练循环。此外，计算得到的指标会为每个周期保存。
- en: '[PRE9]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Evaluate the Results
  id: totrans-74
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 评估结果
- en: To evaluate the results we can have a look at the loss and the metrics during
    training and evaluation.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 为了评估结果，我们可以查看训练和评估过程中的损失和指标。
- en: '![](../Images/ed6db4f285385f8aa7d567e2e4d7bc7a.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ed6db4f285385f8aa7d567e2e4d7bc7a.png)'
- en: The loss for training and validation for 150 epochs.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 训练和验证的损失为150个周期。
- en: '![](../Images/140d4c2d9d8d9d601fb2f02dde90be62.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/140d4c2d9d8d9d601fb2f02dde90be62.png)'
- en: The recall for training and validation for 150 epochs.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 训练和验证的回顾持续150个周期。
- en: You can find the plots for the other calculated metrics in the [notebook](https://colab.research.google.com/drive/1p31TH09BExMYyo-cm2DfcacoTdxONgwe).
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在[笔记本](https://colab.research.google.com/drive/1p31TH09BExMYyo-cm2DfcacoTdxONgwe)中找到其他计算指标的图表。
- en: Save the Model
  id: totrans-81
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 保存模型
- en: If we later want to use our model we need to save it. We can do that by saving
    it’s `state_dict()`.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们以后想使用我们的模型，我们需要保存它。我们可以通过保存它的`state_dict()`来做到这一点。
- en: '[PRE10]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: We can then load it with
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以用以下方式加载它
- en: '[PRE11]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Don’t forget to set the model to evaluation mode using `model.eval()` to put
    the dropout and batch normalization layers in evaluation mode.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 不要忘记使用`model.eval()`将模型设置为评估模式，以将丢弃和批量归一化层设置为评估模式。
- en: Apply the Model to the Test Set
  id: totrans-87
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将模型应用于测试集
- en: We now apply our trained model to the test data.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将训练好的模型应用于测试数据。
- en: '[PRE12]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Conclusion
  id: totrans-90
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: In this article, a detailed example of how to use PyTorch for a Deep Learning
    project was shown. The individual steps in the Deep Learning workflow were discussed
    and applied to a concrete dataset. An essential step before training the model
    is to bring the data into the correct form and define a customized dataset for
    the specific application. When training the model the four main steps are (1)
    apply the model, (2) calculate the loss, (3) perform backpropagation and, (4)
    update the weights. An example training function, where all these steps are performed
    is defined and applied. Finally, to use a model it is important to know how to
    store and reload it.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 本文展示了如何使用PyTorch进行深度学习项目的详细示例。讨论并应用了深度学习工作流中的各个步骤到一个具体的数据集。在训练模型之前，一个重要的步骤是将数据转换为正确的形式，并为特定应用定义一个定制的数据集。在训练模型时，四个主要步骤是（1）应用模型，（2）计算损失，（3）进行反向传播，（4）更新权重。定义并应用了一个包含所有这些步骤的示例训练函数。最后，要使用模型，了解如何存储和重新加载模型是很重要的。
- en: Resources
  id: totrans-92
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 资源
- en: '[1] Aditya Kadiwal, 2021, Water quality — Dataset for water quality classification,
    [https://www.kaggle.com/datasets/mssmartypants/water-quality](https://www.kaggle.com/datasets/mssmartypants/water-quality),
    downloaded January 2023, License: CC0: Public Domain'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] Aditya Kadiwal, 2021, 水质 — 水质分类数据集, [https://www.kaggle.com/datasets/mssmartypants/water-quality](https://www.kaggle.com/datasets/mssmartypants/water-quality),
    下载于2023年1月, 许可证：CC0: 公共领域'
- en: '[2] PyTorch, 2022, [https://pytorch.org/tutorials/beginner/basics/intro.html](https://pytorch.org/tutorials/beginner/basics/intro.html)'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] PyTorch, 2022, [https://pytorch.org/tutorials/beginner/basics/intro.html](https://pytorch.org/tutorials/beginner/basics/intro.html)'
- en: All images unless otherwise noted are by the author.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 除非另有说明，所有图片均为作者提供。
- en: 'Find more Data Science and Machine Learning posts here:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里查看更多数据科学和机器学习的帖子：
- en: '[## More'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '[## 更多'
- en: Data Science and Machine Learning Blog
  id: totrans-98
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据科学和机器学习博客
- en: datamapu.com](https://datamapu.com/?source=post_page-----e0235570a080--------------------------------)
    [](https://medium.com/@pumaline/subscribe?source=post_page-----e0235570a080--------------------------------)
    [## Get an email whenever Pumaline publishes.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '[datamapu.com](https://datamapu.com/?source=post_page-----e0235570a080--------------------------------)
    [medium.com](https://medium.com/@pumaline/subscribe?source=post_page-----e0235570a080--------------------------------)
    [## 每当Pumaline发布时获得电子邮件。'
- en: Get an email whenever Pumaline publishes. By signing up, you will create a Medium
    account if you don't already have…
  id: totrans-100
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 每当Pumaline发布时获得电子邮件。通过注册，如果你还没有Medium账户，将会创建一个…
- en: medium.com](https://medium.com/@pumaline/subscribe?source=post_page-----e0235570a080--------------------------------)
    [](https://www.buymeacoffee.com/pumaline?source=post_page-----e0235570a080--------------------------------)
    [## Pumaline
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '[medium.com](https://medium.com/@pumaline/subscribe?source=post_page-----e0235570a080--------------------------------)
    [www.buymeacoffee.com](https://www.buymeacoffee.com/pumaline?source=post_page-----e0235570a080--------------------------------)
    [## Pumaline'
- en: Hey, I like to learn and share knowledge about Data Science and Machine Learning.
  id: totrans-102
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 嗨，我喜欢学习和分享有关数据科学和机器学习的知识。
- en: www.buymeacoffee.com](https://www.buymeacoffee.com/pumaline?source=post_page-----e0235570a080--------------------------------)
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '[www.buymeacoffee.com](https://www.buymeacoffee.com/pumaline?source=post_page-----e0235570a080--------------------------------)'
