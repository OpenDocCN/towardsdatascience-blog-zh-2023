- en: Primer on Bayesian Deep Learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 贝叶斯深度学习入门
- en: 原文：[https://towardsdatascience.com/primer-on-bayesian-deep-learning-d06e0601c2ae](https://towardsdatascience.com/primer-on-bayesian-deep-learning-d06e0601c2ae)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/primer-on-bayesian-deep-learning-d06e0601c2ae](https://towardsdatascience.com/primer-on-bayesian-deep-learning-d06e0601c2ae)
- en: Probabilistic Deep Learning
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 概率深度学习
- en: '[](https://medium.com/@luisroque?source=post_page-----d06e0601c2ae--------------------------------)[![Luís
    Roque](../Images/e281d470b403375ba3c6f521b1ccf915.png)](https://medium.com/@luisroque?source=post_page-----d06e0601c2ae--------------------------------)[](https://towardsdatascience.com/?source=post_page-----d06e0601c2ae--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----d06e0601c2ae--------------------------------)
    [Luís Roque](https://medium.com/@luisroque?source=post_page-----d06e0601c2ae--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@luisroque?source=post_page-----d06e0601c2ae--------------------------------)[![Luís
    Roque](../Images/e281d470b403375ba3c6f521b1ccf915.png)](https://medium.com/@luisroque?source=post_page-----d06e0601c2ae--------------------------------)[](https://towardsdatascience.com/?source=post_page-----d06e0601c2ae--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----d06e0601c2ae--------------------------------)
    [Luís Roque](https://medium.com/@luisroque?source=post_page-----d06e0601c2ae--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----d06e0601c2ae--------------------------------)
    ·8 min read·Feb 1, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----d06e0601c2ae--------------------------------)
    ·阅读时间8分钟·2023年2月1日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: Introduction
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: This article belongs to the series “Probabilistic Deep Learning”. This weekly
    series covers probabilistic approaches to deep learning. The main goal is to extend
    deep learning models to quantify uncertainty, i.e., know what they do not know.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 本文属于“概率深度学习”系列。该系列每周涵盖概率方法在深度学习中的应用。主要目标是扩展深度学习模型以量化不确定性，即了解它们不知道什么。
- en: Bayesian Deep Learning is an emerging field that combines the expressiveness
    and representational power of deep learning with the uncertainty modeling capabilities
    of Bayesian methods. The integration of these two paradigms offers a principled
    framework for addressing various challenges in deep learning, such as overfitting,
    weight uncertainty, and model comparison.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 贝叶斯深度学习是一个新兴领域，它将深度学习的表达能力和表征能力与贝叶斯方法的不确定性建模能力相结合。这两种范式的整合提供了一个原则性框架，用于解决深度学习中的各种挑战，如过拟合、权重不确定性和模型比较。
- en: In this article, we provide a comprehensive introduction to Bayesian Deep Learning,
    covering its foundations, methodology, and recent advances. Our aim is to present
    the fundamental concepts and ideas in a clear and accessible manner, making it
    an ideal resource for researchers and practitioners who are new to the field.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们提供了对贝叶斯深度学习的全面介绍，涵盖其基础、方法论和最新进展。我们的目标是以清晰易懂的方式展示基本概念和思想，使其成为研究人员和实践者的理想资源，特别是那些对该领域较新的人员。
- en: 'Articles published so far:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 迄今为止已发布的文章：
- en: '[Gentle Introduction to TensorFlow Probability: Distribution Objects](https://medium.com/towards-data-science/gentle-introduction-to-tensorflow-probability-distribution-objects-1bb6165abee1)'
  id: totrans-11
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[TensorFlow 概率的温和介绍：分布对象](https://medium.com/towards-data-science/gentle-introduction-to-tensorflow-probability-distribution-objects-1bb6165abee1)'
- en: '[Gentle Introduction to TensorFlow Probability: Trainable Parameters](https://medium.com/towards-data-science/gentle-introduction-to-tensorflow-probability-trainable-parameters-5098ea4fed15)'
  id: totrans-12
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[TensorFlow 概率的温和介绍：可训练参数](https://medium.com/towards-data-science/gentle-introduction-to-tensorflow-probability-trainable-parameters-5098ea4fed15)'
- en: '[Maximum Likelihood Estimation from scratch in TensorFlow Probability](/maximum-likelihood-estimation-from-scratch-in-tensorflow-probability-2fc0eefdbfc2)'
  id: totrans-13
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[从头开始在 TensorFlow Probability 中进行最大似然估计](/maximum-likelihood-estimation-from-scratch-in-tensorflow-probability-2fc0eefdbfc2)'
- en: '[Probabilistic Linear Regression from scratch in TensorFlow](/probabilistic-linear-regression-from-scratch-in-tensorflow-2eb633fffc00)'
  id: totrans-14
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[从头开始在 TensorFlow 中进行概率线性回归](/probabilistic-linear-regression-from-scratch-in-tensorflow-2eb633fffc00)'
- en: '[Probabilistic vs. Deterministic Regression with Tensorflow](https://medium.com/towards-data-science/probabilistic-vs-deterministic-regression-with-tensorflow-85ef791beeef)'
  id: totrans-15
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[Tensorflow 中的概率回归与确定性回归](https://medium.com/towards-data-science/probabilistic-vs-deterministic-regression-with-tensorflow-85ef791beeef)'
- en: '[Frequentist vs. Bayesian Statistics with Tensorflow](https://medium.com/towards-data-science/frequentist-vs-bayesian-statistics-with-tensorflow-fbba2c6c9ae5)'
  id: totrans-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[使用 Tensorflow 的频率主义与贝叶斯统计](https://medium.com/towards-data-science/frequentist-vs-bayesian-statistics-with-tensorflow-fbba2c6c9ae5)'
- en: '[Deterministic vs. Probabilistic Deep Learning](/deterministic-vs-probabilistic-deep-learning-5325769dc758)'
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[确定性与概率深度学习](/deterministic-vs-probabilistic-deep-learning-5325769dc758)'
- en: '[Naive Bayes from scratch with TensorFlow](/naive-bayes-from-scratch-with-tensorflow-6e04c5a25947)'
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[从头开始使用 TensorFlow 实现朴素贝叶斯](/naive-bayes-from-scratch-with-tensorflow-6e04c5a25947)'
- en: '[Probabilistic Logistic Regression with TensorFlow](https://medium.com/towards-data-science/probabilistic-logistic-regression-with-tensorflow-73e18f0ddc48)'
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[使用 TensorFlow 的概率逻辑回归](https://medium.com/towards-data-science/probabilistic-logistic-regression-with-tensorflow-73e18f0ddc48)'
- en: Bayesian Deep Learning
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 贝叶斯深度学习
- en: '![](../Images/12127b517c9ad0820110791614e60d31.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/12127b517c9ad0820110791614e60d31.png)'
- en: 'Figure 1: Motto for today: it is layers all the way down ([source](https://unsplash.com/photos/1fzR7q6GB6A))'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：今日格言：层层深入 ([source](https://unsplash.com/photos/1fzR7q6GB6A))
- en: Deterministic, Probabilistic and Bayesian Deep Learning
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 确定性、概率性和贝叶斯深度学习
- en: Deep learning has achieved remarkable success in a wide range of applications,
    including computer vision, natural language processing, and game playing. Despite
    its successes, traditional deep learning models are inherently deterministic and
    provide limited ability to quantify uncertainty in their predictions. To address
    this issue, Bayesian deep learning and probabilistic deep learning have emerged
    as important paradigms that allow for incorporating uncertainty into deep learning
    models.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习在计算机视觉、自然语言处理和游戏等多个应用领域取得了显著成功。尽管如此，传统的深度学习模型本质上是确定性的，并且在其预测中提供了有限的不确定性量化能力。为了解决这个问题，贝叶斯深度学习和概率深度学习作为重要范式出现，允许将不确定性融入深度学习模型中。
- en: Bayesian deep learning and probabilistic deep learning represent important paradigms
    for incorporating uncertainty into deep learning models. These approaches offer
    several advantages over traditional deterministic deep learning, including the
    ability to provide uncertainty estimates and the ability to perform robust inference
    in the presence of out-of-distribution data.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 贝叶斯深度学习和概率深度学习代表了将不确定性融入深度学习模型的重要范式。这些方法相比传统的确定性深度学习具有若干优势，包括能够提供不确定性估计以及在存在数据分布外数据时进行稳健推理的能力。
- en: In Bayesian deep learning, the model parameters are treated as random variables
    and a prior distribution is placed over them. This prior represents prior knowledge
    about the model parameters, such as their expected values or their distributional
    shape. The posterior distribution over the parameters is then updated through
    Bayesian inference, using the data to form a posterior that represents our updated
    beliefs about the parameters given the data. This results in a distribution over
    the model parameters and a measure of uncertainty in the model predictions.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在贝叶斯深度学习中，模型参数被视为随机变量，并对其施加先验分布。这个先验代表了关于模型参数的先验知识，例如它们的期望值或分布形状。然后，通过贝叶斯推理更新参数的后验分布，使用数据形成一个后验分布，这代表了我们在数据给定下对参数的更新信念。这将导致模型参数的分布以及模型预测中的不确定性度量。
- en: Probabilistic deep learning, on the other hand, models the data-generating process
    as a probabilistic function. Given an input, the model predicts a distribution
    over outputs, allowing for the quantification of uncertainty in predictions. This
    approach is particularly useful for problems where the output space is complex,
    such as image generation or speech synthesis. In these cases, modeling the data-generating
    process as a probabilistic function allows for the capture of complex patterns
    in the data and the generation of high-quality outputs.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 概率深度学习则将数据生成过程建模为概率函数。给定一个输入，模型预测一个输出分布，从而允许量化预测中的不确定性。这种方法在输出空间复杂的问题中尤为有用，例如图像生成或语音合成。在这些情况下，将数据生成过程建模为概率函数可以捕捉数据中的复杂模式，并生成高质量的输出。
- en: Both Bayesian deep learning and probabilistic deep learning are important and
    active areas of research, with recent advances in techniques such as Bayesian
    neural networks, variational inference, and deep generative models. Despite their
    potential benefits, these approaches remain challenging due to the computational
    and statistical difficulties posed by high-dimensional models and the large amounts
    of data typically used in deep learning. Nevertheless, significant progress has
    been made in this field in recent years, and there is much excitement and potential
    for future research in Bayesian deep learning and probabilistic deep learning.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 贝叶斯深度学习和概率深度学习都是重要且活跃的研究领域，最近在贝叶斯神经网络、变分推断和深度生成模型等技术方面取得了进展。尽管这些方法具有潜在的好处，但由于高维模型和深度学习中通常使用的大量数据所带来的计算和统计困难，这些方法仍然具有挑战性。尽管如此，近年来该领域已经取得了显著进展，贝叶斯深度学习和概率深度学习的未来研究充满了兴奋和潜力。
- en: 'Bayesian Learning: A Primer'
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 贝叶斯学习：概述
- en: In this article, we consider the Bayesian framework for statistical inference,
    where we represent probability density with the notation 𝑃.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我们考虑了用于统计推断的贝叶斯框架，其中我们用符号𝑃表示概率密度。
- en: 'Bayesian methods provide a unified approach for conducting statistical inference,
    where we aim to compute the distribution of model parameters given some observed
    data. In the context of neural networks, this is particularly useful for estimating
    the uncertainty in weight parameters, as we can calculate the posterior distribution
    of these parameters given the training data. This is achieved by applying Bayes’
    theorem, which states:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 贝叶斯方法提供了一种统一的统计推断方法，我们的目标是计算给定一些观察数据的模型参数分布。在神经网络的背景下，这对于估计权重参数的不确定性尤为有用，因为我们可以计算给定训练数据的这些参数的后验分布。这是通过应用贝叶斯定理实现的，该定理指出：
- en: '![](../Images/1e00cd974ec578996564be058a73c5a5.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1e00cd974ec578996564be058a73c5a5.png)'
- en: 'where:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 其中：
- en: '𝐷: The observed data, represented as pairs of 𝑥 and 𝑦 values, e.g. 𝐷={(𝑥1,𝑦1),…,(𝑥𝑛,𝑦𝑛)}'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 𝐷：观察数据，以𝑥和𝑦值对表示，例如 𝐷={(𝑥1,𝑦1),…,(𝑥𝑛,𝑦𝑛)}
- en: '𝑤: The value of a model weight'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 𝑤：模型权重的值
- en: '𝑃(𝑤): The prior density, representing our initial belief on the distribution
    of model weights before seeing the data'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 𝑃(𝑤)：先验密度，表示在观察到数据之前对模型权重分布的初始信念
- en: '𝑃(𝐷|𝑤): The likelihood of observing the data given the weight 𝑤'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 𝑃(𝐷|𝑤)：给定权重𝑤时观察到数据的似然
- en: '𝑃(𝑤|𝐷): The posterior density of the model weight, calculated by incorporating
    the observed data and prior belief through Bayes’ theorem.'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 𝑃(𝑤|𝐷)：模型权重的后验密度，通过贝叶斯定理将观察到的数据和先验信念结合计算得出。
- en: The normalization term ∫𝑃(𝐷|𝑤′)𝑃(𝑤′)d𝑤′=𝑃(𝐷) is independent of 𝑤 and serves
    to normalize the posterior density.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 归一化项 ∫𝑃(𝐷|𝑤′)𝑃(𝑤′)d𝑤′=𝑃(𝐷) 是与𝑤无关的，用于归一化后验密度。
- en: Bayes’ theorem enables us to synthesize the observed data with our prior belief
    to obtain the posterior distribution of model parameters, providing a probabilistic
    representation of their uncertainty. The implementation of Bayesian learning in
    the context of neural networks can be challenging due to the complexity of calculating
    the normalization constant. To overcome this, various approximate methods such
    as Variational Bayes are utilized. These methods aim to efficiently estimate the
    posterior distribution of the NN weights.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 贝叶斯定理使我们能够将观察到的数据与我们的先验信念结合，以获得模型参数的后验分布，提供它们不确定性的概率表示。在神经网络背景下实施贝叶斯学习可能很具挑战性，因为计算归一化常数的复杂性。为了克服这一点，使用了各种近似方法，例如变分贝叶斯。这些方法旨在有效地估计神经网络权重的后验分布。
- en: Variational Bayesian Inference
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 变分贝叶斯推断
- en: In Variational Bayes, we approximate the posterior distribution with a second
    function, known as the variational posterior. This function is parameterized by
    some set of parameters, denoted by 𝜃, which can be learned through optimization.
    The key idea is to choose a functional form for the variational posterior such
    that it can be effectively optimized to approximate the true posterior as closely
    as possible.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在变分贝叶斯方法中，我们使用第二个函数来近似后验分布，这个函数称为变分后验。这个函数由一组参数来表征，记作𝜃，可以通过优化来学习。关键思想是选择一个变分后验的函数形式，使其能够有效地优化，以尽可能接近真实的后验分布。
- en: One common approach to measuring the quality of the approximation is through
    the use of the Kullback-Leibler (KL) divergence between the variational posterior
    and the true posterior. The KL divergence is a measure of the difference between
    two probability distributions, and its value is 0 when the two distributions are
    equal.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 一种常见的评估近似质量的方法是使用变分后验和真实后验之间的 Kullback-Leibler (KL) 散度。KL 散度是衡量两个概率分布之间差异的指标，当两个分布相等时，其值为0。
- en: Given the observed data, 𝐷, the KL divergence between the variational posterior,
    𝑞(𝑤|𝜃), and the true posterior, 𝑃(𝑤|𝐷), is defined as
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 给定观察数据 𝐷，变分后验 𝑞(𝑤|𝜃) 和真实后验 𝑃(𝑤|𝐷) 之间的 KL 散度定义为
- en: '![](../Images/c6241bd40be3a89f3550ea48e775964a.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c6241bd40be3a89f3550ea48e775964a.png)'
- en: 'When considering the observed data as constant, we can simplify this expression
    to a function that depends only on 𝜃 and 𝐷:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 当将观察数据视为常量时，我们可以将这个表达式简化为仅依赖于 𝜃 和 𝐷 的函数：
- en: '![](../Images/0ebf632c807e607833aee3ca46b04c29.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0ebf632c807e607833aee3ca46b04c29.png)'
- en: where the first term measures the discrepancy between the variational and prior
    distributions, and the second term is the expected negative log likelihood under
    the variational posterior.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 第一项测量变分分布和先验分布之间的差异，第二项是变分后验下的期望负对数似然。
- en: This function, known as the variational lower bound, can then be optimized with
    respect to 𝜃 to obtain the best possible approximation of the true posterior.
    The optimization can be performed using gradient-based methods, making it possible
    to scale Variational Bayes to large, complex models.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 这个函数，称为变分下界，可以针对 𝜃 进行优化，以获得对真实后验的最佳近似。优化可以使用基于梯度的方法进行，使变分贝叶斯能够扩展到大型复杂模型。
- en: The Backpropagation Scheme for Bayesian Neural Networks
  id: totrans-50
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 贝叶斯神经网络的反向传播方案
- en: In this section, we describe a backpropagation algorithm for Bayesian neural
    networks, which allow us to incorporate weight uncertainty into our models. Our
    approach involves introducing a variational posterior with density 𝑞(𝑤|𝜃), where
    𝑤 is a weight in the network and 𝜃 is a set of trainable parameters. This posterior
    represents our approximation of the true weight posterior, which is determined
    by the observed data and our prior beliefs about the weights encoded by the prior
    density 𝑃(𝑤).
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们描述了一种用于贝叶斯神经网络的反向传播算法，允许我们将权重不确定性纳入模型。我们的方法涉及引入一个密度为 𝑞(𝑤|𝜃) 的变分后验，其中
    𝑤 是网络中的一个权重，𝜃 是一组可训练的参数。这个后验表示了我们对真实权重后验的近似，真实权重后验由观察数据和我们对权重的先验信念（通过先验密度 𝑃(𝑤)
    编码）决定。
- en: 'Our goal is to update the variational posterior so that it accurately approximates
    the true weight posterior, which we quantify by the Evidence Lower Bound (ELBO).
    To do so, we minimize the ELBO between the variational posterior and the prior,
    which can be written as:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的目标是更新变分后验，使其准确地逼近真实的权重后验，这一目标通过证据下界（ELBO）来量化。为此，我们最小化变分后验和先验之间的ELBO，公式如下：
- en: '![](../Images/76fb86a007c33e8767c16d006e8a9d5f.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/76fb86a007c33e8767c16d006e8a9d5f.png)'
- en: 'where 𝐷 is the training data. This expression, however, involves an integral
    over the weight 𝑤 which may be computationally intractable. To circumvent this
    difficulty, we re-write the ELBO as an expectation:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 𝐷 是训练数据。然而，这个表达式涉及对权重 𝑤 的积分，这可能在计算上不可行。为了解决这个难题，我们将ELBO重写为期望：
- en: '![](../Images/95494b29915faa629278884b262d0660.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/95494b29915faa629278884b262d0660.png)'
- en: In order to perform backpropagation, we need to take derivatives of *𝐿*(*𝜃*|*𝐷*)
    with respect to *𝜃*. However, this is challenging because the underlying distribution
    with respect to which the expectation is taken depends on *𝜃*. We overcome this
    issue through the reparameterization trick, which allows us to differentiate through
    the expectation.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进行反向传播，我们需要对 *𝐿*(*𝜃*|*𝐷*) 关于 *𝜃* 进行求导。然而，这很具有挑战性，因为期望的底层分布依赖于 *𝜃*。我们通过重参数化技巧克服了这个问题，这使得我们能够通过期望进行求导。
- en: 'The Reparameterization Trick: A Key Technique for Stochastic Gradient Optimization
    in Bayesian Neural Networks'
  id: totrans-57
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 重参数化技巧：贝叶斯神经网络中随机梯度优化的关键技术
- en: The reparameterization trick is a mathematical technique used to enable the
    optimization of models with stochastic parameters. The goal is to convert the
    dependent variable, 𝑤, into a random variable, 𝜖, with a fixed distribution independent
    of model parameters, 𝜃. This conversion allows the expectation of 𝑓(𝑤;𝜇,𝜎) with
    respect to 𝑞(𝑤|𝜇,𝜎) to be calculated independently of 𝜃, enabling efficient optimization
    of the model.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 重新参数化技巧是一种数学技术，用于优化具有随机参数的模型。其目标是将依赖变量𝑤转换为具有固定分布且与模型参数𝜃无关的随机变量𝜖。这种转换使得可以独立于𝜃计算𝑓(𝑤;𝜇,𝜎)对𝑞(𝑤|𝜇,𝜎)的期望，从而实现模型的高效优化。
- en: As an example, consider a Gaussian distribution 𝑞(𝑤|𝜇,𝜎) where 𝜃=(𝜇,𝜎). Using
    a change of variables 𝑤=𝜇+𝜎𝜖, where 𝜖∼𝑁(0,1), we obtain
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，考虑一个高斯分布𝑞(𝑤|𝜇,𝜎)，其中𝜃=(𝜇,𝜎)。通过变量变换𝑤=𝜇+𝜎𝜖，其中𝜖∼𝑁(0,1)，我们得到
- en: '![](../Images/e57ba17221fe7b4704dc142ae6f77473.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e57ba17221fe7b4704dc142ae6f77473.png)'
- en: The derivatives with respect to 𝜇 and 𝜎 can then be easily calculated, and the
    expectations can be estimated through Monte Carlo sampling.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 然后可以轻松计算关于𝜇和𝜎的导数，并通过蒙特卡罗采样来估计期望。
- en: The reparameterization trick is a crucial tool for efficient optimization in
    Bayesian Neural Networks and has been widely adopted in modern deep learning research.
    It enables the gradient-based optimization of models with stochastic parameters,
    allowing for efficient training and inference.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 重新参数化技巧是贝叶斯神经网络中高效优化的关键工具，并且在现代深度学习研究中得到了广泛应用。它使得可以对具有随机参数的模型进行基于梯度的优化，从而实现高效的训练和推断。
- en: The Final Connection
  id: totrans-63
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 最终连接
- en: Bayesian deep learning offers a framework for incorporating uncertainty into
    deep learning models. By treating neural network weights as random variables,
    we can capture both aleatoric and epistemic uncertainty, allowing for more robust
    and reliable predictions. Variational Bayesian inference provides a way to learn
    the parameters of the posterior distribution over network weights, which can be
    optimized using the backpropagation scheme.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 贝叶斯深度学习提供了一种将不确定性融入深度学习模型的框架。通过将神经网络权重视为随机变量，我们可以捕捉到内在不确定性和认识不确定性，从而提供更稳健和可靠的预测。变分贝叶斯推断提供了一种学习网络权重后验分布参数的方法，这些参数可以通过反向传播方案进行优化。
- en: The reparameterization trick, a key technique for stochastic gradient optimization
    in Bayesian neural networks, allows us to estimate gradients efficiently by transforming
    random variables into differentiable forms. This trick enables the use of standard
    gradient-based optimization algorithms to learn the parameters of the posterior
    distribution over network weights.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 重新参数化技巧（reparameterization trick），这是贝叶斯神经网络中用于随机梯度优化的关键技术，使我们能够通过将随机变量转换为可微分的形式来有效地估计梯度。这一技巧使得可以使用标准的基于梯度的优化算法来学习网络权重后验分布的参数。
- en: It is important to note that while aleatoric uncertainty accounts for the inherent
    noise in the data, epistemic uncertainty captures the uncertainty in our knowledge
    of the underlying model. In Bayesian deep learning, epistemic uncertainty is modeled
    by placing a prior distribution over network weights and updating it with observed
    data through the variational inference process. This results in a more flexible
    and informative model, as the network can adapt to new data and incorporate additional
    information about the underlying relationships.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的是，尽管内在不确定性（aleatoric uncertainty）考虑了数据中的固有噪声，但认识不确定性（epistemic uncertainty）则捕捉了我们对基础模型知识的不了解。在贝叶斯深度学习中，认识不确定性通过对网络权重施加先验分布并通过变分推断过程用观察数据更新它来建模。这导致了一个更灵活和信息量更大的模型，因为网络可以适应新数据并融入有关基础关系的额外信息。
- en: Conclusion
  id: totrans-67
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: In conclusion, Bayesian Deep Learning provides a framework for incorporating
    uncertainty into the predictions of Deep Neural Networks. By treating the network
    weights as random variables and modeling their posterior distributions, we can
    obtain more robust and informative models compared to traditional deterministic
    deep learning approaches.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，贝叶斯深度学习提供了一种将不确定性融入深度神经网络预测的框架。通过将网络权重视为随机变量并建模其后验分布，我们可以获得比传统确定性深度学习方法更稳健和信息量更大的模型。
- en: The reparameterization trick, combined with gradient-based optimization algorithms,
    enables scalable Bayesian inference in deep models. As a result, Bayesian Deep
    Learning has gained increasing attention and found success in various applications
    including active learning, out-of-distribution detection, and uncertainty-aware
    reinforcement learning. The field is still relatively new, and there is much room
    for further exploration and innovation.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 重参数化技巧与基于梯度的优化算法相结合，使得深度模型中的可扩展贝叶斯推断成为可能。因此，贝叶斯深度学习受到越来越多的关注，并在包括主动学习、分布外检测和不确定性感知强化学习等各种应用中取得了成功。这个领域仍然相对较新，仍有很大的探索和创新空间。
- en: In the future, we expect Bayesian Deep Learning to play an increasingly important
    role in the advancement of artificial intelligence and machine learning.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在未来，我们期望贝叶斯深度学习在人工智能和机器学习的发展中扮演越来越重要的角色。
- en: About me
  id: totrans-71
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 关于我
- en: Serial entrepreneur and leader in the AI space. I develop AI products for businesses
    and invest in AI-focused startups.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 连续创业者和AI领域的领导者。我为企业开发AI产品，并投资于专注于AI的初创公司。
- en: '[Founder @ ZAAI](http://zaai.ai) | [LinkedIn](https://www.linkedin.com/in/luisbrasroque/)
    | [X/Twitter](https://x.com/luisbrasroque)'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '[创始人 @ ZAAI](http://zaai.ai) | [LinkedIn](https://www.linkedin.com/in/luisbrasroque/)
    | [X/Twitter](https://x.com/luisbrasroque)'
- en: References and Materials
  id: totrans-74
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考资料
- en: '[1] — [Coursera: Deep Learning Specialization](https://www.coursera.org/specializations/deep-learning)'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] — [Coursera: Deep Learning Specialization](https://www.coursera.org/specializations/deep-learning)'
- en: '[2] — [Coursera: TensorFlow 2 for Deep Learning](https://www.coursera.org/specializations/tensorflow2-deeplearning)
    Specialization'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] — [Coursera: TensorFlow 2 for Deep Learning](https://www.coursera.org/specializations/tensorflow2-deeplearning)
    专业化课程'
