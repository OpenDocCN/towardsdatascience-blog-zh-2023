- en: Primer on Bayesian Deep Learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: è´å¶æ–¯æ·±åº¦å­¦ä¹ å…¥é—¨
- en: åŸæ–‡ï¼š[https://towardsdatascience.com/primer-on-bayesian-deep-learning-d06e0601c2ae](https://towardsdatascience.com/primer-on-bayesian-deep-learning-d06e0601c2ae)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡ï¼š[https://towardsdatascience.com/primer-on-bayesian-deep-learning-d06e0601c2ae](https://towardsdatascience.com/primer-on-bayesian-deep-learning-d06e0601c2ae)
- en: Probabilistic Deep Learning
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ¦‚ç‡æ·±åº¦å­¦ä¹ 
- en: '[](https://medium.com/@luisroque?source=post_page-----d06e0601c2ae--------------------------------)[![LuÃ­s
    Roque](../Images/e281d470b403375ba3c6f521b1ccf915.png)](https://medium.com/@luisroque?source=post_page-----d06e0601c2ae--------------------------------)[](https://towardsdatascience.com/?source=post_page-----d06e0601c2ae--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----d06e0601c2ae--------------------------------)
    [LuÃ­s Roque](https://medium.com/@luisroque?source=post_page-----d06e0601c2ae--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@luisroque?source=post_page-----d06e0601c2ae--------------------------------)[![LuÃ­s
    Roque](../Images/e281d470b403375ba3c6f521b1ccf915.png)](https://medium.com/@luisroque?source=post_page-----d06e0601c2ae--------------------------------)[](https://towardsdatascience.com/?source=post_page-----d06e0601c2ae--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----d06e0601c2ae--------------------------------)
    [LuÃ­s Roque](https://medium.com/@luisroque?source=post_page-----d06e0601c2ae--------------------------------)'
- en: Â·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----d06e0601c2ae--------------------------------)
    Â·8 min readÂ·Feb 1, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: Â·å‘è¡¨äº [Towards Data Science](https://towardsdatascience.com/?source=post_page-----d06e0601c2ae--------------------------------)
    Â·é˜…è¯»æ—¶é—´8åˆ†é’ŸÂ·2023å¹´2æœˆ1æ—¥
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: Introduction
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ä»‹ç»
- en: This article belongs to the series â€œProbabilistic Deep Learningâ€. This weekly
    series covers probabilistic approaches to deep learning. The main goal is to extend
    deep learning models to quantify uncertainty, i.e., know what they do not know.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬æ–‡å±äºâ€œæ¦‚ç‡æ·±åº¦å­¦ä¹ â€ç³»åˆ—ã€‚è¯¥ç³»åˆ—æ¯å‘¨æ¶µç›–æ¦‚ç‡æ–¹æ³•åœ¨æ·±åº¦å­¦ä¹ ä¸­çš„åº”ç”¨ã€‚ä¸»è¦ç›®æ ‡æ˜¯æ‰©å±•æ·±åº¦å­¦ä¹ æ¨¡å‹ä»¥é‡åŒ–ä¸ç¡®å®šæ€§ï¼Œå³äº†è§£å®ƒä»¬ä¸çŸ¥é“ä»€ä¹ˆã€‚
- en: Bayesian Deep Learning is an emerging field that combines the expressiveness
    and representational power of deep learning with the uncertainty modeling capabilities
    of Bayesian methods. The integration of these two paradigms offers a principled
    framework for addressing various challenges in deep learning, such as overfitting,
    weight uncertainty, and model comparison.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: è´å¶æ–¯æ·±åº¦å­¦ä¹ æ˜¯ä¸€ä¸ªæ–°å…´é¢†åŸŸï¼Œå®ƒå°†æ·±åº¦å­¦ä¹ çš„è¡¨è¾¾èƒ½åŠ›å’Œè¡¨å¾èƒ½åŠ›ä¸è´å¶æ–¯æ–¹æ³•çš„ä¸ç¡®å®šæ€§å»ºæ¨¡èƒ½åŠ›ç›¸ç»“åˆã€‚è¿™ä¸¤ç§èŒƒå¼çš„æ•´åˆæä¾›äº†ä¸€ä¸ªåŸåˆ™æ€§æ¡†æ¶ï¼Œç”¨äºè§£å†³æ·±åº¦å­¦ä¹ ä¸­çš„å„ç§æŒ‘æˆ˜ï¼Œå¦‚è¿‡æ‹Ÿåˆã€æƒé‡ä¸ç¡®å®šæ€§å’Œæ¨¡å‹æ¯”è¾ƒã€‚
- en: In this article, we provide a comprehensive introduction to Bayesian Deep Learning,
    covering its foundations, methodology, and recent advances. Our aim is to present
    the fundamental concepts and ideas in a clear and accessible manner, making it
    an ideal resource for researchers and practitioners who are new to the field.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æä¾›äº†å¯¹è´å¶æ–¯æ·±åº¦å­¦ä¹ çš„å…¨é¢ä»‹ç»ï¼Œæ¶µç›–å…¶åŸºç¡€ã€æ–¹æ³•è®ºå’Œæœ€æ–°è¿›å±•ã€‚æˆ‘ä»¬çš„ç›®æ ‡æ˜¯ä»¥æ¸…æ™°æ˜“æ‡‚çš„æ–¹å¼å±•ç¤ºåŸºæœ¬æ¦‚å¿µå’Œæ€æƒ³ï¼Œä½¿å…¶æˆä¸ºç ”ç©¶äººå‘˜å’Œå®è·µè€…çš„ç†æƒ³èµ„æºï¼Œç‰¹åˆ«æ˜¯é‚£äº›å¯¹è¯¥é¢†åŸŸè¾ƒæ–°çš„äººå‘˜ã€‚
- en: 'Articles published so far:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: è¿„ä»Šä¸ºæ­¢å·²å‘å¸ƒçš„æ–‡ç« ï¼š
- en: '[Gentle Introduction to TensorFlow Probability: Distribution Objects](https://medium.com/towards-data-science/gentle-introduction-to-tensorflow-probability-distribution-objects-1bb6165abee1)'
  id: totrans-11
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[TensorFlow æ¦‚ç‡çš„æ¸©å’Œä»‹ç»ï¼šåˆ†å¸ƒå¯¹è±¡](https://medium.com/towards-data-science/gentle-introduction-to-tensorflow-probability-distribution-objects-1bb6165abee1)'
- en: '[Gentle Introduction to TensorFlow Probability: Trainable Parameters](https://medium.com/towards-data-science/gentle-introduction-to-tensorflow-probability-trainable-parameters-5098ea4fed15)'
  id: totrans-12
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[TensorFlow æ¦‚ç‡çš„æ¸©å’Œä»‹ç»ï¼šå¯è®­ç»ƒå‚æ•°](https://medium.com/towards-data-science/gentle-introduction-to-tensorflow-probability-trainable-parameters-5098ea4fed15)'
- en: '[Maximum Likelihood Estimation from scratch in TensorFlow Probability](/maximum-likelihood-estimation-from-scratch-in-tensorflow-probability-2fc0eefdbfc2)'
  id: totrans-13
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[ä»å¤´å¼€å§‹åœ¨ TensorFlow Probability ä¸­è¿›è¡Œæœ€å¤§ä¼¼ç„¶ä¼°è®¡](/maximum-likelihood-estimation-from-scratch-in-tensorflow-probability-2fc0eefdbfc2)'
- en: '[Probabilistic Linear Regression from scratch in TensorFlow](/probabilistic-linear-regression-from-scratch-in-tensorflow-2eb633fffc00)'
  id: totrans-14
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[ä»å¤´å¼€å§‹åœ¨ TensorFlow ä¸­è¿›è¡Œæ¦‚ç‡çº¿æ€§å›å½’](/probabilistic-linear-regression-from-scratch-in-tensorflow-2eb633fffc00)'
- en: '[Probabilistic vs. Deterministic Regression with Tensorflow](https://medium.com/towards-data-science/probabilistic-vs-deterministic-regression-with-tensorflow-85ef791beeef)'
  id: totrans-15
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[Tensorflow ä¸­çš„æ¦‚ç‡å›å½’ä¸ç¡®å®šæ€§å›å½’](https://medium.com/towards-data-science/probabilistic-vs-deterministic-regression-with-tensorflow-85ef791beeef)'
- en: '[Frequentist vs. Bayesian Statistics with Tensorflow](https://medium.com/towards-data-science/frequentist-vs-bayesian-statistics-with-tensorflow-fbba2c6c9ae5)'
  id: totrans-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[ä½¿ç”¨ Tensorflow çš„é¢‘ç‡ä¸»ä¹‰ä¸è´å¶æ–¯ç»Ÿè®¡](https://medium.com/towards-data-science/frequentist-vs-bayesian-statistics-with-tensorflow-fbba2c6c9ae5)'
- en: '[Deterministic vs. Probabilistic Deep Learning](/deterministic-vs-probabilistic-deep-learning-5325769dc758)'
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[ç¡®å®šæ€§ä¸æ¦‚ç‡æ·±åº¦å­¦ä¹ ](/deterministic-vs-probabilistic-deep-learning-5325769dc758)'
- en: '[Naive Bayes from scratch with TensorFlow](/naive-bayes-from-scratch-with-tensorflow-6e04c5a25947)'
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[ä»å¤´å¼€å§‹ä½¿ç”¨ TensorFlow å®ç°æœ´ç´ è´å¶æ–¯](/naive-bayes-from-scratch-with-tensorflow-6e04c5a25947)'
- en: '[Probabilistic Logistic Regression with TensorFlow](https://medium.com/towards-data-science/probabilistic-logistic-regression-with-tensorflow-73e18f0ddc48)'
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[ä½¿ç”¨ TensorFlow çš„æ¦‚ç‡é€»è¾‘å›å½’](https://medium.com/towards-data-science/probabilistic-logistic-regression-with-tensorflow-73e18f0ddc48)'
- en: Bayesian Deep Learning
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: è´å¶æ–¯æ·±åº¦å­¦ä¹ 
- en: '![](../Images/12127b517c9ad0820110791614e60d31.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/12127b517c9ad0820110791614e60d31.png)'
- en: 'Figure 1: Motto for today: it is layers all the way down ([source](https://unsplash.com/photos/1fzR7q6GB6A))'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 1ï¼šä»Šæ—¥æ ¼è¨€ï¼šå±‚å±‚æ·±å…¥ ([source](https://unsplash.com/photos/1fzR7q6GB6A))
- en: Deterministic, Probabilistic and Bayesian Deep Learning
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç¡®å®šæ€§ã€æ¦‚ç‡æ€§å’Œè´å¶æ–¯æ·±åº¦å­¦ä¹ 
- en: Deep learning has achieved remarkable success in a wide range of applications,
    including computer vision, natural language processing, and game playing. Despite
    its successes, traditional deep learning models are inherently deterministic and
    provide limited ability to quantify uncertainty in their predictions. To address
    this issue, Bayesian deep learning and probabilistic deep learning have emerged
    as important paradigms that allow for incorporating uncertainty into deep learning
    models.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: æ·±åº¦å­¦ä¹ åœ¨è®¡ç®—æœºè§†è§‰ã€è‡ªç„¶è¯­è¨€å¤„ç†å’Œæ¸¸æˆç­‰å¤šä¸ªåº”ç”¨é¢†åŸŸå–å¾—äº†æ˜¾è‘—æˆåŠŸã€‚å°½ç®¡å¦‚æ­¤ï¼Œä¼ ç»Ÿçš„æ·±åº¦å­¦ä¹ æ¨¡å‹æœ¬è´¨ä¸Šæ˜¯ç¡®å®šæ€§çš„ï¼Œå¹¶ä¸”åœ¨å…¶é¢„æµ‹ä¸­æä¾›äº†æœ‰é™çš„ä¸ç¡®å®šæ€§é‡åŒ–èƒ½åŠ›ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œè´å¶æ–¯æ·±åº¦å­¦ä¹ å’Œæ¦‚ç‡æ·±åº¦å­¦ä¹ ä½œä¸ºé‡è¦èŒƒå¼å‡ºç°ï¼Œå…è®¸å°†ä¸ç¡®å®šæ€§èå…¥æ·±åº¦å­¦ä¹ æ¨¡å‹ä¸­ã€‚
- en: Bayesian deep learning and probabilistic deep learning represent important paradigms
    for incorporating uncertainty into deep learning models. These approaches offer
    several advantages over traditional deterministic deep learning, including the
    ability to provide uncertainty estimates and the ability to perform robust inference
    in the presence of out-of-distribution data.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: è´å¶æ–¯æ·±åº¦å­¦ä¹ å’Œæ¦‚ç‡æ·±åº¦å­¦ä¹ ä»£è¡¨äº†å°†ä¸ç¡®å®šæ€§èå…¥æ·±åº¦å­¦ä¹ æ¨¡å‹çš„é‡è¦èŒƒå¼ã€‚è¿™äº›æ–¹æ³•ç›¸æ¯”ä¼ ç»Ÿçš„ç¡®å®šæ€§æ·±åº¦å­¦ä¹ å…·æœ‰è‹¥å¹²ä¼˜åŠ¿ï¼ŒåŒ…æ‹¬èƒ½å¤Ÿæä¾›ä¸ç¡®å®šæ€§ä¼°è®¡ä»¥åŠåœ¨å­˜åœ¨æ•°æ®åˆ†å¸ƒå¤–æ•°æ®æ—¶è¿›è¡Œç¨³å¥æ¨ç†çš„èƒ½åŠ›ã€‚
- en: In Bayesian deep learning, the model parameters are treated as random variables
    and a prior distribution is placed over them. This prior represents prior knowledge
    about the model parameters, such as their expected values or their distributional
    shape. The posterior distribution over the parameters is then updated through
    Bayesian inference, using the data to form a posterior that represents our updated
    beliefs about the parameters given the data. This results in a distribution over
    the model parameters and a measure of uncertainty in the model predictions.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è´å¶æ–¯æ·±åº¦å­¦ä¹ ä¸­ï¼Œæ¨¡å‹å‚æ•°è¢«è§†ä¸ºéšæœºå˜é‡ï¼Œå¹¶å¯¹å…¶æ–½åŠ å…ˆéªŒåˆ†å¸ƒã€‚è¿™ä¸ªå…ˆéªŒä»£è¡¨äº†å…³äºæ¨¡å‹å‚æ•°çš„å…ˆéªŒçŸ¥è¯†ï¼Œä¾‹å¦‚å®ƒä»¬çš„æœŸæœ›å€¼æˆ–åˆ†å¸ƒå½¢çŠ¶ã€‚ç„¶åï¼Œé€šè¿‡è´å¶æ–¯æ¨ç†æ›´æ–°å‚æ•°çš„åéªŒåˆ†å¸ƒï¼Œä½¿ç”¨æ•°æ®å½¢æˆä¸€ä¸ªåéªŒåˆ†å¸ƒï¼Œè¿™ä»£è¡¨äº†æˆ‘ä»¬åœ¨æ•°æ®ç»™å®šä¸‹å¯¹å‚æ•°çš„æ›´æ–°ä¿¡å¿µã€‚è¿™å°†å¯¼è‡´æ¨¡å‹å‚æ•°çš„åˆ†å¸ƒä»¥åŠæ¨¡å‹é¢„æµ‹ä¸­çš„ä¸ç¡®å®šæ€§åº¦é‡ã€‚
- en: Probabilistic deep learning, on the other hand, models the data-generating process
    as a probabilistic function. Given an input, the model predicts a distribution
    over outputs, allowing for the quantification of uncertainty in predictions. This
    approach is particularly useful for problems where the output space is complex,
    such as image generation or speech synthesis. In these cases, modeling the data-generating
    process as a probabilistic function allows for the capture of complex patterns
    in the data and the generation of high-quality outputs.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: æ¦‚ç‡æ·±åº¦å­¦ä¹ åˆ™å°†æ•°æ®ç”Ÿæˆè¿‡ç¨‹å»ºæ¨¡ä¸ºæ¦‚ç‡å‡½æ•°ã€‚ç»™å®šä¸€ä¸ªè¾“å…¥ï¼Œæ¨¡å‹é¢„æµ‹ä¸€ä¸ªè¾“å‡ºåˆ†å¸ƒï¼Œä»è€Œå…è®¸é‡åŒ–é¢„æµ‹ä¸­çš„ä¸ç¡®å®šæ€§ã€‚è¿™ç§æ–¹æ³•åœ¨è¾“å‡ºç©ºé—´å¤æ‚çš„é—®é¢˜ä¸­å°¤ä¸ºæœ‰ç”¨ï¼Œä¾‹å¦‚å›¾åƒç”Ÿæˆæˆ–è¯­éŸ³åˆæˆã€‚åœ¨è¿™äº›æƒ…å†µä¸‹ï¼Œå°†æ•°æ®ç”Ÿæˆè¿‡ç¨‹å»ºæ¨¡ä¸ºæ¦‚ç‡å‡½æ•°å¯ä»¥æ•æ‰æ•°æ®ä¸­çš„å¤æ‚æ¨¡å¼ï¼Œå¹¶ç”Ÿæˆé«˜è´¨é‡çš„è¾“å‡ºã€‚
- en: Both Bayesian deep learning and probabilistic deep learning are important and
    active areas of research, with recent advances in techniques such as Bayesian
    neural networks, variational inference, and deep generative models. Despite their
    potential benefits, these approaches remain challenging due to the computational
    and statistical difficulties posed by high-dimensional models and the large amounts
    of data typically used in deep learning. Nevertheless, significant progress has
    been made in this field in recent years, and there is much excitement and potential
    for future research in Bayesian deep learning and probabilistic deep learning.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: è´å¶æ–¯æ·±åº¦å­¦ä¹ å’Œæ¦‚ç‡æ·±åº¦å­¦ä¹ éƒ½æ˜¯é‡è¦ä¸”æ´»è·ƒçš„ç ”ç©¶é¢†åŸŸï¼Œæœ€è¿‘åœ¨è´å¶æ–¯ç¥ç»ç½‘ç»œã€å˜åˆ†æ¨æ–­å’Œæ·±åº¦ç”Ÿæˆæ¨¡å‹ç­‰æŠ€æœ¯æ–¹é¢å–å¾—äº†è¿›å±•ã€‚å°½ç®¡è¿™äº›æ–¹æ³•å…·æœ‰æ½œåœ¨çš„å¥½å¤„ï¼Œä½†ç”±äºé«˜ç»´æ¨¡å‹å’Œæ·±åº¦å­¦ä¹ ä¸­é€šå¸¸ä½¿ç”¨çš„å¤§é‡æ•°æ®æ‰€å¸¦æ¥çš„è®¡ç®—å’Œç»Ÿè®¡å›°éš¾ï¼Œè¿™äº›æ–¹æ³•ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚å°½ç®¡å¦‚æ­¤ï¼Œè¿‘å¹´æ¥è¯¥é¢†åŸŸå·²ç»å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œè´å¶æ–¯æ·±åº¦å­¦ä¹ å’Œæ¦‚ç‡æ·±åº¦å­¦ä¹ çš„æœªæ¥ç ”ç©¶å……æ»¡äº†å…´å¥‹å’Œæ½œåŠ›ã€‚
- en: 'Bayesian Learning: A Primer'
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: è´å¶æ–¯å­¦ä¹ ï¼šæ¦‚è¿°
- en: In this article, we consider the Bayesian framework for statistical inference,
    where we represent probability density with the notation ğ‘ƒ.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘ä»¬è€ƒè™‘äº†ç”¨äºç»Ÿè®¡æ¨æ–­çš„è´å¶æ–¯æ¡†æ¶ï¼Œå…¶ä¸­æˆ‘ä»¬ç”¨ç¬¦å·ğ‘ƒè¡¨ç¤ºæ¦‚ç‡å¯†åº¦ã€‚
- en: 'Bayesian methods provide a unified approach for conducting statistical inference,
    where we aim to compute the distribution of model parameters given some observed
    data. In the context of neural networks, this is particularly useful for estimating
    the uncertainty in weight parameters, as we can calculate the posterior distribution
    of these parameters given the training data. This is achieved by applying Bayesâ€™
    theorem, which states:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: è´å¶æ–¯æ–¹æ³•æä¾›äº†ä¸€ç§ç»Ÿä¸€çš„ç»Ÿè®¡æ¨æ–­æ–¹æ³•ï¼Œæˆ‘ä»¬çš„ç›®æ ‡æ˜¯è®¡ç®—ç»™å®šä¸€äº›è§‚å¯Ÿæ•°æ®çš„æ¨¡å‹å‚æ•°åˆ†å¸ƒã€‚åœ¨ç¥ç»ç½‘ç»œçš„èƒŒæ™¯ä¸‹ï¼Œè¿™å¯¹äºä¼°è®¡æƒé‡å‚æ•°çš„ä¸ç¡®å®šæ€§å°¤ä¸ºæœ‰ç”¨ï¼Œå› ä¸ºæˆ‘ä»¬å¯ä»¥è®¡ç®—ç»™å®šè®­ç»ƒæ•°æ®çš„è¿™äº›å‚æ•°çš„åéªŒåˆ†å¸ƒã€‚è¿™æ˜¯é€šè¿‡åº”ç”¨è´å¶æ–¯å®šç†å®ç°çš„ï¼Œè¯¥å®šç†æŒ‡å‡ºï¼š
- en: '![](../Images/1e00cd974ec578996564be058a73c5a5.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1e00cd974ec578996564be058a73c5a5.png)'
- en: 'where:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­ï¼š
- en: 'ğ·: The observed data, represented as pairs of ğ‘¥ and ğ‘¦ values, e.g. ğ·={(ğ‘¥1,ğ‘¦1),â€¦,(ğ‘¥ğ‘›,ğ‘¦ğ‘›)}'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ğ·ï¼šè§‚å¯Ÿæ•°æ®ï¼Œä»¥ğ‘¥å’Œğ‘¦å€¼å¯¹è¡¨ç¤ºï¼Œä¾‹å¦‚ ğ·={(ğ‘¥1,ğ‘¦1),â€¦,(ğ‘¥ğ‘›,ğ‘¦ğ‘›)}
- en: 'ğ‘¤: The value of a model weight'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ğ‘¤ï¼šæ¨¡å‹æƒé‡çš„å€¼
- en: 'ğ‘ƒ(ğ‘¤): The prior density, representing our initial belief on the distribution
    of model weights before seeing the data'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ğ‘ƒ(ğ‘¤)ï¼šå…ˆéªŒå¯†åº¦ï¼Œè¡¨ç¤ºåœ¨è§‚å¯Ÿåˆ°æ•°æ®ä¹‹å‰å¯¹æ¨¡å‹æƒé‡åˆ†å¸ƒçš„åˆå§‹ä¿¡å¿µ
- en: 'ğ‘ƒ(ğ·|ğ‘¤): The likelihood of observing the data given the weight ğ‘¤'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ğ‘ƒ(ğ·|ğ‘¤)ï¼šç»™å®šæƒé‡ğ‘¤æ—¶è§‚å¯Ÿåˆ°æ•°æ®çš„ä¼¼ç„¶
- en: 'ğ‘ƒ(ğ‘¤|ğ·): The posterior density of the model weight, calculated by incorporating
    the observed data and prior belief through Bayesâ€™ theorem.'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ğ‘ƒ(ğ‘¤|ğ·)ï¼šæ¨¡å‹æƒé‡çš„åéªŒå¯†åº¦ï¼Œé€šè¿‡è´å¶æ–¯å®šç†å°†è§‚å¯Ÿåˆ°çš„æ•°æ®å’Œå…ˆéªŒä¿¡å¿µç»“åˆè®¡ç®—å¾—å‡ºã€‚
- en: The normalization term âˆ«ğ‘ƒ(ğ·|ğ‘¤â€²)ğ‘ƒ(ğ‘¤â€²)dğ‘¤â€²=ğ‘ƒ(ğ·) is independent of ğ‘¤ and serves
    to normalize the posterior density.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: å½’ä¸€åŒ–é¡¹ âˆ«ğ‘ƒ(ğ·|ğ‘¤â€²)ğ‘ƒ(ğ‘¤â€²)dğ‘¤â€²=ğ‘ƒ(ğ·) æ˜¯ä¸ğ‘¤æ— å…³çš„ï¼Œç”¨äºå½’ä¸€åŒ–åéªŒå¯†åº¦ã€‚
- en: Bayesâ€™ theorem enables us to synthesize the observed data with our prior belief
    to obtain the posterior distribution of model parameters, providing a probabilistic
    representation of their uncertainty. The implementation of Bayesian learning in
    the context of neural networks can be challenging due to the complexity of calculating
    the normalization constant. To overcome this, various approximate methods such
    as Variational Bayes are utilized. These methods aim to efficiently estimate the
    posterior distribution of the NN weights.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: è´å¶æ–¯å®šç†ä½¿æˆ‘ä»¬èƒ½å¤Ÿå°†è§‚å¯Ÿåˆ°çš„æ•°æ®ä¸æˆ‘ä»¬çš„å…ˆéªŒä¿¡å¿µç»“åˆï¼Œä»¥è·å¾—æ¨¡å‹å‚æ•°çš„åéªŒåˆ†å¸ƒï¼Œæä¾›å®ƒä»¬ä¸ç¡®å®šæ€§çš„æ¦‚ç‡è¡¨ç¤ºã€‚åœ¨ç¥ç»ç½‘ç»œèƒŒæ™¯ä¸‹å®æ–½è´å¶æ–¯å­¦ä¹ å¯èƒ½å¾ˆå…·æŒ‘æˆ˜æ€§ï¼Œå› ä¸ºè®¡ç®—å½’ä¸€åŒ–å¸¸æ•°çš„å¤æ‚æ€§ã€‚ä¸ºäº†å…‹æœè¿™ä¸€ç‚¹ï¼Œä½¿ç”¨äº†å„ç§è¿‘ä¼¼æ–¹æ³•ï¼Œä¾‹å¦‚å˜åˆ†è´å¶æ–¯ã€‚è¿™äº›æ–¹æ³•æ—¨åœ¨æœ‰æ•ˆåœ°ä¼°è®¡ç¥ç»ç½‘ç»œæƒé‡çš„åéªŒåˆ†å¸ƒã€‚
- en: Variational Bayesian Inference
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å˜åˆ†è´å¶æ–¯æ¨æ–­
- en: In Variational Bayes, we approximate the posterior distribution with a second
    function, known as the variational posterior. This function is parameterized by
    some set of parameters, denoted by ğœƒ, which can be learned through optimization.
    The key idea is to choose a functional form for the variational posterior such
    that it can be effectively optimized to approximate the true posterior as closely
    as possible.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å˜åˆ†è´å¶æ–¯æ–¹æ³•ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨ç¬¬äºŒä¸ªå‡½æ•°æ¥è¿‘ä¼¼åéªŒåˆ†å¸ƒï¼Œè¿™ä¸ªå‡½æ•°ç§°ä¸ºå˜åˆ†åéªŒã€‚è¿™ä¸ªå‡½æ•°ç”±ä¸€ç»„å‚æ•°æ¥è¡¨å¾ï¼Œè®°ä½œğœƒï¼Œå¯ä»¥é€šè¿‡ä¼˜åŒ–æ¥å­¦ä¹ ã€‚å…³é”®æ€æƒ³æ˜¯é€‰æ‹©ä¸€ä¸ªå˜åˆ†åéªŒçš„å‡½æ•°å½¢å¼ï¼Œä½¿å…¶èƒ½å¤Ÿæœ‰æ•ˆåœ°ä¼˜åŒ–ï¼Œä»¥å°½å¯èƒ½æ¥è¿‘çœŸå®çš„åéªŒåˆ†å¸ƒã€‚
- en: One common approach to measuring the quality of the approximation is through
    the use of the Kullback-Leibler (KL) divergence between the variational posterior
    and the true posterior. The KL divergence is a measure of the difference between
    two probability distributions, and its value is 0 when the two distributions are
    equal.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ç§å¸¸è§çš„è¯„ä¼°è¿‘ä¼¼è´¨é‡çš„æ–¹æ³•æ˜¯ä½¿ç”¨å˜åˆ†åéªŒå’ŒçœŸå®åéªŒä¹‹é—´çš„ Kullback-Leibler (KL) æ•£åº¦ã€‚KL æ•£åº¦æ˜¯è¡¡é‡ä¸¤ä¸ªæ¦‚ç‡åˆ†å¸ƒä¹‹é—´å·®å¼‚çš„æŒ‡æ ‡ï¼Œå½“ä¸¤ä¸ªåˆ†å¸ƒç›¸ç­‰æ—¶ï¼Œå…¶å€¼ä¸º0ã€‚
- en: Given the observed data, ğ·, the KL divergence between the variational posterior,
    ğ‘(ğ‘¤|ğœƒ), and the true posterior, ğ‘ƒ(ğ‘¤|ğ·), is defined as
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: ç»™å®šè§‚å¯Ÿæ•°æ® ğ·ï¼Œå˜åˆ†åéªŒ ğ‘(ğ‘¤|ğœƒ) å’ŒçœŸå®åéªŒ ğ‘ƒ(ğ‘¤|ğ·) ä¹‹é—´çš„ KL æ•£åº¦å®šä¹‰ä¸º
- en: '![](../Images/c6241bd40be3a89f3550ea48e775964a.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c6241bd40be3a89f3550ea48e775964a.png)'
- en: 'When considering the observed data as constant, we can simplify this expression
    to a function that depends only on ğœƒ and ğ·:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: å½“å°†è§‚å¯Ÿæ•°æ®è§†ä¸ºå¸¸é‡æ—¶ï¼Œæˆ‘ä»¬å¯ä»¥å°†è¿™ä¸ªè¡¨è¾¾å¼ç®€åŒ–ä¸ºä»…ä¾èµ–äº ğœƒ å’Œ ğ· çš„å‡½æ•°ï¼š
- en: '![](../Images/0ebf632c807e607833aee3ca46b04c29.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0ebf632c807e607833aee3ca46b04c29.png)'
- en: where the first term measures the discrepancy between the variational and prior
    distributions, and the second term is the expected negative log likelihood under
    the variational posterior.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: ç¬¬ä¸€é¡¹æµ‹é‡å˜åˆ†åˆ†å¸ƒå’Œå…ˆéªŒåˆ†å¸ƒä¹‹é—´çš„å·®å¼‚ï¼Œç¬¬äºŒé¡¹æ˜¯å˜åˆ†åéªŒä¸‹çš„æœŸæœ›è´Ÿå¯¹æ•°ä¼¼ç„¶ã€‚
- en: This function, known as the variational lower bound, can then be optimized with
    respect to ğœƒ to obtain the best possible approximation of the true posterior.
    The optimization can be performed using gradient-based methods, making it possible
    to scale Variational Bayes to large, complex models.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªå‡½æ•°ï¼Œç§°ä¸ºå˜åˆ†ä¸‹ç•Œï¼Œå¯ä»¥é’ˆå¯¹ ğœƒ è¿›è¡Œä¼˜åŒ–ï¼Œä»¥è·å¾—å¯¹çœŸå®åéªŒçš„æœ€ä½³è¿‘ä¼¼ã€‚ä¼˜åŒ–å¯ä»¥ä½¿ç”¨åŸºäºæ¢¯åº¦çš„æ–¹æ³•è¿›è¡Œï¼Œä½¿å˜åˆ†è´å¶æ–¯èƒ½å¤Ÿæ‰©å±•åˆ°å¤§å‹å¤æ‚æ¨¡å‹ã€‚
- en: The Backpropagation Scheme for Bayesian Neural Networks
  id: totrans-50
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: è´å¶æ–¯ç¥ç»ç½‘ç»œçš„åå‘ä¼ æ’­æ–¹æ¡ˆ
- en: In this section, we describe a backpropagation algorithm for Bayesian neural
    networks, which allow us to incorporate weight uncertainty into our models. Our
    approach involves introducing a variational posterior with density ğ‘(ğ‘¤|ğœƒ), where
    ğ‘¤ is a weight in the network and ğœƒ is a set of trainable parameters. This posterior
    represents our approximation of the true weight posterior, which is determined
    by the observed data and our prior beliefs about the weights encoded by the prior
    density ğ‘ƒ(ğ‘¤).
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ä¸€éƒ¨åˆ†ï¼Œæˆ‘ä»¬æè¿°äº†ä¸€ç§ç”¨äºè´å¶æ–¯ç¥ç»ç½‘ç»œçš„åå‘ä¼ æ’­ç®—æ³•ï¼Œå…è®¸æˆ‘ä»¬å°†æƒé‡ä¸ç¡®å®šæ€§çº³å…¥æ¨¡å‹ã€‚æˆ‘ä»¬çš„æ–¹æ³•æ¶‰åŠå¼•å…¥ä¸€ä¸ªå¯†åº¦ä¸º ğ‘(ğ‘¤|ğœƒ) çš„å˜åˆ†åéªŒï¼Œå…¶ä¸­
    ğ‘¤ æ˜¯ç½‘ç»œä¸­çš„ä¸€ä¸ªæƒé‡ï¼Œğœƒ æ˜¯ä¸€ç»„å¯è®­ç»ƒçš„å‚æ•°ã€‚è¿™ä¸ªåéªŒè¡¨ç¤ºäº†æˆ‘ä»¬å¯¹çœŸå®æƒé‡åéªŒçš„è¿‘ä¼¼ï¼ŒçœŸå®æƒé‡åéªŒç”±è§‚å¯Ÿæ•°æ®å’Œæˆ‘ä»¬å¯¹æƒé‡çš„å…ˆéªŒä¿¡å¿µï¼ˆé€šè¿‡å…ˆéªŒå¯†åº¦ ğ‘ƒ(ğ‘¤)
    ç¼–ç ï¼‰å†³å®šã€‚
- en: 'Our goal is to update the variational posterior so that it accurately approximates
    the true weight posterior, which we quantify by the Evidence Lower Bound (ELBO).
    To do so, we minimize the ELBO between the variational posterior and the prior,
    which can be written as:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬çš„ç›®æ ‡æ˜¯æ›´æ–°å˜åˆ†åéªŒï¼Œä½¿å…¶å‡†ç¡®åœ°é€¼è¿‘çœŸå®çš„æƒé‡åéªŒï¼Œè¿™ä¸€ç›®æ ‡é€šè¿‡è¯æ®ä¸‹ç•Œï¼ˆELBOï¼‰æ¥é‡åŒ–ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æœ€å°åŒ–å˜åˆ†åéªŒå’Œå…ˆéªŒä¹‹é—´çš„ELBOï¼Œå…¬å¼å¦‚ä¸‹ï¼š
- en: '![](../Images/76fb86a007c33e8767c16d006e8a9d5f.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/76fb86a007c33e8767c16d006e8a9d5f.png)'
- en: 'where ğ· is the training data. This expression, however, involves an integral
    over the weight ğ‘¤ which may be computationally intractable. To circumvent this
    difficulty, we re-write the ELBO as an expectation:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­ ğ· æ˜¯è®­ç»ƒæ•°æ®ã€‚ç„¶è€Œï¼Œè¿™ä¸ªè¡¨è¾¾å¼æ¶‰åŠå¯¹æƒé‡ ğ‘¤ çš„ç§¯åˆ†ï¼Œè¿™å¯èƒ½åœ¨è®¡ç®—ä¸Šä¸å¯è¡Œã€‚ä¸ºäº†è§£å†³è¿™ä¸ªéš¾é¢˜ï¼Œæˆ‘ä»¬å°†ELBOé‡å†™ä¸ºæœŸæœ›ï¼š
- en: '![](../Images/95494b29915faa629278884b262d0660.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/95494b29915faa629278884b262d0660.png)'
- en: In order to perform backpropagation, we need to take derivatives of *ğ¿*(*ğœƒ*|*ğ·*)
    with respect to *ğœƒ*. However, this is challenging because the underlying distribution
    with respect to which the expectation is taken depends on *ğœƒ*. We overcome this
    issue through the reparameterization trick, which allows us to differentiate through
    the expectation.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†è¿›è¡Œåå‘ä¼ æ’­ï¼Œæˆ‘ä»¬éœ€è¦å¯¹ *ğ¿*(*ğœƒ*|*ğ·*) å…³äº *ğœƒ* è¿›è¡Œæ±‚å¯¼ã€‚ç„¶è€Œï¼Œè¿™å¾ˆå…·æœ‰æŒ‘æˆ˜æ€§ï¼Œå› ä¸ºæœŸæœ›çš„åº•å±‚åˆ†å¸ƒä¾èµ–äº *ğœƒ*ã€‚æˆ‘ä»¬é€šè¿‡é‡å‚æ•°åŒ–æŠ€å·§å…‹æœäº†è¿™ä¸ªé—®é¢˜ï¼Œè¿™ä½¿å¾—æˆ‘ä»¬èƒ½å¤Ÿé€šè¿‡æœŸæœ›è¿›è¡Œæ±‚å¯¼ã€‚
- en: 'The Reparameterization Trick: A Key Technique for Stochastic Gradient Optimization
    in Bayesian Neural Networks'
  id: totrans-57
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: é‡å‚æ•°åŒ–æŠ€å·§ï¼šè´å¶æ–¯ç¥ç»ç½‘ç»œä¸­éšæœºæ¢¯åº¦ä¼˜åŒ–çš„å…³é”®æŠ€æœ¯
- en: The reparameterization trick is a mathematical technique used to enable the
    optimization of models with stochastic parameters. The goal is to convert the
    dependent variable, ğ‘¤, into a random variable, ğœ–, with a fixed distribution independent
    of model parameters, ğœƒ. This conversion allows the expectation of ğ‘“(ğ‘¤;ğœ‡,ğœ) with
    respect to ğ‘(ğ‘¤|ğœ‡,ğœ) to be calculated independently of ğœƒ, enabling efficient optimization
    of the model.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: é‡æ–°å‚æ•°åŒ–æŠ€å·§æ˜¯ä¸€ç§æ•°å­¦æŠ€æœ¯ï¼Œç”¨äºä¼˜åŒ–å…·æœ‰éšæœºå‚æ•°çš„æ¨¡å‹ã€‚å…¶ç›®æ ‡æ˜¯å°†ä¾èµ–å˜é‡ğ‘¤è½¬æ¢ä¸ºå…·æœ‰å›ºå®šåˆ†å¸ƒä¸”ä¸æ¨¡å‹å‚æ•°ğœƒæ— å…³çš„éšæœºå˜é‡ğœ–ã€‚è¿™ç§è½¬æ¢ä½¿å¾—å¯ä»¥ç‹¬ç«‹äºğœƒè®¡ç®—ğ‘“(ğ‘¤;ğœ‡,ğœ)å¯¹ğ‘(ğ‘¤|ğœ‡,ğœ)çš„æœŸæœ›ï¼Œä»è€Œå®ç°æ¨¡å‹çš„é«˜æ•ˆä¼˜åŒ–ã€‚
- en: As an example, consider a Gaussian distribution ğ‘(ğ‘¤|ğœ‡,ğœ) where ğœƒ=(ğœ‡,ğœ). Using
    a change of variables ğ‘¤=ğœ‡+ğœğœ–, where ğœ–âˆ¼ğ‘(0,1), we obtain
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: ä¾‹å¦‚ï¼Œè€ƒè™‘ä¸€ä¸ªé«˜æ–¯åˆ†å¸ƒğ‘(ğ‘¤|ğœ‡,ğœ)ï¼Œå…¶ä¸­ğœƒ=(ğœ‡,ğœ)ã€‚é€šè¿‡å˜é‡å˜æ¢ğ‘¤=ğœ‡+ğœğœ–ï¼Œå…¶ä¸­ğœ–âˆ¼ğ‘(0,1)ï¼Œæˆ‘ä»¬å¾—åˆ°
- en: '![](../Images/e57ba17221fe7b4704dc142ae6f77473.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e57ba17221fe7b4704dc142ae6f77473.png)'
- en: The derivatives with respect to ğœ‡ and ğœ can then be easily calculated, and the
    expectations can be estimated through Monte Carlo sampling.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åå¯ä»¥è½»æ¾è®¡ç®—å…³äºğœ‡å’Œğœçš„å¯¼æ•°ï¼Œå¹¶é€šè¿‡è’™ç‰¹å¡ç½—é‡‡æ ·æ¥ä¼°è®¡æœŸæœ›ã€‚
- en: The reparameterization trick is a crucial tool for efficient optimization in
    Bayesian Neural Networks and has been widely adopted in modern deep learning research.
    It enables the gradient-based optimization of models with stochastic parameters,
    allowing for efficient training and inference.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: é‡æ–°å‚æ•°åŒ–æŠ€å·§æ˜¯è´å¶æ–¯ç¥ç»ç½‘ç»œä¸­é«˜æ•ˆä¼˜åŒ–çš„å…³é”®å·¥å…·ï¼Œå¹¶ä¸”åœ¨ç°ä»£æ·±åº¦å­¦ä¹ ç ”ç©¶ä¸­å¾—åˆ°äº†å¹¿æ³›åº”ç”¨ã€‚å®ƒä½¿å¾—å¯ä»¥å¯¹å…·æœ‰éšæœºå‚æ•°çš„æ¨¡å‹è¿›è¡ŒåŸºäºæ¢¯åº¦çš„ä¼˜åŒ–ï¼Œä»è€Œå®ç°é«˜æ•ˆçš„è®­ç»ƒå’Œæ¨æ–­ã€‚
- en: The Final Connection
  id: totrans-63
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æœ€ç»ˆè¿æ¥
- en: Bayesian deep learning offers a framework for incorporating uncertainty into
    deep learning models. By treating neural network weights as random variables,
    we can capture both aleatoric and epistemic uncertainty, allowing for more robust
    and reliable predictions. Variational Bayesian inference provides a way to learn
    the parameters of the posterior distribution over network weights, which can be
    optimized using the backpropagation scheme.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: è´å¶æ–¯æ·±åº¦å­¦ä¹ æä¾›äº†ä¸€ç§å°†ä¸ç¡®å®šæ€§èå…¥æ·±åº¦å­¦ä¹ æ¨¡å‹çš„æ¡†æ¶ã€‚é€šè¿‡å°†ç¥ç»ç½‘ç»œæƒé‡è§†ä¸ºéšæœºå˜é‡ï¼Œæˆ‘ä»¬å¯ä»¥æ•æ‰åˆ°å†…åœ¨ä¸ç¡®å®šæ€§å’Œè®¤è¯†ä¸ç¡®å®šæ€§ï¼Œä»è€Œæä¾›æ›´ç¨³å¥å’Œå¯é çš„é¢„æµ‹ã€‚å˜åˆ†è´å¶æ–¯æ¨æ–­æä¾›äº†ä¸€ç§å­¦ä¹ ç½‘ç»œæƒé‡åéªŒåˆ†å¸ƒå‚æ•°çš„æ–¹æ³•ï¼Œè¿™äº›å‚æ•°å¯ä»¥é€šè¿‡åå‘ä¼ æ’­æ–¹æ¡ˆè¿›è¡Œä¼˜åŒ–ã€‚
- en: The reparameterization trick, a key technique for stochastic gradient optimization
    in Bayesian neural networks, allows us to estimate gradients efficiently by transforming
    random variables into differentiable forms. This trick enables the use of standard
    gradient-based optimization algorithms to learn the parameters of the posterior
    distribution over network weights.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: é‡æ–°å‚æ•°åŒ–æŠ€å·§ï¼ˆreparameterization trickï¼‰ï¼Œè¿™æ˜¯è´å¶æ–¯ç¥ç»ç½‘ç»œä¸­ç”¨äºéšæœºæ¢¯åº¦ä¼˜åŒ–çš„å…³é”®æŠ€æœ¯ï¼Œä½¿æˆ‘ä»¬èƒ½å¤Ÿé€šè¿‡å°†éšæœºå˜é‡è½¬æ¢ä¸ºå¯å¾®åˆ†çš„å½¢å¼æ¥æœ‰æ•ˆåœ°ä¼°è®¡æ¢¯åº¦ã€‚è¿™ä¸€æŠ€å·§ä½¿å¾—å¯ä»¥ä½¿ç”¨æ ‡å‡†çš„åŸºäºæ¢¯åº¦çš„ä¼˜åŒ–ç®—æ³•æ¥å­¦ä¹ ç½‘ç»œæƒé‡åéªŒåˆ†å¸ƒçš„å‚æ•°ã€‚
- en: It is important to note that while aleatoric uncertainty accounts for the inherent
    noise in the data, epistemic uncertainty captures the uncertainty in our knowledge
    of the underlying model. In Bayesian deep learning, epistemic uncertainty is modeled
    by placing a prior distribution over network weights and updating it with observed
    data through the variational inference process. This results in a more flexible
    and informative model, as the network can adapt to new data and incorporate additional
    information about the underlying relationships.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œå°½ç®¡å†…åœ¨ä¸ç¡®å®šæ€§ï¼ˆaleatoric uncertaintyï¼‰è€ƒè™‘äº†æ•°æ®ä¸­çš„å›ºæœ‰å™ªå£°ï¼Œä½†è®¤è¯†ä¸ç¡®å®šæ€§ï¼ˆepistemic uncertaintyï¼‰åˆ™æ•æ‰äº†æˆ‘ä»¬å¯¹åŸºç¡€æ¨¡å‹çŸ¥è¯†çš„ä¸äº†è§£ã€‚åœ¨è´å¶æ–¯æ·±åº¦å­¦ä¹ ä¸­ï¼Œè®¤è¯†ä¸ç¡®å®šæ€§é€šè¿‡å¯¹ç½‘ç»œæƒé‡æ–½åŠ å…ˆéªŒåˆ†å¸ƒå¹¶é€šè¿‡å˜åˆ†æ¨æ–­è¿‡ç¨‹ç”¨è§‚å¯Ÿæ•°æ®æ›´æ–°å®ƒæ¥å»ºæ¨¡ã€‚è¿™å¯¼è‡´äº†ä¸€ä¸ªæ›´çµæ´»å’Œä¿¡æ¯é‡æ›´å¤§çš„æ¨¡å‹ï¼Œå› ä¸ºç½‘ç»œå¯ä»¥é€‚åº”æ–°æ•°æ®å¹¶èå…¥æœ‰å…³åŸºç¡€å…³ç³»çš„é¢å¤–ä¿¡æ¯ã€‚
- en: Conclusion
  id: totrans-67
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç»“è®º
- en: In conclusion, Bayesian Deep Learning provides a framework for incorporating
    uncertainty into the predictions of Deep Neural Networks. By treating the network
    weights as random variables and modeling their posterior distributions, we can
    obtain more robust and informative models compared to traditional deterministic
    deep learning approaches.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: æ€»ä¹‹ï¼Œè´å¶æ–¯æ·±åº¦å­¦ä¹ æä¾›äº†ä¸€ç§å°†ä¸ç¡®å®šæ€§èå…¥æ·±åº¦ç¥ç»ç½‘ç»œé¢„æµ‹çš„æ¡†æ¶ã€‚é€šè¿‡å°†ç½‘ç»œæƒé‡è§†ä¸ºéšæœºå˜é‡å¹¶å»ºæ¨¡å…¶åéªŒåˆ†å¸ƒï¼Œæˆ‘ä»¬å¯ä»¥è·å¾—æ¯”ä¼ ç»Ÿç¡®å®šæ€§æ·±åº¦å­¦ä¹ æ–¹æ³•æ›´ç¨³å¥å’Œä¿¡æ¯é‡æ›´å¤§çš„æ¨¡å‹ã€‚
- en: The reparameterization trick, combined with gradient-based optimization algorithms,
    enables scalable Bayesian inference in deep models. As a result, Bayesian Deep
    Learning has gained increasing attention and found success in various applications
    including active learning, out-of-distribution detection, and uncertainty-aware
    reinforcement learning. The field is still relatively new, and there is much room
    for further exploration and innovation.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: é‡å‚æ•°åŒ–æŠ€å·§ä¸åŸºäºæ¢¯åº¦çš„ä¼˜åŒ–ç®—æ³•ç›¸ç»“åˆï¼Œä½¿å¾—æ·±åº¦æ¨¡å‹ä¸­çš„å¯æ‰©å±•è´å¶æ–¯æ¨æ–­æˆä¸ºå¯èƒ½ã€‚å› æ­¤ï¼Œè´å¶æ–¯æ·±åº¦å­¦ä¹ å—åˆ°è¶Šæ¥è¶Šå¤šçš„å…³æ³¨ï¼Œå¹¶åœ¨åŒ…æ‹¬ä¸»åŠ¨å­¦ä¹ ã€åˆ†å¸ƒå¤–æ£€æµ‹å’Œä¸ç¡®å®šæ€§æ„ŸçŸ¥å¼ºåŒ–å­¦ä¹ ç­‰å„ç§åº”ç”¨ä¸­å–å¾—äº†æˆåŠŸã€‚è¿™ä¸ªé¢†åŸŸä»ç„¶ç›¸å¯¹è¾ƒæ–°ï¼Œä»æœ‰å¾ˆå¤§çš„æ¢ç´¢å’Œåˆ›æ–°ç©ºé—´ã€‚
- en: In the future, we expect Bayesian Deep Learning to play an increasingly important
    role in the advancement of artificial intelligence and machine learning.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœªæ¥ï¼Œæˆ‘ä»¬æœŸæœ›è´å¶æ–¯æ·±åº¦å­¦ä¹ åœ¨äººå·¥æ™ºèƒ½å’Œæœºå™¨å­¦ä¹ çš„å‘å±•ä¸­æ‰®æ¼”è¶Šæ¥è¶Šé‡è¦çš„è§’è‰²ã€‚
- en: About me
  id: totrans-71
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å…³äºæˆ‘
- en: Serial entrepreneur and leader in the AI space. I develop AI products for businesses
    and invest in AI-focused startups.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: è¿ç»­åˆ›ä¸šè€…å’ŒAIé¢†åŸŸçš„é¢†å¯¼è€…ã€‚æˆ‘ä¸ºä¼ä¸šå¼€å‘AIäº§å“ï¼Œå¹¶æŠ•èµ„äºä¸“æ³¨äºAIçš„åˆåˆ›å…¬å¸ã€‚
- en: '[Founder @ ZAAI](http://zaai.ai) | [LinkedIn](https://www.linkedin.com/in/luisbrasroque/)
    | [X/Twitter](https://x.com/luisbrasroque)'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '[åˆ›å§‹äºº @ ZAAI](http://zaai.ai) | [LinkedIn](https://www.linkedin.com/in/luisbrasroque/)
    | [X/Twitter](https://x.com/luisbrasroque)'
- en: References and Materials
  id: totrans-74
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å‚è€ƒèµ„æ–™
- en: '[1] â€” [Coursera: Deep Learning Specialization](https://www.coursera.org/specializations/deep-learning)'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] â€” [Coursera: Deep Learning Specialization](https://www.coursera.org/specializations/deep-learning)'
- en: '[2] â€” [Coursera: TensorFlow 2 for Deep Learning](https://www.coursera.org/specializations/tensorflow2-deeplearning)
    Specialization'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] â€” [Coursera: TensorFlow 2 for Deep Learning](https://www.coursera.org/specializations/tensorflow2-deeplearning)
    ä¸“ä¸šåŒ–è¯¾ç¨‹'
