- en: 'Time-Series Forecasting: Deep Learning vs Statistics — Who Wins?'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 时间序列预测：深度学习与统计学——谁能赢？
- en: 原文：[https://towardsdatascience.com/time-series-forecasting-deep-learning-vs-statistics-who-wins-c568389d02df](https://towardsdatascience.com/time-series-forecasting-deep-learning-vs-statistics-who-wins-c568389d02df)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/time-series-forecasting-deep-learning-vs-statistics-who-wins-c568389d02df](https://towardsdatascience.com/time-series-forecasting-deep-learning-vs-statistics-who-wins-c568389d02df)
- en: A comprehensive guide on the ultimate dilemma
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 关于最终难题的全面指南
- en: '[](https://medium.com/@nikoskafritsas?source=post_page-----c568389d02df--------------------------------)[![Nikos
    Kafritsas](../Images/de965cfcd8fbd8e1baf849017d365cbb.png)](https://medium.com/@nikoskafritsas?source=post_page-----c568389d02df--------------------------------)[](https://towardsdatascience.com/?source=post_page-----c568389d02df--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----c568389d02df--------------------------------)
    [Nikos Kafritsas](https://medium.com/@nikoskafritsas?source=post_page-----c568389d02df--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@nikoskafritsas?source=post_page-----c568389d02df--------------------------------)[![Nikos
    Kafritsas](../Images/de965cfcd8fbd8e1baf849017d365cbb.png)](https://medium.com/@nikoskafritsas?source=post_page-----c568389d02df--------------------------------)[](https://towardsdatascience.com/?source=post_page-----c568389d02df--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----c568389d02df--------------------------------)
    [Nikos Kafritsas](https://medium.com/@nikoskafritsas?source=post_page-----c568389d02df--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----c568389d02df--------------------------------)
    ·14 min read·Apr 5, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page-----c568389d02df--------------------------------)
    ·14 分钟阅读·2023年4月5日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/91b4bda3cb99b5a14f28d669d8fa4454.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/91b4bda3cb99b5a14f28d669d8fa4454.png)'
- en: Created with Stable Diffusion [1]
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 使用稳定扩散创建[1]
- en: '**In recent years, Deep Learning has made remarkable progress in the field
    of NLP.**'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '**近年来，深度学习在 NLP 领域取得了显著进展。**'
- en: 'Time series, also sequential in nature, raise the question: *what happens if
    we bring the full power of pretrained transformers to time-series forecasting*?'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 时间序列，本质上也具有顺序性，提出了一个问题：*如果我们将预训练变压器的全部能力应用于时间序列预测会发生什么？*
- en: However, some papers, such as [2] and [3] have scrutinized Deep Learning models.
    These papers do not present the full picture. Even for NLP cases, some people
    attribute the breakthrough of GPT models to “*more data and computing power*”
    instead of “*better ML research*”.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，一些论文，如[2]和[3]，对深度学习模型进行了详细审查。这些论文并没有呈现出完整的画面。即使在 NLP 案例中，有些人将 GPT 模型的突破归因于“*更多的数据和计算能力*”而不是“*更好的
    ML 研究*”。
- en: 'This article aims to clear the confusion and provide an unbiased view, using
    reliable data and sources from both academia and industry. Specifically, we will
    cover:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 本文旨在澄清混淆并提供客观观点，使用来自学术界和行业的可靠数据和来源。具体内容包括：
- en: The pros and cons of **Deep Learning** and **Statistical Models**.
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**深度学习**和**统计模型**的优缺点。'
- en: When to use Statistical models and when Deep Learning.
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 何时使用统计模型，何时使用深度学习。
- en: How to approach a forecasting case.
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何处理预测案例。
- en: How to save time and money by selecting the best model for your case and dataset.
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何通过选择适合你情况和数据集的最佳模型来节省时间和金钱。
- en: Let’s dive in.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们深入探讨。
- en: I’ve launched [**AI Horizon Forecast**](https://aihorizonforecast.substack.com)**,**
    a newsletter focusing on time-series and innovative AI research. Subscribe [here](https://aihorizonforecast.substack.com)
    to broaden your horizons!
  id: totrans-17
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 我刚刚发布了[**AI Horizon Forecast**](https://aihorizonforecast.substack.com)**，**这是一个专注于时间序列和创新
    AI 研究的新闻简报。订阅[这里](https://aihorizonforecast.substack.com)以拓宽视野！
- en: '[](https://aihorizonforecast.substack.com/p/autogluon-timeseries-creating-powerful?source=post_page-----c568389d02df--------------------------------)
    [## AutoGluon-TimeSeries : Creating Powerful Ensemble Forecasts - Complete Tutorial'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://aihorizonforecast.substack.com/p/autogluon-timeseries-creating-powerful?source=post_page-----c568389d02df--------------------------------)
    [## AutoGluon-TimeSeries：创建强大的集成预测 - 完整教程'
- en: Amazon's framework for time-series forecasting has it all.
  id: totrans-19
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 亚马逊的时间序列预测框架应有尽有。
- en: aihorizonforecast.substack.com](https://aihorizonforecast.substack.com/p/autogluon-timeseries-creating-powerful?source=post_page-----c568389d02df--------------------------------)
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: aihorizonforecast.substack.com](https://aihorizonforecast.substack.com/p/autogluon-timeseries-creating-powerful?source=post_page-----c568389d02df--------------------------------)
- en: Makridakis et al. Paper [4]
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Makridakis 等人的论文[4]
- en: We can’t discuss the progress of different forecasting models without considering
    the insights gained from **Makridakis competitions (M competitions).**
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不能讨论不同预测模型的进展，而不考虑从**Makridakis竞赛（M竞赛）**中获得的见解。
- en: Makridakis competitions are a series of large-scale challenges that demonstrate
    the latest advancements in time-series forecasting.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: Makridakis竞赛是一系列大规模挑战，展示了时间序列预测的最新进展。
- en: 'Recently, Makridakis et al. published a new paper that:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，Makridakis等人发表了一篇新论文，内容如下：
- en: Summarizes the current state of forecasting from the first 5 M-competitions.
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 总结了前五次M竞赛中的预测现状。
- en: Provides an extensive benchmark of various statistical, machine learning, and
    deep learning forecasting models.
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提供了各种统计、机器学习和深度学习预测模型的广泛基准。
- en: '**Note:** We will discuss the limitations of the paper later in this article.'
  id: totrans-27
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**注：** 我们将在本文后面讨论论文的局限性。'
- en: Benchmark Setup
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 基准设置
- en: Traditionally, Makridakis and his associates release a paper summarizing the
    results of the last M-competition.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 传统上，Makridakis及其同事会发布总结最后一次M竞赛结果的论文。
- en: For the first time, however, the authors have included Deep Learning models
    in their experiments. Why?
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，作者首次在实验中引入了深度学习模型。为什么？
- en: Unlike NLP, it was only in 2018–2019 that the first DL forecasting models were
    mature enough to challenge traditional forecasting models. In fact, during the
    M4 competition in 2018, the ML/DL models ranked last.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 与NLP不同，直到2018–2019年，首批DL预测模型才足够成熟以挑战传统预测模型。实际上，在2018年的M4竞赛中，ML/DL模型排名最后。
- en: '![](../Images/ade71ca0a631e914e519cd932c42006e.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ade71ca0a631e914e519cd932c42006e.png)'
- en: '**Figure 1:** Forecasting accuracy (sMAPE) of the eight statistical and the
    ten ML forecasting methods examined by Makridakis et al. back in 2018\. All ML
    methods occupied the last places.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '**图1：** 2018年Makridakis等人检查的八种统计和十种ML预测方法的预测准确率（sMAPE）。所有ML方法都排在最后。'
- en: 'Now, let’s see the DL/ML models that were used in the **new** paper:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们来看看在**新**论文中使用的DL/ML模型：
- en: '**Multi-layer Perceptron (MLP):** Ourfamiliar feed-forward network.'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**多层感知机（MLP）：** 我们熟悉的前馈网络。'
- en: '**WaveNet:** An autoregressive neural network that combines convolutional layers
    (2016).'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**WaveNet：** 一种自回归神经网络，结合了卷积层（2016年）。'
- en: '**Transformer:** The original Transformer, introduced in 2017.'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Transformer：** 原始Transformer，介绍于2017年。'
- en: '[**DeepAR**](https://medium.com/towards-data-science/deepar-mastering-time-series-forecasting-with-deep-learning-bc717771ce85)**:**
    Amazon’s first successful auto-regressive network that combines LSTMs (2017)'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**DeepAR**](https://medium.com/towards-data-science/deepar-mastering-time-series-forecasting-with-deep-learning-bc717771ce85)**：**
    亚马逊首个成功的自回归网络，结合了LSTMs（2017年）'
- en: '**Note:** The Deep Learning models of that study are not SOTA (state-of-the-art)
    anymore (more to that later). Also MLP is considered an ML and not a “deep” model.'
  id: totrans-39
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**注：** 那项研究的深度学习模型已不再是SOTA（最先进的技术）（更多内容请稍后）。另外，MLP被认为是ML模型而不是“深度”模型。'
- en: The statistical models of the benchmark are **ARIMA** and **ETS** (Exponential
    Smoothing) — well-known & battle-tested models.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 基准的统计模型是**ARIMA**和**ETS**（指数平滑）—— 这些是广为人知且经过实践验证的模型。
- en: 'Moreover:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 此外：
- en: The ML/DL models were fine-tuned first through hyper-parameter tuning.
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ML/DL模型首先通过超参数调优进行了微调。
- en: The statistical models are trained in a **series-by-series** fashion. Conversely,
    the DL models are **global** (a single model trained on all time series of the
    dataset). Hence, they take advantage of *cross-learning*.
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 统计模型是以**逐系列**的方式训练的。相反，DL模型是**全球**性的（一个模型训练于数据集的所有时间序列）。因此，它们利用了*跨学习*。
- en: 'The authors used ensembling: An **Ensemble-DL** modelwas createdfrom Deep Learning
    models, and an **Ensemble-S,** consisting of statistical models. The ensembling
    method was the median of forecasts.'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 作者使用了集成方法：从深度学习模型创建了一个**Ensemble-DL**模型，另一个**Ensemble-S**由统计模型组成。集成方法是预测的中位数。
- en: 'The **Ensemble-DL** consists of 200 models, with 50 models from each category:
    DeepAR, Transformer, WaveNet, and MLP.'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Ensemble-DL**由200个模型组成，每个类别有50个模型：DeepAR、Transformer、WaveNet和MLP。'
- en: 'The study utilized the M3 dataset: First, the authors tested 1,045 time series,
    and then the full dataset (3,003 series).'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 研究使用了M3数据集：首先，作者测试了1,045个时间序列，然后是完整数据集（3,003个序列）。
- en: The authors measured forecasting accuracy using **MASE** (*Mean Absolute Scaled
    Error*) and **SMAPE** (*Mean Absolute Percentage Error*). These error metrics
    are commonly used in forecasting.
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 作者使用 **MASE**（*均值绝对尺度误差*）和 **SMAPE**（*均值绝对百分比误差*）来测量预测准确性。这些误差度量常用于预测。
- en: Next, we provide a summary of the results and conclusions obtained from the
    benchmark.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们提供了从基准测试中获得的结果和结论的总结。
- en: 1\. Deep Learning Models are Better
  id: totrans-49
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 1. 深度学习模型更好
- en: The authors conclude that on average, DL models outperform the statistical ones.
    The result is shown in **Figure 2:**
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 作者总结道，平均而言，DL 模型优于统计模型。结果显示在 **图2** 中：
- en: '![](../Images/c2c408fbc353c3770e58de0b677be013.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c2c408fbc353c3770e58de0b677be013.png)'
- en: '**Figure 2:** Average ranks and 95% confidence intervals of all models, using
    sMAPE used for ranking.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '**图2：** 所有模型的平均排名及 95% 置信区间，使用 sMAPE 进行排名。'
- en: The **Ensemble-DL** model clearly outperforms the **Ensemble-S.** Also, DeepAR
    achieves very similar results with **Ensemble-S.**
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '**Ensemble-DL** 模型显然优于 **Ensemble-S**。此外，DeepAR 的结果与 **Ensemble-S** 非常相似。'
- en: Interestingly, **Figure 2** shows that although **Ensemble-DL** outperforms
    **Ensemble-S,** onlyDeepAR beats the individual statistical models. Why is that?
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是，**图2** 显示虽然 **Ensemble-DL** 优于 **Ensemble-S**，但只有 DeepAR 击败了单独的统计模型。这是为什么呢？
- en: We will answer that question later in the article.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在文章后面回答这个问题。
- en: 2\. But, Deep Learning Models Are Expensive
  id: totrans-56
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 2. 但是，深度学习模型是昂贵的
- en: Deep Learning models require a lot of time to train (and money). This is expected.
    The results are shown in **Figure 3:**
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习模型需要大量的训练时间（和资金）。这是预期之中的。结果显示在 **图3** 中：
- en: '![](../Images/7ae77f636242bc7f97e59c822a7bcea8.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7ae77f636242bc7f97e59c822a7bcea8.png)'
- en: '**Figure 3:** SMAPE vs Computational time. An ln(CT) of zero corresponds to
    about 1 minute of computational time, while an ln(CT) of 2, 4, 6, 8, and 10 correspond
    to about 7 minutes, 1 hour, 7 hours, 2 days, and 15 days, respectively.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '**图3：** SMAPE 与计算时间的关系。ln(CT) 为零对应大约 1 分钟的计算时间，而 ln(CT) 为 2、4、6、8 和 10 分别对应大约
    7 分钟、1 小时、7 小时、2 天和 15 天的计算时间。'
- en: The computational difference is significant.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 计算差异是显著的。
- en: 'Hence**,** lowering the forecasting error by 10% requires an extra computational
    time of about 15 days(**Ensemble-DL**). While this number seems enormous, there
    are some things to consider:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，降低 10% 的预测误差需要额外大约 15 天的计算时间 (**Ensemble-DL**)。虽然这个数字看起来很庞大，但有一些因素需要考虑：
- en: The authors do not specify what type of hardware they used.
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 作者没有说明他们使用了什么类型的硬件。
- en: They also don’t mention if any parallelization or training optimization is used.
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 他们也没有提到是否使用了任何并行化或训练优化。
- en: The computational time of **Ensemble-DL** can be significantly reduced if fewer
    models are used in the ensemble. This is displayed in **Figure 4:**
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果在集成中使用较少的模型，**Ensemble-DL** 的计算时间可以显著减少。这在 **图4** 中显示了：
- en: '![](../Images/b944af6cece9a2d33eef11b463345c05.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b944af6cece9a2d33eef11b463345c05.png)'
- en: '**Figure 4:** SMAPE vs number of models in the **Ensemble-DL** model.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '**图4：** **Ensemble-DL** 模型中模型数量与 SMAPE 的关系。'
- en: I mentioned previously that the **Ensemble-DL** model is an ensemble of 200
    DL models.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 我之前提到过，**Ensemble-DL** 模型是 200 个 DL 模型的集成。
- en: '**Figure 4** shows that 75 models can achieve comparable accuracy to 200 models
    with only one-third of the computational cost. This number can be further reduced
    if a more clever method to do the ensemble is used.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '**图4** 显示，75 个模型可以以只有 200 个模型三分之一的计算成本实现相当的准确性。如果使用更聪明的集成方法，这个数字可以进一步减少。'
- en: Finally, the current paper does not explore the transfer-learning capabilities
    of Deep Learning models. We will also discuss that later.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，当前论文并未探讨深度学习模型的迁移学习能力。我们将在后面讨论这一点。
- en: '**3\. Ensembling is All You Need**'
  id: totrans-70
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**3. 集成就是你需要的一切**'
- en: The power of ensembling is indisputable (**Figure 2**, **Figure 3**).
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 集成的威力是不容置疑的 (**图2**，**图3**)。
- en: Both **Ensemble-DL** and **Ensemble-SL** are the top-performing models. The
    idea is that each individual model excels at capturing different temporal dynamics.
    Combining their predictions enables the identification of complex patterns and
    accurate extrapolation.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '**Ensemble-DL** 和 **Ensemble-SL** 都是表现最好的模型。其理念是，每个单独的模型在捕捉不同的时间动态方面都表现出色。结合它们的预测可以识别复杂的模式和进行准确的外推。'
- en: 4\. Short-term vs Long-Term Forecasting
  id: totrans-73
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 4. 短期预测与长期预测
- en: The authors investigated whether there is a difference in models’ ability to
    forecast in the short-term versus the long-term.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 作者调查了模型在短期和长期预测能力上的差异。
- en: There was indeed.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 确实如此。
- en: '**Figure 5** breaks down the accuracy of each model for each forecasting horizon.
    For example, column 1 displays the one-step ahead forecast error. Similarly, column
    18 displays the error of the 18th step-ahead forecast.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '**图5**展示了每个模型在每个预测期的准确性。例如，第1列显示了一步预测误差。类似地，第18列显示了第18步预测的误差。'
- en: '![](../Images/26653f7678bc42c6b860bd9c7fe8eb09.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/26653f7678bc42c6b860bd9c7fe8eb09.png)'
- en: '**Figure 5:** sMAPE error of every model for 1045 series — lower is better
    ([Click here to see full image](https://gist.github.com/nkafr/ef8d2a84aafd626bc603cc4234291135))'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '**图5：** 1045个系列中每个模型的sMAPE误差——数值越低越好（[点击这里查看完整图像](https://gist.github.com/nkafr/ef8d2a84aafd626bc603cc4234291135)）'
- en: 'There are 3 key observations here:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有三个关键观察点：
- en: First, long-term forecasts are less accurate than short-term ones (no surprise
    here).
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 首先，长期预测的准确性低于短期预测（这并不意外）。
- en: In the first 4 horizons, **statistical models win.** Beyond that, **Deep Learning
    models start becoming better and Ensemble-DL wins.**
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在前4个预测期内，**统计模型胜出。** 在此之后，**深度学习模型开始变得更好，Ensemble-DL胜出。**
- en: Specifically, in the first horizon, **Ensemble-S** is 8.1% more accurate. However,
    in the last horizon, **Ensemble-DL** is 8.5% more accurate.
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 具体来说，在第一个预测期内，**Ensemble-S**的准确率高出8.1%。然而，在最后一个预测期，**Ensemble-DL**的准确率高出8.5%。
- en: 'If you think about this, it makes sense:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你考虑这个问题，就会明白：
- en: Statistical models are auto-regressive. As the forecasting horizons increase,
    the errors accumulate.
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 统计模型是自回归的。随着预测期的增加，误差会累积。
- en: In contrast, deep learning models are multi-output models. Hence, their forecasting
    errors are distributed across the entire prediction sequence.
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 相对而言，深度学习模型是多输出模型。因此，它们的预测误差分布在整个预测序列中。
- en: The only DL autoregressive model is DeepAR. That’s why DeepAR performs very
    well in the first horizons contrary to the other DL models.
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 唯一的深度学习自回归模型是DeepAR。这就是为什么DeepAR在前几个预测期表现非常好，而其他深度学习模型表现相对较差的原因。
- en: 5\. Do Deep Learning Models improve with more data?
  id: totrans-87
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 5\. 深度学习模型在更多数据下是否有所改善？
- en: In the previous experiment, the authors used just 1,045 time series from the
    M3 dataset.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的实验中，作者只使用了M3数据集中的1,045个时间序列。
- en: Next, the authors re-run their experiment using the full dataset (3,003 series).
    They also analyzed the forecasting losses per horizon. The results are shown in
    **Figure 6:**
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，作者使用完整的数据集（3,003个系列）重新进行了实验。他们还分析了每个预测期的预测损失。结果如**图6**所示：
- en: '![](../Images/4618207b7663ce42dc9d9917e0793f47.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4618207b7663ce42dc9d9917e0793f47.png)'
- en: '**Figure 6:** sMAPE error of every model for 3003 series — lower is better
    ([Click here to see full image](https://gist.github.com/nkafr/f1c86b825271abf55ce8577691ac669d))'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '**图6：** 3003个系列中每个模型的sMAPE误差——数值越低越好（[点击这里查看完整图像](https://gist.github.com/nkafr/f1c86b825271abf55ce8577691ac669d)）'
- en: Now, the gap between **Ensemble-DL** and **Ensemble-S** narrowed. The statistical
    models matched the deep learning models in the first horizon, but after that,
    the Ensemble-DL outperformed them.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，**Ensemble-DL**和**Ensemble-S**之间的差距缩小了。统计模型在第一个预测期与深度学习模型相当，但之后，Ensemble-DL超越了它们。
- en: Let’s further analyze the differences between **Ensemble-DL** and **Ensemble-S:**
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们进一步分析**Ensemble-DL**和**Ensemble-S**之间的差异：
- en: '![](../Images/236bd9ebe811660305d9da620d17148e.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/236bd9ebe811660305d9da620d17148e.png)'
- en: '**Figure 7:** Percentage improvement of Ensemble-DL over Ensemble-S ([Click
    here to see full image](https://gist.github.com/nkafr/d742007a350ab94c3827e191fc7e203c))'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '**图7：** Ensemble-DL相对于Ensemble-S的百分比改善（[点击这里查看完整图像](https://gist.github.com/nkafr/d742007a350ab94c3827e191fc7e203c)）'
- en: As the prediction step increases, Deep Learning models outperform the statistical
    ensemble.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 随着预测步数的增加，深度学习模型优于统计集合模型。
- en: 6\. On Trend and Seasonality Analysis
  id: totrans-97
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 6\. 趋势和季节性分析
- en: Finally, the authors investigate how statistical and DL models handle important
    time series characteristics like trend and seasonality.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，作者调查了统计模型和深度学习模型如何处理重要的时间序列特征，如趋势和季节性。
- en: 'To achieve this, the authors used the methodology by [5]. Specifically, they
    fitted a multiple linear regression model that correlated sMAPE error with 5 key
    time series characteristics: **forecastability(**randomness of errors**)**, **trend**,
    **seasonality**, **linearity**, and **stability(**optimal Box-Cox parameter transformation
    that decides data normality**)**. The results are shown in **Figure 8**:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 为此，作者使用了[5]中的方法论。具体来说，他们拟合了一个多元线性回归模型，将sMAPE误差与5个关键时间序列特征相关联：**预测能力（**错误的随机性**）**、**趋势**、**季节性**、**线性**和**稳定性（**决定数据正态性的最佳Box-Cox参数变换**）**。结果如**图8**所示：
- en: '![](../Images/0b9990bd9c33f21416f7c186681903e3.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0b9990bd9c33f21416f7c186681903e3.png)'
- en: '**Figure 8:** Linear Regression coefficients0 for different metrics. Lower
    is better'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 8：** 不同指标的线性回归系数0。数值越低越好。'
- en: 'We observe that:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 我们观察到：
- en: DL models perform better with **noisy, trended,** and **non-linear** data.
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深度学习模型在**嘈杂、趋势化**和**非线性**数据上表现更佳。
- en: Statistical models are more appropriate for **seasonal** & **low-variance**
    data with **linear relationships.**
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 统计模型更适用于**季节性**和**低方差**的数据，以及**线性关系**的数据。
- en: These insights are invaluable.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 这些见解是非常宝贵的。
- en: Hence, it is crucial to conduct extensive exploratory data analysis (EDA) and
    understand the nature of the data before selecting the appropriate model for your
    use case.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在选择适合您的用例的模型之前，进行广泛的探索性数据分析（EDA）并了解数据的性质是至关重要的。
- en: Study’s Limitations
  id: totrans-107
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 研究的局限性
- en: 'This paper is undoubtedly one of the best studies on the current state of the
    time-series forecasting landscape, yet it has some limitations. Let’s examine
    them:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 这篇论文无疑是对当前时间序列预测领域状态的最佳研究之一，但它也存在一些局限性。让我们来看看这些局限性：
- en: 'Lack of ML algorithms: Trees / Boosted Trees'
  id: totrans-109
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 缺乏 ML 算法：树 / Boosted Trees
- en: The family of Boosted Trees models has a significant place in time series forecasting
    problems.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: Boosted Trees 模型家族在时间序列预测问题中占有重要地位。
- en: The most popular ones are XGBoost, LightGBM, and CatBoost. Besides, LightGBM
    won the M5 competition.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 最受欢迎的模型有 XGBoost、LightGBM 和 CatBoost。此外，LightGBM 赢得了 M5 竞赛。
- en: These models excel with tabular-like data. In fact, to this day, Boosted Trees
    are the best choice for tabular data. However, the M3 dataset used in this study
    is simple as it mostly contains univariate series.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 这些模型在表格型数据中表现出色。事实上，到今天为止，Boosted Trees 仍然是表格数据的最佳选择。然而，本文使用的 M3 数据集非常简单，因为它主要包含单变量序列。
- en: In a future study, it would be a great idea to add Boosted Trees to the dataset,
    especially for more complex datasets.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 在未来的研究中，将 Boosted Trees 添加到数据集中，特别是对于更复杂的数据集，将是一个很好的主意。
- en: Choosing M3 as the Benchmark Dataset
  id: totrans-114
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 选择 M3 作为基准数据集
- en: 'Professor Rob Hyndman, Editor-in-Chief of the IJF journal said: “*The M3 dataset
    has been used since 2000 for testing forecasting methods; newly proposed methods
    must beat M3 to be published in IJF.*”'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: IJF 期刊主编 Rob Hyndman 教授说：“*自 2000 年以来，M3 数据集一直用于测试预测方法；新提出的方法必须超越 M3 才能在 IJF
    上发表。*”
- en: However, by modern standards, the M3 dataset is considered small and simple,
    and therefore not indicative of modern forecasting applications and practical
    scenarios.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，按现代标准，M3 数据集被认为是小型和简单的，因此不具备现代预测应用和实际场景的代表性。
- en: Of course, the choice of the dataset does not diminish the value of the study.
    Nevertheless, conducting a future benchmark with a larger dataset could provide
    valuable insights.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，数据集的选择并不会减少研究的价值。然而，进行一次使用更大数据集的未来基准测试可能会提供有价值的见解。
- en: The Deep Learning Models are not SOTA
  id: totrans-118
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 深度学习模型还不是最先进的（SOTA）。
- en: Now, it’s time to address the elephant in the room.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，是时候处理眼前的主要问题了。
- en: The Deep Learning models of the study are far from being state-of-the-art.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 该研究中的深度学习模型远未达到最先进水平。
- en: The study identified Amazon’s DeepAR as the best DL model in terms of theoretical
    forecasting accuracy. That’s why, DeepAR was the only model capable of outperforming
    the statistical models on an individual level. However, the DeepAR model is now
    more than 6 years old.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 该研究将亚马逊的 DeepAR 识别为在理论预测准确性方面最好的深度学习模型。因此，DeepAR 是唯一一个能够在单独层面上超越统计模型的模型。然而，DeepAR
    模型现在已经超过 6 年了。
- en: Amazon has since released its improved version of DeepAR, called [**Deep GPVAR**](https://medium.com/towards-data-science/deep-gpvar-upgrading-deepar-for-multi-dimensional-forecasting-e39204d90af3).
    In fact, **Deep GPVAR** is also outdated — Amazon’s latest Deep forecasting model
    is the MQTransformer, which was published in 2020.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 亚马逊随后发布了其改进版的 DeepAR，称为 [**Deep GPVAR**](https://medium.com/towards-data-science/deep-gpvar-upgrading-deepar-for-multi-dimensional-forecasting-e39204d90af3)。实际上，**Deep
    GPVAR** 也已过时——亚马逊最新的深度预测模型是 MQTransformer，它于 2020 年发布。
- en: Moreover, other powerful models like [**Temporal Fusion Transformer**](https://medium.com/towards-data-science/temporal-fusion-transformer-time-series-forecasting-with-deep-learning-complete-tutorial-d32c1e51cd91)
    (TFT) and [**N-BEATS**](https://medium.com/towards-data-science/n-beats-time-series-forecasting-with-neural-basis-expansion-af09ea39f538)
    (which was recently outperformed by N-HITS) are also not used in the Deep Learning
    ensemble.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，其他强大的模型，如[**时序融合变换器**](https://medium.com/towards-data-science/temporal-fusion-transformer-time-series-forecasting-with-deep-learning-complete-tutorial-d32c1e51cd91)（TFT）和[**N-BEATS**](https://medium.com/towards-data-science/n-beats-time-series-forecasting-with-neural-basis-expansion-af09ea39f538)（最近被N-HITS超越），在深度学习集成中也没有被使用。
- en: Therefore, the deep learning models used in the study are at least two generations
    behind the current state of the art. Undoubtedly, the current generation of deep
    forecasting models would have produced much better results.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，研究中使用的深度学习模型至少落后于当前技术水平的两代。不容置疑，当前一代深度预测模型将产生更好的结果。
- en: Forecasting is not Everything
  id: totrans-125
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 预测并不是一切
- en: 'Accuracy is essential in forecasting, but it’s not the only important factor.
    Other critical areas are:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 准确性在预测中至关重要，但并不是唯一重要的因素。其他关键领域包括：
- en: Uncertainty quantification
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不确定性量化
- en: Forecast interpretability
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预测可解释性
- en: Zero-Shot Learning / Meta-Learning
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 零-shot学习 / 元学习
- en: Regime Shift Segregation
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 政策转变隔离
- en: Speaking of **Zero-Shot Learning,** it’s one of the most promising areas in
    AI.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 说到**零-shot学习**，它是AI中最有前途的领域之一。
- en: Zero-shot learning is a model’s ability to correctly estimate unseen data, without
    being specifically trained on them. This learning method better reflects the human
    perception.
  id: totrans-132
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 零-shot学习是模型在没有专门训练的情况下，正确估计未见数据的能力。这种学习方法更好地反映了人类的认知。
- en: All Deep Learning models, including the GPT models, are based on this principle.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 所有的深度学习模型，包括GPT模型，都是基于这一原则的。
- en: The first well-acclaimed forecasting models that leverage this principle are
    [N-BEATS / N-HITS](https://medium.com/towards-data-science/n-beats-time-series-forecasting-with-neural-basis-expansion-af09ea39f538).
    These models can be trained on a vast time-series dataset and produce forecasts
    on completely novel data with similar accuracy as if the models had been explicitly
    trained on them.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 利用这一原则的首批广受好评的预测模型是[N-BEATS / N-HITS](https://medium.com/towards-data-science/n-beats-time-series-forecasting-with-neural-basis-expansion-af09ea39f538)。这些模型可以在庞大的时间序列数据集上进行训练，并在完全新的数据上进行预测，其准确性与模型专门训练过的数据相当。
- en: '*Zero-shot learning* is just a specific instance of *meta-learning*. Further
    progress with meta-learning on time-series has been made since. Take the [M6 competition](https://m6competition.com)
    for example, whose goal was to find if data science forecasting & econometrics
    can be used to beat the market, like legendary investors do (e.g. Warren Buffet).
    [The winning solution was a novel architecture that used, among other, neural
    networks and meta-learning](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4355794).'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '*零-shot学习*只是*元学习*的一个特定实例。自此以来，在时间序列上的元学习取得了进一步的进展。以[M6竞赛](https://m6competition.com)为例，其目标是检验数据科学预测与计量经济学是否能像传奇投资者（如沃伦·巴菲特）一样击败市场。[获胜解决方案是一种新型架构，其中使用了神经网络和元学习](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4355794)。'
- en: Unfortunately, this study does not explore the competitive advantage of Deep
    Learning models in a zero-shot learning setup.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，本研究并未探索深度学习模型在零-shot学习设置中的竞争优势。
- en: The Nixtla Study
  id: totrans-137
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Nixtla研究
- en: Nixtla, a promising start-up in the field of time-series forecasting, recently
    published a [benchmark](https://github.com/Nixtla/statsforecast/tree/main/experiments/m3)
    follow-up to the Makridakis et al. paper [4].
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: Nixtla，一家在时间序列预测领域有前景的初创公司，最近发布了对Makridakis等人论文[4]的[基准](https://github.com/Nixtla/statsforecast/tree/main/experiments/m3)后续研究。
- en: 'Specifically, the Nixtla team added 2 additional models: [Complex Exponential
    Smoothing](https://onlinelibrary.wiley.com/doi/full/10.1002/nav.22074) and [Dynamic
    Optimized Theta](https://nixtla.github.io/statsforecast/models.html#dynamic-optimized-theta-method).'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，Nixtla团队添加了2个额外的模型：[复杂指数平滑](https://onlinelibrary.wiley.com/doi/full/10.1002/nav.22074)和[动态优化Theta](https://nixtla.github.io/statsforecast/models.html#dynamic-optimized-theta-method)。
- en: The addition of these models reduced the gap between statistical and deep learning
    models. Furthermore, the Nixtla team correctly pointed out the significant difference
    in cost and resources required between the two categories.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 这些模型的加入缩小了统计模型和深度学习模型之间的差距。此外，Nixtla团队正确地指出了这两类模型在成本和资源需求上的显著差异。
- en: Indeed, many data scientists are misled by the overhyped promises of Deep Learning
    and lack the proper approach to solving a forecasting problem.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 确实，许多数据科学家被深度学习的过度炒作所误导，缺乏解决预测问题的正确方法。
- en: We will discuss this further in the next section. But before that, we need to
    address the criticism that Deep Learning faces.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在下一部分进一步讨论这个问题。但在此之前，我们需要解决深度学习面临的批评。
- en: Deep Learning Under Fire
  id: totrans-143
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度学习受到批评
- en: The progress of Deep Learning during the past decade is phenomenal. And there
    are no signs yet of slowing down.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 过去十年中，深度学习的发展是惊人的。至今尚无减缓的迹象。
- en: 'However, every revolutionary breakthrough that threatens to change the status
    quo is often met with skepticism and criticism. Take GPT-4 for example: this new
    development threatens 20% of US jobs in the next decade [6].'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，每一个威胁到现状的革命性突破通常都会遭遇怀疑和批评。以GPT-4为例：这一新发展在下一个十年威胁到20%的美国工作岗位[6]。
- en: 'The dominance of Deep Learning and Transformers in the field of NLP is undeniable.
    And yet, people in interviews ask questions that go something like this:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习和变换器在自然语言处理领域的主导地位是不容否认的。然而，面试中人们却提出类似这样的问题：
- en: Are the advances in NLP attributed to better research, or simply to the availability
    of more data and increased computer power?
  id: totrans-147
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 自然语言处理（NLP）的进步是归功于更好的研究，还是仅仅因为数据的增加和计算能力的提升？
- en: In time-series forecasting, things are worse. To understand the reason behind
    this, you must first comprehend how forecasting problems were traditionally tackled.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 在时间序列预测中，情况更为糟糕。要理解这一点的原因，你必须首先了解传统上是如何处理预测问题的。
- en: Before the widespread adoption of ML/DL, forecasting was all about crafting
    the right transformations for your dataset. This entailed making the time-series
    stationary, removing trends and seasonalities, accounting for volatility, and
    using techniques like box-cox transformations, among others. All of these approaches
    required manual intervention and a deep understanding of mathematics and time
    series.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习/深度学习广泛应用之前，预测完全是关于为数据集制定正确的转换。这包括使时间序列平稳，去除趋势和季节性，考虑波动性，并使用如box-cox变换等技术。所有这些方法都需要手动干预，并且对数学和时间序列有深刻理解。
- en: With the advent of ML, time-series algorithms became more automated. You can
    readily apply them to time-series problems with little to no preprocessing aside
    from cleaning (although additional preprocessing and feature engineering always
    help). Nowadays, much of the improvement effort on such a project is limited to
    hyperparameter tuning.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 随着机器学习的出现，时间序列算法变得更加自动化。你可以在几乎没有预处理的情况下直接应用这些算法（尽管额外的预处理和特征工程总是有帮助的）。如今，许多改进工作主要集中在超参数调优上。
- en: Therefore, people who used advanced maths & statistics cannot grasp the fact
    that an ML/DL algorithm can outperform a traditional statistical model. And the
    funny thing is, researchers have no idea how and why some DL concepts truly work.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，使用高级数学和统计学的人难以理解机器学习/深度学习算法能够超越传统统计模型的事实。而有趣的是，研究人员对一些深度学习概念真正有效的原因和机制却一无所知。
- en: Time-Series Forecasting in Recent Literature
  id: totrans-152
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 最近文献中的时间序列预测
- en: 'As far as I know, the current literature lacks sufficient evidence to illustrate
    the advantages and disadvantages of various categories of forecasting models.
    The 2 papers below are the most relevant:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 就我所知，现有文献缺乏足够的证据来说明各种预测模型的优缺点。以下两篇论文是最相关的：
- en: Are Transformers Effective for Time Series Forecasting?
  id: totrans-154
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 变换器模型对于时间序列预测是否有效？
- en: One interesting paper [2] displays the weaknesses of some forecasting Transformers
    models. The paper explains for example how positional encoding schemes, used in
    modern Transformer models fail to capture the temporal dynamics of time sequences.
    This is true — **self-attention is permutation-invariant**. However, the paper
    fails to mention the Transformer models that have effectively addressed this issue.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 一篇有趣的论文[2]展示了某些预测变换器模型的弱点。该论文举例说明了现代变换器模型中使用的位置信息编码方案如何未能捕捉时间序列的时间动态。这一点确实是正确的——**自注意力机制是不变的**。然而，论文未提及那些已经有效解决此问题的变换器模型。
- en: For example, Google’s [Temporal Fusion Transformer](https://medium.com/towards-data-science/temporal-fusion-transformer-googles-model-for-interpretable-time-series-forecasting-5aa17beb621)
    (TFT) uses an encoder-decoder LSTM layer to create time-aware and context-aware
    embeddings. Also, TFT uses a novel attention mechanism, adapted for time-series
    problems to capture temporal dynamics and provide interpretability.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，谷歌的 [Temporal Fusion Transformer](https://medium.com/towards-data-science/temporal-fusion-transformer-googles-model-for-interpretable-time-series-forecasting-5aa17beb621)（TFT）使用编码器-解码器
    LSTM 层来创建时间感知和上下文感知的嵌入。此外，TFT 使用了为时间序列问题适配的新颖注意力机制，以捕捉时间动态并提供可解释性。
- en: Similarly, Amazon’s MQTransformer uses its novel positional encoding scheme
    (***context-dependent seasonality encoding***) and attention mechanism (**f*eedback-aware
    attention***).
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，亚马逊的 MQTransformer 使用其新颖的位置编码方案（***上下文相关的季节性编码***）和注意力机制（**反馈感知注意力**）。
- en: Do We Really Need DL Models for Time Series Forecasting?
  id: totrans-158
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们真的需要深度学习模型来进行时间序列预测吗？
- en: This paper [3] is also interesting as it compares various forecasting methods
    across **statistical**, **Boosted Trees**, **ML**, and **DL** categories.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 这篇论文 [3] 也很有趣，因为它比较了 **统计**、**提升树**、**机器学习** 和 **深度学习** 类别的各种预测方法。
- en: 'Unfortunately, it falls short of its title, as the best model among the 12
    models is Google’s TFT, a pure Deep Learning model. The paper mentions:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，它未能达到其标题所述，因为在 12 个模型中表现最好的还是谷歌的 TFT，这是一种纯粹的深度学习模型。论文提到：
- en: … The results in Table 5 above underline the competitiveness of the rolling
    forecast configured GBRT, but also show that considerably stronger transformer-based
    models, such as the TFT [12], rightfully surpass the boosted regression tree performance..
  id: totrans-161
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: …… 表 5 中的结果强调了配置了滚动预测的 GBRT 的竞争力，但也显示了显著强大的基于变换器的模型，如 TFT [12]，确实超越了提升回归树的表现。
- en: In general, be cautious when reading sophisticated forecasting papers and models,
    particularly regarding the source of publication. The [International Journal of
    Forecasting (IJF)](https://www.sciencedirect.com/journal/international-journal-of-forecasting)
    is an example of a reputable journal focusing on forecasting.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 总的来说，阅读复杂的预测论文和模型时要小心，特别是关于出版来源的部分。[国际预测期刊 (IJF)](https://www.sciencedirect.com/journal/international-journal-of-forecasting)
    就是一个专注于预测的信誉良好的期刊的例子。
- en: How to Approach a Forecasting Problem
  id: totrans-163
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何处理预测问题
- en: This is not simple. Each dataset is unique, and the objectives of each project
    vary, making forecasting challenging.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 这并不简单。每个数据集都是独特的，每个项目的目标也各不相同，使得预测变得具有挑战性。
- en: Nevertheless, this article offers general advice that may be beneficial for
    most approaches.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，本文提供了一些可能对大多数方法有益的通用建议。
- en: As you have learned from this article, Deep Learning models are an emerging
    trend in forecasting projects, but they are still in their early stages. Despite
    their potential, they can also be a pitfall.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你从本文中了解到的，深度学习模型是预测项目中的一种新兴趋势，但它们仍处于早期阶段。尽管它们具有潜力，但也可能存在陷阱。
- en: It is not recommended to prioritize Deep Learning models for your project right
    away. According to Makridakis et al. and Nixtla's studies, it is best to start
    with statistical models. An ensemble of 3–4 statistical models may be more powerful
    than you expect. Also, give boosted trees a try, especially if you have tabular
    data. For small datasets (in the order of thousands), these methods may be adequate.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 不建议立即将深度学习模型作为项目的首选。根据 Makridakis 等人和 Nixtla 的研究，最好从统计模型开始。3–4 个统计模型的集成可能比你预期的更强大。此外，尝试使用提升树，特别是如果你有表格数据的话。对于小型数据集（数千条数据），这些方法可能已足够。
- en: Deep Learning models may provide an additional 3–10% accuracy boost. However,
    training these models can be time-consuming and expensive. For some fields, such
    as finance and retail, that extra accuracy boost may be more beneficial and justify
    using a DL model. A more accurate product sales prediction or an ETF’s closing
    price might translate to thousands of dollars in incremental revenue.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习模型可能提供额外的 3–10% 准确度提升。然而，训练这些模型可能耗时且昂贵。对于一些领域，如金融和零售，这额外的准确度提升可能更具价值，并使使用深度学习模型成为合理选择。更准确的产品销售预测或
    ETF 的收盘价可能会转化为数千美元的增量收入。
- en: On the other hand, DL models like N-BEATS and N-HITS have transfer-learning
    capabilities.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，像 N-BEATS 和 N-HITS 这样的深度学习模型具有迁移学习能力。
- en: If a large enough time-series dataset is constructed, and a willing entity pre-trains
    those 2 models and shares their parameters, we could readily use these models
    and achieve top-notch forecasting accuracy (or perform a small fine-tuning to
    our dataset first).
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 如果构建了足够大的时间序列数据集，并且一个愿意的实体对这 2 个模型进行预训练并共享其参数，我们可以直接使用这些模型并实现顶尖的预测准确性（或先对我们的数据集进行少量微调）。
- en: Closing Remarks
  id: totrans-171
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结束语
- en: Time-series forecasting is a key area of Data Science.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 时间序列预测是数据科学的一个关键领域。
- en: But it’s also very undervalued compared to other areas. The Makridakis et al.
    paper[4] provided some valuable insights for the future, but there is still a
    lot of work and research to be done.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 但与其他领域相比，它也被严重低估。Makridakis 等人[4]的论文为未来提供了一些有价值的见解，但仍有很多工作和研究需要完成。
- en: On top of that, DL models in forecasting are largely unexplored.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，深度学习模型在预测中的应用仍未被广泛探索。
- en: For example, **Multi-Modal** architecures in Deep-Leaning are everywhere. These
    architectures leverage more than one domain to learn a specific task. For instance,
    [**CLIP**](https://medium.com/p/f8ee408958b1) (used by **DALLE-2** and **Stable
    Diffusion**) combines *Natural Language Processing* and *Computer Vision*.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，**多模态**架构在深度学习中随处可见。这些架构利用多个领域来学习特定任务。例如，[**CLIP**](https://medium.com/p/f8ee408958b1)（由**DALLE-2**和**Stable
    Diffusion**使用）结合了*自然语言处理*和*计算机视觉*。
- en: The benchmark M3 dataset contains only 3,003 time series, each with no more
    than 500 observations. In contrast, the successful [**Deep GPVAR**](https://medium.com/p/e39204d90af3)
    forecasting model consists of an average of 44K parameters. In comparison, the
    smallest version of Facebook’s LLaMA language Transformer model has 7 billion
    parameters and was trained on 1 trillion tokens.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 基准 M3 数据集仅包含 3,003 个时间序列，每个序列最多 500 个观测值。相比之下，成功的 [**Deep GPVAR**](https://medium.com/p/e39204d90af3)
    预测模型包含平均 44K 个参数。相比之下，Facebook 的 LLaMA 语言 Transformer 模型的最小版本拥有 70 亿个参数，并在 1 万亿个令牌上进行训练。
- en: So, regarding the original question, there is no definitive answer as to which
    model is the best since each model has its own advantages and shortcomings.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，关于原始问题，没有明确的答案说明哪个模型是最好的，因为每个模型都有其优缺点。
- en: Instead, this article aims to provide all the necessary information to help
    you select the most suitable model for your project or case.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，本文旨在提供所有必要的信息，帮助您选择最适合您项目或案例的模型。
- en: Thank you for reading!
  id: totrans-179
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 感谢阅读！
- en: Follow me on [Linkedin](https://www.linkedin.com/in/nikos-kafritsas-b3699180/)!
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 关注我 [Linkedin](https://www.linkedin.com/in/nikos-kafritsas-b3699180/)！
- en: Subscribe to my [newsletter](https://aihorizonforecast.substack.com/welcome),
    AI Horizon Forecast!
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 订阅我的 [新闻通讯](https://aihorizonforecast.substack.com/welcome)，AI Horizon Forecast！
- en: '[](https://aihorizonforecast.substack.com/p/autogluon-timeseries-creating-powerful?source=post_page-----c568389d02df--------------------------------)
    [## AutoGluon-TimeSeries : Creating Powerful Ensemble Forecasts - Complete Tutorial'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://aihorizonforecast.substack.com/p/autogluon-timeseries-creating-powerful?source=post_page-----c568389d02df--------------------------------)
    [## AutoGluon-TimeSeries：创建强大的集成预测——完整教程'
- en: Amazon's framework for time-series forecasting has it all.
  id: totrans-183
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 亚马逊的时间序列预测框架具备了一切。
- en: aihorizonforecast.substack.com](https://aihorizonforecast.substack.com/p/autogluon-timeseries-creating-powerful?source=post_page-----c568389d02df--------------------------------)
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: '[aihorizonforecast.substack.com](https://aihorizonforecast.substack.com/p/autogluon-timeseries-creating-powerful?source=post_page-----c568389d02df--------------------------------)'
- en: References
  id: totrans-185
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] Created from Stable Diffusion with the text prompt “a blue cyan time-series
    abstract, shiny, digital painting, concept art”'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] 从 Stable Diffusion 创建，文本提示为“一个蓝青色时间序列抽象，闪亮的数字画，概念艺术”'
- en: '[2] Ailing Zeng et al. [*Are Transformers Effective for Time Series Forecasting?*](https://arxiv.org/pdf/2205.13504.pdf)
    (August 2022)'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] Ailing Zeng 等人 [*变压器在时间序列预测中是否有效？*](https://arxiv.org/pdf/2205.13504.pdf)（2022年8月）'
- en: '[3] Shereen Elsayed et al. [*Do We Really Need Deep Learning Models for Time
    Series Forecasting?*](https://arxiv.org/pdf/2101.02118.pdf)(October 2021)'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] Shereen Elsayed 等人 [*我们真的需要深度学习模型来进行时间序列预测吗？*](https://arxiv.org/pdf/2101.02118.pdf)（2021年10月）'
- en: '[4] Makridakis et al. [*Statistical, machine learning and deep learning forecasting
    methods: Comparisons and ways forward*](https://www.tandfonline.com/doi/epdf/10.1080/01605682.2022.2118629)
    *(*August 2022)'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] Makridakis 等人 [*统计学、机器学习和深度学习预测方法：比较与前进方向*](https://www.tandfonline.com/doi/epdf/10.1080/01605682.2022.2118629)
    *（2022年8月）*'
- en: '[5] Kang et al. [*Visualising forecasting algorithm performance using time
    series instance spaces*](https://www.sciencedirect.com/science/article/abs/pii/S0169207016301030)
    *(*International Journal of Forecasting, 2017)'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: '[5] Kang 等人 [*可视化预测算法性能使用时间序列实例空间*](https://www.sciencedirect.com/science/article/abs/pii/S0169207016301030)
    *（《国际预测学杂志》，2017年）*'
- en: '[6] Eloundou et al. [*GPTs are GPTs: An Early Look at the Labor Market Impact
    Potential of Large Language Models*](https://arxiv.org/pdf/2303.10130.pdf)(March
    2023)'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: '[6] Eloundou 等人 [*GPTs 即 GPTs：大型语言模型对劳动市场潜在影响的早期观察*](https://arxiv.org/pdf/2303.10130.pdf)（2023年3月）'
- en: All images used in the article are from [4]. [The M3 dataset as well as all
    M-datasets “are free to use without further permission by the IIF”](https://forecasters.org/resources/time-series-data/).
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 文章中使用的所有图像均来自[4]。 [M3 数据集以及所有 M 数据集“可以在不需进一步许可的情况下自由使用”](https://forecasters.org/resources/time-series-data/)。
