- en: 'Mistral 7B: Recipes for Fine-tuning and Quantization on Your Computer'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Mistral 7B：在您的计算机上进行微调和量化的配方
- en: 原文：[https://towardsdatascience.com/mistral-7b-recipes-for-fine-tuning-and-quantization-on-your-computer-631401583f77?source=collection_archive---------0-----------------------#2023-10-26](https://towardsdatascience.com/mistral-7b-recipes-for-fine-tuning-and-quantization-on-your-computer-631401583f77?source=collection_archive---------0-----------------------#2023-10-26)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/mistral-7b-recipes-for-fine-tuning-and-quantization-on-your-computer-631401583f77?source=collection_archive---------0-----------------------#2023-10-26](https://towardsdatascience.com/mistral-7b-recipes-for-fine-tuning-and-quantization-on-your-computer-631401583f77?source=collection_archive---------0-----------------------#2023-10-26)
- en: Cheap supervised fine-tuning with an impressive LLM
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 便宜的监督微调与令人印象深刻的LLM
- en: '[](https://medium.com/@bnjmn_marie?source=post_page-----631401583f77--------------------------------)[![Benjamin
    Marie](../Images/3ea1ad230cb1e67610418a8e36a5e5dd.png)](https://medium.com/@bnjmn_marie?source=post_page-----631401583f77--------------------------------)[](https://towardsdatascience.com/?source=post_page-----631401583f77--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----631401583f77--------------------------------)
    [Benjamin Marie](https://medium.com/@bnjmn_marie?source=post_page-----631401583f77--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@bnjmn_marie?source=post_page-----631401583f77--------------------------------)[![Benjamin
    Marie](../Images/3ea1ad230cb1e67610418a8e36a5e5dd.png)](https://medium.com/@bnjmn_marie?source=post_page-----631401583f77--------------------------------)[](https://towardsdatascience.com/?source=post_page-----631401583f77--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----631401583f77--------------------------------)
    [Benjamin Marie](https://medium.com/@bnjmn_marie?source=post_page-----631401583f77--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fad2a414578b3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmistral-7b-recipes-for-fine-tuning-and-quantization-on-your-computer-631401583f77&user=Benjamin+Marie&userId=ad2a414578b3&source=post_page-ad2a414578b3----631401583f77---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----631401583f77--------------------------------)
    ·9 min read·Oct 26, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F631401583f77&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmistral-7b-recipes-for-fine-tuning-and-quantization-on-your-computer-631401583f77&user=Benjamin+Marie&userId=ad2a414578b3&source=-----631401583f77---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fad2a414578b3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmistral-7b-recipes-for-fine-tuning-and-quantization-on-your-computer-631401583f77&user=Benjamin+Marie&userId=ad2a414578b3&source=post_page-ad2a414578b3----631401583f77---------------------post_header-----------)
    发布于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----631401583f77--------------------------------)
    · 9分钟阅读 · 2023年10月26日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F631401583f77&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmistral-7b-recipes-for-fine-tuning-and-quantization-on-your-computer-631401583f77&user=Benjamin+Marie&userId=ad2a414578b3&source=-----631401583f77---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F631401583f77&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmistral-7b-recipes-for-fine-tuning-and-quantization-on-your-computer-631401583f77&source=-----631401583f77---------------------bookmark_footer-----------)![](../Images/bfa5a4ac457b91e1272b5d1cd9f44188.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F631401583f77&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmistral-7b-recipes-for-fine-tuning-and-quantization-on-your-computer-631401583f77&source=-----631401583f77---------------------bookmark_footer-----------)![](../Images/bfa5a4ac457b91e1272b5d1cd9f44188.png)'
- en: The mistral is a wind blowing in the northern Mediterranean Sea — Illustration
    from [Pixabay](https://pixabay.com/illustrations/wind-girl-tree-long-hair-4054954/)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: mistral 是一种吹拂在北部地中海的风 —— 图片来自 [Pixabay](https://pixabay.com/illustrations/wind-girl-tree-long-hair-4054954/)
- en: Mistral 7B is a very popular large language model (LLM) created by Mistral AI.
    It outperforms all [the other pre-trained LLMs of similar size](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard)
    and is even better than larger LLMs such as Llama 2 13B.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: Mistral 7B 是由 Mistral AI 创建的非常受欢迎的大型语言模型（LLM）。它超越了所有 [其他相似规模的预训练LLM](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard)，甚至优于更大的LLM，如
    Llama 2 13B。
- en: It is also very well optimized for fast decoding, especially for long contexts,
    thanks to the use of a sliding window to compute attention and grouped-query attention
    (GQA). You can find more details in [the arXiv paper presenting Mistral 7B](https://arxiv.org/abs/2310.06825)
    and in [this excellent article](https://medium.com/gitconnected/mistral-7b-a-new-wind-blowing-other-language-models-b74d7bfe137e)
    by [Salvatore Raieli](https://medium.com/u/f1a08d9452cd?source=post_page-----631401583f77--------------------------------).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 它也非常优化了快速解码，特别是对于长上下文，这要归功于使用滑动窗口计算注意力和分组查询注意力（GQA）。你可以在[介绍Mistral 7B的arXiv论文](https://arxiv.org/abs/2310.06825)和[Salvatore
    Raieli](https://medium.com/u/f1a08d9452cd?source=post_page-----631401583f77--------------------------------)的[这篇精彩文章](https://medium.com/gitconnected/mistral-7b-a-new-wind-blowing-other-language-models-b74d7bfe137e)中找到更多细节。
- en: Mistral 7B is good but small enough to be exploited with affordable hardware.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: Mistral 7B表现良好，但足够小，可以在经济实惠的硬件上利用。
- en: In this article, I will show you how to fine-tune Mistral 7B with QLoRA. We
    will use the dataset “ultrachat” that I modified for this article. Ultrachat was
    used by Hugging Face to create [Zephyr 7B](https://huggingface.co/HuggingFaceH4/zephyr-7b-alpha).
    We will also see how to quantize Mistral7B with AutoGPTQ.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我将展示如何使用QLoRA微调Mistral 7B。我们将使用我为本文修改的“ultrachat”数据集。Ultrachat被Hugging
    Face用来创建[Zephyr 7B](https://huggingface.co/HuggingFaceH4/zephyr-7b-alpha)。我们还将看看如何使用AutoGPTQ对Mistral7B进行量化。
- en: 'I wrote notebooks implementing all the sections. You can find them here:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 我编写了实现所有部分的笔记本。你可以在这里找到它们：
- en: '[Get the notebooks (#22, #23)](https://kaitchup.substack.com/p/notebooks)'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '[获取笔记本（#22，#23）](https://kaitchup.substack.com/p/notebooks)'
- en: Supervised Fine-Tuning of Mistral 7B with TRL
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用TRL对Mistral 7B进行监督微调
- en: Mistral 7B is a 7 billion parameter model. You roughly need 15 GB of VRAM to
    load it on a GPU. Then, full fine-tuning with batches will consume even more VRAM.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: Mistral 7B是一个拥有70亿参数的模型。你大约需要15 GB的VRAM来加载它到GPU上。然后，使用批量进行全面微调将消耗更多的VRAM。
- en: An alternative to standard full fine-tuning is to fine-tune with QLoRA. QLoRA
    fine-tunes LoRA adapters on top of a frozen quantized model. In previous articles,
    [I have used it to fine-tune Llama 2 7B](https://kaitchup.substack.com/p/fine-tune-llama-2-on-your-computer).
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 标准全面微调的替代方案是使用QLoRA进行微调。QLoRA在冻结的量化模型上微调LoRA适配器。在之前的文章中，[我已经使用它来微调Llama 2 7B](https://kaitchup.substack.com/p/fine-tune-llama-2-on-your-computer)。
- en: Since QLoRA quantizes to 4-bit (NF4), we approximately divide by 4 the memory
    consumption for loading the model, i.e., Mistral 7B quantized with NF4 consumes
    around 4 GB of VRAM. If you have a GPU with 12 GB of VRAM, it leaves a lot of
    space for increasing batch size and targeting more modules with LoRA.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 由于QLoRA将量化到4位（NF4），我们大约将加载模型的内存消耗减少了4倍，即，使用NF4量化的Mistral 7B消耗约4 GB的VRAM。如果你有一块12
    GB VRAM的GPU，这会留出大量空间来增加批量大小并使用LoRA针对更多模块。
- en: In other words, you can fine-tune Mistral 7B for free, on your machine if you
    have enough VRAM, or with the free instance of Google Colab which is equipped
    with a T4 GPU.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，如果你的机器有足够的VRAM，或者使用配备T4 GPU的Google Colab免费实例，你可以免费微调Mistral 7B。
- en: 'To fine-tune with QLoRA, you will need to install the following libraries:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用QLoRA进行微调，你需要安装以下库：
- en: '[PRE0]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Then, import the following:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，导入以下内容：
- en: '[PRE1]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Next, we load the tokenizer and configure it. As with Llama 2, we need to define
    a padding token. As usual, I chose the UNK token to pad the training examples.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们加载并配置标记器。与Llama 2一样，我们需要定义一个填充标记。像往常一样，我选择了UNK标记来填充训练示例。
- en: '*Note: Mistral 7B is fully open. You don’t need to be connected to Hugging
    Face or to sign a license agreement before downloading the model and the tokenizer.*'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '*注意：Mistral 7B完全开放。你无需连接到Hugging Face或在下载模型和标记器之前签署许可协议。*'
- en: '[PRE2]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: For fine-tuning, I made a custom version of [ultrachat](https://huggingface.co/datasets/stingning/ultrachat)
    (MIT license). Ultrachat contains 774k dialogues in the JSON format. This is way
    too many for a cheap fine-tuning and the format is not optimal for fine-tuning
    with TRL on consumer hardware. *Note:* [*TRL*](https://huggingface.co/docs/trl/index)
    *is a library developed by Hugging Face that simplifies fine-tuning of instruct
    LLMs.*
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 对于微调，我制作了一个自定义版本的[ultrachat](https://huggingface.co/datasets/stingning/ultrachat)（MIT许可证）。Ultrachat包含774k个JSON格式的对话。这对便宜的微调来说太多了，而且该格式对于在消费级硬件上使用TRL微调并不理想。*注意：*
    [*TRL*](https://huggingface.co/docs/trl/index) *是Hugging Face开发的一个库，它简化了对指令LLMs的微调。*
- en: 'I randomly subsampled ultrachat to only keep 100k dialogues. Then, I flattened
    the dialogues. For instance, in the original dataset, one example is formatted
    like this:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 我随机从 ultrachat 中抽取了 100k 对话，然后将对话展开。例如，在原始数据集中，一个示例如下：
- en: '[PRE3]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Once flattened, we have everything into one single sequence of tokens with
    “### Human” and “### Assistant” tags:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦展开，我们将所有内容放入一个单一的 token 序列中，并标记为“### Human”和“### Assistant”：
- en: '[PRE4]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'This is the same format as [timdettmers/openassistant-guanaco](https://huggingface.co/datasets/timdettmers/openassistant-guanaco).
    This format can be directly used by TRL. You can find this version of ultrachat
    here:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 这与 [timdettmers/openassistant-guanaco](https://huggingface.co/datasets/timdettmers/openassistant-guanaco)
    的格式相同。这个格式可以直接被 TRL 使用。你可以在这里找到这个版本的 ultrachat：
- en: '[kaitchup/ultrachat-100k-flattened](https://huggingface.co/datasets/kaitchup/ultrachat-100k-flattened)'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[kaitchup/ultrachat-100k-flattened](https://huggingface.co/datasets/kaitchup/ultrachat-100k-flattened)'
- en: 'To load the dataset, we do:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 要加载数据集，我们执行：
- en: '[PRE5]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Now, let’s load the model and prepare it for QLoRA:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们加载模型并为 QLoRA 做准备：
- en: '[PRE6]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Replace “float16” with “bfloat16“ if your GPU supports it. It will make the
    training more stable.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你的 GPU 支持，请将“float16”替换为“bfloat16”。这将使训练更加稳定。
- en: Next, we define the configuration of LoRA.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们定义 LoRA 的配置。
- en: '[PRE7]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: I usually set up r=lora_alpha=16 since I read the Platypus paper.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 我通常将 r=lora_alpha 设为 16，因为我阅读了 Platypus 论文。
- en: '[](https://kaitchup.substack.com/p/platypus-dataset-curation-and-adapters?source=post_page-----631401583f77--------------------------------)
    [## Platypus: Dataset Curation and Adapters for Better Large Language Models'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://kaitchup.substack.com/p/platypus-dataset-curation-and-adapters?source=post_page-----631401583f77--------------------------------)
    [## Platypus: 数据集策划与适配器，用于更好的大型语言模型]'
- en: Achieve low-cost state-of-the-art on your target tasks
  id: totrans-43
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 在你的目标任务上实现低成本的最先进技术
- en: kaitchup.substack.com](https://kaitchup.substack.com/p/platypus-dataset-curation-and-adapters?source=post_page-----631401583f77--------------------------------)
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: kaitchup.substack.com](https://kaitchup.substack.com/p/platypus-dataset-curation-and-adapters?source=post_page-----631401583f77--------------------------------)
- en: I target several modules. Here, I chose all the “proj” modules but there are
    more. You can find them all by running “print(model)”. Note that adding more modules
    may improve the performance but it also adds trainable parameters that will consume
    more VRAM.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 我瞄准了几个模块。这里，我选择了所有“proj”模块，但还有更多。你可以通过运行“print(model)”找到它们。请注意，添加更多模块可能会提升性能，但也会增加可训练参数，从而消耗更多
    VRAM。
- en: 'For training, I chose the following hyperparameters:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 对于训练，我选择了以下超参数：
- en: '[PRE8]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: I trained for only 100 steps. Since the examples are very long, training for
    one epoch would take more than 200 hours using a T4 GPU. If you use a V100 or
    an RTX 40xx, you may reduce it to 100 hours.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 我仅训练了 100 步。由于样本非常长，使用 T4 GPU 训练一个 epoch 需要超过 200 小时。如果你使用 V100 或 RTX 40xx，你可以将时间减少到
    100 小时。
- en: I recommend at least 2,000 training steps to get a reasonably good model. It
    should take around two days of training with a T4 or one day with a more recent
    GPU.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 我推荐至少进行 2,000 次训练步骤，以获得一个相当好的模型。使用 T4 训练大约需要两天时间，使用更新的 GPU 只需一天。
- en: I also commented with “#” the hyperparameters related to the evaluation which
    is costly. Uncomment them to log the validation loss.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 我还用“#”注释了与评估相关的超参数，因为评估成本较高。取消注释以记录验证损失。
- en: 'Start the training with:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 开始训练：
- en: '[PRE9]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Then, once training is done, test the trained adapter by running:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，一旦训练完成，通过运行以下命令测试训练后的适配器：
- en: '[PRE10]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'All this code is implemented in [this notebook #22](https://kaitchup.substack.com/p/notebooks).'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '所有这些代码都实现于 [这个 notebook #22](https://kaitchup.substack.com/p/notebooks)。'
- en: Quantization of Mistral 7B with AutoGPTQ and bitsandbytes NF4
  id: totrans-56
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 AutoGPTQ 和 bitsandbytes NF4 对 Mistral 7B 进行量化
- en: With bitsandbytes NF4
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 bitsandbytes NF4
- en: To keep memory consumption low, we want to run quantized models.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 为了保持内存消耗低，我们希望运行量化模型。
- en: 'If you have fine-tuned your own model with QLoRA and would like to quantize
    it, your best option is to load and quantize Mistral 7B with bitsandbytes nf4
    as we did for QLoRA. Then, load your fine-tuned adapter on top of it. This is
    what we did in the last code sample in the previous section:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你已经用 QLoRA 微调了自己的模型并希望对其进行量化，你最好的选择是加载并量化 Mistral 7B，使用 bitsandbytes nf4，像我们对
    QLoRA 所做的那样。然后，将微调的适配器加载到其上。这是我们在上一节的最后一个代码示例中所做的：
- en: '[PRE11]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: I have explored several other options, for instance merging the adapter to the
    base model, but none of them are optimal.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 我探索了几个其他选项，例如将适配器合并到基础模型中，但没有一个是最优的。
- en: '[](https://kaitchup.substack.com/p/lora-adapters-when-a-naive-merge?source=post_page-----631401583f77--------------------------------)
    [## LoRA Adapters: When a Naive Merge Leads to Poor Performance'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://kaitchup.substack.com/p/lora-adapters-when-a-naive-merge?source=post_page-----631401583f77--------------------------------)
    [## LoRA Adapters: When a Naive Merge Leads to Poor Performance'
- en: The case of LoRA adapters fine-tuned with QLoRA
  id: totrans-63
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: LoRA适配器微调的情况，使用QLoRA
- en: kaitchup.substack.com](https://kaitchup.substack.com/p/lora-adapters-when-a-naive-merge?source=post_page-----631401583f77--------------------------------)
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: kaitchup.substack.com](https://kaitchup.substack.com/p/lora-adapters-when-a-naive-merge?source=post_page-----631401583f77--------------------------------)
- en: We could have used [QA-LoRA to fine-tune a quantization-aware LoRA](https://kaitchup.substack.com/p/qa-lora-quantization-aware-fine-tuning)
    but the framework to do that doesn’t support yet Mistral 7B.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 我们本可以使用[QA-LoRA对量化感知的LoRA进行微调](https://kaitchup.substack.com/p/qa-lora-quantization-aware-fine-tuning)，但该框架目前尚不支持Mistral
    7B。
- en: 'Mistral AI also released an instruct version of Mistral 7B. You can get it
    here instead of training your instruct Mistral 7B:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: Mistral AI还发布了Mistral 7B的指令版。你可以在这里获取，而不是训练自己的指令Mistral 7B：
- en: '[mistralai/Mistral-7B-Instruct-v0.1](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1)'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[mistralai/Mistral-7B-Instruct-v0.1](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1)'
- en: I use this model for the following quantization examples.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 我使用这个模型进行以下量化示例。
- en: 'To load and quantize the model with bitsandbytes, you first need to install
    the following libraries:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 要用bitsandbytes加载并量化模型，你首先需要安装以下库：
- en: '[PRE12]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Then, run:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，运行：
- en: '[PRE13]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'For testing generation, you can use the generate function I defined in the
    previous section with the right prompt format used by Mistral Instruct:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 对于生成测试，你可以使用我在前面部分定义的`generate`函数，配合Mistral Instruct使用的正确提示格式：
- en: '[PRE14]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: With AutoGPTQ
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用AutoGPTQ
- en: bitsandbytes nf4 is fast to quantize but slow for inference. Currently, we can’t
    serialize nf4 models.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: bitsandbytes nf4量化速度快，但推理速度慢。目前，我们无法序列化nf4模型。
- en: AutoGPTQ is much slower for quantization but you only need to do it once. It’s
    also [much faster than nf4 for decoding](https://kaitchup.substack.com/p/gptq-or-bitsandbytes-which-quantization)
    but keep in mind that models quantized with [AutoGPTQ (INT4) are slightly worse
    than models quantized with nf4](https://kaitchup.substack.com/p/quantize-and-fine-tune-llms-with).
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: AutoGPTQ的量化速度较慢，但你只需做一次。它也比[nf4解码速度快](https://kaitchup.substack.com/p/gptq-or-bitsandbytes-which-quantization)，但请注意，用[AutoGPTQ
    (INT4) 量化的模型略逊色于用nf4量化的模型](https://kaitchup.substack.com/p/quantize-and-fine-tune-llms-with)。
- en: 'We need the following libraries:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要以下库：
- en: '[PRE15]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Then, import the following package:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，导入以下包：
- en: '[PRE16]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Then, the model will be quantized when we load it with “AutoModelForCausalLM.from_pretrained”.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，当我们用“AutoModelForCausalLM.from_pretrained”加载模型时，它会被量化。
- en: '[PRE17]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Quantization is very costly in memory. You will need more than 24 GB of VRAM,
    e.g., the A100 of Google Colab Pro would work. I recommend saving the model once
    it’s loaded to make sure you won’t have to do it again.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 量化非常消耗内存。你需要超过24 GB的VRAM，例如，Google Colab Pro的A100会有效。我建议在加载模型后保存它，以确保你不会再需要重复操作。
- en: 'All this quantization code is also available in my [notebook #23](https://kaitchup.substack.com/p/notebooks).'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '所有这些量化代码也可以在我的[笔记本 #23](https://kaitchup.substack.com/p/notebooks)中找到。'
- en: 'Note that TheBloke proposes Mistral Instruct 7B quantized with AutoGPTQ on
    the Hugging Face Hub:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，TheBloke在Hugging Face Hub上提议了使用AutoGPTQ量化的Mistral Instruct 7B：
- en: '[TheBloke/Mistral-7B-Instruct-v0.1-GPTQ](https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-GPTQ)'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[TheBloke/Mistral-7B-Instruct-v0.1-GPTQ](https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-GPTQ)'
- en: Conclusion
  id: totrans-88
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: Mistral 7B is an impressive pre-trained LLM. You can easily fine-tune it on
    your computer with QLoRA. However, fine-tuning remains very time-consuming. As
    we saw, you may need to run it for a hundred hours or more.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: Mistral 7B是一个令人印象深刻的预训练LLM。你可以很容易地在电脑上用QLoRA进行微调。然而，微调依然非常耗时。正如我们所见，你可能需要运行几百小时或更长时间。
- en: If you don’t need to fine-tune Mistral 7B on your data, Mistral Instruct is
    a good alternative proposed by Mistral AI. To run it on consumer hardware, you
    can quantize it with bitsandbytes nf4 or GPTQ.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你不需要在你的数据上微调Mistral 7B，Mistral AI提出的Mistral Instruct是一个不错的替代方案。要在消费级硬件上运行它，你可以使用bitsandbytes
    nf4或GPTQ进行量化。
- en: 'To support my work, consider subscribing to my newsletter:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 支持我的工作，请考虑订阅我的新闻通讯：
- en: '[](https://kaitchup.substack.com/?source=post_page-----631401583f77--------------------------------)
    [## The Kaitchup - AI on a Budget | Benjamin Marie, PhD | Substack'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://kaitchup.substack.com/?source=post_page-----631401583f77--------------------------------)
    [## Kaitchup - AI on a Budget | Benjamin Marie, PhD | Substack'
- en: Weekly news, tips, and tutorials on fine-tuning, running, and serving large
    language models on your computer. Each…
  id: totrans-93
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 每周提供关于在你的计算机上微调、运行和服务大型语言模型的新闻、技巧和教程。每一个……
- en: kaitchup.substack.com](https://kaitchup.substack.com/?source=post_page-----631401583f77--------------------------------)
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '[kaitchup.substack.com](https://kaitchup.substack.com/?source=post_page-----631401583f77--------------------------------)'
