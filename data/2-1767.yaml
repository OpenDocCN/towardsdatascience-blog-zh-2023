- en: Ranking Diamonds with PCA in PySpark
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 PCA 在 PySpark 中对钻石进行排名
- en: 原文：[https://towardsdatascience.com/ranking-diamonds-with-pca-in-pyspark-a59cab7f4f1a](https://towardsdatascience.com/ranking-diamonds-with-pca-in-pyspark-a59cab7f4f1a)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/ranking-diamonds-with-pca-in-pyspark-a59cab7f4f1a](https://towardsdatascience.com/ranking-diamonds-with-pca-in-pyspark-a59cab7f4f1a)
- en: The challenges of running Principal Component Analysis in PySpark
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在 PySpark 中运行主成分分析的挑战
- en: '[](https://gustavorsantos.medium.com/?source=post_page-----a59cab7f4f1a--------------------------------)[![Gustavo
    Santos](../Images/a19a9f4525cdeb6e7a76cd05246aa622.png)](https://gustavorsantos.medium.com/?source=post_page-----a59cab7f4f1a--------------------------------)[](https://towardsdatascience.com/?source=post_page-----a59cab7f4f1a--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----a59cab7f4f1a--------------------------------)
    [Gustavo Santos](https://gustavorsantos.medium.com/?source=post_page-----a59cab7f4f1a--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://gustavorsantos.medium.com/?source=post_page-----a59cab7f4f1a--------------------------------)[![Gustavo
    Santos](../Images/a19a9f4525cdeb6e7a76cd05246aa622.png)](https://gustavorsantos.medium.com/?source=post_page-----a59cab7f4f1a--------------------------------)[](https://towardsdatascience.com/?source=post_page-----a59cab7f4f1a--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----a59cab7f4f1a--------------------------------)
    [Gustavo Santos](https://gustavorsantos.medium.com/?source=post_page-----a59cab7f4f1a--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----a59cab7f4f1a--------------------------------)
    ·8 min read·Dec 22, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----a59cab7f4f1a--------------------------------)
    ·阅读时间 8 分钟·2023年12月22日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/829083190a338cf53e34959cadcae252.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/829083190a338cf53e34959cadcae252.png)'
- en: Photo by [Edgar Soto](https://unsplash.com/@edgardo1987?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)
    on [Unsplash](https://unsplash.com/photos/two-diamond-studded-silver-rings-gb0BZGae1Nk?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 照片由 [Edgar Soto](https://unsplash.com/@edgardo1987?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)
    提供，来源于 [Unsplash](https://unsplash.com/photos/two-diamond-studded-silver-rings-gb0BZGae1Nk?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)
- en: Introduction
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: Here we go for another post about PySpark. I have been enjoying writing about
    this subject, as it feels to me that we are lacking of good blog posts about PySpark,
    especially when we talk about Machine Learning in MLlib — by the way, that is
    Spark’s **M**achine **L**earning **Lib**rary, designed to work with Big Data in
    a parallelized environment.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是另一篇关于 PySpark 的帖子。我很享受写关于这个主题的文章，因为我感觉我们缺乏关于 PySpark 的优质博客，特别是当我们谈论 MLlib
    中的机器学习时——顺便说一下，MLlib 是 Spark 的**M**achine **L**earning **Lib**rary，旨在与大数据在并行环境中一起工作。
- en: I can tell that the Spark Documentation is excellent. It is super organized
    and easy to follow the examples. But working with machine learning in Spark is
    not the most friendly thing to do.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 我可以说 Spark 文档非常出色。它组织得很有条理，示例也很容易跟随。但是，在 Spark 中进行机器学习并不是最友好的事情。
- en: In this post, I work on a PCA model to help me creating a ranking of diamonds,
    and I had to face a couple of challenges that we will go over in the next lines.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我使用 PCA 模型来帮助我创建钻石的排名，并且遇到了一些挑战，我们将在接下来的内容中讨论这些挑战。
- en: I have already [written about PCA before](https://medium.com/towards-data-science/pca-beyond-the-dimensionality-reduction-e352eb0bdf52)
    and how it’s useful for dimensionality reduction, as well as for [creating rankings](https://medium.com/towards-data-science/creating-scores-and-rankings-with-pca-c2c3081fdb26).
    However, this is the first time I do this using Spark, aiming to reproduce the
    technique on a Big Data environment.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 我之前已经 [写过关于 PCA 的文章](https://medium.com/towards-data-science/pca-beyond-the-dimensionality-reduction-e352eb0bdf52)，以及它在降维方面的作用，还有
    [创建排名](https://medium.com/towards-data-science/creating-scores-and-rankings-with-pca-c2c3081fdb26)
    的方法。然而，这是我第一次使用 Spark 进行这个操作，目的是在大数据环境中复现这一技术。
- en: Let’s see the result.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看结果。
- en: Coding
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 编码
- en: Let’s start our coding with the modules to import.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从导入模块开始我们的编码。
- en: '[PRE0]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Dataset
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据集
- en: The dataset to be used in this exercise is the *Diamonds*, from the ggplot2
    package and licensed under Creative Commons 4.0.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 本练习中使用的数据集是 *Diamonds*，来自 ggplot2 包，并且按照 Creative Commons 4.0 许可协议使用。
- en: Here, I am loading it from the Databricks sample datasets and removing two known
    outliers from one of the variables. PCA is affected by outliers. They tend to
    dominate a component due to very large and distorted variance.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我从Databricks示例数据集中加载数据，并从一个变量中删除两个已知的异常值。PCA受到异常值的影响。它们由于非常大的扭曲方差而倾向于主导一个组件。
- en: '[PRE1]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Next, since PCA is a technique to be used for numerical values, I have chosen
    to work with the `carat` , `table` and `depth` variables from the data. I am not
    working with `price` as it dominates completely the variance of the components,
    making the model ineffective.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，由于PCA是一种用于数值值的技术，我选择使用数据中的`carat`、`table`和`depth`变量。我没有使用`price`，因为它完全主导了组件的方差，使得模型失效。
- en: Furthermore, as the scales are different, I am also transforming the data to
    a logarithmic value, so they’re all in the same scale.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，由于尺度不同，我还将数据转换为对数值，以便它们在相同的尺度上。
- en: '[PRE2]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Here’s how it looks.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是它的样子。
- en: '![](../Images/c9f6f8279d56fb62280c8091697ab265.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c9f6f8279d56fb62280c8091697ab265.png)'
- en: Log values of the selected variables. Image by the author.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 选择变量的对数值。图片来源于作者。
- en: The next step is to vectorize our dataset.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是对我们的数据集进行向量化。
- en: Vectorization
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 向量化
- en: One of the challenges we face when working with Big Data in Spark is that the
    machine learning algorithms require that the data is inputted as a vector. This
    could be more straightforward, but unfortunately it adds a couple of steps to
    our code.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们在Spark中处理大数据时面临的挑战之一是机器学习算法要求数据以向量形式输入。这可能更直接，但不幸的是，它给我们的代码增加了几个步骤。
- en: If we had other steps to perform prior to the vectorization, we could create
    a `Pipeline` that can take the *steps* previously coded and run them at once.
    In our case, this step is not necessary, as we only have one step.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们在向量化之前还有其他步骤可以执行，我们可以创建一个`Pipeline`，可以一次运行之前编写的*步骤*。在我们的案例中，这一步是不必要的，因为我们只有一个步骤。
- en: So, next we are creating a `VectorAssembler`. As the name intuitively tells
    us, it will get the numbers and gather them as a single vector by observation,
    putting all of them in a column named `features`.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，接下来我们将创建一个`VectorAssembler`。顾名思义，它会获取数字并将它们汇集成一个单一的向量，通过观察，将所有这些数字放在一个名为`features`的列中。
- en: '[PRE3]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The result will look like this, as follows.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 结果将如下所示。
- en: '![](../Images/d05a7c4344e5a6cb9fcd9d467729696f.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d05a7c4344e5a6cb9fcd9d467729696f.png)'
- en: Vectorized data. Image by the author.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 向量化的数据。图片来源于作者。
- en: PCA
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: PCA
- en: To run the algorithm is fairly simple using MLlib. We instantiate PCA and fit
    the data.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 使用MLlib运行算法非常简单。我们实例化PCA并拟合数据。
- en: '[PRE4]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: If we want to see how is the explained variance per Principal Component after
    we ran PCA, we will need Pandas to make our life easier and be able to quickly
    create a data frame out of the `model.explainedVariance` and then we `insert`
    a new column with the PC names.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想查看PCA运行后每个主成分的解释方差，我们需要Pandas来简化我们的工作，能够快速创建一个数据框架并从`model.explainedVariance`中插入一个新列，列名为PC名称。
- en: '[PRE5]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: In Databricks, it is as simple as clicking a couple of buttons to create this
    plot.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在Databricks中，创建这个图表只需点击几下按钮。
- en: '![](../Images/e2657123b6ded48fb951b31ac504b5a3.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e2657123b6ded48fb951b31ac504b5a3.png)'
- en: PC1 explains 96% of the variance. Image by the author.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: PC1解释了96%的方差。图片来源于作者。
- en: Getting the Transformed Data
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 获取转换后的数据
- en: Given that Spark is designed for Big Data, it lacks a more complete output for
    the Principal Components. Numbers like *eigenvectors, eigenvalues* and *loadings*
    are not easily found as attributes of the PCA model.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于Spark是为大数据设计的，它缺乏对主成分的更完整的输出。像*特征向量、特征值*和*载荷*这样的数字不容易找到作为PCA模型的属性。
- en: 'All we get as output is the explained variance by PC and the table with the
    transformed data. Those numbers can give us the information of: (1) direction,
    being positive for those data points varying in the same direction of the PC and
    negative when their variation is to the opposite direction; (2) value, where the
    higher it means that it is varies more in that component.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到的输出只有PC解释的方差和转换后数据的表。这些数字可以给我们以下信息：（1）方向，对于那些在PC相同方向上变化的数据点为正值，而当其变化方向相反时为负值；（2）值，数值越高，表示在该组件中变化越大。
- en: This is the code snippet to get the transformed data.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 这是获取转换后数据的代码片段。
- en: '[PRE6]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: And the resulting vectors.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 以及生成的向量。
- en: '![](../Images/a174bff70fa7f982249040fe289c085b.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a174bff70fa7f982249040fe289c085b.png)'
- en: PCA results. Image by the author.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: PCA结果。图片来源于作者。
- en: 'Look how the observation #5 is the one with the most variance in the opposite
    direction of PC1, as well as the most variance in PC3.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 看看观察值#5是与PC1方向相反的方差最大，同时也是PC3中方差最大的。
- en: These numbers can give us the base to rank these observations. Since we know
    how much they are varying in each component and how much variance is explained
    by component, we can quickly create a score by multiplying the values in each
    PC by the respective explained variance.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 这些数字可以为我们提供排名观察值的基础。由于我们知道它们在每个组件中的变异程度以及每个组件解释的方差，我们可以通过将每个PC中的值乘以相应的解释方差来快速创建分数。
- en: Let’s see how.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看怎么做。
- en: Cleaning the Transformed Data
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 清理变换后的数据
- en: Here, there is another challenge. Those vectors are not very manipulation friendly.
    So we must workaround the problem.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 这里还有另一个挑战。这些向量并不太容易操作。所以我们必须绕过这个问题。
- en: First, let’s just collect the transformed values.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们只需收集变换后的值。
- en: '[PRE7]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Good. But they don’t come ready for use. Each row is still a `DenseVector`.
    A solution I found to deal with that was casting the temporary dataset to string,
    so I can split that into 3 separate columns.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 好的。但这些数据还不能直接使用。每行仍然是一个`DenseVector`。我找到的处理方法是将临时数据集转换为字符串，以便将其拆分成3列。
- en: '[PRE8]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Since the values are now strings, the `[]` must be removed from the first and
    last characters. Using the `split` function, we can break the lines by the `,`
    as our separator. To remove the square brackets, we just slice the first character
    in the PC1 and the last from PC3\. All the columns are `cast('float')` to make
    it numerical again.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 由于值现在是字符串，因此必须去除首尾的`[]`。使用`split`函数，我们可以用`,`作为分隔符拆分行。要去除方括号，只需切除PC1的第一个字符和PC3的最后一个字符。所有列都`cast('float')`以再次转换为数值。
- en: '[PRE9]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '*Voilà*. We have our data ready for the ranking.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '*瞧*。我们的数据准备好了用于排名。'
- en: '![](../Images/ffb11eede93dd3ff4f2bf8e38f31f6ed.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ffb11eede93dd3ff4f2bf8e38f31f6ed.png)'
- en: Clean transformed data. Image by the author.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 清理后的数据。图像由作者提供。
- en: Calculating Scores
  id: totrans-66
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 计算分数
- en: The scores can be calculated by multiplying the transformed data from each PC
    by the respective explained variance and adding all together.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 分数可以通过将每个PC的变换数据乘以相应的解释方差然后相加来计算。
- en: PC1 * Explained Var PC1 +
  id: totrans-68
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: PC1 * 解释的方差 PC1 +
- en: ''
  id: totrans-69
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: PC2 * Explained Var PC2 +
  id: totrans-70
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: PC2 * 解释的方差 PC2 +
- en: ''
  id: totrans-71
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: PC3 * Explained Var PC3
  id: totrans-72
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: PC3 * 解释的方差 PC3
- en: Then, we will create a column with an index, using `row_number` and `Window`,
    so we can join the results to the original data. And finally a column with a `dense_rank`,
    which is our ranking of diamonds.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将创建一个带有索引的列，使用`row_number`和`Window`，以便将结果与原始数据连接。最后创建一个`dense_rank`列，这就是我们的钻石排名。
- en: '[PRE10]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Let’s display.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们展示一下。
- en: '![](../Images/e854679e57592f8c1bcdb35f6977bf38.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e854679e57592f8c1bcdb35f6977bf38.png)'
- en: Top 10 in the ranking. Image by the author.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 排名前10名。图像由作者提供。
- en: 'The previous table is the rank of diamonds based on the combination of `carat`
    , `table` and `depth` variables. It looks pretty good. We can see, at least in
    these top 10 results, that even though the categorical columns were not considered,
    we still see better colors and clarity better ranked. The colors go from D (best)
    to J (worst) and the clarity follows this sequence: (I1 (worst), SI2, SI1, VS2,
    VS1, VVS2, VVS1, IF (best)).'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的表格是基于`carat`、`table`和`depth`变量的钻石排名。效果相当不错。我们可以看到，至少在这前10个结果中，即使没有考虑分类列，我们仍然看到更好的颜色和更高的清晰度排名。颜色从D（最好）到J（最差），清晰度的顺序是：（I1（最差）、SI2、SI1、VS2、VS1、VVS2、VVS1、IF（最好））。
- en: 'For example: rank 1 has a better color than 2, but smaller carat and worse
    clarity. Rank 5 has a better color and better clarity than 6 and 7, with a better
    price, but smaller carat. Certainly the `depth, table` are getting a role in those.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 例如：排名1的颜色比排名2好，但克拉更小且清晰度较差。排名5的颜色和清晰度都比排名6和7好，价格更好，但克拉较小。`depth`、`table`的确在其中起了作用。
- en: Before You Go
  id: totrans-80
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 离开前
- en: We have reached the end of this post. I found a couple of challenges performing
    PCA in Spark. It is not one of the most friendly methods I worked with.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经结束了这篇文章。我在Spark中执行PCA时发现了一些挑战。这不是我工作过的最友好的方法之一。
- en: I believe that there is a lot missing, way far from statistical tools like R,
    where you can get a much superior implementation of the algorithm. On the other
    hand, Spark is not meant to be a statistical tool. It is a Big Data tool initially
    created for ETL and that has been growing and adding more functionalities.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 我认为目前的功能还远不如统计工具如R，后者能提供更优的算法实现。另一方面，Spark并不是为了做统计分析而设计的。它最初是一个为ETL创建的大数据工具，后来不断发展并添加了更多功能。
- en: Furthermore, PCA can be used for dimensionality reduction when we change the
    `k` value of components to extract. For that use with Big Data, it can be a helper
    to reduce the dimensions of the dataset and serve as input for other steps in
    an analysis.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，PCA可以用于降维，当我们更改`k`值来提取组件时。对于大数据应用，它可以作为一个辅助工具来减少数据集的维度，并作为分析中其他步骤的输入。
- en: '[PRE11]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: If you liked this content, follow my blog for more.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你喜欢这些内容，请关注我的博客获取更多信息。
- en: '[](https://gustavorsantos.medium.com/?source=post_page-----a59cab7f4f1a--------------------------------)
    [## Gustavo Santos - Medium'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://gustavorsantos.medium.com/?source=post_page-----a59cab7f4f1a--------------------------------)
    [## Gustavo Santos - Medium'
- en: Read writing from Gustavo Santos on Medium. Data Scientist. I extract insights
    from data to help people and companies…
  id: totrans-87
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 阅读Gustavo Santos在Medium上的文章。数据科学家。我从数据中提取洞察，以帮助个人和公司……
- en: gustavorsantos.medium.com](https://gustavorsantos.medium.com/?source=post_page-----a59cab7f4f1a--------------------------------)
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: gustavorsantos.medium.com](https://gustavorsantos.medium.com/?source=post_page-----a59cab7f4f1a--------------------------------)
- en: Also, find me on [LinkedIn](https://www.linkedin.com/in/gurezende/).
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以在[LinkedIn](https://www.linkedin.com/in/gurezende/)找到我。
- en: 'Complete code in GitHub:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: GitHub上的完整代码：
- en: '[](https://github.com/gurezende/Studying/tree/master/PySpark/PCA?source=post_page-----a59cab7f4f1a--------------------------------)
    [## Studying/PySpark/PCA at master · gurezende/Studying'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://github.com/gurezende/Studying/tree/master/PySpark/PCA?source=post_page-----a59cab7f4f1a--------------------------------)
    [## Studying/PySpark/PCA at master · gurezende/Studying'
- en: This is a repository with my tests and studies of new packages - Studying/PySpark/PCA
    at master · gurezende/Studying
  id: totrans-92
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 这是一个包含我测试和研究新包的代码库 - Studying/PySpark/PCA at master · gurezende/Studying
- en: github.com](https://github.com/gurezende/Studying/tree/master/PySpark/PCA?source=post_page-----a59cab7f4f1a--------------------------------)
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: github.com](https://github.com/gurezende/Studying/tree/master/PySpark/PCA?source=post_page-----a59cab7f4f1a--------------------------------)
- en: Interested in learning more about PySpark?
  id: totrans-94
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 有兴趣了解更多关于PySpark的内容吗？
- en: '![](../Images/dce6d84bd912c6135a9ce6930bc033b7.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/dce6d84bd912c6135a9ce6930bc033b7.png)'
- en: '[Enroll here: Master Data Wrangling With PySpark](https://www.udemy.com/course/master-data-processing-pyspark/?couponCode=WEBINAR70).
    Image by the author.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '[点击这里注册：Master Data Wrangling With PySpark](https://www.udemy.com/course/master-data-processing-pyspark/?couponCode=WEBINAR70)。图片由作者提供。'
- en: 'Here is a link to my course with a discount coupon applied: [**Mater Data Wrangling
    With PySpark**](https://www.udemy.com/course/master-data-processing-pyspark/?couponCode=WEBINAR70)
    **in Udemy.**'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我的课程链接，已应用折扣优惠券：[**Mater Data Wrangling With PySpark**](https://www.udemy.com/course/master-data-processing-pyspark/?couponCode=WEBINAR70)
    **在Udemy上。**
- en: Reference
  id: totrans-98
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考资料
- en: '[](/creating-scores-and-rankings-with-pca-c2c3081fdb26?source=post_page-----a59cab7f4f1a--------------------------------)
    [## Creating Scores and Rankings with PCA'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/creating-scores-and-rankings-with-pca-c2c3081fdb26?source=post_page-----a59cab7f4f1a--------------------------------)
    [## Creating Scores and Rankings with PCA'
- en: Use R Language to create scores for observations based on many variables
  id: totrans-100
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用R语言根据多个变量为观察值创建评分
- en: 'towardsdatascience.com](/creating-scores-and-rankings-with-pca-c2c3081fdb26?source=post_page-----a59cab7f4f1a--------------------------------)
    [](/pca-beyond-the-dimensionality-reduction-e352eb0bdf52?source=post_page-----a59cab7f4f1a--------------------------------)
    [## PCA: Beyond Dimensionality Reduction'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 'towardsdatascience.com](/creating-scores-and-rankings-with-pca-c2c3081fdb26?source=post_page-----a59cab7f4f1a--------------------------------)
    [](/pca-beyond-the-dimensionality-reduction-e352eb0bdf52?source=post_page-----a59cab7f4f1a--------------------------------)
    [## PCA: Beyond Dimensionality Reduction'
- en: Learn how to use PCA algorithm to find variables that vary together
  id: totrans-102
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 学习如何使用PCA算法来找到共同变化的变量
- en: towardsdatascience.com](/pca-beyond-the-dimensionality-reduction-e352eb0bdf52?source=post_page-----a59cab7f4f1a--------------------------------)  [##
    PCA - PySpark 3.5.0 documentation
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/pca-beyond-the-dimensionality-reduction-e352eb0bdf52?source=post_page-----a59cab7f4f1a--------------------------------)  [##
    PCA - PySpark 3.5.0 documentation
- en: pyspark.sql.SparkSession.builder.getOrCreate
  id: totrans-104
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: pyspark.sql.SparkSession.builder.getOrCreate
- en: spark.apache.org](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.feature.PCA.html?source=post_page-----a59cab7f4f1a--------------------------------)
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '[spark.apache.org](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.feature.PCA.html?source=post_page-----a59cab7f4f1a--------------------------------)'
