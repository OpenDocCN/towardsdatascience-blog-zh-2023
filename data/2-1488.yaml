- en: 'Mastering Long Short-Term Memory with Python: Unleashing the Power of LSTM
    in NLP'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 精通Python中的长短期记忆：释放LSTM在NLP中的力量
- en: 原文：[https://towardsdatascience.com/mastering-long-short-term-memory-with-python-unleashing-the-power-of-lstm-in-nlp-381ec3430f50](https://towardsdatascience.com/mastering-long-short-term-memory-with-python-unleashing-the-power-of-lstm-in-nlp-381ec3430f50)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/mastering-long-short-term-memory-with-python-unleashing-the-power-of-lstm-in-nlp-381ec3430f50](https://towardsdatascience.com/mastering-long-short-term-memory-with-python-unleashing-the-power-of-lstm-in-nlp-381ec3430f50)
- en: A comprehensive guide to understanding and implementing LSTM layers for natural
    language processing with Python
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 一本关于理解和实现LSTM层用于Python自然语言处理的全面指南
- en: '[](https://eligijus-bujokas.medium.com/?source=post_page-----381ec3430f50--------------------------------)[![Eligijus
    Bujokas](../Images/061fd30136caea2ba927140e8b3fae3c.png)](https://eligijus-bujokas.medium.com/?source=post_page-----381ec3430f50--------------------------------)[](https://towardsdatascience.com/?source=post_page-----381ec3430f50--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----381ec3430f50--------------------------------)
    [Eligijus Bujokas](https://eligijus-bujokas.medium.com/?source=post_page-----381ec3430f50--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://eligijus-bujokas.medium.com/?source=post_page-----381ec3430f50--------------------------------)[![Eligijus
    Bujokas](../Images/061fd30136caea2ba927140e8b3fae3c.png)](https://eligijus-bujokas.medium.com/?source=post_page-----381ec3430f50--------------------------------)[](https://towardsdatascience.com/?source=post_page-----381ec3430f50--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----381ec3430f50--------------------------------)
    [Eligijus Bujokas](https://eligijus-bujokas.medium.com/?source=post_page-----381ec3430f50--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----381ec3430f50--------------------------------)
    ·17 min read·Nov 28, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于[Towards Data Science](https://towardsdatascience.com/?source=post_page-----381ec3430f50--------------------------------)
    ·17分钟阅读·2023年11月28日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/8590a68892231bb5a03f41691fc9f697.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8590a68892231bb5a03f41691fc9f697.png)'
- en: Photo by [Sven Brandsma](https://unsplash.com/@seffen99?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[Sven Brandsma](https://unsplash.com/@seffen99?utm_source=medium&utm_medium=referral)拍摄的照片，来自[Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)。'
- en: This work is a continuation of my [article about RNNs and NLP with Python](https://medium.com/towards-data-science/mastering-nlp-in-depth-python-coding-for-deep-learning-models-a15055e989bf).
    A natural progression of a deep learning network with a simple recurrent layer
    is a deep learning network with a **L**ong **S**hort **T**erm **M**emory (**LSTM**
    for short) layer.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 这项工作是我[关于RNN和Python NLP的文章](https://medium.com/towards-data-science/mastering-nlp-in-depth-python-coding-for-deep-learning-models-a15055e989bf)的延续。一个简单的递归层的深度学习网络的自然发展是带有**长短期记忆**（**LSTM**）层的深度学习网络。
- en: As with the RNN and NLP, I will try to explain the LSTM layer in great detail
    and code the forward pass of the layer from scratch.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 与RNN和NLP一样，我将尝试详细解释LSTM层，并从头编写该层的前向传播代码。
- en: 'All the codes can be viewed here: [https://github.com/Eligijus112/NLP-python](https://github.com/Eligijus112/NLP-python)'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '所有代码可以在这里查看: [https://github.com/Eligijus112/NLP-python](https://github.com/Eligijus112/NLP-python)'
- en: 'We will work with the same dataset¹ as in the previous article:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用与上一篇文章相同的¹数据集：
- en: '[PRE0]'
  id: totrans-12
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '![](../Images/bcb705d18dcb689363a428e44c88f127.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bcb705d18dcb689363a428e44c88f127.png)'
- en: Random rows from the dataset; Picture by author
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集中的随机行；作者拍摄的图片
- en: Remember, that SENTIMENT=1 is a negative sentiment, and SENTIMENT=0 is a positive
    sentiment.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，SENTIMENT=1表示负面情感，SENTIMENT=0表示正面情感。
- en: We need to convert the text data into a sequence of integers. Unlike in the
    previous article though, we will now create a sequence not of words but of individual
    characters.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要将文本数据转换为整数序列。不过，与上一篇文章不同的是，我们现在将创建一个字符序列，而不是单词序列。
- en: 'For example, the text “*Nice Game*” could be converted to the following example
    vector:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，文本“*Nice Game*”可以转换为以下示例向量：
- en: '*[1, 2, 3, 4, 5, 6, 7, 8, 3]*'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '*[1, 2, 3, 4, 5, 6, 7, 8, 3]*'
- en: Each individual character, including whitespaces and punctuations, will have
    an index.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 每个字符，包括空格和标点符号，都会有一个索引。
- en: '[PRE1]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Let us split our data into a train-test split and apply our created function:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将数据拆分为训练集和测试集，并应用我们创建的函数：
- en: '[PRE2]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'There are 274 unique characters in our data. Let us print the top 10 entries
    in our **word2idx** dictionary:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的数据中有274个独特的字符。让我们打印**word2idx**字典中的前10项：
- en: '[PRE3]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Let us convert the texts to sequences:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将文本转换为序列：
- en: '[PRE4]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: To recall, splitting the texts by word level led to the mean length of a sequence
    being equal to **~22** tokens. Now, we have sequences of length **~103** tokens.
    The standard deviation is very high, thus we will use the max sequence length
    of **200** in padding.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 回顾一下，按词级别拆分文本导致序列的平均长度为**~22** 个令牌。现在，我们有长度为**~103** 个令牌的序列。标准差非常高，因此我们将使用最大序列长度为**200**
    进行填充。
- en: '[PRE5]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The train and val datasets thus far look like the following:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，训练集和验证集的数据集如下：
- en: '![](../Images/895432fe15cb4e3a0d476637def6883a.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/895432fe15cb4e3a0d476637def6883a.png)'
- en: A snippet of data; Photo by author
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 一段数据；作者拍摄的照片
- en: 'Why should we switch from a vanilla RNN to an LSTM network? The problems are
    twofold:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么我们应该从普通的 RNN 切换到 LSTM 网络？问题有两个方面：
- en: A simple RNN has the so-called **vanishing gradient problem²** or the **exploding
    gradient problem** associated with the weights used in the ***for*** loop of the
    network.
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个简单的 RNN 有所谓的**消失梯度问题²**或**爆炸梯度问题**，与网络中***for*** 循环使用的权重相关。
- en: The network tends to **“forget”** the initial steps input of a long sequence
    of data.
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 网络往往会**“忘记”**长序列数据的初始步骤输入。
- en: 'To illustrate the ***forgetness***, consider the example:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明***遗忘***，请考虑以下示例：
- en: 'In our data, on average, there are 103 timesteps (the number of tokens in a
    text going from left to right). Recall the graph from the RNN article:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的数据中，平均而言，有 103 个时间步（文本中从左到右的令牌数量）。回顾 RNN 文章中的图表：
- en: '![](../Images/3de05ae2d60ac52ad02c3a95ad79fc0e.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3de05ae2d60ac52ad02c3a95ad79fc0e.png)'
- en: Unrolled n step RNN; Picture by author
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 展开 n 步 RNN；作者拍摄的照片
- en: We have the same weight **W** that we multiply the output of the ReLU layer
    with. Then, we add that signal to the next time step, and so on. If we choose
    a relatively small value for **W** (let us say 0.5) and we have 103 steps of time
    series data, the impact from the first timestep input to the final output would
    be, roughly speaking, **0.5¹⁰³ * input1** which is approximately zero.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有相同的权重**W**，用来乘以 ReLU 层的输出。然后，我们将该信号添加到下一个时间步，依此类推。如果我们为**W**选择一个相对较小的值（例如
    0.5），并且我们有 103 步时间序列数据，从第一次时间步输入到最终输出的影响大致为**0.5¹⁰³ * input1**，这大致等于零。
- en: The signal from the second input would be **0.5¹⁰² * input2** and so on.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 第二次输入的信号将是**0.5¹⁰² * input2**，以此类推。
- en: One can see, that the more timesteps we add, the less information is left to
    the final output from the initial time steps.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 可以看出，随着时间步的增加，初始时间步的信息在最终输出中的占比越来越少。
- en: To battle this problem of forgetting the past, great minds have come up with
    an LSTM layer³ for use in time series problems.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 为了应对遗忘过去的问题，伟大的思想家们提出了用于时间序列问题的 LSTM 层³。
- en: 'Internally, an LSTM layer uses two activation functions:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 从内部来看，LSTM 层使用两个激活函数：
- en: Sigmoid function
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sigmoid 函数
- en: Tanh function
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tanh 函数
- en: 'Key facts to remember about these functions are:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 关于这些函数要记住的关键事实是：
- en: The **sigmoid** activation function takes in any value on a real number plane
    and outputs a value between **0 and 1**.
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**sigmoid** 激活函数接受实数平面上的任何值，并输出一个**在 0 和 1 之间**的值。'
- en: The **tanh** function takes in any value on a real number plane and outputs
    a value **between -1 and 1.**
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**tanh** 函数接受实数平面上的任何值，并输出一个**在 -1 和 1 之间**的值。'
- en: '[PRE6]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Now that we have the sigmoid and tanh activation functions in our minds, let
    us return to the LSTM layer.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们已经了解了 sigmoid 和 tanh 激活函数，让我们回到 LSTM 层。
- en: 'The LSTM layer is made up of 2 parts (hence the name):'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: LSTM 层由 2 部分组成（因此得名）：
- en: Long-term memory block
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 长期记忆块
- en: Short-term memory block
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 短期记忆块
- en: 'At every time step (or token step), the LSTM layer outputs two predictions,
    the **long-term prediction** and the **short-term prediction**. A high-level diagram
    of an LSTM unit can be visualized like this:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在每个时间步（或令牌步），LSTM 层输出两个预测：**长期预测**和**短期预测**。LSTM 单元的高层次图示可以这样可视化：
- en: '![](../Images/756d64f334e9b754acda7b6b6a6b85db.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/756d64f334e9b754acda7b6b6a6b85db.png)'
- en: Unrolled simple LSTM network; Graph by author
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 展开的简单 LSTM 网络；作者拍摄的图表
- en: At each time step, the LSTM layer outputs a number and this is what we call
    the **short-term memory output.** It is usually just a scalar. Additionally, the
    **long-term memory** scalar is also calculated in the LSTM layer but it is not
    output and transferred to the second step in the sequence. It is very important
    to note that at each time step, both the short-term and the long-term memories
    are updated.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在每个时间步骤，LSTM 层输出一个数字，这就是我们所称的**短期记忆输出**。它通常只是一个标量。此外，**长期记忆**标量也在 LSTM 层中计算，但它不被输出并传递到序列的第二步。值得注意的是，在每个时间步骤中，短期和长期记忆都会被更新。
- en: 'Now let us dive deep into the LSTM layer. The first part of an LSTM layer is
    the so-called **Forget Gate** operation:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们深入探讨 LSTM 层。LSTM 层的第一部分是所谓的**忘记门**操作：
- en: '![](../Images/04fd92b4546ccff1103dbc14fec6b8c5.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/04fd92b4546ccff1103dbc14fec6b8c5.png)'
- en: Forget gate; Graph by author
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 忘记门；图表由作者提供
- en: The forget gate gets its name from the fact that we **calculate the percentage
    of the long-term memory that we want to keep.** This is due to the fact that the
    sigmoid activation function will output a number between 0 and 1 and we will multiply
    that number by the long-term memory and pass it along the network.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 忘记门得名于我们**计算希望保留的长期记忆百分比**。这是因为 sigmoid 激活函数会输出一个介于 0 和 1 之间的数字，我们将这个数字乘以长期记忆并传递到网络中。
- en: 'We can start to see the weights that will be updated at training time: **w1,
    w2,** and **b1**. These weights directly influence the amount of long-term memory
    to keep.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以开始看到在训练时会更新的权重：**w1, w2** 和 **b1**。这些权重直接影响保持的长期记忆量。
- en: Note that at this step, the short-term memory is not adjusted and gets passed
    along to the second steps of the network.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在这个步骤中，短期记忆没有调整，而是传递到网络的第二步。
- en: '[PRE7]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '[PRE8]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Next up in the LSTM layer is the input gate:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来在 LSTM 层的是输入门：
- en: '![](../Images/e4e654110995642ce727a2ad2d43c203.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e4e654110995642ce727a2ad2d43c203.png)'
- en: Input gate; Graph by author
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 输入门；图表由作者提供
- en: The input gate only adjusts the long-term memory part of the LSTM network, but
    in order to do that, it uses the current input and the current short-term memory
    values.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 输入门仅调整 LSTM 网络的长期记忆部分，但为此，它使用当前输入和当前短期记忆值。
- en: 'Looking at the graph, just before the multiplication step, we have two outputs:
    one from the sigmoid activation function and another from the tanh activation
    layer. Loosely speaking, the sigmoid layer outputs the percentage of the memory
    to remember (0, 1) and the tanh outputs the potential memory to remember (-1,
    1).'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 从图表来看，在乘法步骤之前，我们有两个输出：一个来自 sigmoid 激活函数，另一个来自 tanh 激活层。宽泛地说，sigmoid 层输出要记住的记忆百分比（0，1），而
    tanh 输出记住的潜在记忆（-1，1）。
- en: We then sum up the current long-term memory, which was a bit adjusted in the
    forget gate, with the input gate output.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们将当前长期记忆（在忘记门中稍作调整）与输入门输出相加。
- en: '[PRE9]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '[PRE10]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: As we can see from the code snippet above, the only thing that has changed is
    the long-term memory.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 从上面的代码片段可以看出，唯一变化的是长期记忆。
- en: 'The last piece of the LSTM layer is the **output gate**. The output gate is
    the step where we will adjust the **short-term** memory of the layer:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: LSTM 层的最后一部分是**输出门**。输出门是我们将调整**短期**记忆的步骤：
- en: '![](../Images/7edebcc737e85b30b2762841029d4a03.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7edebcc737e85b30b2762841029d4a03.png)'
- en: Output gate; Graph by author
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 输出门；图表由作者提供
- en: 'The logic is very similar to the logic that was present in the previous gates:
    the sigmoid activation calculates the percentage of memory to keep and the tanh
    function calculates the overall signal.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑与之前门的逻辑非常相似：sigmoid 激活函数计算要保留的记忆百分比，而 tanh 函数计算总体信号。
- en: '[PRE11]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '[PRE12]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: As we can see, the output gate only adjusted the short-term memory scalar.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，输出门仅调整了短期记忆标量。
- en: '![](../Images/577260ddbca493db6a1fcf95b06d7a75.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/577260ddbca493db6a1fcf95b06d7a75.png)'
- en: LSTM layer; Graph by author
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: LSTM 层；图表由作者提供
- en: The above graph shows the forget, input, and output gates on one graph⁴.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 上图显示了忘记门、输入门和输出门的合成图⁴。
- en: 'When we have an input sequence of x variables, the inner loop when using the
    LSTM layer is this:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们有一个 x 变量的输入序列时，使用 LSTM 层的内部循环是这样的：
- en: Initiate randomly short-term and long-term memory.
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 随机初始化短期和长期记忆。
- en: '2\. For each **x1** to **xn**:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 2. 对于每个**x1**到**xn**：
- en: 2.1 Forward propagate through the LSTM layer.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 2.1 通过 LSTM 层向前传播。
- en: 2.2 Output the short-term memory
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 2.2 输出短期记忆
- en: 2.3 Save the long-term and short-term memories to the layer.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 将长期和短期记忆保存到层中。
- en: Let us wrap every gate to a class and create a Python example.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将每个门封装到一个类中，并创建一个 Python 示例。
- en: '[PRE13]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '[PRE14]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Now to wrap everything in a nice pytorch example with the LSTM layer. The syntax
    is very similar to a basic RNN model:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将所有内容包裹在一个漂亮的 pytorch 示例中，使用 LSTM 层。语法与基本的 RNN 模型非常相似：
- en: '[PRE15]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '[PRE16]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '[PRE17]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '[PRE18]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: This article went into the nitty gritty details about the inner workings of
    an LSTM cell. Some implementations of the LSTM layer may differ from the one presented
    here, but the overall parts of long-term and short-term memory are present throughout
    the vast majority of the implementations.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 这篇文章深入探讨了 LSTM 单元的内部工作细节。某些 LSTM 层的实现可能与这里展示的有所不同，但长期和短期记忆的整体部分在绝大多数实现中都存在。
- en: I hope the reader now has a better understanding of the LSTM layers and I hope
    he or she will start implementing it into their pipeline right away!
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 我希望读者现在对 LSTM 层有了更好的理解，并希望他们能够立即将其实施到他们的工作流程中！
- en: Special shoutout to the wonderful explainer video by StatQuest⁵.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 特别感谢 StatQuest 提供的精彩讲解视频⁵。
- en: '[1]'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '[1]'
- en: '**Name:** Twitter Sentiment Analysis'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '**名称：** 推特情感分析'
- en: '**URL:** [https://www.kaggle.com/datasets/jp797498e/twitter-entity-sentiment-analysis](https://www.kaggle.com/datasets/jp797498e/twitter-entity-sentiment-analysis)'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '**网址：** [https://www.kaggle.com/datasets/jp797498e/twitter-entity-sentiment-analysis](https://www.kaggle.com/datasets/jp797498e/twitter-entity-sentiment-analysis)'
- en: '**Dataset Licence:** [https://creativecommons.org/publicdomain/zero/1.0/](https://creativecommons.org/publicdomain/zero/1.0/)'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '**数据集许可证：** [https://creativecommons.org/publicdomain/zero/1.0/](https://creativecommons.org/publicdomain/zero/1.0/)'
- en: '[2]'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '[2]'
- en: '**Name:** Vanishing Gradient Problem'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '**名称：** 梯度消失问题'
- en: '**URL:** [https://k21academy.com/datascience-blog/machine-learning/recurrent-neural-networks/#:~:text=Two%20Issues%20of%20Standard%20RNNs&text=RNNs%20suffer%20from%20the%20matter,of%20long%20data%20sequences%20difficult](https://k21academy.com/datascience-blog/machine-learning/recurrent-neural-networks/#:~:text=Two%20Issues%20of%20Standard%20RNNs&text=RNNs%20suffer%20from%20the%20matter,of%20long%20data%20sequences%20difficult).'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '**网址：** [https://k21academy.com/datascience-blog/machine-learning/recurrent-neural-networks/#:~:text=Two%20Issues%20of%20Standard%20RNNs&text=RNNs%20suffer%20from%20the%20matter,of%20long%20data%20sequences%20difficult](https://k21academy.com/datascience-blog/machine-learning/recurrent-neural-networks/#:~:text=Two%20Issues%20of%20Standard%20RNNs&text=RNNs%20suffer%20from%20the%20matter,of%20long%20data%20sequences%20difficult)'
- en: '[3]'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '[3]'
- en: '**Name:** LONG SHORT-TERM MEMORY'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '**名称：** 长短期记忆'
- en: '**URL:** [https://www.bioinf.jku.at/publications/older/2604.pdf](https://www.bioinf.jku.at/publications/older/2604.pdf)'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '**网址：** [https://www.bioinf.jku.at/publications/older/2604.pdf](https://www.bioinf.jku.at/publications/older/2604.pdf)'
- en: '**Year:** 1997'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '**年份：** 1997'
- en: '[4]'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '[4]'
- en: '**Name:** Understanding LSTM Networks'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '**名称：** 理解 LSTM 网络'
- en: '**URL:** [https://colah.github.io/posts/2015-08-Understanding-LSTMs/](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '**网址：** [https://colah.github.io/posts/2015-08-Understanding-LSTMs/](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)'
- en: '**Year:** 2015'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '**年份：** 2015'
- en: '[5]'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '[5]'
- en: '**Name:** Long Short-Term Memory (LSTM), Clearly Explained'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '**名称：** 长短期记忆（LSTM），清晰解释'
- en: '**URL:** [https://www.youtube.com/watch?v=YCzL96nL7j0](https://www.youtube.com/watch?v=YCzL96nL7j0&t=358s)'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '**网址：** [https://www.youtube.com/watch?v=YCzL96nL7j0](https://www.youtube.com/watch?v=YCzL96nL7j0&t=358s)'
- en: '**Year:** 2022'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '**年份：** 2022'
