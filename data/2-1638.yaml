- en: 'PatchTST: A Breakthrough in Time Series Forecasting'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: PatchTSTï¼šæ—¶é—´åºåˆ—é¢„æµ‹ä¸­çš„çªç ´
- en: åŸæ–‡ï¼š[https://towardsdatascience.com/patchtst-a-breakthrough-in-time-series-forecasting-e02d48869ccc](https://towardsdatascience.com/patchtst-a-breakthrough-in-time-series-forecasting-e02d48869ccc)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡ï¼š[https://towardsdatascience.com/patchtst-a-breakthrough-in-time-series-forecasting-e02d48869ccc](https://towardsdatascience.com/patchtst-a-breakthrough-in-time-series-forecasting-e02d48869ccc)
- en: From theory to practice, understand the PatchTST algorithm and apply it in Python
    alongside N-BEATS and N-HiTS
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä»ç†è®ºåˆ°å®è·µï¼Œäº†è§£ PatchTST ç®—æ³•ï¼Œå¹¶åœ¨ Python ä¸­ä¸ N-BEATS å’Œ N-HiTS ä¸€èµ·åº”ç”¨ã€‚
- en: '[](https://medium.com/@marcopeixeiro?source=post_page-----e02d48869ccc--------------------------------)[![Marco
    Peixeiro](../Images/7cf0a81d87281d35ff47f51e3026a3e9.png)](https://medium.com/@marcopeixeiro?source=post_page-----e02d48869ccc--------------------------------)[](https://towardsdatascience.com/?source=post_page-----e02d48869ccc--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----e02d48869ccc--------------------------------)
    [Marco Peixeiro](https://medium.com/@marcopeixeiro?source=post_page-----e02d48869ccc--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@marcopeixeiro?source=post_page-----e02d48869ccc--------------------------------)[![Marco
    Peixeiro](../Images/7cf0a81d87281d35ff47f51e3026a3e9.png)](https://medium.com/@marcopeixeiro?source=post_page-----e02d48869ccc--------------------------------)[](https://towardsdatascience.com/?source=post_page-----e02d48869ccc--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----e02d48869ccc--------------------------------)
    [Marco Peixeiro](https://medium.com/@marcopeixeiro?source=post_page-----e02d48869ccc--------------------------------)'
- en: Â·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----e02d48869ccc--------------------------------)
    Â·10 min readÂ·Jun 20, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: Â·å‘è¡¨äº [Towards Data Science](https://towardsdatascience.com/?source=post_page-----e02d48869ccc--------------------------------)
    Â·é˜…è¯»æ—¶é—´ 10 åˆ†é’ŸÂ·2023 å¹´ 6 æœˆ 20 æ—¥
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/3a14de30ec6761d39da8521d597c09d3.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3a14de30ec6761d39da8521d597c09d3.png)'
- en: Photo by [Ray Hennessy](https://unsplash.com/@rayhennessy?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ç‰‡ç”± [Ray Hennessy](https://unsplash.com/@rayhennessy?utm_source=medium&utm_medium=referral)
    æä¾›ï¼Œæ¥æºäº [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: Transformer-based models have been successfully applied in many fields like
    natural language processing (think BERT or GPT models) and computer vision to
    name a few.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: åŸºäºå˜æ¢å™¨çš„æ¨¡å‹å·²æˆåŠŸåº”ç”¨äºè®¸å¤šé¢†åŸŸï¼Œå¦‚è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆä¾‹å¦‚ BERT æˆ– GPT æ¨¡å‹ï¼‰å’Œè®¡ç®—æœºè§†è§‰ç­‰ã€‚
- en: However, when it comes to time series, state-of-the-art results have mostly
    been achieved by MLP models (multilayer perceptron) such as [N-BEATS](https://medium.com/towards-data-science/the-easiest-way-to-forecast-time-series-using-n-beats-d778fcc2ba60)
    and [N-HiTS](/all-about-n-hits-the-latest-breakthrough-in-time-series-forecasting-a8ddcb27b0d5).
    A recent paper even shows that simple linear models outperform complex transformer-based
    forecasting models on many benchmark datasets (see [Zheng et al., 2022](https://arxiv.org/pdf/2205.13504.pdf)).
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶è€Œï¼Œåœ¨æ—¶é—´åºåˆ—æ–¹é¢ï¼Œæœ€å…ˆè¿›çš„ç»“æœä¸»è¦ç”± MLP æ¨¡å‹ï¼ˆå¤šå±‚æ„ŸçŸ¥å™¨ï¼‰å¦‚ [N-BEATS](https://medium.com/towards-data-science/the-easiest-way-to-forecast-time-series-using-n-beats-d778fcc2ba60)
    å’Œ [N-HiTS](/all-about-n-hits-the-latest-breakthrough-in-time-series-forecasting-a8ddcb27b0d5)
    å®ç°ã€‚æœ€è¿‘çš„ä¸€ç¯‡è®ºæ–‡ç”šè‡³è¡¨æ˜ï¼Œç®€å•çš„çº¿æ€§æ¨¡å‹åœ¨è®¸å¤šåŸºå‡†æ•°æ®é›†ä¸Šä¼˜äºå¤æ‚çš„åŸºäºå˜æ¢å™¨çš„é¢„æµ‹æ¨¡å‹ï¼ˆè§ [Zheng ç­‰äºº, 2022](https://arxiv.org/pdf/2205.13504.pdf)ï¼‰ã€‚
- en: 'Still, a new transformer-based model has been proposed that achieves state-of-the-art
    results for long-term forecasting tasks: **PatchTST**.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: å°½ç®¡å¦‚æ­¤ï¼Œä¸€ç§æ–°çš„åŸºäºå˜æ¢å™¨çš„æ¨¡å‹å·²ç»è¢«æå‡ºï¼Œå¹¶åœ¨é•¿æœŸé¢„æµ‹ä»»åŠ¡ä¸­å–å¾—äº†æœ€å…ˆè¿›çš„ç»“æœï¼š**PatchTST**ã€‚
- en: 'PatchTST stands for patch time series transformer, and it was first proposed
    in March 2023 by Nie, Nguyen et al in their paper: [A Time Series is Worth 64
    Words: Long-Term Forecasting with Transformers](https://arxiv.org/pdf/2211.14730.pdf).
    Their proposed method achieved state-of-the-art results when compared to other
    transformer-based models.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: PatchTST ä»£è¡¨è¡¥ä¸æ—¶é—´åºåˆ—å˜æ¢å™¨ï¼Œå®ƒé¦–æ¬¡ç”± Nieã€Nguyen ç­‰äººåœ¨ 2023 å¹´ 3 æœˆæå‡ºï¼Œè¯¦ç»†ä»‹ç»åœ¨ä»–ä»¬çš„è®ºæ–‡ä¸­ï¼š[æ—¶é—´åºåˆ—å€¼å¾— 64
    ä¸ªè¯ï¼šä½¿ç”¨å˜æ¢å™¨è¿›è¡Œé•¿æœŸé¢„æµ‹](https://arxiv.org/pdf/2211.14730.pdf)ã€‚ä»–ä»¬æå‡ºçš„æ–¹æ³•åœ¨ä¸å…¶ä»–åŸºäºå˜æ¢å™¨çš„æ¨¡å‹æ¯”è¾ƒæ—¶ï¼Œå–å¾—äº†æœ€å…ˆè¿›çš„ç»“æœã€‚
- en: In this article, we first explore the inner workings of PatchTST, using intuition
    and no equations. Then, we apply the model in a forecasting project and compare
    its performance to MLP models, like N-BEATS and N-HiTS, and assess its performance.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘ä»¬é¦–å…ˆç›´è§‚åœ°æ¢ç´¢ PatchTST çš„å†…éƒ¨å·¥ä½œåŸç†ï¼Œè€Œä¸ä½¿ç”¨ä»»ä½•æ–¹ç¨‹å¼ã€‚æ¥ç€ï¼Œæˆ‘ä»¬å°†è¯¥æ¨¡å‹åº”ç”¨äºé¢„æµ‹é¡¹ç›®ï¼Œå¹¶å°†å…¶æ€§èƒ½ä¸ MLP æ¨¡å‹ï¼ˆå¦‚
    N-BEATS å’Œ N-HiTSï¼‰è¿›è¡Œæ¯”è¾ƒå’Œè¯„ä¼°ã€‚
- en: Of course, for more details about PatchTST, make sure to refer to the [original
    paper](https://arxiv.org/pdf/2211.14730.pdf).
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: å½“ç„¶ï¼Œå…³äº PatchTST çš„æ›´å¤šç»†èŠ‚ï¼Œè¯·å‚è€ƒ [åŸå§‹è®ºæ–‡](https://arxiv.org/pdf/2211.14730.pdf)ã€‚
- en: '***Learn the latest time series analysis techniques with my*** [***free time
    series cheat sheet***](https://www.datasciencewithmarco.com/pl/2147608294) ***in
    Python! Get the implementation of statistical and deep learning techniques, all
    in Python and TensorFlow!***'
  id: totrans-14
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '***ä½¿ç”¨æˆ‘çš„*** [***å…è´¹æ—¶é—´åºåˆ—å¤‡å¿˜å•***](https://www.datasciencewithmarco.com/pl/2147608294)
    ***åœ¨ Python ä¸­å­¦ä¹ æœ€æ–°çš„æ—¶é—´åºåˆ—åˆ†ææŠ€æœ¯ï¼è·å–ç»Ÿè®¡å’Œæ·±åº¦å­¦ä¹ æŠ€æœ¯çš„å®ç°ï¼Œå…¨éƒ¨ä½¿ç”¨ Python å’Œ TensorFlowï¼***'
- en: Letâ€™s get started!
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬å¼€å§‹å§ï¼
- en: Exploring PatchTST
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ¢ç´¢ PatchTST
- en: As mentioned, PatchTST stands for patch time series transformer.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚å‰æ‰€è¿°ï¼ŒPatchTST ä»£è¡¨è¡¥ä¸æ—¶é—´åºåˆ— transformerã€‚
- en: As the name suggests, it makes use of patching and of the transformer architecture.
    It also includes channel-independence to treat multivariate time series. The general
    architecture is shown below.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: æ­£å¦‚åå­—æ‰€ç¤ºï¼Œå®ƒåˆ©ç”¨äº†è¡¥ä¸å’Œ transformer æ¶æ„ã€‚å®ƒè¿˜åŒ…æ‹¬é€šé“ç‹¬ç«‹æ€§æ¥å¤„ç†å¤šå˜é‡æ—¶é—´åºåˆ—ã€‚ä¸‹é¢å±•ç¤ºäº†æ€»ä½“æ¶æ„ã€‚
- en: '![](../Images/4b69462b040217b63054f4cae1f6fb80.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4b69462b040217b63054f4cae1f6fb80.png)'
- en: 'The PatchTST model architecture. We see that the model makes use of channel-independence
    to treat multivariate time series. In the transformer backbone, we also see the
    use of patching (illustrated by the rectangles). Plus, there are two versions
    to the model: supervised and self-supervised. Image by Nie Y., Nguyen N., Sinthong
    P., Kalagnanam J. from [A Time Series is Worth 64 Words: Long-Term Forecasting
    with Transformers](https://arxiv.org/pdf/2211.14730.pdf).'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: PatchTST æ¨¡å‹æ¶æ„ã€‚æˆ‘ä»¬çœ‹åˆ°è¯¥æ¨¡å‹åˆ©ç”¨é€šé“ç‹¬ç«‹æ€§æ¥å¤„ç†å¤šå˜é‡æ—¶é—´åºåˆ—ã€‚åœ¨ transformer ä¸»å¹²ä¸­ï¼Œæˆ‘ä»¬è¿˜çœ‹åˆ°ä½¿ç”¨äº†è¡¥ä¸ï¼ˆç”±çŸ©å½¢è¡¨ç¤ºï¼‰ã€‚æ­¤å¤–ï¼Œæ¨¡å‹æœ‰ä¸¤ä¸ªç‰ˆæœ¬ï¼šç›‘ç£å­¦ä¹ å’Œè‡ªç›‘ç£å­¦ä¹ ã€‚å›¾ç‰‡ç”±
    Nie Y.ã€Nguyen N.ã€Sinthong P.ã€Kalagnanam J. æä¾›ï¼Œæ¥æºäº [â€œä¸€ä¸ªæ—¶é—´åºåˆ—å€¼ 64 ä¸ªè¯ï¼šä½¿ç”¨ Transformer
    è¿›è¡Œé•¿æœŸé¢„æµ‹â€](https://arxiv.org/pdf/2211.14730.pdf)ã€‚
- en: 'There is a lot of information to gather from the figure above. Here, the key
    elements are that PatchTST uses channel-independence to forecast multivariate
    time series. Then, in its transformer backbone, the model uses patching, which
    are illustrated by the small vertical rectangles. Also, the model comes in two
    versions: supervised and self-supervised.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: ä»ä¸Šå›¾ä¸­éœ€è¦æ”¶é›†å¤§é‡ä¿¡æ¯ã€‚è¿™é‡Œçš„å…³é”®è¦ç´ æ˜¯ï¼ŒPatchTST ä½¿ç”¨é€šé“ç‹¬ç«‹æ€§æ¥é¢„æµ‹å¤šå˜é‡æ—¶é—´åºåˆ—ã€‚ç„¶åï¼Œåœ¨å…¶ transformer ä¸»å¹²ä¸­ï¼Œè¯¥æ¨¡å‹ä½¿ç”¨è¡¥ä¸ï¼Œè¿™äº›è¡¥ä¸ç”±å°çš„å‚ç›´çŸ©å½¢è¡¨ç¤ºã€‚æ­¤å¤–ï¼Œè¯¥æ¨¡å‹æœ‰ä¸¤ä¸ªç‰ˆæœ¬ï¼šç›‘ç£å­¦ä¹ å’Œè‡ªç›‘ç£å­¦ä¹ ã€‚
- en: Letâ€™s explore in more detail the architecture and inner workings of PatchTST.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬æ›´è¯¦ç»†åœ°æ¢è®¨ PatchTST çš„æ¶æ„å’Œå†…éƒ¨å·¥ä½œåŸç†ã€‚
- en: Channel-independence
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: é€šé“ç‹¬ç«‹æ€§
- en: Here, a multivariate time series is considered as a multi-channel signal. Each
    time series is basically a channel containing a signal.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™é‡Œï¼Œå¤šå˜é‡æ—¶é—´åºåˆ—è¢«è§†ä¸ºå¤šé€šé“ä¿¡å·ã€‚æ¯ä¸ªæ—¶é—´åºåˆ—åŸºæœ¬ä¸Šæ˜¯ä¸€ä¸ªåŒ…å«ä¿¡å·çš„é€šé“ã€‚
- en: '![](../Images/4151bc1afe49fef36ba9ca36837b6cd7.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4151bc1afe49fef36ba9ca36837b6cd7.png)'
- en: 'Overview of the PatchTST model. Here, we really emphasize on the implementation
    of channel-independence, where each input token to the Transformer backbone contains
    information from only one channel, or one time series. Image by Nie Y., Nguyen
    N., Sinthong P., Kalagnanam J. from [A Time Series is Worth 64 Words: Long-Term
    Forecasting with Transformers](https://arxiv.org/pdf/2211.14730.pdf).'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: PatchTST æ¨¡å‹æ¦‚è¿°ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬ç‰¹åˆ«å¼ºè°ƒé€šé“ç‹¬ç«‹æ€§çš„å®ç°ï¼Œå…¶ä¸­æ¯ä¸ªè¾“å…¥ token åˆ° Transformer ä¸»å¹²ä»…åŒ…å«æ¥è‡ªä¸€ä¸ªé€šé“æˆ–ä¸€ä¸ªæ—¶é—´åºåˆ—çš„ä¿¡æ¯ã€‚å›¾ç‰‡ç”±
    Nie Y.ã€Nguyen N.ã€Sinthong P.ã€Kalagnanam J. æä¾›ï¼Œæ¥æºäº [â€œä¸€ä¸ªæ—¶é—´åºåˆ—å€¼ 64 ä¸ªè¯ï¼šä½¿ç”¨ Transformer
    è¿›è¡Œé•¿æœŸé¢„æµ‹â€](https://arxiv.org/pdf/2211.14730.pdf)ã€‚
- en: In the figure above, we see how a multivariate time series is separated into
    individual series, and each is fed to the Transformer backbone as an input token.
    Then, predictions are made for each series and the results are concatenated for
    the final predictions.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä¸Šå›¾ä¸­ï¼Œæˆ‘ä»¬çœ‹åˆ°å¦‚ä½•å°†å¤šå˜é‡æ—¶é—´åºåˆ—åˆ†ç¦»æˆå•ç‹¬çš„åºåˆ—ï¼Œå¹¶å°†æ¯ä¸ªåºåˆ—ä½œä¸ºè¾“å…¥ token ä¼ é€’ç»™ Transformer ä¸»å¹²ã€‚ç„¶åï¼Œå¯¹æ¯ä¸ªåºåˆ—è¿›è¡Œé¢„æµ‹ï¼Œç»“æœè¢«è¿æ¥èµ·æ¥å½¢æˆæœ€ç»ˆé¢„æµ‹ã€‚
- en: Patching
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è¡¥ä¸
- en: Most work on Transformer-based forecasting models focused on building new mechanisms
    to simplify the original attention mechanism. However, they still relied on point-wise
    attention, which is not ideal when it comes to time series.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: å¤§å¤šæ•°åŸºäº Transformer çš„é¢„æµ‹æ¨¡å‹çš„å·¥ä½œé›†ä¸­åœ¨æ„å»ºæ–°çš„æœºåˆ¶æ¥ç®€åŒ–åŸå§‹æ³¨æ„åŠ›æœºåˆ¶ã€‚ç„¶è€Œï¼Œå®ƒä»¬ä»ç„¶ä¾èµ–äºé€ç‚¹æ³¨æ„åŠ›ï¼Œè¿™åœ¨æ—¶é—´åºåˆ—çš„æƒ…å†µä¸‹å¹¶ä¸ç†æƒ³ã€‚
- en: In time series forecasting, we want to extract relationships between past time
    steps and future time steps to make predictions. With point-wise attention, we
    are trying to retrieve information from a single time step, without looking at
    what surrounds that point. In other words, we isolate a time step, and do not
    look at points before or after.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ—¶é—´åºåˆ—é¢„æµ‹ä¸­ï¼Œæˆ‘ä»¬å¸Œæœ›æå–è¿‡å»æ—¶é—´æ­¥å’Œæœªæ¥æ—¶é—´æ­¥ä¹‹é—´çš„å…³ç³»ä»¥è¿›è¡Œé¢„æµ‹ã€‚ä½¿ç”¨é€ç‚¹æ³¨æ„åŠ›æ—¶ï¼Œæˆ‘ä»¬è¯•å›¾ä»å•ä¸ªæ—¶é—´æ­¥ä¸­æ£€ç´¢ä¿¡æ¯ï¼Œè€Œä¸è€ƒè™‘è¯¥ç‚¹å‘¨å›´çš„å†…å®¹ã€‚æ¢å¥è¯è¯´ï¼Œæˆ‘ä»¬å­¤ç«‹ä¸€ä¸ªæ—¶é—´æ­¥ï¼Œè€Œä¸æŸ¥çœ‹ä¹‹å‰æˆ–ä¹‹åçš„ç‚¹ã€‚
- en: This is like trying to understand the meaning of a word without looking at the
    words around it in a sentence.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å°±åƒè¯•å›¾ç†è§£ä¸€ä¸ªè¯çš„æ„ä¹‰è€Œä¸çœ‹å¥å­ä¸­çš„å…¶ä»–è¯ä¸€æ ·ã€‚
- en: Therefore, PatchTST makes use of patching to extract local semantic information
    in time series.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: å› æ­¤ï¼ŒPatchTST åˆ©ç”¨åˆ†å—æ¥æå–æ—¶é—´åºåˆ—ä¸­çš„å±€éƒ¨è¯­ä¹‰ä¿¡æ¯ã€‚
- en: How patching works
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: åˆ†å—å¦‚ä½•å·¥ä½œ
- en: Each input series is divided into patches, which are simply shorter series coming
    from the original one.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: æ¯ä¸ªè¾“å…¥åºåˆ—è¢«åˆ†æˆå—ï¼Œè¿™äº›å—åªæ˜¯æ¥è‡ªåŸå§‹åºåˆ—çš„è¾ƒçŸ­åºåˆ—ã€‚
- en: '![](../Images/7a2a35f59c87380c06d755afe15d0cad.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7a2a35f59c87380c06d755afe15d0cad.png)'
- en: 'The Transformer backbone of PatchTST. Here, we see that the input time series
    (at the bottom of the figure) goes through patching, resulting in multiple patches
    (the vertical rectangles) which are then sent to the Transformer encoder. Image
    by Nie Y., Nguyen N., Sinthong P., Kalagnanam J. from [A Time Series is Worth
    64 Words: Long-Term Forecasting with Transformers](https://arxiv.org/pdf/2211.14730.pdf).'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 'PatchTST çš„ Transformer ä¸»å¹²ã€‚è¿™é‡Œï¼Œæˆ‘ä»¬çœ‹åˆ°è¾“å…¥æ—¶é—´åºåˆ—ï¼ˆå›¾åº•éƒ¨ï¼‰ç»è¿‡åˆ†å—ï¼Œç»“æœæ˜¯å¤šä¸ªå—ï¼ˆå‚ç›´çŸ©å½¢ï¼‰ï¼Œç„¶åè¢«é€åˆ° Transformer
    ç¼–ç å™¨ã€‚å›¾ç‰‡ç”± Nie Y.ã€Nguyen N.ã€Sinthong P.ã€Kalagnanam J. æä¾›ï¼Œæ¥è‡ª [A Time Series is Worth
    64 Words: Long-Term Forecasting with Transformers](https://arxiv.org/pdf/2211.14730.pdf)ã€‚'
- en: Here, the patch can be overlapping or non-overlapping. The number of patches
    depends on the length of the patch *P* and the stride *S*. Here, the stride is
    like in convolution, it is simply how many timesteps separate the beginning of
    consecutive patches.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é‡Œï¼Œå—å¯ä»¥æ˜¯é‡å çš„ä¹Ÿå¯ä»¥æ˜¯ä¸é‡å çš„ã€‚å—çš„æ•°é‡å–å†³äºå—çš„é•¿åº¦ *P* å’Œæ­¥å¹… *S*ã€‚è¿™é‡Œï¼Œæ­¥å¹…ç±»ä¼¼äºå·ç§¯ï¼Œå®ƒåªæ˜¯åˆ†éš”è¿ç»­å—å¼€å§‹çš„æ—¶é—´æ­¥æ•°ã€‚
- en: '![](../Images/bdf457f74ce8a32e97c5b832f4219c36.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bdf457f74ce8a32e97c5b832f4219c36.png)'
- en: Visualizing patching. Here, we have a sequence of 15 timesteps, with a patch
    length of 5 and a stride of 5 as well, resulting in three patches. Image by the
    author.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: å¯è§†åŒ–åˆ†å—ã€‚è¿™é‡Œï¼Œæˆ‘ä»¬æœ‰ä¸€ä¸ª 15 ä¸ªæ—¶é—´æ­¥çš„åºåˆ—ï¼Œåˆ†å—é•¿åº¦ä¸º 5ï¼Œæ­¥å¹…ä¹Ÿä¸º 5ï¼Œç»“æœæ˜¯ä¸‰ä¸ªå—ã€‚å›¾ç‰‡ç”±ä½œè€…æä¾›ã€‚
- en: In the figure above, we can visualize the result of patching. Here, we have
    a sequence length (*L*) of 15 time steps, with a patch length (*P*) of 5 and a
    stride (*S*) of 5\. The result is the series being separated into 3 patches.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä¸Šå›¾ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥å¯è§†åŒ–åˆ†å—çš„ç»“æœã€‚è¿™é‡Œï¼Œæˆ‘ä»¬æœ‰ä¸€ä¸ªåºåˆ—é•¿åº¦ (*L*) ä¸º 15 ä¸ªæ—¶é—´æ­¥ï¼Œåˆ†å—é•¿åº¦ (*P*) ä¸º 5ï¼Œæ­¥å¹… (*S*) ä¸º 5ã€‚ç»“æœæ˜¯åºåˆ—è¢«åˆ†ä¸º
    3 ä¸ªå—ã€‚
- en: Advantages of patching
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: åˆ†å—çš„ä¼˜åŠ¿
- en: With patching, the model can extract local semantic meaning by looking at groups
    of time steps, instead of looking at a single time step.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: é€šè¿‡åˆ†å—ï¼Œæ¨¡å‹å¯ä»¥é€šè¿‡æŸ¥çœ‹æ—¶é—´æ­¥çš„ç»„è€Œä¸æ˜¯å•ä¸ªæ—¶é—´æ­¥æ¥æå–å±€éƒ¨è¯­ä¹‰ä¿¡æ¯ã€‚
- en: It also has the added benefit of greatly reducing the number of token being
    fed to the transformer encoder. Here, each patch becomes an input token to be
    input to the Transformer. That way, we can reduce the number of token from *L*
    to approximately *L/S*.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: å®ƒè¿˜æœ‰ä¸€ä¸ªé¢å¤–çš„å¥½å¤„ï¼Œå°±æ˜¯å¤§å¤§å‡å°‘äº†é€å…¥ Transformer ç¼–ç å™¨çš„æ ‡è®°æ•°é‡ã€‚è¿™é‡Œï¼Œæ¯ä¸ªå—æˆä¸ºè¾“å…¥åˆ° Transformer çš„è¾“å…¥æ ‡è®°ã€‚è¿™æ ·ï¼Œæˆ‘ä»¬å¯ä»¥å°†æ ‡è®°æ•°é‡ä»
    *L* å‡å°‘åˆ°å¤§çº¦ *L/S*ã€‚
- en: That way, we greatly reduce the space and time complexity of the model. This
    in turn means that we can feed the model a longer input sequence to extract meaningful
    temporal relationships.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ ·ï¼Œæˆ‘ä»¬å¤§å¤§å‡å°‘äº†æ¨¡å‹çš„ç©ºé—´å’Œæ—¶é—´å¤æ‚åº¦ã€‚è¿™åè¿‡æ¥æ„å‘³ç€æˆ‘ä»¬å¯ä»¥ç»™æ¨¡å‹è¾“å…¥æ›´é•¿çš„åºåˆ—ï¼Œä»¥æå–æœ‰æ„ä¹‰çš„æ—¶é—´å…³ç³»ã€‚
- en: Therefore, with patching, the model is faster, lighter, and can treat a longer
    input sequence, meaning that it can potentially learn more about the series and
    make better forecasts.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: å› æ­¤ï¼Œé€šè¿‡åˆ†å—ï¼Œæ¨¡å‹å˜å¾—æ›´å¿«ã€æ›´è½»ï¼Œå¹¶ä¸”å¯ä»¥å¤„ç†æ›´é•¿çš„è¾“å…¥åºåˆ—ï¼Œè¿™æ„å‘³ç€å®ƒå¯èƒ½ä¼šå¯¹åºåˆ—å­¦ä¹ å¾—æ›´å¤šå¹¶åšå‡ºæ›´å¥½çš„é¢„æµ‹ã€‚
- en: Transformer encoder
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Transformer ç¼–ç å™¨
- en: Once the series is patched, it is then fed to the transformer encoder. This
    is the classical transformer architecture. Nothing was modified.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€æ—¦åºåˆ—è¢«åˆ†å—ï¼Œå®ƒä¼šè¢«é€å…¥ Transformer ç¼–ç å™¨ã€‚è¿™æ˜¯ç»å…¸çš„ Transformer æ¶æ„ï¼Œæ²¡æœ‰ä»»ä½•ä¿®æ”¹ã€‚
- en: Then, the output is fed to linear layer, and predictions are made.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åï¼Œå°†è¾“å‡ºé€å…¥çº¿æ€§å±‚ï¼Œå¹¶è¿›è¡Œé¢„æµ‹ã€‚
- en: Improving PatchTST with representation learning
  id: totrans-49
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä½¿ç”¨è¡¨ç¤ºå­¦ä¹ æ”¹è¿› PatchTST
- en: The authors of the paper suggested another improvement to the model by using
    representation learning.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: è®ºæ–‡çš„ä½œè€…å»ºè®®é€šè¿‡ä½¿ç”¨è¡¨ç¤ºå­¦ä¹ æ¥æ”¹è¿›æ¨¡å‹ã€‚
- en: '![](../Images/25ac32984418b4240d427515e4af5e8d.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/25ac32984418b4240d427515e4af5e8d.png)'
- en: 'Visualizing self-supervised representation learning in PatchTST. Here, the
    model will randomly mask patches and learn to reconstruct them. Image by Nie Y.,
    Nguyen N., Sinthong P., Kalagnanam J. from [A Time Series is Worth 64 Words: Long-Term
    Forecasting with Transformers](https://arxiv.org/pdf/2211.14730.pdf).'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 'åœ¨PatchTSTä¸­å¯è§†åŒ–è‡ªç›‘ç£è¡¨ç¤ºå­¦ä¹ ã€‚åœ¨è¿™é‡Œï¼Œæ¨¡å‹å°†éšæœºæ©ç›–è¡¥ä¸å¹¶å­¦ä¹ é‡å»ºå®ƒä»¬ã€‚å›¾ç‰‡ç”±Nie Y.ã€Nguyen N.ã€Sinthong P.ã€Kalagnanam
    J.æä¾›ï¼Œæ¥è‡ª[A Time Series is Worth 64 Words: Long-Term Forecasting with Transformers](https://arxiv.org/pdf/2211.14730.pdf)ã€‚'
- en: From the figure above, we can see that PatchTST can use self-supervised representation
    learning to capture abstract representations of the data. This can lead to potential
    improvements in forecasting performance.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: ä»ä¸Šå›¾ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°PatchTSTå¯ä»¥åˆ©ç”¨è‡ªç›‘ç£è¡¨ç¤ºå­¦ä¹ æ¥æ•æ‰æ•°æ®çš„æŠ½è±¡è¡¨ç¤ºã€‚è¿™å¯èƒ½ä¼šå¯¼è‡´é¢„æµ‹æ€§èƒ½çš„æ½œåœ¨æ”¹å–„ã€‚
- en: Here, the process is fairly simple, as random patches will be masked, meaning
    that they will be set to 0\. This is shown, in the figure above, by the blank
    vertical rectangles. Then, the model is trained to recreate the original patches,
    which is what is output at the top of the figure, as the grey vertical rectangles.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™é‡Œï¼Œè¿‡ç¨‹ç›¸å½“ç®€å•ï¼Œå› ä¸ºéšæœºè¡¥ä¸å°†è¢«æ©ç›–ï¼Œæ„å‘³ç€å®ƒä»¬å°†è¢«è®¾ç½®ä¸º0ã€‚è¿™åœ¨ä¸Šå›¾ä¸­ç”±ç©ºç™½çš„å‚ç›´çŸ©å½¢è¡¨ç¤ºã€‚ç„¶åï¼Œæ¨¡å‹è¢«è®­ç»ƒä»¥é‡å»ºåŸå§‹è¡¥ä¸ï¼Œè¿™å°±æ˜¯å›¾é¡¶ç«¯è¾“å‡ºçš„ç°è‰²å‚ç›´çŸ©å½¢ã€‚
- en: Now that we have a good understanding of how PatchTST works, letâ€™s test it against
    other models and see how it performs.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬å¯¹PatchTSTçš„å·¥ä½œåŸç†æœ‰äº†å¾ˆå¥½çš„ç†è§£ï¼Œè®©æˆ‘ä»¬å°†å…¶ä¸å…¶ä»–æ¨¡å‹è¿›è¡Œæµ‹è¯•ï¼Œçœ‹çœ‹å…¶è¡¨ç°å¦‚ä½•ã€‚
- en: Forecasting with PatchTST
  id: totrans-56
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ä½¿ç”¨PatchTSTè¿›è¡Œé¢„æµ‹
- en: In the paper, PatchTST is compared with other Transformer-based models. However,
    recent MLP-based models have been published, like N-BEATS and N-HiTS, and have
    also demonstrated state-of-the-art performance on long horizon forecasting tasks.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è®ºæ–‡ä¸­ï¼ŒPatchTSTä¸å…¶ä»–åŸºäºTransformerçš„æ¨¡å‹è¿›è¡Œäº†æ¯”è¾ƒã€‚ç„¶è€Œï¼Œæœ€è¿‘ä¹Ÿå‘å¸ƒäº†åŸºäºMLPçš„æ¨¡å‹ï¼Œå¦‚N-BEATSå’ŒN-HiTSï¼Œå®ƒä»¬åœ¨é•¿æœŸé¢„æµ‹ä»»åŠ¡ä¸­ä¹Ÿå±•ç¤ºäº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚
- en: The complete source code for this section is available on [GitHub](https://github.com/marcopeix/datasciencewithmarco/blob/master/PatchTST.ipynb).
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬èŠ‚çš„å®Œæ•´æºä»£ç å¯åœ¨[GitHub](https://github.com/marcopeix/datasciencewithmarco/blob/master/PatchTST.ipynb)ä¸Šæ‰¾åˆ°ã€‚
- en: Here, letâ€™s apply PatchTST, along with N-BEATS and N-HiTS and evaluate its performance
    against these two MLP-based models.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™é‡Œï¼Œè®©æˆ‘ä»¬åº”ç”¨PatchTSTï¼Œä»¥åŠN-BEATSå’ŒN-HiTSï¼Œå¹¶è¯„ä¼°å®ƒåœ¨è¿™ä¸¤ä¸ªåŸºäºMLPçš„æ¨¡å‹ä¸­çš„è¡¨ç°ã€‚
- en: For this exercise, we use the Exchange dataset, which is a common benchmark
    dataset for long-term forecasting in research. The dataset contains daily exchange
    rates of eight countries relative to the US dollar, from 1990 to 2016\. The dataset
    is made available through the MIT License.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºè¿™ä¸ªç»ƒä¹ ï¼Œæˆ‘ä»¬ä½¿ç”¨Exchangeæ•°æ®é›†ï¼Œè¿™æ˜¯ç ”ç©¶ä¸­ç”¨äºé•¿æœŸé¢„æµ‹çš„å¸¸è§åŸºå‡†æ•°æ®é›†ã€‚è¯¥æ•°æ®é›†åŒ…å«1990å¹´è‡³2016å¹´æœŸé—´ç›¸å¯¹äºç¾å…ƒçš„å…«ä¸ªå›½å®¶çš„æ¯æ—¥æ±‡ç‡ã€‚è¯¥æ•°æ®é›†é€šè¿‡MITè®¸å¯è¯æä¾›ã€‚
- en: Initial setup
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: åˆå§‹è®¾ç½®
- en: Letâ€™s start by importing the required libraries. Here, we will work with `neuralforecast`,
    as they have an out-of-the-box implementation of PatchTST. For the dataset, we
    use the `datasetsforecast` library, which includes all popular datasets for evaluating
    forecasting algorithms.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬é¦–å…ˆå¯¼å…¥æ‰€éœ€çš„åº“ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬å°†ä½¿ç”¨`neuralforecast`ï¼Œå› ä¸ºå®ƒä»¬æœ‰ä¸€ä¸ªç°æˆçš„PatchTSTå®ç°ã€‚å¯¹äºæ•°æ®é›†ï¼Œæˆ‘ä»¬ä½¿ç”¨`datasetsforecast`åº“ï¼Œå…¶ä¸­åŒ…å«æ‰€æœ‰æµè¡Œçš„æ•°æ®é›†ï¼Œç”¨äºè¯„ä¼°é¢„æµ‹ç®—æ³•ã€‚
- en: '[PRE0]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: If you have CUDA installed, then `neuralforecast` will automatically leverage
    your GPU to train the models. On my end, I do not have it installed, which is
    why I am not doing extensive hyperparameter tuning, or training on very large
    datasets.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœä½ å·²ç»å®‰è£…äº†CUDAï¼Œé‚£ä¹ˆ`neuralforecast`å°†è‡ªåŠ¨åˆ©ç”¨ä½ çš„GPUæ¥è®­ç»ƒæ¨¡å‹ã€‚åœ¨æˆ‘è¿™è¾¹ï¼Œæˆ‘æ²¡æœ‰å®‰è£…CUDAï¼Œè¿™å°±æ˜¯ä¸ºä»€ä¹ˆæˆ‘æ²¡æœ‰è¿›è¡Œå¹¿æ³›çš„è¶…å‚æ•°è°ƒæ•´æˆ–åœ¨éå¸¸å¤§çš„æ•°æ®é›†ä¸Šè®­ç»ƒçš„åŸå› ã€‚
- en: Once that is done, letâ€™s download the Exchange dataset.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: å®Œæˆè¿™ä¸€æ­¥åï¼Œè®©æˆ‘ä»¬ä¸‹è½½Exchangeæ•°æ®é›†ã€‚
- en: '[PRE1]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Here, we see that we get three DataFrames. The first one contains the daily
    exchange rates for each country. The second one contains exogenous time series.
    The third one, contains static exogenous variables (like day, month, year, hour,
    or any future information that we know).
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬çœ‹åˆ°å¾—åˆ°ä¸‰ä¸ªDataFramesã€‚ç¬¬ä¸€ä¸ªåŒ…å«æ¯ä¸ªå›½å®¶çš„æ¯æ—¥æ±‡ç‡ã€‚ç¬¬äºŒä¸ªåŒ…å«å¤–ç”Ÿæ—¶é—´åºåˆ—ã€‚ç¬¬ä¸‰ä¸ªåŒ…å«é™æ€å¤–ç”Ÿå˜é‡ï¼ˆå¦‚æ—¥æœŸã€æœˆä»½ã€å¹´ä»½ã€å°æ—¶æˆ–æˆ‘ä»¬å·²çŸ¥çš„ä»»ä½•æœªæ¥ä¿¡æ¯ï¼‰ã€‚
- en: For this exercise, we only work with `Y_df`.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºè¿™ä¸ªç»ƒä¹ ï¼Œæˆ‘ä»¬åªä½¿ç”¨`Y_df`ã€‚
- en: Then, letâ€™s make sure that the dates have the right type.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åï¼Œç¡®ä¿æ—¥æœŸå…·æœ‰æ­£ç¡®çš„ç±»å‹ã€‚
- en: '[PRE2]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '![](../Images/5bdc85c98c0f9baa3c85b7e7541aa8a7.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5bdc85c98c0f9baa3c85b7e7541aa8a7.png)'
- en: First five rows of the Exchange dataset. Image by the author.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: Exchangeæ•°æ®é›†çš„å‰äº”è¡Œã€‚ä½œè€…æä¾›çš„å›¾ç‰‡ã€‚
- en: In the figure above, we see that we have three columns. The first column is
    a unique identifier and it is necessary to have an id column when working with
    `neuralforecast`. Then, the `ds` column has the date, and the `y` column has the
    exchange rate.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä¸Šå›¾ä¸­ï¼Œæˆ‘ä»¬çœ‹åˆ°æœ‰ä¸‰åˆ—ã€‚ç¬¬ä¸€åˆ—æ˜¯å”¯ä¸€æ ‡è¯†ç¬¦ï¼Œå½“ä½¿ç”¨`neuralforecast`æ—¶ï¼Œéœ€è¦æœ‰ä¸€ä¸ªidåˆ—ã€‚ç„¶åï¼Œ`ds`åˆ—æœ‰æ—¥æœŸï¼Œ`y`åˆ—æœ‰æ±‡ç‡ã€‚
- en: '[PRE3]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '![](../Images/a8d8e7e94211b15b058816ad8f55e48b.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a8d8e7e94211b15b058816ad8f55e48b.png)'
- en: Showing the number of observations per unique id. Image by the author.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: æ˜¾ç¤ºæ¯ä¸ªå”¯ä¸€idçš„è§‚æµ‹æ•°é‡ã€‚å›¾ç‰‡ç”±ä½œè€…æä¾›ã€‚
- en: From the picture above, we can see that each unique id corresponds to a country,
    and that we have 7588 observations per country.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: ä»ä¸Šå›¾å¯ä»¥çœ‹å‡ºï¼Œæ¯ä¸ªå”¯ä¸€idå¯¹åº”ä¸€ä¸ªå›½å®¶ï¼Œæˆ‘ä»¬æ¯ä¸ªå›½å®¶æœ‰7588ä¸ªè§‚æµ‹å€¼ã€‚
- en: Now, we define the sizes of our validation and test sets. Here, I chose 760
    time steps for validation, and 1517 for the test set, as specified by the `[datasets](https://github.com/Nixtla/datasetsforecast/blob/main/datasetsforecast/long_horizon.py)`
    [library](https://github.com/Nixtla/datasetsforecast/blob/main/datasetsforecast/long_horizon.py).
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ï¼Œæˆ‘ä»¬å®šä¹‰éªŒè¯é›†å’Œæµ‹è¯•é›†çš„å¤§å°ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘é€‰æ‹©äº†760ä¸ªæ—¶é—´æ­¥ä½œä¸ºéªŒè¯é›†ï¼Œ1517ä¸ªæ—¶é—´æ­¥ä½œä¸ºæµ‹è¯•é›†ï¼Œå¦‚`[datasets](https://github.com/Nixtla/datasetsforecast/blob/main/datasetsforecast/long_horizon.py)`
    [åº“](https://github.com/Nixtla/datasetsforecast/blob/main/datasetsforecast/long_horizon.py)æ‰€è§„å®šã€‚
- en: '[PRE4]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Then, letâ€™s plot one of the series, to see what we are working with. Here, I
    decided to plot the series for the first country (unique_id = 0), but feel free
    to plot another series.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åï¼Œè®©æˆ‘ä»¬ç»˜åˆ¶å…¶ä¸­ä¸€ä¸ªåºåˆ—ï¼Œçœ‹çœ‹æˆ‘ä»¬åœ¨å¤„ç†ä»€ä¹ˆã€‚åœ¨è¿™é‡Œï¼Œæˆ‘å†³å®šç»˜åˆ¶ç¬¬ä¸€ä¸ªå›½å®¶çš„åºåˆ—ï¼ˆunique_id = 0ï¼‰ï¼Œä½†å¯ä»¥è‡ªç”±ç»˜åˆ¶å…¶ä»–åºåˆ—ã€‚
- en: '[PRE5]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '![](../Images/8fb95dd385ff7a81b55fbc9cf21a8618.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8fb95dd385ff7a81b55fbc9cf21a8618.png)'
- en: Daily exchange rate for the first country, from 1990 to 2016\. Image by the
    author.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: ç¬¬ä¸€ä¸ªå›½å®¶ä»1990å¹´åˆ°2016å¹´çš„æ¯æ—¥æ±‡ç‡ã€‚å›¾ç‰‡ç”±ä½œè€…æä¾›ã€‚
- en: From the figure above, we see that we have fairly noisy data with no clear seasonality.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: ä»ä¸Šå›¾ä¸­ï¼Œæˆ‘ä»¬çœ‹åˆ°æ•°æ®ç›¸å½“å˜ˆæ‚ï¼Œæ²¡æœ‰æ˜æ˜¾çš„å­£èŠ‚æ€§ã€‚
- en: Modelling
  id: totrans-85
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å»ºæ¨¡
- en: Having explored the data, letâ€™s get started on modelling with `neuralforecast`.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: æ¢ç´¢äº†æ•°æ®åï¼Œè®©æˆ‘ä»¬å¼€å§‹ä½¿ç”¨`neuralforecast`è¿›è¡Œå»ºæ¨¡ã€‚
- en: First, we need to set the horizon. In this case, I use 96 time steps, as this
    horizon is also used in the [PatchTST paper](https://arxiv.org/pdf/2211.14730.pdf).
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: é¦–å…ˆï¼Œæˆ‘ä»¬éœ€è¦è®¾ç½®é¢„æµ‹æœŸã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä½¿ç”¨96ä¸ªæ—¶é—´æ­¥ï¼Œå› ä¸ºè¿™ä¸ªé¢„æµ‹æœŸä¹Ÿåœ¨[PatchTSTè®ºæ–‡](https://arxiv.org/pdf/2211.14730.pdf)ä¸­ä½¿ç”¨ã€‚
- en: Then, to have a fair evaluation of each model, I decided to set the input size
    to twice the horizon (so 192 time steps), and set the maximum number of epochs
    to 50\. All other hyperparameters are kept to their default values.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åï¼Œä¸ºäº†å…¬å¹³è¯„ä¼°æ¯ä¸ªæ¨¡å‹ï¼Œæˆ‘å†³å®šå°†è¾“å…¥å¤§å°è®¾ç½®ä¸ºé¢„æµ‹æœŸçš„ä¸¤å€ï¼ˆå³192ä¸ªæ—¶é—´æ­¥ï¼‰ï¼Œå¹¶å°†æœ€å¤§è®­ç»ƒè½®æ•°è®¾ç½®ä¸º50ã€‚æ‰€æœ‰å…¶ä»–è¶…å‚æ•°ä¿æŒé»˜è®¤å€¼ã€‚
- en: '[PRE6]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Then, we initialize the `NeuralForecast`object, by specifying the models we
    want to use and the frequency of the forecast, which in this is case is daily.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åï¼Œæˆ‘ä»¬åˆå§‹åŒ–`NeuralForecast`å¯¹è±¡ï¼ŒæŒ‡å®šæˆ‘ä»¬è¦ä½¿ç”¨çš„æ¨¡å‹å’Œé¢„æµ‹é¢‘ç‡ï¼Œåœ¨è¿™é‡Œæ˜¯æ¯æ—¥ã€‚
- en: '[PRE7]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: We are now ready to make predictions.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ç°åœ¨å‡†å¤‡è¿›è¡Œé¢„æµ‹ã€‚
- en: Forecasting
  id: totrans-93
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: é¢„æµ‹
- en: To generate predictions, we use the `cross_validation` method to make use of
    the validation and test sets. It will return a DataFrame with predictions from
    all models and the associated true value.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†ç”Ÿæˆé¢„æµ‹ï¼Œæˆ‘ä»¬ä½¿ç”¨`cross_validation`æ–¹æ³•åˆ©ç”¨éªŒè¯é›†å’Œæµ‹è¯•é›†ã€‚å®ƒå°†è¿”å›ä¸€ä¸ªåŒ…å«æ‰€æœ‰æ¨¡å‹é¢„æµ‹å€¼åŠç›¸å…³çœŸå®å€¼çš„DataFrameã€‚
- en: '[PRE8]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '![](../Images/e7922f2c01e0b5086adf5558a05930bf.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e7922f2c01e0b5086adf5558a05930bf.png)'
- en: First five rows of the predictions DataFrame. Image by the author.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: é¢„æµ‹DataFrameçš„å‰äº”è¡Œã€‚å›¾ç‰‡ç”±ä½œè€…æä¾›ã€‚
- en: As you can see, for each id, we have the predictions from each model as well
    as the true value in the `y` column.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æ‚¨æ‰€è§ï¼Œå¯¹äºæ¯ä¸ªidï¼Œæˆ‘ä»¬æœ‰æ¥è‡ªæ¯ä¸ªæ¨¡å‹çš„é¢„æµ‹ä»¥åŠ`y`åˆ—ä¸­çš„çœŸå®å€¼ã€‚
- en: Now, to evaluate the models, we have to reshape the arrays of actual and predicted
    values to have the shape `(number of series, number of windows, forecast horizon)`.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ï¼Œä¸ºäº†è¯„ä¼°æ¨¡å‹ï¼Œæˆ‘ä»¬éœ€è¦å°†å®é™…å€¼å’Œé¢„æµ‹å€¼çš„æ•°ç»„é‡å¡‘ä¸ºå½¢çŠ¶ä¸º`(seriesçš„æ•°é‡, çª—å£æ•°é‡, é¢„æµ‹æœŸ)`ã€‚
- en: '[PRE9]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: With that done, we can optionally plot the predictions of our models. Here,
    we plot the predictions in the first window of the first series.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: å®Œæˆåï¼Œæˆ‘ä»¬å¯ä»¥é€‰æ‹©æ€§åœ°ç»˜åˆ¶æ¨¡å‹çš„é¢„æµ‹ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬ç»˜åˆ¶äº†ç¬¬ä¸€ä¸ªåºåˆ—çš„ç¬¬ä¸€ä¸ªçª—å£ä¸­çš„é¢„æµ‹ã€‚
- en: '[PRE10]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '![](../Images/a69608da3942d28cec8980644b40e72d.png)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a69608da3942d28cec8980644b40e72d.png)'
- en: Predictions of the daily exchange rate for the first series, in the first window.
    Image by the author.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: ç¬¬ä¸€ä¸ªåºåˆ—åœ¨ç¬¬ä¸€ä¸ªçª—å£ä¸­çš„æ¯æ—¥æ±‡ç‡é¢„æµ‹ã€‚å›¾ç‰‡ç”±ä½œè€…æä¾›ã€‚
- en: This figure is a bit underwhelming, as N-BEATS and N-HiTS seem to have predictions
    that are very off from the actual values. However, PatchTST, while also off, seems
    to be the closest to the actual values.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªå›¾æœ‰ç‚¹è®©äººå¤±æœ›ï¼Œå› ä¸ºN-BEATSå’ŒN-HiTSçš„é¢„æµ‹ç»“æœä¸å®é™…å€¼å·®å¼‚å¾ˆå¤§ã€‚ç„¶è€Œï¼Œå°½ç®¡PatchTSTä¹Ÿæœ‰åå·®ï¼Œä½†ä¼¼ä¹æœ€æ¥è¿‘å®é™…å€¼ã€‚
- en: Of course, we must takes this with a grain of salt, because we are only visualizing
    the prediction for one series, in one prediction window.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: å½“ç„¶ï¼Œæˆ‘ä»¬å¿…é¡»å¯¹æ­¤æŒä¿ç•™æ€åº¦ï¼Œå› ä¸ºæˆ‘ä»¬åªæ˜¯å¯è§†åŒ–äº†ä¸€ä¸ªåºåˆ—ä¸­çš„ä¸€ä¸ªé¢„æµ‹çª—å£ã€‚
- en: Evaluation
  id: totrans-107
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è¯„ä¼°
- en: So, letâ€™s evaluate the performance of each model. To replicate the methodology
    from the paper, we use both the MAE and MSE as performance metrics.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥ï¼Œè®©æˆ‘ä»¬è¯„ä¼°æ¯ä¸ªæ¨¡å‹çš„è¡¨ç°ã€‚ä¸ºäº†å¤åˆ¶è®ºæ–‡ä¸­çš„æ–¹æ³•ï¼Œæˆ‘ä»¬ä½¿ç”¨MAEå’ŒMSEä½œä¸ºæ€§èƒ½æŒ‡æ ‡ã€‚
- en: '[PRE11]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '![](../Images/0c6660f2c34408d4bb5c2f7e1b609783.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0c6660f2c34408d4bb5c2f7e1b609783.png)'
- en: Performance of all models. Here, PatchTST achieves the lowest MAE and MSE. Image
    by the author.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€æœ‰æ¨¡å‹çš„æ€§èƒ½ã€‚åœ¨è¿™é‡Œï¼ŒPatchTSTå®ç°äº†æœ€ä½çš„MAEå’ŒMSEã€‚å›¾ç‰‡ç”±ä½œè€…æä¾›ã€‚
- en: In the table above, we see that PatchTST is the champion model as it achieves
    the lowest MAE and MSE.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä¸Šè¡¨ä¸­ï¼Œæˆ‘ä»¬çœ‹åˆ°PatchTSTæ˜¯å† å†›æ¨¡å‹ï¼Œå› ä¸ºå®ƒå®ç°äº†æœ€ä½çš„MAEå’ŒMSEã€‚
- en: Of course, this was not the most thorough experiment, as we only used one dataset
    and one forecast horizon. Still, it is interesting to see that a Transformer-based
    model can compete with state-of-the-art MLP models.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: å½“ç„¶ï¼Œè¿™ä¸æ˜¯æœ€å½»åº•çš„å®éªŒï¼Œå› ä¸ºæˆ‘ä»¬åªä½¿ç”¨äº†ä¸€ä¸ªæ•°æ®é›†å’Œä¸€ä¸ªé¢„æµ‹èŒƒå›´ã€‚å°½ç®¡å¦‚æ­¤ï¼Œçœ‹åˆ°ä¸€ä¸ªåŸºäºTransformerçš„æ¨¡å‹èƒ½å¤Ÿä¸æœ€å…ˆè¿›çš„MLPæ¨¡å‹ç«äº‰ä»ç„¶å¾ˆæœ‰è¶£ã€‚
- en: Conclusion
  id: totrans-114
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç»“è®º
- en: PatchTST is a Transformer-based models that uses patching to extract local semantic
    meaning in time series data. This allows the model to be faster to train and to
    have a longer input window.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: PatchTSTæ˜¯åŸºäºTransformerçš„æ¨¡å‹ï¼Œé€šè¿‡è¡¥ä¸æ¥æå–æ—¶é—´åºåˆ—æ•°æ®ä¸­çš„å±€éƒ¨è¯­ä¹‰æ„ä¹‰ã€‚è¿™ä½¿å¾—æ¨¡å‹çš„è®­ç»ƒé€Ÿåº¦æ›´å¿«ï¼Œå¹¶ä¸”å…·æœ‰æ›´é•¿çš„è¾“å…¥çª—å£ã€‚
- en: It has achieved state-of-the-art performances when compared to other Transformer-based
    models. In our little exercise, we saw that it also achieved better performances
    than N-BEATS and N-HiTS.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: ç›¸è¾ƒäºå…¶ä»–åŸºäºTransformerçš„æ¨¡å‹ï¼Œå®ƒå·²ç»è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚åœ¨æˆ‘ä»¬çš„ç»ƒä¹ ä¸­ï¼Œæˆ‘ä»¬çœ‹åˆ°å®ƒçš„è¡¨ç°ä¹Ÿä¼˜äºN-BEATSå’ŒN-HiTSã€‚
- en: While this does not mean that it is better than N-HiTS or N-BEATS, it remains
    an interesting option when forecasting on a long horizon.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: è™½ç„¶è¿™å¹¶ä¸æ„å‘³ç€å®ƒæ¯”N-HiTSæˆ–N-BEATSæ›´å¥½ï¼Œä½†å®ƒä»ç„¶æ˜¯ä¸€ä¸ªæœ‰è¶£çš„é€‰é¡¹ï¼Œå°¤å…¶æ˜¯åœ¨é•¿æœŸé¢„æµ‹æ—¶ã€‚
- en: Thanks for reading! I hope that you enjoyed it and that you learned something
    new!
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: æ„Ÿè°¢é˜…è¯»ï¼å¸Œæœ›ä½ å–œæ¬¢è¿™ç¯‡æ–‡ç« ï¼Œå¹¶ä¸”å­¦åˆ°äº†ä¸€äº›æ–°ä¸œè¥¿ï¼
- en: Looking to master time series forecasting? The check out [Applied Time Series
    Forecasting in Python](https://www.datasciencewithmarco.com/offers/zTAs2hi6/checkout?coupon_code=ATSFP10).
    This is the only course that uses Python to implement statistical, deep learning
    and state-of-the-art models in 15 guided hands-on projects.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: æƒ³è¦æŒæ¡æ—¶é—´åºåˆ—é¢„æµ‹ï¼Ÿé‚£å°±çœ‹çœ‹[Pythonåº”ç”¨æ—¶é—´åºåˆ—é¢„æµ‹](https://www.datasciencewithmarco.com/offers/zTAs2hi6/checkout?coupon_code=ATSFP10)ã€‚è¿™æ˜¯å”¯ä¸€ä¸€ä¸ªé€šè¿‡15ä¸ªå¼•å¯¼å®è·µé¡¹ç›®ä½¿ç”¨Pythonå®ç°ç»Ÿè®¡å­¦ã€æ·±åº¦å­¦ä¹ å’Œæœ€å…ˆè¿›æ¨¡å‹çš„è¯¾ç¨‹ã€‚
- en: Cheers ğŸ»
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: å¹²æ¯ ğŸ»
- en: Support me
  id: totrans-121
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ”¯æŒæˆ‘
- en: Enjoying my work? Show your support with [Buy me a coffee](http://buymeacoffee.com/dswm),
    a simple way for you to encourage me, and I get to enjoy a cup of coffee! If you
    feel like it, just click the button below ğŸ‘‡
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: å–œæ¬¢æˆ‘çš„å·¥ä½œå—ï¼Ÿé€šè¿‡[ç»™æˆ‘ä¹°æ¯å’–å•¡](http://buymeacoffee.com/dswm)æ¥è¡¨ç¤ºæ”¯æŒï¼Œè¿™æ˜¯é¼“åŠ±æˆ‘çš„ç®€å•æ–¹å¼ï¼Œæˆ‘å¯ä»¥äº«å—ä¸€æ¯å’–å•¡ï¼å¦‚æœä½ æ„¿æ„ï¼Œè¯·ç‚¹å‡»ä¸‹é¢çš„æŒ‰é’®
    ğŸ‘‡
- en: '[![](../Images/7ad9438bd50b1698fdd722fa6661b16c.png)](http://buymeacoffee.com/dswm)'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '[![](../Images/7ad9438bd50b1698fdd722fa6661b16c.png)](http://buymeacoffee.com/dswm)'
- en: References
  id: totrans-124
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å‚è€ƒæ–‡çŒ®
- en: '[A Time Series is Worth 64 Words: Long-Term Forecasting with Transformers](https://arxiv.org/pdf/2211.14730.pdf)
    by Nie Y., Nguyen N. et al.'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '[æ—¶é—´åºåˆ—çš„ä»·å€¼ä¸º64ä¸ªè¯ï¼šä½¿ç”¨Transformerè¿›è¡Œé•¿æœŸé¢„æµ‹](https://arxiv.org/pdf/2211.14730.pdf) ä½œè€…ï¼šNie
    Y., Nguyen N. ç­‰ã€‚'
- en: '[Neuralforecast](https://nixtla.github.io/neuralforecast/) by Olivares K.,
    Challu C., Garza F., Canseco M., Dubrawski A.'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '[Neuralforecast](https://nixtla.github.io/neuralforecast/) ä½œè€…ï¼šOlivares K.,
    Challu C., Garza F., Canseco M., Dubrawski A.'
