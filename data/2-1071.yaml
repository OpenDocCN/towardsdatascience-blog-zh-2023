- en: How Exactly Does a Decision Tree Solve a Regression Problem?
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 决策树如何解决回归问题？
- en: 原文：[https://towardsdatascience.com/how-exactly-does-a-decision-tree-solve-a-regression-problem-fbb908cf548b](https://towardsdatascience.com/how-exactly-does-a-decision-tree-solve-a-regression-problem-fbb908cf548b)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/how-exactly-does-a-decision-tree-solve-a-regression-problem-fbb908cf548b](https://towardsdatascience.com/how-exactly-does-a-decision-tree-solve-a-regression-problem-fbb908cf548b)
- en: Build your own decision tree regressor (from scratch in Python) and uncover
    what’s under the hood
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从头开始构建你自己的决策树回归器（用 Python），揭示其内部机制
- en: '[](https://medium.com/@gurjinderkaur95?source=post_page-----fbb908cf548b--------------------------------)[![Gurjinder
    Kaur](../Images/d5c6746466025dad06077b1a89a789d1.png)](https://medium.com/@gurjinderkaur95?source=post_page-----fbb908cf548b--------------------------------)[](https://towardsdatascience.com/?source=post_page-----fbb908cf548b--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----fbb908cf548b--------------------------------)
    [Gurjinder Kaur](https://medium.com/@gurjinderkaur95?source=post_page-----fbb908cf548b--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@gurjinderkaur95?source=post_page-----fbb908cf548b--------------------------------)[![Gurjinder
    Kaur](../Images/d5c6746466025dad06077b1a89a789d1.png)](https://medium.com/@gurjinderkaur95?source=post_page-----fbb908cf548b--------------------------------)[](https://towardsdatascience.com/?source=post_page-----fbb908cf548b--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----fbb908cf548b--------------------------------)
    [Gurjinder Kaur](https://medium.com/@gurjinderkaur95?source=post_page-----fbb908cf548b--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----fbb908cf548b--------------------------------)
    ·12 min read·Nov 25, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----fbb908cf548b--------------------------------)
    ·阅读时长 12 分钟 ·2023年11月25日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/79b7360f9005933ba478d9db039fa1bc.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/79b7360f9005933ba478d9db039fa1bc.png)'
- en: Photo by [Chris Lawton](https://unsplash.com/@chrislawton?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源于 [Chris Lawton](https://unsplash.com/@chrislawton?utm_source=medium&utm_medium=referral)
    在 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: In this article, I’m going to demonstrate through a simple example, flowchart,
    and code — the entire logic implemented under the hood of a decision tree regressor
    (*aka regression tree*). After reading it, you will be able to get a clear idea
    behind the working of regression trees, and you will be more thoughtful & confident
    in using and tuning them in your next regression challenge.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我将通过一个简单的示例、流程图和代码展示决策树回归器（*即回归树*）内部实现的整个逻辑。阅读后，你将对回归树的工作原理有一个清晰的理解，并在下一次回归挑战中更加深思熟虑和自信地使用和调整它们。
- en: 'We will cover the following:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将涵盖以下内容：
- en: An awesome introduction to decision trees
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对决策树的精彩介绍
- en: Generating a toy data set for training our regression tree
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生成一个用于训练回归树的玩具数据集
- en: Outlining the logic of the regression tree in the form of a flowchart
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 绘制回归树逻辑的流程图
- en: Referencing the flowchart to write the code using NumPy and Pandas and make
    the first split
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 参考流程图编写代码，使用 NumPy 和 Pandas 进行首次分裂
- en: Visualizing the decision tree after our first split, using Plotly
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Plotly 在首次分裂后可视化决策树
- en: Generalizing the code using recursion to build the entire regression tree
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用递归将代码泛化以构建整个回归树
- en: 'Use scikit-learn to perform the same task and compare the results (*spoiler:
    you’ll be so proud that you produced the same output as scikit-learn did!)*'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 scikit-learn 执行相同任务并比较结果（*剧透：你将为自己与 scikit-learn 得到相同的输出感到非常自豪！*）
- en: Introduction
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: Decision trees are machine learning algorithms that can be used to solve both
    classification as well as regression problems. Even though classification and
    regression are inherently different from each other, decision trees try to approach
    both of these problems in an elegant way where the ultimate goal is to find the
    best split at a given node. And, how that best split is determined is what makes
    a classification tree and a regression tree different from each other.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树是可以用来解决分类和回归问题的机器学习算法。尽管分类和回归本质上是不同的，但决策树试图以优雅的方式解决这两种问题，其*最终目标*是在给定节点找到最佳分裂。确定最佳分裂的方式就是使分类树和回归树彼此不同的原因。
- en: In my previous [article](https://medium.com/towards-data-science/how-does-a-decision-tree-know-the-next-best-question-to-ask-from-the-data-0d44c9433b06),
    I touched upon the basics of how a decision tree solves a classification problem.
    I used a two-class dataset to demonstrate a step-by-step process to understand
    how a decision rule is generated at each node using data impurity measures such
    as entropy, and then later implemented a recursive algorithm in Python to output
    the final decision tree. Not sure if you should add [this](https://medium.com/towards-data-science/how-does-a-decision-tree-know-the-next-best-question-to-ask-from-the-data-0d44c9433b06)
    article to your reading list? Let’s use a decision tree to find out!
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在我之前的[文章](https://medium.com/towards-data-science/how-does-a-decision-tree-know-the-next-best-question-to-ask-from-the-data-0d44c9433b06)中，我介绍了决策树如何解决分类问题的基础知识。我使用了一个两类数据集来演示如何逐步理解如何通过数据不纯度度量（如熵）在每个节点生成决策规则，然后在
    Python 中实现了一个递归算法以输出最终的决策树。不确定是否应该将[这篇](https://medium.com/towards-data-science/how-does-a-decision-tree-know-the-next-best-question-to-ask-from-the-data-0d44c9433b06)文章添加到你的阅读列表中？让我们使用决策树来找出答案吧！
- en: '![](../Images/c44acf7789690e9b12a59ea1bd77e01e.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c44acf7789690e9b12a59ea1bd77e01e.png)'
- en: 'Source: Image by author'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 来源：作者提供的图片
- en: '***Important Note:*** *This was just an example to show what a decision tree
    is, AND you’re awesome no matter if it says that or not.*'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '***重要提示：*** *这只是一个展示决策树是什么的示例，无论它是否这样说，你都很棒。*'
- en: As shown above, a decision tree classifier aims to predict discrete labels (or
    *classes)* which in our case are *You R Awesome!* and *Go READ IT!.* In such cases,
    the decision tree looks at the probability distribution of the classes at each
    split to compute measures such as entropy and hence decides the best feature and
    value to split on.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 如上所示，决策树分类器旨在预测离散标签（或*类别*），在我们的案例中是*你很棒！*和*去阅读它！*。在这种情况下，决策树会查看每个分裂点的类别概率分布，以计算熵等指标，从而决定最佳的特征和分裂值。
- en: However, in regression problems, the target variable is *continuous* and we
    cannot use entropy (or Gini index) as the split criterion in such cases. So, regression
    trees make use of ***Mean Squared Error (MSE)***and choose the feature and value
    that leads to the minimum MSE at each node.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在回归问题中，目标变量是*连续的*，我们不能在这种情况下使用熵（或基尼指数）作为分裂标准。因此，回归树使用***均方误差（MSE）***并选择在每个节点上导致最小
    MSE 的特征和数值。
- en: '**Mean Squared Error:** It is defined as the sum of squared differences between
    the true values and predicted values.'
  id: totrans-25
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**均方误差：** 它被定义为真实值与预测值之间的平方差之和。'
- en: '![](../Images/4d67e7de1cb32e4d1409d5cbb854b78a.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4d67e7de1cb32e4d1409d5cbb854b78a.png)'
- en: 'Source: Image by author'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 来源：作者提供的图片
- en: Building a Regression Tree
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建回归树
- en: To demonstrate how a regression tree is learned, I’m going to use NumPy to generate
    a toy dataset resembling a quadratic function that will be used as the training
    data. Feel free to pause and bring up a new Python notebook on the side to code
    along as you read.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 为了演示回归树是如何学习的，我将使用 NumPy 生成一个类似于二次函数的玩具数据集，用作训练数据。可以暂停并打开一个新的 Python 笔记本，以便在阅读时进行编码。
- en: Let’s import the libraries first.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 首先让我们导入库。
- en: '[PRE0]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '**Generating the dataset**'
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**生成数据集**'
- en: In the following code, we’re going to generate the training data where our target
    i.e. **dependent variable** ***y*** is a quadratic function of **independent variable
    *X.*** For the sake of simplicity, I’m considering a single feature *X*, but it
    can be extended easily with multiple features (both continuous and categorical
    features).
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的代码中，我们将生成训练数据，其中我们的目标，即**因变量** ***y*** 是**自变量 *X* 的二次函数**。为了简单起见，我考虑了一个单一特征
    *X*，但可以很容易地扩展到多个特征（包括连续特征和分类特征）。
- en: Here, both *X* and *y* are continuous (*y has to be continuous since it’s regression
    and X could be either categorical or continuous*).
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*X* 和 *y* 都是连续的（*y 必须是连续的，因为这是回归，而 X 可以是分类的或连续的*）。
- en: '[PRE1]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '**Visualizing the data**'
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**数据可视化**'
- en: We have created 100 data points. Let’s use Plotly to visualize our data as follows.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经创建了 100 个数据点。接下来我们将使用 Plotly 以如下方式可视化我们的数据。
- en: '[PRE2]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '`Cell Output:`'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '`单元格输出：`'
- en: '![](../Images/e829f0f1a028676fd00daba18b133d1f.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e829f0f1a028676fd00daba18b133d1f.png)'
- en: 'Source: Image by author'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 来源：作者提供的图片
- en: '**Ready to make our first split?**'
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**准备好进行第一次分裂了吗？**'
- en: Before doing that, let’s first outline the steps a regression tree takes and
    then we will refer to these steps to write our code. **Spending some time studying
    the flowchart below is all we need to do to understand the underlying logic of
    a regression tree to find the optimal split at a given node.**
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在这之前，让我们首先概述一下回归树所采取的步骤，然后我们将参考这些步骤来编写我们的代码。 **花一点时间研究下面的流程图就足以理解回归树的基本逻辑，以在给定节点找到最佳划分。**
- en: '![](../Images/d810e13009fea923553e4eea3acd1754.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d810e13009fea923553e4eea3acd1754.png)'
- en: 'Flowchart demonstrating regression tree logic (Source: Image by author)'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 流程图展示了回归树逻辑（来源：作者提供的图片）
- en: Once the above logic is clear, we are all set to witness the regression tree
    make its first optimal split using the following code.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦上述逻辑清楚，我们就可以使用以下代码来见证回归树进行第一次最佳划分。
- en: We need to first define a helper function to calculate MSE.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要首先定义一个辅助函数来计算MSE。
- en: '[PRE3]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The following code is the core logic as explained in the flowchart above.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码是如上流程图所示的核心逻辑。
- en: '[PRE4]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '`Cell Output:`'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '`Cell Output:`'
- en: '![](../Images/7fcc643de9d77121f7696dd7e7e18067.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7fcc643de9d77121f7696dd7e7e18067.png)'
- en: 'Yayy! We got our first optimal split at `X=6.869`. Let’s visualize it as follows:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 太棒了！我们在`X=6.869`得到了第一个最佳划分。让我们如下可视化它：
- en: '[PRE5]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '`Cell Output:`'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '`Cell Output:`'
- en: '![](../Images/ad157b8c585324b34255658b9c5ffd20.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ad157b8c585324b34255658b9c5ffd20.png)'
- en: 'Source: Image by author'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 来源：作者提供的图片
- en: '**Interpretation of the above result:**'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '**上述结果的解释：**'
- en: '`X<=6.869` corresponds to the split value. It means that the data at the current
    node will get subdivided into two parts — one where `X<=6.869` and the other where
    `X>6.869`. That’s what the black vertical line represents.'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`X<=6.869`对应划分值。这意味着当前节点的数据将被划分为两部分——一部分是`X<=6.869`，另一部分是`X>6.869`。这就是黑色垂直线所表示的。'
- en: '`samples=100` corresponds to the total number of data points at the current
    node. Initially, the root node consists of all data points.'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`samples=100`对应当前节点的总数据点数。最初，根节点包含所有数据点。'
- en: '`MSE=1381.82` corresponds to the value of the mean squared error at this current
    node. We know that MSE needs true values and predicted values for its calculation;
    we already have true values, what are the predicted values you might be wondering,
    right? That’s `value`.'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`MSE=1381.82`对应当前节点的均方误差值。我们知道MSE需要真实值和预测值来计算；我们已经有了真实值，预测值是什么呢？那就是`value`。'
- en: '`value=37.605` corresponds to the prediction at the current node. ***The prediction
    at any node is the average of all true values belonging to the data at the given
    node.***Initially, prediction at the root node will simply be the average of *y*
    values of all 100 rows.'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`value=37.605`对应当前节点的预测值。***任何节点的预测值都是该节点数据的所有真实值的平均值。***最初，根节点的预测值将仅仅是所有100行*y*值的平均值。'
- en: The horizontal gray lines on both sides of the split represent the predictions
    for the left partition and right partition.
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 划分两侧的水平灰线表示左分区和右分区的预测值。
- en: '*Optional exercise: Copy the two cells we used above and replace the original
    df with either of the left partition or right partition data frames. You can see
    how subsequent splits are being made for them in the exact same manner. Just the
    input data frame changes and everything else (the core logic) remains the same.*'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '*可选练习：复制我们上面使用的两个单元格，并将原始数据框替换为左分区或右分区数据框。你可以看到后续的划分是如何以完全相同的方式进行的。只是输入数据框改变了，其他（核心逻辑）保持不变。*'
- en: 'Following is how the current state of our regression tree looks like after
    repeating the code for the left and right branches:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是当前回归树的状态，经过对左分支和右分支重复代码后的样子：
- en: '![](../Images/d62bca613bb2a02fc49dc5cd82399ce3.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d62bca613bb2a02fc49dc5cd82399ce3.png)'
- en: 'Source: Image by author'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 来源：作者提供的图片
- en: 'I hope it makes it easier now to interpret a regression tree that looks like
    below:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 我希望这使得解读如下回归树变得更容易：
- en: '![](../Images/66be38215d1db309390ee7675456e1a8.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/66be38215d1db309390ee7675456e1a8.png)'
- en: 'Source: Image by author'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 来源：作者提供的图片
- en: The above process gets repeated for the subsequent partitions until a stopping
    criterion is met (*such as max depth reached, no further splitting possible, etc.*).
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 上述过程会对后续的分区重复进行，直到满足停止准则（*例如达到最大深度、无法进一步划分等*）。
- en: Generalize the code using recursion
  id: totrans-72
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用递归来概括代码
- en: Let’s wrap the above code inside a function `build_tree` that we can call recursively
    to build the regression tree.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将上述代码封装到一个函数`build_tree`中，我们可以递归调用它来构建回归树。
- en: '**Helper functions:**'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '**辅助函数：**'
- en: '`mean_squared_error:` Returns the mean squared error for the given actual values
    and predicted values. Defined above.'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '`mean_squared_error:` 返回给定实际值和预测值的均方误差。如上所定义。'
- en: '`get_best_params:` Returns the *best_params* dictionary that consists of the
    feature and value to use for splitting the data at the current node as well as
    contains the prediction for the current node.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '`get_best_params:` 返回包含特征和用于在当前节点拆分数据的值的*best_params*字典，并且包含当前节点的预测值。'
- en: '[PRE6]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '`build_tree:` It is the main driver function that utilizes helper functions
    to build the regression tree recursively. I have added parameters such as *max_depth*
    so that the growth of the regression tree can be controlled to avoid *overfitting,*
    and also to serve as a stopping criterion.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '`build_tree:` 它是一个主要的驱动函数，利用辅助函数递归地构建回归树。我添加了如*max_depth*等参数，以便可以控制回归树的生长，以避免*过拟合*，并作为停止准则。'
- en: '[PRE7]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Now let’s call the above function on our training data.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们在训练数据上调用上述函数。
- en: '[PRE8]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '`Cell Output:`'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '`Cell Output:`'
- en: '![](../Images/040e716f15379f1b7c9b4cfd6900ff83.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/040e716f15379f1b7c9b4cfd6900ff83.png)'
- en: The above output corresponds to the regression tree given `max_depth=3`. For
    more clarity, refer to the following diagram of our regression tree.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 上述输出对应于设置了`max_depth=3`的回归树。为了更清晰，请参阅下图中的回归树。
- en: '![](../Images/4e105d294cee92b19d44e14666843a86.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4e105d294cee92b19d44e14666843a86.png)'
- en: 'Final regression tree (Source: Image by author)'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 最终回归树（来源：作者图片）
- en: 'The leaf nodes in the above regression tree correspond to the prediction values.
    On plotting the predictions and decision boundaries as per our final regression
    tree, we will get something like this:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 上述回归树中的叶子节点对应于预测值。根据我们的最终回归树绘制预测值和决策边界时，我们会得到如下结果：
- en: '![](../Images/a31de8df23e5853c4bfd77269cfc5792.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a31de8df23e5853c4bfd77269cfc5792.png)'
- en: 'Final predictions and decision boundaries (Source: Image by author)'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 最终预测和决策边界（来源：作者图片）
- en: Link to Code
  id: totrans-90
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 代码链接
- en: You can find the full notebook [here](https://github.com/gurjinderbassi/Machine-Learning/blob/main/Regression%20Trees%20from%20scratch.ipynb).
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在[这里](https://github.com/gurjinderbassi/Machine-Learning/blob/main/Regression%20Trees%20from%20scratch.ipynb)找到完整的笔记本。
- en: The moment of truth — Implementing regression tree using scikit-learn and comparing
    the final tree with ours
  id: totrans-92
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 关键时刻 — 使用scikit-learn实现回归树，并将最终树与我们的进行比较
- en: Now we need to know whether our understanding of the working of regression trees
    is accurate or not. To do that, let’s use scikit-learn to train a regression tree
    on the same data and look at the final results as follows.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们需要知道我们对回归树工作原理的理解是否准确。为此，让我们使用scikit-learn在相同的数据上训练回归树，并查看最终结果如下。
- en: '[PRE9]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '`Cell Output:`'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '`Cell Output:`'
- en: '![](../Images/e9916d94eb682b1221740c708c618dc4.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e9916d94eb682b1221740c708c618dc4.png)'
- en: 'Source: Image by author'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 来源：作者图片
- en: The regression tree returned by scikit-learn’s DecisionTreeRegressor is the
    exact same as the one we created previously.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: scikit-learn的DecisionTreeRegressor返回的回归树与我们之前创建的完全相同。
- en: Not a mystery anymore!
  id: totrans-99
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 不再是谜！
- en: I hope that staying so far in this article has been valuable for you and hopefully
    this one line of code i.e., `regressor.fit(X, y)` is no longer a mystery. Since
    we are now well aware of how the underlying algorithm has been fabricated, we
    can be more thoughtful while tweaking our tree-based regression models in the
    future.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 我希望到目前为止，本文对你有所帮助，希望这行代码`regressor.fit(X, y)`不再是一个谜。既然我们现在对底层算法的构造有了充分了解，我们可以在未来调整基于树的回归模型时更加深思熟虑。
- en: This is not the end
  id: totrans-101
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 这还不是结束
- en: The purpose of this article was to solely demonstrate the logic used by a decision
    tree regressor to make the optimal split at a node. However, it’s not a comprehensive
    guide on decision trees since there are many other important aspects that also
    need to be taken care of while modeling. It will be unfair if we don’t discuss
    them at all, so providing a brief summary of some points here.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 本文的目的是仅仅演示决策树回归器在节点处做出最佳分割所使用的逻辑。然而，这不是关于决策树的全面指南，因为在建模时还有许多其他重要方面需要关注。如果我们完全不讨论这些方面是不公平的，因此在这里提供一些要点的简要总结。
- en: '**Advantages of decision trees:**'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '**决策树的优点：**'
- en: Decision trees are *interpretable* and *intuitive* since one can get a clear
    idea of what led to a specific prediction.
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 决策树是*可解释的*和*直观的*，因为人们可以清楚地了解导致特定预测的原因。
- en: They do not demand much preprocessing of the training data, and can intrinsically
    handle categorical features and missing values.
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它们不需要对训练数据进行大量预处理，并且可以内在处理类别特征和缺失值。
- en: '**Disadvantages of decision trees:**'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '**决策树的缺点：**'
- en: Decision trees are sensitive to small variations in the data.
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 决策树对数据的小变化非常敏感。
- en: When not controlled properly, decision trees are highly prone to *overfitting,*
    and suitable measures need to be taken to prevent the tree from overgrowth so
    that they generalize well.
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果控制不当，决策树非常容易*过拟合*，需要采取适当措施以防止树的过度生长，从而使其具有良好的泛化能力。
- en: Conclusion
  id: totrans-109
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: In this article, we learned to build a regression tree from scratch to thoroughly
    understand the logic behind it. We generated a simple dataset with one continuous
    feature *X* and target *y*, that was used to train the decision tree regressor.
    It was followed by communicating the approach taken by the regression tree at
    each node to determine the best split via a flowchart so that it is easier for
    us to write code with having clear picture of the sequence of steps. We visualized
    the first best split returned by the code and discussed how the same logic can
    be extended further to build the entire tree.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我们从零开始构建了一个回归树，以全面理解其背后的逻辑。我们生成了一个包含一个连续特征 *X* 和目标 *y* 的简单数据集，该数据集用于训练决策树回归器。接着，我们通过流程图传达了回归树在每个节点所采取的方法，以确定最佳分裂，使我们能够更清晰地编写代码并了解步骤的顺序。我们可视化了代码返回的第一个最佳分裂，并讨论了如何将相同的逻辑扩展到构建整个树。
- en: We also trained a decision tree regressor using scikit-learn on the same data
    and noticed that it produced the same results as we did previously from scratch.
    The goal of this article was to look at what exactly is going on in the backend
    when we call `.fit()` on our data to train a DecisionTreeRegressor model from
    scikit-learn.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还使用 scikit-learn 在相同的数据上训练了一个决策树回归器，并注意到它产生的结果与我们之前从零开始得到的结果相同。本文的目标是了解在调用
    `.fit()` 来训练 scikit-learn 的 DecisionTreeRegressor 模型时，后台究竟发生了什么。
- en: Hopefully, having this knowledge is going to help us better tweak, tune, and
    interpret our tree-based regression models in the future.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 希望拥有这些知识将帮助我们在未来更好地调整、优化和解读基于树的回归模型。
- en: Thank you for reading :)
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 感谢阅读 :)
- en: Open to any feedback or suggestions!
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 欢迎任何反馈或建议！
- en: '**Want to learn more about decision trees?** Take a look at the following articles:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '**想了解更多关于决策树的信息？** 请查看以下文章：'
- en: '[](/how-does-a-decision-tree-know-the-next-best-question-to-ask-from-the-data-0d44c9433b06?source=post_page-----fbb908cf548b--------------------------------)
    [## How does a decision tree know the next best question to ask from the data?'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/how-does-a-decision-tree-know-the-next-best-question-to-ask-from-the-data-0d44c9433b06?source=post_page-----fbb908cf548b--------------------------------)
    [## 决策树如何知道从数据中询问下一个最佳问题？'
- en: Build your own decision tree (from scratch in Python) and understand how it
    uses entropy to split
  id: totrans-117
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 从零开始构建自己的决策树（使用 Python），并了解它如何利用熵来进行分裂
- en: towardsdatascience.com](/how-does-a-decision-tree-know-the-next-best-question-to-ask-from-the-data-0d44c9433b06?source=post_page-----fbb908cf548b--------------------------------)
    [](/entropy-and-gini-index-c04b7452efbe?source=post_page-----fbb908cf548b--------------------------------)
    [## Entropy and Gini Index
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/how-does-a-decision-tree-know-the-next-best-question-to-ask-from-the-data-0d44c9433b06?source=post_page-----fbb908cf548b--------------------------------)
    [](/entropy-and-gini-index-c04b7452efbe?source=post_page-----fbb908cf548b--------------------------------)
    [## 熵与基尼指数
- en: Understanding how these measures help us quantify uncertainty in a dataset
  id: totrans-119
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 理解这些度量如何帮助我们量化数据集中的不确定性
- en: towardsdatascience.com](/entropy-and-gini-index-c04b7452efbe?source=post_page-----fbb908cf548b--------------------------------)
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/entropy-and-gini-index-c04b7452efbe?source=post_page-----fbb908cf548b--------------------------------)
