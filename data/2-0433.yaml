- en: Building a Comment Toxicity Ranker Using Hugging Face’s Transformer Models
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Hugging Face的Transformer模型构建评论毒性排序器
- en: 原文：[https://towardsdatascience.com/building-a-comment-toxicity-ranker-using-hugging-faces-transformer-models-aa5b4201d7c6](https://towardsdatascience.com/building-a-comment-toxicity-ranker-using-hugging-faces-transformer-models-aa5b4201d7c6)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/building-a-comment-toxicity-ranker-using-hugging-faces-transformer-models-aa5b4201d7c6](https://towardsdatascience.com/building-a-comment-toxicity-ranker-using-hugging-faces-transformer-models-aa5b4201d7c6)
- en: Catching up on NLP and LLM (Part I)
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 赶上NLP和LLM（第一部分）
- en: '[](https://medium.com/@jacky.kaub?source=post_page-----aa5b4201d7c6--------------------------------)[![Jacky
    Kaub](../Images/e66c699ee5a9d5bbd58a1a72d688234a.png)](https://medium.com/@jacky.kaub?source=post_page-----aa5b4201d7c6--------------------------------)[](https://towardsdatascience.com/?source=post_page-----aa5b4201d7c6--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----aa5b4201d7c6--------------------------------)
    [Jacky Kaub](https://medium.com/@jacky.kaub?source=post_page-----aa5b4201d7c6--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@jacky.kaub?source=post_page-----aa5b4201d7c6--------------------------------)[![Jacky
    Kaub](../Images/e66c699ee5a9d5bbd58a1a72d688234a.png)](https://medium.com/@jacky.kaub?source=post_page-----aa5b4201d7c6--------------------------------)[](https://towardsdatascience.com/?source=post_page-----aa5b4201d7c6--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----aa5b4201d7c6--------------------------------)
    [Jacky Kaub](https://medium.com/@jacky.kaub?source=post_page-----aa5b4201d7c6--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----aa5b4201d7c6--------------------------------)
    ·18 min read·Aug 6, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page-----aa5b4201d7c6--------------------------------)
    ·18分钟阅读·2023年8月6日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/138fd036a0427c78ad23c3b834c93d74.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/138fd036a0427c78ad23c3b834c93d74.png)'
- en: Photo by [Brett Jordan](https://unsplash.com/@brett_jordan?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：[Brett Jordan](https://unsplash.com/@brett_jordan?utm_source=medium&utm_medium=referral)于[Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: Introduction
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: As a Data Scientist, I have never had the opportunity to properly explore the
    latest progress in Natural Language Processing. With the summer and the new boom
    of Large Language Models since the beginning of the year, I decided it was time
    to dive deep into the field and embark on some mini-projects. After all, there
    is never a better way to learn than by practicing.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一名数据科学家，我从未有机会深入探索自然语言处理的最新进展。随着夏季和今年初大语言模型的新热潮，我决定是时候深入这个领域并开始一些小项目。毕竟，没有比实践更好的学习方法了。
- en: As my journey started, I realized it was complicated to find content that takes
    the reader by the hand and goes, one step at a time, towards a deep comprehension
    of new NLP models with concrete projects. This is how I decided to start this
    new series of articles.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在我的旅程开始时，我意识到很难找到能够手把手引导读者、一步一步深入理解新NLP模型并通过具体项目进行的内容。因此，我决定开始这一新系列的文章。
- en: Building a Comment Toxicity Ranker Using HuggingFace’s Transformer Models
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用HuggingFace的Transformer模型构建评论毒性排序器
- en: In this first article, we are going to take a deep dive into building a comment
    toxicity ranker. This project is inspired by the [“Jigsaw Rate Severity of Toxic
    Comments” competition](https://www.kaggle.com/competitions/jigsaw-toxic-severity-rating)
    which took place on Kaggle last year.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我们将深入探讨构建评论毒性排序器。这个项目灵感来源于去年在Kaggle上举办的[“Jigsaw毒性评论严重性评估”竞赛](https://www.kaggle.com/competitions/jigsaw-toxic-severity-rating)。
- en: The objective of the competition was to build a model with the capacity to determine
    which comment (out of two comments given as input) is the most toxic.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 竞赛的目标是构建一个能够判断哪个评论（在给定的两个评论中）最具毒性的模型。
- en: To do so, the model will attribute to every comment passed as input a score,
    which determines its relative toxicity.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 为此，模型会为每个输入的评论分配一个分数，以确定其相对毒性。
- en: What this article will cover
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 本文涵盖内容
- en: In this article, we are going to train our first NLP Classifier using Pytorch
    and Hugging Face transformers. I will not go into the details of how works transformers,
    but more into practical details and implementations and initiate some concepts
    that will be useful for the next articles of the series.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我们将使用Pytorch和Hugging Face transformers训练我们的第一个NLP分类器。我不会深入讲解transformers的工作原理，而是更多地关注实际细节和实现，并引入一些对系列后续文章有用的概念。
- en: 'In particular, we will see:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，我们将看到：
- en: How to download a model from Hugging Face Hub
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何从 Hugging Face Hub 下载模型
- en: How to customize and use an Encoder
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何自定义和使用编码器
- en: Build and train a Pytorch ranker from one of the Hugging Face models
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从 Hugging Face 模型中构建并训练一个 Pytorch 排名器
- en: This article is directly addressed to data scientists that would like to step
    their game in NLP from a practical point of view. I will not do much theory around
    transformers and even if I will write code in detail, I am expected that you already
    played a bit with PyTorch in the past.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 本文直接面向希望从实际角度提升其自然语言处理技能的数据科学家。我不会详细讲解变换器的理论，即使我会详细编写代码，也希望你已经对 PyTorch 有一些了解。
- en: Exploration and Architecture
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索与架构
- en: The Training Dataset
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练数据集
- en: We will work on a dataset that pairs comments and classifies them as one being
    “less toxic” and one being “more toxic”.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将处理一个将评论配对并将其分类为“较少毒性”和“更多毒性”的数据集。
- en: The choice of relative toxicity has been made by a group of labelers.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 相对毒性的选择是由一组标注者做出的。
- en: The figure below shows a sample of data from the training set. The worker field
    represents the id of the labeler that made the classification.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 下图显示了来自训练集的数据样本。工人字段表示进行分类的标注者的 id。
- en: '![](../Images/14c039579153181c775d353a0ce968ef.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/14c039579153181c775d353a0ce968ef.png)'
- en: Training set sample, Author Illustration
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 训练集样本，作者插图
- en: 'Note: The dataset is available under an Open Source Licence, according to [the
    Kaggle competition rules](https://www.kaggle.com/competitions/jigsaw-toxic-severity-rating/rules).'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：数据集在开源许可证下提供，遵循[Kaggle 竞赛规则](https://www.kaggle.com/competitions/jigsaw-toxic-severity-rating/rules)。
- en: The ranking system
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 排名系统
- en: In any ML project, comprehending the task holds paramount significance as it
    significantly impacts the selection of an appropriate model and strategy. This
    understanding should be established right from the project’s kick-off.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在任何机器学习项目中，理解任务具有至关重要的意义，因为它显著影响了合适模型和策略的选择。这种理解应从项目启动时就建立起来。
- en: In this particular project, our objective is to construct a ranking system.
    Instead of predicting a specific target, our focus is on determining an arbitrary
    value that facilitates efficient comparison between pairs of samples.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个具体的项目中，我们的目标是构建一个排名系统。与其预测一个具体的目标，我们的重点是确定一个任意值，以便在样本对之间进行有效比较。
- en: Let’s begin by sketching a basic diagram to represent the concept, knowing that
    we will go deeper into the workings of the “Model” later on.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们首先绘制一个基本的图示来表示这个概念，知道我们稍后会更深入地探讨“模型”的工作原理。
- en: '![](../Images/22b9222296a975270c41dc9347545820.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/22b9222296a975270c41dc9347545820.png)'
- en: A very basic view of what we want to achieve
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 我们想要实现的一个非常基本的视图
- en: Visualizing the task this way is crucial as it demonstrates that the project’s
    objective goes beyond training a simple binary classifier based on the training
    data. Instead of simply predicting a value of 0 or 1 to identify the most toxic
    comment, the ranking system aims to assign arbitrary values that allow efficient
    comparison between comments.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 以这种方式可视化任务至关重要，因为它表明项目的目标不仅仅是基于训练数据训练一个简单的二分类器。与其仅仅预测0或1来识别最有毒的评论，排名系统旨在分配任意值，从而有效地比较评论。
- en: Model Training & Margin Ranking Loss
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型训练与边际排名损失
- en: Considering the “Model” remains a black box Neural Network, we need to establish
    a way to utilize this system and leverage our training data to update the model’s
    weights. To achieve this, we need a suitable loss function.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到“模型”仍然是一个黑箱神经网络，我们需要建立一种利用这个系统并利用我们的训练数据来更新模型权重的方法。为此，我们需要一个合适的损失函数。
- en: Given that our goal is to build a ranking system, the [Margin Ranking Loss](https://pytorch.org/docs/stable/generated/torch.nn.MarginRankingLoss.html)
    is a relevant choice. This loss function is inspired by the hinge loss, which
    is commonly used to optimize maximum margins between samples.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于我们的目标是构建一个排名系统，[边际排名损失](https://pytorch.org/docs/stable/generated/torch.nn.MarginRankingLoss.html)是一个相关的选择。这个损失函数受到铰链损失的启发，后者通常用于优化样本之间的最大边际。
- en: The Margin Ranking Loss operates on pairs of samples. For each pair, it compares
    the scores produced by the “Model” for the two samples and enforces a margin between
    them. The margin indicates the desired difference in scores between correctly
    ranked samples.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 边际排名损失对样本对进行操作。对于每一对样本，它比较“模型”对两个样本产生的分数，并强制它们之间有一个边际。这个边际表示正确排序的样本之间期望的分数差异。
- en: '![](../Images/ea8968cc4c66377b1036e886f2383d8a.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ea8968cc4c66377b1036e886f2383d8a.png)'
- en: The Margin Ranking Loss Function formula, Author Illustration
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: Margin Ranking Loss 函数公式，作者插图
- en: In the formula above, x1 and x2 are the ranking score of two samples, and y
    is a coefficient equal to 1 if x1 should be ranked higher than x2, otherwise -1\.
    “margin” is a hyperparameter of the formula which sets a minimum marges to reach.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述公式中，x1 和 x2 是两个样本的排名得分，y 是一个系数，如果 x1 应该排名高于 x2，则 y 等于 1，否则为 -1。 “margin”
    是公式的超参数，设置了需要达到的最小间隔。
- en: 'Let’s have a look at how works this loss function:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看这个损失函数是如何工作的：
- en: 'Assuming y=1, which means the sample associated with x1 should be ranked higher
    than the sample associated with x2:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 假设 y=1，这意味着与 x1 相关的样本应该比与 x2 相关的样本排名更高：
- en: If (x1 — x2) > margin, the score of sample 1 is higher than the score of sample
    2 by a sufficient marge, and the right term of the max() is negative. The loss
    returned will then be equal to 0, and there is no penalty associated with those
    two ranks.
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果 (x1 — x2) > margin，样本 1 的得分比样本 2 的得分高出足够的间隔，则 max() 的右侧项为负数。返回的损失将等于 0，并且这两个排名之间没有惩罚。
- en: If (x1 — x2) < margin, it means that the margins between x1 and x2 are not sufficient,
    or worst, that the score of x2 is higher than the score of x1\. In that case,
    the loss will be higher as the score of sample 2 is higher compared to the score
    of sample 1, which will penalize the model.
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果 (x1 — x2) < margin，这意味着 x1 和 x2 之间的间隔不足，或者更糟的是，x2 的得分高于 x1 的得分。在这种情况下，损失会更高，因为样本
    2 的得分高于样本 1 的得分，这会惩罚模型。
- en: 'With this in mind, we can now revise our training methodology as followed:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于此，我们现在可以按照如下修订我们的训练方法：
- en: 'For a sample (or a batch) in the Training set:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 对于训练集中的一个样本（或一个批次）：
- en: Forward-pass the more_toxic message(s) to the Model, get Rank_Score1 (x1)
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将 more_toxic 消息传递给模型，得到 Rank_Score1 (x1)
- en: Forward-pass the less_toxic message(s) to the Model, get Rank_Score2 (x2)
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将 less_toxic 消息传递给模型，得到 Rank_Score2 (x2)
- en: Compute the MarginRankingLoss with y = 1
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算 y = 1 时的 MarginRankingLoss
- en: Update the weight of the model based on the computed loss (backpropagation step)
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据计算出的损失更新模型的权重（反向传播步骤）
- en: '![](../Images/64ef5386783c2c1de22401c8f01997c4.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/64ef5386783c2c1de22401c8f01997c4.png)'
- en: Training step of the Model with the Margin Ranking Loss, Author Illustration
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 Margin Ranking Loss 的模型训练步骤，作者插图
- en: 'From Text to Features Representation: the Encoder block'
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从文本到特征表示：编码器块
- en: 'Our training procedure is now set up. It’s time to go deeper into the ‘Model’
    component himself. In the world of NLP, you’ll often come across three primary
    types of models: encoders, decoders, and encoder-decoder combinations. In this
    series of articles, we’ll examine these types of models more closely.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的训练程序现在已设置完成。是时候深入了解‘模型’组件本身了。在 NLP 的世界里，你会经常遇到三种主要类型的模型：编码器、解码器和编码器-解码器组合。在这一系列文章中，我们将更详细地研究这些类型的模型。
- en: For the purposes of this specific article, our requirement is a model that can
    transform a message into a feature vector. This vector serves as the input to
    generate the final ranking score. This feature vector will be directly derived
    from the Encoder of a transformer architecture.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 对于本特定文章的目的，我们需要一个可以将消息转换为特征向量的模型。这个向量作为输入生成最终的排名得分。这个特征向量将直接从变换器架构的编码器中派生。
- en: I won’t dive into the theory here as others have explained it way better (I
    recommend [the introduction class](https://huggingface.co/learn/nlp-course/chapter1/1)
    from Hugging Face which is very well written). Just keep in mind that the key
    part of this process is something called the attention mechanism. It helps transformers
    make sense of the text by looking at other related words, even if they’re far
    apart.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 我不会在这里深入理论，因为其他人已经解释得很好（我推荐 Hugging Face 的[入门课程](https://huggingface.co/learn/nlp-course/chapter1/1)，写得非常好）。只需记住这个过程的关键部分叫做注意力机制。它通过查看其他相关词，即使它们相隔很远，也帮助变换器理解文本。
- en: With this architecture in place, we will be able to adjust the weights that
    produce the best vector representation of our texts to identify the most important
    features for our task, and simply connect the final layer from the transformer
    to a final node (called the “head”) that will produce the final rank score.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这种架构，我们将能够调整权重，以生成我们文本的最佳向量表示，从而识别出对任务最重要的特征，并将最终层从变换器连接到一个最终节点（称为“头”），该节点将生成最终的排名得分。
- en: 'Let’s update our diagram accordingly:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们相应地更新我们的图示：
- en: '![](../Images/4c4db09f498c5f162c1b9e069d0a994f.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4c4db09f498c5f162c1b9e069d0a994f.png)'
- en: An updated view of our training pipeline, Author illustration
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 我们训练流水线的更新视图，作者插图
- en: The Tokenizer
  id: totrans-64
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分词器
- en: 'As you can see from the graph above, another component appeared inside of the
    model which we did not mention yet: a preprocessing step.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你从上述图表中看到的，模型内部出现了一个我们尚未提及的组件：预处理步骤。
- en: This preprocessing step is here to convert the raw text into something that
    can be passed through a Neural Network (numbers), and this is the role of the
    Tokenizer.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 这个预处理步骤旨在将原始文本转换为可以通过神经网络处理的内容（数字），而这就是分词器的作用。
- en: 'The tokenizer does two main things: splitting (= cutting the text into pieces,
    which can be words, part of words, or just letters) and indexing (= mapping each
    piece of text to a unique value, referenced in a dictionary so the operation can
    be reversed).'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 分词器的主要功能有两个：分割（即将文本切割成片段，这些片段可以是单词、单词的一部分或字母）和索引（即将每个文本片段映射到一个唯一的值，该值在字典中引用，以便可以反向操作）。
- en: One really important thing to keep in mind is that there are multiple ways to
    tokenize a text, but if you use a pre-trained model, you need to use the same
    Tokenizer or the pre-trained weights will be meaningless (due to different splitting
    and indexing).
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 需要记住的一件非常重要的事情是，文本的分词方式有多种，但如果你使用预训练模型，你需要使用相同的分词器，否则预训练权重将毫无意义（由于不同的分割和索引）。
- en: 'Another important thing is to remember that the encoder is nothing more than
    a Neural Network. As such, its input needs to be of a fixed size, which is not
    necessarily the case for your input text. The Tokenizer allows you to control
    the size of your token vector via two operations: padding and truncation. This
    is also an important parameter to consider because some pre-trained models will
    use a smaller, or larger, input space.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个重要的事情是要记住，编码器只是一个神经网络。因此，它的输入需要是固定大小的，但你的输入文本不一定符合这一点。分词器允许你通过两个操作来控制你的词向量的大小：填充和截断。这也是一个重要的参数，因为一些预训练模型会使用更小或更大的输入空间。
- en: In the figure below, we add the Tokenizer and we show how the message in transformed
    from module to module.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的图中，我们添加了分词器，并展示了消息如何从模块到模块进行转换。
- en: '![](../Images/10ba9651d96992fd16ec09b76f2c6f4f.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/10ba9651d96992fd16ec09b76f2c6f4f.png)'
- en: Final training diagram, Author Illustration
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 最终训练示意图，作者插图
- en: 'And that’s it, we have exposed here all the components we need to know in order
    to efficiently tackle our “Comment Toxic Ranking” task. To summarize the graph
    above: each pair of messages (the less toxic and the more toxic) will be passed
    individually to the model pipeline. They will first pass through the Tokenizer,
    the Encoder, and the ranking layer to produce a pair of scores. This pair of scores
    will be then used to compute the Margin Ranking Loss, which will be used during
    the backpropagation step to upload the weights to the encoder and the final ranking
    layer and optimize them for the task.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 就这样，我们已经揭示了所有需要了解的组件，以便有效地处理我们的“评论毒性排名”任务。总结上述图表：每对消息（较少毒性和较多毒性）将分别传递给模型流水线。它们将首先经过分词器、编码器和排名层，以产生一对分数。这对分数将用于计算边际排名损失，这将用于反向传播步骤中，更新编码器和最终排名层的权重，并优化它们以完成任务。
- en: In the next part, we are going to put our hands in the code and build the above
    pipeline using the Hugging Face transformers module and Pytorch.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一部分，我们将亲自动手编写代码，使用 Hugging Face transformers 模块和 Pytorch 构建上述流水线。
- en: Build, train, and evaluate the model
  id: totrans-75
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建、训练和评估模型
- en: We have covered the theory in the previous part, it is now time to put our hands
    in the dirt and work on our model.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在前面的部分中涵盖了理论，现在是时候亲自动手，开始处理我们的模型了。
- en: While building and training a complex deep-learning model could have been complicated
    in the past, the new modern frameworks.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然过去构建和训练复杂的深度学习模型可能很复杂，但新的现代框架使其变得更简单。
- en: Hugging Face is all you need
  id: totrans-78
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Hugging Face 是你所需的一切
- en: Hugging Face is an amazing company that is working on democratizing complex
    deep-learning models.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: Hugging Face 是一家了不起的公司，致力于使复杂的深度学习模型民主化。
- en: They build abstractions that help you build, load, fine-tune and share complex
    transformers models.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 它们构建了帮助你构建、加载、微调和共享复杂变换器模型的抽象。
- en: In the coming section, we are going to use their **transformers** package, which
    provides all the necessary tools to build pre-trained NLP models and use them
    for your own tasks. In the coming weeks, we are going to explore more in detail
    the different possibilities offered by the package
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的部分中，我们将使用他们的**transformers**包，该包提供了构建预训练NLP模型并用于自己任务所需的所有工具。在接下来的几周内，我们将更详细地探索该包提供的不同可能性
- en: The package is compatible with both TensorFlow and PyTorch libraries.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 该包与TensorFlow和PyTorch库兼容。
- en: To start, let’s install the transformers package
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们安装transformers包
- en: '[PRE0]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The models available from Hugging Face are available from their [Model Hub](https://huggingface.co/models)
    on their website. You can find all types of models as well as descriptions to
    understand what the model does, how many parameters, what datasets it has been
    trained on, etc…
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 从Hugging Face获取的模型可以在他们的[Model Hub](https://huggingface.co/models)网站上找到。你可以找到各种类型的模型以及描述，以了解模型的功能、参数数量、训练数据集等。
- en: In this article, we are going to use the architecture [roberta-base](https://huggingface.co/roberta-base)
    which is a relatively light encoder trained on several English corpus.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们将使用架构[roberta-base](https://huggingface.co/roberta-base)，这是一个相对轻量的编码器，经过多个英文语料库的训练。
- en: 'The model description tells us a lot of very important information that is
    relevant to our task:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 模型描述提供了大量与我们的任务相关的非常重要的信息：
- en: The model has 125M parameters
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 该模型具有125M个参数
- en: The model has been trained on several English corpus, which is important as
    our comment dataset is in English
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 该模型已在多个英文语料库上进行过训练，这一点很重要，因为我们的评论数据集是英文的
- en: It has been trained on a Mask Language Modeling objective, which consists of
    trying to predict words masked in a text and using both the text before and after
    to make its prediction, which is not always the case (models like GPT only use
    the context from before the word to predict for example as they don’t have to
    the future of the sentence when they infer a new text).
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 该模型已经在掩蔽语言模型的目标上进行过训练，这一目标是尝试预测文本中被掩蔽的单词，并使用前后的文本进行预测，这并非总是如此（例如，GPT等模型只使用单词前的上下文来进行预测，因为它们在推断新文本时无法看到句子的未来）。
- en: The model is case sensitive, which means it will make a difference between “WORD”
    and “word”. This is particularly important in the case of a toxicity detector
    as letter capitalization is an important clue of toxicity.
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 该模型对大小写敏感，这意味着它会区分“WORD”和“word”。这在毒性检测器中尤为重要，因为字母的大小写是判断毒性的一个重要线索。
- en: 'Hugging Face can provide for each model the tokenizer used as well as the base
    NN in different configurations (you might not want all the weights: sometimes
    you want to restrict yourself to the encoder part, and the decoder part, stop
    at the hidden layer, etc..).'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: Hugging Face可以为每个模型提供使用的分词器以及不同配置的基本神经网络（你可能不希望所有的权重：有时你只想限制在编码器部分，解码器部分，停留在隐藏层等）。
- en: Models available from the Hugging Face hub can be cloned locally (which will
    make them faster to run) or loaded directly in your code, by using its repo id
    (for example roberta-base in our case)
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 从Hugging Face hub获取的模型可以在本地克隆（这样运行会更快）或直接在代码中加载，通过使用其repo id（例如我们案例中的roberta-base）
- en: Loading and testing the Tokenizer
  id: totrans-94
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 加载和测试分词器
- en: To load the tokenizer, we can simply use the AutoTokenizer class from the transformers
    package, and specify which tokenizer we want to use
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 要加载分词器，我们可以简单地使用transformers包中的AutoTokenizer类，并指定我们想要使用的分词器
- en: '[PRE1]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: In order to tokenize a text, we can simply call the “encode” or “encode_plus”
    methods. The “encode_plus” will not only provide you with the tokenized version
    of your text but also an attention mask, which will be used to ignore the part
    of the encoding which is purely padding.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 为了对文本进行分词，我们可以简单地调用“encode”或“encode_plus”方法。“encode_plus”不仅会提供你文本的分词版本，还会提供一个注意力掩码，用于忽略纯填充部分的编码。
- en: '[PRE2]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Will return a dictionary, where “input_ids” is the encoded sequence, and “attention_mask”
    is used to allow the transformer to ignore the padded tokens:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 将返回一个字典，其中“input_ids”是编码序列，“attention_mask”用于允许变换器忽略填充的标记：
- en: '[PRE3]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Among the parameters we use, we have:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们使用的参数中，有：
- en: 'max_length: states the maximum length of the encoded sequence'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'max_length: 指定编码序列的最大长度'
- en: 'add_special_tokens: adds a <start> and <end> token to the text'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'add_special_tokens: 向文本中添加<start>和<end>标记'
- en: 'truncation: slices the text if it does not fit the max_length'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'truncation: 如果文本不适合max_length，则会截断文本'
- en: 'padding: add padding token up to the max_length'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'padding: 添加填充标记直到 max_length'
- en: Loading a pre-train model
  id: totrans-106
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 加载预训练模型
- en: To load a pre-train model, Hugging Face provides multiple classes depending
    on your need (are you working with TensorFlow or Pytorch? What type of task are
    you trying to achieve).
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 要加载预训练模型，Hugging Face 提供了多个类，具体取决于你的需求（你是在使用 TensorFlow 还是 Pytorch？你尝试实现什么类型的任务）。
- en: In our case, we will work with AutoModel, which allows you to load a model architecture
    together with pre-trained weights directly. Note that if you work with TensorFlow,
    you can achieve the same by using the TFAutoModel class instead of the AutoModel
    class.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的案例中，我们将使用 AutoModel，它允许你直接加载模型架构及预训练权重。请注意，如果你使用 TensorFlow，你可以通过使用 TFAutoModel
    类而不是 AutoModel 类来实现相同的功能。
- en: The AutoModel class will directly load the model architecture from [RobertaModel](https://huggingface.co/docs/transformers/model_doc/roberta#robertamodel)
    and load the pre-trained weights associated with the “roberta-base” repo in Hugging
    Face.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: AutoModel 类将直接从 [RobertaModel](https://huggingface.co/docs/transformers/model_doc/roberta#robertamodel)
    加载模型架构，并加载与 Hugging Face 中的 “roberta-base” 仓库相关联的预训练权重。
- en: 'As for the Tokenizer, we can directly load the model from the repo-id or from
    the path of a local repository, by using the from_pretrained method from AutoModel:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 至于 Tokenizer，我们可以直接从 repo-id 或本地仓库路径加载模型，通过使用 AutoModel 的 from_pretrained 方法：
- en: '[PRE4]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Note that the encoder has not been trained on a particular task on its own,
    and we cannot use simply the model as it is. Instead, we will have to fine-tune
    it with our dataset.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，编码器没有在特定任务上进行训练，我们不能简单地使用模型。相反，我们需要用我们的数据集进行微调。
- en: 'We can double-check that robertaBase is an instance of pytorch.nn.Module, and
    can be integrated into a more complex PyTorch architecture:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以再三检查 robertaBase 是否是 pytorch.nn.Module 的实例，并且可以集成到更复杂的 PyTorch 架构中：
- en: '[PRE5]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'You can also check its architecture by simply doing a print like you would
    do with a standard PyTorch module:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以通过简单地打印它来检查其架构，就像你对待标准 PyTorch 模块一样：
- en: '[PRE6]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Build a custom Neural Network
  id: totrans-117
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 构建自定义神经网络
- en: This last layer is actually a vector representation of the whole text we were
    discussing in the first part of this article, and we just have to connect it to
    a final node used as a ranker to complete our NN architecture.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 这个最后的层实际上是我们在本文第一部分讨论的整个文本的向量表示，我们只需将其连接到用于排序的最终节点，以完成我们的神经网络架构。
- en: To do so, we will simply build our own custom module by encapsulating nn.Module,
    as we would do with a classic NN with PyTorch.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 为此，我们将通过封装 nn.Module 来简单地构建自己的自定义模块，就像我们用 PyTorch 构建经典神经网络一样。
- en: '[PRE7]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'A few things to note here in the forward() method:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 在 forward() 方法中需要注意几点：
- en: We pass two main inputs to the robertBase model, input_ids and attention_mask.
    They were both generated by the Tokenizer.
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将两个主要输入传递给 robertBase 模型，input_ids 和 attention_mask。它们都是由 Tokenizer 生成的。
- en: The AutoModel has parameters (like output_hidden_states). Depending on the parameters
    you chose, you can make the model behave as an encoder, or a decoder and custom
    the model for different NLP tasks
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: AutoModel 具有参数（如 output_hidden_states）。根据你选择的参数，你可以让模型作为编码器或解码器运行，并将模型定制用于不同的
    NLP 任务。
- en: 'Did you notice that we pass output[1] in the dropout? This is because the base
    model provides two inputs:'
  id: totrans-124
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你是否注意到我们在 dropout 中传递了 output[1]？这是因为基本模型提供了两个输入：
- en: First, the last hidden state, which contains a contextual representation (or
    contextual embedding) of each token that can be used for tasks like entity recognition
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 首先，最后的隐藏状态，它包含每个标记的上下文表示（或上下文嵌入），可以用于实体识别等任务。
- en: Second, the output from the Pooler, which contains a vector representation of
    the whole text, is what we are looking for here.
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 其次，来自 Pooler 的输出，它包含整个文本的向量表示，就是我们在这里寻找的。
- en: Build a custom Dataset
  id: totrans-127
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 构建自定义数据集
- en: With Pytorch, we need also to create our own Dataset class, which will be used
    to store the raw data, and a DataLoader, which will be used to feed the neural
    network by batches during training.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 Pytorch，我们还需要创建自己的 Dataset 类，用于存储原始数据，以及 DataLoader，用于在训练过程中按批次馈送神经网络。
- en: 'When building a custom dataset with Pytorch, you must implement two mandatory
    methods:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用 Pytorch 构建自定义数据集时，你必须实现两个强制性方法：
- en: __len__, which gives the size of the training data (important information for
    the data loader)
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: __len__，它给出训练数据的大小（对数据加载器来说是重要信息）
- en: __getitem__, which takes a raw input (from row “i”) and preprocess it so it
    can be handled by the neural network (as tensor)
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: __getitem__，它接受原始输入（来自第“i”行）并进行预处理，以便神经网络（作为张量）可以处理
- en: 'If you recall the diagram from the previous part, we are actually passing in
    parallel two inputs two the models before computing the loss: the less_toxic and
    the more_toxic.'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你记得之前部分的图示，我们实际上是在计算损失之前并行传递两个输入到模型中：less_toxic 和 more_toxic。
- en: The __getitem__ method will handle the tokenization of the message and prepare
    the input for the transformer, converting the tokenized inputs as tensors.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: __getitem__ 方法将处理消息的分词，并为转换器准备输入，将分词后的输入转换为张量。
- en: '[PRE8]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: We can now generate the DataLoader which will be used for the batch training
    of the model.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以生成 DataLoader，用于模型的批量训练。
- en: '[PRE9]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: batch_size specify the amount of sample to be loaded for the forward pass/backpropagation
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: batch_size 指定了用于前向传递/反向传播的样本数量
- en: shuffle = True means the dataset is shuffled between two epochs
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: shuffle = True 意味着数据集在两个 epoch 之间会被打乱
- en: drop_last means that if the last batch has not had the right amount of samples,
    it will be dropped. This can be important as batch normalization does not work
    well with incomplete batch.
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: drop_last 意味着如果最后一个 batch 没有正确数量的样本，它将被丢弃。这一点很重要，因为 batch normalization 对于不完整的
    batch 处理效果不好。
- en: Training the model
  id: totrans-140
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练模型
- en: We are almost there, it's time to prepare the training routing for one epoch.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 我们快完成了，现在是时候为一个 epoch 准备训练流程了。
- en: '**Custom Loss**'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '**自定义损失**'
- en: To start with, let’s define a custom loss to be used. Pytorch already provides
    the MarginRankingLoss, we are simply going to encapsulate it with y = 1 (as we
    will always pass more_toxic as x1 and less_toxic as x2.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们定义一个自定义损失函数。 Pytorch 已经提供了 MarginRankingLoss，我们只是将其封装为 y = 1（因为我们将始终将
    more_toxic 作为 x1，less_toxic 作为 x2）。
- en: '[PRE10]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '**Optimizer**'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '**优化器**'
- en: For this experiment, we will go with a classic AdamW, which is currently state-of-the-art,
    and fix some of the problems from the original Adam implementation.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个实验，我们将使用经典的 AdamW，它目前是最先进的，并解决了原始 Adam 实现的一些问题。
- en: '[PRE11]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '**Scheduler**'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '**调度器**'
- en: The scheduler helps adapt the learning rate. During the start, we want a higher
    learning rate to converge faster to an optimum solution, and toward the end of
    the training, we want a much smaller learning rate to really fine-tune the weights.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 调度器有助于调整学习率。在开始时，我们希望较高的学习率以更快地收敛到最佳解，而在训练结束时，我们希望较小的学习率以真正微调权重。
- en: '[PRE12]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '**Training Routine**'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '**训练例程**'
- en: We are now ready to train our NLP model for toxic comment ranking.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在准备好训练我们的 NLP 模型以进行毒性评论排序。
- en: 'The training of an epoch is quite straightforward with Pytorch:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 Pytorch 训练一个 epoch 非常简单：
- en: We iterate through our data loader, which shuffles and selects the pre-processed
    data from the dataset
  id: totrans-154
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们迭代通过我们的数据加载器，它会从数据集中打乱并选择预处理的数据
- en: We retrieve the tokens and the masks from the data loader
  id: totrans-155
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们从数据加载器中提取 tokens 和 masks
- en: We calculate the rank of each message by making a forward pass to our model
  id: totrans-156
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们通过对模型进行前向传递来计算每条消息的排名
- en: When both ranks are calculated, we can compute the MarginRankingLoss (to use
    for the backpropagation), as well as an accuracy score which tells the % of pairs
    that are correctly classified (for reference only)
  id: totrans-157
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当两个排名都计算完毕后，我们可以计算 MarginRankingLoss（用于反向传播），以及一个准确率分数，表示正确分类的对数百分比（仅供参考）
- en: We update our system (backpropagation, optimizer, and scheduler)
  id: totrans-158
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们更新我们的系统（反向传播、优化器和调度器）
- en: We iterate until all the data in the data loader has been used.
  id: totrans-159
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们迭代直到数据加载器中的所有数据都被使用完。
- en: '[PRE13]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: I trained the model on a GPU T4 from Kaggle, which brought me to an honorable
    score of 70% of comments correctly classified. I can probably gain accuracy by
    playing more with the different parameters and using more epochs, but it is good
    enough for the purpose of this article.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 我在 Kaggle 的 GPU T4 上训练了模型，使我获得了 70% 的评论正确分类的可观成绩。我可能通过调整不同的参数和使用更多的 epochs 提高准确性，但这对于本文的目的来说已经足够了。
- en: A final word on inference
  id: totrans-162
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 关于推断的最后一点
- en: The framework we put in place works well for training from a set of comments
    pre-formatted as in our training set.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 我们建立的框架在从预格式化的评论集合中训练时效果很好。
- en: But it won’t work during a “production” scenario where you will receive a bunch
    of messages for which you need to evaluate the toxicity score.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 但在“生产”场景下，这种方法就不起作用了，因为你会接收到一堆需要评估毒性评分的消息。
- en: '![](../Images/c98f4de433da1d57aefea5cef4971130.png)'
  id: totrans-165
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c98f4de433da1d57aefea5cef4971130.png)'
- en: An example of a dataset in production mode, where we only get single messages,
    not pairs of messages
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个生产模式下的数据集示例，在这种模式下，我们只接收单条消息，而不是消息对。
- en: 'For inference, you will design another Dataset class and another DataLoader
    which will be slightly different from what we did before:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 对于推断，你将设计另一个 Dataset 类和另一个 DataLoader，这些将与我们之前做的有所不同：
- en: '[PRE14]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'What has changed:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 变化了什么：
- en: We are not loading pair of messages anymore, but single messages
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们不再加载消息对，而是单条消息。
- en: The Loader is not shuffling the data (this is very important if you don’t want
    bad surprises with random scores associated with your original vector)
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Loader 没有对数据进行打乱（如果你不想要与原始向量关联的随机分数带来不好的惊喜，这一点非常重要）。
- en: As there is no batch norm calculation and as we want all the data to be inferred,
    we set drop_last to False to get all batches, even incomplete ones
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于没有批量归一化计算，并且我们希望对所有数据进行推断，我们将 drop_last 设置为 False，以获取所有批次，即使是未完成的批次。
- en: 'And finally, to produce the ranked score:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，为了生成排序分数：
- en: '[PRE15]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: This is the top 5 classified messages after inference. In order to stay politically
    correct, I had to apply a bit of censorship here…
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 这是推断后的前 5 条分类消息。为了保持政治正确，我在这里进行了些许审查…
- en: '![](../Images/cad3789ed07b0b481df419785eeec209.png)'
  id: totrans-176
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/cad3789ed07b0b481df419785eeec209.png)'
- en: The most toxic messages identified
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 已识别的最具毒性的消息
- en: Not very constructive… :)
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 不太具有建设性… :)
- en: Conclusion
  id: totrans-179
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: In this article, we leveraged a Hugging Face pre-trained model and Pytorch to
    produce a model able to rank the level of toxicity of messages.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我们利用了 Hugging Face 预训练模型和 Pytorch 生产了一个能够对消息的毒性等级进行排序的模型。
- en: To do so, we took a “Roberta” transformer (a small one) and connected a final
    simple node at the end of its encoder with PyTorch. The rest was more classic
    and probably similar to other projects you might have done if you are already
    familiar with PyTorch.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 为此，我们采用了一个“小型”的“Roberta”变换器，并使用 PyTorch 在其编码器末尾连接了一个最终简单的节点。其余部分则更为经典，可能与你之前用
    PyTorch 做的其他项目类似。
- en: This project is an initiation around the possibilities that offer NLP and I
    wanted to start simply to introduce some basic concepts that are required to go
    further and play with more challenging tasks or much larger models.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 这个项目是对 NLP 提供的可能性的初步探索，我想简单地介绍一些基础概念，以便进一步研究更具挑战性的任务或更大的模型。
- en: I hope you enjoyed reading, if you want to play with the model you can download
    a Notebook from [my GitHub](https://github.com/jkaub/toxicity-ranker).
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 希望你喜欢阅读，如果你想玩玩这个模型，你可以从 [我的 GitHub](https://github.com/jkaub/toxicity-ranker)
    下载一个 Notebook。
