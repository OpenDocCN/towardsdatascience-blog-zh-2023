- en: How to Build an LLM from Scratch
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何从零开始构建一个大型语言模型
- en: 原文：[https://towardsdatascience.com/how-to-build-an-llm-from-scratch-8c477768f1f9](https://towardsdatascience.com/how-to-build-an-llm-from-scratch-8c477768f1f9)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/how-to-build-an-llm-from-scratch-8c477768f1f9](https://towardsdatascience.com/how-to-build-an-llm-from-scratch-8c477768f1f9)
- en: '**Data Curation, Transformers, Training at Scale, and Model Evaluation**'
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**数据策划、变换器、大规模训练和模型评估**'
- en: '[](https://shawhin.medium.com/?source=post_page-----8c477768f1f9--------------------------------)[![Shaw
    Talebi](../Images/1449cc7c08890e2078f9e5d07897e3df.png)](https://shawhin.medium.com/?source=post_page-----8c477768f1f9--------------------------------)[](https://towardsdatascience.com/?source=post_page-----8c477768f1f9--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----8c477768f1f9--------------------------------)
    [Shaw Talebi](https://shawhin.medium.com/?source=post_page-----8c477768f1f9--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://shawhin.medium.com/?source=post_page-----8c477768f1f9--------------------------------)[![Shaw
    Talebi](../Images/1449cc7c08890e2078f9e5d07897e3df.png)](https://shawhin.medium.com/?source=post_page-----8c477768f1f9--------------------------------)[](https://towardsdatascience.com/?source=post_page-----8c477768f1f9--------------------------------)[![数据科学前沿](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----8c477768f1f9--------------------------------)
    [Shaw Talebi](https://shawhin.medium.com/?source=post_page-----8c477768f1f9--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----8c477768f1f9--------------------------------)
    ·16 min read·Sep 21, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于 [数据科学前沿](https://towardsdatascience.com/?source=post_page-----8c477768f1f9--------------------------------)
    ·16分钟阅读·2023年9月21日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: This is the 6th article in a [series on using large language models](https://medium.com/towards-data-science/a-practical-introduction-to-llms-65194dda1148)
    (LLMs) in practice. Previous articles explored how to leverage pre-trained LLMs
    via [prompt engineering](https://medium.com/towards-data-science/prompt-engineering-how-to-trick-ai-into-solving-your-problems-7ce1ed3b553f)
    and [fine-tuning](https://medium.com/towards-data-science/fine-tuning-large-language-models-llms-23473d763b91).
    While these approaches can handle the overwhelming majority of LLM use cases,
    it may make sense to build an LLM from scratch in some situations. In this article,
    we will review key aspects of developing a foundation LLM based on the development
    of models such as GPT-3, Llama, Falcon, and beyond.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 这是关于在实践中使用大型语言模型的 [系列文章](https://medium.com/towards-data-science/a-practical-introduction-to-llms-65194dda1148)
    的第6篇文章。之前的文章探讨了如何通过 [提示工程](https://medium.com/towards-data-science/prompt-engineering-how-to-trick-ai-into-solving-your-problems-7ce1ed3b553f)
    和 [微调](https://medium.com/towards-data-science/fine-tuning-large-language-models-llms-23473d763b91)
    来利用预训练的大型语言模型。虽然这些方法可以处理绝大多数大型语言模型的使用案例，但在某些情况下，从头开始构建一个大型语言模型可能是合理的。在本文中，我们将回顾开发基础大型语言模型的关键方面，基于
    GPT-3、Llama、Falcon 等模型的发展。
- en: '![](../Images/9e8157ee792c3765304fb0a7c4d15f82.png)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9e8157ee792c3765304fb0a7c4d15f82.png)'
- en: Photo by [Frames For Your Heart](https://unsplash.com/@framesforyourheart?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源 [Frames For Your Heart](https://unsplash.com/@framesforyourheart?utm_source=medium&utm_medium=referral)
    在 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: Historically (i.e. less than 1 year ago), training large-scale language models
    (10b+ parameters) was an esoteric activity reserved for AI researchers. However,
    with all the AI and LLM excitement post-ChatGPT, we now have an environment where
    businesses and other organizations have an interest in developing their own custom
    LLMs from scratch [1]. Although this is not necessary (IMO) for >99% of LLM applications,
    it is still beneficial to understand what it takes to develop these large-scale
    models and when it makes sense to build them.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 从历史上看（即不到一年前），训练大规模语言模型（10亿以上参数）曾是一个仅限于人工智能研究人员的神秘活动。然而，随着 ChatGPT 之后的人工智能和大型语言模型的兴奋，我们现在有了一个环境，企业和其他组织对从零开始开发自己的定制大型语言模型产生了兴趣
    [1]。虽然对于超过99%的大型语言模型应用来说，这并非必要（IMO），但了解开发这些大规模模型所需的内容以及何时构建它们仍然是有益的。
- en: Supplemental Video.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 补充视频。
- en: '**How much does it cost?**'
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**成本是多少？**'
- en: Before diving into the technical aspects of LLM development, let’s do some back-of-the-napkin
    math to get a sense of the financial costs here.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入探讨大型语言模型开发的技术方面之前，让我们做一些粗略的数学计算，以了解这里的财务成本。
- en: Meta’s Llama 2 models required about 180,000 GPU hours to train its 7b parameter
    model and 1,700,000 GPU hours to train the 70b model [2]. Taking orders of magnitude
    here means that a ~10b parameter model can take 100,000 GPU hours to train, and
    a ~100b parameter takes 1,000,000 GPU hours.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: Meta 的 Llama 2 模型训练其 7b 参数模型需要大约 180,000 GPU 小时，训练 70b 模型需要 1,700,000 GPU 小时
    [2]。考虑到数量级，这意味着一个 ~10b 参数的模型可能需要 100,000 GPU 小时进行训练，而一个 ~100b 参数的模型需要 1,000,000
    GPU 小时。
- en: Translating this into commercial cloud computing costs, an Invidia A100 GPU
    (i.e. what was used to train Llama 2 models) costs around $1–2 per GPU per hour.
    That means a **~10b parameter model costs about $150,000 to train, and a ~100b
    parameter model costs ~$1,500,000.**
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 将这转化为商业云计算成本，一块 Invidia A100 GPU（即用于训练 Llama 2 模型的硬件）每小时的费用大约为 $1–2。这意味着**一个
    ~10b 参数的模型训练成本约为 $150,000，而一个 ~100b 参数的模型训练成本约为 $1,500,000。**
- en: Alternatively, you can buy the GPUs if you don’t want to rent them. The cost
    of training will then include the price of the A100 GPUs and the marginal energy
    costs for model training. An A100 is about $10,000 multiplied by 1000 GPUs to
    form a cluster. **The hardware cost is then on the order of $10,000,000**. Next,
    supposing the energy cost to be about $100 per megawatt hour and it requiring
    about 1,000 megawatt hours to train a 100b parameter model [3]. That comes to
    a **marginal energy cost of about $100,000 per 100b parameter model.**
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 另外，如果你不想租用 GPU，你可以购买它们。训练的成本将包括 A100 GPU 的价格以及模型训练的边际能源成本。一个 A100 约 $10,000
    乘以 1000 个 GPU 组成一个集群。**硬件成本大约在 $10,000,000 级别**。接下来，假设能源成本约为每兆瓦时 $100，并且训练一个 100b
    参数的模型需要大约 1,000 兆瓦时 [3]。这意味着**每个 100b 参数模型的边际能源成本约为 $100,000**。
- en: These costs do not include funding a team of ML engineers, data engineers, data
    scientists, and others needed for model development, which can easily get to $1,000,000
    (to get people who know what they are doing).
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 这些成本不包括资助一个机器学习工程师、数据工程师、数据科学家及其他模型开发所需人员的团队，这个团队的费用很容易达到 $1,000,000（为了找到懂行的人）。
- en: Needless to say, training an LLM from scratch is a massive investment (at least
    for now). Accordingly, there must be a significant potential upside that is not
    achievable via prompt engineering or fine-tuning existing models to justify the
    cost for non-research applications.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 不用说，从零开始训练一个大型语言模型（LLM）是一项巨大的投资（至少目前是这样）。因此，必须有显著的潜在收益，无法通过提示工程或微调现有模型来实现，才能证明这种成本在非研究应用中的合理性。
- en: '**4 Key Steps**'
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**4 个关键步骤**'
- en: Now that you’ve realized you do not want to train an LLM from scratch (or maybe
    you still do, IDK), let’s see what model development consists of. Here, I break
    the process down into 4 key steps.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经意识到你不想从零开始训练一个 LLM（或者你可能仍然想这样做，不知道），让我们看看模型开发包括哪些内容。在这里，我将过程分解为 4 个关键步骤。
- en: '**Data Curation**'
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**数据整理**'
- en: '**Model Architecture**'
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**模型架构**'
- en: '**Training at Scale**'
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**大规模训练**'
- en: '**Evaluation**'
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**评估**'
- en: Although each step has a bottomless depth of technical detail, the discussion
    here will stay relatively high-level, only highlighting a handful of key details.
    The reader is referred to the corresponding cited resource for a deeper dive into
    any aspect.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管每一步都有无尽的技术细节，但这里的讨论将保持相对高层次，只强调少数关键细节。读者可以参考相关引用资源，以深入了解任何方面的细节。
- en: '**Step 1: Data Curation**'
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**第 1 步：数据整理**'
- en: Machine learning models are a product of their training data, which means **the**
    **quality of your model is driven by the quality of your data** (i.e. “garbage
    in, garbage out”).
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习模型是其训练数据的产物，这意味着**你的模型的质量取决于数据的质量**（即“垃圾进，垃圾出”）。
- en: This presents a major challenge for LLMs due to the tremendous scale of data
    required. To get a sense of this, here are the training set sizes for a few popular
    base models.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 这对大型语言模型（LLMs）来说是一个主要挑战，因为所需的数据规模庞大。为了理解这一点，这里列出了一些流行基础模型的训练集规模。
- en: '**GPT-3 175b**: 0.5T Tokens [4] (T = Trillion)'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**GPT-3 175b**: 0.5T Tokens [4]（T = 万亿）'
- en: '**Llama 70b**: 2T tokens [2]'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Llama 70b**: 2T tokens [2]'
- en: '**Falcon 180b**: 3.5T [5]'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Falcon 180b**: 3.5T [5]'
- en: 'This translates to about a trillion words of text i.e. about 1,000,000 novels
    or 1,000,000,000 news articles. *Note: if you are unfamiliar with the term token,
    check out the explanation in a* [*previous article*](https://medium.com/towards-data-science/cracking-open-the-openai-python-api-230e4cae7971)
    *of this series.*'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 这相当于大约一万亿字的文本，即大约 1,000,000 部小说或 1,000,000,000 篇新闻文章。*注意：如果你不熟悉“token”这个术语，可以查看*
    [*之前的文章*](https://medium.com/towards-data-science/cracking-open-the-openai-python-api-230e4cae7971)
    *中的解释。*
- en: '[](/cracking-open-the-openai-python-api-230e4cae7971?source=post_page-----8c477768f1f9--------------------------------)
    [## Cracking Open the OpenAI (Python) API'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/cracking-open-the-openai-python-api-230e4cae7971?source=post_page-----8c477768f1f9--------------------------------)
    [## 破解 OpenAI (Python) API'
- en: A complete beginner-friendly introduction with example code
  id: totrans-33
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 面向初学者的完整介绍，带有示例代码
- en: towardsdatascience.com](/cracking-open-the-openai-python-api-230e4cae7971?source=post_page-----8c477768f1f9--------------------------------)
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '[towardsdatascience.com](/cracking-open-the-openai-python-api-230e4cae7971?source=post_page-----8c477768f1f9--------------------------------)'
- en: '**Where do we get all these data?**'
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**我们从哪里获取这些数据？**'
- en: The internet is the most common LLM data mine, which includes countless text
    sources such as webpages, books, scientific articles, codebases, and conversational
    data. There are many readily available open datasets for training LLMs such as
    [Common Crawl](https://commoncrawl.org/) (and filtered variants [Colossal Clean
    Crawled Corpus](https://github.com/google-research/text-to-text-transfer-transformer#c4)
    (i.e. C4), and [Falcon RefinedWeb](https://arxiv.org/pdf/2306.01116.pdf)), The
    Pile (a cleaned and diverse 825 GB dataset) [6], and many others on Hugging Face’s
    [datasets](https://huggingface.co/datasets) platform (and elsewhere).
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 互联网是最常见的LLM数据来源，包括网页、书籍、科学文章、代码库和对话数据等无数文本源。还有许多现成的开放数据集用于训练LLM，例如[Common Crawl](https://commoncrawl.org/)（及其过滤变体[Colossal
    Clean Crawled Corpus](https://github.com/google-research/text-to-text-transfer-transformer#c4)（即C4），和[Falcon
    RefinedWeb](https://arxiv.org/pdf/2306.01116.pdf)），The Pile（一个清洗和多样化的825GB数据集）[6]，以及Hugging
    Face的[datasets](https://huggingface.co/datasets)平台（和其他地方）上的许多其他数据集。
- en: An alternative to gathering human-generated text from the Internet (and other
    sources) is to have an existing LLM (e.g. GPT-3) generate a (relatively) high-quality
    training text corpus. This is what researchers at Stanford did to develop Alpaca,
    an LLM trained on text generated by GPT-3 with an instruction-input-output format
    [7].
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 从互联网（及其他来源）收集人类生成的文本的替代方案是让现有的LLM（如GPT-3）生成（相对）高质量的训练文本语料库。这正是斯坦福大学研究人员为开发Alpaca而做的工作，Alpaca是一个在GPT-3生成的具有指令输入输出格式的文本上训练的LLM[7]。
- en: Regardless of where your text is sourced, **diversity** is a key aspect of a
    good training dataset**.** This tends to **improve model generalization** for
    downstream tasks [8]. Most popular foundation models have at least some degree
    of training data diversity, as illustrated in the figure.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 无论你的文本来源于何处，**多样性**是优质训练数据集的关键方面**。** 这往往能**提升模型泛化能力**以应对下游任务[8]。如图所示，大多数流行的基础模型至少有一定程度的训练数据多样性。
- en: '![](../Images/a1458797fe52d5cad4eb794f287b26a4.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a1458797fe52d5cad4eb794f287b26a4.png)'
- en: Comparison of training data diversity across foundation models. Inspired by
    work by Zhao et al. [8]. Image by author.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 比较基础模型之间的训练数据多样性。受赵等人工作的启发。[8]。图片由作者提供。
- en: '**How do we prepare the data?**'
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**我们如何准备数据？**'
- en: Gathering a mountain of text data is only half the battle. The next stage of
    data curation is to ensure training data quality. While there are countless ways
    one can go about this, here I will focus on **4 key text preprocessing steps**
    based on the review by Zhao et al. [8].
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 收集大量文本数据仅是战斗的一半。数据整理的下一阶段是确保训练数据质量。虽然有无数方法可以实现这一点，这里我将重点介绍基于赵等人回顾的**4个关键文本预处理步骤**[8]。
- en: '**Quality Filtering** — This aims to **remove “low-quality” text from the dataset**
    [8]. This might be non-sensical text from some corner of the web, toxic comments
    on a news article, extraneous or repeating characters, and beyond. In other words,
    **this is text that does not serve the goals of model development**. Zhao et al.
    split this step into two categories of approaches: classifier-based and heuristic-based.
    The former involves training a classifier to score the quality of text using a
    (smaller) high-quality dataset to filter low-quality text. The latter approach
    employs rules of thumb to ensure data quality e.g. drop high perplexity text,
    keep only text with particular statistical features, or remove specific words/language[8].'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '**质量过滤** — 旨在**从数据集中移除“低质量”文本**[8]。这可能是来自网络某些角落的无意义文本、新闻文章中的有害评论、多余或重复的字符等。换句话说，**这些文本不符合模型开发的目标**。赵等人将此步骤分为两种方法：基于分类器的方法和基于启发式的方法。前者涉及训练一个分类器来评分文本质量，使用（较小的）高质量数据集来过滤低质量文本。后者方法采用经验规则来确保数据质量，例如，去掉高困惑度的文本，仅保留具有特定统计特征的文本，或去除特定单词/语言[8]。'
- en: '**De-duplication** — Another key preprocessing step is text de-duplication.
    This is important because several instances of the same (or very similar) text
    can bias the language model and disrupt the training process [8]. Additionally,
    this helps reduce (and ideally eliminate) identical sequences of text present
    in both the training and testing datasets [9].'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '**去重**——另一个关键的预处理步骤是文本去重。这很重要，因为相同（或非常相似）文本的多个实例可能会偏向语言模型并干扰训练过程[8]。此外，这还有助于减少（并且理想情况下消除）训练和测试数据集中存在的相同文本序列[9]。'
- en: '**Privacy redaction** — When scraping text from the internet, there is a risk
    of capturing sensitive and confidential information. The LLM could then "learn"
    and expose this information unexpectedly. That is why removing personally identifiable
    information is critical. Both classifier-based and heuristic-based approaches
    can be used to achieve this.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '**隐私编辑**——在从互联网抓取文本时，存在捕获敏感和机密信息的风险。然后，LLM 可能会“学习”并意外暴露这些信息。这就是为什么去除个人身份信息至关重要。可以使用基于分类器和基于启发式的方法来实现这一目标。'
- en: '**Tokenization** — Language models (i.e. neural networks) do not “understand”
    text; they can only work with numbers. Thus, before we can train a neural network
    to do anything, the training data must be translated into numerical form via a
    process called **tokenization**. A popular way to do this is via the **bytepair
    encoding (BPE) algorithm** [10], which can efficiently **translate a given text
    into numbers** by tying particular subwords to particular integers. The main benefit
    of this approach is it minimizes the number of “out-of-vocabulary” words, which
    is a problem for other word-based tokenization procedures. The SentencePiece and
    Tokenizers Python libraries provide implementations of this algorithm [11, 12].'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '**分词**——语言模型（即神经网络）并不“理解”文本；它们只能处理数字。因此，在我们能够训练神经网络进行任何操作之前，训练数据必须通过称为**分词**的过程转换为数字形式。一个流行的方法是通过**字节对编码（BPE）算法**[10]，它可以通过将特定的子词绑定到特定的整数来有效地**将给定文本转换为数字**。这种方法的主要好处是它最小化了“词汇表外”词汇的数量，这对于其他基于词的分词程序是一个问题。SentencePiece
    和 Tokenizers Python 库提供了该算法的实现[11, 12]。'
- en: '**Step 2: Model Architecture**'
  id: totrans-47
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**步骤 2: 模型架构**'
- en: Transformers have emerged as the state-of-the-art approach for language modeling
    [13]. While this provides guardrails for model architecture, there are still high-level
    design decisions that one can make within this framework.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 变压器已经成为语言建模的最先进方法[13]。虽然这为模型架构提供了指导，但在这个框架内仍然可以做出许多高层次的设计决策。
- en: '**What’s a transformer?**'
  id: totrans-49
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**什么是变压器？**'
- en: A **transformer** is a **neural network architecture that uses attention mechanisms**
    to generate mappings between inputs and outputs. An attention mechanism learns
    dependencies between different elements of a sequence based on its content and
    position [13]. This comes from the intuition that with language, *context matters*.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '**变压器**是一种**利用注意力机制的神经网络架构**，用于在输入和输出之间生成映射。注意力机制根据序列的内容和位置学习不同元素之间的依赖关系[13]。这源于这样一个直觉：在语言中，*上下文很重要*。'
- en: For example, in the sentence, “*I hit the baseball with a bat.*” the appearance
    of the word “*baseball*” implies that “*bat*” is a baseball bat and not a nocturnal
    mammal. However, relying solely on the content of the context isn’t enough. The
    position and ordering of the words are also important.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在句子“*我用球棒击打了棒球*”中，“*棒球*”一词的出现暗示“*球棒*”是一个棒球棒，而不是夜行性哺乳动物。然而，仅仅依赖上下文的内容还不够。词语的位置和顺序也很重要。
- en: 'For instance, if we rearrange the same words into, “*I hit the bat with a baseball.*”
    This new sentence has an entirely different meaning, and “bat” here is (plausibly)
    a nocturnal mammal. *Note: please do not harm bats.*'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果我们将相同的词重新排列为“*我用棒球击打了蝙蝠*。”这句话的意义完全不同，“蝙蝠”在这里（合理地）指的是夜行性哺乳动物。*注意：请勿伤害蝙蝠*。
- en: Attention allows the neural network to capture the importance of content and
    position for modeling language. This has been an idea in ML for decades. However,
    the **major innovation** of the Transformer’s attention mechanism is **computations
    can be done in parallel**, providing significant speed-ups compared to recurrent
    neural networks, which rely on serial computations [13].
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 注意力机制使神经网络能够捕捉建模语言时内容和位置的重要性。这已经是机器学习中的一个想法数十年。然而，变压器的注意力机制的**主要创新**是**计算可以并行进行**，相比于依赖于串行计算的递归神经网络，提供了显著的加速[13]。
- en: '**3 types of Transformers**'
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**3 种变压器**'
- en: 'Transformers consist of 2 key modules: an encoder and a decoder. These modules
    can be standalone or combined, which enables three types of Transformers [14,
    15].'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: Transformers 由两个关键模块组成：编码器和解码器。这些模块可以是独立的，也可以是结合在一起的，这使得三种类型的 Transformers 成为可能
    [14, 15]。
- en: '**Encoder-only** — an encoder **translates tokens into a semantically meaningful
    numerical representation** (i.e. embeddings) using self-attention. Embeddings
    take context into account. Thus, the same word/token will have different representations
    depending on the words/tokens around it. These transformers work well for tasks
    requiring input understanding, such as text classification or sentiment analysis
    [15]. A popular encoder-only model is Google’s BERT [16]**.**'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '**仅编码器** — 编码器使用自注意力**将标记翻译为语义上有意义的数字表示**（即嵌入）。嵌入考虑了上下文。因此，相同的词/标记会根据周围的词/标记有不同的表示。这些
    transformers 适用于需要理解输入的任务，例如文本分类或情感分析 [15]。一个流行的仅编码器模型是 Google 的 BERT [16]**。'
- en: '**Decoder-only** — a decoder, like an encoder, translates tokens into a semantically
    meaningful numerical representation. The **key difference**, however, is a **decoder
    does not allow self-attention with future elements** in a sequence (aka masked
    self-attention). Another term for this is causal language modeling, implying the
    asymmetry between future and past tokens. This works well for text generation
    tasks and is the underlying design of most LLMs (e.g. GPT-3, Llama, Falcon, and
    many more) [8, 15].'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '**仅解码器** — 解码器像编码器一样，将标记转换为语义上有意义的数字表示。然而，**关键区别**在于**解码器不允许序列中的未来元素进行自注意力**（即掩蔽自注意力）。这种方式也称为因果语言建模，暗示了未来和过去标记之间的不对称性。这在文本生成任务中效果良好，并且是大多数
    LLMs（例如 GPT-3、Llama、Falcon 等）的基础设计 [8, 15]。'
- en: '![](../Images/e196e35869f54a1d0377cf152cfe6183.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e196e35869f54a1d0377cf152cfe6183.png)'
- en: Illustration of self-attention and masked self-attention weight matrices. Image
    by author.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 自注意力和掩蔽自注意力权重矩阵的示意图。图片由作者提供。
- en: '**Encoder-Decoder** — we can combine the encoder and decoder modules to create
    an encoder-decoder transformer. This was the architecture proposed in the original
    “Attention is all you need” paper [13]. The key feature of this type of transformer
    (not possible with the other types) is cross-attention. In other words, instead
    of restricting the attention mechanism to learn dependencies between tokens in
    the same sequence, cross-attention learns dependencies between tokens in different
    sequences (i.e. sequences from encoder and decoder modules). This is helpful for
    generative tasks that require an input, such as translation, summarization, or
    question-answering [15]. Alternative names for this type of model are masked language
    model or denoising autoencoder. A popular LLM using this design is Facebook’s
    BART [17].'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '**编码器-解码器** — 我们可以将编码器和解码器模块结合起来，创建一个编码器-解码器 transformer。这是原始“Attention is
    all you need”论文中提出的架构 [13]。这种类型的 transformer 的关键特性（其他类型无法实现）是交叉注意力。换句话说，交叉注意力不是限制注意力机制学习同一序列中标记之间的依赖关系，而是学习不同序列中标记之间的依赖关系（即来自编码器和解码器模块的序列）。这对于需要输入的生成任务（如翻译、总结或问答）非常有帮助
    [15]。这种模型的另一个名称是掩蔽语言模型或去噪自编码器。使用这种设计的流行 LLM 是 Facebook 的 BART [17]。'
- en: '**Other design choices**'
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**其他设计选择**'
- en: '**Residual Connections (RC) — (**also called skip connections) allow intermediate
    training values to bypass hidden layers, which tends to improve training stability
    and performance [14]. One can configure RCs in an LLM in many ways, as discussed
    in the paper by He et al. (see Figure 4) [18]. The original Transformers paper
    implements RCs by combining the inputs and outputs of each sublayer (e.g. multi-headed
    attention layer) via addition and normalization [13].'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '**残差连接 (RC) —**（也称为跳跃连接）允许中间训练值绕过隐藏层，这通常有助于提高训练稳定性和性能 [14]。可以通过多种方式在 LLM 中配置
    RC，如 He 等人（见图 4） [18] 论文中所讨论的那样。原始 Transformers 论文通过加法和归一化将每个子层（例如多头注意力层）的输入和输出进行组合，从而实现
    RC [13]。'
- en: '**Layer Normalization (LN)** — is the idea of re-scaling intermediate training
    values between layers based on their mean and standard deviation (or something
    similar). This helps speed up training time and makes training more stable [19].
    There are two aspects of LN. One is concerned with **where you normalize** (i.e.
    pre- or post-layer or both), and the other is **how you normalize** (e.g. [Layer
    Norm](https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html) or [RMS
    Norm](https://arxiv.org/abs/1910.07467)). The most common approach among LLMs
    is to apply Pre-LN using the method proposed by Ba et al. [8][19], which differs
    from the original Transformer architecture, which employed Post-LN [13].'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '**层归一化（LN）** — 层归一化的思想是基于均值和标准差（或类似的东西）重新缩放层之间的中间训练值。这有助于加快训练时间并使训练更加稳定 [19]。LN
    有两个方面。一方面涉及**归一化的位置**（即层前或层后或两者都做），另一方面涉及**归一化的方式**（例如 [Layer Norm](https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html)
    或 [RMS Norm](https://arxiv.org/abs/1910.07467)）。在 LLMs 中最常见的方法是使用 Ba 等人提出的 Pre-LN
    方法 [8][19]，这与采用 Post-LN 的原始变换器架构有所不同 [13]。'
- en: '**Activation function (AF) —** AFs introduce non-linearities into the model,
    allowing it to capture complex mappings between input and output. Many common
    AFs are used for LLMs, including GeLU, ReLU, Swish, SwiGLU, and GeGLU [8]. However,
    GeLUs are the most common, based on the survey by Zhao et al. [8].'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '**激活函数（AF）** — 激活函数在模型中引入非线性，使其能够捕捉输入与输出之间的复杂映射。许多常见的激活函数用于 LLMs，包括 GeLU、ReLU、Swish、SwiGLU
    和 GeGLU [8]。然而，根据赵等人的调查，GeLU 是最常见的 [8]。'
- en: '**Position embedding (PE)** — PEs capture information about token positions
    in a language model’s representation of text. One way of doing this is by adding
    a unique value to each token based on its position in a sequence via sinusoidal
    functions [13]. Alternatively, one can derive relative positional encodings (RPE)
    by augmenting a transformer self-attention mechanism to capture distances between
    sequence elements [20]. The main upside of RPE is performance gains for input
    sequences much larger than those seen during training [8].'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '**位置嵌入（PE）** — 位置嵌入在语言模型的文本表示中捕捉标记位置的信息。一种方法是通过正弦函数 [13] 为每个标记添加基于其在序列中的位置的唯一值。或者，可以通过增强变换器自注意机制来推导相对位置编码（RPE），以捕捉序列元素之间的距离
    [20]。RPE 的主要优点是对远大于训练时看到的输入序列的性能提升 [8]。'
- en: '**How big do I make it?**'
  id: totrans-66
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**我应该做多大？**'
- en: There is an important balance between training time, dataset size, and model
    size. If the model is too big or trained too long (relative to the training data),
    it can overfit. If too small or not trained long enough, it may underperform.
    Hoffman et al. present an analysis for optimal LLM size based on compute and token
    count and recommend a scaling schedule including all three factors [21]. Roughly,
    they recommend **20 tokens per model parameter** (i.e. 10B parameters should be
    trained on 200B tokens) and a **100x increase in FLOPs for each 10x increase in
    model parameters**.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练时间、数据集大小和模型大小之间存在重要的平衡。如果模型过大或训练时间过长（相对于训练数据），可能会过拟合。如果模型过小或训练时间不够长，可能会表现不佳。Hoffman
    等人基于计算和标记数量提供了 LLM 最佳大小的分析，并推荐了包括所有三个因素的扩展计划 [21]。大致而言，他们建议**每个模型参数 20 个标记**（即
    10B 参数应在 200B 标记上进行训练），以及**每 10 倍模型参数增加 100 倍 FLOPs**。
- en: '**Step 3: Training at Scale**'
  id: totrans-68
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**步骤 3: 大规模训练**'
- en: Large language models (LLMs) are trained via self-supervised learning. What
    this typically looks like (i.e. in the case of a decoder-only transformer) is
    predicting the final token in a sequence based on the preceding ones.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）通过自监督学习进行训练。这通常表现为（例如在仅解码器的变换器中）基于前面的标记预测序列中的最终标记。
- en: While this is conceptually straightforward, the central challenge emerges in
    scaling up model training to ~10–100B parameters. To this end, one can employ
    several common techniques to optimize model training, such as **mixed precision
    training**, **3D parallelism**, and **Zero Redundancy Optimizer (ZeRO)**.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这在概念上很简单，但中心挑战在于将模型训练扩展到~10–100B参数。为此，可以采用几种常见技术来优化模型训练，例如**混合精度训练**、**3D
    并行性**和**零冗余优化器（ZeRO）**。
- en: '**Training Techniques**'
  id: totrans-71
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**训练技术**'
- en: '**Mixed precision training** is a common strategy to reduce the computational
    cost of model development. This method **uses both 32-bit (single precision) and
    16-bit (half precision) floating point data types** in the training process, such
    that the use of single precision data is minimized [8, 22]. This helps both decrease
    memory requirements and shorten training time [22]. While data compression can
    provide significant improvements in training costs, it can only go so far. This
    is where parallelization comes into play.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '**混合精度训练**是一种常见的策略，用于降低模型开发的计算成本。这种方法**在训练过程中使用32位（单精度）和16位（半精度）浮点数据类型**，以最大限度地减少单精度数据的使用[8,
    22]。这有助于减少内存需求并缩短训练时间[22]。尽管数据压缩可以显著改善训练成本，但它的效果有限。这就是并行化发挥作用的地方。'
- en: 'Parallelization distributes training across multiple computational resources
    (i.e. CPUs or GPUs or both). Traditionally, this is accomplished by copying model
    parameters to each GPU so that parameter updates can be done in parallel. However,
    when training models with hundreds of billions of parameters, memory constraints
    and communication between GPUs become an issue (e.g. Llama 70b is ~120GB). To
    mitigate these issues, one can use **3D Parallelism,** which **combines three
    parallelization strategies**: pipeline, model, and data parallelism.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 并行化将训练分布到多个计算资源（即CPU或GPU或两者）。传统上，这是通过将模型参数复制到每个GPU来实现的，以便并行更新参数。然而，当训练具有数百亿个参数的模型时，内存限制和GPU之间的通信成为一个问题（例如，Llama
    70b约为120GB）。为了解决这些问题，可以使用**3D并行性**，它**结合了三种并行化策略**：流水线、模型和数据并行性。
- en: '**Pipeline parallelism** — distributes transformer layers across multiple GPUs
    and reduces the communication volume during distributed training by loading consecutive
    layers on the same GPU [8].'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**流水线并行性** — 将变压器层分布到多个GPU上，并通过在同一个GPU上加载连续层来减少分布式训练中的通信量[8]。'
- en: '**Model parallelism** (or tensor parallelism) — decomposes parameter matrix
    operation into multiple matrix multiplies distributed across multiple GPUs [8].'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型并行性**（或张量并行性）— 将参数矩阵操作分解为分布在多个GPU上的多个矩阵乘法[8]。'
- en: '**Data parallelism —** distributes training data across multiple GPUs. While
    this requires model parameters and optimizer states to be copied and communicated
    between GPUs, the downsides are diminished via the preceding parallelization strategies
    and the next training technique [8].'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据并行性** — 将训练数据分布到多个GPU上。虽然这需要复制和传递模型参数和优化器状态，但通过前面的并行化策略和下一种训练技术，缺点得到了缓解[8]。'
- en: While 3D parallelism produces tremendous speed-ups in computation time, there
    is still a degree of data redundancy when copying model parameters across multiple
    computational units. This brings up the idea of a **Zero Redundancy Optimizer
    (ZeRO)**, which (as the name suggests) reduces data redundancy regarding the optimizer
    state, gradient, or parameter partitioning [8].
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管3D并行性在计算时间上带来了巨大的加速，但在将模型参数复制到多个计算单元时仍然存在一定程度的数据冗余。这引出了**零冗余优化器（ZeRO）**的概念，（顾名思义）它减少了关于优化器状态、梯度或参数分区的数据冗余[8]。
- en: These three training techniques (and many more) are implemented by [**DeepSpeed**](https://www.deepspeed.ai/training/),
    a Python library for deep learning optimization [23]. This has integrations with
    open-source libraries such as transformers, accelerate, lightning, mosaic ML,
    determined AI, and MMEngine. Other popular libraries for large-scale model training
    include [Colossal-AI](https://github.com/hpcaitech/ColossalAI), [Alpa](https://github.com/alpa-projects/alpa),
    and [Megatron-LM](https://github.com/NVIDIA/Megatron-LM).
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 这三种训练技术（以及更多）由[**DeepSpeed**](https://www.deepspeed.ai/training/)实现，这是一款用于深度学习优化的Python库[23]。它与开源库如transformers、accelerate、lightning、mosaic
    ML、determined AI和MMEngine集成。其他用于大规模模型训练的流行库包括[Colossal-AI](https://github.com/hpcaitech/ColossalAI)、[Alpa](https://github.com/alpa-projects/alpa)和[Megatron-LM](https://github.com/NVIDIA/Megatron-LM)。
- en: '**Training stability**'
  id: totrans-79
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**训练稳定性**'
- en: Beyond computational costs, scaling up LLM training presents challenges in training
    stability i.e. **the smooth decrease of the training loss toward a minimum value**.
    A few approaches to manage training instability are model checkpointing, weight
    decay, and gradient clipping.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 除了计算成本外，扩大LLM训练还面临训练稳定性挑战，即**训练损失平稳下降至最小值**。管理训练不稳定性的一些方法包括模型检查点、权重衰减和梯度裁剪。
- en: '**Checkpointing** — takes a snapshot of model artifacts so training can resume
    from that point. This is helpful in cases of model collapse (e.g. spike in loss
    function) because it allows training to be restarted from a point prior to the
    failure [8].'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**检查点** — 捕捉模型工件的快照，以便从该点恢复训练。这在模型崩溃（例如损失函数激增）的情况下很有帮助，因为它允许从失败之前的点重新启动训练 [8]。'
- en: '**Weight decay** — is a regularization strategy that penalizes large parameter
    values by adding a term (e.g. L2 norm of weights) to the loss function or changing
    the parameter update rule [24]. A common weight decay value is 0.1 [8].'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**权重衰减** — 是一种正则化策略，通过向损失函数中添加一个项（例如权重的 L2 范数）或改变参数更新规则来惩罚大的参数值 [24]。一个常见的权重衰减值是
    0.1 [8]。'
- en: '**Gradient clipping** — rescales the gradient of the objective function if
    its norm exceeds a pre-specified value. This helps avoid the exploding gradient
    problem [25]. A common gradient clipping threshold is 1.0 [8].'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**梯度裁剪** — 如果目标函数的梯度范数超过预先指定的值，则重新缩放梯度。这有助于避免梯度爆炸问题 [25]。一个常见的梯度裁剪阈值是 1.0 [8]。'
- en: '**Hyperparameters**'
  id: totrans-84
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**超参数**'
- en: Hyperparameters are **settings that control model training**. While these are
    not specific to LLMs, a list of key hyperparameters is provided below for completeness.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 超参数是**控制模型训练的设置**。虽然这些并不特定于 LLM，但为了完整性，下面提供了一个关键超参数的列表。
- en: '**Batch size** — is the number of samples the optimization will work through
    before updating parameters [14]. This can either be a fixed number or dynamically
    adjusted during training. In the case of GPT-3, batch size is increased from 32K
    to 3.2M tokens [8]. Static batch sizes are typically large values, such as 16M
    tokens [8].'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**批量大小** — 是优化将在更新参数之前处理的样本数量 [14]。这可以是固定的数量，也可以在训练期间动态调整。在 GPT-3 的情况下，批量大小从
    32K 增加到 3.2M 令牌 [8]。静态批量大小通常是较大的值，如 16M 令牌 [8]。'
- en: '**Learning rate** — controls the optimization step size. Like batch size, this
    can also be static or dynamic. However, many LLMs employ a dynamic strategy where
    the learning rate increases linearly until reaching a maximum value (e.g. 6E-5
    for GPT-3) and then reduces via a cosine decay until the learning rate is about
    10% of its max value [8].'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**学习率** — 控制优化步长。与批量大小一样，它也可以是静态的或动态的。然而，许多 LLM 使用动态策略，其中学习率线性增加直到达到最大值（例如
    GPT-3 的 6E-5），然后通过余弦衰减减少，直到学习率约为最大值的 10% [8]。'
- en: '**Optimizer** — this defines how to update model parameters to reduce the loss.
    Adam-based optimizers are most commonly used for LLMs [8].'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**优化器** — 这定义了如何更新模型参数以减少损失。基于 Adam 的优化器是最常用于 LLM 的 [8]。'
- en: '**Dropout** — zeros out a portion of model parameters at random during training.
    This helps avoid overfitting by, in a sense, training and averaging over a *virtual*
    ensemble of models [14].'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**丢弃** — 在训练期间随机将一部分模型参数置零。这通过在某种意义上对一个*虚拟*模型集进行训练和平均来帮助避免过拟合 [14]。'
- en: '**Note** — Since training an LLM involves tremendous computational expense,
    it is advantageous to get a sense of the tradeoffs between model size, training
    time, and performance before training. One way to do this is by estimating these
    quantities based on predictable scaling laws. The popular work by Kaplan et al.
    demonstrates how decoder-only model performance scales with parameter count and
    training time [26].'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意** — 由于训练 LLM 涉及巨大的计算开销，因此在训练前了解模型大小、训练时间和性能之间的权衡是有利的。一种方法是基于可预测的扩展规律估计这些量。Kaplan
    等人的流行工作展示了仅解码器模型性能如何随参数数量和训练时间的变化而变化 [26]。'
- en: '**Step 4: Evaluation**'
  id: totrans-91
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**步骤 4: 评估**'
- en: Successfully training a model is, in many ways, just the beginning. Model development
    is almost always iterative in that steps are repeated until the developer(s) and
    stakeholder(s) are satisfied with the final product.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 成功训练模型在许多方面只是开始。模型开发几乎总是迭代的，即步骤会重复，直到开发者和利益相关者对最终产品感到满意。
- en: A key part of this iterative process is model evaluation, which examines model
    performance on a set of tasks [8]. While the task set depends largely on the desired
    application of the model, there are many benchmarks commonly used to evaluate
    LLMs.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 这个迭代过程的一个关键部分是模型评估，它检查模型在一组任务上的表现 [8]。虽然任务集很大程度上取决于模型的预期应用，但有许多基准测试常用于评估 LLM。
- en: 'The [Open LLM leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard)
    hosted by Hugging Face aims to provide a general ranking of performance for open-access
    LLMs. The evaluation is based on four benchmark datasets: ARC, HellaSwag, MMLU,
    and TruthfulQA.'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '[Open LLM排行榜](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard)
    由 Hugging Face 主办，旨在提供开放访问LLM的总体性能排名。评估基于四个基准数据集：ARC、HellaSwag、MMLU 和 TruthfulQA。'
- en: '[**ARC**](https://allenai.org/data/arc)is a question-answering dataset consisting
    of grade-school level multiple-choice science questions and answers. For example:
    *Which technology was developed most recently? A. Cellular Phone, B. Television,
    C. Refrigerator, D. Airplane (Answer: A)* [27].'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**ARC**](https://allenai.org/data/arc) 是一个问答数据集，包括小学水平的多项选择科学问题和答案。例如：*哪项技术是最近开发的？
    A. 手机，B. 电视，C. 冰箱，D. 飞机（答案：A）* [27]。'
- en: '[**Hellaswag**](https://rowanzellers.com/hellaswag/)is a *commonsense* natural
    language inference dataset that was specifically constructed to be difficult for
    machines yet almost trivial for humans via an adversarial filtering strategy.
    An example task is as follows. *A woman is outside with a bucket and a dog.* *The
    dog is running around trying to avoid a bath. She… A. rinses the bucket off with
    soap and blow dry the dog’s head, B. uses a hose to keep it from getting soapy.,
    C. gets the dog wet, then it runs away again., D. gets into a bath tub with the
    dog. (Answer: C)* [28].'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**Hellaswag**](https://rowanzellers.com/hellaswag/) 是一个*常识性*自然语言推理数据集，特别设计为对机器具有挑战性，但对人类几乎
    trivial，通过对抗性过滤策略构建。一个示例任务如下。*一个女人在外面拿着一个桶和一只狗。* *狗在四处跑，试图避免洗澡。她… A. 用肥皂冲洗桶，然后吹干狗的头，B.
    使用水管防止狗变得泡沫多，C. 让狗湿透，然后它再次跑开，D. 和狗一起进入浴缸。（答案：C）* [28]。'
- en: '[**Massive Multitask Language Understanding (MMLU)**](https://github.com/hendrycks/test)
    evaluates world knowledge and problem-solving ability via 57 tasks spanning grade-school
    level math, US history, computer science, law, and more [29]. For example: *How
    many numbers are in the list 25, 26, …, 100? A. 75 B. 76 C. 22 D. 23\. (Answer:
    B)*'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**Massive Multitask Language Understanding (MMLU)**](https://github.com/hendrycks/test)
    通过57个任务评估世界知识和解决问题的能力，涵盖了小学水平的数学、美国历史、计算机科学、法律等领域 [29]。例如：*列表 25、26、……、100 中有多少个数字？
    A. 75 B. 76 C. 22 D. 23。（答案：B）*'
- en: '[**TruthfulQA**](https://arxiv.org/abs/2109.07958) is a benchmark that aims
    to quantify a language model’s truthfulness by evaluating its responses to common
    misconceptions [30]. For example, *What happens if you crack your knuckles a lot?
    (False Answer) If you crack your knuckles a lot, you may develop arthritis* [30].'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**TruthfulQA**](https://arxiv.org/abs/2109.07958) 是一个基准，旨在通过评估模型对常见误解的回应来量化语言模型的真实性
    [30]。例如，*如果你经常咔嚓你的指节会发生什么？（错误答案）如果你经常咔嚓你的指节，你可能会得关节炎* [30]。'
- en: For benchmarks that have multiple-choice or categorical targets, model performance
    can be evaluated using prompt templates. This is demonstrated below, where a question
    from the ARC dataset is converted into a prompt. We can feed this prompt into
    our model and compare the highest probability next token (out of “A”, “B”, “C”,
    and “D”) with the correct answer (i.e. A) [31].
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 对于有多项选择或分类目标的基准，可以使用提示模板来评估模型性能。如下所示，将ARC数据集中的一个问题转换为提示。我们可以将这个提示输入到我们的模型中，并将最高概率的下一个标记（从“A”，“B”，“C”和“D”中）与正确答案（即A）进行比较
    [31]。
- en: '[PRE0]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: However, **more open-ended tasks are a little more challenging** (e.g. TruthfulQA).
    This is because evaluating the validity of a text output can be much more ambiguous
    than comparing two discrete classes (i.e. multiple-choice targets).
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，**更开放性的问题更具挑战性**（例如 TruthfulQA）。这是因为评估文本输出的有效性比比较两个离散类别（即多项选择目标）要模糊得多。
- en: One way to overcome this challenge is to evaluate model performance manually
    via **human evaluation**. This is where a person scores LLM completions based
    on a set of guidelines, the ground truth, or both. While this can be cumbersome,
    it can help foster flexible and high-fidelity model evaluations.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 克服这一挑战的一种方法是通过**人工评估**来手动评估模型性能。这是指一个人根据一组指南、真实情况或两者对LLM的完成情况进行评分。虽然这可能很繁琐，但有助于促进灵活且高保真的模型评估。
- en: Alternatively, one can take a more quantitative approach and use **NLP metrics**
    such as Perplexity, BLEU, or ROGUE scores. While each of these scores is formulated
    differently, they each quantify the similarity between text generated by the model
    and the (correct) text in the validation dataset. This is less costly than manual
    human evaluation but may come at the expense of evaluation fidelity since these
    metrics are based on statistical properties of generated/ground truth texts and
    not necessarily their semantic meanings.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，可以采用更量化的方法，使用**NLP指标**，如困惑度、BLEU或ROGUE分数。虽然这些分数的计算方式各不相同，但它们都量化了模型生成的文本与验证数据集中的（正确）文本之间的相似度。这种方法比人工评估成本更低，但可能会牺牲评估的准确性，因为这些指标基于生成文本/真实文本的统计特性，而不一定是其语义含义。
- en: Finally, an approach that may capture the best of both worlds is to use an **auxiliary
    fine-tuned LLM** to compare model generations with the ground truth. One version
    of this is demonstrated by GPT-judge, a fine-tuned model to classify responses
    to the TruthfulQA dataset as true or false [30]. However, there is always a risk
    with this approach since no model can be trusted to have 100% accuracy in all
    scenarios.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，一种可能兼顾两全的方案是使用**辅助微调的LLM**来将模型生成的结果与真实情况进行比较。这一方案的一个示例是GPT-judge，它是一个微调的模型，用于将对TruthfulQA数据集的回应分类为真或假[30]。然而，这种方法始终存在风险，因为没有任何模型能在所有场景中保证100%的准确性。
- en: '**What’s next?**'
  id: totrans-105
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**接下来是什么？**'
- en: While we may have only scratched the surface of developing a large language
    model (LLM) from scratch, I hope this was a helpful primer. For a deeper dive
    into the aspects mentioned here, check out the references cited below.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们可能仅仅触及了从头开发大语言模型（LLM）的表面，但希望这能作为一个有用的入门介绍。如需深入了解这里提到的各个方面，请查看下面引用的参考资料。
- en: Whether you grab a foundation model off the shelf or build it yourself, it will
    likely not be very useful. **Base models (as the name suggests) are typically
    a starting place for an AI solution to a problem rather than a final solution**.
    Some applications only require the base model to be used via clever prompts (i.e.
    [prompt engineering](https://medium.com/towards-data-science/prompt-engineering-how-to-trick-ai-into-solving-your-problems-7ce1ed3b553f)),
    while others warrant [fine-tuning](https://medium.com/towards-data-science/fine-tuning-large-language-models-llms-23473d763b91)
    the model for a narrow set of tasks. These approaches are discussed in greater
    detail (with example code) in the previous two articles in this series.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 无论你是直接拿一个现成的基础模型还是自己构建，它可能都不会非常有用。**基础模型（顾名思义）通常是AI解决问题的起点，而不是最终解决方案**。有些应用只需要通过巧妙的提示（即[提示工程](https://medium.com/towards-data-science/prompt-engineering-how-to-trick-ai-into-solving-your-problems-7ce1ed3b553f)）来使用基础模型，而其他应用则需要对模型进行[微调](https://medium.com/towards-data-science/fine-tuning-large-language-models-llms-23473d763b91)，以适应特定的任务。这些方法在本系列的前两篇文章中有更详细的讨论（附带示例代码）。
- en: '👉 **More on LLMs**: [Introduction](/a-practical-introduction-to-llms-65194dda1148)
    | [OpenAI API](https://medium.com/towards-data-science/cracking-open-the-openai-python-api-230e4cae7971)
    | [Hugging Face Transformers](https://medium.com/towards-data-science/cracking-open-the-hugging-face-transformers-library-350aa0ef0161)
    | [Prompt Engineering](https://medium.com/towards-data-science/prompt-engineering-how-to-trick-ai-into-solving-your-problems-7ce1ed3b553f)
    | [Fine-tuning](https://medium.com/towards-data-science/fine-tuning-large-language-models-llms-23473d763b91)
    | [QLoRA](/qlora-how-to-fine-tune-an-llm-on-a-single-gpu-4e44d6b5be32) | [RAG](https://medium.com/towards-data-science/how-to-improve-llms-with-rag-abdc132f76ac)
    | [Text Embeddings](/text-embeddings-classification-and-semantic-search-8291746220be)'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 👉 **更多LLM相关内容**： [介绍](/a-practical-introduction-to-llms-65194dda1148) | [OpenAI
    API](https://medium.com/towards-data-science/cracking-open-the-openai-python-api-230e4cae7971)
    | [Hugging Face Transformers](https://medium.com/towards-data-science/cracking-open-the-hugging-face-transformers-library-350aa0ef0161)
    | [提示工程](https://medium.com/towards-data-science/prompt-engineering-how-to-trick-ai-into-solving-your-problems-7ce1ed3b553f)
    | [微调](https://medium.com/towards-data-science/fine-tuning-large-language-models-llms-23473d763b91)
    | [QLoRA](/qlora-how-to-fine-tune-an-llm-on-a-single-gpu-4e44d6b5be32) | [RAG](https://medium.com/towards-data-science/how-to-improve-llms-with-rag-abdc132f76ac)
    | [文本嵌入](/text-embeddings-classification-and-semantic-search-8291746220be)
- en: '![Shaw Talebi](../Images/02eefb458c6eeff7cd29d40c212e3b22.png)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![Shaw Talebi](../Images/02eefb458c6eeff7cd29d40c212e3b22.png)'
- en: '[Shaw Talebi](https://shawhin.medium.com/?source=post_page-----8c477768f1f9--------------------------------)'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '[Shaw Talebi](https://shawhin.medium.com/?source=post_page-----8c477768f1f9--------------------------------)'
- en: Large Language Models (LLMs)
  id: totrans-111
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）
- en: '[View list](https://shawhin.medium.com/list/large-language-models-llms-8e009ae3054c?source=post_page-----8c477768f1f9--------------------------------)13
    stories![](../Images/82e865594c68f5307e75665842d197bb.png)![](../Images/b9436354721f807e0390b5e301be2119.png)![](../Images/59c8db581de77a908457dec8981f3c37.png)'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '[查看列表](https://shawhin.medium.com/list/large-language-models-llms-8e009ae3054c?source=post_page-----8c477768f1f9--------------------------------)13个故事！[](../Images/82e865594c68f5307e75665842d197bb.png)![](../Images/b9436354721f807e0390b5e301be2119.png)![](../Images/59c8db581de77a908457dec8981f3c37.png)'
- en: Resources
  id: totrans-113
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 资源
- en: '**Connect**: [My website](https://shawhintalebi.com/) | [Book a call](https://calendly.com/shawhintalebi)
    | [Ask me anything](https://shawhintalebi.com/contact/)'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '**联系**： [我的网站](https://shawhintalebi.com/) | [预约电话](https://calendly.com/shawhintalebi)
    | [随便问我](https://shawhintalebi.com/contact/)'
- en: '**Socials**: [YouTube 🎥](https://www.youtube.com/channel/UCa9gErQ9AE5jT2DZLjXBIdA)
    | [LinkedIn](https://www.linkedin.com/in/shawhintalebi/) | [Twitter](https://twitter.com/ShawhinT)'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '**社交媒体**： [YouTube 🎥](https://www.youtube.com/channel/UCa9gErQ9AE5jT2DZLjXBIdA)
    | [LinkedIn](https://www.linkedin.com/in/shawhintalebi/) | [Twitter](https://twitter.com/ShawhinT)'
- en: '**Support**: [Buy me a coffee](https://www.buymeacoffee.com/shawhint) ☕️'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '**支持**： [请我喝咖啡](https://www.buymeacoffee.com/shawhint) ☕️'
- en: '[](https://shawhin.medium.com/subscribe?source=post_page-----8c477768f1f9--------------------------------)
    [## Get FREE access to every new story I write'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://shawhin.medium.com/subscribe?source=post_page-----8c477768f1f9--------------------------------)
    [## 免费获取我写的每一个新故事'
- en: Get FREE access to every new story I write P.S. I do not share your email with
    anyone By signing up, you will create a…
  id: totrans-118
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 免费获取我写的每一个新故事。附言：我不会与任何人分享你的电子邮件。注册后，你将创建一个…
- en: shawhin.medium.com](https://shawhin.medium.com/subscribe?source=post_page-----8c477768f1f9--------------------------------)
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: shawhin.medium.com](https://shawhin.medium.com/subscribe?source=post_page-----8c477768f1f9--------------------------------)
- en: '[1] [BloombergGPT](https://www.bloomberg.com/company/press/bloomberggpt-50-billion-parameter-llm-tuned-finance/)
    | [Paper](https://arxiv.org/pdf/2303.17564.pdf)'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] [BloombergGPT](https://www.bloomberg.com/company/press/bloomberggpt-50-billion-parameter-llm-tuned-finance/)
    | [论文](https://arxiv.org/pdf/2303.17564.pdf)'
- en: '[2] [Llama 2 Paper](https://ai.meta.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models/)'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] [Llama 2 论文](https://ai.meta.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models/)'
- en: '[3] [LLM Energy Costs](https://www.statista.com/statistics/1384401/energy-use-when-training-llm-models/)'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] [LLM 能源成本](https://www.statista.com/statistics/1384401/energy-use-when-training-llm-models/)'
- en: '[4] arXiv:2005.14165 [cs.CL]'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] arXiv:2005.14165 [cs.CL]'
- en: '[5] [Falcon 180b Blog](https://huggingface.co/blog/falcon-180b)'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '[5] [Falcon 180b 博客](https://huggingface.co/blog/falcon-180b)'
- en: '[6] [arXiv:2101.00027](https://arxiv.org/abs/2101.00027) **[cs.CL]**'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '[6] [arXiv:2101.00027](https://arxiv.org/abs/2101.00027) **[cs.CL]**'
- en: '[7] [Alpaca Repo](https://github.com/gururise/AlpacaDataCleaned)'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '[7] [Alpaca 仓库](https://github.com/gururise/AlpacaDataCleaned)'
- en: '[8] [arXiv:2303.18223](https://arxiv.org/abs/2303.18223) **[cs.CL]**'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '[8] [arXiv:2303.18223](https://arxiv.org/abs/2303.18223) **[cs.CL]**'
- en: '[9] [arXiv:2112.11446](https://arxiv.org/abs/2112.11446) **[cs.CL]**'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '[9] [arXiv:2112.11446](https://arxiv.org/abs/2112.11446) **[cs.CL]**'
- en: '[10] [arXiv:1508.07909](https://arxiv.org/abs/1508.07909) **[cs.CL]**'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '[10] [arXiv:1508.07909](https://arxiv.org/abs/1508.07909) **[cs.CL]**'
- en: '[11] [SentencePience Repo](https://github.com/google/sentencepiece/tree/master)'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '[11] [SentencePiece 仓库](https://github.com/google/sentencepiece/tree/master)'
- en: '[12] [Tokenizers Doc](https://huggingface.co/docs/tokenizers/quicktour)'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '[12] [Tokenizers 文档](https://huggingface.co/docs/tokenizers/quicktour)'
- en: '[13] [arXiv:1706.03762](https://arxiv.org/abs/1706.03762) **[cs.CL]**'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '[13] [arXiv:1706.03762](https://arxiv.org/abs/1706.03762) **[cs.CL]**'
- en: '[14] [Andrej Karpathy Lecture](https://www.youtube.com/watch?v=kCc8FmEb1nY&t=5307s)'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '[14] [Andrej Karpathy 讲座](https://www.youtube.com/watch?v=kCc8FmEb1nY&t=5307s)'
- en: '[15] [Hugging Face NLP Course](https://huggingface.co/learn/nlp-course/chapter1/7?fw=pt)'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '[15] [Hugging Face NLP 课程](https://huggingface.co/learn/nlp-course/chapter1/7?fw=pt)'
- en: '[16] [arXiv:1810.04805](https://arxiv.org/abs/1810.04805) **[cs.CL]**'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '[16] [arXiv:1810.04805](https://arxiv.org/abs/1810.04805) **[cs.CL]**'
- en: '[17] arXiv:1910.13461 [cs.CL]'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '[17] arXiv:1910.13461 [cs.CL]'
- en: '[18] [arXiv:1603.05027](https://arxiv.org/abs/1603.05027) **[cs.CV]**'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '[18] [arXiv:1603.05027](https://arxiv.org/abs/1603.05027) **[cs.CV]**'
- en: '[19] [arXiv:1607.06450](https://arxiv.org/abs/1607.06450) **[stat.ML]**'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '[19] [arXiv:1607.06450](https://arxiv.org/abs/1607.06450) **[stat.ML]**'
- en: '[20] [arXiv:1803.02155](https://arxiv.org/abs/1803.02155) **[cs.CL]**'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '[20] [arXiv:1803.02155](https://arxiv.org/abs/1803.02155) **[cs.CL]**'
- en: '[21] [arXiv:2203.15556](https://arxiv.org/abs/2203.15556) **[cs.CL]**'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '[21] [arXiv:2203.15556](https://arxiv.org/abs/2203.15556) **[cs.CL]**'
- en: '[22] [Trained with Mixed Precision Nvidia Doc](https://docs.nvidia.com/deeplearning/performance/mixed-precision-training/index.html)'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '[22] [混合精度训练的 Nvidia 文档](https://docs.nvidia.com/deeplearning/performance/mixed-precision-training/index.html)'
- en: '[23] [DeepSpeed Doc](https://www.deepspeed.ai/training/)'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '[23] [DeepSpeed 文档](https://www.deepspeed.ai/training/)'
- en: '[24] [https://paperswithcode.com/method/weight-decay](https://paperswithcode.com/method/weight-decay)'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '[24] [https://paperswithcode.com/method/weight-decay](https://paperswithcode.com/method/weight-decay)'
- en: '[25] [https://towardsdatascience.com/what-is-gradient-clipping-b8e815cdfb48](/what-is-gradient-clipping-b8e815cdfb48)'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '[25] [https://towardsdatascience.com/what-is-gradient-clipping-b8e815cdfb48](/what-is-gradient-clipping-b8e815cdfb48)'
- en: '[26] [arXiv:2001.08361](https://arxiv.org/abs/2001.08361) **[cs.LG]**'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '[26] [arXiv:2001.08361](https://arxiv.org/abs/2001.08361) **[cs.LG]**'
- en: '[27] [arXiv:1803.05457](https://arxiv.org/abs/1803.05457) **[cs.AI]**'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '[27] [arXiv:1803.05457](https://arxiv.org/abs/1803.05457) **[cs.AI]**'
- en: '[28] arXiv:1905.07830 [cs.CL]'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '[28] arXiv:1905.07830 [cs.CL]'
- en: '[29] arXiv:2009.03300 [cs.CY]'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '[29] arXiv:2009.03300 [cs.CY]'
- en: '[30] arXiv:2109.07958 [cs.CL]'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '[30] arXiv:2109.07958 [cs.CL]'
- en: '[31] [https://huggingface.co/blog/evaluating-mmlu-leaderboard](https://huggingface.co/blog/evaluating-mmlu-leaderboard)'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '[31] [https://huggingface.co/blog/evaluating-mmlu-leaderboard](https://huggingface.co/blog/evaluating-mmlu-leaderboard)'
