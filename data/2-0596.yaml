- en: Create your own Generative AI Text-to-Image API
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: åˆ›å»ºä½ è‡ªå·±çš„ç”Ÿæˆ AI æ–‡æœ¬åˆ°å›¾åƒ API
- en: åŸæ–‡ï¼š[https://towardsdatascience.com/create-your-own-generative-ai-text-to-image-api-548c07a4d839](https://towardsdatascience.com/create-your-own-generative-ai-text-to-image-api-548c07a4d839)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡ï¼š[https://towardsdatascience.com/create-your-own-generative-ai-text-to-image-api-548c07a4d839](https://towardsdatascience.com/create-your-own-generative-ai-text-to-image-api-548c07a4d839)
- en: Turn your ramblings into masterpieces, on demand
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å°†ä½ çš„éšæƒ³è½¬åŒ–ä¸ºæ°ä½œï¼ŒæŒ‰éœ€åˆ¶ä½œ
- en: '[](https://medium.com/@omermx?source=post_page-----548c07a4d839--------------------------------)[![Omer
    Mahmood](../Images/0c87da4134bea397c77bc4ba6640e34b.png)](https://medium.com/@omermx?source=post_page-----548c07a4d839--------------------------------)[](https://towardsdatascience.com/?source=post_page-----548c07a4d839--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----548c07a4d839--------------------------------)
    [Omer Mahmood](https://medium.com/@omermx?source=post_page-----548c07a4d839--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@omermx?source=post_page-----548c07a4d839--------------------------------)[![å¥¥é»˜å°”Â·é©¬èµ«ç©†å¾·](../Images/0c87da4134bea397c77bc4ba6640e34b.png)](https://medium.com/@omermx?source=post_page-----548c07a4d839--------------------------------)[](https://towardsdatascience.com/?source=post_page-----548c07a4d839--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----548c07a4d839--------------------------------)
    [å¥¥é»˜å°”Â·é©¬èµ«ç©†å¾·](https://medium.com/@omermx?source=post_page-----548c07a4d839--------------------------------)'
- en: Â·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----548c07a4d839--------------------------------)
    Â·17 min readÂ·Apr 12, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: Â·å‘è¡¨äº [Towards Data Science](https://towardsdatascience.com/?source=post_page-----548c07a4d839--------------------------------)
    Â·17åˆ†é’Ÿé˜…è¯»Â·2023å¹´4æœˆ12æ—¥
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/738244045935fe00f21f0a7c31c3f546.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/738244045935fe00f21f0a7c31c3f546.png)'
- en: Image generated by the author using Midjourney on General Commercial Terms.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ç‰‡ç”±ä½œè€…ä½¿ç”¨ Midjourney æ ¹æ®ä¸€èˆ¬å•†ä¸šæ¡æ¬¾ç”Ÿæˆã€‚
- en: The TL;DR
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: TL;DR
- en: Recent advances in Generative AI have led to the launch of a whole host of services
    such as DALL-E 2, Midjourney and Stability AI that have the potential to drastically
    change the way we approach content creation.
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç”Ÿæˆ AI çš„æœ€æ–°è¿›å±•å¯¼è‡´äº†ä¸€ç³»åˆ—æœåŠ¡çš„æ¨å‡ºï¼Œå¦‚ DALL-E 2ã€Midjourney å’Œ Stability AIï¼Œå®ƒä»¬æœ‰æ½œåŠ›å½»åº•æ”¹å˜æˆ‘ä»¬å¯¹å†…å®¹åˆ›ä½œçš„æ–¹å¼ã€‚
- en: In this post I show you how to build and serve your very own, high performance,
    text-to-image service over an API. Based on [Stable Diffusion](https://github.com/CompVis/stable-diffusion)
    via [HuggingFace](https://medium.com/towards-data-science/whats-hugging-face-122f4e7eb11a),
    using Vertex AI Workbench and Endpoints.
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åœ¨è¿™ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘å°†å±•ç¤ºå¦‚ä½•é€šè¿‡ API æ„å»ºå¹¶æä¾›ä½ è‡ªå·±é«˜æ€§èƒ½çš„æ–‡æœ¬åˆ°å›¾åƒæœåŠ¡ã€‚åŸºäº [ç¨³å®šæ‰©æ•£](https://github.com/CompVis/stable-diffusion)
    é€šè¿‡ [HuggingFace](https://medium.com/towards-data-science/whats-hugging-face-122f4e7eb11a)ï¼Œä½¿ç”¨
    Vertex AI Workbench å’Œ Endpointsã€‚
- en: ğŸš£ğŸ¼ How we got here
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ğŸš£ğŸ¼ æˆ‘ä»¬æ˜¯å¦‚ä½•åˆ°è¾¾è¿™é‡Œçš„
- en: 'As George Lawton mentions in his [article](https://www.techtarget.com/searchenterpriseai/definition/generative-AI):
    â€œGenerative AI is a type of artificial intelligence technology that can produce
    various types of content including text, imagery, audio and synthetic data. The
    recent buzz around generative AI has been driven by the simplicity of new user
    interfaces for creating high-quality text, graphics and videos in a matter of
    seconds.â€[2]'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: æ­£å¦‚ä¹”æ²»Â·åŠ³é¡¿åœ¨ä»–çš„ [æ–‡ç« ](https://www.techtarget.com/searchenterpriseai/definition/generative-AI)
    ä¸­æåˆ°çš„ï¼šâ€œç”Ÿæˆ AI æ˜¯ä¸€ç§äººå·¥æ™ºèƒ½æŠ€æœ¯ï¼Œèƒ½å¤Ÿç”Ÿæˆå„ç§ç±»å‹çš„å†…å®¹ï¼ŒåŒ…æ‹¬æ–‡æœ¬ã€å›¾åƒã€éŸ³é¢‘å’Œåˆæˆæ•°æ®ã€‚æœ€è¿‘å›´ç»•ç”Ÿæˆ AI çš„çƒ­è®®æ˜¯ç”±äºæ–°ç”¨æˆ·ç•Œé¢çš„ç®€ä¾¿æ€§ï¼Œè¿™äº›ç•Œé¢å¯ä»¥åœ¨å‡ ç§’é’Ÿå†…åˆ›å»ºé«˜è´¨é‡çš„æ–‡æœ¬ã€å›¾å½¢å’Œè§†é¢‘ã€‚â€[2]
- en: Machine Learning is nothing new, in fact itâ€™s been around in some shape or form
    since the 1960s[1]. â€œBut it was not until 2014, with the introduction of [generative
    adversarial networks](https://en.wikipedia.org/wiki/Generative_adversarial_network)
    (GANs), a type of machine learning algorithm, that generative AI could create
    convincingly authentic images, videos and audio of real people.â€[2]
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: æœºå™¨å­¦ä¹ å¹¶ä¸æ˜¯ä»€ä¹ˆæ–°é²œäº‹ï¼Œäº‹å®ä¸Šï¼Œè‡ª1960å¹´ä»£ä»¥æ¥ï¼Œå®ƒä»¥æŸç§å½¢å¼å­˜åœ¨ã€‚â€œä½†ç›´åˆ°2014å¹´ï¼Œéšç€ [ç”Ÿæˆå¯¹æŠ—ç½‘ç»œ](https://en.wikipedia.org/wiki/Generative_adversarial_network)ï¼ˆGANsï¼‰çš„å¼•å…¥ï¼Œä¸€ç§æœºå™¨å­¦ä¹ ç®—æ³•ï¼Œç”Ÿæˆ
    AI æ‰å¾—ä»¥åˆ›é€ ä»¤äººä¿¡æœçš„çœŸå®äººç‰©å›¾åƒã€è§†é¢‘å’ŒéŸ³é¢‘ã€‚â€[2]
- en: Combined with the power of Large Language Models (LLMs) that can take a user
    prompt in natural language describing something and then produce photorealistic
    images, weâ€™ve come a very long way in a short period of time. The first to do
    this was [OpenAIâ€™s DALLÂ·E](https://openai.com/product/dall-e-2), in April 2022,
    followed by Disco Diffusion in August 2022, which was eventually succeeded by
    Stable Diffusion. In parallel to all these offerings, there is a company called
    [Midjourney](https://www.midjourney.com/home/?callbackUrl=%2Fapp%2F) that has
    built a very popular model that is used by interacting with a Discord bot.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: ç»“åˆèƒ½å¤Ÿæ¥æ”¶è‡ªç„¶è¯­è¨€æç¤ºå¹¶ç”Ÿæˆç…§ç‰‡çº§çœŸå®å›¾åƒçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„èƒ½åŠ›ï¼Œæˆ‘ä»¬åœ¨çŸ­æ—¶é—´å†…å–å¾—äº†å·¨å¤§çš„è¿›æ­¥ã€‚ç¬¬ä¸€ä¸ªåšåˆ°è¿™ä¸€ç‚¹çš„æ˜¯[OpenAIçš„DALLÂ·E](https://openai.com/product/dall-e-2)ï¼Œäº2022å¹´4æœˆæ¨å‡ºï¼Œéšåæ˜¯2022å¹´8æœˆçš„Disco
    Diffusionï¼Œæœ€ç»ˆè¢«ç¨³å®šæ‰©æ•£æ‰€å–ä»£ã€‚ä¸è¿™äº›äº§å“å¹¶è¡Œçš„æ˜¯ä¸€å®¶åä¸º[Midjourney](https://www.midjourney.com/home/?callbackUrl=%2Fapp%2F)çš„å…¬å¸ï¼Œå®ƒå¼€å‘äº†ä¸€ä¸ªéå¸¸å—æ¬¢è¿çš„æ¨¡å‹ï¼Œé€šè¿‡ä¸Discordæœºå™¨äººäº’åŠ¨ä½¿ç”¨ã€‚
- en: Since then progress made in the state of the art has been quite astounding.
    The screenshot below shows what was accomplished in just the space of a few months
    â€” two different models were provided with the same prompt, but the contrast between
    early and later models is stark!
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: ä»é‚£æ—¶èµ·ï¼Œè‰ºæœ¯çŠ¶æ€çš„è¿›å±•ä»¤äººæƒŠå¹ã€‚ä¸‹é¢çš„æˆªå›¾å±•ç¤ºäº†ä»…åœ¨å‡ ä¸ªæœˆçš„æ—¶é—´å†…æ‰€å–å¾—çš„æˆå°±â€”â€”ä¸¤ä¸ªä¸åŒçš„æ¨¡å‹æä¾›äº†ç›¸åŒçš„æç¤ºï¼Œä½†æ—©æœŸæ¨¡å‹å’ŒåæœŸæ¨¡å‹ä¹‹é—´çš„å¯¹æ¯”éå¸¸æ˜æ˜¾ï¼
- en: '![](../Images/c0322ec3ed14c73845d0dc9300b08599.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c0322ec3ed14c73845d0dc9300b08599.png)'
- en: '*Figure 1: Advances in Generative AI Art, images produced by Disco Diffusion
    and Midjourney.*'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '*å›¾1ï¼šç”Ÿæˆæ€§äººå·¥æ™ºèƒ½è‰ºæœ¯çš„è¿›å±•ï¼Œç”±Disco Diffusionå’ŒMidjourneyç”Ÿæˆçš„å›¾åƒã€‚*'
- en: Fast forward to 2023, Midjourney just released version 5 on March 15th. This
    model has very high Coherency, excels at interpreting natural language prompts,
    is higher resolution, and supports advanced features like repeating patterns[4].
    Stability AI has also released Stable Diffusion 2.1\. The pace of advancement
    for these models moves at a blistering pace!
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: å¿«è¿›åˆ°2023å¹´ï¼ŒMidjourneyäº3æœˆ15æ—¥åˆšåˆšå‘å¸ƒäº†ç¬¬5ç‰ˆã€‚è¿™ä¸ªæ¨¡å‹å…·æœ‰éå¸¸é«˜çš„è¿è´¯æ€§ï¼Œæ“…é•¿è§£é‡Šè‡ªç„¶è¯­è¨€æç¤ºï¼Œåˆ†è¾¨ç‡æ›´é«˜ï¼Œå¹¶æ”¯æŒåƒé‡å¤å›¾æ¡ˆè¿™æ ·çš„é«˜çº§åŠŸèƒ½[4]ã€‚Stability
    AIä¹Ÿå‘å¸ƒäº†ç¨³å®šæ‰©æ•£2.1ã€‚è¿™äº›æ¨¡å‹çš„å‘å±•é€Ÿåº¦éå¸¸è¿…çŒ›ï¼
- en: 'As described in a recent [white paper](https://www2.deloitte.com/us/en/pages/consulting/articles/generative-artificial-intelligence.html)
    published by Deloitte: â€œIt feels like we may only be just beginning to see the
    impact of what Generative AI models are capable of. Although early traction has
    been through consumer releases, which could be era-defining, Generative AI also
    has the potential to add contextual awareness and human-like decision-making to
    nearly all facets of life.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: æ­£å¦‚å¾·å‹¤æœ€è¿‘å‘å¸ƒçš„[ç™½çš®ä¹¦](https://www2.deloitte.com/us/en/pages/consulting/articles/generative-artificial-intelligence.html)ä¸­æè¿°çš„ï¼šâ€œæˆ‘ä»¬æ„Ÿè§‰å¯èƒ½åªæ˜¯åˆšåˆšå¼€å§‹çœ‹åˆ°ç”Ÿæˆæ€§äººå·¥æ™ºèƒ½æ¨¡å‹æ‰€èƒ½å¸¦æ¥çš„å½±å“ã€‚è™½ç„¶æ—©æœŸçš„å¸å¼•åŠ›ä¸»è¦æ¥è‡ªæ¶ˆè´¹è€…å‘å¸ƒï¼Œè¿™å¯èƒ½ä¼šæˆä¸ºå®šä¹‰æ—¶ä»£çš„é‡Œç¨‹ç¢‘ï¼Œä½†ç”Ÿæˆæ€§äººå·¥æ™ºèƒ½ä¹Ÿæœ‰æ½œåŠ›ä¸ºå‡ ä¹æ‰€æœ‰ç”Ÿæ´»é¢†åŸŸå¢åŠ æƒ…å¢ƒæ„è¯†å’Œç±»äººå†³ç­–èƒ½åŠ›ã€‚
- en: As such, Generative AI has attracted interest from traditional (e.g., Venture
    Capital (VC), Mergers & Acquisitions (M&A)) and emerging (e.g., ecosystem partnerships)
    sources. In 2022 alone, venture capital firms invested more than $2B, and technology
    leaders made significant investments, such as Microsoftâ€™s $10B stake in OpenAI
    and Googleâ€™s $300M stake in Anthropic.â€[3]
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: å› æ­¤ï¼Œç”Ÿæˆæ€§äººå·¥æ™ºèƒ½å¸å¼•äº†æ¥è‡ªä¼ ç»Ÿï¼ˆä¾‹å¦‚é£é™©æŠ•èµ„ï¼ˆVCï¼‰ï¼Œå¹¶è´­ï¼ˆM&Aï¼‰ï¼‰å’Œæ–°å…´ï¼ˆä¾‹å¦‚ç”Ÿæ€ç³»ç»Ÿåˆä½œä¼™ä¼´ï¼‰æ¥æºçš„å…´è¶£ã€‚ä»…åœ¨2022å¹´ï¼Œé£é™©æŠ•èµ„å…¬å¸å°±æŠ•èµ„äº†è¶…è¿‡20äº¿ç¾å…ƒï¼Œç§‘æŠ€é¢†è¢–ä¹Ÿè¿›è¡Œäº†é‡è¦æŠ•èµ„ï¼Œå¦‚å¾®è½¯å¯¹OpenAIçš„100äº¿ç¾å…ƒæŠ•èµ„å’Œè°·æ­Œå¯¹Anthropicçš„3äº¿ç¾å…ƒæŠ•èµ„ã€‚â€[3]
- en: ğŸ§ Whatâ€™s Stable Diffusion?
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ğŸ§ ä»€ä¹ˆæ˜¯ç¨³å®šæ‰©æ•£ï¼Ÿ
- en: Stable Diffusion is a method used in AI text-to-image generation that uses a
    diffusion model to create images from text descriptions. The diffusion model starts
    with a random image and then gradually adds noise to it, step by step. The noise
    is added in a controlled way, so that the image remains recognisable.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: ç¨³å®šæ‰©æ•£æ˜¯ä¸€ç§ç”¨äºäººå·¥æ™ºèƒ½æ–‡æœ¬ç”Ÿæˆå›¾åƒçš„æ–¹æ³•ï¼Œå®ƒåˆ©ç”¨æ‰©æ•£æ¨¡å‹ä»æ–‡æœ¬æè¿°ä¸­åˆ›å»ºå›¾åƒã€‚æ‰©æ•£æ¨¡å‹ä»ä¸€å¼ éšæœºå›¾åƒå¼€å§‹ï¼Œç„¶åé€æ­¥å‘å…¶ä¸­æ·»åŠ å™ªå£°ã€‚å™ªå£°ä»¥å—æ§çš„æ–¹å¼æ·»åŠ ï¼Œä½¿å¾—å›¾åƒä»ç„¶å¯ä»¥è¾¨è®¤ã€‚
- en: 'At each diffusion step, the image becomes more refined, with the details becoming
    clearer. This process continues for several diffusion steps until the generated
    image is considered â€œstableâ€, meaning weâ€™ve hit a point where further iterations
    arenâ€™t likely to improve it. The process is shown below:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ¯ä¸ªæ‰©æ•£æ­¥éª¤ä¸­ï¼Œå›¾åƒå˜å¾—æ›´åŠ ç²¾ç»†ï¼Œç»†èŠ‚å˜å¾—æ›´åŠ æ¸…æ™°ã€‚è¿™ä¸ªè¿‡ç¨‹ä¼šæŒç»­å‡ ä¸ªæ‰©æ•£æ­¥éª¤ï¼Œç›´åˆ°ç”Ÿæˆçš„å›¾åƒè¢«è®¤ä¸ºâ€œç¨³å®šâ€ï¼Œè¿™æ„å‘³ç€æˆ‘ä»¬å·²ç»è¾¾åˆ°äº†ä¸€ä¸ªè¿›ä¸€æ­¥è¿­ä»£ä¸ä¼šæ”¹å–„å›¾åƒçš„ç‚¹ã€‚è¿‡ç¨‹å¦‚ä¸‹æ‰€ç¤ºï¼š
- en: '![](../Images/ce6279d504e7f24b30e95752aabbdd18.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ce6279d504e7f24b30e95752aabbdd18.png)'
- en: '*Figure 2: Process of Stable Diffusion, image credit:* [*Benlisquare*](https://en.wikipedia.org/wiki/Stable_Diffusion#/media/File:X-Y_plot_of_algorithmically-generated_AI_art_of_European-style_castle_in_Japan_demonstrating_DDIM_diffusion_steps.png)*,
    shared under licence:* [*CC BY-SA 4.0*](http://creativecommons.org/licenses/by-nc/4.0/)'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '*å›¾ 2ï¼šStable Diffusion è¿‡ç¨‹ï¼Œå›¾åƒæ¥æºï¼š* [*Benlisquare*](https://en.wikipedia.org/wiki/Stable_Diffusion#/media/File:X-Y_plot_of_algorithmically-generated_AI_art_of_European-style_castle_in_Japan_demonstrating_DDIM_diffusion_steps.png)*ï¼Œæ ¹æ®è®¸å¯å…±äº«ï¼š*
    [*CC BY-SA 4.0*](http://creativecommons.org/licenses/by-nc/4.0/)'
- en: Diffusion models are trained on a dataset of images and text descriptions, and
    it learns to associate the text descriptions with the images that match them.
    When you give the diffusion model a new text description, it uses its knowledge
    to generate an image that matches the description.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰©æ•£æ¨¡å‹åœ¨ä¸€ä¸ªå›¾åƒå’Œæ–‡æœ¬æè¿°çš„æ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒï¼Œå®ƒå­¦ä¼šå°†æ–‡æœ¬æè¿°ä¸åŒ¹é…çš„å›¾åƒå…³è”èµ·æ¥ã€‚å½“ä½ ç»™æ‰©æ•£æ¨¡å‹ä¸€ä¸ªæ–°çš„æ–‡æœ¬æè¿°æ—¶ï¼Œå®ƒä¼šåˆ©ç”¨å…¶çŸ¥è¯†ç”Ÿæˆä¸€ä¸ªåŒ¹é…æè¿°çš„å›¾åƒã€‚
- en: The main advantage of Stable Diffusion is that it is very fast. It can generate
    an image from a text description in just a few seconds. This is much faster than
    other methods, such as GANs. Our modern contenders (e.g. Midjourney, DALL E, etc.)
    all use some variant of the Stable Diffusion technique.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: Stable Diffusion çš„ä¸»è¦ä¼˜åŠ¿åœ¨äºå®ƒéå¸¸å¿«é€Ÿã€‚å®ƒå¯ä»¥åœ¨å‡ ç§’é’Ÿå†…ä»æ–‡æœ¬æè¿°ç”Ÿæˆå›¾åƒã€‚è¿™æ¯”å…¶ä»–æ–¹æ³•ï¼ˆå¦‚ GANsï¼‰å¿«å¾—å¤šã€‚æˆ‘ä»¬çš„ç°ä»£ç«äº‰è€…ï¼ˆä¾‹å¦‚
    Midjourneyã€DALL E ç­‰ï¼‰éƒ½ä½¿ç”¨æŸç§å˜ä½“çš„ Stable Diffusion æŠ€æœ¯ã€‚
- en: But Stable Diffusion is not perfect. Although its application has improved in
    leaps and bounds over the past year or so, sometimes the images produced can be
    distorted or lack detail. This is probably because models like these hallucinate
    or â€œguessâ€ certain details of what an image should look like, based on the data
    they are trained on. As training datasets and model algorithms improve this will
    become less of an issue.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: ä½† Stable Diffusion å¹¶ä¸å®Œç¾ã€‚è™½ç„¶åœ¨è¿‡å»ä¸€å¹´ä¸­å®ƒçš„åº”ç”¨æœ‰äº†é£è·ƒå¼çš„è¿›æ­¥ï¼Œä½†æœ‰æ—¶ç”Ÿæˆçš„å›¾åƒå¯èƒ½ä¼šå¤±çœŸæˆ–ç¼ºä¹ç»†èŠ‚ã€‚è¿™å¯èƒ½æ˜¯å› ä¸ºåƒè¿™æ ·çš„æ¨¡å‹ä¼šæ ¹æ®è®­ç»ƒæ•°æ®æ¥â€œçŒœæµ‹â€å›¾åƒåº”è¯¥æ˜¯ä»€ä¹ˆæ ·å­ã€‚éšç€è®­ç»ƒæ•°æ®é›†å’Œæ¨¡å‹ç®—æ³•çš„æ”¹è¿›ï¼Œè¿™ä¸ªé—®é¢˜å°†ä¼šå‡å°‘ã€‚
- en: ğŸ‘·ğŸ¾â€â™€ï¸ Letâ€™s get started!
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ğŸ‘·ğŸ¾â€â™€ï¸ å¼€å§‹å§ï¼
- en: 'In this post Iâ€™m going to show you how to take some code to:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘å°†å‘ä½ å±•ç¤ºå¦‚ä½•ä½¿ç”¨ä¸€äº›ä»£ç æ¥ï¼š
- en: '**Experiment with the Stable Diffusion** model interactively and generate some
    cool art!'
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**äº’åŠ¨åœ°å®éªŒ Stable Diffusion** æ¨¡å‹ï¼Œç”Ÿæˆä¸€äº›é…·ç‚«çš„è‰ºæœ¯ä½œå“ï¼'
- en: '**Serve the model via an endpoint** using Vertex AI.'
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**é€šè¿‡ç«¯ç‚¹æœåŠ¡æ¨¡å‹** ä½¿ç”¨ Vertex AIã€‚'
- en: '**Create a simple RESTful API** using FLASK.'
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**ä½¿ç”¨ FLASK åˆ›å»ºä¸€ä¸ªç®€å•çš„ RESTful API**ã€‚'
- en: Sure you could just use an existing, consumer facing model like those we mentioned
    in the previous section, but where would the fun be in that? ğŸ˜œ
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: å½“ç„¶ï¼Œä½ å¯ä»¥ç›´æ¥ä½¿ç”¨æˆ‘ä»¬åœ¨å‰é¢ç« èŠ‚ä¸­æåˆ°çš„ç°æœ‰çš„é¢å‘æ¶ˆè´¹è€…çš„æ¨¡å‹ï¼Œä½†é‚£æ ·ä¼šæ²¡æœ‰ä¹è¶£å•Šï¼ŸğŸ˜œ
- en: â¡ï¸ Feel free to jump to the part you find most interesting!
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: â¡ï¸ éšæ„è·³åˆ°ä½ æ„Ÿå…´è¶£çš„éƒ¨åˆ†ï¼
- en: ğŸ‘¾ Iâ€™ll pop any code snippets (not already linked) in a github repo, link at
    the end under Useful Resources
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ‘¾ æˆ‘ä¼šæŠŠä»»ä½•ä»£ç ç‰‡æ®µï¼ˆå¦‚æœæ²¡æœ‰é“¾æ¥çš„è¯ï¼‰æ”¾åœ¨ä¸€ä¸ª github ä»“åº“ä¸­ï¼Œé“¾æ¥è§æœ€åçš„æœ‰ç”¨èµ„æºéƒ¨åˆ†
- en: ğŸ‘©ğŸ»â€ğŸ’» 1\. Experimenting with Stable Diffusion
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ğŸ‘©ğŸ»â€ğŸ’» 1. å®éªŒ Stable Diffusion
- en: When you start working on a new machine learning problem, notebooks provide
    a super agile way to test models and iterate fast. Maybe you like running [Jupyter](https://jupyter.org/)
    in a local environment, using a [Kaggle Kernel](https://www.kaggle.com/code),
    or my personal favourite, [Colab](https://colab.research.google.com/). With tools
    like these, creating and experimenting with machine learning is becoming increasingly
    accessible.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: å½“ä½ å¼€å§‹å¤„ç†ä¸€ä¸ªæ–°çš„æœºå™¨å­¦ä¹ é—®é¢˜æ—¶ï¼Œç¬”è®°æœ¬æä¾›äº†ä¸€ç§éå¸¸çµæ´»çš„æ–¹å¼æ¥æµ‹è¯•æ¨¡å‹å’Œå¿«é€Ÿè¿­ä»£ã€‚ä¹Ÿè®¸ä½ å–œæ¬¢åœ¨æœ¬åœ°ç¯å¢ƒä¸­è¿è¡Œ [Jupyter](https://jupyter.org/)ï¼Œä½¿ç”¨
    [Kaggle Kernel](https://www.kaggle.com/code)ï¼Œæˆ–è€…æˆ‘ä¸ªäººæœ€å–œæ¬¢çš„ [Colab](https://colab.research.google.com/)ã€‚æœ‰äº†è¿™äº›å·¥å…·ï¼Œåˆ›å»ºå’Œå®éªŒæœºå™¨å­¦ä¹ å˜å¾—è¶Šæ¥è¶Šå¯åŠã€‚
- en: Weâ€™re going to create a quick notebook that will introduce you to the Stable
    Diffusion model
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†åˆ›å»ºä¸€ä¸ªå¿«é€Ÿçš„ç¬”è®°æœ¬ï¼Œå‘ä½ ä»‹ç» Stable Diffusion æ¨¡å‹
- en: ğŸ’¨ See the Resources section to download the notebook file if you donâ€™t have
    time (or want) to build it step-by-step!
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ’¨ å¦‚æœä½ æ²¡æœ‰æ—¶é—´ï¼ˆæˆ–ä¸æƒ³ï¼‰ä¸€æ­¥æ­¥æ„å»ºï¼Œå¯ä»¥æŸ¥çœ‹èµ„æºéƒ¨åˆ†ä¸‹è½½ç¬”è®°æœ¬æ–‡ä»¶ï¼
- en: '**Prerequisites:**'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '**å‰ææ¡ä»¶ï¼š**'
- en: Google account to sign into Colab and save notebooks
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç”¨äºç™»å½• Colab å’Œä¿å­˜ç¬”è®°æœ¬çš„ Google è´¦æˆ·
- en: HuggingFace account and an API token (free)
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: HuggingFace è´¦æˆ·å’Œ API ä»¤ç‰Œï¼ˆå…è´¹ï¼‰
- en: 1\. Head over to [https://colab.research.google.com/](https://colab.research.google.com/).
    This will create a new Python notebook for you and save it to your Google Drive.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 1. è®¿é—® [https://colab.research.google.com/](https://colab.research.google.com/)ã€‚è¿™å°†ä¸ºä½ åˆ›å»ºä¸€ä¸ªæ–°çš„
    Python ç¬”è®°æœ¬ï¼Œå¹¶å°†å…¶ä¿å­˜åˆ°ä½ çš„ Google Drive ä¸­ã€‚
- en: 2\. Colab is made available for free, but youâ€™ll have to pay if you want access
    to more memory and compute resources on which to run your notebook. Weâ€™ll just
    use whatâ€™s available for free, the first thing youâ€™ll need to do is configure
    the notebook runtime to include a GPU accelerator. Using a GPU will speed up the
    time it takes for the model to generate an image aka. perform inference. The free
    tier provides an NVIDIA Tesla T4 GPU with 12GB of memory (which is just enough
    for what we want to do). From the file menu select *Runtime -> Change Runtime
    Type, and select Hardware accelerator = GPU*.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 2. Colab å…è´¹æä¾›ï¼Œä½†å¦‚æœä½ éœ€è¦æ›´å¤šçš„å†…å­˜å’Œè®¡ç®—èµ„æºæ¥è¿è¡Œä½ çš„ç¬”è®°æœ¬ï¼Œä½ éœ€è¦ä»˜è´¹ã€‚æˆ‘ä»¬å°†åªä½¿ç”¨å…è´¹çš„èµ„æºï¼Œé¦–å…ˆä½ éœ€è¦é…ç½®ç¬”è®°æœ¬è¿è¡Œæ—¶ä»¥åŒ…å« GPU
    åŠ é€Ÿå™¨ã€‚ä½¿ç”¨ GPU ä¼šåŠ å¿«æ¨¡å‹ç”Ÿæˆå›¾åƒçš„æ—¶é—´ï¼Œä¹Ÿå°±æ˜¯æ‰§è¡Œæ¨ç†ã€‚å…è´¹å±‚æä¾›äº† 12GB å†…å­˜çš„ NVIDIA Tesla T4 GPUï¼ˆè¿™å¯¹æˆ‘ä»¬è¦åšçš„äº‹æƒ…æ­£å¥½è¶³å¤Ÿï¼‰ã€‚ä»æ–‡ä»¶èœå•ä¸­é€‰æ‹©
    *è¿è¡Œæ—¶ -> æ›´æ”¹è¿è¡Œæ—¶ç±»å‹*ï¼Œç„¶åé€‰æ‹©ç¡¬ä»¶åŠ é€Ÿå™¨ = GPUã€‚
- en: '![](../Images/f221b1b327a28f16c52af8d6322c8787.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f221b1b327a28f16c52af8d6322c8787.png)'
- en: '*Figure 3: Colab Notebook, Change notebook runtime.*'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '*å›¾ 3ï¼šColab ç¬”è®°æœ¬ï¼Œæ›´æ”¹ç¬”è®°æœ¬è¿è¡Œæ—¶ã€‚*'
- en: 'Then youâ€™ll need to connect to your Runtime so we can execute any code we write
    in the notebook. Click on *Connect/Reconnect -> Connect to a hosted runtime*:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åä½ éœ€è¦è¿æ¥åˆ°ä½ çš„è¿è¡Œæ—¶ï¼Œä»¥ä¾¿æˆ‘ä»¬å¯ä»¥æ‰§è¡Œç¬”è®°æœ¬ä¸­ç¼–å†™çš„ä»»ä½•ä»£ç ã€‚ç‚¹å‡» *è¿æ¥/é‡æ–°è¿æ¥ -> è¿æ¥åˆ°æ‰˜ç®¡è¿è¡Œæ—¶*ï¼š
- en: '![](../Images/ba22e38c451cbb8a3098582d01982b16.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ba22e38c451cbb8a3098582d01982b16.png)'
- en: '*Figure 4: Colab Notebook, â€œ+ Codeâ€ button to add cells, and Connecting to
    a hosted runtime.*'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '*å›¾ 4ï¼šColab ç¬”è®°æœ¬ï¼Œâ€œ+ ä»£ç â€æŒ‰é’®æ·»åŠ å•å…ƒæ ¼ï¼Œä»¥åŠè¿æ¥åˆ°æ‰˜ç®¡è¿è¡Œæ—¶ã€‚*'
- en: '3\. Once youâ€™ve connected to your runtime, itâ€™s time to enter some code! You
    can enter code in a single notebook â€œcellâ€, or if you want to execute steps separately,
    using the â€œ+ Codeâ€ button to create a new â€œcellâ€. I find it useful to break up
    the code into cells so itâ€™s easier to identify and debug issues. Each cell has
    a â€œplayâ€ button when you hover over, click it to run the code. You can run the
    following steps after typing or copy/pasting the code into a new cell:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 3. ä¸€æ—¦ä½ è¿æ¥åˆ°ä½ çš„è¿è¡Œæ—¶ï¼Œå°±å¯ä»¥è¾“å…¥ä¸€äº›ä»£ç äº†ï¼ä½ å¯ä»¥åœ¨å•ä¸ªç¬”è®°æœ¬â€œå•å…ƒæ ¼â€ä¸­è¾“å…¥ä»£ç ï¼Œæˆ–è€…å¦‚æœä½ æƒ³å•ç‹¬æ‰§è¡Œæ­¥éª¤ï¼Œå¯ä»¥ä½¿ç”¨â€œ+ ä»£ç â€æŒ‰é’®åˆ›å»ºä¸€ä¸ªæ–°çš„â€œå•å…ƒæ ¼â€ã€‚æˆ‘å‘ç°å°†ä»£ç åˆ†è§£æˆå•å…ƒæ ¼å¾ˆæœ‰ç”¨ï¼Œè¿™æ ·æ›´å®¹æ˜“è¯†åˆ«å’Œè°ƒè¯•é—®é¢˜ã€‚æ¯ä¸ªå•å…ƒæ ¼åœ¨ä½ æ‚¬åœæ—¶éƒ½æœ‰ä¸€ä¸ªâ€œæ’­æ”¾â€æŒ‰é’®ï¼Œç‚¹å‡»å®ƒä»¥è¿è¡Œä»£ç ã€‚è¾“å…¥æˆ–å¤åˆ¶ç²˜è´´ä»£ç åˆ°æ–°çš„å•å…ƒæ ¼ä¸­åï¼Œä½ å¯ä»¥è¿è¡Œä»¥ä¸‹æ­¥éª¤ï¼š
- en: 'a. Because weâ€™re pulling the model from HuggingFace, we need to install some
    python libraries and authenticate using an API token:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: a. ç”±äºæˆ‘ä»¬éœ€è¦ä» HuggingFace æ‹‰å–æ¨¡å‹ï¼Œå› æ­¤éœ€è¦å®‰è£…ä¸€äº› Python åº“å¹¶ä½¿ç”¨ API ä»¤ç‰Œè¿›è¡Œè®¤è¯ï¼š
- en: '[PRE0]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'b. Next we need to [authenticate using a token from you HuggingFace account](https://huggingface.co/docs/hub/security-tokens#how-to-manage-user-access-tokens),
    when you execute the code below, you will be prompted in your notebook to paste
    in your HuggingFace API token:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: b. æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬éœ€è¦ [ä½¿ç”¨ä½  HuggingFace è´¦æˆ·çš„ä»¤ç‰Œè¿›è¡Œè®¤è¯](https://huggingface.co/docs/hub/security-tokens#how-to-manage-user-access-tokens)ï¼Œå½“ä½ æ‰§è¡Œä»¥ä¸‹ä»£ç æ—¶ï¼Œä½ ä¼šåœ¨ç¬”è®°æœ¬ä¸­è¢«æç¤ºç²˜è´´ä½ çš„
    HuggingFace API ä»¤ç‰Œï¼š
- en: '[PRE1]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'c. After successfully logging into your HuggingFace account, weâ€™re going download
    the diffusers and transformers python libraries:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: c. æˆåŠŸç™»å½•ä½ çš„ HuggingFace è´¦æˆ·åï¼Œæˆ‘ä»¬å°†ä¸‹è½½ diffusers å’Œ transformers Python åº“ï¼š
- en: '[PRE2]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'd. We need to create a StableDiffusion model pipeline so we can basically pass
    the model some text and have it generate an image based on that prompt. You might
    notice that one of the parameters weâ€™re passing is a path to a Stable Diffusion
    model hosted on HuggingFace. The examples in this post were tested using v1.5,
    you can try swapping for the latest (v2.1) at the time of writing:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: d. æˆ‘ä»¬éœ€è¦åˆ›å»ºä¸€ä¸ª StableDiffusion æ¨¡å‹ç®¡é“ï¼Œä»¥ä¾¿æˆ‘ä»¬å¯ä»¥å°†æ¨¡å‹ä¼ é€’ä¸€äº›æ–‡æœ¬ï¼Œå¹¶æ ¹æ®è¯¥æç¤ºç”Ÿæˆå›¾åƒã€‚ä½ å¯èƒ½ä¼šæ³¨æ„åˆ°æˆ‘ä»¬ä¼ é€’çš„å‚æ•°ä¹‹ä¸€æ˜¯
    HuggingFace ä¸Šæ‰˜ç®¡çš„ Stable Diffusion æ¨¡å‹çš„è·¯å¾„ã€‚æ­¤å¸–å­ä¸­çš„ç¤ºä¾‹ä½¿ç”¨çš„æ˜¯ v1.5ï¼Œä½ å¯ä»¥å°è¯•åœ¨å†™ä½œæ—¶äº¤æ¢ä¸ºæœ€æ–°çš„ï¼ˆv2.1ï¼‰ï¼š
- en: '[PRE3]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'e. Now for the fun bit, weâ€™re going to generate some art! Load the torch python
    library, and then add a cell, replacing the string value for the prompt variable
    with whatever you want! I put in simple example to get you started:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: e. ç°åœ¨è¿›å…¥æœ‰è¶£çš„éƒ¨åˆ†ï¼Œæˆ‘ä»¬å°†ç”Ÿæˆä¸€äº›è‰ºæœ¯ä½œå“ï¼åŠ è½½ torch Python åº“ï¼Œç„¶åæ·»åŠ ä¸€ä¸ªå•å…ƒæ ¼ï¼Œå°† prompt å˜é‡çš„å­—ç¬¦ä¸²å€¼æ›¿æ¢ä¸ºä½ æƒ³è¦çš„ä»»ä½•å†…å®¹ï¼æˆ‘æä¾›äº†ä¸€ä¸ªç®€å•çš„ç¤ºä¾‹ä»¥å¸®åŠ©ä½ å…¥é—¨ï¼š
- en: '[PRE4]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'You should end up with something similar to the screenshot below:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ åº”è¯¥å¾—åˆ°å¦‚ä¸‹æ‰€ç¤ºçš„ç»“æœï¼š
- en: '![](../Images/e98a0eeada649dd901d490466eea6b7c.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e98a0eeada649dd901d490466eea6b7c.png)'
- en: '*Figure 5: Colab Notebook, image output by Stable Diffusion model.*'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '*å›¾ 5ï¼šColab ç¬”è®°æœ¬ï¼ŒStable Diffusion æ¨¡å‹ç”Ÿæˆçš„å›¾åƒè¾“å‡ºã€‚*'
- en: '**ğŸ’¡Tip:** if you find yourself running into â€˜out of memoryâ€™ errors, you can
    periodically clear the GPUâ€™s cache by adding this line of code above your text
    prompt:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '**ğŸ’¡æç¤ºï¼š** å¦‚æœä½ é‡åˆ°â€˜å†…å­˜ä¸è¶³â€™çš„é”™è¯¯ï¼Œä½ å¯ä»¥é€šè¿‡åœ¨æ–‡æœ¬æç¤ºä¸Šæ–¹æ·»åŠ ä»¥ä¸‹ä»£ç è¡Œå®šæœŸæ¸…é™¤ GPU çš„ç¼“å­˜ï¼š'
- en: '[PRE5]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: With just a few lines of code in a notebook, you can generate art from a text
    based prompt!
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: åªéœ€åœ¨ä¸€ä¸ªç¬”è®°æœ¬ä¸­å†™å‡ è¡Œä»£ç ï¼Œä½ å°±å¯ä»¥ä»åŸºäºæ–‡æœ¬çš„æç¤ºç”Ÿæˆè‰ºæœ¯ä½œå“ï¼
- en: ğŸ¦ 2\. Serve the model via a Vertex AI Endpoint
  id: totrans-68
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ğŸ¦ 2\. é€šè¿‡ Vertex AI ç«¯ç‚¹æœåŠ¡æ¨¡å‹
- en: Making production applications or training large models requires additional
    tooling to help you scale beyond just code in a notebook, and using a cloud service
    provider can help.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: åˆ¶ä½œç”Ÿäº§åº”ç”¨ç¨‹åºæˆ–è®­ç»ƒå¤§å‹æ¨¡å‹éœ€è¦é¢å¤–çš„å·¥å…·æ¥å¸®åŠ©ä½ è¶…è¶Šç¬”è®°æœ¬ä¸­çš„ä»£ç ï¼Œå¹¶ä¸”ä½¿ç”¨äº‘æœåŠ¡æä¾›å•†å¯ä»¥æä¾›å¸®åŠ©ã€‚
- en: Our goal is to package the Stable Diffusion model and host it on an endpoint
    that we can use to handle prediction requests from application code.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬çš„ç›®æ ‡æ˜¯å°† Stable Diffusion æ¨¡å‹æ‰“åŒ…ï¼Œå¹¶æ‰˜ç®¡åœ¨ä¸€ä¸ªå¯ä»¥å¤„ç†æ¥è‡ªåº”ç”¨ä»£ç çš„é¢„æµ‹è¯·æ±‚çš„ç«¯ç‚¹ä¸Šã€‚
- en: You could use another public cloud service provider to achieve approximately
    the same results, but Iâ€™m going to go with Google Cloud Platform (GCP) and in
    particular itâ€™s Vertex AI toolset given Iâ€™m most familiar with it.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ å¯ä»¥ä½¿ç”¨å…¶ä»–å…¬å…±äº‘æœåŠ¡æä¾›å•†æ¥å®ç°å¤§è‡´ç›¸åŒçš„ç»“æœï¼Œä½†æˆ‘å°†ä½¿ç”¨ Google Cloud Platform (GCP)ï¼Œç‰¹åˆ«æ˜¯å®ƒçš„ Vertex AI
    å·¥å…·é›†ï¼Œå› ä¸ºæˆ‘å¯¹æ­¤æœ€ä¸ºç†Ÿæ‚‰ã€‚
- en: '**Prerequisites:**'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '**å‰ææ¡ä»¶ï¼š**'
- en: GCP account with billing enabled/free starter credits
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¯ç”¨è®¡è´¹çš„ GCP è´¦æˆ·/å…è´¹èµ·å§‹ç§¯åˆ†
- en: Have some basic knowledge of GCP admin e.g. how to create a Project, VMs, storage
    buckets and other resources,
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å…·å¤‡ä¸€äº›åŸºæœ¬çš„ GCP ç®¡ç†çŸ¥è¯†ï¼Œä¾‹å¦‚å¦‚ä½•åˆ›å»ºé¡¹ç›®ã€è™šæ‹Ÿæœºã€å­˜å‚¨æ¡¶å’Œå…¶ä»–èµ„æºï¼Œ
- en: Give a service account permissions via IAM,
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: é€šè¿‡ IAM ç»™æœåŠ¡è´¦æˆ·èµ‹äºˆæƒé™ï¼Œ
- en: download a service account key,
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¸‹è½½æœåŠ¡è´¦æˆ·å¯†é’¥ï¼Œ
- en: use the Google Cloud SDK + a bit of Python.
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä½¿ç”¨ Google Cloud SDK + ä¸€äº› Python ä»£ç ã€‚
- en: '1\. Weâ€™re going to use a pre-built notebook example: ([https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/vertex_endpoints/torchserve/dreambooth_stablediffusion.ipynb](https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/vertex_endpoints/torchserve/dreambooth_stablediffusion.ipynb))
    from the official repository of GCP Vertex AI samples on Github. **Iâ€™m not going
    to regurgitate the whole thing here**, Iâ€™ll just explain the most relevant parts
    and any hiccups I had to overcome to actually get it workingâ€¦code examples are
    never as straightforward as they seem!'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 1\. æˆ‘ä»¬å°†ä½¿ç”¨ä¸€ä¸ªé¢„æ„å»ºçš„ç¬”è®°æœ¬ç¤ºä¾‹ï¼š[https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/vertex_endpoints/torchserve/dreambooth_stablediffusion.ipynb](https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/vertex_endpoints/torchserve/dreambooth_stablediffusion.ipynb)ï¼Œè¯¥ç¤ºä¾‹æ¥è‡ª
    GCP Vertex AI åœ¨ Github ä¸Šçš„å®˜æ–¹ä»“åº“ã€‚**æˆ‘ä¸ä¼šåœ¨è¿™é‡Œè¯¦ç»†å¤è¿°æ•´ä¸ªå†…å®¹**ï¼Œåªä¼šè§£é‡Šæœ€ç›¸å…³çš„éƒ¨åˆ†ä»¥åŠæˆ‘åœ¨å®é™…æ“ä½œä¸­é‡åˆ°çš„é—®é¢˜â€¦â€¦ä»£ç ç¤ºä¾‹é€šå¸¸æ²¡æœ‰çœ‹èµ·æ¥é‚£ä¹ˆç®€å•ï¼
- en: âš ï¸ **Creating the user managed notebook and deploying the model to an endpoint
    in this example will incur costs**. You can get started with $300 of free credits
    when you create a GCP account for the first time, but the recommended hardware
    config (and any subsequent calls to the endpoint where your model is deployed)
    is likely to burn through those credits pretty quickly â€” you have been warned.
    Iâ€™ll share the actual costs I racked up in the Closing Thoughts section.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: âš ï¸ **åœ¨è¿™ä¸ªç¤ºä¾‹ä¸­åˆ›å»ºç”¨æˆ·ç®¡ç†çš„ç¬”è®°æœ¬å¹¶å°†æ¨¡å‹éƒ¨ç½²åˆ°ç«¯ç‚¹å°†äº§ç”Ÿè´¹ç”¨**ã€‚å½“ä½ ç¬¬ä¸€æ¬¡åˆ›å»º GCP è´¦æˆ·æ—¶ï¼Œå¯ä»¥è·å¾— $300 çš„å…è´¹ç§¯åˆ†ï¼Œä½†æ¨èçš„ç¡¬ä»¶é…ç½®ï¼ˆä»¥åŠæ¨¡å‹éƒ¨ç½²çš„ç«¯ç‚¹çš„ä»»ä½•åç»­è°ƒç”¨ï¼‰å¯èƒ½ä¼šå¾ˆå¿«æ¶ˆè€—è¿™äº›ç§¯åˆ†â€”â€”ä½ å·²ç»è¢«è­¦å‘Šäº†ã€‚æˆ‘å°†åœ¨â€œç»“è¯­â€éƒ¨åˆ†åˆ†äº«æˆ‘å®é™…äº§ç”Ÿçš„è´¹ç”¨ã€‚
- en: '2\. If this is the first time youâ€™re using Vertex AI, sign into your GCP console
    and be sure to enable all of the necessary APIs from the Vertex AI dashboard:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 2\. å¦‚æœè¿™æ˜¯ä½ ç¬¬ä¸€æ¬¡ä½¿ç”¨ Vertex AIï¼Œè¯·ç™»å½•åˆ°ä½ çš„ GCP æ§åˆ¶å°ï¼Œå¹¶ç¡®ä¿ä» Vertex AI ä»ªè¡¨æ¿å¯ç”¨æ‰€æœ‰å¿…è¦çš„ APIï¼š
- en: '![](../Images/f5477d0fd316762a0569855edf4e1316.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f5477d0fd316762a0569855edf4e1316.png)'
- en: '*Figure 6: Vertex AI dashboard in GCP, Enable All Recommended APIs button.*'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '*å›¾ 6: GCP ä¸­çš„ Vertex AI ä»ªè¡¨æ¿ï¼Œå¯ç”¨æ‰€æœ‰æ¨èçš„ API æŒ‰é’®ã€‚*'
- en: 3\. Open the [notebook](https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/vertex_endpoints/torchserve/dreambooth_stablediffusion.ipynb),
    just below its title, you should see some buttons including â€œRun in Colabâ€ and
    â€œ**Open in Vertex AI Workbench**â€. You want to click the latter.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 3\. æ‰“å¼€ [ç¬”è®°æœ¬](https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/vertex_endpoints/torchserve/dreambooth_stablediffusion.ipynb)ï¼Œåœ¨æ ‡é¢˜ä¸‹æ–¹ï¼Œä½ åº”è¯¥èƒ½çœ‹åˆ°ä¸€äº›æŒ‰é’®ï¼ŒåŒ…æ‹¬â€œåœ¨
    Colab ä¸­è¿è¡Œâ€å’Œâ€œ**åœ¨ Vertex AI Workbench ä¸­æ‰“å¼€**â€ã€‚ä½ éœ€è¦ç‚¹å‡»åè€…ã€‚
- en: 4\. Youâ€™ll then be asked to configure a VM instance on which to host a User
    Managed notebook. The example recommends using an NVIDIA A100 with 85GB RAM instance
    aka. â€œa2-highgpu-1gâ€. Inference (creating an image from the example in the notebook)
    was quick, returning an image from the Stable Diffusion model in about 11 seconds.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 4\. ç„¶åä½ éœ€è¦é…ç½®ä¸€ä¸ª VM å®ä¾‹æ¥æ‰˜ç®¡ç”¨æˆ·ç®¡ç†çš„ç¬”è®°æœ¬ã€‚ç¤ºä¾‹å»ºè®®ä½¿ç”¨å¸¦æœ‰ 85GB RAM çš„ NVIDIA A100 å®ä¾‹ï¼Œä¹Ÿå°±æ˜¯â€œa2-highgpu-1gâ€ã€‚æ¨ç†ï¼ˆä»ç¬”è®°æœ¬ä¸­çš„ç¤ºä¾‹åˆ›å»ºå›¾åƒï¼‰å¾ˆå¿«ï¼Œå¤§çº¦
    11 ç§’é’Ÿå°±èƒ½ä» Stable Diffusion æ¨¡å‹ä¸­è¿”å›å›¾åƒã€‚
- en: 5\. Once your instance has been created you can access it via *Workbench ->
    User Managed Notebooks.*
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 5\. ä¸€æ—¦ä½ çš„å®ä¾‹åˆ›å»ºå®Œæˆï¼Œä½ å¯ä»¥é€šè¿‡ *Workbench -> ç”¨æˆ·ç®¡ç†çš„ç¬”è®°æœ¬* è®¿é—®å®ƒã€‚
- en: '![](../Images/9f903e4a5efbd63cfc2ec27b1d02f917.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9f903e4a5efbd63cfc2ec27b1d02f917.png)'
- en: '*Figure 7: Vertex AI Workbench, User-Managed Notebooks list.*'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '*å›¾ 7ï¼šVertex AI å·¥ä½œå°ï¼Œç”¨æˆ·ç®¡ç†çš„ç¬”è®°æœ¬åˆ—è¡¨ã€‚*'
- en: Before you dive in and click *OPEN JUPYTERLAB*, make sure you give the Service
    Account for the VM your notebook is running on permissions to do things like create
    storage buckets and endpoints. **This will not be done for you automatically given
    its â€œUser Managedâ€.**
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä½ æ·±å…¥å¹¶ç‚¹å‡» *OPEN JUPYTERLAB* ä¹‹å‰ï¼Œç¡®ä¿ç»™ä½ çš„ç¬”è®°æœ¬æ‰€åœ¨ VM çš„æœåŠ¡è´¦æˆ·æˆäºˆæƒé™ï¼Œä»¥ä¾¿è¿›è¡Œè¯¸å¦‚åˆ›å»ºå­˜å‚¨æ¡¶å’Œç«¯ç‚¹ç­‰æ“ä½œã€‚**ç”±äºå®ƒæ˜¯â€œç”¨æˆ·ç®¡ç†â€çš„ï¼Œè¿™ä¸ä¼šè‡ªåŠ¨ä¸ºä½ å®Œæˆã€‚**
- en: 6\. Click on the notebook name, in the Notebook details you will see the Owner
    or service account alias.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 6\. ç‚¹å‡»ç¬”è®°æœ¬åç§°ï¼Œåœ¨ç¬”è®°æœ¬è¯¦ç»†ä¿¡æ¯ä¸­ä½ å°†çœ‹åˆ°æ‰€æœ‰è€…æˆ–æœåŠ¡è´¦æˆ·åˆ«åã€‚
- en: '![](../Images/63111df16dbce5738f2d20cba3cb731a.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/63111df16dbce5738f2d20cba3cb731a.png)'
- en: '*Figure 8: Vertex AI, Notebook details.*'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '*å›¾ 8ï¼šVertex AIï¼Œç¬”è®°æœ¬è¯¦ç»†ä¿¡æ¯ã€‚*'
- en: 7\. Go to *IAM & Admin -> IAM, then c*lick *Grant Access*, paste or type in
    the service account alias for your notebook instance and to make life simple I
    gave it blanket â€œEditorâ€ access. If youâ€™re finicky, you can grant only the specific
    permissions needed by the steps in the dreambooth_diffusion example.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 7\. è½¬åˆ° *IAM & Admin -> IAM*ï¼Œç„¶åç‚¹å‡» *æˆäºˆè®¿é—®æƒé™*ï¼Œç²˜è´´æˆ–è¾“å…¥ä½ çš„ç¬”è®°æœ¬å®ä¾‹çš„æœåŠ¡è´¦æˆ·åˆ«åï¼Œä¸ºäº†ç®€åŒ–æ“ä½œï¼Œæˆ‘æˆäºˆäº†â€œç¼–è¾‘è€…â€è®¿é—®æƒé™ã€‚å¦‚æœä½ å¾ˆæŒ‘å‰”ï¼Œä½ å¯ä»¥åªæˆäºˆ
    `dreambooth_diffusion` ç¤ºä¾‹æ­¥éª¤æ‰€éœ€çš„ç‰¹å®šæƒé™ã€‚
- en: '![](../Images/a28ceee2caddfcc72ef7537fb5b7a97d.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a28ceee2caddfcc72ef7537fb5b7a97d.png)'
- en: '*Figure 9: GCP IAM & Admin, IAM, Edit access/assign roles.*'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '*å›¾ 9ï¼šGCP IAM & Admin, IAM, ç¼–è¾‘è®¿é—®/åˆ†é…è§’è‰²ã€‚*'
- en: '8\. Now you should be ready to start running the dreambooth_diffusion.ipynb
    notebook code in Jupyterlab, open it from the Workbench (as shown in Step 5.).
    For whatever reason, the code from the exampleâ€™s git repo was not copied to my
    notebook instance so I just opened a terminal and did a quick clone of the Github
    repo:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 8\. ç°åœ¨ä½ åº”è¯¥å‡†å¤‡å¥½åœ¨ Jupyterlab ä¸­è¿è¡Œ `dreambooth_diffusion.ipynb` ç¬”è®°æœ¬ä»£ç äº†ï¼Œä»å·¥ä½œå°ä¸­æ‰“å¼€å®ƒï¼ˆå¦‚æ­¥éª¤
    5 æ‰€ç¤ºï¼‰ã€‚ç”±äºæŸç§åŸå› ï¼Œç¤ºä¾‹çš„ git ä»“åº“ä¸­çš„ä»£ç æ²¡æœ‰è¢«å¤åˆ¶åˆ°æˆ‘çš„ç¬”è®°æœ¬å®ä¾‹ä¸­ï¼Œæ‰€ä»¥æˆ‘åªæ˜¯æ‰“å¼€äº†ç»ˆç«¯å¹¶å¿«é€Ÿå…‹éš†äº† GitHub ä»“åº“ï¼š
- en: '[PRE6]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 9\. Once you have opened the dreambooth_diffusion.ipynb in Jupyterlab, you should
    be able to run the notebook cells without any major issues. The first part of
    the notebook runs through the steps to download the Stable Diffusion model and
    create an image from a prompt. The next step is to create a Vertex AI Endpoint
    and deploy the model there for serving.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 9\. ä¸€æ—¦ä½ åœ¨ Jupyterlab ä¸­æ‰“å¼€äº† `dreambooth_diffusion.ipynb`ï¼Œä½ åº”è¯¥èƒ½å¤Ÿè¿è¡Œç¬”è®°æœ¬å•å…ƒè€Œä¸ä¼šé‡åˆ°é‡å¤§é—®é¢˜ã€‚ç¬”è®°æœ¬çš„ç¬¬ä¸€éƒ¨åˆ†æ˜¯é€šè¿‡æ­¥éª¤ä¸‹è½½
    Stable Diffusion æ¨¡å‹å¹¶ä»æç¤ºä¸­åˆ›å»ºå›¾åƒã€‚ä¸‹ä¸€æ­¥æ˜¯åˆ›å»º Vertex AI ç«¯ç‚¹å¹¶å°†æ¨¡å‹éƒ¨ç½²åˆ°é‚£é‡Œè¿›è¡ŒæœåŠ¡ã€‚
- en: '10\. Follow the steps in the notebook to:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 10\. æŒ‰ç…§ç¬”è®°æœ¬ä¸­çš„æ­¥éª¤è¿›è¡Œï¼š
- en: a. Create a custom TorchServe handler.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: a. åˆ›å»ºè‡ªå®šä¹‰ TorchServe å¤„ç†ç¨‹åºã€‚
- en: b. Upload the model artifacts onto Google Cloud Storage.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: b. å°†æ¨¡å‹å·¥ä»¶ä¸Šä¼ åˆ° Google Cloud Storageã€‚
- en: c. Create a Vertex AI model with the model artifacts and a prebuilt PyTorch
    container image.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: c. ä½¿ç”¨æ¨¡å‹å·¥ä»¶å’Œé¢„æ„å»ºçš„ PyTorch å®¹å™¨é•œåƒåˆ›å»º Vertex AI æ¨¡å‹ã€‚
- en: d. Deploy the Vertex AI model onto an endpoint.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: d. å°† Vertex AI æ¨¡å‹éƒ¨ç½²åˆ°ç«¯ç‚¹ä¸Šã€‚
- en: 'This should all go smoothly, provided you enabled the Vertex API earlier (see
    Step 2\. If you forgot!). For me, after creating the endpoint, the model deployment
    took about 30 minutes. When itâ€™s ready to serve, you will see something like this
    under *Endpoints*:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: åªè¦ä½ åœ¨ä¹‹å‰å¯ç”¨äº† Vertex APIï¼ˆå‚è§æ­¥éª¤ 2ã€‚å¦‚æœä½ å¿˜è®°äº†ï¼ï¼‰ï¼Œè¿™ä¸€åˆ‡åº”è¯¥éƒ½ä¼šé¡ºåˆ©è¿›è¡Œã€‚å¯¹æˆ‘è€Œè¨€ï¼Œåœ¨åˆ›å»ºç«¯ç‚¹ä¹‹åï¼Œæ¨¡å‹éƒ¨ç½²å¤§çº¦èŠ±è´¹äº† 30 åˆ†é’Ÿã€‚å½“å®ƒå‡†å¤‡å¥½æä¾›æœåŠ¡æ—¶ï¼Œä½ ä¼šåœ¨
    *ç«¯ç‚¹* ä¸‹çœ‹åˆ°ç±»ä¼¼çš„å†…å®¹ï¼š
- en: '![](../Images/c7daecac9d9e3b07f85e43677b8db55e.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c7daecac9d9e3b07f85e43677b8db55e.png)'
- en: '*Figure 10: Vertex AI Endpoints.*'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '*å›¾ 10ï¼šVertex AI ç«¯ç‚¹ã€‚*'
- en: You are now ready to serve requests from your Stable Diffusion model!! At this
    point you can stop the notebook instance VM and delete the bucket you created
    if you want to save on costs. See the â€œCleaning upâ€ steps at the very end of dreambooth_diffusion.ipynb.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ ç°åœ¨å‡†å¤‡å¥½ä»ä½ çš„ Stable Diffusion æ¨¡å‹ä¸­å¤„ç†è¯·æ±‚äº†ï¼ï¼æ­¤æ—¶ï¼Œå¦‚æœä½ æƒ³èŠ‚çœæˆæœ¬ï¼Œå¯ä»¥åœæ­¢ç¬”è®°æœ¬å®ä¾‹ VM å¹¶åˆ é™¤ä½ åˆ›å»ºçš„å­˜å‚¨æ¡¶ã€‚è¯·å‚è§
    `dreambooth_diffusion.ipynb` æœ€åéƒ¨åˆ†çš„â€œæ¸…ç†â€æ­¥éª¤ã€‚
- en: ğŸ§ª Test your Endpoint
  id: totrans-107
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ğŸ§ª æµ‹è¯•ä½ çš„ç«¯ç‚¹
- en: To send a request to a Vertex AI endpoint, you will need to use an HTTP client
    library or a command-line tool that supports sending requests with the appropriate
    HTTP method and request parameters.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: è¦å‘ Vertex AI ç«¯ç‚¹å‘é€è¯·æ±‚ï¼Œä½ éœ€è¦ä½¿ç”¨æ”¯æŒå‘é€å…·æœ‰é€‚å½“ HTTP æ–¹æ³•å’Œè¯·æ±‚å‚æ•°çš„è¯·æ±‚çš„ HTTP å®¢æˆ·ç«¯åº“æˆ–å‘½ä»¤è¡Œå·¥å…·ã€‚
- en: I carried out testing locally on my laptop. To do this you will need to download
    and install the [Vertex AI SDK for Python](https://cloud.google.com/python/docs/reference/aiplatform/latest/index.html#endpoints),
    then create and download a service key for authentication.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘åœ¨æˆ‘çš„ç¬”è®°æœ¬ç”µè„‘ä¸Šè¿›è¡Œäº†æœ¬åœ°æµ‹è¯•ã€‚ä¸ºæ­¤ï¼Œä½ éœ€è¦ä¸‹è½½å¹¶å®‰è£… [Vertex AI Python SDK](https://cloud.google.com/python/docs/reference/aiplatform/latest/index.html#endpoints)ï¼Œç„¶ååˆ›å»ºå¹¶ä¸‹è½½ä¸€ä¸ªæœåŠ¡å¯†é’¥ç”¨äºè®¤è¯ã€‚
- en: If you kept your notebook instance VM, you could use the same service account
    alias as before or just create a new one with permissions to, at a minimum, [get
    predictions from an endpoint](https://cloud.google.com/vertex-ai/docs/general/iam-permissions).
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœä½ ä¿ç•™äº†ä½ çš„ç¬”è®°æœ¬å®ä¾‹è™šæ‹Ÿæœºï¼Œä½ å¯ä»¥ä½¿ç”¨ä¹‹å‰çš„æœåŠ¡å¸æˆ·åˆ«åï¼Œæˆ–è€…åªéœ€åˆ›å»ºä¸€ä¸ªæ–°çš„æœåŠ¡å¸æˆ·ï¼Œå…¶æƒé™è‡³å°‘åŒ…æ‹¬[ä»ç«¯ç‚¹è·å–é¢„æµ‹](https://cloud.google.com/vertex-ai/docs/general/iam-permissions)ã€‚
- en: '1\. Go to *IAM & Admin -> Service Accounts*. Then click on the three dots under
    Action on the right of the service account alias you want to create keys for and
    then click *Manage Keys*:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 1\. è½¬åˆ° *IAM & Admin -> æœåŠ¡å¸æˆ·*ã€‚ç„¶åç‚¹å‡»æœåŠ¡å¸æˆ·åˆ«åå³ä¾§æ“ä½œä¸‹çš„ä¸‰ä¸ªç‚¹ï¼Œç„¶åç‚¹å‡» *ç®¡ç†å¯†é’¥*ï¼š
- en: '![](../Images/855a1d349360219d1836d9c84000eb22.png)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/855a1d349360219d1836d9c84000eb22.png)'
- en: '*Figure 11: GCP IAM & Admin, Service accounts, manage keys for a service account.*'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '*å›¾ 11ï¼šGCP IAM & Adminï¼ŒæœåŠ¡å¸æˆ·ï¼Œç®¡ç†æœåŠ¡å¸æˆ·çš„å¯†é’¥ã€‚*'
- en: 'then click *Add Key -> Create new Key* and download as in the recommended .JSON
    format:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åç‚¹å‡»*æ·»åŠ å¯†é’¥ -> åˆ›å»ºæ–°å¯†é’¥*ï¼Œå¹¶æŒ‰æ¨èçš„ .JSON æ ¼å¼ä¸‹è½½ï¼š
- en: '![](../Images/658dd6110b2eb15cc1c7f0f98d6f40c5.png)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/658dd6110b2eb15cc1c7f0f98d6f40c5.png)'
- en: '*Figure 12: GCP IAM & Admin, Service accounts, create key for a service account.*'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '*å›¾ 12ï¼šGCP IAM & Adminï¼ŒæœåŠ¡å¸æˆ·ï¼Œä¸ºæœåŠ¡å¸æˆ·åˆ›å»ºå¯†é’¥ã€‚*'
- en: âš ï¸ **Remember, a service account key file grants the same permissions in your
    GCP project as the service account itself.** Always be very careful with the file,
    delete it when itâ€™s no longer required and never, ever, upload it to a Github
    repo, Iâ€™ve seen this happen way too many times!
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: âš ï¸ **è¯·è®°ä½ï¼ŒæœåŠ¡å¸æˆ·å¯†é’¥æ–‡ä»¶æˆäºˆä¸ä½ çš„ GCP é¡¹ç›®ä¸­æœåŠ¡å¸æˆ·ç›¸åŒçš„æƒé™ã€‚** å§‹ç»ˆéå¸¸å°å¿ƒå¤„ç†è¯¥æ–‡ä»¶ï¼Œåˆ é™¤å®ƒæ—¶ä¸è¦å†ä½¿ç”¨ï¼Œå¹¶ä¸”æ°¸è¿œä¸è¦å°†å…¶ä¸Šä¼ åˆ°
    Github ä»“åº“ï¼Œæˆ‘è§è¿‡å¤ªå¤šè¿™ç§æƒ…å†µï¼
- en: '2\. In a terminal window, or from wherever youâ€™re going to run your code to
    test your endpoint, youâ€™ll want to set a variable to point to the service key
    you downloaded. This will authenticate requests to your endpoint:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 2\. åœ¨ç»ˆç«¯çª—å£ä¸­ï¼Œæˆ–åœ¨ä½ è¿è¡Œä»£ç ä»¥æµ‹è¯•ç«¯ç‚¹çš„åœ°æ–¹ï¼Œä½ éœ€è¦è®¾ç½®ä¸€ä¸ªå˜é‡ä»¥æŒ‡å‘ä½ ä¸‹è½½çš„æœåŠ¡å¯†é’¥ã€‚è¿™å°†å¯¹ç«¯ç‚¹è¯·æ±‚è¿›è¡Œè®¤è¯ï¼š
- en: '[PRE7]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '3\. Now just modify this python code snippet with your Project Name, Region,
    and Endpoint ID. It will pass a prompt to your endpoint and store the response
    i.e. image generated by Stable Diffusion in a JPEG file:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 3\. ç°åœ¨åªéœ€ç”¨ä½ çš„é¡¹ç›®åç§°ã€åŒºåŸŸå’Œç«¯ç‚¹ ID ä¿®æ”¹è¿™ä¸ª Python ä»£ç ç‰‡æ®µã€‚å®ƒå°†å‘ä½ çš„ç«¯ç‚¹ä¼ é€’ä¸€ä¸ªæç¤ºï¼Œå¹¶å°†å“åº”ï¼ˆå³ç”± Stable Diffusion
    ç”Ÿæˆçš„å›¾åƒï¼‰å­˜å‚¨åœ¨ JPEG æ–‡ä»¶ä¸­ï¼š
- en: '[PRE8]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: If youâ€™ve installed the Vertex AI for Python SDK, authenticated to GCP, and
    the endpoint is active, after a few seconds you should see the generated image
    file appear in your file system!
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœä½ å®‰è£…äº† Vertex AI Python SDKã€è¿›è¡Œäº† GCP è®¤è¯ï¼Œå¹¶ä¸”ç«¯ç‚¹å¤„äºæ´»åŠ¨çŠ¶æ€ï¼Œå‡ ç§’é’Ÿåä½ åº”è¯¥ä¼šçœ‹åˆ°ç”Ÿæˆçš„å›¾åƒæ–‡ä»¶å‡ºç°åœ¨ä½ çš„æ–‡ä»¶ç³»ç»Ÿä¸­ï¼
- en: ğŸ 3\. Create a simple RESTful API using FLASK
  id: totrans-123
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ğŸ 3\. ä½¿ç”¨ FLASK åˆ›å»ºä¸€ä¸ªç®€å•çš„ RESTful API
- en: At this point you could quite easily integrate the code to call your generative
    AI model endpoint with an existing app using the Vertex AI Python SDK.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤æ—¶ï¼Œä½ å¯ä»¥å¾ˆå®¹æ˜“åœ°å°†è°ƒç”¨ç”Ÿæˆ AI æ¨¡å‹ç«¯ç‚¹çš„ä»£ç é›†æˆåˆ°ä½¿ç”¨ Vertex AI Python SDK çš„ç°æœ‰åº”ç”¨ç¨‹åºä¸­ã€‚
- en: But, I did promise an API, so that is what Iâ€™m going to run through in this
    final part of the section.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†æˆ‘ç¡®å®æ‰¿è¯ºäº†ä¸€ä¸ª APIï¼Œæ‰€ä»¥æˆ‘å°†åœ¨è¿™ä¸€éƒ¨åˆ†çš„æœ€åéƒ¨åˆ†è¯¦ç»†ä»‹ç»å®ƒã€‚
- en: '**Prerequisites:**'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '**å‰ææ¡ä»¶ï¼š**'
- en: Install [Flask](https://pypi.org/project/Flask/) and [Pillow](https://pypi.org/project/Pillow/)
    python libraries.
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å®‰è£… [Flask](https://pypi.org/project/Flask/) å’Œ [Pillow](https://pypi.org/project/Pillow/)
    Python åº“ã€‚
- en: '[Download Postman](https://www.postman.com/downloads/?utm_source=postman-home)
    and install, itâ€™s free! Weâ€™ll use it to simulate calls to our API.'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[ä¸‹è½½ Postman](https://www.postman.com/downloads/?utm_source=postman-home) å¹¶å®‰è£…ï¼Œå®ƒæ˜¯å…è´¹çš„ï¼æˆ‘ä»¬å°†ç”¨å®ƒæ¥æ¨¡æ‹Ÿå¯¹æˆ‘ä»¬çš„
    API çš„è°ƒç”¨ã€‚'
- en: 1\. As in the previous section, make sure you have an environment variable from
    where you plan to run your code, that points to your service account key.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 1\. ä¸å‰ä¸€èŠ‚ä¸€æ ·ï¼Œç¡®ä¿ä½ æœ‰ä¸€ä¸ªç¯å¢ƒå˜é‡ï¼ŒæŒ‡å‘ä½ è®¡åˆ’è¿è¡Œä»£ç çš„åœ°æ–¹ï¼Œè¿™ä¸ªå˜é‡æŒ‡å‘ä½ çš„æœåŠ¡å¸æˆ·å¯†é’¥ã€‚
- en: '2\. Here is the code you need to create a simple RESTful API using the Flask
    web application framework. See the comments in the code for an explanation of
    whatâ€™s going on. We basically used the code from earlier to query the endpoint
    and wrapped it in an API call:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 2\. è¿™æ˜¯ä½ éœ€è¦çš„ä»£ç ï¼Œç”¨äºä½¿ç”¨ Flask Web åº”ç”¨ç¨‹åºæ¡†æ¶åˆ›å»ºä¸€ä¸ªç®€å•çš„ RESTful APIã€‚è¯·å‚è§ä»£ç ä¸­çš„æ³¨é‡Šï¼Œäº†è§£æ­£åœ¨å‘ç”Ÿçš„äº‹æƒ…ã€‚æˆ‘ä»¬åŸºæœ¬ä¸Šä½¿ç”¨äº†ä¹‹å‰çš„ä»£ç æ¥æŸ¥è¯¢ç«¯ç‚¹ï¼Œå¹¶å°†å…¶å°è£…åœ¨
    API è°ƒç”¨ä¸­ï¼š
- en: '[PRE9]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '3\. Run the resulting file, and you should have a Flask server hosting your
    API, ready to receive requests:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 3\. è¿è¡Œç”Ÿæˆçš„æ–‡ä»¶ï¼Œä½ åº”è¯¥ä¼šæœ‰ä¸€ä¸ªæ‰˜ç®¡ä½  API çš„ Flask æœåŠ¡å™¨ï¼Œå‡†å¤‡æ¥æ”¶è¯·æ±‚ï¼š
- en: '![](../Images/7a83c0c778babc2f9b0881ebd22530da.png)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7a83c0c778babc2f9b0881ebd22530da.png)'
- en: '*Figure 13: MacOS terminal, running Flask server locally.*'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '*å›¾ 13ï¼šMacOS ç»ˆç«¯ï¼Œæœ¬åœ°è¿è¡Œ Flask æœåŠ¡å™¨ã€‚*'
- en: '4\. Next weâ€™re going to fire up Postman, go to *File -> Import*, paste in the
    following cURL command (you might need to modify the server address if you have
    it configured differently to the default):'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 4\. æ¥ä¸‹æ¥æˆ‘ä»¬å°†å¯åŠ¨ Postmanï¼Œè¿›å…¥ *æ–‡ä»¶ -> å¯¼å…¥*ï¼Œç²˜è´´ä»¥ä¸‹ cURL å‘½ä»¤ï¼ˆå¦‚æœä½ çš„æœåŠ¡å™¨åœ°å€é…ç½®ä¸é»˜è®¤ä¸åŒï¼Œä½ å¯èƒ½éœ€è¦ä¿®æ”¹å®ƒï¼‰ï¼š
- en: '[PRE10]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: We could run this from a terminal and the response from our API would be saved
    as a JPEG image in our local file system. But to simulate an app, and because
    I didnâ€™t have time to write a Discord bot or HTML front endâ€¦ ğŸ˜…
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥ä»ç»ˆç«¯è¿è¡Œè¿™ä¸ªï¼ŒAPI çš„å“åº”ä¼šè¢«ä¿å­˜ä¸ºæœ¬åœ°æ–‡ä»¶ç³»ç»Ÿä¸­çš„ JPEG å›¾åƒã€‚ä½†ä¸ºäº†æ¨¡æ‹Ÿä¸€ä¸ªåº”ç”¨ç¨‹åºï¼Œè€Œä¸”å› ä¸ºæˆ‘æ²¡æœ‰æ—¶é—´ç¼–å†™ Discord æœºå™¨äººæˆ–
    HTML å‰ç«¯â€¦ ğŸ˜…
- en: '![](../Images/1ab9d69c5b3463030296060bc70e88b2.png)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1ab9d69c5b3463030296060bc70e88b2.png)'
- en: '*Figure 14: Postman, API request example, image response generated by Stable
    Diffusion model.*'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '*å›¾ 14ï¼šPostmanï¼ŒAPI è¯·æ±‚ç¤ºä¾‹ï¼ŒStable Diffusion æ¨¡å‹ç”Ÿæˆçš„å›¾åƒå“åº”ã€‚*'
- en: If all has gone well, youâ€™ll end up with a nicely generated image, served from
    your very own API. You can modify the prompt under the Body tab, creativity is
    your only limit!
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœä¸€åˆ‡é¡ºåˆ©ï¼Œä½ å°†å¾—åˆ°ä¸€ä¸ªæ¼‚äº®ç”Ÿæˆçš„å›¾åƒï¼Œé€šè¿‡ä½ è‡ªå·±çš„ API æä¾›æœåŠ¡ã€‚ä½ å¯ä»¥åœ¨â€œBodyâ€æ ‡ç­¾ä¸‹ä¿®æ”¹æç¤ºï¼Œåˆ›é€ åŠ›æ˜¯å”¯ä¸€çš„é™åˆ¶ï¼
- en: 'ğŸ—‘ï¸ **Done experimenting?** You can go back to *Vertex AI -> Endpoints*, select
    the endpoint, un-deploy the model, go back up a level to the endpoint and delete
    it (hint: triple dots menu at the end of each line item). Double check you spun
    down or deleted anything else related to this post so youâ€™re not consuming resources.
    You can also revoke the service account key you created just to be safe.'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ—‘ï¸ **å®éªŒå®Œæˆäº†å—ï¼Ÿ** ä½ å¯ä»¥è¿”å›åˆ° *Vertex AI -> ç«¯ç‚¹*ï¼Œé€‰æ‹©ç«¯ç‚¹ï¼Œæ’¤é”€æ¨¡å‹éƒ¨ç½²ï¼Œè¿”å›ä¸Šä¸€å±‚åˆ é™¤å®ƒï¼ˆæç¤ºï¼šæ¯è¡Œæœ«å°¾çš„ä¸‰ç‚¹èœå•ï¼‰ã€‚ä»”ç»†æ£€æŸ¥ä½ æ˜¯å¦å…³é—­æˆ–åˆ é™¤äº†ä¸è¿™ç¯‡æ–‡ç« ç›¸å…³çš„ä»»ä½•å…¶ä»–å†…å®¹ï¼Œä»¥é¿å…æ¶ˆè€—èµ„æºã€‚ä½ ä¹Ÿå¯ä»¥æ’¤é”€ä½ åˆ›å»ºçš„æœåŠ¡è´¦æˆ·å¯†é’¥ï¼Œä»¥ç¡®ä¿å®‰å…¨ã€‚
- en: âš ï¸ **Final warning â€”** this is clearly not production ready code. If you were
    to make your API public facing thereâ€™s work to be done around authentication,
    amongst other things that are out of scope for this post! Relatively speaking
    the cost per request (or compute hours) for our endpoint is also much more expensive
    than say Midjourney or DALL-E 2, so probably not a viable thing for you to launch
    as a service.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: âš ï¸ **æœ€åè­¦å‘Š â€”** è¿™æ˜¾ç„¶ä¸æ˜¯ç”Ÿäº§å°±ç»ªçš„ä»£ç ã€‚å¦‚æœä½ æ‰“ç®—è®©ä½ çš„ API å¯¹å¤–å…¬å¼€ï¼Œè¿˜æœ‰å¾ˆå¤šå·¥ä½œéœ€è¦åšï¼Œæ¯”å¦‚è®¤è¯ç­‰ï¼Œè¿™äº›è¶…å‡ºäº†æœ¬æ–‡çš„èŒƒå›´ï¼ç›¸å¯¹è€Œè¨€ï¼Œæˆ‘ä»¬çš„ç«¯ç‚¹æ¯æ¬¡è¯·æ±‚çš„æˆæœ¬ï¼ˆæˆ–è®¡ç®—æ—¶é—´ï¼‰ä¹Ÿæ¯”
    Midjourney æˆ– DALL-E 2 é«˜å¾—å¤šï¼Œå› æ­¤å¯èƒ½ä¸é€‚åˆä½œä¸ºæœåŠ¡ä¸Šçº¿ã€‚
- en: ğŸ† Wrap up and closing thoughts
  id: totrans-143
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ğŸ† æ€»ç»“å’Œç»“æŸæ€è€ƒ
- en: I had **a lot** of fun writing this post, and learnt loads about the generative
    AI space, Stable Diffusion and what it means to package it up into something loosely
    resembling one of the popular consumer facing services available today. I salute
    those dev teams on the bleeding edge of this technology, itâ€™s such an exciting
    space to be in at the moment!
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘åœ¨å†™è¿™ç¯‡æ–‡ç« æ—¶**éå¸¸**å¼€å¿ƒï¼Œå¹¶ä¸”å­¦åˆ°äº†å¾ˆå¤šå…³äºç”Ÿæˆå¼ AIã€Stable Diffusion ä»¥åŠå°†å…¶æ‰“åŒ…æˆç±»ä¼¼ä»Šå¤©æµè¡Œçš„æ¶ˆè´¹è€…æœåŠ¡çš„å†…å®¹ã€‚æˆ‘å‘é‚£äº›ç«™åœ¨è¿™é¡¹æŠ€æœ¯æœ€å‰æ²¿çš„å¼€å‘å›¢é˜Ÿè‡´æ•¬ï¼Œè¿™ç¡®å®æ˜¯ä¸€ä¸ªä»¤äººå…´å¥‹çš„é¢†åŸŸï¼
- en: ğŸ’¸ Costs
  id: totrans-145
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ğŸ’¸ æˆæœ¬
- en: I was a little nervous about running large GPU attached instances. If youâ€™re
    a long time public cloud user, you will know how easy it is to overspend without
    the right checks and balances in place.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘æœ‰ç‚¹æ‹…å¿ƒè¿è¡Œå¤§å‹ GPU é™„åŠ å®ä¾‹ã€‚å¦‚æœä½ æ˜¯é•¿æœŸä½¿ç”¨å…¬å…±äº‘çš„ç”¨æˆ·ï¼Œä½ ä¼šçŸ¥é“æ²¡æœ‰é€‚å½“çš„æ£€æŸ¥å’Œåˆ¶è¡¡æœºåˆ¶ï¼Œå¾ˆå®¹æ˜“è¶…æ”¯ã€‚
- en: At the end of writing this post, I had a look at the Billing section in my GCP
    console to see what the damage wasâ€¦
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å†™å®Œè¿™ç¯‡æ–‡ç« çš„æœ€åï¼Œæˆ‘æŸ¥çœ‹äº† GCP æ§åˆ¶å°ä¸­çš„è´¦å•éƒ¨åˆ†ï¼Œçœ‹çœ‹èŠ±äº†å¤šå°‘é’±â€¦
- en: '![](../Images/f0e6619fbe9fabd43cfb448f83477b81.png)'
  id: totrans-148
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f0e6619fbe9fabd43cfb448f83477b81.png)'
- en: '*Figure 15: GCP Billing, Reports.*'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '*å›¾ 15ï¼šGCP è´¦å•ï¼ŒæŠ¥å‘Šã€‚*'
- en: During the two days that I was using GCP, with a project created exclusively
    for this post, I spent about USD $40\. You can see the breakdown in the screenshot
    above.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä½¿ç”¨ GCP çš„ä¸¤å¤©é‡Œï¼Œä¸“é—¨ä¸ºè¿™ç¯‡æ–‡ç« åˆ›å»ºçš„é¡¹ç›®ä¸­ï¼Œæˆ‘èŠ±è´¹äº†å¤§çº¦ 40 ç¾å…ƒã€‚ä½ å¯ä»¥åœ¨ä¸Šé¢çš„æˆªå›¾ä¸­çœ‹åˆ°è¯¦ç»†åˆ†è§£ã€‚
- en: Most of the cost was related to inference (see the Vertex AI service line item).
    This is the cost in â€˜compute hoursâ€™ for making my endpoint available to generate
    images using the Stable Diffusion model I deployed there.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: å¤§éƒ¨åˆ†è´¹ç”¨ä¸æ¨ç†ç›¸å…³ï¼ˆå‚è§ Vertex AI æœåŠ¡æ¡ç›®ï¼‰ã€‚è¿™æ˜¯ä½¿æˆ‘çš„ç«¯ç‚¹èƒ½å¤Ÿä½¿ç”¨æˆ‘éƒ¨ç½²çš„ Stable Diffusion æ¨¡å‹ç”Ÿæˆå›¾åƒçš„â€œè®¡ç®—å°æ—¶â€è´¹ç”¨ã€‚
- en: The endpoint used the â€œVertex AI:Online/Batch Prediction NVIDIA Tesla P100 GPU
    running in Americas for AI Platform SKUâ€, with some other costs for running a
    Predefined instance. We could have trimmed the fat by choosing a cheaper GPU family,
    but the trade off would be an increased time period to get back a generated image.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: ç«¯ç‚¹ä½¿ç”¨äº†â€œVertex AI:åœ¨çº¿/æ‰¹å¤„ç†é¢„æµ‹ NVIDIA Tesla P100 GPU è¿è¡Œäºç¾æ´²çš„ AI å¹³å° SKUâ€ï¼Œè¿˜åŒ…æ‹¬è¿è¡Œé¢„å®šä¹‰å®ä¾‹çš„ä¸€äº›å…¶ä»–è´¹ç”¨ã€‚æˆ‘ä»¬æœ¬å¯ä»¥é€šè¿‡é€‰æ‹©æ›´ä¾¿å®œçš„
    GPU å®¶æ—æ¥å‡å°‘å¼€æ”¯ï¼Œä½†è¿™å°†å¯¼è‡´ç”Ÿæˆå›¾åƒçš„æ—¶é—´å»¶é•¿ã€‚
- en: Comparing this to a commercially available solution, the Midjourney [basic plan](https://docs.midjourney.com/docs/plans)
    costs USD $10 on a monthly subscription. For that you get 3.3 fast GPU hrs/month,
    then it costs another $4/hr beyond that. In our case, I left the endpoint up for
    about 24 hours and spent $30 with no limitations on concurrency or jobs waiting.
    Again thereâ€™s a tradeoff between a fully managed service like Midjourney whose
    engineers are always making improvements to the model vs. iterating and deploying
    your own models on scalable cloud infrastructure.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸å•†ä¸šä¸Šå¯ç”¨çš„è§£å†³æ–¹æ¡ˆç›¸æ¯”ï¼ŒMidjourney çš„ [åŸºç¡€è®¡åˆ’](https://docs.midjourney.com/docs/plans) æ¯æœˆè®¢é˜…è´¹ç”¨ä¸º
    10 ç¾å…ƒã€‚è¿™ä¸ªè®¡åˆ’åŒ…æ‹¬æ¯æœˆ 3.3 å°æ—¶çš„å¿«é€Ÿ GPU è®¡ç®—ï¼Œè¶…å‡ºéƒ¨åˆ†è´¹ç”¨ä¸ºæ¯å°æ—¶ 4 ç¾å…ƒã€‚åœ¨æˆ‘ä»¬çš„æ¡ˆä¾‹ä¸­ï¼Œæˆ‘è®©ç«¯ç‚¹è¿è¡Œäº†å¤§çº¦ 24 å°æ—¶ï¼ŒèŠ±è´¹äº† 30
    ç¾å…ƒï¼Œæ²¡æœ‰å¹¶å‘æˆ–ä½œä¸šç­‰å¾…çš„é™åˆ¶ã€‚å†æ¬¡è¯´æ˜ï¼Œä¸ Midjourney è¿™æ ·çš„å®Œå…¨æ‰˜ç®¡æœåŠ¡ç›¸æ¯”ï¼Œå·¥ç¨‹å¸ˆä»¬å§‹ç»ˆåœ¨æ”¹è¿›æ¨¡å‹ï¼Œè€Œé€‰æ‹©åœ¨å¯æ‰©å±•çš„äº‘åŸºç¡€è®¾æ–½ä¸Šè¿­ä»£å’Œéƒ¨ç½²è‡ªå·±çš„æ¨¡å‹ä¹‹é—´å­˜åœ¨æƒè¡¡ã€‚
- en: '**The important point to note here is that youâ€™re not paying per prediction
    request, but for how long your endpoint is up and the size/type of instance it
    runs on.**'
  id: totrans-154
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**è¿™é‡Œéœ€è¦æ³¨æ„çš„é‡è¦ä¸€ç‚¹æ˜¯ï¼Œä½ æ”¯ä»˜çš„ä¸æ˜¯æ¯æ¬¡é¢„æµ‹è¯·æ±‚çš„è´¹ç”¨ï¼Œè€Œæ˜¯ä½ çš„ç«¯ç‚¹è¿è¡Œçš„æ—¶é—´ä»¥åŠå®ƒæ‰€è¿è¡Œçš„å®ä¾‹çš„å¤§å°/ç±»å‹ã€‚**'
- en: The cost to test the model and actually deploy the endpoint via a User Managed
    notebook came in at just under USD $10 (see Compute Engine & Notebooks line items).
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: æµ‹è¯•æ¨¡å‹å’Œé€šè¿‡ç”¨æˆ·ç®¡ç†çš„ç¬”è®°æœ¬å®é™…éƒ¨ç½²ç«¯ç‚¹çš„è´¹ç”¨ä¸åˆ° 10 ç¾å…ƒï¼ˆå‚è§è®¡ç®—å¼•æ“å’Œç¬”è®°æœ¬æ¡ç›®ï¼‰ã€‚
- en: So there you have it, in this post I gave a brief overview of generative AI
    and in particular its use for creating art using the Stable Diffusion technique.
    We then dived into some code examples to demonstrate the steps needed to generate
    some art using the Stable Diffusion model and indeed how to deploy it to an endpoint
    and consume it via an API. **I hope you enjoyed this post, and Iâ€™ll see you in
    the next!** ğŸ‘‹ğŸ¼
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥ï¼Œæœ¬æ–‡ç®€è¦æ¦‚è¿°äº†ç”Ÿæˆå¼äººå·¥æ™ºèƒ½ï¼Œç‰¹åˆ«æ˜¯ä½¿ç”¨ Stable Diffusion æŠ€æœ¯åˆ›å»ºè‰ºæœ¯çš„åº”ç”¨ã€‚æ¥ç€ï¼Œæˆ‘ä»¬æ·±å…¥æ¢è®¨äº†ä¸€äº›ä»£ç ç¤ºä¾‹ï¼Œä»¥å±•ç¤ºä½¿ç”¨ Stable
    Diffusion æ¨¡å‹ç”Ÿæˆè‰ºæœ¯çš„æ­¥éª¤ï¼Œä»¥åŠå¦‚ä½•å°†å…¶éƒ¨ç½²åˆ°ç«¯ç‚¹å¹¶é€šè¿‡ API ä½¿ç”¨ã€‚**å¸Œæœ›ä½ å–œæ¬¢è¿™ç¯‡æ–‡ç« ï¼Œæˆ‘ä»¬ä¸‹æ¬¡è§ï¼** ğŸ‘‹ğŸ¼
- en: The figures in this post were created by the author, unless otherwise stated.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬æ–‡ä¸­çš„æ•°æ®ç”±ä½œè€…åˆ›å»ºï¼Œé™¤éå¦æœ‰è¯´æ˜ã€‚
- en: ğŸ“‡ References
  id: totrans-158
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ğŸ“‡ å‚è€ƒèµ„æ–™
- en: '[1] Machine Learning, History and relationships to other fields: [https://en.wikipedia.org/wiki/Machine_learning#History_and_relationships_to_other_fields](https://en.wikipedia.org/wiki/Machine_learning#History_and_relationships_to_other_fields)'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] æœºå™¨å­¦ä¹ ï¼Œå†å²åŠå…¶ä¸å…¶ä»–é¢†åŸŸçš„å…³ç³»: [https://en.wikipedia.org/wiki/Machine_learning#History_and_relationships_to_other_fields](https://en.wikipedia.org/wiki/Machine_learning#History_and_relationships_to_other_fields)'
- en: '[2] What is generative AI? Everything you need to know, George Lawton: [https://www.techtarget.com/searchenterpriseai/definition/generative-AI](https://www.techtarget.com/searchenterpriseai/definition/generative-AI)'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] ä»€ä¹ˆæ˜¯ç”Ÿæˆå¼äººå·¥æ™ºèƒ½ï¼Ÿä½ éœ€è¦çŸ¥é“çš„ä¸€åˆ‡ï¼ŒGeorge Lawton: [https://www.techtarget.com/searchenterpriseai/definition/generative-AI](https://www.techtarget.com/searchenterpriseai/definition/generative-AI)'
- en: '[3] The implications of Generative AI for businesses â€” A new frontier in Artificial
    Intelligence, Deloitte LLP: [https://www2.deloitte.com/us/en/pages/consulting/articles/generative-artificial-intelligence.html](https://www2.deloitte.com/us/en/pages/consulting/articles/generative-artificial-intelligence.html)'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] ç”Ÿæˆå¼äººå·¥æ™ºèƒ½å¯¹ä¼ä¸šçš„å½±å“â€”â€”äººå·¥æ™ºèƒ½çš„æ–°å‰æ²¿ï¼ŒDeloitte LLP: [https://www2.deloitte.com/us/en/pages/consulting/articles/generative-artificial-intelligence.html](https://www2.deloitte.com/us/en/pages/consulting/articles/generative-artificial-intelligence.html)'
- en: '[4] Midjourney docs, Version: [https://docs.midjourney.com/docs/model-versions#:~:text=Current%20Model,places%2C%20objects%2C%20and%20more](https://docs.midjourney.com/docs/model-versions#:~:text=Current%20Model,places%2C%20objects%2C%20and%20more).'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] Midjourney æ–‡æ¡£ï¼Œç‰ˆæœ¬: [https://docs.midjourney.com/docs/model-versions#:~:text=Current%20Model,places%2C%20objects%2C%20and%20more](https://docs.midjourney.com/docs/model-versions#:~:text=Current%20Model,places%2C%20objects%2C%20and%20more)ã€‚'
- en: ğŸ“š Useful Resources
  id: totrans-163
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ğŸ“š æœ‰ç”¨èµ„æº
- en: 'You can find the completed notebook (StableDiffusion_Hugging_Face.ipynb) from
    â€œ1\. Experimenting with Stable Diffusionâ€ and python code from â€œ2\. Serve the
    model via an API -> endpointâ€ here: [https://github.com/omermx/medium_text_to_image_api](https://github.com/omermx/medium_text_to_image_api)'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä½ å¯ä»¥ä»â€œ1. å®éªŒç¨³å®šæ‰©æ•£â€å’Œâ€œ2. é€šè¿‡ API æä¾›æ¨¡å‹ -> ç«¯ç‚¹â€ä¸­æ‰¾åˆ°å®Œæ•´çš„ç¬”è®°æœ¬ï¼ˆStableDiffusion_Hugging_Face.ipynbï¼‰å’Œ
    Python ä»£ç ï¼Œé“¾æ¥å¦‚ä¸‹ï¼š[https://github.com/omermx/medium_text_to_image_api](https://github.com/omermx/medium_text_to_image_api)
- en: 'You can swap out the version Stable Diffusion I used in my examples (1.5) with
    the latest at the time of writing (2.1), here on Hugging Face: [https://huggingface.co/stabilityai/stable-diffusion-2-1](https://huggingface.co/stabilityai/stable-diffusion-2-1)'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä½ å¯ä»¥å°†æˆ‘åœ¨ç¤ºä¾‹ä¸­ä½¿ç”¨çš„ç¨³å®šæ‰©æ•£ç‰ˆæœ¬ï¼ˆ1.5ï¼‰æ›¿æ¢ä¸ºå†™ä½œæ—¶çš„æœ€æ–°ç‰ˆæœ¬ï¼ˆ2.1ï¼‰ï¼Œè¯·è®¿é—® Hugging Faceï¼š[https://huggingface.co/stabilityai/stable-diffusion-2-1](https://huggingface.co/stabilityai/stable-diffusion-2-1)
