- en: A Deep Dive into the Code of the Visual Transformer (ViT) Model
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深入探讨Visual Transformer (ViT) 模型的代码
- en: 原文：[https://towardsdatascience.com/a-deep-dive-into-the-code-of-the-visual-transformer-vit-model-1ce4cc05ca8d](https://towardsdatascience.com/a-deep-dive-into-the-code-of-the-visual-transformer-vit-model-1ce4cc05ca8d)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/a-deep-dive-into-the-code-of-the-visual-transformer-vit-model-1ce4cc05ca8d](https://towardsdatascience.com/a-deep-dive-into-the-code-of-the-visual-transformer-vit-model-1ce4cc05ca8d)
- en: Breaking down the HuggingFace ViT Implementation
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分析HuggingFace ViT实现
- en: '[](https://medium.com/@alexml0123?source=post_page-----1ce4cc05ca8d--------------------------------)[![Alexey
    Kravets](../Images/3b31f9b3c73c6c7ca709f845e6f70023.png)](https://medium.com/@alexml0123?source=post_page-----1ce4cc05ca8d--------------------------------)[](https://towardsdatascience.com/?source=post_page-----1ce4cc05ca8d--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----1ce4cc05ca8d--------------------------------)
    [Alexey Kravets](https://medium.com/@alexml0123?source=post_page-----1ce4cc05ca8d--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@alexml0123?source=post_page-----1ce4cc05ca8d--------------------------------)[![Alexey
    Kravets](../Images/3b31f9b3c73c6c7ca709f845e6f70023.png)](https://medium.com/@alexml0123?source=post_page-----1ce4cc05ca8d--------------------------------)[](https://towardsdatascience.com/?source=post_page-----1ce4cc05ca8d--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----1ce4cc05ca8d--------------------------------)
    [Alexey Kravets](https://medium.com/@alexml0123?source=post_page-----1ce4cc05ca8d--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----1ce4cc05ca8d--------------------------------)
    ·14 min read·Aug 15, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----1ce4cc05ca8d--------------------------------)
    ·14分钟阅读·2023年8月15日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: Vision Transformer (ViT) stands as a remarkable milestone in the evolution of
    computer vision. ViT challenges the conventional wisdom that images are best processed
    through convolutional layers, proving that sequence-based attention mechanisms
    can effectively capture the intricate patterns, context, and semantics present
    in images. By breaking down images into manageable patches and leveraging self-attention,
    ViT captures both local and global relationships, enabling it to excel in diverse
    vision tasks, from image classification to object detection and beyond. In this
    article, we are going to break down how ViT for classification works under the
    hood.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Vision Transformer (ViT) 是计算机视觉演变中的一个显著里程碑。ViT挑战了图像最好通过卷积层处理的传统观念，证明了基于序列的注意力机制可以有效地捕捉图像中的复杂模式、上下文和语义。通过将图像分解为可管理的补丁并利用自注意力机制，ViT能够捕捉局部和全局关系，使其在图像分类、目标检测等多种视觉任务中表现出色。在本文中，我们将深入剖析ViT在分类任务中的工作原理。
- en: '![](../Images/ac2ea1bf46319ca84c69d7fe2d300fa5.png)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ac2ea1bf46319ca84c69d7fe2d300fa5.png)'
- en: '[https://unsplash.com/photos/aVvZJC0ynBQ](https://unsplash.com/photos/aVvZJC0ynBQ)'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://unsplash.com/photos/aVvZJC0ynBQ](https://unsplash.com/photos/aVvZJC0ynBQ)'
- en: Introduction
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: The core idea of ViT is to treat an image as a sequence of fixed-size patches,
    which are then flattened and converted into 1D vectors. These patches are subsequently
    processed by a transformer encoder, which enables the model to capture global
    context and dependencies across the entire image. By dividing the image into patches,
    ViT effectively reduces the computational complexity of handling large images
    while retaining the ability to model complex spatial interactions.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: ViT的核心思想是将图像视为一系列固定大小的补丁，然后将这些补丁展平并转换为1D向量。这些补丁随后由一个transformer编码器处理，使得模型能够捕捉整个图像的全局上下文和依赖关系。通过将图像分割成补丁，ViT有效地降低了处理大图像的计算复杂性，同时保持了建模复杂空间交互的能力。
- en: 'First of all, we import the ViT model for classification from hugging face
    transformers library:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们从hugging face transformers库中导入ViT分类模型：
- en: '[PRE0]'
  id: totrans-12
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '*patch16–224* indicates that the model accepts images of size 224x224 and each
    patch has width and hight of 16 pixels.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '*patch16–224* 表示模型接受大小为224x224的图像，并且每个补丁的宽度和高度为16像素。'
- en: 'This is what the model architecture looks like:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是模型架构的样子：
- en: '[PRE1]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Embeddings
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 嵌入
- en: Patch Embedding
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 补丁嵌入
- en: Transformation of the image into patches is performed using a Conv2D layer.
    As we know, Conv2D layer does a 2-dimensional convolutional operations on input
    data to learn features and patterns from images. In this case though Conv2D layer
    is used to divide the image into NxN number of patches by using the `stride` parameter.
    Stride determines the step size at which the filter slides over the input data.
    In this case, because our images are 224x224 and the patch is of size 16, meaning
    that there are 224/16 = 14 patches in each dimension, if we choose `stride=16`
    we effectively separate our image in 14 **non-overlapping** patches.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 图像转换为补丁是通过Conv2D层完成的。如我们所知，Conv2D层在输入数据上进行二维卷积操作，以从图像中学习特征和模式。然而，在这种情况下，Conv2D层用于通过使用`stride`参数将图像分成NxN个补丁。步幅决定了滤波器在输入数据上滑动的步长。在这种情况下，因为我们的图像是224x224，补丁大小为16，意味着每个维度有224/16
    = 14个补丁，如果我们选择`stride=16`，我们实际上将图像分为14个**不重叠**的补丁。
- en: 'To be visual and assuming an image of shape 4x4 with and stride of 2:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 为了直观说明，假设图像的形状为4x4，步幅为2：
- en: '![](../Images/b3e1405160e6912bda1d6449d47e99be.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b3e1405160e6912bda1d6449d47e99be.png)'
- en: Patches creation, Image by Author
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 补丁创建，由作者提供的图像
- en: 'So for example, the first & the second patches are going to be :'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，第一个和第二个补丁将是：
- en: '[PRE2]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The pattern is clear — to compute each patch we skip 16 pixels to get non-overlapping
    patches. If we do this operation for the entire image we end up with 1 x 14 x
    14 tensor where each patch is represented by one number computed using the first
    filter of Conv2D. However, there are **768 filters** which means that at the end
    we get a 768 x 14 x 14 dimensional tensor. So now we effectively have for each
    patch a 768 dimensional representation, that is our patch embedding. We also flatten
    and transpose the tensor, thus the embedding shape becomes *[batch_size, 196,
    768]* where the second dimension is flattened 14 x 14 = 196 and we effectively
    have a sequence of 196 patches with embedding size of 768.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 模式很明确——为了计算每个补丁，我们跳过16个像素以获取不重叠的补丁。如果我们对整个图像执行此操作，我们将得到一个1 x 14 x 14的张量，其中每个补丁由使用Conv2D的第一个滤波器计算得出。然而，有**768个滤波器**，这意味着最后我们得到一个768
    x 14 x 14维度的张量。所以现在我们实际上为每个补丁得到一个768维的表示，这就是我们的补丁嵌入。我们还对张量进行展平和转置，因此嵌入形状变为 *[batch_size,
    196, 768]*，其中第二维展平为14 x 14 = 196，我们实际上拥有一个196个补丁的序列，每个补丁的嵌入大小为768。
- en: '[PRE3]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'If we want to reproduce the layer entirely from scratch, this is the code:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想从头完全重现这一层，这是代码：
- en: '[PRE4]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Now, if you are familiar with the Language Transformer (check it out [here](/deep-dive-into-the-code-of-bert-model-9f618472353e)
    if needed) you should recall the [CLS] token, whose representation serves as a
    condensed and informative summary of the entire text, enabling the model to make
    accurate predictions based on the extracted features from the transformer encoder.
    Also in ViT we have the [CLS] token that has the same function as for text, and
    it’s appended to the representation computed above.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，如果你对语言转换器 (如有需要请查看[这里](/deep-dive-into-the-code-of-bert-model-9f618472353e))
    熟悉，你应该记得[CLS]标记，它的表示作为整个文本的浓缩和信息性摘要，使模型能够基于从转换器编码器中提取的特征做出准确的预测。在ViT中，[CLS]标记也具有与文本相同的功能，它被附加到上面计算的表示中。
- en: '[CLS] token is a parameter that we are going to learn using back-propagation:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '[CLS]标记是一个我们将通过反向传播学习的参数：'
- en: '[PRE5]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Positional Embedding
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 位置嵌入
- en: Just like in Language Transformer, to **preserve the positional information
    of the patches**, ViT includes positional embeddings. Positional embeddings help
    the model understand the spatial relationships between different patches, enabling
    it to capture the image’s structure.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 就像在语言转换器中一样，为了**保留补丁的位置信息**，ViT包括位置嵌入。位置嵌入帮助模型理解不同补丁之间的空间关系，使其能够捕捉图像的结构。
- en: Positional embedding is a Tensor of the same shape of the embeddings with [CLS]
    token compute before, i.e., *[batch_size, 197, 768]*
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 位置嵌入是与之前计算的[CLS]标记形状相同的张量，即 *[batch_size, 197, 768]*。
- en: '[PRE6]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Dropout
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Dropout
- en: Patch embedding is followed by a [Dropout](https://arxiv.org/abs/1706.06859)
    layer. In dropout we replace with zero some of the values with certain dropout
    probability. Dropout helps to reduce overfitting as we randomly block signals
    from certain neurons so the network needs to find other paths to reduce the loss
    function, and thus it learns how to generalize better instead of relying on certain
    paths. We can also see dropout as a kind of models ensemble technique as during
    training at each step we randomly deactivate certain neurons ending up with “different”
    networks which we eventually ensemble during the evaluation time.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 补丁嵌入之后是一个 [Dropout](https://arxiv.org/abs/1706.06859) 层。在 dropout 中，我们以一定的 dropout
    概率将一些值替换为零。Dropout 有助于减少过拟合，因为我们随机阻塞某些神经元的信号，使得网络需要找到其他路径来减少损失函数，从而学会更好地泛化，而不是依赖于某些特定路径。我们也可以将
    dropout 看作是一种模型集成技术，因为在训练过程中，每一步我们随机停用某些神经元，最终得到“不同”的网络，这些网络在评估时被集成在一起。
- en: 'At the end of the Embeddings layer we have:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Embeddings 层的末尾，我们有：
- en: '[PRE7]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Encoder
  id: totrans-39
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 编码器
- en: ViT employs a stack of transformer encoder blocks, similar to those used in
    language models such as BERT. Each encoder block consists of multi-head self-attention
    and feed-forward neural networks. The self-attention mechanism enables the model
    to capture relationships between different patches, while the feed-forward neural
    networks perform non-linear transformations.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: ViT 使用一系列的 Transformer 编码器块，类似于语言模型如 BERT 中使用的块。每个编码器块包括多头自注意力机制和前馈神经网络。自注意力机制使模型能够捕捉不同补丁之间的关系，而前馈神经网络则执行非线性变换。
- en: Specifically, each layer is composed of Self-Attention, Intermediate and Output
    modules.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 具体而言，每一层由自注意力、中间和输出模块组成。
- en: '[PRE8]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Self-Attention
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 自注意力
- en: Self-attention is a pivotal mechanism within the Vision Transformer (ViT) model
    that enables it to capture relationships and dependencies between different patches
    in an image. It plays a crucial role in extracting contextual information and
    understanding long and short-range interactions among the patches.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 自注意力是 Vision Transformer (ViT) 模型中的一个关键机制，使其能够捕捉图像中不同补丁之间的关系和依赖性。它在提取上下文信息和理解补丁之间的长短程交互中发挥了重要作用。
- en: 'Each patch is associated with three vectors: Key, Query, and Value. These vectors
    are learned through linear transformations of the original patch embeddings. The
    **Key vector represents information from the current patches**, the **Query vector
    is used to ask questions about other patches**, and the **Value vector holds the
    information that is relevant to other patches**.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 每个补丁关联有三个向量：Key、Query 和 Value。这些向量是通过对原始补丁嵌入进行线性变换学习得到的。**Key 向量代表当前补丁的信息**，**Query
    向量用于询问其他补丁**，而**Value 向量包含与其他补丁相关的信息**。
- en: 'As we have already computed the embeddings in the previous section, we compute
    the Key, Query and Value projecting the embeddings with the Key, Query and Value
    matrices:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们在前一部分已经计算了嵌入，我们通过使用 Key、Query 和 Value 矩阵对嵌入进行投影，来计算 Key、Query 和 Value：
- en: '[PRE9]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Note that we skipped the LayerNorm operation, that we will cover later.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们跳过了 LayerNorm 操作，稍后将介绍。
- en: For each Query vector, attention scores are computed by measuring the compatibility
    or similarity between the Query and Key vectors of all other patches. This is
    done through a dot product operation and then applying the Softmax function to
    get normalized attention scores with the shape *[b_size, 197, 197].* The attention
    matrix is square because all patches attend to each other, and this is why it’s
    called self-attention. These scores indicate how much focus or attention should
    be placed on each patch when processing the query patch. Because new embedding
    for the next layer of each patch is derived based on the attention scores and
    the values of all other patches, we get a **contextual embedding** for each patch
    as its derived based on all other patches in the image.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个 Query 向量，通过测量 Query 向量和所有其他补丁的 Key 向量之间的兼容性或相似性来计算注意力分数。这是通过点积操作完成的，然后应用
    Softmax 函数以获得形状为 *[b_size, 197, 197]* 的归一化注意力分数。注意力矩阵是方形的，因为所有补丁都相互关注，这就是为什么它被称为自注意力。这些分数表明在处理查询补丁时应给予每个补丁多少关注。由于每个补丁的下一层的新嵌入是基于注意力分数和所有其他补丁的值推导出来的，因此我们为每个补丁获得一个**上下文嵌入**，因为它是基于图像中的所有其他补丁推导的。
- en: 'To clarify this further, recall that at the beginning we split the image into
    patches using the Conv2D layer to get a 768-dimensional embedding vector for each
    patch - these embedding are independent as there was no interaction (no overlap)
    between the patches. However, in the transformer layers the patches embeddings
    get mixed becoming a function of the embeddings of other patches. For example,
    the embedding in the first layer is:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进一步澄清这一点，请回忆一下我们最开始用Conv2D层将图像拆分成补丁，以获得每个补丁的768维嵌入向量——这些嵌入是独立的，因为补丁之间没有交互（没有重叠）。然而，在transformer层中，补丁嵌入被混合，成为其他补丁嵌入的函数。例如，第一层的嵌入是：
- en: '[PRE10]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'If we zoom in and look at the first patch:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们放大并查看第一个补丁：
- en: '[PRE11]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: the new embeddings for it (token indexed at 0 is [CLS] token) is a combination
    of embeddings of different patches with most attention on the first patch itself
    (0.73), [CLS] token (0.24) and the remaining on all other patches. But this is
    not always the case. Indeed, in next layers the first patch might pay more attention
    to patches around it instead of the patch itself and [CLS] token or even to patches
    very far away — this depends on what the model thinks is useful to solve a certain
    task.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 对它的新嵌入（索引为0的是[CLS] token）是不同补丁的嵌入的组合，其中对第一个补丁本身（0.73）、[CLS] token（0.24）以及其他所有补丁的关注度最高。但情况并不总是如此。实际上，在后续层中，第一个补丁可能会更多地关注其周围的补丁，而不是自身和[CLS]
    token，甚至可能关注距离很远的补丁——这取决于模型认为对解决某个任务有用的东西。
- en: Also, you might have noticed that I selected only the first 64 columns from
    the weight matrices of query, key and value. These first 64 columns represent
    the **first attention head**, but actually there are 12 of them (in this model
    size). Each of these attention heads creates different representation of patches.
    Indeed, if we look at the third attention head for the first patch we can see
    that the first patch pays most attention (0.26) at the second patch rather than
    to itself like in the first attention head.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，你可能已经注意到，我只选择了查询、键和值权重矩阵中的前64列。这64列代表了**第一个注意力头**，但实际上（在这个模型大小下）有12个注意力头。每个注意力头对补丁的表示都会有所不同。实际上，如果我们查看第一个补丁的第三个注意力头，可以看到第一个补丁对第二个补丁的关注度最高（0.26），而不是像第一个注意力头那样对自己。
- en: '[PRE12]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Thus, different attention heads will capture different types of relations among
    patches helping the model to see things from different prospective.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，不同的注意力头将捕捉到补丁之间不同类型的关系，帮助模型从不同的角度观察事物。
- en: 'To compute all these heads in parallel we do as follows:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 为了并行计算所有这些注意力头，我们进行如下操作：
- en: '[PRE13]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: After applying self-attention we apply another projection layer and Dropout
    — and here we go, we got through the self-attention layer!
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在应用自注意力后，我们应用另一个投影层和Dropout——这样我们就完成了自注意力层的处理！
- en: '[PRE14]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Ops, wait a second, I promised I would explain the *LayerNorm* operation.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 哎，等一下，我答应过要解释*LayerNorm*操作。
- en: Layer Normalization is a normalization technique used to enhance the training
    and performance of deep learning models. It addresses the problem of internal
    covariate shifts — during training, as the weights of the neural network change,
    the distribution of inputs to each layer can change significantly, making it difficult
    for the model to converge. Layer Normalization addresses this by ensuring that
    the inputs to each layer have a consistent mean and variance, stabilizing the
    learning process. It’s implemented by standardizing each patch embedding by its
    mean and standard deviation so that it has zero mean and unit variance. We then
    apply a trained weights and bias so it can be shifted to have a different mean
    and variance for the model to adapt automatically during training. Because we
    compute mean and standard deviation across different examples independently from
    the others, it is different from [Batch Normalization](https://en.wikipedia.org/wiki/Batch_normalization#:~:text=Batch%20normalization%20(also%20known%20as,and%20Christian%20Szegedy%20in%202015.)
    where the normalization is across the batch dimension and thus depends on other
    examples in the batch.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 层归一化是一种用于提升深度学习模型训练和性能的规范化技术。它解决了内部协变量偏移的问题——在训练过程中，由于神经网络的权重变化，每层的输入分布可能会显著改变，从而使模型难以收敛。层归一化通过确保每层的输入具有一致的均值和方差来解决这个问题，从而稳定学习过程。它的实现方法是通过其均值和标准差对每个补丁嵌入进行标准化，使其均值为零，方差为一。然后应用训练的权重和偏差，使其可以被移动到具有不同的均值和方差，以便模型在训练过程中自动适应。由于我们在不同示例之间独立计算均值和标准差，因此它与[批量归一化](https://en.wikipedia.org/wiki/Batch_normalization#:~:text=Batch%20normalization%20(also%20known%20as,and%20Christian%20Szegedy%20in%202015.)不同，后者是在批量维度上进行归一化，因此依赖于批量中的其他示例。
- en: 'Let’s take the first patch embedding:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看第一个补丁嵌入：
- en: '[PRE15]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Intermediate
  id: totrans-66
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 中级
- en: Before the Intermediate class we perform another layer normalization and a **residual
    connection**. By now it should be clear why we want to apply another layer normalization
    — we need to normalize the contextual embeddings coming from the self-attention
    to improve convergence, but what is that other residual thing I mentioned you
    are probably wondering?
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在中级课程之前，我们执行另一个层归一化和一个**残差连接**。到现在为止，应该清楚为什么我们需要应用另一个层归一化——我们需要规范化来自自注意力的上下文嵌入以提高收敛性，但你可能会想，我提到的那个其他的残差东西是什么？
- en: Residual Connection is a critical component in deep neural networks that mitigates
    the challenges of training very deep architectures. As we increase the depth of
    a neural network by stacking more layers we bump into the problem of vanishing/exploding
    gradients, where in case of vanishing gradients the model is not able to learn
    anymore as the propagated gradients are close to zero and initial layers stop
    changing weights and improve (Check [this article](https://medium.com/towards-artificial-intelligence/backpropagation-and-vanishing-gradient-problem-in-rnn-clearly-explained-efce8824971b)
    and [this](https://medium.com/towards-artificial-intelligence/backpropagation-and-vanishing-gradient-problem-in-rnn-part-2-4fa4c0e27b54)
    if you want to learn more about the vanishing gradient). Opposite problem with
    exploding gradients when the weights cannot stabilize because of extreme updates
    which eventually explode (go to infinity). Now, proper initialisation of weights
    and normalization helps to address this problem but what has been observed is
    even if the network becomes more stable, the performance decreases as the optimization
    is harder. Adding these residual connections helps to improve performance and
    the network becomes easier to optimize even if we keep increasing depth.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 残差连接是深度神经网络中的一个关键组件，可以缓解训练非常深的架构的挑战。随着我们通过堆叠更多层来增加神经网络的深度，我们会遇到梯度消失/爆炸的问题，在梯度消失的情况下，模型无法再学习，因为传播的梯度接近零，初始层停止变化权重和改进（如果你想了解更多关于梯度消失的问题，可以查看[这篇文章](https://medium.com/towards-artificial-intelligence/backpropagation-and-vanishing-gradient-problem-in-rnn-clearly-explained-efce8824971b)和[这篇](https://medium.com/towards-artificial-intelligence/backpropagation-and-vanishing-gradient-problem-in-rnn-part-2-4fa4c0e27b54)）。相反，梯度爆炸的问题发生在权重无法因极端更新而稳定，最终爆炸（趋于无穷大）。现在，适当的权重初始化和规范化有助于解决这个问题，但观察到的是，即使网络变得更加稳定，性能也会下降，因为优化变得更加困难。添加这些残差连接有助于提高性能，即使我们继续增加深度，网络也更容易优化。
- en: 'How is it implemented? Simple — we just add the original input to the transformed
    output after some transformations of the original input:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 它是如何实现的？很简单——我们只是将原始输入添加到经过一些变换后的变换输出中。
- en: '[PRE16]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Another key insight is that if the *transformations* of a residual connection
    learn to approximate the identity function, the addition of the input with the
    learned features will not have any effect. In fact, the network can learn to modify
    or refine the features if needed.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个关键见解是，如果残差连接的 *变换* 学会了逼近恒等函数，那么输入与学习到的特征的加法将没有任何效果。实际上，网络可以在需要时学习修改或细化特征。
- en: In our case the residual connection is the sum between the initial *embeddings*
    and the *attention_output* which are *embeddings* after all the transformations
    in the *self-attention* layer.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的案例中，残差连接是初始 *嵌入* 与 *attention_output* 之间的和，这些是经过 *self-attention* 层所有变换后的
    *嵌入*。
- en: '[PRE17]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'In the Intermediate class we perform a linear projection and apply a **non-linearity**:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Intermediate 类中，我们执行线性投影并应用 **非线性**：
- en: '[PRE18]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The non-linearity used in ViT is GeLU activation function. It is defined as
    the cumulative distribution function of the standard normal distribution:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: ViT 中使用的非线性是 GeLU 激活函数。它被定义为标准正态分布的累积分布函数：
- en: '![](../Images/d62753c56370e863409c8afbfc51a50e.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d62753c56370e863409c8afbfc51a50e.png)'
- en: '[https://arxiv.org/pdf/1606.08415v3.pdf](https://arxiv.org/pdf/1606.08415v3.pdf)'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://arxiv.org/pdf/1606.08415v3.pdf](https://arxiv.org/pdf/1606.08415v3.pdf)'
- en: 'It is normally approximated with the following formula for faster calculations:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，它用以下公式进行逼近以加快计算：
- en: '![](../Images/6601dd9a9024fb6c7c9dbe7220f1f659.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6601dd9a9024fb6c7c9dbe7220f1f659.png)'
- en: '[https://arxiv.org/pdf/1606.08415v3.pdf](https://arxiv.org/pdf/1606.08415v3.pdf)'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://arxiv.org/pdf/1606.08415v3.pdf](https://arxiv.org/pdf/1606.08415v3.pdf)'
- en: Looking at the graph below we can see that if *ReLU,* that is given by the formula
    *max(input, 0),* is monotonic, convex and linear in the positive domain, *GeLU*
    is non-monotonic, non-convex and non-linear in the positive domain and thus can
    approximate more easily complicated functions.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 从下面的图表可以看出，*ReLU* 函数，由公式 *max(input, 0)* 给出，在正域内是单调的、凸的和线性的，而 *GeLU* 在正域内是非单调的、非凸的和非线性的，因此可以更容易地逼近复杂的函数。
- en: Additionally, *GeLU* function is smooth — unlike the *ReLU* function, which
    is piecewise linear with a sharp transition at zero, *GeLU* provides a smooth
    transition across all values, making it more amenable to gradient-based optimization
    during training.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，*GeLU* 函数是平滑的——与 *ReLU* 函数不同，后者在零点有一个急剧的过渡，*GeLU* 在所有值之间提供了平滑的过渡，使其在训练过程中更适合基于梯度的优化。
- en: '![](../Images/6ceb845234321b15629db8d2ddc35170.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6ceb845234321b15629db8d2ddc35170.png)'
- en: '[https://arxiv.org/pdf/1606.08415v3.pdf](https://arxiv.org/pdf/1606.08415v3.pdf)'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://arxiv.org/pdf/1606.08415v3.pdf](https://arxiv.org/pdf/1606.08415v3.pdf)'
- en: Output
  id: totrans-86
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 输出
- en: 'The final bit remaining of the Encoder is the Output class. To compute it we
    already have all the elements we need — it is linear projection, Dropout and a
    residual connection:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 编码器的最后一个部分是 Output 类。为了计算它，我们已经拥有了所有需要的元素——它是线性投影、Dropout 和残差连接：
- en: '[PRE19]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Well, we went through the first layer ViT Layer, there are other 11 to go through
    and this is where the hard part comes …
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 好吧，我们已经完成了第一个 ViT 层，还有其他 11 层要处理，这就是困难的部分……
- en: Joking! We are actually done — all the other layers are exactly the same as
    the first, the only difference is that instead of starting from the embeddings
    like in the first layer the embeddings for the next layer are *output_res* we
    computed previously.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 开玩笑！我们实际上完成了——所有其他层与第一层完全相同，唯一的区别是，下一层的嵌入是之前计算的 *output_res*。
- en: 'So the output after 12 layer of the encoder is:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，编码器经过 12 层后的输出是：
- en: '[PRE20]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Pooler
  id: totrans-93
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Pooler
- en: Generally, in a Transformer model Pooler is a component used to aggregate information
    from the sequence of tokens embeddings after the transformer encoder blocks. Its
    role is to generate a fixed-size representation that captures the global context
    and summarizes the information extracted from the image patches, in case of ViT.
    The Pooler is essential for obtaining a compact and context-aware representation
    of the image, which can then be used for various downstream tasks such as image
    classification.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，在 Transformer 模型中，Pooler 是一个用于聚合在 transformer 编码器块之后的序列中的 token 嵌入信息的组件。它的作用是生成一个固定大小的表示，该表示捕获了全局上下文并总结了从图像块中提取的信息，在
    ViT 的情况下尤为重要。Pooler 对于获得紧凑且具有上下文感知的图像表示至关重要，这可以用于各种下游任务，如图像分类。
- en: In this case Pooler is very simple — we take [CLS] token and use it as the compact
    and context-aware representation of the image.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，Pooler非常简单——我们取[CLS]标记并将其用作图像的紧凑且上下文感知的表示。
- en: '[PRE21]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Classifier
  id: totrans-97
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分类器
- en: 'Finally, we are ready to use the the *pooled_output* to classify the image.
    The classifier is a simple linear layer with output dimension equal to the number
    of classes:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们准备使用*pooled_output*来对图像进行分类。分类器是一个简单的线性层，输出维度等于类别数：
- en: '[PRE22]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Conclusions
  id: totrans-100
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: ViT fully revolutionized computer vision replacing Convolutional Neural Networks
    almost in every application, this is why it’s so important to understand how it
    works. Let’s not forget that the transformer architecture, which is the main component
    of ViT, originated in NLP, thus you should check out my previous article on BERT
    Transformer [here](https://medium.com/p/9f618472353e). Hope you enjoyed this read,
    see you next time!
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: ViT彻底革新了计算机视觉，几乎在所有应用中取代了卷积神经网络，这就是为什么理解它的工作原理如此重要。不要忘记，ViT的主要组件——变换器架构，源于自然语言处理，因此你应该查看我之前关于BERT变换器的文章
    [这里](https://medium.com/p/9f618472353e)。希望你喜欢这篇文章，下次见！
- en: '[](https://medium.com/@alexml0123/membership?source=post_page-----1ce4cc05ca8d--------------------------------)
    [## Join Medium with my referral link - Alexey Kravets'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@alexml0123/membership?source=post_page-----1ce4cc05ca8d--------------------------------)
    [## 通过我的推荐链接加入Medium - Alexey Kravets'
- en: As a Medium member, a portion of your membership fee goes to writers you read,
    and you get full access to every story…
  id: totrans-103
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 作为Medium会员，你的会员费用的一部分将分配给你阅读的作者，你可以完全访问每个故事…
- en: medium.com](https://medium.com/@alexml0123/membership?source=post_page-----1ce4cc05ca8d--------------------------------)
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: medium.com](https://medium.com/@alexml0123/membership?source=post_page-----1ce4cc05ca8d--------------------------------)
- en: References
  id: totrans-105
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] [https://github.com/huggingface/transformers](https://github.com/huggingface/transformers)'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] [https://github.com/huggingface/transformers](https://github.com/huggingface/transformers)'
- en: '[2] [[2010.11929] An Image is Worth 16x16 Words: Transformers for Image Recognition
    at Scale (arxiv.org)](https://arxiv.org/abs/2010.11929)'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] [[2010.11929] 一张图胜过16x16个词：用于大规模图像识别的变换器（arxiv.org）](https://arxiv.org/abs/2010.11929)'
