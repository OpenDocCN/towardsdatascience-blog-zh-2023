- en: Data Pipeline with Airflow and AWS Tools (S3, Lambda & Glue)
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据管道与 Airflow 和 AWS 工具（S3、Lambda 和 Glue）
- en: 原文：[https://towardsdatascience.com/data-pipeline-with-airflow-and-aws-tools-s3-lambda-glue-18585d269761?source=collection_archive---------1-----------------------#2023-04-06](https://towardsdatascience.com/data-pipeline-with-airflow-and-aws-tools-s3-lambda-glue-18585d269761?source=collection_archive---------1-----------------------#2023-04-06)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/data-pipeline-with-airflow-and-aws-tools-s3-lambda-glue-18585d269761?source=collection_archive---------1-----------------------#2023-04-06](https://towardsdatascience.com/data-pipeline-with-airflow-and-aws-tools-s3-lambda-glue-18585d269761?source=collection_archive---------1-----------------------#2023-04-06)
- en: Learning a little about these tools and how to integrate them
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 了解这些工具及其集成方式
- en: '[](https://joaopedro214.medium.com/?source=post_page-----18585d269761--------------------------------)[![João
    Pedro](../Images/64a0e14527be213e5fde0a02439fbfa7.png)](https://joaopedro214.medium.com/?source=post_page-----18585d269761--------------------------------)[](https://towardsdatascience.com/?source=post_page-----18585d269761--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----18585d269761--------------------------------)
    [João Pedro](https://joaopedro214.medium.com/?source=post_page-----18585d269761--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://joaopedro214.medium.com/?source=post_page-----18585d269761--------------------------------)[![João
    Pedro](../Images/64a0e14527be213e5fde0a02439fbfa7.png)](https://joaopedro214.medium.com/?source=post_page-----18585d269761--------------------------------)[](https://towardsdatascience.com/?source=post_page-----18585d269761--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----18585d269761--------------------------------)
    [João Pedro](https://joaopedro214.medium.com/?source=post_page-----18585d269761--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fb111eee95c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdata-pipeline-with-airflow-and-aws-tools-s3-lambda-glue-18585d269761&user=Jo%C3%A3o+Pedro&userId=b111eee95c&source=post_page-b111eee95c----18585d269761---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----18585d269761--------------------------------)
    ·17 min read·Apr 6, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F18585d269761&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdata-pipeline-with-airflow-and-aws-tools-s3-lambda-glue-18585d269761&user=Jo%C3%A3o+Pedro&userId=b111eee95c&source=-----18585d269761---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fb111eee95c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdata-pipeline-with-airflow-and-aws-tools-s3-lambda-glue-18585d269761&user=Jo%C3%A3o+Pedro&userId=b111eee95c&source=post_page-b111eee95c----18585d269761---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----18585d269761--------------------------------)
    · 17分钟阅读 · 2023年4月6日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F18585d269761&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdata-pipeline-with-airflow-and-aws-tools-s3-lambda-glue-18585d269761&user=Jo%C3%A3o+Pedro&userId=b111eee95c&source=-----18585d269761---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F18585d269761&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdata-pipeline-with-airflow-and-aws-tools-s3-lambda-glue-18585d269761&source=-----18585d269761---------------------bookmark_footer-----------)![](../Images/259261d786f73ae03003d560d0121b55.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F18585d269761&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdata-pipeline-with-airflow-and-aws-tools-s3-lambda-glue-18585d269761&source=-----18585d269761---------------------bookmark_footer-----------)![](../Images/259261d786f73ae03003d560d0121b55.png)'
- en: Photo by [Nolan Krattinger](https://unsplash.com/fr/@odes?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由[Nolan Krattinger](https://unsplash.com/fr/@odes?utm_source=medium&utm_medium=referral)拍摄，发布在[Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: Introduction
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: 'A few weeks ago, while doing my mental stretch to think about new post ideas,
    I thought: *Well, I need to learn (and talk) more about cloud and these things,
    I’ve practiced a lot on on-premise ambients, using open-source tools, and running
    away from proprietary solutions… But the world is cloud and I don’t think that
    this is gonna change any time soon…*'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 几周前，当我在思考新的文章创意时，我想：*嗯，我需要更多地了解（和讨论）云计算和这些相关内容。我在本地环境中已经练习了很多，使用了开源工具，远离了专有解决方案……但世界是云的，我认为这种情况不会很快改变……*
- en: 'I then wrote a post about creating a [data pipeline with local Spark and GCP](/creating-a-data-pipeline-with-spark-google-cloud-storage-and-big-query-a72ede294f4c),
    my first one using cloud infrastructure. Today’s post follows the same philosophy:
    fitting local and cloud pieces together to build a data pipeline. But, instead
    of GCP, we’ll be using AWS.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我写了一篇关于创建 [使用本地 Spark 和 GCP 的数据管道](/creating-a-data-pipeline-with-spark-google-cloud-storage-and-big-query-a72ede294f4c)
    的文章，这是我第一次使用云基础设施。今天的文章遵循相同的理念：将本地和云端的组件组合在一起，构建数据管道。但这次，我们将使用 AWS，而不是 GCP。
- en: 'AWS is, by far, the most popular cloud computing platform, it has an absurd
    number of products to solve every type of specific problem you imagine. And, when
    it comes to data engineering solutions, it’s no different: They have databases,
    ETL tools, streaming platforms, and so on — a set of tools that makes our life
    easier (as long as you pay for them).'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: AWS 是迄今为止最受欢迎的云计算平台，它拥有大量的产品来解决你想象中的每种特定问题。至于数据工程解决方案，它也不例外：它们有数据库、ETL 工具、流媒体平台等等
    — 一整套工具，让我们的生活更轻松（只要你为它们付费）。
- en: So, join me on this post to develop a full data pipeline from scratch using
    some pieces from the AWS toolset.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，请跟随我在这篇文章中从零开始开发一个完整的数据管道，使用 AWS 工具集中的一些组件。
- en: not sponsored.
  id: totrans-14
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 未赞助。
- en: The tools — TLDR
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 工具 — TLDR
- en: '**Lambda functions** are AWS’s most famous serverless computing solution. ‘Serverless’
    means the application is not attached to a particular server. Instead, whenever
    a request is made, a new computing instance is quickly initiated, the application
    responds, and the instance is terminated. Because of this, these applications
    are meant to be small and stateless.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '**Lambda functions** 是 AWS 最著名的无服务器计算解决方案。“无服务器”意味着应用程序不依附于特定的服务器。相反，每当发出请求时，会快速启动一个新的计算实例，应用程序响应后，该实例将被终止。因此，这些应用程序应该是小型的、无状态的。'
- en: '**Glue** is a simple serverless ETL solution in AWS. Create Python or Spark
    processing jobs using the visual interface, code editor, or Jupyter notebooks.
    Run the jobs on-demand and pay only for the execution time.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '**Glue** 是 AWS 的一种简单的无服务器 ETL 解决方案。使用可视化界面、代码编辑器或 Jupyter notebooks 创建 Python
    或 Spark 处理作业。按需运行作业，只为执行时间付费。'
- en: '**S3** is AWS’ blob storage. The idea is simple: create a bucket and store
    files in it. Read them later using their “path”. Folders are a lie and the objects
    are immutable.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '**S3** 是 AWS 的 blob 存储。这个概念很简单：创建一个存储桶并在其中存储文件。稍后通过它们的“路径”读取这些文件。文件夹是虚假的，对象是不可变的。'
- en: '**Airflow** is a ‘workflow orchestrator’. It’s a tool to develop, organize,
    order, schedule, and monitor tasks using a structure called DAG — Direct Acyclic
    Graph, defined with Python code.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '**Airflow** 是一个“工作流协调器”。它是一个开发、组织、排序、调度和监控任务的工具，使用一种称为 DAG 的结构 — 有向无环图，用 Python
    代码定义。'
- en: The Data
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据
- en: 'To properly explore the functionalities of all these tools, I’ve chosen to
    work with data from the Brazillian ENEM (National Exam of High School, on literal
    translation). This exam occurs yearly and is the main entrance door to most public
    and private Brazilian universities; it evaluates the student in 4 great areas
    of knowledge: Human Sciences; Natural Sciences; Math and Languages (45 questions
    each).'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 为了充分探索这些工具的功能，我选择使用来自巴西 ENEM（国家高中考试）的数据。这个考试每年举行，是大多数巴西公立和私立大学的主要入学门槛；它在四个主要知识领域评估学生：人文科学、自然科学、数学和语言（每个领域
    45 道题目）。
- en: '![](../Images/c792ed23c90d56e4acf68021de8528eb.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c792ed23c90d56e4acf68021de8528eb.png)'
- en: ENEM 2010, Human sciences and its technologies. Image by Author.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: ENEM 2010，人文科学及其技术。图片来源：作者。
- en: Our task will be to extract these questions from the actual exams, which are
    available as PDFs on the MEC (Ministry of Education) website [CC BY-ND 3.0].
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的任务是从实际的考试中提取这些问题，这些考试以 PDF 形式在 MEC（教育部）网站上提供 [CC BY-ND 3.0]。
- en: '![](../Images/dadf3b7f6d9327341e0b796818df1eb9.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/dadf3b7f6d9327341e0b796818df1eb9.png)'
- en: Extract questions from PDF. Image by Author.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 从 PDF 中提取问题。图片来源：作者。
- en: The Implementation
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现过程
- en: After reading one line or two about the available data processing tools in AWS,
    I chose to build a data pipeline with Lambda and Glue as data processing components,
    S3 as storage, and a local Airflow to orchestrate everything.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在阅读了关于 AWS 可用的数据处理工具的一两行内容后，我决定用 Lambda 和 Glue 作为数据处理组件，S3 作为存储，和本地 Airflow
    来协调一切，来构建一个数据管道。
- en: Simple idea, right?
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 简单的想法，对吧？
- en: Well, sort of.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 嗯，可以说是这样。
- en: As you will note through this post, the problem is that there are a lot of configurations,
    authorizations, roles, users, connections, and keys that need to be created to
    make these tools work together nicely.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你在这篇帖子中会注意到的那样，问题在于有很多配置、授权、角色、用户、连接和密钥需要创建，以使这些工具能够顺利协作。
- en: I promise that I’ll try to cover the steps as most as I can, but I’ll need to
    cut off some details to make a shorter post.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 我保证会尽量覆盖大部分步骤，但为了缩短帖子，我需要省略一些细节。
- en: With that said, let’s have a look at each tool’s function, see the figure below.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 说到这里，让我们看看每个工具的功能，见下图。
- en: '![](../Images/10aab5419d146cdb392c5c4987fb3f26.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/10aab5419d146cdb392c5c4987fb3f26.png)'
- en: Proposed pipeline. Image by Author.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 提议的管道。图像由作者提供。
- en: The local Airflow instance will orchestrate everything, downloading the PDFs
    from the MEC website and uploading them to S3\. This process should automatically
    trigger a Lambda Function execution, which will read the PDF, extract its text,
    and save the result in ‘another place’ of S3\. Airflow should then trigger a Glue
    Job that will read these texts, extract the questions, and save the results in
    CSV to S3.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 本地 Airflow 实例将负责协调所有操作，从 MEC 网站下载 PDF 并将其上传到 S3。此过程应该会自动触发 Lambda 函数执行，该函数将读取
    PDF，提取其文本，并将结果保存到 S3 的“另一个地方”。然后，Airflow 应触发一个 Glue 作业，该作业将读取这些文本，提取问题，并将结果以 CSV
    格式保存到 S3。
- en: 'In steps:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 步骤：
- en: (**Airflow**) Download the PDF and upload it to S3
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (**Airflow**) 下载 PDF 并上传到 S3
- en: (**Lambda**) Extract the text from the PDF, writing the result in JSON to S3
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (**Lambda**) 从 PDF 中提取文本，将结果以 JSON 格式写入 S3
- en: (**Airflow->Glue**) Read the text, split the questions, add the proper metadata,
    and save the results in CSV
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (**Airflow->Glue**) 读取文本，拆分问题，添加适当的元数据，并将结果保存为 CSV
- en: 0\. Setting up the environment
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 0. 设置环境
- en: All the code used in this project is available in this [GitHub repository](https://github.com/jaumpedro214/posts).
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 本项目中使用的所有代码都可以在这个 [GitHub 仓库](https://github.com/jaumpedro214/posts)中找到。
- en: The first step is to configure the local environment.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步是配置本地环境。
- en: You will need Docker installed on your local machine to create the Airflow cluster.
    The docker images are already configured to automatically create a new environment
    from scratch, so we can focus more on the implementation.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 你需要在本地机器上安装 Docker 来创建 Airflow 集群。Docker 镜像已经配置好，可以自动从头创建一个新环境，因此我们可以更多地关注实现部分。
- en: 'On the same folder of the **docker-compose.yaml** file, start the environment
    with:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在 **docker-compose.yaml** 文件的相同文件夹中，使用以下命令启动环境：
- en: '[PRE0]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: After the initial configuration, the airflow web service should start at localhost:8080\.
    The default user and password are both ‘airflow’.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在初始配置后，airflow Web 服务应该在 localhost:8080 启动。默认的用户名和密码都是 ‘airflow’。
- en: 'If you run into problems while starting Airflow, try to give read and write
    permissions to the newly created volumes, ex: chmod 777 <foldername>.'
  id: totrans-48
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 如果在启动 Airflow 时遇到问题，请尝试为新创建的卷赋予读写权限，例如：chmod 777 <foldername>。
- en: Moving on to the cloud environment.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来进入云环境。
- en: You’ll need an AWS account, and here is a warning — watch out for the bills.
    The S3 storage and Lambda functions uses will be under the free quota (if you
    have not already spent it), but the Glue executions will charge you a few USD
    cents. **Always remember to shut down everything when the work is finished.**
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 你需要一个 AWS 账户，这里有一个警告——注意账单。S3 存储和 Lambda 函数的使用将会在免费配额范围内（如果你还没有用完），但 Glue 执行会收取一些美元美分。**记得在工作完成后关闭所有服务。**
- en: 'Once you have created the account, follow the steps:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你创建了账户，请按照以下步骤操作：
- en: Create a new Bucket in S3 called **enem-bucket**.
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 S3 中创建一个名为 **enem-bucket** 的新 Bucket。
- en: Create a new **IAM** user with authorization to read and write to S3 and run
    Glue Jobs, store the access key pair generated.
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个新的 **IAM** 用户，授权读取和写入 S3 并运行 Glue 作业，存储生成的访问密钥对。
- en: In the airflow UI (localhost:8080), under the **admin->connections** tab, create
    a new AWS connection, named **AWSConnection**, using the previously created access
    key pair**.**
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 airflow UI（localhost:8080）中，点击 **admin->connections** 标签，创建一个新的 AWS 连接，命名为
    **AWSConnection**，使用之前创建的访问密钥对**。**
- en: '![](../Images/8f865b4bd5b0d5e8e60a39ee0f095c6b.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8f865b4bd5b0d5e8e60a39ee0f095c6b.png)'
- en: Create AWS Connection. Image by Author.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 创建 AWS 连接。图像由作者提供。
- en: Some other minor tweaks may be needed, AWS is a crazy place, but the list above
    should cover the overall process.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 可能还需要一些其他小调整，AWS 是一个疯狂的地方，但上面的列表应该涵盖了整体过程。
- en: Once a man ate an entire airplane. The secret is that the process took 2 years
    and he ate it piece by piece. Take this philosophy with you along this post. The
    next sections will detail the implementation of each pipeline piece and, one by
    one, we will build the full project.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 曾经有一个人吃了一整架飞机。秘密在于这个过程持续了2年，他一块一块地吃掉了它。请将这种哲学带到本文中。接下来的部分将详细介绍每个管道的实现，一步一步地构建完整的项目。
- en: 1\. Uploading files to AWS using Airflow
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1\. 使用Airflow上传文件到AWS
- en: 'First, create a Python file inside the **/dags** folder, I named mine **process_enem_pdf.py**.
    This is the default folder where Airflow searches for dags definitions. Inside
    the script, import the following dependencies:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，在**/dags**文件夹中创建一个Python文件，我将其命名为**process_enem_pdf.py**。这是Airflow默认搜索dags定义的文件夹。在脚本中，导入以下依赖项：
- en: '[PRE1]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: In a real scenario, a web scraping application would search the PDFs’ download
    links on the MEC page but, for simplicity, I collected the links manually (there
    are not so many) and hard-coded them in a dictionary.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在实际场景中，网络抓取应用程序会在MEC页面上搜索PDF的下载链接，但为了简化，我手动收集了这些链接（数量不多）并将它们硬编码在一个字典中。
- en: '[PRE2]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Always be responsible when planning to create a web scraper: check the site’s
    terms of use and the hosted content copyright.'
  id: totrans-64
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 规划创建网络抓取器时要始终负责：检查网站的使用条款和托管内容的版权。
- en: To better simulate the behavior of a scraping application, I’ve also created
    a ‘year’ variable in the Airflow UI (admin->variables). This variable simulates
    the ‘year’ when the scraping script should execute, starting in 2010 and being
    automatically incremented (+1) by the end of the task execution. This way, each
    task run will only process data from one year.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地模拟抓取应用程序的行为，我还在Airflow UI中创建了一个“年”变量（admin->variables）。这个变量模拟了抓取脚本应执行的“年份”，从2010年开始，并在任务执行结束时自动递增（+1）。这样，每次任务运行将仅处理一年的数据。
- en: '![](../Images/759a496d93ab3539112d1e9b4762eada.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/759a496d93ab3539112d1e9b4762eada.png)'
- en: Variables list. Image by Author.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 变量列表。图像由作者提供。
- en: 'Airflow variables and connections are referenced inside the code using their
    ID (name). I usually put their names as constants:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: Airflow变量和连接在代码中通过其ID（名称）引用。我通常将它们的名称作为常量：
- en: '[PRE3]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The most common way of executing Python code inside Airflow DAGs is using the
    PythonOperator, which creates a task based on a Python function.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在Airflow DAGs中执行Python代码最常见的方式是使用PythonOperator，它基于Python函数创建任务。
- en: Because of this, the process of downloading the PDF and uploading it to the
    S3 bucket needs to be wrapped inside a function. See below.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，下载PDF并将其上传到S3桶的过程需要封装在一个函数中。见下文。
- en: '[PRE4]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Now, it’s just a matter of instantiating the DAG object itself:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，只需实例化DAG对象本身：
- en: '[PRE5]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'And writing the tasks:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 书写任务：
- en: '[PRE6]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The DAG will be visible in the Airflow UI, where we can activate it and trigger
    executions:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: DAG将在Airflow UI中可见，我们可以激活它并触发执行：
- en: '![](../Images/08776e16d257a34808249f987312a49f.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/08776e16d257a34808249f987312a49f.png)'
- en: DAGs list. Image by Author.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: DAG列表。图像由作者提供。
- en: And here is (the first) moment of truth, trigger the dag and watch the S3 Bucket.
    If all goes well, the PDFs should appear in the S3 bucket.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 这是（第一次）关键时刻，触发dag并查看S3桶。如果一切顺利，PDF应出现在S3桶中。
- en: '![](../Images/c2563e5c5f86cd50d8dca5ca5cf97ac5.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c2563e5c5f86cd50d8dca5ca5cf97ac5.png)'
- en: S3 Bucket with the PDFs uploaded. Image by Author.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 上传PDF的S3桶。图像由作者提供。
- en: If not (which is likely, as things tend to go wrong in the technology field),
    start debugging the DAG logs and search for misconfiguration.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 如果没有（这很可能，因为技术领域的事情往往会出错），开始调试DAG日志并搜索配置错误。
- en: '![](../Images/45da3fdc5a6aff6b8bc5ada8631b9b5a.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/45da3fdc5a6aff6b8bc5ada8631b9b5a.png)'
- en: Errors in the DAG runs. Image by Author.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: DAG运行中的错误。图像由作者提供。
- en: 2\. Extracting the PDF’s text using Lambda Functions
  id: totrans-86
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2\. 使用Lambda Functions提取PDF文本
- en: 'With the PDFs already being uploaded to S3, it’s time for the next step: extracting
    their texts.'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: PDF文件已经上传到S3，现在是下一步：提取它们的文本。
- en: 'This is a perfect task to implement using AWS Lambda Functions: A stateless,
    small, and quick process.'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 这是使用AWS Lambda Functions实现的完美任务：一个无状态、小型且快速的过程。
- en: Just a recap of how this serverless thing works. In usual ‘server’ applications,
    we buy a particular server (machine), with a proper IP, where our application
    gets installed, and it stays up 24/7 (or something like that) to serve our needs.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 简单回顾一下无服务器技术的工作原理。在常规“服务器”应用程序中，我们购买一个特定的服务器（机器），具有合适的IP地址，将我们的应用程序安装在其中，并保持24/7运行（或类似的状态）以满足我们的需求。
- en: The problem with using this approach to things like this simple text-extraction
    preprocessing task is that we need to build a full robust server from scratch,
    which takes time and may not be so cost-effective in the long run. Serverless
    technology has arrived to solve this.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这种方法来处理像这样简单的文本提取预处理任务的问题在于，我们需要从头构建一个完整的健壮服务器，这需要时间，并且从长远来看可能不够成本效益。无服务器技术的到来是为了解决这个问题。
- en: In serverless, whenever a request is made, a new small server instance is quickly
    initiated, the application responds, and the instance is terminated.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 在无服务器环境中，每当发出请求时，都会快速启动一个新的小型服务器实例，应用程序响应后，该实例会被终止。
- en: It’s like renting a car *vs* taking a Uber to make a small 5 min trip.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 就像租车*vs*叫Uber去进行一次小的5分钟行程。
- en: Let’s get back to coding.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回到编码。
- en: Search for Lambda in your AWS account and create a new lambda function **in
    the same region as the S3 bucket used earlier**, otherwise, it will not be able
    to interact with it usingtriggers (more on that later).
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在你的AWS账户中搜索Lambda，并创建一个新的lambda函数，**与之前使用的S3桶在同一区域**，否则它将无法使用触发器与之互动（更多细节稍后说明）。
- en: '![](../Images/1e4279c2f8e65d464c67eafceba829e0.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1e4279c2f8e65d464c67eafceba829e0.png)'
- en: Search for AWS Lambda. Image by Author.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 搜索AWS Lambda。图片由作者提供。
- en: Create a new function from scratch, name it **process-enem-pdf**, choose Python
    3.9 runtime and we’re good to go. AWS will probably instruct you on creating a
    new IAM role for Lambda Functions, make sure that this role has the read and write
    permissions in the **enem-bucket** S3 bucket.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 从头创建一个新函数，将其命名为**process-enem-pdf**，选择Python 3.9运行时，就可以开始了。AWS可能会指导你创建一个新的IAM角色用于Lambda函数，确保这个角色在**enem-bucket**
    S3桶中拥有读写权限。
- en: You may also need to increase the function max execution time to around 3min,
    the default value is 3 seconds (or something close to it), which is insufficient
    for our purposes.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能还需要将函数的最大执行时间增加到大约3分钟，默认值是3秒（或接近的值），这对于我们的目的来说是不够的。
- en: Python Lambda functions in AWS take the form of a simple Python file called
    **lambda_function.py** with a function **lambda_handler(event, context)** inside,
    where ‘event’ is a JSON object representing the event that triggered the execution.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: AWS中的Python Lambda函数呈现为一个名为**lambda_function.py**的简单Python文件，其中包含一个**lambda_handler(event,
    context)**函数，其中‘event’是一个JSON对象，表示触发执行的事件。
- en: You can edit the Python file directly on the AWS built-in IDE or upload a local
    file using a compressed zip folder.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以直接在AWS内置IDE中编辑Python文件，或使用压缩的zip文件上传本地文件。
- en: '![](../Images/675520c750dfaa7fae0ff4bbd020c15e.png)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/675520c750dfaa7fae0ff4bbd020c15e.png)'
- en: Example code in the Lambda Functions code editor. Image by Author.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: Lambda函数代码编辑器中的示例代码。图片由作者提供。
- en: And here things get a bit tricky.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 这时事情变得有点棘手。
- en: To extract the text from the PDFs, we’re going to use the PyPDF2 package. However,
    installing this dependency in the AWS Lambda function environment is not as easy
    as running ‘pip install’.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 要从PDF中提取文本，我们将使用PyPDF2包。然而，在AWS Lambda函数环境中安装这个依赖项并不像运行‘pip install’那么简单。
- en: We need to install the packages locally and send the code + dependencies **compressed
    (zip)** together.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要本地安装这些包，并将代码和依赖项**压缩（zip）**一起发送。
- en: 'To do this, make the following procedure:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 为此，按照以下步骤操作：
- en: Create a Python virtual env with *venv:* **python3 -m venv pdfextractor**
  id: totrans-107
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个Python虚拟环境，使用*venv:* **python3 -m venv pdfextractor**
- en: Activate the environment and install the dependencies
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 激活环境并安装依赖项。
- en: '[PRE7]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 3\. Create a local **lambda_function.py** file with the **lambda_handler** function
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个本地**lambda_function.py**文件，并包含**lambda_handler**函数。
- en: '[PRE8]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 4\. Copy the **lambda_function.py** to the **pdfextractor/lib/python3/site-packages/**
    path.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 将**lambda_function.py**复制到**pdfextractor/lib/python3/site-packages/**路径下。
- en: 5\. Compress the contents of the **pdfextractor/lib/python3/site-packages/**
    folder in a .zip file
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 将**pdfextractor/lib/python3/site-packages/**文件夹的内容压缩成一个.zip文件。
- en: 6\. Upload this file in the Lambda Functions UI
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 将这个文件上传到Lambda函数UI中。
- en: Now that you (probably) understand this process, we can move on to developing
    the code itself.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你（可能）已经了解了这个过程，我们可以继续开发代码本身。
- en: 'The idea is simple: every time a new PDF object is added to the S3 bucket,
    the Lambda function should be triggered, extract its text, and write the result
    to S3.'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 想法很简单：每当向S3桶添加一个新的PDF对象时，Lambda函数应该被触发，提取其文本，并将结果写入S3。
- en: Luckily, we don’t need to code this trigger rule *by hand*, because AWS provides
    built-in triggers that interact with different parts of its infrastructure. In
    the **process-enem-pdf** page**,** click on *add trigger*.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，我们不需要*手动*编写这个触发规则，因为 AWS 提供了与其基础设施不同部分交互的内置触发器。在**process-enem-pdf**页面，点击*添加触发器*。
- en: '![](../Images/1b52048230b0a40f6c6663ec67ad3d29.png)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1b52048230b0a40f6c6663ec67ad3d29.png)'
- en: Adding a Trigger. Image by Author.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 添加触发器。作者提供的图片。
- en: Now, configure a new rule based on S3 …
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，基于 S3 配置一个新规则……
- en: '![](../Images/641c69c50af8ac98bedbaebdfea39c75.png)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/641c69c50af8ac98bedbaebdfea39c75.png)'
- en: Configuring the trigger. Image by Author.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 配置触发器。作者提供的图片。
- en: '**Bucket**: enem-bucket; **Event types:** All object create events; **Suffix**:
    .pdf'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '**桶**：enem-bucket；**事件类型**：所有对象创建事件；**后缀**：.pdf'
- en: '**IT IS VERY IMPORTANT TO PROPERLY ADD THE SUFFIX.** We’re going to use this
    function to write new files to this same bucket, if the suffix filter is not correctly
    configured, **it may cause an infinite recursive loop that will cost you an infinite
    amount of money**.'
  id: totrans-124
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**正确添加后缀是非常重要的。** 我们将使用这个功能将新文件写入相同的桶中，如果后缀过滤器配置不正确，**可能会导致无限递归循环，从而消耗无限的资金**。'
- en: 'Now, every time a new object is created in the S3 bucket, it will trigger a
    new execution. The parameter *event* will store a JSON describing this newly created
    object, which looks something like that:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，每当 S3 桶中创建一个新对象时，它将触发一次新的执行。参数*event*将存储一个 JSON，描述这个新创建的对象，其格式大致如下：
- en: '[PRE9]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: With this information, the function can read the PDF from S3, extract its text,
    and save the results. See the code below.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 利用这些信息，函数可以从 S3 读取 PDF，提取其文本，并保存结果。请参见下面的代码。
- en: '[PRE10]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Create this function and reproduce the deploy steps explained earlier (venv,
    zip and upload) and everything should work fine (probably). As soon as our airflow
    pipeline saves a new PDF to the bucket, its text should be extracted and saved
    as a JSON in the **/content** “folder” (remember, folders are a lie).
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 创建这个功能并重复之前解释的部署步骤（venv、zip 和上传），一切应该运行良好（可能）。一旦我们的 airflow 管道将新的 PDF 保存到桶中，其文本应该被提取并作为
    JSON 保存到**/content** “文件夹”（记住，文件夹是虚假的）。
- en: '![](../Images/236b8f59e1f175ee26bf6ffcc569986c.png)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/236b8f59e1f175ee26bf6ffcc569986c.png)'
- en: JSON with the extracted texts. Image by Author.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 提取的文本 JSON。作者提供的图片。
- en: 3\. Processing the text using Glue
  id: totrans-132
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3\. 使用 Glue 处理文本
- en: And we finally get to the final piece of our pipeline. The texts are already
    extracted and stored in a format that can be easily handled (JSON) by most of
    the data processing engines.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们完成了管道的最后一部分。文本已经被提取并以大多数数据处理引擎可以轻松处理的格式（JSON）存储。
- en: The final task is to process these texts to isolate the individual questions,
    and that’s where AWS Glue comes in.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 最后的任务是处理这些文本以孤立出单个问题，这就是 AWS Glue 的作用。
- en: 'Glue is a pair of solutions: a data catalog, with crawlers to find and catalog
    data and map schemas, and the serverless ETL engine, responsible for the data
    processing.'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: Glue 是一对解决方案：一个数据目录，带有爬虫来查找和编目数据以及映射模式，还有无服务器 ETL 引擎，负责数据处理。
- en: Search for Glue in the AWS console and select it.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 在 AWS 控制台中搜索 Glue 并选择它。
- en: '![](../Images/52eb3505035d1d156299995c0c2d8671.png)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/52eb3505035d1d156299995c0c2d8671.png)'
- en: Search for Glue. Image by Author.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 搜索 Glue。作者提供的图片。
- en: Before writing a job, we’re going to create a new **dataset** in the **data
    catalog** using **crawlers.** Too many new concepts, I know, but the process is
    simple.On Glue’s main page, go to crawlers on the left menu.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 在编写作业之前，我们将使用**爬虫**在**数据目录**中创建一个新的**数据集**。我知道新概念太多了，但过程很简单。在 Glue 的主页面，转到左侧菜单中的爬虫。
- en: '![](../Images/346fb29e1cb7cb6c0c993f0301de5ebf.png)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/346fb29e1cb7cb6c0c993f0301de5ebf.png)'
- en: AWS Glue sidebar. Image by Author.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: AWS Glue 侧边栏。作者提供的图片。
- en: Create a new crawler, give it a name in step 1, and move to step 2\. Here, add
    a new **data source** pointing to the **s3://enem-bucket/content**, our ‘folder’
    where all the texts are stored.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个新的爬虫，在步骤 1 中给它命名，然后进入步骤 2。在这里，添加一个新的**数据源**，指向**s3://enem-bucket/content**，这是我们存储所有文本的“文件夹”。
- en: '![](../Images/9acce3d0c38dccdf931ea46c7955a5b0.png)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9acce3d0c38dccdf931ea46c7955a5b0.png)'
- en: Configuring the crawler. Image by Author.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 配置爬虫。作者提供的图片。
- en: Move to step 3, creating a new **IAM** role if needed. Step 4 will ask you for
    a database, click on **Add database** and create a new one called **enem_pdf_project**.
    Review the info on step 5 and save the crawler.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 进入步骤 3，如果需要，创建一个新的**IAM**角色。步骤 4 将要求你选择一个数据库，点击**添加数据库**并创建一个名为**enem_pdf_project**的新数据库。在步骤
    5 中查看信息并保存爬虫。
- en: You will be redirected to the crawler page. Now the **danger** zone (it will
    cost you a few cents ;-;), click on **run crawler** and it will start to map the
    data in the specified sources (**s3://enem-bucket/content)**.A few seconds later,
    the process finishes and, if everything goes well, a new table called **content**
    should appear in the **enem_pdf_project** database**.**
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 你将被重定向到爬虫页面。现在进入**危险**区域（这会花费你几分钱 ;-;），点击**运行爬虫**，它将开始在指定的源（**s3://enem-bucket/content**）中映射数据。几秒钟后，处理完成，如果一切顺利，**enem_pdf_project**
    数据库中应该会出现一个名为**content**的新表。
- en: Now, the Glue job will be able to read the S3 JSON files referencing this table
    from the catalog.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，Glue 作业将能够读取引用目录中此表的 S3 JSON 文件。
- en: I think this is actually not *needed*, as you can query S3 directly, but the
    lesson stays.
  id: totrans-148
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 我认为这实际上是*不必要的*，因为你可以直接查询 S3，但这个教训仍然适用。
- en: Now, we’re ready to code our job.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们准备好编码我们的作业了。
- en: 'Inside the Jobs task, you can choose many ways to develop a new job: visually
    connecting blocks, using interactive pyspark notebooks sessions, or coding directly
    on a script editor.'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Jobs 任务中，你可以选择多种方式来开发新作业：可视化连接块，使用交互式 pyspark 笔记本会话，或直接在脚本编辑器中编写代码。
- en: '![](../Images/a822b66824ba676c3fd98bca721458f2.png)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a822b66824ba676c3fd98bca721458f2.png)'
- en: Glue jobs interface. Image by Author.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: Glue 作业界面。图片来源：作者。
- en: I suggest that you explore the options yourself (watch out for the notebook
    sessions, you pay for them). Regardless of your choice, name the created job **Spark_EnemExtractQuestionsJSON**.I
    choose to use Spark because I’m more familiar with it. See the code below.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 我建议你自己探索这些选项（注意笔记本会话，你需要为它们付费）。无论你选择什么，都将创建的作业命名为**Spark_EnemExtractQuestionsJSON**。我选择使用
    Spark，因为我对它更熟悉。见下面的代码。
- en: '[PRE11]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Besides some extra bits of code needed to interact with the AWS infrastructure
    (on the readings and writings), all the processing logic is written using standard
    pyspark operations. If you are interested in understanding a little more about
    Spark, check out [one of my previous posts](https://medium.com/towards-data-science/a-fast-look-at-spark-structured-streaming-kafka-f0ff64107325).
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 除了与 AWS 基础设施（读取和写入）交互所需的一些额外代码外，所有处理逻辑都是使用标准 pyspark 操作编写的。如果你对了解更多关于 Spark
    的内容感兴趣，可以查看[我之前的一篇文章](https://medium.com/towards-data-science/a-fast-look-at-spark-structured-streaming-kafka-f0ff64107325)。
- en: By default, Glue jobs are configured to run on-demand, which means that we have
    to trigger its execution manually, using the AWS interface, or through API calls.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，Glue 作业配置为按需运行，这意味着我们必须手动触发其执行，使用 AWS 界面或通过 API 调用。
- en: So, we only need a new task in the Airflow DAG to trigger the job and finish
    the pipeline.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们只需要在 Airflow DAG 中添加一个新任务来触发作业并完成管道。
- en: Luckily, the code needed to do this is very simple, so let’s go back to the
    **process_enem_pdf.py** file and create a new function
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，所需的代码非常简单，所以让我们回到**process_enem_pdf.py** 文件中创建一个新函数。
- en: '[PRE12]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: And add this function as a task in the DAG …
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 并将此函数作为任务添加到 DAG 中…
- en: '[PRE13]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: And, *voilá*, the pipeline is finished.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 而且，*瞧*，管道完成了。
- en: '![](../Images/8fc72b537811e018da9c429a31cf93fd.png)'
  id: totrans-163
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8fc72b537811e018da9c429a31cf93fd.png)'
- en: DAG’s Graph representation. Image by Author.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: DAG 的图形表示。图片来源：作者。
- en: Now, on every run, the pipeline should download the newest PDFs available, and
    upload them to S3, which triggers a Lambda Function that extracts their texts
    and saves them in the **/content** path. This path was mapped by a crawler and
    is available in the data catalog. When the pipeline triggers the Glue job, it
    reads these texts, extracts each question, and saves the results as CSV in the
    **/processed** path.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，每次运行时，管道应该会下载最新的 PDF 文件，并将它们上传到 S3，这会触发一个 Lambda 函数，该函数提取文本并将其保存到**/content**
    路径。这个路径是由爬虫映射的，并在数据目录中可用。当管道触发 Glue 作业时，它读取这些文本，提取每个问题，并将结果保存为 CSV 文件在**/processed**
    路径中。
- en: '![](../Images/879f3029d9b1232197781f3cfbadf91e.png)'
  id: totrans-166
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/879f3029d9b1232197781f3cfbadf91e.png)'
- en: ‘processed’ path in S3\. Image by Author.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: ‘processed’ 路径在 S3 中。图片来源：作者。
- en: See the results below…
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 请查看下面的结果…
- en: '![](../Images/bbf62ecd97c36ec247b62af02ca76b06.png)'
  id: totrans-169
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bbf62ecd97c36ec247b62af02ca76b06.png)'
- en: CSV files created in S3\. Image by Author.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 在 S3 中创建的 CSV 文件。图片来源：作者。
- en: Conclusion
  id: totrans-171
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: '*This was a long adventure.*'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: '*这是一场漫长的冒险。*'
- en: In this post, we built an entire data pipeline from scratch mixing the power
    of various famous data-related tools, both from the AWS cloud (Lambda, Glue, and
    S3) and the local environment (Airflow+Docker).
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我们从头开始构建了一个完整的数据管道，混合了多种著名的数据工具的强大功能，包括 AWS 云（Lambda、Glue 和 S3）和本地环境（Airflow+Docker）。
- en: We explored the functionalities of Lambda and Glue in the context of data processing,
    discussing their advantages and use cases. We also learned a little about Airflow,
    the most famous orchestration tool for data pipelines.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 我们探讨了Lambda和Glue在数据处理中的功能，讨论了它们的优点和使用案例。我们还学到了一些关于Airflow的知识，Airflow是数据管道中最著名的编排工具。
- en: Each one of these tools is a world by itself. I tried to compress all the information
    learned during the project’s development period into the smallest post possible
    so, unavoidably, some information was lost. Let me know in the comments if you
    have problems or doubts.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 这些工具每一个都是一个独立的世界。我试图将项目开发期间学到的所有信息压缩成尽可能小的文章，因此不可避免地，一些信息被遗漏了。如果你遇到问题或有疑问，请在评论中告诉我。
- en: I know that the data pipeline proposed is probably not *optimal*, especially
    in terms of cost *vs* efficiency, but the main point of this post (for me, and
    I expect that it worked this way for you as well) is to learn the overall process
    of developing data products with the tools addressed.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 我知道所提议的数据管道可能不是*最优*的，尤其是在成本*与*效率方面，但我认为这篇文章的主要观点（对我来说，我希望对你也是如此）是学习使用所涉及工具开发数据产品的整体过程。
- en: Also, the majority of the data available today, especially on the internet,
    is in the so-called unstructured format, such as PDFs, videos, images, and so
    on. Processing this kind of data is a crucial skill that involves knowing a broader
    set of tools that go beyond the usual Pandas/Spark/SQL group. The pipeline we
    built today addresses exactly this kind of problem by transforming raw PDFs stored
    on a website into semi-structured CSV files stored in our cloud infrastructure.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，如今大多数可用数据，特别是在互联网上，都是所谓的非结构化格式，如PDF、视频、图像等。处理这种数据是一项关键技能，涉及到知道超出常见的Pandas/Spark/SQL工具集的更广泛工具。我们今天构建的管道正是解决这一问题，通过将存储在网站上的原始PDF转化为存储在我们云基础设施中的半结构化CSV文件。
- en: For me, a *highlight* of this pipeline was the text extraction step deployed
    with AWS Lambda, because this task would probably be impossible or very difficult
    (as far as I know) to implement using only Spark.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 对我来说，这个管道的一个*亮点*是使用AWS Lambda部署的文本提取步骤，因为仅仅依靠Spark实施这一任务可能是不可能或非常困难的（据我所知）。
- en: 'And this is what I hope you took away from this post: Developing a good data
    infrastructure requires not only some theoretical knowledge of data architectures,
    data modeling, or streaming but also a good understanding of the available tools
    that can help materialize your vision.'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 我希望你从这篇文章中得到的主要信息是：构建良好的数据基础设施不仅需要对数据架构、数据建模或流媒体的理论知识，还需要对可以帮助实现你愿景的可用工具有良好的理解。
- en: As always, I am not an expert in any of the subjects discussed, and I strongly
    recommend further reading and discussion (see some references below).
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 和往常一样，我并不是所讨论主题的专家，我强烈推荐进一步阅读和讨论（请参见下面的一些参考资料）。
- en: '*It cost me 36 cents + taxes ;-;*'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: '*这让我花费了36美分 + 税费 ;-;*'
- en: Thank you for reading! ;)
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 感谢你的阅读！ ;)
- en: References
  id: totrans-183
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: All the code is available in [this GitHub repository](https://github.com/jaumpedro214/posts).
  id: totrans-184
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 所有代码均可在[这个GitHub仓库](https://github.com/jaumpedro214/posts)中找到。
- en: Data used —[ENEM PDFs](https://www.gov.br/inep/pt-br/areas-de-atuacao/avaliacao-e-exames-educacionais/enem/provas-e-gabaritos),
    [CC BY-ND 3.0], MEC-Brazilian Gov.
  id: totrans-185
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 使用的数据 — [ENEM PDF](https://www.gov.br/inep/pt-br/areas-de-atuacao/avaliacao-e-exames-educacionais/enem/provas-e-gabaritos)，[CC
    BY-ND 3.0]，巴西教育部。
- en: '[1] Amazon Web Services Latin America. (2021, December 6). *Transforme e catalogue
    dados com o AWS Glue Parte 1 — Português* [Video]. YouTube. [Link](https://www.youtube.com/watch?v=HFFiAy2J2OQ).'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] 亚马逊网络服务拉丁美洲. (2021年12月6日). *使用AWS Glue转化和目录化数据第1部分 — 葡萄牙语* [视频]. YouTube.
    [链接](https://www.youtube.com/watch?v=HFFiAy2J2OQ)。'
- en: '[2] Bakshi, U. (2023, February 9). *How to Upload File to S3 using Python AWS
    Lambda — Geek Culture — Medium*. Medium. [Link](https://medium.com/geekculture/how-to-upload-file-to-s3-using-python-aws-lambda-9aa03bb2c752).'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] Bakshi, U. (2023年2月9日). *如何使用Python AWS Lambda将文件上传到S3 — 极客文化 — Medium*.
    Medium. [链接](https://medium.com/geekculture/how-to-upload-file-to-s3-using-python-aws-lambda-9aa03bb2c752)。'
- en: '[3] Cairocoders. (2020, March 5). *How to Import Custom Python Packages on
    AWS Lambda Function* [Video]. YouTube. [Link](https://www.youtube.com/watch?v=yyBSeGkuPqk).'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] Cairocoders. (2020年3月5日). *如何在AWS Lambda函数中导入自定义Python包* [视频]. YouTube.
    [链接](https://www.youtube.com/watch?v=yyBSeGkuPqk)。'
- en: '[4] *How to extract, transform, and load data for analytic processing using
    AWS Glue (Part 2) | Amazon Web Services*. (2022, April 4). Amazon Web Services.
    [Link](https://aws.amazon.com/pt/blogs/database/how-to-extract-transform-and-load-data-for-analytic-processing-using-aws-glue-part-2/).'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] *如何使用 AWS Glue 提取、转换和加载数据以进行分析处理（第 2 部分） | 亚马逊网络服务*。（2022年4月4日）。亚马逊网络服务。
    [Link](https://aws.amazon.com/pt/blogs/database/how-to-extract-transform-and-load-data-for-analytic-processing-using-aws-glue-part-2/).'
- en: '[5] *How to write a file or data to an S3 object using boto3*. (n.d.). Stack
    Overflow. [Link](https://stackoverflow.com/questions/40336918/how-to-write-a-file-or-data-to-an-s3-object-using-boto3).'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: '[5] *如何使用 boto3 向 S3 对象写入文件或数据*。（无日期）。Stack Overflow。 [Link](https://stackoverflow.com/questions/40336918/how-to-write-a-file-or-data-to-an-s3-object-using-boto3).'
- en: '[6] *Tutorial: Using an Amazon S3 trigger to invoke a Lambda function — AWS
    Lambda*. (n.d.). [Link](https://docs.aws.amazon.com/lambda/latest/dg/with-s3-example.html).'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: '[6] *教程：使用 Amazon S3 触发器调用 Lambda 函数 — AWS Lambda*。（无日期）。 [Link](https://docs.aws.amazon.com/lambda/latest/dg/with-s3-example.html).'
- en: '[7] Um Inventor Qualquer. (2022, January 10). *Aprenda AWS Lambda neste Curso
    Prático GRATUITO! | Aula 17 — #70* [Video]. YouTube. [Link](https://www.youtube.com/watch?v=RCK9fBwrZeY).'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: '[7] Um Inventor Qualquer。（2022年1月10日）。*在这个免费的实用课程中学习 AWS Lambda！| 课程 17 — #70*
    [视频]。YouTube。 [Link](https://www.youtube.com/watch?v=RCK9fBwrZeY).'
- en: '[8] Chambers, B., & Zaharia, M. (2018). Spark: The definitive guide: Big data
    processing made simple. “ O’Reilly Media, Inc.”.'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: '[8] Chambers, B., & Zaharia, M.（2018年）。*Spark: 权威指南：大数据处理变得简单*。 “O’Reilly Media,
    Inc.”。'
