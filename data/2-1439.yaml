- en: Load Testing Simplified With SageMaker Inference Recommender
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 SageMaker 推理推荐器简化负载测试
- en: 原文：[https://towardsdatascience.com/load-testing-simplified-with-sagemaker-inference-recommender-b96746b69292](https://towardsdatascience.com/load-testing-simplified-with-sagemaker-inference-recommender-b96746b69292)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/load-testing-simplified-with-sagemaker-inference-recommender-b96746b69292](https://towardsdatascience.com/load-testing-simplified-with-sagemaker-inference-recommender-b96746b69292)
- en: Test TensorFlow ResNet50 on SageMaker Real-Time Endpoints
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在 SageMaker 实时终端上测试 TensorFlow ResNet50
- en: '[](https://ram-vegiraju.medium.com/?source=post_page-----b96746b69292--------------------------------)[![Ram
    Vegiraju](../Images/07d9334e905f710d9f3c6187cf69a1a5.png)](https://ram-vegiraju.medium.com/?source=post_page-----b96746b69292--------------------------------)[](https://towardsdatascience.com/?source=post_page-----b96746b69292--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----b96746b69292--------------------------------)
    [Ram Vegiraju](https://ram-vegiraju.medium.com/?source=post_page-----b96746b69292--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://ram-vegiraju.medium.com/?source=post_page-----b96746b69292--------------------------------)[![Ram
    Vegiraju](../Images/07d9334e905f710d9f3c6187cf69a1a5.png)](https://ram-vegiraju.medium.com/?source=post_page-----b96746b69292--------------------------------)[](https://towardsdatascience.com/?source=post_page-----b96746b69292--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----b96746b69292--------------------------------)
    [Ram Vegiraju](https://ram-vegiraju.medium.com/?source=post_page-----b96746b69292--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----b96746b69292--------------------------------)
    ·7 min read·Mar 7, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----b96746b69292--------------------------------)
    ·阅读时间 7 分钟·2023年3月7日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/df9ae88a89ce233ff5f795f837fb53bf.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/df9ae88a89ce233ff5f795f837fb53bf.png)'
- en: Image from [Unsplash](https://unsplash.com/photos/o4WaeT3XhV4) by [**Amokrane
    Ait-Kaci**](https://unsplash.com/@amokraneaitk)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源 [Unsplash](https://unsplash.com/photos/o4WaeT3XhV4)，作者 [**Amokrane Ait-Kaci**](https://unsplash.com/@amokraneaitk)
- en: In the past I’ve written extensively about the importance of [load testing](/why-load-testing-is-essential-to-take-your-ml-app-to-production-faab0df1c4e1)
    your Machine Learning models before deploying them into production. When it comes
    to real-time inference use-cases in specific it’s essential to ensure your solution
    meets your target latency and throughput. We’ve also explored how we can use the
    Python library, [Locust](https://locust.io/) to define scripts that can simulate
    our expected traffic patterns.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 过去我曾广泛讨论过在将机器学习模型部署到生产环境之前进行 [负载测试](/why-load-testing-is-essential-to-take-your-ml-app-to-production-faab0df1c4e1)
    的重要性。对于特定的实时推理使用案例，确保你的解决方案满足目标延迟和吞吐量是至关重要的。我们还探讨了如何使用 Python 库 [Locust](https://locust.io/)
    来定义可以模拟预期流量模式的脚本。
- en: While Locust is an incredibly powerful tool, it can be difficult to setup and
    also requires a lot of iterations across the different hyperparameters and hardware
    you may be testing to identify the proper configuration for production. For [SageMaker
    Real-Time Inference](https://docs.aws.amazon.com/sagemaker/latest/dg/realtime-endpoints.html),
    a key tool to take a look at is [SageMaker Inference Recommender](https://docs.aws.amazon.com/sagemaker/latest/dg/inference-recommender.html).
    Rather than having to repeatedly run a Locust script across different configurations,
    you can essentially pass in an array of EC2 instance types to test your endpoint,
    as well as hyperparameters for your specific model container for more advanced
    deployments. In today’s blog we’ll take a look at how we can configure this feature
    and how we can simplify load testing SageMaker Real-Time endpoints.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然 Locust 是一个功能强大的工具，但它的设置可能会很复杂，并且需要针对不同的超参数和硬件进行大量的迭代，以确定生产环境中的适当配置。对于 [SageMaker
    实时推理](https://docs.aws.amazon.com/sagemaker/latest/dg/realtime-endpoints.html)，一个关键工具是
    [SageMaker 推理推荐器](https://docs.aws.amazon.com/sagemaker/latest/dg/inference-recommender.html)。你可以通过传递一个
    EC2 实例类型数组来测试你的终端，以及针对你的特定模型容器的超参数，以便进行更高级的部署，而不是重复运行 Locust 脚本。在今天的博客中，我们将探讨如何配置这一功能以及如何简化
    SageMaker 实时终端的负载测试。
- en: '**NOTE**: This article will assume basic knowledge of AWS, SageMaker, and Python.
    To understand what SageMaker Real-Time Inference is please take a look at the
    following starter [blog](https://aws.amazon.com/blogs/machine-learning/part-2-model-hosting-patterns-in-amazon-sagemaker-getting-started-with-deploying-real-time-models-on-sagemaker/).'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意**：本文假定读者具有 AWS、SageMaker 和 Python 的基础知识。要了解 SageMaker 实时推理，请查看以下入门 [博客](https://aws.amazon.com/blogs/machine-learning/part-2-model-hosting-patterns-in-amazon-sagemaker-getting-started-with-deploying-real-time-models-on-sagemaker/)。'
- en: Setup & Locally Test Inference
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 设置并在本地测试推理
- en: For development you can utilize either a SageMaker Classic Notebook Instance
    or a SageMaker Studio Kernel. For our environment we utilized a TensorFlow 2.0
    Kernel with Python3 on a ml.t3.medium base instance.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在开发过程中，你可以使用 SageMaker Classic Notebook 实例或 SageMaker Studio Kernel。对于我们的环境，我们使用了一个
    TensorFlow 2.0 Kernel，基于 Python3 的 ml.t3.medium 实例。
- en: The model we will be utilizing today is a pre-trained [TensorFlow ResNet50](https://www.tensorflow.org/api_docs/python/tf/keras/applications/resnet50/ResNet50)
    for Image Classification. We can first retrieve this model from the TensorFlow
    model hub and pull it into our notebook.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 今天我们将使用的是一个预训练的 [TensorFlow ResNet50](https://www.tensorflow.org/api_docs/python/tf/keras/applications/resnet50/ResNet50)
    图像分类模型。我们可以首先从 TensorFlow 模型中心获取该模型，并将其导入到我们的笔记本中。
- en: '[PRE0]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Before we get to testing on SageMaker we want to locally test this model so
    we can get an idea of the type of input format we need to configure for our endpoint.
    For our sample data point, we will use a picture of my dog Milo when he was a
    puppy (he’s a behemoth now).
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始在 SageMaker 上进行测试之前，我们希望在本地测试该模型，以便了解我们需要为端点配置的输入格式。对于我们的样本数据点，我们将使用我狗狗
    Milo 当小狗时的图片（现在他已经变成一个庞然大物了）。
- en: '![](../Images/46e9a8cf71551b16a89d46b3141bc0a2.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/46e9a8cf71551b16a89d46b3141bc0a2.png)'
- en: Milo (Picture by Author)
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: Milo（作者图片）
- en: '[PRE1]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '![](../Images/e4708dc24bf777ae7a5479714f5cd68e.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e4708dc24bf777ae7a5479714f5cd68e.png)'
- en: Model Results (Screenshot by Author)
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 模型结果（作者截图）
- en: We’ve now verified the format in which our model expects inference so we can
    focus on getting it configured for SageMaker.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在已经验证了模型期望的推理格式，因此可以集中精力将其配置到 SageMaker 上。
- en: Prepare Model and Payload
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备模型和负载
- en: 'SageMaker Inference Recommender expects two mandatory inputs: the model data
    and a sample payload. For both it expects it in a tarball format so we take our
    artifacts and zip them in a format that the service will understand.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: SageMaker Inference Recommender 需要两个必需的输入：模型数据和一个示例负载。它们都需要以 tarball 格式提供，因此我们将工件压缩成服务可以理解的格式。
- en: For our model we can either take the model we already loaded earlier in the
    notebook or instantiate a new version. We download the model artifacts into a
    local directory with the necessary metadata for TensorFlow Serving.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的模型，我们可以使用之前在笔记本中加载的模型，或者实例化一个新版本。我们将模型工件下载到一个本地目录，并包含 TensorFlow Serving
    所需的元数据。
- en: '[PRE2]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: We can then tar this into a model.tar.gz and upload it to an S3 bucket that
    we can point Inference Recommender to.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们可以将其打包成 model.tar.gz，并上传到可以供 Inference Recommender 指向的 S3 存储桶中。
- en: '[PRE3]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: We then take our sample image and convert it into a JSON for our model and save
    this to a tarball in a similar manner as we did for the model artifacts.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们将样本图像转换为 JSON 格式，以供我们的模型使用，并以类似于处理模型工件的方式将其保存到 tarball 中。
- en: '[PRE4]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Now that we have our inputs configured we can move onto the SageMaker portion
    of the project.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经配置了输入，可以继续进行项目的 SageMaker 部分。
- en: Create SageMaker Model & Track With Model Registry
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建 SageMaker 模型并使用模型注册表进行跟踪
- en: 'SageMaker has a few specific objects specific to its service, an important
    one for us in this case is the [SageMaker Model](https://sagemaker.readthedocs.io/en/stable/api/inference/model.html)
    entity. This model entity consists of two core factors: model data and container/image.
    The model data can be your trained or pre-trained model artifacts that you provide
    in an S3 Bucket. The container is essentially the framework of your model. In
    this case we can [retrieve](https://aws.plainenglish.io/how-to-retrieve-amazon-sagemaker-deep-learning-images-ff4a5866299e)
    the managed SageMaker TensorFlow image, but you can also build and push your own
    container if it’s unsupported by [AWS Deep Learning Containers](https://github.com/aws/deep-learning-containers/blob/master/available_images.md).
    Here we define this SageMaker [Model object](https://sagemaker.readthedocs.io/en/stable/api/inference/model.html#),
    utilizing the [SageMaker Python SDK](/sagemaker-python-sdk-vs-boto3-sdk-45c424e8e250).'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: SageMaker有一些特定于其服务的对象，其中对我们来说重要的是[SageMaker模型](https://sagemaker.readthedocs.io/en/stable/api/inference/model.html)实体。这个模型实体包括两个核心因素：模型数据和容器/镜像。模型数据可以是你在S3桶中提供的训练或预训练模型工件。容器本质上是你模型的框架。在这种情况下，我们可以[获取](https://aws.plainenglish.io/how-to-retrieve-amazon-sagemaker-deep-learning-images-ff4a5866299e)托管的SageMaker
    TensorFlow镜像，但如果不受[AWS深度学习容器](https://github.com/aws/deep-learning-containers/blob/master/available_images.md)支持，你也可以构建并推送自己的容器。在这里我们定义了这个SageMaker
    [模型对象](https://sagemaker.readthedocs.io/en/stable/api/inference/model.html#)，利用[SageMaker
    Python SDK](/sagemaker-python-sdk-vs-boto3-sdk-45c424e8e250)。
- en: '[PRE5]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: An optional step that you can utilize is registering your model with [SageMaker
    Model Registry](/register-and-deploy-models-with-sagemaker-model-registry-5af42d678912).
    Tracking hundreds of models can be a difficult process and with Model Registry
    you can essentially simplify model versioning and lineage so that you have all
    model entities in one core space. We can register a model with the following API
    call.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 一个可选步骤是将你的模型注册到[SageMaker模型注册表](/register-and-deploy-models-with-sagemaker-model-registry-5af42d678912)。跟踪数百个模型可能是一个困难的过程，通过模型注册表，你可以简化模型版本控制和血统，使所有模型实体都集中在一个核心空间。我们可以通过以下API调用来注册模型。
- en: '[PRE6]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: We can also view the model package that we just created in the SageMaker Studio
    Console.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以在SageMaker Studio控制台中查看刚刚创建的模型包。
- en: '![](../Images/d0c146b9c3e90bdeb42b526f1a62499a.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d0c146b9c3e90bdeb42b526f1a62499a.png)'
- en: Model Package (Screenshot by Author)
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 模型包（作者截图）
- en: In a real-world use-case you may have multiple models within a singular model
    package and you can approve the one that you choose to deploy to production.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在实际应用中，你可能在一个模型包内有多个模型，你可以选择批准部署到生产环境的模型。
- en: Now that we have our Model object prepared we can run an Inference Recommender
    Job on this entity.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经准备好了模型对象，我们可以在这个实体上运行推理推荐工作。
- en: Inference Recommender Job
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 推理推荐工作
- en: 'There are two types of Inference Recommender Jobs: Default and [Advanced](https://docs.aws.amazon.com/sagemaker/latest/dg/inference-recommender-load-test.html).
    With a Default Job, we can simply pass in our sample payload along with an array
    of EC2 instances that you want to test your model against. Underneath the hood,
    Inference Recommender will test your model against all these instances and track
    throughput and latency for your. We can utilize the [right_size](https://sagemaker.readthedocs.io/en/stable/api/inference/model.html#sagemaker.model.Model.right_size)
    API call to kick off an Inference Recommender Job.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 有两种类型的推理推荐工作：默认和[高级](https://docs.aws.amazon.com/sagemaker/latest/dg/inference-recommender-load-test.html)。使用默认工作，我们可以简单地传入样本负载以及一组你想测试模型的EC2实例。推理推荐器将在这些实例上测试你的模型，并跟踪吞吐量和延迟。我们可以利用[right_size](https://sagemaker.readthedocs.io/en/stable/api/inference/model.html#sagemaker.model.Model.right_size)
    API调用来启动推理推荐工作。
- en: '[PRE7]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: This job will take approximately 35–40 minutes to complete as it will iterate
    across the different instance types that you have provided. We can then view these
    results in the SageMaker Studio UI.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 这个工作大约需要35-40分钟完成，因为它将遍历你提供的不同实例类型。然后我们可以在SageMaker Studio UI中查看这些结果。
- en: '![](../Images/5cd28ba093d5a8c81dc468fd7e996f60.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5cd28ba093d5a8c81dc468fd7e996f60.png)'
- en: Default Job Results (Screenshot by Author)
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 默认工作结果（作者截图）
- en: Here you can toggle cost, latency, and throughput on importance levels and get
    the optimal hardware configuration. You can also directly create your endpoint
    from the console if you are happy with the performance shown by the tests.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，你可以根据重要性级别切换成本、延迟和吞吐量，以获得最佳的硬件配置。如果对测试显示的性能满意，你还可以直接从控制台创建端点。
- en: Lastly, if you want to test different hyperparameters for your container, this
    is also available through the Advanced Inference Recommender Job. Here you can
    specify hyperparameters that are tunable for your specific model container.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，如果你想测试容器的不同超参数，可以通过高级推理推荐作业来实现。在这里，你可以指定适用于特定模型容器的可调超参数。
- en: '[PRE8]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Along with this, you can also configure the traffic patterns for your load test.
    For example, if you want to scale up the number of users across different intervals
    you can configure this behavior.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 除此之外，你还可以配置负载测试的流量模式。例如，如果你想在不同时间间隔内增加用户数量，可以配置这种行为。
- en: '[PRE9]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: You can also set thresholds, for example if you have a strict latency requirement
    of 200ms, this can be set as a stopping parameter if your configuration is not
    achieving these results.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 你还可以设置阈值，例如，如果你有严格的 200 毫秒延迟要求，如果你的配置未达到这些结果，可以将其设置为停止参数。
- en: '[PRE10]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: You can then kick off and view the results of the Advanced Job in a similar
    fashion to the default job.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，你可以以类似于默认作业的方式启动并查看高级作业的结果。
- en: '[PRE11]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Additional Resources & Conclusion
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 额外资源与结论
- en: '[](https://github.com/aws-samples/sagemaker-inference-recommender-examples/blob/main/tf-resnet/inference-recommender-tf-resnet.ipynb?source=post_page-----b96746b69292--------------------------------)
    [## sagemaker-inference-recommender-examples/inference-recommender-tf-resnet.ipynb
    at main ·…'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '[## sagemaker-inference-recommender-examples/inference-recommender-tf-resnet.ipynb
    at main ·…](https://github.com/aws-samples/sagemaker-inference-recommender-examples/blob/main/tf-resnet/inference-recommender-tf-resnet.ipynb?source=post_page-----b96746b69292--------------------------------)'
- en: Contribute to aws-samples/sagemaker-inference-recommender-examples development
    by creating an account on GitHub.
  id: totrans-58
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 通过在 GitHub 上创建账户，为 aws-samples/sagemaker-inference-recommender-examples 的开发做出贡献。
- en: github.com](https://github.com/aws-samples/sagemaker-inference-recommender-examples/blob/main/tf-resnet/inference-recommender-tf-resnet.ipynb?source=post_page-----b96746b69292--------------------------------)
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '[GitHub 地址](https://github.com/aws-samples/sagemaker-inference-recommender-examples/blob/main/tf-resnet/inference-recommender-tf-resnet.ipynb?source=post_page-----b96746b69292--------------------------------)'
- en: You can find the code for this example and more at the link above. SageMaker
    Inference Recommender is a powerful tool that can automate the difficult portion
    of load testing setup. It’s important to note, however that at the moment there
    is no support for advanced hosting options such as [Multi-Model and Multi-Container
    Endpoints](/sagemaker-multi-model-vs-multi-container-endpoints-304f4c151540),
    so for those use-cases utilizing a third party framework such as [Locust](/load-testing-sagemaker-multi-model-endpoints-f0db7b305770)
    will be necessary. As always any feedback is appreciated and feel free to reach
    out with any questions or comments, thank you for reading!
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在上面的链接中找到此示例的代码及更多内容。SageMaker 推理推荐器是一个强大的工具，可以自动化负载测试设置的复杂部分。然而，需要注意的是，目前不支持如[多模型和多容器端点](/sagemaker-multi-model-vs-multi-container-endpoints-304f4c151540)这样的高级托管选项，因此对于这些用例，使用像[Locust](/load-testing-sagemaker-multi-model-endpoints-f0db7b305770)这样的第三方框架将是必要的。如往常一样，任何反馈都是受欢迎的，欢迎随时提出问题或评论，感谢阅读！
- en: '*If you enjoyed this article feel free to connect with me on* [*LinkedIn*](https://www.linkedin.com/in/ram-vegiraju-81272b162/)
    *and subscribe to my Medium* [*Newsletter*](https://ram-vegiraju.medium.com/subscribe)*.
    If you’re new to Medium, sign up using my* [*Membership Referral*](https://ram-vegiraju.medium.com/membership)*.*'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '*如果你喜欢这篇文章，欢迎通过* [*LinkedIn*](https://www.linkedin.com/in/ram-vegiraju-81272b162/)
    *与我联系，并订阅我的 Medium* [*Newsletter*](https://ram-vegiraju.medium.com/subscribe)*。如果你是
    Medium 新用户，可以使用我的* [*会员推荐链接*](https://ram-vegiraju.medium.com/membership)*注册。*'
