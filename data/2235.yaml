- en: Image composition with pre-trained diffusion models
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用预训练扩散模型进行图像合成
- en: 原文：[https://towardsdatascience.com/image-composition-with-pre-trained-diffusion-models-772cd01b5022?source=collection_archive---------5-----------------------#2023-07-12](https://towardsdatascience.com/image-composition-with-pre-trained-diffusion-models-772cd01b5022?source=collection_archive---------5-----------------------#2023-07-12)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/image-composition-with-pre-trained-diffusion-models-772cd01b5022?source=collection_archive---------5-----------------------#2023-07-12](https://towardsdatascience.com/image-composition-with-pre-trained-diffusion-models-772cd01b5022?source=collection_archive---------5-----------------------#2023-07-12)
- en: A technique to increase control over the images generated by pre-trained text-to-image
    diffusion models
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 一种提高对预训练文本到图像扩散模型生成图像的控制的方法
- en: '[](https://medium.com/@gabrielesgroi94?source=post_page-----772cd01b5022--------------------------------)[![Gabriele
    Sgroi, PhD](../Images/b81978d35e6238d160457de2affc2b0e.png)](https://medium.com/@gabrielesgroi94?source=post_page-----772cd01b5022--------------------------------)[](https://towardsdatascience.com/?source=post_page-----772cd01b5022--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----772cd01b5022--------------------------------)
    [Gabriele Sgroi, PhD](https://medium.com/@gabrielesgroi94?source=post_page-----772cd01b5022--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@gabrielesgroi94?source=post_page-----772cd01b5022--------------------------------)[![Gabriele
    Sgroi, PhD](../Images/b81978d35e6238d160457de2affc2b0e.png)](https://medium.com/@gabrielesgroi94?source=post_page-----772cd01b5022--------------------------------)[](https://towardsdatascience.com/?source=post_page-----772cd01b5022--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----772cd01b5022--------------------------------)
    [Gabriele Sgroi, PhD](https://medium.com/@gabrielesgroi94?source=post_page-----772cd01b5022--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F97ea0c34751b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fimage-composition-with-pre-trained-diffusion-models-772cd01b5022&user=Gabriele+Sgroi%2C+PhD&userId=97ea0c34751b&source=post_page-97ea0c34751b----772cd01b5022---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----772cd01b5022--------------------------------)
    ·8 min read·Jul 12, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F772cd01b5022&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fimage-composition-with-pre-trained-diffusion-models-772cd01b5022&user=Gabriele+Sgroi%2C+PhD&userId=97ea0c34751b&source=-----772cd01b5022---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F97ea0c34751b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fimage-composition-with-pre-trained-diffusion-models-772cd01b5022&user=Gabriele+Sgroi%2C+PhD&userId=97ea0c34751b&source=post_page-97ea0c34751b----772cd01b5022---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----772cd01b5022--------------------------------)
    ·8分钟阅读·2023年7月12日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F772cd01b5022&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fimage-composition-with-pre-trained-diffusion-models-772cd01b5022&user=Gabriele+Sgroi%2C+PhD&userId=97ea0c34751b&source=-----772cd01b5022---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F772cd01b5022&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fimage-composition-with-pre-trained-diffusion-models-772cd01b5022&source=-----772cd01b5022---------------------bookmark_footer-----------)![](../Images/022b88492b43e8b995f3e5257036c4b7.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F772cd01b5022&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fimage-composition-with-pre-trained-diffusion-models-772cd01b5022&source=-----772cd01b5022---------------------bookmark_footer-----------)![](../Images/022b88492b43e8b995f3e5257036c4b7.png)'
- en: An image generated with Stable Diffusion using the method described in the post.
    Image by the author.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 使用文章中描述的方法生成的稳定扩散图像。图片由作者提供。
- en: Text-to-image diffusion models have achieved stunning performance in generating
    photorealistic images adhering to natural language description prompts. The release
    of open-source pre-trained models, such as Stable Diffusion, has contributed to
    the democratization of these techniques. Pre-trained diffusion models allow anyone
    to create amazing images without the need for a huge amount of computing power
    or a long training process.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 文本到图像的扩散模型在生成符合自然语言描述的逼真图像方面取得了惊人的表现。开源预训练模型的发布，例如**稳定扩散**，促进了这些技术的民主化。预训练扩散模型使任何人都可以创造出令人惊叹的图像，而不需要大量计算能力或漫长的训练过程。
- en: Despite the level of control offered by text-guided image generation, obtaining
    an image with a predetermined composition is often tricky, even with extensive
    prompting. In fact, standard text-to-image diffusion models offer little control
    over the various elements that will be depicted in the generated image.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管文本引导的图像生成提供了控制水平，但即使有大量提示，获得具有预定组成的图像仍然很棘手。实际上，标准的文本到图像扩散模型对生成图像中将描绘的各种元素几乎没有控制。
- en: 'In this post, I will explain a recent technique based on the paper [MultiDiffusion:
    Fusing Diffusion Paths for Controlled Image Generation](https://arxiv.org/abs/2302.08113).
    This technique makes it possible to obtain greater control in placing elements
    in an image generated by a text-guided diffusion model. The method presented in
    the paper is more general and allows for other applications, such as generating
    panoramic images, but I will restrict here to the case of image compositionality
    using region-based text prompts. The main advantage of this method is that it
    can be used with out-of-the-box pre-trained diffusion models without the need
    for expensive retraining or fine-tuning.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '在这篇文章中，我将解释一种基于论文[MultiDiffusion: Fusing Diffusion Paths for Controlled Image
    Generation](https://arxiv.org/abs/2302.08113)的最新技术。这种技术使得在由文本引导的扩散模型生成的图像中放置元素的控制更为精确。论文中提出的方法更为通用，还可以用于其他应用，如生成全景图像，但我将在这里限制讨论图像组成性，使用基于区域的文本提示。该方法的主要优点是可以与开箱即用的预训练扩散模型一起使用，无需昂贵的重新训练或微调。'
- en: To complement this post with code, I have prepared a simple [Colab notebook](https://colab.research.google.com/drive/1MzzGN5FJNqlJESuO5FigYC8eoOfwzplO?usp=sharing)
    and a [GitHub repository](https://github.com/GabrieleSgroi/image_composition_diffusion)
    with the code implementation I used to generate the images in this post. The code
    is based on the pipeline for Stable Diffusion contained in the [diffusers library](https://github.com/huggingface/diffusers)
    by Hugging Face, but it implements only the parts necessary for its functioning
    to make it simpler and easier to read.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 为了补充这篇文章的代码，我准备了一个简单的[Colab notebook](https://colab.research.google.com/drive/1MzzGN5FJNqlJESuO5FigYC8eoOfwzplO?usp=sharing)和一个[GitHub
    仓库](https://github.com/GabrieleSgroi/image_composition_diffusion)，其中包含了我用于生成本文中图像的代码实现。该代码基于Hugging
    Face的[diffusers library](https://github.com/huggingface/diffusers)中的稳定扩散管道，但只实现了其功能所需的部分，使其更简单易读。
- en: Diffusion models
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 扩散模型
- en: In this section, I will recall some basic facts about diffusion models. Diffusion
    models are generative models that generate new data by inverting a *diffusion
    process* that maps the data distribution to an isotropic Gaussian distribution.
    More specifically, given an image, the diffusion process consists of a series
    of steps each adding a small amount of Gaussian noise to that image. In the limit
    of an infinite number of steps, the noised image will be indistinguishable from
    pure noise sampled from an isotropic Gaussian distribution.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分中，我将回顾一些关于扩散模型的基本事实。扩散模型是生成模型，通过逆转*扩散过程*来生成新数据，该过程将数据分布映射到各向同性的高斯分布。更具体地说，给定一个图像，扩散过程包括一系列步骤，每一步都向图像中添加少量高斯噪声。在无限多步的极限下，噪声图像将与从各向同性高斯分布中采样的纯噪声无法区分。
- en: The goal of the diffusion model is to invert this process by trying to guess
    the noised image at step t-1 in the diffusion process given the noised image at
    step t. This can be done, for instance, by training a neural network to predict
    the noise added at that step and subtracting it from the noised image.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 扩散模型的目标是通过尝试猜测扩散过程中的步骤t-1处的噪声图像来逆转这一过程，给定步骤t处的噪声图像。例如，可以通过训练一个神经网络来预测该步骤添加的噪声，并将其从噪声图像中减去来实现这一目标。
- en: Once we have trained such a model, we can generate new images by sampling noise
    from an isotropic Gaussian distribution and use the model to invert the diffusion
    process by gradually removing noise.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们训练好这样一个模型，就可以通过从各向同性的高斯分布中采样噪声来生成新图像，并使用模型通过逐渐去除噪声来逆转扩散过程。
- en: '![](../Images/81377b92a5540eae4d6882362fb7b5b8.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/81377b92a5540eae4d6882362fb7b5b8.png)'
- en: 'The goal of the diffusion model is to learn the probability q(x(t-1)|x(t))
    for all time steps t. Image from the paper: [Denoising Diffusion Probabilistic
    Models](https://arxiv.org/abs/2006.11239).'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 扩散模型的目标是学习所有时间步t的概率q(x(t-1)|x(t))。图像来自论文：[Denoising Diffusion Probabilistic
    Models](https://arxiv.org/abs/2006.11239)。
- en: Text-to-image diffusion models invert the diffusion process trying to reach
    an image that corresponds to the description of a text prompt. This is usually
    done by a neural network that, at each step t, predicts the noised image at step
    t-1 conditioned not only to the noised image at step t but also to a text prompt
    describing the image it is trying to reconstruct.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 文本到图像扩散模型反转扩散过程，试图达到与文本提示描述相对应的图像。这通常是通过神经网络完成的，该网络在每一步t预测步骤t-1的噪声图像，条件不仅是步骤t的噪声图像，还包括描述其试图重建的图像的文本提示。
- en: Many image diffusion models, including Stable Diffusion, don’t operate in the
    original image space but rather in a smaller learned *latent space*. In this way,
    it is possible to reduce the required computational resources with minimal quality
    loss. The latent space is usually learned through a *variational autoencoder*.
    The diffusion process in latent space works exactly as before, allowing to generate
    new latent vectors from Gaussian noise. From these, it is possible to obtain a
    newly generated image using the decoder of the variational autoencoder.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 许多图像扩散模型，包括稳定扩散，不是在原始图像空间中操作，而是在一个较小的学习*潜空间*中操作。通过这种方式，可以在最小质量损失的情况下减少所需的计算资源。潜空间通常通过*变分自编码器*来学习。潜空间中的扩散过程与之前完全相同，允许从高斯噪声生成新的潜向量。从这些向量中，可以使用变分自编码器的解码器获得新生成的图像。
- en: Image composition with MultiDiffusion
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用MultiDiffusion进行图像组合
- en: Let us now turn to explain how to get controllable image composition using the
    MultiDiffusion method. The goal is to gain better control over the elements generated
    in an image by a pre-trained text-to-image diffusion model. More specifically,
    given a general description for the image (e.g. a living room, as in the cover
    image), we want a series of elements, specified through text prompts, to be present
    at specific locations (e.g. a red couch in the center, a house plant on the left
    and a painting on the top right). This can be achieved by providing a set of text
    prompts describing the desired elements, and a set of region-based binary masks
    specifying the location inside which the elements must be depicted. As an example,
    the image below contains the bounding boxes for the image elements in the cover
    image.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们转向解释如何使用MultiDiffusion方法获得可控图像组合。目标是通过预训练的文本到图像扩散模型更好地控制生成图像中的元素。具体而言，给定图像的一般描述（例如封面图像中的客厅），我们希望一系列通过文本提示指定的元素出现在特定位置（例如中心的红色沙发，左侧的盆栽和右上角的画作）。这可以通过提供一组描述所需元素的文本提示和一组基于区域的二进制掩码来实现，该掩码指定了元素必须描绘在其中的位置。例如，下面的图像包含封面图像中图像元素的边界框。
- en: '![](../Images/003bdec98e7dbab19c0ae90f0507de8a.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/003bdec98e7dbab19c0ae90f0507de8a.png)'
- en: Bounding boxes and prompts used to generate the cover image. Image by the author.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 生成封面图像所用的边界框和提示。图像由作者提供。
- en: The core idea of [MultiDiffusion](https://arxiv.org/pdf/2302.08113.pdf) for
    controllable image generation is to combine together multiple diffusion processes,
    relative to different specified prompts, to obtain a coherent and smooth image
    showing the content of each prompt in a pre-determined region. The region associated
    with each prompt is specified through a binary mask of the same dimension as the
    image. The pixels of the mask are set to 1 if the prompt has to be depicted in
    that location and 0 otherwise.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '[MultiDiffusion](https://arxiv.org/pdf/2302.08113.pdf)用于可控图像生成的核心思想是将多个扩散过程结合在一起，针对不同指定的提示，以获得在预定区域显示每个提示内容的连贯和平滑图像。与每个提示关联的区域通过与图像相同尺寸的二进制掩码来指定。如果提示必须在该位置描绘，则掩码的像素设为1，否则设为0。'
- en: More specifically, let us denote by t a generic step in a diffusion process
    operating in latent space. Given the noisy latent vectors at timestep t, the model
    will predict the noise for each specified text prompt. From these predicted noises,
    we obtain a set of latent vectors at timestep t-1 (one for each prompt) by removing
    each of the predicted noises from the previous latent vectors at timestep t. To
    get the input for the next time step in the diffusion process, we need to combine
    these different vectors together. This can be done by multiplying each latent
    vector by the corresponding prompt mask and then taking a per-pixel average weighted
    by the masks. Following this procedure, in the region specified by a particular
    mask, the latent vectors will follow the trajectories of the diffusion process
    guided by the corresponding local prompt. Combining the latent vectors together
    at each step, before predicting the noise, ensures global cohesion of the generated
    image as well as smooth transitions between different masked regions.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 更具体地说，我们用 t 表示在潜在空间中运行的扩散过程中的一个通用步骤。给定时间步 t 的噪声潜在向量，模型将预测每个指定文本提示的噪声。从这些预测的噪声中，我们通过从时间步
    t 的前一个潜在向量中去除每个预测噪声，获得时间步 t-1 的一组潜在向量（每个提示一个）。为了获得扩散过程下一时间步的输入，我们需要将这些不同的向量组合在一起。这可以通过将每个潜在向量乘以相应的提示掩码，然后按掩码加权取每像素的平均值来完成。按照这个程序，在特定掩码指定的区域内，潜在向量将遵循由相应局部提示引导的扩散过程轨迹。在每一步将潜在向量组合在一起后，再预测噪声，可以确保生成图像的全球一致性以及不同掩码区域之间的平滑过渡。
- en: MultiDiffusion introduces a bootstrapping phase at the beginning of the diffusion
    process for better adherence to tight masks. During these initial steps, the denoised
    latent vectors corresponding to different prompts are not combined together but
    are instead combined with some noised latent vectors corresponding to a constant
    color background. In this way, as the layout is generally determined early in
    the diffusion process, it is possible to obtain a better match with the specified
    masks as the model can initially focus only on the masked region to depict the
    prompt.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: MultiDiffusion 在扩散过程开始时引入了一个自举阶段，以更好地遵循紧密的掩码。在这些初步步骤中，与不同提示相对应的去噪潜在向量不会被组合在一起，而是与一些对应于常色背景的噪声潜在向量结合在一起。通过这种方式，由于布局通常在扩散过程早期就已经确定，因此可以在模型最初只专注于掩码区域来描绘提示的情况下，获得与指定掩码更好的匹配。
- en: Examples
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 示例
- en: In this section, I will show some applications of the method. I have used the
    pre-trained [stable diffusion 2](https://huggingface.co/stabilityai/stable-diffusion-2-base)
    model hosted by HuggingFace to create all the images in this post, including the
    cover image.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我将展示该方法的一些应用。我使用了 HuggingFace 托管的预训练 [stable diffusion 2](https://huggingface.co/stabilityai/stable-diffusion-2-base)
    模型创建了本文中的所有图像，包括封面图像。
- en: As discussed, a straightforward application of the method is to obtain an image
    containing elements generated in pre-defined locations.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 正如讨论的那样，该方法的一个直接应用是获得包含在预定义位置生成的元素的图像。
- en: '![](../Images/20bcbe3ee87f7a87ef4e43c84398850d.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/20bcbe3ee87f7a87ef4e43c84398850d.png)'
- en: Bounding boxes. Image by the author.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 边界框。图片由作者提供。
- en: '![](../Images/f40fb0daa699be8183ae53738f0d82aa.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f40fb0daa699be8183ae53738f0d82aa.png)'
- en: Image generated using the above bounding boxes. Image by the author.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 使用上述边界框生成的图像。图片由作者提供。
- en: The method allows to specify the styles, or some other property, of the single
    elements to be depicted. This can be used for example to gain a sharp image on
    a blurred background.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 该方法允许指定单个元素的风格或其他属性。这可以用于例如在模糊背景上获得清晰的图像。
- en: '![](../Images/ddb90fb6c46ce9eec3133573f24f925c.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ddb90fb6c46ce9eec3133573f24f925c.png)'
- en: Bounding boxes for blurred background. Image by the author.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 模糊背景的边界框。图片由作者提供。
- en: '![](../Images/8f62fd8e609d28ec050ec2769707fe37.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8f62fd8e609d28ec050ec2769707fe37.png)'
- en: Image generated using the above bounding boxes. Image by the author.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 使用上述边界框生成的图像。图片由作者提供。
- en: The styles of the elements can also be very different, leading to stunning visual
    results. As an example, the image below is obtained by mixing a high-quality photo
    style with a van Gogh-style painting.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 元素的风格也可以非常不同，带来令人惊叹的视觉效果。例如，下面的图像是通过将高质量照片风格与梵高风格的画作混合获得的。
- en: '![](../Images/a13d4eeebd0571dde218bab7a5def55c.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a13d4eeebd0571dde218bab7a5def55c.png)'
- en: Bounding boxes with different styles. Image by the author.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 不同风格的边界框。图片由作者提供。
- en: '![](../Images/faeca8db03b1e36a378c310ddb15e22b.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/faeca8db03b1e36a378c310ddb15e22b.png)'
- en: Image generated using the above bounding boxes. Image by the author.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 使用上述边界框生成的图像。图像由作者提供。
- en: Conclusion
  id: totrans-45
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: In this post, we have explored a method combining different diffusion processes
    together to improve control over the images generated by text-conditioned diffusion
    models. This method increases control over the location in which the elements
    of the image are generated and also to combine seamlessly elements depicted in
    different styles.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我们探讨了一种结合不同扩散过程的方法，以提高对由文本条件扩散模型生成的图像的控制能力。该方法增强了对图像中元素生成位置的控制，并且能够无缝地结合以不同风格描绘的元素。
- en: One of the main advantages of the described procedure is that it can be used
    with pre-trained text-to-image diffusion models without the need for fine-tuning,
    which is generally an expensive procedure. Another strong point is that controllable
    image generation is obtained through binary masks that are simpler to specify
    and handle than more complicated conditionings.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 描述的程序的主要优点之一是它可以与预训练的文本到图像扩散模型一起使用，而无需进行通常较为昂贵的微调。另一个优势是可控图像生成通过二进制掩码实现，这比更复杂的条件设置更容易指定和处理。
- en: The main drawback of this technique is that it needs to make, at each diffusion
    step, one neural network pass per prompt in order to predict the corresponding
    noise. Fortunately, these can be performed in batches to reduce the inference
    time overhead, but at the cost of larger GPU memory utilization. Furthermore,
    sometimes some of the prompts (especially the ones specified only in a small portion
    of the image) are neglected or they cover a bigger area than the one specified
    by the corresponding mask. While this can be mitigated with bootstrapping steps,
    an excessive number of them can reduce the overall quality of the image quite
    significantly as there are fewer steps available to harmonize the elements together.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 这种技术的主要缺点是，在每个扩散步骤中需要为每个提示进行一次神经网络传递，以预测相应的噪声。幸运的是，这些操作可以批量进行，以减少推断时间开销，但代价是更大的
    GPU 内存使用。此外，有时一些提示（特别是仅在图像小部分中指定的提示）会被忽视或覆盖的区域比相应掩码指定的区域要大。虽然可以通过引导步骤来缓解这个问题，但过多的引导步骤可能会显著降低图像的整体质量，因为可以用来协调元素的步骤减少了。
- en: 'It is worth mentioning that the idea of combining different diffusion processes
    is not limited to what is described in this post, but it can also be used for
    further applications, such as panorama image generation as described in the paper
    [MultiDiffusion: Fusing Diffusion Paths for Controlled Image Generation](https://arxiv.org/abs/2302.08113).'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '值得一提的是，结合不同扩散过程的想法并不限于本文描述的内容，它还可以用于其他应用，例如论文中描述的全景图像生成[MultiDiffusion: Fusing
    Diffusion Paths for Controlled Image Generation](https://arxiv.org/abs/2302.08113)。'
- en: I hope you liked this article, if you want to go deeper into the technical details
    you can check this [Colab notebook](https://colab.research.google.com/drive/1MzzGN5FJNqlJESuO5FigYC8eoOfwzplO?usp=sharing)
    and the [GitHub repository](https://github.com/GabrieleSgroi/image_composition_diffusion)
    with the code implementation.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 希望你喜欢这篇文章，如果你想深入了解技术细节，可以查看这个[Colab 笔记本](https://colab.research.google.com/drive/1MzzGN5FJNqlJESuO5FigYC8eoOfwzplO?usp=sharing)和[GitHub
    仓库](https://github.com/GabrieleSgroi/image_composition_diffusion)的代码实现。
