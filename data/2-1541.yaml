- en: Multi-Layer Perceptrons Explained and Illustrated
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 多层感知器的解释与说明
- en: 原文：[https://towardsdatascience.com/multi-layer-perceptrons-8d76972afa2b](https://towardsdatascience.com/multi-layer-perceptrons-8d76972afa2b)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/multi-layer-perceptrons-8d76972afa2b](https://towardsdatascience.com/multi-layer-perceptrons-8d76972afa2b)
- en: Understand the first fully-functional model of neural networks
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解神经网络的第一个完全功能模型
- en: '[](https://medium.com/@roiyeho?source=post_page-----8d76972afa2b--------------------------------)[![Dr.
    Roi Yehoshua](../Images/905a512ffc8879069403a87dbcbeb4db.png)](https://medium.com/@roiyeho?source=post_page-----8d76972afa2b--------------------------------)[](https://towardsdatascience.com/?source=post_page-----8d76972afa2b--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----8d76972afa2b--------------------------------)
    [Dr. Roi Yehoshua](https://medium.com/@roiyeho?source=post_page-----8d76972afa2b--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@roiyeho?source=post_page-----8d76972afa2b--------------------------------)[![Dr.
    Roi Yehoshua](../Images/905a512ffc8879069403a87dbcbeb4db.png)](https://medium.com/@roiyeho?source=post_page-----8d76972afa2b--------------------------------)[](https://towardsdatascience.com/?source=post_page-----8d76972afa2b--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----8d76972afa2b--------------------------------)
    [Dr. Roi Yehoshua](https://medium.com/@roiyeho?source=post_page-----8d76972afa2b--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----8d76972afa2b--------------------------------)
    ·13 min read·Apr 2, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----8d76972afa2b--------------------------------)
    ·13分钟阅读·2023年4月2日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: In [the previous article](https://medium.com/@roiyeho/perceptrons-the-first-neural-network-model-8b3ee4513757)
    we talked about perceptrons as one of the earliest models of neural networks.
    As we have seen, single perceptrons are limited in their computational power since
    they can solve only linearly separable problems.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [上一篇文章](https://medium.com/@roiyeho/perceptrons-the-first-neural-network-model-8b3ee4513757)
    中，我们讨论了感知器作为最早的神经网络模型之一。正如我们所见，单个感知器在计算能力上有限，因为它们只能解决线性可分的问题。
- en: In this article we will discuss multi-layer perceptrons (MLPs), which are networks
    consisting of multiple layers of perceptrons and are much more powerful than single-layer
    perceptrons. We will see how these networks operate and how to use them to solve
    complex tasks such as image classification.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们将讨论多层感知器（MLP），这些网络由多个感知器层组成，比单层感知器强大得多。我们将探讨这些网络如何运作，以及如何利用它们解决复杂任务，例如图像分类。
- en: Definitions and Notations
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 定义与符号
- en: 'A **multi-layer perceptron** (MLP) is a neural network that has at least three
    layers: an input layer, an hidden layer and an output layer. Each layer operates
    on the outputs of its preceding layer:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '**多层感知器**（MLP）是一个至少有三层的神经网络：输入层、隐藏层和输出层。每一层都处理其前一层的输出：'
- en: '![](../Images/a1eb1fbb0ed55fbf175252ae54f14c61.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a1eb1fbb0ed55fbf175252ae54f14c61.png)'
- en: The MLP architecture
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: MLP 架构
- en: 'We will use the following notations:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用以下符号：
- en: '*aᵢˡ* is the activation (output) of neuron *i* in layer *l*'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*aᵢˡ* 是层 *l* 中神经元 *i* 的激活值（输出）'
- en: '*wᵢⱼˡ* is the weight of the connection from neuron *j* in layer *l*-1 to neuron
    *i* in layer *l*'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*wᵢⱼˡ* 是从层 *l*-1 中神经元 *j* 到层 *l* 中神经元 *i* 的连接权重'
- en: '*bᵢˡ* is the bias term of neuron *i* in layer *l*'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*bᵢˡ* 是层 *l* 中神经元 *i* 的偏置项'
- en: The intermediate layers between the input and the output are called **hidden
    layers** since they are not visible outside of the network (they form the “internal
    brain” of the network).
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 输入层和输出层之间的中间层称为 **隐藏层**，因为它们在网络之外不可见（它们构成了网络的“内部大脑”）。
- en: The input layer is typically not counted in the number of layers in the network.
    For example, a 3-layer network has one input layer, two hidden layers, and an
    output layer.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 输入层通常不算作网络中的层数。例如，一个3层网络有一个输入层、两个隐藏层和一个输出层。
- en: Forward Propagation
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 前向传播
- en: Forward propagation is the process where the input data is fed through the network
    in a forward direction, layer-by-layer, until it generates the output.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 前向传播是将输入数据逐层通过网络的过程，直到生成输出。
- en: The activations of the neurons during the forward propagation phase are computed
    similar to how the activation of a single perceptron is computed.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在前向传播阶段，神经元的激活值计算类似于单个感知器的激活值计算方式。
- en: 'For example, let’s look at neuron *i* in layer *l*. The activation of this
    neuron is computed in two steps:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，让我们看一下第*l*层的神经元 *i*。该神经元的激活值是通过两个步骤计算得出的：
- en: 'We first compute the net input of the neuron as the weighted sum of its incoming
    inputs plus its bias:'
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们首先计算神经元的净输入，作为其输入的加权和加上偏置：
- en: '![](../Images/c1bdf5f6f5c68e0d2e90012c8360fb73.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c1bdf5f6f5c68e0d2e90012c8360fb73.png)'
- en: The net input of neuron i in layer l
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 第 *l* 层的神经元 *i* 的净输入
- en: '2\. We now apply the activation function to the net input to get the neuron’s
    activation:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 2\. 我们现在对净输入应用激活函数以获得神经元的激活值：
- en: '![](../Images/cb4bcad164ee6ecf2463d814e5fd5155.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/cb4bcad164ee6ecf2463d814e5fd5155.png)'
- en: The activation of neuron i in layer l
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 第 *l* 层神经元 *i* 的激活值
- en: By definition, the activations of the neurons in the input layer are equal to
    the feature values of the example currently presented to the network, i.e.,
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 根据定义，输入层神经元的激活值等于当前呈现给网络的示例的特征值，即，
- en: '![](../Images/b5717c3d1ced500889945d444a0d6255.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b5717c3d1ced500889945d444a0d6255.png)'
- en: The activations of the input neurons
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 输入神经元的激活值
- en: where *m* is the number of features in the data set.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *m* 是数据集中的特征数量。
- en: Vectorized Form
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 向量化形式
- en: To make the computations more efficient (especially when using numerical libraries
    like NumPy), we typically use the vectorized form of the above equations.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 为了提高计算效率（特别是在使用像 NumPy 这样的数值库时），我们通常使用上述方程的向量化形式。
- en: We first define the vector **a***ˡ* as the vector containing the activations
    of all the neurons in layer *l*, and the vector**b***ˡ* as the vector with the
    biases of all the neurons in layer *l*.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先定义向量 **a***ˡ* 为包含第 *l* 层所有神经元激活值的向量，以及向量 **b***ˡ* 为包含第 *l* 层所有神经元偏置的向量。
- en: We also define *Wˡ* as the matrix of connection weights from all the neurons
    in layer *l* — 1 to all the neurons in layer *l*. For example, *W*¹₂₃ is the weight
    of the connection between neuron no. 2 in layer 0 (the input layer) and neuron
    no. 3 in layer 1 (the first hidden layer).
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还定义 *Wˡ* 为从第 *l* 层到第 *l* 层 - 1 的所有神经元的连接权重矩阵。例如，*W*¹₂₃ 是层 0（输入层）中神经元编号 2 与层
    1（第一隐藏层）中神经元编号 3 之间连接的权重。
- en: 'We can now write the forward propagation equations in vector form. For each
    layer *l* we compute:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以将前向传播方程写成向量形式。对于每一层 *l*，我们计算：
- en: '![](../Images/9ce5b15f01e65393f47a2048d9ebba24.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9ce5b15f01e65393f47a2048d9ebba24.png)'
- en: The vectorized form of the forward propagation equations
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 前向传播方程的向量化形式
- en: Solving the XOR Problem
  id: totrans-39
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 解决 XOR 问题
- en: 'The first demonstration of the power of MLPs over single perceptrons has shown
    that they are capable of solving the XOR problem. The XOR problem is not linearly
    separable, thus a single perceptron cannot solve it:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 多层感知器（MLP）相对于单层感知器的首次演示表明，它们能够解决 XOR 问题。XOR 问题是非线性可分的，因此单层感知器无法解决：
- en: '![](../Images/37e6380514099aaec8d2ef89be3a9642.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/37e6380514099aaec8d2ef89be3a9642.png)'
- en: The XOR problem
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: XOR 问题
- en: 'However, an MLP with a single hidden layer can easily solve this problem:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，具有单层隐藏层的 MLP 可以轻松解决这个问题：
- en: '![](../Images/cf692a0df09ba84762cf196f2cc00473.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/cf692a0df09ba84762cf196f2cc00473.png)'
- en: An MLP that solves the XOR problem. The bias terms are written inside the nodes.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 解决 XOR 问题的多层感知器（MLP）。偏置项写在节点内部。
- en: Let’s analyze how this MLP works. The MLP has three hidden neurons and one output
    neuron in addition to the two input neurons. We assume here that all the neurons
    use the step activation function (i.e., the function whose value is 1 for all
    non-negative inputs, and 0 for all negative inputs).
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们分析一下这个 MLP 是如何工作的。这个 MLP 除了两个输入神经元外，还有三个隐藏神经元和一个输出神经元。我们在这里假设所有的神经元都使用阶跃激活函数（即，对于所有非负输入，函数值为
    1，而对于所有负输入，函数值为 0）。
- en: The top hidden neuron is connected only to the first input *x*₁ with a connection
    weight of 1, and it has a bias of -1\. Therefore, this neuron fires only when
    *x*₁ = 1 (in which case its net input is 1 × 1 + (-1) = 0, and *f*(0) = 1, where
    *f* is the step function).
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 顶层隐藏神经元仅与第一个输入 *x*₁ 连接，连接权重为 1，且有一个偏置 -1。因此，这个神经元仅在 *x*₁ = 1 时激活（此时其净输入为 1 ×
    1 + (-1) = 0，*f*(0) = 1，其中 *f* 是阶跃函数）。
- en: The middle hidden neuron is connected to both inputs with connection weights
    of 1, and it has a bias of -2\. Therefore, this neuron fires only when both inputs
    are 1.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 中间隐藏神经元与两个输入相连，连接权重为 1，且有一个偏置 -2。因此，这个神经元仅在两个输入都为 1 时激活。
- en: The bottom hidden neuron is connected only to the second input *x*₂ with a connection
    weight of 1, and it has a bias of -1\. Therefore, this neuron fires only when
    *x*₂ = 1.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 底部隐藏神经元仅与第二输入 *x*₂ 连接，连接权重为 1，且偏置为 -1。因此，这个神经元仅在 *x*₂ = 1 时被激活。
- en: The output neuron is connected to the top and the bottom hidden neurons with
    a weight of 1, and to the middle hidden neuron with a weight of -2, and it has
    a bias of -1\. Therefore, it fires only when either the top or the bottom hidden
    neuron fire, but not when both of them fire together. In other words, it fires
    only when *x*₁ = 1 or *x*₂ = 1 but not when both inputs are 1, which is exactly
    what we expect the output of the XOR function to be.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 输出神经元与顶部和底部隐藏神经元的权重为 1，且与中间隐藏神经元的权重为 -2，偏置为 -1。因此，它仅在顶部或底部隐藏神经元激活时被激活，而在两者同时激活时不被激活。换句话说，它仅在
    *x*₁ = 1 或 *x*₂ = 1 时被激活，而在两个输入都为 1 时不会被激活，这正是我们对 XOR 函数输出的预期。
- en: 'For example, let’s compute the forward propagation of this MLP for the inputs
    *x*₁ = 1 and *x*₂ = 0\. The activations of the hidden neurons in this case are:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，计算这个多层感知机（MLP）在输入 *x*₁ = 1 和 *x*₂ = 0 时的前向传播。此时隐藏神经元的激活情况是：
- en: '![](../Images/04c4f48e457481f6411dae3a8eb2ba51.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/04c4f48e457481f6411dae3a8eb2ba51.png)'
- en: The activations of the hidden neurons for x1 = 1 and x2 = 0
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: x1 = 1 和 x2 = 0 时隐藏神经元的激活情况
- en: We can see that only the top hidden neuron fires in this case.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到在这种情况下只有顶部的隐藏神经元被激活。
- en: 'The activation of the output neuron is therefore:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 输出神经元的激活情况是：
- en: '![](../Images/414f961379e3a54e7d06cf06e737f52d.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/414f961379e3a54e7d06cf06e737f52d.png)'
- en: The output of the MLP for x1 = 1 and x2 = 0
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: MLP 在 x1 = 1 和 x2 = 0 时的输出
- en: The output neuron fires in this case, which is what we expect the output of
    XOR to be for the inputs *x*₁ = 1 and *x*₂ = 0.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，输出神经元被激活，这正是我们对输入 *x*₁ = 1 和 *x*₂ = 0 时 XOR 输出的预期。
- en: Verify that you understand how the MLP computes the other three cases of the
    XOR function as well!
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 验证你是否理解 MLP 如何计算 XOR 函数的其他三个情况！
- en: MLP Construction Exercise
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: MLP 构建练习
- en: 'As another example, consider the following data set that contains points from
    three different classes:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 作为另一个例子，考虑以下数据集，其中包含来自三个不同类别的点：
- en: '![](../Images/7dfb7d4a25f24acdf00bcbaec13823d1.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7dfb7d4a25f24acdf00bcbaec13823d1.png)'
- en: Build an MLP that correctly classifies all the points in this data set.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 构建一个 MLP，正确分类数据集中所有点。
- en: '*Hint*: Use the hidden neurons to identify the three classification areas.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '*提示*：使用隐藏神经元来识别三个分类区域。'
- en: The solution can be found at the bottom of this article.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 解决方案可以在本文底部找到。
- en: The Universal Approximation Theorem
  id: totrans-66
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通用逼近定理
- en: One of the remarkable facts about MLPs is that they can compute any arbitrary
    function (even though each neuron in the network computes a very simple function
    such as the step function).
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 关于 MLP 的一个显著事实是它们可以计算任何任意的函数（尽管网络中的每个神经元计算的是非常简单的函数，如阶跃函数）。
- en: The [universal approximation theorem](https://en.wikipedia.org/wiki/Universal_approximation_theorem)
    states that an MLP with one hidden layer (with a sufficient number of neurons)
    can approximate any continuous function of the inputs arbitrarily well. With two
    hidden layers, it can even approximate discontinuous functions. This means that
    even very simple network architectures can be extremely powerful.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '[通用逼近定理](https://en.wikipedia.org/wiki/Universal_approximation_theorem)指出，一个具有足够数量神经元的单隐层
    MLP 可以任意精确地逼近任何连续的输入函数。具有两个隐层的 MLP 甚至可以逼近不连续的函数。这意味着即使是非常简单的网络架构也可以非常强大。'
- en: Unfortunately, the proof of the theorem is non-constructive, i.e., it does not
    tell us how to build a network to compute a specific function but only shows that
    such a network exists.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，定理的证明是非构造性的，即它没有告诉我们如何构建一个网络来计算特定的函数，只是展示了这样的网络存在。
- en: 'Learning in MLPs: Backpropagation'
  id: totrans-70
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: MLP 中的学习：反向传播
- en: Although MLPs have proven to be computationally powerful, for a long time it
    was not clear how to train them on a specific data set. While single perceptrons
    have a simple weight update rule, it was not clear how to apply this rule to the
    weights of the hidden layers, since these do not directly affect the output of
    the network (and hence its training loss).
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管 MLP 已被证明在计算上非常强大，但很长一段时间内还不清楚如何在特定的数据集上训练它们。虽然单层感知器有一个简单的权重更新规则，但不清楚如何将此规则应用于隐藏层的权重，因为这些权重不会直接影响网络的输出（因此也不会直接影响训练损失）。
- en: It took the AI community more than 30 years to solve this problem when in 1986
    Rumelhart et al. introduced their groundbreaking **backpropagation** algorithm
    for training MLPs.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 当 1986 年 Rumelhart 等人引入其突破性的**反向传播**算法用于训练 MLP 时，AI 社区花费了超过 30 年的时间才解决了这个问题。
- en: The main idea of backpropagation is to first compute the gradients of the error
    function of the network with respect to each one of its weights, and then use
    gradient descent to minimize the error. It is called backpropagation because we
    propagate the gradients of the error from the output layer back to the input layer
    using the **chain rule of derivatives**.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 反向传播的主要思想是首先计算网络误差函数相对于每个权重的梯度，然后使用梯度下降来最小化误差。之所以称之为反向传播，是因为我们利用**导数链式法则**将误差的梯度从输出层传播回输入层。
- en: The backpropagation algorithm is thoroughly explained in [this article](https://medium.com/towards-data-science/backpropagation-step-by-step-derivation-99ac8fbdcc28).
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 反向传播算法在 [这篇文章](https://medium.com/towards-data-science/backpropagation-step-by-step-derivation-99ac8fbdcc28)
    中有详细解释。
- en: Activation Functions
  id: totrans-75
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 激活函数
- en: In single-layer perceptrons we have used either the step or the sign functions
    for the neuron’s activation. The issue with these functions is that their gradient
    is 0 almost everywhere (since they are equal to a constant value for *x* > 0 and
    for *x* < 0). This means that we cannot use them in gradient descent to find the
    minimum error of the network.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在单层感知器中，我们使用了步进函数或符号函数作为神经元的激活函数。这些函数的问题在于它们的梯度几乎为 0（因为它们在 *x* > 0 和 *x* < 0
    时等于常数值）。这意味着我们无法在梯度下降中使用它们来找到网络的最小误差。
- en: Therefore, in MLPs we need to use other activation functions. These functions
    should be both differentiable and non-linear (if all the neurons in an MLP use
    a linear activation function then the MLP behaves like a single-layer perceptron).
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在 MLP 中我们需要使用其他激活函数。这些函数应该既可微分又是非线性的（如果 MLP 中所有神经元使用线性激活函数，则 MLP 的行为类似于单层感知器）。
- en: 'For the hidden layers, the three most common activation functions are:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 对于隐藏层，最常见的三种激活函数是：
- en: The sigmoid function
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Sigmoid 函数
- en: '![](../Images/626faaeee07320671c208244f8fc9d07.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/626faaeee07320671c208244f8fc9d07.png)'
- en: 2\. The hyperpoblic tangent function
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 2\. 双曲正切函数
- en: '![](../Images/722972c5d254f95a20b77ace39781619.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/722972c5d254f95a20b77ace39781619.png)'
- en: 3\. The ReLU (rectified linear unit) function
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 3\. ReLU（修正线性单元）函数
- en: '![](../Images/6abf1e2e4d040e3f101274fac04c0b11.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6abf1e2e4d040e3f101274fac04c0b11.png)'
- en: 'The activation function in the output layer depends on the problem the network
    is trying to solve:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 输出层的激活函数取决于网络试图解决的问题：
- en: For regression problems we use the identity function *f*(*x*) = *x*.
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于回归问题，我们使用恒等函数 *f*(*x*) = *x*。
- en: For binary classification problems we use the sigmoid function (shown above).
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于二分类问题，我们使用 Sigmoid 函数（如上所示）。
- en: 'For multi-class classification problems we use the softmax function, which
    converts a vector of *k* real numbers into a probability distribution of *k* possible
    outcomes:'
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于多类分类问题，我们使用 softmax 函数，它将 *k* 个实数的向量转换为 *k* 种可能结果的概率分布：
- en: '![](../Images/55e040a35e031692c9b4ee9159a4cb44.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/55e040a35e031692c9b4ee9159a4cb44.png)'
- en: The softmax function
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: Softmax 函数
- en: 'The reason why we use the softmax function in multi-class problems is explained
    in depth in this article:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中深入解释了为什么我们在多类问题中使用 Softmax 函数：
- en: '[](/deep-dive-into-softmax-regression-62deea103cb8?source=post_page-----8d76972afa2b--------------------------------)
    [## Deep Dive into Softmax Regression'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/deep-dive-into-softmax-regression-62deea103cb8?source=post_page-----8d76972afa2b--------------------------------)
    [## 深入了解 Softmax 回归'
- en: Understand the math behind softmax regression and how to use it to solve an
    image classification task
  id: totrans-93
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 理解 Softmax 回归背后的数学原理以及如何使用它来解决图像分类任务
- en: towardsdatascience.com](/deep-dive-into-softmax-regression-62deea103cb8?source=post_page-----8d76972afa2b--------------------------------)
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/deep-dive-into-softmax-regression-62deea103cb8?source=post_page-----8d76972afa2b--------------------------------)
- en: MLPs in Scikit-Learn
  id: totrans-95
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Scikit-Learn 中的 MLP
- en: 'Scikit-Learn provides two classes that implement MLPs in the sklearn.neural_network
    module:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: Scikit-Learn 提供了两个实现 MLP 的类，位于 sklearn.neural_network 模块中：
- en: '[MLPClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html)
    is used for classification problems.'
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[MLPClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html)
    用于分类问题。'
- en: '[MLPRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPRegressor.html)
    is used for regression problems.'
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[MLPRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPRegressor.html)
    用于回归问题。'
- en: 'The important hyperparameters in these classes are:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 这些类中的重要超参数有：
- en: '*hidden_layer_sizes* — a tuple that defines the number of neurons in each hidden
    layer. The default is (100,), i.e., a single hidden layer with 100 neurons. For
    many problems, using just one or two hidden layers should be enough. For more
    complex problems, you can gradually increase the number of hidden layers, until
    the network starts overfitting the training set.'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*hidden_layer_sizes* — 定义每个隐藏层中神经元数量的元组。默认值是 (100,)，即一个包含 100 个神经元的隐藏层。对于许多问题，使用一两个隐藏层应该足够。对于更复杂的问题，你可以逐渐增加隐藏层的数量，直到网络开始过拟合训练集。'
- en: '*activation* — the activation function to use in the hidden layers. The options
    are ‘identity’, ‘logistic’, ‘tanh’, and ‘relu’ (the default).'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*activation* — 在隐藏层中使用的激活函数。选项有 ‘identity’，‘logistic’，‘tanh’，和 ‘relu’（默认）。'
- en: '*solver* — the solver to use for the weight optimization. The default is ‘adam’,
    which works well on most data sets. The behavior of the various optimizers will
    be explained in a future article.'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*solver* — 用于权重优化的求解器。默认值是 ‘adam’，它在大多数数据集上表现良好。各种优化器的行为将在未来的文章中解释。'
- en: '*alpha* — the L2 regularization coefficient (defaults to 0.0001)'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*alpha* — L2 正则化系数（默认为 0.0001）'
- en: '*batch_size* — the size of the mini-batches used for training (defaults to
    200).'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*batch_size* — 用于训练的迷你批次的大小（默认为 200）。'
- en: '*learning_rate* — learning rate schedule for weight updates (defaults to ‘constant’).'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*learning_rate* — 权重更新的学习率调度（默认为 ‘constant’）。'
- en: '*learning_rate_init* — the initial learning rate used (defaults to 0.001).'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*learning_rate_init* — 使用的初始学习率（默认为 0.001）。'
- en: '*early_stopping* — whether to stop the training when the validation score is
    not improving (defaults to False).'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*early_stopping* — 是否在验证得分没有改善时停止训练（默认为 False）。'
- en: '*validation_fraction* — the proportion of the training set to set aside for
    validation (defaults to 0.1).'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*validation_fraction* — 从训练集中留出用于验证的比例（默认为 0.1）。'
- en: We normally use grid search and cross-validation to tune these hyperparameters.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通常使用网格搜索和交叉验证来调整这些超参数。
- en: Training an MLP on MNIST
  id: totrans-110
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在 MNIST 上训练 MLP
- en: For example, let’s train an MLP on the [MNIST data set](https://en.wikipedia.org/wiki/MNIST_database),
    which is a widely used data set for image classification tasks.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，让我们在 [MNIST 数据集](https://en.wikipedia.org/wiki/MNIST_database) 上训练一个 MLP，这是一个广泛用于图像分类任务的数据集。
- en: The data set contains 60,000 training images and 10,000 testing images of handwritten
    digits. Each image is 28 × 28 pixels in size, and is typically represented by
    a vector of 784 numbers in the range [0, 255]. The task is to classify these images
    into one of the ten digits (0–9).
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集包含 60,000 张训练图像和 10,000 张测试图像，每张图像为 28 × 28 像素，通常用一个包含 784 个数字的向量表示，范围在 [0,
    255] 之间。任务是将这些图像分类到十个数字（0-9）之一。
- en: 'We first fetch the MNIST data set using the [fetch_openml()](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_openml.html)
    function:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先使用 [fetch_openml()](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_openml.html)
    函数获取 MNIST 数据集：
- en: '[PRE0]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The *as_frame* parameter specifies that we want to get the data and the labels
    as NumPy arrays instead of DataFrames (the default of this parameter has changed
    in Scikit-Learn 0.24 from False to ‘auto’).
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '*as_frame* 参数指定我们希望以 NumPy 数组而不是 DataFrame 的形式获取数据和标签（这个参数的默认值在 Scikit-Learn
    0.24 中从 False 改为 ‘auto’）。'
- en: 'Let’s examine the shape of *X*:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们检查 *X* 的形状：
- en: '[PRE1]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '[PRE2]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: That is, *X* consists of 70,000 flat vectors of 784 pixels.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 也就是说，*X* 由 70,000 个 784 像素的平面向量组成。
- en: 'Let’s display the first 50 digits in the data set:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们显示数据集中的前 50 个数字：
- en: '[PRE3]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '![](../Images/8cf5f43a5f3f214df8bf8277c60c3530.png)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8cf5f43a5f3f214df8bf8277c60c3530.png)'
- en: The first 50 digits from the MNIST data set
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: MNIST 数据集中的前 50 个数字
- en: 'Let’s check how many samples we have from each digit:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们检查每个数字的样本数量：
- en: '[PRE4]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '[PRE5]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The data set is fairly balanced between the 10 classes.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集在 10 个类别之间比较均衡。
- en: 'We now scale the inputs to be within the range [0, 1] instead of [0, 255]:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将输入缩放到 [0, 1] 范围内，而不是 [0, 255]：
- en: '[PRE6]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Feature scaling makes the training of neural networks faster and prevents them
    from getting stuck in local optima.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 特征缩放使神经网络的训练更快，并防止它们陷入局部最优解。
- en: 'We now split the data into training and test sets. Note that the first 60,000
    images in MNIST are already designated for training, so we can just use simple
    slicing for the split:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将数据分为训练集和测试集。请注意，MNIST 中前 60,000 张图像已被指定用于训练，因此我们可以通过简单的切片操作来进行拆分：
- en: '[PRE7]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'We now create an MLP classifier with a single hidden layer with 300 neurons.
    We will keep all the other hyperparameters with their default values, except for
    *early_stopping* which we will change to True. We will also set verbose=True in
    order to track the progress of the training:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在创建一个具有 300 个神经元的单隐藏层 MLP 分类器。我们将保持所有其他超参数的默认值，除了*early_stopping*，我们将其更改为
    True。我们还将设置 verbose=True，以便跟踪训练进度：
- en: '[PRE8]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Let’s fit the classifier to the training set:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将分类器拟合到训练集上：
- en: '[PRE9]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The output we get during training is:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 训练期间得到的输出是：
- en: '[PRE10]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: The training stopped after 31 iterations, since the validation score has not
    improved during the previous 10 iterations.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 训练在 31 次迭代后停止，因为在前 10 次迭代中验证分数没有改善。
- en: 'Let’s check the accuracy of the MLP on the training and the test sets:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们检查一下 MLP 在训练集和测试集上的准确性：
- en: '[PRE11]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '[PRE12]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: These are great results, but networks with more complex architectures such as
    convolutional neural networks (CNNs) can achieve even better results on this data
    set (up to 99.91% accuracy on the test!). You can find the state-of-the-art results
    on MNIST with links to the relevant papers [here](https://paperswithcode.com/sota/image-classification-on-mnist).
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是很好的结果，但具有更复杂架构的网络，如卷积神经网络（CNNs），可以在这个数据集上获得更好的结果（在测试集上的准确率高达 99.91%！）。你可以在[这里](https://paperswithcode.com/sota/image-classification-on-mnist)找到关于
    MNIST 的最先进结果及相关论文的链接。
- en: 'To understand better the errors of our model, let’s display its confusion matrix:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地理解我们模型的错误，让我们显示其混淆矩阵：
- en: '[PRE13]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '![](../Images/565ee83303ba8410af3687aa5c8e7f26.png)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/565ee83303ba8410af3687aa5c8e7f26.png)'
- en: The confusion matrix on the test set
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 测试集上的混淆矩阵
- en: We can see that the main confusions of the model are between the digits 4⇔9,
    7⇔9 and 2⇔8\. This makes sense since these digits often resemble each other when
    written by hand. To help our model distinguish between these digits, we can add
    more examples from these digits (e.g., by using data augmentation) or extract
    additional features from the images (e.g., the number of closed loops in the digit).
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，模型的主要混淆发生在数字 4⇔9、7⇔9 和 2⇔8 之间。这是有道理的，因为这些数字在手写时经常彼此相似。为了帮助我们的模型区分这些数字，我们可以增加这些数字的样本（例如，通过数据增强）或从图像中提取额外的特征（例如，数字中的闭合环数）。
- en: Visualizing the MLP Weights
  id: totrans-149
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 可视化 MLP 权重
- en: Although neural networks are generally considered to be “black-box” models,
    in simple networks that consist of one or two hidden layers, we can visualize
    the learned weights and occasionally gain some insight into how these networks
    work internally.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管神经网络通常被认为是“黑箱”模型，但在由一到两个隐藏层组成的简单网络中，我们可以可视化学到的权重，并偶尔对这些网络如何在内部工作有所了解。
- en: 'For example, we can plot the weights between the input and the hidden layers
    of our MLP classifier. The weight matrix has a shape of (784, 300), and is stored
    in a variable called mlp.coefs_[0]:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，我们可以绘制 MLP 分类器的输入层和隐藏层之间的权重。权重矩阵的形状为 (784, 300)，并存储在一个名为 mlp.coefs_[0] 的变量中：
- en: '[PRE14]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '[PRE15]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Column *i* of this matrix represents the weights of the incoming inputs to hidden
    neuron *i*. We can display this column as a 28 × 28 pixel image, in order to examine
    which input neurons have a stronger influence on this neuron’s activation.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 这个矩阵的 *i* 列表示输入到隐藏神经元 *i* 的权重。我们可以将这一列显示为一个 28 × 28 像素的图像，以检查哪些输入神经元对该神经元的激活有较强的影响。
- en: 'The following plot displays the weights of the first 20 hidden neurons:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图展示了前 20 个隐藏神经元的权重：
- en: '[PRE16]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '![](../Images/ec1a1c851e805c9f1ec63ced661523f7.png)'
  id: totrans-157
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ec1a1c851e805c9f1ec63ced661523f7.png)'
- en: The weights of the first 20 hidden neurons
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 前 20 个隐藏神经元的权重
- en: We can see that each hidden neuron focuses on different segments of the image.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到每个隐藏神经元关注图像的不同部分。
- en: MLPs in Other Libraries
  id: totrans-160
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 其他库中的 MLP
- en: Although the MLP classifier in Scikit-Learn is easy to use, in practical applications
    you are more likely to use a deep learning library such as TensorFlow or PyTorch
    to build MLPs. These libraries can take advantage of faster GPU processing and
    they also provide many additional options, such as additional activation functions
    and optimizers. You can find an example of how to use these libraries in [this
    post](https://medium.com/@roiyeho/pytorch-2-0-or-tensorflow-2-10-which-one-is-better-52669cec994).
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管 Scikit-Learn 中的 MLP 分类器易于使用，但在实际应用中，你更可能使用如 TensorFlow 或 PyTorch 等深度学习库来构建
    MLP。这些库可以利用更快的 GPU 处理速度，还提供许多附加选项，如额外的激活函数和优化器。你可以在[这篇文章](https://medium.com/@roiyeho/pytorch-2-0-or-tensorflow-2-10-which-one-is-better-52669cec994)中找到如何使用这些库的示例。
- en: Solution to the MLP Construction Exercise
  id: totrans-162
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: MLP 构建练习的解决方案
- en: 'The following MLP classifies correctly all the points in the data set:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 以下 MLP 正确分类了数据集中的所有点：
- en: '![](../Images/78fe6af3673e411edf25fde4e044d6bc.png)'
  id: totrans-164
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/78fe6af3673e411edf25fde4e044d6bc.png)'
- en: MLP for solving the classification problem. The bias terms are written inside
    the nodes.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: MLP 用于解决分类问题。偏置项被写在节点内部。
- en: 'Explanation:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 解释：
- en: The left hidden neuron fires only when *x*₁ ≤ 3, the middle hidden neuron fires
    only when *x*₂ ≥ 4, and the right hidden neuron fires only when *x*₂ ≤ 0.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 左侧隐藏神经元只有在*x*₁ ≤ 3 时才会激活，中间隐藏神经元只有在*x*₂ ≥ 4 时才会激活，右侧隐藏神经元只有在*x*₂ ≤ 0 时才会激活。
- en: The left output neuron performs an OR between the left and the middle hidden
    neurons, therefore it fires only if *x*₁ ≤ 3 OR *x*₂ ≥ 4, i.e., only when the
    point is blue.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 左侧输出神经元对左侧和中间隐藏神经元执行 OR 操作，因此只有在*x*₁ ≤ 3 OR *x*₂ ≥ 4 时才会激活，即只有当点是蓝色时。
- en: The middle output neuron performs a NOR (Not OR) between all the hidden neurons,
    therefore it fires only when NOT (*x*₁ ≤ 3 OR *x*₂ ≥ 4 OR *x*₂ ≤ 0). In other
    words, it fires only when *x*₁ > 3 AND 0 < *x*₂ < 4, i.e., only when the point
    is red.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 中间输出神经元对所有隐藏神经元执行 NOR（非 OR）操作，因此只有在 NOT（*x*₁ ≤ 3 OR *x*₂ ≥ 4 OR *x*₂ ≤ 0）时才会激活。换句话说，只有当*x*₁
    > 3 AND 0 < *x*₂ < 4 时，它才会激活，即只有当点是红色时。
- en: The right output neuron fires only when the right hidden neuron fires, i.e.,
    only when *x*₂ ≤ 0, which is true only for the purple points.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 只有当右侧隐藏神经元激活时，右侧输出神经元才会激活，即只有当*x*₂ ≤ 0 时，才会发生这种情况，这仅对紫色点有效。
- en: Final Notes
  id: totrans-171
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 最终说明
- en: 'You can find the code examples of this article on my github: [https://github.com/roiyeho/medium/tree/main/mlp](https://github.com/roiyeho/medium/tree/main/mlp)'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在我的 GitHub 上找到本文的代码示例：[https://github.com/roiyeho/medium/tree/main/mlp](https://github.com/roiyeho/medium/tree/main/mlp)
- en: All images unless otherwise noted are by the author.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 除非另有说明，否则所有图片均由作者提供。
- en: 'MNIST Dataset Info:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: MNIST 数据集信息：
- en: '**Citation:** Deng, L., 2012\. The mnist database of handwritten digit images
    for machine learning research. *IEEE Signal Processing Magazine*, 29(6), pp. 141–142.'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**引用：** 邓丽君，2012。用于机器学习研究的手写数字图像的 MNIST 数据库。*IEEE 信号处理杂志*，29(6)，第 141–142 页。'
- en: '**License:** Yann LeCun and Corinna Cortes hold the copyright of the MNIST
    dataset which is available under the *Creative Commons Attribution-ShareAlike
    4.0 International License* ([CC BY-SA](https://creativecommons.org/licenses/by-sa/4.0/)).'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**许可证：** Yann LeCun 和 Corinna Cortes 拥有 MNIST 数据集的版权，该数据集根据*知识共享署名-相同方式共享 4.0
    国际许可证*（[CC BY-SA](https://creativecommons.org/licenses/by-sa/4.0/)）提供。'
- en: Thanks for reading!
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 感谢阅读！
