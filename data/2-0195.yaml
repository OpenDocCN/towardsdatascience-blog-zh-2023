- en: A Gentle Introduction To Generative AI For Beginners
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 《初学者友好的生成式 AI 介绍》
- en: 原文：[https://towardsdatascience.com/a-gentle-introduction-to-generative-ai-for-beginners-8c8752085900](https://towardsdatascience.com/a-gentle-introduction-to-generative-ai-for-beginners-8c8752085900)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/a-gentle-introduction-to-generative-ai-for-beginners-8c8752085900](https://towardsdatascience.com/a-gentle-introduction-to-generative-ai-for-beginners-8c8752085900)
- en: Let’s understand the big picture behind generative AI
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 让我们深入了解生成式 AI 的整体图景
- en: '[](https://federicotrotta.medium.com/?source=post_page-----8c8752085900--------------------------------)[![Federico
    Trotta](../Images/e997e3a96940c16ab5071629016d82fd.png)](https://federicotrotta.medium.com/?source=post_page-----8c8752085900--------------------------------)[](https://towardsdatascience.com/?source=post_page-----8c8752085900--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----8c8752085900--------------------------------)
    [Federico Trotta](https://federicotrotta.medium.com/?source=post_page-----8c8752085900--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://federicotrotta.medium.com/?source=post_page-----8c8752085900--------------------------------)[![Federico
    Trotta](../Images/e997e3a96940c16ab5071629016d82fd.png)](https://federicotrotta.medium.com/?source=post_page-----8c8752085900--------------------------------)[](https://towardsdatascience.com/?source=post_page-----8c8752085900--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----8c8752085900--------------------------------)
    [Federico Trotta](https://federicotrotta.medium.com/?source=post_page-----8c8752085900--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----8c8752085900--------------------------------)
    ·8 min read·Jun 29, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----8c8752085900--------------------------------)
    ·阅读时间 8 分钟·2023年6月29日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/738f387e4050427ddb87dbad2b2f05f0.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/738f387e4050427ddb87dbad2b2f05f0.png)'
- en: Image by [Susan Cipriano](https://pixabay.com/it/users/susan-lu4esm-7009216/?utm_source=link-attribution&utm_medium=referral&utm_campaign=image&utm_content=8015423)
    on [Pixabay](https://pixabay.com/it//?utm_source=link-attribution&utm_medium=referral&utm_campaign=image&utm_content=8015423)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由 [Susan Cipriano](https://pixabay.com/it/users/susan-lu4esm-7009216/?utm_source=link-attribution&utm_medium=referral&utm_campaign=image&utm_content=8015423)
    提供，来源于 [Pixabay](https://pixabay.com/it//?utm_source=link-attribution&utm_medium=referral&utm_campaign=image&utm_content=8015423)
- en: The last months have seen the rise of the so-called “Generative AI”, which is
    a sub-field of Artificial Intelligence (AI). Tools like ChatGPT have become one
    of the most spoken words and are becoming fundamental tools for everyday tasks
    in many jobs ([even to learn to code](/how-to-effectively-start-coding-in-the-era-of-chatgpt-cfc5151e1c42)).
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 过去几个月里，所谓的“生成式 AI”得到了广泛关注，这是一种人工智能（AI）的子领域。像 ChatGPT 这样的工具已经成为最常被提及的词汇之一，并且正成为许多工作中日常任务的基础工具（[甚至用来学习编程](/how-to-effectively-start-coding-in-the-era-of-chatgpt-cfc5151e1c42)）。
- en: Words like “[DALL-E](https://medium.com/mlearning-ai/can-i-sell-ai-generated-images-a5d4619c8e1b)”,
    “ChatGPT” and “Generative AI” have pervaded socials, media, chats with colleagues,
    and everything related to our world over the last few months. Literally, everyone
    is talking about that.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 诸如“[DALL-E](https://medium.com/mlearning-ai/can-i-sell-ai-generated-images-a5d4619c8e1b)”、“ChatGPT”和“生成式
    AI”这些词汇在过去几个月里充斥了社交网络、媒体、同事聊天和我们世界的方方面面。几乎每个人都在谈论这些。
- en: But what is generative AI? Why is that any different from “normal” AI?
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，什么是生成式 AI？它与“普通” AI 有何不同？
- en: In this article, we’ll clarify the big picture behind generative AI. So, if
    you’ve participated in discussions but don’t have clear ideas on this topic, this
    article is definitely for you.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我们将澄清生成式 AI 背后的大图景。因此，如果你参与了相关讨论但对这个话题没有明确的理解，这篇文章绝对适合你。
- en: 'This is a discursive explanation to understand the basics of what’s behind
    the scene of generative AI. So, don’t worry: you won’t find any code here. Just
    ideas and descriptions and these will be presented in a very short and concise
    way. In particular, we’ll focus on Large Language Models and Image Generation
    Models.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一种探讨性解释，以理解生成式 AI 背后的基础内容。因此，不用担心：这里不会有任何代码。只有想法和描述，它们将以非常简洁的方式呈现。特别是，我们将重点关注大规模语言模型和图像生成模型。
- en: 'Here’s a summary of what you’ll learn here:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是你将学习的内容概要：
- en: '[PRE0]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: What is generative AI and how does it differ from traditional AI?
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是生成式 AI，它与传统 AI 有何不同？
- en: Generative AI is a subfield of AI that involves creating algorithms that can
    generate new data such as images, text, code, and music.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 生成式 AI 是 AI 的一个子领域，涉及创建能够生成新数据的算法，如图像、文本、代码和音乐。
- en: The big difference between generative AI and “traditional AI” is that the former
    generates new data based on the training data. Also, it works with types of data
    that “traditional AI” can’t.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 生成式人工智能和“传统人工智能”之间的主要区别在于前者根据训练数据生成新的数据。此外，它可以处理“传统人工智能”无法处理的数据类型。
- en: 'Let’s say it a little more technically:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们稍微技术性地说一下：
- en: “Traditional AI” can be defined as discriminative AI. In this case, in fact,
    we train Machine Learning models so that they can make predictions or classifications
    on new, unseen data. These ML models can work only with numbers, and sometimes
    with text (for example, in the case of Natural Language Processing).
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “传统人工智能”可以定义为区分性人工智能。在这种情况下，实际上，我们训练机器学习模型，使其能够对新的、未见过的数据进行预测或分类。这些机器学习模型只能处理数字，有时也处理文本（例如，在自然语言处理的情况下）。
- en: In generative AI, we train an ML model and it creates an output that is similar
    to the data it has been trained on. These kinds of ML models can work with different
    kinds of data like numbers, text, images, and audio.
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在生成式人工智能中，我们训练一个机器学习模型，它创建的输出类似于它所训练的数据。这些类型的机器学习模型可以处理不同类型的数据，如数字、文本、图像和音频。
- en: 'Let’s visualize the processes:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们可视化这些过程：
- en: '![](../Images/6d56bfd8216ebc2e01e106b7576454ee.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6d56bfd8216ebc2e01e106b7576454ee.png)'
- en: The process behind traditional AI. Image by Author.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 传统人工智能的过程。图片来源于作者。
- en: So, in traditional AI, we train an ML model to learn from data. Then, we feed
    it with new and unseen data and it can discriminate, making predictions or classifications.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，在传统人工智能中，我们训练一个机器学习模型从数据中学习。然后，我们将其输入新的、未见过的数据，它可以进行区分，做出预测或分类。
- en: Regarding the presented example, we’ve trained an ML model to recognize dogs
    from images. Then, we feed the trained ML model with new and unseen pictures of
    dogs and it will be able to classify whether these new images are representing
    dogs or not.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 关于所展示的示例，我们已经训练了一个机器学习模型来识别图像中的狗。然后，我们将训练好的机器学习模型输入新的、未见过的狗的图片，它将能够分类这些新图像是否代表狗。
- en: This is the typical task for a Deep Learning algorithm, in the case of a classification
    problem.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 这是深度学习算法在分类问题中的典型任务。
- en: '![](../Images/9d3e30bae09a261503bfdf3b1d4e00ec.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9d3e30bae09a261503bfdf3b1d4e00ec.png)'
- en: The process behind generative AI. Image by Author.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 生成式人工智能的过程。图片来源于作者。
- en: In the case of generative AI, instead, we train an ML model with data from various
    sources using a vast quantity of data. Then, thanks to a prompt (a query in natural
    language inserted by a user), the model gives us an output that is similar to
    the data it’s been trained on.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在生成式人工智能的情况下，我们用来自各种来源的数据训练一个机器学习模型，使用大量的数据。然后，得益于一个提示（用户插入的自然语言查询），模型给出一个类似于它所训练的数据的输出。
- en: To stick to the example, our model has been trained on a massive amount of (text)
    data that, among others, explains what a dog is. Then, if a user queries the model
    asking what a dog is, the model will describe what a dog is in natural language.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 以这个示例为例，我们的模型已经在大量（文本）数据上进行了训练，其中包括解释什么是狗的数据。然后，如果用户向模型查询什么是狗，模型将用自然语言描述什么是狗。
- en: This is the typical task performed by tools like ChatGPT.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 这是像 ChatGPT 这样的工具执行的典型任务。
- en: Now, let’s see some types of generative AI models.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看一些生成式人工智能模型的类型。
- en: Large Language Models
  id: totrans-33
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 大型语言模型
- en: 'Let’s start to dive into the various kinds of generative AI subfields by starting
    with Large Language Models (LLMs). [An LLM is](https://en.wikipedia.org/wiki/Large_language_model#:~:text=A%20large%20language%20model%20(LLM,learning%20or%20semi%2Dsupervised%20learning.)
    (from Wikipedia):'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从大型语言模型（LLMs）开始，深入了解各种生成式人工智能子领域。[LLM 是](https://en.wikipedia.org/wiki/Large_language_model#:~:text=A%20large%20language%20model%20(LLM,learning%20or%20semi%2Dsupervised%20learning.)（来自维基百科）：
- en: a computerized language model consisting of an artificial neural network with
    many parameters (tens of millions to billions), trained on large quantities of
    unlabeled text using self-supervised learning or semi-supervised learning.
  id: totrans-35
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 是一个计算机化的语言模型，由一个具有大量参数（从几千万到几十亿）的人工神经网络组成，使用自监督学习或半监督学习在大量未标记的文本上进行训练。
- en: ''
  id: totrans-36
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Though the term large language model has no formal definition, it often refers
    to deep learning models with millions or even billions of parameters, that have
    been “pre-trained” on a large corpus.
  id: totrans-37
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 尽管“大型语言模型”这个术语没有正式定义，但它通常指的是具有数百万甚至数十亿个参数的深度学习模型，这些模型已在大量语料库上“预训练”。
- en: 'So, LLMs are Deep Learning (DL) models (aka, Neural Networks) trained with
    millions of parameters on a huge amount of text (this is why we call them “large”)
    and are useful to solve some language problems like:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，LLM是深度学习（DL）模型（即神经网络），使用数百万个参数在大量文本上进行训练（这就是我们称之为“大型”的原因），并且对解决一些语言问题很有用，如：
- en: Text classification
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文本分类
- en: Question & Answering
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 问答
- en: Document summarization
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文档总结
- en: Text generation
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文本生成
- en: So, another important difference between standard ML models is that, in this
    case, we can train a DL algorithm that can be used for different tasks.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，标准机器学习模型的另一个重要区别在于，在这种情况下，我们可以训练一个可以用于不同任务的深度学习算法。
- en: Let me explain better.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 让我进一步解释。
- en: 'If we need to develop a system that can recognize dogs in images as we’ve seen
    before, we need to train a DL algorithm to [solve a classification task](/classification-metrics-the-complete-guide-for-aspiring-data-scientists-9f02eab796ae)
    that is: tell us if new, unseen images are representing dogs or not. Nothing more.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们需要开发一个可以识别图像中狗的系统，如前所述，我们需要训练一个深度学习算法来[解决分类任务](/classification-metrics-the-complete-guide-for-aspiring-data-scientists-9f02eab796ae)，即：告诉我们新的、未见过的图像是否代表狗。仅此而已。
- en: Instead, training an LLM can help us in all the tasks we’ve described above.
    So, this also justifies the amount of computing power (and money!) needed to train
    an LLM (which requires petabytes of data!).
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，训练一个LLM可以帮助我们完成上述所有任务。因此，这也证明了训练LLM所需的计算能力（和资金！）的必要性（这需要PB级的数据！）。
- en: 'As we know, LLMs are queried by users thanks to prompts. Now, we have to spot
    the difference between prompt design and prompt engineering:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 我们知道，LLM是通过用户的提示来查询的。现在，我们需要区分提示设计和提示工程：
- en: '**Prompt design**. This is the art of creating a prompt that is specifically
    suitable for the specific task that the system is performing. For example, if
    we want to ask our LLM to translate a text from English to Italian, we have to
    write a specific prompt in English asking the model to translate the text we’re
    pasting into Italian.'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**提示设计**。这是创造一个专门适用于系统执行的具体任务的提示的艺术。例如，如果我们想让我们的LLM将文本从英语翻译成意大利语，我们必须用英语写一个具体的提示，要求模型将我们粘贴的文本翻译成意大利语。'
- en: '**Prompt engineering**. This is the process of creating prompts to improve
    the performance of our LLM. This means using our domain knowledge to add details
    to the prompt like specific keywords, specific context and examples, and the desired
    output if necessary.'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**提示工程**。这是创建提示以提高我们的LLM性能的过程。这意味着使用我们的领域知识来向提示中添加细节，如特定的关键词、特定的上下文和示例，以及必要时所需的输出。'
- en: Of course, when we’re prompting, sometimes we use a mix of both. For example,
    we may want a translation from English to Italian that interests a particular
    domain of knowledge, like mechanics.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，当我们进行提示时，有时会使用两者的混合。例如，我们可能希望将从英语到意大利语的翻译应用于特定知识领域，如力学。
- en: So, for example, a prompt may be:” *Translate in Italian the following:*
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，一个提示可能是：“*将以下内容翻译成意大利语：*
- en: '*the beam is subject to normal stress.*'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '*光束受到正常应力作用。*'
- en: '*Consider that we’re in the field of mechanics, so ‘normal stress’ must be
    related to it*”.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '*考虑到我们处于力学领域，因此‘正常应力’必须与其相关*。'
- en: 'Because, you know: “normal” and “stress” may be misunderstood by the model
    (but even by humans!).'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 因为，你知道：“正常”和“应力”可能会被模型（甚至是人类！）误解。
- en: The three types of LLMs
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 三种类型的LLM
- en: 'There are three types of LLMs:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 有三种类型的LLM：
- en: '**Generic Language Models**. These are able to predict a word (or a phrase)
    based on the language in the training data. Think, for example, of your email
    auto-completion feature to understand this type.'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**通用语言模型**。这些模型能够基于训练数据中的语言预测一个词（或一个短语）。例如，可以考虑你的电子邮件自动完成功能来理解这种类型。'
- en: '**Instruction Tuned Models**. These kinds of models are trained to predict
    a response to the instructions given in the input. Summarizing a given text is
    a typical example.'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**指令调优模型**。这些模型被训练以预测对输入中给出的指令的响应。总结给定文本是一个典型的例子。'
- en: '**Dialog Tuned Models**. These are trained to have a dialogue with the user,
    using the subsequent responses. An AI-powered chatbot is a typical example.'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**对话调优模型**。这些模型被训练与用户进行对话，使用后续的回应。一个典型的例子是AI驱动的聊天机器人。'
- en: Anyway, consider that the models that are actually distributed have mixed features.
    Or, at least, they can perform actions that are typical of more than one of these
    types.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 无论如何，请注意，实际分发的模型具有混合特征。或者，至少，它们可以执行多个这些类型典型的操作。
- en: 'For example, if we think of ChatGPT we can clearly say that it:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果我们考虑ChatGPT，我们可以明确地说它：
- en: Can predict a response to the instructions, given an input. In fact, for example,
    it can summarize texts, give insights on a certain argument we provide via prompts,
    etc… So, it has features like an Instruction Tuned Model.
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可以根据输入预测对指令的响应。事实上，例如，它可以总结文本，提供我们通过提示提供的某个论点的见解，等等……因此，它具有指令调优模型等功能。
- en: Is trained to have a dialog with the users. And this is very clear, as it works
    with consequent prompts until we’re happy with its answer. So, it has also features
    like a Dialog Tuned Model.
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 经过训练以与用户对话。这很明显，因为它会根据后续提示进行工作，直到我们对其答案感到满意。因此，它还具有对话调优模型等功能。
- en: Image generation
  id: totrans-64
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 图像生成
- en: Image generation has been around for quite some time, contrary to what one might
    believe.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 图像生成已经存在了一段时间，这与人们的看法相反。
- en: Anyway, in recent times it gained popularity, especially with tools like “DALL-E”
    or “stable diffusion” that have cleared their use, making this technology accessible
    to the masses worldwide.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 无论如何，近年来它得到了流行，特别是像“DALL-E”或“稳定扩散”这样的工具，它们明确了其用途，使这一技术对全球大众变得可及。
- en: 'We can say that image generation can be divided into four categories:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以说图像生成可以分为四类：
- en: '**Variational Autoencoders** (VAEs). [Variational autoencoders are](https://en.wikipedia.org/wiki/Variational_autoencoder#:~:text=Variational%20autoencoders%20are%20probabilistic%20generative,first%20and%20second%20component%20respectively.)
    “*probabilistic generative models that require neural networks as only a part
    of their overall structure*”. In operational words, they encode images to a compressed
    size and decode them to the original size. During this process, they learn the
    distribution of the data.'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**变分自编码器**（VAEs）。[变分自编码器是](https://en.wikipedia.org/wiki/Variational_autoencoder#:~:text=Variational%20autoencoders%20are%20probabilistic%20generative,first%20and%20second%20component%20respectively.)
    “*需要神经网络作为其整体结构的一部分的概率生成模型*”。用操作性的话来说，它们将图像编码成压缩大小，并解码回原始大小。在此过程中，它们学习数据的分布。'
- en: '**Generative Adversarial Models** (GANs). These are generally the most known,
    at least as a word that resonates in the field of generative AI. [A GAN is](https://en.wikipedia.org/wiki/Generative_adversarial_network)
    “*a class of ML framework in which two Neural Networks are pith against each other
    where the gain of one is the loss of the other*”. This means that one Neural Network
    creates the image while the other predicts if it is real or fake.'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**生成对抗模型**（GANs）。这些通常是最知名的，至少在生成AI领域的术语中是如此。[GAN是](https://en.wikipedia.org/wiki/Generative_adversarial_network)
    “*一个机器学习框架的类别，其中两个神经网络相互对抗，其中一个的收益是另一个的损失*”。这意味着一个神经网络创建图像，而另一个预测它是真实的还是伪造的。'
- en: '**Autoregressive models**. In statistics, an autoregressive model is the representation
    of a random process. In the context of generative images, these kinds of models
    generate images by treating images as a sequence of pixels.'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**自回归模型**。在统计学中，自回归模型是随机过程的表示。在生成图像的背景下，这些模型通过将图像视为像素序列来生成图像。'
- en: '**Diffusion models**. Diffusion models have been inspired by thermodynamics
    and are definitely the most promising and interesting kinds of models in the subfield
    of image generation.'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**扩散模型**。扩散模型受到热力学的启发，毫无疑问是图像生成子领域中最具前景和最有趣的模型。'
- en: 'This is the process working under the hood of diffusion models:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 这是在扩散模型的“引擎盖下”工作的过程：
- en: '**Forward distribution process**. We have an initial, iterative, process where
    the structure of the image is “destroyed” in a data distribution. In simple words,
    is like we iteratively add noise to the image, until all the pixels become pure
    noise and the image is not recognizable (by the human eye).'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**前向分布过程**。我们有一个初始的迭代过程，其中图像的结构在数据分布中被“破坏”。简单来说，就像我们反复向图像中添加噪声，直到所有像素变成纯噪声，图像无法被识别（通过人眼）。'
- en: '**Reverse diffusion process**. Then, there is a reverse diffusion process which
    is the actual learning process: this restores the structure of the data. It is
    like our model learns how to “de-noise” the pixels to recreate the image.'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**反向扩散过程**。然后，有一个反向扩散过程，它是实际的学习过程：它恢复数据的结构。这就像我们的模型学习如何“去噪”像素以重建图像。'
- en: The power of connecting it all
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 连接这一切的力量
- en: 'If you maintained the attention until now, a question should naturally pop
    up in your mind:” *Ok, Federico, it’s clear. But I’m missing something: when I
    use “DALL-E” I insert a prompt and it outputs an image: we haven''t talked about
    that, do we?!*”.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你保持了注意力到现在，你的脑海中自然会浮现一个问题：“*好的，费德里科，这很清楚。但我还有一点没弄明白：当我使用‘DALL-E’时，我输入一个提示，它输出一张图像：我们还没有讨论过这一点，是吗？！*”。
- en: No, we haven’t.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 不，我们没有。
- en: Above we made a brief description of the most promising (and currently, the
    most used) model for generating images, but the missing part is the prompt.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 上面我们简要描述了目前最有前景（也是使用最多）的图像生成模型，但缺少的部分是提示。
- en: 'We’ve discussed, in fact, how they work at a high level. Meaning: we gave a
    short explanation of how their learning process works.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 我们实际上讨论了它们在高层次上的工作原理。也就是说，我们简要解释了它们的学习过程是如何运作的。
- en: But the real power of these models arrives when they are coupled with LLMs.
    This coupling, in fact, gives us the possibility to combine the power of prompt
    engineering to ask our models for outputs.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 但这些模型的真正力量体现在它们与大型语言模型的结合上。实际上，这种结合使我们能够结合提示工程的力量来请求模型输出。
- en: 'In other words: we’ve combined the possibility to use natural language as inputs
    to models that can actually understand it and can generate images according to
    it.'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说：我们结合了使用自然语言作为输入的可能性，这些模型可以实际理解这些输入并生成相应的图像。
- en: Isn’t it a superpower?!?
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 这难道不是一种超级能力吗？！
- en: Conclusions
  id: totrans-83
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: Concluding, we can say that generative AI is a subfield of AI that generates
    new data similar to the train data.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，我们可以说生成型人工智能是人工智能的一个子领域，旨在生成类似于训练数据的新数据。
- en: While, on the one hand, LLMs can generate text based on training data and image
    generation models can generate new images based on the training images, the real
    power of generative AI, at least in the case of images, relies on the combination
    of LLM and models for image generation. This gives us the possibility to create
    images according to prompts as inputs.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 一方面，大型语言模型可以根据训练数据生成文本，而图像生成模型可以基于训练图像生成新图像，生成型人工智能的真正力量，至少在图像的情况下，依赖于大型语言模型和图像生成模型的结合。这让我们可以根据提示输入创建图像。
- en: '***NOTE****: this article has been freely inspired by the Generative AI course
    provided by Google, and some references are taken from it. I suggest* [*taking
    this course*](https://www.cloudskillsboost.google/journeys/118)*, for a better
    understanding of generative AI.*'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '***注意***：本文灵感来自谷歌提供的生成型人工智能课程，部分参考资料来自其中。我建议* [*参加此课程*](https://www.cloudskillsboost.google/journeys/118)*，以更好地理解生成型人工智能。*'
- en: '![](../Images/5079e3af9eda458328cb258c452fb935.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5079e3af9eda458328cb258c452fb935.png)'
- en: Federico Trotta
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 费德里科·特罗塔
- en: Hi, I’m Federico Trotta and I’m a freelance Technical Writer.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 嗨，我是费德里科·特罗塔，我是一名自由职业的技术写作员。
- en: Want to collaborate with me? [Contact me](https://bio.link/federicotrotta).
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 想与我合作吗？ [联系我](https://bio.link/federicotrotta)。
