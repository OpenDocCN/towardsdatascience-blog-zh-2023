- en: A primer on functional PyTorch
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 功能性 PyTorch 入门
- en: 原文：[https://towardsdatascience.com/introduction-to-functional-pytorch-b5bf739e1e6e?source=collection_archive---------1-----------------------#2023-05-07](https://towardsdatascience.com/introduction-to-functional-pytorch-b5bf739e1e6e?source=collection_archive---------1-----------------------#2023-05-07)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/introduction-to-functional-pytorch-b5bf739e1e6e?source=collection_archive---------1-----------------------#2023-05-07](https://towardsdatascience.com/introduction-to-functional-pytorch-b5bf739e1e6e?source=collection_archive---------1-----------------------#2023-05-07)
- en: How to use `write Jax-style PyTorch models`
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何使用 `write Jax-style PyTorch models`
- en: '[](https://mariodagrada.medium.com/?source=post_page-----b5bf739e1e6e--------------------------------)[![Mario
    Dagrada](../Images/32dff27962b941b1efd7a7bdf680fca9.png)](https://mariodagrada.medium.com/?source=post_page-----b5bf739e1e6e--------------------------------)[](https://towardsdatascience.com/?source=post_page-----b5bf739e1e6e--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----b5bf739e1e6e--------------------------------)
    [Mario Dagrada](https://mariodagrada.medium.com/?source=post_page-----b5bf739e1e6e--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://mariodagrada.medium.com/?source=post_page-----b5bf739e1e6e--------------------------------)[![Mario
    Dagrada](../Images/32dff27962b941b1efd7a7bdf680fca9.png)](https://mariodagrada.medium.com/?source=post_page-----b5bf739e1e6e--------------------------------)[](https://towardsdatascience.com/?source=post_page-----b5bf739e1e6e--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----b5bf739e1e6e--------------------------------)
    [Mario Dagrada](https://mariodagrada.medium.com/?source=post_page-----b5bf739e1e6e--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F99ed96040994&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintroduction-to-functional-pytorch-b5bf739e1e6e&user=Mario+Dagrada&userId=99ed96040994&source=post_page-99ed96040994----b5bf739e1e6e---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----b5bf739e1e6e--------------------------------)
    ·6 min read·May 7, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fb5bf739e1e6e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintroduction-to-functional-pytorch-b5bf739e1e6e&user=Mario+Dagrada&userId=99ed96040994&source=-----b5bf739e1e6e---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F99ed96040994&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintroduction-to-functional-pytorch-b5bf739e1e6e&user=Mario+Dagrada&userId=99ed96040994&source=post_page-99ed96040994----b5bf739e1e6e---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----b5bf739e1e6e--------------------------------)
    · 6分钟阅读 · 2023年5月7日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fb5bf739e1e6e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintroduction-to-functional-pytorch-b5bf739e1e6e&user=Mario+Dagrada&userId=99ed96040994&source=-----b5bf739e1e6e---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fb5bf739e1e6e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintroduction-to-functional-pytorch-b5bf739e1e6e&source=-----b5bf739e1e6e---------------------bookmark_footer-----------)![](../Images/363519458d8628ce115a6d939b8f089d.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fb5bf739e1e6e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintroduction-to-functional-pytorch-b5bf739e1e6e&source=-----b5bf739e1e6e---------------------bookmark_footer-----------)![](../Images/363519458d8628ce115a6d939b8f089d.png)'
- en: Photo by [Ricardo Gomez Angel](https://unsplash.com/@rgaleriacom?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 照片由 [Ricardo Gomez Angel](https://unsplash.com/@rgaleriacom?utm_source=medium&utm_medium=referral)
    提供，来源于 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: PyTorch has recently integrated the `torch.func` module into its main codebase
    in the 2.0 release. This module, previously known as `functorch`, enables the
    development of purely functional neural network models in PyTorch with a straightforward
    API. This package is PyTorch’s response to the growing [popularity](https://www.assemblyai.com/blog/why-you-should-or-shouldnt-be-using-jax-in-2023/#:~:text=Since%20Google's%20JAX%20hit%20the,and%20others%20are%20using%20JAX.)
    of [Jax](https://github.com/google/jax), a Python framework for general differentiable
    programming built using a functional programming paradigm from the ground up.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch 最近在 2.0 版本中将 `torch.func` 模块集成到了主代码库中。该模块，之前被称为 `functorch`，使得在 PyTorch
    中以简洁的 API 开发纯函数式神经网络模型成为可能。这个包是 PyTorch 对 [Jax](https://github.com/google/jax)
    的日益[流行](https://www.assemblyai.com/blog/why-you-should-or-shouldnt-be-using-jax-in-2023/#:~:text=Since%20Google's%20JAX%20hit%20the,and%20others%20are%20using%20JAX.)
    的回应，Jax 是一个从底层使用函数式编程范式构建的 Python 通用可微编程框架。
- en: In this post, we will first introduce the basics of `torch.func`, followed by
    a **simple end-to-end example** of using a neural network (NN) model to fit a
    non-linear function. While using an NN for this task is admittedly overkill, it
    works well for illustrative purposes. Additionally, we will discover some of the
    benefits of adopting a functional approach when constructing NN models.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我们将首先介绍 `torch.func` 的基础知识，然后通过一个**简单的端到端示例**来使用神经网络（NN）模型拟合非线性函数。虽然使用
    NN 完成这项任务显然有些过度，但它在说明目的上效果很好。此外，我们还将发现构建 NN 模型时采用函数式方法的一些好处。
- en: Write a functional model
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 编写一个函数式模型
- en: 'Using `torch.func` begins in the same way as standard PyTorch: you need to
    construct a neural network. For simplicity, let us define a very simple one composed
    of an arbitrary number of linear layers and non-linear activation functions. The
    forward pass takes a batch of data points as input, where the model is evaluated.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `torch.func` 的方法与标准 PyTorch 开始时相同：你需要构建一个神经网络。为了简单起见，我们定义一个非常简单的网络，由任意数量的线性层和非线性激活函数组成。前向传播接收一批数据点作为输入，并对模型进行评估。
- en: '[PRE0]'
  id: totrans-13
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Now things get interesting. Recall that `torch.func` allows to build purely
    functional models. But what does in mean in practice?
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 现在事情变得有趣了。请记住，`torch.func` 允许构建纯函数式模型。但这在实践中意味着什么？
- en: 'First of all, one needs to grasp the central concept of functional programming:
    [**pure functions**](https://en.wikipedia.org/wiki/Pure_function). Essentially,
    a pure function has two defining properties:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，需要掌握函数式编程的核心概念：[**纯函数**](https://en.wikipedia.org/wiki/Pure_function)。本质上，纯函数有两个定义特征：
- en: its return values are identical for identical input arguments
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于相同的输入参数，其返回值是相同的
- en: it has no side effects, meaning it does not modify its input arguments in any
    way
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它没有副作用，即不会以任何方式修改其输入参数
- en: 'By this definition, most of the methods of a standard PyTorch module are not
    pure since the parameters are stored within the PyTorch model. In other words,
    a standard PyTorch model is **stateful** rather than stateless, as required by
    the functional paradigm. Consider this code:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 根据这个定义，大多数标准 PyTorch 模块的方法不是纯粹的，因为参数存储在 PyTorch 模型中。换句话说，标准 PyTorch 模型是**有状态的**，而不是无状态的，这符合函数式范式。考虑以下代码：
- en: '[PRE1]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The forward pass of the model has not the same output for identical input arguments
    since the optimizer updated the parameters in place.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 模型的前向传播对相同输入参数的输出不同，因为优化器在原地更新了参数。
- en: 'One way for making a PyTorch module pure would then be to decouple the parameters
    from the model, thus making the model completely **stateless**. This is exactly
    what the `torch.func.functional_call()` routine does:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 使 PyTorch 模块变得纯粹的一种方法是将参数与模型解耦，从而使模型完全**无状态**。这正是 `torch.func.functional_call()`
    例程所做的：
- en: '[PRE2]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Now that we have a purely functional forward pass for our neural network model,
    we can explore how to use it with the composable primitives provided by PyTorch’s
    functional API. This allows us to construct complex models using modular building
    blocks, each with its own functional implementation.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经拥有了纯函数式的前向传播，用于我们的神经网络模型，我们可以探索如何使用 PyTorch 的函数式 API 提供的可组合原语。这使我们能够使用每个具有自身函数实现的模块化构建块来构建复杂模型。
- en: Composable function transforms
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 可组合函数转换
- en: I just showed how to define a purely functional forward pass for our model.
    But how can we define differentiation rules and loss functions with it? We need
    to use the **composable** **function transforms** provided by `torch.func`.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 我刚刚展示了如何为我们的模型定义一个完全功能的前向传递。但是我们如何定义微分规则和损失函数呢？我们需要使用`torch.func` 提供的 **可组合的**
    **函数变换**。
- en: 'Function transforms consist of a set of routines, each of which returns a function
    that can be used to evaluate specific quantities. This kind of function that returns
    another function is known as a *higher-order function*. For example, one can use
    the `grad` primitive to evaluate the gradients with respect to the input data
    `x` as follow:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 函数变换由一组例程组成，每个例程返回一个函数，该函数可以用于评估特定的量。这种返回另一个函数的函数被称为 *高阶函数*。例如，可以使用 `grad` 原语来计算相对于输入数据
    `x` 的梯度，如下所示：
- en: '[PRE3]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Notice that, by default, the `grad` function applies to a single number. One
    can use another function transform called`vmap` to efficiently deal with batches
    of inputs. Notice that `vmap` performs also automatic parallelization when multiple
    CPUs or GPUs are available without any code change.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，默认情况下，`grad` 函数应用于单个数字。可以使用另一个函数变换 `vmap` 来高效地处理输入批次。请注意，`vmap` 在多个 CPU
    或 GPU 可用时也会自动进行并行化，而无需任何代码更改。
- en: One important consequence of all the functions in the `torch.func` module being
    pure is their ability to be arbitrarily composed together (hence the term “composable”
    in their name). This is because a pure function can always be replaced with its
    result without affecting program execution, a direct consequence of the two properties
    mentioned above.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '`torch.func` 模块中所有函数都是纯函数的一个重要后果是它们能够被任意组合（因此它们的名称中有“可组合”一词）。这是因为纯函数总是可以用其结果替代，而不影响程序执行，这是上述两个属性的直接结果。'
- en: 'With this in mind, let’s calculate the second derivative for a batch of input
    data `x`:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这些考虑，让我们计算一批输入数据`x`的二阶导数：
- en: '[PRE4]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'It’s worth noting that the forward pass of the model does not take parameters
    as input. As a result, to compute the gradient with respect to the parameters,
    we need to define an auxiliary `make_functional_fwd` routine with the appropriate
    arguments. In practice, we can achieve this using a closure, as shown below:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，模型的前向传递不接受参数作为输入。因此，为了计算相对于参数的梯度，我们需要定义一个辅助的 `make_functional_fwd` 例程，带有适当的参数。实际上，我们可以使用闭包来实现，如下所示：
- en: '[PRE5]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The `torch.func` module offers many more composable function transforms for
    computing, for example, vector-Jacobian products. [Here](https://pytorch.org/functorch/stable/notebooks/whirlwind_tour.html#why-composable-function-transforms)
    you can find more details.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '`torch.func` 模块提供了许多可组合的函数变换，例如用于计算向量-雅可比积的变换。[这里](https://pytorch.org/functorch/stable/notebooks/whirlwind_tour.html#why-composable-function-transforms)你可以找到更多详细信息。'
- en: Optimization with functional models
  id: totrans-35
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用功能模型进行优化
- en: If you made until here, you might wonder how can you perform gradient-based
    optimization with a functional model. After all, the standard PyTorch optimizers
    works by modifying the model parameters in place which, as I just showed, breaks
    the pure function requirements.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你读到这里，你可能会想知道如何使用功能模型进行基于梯度的优化。毕竟，标准的 PyTorch 优化器通过就地修改模型参数来工作，而正如我刚刚展示的，这破坏了纯函数的要求。
- en: Unfortunately, PyTorch does not natively provide functional optimizers. However,
    one can use the`[torchopt](https://github.com/metaopt/torchopt)` library for this
    purpose.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，PyTorch 本身并未提供功能优化器。然而，可以使用`[torchopt](https://github.com/metaopt/torchopt)`
    库来实现这一目的。
- en: 'For showing how functional optimization works, let’s assume that we want to
    fit a simple function, for example *f(x) = 2 sin(x + 2π)* using some random input
    points in the domain *[0, 2π]*. We can generate some training and test data points
    as follows:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 为了展示功能优化的工作原理，我们假设我们要拟合一个简单的函数，例如 *f(x) = 2 sin(x + 2π)*，使用一些在区间 *[0, 2π]* 内的随机输入点。我们可以生成一些训练和测试数据点，如下所示：
- en: '[PRE6]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Now let’s use `torchopt`and the PyTorch functional API to train our NN to fit
    this function.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们使用`torchopt`和 PyTorch 函数 API 来训练我们的神经网络以拟合这个函数。
- en: '[PRE7]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: As you can see, the optimization loop is pretty similar to standard PyTorch,
    with the crucial difference that the optimizer step now requires the current loss
    and the current parameters values, and evaluates the updated parameters in a fully
    stateless manner. In my opinion, this approach looks much cleaner than the typical
    stateful API of PyTorch!
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所看到的，优化循环与标准的PyTorch非常相似，关键区别在于优化器步骤现在需要当前损失和当前参数值，并以完全无状态的方式评估更新后的参数。在我看来，这种方法看起来比PyTorch典型的有状态API要干净得多！
- en: For the complete code for this blog post, you can look at [this code snippet](https://gist.github.com/madagra/64afe1b56ff5656b2b1acb19cc68f477).
    As you will notice, some details of the implementation (particularly the interaction
    between `torch.func.functional_call` and the `torchopt` optimizer) have not been
    covered in this blog. Feel free to send me a message on [Linkedn](https://www.linkedin.com/in/mariodagrada/)
    if you have any questions.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想获取本博客文章的完整代码，可以查看[此代码片段](https://gist.github.com/madagra/64afe1b56ff5656b2b1acb19cc68f477)。正如你所注意到的那样，一些实现细节（特别是`torch.func.functional_call`与`torchopt`优化器之间的交互）在本博客中没有涉及。如果你有任何问题，请随时在[Linkedn](https://www.linkedin.com/in/mariodagrada/)上给我发消息。
- en: Conclusions
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: Thank you for reading this blog post. The functional API of PyTorch is a powerful
    tool that enables you to write high-performance neural network models and utilize
    composable functions and automatic parallelization and vectorization, similar
    to Jax. However, it is still an experimental feature and should be used with caution.
    Happy coding!
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 感谢阅读本博客文章。PyTorch的函数式API是一个强大的工具，可以帮助你编写高性能的神经网络模型，并利用可组合函数、自动并行化和矢量化，类似于Jax。然而，它仍然是一个实验性的功能，应谨慎使用。祝你编程愉快！
- en: '[1] Concise comparison with Jax: [Tutorial: Writing JAX-like code in PyTorch
    with functorch — Simone Scardapane (sscardapane.it)](https://www.sscardapane.it/tutorials/functorch/)'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] 与Jax的简洁比较：[使用PyTorch和functorch编写类似JAX的代码教程 — Simone Scardapane (sscardapane.it)](https://www.sscardapane.it/tutorials/functorch/)'
- en: '[2] A bit old but very well-explained tutorial: [Working with FuncTorch: An
    Introduction | functorch-examples — Weights & Biases (wandb.ai)](https://wandb.ai/functorch-examples/functorch-examples/reports/Working-with-FuncTorch-An-Introduction--VmlldzoxNzMxNDI1)'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] 虽然有点老但解释得非常好的教程：[使用FuncTorch工作：介绍 | functorch-examples — Weights & Biases
    (wandb.ai)](https://wandb.ai/functorch-examples/functorch-examples/reports/Working-with-FuncTorch-An-Introduction--VmlldzoxNzMxNDI1)'
