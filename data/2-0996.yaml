- en: Google Pub/Sub to BigQuery the Simple Way
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 简单的 Google Pub/Sub 到 BigQuery 方法
- en: 原文：[https://towardsdatascience.com/google-pub-sub-to-bigquery-the-simple-way-de116234fb87](https://towardsdatascience.com/google-pub-sub-to-bigquery-the-simple-way-de116234fb87)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/google-pub-sub-to-bigquery-the-simple-way-de116234fb87](https://towardsdatascience.com/google-pub-sub-to-bigquery-the-simple-way-de116234fb87)
- en: A hands-on guide to implementing BigQuery Subscriptions in Pub/Sub for simple
    message and streaming ingestion
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实施 BigQuery 订阅的动手指南，适用于 Pub/Sub 简单消息和流式数据摄取
- en: '[](https://jim-barlow.medium.com/?source=post_page-----de116234fb87--------------------------------)[![Jim
    Barlow](../Images/1494580717cb92defb17328e4bae1b13.png)](https://jim-barlow.medium.com/?source=post_page-----de116234fb87--------------------------------)[](https://towardsdatascience.com/?source=post_page-----de116234fb87--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----de116234fb87--------------------------------)
    [Jim Barlow](https://jim-barlow.medium.com/?source=post_page-----de116234fb87--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://jim-barlow.medium.com/?source=post_page-----de116234fb87--------------------------------)[![Jim
    Barlow](../Images/1494580717cb92defb17328e4bae1b13.png)](https://jim-barlow.medium.com/?source=post_page-----de116234fb87--------------------------------)[](https://towardsdatascience.com/?source=post_page-----de116234fb87--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----de116234fb87--------------------------------)
    [Jim Barlow](https://jim-barlow.medium.com/?source=post_page-----de116234fb87--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----de116234fb87--------------------------------)
    ·8 min read·Sep 21, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----de116234fb87--------------------------------)
    ·阅读时长 8 分钟·2023年9月21日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/b31e6bb2ff00e2b71bba4bbff6d010fc.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b31e6bb2ff00e2b71bba4bbff6d010fc.png)'
- en: 'Google’s latest planet-scale data warehouse subscription-based streaming ingestion
    water-borne military capability: BigSub. In this case, the Pub never made it to
    General Availability, so you will have to get your pints elsewhere. Photo by [Thomas
    Haas](https://unsplash.com/@thomashaas?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Google 最新的行星规模数据仓库订阅基础的流式摄取水下军事能力：BigSub。在这种情况下，Pub 从未达到正式发布状态，因此你必须去其他地方寻找你的啤酒。照片由
    [Thomas Haas](https://unsplash.com/@thomashaas?utm_source=medium&utm_medium=referral)
    提供，来源于 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: Motivation
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 动机
- en: I have encountered many situations in the past where I wanted to get Pub/Sub
    messages into a BigQuery table, but I never managed to find a particularly simple
    way of doing this.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 我过去遇到过许多情况，我想将 Pub/Sub 消息导入 BigQuery 表中，但我从未找到一种特别简单的方法来实现这一点。
- en: You could set up a [dataflow pipeline](https://cloud.google.com/dataflow/docs/tutorials/dataflow-stream-to-bigquery),
    but this requires additional infrastructure to understand, configure, manage and
    debug. Plus Dataflow (which is a managed Apache Beam service) is designed for
    high-throughput streaming, so always seemed like overkill for a simple message
    logging or monitoring system.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以设置一个 [dataflow pipeline](https://cloud.google.com/dataflow/docs/tutorials/dataflow-stream-to-bigquery)，但这需要额外的基础设施来理解、配置、管理和调试。而且
    Dataflow（作为一种托管的 Apache Beam 服务）设计用于高吞吐量流式处理，因此对于简单的消息日志记录或监控系统来说总是显得过于复杂。
- en: And it’s Java. But Python 😀! And Java… 😫!
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 而且它是 Java。可是 Python 😀！而 Java… 😫！
- en: '[PRE0]'
  id: totrans-12
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '*Sorry, I still get flashbacks from my first attempts to learn to code (last
    century) in Java. Please do not attempt to use that code snippet … step away from
    the code snippet.*'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '*对不起，我仍然会对我第一次尝试在 Java 中学习编码（上个世纪）时的经历感到闪回。请不要尝试使用那个代码片段……远离代码片段。*'
- en: I then stumbled upon [this](https://medium.com/google-cloud/streaming-from-google-cloud-pub-sub-to-bigquery-without-the-middlemen-327ef24f4d15),
    which — although promising simplicity — seems to be even more complicated than
    the previous method (Debezium wtf?)!
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我偶然发现了 [这个](https://medium.com/google-cloud/streaming-from-google-cloud-pub-sub-to-bigquery-without-the-middlemen-327ef24f4d15)，虽然它承诺简单性，但似乎比之前的方法（Debezium
    wtf？）还要复杂！
- en: It’s also possible to deploy a lightweight Cloud Function to trigger on receipt
    of a Pub/Sub message and stream or load this into BigQuery, but this still seemed
    a little too complex for something which felt like it should and could have been
    native functionality.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 也可以部署一个轻量级的 Cloud Function 以在接收到 Pub/Sub 消息时触发，并将其流式传输或加载到 BigQuery 中，但这似乎仍然有些复杂，因为这本应是原生功能。
- en: And now it is!
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 现在它已经实现了！
- en: The kind folks at Google Cloud [announced](https://cloud.google.com/blog/products/data-analytics/pub-sub-launches-direct-path-to-bigquery-for-streaming-analytics)
    a direct connection from Pub/Sub to BigQuery a while ago, awesome! However, having
    tried (and failed) to quickly set up a test a couple of times, I finally had a
    real-life use-case which required me to get it working for a client.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 谷歌云的友善团队[宣布](https://cloud.google.com/blog/products/data-analytics/pub-sub-launches-direct-path-to-bigquery-for-streaming-analytics)了一段时间前从
    Pub/Sub 到 BigQuery 的直接连接，太棒了！然而，尝试（并且失败）快速设置测试几次后，我终于有了一个实际的用例，需要为一个客户使其正常工作。
- en: It turns out that there are a couple of nuances, so this article aims to help
    you get this up and running as quickly as possible.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 结果发现有几个细微之处，所以这篇文章旨在帮助你尽快地让这一切运行起来。
- en: Situation
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 现状
- en: 'Pub/Sub is an incredibly useful, powerful and scaleable service in the Google
    Cloud ecosystem, with two core use-cases: streaming and messaging. I will let
    Google explain this themselves (disappointing spoiler alert: it has nothing to
    do with a Public House located on a Submarine.):'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: Pub/Sub 是 Google Cloud 生态系统中一个极其有用、强大且可扩展的服务，具有两个核心用例：流式传输和消息传递。我将让 Google 自行解释（令人失望的剧透：这与位于潜艇上的公共房屋无关）。
- en: Pub/Sub is used for streaming analytics and data integration pipelines to ingest
    and distribute data. It’s equally effective as a messaging-oriented middleware
    for service integration or as a queue to parallelize tasks.
  id: totrans-21
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: Pub/Sub 用于流式分析和数据集成管道，以摄取和分发数据。它同样有效地作为服务集成的消息中介或作为队列来并行化任务。
- en: ''
  id: totrans-22
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Pub/Sub enables you to create systems of event producers and consumers, called
    **publishers** and **subscribers**. Publishers communicate with subscribers asynchronously
    by broadcasting events, rather than by synchronous remote procedure calls (RPCs).
  id: totrans-23
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: Pub/Sub 使你能够创建事件生产者和消费者系统，称为**发布者**和**订阅者**。发布者通过广播事件而不是通过同步远程过程调用（RPC）与订阅者异步通信。
- en: Messages are published to a topic, and subscribers to the topic can receive
    the message and take action accordingly. The Pub(lisher) knows nothing about the
    Sub(scribers), but when the messages are published, the subscribers can then take
    actions based on the message contents.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 消息被发布到一个主题，主题的订阅者可以接收消息并相应地采取行动。发布者对订阅者一无所知，但当消息被发布时，订阅者可以根据消息内容采取行动。
- en: Client libraries or [notifications on Cloud Storage Buckets](https://cloud.google.com/storage/docs/pubsub-notifications)
    make it simple to publish messages containing configurable metadata, and Pub/Sub
    gets those messages to other Google Cloud destinations to [trigger Cloud Functions](https://cloud.google.com/functions/docs/calling/pubsub)
    or all manner of different actions, limited only by your imagination.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 客户端库或[Cloud Storage Buckets 上的通知](https://cloud.google.com/storage/docs/pubsub-notifications)使发布包含可配置元数据的消息变得简单，而
    Pub/Sub 会将这些消息发送到其他 Google Cloud 目的地以[触发 Cloud Functions](https://cloud.google.com/functions/docs/calling/pubsub)或各种不同的操作，只有你的想象力限制了这些操作。
- en: And now we can get this data natively into BigQuery (apparently trivially),
    so I jumped at the opportunity to get this working in minutes! Except it turned
    out to not be that simple. But I got it done, so I wanted to write this article
    to help anybody else who needs to get this set up with minimal fuss.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以将这些数据原生地导入 BigQuery（显然很简单），所以我抓住机会在几分钟内完成这个工作！但结果证明并没有那么简单。不过我完成了这个任务，所以我想写这篇文章来帮助其他需要快速设置的人，尽量减少麻烦。
- en: Solution
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 解决方案
- en: So where to start? Let’s start with the [docs](https://cloud.google.com/pubsub/docs/bigquery).
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 那么从哪里开始呢？我们从[文档](https://cloud.google.com/pubsub/docs/bigquery)开始吧。
- en: A BigQuery subscription writes messages to an existing BigQuery table as they
    are received. You’re not required to configure a subscriber client separately.
    Use the Google Cloud console, the Google Cloud CLI, the client libraries, or the
    Pub/Sub API to create, update, list, detach, or delete a BigQuery subscription.
  id: totrans-29
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: BigQuery 订阅在接收到消息时会将其写入现有的 BigQuery 表。你不需要单独配置订阅客户端。使用 Google Cloud 控制台、Google
    Cloud CLI、客户端库或 Pub/Sub API 来创建、更新、列出、分离或删除 BigQuery 订阅。
- en: Sweet. Let’s go.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 很好。我们开始吧。
- en: 'As an alternative for simple data ingestion pipelines that often use Dataflow
    to write to BigQuery, the BigQuery subscription has the following advantages:'
  id: totrans-31
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 作为简单数据摄取管道的替代方案，这些管道通常使用 Dataflow 写入 BigQuery，BigQuery 订阅具有以下优点：
- en: ''
  id: totrans-32
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**Simple deployment.** You can set up a BigQuery subscription through a single
    workflow in the console, Google Cloud CLI, client library, or Pub/Sub API.'
  id: totrans-33
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**简单部署。** 你可以通过控制台、Google Cloud CLI、客户端库或 Pub/Sub API 通过一个工作流设置 BigQuery 订阅。'
- en: ''
  id: totrans-34
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**Offers low costs.** Removes the additional cost and latency of similar Pub/Sub
    pipelines that include Dataflow jobs. This cost optimization is useful for messaging
    systems that do not require additional processing before storage.'
  id: totrans-35
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**提供低成本。** 除去了类似 Pub/Sub 管道中包含 Dataflow 作业的额外成本和延迟。这种成本优化对那些在存储前不需要额外处理的消息系统特别有用。'
- en: ''
  id: totrans-36
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**Minimizes monitoring.** BigQuery subscriptions are part of the multi-tenant
    Pub/Sub service and do not require you to run separate monitoring jobs.'
  id: totrans-37
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**减少监控。** BigQuery 订阅是多租户 Pub/Sub 服务的一部分，无需你运行单独的监控作业。'
- en: Sound’s good! Keep going… to the [schema section](https://cloud.google.com/pubsub/docs/bigquery#properties_subscription).
    I must confess reading this a few times and being a little baffled by what I actually
    needed to do.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 听起来不错！继续前往[模式部分](https://cloud.google.com/pubsub/docs/bigquery#properties_subscription)。我必须承认，阅读几遍后，我对需要做的事情感到有些困惑。
- en: 'Managing schemas can be quite complex in normal environments, and it seemed
    that in this case I would need to create a BigQuery table with a schema which
    mirrored the inbound JSON exactly:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在普通环境中，管理模式可能相当复杂，在这种情况下，我需要创建一个 BigQuery 表，其模式完全镜像输入的 JSON 数据：
- en: '**Use topic schema.** This option lets Pub/Sub use the [schema of the Pub/Sub
    topic](https://cloud.google.com/pubsub/docs/admin#schemas) to which the subscription
    is attached. In addition, Pub/Sub writes the fields in messages to the corresponding
    columns in the BigQuery table. When you use this option, remember to check the
    following additional requirements:'
  id: totrans-40
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**使用主题模式。** 这个选项允许 Pub/Sub 使用[Pub/Sub 主题的模式](https://cloud.google.com/pubsub/docs/admin#schemas)，即订阅所附加的主题。此外，Pub/Sub
    将消息中的字段写入 BigQuery 表中的相应列。当你使用此选项时，请记得检查以下额外要求：'
- en: ''
  id: totrans-41
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The fields in the topic schema and the BigQuery schema must have the same names
    and their types must be compatible with each other.
  id: totrans-42
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 主题模式中的字段和 BigQuery 模式中的字段必须具有相同的名称，并且它们的类型必须兼容。
- en: Oh shit. Real-life inbound data can be pretty complex. It can be nested, containing
    arrays and potentially hundreds of fields. JSON and BigQuery structures map pretty
    cleanly (array = `ARRAY`, object = `STRUCT`) but it is not a simple task to generate
    an empty BigQuery table which maps exactly.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 哎呀。实际的输入数据可能非常复杂。它可能是嵌套的，包含数组并可能有数百个字段。JSON 和 BigQuery 结构映射得相当干净（数组 = `ARRAY`，对象
    = `STRUCT`），但生成一个完全匹配的空 BigQuery 表并不是一件简单的事。
- en: But there is an alternative.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 但也有另一种选择。
- en: If you do not select the **Use topic schema** option, ensure that the BigQuery
    table has a column called `data` of type `BYTES` or `STRING`. Pub/Sub writes the
    message to this BigQuery column.
  id: totrans-45
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 如果你没有选择 **使用主题模式** 选项，请确保 BigQuery 表中有一个名为 `data` 的 `BYTES` 或 `STRING` 类型的列。Pub/Sub
    将消息写入这个 BigQuery 列。
- en: OK, this seems like it might be a viable approach. Let’s just get the data into
    BigQuery and deal with decoding the `JSON` later. In fact, I tend to prefer this
    architecture as any non-compliant data will still be received and we can try and
    figure out how to decode it downstream.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，这看起来可能是一个可行的方法。我们先将数据导入 BigQuery，然后再处理解码 `JSON` 的问题。实际上，我倾向于这种架构，因为任何不合规的数据仍会被接收，我们可以尝试在下游解码它。
- en: The same goes for evolving schemas. The beauty of ingesting raw `JSON` into
    BigQuery is that — as it is simply text — it is an extremely robust ingestion
    point. We can then use the recently-expanded [JSON functions in BigQuery](https://cloud.google.com/bigquery/docs/reference/standard-sql/json_functions)
    to perform all sorts of magic.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 演变模式也是如此。将原始 `JSON` 导入 BigQuery 的好处在于——由于它仅仅是文本——这是一个极其稳健的数据导入点。然后我们可以使用最近扩展的[BigQuery
    JSON 函数](https://cloud.google.com/bigquery/docs/reference/standard-sql/json_functions)来执行各种操作。
- en: 'Downstream transformations are also more robust: attempting to select a column
    which does not exist in normal SQL will cause a query to error, however attempting
    to extract the equivalent non-existent field from a JSON object will simply return
    a NULL. This, when properly handled, results in a more robust flow.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 下游转换也更为稳健：尝试选择在普通 SQL 中不存在的列会导致查询错误，而尝试从 JSON 对象中提取等效的不存在字段将简单地返回 NULL。这样处理后，会得到一个更为稳健的流程。
- en: '**Inbound Table Creation**'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '**输入表创建**'
- en: So, on to creating the inbound table into which we will ingest the raw JSON
    data.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来是创建用于接收原始 JSON 数据的输入表。
- en: You can create tables in the UI, but it’s actually very simple to do this via
    [DDL](https://cloud.google.com/bigquery/docs/reference/standard-sql/data-definition-language)
    in the BigQuery console (and quicker for me to type than taking screenshots 😀),
    and then when you inevitably get it wrong the first time it’s a lot less frustrating
    to just change a parameter (or add a missing character) and hit run.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在UI中创建表格，但实际上通过[DDL](https://cloud.google.com/bigquery/docs/reference/standard-sql/data-definition-language)在BigQuery控制台中完成这一操作非常简单（而且对我来说比截图更快
    😀），然后当你第一次做错时，改变一个参数（或添加缺失的字符）并点击运行就会少很多挫败感。
- en: The table schema is actually in the section of the docs which explains how to
    [write metadata](https://cloud.google.com/pubsub/docs/create-bigquery-subscription#write-metadata),
    which is almost always a good idea for additional context and debugging support.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 表格模式实际上在文档的[编写元数据](https://cloud.google.com/pubsub/docs/create-bigquery-subscription#write-metadata)部分中解释，这几乎总是增加上下文和调试支持的好主意。
- en: 'The DDL to create the table (with all possible fields), is then:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 创建表格的DDL（包含所有可能的字段）如下：
- en: '[PRE1]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '**Table Partitioning**'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '**表格分区**'
- en: Partitioning is nearly always a good idea (and sometimes an essential approach)
    for tables in BigQuery as it physically segregates the data into different partitions,
    which can then be queried directly without requiring expensive full-table-scans.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在BigQuery中，分区几乎总是一个好主意（有时是必不可少的方法），因为它将数据物理上隔离到不同的分区中，这样可以直接查询这些分区，而无需昂贵的全表扫描。
- en: There are a number of different [options](https://cloud.google.com/bigquery/docs/reference/standard-sql/data-definition-language#partition_expression)
    for table partitioning, however it should be noted that there is currently a [limit
    of 4000 partitions](https://cloud.google.com/bigquery/docs/partitioned-tables#ingestion_time)
    so daily partitioning will max out after nearly 11 years but hourly partitioning
    will max out after less than 24 weeks.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 表格分区有许多不同的[选项](https://cloud.google.com/bigquery/docs/reference/standard-sql/data-definition-language#partition_expression)，但需要注意的是，目前有一个[4000个分区的限制](https://cloud.google.com/bigquery/docs/partitioned-tables#ingestion_time)，因此每日分区将在近11年后达到上限，但每小时分区将在不到24周后达到上限。
- en: One side note here, sometimes it *is* necessary to use hourly partitioning due
    to the volume of streaming data (e.g. we have implemented this to consume [EventStream
    data from Tealium](https://tealium.com/integrations/google-cloud-pub-sub/)). In
    this case it was necessary to deploy additional architecture to back up historic
    data to Google Cloud Storage and set a [partition expiration](https://cloud.google.com/bigquery/docs/managing-partitioned-tables#partition-expiration)
    on the inbound table.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一个附注，有时由于流数据的量，*确实*需要使用每小时分区（例如，我们已经实现了从[Tealium的EventStream数据](https://tealium.com/integrations/google-cloud-pub-sub/)）。在这种情况下，需要部署额外的架构，将历史数据备份到Google
    Cloud Storage，并设置[分区过期](https://cloud.google.com/bigquery/docs/managing-partitioned-tables#partition-expiration)。
- en: 'One more side note: for brevity I am omitting details on the testing steps
    we took to get this process running reliably. Such is the beauty of learning from
    the experience of others: you can skip the tedious parts and go directly to the
    answer!'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个附注：为了简洁起见，我省略了我们为使这个过程可靠运行而采取的测试步骤。这就是从他人经验中学习的美妙之处：你可以跳过繁琐的部分，直接得到答案！
- en: '**Setting up Pub/Sub Topic**'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '**设置Pub/Sub主题**'
- en: Now for the actual setup of the Pub/Sub topic and the BigQuery Subscription,
    which should be a pretty quick process. Head to [Pub/Sub in the Cloud Console](https://console.cloud.google.com/cloudpubsub/topic/)
    and click the `[+] CREATE TOPIC` button. You have quite a lot of naming freedom
    here, so I tend to give it exactly the same name as the destination `dataset_id`
    without the `project_id` (i.e. `dataset_id.table_name`). Uncheck the `Add a default
    subscription` box (as we are going to create a BigQuery Subscription next) and
    click `CREATE`.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是实际设置Pub/Sub主题和BigQuery订阅的部分，这应该是一个相当快速的过程。前往[Cloud Console中的Pub/Sub](https://console.cloud.google.com/cloudpubsub/topic/)并点击`[+]
    CREATE TOPIC`按钮。你在这里有相当大的命名自由度，所以我倾向于给它和目标`dataset_id`完全相同的名称，不带`project_id`（即`dataset_id.table_name`）。取消勾选`Add
    a default subscription`框（因为我们接下来要创建一个BigQuery订阅），然后点击`CREATE`。
- en: '**Setting up Pub/Sub Subscription**'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '**设置Pub/Sub订阅**'
- en: Once the topic is set up, now onto the Subscription.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦主题设置好，就可以开始创建订阅了。
- en: Click on the topic you have just created, and you should see a bottom section
    with a number of tabs, of which `SUBSCRIPTIONS` is selected by default. Click
    `CREATE SUBSCRIPTIONS` in this section and name the Subscription (you can actually
    name the subscription identically as the Topic, with which I have never experienced
    problems).
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 点击你刚刚创建的主题，你应该看到一个底部区域，里面有多个选项卡，默认情况下选中的是`SUBSCRIPTIONS`。在这个区域点击`CREATE SUBSCRIPTIONS`并为订阅命名（实际上你可以将订阅名称与主题名称相同，我从未遇到过问题）。
- en: In the `Delivery Type` section, check the radio button next to `Write to BigQuery`,
    and select the `project_id`, `dataset_id` and `table_name` of the table created
    previously. Check the `Write Metadata` check box (we like metadata) and — in the
    interests of simplicity — leave the other options as the default settings. It
    does recommend that BigQuery subscriptions should enable dead lettering, but this
    requires an additional topic so we do not in this case. Also since we are not
    depending on using the topic schema we are much less likely to encounter failed
    messages.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在`Delivery Type`部分，选中`Write to BigQuery`旁边的单选按钮，选择之前创建的表的`project_id`、`dataset_id`和`table_name`。勾选`Write
    Metadata`复选框（我们喜欢元数据），并且—为了简单起见—将其他选项保留为默认设置。虽然建议 BigQuery 订阅应该启用死信队列，但这需要额外的主题，所以在这种情况下我们不启用。同时，由于我们不依赖使用主题模式，我们遇到失败消息的可能性也大大降低。
- en: Done!
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 完成！
- en: Oh, wait. Hang on.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 哦，等等。稍等一下。
- en: '**Permissions**'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '**权限**'
- en: In all likelihood you will now see an error message preventing you from successfully
    creating the subscription.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 你很可能会看到一个错误信息，阻止你成功创建订阅。
- en: '[PRE2]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: This is actually pretty simple to deal with, but you have a couple of options.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，这很简单，但你有几个选择。
- en: The most secure option, compliant with the principle of [least privilege](https://cloud.google.com/iam/docs/using-iam-securely#least_privilege),
    is the creation of a [custom role](https://console.cloud.google.com/iam-admin/roles)
    with the precise permissions required (`bigquery.tables.get`, `bigquery.tables.updateData`)
    and then assigning this role to the service account.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 最安全的选项，符合[最小权限](https://cloud.google.com/iam/docs/using-iam-securely#least_privilege)原则，是创建一个具有精确权限（`bigquery.tables.get`、`bigquery.tables.updateData`）的[自定义角色](https://console.cloud.google.com/iam-admin/roles)，然后将此角色分配给服务账户。
- en: However, since this is a Google Cloud Service Account which is never used for
    anything else on my project I am happy to give it a simpler, more permissive role.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，由于这是一个在我的项目中从未用于其他用途的 Google Cloud 服务账户，我愿意给予它一个更简单、更宽松的角色。
- en: Either way, copy the service account email in the message, go to the inbound
    table, click `SHARE` and `ADD PRINCIPAL`. Copy the service account email into
    the `New Principals` field. Add either the `BigQuery Data Editor` role or the
    optional custom role created to the assigned roles and `SAVE`.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 无论如何，复制消息中的服务账户邮箱，进入入站表，点击`SHARE`和`ADD PRINCIPAL`。将服务账户邮箱复制到`New Principals`字段中。将`BigQuery
    Data Editor`角色或创建的自定义角色添加到分配角色中，然后`SAVE`。
- en: Hopefully this will complete successfully and you will have a fully functional,
    simple, serverless data ingestion flow from any arbitrary Pub/Sub topic.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 希望这将顺利完成，你将拥有一个完全功能、简单的无服务器数据摄取流程，来自任何任意的 Pub/Sub 主题。
- en: Well done!
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 做得好！
- en: '**Next Steps**'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '**下一步**'
- en: Now that your ingestion flow is operating properly, I need to go and do some
    actual work so I will leave you to test it with some real Pub/Sub messages.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你的数据摄取流程正常运行，我需要去做一些实际工作，所以我将留下来让你用一些真实的 Pub/Sub 消息进行测试。
- en: One of the easiest things to use is to set up a Pub/Sub Notification on a BigQuery
    Scheduled Query and then run it a few times. You will see the table populating
    with a row for each run, with the `JSON` data in the `data` and `attributes` columns
    containing all of the useful data about each run.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 使用最简单的方法之一是设置 BigQuery 定时查询的 Pub/Sub 通知，然后运行几次。你将看到每次运行时表格中都会增加一行，其中的`JSON`数据在`data`和`attributes`列中包含了每次运行的所有有用数据。
- en: My next article will explain the optimal setup to decode the `JSON` payloads
    into BigQuery data types, so that you can start using and visualising the data.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 我的下一篇文章将解释将`JSON`有效负载解码为 BigQuery 数据类型的最佳设置，以便你可以开始使用和可视化数据。
- en: Remember to follow me if you want to receive this second instalment (hopefully
    next week)!
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想接收第二部分（希望下周发布），记得关注我！
