- en: Using Fourier Transform of Vector Representations Derived from BERT Embeddings
    for Semantic Closeness Evaluation
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用从 BERT 嵌入中衍生的向量表示的傅里叶变换进行语义相似度评估
- en: 原文：[https://towardsdatascience.com/using-fourier-transform-of-vector-representations-derived-from-bert-embeddings-for-semantic-9d91a7d4839c?source=collection_archive---------1-----------------------#2023-01-14](https://towardsdatascience.com/using-fourier-transform-of-vector-representations-derived-from-bert-embeddings-for-semantic-9d91a7d4839c?source=collection_archive---------1-----------------------#2023-01-14)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/using-fourier-transform-of-vector-representations-derived-from-bert-embeddings-for-semantic-9d91a7d4839c?source=collection_archive---------1-----------------------#2023-01-14](https://towardsdatascience.com/using-fourier-transform-of-vector-representations-derived-from-bert-embeddings-for-semantic-9d91a7d4839c?source=collection_archive---------1-----------------------#2023-01-14)
- en: '![](../Images/7727543fc629cc19f66d6aa3a5236e98.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7727543fc629cc19f66d6aa3a5236e98.png)'
- en: Photo by Igor Shabalin, with permission
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 照片由 Igor Shabalin 提供，已获许可
- en: Exploring the mutual influence of words in a sentence by evaluating different
    representations of BERT embeddings
  id: totrans-4
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 探索通过评估 BERT 嵌入的不同表示来了解句子中词语的相互影响
- en: '[](https://jxireal.medium.com/?source=post_page-----9d91a7d4839c--------------------------------)[![Yuli
    Vasiliev](../Images/7a5fbd7fc0d48c87f0163e2ec4622f45.png)](https://jxireal.medium.com/?source=post_page-----9d91a7d4839c--------------------------------)[](https://towardsdatascience.com/?source=post_page-----9d91a7d4839c--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----9d91a7d4839c--------------------------------)
    [Yuli Vasiliev](https://jxireal.medium.com/?source=post_page-----9d91a7d4839c--------------------------------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://jxireal.medium.com/?source=post_page-----9d91a7d4839c--------------------------------)[![Yuli
    Vasiliev](../Images/7a5fbd7fc0d48c87f0163e2ec4622f45.png)](https://jxireal.medium.com/?source=post_page-----9d91a7d4839c--------------------------------)[](https://towardsdatascience.com/?source=post_page-----9d91a7d4839c--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----9d91a7d4839c--------------------------------)
    [Yuli Vasiliev](https://jxireal.medium.com/?source=post_page-----9d91a7d4839c--------------------------------)'
- en: ·
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F83cfb869ab36&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fusing-fourier-transform-of-vector-representations-derived-from-bert-embeddings-for-semantic-9d91a7d4839c&user=Yuli+Vasiliev&userId=83cfb869ab36&source=post_page-83cfb869ab36----9d91a7d4839c---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----9d91a7d4839c--------------------------------)
    ·5 min read·Jan 14, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F9d91a7d4839c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fusing-fourier-transform-of-vector-representations-derived-from-bert-embeddings-for-semantic-9d91a7d4839c&user=Yuli+Vasiliev&userId=83cfb869ab36&source=-----9d91a7d4839c---------------------clap_footer-----------)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F83cfb869ab36&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fusing-fourier-transform-of-vector-representations-derived-from-bert-embeddings-for-semantic-9d91a7d4839c&user=Yuli+Vasiliev&userId=83cfb869ab36&source=post_page-83cfb869ab36----9d91a7d4839c---------------------post_header-----------)
    发布于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----9d91a7d4839c--------------------------------)
    ·5 min read·2023年1月14日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F9d91a7d4839c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fusing-fourier-transform-of-vector-representations-derived-from-bert-embeddings-for-semantic-9d91a7d4839c&user=Yuli+Vasiliev&userId=83cfb869ab36&source=-----9d91a7d4839c---------------------clap_footer-----------)'
- en: --
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F9d91a7d4839c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fusing-fourier-transform-of-vector-representations-derived-from-bert-embeddings-for-semantic-9d91a7d4839c&source=-----9d91a7d4839c---------------------bookmark_footer-----------)'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F9d91a7d4839c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fusing-fourier-transform-of-vector-representations-derived-from-bert-embeddings-for-semantic-9d91a7d4839c&source=-----9d91a7d4839c---------------------bookmark_footer-----------)'
- en: BERT embedding is the thing that provides great opportunities when it comes
    to programmatic ways of extracting meaning from text. It seems everything we (as
    well as machines) need to make sense of the text is hidden in those numbers. It’s
    just a matter of properly manipulating those numbers. I discussed this concept
    in my recent post [Discovering Trends in BERT Embeddings of Different Levels for
    the Task of Semantic Context Determining](https://medium.com/towards-data-science/discovering-trends-in-bert-embeddings-of-different-levels-for-the-task-of-semantic-context-268733fdb17e).
    This article continues this discussion, looking at whether the vector representations
    obtained by applying Fourier transform to BERT embeddings can also be useful in
    NLP tasks of meaning extraction.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: BERT 嵌入在程序化提取文本意义方面提供了极大的机会。似乎我们（以及机器）理解文本所需的一切都隐藏在这些数字中。关键在于如何正确地操作这些数字。我在我最近的帖子中讨论了这一概念
    [发现 BERT 嵌入在不同层次中的趋势，用于语义上下文确定任务](https://medium.com/towards-data-science/discovering-trends-in-bert-embeddings-of-different-levels-for-the-task-of-semantic-context-268733fdb17e)。本文继续讨论这一主题，探讨通过对
    BERT 嵌入应用傅里叶变换得到的向量表示是否在 NLP 任务中的意义提取方面也有用。
- en: Hypothesis
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 假设
- en: As you probably know from physics, the Fourier transform allows us to understand
    the frequencies inside a signal. Does a vector representing a word embedding look
    like a signal? That is, could the knowledge of the frequency domain derived from
    the Fourier transform be any useful when processing BERT embeddings for discovering
    semantic closeness? In simple terms, does the Fourier transform make sense when
    analyzing BERT embeddings? Let’s check it out.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你可能从物理学中知道的那样，傅里叶变换使我们能够理解信号中的频率。表示单词嵌入的向量是否看起来像信号？也就是说，从傅里叶变换得出的频域知识在处理 BERT
    嵌入以发现语义接近性时是否有用？简单来说，傅里叶变换在分析 BERT 嵌入时是否有意义？让我们来检查一下。
- en: Experiment
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实验
- en: 'The example discussed in the rest of this article assumes you have the model
    defined as discussed in the example in the [previous post](https://medium.com/towards-data-science/discovering-trends-in-bert-embeddings-of-different-levels-for-the-task-of-semantic-context-268733fdb17e).
    We’ll also need the representations derived from the sample sentences as discussed
    in that [previous post](https://medium.com/towards-data-science/discovering-trends-in-bert-embeddings-of-different-levels-for-the-task-of-semantic-context-268733fdb17e).
    Note, however, that this article uses a set of samples that are a bit different
    from those used in the above post. So, before creating the representations, you’ll
    need to define the following samples:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 本文剩余部分讨论的例子假设你已经按照 [之前的帖子](https://medium.com/towards-data-science/discovering-trends-in-bert-embeddings-of-different-levels-for-the-task-of-semantic-context-268733fdb17e)
    中的示例定义了模型。我们还需要从该 [之前的帖子](https://medium.com/towards-data-science/discovering-trends-in-bert-embeddings-of-different-levels-for-the-task-of-semantic-context-268733fdb17e)
    中讨论的样本句子中获得的表示。请注意，本文使用了一组与上述帖子中使用的样本稍有不同的样本。因此，在创建表示之前，你需要定义以下样本：
- en: '[PRE0]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: As you can see, all the sentences have the same set of the direct object modifiers,
    while the direct objects are different in each case. The purpose of our experiment
    is to check out how the semantic closeness of direct objects affects the proximity
    of modifiers.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所看到的，所有句子都有相同的直接宾语修饰词集合，而直接宾语在每个句子中都是不同的。我们实验的目的是检查直接宾语的语义接近性如何影响修饰词的接近程度。
- en: The process of tokenizing and generating the hidden states for sample sentences
    was covered in the above post. So we won’t cover this process again here.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的帖子中已经介绍了对样本句子进行分词和生成隐藏状态的过程。因此我们在这里不会重复这个过程。
- en: 'Let’s focus on the word modifiers (“a/very/good”) of the direct object (“apple/orange/adventure”)
    in each sentence. To use right indexing when obtaining their embeddings, let’s
    first check out the tokens in our sentences:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们专注于每个句子中直接宾语（“apple/orange/adventure”）的修饰词（“a/very/good”）。为了在获取其嵌入时使用正确的索引，我们首先检查一下句子中的词元：
- en: '[PRE1]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '[PRE2]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Since we’re interested in the modifiers of the direct object (in this particular
    example, we have three modifiers: *a, very, good*.), we need the tokens with indexes:
    *3,4,5*. So we need to shift indexing by 3\. Below, we obtain the contextual embedding
    in each sentence for each modifier. We’ll hold modifier embeddings in a list defined
    for each sentence:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们对直接宾语的修饰词（在这个具体的例子中，我们有三个修饰词：*a, very, good*）感兴趣，我们需要索引为 *3,4,5* 的词元。因此，我们需要将索引偏移
    3。以下，我们将为每个修饰词在每个句子中获取上下文嵌入。我们将把修饰词嵌入保存在为每个句子定义的列表中：
- en: '[PRE3]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Let’s now look at how the semantic closeness of the direct objects in different
    sentences affects the closeness of their respective modifiers.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们来探讨不同句子中直接宾语的语义相似性如何影响各自修饰语的相似性。
- en: '[PRE4]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '[PRE5]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '[PRE6]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'In the above output, we can see that the contextual embeddings of Apple and
    Orange modifiers show a high level of closeness. This is quite understandable
    because the direct objects themselves: Apple and Orange are very close.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述输出中，我们可以看到“Apple”和“Orange”修饰语的上下文嵌入显示了高度的相似性。这是可以理解的，因为直接宾语“Apple”和“Orange”本身非常接近。
- en: 'While the representations derived of the Apple and Adventure modifiers are
    not so close, as can be seen from the following:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然从以下结果来看，“Apple”和“Adventure”修饰语的表示并不太接近：
- en: '[PRE7]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '[PRE8]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The Orange and Adventure modifiers are not supposed to be that close either:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: “Orange”和“Adventure”修饰语也不应该太接近：
- en: '[PRE9]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '[PRE10]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Let’s now derive more complicated representations from the embeddings provided
    by BERT. To start with, let’s get the initial embeddings for the modifiers in
    each sentence:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们从BERT提供的嵌入中推导出更复杂的表示。首先，让我们获取每个句子中修饰语的初始嵌入：
- en: '[PRE11]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Now we can, for example, divide contextual embeddings (generated in the 12th
    Encoder layer) by the corresponding initial embeddings (as it was discussed in
    the previous post), to get some new embedding representations to be used in further
    analysis.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以，例如，通过将上下文嵌入（在第12层编码器中生成）除以相应的初始嵌入（如前面帖子中讨论的），得到一些新的嵌入表示，用于进一步分析。
- en: '[PRE12]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: For analysis purposes, you might also want to create another set of representations,
    calculating just element-wise difference between the contextual and non-contextual
    (initial) embedding.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 为了分析目的，你可能还想创建另一组表示，计算上下文嵌入与非上下文（初始）嵌入之间的逐元素差异。
- en: '[PRE13]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Before proceeding to evaluate the representations we just created, let’s create
    another set of representations using Fourier transform, so that we can then compare
    all our representations obtained by different methods.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在继续评估我们刚创建的表示之前，让我们使用傅里叶变换创建另一组表示，以便随后可以比较不同方法获得的所有表示。
- en: '[PRE14]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Now we can compare the obtained representations for each pair of sentences per
    modifier.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以比较每个修饰语的每对句子的获得表示。
- en: '[PRE15]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The produced output should look as follows:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 生成的输出应如下所示：
- en: '[PRE16]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '[PRE17]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '[PRE18]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '[PRE19]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: In the above experiment, we expect to see a high level of similarity between
    the same modifiers in the first two sentences. In fact, we can see that the difference
    and Fourier transform methods did well on the task.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述实验中，我们期望看到前两个句子中的相同修饰语之间具有高度的相似性。实际上，我们可以看到差异和傅里叶变换方法在任务中表现良好。
- en: The purpose of the following experiment is determine the closeness of the modifiers
    whose nouns being modified aren’t that close.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 以下实验的目的是确定那些被修饰的名词不太接近的修饰语的相似性。
- en: '[PRE20]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Here’s the output:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 这是输出结果：
- en: '[PRE21]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '[PRE22]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '[PRE23]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '[PRE24]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: The above output shows that the difference and log quotient were the best when
    evaluating the closeness of the modifiers whose related nouns aren’t that close.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 上述输出显示，当评估那些相关名词不是非常接近的修饰语的相似性时，差异和对数商的效果最好。
- en: '[PRE25]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'The outputs are the following:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '[PRE26]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '[PRE27]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: '[PRE28]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '[PRE29]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Once again, we can see that the difference and log quotient turned out to be
    the best when evaluating the closeness of the modifiers whose related nouns aren’t
    that close.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 我们再次看到，当评估那些相关名词不是非常接近的修饰语的相似性时，差异和对数商的效果最好。
- en: Conclusion
  id: totrans-65
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: Does the Fourier transform make sense when analyzing BERT embeddings? According
    to the experiment performed in this article, we may conclude that this approach
    can be used effectively along with other methods.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在分析BERT嵌入时，傅里叶变换是否有意义？根据本文所做的实验，我们可以得出结论，这种方法可以有效地与其他方法结合使用。
