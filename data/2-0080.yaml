- en: 4 Ideas for Physics-Informed Neural Networks that FAILED
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 4个失败的物理信息神经网络的想法
- en: 原文：[https://towardsdatascience.com/4-ideas-for-physics-informed-neural-networks-that-failed-ce054270e62a](https://towardsdatascience.com/4-ideas-for-physics-informed-neural-networks-that-failed-ce054270e62a)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/4-ideas-for-physics-informed-neural-networks-that-failed-ce054270e62a](https://towardsdatascience.com/4-ideas-for-physics-informed-neural-networks-that-failed-ce054270e62a)
- en: Here is a list of extensions for PINNs that either did not improve their performance,
    or broke them completely — so you do not have to try them yourself.
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 这是一个关于PINNs的扩展列表，这些扩展要么没有提高性能，要么完全破坏了它们——所以你不必自己尝试。
- en: '[](https://rabischof.medium.com/?source=post_page-----ce054270e62a--------------------------------)[![Rafael
    Bischof](../Images/a1d468ea5b61c26a18541f0c0f42c5c6.png)](https://rabischof.medium.com/?source=post_page-----ce054270e62a--------------------------------)[](https://towardsdatascience.com/?source=post_page-----ce054270e62a--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----ce054270e62a--------------------------------)
    [Rafael Bischof](https://rabischof.medium.com/?source=post_page-----ce054270e62a--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://rabischof.medium.com/?source=post_page-----ce054270e62a--------------------------------)[![拉斐尔·比绍夫](../Images/a1d468ea5b61c26a18541f0c0f42c5c6.png)](https://rabischof.medium.com/?source=post_page-----ce054270e62a--------------------------------)[](https://towardsdatascience.com/?source=post_page-----ce054270e62a--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----ce054270e62a--------------------------------)
    [拉斐尔·比绍夫](https://rabischof.medium.com/?source=post_page-----ce054270e62a--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----ce054270e62a--------------------------------)
    ·9 min read·Feb 11, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----ce054270e62a--------------------------------)
    ·9分钟阅读·2023年2月11日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/3cf70f1cea6074bd3edb27d1586c5dfd.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3cf70f1cea6074bd3edb27d1586c5dfd.png)'
- en: Photo by [DeepMind](https://unsplash.com/es/@deepmind?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由 [DeepMind](https://unsplash.com/es/@deepmind?utm_source=medium&utm_medium=referral)
    提供，来自 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: 'In the world of [Physics-informed neural networks (PINNs)](https://www.sciencedirect.com/science/article/pii/S0021999118307125)
    [1], just like in any other emerging field of Machine Learning, it seems like
    everyone is eager to share the amazing techniques they have discovered for improving
    these models. I myself am not an exception, having published three articles with
    useful extensions for improving the performance of PINNs:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [物理信息神经网络（PINNs）](https://www.sciencedirect.com/science/article/pii/S0021999118307125)
    的世界里，就像在任何其他新兴的机器学习领域一样，似乎每个人都急于分享他们发现的改进这些模型的惊人技术。我自己也不例外，已经发布了三篇关于改进PINNs性能的有用扩展的文章：
- en: '[Improving PINNs through Adaptive Loss Balancing](https://medium.com/@rafael.bischof07/improving-pinns-through-adaptive-loss-balancing-55662759e701)'
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[通过自适应损失平衡来改进PINNs](https://medium.com/@rafael.bischof07/improving-pinns-through-adaptive-loss-balancing-55662759e701)'
- en: '[Mixture of Experts for PINNs (MoE-PINNs)](https://medium.com/@rafael.bischof07/mixture-of-experts-for-pinns-moe-pinns-6520adf32438)'
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[PINNs的专家混合（MoE-PINNs）](https://medium.com/@rafael.bischof07/mixture-of-experts-for-pinns-moe-pinns-6520adf32438)'
- en: '[10 useful Hints and Tricks for improving Physics-Informed Neural Networks
    (PINNs)](https://medium.com/@rafael.bischof07/10-useful-hints-and-tricks-for-improving-pinns-1a5dd7b86001)'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10个提升物理信息神经网络（PINNs）的实用提示和技巧](https://medium.com/@rafael.bischof07/10-useful-hints-and-tricks-for-improving-pinns-1a5dd7b86001)'
- en: However, what often goes untold is the countless ideas that did not pan out.
    The reality is that the journey to enhance PINNs is not always a straightforward
    one, and many promising ideas end up falling by the wayside.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，常常被忽视的是那些未能奏效的无数想法。现实是，提升PINNs的旅程并不总是一帆风顺，许多有前途的想法最终都未能实现。
- en: I have not failed. I’ve just found 10,000 ways that won’t work. — **Thomas A.**
    **Edison**
  id: totrans-13
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 我没有失败。我只是找到了一万种行不通的方法。— **托马斯·A.** **爱迪生**
- en: In this article, I want to explore some promising ideas for enhancing PINNs
    that, unfortunately, did not deliver the desired results. So, in the spirit of
    Thomas Edison, join me as we dive into the abyss of unsuccessful extensions to
    PINNs.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我想探讨一些有前途但遗憾未能取得预期结果的PINNs增强想法。因此，秉持托马斯·爱迪生的精神，加入我一起深入探讨那些未成功的PINNs扩展。
- en: 'As usual, the article is accompanied by a notebook containing the concepts
    introduced in this article:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 一如既往，文章附带一个包含本文介绍概念的笔记本：
- en: '[## Burgers_PINN_fails.ipynb'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '[## Burgers_PINN_fails.ipynb'
- en: Colaboratory notebook
  id: totrans-17
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Colaboratory 笔记本
- en: drive.google.com](https://drive.google.com/file/d/1dEJeWjmHjmKBzpDnQQqw0IIEXlnr-Rj1/view?usp=sharing&source=post_page-----ce054270e62a--------------------------------)
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '[drive.google.com](https://drive.google.com/file/d/1dEJeWjmHjmKBzpDnQQqw0IIEXlnr-Rj1/view?usp=sharing&source=post_page-----ce054270e62a--------------------------------)'
- en: Introduction
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 引言
- en: 'PINNs differ from classical neural networks in the way they process and represent
    data. This immediately becomes apparent when looking at the TSNE plots of the
    latent representations in a PINN:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: PINN 与经典神经网络在处理和表示数据的方式上有所不同。当查看 PINN 中隐含表示的 TSNE 图时，这一点立即变得明显：
- en: '![](../Images/1f7fccf8759b3badfc2883e2a6582adc.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1f7fccf8759b3badfc2883e2a6582adc.png)'
- en: '3D TSNE representation of latent vectors in a PINN with three layers trained
    on the Kirchhoff PDE. Left: reference solution of the Kirchhoff PDE. Center: representations
    in first (blue), second (green) and third layer (orange). Right: data-points coloured
    according to the x + y values of the collocation points, where x controls the
    amount of red and y the amount of blue in the pixel colour. Figure by author.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练有三层的PINN模型上进行的隐含向量的3D TSNE表示。左侧：基尔霍夫 PDE 的参考解。中间：第一层（蓝色）、第二层（绿色）和第三层（橙色）的表示。右侧：数据点根据位置点的
    x + y 值着色，其中 x 控制像素颜色中的红色量，y 控制蓝色量。图由作者提供。
- en: In a classical neural network, collocation points evaluating to similar results
    would be represented close together in the latent vectors. This is not the case
    in PINNs. Not only do the representations of each layer live in completely different
    regions of the manifold, even the coordinates of the collocation points do not
    mix.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在经典神经网络中，评估出相似结果的协同位置点会在隐含向量中表示得很接近。在 PINN 中情况并非如此。每一层的表示不仅位于流形的完全不同区域，即使是协同位置点的坐标也不会混合。
- en: These peculiar properties can make working with PINNs difficult and counter-intuitive,
    even for experienced data scientists. While progress in this field has mainly
    come from the theoretical and mathematical community, empirically-driven researchers
    have struggled to get a foothold.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 这些特殊性质使得处理 PINN 变得困难且违反直觉，即使对于经验丰富的数据科学家也是如此。尽管该领域的进展主要来自理论和数学界，但以经验为驱动的研究人员在获得立足点方面遇到了困难。
- en: This article is an attempt at increasing the understanding of the inner workings
    of PINNs by showcasing extensions and ideas that did NOT work, meaning that they
    either did not yield any improvement in accuracy, or broke the training pipeline
    of PINNs completely.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 本文尝试通过展示未能成功的扩展和想法来增加对 PINN 内部工作原理的理解，这些扩展和想法要么未能提高准确性，要么完全破坏了 PINN 的训练流程。
- en: 1\. Batch Normalisation
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 1. 批量归一化
- en: Batch normalisation is a popular technique in deep learning for normalising
    the activations of each layer, which can improve the training stability and reduce
    overfitting. However, batch normalisation can not be used in PINNs due to the
    particular nature of their training process.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 批量归一化是深度学习中一种流行的技术，用于归一化每一层的激活，这可以提高训练稳定性并减少过拟合。然而，由于其训练过程的特殊性，PINN 中不能使用批量归一化。
- en: In PINNs, the training process involves minimising a loss function that measures
    the discrepancy between the predictions of the model and the underlying physics
    of the system. These laws usually do not contain information about how to deal
    with inputs that are dynamically shifted and scaled based on the mean and standard
    deviation of the points in the mini-batch. By messing with the data based on statistics
    from a stochastic batch of samples, batch normalisation “breaks” the physics that
    guide the training of PINNs and should therefore not be used.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在 PINN 中，训练过程涉及最小化一个损失函数，该函数衡量模型预测与系统底层物理之间的差异。这些物理定律通常不包含有关如何处理基于点的均值和标准差进行动态偏移和缩放的输入的信息。通过基于随机小批量样本的统计数据来扰动数据，批量归一化“破坏”了指导
    PINN 训练的物理规律，因此不应使用。
- en: Alternatives to batch normalisation could be layer normalisation, which normalises
    the values on the nodes based on the values on the nodes in the same layer, or
    learnable normalisation, where a dense layer is in charge of producing the shift
    and scale and is regularised to make the values in a layer have zero mean and
    unit variance.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 批量归一化的替代方法可以是层归一化，它根据同一层中的节点值对节点上的值进行归一化，或是可学习的归一化，其中一个密集层负责产生偏移和缩放，并且进行正则化，以使层中的值具有零均值和单位方差。
- en: However, after conducting various experiments, I have found that normalising
    on a layer-level may not be necessary in PINNs. In fact, an effective and straightforward
    way to speed up the convergence of the model is by adding a layer right after
    the input to the network. This layer scales the collocation points to a range
    of [-1, 1] by using the extents of the physical domain where the collocation points
    are sampled.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在进行各种实验后，我发现，在 PINN 中，逐层归一化可能并非必要。实际上，加快模型收敛的一个有效且简单的方法是，在输入网络之后添加一层。这一层通过使用样本的物理域范围，将配点缩放到
    [-1, 1] 的范围。
- en: '[PRE0]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 2\. ReLU**(n+1) activation
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 2\. ReLU**(n+1) 激活
- en: It is crucial to ensure that the activation function used in the network is
    differentiable a sufficient number of times. This is because the outputs of PINNs
    are derived multiple times with respect to the input (based on the order of differentiation
    of the PDE) and an additional time with respect to the model’s weights.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 确保网络中使用的激活函数可以足够多次可微是至关重要的。这是因为 PINN 的输出需要多次相对于输入进行导数计算（根据 PDE 的微分阶数），并且还需要一次相对于模型权重进行导数计算。
- en: Specifically, the activation function should have n+1 non-zero derivatives,
    where n is the order of differentiation in the PDE being solved. This requirement
    eliminates the popular activation function ReLU, as its first derivative is constant
    (either 0 or 1) and its second derivative is zero everywhere (excluding the undefined
    zero point).
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 具体而言，激活函数应具有 n+1 个非零导数，其中 n 是所解 PDE 的微分阶数。这个要求排除了流行的激活函数 ReLU，因为它的一阶导数是常数（0
    或 1），而其二阶导数在所有地方都是零（不包括未定义的零点）。
- en: However, raising the ReLU activation to a power n, it can be made differentiable
    n times, which could be a promising idea to overcome this limitation. Even better,
    by initialising PINNs with several layers in parallel, each using ReLU raised
    to a different value k ≤ n+1, we would ensure that patterns in different orders
    of differentiation are captured by different parts of the network.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，将 ReLU 激活函数提升到功率 n，可以使其可微 n 次，这可能是克服这一限制的一个有希望的思路。更好的是，通过将 PINN 初始化为多个并行层，每个层使用不同值
    k ≤ n+1 的 ReLU，我们可以确保不同微分阶数的模式由网络的不同部分捕捉。
- en: '![](../Images/951539246a5ad15d9861eb75b5ac2309.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/951539246a5ad15d9861eb75b5ac2309.png)'
- en: Example architecture of a PINN with several parallel layers using the ReLU activation
    raised to a different power k ≤ n + 1\. Figure by author.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 示例架构展示了一个具有多个平行层的 PINN，使用了不同功率 k ≤ n + 1 的 ReLU 激活函数。图由作者提供。
- en: However, to my great disappointment, this architecture failed to converge. I
    can only conjecture that this is caused by the layers with power k not receiving
    updates from terms in the loss operating on higher orders of differentiation.
    But, at inference time, when predicting u, all layers contribute to its calculation,
    even if not having been informed by all components of the PDE. This probably leads
    to injection of noise, thus limiting the architecture’s modelling capabilities.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，让我非常失望的是，这种架构未能收敛。我只能猜测这是由于功率 k 的层没有从作用于更高阶微分的损失项中获得更新。但在推断时，当预测 u 时，即使所有
    PDE 的组件没有完全通知，每一层都会参与计算。这可能导致噪声的注入，从而限制了架构的建模能力。
- en: 3\. Convolutional Neural Networks
  id: totrans-39
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 3\. 卷积神经网络
- en: When solving PDEs on a 2D domain, one could be tempted to sample the collocation
    points in a grid and then treating them like pixels in an image. This would then
    allow the use of a CNN, which, thanks to its inductive bias of spatial invariance,
    is a widely used architecture for image processing.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在二维域上求解 PDE 时，人们可能会倾向于在网格中采样配点，然后将其视为图像中的像素。这将允许使用 CNN，由于其空间不变性的归纳偏差，CNN 是图像处理中的广泛使用架构。
- en: However, remember that PINNs are trained through physical laws. These laws generally
    do not contain information about how to aggregate values from several collocation
    points. Therefore, having a kernel that takes more than one collocation point
    into account does not make much sense. In addition, a great property of CNNs is
    the fact that they can find localised patterns at several levels of abstraction,
    with the first few layers acting essentially as edge detectors. But in PINNs,
    the inputs to the kernels of the first layers would just be coordinates of equally
    spaced collocation points. There are not patterns to be learned from that.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，请记住，PINNs是通过物理定律进行训练的。这些定律通常不包含有关如何从多个配点处聚合值的信息。因此，考虑多个配点的内核并没有太大意义。此外，CNN的一个重要特性是它们可以在多个抽象层次上找到局部模式，前几层本质上作为边缘检测器。但在PINNs中，第一层内核的输入只是等间距配点的坐标。从中没有模式可以学习。
- en: In order to make CNNs work, you would have to [modify the PINNs’ problem setting
    and objective](https://arxiv.org/abs/2004.13145) [2].
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使CNN有效，你需要[修改PINNs的设置和目标](https://arxiv.org/abs/2004.13145) [2]。
- en: 4\. Vector Quantisation
  id: totrans-43
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 4\. 向量量化
- en: '[Vector quantisation](https://arxiv.org/abs/1711.00937) [3] is a machine learning
    technique that maps high-dimensional continuous data into a set of discrete symbols.
    The idea behind using vector quantisation layers in PINNs is to introduce a discrete
    component that would help capturing discontinuities in PDEs.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '[向量量化](https://arxiv.org/abs/1711.00937) [3]是一种机器学习技术，它将高维连续数据映射到一组离散符号中。在PINNs中使用向量量化层的想法是引入一个离散组件，以帮助捕捉PDE中的间断性。'
- en: Neural networks are notoriously bad at modelling sharp discontinuities, as their
    approximation would require the use of a step function which is not per-se differentiable.
    By incorporating vector quantisation into the architecture, one could be forgiven
    for thinking that PINNs would be better equipped to handle these sharp transitions.
    This is because the vector quantisation layer essentially serves as a “discretisation”
    step, breaking the continuous input space into smaller, more manageable segments
    with sharp boundaries between them.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络在建模尖锐的间断性时表现不佳，因为其逼近需要使用一个步进函数，而步进函数本身不可微。通过将向量量化融入架构中，可能会认为PINNs能够更好地处理这些尖锐的过渡。这是因为向量量化层本质上充当了一个“离散化”步骤，将连续输入空间划分为较小、更易管理的段，并在它们之间形成明显的边界。
- en: '![](../Images/2ad5a78570a2e614a4e082d450b38624.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2ad5a78570a2e614a4e082d450b38624.png)'
- en: Sketch of how what Vector Quantised PINNs (VQPINNs) could look like. Figure
    by author.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 向量量化PINNs（VQPINNs）可能的外观示意图。作者绘制的图。
- en: First, the coordinates x and y are mapped onto a higher-dimensional space using
    a dense layer. Then the cosine-similarity between this vector and a set of vectors
    in a trainable dictionary is calculated and the most similar vector is selected
    to serve as additional input to the PINN. The PINN thus receives as input the
    collocation points as well as a vector originating from a discrete operation.
    This operation is made approximately differentiable by a [straight-through estimator](https://arxiv.org/abs/1308.3432)
    [4].
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，坐标x和y通过一个密集层映射到更高维空间。然后计算该向量与训练字典中一组向量之间的余弦相似度，并选择最相似的向量作为PINN的附加输入。因此，PINN接收的输入包括配点和源自离散操作的向量。该操作通过[直通估计器](https://arxiv.org/abs/1308.3432)
    [4]被近似为可微。
- en: However, adhering to the topic of this article, this endeavour was filled with
    frustration and disappointment, as, again, a promising idea did not bear fruit.
    When I trained a PINN on the [Buckley-Leverett equation](https://arxiv.org/pdf/2112.14826.pdf)
    [5] featuring a discontinuity across the entire domain, my model indeed broke
    the it into two segments with a sharp boundary in-between. But the boundary was
    never at the correct place and the modelling of the governing equation inside
    the sub-region was unsatisfactory.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，回到本文主题，这一努力充满了挫折和失望，因为一个有前途的想法再次没有结出成果。当我在[巴克利-利弗雷特方程](https://arxiv.org/pdf/2112.14826.pdf)
    [5]上训练一个PINN，该方程在整个领域内存在间断时，我的模型确实将其划分为两个段，并在之间形成了一个尖锐的边界。但边界从未在正确的位置，并且子区域内主方程的建模也不令人满意。
- en: '![](../Images/b7125210073ddb489644e79a956acbe1.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b7125210073ddb489644e79a956acbe1.png)'
- en: Buckley-Leverett PDE with a sharp discontinuity across the entire domain. Figure
    by [Diab et al.](https://arxiv.org/pdf/2112.14826.pdf) [5]
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: Buckley-Leverett PDE在整个领域上有一个明显的不连续性。图由[Diab et al.](https://arxiv.org/pdf/2112.14826.pdf)
    [5]
- en: My best guess is that the discontinuities made a hard split of the physical
    domain, with different parts of the network focussing on distinct sub-domains.
    This means that parts of the network could be disconnected from boundary- or initial
    conditions if they applied only to regions outside of the own sub-domain. This
    is also why ensembles of PINNs, like [MoE-PINNs](https://medium.com/@rafael.bischof07/mixture-of-experts-for-pinns-moe-pinns-6520adf32438)
    [6], must ensure that the gating network remains a continuous function, i.e. do
    not make the softmax operation too sharp. Otherwise, the boundary- and initial
    conditions are not enforced on the entire domain, thus leading to incorrect solutions.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 我最好的猜测是，不连续性使物理领域出现了明显的分割，网络的不同部分专注于不同的子领域。这意味着，如果网络的部分仅应用于自己子领域之外的区域，它们可能会与边界或初始条件断开连接。这也是为什么PINNs的集合，如[MoE-PINNs](https://medium.com/@rafael.bischof07/mixture-of-experts-for-pinns-moe-pinns-6520adf32438)
    [6]，必须确保门控网络保持连续函数，即不要使softmax操作过于尖锐。否则，边界条件和初始条件不会在整个领域内强加，从而导致不正确的解决方案。
- en: Conclusion
  id: totrans-53
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: The successful implementation and extension of PINNs is a difficult and oftentimes
    counter-intuitive endeavour. The fact that established (batch normalisation, CNNs)
    or promising features (vector quantisation) coming from other fields of machine
    learning can not be applied to PINNs goes to show that practitioners need a deep
    understanding of the inner workings in order to propose effective extensions and
    ideas.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 成功实现和扩展PINNs是一项困难且常常违背直觉的工作。已建立的（批量归一化，CNNs）或来自其他机器学习领域的有前景的特性（向量量化）无法应用于PINNs，这表明从业者需要深入了解内部机制，以提出有效的扩展和想法。
- en: Such ideas are dearly needed for making PINNs more robust, speeding up convergence,
    and finally making them a promising alternative to established numerical methods
    like the Finite Element Method.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 这些想法对于使PINNs更强大，加速收敛，以及最终使其成为像有限元方法这样的已建立数值方法的有前景的替代方案是非常需要的。
- en: 'Thank you for taking the time to read this article. I encourage you to try
    out these ideas for yourself and would be thrilled to hear about someone adding
    a small tweak to these ideas that suddenly made them work! To pave the way for
    such discoveries, I have set up a notebook where you can play with the implementation
    of these ideas:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 感谢你抽时间阅读这篇文章。我鼓励你亲自尝试这些想法，并且非常期待听到有人对这些想法进行小的调整，从而突然使它们有效的消息！为了铺平这样的发现之路，我创建了一个笔记本，你可以在其中玩转这些想法的实现：
- en: '[## Burgers_PINN_fails.ipynb'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '[## Burgers_PINN_fails.ipynb'
- en: Colaboratory notebook
  id: totrans-58
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Colaboratory笔记本
- en: drive.google.com](https://drive.google.com/file/d/1dEJeWjmHjmKBzpDnQQqw0IIEXlnr-Rj1/view?usp=sharing&source=post_page-----ce054270e62a--------------------------------)
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: drive.google.com](https://drive.google.com/file/d/1dEJeWjmHjmKBzpDnQQqw0IIEXlnr-Rj1/view?usp=sharing&source=post_page-----ce054270e62a--------------------------------)
- en: If you have more suggestions or recommendations, I would love to hear about
    them in the comments! You can find more information about my colleague Michael
    Kraus on [mkrausai.com](http://mkrausai.com/) and about myself on [rabischof.ch](http://rabischof.ch/).
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你有更多的建议或推荐，我非常愿意在评论中听到它们！你可以在[mkrausai.com](http://mkrausai.com/)上找到关于我的同事Michael
    Kraus的更多信息，在[rabischof.ch](http://rabischof.ch/)上找到关于我自己的更多信息。
- en: '[1] M. Raissi, P. Perdikaris, and G. E. Karniadakis, Physics-informed neural
    networks: A deep learning framework for solving forward and inverse problems involving
    nonlinear partial differential equations, *Journal of Computational Physics* 378
    (2019), 686–707.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] M. Raissi, P. Perdikaris, 和 G. E. Karniadakis, 物理信息神经网络：解决涉及非线性偏微分方程的正向和逆向问题的深度学习框架，*计算物理学杂志*
    378 (2019), 686–707。'
- en: '[2] Gao, Han, Luning Sun, and Jian-Xun Wang. “PhyGeoNet: Physics-informed geometry-adaptive
    convolutional neural networks for solving parameterized steady-state PDEs on irregular
    domain.” *Journal of Computational Physics* 428 (2021): 110079.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] Gao, Han, Luning Sun, 和 Jian-Xun Wang. “PhyGeoNet: 物理信息几何自适应卷积神经网络用于解决不规则领域的参数化稳态PDEs。”
    *计算物理学杂志* 428 (2021): 110079。'
- en: '[3] Van Den Oord, Aaron, and Oriol Vinyals. “Neural discrete representation
    learning.” *Advances in neural information processing systems* 30 (2017).'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] Van Den Oord, Aaron, 和 Oriol Vinyals. “神经离散表示学习。” *神经信息处理系统进展* 30 (2017)。'
- en: '[4] Bengio, Yoshua, Nicholas Léonard, and Aaron Courville. “Estimating or propagating
    gradients through stochastic neurons for conditional computation.” *arXiv preprint
    arXiv:1308.3432* (2013).'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] Bengio, Yoshua, Nicholas Léonard 和 Aaron Courville。“通过随机神经元估计或传播梯度以进行条件计算。”
    *arXiv预印本 arXiv:1308.3432* (2013)。'
- en: '[5] Diab, Waleed, and Mohammed Al Kobaisi. “PINNs for the Solution of the Hyperbolic
    Buckley-Leverett Problem with a Non-convex Flux Function.” *arXiv preprint arXiv:2112.14826*
    (2021).'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '[5] Diab, Waleed 和 Mohammed Al Kobaisi。“用于非凸流量函数的双曲Buckley-Leverett问题的PINNs解决方案。”
    *arXiv预印本 arXiv:2112.14826* (2021)。'
- en: '[6] R. Bischof and M. A. Kraus, “[Mixture-of-Experts-Ensemble Meta-Learning
    for Physics-Informed Neural Networks](https://scholar.google.com/scholar?oi=bibs&cluster=12593425659851579449&btnI=1&hl=en)”,
    Proceedings of 33\. Forum Bauinformatik, 2022'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '[6] R. Bischof 和 M. A. Kraus，“[物理信息神经网络的专家混合集成元学习](https://scholar.google.com/scholar?oi=bibs&cluster=12593425659851579449&btnI=1&hl=en)”，第33届建筑信息学论坛论文集，2022。'
