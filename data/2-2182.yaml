- en: 'Understanding Gradient Boosting: A Data Scientist’s Guide'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解梯度提升：数据科学家的指南
- en: 原文：[https://towardsdatascience.com/understanding-gradient-boosting-a-data-scientists-guide-f5e0e013f441](https://towardsdatascience.com/understanding-gradient-boosting-a-data-scientists-guide-f5e0e013f441)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/understanding-gradient-boosting-a-data-scientists-guide-f5e0e013f441](https://towardsdatascience.com/understanding-gradient-boosting-a-data-scientists-guide-f5e0e013f441)
- en: '![](../Images/ddead02c0234cca8db16b03529386445.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ddead02c0234cca8db16b03529386445.png)'
- en: Image by Midjourney
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由Midjourney提供
- en: '[](https://louis-chan.medium.com/?source=post_page-----f5e0e013f441--------------------------------)[![Louis
    Chan](../Images/6d8df9a478e929dd521059631f26e081.png)](https://louis-chan.medium.com/?source=post_page-----f5e0e013f441--------------------------------)[](https://towardsdatascience.com/?source=post_page-----f5e0e013f441--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----f5e0e013f441--------------------------------)
    [Louis Chan](https://louis-chan.medium.com/?source=post_page-----f5e0e013f441--------------------------------)'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://louis-chan.medium.com/?source=post_page-----f5e0e013f441--------------------------------)[![Louis
    Chan](../Images/6d8df9a478e929dd521059631f26e081.png)](https://louis-chan.medium.com/?source=post_page-----f5e0e013f441--------------------------------)[](https://towardsdatascience.com/?source=post_page-----f5e0e013f441--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----f5e0e013f441--------------------------------)
    [Louis Chan](https://louis-chan.medium.com/?source=post_page-----f5e0e013f441--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----f5e0e013f441--------------------------------)
    ·10 min read·Feb 7, 2023
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----f5e0e013f441--------------------------------)
    ·阅读时间10分钟·2023年2月7日
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: 'Gradient boosting machine (GBM) is one of the most significant advances in
    machine learning and data science that has enabled us as practitioners to use
    ensembles of models to best many domain-specific problems. While this tool is
    widely available in python packages like `scikit-learn` and `xgboost`, as a data
    scientist, we should always look into the theory and mathematics of the model
    instead of using it as a black box. In this blog, we will dive into the following
    areas:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度提升机（GBM）是机器学习和数据科学领域最重要的进展之一，它使我们这些从业者能够使用模型集成来解决许多领域特定的问题。尽管这一工具在`scikit-learn`和`xgboost`等Python包中广泛可用，但作为数据科学家，我们应该始终深入探讨模型的理论和数学，而不是将其视为黑箱。在这篇博客中，我们将深入探讨以下领域：
- en: Different backing concepts of GBM
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GBM的不同支持概念
- en: Step-by-step illustrated to recreate GBM
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分步图解以重建GBM
- en: Pro’s and Con’s
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 优点和缺点
- en: '![](../Images/babd686e12a9031bbac988669119ad87.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/babd686e12a9031bbac988669119ad87.png)'
- en: Let’s jump into it — Image from [GIPHY](https://giphy.com/gifs/14uzPzKMOuVIPu)
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们深入了解一下——图片来自 [GIPHY](https://giphy.com/gifs/14uzPzKMOuVIPu)
- en: Fundamentals of Gradient Boosting
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 梯度提升的基本原理
- en: 1\. Weak learners and ensemble learning
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1. 弱学习者和集成学习
- en: Weak learners and ensemble learning are the two key concepts that make gradient
    boosting work. A weak learner is a model that is only slightly better than random
    guessing. Combined with many other weak learners, they can form a robust ensemble
    model to make accurate predictions.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 弱学习者和集成学习是使梯度提升有效的两个关键概念。弱学习者是一个模型，它的表现仅比随机猜测稍好。与许多其他弱学习者结合，可以形成一个强大的集成模型，进行准确预测。
- en: Too wordy, too complicated
  id: totrans-16
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 文字过多，过于复杂
- en: Okay, imagine we are playing 10,000-piece jigsaw with two other friends. (They
    must be excellent friends to sign up for this) Each of us is responsible for piecing
    one of the four quadrants. Although we might only be able to solve a small piece
    of the puzzle on our own, when we work together as a team, we can quickly complete
    the entire puzzle.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 好吧，假设我们正在和两个朋友一起玩10000片拼图。（他们一定是很棒的朋友才愿意报名）我们每个人负责拼接四个象限中的一个。虽然我们可能只能自己解决拼图的一小部分，但当我们作为团队合作时，我们可以迅速完成整个拼图。
- en: In this scenario, every one of us is a weak learner, and the team of four is
    the ensemble model. Like how we focus on our quadrant, each weak learner in an
    ensemble model is good at predicting based on certain features and characteristics.
    When all four of us weak learners come together to share our thoughts on whether
    one of the pieces belongs to a quadrant, the ensemble model can pump out more
    accurate predictions than we can individually.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们每个人都是一个弱学习者，四个人的团队就是集成模型。就像我们关注我们的象限一样，集成模型中的每个弱学习者擅长根据某些特征和特性进行预测。当我们四个弱学习者聚在一起分享对某个数据点是否属于某个象限的看法时，集成模型能够提供比我们单独预测更准确的结果。
- en: 'In essence, weak learners and ensemble learning are the power of collective
    intelligence. Or as an old saying goes:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 本质上，弱学习者和集成学习是集体智慧的力量。正如古老的谚语所说：
- en: The whole is greater than the sum of its parts.
  id: totrans-20
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 整体大于部分之和。
- en: 2\. Additive Models
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2\. 加法模型
- en: In Gradient Boosting algorithms, weak learner models are add to the ensemble
    iteratively. It will almost look like a Taylor Approximation where the final value
    is predicted using a rough estimate corrected by a series of correction terms.
    As each of the weak learners will contribute one correction terms, this makes
    GBM very flexible in terms of adding models when prediction results indicates
    overfitting.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在梯度提升算法中，弱学习者模型是迭代地添加到集成中的。它几乎看起来像是泰勒近似，其中最终值是使用粗略估计通过一系列修正项进行修正。由于每个弱学习者都会贡献一个修正项，这使得GBM在添加模型时非常灵活，当预测结果表明过拟合时可以进行调整。
- en: '![](../Images/8035aa728960dfd581bc152a68ed333c.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8035aa728960dfd581bc152a68ed333c.png)'
- en: Taylor Expansion for e^x starts with 1 and get iteratively corrected — Image
    by Author
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: e^x的泰勒展开从1开始并进行迭代修正 — 作者图片
- en: '![](../Images/f03110eaceea6be85a989d4dad4da3e2.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f03110eaceea6be85a989d4dad4da3e2.png)'
- en: GBM starts with a baseline and get iteratively corrected by additive models
    — Image by Author
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: GBM从基线开始，通过加法模型进行迭代修正 — 作者图片
- en: 3\. Loss Function
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3\. 损失函数
- en: When we say a model gets iteratively corrected or improved, we have got to understand
    that not all improvements are equal. For example, me and my brother’s exam scores
    have improved from 10 to 20 and from 80 to 90 respectively out of a full score
    of 100\. Although both of us has improved for 10 points, can we say that the improvements
    are the same?
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们说一个模型经过迭代修正或改进时，我们必须明白并不是所有的改进都是相同的。例如，我和我哥哥的考试分数从10分提高到20分和从80分提高到90分，满分为100分。尽管我们都提高了10分，但我们能说这些改进是一样的吗？
- en: Here comes loss functions.
  id: totrans-29
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 这就引出了损失函数。
- en: While loss functions are just mathematical ways for measuring the difference
    between two values. In the context of machine learning, it can be used as a scorecard
    for measuring the difference between predicted value the actual value, and hence
    for evaluating model performance. The more significant the loss, the worse our
    model performs.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 损失函数只是测量两个值之间差异的数学方法。在机器学习的背景下，它可以用作衡量预测值与实际值之间差异的评分卡，从而评估模型性能。损失越大，模型表现越差。
- en: In our example above, we can say that 10-point improvement from 10 to 20 is
    more significant if we consider the percentage improvement, or it can also be
    less significant if we consider how far our scores are from the full score of
    100.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们上面的例子中，如果我们考虑百分比改进，从10到20的10分提高显得更加重要，或者如果我们考虑我们的分数离满分100有多远，这种提高也可能显得不那么重要。
- en: '![](../Images/9e33919545929e1ad848b90675d113d2.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9e33919545929e1ad848b90675d113d2.png)'
- en: Loss functions on our exam example — Image by Author
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 考试示例中的损失函数 — 作者图片
- en: 'Depending on how we define our model to be successful, we might want to choose
    different loss functions. Some common choices are as follows:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 根据我们定义模型成功的标准，我们可能会选择不同的损失函数。一些常见的选择如下：
- en: '**MSE (Mean Square Error):** Commonly used for regression models'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**MSE（均方误差）：** 通常用于回归模型'
- en: '**MAE (Mean Absolute Error)**'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**MAE（均方绝对误差）**'
- en: '**Log Loss (Cross Entropy):** Commonly used for classification models'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Log Loss（交叉熵）：** 通常用于分类模型'
- en: 4\. Gradient Descent
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4\. 梯度下降
- en: Still following the same example, if we want to be the most efficient in improving
    our score, we might want to study harder where it is easiest to gain maximal additional
    points (e.g. spelling mistakes for an absentminded person like me). Once we have
    conquered this, we then move on to the next easiest item to gain maximal additional
    point. Rinse and repeat until we get to 100.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 仍以相同的例子为例，如果我们希望在提高分数方面最有效率，我们可能需要在最容易获得额外积分的地方更加努力学习（例如，对像我这样心不在焉的人来说拼写错误）。一旦征服了这一点，我们就继续处理下一个最容易获得额外积分的项目。如此反复，直到达到100分。
- en: This is exactly how gradient descent works!
  id: totrans-40
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 这正是梯度下降的工作原理！
- en: Technically speaking, gradient descent is a mechanism that aims to explore a
    function's minimum value by iteratively moving in the direction of the steepest
    decrease in the function value. In the context of machine learning, by **minimising
    the loss function**, we are trying to identify the best set of parameters for
    our model to make accurate predictions.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 从技术上讲，梯度下降是一种机制，旨在通过反复沿着函数值减少最快的方向移动，来探索函数的最小值。在机器学习的背景下，通过**最小化损失函数**，我们试图识别出一组最佳的模型参数，以便做出准确的预测。
- en: Without digressing too much into a blog post about gradient descent, an important
    issue with this technique is the possibility for the algorithm to be converging
    to a suboptimal minima.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 不要过多地跑题到梯度下降的博客文章中，这种技术的一个重要问题是算法可能会收敛到一个次优的最小值。
- en: '![](../Images/0ff5f9838b088583a44b487475973df8.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0ff5f9838b088583a44b487475973df8.png)'
- en: Minima — Image by Author
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 最小值 — 作者提供的图像
- en: 'Rest assured. There are different approaches for handling these situations.
    Examples include the following:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 请放心。有不同的方法来处理这些情况。示例包括以下内容：
- en: 'Learning Rate: This adjusts how much we move at each step. The intuition behind
    this is that if it is not the absolute trough, we could have skipped it by having
    a larger step.'
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 学习率：这调整了我们每一步的移动量。其直觉是，如果它不是绝对的低谷，我们可能会因为步长过大而错过它。
- en: 'Momentum Approach: This approach takes into account of our previous steps.
    The algorithm will feed forward a fraction of the previous step (hence momentum)
    for smoothing out oscillations. (i.e. instead of a 90-degree right hand turn,
    momentum makes it into a gentle curve)'
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 动量方法：这种方法考虑了我们之前的步骤。算法会将前一步的一个部分（即动量）传递给下一步，以平滑振荡。（即，动量将90度的右转变成一个平缓的曲线）
- en: Gradient descent is basically a root searching algorithm. If you are interested
    in learning more about root searching algorithms, you might want to give me blog
    on root searching algorithms a read!
  id: totrans-48
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 梯度下降基本上是一种根查找算法。如果你对根查找算法感兴趣，可以看看我关于根查找算法的博客！
- en: '[](/mastering-root-searching-algorithms-in-python-7120c335a2a8?source=post_page-----f5e0e013f441--------------------------------)
    [## Efficient Root Searching Algorithms in Python'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '[## 高效的根查找算法在Python中](/mastering-root-searching-algorithms-in-python-7120c335a2a8?source=post_page-----f5e0e013f441--------------------------------)'
- en: Implementing efficient searching algorithms for finding roots, and optimisation
    in Python
  id: totrans-50
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 在Python中实现高效的根查找算法和优化
- en: towardsdatascience.com](/mastering-root-searching-algorithms-in-python-7120c335a2a8?source=post_page-----f5e0e013f441--------------------------------)
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '[towardsdatascience.com](/mastering-root-searching-algorithms-in-python-7120c335a2a8?source=post_page-----f5e0e013f441--------------------------------)'
- en: Before we jump into understanding how gradient boosting algorithm work from
    scratch, a quick plug-in for myself.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们深入理解梯度提升算法的工作原理之前，先简单介绍一下自己。
- en: If you have enjoyed the read so far, you can also support me by subscribing
    to Medium using my affiliate link below. This has been a platform where I have
    found lots of enjoyable reads. Even if you are perfectly content with not subscribing,
    you can also support me and my creation using claps.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你喜欢这篇文章，你也可以通过下面的附属链接来支持我订阅Medium。这是一个我发现很多有趣阅读的平台。即使你不打算订阅，你也可以通过点赞来支持我和我的创作。
- en: '[](https://louis-chan.medium.com/membership?source=post_page-----f5e0e013f441--------------------------------)
    [## Join Medium with my referral link - Louis Chan'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '[## 通过我的推荐链接加入Medium - Louis Chan](https://louis-chan.medium.com/membership?source=post_page-----f5e0e013f441--------------------------------)'
- en: Read every story from Louis Chan (and thousands of other writers on Medium).
    Your membership fee directly supports…
  id: totrans-55
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 阅读Louis Chan的每一个故事（以及Medium上成千上万的其他作家的故事）。你的会员费用直接支持…
- en: louis-chan.medium.com](https://louis-chan.medium.com/membership?source=post_page-----f5e0e013f441--------------------------------)
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '[louis-chan.medium.com](https://louis-chan.medium.com/membership?source=post_page-----f5e0e013f441--------------------------------)'
- en: Much appreciated for tolerating that plug-in. Here is a cute wink from a Samoyed
    as my gratitude.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 感谢你容忍这个插件。这是来自萨摩耶的一次可爱眨眼，作为我的感谢。
- en: Image from [GIPHY](https://giphy.com/gifs/cute-dog-6MWahPArixa6I)
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来自 [GIPHY](https://giphy.com/gifs/cute-dog-6MWahPArixa6I)
- en: Now back into business!
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 现在回到正题！
- en: The Gradient Boosting Algorithm
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 梯度提升算法
- en: The algorithm from scratch
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从头开始的算法
- en: 'Let’s say we want to predict the rents of apartments in an area using a Gradient
    Boosted machine with **1 tree**:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们想用一个**1棵树**的梯度提升机来预测某个地区的公寓租金：
- en: '![](../Images/d74dae66d96e5a7fbb4cf18a87e09b7a.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d74dae66d96e5a7fbb4cf18a87e09b7a.png)'
- en: Image by Author
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供
- en: We start by calculating the average rents, which is 688\. This will be our benchmark
    model i.e. predict the rent to be an average of the area. **We can also understand
    this as a naive prediction of the target variable.**
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先计算平均租金，即 688。这将是我们的基准模型，即预测租金为该区域的平均值。**我们也可以将其理解为目标变量的简单预测。**
- en: '![](../Images/865df0ad8f02e100f0618f35ebcc77d3.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/865df0ad8f02e100f0618f35ebcc77d3.png)'
- en: Image by Author
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供
- en: Let’s then calculate the different between the rents and the average — this
    is the gap that our weak learners would try to minimise iteratively. In our case,
    we have just got one weak learner.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 然后计算租金与平均值之间的差异——这是我们的弱学习器会尝试迭代最小化的间隙。在我们的例子中，我们只有一个弱学习器。
- en: '![](../Images/2c8eb0eb7fa41d8a7ff7de00633882da.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2c8eb0eb7fa41d8a7ff7de00633882da.png)'
- en: Image by Author
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供
- en: 'Now it is time to build up our group of weak learners that predicts the **residual
    from the average**. Let’s start with the decision tree:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是时候建立我们的弱学习器组来预测**来自平均值的残差**了。让我们从决策树开始：
- en: '![](../Images/836fd45995b88a8101973cc2b5109237.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/836fd45995b88a8101973cc2b5109237.png)'
- en: Image by Author
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供
- en: Wherever there are more than one item in the leave node, the predicted result
    should be the average of the items.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 如果叶节点中有多个项目，则预测结果应为这些项目的平均值。
- en: '![](../Images/e773dbf367a9a30414dc0f62e7772617.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e773dbf367a9a30414dc0f62e7772617.png)'
- en: Image by Author
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供
- en: 'Now let’s try to predict using this new decision tree and compare it with the
    benchmark we have obtained earlier. The formula for calculating the predicted
    value is as follows:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们尝试使用这棵新决策树进行预测，并将其与我们之前获得的基准进行比较。计算预测值的公式如下：
- en: '![](../Images/6a96d6b89562680937ebeee571a6f510.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6a96d6b89562680937ebeee571a6f510.png)'
- en: Image by Author
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供
- en: By applying the learning rate as a modifier to the predicted residual, we are
    applying the concept from gradient descent where we are taking a stepwise approach
    to minimise the loss function. **In our case, the “loss function” is our residual.**
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将学习率作为预测残差的修正因子，我们应用了梯度下降的概念，即采用逐步方法来最小化损失函数。**在我们的案例中，“损失函数”是我们的残差。**
- en: '![](../Images/db09f6b37b7e84d909666322787a9e3f.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/db09f6b37b7e84d909666322787a9e3f.png)'
- en: Image by Author
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供
- en: With the predicted values based on the first tree, we can then compute the new
    residual.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 基于第一棵树的预测值，我们可以计算新的残差。
- en: '![](../Images/50bb4d0a557587a6bde4cd6317ed5fba.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/50bb4d0a557587a6bde4cd6317ed5fba.png)'
- en: Image by Author
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供
- en: See! The residuals are already getting smaller by “adding” the tree we have
    created to the baseline prediction we made earlier using just the average!
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 看！通过将我们创建的树“添加”到我们之前仅使用平均值做出的基线预测中，残差已经变得更小了！
- en: If we are to train a proper Gradient Boosted model, we will need to repeat the
    process of fitting a lot of tree-based models (often over 1,000 trees) to form
    the additive model.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们要训练一个合适的梯度提升模型，我们需要重复拟合大量基于树的模型（通常超过 1,000 棵树）以形成加法模型。
- en: 'In general, after fitting say 1,000 trees, we can use the following formula
    for calculating the final predicted value:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，在拟合了 1,000 棵树后，我们可以使用以下公式来计算最终预测值：
- en: '![](../Images/fb6bb8ab28f8fa6032552d72bbb5705e.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fb6bb8ab28f8fa6032552d72bbb5705e.png)'
- en: Image by Author
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供
- en: Hyperparameters
  id: totrans-91
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 超参数
- en: 'Based on the above strawman example, we can summarise the characteristics of
    a GBM with the following hyperparameters:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 根据上述例子，我们可以总结出具有以下超参数的 GBM 的特点：
- en: '**Number of additive models:** The total number of weak learners in the ensemble.'
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**加法模型数量：** 集成中的弱学习器总数。'
- en: '**Learning rate:** The modifier that decides the contribution of the weak learners
    to the final outcome. This also dictates how quickly the model navigates the gradients.'
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**学习率：** 决定弱学习者对最终结果贡献的修饰符。这也决定了模型调整梯度的速度。'
- en: '**Maximum depth of each model:** The maximum depth of every weak learner. Shallower
    weak learner means more memory efficiency when fitting individual weak learner
    with a drawback of potentially inability to capture more complex interactions
    between variables.'
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**每个模型的最大深度：** 每个弱学习者的最大深度。较浅的弱学习者意味着在拟合单个弱学习者时更具内存效率，但可能无法捕捉变量之间更复杂的交互。'
- en: '**Minimum number of observations in terminal nodes of the weak learners:**
    A smaller value are typical for imbalanced dataset while a larger value can be
    set for a more balanced dataset.'
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**弱学习者终端节点中的最小观察数：** 较小的值通常适用于不平衡的数据集，而较大的值适用于更平衡的数据集。'
- en: Pros of Gradient Boosting
  id: totrans-97
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 梯度提升的优点
- en: '**Strong prediction performance:** While this is not naturally an advantage
    of using gradient boosting, retrospectively speaking, gradient boosting has been
    a very common winner on various competitions on Kaggle. This could be accredited
    to gradient boosting’s characteristics of combining a lot of smaller models and
    use the crowd intelligence for making the final prediction rather than trying
    to fit all the data patterns in one model.'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**强大的预测性能：** 尽管这并非使用梯度提升的自然优势，但回顾来说，梯度提升在 Kaggle 上的各种比赛中常常获胜。这可以归功于梯度提升结合了许多较小模型的特点，并利用群体智慧进行最终预测，而不是试图将所有数据模式拟合到一个模型中。'
- en: '**Flexibility:** Gradient boosting is a model type that can be applied to regression
    or classification problem using different weak learners (not necessarily decision
    trees), loss functions, and data types (ordinal, continuous, categorical etc)'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**灵活性：** 梯度提升是一种可以应用于回归或分类问题的模型类型，使用不同的弱学习者（不一定是决策树）、损失函数和数据类型（有序、连续、类别等）。'
- en: '**Easily accessible:** There are a number of modules from which you can apply
    gradient boosting onto your data problem whether you are using R ([gbm](https://www.rdocumentation.org/packages/gbm/versions/2.1.8.1),
    [xgboost](https://www.rdocumentation.org/packages/xgboost/versions/1.7.3.1), [lightgbm](https://www.rdocumentation.org/packages/lightgbm/versions/3.3.5)),
    Julia ([GradientBoost](https://juliapackages.com/p/gradientboost)), or Python
    ([sklearn](https://pypi.org/project/sklearn/), [xgboost](https://pypi.org/project/xgboost/),
    [lightgbm](https://pypi.org/project/lightgbm/), [catboost](https://pypi.org/project/catboost/)).'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**易于获取：** 不论你使用 R ([gbm](https://www.rdocumentation.org/packages/gbm/versions/2.1.8.1),
    [xgboost](https://www.rdocumentation.org/packages/xgboost/versions/1.7.3.1), [lightgbm](https://www.rdocumentation.org/packages/lightgbm/versions/3.3.5)),
    Julia ([GradientBoost](https://juliapackages.com/p/gradientboost)), 还是 Python
    ([sklearn](https://pypi.org/project/sklearn/), [xgboost](https://pypi.org/project/xgboost/),
    [lightgbm](https://pypi.org/project/lightgbm/), [catboost](https://pypi.org/project/catboost/))，都有许多模块可以将梯度提升应用于你的数据问题。'
- en: '**Interpretability:** Gradient boosting machines are arguably one of the model
    topologies that has a better balance between complexity and interpretability compared
    against likes of neural networks. If you want to read more about interpreting
    machine learning models in general, here is a piece of blog where I have dived
    deep into how SHAP works.'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可解释性：** 相比于神经网络，梯度提升机器可以说是模型拓扑中在复杂性和可解释性之间平衡较好的。如果你想了解更多关于解释机器学习模型的内容，这里有一篇博文，我深入探讨了
    SHAP 的工作原理。'
- en: '[](/shap-explain-any-machine-learning-model-in-python-72f0bea35f7c?source=post_page-----f5e0e013f441--------------------------------)
    [## SHAP: Explain Any Machine Learning Model in Python'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/shap-explain-any-machine-learning-model-in-python-72f0bea35f7c?source=post_page-----f5e0e013f441--------------------------------)
    [## SHAP: 用 Python 解释任何机器学习模型'
- en: Your Comprehensive Guide to SHAP, TreeSHAP, and DeepSHAP
  id: totrans-103
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 你的 SHAP、TreeSHAP 和 DeepSHAP 综合指南
- en: towardsdatascience.com](/shap-explain-any-machine-learning-model-in-python-72f0bea35f7c?source=post_page-----f5e0e013f441--------------------------------)
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/shap-explain-any-machine-learning-model-in-python-72f0bea35f7c?source=post_page-----f5e0e013f441--------------------------------)
- en: Cons of Gradient Boosting
  id: totrans-105
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 梯度提升的缺点
- en: '**Computational complexity:** Fitting/training a gradient boosting machine
    involves fitting oftern over 1,000 smaller weak learners. While keeping a weak
    learner small would reduce the training time, that would still aggregate very
    quickly as we start scaling the model.'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**计算复杂度：** 拟合/训练梯度提升机涉及通常拟合超过1,000个小弱学习者。虽然保持弱学习者较小可以减少训练时间，但当我们开始扩展模型时，这种时间仍会很快累积。'
- en: '**Overfitting:** Another possible drawback of having so many smaller weak learners
    is the risk of overfitting. One way of mitigating this would be to set aside a
    validation set or use cross validation for evaluating the model’s performance.'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**过拟合：** 另一个拥有这么多小弱学习者的可能缺点是过拟合的风险。减轻这种风险的一种方法是预留一个验证集或使用交叉验证来评估模型的表现。'
- en: '**Hyperparameter tuning:** This is perhaps one of the least mentioned cons
    of, in fact, any machine learning models. While models comes with some predefined
    hyperparameters e.g. maximum depth of tree = 3, these assumptions were made under
    certain statistical observations. Whether these assumptions are representative
    of our data problem, that is a completely different story. Hence it is critical
    that we look into not just the what’s of hyperparameters, but also the why’s.'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**超参数调优：** 这可能是实际上任何机器学习模型中最少提及的缺点之一。虽然模型具有一些预定义的超参数，例如树的最大深度=3，但这些假设是基于特定的统计观察做出的。这些假设是否代表了我们的数据问题，那是完全不同的故事。因此，我们不仅要关注超参数的“是什么”，还要关注“为什么”。'
- en: Conclusion
  id: totrans-109
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: There you go, a step-by-step tutorial data science guide on how gradient boosting
    works.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 就这样，一个逐步教程数据科学指南，讲解梯度提升的工作原理。
- en: '*Don’t stop there*'
  id: totrans-111
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*不要止步于此*'
- en: 'Like any domain, data science is a rabbit hole in which we need to constantly
    polish our thinking and acquire new knowledge to stand out from the crowd. If
    you would like to read more about my thoughts on different data science related
    topics, feel free to choose from the list below:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 像任何领域一样，数据科学是一个需要不断打磨思维和获取新知识的领域，以便在众人中脱颖而出。如果你想了解更多关于我对各种数据科学相关主题的看法，可以从下面的列表中选择：
- en: '[5 Mistakes Every Data Scientist Should Avoid](https://medium.com/towards-data-science/5-mistakes-every-data-scientist-should-avoid-7e3523f6a9ec)'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[每个数据科学家都应该避免的5个错误](https://medium.com/towards-data-science/5-mistakes-every-data-scientist-should-avoid-7e3523f6a9ec)'
- en: '[How To Call a Python Function With A String?](https://medium.com/towards-artificial-intelligence/python-trick-how-to-call-a-function-by-its-name-f35309469c66)'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[如何通过字符串调用Python函数？](https://medium.com/towards-artificial-intelligence/python-trick-how-to-call-a-function-by-its-name-f35309469c66)'
- en: '[Python Tricks: How to Check Table Merging with Pandas](https://medium.com/towards-data-science/python-tricks-how-to-check-table-merging-with-pandas-cae6b9b1d540)'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Python技巧：如何使用Pandas检查表格合并](https://medium.com/towards-data-science/python-tricks-how-to-check-table-merging-with-pandas-cae6b9b1d540)'
- en: '[ANN Recommendation System for Stock Selection](https://medium.com/towards-artificial-intelligence/ann-recommendation-system-for-stock-selection-c9751a3a0520)'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[股票选择的ANN推荐系统](https://medium.com/towards-artificial-intelligence/ann-recommendation-system-for-stock-selection-c9751a3a0520)'
- en: '[Genetic Algorithm for Trading Strategy Optimization in Python](https://medium.com/towards-artificial-intelligence/genetic-algorithm-for-trading-strategy-optimization-in-python-614eb660990d)'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Python中的交易策略优化遗传算法](https://medium.com/towards-artificial-intelligence/genetic-algorithm-for-trading-strategy-optimization-in-python-614eb660990d)'
- en: '[Efficient Conditional Logic on Pandas DataFrames](https://medium.com/towards-data-science/efficient-implementation-of-conditional-logic-on-pandas-dataframes-4afa61eb7fce)'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[在Pandas数据框上高效的条件逻辑实现](https://medium.com/towards-data-science/efficient-implementation-of-conditional-logic-on-pandas-dataframes-4afa61eb7fce)'
- en: '[3 Common Problems with Neural Network Initialisation](https://medium.com/towards-data-science/3-common-problems-with-neural-network-initialisation-5e6cacfcd8e6)'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[神经网络初始化的3个常见问题](https://medium.com/towards-data-science/3-common-problems-with-neural-network-initialisation-5e6cacfcd8e6)'
- en: Last but definitely not least, if I have missed/mistaken anything critical,
    please feel free to drop a comment or send me a DM through LinkedIn. Let’s keep
    the knowledge flowing and get better at this domain together!
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 最后但绝对不容忽视的是，如果我遗漏了或误解了任何关键内容，请随时在评论区留言或通过LinkedIn给我发私信。让我们一起保持知识流动，共同在这一领域进步！
- en: '[](https://www.linkedin.com/in/louis-chan-b55b9287?source=post_page-----f5e0e013f441--------------------------------)
    [## Louis Chan — Lead GCP Data & ML Engineer — Associate Director — KPMG UK |
    LinkedIn'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://www.linkedin.com/in/louis-chan-b55b9287?source=post_page-----f5e0e013f441--------------------------------)
    [## Louis Chan — 领先的GCP数据与机器学习工程师 — 副总监 — KPMG UK | LinkedIn'
- en: Ambitious, curious and creative individual with a strong belief in inter-connectivity
    between branches of knowledge and a…
  id: totrans-122
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 有雄心、好奇心强且富有创造力的个人，对知识各领域之间的**相互联系**有着强烈的信念以及…
- en: www.linkedin.com](https://www.linkedin.com/in/louis-chan-b55b9287?source=post_page-----f5e0e013f441--------------------------------)
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '[www.linkedin.com](https://www.linkedin.com/in/louis-chan-b55b9287?source=post_page-----f5e0e013f441--------------------------------)'
- en: References
  id: totrans-124
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: “A Gentle Introduction to Gradient Boosting” by Brownlee (2018)
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**《梯度提升的温和介绍》**，作者**Brownlee**（2018年）'
- en: “Gradient Boosting and XGBoost with Python” by Raschka (2017)
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**《使用 Python 的梯度提升与 XGBoost》**，作者**Raschka**（2017年）'
- en: “An Introduction to Statistical Learning” by James et al. (2013)
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**《统计学习简介》**，作者**James**等（2013年）'
- en: “Ensemble Learning” by Alpaydin (2010)
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**《集成学习》**，作者**Alpaydin**（2010年）'
- en: “Introduction to Boosting” by Schapire (2003)
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**《提升方法简介》**，作者**Schapire**（2003年）'
