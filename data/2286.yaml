- en: 'Practical Introduction to Transformer Models: BERT'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实用介绍 Transformer 模型：BERT
- en: 原文：[https://towardsdatascience.com/practical-introduction-to-transformer-models-bert-4715ed0deede?source=collection_archive---------6-----------------------#2023-07-17](https://towardsdatascience.com/practical-introduction-to-transformer-models-bert-4715ed0deede?source=collection_archive---------6-----------------------#2023-07-17)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/practical-introduction-to-transformer-models-bert-4715ed0deede?source=collection_archive---------6-----------------------#2023-07-17](https://towardsdatascience.com/practical-introduction-to-transformer-models-bert-4715ed0deede?source=collection_archive---------6-----------------------#2023-07-17)
- en: '![](../Images/15139494860ad96a134be1b20fb42fa3.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/15139494860ad96a134be1b20fb42fa3.png)'
- en: Photo by [Alex Padurariu](https://unsplash.com/@alexpadurariu?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源于 [Alex Padurariu](https://unsplash.com/@alexpadurariu?utm_source=medium&utm_medium=referral)
    在 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: '[Hands-on Tutorials](https://towardsdatascience.com/tagged/hands-on-tutorials)'
  id: totrans-4
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '[动手教程](https://towardsdatascience.com/tagged/hands-on-tutorials)'
- en: Hands-on tutorial on how to build your first sentiment analysis model using
    BERT
  id: totrans-5
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 动手教程：如何使用 BERT 构建你的第一个情感分析模型
- en: '[](https://medium.com/@shashank.kapadia?source=post_page-----4715ed0deede--------------------------------)[![Shashank
    Kapadia](../Images/347e4cb92a7d27f032c5761e4526f2fa.png)](https://medium.com/@shashank.kapadia?source=post_page-----4715ed0deede--------------------------------)[](https://towardsdatascience.com/?source=post_page-----4715ed0deede--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----4715ed0deede--------------------------------)
    [Shashank Kapadia](https://medium.com/@shashank.kapadia?source=post_page-----4715ed0deede--------------------------------)'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@shashank.kapadia?source=post_page-----4715ed0deede--------------------------------)[![Shashank
    Kapadia](../Images/347e4cb92a7d27f032c5761e4526f2fa.png)](https://medium.com/@shashank.kapadia?source=post_page-----4715ed0deede--------------------------------)[](https://towardsdatascience.com/?source=post_page-----4715ed0deede--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----4715ed0deede--------------------------------)
    [Shashank Kapadia](https://medium.com/@shashank.kapadia?source=post_page-----4715ed0deede--------------------------------)'
- en: ·
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fcc7314ace45c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpractical-introduction-to-transformer-models-bert-4715ed0deede&user=Shashank+Kapadia&userId=cc7314ace45c&source=post_page-cc7314ace45c----4715ed0deede---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----4715ed0deede--------------------------------)
    ·7 min read·Jul 17, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F4715ed0deede&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpractical-introduction-to-transformer-models-bert-4715ed0deede&user=Shashank+Kapadia&userId=cc7314ace45c&source=-----4715ed0deede---------------------clap_footer-----------)'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fcc7314ace45c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpractical-introduction-to-transformer-models-bert-4715ed0deede&user=Shashank+Kapadia&userId=cc7314ace45c&source=post_page-cc7314ace45c----4715ed0deede---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----4715ed0deede--------------------------------)
    ·7分钟阅读·2023年7月17日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F4715ed0deede&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpractical-introduction-to-transformer-models-bert-4715ed0deede&user=Shashank+Kapadia&userId=cc7314ace45c&source=-----4715ed0deede---------------------clap_footer-----------)'
- en: --
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4715ed0deede&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpractical-introduction-to-transformer-models-bert-4715ed0deede&source=-----4715ed0deede---------------------bookmark_footer-----------)'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4715ed0deede&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpractical-introduction-to-transformer-models-bert-4715ed0deede&source=-----4715ed0deede---------------------bookmark_footer-----------)'
- en: '*Preface: This article presents a summary of information about the given topic.
    It should not be considered original research. The information and code included
    in this article have may be influenced by things I have read or seen in the past
    from various online articles, research papers, books, and open-source code.*'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '*前言：本文总结了关于给定主题的信息。它不应被视为原创研究。本文中包含的信息和代码可能受到我过去从各种在线文章、研究论文、书籍和开源代码中阅读或看到的内容的影响。*'
- en: Table of Content
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 目录
- en: Introduction to BERT
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: BERT 简介
- en: Pre-Training and Fine-Tuning
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预训练和微调
- en: 'Hands On: Using BERT for sentiment analysis'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实践操作：使用 BERT 进行情感分析
- en: Interpreting Results
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 解释结果
- en: Closing Thoughts
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 结束语
- en: In NLP, the transformer model architecture has been a revolutionary that greatly
    enhanced the ability to understand and generate textual information.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在 NLP 中，变换器模型架构是一项革命性的进展，大大增强了理解和生成文本信息的能力。
- en: In this tutorial, we are going to dig-deep into BERT, a well-known transformer-based
    model, and provide an hands-on example to fine-tune the base BERT model for sentiment
    analysis.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在本教程中，我们将深入探讨 BERT，这是一种著名的基于变换器的模型，并提供一个实际的示例来微调基础 BERT 模型以进行情感分析。
- en: Introduction to BERT
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: BERT 介绍
- en: BERT, introduced by researchers at Google in 2018, is a powerful language model
    that uses transformer architecture. Pushing the boundaries of earlier model architecture,
    such as LSTM and GRU, that were either unidirectional or sequentially bi-directional,
    BERT considers context from both past and future simultaneously. This is due to
    the innovative “attention mechanism,” which allows the model to weigh the importance
    of words in a sentence when generating representations.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: BERT 由 Google 研究人员于 2018 年推出，是一种强大的语言模型，使用变换器架构。与早期的 LSTM 和 GRU 等单向或顺序双向模型架构相比，BERT
    同时考虑过去和未来的上下文。这得益于创新的“注意力机制”，它使模型在生成表示时能够权衡句子中单词的重要性。
- en: 'The BERT model is pre-trained on the following two NLP tasks:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: BERT 模型在以下两个 NLP 任务上进行了预训练：
- en: Masked Language Model (MLM)
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 掩码语言模型（MLM）
- en: Next Sentence Prediction (NSP)
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 下一句预测（NSP）
- en: and is generally used as the base model for various downstream NLP tasks, such
    as sentiment analysis which we will cover in this tutorial.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 并且通常被用作各种下游 NLP 任务的基础模型，例如我们在本教程中将涵盖的情感分析。
- en: Pre-Training and Fine-Tuning
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 预训练和微调
- en: 'The power of BERT comes from its two-step process:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: BERT 的强大之处在于它的两步过程：
- en: '**Pre-training** is the phase where BERT is trained on large amounts of data.
    As a result, it learns to predict masked words in a sentence (MLM task) and to
    predict if a sentence follows another one (NSP task). The output of this stage
    is a a pre-trained NLP model with a general-purpose “understanding” of the language'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**预训练**是 BERT 在大量数据上进行训练的阶段。因此，它学会了预测句子中的掩码词（MLM 任务）和预测一个句子是否跟随另一个句子（NSP 任务）。这一阶段的输出是一个具有通用“语言理解”能力的预训练
    NLP 模型。'
- en: '**Fine-tuning** is where the pre-trained BERT model is further trained on a
    specific task. The model is initialized with the pre-trained parameters, and the
    entire model is trained on a downstream task, allowing BERT to fine-tune its understanding
    of language to the specifics of the task at hand.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**微调**是在特定任务上进一步训练预训练的 BERT 模型。模型以预训练的参数进行初始化，并在下游任务上对整个模型进行训练，使 BERT 能够根据当前任务的具体情况微调其语言理解能力。'
- en: 'Hands On: Using BERT for sentiment analysis'
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实践操作：使用 BERT 进行情感分析
- en: The complete code is available as a [Jupyter Notebook on GitHub](https://github.com/kapadias/medium-articles/blob/master/natural-language-processing/transformers-series/sentiment_analysis_bert.ipynb)
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 完整代码可以作为 [GitHub 上的 Jupyter Notebook](https://github.com/kapadias/medium-articles/blob/master/natural-language-processing/transformers-series/sentiment_analysis_bert.ipynb)获得
- en: 'In this hands-on exercise, we will train the sentiment analysis model on the
    IMDB movie reviews dataset [4] [(license: Apache 2.0)](https://github.com/huggingface/datasets/blob/main/LICENSE),
    which comes labeled whether a review is positive or negative. We will also load
    the model using the Hugging Face’s transformers library.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个动手练习中，我们将使用 IMDB 电影评论数据集[4] [(许可证：Apache 2.0)](https://github.com/huggingface/datasets/blob/main/LICENSE)来训练情感分析模型，该数据集标记了评论是积极还是消极。我们还将使用
    Hugging Face 的 transformers 库加载模型。
- en: Let’s load all the libraries
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们加载所有库
- en: '[PRE0]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: First, we need to load the dataset and the model tokenizer.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要加载数据集和模型分词器。
- en: '[PRE1]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Next, we’ll create a plot to see the distribution of the positive and negative
    classes.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将创建一个图表，以查看积极和消极类别的分布。
- en: '[PRE2]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '![](../Images/a751c4f02714c162279b8e2dbac1071c.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a751c4f02714c162279b8e2dbac1071c.png)'
- en: Fig 1\. Class distribution of the training dataset
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1\. 训练数据集的类别分布
- en: Next, we preprocess our dataset by tokenizing the texts. We use BERT’s tokenizer,
    which will convert the text into tokens that correspond to BERT’s vocabulary.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们通过对文本进行分词来预处理我们的数据集。我们使用 BERT 的分词器，将文本转换为与 BERT 词汇表对应的标记。
- en: '[PRE3]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '![](../Images/ae0915081c37ac5286fdd5ded5e033d4.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ae0915081c37ac5286fdd5ded5e033d4.png)'
- en: After that, we prepare our training and evaluation datasets. Remember, if you
    want to use all the data, you can set the `num_samples` variable to `-1`.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 之后，我们准备训练和评估数据集。请记住，如果你想使用所有数据，可以将`num_samples`变量设置为`-1`。
- en: '[PRE4]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Then, we load the pre-trained BERT model. We’ll use the `AutoModelForSequenceClassification`
    class, a BERT model designed for classification tasks.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们加载预训练的BERT模型。我们将使用`AutoModelForSequenceClassification`类，这是一种针对分类任务设计的BERT模型。
- en: For this tutorial, we use the ‘[bert-base-uncased](https://huggingface.co/bert-base-uncased)’
    version of BERT, which is trained on lower-case English text, is used for this
    tutorial.
  id: totrans-47
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 在本教程中，我们使用‘[bert-base-uncased](https://huggingface.co/bert-base-uncased)’版本的BERT，它是基于小写英文文本进行训练的。
- en: '[PRE5]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Now, we’re ready to define our training arguments and create a `Trainer` instance
    to train our model.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们准备定义我们的训练参数并创建一个`Trainer`实例来训练我们的模型。
- en: '[PRE6]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Interpreting Results
  id: totrans-51
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结果解释
- en: Having trained our model, let’s evaluate it. We’ll calculate the confusion matrix
    and the ROC curve to understand how well our model performs.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 训练完模型后，让我们来评估一下。我们将计算混淆矩阵和ROC曲线，以了解我们的模型表现如何。
- en: '[PRE7]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '![](../Images/cb104a044e899ad272b1ce2d8f888394.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/cb104a044e899ad272b1ce2d8f888394.png)'
- en: Fig 2\. Confusion Matrix
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2\. 混淆矩阵
- en: '![](../Images/aa9c8ed466a7fc3bef2b259d818a359d.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/aa9c8ed466a7fc3bef2b259d818a359d.png)'
- en: Fig 3\. ROC curve
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3\. ROC曲线
- en: The confusion matrix gives a detailed breakdown of how our predictions measure
    up to the actual labels, while the ROC curve shows us the trade-off between the
    true positive rate (sensitivity) and the false positive rate (1 — specificity)
    at various threshold settings.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 混淆矩阵详细地分解了我们的预测与实际标签的匹配情况，而ROC曲线则展示了在不同阈值设置下，真正例率（敏感度）与假正例率（1 — 特异度）之间的权衡。
- en: Finally, to see our model in action, let’s use it to infer the sentiment of
    a sample text.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，为了查看我们的模型的实际效果，让我们用它来推断样本文本的情感。
- en: '[PRE8]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '![](../Images/84a2556951138b36dc6bda8b93d1a4a9.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/84a2556951138b36dc6bda8b93d1a4a9.png)'
- en: Closing Thoughts
  id: totrans-62
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结语
- en: By walking through an example of sentiment analysis on IMDb movie reviews, I
    hope you’ve gained a clear understanding of how to apply BERT to real-world NLP
    problems. The Python code I’ve included here can be adjusted and extended to tackle
    different tasks and datasets, paving the way for even more sophisticated and accurate
    language models.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 通过对IMDb电影评论的情感分析示例进行讲解，我希望你能清晰地理解如何将BERT应用于现实世界的自然语言处理问题。我在这里包含的Python代码可以进行调整和扩展，以应对不同的任务和数据集，为更复杂和准确的语言模型铺平道路。
- en: References
  id: totrans-64
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] Devlin, J., Chang, M. W., Lee, K., & Toutanova, K. (2018). BERT: Pre-training
    of Deep Bidirectional Transformers for Language Understanding. arXiv preprint
    arXiv:1810.04805'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] Devlin, J., Chang, M. W., Lee, K., & Toutanova, K. (2018). BERT：用于语言理解的深度双向变换器的预训练。arXiv预印本
    arXiv:1810.04805'
- en: '[2] Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez,
    A. N., … & Polosukhin, I. (2017). Attention is all you need. In Advances in neural
    information processing systems (pp. 5998–6008).'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez,
    A. N., … & Polosukhin, I. (2017). 注意力机制即一切。在《神经信息处理系统进展》中（第5998–6008页）。'
- en: '[3] Wolf, T., Debut, L., Sanh, V., Chaumond, J., Delangue, C., Moi, A., … &
    Rush, A. M. (2019). Huggingface’s transformers: State-of-the-art natural language
    processing. ArXiv, abs/1910.03771.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] Wolf, T., Debut, L., Sanh, V., Chaumond, J., Delangue, C., Moi, A., … &
    Rush, A. M. (2019). Huggingface的变换器：最先进的自然语言处理。ArXiv, abs/1910.03771。'
- en: '[4] Lhoest, Q., Villanova del Moral, A., Jernite, Y., Thakur, A., von Platen,
    P., Patil, S., Chaumond, J., Drame, M., Plu, J., Tunstall, L., Davison, J., Šaško,
    M., Chhablani, G., Malik, B., Brandeis, S., Le Scao, T., Sanh, V., Xu, C., Patry,
    N., McMillan-Major, A., Schmid, P., Gugger, S., Delangue, C., Matussière, T.,
    Debut, L., Bekman, S., Cistac, P., Goehringer, T., Mustar, V., Lagunas, F., Rush,
    A., & Wolf, T. (2021). Datasets: A Community Library for Natural Language Processing.
    In Proceedings of the 2021 Conference on Empirical Methods in Natural Language
    Processing: System Demonstrations (pp. 175–184). Online and Punta Cana, Dominican
    Republic: Association for Computational Linguistics. Retrieved from [https://aclanthology.org/2021.emnlp-demo.21](https://aclanthology.org/2021.emnlp-demo.21)'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] Lhoest, Q., Villanova del Moral, A., Jernite, Y., Thakur, A., von Platen,
    P., Patil, S., Chaumond, J., Drame, M., Plu, J., Tunstall, L., Davison, J., Šaško,
    M., Chhablani, G., Malik, B., Brandeis, S., Le Scao, T., Sanh, V., Xu, C., Patry,
    N., McMillan-Major, A., Schmid, P., Gugger, S., Delangue, C., Matussière, T.,
    Debut, L., Bekman, S., Cistac, P., Goehringer, T., Mustar, V., Lagunas, F., Rush,
    A., & Wolf, T. (2021). 数据集：自然语言处理的社区库。收录于 2021 年自然语言处理实证方法会议：系统演示 (第 175–184 页)。在线和多米尼加共和国蓬塔卡纳：计算语言学协会。取自
    [https://aclanthology.org/2021.emnlp-demo.21](https://aclanthology.org/2021.emnlp-demo.21)'
- en: Thanks for reading. *If you have any feedback, please feel to reach out by commenting
    on this post, messaging me on* [*LinkedIn*](https://www.linkedin.com/in/shashankkapadia/)*,
    or shooting me an email (smhkapadia[at]gmail.com)*
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 感谢阅读。*如果您有任何反馈，请随时通过评论此帖子、在* [*LinkedIn*](https://www.linkedin.com/in/shashankkapadia/)*上给我留言，或发送电子邮件
    (smhkapadia[at]gmail.com)*
- en: '*If you enjoyed this article, visit my other articles*'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '*如果您喜欢这篇文章，请访问我的其他文章*'
- en: '[](/domain-adaption-fine-tune-pre-trained-nlp-models-a06659ca6668?source=post_page-----4715ed0deede--------------------------------)
    [## Domain Adaption: Fine-Tune Pre-Trained NLP Models'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/domain-adaption-fine-tune-pre-trained-nlp-models-a06659ca6668?source=post_page-----4715ed0deede--------------------------------)
    [## 领域适应：微调预训练的 NLP 模型'
- en: A step-by-step guide to fine-tuning pre-trained NLP models for any domain
  id: totrans-72
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 逐步指南：为任何领域微调预训练的 NLP 模型
- en: towardsdatascience.com](/domain-adaption-fine-tune-pre-trained-nlp-models-a06659ca6668?source=post_page-----4715ed0deede--------------------------------)
    [](https://medium.com/aimonks/the-evolution-of-natural-language-processing-56ce27916e10?source=post_page-----4715ed0deede--------------------------------)
    [## The Evolution of Natural Language Processing
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/domain-adaption-fine-tune-pre-trained-nlp-models-a06659ca6668?source=post_page-----4715ed0deede--------------------------------)
    [](https://medium.com/aimonks/the-evolution-of-natural-language-processing-56ce27916e10?source=post_page-----4715ed0deede--------------------------------)
    [## 自然语言处理的演变
- en: A Historical Perspective on the Development of Language Models
  id: totrans-74
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 语言模型发展的历史视角
- en: 'medium.com](https://medium.com/aimonks/the-evolution-of-natural-language-processing-56ce27916e10?source=post_page-----4715ed0deede--------------------------------)
    [](/recommendation-system-in-python-lightfm-61c85010ce17?source=post_page-----4715ed0deede--------------------------------)
    [## Recommendation System in Python: LightFM'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: medium.com](https://medium.com/aimonks/the-evolution-of-natural-language-processing-56ce27916e10?source=post_page-----4715ed0deede--------------------------------)
    [](/recommendation-system-in-python-lightfm-61c85010ce17?source=post_page-----4715ed0deede--------------------------------)
    [## Python 中的推荐系统：LightFM
- en: A Step-by-Step guide to building a recommender system in Python using LightFM
  id: totrans-76
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用 LightFM 在 Python 中构建推荐系统的逐步指南
- en: 'towardsdatascience.com](/recommendation-system-in-python-lightfm-61c85010ce17?source=post_page-----4715ed0deede--------------------------------)
    [](/evaluate-topic-model-in-python-latent-dirichlet-allocation-lda-7d57484bb5d0?source=post_page-----4715ed0deede--------------------------------)
    [## Evaluate Topic Models: Latent Dirichlet Allocation (LDA)'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/recommendation-system-in-python-lightfm-61c85010ce17?source=post_page-----4715ed0deede--------------------------------)
    [](/evaluate-topic-model-in-python-latent-dirichlet-allocation-lda-7d57484bb5d0?source=post_page-----4715ed0deede--------------------------------)
    [## 评估主题模型：潜在狄利克雷分配 (LDA)
- en: A step-by-step guide to building interpretable topic models
  id: totrans-78
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 构建可解释的主题模型的逐步指南
- en: towardsdatascience.com](/evaluate-topic-model-in-python-latent-dirichlet-allocation-lda-7d57484bb5d0?source=post_page-----4715ed0deede--------------------------------)
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/evaluate-topic-model-in-python-latent-dirichlet-allocation-lda-7d57484bb5d0?source=post_page-----4715ed0deede--------------------------------)
