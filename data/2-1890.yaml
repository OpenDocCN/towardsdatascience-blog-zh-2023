- en: 'Machines That Learn Like Us: Solving the Generalization-Memorization Dilemma'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 像我们一样学习的机器：解决泛化-记忆困境
- en: 原文：[https://towardsdatascience.com/solving-machine-learnings-generalization-memorization-dilemma-3-promising-paradigms-ab9c236add3e](https://towardsdatascience.com/solving-machine-learnings-generalization-memorization-dilemma-3-promising-paradigms-ab9c236add3e)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/solving-machine-learnings-generalization-memorization-dilemma-3-promising-paradigms-ab9c236add3e](https://towardsdatascience.com/solving-machine-learnings-generalization-memorization-dilemma-3-promising-paradigms-ab9c236add3e)
- en: 3 paradigms to solve one of the most important problems in Machine Learning
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决机器学习中最重要问题的3种范式
- en: '[](https://medium.com/@samuel.flender?source=post_page-----ab9c236add3e--------------------------------)[![Samuel
    Flender](../Images/390d82a673de8a8bb11cef66978269b5.png)](https://medium.com/@samuel.flender?source=post_page-----ab9c236add3e--------------------------------)[](https://towardsdatascience.com/?source=post_page-----ab9c236add3e--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----ab9c236add3e--------------------------------)
    [Samuel Flender](https://medium.com/@samuel.flender?source=post_page-----ab9c236add3e--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@samuel.flender?source=post_page-----ab9c236add3e--------------------------------)[![Samuel
    Flender](../Images/390d82a673de8a8bb11cef66978269b5.png)](https://medium.com/@samuel.flender?source=post_page-----ab9c236add3e--------------------------------)[](https://towardsdatascience.com/?source=post_page-----ab9c236add3e--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----ab9c236add3e--------------------------------)
    [Samuel Flender](https://medium.com/@samuel.flender?source=post_page-----ab9c236add3e--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----ab9c236add3e--------------------------------)
    ·7 min read·Apr 5, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----ab9c236add3e--------------------------------)
    ·7分钟阅读·2023年4月5日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/d07a01c4830a1e5c49f4bbf135d147d5.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d07a01c4830a1e5c49f4bbf135d147d5.png)'
- en: (Midjourney)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: (Midjourney)
- en: The “holy grail” of Machine Learning is the ability to build systems that can
    both memorize known patterns in the training data as well as generalize to unknown
    patterns in the wild.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习的“圣杯”是能够构建既能记忆训练数据中的已知模式，又能泛化到未知模式的系统。
- en: It’s the holy grail because this is how we humans learn as well. You can recognize
    your grandma in an old photo, but you could also recognize a Xoloitzcuintli as
    a dog even though you’ve never actually seen one before. Without memorization
    we’d have to constantly re-learn everything from scratch, and without generalization
    we wouldn’t be able to adapt to an ever-changing world. To survive, we need both.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 之所以称之为圣杯，是因为这也是我们人类学习的方式。你可以在一张旧照片中识别出你的奶奶，但即使你从未见过Xoloitzcuintli，你也能将其识别为狗。如果没有记忆，我们就得不断从头再学一遍一切；如果没有泛化，我们将无法适应不断变化的世界。为了生存，我们需要两者兼备。
- en: 'Traditional statistical learning theory tells us that this is impossible: models
    can either generalize well or memorize well, but not both. It’s the well-known
    bias-variance trade-off, one of the first things we learn in standard ML curricula.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 传统的统计学习理论告诉我们这是不可能的：模型要么能够很好地泛化，要么能够很好地记忆，但不能兼顾两者。这就是著名的偏差-方差权衡，这是我们在标准机器学习课程中首先学到的东西。
- en: How then can we build such universal learning systems? Is the holy grail within
    reach?
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 那么我们如何构建这样的通用学习系统呢？圣杯是否触手可及？
- en: In this post, let’s dive into 3 paradigms from the literature,
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，让我们深入探讨文献中的3种范式，
- en: Generalize first, memorize later
  id: totrans-13
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 先泛化，后记忆
- en: Generalize and memorize at the same time
  id: totrans-14
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 同时泛化和记忆
- en: Generalize with machines, memorize with humans
  id: totrans-15
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 机器泛化，人类记忆
- en: Let’s get started.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始吧。
- en: 1 — Generalize first, memorize later
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 1 — 先泛化，后记忆
- en: '[BERT](/what-exactly-happens-when-we-fine-tune-bert-f5dc32885d76) revolutionized
    Machine Leaning with its introduction of the pre-training/fine-tuning paradigm:
    after pre-training in an unsupervised way on a massive amount of text data, the
    model can be rapidly fine-tuned on a specific downstream task with relatively
    fewer labels.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '[BERT](/what-exactly-happens-when-we-fine-tune-bert-f5dc32885d76)通过引入预训练/微调范式彻底改变了机器学习：在大量文本数据上进行无监督的预训练后，该模型可以在特定的下游任务上快速微调，所需的标签相对较少。'
- en: Surprisingly, this pre-training/fine-tuning approach turns out to solve the
    generalization/memorization problem. BERT can generalize as well as memorize,
    show Michael Tänzer and collaborators from the Imperial College London in a 2022
    [paper](https://aclanthology.org/2022.acl-long.521/).
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 惊人的是，这种预训练/微调方法解决了泛化/记忆化问题。BERT 能够进行泛化和记忆，正如来自帝国理工学院的 Michael Tänzer 和他的合作者在
    2022 年的 [论文](https://aclanthology.org/2022.acl-long.521/) 中展示的那样。
- en: 'In particular, the authors show that during fine-tuning, BERT learns in 3 distinct
    phases:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 特别是，作者展示了在微调过程中，BERT 学习的 3 个不同的阶段：
- en: 'Fitting (epoch 1): the model learns simple, generic pattens that explain as
    much of the training data as possible. During this phase, both training and validation
    performance increases.'
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 拟合（训练轮次 1）：模型学习简单的通用模式，尽可能解释训练数据。在此阶段，训练和验证性能都在提高。
- en: 'Setting (epochs 2–5): there are no more simple patterns left to learn. Both
    training and validation performance saturate, forming a plateau in the learning
    curve.'
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置（训练轮次 2–5）：没有更多简单的模式可以学习。训练和验证性能饱和，在学习曲线上形成一个平台。
- en: 'Memorization (epochs 6+): the model starts to memorize specific examples in
    the training set, including noise, which improves training performance but degrades
    validation performance.'
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 记忆化（训练轮次 6+）：模型开始记忆训练集中的特定示例，包括噪声，这提高了训练性能但降低了验证性能。
- en: 'How did they figure this out? By starting with a noise-free training set (CoNLL03,
    a named-entity-recognition benchmark dataset), and then gradually introducing
    more and more artificial label noise. Comparing the learning curves with different
    amounts of noise clearly reveals the 3 distinct phases: more noise results in
    a steeper drop during phase 3.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 他们是如何发现这一点的？通过从无噪声的训练集（CoNLL03，一个命名实体识别基准数据集）开始，然后逐渐引入更多的人工标签噪声。比较不同噪声量的学习曲线清楚地揭示了
    3 个不同的阶段：更多噪声会导致第 3 阶段的下降更加陡峭。
- en: '![](../Images/a4f7edec8029dad338c529bd40303703.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a4f7edec8029dad338c529bd40303703.png)'
- en: Fine-tuning BERT exhibits 3 distinct learning phases. Figure taken from Tänzer
    et al, “Memorisation versus Generalisation in Pre-trained Language Models” ([link](https://aclanthology.org/2022.acl-long.521/))
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 微调 BERT 显示了 3 个不同的学习阶段。图来自 Tänzer 等人，“预训练语言模型中的记忆化与泛化” ([link](https://aclanthology.org/2022.acl-long.521/))
- en: 'Tänzer et al also show that memorization in BERT requires repetition: BERT
    memorizes a specific training example only once it has seen that example a certain
    number of times. This can be deduced from the learning curve for the artificially
    introduced noise: it’s a step function, which improves with each epoch. In other
    words, during phase 3 BERT can eventually memorize the entire training set, if
    we just let it train for a sufficient number of epochs.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: Tänzer 等人还表明，BERT 的记忆化需要重复：BERT 只有在看到特定训练示例一定次数后才会记住该示例。这可以从人为引入噪声的学习曲线中推断出来：它是一个阶跃函数，每个训练轮次都会改善。换句话说，在第
    3 阶段，如果我们让 BERT 训练足够多的轮次，它最终可以记住整个训练集。
- en: 'BERT, in conclusion, appears to generalize first and memorize later, as evidenced
    by the observation of its 3 distinct learning phases during fine-tuning. In fact,
    it can also be shown that this behavior is a direct consequence of pre-training:
    Tänzer et al show that a randomly initialized BERT model does not share the same
    3 learning phases. This leads to the conclusion that the pre-training/fine-tuning
    paradigm may be a possible solution to the generalization/memorization dilemma.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，BERT 似乎首先进行泛化，然后进行记忆，这从其在微调过程中观察到的 3 个不同的学习阶段可以得到证实。事实上，还可以证明这种行为是预训练的直接结果：Tänzer
    等人展示了一个随机初始化的 BERT 模型不会共享相同的 3 个学习阶段。这导致了预训练/微调范式可能是解决泛化/记忆化困境的一个可能方案的结论。
- en: 2 — Generalize and memorize at the same time
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 2 — 同时进行泛化和记忆
- en: Let’s leave the world of natural language processing and enter the world of
    [recommender systems](https://medium.com/towards-data-science/biases-in-recommender-systems-top-challenges-and-recent-breakthroughs-edcda59d30bf).
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们离开自然语言处理的世界，进入 [推荐系统](https://medium.com/towards-data-science/biases-in-recommender-systems-top-challenges-and-recent-breakthroughs-edcda59d30bf)
    的世界。
- en: In modern recommender systems, the ability to memorize and generalize at the
    same time is critical. YouTube, for example, wants to show you videos that are
    similar to the ones you’ve watched in the past (memorization), but also new ones
    that are little bit different and you didn’t even knew you’d like (generalization).
    Without memorization you’d get frustrated, and without generalization you’d get
    bored.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在现代推荐系统中，同时具备记忆和泛化的能力至关重要。例如，YouTube 想向你展示与你过去观看的视频类似的内容（记忆），但也展示一些你甚至不知道自己会喜欢的新视频（泛化）。没有记忆你会感到沮丧，没有泛化你会感到无聊。
- en: The best recommender systems today need to do both. But how?
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 目前最好的推荐系统需要同时做到这两点。但是怎么做呢？
- en: In a 2016 [paper](https://arxiv.org/abs/1606.07792), Heng-Tze Cheng and collaborators
    from Google propose what they call “Wide and Deep Learning” to address this problem.
    The key idea is to build a single neural network that has both a deep component
    (a deep neural net with embedding inputs) for generalization as well as a wide
    component (a linear model with a large number of sparse inputs) for memorization.
    The authors demonstrate the effectiveness of this approach on recommendations
    within the Google Play store, which recommends apps to users.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在 2016 年的[论文](https://arxiv.org/abs/1606.07792)中，Heng-Tze Cheng 和来自 Google 的合作者提出了他们所称的“宽广而深度学习”来解决这个问题。关键思想是构建一个包含深度组件（具有嵌入输入的深度神经网络）用于泛化以及宽广组件（具有大量稀疏输入的线性模型）用于记忆的单一神经网络。作者在
    Google Play 商店的推荐中演示了这种方法的有效性，该商店向用户推荐应用程序。
- en: The inputs to the deep component are dense features as well as embeddings of
    categorical features such as user language, user gender, impressed app, installed
    apps, and so on. These embeddings are initialized randomly, and then tuned during
    model training along with the other parameters in the neural network.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 深度组件的输入是密集特征以及分类特征的嵌入，例如用户语言、用户性别、印象中的应用程序、已安装的应用程序等。这些嵌入被随机初始化，然后在模型训练过程中与神经网络中的其他参数一起调整。
- en: The inputs to the wide component of the network are granular cross-features
    such as
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 网络宽广组件的输入是细粒度的交叉特征，例如
- en: '[PRE0]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: the value of which is 1 if the user has Netflix installed and the impressed
    app is Hulu.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 如果用户安装了Netflix且印象中的应用程序是Hulu，则其值为1。
- en: 'It’s easy to see why the wide component enables a form of memorization: if
    99% of users that install Netflix also end up installing Hulu, the wide component
    will be able to learn this piece of information, while it may get lost in the
    deep component. Having both the wide and deep components really is the key to
    peak performance, argue the Cheng et al.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 很容易看出为什么宽广组件能实现一种记忆：如果 99% 的安装了 Netflix 的用户也最终安装了 Hulu，宽广组件将能够学习这一信息，而它可能会在深度组件中丢失。
    Cheng 等人认为，同时拥有宽广和深度组件确实是达到最佳性能的关键。
- en: And in fact, the experimental results confirm the authors’ hypothesis. The wide
    & deep model outperformed both a wide-only model (by 2.9%) and a deep-only model
    (by 1%) in terms of online acquisition gain in the Google Play store. These experimental
    results indicate that “Wide & Deep” is another promising paradigm to solve the
    generalization/memorization dilemma.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 实验结果实际上证实了作者的假设。宽广&深度模型在 Google Play 商店的在线获取增益方面优于仅宽广模型（提高了 2.9%）和仅深度模型（提高了
    1%）。这些实验结果表明，“宽广&深度”是解决泛化/记忆困境的另一种有前途的范式。
- en: 3 — Generalize with machines, memorize with humans
  id: totrans-40
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 3 — 让机器泛化，让人类记忆
- en: 'Both Tänzer and Cheng proposed approaches to solve the generalization/memorization
    dilemma with machines alone. However, machines have a hard time memorizing singular
    examples: Tänzer et al find that BERT requires at least 25 instances of a class
    to learn to predict it at all and 100 examples to predict it “with some accuracy”.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: Tänzer 和 Cheng 提出了仅用机器解决泛化/记忆困境的方法。然而，机器很难记住单个示例：Tänzer 等人发现 BERT 需要至少 25 个实例来学习预测一个类别，且需要
    100 个示例才能“有一定准确性”地预测。
- en: Taking a step back, we don’t have to let machines do all of the work. Instead
    of fighting our machines’ failure to memorize, why not embrace it? Why not build
    a hybrid system that combines ML with human expertise?
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 退一步说，我们不必让机器完成所有的工作。与其对抗机器的记忆失败，不如接受它？为什么不构建一个结合了机器学习和人类专业知识的混合系统呢？
- en: That’s precisely the idea behind Chimera, Walmart’s production system for large-scale
    e-commerce item classification, presented in a 2014 [paper](https://pages.cs.wisc.edu/~anhai/papers/chimera-vldb14.pdf)
    by Chong Sun and collaborators from Walmart Labs. The premise behind Chimera is
    that Machine Learning alone is not the enough to handle item classification at
    scale due to the existence of a large number of edge cases with little training
    data.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 这正是 Chimera 背后的理念，Chimera 是沃尔玛用于大规模电子商务商品分类的生产系统，由 Chong Sun 和沃尔玛实验室的合作者在 2014
    年的[论文](https://pages.cs.wisc.edu/~anhai/papers/chimera-vldb14.pdf)中提出。Chimera
    的前提是，仅凭机器学习不足以处理大规模的商品分类，因为存在大量训练数据稀缺的边缘案例。
- en: For example, Walmart may agree to carry a limited number of new products from
    a new vendor on a trial basis. An ML system may not be able to accurately classify
    these products because there’s not enough training data. However, human analysts
    can write rules to cover these cases precisely. These rule-based decisions can
    then later be used in the model training, so that after some time the model can
    catch up to the new patterns.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，沃尔玛可能同意试验性地从新供应商那里采购有限数量的新产品。由于缺乏足够的训练数据，机器学习系统可能无法准确分类这些产品。然而，人类分析师可以编写规则来准确覆盖这些情况。这些基于规则的决策随后可以用于模型训练，使模型在经过一段时间后能够适应新模式。
- en: The authors conclude;
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 作者总结道：
- en: We use both machine learning and hand-crafted rules extensively. Rules in our
    system are not “nice to have”. They are absolutely essential to achieving the
    desired performance, and they give domain analysts a fast and effective way to
    provide feedback into the system. As far as we know, we are the first to describe
    an industrial-strength system where both [Machine] learning and rules co-exist
    as first-class citizens.
  id: totrans-46
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 我们广泛使用机器学习和手工编写的规则。我们系统中的规则不是“锦上添花”。它们对实现预期性能至关重要，并且为领域分析师提供了一种快速有效的方式将反馈输入系统。据我们所知，我们是第一个描述一个工业级系统的团队，在其中[机器学习](https://medium.com/towards-data-science/the-origin-of-intelligent-behavior-3d3f2f659dc2)和规则作为一流公民共存。
- en: Alas, this co-existence may also be the key to solving the generalization/memorization
    problem.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 唉，这种共存也可能是解决概括/记忆问题的关键。
- en: 'Coda: systems that learn like us'
  id: totrans-48
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 附录：像我们一样学习的系统
- en: 'Let’s recap. Building systems that can both memorize known patterns and generalize
    to unknown ones is the holy grail of Machine Learning. As of today, no one has
    yet cracked this problem completely, but we’ve seen a few promising directions:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 总结一下。构建能够同时记忆已知模式和对未知模式进行概括的系统是机器学习的**圣杯**。到目前为止，还没有人完全破解这个问题，但我们已经看到了一些有前景的方向：
- en: BERT has been shown to generalize first and memorize later during fine-tuning,
    a capability that’s possible due to being pre-trained beforehand.
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: BERT 已被证明在微调过程中先进行概括，再进行记忆，这种能力得益于其之前的预训练。
- en: Wide & Deep neural networks have been designed to both generalize (using the
    deep component) and memorize (using the wide component) at the same time, outperforming
    both wide-only and deep-only networks in Google Play store recommendations.
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wide & Deep 神经网络被设计为同时进行概括（使用深度组件）和记忆（使用宽度组件），在 Google Play 商店推荐中优于仅宽度或仅深度网络。
- en: Walmart’s hybrid production system Chimera leverages human experts to write
    rules for edge cases that their ML models fail to memorize. By adding these rule-based
    decisions back into the training data, over time the ML models can catch up, but
    ultimately ML and rules co-exist as first-class citizens.
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 沃尔玛的混合生产系统 Chimera 利用人类专家为其机器学习模型无法记忆的边缘案例编写规则。通过将这些基于规则的决策重新加入训练数据中，随着时间的推移，机器学习模型可以赶上，但**最终**机器学习和规则作为一流公民共存。
- en: And this is really just a small glimpse of what’s out there. Any industrial
    ML team fundamentally needs to tackle some version of the memorization/generalization
    dilemma. If you’re working in ML, chances are you will eventually encounter it
    as well.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 这实际上只是对现状的一个小小的窥视。任何工业机器学习团队都需要在某种程度上解决记忆/概括困境。如果你从事机器学习工作，你很可能最终也会遇到这个问题。
- en: Ultimately, solving this problem will not just enable us to build vastly superior
    ML systems. It will enable us build systems that [learn more like us](https://medium.com/towards-data-science/the-origin-of-intelligent-behavior-3d3f2f659dc2).
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 解决这个问题的**最终**目标不仅是使我们能够构建更为优越的机器学习系统。它还将使我们能够构建[更像我们学习的系统](https://medium.com/towards-data-science/the-origin-of-intelligent-behavior-3d3f2f659dc2)。
- en: '[](https://medium.com/@samuel.flender/subscribe?source=post_page-----ab9c236add3e--------------------------------)
    [## Don''t want to rely on Medium''s algorithms? Sign up.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@samuel.flender/subscribe?source=post_page-----ab9c236add3e--------------------------------)
    [## 不想依赖 Medium 的算法？请注册。'
- en: Don't want to rely on Medium's algorithms? Sign up. Make sure you won't miss
    my next article by signing up to my Email…
  id: totrans-56
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 不想依赖 Medium 的算法？请注册。通过注册我的电子邮件，确保你不会错过我的下一篇文章…
- en: medium.com](https://medium.com/@samuel.flender/subscribe?source=post_page-----ab9c236add3e--------------------------------)
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: medium.com](https://medium.com/@samuel.flender/subscribe?source=post_page-----ab9c236add3e--------------------------------)
