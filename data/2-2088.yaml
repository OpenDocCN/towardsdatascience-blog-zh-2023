- en: Theoretical Deep Dive Into Linear Regression
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: çº¿æ€§å›å½’çš„ç†è®ºæ·±åº¦è§£æ
- en: åŸæ–‡ï¼š[https://towardsdatascience.com/theoretical-deep-dive-into-linear-regression-e53c579aef5b](https://towardsdatascience.com/theoretical-deep-dive-into-linear-regression-e53c579aef5b)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡ï¼š[https://towardsdatascience.com/theoretical-deep-dive-into-linear-regression-e53c579aef5b](https://towardsdatascience.com/theoretical-deep-dive-into-linear-regression-e53c579aef5b)
- en: '[EXPLAINABLE AI](https://medium.com/tag/explainable-ai)'
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '[å¯è§£é‡Šçš„äººå·¥æ™ºèƒ½](https://medium.com/tag/explainable-ai)'
- en: Learn about why linear regression is how it is, and how to naturally extend
    it in various ways
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: äº†è§£çº¿æ€§å›å½’çš„æœ¬è´¨åŠå…¶å¦‚ä½•ä»¥è‡ªç„¶çš„æ–¹å¼æ‰©å±•
- en: '[](https://dr-robert-kuebler.medium.com/?source=post_page-----e53c579aef5b--------------------------------)[![Dr.
    Robert KÃ¼bler](../Images/3b8d8b88f76c0c43d9c305e3885e7ab9.png)](https://dr-robert-kuebler.medium.com/?source=post_page-----e53c579aef5b--------------------------------)[](https://towardsdatascience.com/?source=post_page-----e53c579aef5b--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----e53c579aef5b--------------------------------)
    [Dr. Robert KÃ¼bler](https://dr-robert-kuebler.medium.com/?source=post_page-----e53c579aef5b--------------------------------)'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://dr-robert-kuebler.medium.com/?source=post_page-----e53c579aef5b--------------------------------)[![Dr.
    Robert KÃ¼bler](../Images/3b8d8b88f76c0c43d9c305e3885e7ab9.png)](https://dr-robert-kuebler.medium.com/?source=post_page-----e53c579aef5b--------------------------------)[](https://towardsdatascience.com/?source=post_page-----e53c579aef5b--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----e53c579aef5b--------------------------------)
    [Dr. Robert KÃ¼bler](https://dr-robert-kuebler.medium.com/?source=post_page-----e53c579aef5b--------------------------------)'
- en: Â·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----e53c579aef5b--------------------------------)
    Â·10 min readÂ·Jun 23, 2023
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: Â·å‘å¸ƒäº [Towards Data Science](https://towardsdatascience.com/?source=post_page-----e53c579aef5b--------------------------------)
    Â·10 åˆ†é’Ÿé˜…è¯»Â·2023å¹´6æœˆ23æ—¥
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/8b8ab36b31e37bf56824bc5f7b55d548.png)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8b8ab36b31e37bf56824bc5f7b55d548.png)'
- en: Photo by [Erik van Dijk](https://unsplash.com/@erikvandijk?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ç‰‡ç”± [Erik van Dijk](https://unsplash.com/@erikvandijk?utm_source=medium&utm_medium=referral)
    æä¾›ï¼Œæ¥æºäº [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: 'Most aspiring data science bloggers do it: write an introductory article about
    linear regression â€” and it is a natural choice since this is one of the first
    models we learn when entering the field. While these articles are great for beginners,
    most do not go deep enough to satisfy senior data scientists.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: å¤§å¤šæ•°æœ‰æŠ±è´Ÿçš„æ•°æ®ç§‘å­¦åšå®¢ä½œè€…éƒ½ä¼šè¿™æ ·åšï¼šæ’°å†™ä¸€ç¯‡å…³äºçº¿æ€§å›å½’çš„ä»‹ç»æ€§æ–‡ç« â€”â€”è¿™æ˜¯ä¸€ä¸ªè‡ªç„¶çš„é€‰æ‹©ï¼Œå› ä¸ºè¿™æ˜¯æˆ‘ä»¬è¿›å…¥è¿™ä¸ªé¢†åŸŸæ—¶å­¦ä¹ çš„ç¬¬ä¸€ä¸ªæ¨¡å‹ä¹‹ä¸€ã€‚è™½ç„¶è¿™äº›æ–‡ç« å¯¹åˆå­¦è€…éå¸¸æœ‰ç”¨ï¼Œä½†å¤§å¤šæ•°éƒ½æœªèƒ½æ·±å…¥æŒ–æ˜ä»¥æ»¡è¶³é«˜çº§æ•°æ®ç§‘å­¦å®¶ã€‚
- en: So, let me guide you through some unsung, yet refreshing details about linear
    regression that will make you a better data scientist (and give you bonus points
    during interviews).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥ï¼Œè®©æˆ‘å¸¦ä½ äº†è§£ä¸€äº›é²œä¸ºäººçŸ¥ä½†ä»¤äººè€³ç›®ä¸€æ–°çš„çº¿æ€§å›å½’ç»†èŠ‚ï¼Œè¿™å°†ä½¿ä½ æˆä¸ºæ›´å¥½çš„æ•°æ®ç§‘å­¦å®¶ï¼ˆå¹¶åœ¨é¢è¯•ä¸­è·å¾—åŠ åˆ†ï¼‰ã€‚
- en: '*This article is quite math-heavy, so in order to follow, it is beneficial
    to have some solid foundation with probabilities and calculus.*'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '*è¿™ç¯‡æ–‡ç« å†…å®¹ç›¸å½“æ•°å­¦åŒ–ï¼Œå› æ­¤ä¸ºäº†è·Ÿä¸Šå†…å®¹ï¼Œå…·å¤‡ä¸€äº›æ¦‚ç‡å’Œå¾®ç§¯åˆ†çš„åšå®åŸºç¡€ä¼šå¾ˆæœ‰å¸®åŠ©ã€‚*'
- en: The Data Generation Process
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ•°æ®ç”Ÿæˆè¿‡ç¨‹
- en: 'Iâ€™m a big fan of thinking about the data generation process when modeling.
    People who dealt with Bayesian modeling know what I mean, but for the others:
    Imagine you have a dataset (*X*, *y*) consisting of samples (*x*, *y*). Given
    *x*, how to get to a target *y*?'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘éå¸¸å–œæ¬¢åœ¨å»ºæ¨¡æ—¶è€ƒè™‘æ•°æ®ç”Ÿæˆè¿‡ç¨‹ã€‚å¤„ç†è¿‡è´å¶æ–¯å»ºæ¨¡çš„äººä¼šæ˜ç™½æˆ‘çš„æ„æ€ï¼Œä½†å¯¹äºå…¶ä»–äººï¼šæƒ³è±¡ä¸€ä¸‹ä½ æœ‰ä¸€ä¸ªæ•°æ®é›† (*X*, *y*)ï¼Œç”±æ ·æœ¬ (*x*,
    *y*) ç»„æˆã€‚ç»™å®š *x*ï¼Œå¦‚ä½•å¾—åˆ°ç›®æ ‡ *y*ï¼Ÿ
- en: '*Let us assume that we have* n *data points and that each* x *has* k *components/features.*'
  id: totrans-14
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*å‡è®¾æˆ‘ä»¬æœ‰* n *ä¸ªæ•°æ®ç‚¹ï¼Œæ¯ä¸ª* x *æœ‰* k *ä¸ªç»„ä»¶/ç‰¹å¾*ã€‚'
- en: 'For a linear model with the parameters ***w*â‚, â€¦, *wâ‚–* (coefficients)*, b*
    (intercept)*, Ïƒ* (noise)**, the assumption is that the data generation process
    looks like this:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºä¸€ä¸ªçº¿æ€§æ¨¡å‹ï¼Œå‚æ•°ä¸º ***w*â‚, â€¦, *wâ‚–*ï¼ˆç³»æ•°ï¼‰*ï¼Œ*b*ï¼ˆæˆªè·ï¼‰*ï¼Œ*Ïƒ*ï¼ˆå™ªå£°ï¼‰**ï¼Œå‡è®¾æ•°æ®ç”Ÿæˆè¿‡ç¨‹å¦‚ä¸‹ï¼š
- en: Compute *Âµ* = *w*â‚*x*â‚ + *w*â‚‚*x*â‚‚ + â€¦ + *wâ‚–xâ‚–* + *b.*
  id: totrans-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: è®¡ç®— *Âµ* = *w*â‚*x*â‚ + *w*â‚‚*x*â‚‚ + â€¦ + *wâ‚–xâ‚–* + *b*ã€‚
- en: Roll a random *y* ~ *N*(*Âµ, ÏƒÂ²*). This is independent of other randomly generated
    numbers. *Alternatively:* Roll *Îµ* ~ *N*(0*, ÏƒÂ²*) and output *y* = *Âµ* + *Îµ***.**
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: éšæœºç”Ÿæˆä¸€ä¸ª *y* ~ *N*(*Âµ, ÏƒÂ²*)ã€‚è¿™ä¸å…¶ä»–éšæœºç”Ÿæˆçš„æ•°å­—ç‹¬ç«‹ã€‚*æˆ–è€…ï¼š* ç”Ÿæˆ *Îµ* ~ *N*(0, ÏƒÂ²*) å¹¶è¾“å‡º *y* =
    *Âµ* + *Îµ*ã€‚
- en: Thatâ€™s it already. These simple two lines are equivalent to the most important
    linear regression assumptions that people like to explain at great length, namely
    **linearity, homoscedasticity, and independence of errors.**
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: å°±æ˜¯è¿™æ ·ã€‚è¿™ä¸¤è¡Œç®€å•çš„æ–‡å­—ç­‰åŒäºäººä»¬å–œæ¬¢è¯¦ç»†è§£é‡Šçš„æœ€é‡è¦çš„çº¿æ€§å›å½’å‡è®¾ï¼Œå³**çº¿æ€§ã€åŒæ–¹å·®æ€§å’Œè¯¯å·®ç‹¬ç«‹æ€§**ã€‚
- en: From step 1\. of the process, you can also see that we model the expectation
    *Âµ* with the typical linear equation *w*â‚*x*â‚ + *w*â‚‚*x*â‚‚ + â€¦ + *wâ‚–xâ‚–* + *b* rather
    than the actual target. We know that we will not hit the target anyway, so we
    settle for the mean of the distribution that generates *y* instead.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: ä»è¿‡ç¨‹çš„ç¬¬1æ­¥å¼€å§‹ï¼Œä½ ä¹Ÿå¯ä»¥çœ‹åˆ°æˆ‘ä»¬ç”¨å…¸å‹çš„çº¿æ€§æ–¹ç¨‹ *w*â‚*x*â‚ + *w*â‚‚*x*â‚‚ + â€¦ + *w*â‚–x*â‚–* + *b* æ¥å»ºæ¨¡æœŸæœ› *Âµ*ï¼Œè€Œä¸æ˜¯å®é™…ç›®æ ‡ã€‚æˆ‘ä»¬çŸ¥é“æ— è®ºå¦‚ä½•ä¸ä¼šå‡»ä¸­ç›®æ ‡ï¼Œå› æ­¤æˆ‘ä»¬æ¥å—ç”Ÿæˆ
    *y* çš„åˆ†å¸ƒçš„å‡å€¼ã€‚
- en: Extensions
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ‰©å±•
- en: '**Generalized Linear Models.** We are not forced to use a normal distribution
    for the generation process. If we deal with a dataset that **only contains positive
    targets**, it might be beneficial to assume that a **Poisson** **distribution**
    Poi(*Âµ*) is used instead of a normal distribution. This gives you **Poisson regression**.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '**å¹¿ä¹‰çº¿æ€§æ¨¡å‹**ã€‚æˆ‘ä»¬ä¸å¿…ä½¿ç”¨æ­£æ€åˆ†å¸ƒä½œä¸ºç”Ÿæˆè¿‡ç¨‹ã€‚å¦‚æœæˆ‘ä»¬å¤„ç†çš„æ•°æ®é›†**ä»…åŒ…å«æ­£ç›®æ ‡**ï¼Œé‚£ä¹ˆå‡è®¾ä½¿ç”¨**æ³Šæ¾****åˆ†å¸ƒ** Poi(*Âµ*)
    å¯èƒ½æ›´æœ‰åˆ©ï¼Œè¿™æ ·ä½ å°±å¾—åˆ°äº†**æ³Šæ¾å›å½’**ã€‚'
- en: 'If our dataset only has the targets 0 and 1, use a **Bernoulli distribution**
    Ber(*p*), where *p* = sigmoid(*Âµ*), et voilÃ : you got **logistic regression**.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæˆ‘ä»¬çš„æ•°æ®é›†åªæœ‰ç›®æ ‡0å’Œ1ï¼Œä½¿ç”¨**ä¼¯åŠªåˆ©åˆ†å¸ƒ** Ber(*p*)ï¼Œå…¶ä¸­ *p* = sigmoid(*Âµ*)ï¼Œé‚£å°±æ˜¯**é€»è¾‘å›å½’**ã€‚
- en: Only numbers between 0, 1, â€¦, *n*? Use a **binomial distribution** to get [**binomial
    regression**](https://en.wikipedia.org/wiki/Binomial_regression).
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: åªæœ‰0, 1, â€¦, *n* ä¹‹é—´çš„æ•°å­—ï¼Ÿä½¿ç”¨**äºŒé¡¹åˆ†å¸ƒ**æ¥è·å–[**äºŒé¡¹å›å½’**](https://en.wikipedia.org/wiki/Binomial_regression)ã€‚
- en: 'The list goes on and on. Long story short:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: åˆ—è¡¨è¿˜åœ¨ç»§ç»­ã€‚é•¿è¯çŸ­è¯´ï¼š
- en: Think about which distribution could have generated the labelsyou observe in
    the data.
  id: totrans-25
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: æ€è€ƒä¸€ä¸‹å“ªä¸ªåˆ†å¸ƒå¯èƒ½ç”Ÿæˆä½ åœ¨æ•°æ®ä¸­è§‚å¯Ÿåˆ°çš„æ ‡ç­¾ã€‚
- en: What Are We Actually Minimizing?
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬åˆ°åº•åœ¨æœ€å°åŒ–ä»€ä¹ˆï¼Ÿ
- en: 'Ok, so we decided on a model now. How do we train it now? How do we learn the
    parameters? Of course, you know: we minimized the (mean) squared error. But why?'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: å¥½çš„ï¼Œæˆ‘ä»¬ç°åœ¨å†³å®šäº†ä¸€ä¸ªæ¨¡å‹ã€‚é‚£ä¹ˆæˆ‘ä»¬æ€ä¹ˆè®­ç»ƒå®ƒï¼Ÿæˆ‘ä»¬æ€ä¹ˆå­¦ä¹ å‚æ•°ï¼Ÿå½“ç„¶ï¼Œä½ çŸ¥é“ï¼šæˆ‘ä»¬æœ€å°åŒ–äº†ï¼ˆå‡æ–¹ï¼‰è¯¯å·®ã€‚ä½†ä¸ºä»€ä¹ˆï¼Ÿ
- en: 'The secret is that you just do a plan **maximum likelihood estimate** using
    the generation process we described before. The labels that we observe are *y*â‚,
    *y*â‚‚, â€¦, *yâ‚™*, all of them independently generated via a normal distribution with
    means *Âµ*â‚, *Âµ*â‚‚, â€¦, *Âµâ‚™.* What is the likelihood to see these *y*â€™s? It is:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: å…³é”®åœ¨äºï¼Œä½ åªéœ€ä½¿ç”¨æˆ‘ä»¬ä¹‹å‰æè¿°çš„ç”Ÿæˆè¿‡ç¨‹è¿›è¡Œ**æœ€å¤§ä¼¼ç„¶ä¼°è®¡**ã€‚æˆ‘ä»¬è§‚å¯Ÿåˆ°çš„æ ‡ç­¾æ˜¯ *y*â‚, *y*â‚‚, â€¦, *y*â‚™*ï¼Œå®ƒä»¬éƒ½æ˜¯é€šè¿‡å…·æœ‰å‡å€¼
    *Âµ*â‚, *Âµ*â‚‚, â€¦, *Âµ*â‚™ çš„æ­£æ€åˆ†å¸ƒç‹¬ç«‹ç”Ÿæˆçš„ã€‚çœ‹åˆ°è¿™äº› *y* çš„å¯èƒ½æ€§æ˜¯å¤šå°‘ï¼Ÿè¿™æ˜¯ï¼š
- en: '![](../Images/fb050a922b3f6db694c93cb8bcc19a2f.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fb050a922b3f6db694c93cb8bcc19a2f.png)'
- en: Image by the author.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ç‰‡ç”±ä½œè€…æä¾›ã€‚
- en: We now want to find the parameters (that are hidden in the *Âµáµ¢*â€™s) to **maximize**
    this term. This is equivalent to minimizing the mean squared error, as you can
    see.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ç°åœ¨æƒ³æ‰¾åˆ°ï¼ˆéšè—åœ¨ *Âµáµ¢* ä¸­çš„ï¼‰å‚æ•°ï¼Œä»¥**æœ€å¤§åŒ–**è¿™ä¸€é¡¹ã€‚è¿™ç­‰åŒäºæœ€å°åŒ–å‡æ–¹è¯¯å·®ï¼Œå¦‚ä½ æ‰€è§ã€‚
- en: Extensions
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ‰©å±•
- en: '**Unequal Variances**. In fact, *Ïƒ* does not have to be constant. You can have
    a different *Ïƒáµ¢* for each observation in your dataset. Then, you would minimize'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '**ä¸ç­‰æ–¹å·®**ã€‚å®é™…ä¸Šï¼Œ*Ïƒ* ä¸å¿…æ˜¯æ’å®šçš„ã€‚ä½ å¯ä»¥ä¸ºæ•°æ®é›†ä¸­æ¯ä¸ªè§‚å¯Ÿå€¼è®¾ç½®ä¸åŒçš„ *Ïƒáµ¢*ã€‚ç„¶åï¼Œä½ å°†æœ€å°åŒ–'
- en: '![](../Images/71c6b101588ff794cbf66d13b9fa9aa9.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/71c6b101588ff794cbf66d13b9fa9aa9.png)'
- en: Image by the author.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ç‰‡ç”±ä½œè€…æä¾›ã€‚
- en: instead, which is **least squares with sample weights *s*.** Modeling libraries
    typically allow you to set these weights. In scikit-learn, for example, you can
    set the `sample_weight` keyword in the `fit` function.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: ä»£æ›¿ï¼Œè¿™å°±æ˜¯**å¸¦æœ‰æ ·æœ¬æƒé‡ *s* çš„æœ€å°äºŒä¹˜æ³•**ã€‚å»ºæ¨¡åº“é€šå¸¸å…è®¸ä½ è®¾ç½®è¿™äº›æƒé‡ã€‚åœ¨ scikit-learn ä¸­ï¼Œä¾‹å¦‚ï¼Œä½ å¯ä»¥åœ¨ `fit` å‡½æ•°ä¸­è®¾ç½®
    `sample_weight` å…³é”®å­—ã€‚
- en: This way, you can put more emphasis on certain observations by increasing the
    corresponding *s*. This is equivalent to decreasing the variance *ÏƒÂ²*, i.e. you
    are more confident that the error for this observation is smaller. This method
    is also called [**weighted least squares**](https://en.wikipedia.org/wiki/Weighted_least_squares).
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ ·ï¼Œä½ å¯ä»¥é€šè¿‡å¢åŠ ç›¸åº”çš„ *s* æ¥æ›´å¼ºè°ƒæŸäº›è§‚å¯Ÿå€¼ã€‚è¿™ç­‰åŒäºå‡å°æ–¹å·® *ÏƒÂ²*ï¼Œå³ä½ æ›´ç¡®ä¿¡è¿™ä¸ªè§‚å¯Ÿå€¼çš„è¯¯å·®è¾ƒå°ã€‚è¿™ç§æ–¹æ³•ä¹Ÿç§°ä¸º[**åŠ æƒæœ€å°äºŒä¹˜æ³•**](https://en.wikipedia.org/wiki/Weighted_least_squares)ã€‚
- en: '**Variances Depending on The Input.** You can even say that the variance is
    also dependent on the input *x*. In this case, you get the interesting loss function
    that is also called **variance attenuation**:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '**è¾“å…¥çš„æ–¹å·®ä¾èµ–æ€§ã€‚** ä½ ç”šè‡³å¯ä»¥è¯´æ–¹å·®ä¹Ÿä¾èµ–äºè¾“å…¥ *x*ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œä½ ä¼šå¾—åˆ°ä¸€ä¸ªæœ‰è¶£çš„æŸå¤±å‡½æ•°ï¼Œè¿™ä¹Ÿè¢«ç§°ä¸º**æ–¹å·®è¡°å‡**ï¼š'
- en: '![](../Images/9d4d422fccae6230d67479541e8be6ec.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9d4d422fccae6230d67479541e8be6ec.png)'
- en: 'The whole derivation process is outlined here:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: æ•´ä¸ªæ¨å¯¼è¿‡ç¨‹åœ¨è¿™é‡Œæ¦‚è¿°ï¼š
- en: '[](/get-uncertainty-estimates-in-neural-networks-for-free-48f2edb82c8f?source=post_page-----e53c579aef5b--------------------------------)
    [## Get Uncertainty Estimates in Regression Neural Networks for Free'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/get-uncertainty-estimates-in-neural-networks-for-free-48f2edb82c8f?source=post_page-----e53c579aef5b--------------------------------)
    [## å…è´¹è·å–å›å½’ç¥ç»ç½‘ç»œä¸­çš„ä¸ç¡®å®šæ€§ä¼°è®¡'
- en: Given the right loss function, a standard neural network can output uncertainty
    as well
  id: totrans-42
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ç»™å®šåˆé€‚çš„æŸå¤±å‡½æ•°ï¼Œæ ‡å‡†ç¥ç»ç½‘ç»œä¹Ÿå¯ä»¥è¾“å‡ºä¸ç¡®å®šæ€§
- en: towardsdatascience.com](/get-uncertainty-estimates-in-neural-networks-for-free-48f2edb82c8f?source=post_page-----e53c579aef5b--------------------------------)
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/get-uncertainty-estimates-in-neural-networks-for-free-48f2edb82c8f?source=post_page-----e53c579aef5b--------------------------------)
- en: '**Regularization.** Instead of only maximizing the likelihood of the observed
    labels*y*â‚, *y*â‚‚, â€¦, *yâ‚™,* you can take a **Bayesian standpoint** and **maximize
    the a posteriori likelihood**'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '**æ­£åˆ™åŒ–ã€‚** é™¤äº†ä»…ä»…æœ€å¤§åŒ–è§‚å¯Ÿåˆ°çš„æ ‡ç­¾ *y*â‚, *y*â‚‚, â€¦, *yâ‚™* çš„ä¼¼ç„¶ï¼Œä½ è¿˜å¯ä»¥é‡‡ç”¨**è´å¶æ–¯è§‚ç‚¹**å¹¶**æœ€å¤§åŒ–åéªŒä¼¼ç„¶**'
- en: '![](../Images/1ec367c68abfd26a1b36928a4056662b.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1ec367c68abfd26a1b36928a4056662b.png)'
- en: Image by the author.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: ä½œè€…æä¾›çš„å›¾åƒã€‚
- en: Here, *p*(*y* | *w*) is the likelihood function from above. We have to decide
    on a probability density for *p*(*w*), a so-called **prior or a prior distribution**.
    If we say that the parameters are independently normally distributed around 0,
    i.e. *wáµ¢* ~ *N*(0, *Î½Â²*), then we end up with **L2 regularization, i.e. ridge
    regression**. For a Laplace distribution, we recover **L1 regularization, i.e.
    LASSO**.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™é‡Œï¼Œ*p*(*y* | *w*) æ˜¯ä¸Šè¿°çš„ä¼¼ç„¶å‡½æ•°ã€‚æˆ‘ä»¬å¿…é¡»å†³å®šä¸€ä¸ª *p*(*w*) çš„æ¦‚ç‡å¯†åº¦ï¼Œå³æ‰€è°“çš„**å…ˆéªŒæˆ–å…ˆéªŒåˆ†å¸ƒ**ã€‚å¦‚æœæˆ‘ä»¬è¯´å‚æ•°ç‹¬ç«‹åœ°æœä»å›´ç»•
    0 çš„æ­£æ€åˆ†å¸ƒï¼Œå³ *wáµ¢* ~ *N*(0, *Î½Â²*)ï¼Œé‚£ä¹ˆæˆ‘ä»¬æœ€ç»ˆä¼šå¾—åˆ°**L2 æ­£åˆ™åŒ–ï¼Œå³å²­å›å½’**ã€‚å¯¹äºæ‹‰æ™®æ‹‰æ–¯åˆ†å¸ƒï¼Œæˆ‘ä»¬å¾—åˆ°**L1 æ­£åˆ™åŒ–ï¼Œå³
    LASSO**ã€‚
- en: Why is that? Letâ€™s use the normal distribution as an example. We have
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºä»€ä¹ˆå‘¢ï¼Ÿè®©æˆ‘ä»¬ä»¥æ­£æ€åˆ†å¸ƒä¸ºä¾‹ã€‚æˆ‘ä»¬æœ‰
- en: '![](../Images/bc7b382737b4d8a13915832e1561e80f.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bc7b382737b4d8a13915832e1561e80f.png)'
- en: Image by the author.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: ä½œè€…æä¾›çš„å›¾åƒã€‚
- en: so together with our formula for *p*(*y* | *w*) from above, we have to maximize
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: å› æ­¤ï¼ŒåŠ ä¸Šæˆ‘ä»¬ä¸Šè¿° *p*(*y* | *w*) çš„å…¬å¼ï¼Œæˆ‘ä»¬å¿…é¡»æœ€å¤§åŒ–
- en: '![](../Images/3a2d7162b93c1510637fec06a34b3e4a.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3a2d7162b93c1510637fec06a34b3e4a.png)'
- en: Image by the author.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: ä½œè€…æä¾›çš„å›¾åƒã€‚
- en: which means that we have to **minimize the mean squared error plus some regularization
    hyperparameter time the L2 norm of *w*.**
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ„å‘³ç€æˆ‘ä»¬å¿…é¡»**æœ€å°åŒ–å‡æ–¹è¯¯å·®åŠ ä¸Šä¸€äº›æ­£åˆ™åŒ–è¶…å‚æ•°ä¹˜ä»¥ *w* çš„ L2 èŒƒæ•°**ã€‚
- en: '*Note that we dropped the denominator* p*(*y*) from the Bayes formula since
    it does not depend on* w*, so we can ignore it for optimization.*'
  id: totrans-55
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*æ³¨æ„ï¼Œæˆ‘ä»¬ä»è´å¶æ–¯å…¬å¼ä¸­çœç•¥äº†åˆ†æ¯ *p*(*y*)ï¼Œå› ä¸ºå®ƒä¸ä¾èµ–äº *w*ï¼Œæ‰€ä»¥æˆ‘ä»¬å¯ä»¥å¿½ç•¥å®ƒè¿›è¡Œä¼˜åŒ–ã€‚*'
- en: You can use any other prior distribution for your parameters to create more
    interesting regularizations. You can even say that your parameters *w* are normally
    distributed but **correlated** with some correlation matrix Î£*.*
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ å¯ä»¥ä½¿ç”¨ä»»ä½•å…¶ä»–çš„å…ˆéªŒåˆ†å¸ƒæ¥åˆ›å»ºæ›´æœ‰è¶£çš„æ­£åˆ™åŒ–ã€‚ä½ ç”šè‡³å¯ä»¥è¯´ä½ çš„å‚æ•° *w* æ˜¯æ­£æ€åˆ†å¸ƒçš„ï¼Œä½†ä¸æŸä¸ªç›¸å…³çŸ©é˜µ Î£* **ç›¸å…³**ã€‚*
- en: '*Let us assume that Î£ is* positive-definite*, i.e. we are in the non-degenerate
    case. Otherwise, there is no density* p*(*w*).*'
  id: totrans-57
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*å‡è®¾ Î£ æ˜¯* æ­£å®šçš„*ï¼Œå³æˆ‘ä»¬å¤„äºéé€€åŒ–æƒ…å†µã€‚å¦åˆ™ï¼Œæ²¡æœ‰å¯†åº¦ *p*(*w*)ã€‚*'
- en: If you do the math, you will find out that we then have to optimize
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœä½ è¿›è¡Œæ•°å­¦è®¡ç®—ï¼Œä½ ä¼šå‘ç°æˆ‘ä»¬ç„¶åå¿…é¡»ä¼˜åŒ–
- en: '![](../Images/eb5af9d7a702a085ca22cf56f5785b6e.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/eb5af9d7a702a085ca22cf56f5785b6e.png)'
- en: Image by the author.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: ä½œè€…æä¾›çš„å›¾åƒã€‚
- en: 'for some matrix Î“. **Note: Î“ is invertible and we have Î£â»Â¹ = Î“áµ€Î“.** This is
    also called **Tikhonov regularization**.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºæŸä¸ªçŸ©é˜µ Î“ã€‚**æ³¨æ„ï¼šÎ“ æ˜¯å¯é€†çš„ï¼Œæˆ‘ä»¬æœ‰ Î£â»Â¹ = Î“áµ€Î“ã€‚** è¿™ä¹Ÿè¢«ç§°ä¸º**æéœè¯ºå¤«æ­£åˆ™åŒ–**ã€‚
- en: '**Hint:** start with the fact that'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '**æç¤ºï¼š** ä»ä»¥ä¸‹äº‹å®å¼€å§‹'
- en: '![](../Images/81a3a8873f6d531a2ebb9bd177187282.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/81a3a8873f6d531a2ebb9bd177187282.png)'
- en: Image by the author.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: ä½œè€…æä¾›çš„å›¾åƒã€‚
- en: and remember that positive-definite matrices can be [decomposed into a product
    of some invertible matrix and its transpose](https://en.wikipedia.org/wiki/Definite_matrix#Decomposition).
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: å¹¶ä¸”è®°ä½ï¼Œæ­£å®šçŸ©é˜µå¯ä»¥è¢«[åˆ†è§£æˆæŸä¸ªå¯é€†çŸ©é˜µåŠå…¶è½¬ç½®çš„ä¹˜ç§¯](https://en.wikipedia.org/wiki/Definite_matrix#Decomposition)ã€‚
- en: Minimize The Loss Function
  id: totrans-66
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æœ€å°åŒ–æŸå¤±å‡½æ•°
- en: Great, so we defined our model and know what we want to optimize. But how can
    we optimize it, i.e. learn the best parameters that minimize the loss function?
    And when is there a unique solution? Letâ€™s find out.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: å¾ˆå¥½ï¼Œæˆ‘ä»¬å®šä¹‰äº†æˆ‘ä»¬çš„æ¨¡å‹å¹¶çŸ¥é“æˆ‘ä»¬æƒ³ä¼˜åŒ–ä»€ä¹ˆã€‚ä½†æ˜¯æˆ‘ä»¬å¦‚ä½•ä¼˜åŒ–å®ƒï¼Œå³å­¦ä¹ æœ€ä¼˜çš„å‚æ•°ä»¥æœ€å°åŒ–æŸå¤±å‡½æ•°ï¼Ÿä»€ä¹ˆæ—¶å€™ä¼šæœ‰å”¯ä¸€è§£ï¼Ÿè®©æˆ‘ä»¬æ¥çœ‹çœ‹ã€‚
- en: Ordinary Least Squares
  id: totrans-68
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ™®é€šæœ€å°äºŒä¹˜æ³•
- en: Let us assume that we do not regularize and donâ€™t use sample weights. Then,
    the MSE can be written as
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: å‡è®¾æˆ‘ä»¬ä¸è¿›è¡Œæ­£åˆ™åŒ–ï¼Œä¹Ÿä¸ä½¿ç”¨æ ·æœ¬æƒé‡ã€‚é‚£ä¹ˆï¼Œå‡æ–¹è¯¯å·®å¯ä»¥å†™ä½œ
- en: '![](../Images/7af931d447d5df14f791c8484c9a3d4b.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7af931d447d5df14f791c8484c9a3d4b.png)'
- en: Image by the author.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾åƒç”±ä½œè€…æä¾›ã€‚
- en: This is quite abstract, so let us write it differently as
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™éå¸¸æŠ½è±¡ï¼Œæ‰€ä»¥è®©æˆ‘ä»¬ä»¥ä¸åŒçš„æ–¹å¼ä¹¦å†™
- en: '![](../Images/5a5cea791903d850c7d7ff4bd115adab.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5a5cea791903d850c7d7ff4bd115adab.png)'
- en: Image by the author.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾åƒç”±ä½œè€…æä¾›ã€‚
- en: Using [matrix calculus](https://en.wikipedia.org/wiki/Matrix_calculus#Scalar-by-vector_identities),
    you can take the derivative of this function with respect to *w* (we assume that
    the bias term *b* is included there)*.*
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨ [çŸ©é˜µå¾®ç§¯åˆ†](https://en.wikipedia.org/wiki/Matrix_calculus#Scalar-by-vector_identities)ï¼Œä½ å¯ä»¥å¯¹è¿™ä¸ªå‡½æ•°å…³äº
    *w* æ±‚å¯¼ï¼ˆæˆ‘ä»¬å‡è®¾åç½®é¡¹ *b* å·²ç»åŒ…å«åœ¨å†…ï¼‰ã€‚
- en: '![](../Images/f5820d6701156de11010d68969e10f50.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f5820d6701156de11010d68969e10f50.png)'
- en: Image by the author.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾åƒç”±ä½œè€…æä¾›ã€‚
- en: If you set this gradient to zero, you end up with
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœä½ å°†è¿™ä¸ªæ¢¯åº¦è®¾ä¸ºé›¶ï¼Œä½ ä¼šå¾—åˆ°
- en: '![](../Images/dacab3b7869c79f9cab2f4442e478c9e.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/dacab3b7869c79f9cab2f4442e478c9e.png)'
- en: Image by the author.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾åƒç”±ä½œè€…æä¾›ã€‚
- en: If the (*n* Ã— *k*)-matrix *X* has a rank of *k*, so does the (*k* Ã— *k*)-matrix
    *X*áµ€*X,* i.e. it is invertible*. Why?* It follows from [rank(*X*) *=* rank(*X*áµ€*X*)](https://en.wikipedia.org/wiki/Rank_(linear_algebra)#Properties)*.*
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœ (*n* Ã— *k*) çŸ©é˜µ *X* çš„ç§©ä¸º *k*ï¼Œé‚£ä¹ˆ (*k* Ã— *k*) çŸ©é˜µ *X*áµ€*X* ä¹Ÿæ˜¯ï¼Œå³å®ƒæ˜¯å¯é€†çš„ã€‚ä¸ºä»€ä¹ˆï¼Ÿè¿™å¯ä»¥ä» [rank(*X*)
    *=* rank(*X*áµ€*X*)](https://en.wikipedia.org/wiki/Rank_(linear_algebra)#Properties)
    ä¸­æ¨å¯¼å‡ºæ¥ã€‚
- en: In this case, we get the **unique solution**
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬å¾—åˆ°**å”¯ä¸€è§£**
- en: '![](../Images/aa182f0811b2619d41bc9dbac897f904.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/aa182f0811b2619d41bc9dbac897f904.png)'
- en: Image by the author.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾åƒç”±ä½œè€…æä¾›ã€‚
- en: '***Note:*** *Software packages do not optimize like this but instead use gradient
    descent or other iterative techniques because it is faster. Still, the formula
    is nice and gives us some high-level insights about the problem.*'
  id: totrans-85
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '***æ³¨æ„ï¼š*** *è½¯ä»¶åŒ…ä¸åƒè¿™æ ·ä¼˜åŒ–ï¼Œè€Œæ˜¯ä½¿ç”¨æ¢¯åº¦ä¸‹é™æˆ–å…¶ä»–è¿­ä»£æŠ€æœ¯ï¼Œå› ä¸ºè¿™æ ·æ›´å¿«ã€‚ä¸è¿‡ï¼Œå…¬å¼å¾ˆå¥½ï¼Œå¹¶ä¸”ä¸ºæˆ‘ä»¬æä¾›äº†æœ‰å…³é—®é¢˜çš„ä¸€äº›é«˜çº§è§è§£ã€‚*'
- en: But is this really a minimum? We can find out by computing the Hessian, which
    is *X*áµ€*X.* The matrix is positive-semidefinite since *w*áµ€*X*áµ€*Xw = |Xw|Â²* â‰¥ 0
    for any *w*. It is even **strictly** positive-definite since *X*áµ€*X* is invertible,
    i.e. 0 is not an eigenvector, so our optimal *w* is indeed minimizing our problem.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†è¿™çœŸçš„èƒ½è¾¾åˆ°æœ€å°å€¼å—ï¼Ÿæˆ‘ä»¬å¯ä»¥é€šè¿‡è®¡ç®— Hessian çŸ©é˜µæ¥æ‰¾å‡ºï¼ŒHessian çŸ©é˜µæ˜¯ *X*áµ€*X*ã€‚è¯¥çŸ©é˜µæ˜¯åŠæ­£å®šçš„ï¼Œå› ä¸º *w*áµ€*X*áµ€*Xw
    = |Xw|Â²* â‰¥ 0 å¯¹äºä»»ä½• *w*ã€‚å®ƒç”šè‡³æ˜¯**ä¸¥æ ¼**æ­£å®šçš„ï¼Œå› ä¸º *X*áµ€*X* æ˜¯å¯é€†çš„ï¼Œå³ 0 ä¸æ˜¯ç‰¹å¾å€¼ï¼Œæ‰€ä»¥æˆ‘ä»¬çš„æœ€ä¼˜ *w* ç¡®å®åœ¨æœ€å°åŒ–æˆ‘ä»¬çš„é—®é¢˜ã€‚
- en: Perfect Multicollinearity
  id: totrans-87
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å®Œç¾çš„å¤šé‡å…±çº¿æ€§
- en: That was the friendly case. But what happens if *X* has a rank smaller than
    *k*? This might happen if we have two features in our dataset where one is a multiple
    of the other, e.g. we use the features *height (in m)* and *height (in cm)* in
    our dataset. Then we have *height (in cm) = 100 * height (in m).*
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™åªæ˜¯å‹å¥½çš„æƒ…å†µã€‚ä½†å¦‚æœ *X* çš„ç§©å°äº *k* ä¼šå‘ç”Ÿä»€ä¹ˆï¼Ÿå¦‚æœæˆ‘ä»¬æ•°æ®é›†ä¸­æœ‰ä¸¤ä¸ªç‰¹å¾ï¼Œå…¶ä¸­ä¸€ä¸ªæ˜¯å¦ä¸€ä¸ªçš„å€æ•°ï¼Œä¾‹å¦‚ï¼Œæˆ‘ä»¬åœ¨æ•°æ®é›†ä¸­ä½¿ç”¨ *height
    (in m)* å’Œ *height (in cm)* ä½œä¸ºç‰¹å¾ã€‚ç„¶åæˆ‘ä»¬æœ‰ *height (in cm) = 100 * height (in m)*ã€‚
- en: It can also happen if we one-hot encode categorical data and do not drop one
    of the columns. For example, if we have a feature *color* in our dataset that
    can be red, green, or blue, then we can one-hot encode and end up with three columns
    *color_red, color_green,* and *color_blue*. For these features, we have *color_red
    + color_green + color_blue =* 1, which induces perfect multicollinearity as well.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæˆ‘ä»¬å¯¹åˆ†ç±»æ•°æ®è¿›è¡Œç‹¬çƒ­ç¼–ç è€Œä¸ä¸¢å¼ƒå…¶ä¸­ä¸€åˆ—ï¼Œä¹Ÿå¯èƒ½å‘ç”Ÿè¿™ç§æƒ…å†µã€‚ä¾‹å¦‚ï¼Œå¦‚æœæˆ‘ä»¬æ•°æ®é›†ä¸­æœ‰ä¸€ä¸ªç‰¹å¾ *color*ï¼Œå®ƒå¯ä»¥æ˜¯çº¢è‰²ã€ç»¿è‰²æˆ–è“è‰²ï¼Œé‚£ä¹ˆæˆ‘ä»¬å¯ä»¥è¿›è¡Œç‹¬çƒ­ç¼–ç ï¼Œå¾—åˆ°ä¸‰åˆ—
    *color_red, color_green,* å’Œ *color_blue*ã€‚å¯¹äºè¿™äº›ç‰¹å¾ï¼Œæˆ‘ä»¬æœ‰ *color_red + color_green +
    color_blue =* 1ï¼Œè¿™ä¹Ÿä¼šå¼•èµ·å®Œç¾çš„å¤šé‡å…±çº¿æ€§ã€‚
- en: In these cases, the rank of *X*áµ€*X* is also smaller than *k*, so this matrix
    is not invertible.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™äº›æƒ…å†µä¸‹ï¼Œ*X*áµ€*X* çš„ç§©ä¹Ÿå°äº *k*ï¼Œå› æ­¤è¿™ä¸ªçŸ©é˜µæ˜¯ä¸å¯é€†çš„ã€‚
- en: End of story.
  id: totrans-91
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: æ•…äº‹ç»“æŸã€‚
- en: 'Or not? Actually, no, because it can mean two things: (*X*áµ€*X*)*w = X*áµ€*y*
    has'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: è¿˜æ˜¯ä¸è¡Œï¼Ÿå®é™…ä¸Šä¸æ˜¯ï¼Œå› ä¸ºè¿™å¯èƒ½æ„å‘³ç€ä¸¤ä»¶äº‹ï¼š(*X*áµ€*X*)*w = X*áµ€*y* æœ‰
- en: no solution or
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æ²¡æœ‰è§£æˆ–
- en: infinitely many solutions.
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æ— é™å¤šçš„è§£ã€‚
- en: It turns out that in our case, we can obtain one solution using the [Moore-Penrose
    inverse](https://en.wikipedia.org/wiki/Moore%E2%80%93Penrose_inverse). This means
    that we are in the case of infinitely many solutions, all of them giving us the
    same (training) mean squared error loss.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: äº‹å®è¯æ˜ï¼Œåœ¨æˆ‘ä»¬çš„æ¡ˆä¾‹ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ [Moore-Penrose ä¼ªé€†](https://en.wikipedia.org/wiki/Moore%E2%80%93Penrose_inverse)
    è·å¾—ä¸€ä¸ªè§£ã€‚è¿™æ„å‘³ç€æˆ‘ä»¬å¤„äºæ— ç©·å¤šè§£çš„æƒ…å†µï¼Œè¿™äº›è§£éƒ½ç»™æˆ‘ä»¬ç›¸åŒçš„ï¼ˆè®­ç»ƒï¼‰å‡æ–¹è¯¯å·®æŸå¤±ã€‚
- en: If we denote the Moore-Penrose inverse of *A* by *A*âº, we can solve the linear
    system of equations as
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæˆ‘ä»¬ç”¨ *A*âº è¡¨ç¤º *A* çš„ Moore-Penrose ä¼ªé€†ï¼Œæˆ‘ä»¬å¯ä»¥æ±‚è§£çº¿æ€§æ–¹ç¨‹ç»„ä¸º
- en: '![](../Images/b99b7b2b70d821c22f23e667fd03c984.png)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b99b7b2b70d821c22f23e667fd03c984.png)'
- en: Image by the author.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: ä½œè€…æä¾›çš„å›¾ç‰‡ã€‚
- en: To get the other infinitely many solutions, just add the null space of *X*áµ€*X*
    to this specific solution.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: è¦è·å¾—å…¶ä»–æ— ç©·å¤šè§£ï¼Œåªéœ€å°† *X*áµ€*X* çš„é›¶ç©ºé—´æ·»åŠ åˆ°è¿™ä¸ªç‰¹å®šè§£ä¸­ã€‚
- en: Minimization With Tikhonov Regularization
  id: totrans-100
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä½¿ç”¨ Tikhonov æ­£åˆ™åŒ–çš„æœ€å°åŒ–
- en: Recall that we could add a prior distribution to our weights. We then had to
    minimize
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: è®°ä½ï¼Œæˆ‘ä»¬å¯ä»¥å‘æƒé‡ä¸­æ·»åŠ å…ˆéªŒåˆ†å¸ƒã€‚ç„¶åæˆ‘ä»¬éœ€è¦æœ€å°åŒ–
- en: '![](../Images/67f72d916860474636a7287cef747b8e.png)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/67f72d916860474636a7287cef747b8e.png)'
- en: Image by the author.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: ä½œè€…æä¾›çš„å›¾ç‰‡ã€‚
- en: for some invertible matrix Î“. Following the same steps as in ordinary least
    squares, i.e. taking the derivative with respect to *w* and setting the result
    to zero, the solution is
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºæŸä¸ªå¯é€†çŸ©é˜µ Î“ã€‚æŒ‰ç…§æ™®é€šæœ€å°äºŒä¹˜æ³•ä¸­çš„ç›¸åŒæ­¥éª¤ï¼Œå³å¯¹ *w* æ±‚å¯¼å¹¶å°†ç»“æœè®¾ç½®ä¸ºé›¶ï¼Œè§£ä¸º
- en: '![](../Images/ad542ac73d8a2a760adf430eb09b0d4a.png)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ad542ac73d8a2a760adf430eb09b0d4a.png)'
- en: Image by the author.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: ä½œè€…æä¾›çš„å›¾ç‰‡ã€‚
- en: 'The neat part:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: ç²¾å½©çš„éƒ¨åˆ†ï¼š
- en: Xáµ€X + Î“áµ€Î“ is always invertible!
  id: totrans-108
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: Xáµ€X + Î“áµ€Î“ å§‹ç»ˆæ˜¯å¯é€†çš„ï¼
- en: Let us find out why. It suffices to show that the null space of *X*áµ€*X* + Î“áµ€Î“
    is only {0}. So, let us take a *w* with (*X*áµ€*X* + Î“áµ€Î“)*w* = 0\. Now, our goal
    is to show that *w* = 0.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬æ‰¾å‡ºåŸå› ã€‚åªéœ€è¯æ˜ *X*áµ€*X* + Î“áµ€Î“ çš„é›¶ç©ºé—´ä»…ä¸º {0}ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å–ä¸€ä¸ª *w* ä½¿å¾— (*X*áµ€*X* + Î“áµ€Î“)*w* = 0ã€‚ç°åœ¨ï¼Œæˆ‘ä»¬çš„ç›®æ ‡æ˜¯è¯æ˜
    *w* = 0ã€‚
- en: From (*X*áµ€*X* + Î“áµ€Î“)*w* = 0 it follows that
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: ä» (*X*áµ€*X* + Î“áµ€Î“)*w* = 0 å¯å¾—
- en: '![](../Images/dfbadd99aed70af03cecda5cacf7730a.png)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/dfbadd99aed70af03cecda5cacf7730a.png)'
- en: Image by the author.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: ä½œè€…æä¾›çš„å›¾ç‰‡ã€‚
- en: which in turn implies |Î“*w*| = 0 â†’ Î“*w =* 0*.* SinceÎ“ is invertible, *w* has
    to be 0\. Using the same calculation, we can see that the Hessian is also positive-definite.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¹Ÿæ„å‘³ç€ |Î“*w*| = 0 â†’ Î“*w = *0*ã€‚ç”±äº Î“ æ˜¯å¯é€†çš„ï¼Œ*w* å¿…é¡»æ˜¯ 0ã€‚é€šè¿‡ç›¸åŒçš„è®¡ç®—ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ° Hessian çŸ©é˜µä¹Ÿæ˜¯æ­£å®šçš„ã€‚
- en: Nice, so Tikhonov regularization automatically helps make the solution unique!
    Since ridge regression is a special case of Tikhonov regression (for Î“ = Î»*Iâ‚–,
    Iâ‚–* is the *k*-dimensional identity matrix), the same holds there.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: å¾ˆå¥½ï¼Œå› æ­¤ Tikhonov æ­£åˆ™åŒ–è‡ªåŠ¨å¸®åŠ©ä½¿è§£å”¯ä¸€ï¼ç”±äºå²­å›å½’æ˜¯ Tikhonov å›å½’çš„ç‰¹ä¾‹ï¼ˆå¯¹äº Î“ = Î»*Iâ‚–ï¼Œå…¶ä¸­ Iâ‚–* æ˜¯ *k* ç»´å•ä½çŸ©é˜µï¼‰ï¼Œå› æ­¤åŒæ ·é€‚ç”¨ã€‚
- en: Adding Sample Weights
  id: totrans-115
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ·»åŠ æ ·æœ¬æƒé‡
- en: As a last point, let us also add sample weights to the Tikhonov regularization.
    Adding sample weights is equivalent to minimizing
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åï¼Œè®©æˆ‘ä»¬è¿˜å°†æ ·æœ¬æƒé‡æ·»åŠ åˆ° Tikhonov æ­£åˆ™åŒ–ä¸­ã€‚æ·»åŠ æ ·æœ¬æƒé‡ç­‰åŒäºæœ€å°åŒ–
- en: '![](../Images/e85c7dc51fbd41a64ec7c105af11ba2b.png)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e85c7dc51fbd41a64ec7c105af11ba2b.png)'
- en: Image by the author.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: ä½œè€…æä¾›çš„å›¾ç‰‡ã€‚
- en: For some diagonal matrix *S* with positive diagonal entries *sáµ¢.* Minimizing
    is as straightforward as in the case of ordinary least squares. The result is
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºæŸäº›å¯¹è§’çŸ©é˜µ *S* å…¶å¯¹è§’å…ƒç´  *sáµ¢* ä¸ºæ­£ï¼Œæœ€å°åŒ–é—®é¢˜å’Œæ™®é€šæœ€å°äºŒä¹˜æ³•ä¸€æ ·ç®€å•ã€‚ç»“æœæ˜¯
- en: '![](../Images/6b9517176c23bddef1d77cbfd7797681.png)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6b9517176c23bddef1d77cbfd7797681.png)'
- en: Image by the author.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: ä½œè€…æä¾›çš„å›¾ç‰‡ã€‚
- en: '**Note:** The Hessian is also positive-definite.'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '**æ³¨æ„ï¼š** Hessian çŸ©é˜µä¹Ÿæ˜¯æ­£å®šçš„ã€‚'
- en: Homework for you
  id: totrans-123
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä½ çš„ä½œä¸š
- en: Assume that for the Tikhonov regularization, we do not impose that the weights
    should be centered around 0, but some other point *w*â‚€. Show that the optimization
    problem becomes
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: å‡è®¾å¯¹äº Tikhonov æ­£åˆ™åŒ–ï¼Œæˆ‘ä»¬ä¸å¼ºåˆ¶æƒé‡å›´ç»• 0ï¼Œè€Œæ˜¯å›´ç»•æŸä¸ªç‚¹ *w*â‚€ã€‚è¯æ˜ä¼˜åŒ–é—®é¢˜å˜ä¸º
- en: '![](../Images/177006d6d3b5ae0ee092be0de5fd1003.png)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/177006d6d3b5ae0ee092be0de5fd1003.png)'
- en: Image by the author.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: ä½œè€…æä¾›çš„å›¾ç‰‡ã€‚
- en: and that the solution is
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: ç»“æœæ˜¯
- en: '![](../Images/04adac9e8b0679dd816373b0e33fc549.png)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/04adac9e8b0679dd816373b0e33fc549.png)'
- en: Image by the author.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: ä½œè€…æä¾›çš„å›¾ç‰‡ã€‚
- en: This is the most general form of Tikhov regularization. Some people prefer to
    define *P* := *S*Â², *Q* := Î“áµ€Î“, as [done here](https://en.wikipedia.org/wiki/Ridge_regression#Generalized_Tikhonov_regularization).
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯ Tikhonov æ­£åˆ™åŒ–çš„æœ€ä¸€èˆ¬å½¢å¼ã€‚æœ‰äº›äººæ›´å–œæ¬¢å®šä¹‰ *P* := *S*Â²ï¼Œ*Q* := Î“áµ€Î“ï¼Œå¦‚ [è¿™é‡Œæ‰€ç¤º](https://en.wikipedia.org/wiki/Ridge_regression#Generalized_Tikhonov_regularization)ã€‚
- en: Conclusion
  id: totrans-131
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç»“è®º
- en: In this article, I took you on a journey through several advanced aspects of
    linear regression. By adopting a generative view, we could see that generalized
    linear models just differ from the *normal* linear models only in the type of
    distribution that is used to sample the target *y*.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘å¸¦ä½ æ¢è®¨äº†çº¿æ€§å›å½’çš„å‡ ä¸ªé«˜çº§æ–¹é¢ã€‚é€šè¿‡é‡‡ç”¨ç”Ÿæˆè§†è§’ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°å¹¿ä¹‰çº¿æ€§æ¨¡å‹ä¸*æ™®é€š*çº¿æ€§æ¨¡å‹çš„åŒºåˆ«ä»…åœ¨äºç”¨äºæŠ½æ ·ç›®æ ‡*y*çš„åˆ†å¸ƒç±»å‹ã€‚
- en: Then we have seen that minimizing the mean squared error is equivalent to maximizing
    the likelihood of the observed values. If we impose a prior normal distribution
    on the learnable parameters, we end up with Tikhonov (and L2 as a special case)
    regularization. We can use different prior distributions such as the Laplace distribution
    as well, but then there are no closed solution formulas anymore. Still, convex
    programming approaches also let you find the best parameters.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åæˆ‘ä»¬çœ‹åˆ°ï¼Œæœ€å°åŒ–å‡æ–¹è¯¯å·®ç­‰åŒäºæœ€å¤§åŒ–è§‚æµ‹å€¼çš„ä¼¼ç„¶ã€‚å¦‚æœæˆ‘ä»¬å¯¹å¯å­¦ä¹ å‚æ•°æ–½åŠ ä¸€ä¸ªå…ˆéªŒæ­£æ€åˆ†å¸ƒï¼Œæˆ‘ä»¬æœ€ç»ˆä¼šå¾—åˆ°Tikhonovï¼ˆä»¥åŠL2ä½œä¸ºç‰¹ä¾‹ï¼‰æ­£åˆ™åŒ–ã€‚æˆ‘ä»¬ä¹Ÿå¯ä»¥ä½¿ç”¨ä¸åŒçš„å…ˆéªŒåˆ†å¸ƒï¼Œå¦‚æ‹‰æ™®æ‹‰æ–¯åˆ†å¸ƒï¼Œä½†é‚£æ ·å°±æ²¡æœ‰å°é—­çš„è§£å…¬å¼äº†ã€‚ä¸è¿‡ï¼Œå‡¸ä¼˜åŒ–æ–¹æ³•ä¹Ÿå¯ä»¥å¸®åŠ©ä½ æ‰¾åˆ°æœ€ä½³å‚æ•°ã€‚
- en: As a last step, we found a lot of direct solution formulas for each minimization
    problem considered. These formulas are usually not used in practice for large
    datasets, but we could see that the solutions are always unique. And we also learned
    to do some calculus on the way. ğŸ˜‰
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: ä½œä¸ºæœ€åä¸€æ­¥ï¼Œæˆ‘ä»¬ä¸ºæ¯ä¸ªæœ€å°åŒ–é—®é¢˜æ‰¾åˆ°äº†å¾ˆå¤šç›´æ¥çš„è§£å…¬å¼ã€‚è¿™äº›å…¬å¼åœ¨å¤§æ•°æ®é›†çš„å®é™…åº”ç”¨ä¸­é€šå¸¸ä¸ä¼šä½¿ç”¨ï¼Œä½†æˆ‘ä»¬å¯ä»¥çœ‹åˆ°è¿™äº›è§£æ€»æ˜¯å”¯ä¸€çš„ã€‚æˆ‘ä»¬åœ¨è¿‡ç¨‹ä¸­ä¹Ÿå­¦ä¼šäº†ä¸€äº›å¾®ç§¯åˆ†ã€‚ğŸ˜‰
- en: I hope that you learned something new, interesting, and valuable today. Thanks
    for reading!
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘å¸Œæœ›ä½ ä»Šå¤©å­¦åˆ°äº†ä¸€äº›æ–°çš„ã€æœ‰è¶£çš„å’Œæœ‰ä»·å€¼çš„ä¸œè¥¿ã€‚æ„Ÿè°¢é˜…è¯»ï¼
- en: '*If you have any questions, write me on* [*LinkedIn*](https://www.linkedin.com/in/dr-robert-k%C3%BCbler-983859150/)*!*'
  id: totrans-136
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*å¦‚æœä½ æœ‰ä»»ä½•é—®é¢˜ï¼Œè¯·åœ¨* [*LinkedIn*](https://www.linkedin.com/in/dr-robert-k%C3%BCbler-983859150/)*ä¸Šè”ç³»æˆ‘ï¼*'
- en: And if you want to dive deeper into the world of algorithms, give my new publication
    **All About Algorithms** a try! Iâ€™m still searching for writers!
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœä½ æƒ³æ›´æ·±å…¥åœ°äº†è§£ç®—æ³•çš„ä¸–ç•Œï¼Œå¯ä»¥è¯•è¯•æˆ‘çš„æ–°å‡ºç‰ˆç‰©**æ‰€æœ‰å…³äºç®—æ³•**ï¼æˆ‘ä»åœ¨å¯»æ‰¾ä½œè€…ï¼
- en: '[](https://medium.com/all-about-algorithms?source=post_page-----e53c579aef5b--------------------------------)
    [## All About Algorithms'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '[## æ‰€æœ‰å…³äºç®—æ³•](https://medium.com/all-about-algorithms?source=post_page-----e53c579aef5b--------------------------------)'
- en: From intuitive explanations to in-depth analysis, algorithms come to life with
    examples, code, and awesomeâ€¦
  id: totrans-139
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ä»ç›´è§‚çš„è§£é‡Šåˆ°æ·±å…¥çš„åˆ†æï¼Œç®—æ³•é€šè¿‡ç¤ºä¾‹ã€ä»£ç å’Œç²¾å½©çš„å†…å®¹å¾—ä»¥å‘ˆç°â€¦
- en: medium.com](https://medium.com/all-about-algorithms?source=post_page-----e53c579aef5b--------------------------------)
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '[medium.com](https://medium.com/all-about-algorithms?source=post_page-----e53c579aef5b--------------------------------)'
