- en: Theoretical Deep Dive Into Linear Regression
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 线性回归的理论深度解析
- en: 原文：[https://towardsdatascience.com/theoretical-deep-dive-into-linear-regression-e53c579aef5b](https://towardsdatascience.com/theoretical-deep-dive-into-linear-regression-e53c579aef5b)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/theoretical-deep-dive-into-linear-regression-e53c579aef5b](https://towardsdatascience.com/theoretical-deep-dive-into-linear-regression-e53c579aef5b)
- en: '[EXPLAINABLE AI](https://medium.com/tag/explainable-ai)'
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '[可解释的人工智能](https://medium.com/tag/explainable-ai)'
- en: Learn about why linear regression is how it is, and how to naturally extend
    it in various ways
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 了解线性回归的本质及其如何以自然的方式扩展
- en: '[](https://dr-robert-kuebler.medium.com/?source=post_page-----e53c579aef5b--------------------------------)[![Dr.
    Robert Kübler](../Images/3b8d8b88f76c0c43d9c305e3885e7ab9.png)](https://dr-robert-kuebler.medium.com/?source=post_page-----e53c579aef5b--------------------------------)[](https://towardsdatascience.com/?source=post_page-----e53c579aef5b--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----e53c579aef5b--------------------------------)
    [Dr. Robert Kübler](https://dr-robert-kuebler.medium.com/?source=post_page-----e53c579aef5b--------------------------------)'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://dr-robert-kuebler.medium.com/?source=post_page-----e53c579aef5b--------------------------------)[![Dr.
    Robert Kübler](../Images/3b8d8b88f76c0c43d9c305e3885e7ab9.png)](https://dr-robert-kuebler.medium.com/?source=post_page-----e53c579aef5b--------------------------------)[](https://towardsdatascience.com/?source=post_page-----e53c579aef5b--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----e53c579aef5b--------------------------------)
    [Dr. Robert Kübler](https://dr-robert-kuebler.medium.com/?source=post_page-----e53c579aef5b--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----e53c579aef5b--------------------------------)
    ·10 min read·Jun 23, 2023
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----e53c579aef5b--------------------------------)
    ·10 分钟阅读·2023年6月23日
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/8b8ab36b31e37bf56824bc5f7b55d548.png)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8b8ab36b31e37bf56824bc5f7b55d548.png)'
- en: Photo by [Erik van Dijk](https://unsplash.com/@erikvandijk?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由 [Erik van Dijk](https://unsplash.com/@erikvandijk?utm_source=medium&utm_medium=referral)
    提供，来源于 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: 'Most aspiring data science bloggers do it: write an introductory article about
    linear regression — and it is a natural choice since this is one of the first
    models we learn when entering the field. While these articles are great for beginners,
    most do not go deep enough to satisfy senior data scientists.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数有抱负的数据科学博客作者都会这样做：撰写一篇关于线性回归的介绍性文章——这是一个自然的选择，因为这是我们进入这个领域时学习的第一个模型之一。虽然这些文章对初学者非常有用，但大多数都未能深入挖掘以满足高级数据科学家。
- en: So, let me guide you through some unsung, yet refreshing details about linear
    regression that will make you a better data scientist (and give you bonus points
    during interviews).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，让我带你了解一些鲜为人知但令人耳目一新的线性回归细节，这将使你成为更好的数据科学家（并在面试中获得加分）。
- en: '*This article is quite math-heavy, so in order to follow, it is beneficial
    to have some solid foundation with probabilities and calculus.*'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '*这篇文章内容相当数学化，因此为了跟上内容，具备一些概率和微积分的坚实基础会很有帮助。*'
- en: The Data Generation Process
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据生成过程
- en: 'I’m a big fan of thinking about the data generation process when modeling.
    People who dealt with Bayesian modeling know what I mean, but for the others:
    Imagine you have a dataset (*X*, *y*) consisting of samples (*x*, *y*). Given
    *x*, how to get to a target *y*?'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 我非常喜欢在建模时考虑数据生成过程。处理过贝叶斯建模的人会明白我的意思，但对于其他人：想象一下你有一个数据集 (*X*, *y*)，由样本 (*x*,
    *y*) 组成。给定 *x*，如何得到目标 *y*？
- en: '*Let us assume that we have* n *data points and that each* x *has* k *components/features.*'
  id: totrans-14
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*假设我们有* n *个数据点，每个* x *有* k *个组件/特征*。'
- en: 'For a linear model with the parameters ***w*₁, …, *wₖ* (coefficients)*, b*
    (intercept)*, σ* (noise)**, the assumption is that the data generation process
    looks like this:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 对于一个线性模型，参数为 ***w*₁, …, *wₖ*（系数）*，*b*（截距）*，*σ*（噪声）**，假设数据生成过程如下：
- en: Compute *µ* = *w*₁*x*₁ + *w*₂*x*₂ + … + *wₖxₖ* + *b.*
  id: totrans-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算 *µ* = *w*₁*x*₁ + *w*₂*x*₂ + … + *wₖxₖ* + *b*。
- en: Roll a random *y* ~ *N*(*µ, σ²*). This is independent of other randomly generated
    numbers. *Alternatively:* Roll *ε* ~ *N*(0*, σ²*) and output *y* = *µ* + *ε***.**
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 随机生成一个 *y* ~ *N*(*µ, σ²*)。这与其他随机生成的数字独立。*或者：* 生成 *ε* ~ *N*(0, σ²*) 并输出 *y* =
    *µ* + *ε*。
- en: That’s it already. These simple two lines are equivalent to the most important
    linear regression assumptions that people like to explain at great length, namely
    **linearity, homoscedasticity, and independence of errors.**
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 就是这样。这两行简单的文字等同于人们喜欢详细解释的最重要的线性回归假设，即**线性、同方差性和误差独立性**。
- en: From step 1\. of the process, you can also see that we model the expectation
    *µ* with the typical linear equation *w*₁*x*₁ + *w*₂*x*₂ + … + *wₖxₖ* + *b* rather
    than the actual target. We know that we will not hit the target anyway, so we
    settle for the mean of the distribution that generates *y* instead.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 从过程的第1步开始，你也可以看到我们用典型的线性方程 *w*₁*x*₁ + *w*₂*x*₂ + … + *w*ₖx*ₖ* + *b* 来建模期望 *µ*，而不是实际目标。我们知道无论如何不会击中目标，因此我们接受生成
    *y* 的分布的均值。
- en: Extensions
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 扩展
- en: '**Generalized Linear Models.** We are not forced to use a normal distribution
    for the generation process. If we deal with a dataset that **only contains positive
    targets**, it might be beneficial to assume that a **Poisson** **distribution**
    Poi(*µ*) is used instead of a normal distribution. This gives you **Poisson regression**.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '**广义线性模型**。我们不必使用正态分布作为生成过程。如果我们处理的数据集**仅包含正目标**，那么假设使用**泊松****分布** Poi(*µ*)
    可能更有利，这样你就得到了**泊松回归**。'
- en: 'If our dataset only has the targets 0 and 1, use a **Bernoulli distribution**
    Ber(*p*), where *p* = sigmoid(*µ*), et voilà: you got **logistic regression**.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们的数据集只有目标0和1，使用**伯努利分布** Ber(*p*)，其中 *p* = sigmoid(*µ*)，那就是**逻辑回归**。
- en: Only numbers between 0, 1, …, *n*? Use a **binomial distribution** to get [**binomial
    regression**](https://en.wikipedia.org/wiki/Binomial_regression).
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 只有0, 1, …, *n* 之间的数字？使用**二项分布**来获取[**二项回归**](https://en.wikipedia.org/wiki/Binomial_regression)。
- en: 'The list goes on and on. Long story short:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 列表还在继续。长话短说：
- en: Think about which distribution could have generated the labelsyou observe in
    the data.
  id: totrans-25
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 思考一下哪个分布可能生成你在数据中观察到的标签。
- en: What Are We Actually Minimizing?
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 我们到底在最小化什么？
- en: 'Ok, so we decided on a model now. How do we train it now? How do we learn the
    parameters? Of course, you know: we minimized the (mean) squared error. But why?'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，我们现在决定了一个模型。那么我们怎么训练它？我们怎么学习参数？当然，你知道：我们最小化了（均方）误差。但为什么？
- en: 'The secret is that you just do a plan **maximum likelihood estimate** using
    the generation process we described before. The labels that we observe are *y*₁,
    *y*₂, …, *yₙ*, all of them independently generated via a normal distribution with
    means *µ*₁, *µ*₂, …, *µₙ.* What is the likelihood to see these *y*’s? It is:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 关键在于，你只需使用我们之前描述的生成过程进行**最大似然估计**。我们观察到的标签是 *y*₁, *y*₂, …, *y*ₙ*，它们都是通过具有均值
    *µ*₁, *µ*₂, …, *µ*ₙ 的正态分布独立生成的。看到这些 *y* 的可能性是多少？这是：
- en: '![](../Images/fb050a922b3f6db694c93cb8bcc19a2f.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fb050a922b3f6db694c93cb8bcc19a2f.png)'
- en: Image by the author.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供。
- en: We now want to find the parameters (that are hidden in the *µᵢ*’s) to **maximize**
    this term. This is equivalent to minimizing the mean squared error, as you can
    see.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在想找到（隐藏在 *µᵢ* 中的）参数，以**最大化**这一项。这等同于最小化均方误差，如你所见。
- en: Extensions
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 扩展
- en: '**Unequal Variances**. In fact, *σ* does not have to be constant. You can have
    a different *σᵢ* for each observation in your dataset. Then, you would minimize'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '**不等方差**。实际上，*σ* 不必是恒定的。你可以为数据集中每个观察值设置不同的 *σᵢ*。然后，你将最小化'
- en: '![](../Images/71c6b101588ff794cbf66d13b9fa9aa9.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/71c6b101588ff794cbf66d13b9fa9aa9.png)'
- en: Image by the author.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供。
- en: instead, which is **least squares with sample weights *s*.** Modeling libraries
    typically allow you to set these weights. In scikit-learn, for example, you can
    set the `sample_weight` keyword in the `fit` function.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 代替，这就是**带有样本权重 *s* 的最小二乘法**。建模库通常允许你设置这些权重。在 scikit-learn 中，例如，你可以在 `fit` 函数中设置
    `sample_weight` 关键字。
- en: This way, you can put more emphasis on certain observations by increasing the
    corresponding *s*. This is equivalent to decreasing the variance *σ²*, i.e. you
    are more confident that the error for this observation is smaller. This method
    is also called [**weighted least squares**](https://en.wikipedia.org/wiki/Weighted_least_squares).
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 这样，你可以通过增加相应的 *s* 来更强调某些观察值。这等同于减小方差 *σ²*，即你更确信这个观察值的误差较小。这种方法也称为[**加权最小二乘法**](https://en.wikipedia.org/wiki/Weighted_least_squares)。
- en: '**Variances Depending on The Input.** You can even say that the variance is
    also dependent on the input *x*. In this case, you get the interesting loss function
    that is also called **variance attenuation**:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '**输入的方差依赖性。** 你甚至可以说方差也依赖于输入 *x*。在这种情况下，你会得到一个有趣的损失函数，这也被称为**方差衰减**：'
- en: '![](../Images/9d4d422fccae6230d67479541e8be6ec.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9d4d422fccae6230d67479541e8be6ec.png)'
- en: 'The whole derivation process is outlined here:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 整个推导过程在这里概述：
- en: '[](/get-uncertainty-estimates-in-neural-networks-for-free-48f2edb82c8f?source=post_page-----e53c579aef5b--------------------------------)
    [## Get Uncertainty Estimates in Regression Neural Networks for Free'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/get-uncertainty-estimates-in-neural-networks-for-free-48f2edb82c8f?source=post_page-----e53c579aef5b--------------------------------)
    [## 免费获取回归神经网络中的不确定性估计'
- en: Given the right loss function, a standard neural network can output uncertainty
    as well
  id: totrans-42
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 给定合适的损失函数，标准神经网络也可以输出不确定性
- en: towardsdatascience.com](/get-uncertainty-estimates-in-neural-networks-for-free-48f2edb82c8f?source=post_page-----e53c579aef5b--------------------------------)
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/get-uncertainty-estimates-in-neural-networks-for-free-48f2edb82c8f?source=post_page-----e53c579aef5b--------------------------------)
- en: '**Regularization.** Instead of only maximizing the likelihood of the observed
    labels*y*₁, *y*₂, …, *yₙ,* you can take a **Bayesian standpoint** and **maximize
    the a posteriori likelihood**'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '**正则化。** 除了仅仅最大化观察到的标签 *y*₁, *y*₂, …, *yₙ* 的似然，你还可以采用**贝叶斯观点**并**最大化后验似然**'
- en: '![](../Images/1ec367c68abfd26a1b36928a4056662b.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1ec367c68abfd26a1b36928a4056662b.png)'
- en: Image by the author.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图像。
- en: Here, *p*(*y* | *w*) is the likelihood function from above. We have to decide
    on a probability density for *p*(*w*), a so-called **prior or a prior distribution**.
    If we say that the parameters are independently normally distributed around 0,
    i.e. *wᵢ* ~ *N*(0, *ν²*), then we end up with **L2 regularization, i.e. ridge
    regression**. For a Laplace distribution, we recover **L1 regularization, i.e.
    LASSO**.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*p*(*y* | *w*) 是上述的似然函数。我们必须决定一个 *p*(*w*) 的概率密度，即所谓的**先验或先验分布**。如果我们说参数独立地服从围绕
    0 的正态分布，即 *wᵢ* ~ *N*(0, *ν²*)，那么我们最终会得到**L2 正则化，即岭回归**。对于拉普拉斯分布，我们得到**L1 正则化，即
    LASSO**。
- en: Why is that? Let’s use the normal distribution as an example. We have
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么呢？让我们以正态分布为例。我们有
- en: '![](../Images/bc7b382737b4d8a13915832e1561e80f.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bc7b382737b4d8a13915832e1561e80f.png)'
- en: Image by the author.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图像。
- en: so together with our formula for *p*(*y* | *w*) from above, we have to maximize
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，加上我们上述 *p*(*y* | *w*) 的公式，我们必须最大化
- en: '![](../Images/3a2d7162b93c1510637fec06a34b3e4a.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3a2d7162b93c1510637fec06a34b3e4a.png)'
- en: Image by the author.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图像。
- en: which means that we have to **minimize the mean squared error plus some regularization
    hyperparameter time the L2 norm of *w*.**
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着我们必须**最小化均方误差加上一些正则化超参数乘以 *w* 的 L2 范数**。
- en: '*Note that we dropped the denominator* p*(*y*) from the Bayes formula since
    it does not depend on* w*, so we can ignore it for optimization.*'
  id: totrans-55
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*注意，我们从贝叶斯公式中省略了分母 *p*(*y*)，因为它不依赖于 *w*，所以我们可以忽略它进行优化。*'
- en: You can use any other prior distribution for your parameters to create more
    interesting regularizations. You can even say that your parameters *w* are normally
    distributed but **correlated** with some correlation matrix Σ*.*
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用任何其他的先验分布来创建更有趣的正则化。你甚至可以说你的参数 *w* 是正态分布的，但与某个相关矩阵 Σ* **相关**。*
- en: '*Let us assume that Σ is* positive-definite*, i.e. we are in the non-degenerate
    case. Otherwise, there is no density* p*(*w*).*'
  id: totrans-57
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*假设 Σ 是* 正定的*，即我们处于非退化情况。否则，没有密度 *p*(*w*)。*'
- en: If you do the math, you will find out that we then have to optimize
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你进行数学计算，你会发现我们然后必须优化
- en: '![](../Images/eb5af9d7a702a085ca22cf56f5785b6e.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/eb5af9d7a702a085ca22cf56f5785b6e.png)'
- en: Image by the author.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图像。
- en: 'for some matrix Γ. **Note: Γ is invertible and we have Σ⁻¹ = ΓᵀΓ.** This is
    also called **Tikhonov regularization**.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 对于某个矩阵 Γ。**注意：Γ 是可逆的，我们有 Σ⁻¹ = ΓᵀΓ。** 这也被称为**提霍诺夫正则化**。
- en: '**Hint:** start with the fact that'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '**提示：** 从以下事实开始'
- en: '![](../Images/81a3a8873f6d531a2ebb9bd177187282.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/81a3a8873f6d531a2ebb9bd177187282.png)'
- en: Image by the author.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图像。
- en: and remember that positive-definite matrices can be [decomposed into a product
    of some invertible matrix and its transpose](https://en.wikipedia.org/wiki/Definite_matrix#Decomposition).
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 并且记住，正定矩阵可以被[分解成某个可逆矩阵及其转置的乘积](https://en.wikipedia.org/wiki/Definite_matrix#Decomposition)。
- en: Minimize The Loss Function
  id: totrans-66
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 最小化损失函数
- en: Great, so we defined our model and know what we want to optimize. But how can
    we optimize it, i.e. learn the best parameters that minimize the loss function?
    And when is there a unique solution? Let’s find out.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 很好，我们定义了我们的模型并知道我们想优化什么。但是我们如何优化它，即学习最优的参数以最小化损失函数？什么时候会有唯一解？让我们来看看。
- en: Ordinary Least Squares
  id: totrans-68
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 普通最小二乘法
- en: Let us assume that we do not regularize and don’t use sample weights. Then,
    the MSE can be written as
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们不进行正则化，也不使用样本权重。那么，均方误差可以写作
- en: '![](../Images/7af931d447d5df14f791c8484c9a3d4b.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7af931d447d5df14f791c8484c9a3d4b.png)'
- en: Image by the author.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 图像由作者提供。
- en: This is quite abstract, so let us write it differently as
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 这非常抽象，所以让我们以不同的方式书写
- en: '![](../Images/5a5cea791903d850c7d7ff4bd115adab.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5a5cea791903d850c7d7ff4bd115adab.png)'
- en: Image by the author.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 图像由作者提供。
- en: Using [matrix calculus](https://en.wikipedia.org/wiki/Matrix_calculus#Scalar-by-vector_identities),
    you can take the derivative of this function with respect to *w* (we assume that
    the bias term *b* is included there)*.*
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 [矩阵微积分](https://en.wikipedia.org/wiki/Matrix_calculus#Scalar-by-vector_identities)，你可以对这个函数关于
    *w* 求导（我们假设偏置项 *b* 已经包含在内）。
- en: '![](../Images/f5820d6701156de11010d68969e10f50.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f5820d6701156de11010d68969e10f50.png)'
- en: Image by the author.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 图像由作者提供。
- en: If you set this gradient to zero, you end up with
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你将这个梯度设为零，你会得到
- en: '![](../Images/dacab3b7869c79f9cab2f4442e478c9e.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/dacab3b7869c79f9cab2f4442e478c9e.png)'
- en: Image by the author.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 图像由作者提供。
- en: If the (*n* × *k*)-matrix *X* has a rank of *k*, so does the (*k* × *k*)-matrix
    *X*ᵀ*X,* i.e. it is invertible*. Why?* It follows from [rank(*X*) *=* rank(*X*ᵀ*X*)](https://en.wikipedia.org/wiki/Rank_(linear_algebra)#Properties)*.*
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 如果 (*n* × *k*) 矩阵 *X* 的秩为 *k*，那么 (*k* × *k*) 矩阵 *X*ᵀ*X* 也是，即它是可逆的。为什么？这可以从 [rank(*X*)
    *=* rank(*X*ᵀ*X*)](https://en.wikipedia.org/wiki/Rank_(linear_algebra)#Properties)
    中推导出来。
- en: In this case, we get the **unique solution**
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们得到**唯一解**
- en: '![](../Images/aa182f0811b2619d41bc9dbac897f904.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/aa182f0811b2619d41bc9dbac897f904.png)'
- en: Image by the author.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 图像由作者提供。
- en: '***Note:*** *Software packages do not optimize like this but instead use gradient
    descent or other iterative techniques because it is faster. Still, the formula
    is nice and gives us some high-level insights about the problem.*'
  id: totrans-85
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '***注意：*** *软件包不像这样优化，而是使用梯度下降或其他迭代技术，因为这样更快。不过，公式很好，并且为我们提供了有关问题的一些高级见解。*'
- en: But is this really a minimum? We can find out by computing the Hessian, which
    is *X*ᵀ*X.* The matrix is positive-semidefinite since *w*ᵀ*X*ᵀ*Xw = |Xw|²* ≥ 0
    for any *w*. It is even **strictly** positive-definite since *X*ᵀ*X* is invertible,
    i.e. 0 is not an eigenvector, so our optimal *w* is indeed minimizing our problem.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 但这真的能达到最小值吗？我们可以通过计算 Hessian 矩阵来找出，Hessian 矩阵是 *X*ᵀ*X*。该矩阵是半正定的，因为 *w*ᵀ*X*ᵀ*Xw
    = |Xw|²* ≥ 0 对于任何 *w*。它甚至是**严格**正定的，因为 *X*ᵀ*X* 是可逆的，即 0 不是特征值，所以我们的最优 *w* 确实在最小化我们的问题。
- en: Perfect Multicollinearity
  id: totrans-87
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 完美的多重共线性
- en: That was the friendly case. But what happens if *X* has a rank smaller than
    *k*? This might happen if we have two features in our dataset where one is a multiple
    of the other, e.g. we use the features *height (in m)* and *height (in cm)* in
    our dataset. Then we have *height (in cm) = 100 * height (in m).*
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 这只是友好的情况。但如果 *X* 的秩小于 *k* 会发生什么？如果我们数据集中有两个特征，其中一个是另一个的倍数，例如，我们在数据集中使用 *height
    (in m)* 和 *height (in cm)* 作为特征。然后我们有 *height (in cm) = 100 * height (in m)*。
- en: It can also happen if we one-hot encode categorical data and do not drop one
    of the columns. For example, if we have a feature *color* in our dataset that
    can be red, green, or blue, then we can one-hot encode and end up with three columns
    *color_red, color_green,* and *color_blue*. For these features, we have *color_red
    + color_green + color_blue =* 1, which induces perfect multicollinearity as well.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们对分类数据进行独热编码而不丢弃其中一列，也可能发生这种情况。例如，如果我们数据集中有一个特征 *color*，它可以是红色、绿色或蓝色，那么我们可以进行独热编码，得到三列
    *color_red, color_green,* 和 *color_blue*。对于这些特征，我们有 *color_red + color_green +
    color_blue =* 1，这也会引起完美的多重共线性。
- en: In these cases, the rank of *X*ᵀ*X* is also smaller than *k*, so this matrix
    is not invertible.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些情况下，*X*ᵀ*X* 的秩也小于 *k*，因此这个矩阵是不可逆的。
- en: End of story.
  id: totrans-91
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 故事结束。
- en: 'Or not? Actually, no, because it can mean two things: (*X*ᵀ*X*)*w = X*ᵀ*y*
    has'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 还是不行？实际上不是，因为这可能意味着两件事：(*X*ᵀ*X*)*w = X*ᵀ*y* 有
- en: no solution or
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 没有解或
- en: infinitely many solutions.
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 无限多的解。
- en: It turns out that in our case, we can obtain one solution using the [Moore-Penrose
    inverse](https://en.wikipedia.org/wiki/Moore%E2%80%93Penrose_inverse). This means
    that we are in the case of infinitely many solutions, all of them giving us the
    same (training) mean squared error loss.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 事实证明，在我们的案例中，我们可以使用 [Moore-Penrose 伪逆](https://en.wikipedia.org/wiki/Moore%E2%80%93Penrose_inverse)
    获得一个解。这意味着我们处于无穷多解的情况，这些解都给我们相同的（训练）均方误差损失。
- en: If we denote the Moore-Penrose inverse of *A* by *A*⁺, we can solve the linear
    system of equations as
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们用 *A*⁺ 表示 *A* 的 Moore-Penrose 伪逆，我们可以求解线性方程组为
- en: '![](../Images/b99b7b2b70d821c22f23e667fd03c984.png)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b99b7b2b70d821c22f23e667fd03c984.png)'
- en: Image by the author.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片。
- en: To get the other infinitely many solutions, just add the null space of *X*ᵀ*X*
    to this specific solution.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 要获得其他无穷多解，只需将 *X*ᵀ*X* 的零空间添加到这个特定解中。
- en: Minimization With Tikhonov Regularization
  id: totrans-100
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 Tikhonov 正则化的最小化
- en: Recall that we could add a prior distribution to our weights. We then had to
    minimize
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，我们可以向权重中添加先验分布。然后我们需要最小化
- en: '![](../Images/67f72d916860474636a7287cef747b8e.png)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/67f72d916860474636a7287cef747b8e.png)'
- en: Image by the author.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片。
- en: for some invertible matrix Γ. Following the same steps as in ordinary least
    squares, i.e. taking the derivative with respect to *w* and setting the result
    to zero, the solution is
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 对于某个可逆矩阵 Γ。按照普通最小二乘法中的相同步骤，即对 *w* 求导并将结果设置为零，解为
- en: '![](../Images/ad542ac73d8a2a760adf430eb09b0d4a.png)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ad542ac73d8a2a760adf430eb09b0d4a.png)'
- en: Image by the author.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片。
- en: 'The neat part:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 精彩的部分：
- en: XᵀX + ΓᵀΓ is always invertible!
  id: totrans-108
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: XᵀX + ΓᵀΓ 始终是可逆的！
- en: Let us find out why. It suffices to show that the null space of *X*ᵀ*X* + ΓᵀΓ
    is only {0}. So, let us take a *w* with (*X*ᵀ*X* + ΓᵀΓ)*w* = 0\. Now, our goal
    is to show that *w* = 0.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们找出原因。只需证明 *X*ᵀ*X* + ΓᵀΓ 的零空间仅为 {0}。因此，我们取一个 *w* 使得 (*X*ᵀ*X* + ΓᵀΓ)*w* = 0。现在，我们的目标是证明
    *w* = 0。
- en: From (*X*ᵀ*X* + ΓᵀΓ)*w* = 0 it follows that
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 从 (*X*ᵀ*X* + ΓᵀΓ)*w* = 0 可得
- en: '![](../Images/dfbadd99aed70af03cecda5cacf7730a.png)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/dfbadd99aed70af03cecda5cacf7730a.png)'
- en: Image by the author.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片。
- en: which in turn implies |Γ*w*| = 0 → Γ*w =* 0*.* SinceΓ is invertible, *w* has
    to be 0\. Using the same calculation, we can see that the Hessian is also positive-definite.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 这也意味着 |Γ*w*| = 0 → Γ*w = *0*。由于 Γ 是可逆的，*w* 必须是 0。通过相同的计算，我们可以看到 Hessian 矩阵也是正定的。
- en: Nice, so Tikhonov regularization automatically helps make the solution unique!
    Since ridge regression is a special case of Tikhonov regression (for Γ = λ*Iₖ,
    Iₖ* is the *k*-dimensional identity matrix), the same holds there.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 很好，因此 Tikhonov 正则化自动帮助使解唯一！由于岭回归是 Tikhonov 回归的特例（对于 Γ = λ*Iₖ，其中 Iₖ* 是 *k* 维单位矩阵），因此同样适用。
- en: Adding Sample Weights
  id: totrans-115
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 添加样本权重
- en: As a last point, let us also add sample weights to the Tikhonov regularization.
    Adding sample weights is equivalent to minimizing
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们还将样本权重添加到 Tikhonov 正则化中。添加样本权重等同于最小化
- en: '![](../Images/e85c7dc51fbd41a64ec7c105af11ba2b.png)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e85c7dc51fbd41a64ec7c105af11ba2b.png)'
- en: Image by the author.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片。
- en: For some diagonal matrix *S* with positive diagonal entries *sᵢ.* Minimizing
    is as straightforward as in the case of ordinary least squares. The result is
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 对于某些对角矩阵 *S* 其对角元素 *sᵢ* 为正，最小化问题和普通最小二乘法一样简单。结果是
- en: '![](../Images/6b9517176c23bddef1d77cbfd7797681.png)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6b9517176c23bddef1d77cbfd7797681.png)'
- en: Image by the author.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片。
- en: '**Note:** The Hessian is also positive-definite.'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意：** Hessian 矩阵也是正定的。'
- en: Homework for you
  id: totrans-123
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 你的作业
- en: Assume that for the Tikhonov regularization, we do not impose that the weights
    should be centered around 0, but some other point *w*₀. Show that the optimization
    problem becomes
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 假设对于 Tikhonov 正则化，我们不强制权重围绕 0，而是围绕某个点 *w*₀。证明优化问题变为
- en: '![](../Images/177006d6d3b5ae0ee092be0de5fd1003.png)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/177006d6d3b5ae0ee092be0de5fd1003.png)'
- en: Image by the author.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片。
- en: and that the solution is
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 结果是
- en: '![](../Images/04adac9e8b0679dd816373b0e33fc549.png)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/04adac9e8b0679dd816373b0e33fc549.png)'
- en: Image by the author.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片。
- en: This is the most general form of Tikhov regularization. Some people prefer to
    define *P* := *S*², *Q* := ΓᵀΓ, as [done here](https://en.wikipedia.org/wiki/Ridge_regression#Generalized_Tikhonov_regularization).
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 这是 Tikhonov 正则化的最一般形式。有些人更喜欢定义 *P* := *S*²，*Q* := ΓᵀΓ，如 [这里所示](https://en.wikipedia.org/wiki/Ridge_regression#Generalized_Tikhonov_regularization)。
- en: Conclusion
  id: totrans-131
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: In this article, I took you on a journey through several advanced aspects of
    linear regression. By adopting a generative view, we could see that generalized
    linear models just differ from the *normal* linear models only in the type of
    distribution that is used to sample the target *y*.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我带你探讨了线性回归的几个高级方面。通过采用生成视角，我们可以看到广义线性模型与*普通*线性模型的区别仅在于用于抽样目标*y*的分布类型。
- en: Then we have seen that minimizing the mean squared error is equivalent to maximizing
    the likelihood of the observed values. If we impose a prior normal distribution
    on the learnable parameters, we end up with Tikhonov (and L2 as a special case)
    regularization. We can use different prior distributions such as the Laplace distribution
    as well, but then there are no closed solution formulas anymore. Still, convex
    programming approaches also let you find the best parameters.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们看到，最小化均方误差等同于最大化观测值的似然。如果我们对可学习参数施加一个先验正态分布，我们最终会得到Tikhonov（以及L2作为特例）正则化。我们也可以使用不同的先验分布，如拉普拉斯分布，但那样就没有封闭的解公式了。不过，凸优化方法也可以帮助你找到最佳参数。
- en: As a last step, we found a lot of direct solution formulas for each minimization
    problem considered. These formulas are usually not used in practice for large
    datasets, but we could see that the solutions are always unique. And we also learned
    to do some calculus on the way. 😉
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 作为最后一步，我们为每个最小化问题找到了很多直接的解公式。这些公式在大数据集的实际应用中通常不会使用，但我们可以看到这些解总是唯一的。我们在过程中也学会了一些微积分。😉
- en: I hope that you learned something new, interesting, and valuable today. Thanks
    for reading!
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 我希望你今天学到了一些新的、有趣的和有价值的东西。感谢阅读！
- en: '*If you have any questions, write me on* [*LinkedIn*](https://www.linkedin.com/in/dr-robert-k%C3%BCbler-983859150/)*!*'
  id: totrans-136
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*如果你有任何问题，请在* [*LinkedIn*](https://www.linkedin.com/in/dr-robert-k%C3%BCbler-983859150/)*上联系我！*'
- en: And if you want to dive deeper into the world of algorithms, give my new publication
    **All About Algorithms** a try! I’m still searching for writers!
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想更深入地了解算法的世界，可以试试我的新出版物**所有关于算法**！我仍在寻找作者！
- en: '[](https://medium.com/all-about-algorithms?source=post_page-----e53c579aef5b--------------------------------)
    [## All About Algorithms'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '[## 所有关于算法](https://medium.com/all-about-algorithms?source=post_page-----e53c579aef5b--------------------------------)'
- en: From intuitive explanations to in-depth analysis, algorithms come to life with
    examples, code, and awesome…
  id: totrans-139
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 从直观的解释到深入的分析，算法通过示例、代码和精彩的内容得以呈现…
- en: medium.com](https://medium.com/all-about-algorithms?source=post_page-----e53c579aef5b--------------------------------)
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '[medium.com](https://medium.com/all-about-algorithms?source=post_page-----e53c579aef5b--------------------------------)'
