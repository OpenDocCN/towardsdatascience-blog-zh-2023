- en: Understand & Implement Masked AutoRegressive Flow with TensorFlow
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解并实现带掩码的自回归流与 TensorFlow
- en: 原文：[https://towardsdatascience.com/understand-implement-masked-autoregressive-flow-with-tensorflow-9c361cd1354c](https://towardsdatascience.com/understand-implement-masked-autoregressive-flow-with-tensorflow-9c361cd1354c)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/understand-implement-masked-autoregressive-flow-with-tensorflow-9c361cd1354c](https://towardsdatascience.com/understand-implement-masked-autoregressive-flow-with-tensorflow-9c361cd1354c)
- en: Flow-Based Models for Density Estimation with TensorFlow
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 TensorFlow 进行密度估计的流模型
- en: '[](https://saptashwa.medium.com/?source=post_page-----9c361cd1354c--------------------------------)[![Saptashwa
    Bhattacharyya](../Images/b01238113a1f6b91cb6fb0fbfa50303a.png)](https://saptashwa.medium.com/?source=post_page-----9c361cd1354c--------------------------------)[](https://towardsdatascience.com/?source=post_page-----9c361cd1354c--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----9c361cd1354c--------------------------------)
    [Saptashwa Bhattacharyya](https://saptashwa.medium.com/?source=post_page-----9c361cd1354c--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://saptashwa.medium.com/?source=post_page-----9c361cd1354c--------------------------------)[![Saptashwa
    Bhattacharyya](../Images/b01238113a1f6b91cb6fb0fbfa50303a.png)](https://saptashwa.medium.com/?source=post_page-----9c361cd1354c--------------------------------)[](https://towardsdatascience.com/?source=post_page-----9c361cd1354c--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----9c361cd1354c--------------------------------)
    [Saptashwa Bhattacharyya](https://saptashwa.medium.com/?source=post_page-----9c361cd1354c--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----9c361cd1354c--------------------------------)
    ·8 min read·Feb 21, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page-----9c361cd1354c--------------------------------)
    ·8 分钟阅读·2023年2月21日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/891350cbc87604e6ea8e847db10585fc.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/891350cbc87604e6ea8e847db10585fc.png)'
- en: 'Fig: From random to not-so-random! Source: Author’s Notebook (see References
    below).'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图：从随机到不那么随机！来源：作者笔记本（见下文参考文献）。
- en: Previously we went through the details of the [mathematics behind Normalizing
    Flows](/getting-started-with-normalizing-flows-linear-algebra-probability-f2b863ff427d)
    and some examples of [transforming probability distributions](/transforming-probability-distributions-using-normalizing-flows-bcc5ed6ac2c9).
    Here we combine all these concepts to understand Autoregressive Flows and how
    to implement them using TensorFlow Probability library. What can you expect from
    this post —
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 之前我们详细介绍了[正常化流背后的数学](/getting-started-with-normalizing-flows-linear-algebra-probability-f2b863ff427d)以及一些[变换概率分布的示例](/transforming-probability-distributions-using-normalizing-flows-bcc5ed6ac2c9)。在这里，我们结合所有这些概念来理解自回归流以及如何使用
    TensorFlow Probability 库实现它们。您可以从这篇文章中期待什么 —
- en: Why Triangular Matrices are crucial for Autoregressive Flows?
  id: totrans-9
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为什么三角矩阵对自回归流至关重要？
- en: Basic constructions of Autoregressive Flow-based models
  id: totrans-10
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 自回归流模型的基本构造
- en: — Masked Autoregressive Flow (MAF)
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: — 掩码自回归流（MAF）
- en: — Inverse Autoregressive Flow (IAF)
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: — 反向自回归流（IAF）
- en: 3\. How to implement MAF in TensorFlow and train them for density estimation
    tasks?
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 3\. 如何在 TensorFlow 中实现 MAF 并训练它们以进行密度估计任务？
- en: Without any delay, let’s begin!
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 不再耽搁，让我们开始吧！
- en: 'Computational Issues in Normalizing Flows:'
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 正常化流中的计算问题：
- en: Before we discuss models such as Masked Autoregressive Flows etc., we review
    the change of variable rules for 1D and higher Dimensional scenarios, which will
    help us appreciate the computational cost in Normalizing Flows.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在讨论诸如掩码自回归流等模型之前，我们回顾一维和更高维场景中的变量变换规则，这将帮助我们理解正常化流中的计算成本。
- en: 'Previously [we went into detail](https://medium.com/towards-data-science/getting-started-with-normalizing-flows-linear-algebra-probability-f2b863ff427d)
    to derive the change of variable rule where we start from a base distribution
    *u* and a bijector *ϕ* such that *x* = *ϕ*(*u*). In this case, we can simply write
    the change of variables rule as follows:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 之前我们[详细讨论了](https://medium.com/towards-data-science/getting-started-with-normalizing-flows-linear-algebra-probability-f2b863ff427d)如何推导变量变换规则，其中我们从基础分布
    *u* 和双射 *ϕ* 开始，使得 *x* = *ϕ*(*u*)。在这种情况下，我们可以简单地写出变量变换规则如下：
- en: '![](../Images/6f7d2931f1a46dcdc234bcce784d7183.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6f7d2931f1a46dcdc234bcce784d7183.png)'
- en: 'Eq. 1: Change of variable rule for 1D'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 等式 1：一维的变量变换规则
- en: 'For Normalizing Flows we compose (‘chain’) several bijectors to transform a
    simple distribution into a more complicated one. For example, we can compose *K*
    bijection operations as below to transform our base distribution (say *u_0*) to
    our desired complex distribution *x* as below:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 对于归一化流，我们组合（‘链式’）几个双射，将简单分布转变为更复杂的分布。例如，我们可以如下组合 *K* 次双射操作，将我们的基础分布（如 *u_0*）转换为我们所需的复杂分布
    *x*。
- en: '![](../Images/47d8e9e76cdbb006878c15a0a66e86b5.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/47d8e9e76cdbb006878c15a0a66e86b5.png)'
- en: 'Eq. 2: Compose bijectors to transform simple distribution into a complicated
    one'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 方程 2：组合双射以将简单分布转变为复杂分布。
- en: 'For *K* transformations we can modify Eq. 1 as below:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 *K* 变换，我们可以如下修改方程 1：
- en: '![](../Images/c32c9c9f7b7deb3d4c0ba073909392e6.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c32c9c9f7b7deb3d4c0ba073909392e6.png)'
- en: 'Eq. 3: Rewriting Eq. 1 from 1 bijection operation to K transformations.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 方程 3：将方程 1 从 1 次双射操作重写为 K 次变换。
- en: One of the biggest issues with implementing Normalizing Flows is the computational
    complexity of the log-det-jacobian. **Calculating the determinant of the Jacobian
    of *n*×*n* via a process like Gaussian Elimination has a** [**runtime complexity**](https://en.wikipedia.org/wiki/Gaussian_elimination#Computing_determinants)
    **of O(*n³*).**
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 实现归一化流的最大问题之一是计算对数-行列式雅可比矩阵的计算复杂度。**通过像高斯消去这样的过程计算 *n*×*n* 的雅可比矩阵的行列式具有** [**运行时间复杂度**](https://en.wikipedia.org/wiki/Gaussian_elimination#Computing_determinants)
    **为 O(*n³*)。**
- en: So we need some simplifications in the procedures described above and we can
    now move to learn some of the simplifications that will help us reduce the computational
    complexity.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们需要对上述过程进行一些简化，现在可以开始学习一些有助于减少计算复杂度的简化方法。
- en: 'Triangular Matrix & Autoregressive Flow:'
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 三角矩阵与自回归流：
- en: If the transformation matrix is triangular, then calculating the determinant
    is fairly easy. For an *n × n* square matrix, the runtime complexity is *O(n).*
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 如果变换矩阵是三角形的，那么计算行列式相当容易。对于 *n × n* 的方阵，运行时间复杂度是 *O(n)*。
- en: '![](../Images/8b87f1d1f0d9bf67fc01059e34562652.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8b87f1d1f0d9bf67fc01059e34562652.png)'
- en: Calculating the Determinant Triangular Matrix only requires diagonal elements.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 计算三角矩阵的行列式只需要对角元素。
- en: As we can see above that for triangular matrices (upper/lower), we need only
    the diagonal elements to calculate the determinants, so the runtime complexity
    is linear.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 如上所示，对于三角矩阵（上三角/下三角），我们只需要对角元素来计算行列式，因此运行时间复杂度是线性的。
- en: Let’s consider the lower triangular matrix where *a[i][j]*=0; *j* > *i*. We
    impose a very similar concept in Autoregressive flows.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑下三角矩阵，其中 *a[i][j]*=0; *j* > *i*。我们在自回归流中施加了非常类似的概念。
- en: We consider a *D* dimensional vector *u* and it goes through 1,2,…*K* transformations
    just as before. The idea of Flow-based models is to use a transformation (or chain
    of transformations) *ϕ* operating on a real vector *u* sampled from p_u(u). Before
    in the basics of [Bijection & Diffeomorphism](https://medium.com/towards-data-science/getting-started-with-normalizing-flows-linear-algebra-probability-f2b863ff427d),
    we discussed that when *u as a D dimensional vector gets transformed to x via
    K diffeomorphisms, we are saying that the base distribution is D dimensional and
    so will the final distribution (x) be. With this, we impose our Autoregressive
    condition to get a triangular matrix for log-det-Jacobian calculation as below:*
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 我们考虑一个 *D* 维向量 *u*，它经过 1,2,…*K* 次变换，就像以前一样。基于流模型的思想是使用一个（或一系列）变换 *ϕ* 对从 p_u(u)
    采样得到的实际向量 *u* 进行操作。在[双射与微分同胚](https://medium.com/towards-data-science/getting-started-with-normalizing-flows-linear-algebra-probability-f2b863ff427d)的基础知识中，我们讨论了当
    *u 作为 D 维向量通过 K 次微分同胚变换为 x 时，我们说基础分布是 D 维的，最终分布 (x) 也将是。这样，我们施加了自回归条件，以便获得一个三角矩阵来计算对数-行列式雅可比矩阵，如下所示：*
- en: '![](../Images/656e9be26e4582a8408d23d6f7f6192b.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/656e9be26e4582a8408d23d6f7f6192b.png)'
- en: 'Eq. 4: The autoregressive condition (eq. on left), gives rise to a triangular
    log-det jacobian matrix.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 方程 4：自回归条件（左侧的方程）产生一个三角形的对数-行列式雅可比矩阵。
- en: 'If you have ever used ARIMA models for time series analysis, then you know
    that the *autoregressive* term suggests the time series is based on past values.
    So we can constraint sequential data [*x*1, *x*2, …, *xD*] where each output (at
    a particular step) only depends on the observed values before and not the future
    ones. In a more mathematical notation, this would be the probability of observing
    *xi* conditioned on *x1*,…,*xi*−1 and the product of these conditional probabilities
    gives us the probability of observing the complete sequence:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你曾经使用过 ARIMA 模型进行时间序列分析，那么你会知道 *自回归* 项表明时间序列基于过去的值。因此，我们可以约束序列数据 [*x*1, *x*2,
    …, *xD*]，其中每个输出（在特定步骤）仅依赖于之前观察到的值，而不是未来的值。用更数学化的符号表示，就是观察 *xi* 的概率以 *x1*,…,*xi*−1
    为条件，这些条件概率的乘积给出观察完整序列的概率：
- en: '![](../Images/c406f35f47c2ff9c18919c47638c86d6.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c406f35f47c2ff9c18919c47638c86d6.png)'
- en: 'Eq. 5: Product of conditional probabilities gives the probability of observing
    the full data X.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 'Eq. 5: 条件概率的乘积给出观察完整数据 X 的概率。'
- en: The modeling of this conditional density is of our choice and several proposals
    have been made from simple univariate Gaussian to even a neural network. Let’s
    discuss a few of the popular ones!
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 条件密度的建模由我们选择，已经提出了多种方案，从简单的单变量高斯分布到甚至神经网络。让我们讨论一些流行的方法！
- en: 'Masked Autoregressive Flow (MAF):'
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 'Masked Autoregressive Flow (MAF):'
- en: 'For MAF, the conditionals as described above in Eq. 5 would be taken as a simple
    Normal distribution as below:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 MAF，上述 Eq. 5 中描述的条件分布将被视为简单的正态分布，如下所示：
- en: '![](../Images/b8a6b5846d54ab4d3425f6dcd4da5659.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b8a6b5846d54ab4d3425f6dcd4da5659.png)'
- en: 'Eq. 6: Conditionals above in Eq. 5 are assumed to be simple Gaussians'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 'Eq. 6: Eq. 5 中的条件分布假设为简单高斯分布'
- en: 'It’s also possible to generate new data starting from the base distribution
    *u* as below:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 也可以从基础分布 *u* 生成新数据，如下所示：
- en: '![](../Images/38ee95250c026eaff43457b2270cebc3.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/38ee95250c026eaff43457b2270cebc3.png)'
- en: 'Eq. 7: Generating new points given the base distribution (u), i.e. a set of
    random numbers'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 'Eq. 7: 给定基础分布 (u) 生成新点，即一组随机数'
- en: 'The above equation tells us another way to think about Autoregressive models
    as a transformation *f* from the space of random numbers (*u*) to the data *x*.
    Since the transformations are affine (scale & shift), to get back our base variables
    *u_i*, we don’t need to invert the functions themselves. This is also mentioned
    in MADE paper:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 上述方程告诉我们另一个将自回归模型视为从随机数空间 (*u*) 到数据 *x* 的变换 *f* 的方法。由于这些变换是仿射的（缩放和偏移），为了找回基础变量
    *u_i*，我们不需要逆转这些函数。这也在 MADE 论文中提到：
- en: '![](../Images/db54a75149303105e80444256727dc71.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/db54a75149303105e80444256727dc71.png)'
- en: 'Eq. 8: Invert transformation from Eq. 7 to get back the base variable.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 'Eq. 8: 从 Eq. 7 中的变换中反转回基础变量。'
- en: '**This is extremely important for training strategy as we do not need to explicitly
    calculate the inverse of the functions *f_αi*, *f_μi*, and just need to evaluate
    them once (say during forward pass), we can use non-invertible functions like
    RELU.**'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '**这对训练策略极为重要，因为我们不需要显式计算函数 *f_αi*、*f_μi* 的逆，只需对它们进行一次评估（例如，在前向传递时），我们可以使用不可逆的函数，如
    RELU。**'
- en: There’s an excellent pictorial representation of the forward pass (and the inverse)
    MAF is given in [*Eric Jang’s blog*](https://blog.evjang.com/2018/01/nf2.html)(Check
    the references)
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [*Eric Jang 的博客*](https://blog.evjang.com/2018/01/nf2.html) 中提供了 MAF 的前向传递（以及反向传递）的优秀图示（查看参考文献）。
- en: '![](../Images/7706871f9aff789f898316e9a9a1b85d.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7706871f9aff789f898316e9a9a1b85d.png)'
- en: 'Fig. 2: The forward pass of MAF. Source: [Eric Jang’s Amazing Blog](https://blog.evjang.com/2018/01/nf2.html)
    on Normalizing Flows.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '图 2: MAF 的前向传递。来源: [Eric Jang 的精彩博客](https://blog.evjang.com/2018/01/nf2.html)
    关于归一化流。'
- en: These are the basis of [Masked AutoRegressive Flow for Density Estimation](https://arxiv.org/pdf/1705.07057.pdf)
    paper and hopefully, this Normalizing Flows series is helping you to decipher
    large chunks of it.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是 [Masked AutoRegressive Flow for Density Estimation](https://arxiv.org/pdf/1705.07057.pdf)
    论文的基础，希望这个归一化流系列能帮助你解读其中的大部分内容。
- en: 'Inverse Autoregressive Flow (IAF):'
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 'Inverse Autoregressive Flow (IAF):'
- en: 'The transformation rules for IAF are also clearly explained in the [MAF](https://arxiv.org/pdf/1705.07057.pdf)
    paper. The main difference from MAF in IAF is that for calculating the scale and
    shift variables (for affine transformation) we use the random variables (*u*)
    instead of the data variables (*x*). Below are the transformation rules:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: IAF的变换规则在[MAF](https://arxiv.org/pdf/1705.07057.pdf)论文中也有清晰解释。IAF与MAF的主要区别在于，对于计算缩放和偏移变量（用于仿射变换），我们使用随机变量(*u*)而不是数据变量(*x*)。以下是变换规则：
- en: '![](../Images/7a1bd9784aa7adbf94bb3f64adc7155a.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7a1bd9784aa7adbf94bb3f64adc7155a.png)'
- en: 'Eq. 9: Compare this to Eq. 7 to see the difference between MAF and IAF'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '等式 9: 将其与等式 7 对比，查看MAF和IAF之间的差异'
- en: The uncanny similarity between MAF and IAF is hard to miss; inverse of the IAF
    is the forward pass of the MAF!
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: MAF和IAF之间惊人的相似性是难以忽视的；IAF的逆是MAF的前向传播！
- en: 'Train MAF with TensorFlow Probability:'
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用TensorFlow Probability训练MAF：
- en: 'After the basics, now we are ready to use the TensorFlow Probability to implement
    MAF and train it to produce a particular distribution. Let’s make use of `sklearn.datasets`
    , specifically the `make_circles` dataset as below:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在掌握基础知识后，我们现在准备使用TensorFlow Probability来实现MAF，并训练它以产生特定的分布。我们使用`sklearn.datasets`，特别是如下所示的`make_circles`数据集：
- en: '**1\. Load Dataset:**'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '**1\. 加载数据集：**'
- en: '[PRE0]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '![](../Images/b93a370d0081b43605a2051e75255d35.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b93a370d0081b43605a2051e75255d35.png)'
- en: 'Fig. 3: Our Target Distribution [Source: Author’s Notebook]'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '图 3: 我们的目标分布 [来源: 作者笔记本]'
- en: 'This will be our target distribution, datapoints distributed in circular manner.
    Our target will be starting from a random distribution to reach this orderly distribution
    using MAF. In TensorFlow Probability, it is fairly easy to implement MAF as it
    exists as a bijector called `MaskedAutoregressiveFlow` . Within this bijector,
    for the affine function (shift and scale), we can use another bijector `AutoregressiveNetwork`
    that implements the [Masked AutoEncoder for Density Estimation (MADE)](https://arxiv.org/abs/1502.03509)
    architecture, which the authors suggested for a computationally inexpensive way
    to obtain joint probabilities from a single pass through an autoencoder. Let’s
    see the implementation below:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 这将是我们的目标分布，数据点以圆形方式分布。我们的目标是从随机分布开始，通过使用MAF达到这种有序分布。在TensorFlow Probability中，实现MAF相当简单，因为它存在一个称为`MaskedAutoregressiveFlow`的双射器。在这个双射器中，对于仿射函数（平移和缩放），我们可以使用另一个双射器`AutoregressiveNetwork`，它实现了[Masked
    AutoEncoder for Density Estimation (MADE)](https://arxiv.org/abs/1502.03509)架构，作者建议这种方法是从自动编码器中一次通过得到联合概率的计算上便宜的方式。下面我们来看实现：
- en: Forward Pass through the MAF w base distribution as Normal
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 通过MAF的前向传播，基础分布为正态分布
- en: We started with a normal distribution and define the MAF function where the
    affine transformation is defined by the MADE architecture. The MADE architecture
    contains 2 hidden layers with 32 units each and ‘Relu’ activation. Following the
    previous post, where we describe how to transform the distribution using `TransformedDistribution`
    , we use the Normal distribution as the base and MAF as a bijector. Finally, we
    plot the contour plot of the probability of the transformed distribution.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从一个正态分布开始，并定义MAF函数，其中仿射变换由MADE架构定义。MADE架构包含2个隐藏层，每层32个单元，激活函数为‘Relu’。根据前一篇文章，我们描述了如何使用`TransformedDistribution`变换分布，我们使用正态分布作为基础，MAF作为双射器。最后，我们绘制了变换分布的概率等高线图。
- en: '![](../Images/d1da180ae07a8e1222c53d1b37fb9102.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d1da180ae07a8e1222c53d1b37fb9102.png)'
- en: 'Fig. 4: Starting from a Normal distribution we pass it through MAF where the
    affine transformation is provided by MADE structure (with Relu as activation).
    Source: Author’s notebook'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '图 4: 从正态分布开始，我们将其通过MAF，其中仿射变换由MADE结构提供（激活函数为Relu）。来源: 作者笔记本'
- en: '![](../Images/f7ae0c6dbfd245653e8b9dbce1ad5660.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f7ae0c6dbfd245653e8b9dbce1ad5660.png)'
- en: 'Fig. 5: Same as fig. 4, but instead of Relu as an activation we use sigmoid
    here. Source: Author’s notebook.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '图 5: 与图 4 相同，但激活函数由Relu改为sigmoid。来源: 作者笔记本。'
- en: 'Once we define the forward pass, we now are ready to train the Flow model.
    We are minimizing the negative log-likelihood and let’s just start with only one
    bijector (MAF network) to train:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们定义了前向传播，我们现在就可以开始训练Flow模型。我们在最小化负对数似然，首先从一个双射器（MAF网络）开始训练：
- en: Training Loop with only bijection operation (MAF)
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 仅使用双射操作（MAF）的训练循环
- en: 'As expected, once we plot the trained distribution, it’s easy to see that the
    result is far away from the true distribution. We sample from the trained distribution
    to plot the figure below:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 正如预期的那样，一旦我们绘制训练后的分布，很容易看出结果远离真实分布。我们从训练后的分布中采样，以绘制下面的图：
- en: '![](../Images/a013f0df2f7bbc04a9d7e63b5ffedbee.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a013f0df2f7bbc04a9d7e63b5ffedbee.png)'
- en: 'Fig. 6: Starting from a random distribution (middle), using only bijector,
    we can’t replicate the true distribution. Source: Author’s notebook'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6：从随机分布（中间）开始，仅使用双射，我们无法复制真实分布。来源：作者的笔记本
- en: 'But we also know that the idea of the flow-based models is to chain bijectors
    to transform from a simple to a complicated distribution. Here instead of only
    1 bijector, now we chain 4 bijectors (MAF) and minimize the negative log-likelihood
    of the final distribution:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 但我们也知道，基于流的模型的思想是链式双射将简单分布转换为复杂分布。在这里，我们不再仅使用 1 个双射，而是链式使用 4 个双射（MAF），并最小化最终分布的负对数似然：
- en: '[PRE1]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The permutation part ensures that the different dimensions of the *D* dimensional
    data (here *D*=2) influence each other. If the ordering of the dimensions never
    changes, it greatly reduces the expressive power of chaining bijectors in normalizing
    flows. While chaining the bijectors, we discard the last permutation as it is
    irrelevant in training (no other operation follows that permutation). With this
    more expressive model, we can expect something interesting! Let’s see the contour
    plot of the probability density of the trained distribution:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 排列部分确保了*D*维数据（这里*D*=2）的不同维度之间相互影响。如果维度的排序从未改变，这会大大降低在归一化流中链式双射的表达能力。在链式双射时，我们丢弃了最后的排列，因为它在训练中无关紧要（没有其他操作跟随这个排列）。有了这个更具表达力的模型，我们可以期待一些有趣的东西！让我们看看训练分布的概率密度等高线图：
- en: '![](../Images/89e36272b9caa05b0179c0de7a4f6460.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/89e36272b9caa05b0179c0de7a4f6460.png)'
- en: 'Fig. 7: Looks pretty similar to our target data (fig. 3)!! Source: Author’s
    notebook!'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7：看起来与我们的目标数据（图 3）非常相似！！来源：作者的笔记本！
- en: 'This is pretty cool, but also we can plot the sample distribution as our base
    normal distribution passes through these 4 MAF bijectors, let’s see:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 这很酷，但我们也可以绘制样本分布，因为我们的基础正态分布通过这 4 个 MAF 双射，来看一下：
- en: '![](../Images/ddb43d602eaf8898859767c92731329e.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ddb43d602eaf8898859767c92731329e.png)'
- en: 'Fig. 8: Transforming from random samples to circular samples by chaining 4
    MAF bijectors. Source: Author’s notebook.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8：通过链式 4 个 MAF 双射将随机样本转换为圆形样本。来源：作者的笔记本。
- en: We started from the very basics of [Normalizing Flows](https://medium.com/towards-data-science/getting-started-with-normalizing-flows-linear-algebra-probability-f2b863ff427d)
    i.e. diffeomorphism, and transformation rules for probability distribution etc.
    Then we used TensorFlow probability library to implement some of those concepts
    to transform probability distributions like from Normal to Bi-Modal in the [second
    post](https://medium.com/towards-data-science/transforming-probability-distributions-using-normalizing-flows-bcc5ed6ac2c9).
    Finally, we went through the basics of state of the art autoregressive flow models
    and why triangular matrices are important in this regard. Finally, we implemented
    MAF using TensorFlow and saw an example of training chained MAFs to transform
    normal to a bit more complex distribution. I hope this helps you to get started
    with flow-based models and also take steps towards diffusion models!!
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从[归一化流](https://medium.com/towards-data-science/getting-started-with-normalizing-flows-linear-algebra-probability-f2b863ff427d)的基础开始，即微分同胚和概率分布的变换规则等。然后我们使用
    TensorFlow 概率库实现了一些这些概念，将概率分布从正态分布转变为双峰分布，详见[第二篇文章](https://medium.com/towards-data-science/transforming-probability-distributions-using-normalizing-flows-bcc5ed6ac2c9)。最后，我们了解了最先进的自回归流模型的基础知识以及三角矩阵在这方面的重要性。最后，我们使用
    TensorFlow 实现了 MAF，并展示了训练链式 MAF 以将正态分布转换为稍微复杂一点的分布的示例。希望这能帮助你入门流式模型，并迈出向扩散模型发展的第一步！！
- en: '***If you’re interested in further fundamental machine learning concepts, you
    can consider joining Medium using*** [***My Link***](https://saptashwa.medium.com/membership)***.
    You don’t pay anything extra but I’ll get a tiny commission. Appreciate you all!!***'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '***如果你对进一步的基础机器学习概念感兴趣，你可以考虑加入 Medium 使用*** [***我的链接***](https://saptashwa.medium.com/membership)***。你无需支付额外费用，但我会获得一小部分佣金。感谢大家！！***'
- en: '[](https://medium.com/@saptashwa/membership?source=post_page-----9c361cd1354c--------------------------------)
    [## Join Medium with my referral link - Saptashwa Bhattacharyya'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@saptashwa/membership?source=post_page-----9c361cd1354c--------------------------------)
    [## 使用我的推荐链接加入 Medium - Saptashwa Bhattacharyya'
- en: More from Saptashwa (and many other writers on Medium). Your membership fee
    directly supports Saptashwa and other…
  id: totrans-90
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多来自 Saptashwa（以及 Medium 上的许多其他作者）。您的会员费直接支持 Saptashwa 和其他…
- en: medium.com](https://medium.com/@saptashwa/membership?source=post_page-----9c361cd1354c--------------------------------)
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '[medium.com](https://medium.com/@saptashwa/membership?source=post_page-----9c361cd1354c--------------------------------)'
- en: 'References:'
  id: totrans-92
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献：
- en: '[1] [Masked Autoregressive Flow for Density Estimation](https://arxiv.org/abs/1705.07057):
    Papamakarios, G. et.al.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] [用于密度估计的掩蔽自回归流](https://arxiv.org/abs/1705.07057): Papamakarios, G. 等'
- en: '[2] [Density Estimation Using RealNVP](https://arxiv.org/abs/1605.08803): Dinh,
    L. et.al.'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] [使用 RealNVP 进行密度估计](https://arxiv.org/abs/1605.08803): Dinh, L. 等'
- en: '[3] [Normalizing Flows: Part 2](https://blog.evjang.com/2018/01/nf2.html);
    Blog by Jang, E.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] [归一化流：第二部分](https://blog.evjang.com/2018/01/nf2.html); Jang, E. 的博客'
- en: '[4] [Flow Based Models](https://lilianweng.github.io/posts/2018-10-13-flow-models/);
    Blog by Weng, L.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] [基于流的模型](https://lilianweng.github.io/posts/2018-10-13-flow-models/); Weng,
    L. 的博客'
- en: '[5] [TensorFlow Probability Library: Specially MAF](https://www.tensorflow.org/probability/api_docs/python/tfp/bijectors/MaskedAutoregressiveFlow).'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '[5] [TensorFlow 概率库：特别是 MAF](https://www.tensorflow.org/probability/api_docs/python/tfp/bijectors/MaskedAutoregressiveFlow)。'
- en: '[6] Notebook for Codes Used Here: [My GitHub](https://github.com/suvoooo/Learn-TensorFlow/blob/master/TF-Proba/Norm-Flows/NormFlows_AutoReg.ipynb)'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '[6] 这里使用的代码笔记本：[我的 GitHub](https://github.com/suvoooo/Learn-TensorFlow/blob/master/TF-Proba/Norm-Flows/NormFlows_AutoReg.ipynb)'
