- en: 'Solving Inverse Problems With Physics-Informed DeepONet: A Practical Guide
    With Code Implementation'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: è§£å†³é€†é—®é¢˜çš„ç‰©ç†ä¿¡æ¯æ·±åº¦æ“ä½œç½‘ç»œï¼šå¸¦æœ‰ä»£ç å®ç°çš„å®ç”¨æŒ‡å—
- en: åŸæ–‡ï¼š[https://towardsdatascience.com/solving-inverse-problems-with-physics-informed-deeponet-a-practical-guide-with-code-implementation-27795eb4f502](https://towardsdatascience.com/solving-inverse-problems-with-physics-informed-deeponet-a-practical-guide-with-code-implementation-27795eb4f502)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡ï¼š[https://towardsdatascience.com/solving-inverse-problems-with-physics-informed-deeponet-a-practical-guide-with-code-implementation-27795eb4f502](https://towardsdatascience.com/solving-inverse-problems-with-physics-informed-deeponet-a-practical-guide-with-code-implementation-27795eb4f502)
- en: Two case studies with parameter estimation and input function calibration
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä¸¤ä¸ªå¸¦æœ‰å‚æ•°ä¼°è®¡å’Œè¾“å…¥å‡½æ•°æ ¡å‡†çš„æ¡ˆä¾‹ç ”ç©¶
- en: '[](https://shuaiguo.medium.com/?source=post_page-----27795eb4f502--------------------------------)[![Shuai
    Guo](../Images/d673c066f8006079be5bf92757e73a59.png)](https://shuaiguo.medium.com/?source=post_page-----27795eb4f502--------------------------------)[](https://towardsdatascience.com/?source=post_page-----27795eb4f502--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----27795eb4f502--------------------------------)
    [Shuai Guo](https://shuaiguo.medium.com/?source=post_page-----27795eb4f502--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://shuaiguo.medium.com/?source=post_page-----27795eb4f502--------------------------------)[![Shuai
    Guo](../Images/d673c066f8006079be5bf92757e73a59.png)](https://shuaiguo.medium.com/?source=post_page-----27795eb4f502--------------------------------)[](https://towardsdatascience.com/?source=post_page-----27795eb4f502--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----27795eb4f502--------------------------------)
    [Shuai Guo](https://shuaiguo.medium.com/?source=post_page-----27795eb4f502--------------------------------)'
- en: Â·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----27795eb4f502--------------------------------)
    Â·21 min readÂ·Jul 17, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: Â·å‘å¸ƒäº[Towards Data Science](https://towardsdatascience.com/?source=post_page-----27795eb4f502--------------------------------)
    Â·21åˆ†é’Ÿé˜…è¯»Â·2023å¹´7æœˆ17æ—¥
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/375cf738f91896940d75caed2c8b983d.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/375cf738f91896940d75caed2c8b983d.png)'
- en: Photo by [æ„šæœ¨æ··æ ª cdd20](https://unsplash.com/@cdd20?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: ç”±[æ„šæœ¨æ··æ ª cdd20](https://unsplash.com/@cdd20?utm_source=medium&utm_medium=referral)æ‹æ‘„ï¼Œæ¥è‡ª[Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: In my [previous blog](https://medium.com/towards-data-science/operator-learning-via-physics-informed-deeponet-lets-implement-it-from-scratch-6659f3179887),
    we delved into the concept of physics-informed DeepONet (PI-DeepONet) and explored
    why it is particularly suitable for operator learning, i.e., learning mappings
    from an input function to an output function. We also turned theory into code
    and implemented a PI-DeepONet that can accurately solve an ordinary differential
    equation (ODE) even with unseen input forcing profiles.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æˆ‘çš„[ä¸Šä¸€ç¯‡åšå®¢](https://medium.com/towards-data-science/operator-learning-via-physics-informed-deeponet-lets-implement-it-from-scratch-6659f3179887)ä¸­ï¼Œæˆ‘ä»¬æ·±å…¥æ¢è®¨äº†ç‰©ç†ä¿¡æ¯æ·±åº¦æ“ä½œç½‘ç»œï¼ˆPI-DeepONetï¼‰çš„æ¦‚å¿µï¼Œå¹¶æ¢è®¨äº†å®ƒä¸ºä½•ç‰¹åˆ«é€‚åˆäºæ“ä½œç¬¦å­¦ä¹ ï¼Œå³ä»è¾“å…¥å‡½æ•°å­¦ä¹ åˆ°è¾“å‡ºå‡½æ•°ã€‚æˆ‘ä»¬è¿˜å°†ç†è®ºè½¬åŒ–ä¸ºä»£ç ï¼Œå®ç°äº†ä¸€ä¸ªPI-DeepONetï¼Œè¯¥ç½‘ç»œå³ä½¿åœ¨é‡åˆ°æœªè§è¿‡çš„è¾“å…¥å¼ºè¿«é…ç½®æ–‡ä»¶æ—¶ï¼Œä¹Ÿèƒ½å‡†ç¡®åœ°è§£å†³å¸¸å¾®åˆ†æ–¹ç¨‹ï¼ˆODEï¼‰ã€‚
- en: '![](../Images/7ac9405ffbb94748bfce6bc08ba4c3b5.png)'
  id: totrans-9
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7ac9405ffbb94748bfce6bc08ba4c3b5.png)'
- en: Figure 1\. Operators transform one function into another, which is a concept
    frequently encountered in real-world dynamical systems. **Operator learning**
    essentially involves training a neural network model to approximate this underlying
    operator. A promising method to achieve that is **DeepONet**. (Image by author)
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾1\. æ“ä½œç¬¦å°†ä¸€ä¸ªå‡½æ•°è½¬æ¢ä¸ºå¦ä¸€ä¸ªå‡½æ•°ï¼Œè¿™æ˜¯åœ¨å®é™…åŠ¨æ€ç³»ç»Ÿä¸­ç»å¸¸é‡åˆ°çš„æ¦‚å¿µã€‚**æ“ä½œç¬¦å­¦ä¹ **æœ¬è´¨ä¸Šæ¶‰åŠè®­ç»ƒä¸€ä¸ªç¥ç»ç½‘ç»œæ¨¡å‹ä»¥é€¼è¿‘è¿™ä¸ªåŸºç¡€æ“ä½œç¬¦ã€‚ä¸€ç§æœ‰å‰é€”çš„æ–¹æ³•æ˜¯**DeepONet**ã€‚
    ï¼ˆå›¾ç‰‡ç”±ä½œè€…æä¾›ï¼‰
- en: The ability to solve these ***forward*** problems with PI-DeepONet is certainly
    valuable. But is that all PI-DeepONet can do? Well, definitely not!
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨PI-DeepONetè§£å†³è¿™äº›***å‰å‘***é—®é¢˜çš„èƒ½åŠ›æ— ç–‘æ˜¯æœ‰ä»·å€¼çš„ã€‚ä½†è¿™å°±æ˜¯PI-DeepONetèƒ½åšçš„å…¨éƒ¨å—ï¼Ÿæ˜¾ç„¶ä¸æ˜¯ï¼
- en: 'Another important problem category we frequently encountered in computational
    science and engineering is the so-called ***inverse problem***. In essence, this
    type of problem **reverses the flow of information from output to input**: the
    input is unknown and the output is observable, and the task is to estimate the
    unknown input from the observed output.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: å¦ä¸€ä¸ªæˆ‘ä»¬åœ¨è®¡ç®—ç§‘å­¦å’Œå·¥ç¨‹ä¸­ç»å¸¸é‡åˆ°çš„é‡è¦é—®é¢˜ç±»åˆ«æ˜¯æ‰€è°“çš„***é€†é—®é¢˜***ã€‚æœ¬è´¨ä¸Šï¼Œè¿™ç±»é—®é¢˜**å°†ä¿¡æ¯æµåŠ¨ä»è¾“å‡ºåå‘åˆ°è¾“å…¥**ï¼šè¾“å…¥æ˜¯æœªçŸ¥çš„ï¼Œè€Œè¾“å‡ºæ˜¯å¯è§‚å¯Ÿçš„ï¼Œä»»åŠ¡æ˜¯ä»è§‚å¯Ÿåˆ°çš„è¾“å‡ºä¸­ä¼°è®¡æœªçŸ¥çš„è¾“å…¥ã€‚
- en: '![](../Images/cab35d9a2efb313c05f626f8ae922355.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/cab35d9a2efb313c05f626f8ae922355.png)'
- en: 'Figure 2\. In forward problems, the objective is to predict the outputs given
    the known inputs via the operator. In inverse problems, the process is reversed:
    known outputs are used to estimate the original, unknown inputs, often with only
    partial knowledge of the underlying operator. Both forward and inverse problems
    are commonly encountered in computational science and engineering. (Image by author)'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 2\. åœ¨å‰å‘é—®é¢˜ä¸­ï¼Œç›®æ ‡æ˜¯é€šè¿‡ç®—å­é¢„æµ‹å·²çŸ¥è¾“å…¥ä¸‹çš„è¾“å‡ºã€‚åœ¨åå‘é—®é¢˜ä¸­ï¼Œè¿‡ç¨‹æ˜¯ç›¸åçš„ï¼šä½¿ç”¨å·²çŸ¥çš„è¾“å‡ºä¼°ç®—åŸå§‹çš„ã€æœªçŸ¥çš„è¾“å…¥ï¼Œé€šå¸¸å¯¹åº•å±‚ç®—å­åªæœ‰éƒ¨åˆ†äº†è§£ã€‚å‰å‘é—®é¢˜å’Œåå‘é—®é¢˜åœ¨è®¡ç®—ç§‘å­¦å’Œå·¥ç¨‹ä¸­éƒ½å¾ˆå¸¸è§ã€‚ï¼ˆå›¾åƒä½œè€…æä¾›ï¼‰
- en: 'As you might have guessed, PI-DeepONet can also be a super useful tool for
    tackling these types of problems. In this blog, we will take a close look at how
    that can be achieved. More concretely, we will address two case studies: one with
    parameter estimation, and the other one with input functionÂ calibrations.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: æ­£å¦‚ä½ å¯èƒ½å·²ç»çŒœåˆ°çš„ï¼ŒPI-DeepONet ä¹Ÿå¯ä»¥æˆä¸ºè§£å†³è¿™äº›ç±»å‹é—®é¢˜çš„è¶…çº§æœ‰ç”¨å·¥å…·ã€‚åœ¨æœ¬åšå®¢ä¸­ï¼Œæˆ‘ä»¬å°†è¯¦ç»†æ¢è®¨å¦‚ä½•å®ç°è¿™ä¸€ç‚¹ã€‚æ›´å…·ä½“åœ°ï¼Œæˆ‘ä»¬å°†è§£å†³ä¸¤ä¸ªæ¡ˆä¾‹ç ”ç©¶ï¼šä¸€ä¸ªæ˜¯å‚æ•°ä¼°ç®—ï¼Œå¦ä¸€ä¸ªæ˜¯è¾“å…¥å‡½æ•°æ ¡å‡†ã€‚
- en: This blog intends to be self-contained, with only a brief discussion on the
    basics of physics-informed (PI-) learning, DeepONet, as well as our main focus,
    PI-DeepONet. For a more comprehensive intro to those topics, feel free to check
    out [my previous blog](https://medium.com/towards-data-science/operator-learning-via-physics-informed-deeponet-lets-implement-it-from-scratch-6659f3179887).
  id: totrans-16
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: æœ¬åšå®¢æ—¨åœ¨è‡ªæˆä¸€ä½“ï¼Œä»…å¯¹ç‰©ç†ä¿¡æ¯ï¼ˆPI-ï¼‰å­¦ä¹ ã€DeepONet ä»¥åŠæˆ‘ä»¬çš„ä¸»è¦å…³æ³¨ç‚¹ PI-DeepONet è¿›è¡Œç®€è¦è®¨è®ºã€‚æœ‰å…³è¿™äº›ä¸»é¢˜çš„æ›´å…¨é¢ä»‹ç»ï¼Œè¯·éšæ—¶æŸ¥çœ‹[æˆ‘ä¹‹å‰çš„åšå®¢](https://medium.com/towards-data-science/operator-learning-via-physics-informed-deeponet-lets-implement-it-from-scratch-6659f3179887)ã€‚
- en: With that in mind, letâ€™s get started!
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: é‰´äºæ­¤ï¼Œè®©æˆ‘ä»¬å¼€å§‹å§ï¼
- en: Table of Content
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ç›®å½•
- en: 'Â· [1\. PI-DeepONet: A refresher](#56d0)'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 'Â· [1\. PI-DeepONet: ç®€ä»‹](#56d0)'
- en: Â· [2\. Problem Statements](#4238)
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: Â· [2\. é—®é¢˜é™ˆè¿°](#4238)
- en: 'Â· [3\. Problem 1: Parameter Estimation](#50f4)'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: Â· [3\. é—®é¢˜ 1ï¼šå‚æ•°ä¼°ç®—](#50f4)
- en: âˆ˜ [3.1 How it works](#c17f)
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: âˆ˜ [3.1 å¦‚ä½•è¿ä½œ](#c17f)
- en: âˆ˜ [3.2 Implementing a PI-DeepONet pipeline](#33fd)
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: âˆ˜ [3.2 å®ç° PI-DeepONet ç®¡é“](#33fd)
- en: âˆ˜ [3.3 Results discussion](#5bac)
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: âˆ˜ [3.3 ç»“æœè®¨è®º](#5bac)
- en: 'Â· [4\. Problem 2: Input Function Estimation](#cc01)'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: Â· [4\. é—®é¢˜ 2ï¼šè¾“å…¥å‡½æ•°ä¼°ç®—](#cc01)
- en: âˆ˜ [4.1 Solution stratgies](#e448)
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: âˆ˜ [4.1 è§£å†³ç­–ç•¥](#e448)
- en: 'âˆ˜ [4.2 Optimization routine: TensorFlow](#da8c)'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: âˆ˜ [4.2 ä¼˜åŒ–å¸¸è§„ï¼šTensorFlow](#da8c)
- en: 'âˆ˜ [4.3 Optimization routine: L-BFGS](#6bf1)'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: âˆ˜ [4.3 ä¼˜åŒ–å¸¸è§„ï¼šL-BFGS](#6bf1)
- en: Â· [5\. Take-away](#a486)
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: Â· [5\. æ€»ç»“](#a486)
- en: Â· [Reference](#b410)
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: Â· [å‚è€ƒæ–‡çŒ®](#b410)
- en: '1\. PI-DeepONet: A refresher'
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '1\. PI-DeepONet: ç®€ä»‹'
- en: 'As its name implies, PI-DeepONet is the combination of two concepts: *physics-informed
    learning*, and *DeepONet*.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: æ­£å¦‚å…¶åå­—æ‰€ç¤ºï¼ŒPI-DeepONet æ˜¯ä¸¤ä¸ªæ¦‚å¿µçš„ç»“åˆï¼š*ç‰©ç†ä¿¡æ¯å­¦ä¹ * å’Œ *DeepONet*ã€‚
- en: Physics-informed learning is a new paradigm of machine learning and gains particular
    traction in the domain of dynamical system modeling. Its key idea is to explicitly
    bake the governing differential equations directly into the machine learning model,
    often through the introduction of an additional loss term in the loss function
    that accounts for the residuals of the governing equations. The premise of this
    learning approach is that the model built this way will respect known physical
    laws and offer better generalizability, interpretability, and trustworthiness.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: ç‰©ç†ä¿¡æ¯å­¦ä¹ æ˜¯æœºå™¨å­¦ä¹ çš„ä¸€ç§æ–°èŒƒå¼ï¼Œå¹¶åœ¨åŠ¨æ€ç³»ç»Ÿå»ºæ¨¡é¢†åŸŸè·å¾—äº†ç‰¹åˆ«çš„å…³æ³¨ã€‚å…¶æ ¸å¿ƒæ€æƒ³æ˜¯å°†æ§åˆ¶å¾®åˆ†æ–¹ç¨‹ç›´æ¥èå…¥æœºå™¨å­¦ä¹ æ¨¡å‹ä¸­ï¼Œé€šå¸¸é€šè¿‡åœ¨æŸå¤±å‡½æ•°ä¸­å¼•å…¥é¢å¤–çš„æŸå¤±é¡¹æ¥è€ƒè™‘æ§åˆ¶æ–¹ç¨‹çš„æ®‹å·®ã€‚è¿™ç§å­¦ä¹ æ–¹æ³•çš„å‰ææ˜¯ï¼Œä»¥è¿™ç§æ–¹å¼æ„å»ºçš„æ¨¡å‹å°†å°Šé‡å·²çŸ¥çš„ç‰©ç†å®šå¾‹ï¼Œå¹¶æä¾›æ›´å¥½çš„æ³›åŒ–èƒ½åŠ›ã€å¯è§£é‡Šæ€§å’Œå¯ä¿¡åº¦ã€‚
- en: DeepONet, on the other hand, resides in the traditional pure data-driven modeling
    domain. However, whatâ€™s unique about it is that DeepONet is specifically designed
    for **operator learning**, i.e., learning the mapping from an input function to
    an output function. This situation is frequently encountered in many dynamical
    systems. For instance, in a simple mass-spring system, the time-varying driving
    force serves as an input function (of time), while the resultant mass displacement
    is the output function (of time as well).
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: DeepONet ä¸æ­¤åŒæ—¶å±äºä¼ ç»Ÿçš„çº¯æ•°æ®é©±åŠ¨å»ºæ¨¡é¢†åŸŸã€‚ç„¶è€Œï¼Œå…¶ç‹¬ç‰¹ä¹‹å¤„åœ¨äº DeepONet ä¸“é—¨è®¾è®¡ç”¨äº**ç®—å­å­¦ä¹ **ï¼Œå³å­¦ä¹ ä»è¾“å…¥å‡½æ•°åˆ°è¾“å‡ºå‡½æ•°çš„æ˜ å°„ã€‚è¿™ç§æƒ…å†µåœ¨è®¸å¤šåŠ¨æ€ç³»ç»Ÿä¸­ç»å¸¸é‡åˆ°ã€‚ä¾‹å¦‚ï¼Œåœ¨ä¸€ä¸ªç®€å•çš„è´¨é‡-å¼¹ç°§ç³»ç»Ÿä¸­ï¼Œéšæ—¶é—´å˜åŒ–çš„é©±åŠ¨åŠ›ä½œä¸ºè¾“å…¥å‡½æ•°ï¼ˆæ—¶é—´çš„å‡½æ•°ï¼‰ï¼Œè€Œè´¨é‡çš„ä½ç§»åˆ™æ˜¯è¾“å‡ºå‡½æ•°ï¼ˆä¹Ÿæ˜¯æ—¶é—´çš„å‡½æ•°ï¼‰ã€‚
- en: DeepONet proposed a novel network architecture (as shown in Figure 3), where
    a **branch net** is used to transform the profile of the input function, and a
    **trunk net** is used to transform the temporal/spatial coordinates. The feature
    vectors output by these two nets are then merged via a dot product.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: DeepONetæå‡ºäº†ä¸€ç§æ–°å‹ç½‘ç»œæ¶æ„ï¼ˆå¦‚å›¾3æ‰€ç¤ºï¼‰ï¼Œå…¶ä¸­**åˆ†æ”¯ç½‘ç»œ**ç”¨äºè½¬æ¢è¾“å…¥å‡½æ•°çš„ç‰¹å¾ï¼Œ**ä¸»å¹²ç½‘ç»œ**ç”¨äºè½¬æ¢æ—¶é—´/ç©ºé—´åæ ‡ã€‚è¿™ä¸¤ä¸ªç½‘ç»œè¾“å‡ºçš„ç‰¹å¾å‘é‡é€šè¿‡ç‚¹ç§¯åˆå¹¶ã€‚
- en: '![](../Images/1fe10410dafdbe1907c7408142907b24.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1fe10410dafdbe1907c7408142907b24.png)'
- en: Figure 3\. The architecture of DeepONet. The uniqueness of this method lies
    in its separation of branch and trunk nets to handle input function profiles and
    temporal/spatial coordinates, respectively. (Image by author)
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾3\. DeepONetçš„æ¶æ„ã€‚è¿™ç§æ–¹æ³•çš„ç‹¬ç‰¹æ€§åœ¨äºå°†åˆ†æ”¯ç½‘ç»œå’Œä¸»å¹²ç½‘ç»œåˆ†å¼€ï¼Œåˆ†åˆ«å¤„ç†è¾“å…¥å‡½æ•°ç‰¹å¾å’Œæ—¶é—´/ç©ºé—´åæ ‡ã€‚ï¼ˆä½œè€…å›¾ç‰‡ï¼‰
- en: Now, if we layer the concept of physics-informed learning on top of the DeepONet,
    we obtain what is known as PI-DeepONet.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ï¼Œå¦‚æœå°†ç‰©ç†ä¿¡æ¯å­¦ä¹ çš„æ¦‚å¿µå åŠ åˆ°DeepONetä¸Šï¼Œæˆ‘ä»¬å¾—åˆ°çš„å°±æ˜¯æ‰€è°“çš„PI-DeepONetã€‚
- en: '![](../Images/d1611ed23d6993195925a6cd19dc2691.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d1611ed23d6993195925a6cd19dc2691.png)'
- en: Figure 4\. Compared to a DeepONet, a PI-DeepONet contains extra loss terms such
    as the **ODE/PDE residual loss**, as well as the initial condition loss (IC loss)
    and boundary condition loss (BC loss). The conventional data loss is optional
    for PI-DeepONet, as it can directly learn the operator of the underlying dynamical
    system solely from the associated governing equations. (Image by author)
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾4\. ä¸DeepONetç›¸æ¯”ï¼ŒPI-DeepONetåŒ…å«é¢å¤–çš„æŸå¤±é¡¹ï¼Œå¦‚**ODE/PDEæ®‹å·®æŸå¤±**ï¼Œä»¥åŠåˆå§‹æ¡ä»¶æŸå¤±ï¼ˆICæŸå¤±ï¼‰å’Œè¾¹ç•Œæ¡ä»¶æŸå¤±ï¼ˆBCæŸå¤±ï¼‰ã€‚å¯¹äºPI-DeepONetï¼Œä¼ ç»Ÿçš„æ•°æ®æŸå¤±æ˜¯å¯é€‰çš„ï¼Œå› ä¸ºå®ƒå¯ä»¥ä»…é€šè¿‡ç›¸å…³æ§åˆ¶æ–¹ç¨‹ç›´æ¥å­¦ä¹ åŸºç¡€åŠ¨æ€ç³»ç»Ÿçš„ç®—å­ã€‚ï¼ˆä½œè€…å›¾ç‰‡ï¼‰
- en: Once a PI-DeepONet is trained, it can predict the profile of the output function
    for a given new input function profile in real-time, while ensuring that the predictions
    align with the governing equations. As you can imagine, this makes PI-DeepONet
    a potentially very powerful tool for a diverse range of dynamic system modeling
    tasks.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€æ—¦PI-DeepONetç»è¿‡è®­ç»ƒï¼Œå®ƒå¯ä»¥å®æ—¶é¢„æµ‹ç»™å®šæ–°è¾“å…¥å‡½æ•°ç‰¹å¾çš„è¾“å‡ºå‡½æ•°ç‰¹å¾ï¼ŒåŒæ—¶ç¡®ä¿é¢„æµ‹ç»“æœä¸æ§åˆ¶æ–¹ç¨‹ä¸€è‡´ã€‚æ­£å¦‚ä½ æ‰€æƒ³è±¡çš„ï¼Œè¿™ä½¿å¾—PI-DeepONetæˆä¸ºä¸€ä¸ªæ½œåœ¨éå¸¸å¼ºå¤§çš„å·¥å…·ï¼Œé€‚ç”¨äºå„ç§åŠ¨æ€ç³»ç»Ÿå»ºæ¨¡ä»»åŠ¡ã€‚
- en: 'However, in many other system modeling scenarios, we may also need to perform
    the exact opposite operation, i.e., we know the outputs and want to estimate the
    unknown inputs based on observed output and our prior knowledge of system dynamics.
    Generally speaking, this type of scenario falls in the scope of **inverse modeling**.
    A question that naturally arises here is: can we also use PI-DeepONet to address
    inverse estimation problems?'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶è€Œï¼Œåœ¨è®¸å¤šå…¶ä»–ç³»ç»Ÿå»ºæ¨¡åœºæ™¯ä¸­ï¼Œæˆ‘ä»¬å¯èƒ½è¿˜éœ€è¦æ‰§è¡Œå®Œå…¨ç›¸åçš„æ“ä½œï¼Œå³æˆ‘ä»¬çŸ¥é“è¾“å‡ºå¹¶å¸Œæœ›æ ¹æ®è§‚å¯Ÿåˆ°çš„è¾“å‡ºå’Œæˆ‘ä»¬å¯¹ç³»ç»ŸåŠ¨æ€çš„å…ˆéªŒçŸ¥è¯†æ¥ä¼°è®¡æœªçŸ¥çš„è¾“å…¥ã€‚ä¸€èˆ¬æ¥è¯´ï¼Œè¿™ç§æƒ…å†µå±äº**é€†å‘å»ºæ¨¡**çš„èŒƒå›´ã€‚è¿™é‡Œè‡ªç„¶å‡ºç°äº†ä¸€ä¸ªé—®é¢˜ï¼šæˆ‘ä»¬æ˜¯å¦ä¹Ÿå¯ä»¥ä½¿ç”¨PI-DeepONetæ¥è§£å†³é€†å‘ä¼°è®¡é—®é¢˜ï¼Ÿ
- en: Before we get into that, letâ€™s first more precisely formulate the problems we
    are aiming to solve.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æˆ‘ä»¬æ·±å…¥æ¢è®¨ä¹‹å‰ï¼Œè®©æˆ‘ä»¬é¦–å…ˆæ›´å‡†ç¡®åœ°åˆ¶å®šæˆ‘ä»¬æ—¨åœ¨è§£å†³çš„é—®é¢˜ã€‚
- en: 2\. Problem Statements
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 2\. é—®é¢˜é™ˆè¿°
- en: 'We will use the same ODE discussed in the previous blog as our base model.
    Previously, we investigated an initial value problem described by the following
    equation:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†ä½¿ç”¨å‰é¢åšå®¢ä¸­è®¨è®ºçš„ç›¸åŒODEä½œä¸ºåŸºç¡€æ¨¡å‹ã€‚ä¹‹å‰ï¼Œæˆ‘ä»¬ç ”ç©¶äº†ç”±ä»¥ä¸‹æ–¹ç¨‹æè¿°çš„åˆå§‹å€¼é—®é¢˜ï¼š
- en: '![](../Images/a2965a8118cbca756dc4b2c9fbbb113a.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a2965a8118cbca756dc4b2c9fbbb113a.png)'
- en: 'with an initial condition s(0) = 0\. In the equation, u(*t*) is the input function
    that varies over time, and s(*t*) denotes the state of the system at time *t*.
    Our previous focus is on solving the **forward** problem, i.e., predicting s(Â·)
    given u(Â·). Now, we will shift our focus and consider solving two types of inverse
    problems:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: ä»¥åˆå§‹æ¡ä»¶s(0) = 0ä¸ºä¾‹ã€‚åœ¨æ–¹ç¨‹ä¸­ï¼Œu(*t*)æ˜¯éšæ—¶é—´å˜åŒ–çš„è¾“å…¥å‡½æ•°ï¼Œè€Œs(*t*)è¡¨ç¤ºç³»ç»Ÿåœ¨æ—¶é—´*t*çš„çŠ¶æ€ã€‚æˆ‘ä»¬ä¹‹å‰å…³æ³¨çš„æ˜¯è§£å†³**å‰å‘**é—®é¢˜ï¼Œå³ç»™å®šu(Â·)é¢„æµ‹s(Â·)ã€‚ç°åœ¨ï¼Œæˆ‘ä»¬å°†è½¬å˜å…³æ³¨ç‚¹ï¼Œè€ƒè™‘è§£å†³ä¸¤ç§ç±»å‹çš„é€†å‘é—®é¢˜ï¼š
- en: 1ï¸âƒ£ Estimating unknown input parameters
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 1ï¸âƒ£ ä¼°è®¡æœªçŸ¥çš„è¾“å…¥å‚æ•°
- en: 'Letâ€™s start with a straightforward inverse problem. Imagine our governing ODE
    has now evolved to be like this:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬ä»ä¸€ä¸ªç®€å•çš„é€†å‘é—®é¢˜å¼€å§‹ã€‚è®¾æƒ³æˆ‘ä»¬çš„æ§åˆ¶ODEç°åœ¨æ¼”å˜æˆè¿™æ ·ï¼š
- en: '![](../Images/298a2c22c0b53231f8bc8863fb14b9a5.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/298a2c22c0b53231f8bc8863fb14b9a5.png)'
- en: initial condition s(0) = 0, a and b are **unknowns**.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: åˆå§‹æ¡ä»¶s(0) = 0ï¼Œaå’Œbæ˜¯**æœªçŸ¥æ•°**ã€‚
- en: with *a* and *b* being the two unknown parameters. Our objective here is to
    estimate the values of *a* and *b*, given the observed u(Â·) and s(Â·) profiles.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­ *a* å’Œ *b* æ˜¯ä¸¤ä¸ªæœªçŸ¥å‚æ•°ã€‚æˆ‘ä»¬çš„ç›®æ ‡æ˜¯ä¼°è®¡ *a* å’Œ *b* çš„å€¼ï¼Œç»™å®šè§‚å¯Ÿåˆ°çš„ u(Â·) å’Œ s(Â·) è½®å»“ã€‚
- en: This type of problem falls in the scope of **parameter estimation,** where unknown
    parameters of the system need to be identified from the measured data. Typical
    examples of this type of problem include system identification for control engineering,
    material thermal coefficients estimation in computational heat transfer, etc.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ç±»é—®é¢˜å±äº **å‚æ•°ä¼°è®¡** çš„èŒƒå›´ï¼Œå…¶ä¸­éœ€è¦ä»æµ‹é‡æ•°æ®ä¸­è¯†åˆ«ç³»ç»Ÿçš„æœªçŸ¥å‚æ•°ã€‚è¿™ç±»é—®é¢˜çš„å…¸å‹ç¤ºä¾‹åŒ…æ‹¬æ§åˆ¶å·¥ç¨‹ä¸­çš„ç³»ç»Ÿè¯†åˆ«ã€è®¡ç®—çƒ­ä¼ å¯¼ä¸­çš„ææ–™çƒ­ç³»æ•°ä¼°è®¡ç­‰ã€‚
- en: In our current case study, we will assume the true values for *a* and *b* are
    both 0.5.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æˆ‘ä»¬å½“å‰çš„æ¡ˆä¾‹ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬å°†å‡è®¾ *a* å’Œ *b* çš„çœŸå®å€¼å‡ä¸º 0.5ã€‚
- en: 2ï¸âƒ£ Estimating the entire input function profile
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 2ï¸âƒ£ ä¼°è®¡æ•´ä¸ªè¾“å…¥å‡½æ•°è½®å»“
- en: 'For the second case study, we ramp up the problem complexity: Suppose that
    we know perfectly about the ODE (i.e., we know the exact values of *a* and *b*).
    However, while we have observed the s(Â·) profile, we donâ€™t yet know the u(Â·) profile
    that has generated this observed output function. Consequently, our objective
    here is to estimate the u(Â·) profile, given the observed s(Â·) profile and known
    ODE:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºç¬¬äºŒä¸ªæ¡ˆä¾‹ç ”ç©¶ï¼Œæˆ‘ä»¬æé«˜äº†é—®é¢˜çš„å¤æ‚æ€§ï¼šå‡è®¾æˆ‘ä»¬å¯¹å¸¸å¾®åˆ†æ–¹ç¨‹ (ODE) å®Œå…¨äº†è§£ï¼ˆå³ï¼Œæˆ‘ä»¬çŸ¥é“ *a* å’Œ *b* çš„ç¡®åˆ‡å€¼ï¼‰ã€‚ç„¶è€Œï¼Œå°½ç®¡æˆ‘ä»¬è§‚å¯Ÿäº†
    s(Â·) è½®å»“ï¼Œä½†æˆ‘ä»¬å°šä¸çŸ¥é“ç”Ÿæˆè¯¥è§‚å¯Ÿè¾“å‡ºå‡½æ•°çš„ u(Â·) è½®å»“ã€‚å› æ­¤ï¼Œæˆ‘ä»¬çš„ç›®æ ‡æ˜¯ä¼°è®¡ u(Â·) è½®å»“ï¼Œç»™å®šè§‚å¯Ÿåˆ°çš„ s(Â·) è½®å»“å’Œå·²çŸ¥ ODEï¼š
- en: '![](../Images/3e845f5f86ad6e2e52b414c01d6942d7.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3e845f5f86ad6e2e52b414c01d6942d7.png)'
- en: initial condition s(0) = 0, a=0.5, b=0.5.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: åˆå§‹æ¡ä»¶ s(0) = 0ï¼Œa=0.5ï¼Œb=0.5ã€‚
- en: Since we now aim to recover an entire input function instead of a small set
    of unknown parameters, this case study will be much more challenging than the
    first one. Unfortunately, this type of problem is inherently ill-posed and requires
    strong regularization to help constrain the solution space. Nevertheless, they
    often arise in various fields, including environmental engineering (e.g., identifying
    the profile of pollutant sources), aerospace engineering (e.g., calibrating the
    applied loads on aircraft), and wind engineering (e.g., wind force estimation).
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: ç”±äºæˆ‘ä»¬ç°åœ¨çš„ç›®æ ‡æ˜¯æ¢å¤æ•´ä¸ªè¾“å…¥å‡½æ•°è€Œä¸æ˜¯ä¸€å°éƒ¨åˆ†æœªçŸ¥å‚æ•°ï¼Œå› æ­¤è¿™ä¸ªæ¡ˆä¾‹ç ”ç©¶å°†æ¯”ç¬¬ä¸€ä¸ªæ¡ˆä¾‹æ›´å…·æŒ‘æˆ˜æ€§ã€‚ä¸å¹¸çš„æ˜¯ï¼Œè¿™ç±»é—®é¢˜æœ¬è´¨ä¸Šæ˜¯ä¸é€‚å®šçš„ï¼Œéœ€è¦å¼ºæœ‰åŠ›çš„æ­£åˆ™åŒ–æ¥å¸®åŠ©çº¦æŸè§£ç©ºé—´ã€‚å°½ç®¡å¦‚æ­¤ï¼Œè¿™ç±»é—®é¢˜åœ¨å¤šä¸ªé¢†åŸŸä¸­ç»å¸¸å‡ºç°ï¼ŒåŒ…æ‹¬ç¯å¢ƒå·¥ç¨‹ï¼ˆä¾‹å¦‚ï¼Œè¯†åˆ«æ±¡æŸ“æºçš„è½®å»“ï¼‰ã€èˆªç©ºèˆªå¤©å·¥ç¨‹ï¼ˆä¾‹å¦‚ï¼Œæ ¡å‡†é£æœºä¸Šçš„æ–½åŠ è½½è·ï¼‰å’Œé£å·¥ç¨‹ï¼ˆä¾‹å¦‚ï¼Œé£åŠ›ä¼°è®¡ï¼‰ã€‚
- en: In the following two sections, we will address these two case studies one by
    one.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ¥ä¸‹æ¥çš„ä¸¤ä¸ªéƒ¨åˆ†ä¸­ï¼Œæˆ‘ä»¬å°†é€ä¸€è®¨è®ºè¿™ä¸¤ä¸ªæ¡ˆä¾‹ç ”ç©¶ã€‚
- en: '3\. Problem 1: Parameter Estimation'
  id: totrans-61
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 3\. é—®é¢˜ 1ï¼šå‚æ•°ä¼°è®¡
- en: 'In this section, we tackle the first case study: estimating the unknown parameters
    in our target ODE. We will start with a brief discussion on how the general physics-informed
    neural networks can be used to solve this type of problem, followed by implementing
    a PI-DeepONet-based pipeline for parameter estimation. Afterward, we will apply
    it to our case study and discuss the obtained results.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬å¤„ç†ç¬¬ä¸€ä¸ªæ¡ˆä¾‹ç ”ç©¶ï¼šä¼°è®¡æˆ‘ä»¬ç›®æ ‡ ODE ä¸­çš„æœªçŸ¥å‚æ•°ã€‚æˆ‘ä»¬å°†é¦–å…ˆç®€è¦è®¨è®ºå¦‚ä½•ä½¿ç”¨é€šç”¨çš„ç‰©ç†ä¿¡æ¯ç¥ç»ç½‘ç»œæ¥è§£å†³è¿™ç±»é—®é¢˜ï¼Œç„¶åå®æ–½åŸºäº PI-DeepONet
    çš„å‚æ•°ä¼°è®¡ç®¡é“ã€‚ä¹‹åï¼Œæˆ‘ä»¬å°†å…¶åº”ç”¨äºæˆ‘ä»¬çš„æ¡ˆä¾‹ç ”ç©¶ï¼Œå¹¶è®¨è®ºè·å¾—çš„ç»“æœã€‚
- en: 3.1 How it works
  id: totrans-63
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.1 å·¥ä½œåŸç†
- en: 'In the [original paper](https://www.sciencedirect.com/science/article/abs/pii/S0021999118307125)
    on physics-informed neural networks (PINNs), Raissi and co-authors outlined the
    strategy of using PINNs to solve inverse problem calibration problems: in essence,
    we can simply set the unknown parameters (in our current case, parameters *a*
    and *b*) as **trainable parameters** in the neural network, and optimize those
    unknown parameters together with the weights and bias of the neural net to minimize
    the loss function.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ [åŸå§‹è®ºæ–‡](https://www.sciencedirect.com/science/article/abs/pii/S0021999118307125)
    ä¸­ï¼ŒRaissi å’Œåˆè‘—è€…æ¦‚è¿°äº†ä½¿ç”¨ PINNs è§£å†³é€†é—®é¢˜æ ¡å‡†é—®é¢˜çš„ç­–ç•¥ï¼šæœ¬è´¨ä¸Šï¼Œæˆ‘ä»¬å¯ä»¥ç®€å•åœ°å°†æœªçŸ¥å‚æ•°ï¼ˆåœ¨æˆ‘ä»¬å½“å‰çš„æ¡ˆä¾‹ä¸­ï¼Œå³å‚æ•° *a* å’Œ *b*ï¼‰è®¾ä¸º
    **å¯è®­ç»ƒå‚æ•°**ï¼Œå¹¶ä¸ç¥ç»ç½‘ç»œçš„æƒé‡å’Œåç½®ä¸€èµ·ä¼˜åŒ–è¿™äº›æœªçŸ¥å‚æ•°ï¼Œä»¥æœ€å°åŒ–æŸå¤±å‡½æ•°ã€‚
- en: 'Of course, the secret sauce lies in constructing the loss function: as a **physics-informed
    learning approach**, it not only contains a data mismatch term, which measures
    the discrepancy between the predicted output of the network and the observed data,
    but also a physics-informed regularization term, which calculates the residuals
    (i.e., the difference between the left and right-hand side of the differential
    equation) using the outputs of the neural network (and their derivatives) and
    the current estimates of the parameters *a* and *b*.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: å½“ç„¶ï¼Œå…³é”®åœ¨äºæ„é€ æŸå¤±å‡½æ•°ï¼šä½œä¸ºä¸€ç§**ç‰©ç†ä¿¡æ¯å­¦ä¹ æ–¹æ³•**ï¼Œå®ƒä¸ä»…åŒ…å«ä¸€ä¸ªæ•°æ®ä¸åŒ¹é…é¡¹ï¼Œè¯¥é¡¹è¡¡é‡ç½‘ç»œé¢„æµ‹è¾“å‡ºä¸è§‚å¯Ÿæ•°æ®ä¹‹é—´çš„å·®å¼‚ï¼Œè¿˜åŒ…å«ä¸€ä¸ªç‰©ç†ä¿¡æ¯æ­£åˆ™åŒ–é¡¹ï¼Œè¯¥é¡¹ä½¿ç”¨ç¥ç»ç½‘ç»œçš„è¾“å‡ºï¼ˆåŠå…¶å¯¼æ•°ï¼‰å’Œå½“å‰çš„
    *a* å’Œ *b* å‚æ•°ä¼°è®¡å€¼æ¥è®¡ç®—æ®‹å·®ï¼ˆå³å¾®åˆ†æ–¹ç¨‹å·¦å³ä¸¤ä¾§çš„å·®å¼‚ï¼‰ã€‚
- en: Now, when we perform this joint optimization, weâ€™re effectively searching for
    *a* and *b* values that would lead to a networkâ€™s outputs that simultaneously
    fit the observed data and satisfy the governing differential equation. When the
    loss function reaches its minimum value (i.e., the training is converged), the
    final values of *a* and *b* we obtain are the ones that have achieved this balance,
    and they are thus constituting the estimates of the unknown parameters.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ï¼Œå½“æˆ‘ä»¬æ‰§è¡Œè¿™ç§è”åˆä¼˜åŒ–æ—¶ï¼Œæˆ‘ä»¬å®é™…ä¸Šæ˜¯åœ¨å¯»æ‰¾ *a* å’Œ *b* å€¼ï¼Œè¿™äº›å€¼ä¼šå¯¼è‡´ç½‘ç»œçš„è¾“å‡ºåŒæ—¶é€‚åº”è§‚å¯Ÿåˆ°çš„æ•°æ®å¹¶æ»¡è¶³æ§åˆ¶å¾®åˆ†æ–¹ç¨‹ã€‚å½“æŸå¤±å‡½æ•°è¾¾åˆ°æœ€å°å€¼æ—¶ï¼ˆå³è®­ç»ƒæ”¶æ•›ï¼‰ï¼Œæˆ‘ä»¬è·å¾—çš„
    *a* å’Œ *b* çš„æœ€ç»ˆå€¼å°±æ˜¯å®ç°è¿™ç§å¹³è¡¡çš„å€¼ï¼Œå› æ­¤å®ƒä»¬å°±æ˜¯æœªçŸ¥å‚æ•°çš„ä¼°è®¡å€¼ã€‚
- en: '![](../Images/29eabf0d14cb453bf39079298d746c1f.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/29eabf0d14cb453bf39079298d746c1f.png)'
- en: Figure 5\. When using PI-DeepONet, the unknown parameters a and b are jointly
    optimized with the weights and biases of the DeepONet model. When the training
    is converged, the a and b values we end up with constitute their estimates. (Image
    by author)
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 5\. åœ¨ä½¿ç”¨ PI-DeepONet æ—¶ï¼ŒæœªçŸ¥å‚æ•° *a* å’Œ *b* ä¸ DeepONet æ¨¡å‹çš„æƒé‡å’Œåç½®ä¸€èµ·ä¼˜åŒ–ã€‚å½“è®­ç»ƒæ”¶æ•›æ—¶ï¼Œæœ€ç»ˆå¾—åˆ°çš„
    *a* å’Œ *b* å€¼å³ä¸ºå®ƒä»¬çš„ä¼°è®¡å€¼ã€‚ï¼ˆå›¾åƒæ¥è‡ªä½œè€…ï¼‰
- en: 3.2 Implementing a PI-DeepONet pipeline
  id: totrans-69
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.2 å®ç° PI-DeepONet æµæ°´çº¿
- en: Enough about the theory, itâ€™s time to see some code ğŸ’»
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: è¯´å¤Ÿç†è®ºï¼Œæ¥ä¸‹æ¥æ˜¯ä»£ç éƒ¨åˆ† ğŸ’»
- en: 'Letâ€™s start with the definition of PI-DeepONet:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ä» PI-DeepONet çš„å®šä¹‰å¼€å§‹ï¼š
- en: '[PRE0]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'In the code above:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä¸Šè¿°ä»£ç ä¸­ï¼š
- en: We start by defining the trunk and branch networks (which are all simple fully
    connected nets). The feature vectors produced by both nets are merged via a dot
    product.
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬é¦–å…ˆå®šä¹‰äº†ä¸»å¹²ç½‘ç»œå’Œåˆ†æ”¯ç½‘ç»œï¼ˆå®ƒä»¬éƒ½æ˜¯ç®€å•çš„å…¨è¿æ¥ç½‘ç»œï¼‰ã€‚ä¸¤ä¸ªç½‘ç»œç”Ÿæˆçš„ç‰¹å¾å‘é‡é€šè¿‡ç‚¹ç§¯åˆå¹¶åœ¨ä¸€èµ·ã€‚
- en: We proceed by adding a bias term on top of the dot product result, which is
    achieved by defining a custom `BiasLayer()`. This strategy could improve the prediction
    accuracy, as indicated in the [original DeepONet paper](https://arxiv.org/abs/1910.03193).
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬é€šè¿‡åœ¨ç‚¹ç§¯ç»“æœä¸Šæ·»åŠ ä¸€ä¸ªåç½®é¡¹æ¥è¿›è¡Œä¸‹ä¸€æ­¥ï¼Œè¿™é€šè¿‡å®šä¹‰ä¸€ä¸ªè‡ªå®šä¹‰çš„ `BiasLayer()` å®ç°ã€‚è¯¥ç­–ç•¥å¯ä»¥æé«˜é¢„æµ‹å‡†ç¡®æ€§ï¼Œæ­£å¦‚ [åŸå§‹ DeepONet
    è®ºæ–‡](https://arxiv.org/abs/1910.03193) ä¸­æ‰€æŒ‡å‡ºçš„ã€‚
- en: 'Here comes the main change that makes our model capable of solving parameter
    estimation problems: we add *a* and *b* to the collection of the neural network
    model parameters. This way, when we set the `trainable` to be true, *a* and *b*
    will be optimized together with the other usual weights and biases of the neural
    network. Technically, we achieve this goal by defining a custom layer:'
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: è¿™é‡Œæ˜¯ä½¿æˆ‘ä»¬çš„æ¨¡å‹èƒ½å¤Ÿè§£å†³å‚æ•°ä¼°è®¡é—®é¢˜çš„ä¸»è¦å˜åŒ–ï¼šæˆ‘ä»¬å°† *a* å’Œ *b* æ·»åŠ åˆ°ç¥ç»ç½‘ç»œæ¨¡å‹å‚æ•°é›†åˆä¸­ã€‚è¿™æ ·ï¼Œå½“æˆ‘ä»¬å°† `trainable` è®¾ç½®ä¸º
    true æ—¶ï¼Œ*a* å’Œ *b* å°†ä¸ç¥ç»ç½‘ç»œçš„å…¶ä»–å¸¸è§„æƒé‡å’Œåç½®ä¸€èµ·è¿›è¡Œä¼˜åŒ–ã€‚æŠ€æœ¯ä¸Šï¼Œæˆ‘ä»¬é€šè¿‡å®šä¹‰ä¸€ä¸ªè‡ªå®šä¹‰å±‚æ¥å®ç°è¿™ä¸ªç›®æ ‡ï¼š
- en: '[PRE1]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Note that this layer does nothing besides introducing the two parameters as
    the model attributes.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: è¯·æ³¨æ„ï¼Œè¿™ä¸€å±‚é™¤äº†å°†è¿™ä¸¤ä¸ªå‚æ•°å¼•å…¥ä¸ºæ¨¡å‹å±æ€§å¤–æ²¡æœ‰å…¶ä»–åŠŸèƒ½ã€‚
- en: 'Next, we define the function to calculate ODE residuals, which will serve as
    the physics-informed loss term:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å®šä¹‰è®¡ç®—å¸¸å¾®åˆ†æ–¹ç¨‹æ®‹å·®çš„å‡½æ•°ï¼Œè¿™å°†ä½œä¸ºç‰©ç†ä¿¡æ¯æŸå¤±é¡¹ï¼š
- en: '[PRE2]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Note that we have used `model.layers[-1]` to retrieve the parameter layer we
    defined earlier. This would give us the *a* and *b* values to calculate the ODE
    residuals.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: è¯·æ³¨æ„ï¼Œæˆ‘ä»¬ä½¿ç”¨ `model.layers[-1]` æ¥æ£€ç´¢æˆ‘ä»¬ä¹‹å‰å®šä¹‰çš„å‚æ•°å±‚ã€‚è¿™å°†ä¸ºæˆ‘ä»¬æä¾› *a* å’Œ *b* å€¼ï¼Œä»¥è®¡ç®—å¸¸å¾®åˆ†æ–¹ç¨‹çš„æ®‹å·®ã€‚
- en: 'Next up, we define the logic for calculating the gradients of total loss with
    respect to the parameters (including both usual weights and biases, as well as
    the unknown parameters *a* and *b*). This will prepare us for performing the gradient
    descent for model training:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å®šä¹‰è®¡ç®—æ€»æŸå¤±å¯¹å‚æ•°ï¼ˆåŒ…æ‹¬å¸¸è§„æƒé‡å’Œåç½®ä»¥åŠæœªçŸ¥å‚æ•° *a* å’Œ *b*ï¼‰çš„æ¢¯åº¦çš„é€»è¾‘ã€‚è¿™å°†ä¸ºæˆ‘ä»¬è¿›è¡Œæ¢¯åº¦ä¸‹é™ä»¥è®­ç»ƒæ¨¡å‹åšå¥½å‡†å¤‡ï¼š
- en: '[PRE3]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Note that our loss function consists of three terms: the initial condition
    loss, the ODE residuals, and the data loss. For forward problems, data loss is
    optional, as the model can directly learn the underlying operator solely based
    on the given differential equations. However, for inverse problems (as in the
    current case, parameter estimations), incorporating a data loss is essential to
    ensure we find the correct *a* and *b* values. For our current case, it is sufficient
    to simply set all weights, i.e.,`IC_weight`, `ODE_weight`, and `data_weight` as
    1.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: è¯·æ³¨æ„ï¼Œæˆ‘ä»¬çš„æŸå¤±å‡½æ•°ç”±ä¸‰ä¸ªéƒ¨åˆ†ç»„æˆï¼šåˆå§‹æ¡ä»¶æŸå¤±ã€ODE æ®‹å·®å’Œæ•°æ®æŸå¤±ã€‚å¯¹äºæ­£å‘é—®é¢˜ï¼Œæ•°æ®æŸå¤±æ˜¯å¯é€‰çš„ï¼Œå› ä¸ºæ¨¡å‹å¯ä»¥ä»…æ ¹æ®ç»™å®šçš„å¾®åˆ†æ–¹ç¨‹ç›´æ¥å­¦ä¹ åŸºç¡€ç®—å­ã€‚ç„¶è€Œï¼Œå¯¹äºåå‘é—®é¢˜ï¼ˆå¦‚å½“å‰çš„å‚æ•°ä¼°è®¡æƒ…å†µï¼‰ï¼Œç»“åˆæ•°æ®æŸå¤±æ˜¯å¿…è¦çš„ï¼Œä»¥ç¡®ä¿æˆ‘ä»¬æ‰¾åˆ°æ­£ç¡®çš„
    *a* å’Œ *b* å€¼ã€‚åœ¨æˆ‘ä»¬å½“å‰çš„æ¡ˆä¾‹ä¸­ï¼Œä»…éœ€å°†æ‰€æœ‰æƒé‡ï¼Œå³ `IC_weight`ã€`ODE_weight` å’Œ `data_weight` è®¾ç½®ä¸º 1
    å³å¯ã€‚
- en: 'Now we are ready to define the main training logic:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬å‡†å¤‡å¥½å®šä¹‰ä¸»è¦çš„è®­ç»ƒé€»è¾‘ï¼š
- en: '[PRE4]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Here we set the same initial value of 1 for both *a* and *b*. Recall that the
    true values of *a* and *b* are actually 0.5*.* In the following, letâ€™s train the
    PI-DeepONet model and see if it can recover the true values of *a* and *b*.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬å°† *a* å’Œ *b* çš„åˆå§‹å€¼éƒ½è®¾ç½®ä¸º 1ã€‚è¯·è®°ä½ï¼Œ*a* å’Œ *b* çš„çœŸå®å€¼å®é™…ä¸Šæ˜¯ 0.5*ã€‚æ¥ä¸‹æ¥ï¼Œè®©æˆ‘ä»¬è®­ç»ƒ PI-DeepONet
    æ¨¡å‹ï¼Œçœ‹çœ‹å®ƒæ˜¯å¦èƒ½å¤Ÿæ¢å¤ *a* å’Œ *b* çš„çœŸå®å€¼ã€‚
- en: Please note that the code we discussed are only key snippets extracted from
    the full training/validation logic. To see the complete code, check out the [notebook](https://github.com/ShuaiGuo16/PI-DeepONet/blob/main/case-study-inverse-parameter.ipynb).
  id: totrans-88
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: è¯·æ³¨æ„ï¼Œæˆ‘ä»¬è®¨è®ºçš„ä»£ç ä»…æ˜¯ä»å®Œæ•´çš„è®­ç»ƒ/éªŒè¯é€»è¾‘ä¸­æå–çš„å…³é”®ç‰‡æ®µã€‚è¦æŸ¥çœ‹å®Œæ•´ä»£ç ï¼Œè¯·æŸ¥çœ‹[notebook](https://github.com/ShuaiGuo16/PI-DeepONet/blob/main/case-study-inverse-parameter.ipynb)ã€‚
- en: 3.3 Results discussion
  id: totrans-89
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.3 ç»“æœè®¨è®º
- en: 'To use PI-DeepONet to estimate unknown ODE parameters, we first need to generate
    the training dataset. In our current case, the data generation proceeds in two
    steps: firstly, we generate the u(Â·) profiles (i.e., the input function) using
    a zero-mean **Gaussian Process**, with a radial basis function (RBF) kernel. Afterward,
    we run `scipy.integrate.solve_ivp` on our target ODE (with both *a* and *b* set
    as 0.5) to calculate the corresponding s(Â·) profiles (i.e., the output function).
    The figure below shows three random samples used for training.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: è¦ä½¿ç”¨ PI-DeepONet ä¼°è®¡æœªçŸ¥çš„ ODE å‚æ•°ï¼Œæˆ‘ä»¬é¦–å…ˆéœ€è¦ç”Ÿæˆè®­ç»ƒæ•°æ®é›†ã€‚åœ¨æˆ‘ä»¬å½“å‰çš„æ¡ˆä¾‹ä¸­ï¼Œæ•°æ®ç”Ÿæˆåˆ†ä¸ºä¸¤ä¸ªæ­¥éª¤ï¼šé¦–å…ˆï¼Œæˆ‘ä»¬ä½¿ç”¨é›¶å‡å€¼çš„**é«˜æ–¯è¿‡ç¨‹**ç”Ÿæˆ
    u(Â·) æ›²çº¿ï¼ˆå³è¾“å…¥å‡½æ•°ï¼‰ï¼Œå…¶å…·æœ‰å¾„å‘åŸºå‡½æ•°ï¼ˆRBFï¼‰æ ¸ã€‚ä¹‹åï¼Œæˆ‘ä»¬åœ¨ç›®æ ‡ ODE ä¸Šè¿è¡Œ `scipy.integrate.solve_ivp`ï¼ˆå°† *a*
    å’Œ *b* éƒ½è®¾ç½®ä¸º 0.5ï¼‰ä»¥è®¡ç®—ç›¸åº”çš„ s(Â·) æ›²çº¿ï¼ˆå³è¾“å‡ºå‡½æ•°ï¼‰ã€‚ä¸‹å›¾å±•ç¤ºäº†ç”¨äºè®­ç»ƒçš„ä¸‰ä¸ªéšæœºæ ·æœ¬ã€‚
- en: '![](../Images/49abe28b01c92a89948281ba89ea477e.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/49abe28b01c92a89948281ba89ea477e.png)'
- en: Figure 6\. Three samples were randomly selected from the training dataset for
    illustration purposes. The upper row shows the u(Â·) profiles, while the lower
    row shows the corresponding s(Â·) profiles calculated by running an ODE solver.
    (Image by author)
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 6\. ä»è®­ç»ƒæ•°æ®é›†ä¸­éšæœºé€‰æ‹©çš„ä¸‰ä¸ªæ ·æœ¬ç”¨äºè¯´æ˜ã€‚ä¸Šæ’æ˜¾ç¤ºäº† u(Â·) æ›²çº¿ï¼Œä¸‹æ’æ˜¾ç¤ºäº†é€šè¿‡è¿è¡Œ ODE æ±‚è§£å™¨è®¡ç®—çš„å¯¹åº” s(Â·) æ›²çº¿ã€‚ï¼ˆå›¾ç‰‡ç”±ä½œè€…æä¾›ï¼‰
- en: Once the training dataset is ready, we can kick off the PI-DeepONet training
    process. The following figure displays the loss evolution, which indicates that
    the model is properly trained and is able to fit both the data and ODE very well.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€æ—¦è®­ç»ƒæ•°æ®é›†å‡†å¤‡å¥½ï¼Œæˆ‘ä»¬å°±å¯ä»¥å¯åŠ¨ PI-DeepONet çš„è®­ç»ƒè¿‡ç¨‹ã€‚ä¸‹å›¾æ˜¾ç¤ºäº†æŸå¤±çš„æ¼”å˜ï¼Œè¿™è¡¨æ˜æ¨¡å‹å·²ç»å¾—åˆ°äº†é€‚å½“çš„è®­ç»ƒï¼Œå¹¶èƒ½å¤Ÿå¾ˆå¥½åœ°æ‹Ÿåˆæ•°æ®å’Œ
    ODEã€‚
- en: '![](../Images/3cf5fe0b651ee8535a3c306eba57bec5.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3cf5fe0b651ee8535a3c306eba57bec5.png)'
- en: Figure 7\. Loss convergence plot. (Image by author)
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 7\. æŸå¤±æ”¶æ•›å›¾ã€‚ï¼ˆå›¾ç‰‡ç”±ä½œè€…æä¾›ï¼‰
- en: Training the PI-DeepONet is not the end. Instead, our ultimate goal is to estimate
    the values of *a* and *b*. The following figure depicts the evolution of the unknown
    parameters. We can clearly see that our PI-DeepONet is doing its job and has accurately
    estimated the true values of *a* and *b*.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: è®­ç»ƒ PI-DeepONet å¹¶ä¸æ˜¯ç»ˆç‚¹ã€‚æˆ‘ä»¬çš„æœ€ç»ˆç›®æ ‡æ˜¯ä¼°è®¡ *a* å’Œ *b* çš„å€¼ã€‚ä¸‹å›¾å±•ç¤ºäº†æœªçŸ¥å‚æ•°çš„æ¼”å˜ã€‚æˆ‘ä»¬å¯ä»¥æ¸…æ¥šåœ°çœ‹åˆ°ï¼ŒPI-DeepONet
    æ­£åœ¨å‘æŒ¥ä½œç”¨ï¼Œå¹¶å‡†ç¡®åœ°ä¼°è®¡äº† *a* å’Œ *b* çš„çœŸå®å€¼ã€‚
- en: '![](../Images/5b1ad194c77c63455d22ca0699945120.png)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5b1ad194c77c63455d22ca0699945120.png)'
- en: Figure 8\. Our unknown parameters a and b gradually moved away from the specified
    initial values and converged to their true values. This demonstrates that PI-DeepONet
    is capable of performing inverse parameter calibration for differential equations
    with functional inputs. (Image by author)
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 8\. æˆ‘ä»¬çš„æœªçŸ¥å‚æ•° *a* å’Œ *b* é€æ¸è¿œç¦»æŒ‡å®šçš„åˆå§‹å€¼ï¼Œå¹¶æ”¶æ•›åˆ°å®ƒä»¬çš„çœŸå®å€¼ã€‚è¿™è¡¨æ˜ PI-DeepONet èƒ½å¤Ÿå¯¹å…·æœ‰å‡½æ•°è¾“å…¥çš„å¾®åˆ†æ–¹ç¨‹è¿›è¡Œåå‘å‚æ•°æ ¡å‡†ã€‚ï¼ˆå›¾ç‰‡ç”±ä½œè€…æä¾›ï¼‰
- en: 'We can further test the sensitivity of the estimation results with respect
    to the initial values. The following figure shows the evolution of the parameters
    under a different set of initial values (*a_init*=0.2, *b_init*=0.8):'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥è¿›ä¸€æ­¥æµ‹è¯•ä¼°è®¡ç»“æœå¯¹åˆå§‹å€¼çš„æ•æ„Ÿæ€§ã€‚ä¸‹å›¾æ˜¾ç¤ºäº†åœ¨ä¸åŒåˆå§‹å€¼ï¼ˆ*a_init*=0.2ï¼Œ*b_init*=0.8ï¼‰ä¸‹å‚æ•°çš„æ¼”å˜ï¼š
- en: '![](../Images/b99ab18ff1d4b58a8435c2f0c165ccc3.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b99ab18ff1d4b58a8435c2f0c165ccc3.png)'
- en: Figure 9\. For a different set of initial values, our developed PI-DeepONet
    model is able to accurately estimate the true values of a and b. (Image by author)
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾9\. å¯¹äºä¸åŒçš„åˆå§‹å€¼é›†ï¼Œæˆ‘ä»¬å¼€å‘çš„PI-DeepONetæ¨¡å‹èƒ½å¤Ÿå‡†ç¡®ä¼°è®¡aå’Œbçš„çœŸå®å€¼ã€‚ï¼ˆå›¾ç‰‡ç”±ä½œè€…æä¾›ï¼‰
- en: Overall, we can see that the PI-DeepONet model is capable of performing parameter
    calibrations when the input to the ODE is a function.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: æ€»ä½“è€Œè¨€ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°PI-DeepONetæ¨¡å‹èƒ½å¤Ÿåœ¨ODEçš„è¾“å…¥ä¸ºå‡½æ•°æ—¶è¿›è¡Œå‚æ•°æ ¡å‡†ã€‚
- en: '4\. Problem 2: Input Function Estimation'
  id: totrans-103
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 4\. é—®é¢˜2ï¼šè¾“å…¥å‡½æ•°ä¼°è®¡
- en: 'Time to level up our game! In this section, we will tackle a more challenging
    case study: estimating the entire profile of the input function u(Â·). Similar
    to the previous section, we will start with a brief discussion of the solution
    strategies, followed by implementing the proposed strategies to address our target
    problem. We will look into two different strategies: one employing native TensorFlow
    and another using the L-BFGS optimization algorithm.'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: è¯¥å‡çº§æˆ‘ä»¬çš„æ¸¸æˆæ—¶é—´åˆ°äº†ï¼åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†è§£å†³ä¸€ä¸ªæ›´å…·æŒ‘æˆ˜æ€§çš„æ¡ˆä¾‹ç ”ç©¶ï¼šä¼°è®¡è¾“å…¥å‡½æ•°u(Â·)çš„æ•´ä¸ªè½®å»“ã€‚ç±»ä¼¼äºå‰ä¸€èŠ‚ï¼Œæˆ‘ä»¬å°†ä»ç®€è¦è®¨è®ºè§£å†³æ–¹æ¡ˆç­–ç•¥å¼€å§‹ï¼Œç„¶åå®æ–½æå‡ºçš„ç­–ç•¥æ¥è§£å†³æˆ‘ä»¬çš„ç›®æ ‡é—®é¢˜ã€‚æˆ‘ä»¬å°†æ¢è®¨ä¸¤ç§ä¸åŒçš„ç­–ç•¥ï¼šä¸€ç§ä½¿ç”¨åŸç”ŸTensorFlowï¼Œå¦ä¸€ç§ä½¿ç”¨L-BFGSä¼˜åŒ–ç®—æ³•ã€‚
- en: 4.1 Solution stratgies
  id: totrans-105
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.1 è§£å†³æ–¹æ¡ˆç­–ç•¥
- en: 'To devise our solution strategy, we need to consider two things:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†åˆ¶å®šæˆ‘ä»¬çš„è§£å†³æ–¹æ¡ˆç­–ç•¥ï¼Œæˆ‘ä»¬éœ€è¦è€ƒè™‘ä¸¤ä»¶äº‹ï¼š
- en: 1ï¸âƒ£ Optimization efficiency
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 1ï¸âƒ£ ä¼˜åŒ–æ•ˆç‡
- en: At its core, our target inverse problem requires optimizing the input profile
    u(Â·) such that the predicted s(Â·) matches with the observed s(Â·). Assuming that
    we can calculate s(Â·) given u(Â·), many off-the-shelf optimizers can be used to
    achieve our goal. Now the question is, how do we calculate s(Â·) given u(Â·)?
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: ä»æ ¹æœ¬ä¸Šè®²ï¼Œæˆ‘ä»¬çš„ç›®æ ‡åé—®é¢˜è¦æ±‚ä¼˜åŒ–è¾“å…¥è½®å»“u(Â·)ï¼Œä½¿å¾—é¢„æµ‹çš„s(Â·)ä¸è§‚å¯Ÿåˆ°çš„s(Â·)åŒ¹é…ã€‚å‡è®¾æˆ‘ä»¬å¯ä»¥ç»™å®šu(Â·)è®¡ç®—s(Â·)ï¼Œè®¸å¤šç°æˆçš„ä¼˜åŒ–å™¨å¯ä»¥ç”¨æ¥å®ç°æˆ‘ä»¬çš„ç›®æ ‡ã€‚ç°åœ¨çš„é—®é¢˜æ˜¯ï¼Œç»™å®šu(Â·)æˆ‘ä»¬å¦‚ä½•è®¡ç®—s(Â·)ï¼Ÿ
- en: The traditional method would be to employ a standard numerical ODE solver to
    predict the output function s(Â·) given the input function u(Â·). However, optimization
    processes typically involve multiple iterations. As a result, this approach of
    using a numerical ODE solver to calculate s(Â·) would be too inefficient, as in
    every iteration of the optimization loop, the u(Â·) will be updated, and a new
    s(Â·) needs to be calculated. This is where PI-DeepONet comes into play.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: ä¼ ç»Ÿçš„æ–¹æ³•æ˜¯ä½¿ç”¨æ ‡å‡†çš„æ•°å€¼ODEæ±‚è§£å™¨æ¥é¢„æµ‹ç»™å®šè¾“å…¥å‡½æ•°u(Â·)çš„è¾“å‡ºå‡½æ•°s(Â·)ã€‚ç„¶è€Œï¼Œä¼˜åŒ–è¿‡ç¨‹é€šå¸¸æ¶‰åŠå¤šä¸ªè¿­ä»£ã€‚å› æ­¤ï¼Œä½¿ç”¨æ•°å€¼ODEæ±‚è§£å™¨æ¥è®¡ç®—s(Â·)çš„æ–¹æ³•ä¼šå¤ªä½æ•ˆï¼Œå› ä¸ºåœ¨æ¯æ¬¡ä¼˜åŒ–å¾ªç¯è¿­ä»£ä¸­ï¼Œu(Â·)ä¼šè¢«æ›´æ–°ï¼Œå¹¶ä¸”éœ€è¦è®¡ç®—æ–°çš„s(Â·)ã€‚è¿™å°±æ˜¯PI-DeepONetå‘æŒ¥ä½œç”¨çš„åœ°æ–¹ã€‚
- en: As a first step, we need a fully trained PI-DeepONet. Since we have full knowledge
    of the target ODE (recall that in this case study, we assume we know the true
    values of *a* and *b*), we can easily train a PI-DeepONet solely based on ODE
    residual loss.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: ä½œä¸ºç¬¬ä¸€æ­¥ï¼Œæˆ‘ä»¬éœ€è¦ä¸€ä¸ªå®Œå…¨è®­ç»ƒå¥½çš„PI-DeepONetã€‚ç”±äºæˆ‘ä»¬å¯¹ç›®æ ‡ODEæœ‰å®Œå…¨çš„äº†è§£ï¼ˆå›é¡¾ä¸€ä¸‹åœ¨è¿™ä¸ªæ¡ˆä¾‹ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬å‡è®¾çŸ¥é“*a*å’Œ*b*çš„çœŸå®å€¼ï¼‰ï¼Œæˆ‘ä»¬å¯ä»¥ä»…åŸºäºODEæ®‹å·®æŸå¤±è½»æ¾åœ°è®­ç»ƒä¸€ä¸ªPI-DeepONetã€‚
- en: Once the PI-DeepONet is trained, we can then freeze its weights & biases, treat
    it as a fast â€œsurrogateâ€ model that can efficiently calculate s(Â·) given any u(Â·),
    and embed this trained PI-DeepONet into the optimization routine. Since the inference
    time of PI-DeepONet is negligible, the computational cost can be greatly reduced.
    Meanwhile, the trained PI-DeepONet model will be fully differentiable. This suggests
    that we are able to provide the gradients of the optimization objective function
    with respect to the u(Â·), thus harnessing the efficiency of gradient-based optimization
    algorithms. Both of those aspects make iterative optimization feasible.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€æ—¦PI-DeepONetè®­ç»ƒå®Œæˆï¼Œæˆ‘ä»¬å¯ä»¥å†»ç»“å…¶æƒé‡å’Œåç½®ï¼Œå°†å…¶è§†ä¸ºä¸€ä¸ªå¿«é€Ÿçš„â€œæ›¿ä»£â€æ¨¡å‹ï¼Œå¯ä»¥é«˜æ•ˆåœ°è®¡ç®—ç»™å®šä»»ä½•u(Â·)çš„s(Â·)ï¼Œå¹¶å°†è¿™ä¸ªè®­ç»ƒå¥½çš„PI-DeepONetåµŒå…¥åˆ°ä¼˜åŒ–ä¾‹ç¨‹ä¸­ã€‚ç”±äºPI-DeepONetçš„æ¨ç†æ—¶é—´å¯ä»¥å¿½ç•¥ä¸è®¡ï¼Œè®¡ç®—æˆæœ¬å¯ä»¥å¤§å¤§é™ä½ã€‚åŒæ—¶ï¼Œè®­ç»ƒåçš„PI-DeepONetæ¨¡å‹å°†æ˜¯å®Œå…¨å¯å¾®çš„ã€‚è¿™è¡¨æ˜æˆ‘ä»¬èƒ½å¤Ÿæä¾›ä¼˜åŒ–ç›®æ ‡å‡½æ•°ç›¸å¯¹äºu(Â·)çš„æ¢¯åº¦ï¼Œä»è€Œåˆ©ç”¨åŸºäºæ¢¯åº¦çš„ä¼˜åŒ–ç®—æ³•çš„æ•ˆç‡ã€‚è¿™ä¸¤ä¸ªæ–¹é¢ä½¿å¾—è¿­ä»£ä¼˜åŒ–å˜å¾—å¯è¡Œã€‚
- en: 2ï¸âƒ£ Optimization quantity
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 2ï¸âƒ£ ä¼˜åŒ–é‡
- en: When we talk about estimating the profile of u(Â·), we are not attempting to
    estimate the symbolic functional form of u(Â·). Instead, we are estimating **discrete
    u(Â·) values** evaluated at a fixed set of time coordinates *t*â‚, *t*â‚‚, etc. This
    also aligns with how we feed u(Â·) into the PI-DeepONet.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: å½“æˆ‘ä»¬è°ˆè®ºä¼°è®¡ u(Â·) çš„è½®å»“æ—¶ï¼Œæˆ‘ä»¬å¹¶ä¸æ˜¯è¯•å›¾ä¼°è®¡ u(Â·) çš„ç¬¦å·å‡½æ•°å½¢å¼ã€‚è€Œæ˜¯ä¼°è®¡**ç¦»æ•£çš„ u(Â·) å€¼**ï¼Œè¿™äº›å€¼æ˜¯åœ¨å›ºå®šæ—¶é—´åæ ‡ *t*â‚ã€*t*â‚‚
    ç­‰ä¸Šè¯„ä¼°çš„ã€‚è¿™ä¹Ÿä¸æˆ‘ä»¬å¦‚ä½•å°† u(Â·) è¾“å…¥åˆ° PI-DeepONet ä¸­çš„æ–¹å¼ä¸€è‡´ã€‚
- en: 'Generally, we would need to have a high number of discrete u(Â·) values to sufficiently
    describe the profile of u(Â·). Consequently, our optimization problem would be
    a high-dimensional one, as there are many parameters to optimize. This type of
    problem is inherently ill-posed: simply finding a u(Â·) that can generate a matching
    s(Â·) most likely would not be a very strong constraint, as there could easily
    be many u(Â·)â€™s that can generate the exact same s(Â·) profile. We need a stronger
    regularization to help constrain the solution space.'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: é€šå¸¸ï¼Œæˆ‘ä»¬éœ€è¦æœ‰å¤§é‡ç¦»æ•£çš„ u(Â·) å€¼æ¥å……åˆ†æè¿° u(Â·) çš„è½®å»“ã€‚å› æ­¤ï¼Œæˆ‘ä»¬çš„ä¼˜åŒ–é—®é¢˜å°†æ˜¯é«˜ç»´çš„ï¼Œå› ä¸ºæœ‰è®¸å¤šå‚æ•°éœ€è¦ä¼˜åŒ–ã€‚è¿™ç§ç±»å‹çš„é—®é¢˜æœ¬è´¨ä¸Šæ˜¯ç—…æ€çš„ï¼šä»…ä»…æ‰¾åˆ°ä¸€ä¸ªèƒ½å¤Ÿç”ŸæˆåŒ¹é…çš„
    s(Â·) çš„ u(Â·) å¾ˆå¯èƒ½ä¸æ˜¯ä¸€ä¸ªå¾ˆå¼ºçš„çº¦æŸï¼Œå› ä¸ºå¾ˆå®¹æ˜“å­˜åœ¨å¤šä¸ª u(Â·) èƒ½ç”Ÿæˆå®Œå…¨ç›¸åŒçš„ s(Â·) è½®å»“ã€‚æˆ‘ä»¬éœ€è¦æ›´å¼ºçš„æ­£åˆ™åŒ–æ¥å¸®åŠ©çº¦æŸè§£ç©ºé—´ã€‚
- en: So what should we do? Well, we can leverage the constraint brought by the known
    ODE. Simply put, our goal would be to find a u(Â·), that **not only generates the
    s(Â·) that matches with the observed s(Â·), but also satisfy the known ODE that
    dictates the relationship between u(Â·) and s(Â·)**.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: é‚£ä¹ˆæˆ‘ä»¬è¯¥æ€ä¹ˆåšå‘¢ï¼Ÿå®é™…ä¸Šï¼Œæˆ‘ä»¬å¯ä»¥åˆ©ç”¨å·²çŸ¥ ODE å¸¦æ¥çš„çº¦æŸã€‚ç®€å•æ¥è¯´ï¼Œæˆ‘ä»¬çš„ç›®æ ‡æ˜¯æ‰¾åˆ°ä¸€ä¸ª u(Â·)ï¼Œ**ä¸ä»…ç”Ÿæˆä¸è§‚å¯Ÿåˆ°çš„ s(Â·) åŒ¹é…çš„ s(Â·)ï¼Œè€Œä¸”æ»¡è¶³è§„å®š
    u(Â·) å’Œ s(Â·) ä¹‹é—´å…³ç³»çš„å·²çŸ¥ ODE**ã€‚
- en: '![](../Images/a373f0c2955a2f1cffab83f0b704f31b.png)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a373f0c2955a2f1cffab83f0b704f31b.png)'
- en: Figure 10\. Illustration of the optimization strategy. (Imag by author)
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 10\. ä¼˜åŒ–ç­–ç•¥çš„ç¤ºæ„å›¾ã€‚ï¼ˆå›¾åƒç”±ä½œè€…æä¾›ï¼‰
- en: Now with those points clarified, letâ€™s proceed to code up the optimization routine.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨è¿™äº›é—®é¢˜å·²ç»æ¾„æ¸…ï¼Œè®©æˆ‘ä»¬ç»§ç»­ç¼–å†™ä¼˜åŒ–ç¨‹åºã€‚
- en: '4.2 Optimization routine: TensorFlow'
  id: totrans-119
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.2 ä¼˜åŒ–ç¨‹åºï¼šTensorFlow
- en: 'As we have discussed previously, we start by training a PI-DeepONet that observes
    the known governing ODE. We can reuse the code developed for the first case study.
    The only changes we need to introduce are:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æˆ‘ä»¬ä¹‹å‰è®¨è®ºçš„ï¼Œæˆ‘ä»¬ä»è®­ç»ƒä¸€ä¸ªè§‚å¯Ÿå·²çŸ¥ ODE çš„ PI-DeepONet å¼€å§‹ã€‚æˆ‘ä»¬å¯ä»¥é‡ç”¨ä¸ºç¬¬ä¸€ä¸ªæ¡ˆä¾‹ç ”ç©¶å¼€å‘çš„ä»£ç ã€‚æˆ‘ä»¬éœ€è¦å¼•å…¥çš„å”¯ä¸€æ›´æ”¹æ˜¯ï¼š
- en: Set `a_init` and `b_init` to 0.5, as these are the true values of *a* and *b*.
  id: totrans-121
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å°† `a_init` å’Œ `b_init` è®¾ç½®ä¸º 0.5ï¼Œå› ä¸ºè¿™äº›æ˜¯ *a* å’Œ *b* çš„çœŸå®å€¼ã€‚
- en: Set the `trainable` flag of the `ParameterLayer` as False. This will prevent
    updating *a* and *b* values during backpropagation.
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å°† `ParameterLayer` çš„ `trainable` æ ‡å¿—è®¾ç½®ä¸º Falseã€‚è¿™å°†é˜²æ­¢åœ¨åå‘ä¼ æ’­æœŸé—´æ›´æ–° *a* å’Œ *b* çš„å€¼ã€‚
- en: Set `data_weight` to 0, as we donâ€™t need the paired u(Â·)-s(Â·) for training.
    The PI-DeepONet can be trained solely based on ODE residual loss, as we have shown
    in the [previous blog](https://medium.com/towards-data-science/operator-learning-via-physics-informed-deeponet-lets-implement-it-from-scratch-6659f3179887).
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å°† `data_weight` è®¾ç½®ä¸º 0ï¼Œå› ä¸ºæˆ‘ä»¬ä¸éœ€è¦é…å¯¹çš„ u(Â·)-s(Â·) æ¥è¿›è¡Œè®­ç»ƒã€‚PI-DeepONet å¯ä»¥ä»…åŸºäº ODE æ®‹å·®æŸå¤±è¿›è¡Œè®­ç»ƒï¼Œæ­£å¦‚æˆ‘ä»¬åœ¨[ä¹‹å‰çš„åšå®¢](https://medium.com/towards-data-science/operator-learning-via-physics-informed-deeponet-lets-implement-it-from-scratch-6659f3179887)ä¸­æ‰€å±•ç¤ºçš„é‚£æ ·ã€‚
- en: 'The following figure shows the training process: the loss values are converged
    and the PI-DeepONet has been properly trained.'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸‹å›¾å±•ç¤ºäº†è®­ç»ƒè¿‡ç¨‹ï¼šæŸå¤±å€¼å·²ç»æ”¶æ•›ï¼ŒPI-DeepONet å·²ç»å¾—åˆ°é€‚å½“è®­ç»ƒã€‚
- en: '![](../Images/23ffaeda1b53ce4053cf1446dae31371.png)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/23ffaeda1b53ce4053cf1446dae31371.png)'
- en: Figure 11\. Loss convergence of training PI-DeepONet without paired input-output
    data. (Image by author)
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 11\. PI-DeepONet åœ¨æ²¡æœ‰é…å¯¹è¾“å…¥è¾“å‡ºæ•°æ®çš„æƒ…å†µä¸‹çš„æŸå¤±æ”¶æ•›ã€‚ï¼ˆå›¾åƒç”±ä½œè€…æä¾›ï¼‰
- en: 'Next, we define the objective function and its gradients with respect to u(Â·)
    for our optimization process:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å®šä¹‰ç›®æ ‡å‡½æ•°åŠå…¶ç›¸å¯¹äº u(Â·) çš„æ¢¯åº¦ï¼Œä»¥ç”¨äºæˆ‘ä»¬çš„ä¼˜åŒ–è¿‡ç¨‹ï¼š
- en: '[PRE5]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: As you can see, we mostly repurposed the `train_step()` function developed in
    the first case study to define the objective function for optimization. For gradient
    calculation, a major difference between the two functions is that, here, we calculate
    the *gradients of the objective function (i.e., total loss) with respect to the
    input u(Â·)*, whereas `train_step()` calculates the *gradients of the total loss
    with respect to the neural network model parameters*. This makes sense as previously,
    we want to optimize the neural network parameters (i.e., weights & biases), while
    here we want to optimize the input u(Â·).
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚ä½ æ‰€è§ï¼Œæˆ‘ä»¬ä¸»è¦é‡æ–°åˆ©ç”¨äº†åœ¨ç¬¬ä¸€ä¸ªæ¡ˆä¾‹ç ”ç©¶ä¸­å¼€å‘çš„ `train_step()` å‡½æ•°æ¥å®šä¹‰ä¼˜åŒ–çš„ç›®æ ‡å‡½æ•°ã€‚åœ¨æ¢¯åº¦è®¡ç®—æ–¹é¢ï¼Œä¸¤ä¸ªå‡½æ•°çš„ä¸»è¦åŒºåˆ«åœ¨äºè¿™é‡Œæˆ‘ä»¬è®¡ç®—çš„æ˜¯*ç›®æ ‡å‡½æ•°ï¼ˆå³æ€»æŸå¤±ï¼‰ç›¸å¯¹äºè¾“å…¥
    u(Â·) çš„æ¢¯åº¦*ï¼Œè€Œ `train_step()` è®¡ç®—çš„æ˜¯*æ€»æŸå¤±ç›¸å¯¹äºç¥ç»ç½‘ç»œæ¨¡å‹å‚æ•°çš„æ¢¯åº¦*ã€‚è¿™å¾ˆåˆç†ï¼Œå› ä¸ºä¹‹å‰æˆ‘ä»¬æƒ³è¦ä¼˜åŒ–ç¥ç»ç½‘ç»œå‚æ•°ï¼ˆå³æƒé‡å’Œåå·®ï¼‰ï¼Œè€Œåœ¨è¿™é‡Œæˆ‘ä»¬å¸Œæœ›ä¼˜åŒ–è¾“å…¥
    u(Â·)ã€‚
- en: Also note that we discretize u(Â·) with 100 points equally spaced within the
    domain of [0, 1]. Therefore, the u(Â·) estimation task at our hand is essentially
    optimizing those 100 discrete u(Â·) values.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: è¿˜éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬å°† u(Â·) ç¦»æ•£åŒ–ä¸ºåœ¨ [0, 1] åŒºé—´å†…å‡åŒ€åˆ†å¸ƒçš„ 100 ä¸ªç‚¹ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æ‰‹å¤´çš„ u(Â·) ä¼°è®¡ä»»åŠ¡æœ¬è´¨ä¸Šæ˜¯ä¼˜åŒ–è¿™ 100
    ä¸ªç¦»æ•£çš„ u(Â·) å€¼ã€‚
- en: 'Next, we define the main logic for optimizing the u(Â·):'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å®šä¹‰äº†ä¼˜åŒ– u(Â·) çš„ä¸»è¦é€»è¾‘ï¼š
- en: '[PRE6]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Afterward, we can kick off the optimization process. For this case study, we
    randomly generate one u(Â·) profile and use the numerical solver to calculate its
    corresponding s(Â·) profile. Then, we assign the observed s(Â·) to `s_target` and
    test if our optimization algorithm can indeed identify the u(Â·) we used to generate
    the s(Â·).
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: éšåï¼Œæˆ‘ä»¬å¯ä»¥å¯åŠ¨ä¼˜åŒ–è¿‡ç¨‹ã€‚å¯¹äºè¿™ä¸ªæ¡ˆä¾‹ç ”ç©¶ï¼Œæˆ‘ä»¬éšæœºç”Ÿæˆä¸€ä¸ª u(Â·) é…ç½®ï¼Œå¹¶ä½¿ç”¨æ•°å€¼æ±‚è§£å™¨è®¡ç®—å…¶å¯¹åº”çš„ s(Â·) é…ç½®ã€‚ç„¶åï¼Œæˆ‘ä»¬å°†è§‚å¯Ÿåˆ°çš„ s(Â·)
    åˆ†é…ç»™ `s_target`ï¼Œå¹¶æµ‹è¯•æˆ‘ä»¬çš„ä¼˜åŒ–ç®—æ³•æ˜¯å¦ç¡®å®èƒ½å¤Ÿè¯†åˆ«æˆ‘ä»¬ç”¨æ¥ç”Ÿæˆ s(Â·) çš„ u(Â·)ã€‚
- en: For initialization, we simply use an all-zero profile for u(Â·) and assigned
    it to `initial_u`. The following figure shows the convergence of the objective
    function.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨åˆå§‹åŒ–æ—¶ï¼Œæˆ‘ä»¬ç®€å•åœ°ä½¿ç”¨å…¨é›¶çš„ u(Â·) é…ç½®ï¼Œå¹¶å°†å…¶åˆ†é…ç»™ `initial_u`ã€‚ä¸‹å›¾æ˜¾ç¤ºäº†ç›®æ ‡å‡½æ•°çš„æ”¶æ•›æƒ…å†µã€‚
- en: '![](../Images/d376db67f81aed828f0ea09406cd4ad8.png)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d376db67f81aed828f0ea09406cd4ad8.png)'
- en: Figure 12\. We used Adam optimizer with a fixed learning rate (5e-3) to optimize
    u(Â·). (Image by author)
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 12\. æˆ‘ä»¬ä½¿ç”¨å›ºå®šå­¦ä¹ ç‡ï¼ˆ5e-3ï¼‰çš„ Adam ä¼˜åŒ–å™¨æ¥ä¼˜åŒ– u(Â·)ã€‚ï¼ˆå›¾åƒç”±ä½œè€…æä¾›ï¼‰
- en: The following figure shows the evolution of the u(Â·) profile. We can see that
    as the number of iterations increased, the u(Â·) profile gradually converged to
    the true u(Â·).
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸‹å›¾æ˜¾ç¤ºäº† u(Â·) é…ç½®çš„æ¼”å˜ã€‚æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œéšç€è¿­ä»£æ¬¡æ•°çš„å¢åŠ ï¼Œu(Â·) é…ç½®é€æ¸æ”¶æ•›åˆ°çœŸå®çš„ u(Â·)ã€‚
- en: '![](../Images/f1c552a5b7c13f001ffbcffe747e1ded.png)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f1c552a5b7c13f001ffbcffe747e1ded.png)'
- en: Figure 13\. During the inverse calibration process, u(Â·) gradually converged
    to the true u(Â·), indicating that the PI-DeepONet approach is able to successfully
    perform inverse calibration of the input function profile. (Image by author)
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 13\. åœ¨é€†æ ¡å‡†è¿‡ç¨‹ä¸­ï¼Œu(Â·) é€æ¸æ”¶æ•›åˆ°çœŸå®çš„ u(Â·)ï¼Œè¿™è¡¨æ˜ PI-DeepONet æ–¹æ³•èƒ½å¤ŸæˆåŠŸåœ°æ‰§è¡Œè¾“å…¥å‡½æ•°é…ç½®çš„é€†æ ¡å‡†ã€‚ï¼ˆå›¾åƒç”±ä½œè€…æä¾›ï¼‰
- en: We can then perform the usual forward simulation to obtain the predicted s(Â·)
    given the optimized u(Â·). The forward simulation was conducted via both the numerical
    ODE solver and the trained PI-DeepONet. We can see that the predicted s(Â·)â€™s follow
    closely to the ground truth.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åï¼Œæˆ‘ä»¬å¯ä»¥æ‰§è¡Œé€šå¸¸çš„å‰å‘æ¨¡æ‹Ÿï¼Œä»¥è·å¾—ç»™å®šä¼˜åŒ– u(Â·) çš„é¢„æµ‹ s(Â·)ã€‚å‰å‘æ¨¡æ‹Ÿé€šè¿‡æ•°å€¼ ODE æ±‚è§£å™¨å’Œè®­ç»ƒè¿‡çš„ PI-DeepONet è¿›è¡Œã€‚æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œé¢„æµ‹çš„
    s(Â·) ç´§å¯†è·Ÿéšå®é™…å€¼ã€‚
- en: '![](../Images/ecaa151acbcd93c1ac8f043e049c8e7f.png)'
  id: totrans-141
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ecaa151acbcd93c1ac8f043e049c8e7f.png)'
- en: Figure 14\. Forward simulation to calculate the s(Â·) given the optimized u(Â·).
    The prediction results match very well with the ground truth. (Image by author)
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 14\. å‘å‰æ¨¡æ‹Ÿä»¥è®¡ç®—ç»™å®šä¼˜åŒ– u(Â·) çš„ s(Â·)ã€‚é¢„æµ‹ç»“æœä¸å®é™…å€¼éå¸¸å»åˆã€‚ï¼ˆå›¾åƒç”±ä½œè€…æä¾›ï¼‰
- en: 'We can further test if our optimization strategy also worked for other u(Â·)
    profiles. The following plots show six more random u(Â·) profiles we used in the
    testing:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬è¿˜å¯ä»¥è¿›ä¸€æ­¥æµ‹è¯•æˆ‘ä»¬çš„ä¼˜åŒ–ç­–ç•¥æ˜¯å¦é€‚ç”¨äºå…¶ä»– u(Â·) é…ç½®ã€‚ä»¥ä¸‹å›¾è¡¨å±•ç¤ºäº†æˆ‘ä»¬åœ¨æµ‹è¯•ä¸­ä½¿ç”¨çš„å¦å¤–å…­ä¸ªéšæœº u(Â·) é…ç½®ï¼š
- en: '![](../Images/e45851dbe4f948a5f9ad5fa4cff149fa.png)'
  id: totrans-144
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e45851dbe4f948a5f9ad5fa4cff149fa.png)'
- en: Figure 15\. For different u(Â·) profiles, we ran the same PI-DeepONet-based optimization
    routine with all-zero initialization. The results showed that our strategy has
    managed to deliver satisfactory results. (Image by author)
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 15\. å¯¹äºä¸åŒçš„ u(Â·) é…ç½®ï¼Œæˆ‘ä»¬ä½¿ç”¨å…¨é›¶åˆå§‹åŒ–è¿è¡Œäº†ç›¸åŒçš„åŸºäº PI-DeepONet çš„ä¼˜åŒ–ä¾‹ç¨‹ã€‚ç»“æœè¡¨æ˜æˆ‘ä»¬çš„ç­–ç•¥æˆåŠŸåœ°æä¾›äº†ä»¤äººæ»¡æ„çš„ç»“æœã€‚ï¼ˆå›¾åƒç”±ä½œè€…æä¾›ï¼‰
- en: Overall, the results showed that the PI-DeepONet approach can successfully perform
    inverse calibration for the entire input function profile.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: æ€»ä½“è€Œè¨€ï¼Œç»“æœæ˜¾ç¤º PI-DeepONet æ–¹æ³•èƒ½å¤ŸæˆåŠŸåœ°å¯¹æ•´ä¸ªè¾“å…¥å‡½æ•°è½®å»“è¿›è¡Œåå‘æ ¡å‡†ã€‚
- en: '4.3 Optimization routine: L-BFGS'
  id: totrans-147
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.3 ä¼˜åŒ–ä¾‹ç¨‹ï¼šL-BFGS
- en: The optimization routine we developed above was entirely operated in the TensorFlow
    environment. As a matter of fact, we can also leverage second-order optimization
    methods, such as L-BGFS implemented in SciPy, to achieve our goal. In this section,
    we will look into how to integrate our developed PI-DeepONet into the L-BGFS optimization
    workflow.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ä¸Šé¢å¼€å‘çš„ä¼˜åŒ–ä¾‹ç¨‹å®Œå…¨åœ¨ TensorFlow ç¯å¢ƒä¸­æ“ä½œã€‚å®é™…ä¸Šï¼Œæˆ‘ä»¬è¿˜å¯ä»¥åˆ©ç”¨äºŒé˜¶ä¼˜åŒ–æ–¹æ³•ï¼Œå¦‚ SciPy ä¸­å®ç°çš„ L-BGFSï¼Œæ¥å®ç°æˆ‘ä»¬çš„ç›®æ ‡ã€‚åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†æ¢è®¨å¦‚ä½•å°†æˆ‘ä»¬å¼€å‘çš„
    PI-DeepONet é›†æˆåˆ° L-BGFS ä¼˜åŒ–å·¥ä½œæµä¸­ã€‚
- en: 'If you would like to learn more about L-BFGS approach, take a look at this
    blog post: [Numerical optimization based on the L-BFGS method](/numerical-optimization-based-on-the-l-bfgs-method-f6582135b0ca)'
  id: totrans-149
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: å¦‚æœä½ æƒ³äº†è§£æ›´å¤šå…³äº L-BFGS æ–¹æ³•çš„ä¿¡æ¯ï¼Œå¯ä»¥æŸ¥çœ‹è¿™ç¯‡åšå®¢æ–‡ç« ï¼š[åŸºäº L-BFGS æ–¹æ³•çš„æ•°å€¼ä¼˜åŒ–](/numerical-optimization-based-on-the-l-bfgs-method-f6582135b0ca)
- en: 'To begin with, we define the objective function for the L-BFGS optimizer. The
    objective function is again a combination of data loss and ODE residual loss:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: é¦–å…ˆï¼Œæˆ‘ä»¬å®šä¹‰äº† L-BFGS ä¼˜åŒ–å™¨çš„ç›®æ ‡å‡½æ•°ã€‚ç›®æ ‡å‡½æ•°å†æ¬¡æ˜¯æ•°æ®æŸå¤±å’Œ ODE æ®‹å·®æŸå¤±çš„ç»„åˆï¼š
- en: '[PRE7]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Additionally, we need to define two different instances of the `obj_fn`: one
    for returning the Numpy object, and one for returning the TensorFlow object.'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤å¤–ï¼Œæˆ‘ä»¬éœ€è¦å®šä¹‰ `obj_fn` çš„ä¸¤ä¸ªä¸åŒå®ä¾‹ï¼šä¸€ä¸ªç”¨äºè¿”å› Numpy å¯¹è±¡ï¼Œå¦ä¸€ä¸ªç”¨äºè¿”å› TensorFlow å¯¹è±¡ã€‚
- en: '[PRE8]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The `obj_fn_numpy` will be used by L-BFGS in SciPy to calculate the main objective
    function, while the `obj_fn_tensor` will be used by TensorFlow to calculate the
    gradients of the objective function with respect to u(Â·), as shown below:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '`obj_fn_numpy` å°†ç”± SciPy çš„ L-BFGS ç”¨äºè®¡ç®—ä¸»è¦çš„ç›®æ ‡å‡½æ•°ï¼Œè€Œ `obj_fn_tensor` å°†ç”± TensorFlow
    ç”¨äºè®¡ç®—ç›®æ ‡å‡½æ•°å…³äº u(Â·) çš„æ¢¯åº¦ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š'
- en: '[PRE9]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Now we are ready to define the main optimization logic:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬å‡†å¤‡å®šä¹‰ä¸»è¦çš„ä¼˜åŒ–é€»è¾‘ï¼š
- en: '[PRE10]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'To test the effectiveness of the L-BFGS algorithm, we adopted the same u(Â·)
    and s(Â·) investigated at the beginning of the previous subsection. We also set
    the initial u(Â·) to be an all-zero profile. After running 1000 L-BFGS iterations,
    we have obtained the following optimization results:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†æµ‹è¯• L-BFGS ç®—æ³•çš„æœ‰æ•ˆæ€§ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†å‰ä¸€å°èŠ‚å¼€å§‹æ—¶ç ”ç©¶çš„ç›¸åŒ u(Â·) å’Œ s(Â·)ã€‚æˆ‘ä»¬è¿˜å°†åˆå§‹ u(Â·) è®¾ç½®ä¸ºå…¨é›¶è½®å»“ã€‚ç»è¿‡ 1000
    æ¬¡ L-BFGS è¿­ä»£ï¼Œæˆ‘ä»¬è·å¾—äº†ä»¥ä¸‹ä¼˜åŒ–ç»“æœï¼š
- en: '![](../Images/082e63f40279dc4ba8dba9e542ea7dc7.png)'
  id: totrans-159
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/082e63f40279dc4ba8dba9e542ea7dc7.png)'
- en: Figure 16\. The calibrated u(Â·) and the predicted s(Â·). (Image by author)
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾16\. æ ¡å‡†åçš„ u(Â·) å’Œé¢„æµ‹çš„ s(Â·)ã€‚ï¼ˆå›¾ç‰‡ä½œè€…æä¾›ï¼‰
- en: We can see that the L-BFGS algorithm also achieved desired results and successfully
    estimated the u(Â·) profile. We further inspect six more u(Â·) profiles and obtained
    similar results.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼ŒL-BFGS ç®—æ³•ä¹Ÿå–å¾—äº†é¢„æœŸçš„ç»“æœï¼ŒæˆåŠŸä¼°è®¡äº† u(Â·) è½®å»“ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥æ£€æŸ¥äº†å…­ä¸ªä¸åŒçš„ u(Â·) è½®å»“ï¼Œå¹¶è·å¾—äº†ç±»ä¼¼çš„ç»“æœã€‚
- en: '![](../Images/efc4559a620a0774752116669f814eb2.png)'
  id: totrans-162
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/efc4559a620a0774752116669f814eb2.png)'
- en: Figure 17\. Calibration results with L-BFGS algorithm for different u(Â·) profiles.
    (Image by author)
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾17\. ä½¿ç”¨ L-BFGS ç®—æ³•å¯¹ä¸åŒ u(Â·) è½®å»“è¿›è¡Œæ ¡å‡†çš„ç»“æœã€‚ï¼ˆå›¾ç‰‡ä½œè€…æä¾›ï¼‰
- en: 5\. Take-away
  id: totrans-164
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 5\. é‡ç‚¹æ€»ç»“
- en: 'In this blog, we investigated how to use PI-DeepONet, a novel network architecture
    that combines the strengths of physics-informed learning and operator learning,
    to solve inverse problems. We looked into two case studies:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ç¯‡åšå®¢ä¸­ï¼Œæˆ‘ä»¬ç ”ç©¶äº†å¦‚ä½•ä½¿ç”¨ PI-DeepONetï¼Œè¿™æ˜¯ä¸€ç§ç»“åˆäº†ç‰©ç†ä¿¡æ¯å­¦ä¹ å’Œç®—å­å­¦ä¹ ä¼˜åŠ¿çš„æ–°å‹ç½‘ç»œæ¶æ„ï¼Œæ¥è§£å†³åé—®é¢˜ã€‚æˆ‘ä»¬ç ”ç©¶äº†ä¸¤ä¸ªæ¡ˆä¾‹ï¼š
- en: 1ï¸âƒ£ Parameter estimation
  id: totrans-166
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1ï¸âƒ£ å‚æ•°ä¼°è®¡
- en: Our goal is to estimate the unknown ODE parameters given the observed input
    function u(Â·) and output function s(Â·). Our strategy is to incorporate those unknown
    parameters as trainable parameters in the neural network, and optimize them jointly
    with the weights and biases of the neural network.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬çš„ç›®æ ‡æ˜¯ç»™å®šè§‚æµ‹åˆ°çš„è¾“å…¥å‡½æ•° u(Â·) å’Œè¾“å‡ºå‡½æ•° s(Â·)ï¼Œä¼°è®¡æœªçŸ¥çš„ ODE å‚æ•°ã€‚æˆ‘ä»¬çš„ç­–ç•¥æ˜¯å°†è¿™äº›æœªçŸ¥å‚æ•°ä½œä¸ºå¯è®­ç»ƒå‚æ•°çº³å…¥ç¥ç»ç½‘ç»œï¼Œå¹¶ä¸ç¥ç»ç½‘ç»œçš„æƒé‡å’Œåç½®ä¸€èµ·ä¼˜åŒ–ã€‚
- en: 2ï¸âƒ£ Input function calibration
  id: totrans-168
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2ï¸âƒ£ è¾“å…¥å‡½æ•°æ ¡å‡†
- en: Our goal is to estimate the entire u(Â·) profile given the perfectly known ODE
    and observed s(Â·). Our strategy is to first train a PI-DeepONet based on the known
    ODE to serve as a fast surrogate model, and then optimize the discretized u(Â·)
    values such that not only the generated s(Â·) match with the observed truth, but
    also fulfill the known ODE that connects u(Â·) and s(Â·).
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬çš„ç›®æ ‡æ˜¯ç»™å®šå®Œå…¨å·²çŸ¥çš„ODEå’Œè§‚å¯Ÿåˆ°çš„s(Â·)æ¥ä¼°è®¡æ•´ä¸ªu(Â·)æ›²çº¿ã€‚æˆ‘ä»¬çš„ç­–ç•¥æ˜¯é¦–å…ˆåŸºäºå·²çŸ¥çš„ODEè®­ç»ƒä¸€ä¸ªPI-DeepONetï¼Œä½œä¸ºå¿«é€Ÿçš„æ›¿ä»£æ¨¡å‹ï¼Œç„¶åä¼˜åŒ–ç¦»æ•£åŒ–çš„u(Â·)å€¼ï¼Œä»¥ä½¿ç”Ÿæˆçš„s(Â·)ä¸ä»…ä¸è§‚å¯Ÿåˆ°çš„çœŸå®æƒ…å†µåŒ¹é…ï¼Œè€Œä¸”ä¹Ÿæ»¡è¶³è¿æ¥u(Â·)å’Œs(Â·)çš„å·²çŸ¥ODEã€‚
- en: For both case studies, we have seen satisfactory results produced by the PI-DeepONet
    approach, suggesting that this approach can be used to effectively address both
    forward and inverse problems, thus serving as a promising tool for dynamical system
    modeling.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºè¿™ä¸¤ä¸ªæ¡ˆä¾‹ç ”ç©¶ï¼Œæˆ‘ä»¬å·²ç»çœ‹åˆ°PI-DeepONetæ–¹æ³•äº§ç”Ÿäº†ä»¤äººæ»¡æ„çš„ç»“æœï¼Œè¿™è¡¨æ˜è¯¥æ–¹æ³•å¯ä»¥æœ‰æ•ˆåœ°è§£å†³æ­£å‘å’Œé€†å‘é—®é¢˜ï¼Œä»è€Œä½œä¸ºåŠ¨æ€ç³»ç»Ÿå»ºæ¨¡çš„æœ‰å‰é€”çš„å·¥å…·ã€‚
- en: 'Inverse problems pose unique challenges in computational science and engineering,
    and yet we only scratched the surface of it in this blog. For many real-life applications,
    we need to further consider other aspects:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: é€†å‘é—®é¢˜åœ¨è®¡ç®—ç§‘å­¦å’Œå·¥ç¨‹ä¸­æå‡ºäº†ç‹¬ç‰¹çš„æŒ‘æˆ˜ï¼Œè€Œæˆ‘ä»¬åœ¨æœ¬åšå®¢ä¸­ä»…ä»…è§¦åŠäº†è¿™ä¸€é¢†åŸŸçš„è¡¨é¢ã€‚å¯¹äºè®¸å¤šå®é™…åº”ç”¨ï¼Œæˆ‘ä»¬è¿˜éœ€è¦è¿›ä¸€æ­¥è€ƒè™‘å…¶ä»–æ–¹é¢ï¼š
- en: 1ï¸âƒ£ Uncertainty quantification (UQ)
  id: totrans-172
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1ï¸âƒ£ ä¸ç¡®å®šæ€§é‡åŒ–ï¼ˆUQï¼‰
- en: In this blog, we simply assumed the observed input and output function values
    are perfect. However, in practical scenarios, both the input and output data may
    be contaminated by noise. Understanding how this uncertainty propagates through
    our model is essential for making reliable estimations.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬åšå®¢ä¸­ï¼Œæˆ‘ä»¬ç®€å•åœ°å‡è®¾è§‚å¯Ÿåˆ°çš„è¾“å…¥å’Œè¾“å‡ºå‡½æ•°å€¼æ˜¯å®Œç¾çš„ã€‚ç„¶è€Œï¼Œåœ¨å®é™…åœºæ™¯ä¸­ï¼Œè¾“å…¥å’Œè¾“å‡ºæ•°æ®å¯èƒ½ä¼šå—åˆ°å™ªå£°çš„æ±¡æŸ“ã€‚ç†è§£è¿™ç§ä¸ç¡®å®šæ€§å¦‚ä½•åœ¨æˆ‘ä»¬çš„æ¨¡å‹ä¸­ä¼ æ’­å¯¹äºåšå‡ºå¯é çš„ä¼°è®¡è‡³å…³é‡è¦ã€‚
- en: 2ï¸âƒ£ Modeling error
  id: totrans-174
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2ï¸âƒ£ å»ºæ¨¡è¯¯å·®
- en: 'In this blog, we assumed our ODE perfectly describes the dynamics of the system.
    However, in practical scenarios, this assumption will most likely not hold: the
    differential equation might only be an approximation to the true underlying physical
    processes, and there are missing physics that are not captured by the equation.
    How to properly account for these induced modeling errors could be a challenging
    yet interesting aspect to look at.'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬åšå®¢ä¸­ï¼Œæˆ‘ä»¬å‡è®¾æˆ‘ä»¬çš„ODEå®Œç¾åœ°æè¿°äº†ç³»ç»Ÿçš„åŠ¨æ€ã€‚ç„¶è€Œï¼Œåœ¨å®é™…åœºæ™¯ä¸­ï¼Œè¿™ä¸€å‡è®¾å¾ˆå¯èƒ½ä¸æˆç«‹ï¼šå¾®åˆ†æ–¹ç¨‹å¯èƒ½åªæ˜¯å¯¹çœŸå®ç‰©ç†è¿‡ç¨‹çš„è¿‘ä¼¼ï¼Œè¿˜æœ‰ä¸€äº›ç‰©ç†ç°è±¡æ²¡æœ‰è¢«æ–¹ç¨‹æ•æ‰åˆ°ã€‚å¦‚ä½•æ­£ç¡®è€ƒè™‘è¿™äº›å¼•å…¥çš„å»ºæ¨¡è¯¯å·®å¯èƒ½æ˜¯ä¸€ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§ä½†åˆæœ‰è¶£çš„æ–¹é¢ã€‚
- en: 3ï¸âƒ£ Extending to PDE
  id: totrans-176
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3ï¸âƒ£ æ‰©å±•åˆ°PDE
- en: In this blog, we demonstrated the workflow of solving inverse problems considering
    only a simple ODE. However, many practical systems are governed by PDEs with more
    spatial/temporal coordinates and complex input/output functional profiles. More
    challenges can be expected when we extend our developed methodology to PDEs (e.g.,
    computational efficiency will likely be a major issue, not only in training the
    PI-DeepONet but also in performing optimization for inverse calibration).
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬åšå®¢ä¸­ï¼Œæˆ‘ä»¬å±•ç¤ºäº†è§£å†³é€†å‘é—®é¢˜çš„å·¥ä½œæµç¨‹ï¼Œä»…è€ƒè™‘äº†ä¸€ä¸ªç®€å•çš„ODEã€‚ç„¶è€Œï¼Œè®¸å¤šå®é™…ç³»ç»Ÿç”±å…·æœ‰æ›´å¤šç©ºé—´/æ—¶é—´åæ ‡å’Œå¤æ‚è¾“å…¥/è¾“å‡ºåŠŸèƒ½ç‰¹å¾çš„PDEæ”¯é…ã€‚å½“æˆ‘ä»¬å°†å¼€å‘çš„æ–¹æ³•æ‰©å±•åˆ°PDEæ—¶ï¼Œå¯èƒ½ä¼šé‡åˆ°æ›´å¤šæŒ‘æˆ˜ï¼ˆä¾‹å¦‚ï¼Œè®¡ç®—æ•ˆç‡å¯èƒ½æˆä¸ºä¸»è¦é—®é¢˜ï¼Œä¸ä»…åœ¨è®­ç»ƒPI-DeepONetæ—¶ï¼Œè¿˜åœ¨è¿›è¡Œé€†å‘æ ‡å®šä¼˜åŒ–æ—¶ï¼‰ã€‚
- en: Congrats on reaching this far! If you find my content useful, you could buy
    me a coffee [here](https://www.buymeacoffee.com/Shuaiguo09f) ğŸ¤— Thank you very
    much for your support!
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: æ­å–œä½ çœ‹åˆ°è¿™é‡Œï¼å¦‚æœä½ è§‰å¾—æˆ‘çš„å†…å®¹æœ‰ç”¨ï¼Œå¯ä»¥åœ¨[è¿™é‡Œ](https://www.buymeacoffee.com/Shuaiguo09f)è¯·æˆ‘å–æ¯å’–å•¡ğŸ¤—
    éå¸¸æ„Ÿè°¢ä½ çš„æ”¯æŒï¼
- en: You can find the companion notebooks with full code [here](https://github.com/ShuaiGuo16/PI-DeepONet/tree/main)
    *ğŸ’»*
  id: totrans-179
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ä½ å¯ä»¥åœ¨[è¿™é‡Œ](https://github.com/ShuaiGuo16/PI-DeepONet/tree/main)æ‰¾åˆ°å¸¦æœ‰å®Œæ•´ä»£ç çš„é…å¥—ç¬”è®°æœ¬
    *ğŸ’»*
- en: ''
  id: totrans-180
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'To learn more about PI-DeepONet for **forward modeling**: [Operator Learning
    via Physics-Informed DeepONet](/operator-learning-via-physics-informed-deeponet-lets-implement-it-from-scratch-6659f3179887)'
  id: totrans-181
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: è¦äº†è§£æ›´å¤šå…³äº**æ­£å‘å»ºæ¨¡**çš„PI-DeepONetï¼š[é€šè¿‡ç‰©ç†ä¿¡æ¯æ·±åº¦æ“ä½œç½‘ç»œè¿›è¡Œçš„æ“ä½œå­¦ä¹ ](/operator-learning-via-physics-informed-deeponet-lets-implement-it-from-scratch-6659f3179887)
- en: ''
  id: totrans-182
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: To learn the best practices of physics-informed learning:[Unraveling the Design
    Pattern of Physics-Informed Neural Networks](https://medium.com/towards-data-science/unraveling-the-design-pattern-of-physics-informed-neural-networks-series-01-8190df459527)
  id: totrans-183
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: è¦äº†è§£ç‰©ç†ä¿¡æ¯å­¦ä¹ çš„æœ€ä½³å®è·µï¼š[æ­ç¤ºç‰©ç†ä¿¡æ¯ç¥ç»ç½‘ç»œçš„è®¾è®¡æ¨¡å¼](https://medium.com/towards-data-science/unraveling-the-design-pattern-of-physics-informed-neural-networks-series-01-8190df459527)
- en: ''
  id: totrans-184
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: You can also subscribe to my [newsletter](https://shuaiguo.medium.com/subscribe)
    or follow me on [Medium](https://shuaiguo.medium.com/).
  id: totrans-185
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ä½ ä¹Ÿå¯ä»¥è®¢é˜…æˆ‘çš„[é€šè®¯](https://shuaiguo.medium.com/subscribe)æˆ–åœ¨[Medium](https://shuaiguo.medium.com/)ä¸Šå…³æ³¨æˆ‘ã€‚
- en: Reference
  id: totrans-186
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å‚è€ƒæ–‡çŒ®
- en: '[1] Wang et al., Learning the solution operator of parametric partial differential
    equations with physics-informed DeepOnets. arXiv, 2021.'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] ç‹ç­‰ï¼Œåˆ©ç”¨ç‰©ç†ä¿¡æ¯æ·±åº¦ç½‘ç»œå­¦ä¹ å‚æ•°åŒ–åå¾®åˆ†æ–¹ç¨‹çš„è§£ç®—å­ã€‚arXivï¼Œ2021å¹´ã€‚'
