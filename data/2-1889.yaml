- en: 'Solving Inverse Problems With Physics-Informed DeepONet: A Practical Guide
    With Code Implementation'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 解决逆问题的物理信息深度操作网络：带有代码实现的实用指南
- en: 原文：[https://towardsdatascience.com/solving-inverse-problems-with-physics-informed-deeponet-a-practical-guide-with-code-implementation-27795eb4f502](https://towardsdatascience.com/solving-inverse-problems-with-physics-informed-deeponet-a-practical-guide-with-code-implementation-27795eb4f502)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/solving-inverse-problems-with-physics-informed-deeponet-a-practical-guide-with-code-implementation-27795eb4f502](https://towardsdatascience.com/solving-inverse-problems-with-physics-informed-deeponet-a-practical-guide-with-code-implementation-27795eb4f502)
- en: Two case studies with parameter estimation and input function calibration
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 两个带有参数估计和输入函数校准的案例研究
- en: '[](https://shuaiguo.medium.com/?source=post_page-----27795eb4f502--------------------------------)[![Shuai
    Guo](../Images/d673c066f8006079be5bf92757e73a59.png)](https://shuaiguo.medium.com/?source=post_page-----27795eb4f502--------------------------------)[](https://towardsdatascience.com/?source=post_page-----27795eb4f502--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----27795eb4f502--------------------------------)
    [Shuai Guo](https://shuaiguo.medium.com/?source=post_page-----27795eb4f502--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://shuaiguo.medium.com/?source=post_page-----27795eb4f502--------------------------------)[![Shuai
    Guo](../Images/d673c066f8006079be5bf92757e73a59.png)](https://shuaiguo.medium.com/?source=post_page-----27795eb4f502--------------------------------)[](https://towardsdatascience.com/?source=post_page-----27795eb4f502--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----27795eb4f502--------------------------------)
    [Shuai Guo](https://shuaiguo.medium.com/?source=post_page-----27795eb4f502--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----27795eb4f502--------------------------------)
    ·21 min read·Jul 17, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于[Towards Data Science](https://towardsdatascience.com/?source=post_page-----27795eb4f502--------------------------------)
    ·21分钟阅读·2023年7月17日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/375cf738f91896940d75caed2c8b983d.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/375cf738f91896940d75caed2c8b983d.png)'
- en: Photo by [愚木混株 cdd20](https://unsplash.com/@cdd20?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 由[愚木混株 cdd20](https://unsplash.com/@cdd20?utm_source=medium&utm_medium=referral)拍摄，来自[Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: In my [previous blog](https://medium.com/towards-data-science/operator-learning-via-physics-informed-deeponet-lets-implement-it-from-scratch-6659f3179887),
    we delved into the concept of physics-informed DeepONet (PI-DeepONet) and explored
    why it is particularly suitable for operator learning, i.e., learning mappings
    from an input function to an output function. We also turned theory into code
    and implemented a PI-DeepONet that can accurately solve an ordinary differential
    equation (ODE) even with unseen input forcing profiles.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在我的[上一篇博客](https://medium.com/towards-data-science/operator-learning-via-physics-informed-deeponet-lets-implement-it-from-scratch-6659f3179887)中，我们深入探讨了物理信息深度操作网络（PI-DeepONet）的概念，并探讨了它为何特别适合于操作符学习，即从输入函数学习到输出函数。我们还将理论转化为代码，实现了一个PI-DeepONet，该网络即使在遇到未见过的输入强迫配置文件时，也能准确地解决常微分方程（ODE）。
- en: '![](../Images/7ac9405ffbb94748bfce6bc08ba4c3b5.png)'
  id: totrans-9
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7ac9405ffbb94748bfce6bc08ba4c3b5.png)'
- en: Figure 1\. Operators transform one function into another, which is a concept
    frequently encountered in real-world dynamical systems. **Operator learning**
    essentially involves training a neural network model to approximate this underlying
    operator. A promising method to achieve that is **DeepONet**. (Image by author)
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 图1\. 操作符将一个函数转换为另一个函数，这是在实际动态系统中经常遇到的概念。**操作符学习**本质上涉及训练一个神经网络模型以逼近这个基础操作符。一种有前途的方法是**DeepONet**。
    （图片由作者提供）
- en: The ability to solve these ***forward*** problems with PI-DeepONet is certainly
    valuable. But is that all PI-DeepONet can do? Well, definitely not!
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 使用PI-DeepONet解决这些***前向***问题的能力无疑是有价值的。但这就是PI-DeepONet能做的全部吗？显然不是！
- en: 'Another important problem category we frequently encountered in computational
    science and engineering is the so-called ***inverse problem***. In essence, this
    type of problem **reverses the flow of information from output to input**: the
    input is unknown and the output is observable, and the task is to estimate the
    unknown input from the observed output.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个我们在计算科学和工程中经常遇到的重要问题类别是所谓的***逆问题***。本质上，这类问题**将信息流动从输出反向到输入**：输入是未知的，而输出是可观察的，任务是从观察到的输出中估计未知的输入。
- en: '![](../Images/cab35d9a2efb313c05f626f8ae922355.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/cab35d9a2efb313c05f626f8ae922355.png)'
- en: 'Figure 2\. In forward problems, the objective is to predict the outputs given
    the known inputs via the operator. In inverse problems, the process is reversed:
    known outputs are used to estimate the original, unknown inputs, often with only
    partial knowledge of the underlying operator. Both forward and inverse problems
    are commonly encountered in computational science and engineering. (Image by author)'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2\. 在前向问题中，目标是通过算子预测已知输入下的输出。在反向问题中，过程是相反的：使用已知的输出估算原始的、未知的输入，通常对底层算子只有部分了解。前向问题和反向问题在计算科学和工程中都很常见。（图像作者提供）
- en: 'As you might have guessed, PI-DeepONet can also be a super useful tool for
    tackling these types of problems. In this blog, we will take a close look at how
    that can be achieved. More concretely, we will address two case studies: one with
    parameter estimation, and the other one with input function calibrations.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你可能已经猜到的，PI-DeepONet 也可以成为解决这些类型问题的超级有用工具。在本博客中，我们将详细探讨如何实现这一点。更具体地，我们将解决两个案例研究：一个是参数估算，另一个是输入函数校准。
- en: This blog intends to be self-contained, with only a brief discussion on the
    basics of physics-informed (PI-) learning, DeepONet, as well as our main focus,
    PI-DeepONet. For a more comprehensive intro to those topics, feel free to check
    out [my previous blog](https://medium.com/towards-data-science/operator-learning-via-physics-informed-deeponet-lets-implement-it-from-scratch-6659f3179887).
  id: totrans-16
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 本博客旨在自成一体，仅对物理信息（PI-）学习、DeepONet 以及我们的主要关注点 PI-DeepONet 进行简要讨论。有关这些主题的更全面介绍，请随时查看[我之前的博客](https://medium.com/towards-data-science/operator-learning-via-physics-informed-deeponet-lets-implement-it-from-scratch-6659f3179887)。
- en: With that in mind, let’s get started!
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于此，让我们开始吧！
- en: Table of Content
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 目录
- en: '· [1\. PI-DeepONet: A refresher](#56d0)'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '· [1\. PI-DeepONet: 简介](#56d0)'
- en: · [2\. Problem Statements](#4238)
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: · [2\. 问题陈述](#4238)
- en: '· [3\. Problem 1: Parameter Estimation](#50f4)'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: · [3\. 问题 1：参数估算](#50f4)
- en: ∘ [3.1 How it works](#c17f)
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: ∘ [3.1 如何运作](#c17f)
- en: ∘ [3.2 Implementing a PI-DeepONet pipeline](#33fd)
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: ∘ [3.2 实现 PI-DeepONet 管道](#33fd)
- en: ∘ [3.3 Results discussion](#5bac)
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: ∘ [3.3 结果讨论](#5bac)
- en: '· [4\. Problem 2: Input Function Estimation](#cc01)'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: · [4\. 问题 2：输入函数估算](#cc01)
- en: ∘ [4.1 Solution stratgies](#e448)
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: ∘ [4.1 解决策略](#e448)
- en: '∘ [4.2 Optimization routine: TensorFlow](#da8c)'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: ∘ [4.2 优化常规：TensorFlow](#da8c)
- en: '∘ [4.3 Optimization routine: L-BFGS](#6bf1)'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: ∘ [4.3 优化常规：L-BFGS](#6bf1)
- en: · [5\. Take-away](#a486)
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: · [5\. 总结](#a486)
- en: · [Reference](#b410)
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: · [参考文献](#b410)
- en: '1\. PI-DeepONet: A refresher'
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '1\. PI-DeepONet: 简介'
- en: 'As its name implies, PI-DeepONet is the combination of two concepts: *physics-informed
    learning*, and *DeepONet*.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 正如其名字所示，PI-DeepONet 是两个概念的结合：*物理信息学习* 和 *DeepONet*。
- en: Physics-informed learning is a new paradigm of machine learning and gains particular
    traction in the domain of dynamical system modeling. Its key idea is to explicitly
    bake the governing differential equations directly into the machine learning model,
    often through the introduction of an additional loss term in the loss function
    that accounts for the residuals of the governing equations. The premise of this
    learning approach is that the model built this way will respect known physical
    laws and offer better generalizability, interpretability, and trustworthiness.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 物理信息学习是机器学习的一种新范式，并在动态系统建模领域获得了特别的关注。其核心思想是将控制微分方程直接融入机器学习模型中，通常通过在损失函数中引入额外的损失项来考虑控制方程的残差。这种学习方法的前提是，以这种方式构建的模型将尊重已知的物理定律，并提供更好的泛化能力、可解释性和可信度。
- en: DeepONet, on the other hand, resides in the traditional pure data-driven modeling
    domain. However, what’s unique about it is that DeepONet is specifically designed
    for **operator learning**, i.e., learning the mapping from an input function to
    an output function. This situation is frequently encountered in many dynamical
    systems. For instance, in a simple mass-spring system, the time-varying driving
    force serves as an input function (of time), while the resultant mass displacement
    is the output function (of time as well).
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: DeepONet 与此同时属于传统的纯数据驱动建模领域。然而，其独特之处在于 DeepONet 专门设计用于**算子学习**，即学习从输入函数到输出函数的映射。这种情况在许多动态系统中经常遇到。例如，在一个简单的质量-弹簧系统中，随时间变化的驱动力作为输入函数（时间的函数），而质量的位移则是输出函数（也是时间的函数）。
- en: DeepONet proposed a novel network architecture (as shown in Figure 3), where
    a **branch net** is used to transform the profile of the input function, and a
    **trunk net** is used to transform the temporal/spatial coordinates. The feature
    vectors output by these two nets are then merged via a dot product.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: DeepONet提出了一种新型网络架构（如图3所示），其中**分支网络**用于转换输入函数的特征，**主干网络**用于转换时间/空间坐标。这两个网络输出的特征向量通过点积合并。
- en: '![](../Images/1fe10410dafdbe1907c7408142907b24.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1fe10410dafdbe1907c7408142907b24.png)'
- en: Figure 3\. The architecture of DeepONet. The uniqueness of this method lies
    in its separation of branch and trunk nets to handle input function profiles and
    temporal/spatial coordinates, respectively. (Image by author)
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 图3\. DeepONet的架构。这种方法的独特性在于将分支网络和主干网络分开，分别处理输入函数特征和时间/空间坐标。（作者图片）
- en: Now, if we layer the concept of physics-informed learning on top of the DeepONet,
    we obtain what is known as PI-DeepONet.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，如果将物理信息学习的概念叠加到DeepONet上，我们得到的就是所谓的PI-DeepONet。
- en: '![](../Images/d1611ed23d6993195925a6cd19dc2691.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d1611ed23d6993195925a6cd19dc2691.png)'
- en: Figure 4\. Compared to a DeepONet, a PI-DeepONet contains extra loss terms such
    as the **ODE/PDE residual loss**, as well as the initial condition loss (IC loss)
    and boundary condition loss (BC loss). The conventional data loss is optional
    for PI-DeepONet, as it can directly learn the operator of the underlying dynamical
    system solely from the associated governing equations. (Image by author)
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 图4\. 与DeepONet相比，PI-DeepONet包含额外的损失项，如**ODE/PDE残差损失**，以及初始条件损失（IC损失）和边界条件损失（BC损失）。对于PI-DeepONet，传统的数据损失是可选的，因为它可以仅通过相关控制方程直接学习基础动态系统的算子。（作者图片）
- en: Once a PI-DeepONet is trained, it can predict the profile of the output function
    for a given new input function profile in real-time, while ensuring that the predictions
    align with the governing equations. As you can imagine, this makes PI-DeepONet
    a potentially very powerful tool for a diverse range of dynamic system modeling
    tasks.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦PI-DeepONet经过训练，它可以实时预测给定新输入函数特征的输出函数特征，同时确保预测结果与控制方程一致。正如你所想象的，这使得PI-DeepONet成为一个潜在非常强大的工具，适用于各种动态系统建模任务。
- en: 'However, in many other system modeling scenarios, we may also need to perform
    the exact opposite operation, i.e., we know the outputs and want to estimate the
    unknown inputs based on observed output and our prior knowledge of system dynamics.
    Generally speaking, this type of scenario falls in the scope of **inverse modeling**.
    A question that naturally arises here is: can we also use PI-DeepONet to address
    inverse estimation problems?'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在许多其他系统建模场景中，我们可能还需要执行完全相反的操作，即我们知道输出并希望根据观察到的输出和我们对系统动态的先验知识来估计未知的输入。一般来说，这种情况属于**逆向建模**的范围。这里自然出现了一个问题：我们是否也可以使用PI-DeepONet来解决逆向估计问题？
- en: Before we get into that, let’s first more precisely formulate the problems we
    are aiming to solve.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们深入探讨之前，让我们首先更准确地制定我们旨在解决的问题。
- en: 2\. Problem Statements
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 2\. 问题陈述
- en: 'We will use the same ODE discussed in the previous blog as our base model.
    Previously, we investigated an initial value problem described by the following
    equation:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用前面博客中讨论的相同ODE作为基础模型。之前，我们研究了由以下方程描述的初始值问题：
- en: '![](../Images/a2965a8118cbca756dc4b2c9fbbb113a.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a2965a8118cbca756dc4b2c9fbbb113a.png)'
- en: 'with an initial condition s(0) = 0\. In the equation, u(*t*) is the input function
    that varies over time, and s(*t*) denotes the state of the system at time *t*.
    Our previous focus is on solving the **forward** problem, i.e., predicting s(·)
    given u(·). Now, we will shift our focus and consider solving two types of inverse
    problems:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 以初始条件s(0) = 0为例。在方程中，u(*t*)是随时间变化的输入函数，而s(*t*)表示系统在时间*t*的状态。我们之前关注的是解决**前向**问题，即给定u(·)预测s(·)。现在，我们将转变关注点，考虑解决两种类型的逆向问题：
- en: 1️⃣ Estimating unknown input parameters
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 1️⃣ 估计未知的输入参数
- en: 'Let’s start with a straightforward inverse problem. Imagine our governing ODE
    has now evolved to be like this:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从一个简单的逆向问题开始。设想我们的控制ODE现在演变成这样：
- en: '![](../Images/298a2c22c0b53231f8bc8863fb14b9a5.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/298a2c22c0b53231f8bc8863fb14b9a5.png)'
- en: initial condition s(0) = 0, a and b are **unknowns**.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 初始条件s(0) = 0，a和b是**未知数**。
- en: with *a* and *b* being the two unknown parameters. Our objective here is to
    estimate the values of *a* and *b*, given the observed u(·) and s(·) profiles.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *a* 和 *b* 是两个未知参数。我们的目标是估计 *a* 和 *b* 的值，给定观察到的 u(·) 和 s(·) 轮廓。
- en: This type of problem falls in the scope of **parameter estimation,** where unknown
    parameters of the system need to be identified from the measured data. Typical
    examples of this type of problem include system identification for control engineering,
    material thermal coefficients estimation in computational heat transfer, etc.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 这类问题属于 **参数估计** 的范围，其中需要从测量数据中识别系统的未知参数。这类问题的典型示例包括控制工程中的系统识别、计算热传导中的材料热系数估计等。
- en: In our current case study, we will assume the true values for *a* and *b* are
    both 0.5.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们当前的案例研究中，我们将假设 *a* 和 *b* 的真实值均为 0.5。
- en: 2️⃣ Estimating the entire input function profile
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 2️⃣ 估计整个输入函数轮廓
- en: 'For the second case study, we ramp up the problem complexity: Suppose that
    we know perfectly about the ODE (i.e., we know the exact values of *a* and *b*).
    However, while we have observed the s(·) profile, we don’t yet know the u(·) profile
    that has generated this observed output function. Consequently, our objective
    here is to estimate the u(·) profile, given the observed s(·) profile and known
    ODE:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 对于第二个案例研究，我们提高了问题的复杂性：假设我们对常微分方程 (ODE) 完全了解（即，我们知道 *a* 和 *b* 的确切值）。然而，尽管我们观察了
    s(·) 轮廓，但我们尚不知道生成该观察输出函数的 u(·) 轮廓。因此，我们的目标是估计 u(·) 轮廓，给定观察到的 s(·) 轮廓和已知 ODE：
- en: '![](../Images/3e845f5f86ad6e2e52b414c01d6942d7.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3e845f5f86ad6e2e52b414c01d6942d7.png)'
- en: initial condition s(0) = 0, a=0.5, b=0.5.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 初始条件 s(0) = 0，a=0.5，b=0.5。
- en: Since we now aim to recover an entire input function instead of a small set
    of unknown parameters, this case study will be much more challenging than the
    first one. Unfortunately, this type of problem is inherently ill-posed and requires
    strong regularization to help constrain the solution space. Nevertheless, they
    often arise in various fields, including environmental engineering (e.g., identifying
    the profile of pollutant sources), aerospace engineering (e.g., calibrating the
    applied loads on aircraft), and wind engineering (e.g., wind force estimation).
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们现在的目标是恢复整个输入函数而不是一小部分未知参数，因此这个案例研究将比第一个案例更具挑战性。不幸的是，这类问题本质上是不适定的，需要强有力的正则化来帮助约束解空间。尽管如此，这类问题在多个领域中经常出现，包括环境工程（例如，识别污染源的轮廓）、航空航天工程（例如，校准飞机上的施加载荷）和风工程（例如，风力估计）。
- en: In the following two sections, we will address these two case studies one by
    one.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的两个部分中，我们将逐一讨论这两个案例研究。
- en: '3\. Problem 1: Parameter Estimation'
  id: totrans-61
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 3\. 问题 1：参数估计
- en: 'In this section, we tackle the first case study: estimating the unknown parameters
    in our target ODE. We will start with a brief discussion on how the general physics-informed
    neural networks can be used to solve this type of problem, followed by implementing
    a PI-DeepONet-based pipeline for parameter estimation. Afterward, we will apply
    it to our case study and discuss the obtained results.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们处理第一个案例研究：估计我们目标 ODE 中的未知参数。我们将首先简要讨论如何使用通用的物理信息神经网络来解决这类问题，然后实施基于 PI-DeepONet
    的参数估计管道。之后，我们将其应用于我们的案例研究，并讨论获得的结果。
- en: 3.1 How it works
  id: totrans-63
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.1 工作原理
- en: 'In the [original paper](https://www.sciencedirect.com/science/article/abs/pii/S0021999118307125)
    on physics-informed neural networks (PINNs), Raissi and co-authors outlined the
    strategy of using PINNs to solve inverse problem calibration problems: in essence,
    we can simply set the unknown parameters (in our current case, parameters *a*
    and *b*) as **trainable parameters** in the neural network, and optimize those
    unknown parameters together with the weights and bias of the neural net to minimize
    the loss function.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [原始论文](https://www.sciencedirect.com/science/article/abs/pii/S0021999118307125)
    中，Raissi 和合著者概述了使用 PINNs 解决逆问题校准问题的策略：本质上，我们可以简单地将未知参数（在我们当前的案例中，即参数 *a* 和 *b*）设为
    **可训练参数**，并与神经网络的权重和偏置一起优化这些未知参数，以最小化损失函数。
- en: 'Of course, the secret sauce lies in constructing the loss function: as a **physics-informed
    learning approach**, it not only contains a data mismatch term, which measures
    the discrepancy between the predicted output of the network and the observed data,
    but also a physics-informed regularization term, which calculates the residuals
    (i.e., the difference between the left and right-hand side of the differential
    equation) using the outputs of the neural network (and their derivatives) and
    the current estimates of the parameters *a* and *b*.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，关键在于构造损失函数：作为一种**物理信息学习方法**，它不仅包含一个数据不匹配项，该项衡量网络预测输出与观察数据之间的差异，还包含一个物理信息正则化项，该项使用神经网络的输出（及其导数）和当前的
    *a* 和 *b* 参数估计值来计算残差（即微分方程左右两侧的差异）。
- en: Now, when we perform this joint optimization, we’re effectively searching for
    *a* and *b* values that would lead to a network’s outputs that simultaneously
    fit the observed data and satisfy the governing differential equation. When the
    loss function reaches its minimum value (i.e., the training is converged), the
    final values of *a* and *b* we obtain are the ones that have achieved this balance,
    and they are thus constituting the estimates of the unknown parameters.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，当我们执行这种联合优化时，我们实际上是在寻找 *a* 和 *b* 值，这些值会导致网络的输出同时适应观察到的数据并满足控制微分方程。当损失函数达到最小值时（即训练收敛），我们获得的
    *a* 和 *b* 的最终值就是实现这种平衡的值，因此它们就是未知参数的估计值。
- en: '![](../Images/29eabf0d14cb453bf39079298d746c1f.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/29eabf0d14cb453bf39079298d746c1f.png)'
- en: Figure 5\. When using PI-DeepONet, the unknown parameters a and b are jointly
    optimized with the weights and biases of the DeepONet model. When the training
    is converged, the a and b values we end up with constitute their estimates. (Image
    by author)
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5\. 在使用 PI-DeepONet 时，未知参数 *a* 和 *b* 与 DeepONet 模型的权重和偏置一起优化。当训练收敛时，最终得到的
    *a* 和 *b* 值即为它们的估计值。（图像来自作者）
- en: 3.2 Implementing a PI-DeepONet pipeline
  id: totrans-69
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.2 实现 PI-DeepONet 流水线
- en: Enough about the theory, it’s time to see some code 💻
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 说够理论，接下来是代码部分 💻
- en: 'Let’s start with the definition of PI-DeepONet:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从 PI-DeepONet 的定义开始：
- en: '[PRE0]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'In the code above:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述代码中：
- en: We start by defining the trunk and branch networks (which are all simple fully
    connected nets). The feature vectors produced by both nets are merged via a dot
    product.
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们首先定义了主干网络和分支网络（它们都是简单的全连接网络）。两个网络生成的特征向量通过点积合并在一起。
- en: We proceed by adding a bias term on top of the dot product result, which is
    achieved by defining a custom `BiasLayer()`. This strategy could improve the prediction
    accuracy, as indicated in the [original DeepONet paper](https://arxiv.org/abs/1910.03193).
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们通过在点积结果上添加一个偏置项来进行下一步，这通过定义一个自定义的 `BiasLayer()` 实现。该策略可以提高预测准确性，正如 [原始 DeepONet
    论文](https://arxiv.org/abs/1910.03193) 中所指出的。
- en: 'Here comes the main change that makes our model capable of solving parameter
    estimation problems: we add *a* and *b* to the collection of the neural network
    model parameters. This way, when we set the `trainable` to be true, *a* and *b*
    will be optimized together with the other usual weights and biases of the neural
    network. Technically, we achieve this goal by defining a custom layer:'
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这里是使我们的模型能够解决参数估计问题的主要变化：我们将 *a* 和 *b* 添加到神经网络模型参数集合中。这样，当我们将 `trainable` 设置为
    true 时，*a* 和 *b* 将与神经网络的其他常规权重和偏置一起进行优化。技术上，我们通过定义一个自定义层来实现这个目标：
- en: '[PRE1]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Note that this layer does nothing besides introducing the two parameters as
    the model attributes.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，这一层除了将这两个参数引入为模型属性外没有其他功能。
- en: 'Next, we define the function to calculate ODE residuals, which will serve as
    the physics-informed loss term:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们定义计算常微分方程残差的函数，这将作为物理信息损失项：
- en: '[PRE2]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Note that we have used `model.layers[-1]` to retrieve the parameter layer we
    defined earlier. This would give us the *a* and *b* values to calculate the ODE
    residuals.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们使用 `model.layers[-1]` 来检索我们之前定义的参数层。这将为我们提供 *a* 和 *b* 值，以计算常微分方程的残差。
- en: 'Next up, we define the logic for calculating the gradients of total loss with
    respect to the parameters (including both usual weights and biases, as well as
    the unknown parameters *a* and *b*). This will prepare us for performing the gradient
    descent for model training:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们定义计算总损失对参数（包括常规权重和偏置以及未知参数 *a* 和 *b*）的梯度的逻辑。这将为我们进行梯度下降以训练模型做好准备：
- en: '[PRE3]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Note that our loss function consists of three terms: the initial condition
    loss, the ODE residuals, and the data loss. For forward problems, data loss is
    optional, as the model can directly learn the underlying operator solely based
    on the given differential equations. However, for inverse problems (as in the
    current case, parameter estimations), incorporating a data loss is essential to
    ensure we find the correct *a* and *b* values. For our current case, it is sufficient
    to simply set all weights, i.e.,`IC_weight`, `ODE_weight`, and `data_weight` as
    1.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们的损失函数由三个部分组成：初始条件损失、ODE 残差和数据损失。对于正向问题，数据损失是可选的，因为模型可以仅根据给定的微分方程直接学习基础算子。然而，对于反向问题（如当前的参数估计情况），结合数据损失是必要的，以确保我们找到正确的
    *a* 和 *b* 值。在我们当前的案例中，仅需将所有权重，即 `IC_weight`、`ODE_weight` 和 `data_weight` 设置为 1
    即可。
- en: 'Now we are ready to define the main training logic:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们准备好定义主要的训练逻辑：
- en: '[PRE4]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Here we set the same initial value of 1 for both *a* and *b*. Recall that the
    true values of *a* and *b* are actually 0.5*.* In the following, let’s train the
    PI-DeepONet model and see if it can recover the true values of *a* and *b*.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将 *a* 和 *b* 的初始值都设置为 1。请记住，*a* 和 *b* 的真实值实际上是 0.5*。接下来，让我们训练 PI-DeepONet
    模型，看看它是否能够恢复 *a* 和 *b* 的真实值。
- en: Please note that the code we discussed are only key snippets extracted from
    the full training/validation logic. To see the complete code, check out the [notebook](https://github.com/ShuaiGuo16/PI-DeepONet/blob/main/case-study-inverse-parameter.ipynb).
  id: totrans-88
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 请注意，我们讨论的代码仅是从完整的训练/验证逻辑中提取的关键片段。要查看完整代码，请查看[notebook](https://github.com/ShuaiGuo16/PI-DeepONet/blob/main/case-study-inverse-parameter.ipynb)。
- en: 3.3 Results discussion
  id: totrans-89
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.3 结果讨论
- en: 'To use PI-DeepONet to estimate unknown ODE parameters, we first need to generate
    the training dataset. In our current case, the data generation proceeds in two
    steps: firstly, we generate the u(·) profiles (i.e., the input function) using
    a zero-mean **Gaussian Process**, with a radial basis function (RBF) kernel. Afterward,
    we run `scipy.integrate.solve_ivp` on our target ODE (with both *a* and *b* set
    as 0.5) to calculate the corresponding s(·) profiles (i.e., the output function).
    The figure below shows three random samples used for training.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用 PI-DeepONet 估计未知的 ODE 参数，我们首先需要生成训练数据集。在我们当前的案例中，数据生成分为两个步骤：首先，我们使用零均值的**高斯过程**生成
    u(·) 曲线（即输入函数），其具有径向基函数（RBF）核。之后，我们在目标 ODE 上运行 `scipy.integrate.solve_ivp`（将 *a*
    和 *b* 都设置为 0.5）以计算相应的 s(·) 曲线（即输出函数）。下图展示了用于训练的三个随机样本。
- en: '![](../Images/49abe28b01c92a89948281ba89ea477e.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/49abe28b01c92a89948281ba89ea477e.png)'
- en: Figure 6\. Three samples were randomly selected from the training dataset for
    illustration purposes. The upper row shows the u(·) profiles, while the lower
    row shows the corresponding s(·) profiles calculated by running an ODE solver.
    (Image by author)
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6\. 从训练数据集中随机选择的三个样本用于说明。上排显示了 u(·) 曲线，下排显示了通过运行 ODE 求解器计算的对应 s(·) 曲线。（图片由作者提供）
- en: Once the training dataset is ready, we can kick off the PI-DeepONet training
    process. The following figure displays the loss evolution, which indicates that
    the model is properly trained and is able to fit both the data and ODE very well.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦训练数据集准备好，我们就可以启动 PI-DeepONet 的训练过程。下图显示了损失的演变，这表明模型已经得到了适当的训练，并能够很好地拟合数据和
    ODE。
- en: '![](../Images/3cf5fe0b651ee8535a3c306eba57bec5.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3cf5fe0b651ee8535a3c306eba57bec5.png)'
- en: Figure 7\. Loss convergence plot. (Image by author)
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7\. 损失收敛图。（图片由作者提供）
- en: Training the PI-DeepONet is not the end. Instead, our ultimate goal is to estimate
    the values of *a* and *b*. The following figure depicts the evolution of the unknown
    parameters. We can clearly see that our PI-DeepONet is doing its job and has accurately
    estimated the true values of *a* and *b*.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 训练 PI-DeepONet 并不是终点。我们的最终目标是估计 *a* 和 *b* 的值。下图展示了未知参数的演变。我们可以清楚地看到，PI-DeepONet
    正在发挥作用，并准确地估计了 *a* 和 *b* 的真实值。
- en: '![](../Images/5b1ad194c77c63455d22ca0699945120.png)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5b1ad194c77c63455d22ca0699945120.png)'
- en: Figure 8\. Our unknown parameters a and b gradually moved away from the specified
    initial values and converged to their true values. This demonstrates that PI-DeepONet
    is capable of performing inverse parameter calibration for differential equations
    with functional inputs. (Image by author)
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8\. 我们的未知参数 *a* 和 *b* 逐渐远离指定的初始值，并收敛到它们的真实值。这表明 PI-DeepONet 能够对具有函数输入的微分方程进行反向参数校准。（图片由作者提供）
- en: 'We can further test the sensitivity of the estimation results with respect
    to the initial values. The following figure shows the evolution of the parameters
    under a different set of initial values (*a_init*=0.2, *b_init*=0.8):'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以进一步测试估计结果对初始值的敏感性。下图显示了在不同初始值（*a_init*=0.2，*b_init*=0.8）下参数的演变：
- en: '![](../Images/b99ab18ff1d4b58a8435c2f0c165ccc3.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b99ab18ff1d4b58a8435c2f0c165ccc3.png)'
- en: Figure 9\. For a different set of initial values, our developed PI-DeepONet
    model is able to accurately estimate the true values of a and b. (Image by author)
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 图9\. 对于不同的初始值集，我们开发的PI-DeepONet模型能够准确估计a和b的真实值。（图片由作者提供）
- en: Overall, we can see that the PI-DeepONet model is capable of performing parameter
    calibrations when the input to the ODE is a function.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，我们可以看到PI-DeepONet模型能够在ODE的输入为函数时进行参数校准。
- en: '4\. Problem 2: Input Function Estimation'
  id: totrans-103
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 4\. 问题2：输入函数估计
- en: 'Time to level up our game! In this section, we will tackle a more challenging
    case study: estimating the entire profile of the input function u(·). Similar
    to the previous section, we will start with a brief discussion of the solution
    strategies, followed by implementing the proposed strategies to address our target
    problem. We will look into two different strategies: one employing native TensorFlow
    and another using the L-BFGS optimization algorithm.'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 该升级我们的游戏时间到了！在本节中，我们将解决一个更具挑战性的案例研究：估计输入函数u(·)的整个轮廓。类似于前一节，我们将从简要讨论解决方案策略开始，然后实施提出的策略来解决我们的目标问题。我们将探讨两种不同的策略：一种使用原生TensorFlow，另一种使用L-BFGS优化算法。
- en: 4.1 Solution stratgies
  id: totrans-105
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.1 解决方案策略
- en: 'To devise our solution strategy, we need to consider two things:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 为了制定我们的解决方案策略，我们需要考虑两件事：
- en: 1️⃣ Optimization efficiency
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 1️⃣ 优化效率
- en: At its core, our target inverse problem requires optimizing the input profile
    u(·) such that the predicted s(·) matches with the observed s(·). Assuming that
    we can calculate s(·) given u(·), many off-the-shelf optimizers can be used to
    achieve our goal. Now the question is, how do we calculate s(·) given u(·)?
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 从根本上讲，我们的目标反问题要求优化输入轮廓u(·)，使得预测的s(·)与观察到的s(·)匹配。假设我们可以给定u(·)计算s(·)，许多现成的优化器可以用来实现我们的目标。现在的问题是，给定u(·)我们如何计算s(·)？
- en: The traditional method would be to employ a standard numerical ODE solver to
    predict the output function s(·) given the input function u(·). However, optimization
    processes typically involve multiple iterations. As a result, this approach of
    using a numerical ODE solver to calculate s(·) would be too inefficient, as in
    every iteration of the optimization loop, the u(·) will be updated, and a new
    s(·) needs to be calculated. This is where PI-DeepONet comes into play.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 传统的方法是使用标准的数值ODE求解器来预测给定输入函数u(·)的输出函数s(·)。然而，优化过程通常涉及多个迭代。因此，使用数值ODE求解器来计算s(·)的方法会太低效，因为在每次优化循环迭代中，u(·)会被更新，并且需要计算新的s(·)。这就是PI-DeepONet发挥作用的地方。
- en: As a first step, we need a fully trained PI-DeepONet. Since we have full knowledge
    of the target ODE (recall that in this case study, we assume we know the true
    values of *a* and *b*), we can easily train a PI-DeepONet solely based on ODE
    residual loss.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 作为第一步，我们需要一个完全训练好的PI-DeepONet。由于我们对目标ODE有完全的了解（回顾一下在这个案例研究中，我们假设知道*a*和*b*的真实值），我们可以仅基于ODE残差损失轻松地训练一个PI-DeepONet。
- en: Once the PI-DeepONet is trained, we can then freeze its weights & biases, treat
    it as a fast “surrogate” model that can efficiently calculate s(·) given any u(·),
    and embed this trained PI-DeepONet into the optimization routine. Since the inference
    time of PI-DeepONet is negligible, the computational cost can be greatly reduced.
    Meanwhile, the trained PI-DeepONet model will be fully differentiable. This suggests
    that we are able to provide the gradients of the optimization objective function
    with respect to the u(·), thus harnessing the efficiency of gradient-based optimization
    algorithms. Both of those aspects make iterative optimization feasible.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦PI-DeepONet训练完成，我们可以冻结其权重和偏置，将其视为一个快速的“替代”模型，可以高效地计算给定任何u(·)的s(·)，并将这个训练好的PI-DeepONet嵌入到优化例程中。由于PI-DeepONet的推理时间可以忽略不计，计算成本可以大大降低。同时，训练后的PI-DeepONet模型将是完全可微的。这表明我们能够提供优化目标函数相对于u(·)的梯度，从而利用基于梯度的优化算法的效率。这两个方面使得迭代优化变得可行。
- en: 2️⃣ Optimization quantity
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 2️⃣ 优化量
- en: When we talk about estimating the profile of u(·), we are not attempting to
    estimate the symbolic functional form of u(·). Instead, we are estimating **discrete
    u(·) values** evaluated at a fixed set of time coordinates *t*₁, *t*₂, etc. This
    also aligns with how we feed u(·) into the PI-DeepONet.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们谈论估计 u(·) 的轮廓时，我们并不是试图估计 u(·) 的符号函数形式。而是估计**离散的 u(·) 值**，这些值是在固定时间坐标 *t*₁、*t*₂
    等上评估的。这也与我们如何将 u(·) 输入到 PI-DeepONet 中的方式一致。
- en: 'Generally, we would need to have a high number of discrete u(·) values to sufficiently
    describe the profile of u(·). Consequently, our optimization problem would be
    a high-dimensional one, as there are many parameters to optimize. This type of
    problem is inherently ill-posed: simply finding a u(·) that can generate a matching
    s(·) most likely would not be a very strong constraint, as there could easily
    be many u(·)’s that can generate the exact same s(·) profile. We need a stronger
    regularization to help constrain the solution space.'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，我们需要有大量离散的 u(·) 值来充分描述 u(·) 的轮廓。因此，我们的优化问题将是高维的，因为有许多参数需要优化。这种类型的问题本质上是病态的：仅仅找到一个能够生成匹配的
    s(·) 的 u(·) 很可能不是一个很强的约束，因为很容易存在多个 u(·) 能生成完全相同的 s(·) 轮廓。我们需要更强的正则化来帮助约束解空间。
- en: So what should we do? Well, we can leverage the constraint brought by the known
    ODE. Simply put, our goal would be to find a u(·), that **not only generates the
    s(·) that matches with the observed s(·), but also satisfy the known ODE that
    dictates the relationship between u(·) and s(·)**.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 那么我们该怎么做呢？实际上，我们可以利用已知 ODE 带来的约束。简单来说，我们的目标是找到一个 u(·)，**不仅生成与观察到的 s(·) 匹配的 s(·)，而且满足规定
    u(·) 和 s(·) 之间关系的已知 ODE**。
- en: '![](../Images/a373f0c2955a2f1cffab83f0b704f31b.png)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a373f0c2955a2f1cffab83f0b704f31b.png)'
- en: Figure 10\. Illustration of the optimization strategy. (Imag by author)
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10\. 优化策略的示意图。（图像由作者提供）
- en: Now with those points clarified, let’s proceed to code up the optimization routine.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 现在这些问题已经澄清，让我们继续编写优化程序。
- en: '4.2 Optimization routine: TensorFlow'
  id: totrans-119
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.2 优化程序：TensorFlow
- en: 'As we have discussed previously, we start by training a PI-DeepONet that observes
    the known governing ODE. We can reuse the code developed for the first case study.
    The only changes we need to introduce are:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们之前讨论的，我们从训练一个观察已知 ODE 的 PI-DeepONet 开始。我们可以重用为第一个案例研究开发的代码。我们需要引入的唯一更改是：
- en: Set `a_init` and `b_init` to 0.5, as these are the true values of *a* and *b*.
  id: totrans-121
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将 `a_init` 和 `b_init` 设置为 0.5，因为这些是 *a* 和 *b* 的真实值。
- en: Set the `trainable` flag of the `ParameterLayer` as False. This will prevent
    updating *a* and *b* values during backpropagation.
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将 `ParameterLayer` 的 `trainable` 标志设置为 False。这将防止在反向传播期间更新 *a* 和 *b* 的值。
- en: Set `data_weight` to 0, as we don’t need the paired u(·)-s(·) for training.
    The PI-DeepONet can be trained solely based on ODE residual loss, as we have shown
    in the [previous blog](https://medium.com/towards-data-science/operator-learning-via-physics-informed-deeponet-lets-implement-it-from-scratch-6659f3179887).
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将 `data_weight` 设置为 0，因为我们不需要配对的 u(·)-s(·) 来进行训练。PI-DeepONet 可以仅基于 ODE 残差损失进行训练，正如我们在[之前的博客](https://medium.com/towards-data-science/operator-learning-via-physics-informed-deeponet-lets-implement-it-from-scratch-6659f3179887)中所展示的那样。
- en: 'The following figure shows the training process: the loss values are converged
    and the PI-DeepONet has been properly trained.'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 下图展示了训练过程：损失值已经收敛，PI-DeepONet 已经得到适当训练。
- en: '![](../Images/23ffaeda1b53ce4053cf1446dae31371.png)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/23ffaeda1b53ce4053cf1446dae31371.png)'
- en: Figure 11\. Loss convergence of training PI-DeepONet without paired input-output
    data. (Image by author)
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11\. PI-DeepONet 在没有配对输入输出数据的情况下的损失收敛。（图像由作者提供）
- en: 'Next, we define the objective function and its gradients with respect to u(·)
    for our optimization process:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们定义目标函数及其相对于 u(·) 的梯度，以用于我们的优化过程：
- en: '[PRE5]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: As you can see, we mostly repurposed the `train_step()` function developed in
    the first case study to define the objective function for optimization. For gradient
    calculation, a major difference between the two functions is that, here, we calculate
    the *gradients of the objective function (i.e., total loss) with respect to the
    input u(·)*, whereas `train_step()` calculates the *gradients of the total loss
    with respect to the neural network model parameters*. This makes sense as previously,
    we want to optimize the neural network parameters (i.e., weights & biases), while
    here we want to optimize the input u(·).
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，我们主要重新利用了在第一个案例研究中开发的 `train_step()` 函数来定义优化的目标函数。在梯度计算方面，两个函数的主要区别在于这里我们计算的是*目标函数（即总损失）相对于输入
    u(·) 的梯度*，而 `train_step()` 计算的是*总损失相对于神经网络模型参数的梯度*。这很合理，因为之前我们想要优化神经网络参数（即权重和偏差），而在这里我们希望优化输入
    u(·)。
- en: Also note that we discretize u(·) with 100 points equally spaced within the
    domain of [0, 1]. Therefore, the u(·) estimation task at our hand is essentially
    optimizing those 100 discrete u(·) values.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 还需要注意的是，我们将 u(·) 离散化为在 [0, 1] 区间内均匀分布的 100 个点。因此，我们手头的 u(·) 估计任务本质上是优化这 100
    个离散的 u(·) 值。
- en: 'Next, we define the main logic for optimizing the u(·):'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们定义了优化 u(·) 的主要逻辑：
- en: '[PRE6]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Afterward, we can kick off the optimization process. For this case study, we
    randomly generate one u(·) profile and use the numerical solver to calculate its
    corresponding s(·) profile. Then, we assign the observed s(·) to `s_target` and
    test if our optimization algorithm can indeed identify the u(·) we used to generate
    the s(·).
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 随后，我们可以启动优化过程。对于这个案例研究，我们随机生成一个 u(·) 配置，并使用数值求解器计算其对应的 s(·) 配置。然后，我们将观察到的 s(·)
    分配给 `s_target`，并测试我们的优化算法是否确实能够识别我们用来生成 s(·) 的 u(·)。
- en: For initialization, we simply use an all-zero profile for u(·) and assigned
    it to `initial_u`. The following figure shows the convergence of the objective
    function.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 在初始化时，我们简单地使用全零的 u(·) 配置，并将其分配给 `initial_u`。下图显示了目标函数的收敛情况。
- en: '![](../Images/d376db67f81aed828f0ea09406cd4ad8.png)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d376db67f81aed828f0ea09406cd4ad8.png)'
- en: Figure 12\. We used Adam optimizer with a fixed learning rate (5e-3) to optimize
    u(·). (Image by author)
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12\. 我们使用固定学习率（5e-3）的 Adam 优化器来优化 u(·)。（图像由作者提供）
- en: The following figure shows the evolution of the u(·) profile. We can see that
    as the number of iterations increased, the u(·) profile gradually converged to
    the true u(·).
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 下图显示了 u(·) 配置的演变。我们可以看到，随着迭代次数的增加，u(·) 配置逐渐收敛到真实的 u(·)。
- en: '![](../Images/f1c552a5b7c13f001ffbcffe747e1ded.png)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f1c552a5b7c13f001ffbcffe747e1ded.png)'
- en: Figure 13\. During the inverse calibration process, u(·) gradually converged
    to the true u(·), indicating that the PI-DeepONet approach is able to successfully
    perform inverse calibration of the input function profile. (Image by author)
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13\. 在逆校准过程中，u(·) 逐渐收敛到真实的 u(·)，这表明 PI-DeepONet 方法能够成功地执行输入函数配置的逆校准。（图像由作者提供）
- en: We can then perform the usual forward simulation to obtain the predicted s(·)
    given the optimized u(·). The forward simulation was conducted via both the numerical
    ODE solver and the trained PI-DeepONet. We can see that the predicted s(·)’s follow
    closely to the ground truth.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以执行通常的前向模拟，以获得给定优化 u(·) 的预测 s(·)。前向模拟通过数值 ODE 求解器和训练过的 PI-DeepONet 进行。我们可以看到，预测的
    s(·) 紧密跟随实际值。
- en: '![](../Images/ecaa151acbcd93c1ac8f043e049c8e7f.png)'
  id: totrans-141
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ecaa151acbcd93c1ac8f043e049c8e7f.png)'
- en: Figure 14\. Forward simulation to calculate the s(·) given the optimized u(·).
    The prediction results match very well with the ground truth. (Image by author)
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14\. 向前模拟以计算给定优化 u(·) 的 s(·)。预测结果与实际值非常吻合。（图像由作者提供）
- en: 'We can further test if our optimization strategy also worked for other u(·)
    profiles. The following plots show six more random u(·) profiles we used in the
    testing:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以进一步测试我们的优化策略是否适用于其他 u(·) 配置。以下图表展示了我们在测试中使用的另外六个随机 u(·) 配置：
- en: '![](../Images/e45851dbe4f948a5f9ad5fa4cff149fa.png)'
  id: totrans-144
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e45851dbe4f948a5f9ad5fa4cff149fa.png)'
- en: Figure 15\. For different u(·) profiles, we ran the same PI-DeepONet-based optimization
    routine with all-zero initialization. The results showed that our strategy has
    managed to deliver satisfactory results. (Image by author)
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15\. 对于不同的 u(·) 配置，我们使用全零初始化运行了相同的基于 PI-DeepONet 的优化例程。结果表明我们的策略成功地提供了令人满意的结果。（图像由作者提供）
- en: Overall, the results showed that the PI-DeepONet approach can successfully perform
    inverse calibration for the entire input function profile.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，结果显示 PI-DeepONet 方法能够成功地对整个输入函数轮廓进行反向校准。
- en: '4.3 Optimization routine: L-BFGS'
  id: totrans-147
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.3 优化例程：L-BFGS
- en: The optimization routine we developed above was entirely operated in the TensorFlow
    environment. As a matter of fact, we can also leverage second-order optimization
    methods, such as L-BGFS implemented in SciPy, to achieve our goal. In this section,
    we will look into how to integrate our developed PI-DeepONet into the L-BGFS optimization
    workflow.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 我们上面开发的优化例程完全在 TensorFlow 环境中操作。实际上，我们还可以利用二阶优化方法，如 SciPy 中实现的 L-BGFS，来实现我们的目标。在本节中，我们将探讨如何将我们开发的
    PI-DeepONet 集成到 L-BGFS 优化工作流中。
- en: 'If you would like to learn more about L-BFGS approach, take a look at this
    blog post: [Numerical optimization based on the L-BFGS method](/numerical-optimization-based-on-the-l-bfgs-method-f6582135b0ca)'
  id: totrans-149
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 如果你想了解更多关于 L-BFGS 方法的信息，可以查看这篇博客文章：[基于 L-BFGS 方法的数值优化](/numerical-optimization-based-on-the-l-bfgs-method-f6582135b0ca)
- en: 'To begin with, we define the objective function for the L-BFGS optimizer. The
    objective function is again a combination of data loss and ODE residual loss:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们定义了 L-BFGS 优化器的目标函数。目标函数再次是数据损失和 ODE 残差损失的组合：
- en: '[PRE7]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Additionally, we need to define two different instances of the `obj_fn`: one
    for returning the Numpy object, and one for returning the TensorFlow object.'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们需要定义 `obj_fn` 的两个不同实例：一个用于返回 Numpy 对象，另一个用于返回 TensorFlow 对象。
- en: '[PRE8]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The `obj_fn_numpy` will be used by L-BFGS in SciPy to calculate the main objective
    function, while the `obj_fn_tensor` will be used by TensorFlow to calculate the
    gradients of the objective function with respect to u(·), as shown below:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '`obj_fn_numpy` 将由 SciPy 的 L-BFGS 用于计算主要的目标函数，而 `obj_fn_tensor` 将由 TensorFlow
    用于计算目标函数关于 u(·) 的梯度，如下所示：'
- en: '[PRE9]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Now we are ready to define the main optimization logic:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们准备定义主要的优化逻辑：
- en: '[PRE10]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'To test the effectiveness of the L-BFGS algorithm, we adopted the same u(·)
    and s(·) investigated at the beginning of the previous subsection. We also set
    the initial u(·) to be an all-zero profile. After running 1000 L-BFGS iterations,
    we have obtained the following optimization results:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 为了测试 L-BFGS 算法的有效性，我们采用了前一小节开始时研究的相同 u(·) 和 s(·)。我们还将初始 u(·) 设置为全零轮廓。经过 1000
    次 L-BFGS 迭代，我们获得了以下优化结果：
- en: '![](../Images/082e63f40279dc4ba8dba9e542ea7dc7.png)'
  id: totrans-159
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/082e63f40279dc4ba8dba9e542ea7dc7.png)'
- en: Figure 16\. The calibrated u(·) and the predicted s(·). (Image by author)
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 图16\. 校准后的 u(·) 和预测的 s(·)。（图片作者提供）
- en: We can see that the L-BFGS algorithm also achieved desired results and successfully
    estimated the u(·) profile. We further inspect six more u(·) profiles and obtained
    similar results.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，L-BFGS 算法也取得了预期的结果，成功估计了 u(·) 轮廓。我们进一步检查了六个不同的 u(·) 轮廓，并获得了类似的结果。
- en: '![](../Images/efc4559a620a0774752116669f814eb2.png)'
  id: totrans-162
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/efc4559a620a0774752116669f814eb2.png)'
- en: Figure 17\. Calibration results with L-BFGS algorithm for different u(·) profiles.
    (Image by author)
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 图17\. 使用 L-BFGS 算法对不同 u(·) 轮廓进行校准的结果。（图片作者提供）
- en: 5\. Take-away
  id: totrans-164
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 5\. 重点总结
- en: 'In this blog, we investigated how to use PI-DeepONet, a novel network architecture
    that combines the strengths of physics-informed learning and operator learning,
    to solve inverse problems. We looked into two case studies:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇博客中，我们研究了如何使用 PI-DeepONet，这是一种结合了物理信息学习和算子学习优势的新型网络架构，来解决反问题。我们研究了两个案例：
- en: 1️⃣ Parameter estimation
  id: totrans-166
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1️⃣ 参数估计
- en: Our goal is to estimate the unknown ODE parameters given the observed input
    function u(·) and output function s(·). Our strategy is to incorporate those unknown
    parameters as trainable parameters in the neural network, and optimize them jointly
    with the weights and biases of the neural network.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的目标是给定观测到的输入函数 u(·) 和输出函数 s(·)，估计未知的 ODE 参数。我们的策略是将这些未知参数作为可训练参数纳入神经网络，并与神经网络的权重和偏置一起优化。
- en: 2️⃣ Input function calibration
  id: totrans-168
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2️⃣ 输入函数校准
- en: Our goal is to estimate the entire u(·) profile given the perfectly known ODE
    and observed s(·). Our strategy is to first train a PI-DeepONet based on the known
    ODE to serve as a fast surrogate model, and then optimize the discretized u(·)
    values such that not only the generated s(·) match with the observed truth, but
    also fulfill the known ODE that connects u(·) and s(·).
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的目标是给定完全已知的ODE和观察到的s(·)来估计整个u(·)曲线。我们的策略是首先基于已知的ODE训练一个PI-DeepONet，作为快速的替代模型，然后优化离散化的u(·)值，以使生成的s(·)不仅与观察到的真实情况匹配，而且也满足连接u(·)和s(·)的已知ODE。
- en: For both case studies, we have seen satisfactory results produced by the PI-DeepONet
    approach, suggesting that this approach can be used to effectively address both
    forward and inverse problems, thus serving as a promising tool for dynamical system
    modeling.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这两个案例研究，我们已经看到PI-DeepONet方法产生了令人满意的结果，这表明该方法可以有效地解决正向和逆向问题，从而作为动态系统建模的有前途的工具。
- en: 'Inverse problems pose unique challenges in computational science and engineering,
    and yet we only scratched the surface of it in this blog. For many real-life applications,
    we need to further consider other aspects:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 逆向问题在计算科学和工程中提出了独特的挑战，而我们在本博客中仅仅触及了这一领域的表面。对于许多实际应用，我们还需要进一步考虑其他方面：
- en: 1️⃣ Uncertainty quantification (UQ)
  id: totrans-172
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1️⃣ 不确定性量化（UQ）
- en: In this blog, we simply assumed the observed input and output function values
    are perfect. However, in practical scenarios, both the input and output data may
    be contaminated by noise. Understanding how this uncertainty propagates through
    our model is essential for making reliable estimations.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 在本博客中，我们简单地假设观察到的输入和输出函数值是完美的。然而，在实际场景中，输入和输出数据可能会受到噪声的污染。理解这种不确定性如何在我们的模型中传播对于做出可靠的估计至关重要。
- en: 2️⃣ Modeling error
  id: totrans-174
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2️⃣ 建模误差
- en: 'In this blog, we assumed our ODE perfectly describes the dynamics of the system.
    However, in practical scenarios, this assumption will most likely not hold: the
    differential equation might only be an approximation to the true underlying physical
    processes, and there are missing physics that are not captured by the equation.
    How to properly account for these induced modeling errors could be a challenging
    yet interesting aspect to look at.'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 在本博客中，我们假设我们的ODE完美地描述了系统的动态。然而，在实际场景中，这一假设很可能不成立：微分方程可能只是对真实物理过程的近似，还有一些物理现象没有被方程捕捉到。如何正确考虑这些引入的建模误差可能是一个具有挑战性但又有趣的方面。
- en: 3️⃣ Extending to PDE
  id: totrans-176
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3️⃣ 扩展到PDE
- en: In this blog, we demonstrated the workflow of solving inverse problems considering
    only a simple ODE. However, many practical systems are governed by PDEs with more
    spatial/temporal coordinates and complex input/output functional profiles. More
    challenges can be expected when we extend our developed methodology to PDEs (e.g.,
    computational efficiency will likely be a major issue, not only in training the
    PI-DeepONet but also in performing optimization for inverse calibration).
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 在本博客中，我们展示了解决逆向问题的工作流程，仅考虑了一个简单的ODE。然而，许多实际系统由具有更多空间/时间坐标和复杂输入/输出功能特征的PDE支配。当我们将开发的方法扩展到PDE时，可能会遇到更多挑战（例如，计算效率可能成为主要问题，不仅在训练PI-DeepONet时，还在进行逆向标定优化时）。
- en: Congrats on reaching this far! If you find my content useful, you could buy
    me a coffee [here](https://www.buymeacoffee.com/Shuaiguo09f) 🤗 Thank you very
    much for your support!
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜你看到这里！如果你觉得我的内容有用，可以在[这里](https://www.buymeacoffee.com/Shuaiguo09f)请我喝杯咖啡🤗
    非常感谢你的支持！
- en: You can find the companion notebooks with full code [here](https://github.com/ShuaiGuo16/PI-DeepONet/tree/main)
    *💻*
  id: totrans-179
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 你可以在[这里](https://github.com/ShuaiGuo16/PI-DeepONet/tree/main)找到带有完整代码的配套笔记本
    *💻*
- en: ''
  id: totrans-180
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'To learn more about PI-DeepONet for **forward modeling**: [Operator Learning
    via Physics-Informed DeepONet](/operator-learning-via-physics-informed-deeponet-lets-implement-it-from-scratch-6659f3179887)'
  id: totrans-181
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 要了解更多关于**正向建模**的PI-DeepONet：[通过物理信息深度操作网络进行的操作学习](/operator-learning-via-physics-informed-deeponet-lets-implement-it-from-scratch-6659f3179887)
- en: ''
  id: totrans-182
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: To learn the best practices of physics-informed learning:[Unraveling the Design
    Pattern of Physics-Informed Neural Networks](https://medium.com/towards-data-science/unraveling-the-design-pattern-of-physics-informed-neural-networks-series-01-8190df459527)
  id: totrans-183
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 要了解物理信息学习的最佳实践：[揭示物理信息神经网络的设计模式](https://medium.com/towards-data-science/unraveling-the-design-pattern-of-physics-informed-neural-networks-series-01-8190df459527)
- en: ''
  id: totrans-184
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: You can also subscribe to my [newsletter](https://shuaiguo.medium.com/subscribe)
    or follow me on [Medium](https://shuaiguo.medium.com/).
  id: totrans-185
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 你也可以订阅我的[通讯](https://shuaiguo.medium.com/subscribe)或在[Medium](https://shuaiguo.medium.com/)上关注我。
- en: Reference
  id: totrans-186
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] Wang et al., Learning the solution operator of parametric partial differential
    equations with physics-informed DeepOnets. arXiv, 2021.'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] 王等，利用物理信息深度网络学习参数化偏微分方程的解算子。arXiv，2021年。'
