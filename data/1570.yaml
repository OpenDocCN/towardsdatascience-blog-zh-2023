- en: Boosting image generation by intersecting GANs with Diffusion models
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: é€šè¿‡å°†GANä¸æ‰©æ•£æ¨¡å‹äº¤å‰æå‡å›¾åƒç”Ÿæˆ
- en: åŸæ–‡ï¼š[https://towardsdatascience.com/boosting-image-generation-by-intersecting-gans-with-diffusion-models-6a22f935b0f3?source=collection_archive---------9-----------------------#2023-05-09](https://towardsdatascience.com/boosting-image-generation-by-intersecting-gans-with-diffusion-models-6a22f935b0f3?source=collection_archive---------9-----------------------#2023-05-09)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡ï¼š[https://towardsdatascience.com/boosting-image-generation-by-intersecting-gans-with-diffusion-models-6a22f935b0f3?source=collection_archive---------9-----------------------#2023-05-09](https://towardsdatascience.com/boosting-image-generation-by-intersecting-gans-with-diffusion-models-6a22f935b0f3?source=collection_archive---------9-----------------------#2023-05-09)
- en: A recipe for stable and efficient image-to-image translation
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä¸€ç§ç¨³å®šé«˜æ•ˆçš„å›¾åƒåˆ°å›¾åƒè½¬æ¢æ–¹æ³•
- en: '[](https://medium.com/@essolanoc?source=post_page-----6a22f935b0f3--------------------------------)[![Edgardo
    Solano Carrillo](../Images/e160dddf2cdcfc793c0e2cd81bfb5b61.png)](https://medium.com/@essolanoc?source=post_page-----6a22f935b0f3--------------------------------)[](https://towardsdatascience.com/?source=post_page-----6a22f935b0f3--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----6a22f935b0f3--------------------------------)
    [Edgardo Solano Carrillo](https://medium.com/@essolanoc?source=post_page-----6a22f935b0f3--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@essolanoc?source=post_page-----6a22f935b0f3--------------------------------)[![Edgardo
    Solano Carrillo](../Images/e160dddf2cdcfc793c0e2cd81bfb5b61.png)](https://medium.com/@essolanoc?source=post_page-----6a22f935b0f3--------------------------------)[](https://towardsdatascience.com/?source=post_page-----6a22f935b0f3--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----6a22f935b0f3--------------------------------)
    [Edgardo Solano Carrillo](https://medium.com/@essolanoc?source=post_page-----6a22f935b0f3--------------------------------)'
- en: Â·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: Â·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fdb88c072b0d3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fboosting-image-generation-by-intersecting-gans-with-diffusion-models-6a22f935b0f3&user=Edgardo+Solano+Carrillo&userId=db88c072b0d3&source=post_page-db88c072b0d3----6a22f935b0f3---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----6a22f935b0f3--------------------------------)
    Â·8 min readÂ·May 9, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F6a22f935b0f3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fboosting-image-generation-by-intersecting-gans-with-diffusion-models-6a22f935b0f3&user=Edgardo+Solano+Carrillo&userId=db88c072b0d3&source=-----6a22f935b0f3---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[å…³æ³¨](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fdb88c072b0d3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fboosting-image-generation-by-intersecting-gans-with-diffusion-models-6a22f935b0f3&user=Edgardo+Solano+Carrillo&userId=db88c072b0d3&source=post_page-db88c072b0d3----6a22f935b0f3---------------------post_header-----------)
    å‘è¡¨åœ¨ [Towards Data Science](https://towardsdatascience.com/?source=post_page-----6a22f935b0f3--------------------------------)
    Â·8åˆ†é’Ÿé˜…è¯»Â·2023å¹´5æœˆ9æ—¥[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F6a22f935b0f3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fboosting-image-generation-by-intersecting-gans-with-diffusion-models-6a22f935b0f3&user=Edgardo+Solano+Carrillo&userId=db88c072b0d3&source=-----6a22f935b0f3---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F6a22f935b0f3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fboosting-image-generation-by-intersecting-gans-with-diffusion-models-6a22f935b0f3&source=-----6a22f935b0f3---------------------bookmark_footer-----------)![](../Images/3424e2a8c94a60eb2f2cee483a0cfb08.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F6a22f935b0f3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fboosting-image-generation-by-intersecting-gans-with-diffusion-models-6a22f935b0f3&source=-----6a22f935b0f3---------------------bookmark_footer-----------)![](../Images/3424e2a8c94a60eb2f2cee483a0cfb08.png)'
- en: ATME is a model in the GAN âˆ© Diffusion class. Image generated using DALLÂ·E 2.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: ATMEæ˜¯GAN âˆ© æ‰©æ•£æ¨¡å‹ç±»ä¸­çš„ä¸€ä¸ªæ¨¡å‹ã€‚å›¾åƒç”±DALLÂ·E 2ç”Ÿæˆã€‚
- en: 'Visual Foundation Models (VFM) are at the core of cutting-edge technologies
    such as [Visual ChatGPT](https://arxiv.org/abs/2303.04671)Â¹. In this article,
    we will briefly discuss recent advances to blend two important ingredients of
    the VFM soup: GANs and Diffusion models, ending up in ATME at their intersection.
    ATME is a novel model that I introduced in the paper [Look ATME: The Discriminator
    Mean Entropy Needs Attention](https://arxiv.org/abs/2304.09024)Â², with GitHub
    repository available [here](https://github.com/dlr-mi/atme).'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 'è§†è§‰åŸºç¡€æ¨¡å‹ï¼ˆVFMï¼‰æ˜¯å¦‚[Visual ChatGPT](https://arxiv.org/abs/2303.04671)Â¹ç­‰å‰æ²¿æŠ€æœ¯çš„æ ¸å¿ƒã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å°†ç®€è¦è®¨è®ºæœ€è¿‘çš„è¿›å±•ï¼ŒèåˆVFMâ€œæ±¤â€çš„ä¸¤ä¸ªé‡è¦æˆåˆ†ï¼šGANså’Œæ‰©æ•£æ¨¡å‹ï¼Œæœ€ç»ˆè¾¾åˆ°å®ƒä»¬çš„äº¤é›†ATMEã€‚ATMEæ˜¯æˆ‘åœ¨è®ºæ–‡[Look
    ATME: The Discriminator Mean Entropy Needs Attention](https://arxiv.org/abs/2304.09024)Â²ä¸­ä»‹ç»çš„ä¸€ç§æ–°æ¨¡å‹ï¼ŒGitHubä»“åº“å¯åœ¨[æ­¤å¤„](https://github.com/dlr-mi/atme)æ‰¾åˆ°ã€‚'
- en: 'We will go through relevant weaknesses and strengths of each type of generative
    modeling. Then we discuss two categories of solutions to merge them: the naive
    GAN âˆª Diffusion and, in more depth, the efficient GAN âˆ© Diffusion classes of models.
    At the end, you will get a picture of how research around some of the VFM is currently
    evolving.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†è®¨è®ºæ¯ç§ç”Ÿæˆæ¨¡å‹çš„ç›¸å…³å¼±ç‚¹å’Œä¼˜åŠ¿ã€‚ç„¶åæˆ‘ä»¬è®¨è®ºåˆå¹¶å®ƒä»¬çš„ä¸¤ç±»è§£å†³æ–¹æ¡ˆï¼šæœ´ç´ çš„GAN âˆª Diffusion å’Œæ›´æ·±å…¥çš„é«˜æ•ˆçš„GAN âˆ© Diffusionæ¨¡å‹ç±»åˆ«ã€‚æœ€åï¼Œä½ å°†äº†è§£ä¸€äº›VFMç ”ç©¶ç›®å‰å¦‚ä½•å‘å±•ã€‚
- en: Generative models
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç”Ÿæˆæ¨¡å‹
- en: 'First, a little bit of background. The aim of (conditional) generative models
    is to learn how to generate data *y* from a target domain using information *x*
    from a source domain. Both domains can be images, text, semantic maps, audio,
    etc. Two types of modeling have become very successful: Generative Adversarial
    Networks (GANs) and Diffusion Probabilistic Models. Concretely,'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: é¦–å…ˆï¼Œæä¾›ä¸€äº›èƒŒæ™¯ã€‚æ¡ä»¶ç”Ÿæˆæ¨¡å‹çš„ç›®æ ‡æ˜¯å­¦ä¹ å¦‚ä½•ä»ç›®æ ‡é¢†åŸŸç”Ÿæˆæ•°æ® *y*ï¼Œåˆ©ç”¨æºé¢†åŸŸçš„ä¿¡æ¯ *x*ã€‚è¿™ä¸¤ä¸ªé¢†åŸŸå¯ä»¥æ˜¯å›¾åƒã€æ–‡æœ¬ã€è¯­ä¹‰å›¾ã€éŸ³é¢‘ç­‰ã€‚ä¸¤ç§å»ºæ¨¡æ–¹æ³•å·²å–å¾—å¾ˆå¤§æˆåŠŸï¼šç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANsï¼‰å’Œæ‰©æ•£æ¦‚ç‡æ¨¡å‹ã€‚å…·ä½“æ¥è¯´ï¼Œ
- en: GANs learn how to sample from the data distribution *p*(*y*âˆ£*x*) by training
    a generator model that produces data distributed according to *g*â€‹(*y*âˆ£*x*). It
    uses a discriminator model that guides the generator from blindly to accurately
    generating data by minimizing a divergence (or distance) between the distributions
    *g*â€‹ and *p*.
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GANs é€šè¿‡è®­ç»ƒç”Ÿæˆå™¨æ¨¡å‹æ¥å­¦ä¹ å¦‚ä½•ä»æ•°æ®åˆ†å¸ƒ *p*(*y*âˆ£*x*) ä¸­é‡‡æ ·ï¼Œç”Ÿæˆçš„æ•°æ®æŒ‰ç…§ *g*â€‹(*y*âˆ£*x*) åˆ†å¸ƒã€‚å®ƒä½¿ç”¨ä¸€ä¸ªé‰´åˆ«å™¨æ¨¡å‹ï¼ŒæŒ‡å¯¼ç”Ÿæˆå™¨ä»ç›²ç›®ç”Ÿæˆæ•°æ®åˆ°å‡†ç¡®ç”Ÿæˆæ•°æ®ï¼Œé€šè¿‡æœ€å°åŒ–
    *g*â€‹ å’Œ *p* ä¹‹é—´çš„æ•£åº¦ï¼ˆæˆ–è·ç¦»ï¼‰æ¥å®ç°ã€‚
- en: Diffusion models learn how to sample from *p*(*y*âˆ£*x*) by reducing the latent
    variables *y*â‚â€‹, *y*â‚‚â€‹, â‹¯, *y*â‚™â€‹ from *p*(*y*âˆ£*x*, *y*â‚â€‹â€‹, *y*â‚‚, â‹¯, *y*â‚™â€‹â€‹). These
    variables are a sequence of increasingly noisy versions of *y* (or an encoding
    of *y*), and the reduction is done by learning a denoising model.
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ‰©æ•£æ¨¡å‹é€šè¿‡å‡å°‘ä» *p*(*y*âˆ£*x*) ä¸­é‡‡æ ·çš„æ½œåœ¨å˜é‡ *y*â‚â€‹ã€*y*â‚‚â€‹ã€â‹¯ã€*y*â‚™â€‹ æ¥å­¦ä¹ ã€‚è¿™äº›å˜é‡æ˜¯ *y*ï¼ˆæˆ– *y* çš„ç¼–ç ï¼‰çš„ä¸€ç³»åˆ—é€æ¸å™ªå£°åŒ–çš„ç‰ˆæœ¬ï¼Œå‡å°‘çš„è¿‡ç¨‹æ˜¯é€šè¿‡å­¦ä¹ å»å™ªæ¨¡å‹æ¥å®Œæˆçš„ã€‚
- en: If you need more details about these types of modeling, there are countless
    sources available online. For GANs, you may want to start from [this article](/must-read-papers-on-gans-b665bbae3317)
    and, for Diffusion models, from [this one](https://lilianweng.github.io/posts/2021-07-11-diffusion-models/).
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœä½ éœ€è¦å…³äºè¿™äº›å»ºæ¨¡ç±»å‹çš„æ›´å¤šç»†èŠ‚ï¼Œç½‘ä¸Šæœ‰å¤§é‡èµ„æºå¯ä¾›æŸ¥é˜…ã€‚å…³äºGANsï¼Œä½ å¯ä»¥ä»[è¿™ç¯‡æ–‡ç« ](/must-read-papers-on-gans-b665bbae3317)å¼€å§‹ï¼Œå¯¹äºæ‰©æ•£æ¨¡å‹ï¼Œå¯ä»¥ä»[è¿™ç¯‡æ–‡ç« ](https://lilianweng.github.io/posts/2021-07-11-diffusion-models/)å¼€å§‹ã€‚
- en: '![](../Images/a1e76447e5b6b44d18afbf198d1576bc.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a1e76447e5b6b44d18afbf198d1576bc.png)'
- en: '*Figure 1: Visual ChatGPT* [*demo*](https://github.com/microsoft/TaskMatrix)
    *from Microsoft. Used with permission.*'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '*å›¾1ï¼šVisual ChatGPT* [*æ¼”ç¤º*](https://github.com/microsoft/TaskMatrix) *æ¥è‡ªå¾®è½¯ï¼Œå·²è·è®¸å¯ã€‚*'
- en: Now that we have set the basics, letâ€™s discuss some applications. Figure 1 shows
    the official Visual ChatGPT demo. It uses several models for vision-language interactions,
    some of which are listed in the table below.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬å·²ç»è®¾ç½®äº†åŸºç¡€ï¼Œè®©æˆ‘ä»¬è®¨è®ºä¸€äº›åº”ç”¨ã€‚å›¾1å±•ç¤ºäº†å®˜æ–¹çš„Visual ChatGPTæ¼”ç¤ºã€‚å®ƒä½¿ç”¨äº†å‡ ä¸ªç”¨äºè§†è§‰-è¯­è¨€äº¤äº’çš„æ¨¡å‹ï¼Œéƒ¨åˆ†æ¨¡å‹åˆ—åœ¨ä¸‹é¢çš„è¡¨æ ¼ä¸­ã€‚
- en: 'Table 1: Some of the VFM powering Visual ChatGPT. Inferred from the appendix
    in the [paper](https://arxiv.org/abs/2303.04671)Â¹.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: è¡¨1ï¼šä¸€äº›æ”¯æŒVisual ChatGPTçš„VFMã€‚ä¿¡æ¯æ¥æºäº[è®ºæ–‡](https://arxiv.org/abs/2303.04671)Â¹çš„é™„å½•ã€‚
- en: Most of these are generative models, with the majority being based on [Stable
    Diffusion](https://github.com/CompVis/stable-diffusion). This speaks about a recent
    switch of interest from GANs to Diffusion models, triggered by [evidence](https://papers.nips.cc/paper/2021/hash/49ad23d1ec9fa4bd8d77d02681df5cfa-Abstract.html)Â³
    that the latter are superior on image synthesis than the former. One take away
    from the present article is that this doesnâ€™t imply that Diffusion models are
    better than GANs for all image generation tasks, as the aggregation of these models
    tends to perform better than the independent parts.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­å¤§å¤šæ•°æ˜¯ç”Ÿæˆæ¨¡å‹ï¼Œå…¶ä¸­å¤§éƒ¨åˆ†åŸºäº[ç¨³å®šæ‰©æ•£](https://github.com/CompVis/stable-diffusion)ã€‚è¿™è¡¨æ˜äº†ä¸€ä¸ªè¿‘æœŸçš„å…´è¶£è½¬å˜ï¼Œä»
    GAN è½¬å‘æ‰©æ•£æ¨¡å‹ï¼Œè¿™ä¸€è½¬å˜ç”±[è¯æ®](https://papers.nips.cc/paper/2021/hash/49ad23d1ec9fa4bd8d77d02681df5cfa-Abstract.html)Â³
    è§¦å‘ï¼Œè¡¨æ˜åè€…åœ¨å›¾åƒåˆæˆæ–¹é¢ä¼˜äºå‰è€…ã€‚æœ¬æ–‡çš„ä¸€ä¸ªè¦ç‚¹æ˜¯ï¼Œè¿™å¹¶ä¸æ„å‘³ç€æ‰©æ•£æ¨¡å‹åœ¨æ‰€æœ‰å›¾åƒç”Ÿæˆä»»åŠ¡ä¸­éƒ½ä¼˜äº GANï¼Œå› ä¸ºè¿™äº›æ¨¡å‹çš„èšåˆå¾€å¾€æ¯”ç‹¬ç«‹éƒ¨åˆ†è¡¨ç°æ›´å¥½ã€‚
- en: Before discussing this and arriving to ATME, letâ€™s pave the way by revisiting
    the main weaknesses and strengths of GANs and Diffusion models.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è®¨è®ºè¿™ä¸€ç‚¹å¹¶åˆ°è¾¾ ATME ä¹‹å‰ï¼Œè®©æˆ‘ä»¬é€šè¿‡é‡æ–°å®¡è§† GAN å’Œæ‰©æ•£æ¨¡å‹çš„ä¸»è¦å¼±ç‚¹å’Œä¼˜åŠ¿æ¥é“ºå¹³é“è·¯ã€‚
- en: GANs
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: GANs
- en: The main premise introduced in the original GAN [paper](https://papers.nips.cc/paper_files/paper/2014/hash/5ca3e9b122f61f8f06494c97b1afccf3-Abstract.html)â´
    and emphasized in the [tutorial](https://arxiv.org/abs/1701.00160) is that, in
    the limit of a large enough model and infinite data, the minimax game played by
    the generator and discriminator converges to the Nash equilibrium, where the (vanilla)
    GAN objective achieves the value âˆ’log4\. In practice, however, this is hardly
    observed. The departures from this theoretical result give rise to what is popularly
    known as the training instability of GANs. This, together with mode collapse,
    are their main drawbacks. They are compensated by still high image generation
    quality achieved, in one shot, with lightweight models.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: åŸå§‹ GAN [è®ºæ–‡](https://papers.nips.cc/paper_files/paper/2014/hash/5ca3e9b122f61f8f06494c97b1afccf3-Abstract.html)â´
    ä¸­å¼•å…¥çš„ä¸»è¦å‰æï¼Œå¹¶åœ¨[æ•™ç¨‹](https://arxiv.org/abs/1701.00160)ä¸­å¼ºè°ƒçš„æ˜¯ï¼Œåœ¨ä¸€ä¸ªè¶³å¤Ÿå¤§çš„æ¨¡å‹å’Œæ— é™æ•°æ®çš„æé™æƒ…å†µä¸‹ï¼Œç”Ÿæˆå™¨å’Œåˆ¤åˆ«å™¨ä¹‹é—´çš„æå°æå¤§æ¸¸æˆä¼šæ”¶æ•›åˆ°çº³ä»€å‡è¡¡ï¼Œå…¶ä¸­ï¼ˆåŸå§‹ï¼‰GAN
    ç›®æ ‡è¾¾åˆ°å€¼ âˆ’log4ã€‚ç„¶è€Œï¼Œåœ¨å®è·µä¸­ï¼Œè¿™å‡ ä¹æ²¡æœ‰è§‚å¯Ÿåˆ°ã€‚è¿™ä¸€ç†è®ºç»“æœçš„åç¦»äº§ç”Ÿäº†å¹¿æ³›è¢«ç§°ä¸º GAN è®­ç»ƒä¸ç¨³å®šçš„é—®é¢˜ã€‚è¿™ä¸æ¨¡å¼å´©æºƒä¸€èµ·ï¼Œæ˜¯å…¶ä¸»è¦ç¼ºç‚¹ã€‚å®ƒä»¬é€šè¿‡è½»é‡çº§æ¨¡å‹ä¸€æ¬¡ç”Ÿæˆä»èƒ½è¾¾åˆ°è¾ƒé«˜çš„å›¾åƒç”Ÿæˆè´¨é‡æ¥å¼¥è¡¥è¿™ä¸€ç‚¹ã€‚
- en: Diffusion Models
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ‰©æ•£æ¨¡å‹
- en: In contrast, Diffusion models are stable but known to be inefficient due to
    the large number of steps required to learn the denoising distribution. This is
    the case because such a distribution is commonly assumed to be Gaussian, which
    is only justified in the infinitesimal limit of small denoising steps.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: ç›¸æ¯”ä¹‹ä¸‹ï¼Œæ‰©æ•£æ¨¡å‹è™½ç„¶ç¨³å®šï¼Œä½†ç”±äºå­¦ä¹ å»å™ªåˆ†å¸ƒæ‰€éœ€çš„æ­¥éª¤è¿‡å¤šè€Œæ•ˆç‡è¾ƒä½ã€‚è¿™æ˜¯å› ä¸ºè¿™ç§åˆ†å¸ƒé€šå¸¸å‡è®¾ä¸ºé«˜æ–¯åˆ†å¸ƒï¼Œè¿™åªæœ‰åœ¨å»å™ªæ­¥éª¤éå¸¸å°æ—¶æ‰æ˜¯åˆç†çš„ã€‚
- en: Recently developed alternatives to reduce the number of denoising steps (even
    further down to 2), using multi-modal distributions, exist. This requires combining
    Diffusion models with GANs, as we discuss in the following.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€è¿‘å¼€å‘çš„æ›¿ä»£æ–¹æ³•é€šè¿‡ä½¿ç”¨å¤šæ¨¡æ€åˆ†å¸ƒæ¥å‡å°‘å»å™ªæ­¥éª¤çš„æ•°é‡ï¼ˆç”šè‡³å‡å°‘åˆ° 2 æ­¥ï¼‰ï¼Œè¿™éœ€è¦å°†æ‰©æ•£æ¨¡å‹ä¸ GAN ç»“åˆï¼Œå¦‚æˆ‘ä»¬åœ¨æ¥ä¸‹æ¥çš„è®¨è®ºä¸­æ‰€è¿°ã€‚
- en: GAN âˆª Diffusion
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: GAN âˆª æ‰©æ•£
- en: Current approaches to train GANs with Diffusion are very promising. They can
    be classified as belonging to the GAN âˆª Diffusion class of models that use generative
    adversarial training together with **multi-step** diffusion processes.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: ç›®å‰çš„ GAN ä¸æ‰©æ•£è®­ç»ƒæ–¹æ³•éå¸¸æœ‰å‰æ™¯ã€‚å®ƒä»¬å¯ä»¥è¢«å½’ç±»ä¸ºä½¿ç”¨ç”Ÿæˆå¯¹æŠ—è®­ç»ƒä¸**å¤šæ­¥éª¤**æ‰©æ•£è¿‡ç¨‹ç›¸ç»“åˆçš„ GAN âˆª æ‰©æ•£æ¨¡å‹ç±»ã€‚
- en: In order to improve training stability and mode coverage in the GANs, these
    models inject instance noise by following diffusion processes which may have from
    up to thousands of steps (as in [*Diffusion-GAN*](https://github.com/Zhendong-Wang/Diffusion-GAN)âµ)
    to as few as two steps (as in [*Denoising Diffusion GANs*](https://nvlabs.github.io/denoising-diffusion-gan/)â¶).
    These models perform better than strong GAN baselines on various datasets, but
    still need multiple denoising steps. So,
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†æ”¹å–„ GAN çš„è®­ç»ƒç¨³å®šæ€§å’Œæ¨¡å¼è¦†ç›–ï¼Œè¿™äº›æ¨¡å‹é€šè¿‡éµå¾ªæ‰©æ•£è¿‡ç¨‹æ³¨å…¥å®ä¾‹å™ªå£°ï¼Œè¿™äº›è¿‡ç¨‹å¯èƒ½æœ‰å¤šè¾¾å‡ åƒä¸ªæ­¥éª¤ï¼ˆå¦‚åœ¨ [*Diffusion-GAN*](https://github.com/Zhendong-Wang/Diffusion-GAN)âµ
    ä¸­ï¼‰ï¼Œä¹Ÿå¯èƒ½åªæœ‰ä¸¤ä¸ªæ­¥éª¤ï¼ˆå¦‚åœ¨ [*å»å™ªæ‰©æ•£ GAN*](https://nvlabs.github.io/denoising-diffusion-gan/)â¶
    ä¸­ï¼‰ã€‚è¿™äº›æ¨¡å‹åœ¨å„ç§æ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºå¼º GAN åŸºçº¿ï¼Œä½†ä»éœ€å¤šä¸ªå»å™ªæ­¥éª¤ã€‚å› æ­¤ï¼Œ
- en: '*is it possible to generate images with a GAN in one shot and still leverage
    denoising diffusion processes?*'
  id: totrans-30
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*æ˜¯å¦æœ‰å¯èƒ½åœ¨ä¸€æ¬¡ç”Ÿæˆä¸­ä½¿ç”¨ GAN åˆ›å»ºå›¾åƒï¼Œå¹¶åŒæ—¶åˆ©ç”¨å»å™ªæ‰©æ•£è¿‡ç¨‹ï¼Ÿ*'
- en: The answer is yes, and this defines the GAN âˆ© Diffusion class of models.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: ç­”æ¡ˆæ˜¯è‚¯å®šçš„ï¼Œè¿™å®šä¹‰äº† GAN âˆ© æ‰©æ•£ç±»æ¨¡å‹ã€‚
- en: GAN âˆ© Diffusion
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: GAN âˆ© æ‰©æ•£
- en: 'It turns out that a single trick can make the [pix2pix](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix)â·
    visual foundation GAN model stable by design: *paying attention to the discriminator
    mean entropy*.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: ç»“æœæ˜¯ï¼Œå•ä¸ªæŠ€å·§å¯ä»¥ä½¿ [pix2pix](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix)â·
    è§†è§‰åŸºç¡€GANæ¨¡å‹åœ¨è®¾è®¡ä¸Šç¨³å®šï¼š*å…³æ³¨åˆ¤åˆ«å™¨å‡å€¼ç†µ*ã€‚
- en: '![](../Images/f7ae64932bbe981be9858f7315f79c53.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f7ae64932bbe981be9858f7315f79c53.png)'
- en: '*Figure 2: ATME generates images using the UNet from Diffusion models, which
    are judged by the patchGAN discriminator from pix2pix. Image by author.*'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '*å›¾2ï¼šATME ä½¿ç”¨æ¥è‡ªæ‰©æ•£æ¨¡å‹çš„ UNet ç”Ÿæˆå›¾åƒï¼Œè¿™äº›å›¾åƒç”±æ¥è‡ª pix2pix çš„ patchGAN åˆ¤åˆ«å™¨è¿›è¡Œåˆ¤æ–­ã€‚å›¾ç‰‡ç”±ä½œè€…æä¾›ã€‚*'
- en: The resulting model, [ATME](https://github.com/DLR-MI/atme), is shown in Figure
    2\. Given a joint distribution *p*(*x*,*y*) of source and target images, the input
    image *x* at epoch *t* is corrupted with *Wâ‚œ* â€‹=*W*(*Dâ‚œ-*), as follows
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: ç»“æœæ¨¡å‹ï¼Œ[ATME](https://github.com/DLR-MI/atme)ï¼Œå¦‚å›¾2æ‰€ç¤ºã€‚ç»™å®šæºå›¾åƒå’Œç›®æ ‡å›¾åƒçš„è”åˆåˆ†å¸ƒ *p*(*x*,*y*)ï¼Œè¾“å…¥å›¾åƒ
    *x* åœ¨æ—¶ä»£ *t* è¢« *Wâ‚œ* = *W*(*Dâ‚œ-*ï¼‰æ‰€æŸåï¼Œå¦‚ä¸‹æ‰€ç¤ºã€‚
- en: '*xâ‚œ*â€‹ = *x* (1+*Wâ‚œ*â€‹)'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '*xâ‚œ* = *x* (1+*Wâ‚œ*)'
- en: with *W* being a small deterministic net which transforms into *Wâ‚œ*â€‹ the discriminator
    decision map *Dâ‚œ-*â€‹ at the previous epoch ğ‘¡- *= t-1*. The transformed map *Wâ‚œ*
    contains patterns related to patches of the input space where the generator previously
    failed to cheat the discriminator as well as noise coming from the discriminator
    not yet being fully optimized and therefore erring on its decisions.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­ *W* æ˜¯ä¸€ä¸ªå°å‹ç¡®å®šæ€§ç½‘ç»œï¼Œå®ƒå°†å‰ä¸€æ—¶ä»£ ğ‘¡- = *t-1* çš„åˆ¤åˆ«å™¨å†³ç­–å›¾ *Dâ‚œ-*â€‹ è½¬æ¢ä¸º *Wâ‚œ*ã€‚è½¬æ¢åçš„å›¾ *Wâ‚œ* åŒ…å«ä¸è¾“å…¥ç©ºé—´ä¸­çš„è¡¥ä¸ç›¸å…³çš„æ¨¡å¼ï¼Œè¿™äº›è¡¥ä¸æ˜¯ç”Ÿæˆå™¨ä¹‹å‰æœªèƒ½æ¬ºéª—åˆ¤åˆ«å™¨çš„åŒºåŸŸï¼Œä»¥åŠæ¥è‡ªåˆ¤åˆ«å™¨çš„å™ªå£°ï¼Œåˆ¤åˆ«å™¨å°šæœªå®Œå…¨ä¼˜åŒ–ï¼Œå› æ­¤åœ¨å†³ç­–ä¸Šå‡ºç°é”™è¯¯ã€‚
- en: The generator sees corrupted source images and generates target images according
    to ğ‘¦Ì‚â€‹ = *y*áµ©(*xâ‚œ*â€‹, ğ‘¡Ìƒ), using the net *y*áµ©â€‹ for [denoising diffusion probabilistic
    models](https://hojonathanho.github.io/diffusion/), which have suitable attention
    mechanisms. The task of the generator is then to make *x* âŠ• ğ‘¦Ì‚â€‹â€‹ look indistinguishable
    from *x* âŠ• *y* to the discriminator (where âŠ• denotes concatenation). In doing
    so, it learns to remove the injected noise in the input images. Remarkably, the
    denoising happens along a â€œtime axisâ€ unfold over the course of the training epochs,
    unlike Diffusion models which require an independent time axis within an epoch.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: ç”Ÿæˆå™¨çœ‹åˆ°æŸåçš„æºå›¾åƒï¼Œå¹¶æ ¹æ® ğ‘¦Ì‚â€‹ = *y*áµ©(*xâ‚œ*â€‹, ğ‘¡Ìƒ) ç”Ÿæˆç›®æ ‡å›¾åƒï¼Œä½¿ç”¨ç”¨äº [å»å™ªæ‰©æ•£æ¦‚ç‡æ¨¡å‹](https://hojonathanho.github.io/diffusion/)
    çš„ç½‘ *y*áµ©â€‹ï¼Œè¿™äº›æ¨¡å‹å…·æœ‰é€‚å½“çš„æ³¨æ„æœºåˆ¶ã€‚ç”Ÿæˆå™¨çš„ä»»åŠ¡æ˜¯ä½¿ *x* âŠ• ğ‘¦Ì‚â€‹â€‹ çœ‹èµ·æ¥ä¸ *x* âŠ• *y* å¯¹åˆ¤åˆ«å™¨ä¸å¯åŒºåˆ†ï¼ˆå…¶ä¸­ âŠ• è¡¨ç¤ºè¿æ¥ï¼‰ã€‚é€šè¿‡è¿™æ ·åšï¼Œå®ƒå­¦ä¼šäº†å»é™¤è¾“å…¥å›¾åƒä¸­çš„æ³¨å…¥å™ªå£°ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œå»å™ªå‘ç”Ÿåœ¨ä¸€ä¸ªæ²¿ç€â€œæ—¶é—´è½´â€çš„å±•å¼€è¿‡ç¨‹ä¸­ï¼Œè€Œä¸æ˜¯åƒæ‰©æ•£æ¨¡å‹é‚£æ ·åœ¨æ¯ä¸ªæ—¶ä»£å†…éœ€è¦ä¸€ä¸ªç‹¬ç«‹çš„æ—¶é—´è½´ã€‚
- en: Flattening of the signals in *Wâ‚œ*â€‹, as a consequence of denoising the input
    images to the generator, translates to a flat distribution for all the entries
    of *Dâ‚œ*â€‹. This is precisely the Nash equilibrium, with the discriminator being
    in a maximum entropy state.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '*Wâ‚œ* ä¿¡å·çš„å±•å¹³ï¼Œä½œä¸ºå°†è¾“å…¥å›¾åƒå»å™ªåˆ°ç”Ÿæˆå™¨çš„ç»“æœï¼Œè½¬åŒ–ä¸º *Dâ‚œ* æ‰€æœ‰æ¡ç›®çš„å¹³å¦åˆ†å¸ƒã€‚è¿™æ­£æ˜¯çº³ä»€å‡è¡¡ï¼Œåˆ¤åˆ«å™¨å¤„äºæœ€å¤§ç†µçŠ¶æ€ã€‚'
- en: â€¦ and where is the diffusion?
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: â€¦ æ‰©æ•£åœ¨å“ªé‡Œï¼Ÿ
- en: The evolution of a corrupted input image, over the course of the training epochs,
    can be written as
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: è¢«æŸåçš„è¾“å…¥å›¾åƒåœ¨è®­ç»ƒæ—¶æœŸçš„æ¼”å˜å¯ä»¥å†™ä½œ
- en: '*dxâ‚œ*â€‹â€‹= *x dWâ‚œ*â€‹'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '*dxâ‚œ* = *x dWâ‚œ* '
- en: which may be seen as a finite-difference instance of the more general [SDE](https://en.wikipedia.org/wiki/Stochastic_differential_equation)
    for diffusion processes *dxâ‚œ*â€‹ = *Î¼*(*xâ‚œ*â€‹â€‹,*t*) *dt* + *Ïƒ*(*xâ‚œ*â€‹â€‹,*t*) *dWâ‚œ*â€‹,
    provided that *Wâ‚œ*â€‹ is a Wiener process (aka standard Brownian motion).
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å¯ä»¥è§†ä¸ºæ›´ä¸€èˆ¬çš„ [SDE](https://en.wikipedia.org/wiki/Stochastic_differential_equation)
    æ‰©æ•£è¿‡ç¨‹çš„æœ‰é™å·®åˆ†å®ä¾‹ï¼Œå³ *dxâ‚œ* = *Î¼*(*xâ‚œ*â€‹,*t*) *dt* + *Ïƒ*(*xâ‚œ*â€‹,*t*) *dWâ‚œ*ï¼Œå‰ææ˜¯ *Wâ‚œ* æ˜¯ä¸€ä¸ªç»´çº³è¿‡ç¨‹ï¼ˆä¹Ÿç§°æ ‡å‡†å¸ƒæœ—è¿åŠ¨ï¼‰ã€‚
- en: In ATME, there is no design choice to make *W* produce Wiener processes. This
    rather happens naturally in a significant number of cases, as can be seen from
    Figure 3, by analyzing the properties of the time series *dWâ‚œ*â€‹â€‹ for 5 randomly
    selected pixels of 5 randomly selected images during training on the [Maps](http://efrosgans.eecs.berkeley.edu/pix2pix/datasets/)
    dataset.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ ATME ä¸­ï¼Œæ²¡æœ‰è®¾è®¡é€‰æ‹©ä½¿ *W* äº§ç”Ÿç»´çº³è¿‡ç¨‹ã€‚è¿™åœ¨å¤§é‡æƒ…å†µä¸‹è‡ªç„¶å‘ç”Ÿï¼Œä»å›¾3ä¸­å¯ä»¥çœ‹å‡ºï¼Œé€šè¿‡åˆ†æåœ¨ [Maps](http://efrosgans.eecs.berkeley.edu/pix2pix/datasets/)
    æ•°æ®é›†ä¸Šè®­ç»ƒè¿‡ç¨‹ä¸­ 5 ä¸ªéšæœºé€‰æ‹©çš„å›¾åƒçš„ 5 ä¸ªéšæœºé€‰æ‹©åƒç´ çš„æ—¶é—´åºåˆ— *dWâ‚œ*â€‹â€‹ çš„å±æ€§ã€‚
- en: First, a Wiener process is stationary. This is tested using the Augmented Dickey-Fuller
    test, whose *p* values for the resulting test statistics are all way less than
    0.01, so the null hypotheses of having non-stationary time series are all rejected.
    Second, a Wiener process is a Markov process and then the autocorrelation function
    of all *dWâ‚œ* should vanish at all lags. This is appreciable from Figure 3 at the
    99% confidence level.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: é¦–å…ˆï¼Œç»´çº³è¿‡ç¨‹æ˜¯å¹³ç¨³çš„ã€‚è¿™æ˜¯ä½¿ç”¨æ‰©å±•çš„è¿ªåŸº-å¯Œå‹’æ£€éªŒè¿›è¡Œæµ‹è¯•çš„ï¼Œå…¶ç»“æœç»Ÿè®¡é‡çš„ *p* å€¼éƒ½è¿œä½äº 0.01ï¼Œå› æ­¤æ‰€æœ‰éå¹³ç¨³æ—¶é—´åºåˆ—çš„åŸå‡è®¾éƒ½è¢«æ‹’ç»ã€‚å…¶æ¬¡ï¼Œç»´çº³è¿‡ç¨‹æ˜¯é©¬å°”å¯å¤«è¿‡ç¨‹ï¼Œå› æ­¤æ‰€æœ‰
    *dWâ‚œ* çš„è‡ªç›¸å…³å‡½æ•°åœ¨æ‰€æœ‰æ»åå¤„åº”å½“æ¶ˆå¤±ã€‚ä»å›¾ 3 ä¸­ä»¥ 99% çš„ç½®ä¿¡æ°´å¹³å¯ä»¥æ˜æ˜¾çœ‹å‡ºè¿™ä¸€ç‚¹ã€‚
- en: '![](../Images/1658f1f8ea0c94d17192b2e69e17e379.png)![](../Images/3aa843f74e35fd69f6d5b56e81e6fe27.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1658f1f8ea0c94d17192b2e69e17e379.png)![](../Images/3aa843f74e35fd69f6d5b56e81e6fe27.png)'
- en: '*Figure 3: Time series of* dW *for selected images and pixels (top) and corresponding
    autocorrelation functions (bottom). Image by author.*'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '*å›¾ 3: é€‰å®šå›¾åƒå’Œåƒç´ çš„ dW æ—¶é—´åºåˆ— (ä¸Š) å’Œç›¸åº”çš„è‡ªç›¸å…³å‡½æ•° (ä¸‹)ã€‚å›¾åƒä½œè€…æä¾›ã€‚*'
- en: Finally, a Wiener process has Gaussian *dWâ‚œ*â€‹. This is tested using the Shapiro-Wilk
    test, giving (in 64% of the cases) *p* values for the test statistics greater
    than 0.01, so the null hypotheses that the series are drawn from a normal distribution
    cannot be rejected in a significant number of cases.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åï¼Œç»´çº³è¿‡ç¨‹å…·æœ‰é«˜æ–¯ *dWâ‚œ*ã€‚è¿™æ˜¯ä½¿ç”¨ Shapiro-Wilk æ£€éªŒè¿›è¡Œçš„æµ‹è¯•ï¼ˆåœ¨ 64% çš„æƒ…å†µä¸‹ï¼‰ï¼Œæµ‹è¯•ç»Ÿè®¡é‡çš„ *p* å€¼å¤§äº 0.01ï¼Œå› æ­¤ä¸èƒ½åœ¨ç›¸å½“å¤šçš„æƒ…å†µä¸‹æ‹’ç»ç³»åˆ—æ¥è‡ªæ­£æ€åˆ†å¸ƒçš„åŸå‡è®¾ã€‚
- en: '![](../Images/72206bd7896a545500abfddeabfec815.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/72206bd7896a545500abfddeabfec815.png)'
- en: '*Figure 4: (Rescaled) discriminator decision map (left) with its associated
    representation (center) and change (right) in the space of the source images.
    Image by author.*'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '*å›¾ 4: (é‡ç¼©æ”¾) åˆ¤åˆ«å™¨å†³ç­–å›¾ (å·¦) åŠå…¶ç›¸å…³è¡¨ç¤º (ä¸­) å’Œåœ¨æºå›¾åƒç©ºé—´ä¸­çš„å˜åŒ– (å³)ã€‚å›¾åƒä½œè€…æä¾›ã€‚*'
- en: As the epochs are traversed by the model, the entries of *Dâ‚œ*â€‹â€‹ tends to a flat
    distribution, as observed from Figure 4 for one of the images processed in Figure
    3\. A Jupyter notebook with these tests may be found in ATMEâ€™s GitHub [repository](https://github.com/DLR-MI/atme).
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: éšç€æ¨¡å‹éå†çš„è½®æ¬¡å¢åŠ ï¼Œ*Dâ‚œ* çš„æ¡ç›®è¶‹å‘äºå¹³å¦åˆ†å¸ƒï¼Œä»å›¾ 4 ä¸­è§‚å¯Ÿåˆ°è¿™ä¸€ç‚¹ï¼Œå¯¹åº”äºå›¾ 3 ä¸­å¤„ç†çš„ä¸€å¼ å›¾åƒã€‚å¯ä»¥åœ¨ ATME çš„ GitHub [ä»£ç åº“](https://github.com/DLR-MI/atme)
    ä¸­æ‰¾åˆ°åŒ…å«è¿™äº›æµ‹è¯•çš„ Jupyter notebookã€‚
- en: how to check for stability?
  id: totrans-53
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å¦‚ä½•æ£€æŸ¥ç¨³å®šæ€§ï¼Ÿ
- en: Stability means that, regardless of dataset and initialization of model weights,
    the GAN objective converges towards the same value (âˆ’log4 for a vanilla GAN),
    provided that the conditions of enough data and model capacity are met. This is
    typically the case for ATME, as observed in Figure 5, with more examples found
    in the [paper](https://arxiv.org/abs/2304.09024)Â². Other popular GANs fail to
    achieve this, a reason why you might have heard of GAN models being â€œdifficult
    to trainâ€.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: ç¨³å®šæ€§æ„å‘³ç€ï¼Œæ— è®ºæ•°æ®é›†å’Œæ¨¡å‹æƒé‡çš„åˆå§‹åŒ–å¦‚ä½•ï¼Œåªè¦æ»¡è¶³è¶³å¤Ÿçš„æ•°æ®å’Œæ¨¡å‹å®¹é‡æ¡ä»¶ï¼ŒGAN çš„ç›®æ ‡å°±ä¼šæ”¶æ•›åˆ°ç›¸åŒçš„å€¼ï¼ˆå¯¹äºæ™®é€š GAN ä¸º -log4ï¼‰ã€‚åœ¨
    ATME ä¸­é€šå¸¸æ˜¯è¿™ç§æƒ…å†µï¼Œæ­£å¦‚å›¾ 5 æ‰€è§‚å¯Ÿåˆ°çš„ï¼Œæ›´å¤šçš„ç¤ºä¾‹å¯ä»¥åœ¨ [è®ºæ–‡](https://arxiv.org/abs/2304.09024)Â² ä¸­æ‰¾åˆ°ã€‚å…¶ä»–æµè¡Œçš„
    GAN æ— æ³•å®ç°è¿™ä¸€ç‚¹ï¼Œè¿™ä¹Ÿæ˜¯ä½ å¯èƒ½å¬è¯´è¿‡ GAN æ¨¡å‹â€œéš¾ä»¥è®­ç»ƒâ€çš„åŸå› ã€‚
- en: '![](../Images/c16b9051205cec6c7625ef7649c037b6.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c16b9051205cec6c7625ef7649c037b6.png)'
- en: '*Figure 5: The GAN objective in ATME tends to the theoretical value at Nash
    equilibrium. Image by author.*'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '*å›¾ 5: ATME ä¸­çš„ GAN ç›®æ ‡è¶‹å‘äºçº³ä»€å‡è¡¡çš„ç†è®ºå€¼ã€‚å›¾åƒä½œè€…æä¾›ã€‚*'
- en: By attending the discriminator mean entropy, the denoising procedure in ATME
    is designed to stably take the GAN towards the maximum entropy equilibrium state.
    Isnâ€™t this awesome?
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: é€šè¿‡å…³æ³¨åˆ¤åˆ«å™¨çš„å‡å€¼ç†µï¼ŒATME çš„å»å™ªç¨‹åºè®¾è®¡ä¸ºç¨³å®šåœ°å°† GAN å¸¦åˆ°æœ€å¤§ç†µå¹³è¡¡çŠ¶æ€ã€‚è¿™æ˜¯ä¸æ˜¯å¾ˆæ£’ï¼Ÿ
- en: Closing remarks
  id: totrans-58
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç»“è¯­
- en: ATME produces state-of-the-art results on supervised image-to-image translation
    at a lesser cost than popular GANs and Latent Diffusion. If you like Physics,
    you might find interesting how the ideas leading to ATME relate to the violation
    of the second law of thermodynamics by Maxwellâ€™s demon. You can find this and
    more in the [paper](https://arxiv.org/abs/2304.09024)Â².
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: ATME åœ¨æœ‰ç›‘ç£çš„å›¾åƒåˆ°å›¾åƒç¿»è¯‘ä¸­å–å¾—äº†æœ€å…ˆè¿›çš„ç»“æœï¼Œæˆæœ¬ä½äºæµè¡Œçš„ GAN å’Œæ½œåœ¨æ‰©æ•£ã€‚å¦‚æœä½ å–œæ¬¢ç‰©ç†å­¦ï¼Œä½ å¯èƒ½ä¼šå¯¹ ATME çš„æ€æƒ³å¦‚ä½•ä¸éº¦å…‹æ–¯éŸ¦å¦–å¯¹ç¬¬äºŒçƒ­åŠ›å­¦å®šå¾‹çš„è¿åç›¸å…³æ„Ÿå…´è¶£ã€‚ä½ å¯ä»¥åœ¨
    [è®ºæ–‡](https://arxiv.org/abs/2304.09024)Â² ä¸­æ‰¾åˆ°è¿™äº›å†…å®¹åŠæ›´å¤šä¿¡æ¯ã€‚
- en: '![](../Images/e59b922b5673c856a8bfee0f3c4bf41a.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e59b922b5673c856a8bfee0f3c4bf41a.png)'
- en: '*Maxwellâ€™s demon (Source:* [*astrogewgaw*](https://astrogewgaw.com/post/demons/)*).
    Used with permission.*'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '*éº¦å…‹æ–¯éŸ¦å¦– (æ¥æº:* [*astrogewgaw*](https://astrogewgaw.com/post/demons/)*). å·²è·å¾—è®¸å¯ã€‚*'
- en: The way in which technology is advancing is impressive and encouraging. Looking
    forward to seeing what the marriage of GANs with Diffusion models still promises
    to bring.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: æŠ€æœ¯çš„è¿›æ­¥æ–¹å¼ä»¤äººå°è±¡æ·±åˆ»ä¸”é¼“èˆäººå¿ƒã€‚æœŸå¾…çœ‹åˆ° GAN å’Œæ‰©æ•£æ¨¡å‹ç»“åˆåè¿˜ä¼šå¸¦æ¥ä»€ä¹ˆæ–°è¿›å±•ã€‚
- en: Thatâ€™s it for now! I hope you enjoyed reading as I did writing ğŸ˜‰
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: ç›®å‰å°±æ˜¯è¿™äº›ï¼æˆ‘å¸Œæœ›ä½ å’Œæˆ‘ä¸€æ ·å–œæ¬¢é˜…è¯»è¿™ç¯‡æ–‡ç« ğŸ˜‰
- en: '[1] Chenfei Wu *et al*, Visual ChatGPT: Talking, Drawing and Editing with Visual
    Foundation Models, arXiv 2303.04671 (2023).'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] Chenfei Wu *ç­‰*ï¼Œã€Šè§†è§‰ ChatGPTï¼šä¸è§†è§‰åŸºç¡€æ¨¡å‹å¯¹è¯ã€ç»˜å›¾å’Œç¼–è¾‘ã€‹ï¼ŒarXiv 2303.04671ï¼ˆ2023ï¼‰ã€‚'
- en: '[2] Edgardo Solano-Carrillo *et al*, Look ATME: The Discriminator Mean Entropy
    Needs Attention, arXiv 2304.09024 (2023).'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] Edgardo Solano-Carrillo *ç­‰*ï¼Œã€ŠLook ATMEï¼šåˆ¤åˆ«å™¨å‡å€¼ç†µéœ€è¦å…³æ³¨ã€‹ï¼ŒarXiv 2304.09024ï¼ˆ2023ï¼‰ã€‚'
- en: '[3] Prafulla Dhariwal, Alexander Nichol, Diffusion Models Beat GANs on Image
    Synthesis, Advances in Neural Information Processing Systems 34 (NeurIPS 2021)'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] Prafulla Dhariwal, Alexander Nicholï¼Œã€Šæ‰©æ•£æ¨¡å‹åœ¨å›¾åƒåˆæˆä¸­è¶…è¶Š GANã€‹ï¼Œç¥ç»ä¿¡æ¯å¤„ç†ç³»ç»Ÿè¿›å±• 34ï¼ˆNeurIPS
    2021ï¼‰ã€‚'
- en: '[4]Ian Goodfellow *et al*, Generative Adversarial Nets, Advances in Neural
    Information Processing Systems 27 (NIPS 2014).'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] Ian Goodfellow *ç­‰*ï¼Œã€Šç”Ÿæˆå¯¹æŠ—ç½‘ç»œã€‹ï¼Œç¥ç»ä¿¡æ¯å¤„ç†ç³»ç»Ÿè¿›å±• 27ï¼ˆNIPS 2014ï¼‰ã€‚'
- en: '[5] Zhendong Wang *et al*, Diffusion-GAN: Training GANs with Diffusion, arXiv
    2303.04671(2022).'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '[5] Zhendong Wang *ç­‰*ï¼Œã€Šæ‰©æ•£-GANï¼šç”¨æ‰©æ•£è®­ç»ƒ GANã€‹ï¼ŒarXiv 2303.04671ï¼ˆ2022ï¼‰ã€‚'
- en: '[6] Zhisheng Xiao, Karsten Kreis, Arash Vahdat, Tackling the Generative Learning
    Trilemma with Denoising Diffusion GANs, International Conference on Learning Representations
    (ICLR 2022).'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '[6] Zhisheng Xiao, Karsten Kreis, Arash Vahdatï¼Œã€Šä½¿ç”¨å»å™ªæ‰©æ•£ GAN è§£å†³ç”Ÿæˆå­¦ä¹ ä¸‰éš¾é—®é¢˜ã€‹ï¼Œå›½é™…å­¦ä¹ è¡¨ç¤ºå¤§ä¼šï¼ˆICLR
    2022ï¼‰ã€‚'
- en: '[7] Phillip Isola *et al*, Image-to-Image Translation with Conditional Adversarial
    Networks, Computer Vision and Pattern Recognition (CVPR 2017).'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '[7] Phillip Isola *ç­‰*ï¼Œã€ŠåŸºäºæ¡ä»¶å¯¹æŠ—ç½‘ç»œçš„å›¾åƒåˆ°å›¾åƒç¿»è¯‘ã€‹ï¼Œè®¡ç®—æœºè§†è§‰ä¸æ¨¡å¼è¯†åˆ«ä¼šè®®ï¼ˆCVPR 2017ï¼‰ã€‚'
