- en: Boosting image generation by intersecting GANs with Diffusion models
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通过将GAN与扩散模型交叉提升图像生成
- en: 原文：[https://towardsdatascience.com/boosting-image-generation-by-intersecting-gans-with-diffusion-models-6a22f935b0f3?source=collection_archive---------9-----------------------#2023-05-09](https://towardsdatascience.com/boosting-image-generation-by-intersecting-gans-with-diffusion-models-6a22f935b0f3?source=collection_archive---------9-----------------------#2023-05-09)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/boosting-image-generation-by-intersecting-gans-with-diffusion-models-6a22f935b0f3?source=collection_archive---------9-----------------------#2023-05-09](https://towardsdatascience.com/boosting-image-generation-by-intersecting-gans-with-diffusion-models-6a22f935b0f3?source=collection_archive---------9-----------------------#2023-05-09)
- en: A recipe for stable and efficient image-to-image translation
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 一种稳定高效的图像到图像转换方法
- en: '[](https://medium.com/@essolanoc?source=post_page-----6a22f935b0f3--------------------------------)[![Edgardo
    Solano Carrillo](../Images/e160dddf2cdcfc793c0e2cd81bfb5b61.png)](https://medium.com/@essolanoc?source=post_page-----6a22f935b0f3--------------------------------)[](https://towardsdatascience.com/?source=post_page-----6a22f935b0f3--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----6a22f935b0f3--------------------------------)
    [Edgardo Solano Carrillo](https://medium.com/@essolanoc?source=post_page-----6a22f935b0f3--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@essolanoc?source=post_page-----6a22f935b0f3--------------------------------)[![Edgardo
    Solano Carrillo](../Images/e160dddf2cdcfc793c0e2cd81bfb5b61.png)](https://medium.com/@essolanoc?source=post_page-----6a22f935b0f3--------------------------------)[](https://towardsdatascience.com/?source=post_page-----6a22f935b0f3--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----6a22f935b0f3--------------------------------)
    [Edgardo Solano Carrillo](https://medium.com/@essolanoc?source=post_page-----6a22f935b0f3--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fdb88c072b0d3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fboosting-image-generation-by-intersecting-gans-with-diffusion-models-6a22f935b0f3&user=Edgardo+Solano+Carrillo&userId=db88c072b0d3&source=post_page-db88c072b0d3----6a22f935b0f3---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----6a22f935b0f3--------------------------------)
    ·8 min read·May 9, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F6a22f935b0f3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fboosting-image-generation-by-intersecting-gans-with-diffusion-models-6a22f935b0f3&user=Edgardo+Solano+Carrillo&userId=db88c072b0d3&source=-----6a22f935b0f3---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fdb88c072b0d3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fboosting-image-generation-by-intersecting-gans-with-diffusion-models-6a22f935b0f3&user=Edgardo+Solano+Carrillo&userId=db88c072b0d3&source=post_page-db88c072b0d3----6a22f935b0f3---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----6a22f935b0f3--------------------------------)
    ·8分钟阅读·2023年5月9日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F6a22f935b0f3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fboosting-image-generation-by-intersecting-gans-with-diffusion-models-6a22f935b0f3&user=Edgardo+Solano+Carrillo&userId=db88c072b0d3&source=-----6a22f935b0f3---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F6a22f935b0f3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fboosting-image-generation-by-intersecting-gans-with-diffusion-models-6a22f935b0f3&source=-----6a22f935b0f3---------------------bookmark_footer-----------)![](../Images/3424e2a8c94a60eb2f2cee483a0cfb08.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F6a22f935b0f3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fboosting-image-generation-by-intersecting-gans-with-diffusion-models-6a22f935b0f3&source=-----6a22f935b0f3---------------------bookmark_footer-----------)![](../Images/3424e2a8c94a60eb2f2cee483a0cfb08.png)'
- en: ATME is a model in the GAN ∩ Diffusion class. Image generated using DALL·E 2.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: ATME是GAN ∩ 扩散模型类中的一个模型。图像由DALL·E 2生成。
- en: 'Visual Foundation Models (VFM) are at the core of cutting-edge technologies
    such as [Visual ChatGPT](https://arxiv.org/abs/2303.04671)¹. In this article,
    we will briefly discuss recent advances to blend two important ingredients of
    the VFM soup: GANs and Diffusion models, ending up in ATME at their intersection.
    ATME is a novel model that I introduced in the paper [Look ATME: The Discriminator
    Mean Entropy Needs Attention](https://arxiv.org/abs/2304.09024)², with GitHub
    repository available [here](https://github.com/dlr-mi/atme).'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '视觉基础模型（VFM）是如[Visual ChatGPT](https://arxiv.org/abs/2303.04671)¹等前沿技术的核心。在本文中，我们将简要讨论最近的进展，融合VFM“汤”的两个重要成分：GANs和扩散模型，最终达到它们的交集ATME。ATME是我在论文[Look
    ATME: The Discriminator Mean Entropy Needs Attention](https://arxiv.org/abs/2304.09024)²中介绍的一种新模型，GitHub仓库可在[此处](https://github.com/dlr-mi/atme)找到。'
- en: 'We will go through relevant weaknesses and strengths of each type of generative
    modeling. Then we discuss two categories of solutions to merge them: the naive
    GAN ∪ Diffusion and, in more depth, the efficient GAN ∩ Diffusion classes of models.
    At the end, you will get a picture of how research around some of the VFM is currently
    evolving.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将讨论每种生成模型的相关弱点和优势。然后我们讨论合并它们的两类解决方案：朴素的GAN ∪ Diffusion 和更深入的高效的GAN ∩ Diffusion模型类别。最后，你将了解一些VFM研究目前如何发展。
- en: Generative models
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 生成模型
- en: 'First, a little bit of background. The aim of (conditional) generative models
    is to learn how to generate data *y* from a target domain using information *x*
    from a source domain. Both domains can be images, text, semantic maps, audio,
    etc. Two types of modeling have become very successful: Generative Adversarial
    Networks (GANs) and Diffusion Probabilistic Models. Concretely,'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，提供一些背景。条件生成模型的目标是学习如何从目标领域生成数据 *y*，利用源领域的信息 *x*。这两个领域可以是图像、文本、语义图、音频等。两种建模方法已取得很大成功：生成对抗网络（GANs）和扩散概率模型。具体来说，
- en: GANs learn how to sample from the data distribution *p*(*y*∣*x*) by training
    a generator model that produces data distributed according to *g*​(*y*∣*x*). It
    uses a discriminator model that guides the generator from blindly to accurately
    generating data by minimizing a divergence (or distance) between the distributions
    *g*​ and *p*.
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GANs 通过训练生成器模型来学习如何从数据分布 *p*(*y*∣*x*) 中采样，生成的数据按照 *g*​(*y*∣*x*) 分布。它使用一个鉴别器模型，指导生成器从盲目生成数据到准确生成数据，通过最小化
    *g*​ 和 *p* 之间的散度（或距离）来实现。
- en: Diffusion models learn how to sample from *p*(*y*∣*x*) by reducing the latent
    variables *y*₁​, *y*₂​, ⋯, *y*ₙ​ from *p*(*y*∣*x*, *y*₁​​, *y*₂, ⋯, *y*ₙ​​). These
    variables are a sequence of increasingly noisy versions of *y* (or an encoding
    of *y*), and the reduction is done by learning a denoising model.
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 扩散模型通过减少从 *p*(*y*∣*x*) 中采样的潜在变量 *y*₁​、*y*₂​、⋯、*y*ₙ​ 来学习。这些变量是 *y*（或 *y* 的编码）的一系列逐渐噪声化的版本，减少的过程是通过学习去噪模型来完成的。
- en: If you need more details about these types of modeling, there are countless
    sources available online. For GANs, you may want to start from [this article](/must-read-papers-on-gans-b665bbae3317)
    and, for Diffusion models, from [this one](https://lilianweng.github.io/posts/2021-07-11-diffusion-models/).
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你需要关于这些建模类型的更多细节，网上有大量资源可供查阅。关于GANs，你可以从[这篇文章](/must-read-papers-on-gans-b665bbae3317)开始，对于扩散模型，可以从[这篇文章](https://lilianweng.github.io/posts/2021-07-11-diffusion-models/)开始。
- en: '![](../Images/a1e76447e5b6b44d18afbf198d1576bc.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a1e76447e5b6b44d18afbf198d1576bc.png)'
- en: '*Figure 1: Visual ChatGPT* [*demo*](https://github.com/microsoft/TaskMatrix)
    *from Microsoft. Used with permission.*'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '*图1：Visual ChatGPT* [*演示*](https://github.com/microsoft/TaskMatrix) *来自微软，已获许可。*'
- en: Now that we have set the basics, let’s discuss some applications. Figure 1 shows
    the official Visual ChatGPT demo. It uses several models for vision-language interactions,
    some of which are listed in the table below.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经设置了基础，让我们讨论一些应用。图1展示了官方的Visual ChatGPT演示。它使用了几个用于视觉-语言交互的模型，部分模型列在下面的表格中。
- en: 'Table 1: Some of the VFM powering Visual ChatGPT. Inferred from the appendix
    in the [paper](https://arxiv.org/abs/2303.04671)¹.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 表1：一些支持Visual ChatGPT的VFM。信息来源于[论文](https://arxiv.org/abs/2303.04671)¹的附录。
- en: Most of these are generative models, with the majority being based on [Stable
    Diffusion](https://github.com/CompVis/stable-diffusion). This speaks about a recent
    switch of interest from GANs to Diffusion models, triggered by [evidence](https://papers.nips.cc/paper/2021/hash/49ad23d1ec9fa4bd8d77d02681df5cfa-Abstract.html)³
    that the latter are superior on image synthesis than the former. One take away
    from the present article is that this doesn’t imply that Diffusion models are
    better than GANs for all image generation tasks, as the aggregation of these models
    tends to perform better than the independent parts.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 其中大多数是生成模型，其中大部分基于[稳定扩散](https://github.com/CompVis/stable-diffusion)。这表明了一个近期的兴趣转变，从
    GAN 转向扩散模型，这一转变由[证据](https://papers.nips.cc/paper/2021/hash/49ad23d1ec9fa4bd8d77d02681df5cfa-Abstract.html)³
    触发，表明后者在图像合成方面优于前者。本文的一个要点是，这并不意味着扩散模型在所有图像生成任务中都优于 GAN，因为这些模型的聚合往往比独立部分表现更好。
- en: Before discussing this and arriving to ATME, let’s pave the way by revisiting
    the main weaknesses and strengths of GANs and Diffusion models.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在讨论这一点并到达 ATME 之前，让我们通过重新审视 GAN 和扩散模型的主要弱点和优势来铺平道路。
- en: GANs
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: GANs
- en: The main premise introduced in the original GAN [paper](https://papers.nips.cc/paper_files/paper/2014/hash/5ca3e9b122f61f8f06494c97b1afccf3-Abstract.html)⁴
    and emphasized in the [tutorial](https://arxiv.org/abs/1701.00160) is that, in
    the limit of a large enough model and infinite data, the minimax game played by
    the generator and discriminator converges to the Nash equilibrium, where the (vanilla)
    GAN objective achieves the value −log4\. In practice, however, this is hardly
    observed. The departures from this theoretical result give rise to what is popularly
    known as the training instability of GANs. This, together with mode collapse,
    are their main drawbacks. They are compensated by still high image generation
    quality achieved, in one shot, with lightweight models.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 原始 GAN [论文](https://papers.nips.cc/paper_files/paper/2014/hash/5ca3e9b122f61f8f06494c97b1afccf3-Abstract.html)⁴
    中引入的主要前提，并在[教程](https://arxiv.org/abs/1701.00160)中强调的是，在一个足够大的模型和无限数据的极限情况下，生成器和判别器之间的极小极大游戏会收敛到纳什均衡，其中（原始）GAN
    目标达到值 −log4。然而，在实践中，这几乎没有观察到。这一理论结果的偏离产生了广泛被称为 GAN 训练不稳定的问题。这与模式崩溃一起，是其主要缺点。它们通过轻量级模型一次生成仍能达到较高的图像生成质量来弥补这一点。
- en: Diffusion Models
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 扩散模型
- en: In contrast, Diffusion models are stable but known to be inefficient due to
    the large number of steps required to learn the denoising distribution. This is
    the case because such a distribution is commonly assumed to be Gaussian, which
    is only justified in the infinitesimal limit of small denoising steps.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，扩散模型虽然稳定，但由于学习去噪分布所需的步骤过多而效率较低。这是因为这种分布通常假设为高斯分布，这只有在去噪步骤非常小时才是合理的。
- en: Recently developed alternatives to reduce the number of denoising steps (even
    further down to 2), using multi-modal distributions, exist. This requires combining
    Diffusion models with GANs, as we discuss in the following.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 最近开发的替代方法通过使用多模态分布来减少去噪步骤的数量（甚至减少到 2 步），这需要将扩散模型与 GAN 结合，如我们在接下来的讨论中所述。
- en: GAN ∪ Diffusion
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: GAN ∪ 扩散
- en: Current approaches to train GANs with Diffusion are very promising. They can
    be classified as belonging to the GAN ∪ Diffusion class of models that use generative
    adversarial training together with **multi-step** diffusion processes.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 目前的 GAN 与扩散训练方法非常有前景。它们可以被归类为使用生成对抗训练与**多步骤**扩散过程相结合的 GAN ∪ 扩散模型类。
- en: In order to improve training stability and mode coverage in the GANs, these
    models inject instance noise by following diffusion processes which may have from
    up to thousands of steps (as in [*Diffusion-GAN*](https://github.com/Zhendong-Wang/Diffusion-GAN)⁵)
    to as few as two steps (as in [*Denoising Diffusion GANs*](https://nvlabs.github.io/denoising-diffusion-gan/)⁶).
    These models perform better than strong GAN baselines on various datasets, but
    still need multiple denoising steps. So,
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 为了改善 GAN 的训练稳定性和模式覆盖，这些模型通过遵循扩散过程注入实例噪声，这些过程可能有多达几千个步骤（如在 [*Diffusion-GAN*](https://github.com/Zhendong-Wang/Diffusion-GAN)⁵
    中），也可能只有两个步骤（如在 [*去噪扩散 GAN*](https://nvlabs.github.io/denoising-diffusion-gan/)⁶
    中）。这些模型在各种数据集上的表现优于强 GAN 基线，但仍需多个去噪步骤。因此，
- en: '*is it possible to generate images with a GAN in one shot and still leverage
    denoising diffusion processes?*'
  id: totrans-30
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*是否有可能在一次生成中使用 GAN 创建图像，并同时利用去噪扩散过程？*'
- en: The answer is yes, and this defines the GAN ∩ Diffusion class of models.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 答案是肯定的，这定义了 GAN ∩ 扩散类模型。
- en: GAN ∩ Diffusion
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: GAN ∩ 扩散
- en: 'It turns out that a single trick can make the [pix2pix](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix)⁷
    visual foundation GAN model stable by design: *paying attention to the discriminator
    mean entropy*.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 结果是，单个技巧可以使 [pix2pix](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix)⁷
    视觉基础GAN模型在设计上稳定：*关注判别器均值熵*。
- en: '![](../Images/f7ae64932bbe981be9858f7315f79c53.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f7ae64932bbe981be9858f7315f79c53.png)'
- en: '*Figure 2: ATME generates images using the UNet from Diffusion models, which
    are judged by the patchGAN discriminator from pix2pix. Image by author.*'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '*图2：ATME 使用来自扩散模型的 UNet 生成图像，这些图像由来自 pix2pix 的 patchGAN 判别器进行判断。图片由作者提供。*'
- en: The resulting model, [ATME](https://github.com/DLR-MI/atme), is shown in Figure
    2\. Given a joint distribution *p*(*x*,*y*) of source and target images, the input
    image *x* at epoch *t* is corrupted with *Wₜ* ​=*W*(*Dₜ-*), as follows
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 结果模型，[ATME](https://github.com/DLR-MI/atme)，如图2所示。给定源图像和目标图像的联合分布 *p*(*x*,*y*)，输入图像
    *x* 在时代 *t* 被 *Wₜ* = *W*(*Dₜ-*）所损坏，如下所示。
- en: '*xₜ*​ = *x* (1+*Wₜ*​)'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '*xₜ* = *x* (1+*Wₜ*)'
- en: with *W* being a small deterministic net which transforms into *Wₜ*​ the discriminator
    decision map *Dₜ-*​ at the previous epoch 𝑡- *= t-1*. The transformed map *Wₜ*
    contains patterns related to patches of the input space where the generator previously
    failed to cheat the discriminator as well as noise coming from the discriminator
    not yet being fully optimized and therefore erring on its decisions.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *W* 是一个小型确定性网络，它将前一时代 𝑡- = *t-1* 的判别器决策图 *Dₜ-*​ 转换为 *Wₜ*。转换后的图 *Wₜ* 包含与输入空间中的补丁相关的模式，这些补丁是生成器之前未能欺骗判别器的区域，以及来自判别器的噪声，判别器尚未完全优化，因此在决策上出现错误。
- en: The generator sees corrupted source images and generates target images according
    to 𝑦̂​ = *y*ᵩ(*xₜ*​, 𝑡̃), using the net *y*ᵩ​ for [denoising diffusion probabilistic
    models](https://hojonathanho.github.io/diffusion/), which have suitable attention
    mechanisms. The task of the generator is then to make *x* ⊕ 𝑦̂​​ look indistinguishable
    from *x* ⊕ *y* to the discriminator (where ⊕ denotes concatenation). In doing
    so, it learns to remove the injected noise in the input images. Remarkably, the
    denoising happens along a “time axis” unfold over the course of the training epochs,
    unlike Diffusion models which require an independent time axis within an epoch.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 生成器看到损坏的源图像，并根据 𝑦̂​ = *y*ᵩ(*xₜ*​, 𝑡̃) 生成目标图像，使用用于 [去噪扩散概率模型](https://hojonathanho.github.io/diffusion/)
    的网 *y*ᵩ​，这些模型具有适当的注意机制。生成器的任务是使 *x* ⊕ 𝑦̂​​ 看起来与 *x* ⊕ *y* 对判别器不可区分（其中 ⊕ 表示连接）。通过这样做，它学会了去除输入图像中的注入噪声。值得注意的是，去噪发生在一个沿着“时间轴”的展开过程中，而不是像扩散模型那样在每个时代内需要一个独立的时间轴。
- en: Flattening of the signals in *Wₜ*​, as a consequence of denoising the input
    images to the generator, translates to a flat distribution for all the entries
    of *Dₜ*​. This is precisely the Nash equilibrium, with the discriminator being
    in a maximum entropy state.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '*Wₜ* 信号的展平，作为将输入图像去噪到生成器的结果，转化为 *Dₜ* 所有条目的平坦分布。这正是纳什均衡，判别器处于最大熵状态。'
- en: … and where is the diffusion?
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: … 扩散在哪里？
- en: The evolution of a corrupted input image, over the course of the training epochs,
    can be written as
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 被损坏的输入图像在训练时期的演变可以写作
- en: '*dxₜ*​​= *x dWₜ*​'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '*dxₜ* = *x dWₜ* '
- en: which may be seen as a finite-difference instance of the more general [SDE](https://en.wikipedia.org/wiki/Stochastic_differential_equation)
    for diffusion processes *dxₜ*​ = *μ*(*xₜ*​​,*t*) *dt* + *σ*(*xₜ*​​,*t*) *dWₜ*​,
    provided that *Wₜ*​ is a Wiener process (aka standard Brownian motion).
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 这可以视为更一般的 [SDE](https://en.wikipedia.org/wiki/Stochastic_differential_equation)
    扩散过程的有限差分实例，即 *dxₜ* = *μ*(*xₜ*​,*t*) *dt* + *σ*(*xₜ*​,*t*) *dWₜ*，前提是 *Wₜ* 是一个维纳过程（也称标准布朗运动）。
- en: In ATME, there is no design choice to make *W* produce Wiener processes. This
    rather happens naturally in a significant number of cases, as can be seen from
    Figure 3, by analyzing the properties of the time series *dWₜ*​​ for 5 randomly
    selected pixels of 5 randomly selected images during training on the [Maps](http://efrosgans.eecs.berkeley.edu/pix2pix/datasets/)
    dataset.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在 ATME 中，没有设计选择使 *W* 产生维纳过程。这在大量情况下自然发生，从图3中可以看出，通过分析在 [Maps](http://efrosgans.eecs.berkeley.edu/pix2pix/datasets/)
    数据集上训练过程中 5 个随机选择的图像的 5 个随机选择像素的时间序列 *dWₜ*​​ 的属性。
- en: First, a Wiener process is stationary. This is tested using the Augmented Dickey-Fuller
    test, whose *p* values for the resulting test statistics are all way less than
    0.01, so the null hypotheses of having non-stationary time series are all rejected.
    Second, a Wiener process is a Markov process and then the autocorrelation function
    of all *dWₜ* should vanish at all lags. This is appreciable from Figure 3 at the
    99% confidence level.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，维纳过程是平稳的。这是使用扩展的迪基-富勒检验进行测试的，其结果统计量的 *p* 值都远低于 0.01，因此所有非平稳时间序列的原假设都被拒绝。其次，维纳过程是马尔可夫过程，因此所有
    *dWₜ* 的自相关函数在所有滞后处应当消失。从图 3 中以 99% 的置信水平可以明显看出这一点。
- en: '![](../Images/1658f1f8ea0c94d17192b2e69e17e379.png)![](../Images/3aa843f74e35fd69f6d5b56e81e6fe27.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1658f1f8ea0c94d17192b2e69e17e379.png)![](../Images/3aa843f74e35fd69f6d5b56e81e6fe27.png)'
- en: '*Figure 3: Time series of* dW *for selected images and pixels (top) and corresponding
    autocorrelation functions (bottom). Image by author.*'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 3: 选定图像和像素的 dW 时间序列 (上) 和相应的自相关函数 (下)。图像作者提供。*'
- en: Finally, a Wiener process has Gaussian *dWₜ*​. This is tested using the Shapiro-Wilk
    test, giving (in 64% of the cases) *p* values for the test statistics greater
    than 0.01, so the null hypotheses that the series are drawn from a normal distribution
    cannot be rejected in a significant number of cases.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，维纳过程具有高斯 *dWₜ*。这是使用 Shapiro-Wilk 检验进行的测试（在 64% 的情况下），测试统计量的 *p* 值大于 0.01，因此不能在相当多的情况下拒绝系列来自正态分布的原假设。
- en: '![](../Images/72206bd7896a545500abfddeabfec815.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/72206bd7896a545500abfddeabfec815.png)'
- en: '*Figure 4: (Rescaled) discriminator decision map (left) with its associated
    representation (center) and change (right) in the space of the source images.
    Image by author.*'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 4: (重缩放) 判别器决策图 (左) 及其相关表示 (中) 和在源图像空间中的变化 (右)。图像作者提供。*'
- en: As the epochs are traversed by the model, the entries of *Dₜ*​​ tends to a flat
    distribution, as observed from Figure 4 for one of the images processed in Figure
    3\. A Jupyter notebook with these tests may be found in ATME’s GitHub [repository](https://github.com/DLR-MI/atme).
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 随着模型遍历的轮次增加，*Dₜ* 的条目趋向于平坦分布，从图 4 中观察到这一点，对应于图 3 中处理的一张图像。可以在 ATME 的 GitHub [代码库](https://github.com/DLR-MI/atme)
    中找到包含这些测试的 Jupyter notebook。
- en: how to check for stability?
  id: totrans-53
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何检查稳定性？
- en: Stability means that, regardless of dataset and initialization of model weights,
    the GAN objective converges towards the same value (−log4 for a vanilla GAN),
    provided that the conditions of enough data and model capacity are met. This is
    typically the case for ATME, as observed in Figure 5, with more examples found
    in the [paper](https://arxiv.org/abs/2304.09024)². Other popular GANs fail to
    achieve this, a reason why you might have heard of GAN models being “difficult
    to train”.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 稳定性意味着，无论数据集和模型权重的初始化如何，只要满足足够的数据和模型容量条件，GAN 的目标就会收敛到相同的值（对于普通 GAN 为 -log4）。在
    ATME 中通常是这种情况，正如图 5 所观察到的，更多的示例可以在 [论文](https://arxiv.org/abs/2304.09024)² 中找到。其他流行的
    GAN 无法实现这一点，这也是你可能听说过 GAN 模型“难以训练”的原因。
- en: '![](../Images/c16b9051205cec6c7625ef7649c037b6.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c16b9051205cec6c7625ef7649c037b6.png)'
- en: '*Figure 5: The GAN objective in ATME tends to the theoretical value at Nash
    equilibrium. Image by author.*'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 5: ATME 中的 GAN 目标趋向于纳什均衡的理论值。图像作者提供。*'
- en: By attending the discriminator mean entropy, the denoising procedure in ATME
    is designed to stably take the GAN towards the maximum entropy equilibrium state.
    Isn’t this awesome?
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 通过关注判别器的均值熵，ATME 的去噪程序设计为稳定地将 GAN 带到最大熵平衡状态。这是不是很棒？
- en: Closing remarks
  id: totrans-58
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结语
- en: ATME produces state-of-the-art results on supervised image-to-image translation
    at a lesser cost than popular GANs and Latent Diffusion. If you like Physics,
    you might find interesting how the ideas leading to ATME relate to the violation
    of the second law of thermodynamics by Maxwell’s demon. You can find this and
    more in the [paper](https://arxiv.org/abs/2304.09024)².
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: ATME 在有监督的图像到图像翻译中取得了最先进的结果，成本低于流行的 GAN 和潜在扩散。如果你喜欢物理学，你可能会对 ATME 的思想如何与麦克斯韦妖对第二热力学定律的违反相关感兴趣。你可以在
    [论文](https://arxiv.org/abs/2304.09024)² 中找到这些内容及更多信息。
- en: '![](../Images/e59b922b5673c856a8bfee0f3c4bf41a.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e59b922b5673c856a8bfee0f3c4bf41a.png)'
- en: '*Maxwell’s demon (Source:* [*astrogewgaw*](https://astrogewgaw.com/post/demons/)*).
    Used with permission.*'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '*麦克斯韦妖 (来源:* [*astrogewgaw*](https://astrogewgaw.com/post/demons/)*). 已获得许可。*'
- en: The way in which technology is advancing is impressive and encouraging. Looking
    forward to seeing what the marriage of GANs with Diffusion models still promises
    to bring.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 技术的进步方式令人印象深刻且鼓舞人心。期待看到 GAN 和扩散模型结合后还会带来什么新进展。
- en: That’s it for now! I hope you enjoyed reading as I did writing 😉
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 目前就是这些！我希望你和我一样喜欢阅读这篇文章😉
- en: '[1] Chenfei Wu *et al*, Visual ChatGPT: Talking, Drawing and Editing with Visual
    Foundation Models, arXiv 2303.04671 (2023).'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] Chenfei Wu *等*，《视觉 ChatGPT：与视觉基础模型对话、绘图和编辑》，arXiv 2303.04671（2023）。'
- en: '[2] Edgardo Solano-Carrillo *et al*, Look ATME: The Discriminator Mean Entropy
    Needs Attention, arXiv 2304.09024 (2023).'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] Edgardo Solano-Carrillo *等*，《Look ATME：判别器均值熵需要关注》，arXiv 2304.09024（2023）。'
- en: '[3] Prafulla Dhariwal, Alexander Nichol, Diffusion Models Beat GANs on Image
    Synthesis, Advances in Neural Information Processing Systems 34 (NeurIPS 2021)'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] Prafulla Dhariwal, Alexander Nichol，《扩散模型在图像合成中超越 GAN》，神经信息处理系统进展 34（NeurIPS
    2021）。'
- en: '[4]Ian Goodfellow *et al*, Generative Adversarial Nets, Advances in Neural
    Information Processing Systems 27 (NIPS 2014).'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] Ian Goodfellow *等*，《生成对抗网络》，神经信息处理系统进展 27（NIPS 2014）。'
- en: '[5] Zhendong Wang *et al*, Diffusion-GAN: Training GANs with Diffusion, arXiv
    2303.04671(2022).'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '[5] Zhendong Wang *等*，《扩散-GAN：用扩散训练 GAN》，arXiv 2303.04671（2022）。'
- en: '[6] Zhisheng Xiao, Karsten Kreis, Arash Vahdat, Tackling the Generative Learning
    Trilemma with Denoising Diffusion GANs, International Conference on Learning Representations
    (ICLR 2022).'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '[6] Zhisheng Xiao, Karsten Kreis, Arash Vahdat，《使用去噪扩散 GAN 解决生成学习三难问题》，国际学习表示大会（ICLR
    2022）。'
- en: '[7] Phillip Isola *et al*, Image-to-Image Translation with Conditional Adversarial
    Networks, Computer Vision and Pattern Recognition (CVPR 2017).'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '[7] Phillip Isola *等*，《基于条件对抗网络的图像到图像翻译》，计算机视觉与模式识别会议（CVPR 2017）。'
