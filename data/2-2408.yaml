- en: 'XGBoost: The Definitive Guide (Part 1)'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 'XGBoost: 权威指南（第一部分）'
- en: 原文：[https://towardsdatascience.com/xgboost-the-definitive-guide-part-1-cc24d2dcd87a](https://towardsdatascience.com/xgboost-the-definitive-guide-part-1-cc24d2dcd87a)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/xgboost-the-definitive-guide-part-1-cc24d2dcd87a](https://towardsdatascience.com/xgboost-the-definitive-guide-part-1-cc24d2dcd87a)
- en: A step-by-step derivation of the popular XGBoost algorithm including a detailed
    numerical illustration
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 对流行的XGBoost算法进行逐步推导，包括详细的数值示例。
- en: '[](https://medium.com/@roiyeho?source=post_page-----cc24d2dcd87a--------------------------------)[![Dr.
    Roi Yehoshua](../Images/905a512ffc8879069403a87dbcbeb4db.png)](https://medium.com/@roiyeho?source=post_page-----cc24d2dcd87a--------------------------------)[](https://towardsdatascience.com/?source=post_page-----cc24d2dcd87a--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----cc24d2dcd87a--------------------------------)
    [Dr. Roi Yehoshua](https://medium.com/@roiyeho?source=post_page-----cc24d2dcd87a--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@roiyeho?source=post_page-----cc24d2dcd87a--------------------------------)[![Dr.
    Roi Yehoshua](../Images/905a512ffc8879069403a87dbcbeb4db.png)](https://medium.com/@roiyeho?source=post_page-----cc24d2dcd87a--------------------------------)[](https://towardsdatascience.com/?source=post_page-----cc24d2dcd87a--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----cc24d2dcd87a--------------------------------)
    [Dr. Roi Yehoshua](https://medium.com/@roiyeho?source=post_page-----cc24d2dcd87a--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----cc24d2dcd87a--------------------------------)
    ·15 min read·Aug 9, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----cc24d2dcd87a--------------------------------)
    ·15 min read·2023年8月9日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/6146a3f112054a1a9ea8ef91d58e7b4e.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6146a3f112054a1a9ea8ef91d58e7b4e.png)'
- en: Photo by [Sam Battaglieri](https://unsplash.com/@st_b?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
    on [Unsplash](https://unsplash.com/photos/_PXtCCQ4Dj0?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由 [Sam Battaglieri](https://unsplash.com/@st_b?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
    提供，来源于 [Unsplash](https://unsplash.com/photos/_PXtCCQ4Dj0?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
- en: XGBoost (short for eXtreme Gradient Boosting) is an open-source library that
    provides an optimized and scalable implementation of gradient boosted decision
    trees. It incorporates various software and hardware optimization techniques that
    allow it to deal with huge amounts of data.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: XGBoost（极端梯度提升的简称）是一个开源库，提供了梯度提升决策树的优化和可扩展实现。它结合了各种软件和硬件优化技术，使其能够处理大量数据。
- en: Originally developed as a research project by Tianqi Chen and Carlos Guestrin
    in 2016 [1], XGBoost has become the go-to solution for solving supervised learning
    tasks on structured (tabular) data. It provides state-of-the-art results on many
    standard regression and classification tasks, and many Kaggle competition winners
    have used XGBoost as part of their winning solutions.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 最初由Tianqi Chen和Carlos Guestrin于2016年开发为研究项目[1]，XGBoost已经成为处理结构化（表格）数据的监督学习任务的首选解决方案。它在许多标准回归和分类任务中提供了最先进的结果，许多Kaggle竞赛获胜者将XGBoost作为他们获胜解决方案的一部分。
- en: Although significant progress has been made using deep neural networks for tabular
    data, they are still outperformed by XGBoost and other tree-based models on many
    standard benchmarks [2, 3]. In addition, XGBoost requires much less tuning than
    deep models.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管使用深度神经网络对表格数据取得了显著进展，但它们在许多标准基准测试中仍被XGBoost和其他基于树的模型超越[2, 3]。此外，XGBoost所需的调优比深度模型少得多。
- en: 'The main innovations of XGBoost with respect to other gradient boosting algorithms
    include:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: XGBoost相对于其他梯度提升算法的主要创新包括：
- en: Clever regularization of the decision trees.
  id: totrans-12
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 聪明的决策树正则化。
- en: Using second-order approximation to optimize the objective (Newton boosting).
  id: totrans-13
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用二阶近似优化目标（牛顿提升）。
- en: A weighted quantile sketch procedure for efficient computation.
  id: totrans-14
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 高效计算的加权分位数草图过程。
- en: A novel tree learning algorithm for handling sparse data.
  id: totrans-15
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 处理稀疏数据的新颖树学习算法。
- en: Support for parallel and distributed processing of the data.
  id: totrans-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 支持数据的并行和分布式处理。
- en: Cache-aware block structure for out-of-core tree learning.
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 针对外部树学习的缓存感知块结构。
- en: In this series of articles we will cover XGBoost in depth, including the mathematical
    details of the algorithm, implementation of the algorithm in Python from scratch,
    an overview of the XGBoost library and how to use it in practice.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在本系列文章中，我们将深入探讨 XGBoost，包括算法的数学细节、从零开始用 Python 实现算法、XGBoost 库的概述以及如何在实际中使用它。
- en: In this first article of the series, we are going to derive the XGBoost algorithm
    step-by-step, provide an implementation of the algorithm in pseudocode, and then
    illustrate its working on a toy data set.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在本系列的第一篇文章中，我们将一步步推导 XGBoost 算法，提供算法的伪代码实现，然后在一个玩具数据集上展示其工作原理。
- en: 'The description of the algorithm given in this article is based on XGBoost’s
    original paper [1] and the official documentation of the XGBoost library ([https://xgboost.readthedocs.io/](https://xgboost.readthedocs.io/en/stable/)).
    However, the article goes beyond the existing documentation in the following respects:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 本文中对算法的描述基于 XGBoost 的原始论文 [1] 和 XGBoost 库的官方文档（[https://xgboost.readthedocs.io/](https://xgboost.readthedocs.io/en/stable/)）。不过，本文在以下几个方面超越了现有的文档：
- en: It explains every step of the mathematical derivation in detail. Understanding
    the mathematical details of the algorithm will help you grasp the meaning of the
    various hyperparameters of XGBoost (and there are quite a lot of them) and how
    to tune them in practice.
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 详细解释了数学推导的每一步。理解算法的数学细节将帮助你掌握 XGBoost 的各种超参数的含义（它们的数量相当多）以及如何在实践中调整它们。
- en: Provides a complete pseudocode of the algorithm (the pseudocode in [1] only
    describes specific parts of the algorithm in a very concise way).
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提供算法的完整伪代码（[1]中的伪代码仅以非常简洁的方式描述了算法的具体部分）。
- en: Goes through a detailed numerical example of applying XGBoost to a toy data
    set.
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 详细讲解将 XGBoost 应用到玩具数据集上的数值示例。
- en: 'This article assumes that you are already familiar with gradient boosting.
    If not, please check out the article below:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 本文假设你已经对梯度提升有所了解。如果没有，请查看以下文章：
- en: '[](/gradient-boosting-from-theory-to-practice-part-1-940b2c9d8050?source=post_page-----cc24d2dcd87a--------------------------------)
    [## Gradient Boosting from Theory to Practice (Part 1)'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/gradient-boosting-from-theory-to-practice-part-1-940b2c9d8050?source=post_page-----cc24d2dcd87a--------------------------------)
    [## 从理论到实践的梯度提升（第 1 部分）'
- en: Understand the math behind the popular gradient boosting algorithm and how to
    use it in practice
  id: totrans-26
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 理解流行的梯度提升算法背后的数学原理，以及如何在实际中使用它
- en: towardsdatascience.com](/gradient-boosting-from-theory-to-practice-part-1-940b2c9d8050?source=post_page-----cc24d2dcd87a--------------------------------)
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '[towardsdatascience.com](/gradient-boosting-from-theory-to-practice-part-1-940b2c9d8050?source=post_page-----cc24d2dcd87a--------------------------------)'
- en: Now let’s dive in!
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们深入了解吧！
- en: The XGBoost Algorithm
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: XGBoost 算法
- en: 'Recall that in [supervised learning](https://medium.com/@roiyeho/introduction-to-supervised-machine-learning-313730eb5aa2)
    problems, we are given a training set with *n* labeled samples: *D* = {(**x**₁,
    *y*₁), (**x**₂, *y*₂), … , (**x***ₙ, yₙ*)}, where **x***ᵢ* is a *m*-dimensional
    vector that contains the features of sample *i*, and *yᵢ* is the label of that
    sample. Our goal is to build a model whose predictions are as close as possible
    to the true labels.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 回顾一下，在[监督学习](https://medium.com/@roiyeho/introduction-to-supervised-machine-learning-313730eb5aa2)问题中，我们会得到一个包含
    *n* 个标记样本的训练集：*D* = {(**x**₁, *y*₁), (**x**₂, *y*₂), … , (**x**ₙ, *y*ₙ*)}，其中 **x**ᵢ
    是一个 *m* 维向量，包含样本 *i* 的特征，而 *yᵢ* 是该样本的标签。我们的目标是构建一个预测结果尽可能接近真实标签的模型。
- en: 'Similar to gradient tree boosting, XGBoost builds an ensemble of regression
    trees, which consists of *K* additive functions:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于梯度树提升，XGBoost 构建了一个回归树的集成，这个集成由 *K* 个加性函数组成：
- en: '![](../Images/2e7e0d281a62a8f361c170cd2c28e6c5.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2e7e0d281a62a8f361c170cd2c28e6c5.png)'
- en: where *K* is the number of trees, and *F* is the set of all possible regression
    tree functions. Note that we use here *f*(**x**) to denote the hypothesis of the
    base models instead of our typical notation *h*(**x**), since the letter *h* will
    be used later to represent another entity (the Hessian of the loss function).
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *K* 是树的数量，*F* 是所有可能的回归树函数的集合。请注意，我们在这里使用 *f*(**x**) 来表示基础模型的假设，而不是我们通常使用的
    *h*(**x**)，因为字母 *h* 稍后将用于表示另一个实体（损失函数的 Hessian）。
- en: 'Given a loss function *L*(*y*, *F*(**x**)) that measures the difference between
    the true label *y* and the ensemble''s prediction *F*(**x**), XGBoost aims to
    find an ensemble that minimizes the loss on the training set, while also not being
    overly complex. To that end, it defines the following *regularized* cost function
    as the objective function of the model:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个损失函数 *L*(*y*, *F*(**x**))，它衡量真实标签 *y* 和集成预测 *F*(**x**) 之间的差异，XGBoost 旨在找到一个在训练集上最小化损失的集成模型，同时避免过于复杂。为此，它将以下
    *正则化* 成本函数定义为模型的目标函数：
- en: '![](../Images/4e0da6c9b4fa289a93d4e3bc27754761.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4e0da6c9b4fa289a93d4e3bc27754761.png)'
- en: The regularized cost function
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 正则化成本函数
- en: 'where *ω*(*fₖ*) is the complexity of the tree *fₖ*, which will be defined in
    detail later. Similar to [ridge regression](https://medium.com/@roiyeho/regularization-19b1879415a1),
    this cost function consists of two parts: the total loss of the model on the training
    set and a regularization term that penalizes the complexity of the base trees.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *ω*(*fₖ*) 是树 *fₖ* 的复杂度，这将在后续详细定义。类似于[岭回归](https://medium.com/@roiyeho/regularization-19b1879415a1)，该成本函数由两部分组成：模型在训练集上的总损失和一个对基树复杂度进行惩罚的正则化项。
- en: Note that in XGBoost regularization is an integral part of the tree learning
    objective, rather than being imposed by external heuristics such as limiting the
    maximum tree depth or maximum number of leaves as in other tree-based models.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在 XGBoost 中，正则化是树学习目标的一个组成部分，而不是像其他树模型中那样通过限制最大树深度或最大叶子数等外部启发式方法来施加的。
- en: Additive Training
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 加性训练
- en: Since the objective function *J* includes functions as parameters (the regression
    tree functions), it cannot be optimized using traditional optimization methods
    such as gradient descent. Instead, the model is trained in a greedy stepwise manner,
    by adding one new tree at a time.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 由于目标函数 *J* 包含作为参数的函数（回归树函数），它不能使用传统的优化方法如梯度下降进行优化。相反，模型以贪婪的逐步方式进行训练，每次添加一棵新树。
- en: 'Formally, let *Fₖ* be the ensemble at the *k-*th iteration:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 正式来说，让 *Fₖ* 表示第 *k-* 次迭代时的集成模型：
- en: '![](../Images/f3550ab69d8e2542427a888e0e5d34c7.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f3550ab69d8e2542427a888e0e5d34c7.png)'
- en: 'At this iteration, we add to the previous ensemble a tree *fₖ*,which minimizes
    the following objective:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一迭代中，我们将一个树 *fₖ* 加入到之前的集成模型中，该树最小化以下目标：
- en: '![](../Images/7345cfd98afb54825e0a0a78b954ec67.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7345cfd98afb54825e0a0a78b954ec67.png)'
- en: That is, we would like to find a tree *fₖ* that reduces the overall training
    loss, but also is not too complex (has a low complexity *ω*(*fₖ*)).
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 即，我们希望找到一棵树 *fₖ*，它可以降低整体训练损失，但同时也不过于复杂（具有较低复杂度 *ω*(*fₖ*)）。
- en: Newton Boosting
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 牛顿提升
- en: 'Finding the best tree *fₖ* for an arbitrary loss function *L* is computationally
    infeasible, since it would require us to enumerate all the possible trees and
    pick the best one. Instead, we use an iterative optimization approach: in every
    boosting iteration we choose a tree *fₖ* that will get us one step closer to the
    minimum cost.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 对于任意损失函数 *L*，找到最佳树 *fₖ* 是计算上不可行的，因为这需要枚举所有可能的树并选择最佳的一棵。相反，我们使用一种迭代优化方法：在每次提升迭代中，我们选择一棵树
    *fₖ*，使我们更接近最小成本。
- en: In the original (unextreme) gradient boosting algorithm, the function *fₖ* was
    chosen as the one that pointed in the negative gradient direction of the loss
    function with respect to the predictions of the previous ensemble. This made the
    optimization process work as gradient descent in the function space (hence it
    is known as functional gradient descent).
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在原始（非极端）梯度提升算法中，函数 *fₖ* 是选择为指向损失函数相对于先前集成模型预测的负梯度方向的。这使得优化过程在函数空间中如同梯度下降（因此称为功能梯度下降）。
- en: XGBoost uses a similar idea, but instead of working as gradient descent in the
    function space (i.e., using a first-order approximation), it works as a [Newton-Raphson](https://en.wikipedia.org/wiki/Newton%27s_method)
    method in the function space (i.e., using a second-order approximation). Newton’s
    method usually converges faster to the minimum, when the second derivative of
    the objective function is known and easy to compute (which is true of the XGBoost
    objective function when using common loss functions such as squared loss or log
    loss).
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: XGBoost 使用类似的思想，但它不是在函数空间中进行梯度下降（即使用一阶近似），而是在函数空间中使用[牛顿-拉夫森](https://en.wikipedia.org/wiki/Newton%27s_method)
    方法（即使用二阶近似）。当目标函数的二阶导数已知且易于计算时，牛顿法通常会更快地收敛到最小值（对于 XGBoost 的目标函数，当使用常见的损失函数如平方损失或对数损失时，这一点是成立的）。
- en: 'As a reminder, Newton’s method tries to find the minimum of a twice-differentiable
    function *f*: *R* → *R*, by building a sequence {*xₖ*} from an initial guess *x*₀
    ∈ *R* . This sequence is constructed from the second-order Taylor approximations
    of *f* around the elements *xₖ*:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 作为提醒，牛顿方法尝试通过构建从初始猜测*x*₀ ∈ *R*的序列{*xₖ*}来寻找二次可微函数*f*：*R* → *R*的最小值。该序列是由*f*在*xₖ*附近的二阶泰勒近似构成的：
- en: '![](../Images/d2615913639b06f046cc532536d97653.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d2615913639b06f046cc532536d97653.png)'
- en: 'The next element in the sequence *xₖ*₊₁ is chosen as to minimize the quadratic
    expansion written on the right-hand side of this equation. We can find the minimum
    of that expansion by taking its derivative with respect to the step size *t*,
    and set it to 0:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 序列中的下一个元素*xₖ*₊₁被选择为最小化此方程右侧写出的二次扩展。我们可以通过对步长*t*取导数并设置为0来找到该扩展的最小值：
- en: '![](../Images/0ceb6e19b1685d180a4b2e7a81440433.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0ceb6e19b1685d180a4b2e7a81440433.png)'
- en: 'Thus, the minimum is achieved for:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，最小值在以下情况下实现：
- en: '![](../Images/52831f103e73813a29ea6a78a83a0328.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/52831f103e73813a29ea6a78a83a0328.png)'
- en: 'Accordingly, Newton’s method performs the following iteration:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，牛顿方法执行以下迭代：
- en: '![](../Images/96d5000d99ad41012a8672d83183e139.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/96d5000d99ad41012a8672d83183e139.png)'
- en: 'This sequence is guaranteed to converge to the minimum *x** of *f* quadratically
    fast, if *f* is a strongly convex function and provided that *x*₀ is close enough
    to *x**:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 该序列保证以*二次*速度收敛到最小值**x**，前提是*f*是一个强凸函数，并且*x*₀足够接近**x**：
- en: '![](../Images/aaace1506c87bb9d967b08d7e70627c4.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/aaace1506c87bb9d967b08d7e70627c4.png)'
- en: Newton’s method (red) uses curvature information (the second derivative) to
    take a more direct route to the minimum than gradient descent (green). Image made
    by Oleg Alexandrov and released to public domain ([https://en.wikipedia.org/wiki/Newton%27s_method_in_optimization#/media/File:Newton_optimization_vs_grad_descent.svg](https://en.wikipedia.org/wiki/Newton%27s_method_in_optimization#/media/File:Newton_optimization_vs_grad_descent.svg))
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 牛顿方法（红色）利用曲率信息（第二导数）比梯度下降（绿色）更直接地达到最小值。图像由Oleg Alexandrov制作并发布于公共领域（[https://en.wikipedia.org/wiki/Newton%27s_method_in_optimization#/media/File:Newton_optimization_vs_grad_descent.svg](https://en.wikipedia.org/wiki/Newton%27s_method_in_optimization#/media/File:Newton_optimization_vs_grad_descent.svg)）
- en: 'Coming back to XGBoost, we first write the second-order Taylor expansion of
    the loss function around a given data point **x***ᵢ*:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 回到XGBoost，我们首先写出给定数据点**x**ᵢ附近的损失函数的二阶泰勒展开：
- en: '![](../Images/58b7266359e33a7d0893bba11d3b7db9.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/58b7266359e33a7d0893bba11d3b7db9.png)'
- en: 'Here, *gᵢ* is the first derivative (gradient) of the loss function, and *hᵢ*
    is the second derivative (Hessian) of the loss function, both with respect to
    the predicted value of the previous ensemble at **x***ᵢ*:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*gᵢ*是损失函数的第一导数（梯度），*hᵢ*是损失函数的第二导数（Hessian），都与前一个集成的预测值**x**ᵢ有关：
- en: '![](../Images/dc21ea773dd7cb0d6ff99f38e00b26bd.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/dc21ea773dd7cb0d6ff99f38e00b26bd.png)'
- en: 'Plugging this approximation into our objective function at iteration *k* yields:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 将此近似值代入我们在迭代*k*时的目标函数中得到：
- en: '![](../Images/65c96f27beb89fa68b0d13d27002c676.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/65c96f27beb89fa68b0d13d27002c676.png)'
- en: 'After removing the constant terms, we obtain the following simplified objective:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在去除常数项后，我们得到以下简化目标：
- en: '![](../Images/56d6f7d263d260408fcbed099ba01540.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/56d6f7d263d260408fcbed099ba01540.png)'
- en: Our goal is to find a tree *fₖ*(**x**) (our “step size”) that will minimize
    this objective function. Note that this function only depends on *gᵢ* and *hᵢ*,
    which allows XGBoost to support any custom loss function that is twice differentiable.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的目标是找到一个树*fₖ*（**x**）（我们的“步长”），使其最小化这个目标函数。注意，该函数仅依赖于*gᵢ*和*hᵢ*，这使得XGBoost能够支持任何二次可微的自定义损失函数。
- en: Definition of Tree Complexity
  id: totrans-70
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 树复杂度的定义
- en: Now that we have introduced the training step, we need to define a measure for
    the complexity of the tree. To that end, we first write a more explicit expression
    for the function *f*(**x**) computed by the regression tree.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们引入了训练步骤，我们需要定义树的复杂度度量。为此，我们首先写出回归树计算的函数*f*（**x**）的更明确表达式。
- en: 'Let *T* be the number of leaves in the tree, **w** = (*w*₁, …, *wₜ*) ∈ *Rᵗ*
    a vector of the scores (or weights) at the leaf nodes, and *q*(**x**): *Rᵐ* →
    {1, 2, …, *T*} a function that assigns each sample **x** (an *m*-dimensional vector)
    to its corresponding leaf index (according to the decision rules in the tree nodes).
    Then we can write *f*(**x**) as follows:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '设 *T* 为树中的叶子数量，**w** = (*w*₁, …, *wₜ*) ∈ *Rᵗ* 是叶节点上的分数（或权重）向量，*q*(**x**): *Rᵐ*
    → {1, 2, …, *T*} 是一个将每个样本**x**（一个*m*维向量）分配到其对应叶子索引的函数（根据树节点中的决策规则）。那么我们可以将 *f*(**x**)
    表示为：'
- en: '![](../Images/9c42787fedc14f09da8a7ce5624a1d36.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9c42787fedc14f09da8a7ce5624a1d36.png)'
- en: That is, the output value assigned to a sample **x** is the weight of the leaf
    node to which this sample is mapped to by the tree.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 也就是说，分配给样本**x**的输出值是该样本通过树映射到的叶节点的权重。
- en: 'We now define the complexity of the tree as follows:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在定义树的复杂度如下：
- en: '![](../Images/e1b24f9c56cf8a29fcf24c96c14004ed.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e1b24f9c56cf8a29fcf24c96c14004ed.png)'
- en: That is, the complexity of the tree is a function of the number of its leaves
    (*γT*, where *γ* is a hyperparameter), and the sum of the weights of the leaves
    squared multiplied by another hyperparameter *λ.* Increasing *γ* tends to create
    smaller trees, while increasing *λ* encourages assigning smaller weights to the
    leaves, which in turn decreases the contribution of the tree to the reduction
    in the loss function (similar to the shrinkage factor in gradient boosting).
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 也就是说，树的复杂度是其叶子数量的函数（*γT*，其中 *γ* 是一个超参数），以及叶子权重的平方和乘以另一个超参数 *λ*。增加 *γ* 倾向于创建较小的树，而增加
    *λ* 则鼓励给叶子分配较小的权重，这反过来减少了树对减少损失函数的贡献（类似于梯度提升中的收缩因子）。
- en: 'Using this definition for the tree complexity, we can now rewrite the objective
    function at step *k* as follows:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这个树复杂度的定义，我们现在可以将第 *k* 步的目标函数重写如下：
- en: '![](../Images/1b83a45b70f1e8ad2488dc32dd567924.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1b83a45b70f1e8ad2488dc32dd567924.png)'
- en: where *Iⱼ* = {*i*|*q*(**x***ᵢ*) = *j*} is the set of indices of the samples
    that are assigned to the *j*-th leaf. Note that in the second line of the equation
    we changed the index of the summation from *i* to *j*, since all the samples at
    the same leaf node get the same weight.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *Iⱼ* = {*i*|*q*(**x***ᵢ*) = *j*} 是分配给 *j* -th 叶子的样本的索引集合。注意在方程的第二行中我们将求和的索引从
    *i* 改为 *j*，因为同一叶节点上的所有样本得到相同的权重。
- en: 'We can further simplify the objective function by defining:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过定义进一步简化目标函数：
- en: '![](../Images/29576859b75402fd7aa5c25fe2c0e298.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/29576859b75402fd7aa5c25fe2c0e298.png)'
- en: '*Gⱼ* is the sum of gradients of the loss function with respect to the samples
    at leaf *j*, and *Hⱼ* is the sum of Hessians of the loss function with respect
    to the same samples.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '*Gⱼ* 是关于叶子*j* 的样本的损失函数梯度之和，*Hⱼ* 是关于同一组样本的损失函数的Hessian矩阵之和。'
- en: 'We can now write:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以写出：
- en: '![](../Images/e7409c484380bf3266caf4000ce85de3.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e7409c484380bf3266caf4000ce85de3.png)'
- en: 'Our goal is to find the weights for the leaves that will minimize this cost
    function. To that end, we take the partial derivative of *Jₖ* with respect to
    each weight *wⱼ*:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的目标是找到使这个成本函数最小化的叶子权重。为此，我们对每个权重 *wⱼ* 求 *Jₖ* 的偏导数：
- en: '![](../Images/5db10540d074227510f758408f00fbe9.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5db10540d074227510f758408f00fbe9.png)'
- en: 'Setting this derivative to 0 gives us the optimal weight for leaf *wⱼ*:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 将这个导数设置为0，给我们叶子* wⱼ* 的最优权重：
- en: '![](../Images/cfe587938f2cc0a6785ee589fa0227e1.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/cfe587938f2cc0a6785ee589fa0227e1.png)'
- en: 'Plugging the optimal weights back into the objective function gives us the
    best objective reduction we can get from this tree:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 将最优权重代入目标函数中，给我们这个树可以获得的最佳目标减少量：
- en: '![](../Images/d13f82b08416488b91fc9749c9cd0b3b.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d13f82b08416488b91fc9749c9cd0b3b.png)'
- en: Learning the Tree Structure
  id: totrans-92
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 学习树结构
- en: Now that we know how to measure the quality of a given tree, theoretically we
    could enumerate all the possible trees and pick the best one. In practice this
    is intractable. Instead, we build the regression tree in a greedy top-down fashion,
    similar to how standard decision trees are built.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们知道如何衡量给定树的质量，从理论上讲，我们可以枚举所有可能的树并选择最佳的一棵。实际上，这是不可行的。相反，我们以贪婪的自顶向下方式构建回归树，类似于标准决策树的构建方式。
- en: In each node of the tree, we evaluate every possible split by computing how
    much reduction in the cost function *Jₖ* can be obtained from that split.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在树的每个节点中，我们通过计算从该分割中可以获得的成本函数 *Jₖ* 的减少量来评估所有可能的分割。
- en: 'Formally, let *Iₗ* and *Iᵣ* be the sets of samples at the left and right child
    nodes after the split, respectively, and let *I* = *Iₗ* ∪ *Iᵣ* be the set of samples
    at the parent node*.* Then the sums of the gradients and Hessians in the child
    nodes are:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 正式地，设*Iₗ*和*Iᵣ*分别为分裂后左子节点和右子节点的样本集合，*I* = *Iₗ* ∪ *Iᵣ*为父节点的样本集合。则子节点中的梯度和Hessian的总和为：
- en: '![](../Images/7610124302c1998b21c0ce2c4c7b8c96.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7610124302c1998b21c0ce2c4c7b8c96.png)'
- en: 'And the sums of gradients and Hessians in the parent node are:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 父节点中的梯度和Hessian的总和为：
- en: '![](../Images/118db0775a45bc8ec0b35e2f3bd594fc.png)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/118db0775a45bc8ec0b35e2f3bd594fc.png)'
- en: 'Therefore, the reduction in the cost we get after the split is:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，分裂后的成本减少为：
- en: '![](../Images/056d7b71b708348adcd73fc28b02206d.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/056d7b71b708348adcd73fc28b02206d.png)'
- en: We use this formula to evaluate all the split candidates and then choose the
    split with the highest gain (highest reduction in the cost). If the gain obtained
    by the best split is less than 0, i.e., the reduction in the cost is less than
    *γ*, then we choose not to split the parent node at all (it will become a leaf
    in the tree).
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用这个公式来评估所有的分裂候选，然后选择增益（成本减少最高）的分裂。如果最佳分裂获得的增益小于0，即成本减少小于*γ*，那么我们选择不对父节点进行分裂（它将成为树中的一个叶子节点）。
- en: Note that the equation for computing the contribution of each leaf node to the
    gain is very similar to the equation for the optimal weight of a leaf, except
    that we do not square the sum of the gradients in the numerator.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，每个叶子节点对增益的贡献的计算公式与叶子节点的最优权重公式非常相似，只是我们没有对分子中的梯度总和进行平方处理。
- en: The Complete Algorithm in Pseudocode
  id: totrans-103
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 伪代码中的完整算法
- en: 'The following pseudocode shows the XGBoost algorithm in its full glory:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 以下伪代码展示了XGBoost算法的全部风采：
- en: '![](../Images/678187956dbd7f09c7b9c9f0e4f25b59.png)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/678187956dbd7f09c7b9c9f0e4f25b59.png)'
- en: 'The algorithm uses a helper function called Build-Tree to build the next regression
    tree in the ensemble:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 算法使用一个名为Build-Tree的辅助函数来构建集合中的下一个回归树：
- en: '![](../Images/9d79f96305421f8a248e94db0bc20eae.png)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9d79f96305421f8a248e94db0bc20eae.png)'
- en: 'This function in turn uses another helper function Find-Best-Split, which evaluates
    every possible split at a given node and returns the split with the highest gain
    (called an **exact greedy algorithm** in the original XGBoost paper). The best
    split is returned as a tuple that contains the subsets of samples that go to the
    left and the right child nodes and also the sum of their gradients and Hessians.
    The pseudocode of this function is shown below:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 该函数使用了另一个辅助函数Find-Best-Split，它评估给定节点的所有可能分裂，并返回具有最高增益的分裂（在原始XGBoost论文中称为**精确贪婪算法**）。最佳分裂被返回为一个元组，其中包含了分配到左子节点和右子节点的样本子集以及它们梯度和Hessian的总和。该函数的伪代码如下所示：
- en: '![](../Images/fc400ffb7f3f2bc9234b848da1005a0a.png)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fc400ffb7f3f2bc9234b848da1005a0a.png)'
- en: As an exercise, try to implement the algorithm in your favorite programming
    language (a Python implementation is provided in the [next article](https://medium.com/towards-data-science/xgboost-the-definitive-guide-part-2-c38ef02f74d0)
    of this series).
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 作为练习，尝试用你喜欢的编程语言实现该算法（在本系列的[下一篇文章](https://medium.com/towards-data-science/xgboost-the-definitive-guide-part-2-c38ef02f74d0)中提供了Python实现）。
- en: In the following sections we will derive more explicit formulas for the gradient
    and Hessian of the loss function for different types of problems.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的部分，我们将推导不同类型问题的损失函数的梯度和Hessian的更明确公式。
- en: XGBoost for Regression
  id: totrans-112
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: XGBoost 回归
- en: 'In regression problems, the most commonly used loss function is the squared
    loss:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 在回归问题中，最常用的损失函数是平方损失：
- en: '![](../Images/e47907f9a2bc14da524a7280d823a7e6.png)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e47907f9a2bc14da524a7280d823a7e6.png)'
- en: 'Its first derivative with respect to the predicted value of the previous ensemble
    is:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 相对于前一个集合的预测值，其一阶导数为：
- en: '![](../Images/f6a8a26f70f181c58d1189554b1d3341.png)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f6a8a26f70f181c58d1189554b1d3341.png)'
- en: 'And its second derivative is:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 而其二阶导数为：
- en: '![](../Images/d217ca971789b9f8712d1bef937a991e.png)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d217ca971789b9f8712d1bef937a991e.png)'
- en: 'Therefore, the optimal output value for leaf *j* in this case is:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在这种情况下，叶子节点*j*的最优输出值为：
- en: '![](../Images/384c312a8813e638ecd09bd4abd5efc8.png)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/384c312a8813e638ecd09bd4abd5efc8.png)'
- en: 'And the contribution of this leaf to the reduction in the loss is:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 该叶子节点对损失减少的贡献为：
- en: '![](../Images/1a24ef2355a782a5e12f36a7cdc1a5b6.png)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1a24ef2355a782a5e12f36a7cdc1a5b6.png)'
- en: XGBoost for Classification
  id: totrans-123
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: XGBoost 分类
- en: 'In binary classification, we use log loss as the loss function:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 在二分类中，我们使用对数损失作为损失函数：
- en: '![](../Images/aa7e0542287e1d240b3a8e728276a8d1.png)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/aa7e0542287e1d240b3a8e728276a8d1.png)'
- en: 'where pi is the previously predicted probability:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 其中pi是之前预测的概率：
- en: '![](../Images/f6d72af3aa7b0be536cc2465739c7eae.png)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f6d72af3aa7b0be536cc2465739c7eae.png)'
- en: 'We have already found the first and second order derivatives of log loss in
    the article on [gradient boosting](https://medium.com/towards-data-science/gradient-boosting-from-theory-to-practice-part-1-940b2c9d8050)
    (see the section “Gradient Tree Boosting for Classification”):'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经在[梯度提升](https://medium.com/towards-data-science/gradient-boosting-from-theory-to-practice-part-1-940b2c9d8050)的文章中找到了对数损失的一阶和二阶导数（参见“分类的梯度树提升”部分）：
- en: '![](../Images/6432001e27cb698317dc527f380b6e79.png)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6432001e27cb698317dc527f380b6e79.png)'
- en: 'Therefore, the optimal output value for leaf *j* is:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，叶子* j *的最佳输出值为：
- en: '![](../Images/09aa0e91a6de76bb2fcc33e7d3e7693f.png)'
  id: totrans-131
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/09aa0e91a6de76bb2fcc33e7d3e7693f.png)'
- en: Note that with the exception of lambda, this is the same formula we used to
    find the output value for the leaves in gradient boosting.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，除了λ之外，这与我们在梯度提升中用于找到叶子输出值的公式相同。
- en: 'The contribution of this leaf to the reduction in the loss is:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 这个叶子对损失减少的贡献是：
- en: '![](../Images/5a5ef5984b6d2e7c17c49ca75ae2f5c6.png)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5a5ef5984b6d2e7c17c49ca75ae2f5c6.png)'
- en: Demonstration on a Toy Dataset
  id: totrans-135
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在玩具数据集上的演示
- en: 'For illustration, we are going to use the same data set that we used to illustrate
    the classical, unextreme gradient boosting algorithm:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明，我们将使用与经典梯度提升算法相同的数据集：
- en: '![](../Images/267dc3df98529f56affc475935ff8b37.png)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/267dc3df98529f56affc475935ff8b37.png)'
- en: 'The goal in this data set to predict whether a customer will buy a given product
    based on three attributes: the customer’s age, level of income (Low, Medium or
    High) and level of education (High School or College).'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集中的目标是预测客户是否会购买给定的产品，基于三个属性：客户的年龄、收入水平（低、中或高）和教育水平（高中或大学）。
- en: To solve this problem we will build an ensemble of XGBoost trees with a maximum
    depth of 2, and a learning rate of *η* = 0.5\. To keep the example simple, the
    regularization parameters will be set to 0 (*λ* = 0 and *γ* = 0).
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这个问题，我们将构建一个最大深度为2的XGBoost树集，并将学习率设置为*η* = 0.5。为了简化示例，正则化参数将设置为0（*λ* = 0
    和 *γ* = 0）。
- en: 'First, we initialize the model with a constant value, which is the log odds
    of the positive class:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们用一个常数值初始化模型，该值是正类的对数几率：
- en: '![](../Images/13efb5fb81fe206610ccfe8a69549225.png)'
  id: totrans-141
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/13efb5fb81fe206610ccfe8a69549225.png)'
- en: 'Next, we calculate the gradients and Hessians of the samples in the training
    set:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们计算训练集中样本的梯度和Hessians：
- en: '![](../Images/d67031cf350522402ce215b5b3b305cb.png)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d67031cf350522402ce215b5b3b305cb.png)'
- en: 'We now build the first XGBoost tree. We start by finding the best split for
    the root node. The sums of the gradients and Hessians at the root node are:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在构建第一个XGBoost树。我们从找到根节点的最佳分裂开始。根节点的梯度和Hessians的和为：
- en: '![](../Images/1be584da34a195a2c72c1c6fcb33a02c.png)'
  id: totrans-145
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1be584da34a195a2c72c1c6fcb33a02c.png)'
- en: 'Next, we compute the gain that can be achieved by every possible split in every
    feature. We start from the two categorical attributes:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们计算每个特征中每个可能分裂点的增益。我们从两个分类属性开始：
- en: '![](../Images/fa88d9ede8276c1ac65693a6824dc2be.png)'
  id: totrans-147
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fa88d9ede8276c1ac65693a6824dc2be.png)'
- en: 'For the Age attribute, we first sort the gradients and Hessians according to
    the age value:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 对于年龄属性，我们首先根据年龄值对梯度和Hessians进行排序：
- en: Ages (sorted) = [22, 25, 28, 30, 35, 40]
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 年龄（排序后）= [22, 25, 28, 30, 35, 40]
- en: Gradients (sorted) = [-0.333, -0.333, 0.667, -0.333, 0.667, -0.333]
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度（排序后）= [-0.333, -0.333, 0.667, -0.333, 0.667, -0.333]
- en: Hessians (sorted) = [0.222, 0.222, 0.222, 0.222, 0.222, 0.222]
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: Hessians（排序后）= [0.222, 0.222, 0.222, 0.222, 0.222, 0.222]
- en: 'We now consider each middle point between two consecutive ages as a candidate
    split point:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在考虑两个连续年龄之间的每一个中点作为候选分裂点：
- en: '![](../Images/7e325b694002f96b0e540402c29ae095.png)![](../Images/007896a1caabc80d8be06eee9ce279e8.png)'
  id: totrans-153
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7e325b694002f96b0e540402c29ae095.png)![](../Images/007896a1caabc80d8be06eee9ce279e8.png)'
- en: 'The split with the highest gain is Age < 26.5\. Therefore, the first level
    of the tree looks as follows:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 增益最高的分裂点是年龄 < 26.5。因此，树的第一层如下所示：
- en: '![](../Images/91a826364c79fd3cfbc7c1cec9490e32.png)'
  id: totrans-155
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/91a826364c79fd3cfbc7c1cec9490e32.png)'
- en: The gradients and the Hessians of the two samples at the left child node are
    exactly the same, thus there is no need to split it anymore.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 左子节点的两个样本的梯度和Hessians完全相同，因此不需要再分裂。
- en: 'We now need to find the best split for the right child node. The sums of the
    gradients and Hessians of the samples at this node (samples 2, 4, 5, 6) are *G*
    = 0.666 and *H* = 0.888 (as shown inside the node). We consider all the possible
    splits of these samples:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在需要找到右子节点的最佳拆分。该节点（样本 2、4、5、6）上的梯度和 Hessian 的总和为 *G* = 0.666 和 *H* = 0.888（如节点内所示）。我们考虑所有可能的拆分方式：
- en: '![](../Images/739b9fff9b1a9f5be923dd16e31ceb3f.png)![](../Images/b5ae93677bde81f877e3c4791d96dabb.png)'
  id: totrans-158
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/739b9fff9b1a9f5be923dd16e31ceb3f.png)![](../Images/b5ae93677bde81f877e3c4791d96dabb.png)'
- en: 'In this case we have multiple candidate splits that lead to the maximum gain
    (1.5). Let’s choose arbitrarily the split Income = Medium. The resulting tree
    is:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们有多个候选拆分，这些拆分能带来最大的增益（1.5）。我们可以任意选择拆分条件为 Income = Medium。得到的树是：
- en: '![](../Images/b459935c727cebc3b9b465dbe5965265.png)'
  id: totrans-160
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b459935c727cebc3b9b465dbe5965265.png)'
- en: 'Next, we compute the optimal output values (weights) for the leaf nodes. The
    weight of the leftmost leaf is:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们计算叶节点的最佳输出值（权重）。最左边叶节点的权重是：
- en: '![](../Images/bd8ee575899f22948e22dfb3a7d505a9.png)'
  id: totrans-162
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bd8ee575899f22948e22dfb3a7d505a9.png)'
- en: 'Similarly, the weights of the other two leaves are:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，另外两个叶节点的权重是：
- en: '![](../Images/229011b27d31f028d2ba5d2f06c4dc56.png)'
  id: totrans-164
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/229011b27d31f028d2ba5d2f06c4dc56.png)'
- en: 'Thus, we get the following predictions from the first XGBoost tree:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们从第一棵 XGBoost 树中得到以下预测：
- en: '![](../Images/da26e447e72481979872a4a064478548.png)'
  id: totrans-166
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/da26e447e72481979872a4a064478548.png)'
- en: 'We now scale these predictions by the learning rate and add them to the predictions
    of the previous iteration. We then use the new predictions to calculate the gradients
    and Hessians for the next iteration:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在通过学习率来缩放这些预测值，并将其添加到前一迭代的预测中。然后我们使用新的预测值计算下一次迭代的梯度和 Hessian：
- en: '![](../Images/513f2ab1f312855900f2c8a5d265d5a4.png)'
  id: totrans-168
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/513f2ab1f312855900f2c8a5d265d5a4.png)'
- en: 'We now build a second XGBoost tree. Following the same process as in the previous
    iteration, we get the following tree (verify that this is indeed the correct tree!):'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在构建第二棵 XGBoost 树。按照与前一个迭代相同的过程，我们得到以下树（请确认这确实是正确的树！）：
- en: '![](../Images/b480d898772d74066f3830638d6f95bd.png)'
  id: totrans-170
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b480d898772d74066f3830638d6f95bd.png)'
- en: 'We now scale the predictions of the second tree by the learning rate and add
    them to the predictions of the previous ensemble:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们通过学习率来缩放第二棵树的预测值，并将其添加到之前的集成预测中：
- en: '![](../Images/ff9ab8fffc00d814a6bfdee7d4e7a989.png)'
  id: totrans-172
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ff9ab8fffc00d814a6bfdee7d4e7a989.png)'
- en: We can see that after three iterations, our ensemble correctly classifies all
    the samples in the training set!
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到经过三次迭代后，我们的集成模型正确地对训练集中的所有样本进行了分类！
- en: The keen reader may have noticed that the resulting ensemble is exactly the
    same ensemble we have obtained with the classical, unextreme gradient boosting
    algorithm. This is not surprising, since in classification problems the classical
    algorithm also uses a second-order approximation to optimize the log loss function.
    The advantage of XGBoost over the classical algorithm is that it allows us to
    incorporate a regularization coefficient as part of the objective function (which
    we did not take advantage of in this example).
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 细心的读者可能会注意到，得到的集成与我们使用经典的非极端梯度提升算法得到的集成完全相同。这并不令人惊讶，因为在分类问题中，经典算法也使用二阶近似来优化对数损失函数。XGBoost
    相对于经典算法的优势在于，它允许我们将正则化系数作为目标函数的一部分（在这个例子中我们没有利用这一点）。
- en: Final Notes
  id: totrans-175
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 最终说明
- en: All the images are by the author unless stated otherwise.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 除非另有说明，所有图片均由作者提供。
- en: Thanks for reading!
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 感谢阅读！
- en: References
  id: totrans-178
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] Chen, T., & Guestrin, C. (2016). XGBoost: A scalable tree boosting system.
    *Proceedings of the 22nd acm sigkdd international conference on knowledge discovery
    and data mining*, 785–794.'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] Chen, T., & Guestrin, C. (2016). XGBoost：一个可扩展的树提升系统。*第22届 ACM SIGKDD 国际知识发现与数据挖掘大会论文集*，785–794。'
- en: '[2] Grinsztajn, L., Oyallon, E., & Varoquaux, G. (2022). Why do tree-based
    models still outperform deep learning on typical tabular data?. *Advances in Neural
    Information Processing Systems*, 35: 507–520.'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] Grinsztajn, L., Oyallon, E., & Varoquaux, G. (2022). 为什么基于树的模型在典型的表格数据上仍然优于深度学习？
    *神经信息处理系统进展*，35：507–520。'
- en: '[3] Shwartz-Ziv, R., & Armon, A. (2022). Tabular data: Deep learning is not
    all you need. *Information Fusion*, 81: 84–90.'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] Shwartz-Ziv, R., & Armon, A. (2022). 表格数据：深度学习并非你所需的一切。*信息融合*，81：84–90。'
