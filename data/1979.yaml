- en: 'Beware of Unreliable Data in Model Evaluation: A LLM Prompt Selection case
    study with Flan-T5'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 注意在模型评估中数据的不可靠性：一个关于Flan-T5的LLM提示选择案例研究
- en: 原文：[https://towardsdatascience.com/beware-of-unreliable-data-in-model-evaluation-a-llm-prompt-selection-case-study-with-flan-t5-88cfd469d058?source=collection_archive---------9-----------------------#2023-06-16](https://towardsdatascience.com/beware-of-unreliable-data-in-model-evaluation-a-llm-prompt-selection-case-study-with-flan-t5-88cfd469d058?source=collection_archive---------9-----------------------#2023-06-16)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/beware-of-unreliable-data-in-model-evaluation-a-llm-prompt-selection-case-study-with-flan-t5-88cfd469d058?source=collection_archive---------9-----------------------#2023-06-16](https://towardsdatascience.com/beware-of-unreliable-data-in-model-evaluation-a-llm-prompt-selection-case-study-with-flan-t5-88cfd469d058?source=collection_archive---------9-----------------------#2023-06-16)
- en: You may choose suboptimal prompts for your LLM (or make other suboptimal choices
    via model evaluation) unless you clean your test data
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 除非你清理测试数据，否则你可能会为你的LLM选择次优提示（或通过模型评估做出其他次优选择）。
- en: '[](https://medium.com/@chrismauck10?source=post_page-----88cfd469d058--------------------------------)[![Chris
    Mauck](../Images/d0aeb4d0458544afdfdd59915a962b18.png)](https://medium.com/@chrismauck10?source=post_page-----88cfd469d058--------------------------------)[](https://towardsdatascience.com/?source=post_page-----88cfd469d058--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----88cfd469d058--------------------------------)
    [Chris Mauck](https://medium.com/@chrismauck10?source=post_page-----88cfd469d058--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@chrismauck10?source=post_page-----88cfd469d058--------------------------------)[![Chris
    Mauck](../Images/d0aeb4d0458544afdfdd59915a962b18.png)](https://medium.com/@chrismauck10?source=post_page-----88cfd469d058--------------------------------)[](https://towardsdatascience.com/?source=post_page-----88cfd469d058--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----88cfd469d058--------------------------------)
    [Chris Mauck](https://medium.com/@chrismauck10?source=post_page-----88cfd469d058--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F96a38f7ac238&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbeware-of-unreliable-data-in-model-evaluation-a-llm-prompt-selection-case-study-with-flan-t5-88cfd469d058&user=Chris+Mauck&userId=96a38f7ac238&source=post_page-96a38f7ac238----88cfd469d058---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----88cfd469d058--------------------------------)
    ·10 min read·Jun 16, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F88cfd469d058&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbeware-of-unreliable-data-in-model-evaluation-a-llm-prompt-selection-case-study-with-flan-t5-88cfd469d058&user=Chris+Mauck&userId=96a38f7ac238&source=-----88cfd469d058---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F96a38f7ac238&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbeware-of-unreliable-data-in-model-evaluation-a-llm-prompt-selection-case-study-with-flan-t5-88cfd469d058&user=Chris+Mauck&userId=96a38f7ac238&source=post_page-96a38f7ac238----88cfd469d058---------------------post_header-----------)
    发布于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----88cfd469d058--------------------------------)
    ·10 min阅读·2023年6月16日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F88cfd469d058&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbeware-of-unreliable-data-in-model-evaluation-a-llm-prompt-selection-case-study-with-flan-t5-88cfd469d058&user=Chris+Mauck&userId=96a38f7ac238&source=-----88cfd469d058---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F88cfd469d058&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbeware-of-unreliable-data-in-model-evaluation-a-llm-prompt-selection-case-study-with-flan-t5-88cfd469d058&source=-----88cfd469d058---------------------bookmark_footer-----------)![](../Images/1fba4029952f6fbbdc9ad048d189e4e1.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F88cfd469d058&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbeware-of-unreliable-data-in-model-evaluation-a-llm-prompt-selection-case-study-with-flan-t5-88cfd469d058&source=-----88cfd469d058---------------------bookmark_footer-----------)![](../Images/1fba4029952f6fbbdc9ad048d189e4e1.png)'
- en: 'Credit: Arthur Osipyan, Unsplash'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 版权：Arthur Osipyan, Unsplash
- en: '*Authors: Chris Mauck, Jonas Mueller*'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '*作者：Chris Mauck, Jonas Mueller*'
- en: Reliable **model evaluation** lies at the heart of MLops and LLMops, guiding
    crucial decisions like which model or prompt to deploy (and whether to deploy
    at all). In this article, we prompt the FLAN-T5 LLM from [Google Research](https://ai.googleblog.com/2021/10/introducing-flan-more-generalizable.html)
    with various prompts in an effort to classify text as polite or impolite. Amongst
    the prompt candidates, we find the prompts that appear to perform best based on
    observed test accuracy are often *actually* *worse* than other prompt candidates.
    A closer review of the test data reveals this is due to unreliable annotations.
    **In real-world applications, you may choose suboptimal prompts for your LLM (or
    make other suboptimal choices guided by model evaluation) unless you clean your
    test data to ensure it is reliable.**
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 可靠的**模型评估**是MLops和LLMops的核心，指导关键决策，例如选择哪个模型或提示（以及是否部署）。在本文中，我们使用[Google Research](https://ai.googleblog.com/2021/10/introducing-flan-more-generalizable.html)的FLAN-T5
    LLM对各种提示进行测试，以将文本分类为礼貌或不礼貌。在提示候选中，我们发现基于观察到的测试准确性表现最好的提示往往*实际上*比其他提示候选*更差*。对测试数据的仔细审查揭示了这是由于不可靠的注释所致。**在实际应用中，除非你清理你的测试数据以确保其可靠性，否则你可能会为你的LLM选择次优提示（或做出其他由模型评估指导的次优选择）。**
- en: '![](../Images/37a5fbd75a640b7e870f173c88982f6a.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/37a5fbd75a640b7e870f173c88982f6a.png)'
- en: Selecting great prompts is essential for ensuring accurate responses from Large
    Language Models.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 选择优秀的提示对于确保大型语言模型的准确响应至关重要。
- en: While the harms of noisy annotations are well-characterized in training data,
    this article demonstrates their often-overlooked consequences in test data.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然噪声注释在训练数据中的危害已经得到了充分的描述，但本文展示了它们在测试数据中常被忽视的后果。
- en: I am currently a data scientist at [Cleanlab](https://cleanlab.ai/) and I’m
    excited to share the importance of (and how you can ensure) high-quality test
    data to ensure optimal LLM prompt selection.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 我目前是[Cleanlab](https://cleanlab.ai/)的一名数据科学家，我很高兴分享高质量测试数据的重要性（以及如何确保高质量测试数据），以确保最佳的LLM提示选择。
- en: Overview
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 概述
- en: You can download the data [here](https://s.cleanlab.ai/stanford-politeness-prompt-selection.csv).
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以[在这里](https://s.cleanlab.ai/stanford-politeness-prompt-selection.csv)下载数据。
- en: This article studies a binary classification variant of the [Stanford Politeness
    Dataset](https://convokit.cornell.edu/documentation/wiki_politeness.html) (used
    under [CC BY license v4.0](https://creativecommons.org/licenses/by/4.0/)), which
    has text phrases labeled as *polite* or *impolite*. We evaluate models using a
    fixed test dataset containing 700 phrases.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 本文研究了[斯坦福礼貌数据集](https://convokit.cornell.edu/documentation/wiki_politeness.html)的二分类变体（使用[CC
    BY 4.0许可证](https://creativecommons.org/licenses/by/4.0/)），该数据集中的文本短语被标记为*礼貌*或*不礼貌*。我们使用包含700个短语的固定测试数据集来评估模型。
- en: '![](../Images/1ade1f412d93083e70b3a78a8403338e.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1ade1f412d93083e70b3a78a8403338e.png)'
- en: Snapshot of the dataset showing the text and ground truth politeness label.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集快照，显示了文本和真实的礼貌标签。
- en: 'It is standard practice to evaluate how “good” a classification model is by
    measuring the accuracy of its predictions against the given labels for examples
    the model did not see during training, usually referred to as “test”, “evaluation”,
    or “validation” data. This provides a numerical metric to gauge how good model
    A is against model B — if model A displays higher test accuracy, we estimate it
    to be the better model and would choose to deploy it over model B. Beyond model
    selection, the same decision-making framework can be applied to other choices
    like whether to use: hyperparameter-setting A or B, prompt A or B, feature-set
    A or B, etc.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 评估分类模型“好坏”的标准做法是通过测量其对未在训练期间见过的样本的预测准确性来进行，这些样本通常被称为“测试”、“评估”或“验证”数据。这提供了一个数值指标来衡量模型A相对于模型B的优劣——如果模型A表现出更高的测试准确率，我们估计它是更好的模型，并会选择部署它而不是模型B。除了模型选择之外，类似的决策框架也可以应用于其他选择，例如使用超参数设置A或B、提示A或B、特征集A或B等。
- en: A [common problem](https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/file/f2217062e9a397a1dca429e7d70bc6ca-Paper-round1.pdf)
    in real-world test data is some examples have incorrect labels, whether due to
    human annotation error, data processing error, sensor noise, etc. In such cases,
    test accuracy becomes a less reliable indicator of the **relative performance**
    between model A and model B. Let’s use a very simple example to illustrate this.
    Imagine your test dataset has two examples of *impolite* text, but **unknowingly
    to you**, they are (mis)labeled as *polite*. For instance, in our Stanford Politeness
    dataset, we see an actual human annotator mistakenly labeled this text “*Are you
    crazy down here?! What the heck is going on?*” as *polite* when the language is
    clearly agitated. Now your job is to pick the best model to classify these examples.
    Model A says that both examples are *impolite* and model B says both examples
    are *polite*. Based on these (incorrect) labels, model A scores 0% while model
    B scores 100% — you pick model B to deploy! But wait, which model *actually* is
    stronger?
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '[常见问题](https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/file/f2217062e9a397a1dca429e7d70bc6ca-Paper-round1.pdf)在现实世界的测试数据中，某些示例由于人为注释错误、数据处理错误、传感器噪声等原因具有错误标签。在这种情况下，测试准确度成为评估模型A和模型B之间**相对性能**不太可靠的指标。让我们举一个非常简单的例子来说明这一点。想象一下，你的测试数据集中有两个*不礼貌*文本示例，但你不知道它们被错误地标记为*礼貌*。例如，在我们的斯坦福礼貌数据集中，我们看到一个实际的人类注释者错误地将这段文本“*你在这里疯了吗？到底发生了什么？*”标记为*礼貌*，而语言显然是激动的。现在你的工作是选择最好的模型来分类这些示例。模型A说这两个示例都是*不礼貌*，而模型B说这两个示例都是*礼貌*。基于这些（错误的）标签，模型A得分为0%，而模型B得分为100%
    — 你选择了模型B来部署！但等等，哪个模型才*实际上*更强大？'
- en: Although these implications are trivial and many are aware that real-world data
    is full of labeling errors, folks often focus only on noisy labels in their training
    data, forgetting to carefully curate their test data even though it guides crucial
    decisions. Using real data, this article illustrates the importance of high-quality
    test data to guide the choice of LLM prompts and demonstrates one way to easily
    improve data quality via algorithmic techniques.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这些含义微不足道，许多人知道现实世界数据充满标记错误，但人们往往只关注其训练数据中的嘈杂标签，却忘记仔细筛选其测试数据，尽管后者指导了重要决策。通过真实数据，本文说明了高质量测试数据对指导LLM提示选择的重要性，并演示了通过算法技术轻松提高数据质量的一种方式。
- en: Observed Test Accuracy vs Clean Test Accuracy
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 观察到的测试准确度与清洁测试准确度
- en: Here we consider two possible test sets constructed out of the same set of text
    examples which only differ in some (~30%) of the labels. Representing typical
    data you’d use to evaluate accuracy, one version has labels sourced from a single
    annotation (human rater) per example, and we report the accuracy of model predictions
    computed on this version as *Observed Test Accuracy*. A second *cleaner* version
    of this same test set has high-quality labels established via consensus amongst
    many agreeing annotations per example (derived from multiple human raters). We
    report accuracy measured on the cleaner version as *Clean Test Accuracy*. Thus,
    *Clean Test Accuracy* more closely reflects what you care about (actual model
    deployment performance), but the *Observed Test Accuracy* is all you get to observe
    in most applications — unless you first clean your test data!
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 这里我们考虑由同一组文本示例构成的两个可能的测试集，只有在某些示例（约30%）的标签上有所不同。代表您用来评估准确性的典型数据，一个版本的标签来源于每个示例的单个注释（人类评分者），我们将计算在该版本上模型预测的准确率作为*观察到的测试准确度*。同一测试集的第二个*更清洁*版本具有通过多个人类评分者达成一致共识而建立的高质量标签。我们将在更干净的版本上测量的准确性称为*清洁测试准确度*。因此，*清洁测试准确度*更接近您关心的内容（实际模型部署性能），但*观察到的测试准确度*是大多数应用中您只能观察到的
    —— 除非您先清理您的测试数据！
- en: Below are two test examples where the single human annotator mislabeled the
    example, but the group of many human annotators agreed on the correct label.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是两个测试示例，单个人类注释者误标示例，但许多人类注释者组同意正确标记。
- en: '![](../Images/f46afa1e51716d912df4d0b3c5c74ae9.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f46afa1e51716d912df4d0b3c5c74ae9.png)'
- en: The orange annotations collected from a single annotator are cheaper to collect,
    but oftentimes incorrect. The blue annotations are collected from multiple annotators
    which are more expensive, but usually more accurate.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 从单个注释者收集的橙色注释成本更低，但常常是错误的。从多个注释者收集的蓝色注释成本更高，但通常更准确。
- en: In real-world projects, you often don’t have access to such “clean” labels,
    so you can only measure *Observed Test Accuracy*. If you are making critical decisions
    such as which LLM or prompt to use based on this metric, be sure to first verify
    the labels are high-quality. Otherwise, we find you may make the **wrong decisions**,
    as observed below when selecting prompts for politeness classification.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在现实项目中，您通常无法获得这样的“干净”标签，因此您只能测量*观察测试准确性*。如果您根据这个度量来做出诸如使用哪个LLM或提示的关键决策，请务必首先验证标签的质量。否则，我们发现您可能会做出错误的决定，如在选择礼貌分类提示时观察到的那样。
- en: Impact of Noisy Evaluation Data
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 噪声评估数据的影响
- en: As a predictive model to classify the politeness of text, it is natural to employ
    a pretrained Large Language Model (LLM). Here, we specifically use data scientists’
    favorite LLM — the open-source FLAN-T5 model. To get this LLM to accurately predict
    the politeness of text, we must feed it just the right prompts. Prompt engineering
    can be very sensitive, with small changes greatly affecting accuracy!
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 作为分类文本礼貌的预测模型，自然而然地使用预训练的大型语言模型（LLM）是合理的。在这里，我们特别使用数据科学家最喜爱的开源FLAN-T5模型。要让这个LLM准确预测文本的礼貌，我们必须提供恰到好处的提示。提示工程可能非常敏感，小的改变会极大地影响准确性！
- en: Prompts A and B shown below (highlighted text) are two different examples of
    *chain-of-thought* prompts, that can be appended in front of any **text sample**
    in order to get the LLM to classify its politeness. These prompts combine *few-shot*
    and *instruction* prompts (details later) that provide examples, the correct response,
    and a justification that encourages the LLM to explain its reasoning. The only
    difference between these two prompts is the highlighted text that is actually
    eliciting a response from the LLM. The few-shot examples and reasoning remain
    the same.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 下面显示的 Prompt A 和 Prompt B（高亮文本）是两个不同的*逻辑链式*提示示例，可以附加在任何**文本样本**前面，以使LLM对其礼貌进行分类。这些提示结合了*少样本*和*指令*提示（稍后详细介绍），提供示例、正确的响应和鼓励LLM解释其推理的理由。这两个提示唯一的区别是实际上在引起LLM回应的高亮文本。少样本示例和推理保持不变。
- en: '![](../Images/313afddfe3d4b4c04410243235b488ea.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/313afddfe3d4b4c04410243235b488ea.png)'
- en: chain-of-thought prompts provide the model with reasoning as to why the answer
    is correct for each text example given.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑链式提示为模型提供了每个文本示例为什么正确的推理。
- en: The natural way to decide which prompt is better is based on their *Observed
    Test Accuracy.* When used to prompt the FLAN-T5 LLM, we see below that the classifications
    produced by Prompt A have higher *Observed Test Accuracy* on the original test
    set than those from Prompt B. So obviously we should deploy our LLM with Prompt
    A, *right*? Not so fast!
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 决定哪个提示更好的自然方法是基于它们的*观察测试准确性*。当用于提示 FLAN-T5 LLM 时，我们可以看到 Prompt A 生成的分类在原始测试集上比
    Prompt B 的*观察测试准确性*更高。所以显然我们应该部署我们的LLM与 Prompt A，*对吗*？不要那么快！
- en: When we assess the *Clean Test Accuracy* of each prompt, we find that Prompt
    B is actually **much better** than Prompt A (by 4.5 percentage points). Since
    *Clean Test Accuracy* more closely reflects the true performance we actually care
    about, we would’ve made the wrong decision if we just relied on the original test
    data without examining its label quality!
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们评估每个提示的*干净测试准确性*时，我们发现 Prompt B 实际上比 Prompt A **好得多**（高出4.5个百分点）。由于*干净测试准确性*更接近我们实际关心的真实表现，如果我们仅仅依赖于原始测试数据而不检查其标签质量，我们将会做出错误的决定！
- en: '![](../Images/595291e25668a8dcb82a4ad60c234202.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/595291e25668a8dcb82a4ad60c234202.png)'
- en: Using the observed accuracy, you would select Prompt A as better. Prompt B is
    actually the better prompt when evaluated on the clean test set.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 使用观察到的准确性，您会选择 Prompt A 更好。在干净的测试集上评估时，Prompt B 实际上是更好的提示。
- en: Is this just statistical fluctuation?
  id: totrans-38
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 这只是统计波动吗？
- en: '[McNemar’s test](https://en.wikipedia.org/wiki/McNemar%27s_test) is a recommended
    way to assess the statistical significance of reported differences in ML accuracy.
    When we apply this test to assess the 4.5% difference in *Clean Test Accuracy*
    between Prompt A vs. B over our 700 text examples, the difference is highly statistically
    significant (p-value = 0.007, *X²* = 7.086). Thus all evidence suggests Prompt
    B is a meaningfully better choice — we should not have failed to select it by
    carefully auditing our original test data!'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '[McNemar检验](https://en.wikipedia.org/wiki/McNemar%27s_test)是一种评估ML准确性报告差异统计显著性的方法。当我们应用此检验评估Prompt
    A与B在700个文本示例中的*清洁测试准确性*的4.5%差异时，差异具有高度统计显著性（p值 = 0.007，*X²* = 7.086）。因此，所有证据表明Prompt
    B是一个意义深远的更好选择——我们不应未能通过仔细审计原始测试数据而未选择它！'
- en: Is this a fluke result that just happened to be the case for these two prompts?
  id: totrans-40
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 这是一个偶然结果，还是仅仅发生在这两个提示上？
- en: Let’s look at other types of prompts as well to see if the results were just
    coincidental for our pair of chain-of-thought prompts.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看其他类型的提示，以了解结果是否只是我们一对思维链提示的巧合。
- en: Instruction Prompts
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 指令提示
- en: This type of prompt simply provides an *instruction* to the LLM on what it needs
    to do with the text example given. Consider the following pair of such prompts
    we might want to choose between.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 这种类型的提示仅向LLM提供一个*指令*，说明它需要对给定的文本示例做什么。考虑以下我们可能想要选择的这种提示对。
- en: '![](../Images/6bb28fb6b3a06ad2cdc804d9261be357.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6bb28fb6b3a06ad2cdc804d9261be357.png)'
- en: Few-Shot Prompts
  id: totrans-45
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 少量样本提示
- en: This type of prompt uses two instructions, a *prefix,* and a *suffix,* and also
    includes two (pre-selected) examples from the text corpus to provide clear demonstrations
    to the LLM of the desired input-output mapping. Consider the following pair of
    such prompts we might want to choose between.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 这种类型的提示使用两个指令，一个*前缀*和一个*后缀*，还包括两个（预先选择的）文本示例，以向LLM提供所需的输入-输出映射的清晰演示。考虑以下我们可能想要选择的这种提示对。
- en: '![](../Images/c9c06c3ea41b82767cdb0a012772d02f.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c9c06c3ea41b82767cdb0a012772d02f.png)'
- en: Templatized Prompts
  id: totrans-48
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模板化提示
- en: This type of prompt uses two instructions, an optional *prefix,* and a *suffix,*
    in addition to multiple-choice formatting so that the model performs classification
    as a multiple-choice answer rather than responding directly with a predicted class.
    Consider the following pair of such prompts we might want to choose between.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 这种类型的提示使用两个指令，一个可选的*前缀*和一个*后缀*，以及多选格式，以便模型作为多项选择答案进行分类，而不是直接以预测的类别进行响应。考虑以下我们可能想要选择的这种提示对。
- en: '![](../Images/9cf3c405b4de34cea5b1a75e418216e8.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9cf3c405b4de34cea5b1a75e418216e8.png)'
- en: Results for various types of prompts
  id: totrans-51
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 各种类型提示的结果
- en: Beyond chain-of-thought, we also evaluated the classification performance of
    the same FLAN-T5 LLM with these three additional types of prompts. Plotting the
    *Observed Test Accuracy* vs. *Clean Test Accuracy* achieved with all of these
    prompts below, we see many pairs of prompts that suffer from the same aforementioned
    problem, where relying on *Observed Test Accuracy* leads to selecting the prompt
    that is actually worse.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 除了思维链外，我们还评估了相同FLAN-T5 LLM在这三种额外类型提示下的分类性能。绘制所有这些提示下的*观察测试准确性*与*清洁测试准确性*，我们看到许多提示对存在相同的上述问题，即依赖于*观察测试准确性*导致选择实际更差的提示。
- en: '![](../Images/be3a032c2c15f9de4ec3702dd35c282b.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/be3a032c2c15f9de4ec3702dd35c282b.png)'
- en: As a prompt engineer using the available test data, you would choose the gray
    A prompt in the upper left (highest observed accuracy) yet the optimal prompt
    is actually the gray B in the upper right (highest clean accuracy).
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一个提示工程师，使用可用的测试数据，你会选择左上角的灰色A提示（最高观察准确性），然而最佳提示实际上是右上角的灰色B（最高清洁准确性）。
- en: Based on solely the *Observed Test Accuracy*, you would be inclined to select
    each of the “A” prompts over the “B” prompts amongst each type of prompt. However,
    the better prompt for each of the prompt types is actually prompt B (which has
    higher *Clean Test Accuracy*). **Each of these prompt pairs highlights the need
    to verify test data quality, otherwise, you can make suboptimal decisions due
    to data issues like noisy annotations.**
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 基于*观察测试准确性*，你会倾向于选择每种类型提示中的“A”提示而不是“B”提示。然而，每种提示类型的更好提示实际上是提示B（具有更高的*清洁测试准确性*）。**这些提示对突显了验证测试数据质量的必要性，否则由于数据问题如噪声注释，你可能会做出次优决策。**
- en: '![](../Images/da676344e2573dad4730dae95fb885ca.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/da676344e2573dad4730dae95fb885ca.png)'
- en: All of the A prompts appear to be better due to their higher observed accuracy,
    yet all of the B prompts are objectively better when evaluated on the ground truth
    test data.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 所有A提示看起来由于其更高的观测准确度而表现更好，但在基于真实数据测试时，所有B提示在客观上表现更好。
- en: You can also see in this graphic how all of the A prompts observed accuracies
    are circled, meaning that they have higher accuracies than their B counterparts.
    Similarly, all of the B prompts clean accuracies are circled, meaning that they
    have higher accuracies than their B counterparts. Just like the simple example
    at the beginning of this article, you would be inclined to pick all of the A prompts,
    when in actuality the B prompts do a much better job.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个图示中，你还可以看到所有A提示的观测准确度被圈了起来，这意味着它们的准确度比B提示更高。类似地，所有B提示的清理准确度也被圈了起来，这意味着它们的准确度比A提示更高。就像本文开头的简单示例一样，你可能倾向于选择所有A提示，但实际上B提示的表现要好得多。
- en: Improving Available Test Data for More Reliable Evaluation
  id: totrans-59
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 改进可用测试数据以获得更可靠的评估
- en: Hopefully, the importance of high-quality evaluation data is clear. Let’s look
    at a couple of ways you could go about fixing the available test data.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 希望你能明白高质量评估数据的重要性。让我们来看几个修正可用测试数据的方法。
- en: Manual Correction
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 手动修正
- en: The easiest way to ensure the quality of your test data is to simply review
    it by hand! Make sure to look through each of the examples to verify it is labeled
    correctly. Depending on the size of your test set, this may or may not be feasible.
    If your test set is relatively small (~100 examples) you could just look through
    them and make any corrections necessary. If your test set is large (1000+ examples),
    this would be too time-consuming and mentally to taxing to do by hand. Our test
    set is quite large, so we won’t be using this method!
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 确保测试数据质量的最简单方法是手动检查！确保逐一查看每个示例，验证其标签是否正确。根据测试集的大小，这可能可行也可能不可行。如果测试集相对较小（约100个示例），你可以查看它们并进行必要的修正。如果测试集较大（1000+示例），手动完成这一任务将过于耗时且心理负担过重。我们的测试集相当大，所以我们不会使用这种方法！
- en: Algorithmic Correction
  id: totrans-63
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 算法修正
- en: Another way to assess your available (possibly noisy) test set is to use data-centric
    AI algorithms in order to diagnose issues that can be fixed to obtain a more reliable
    version of the same dataset (without having to collect many additional human annotations).
    Here we use Confident Learning algorithms (via the open-source [cleanlab](https://github.com/cleanlab/cleanlab)
    package) to check our test data, which automatically estimate which examples appear
    to be mislabeled. We then inspect only these auto-detected label issues and fix
    their labels as needed to produce a higher-quality version of our test dataset.
    We call model accuracy measurements made over this version of the test dataset,
    the *CL Test Accuracy.*
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 评估你的可用（可能有噪声的）测试集的另一种方法是使用以数据为中心的AI算法来诊断可以修正的问题，以获得更可靠的同一数据集版本（无需收集许多额外的人类注释）。在这里，我们使用Confident
    Learning算法（通过开源的[cleanlab](https://github.com/cleanlab/cleanlab)包）来检查我们的测试数据，这些算法自动估计哪些示例可能被错误标记。然后我们仅检查这些自动检测的标签问题，并根据需要修正其标签，以产生更高质量的测试数据集版本。我们将对这个版本的测试数据集进行的模型准确度测量称为*CL测试准确度*。
- en: '![](../Images/442aa7374c7c2f78afb7370499bd246c.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/442aa7374c7c2f78afb7370499bd246c.png)'
- en: The CL test accuracy is greater for all of the B prompts. Using CL we corrected
    the original test data and now can trust our model and prompt decisions.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 所有B提示的CL测试准确度都更高。通过CL我们纠正了原始测试数据，现在可以信任我们的模型和提示决策。
- en: Using this new CL-corrected test set for model evaluation, we see that all of
    the B prompts from before now properly display higher accuracy than their A counterparts.
    This means we can trust our decisions made based on the CL-corrected test set
    to be more reliable than those made based on the noisy original test data.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这个新的CL纠正测试集进行模型评估，我们发现之前的所有B提示现在准确度正确地高于A提示。这意味着我们可以信任基于CL纠正测试集做出的决策，比基于原始噪声测试数据做出的决策更可靠。
- en: Of course, Confident Learning cannot magically identify all errors in any dataset.
    How well this algorithm detects labeling errors will depend on having reasonable
    predictions from a baseline ML model and even then, certain types of systematically-introduced
    errors will remain undetectable (for instance if we swap the definition of two
    classes entirely). For the precise list of mathematical assumptions under which
    Confident Learning can be proven effective, refer to the [original paper by Northcutt
    et al.](https://dl.acm.org/doi/10.1613/jair.1.12125) For many real-world text/image/audio/tabular
    datasets, this algorithm appears to at least offer an effective way to focus limited
    data reviewing resources on the most suspicious examples lurking in a large dataset.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，Confident Learning 并不能神奇地识别数据集中所有的错误。这个算法检测标签错误的效果将依赖于基线 ML 模型的合理预测，即便如此，某些系统性引入的错误仍然无法被检测到（例如，如果我们完全交换两个类别的定义）。有关
    Confident Learning 在何种数学假设下被证明有效的详细列表，请参考 [Northcutt 等人原始论文](https://dl.acm.org/doi/10.1613/jair.1.12125)。对于许多现实世界的文本/图像/音频/表格数据集，这个算法至少提供了一种有效的方法，将有限的数据审查资源集中在大数据集中最可疑的例子上。
- en: '**You don’t always need to spend the time/resources to curate a “perfect” evaluation
    set — using algorithms like Confident Learning to diagnose and correct possible
    issues in your available test set can provide high-quality data to ensure optimal
    prompt and model selections.**'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '**你不总是需要花费时间/资源来创建一个“完美”的评估集——使用像 Confident Learning 这样的算法来诊断和修正你现有测试集中的潜在问题，可以提供高质量的数据，以确保最佳的提示和模型选择。**'
- en: '*All images unless otherwise noted are by the author.*'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '*除非另有说明，否则所有图像均为作者提供。*'
