- en: 'How ChatGPT Works: The Model Behind The Bot'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ChatGPT 的工作原理：聊天机器人背后的模型
- en: 原文：[https://towardsdatascience.com/how-chatgpt-works-the-models-behind-the-bot-1ce5fca96286](https://towardsdatascience.com/how-chatgpt-works-the-models-behind-the-bot-1ce5fca96286)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/how-chatgpt-works-the-models-behind-the-bot-1ce5fca96286](https://towardsdatascience.com/how-chatgpt-works-the-models-behind-the-bot-1ce5fca96286)
- en: A brief introduction to the intuition and methodology behind the chat bot you
    can’t stop hearing about.
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 对你听说过的聊天机器人背后的直觉和方法的简要介绍。
- en: '[](https://medium.com/@molly.ruby?source=post_page-----1ce5fca96286--------------------------------)[![Molly
    Ruby](../Images/2a493bd01057722138857a90035347cd.png)](https://medium.com/@molly.ruby?source=post_page-----1ce5fca96286--------------------------------)[](https://towardsdatascience.com/?source=post_page-----1ce5fca96286--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----1ce5fca96286--------------------------------)
    [Molly Ruby](https://medium.com/@molly.ruby?source=post_page-----1ce5fca96286--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@molly.ruby?source=post_page-----1ce5fca96286--------------------------------)[![Molly
    Ruby](../Images/2a493bd01057722138857a90035347cd.png)](https://medium.com/@molly.ruby?source=post_page-----1ce5fca96286--------------------------------)[](https://towardsdatascience.com/?source=post_page-----1ce5fca96286--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----1ce5fca96286--------------------------------)
    [Molly Ruby](https://medium.com/@molly.ruby?source=post_page-----1ce5fca96286--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----1ce5fca96286--------------------------------)
    ·9 min read·Jan 30, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----1ce5fca96286--------------------------------)
    ·阅读时间 9 分钟·2023年1月30日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/0e4bc81c17a6711eda750f7e522f100e.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0e4bc81c17a6711eda750f7e522f100e.png)'
- en: This gentle introduction to the machine learning models that power ChatGPT,
    will start at the introduction of Large Language Models, dive into the revolutionary
    self-attention mechanism that enabled GPT-3 to be trained, and then burrow into
    Reinforcement Learning From Human Feedback, the novel technique that made ChatGPT
    exceptional.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 这篇关于驱动 ChatGPT 的机器学习模型的温和介绍，将从大语言模型的介绍开始，深入探讨使 GPT-3 能够训练的革命性自注意力机制，然后深入挖掘使
    ChatGPT 异常出色的强化学习技术。
- en: Large Language Models
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 大语言模型
- en: ChatGPT is an extrapolation of a class of machine learning Natural Language
    Processing models known as Large Language Model (LLMs). LLMs digest huge quantities
    of text data and infer relationships between words within the text. These models
    have grown over the last few years as we’ve seen advancements in computational
    power. LLMs increase their capability as the size of their input datasets and
    parameter space increase.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: ChatGPT 是一种自然语言处理模型的扩展，称为大语言模型（LLMs）。LLMs 消化大量的文本数据，并推断文本中的单词之间的关系。随着计算能力的进步，这些模型在过去几年里得到了发展。LLMs
    随着输入数据集和参数空间的增大而提高其能力。
- en: The most basic training of language models involves predicting a word in a sequence
    of words. Most commonly, this is observed as either next-token-prediction and
    masked-language-modeling.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 语言模型的最基本训练涉及在单词序列中预测一个单词。最常见的是下一词预测和掩码语言建模。
- en: '![](../Images/190916d7e4a8d73ec41e379347e46df9.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/190916d7e4a8d73ec41e379347e46df9.png)'
- en: Arbitrary example of next-token-prediction and masked-language-modeling generated
    by the author.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 作者生成的下一词预测和掩码语言建模的任意示例。
- en: In this basic sequencing technique, often deployed through a Long-Short-Term-Memory
    (LSTM) model, the model is filling in the blank with the most statistically probable
    word given the surrounding context. There are two major limitations with this
    sequential modeling structure.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种基本的序列技术中，通常通过长短期记忆（LSTM）模型实现，模型根据周围的上下文填入最有统计概率的单词。这种序列建模结构有两个主要的限制。
- en: The model is unable to value some of the surrounding words more than others.
    In the above example, while ‘reading’ may most often associate with ‘hates’, in
    the database ‘Jacob’ may be such an avid reader that the model should give more
    weight to ‘Jacob’ than to ‘reading’ and choose ‘love’ instead of ‘hates’.
  id: totrans-14
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 模型无法对一些周围的单词赋予比其他单词更多的权重。在上述示例中，虽然‘reading’通常会与‘hates’相关联，但在数据库中‘Jacob’可能是如此热衷于阅读，以至于模型应给予‘Jacob’比‘reading’更多的权重，从而选择‘love’而不是‘hates’。
- en: The input data is processed individually and sequentially rather than as a whole
    corpus. This means that when an LSTM is trained, the window of context is fixed,
    extending only beyond an individual input for several steps in the sequence. This
    limits the complexity of the relationships between words and the meanings that
    can be derived.
  id: totrans-15
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 输入数据是单独且顺序处理的，而不是作为整个语料库处理。这意味着，当LSTM被训练时，上下文窗口是固定的，仅在序列中的几个步骤之外扩展。这限制了单词之间的关系的复杂性和可以推导出的意义。
- en: In response to this issue, in 2017 a team at Google Brain introduced transformers.
    Unlike LSTMs, transformers can process all input data simultaneously. Using a
    self-attention mechanism, the model can give varying weight to different parts
    of the input data in relation to any position of the language sequence. This feature
    enabled massive improvements in infusing meaning into LLMs and enables processing
    of significantly larger datasets.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 针对这个问题，2017年Google Brain团队引入了变换器。与LSTM不同，变换器可以同时处理所有输入数据。通过使用自注意力机制，模型可以根据语言序列的任何位置对输入数据的不同部分给予不同的权重。这一特性显著提升了将意义注入LLM的能力，并使得处理显著更大的数据集成为可能。
- en: GPT and Self-Attention
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: GPT和自注意力
- en: Generative Pre-training Transformer (GPT) models were first launched in 2018
    by openAI as GPT-1\. The models continued to evolve over 2019 with GPT-2, 2020
    with GPT-3, and most recently in 2022 with InstructGPT and ChatGPT. Prior to integrating
    human feedback into the system, the greatest advancement in the GPT model evolution
    was driven by achievements in computational efficiency, which enabled GPT-3 to
    be trained on significantly more data than GPT-2, giving it a more diverse knowledge
    base and the capability to perform a wider range of tasks.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 生成预训练变换器（GPT）模型最早由openAI在2018年推出，命名为GPT-1。之后，模型在2019年演变为GPT-2，2020年演变为GPT-3，最近在2022年推出了InstructGPT和ChatGPT。在将人类反馈整合到系统之前，GPT模型演变的最大进展是由计算效率的成就驱动的，这使得GPT-3能够在比GPT-2上训练更多的数据，从而拥有更广泛的知识基础和执行更广泛任务的能力。
- en: '![](../Images/27bb3cefc523b0279028e7691d6f0747.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/27bb3cefc523b0279028e7691d6f0747.png)'
- en: Comparison of GPT-2 (left) and GPT-3 (right). Generated by the author.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-2（左）和GPT-3（右）的比较。由作者生成。
- en: All GPT models largely follow the Transformer Architecture established in “Attention
    is All You Need” (Vaswani et al., 2017), which have an encoder to process the
    input sequence and a decoder to generate the output sequence. Both the encoder
    and decoder in the original Transformer have a multi-head self-attention mechanism
    that allows the model to differentially weight parts of the sequence to infer
    meaning and context. As an evolution to original Transformer, GPT models leverage
    a decoder-only transformer with masked self-attention heads, as established in
    Radford et al., 2018\. The architecture was further fine tuned through the works
    of Radford et al., 2019 and Brown et al., 2020\. The decoder-only framework was
    used because the main goal of GPT is to generate generate coherent and contextually
    relevant text. Autoregressive decoding, which is handled by the decoder, allows
    the model to maintain context and generate sequences one token at a time.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 所有GPT模型大体上遵循在“Attention is All You Need”（Vaswani et al., 2017）中建立的变换器架构，该架构包括一个编码器来处理输入序列和一个解码器来生成输出序列。在原始变换器中，编码器和解码器都有一个多头自注意力机制，使得模型能够对序列的不同部分赋予不同的权重，从而推断意义和上下文。作为对原始变换器的演变，GPT模型利用了Radford等人（2018）建立的仅解码器的变换器，采用了掩蔽自注意力头。该架构通过Radford等人（2019）和Brown等人（2020）的工作进一步微调。采用仅解码器的框架是因为GPT的主要目标是生成连贯且上下文相关的文本。自回归解码由解码器处理，使得模型能够保持上下文，并逐个标记生成序列。
- en: The self-attention mechanism that drives GPT works by converting tokens (pieces
    of text, which can be a word, sentence, or other grouping of text) into vectors
    that represent the importance of the token in the input sequence. To do this,
    the model,
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 驱动GPT的自注意力机制通过将标记（文本片段，可以是一个单词、句子或其他文本组）转换为向量，来表示标记在输入序列中的重要性。
- en: Creates a query, key, and value vector for each token in the input sequence.
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为输入序列中的每个标记创建查询、键和值向量。
- en: Calculates the similarity between the query vector from step one and the key
    vector of every other token by taking the dot product of the two vectors.
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过计算步骤一中的查询向量与每个其他标记的键向量之间的点积，计算它们之间的相似性。
- en: Generates normalized weights by feeding the output of step 2 into a [softmax
    function](https://deepai.org/machine-learning-glossary-and-terms/softmax-layer).
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过将步骤2的输出输入到[softmax函数](https://deepai.org/machine-learning-glossary-and-terms/softmax-layer)中生成标准化权重。
- en: Generates a final vector, representing the importance of the token within the
    sequence by multiplying the weights generated in step 3 by the value vectors of
    each token.
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 生成最终向量，通过将步骤3中生成的权重与每个令牌的值向量相乘，来表示令牌在序列中的重要性。
- en: The ‘multi-head’ attention mechanism that GPT uses is an evolution of self-attention.
    Rather than performing steps 1–4 once, in parallel the model iterates this mechanism
    several times, each time generating a new linear projection of the query, key,
    and value vectors. By expanding self-attention in this way, the model is capable
    of grasping sub-meanings and more complex relationships within the input data.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: GPT使用的‘多头’注意机制是自注意力的进化。模型并非仅执行步骤1-4一次，而是并行地多次迭代此机制，每次生成查询、键和值向量的新线性投影。通过这种方式扩展自注意力，模型能够理解输入数据中的子含义和更复杂的关系。
- en: '![](../Images/203d683689d56d1fe795bcdf1c72a21b.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/203d683689d56d1fe795bcdf1c72a21b.png)'
- en: Screenshot from ChatGPT generated by the author.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 作者生成的ChatGPT截图。
- en: Although GPT-3 introduced remarkable advancements in natural language processing,
    it is limited in its ability to align with user intentions. For example, GPT-3
    may produce outputs that
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管GPT-3在自然语言处理方面引入了显著的进展，但其在与用户意图对齐的能力上仍有限。例如，GPT-3可能会生成
- en: '**Lack of helpfulness** meaning they donot follow the user’s explicit instructions.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**缺乏帮助性**，意味着它们不遵循用户的明确指示。'
- en: '**Contain hallucinations** that reflect non-existing or incorrect facts.'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**包含幻觉**，即反映不存在或不正确的事实。'
- en: '**Lack interpretability** making it difficult for humans to understand how
    the model arrived at a particular decision or prediction.'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**缺乏可解释性**，使得人们难以理解模型如何得出特定的决定或预测。'
- en: '**Include toxic or biased content** that is harmful or offensive and spreads
    misinformation.'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**包含有害或偏见内容**，即有害或冒犯性内容，并传播虚假信息。'
- en: Innovative training methodologies were introduced in ChatGPT to counteract some
    of these inherent issues of standard LLMs.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在ChatGPT中引入了创新的训练方法，以应对标准LLM的一些固有问题。
- en: ChatGPT
  id: totrans-36
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ChatGPT
- en: ChatGPT is a spinoff of InstructGPT, which introduced a novel approach to incorporating
    human feedback into the training process to better align the model outputs with
    user intent. Reinforcement Learning from Human Feedback (RLHF) is described in
    depth in [openAI’s 2022](https://arxiv.org/pdf/2203.02155.pdf) paper **Training
    language models to follow instructions with human feedback** and is simplified
    below.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: ChatGPT是InstructGPT的衍生产品，它引入了一种新方法，将人类反馈纳入训练过程，以更好地使模型输出符合用户意图。有关强化学习从人类反馈（RLHF）的详细描述，请参阅[openAI的2022](https://arxiv.org/pdf/2203.02155.pdf)论文**训练语言模型以遵循人类反馈的指示**，下面进行了简化。
- en: 'Step 1: Supervised Fine Tuning (SFT) Model'
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 步骤1：监督微调（SFT）模型
- en: The first development involved fine-tuning the GPT-3 model by hiring 40 contractors
    to create a supervised training dataset, in which the input has a known output
    for the model to learn from. Inputs, or prompts, were collected from actual user
    entries into the Open API. The labelers then wrote an appropriate response to
    the prompt thus creating a known output for each input. The GPT-3 model was then
    fine-tuned using this new, supervised dataset, to create GPT-3.5, also called
    the SFT model.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个开发涉及对GPT-3模型进行微调，通过聘请40名承包商创建一个监督训练数据集，其中输入具有已知输出供模型学习。输入或提示收集自实际用户输入到Open
    API中。标签员随后为每个提示编写了适当的回应，从而为每个输入创建了已知输出。然后使用这个新的监督数据集对GPT-3模型进行微调，创建了GPT-3.5，也称为SFT模型。
- en: In order to maximize diversity in the prompts dataset, only 200 prompts could
    come from any given user ID and any prompts that shared long common prefixes were
    removed. Finally, all prompts containing personally identifiable information (PII)
    were removed.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 为了最大化提示数据集的多样性，每个用户ID最多只能有200个提示，任何共享长前缀的提示都被删除。最后，所有包含个人可识别信息（PII）的提示都被删除。
- en: After aggregating prompts from OpenAI API, labelers were also asked to create
    sample prompts to fill-out categories in which there was only minimal real sample
    data. The categories of interest included
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在汇总来自OpenAI API的提示后，标签员还被要求创建样本提示，以填补仅有少量真实样本数据的类别。感兴趣的类别包括
- en: '**Plain prompts:** any arbitrary ask.'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**简单提示：** 任何任意请求。'
- en: '**Few-shot prompts:** instructions that contain multiple query/response pairs.'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**少样本提示：** 包含多个查询/响应对的指令。'
- en: '**User-based prompts:** correspond to a specific use-case that was requested
    for the OpenAI API.'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基于用户的提示：** 对应于OpenAI API请求的特定用例。'
- en: When generating responses, labelers were asked to do their best to infer what
    the instruction from the user was. The paper describes the main three ways that
    prompts request information.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在生成响应时，要求标注者尽力推断用户的指令是什么。论文描述了提示请求信息的三种主要方式。
- en: '**Direct:** “Tell me about…”'
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**直接：** “告诉我关于……”'
- en: '**Few-shot:** Given these two examples of a story, write another story about
    the same topic.'
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**少样本：** 给出这两个故事的例子，写另一个关于相同主题的故事。'
- en: '**Continuation:** Given the start of a story, finish it.'
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**续写：** 给出一个故事的开头，完成它。'
- en: The compilation of prompts from the OpenAI API and hand-written by labelers
    resulted in 13,000 input / output samples to leverage for the supervised model.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 从OpenAI API和标注者手工编写的提示汇编得到了13,000个输入/输出样本，用于监督模型。
- en: '![](../Images/46ce281745760324f5e4d28caef00644.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/46ce281745760324f5e4d28caef00644.png)'
- en: Image (left) inserted from **Training language models to follow instructions
    with human feedback** *OpenAI et al., 2022* [https://arxiv.org/pdf/2203.02155.pdf](https://arxiv.org/pdf/2203.02155.pdf).
    Additional context added in red (right) by the author.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 图片（左侧）摘自**《训练语言模型以遵循人类反馈的指令》** *OpenAI等，2022* [https://arxiv.org/pdf/2203.02155.pdf](https://arxiv.org/pdf/2203.02155.pdf)。额外的上下文由作者在右侧红色部分添加。
- en: 'Step 2: Reward Model'
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第2步：奖励模型
- en: After the SFT model is trained in step 1, the model generates better aligned
    responses to user prompts. The next refinement comes in the form of training a
    reward model in which a model input is a series of prompts and responses, and
    the output is a scaler value, called a reward. The reward model is required in
    order to leverage Reinforcement Learning in which a model learns to produce outputs
    to maximize its reward (see step 3).
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在第1步中训练好SFT模型后，模型生成了更符合用户提示的响应。下一步的改进形式是训练奖励模型，其中模型输入是一系列提示和响应，而输出是一个缩放值，称为奖励。奖励模型是为了利用强化学习，在这种学习方式中，模型学会生成输出以最大化其奖励（参见第3步）。
- en: To train the reward model, labelers are presented with 4 to 9 SFT model outputs
    for a single input prompt. They are asked to rank these outputs from best to worst,
    creating combinations of output ranking as follows.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 为了训练奖励模型，标注者会看到4到9个SFT模型的输出，这些输出都是针对一个输入提示的。他们需要将这些输出从最好到最差进行排名，生成输出排名的组合如下。
- en: '![](../Images/a1673228cc37425149a900d709638d35.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a1673228cc37425149a900d709638d35.png)'
- en: Example of response ranking combinations. Generated by the author.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 响应排名组合的示例。由作者生成。
- en: Including each combination in the model as a separate datapoint led to overfitting
    (failure to extrapolate beyond seen data). To solve, the model was built leveraging
    each group of rankings as a single batch datapoint.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 将每个组合作为单独的数据点纳入模型导致了过拟合（无法超越已见数据进行推断）。为了解决这个问题，模型是通过将每组排名作为一个单独的批次数据点来构建的。
- en: '![](../Images/0bf445ed52fc76ca26038ac769aa92d6.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0bf445ed52fc76ca26038ac769aa92d6.png)'
- en: Image (left) inserted from **Training language models to follow instructions
    with human feedback** *OpenAI et al., 2022* [https://arxiv.org/pdf/2203.02155.pdf](https://arxiv.org/pdf/2203.02155.pdf).
    Additional context added in red (right) by the author.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 图片（左侧）摘自**《训练语言模型以遵循人类反馈的指令》** *OpenAI等，2022* [https://arxiv.org/pdf/2203.02155.pdf](https://arxiv.org/pdf/2203.02155.pdf)。额外的上下文由作者在右侧红色部分添加。
- en: 'Step 3: Reinforcement Learning Model'
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第3步：强化学习模型
- en: In the final stage, the model is presented with a random prompt and returns
    a response. The response is generated using the ‘policy’ that the model has learned
    in step 2\. The policy represents a strategy that the machine has learned to use
    to achieve its goal; in this case, maximizing its reward. Based on the reward
    model developed in step 2, a scaler reward value is then determined for the prompt
    and response pair. The reward then feeds back into the model to evolve the policy.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在最终阶段，模型会接收到一个随机提示并返回一个响应。这个响应是使用模型在第2步中学到的“策略”生成的。策略代表了机器为了实现目标而学会使用的策略；在这个案例中，目标是最大化奖励。基于在第2步中开发的奖励模型，会为提示和响应对确定一个缩放奖励值。然后，奖励反馈到模型中，以进化策略。
- en: In 2017, Schulman *et al.* introduced [Proximal Policy Optimization (PPO)](/proximal-policy-optimization-ppo-explained-abed1952457b),
    the methodology that is used in updating the model’s policy as each response is
    generated. PPO incorporates a per-token Kullback–Leibler (KL) penalty from the
    SFT model. The KL divergence measures the similarity of two distribution functions
    and penalizes extreme distances. In this case, using a KL penalty reduces the
    distance that the responses can be from the SFT model outputs trained in step
    1 to avoid over-optimizing the reward model and deviating too drastically from
    the human intention dataset.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 2017 年，Schulman *et al.* 介绍了 [Proximal Policy Optimization (PPO)](/proximal-policy-optimization-ppo-explained-abed1952457b)，这一方法用于更新每次生成回应时的模型策略。PPO
    结合了来自 SFT 模型的每个令牌 Kullback–Leibler (KL) 惩罚。KL 散度测量两个分布函数的相似性，并惩罚极端距离。在这种情况下，使用
    KL 惩罚减少了回应与在步骤 1 中训练的 SFT 模型输出之间的距离，以避免过度优化奖励模型，并过于偏离人类意图数据集。
- en: '![](../Images/63cb058ba1ca66a58f3c7f1a95e40004.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/63cb058ba1ca66a58f3c7f1a95e40004.png)'
- en: Image (left) inserted from **Training language models to follow instructions
    with human feedback** *OpenAI et al., 2022* [https://arxiv.org/pdf/2203.02155.pdf](https://arxiv.org/pdf/2203.02155.pdf).
    Additional context added in red (right) by the author.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 从 **Training language models to follow instructions with human feedback** *OpenAI
    et al., 2022* [https://arxiv.org/pdf/2203.02155.pdf](https://arxiv.org/pdf/2203.02155.pdf)
    插入的图片（左）。作者添加的额外背景信息（右）以红色标注。
- en: Steps 2 and 3 of the process can be iterated through repeatedly though in practice
    this has not been done extensively.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 过程的第 2 和第 3 步可以重复迭代，尽管在实践中尚未广泛实施。
- en: '![](../Images/10114a9a3d2694f61c6a87766c260d5e.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/10114a9a3d2694f61c6a87766c260d5e.png)'
- en: Screenshot from ChatGPT generated by the author.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 由作者生成的 ChatGPT 截图。
- en: Evaluation of the Model
  id: totrans-68
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型评估
- en: Evaluation of the model is performed by setting aside a test set during training
    that the model has not seen. On the test set, a series of evaluations are conducted
    to determine if the model is better aligned than its predecessor, GPT-3.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 模型评估通过在训练期间留出一个模型未见过的测试集来进行。在测试集上进行一系列评估，以确定模型是否比其前身 GPT-3 更好地对齐。
- en: '**Helpfulness:** the model’s ability to infer and follow user instructions.
    Labelers preferred outputs from InstructGPT over GPT-3 85 ± 3% of the time.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '**有用性**：模型推断和遵循用户指令的能力。标注者 85 ± 3% 的时间更喜欢 InstructGPT 的输出而不是 GPT-3。'
- en: '**Truthfulness**: the model’s tendency for hallucinations. The PPO model produced
    outputs that showed minor increases in truthfulness and informativeness when assessed
    using the [TruthfulQA](https://arxiv.org/abs/2109.07958) dataset.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '**真实度**：模型出现幻觉的倾向。PPO 模型在使用 [TruthfulQA](https://arxiv.org/abs/2109.07958)
    数据集进行评估时，显示出真实度和信息量的轻微增加。'
- en: '**Harmlessness**: the model’s ability to avoid inappropriate, derogatory, and
    denigrating content. Harmlessness was tested using the RealToxicityPrompts dataset.
    The test was performed under three conditions.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '**无害性**：模型避免不适当、贬低和诋毁内容的能力。无害性通过使用 RealToxicityPrompts 数据集进行了测试。测试在三种条件下进行。'
- en: 'Instructed to provide respectful responses: resulted in a significant decrease
    in toxic responses.'
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 指示提供尊重的回应：结果显著减少了有害回应。
- en: 'Instructed to provide responses, without any setting for respectfulness: no
    significant change in toxicity.'
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 指示提供回应，没有任何尊重设置：毒性没有显著变化。
- en: 'Instructed to provide toxic response: responses were in fact significantly
    more toxic than the GPT-3 model.'
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 指示提供有害回应：回应实际上比 GPT-3 模型显著更有害。
- en: For more information on the methodologies used in creating ChatGPT and InstructGPT,
    read the original paper published by OpenAI **Training language models to follow
    instructions with human feedback**, *2022* [https://arxiv.org/pdf/2203.02155.pdf](https://arxiv.org/pdf/2203.02155.pdf).
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解更多关于创建 ChatGPT 和 InstructGPT 的方法，请阅读由 OpenAI 发布的原始论文 **Training language
    models to follow instructions with human feedback**，*2022* [https://arxiv.org/pdf/2203.02155.pdf](https://arxiv.org/pdf/2203.02155.pdf)。
- en: '![](../Images/3b6e18b5276e5cb70c20909110019b56.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3b6e18b5276e5cb70c20909110019b56.png)'
- en: Screenshot from ChatGPT generated by the author.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 由作者生成的 ChatGPT 截图。
- en: Happy learning!
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 祝学习愉快！
- en: Sources
  id: totrans-80
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 来源
- en: '[https://openai.com/blog/chatgpt/](https://openai.com/blog/chatgpt/)'
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[https://openai.com/blog/chatgpt/](https://openai.com/blog/chatgpt/)'
- en: '[https://arxiv.org/pdf/2203.02155.pdf](https://arxiv.org/pdf/2203.02155.pdf)'
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[https://arxiv.org/pdf/2203.02155.pdf](https://arxiv.org/pdf/2203.02155.pdf)'
- en: '[https://medium.com/r/?url=https%3A%2F%2Fdeepai.org%2Fmachine-learning-glossary-and-terms%2Fsoftmax-layer](https://deepai.org/machine-learning-glossary-and-terms/softmax-layer)'
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[https://medium.com/r/?url=https%3A%2F%2Fdeepai.org%2Fmachine-learning-glossary-and-terms%2Fsoftmax-layer](https://deepai.org/machine-learning-glossary-and-terms/softmax-layer)'
- en: '[https://www.assemblyai.com/blog/how-chatgpt-actually-works/](https://www.assemblyai.com/blog/how-chatgpt-actually-works/)'
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[https://www.assemblyai.com/blog/how-chatgpt-actually-works/](https://www.assemblyai.com/blog/how-chatgpt-actually-works/)'
- en: '[https://medium.com/r/?url=https%3A%2F%2Ftowardsdatascience.com%2Fproximal-policy-optimization-ppo-explained-abed1952457b](/proximal-policy-optimization-ppo-explained-abed1952457b)'
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[https://medium.com/r/?url=https%3A%2F%2Ftowardsdatascience.com%2Fproximal-policy-optimization-ppo-explained-abed1952457b](/proximal-policy-optimization-ppo-explained-abed1952457b)'
