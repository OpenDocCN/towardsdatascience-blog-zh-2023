- en: A Quickstart Guide To Uprooting Model Bias
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 《快速入门指南：如何根除模型偏见》
- en: 原文：[https://towardsdatascience.com/a-quickstart-guide-to-uprooting-model-bias-f4465c8e84bc?source=collection_archive---------12-----------------------#2023-01-19](https://towardsdatascience.com/a-quickstart-guide-to-uprooting-model-bias-f4465c8e84bc?source=collection_archive---------12-----------------------#2023-01-19)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/a-quickstart-guide-to-uprooting-model-bias-f4465c8e84bc?source=collection_archive---------12-----------------------#2023-01-19](https://towardsdatascience.com/a-quickstart-guide-to-uprooting-model-bias-f4465c8e84bc?source=collection_archive---------12-----------------------#2023-01-19)
- en: '![](../Images/af28ba3d159eb1a66dd25bd6ce62218b.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/af28ba3d159eb1a66dd25bd6ce62218b.png)'
- en: Image by author
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源于作者
- en: '[](https://aparnadhinak.medium.com/?source=post_page-----f4465c8e84bc--------------------------------)[![Aparna
    Dhinakaran](../Images/e431ee69563ecb27c86f3428ba53574c.png)](https://aparnadhinak.medium.com/?source=post_page-----f4465c8e84bc--------------------------------)[](https://towardsdatascience.com/?source=post_page-----f4465c8e84bc--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----f4465c8e84bc--------------------------------)
    [Aparna Dhinakaran](https://aparnadhinak.medium.com/?source=post_page-----f4465c8e84bc--------------------------------)'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://aparnadhinak.medium.com/?source=post_page-----f4465c8e84bc--------------------------------)[![Aparna
    Dhinakaran](../Images/e431ee69563ecb27c86f3428ba53574c.png)](https://aparnadhinak.medium.com/?source=post_page-----f4465c8e84bc--------------------------------)[](https://towardsdatascience.com/?source=post_page-----f4465c8e84bc--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----f4465c8e84bc--------------------------------)
    [Aparna Dhinakaran](https://aparnadhinak.medium.com/?source=post_page-----f4465c8e84bc--------------------------------)'
- en: ·
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ff32f85889f3a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-quickstart-guide-to-uprooting-model-bias-f4465c8e84bc&user=Aparna+Dhinakaran&userId=f32f85889f3a&source=post_page-f32f85889f3a----f4465c8e84bc---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----f4465c8e84bc--------------------------------)
    ·12 min read·Jan 19, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Ff4465c8e84bc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-quickstart-guide-to-uprooting-model-bias-f4465c8e84bc&user=Aparna+Dhinakaran&userId=f32f85889f3a&source=-----f4465c8e84bc---------------------clap_footer-----------)'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ff32f85889f3a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-quickstart-guide-to-uprooting-model-bias-f4465c8e84bc&user=Aparna+Dhinakaran&userId=f32f85889f3a&source=post_page-f32f85889f3a----f4465c8e84bc---------------------post_header-----------)
    发布于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----f4465c8e84bc--------------------------------)
    ·12 分钟阅读·2023年1月19日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Ff4465c8e84bc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-quickstart-guide-to-uprooting-model-bias-f4465c8e84bc&user=Aparna+Dhinakaran&userId=f32f85889f3a&source=-----f4465c8e84bc---------------------clap_footer-----------)'
- en: --
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ff4465c8e84bc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-quickstart-guide-to-uprooting-model-bias-f4465c8e84bc&source=-----f4465c8e84bc---------------------bookmark_footer-----------)'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ff4465c8e84bc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-quickstart-guide-to-uprooting-model-bias-f4465c8e84bc&source=-----f4465c8e84bc---------------------bookmark_footer-----------)'
- en: '*This article is co-authored by Amber Roberts, Machine Learning Engineer at
    Arize AI*'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '*本文由 Arize AI 的机器学习工程师 Amber Roberts 共同撰写*'
- en: In today’s world, it is all too common to read about AI acting in discriminatory
    ways. From real estate valuation models that reflect the [continued legacy of
    housing discrimination](https://www.technologyreview.com/2020/10/20/1009452/ai-has-exacerbated-racial-bias-in-housing-could-it-help-eliminate-it-instead/)
    to models used in healthcare that amplify inequities [around access to care and
    health outcomes](https://www.hsph.harvard.edu/news/hsph-in-the-news/study-widely-used-health-care-algorithm-has-racial-bias/),
    examples are unfortunately easy to find. As machine learning (ML) models get more
    complex, the true reach of this issue and its impact on marginalized groups is
    not likely fully known. Fortunately, there are a few simple steps that ML teams
    can take to uproot harmful model biases across the ML lifecycle.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在今天的世界中，读到关于AI表现出歧视行为的新闻已是司空见惯。从反映[持续存在的住房歧视遗产](https://www.technologyreview.com/2020/10/20/1009452/ai-has-exacerbated-racial-bias-in-housing-could-it-help-eliminate-it-instead/)的房地产估值模型到在医疗保健中加剧[获取护理和健康结果不平等](https://www.hsph.harvard.edu/news/hsph-in-the-news/study-widely-used-health-care-algorithm-has-racial-bias/)的模型，不幸的是，例子很容易找到。随着机器学习（ML）模型变得更加复杂，这一问题的真实范围及其对边缘化群体的影响可能尚未完全了解。幸运的是，ML团队可以采取一些简单步骤，以根除ML生命周期中的有害模型偏差。
- en: What Is Model Bias?
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是模型偏差？
- en: Model bias is the tendency of a machine learning model to make consistent, systematic
    errors in its predictions. A model will tend to systematically learn the wrong
    signals by not considering all the information contained within the data. Model
    bias may lead an algorithm to miss the relevant relationship between data inputs
    (features) and targeted outputs (predictions). In essence, bias arises when an
    algorithm has insufficient capability in learning the appropriate signal from
    the dataset. For decades, bias in machine learning has been recognized as a potential
    concern, but it remains a complex and challenging issue for machine learning researchers
    and engineers when deploying models into production.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 模型偏差是指机器学习模型在其预测中做出一致的、系统性的错误。模型往往倾向于系统地学习错误的信号，因为没有考虑数据中包含的所有信息。模型偏差可能导致算法错过数据输入（特征）和目标输出（预测）之间的相关关系。本质上，偏差发生在算法从数据集中学习适当信号的能力不足时。几十年来，机器学习中的偏差已被认为是一个潜在问题，但在将模型投入生产时，它仍然是机器学习研究人员和工程师面临的复杂而具挑战性的问题。
- en: How Does Bias Get Introduced Into a Model?
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 偏差如何进入模型？
- en: Biases can be introduced into the machine learning process and reinforced by
    model predictions from a variety of sources. At various phases of the model’s
    development, insufficient data, inconsistent data collecting, and poor data practices
    can all lead to bias in the model’s decisions. While these biases are typically
    unintentional, their existence can have a substantial influence on machine learning
    systems and result in disastrous outcomes — from [employment discrimination](https://www.thomsonreuters.com/en-us/posts/legal/ai-enabled-anti-black-bias/)
    to [misdiagnosis](https://www.who.int/publications/i/item/9789240029200) in healthcare.
    If the machine learning pipeline you’re using contains inherent biases, the model
    will not only learn them but will also exacerbate and maybe even amplify them.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 偏差可能通过多种来源引入到机器学习过程中，并通过模型预测得到强化。在模型开发的各个阶段，数据不足、不一致的数据收集和不良的数据实践都可能导致模型决策中的偏差。虽然这些偏差通常是无意的，但它们的存在可能对机器学习系统产生重大影响，并导致灾难性结果——从[就业歧视](https://www.thomsonreuters.com/en-us/posts/legal/ai-enabled-anti-black-bias/)到医疗保健中的[误诊](https://www.who.int/publications/i/item/9789240029200)。如果你使用的机器学习管道包含固有的偏差，模型不仅会学习这些偏差，还可能加剧甚至放大它们。
- en: Identifying, assessing, and addressing any potential biases that may impact
    the outcome is a crucial requirement when creating a new machine learning model
    and maintaining it in production. As machine learning practitioners, it is our
    responsibility to inspect, monitor, assess, investigate, and evaluate these systems
    to avoid bias that negatively impacts the effectiveness of the decisions that
    models drive.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 识别、评估和解决可能影响结果的任何潜在偏差，是创建新的机器学习模型并在生产中维护它时的关键要求。作为机器学习从业者，我们有责任检查、监控、评估、调查和评估这些系统，以避免对模型决策有效性产生负面影响的偏差。
- en: Causes of bias in models span both the data and model itself.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 模型中的偏差原因涉及数据和模型本身。
- en: Representation Bias (Data)
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 表示偏差（数据）
- en: '*Application:* bias introduced by historical data, historical bias, skewed
    sample, tainted examples'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*应用：* 由历史数据引入的偏差、历史偏差、样本失衡、受污染的例子'
- en: '*Example:* embeddings trained on news articles exhibit gender-based stereotypes
    in society'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*示例：* 在新闻文章上训练的嵌入展现了社会中的性别刻板印象。'
- en: Measurement Bias (Data)
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 测量偏差（数据）
- en: '*Application:* proxies, sample size disparity, limited features'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*应用：* 代理变量、样本量差异、特征有限'
- en: '*Example:* proxy measurements that predict the likelihood of recidivism lead
    to black defendants getting harsher sentences than white defendants for the same
    crime'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*示例：* 预测再犯可能性的代理测量导致黑人的刑罚比白人在同样罪行上的刑罚更严厉。'
- en: Aggregation Bias (Model)
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 聚合偏差（模型）
- en: '*Application:* single aggregated model for all populations'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*应用：* 为所有人群使用的单一聚合模型'
- en: '*Example:* if you have a single prediction model to predict likelihood for
    a particular disease across minorities, it would likely fail and exhibit bias'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*示例：* 如果你有一个单一的预测模型来预测特定疾病在少数群体中的发生概率，它可能会失败并表现出偏见。'
- en: Evaluation Bias (Model)
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 评估偏差（模型）
- en: '*Application:* benchmarks used for evaluation do not represent the general
    population'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*应用：* 用于评估的基准不代表一般人群。'
- en: '*Example:* if you benchmark a housing price prediction model for California
    and then try applying it to the South Carolina housing market, the prices would
    be biased'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*示例：* 如果你为加利福尼亚州的房价预测模型进行基准测试，然后尝试将其应用于南卡罗来纳州的房市，价格将会有偏差。'
- en: Parity Prerequisites
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 公平性前提
- en: Defining Protected Attributes
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 定义保护属性
- en: Of course, you can’t quantify harmful model bias without first defining who
    is being protected.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，在定义谁是被保护的对象之前，你不能量化有害的模型偏差。
- en: Knowing protected classes under the law is a good first step. Most Americans
    probably know that the U.S. Civil Rights Act of 1964 precludes discrimination
    on the basis of race and sex, for example, but fewer may know that other attributes
    — like genetic information or citizenship — also qualify as protected classes
    under the law and can result in [millions or billions](https://www.justice.gov/opa/pr/justice-department-reaches-settlement-wells-fargo-resulting-more-175-million-relief)
    of dollars in fines when violations occur.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 了解法律下的保护类别是一个好的第一步。大多数美国人可能知道1964年美国民权法案禁止基于种族和性别的歧视，但更少人知道其他属性——如遗传信息或国籍——也符合法律下的保护类别，并且当发生违规时可能会导致[数百万或数十亿美元](https://www.justice.gov/opa/pr/justice-department-reaches-settlement-wells-fargo-resulting-more-175-million-relief)的罚款。
- en: Legal compliance is just a starting point. Many large enterprises also [go beyond](https://purpose.businessroundtable.org/)
    these legal requirements and have additional protected classes or public commitments
    to diversity and equity.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 法律合规只是一个起点。许多大型企业也[超越](https://purpose.businessroundtable.org/)这些法律要求，拥有额外的保护类别或公开承诺多样性和公平性。
- en: '![](../Images/faee444511a6ac4201789bca31f369ba.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/faee444511a6ac4201789bca31f369ba.png)'
- en: Image by author
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：作者
- en: Defining Fairness
  id: totrans-36
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 定义公平性
- en: Once you are clear on protected classes in all relevant jurisdictions, the next
    step is defining what fairness looks like. While this is a [complex topic](https://arxiv.org/pdf/1908.09635.pdf),
    a few fundamentals can help.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你清楚了所有相关司法管辖区的保护类别，下一步就是定义公平性是什么样的。虽然这是一个[复杂的话题](https://arxiv.org/pdf/1908.09635.pdf)，但一些基本原则可以帮助你。
- en: '![](../Images/dbb303dc1ceb8828d59d79a998492ee2.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/dbb303dc1ceb8828d59d79a998492ee2.png)'
- en: Image by author
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：作者
- en: One primary distinction is the difference between group (equal and proportional)
    fairness and individual fairness.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 一个主要的区别是群体（相等和成比例）公平性与个人公平性之间的差异。
- en: '**Group fairness** is defined by protected attributes receiving similar treatments
    or outcomes'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**群体公平性** 的定义是保护属性获得相似的待遇或结果。'
- en: '**Individual fairness** is defined as similar individuals receiving similar
    treatments or outcomes'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**个人公平性** 的定义是相似的个体获得相似的待遇或结果。'
- en: For example, imagine a bank that is evaluating 100 mortgage applications. 70
    applications are from men and 30 from women. For group fairness based on equal
    percentages, you would approve 50% of applications for men (35) and 50% for women
    (15). For group fairness based on equal numbers, on the other hand, 50 approvals
    would be spread evenly — 25 for men and 25 for women. Either outcome might be
    considered unfair if there is a higher percentage of loan-worthy individuals in
    either group. One prevailing approach in the industry to mitigate model bias is
    to ensure prediction accuracy is the same for each group — a measure of equal
    opportunity versus accuracy.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设一家银行正在评估100份抵押贷款申请。70份申请来自男性，30份来自女性。基于等比例的群体公平性，你会批准男性50%的申请（35份）和女性50%的申请（15份）。另一方面，基于等数量的群体公平性，50份批准会平均分配——男性25份，女性25份。如果某一组中贷款资格较高，这两种结果都可能被认为是不公平的。行业中一种主流的方法是确保每个群体的预测准确性相同——这是公平机会与准确性的衡量标准。
- en: '![](../Images/320e687084b7c1159388e585af72ea3a.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/320e687084b7c1159388e585af72ea3a.png)'
- en: Image by author
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图像
- en: What Are the Data Modeling Stages That Are Vulnerable To Bias?
  id: totrans-46
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据建模阶段哪些易受偏见影响？
- en: Model fairness [impacts](https://www.oreilly.com/library/view/practical-fairness/9781492075721/)
    the pre-processing, in-processing, and post-processing stages of the data modeling
    pipeline. Fairness interventions, or actions taken to ensure that the model is
    not biased against certain groups, should be implemented at each stage of this
    process.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 模型公平性 [影响](https://www.oreilly.com/library/view/practical-fairness/9781492075721/)
    数据建模管道的预处理、处理中和后处理阶段。公平干预，即采取措施确保模型不对某些群体产生偏见，应在该过程的每个阶段实施。
- en: Pre-Processing
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 预处理
- en: '**What is it?** The earliest stage of data processing, during which data is
    translated into inputs for a machine learning model.'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**这是什么？** 数据处理的最早阶段，此阶段将数据转换为机器学习模型的输入。'
- en: '**Why intervene on model fairness at this stage?** Intervening at this earliest
    stage in the ML lifecycle can have a big impact on the data modeling process and
    downstream metrics.'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**为什么在此阶段对模型公平性进行干预？** 在机器学习生命周期的这一最早阶段进行干预，可以对数据建模过程及其后续指标产生重大影响。'
- en: '**How can you achieve fairness intervention at this stage?** Removing, obscuring,
    obfuscating, renaming or replacing sensitive attributes.'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**如何在此阶段实现公平干预？** 移除、遮蔽、模糊、重命名或替换敏感属性。'
- en: '**Example:** A bank interested in building a model to predict loan defaults
    might sample data to ensure that it includes a representative mix of applicants
    from different races, genders, and geographic locations before removing sensitive
    variables from the data to prevent them from being used in the model.'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**示例：** 一家银行有意建立一个预测贷款违约的模型，可能会对数据进行抽样，以确保其包含来自不同种族、性别和地理位置的申请人的代表性混合，然后移除数据中的敏感变量，以防这些变量被用于模型中。'
- en: In-Processing
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 处理中
- en: '**What is it?** In-processing refers to any action that changes the training
    process of a machine learning model.'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**这是什么？** 处理过程指的是任何改变机器学习模型训练过程的行动。'
- en: '**Why intervene on model fairness at this stage?** If we can’t intervene at
    the earliest stage due to computational constraints or constraints on proprietary
    or licensed data, the next best option for addressing model fairness is in the
    training process. Intervening at this stage can allow teams to keep their training
    data sets raw and unaltered.'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**为什么在此阶段对模型公平性进行干预？** 如果由于计算限制或对专有或许可数据的限制无法在最早阶段进行干预，那么在训练过程中进行干预是下一最佳选择。在此阶段干预可以让团队保持其训练数据集的原始状态，不被修改。'
- en: '**How can you achieve fairness intervention at this stage?** Model regularization
    via adding an additional term to the model’s loss function so no one feature unfairly
    dominates a model’s decisions. You can alsouse an adversarial model to reduce
    the amount of unfair or false information in your model.'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**如何在此阶段实现公平干预？** 通过向模型的损失函数中添加额外的项来进行模型正则化，以确保没有一个特征不公平地主导模型的决策。你还可以使用对抗模型来减少模型中的不公平或虚假信息。'
- en: '**Example:** A medical provider training a model predicting patient outcomes
    might create an adversary model to use outputs from the target model to predict
    for a protected category for the patient. This is to be sure that a patient’s
    personal information (like, income, race and gender) aren’t predictors of their
    healthcare outcome.'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**示例：** 一家医疗提供商训练一个预测患者结果的模型，可能会创建一个对抗模型，使用目标模型的输出预测患者的受保护类别。这是为了确保患者的个人信息（如收入、种族和性别）不是其医疗结果的预测因素。'
- en: Post-Processing
  id: totrans-58
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 后处理
- en: '**What is it?** Takes place after the model has been trained on processed data.'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**这是什么？** 发生在模型在处理数据上训练之后。'
- en: '**Why intervene on model fairness at this stage?** When teams inherit a model
    from a prior team without any knowledge of that model, this may be their earliest
    stage of fairness intervention.'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**为什么在这个阶段介入模型公平性？** 当团队从前一个团队继承一个模型却不了解该模型时，这可能是他们公平性干预的最早阶段。'
- en: '**How can you achieve fairness intervention at this stage?** Equalize decisions
    before the decision is received by the user; third-party auditing tools and bias
    tracing with an ML observability tool can help.'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**如何在这个阶段实现公平性干预？** 在决策被用户收到之前使决策公平；第三方审计工具和使用机器学习可观测性工具的偏见追踪可以提供帮助。'
- en: '**Example:** A broadband provider with a model that predicts customer churn
    wants to ensure it does not discriminate in offering discounts to customers. The
    company implements fairness checks with fairness-specific metrics as part of the
    model deployment process. Bias tracing then monitors model performance on a diverse
    set of customers to ensure that it is not biased against any particular group
    to ensure that outputs remain fair. If there is algorithm bias present, the decisions
    are equalized.'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**示例：** 一家提供宽带服务的公司有一个预测客户流失的模型，希望确保在向客户提供折扣时不产生歧视。该公司在模型部署过程中实施了公平性检查，使用公平性特定指标。然后，偏见追踪监控模型在多样化客户群体上的表现，以确保它不对任何特定群体存在偏见，从而保持输出公平。如果存在算法偏见，决策会被平等化。'
- en: What Are Prevailing Model Fairness Metrics?
  id: totrans-63
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是主流模型公平性指标？
- en: There are a [wealth of model fairness metrics](http://www.datasciencepublicpolicy.org/our-work/tools-guides/aequitas/)
    that are appropriate depending on your goal. Here are definitions and recommendations
    for prevailing metrics and where to use each.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 根据你的目标，有许多[模型公平性指标](http://www.datasciencepublicpolicy.org/our-work/tools-guides/aequitas/)是适用的。这里是主流指标的定义和建议，以及每种指标的使用场景。
- en: When deciding on which [fairness metric](https://arize.com/blog-course/fairness-bias-metrics/)
    or metrics to use, you must think about what insights you need to ensure your
    model is not acting discriminately. Teams that care about fairness — especially
    those working in highly regulated industries such as health, lending, insurance,
    and financial services — typically want to see if their models are fair and unbiased
    across sensitive attributes (i.e. race or sex). When a model is biased, teams
    then need to know which group is experiencing the most bias so that action can
    be taken.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在决定使用哪个[公平性指标](https://arize.com/blog-course/fairness-bias-metrics/)时，你必须考虑需要什么见解，以确保你的模型没有表现出歧视。关心公平性的团队——特别是那些在高度监管行业如健康、贷款、保险和金融服务领域工作的团队——通常希望看到他们的模型在敏感属性（如种族或性别）上是否公平和无偏。当模型存在偏见时，团队需要知道哪个群体经历了最多的偏见，以便采取措施。
- en: To understand the fairness metric value for the period of time you are evaluating,
    many companies use the four-fifths rule. The four-fifths rule is a threshold that
    is [used](https://www.eeoc.gov/laws/guidance/questions-and-answers-clarify-and-provide-common-interpretation-uniform-guidelines)
    by regulatory agencies like the United States Equal Employment Opportunity Commission
    to help in identifying adverse treatment of protected classes. Since an ideal
    parity score is 1, when leveraging the four-fifths rule you typically measure
    whether your fairness metric parity score falls outside of the 0.8–1.25 threshold.
    If the parity score is less than 0.8 or greater than 1.25, algorithmic bias against
    the selected sensitive group may be present in your model.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解你评估的时间段内的公平性指标值，许多公司使用四分之一规则。四分之一规则是[用于](https://www.eeoc.gov/laws/guidance/questions-and-answers-clarify-and-provide-common-interpretation-uniform-guidelines)的一个阈值，由像美国平等就业机会委员会这样的监管机构帮助识别对受保护类别的负面待遇。由于理想的公平性分数是1，当利用四分之一规则时，你通常会测量你的公平性指标分数是否落在0.8到1.25的范围之外。如果分数低于0.8或高于1.25，你的模型可能对所选的敏感群体存在算法偏见。
- en: From the decision tree above, we can see how the fairness metric of choice depends
    on whether your model is addressing disparate representation, equal numbers or
    assistive actions.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 从上面的决策树可以看出，选择的公平性指标取决于你的模型是否解决了不同表现、相等数量或辅助行为的问题。
- en: 'Let’s look at when and where to use these metrics (*note: FP = False Positive,
    TP = True Positive, FN = False Negative, TN = True Negative*).'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看这些指标的使用时间和场景（*注意：FP = 假阳性，TP = 真阳性，FN = 假阴性，TN = 真阴性*）。
- en: Recall Parity
  id: totrans-69
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 召回平衡
- en: '**Definition:** Measures how “sensitive” the model is for one group compared
    to another, or the model’s ability to predict true positives correctly'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**定义：** 衡量模型对于一个组与另一个组的“敏感性”，或模型正确预测真阳性的能力'
- en: '**When to use it:** Recall parity is achieved if the recall in the subgroups
    are close to each other'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**何时使用：** 如果子组中的召回率接近，则实现了召回平衡'
- en: '**How to calculate it:** *Recall Parity = Recall_sensitive / Recall_base* ;
    Recall = TP / (TP + FN)'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**如何计算：** *召回平衡 = 召回_敏感组 / 召回_基准组* ; 召回 = TP / (TP + FN)'
- en: False Positive Rate Parity
  id: totrans-73
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 假阳性率平衡
- en: '**Definition:** Measures whether a model incorrectly predicts the positive
    class for the sensitive group as compared to the base group'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**定义：** 衡量模型是否对敏感组的正类预测不准确，相对于基准组'
- en: '**When to use it:** False positive rate parity is achieved if the false positive
    rates (the ratio between the number of false positives and the total number of
    negatives) in the subgroups are close to each other'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**何时使用：** 如果子组中的假阳性率（假阳性数与总负数的比率）接近，则实现了假阳性率平衡'
- en: '**How to calculate it:** *False Positive Parity = FPR_underprivileged_group
    / FPR_privileged_group*; False Positive Rate = FP / (FP + TN)'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**如何计算：** *假阳性平衡 = FPR_受限组 / FPR_特权组*; 假阳性率 = FP / (FP + TN)'
- en: Disparate Impact
  id: totrans-77
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 不平等影响
- en: '**Definition:** A quantitative measure of the adverse treatment of protected
    classes'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**定义：** 受保护类别的不利待遇的量化衡量'
- en: '**When to use it:** Disparate impact, also known as proportional parity, is
    used to check whether the ratio of outcomes for different groups is the same as
    the ratio of their presence in the population'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**何时使用：** 不平等影响，也称为比例平衡，用于检查不同组的结果比率是否与它们在总体中的比例相同'
- en: '**How to calculate it:** If a job offer rate for men is 50% and the job offer
    rate for women is 25%, then the ratio of the two rates is 2, indicating disparate
    impact'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**如何计算：** 如果男性的工作机会率为50%，女性的工作机会率为25%，那么这两个率的比率为2，表示存在不平等影响'
- en: Once you have defined fairness in the context of your business problem by consulting
    the fairness tree, you can calculate your parity scores and use the four-fifths
    rule to determine if you need to intervene at the pre-processing, in-processing,
    or post-processing stage in your model development pipeline. For a cheatsheet
    on the type of parity you want to achieve to demonstrate algorithmic neutrality,
    see below.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦通过咨询公平性树定义了业务问题背景中的公平性，你可以计算你的平衡得分，并使用五分之一规则来确定是否需要在模型开发流程的预处理、处理中或后处理阶段进行干预。有关要实现的平衡类型以展示算法中立性，请参见下文。
- en: Type 1 Parity
  id: totrans-82
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第一类平衡
- en: '**Description:** Fairness in both false discovery rate (FDR) parity and false
    positive rate (FPR) parity'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**描述：** 在假发现率（FDR）平衡和假阳性率（FPR）平衡中的公平性'
- en: '**Calculation:** *FDR = FP / (TP+FP)* *; FPR = FP / (TN+FP)*'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**计算：** *FDR = FP / (TP + FP)* ; *FPR = FP / (TN + FP)*'
- en: Type 2 Parity
  id: totrans-85
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第二类平衡
- en: '**Description:** Fairness in both false omission rate (FOR) parity and false
    negative rate (FNR) parity'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**描述：** 在假遗漏率（FOR）平衡和假阴性率（FNR）平衡中的公平性'
- en: '**Calculation:** *FOR = FN / (TN + FN)* ; *FNR = FN / ( TP + FN)*'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**计算：** *FOR = FN / (TN + FN)* ; *FNR = FN / (TP + FN)*'
- en: Equalized Odds
  id: totrans-88
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 平均赔率
- en: '**Description:** Fairness in both false positive rate (FPR) parity and true
    positive rate (TPR) parity'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**描述：** 在假阳性率（FPR）平衡和真阳性率（TPR）平衡中的公平性'
- en: '**Calculation:** *FPR = FP / (TN+FP)* ; *TPR = TP / (TP + FN)*'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**计算：** *FPR = FP / (TN + FP)* ; *TPR = TP / (TP + FN)*'
- en: Supervised Fairness
  id: totrans-91
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 监督公平性
- en: '**Description:** Fairness in both Type 1 and Type 2 parity'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**描述：** 在第一类和第二类平衡中的公平性'
- en: '**Calculation:** *see above*'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**计算：** *见上文*'
- en: Overall Fairness
  id: totrans-94
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 总体公平性
- en: '**Description:** Fairness across all metrics used in a confusion matrix'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**描述：** 在混淆矩阵中使用的所有指标的公平性'
- en: '**Calculation:** *FP, TP, FN, TN*'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**计算：** *FP, TP, FN, TN*'
- en: What Tools Exist To Help Tackle Model Bias?
  id: totrans-97
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 有哪些工具可以帮助应对模型偏见？
- en: There are a variety of tools built to help tackle algorithmic bias across the
    ML lifecycle.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 有多种工具被开发出来以帮助应对整个机器学习生命周期中的算法偏见。
- en: Model Building & Validation
  id: totrans-99
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型构建与验证
- en: Most solutions focus on tackling the initial stages of model development, with
    the goal of providing model fairness checks before a model is shipped.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数解决方案专注于处理模型开发的初始阶段，目的是在模型发布之前提供模型公平性检查。
- en: 'Examples of tools:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 工具示例：
- en: 'Aequitas: an open-source bias audit toolkit to do audits of machine learning
    models for discrimination and bias.'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Aequitas：一个开源偏差审计工具包，用于对机器学习模型进行歧视和偏差审计。
- en: 'Arize AI (**full disclosure**: I am co-founder of Arize!): offers model fairness
    checks and comparisons across a training baseline and production data and root
    cause analysis workflows.'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Arize AI (**完全披露**：我是Arize的联合创始人！)：提供模型公平性检查，比较训练基线和生产数据，并进行根本原因分析工作流。
- en: 'IBM Fairness 360: an open source toolkit can help you examine, report, and
    mitigate discrimination and bias in machine learning models via an audit.'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: IBM Fairness 360：一个开源工具包，帮助你通过审计检查、报告和缓解机器学习模型中的歧视和偏差。
- en: 'Google’s PAIR AI: offers several tools useful for specific use cases, including
    a tool for mitigating fairness and bias issues for image datasets supported by
    the TensorFlow Datasets API.'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Google的PAIR AI：提供多个针对特定用例的工具，包括一个用于缓解图像数据集公平性和偏差问题的工具，支持TensorFlow Datasets
    API。
- en: While some of these tools can be used for aggregated fairness metrics and after-the-fact
    explanations (i.e. model explainability) useful for audits, they are mostly not
    designed for real-time monitoring in production.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管其中一些工具可以用于汇总公平性指标和事后解释（即模型解释性），这些对于审计很有用，但它们大多数并不适用于生产中的实时监控。
- en: Monitoring In Production
  id: totrans-107
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 生产中的监控
- en: 'Monitoring fairness metrics in production is important for a simple reason:
    when it comes to deployed AI, it is a matter of when — not if — model bias will
    occur. [Concept drift](https://arize.com/model-drift/), new patterns not seen
    in training, training-serving skew, and outliers challenge even the most advanced
    teams deploying models that perform flawlessly in training and pass the validation
    stage.'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 在生产中监控公平性指标很重要，原因很简单：在部署的AI中，模型偏差发生是时间问题，而非是否发生的问题。[概念漂移](https://arize.com/model-drift/)、训练中未见的新模式、训练与服务偏差以及异常值挑战着即使是最先进的团队，这些团队在训练中表现完美且通过验证阶段的模型也难以避免。
- en: 'Here are some platforms that offer real-time fairness monitoring in production:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一些提供生产中实时公平性监控的平台：
- en: 'Arize: automatic monitors and fairness checks, with multidimensional comparisons
    to uncover model features and cohorts contributing to algorithmic bias'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Arize：提供自动监控和公平性检查，通过多维比较揭示模型特征和群体，帮助发现算法偏差。
- en: 'DataRobot: monitors fairness metrics like proportional parity with workflows
    to compare production data tor training'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DataRobot：监控如比例平衡的公平性指标，并通过工作流将生产数据与训练数据进行比较。
- en: How Should Teams Resolve Model Bias?
  id: totrans-112
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 团队应如何解决模型偏差？
- en: Resolving model bias starts with understanding the data, ensuring teams have
    the right tools, and ensuring organizational governance is set up to ensure fairness.
    Teams need to be aware of the pre-processing, in-processing, and post-processing
    stages of the data modeling pipeline that are valuable to bias – therefore, fairness
    intervention needs to be present in one ( if not more) of these stages. Here are
    a few steps organizations can take to achieve fairness at these stages.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 解决模型偏差的第一步是理解数据，确保团队拥有正确的工具，并确保组织治理到位以确保公平。团队需要了解数据建模管道中对偏差有价值的预处理、处理和后处理阶段，因此，需要在这些阶段之一（如果不是多个阶段）进行公平性干预。以下是组织在这些阶段实现公平性的一些步骤。
- en: '**Step 1: Make protected class data available to model builders and ML teams
    maintaining models in production**'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤1：使受保护类别数据对模型构建者和维护生产模型的机器学习团队可用**'
- en: According to a [recent survey](https://arize.com/resource/survey-machine-learning-observability-results/),
    79.7% of ML teams report that they “lack access to protected data needed to root
    out bias or ethics issues” at least some of the time, and nearly half (42.1%)
    say this is an issue at least somewhat often. This needs to change. As one researcher
    [puts it](https://fairmlbook.org/classification.html), there is no fairness through
    unawareness.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 根据[最近的一项调查](https://arize.com/resource/survey-machine-learning-observability-results/)，79.7%的机器学习团队报告称，他们“缺乏根除偏差或伦理问题所需的受保护数据”，至少有时如此，近一半（42.1%）表示这至少有时是一个问题。这需要改变。正如一位研究人员[所说](https://fairmlbook.org/classification.html)，无视并不能实现公平。
- en: '**Step 2: Ensure you have tools for visibility into fairness in production,
    ideally before models are shipped**'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤 2：确保你拥有生产中公平性的可视化工具，理想情况下是在模型发布之前**'
- en: Coupling fairness checks at the model-building stage with periodic after-the-fact
    audits is insufficient in a world where model bias can cause real-world harm.
    Continuous monitoring and alerting can help surface blindspots (unknown unknowns)
    that inevitably creep up in the real world and speed up time-to-resolution. When
    model owners and ML engineers maintaining models in production have optimized
    guidance and tools, good things happen.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 在模型构建阶段结合公平性检查与定期事后审计，在模型偏差可能导致现实世界伤害的情况下是远远不够的。持续监控和警报可以帮助揭示在现实世界中不可避免出现的盲点（未知的未知），并加快解决时间。当模型所有者和维护生产模型的机器学习工程师拥有优化的指导和工具时，良好的结果就会发生。
- en: '**Step 3: Be an internal change agent and act with urgency**'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤 3：成为内部变革推动者并迅速行动**'
- en: Addressing model bias is not merely about machine learning. Many challenges
    — such as tradeoffs between fairness and business results or diffused responsibility
    across teams — can only be resolved with multiple teams and executive involvement.
    ML teams are well-positioned to play a key role in building a multi-pronged approach
    that combines purpose-built infrastructure, governance, and dedicated working
    groups for accountability.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 解决模型偏差不仅仅是机器学习的问题。许多挑战——例如公平性与业务结果之间的权衡或团队之间责任的分散——只能通过多个团队和高层的参与来解决。机器学习团队在构建一种多管齐下的方法方面处于良好的位置，该方法结合了专门构建的基础设施、治理和专门的工作组以确保问责。
- en: Conclusion
  id: totrans-120
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: Of course, these steps are just the beginning and the industry has a long way
    to go on fairness. Identifying the problem is only half the battle; taking action
    is critical. ML observability and quickly tracing the cause of a model fairness
    issue at a cohort level can help, particularly in knowing when to retrain or revert
    to a prior model (or no model).
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，这些步骤只是开始，行业在公平性方面还有很长的路要走。识别问题仅仅是战斗的一半；采取行动至关重要。在群体层面上，机器学习可观测性和快速追踪模型公平性问题的原因可以提供帮助，特别是在知道何时重新训练或恢复到以前的模型（或不使用模型）时。
- en: Contact Us
  id: totrans-122
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 联系我们
- en: If this blog caught your attention and you’re eager to learn more about [machine
    learning observability](https://arize.com/ml-observability/) and [model monitoring](https://arize.com/model-monitoring/),
    check out our other [blogs](https://arize.com/blog/) and [resources](https://arize.com/resource-hub/)!
    Feel free to [reach out](https://arize.com/contact/) to us with any questions
    or comments, [signup for a free account](https://app.arize.com/auth/join), or
    find our open positions [here](https://arize.com/careers/) if you’re interested
    in joining a fun, rockstar engineering crew to help make models successful in
    production!
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 如果这篇博客引起了你的兴趣，并且你渴望了解更多关于[机器学习可观测性](https://arize.com/ml-observability/)和[模型监控](https://arize.com/model-monitoring/)的信息，请查看我们的其他[博客](https://arize.com/blog/)和[资源](https://arize.com/resource-hub/)!
    随时[联系我们](https://arize.com/contact/)提出任何问题或意见，或[注册一个免费账户](https://app.arize.com/auth/join)，如果你有兴趣加入一个有趣的、明星般的工程团队，帮助模型在生产中取得成功，可以在[这里](https://arize.com/careers/)找到我们的开放职位！
