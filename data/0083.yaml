- en: Generating Sentence-Level Embedding Based on the Trends in Token-Level BERT
    Embeddings
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于词级BERT嵌入趋势生成句子级别嵌入
- en: 原文：[https://towardsdatascience.com/generating-sentence-level-embedding-based-on-the-trends-in-token-level-bert-embeddings-eb08fa2ad3c8?source=collection_archive---------16-----------------------#2023-01-05](https://towardsdatascience.com/generating-sentence-level-embedding-based-on-the-trends-in-token-level-bert-embeddings-eb08fa2ad3c8?source=collection_archive---------16-----------------------#2023-01-05)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/generating-sentence-level-embedding-based-on-the-trends-in-token-level-bert-embeddings-eb08fa2ad3c8?source=collection_archive---------16-----------------------#2023-01-05](https://towardsdatascience.com/generating-sentence-level-embedding-based-on-the-trends-in-token-level-bert-embeddings-eb08fa2ad3c8?source=collection_archive---------16-----------------------#2023-01-05)
- en: '![](../Images/9275b61be55a869b9a7d56ef53ce7455.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9275b61be55a869b9a7d56ef53ce7455.png)'
- en: Photo by Igor Shabalin
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由Igor Shabalin提供
- en: How to derive sentence-level embedding from word embeddings
  id: totrans-4
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何从词嵌入中推导句子级别的嵌入
- en: '[](https://jxireal.medium.com/?source=post_page-----eb08fa2ad3c8--------------------------------)[![Yuli
    Vasiliev](../Images/7a5fbd7fc0d48c87f0163e2ec4622f45.png)](https://jxireal.medium.com/?source=post_page-----eb08fa2ad3c8--------------------------------)[](https://towardsdatascience.com/?source=post_page-----eb08fa2ad3c8--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----eb08fa2ad3c8--------------------------------)
    [Yuli Vasiliev](https://jxireal.medium.com/?source=post_page-----eb08fa2ad3c8--------------------------------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://jxireal.medium.com/?source=post_page-----eb08fa2ad3c8--------------------------------)[![Yuli
    Vasiliev](../Images/7a5fbd7fc0d48c87f0163e2ec4622f45.png)](https://jxireal.medium.com/?source=post_page-----eb08fa2ad3c8--------------------------------)[](https://towardsdatascience.com/?source=post_page-----eb08fa2ad3c8--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----eb08fa2ad3c8--------------------------------)
    [Yuli Vasiliev](https://jxireal.medium.com/?source=post_page-----eb08fa2ad3c8--------------------------------)'
- en: ·
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F83cfb869ab36&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgenerating-sentence-level-embedding-based-on-the-trends-in-token-level-bert-embeddings-eb08fa2ad3c8&user=Yuli+Vasiliev&userId=83cfb869ab36&source=post_page-83cfb869ab36----eb08fa2ad3c8---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----eb08fa2ad3c8--------------------------------)
    ·5 min read·Jan 5, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Feb08fa2ad3c8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgenerating-sentence-level-embedding-based-on-the-trends-in-token-level-bert-embeddings-eb08fa2ad3c8&user=Yuli+Vasiliev&userId=83cfb869ab36&source=-----eb08fa2ad3c8---------------------clap_footer-----------)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F83cfb869ab36&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgenerating-sentence-level-embedding-based-on-the-trends-in-token-level-bert-embeddings-eb08fa2ad3c8&user=Yuli+Vasiliev&userId=83cfb869ab36&source=post_page-83cfb869ab36----eb08fa2ad3c8---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----eb08fa2ad3c8--------------------------------)
    ·5 min read·2023年1月5日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Feb08fa2ad3c8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgenerating-sentence-level-embedding-based-on-the-trends-in-token-level-bert-embeddings-eb08fa2ad3c8&user=Yuli+Vasiliev&userId=83cfb869ab36&source=-----eb08fa2ad3c8---------------------clap_footer-----------)'
- en: --
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Feb08fa2ad3c8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgenerating-sentence-level-embedding-based-on-the-trends-in-token-level-bert-embeddings-eb08fa2ad3c8&source=-----eb08fa2ad3c8---------------------bookmark_footer-----------)'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Feb08fa2ad3c8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgenerating-sentence-level-embedding-based-on-the-trends-in-token-level-bert-embeddings-eb08fa2ad3c8&source=-----eb08fa2ad3c8---------------------bookmark_footer-----------)'
- en: The sentence (phrase or passage) level embedding is often used as input in many
    NLP classification problems, for example, in spam detection and question-answering
    (QA) systems. In my previous post [Discovering Trends in BERT Embeddings of Different
    Levels for the Task of Semantic Context Determining](https://medium.com/p/268733fdb17e),
    I discussed how you might generate a vector representation that holds information
    about changes in contextual embedding values relative to a static embedding of
    the same token, which you then can use as a component for generating a sentence-level
    embedding. This article expands on this topic, exploring what tokens in a sentence
    you need to derive such trend vectors from to be able to generate an efficient
    embedding for the entire sentence.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 句子（短语或段落）级别的嵌入常用于许多自然语言处理分类问题中，例如，垃圾邮件检测和问答（QA）系统。在我之前的文章中，[发现 BERT 嵌入不同层次的趋势以确定语义上下文](https://medium.com/p/268733fdb17e)，我讨论了如何生成一个向量表示，该向量包含有关相对于同一标记静态嵌入值的上下文嵌入值变化的信息，然后可以将其用作生成句子级别嵌入的组件。本文扩展了这一主题，探索了从句子中的哪些标记中提取这样的趋势向量，以便能够为整个句子生成有效的嵌入。
- en: Intuition
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 直觉
- en: 'The first question that arises in connection with this is: from how many tokens
    in a sentence do you need to derive the embedding in order to generate an efficient
    embedding for the entire sentence? If you recall from the discussion in the previous
    post, we’re getting a single vector — derived for the most important word in a
    sentence — that includes information about the context of the entire sentence.
    However, to get a better picture of the sentence context, it would also be nice
    to have such a vector for the word that is most syntactically related to that
    most important word. Why do we need this?'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 与此相关的第一个问题是：从句子中的多少个标记中提取嵌入以生成整个句子的有效嵌入？如果你回顾之前的讨论，我们得到的是一个向量——为句子中最重要的单词提取的——其中包含有关整个句子上下文的信息。然而，为了更好地了解句子上下文，拥有一个与最重要单词在句法上最相关的单词的向量也很有帮助。为什么我们需要这个？
- en: 'A simple analogy from life can help answer this question: If you admire the
    surrounding beauties while sitting, say, in a restaurant located inside the tower
    — the views you contemplate will not include the tower itself. To take a photo
    of the tower view, you first need to exit the tower.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 生活中的一个简单类比可以帮助回答这个问题：如果你坐在塔楼内的餐厅里欣赏周围的美景——你所观察的视角将不包括塔楼本身。要拍摄塔楼的视角，你首先需要离开塔楼。
- en: 'OK, how can we determine the word that is most syntactically related to the
    most important word in the sentence? (that is, you need to decide on the best
    place to take a photo of the tower, according to the previous analogy) The answer
    is: with the help of the attention weights, which you can also obtain from the
    BERT model.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，我们如何确定句子中与最重要单词在句法上最相关的单词呢？（即，你需要根据之前的类比确定拍摄塔楼照片的最佳位置）答案是：借助注意力权重，你也可以从 BERT
    模型中获得这些权重。
- en: Implementation
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现
- en: 'Before you can follow the code discussed in the rest of this article, you’ll
    need to refer to the example provided in the [previous post](https://medium.com/p/268733fdb17e)
    (we’re going to use the model and the derived vector representations defined in
    that example). The only correction you’ll need to do is the following: When creating
    the model, be sure to make the model return not only the hidden states but also
    the attention weights:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在你可以跟随本文其余部分讨论的代码之前，你需要参考[之前的文章](https://medium.com/p/268733fdb17e)中提供的示例（我们将使用该示例中定义的模型和生成的向量表示）。你需要做的唯一修正是：在创建模型时，确保使模型返回不仅是隐藏状态，还有注意力权重：
- en: '[PRE0]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Everything else, including sample sentences, can be used without modification.
    Actually, we’re going to use only the first sample sentence: ‘I want an apple.’'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 其他所有内容，包括示例句子，都可以直接使用。实际上，我们将只使用第一个示例句子：“我想要一个苹果。”
- en: 'Below we are determining the word that is syntactically closest to the most
    important word (Want, in this particular example). For that we check the attention
    weights in all 12 layers. To start with, we create an empty array (we don’t count
    special symbols, excluding the first and last symbols):'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 下面我们正在确定与最重要的单词（在这个例子中是“想要”）在句法上最接近的单词。为此，我们检查所有 12 层的注意力权重。首先，我们创建一个空数组（不计算特殊符号，排除第一个和最后一个符号）：
- en: '[PRE1]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Next, we fill in the matrix of attention weights:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们填充注意力权重矩阵：
- en: '[PRE2]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'We are not interested in the punctuation symbol. So, we’ll remove the last
    column in the matrix:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对标点符号不感兴趣。所以，我们将删除矩阵中的最后一列：
- en: '[PRE3]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: So our matrix will now look as follows (12x4, i.e 12 layers and 4 words)
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们的矩阵现在看起来如下（12x4，即12层和4个单词）
- en: '[PRE4]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '[PRE5]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Let’s now determine in which layers Want (the second column with the index
    1) drew the most attention:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们确定“Want”（第二列，索引为1）在哪些层中吸引了最多的注意力：
- en: '[PRE6]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '[PRE7]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Next, we can determine which token draws more attention after Want in the layers
    where Want is in the lead. For that, we first delete the Want column, and then
    explore the remaining three:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们可以确定在“Want”领先的层中，哪个标记在“Want”之后吸引了更多的注意力。为此，我们首先删除“Want”列，然后探讨其余的三列：
- en: '[PRE8]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '[PRE9]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: The above shows that we have word Apple (after deleting Want here, Apple’s index
    is 2) as the one that is the most syntactically related to word Want. This is
    quite expected because these words represent the direct object and the transitive
    verb, respectively.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 上述内容表明，我们有单词Apple（在这里删除了“Want”后，Apple的索引为2）作为与单词“Want”在句法上最相关的单词。这是相当意料之中的，因为这些词分别代表了直接宾语和及物动词。
- en: '[PRE10]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Let’s now compare the vectors derived from the embeddings of words Apple and
    Want.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们比较从单词Apple和Want的嵌入中得到的向量。
- en: '[PRE11]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '[PRE12]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '[PRE13]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'As you can see, one of the values in the pair of matching elements in the above
    two vectors is in most cases zero while the other value is non-zero — i.e. the
    vectors look complementary (Remember the tower view analogy: Neighboring sights
    are visible from the tower, but in order to see the tower itself — perhaps the
    main attraction — you need to leave it) So, you can safely sum up these vectors
    elementwise to combine the available information into a single vector.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，上述两个向量中匹配元素对的一项值在大多数情况下为零，而另一项值非零——即这些向量看起来是互补的（记住塔楼视角的类比：塔楼可以看到邻近的景象，但要看到塔楼本身——也许是主要的吸引物——你需要离开它）。因此，你可以安全地逐元素相加这些向量，将可用的信息合并为一个单一的向量。
- en: '[PRE14]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '[PRE15]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: The above vector can next be used as input for sentence-level classification.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 上述向量可以作为句子级别分类的输入。
- en: Conclusion
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: This article provides the intuition along with the code on how you can generate
    sentence-level embedding based on the trends in token-level BERT embeddings when
    moving from static embedding to contextual embedding. This sentence-level embedding
    can be then used as an alternative to the CLS token embedding generated by BERT
    for sentence classification, meaning you can try them both to see which one will
    work best for your particular problem.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 本文提供了关于如何基于从静态嵌入到上下文嵌入的转换趋势，在词级BERT嵌入中生成句子级别嵌入的直观理解和代码。这种句子级别的嵌入可以作为BERT生成的CLS标记嵌入的替代方案用于句子分类，这意味着你可以尝试这两种方法，以查看哪一种最适合你的特定问题。
