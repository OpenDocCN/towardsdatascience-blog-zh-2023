- en: The Data-centric AI Concepts in Segment Anything
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: “Segment Anything”中的以数据为中心的人工智能概念
- en: 原文：[https://towardsdatascience.com/the-data-centric-ai-concepts-in-segment-anything-8eea556ac9d](https://towardsdatascience.com/the-data-centric-ai-concepts-in-segment-anything-8eea556ac9d)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/the-data-centric-ai-concepts-in-segment-anything-8eea556ac9d](https://towardsdatascience.com/the-data-centric-ai-concepts-in-segment-anything-8eea556ac9d)
- en: Unpacking the data-centric AI concepts used in Segment Anything, the first foundation
    model for image segmentation
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解读“Segment Anything”中使用的以数据为中心的人工智能概念，这是第一个用于图像分割的基础模型
- en: '[](https://medium.com/@a0987284901?source=post_page-----8eea556ac9d--------------------------------)[![Henry
    Lai](../Images/eaa1b4eb6f6cebc131f4cf0cfdd4cda7.png)](https://medium.com/@a0987284901?source=post_page-----8eea556ac9d--------------------------------)[](https://towardsdatascience.com/?source=post_page-----8eea556ac9d--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----8eea556ac9d--------------------------------)
    [Henry Lai](https://medium.com/@a0987284901?source=post_page-----8eea556ac9d--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@a0987284901?source=post_page-----8eea556ac9d--------------------------------)[![Henry
    Lai](../Images/eaa1b4eb6f6cebc131f4cf0cfdd4cda7.png)](https://medium.com/@a0987284901?source=post_page-----8eea556ac9d--------------------------------)[](https://towardsdatascience.com/?source=post_page-----8eea556ac9d--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----8eea556ac9d--------------------------------)
    [Henry Lai](https://medium.com/@a0987284901?source=post_page-----8eea556ac9d--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----8eea556ac9d--------------------------------)
    ·7 min read·May 31, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----8eea556ac9d--------------------------------)
    ·阅读时长 7 分钟·2023年5月31日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/48aef715050a81cdac17d39e62afc269.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/48aef715050a81cdac17d39e62afc269.png)'
- en: Segment Anything dataset construction. Image from the paper [https://arxiv.org/pdf/2304.02643.pdf](https://arxiv.org/pdf/2304.02643.pdf)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Segment Anything 数据集构建。图片来源于论文 [https://arxiv.org/pdf/2304.02643.pdf](https://arxiv.org/pdf/2304.02643.pdf)
- en: Artificial Intelligence (AI) has made remarkable progress, especially in developing
    foundation models, which are trained with a large quantity of data and can be
    adapted to a wide range of downstream tasks.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 人工智能 (AI) 已经取得了显著进展，特别是在开发基础模型方面，这些模型经过大量数据的训练，能够适应广泛的下游任务。
- en: A notable success of the foundation models is [Large Language Models (LLMs)](https://medium.com/towards-data-science/what-are-the-data-centric-ai-concepts-behind-gpt-models-a590071bb727).
    These models can perform complex tasks with great precision, such as language
    translation, text summarization, and question-answering.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 基础模型的一个显著成功是 [大语言模型 (LLMs)](https://medium.com/towards-data-science/what-are-the-data-centric-ai-concepts-behind-gpt-models-a590071bb727)。这些模型可以高精度地执行复杂任务，如语言翻译、文本摘要和问答。
- en: Foundation models are also starting to change the game in Computer Vision. Meta’s
    Segment Anything is a recent development that’s causing a stir.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 基础模型也开始改变计算机视觉领域的游戏规则。Meta的“Segment Anything”是最近出现的一个引起轰动的发展。
- en: The success of Segment Anything can be attributed to its large labeled dataset,
    which has played a crucial role in enabling its remarkable performance. The model
    architecture, as described in the [Segment Anything paper](https://arxiv.org/pdf/2304.02643.pdf),
    is surprisingly simple and lightweight.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: “Segment Anything”的成功可以归因于其庞大的标注数据集，这在实现其卓越性能方面发挥了至关重要的作用。如 [Segment Anything
    论文](https://arxiv.org/pdf/2304.02643.pdf) 中所述，模型架构出乎意料地简单且轻量。
- en: In this article, drawing upon insights from our recent survey papers [1,2],
    we will take a closer look at Segment Anything through the lens of [data-centric
    AI](https://github.com/daochenzha/data-centric-AI), a growing concept in the data
    science community.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，借鉴我们最近的调查论文 [1,2] 中的见解，我们将通过 [以数据为中心的人工智能](https://github.com/daochenzha/data-centric-AI)
    的视角更深入地探讨“Segment Anything”。
- en: What Can Segment Anything Do?
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: “Segment Anything”可以做什么？
- en: In a nutshell, the image segmentation task is to predict a mask to separate
    the areas of interest in an image, such as an object, a person, etc. Segmentation
    is a very important task in Computer Visual, making the image more meaningful
    and easier to analyze.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，图像分割任务是预测一个掩码来分离图像中的感兴趣区域，例如物体、人物等。分割是计算机视觉中非常重要的任务，使图像更有意义，更易于分析。
- en: The difference between Segment Anything and other image segmentation approaches
    lies in introducing prompts to specify the segmentation location. Prompts can
    be vague, such as a point, a box, etc.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: Segment Anything 和其他图像分割方法的区别在于引入了提示来指定分割位置。提示可以是模糊的，例如一个点，一个框等。
- en: '![](../Images/24a3ffd16299a8fdbe0f9bfbe0c5cb23.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/24a3ffd16299a8fdbe0f9bfbe0c5cb23.png)'
- en: The image is a screenshot from [https://segment-anything.com/](https://segment-anything.com/)
    by uploading the image taken by the author.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 这张图片是从 [https://segment-anything.com/](https://segment-anything.com/) 截取的，图片由作者上传。
- en: What is Data-centric AI?
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 什么是数据中心的人工智能？
- en: '![](../Images/71b0afd9544c0075ce9be397d7884629.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/71b0afd9544c0075ce9be397d7884629.png)'
- en: Comparison between data-centric AI and model-centric AI. [https://arxiv.org/abs/2301.04819](https://arxiv.org/abs/2301.04819)
    Image by the author.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 数据中心人工智能与模型中心人工智能的比较。 [https://arxiv.org/abs/2301.04819](https://arxiv.org/abs/2301.04819)
    图片由作者提供。
- en: '[Data-centric AI](https://github.com/daochenzha/data-centric-AI) is a novel
    approach to AI system development, which has been gaining traction and is being
    promoted by AI pioneer Andrew Ng.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '[数据中心的人工智能](https://github.com/daochenzha/data-centric-AI)是一种新颖的人工智能系统开发方法，近年来受到关注，并由人工智能先驱**安德鲁·吴**推广。'
- en: '*Data-centric AI is the discipline of systematically engineering the data used
    to build an AI system. — Andrew Ng*'
  id: totrans-22
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*数据中心的人工智能是系统性地工程化用于构建人工智能系统的数据的学科。 — 安德鲁·吴*'
- en: Previously, our primary focus was on developing better models using data that
    remained largely unchanged — this was referred to as model-centric AI. However,
    this approach can be problematic in real-world scenarios since it fails to account
    for issues that may arise in the data, including inaccurate labels, duplicates,
    and biases. Consequently, overfitting a dataset may not necessarily result in
    improved model behavior.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 之前，我们的主要关注点是使用变化不大的数据来开发更好的模型，这被称为模型中心的人工智能。然而，这种方法在现实世界场景中可能存在问题，因为它未能考虑数据中可能出现的问题，包括不准确的标签、重复数据和偏见。因此，过拟合数据集可能不会导致模型行为的改善。
- en: Data-centric AI, on the other hand, prioritizes enhancing the quality and quantity
    of data utilized in creating AI systems. The focus is on the data itself, with
    relatively fixed models. Adopting a data-centric approach in developing AI systems
    has more promise in real-world applications since the maximum capability of a
    model is determined by the data used for training.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，数据中心的人工智能优先考虑提高用于创建人工智能系统的数据的质量和数量。重点放在数据本身，相对固定的模型。采用数据中心方法开发人工智能系统在现实世界应用中更具前景，因为模型的最大能力是由用于训练的数据决定的。
- en: It’s crucial to distinguish between “data-centric” and “data-driven” approaches.
    “Data-driven” methods only rely on data to steer AI development, but the focus
    remains on creating models instead of engineering data, making it fundamentally
    different from “data-centric” approaches.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 区分“数据中心”和“数据驱动”方法至关重要。“数据驱动”方法仅依赖于数据来引导人工智能开发，但重点仍然在于创建模型，而不是工程化数据，因此与“数据中心”方法根本不同。
- en: 'The [data-centric AI framework](https://github.com/daochenzha/data-centric-AI)
    encompasses three main objectives:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '[数据中心人工智能框架](https://github.com/daochenzha/data-centric-AI) 包括三个主要目标：'
- en: '**Training data development** entails gathering and generating high-quality,
    diverse data to facilitate the training of machine learning models.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**训练数据开发** 涉及收集和生成高质量、多样化的数据，以促进机器学习模型的训练。'
- en: '**Inference data development** involves constructing innovative evaluation
    sets that offer detailed insights into the model or unlock specific capabilities
    of the model through engineered data inputs, such as prompt engineering.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**推理数据开发** 涉及构建创新的评估集，通过工程化的数据输入（如提示工程）为模型提供详细的洞察，或解锁模型的特定能力。'
- en: '**Data maintenance** aims to ensure the quality and dependability of data in
    a constantly changing environment.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据维护** 旨在确保在不断变化的环境中数据的质量和可靠性。'
- en: '![](../Images/f44d3f3a407f142178b652b3cea13289.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f44d3f3a407f142178b652b3cea13289.png)'
- en: Data-centric AI framework. [https://arxiv.org/abs/2303.10158](https://arxiv.org/abs/2303.10158).
    Image by the author.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 数据中心人工智能框架。 [https://arxiv.org/abs/2303.10158](https://arxiv.org/abs/2303.10158)。图片由作者提供。
- en: The Model used in Segment Anything
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在 Segment Anything 中使用的模型
- en: '![](../Images/2cde1db92dc417c8cef702a27ad69fc2.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2cde1db92dc417c8cef702a27ad69fc2.png)'
- en: Segment Anything Model. Image from the paper [https://arxiv.org/pdf/2304.02643.pdf](https://arxiv.org/pdf/2304.02643.pdf)
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: Segment Anything 模型。 图片来自论文 [https://arxiv.org/pdf/2304.02643.pdf](https://arxiv.org/pdf/2304.02643.pdf)
- en: 'The model design is surprisingly simple. The model mainly consists of three
    parts:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 模型设计令人惊讶地简单。该模型主要由三部分组成：
- en: '**Prompt encoder:** This part is used to obtain the representation of the prompt,
    either through positional encoding or convolution.'
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**提示编码器：** 这一部分用于获取提示的表示，可以通过位置编码或卷积来实现。'
- en: '**Image encoder:** This part directly uses the Vision Transformer (ViT) without
    any special modifications.'
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**图像编码器：** 这一部分直接使用了 Vision Transformer (ViT)，没有任何特殊修改。'
- en: '**Lightweight mask decoder:** This part mainly fuses prompt embedding and image
    embedding, using mechanisms such as attention. It is called lightweight because
    it has only a few layers.'
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**轻量级掩码解码器：** 这一部分主要融合了提示嵌入和图像嵌入，使用了如注意力机制等方法。之所以称其为轻量级，是因为它只有少量的层。'
- en: The lightweight mask decoder is interesting, as it allows the model to be easily
    deployed, even with just CPUs. Below is the comment provided by the authors of
    Segment Anything.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 轻量级掩码解码器非常有趣，因为它允许模型即使仅使用 CPU 也能轻松部署。以下是 Segment Anything 的作者提供的评论。
- en: 'Surprisingly, we find that a simple design satisfies all three constraints:
    a powerful image encoder computes an image embedding, a prompt encoder embeds
    prompts, and then the two information sources are combined in a lightweight mask
    decoder that predicts segmentation masks.'
  id: totrans-40
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 令人惊讶的是，我们发现一个简单的设计满足了所有三个约束条件：一个强大的图像编码器计算图像嵌入，一个提示编码器嵌入提示，然后这两个信息源在一个轻量级掩码解码器中结合，以预测分割掩码。
- en: Therefore, the secret of Segment Anything’s strong performance is very likely
    not the model design, as it is very simple and lightweight.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，Segment Anything 强大性能的秘密很可能不在于模型设计，因为它非常简单且轻量化。
- en: Data-centric AI Concepts in Segment Anything
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Segment Anything 中的数据驱动 AI 概念
- en: 'The core of training Segment Anything lies in a large annotated dataset containing
    more than a billion masks, which is 400 times larger than existing segmentation
    datasets. How did they achieve this? The authors used a data engine to perform
    the annotation, which can be broadly divided into three steps:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 训练 Segment Anything 的核心在于一个包含超过十亿个掩码的大型标注数据集，这比现有的分割数据集大了400倍。他们是如何做到的呢？作者们使用了一个数据引擎来执行标注，这可以大致分为三个步骤：
- en: '**Assisted-manual annotation:** This step can be understood as an active learning
    process. First, an initial model is trained on public datasets. Next, annotators
    modify the predicted masks. Finally, the model is trained with the newly annotated
    data. These three steps were repeated six times, ultimately resulting in 4.3 million
    mask annotations.'
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**辅助手动标注：** 这一步可以理解为一种主动学习过程。首先，在公共数据集上训练初始模型。接下来，标注者修改预测的掩码。最后，使用新标注的数据训练模型。这三个步骤重复了六次，最终生成了430万条掩码标注。'
- en: '**Semi-automatic annotation:** The goal of this step is to increase the diversity
    of masks, which can also be understood as an active learning process. In simple
    terms, if the model can automatically generate good masks, human annotators don’t
    need to label them, and human efforts can focus on masks where the model is not
    confident enough. The method used to find confident masks is quite interesting,
    involving object detection on masks from the first step. For example, suppose
    there are 20 possible masks in an image. We first use the current model for segmentation,
    but this will probably only annotate a portion of the masks, with some masks not
    being well-annotated. We now need to identify which masks are good (confident)
    automatically. This paper’s approach is to perform object detection on the predicted
    masks to see if objects can be detected in the image. If objects are detected,
    we consider the corresponding mask to be confident. Suppose this process identifies
    eight confident masks; the annotator then labels the remaining 12, saving human
    effort. The above process was repeated five times, adding another 5.9 million
    mask annotations.'
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**半自动标注：** 这一步的目标是增加掩码的多样性，这也可以理解为一种主动学习过程。简单来说，如果模型能够自动生成良好的掩码，那么人工标注者就不需要再标注这些掩码，人工可以集中在模型不够自信的掩码上。用来找出自信掩码的方法非常有趣，涉及对第一步掩码进行目标检测。例如，假设图像中有20个可能的掩码。我们首先使用当前模型进行分割，但这可能只会标注出部分掩码，有些掩码标注得不够好。现在我们需要自动识别哪些掩码是好的（自信的）。本文的方法是对预测的掩码进行目标检测，看看是否能够在图像中检测到物体。如果检测到物体，我们就认为对应的掩码是自信的。假设这个过程识别出八个自信的掩码，那么标注者就可以标注剩下的12个，从而节省了人工努力。上述过程重复了五次，增加了另外590万条掩码标注。'
- en: '**Fully-automatic annotation:** Simply put, this step uses the model trained
    in the previous step to annotate data. Some strategies were used to improve annotation
    quality, including:'
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**全自动标注：** 简单来说，这一步使用在前一步中训练的模型来标注数据。为了提高标注质量，采用了一些策略，包括：'
- en: '**(1) filtering out less confident masks** based on predicted Intersection
    over Union (IoU) values (the model has a head to predict IoU).'
  id: totrans-47
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '**(1) 根据预测的交并比（IoU）值过滤掉不太自信的掩码**（模型有一个头部用于预测 IoU）。'
- en: '**(2) only considering stable masks**, meaning that if the threshold is adjusted
    slightly above or below 0.5, the masks remain mostly unchanged. Specifically,
    for each pixel, the model outputs a value between 0 and 1\. We typically use 0.5
    as the threshold to decide whether a pixel is masked. Stability means that when
    the threshold is adjusted to a certain extent around 0.5 (e.g., 0.45 to 0.55),
    the corresponding mask remains largely unchanged, indicating that the model’s
    predictions are significantly different on either side of the boundary.'
  id: totrans-48
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '**(2) 仅考虑稳定的掩码**，即如果阈值稍微调高或调低 0.5，掩码基本保持不变。具体来说，对于每个像素，模型输出一个介于 0 和 1 之间的值。我们通常使用
    0.5 作为阈值来决定一个像素是否被掩码。稳定性意味着，当阈值调整到接近 0.5 的某个程度（例如 0.45 到 0.55）时，相应的掩码基本保持不变，这表明模型的预测在边界的两侧有显著不同。'
- en: '**(3) deduplication was performed** with non-maximal suppression (NMS).'
  id: totrans-49
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '**(3) 进行了去重**，使用了非极大值抑制（NMS）。'
- en: This step annotated 11 billion masks (an increase of more than 100 times in
    quantity).
  id: totrans-50
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这一步标注了 110 亿个掩码（数量增加了 100 倍以上）。
- en: Does this process sound familiar? That’s right, the Reinforcement Learning from
    Human Feedback (RLHF) used in ChatGPT is quite similar to the process described
    above. The commonality between the two approaches is that instead of directly
    relying on humans to annotate data, a model is first trained by human inputs and
    then used to annotate data. In RLHF, a reward model is trained to give rewards
    for reinforcement learning, while in Segment Anything, the model is trained for
    direct image annotation.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 这个过程听起来是否很熟悉？没错，用于 ChatGPT 的人类反馈强化学习（RLHF）与上述过程非常相似。这两种方法的共同点在于，模型不是直接依赖人类来标注数据，而是首先通过人类输入对模型进行训练，然后再使用模型来标注数据。在
    RLHF 中，训练一个奖励模型来为强化学习提供奖励，而在 Segment Anything 中，模型则是针对直接图像标注进行训练的。
- en: Summary
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 总结
- en: The core contribution of Segment Anything lies in its large annotated data,
    demonstrating the crucial importance of the data-centric AI concept. The success
    of foundation models in the computer vision field can be considered an inevitable
    event, but surprisingly, it happened so quickly. Going forward, I believe other
    AI subfields, and even non-AI and non-computer-related fields, will see the emergence
    of foundation models in due course.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: Segment Anything 的核心贡献在于其大量的标注数据，这显示了数据中心人工智能概念的关键重要性。基础模型在计算机视觉领域的成功可以被视为一个必然事件，但令人惊讶的是，这一切发生得如此之快。展望未来，我相信其他
    AI 子领域，甚至非 AI 和非计算机相关领域，也会在适当的时候出现基础模型。
- en: No matter how technology evolves, improving data quality and quantity will always
    be an effective way to enhance AI performance, making the concept of data-centric
    AI increasingly important.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 无论技术如何演变，提高数据的质量和数量始终是提升 AI 性能的有效途径，使得数据中心 AI 概念愈加重要。
- en: 'I hope this article can inspire you in your own work. You can learn more about
    the data-centric AI framework in the following papers/resources:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 我希望这篇文章能为你的工作提供一些灵感。你可以通过以下论文/资源了解更多关于数据中心 AI 框架的信息：
- en: '[1] [Data-centric Artificial Intelligence: A Survey](https://arxiv.org/abs/2303.10158)'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[数据中心人工智能：综述](https://arxiv.org/abs/2303.10158)'
- en: '[2] [Data-centric AI: Perspectives and Challenges](https://arxiv.org/abs/2301.04819)'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[数据中心 AI：视角与挑战](https://arxiv.org/abs/2301.04819)'
- en: '[3] [Awesome Data Centric-AI Resources](https://github.com/daochenzha/data-centric-AI)'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[精彩的数据中心 AI 资源](https://github.com/daochenzha/data-centric-AI)'
- en: 'If you found this article interesting, you may also want to check out my previous
    article: [What Are the Data-Centric AI Concepts behind GPT Models?](https://medium.com/towards-data-science/what-are-the-data-centric-ai-concepts-behind-gpt-models-a590071bb727)'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你觉得这篇文章有趣，你可能还会想查看我之前的文章：[GPT 模型背后的数据中心 AI 概念是什么？](https://medium.com/towards-data-science/what-are-the-data-centric-ai-concepts-behind-gpt-models-a590071bb727)
- en: Stay tuned!
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 敬请关注！
