- en: Nearest Neighbors Regressors — A Visual Guide
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 最近邻回归器 — 可视化指南
- en: 原文：[https://towardsdatascience.com/nearest-neighbors-regressors-a-visual-guide-78595b78072e](https://towardsdatascience.com/nearest-neighbors-regressors-a-visual-guide-78595b78072e)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/nearest-neighbors-regressors-a-visual-guide-78595b78072e](https://towardsdatascience.com/nearest-neighbors-regressors-a-visual-guide-78595b78072e)
- en: '![](../Images/bc24d842e277ced70e9a10b211985b90.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bc24d842e277ced70e9a10b211985b90.png)'
- en: Visual Understanding of the Models and the Impacts of Hyperparameters
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型的视觉理解及超参数的影响
- en: '[](https://medium.com/@angela.shi?source=post_page-----78595b78072e--------------------------------)[![Angela
    and Kezhan Shi](../Images/a89d678f2f3887c0c2ff3928f9d767b4.png)](https://medium.com/@angela.shi?source=post_page-----78595b78072e--------------------------------)[](https://towardsdatascience.com/?source=post_page-----78595b78072e--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----78595b78072e--------------------------------)
    [Angela and Kezhan Shi](https://medium.com/@angela.shi?source=post_page-----78595b78072e--------------------------------)'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@angela.shi?source=post_page-----78595b78072e--------------------------------)[![Angela
    和 Kezhan Shi](../Images/a89d678f2f3887c0c2ff3928f9d767b4.png)](https://medium.com/@angela.shi?source=post_page-----78595b78072e--------------------------------)[](https://towardsdatascience.com/?source=post_page-----78595b78072e--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----78595b78072e--------------------------------)
    [Angela 和 Kezhan Shi](https://medium.com/@angela.shi?source=post_page-----78595b78072e--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----78595b78072e--------------------------------)
    ·8 min read·Mar 31, 2023
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----78595b78072e--------------------------------)
    ·阅读时间 8 分钟·2023 年 3 月 31 日
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: K Nearest Neighbors or KNN is one of the most simple models in machine learning.
    In fact, to some extent, there is no model, because, for the prediction of a new
    observation, it will use the entirety of the training dataset to find the “nearest
    neighbors” according to a distance (usually the euclidean distance). And then
    in the case of a regression task, the prediction value is calculated by averaging
    the target variable values of these neighbors.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: K 最近邻（KNN）是机器学习中最简单的模型之一。实际上，在某种程度上，它没有模型，因为对于新观测的预测，它将使用整个训练数据集来根据距离（通常是欧几里得距离）找到“最近邻居”。然后在回归任务中，预测值是通过对这些邻居的目标变量值取平均来计算的。
- en: Since we use the notion of distance, only numerical features should be used.
    Well, you can always transform categorical using one-hot encoding or label encoding,
    the algorithm of distance calculation will work, but the distance is kind of meaningless.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们使用的是距离的概念，所以应该只使用数值特征。当然，你可以通过独热编码或标签编码转换分类特征，距离计算算法仍然可以工作，但距离可能会变得没有意义。
- en: It is worth also noting that the value of the target variable is not used to
    find the neighbors.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 还值得注意的是，目标变量的值并未用于寻找邻居。
- en: In this article, we will use some simple datasets to visualize how KNN Regressor
    works and how the hyperparameter k will impact the predictions. We also will discuss
    the impact of feature scaling. We will also explore a less known version of nearest
    neighbors which is radius nearest neighbors. In the end, we will finish with a
    discussion about the more customized versions of distance.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们将使用一些简单的数据集来可视化 KNN 回归器的工作原理，以及超参数 k 如何影响预测。我们还将讨论特征缩放的影响。我们还将探索一个较少为人知的邻近版本，即半径最近邻。最后，我们将讨论更自定义的距离版本。
- en: One continuous feature
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 一个连续特征
- en: We will use a simple dataset with non-linear behavior since we know that KNN
    can handle it.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用一个具有非线性行为的简单数据集，因为我们知道KNN能够处理这种情况。
- en: '![](../Images/eec6d8462f21b1ba92347d0846497300.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/eec6d8462f21b1ba92347d0846497300.png)'
- en: Nearest Neighbors Regressors dataset — image by author
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 最近邻回归器数据集 — 作者提供的图片
- en: For those who read my article about the [Decision Tree Regressor](/decision-tree-regressor-a-visual-guide-with-scikit-learn-2aa9e01f5d7f)
    visualization, you can notice that it is the same data. We will do a quick comparison
    with the Decision Tree Regressor model.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 对于那些读过我关于 [决策树回归器](/decision-tree-regressor-a-visual-guide-with-scikit-learn-2aa9e01f5d7f)
    可视化文章的人，你可以注意到这就是相同的数据。我们将与决策树回归器模型进行快速比较。
- en: We can create and fit a KNeighborsRegressor model with KNeighborsRegressor(n_neighbors
    = 3), then we “fit” the model with model.fit(X, y).
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以创建并拟合一个 KNeighborsRegressor 模型，使用 KNeighborsRegressor(n_neighbors = 3)，然后用
    model.fit(X, y) 来“拟合”模型。
- en: In order to make the process of fitting identical to all models, you can notice
    that the model is “fitted” with the classic fit method. But for KNeighborsRegressor,
    the fitting process is just saving the dataset X and y, and nothing else. Yes,
    that is the most rapid fitting ever! and the model is also the biggest ever!
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使所有模型的拟合过程相同，你可以注意到模型是通过经典的拟合方法“拟合”的。但对于KNeighborsRegressor，拟合过程仅仅是保存数据集X和y，没有其他内容。是的，这是最快的拟合！模型也是最大的一次！
- en: Now, we can test the “model” for one observation. In the following code, we
    will use one single point. The classic predict is to calculate the prediction,
    and the kneighbors method allows us to get the neighbors.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以测试一个观测值的“模型”。在以下代码中，我们将使用一个单点。经典预测方法是计算预测值，而kneighbors方法允许我们获取邻居。
- en: Then we can plot the neighbors. I will show you the plots for x = 10 and x =
    20\. Please feel free to do more tests.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们可以绘制邻居。我将展示x = 10和x = 20的图形。请随意进行更多测试。
- en: '![](../Images/50880de338c86b1a17dda9bb7b13881f.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/50880de338c86b1a17dda9bb7b13881f.png)'
- en: Nearest Neighbors Regressors with kneighbors— image by author
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 最近邻回归器与kneighbors — 图片作者
- en: Now, we also can use a sequence of x values to get all the predictions.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们也可以使用一系列x值来获取所有预测结果。
- en: Here is the resulting plot from the previous code. For each point on the red
    segment, the y value represents the average value of the k nearest neighbors (here
    k = 3)
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是前述代码生成的结果图。对于红色段上的每一个点，y值代表k个最近邻的平均值（这里k = 3）
- en: '![](../Images/3206084794cb1460ccd1e9c9807f1bc4.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3206084794cb1460ccd1e9c9807f1bc4.png)'
- en: Nearest Neighbors Regressors with predictions — image by author
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 带预测的最近邻回归器 — 图片作者
- en: Let’s now create the model predictions for different values of k.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们为不同的k值创建模型预测。
- en: '![](../Images/849798f6874f8a94ecdf549bbafcde0f.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/849798f6874f8a94ecdf549bbafcde0f.png)'
- en: Nearest Neighbors Regressors with different values of k — image by author
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 不同k值下的最近邻回归器 — 图片作者
- en: We can also compare with the [Decision Tree Regressor model](/decision-tree-regressor-a-visual-guide-with-scikit-learn-2aa9e01f5d7f)
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以与[决策树回归模型](/decision-tree-regressor-a-visual-guide-with-scikit-learn-2aa9e01f5d7f)进行比较
- en: '![](../Images/d468b812a3f01418553983b8b32b4f40.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d468b812a3f01418553983b8b32b4f40.png)'
- en: Nearest Neighbors Regressors vs. Decision Tree Regressors — image by author
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 最近邻回归器与决策树回归器 — 图片作者
- en: We can notice that the frontier is always clean-cut for decision tree regressors
    whereas it is more nuanced for k nearest neighbors.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以注意到，对于决策树回归器，边界总是干净利落的，而对于k最近邻回归器，边界则更加细腻。
- en: Two continuous features
  id: totrans-33
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 两个连续特征
- en: We will use the following dataset, with two continuous features, to create a
    KNN model. For the testing dataset, we will use meshgrid to generate a grid.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用以下数据集，具有两个连续特征，来创建一个KNN模型。对于测试数据集，我们将使用meshgrid生成网格。
- en: Then we can use plotly to create interactive 3D plots. In the image below, we
    can see the 3D plot with different values of k.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们可以使用plotly创建交互式3D图。在下图中，我们可以看到不同k值的3D图。
- en: '![](../Images/68a995c02caa80cbfa149d6b7938d506.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/68a995c02caa80cbfa149d6b7938d506.png)'
- en: Nearest Neighbors Regressors with two features — image by author
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 带有两个特征的最近邻回归器 — 图片作者
- en: Here again, we can compare them with Decision Tree Regressor model. We can SEE
    and FEEL the difference in the behavior of these two models.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们可以再次与决策树回归模型进行比较。我们可以*看*到并*感受到*这两个模型的行为差异。
- en: '![](../Images/eac166e59bebc76c1a5409fb6ba41be4.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/eac166e59bebc76c1a5409fb6ba41be4.png)'
- en: Nearest Neighbors Regressors vs. Decision Tree Regressors — image by author
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 最近邻回归器与决策树回归器 — 图片作者
- en: Impact of scaling
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 缩放的影响
- en: Contrary to Decision Trees, the scaling of the features has a direct impact
    on the model.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 与决策树不同，特征的缩放对模型有直接影响。
- en: For example, we can do the following transformation for the two-feature case.
    It is worth noting that for one continuous feature, the scaling has no impact
    since the relative distance is not changed.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，对于两个特征的情况，我们可以进行以下变换。值得注意的是，对于一个连续特征，缩放没有影响，因为相对距离没有变化。
- en: '![](../Images/2c11172937cff92f876873a5f3762b18.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2c11172937cff92f876873a5f3762b18.png)'
- en: Nearest Neighbors Regressors with different feature scales — image by author
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 不同特征尺度下的最近邻回归器 — 图片作者
- en: We can visually conclude that the two series of models are very different. We
    can calculate the usual model performance to compare them. But here my approach
    is really to demonstrate visually how the model behaves differently. Can you feel
    it? The distances are changed, because the scales of the features are changed.
    And in the end, the neighbors change.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以直观地得出这两组模型非常不同。我们可以计算通常的模型性能来比较它们。但在这里，我的方法确实是通过视觉上展示模型如何表现不同。你能感受到吗？距离发生了变化，因为特征的尺度发生了变化。最终，邻居也发生了变化。
- en: Some may say that we should use standardization or min-max scaling. But you
    can see that in the image above one case or another could have been a dataset
    with standardization (or min-max scaling). And you can not say in advance if the
    standardization helps the model to have a better performance.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 有人可能会说我们应该使用标准化或最小-最大缩放。但你可以看到，上面的图像中，某些情况可能是标准化（或最小-最大缩放）数据集。而你无法事先判断标准化是否有助于模型性能的提升。
- en: In fact, in order to take into account the relative importance of each feature
    in the distance calculation, we should give different weights for different features.
    But this would make the tuning process too complex.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，为了考虑每个特征在距离计算中的相对重要性，我们应该为不同的特征赋予不同的权重。但这会使调优过程变得过于复杂。
- en: Just imagine that in a linear regression model, the key is to find the coefficients
    for each feature. And in the distance calculation of k NN, all features are considered
    with the same importance. Intuitively, we can FEEL that this kNN model can not
    be that performant!
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下在线性回归模型中，关键是为每个特征找到系数。在 k NN 的距离计算中，所有特征被视为同等重要。从直觉上讲，我们可以感受到这个 kNN 模型的表现不会太好！
- en: Radius Neighbors
  id: totrans-50
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 半径邻居
- en: In the scikit-learn `[neighbors](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.neighbors)`
    module, there is a lesser-known model called RadiusNeighborsRegressor and you
    can easily understand from its name that instead of taking the fixed number of
    neighbors (in the case of k nearest neighbors), we use a circle of a fixed radius
    around the new observation to find its neighbors.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在 scikit-learn 的 `[neighbors](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.neighbors)`
    模块中，有一个鲜为人知的模型叫做 RadiusNeighborsRegressor，你可以从它的名字轻易理解到，与 K 近邻模型不同，我们使用一个固定半径的圆圈来围绕新的观察点找到它的邻居。
- en: Now, when Radius Neighbors model could be more interesting? Let’s take an example
    of a dataset with an outlier. We can see how the two models behave differently.
    Since this outlier is “far” away, in the case of knn, the number of neighbors
    is fixed, so the neighbors are also far away from each other. But for radius neighbors,
    the effect of the outlier is more important.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，何时半径邻居模型可能会更有趣？让我们以一个包含异常值的数据集为例。我们可以看到这两种模型的表现不同。由于这个异常值“远”离其他点，在 KNN 的情况下，邻居数量是固定的，因此邻居之间也相距较远。但对于半径邻居，异常值的影响更为重要。
- en: '![](../Images/26fc0073dc957a63b97111f4bbeae155.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/26fc0073dc957a63b97111f4bbeae155.png)'
- en: Nearest Neighbors Regressors KNN vs. Radius NN — image by author
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 最近邻回归器 KNN vs. 半径 NN — 图片由作者提供
- en: Here please note that when I use the term “outlier”, it does not necessarily
    mean that we should get rid of this outlier. I just want to show the case of some
    data points are far away from others.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里请注意，当我使用“异常值”一词时，并不一定意味着我们应该去除这个异常值。我只是想展示一些数据点远离其他点的情况。
- en: The impact is then also significant for non-outliers. Because we can not satisfy
    the two cases.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 影响对于非异常值也同样显著。因为我们无法满足这两种情况。
- en: You may notice something strange in the image above when the radius is too small.
    Yes, in this case, for some points, there are no neighbors. Then, a huge negative
    number is assigned. The truth is, in this case, there is no solution. But I still
    think that it is an error that should be corrected in scikit-learn. Yielding an
    error is better than giving this value by default.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 当半径过小时，你可能会注意到上图中有些奇怪的现象。是的，在这种情况下，对于某些点，没有邻居。然后，分配了一个巨大的负值。事实上，在这种情况下没有解决方案。但我仍然认为这是一个应该在
    scikit-learn 中纠正的错误。产生错误总比默认给出这个值要好。
- en: Before finishing with Radius Neighbors, when can it be really interesting? Imagine
    a case when you have lots of data in the area in a city, and in another area nearby,
    you don’t have much data, but you know that you could have collected more. Then
    radius neighbors can be more relevant.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在完成半径邻居之前，什么时候它会真正有趣？想象一下，当你在城市的一个区域有大量数据，而在附近的另一个区域数据较少，但你知道你本可以收集更多数据。此时半径邻居可能更相关。
- en: Similarly, if you have a value for one area (that will be assigned to all addresses
    of this area), then we can use the neighbors to smooth the value. Here again,
    the radius neighbors would be more relevant. Here below you have an illustration
    of the value smoothing using a nearest neighbors model.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，如果你为一个区域有一个值（将分配给该区域的所有地址），那么我们可以使用邻居来平滑该值。这里再次，半径邻居将更相关。下面是使用最近邻模型进行值平滑的示例。
- en: '![](../Images/9fcb829d4253cf3374f3fe6fe7f49cae.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9fcb829d4253cf3374f3fe6fe7f49cae.png)'
- en: Geographical Neighbors Smoothing — image by author
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 地理邻居平滑 — 作者图像
- en: More about the Distance
  id: totrans-62
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 关于距离的更多信息
- en: There is one last interesting discussion around the notion of distance, you
    may already think about it when visualizing the previous cartography. The notion
    of distance can be very specific, because if you calculate the euclidean distance
    with longitude and latitude, then this distance may not correctly reflect the
    geographic neighborhood (which is the distance you may desire to use).
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 关于距离的最后一个有趣讨论，你可能在可视化前述制图时已经考虑过。距离的概念可能非常具体，因为如果你用经纬度计算欧几里得距离，那么这种距离可能无法正确反映地理邻域（这是你可能希望使用的距离）。
- en: You may surely already know, but SEEING is always better. From the image below
    the red circle is the “true circle” for the central location, which is the red
    formed by the locations with an equal geographical distance from the central location.
    The blue “circle” is obtained by calculating a Euclidean distance of latitude
    and longitude. Around the equator area, the two circles are almost the same. But
    they are rather different when for locations far from the equator area. So next
    time you have latitude and longitude in your dataset, and you use Nearest Neighbors
    models, you have to think about it.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能已经知道，但看到总是更好。从下面的图像中，红色圆圈是中央位置的“真实圆”，由与中央位置具有相等地理距离的位置形成的红色区域。蓝色“圆圈”是通过计算经纬度的欧几里得距离得到的。在赤道附近，这两个圆圈几乎相同。但在远离赤道的地方，它们会有很大不同。因此，下次你在数据集中使用经纬度并使用最近邻模型时，你必须考虑这一点。
- en: '![](../Images/eeba56d398e14d87d2b819dc20d564b2.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/eeba56d398e14d87d2b819dc20d564b2.png)'
- en: True circle on Earth vs. “Circle” with lat lon — image by author
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 地球上的真实圆形与经纬度的“圆形” — 作者图像
- en: Now, you can imagine that in other cases, a more customized distance may be
    necessary. So this simple model can become more performant.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你可以想象在其他情况下，更自定义的距离可能是必要的。因此，这种简单模型可能会变得更高效。
- en: 'It is also possible to weigh the neighbors. You can do it with the **weights**
    argument. And here is the description of this argument from the official documentation:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 邻居的加权也是可能的。你可以使用**weights**参数来实现。以下是来自官方文档对该参数的描述：
- en: '**weights :** *{‘uniform’, ‘distance’}, callable or None, default=’uniform’*'
  id: totrans-69
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**weights :** *{‘uniform’, ‘distance’}, callable 或 None, 默认=‘uniform’*'
- en: ''
  id: totrans-70
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Weight function used in prediction. Possible values:'
  id: totrans-71
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 预测中使用的权重函数。可能的值：
- en: ''
  id: totrans-72
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '- ‘uniform’ : uniform weights. All points in each neighborhood are weighted
    equally.'
  id: totrans-73
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '- ‘uniform’ : 均匀权重。每个邻域中的所有点都被赋予相等的权重。'
- en: ''
  id: totrans-74
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '- ‘distance’ : weight points by the inverse of their distance. in this case,
    closer neighbors of a query point will have a greater influence than neighbors
    which are further away.'
  id: totrans-75
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '- ‘distance’ : 通过距离的倒数来加权点。在这种情况下，查询点的较近邻居将比较远的邻居具有更大的影响。'
- en: ''
  id: totrans-76
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '- [callable] : a user-defined function which accepts an array of distances,
    and returns an array of the same shape containing the weights.'
  id: totrans-77
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '- [callable] : 用户定义的函数，该函数接受一个距离数组，并返回一个包含相同形状的权重数组。'
- en: ''
  id: totrans-78
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Uniform weights are used by default.
  id: totrans-79
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 默认使用均匀权重。
- en: '![](../Images/a9a91ce787c593f9eae8b33ed3f6d0e0.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a9a91ce787c593f9eae8b33ed3f6d0e0.png)'
- en: K Nearest Neighbors Regressors with weights = “distance” — image by author
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: K最近邻回归器，权重 = “distance” — 作者图像
- en: However, the distance design can become so complex that it is rather easier
    to adopt another approach such as Decision Trees and Mathematical function-based
    models.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，距离设计可能变得非常复杂，这时采用其他方法如决策树和基于数学函数的模型可能更为简单。
- en: Conclusion
  id: totrans-83
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: 'I am writing a series of similar articles to demonstrate how visualization
    helps us to gain a better understanding of how machine learning models work without
    maths. Please follow me with the link below and get full access to my articles:
    [https://medium.com/@angela.shi/membership](https://medium.com/@angela.shi/membership)'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 我正在撰写一系列类似的文章，演示如何通过可视化帮助我们更好地理解机器学习模型的工作原理而无需数学。请通过下面的链接关注我，获取我文章的全部访问权限：[https://medium.com/@angela.shi/membership](https://medium.com/@angela.shi/membership)
- en: 'If you want to get the code that produced the graphics in this article, you
    can support me here: [https://ko-fi.com/s/4cc6555852](https://ko-fi.com/s/4cc6555852)'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想获取生成本文图形的代码，你可以在这里支持我：[https://ko-fi.com/s/4cc6555852](https://ko-fi.com/s/4cc6555852)
- en: So in this article, we demonstrated that Nearest Neighbors “models” are quite
    intuitive to understand with visualization for simple datasets, because the notion
    of neighbors is intuitive and straightforward.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在本文中，我们展示了最近邻“模型”对于简单数据集来说，借助可视化是相当直观的，因为邻居的概念是直观且简单的。
- en: The choice of the hyperparameter K or the radius has a significant impact on
    the performance of the Nearest neighbors models. If K (or radius) is too small,
    the model may overfit the noise in the data, while if K (or radius) is too large,
    the model may underfit and fail to capture the underlying patterns in the data.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 超参数K或半径的选择对最近邻模型的性能有重要影响。如果K（或半径）太小，模型可能会过拟合数据中的噪声；而如果K（或半径）太大，模型可能会欠拟合，无法捕捉数据中的潜在模式。
- en: Changing the scale of the data is also important when using Nearest Neighbors
    models, as the algorithm is sensitive to the scale of the input features.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用最近邻模型时，数据的缩放也很重要，因为该算法对输入特征的缩放非常敏感。
