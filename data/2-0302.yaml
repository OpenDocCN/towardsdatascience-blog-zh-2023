- en: An Introduction To Analytics Engineering
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: åˆ†æå·¥ç¨‹å­¦ä»‹ç»
- en: åŸæ–‡ï¼š[https://towardsdatascience.com/analytics-engineering-8b0ed0883379](https://towardsdatascience.com/analytics-engineering-8b0ed0883379)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡ï¼š[https://towardsdatascience.com/analytics-engineering-8b0ed0883379](https://towardsdatascience.com/analytics-engineering-8b0ed0883379)
- en: Who is an Analytics Engineer and what are they supposed to do
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: åˆ†æå·¥ç¨‹å¸ˆæ˜¯è°ï¼Œä»–ä»¬åº”è¯¥åšä»€ä¹ˆ
- en: '[](https://gmyrianthous.medium.com/?source=post_page-----8b0ed0883379--------------------------------)[![Giorgos
    Myrianthous](../Images/ff4b116e4fb9a095ce45eb064fde5af3.png)](https://gmyrianthous.medium.com/?source=post_page-----8b0ed0883379--------------------------------)[](https://towardsdatascience.com/?source=post_page-----8b0ed0883379--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----8b0ed0883379--------------------------------)
    [Giorgos Myrianthous](https://gmyrianthous.medium.com/?source=post_page-----8b0ed0883379--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://gmyrianthous.medium.com/?source=post_page-----8b0ed0883379--------------------------------)[![Giorgos
    Myrianthous](../Images/ff4b116e4fb9a095ce45eb064fde5af3.png)](https://gmyrianthous.medium.com/?source=post_page-----8b0ed0883379--------------------------------)[](https://towardsdatascience.com/?source=post_page-----8b0ed0883379--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----8b0ed0883379--------------------------------)
    [Giorgos Myrianthous](https://gmyrianthous.medium.com/?source=post_page-----8b0ed0883379--------------------------------)'
- en: Â·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----8b0ed0883379--------------------------------)
    Â·6 min readÂ·Oct 22, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: Â·å‘å¸ƒåœ¨ [Towards Data Science](https://towardsdatascience.com/?source=post_page-----8b0ed0883379--------------------------------)
    Â·é˜…è¯»æ—¶é—´6åˆ†é’ŸÂ·2023å¹´10æœˆ22æ—¥
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/94ed22629f1f62ce0015550806f265cf.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/94ed22629f1f62ce0015550806f265cf.png)'
- en: Image generated via [DALL-E2](https://labs.openai.com/s/Ibq51s6cLfdgFn68sk94aHLF)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ç‰‡ç”± [DALL-E2](https://labs.openai.com/s/Ibq51s6cLfdgFn68sk94aHLF) ç”Ÿæˆ
- en: Traditionally, data teams were formed by Data Engineers and Data Analysts.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: ä¼ ç»Ÿä¸Šï¼Œæ•°æ®å›¢é˜Ÿç”±æ•°æ®å·¥ç¨‹å¸ˆå’Œæ•°æ®åˆ†æå¸ˆç»„æˆã€‚
- en: The Data Engineers are responsible for building up the infrastructure to support
    data operations. These would include the configuration of databases and the implementation
    of ETL processes that are used to ingest data from external sources into a destination
    system (perhaps another database). Furthermore, Data Engineers are typically in
    charge of ensuring data integrity, freshness and security so that Analysts can
    then query the data. A typical skillset for a Data Engineer includes Python (or
    Java), SQL, orchestration (using tools such as Apache Airflow) and data modeling.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: æ•°æ®å·¥ç¨‹å¸ˆè´Ÿè´£å»ºç«‹æ”¯æŒæ•°æ®æ“ä½œçš„åŸºç¡€è®¾æ–½ã€‚è¿™åŒ…æ‹¬æ•°æ®åº“çš„é…ç½®å’ŒETLæµç¨‹çš„å®æ–½ï¼Œè¿™äº›æµç¨‹ç”¨äºå°†æ•°æ®ä»å¤–éƒ¨æ¥æºå¯¼å…¥åˆ°ç›®æ ‡ç³»ç»Ÿï¼ˆå¯èƒ½æ˜¯å¦ä¸€ä¸ªæ•°æ®åº“ï¼‰ã€‚æ­¤å¤–ï¼Œæ•°æ®å·¥ç¨‹å¸ˆé€šå¸¸è´Ÿè´£ç¡®ä¿æ•°æ®çš„å®Œæ•´æ€§ã€æ–°é²œåº¦å’Œå®‰å…¨æ€§ï¼Œä»¥ä¾¿åˆ†æå¸ˆå¯ä»¥æŸ¥è¯¢æ•°æ®ã€‚æ•°æ®å·¥ç¨‹å¸ˆçš„å…¸å‹æŠ€èƒ½åŒ…æ‹¬Pythonï¼ˆæˆ–Javaï¼‰ã€SQLã€ç¼–æ’ï¼ˆä½¿ç”¨å·¥å…·å¦‚Apache
    Airflowï¼‰å’Œæ•°æ®å»ºæ¨¡ã€‚
- en: On the other hand, Data Analysts are supposed to build dashboards and reports
    using Excel or SQL in order to provide business insights to internal users and
    departments.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: å¦ä¸€æ–¹é¢ï¼Œæ•°æ®åˆ†æå¸ˆåº”è¯¥ä½¿ç”¨Excelæˆ–SQLæ„å»ºä»ªè¡¨æ¿å’ŒæŠ¥å‘Šï¼Œä»¥ä¾¿å‘å†…éƒ¨ç”¨æˆ·å’Œéƒ¨é—¨æä¾›ä¸šåŠ¡æ´å¯Ÿã€‚
- en: '![](../Images/dc94913ce71ac6ef8b9ca0cb68e2e7f6.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/dc94913ce71ac6ef8b9ca0cb68e2e7f6.png)'
- en: Traditional formation of Data Teams
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: æ•°æ®å›¢é˜Ÿçš„ä¼ ç»Ÿç»„æˆ
- en: Transitioning From ETL to ELT
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ä»ETLåˆ°ELTçš„è¿‡æ¸¡
- en: In order to process data and gain valuable insights we first need to extract
    it, right? ğŸ¤¯
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†å¤„ç†æ•°æ®å¹¶è·å¾—æœ‰ä»·å€¼çš„æ´å¯Ÿï¼Œæˆ‘ä»¬é¦–å…ˆéœ€è¦æå–æ•°æ®ï¼Œå¯¹å—ï¼ŸğŸ¤¯
- en: Data Ingestion is performed using ETL (and more recently with ELT) processes.
    Both ETL and ELT paradigms involve three main steps; Extract, Transform and Load.
    For now, letâ€™s ignore the sequence of executing these steps and letâ€™s focus on
    what does each step do independently.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: æ•°æ®æ‘„å–æ˜¯é€šè¿‡ETLï¼ˆæœ€è¿‘ä¹Ÿç”¨ELTï¼‰æµç¨‹è¿›è¡Œçš„ã€‚ETLå’ŒELTèŒƒå¼éƒ½æ¶‰åŠä¸‰ä¸ªä¸»è¦æ­¥éª¤ï¼šæå–ã€è½¬æ¢å’ŒåŠ è½½ã€‚ç›®å‰ï¼Œæˆ‘ä»¬å¿½ç•¥è¿™äº›æ­¥éª¤çš„æ‰§è¡Œé¡ºåºï¼Œä¸“æ³¨äºæ¯ä¸ªæ­¥éª¤çš„ç‹¬ç«‹åŠŸèƒ½ã€‚
- en: Extract
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æå–
- en: This step refers to the process of pulling data from a persistent source. This
    data source could be a database, an API endpoint a file or a message queue.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªæ­¥éª¤æŒ‡çš„æ˜¯ä»æŒä¹…åŒ–æ¥æºä¸­æå–æ•°æ®ã€‚æ•°æ®æ¥æºå¯ä»¥æ˜¯æ•°æ®åº“ã€APIç«¯ç‚¹ã€æ–‡ä»¶æˆ–æ¶ˆæ¯é˜Ÿåˆ—ã€‚
- en: '![](../Images/b797a7fae065fff7f3edad5d5c22e3bd.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b797a7fae065fff7f3edad5d5c22e3bd.png)'
- en: 'Extract step pulls data from various sources â€” Source: Author'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: æå–æ­¥éª¤ä»å„ç§æ¥æºä¸­æå–æ•°æ® â€” æ¥æºï¼šä½œè€…
- en: Transform
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è½¬æ¢
- en: In Transform step, the pipeline is expected to perform some changes in the structure
    and/or format of the data in order to achieve a certain goal. A transformation
    could be a modification (e.g. mapping `â€œUnited Statesâ€` to `â€œUSâ€`), an attribute
    selection, a numerical calculation or a join.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è½¬æ¢æ­¥éª¤ä¸­ï¼Œç®¡é“é¢„è®¡ä¼šå¯¹æ•°æ®çš„ç»“æ„å’Œ/æˆ–æ ¼å¼è¿›è¡Œä¸€äº›æ›´æ”¹ï¼Œä»¥å®ç°æŸä¸ªç›®æ ‡ã€‚è½¬æ¢å¯ä»¥æ˜¯ä¿®æ”¹ï¼ˆä¾‹å¦‚ï¼Œå°†`â€œUnited Statesâ€`æ˜ å°„åˆ°`â€œUSâ€`ï¼‰ã€å±æ€§é€‰æ‹©ã€æ•°å€¼è®¡ç®—æˆ–è¿æ¥ã€‚
- en: '![](../Images/b3a099d68622651016306e797fb028d5.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b3a099d68622651016306e797fb028d5.png)'
- en: 'The transformation steps performs a number of transformation into the input
    raw data â€” Source: Author'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: è½¬æ¢æ­¥éª¤å¯¹è¾“å…¥åŸå§‹æ•°æ®è¿›è¡Œäº†ä¸€ç³»åˆ—è½¬æ¢ â€” æ¥æºï¼šä½œè€…
- en: Load
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: åŠ è½½
- en: This step refers to the process of moving data (either raw, or transformed)
    into a destination system. The target is usually a OLTP system, such as a database
    or an OLAP system, such as a Data Warehouse.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: è¯¥æ­¥éª¤æŒ‡çš„æ˜¯å°†æ•°æ®ï¼ˆæ— è®ºæ˜¯åŸå§‹çš„è¿˜æ˜¯è½¬æ¢åçš„ï¼‰ç§»åŠ¨åˆ°ç›®æ ‡ç³»ç»Ÿçš„è¿‡ç¨‹ã€‚ç›®æ ‡é€šå¸¸æ˜¯OLTPç³»ç»Ÿï¼Œå¦‚æ•°æ®åº“ï¼Œæˆ–OLAPç³»ç»Ÿï¼Œå¦‚æ•°æ®ä»“åº“ã€‚
- en: '![](../Images/0c763a0f12942d486d5d1f76ffd9db28.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0c763a0f12942d486d5d1f76ffd9db28.png)'
- en: 'Loading data into a destination system â€” Source: Author'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: å°†æ•°æ®åŠ è½½åˆ°ç›®æ ‡ç³»ç»Ÿ â€” æ¥æºï¼šä½œè€…
- en: 'ETL: Extract â†’ Transform â†’ Load'
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 'ETL: æå– â†’ è½¬æ¢ â†’ åŠ è½½'
- en: ETL refers to the process where the data extraction step is followed by the
    transformation step and ends with the load step.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: ETLæŒ‡çš„æ˜¯æ•°æ®æå–æ­¥éª¤åè·Ÿç€è½¬æ¢æ­¥éª¤ï¼Œæœ€ç»ˆä»¥åŠ è½½æ­¥éª¤ç»“æŸçš„è¿‡ç¨‹ã€‚
- en: '![](../Images/fa80a89e8a6af7c84ba2ab2af062a169.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fa80a89e8a6af7c84ba2ab2af062a169.png)'
- en: 'A visual representation of an ETL process â€” Source: Author'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: ETLè¿‡ç¨‹çš„å¯è§†åŒ–è¡¨ç¤º â€” æ¥æºï¼šä½œè€…
- en: The data transformation step in ETL processes occurs in a staging environment
    outside of the target system, where the data is transformed just before it gets
    loaded to the destination.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: ETLè¿‡ç¨‹ä¸­ï¼Œæ•°æ®è½¬æ¢æ­¥éª¤å‘ç”Ÿåœ¨ç›®æ ‡ç³»ç»Ÿä¹‹å¤–çš„ä¸´æ—¶ç¯å¢ƒä¸­ï¼Œåœ¨æ•°æ®è¢«åŠ è½½åˆ°ç›®æ ‡ä¹‹å‰è¿›è¡Œè½¬æ¢ã€‚
- en: ETL has been around for a while but its application has slowly started fading
    out.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: ETLå·²ç»å­˜åœ¨ä¸€æ®µæ—¶é—´ï¼Œä½†å…¶åº”ç”¨é€æ¸å¼€å§‹å‡å°‘ã€‚
- en: Since the transformation happens in an intermediate (staging) server, thereâ€™s
    an overhead for moving the transformed data into the target system
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ç”±äºè½¬æ¢å‘ç”Ÿåœ¨ä¸­é—´ï¼ˆä¸´æ—¶ï¼‰æœåŠ¡å™¨ä¸Šï¼Œå°†è½¬æ¢åçš„æ•°æ®ç§»åŠ¨åˆ°ç›®æ ‡ç³»ç»Ÿä¸­ä¼šäº§ç”Ÿé¢å¤–çš„å¼€é”€ã€‚
- en: The target system wonâ€™t contain the raw data (i.e. the data in the format prior
    to the transformation). This means that whenever additional transformations are
    required, we would have to pull the raw data once again.
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ç›®æ ‡ç³»ç»Ÿä¸ä¼šåŒ…å«åŸå§‹æ•°æ®ï¼ˆå³è½¬æ¢å‰çš„æ ¼å¼æ•°æ®ï¼‰ã€‚è¿™æ„å‘³ç€æ¯å½“éœ€è¦é¢å¤–çš„è½¬æ¢æ—¶ï¼Œæˆ‘ä»¬å¿…é¡»é‡æ–°æå–åŸå§‹æ•°æ®ã€‚
- en: The emergence of Cloud technologies have shifted the process of ingesting and
    transforming data. Data Warehouses hosted on the cloud have made it possible to
    store huge volumes of data at a very low cost. Therefore, is there really need
    to apply transformations â€œon the flyâ€ while discarding raw data every time a transformation
    is performed?
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: äº‘æŠ€æœ¯çš„å‡ºç°æ”¹å˜äº†æ•°æ®æ‘„å–å’Œè½¬æ¢çš„è¿‡ç¨‹ã€‚æ‰˜ç®¡åœ¨äº‘ä¸Šçš„æ•°æ®ä»“åº“ä½¿å¾—ä»¥éå¸¸ä½çš„æˆæœ¬å­˜å‚¨å¤§é‡æ•°æ®æˆä¸ºå¯èƒ½ã€‚å› æ­¤ï¼Œæ˜¯å¦çœŸçš„éœ€è¦åœ¨æ¯æ¬¡è¿›è¡Œè½¬æ¢æ—¶éƒ½â€œå®æ—¶â€åº”ç”¨è½¬æ¢å¹¶ä¸¢å¼ƒåŸå§‹æ•°æ®ï¼Ÿ
- en: 'ELT: Extract â†’ Load â†’ Transform'
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 'ELT: æå– â†’ åŠ è½½ â†’ è½¬æ¢'
- en: ELT refers to a process where the extraction step is followed by the load step
    and the final data transformation step happens at the very end.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: ELTæŒ‡çš„æ˜¯æå–æ­¥éª¤åè·Ÿç€åŠ è½½æ­¥éª¤ï¼Œæœ€ç»ˆçš„æ•°æ®è½¬æ¢æ­¥éª¤åœ¨æœ€åè¿›è¡Œçš„è¿‡ç¨‹ã€‚
- en: '![](../Images/c4c87a9f272bfc7b5586ce774a0e5cac.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c4c87a9f272bfc7b5586ce774a0e5cac.png)'
- en: 'A visual representation of an ELT process â€” Source: Author'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: ELTè¿‡ç¨‹çš„å¯è§†åŒ–è¡¨ç¤º â€” æ¥æºï¼šä½œè€…
- en: In contrast to ETL, in ELT no staging environment/server is required since data
    transformation is performed within the destination system, which is usually a
    Data Warehouse or Data Lake hosted on the Cloud.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ETLç›¸æ¯”ï¼ŒELTä¸­ä¸éœ€è¦ä¸´æ—¶ç¯å¢ƒ/æœåŠ¡å™¨ï¼Œå› ä¸ºæ•°æ®è½¬æ¢æ˜¯åœ¨ç›®æ ‡ç³»ç»Ÿå†…è¿›è¡Œçš„ï¼Œç›®æ ‡ç³»ç»Ÿé€šå¸¸æ˜¯æ‰˜ç®¡åœ¨äº‘ä¸Šçš„æ•°æ®ä»“åº“æˆ–æ•°æ®æ¹–ã€‚
- en: In addition, the raw data exists on the destination system and thus available
    for further transformations at any time.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤å¤–ï¼ŒåŸå§‹æ•°æ®å­˜åœ¨äºç›®æ ‡ç³»ç»Ÿä¸­ï¼Œå› æ­¤å¯ä»¥éšæ—¶ç”¨äºè¿›ä¸€æ­¥çš„è½¬æ¢ã€‚
- en: Analytics Engineering
  id: totrans-43
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ•°æ®åˆ†æå·¥ç¨‹
- en: As a reminder, in older data team formations, engineers were in charge of maintaining
    the ETL layer while analysts where responsible for the creation of dashboards
    and reporting. But the question now is **where do Analytics Engineers fit into
    the picture?**
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: ä½œä¸ºæé†’ï¼Œåœ¨è¾ƒæ—©çš„æ•°æ®å›¢é˜Ÿæ„å»ºä¸­ï¼Œå·¥ç¨‹å¸ˆè´Ÿè´£ç»´æŠ¤ETLå±‚ï¼Œè€Œåˆ†æå¸ˆåˆ™è´Ÿè´£åˆ›å»ºä»ªè¡¨æ¿å’ŒæŠ¥å‘Šã€‚ä½†ç°åœ¨çš„é—®é¢˜æ˜¯**æ•°æ®åˆ†æå·¥ç¨‹å¸ˆåœ¨è¿™ä¸€è¿‡ç¨‹ä¸­æ‰®æ¼”ä»€ä¹ˆè§’è‰²ï¼Ÿ**
- en: '![](../Images/9a23f55bf1b11986a968eb78ddda68db.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9a23f55bf1b11986a968eb78ddda68db.png)'
- en: 'In older data team formations, Data Engineers were responsible for ETL and
    Data Analysts for reporting â€” Source: Author'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¾ƒæ—©çš„æ•°æ®å›¢é˜Ÿç»“æ„ä¸­ï¼Œæ•°æ®å·¥ç¨‹å¸ˆè´Ÿè´£ETLï¼Œæ•°æ®åˆ†æå¸ˆè´Ÿè´£æŠ¥å‘Šâ€”â€”æ¥æºï¼šä½œè€…
- en: Analytics Engineers are essentially **the link between Data Engineers and Analysts**.
    Their responsibility is to take the raw data and apply transformations so that
    Data Analysts can then collect the transformed data and prepare Dashboards and
    Reports on the Business Intelligence layer so that internal users can then make
    data-informed decisions. Now the Data Engineers can focus more on the ingestion
    level and the wider data infrastructure of the data platform.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: åˆ†æå·¥ç¨‹å¸ˆå®é™…ä¸Šæ˜¯**æ•°æ®å·¥ç¨‹å¸ˆå’Œåˆ†æå¸ˆä¹‹é—´çš„æ¡¥æ¢**ã€‚ä»–ä»¬çš„è´£ä»»æ˜¯å¤„ç†åŸå§‹æ•°æ®å¹¶åº”ç”¨è½¬æ¢ï¼Œä»¥ä¾¿æ•°æ®åˆ†æå¸ˆå¯ä»¥æ”¶é›†è½¬æ¢åçš„æ•°æ®ï¼Œå‡†å¤‡å•†ä¸šæ™ºèƒ½å±‚çš„ä»ªè¡¨æ¿å’ŒæŠ¥å‘Šï¼Œä»¥ä¾¿å†…éƒ¨ç”¨æˆ·èƒ½å¤Ÿåšå‡ºæ•°æ®é©±åŠ¨çš„å†³ç­–ã€‚ç°åœ¨ï¼Œæ•°æ®å·¥ç¨‹å¸ˆå¯ä»¥æ›´å¤šåœ°å…³æ³¨æ•°æ®å¹³å°çš„æ‘„å–å±‚å’Œæ›´å¹¿æ³›çš„æ•°æ®åŸºç¡€è®¾æ–½ã€‚
- en: '![](../Images/4eee2855d4ef9a3d263a34c847682a4c.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4eee2855d4ef9a3d263a34c847682a4c.png)'
- en: 'In ELT pipelines, Data Engineers are responsible for Extraction and Load of
    data in a Data Warehouse, Analytics Engineers for the data transformation layer
    and Analysts for the creation of business dashboards â€” Source: Author'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ELTæµç¨‹ä¸­ï¼Œæ•°æ®å·¥ç¨‹å¸ˆè´Ÿè´£æ•°æ®åœ¨æ•°æ®ä»“åº“ä¸­çš„æå–å’ŒåŠ è½½ï¼Œåˆ†æå·¥ç¨‹å¸ˆè´Ÿè´£æ•°æ®è½¬æ¢å±‚ï¼Œåˆ†æå¸ˆè´Ÿè´£ä¸šåŠ¡ä»ªè¡¨æ¿çš„åˆ›å»ºâ€”â€”æ¥æºï¼šä½œè€…
- en: 'dbt: The ultimate tool for Analytics Engineering'
  id: totrans-50
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: dbtï¼šåˆ†æå·¥ç¨‹çš„**ç»ˆæå·¥å…·**
- en: Analytics Engineers are people that can help data teams scale and move faster.
    But to do so, they also need to take advantage of tools that can help them get
    the job done. And the ultimate Analytics Engineering tool is **data build tool
    (dbt)**.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: åˆ†æå·¥ç¨‹å¸ˆæ˜¯èƒ½å¤Ÿå¸®åŠ©æ•°æ®å›¢é˜Ÿæ‰©å±•å’ŒåŠ å¿«é€Ÿåº¦çš„äººã€‚ä½†è¦åšåˆ°è¿™ä¸€ç‚¹ï¼Œä»–ä»¬è¿˜éœ€è¦åˆ©ç”¨èƒ½å¤Ÿå¸®åŠ©ä»–ä»¬å®Œæˆå·¥ä½œçš„å·¥å…·ã€‚**æ•°æ®æ„å»ºå·¥å…·ï¼ˆdbtï¼‰**å°±æ˜¯ç»ˆæçš„åˆ†æå·¥ç¨‹å·¥å…·ã€‚
- en: dbt is a tool used to build and manage data models in a scalable and cost effective
    fashion. Instead of taking the time to figure out all inter-dependencies between
    models in order to decide in what sequence models must be executed, dbt does all
    the dirty work for you. Furthermore, it provides functionality to support data
    quality tests, freshness tests and documentation among others.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: dbtæ˜¯ä¸€ä¸ªç”¨äºä»¥å¯æ‰©å±•ä¸”æˆæœ¬æ•ˆç›Šé«˜çš„æ–¹å¼æ„å»ºå’Œç®¡ç†æ•°æ®æ¨¡å‹çš„å·¥å…·ã€‚dbtå¯ä»¥ä¸ºä½ å¤„ç†æ‰€æœ‰æ¨¡å‹ä¹‹é—´çš„ä¾èµ–å…³ç³»ï¼Œè€Œæ— éœ€èŠ±æ—¶é—´å¼„æ¸…æ¥šæ¨¡å‹å¿…é¡»æŒ‰ä»€ä¹ˆé¡ºåºæ‰§è¡Œã€‚æ­¤å¤–ï¼Œå®ƒè¿˜æä¾›äº†æ”¯æŒæ•°æ®è´¨é‡æµ‹è¯•ã€æ–°é²œåº¦æµ‹è¯•å’Œæ–‡æ¡£ç¼–åˆ¶ç­‰åŠŸèƒ½ã€‚
- en: In order to better understand what dbt does, itâ€™s important to visualise the
    wider context and see where it fits within the **modern data stack**. dbt is actually
    sitting on the T layer within an ELT pipeline and transformations are performed
    within the Data Warehouse where the raw data resides.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†æ›´å¥½åœ°ç†è§£dbtçš„ä½œç”¨ï¼Œé‡è¦çš„æ˜¯è¦å¯è§†åŒ–æ›´å¹¿æ³›çš„èƒŒæ™¯ï¼Œçœ‹çœ‹å®ƒåœ¨**ç°ä»£æ•°æ®æ ˆ**ä¸­æ‰€å¤„çš„ä½ç½®ã€‚dbtå®é™…ä¸Šä½äºELTç®¡é“ä¸­çš„Tå±‚ï¼Œè½¬æ¢åœ¨åŸå§‹æ•°æ®æ‰€åœ¨çš„æ•°æ®ä»“åº“ä¸­è¿›è¡Œã€‚
- en: '![](../Images/6b6a1c1b2394e8c0cd699cf12c6f260d.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6b6a1c1b2394e8c0cd699cf12c6f260d.png)'
- en: 'Using dbt to perform transformations over raw data within the Data Warehouse
    â€” Source: Author'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨dbtå¯¹æ•°æ®ä»“åº“ä¸­çš„åŸå§‹æ•°æ®è¿›è¡Œè½¬æ¢â€”â€”æ¥æºï¼šä½œè€…
- en: dbt is a **CLI (Command Line Interface) tool** that enables Analytics Engineering
    teams deploy and manage data models following software engineering best practices.
    Some of these practices include support for multiple environments (development
    and production), version controlling and CI/CD (Continuous Integration and Continuous
    Development). Data models can be written in SQL (jinja templated) but more recent
    versions of the tool also support model definitions with Python!
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: dbtæ˜¯ä¸€ä¸ª**CLIï¼ˆå‘½ä»¤è¡Œæ¥å£ï¼‰å·¥å…·**ï¼Œä½¿åˆ†æå·¥ç¨‹å›¢é˜Ÿèƒ½å¤Ÿéƒ¨ç½²å’Œç®¡ç†æ•°æ®æ¨¡å‹ï¼Œéµå¾ªè½¯ä»¶å·¥ç¨‹çš„æœ€ä½³å®è·µã€‚è¿™äº›å®è·µåŒ…æ‹¬æ”¯æŒå¤šä¸ªç¯å¢ƒï¼ˆå¼€å‘å’Œç”Ÿäº§ï¼‰ã€ç‰ˆæœ¬æ§åˆ¶å’ŒCI/CDï¼ˆæŒç»­é›†æˆå’ŒæŒç»­å¼€å‘ï¼‰ã€‚æ•°æ®æ¨¡å‹å¯ä»¥ç”¨SQLï¼ˆjinjaæ¨¡æ¿ï¼‰ç¼–å†™ï¼Œä½†å·¥å…·çš„æœ€æ–°ç‰ˆæœ¬ä¹Ÿæ”¯æŒä½¿ç”¨Pythonå®šä¹‰æ¨¡å‹ï¼
- en: Final Thoughts..
  id: totrans-57
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æœ€åçš„æƒ³æ³•...
- en: Analytics Engineering is an emerging field in the intersection of Data Engineering
    and Data Analytics that aims to speed up the development of analytics products,
    improve data quality and bring more data trust. The main tool that facilitates
    the lifecycle of data products is dbt that has drastically changed the way data
    teams work and collaborate together. It is therefore important to familiarise
    yourself with it since itâ€™s here to stay for the long run.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: åˆ†æå·¥ç¨‹æ˜¯æ•°æ®å·¥ç¨‹å’Œæ•°æ®åˆ†æäº¤æ±‡å¤„çš„æ–°å…´é¢†åŸŸï¼Œæ—¨åœ¨åŠ å¿«åˆ†æäº§å“çš„å¼€å‘ï¼Œæé«˜æ•°æ®è´¨é‡ï¼Œå¹¶å¢å¼ºæ•°æ®çš„å¯ä¿¡åº¦ã€‚ä¿ƒè¿›æ•°æ®äº§å“ç”Ÿå‘½å‘¨æœŸçš„ä¸»è¦å·¥å…·æ˜¯dbtï¼Œå®ƒæå¤§åœ°æ”¹å˜äº†æ•°æ®å›¢é˜Ÿçš„å·¥ä½œå’Œåä½œæ–¹å¼ã€‚å› æ­¤ï¼Œç†Ÿæ‚‰å®ƒéå¸¸é‡è¦ï¼Œå› ä¸ºå®ƒå°†åœ¨é•¿æœŸå†…å­˜åœ¨ã€‚
- en: In upcoming articles we are going to focus more on dbt and how you can use it
    to build and manage your data models effectively. So make sure to subscribe in
    order to be notified when the articles are out!
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å³å°†å‘å¸ƒçš„æ–‡ç« ä¸­ï¼Œæˆ‘ä»¬å°†æ›´ä¸“æ³¨äºdbtä»¥åŠå¦‚ä½•æœ‰æ•ˆåœ°ä½¿ç”¨å®ƒæ¥æ„å»ºå’Œç®¡ç†æ•°æ®æ¨¡å‹ã€‚å› æ­¤ï¼Œè¯·ç¡®ä¿è®¢é˜…ï¼Œä»¥ä¾¿åœ¨æ–‡ç« å‘å¸ƒæ—¶æ”¶åˆ°é€šçŸ¥ï¼
