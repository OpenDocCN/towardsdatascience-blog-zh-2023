- en: 'Leveraging qLoRA for Fine-Tuning of Task-Fine-Tuned Models Without Catastrophic
    Forgetting: A Case Study with LLaMA2(-chat)'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 利用qLoRA对任务精调模型进行精调而不产生灾难性遗忘：以LLaMA2(-chat)为例
- en: 原文：[https://towardsdatascience.com/leveraging-qlora-for-fine-tuning-of-task-fine-tuned-models-without-catastrophic-forgetting-d9bcd594cff4?source=collection_archive---------1-----------------------#2023-09-07](https://towardsdatascience.com/leveraging-qlora-for-fine-tuning-of-task-fine-tuned-models-without-catastrophic-forgetting-d9bcd594cff4?source=collection_archive---------1-----------------------#2023-09-07)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/leveraging-qlora-for-fine-tuning-of-task-fine-tuned-models-without-catastrophic-forgetting-d9bcd594cff4?source=collection_archive---------1-----------------------#2023-09-07](https://towardsdatascience.com/leveraging-qlora-for-fine-tuning-of-task-fine-tuned-models-without-catastrophic-forgetting-d9bcd594cff4?source=collection_archive---------1-----------------------#2023-09-07)
- en: Learn how to infuse knowledge into purpose-fine-tuned models while keeping their
    task-specific nature
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 了解如何在保持任务特定性质的同时，将知识注入目的精调模型
- en: '[](https://medium.com/@aris.tsakpinis?source=post_page-----d9bcd594cff4--------------------------------)[![Aris
    Tsakpinis](../Images/2cc1101aed68e1f71a0026bfdec28f58.png)](https://medium.com/@aris.tsakpinis?source=post_page-----d9bcd594cff4--------------------------------)[](https://towardsdatascience.com/?source=post_page-----d9bcd594cff4--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----d9bcd594cff4--------------------------------)
    [Aris Tsakpinis](https://medium.com/@aris.tsakpinis?source=post_page-----d9bcd594cff4--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@aris.tsakpinis?source=post_page-----d9bcd594cff4--------------------------------)[![Aris
    Tsakpinis](../Images/2cc1101aed68e1f71a0026bfdec28f58.png)](https://medium.com/@aris.tsakpinis?source=post_page-----d9bcd594cff4--------------------------------)[](https://towardsdatascience.com/?source=post_page-----d9bcd594cff4--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----d9bcd594cff4--------------------------------)
    [Aris Tsakpinis](https://medium.com/@aris.tsakpinis?source=post_page-----d9bcd594cff4--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F8ab3accce432&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fleveraging-qlora-for-fine-tuning-of-task-fine-tuned-models-without-catastrophic-forgetting-d9bcd594cff4&user=Aris+Tsakpinis&userId=8ab3accce432&source=post_page-8ab3accce432----d9bcd594cff4---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----d9bcd594cff4--------------------------------)
    ·11 min read·Sep 7, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fd9bcd594cff4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fleveraging-qlora-for-fine-tuning-of-task-fine-tuned-models-without-catastrophic-forgetting-d9bcd594cff4&user=Aris+Tsakpinis&userId=8ab3accce432&source=-----d9bcd594cff4---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F8ab3accce432&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fleveraging-qlora-for-fine-tuning-of-task-fine-tuned-models-without-catastrophic-forgetting-d9bcd594cff4&user=Aris+Tsakpinis&userId=8ab3accce432&source=post_page-8ab3accce432----d9bcd594cff4---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----d9bcd594cff4--------------------------------)
    · 11分钟阅读 · 2023年9月7日 [](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fd9bcd594cff4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fleveraging-qlora-for-fine-tuning-of-task-fine-tuned-models-without-catastrophic-forgetting-d9bcd594cff4&user=Aris+Tsakpinis&userId=8ab3accce432&source=-----d9bcd594cff4---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fd9bcd594cff4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fleveraging-qlora-for-fine-tuning-of-task-fine-tuned-models-without-catastrophic-forgetting-d9bcd594cff4&source=-----d9bcd594cff4---------------------bookmark_footer-----------)![](../Images/90b4a36383c72f5d33217ebfd35ebbd2.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fd9bcd594cff4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fleveraging-qlora-for-fine-tuning-of-task-fine-tuned-models-without-catastrophic-forgetting-d9bcd594cff4&source=-----d9bcd594cff4---------------------bookmark_footer-----------)![](../Images/90b4a36383c72f5d33217ebfd35ebbd2.png)'
- en: Photo by StableDiffusionXL on Amazon Web Services
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由StableDiffusionXL提供，托管在Amazon Web Services
- en: What is this about and why is it important?
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 这篇文章讲了什么，为什么重要？
- en: Large language models (LLMs) like Anthropic’s Claude or Meta’s LLaMA2 have demonstrated
    impressive capabilities on a variety of natural language tasks. However, their
    knowledge and task-specific skills remain relatively generic — if you want to
    execute more specialized, domain-specific tasks that require explicit knowledge,
    you need to find ways to infuse models with knowledge and teach them task-specific
    behaviour. LLM-powered applications need to work properly in their target domain,
    provide accurate answers instead of hallucinating, and ensure security, privacy,
    and appropriate content.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 像 Anthropic 的 Claude 或 Meta 的 LLaMA2 这样的语言大模型（LLMs）在各种自然语言任务中展示了令人印象深刻的能力。然而，它们的知识和任务特定技能仍然相对通用——如果你想执行需要明确知识的更专业的领域特定任务，你需要找到方法将知识注入模型并教它们任务特定的行为。LLM
    驱动的应用需要在其目标领域中正常工作，提供准确的答案而不是产生幻觉，并确保安全、隐私和适当的内容。
- en: These challenges are commonly denoted as the “three Hs” of helpfulness, honesty,
    and harmlessness. Overcoming these challenges have proved to be particularly important
    when designing FM-powered applications of enterprise-grade quality.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 这些挑战通常被称为“有用性、诚实性和无害性”的“三个 H”。克服这些挑战在设计企业级应用时被证明特别重要。
- en: 'There are a few options for imparting domain knowledge into foundation models:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 将领域知识传递到基础模型中有几种选择：
- en: '![](../Images/e1ef8c600e84ed054e0de9a9eb0a3a0b.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e1ef8c600e84ed054e0de9a9eb0a3a0b.png)'
- en: Illustration by Author
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 作者插图
- en: '**Source knowledge:** You can provide knowledge dynamically through prompt
    engineering and example-based or context-infused generation. This option comes
    in relatively simple fashions like zero-shot or few-shot prompting. However, there
    are also more sophisticated approaches of integrating external knowledge sources
    evolving. Infusion of knowledge through dynamic prompt composition and hence the
    source knowledge by sourcing information from various external “tools” (retrieval-augmented
    generation, database retrievers, data lake retrievers, real-time systems via REST-ful
    APIs, …) are gaining popularity.'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**源知识：** 你可以通过提示工程和基于示例或上下文注入生成动态提供知识。这种选项以相对简单的方式呈现，如零样本或少样本提示。然而，还有更复杂的方法来整合外部知识源。通过动态提示组合注入知识，从各种外部“工具”（检索增强生成、数据库检索器、数据湖检索器、通过
    REST-ful API 的实时系统等）获取信息的源知识正在变得越来越受欢迎。'
- en: '**Parametric knowledge:** You can update the model parameters directly through
    fine-tuning. Thereby the pre-trained weights are adjusted to a smaller (compared
    to pre-training), domain-specific dataset and hence towards a domain-specific
    knowledge base or task.'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**参数知识：** 你可以通过微调直接更新模型参数。这样，预训练的权重将调整到一个比预训练时更小的、领域特定的数据集，从而接近于领域特定的知识库或任务。'
- en: As Heiko Hotz explains in his [blogpost](https://heiko-hotz.medium.com/rag-vs-finetuning-which-is-the-best-tool-to-boost-your-llm-application-94654b1eaba7),
    picking the right approach (or a combination of both) comes with tradeoffs. In
    this blog, we’ll focus on the parametric approach and demonstrate how to fine-tune
    the LLaMA2 model using PEFT (parameter efficient fine-tuning) on Amazon SageMaker.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 正如 Heiko Hotz 在他的 [博客文章](https://heiko-hotz.medium.com/rag-vs-finetuning-which-is-the-best-tool-to-boost-your-llm-application-94654b1eaba7)
    中解释的那样，选择正确的方法（或两者的组合）存在权衡。在这篇博客中，我们将重点介绍参数方法，并演示如何在 Amazon SageMaker 上使用 PEFT（参数高效微调）对
    LLaMA2 模型进行微调。
- en: Our goal is to adapt LLaMA2 to a specific domain, picking up recent knowledge
    to overcome the “knowledge cutoff” problem where models lack awareness of recent
    information not being part of their training data. As opposed to task-specific
    fine-tuning this is a much more achievable task for a lot of practitioners since
    they can simply use text corpora containing domain-specific information as training
    datasets as opposed to manually crafting or collecting task-specific datasets
    like conversational or instruction datasets.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的目标是将 LLaMA2 适应于特定领域，获取最新的知识以克服“知识截止”问题，即模型缺乏对未纳入其训练数据的最新信息的意识。与任务特定的微调不同，这对于许多从业者来说是一个更可实现的任务，因为他们可以简单地使用包含领域特定信息的文本语料库作为训练数据集，而不是像对话或指令数据集那样手动制作或收集任务特定的数据集。
- en: Since for a lot of relevant LLM-powered use cases task-specific models are beneficial,
    we will also show that the proposed setup can be applied equally to models like
    LLaMA2-chat which have already gone through a task-specific fine-tuning without
    loosing their task-specific nature (e.g. instruction following, conversational
    behaviour, …).
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 由于许多相关的LLM驱动的使用场景中，任务特定模型是有益的，我们还将展示所提议的设置如何同样适用于像LLaMA2-chat这样的模型，这些模型已经进行了任务特定的微调而不失去其任务特定特性（例如指令跟随、对话行为等）。
- en: By walking through this end-to-end workflow of knowledge infusion, we provide
    a practical guide for tuning foundation models to your specific needs.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 通过详细介绍这一端到端的知识注入工作流程，我们提供了一个将基础模型调优以满足特定需求的实用指南。
- en: Model
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型
- en: The LLaMA2 models was released in July 2023 together with a [research publication](https://scontent.fham6-1.fna.fbcdn.net/v/t39.2365-6/10000000_662098952474184_2584067087619170692_n.pdf?_nc_cat=105&ccb=1-7&_nc_sid=3c67a6&_nc_ohc=Vldy0tYq-X8AX9NLDca&_nc_ht=scontent.fham6-1.fna&oh=00_AfAiZlF_VS1P3n26xqkC3133VVTndBLCZOGyrxkEryDsIw&oe=64F5C5FF).
    In the paper Touvron et al. state that LLaMA2 is “a collection of pre-trained
    and fine-tuned large language models (LLMs) ranging in scale from 7 billion to
    70 billion parameters. Our fine-tuned LLMs, called Llama 2-Chat, are optimised
    for dialogue use cases. Our models outperform open-source chat models on most
    benchmarks we tested, and based on our human evaluations for helpfulness and safety,
    may be a suitable substitute for closedsource models.”
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: LLaMA2模型于2023年7月发布，并附有一篇[研究论文](https://scontent.fham6-1.fna.fbcdn.net/v/t39.2365-6/10000000_662098952474184_2584067087619170692_n.pdf?_nc_cat=105&ccb=1-7&_nc_sid=3c67a6&_nc_ohc=Vldy0tYq-X8AX9NLDca&_nc_ht=scontent.fham6-1.fna&oh=00_AfAiZlF_VS1P3n26xqkC3133VVTndBLCZOGyrxkEryDsIw&oe=64F5C5FF)。在论文中，Touvron等人表示，LLaMA2是“一个预训练和微调的大型语言模型（LLMs）集合，规模从70亿到700亿个参数不等。我们的微调LLMs，称为Llama
    2-Chat，专门针对对话使用场景进行了优化。我们的模型在我们测试的多数基准上超过了开源聊天模型，并且根据我们对有用性和安全性的人工评估，可能是闭源模型的合适替代品。”
- en: 'As stated, the LLaMA2 models come in three sizes: 7B, 13B and 70B. They are
    available as pure completion models as well as optimised for dialog use cases.
    Pre-trained with roughly 2 Trillion tokens, they support context lengths of up
    to 4096 tokens. The fine-tuning for dialog use cases was carried out with over
    100k examples and further optimised with over 1M training samples representing
    human preference.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，LLaMA2模型有三种尺寸：7B、13B和70B。它们既有纯完成模型，也有针对对话使用场景优化的版本。预训练时使用了大约2万亿个标记，它们支持最长4096个标记的上下文长度。针对对话使用场景的微调使用了超过10万个示例，并通过超过100万个代表人类偏好的训练样本进一步优化。
- en: Within AWS the LLaMA2 models can be deployed as easy as with one click through
    [SageMaker JumpStart](https://aws.amazon.com/blogs/machine-learning/llama-2-foundation-models-from-meta-are-now-available-in-amazon-sagemaker-jumpstart/)
    or sourced from the HuggingFace model hub via the [AWSxHuggingFace LLM DLC](https://www.philschmid.de/sagemaker-llama-llm).
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在AWS中，LLaMA2模型可以通过[SageMaker JumpStart](https://aws.amazon.com/blogs/machine-learning/llama-2-foundation-models-from-meta-are-now-available-in-amazon-sagemaker-jumpstart/)一键部署，也可以通过[AWSxHuggingFace
    LLM DLC](https://www.philschmid.de/sagemaker-llama-llm)从HuggingFace模型库中获取。
- en: Fine-tuning
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 微调
- en: Fine-tuning leverages transfer learning to efficiently inject niche expertise
    into a foundation model like LLaMA2\. The process involves updating the model’s
    weights through continued pre-training on domain-specific data, while keeping
    the overall network architecture unchanged. Unlike full pre-training which requires
    massive datasets and compute, fine-tuning is highly sample and compute efficient.
    Parameter-efficient fine-tuning (PEFT) techniques, such as (q)LoRA approach, enable
    light-weight infusion of specialty knowledge into a general language model like
    LLaMA2 with minimal overhead.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 微调利用迁移学习高效地将特定领域的专业知识注入到像LLaMA2这样的基础模型中。该过程涉及通过在特定领域的数据上继续预训练来更新模型的权重，同时保持整体网络结构不变。与需要大量数据集和计算的全面预训练不同，微调在样本和计算方面非常高效。参数高效的微调（PEFT）技术，例如（q）LoRA方法，使得在像LLaMA2这样的通用语言模型中轻量化地注入专业知识成为可能，且开销最小。
- en: 'When speaking about fine-tuning, two different approaches are possible:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在谈到微调时，有两种不同的方法可以选择：
- en: By fine-tuning LLaMA2’s decoder-only architecture on domain-specific corpora
    using its native auto-regressive causal language modeling (CLM) approach, we can
    inject niche expertise or specific language or vocabulary (e.g. medical research
    terminology/speech) into the LLM while retaining its general linguistic capabilities.
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过在领域特定语料库上微调 LLaMA2 的仅解码器架构，利用其原生的自回归因果语言建模（CLM）方法，我们可以将专业知识或特定语言或词汇（例如医疗研究术语/语言）注入到
    LLM 中，同时保留其通用语言能力。
- en: On the other hand, through using task-specific corpora a model can be fine-tuned
    towards a specific task, e.g. instruction following or conversational behaviour.
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 另一方面，通过使用特定任务的语料库，可以将模型微调为特定任务，例如指令跟随或对话行为。
- en: The models utilise a self-supervised training approach optimizing towards a
    language modelling (LM) specific loss function. Decoder-only models like LLaMA2
    are tied towards a Causal Language Modelling (CLM) approach with a uni-directional
    context. In simple words this means that they are trained towards predicting the
    subsequent token in an auto-regressive manner based on the previous ones as semantic
    context.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 这些模型利用自监督训练方法，优化特定于语言建模（LM）的损失函数。像 LLaMA2 这样的仅解码器模型采用了因果语言建模（CLM）方法，具有单向上下文。简单来说，这意味着它们是通过根据之前的词语作为语义上下文，预测后续标记的自回归方式进行训练的。
- en: As mentioned above, [PEFT](https://arxiv.org/abs/2205.05638) techniques enable
    light-weight infusion of specialty knowledge into a an LLM with minimal overhead
    since only a subset of the model parameters is updated. Approaches Low-Rank Adaptation
    ([LoRA](https://arxiv.org/abs/2106.09685)) or Quantized Low-Rank Adaptation ([QLoRA](https://arxiv.org/abs/2305.14314))
    freeze the pre-trained model weights and inject trainable rank decomposition matrices
    into each layer of the Transformer architecture, greatly reducing the number of
    trainable parameters for downstream tasks. If you want to dive deeper into LoRA
    I recommend checking out Mariano Kamp’s [blogpost](https://medium.com/towards-data-science/dive-into-lora-adapters-38f4da488ede).
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 如上所述，[PEFT](https://arxiv.org/abs/2205.05638) 技术使得将专业知识轻量级地注入到 LLM 中成为可能，因为仅更新模型参数的子集。低秩适配方法（[LoRA](https://arxiv.org/abs/2106.09685)）或量化低秩适配方法（[QLoRA](https://arxiv.org/abs/2305.14314)）冻结预训练模型权重，并将可训练的秩分解矩阵注入
    Transformer 架构的每一层，大大减少了下游任务的可训练参数数量。如果你想深入了解 LoRA，建议查看 Mariano Kamp 的 [博客文章](https://medium.com/towards-data-science/dive-into-lora-adapters-38f4da488ede)。
- en: Scenario
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 场景
- en: 'On July 26th, AWS announced various features around the field of generative
    AI at AWS summit NYC. To share additional details several announcement blogposts
    were published:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 7月26日，AWS 在纽约市 AWS 峰会上宣布了生成式 AI 领域的多项新功能。为了分享更多细节，发布了几篇公告博客：
- en: · [Agents for Amazon Bedrock](https://aws.amazon.com/blogs/aws/preview-enable-foundation-models-to-complete-tasks-with-agents-for-amazon-bedrock/)
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: · [Amazon Bedrock 的代理](https://aws.amazon.com/blogs/aws/preview-enable-foundation-models-to-complete-tasks-with-agents-for-amazon-bedrock/)
- en: · [AWS entity resolution match](https://aws.amazon.com/blogs/aws/aws-entity-resolution-match-and-link-related-records-from-multiple-applications-and-data-stores/)
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: · [AWS 实体解析匹配](https://aws.amazon.com/blogs/aws/aws-entity-resolution-match-and-link-related-records-from-multiple-applications-and-data-stores/)
- en: · [Role of vectorstores in generative AI applications](https://aws.amazon.com/blogs/database/the-role-of-vector-datastores-in-generative-ai-applications/)
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: · [向量存储在生成式 AI 应用中的作用](https://aws.amazon.com/blogs/database/the-role-of-vector-datastores-in-generative-ai-applications/)
- en: · [Vector engine for Amazon OpenSearch Serverless](https://aws.amazon.com/blogs/big-data/introducing-the-vector-engine-for-amazon-opensearch-serverless-now-in-preview/)
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: · [Amazon OpenSearch Serverless 的向量引擎](https://aws.amazon.com/blogs/big-data/introducing-the-vector-engine-for-amazon-opensearch-serverless-now-in-preview/)
- en: · [AWS Glue Studio notebook powered by Amazon CodeWhisperer](https://aws.amazon.com/blogs/big-data/build-data-integration-jobs-with-ai-companion-on-aws-glue-studio-notebook-powered-by-amazon-codewhisperer/)
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: · [AWS Glue Studio 笔记本，由 Amazon CodeWhisperer 提供支持](https://aws.amazon.com/blogs/big-data/build-data-integration-jobs-with-ai-companion-on-aws-glue-studio-notebook-powered-by-amazon-codewhisperer/)
- en: · [Amazon EC2 P5](https://aws.amazon.com/blogs/aws/new-amazon-ec2-p5-instances-powered-by-nvidia-h100-tensor-core-gpus-for-accelerating-generative-ai-and-hpc-applications/)
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: · [Amazon EC2 P5](https://aws.amazon.com/blogs/aws/new-amazon-ec2-p5-instances-powered-by-nvidia-h100-tensor-core-gpus-for-accelerating-generative-ai-and-hpc-applications/)
- en: Considering LLaMA2’s knowledge cutoff date unfortunately this model in its pure
    form will not be able to provide any information on these announcements. We want
    to change this through leveraging fine-tuning to infuse this knowledge into the
    Foundation Model (FM) and elevate its knowledge to be able to do so.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到LLaMA2的知识截止日期，不幸的是，该模型的纯形式将无法提供有关这些公告的信息。我们希望通过利用微调将这些知识注入到基础模型（FM）中，并提升其知识水平以实现这一点。
- en: Since we are not aiming to fine-tune the model towards a specific task but simply
    want to ingest domain-specific knowledge, we can go with a classic CLM-based approach
    (Option 1 in previous section). Philipp Schmid describes in his very comprehensive
    [blog](https://www.philschmid.de/sagemaker-llama2-qlora) how to fine-tune LLaMA2
    models with QLoRA, however focussing on task-specific (instruction) fine-tuning.
    We will take his code samples shared with the blogpost as a starting point and
    adjust the code accordingly.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们不打算将模型微调到特定任务上，而是希望摄取领域特定的知识，我们可以使用经典的CLM基础方法（前面章节中的选项1）。Philipp Schmid在他非常全面的[博客](https://www.philschmid.de/sagemaker-llama2-qlora)中描述了如何使用QLoRA对LLaMA2模型进行微调，但重点是任务特定（指令）微调。我们将以他的博客中分享的代码样本为起点，并相应地调整代码。
- en: To be responsible with resource consumption we will conduct the fine-tuning
    for the LLaMA2–13b and LLaMA2–13b-chat models. Fine-tuning the 7b as well as 70b
    versions will work accordingly with adjusted training cluster configuration (see
    in Philipp’s [blog](https://www.philschmid.de/sagemaker-llama2-qlora)). After
    the fine-tuning itself we will deploy the base models alongside with the fine-tuned
    models and do a high-level performance comparison.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 为了负责任地使用资源，我们将对LLaMA2–13b和LLaMA2–13b-chat模型进行微调。微调7b及70b版本将使用调整后的训练集群配置（参见Philipp的[博客](https://www.philschmid.de/sagemaker-llama2-qlora)）。在微调完成后，我们将部署基础模型和微调后的模型，并进行高层次的性能比较。
- en: Subsequently we will do a step-by-step walkthrough through the single steps
    of fine-tuning the models. If you want to access the full code repository you
    can find it [here](https://github.com/aristsakpinis93/finetuning-llama2).
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 随后，我们将逐步讲解微调模型的每个步骤。如果您想访问完整的代码仓库，可以在[这里](https://github.com/aristsakpinis93/finetuning-llama2)找到。
- en: Data loading
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据加载
- en: For data loading we use LangChain’s WebBaseLoader to load an array of websites
    identified by their URL.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 对于数据加载，我们使用LangChain的WebBaseLoader加载由其URL标识的网站数组。
- en: Licensing aggreement & HuggingFace login
  id: totrans-46
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 许可协议与HuggingFace登录
- en: '![](../Images/4e728c608f6173d34d09c784adb7f15e.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4e728c608f6173d34d09c784adb7f15e.png)'
- en: Accepting licensing agreement for LLaMA2 models
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 接受LLaMA2模型的许可协议
- en: Before we can start with data preparation and training, we need to accept the
    license agreement of the LLaMA2 models. This includes a registration on the Meta
    website with the email address matching your HuggingFace account one.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始数据准备和训练之前，我们需要接受LLaMA2模型的许可协议。这包括在Meta网站上的注册，所用的电子邮件地址需与您的HuggingFace账户邮箱匹配。
- en: Then we authenticate with the HuggingFace hub from our runtime.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们从运行时对HuggingFace hub进行身份验证。
- en: Data preparation
  id: totrans-51
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据准备
- en: First, we do some preprocessing on the raw websites. In a real-world use case
    more emphasis could be put into this stage. For demo purposes we stick to simply
    striping all larger concatenations of spaces so we get a cohesive and fluent text
    corpus. Then we load the list of docs into a HuggingFace Dataset.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们对原始网站进行一些预处理。在实际应用中，可能会更重视这一步骤。为了演示的目的，我们仅仅去除了较大的空格拼接，以获得一个连贯流畅的文本语料库。然后我们将文档列表加载到HuggingFace数据集中。
- en: In the next step we are tokenising our text corpus to make it digestible for
    the LLM. Therefor we use the LLaMA2 tokeniser loaded from the HuggingFace hub.
    After being batched up according to the context window of the model (2048 tokens)
    the loaded tokenizer can be used for tokenization of the created batches.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一步中，我们对文本语料库进行分词处理，使其适合LLM。为此，我们使用从HuggingFace hub加载的LLaMA2分词器。加载后的分词器可以用于对根据模型的上下文窗口（2048个tokens）批量处理后的数据进行分词。
- en: Then we save the dataset on S3 for usage within a training job.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们将数据集保存到S3中，以便在训练作业中使用。
- en: Training
  id: totrans-55
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 培训
- en: Now we can trigger an Amazon SageMaker training job for executing a CLM-tied
    QLoRA fine-tuning script based on the preprocessed data.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以触发一个Amazon SageMaker训练作业，执行基于预处理数据的CLM绑定QLoRA微调脚本。
- en: The hyperparameters and training script is adapted from Philipp’s blogpost.
    The only exception is the number of training epochs executed, where with 20 we
    choose a relatively high amount of epochs. This is to account for the fact that
    our training dataset is rather small (~32k tokens). In real world use cases fine-tuning
    with larger datasets is advised. The training script itself can be found [here](https://github.com/aristsakpinis93/finetuning-llama2/blob/main/scripts/run_clm.py).
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 超参数和训练脚本参考了 Philipp 的博客文章。唯一的例外是执行的训练轮次数量，我们选择了相对较高的 20 轮。这是为了考虑到我们的训练数据集相对较小（约
    32k 个标记）。在实际应用中，建议使用更大的数据集进行微调。训练脚本本身可以在 [这里](https://github.com/aristsakpinis93/finetuning-llama2/blob/main/scripts/run_clm.py)
    找到。
- en: 'Please also note: Since the accepted model license agreement is tied to your
    HuggingFace account, we need to specify a HuggingFace access token.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意：由于接受的模型许可证协议与您的 HuggingFace 帐户绑定，我们需要指定 HuggingFace 访问令牌。
- en: The training job configuration, e.g. training cluster configuration was adapted
    from Philipp’s blogpost as well.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 训练作业配置，例如训练集群配置，也参考了 Philipp 的博客文章。
- en: We can then execute the training job, which will perform the fine-tuning and
    save our model artefacts into S3.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们可以执行训练作业，这将进行微调并将模型工件保存到 S3 中。
- en: Deployment
  id: totrans-61
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 部署
- en: The two base models LLaMA2–13b and LLaMA2–13b-chat can be conveniently deployed
    via SageMaker JumpStart. This [blogpost](https://aws.amazon.com/blogs/machine-learning/llama-2-foundation-models-from-meta-are-now-available-in-amazon-sagemaker-jumpstart/)
    provides detailed instructions on this.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 基础模型 LLaMA2–13b 和 LLaMA2–13b-chat 可以通过 SageMaker JumpStart 方便地部署。这个 [博客文章](https://aws.amazon.com/blogs/machine-learning/llama-2-foundation-models-from-meta-are-now-available-in-amazon-sagemaker-jumpstart/)
    提供了详细的说明。
- en: The two fine-tuned models can be deployed as by using the AWSxHuggingFace LLM
    DLC. Therefor we point the model_data parameter of the HuggingFaceModel class
    to the s3 path of the model artefacts. Then we can point the environment variable
    HF_MODEL_ID set in the hosting container to the default model path within a DLC
    (“/opt/ml/model”).
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 经过微调的两个模型可以通过使用 AWSxHuggingFace LLM DLC 进行部署。因此，我们将 HuggingFaceModel 类的 model_data
    参数指向模型工件的 S3 路径。然后，我们可以将托管容器中设置的环境变量 HF_MODEL_ID 指向 DLC 内的默认模型路径（“/opt/ml/model”）。
- en: Testing
  id: totrans-64
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 测试
- en: 'After having deployed all four models we want to test the performance on an
    example question. The announcement [blogpost](https://aws.amazon.com/blogs/aws/new-amazon-ec2-p5-instances-powered-by-nvidia-h100-tensor-core-gpus-for-accelerating-generative-ai-and-hpc-applications/)
    on Amazon EC2 P5 instances states: “P5 instances provide 8 x NVIDIA H100 Tensor
    Core GPUs with 640 GB of high bandwidth GPU memory, 3rd Gen AMD EPYC processors,
    2 TB of system memory, and 30 TB of local NVMe storage. P5 instances also provide
    3200 Gbps of aggregate network bandwidth with support for GPUDirect RDMA, enabling
    lower latency and efficient scale-out performance by bypassing the CPU on internode
    communication.”'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 部署所有四个模型后，我们希望在示例问题上测试性能。关于 Amazon EC2 P5 实例的公告 [博客文章](https://aws.amazon.com/blogs/aws/new-amazon-ec2-p5-instances-powered-by-nvidia-h100-tensor-core-gpus-for-accelerating-generative-ai-and-hpc-applications/)
    说明：“P5 实例提供 8 个 NVIDIA H100 Tensor Core GPU，配有 640 GB 的高带宽 GPU 内存，第三代 AMD EPYC
    处理器，2 TB 系统内存，以及 30 TB 本地 NVMe 存储。P5 实例还提供 3200 Gbps 的汇总网络带宽，支持 GPUDirect RDMA，通过绕过
    CPU 实现更低延迟和高效的扩展性能。”
- en: 'We frame the following question: “What are Amazon EC2 P5 instances? Which kind
    of GPUs are they equipped with?” The chat models answer as follows with an inference
    configuration of max_new_tokens = 200, top_p = 0.9, temperature = 0.01:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提出以下问题：“什么是 Amazon EC2 P5 实例？它们配备了什么类型的 GPU？”聊天模型在推理配置 max_new_tokens = 200,
    top_p = 0.9, temperature = 0.01 下的回答如下：
- en: 'LLaMA2–13b-chat: “*Amazon Elastic Compute Cloud (EC2) P5 instances are a type
    of instance that provides high-performance computing resources for applications
    that require intense computational power. These instances are equipped with NVIDIA
    Tesla V100 GPUs, which are designed for machine learning, scientific simulations,
    and other high-performance computing workloads.\n\nThe NVIDIA Tesla V100 GPUs
    in P5 instances are based on the Volta architecture and offer a number of features
    that make them well-suited for demanding workloads, including:\n\n* 5120 CUDA
    cores for high-performance computing\n* 640 tensor cores for accelerating deep
    learning and other matrix-based workloads\n* 16 GB of GDDR6 memory for fast data
    transfer and processing\n* Support for NVIDIA’s GPU-accelerated libraries and
    frameworks, such as CU…*”'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'LLaMA2–13b-chat: “*Amazon Elastic Compute Cloud (EC2) P5 实例是一种提供高性能计算资源的实例，适用于需要强大计算能力的应用程序。这些实例配备了
    NVIDIA Tesla V100 GPU，专为机器学习、科学模拟和其他高性能计算工作负载设计。\n\nAmazon EC2 P5 实例的 NVIDIA Tesla
    V100 GPU 基于 Volta 架构，并提供多种功能，使其非常适合需求严格的工作负载，包括：\n\n* 5120 个 CUDA 核心，用于高性能计算\n*
    640 个张量核心，用于加速深度学习和其他基于矩阵的工作负载\n* 16 GB 的 GDDR6 内存，用于快速数据传输和处理\n* 支持 NVIDIA 的
    GPU 加速库和框架，如 CU…*”'
- en: 'LLaMA2–13b-chat-nyc-finetuned: “*Amazon EC2 P5 instances are general-purpose
    instances that provide extreme performance for applications that require heavy
    graphics processing and complex machine learning (ML) models. They are powered
    by NVIDIA H100 Tensor Core GPUs and third- generation AMD EPYC processors. The
    H100 GPUs provide 640 GB of high bandwidth GPU memory, enabling you to run your
    most demanding applications with massive datasets in real time. P5 instances also
    provide 2000 GB of high-speed CPU memory, allowing you to process vast amounts
    of data in memory without having to access disk storage. This reduces the processing
    time and improves response time. You can use these instances for applications
    such as computer vision, video encoding, genome analysis, and language model training.*”'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'LLaMA2–13b-chat-nyc-finetuned: “*Amazon EC2 P5 实例是一种通用实例，为需要大量图形处理和复杂机器学习（ML）模型的应用程序提供极端性能。它们由
    NVIDIA H100 Tensor Core GPU 和第三代 AMD EPYC 处理器驱动。H100 GPU 提供 640 GB 高带宽 GPU 内存，使您能够在实时运行大规模数据集的最具挑战性应用程序。P5
    实例还提供 2000 GB 高速 CPU 内存，允许您在内存中处理大量数据，而无需访问磁盘存储。这减少了处理时间并改善了响应时间。您可以将这些实例用于计算机视觉、视频编码、基因组分析和语言模型训练等应用程序.*”'
- en: We can clearly see that while the base model hallucinates about the GPU type
    (V100) the fine-tuned model provides us with the correct answer (H100). We also
    see that through qLoRA we can preserve the chat-finetuned nature of the base model
    and hence mitigate catastrophic forgetting. This means that we can infuse knowledge
    into a LLM without having to do a fully fledged instruction/chat-fine-tuning afterwards
    by just using a respectively task-fine-tuned model as base model. The reason why
    this works is the nature of LoRA where large parts of every layer of the neural
    network stay untouched while the layers are extended.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以清楚地看到，尽管基础模型幻想了 GPU 类型（V100），但经过精调的模型为我们提供了正确的答案（H100）。我们还看到，通过 qLoRA 我们可以保留基础模型的聊天精调特性，从而减轻灾难性遗忘。这意味着我们可以向
    LLM 注入知识，而不必在之后进行完全的指令/聊天精调，只需使用相应的任务精调模型作为基础模型即可。这种方法有效的原因在于 LoRA 的性质，其中神经网络的每一层的大部分部分保持不变，而层级则被扩展。
- en: For the pure completion version of the models we need to rephrase the question
    since these models are not capable to understand instructions or behave in a conversational
    manner. Instead they are simply completing token sequences through auto-regressive
    next token prediction.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 对于模型的纯完成版本，我们需要重新提出问题，因为这些模型无法理解指令或以对话方式行事。相反，它们只是通过自回归的下一个 token 预测来完成标记序列。
- en: 'We frame the following question: “Amazon EC2 P5 instances are equipped with
    GPUs of the type” The completion models answer as follows with an inference configuration
    of max_new_tokens = 200, top_p = 0.9, temperature = 0.01:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提出以下问题：“Amazon EC2 P5 实例配备了类型的 GPU” 完成模型通过推断配置为 max_new_tokens = 200，top_p
    = 0.9，temperature = 0.01 回答如下：
- en: 'LLaMA2–13b: “*NVIDIA Tesla V100.\nThe NVIDIA Tesla V100 is a GPU that is designed
    for high-performance computing. It is based on the NVIDIA Volta architecture and
    is equipped with 32GB of HBM2 memory. The V100 is capable of delivering up to
    100 teraflops of performance and is designed for use in data centers and supercomputers.\nThe
    NVIDIA Tesla V100 is a powerful GPU that is designed for high-performance computing.
    It is based on the NVIDIA Volta architecture and is equipped with 32GB of HBM2
    memory. The V100 is capable of delivering up to 100 teraflops of performance and
    is designed for use in data centers and supercomputers.\nThe NVIDIA Tesla V1…*”'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LLaMA2–13b：“*NVIDIA Tesla V100。\nNVIDIA Tesla V100 是一种旨在进行高性能计算的 GPU。它基于 NVIDIA
    Volta 架构，并配备了 32GB 的 HBM2 内存。V100 能够提供高达 100 teraflops 的性能，并设计用于数据中心和超级计算机。\nNVIDIA
    Tesla V100 是一种强大的 GPU，旨在进行高性能计算。它基于 NVIDIA Volta 架构，并配备了 32GB 的 HBM2 内存。V100 能够提供高达
    100 teraflops 的性能，并设计用于数据中心和超级计算机。\nNVIDIA Tesla V1…*”
- en: LLaMA2–13b-nyc-finetuned:“*NVIDIA A100 for compute-intensive workloads, such
    as generative AI, high performance computing (HPC), and data analytics. P5 instances
    are ideal for training and running inference in models with trillions of parameters
    and for performing data analysis on vast datasets. They provide 8 times the GPU
    memory and 64 times the compute performance in floating point operations (FLOPs)
    compared to P4 instances.\nYou can use P5 instances in Amazon Elastic Container
    Service (ECS) with the most common container tools and frameworks, such as Docker,
    PyTorch, TensorFlow, Jupyter Notebook, R Studio, and Venv, or you can use P5 instances
    via Amazon Elastic MapReduce (EMR) or Amazon DeepSense with ML and AI frameworks
    and tools provided by Amazon, or by other companies. To make it easier to build
    generative AI applications on P5 instances,…*”
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LLaMA2–13b-nyc-finetuned：“*NVIDIA A100 用于计算密集型工作负载，例如生成 AI、高性能计算（HPC）和数据分析。P5
    实例非常适合训练和运行具有万亿参数的模型以及在庞大数据集上进行数据分析。与 P4 实例相比，它们提供了 8 倍的 GPU 内存和 64 倍的浮点运算（FLOPs）计算性能。\n您可以在
    Amazon Elastic Container Service（ECS）中使用 P5 实例，支持最常见的容器工具和框架，如 Docker、PyTorch、TensorFlow、Jupyter
    Notebook、R Studio 和 Venv，或通过 Amazon Elastic MapReduce（EMR）或 Amazon DeepSense 使用
    Amazon 或其他公司提供的 ML 和 AI 框架和工具来使用 P5 实例。为了便于在 P5 实例上构建生成 AI 应用程序，…*”
- en: While fine-tuning the chat-finetuned model with this small amount of data has
    led to elimination of the model’s halluzination with the base LLaMA2 model this
    does not seem to work in our setup. This might be because of the limited size
    of our fine-tuning dataset. Also, more sophisticated prompt engineering and optimising
    the inference configuration parameter setup could be helpful.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管用少量数据微调聊天微调模型已导致消除了基于LLaMA2模型的幻觉，但在我们的设置中似乎不起作用。这可能是因为我们微调数据集的大小有限。此外，更复杂的提示工程和优化推理配置参数设置可能会有所帮助。
- en: Conclusion
  id: totrans-75
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: In conclusion, this blog post delves into the critical process of infusing domain-specific
    knowledge into large language models (LLMs) like LLaMA2, emphasizing the importance
    of addressing challenges related to helpfulness, honesty, and harmlessness when
    designing LLM-powered applications for enterprise-grade quality. The primary focus
    here is on the parametric approach to fine-tuning, which efficiently injects niche
    expertise into foundation models without compromising their general linguistic
    capabilities.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，这篇博客文章深入探讨了将领域特定知识融入大型语言模型（LLMs）如LLaMA2的关键过程，强调了在设计LLM驱动的企业级应用时，解决有用性、诚实性和无害性等挑战的重要性。这里的主要关注点是参数化微调方法，它可以有效地将专业知识注入基础模型而不影响其通用语言能力。
- en: The blog highlights the steps involved in fine-tuning LLaMA2 using parameter-efficient
    fine-tuning techniques, such as the qLoRA approach, and how this process can be
    conducted on Amazon SageMaker. By adopting this approach, practitioners can adapt
    LLaMA2 to specific domains, ensuring that the models remain up-to-date with recent
    knowledge even beyond their original training data.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 博客文章重点介绍了使用参数高效微调技术（如qLoRA方法）微调LLaMA2的步骤，以及如何在Amazon SageMaker上进行这一过程。通过采用这种方法，实践者可以将LLaMA2适应于特定领域，确保模型即使在超出原始训练数据的情况下也能保持最新的知识。
- en: The article also underscores the versatility of this approach, showing that
    it can be applied to models like LLaMA2-chat, which have already undergone task-specific
    fine-tuning. This opens up opportunities to infuse knowledge into LLMs without
    the need for extensive instruction or chat-based fine-tuning, preserving their
    task-specific nature.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 文章还强调了这种方法的多功能性，显示它可以应用于像LLaMA2-chat这样的模型，这些模型已经经过了特定任务的微调。这为将知识注入到LLM中提供了机会，而无需进行广泛的指令或基于对话的微调，从而保持了其特定任务的特性。
