- en: How I Built a Lo-fi Music Web Player with AI-Generated Tracks
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何构建带有 AI 生成曲目的 Lo-fi 音乐网页播放器
- en: 原文：[https://towardsdatascience.com/how-i-built-a-lo-fi-music-web-player-with-ai-generated-tracks-36f3915e39f8?source=collection_archive---------4-----------------------#2023-01-12](https://towardsdatascience.com/how-i-built-a-lo-fi-music-web-player-with-ai-generated-tracks-36f3915e39f8?source=collection_archive---------4-----------------------#2023-01-12)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/how-i-built-a-lo-fi-music-web-player-with-ai-generated-tracks-36f3915e39f8?source=collection_archive---------4-----------------------#2023-01-12](https://towardsdatascience.com/how-i-built-a-lo-fi-music-web-player-with-ai-generated-tracks-36f3915e39f8?source=collection_archive---------4-----------------------#2023-01-12)
- en: Made with Tone.js and LSTM Model
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 Tone.js 和 LSTM 模型制作
- en: '[](https://medium.com/@leksa_86?source=post_page-----36f3915e39f8--------------------------------)[![Aleksandra
    Ma](../Images/71fc31d60c5b73391d8ed1178639e541.png)](https://medium.com/@leksa_86?source=post_page-----36f3915e39f8--------------------------------)[](https://towardsdatascience.com/?source=post_page-----36f3915e39f8--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----36f3915e39f8--------------------------------)
    [Aleksandra Ma](https://medium.com/@leksa_86?source=post_page-----36f3915e39f8--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@leksa_86?source=post_page-----36f3915e39f8--------------------------------)[![Aleksandra
    Ma](../Images/71fc31d60c5b73391d8ed1178639e541.png)](https://medium.com/@leksa_86?source=post_page-----36f3915e39f8--------------------------------)[](https://towardsdatascience.com/?source=post_page-----36f3915e39f8--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----36f3915e39f8--------------------------------)
    [Aleksandra Ma](https://medium.com/@leksa_86?source=post_page-----36f3915e39f8--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Feffc1ebd4aac&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-i-built-a-lo-fi-music-web-player-with-ai-generated-tracks-36f3915e39f8&user=Aleksandra+Ma&userId=effc1ebd4aac&source=post_page-effc1ebd4aac----36f3915e39f8---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----36f3915e39f8--------------------------------)
    ·11 min read·Jan 12, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F36f3915e39f8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-i-built-a-lo-fi-music-web-player-with-ai-generated-tracks-36f3915e39f8&user=Aleksandra+Ma&userId=effc1ebd4aac&source=-----36f3915e39f8---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Feffc1ebd4aac&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-i-built-a-lo-fi-music-web-player-with-ai-generated-tracks-36f3915e39f8&user=Aleksandra+Ma&userId=effc1ebd4aac&source=post_page-effc1ebd4aac----36f3915e39f8---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----36f3915e39f8--------------------------------)
    · 11分钟阅读 · 2023年1月12日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F36f3915e39f8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-i-built-a-lo-fi-music-web-player-with-ai-generated-tracks-36f3915e39f8&user=Aleksandra+Ma&userId=effc1ebd4aac&source=-----36f3915e39f8---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F36f3915e39f8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-i-built-a-lo-fi-music-web-player-with-ai-generated-tracks-36f3915e39f8&source=-----36f3915e39f8---------------------bookmark_footer-----------)![](../Images/060caa701829c3521df172e7dce64605.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F36f3915e39f8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-i-built-a-lo-fi-music-web-player-with-ai-generated-tracks-36f3915e39f8&source=-----36f3915e39f8---------------------bookmark_footer-----------)![](../Images/060caa701829c3521df172e7dce64605.png)'
- en: Photo by [rupixen.com](https://unsplash.com/@rupixen?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源于 [rupixen.com](https://unsplash.com/@rupixen?utm_source=medium&utm_medium=referral)
    在 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: Introduction
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: Lo-fi hip hop music has been my go-to study buddy ever since college. It creates
    a cozy and calming vibe with a relatively simple musical structure. Some jazzy
    chord progressions, groovy drum patterns, ambience sounds, and nostalgic movie
    quotes can give us a pretty decent sounding lo-fi hip hop track. On top of the
    musical side, animated visuals are also a crucial part of the lo-fi aesthetics,
    setting the ambience along with the nature sounds of water, wind, and fire.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 自大学以来，低保真嘻哈音乐一直是我学习时的首选伴侣。它以相对简单的音乐结构营造出舒适和宁静的氛围。一些爵士和弦进行、律动的鼓点、环境声音和怀旧的电影台词可以让我们制作出相当不错的低保真嘻哈曲目。除了音乐方面，动画视觉效果也是低保真美学的关键部分，与水、风和火的自然声音一起营造氛围。
- en: 'The idea to create my own lo-fi web player occurred to me one Sunday afternoon
    when I was learning about deep generative models. I did some research and finished
    the project during the holiday times. The web player provides two options: users
    can either choose a lo-fi track based on a real song encoded with Tone.js, or
    they could choose an AI-generated solo track. Both options will be layered on
    top of the drum loop, ambience sounds, and quotes that users selected in the previous
    step. This post will mainly talk about how to use LSTM models to generate a midi
    track, and I will briefly discuss how to make a song with Tone.js at the end.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 创建我自己的低保真网页播放器的想法是在一个星期天下午，当时我正在学习深度生成模型。我做了一些研究，并在假期期间完成了这个项目。该网页播放器提供两个选项：用户可以选择基于Tone.js编码的真实歌曲的低保真曲目，或者选择AI生成的独奏曲目。两个选项都会与用户在前一步中选择的鼓循环、环境声音和台词叠加在一起。本文主要讨论如何使用LSTM模型生成midi曲目，并在最后简要讨论如何使用Tone.js制作一首歌曲。
- en: You can try the web player [here](https://mtsandra.github.io/lofi-station),
    and I recommend using the desktop Chrome browser for the best experience.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在[这里](https://mtsandra.github.io/lofi-station)尝试网页版播放器，我建议使用桌面版Chrome浏览器以获得最佳体验。
- en: LSTM Model Architecture & Midi Generation
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LSTM模型架构与Midi生成
- en: In a previous [post](https://mtsandra.github.io/blog/2022/dl4mir-4), I explained
    what an LSTM network is. For a quick refresher, it is a special type of RNN that
    handles long-term dependencies better. It also has a recurrent structure that
    takes the output from the previous time step at the current time step. To better
    understand it, we can unroll the network and think of an LSTM cell as multiple
    copies of the same network, each passing a message to the next time step, as shown
    below.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的[帖子](https://mtsandra.github.io/blog/2022/dl4mir-4)中，我解释了什么是LSTM网络。简要回顾一下，它是一种特殊类型的RNN，能够更好地处理长期依赖。它还具有递归结构，将来自前一个时间步的输出传递到当前时间步。为了更好地理解它，我们可以展开网络，将LSTM单元看作是多个相同网络的副本，每个副本将信息传递给下一个时间步，如下所示。
- en: '![](../Images/f4ad4704c26f827c5ffa69ba666e42c0.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f4ad4704c26f827c5ffa69ba666e42c0.png)'
- en: Unrolled LSTM; Created by Author
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 展开LSTM；作者创建
- en: 'Each cell contains four main components that allow them to handle long term
    dependencies better:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 每个单元格包含四个主要组件，使其能够更好地处理长期依赖：
- en: 'forget gate: determines what information to forget'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 遗忘门：决定遗忘哪些信息
- en: 'input gate: determines what information to update and store in our cell state'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输入门：决定更新和存储哪些信息到我们的单元状态中
- en: 'cell state update: make element-wise operations to update the cell state'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 单元状态更新：进行逐元素操作以更新单元状态
- en: 'output gate: determines what information to output'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输出门：决定输出哪些信息
- en: '![](../Images/075fbe6de0ca864e8082db76df1e6c8f.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/075fbe6de0ca864e8082db76df1e6c8f.png)'
- en: Inside an LSTM cell; created by Author
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在LSTM单元内部；作者创建
- en: Training Data Preparation
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练数据准备
- en: 'We have a couple of options when it comes to the music data format we are training
    the model on: raw audio, audio features (e.g. time frequency representations like
    Mel spectrogram), or symbolic music representation (e.g. midi files). Our goal
    is to generate a solo track (i.e. a sequence of notes, chords, and rests) to layer
    on other components like drum loops, so midi files are the easiest and most effective
    format to achieve our goal. Raw audio is very computationally expensive to train
    on. To put it in perspective, music clips sampled at 48000kHz mean there are 48000
    data points in one second of audio. Even if we downsample it to 8kHz, that is
    still 8000 data points for every second. In addition, clean audio of only the
    melody or chord progression is extremely rare. However, we could still find some
    midi files containing only the chord progression / melody if we try hard enough.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们训练模型所使用的音乐数据格式，我们有几种选择：原始音频、音频特征（例如 Mel 频谱图）或符号音乐表示（例如 midi 文件）。我们的目标是生成一个独奏曲目（即音符、和弦和休止符的序列），以便在其他组件（如鼓循环）上叠加，因此
    midi 文件是实现我们目标的最简单和最有效的格式。原始音频的训练计算开销非常大。换句话说，以 48000kHz 采样的音乐片段意味着每秒音频中有 48000
    个数据点。即使我们将其下采样到 8kHz，这仍然是每秒 8000 个数据点。此外，仅有旋律或和弦进行的干净音频极其稀少。然而，如果我们足够努力，仍然可以找到一些只包含和弦进行/旋律的
    midi 文件。
- en: For this project, I used some lo-fi midi samples from YouTube creator [Michael
    Kim-Sheng](https://www.youtube.com/watch?v=9gAgtc5A5UU&ab_channel=MichaelKim-Sheng),
    who have generously given me permission to use his files. I also leveraged some
    midi files in this [Cymatics lo-fi toolkit](https://cymatics.fm/products/lofi-toolkit)
    licensed for commercial use. In order to make sure that I am training my model
    on quality data (plausible chord progression and meter for lo-fi hip hop), I listened
    to a subset of tracks from each source and filtered out my training dataset. The
    model architecture is inspired by the classical piano composer repository [here](https://github.com/Skuldur/Classical-Piano-Composer).
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个项目中，我使用了一些来自 YouTube 创作者[Michael Kim-Sheng](https://www.youtube.com/watch?v=9gAgtc5A5UU&ab_channel=MichaelKim-Sheng)的
    lo-fi midi 样本，他慷慨地允许我使用他的文件。我还利用了一些在这个[Cymatics lo-fi toolkit](https://cymatics.fm/products/lofi-toolkit)中授权用于商业用途的
    midi 文件。为了确保我在质量数据上训练我的模型（适用于 lo-fi 嘻哈的合理和弦进行和节拍），我听了一部分来自每个来源的曲目，并筛选出我的训练数据集。模型架构的灵感来自于经典钢琴作曲家库[这里](https://github.com/Skuldur/Classical-Piano-Composer)。
- en: Load and encode the midi files
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 加载和编码 midi 文件
- en: A Python package [music21](http://web.mit.edu/music21/doc/index.html) can be
    used to load the midi files. Music21 parses a midi file and stores each musical
    component into a specific Python object. In other words, a note will be saved
    as a Note object, a chord will be saved as a Chord object, and a rest will be
    saved as a Rest object. Their name, duration, pitch class and other attributes
    can be accessed through the dot notation. Music21 stores the music clip in a hierarchy
    shown as below, and we can extract the necessary information accordingly. If you
    are interested in how to use the package, the package website has a beginner-friendly
    user guide and Valerio Velardo from The Sound of AI has a [tutorial](https://www.youtube.com/watch?v=coEgwnMBuo0&ab_channel=ValerioVelardo-TheSoundofAI)
    on how to use music21 as well.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 一个 Python 包[music21](http://web.mit.edu/music21/doc/index.html)可以用于加载 midi 文件。Music21
    解析 midi 文件并将每个音乐组件存储到特定的 Python 对象中。换句话说，音符将被保存为 Note 对象，和弦将被保存为 Chord 对象，而休止符将被保存为
    Rest 对象。它们的名称、持续时间、音高类别和其他属性可以通过点号表示法访问。Music21 将音乐片段存储在如下面所示的层次结构中，我们可以相应地提取必要的信息。如果你对如何使用这个包感兴趣，包的官方网站有一个适合初学者的用户指南，The
    Sound of AI 的 Valerio Velardo 也有一个[教程](https://www.youtube.com/watch?v=coEgwnMBuo0&ab_channel=ValerioVelardo-TheSoundofAI)来介绍如何使用
    music21。
- en: As mentioned previously, music21 stores each note, rest, and chord as a Python
    object, so the next step is to encode them and map them to integers that the model
    can train on. The model output should contain not just notes, but also chords
    and rests, so we will encode each type separately and map the encoded value to
    an integer. We do this for all the midi files and concatenate them into one sequence
    to train the model on.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，music21 将每个音符、休止符和和弦存储为一个 Python 对象，因此下一步是对它们进行编码，并将它们映射到模型可以训练的整数上。模型输出应该包含不仅仅是音符，还包括和弦和休止符，因此我们将分别对每种类型进行编码，并将编码值映射到整数。我们对所有的
    midi 文件执行此操作，并将它们连接成一个序列以训练模型。
- en: 'Chords: get the pitch names of the notes in a chord, convert them to their
    normal order, and connect them with a dot in the string format, “#.#.#”'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 和弦：获取和弦中音符的音高名称，将它们转换为正常顺序，并用点连接成字符串格式，“#.#.#”
- en: 'Notes: use the pitch name as the encoding'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 音符：使用音高名称作为编码
- en: 'Rests: encoded as the string “r”'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 休止符：编码为字符串“r”
- en: '![](../Images/b8911fb7ad38af2c6f52a0f8517f02f1.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b8911fb7ad38af2c6f52a0f8517f02f1.png)'
- en: '*Midi Encoding and Mapping*'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '*Midi 编码与映射*'
- en: Create the Input and Target Pairs
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建输入和目标对
- en: Now we have a model-friendly encoding of our midi data. The next step would
    be to prepare the input and target pairs to feed into the model. In a simple supervised
    classification machine learning problem, there are input and target pairs. For
    example, a model that classifies a dog breed will have the fur color, height,
    weight, and eye color of the dog as input and the label / target will be the specific
    dog breed the dog belongs to. In our case, the input will be a sequence of length
    k starting from time step i, and the corresponding target will be the data point
    at time step i+k. So we will loop through our encoded note sequence and create
    the input and target pairs for the model. As a last step, we change the dimension
    of the input to the keras-friendly format and one-hot encode the output.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经得到了适合模型的 midi 数据编码。下一步是准备输入和目标对，以便输入到模型中。在一个简单的监督分类机器学习问题中，存在输入和目标对。例如，一个对狗品种进行分类的模型将以狗的毛色、身高、体重和眼睛颜色作为输入，而标签/目标将是狗所属的具体品种。在我们的情况下，输入将是从时间步
    i 开始的长度为 k 的序列，而对应的目标将是时间步 i+k 的数据点。因此，我们将遍历编码后的音符序列，并为模型创建输入和目标对。最后一步，我们将输入的维度转换为适合
    keras 的格式，并对输出进行 one-hot 编码。
- en: Model Structure
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型结构
- en: 'As previously mentioned, we will use LSTM layers as our core structure. In
    addition, this network also uses the below components:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，我们将使用 LSTM 层作为核心结构。此外，该网络还使用了以下组件：
- en: 'Dropout layers: to regularize the network and prevent overfitting by randomly
    setting input units to 0 with a frequency of rate at each step during training
    time (in our case, the rate is 0.3)'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dropout 层：通过在训练期间以一定频率随机将输入单元设置为 0 来规范化网络并防止过拟合（在我们的例子中，频率为 0.3）
- en: 'Dense layers: fully connects the preceding layer and performs matrix-vector
    multiplication in each node. The last dense layer needs to have the same amount
    of nodes as the total number of unique notes/chords/rest in our network.'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 密集层：完全连接前一层，并在每个节点执行矩阵-向量乘法。最后一层密集层需要具有与网络中唯一音符/和弦/休止符的总数相同的节点数。
- en: 'Activation layers: adds non-linearity to our network if used in a hidden layer
    and helps the network classify to a class if used in an output layer.'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 激活层：如果用于隐藏层，则为我们的网络添加非线性；如果用于输出层，则帮助网络进行分类。
- en: '[PRE0]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: For this example network, 3 LSTM layers are used with 2 dropout layers each
    following the first two LSTM layers. Then there are 2 fully connected dense layers,
    followed by one softmax activation function. Since the outputs are categorical,
    we will use categorical cross entropy as the loss function. The RMSprop optimizer
    is used, which is pretty common for RNNs. Checkpoints are also added so that weights
    are regularly saved at different epochs and could be used before the model finishes
    training. Please feel free to tweak the model structure, and try with different
    optimizers, epochs, and batch sizes.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个示例网络，使用了 3 层 LSTM，每层 LSTM 后面跟有 2 层 dropout 层。接着是 2 层全连接的密集层，最后是一个 softmax
    激活函数。由于输出是分类的，我们将使用分类交叉熵作为损失函数。使用了 RMSprop 优化器，这在 RNN 中相当常见。还添加了检查点，以便在不同的训练轮次中定期保存权重，并在模型训练结束之前使用。请随意调整模型结构，并尝试不同的优化器、轮次和批量大小。
- en: Output Generation & Decoding Back to Midi Notes
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 输出生成与解码回 Midi 音符
- en: The output generation process is similar to the training process — we give the
    model a sequence of length m (we’ll also call it sequence m for notation simplification)
    and ask it to predict the next data point. This sequence m has a start index randomly
    selected from the input sequence, but we can also specify a specific start index
    if we wish. The model output is a list of probabilities from softmax that tell
    us how much each class is suited as the next data point. We will pick the class
    with the highest probability. In order to generate a sequence of length j, we
    will repeat this process by removing the first element of the sequence m and adding
    the recently generated data point to this sequence m, until the model generates
    j new data points.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 输出生成过程类似于训练过程——我们给模型一个长度为m的序列（我们也称之为序列m以简化符号表示），并要求它预测下一个数据点。这个序列m的起始索引是从输入序列中随机选择的，但如果我们愿意，也可以指定一个特定的起始索引。模型输出是一个来自softmax的概率列表，告诉我们每个类别作为下一个数据点的适合程度。我们将选择概率最高的类别。为了生成长度为j的序列，我们会通过删除序列m的第一个元素并将最近生成的数据点添加到这个序列m中来重复这个过程，直到模型生成j个新的数据点。
- en: The data generated from the last paragraph is still an integer, so we decode
    it back to a note/chord/rest using the same mappings during encoding. If it is
    a chord string format, we will read the integer notation from the string “#.#.#.#”
    and create a music21.chord object. If it is a note or rest, we will create a corresponding
    note and rest object accordingly. At the same time, we append the new data point
    generated to the prediction output sequence at each time step. For an illustration
    of this process, please see the example flow below where we are generating a sequence
    of 4 data points with an input sequence of 3 data points.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 从上一段生成的数据仍然是整数，因此我们使用编码过程中相同的映射将其解码回音符/和弦/休止符。如果它是和弦字符串格式，我们将从字符串“#.#.#.#”中读取整数符号，并创建一个music21.chord对象。如果它是音符或休止符，我们将相应地创建一个对应的音符和休止符对象。同时，我们在每个时间步将生成的新数据点追加到预测输出序列中。有关此过程的示例，请参见下面的示例流程，我们使用3个数据点的输入序列生成4个数据点的序列。
- en: '![](../Images/fc605f964ddbc95d6782a39ea9b72590.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fc605f964ddbc95d6782a39ea9b72590.png)'
- en: '*Output Generation and Midi Decoding*'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '*输出生成和MIDI解码*'
- en: Now we have a sequence of notes, chords, and rests. We could put them in a music21
    stream and write out the midi file, in which case all the notes will be quarter
    notes. To keep the output a little bit more interesting, I added a code snippet
    that randomly samples a duration to specify for each note or chord (The default
    probability distribution is 0.65 for eighth notes, 0.25 for 16th notes, 0.05 for
    both quarter and half notes). Rests are defaulted to 16th rests so that we don’t
    have too long of a silence between notes.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了一系列的音符、和弦和休止符。我们可以将它们放入一个music21流中并写出MIDI文件，在这种情况下，所有的音符都会是四分音符。为了保持输出的趣味性，我添加了一个代码片段，随机抽取一个时长来指定每个音符或和弦的时长（默认的概率分布是0.65用于八分音符，0.25用于十六分音符，0.05用于四分音符和二分音符）。休止符默认为十六分休止符，以避免音符之间的沉默过长。
- en: '[PRE1]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Once we run the model for a few times with different parameters and pick out
    the tracks that we like, we will choose a lofi-esque instrument effect in any
    DAW so that our generated tracks sound more like real music. Then we head over
    to JavaScript to build our web player.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们用不同的参数运行几次模型，并挑选出我们喜欢的曲目，我们会在任何数字音频工作站（DAW）中选择一种lofi风格的乐器效果，以便我们生成的曲目听起来更像真正的音乐。然后，我们转到JavaScript来构建我们的网页播放器。
- en: Building the Web Player with [Tone.js](https://tonejs.github.io/)
  id: totrans-52
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用[Tone.js](https://tonejs.github.io/)构建网页播放器
- en: Tone.js is a web audio framework for creating interactive music in the browser.
    You can use it to build a lot of fun interactive websites (see [demos](https://tonejs.github.io/demos)
    here). But in our case, it provides a global transport to make sure our drum patterns,
    ambience sounds, quotes, and melody play at the same time. It also allows us to
    write in music score, sample an instrument, add sound effects (reverberation,
    gain, etc.), and create loops right in JavaScript. Credit goes to [Kathryn](https://github.com/lawreka/loaf-ai)
    for the code skeleton. If you want a quick and effective crash course on Tone.js,
    I highly recommend the use case examples on their [website](https://tonejs.github.io/).
    The most important takeaway is that for each sound event we create, we need to
    connect them to the AudioDestinationNode (aka our speakers) through `toDestination()`
    or through `samplePlayer.chain(effect1, Tone.Destination)` if we want to add sound
    effects to it. Then through `Tone.Transport`, we will be able to start, pause,
    and schedule events on the master output.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: Tone.js是一个用于在浏览器中创建互动音乐的网络音频框架。你可以使用它来构建很多有趣的互动网站（见[演示](https://tonejs.github.io/demos)）。但在我们的案例中，它提供了一个全局传输功能，以确保我们的鼓点、环境声音、引号和旋律同时播放。它还允许我们编写音乐乐谱、采样乐器、添加音效（混响、增益等）以及在JavaScript中创建循环。代码框架的感谢归功于[Kathryn](https://github.com/lawreka/loaf-ai)。如果你想要一个快速有效的Tone.js速成课程，我强烈推荐他们[网站](https://tonejs.github.io/)上的用例示例。最重要的收获是，对于我们创建的每个声音事件，我们需要通过`toDestination()`将它们连接到AudioDestinationNode（即我们的扬声器），或者通过`samplePlayer.chain(effect1,
    Tone.Destination)`来添加音效。然后，通过`Tone.Transport`，我们将能够在主输出上启动、暂停和调度事件。
- en: Looping the Audio Clips
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 循环音频片段
- en: Drum patterns, ambience sounds, quotes, and the pre-generated AI tracks are
    all audio files (.mp3 or .wav) loaded into our web player through a Player class.
    After loading the user input events from the website, they are then fed into a
    Tone.js Part class to create loops.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 鼓点、环境声音、引号和预生成的AI轨道都是通过Player类加载到网页播放器中的音频文件（.mp3或.wav）。在加载来自网站的用户输入事件后，这些事件会被输入到Tone.js的Part类中以创建循环。
- en: Drum patterns are looped every 8 measures, ambience sounds every 12 measures,
    and AI solo tracks every 30 measures. Quotes are not looped and start at the beginning
    of the 5th measure.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 鼓点每8小节循环一次，环境音效每12小节循环一次，AI独奏轨道每30小节循环一次。引号部分不循环，从第5小节开始。
- en: Creating Melody and Chord Progression with Instrument Samples
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用乐器样本创建旋律和和弦进程
- en: Tone.js does not provide software instrument options that we see in DAW, only
    samplers that allow us to sample our own instruments by loading in a couple of
    notes. The sampler will then repitch the samples automatically to create the pitches
    which were not explicitly included.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: Tone.js不提供我们在DAW中看到的软件乐器选项，只有采样器，允许我们通过加载几个音符来采样自己的乐器。然后，采样器会自动重新调整样本的音高，以创建未明确包含的音高。
- en: 'Then we can write in the melody and chord progression by specifying the notes
    and the time that the note should take place. I recommend using TransportTime
    to encode the beat exactly as we want. TransportTime is in the form of "BARS:QUARTERS:SIXTEENTHS"
    and uses zero-based numbering. For example, "0:0:2" means the note will take place
    after two sixteenth notes passed in the first bar. "2:1:0" means the note will
    take place in the third bar, after one quarter note passed. I wrote in the melody
    and chord progressions for 3 existing songs: Ylang Ylang by FKJ, La Festin by
    Camille, and See You Again by Tyler, the Creator this way.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以通过指定音符和音符出现的时间来写入旋律和和弦进程。我建议使用TransportTime来精确编码节拍。TransportTime的格式为"BARS:QUARTERS:SIXTEENTHS"，并使用零基数编号。例如，"0:0:2"表示音符将在第一小节的两个十六分音符后出现。"2:1:0"表示音符将在第三小节，经过一个四分音符后出现。我这样为3首现有歌曲编写了旋律和和弦进程：FKJ的《Ylang
    Ylang》、Camille的《La Festin》和Tyler, the Creator的《See You Again》。
- en: Web Player Design
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 网页播放器设计
- en: I added functions to change the web player background with different inputs
    for ambience sounds so that a more context-appropriate gif will be displayed for
    each context. There is also a visualizer connected with the song notes to add
    visual appeal, made with p5.js.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 我添加了函数，通过不同的环境声音输入来改变网页播放器的背景，以便每种环境下显示更合适的gif。同时还有一个与歌曲音符连接的可视化工具，由p5.js制作，以增加视觉吸引力。
- en: Future Work
  id: totrans-62
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 未来工作
- en: The LSTM Model
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: LSTM模型
- en: Add start of sequence and end of sequence tokens so that the model can learn
    the music pattern as the song comes to an end.
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 添加序列开始和序列结束标记，以便模型可以学习音乐模式，当歌曲结束时。
- en: Incorporate encoding for note duration so that beat tracking can be enabled.
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 融入音符时长的编码，以便启用节拍跟踪。
- en: The Web Player
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 网络播放器
- en: It would be cool to connect the backend AI model to the web player so that the
    output can be generated live. Currently the roadblock is that the model takes
    a few minutes to generate the output, but likely we could leverage a pre-trained
    model API.
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将后端 AI 模型连接到网络播放器，将会非常酷，这样输出可以实时生成。目前的障碍是模型生成输出需要几分钟，但我们可能可以利用预训练模型 API。
- en: User interactivity could be greatly improved if the web player allows the users
    to 1) put in chord progression of their own choice 2) write down some text that
    the web player will do sentiment analysis on and output a chord progression matching
    the sentiment.
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果网络播放器允许用户 1) 输入他们自己选择的和弦进行 2) 写下文本并对其进行情感分析并输出匹配情感的和弦进行，那么用户互动将大大改善。
- en: Conclusion
  id: totrans-69
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: For code and training datasets, please see the GitHub repo [here](https://github.com/mtsandra/lofi-station).
    While this is just a simple lo-fi web player, I have had a lot of fun with both
    the LSTM model and Tone.js. It amazes me every time to see how we can incorporate
    technology into our experience with music.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 关于代码和训练数据集，请参阅 GitHub 仓库[这里](https://github.com/mtsandra/lofi-station)。虽然这只是一个简单的
    lo-fi 网络播放器，但我在 LSTM 模型和 Tone.js 上玩得很开心。每次看到我们如何将技术融入音乐体验中，总是让我感到惊讶。
