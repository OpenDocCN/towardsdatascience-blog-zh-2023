- en: Linear Regression, Kernel Trick, and Linear-Kernel.
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 线性回归、核技巧和线性核。
- en: 原文：[https://towardsdatascience.com/linear-regression-kernel-trick-and-linear-kernel-39b6be3a3bf5](https://towardsdatascience.com/linear-regression-kernel-trick-and-linear-kernel-39b6be3a3bf5)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/linear-regression-kernel-trick-and-linear-kernel-39b6be3a3bf5](https://towardsdatascience.com/linear-regression-kernel-trick-and-linear-kernel-39b6be3a3bf5)
- en: Sometimes the kernel trick is useless.
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 有时核技巧是无用的。
- en: '[](https://mocquin.medium.com/?source=post_page-----39b6be3a3bf5--------------------------------)[![Yoann
    Mocquin](../Images/b30a0f70c56972aabd2bc0a74baa90bb.png)](https://mocquin.medium.com/?source=post_page-----39b6be3a3bf5--------------------------------)[](https://towardsdatascience.com/?source=post_page-----39b6be3a3bf5--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----39b6be3a3bf5--------------------------------)
    [Yoann Mocquin](https://mocquin.medium.com/?source=post_page-----39b6be3a3bf5--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://mocquin.medium.com/?source=post_page-----39b6be3a3bf5--------------------------------)[![Yoann
    Mocquin](../Images/b30a0f70c56972aabd2bc0a74baa90bb.png)](https://mocquin.medium.com/?source=post_page-----39b6be3a3bf5--------------------------------)[](https://towardsdatascience.com/?source=post_page-----39b6be3a3bf5--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----39b6be3a3bf5--------------------------------)
    [Yoann Mocquin](https://mocquin.medium.com/?source=post_page-----39b6be3a3bf5--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----39b6be3a3bf5--------------------------------)
    ·8 min read·Nov 5, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----39b6be3a3bf5--------------------------------)
    ·8 分钟阅读·2023年11月5日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/6708b3bf40f4fefc9efecd19bc5729e1.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6708b3bf40f4fefc9efecd19bc5729e1.png)'
- en: Unless otherwise noted, all images are by the author
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 除非另有说明，所有图片均由作者提供
- en: 'In this post, I want to show an interesting result that was not obvious to
    me at first, which is the following:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我想展示一个起初对我来说并不明显的有趣结果，即：
- en: Linear regression and linear-kernel ridge regression with no regularization
    are equivalent.
  id: totrans-9
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 线性回归和没有正则化的线性核岭回归是等价的。
- en: There are actually a lot of concepts and techniques involved here, so we will
    review each one by one, and finally use them all to explain this statement.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上涉及了很多概念和技术，因此我们将逐一回顾每一个，最后综合使用它们来解释这一说法。
- en: '**First, we’ll review the classic linear regression. Then I’ll explain what
    the kernel trick is and the linear-kernel, and finally we’ll show a mathematical
    proof for the statement above.**'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '**首先，我们将回顾经典的线性回归。然后我将解释核技巧和线性核是什么，最后我们将展示上述陈述的数学证明。**'
- en: A quick reminder of the classic Linear Regression
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 快速回顾经典的线性回归
- en: The maths of linear regression
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 线性回归的数学
- en: 'The classic — Ordinary Least Squares or OLS- linear regression is the following
    problem:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 经典的——普通最小二乘法（OLS）线性回归是以下问题：
- en: '![](../Images/c47d013a9630701921ca1623f68468e0.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c47d013a9630701921ca1623f68468e0.png)'
- en: 'where:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 其中：
- en: Y is a vector of length n and consists of the target value of the linear model
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Y 是一个长度为 n 的向量，包含线性模型的目标值
- en: 'beta is a vector of length m: this is the unknown the model will have to “learn”'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: beta 是一个长度为 m 的向量：这是模型需要“学习”的未知量
- en: X is the data matrix with shape n rows and m columns. We often say that we have
    n vectors recorded in the m-features space
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: X 是一个数据矩阵，具有 n 行和 m 列。我们通常说我们在 m 特征空间中记录了 n 个向量
- en: 'So the goal is to find the values of beta that minimizes the squared errors:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 所以目标是找到使平方误差最小化的 beta 值：
- en: '![](../Images/63bba3a4c278cd1f2ef575c61e2eb3cd.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/63bba3a4c278cd1f2ef575c61e2eb3cd.png)'
- en: 'This problem actually has a closed form solution, and is known as the Ordinary
    Least Squares problem. The solution is:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 这个问题实际上有一个封闭形式的解，被称为普通最小二乘问题。解为：
- en: '![](../Images/3b47f6a853ba2f944be1b0fea5730e68.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3b47f6a853ba2f944be1b0fea5730e68.png)'
- en: 'Once the solution is known, we can use the fitted model to compute new Y-values
    given new X-values using:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦知道了解决方案，我们可以使用拟合模型计算给定新 X 值时的新 Y 值，使用：
- en: '![](../Images/40e21df22bdfc3ecf72ae0cb1d438c4f.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/40e21df22bdfc3ecf72ae0cb1d438c4f.png)'
- en: Python for linear regression
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 线性回归的 Python 实现
- en: 'Let’s verify our math with scikit-learn: here is a python code that gives an
    example of a linear regression, using sklearn linear regressor, as well as a numpy-based
    regression'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们用 scikit-learn 验证我们的数学：这里有一个 Python 代码示例，展示了如何使用 sklearn 的线性回归器以及基于 numpy
    的回归
- en: '[PRE0]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '[PRE1]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'As you can see, both methods give identical results (duh!):'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，这两种方法给出了相同的结果（显然！）：
- en: '![](../Images/b194088833789c942503ab5aac1231d0.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b194088833789c942503ab5aac1231d0.png)'
- en: Kernel-trick
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 核技巧
- en: Lets now review a common technique known as the kernel trick.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们回顾一种常见的技术，称为核技巧。
- en: 'The idea is the following:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 思路如下：
- en: 'Our original problem (can be anything like classification or regression) lives
    in the space of the input data matrix X, of shape n-vectors in a m-features space.
    Sometimes, the vectors cannot be separated or classified in this low dimension
    space, so we want to transform the input data to a high dimension space. We can
    do this by hand, by creating new features manually. As the number of feature grows,
    the numerical computation will increase as well: imagine computing the dot product
    of 2 vectors of length 1 billion, and repeat for all vectors in your matrix.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 我们原始的问题（可以是分类或回归等）存在于输入数据矩阵X的空间中，形状为m特征空间中的n向量。有时，向量在这个低维空间中无法分离或分类，因此我们希望将输入数据转换到高维空间。我们可以手动创建新的特征来实现这一点。随着特征数量的增加，数值计算也会增加：想象一下计算两个长度为10亿的向量的点积，并对矩阵中的所有向量重复这一过程。
- en: The kernel trick consists in using well-designed transformation functions —
    usually noted T or phi — that creates a new vector x’ of length m’ from a vector
    x of length m, such that our new data will indeed have high dimension, but by
    keeping the computation load to a minimum.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 核技巧的核心在于使用精心设计的变换函数——通常记作T或phi——它将长度为m的向量x转换为长度为m'的新向量x'，使得我们的新数据确实具有高维，但计算负担保持在最低限度。
- en: 'To achieve this, the function phi must fill some properties, such that the
    dot product in the new high dimension feature-space can be written as a function
    — the kernel function- of the corresponding input vectors:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现这一点，函数phi必须满足一些属性，以便新高维特征空间中的点积可以写成一个函数——即核函数——关于相应的输入向量：
- en: '![](../Images/795133ba90788793e7a018ddeaf320a9.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/795133ba90788793e7a018ddeaf320a9.png)'
- en: 'What this means is that the inner product in the high dimension space can be
    expressed as a function of the input vectors. In other words, we can compute the
    inner product in the high dimension space only using the low dimension vectors.
    That is the kernel trick: we can benefit from the high-dimension space versatility,
    while never actually doing any computation there.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着高维空间中的内积可以表示为输入向量的函数。换句话说，我们可以仅使用低维向量来计算高维空间中的内积。这就是核技巧：我们可以利用高维空间的多样性，而无需实际在该空间中进行计算。
- en: The only condition is that we only need the dot product in the high-dimension
    space.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 唯一的条件是我们只需要高维空间中的点积。
- en: There are actually powerful mathematical theorems that describe what are the
    conditions to create such transformation phi and/or such kernel functions.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，有一些强大的数学定理描述了创建这种变换phi和/或核函数的条件。
- en: Examples of kernel functions
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 核函数示例
- en: 'A first example of kernel to create a m^2 dimension space from a m dimension
    is by using the following:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 创建m^2维空间的第一个核函数示例如下：
- en: '![](../Images/c7db348dd13aec2a1e13bf11f5655449.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c7db348dd13aec2a1e13bf11f5655449.png)'
- en: 'Another example: adding a constant in the kernel function increases the number
    of dimension, with new features that are scaled input features:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个例子：在核函数中添加常数会增加维度，生成新的特征，这些特征是缩放的输入特征：
- en: '![](../Images/227cd30b9c9c5154e6aab694a32fa78b.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/227cd30b9c9c5154e6aab694a32fa78b.png)'
- en: 'Another kernel that we’ll use below is the linear-kernel:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个我们下面将使用的核是线性核：
- en: '![](../Images/4aad91bf2ca5f559d668b49d24d1f5b0.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4aad91bf2ca5f559d668b49d24d1f5b0.png)'
- en: So the identity transformation is equivalent to using a kernel function that
    compute the inner product in the original space.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，单位变换等同于使用一个在原始空间中计算内积的核函数。
- en: 'There are actually a lot of other useful kernels, like the radial (RBF) kernel
    or the more general polynomial kernel, that create high-dimension AND non-linear
    feature spaces. For completeness, here is an example of using RBF kernel to compute
    non-linear regression in a linear regression context:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，还有很多其他有用的核函数，比如径向基（RBF）核或更通用的多项式核，它们创建高维且非线性的特征空间。为了完整性，这里有一个使用RBF核在线性回归背景下计算非线性回归的例子：
- en: '[PRE2]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '![](../Images/029202d2fe7775921b5d825b92764bc7.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/029202d2fe7775921b5d825b92764bc7.png)'
- en: The linear-kernel in linear regression
  id: totrans-53
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 线性回归中的线性核
- en: 'If the transformation phi transforms x to phi(x), then we can write a new linear
    regression problem:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 如果变换phi将x转换为phi(x)，那么我们可以写出一个新的线性回归问题：
- en: '![](../Images/e307ab72b9c73215b67d9db64f8390b1.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e307ab72b9c73215b67d9db64f8390b1.png)'
- en: 'Notice how the dimensions change: the input matrix for the linear regression
    problem goes from [nxm] to [nxm’], so the coefficient vector beta goes from length
    m to m’.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 注意维度的变化：线性回归问题的输入矩阵从 [nxm] 变为 [nxm’]，因此系数向量 beta 从长度 m 变为 m’。
- en: 'And this where the kernel trick appears: when computing the solution beta’,
    notice that the product of X’ with its transpose appears, which is actually the
    matrix of all the dot product, called the Kernel matrix for obvious reasons:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是核技巧出现的地方：在计算解 beta’ 时，注意到 X’ 与其转置的乘积出现，这实际上是所有点积的矩阵，称为核矩阵，原因显而易见：
- en: '![](../Images/7ba62cab46f3bc9b75bcaf726e37a9cf.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7ba62cab46f3bc9b75bcaf726e37a9cf.png)'
- en: Linear-kernelized and linear regression
  id: totrans-59
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 线性核化和线性回归
- en: 'Finally, lets see a proof of the original statement: that using the linear-kernel
    in a linear regression is useless as it is equivalent to standard linear regression.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们看看原始陈述的证明：使用线性核在线性回归中是无用的，因为它等同于标准线性回归。
- en: Linear-kernel is usually used in the context of Support Vector Machines, but
    I was wondering how it would perform in linear regression.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 线性核通常在支持向量机的背景下使用，但我想知道它在线性回归中会表现如何。
- en: 'To show that both approaches are equivalent, we have to proove that:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 为了证明这两种方法是等价的，我们必须证明：
- en: '![](../Images/5a0eaac7880c51b75871ba58ccd27bdc.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5a0eaac7880c51b75871ba58ccd27bdc.png)'
- en: The first method using beta is the original linear regression, and the second
    using beta’ is using linear-kernalized approach.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 beta 的第一种方法是原始线性回归，而使用 beta’ 的第二种方法是使用线性核化方法。
- en: 'We can prove this actually using matrix properties and relations seen above:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 我们实际上可以使用上面看到的矩阵属性和关系来证明这一点：
- en: '![](../Images/d631ec69cd7cd59962cd56880a5ddf7b.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d631ec69cd7cd59962cd56880a5ddf7b.png)'
- en: 'We can verify this again using python and scikit learn:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以再次使用 Python 和 scikit-learn 来验证这一点：
- en: '[PRE3]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '![](../Images/2d70922fa57610a798e4743d1318df37.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2d70922fa57610a798e4743d1318df37.png)'
- en: Wrapup
  id: totrans-70
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this post we reviewed the simple linear regression, including the matrix
    formulation of the problem and its solution.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我们回顾了简单线性回归，包括问题及其解的矩阵公式。
- en: We then saw what the kernel trick is and how it allows us to benefit from very
    high dimension space without actually moving our low dimension data to this computation-intensive
    space.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们看到核技巧是什么，以及它如何使我们能够从非常高维的空间中受益，而无需实际将我们的低维数据移动到这个计算密集型空间中。
- en: Finally, I showed that the linear-kernel in the context of linear regression
    is actually useless and correspond to the simple linear regression.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我展示了在线性回归的背景下线性核实际上是无用的，它相当于简单线性回归。
- en: '**If you’re considering joining Medium, use this link to quickly subscribe
    and become one of my referred member** :'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '**如果你考虑加入 Medium，请使用此链接快速订阅并成为我的推荐会员**：'
- en: '[](https://medium.com/@mocquin/membership?source=post_page-----39b6be3a3bf5--------------------------------)
    [## Join Medium with my referral link - Yoann Mocquin'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@mocquin/membership?source=post_page-----39b6be3a3bf5--------------------------------)
    [## 使用我的推荐链接加入 Medium - Yoann Mocquin'
- en: As a Medium member, a portion of your membership fee goes to writers you read,
    and you get full access to every story…
  id: totrans-76
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 作为 Medium 的会员，你的一部分会员费用将分配给你阅读的作者，你可以完全访问每个故事……
- en: medium.com](https://medium.com/@mocquin/membership?source=post_page-----39b6be3a3bf5--------------------------------)
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '[medium.com](https://medium.com/@mocquin/membership?source=post_page-----39b6be3a3bf5--------------------------------)'
- en: '**and subscribe to get an notification when I publish new post:**'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '**并订阅以在我发布新帖子时收到通知：**'
- en: '[](https://mocquin.medium.com/subscribe?source=post_page-----39b6be3a3bf5--------------------------------)
    [## Get an email whenever I publish !'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://mocquin.medium.com/subscribe?source=post_page-----39b6be3a3bf5--------------------------------)
    [## 每当我发布新内容时收到电子邮件！'
- en: Get an email whenever I publish ! New publication will include data transformation,
    advanced plotting and simulation…
  id: totrans-80
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 每当我发布新内容时，你将收到一封电子邮件！ 新发布将包括数据转换、高级绘图和模拟……
- en: mocquin.medium.com](https://mocquin.medium.com/subscribe?source=post_page-----39b6be3a3bf5--------------------------------)
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '[mocquin.medium.com](https://mocquin.medium.com/subscribe?source=post_page-----39b6be3a3bf5--------------------------------)'
- en: 'Finaly, you can check out some of my other post, on Fourier transform, pandas
    dtypes, or linear algebra techniques for datascience:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，你可以查看我其他一些帖子，涉及傅里叶变换、pandas 数据类型或数据科学中的线性代数技术：
- en: '[](/pandas-work-on-your-dtypes-20d9d32d2e42?source=post_page-----39b6be3a3bf5--------------------------------)
    [## pandas: work on your dtypes!'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '[pandas：处理你的数据类型！](https://towardsdatascience.com/pandas-work-on-your-dtypes-20d9d32d2e42?source=post_page-----39b6be3a3bf5--------------------------------)'
- en: Having the right dtypes in pandas is a must for clean data-analysis! Here’s
    how and why.
  id: totrans-84
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 在 pandas 中使用正确的数据类型对于干净的数据分析至关重要！这里是如何以及为什么。
- en: 'towardsdatascience.com](/pandas-work-on-your-dtypes-20d9d32d2e42?source=post_page-----39b6be3a3bf5--------------------------------)
    [](/fourier-transform-for-time-series-detrending-f0f470f4bf14?source=post_page-----39b6be3a3bf5--------------------------------)
    [## Fourier-transform for time-series : detrending'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '[傅里叶变换在时间序列中的去趋势处理](https://towardsdatascience.com/fourier-transform-for-time-series-detrending-f0f470f4bf14?source=post_page-----39b6be3a3bf5--------------------------------)'
- en: Detrending your time-series might be a game-changer.
  id: totrans-86
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 对时间序列进行去趋势处理可能会带来重大变化。
- en: 'towardsdatascience.com](/fourier-transform-for-time-series-detrending-f0f470f4bf14?source=post_page-----39b6be3a3bf5--------------------------------)
    [](/pca-lda-ica-a-components-analysis-algorithms-comparison-c5762c4148ff?source=post_page-----39b6be3a3bf5--------------------------------)
    [## PCA/LDA/ICA : a components analysis algorithms comparison'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '[PCA/LDA/ICA：组件分析算法比较](https://towardsdatascience.com/pca-lda-ica-a-components-analysis-algorithms-comparison-c5762c4148ff?source=post_page-----39b6be3a3bf5--------------------------------)'
- en: Review the concepts and differences between these famous algorithms.
  id: totrans-88
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 复习这些著名算法的概念和区别。
- en: 'towardsdatascience.com](/pca-lda-ica-a-components-analysis-algorithms-comparison-c5762c4148ff?source=post_page-----39b6be3a3bf5--------------------------------)
    [](/pca-whitening-vs-zca-whitening-a-numpy-2d-visual-518b32033edf?source=post_page-----39b6be3a3bf5--------------------------------)
    [## PCA-whitening vs ZCA-whitening : a numpy 2d visual'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '[PCA-白化与 ZCA-白化：numpy 2D 可视化](https://towardsdatascience.com/pca-whitening-vs-zca-whitening-a-numpy-2d-visual-518b32033edf?source=post_page-----39b6be3a3bf5--------------------------------)'
- en: The process of whitening data consists in a transformation such that the transformed
    data has identity matrix as…
  id: totrans-90
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据去白化的过程包括一种变换，使得变换后的数据具有单位矩阵作为…
- en: towardsdatascience.com](/pca-whitening-vs-zca-whitening-a-numpy-2d-visual-518b32033edf?source=post_page-----39b6be3a3bf5--------------------------------)
    [](/300-times-faster-resolution-of-finite-difference-method-using-numpy-de28cdade4e1?source=post_page-----39b6be3a3bf5--------------------------------)
    [## 300-times faster resolution of Finite-Difference Method using numpy
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '[300倍速的有限差分法分辨率使用 numpy](https://towardsdatascience.com/300-times-faster-resolution-of-finite-difference-method-using-numpy-de28cdade4e1?source=post_page-----39b6be3a3bf5--------------------------------)'
- en: Finite-difference method is a powerfull technique to solve complex problems,
    and numpy makes it fast !
  id: totrans-92
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 有限差分法是一种强大的技术来解决复杂问题，而 numpy 使其变得快速！
- en: towardsdatascience.com](/300-times-faster-resolution-of-finite-difference-method-using-numpy-de28cdade4e1?source=post_page-----39b6be3a3bf5--------------------------------)
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '[有限差分法是一种强大的技术来解决复杂问题，而 numpy 使其变得快速！](https://towardsdatascience.com/300-times-faster-resolution-of-finite-difference-method-using-numpy-de28cdade4e1?source=post_page-----39b6be3a3bf5--------------------------------)'
