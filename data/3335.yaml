- en: 'Steady the Course: Navigating the Evaluation of LLM-based Applications'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 稳定航向：导航LLM应用程序评估
- en: 原文：[https://towardsdatascience.com/steady-the-course-navigating-the-evaluation-of-llm-based-applications-8b7a22734fc9?source=collection_archive---------1-----------------------#2023-11-09](https://towardsdatascience.com/steady-the-course-navigating-the-evaluation-of-llm-based-applications-8b7a22734fc9?source=collection_archive---------1-----------------------#2023-11-09)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/steady-the-course-navigating-the-evaluation-of-llm-based-applications-8b7a22734fc9?source=collection_archive---------1-----------------------#2023-11-09](https://towardsdatascience.com/steady-the-course-navigating-the-evaluation-of-llm-based-applications-8b7a22734fc9?source=collection_archive---------1-----------------------#2023-11-09)
- en: Why evaluating LLM apps matters and how to get started
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 为何评估LLM应用程序至关重要及如何入门
- en: '[](https://medium.com/@stijn.goossens?source=post_page-----8b7a22734fc9--------------------------------)[![Stijn
    Goossens](../Images/df94a9620671e68920db453faed473f2.png)](https://medium.com/@stijn.goossens?source=post_page-----8b7a22734fc9--------------------------------)[](https://towardsdatascience.com/?source=post_page-----8b7a22734fc9--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----8b7a22734fc9--------------------------------)
    [Stijn Goossens](https://medium.com/@stijn.goossens?source=post_page-----8b7a22734fc9--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@stijn.goossens?source=post_page-----8b7a22734fc9--------------------------------)[![Stijn
    Goossens](../Images/df94a9620671e68920db453faed473f2.png)](https://medium.com/@stijn.goossens?source=post_page-----8b7a22734fc9--------------------------------)[](https://towardsdatascience.com/?source=post_page-----8b7a22734fc9--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----8b7a22734fc9--------------------------------)
    [斯坦·古森斯](https://medium.com/@stijn.goossens?source=post_page-----8b7a22734fc9--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ffc2031bc0292&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsteady-the-course-navigating-the-evaluation-of-llm-based-applications-8b7a22734fc9&user=Stijn+Goossens&userId=fc2031bc0292&source=post_page-fc2031bc0292----8b7a22734fc9---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----8b7a22734fc9--------------------------------)
    ·10 min read·Nov 9, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F8b7a22734fc9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsteady-the-course-navigating-the-evaluation-of-llm-based-applications-8b7a22734fc9&user=Stijn+Goossens&userId=fc2031bc0292&source=-----8b7a22734fc9---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ffc2031bc0292&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsteady-the-course-navigating-the-evaluation-of-llm-based-applications-8b7a22734fc9&user=Stijn+Goossens&userId=fc2031bc0292&source=post_page-fc2031bc0292----8b7a22734fc9---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----8b7a22734fc9--------------------------------)
    ·10 分钟阅读·2023 年 11 月 9 日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F8b7a22734fc9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsteady-the-course-navigating-the-evaluation-of-llm-based-applications-8b7a22734fc9&user=Stijn+Goossens&userId=fc2031bc0292&source=-----8b7a22734fc9---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F8b7a22734fc9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsteady-the-course-navigating-the-evaluation-of-llm-based-applications-8b7a22734fc9&source=-----8b7a22734fc9---------------------bookmark_footer-----------)![](../Images/d37b7688b4451b9257befa1362bb992b.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F8b7a22734fc9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsteady-the-course-navigating-the-evaluation-of-llm-based-applications-8b7a22734fc9&source=-----8b7a22734fc9---------------------bookmark_footer-----------)![](../Images/d37b7688b4451b9257befa1362bb992b.png)'
- en: A pirate with a hurt knee asks his LLM-based first aid assistant for advice.
    Image generated by the author with DALL·E 3.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 一位膝盖受伤的海盗向他基于LLM的急救助手寻求建议。图像由作者使用DALL·E 3生成。
- en: Introduction
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 简介
- en: 'Large Language Models (LLMs) are all the hype, and lots of people are incorporating
    them into their applications. Chatbots that answer questions over relational databases,
    assistants that help programmers write code more efficiently, and copilots that
    take actions on your behalf are some examples. The powerful capabilities of LLMs
    allow you to start projects with rapid initial success. Nevertheless, as you transition
    from a prototype towards a mature LLM app, a robust evaluation framework becomes
    essential. Such an evaluation framework helps your LLM app reach optimal performance
    and ensures consistent and reliable results. In this blog post, we will cover:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）引起了广泛关注，许多人将其融入到他们的应用程序中。例如，回答关系数据库问题的聊天机器人、帮助程序员更高效编写代码的助手，以及代表你采取行动的副驾驶。LLMs
    的强大能力使你能够快速取得项目的初步成功。然而，随着你从原型过渡到成熟的 LLM 应用程序，一个强大的评估框架变得至关重要。这样的评估框架帮助你的 LLM
    应用程序达到最佳性能，并确保一致和可靠的结果。在本博客文章中，我们将讨论：
- en: The difference between evaluating an LLM vs. an LLM-based application
  id: totrans-11
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 评估 LLM 与基于 LLM 的应用程序之间的差异
- en: The importance of LLM app evaluation
  id: totrans-12
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: LLM 应用评估的重要性
- en: The challenges of LLM app evaluation
  id: totrans-13
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: LLM 应用评估的挑战
- en: Getting started
  id: totrans-14
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 入门
- en: a. Collecting data and building a test set
  id: totrans-15
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: a. 收集数据和构建测试集
- en: b. Measuring performance
  id: totrans-16
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: b. 绩效衡量
- en: The LLM app evaluation framework
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: LLM 应用评估框架
- en: Using the fictional example of FirstAidMatey, a first-aid assistant for pirates,
    we will navigate through the seas of evaluation techniques, challenges, and strategies.
    We’ll wrap up with key takeaways and insights. So, let’s set sail on this enlightening
    journey!
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 通过虚构的例子“FirstAidMatey”，一个为海盗提供急救的助手，我们将深入探讨评估技术、挑战和策略的海洋。最后我们将总结关键要点和洞察。让我们开始这段启发性的旅程吧！
- en: Evaluating an LLM vs. an LLM-based application
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 评估 LLM 与基于 LLM 的应用程序
- en: The evaluation of individual Large Language Models (LLMs) like OpenAI’s GPT-4,
    Google’s PaLM 2 and Anthropic’s Claude is typically done with benchmark tests
    like [MMLU](https://paperswithcode.com/sota/multi-task-language-understanding-on-mmlu).
    In this blog post, however, we’re interested in evaluating LLM-based applications.
    These are applications that are powered by an LLM and contain other components
    like an orchestration framework that manages a sequence of LLM calls. Often Retrieval
    Augmented Generation (RAG) is used to provide context to the LLM and avoid hallucinations.
    In short, RAG requires the context documents to be embedded into a vector store
    from which the relevant snippets can be retrieved and shared with the LLM. In
    contrast to an LLM, an LLM-based application (or LLM app) is built to execute
    one or more specific tasks really well. Finding the right setup often involves
    some experimentation and iterative improvement. RAG, for example, can be implemented
    in many different ways. An evaluation framework as discussed in this blog post
    can help you find the best setup for your use case.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 对 OpenAI 的 GPT-4、Google 的 PaLM 2 和 Anthropic 的 Claude 等个体大型语言模型（LLMs）的评估通常通过基准测试如
    [MMLU](https://paperswithcode.com/sota/multi-task-language-understanding-on-mmlu)
    来进行。然而，在本博客文章中，我们感兴趣的是评估基于 LLM 的应用程序。这些应用程序由 LLM 提供支持，并包含其他组件，如管理 LLM 调用序列的编排框架。通常，检索增强生成（RAG）用于向
    LLM 提供上下文，避免幻觉。简而言之，RAG 需要将上下文文档嵌入到向量存储中，以便从中检索相关片段并与 LLM 共享。与 LLM 相比，基于 LLM 的应用程序（或
    LLM 应用）旨在非常好地执行一个或多个特定任务。找到合适的设置通常涉及一些实验和迭代改进。例如，RAG 可以通过多种方式实现。本文讨论的评估框架可以帮助你找到适合你用例的最佳设置。
- en: '![](../Images/07db4a149ab5c6ca50cbdd892d2df94d.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/07db4a149ab5c6ca50cbdd892d2df94d.png)'
- en: '*An LLM becomes even more powerful when being used in the context of an LLM-based
    application.*'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '*在基于 LLM 的应用程序中使用 LLM 可以使其更强大。*'
- en: FirstAidMatey is an LLM-based application that helps pirates with questions
    like *“Me hand got caught in the ropes and it’s now swollen, what should I do,
    mate?”*. In it simplest form the Orchestrator consists of a single prompt that
    feeds the user question to the LLM and asks it to provide helpful answers. It
    can also instruct the LLM to answer in Pirate Lingo for optimal understanding.
    As an extension, a vector store with embedded first aid documentation could be
    added. Based on the user question, the relevant documentation can be retrieved
    and included into the prompt, so that the LLM can provide more accurate answers.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: FirstAidMatey是一个基于LLM的应用，帮助海盗处理像*“我的手被绳子缠住，现在肿了，我该怎么办，伙计？”*这样的问题。最简单的形式是Orchestrator由一个单一的提示组成，将用户问题传递给LLM，并要求其提供有帮助的回答。它还可以指示LLM用海盗语言回答，以便更好地理解。作为扩展，可以添加一个包含急救文档的向量存储。根据用户问题，可以检索相关文档并将其包含在提示中，以便LLM可以提供更准确的答案。
- en: Importance of LLM app evaluation
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LLM应用评估的重要性
- en: 'Before we get into the how, let’s look at why you should set up a system to
    evaluate your LLM-based application. The main goals are threefold:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入了解如何评估之前，让我们先看看为什么你应该建立一个评估LLM应用的系统。主要目标有三点：
- en: '**Consistency**: Ensure stable and reliable LLM app outputs across all scenarios
    and discover regressions when they occur. For example, when you improve your LLM
    app performance on a specific scenario, you want to be warned in case you compromise
    the performance on another scenario. When using proprietary models like OpenAI’s
    GPT-4, you are also subject to their update schedule. As new versions get released,
    your current version might be deprecated over time. Research shows that [switching
    to a newer GPT version isn’t always for the better](https://arxiv.org/pdf/2307.09009.pdf).
    Thus, it’s important to be able to assess how this new version affects the performance
    of your LLM app.'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**一致性**：确保LLM应用在所有场景下的输出稳定可靠，并在出现回归时发现。例如，当你在特定场景下改善LLM应用表现时，你希望在可能影响其他场景的性能时收到警告。当使用像OpenAI的GPT-4这样的专有模型时，你还需遵循其更新计划。随着新版本的发布，你当前的版本可能会逐渐被弃用。研究表明，[切换到更新的GPT版本并不总是更好](https://arxiv.org/pdf/2307.09009.pdf)。因此，能够评估新版本如何影响LLM应用的表现是很重要的。'
- en: '**Insights**: Understand where the LLM app performs well and where there is
    room for improvement.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**洞察**：了解LLM应用表现良好的地方和需要改进的地方。'
- en: '**Benchmarking**: Establish performance standards for the LLM app, measure
    the effect of experiments and release new versions confidently.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基准测试**：建立LLM应用的性能标准，衡量实验的效果，并自信地发布新版本。'
- en: 'As a result, you will achieve the following outcomes:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 结果是，你将实现以下成果：
- en: '**Gain** **user trust and satisfaction** because your LLM app will perform
    consistently.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**赢得** **用户信任和满意度**，因为你的LLM应用将表现一致。'
- en: '**Increase stakeholder confidence** because you can show how well the LLM app
    is performing and how new versions improve upon older ones.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**提升利益相关者信心**，因为你可以展示LLM应用的表现如何，以及新版本如何改进旧版本。'
- en: '**Boost your competitive advantage** as you can quickly iterate, make improvements
    and confidently deploy new versions.'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**提升你的竞争优势**，因为你可以快速迭代、进行改进并自信地部署新版本。'
- en: Challenges of LLM app evaluation
  id: totrans-33
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LLM应用评估的挑战
- en: 'Having read the above benefits, it’s clear why adopting an LLM-based application
    can be advantageous. But before we can do so, we must solve the following [two
    main challenges](https://fullstackdeeplearning.com/llm-bootcamp/spring-2023/llmops/):'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 阅读了上述好处后，很明显为什么采用基于LLM的应用可能是有利的。但在此之前，我们必须解决以下[两个主要挑战](https://fullstackdeeplearning.com/llm-bootcamp/spring-2023/llmops/)：
- en: '**Lack of labelled data**: Unlike traditional machine learning applications,
    LLM-based ones don’t need labelled data to get started. LLMs can do many tasks
    (like text classification, summarization, generation and more) out of the box,
    without having to show specific examples. This is great because we don’t have
    to wait for data and labels, but on the other hand, it also means we don’t have
    data to check how well the application is performing.'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**缺乏标记数据**：与传统机器学习应用不同，基于LLM的应用不需要标记数据即可开始。LLM可以开箱即用地执行许多任务（如文本分类、摘要、生成等），无需展示具体示例。这很棒，因为我们不必等待数据和标签，但另一方面，这也意味着我们没有数据来检查应用的表现。'
- en: '**Multiple valid answers**: In an LLM app, the same input can often have more
    than one right answer. For instance, a chatbot might provide various responses
    with similar meanings, or code might be generated with identical functionality
    but different structures.'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**多个有效答案**：在LLM应用中，相同的输入通常可以有多个正确答案。例如，聊天机器人可能会提供多个意义相近的回答，或者代码可能会生成具有相同功能但结构不同的版本。'
- en: To address these challenges, we must define the appropriate data and metrics.
    We’ll do that in the next section.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 为了应对这些挑战，我们必须定义合适的数据和指标。我们将在下一节中进行说明。
- en: Getting started
  id: totrans-38
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 开始
- en: Collecting data and building a test set
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 收集数据并构建测试集
- en: 'For evaluating an LLM-based application, we use a test set consisting of **test
    cases, each with specific inputs and targets**. What these contain depends on
    the application’s purpose. For example, a code generation application expects
    verbal instructions as input and outputs code in return. During evaluation, the
    inputs will be provided to the LLM app and the generated output can be compared
    to the reference target. Here are a few test cases for FirstAidMatey:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 对于评估基于LLM的应用，我们使用一个由**每个包含特定输入和目标的测试用例**组成的测试集。这些测试用例的内容取决于应用的目的。例如，代码生成应用期望将口头指令作为输入，并返回代码。评估期间，将把输入提供给LLM应用，生成的输出可以与参考目标进行比较。以下是FirstAidMatey的一些测试用例：
- en: '![](../Images/06e6c8f2dc13357779a2576e60e56140.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/06e6c8f2dc13357779a2576e60e56140.png)'
- en: '*Some test cases for our first aid chatbot application.*'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '*一些用于我们急救聊天机器人应用的测试用例。*'
- en: In this example, every test case contains a question as input and a correct
    reference answer as target. In addition to this, **historical responses**, and
    even historical user feedback could be added to the test case. This allows you
    to check whether newer answers improve upon older ones and whether user feedback
    is being addressed. Ideally, your test set consists of verified reference answers,
    but if you quickly want to compare a new model to an older one, using the historical
    responses and feedback is an option.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，每个测试用例包含一个问题作为输入和一个正确的参考答案作为目标。除此之外，**历史响应**，甚至历史用户反馈也可以添加到测试用例中。这允许你检查较新的回答是否比旧的回答有所改进，以及用户反馈是否得到处理。理想情况下，你的测试集应包括经过验证的参考答案，但如果你希望快速比较新旧模型，使用历史响应和反馈也是一个选项。
- en: As you typically start the development of an LLM-based application without a
    dataset, it is important to **start building the test set early**. You can do
    that iteratively by adding those examples on which the current model fails. Initially,
    it will be the developers who will experiment the most with the LLM app and it’s
    them who will add the first test cases based on their manual tests. Nevertheless,
    it is important to involve the business or end users soon in the process as they
    will have a better understanding of the relevant test cases. To kickstart the
    test set, you can also use an LLM to generate input-target pairs based on, for
    example, your knowledge base. Over time, the test set should cover the entire
    spectrum of topics that matter most to the end users. Therefore, it is important
    to keep collecting user feedback and adding test cases for the underperforming
    and underrepresented topics.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 由于你通常在没有数据集的情况下开始开发基于LLM的应用，因此**早期开始构建测试集**非常重要。你可以通过添加当前模型失败的那些示例来进行迭代。最初，将是开发人员最频繁地尝试LLM应用，并且是他们根据手动测试添加第一个测试用例。然而，尽早涉及业务或最终用户也很重要，因为他们对相关测试用例有更好的理解。为了启动测试集，你也可以使用LLM根据你的知识库等生成输入-目标对。随着时间的推移，测试集应该涵盖对最终用户最重要的所有主题。因此，收集用户反馈并为表现不佳和被低估的主题添加测试用例是很重要的。
- en: Measuring evaluation performance
  id: totrans-45
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 评估性能
- en: 'Given the set of test cases, we can now pass the inputs to the LLM app and
    compare the generated responses with the targets. Because there is not one single
    correct answer for each input, we cannot literally compare the response with the
    reference answer. The response may be worded differently while having the same
    meaning as the reference. What we can do though, is to [**evaluate several properties
    of the response**](/testing-large-language-models-like-we-test-software-92745d28a359),
    as a proxy for the response quality. The relevant properties to test for depend
    on the application and available data. Here is a list of properties of the FirstAidMatey
    responses, and how they can be turned into a metric:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一组测试用例后，我们现在可以将输入传递给 LLM 应用，并将生成的响应与目标进行比较。由于每个输入没有单一的正确答案，我们不能字面上地将响应与参考答案进行比较。响应的措辞可能不同，但意义与参考答案相同。不过，我们可以做的是
    [**评估响应的几个属性**](/testing-large-language-models-like-we-test-software-92745d28a359)，作为响应质量的代理。测试的相关属性取决于应用程序和可用数据。以下是
    FirstAidMatey 响应的属性列表，以及如何将其转化为指标：
- en: '**Factual Consistency**: Use an LLM to assess whether the generated answer
    is factually consistent with the reference answer.'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**事实一致性**：使用 LLM 来评估生成的答案是否与参考答案在事实上一致。'
- en: '**Pirateness**: Use an LLM to evaluate whether the answer is written in Pirate
    Lingo.'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**海盗性**：使用 LLM 来评估答案是否用海盗语言书写。'
- en: '**Semantic Similarity**: Compute the cosine similarity between the embeddings
    of the generated and reference answers. Note that this is much cheaper to compute
    than Factual Consistency, but might not be as correlated with correct answers.'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**语义相似性**：计算生成答案与参考答案的嵌入之间的余弦相似度。请注意，这比事实一致性计算要便宜得多，但可能与正确答案的相关性不那么高。'
- en: '**Verbosity**: Divide the length of the generated answer by the length of the
    reference answer. The higher the verbosity, the more chatty the LLM app is behaving,
    which might not be the app’s intention.'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**冗长性**：将生成答案的长度除以参考答案的长度。冗长性越高，LLM 应用的表现越啰嗦，这可能不是应用的意图。'
- en: '**Latency**: Measure the time it takes the LLM app to generate the response.
    This allows you to make the tradeoff between more accurate, but slower configurations.'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**延迟**：测量 LLM 应用生成响应所需的时间。这使你能够在更准确但较慢的配置之间做出权衡。'
- en: Depending on the use case, you could have more/different properties. For example,
    for a code generation LLM app, one additional property could be the Syntactically
    Correctness of the generated code which could be measured by sending the code
    through a compiler. The image below illustrates how a property can be evaluated
    by using an LLM.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 根据使用案例，你可能会有更多/不同的属性。例如，对于一个代码生成的 LLM 应用，一个额外的属性可能是生成代码的语法正确性，这可以通过将代码发送给编译器来测量。下图说明了如何通过使用
    LLM 来评估一个属性。
- en: '![](../Images/3d0f390839cf36ffc7777a244a952a3f.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3d0f390839cf36ffc7777a244a952a3f.png)'
- en: '*An illustration of the Factual Consistency metric being evaluated via another
    LLM call.*'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '*通过另一个 LLM 调用评估事实一致性的示意图。*'
- en: Once all properties have been evaluated for all tests, the **average score for
    each metric** can be computed and compared with the baseline/target performance.
    When you’re experimenting with different configurations, e.g. for RAG, the test
    scores point you towards the most favourable candidate. The tradeoffs between
    correctness, verbosity and latency become clear as well. In addition to the metric
    scores, the failure cases can provide useful insights for further improvement.
    Have a closer look at some of the failed tests, they might provide useful insights
    for improving your LLM app further.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦所有测试的属性都被评估完成，可以计算出每个指标的**平均分数**并与基准/目标性能进行比较。当你尝试不同的配置时，例如 RAG，测试分数会指向最有利的候选项。正确性、冗长性和延迟之间的权衡也会变得清晰。此外，除了指标分数，失败的案例还可以提供有用的改进见解。仔细查看一些失败的测试，它们可能会为进一步改善你的
    LLM 应用提供有用的见解。
- en: 'You might wonder, *“If the LLM can’t get the answer right, how can we trust
    it to evaluate that same answer? Wouldn’t there be a bias toward its own output?”*
    Indeed, that sounds counterintuitive, but the main idea here is that **evaluation
    is easier than generation**. Consider the analogy of creating versus evaluating
    a painting. When creating a painting you’d need to simultaneously take multiple
    properties like composition, colour, perspective, light and shadow, texture, intended
    message, … into account. Evaluating a painting on the other hand allows you to
    focus on one property at a time; it’s simpler to judge a finished piece than to
    make one from scratch. The same idea applies to LLMs: while generating a proper
    response can be challenging, critically evaluating an existing one is more straightforward.
    The evaluation becomes even more feasible if a reference response is available,
    like for the Factual Consistency property.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会想，*“如果LLM不能正确回答问题，我们怎么能相信它来评估这个答案？难道不会对它自己的输出有偏见吗？”* 的确，这听起来有些违背直觉，但这里的主要观点是**评估比生成更容易**。可以考虑创作与评估画作的类比。当创作一幅画时，你需要同时考虑多个属性，如构图、颜色、视角、光影、质感、预期信息等。而评估一幅画则可以逐一关注每个属性；评价一幅完成的作品比从头开始创作一幅画要简单。LLM也是一样：尽管生成一个恰当的响应可能具有挑战性，但批判性地评估现有的响应则更加直接。如果有参考响应，比如对于事实一致性属性，评估会变得更可行。
- en: The LLM app evaluation framework
  id: totrans-57
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LLM应用评估框架
- en: 'Let’s tie everything together in this final section. The image below shows
    an overview of the evaluation framework and illustrates an important feedback
    loop:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在最后一部分中将所有内容结合起来。下图展示了评估框架的概述，并说明了一个重要的反馈循环：
- en: The **LLM app, properties and test cases are passed to the Evaluator** which
    loops over all test cases and passes the test inputs through the LLM app. The
    resulting outputs are then evaluated by looping over the properties and collecting
    the results as metrics.
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**LLM应用、属性和测试用例会传递给评估器**，评估器会循环遍历所有测试用例，并将测试输入传递给LLM应用。然后，通过遍历属性并收集结果作为指标来评估生成的输出。'
- en: The evaluation results are stored for further analysis. In addition to the metrics,
    it is also important to track the configuration of the LLM app (i.e. the used
    LLM and parameters, the used RAG components and parameters, system prompts, etc.)
    so that you can easily distinguish the most promising experiments and gain insights
    for improving the app further. You could **store the evaluation results and LLM
    app configuration** in your own database, or use tools like [MLflow](https://github.com/mlflow/mlflow/)
    which gives you immediate access to a user-friendly interface.
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 评估结果被存储以供进一步分析。除了指标外，跟踪LLM应用的配置（即使用的LLM和参数、使用的RAG组件和参数、系统提示等）也很重要，这样你可以轻松区分最有前景的实验，并获得进一步改进应用的洞见。你可以将**评估结果和LLM应用配置**存储在自己的数据库中，或者使用像[MLflow](https://github.com/mlflow/mlflow/)这样的工具，它能让你立即访问用户友好的界面。
- en: Once you’re happy with the performance, you can **release the new version**
    of your application, either to internal or external users.
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦你对性能感到满意，你可以**发布新版本**的应用，无论是面向内部用户还是外部用户。
- en: In the early phases of the project, it’ll be the developers who are testing
    and **collecting feedback**. Later on, feedback can be collected from the end
    users, either directly (thumbs up/down, and written feedback) or implicitly (conversational
    turns, session length, accepted code suggestions, etc.).
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在项目的早期阶段，将由开发者进行测试和**收集反馈**。随后，可以从最终用户那里收集反馈，无论是直接的（如点赞/点踩和书面反馈）还是间接的（如对话回合、会话时长、接受的代码建议等）。
- en: Extend the test set by analysing the incoming feedback and **adding test cases**
    for the situations that are underrepresented and not handled well by the current
    model.
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过分析收到的反馈来扩展测试集，并**添加测试用例**以涵盖当前模型处理不充分的情况。
- en: Spot trends in the incoming feedback and turn them into **LLM app improvements**.
    Depending on the situation, you can improve the orchestrator, (e.g. create a chain
    of separate LLM calls instead of a single prompt), the retrieval process (e.g.
    improve the embeddings) or the LLM (e.g. change the model, the parameters or even
    consider fine-tuning).
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在收到的反馈中发现趋势，并将其转化为**LLM应用改进**。根据情况，你可以改进协调器（例如，创建一个单独LLM调用的链而不是一个单一提示）、检索过程（例如，改进嵌入）或LLM（例如，改变模型、参数或考虑微调）。
- en: '![](../Images/14eb8a5ae414efbb5c61d004d6950d80.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/14eb8a5ae414efbb5c61d004d6950d80.png)'
- en: '*Overview of the evaluation framework for LLM-based applications.*'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '*LLM 基础应用的评估框架概述。*'
- en: For an illustration of steps 1 and 2, have a look at the following [Jupyter
    notebook](https://github.com/radix-ai/llm-app-eval/blob/main/src/llm_app_eval/example.ipynb).
    This notebook illustrates the concepts explained in this blog post. You can see
    how properties, test cases and LLM app versions can be defined and sent to the
    Evaluator. In addition to printing the evaluation results, they are also logged
    into an MLflow dashboard for further analysis. Definitely check it out, it will
    make the discussed topics even more concrete.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 关于步骤 1 和 2 的示例，请查看以下 [Jupyter notebook](https://github.com/radix-ai/llm-app-eval/blob/main/src/llm_app_eval/example.ipynb)。该
    notebook 说明了本文博客中解释的概念。你可以看到如何定义和发送属性、测试用例以及 LLM 应用程序版本给 Evaluator。除了打印评估结果外，这些结果还会被记录到
    MLflow 仪表板中以便进一步分析。绝对值得一看，这将使讨论的话题更加具体。
- en: Conclusion
  id: totrans-68
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: Evaluating an LLM-based application is an essential part of LLM app development.
    The evaluation framework presented in this blog post eases comparing experiments
    during development, ensures consistent performance and provides insights for further
    improvements.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 对 LLM 基础应用进行评估是 LLM 应用程序开发的重要部分。本文介绍的评估框架简化了开发过程中实验的比较，确保了一致的性能，并提供了进一步改进的见解。
- en: The lack of labelled data, i.e. the first challenge we encountered, can be solved
    by building a test set early on and iteratively expanding it by adding the hard
    and underrepresented cases. The second challenge, the fact that there are often
    multiple correct answers, can be overcome by looking at different properties of
    the generated outputs. Some of these properties can be measured with simple formulas
    or rules, while others can be evaluated with an LLM.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 我们遇到的第一个挑战是缺乏标记数据，可以通过尽早构建测试集并通过添加困难和代表性不足的案例来逐步扩展解决。第二个挑战是存在多个正确答案这一事实，可以通过查看生成输出的不同属性来克服。其中一些属性可以通过简单的公式或规则来衡量，而其他属性则可以通过
    LLM 来评估。
- en: Finally, we discovered a crucial feedback loop where evaluation results and
    collected user feedback drive the LLM app performance forward. In conclusion,
    systematic evaluation is the compass that steers your LLM app towards success!
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们发现了一个关键的反馈循环，即评估结果和收集的用户反馈推动 LLM 应用程序的性能提升。总之，系统化的评估是指引你的 LLM 应用程序走向成功的指南针！
- en: '*Author’s Note: The inspiration for this blog post originates from my experience
    as a Solution Architect at Radix. Radix is a Belgian artificial intelligence company
    that specializes in developing innovative machine learning solutions and applications
    for various industries. If you’re interested in finding out more about LLM-based
    applications, I encourage you to visit* [*radix.ai*](http://radix.ai) *or reach
    out to me directly on* [*LinkedIn*](https://www.linkedin.com/in/goossensstijn/)*.*'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '*作者注：本文的灵感源于我在 Radix 担任解决方案架构师的经验。Radix 是一家比利时人工智能公司，专注于开发创新的机器学习解决方案和应用程序，服务于各种行业。如果你对
    LLM 基础应用感兴趣，我鼓励你访问* [*radix.ai*](http://radix.ai) *或直接通过* [*LinkedIn*](https://www.linkedin.com/in/goossensstijn/)*与我联系。*'
- en: '*Unless otherwise noted, all images are by the author.*'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '*除非另有说明，否则所有图片均为作者提供。*'
