- en: Attention from Alignment, Practically Explained
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从对齐中获得的注意力，实际解释
- en: 原文：[https://towardsdatascience.com/attention-from-alignment-practically-explained-548ef6588aa4](https://towardsdatascience.com/attention-from-alignment-practically-explained-548ef6588aa4)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/attention-from-alignment-practically-explained-548ef6588aa4](https://towardsdatascience.com/attention-from-alignment-practically-explained-548ef6588aa4)
- en: Learn from what matters, Ignore what doesn’t.
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 学习重要的东西，忽略无关的东西。
- en: '[](https://medium.com/@danielwarfield1?source=post_page-----548ef6588aa4--------------------------------)[![Daniel
    Warfield](../Images/c1c8b4dd514f6813e08e401401324bca.png)](https://medium.com/@danielwarfield1?source=post_page-----548ef6588aa4--------------------------------)[](https://towardsdatascience.com/?source=post_page-----548ef6588aa4--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----548ef6588aa4--------------------------------)
    [Daniel Warfield](https://medium.com/@danielwarfield1?source=post_page-----548ef6588aa4--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@danielwarfield1?source=post_page-----548ef6588aa4--------------------------------)[![Daniel
    Warfield](../Images/c1c8b4dd514f6813e08e401401324bca.png)](https://medium.com/@danielwarfield1?source=post_page-----548ef6588aa4--------------------------------)[](https://towardsdatascience.com/?source=post_page-----548ef6588aa4--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----548ef6588aa4--------------------------------)
    [Daniel Warfield](https://medium.com/@danielwarfield1?source=post_page-----548ef6588aa4--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----548ef6588aa4--------------------------------)
    ·11 min read·Jul 19, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于[Towards Data Science](https://towardsdatascience.com/?source=post_page-----548ef6588aa4--------------------------------)
    ·阅读时间11分钟·2023年7月19日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/fddcbc3132d55c17e2ede623de6dbc4b.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fddcbc3132d55c17e2ede623de6dbc4b.png)'
- en: Photo by [Armand Khoury](https://unsplash.com/@armand_khoury?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由[Armand Khoury](https://unsplash.com/@armand_khoury?utm_source=medium&utm_medium=referral)提供，[Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: Attention, as popularized by the landmark paper [Attention Is All You Need](https://arxiv.org/pdf/1706.03762.pdf)
    (2017), is arguably the most important architectural trend in machine learning
    right now. Originally intended for sequence to sequence modeling, attention has
    exploded into virtually every sub-discipline of machine learning.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 注意力机制，由[Attention Is All You Need](https://arxiv.org/pdf/1706.03762.pdf) (2017)的开创性论文普及开来，可以说是目前机器学习中最重要的架构趋势。最初用于序列到序列建模的注意力机制，现已扩展到几乎所有的机器学习子学科。
- en: This post will describe a particular flavor of attention which proceeded the
    transforner style of attention. We’ll discuss how it works, and why it’s useful.
    We’ll also go over some literature and a tutorial implementing this form of attention
    in PyTorch. By reading this post, you will have a more thorough understanding
    of attention as a general concept, which is useful in exploring more cutting edge
    applications.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 本文将描述一种在变压器风格的注意力之前的特定注意力形式。我们将讨论它的工作原理及其用途。我们还将回顾一些文献，并提供一个在PyTorch中实现这种注意力形式的教程。通过阅读本文，你将对注意力这一概念有更深入的理解，这对探索更前沿的应用非常有用。
- en: The Reason For Attention
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 注意力机制的原因
- en: The attention mechanism was originally popularized in [Neural Machine Translation
    by Jointly Learning to Align and Translate](https://arxiv.org/pdf/1409.0473v7.pdf)(2014),
    which is the guiding reference for this particular post. This paper employs an
    encoder-decoder architecture for english-to-french translation.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 注意力机制最初由[Neural Machine Translation by Jointly Learning to Align and Translate](https://arxiv.org/pdf/1409.0473v7.pdf)
    (2014)推广，这也是本文的指导参考。该论文采用了一个编码器-解码器架构用于英语到法语的翻译。
- en: '![](../Images/432558a8bcc2a2e465bc2f5c9a4a2dc2.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/432558a8bcc2a2e465bc2f5c9a4a2dc2.png)'
- en: The encoder-decoder architecture in a nutshell, for a french to english translation
    task
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 编码器-解码器架构的简要介绍，用于法语到英语的翻译任务
- en: This is a very common architecture, but the exact details can change drastically
    from implementation to implementation. For instance, some of the earlier literature
    in sequence to sequence encoder-decoders were recurrent networks that would incrementally
    “build” and then “deconstruct” the embedding.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一种非常常见的架构，但具体细节可能会因实现而异。例如，一些早期的序列到序列编码器-解码器文献使用的是递归网络，这些网络逐步“构建”然后“解构”嵌入。
- en: '![](../Images/96dfa4a443781e07f6d9da7527e13a2b.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/96dfa4a443781e07f6d9da7527e13a2b.png)'
- en: Conceptualization of the information flow of a simple sequence to sequence recurrent
    encoder-decoder. The encoder incrementally embeds the english words, word by word,
    into the embedding space, which is then deconstructed by the decoder. In this
    diagram, the circles represent the embeddings throughout the encoder (red) the
    intermediate embedding space (white) and throughout the decoder (blue). In this
    case, the embeddings are long and complex vectors with abstract content which
    isn’t easily human interpretable.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 一个简单的序列到序列递归编码器-解码器的信息流概念化。编码器逐步将英语单词一个一个地嵌入到嵌入空间中，然后由解码器进行解构。在此图中，圆圈表示编码器中的嵌入（红色）、中间嵌入空间（白色）以及解码器中的嵌入（蓝色）。在这种情况下，嵌入是长且复杂的向量，包含抽象内容，人类难以直观理解。
- en: This general idea, and minor variations therein, was state of the art for several
    years. However, one problem with this approach is that the entire input sequence
    has to be embedded into the embedding space, which is generally a fixed sized
    vector. As a result, these models can easily forget content from sequences which
    are too long. **The attention mechanism was designed to alleviate the problem
    of needing to fit the entire input sequence into the embedding space. It does
    this by telling the model which inputs are related to which outputs.** Or, in
    other words, the attention mechanism allows a model to focus on relevant portions
    of the input, and disregard the rest.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 这个概念及其小的变体曾经是几年的前沿技术。然而，这种方法的一个问题是整个输入序列必须嵌入到嵌入空间中，这通常是一个固定大小的向量。因此，这些模型很容易忘记过长序列中的内容。**注意力机制旨在缓解将整个输入序列适应到嵌入空间的问题。它通过告诉模型哪些输入与哪些输出相关来实现这一点。**
    换句话说，注意力机制允许模型专注于输入的相关部分，忽略其余部分。
- en: '![](../Images/b4ae9c6464e1c3f38373ff2d71e00c77.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b4ae9c6464e1c3f38373ff2d71e00c77.png)'
- en: An example of what an attention based thought process might look like. In french,
    “je” is exactly identical to the word “I”. “Suis” is a conjugation of the verb
    “etre” which is “to be” and is conjugated as “suis” based on the subject “I” and
    the verb “am”. The choice of “directeur” is mostly related to the word “manager”,
    but is also related to the context in which that word is used. The choice of which
    inputs relate to which outputs is the task of the attention mechanism.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 基于注意力的思考过程的一个例子。在法语中，“je”与“我”完全相同。“Suis”是动词“etre”（“to be”）的一个变位形式，并根据主语“I”和动词“am”变成“suis”。“directeur”的选择与“manager”关系密切，但也与该词使用的上下文相关。选择哪些输入与哪些输出相关是注意力机制的任务。
- en: How Attention is Done, From a High Level
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从高层次来看注意力是如何工作的
- en: Practically, the attention mechanism we will be discussing ends up being a matrix
    of scores, called “alignment” scores. These alignment scores encode the degree
    to which a word in an input sequence relates to a word in the output sequence.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，我们将讨论的注意力机制最终成为一个称为“对齐”分数的矩阵。这些对齐分数编码了输入序列中的一个单词与输出序列中的一个单词之间的关系程度。
- en: '![](../Images/b36cb28b5075881824bec01e5bd8eb35.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b36cb28b5075881824bec01e5bd8eb35.png)'
- en: Two examples of attention matrices for two different english-to-french examples,
    from [Neural Machine Translation by Jointly Learning to Align and Translate](https://arxiv.org/pdf/1409.0473v7.pdf)(2014).
    This paper only tangentially mentions the term “attention”, and actually called
    this an “alignment model”. The term “attention” seems to have been popularized
    after the fact.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 两个注意力矩阵的示例，针对两个不同的英语到法语的例子，来自 [神经机器翻译：通过联合学习对齐和翻译](https://arxiv.org/pdf/1409.0473v7.pdf)(2014)。这篇论文只是间接提到了“注意力”一词，并实际上称之为“对齐模型”。“注意力”这一术语似乎是在事后被普及的。
- en: 'The alignment score can be computed many ways. We’ll stick with our 2014 paper
    and pick apart it’s particular alignment function:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 对齐评分可以通过多种方式计算。我们将坚持使用2014年的论文，详细分析其特定的对齐函数：
- en: '![](../Images/5b353ffc294c4dfe89c46f76613613f5.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5b353ffc294c4dfe89c46f76613613f5.png)'
- en: Alignment score calculation from [Neural Machine Translation by Jointly Learning
    to Align and Translate](https://arxiv.org/pdf/1409.0473v7.pdf)(2014). This is
    not the only alignment function which exists, but it’s the one we will be focusing
    on. “Ua” and “Wa” represent a learnable transformations of the previous output
    embedding (si-1) and a particular input embedding (hj), while “va” represents
    a learnable reduction of the total embedding down to the final alignment scalar.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 从[《通过联合学习对齐和翻译的神经机器翻译》](https://arxiv.org/pdf/1409.0473v7.pdf)（2014年）中计算对齐评分。这不是唯一存在的对齐函数，但这是我们将重点关注的函数。“Ua”和“Wa”表示对先前输出嵌入向量（si-1）和特定输入嵌入向量（hj）的可学习变换，而“va”表示将总嵌入向量减少到最终对齐标量的可学习变换。
- en: When calculating the allignment for the ith output, this approach uses the previous
    embedded state of the decoder (s_i-1), the embedding of an input word (h_j), and
    the learnable parameters W_a, U_a, and v_a, to calculate the alignment of the
    ith output with the jth input. The tanh activation function is included to add
    non-linearity, which is vital in training a model to understand complex relationships.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在计算第i个输出的对齐时，这种方法使用解码器的前一个嵌入状态（s_i-1）、一个输入词的嵌入（h_j）以及可学习的参数W_a、U_a和v_a，以计算第i个输出与第j个输入的对齐。tanh激活函数被包含以添加非线性，这在训练模型以理解复杂关系时至关重要。
- en: '**In other words, the function above calculates a score between the next output
    word and a single input word, denoting how relevant an input word is to the current
    output**. This function is run across all input words (h_j) to calculate an alignment
    score for all input words given the current output.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '**换句话说，上述函数计算了下一个输出词与单个输入词之间的分数，表示输入词对当前输出的相关性**。该函数在所有输入词（h_j）上运行，以根据当前输出计算所有输入词的对齐评分。'
- en: '![](../Images/a75c893188cb49be5a9d11a7b5214f29.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a75c893188cb49be5a9d11a7b5214f29.png)'
- en: A conceptual diagram of how the alignments for a given prediction (word 8) are
    calculated. The alignment function is calculated between the previous output embedding
    of the decoder, and all the inputs, to calculate the attention for the current
    output. Modified from [Neural Machine Translation by Jointly Learning to Align
    and Translate](https://arxiv.org/pdf/1409.0473v7.pdf)(2014)
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 一个关于如何计算给定预测（词8）的对齐的概念图。对齐函数在解码器的前一个输出嵌入向量与所有输入之间计算，以计算当前输出的注意力。改编自[《通过联合学习对齐和翻译的神经机器翻译》](https://arxiv.org/pdf/1409.0473v7.pdf)（2014年）
- en: A softmax function is applied across all of the computed alignments, turning
    them into a probability. This is referred to in the literature as a “soft-search”
    or “soft-alignment”.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 对所有计算出的对齐评分应用softmax函数，将其转化为概率。这在文献中被称为“soft-search”或“soft-alignment”。
- en: Exactly how attention is used can vary from implementation to implementation.
    In [Neural Machine Translation by Jointly Learning to Align and Translate](https://arxiv.org/pdf/1409.0473v7.pdf)(2014),
    the attention mechanism decides which input embeddings to provide to the decoder.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 注意力的具体使用方式可以因实现而异。在[《通过联合学习对齐和翻译的神经机器翻译》](https://arxiv.org/pdf/1409.0473v7.pdf)（2014年）中，注意力机制决定了向解码器提供哪些输入嵌入向量。
- en: '![](../Images/5e7f321781e6cc80460bf5da0a24d774.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5e7f321781e6cc80460bf5da0a24d774.png)'
- en: A graphical illustration of the proposed model trying to generate the t-th target
    word yt given a source sentence (x1, x2, . . . , xT ). Each input embedding is
    multiplied by it’s respective alignment score, then they are summed together to
    form the context vector which is used for the current decoder output step. From
    [Neural Machine Translation by Jointly Learning to Align and Translate](https://arxiv.org/pdf/1409.0473v7.pdf)(2014).
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 一个图示说明了提出的模型尝试生成第t个目标词yt，给定一个源句子（x1, x2, . . . , xT）。每个输入嵌入向量乘以其相应的对齐评分，然后将它们加在一起形成上下文向量，用于当前解码器输出步骤。来自[《通过联合学习对齐和翻译的神经机器翻译》](https://arxiv.org/pdf/1409.0473v7.pdf)（2014年）。
- en: It does this selection process with a weighted sum. All input embeddings are
    multiplied by their respective alignment score (in practice most of the alignment
    scores have a value of zero, while one or two might have a value of 0.8 and 0.2
    for instance), then those weighted embeddings are added together to create the
    context vector for a particular output.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 这个选择过程通过加权和来完成。所有的输入嵌入向量都乘以它们各自的对齐评分（实际上，大多数对齐评分的值为零，而一个或两个可能的值是0.8和0.2），然后这些加权后的嵌入向量被加在一起，形成特定输出的上下文向量。
- en: '![](../Images/1c99df7138d1f2f612325878df8fb68b.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1c99df7138d1f2f612325878df8fb68b.png)'
- en: The context vector used for the ith decoder step, ci. This is the weighted sum
    of all input embeddings based on their alignment score. From [Neural Machine Translation
    by Jointly Learning to Align and Translate](https://arxiv.org/pdf/1409.0473v7.pdf)(2014).
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 用于第 i 次解码步骤的上下文向量 ci。这是基于对齐得分的所有输入嵌入的加权总和。来自 [Neural Machine Translation by
    Jointly Learning to Align and Translate](https://arxiv.org/pdf/1409.0473v7.pdf)(2014)。
- en: '**The context vector is the combination of all of the inputs which are relevant
    to the current output.**'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '**上下文向量是所有与当前输出相关的输入的组合。**'
- en: 'The following figure ties together how attention fits into the bigger picture:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 下图展示了注意力如何融入整体结构：
- en: '![](../Images/97e1a0235e3646bcb0402c2db605abac.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/97e1a0235e3646bcb0402c2db605abac.png)'
- en: A breakdown of the flow of information for a particular output.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 特定输出的信息流的分解。
- en: The inputs are embedded into some initial vector representation (using word2vect,
    for instance).
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 输入被嵌入到一些初始的向量表示中（例如，使用 word2vect）。
- en: Those are passed through a bi-direcional LSTM, to create a bit of context awareness
    between the embeddings
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这些通过双向 LSTM 进行处理，以在嵌入之间创建一定的上下文意识。
- en: Alignment scores are calculated for each input using the previous decoder embedding
    and the learned parameters within the alignment function.
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用前一个解码器嵌入和对齐函数中的学习参数来计算每个输入的对齐得分。
- en: the soft-maxed alignments are multiplied against each input, added together,
    and used to construct the context vector
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 经过 soft-max 处理的对齐结果会与每个输入相乘，汇总在一起，并用于构建上下文向量。
- en: the decoder uses the previous decoder hidden state, along with the context vector,
    to generate the prediction of the current word.
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 解码器使用前一个解码器的隐藏状态以及上下文向量来生成当前单词的预测。
- en: In the next section we will implement this attention mechanism in PyTorch.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一部分，我们将实现这个注意力机制的 PyTorch 版本。
- en: Attention in PyTorch
  id: totrans-48
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: PyTorch 中的注意力。
- en: While I originally set out to implement the entire english to french example,
    it became apparent that the implementation would be excessively long, and contain
    many intricacies which are irrelevant to the explanation of attention itself.
    As a result, I created a toy problem which mimics the grammatical aspects of english
    to french translation to showcase the attention mechanism specifically, without
    the bulk of implementing LSTM’s, embeddings, utility tokens, batching, masking,
    and other problem specific components.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我最初计划实现整个英语到法语的示例，但显然实现过程将过于冗长，并且包含许多与注意力机制解释无关的复杂性。因此，我创建了一个玩具问题，该问题模拟了英语到法语翻译的语法方面，以专门展示注意力机制，而无需实现
    LSTM、嵌入、辅助令牌、批处理、掩码和其他特定问题的组件。
- en: The full code can be found [here](https://github.com/DanielWarfield1/MLWritingAndResearch/blob/main/AttentionDemo.ipynb),
    for those that are interested
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 完整代码可以在 [这里](https://github.com/DanielWarfield1/MLWritingAndResearch/blob/main/AttentionDemo.ipynb)
    找到，供感兴趣的人参考。
- en: 'As previously mentioned, english to french translation can be thought of as
    two subproblems; alignment and translation. The various networks within the encoder
    and decoder translate values, while the attention mechanism re-orients the vectors.
    In other words, **Attention is all about alignment.** To emulate the alignment
    problem of english to french translation the following toy problem was defined:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，英语到法语的翻译可以被认为是两个子问题：对齐和翻译。编码器和解码器中的各种网络翻译值，而注意力机制则重新定位这些向量。换句话说，**注意力完全依赖于对齐。**
    为了模拟英语到法语翻译中的对齐问题，定义了以下玩具问题：
- en: given some shuffled input of values
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一些打乱的值输入。
- en: '[PRE0]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Organize them into a sequential output:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 将它们组织成顺序输出：
- en: '[PRE1]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Practically, the toy question posed to the attention mechanism is: **Given
    the previous output vector, which output should come next, given a selection of
    possible outputs?** This is a very similar question to the gramatical question
    in english to french translation, which is: **Given a previous output word, which
    inputs are relevant to the next one?** Thus, by solving this toy problem, we can
    show the power of attention mechanism without getting too far in the weeds.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，给定之前的输出向量，提出给注意力机制的问题是：**在给定的可能输出中，下一个应该出现哪个输出？** 这与英语到法语翻译中的语法问题非常相似，即：**给定先前的输出单词，哪些输入与下一个输出相关？**
    因此，通过解决这个玩具问题，我们可以展示注意力机制的强大功能，而不会过于深入细节。
- en: Defining the Alignment Function
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 定义对齐函数。
- en: Recall the alignment function
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 回顾对齐函数。
- en: '![](../Images/5b353ffc294c4dfe89c46f76613613f5.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5b353ffc294c4dfe89c46f76613613f5.png)'
- en: The previously discussed alignment function, from [Neural Machine Translation
    by Jointly Learning to Align and Translate](https://arxiv.org/pdf/1409.0473v7.pdf)(2014)
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 上述讨论的对齐函数，来自于[《通过共同学习对齐和翻译的神经机器翻译》](https://arxiv.org/pdf/1409.0473v7.pdf)(2014)。
- en: 'This function, essentially, decides the weight (α) of an input (hj) given the
    previous output (si-1). This can be implemented directly in PyTorch:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 这个函数本质上决定了给定前一个输出(si-1)的输入(hj)的权重(α)。这可以直接在PyTorch中实现：
- en: '[PRE2]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '![](../Images/d579868d41bec4114bcadf359736f305.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d579868d41bec4114bcadf359736f305.png)'
- en: An example output of the alignment function. A scalar, which corresponds to
    a specific input-output pair.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 对齐函数的示例输出。一个标量，对应于特定的输入输出对。
- en: Defining Attention
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 定义注意机制
- en: For a given previous output, the task of the attention mechanism is to calculate
    which inputs to pay attention to. This can be done by calculating the alignment
    for all inputs and passing that vector of alignments through a softmax.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 对于给定的前一个输出，注意机制的任务是计算应关注哪些输入。这可以通过计算所有输入的对齐度，并将该对齐向量通过softmax来完成。
- en: '[PRE3]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '![](../Images/5a3559cac202eaa12883cb87f9e29298.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5a3559cac202eaa12883cb87f9e29298.png)'
- en: A single attention vector for a given output position.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 给定输出位置的单个注意向量。
- en: Defining A Learnable Attention Module
  id: totrans-70
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 定义一个可学习的注意模块
- en: Now we need to wrap the previous function into a PyTorch `nn.Module`. This is
    implemented in such a way where the computation of attention creates a traceable
    gradient, allowing the U, W, and V parameters to be updated through back propagation.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们需要将前面的函数封装到PyTorch `nn.Module`中。这是以一种计算注意机制产生可追踪梯度的方式实现的，允许通过反向传播更新U、W和V参数。
- en: Also, this module supports different encoder and decoder embeddings, which may
    be useful in adapting this module to different applications.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 此模块还支持不同的编码器和解码器嵌入，这可能在将此模块适应不同应用时非常有用。
- en: '[PRE4]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '![](../Images/dca233e2d6285a418dda48b7acd68323.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/dca233e2d6285a418dda48b7acd68323.png)'
- en: Creates a context vector of length 10\. This makes sense, as the input embeddings
    are of length 10, and the output of this attention technique is a context vector
    which is the weighted sum of all the input embeddings.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 创建长度为10的上下文向量。这是合理的，因为输入嵌入的长度为10，这种注意技术的输出是一个上下文向量，即所有输入嵌入的加权和。
- en: Training
  id: totrans-76
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练
- en: Now we can train the attention module to solve the toy problem. This is done
    by generating an X/Y pair (a corresponding shuffled and un-shuffled set), then
    iterating through what every output should be and adjusting weights if the model
    is incorrect.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以训练注意模块来解决玩具问题。这是通过生成X/Y对（一个相应的混洗和未混洗的集合），然后迭代每个输出应该是什么，并在模型错误时调整权重来完成的。
- en: '[PRE5]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '![](../Images/beed80f8b348997931d111ded32c62fa.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/beed80f8b348997931d111ded32c62fa.png)'
- en: Training Loss. As noted in the implementation, the training process could be
    significantly improved to encourage better convergence, but this serves for our
    toy example.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 训练损失。如实现中所述，训练过程可以显著改进，以促进更好的收敛，但这对于我们的示例来说已足够。
- en: Results
  id: totrans-81
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结果
- en: Using the following code, we can generate a randomly shuffled sequence and ask
    our attention model to sort it.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 使用以下代码，我们可以生成一个随机混洗的序列，并让我们的注意模型对其进行排序。
- en: '[PRE6]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '![](../Images/b9dc3234384afa07c1344938c5ca0e24.png)![](../Images/adf7caa77d82635c98feabab743720cc.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b9dc3234384afa07c1344938c5ca0e24.png)![](../Images/adf7caa77d82635c98feabab743720cc.png)'
- en: The shuffled input, the model output, and the alignment scores.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 混洗后的输入、模型输出和对齐分数。
- en: There are some artifacts based on the way the output is processed, but as you
    can see the attention mechanism is properly un-shuffling the input!
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 基于输出处理方式的一些伪影，但正如你所见，注意机制正确地将输入解混洗！
- en: Discussion
  id: totrans-87
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 讨论
- en: We created a general purpose alignment module which can be used within a larger
    network to focus on key information. This is directly inspired by the one used
    in english to french translation, but can be used in a variety of applications
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 我们创建了一个通用的对齐模块，可以在更大的网络中使用，以关注关键信息。这直接受到用于英法翻译的模块的启发，但可以应用于各种应用场景。
- en: Follow For More!
  id: totrans-89
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 关注更多！
- en: In future posts, I’ll also describing several landmark papers in the ML space,
    with an emphasis on practical and intuitive explanations. The attention mechanism
    used in transformers is a bit different than this attention mechanism, which I’ll
    be covering in a future post.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 在未来的文章中，我还将描述机器学习领域的几篇里程碑论文，重点是实际和直观的解释。变换器中使用的注意机制与这个注意机制有些不同，我会在未来的帖子中详细讲解。
- en: '**Please like, share, and follow. As an independent author, your support really
    makes a huge difference!**'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '**请点赞、分享并关注。作为独立作者，你的支持确实能产生巨大的影响！**'
- en: '**Attribution:** All of the images in this document were created by Daniel
    Warfield, unless a source is otherwise provided. You can use any images in this
    post for your own non-commercial purposes, so long as you reference this article,
    [https://danielwarfield.dev](https://danielwarfield.dev/), or both.'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '**署名：** 本文档中的所有图片均由**丹尼尔·沃菲尔德**创作，除非另有来源说明。你可以将本文中的任何图片用于你的非商业目的，只要你引用这篇文章，[https://danielwarfield.dev](https://danielwarfield.dev/)，或两者都引用。'
