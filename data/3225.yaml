- en: Quantisation and co. Reducing inference times on LLMs by 80%
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 量化及其相关技术：将LLMs的推理时间减少80%
- en: 原文：[https://towardsdatascience.com/quantisation-and-co-reducing-inference-times-on-llms-by-80-671db9349bdb?source=collection_archive---------2-----------------------#2023-10-27](https://towardsdatascience.com/quantisation-and-co-reducing-inference-times-on-llms-by-80-671db9349bdb?source=collection_archive---------2-----------------------#2023-10-27)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/quantisation-and-co-reducing-inference-times-on-llms-by-80-671db9349bdb?source=collection_archive---------2-----------------------#2023-10-27](https://towardsdatascience.com/quantisation-and-co-reducing-inference-times-on-llms-by-80-671db9349bdb?source=collection_archive---------2-----------------------#2023-10-27)
- en: '[](https://medium.com/@christopher_karg?source=post_page-----671db9349bdb--------------------------------)[![Christopher
    Karg](../Images/9d163d59e0c3167732f55d497caf9db2.png)](https://medium.com/@christopher_karg?source=post_page-----671db9349bdb--------------------------------)[](https://towardsdatascience.com/?source=post_page-----671db9349bdb--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----671db9349bdb--------------------------------)
    [Christopher Karg](https://medium.com/@christopher_karg?source=post_page-----671db9349bdb--------------------------------)'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@christopher_karg?source=post_page-----671db9349bdb--------------------------------)[![Christopher
    Karg](../Images/9d163d59e0c3167732f55d497caf9db2.png)](https://medium.com/@christopher_karg?source=post_page-----671db9349bdb--------------------------------)[](https://towardsdatascience.com/?source=post_page-----671db9349bdb--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----671db9349bdb--------------------------------)
    [Christopher Karg](https://medium.com/@christopher_karg?source=post_page-----671db9349bdb--------------------------------)'
- en: ·
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F5fbda6d16c39&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fquantisation-and-co-reducing-inference-times-on-llms-by-80-671db9349bdb&user=Christopher+Karg&userId=5fbda6d16c39&source=post_page-5fbda6d16c39----671db9349bdb---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----671db9349bdb--------------------------------)
    ·12 min read·Oct 27, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F671db9349bdb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fquantisation-and-co-reducing-inference-times-on-llms-by-80-671db9349bdb&user=Christopher+Karg&userId=5fbda6d16c39&source=-----671db9349bdb---------------------clap_footer-----------)'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F5fbda6d16c39&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fquantisation-and-co-reducing-inference-times-on-llms-by-80-671db9349bdb&user=Christopher+Karg&userId=5fbda6d16c39&source=post_page-5fbda6d16c39----671db9349bdb---------------------post_header-----------)
    发布于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----671db9349bdb--------------------------------)
    ·12分钟阅读·2023年10月27日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F671db9349bdb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fquantisation-and-co-reducing-inference-times-on-llms-by-80-671db9349bdb&user=Christopher+Karg&userId=5fbda6d16c39&source=-----671db9349bdb---------------------clap_footer-----------)'
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F671db9349bdb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fquantisation-and-co-reducing-inference-times-on-llms-by-80-671db9349bdb&source=-----671db9349bdb---------------------bookmark_footer-----------)![](../Images/c44c0f469d07bf09ab4e359dd48265fe.png)'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F671db9349bdb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fquantisation-and-co-reducing-inference-times-on-llms-by-80-671db9349bdb&source=-----671db9349bdb---------------------bookmark_footer-----------)![](../Images/c44c0f469d07bf09ab4e359dd48265fe.png)'
- en: 'Source: [https://www.pexels.com/photo/cropland-in-autumn-18684338/](https://www.pexels.com/photo/cropland-in-autumn-18684338/)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '来源: [https://www.pexels.com/photo/cropland-in-autumn-18684338/](https://www.pexels.com/photo/cropland-in-autumn-18684338/)'
- en: Quantisation is a technique used for a range of different algorithms but has
    gained prevalence with the fairly recent influx of Large Language Models (LLMs).
    In this article, I aim to provide information on the quantisation of LLMs and
    the impact this technique can have on running these models locally. I’ll cover
    a different strategy outside quantisation that can further reduce computational
    requirements of running these models. I’ll go on to explain why these techniques
    may be of interest to you and will show you some benchmarks with code examples
    as to how effective these techniques are. I also briefly cover hardware requirements/recommendations
    and the modern tools available to you for achieving your LLM goals on your machine.
    In a later article I plan to provide step-by-step instructions and code for fine-tuning
    your own LLM so keep an eye out for that.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 量化是一种用于各种不同算法的技术，但随着大型语言模型（LLMs）的最近普及，它变得越来越重要。在这篇文章中，我旨在提供有关LLM量化的信息以及该技术对本地运行这些模型的影响。我将介绍量化以外的不同策略，以进一步减少运行这些模型的计算需求。我将解释这些技术为何可能对你感兴趣，并展示一些基准和代码示例，以说明这些技术的有效性。我还简要介绍了硬件要求/建议和现代工具，帮助你在自己的机器上实现LLM目标。在后续文章中，我计划提供逐步的说明和代码，用于微调你自己的LLM，请密切关注。
- en: TL;DR — by quantising our LLM and changing the tensor *dtype*, we are able to
    run inference on an LLM with 2x the parameters whilst also reducing *Wall time*
    by 80%.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: TL;DR — 通过对我们的LLM进行量化并改变张量*数据类型*，我们能够在具有2倍参数的LLM上运行推理，同时将*墙时间*减少80%。
- en: As always, if you wish to discuss anything I cover here please [reach out](http://www.linkedin.com/in/-christopherkarg).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 如往常一样，如果你想讨论我在这里提到的任何内容，请[联系我](http://www.linkedin.com/in/-christopherkarg)。
- en: All opinions in this article are my own. This article is not sponsored.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 本文中的所有观点均为我个人观点。本文未获得赞助。
- en: What is quantisation (of LLMs)?
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是LLM的量化？
