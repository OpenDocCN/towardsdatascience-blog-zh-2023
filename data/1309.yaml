- en: VALL-E — The Future of Text to Speech?
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: VALL-E — 语音合成的未来？
- en: 原文：[https://towardsdatascience.com/vall-e-the-future-of-text-to-speech-d090b6ede07b?source=collection_archive---------4-----------------------#2023-04-14](https://towardsdatascience.com/vall-e-the-future-of-text-to-speech-d090b6ede07b?source=collection_archive---------4-----------------------#2023-04-14)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/vall-e-the-future-of-text-to-speech-d090b6ede07b?source=collection_archive---------4-----------------------#2023-04-14](https://towardsdatascience.com/vall-e-the-future-of-text-to-speech-d090b6ede07b?source=collection_archive---------4-----------------------#2023-04-14)
- en: A paper walkthrough of the new text-to-speech model by Microsoft Research
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 微软研究院新文本到语音模型的论文解析
- en: '[](https://medium.com/@erap129?source=post_page-----d090b6ede07b--------------------------------)[![Elad
    Rapaport](../Images/2ad958f92cd8b5735da900ae8f5559f3.png)](https://medium.com/@erap129?source=post_page-----d090b6ede07b--------------------------------)[](https://towardsdatascience.com/?source=post_page-----d090b6ede07b--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----d090b6ede07b--------------------------------)
    [Elad Rapaport](https://medium.com/@erap129?source=post_page-----d090b6ede07b--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@erap129?source=post_page-----d090b6ede07b--------------------------------)[![Elad
    Rapaport](../Images/2ad958f92cd8b5735da900ae8f5559f3.png)](https://medium.com/@erap129?source=post_page-----d090b6ede07b--------------------------------)[](https://towardsdatascience.com/?source=post_page-----d090b6ede07b--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----d090b6ede07b--------------------------------)
    [Elad Rapaport](https://medium.com/@erap129?source=post_page-----d090b6ede07b--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fd2d1ff8f0490&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fvall-e-the-future-of-text-to-speech-d090b6ede07b&user=Elad+Rapaport&userId=d2d1ff8f0490&source=post_page-d2d1ff8f0490----d090b6ede07b---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----d090b6ede07b--------------------------------)
    ·15 min read·Apr 14, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fd090b6ede07b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fvall-e-the-future-of-text-to-speech-d090b6ede07b&user=Elad+Rapaport&userId=d2d1ff8f0490&source=-----d090b6ede07b---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fd2d1ff8f0490&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fvall-e-the-future-of-text-to-speech-d090b6ede07b&user=Elad+Rapaport&userId=d2d1ff8f0490&source=post_page-d2d1ff8f0490----d090b6ede07b---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----d090b6ede07b--------------------------------)
    ·15 min read·2023年4月14日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fd090b6ede07b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fvall-e-the-future-of-text-to-speech-d090b6ede07b&user=Elad+Rapaport&userId=d2d1ff8f0490&source=-----d090b6ede07b---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fd090b6ede07b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fvall-e-the-future-of-text-to-speech-d090b6ede07b&source=-----d090b6ede07b---------------------bookmark_footer-----------)![](../Images/01c0e54ba57f7e90b07dcfda70c8f66d.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fd090b6ede07b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fvall-e-the-future-of-text-to-speech-d090b6ede07b&source=-----d090b6ede07b---------------------bookmark_footer-----------)![](../Images/01c0e54ba57f7e90b07dcfda70c8f66d.png)'
- en: 'DALL-E 2: a record player that receives text on one side, and outputs sound
    waves on the other side. digital art.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: DALL-E 2：一个接收文本输入的一侧，另一侧输出声波的唱片机。数字艺术。
- en: Hello readers,
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 大家好，
- en: In this article, we will dive deep into a new and exciting text-to-speech model
    developed by Microsoft Research, called VALL-E. The paper presenting the work
    has been released on Jan. 5, 2023, and since then has been gaining much attention
    online. It is worth noting that as of writing this article, no pre-trained model
    has been released and the only option currently to battle-test this model is to
    train it by yourself.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们将深入探讨微软研究院开发的一种新兴的文本到语音模型，名为VALL-E。介绍这项工作的论文已于2023年1月5日发布，从那时起，它在网上获得了广泛关注。值得注意的是，撰写本文时，没有发布预训练模型，目前唯一的选择是自行训练该模型。
- en: Nevertheless, the idea presented in this paper is novel and interesting and
    worth digging into, regardless of whether I can immediately clone my voice with
    it or not.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管本文提出的想法新颖有趣，值得深入探讨，不管我是否能立即用它克隆我的声音。
- en: 'This article will be organized as follows:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 本文将按以下结构组织：
- en: Part 1 — Introduction to Text to Speech, Basic Concepts
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第1部分 — 语音合成介绍，基本概念
- en: 'Part 2 — VALL-E: Text to Speech as a Language Model'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '第2部分 — VALL-E: 作为语言模型的语音合成'
- en: 'Part 3 — Encodec: The Workhorse Behind VALL-E'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '第3部分 — Encodec: VALL-E 背后的工作马'
- en: Part 4 — Problem formulation and training of VALL-E
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第4部分 — VALL-E的问题定义与训练
- en: Part 5— Some Coding
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第5部分 — 一些编码
- en: Part 6— Conclusions & Thoughts Ahead
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第6部分 — 结论与未来展望
- en: Part 1 — Introduction to Text to Speech, Basic Concepts
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第1部分 — 语音合成介绍，基本概念
- en: The technology of text-to-speech is not new and has been around since the “Voder”
    — the first electronic voice synthesizer from Bell Labs in 1939 which required
    manual operation. Since then, the field has seen incredible developments and up
    until ~2017, the dominant technology was concatenative speech synthesis. This
    technology is based on the concatenation of pre-recorded speech segments to create
    intelligible speech. Although this technology can produce lifelike results, its
    drawbacks are obvious — it cannot generate new voices which don’t exist in the
    pre-recorded database, and it cannot generate speech with a different tone or
    emotion.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 语音合成技术并不新鲜，自1939年贝尔实验室推出的第一个电子语音合成器“Voder”以来，这项技术就已经存在，并且当时需要手动操作。此后，该领域取得了令人难以置信的发展，直到2017年左右，主流技术仍是连接语音合成。这项技术基于连接预录制的语音片段以生成可理解的语音。尽管这种技术可以产生逼真的效果，但其缺点显而易见——它无法生成预录数据库中不存在的新声音，也无法生成不同语调或情感的语音。
- en: Fast-forward to the era of deep learning. Nowadays, the dominant strategy in
    text-to-speech synthesis is summarized in Figure 1\. Let’s go over its different
    parts.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 快进到深度学习时代。如今，语音合成的主流策略总结在图1中。让我们来看看它的不同部分。
- en: '![](../Images/01a498ff72e9b7b6ab737ee75e596d21.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/01a498ff72e9b7b6ab737ee75e596d21.png)'
- en: Figure 1\. A model neural text-to-speech pipeline. Image by author.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 图1\. 一个模型神经语音合成流程图。作者提供的图像。
- en: First, we have a phonemizer that transforms text into phonemes. Phonemes are
    a textual representation of the pronunciation of words (for example — the word
    tomato will have different phonemes in an American and British accent), and this
    representation helps the downstream model achieve better results.
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 首先，我们有一个音素化器，将文本转化为音素。音素是词语发音的文本表示（例如——“tomato”这个词在美式和英式口音中有不同的音素），这种表示帮助下游模型获得更好的结果。
- en: Afterward, we have an acoustic model which transforms these phonemes into a
    Mel spectrogram, which is a representation of audio in the time X frequency domain.
    A spectrogram is achieved by applying a short Fourier transform (STFT) on overlapping
    time windows of a raw audio waveform (here is an excellent explanation about the
    Mel spectrogram — [https://medium.com/analytics-vidhya/understanding-the-mel-spectrogram-fca2afa2ce53](https://medium.com/analytics-vidhya/understanding-the-mel-spectrogram-fca2afa2ce53)).
    Of course in this case the spectrogram is being created by a statistical model,
    as no input audio exists in real-time text-to-speech. Examples of recent model
    architectures include Tacotron2, DeepVoice 3, and TransformerTTS.
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 之后，我们会有一个声学模型将这些音素转化为梅尔频谱图，这是一种在时间-频率域中表示音频的方式。频谱图是通过对原始音频波形的重叠时间窗口应用短时傅里叶变换（STFT）获得的（这里是一个关于梅尔频谱图的出色解释——[https://medium.com/analytics-vidhya/understanding-the-mel-spectrogram-fca2afa2ce53](https://medium.com/analytics-vidhya/understanding-the-mel-spectrogram-fca2afa2ce53)）。当然，在这种情况下，频谱图是由统计模型创建的，因为实时语音合成中不存在输入音频。最近的模型架构示例包括Tacotron2、DeepVoice
    3和TransformerTTS。
- en: The final stage is the conversion of the Mel spectrogram into a waveform. A
    waveform is usually sampled at 24/48 kHz, where each sample is digitized into
    a 16-bit number. These numbers represent the amount of air pressure at each moment
    in time, which is the sound we eventually hear in our ears. Why can’t we just
    deterministically convert the spectrogram into a waveform? Because it requires
    major upsampling in the time domain which requires us to create information that
    doesn’t explicitly exist in the spectrogram, and also because spectrograms don’t
    contain phase information (only frequency). So, as in the conversion of phonemes
    to a Mel spectrogram, here as well we need a statistical model to convert the
    spectrogram into a waveform and these models are called Vocoders. Examples of
    Vocoders include WaveNet, WaveRNN, and MelGAN.
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最终阶段是将Mel频谱图转换为波形。波形通常以24/48 kHz的采样率进行采样，每个样本被数字化为16位数值。这些数字表示每时刻的气压量，即我们最终听到的声音。为什么我们不能简单地将频谱图确定性地转换为波形？因为这需要在时间域上进行大量的上采样，这需要创建频谱图中不存在的信息，同时频谱图不包含相位信息（只有频率）。因此，就像将音素转换为Mel频谱图一样，在这里我们也需要一个统计模型来将频谱图转换为波形，这些模型称为Vocoders。Vocoders的例子包括WaveNet、WaveRNN和MelGAN。
- en: Additionally, there are recent models such as VITS and YourTTS, which employ
    an end-to-end model to generate waveforms from text input. Another example of
    such an end-to-end system is a paper titled “End-to-End Adversarial Text-to-Speech”
    by Deepmind (which is excellently explained by Yannic Kilcher here — [https://www.youtube.com/watch?v=WTB2p4bqtXU](https://www.youtube.com/watch?v=WTB2p4bqtXU)).
    In this paper, they employ a GAN-like training procedure to produce realistic
    speech sound waves. They also need to tackle the alignment problem, which is the
    degree to which word utterances in the generated samples align in time with the
    same word utterances in the ground truth samples. This problem does not “solve
    on its own” and requires explicit handling in the model architecture.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，还有最近的模型如VITS和YourTTS，它们使用端到端模型从文本输入生成波形。另一个这样的端到端系统的例子是Deepmind的一篇论文《End-to-End
    Adversarial Text-to-Speech》（Yannic Kilcher在此处进行了极好的解释 — [https://www.youtube.com/watch?v=WTB2p4bqtXU](https://www.youtube.com/watch?v=WTB2p4bqtXU)）。在这篇论文中，他们采用了类似GAN的训练过程来生成真实的语音声波。他们还需要解决对齐问题，即生成样本中的单词发音与真实样本中的单词发音在时间上对齐的程度。这个问题不会“自动解决”，需要在模型架构中进行明确处理。
- en: The main drawback of these end-to-end TTS models is their incredible complexity.
    Text and speech are such different modalities, and this requires complex models
    which tackle problems such as alignment, speaker identity, and language in an
    explicit manner, making these models highly complex. The charm of VALL-E, which
    we will soon dive into, is that it takes the relative simplicity of generative
    language models and employs them creatively in the field of speech generation.
    For people like me who are new to the field of TTS and speech in general and have
    some experience in NLP, it allows a good entry point into this fascinating field.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 这些端到端TTS模型的主要缺点是其极其复杂性。文本和语音是如此不同的模态，这要求复杂的模型显式地解决对齐、说话者身份和语言等问题，使得这些模型高度复杂。VALL-E的魅力在于，它将生成语言模型的相对简单性巧妙地应用于语音生成领域。对于像我这样刚接触TTS和语音领域，并且有一定NLP经验的人来说，它提供了一个很好的切入点。
- en: This short overview did not do justice to the immense field of TTS, which one
    can spend a lifetime studying and understanding (I do encourage you to dive a
    bit deeper). Yet, we are here today to talk about VALL-E, so allow me to jump
    straight to it.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 这简短的概述无法充分体现TTS（文本转语音）这一广阔领域的深度，研究和理解它可能需要一生的时间（我鼓励你深入探讨）。然而，我们今天要讨论的是VALL-E，所以请允许我直接切入主题。
- en: 'Part 2 — VALL-E: Text to Speech as a Language Model'
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '第二部分 — VALL-E: 语言模型的文本转语音'
- en: As in other text-to-speech systems, the input to VALL-E is phonemicized text,
    and the output is the corresponding sound waveform. Additionally, VALL-E employs
    a prompting mechanism in which a 3-second audio sample is fed as additional input
    to the model. This allows the generation of a speech utterance of the input text
    which is conditioned on the given audio prompt — in practice, this means the ability
    to perform zero-shot speech generation, which is the generation of speech from
    a voice unseen in the training data. The high-level structure of VALL-E is presented
    in Figure 2.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 与其他文本到语音系统一样，VALL-E的输入是音素化的文本，输出是相应的声音波形。此外，VALL-E采用了提示机制，其中3秒的音频样本作为额外输入提供给模型。这允许生成与给定音频提示条件相关的输入文本的语音发声——实际上，这意味着能够进行零-shot语音生成，即从训练数据中未见过的声音生成语音。VALL-E的高层结构见图2。
- en: '![](../Images/9445e6782483a3cad789e76d49a94e20.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9445e6782483a3cad789e76d49a94e20.png)'
- en: Figure 2\. The high-level structure of VALL-E. Image taken from the original
    paper [1].
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 图2\. VALL-E的高层结构。图片来自原始论文[1]。
- en: Let’s understand what happens in this pipeline. First, we have a phoneme conversion
    of the text, which is standard procedure as we understood already, and doesn’t
    require any learning mechanism. In order to process these phonemes by the model
    we have a phoneme embedding layer that takes as input a vector of indices into
    the phoneme vocabulary and outputs a matrix of embeddings corresponding to the
    input indices.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们理解一下这个管道中的过程。首先，我们对文本进行音素转换，这是一种标准程序，我们已经了解过，不需要任何学习机制。为了让模型处理这些音素，我们有一个音素嵌入层，该层将音素词汇表中的索引向量作为输入，并输出对应于输入索引的嵌入矩阵。
- en: The 3-second acoustic prompt, which the output speech is conditioned on, is
    fed into an audio codec encoder. In VALL-E they use a pre-trained audio encoder
    for this — Encodec (developed by Facebook Research — [https://arxiv.org/abs/2210.13438](https://arxiv.org/abs/2210.13438)).
    Encodec takes as input a waveform of speech and outputs a compressed discrete
    representation of it via recursive vector quantization (RVQ) using an encoder-decoder
    neural architecture. We will dive into Encodec in Part 3 of this article, but
    for now, let’s just assume that it outputs a discrete representation of the audio
    signal by splitting it into fixed time windows and assigning each window a representation
    from a known vocabulary of audio embeddings (conceptually, very similar to word
    embeddings).
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 3秒的音频提示（输出语音的条件）被送入音频编解码器编码器。在VALL-E中，他们使用了一个预训练的音频编码器——Encodec（由Facebook Research开发——[https://arxiv.org/abs/2210.13438](https://arxiv.org/abs/2210.13438)）。Encodec以语音波形为输入，通过递归向量量化（RVQ）和编码-解码神经架构输出其压缩的离散表示。我们将在本文的第3部分深入探讨Encodec，但现在可以假设它通过将音频信号分割成固定时间窗口并为每个窗口分配来自已知音频嵌入词汇表的表示，从而输出音频信号的离散表示（概念上，与词嵌入非常相似）。
- en: Once the model receives these two inputs, it can act as an autoregressive language
    model and output the next discrete audio representation. Because the audio representations
    come from a fixed which was learned by Encodec, we can think of this simply as
    predicting the next word in a sentence out of a fixed vocabulary of words (a fixed
    vocabulary of sound representations, in our case). After these sound representations
    are predicted they are transformed back into the original waveform representation
    using the Decoder part of the Encodec model.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦模型接收到这两个输入，它就可以像自回归语言模型一样输出下一个离散音频表示。由于音频表示来自Encodec学习的固定词汇表，我们可以简单地将其视为从固定词汇表中预测下一个单词（在我们的例子中是声音表示）。在这些声音表示被预测之后，它们通过Encodec模型的解码器部分被转换回原始的波形表示。
- en: In Figure 3 we compare the pipeline of VALL-E to the traditional neural TTS
    pipeline. We see that the main difference is the intermediate representation of
    audio. In VALL-E they gave up on the Mel spectrogram and used the representation
    created by the Encodec model. It is worth noting though that under the hood Encodec
    uses a spectrogram representation as well, so it is still somewhat in use in this
    architecture, albeit less prominently.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在图3中，我们将VALL-E的管道与传统的神经TTS管道进行比较。我们看到主要的区别在于音频的中间表示。在VALL-E中，他们放弃了Mel谱图，使用了由Encodec模型创建的表示。然而值得注意的是，Encodec在底层也使用了谱图表示，因此它在这个架构中仍然有所使用，尽管不那么显著。
- en: '![](../Images/74d049711c89c9c4c4af40f0e62a6b1a.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/74d049711c89c9c4c4af40f0e62a6b1a.png)'
- en: Figure 3\. The VALL-E pipeline VS the traditional neural TTS pipeline. Image
    by author.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3\. VALL-E 流水线与传统神经 TTS 流水线的对比。图片由作者提供。
- en: In the results section of the VALL-E paper, they have shown that they outperform
    the previous state-of-the-art zero-shot TTS model, YourTTS, on the LibriSpeech
    data on several metrics which include human-based evaluations such as similarity
    mean option score (SMOS) and algorithm-based evaluations such as word error rate
    (WER). In an interesting ablation study, they show that the phoneme prompt contributes
    to the content of generation (by reducing WER) and the audio prompt contributes
    to speaker similarity (by improving a speaker similarity metric).
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在 VALL-E 论文的结果部分，他们展示了在 LibriSpeech 数据集上超越了之前的最先进的零样本 TTS 模型 YourTTS，在几个指标上，包括基于人类的评估（如相似性均值选项评分（SMOS））和基于算法的评估（如词错误率（WER））。在一个有趣的消融研究中，他们表明，音素提示有助于生成内容（通过降低
    WER），而音频提示有助于说话人相似性（通过提高说话人相似性指标）。
- en: We will now dive into the Encodec model which is responsible for converting
    audio to discrete tokens and back and is the enabler for using a language model
    approach to audio generation in this paper.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将深入探讨 Encodec 模型，该模型负责将音频转换为离散标记并再转换回来，并且是本文中使用语言模型方法进行音频生成的基础。
- en: 'Part 3 — Encodec: The Workhorse Behind VALL-E'
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第 3 部分 — Encodec：VALL-E 背后的主力
- en: In Figure 4 we can see the Encodec architecture. It is an encoder-decoder architecture
    that learns a condensed representation of the audio signal via the task of reconstruction.
    Let’s go over its different parts to understand what is going on under the hood.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在图 4 中，我们可以看到 Encodec 架构。它是一个编码器-解码器架构，通过重建任务学习音频信号的压缩表示。让我们详细了解其不同部分，以理解其内部运作。
- en: '![](../Images/4cd25570934108973a00e99d2a617869.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4cd25570934108973a00e99d2a617869.png)'
- en: Figure 4\. The Encodec architecture. Image taken from the original paper [2].
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4\. Encodec 架构。图片取自原始论文 [2]。
- en: On the far left we have our original waveform, which is sampled at 24/48 kHz,
    and each sample is represented by 16 bits (65536 options). The raw signal gets
    passed into the Encoder which includes 1D convolution operations for downsampling
    and a two-layer LSTM for sequence modeling. The output of the encoder is 75/150
    latent timesteps (compare this to the original 24/48K!), with a depth dimension
    of 128.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在最左侧，我们有原始波形，其采样频率为 24/48 kHz，每个样本由 16 位（65536 选项）表示。原始信号传递给编码器，编码器包括用于下采样的
    1D 卷积操作和用于序列建模的两层 LSTM。编码器的输出是 75/150 潜在时间步（与原始的 24/48K 比较！），深度维度为 128。
- en: The decoder is simply a mirrored version of the encoder, using transposed convolutions
    in order to upsample the latent space and construct the audio waveform (here is
    a good explanation on transposed convolutions [https://towardsdatascience.com/what-is-transposed-convolutional-layer-40e5e6e31c11](/what-is-transposed-convolutional-layer-40e5e6e31c11)).
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 解码器仅仅是编码器的镜像版本，使用转置卷积来对潜在空间进行上采样并构建音频波形（这是对转置卷积的一个很好的解释 [https://towardsdatascience.com/what-is-transposed-convolutional-layer-40e5e6e31c11](/what-is-transposed-convolutional-layer-40e5e6e31c11)）。
- en: The interesting bit here is, of course, the quantizer. How does Encodec quantize
    the continuous domain of sound? Using a technique called residual vector quantization
    (RVQ) which consists of projecting an input vector onto the closest entry in a
    codebook of a given size. Let’s break that sentence down. First, what is a codebook?
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的部分当然是量化器。Encodec 如何对声音的连续域进行量化？使用一种叫做残差向量量化（RVQ）的技术，该技术包括将输入向量投影到给定大小代码本中的最近条目。让我们详细解析这句话。首先，什么是代码本？
- en: In the case of VALL-E, a codebook is a dictionary of vectors of size 1024, where
    each entry represents a vector of size 128\. Our goal in vector quantization is
    to map a certain vector to the closest vector in the codebook (by Euclidean distance,
    for example), thereafter it can be represented by the index of that vector in
    the codebook (assuming everyone has access to the codebook). Of course, in this
    way, we lose a lot of information. What if no vector in the codebook accurately
    resembles our vector? Hence the “residual” in RVQ!
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 VALL-E 的情况，代码本是一个大小为 1024 的向量字典，其中每个条目表示一个大小为 128 的向量。我们在向量量化中的目标是将某个向量映射到代码本中最接近的向量（例如通过欧几里得距离），之后可以通过该向量在代码本中的索引来表示它（假设每个人都能访问代码本）。当然，这样我们会丢失大量的信息。如果代码本中的向量都不能准确地类似于我们的向量怎么办？这就是
    RVQ 中的“残差”！
- en: In Figure 5, I show how a vector is quantized using residual vector quantization.
    In the example, we have 3 codebooks. The input vector is compared to each of the
    vectors in the first codebook and assigned to the closest one (C1,1). Then, the
    residual between the C1,1 and the input is calculated and we try to match the
    residual to the next codebook, and so on until we reach the end of our codebooks.
    The final RVQ representation is the indices that were matched in each of the codebooks
    (1, 3, 2 in our example). This encoding method is extremely efficient. If we have
    8 codebooks where each contains 1024 entries — we can represent 1024⁸=1.2e+24
    different vectors using only 1024*8=8192 numbers! Of course, the sender and receiver
    must hold the same codebook for this quantization method to work. If you want
    to learn more about RVQ, such as how the codebooks are trained, I recommend reading
    another paper that Encodec is based on called SoundStream — [https://arxiv.org/abs/2107.03312](https://arxiv.org/abs/2107.03312)
    (yes, this is a rabbit hole).
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在图5中，我展示了如何使用残差向量量化对向量进行量化。在这个例子中，我们有3个代码本。输入向量与第一个代码本中的每一个向量进行比较，并分配给最接近的一个（C1,1）。然后，计算C1,1与输入之间的残差，我们尝试将残差匹配到下一个代码本，以此类推，直到我们遍历完所有的代码本。最终的RVQ表示是每个代码本中匹配的索引（在我们的例子中是1、3、2）。这种编码方法非常高效。如果我们有8个代码本，每个包含1024个条目——我们可以用仅仅1024*8=8192个数字表示1024⁸=1.2e+24种不同的向量！当然，发送方和接收方必须持有相同的代码本，这种量化方法才能有效。如果你想深入了解RVQ，比如代码本是如何训练的，我推荐阅读另一篇Encodec所基于的论文《SoundStream》——[https://arxiv.org/abs/2107.03312](https://arxiv.org/abs/2107.03312)（是的，这确实是一个深奥的领域）。
- en: '![](../Images/ffe1cc98ba3fe777711583d8363313f1.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ffe1cc98ba3fe777711583d8363313f1.png)'
- en: Figure 5\. Example of Residual Vector Quantization. Image by author.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 图5\. 残差向量量化示例。图片由作者提供。
- en: 'Back to the Encodec pipeline in Figure 4, let’s notice 3 additional details
    which are relevant to its training procedure:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 回到图4中的Encodec流程，我们来注意与其训练过程相关的3个额外细节：
- en: Mel spectrograms are created both from the input audio and from the generated
    audio. These spectrograms are compared and the signal from the comparison is used
    as a loss to direct the model training.
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 梅尔频谱图是从输入音频和生成音频中创建的。这些频谱图进行比较，比较信号用于作为损失来指导模型训练。
- en: Several discriminators are used in order to compare a short-time Fourier transform
    (STFT) of the original and synthetic waveform. This GAN loss gives a different
    signal than the Mel spectrogram comparison and was found useful for Encodec.
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用了几个鉴别器来比较原始波形和合成波形的短时傅里叶变换（STFT）。这种GAN损失与梅尔频谱图比较给出的信号不同，并且对Encodec有用。
- en: The quantizer contains transformers that are used for further compression of
    the audio signal. This is **not** the transformer in VALL-E that predicts the
    next token of speech, as confusing as it may be. For further understanding of
    the transformers in Encodec, I recommend reading the paper or watching the video
    by Aleksa Gordic — [https://www.youtube.com/watch?v=mV7bhf6b2Hs](https://www.youtube.com/watch?v=mV7bhf6b2Hs).
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 量化器包含用于进一步压缩音频信号的变压器。这**不是**VALL-E中预测下一个语音标记的变压器，尽管可能会造成混淆。要进一步理解Encodec中的变压器，我建议阅读Aleksa
    Gordic的论文或观看视频——[https://www.youtube.com/watch?v=mV7bhf6b2Hs](https://www.youtube.com/watch?v=mV7bhf6b2Hs)。
- en: Let’s summarize what we know so far. VALL-E is a text-to-speech model that resembles
    language models in its operational mode, such that it predicts the next discrete
    audio token for a given prompt, which consists of phonemicized text and audio
    input. These discrete tokens are learned by another model called Encodec (which
    itself is based on SoundStream) that uses an encoder-decoder architecture with
    residual vector quantization to convert audio into discrete codes.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们总结一下我们目前所知道的。VALL-E是一个文本到语音模型，其操作模式类似于语言模型，预测给定提示的下一个离散音频标记，该提示由音素化文本和音频输入组成。这些离散标记由另一个名为Encodec的模型学习（Encodec本身基于SoundStream），它使用编码器-解码器架构和残差向量量化将音频转换为离散代码。
- en: Part 4 — Problem formulation and training of VALL-E
  id: totrans-58
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第四部分——VALL-E的问题表述和训练
- en: VALL-E contains two transformer models which are used to process the input data
    (phonemicized text and audio) — an autoregressive (AR) transformer that attends
    only to past data, and a non-autoregressive (NAR) transformer that attends to
    all points in time. Let’s see why.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: VALL-E包含两个变压器模型，用于处理输入数据（音素化文本和音频）——一个自回归（AR）变压器仅关注过去的数据，而一个非自回归（NAR）变压器关注所有时间点。让我们看看原因。
- en: 'Eight different codebooks are used in VALL-E as part of the Encodec model,
    where each codebook consists of 1024 entries. The codes from the first quantizer
    (codebook) are processed by the AR model according to this Equation 1\. Let’s
    first clarify some terminology here:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: VALL-E 使用了八个不同的编码本作为 Encodec 模型的一部分，其中每个编码本包含 1024 个条目。第一个量化器（编码本）的编码由 AR 模型根据方程
    1 进行处理。让我们首先澄清一些术语：
- en: C represents the generated output — as discrete audio codes
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: C 代表生成的输出 — 作为离散音频编码
- en: C~ is the 3-second input acoustic prompt
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: C~ 是 3 秒的输入声学提示
- en: x is the input text as a phoneme sequence
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: x 是作为音素序列的输入文本
- en: C:,₁ represents data from the first quantizer/codebook for C
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: C:,₁ 代表来自第一个量化器/编码本的数据
- en: So, Equation 1 shows that the output for the first quantizer is conditioned
    on the input data, and on the previous timesteps’ outputs for the first quantizer
    (just like an autoregressive language model).
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，方程 1 显示，第一个量化器的输出基于输入数据以及第一个量化器的前时间步的输出（就像自回归语言模型一样）。
- en: '![](../Images/6f8487ea386361ebd2eb6fe4624fb9bf.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6f8487ea386361ebd2eb6fe4624fb9bf.png)'
- en: Equation 1\. The autoregressive model — is applied to the first quantizer. Image
    taken from the original paper [1].
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 方程 1。自回归模型 — 应用于第一个量化器。图片来源于原始论文 [1]。
- en: 'In Equation 2 we see the generation of codes for quantizers 2 to 8\. Unlike
    the previous case, here the output for each quantizer is conditioned on all of
    the timesteps from the previous quantizers (when calculating codes for quantizer
    #7, the model is conditioned on data generated for quantizers 1 to 6). Unlike
    the AR model, this allows parallel generation of all timesteps in a single quantizer
    because it is only dependent on previous quantizer codes and not on previous timesteps
    of the same quantizer. The authors stressed this point because fast inference
    is especially important in text-to-speech models which need to generate speech
    in real-time scenarios.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '在方程 2 中，我们可以看到量化器 2 到 8 的编码生成过程。与之前的情况不同，这里每个量化器的输出都基于前一个量化器的所有时间步（例如，在计算量化器
    #7 的编码时，模型依赖于量化器 1 到 6 生成的数据）。与自回归模型不同，这种方式允许在单个量化器中并行生成所有时间步，因为它仅依赖于前一个量化器的编码，而不是当前量化器的前时间步。作者强调了这一点，因为在需要实时生成语音的文本转语音模型中，快速推理尤为重要。'
- en: '![](../Images/1c2cbce07941e16a31f648a795b5a8c7.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1c2cbce07941e16a31f648a795b5a8c7.png)'
- en: Equation 1\. The non-autoregressive model — is applied to the second to eight
    quantizers. Image taken from the original paper [1].
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 方程 1。非自回归模型 — 应用于第二到第八个量化器。图片来源于原始论文 [1]。
- en: Equations 1 and 2 are visually depicted in Figure 6, which shows the AR and
    NAR models together and highlights the differences between them. We see that the
    AR transformer is used to predict only C:,₁ which are the tokens for the first
    quantizer. While doing so, it attends to the previous tokens it has generated.
    the NAR transformer attends to the previous quantizers, and not to previous timesteps
    (the previous tokens of the current quantizer are not available in the NAR model).
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 方程 1 和 2 在图 6 中以视觉方式呈现，图中展示了 AR 和 NAR 模型，并突出了它们之间的区别。我们可以看到，自回归变换器仅用于预测 C:,₁，即第一个量化器的标记。在此过程中，它关注于之前生成的标记。NAR
    变换器关注于之前的量化器，而不是之前的时间步（当前量化器的前标记在 NAR 模型中不可用）。
- en: '![](../Images/0b889b99d4226ae33be110855cfa1003.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0b889b99d4226ae33be110855cfa1003.png)'
- en: Figure 6\. AR and NAR models in VALL-E. Image taken from the original paper
    [1].
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6。VALL-E 中的自回归（AR）和非自回归（NAR）模型。图片来源于原始论文 [1]。
- en: VALL-E has been trained on 60K hours of audio from the LibriLight dataset, containing
    7000 distinct speakers (which is over 100 times more data than previous state-of-the-art).
    The dataset is audio-only, hence for labeling an automatic speech recognition
    model was used. The Encodec model is used as a pre-trained model and no fine-tuning
    is performed on it for VALL-E as far as I could understand.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: VALL-E 已经在 LibriLight 数据集上的 60K 小时音频中进行了训练，数据集包含 7000 个不同的说话者（比之前的最先进技术多出 100
    倍以上的数据）。该数据集仅包含音频，因此用于标记的自动语音识别模型被使用。Encodec 模型作为预训练模型使用，VALL-E 并未对其进行微调。
- en: For training, random 10–20 second samples were taken from LibriLight. For the
    acoustic prompt, another 3 seconds were taken from the same utterance. They used
    16 Tesla V-100 GPUs to train the model, which is very modest compared to large
    SOTA language models!
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练中，从 LibriLight 中随机抽取了 10-20 秒的样本。对于声学提示，从相同的语句中提取了另外 3 秒。他们使用了 16 个 Tesla
    V-100 GPU 来训练模型，相较于大型最先进语言模型，这一配置相当简朴！
- en: We learned about the procedure and data, now let’s try to use the unofficial
    Pytorch implementation of VALL-E in GitHub.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 我们了解了流程和数据，现在让我们尝试在GitHub中使用非官方的Pytorch实现的VALL-E。
- en: Part 5 — Some Coding
  id: totrans-77
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第 5 部分 — 一些编码
- en: VALL-E doesn’t have an official implementation on GitHub, so for my experimentations,
    I will rely on the unofficial version which was released — [https://github.com/enhuiz/vall-e](https://github.com/enhuiz/vall-e).
    Moreover, no model checkpoint has been released so you have to train it from scratch.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: VALL-E在GitHub上没有官方实现，因此在我的实验中，我将依赖于已发布的非官方版本 — [https://github.com/enhuiz/vall-e](https://github.com/enhuiz/vall-e)。此外，还没有发布模型检查点，因此你必须从头开始训练。
- en: 'There is also a Google Colab notebook to follow along with a simple training
    example — [https://colab.research.google.com/drive/1wEze0kQ0gt9B3bQmmbtbSXCoCTpq5vg-?usp=sharing](https://colab.research.google.com/drive/1wEze0kQ0gt9B3bQmmbtbSXCoCTpq5vg-?usp=sharing).
    In this example, they overfit the model on a single utterance of “hello world”
    and they show that the model is able to reproduce this single utterance. I was
    interested in two things:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 还有一个Google Colab笔记本可供参考一个简单的训练示例 — [https://colab.research.google.com/drive/1wEze0kQ0gt9B3bQmmbtbSXCoCTpq5vg-?usp=sharing](https://colab.research.google.com/drive/1wEze0kQ0gt9B3bQmmbtbSXCoCTpq5vg-?usp=sharing)。在这个例子中，他们在一个“hello
    world”的单句上过拟合了模型，并展示了模型能够重现这个单句。我对两件事感兴趣：
- en: I wanted to replicate their “hello world” experiment with my own voice, just
    to see that the pipeline is working properly
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我想用自己的声音复制他们的“hello world”实验，只是为了确认流水线正常工作
- en: I wanted to replicate the experiment done by James Skelton from Paperspace —
    [https://blog.paperspace.com/training-vall-e-from-scratch-on-your-own-voice-samples/](https://blog.paperspace.com/training-vall-e-from-scratch-on-your-own-voice-samples/),
    where he trained a model on a very small subset of his own recordings, and managed
    to replicate his voice with it (on something he already recorded)
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我想复制James Skelton在Paperspace上做的实验 — [https://blog.paperspace.com/training-vall-e-from-scratch-on-your-own-voice-samples/](https://blog.paperspace.com/training-vall-e-from-scratch-on-your-own-voice-samples/)，他在自己的少量录音上训练了一个模型，并成功用它复制了他的声音（在他已经录制的东西上）
- en: Why the limited experiments? Because training this model from scratch takes
    many resources which I don’t currently have, plus I assume a pre-trained model
    will be released sooner or later.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么要进行有限的实验？因为从头开始训练这个模型需要很多资源，而我目前没有，另外我认为迟早会发布预训练模型。
- en: So how did I succeed? I managed to replicate the “hello world” experiment, but
    unfortunately, I didn’t manage to replicate the Paperspace experiment — I just
    got a model which creates a garbled sound that vaguely reminds my voice. This
    is probably because of a lack of resources (I am training it on a Google Colab
    instance which times out after 12 hours). But still, I want to go over the process
    with you. My version of the VALL-E notebook is here — [https://colab.research.google.com/drive/1NNOsvfiOfGeV-BBgGkwf0pyGAwAgx3Gi#scrollTo=SbWtNBVg_Tfd](https://colab.research.google.com/drive/1NNOsvfiOfGeV-BBgGkwf0pyGAwAgx3Gi#scrollTo=SbWtNBVg_Tfd).
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 那么我是怎么成功的呢？我设法复制了“hello world”的实验，但不幸的是，我没能复制Paperspace的实验 — 我只得到了一个产生混乱声音的模型，它模糊地提醒了我的声音。这可能是因为资源不足（我在Google
    Colab实例上进行训练，它在12小时后会超时）。但我仍然想和你一起讨论整个过程。我的VALL-E笔记本版本在这里 — [https://colab.research.google.com/drive/1NNOsvfiOfGeV-BBgGkwf0pyGAwAgx3Gi#scrollTo=SbWtNBVg_Tfd](https://colab.research.google.com/drive/1NNOsvfiOfGeV-BBgGkwf0pyGAwAgx3Gi#scrollTo=SbWtNBVg_Tfd)。
- en: Once you run the following line in the Colab notebook —
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦在Colab笔记本中运行以下行 —
- en: '[PRE0]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: You will see a directory called `vall-e` in your file browser. The path `content/vall-e/data/test`contains
    the data for the “hello world” experiment. Notice that it contains two files because
    for some reason it breaks with only one. To replicate this experiment, simply
    delete the files in the data directory using `!rm content/vall-e/data/test/*`,
    record yourself saying “Hello world”, and save it as two .wav files with different
    names. Put the .wav files in the data directory including two text files containing
    the words “hello world” (the text files should have the same names as the .wav
    files with a`.normalized.txt` suffix).
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 你会在文件浏览器中看到一个名为`vall-e`的目录。路径`content/vall-e/data/test`包含了“hello world”实验的数据。请注意，它包含两个文件，因为只有一个文件会出问题。要复制这个实验，只需使用`!rm
    content/vall-e/data/test/*`删除数据目录中的文件，录制自己说“Hello world”，并将其保存为两个带有不同名称的.wav文件。将.wav文件放入包含两个文本文件（文本文件的名称应与带有`.normalized.txt`后缀的.wav文件相同）的数据目录中。
- en: 'Following that, you will run these two cells:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 接着，你将运行这两个单元格：
- en: '[PRE1]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '[PRE2]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The first cell will run the Encodec model on your own data and perform quantization,
    just as we discussed earlier. The second cell will convert the text “hello world”
    into phonemes.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个单元将对你自己的数据运行Encodec模型并进行量化，就像我们之前讨论的那样。第二个单元将把文本“hello world”转换为音素。
- en: Afterward, the processed data is ready and you can run the cells that operate
    the training procedure. There is separate training for the NAR and AR models (remember
    that as we saw earlier, the NAR model training is dependent on the AR model, but
    the AR model uses and produces only the first quantizer data, thus independent
    of the NAR model).
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 之后，处理过的数据已经准备好，你可以运行执行训练过程的单元。NAR和AR模型有独立的训练（记住，如前所述，NAR模型训练依赖于AR模型，但AR模型仅使用和生成第一个量化器数据，因此与NAR模型独立）。
- en: '[PRE3]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '[PRE4]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'After the model has finished training you will run this cell:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 模型训练完成后，你将运行此单元：
- en: '[PRE5]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Which saves the latest model checkpoint (that has been automatically created)
    into a directory called `zoo`.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 这将最新的模型检查点（已自动创建）保存到名为`zoo`的目录中。
- en: 'Finally, you will perform inference with the model using:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，你将使用以下内容进行模型推理：
- en: '[PRE6]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'This will run the model with a text prompt of “Hello world”, and an audio prompt
    of the same utterance. It will save the generated sample as `toy.wav` which you
    can then listen to using:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 这将使用文本提示“Hello world”以及相同的语音提示来运行模型。它将生成的样本保存为`toy.wav`，然后你可以使用以下内容来收听：
- en: '[PRE7]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: And that’s it! You created your own VALL-E “Hello world”. Unless you have many
    computing resources, it is probably best to wait for a pre-trained model to come
    around to actually make use of this model further.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 就这样！你创建了自己的VALL-E“Hello world”。除非你拥有大量计算资源，否则最好等待一个预训练的模型，以进一步使用这个模型。
- en: Part 6 — Conclusions & Thoughts Ahead
  id: totrans-102
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第6部分 — 结论与未来展望
- en: In this article, we saw VALL-E, a new text-to-speech architecture by Microsoft
    Research. VALL-E generates audio in a language-model-like manner, which differentiates
    it from recent state-of-the-art methods that are usually end-to-end or follow
    a text->spectrogram->waveform creation pipeline.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我们了解了VALL-E，这是一种由微软研究院开发的新型文本到语音架构。VALL-E以类似语言模型的方式生成音频，这使其与最近的先进方法有所区别，后者通常是端到端的或遵循文本->谱图->波形创建流程。
- en: We also talked about the Encodec model, which performs audio quantization and
    is used as a pre-trained model in the training of VALL-E. Encodec is fascinating
    in itself and manages to create super-condensed audio representations using residual
    vector quantization. The creators of VALL-E leveraged this feature and built a
    generative “language” model on top of this quantization.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还讨论了Encodec模型，它执行音频量化，并在VALL-E的训练中用作预训练模型。Encodec本身很吸引人，能够使用残差向量量化创建超压缩的音频表示。VALL-E的创建者利用了这一特性，并在此量化之上构建了一个生成的“语言”模型。
- en: Finally, we saw some code and replicated the “hello world” experiment from the
    unofficial code with our own voice. The official code for this paper hasn’t been
    released, nor has a model checkpoint been released. It would be interesting to
    see and use a pre-trained model for VALL-E, which I assume will turn up sooner
    or later. Nevertheless, this was an interesting learning journey.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，我们看到了一些代码，并用自己的声音重复了来自非官方代码的“hello world”实验。该论文的官方代码尚未发布，模型检查点也未发布。看到和使用一个预训练的VALL-E模型会很有趣，我相信它迟早会出现。不过，这次是一次有趣的学习旅程。
- en: See you next time!
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 下次见！
- en: Elad
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: Elad
- en: References
  id: totrans-108
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] [https://arxiv.org/abs/2301.02111](https://arxiv.org/abs/2301.02111) —
    The VALL-E paper (Neural Codec Language Models are Zero-Shot Text to Speech Synthesizers)'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] [https://arxiv.org/abs/2301.02111](https://arxiv.org/abs/2301.02111) —
    VALL-E 论文（神经编解码语言模型是零-shot文本到语音合成器）'
- en: '[2] [https://arxiv.org/abs/2301.02111](https://arxiv.org/abs/2301.02111) —
    The Encodec paper (High Fidelity Neural Audio Compression)'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] [https://arxiv.org/abs/2301.02111](https://arxiv.org/abs/2301.02111) —
    Encodec 论文（高保真神经音频压缩）'
- en: '[3] [https://wiki.aalto.fi/display/ITSP/Concatenative+speech+synthesis](https://wiki.aalto.fi/display/ITSP/Concatenative+speech+synthesis)
    — Explanation on concatenative speech synthesis'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] [https://wiki.aalto.fi/display/ITSP/Concatenative+speech+synthesis](https://wiki.aalto.fi/display/ITSP/Concatenative+speech+synthesis)
    — 拼接语音合成的解释'
- en: '[4] [https://www.youtube.com/watch?v=aLBedWj-5CQ&t=1s](https://www.youtube.com/watch?v=aLBedWj-5CQ&t=1s)
    — Deep dive into speech synthesis meetup (HuggingFace)'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] [https://www.youtube.com/watch?v=aLBedWj-5CQ&t=1s](https://www.youtube.com/watch?v=aLBedWj-5CQ&t=1s)
    — 深入了解语音合成的聚会（HuggingFace）'
- en: '[5] [https://www.youtube.com/watch?v=MA8PCvmr8B0](https://www.youtube.com/watch?v=MA8PCvmr8B0)
    — Pushing the frontier of neural text to speech (Microsoft Research)'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '[5] [https://www.youtube.com/watch?v=MA8PCvmr8B0](https://www.youtube.com/watch?v=MA8PCvmr8B0)
    — 推动神经文本到语音的前沿（微软研究院）'
- en: '[6] [https://www.youtube.com/watch?v=G9k-2mYl6Vo&t=5593s](https://www.youtube.com/watch?v=G9k-2mYl6Vo&t=5593s)
    — Excellent video by John Tan Chong Min about VALL-E'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '[6] [https://www.youtube.com/watch?v=G9k-2mYl6Vo&t=5593s](https://www.youtube.com/watch?v=G9k-2mYl6Vo&t=5593s)
    — John Tan Chong Min 关于 VALL-E 的优秀视频'
- en: '[7] [https://www.youtube.com/watch?v=mV7bhf6b2Hs](https://www.youtube.com/watch?v=mV7bhf6b2Hs)
    — Excellent video by Aleksa Gordic about Encodec'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '[7] [https://www.youtube.com/watch?v=mV7bhf6b2Hs](https://www.youtube.com/watch?v=mV7bhf6b2Hs)
    — Aleksa Gordic 关于 Encodec 的优秀视频'
