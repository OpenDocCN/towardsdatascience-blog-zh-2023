- en: 'Inside GPT — I : Understanding the text generation'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 'Inside GPT — I : 理解文本生成'
- en: 原文：[https://towardsdatascience.com/inside-gpt-i-1e8840ca8093?source=collection_archive---------1-----------------------#2023-08-21](https://towardsdatascience.com/inside-gpt-i-1e8840ca8093?source=collection_archive---------1-----------------------#2023-08-21)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/inside-gpt-i-1e8840ca8093?source=collection_archive---------1-----------------------#2023-08-21](https://towardsdatascience.com/inside-gpt-i-1e8840ca8093?source=collection_archive---------1-----------------------#2023-08-21)
- en: A simple explanation of the model behind ChatGPT
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ChatGPT 背后模型的简单解释
- en: '[](https://medium.com/@fatih-demirci?source=post_page-----1e8840ca8093--------------------------------)[![Fatih
    Demirci](../Images/f60108429c4fac601a511f38954982bf.png)](https://medium.com/@fatih-demirci?source=post_page-----1e8840ca8093--------------------------------)[](https://towardsdatascience.com/?source=post_page-----1e8840ca8093--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----1e8840ca8093--------------------------------)
    [Fatih Demirci](https://medium.com/@fatih-demirci?source=post_page-----1e8840ca8093--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@fatih-demirci?source=post_page-----1e8840ca8093--------------------------------)[![Fatih
    Demirci](../Images/f60108429c4fac601a511f38954982bf.png)](https://medium.com/@fatih-demirci?source=post_page-----1e8840ca8093--------------------------------)[](https://towardsdatascience.com/?source=post_page-----1e8840ca8093--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----1e8840ca8093--------------------------------)
    [Fatih Demirci](https://medium.com/@fatih-demirci?source=post_page-----1e8840ca8093--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fe4aaee0b8cc3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Finside-gpt-i-1e8840ca8093&user=Fatih+Demirci&userId=e4aaee0b8cc3&source=post_page-e4aaee0b8cc3----1e8840ca8093---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----1e8840ca8093--------------------------------)
    ·11 min read·Aug 21, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F1e8840ca8093&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Finside-gpt-i-1e8840ca8093&user=Fatih+Demirci&userId=e4aaee0b8cc3&source=-----1e8840ca8093---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fe4aaee0b8cc3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Finside-gpt-i-1e8840ca8093&user=Fatih+Demirci&userId=e4aaee0b8cc3&source=post_page-e4aaee0b8cc3----1e8840ca8093---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----1e8840ca8093--------------------------------)
    ·11分钟阅读·2023年8月21日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F1e8840ca8093&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Finside-gpt-i-1e8840ca8093&user=Fatih+Demirci&userId=e4aaee0b8cc3&source=-----1e8840ca8093---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F1e8840ca8093&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Finside-gpt-i-1e8840ca8093&source=-----1e8840ca8093---------------------bookmark_footer-----------)![](../Images/96d044269a15eb2a232bf41b6912193c.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F1e8840ca8093&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Finside-gpt-i-1e8840ca8093&source=-----1e8840ca8093---------------------bookmark_footer-----------)![](../Images/96d044269a15eb2a232bf41b6912193c.png)'
- en: '*Generative AI in 19th century (created through midjourney)*'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '*19世纪的生成 AI（通过 midjourney 创建）*'
- en: Regularly engaging with colleagues across diverse domains, I enjoy the challenge
    of conveying machine learning concepts to people who have little to no background
    in data science. Here, I attempt to explain how GPT is wired in simple terms,
    only this time in written form.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 我经常与来自不同领域的同事交流，我喜欢将机器学习概念传达给那些几乎没有数据科学背景的人。在这里，我尝试用简单的术语解释 GPT 的工作原理，这一次以书面形式呈现。
- en: Behind ChatGPT’s popular magic, there is an unpopular logic. You write a prompt
    to ChatGPT and it generates text and whether it is accurate, it resembles human
    answers. How is it able to understand your prompt and generate coherent and comprehensible
    answers?
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在 ChatGPT 的流行魔力背后，隐藏着一种不那么流行的逻辑。你向 ChatGPT 输入提示，它生成文本，无论其准确性如何，都类似于人类的回答。它是如何理解你的提示并生成连贯且易于理解的回答的呢？
- en: '**Transformer Neural Networks.** The architecture designed to process unstructured
    data in vast amounts, in our case, text. When we say architecture, what we mean
    is essentially a series of mathematical operations that were made in several layers
    in parallel. Through this system of equations, several innovations were introduced
    that helped us overcome the long-existing challenges of text generation. The challenges
    that we were struggling to solve up until 5 years ago.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '**Transformer Neural Networks.** 这种架构设计用于处理大量非结构化数据，在我们的情况下是文本。当我们说架构时，我们指的是本质上在多个层并行进行的一系列数学操作。通过这种方程系统，介绍了几项创新，帮助我们克服了长时间存在的文本生成挑战。这些挑战直到5年前我们还在苦苦解决。'
- en: If GPT has already been here for 5 years (indeed GPT paper was published in
    2018), isn’t GPT old news? Why has it become immensely popular recently? What
    is the difference between GPT 1, 2, 3, 3.5 (ChatGPT ) and 4?
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 如果GPT已经存在了5年（实际上GPT论文发表于2018年），GPT难道不是旧闻吗？为什么最近它变得如此受欢迎？GPT 1、2、3、3.5（ChatGPT）和4之间有什么区别？
- en: All GPT versions were built on the same architecture. However each following
    model contained more parameters and trained using larger text datasets. There
    were obviously other novelties introduced by the later GPT releases especially
    in the training processes like reinforcement learning through human feedback which
    we will explain in the 3rd part of this blog series.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 所有GPT版本都建立在相同的架构上。然而，每个后续模型包含了更多的参数，并使用了更大的文本数据集进行训练。后来的GPT发布显然引入了其他新颖之处，尤其是在训练过程中，例如通过人类反馈的强化学习，这一点我们将在本博客系列的第三部分中进行解释。
- en: '**Vectors, matrices, tensors.** All these fancy words are essentially units
    that contain chunks of numbers. Those numbers go through a series of mathematical
    operations(mostly multiplication and summation) until we reach optimal output
    values, which are the probabilities of the possible outcomes.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '**Vectors, matrices, tensors.** 所有这些花哨的词汇本质上都是包含数字块的单位。这些数字经过一系列数学操作（主要是乘法和加法），直到我们达到最佳输出值，即可能结果的概率。'
- en: Output values? In this sense, it is the text generated by the language model,
    right? Yes. Then, what are the input values? Is it my prompt? Yes, but not entirely.
    So what else is behind?
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 输出值？从这个意义上讲，它是由语言模型生成的文本，对吗？是的。那么，输入值是什么？是我的提示吗？是的，但不是完全。那还有什么其他的呢？
- en: Before going on to the different text decoding strategies, which will be the
    topic of the following blog post, it is useful to remove the ambiguity. Let’s
    go back to fundamental question that we asked at the start. How does it understand
    human language?
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在讨论不同的文本解码策略之前，这将是下一篇博客文章的主题，消除歧义是有用的。让我们回到开始时提出的基本问题。它是如何理解人类语言的？
- en: '**Generative Pre-trained Transformers**. Three words that GPT abbreviation
    stands for. We touched the Transformer part above that it represents the architecture
    where heavy calculations are made. But what do we calculate exactly? Where do
    you even get the numbers? It is a language model and all you do is to input some
    text. How can you calculate text?'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '**Generative Pre-trained Transformers**。这是GPT缩写代表的三个词。我们在上面提到了Transformer部分，它代表了进行大量计算的架构。但我们究竟计算什么？你从哪里得到这些数字？它是一个语言模型，你只需输入一些文本。你怎么能计算文本呢？'
- en: Data is agnostic. All data is same whether in the form of text, sound or image.¹
  id: totrans-18
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 数据是不可知的。所有数据无论是以文本、声音还是图像的形式存在都是相同的。¹
- en: '**Tokens**. We split the text into small chunks (tokens) and assign an unique
    number to each one of them(token ID). Models don’t know words, images or audio
    recordings. They learn to represent them in huge series of numbers (parameters)
    that serves us as a tool to illustrate the characteristics of things in numerical
    forms. Tokens are the language units that convey meaning and token IDs are the
    unique numbers that encode tokens.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '**Tokens**。我们将文本拆分成小块（标记），并为每个标记分配一个唯一的编号（标记ID）。模型不知道单词、图像或音频记录。它们学习以巨大的数字序列（参数）来表示它们，这些数字序列作为工具来以数字形式展示事物的特征。标记是传达意义的语言单位，而标记ID是编码标记的唯一数字。'
- en: Obviously, how we tokenise the language can vary. Tokenisation can involve splitting
    texts into sentences, words, parts of words(sub-words), or even individual characters.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，我们标记化语言的方式可能会有所不同。标记化可以包括将文本拆分成句子、单词、单词部分（子词）或甚至单个字符。
- en: Let’s consider a scenario where we have 50,000 tokens in our language corpus(similar
    to GPT-2 which has 50,257). How do we represent those units after tokenisation?
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑一个场景，其中我们的语言语料库有50,000个标记（类似于GPT-2的50,257）。在标记化后，我们如何表示这些单位？
- en: '[PRE0]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Above is an example sentence tokenised into words. Tokenisation approaches can
    differ in their implementation. What’s important for us to understand right now
    is that we acquire numerical representations of language units(tokens) through
    their corresponding token IDs. So, now that we have these token IDs, can we simply
    input them directly into the model where calculations take place?
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 上面是一个被分词的例句示例。分词方法的实现可能有所不同。我们现在需要理解的重要一点是，我们通过其对应的令牌 ID 获得语言单位（令牌）的数值表示。因此，现在我们有了这些令牌
    ID，能否将它们直接输入到模型中进行计算？
- en: Cardinality matters in math. 101 and 2493 as token representation will matter
    to model. Because remember, all we are doing is mainly multiplications and summations
    of big chunks of numbers. So multiplying a number with either with 101 or with
    2493 will matter. Then, how can we make sure a token that is represented with
    number 101 is not less important than 2493, just because we happen to tokenise
    it arbitrarily so? How can we encode the words without causing a fictitious ordering?
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 基数在数学中很重要。101 和 2493 作为令牌表示会对模型产生影响。因为请记住，我们所做的主要是大块数字的乘法和加法。所以用 101 或 2493
    乘一个数字是有区别的。那么，我们如何确保用数字 101 表示的令牌不比 2493 不重要，仅仅因为我们偶然选择了它？我们如何编码单词而不造成虚假的排序？
- en: '**One-hot encoding.** Sparse mapping of tokens. One-hot encoding is the technique
    where we project each token as a binary vector. That means only one single element
    in the vector is 1 (“hot”) and the rest is 0 (“cold”).'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '**独热编码。** 令牌的稀疏映射。独热编码是一种将每个令牌投影为二进制向量的技术。这意味着向量中只有一个元素为 1（“热”），其余为 0（“冷”）。'
- en: '![](../Images/ed15944a808e67ecb32c4eb54ed5764e.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ed15944a808e67ecb32c4eb54ed5764e.png)'
- en: 'Image by the author: one-hot encoding vector example'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供：独热编码向量示例
- en: The tokens are represented with a vector which has length of total token in
    our corpus. In simpler terms, if we have 50k tokens in our language, every token
    is represented by a vector 50k in which only one element is 1 and the rest is
    0\. Since every vector in this projection contains only one non-zero element,
    it is named as sparse representation. However, as you might think this approach
    is very inefficient. Yes, we manage to remove the artificial cardinality between
    the token ids but we can’t extrapolate any information about the semantics of
    the words. We can’t understand whether the word “party” refers to a celebration
    or to a political organisation by using sparse vectors. Besides, representing
    every token with a vector of size 50k will mean, in total of 50k vector of length
    50k. This is very inefficient in terms of required memory and computation. Fortunately
    we have better solutions.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 令牌由一个向量表示，该向量的长度等于我们语料库中的令牌总数。简单来说，如果我们语言中有 50k 个令牌，每个令牌由一个长度为 50k 的向量表示，其中只有一个元素为
    1，其余为 0。由于这个投影中的每个向量只包含一个非零元素，因此它被称为稀疏表示。然而，正如你可能想到的，这种方法非常低效。是的，我们设法消除了令牌 ID
    之间的人工基数，但我们无法推断出单词的语义信息。我们无法通过使用稀疏向量来理解单词“party”是指庆祝活动还是政治组织。此外，用大小为 50k 的向量表示每个令牌，将意味着总共有
    50k 个长度为 50k 的向量。这在所需的内存和计算方面非常低效。幸运的是，我们有更好的解决方案。
- en: '**Embeddings**. Dense representation of tokens. Tokenised units pass through
    an embedding layer where each token is transformed into continuous vector representation
    of a fixed size. For example in the case of GPT 3, each token in is represented
    by a vector of 768 numbers. These numbers are assigned randomly which then are
    being learned by the model after seeing lots of data(training).'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '**嵌入**。令牌的密集表示。分词单位通过一个嵌入层，其中每个令牌被转换为固定大小的连续向量表示。例如，在 GPT-3 的情况下，每个令牌由一个包含
    768 个数字的向量表示。这些数字最初是随机分配的，然后在模型看到大量数据（训练）后进行学习。'
- en: '[PRE1]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Above is the embedding vector example of the word “party”.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 上面是单词“party”的嵌入向量示例。
- en: Now we have 50,000x786 size of vectors which is compare to 50,000x50,000 one-hot
    encoding is significantly more efficient.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有 50,000x786 大小的向量，与 50,000x50,000 的独热编码相比，显著更高效。
- en: Embedding vectors will be the inputs to the model. Thanks to dense numerical
    representations we will able to capture the semantics of words, the embedding
    vectors of tokens that are similar will be closer to each other.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 嵌入向量将作为模型的输入。由于密集的数值表示，我们将能够捕捉单词的语义，相似的令牌的嵌入向量将彼此更接近。
- en: How can you measure the similarity of two language unit in context? There are
    several functions that can measure the similarity between the two vectors of same
    size. Let’s explain it with an example.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 如何在上下文中测量两个语言单位的相似性？有几种函数可以测量两个相同大小的向量之间的相似性。让我们用一个例子来解释。
- en: Consider a simple example where we have the embedding vectors of tokens “cat”
    , “dog”, “car” and “banana”. For simplification let’s use an embedding size of
    4\. That means there will be four learned numbers to represent the each token.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一个简单的例子，我们有“cat”、“dog”、“car”和“banana”的嵌入向量。为了简化，假设嵌入维度为4。这意味着每个词语会由四个学习到的数字表示。
- en: '[PRE2]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Using the vectors above lets calculate the similarity scores using the cosine
    similarity. Human logic would find the word “dog” and “cat” more similar to each
    other than the words banana a car. Can we expect math to simulate our logic?
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 使用上述向量，我们可以通过余弦相似度来计算相似性得分。人类逻辑会发现“dog”和“cat”彼此更相似，而“banana”和“car”相似度较低。我们可以期待数学模拟我们的逻辑吗？
- en: '[PRE3]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '[PRE4]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: We can see that the words “cat” and “dog” have very high similarity score whereas
    the words “car” and “banana” have very low. Now imagine embedding vectors of length
    768 instead of 4 for each 50000 token in our language corpus. That’s how we are
    able find the words that are related to each other.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到“cat”和“dog”的相似度得分非常高，而“car”和“banana”的得分非常低。现在，想象一下在我们的语言语料库中，每个50000个词的嵌入向量长度为768。这就是我们能够找到彼此相关的词语的方法。
- en: Now, let’s have a look at the two sentences below which have higher semantic
    complexity.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看下面这两个语义复杂度较高的句子。
- en: '[PRE5]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The word “party” from the first and second sentence conveys different meanings.
    How are large language models capable of mapping out the difference between the
    “party” as a political organisation and “party” as celebrating social event?
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 第一和第二句中的“party”一词传达了不同的意义。大型语言模型如何区分“party”作为政治组织与“party”作为庆祝社交活动的意义？
- en: Can we distinguish the different meanings of same token by relying on the token
    embeddings? The truth is, although embeddings provide us a range of advantages,
    they are not adequate to disentangle the entire complexity of semantic challenges
    of human language.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 我们能否通过依赖词语嵌入来区分相同词语的不同含义？事实是，尽管嵌入提供了很多优势，但它们不足以解开人类语言语义挑战的复杂性。
- en: '**Self-attention.** The solution was again offered by transformer neural networks.
    We generate new set of weights(another name for parameters) that are namely query,
    key and value matrices. Those weights learn to represent the embedding vectors
    of tokens as a new set of embeddings. How? Simply by taking the weighted average
    of the original embeddings. Each token “attends” to every other token(including
    to itself) in the input sentence and calculates set of attention weights or in
    other word the new so called “*contextual embeddings*”.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '**自注意力。** 解决方案再次由变压器神经网络提供。我们生成一组新的权重（参数的另一名称），即查询、键和值矩阵。这些权重学习将词语的嵌入向量表示为一组新的嵌入。如何做到？简单地通过对原始嵌入的加权平均。每个词语“关注”输入句子中的其他每个词（包括它自身）并计算一组注意力权重，换句话说，就是新的所谓“*上下文嵌入*”。'
- en: All it does really is to map the importance of the words in the input sentence
    by assigning new set of numbers(attention weights) that are calculated using the
    token embeddings.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 它真正做的只是通过分配一组新的数字（注意力权重）来映射输入句子中词语的重要性，这些数字是使用词语嵌入计算得出的。
- en: '![](../Images/ed88f41fed77de49bb001eb5825d92c6.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ed88f41fed77de49bb001eb5825d92c6.png)'
- en: 'Image by the author: Attention weights of a token in different contexts (BertViz
    attention-head view)'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片：不同上下文中词语的注意力权重（BertViz注意力头视图）
- en: Above visualisation demonstrates the “attention” of the token “party” to the
    rest of the tokens in two sentences. The boldness of the connection signals to
    the importance or the relevance of the tokens. Attention and “attending” are the
    terms that refer to a new series of numbers(attention parameters) and their magnitude,
    that we use to represent the importance of words numerically. In the first sentence
    the word “party” attends to the word “celebrate” the most, whereas in the second
    sentence the word “deputy” has the highest attention. That’s how the model is
    able to incorporate the context by examining surrounding words.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 上面的可视化展示了“party”对两个句子中其余标记的“注意力”。连接的粗细表示标记的重要性或相关性。注意力和“attending”是指一系列新数字（注意力参数）及其大小，我们用它们来表示单词的重要性。在第一个句子中，“party”对“celebrate”的关注最高，而在第二个句子中，“deputy”具有最高的注意力。这就是模型如何通过检查周围单词来融入上下文。
- en: 'As we mentioned in the attention mechanism we derive new set of weight matrices,
    namely: Query, Key and Value (simply q,k,v). They are cascading matrices of same
    size(usually smaller than the embedding vectors) that are introduced to the architecture
    to capture complexity in the language units. Attention parameters are learned
    in order to demystify the relationship between the words, pairs of words, pairs
    of pairs of words and pairs of pairs of pairs of words and so on. Below is the
    visualisation of the query, key and value matrices in finding the most relevant
    word.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在注意力机制中提到的那样，我们推导出了一组新的权重矩阵，即：查询、键和值（简称q、k、v）。它们是相同大小的级联矩阵（通常小于嵌入向量），它们被引入架构中以捕捉语言单元的复杂性。注意力参数通过学习以揭示单词、单词对、单词对对、单词对对对的关系等等。以下是查询、键和值矩阵在找到最相关单词中的可视化。
- en: '![](../Images/a5f2be5cf83c1a11c0fa4bb2dc718135.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a5f2be5cf83c1a11c0fa4bb2dc718135.png)'
- en: 'Image by the author: illustration of query key and value matrices and their
    final probabilities(BertViz q,k,v view)'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图像：查询键和值矩阵及其最终概率的插图（BertViz q,k,v视图）
- en: The visualisation illustrates the q and k vectors as vertical bands, where the
    boldness of each band reflects its magnitude. The connections between tokens signify
    the weights determined by attention, indicating that the q vector for “party”
    aligns most significantly with the k vector for “is”, “deputy” and “respected”.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 可视化图展示了q和k向量作为垂直带，其中每个带的粗细反映了其大小。标记之间的连接表示由注意力决定的权重，表明“party”的q向量与“is”、“deputy”和“respected”的k向量最显著对齐。
- en: To make the attention mechanism and the concepts of q, k and v less abstract,
    imagine that you went to a party and heard an amazing song that you fell in love
    with. After the party you are dying to find the song and listen again but you
    only remember barely 5 words from the lyrics and a part of the song melody(query).
    To find the song, you decide to go through the party playlist(keys) and listen(similarity
    function) all the songs in the list that was played at the party. When you finally
    recognise the song, you note the name of the song(value).
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使注意力机制以及q、k和v的概念不那么抽象，想象一下你去参加一个派对，听到了一首你爱上的绝妙歌曲。派对结束后，你急切地想找到这首歌再听一遍，但你只记得歌词中的5个词和一部分歌曲旋律（query）。为了找到这首歌，你决定浏览派对播放列表（keys），并逐一聆听列表中播放的所有歌曲（相似度函数）。当你最终认出这首歌时，你记录下歌曲的名称（value）。
- en: One last important trick that transformers introduced is to add the positional
    encodings to the vector embeddings. Simply because we would like to capture the
    position information of the word. It enhances our chances to predict the next
    token more accurately towards to the true sentence context. It is essential information
    because often swapping the words changes the context entirely. For instance, the
    sentences *“Tim chased clouds all his life”* vs *“clouds chased Tim all his life”*
    are absolutely different in essence.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一个重要的技巧是变压器引入了将位置编码添加到向量嵌入中。仅仅是因为我们希望捕捉单词的位置信息。它增强了我们准确预测下一个标记的机会，使其更接近真实的句子上下文。这是至关重要的信息，因为词语的交换往往会完全改变上下文。例如，句子*“Tim
    chased clouds all his life”*与*“clouds chased Tim all his life”*在本质上是完全不同的。
- en: All the mathematical tricks that we explored at a basic level so far, have the
    objective of predicting the next token, given the sequence of input tokens. Indeed,
    GPT is trained on one simple task which is the text generation or in other words
    the next token prediction. At its core of the matter, we measure the probability
    of a token, given the sequence of tokens appeared before it.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '-   到目前为止，我们探索的所有基本数学技巧，其目标是预测下一个标记，给定输入标记的序列。确实，GPT的训练任务非常简单，即文本生成，或者换句话说是下一个标记预测。从本质上讲，我们测量给定之前出现的标记序列的标记概率。'
- en: You might wonder how do models learn the optimal numbers from randomly assigned
    numbers. It is a topic for another blog post probably however that is actually
    fundamental on understanding. Besides, it is a great sign that you are already
    questioning the basics. To remove unclarity, we use an optimisation algorithm
    that adjusts the parameters based on a metric that is called loss function. This
    metric is calculated by comparing the predicted values with the actual values.
    The model tracks the changes of the metric and depending on how small or large
    the value of loss, it tunes the numbers. This process is done until the loss can
    not be smaller given the rules we set in the algorithm that we call hyperparameters.
    An example hyperparameter can be, how frequently we want to calculate the loss
    and tune the weights. This is the rudimentary idea behind learning.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '-   你可能会好奇模型是如何从随机分配的数字中学习最优数字的。这可能是另一个博客文章的话题，但实际上这是理解的基础。此外，你已经开始质疑基础知识，这本身就是一个很好的迹象。为了消除不明确性，我们使用一种优化算法，该算法基于称为损失函数的指标来调整参数。这个指标是通过将预测值与实际值进行比较来计算的。模型跟踪指标的变化，并根据损失值的大小来调整数字。这个过程会一直进行，直到损失值在我们在算法中设置的超参数规则下无法再减小为止。一个示例超参数可以是，我们希望多频繁地计算损失并调整权重。这是学习背后的基本思想。'
- en: I hope in this short post, I was able to clear the picture at least a little
    bit. The second part of this blog series will focus on decoding strategies namely
    on why your prompt matters. The third and the last part will be dedicated to key
    factor on ChatGPT’s success which is the reinforcement learning through human
    feedback. Many thanks for the read. Until next time.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '-   我希望在这篇简短的文章中，我能至少稍微清晰一下。该博客系列的第二部分将重点关注解码策略，即为什么你的提示很重要。第三部分也是最后一部分将专注于ChatGPT成功的关键因素，即通过人工反馈的强化学习。非常感谢您的阅读。下次见。'
- en: '**References:**'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '**参考文献：**'
- en: A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser,
    and I. Polosukhin, “Attention Is All You Need,” in Advances in Neural Information
    Processing Systems 30 (NIPS 2017), 2017.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser,
    和 I. Polosukhin, “注意力机制就是你所需要的，”发表于《神经信息处理系统进展》第30卷（NIPS 2017），2017年。
- en: 'J. Vig, “A Multiscale Visualization of Attention in the Transformer Model,”
    In Proceedings of the 57th Annual Meeting of the Association for Computational
    Linguistics: System Demonstrations, pp. 37–42, Florence, Italy, Association for
    Computational Linguistics, 2019.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: J. Vig, “变压器模型中的多尺度注意力可视化，”发表于第57届计算语言学协会年会：系统演示，页码37–42，意大利佛罗伦萨，计算语言学协会，2019年。
- en: 'L. Tunstall, L. von Werra, and T. Wolf, “Natural Language Processing with Transformers,
    Revised Edition,” O’Reilly Media, Inc., Released May 2022, ISBN: 9781098136796.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 'L. Tunstall, L. von Werra, 和 T. Wolf, “使用变压器的自然语言处理，修订版，” O’Reilly Media, Inc.,
    2022年5月发布，ISBN: 9781098136796。'
- en: '[1 -Lazy Programmer Blog](https://lazyprogrammer.me/blog/)'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '[1 -懒人程序员博客](https://lazyprogrammer.me/blog/)'
