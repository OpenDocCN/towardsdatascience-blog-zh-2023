- en: 'How to Solve the Protein Folding Problem: AlphaFold2'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何解决蛋白质折叠问题：AlphaFold2
- en: 原文：[https://towardsdatascience.com/how-to-solve-the-protein-folding-problem-alphafold2-6c81faba670d](https://towardsdatascience.com/how-to-solve-the-protein-folding-problem-alphafold2-6c81faba670d)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/how-to-solve-the-protein-folding-problem-alphafold2-6c81faba670d](https://towardsdatascience.com/how-to-solve-the-protein-folding-problem-alphafold2-6c81faba670d)
- en: A deeper look at AlphaFold2 and its neural architecture
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 更深入地了解 AlphaFold2 及其神经网络结构
- en: '[](https://universvm.medium.com/?source=post_page-----6c81faba670d--------------------------------)[![Leonardo
    Castorina](../Images/74ba4ff34c619576da18dbda354f4982.png)](https://universvm.medium.com/?source=post_page-----6c81faba670d--------------------------------)[](https://towardsdatascience.com/?source=post_page-----6c81faba670d--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----6c81faba670d--------------------------------)
    [Leonardo Castorina](https://universvm.medium.com/?source=post_page-----6c81faba670d--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://universvm.medium.com/?source=post_page-----6c81faba670d--------------------------------)[![Leonardo
    Castorina](../Images/74ba4ff34c619576da18dbda354f4982.png)](https://universvm.medium.com/?source=post_page-----6c81faba670d--------------------------------)[](https://towardsdatascience.com/?source=post_page-----6c81faba670d--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----6c81faba670d--------------------------------)
    [Leonardo Castorina](https://universvm.medium.com/?source=post_page-----6c81faba670d--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----6c81faba670d--------------------------------)
    ·16 min read·Mar 11, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----6c81faba670d--------------------------------)
    ·阅读时间 16 分钟·2023年3月11日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/572e0dac09bea160681c694c2b4fb515.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/572e0dac09bea160681c694c2b4fb515.png)'
- en: Illustration of protein sequence to shape. In white the original protein, in
    rainbow the AlphaFold predicted protein [blue=highest confidence, red=lowest confidence]
    (Illustration by the author)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 蛋白质序列到形状的示意图。原始蛋白质用白色表示，AlphaFold 预测的蛋白质用彩虹色表示 [蓝色=最高信心，红色=最低信心]（图示作者提供）
- en: In this series of articles, I will go through protein folding and deep learning
    models such as AlphaFold, OmegaFold, and ESMFold. We will start with AlphaFold2!
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一系列文章中，我将讨论蛋白质折叠及深度学习模型，如 AlphaFold、OmegaFold 和 ESMFold。我们将从 AlphaFold2 开始！
- en: Introduction
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: Proteins are molecules that perform most of the biochemical functions in living
    organisms. They are involved in digestion (enzymes), structural processes (keratin
    — skin), photosynthesis and are also used extensively in the pharmaceutical industry
    [2].
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 蛋白质是执行生物体大多数生化功能的分子。它们参与消化（酶）、结构过程（角蛋白 — 皮肤）、光合作用，并在制药行业中也被广泛使用 [2]。
- en: The 3D structure of the protein is fundamental to its function. Proteins are
    made up of 20 subunits called amino acids (or residues), each with different properties
    such as charge, polarity, length, and the number of atoms. Amino acids are formed
    by a **backbone**, common to all amino acids, and a **side-chain**, unique to
    each amino acid. They are connected by a peptide bond [2].
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 蛋白质的 3D 结构对其功能至关重要。蛋白质由 20 种称为氨基酸（或残基）的子单位组成，每种氨基酸具有不同的特性，如电荷、极性、长度和原子数。氨基酸由所有氨基酸共有的**主链**和每种氨基酸特有的**侧链**组成。它们通过肽键相连
    [2]。
- en: '![](../Images/bba2585b3f0247659bbdd5ef71269b05.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bba2585b3f0247659bbdd5ef71269b05.png)'
- en: Illustration of a protein polypeptide with four residues. (Illustration by the
    author)
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 四个残基的蛋白质多肽示意图。（图示作者提供）
- en: '![](../Images/7a58223094f9764d6b7763d827e2ef4e.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7a58223094f9764d6b7763d827e2ef4e.png)'
- en: Illustration of a protein backbone with four residues. (Illustration by the
    author)
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 四个残基的蛋白质主链示意图。（图示作者提供）
- en: Protein contain residues oriented at specific torsion angles called φ and ψ,
    which give rise to a protein 3D shape.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 蛋白质包含在特定扭转角度 φ 和 ψ 定向的残基，这些角度决定了蛋白质的 3D 结构。
- en: '![](../Images/73f8bf2634b5620c888fa2894b3c79dd.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/73f8bf2634b5620c888fa2894b3c79dd.png)'
- en: Illustration of φ and ψ angles. (Illustration by the author)
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: φ 和 ψ 角度的示意图。（图示作者提供）
- en: The main problem every biologist faces is obtaining this 3D shape of proteins,
    usually requires a crystal of the protein and X-Ray Crystallography. Proteins
    have various properties, for example, membrane proteins tend to be hydrophobic
    meaning it is hard to identify the conditions at which it crystallizes [2]. Obtaining
    crystals is therefore a tedious and (arguably) highly random process takes days
    to years to decades and it can be regarded as more of an art than a science. This
    means that many biologists may spend the entire duration of their Ph.D. trying
    to crystallise a protein.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 每个生物学家面临的主要问题是获得蛋白质的三维形状，这通常需要蛋白质的晶体和X射线晶体学。蛋白质具有各种属性，例如膜蛋白往往是疏水性的，这意味着很难确定其结晶的条件
    [2]。因此，获得晶体是一个繁琐且（可以说）高度随机的过程，需要几天到几十年不等，这可以被视为一种艺术而非科学。这意味着许多生物学家可能会在整个博士生期间致力于结晶一个蛋白质。
- en: 'If you are lucky enough to get a crystal of your protein, you can upload it
    to the Protein Data Bank, a large dataset of proteins:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你有幸获得了蛋白质的晶体，你可以将其上传到蛋白质数据银行，这是一个大型的蛋白质数据集：
- en: '[](https://www.rcsb.org/?source=post_page-----6c81faba670d--------------------------------)
    [## RCSB PDB: Homepage'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '[## RCSB PDB: 主页](https://www.rcsb.org/?source=post_page-----6c81faba670d--------------------------------)'
- en: As a member of the wwPDB, the RCSB PDB curates and annotates PDB data according
    to agreed upon standards. The RCSB PDB…
  id: totrans-22
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 作为wwPDB的成员，RCSB PDB根据约定的标准策划和注释PDB数据。RCSB PDB…
- en: www.rcsb.org](https://www.rcsb.org/?source=post_page-----6c81faba670d--------------------------------)
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '[www.rcsb.org](https://www.rcsb.org/?source=post_page-----6c81faba670d--------------------------------)'
- en: 'This begs the question: can we simulate folding to obtain a 3D structure from
    a sequence? Short answer: Yes, kind of. Long answer: We can use molecular simulations
    to try to fold proteins which are often heavy in computational use. Hence, projects
    like Folding@Home try to distribute the problem over many computers to obtain
    a dynamics simulation of a protein.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 这引出了一个问题：我们能否模拟折叠以从序列中获得三维结构？简短回答：可以，有点。长答案：我们可以使用分子模拟来尝试折叠蛋白质，这通常计算量很大。因此，像Folding@Home这样的项目试图将问题分布到许多计算机上，以获得蛋白质的动态模拟。
- en: Now, a competition, Critical Assessment of Protein Structure Prediction (CASP)
    was made where some 3D structures of proteins would be holdout so that people
    could test their protein folding models. In 2020, DeepMind participated with AlphaFold2
    beating the state-of-the-art and obtaining outstanding performances.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，有一个名为蛋白质结构预测关键评估（CASP）的竞赛，其中一些蛋白质的三维结构被保留，以便人们可以测试他们的蛋白质折叠模型。在2020年，DeepMind参与了AlphaFold2，超越了最先进的技术，取得了杰出的表现。
- en: '![](../Images/1fb7e39f80c01653aa451b99ad04abb6.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1fb7e39f80c01653aa451b99ad04abb6.png)'
- en: Median Global Distance Test for the CASP competition from 2008–2020\. A solution
    of about 90 is considered roughly equivalent to the crystal structure. AlphaFold
    2 outperformed all the previous models achieving state-of-the-art performance
    (Illustration by the author, based on [4]).
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 2008年至2020年CASP竞赛中的中位全球距离测试。大约90的解被认为与晶体结构大致相当。AlphaFold 2超越了所有先前的模型，达到了最先进的性能（作者插图，基于
    [4]）。
- en: In this blog post, I will go over AlphaFold2, explain its inner workings, and
    conclude how it has revolutionized my work as a Ph.D. student on Protein Design
    and Machine Learning.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇博客文章中，我将介绍AlphaFold2，解释其内部工作原理，并总结它如何彻底改变了我作为蛋白质设计与机器学习博士生的工作。
- en: Before we start, I would like to give a shoutout to [OpenFold](https://github.com/aqlaboratory/openfold)
    by the AQ Laboratory, an open-source implementation of AlphaFold that includes
    training code through which I double-checked the dimensions of tensors I refer
    to in this article. Most of this article’s information is in the [Supplementary
    of the original paper](https://www.nature.com/articles/s41586-021-03819-2).
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始之前，我想给AQ Laboratory的[OpenFold](https://github.com/aqlaboratory/openfold)一个特别的感谢，这是一个开源的AlphaFold实现，包括训练代码，通过这些代码我双重检查了本文中提到的张量的维度。本文的大部分信息都在[原始论文的补充材料](https://www.nature.com/articles/s41586-021-03819-2)中。
- en: 0\. Overview
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 0\. 概述
- en: 'Let’s begin with an overview. This is what the overall structure of the model
    looks like:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从概述开始。这是模型的整体结构：
- en: '![](../Images/beeef945ce441d24c166bbf5078212dc.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/beeef945ce441d24c166bbf5078212dc.png)'
- en: Overview of the AlphaFold Architecture [1]
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: AlphaFold架构概述 [1]
- en: 'Typically, you start with a sequence of amino acids of your protein of interest.
    Note that a crystal is ***not*** necessary to obtain the sequence of amino acid
    : this is usually obtained from DNA sequencing (if you know the gene of the protein)
    or Protein Sequencing. The proteins can be broken to smaller -mers and analysed
    in mass spectrometry for example.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，你会从目标蛋白质的氨基酸序列开始。请注意，获得氨基酸序列***不***需要晶体：通常可以通过DNA测序（如果你知道蛋白质的基因）或蛋白质测序获得。蛋白质可以被切割成较小的-mers，并通过质谱等方法进行分析。
- en: The aim is to prepare two key pieces of data the **Multiple Sequence Alignment
    (MSA) representation** and a **pair representation**. For simplicity, I will skip
    the use of templates.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 目标是准备两个关键数据：**多序列比对（MSA）表示**和**对偶表示**。为了简化，我将跳过模板的使用。
- en: The **MSA representation** is obtained by looking for similar sequences in genetic
    databases. As the picture shows, the sequence may also come from different organisms,
    e.g., a fish. Here we are trying to get general information about each index position
    of the protein and understand, in the context of evolution, how the protein has
    changed in different organisms. Proteins like Rubisco (involved in photosynthesis)
    are generally highly conserved and therefore have little differences in plants.
    Others, like the spike protein of a virus, are very variable.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '**MSA表示**是通过查找遗传数据库中的相似序列获得的。如图所示，序列也可能来自不同的生物体，例如鱼类。在这里，我们试图获取有关蛋白质每个索引位置的一般信息，并理解在进化的背景下，蛋白质在不同生物体中的变化情况。像Rubisco（参与光合作用）的蛋白质通常高度保守，因此植物中的差异很小。其他蛋白质，如病毒的刺突蛋白，则变化很大。'
- en: In the **pair representation**, we are trying to infer relationships between
    the sequence elements. For example, position 54 of the protein may interact with
    position 1.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在**对偶表示**中，我们尝试推断序列元素之间的关系。例如，蛋白质的位置54可能与位置1相互作用。
- en: Throughout the network, these representations are updated several times. First,
    they are embedded to create a representation of the data. Then they pass through
    the EvoFormer, which extracts information about sequences and pairs, and finally,
    a structure model which builds the 3D structure of the protein.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在整个网络中，这些表示会多次更新。首先，它们被嵌入以创建数据的表示。然后它们通过EvoFormer，该模块提取序列和对的相关信息，最后通过结构模型构建蛋白质的3D结构。
- en: 1\. Input Embedder
  id: totrans-39
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 1\. 输入嵌入器
- en: 'Algorithm: 3'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 算法：3
- en: 'OpenFold: [https://github.com/aqlaboratory/openfold/blob/c2f46ce86367689864eac6a954dd9204b2576d3b/openfold/model/embedders.py#L24](https://github.com/aqlaboratory/openfold/blob/c2f46ce86367689864eac6a954dd9204b2576d3b/openfold/model/embedders.py#L24)'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'OpenFold: [https://github.com/aqlaboratory/openfold/blob/c2f46ce86367689864eac6a954dd9204b2576d3b/openfold/model/embedders.py#L24](https://github.com/aqlaboratory/openfold/blob/c2f46ce86367689864eac6a954dd9204b2576d3b/openfold/model/embedders.py#L24)'
- en: 'The input embedder attempts to create a different representation of the data.
    For MSA data, AlphaFold uses an arbitrary cluster number rather than the full
    MSA to reduce the number of possible sequences that go through the transformer,
    thus decreasing computation. The MSA data input *msa_feat* **(N_clust, N_res,
    49)** is composed by:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 输入嵌入器尝试创建数据的不同表示。对于MSA数据，AlphaFold使用一个任意的簇编号而不是完整的MSA，以减少经过变换器的可能序列数量，从而减少计算。MSA数据输入*msa_feat*
    **(N_clust, N_res, 49)**由以下部分组成：
- en: '*cluster_msa* **(N_clust, N_res, 23)**: a one-hot encoding of the MSA cluster
    center sequences (20 amino acids + 1 unknown + 1 gap + 1 *masked_msa_token*)'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*cluster_msa* **(N_clust, N_res, 23)**：MSA簇中心序列的独热编码（20种氨基酸 + 1个未知 + 1个缺口 +
    1个*masked_msa_token*）'
- en: '*cluster_profile* **(N_clust, N_res, 23)**: amino acid type distribution for
    each residue in the MSA (20 amino acids + 1 unknown + 1 gap + 1 *masked_msa_token*)'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*cluster_profile* **(N_clust, N_res, 23)**：MSA中每个残基的氨基酸类型分布（20种氨基酸 + 1个未知 +
    1个缺口 + 1个*masked_msa_token*）'
- en: '*cluster_deletion_mean* **(N_clust, N_res, 1)**: average deletions of every
    residue in every cluster (ranges 0–1)'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*cluster_deletion_mean* **(N_clust, N_res, 1)**：每个簇中每个残基的平均删除数量（范围0–1）'
- en: '*cluster_deletion_value* **(N_clust, N_res, 1)**: number of deletions in the
    MSA (ranges 0–1)'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*cluster_deletion_value* **(N_clust, N_res, 1)**：MSA中的删除数量（范围0–1）'
- en: '*cluster_has_deletion* **(N_clust, N_res, 1)**: binary feature indicating whether
    there are deletions'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*cluster_has_deletion* **(N_clust, N_res, 1)**：指示是否存在删除的二进制特征'
- en: For pair representations, it encodes each amino acid with a unique index in
    the sequence with RelPos, which accounts for distance in the sequence. This is
    represented as a distance matrix of each residue against each other, and the distances
    clipped to 32, meaning larger distances are capped to 0, meaning the dimension
    is effectively -32 to 32 + 1 = **65.**
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 对于对偶表示，它使用唯一索引对每个氨基酸进行编码，并使用 RelPos 计算序列中的距离。这被表示为每个残基之间的距离矩阵，距离限制为 32，这意味着较大的距离被限制为
    0，因此维度有效为 -32 到 32 + 1 = **65**。
- en: Both the MSA representation and the pair representations go through several
    independent linear layers and are passed to the EvoFormer.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: MSA 表示和对偶表示都经过几个独立的线性层，然后传递给 EvoFormer。
- en: 2\. Evoformer
  id: totrans-50
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 2\. Evoformer
- en: '![](../Images/028293a460f896b0dd328efb9359a97d.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/028293a460f896b0dd328efb9359a97d.png)'
- en: Architecture of the EvoFormer [1]
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: EvoFormer 的架构 [1]
- en: 'Algorithm: 6'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 算法：6
- en: 'OpenFold: [https://github.com/aqlaboratory/openfold/blob/b2d6bff691e0ad860d585ea6709b81b1aba57d2c/openfold/model/evoformer.py#L575](https://github.com/aqlaboratory/openfold/blob/b2d6bff691e0ad860d585ea6709b81b1aba57d2c/openfold/model/evoformer.py#L575)'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'OpenFold: [https://github.com/aqlaboratory/openfold/blob/b2d6bff691e0ad860d585ea6709b81b1aba57d2c/openfold/model/evoformer.py#L575](https://github.com/aqlaboratory/openfold/blob/b2d6bff691e0ad860d585ea6709b81b1aba57d2c/openfold/model/evoformer.py#L575)'
- en: There are then 48 blocks of the EvoFormer, which uses self-attention to allow
    the MSA and Pairs representations to communicate. We first look at the MSA to
    then merge it into the pairs.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 然后有 48 个 EvoFormer 块，使用自注意力允许 MSA 和对偶表示进行通信。我们首先查看 MSA，然后将其合并到对偶中。
- en: 2.1 MSA Stack
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.1 MSA 堆叠
- en: '![](../Images/e6f3d06982a042b813eb04debd64b5a6.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e6f3d06982a042b813eb04debd64b5a6.png)'
- en: MSA Stack of EvoFormer. [1] (edited by author)
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: EvoFormer 的 MSA 堆叠。 [1]（作者编辑）
- en: This is composed of **row-wise gated self-attention with pair bias**, **column-wise
    gated self-attention**, **transition** and **outer product mean** blocks.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 这由 **行级门控自注意力与对偶偏置**、**列级门控自注意力**、**转换** 和 **外积均值** 块组成。
- en: '**2.1A Row-Wise Gated Self-Attention with Pair Bias**'
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**2.1A 行级门控自注意力与对偶偏置**'
- en: '![](../Images/1caf06795f2a0f1c96cd37f9add06450.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1caf06795f2a0f1c96cd37f9add06450.png)'
- en: Row-Wise Gated Self-Attention with Pair Bias in Evoformer. [1]
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: Evoformer 中的行级门控自注意力与对偶偏置。 [1]
- en: 'Algorithm: 7'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 算法：7
- en: 'OpenFold: [https://github.com/aqlaboratory/openfold/blob/b2d6bff691e0ad860d585ea6709b81b1aba57d2c/openfold/model/msa.py#L290](https://github.com/aqlaboratory/openfold/blob/b2d6bff691e0ad860d585ea6709b81b1aba57d2c/openfold/model/msa.py#L290)'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'OpenFold: [https://github.com/aqlaboratory/openfold/blob/b2d6bff691e0ad860d585ea6709b81b1aba57d2c/openfold/model/msa.py#L290](https://github.com/aqlaboratory/openfold/blob/b2d6bff691e0ad860d585ea6709b81b1aba57d2c/openfold/model/msa.py#L290)'
- en: The key point here is to allow MSA and pair representations communicate information
    with each other.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 关键点在于允许 MSA 和对偶表示之间的信息交流。
- en: First, multi-head attention is used to calculate dot-product affinities **(N_res,
    N_res, N_heads)** from the MSA representation *row*, meaning the amino acids in
    the sequence will learn “conceptual importance” between pairs. In essence, how
    important one amino acid is for another amino acid.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，使用多头注意力从 MSA 表示 *行* 计算点积相似度 **(N_res, N_res, N_heads)**，这意味着序列中的氨基酸将学习“概念重要性”以识别对之间的关系。实质上，是一个氨基酸对另一个氨基酸的重要性。
- en: Then, the pair representation goes through a linear layer without bias, meaning
    only a weight parameter will be learned. The linear layer outputs **N_heads**
    dimensions producing the matrix pair bias matrix **(N_res, N_res, N_heads)**.
    Remember this matrix was initially capped to 32 maximum distance meaning if an
    amino acid is more distant than 32 indices, it will have a value of 0
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，对偶表示经过一个没有偏置的线性层，这意味着只会学习一个权重参数。线性层输出 **N_heads** 维度，生成矩阵对偶偏置矩阵 **(N_res,
    N_res, N_heads)**。请记住，这个矩阵最初被限制在最大距离为 32，这意味着如果氨基酸之间的距离超过 32 个索引，它的值将为 0。
- en: At this point, we have two matrices of shape **(N_res, N_res, N_heads)** that
    we can easily add together and softmax to have values between 0 and 1\. An attention
    block with the added matrices as *Queries* and a row passed through a linear layer
    as values to obtain the attention weights.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 此时，我们有两个形状为 **(N_res, N_res, N_heads)** 的矩阵，我们可以轻松地将它们相加并应用 softmax，使值介于 0 和
    1 之间。一个注意力块使用相加后的矩阵作为 *Queries*，并将一行通过线性层作为值，以获得注意力权重。
- en: 'Now we calculate the dot product between:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们计算点积：
- en: the attention weights and
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 注意力权重和
- en: the Linear + sigmoid of the MSA row as keys (I believe the sigmoid operation
    here returns a probability-like array ranging from 0–1)
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MSA行的线性+sigmoid作为键（我相信此处的sigmoid操作返回一个范围为0–1的类似概率的数组）
- en: '**2.1B Column-Wise Gated Self-Attention**'
  id: totrans-72
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**2.1B 列式门控自注意力**'
- en: '![](../Images/ac3b99d91110d38e38bff439bc56d145.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ac3b99d91110d38e38bff439bc56d145.png)'
- en: Column-Wise Gated Self-Attention in Evoformer. [1]
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: Evoformer中的列式门控自注意力。[1]
- en: 'Algorithm: 8'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 算法：8
- en: 'OpenFold: [https://github.com/aqlaboratory/openfold/blob/b2d6bff691e0ad860d585ea6709b81b1aba57d2c/openfold/model/msa.py#L319](https://github.com/aqlaboratory/openfold/blob/b2d6bff691e0ad860d585ea6709b81b1aba57d2c/openfold/model/msa.py#L319)'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'OpenFold: [https://github.com/aqlaboratory/openfold/blob/b2d6bff691e0ad860d585ea6709b81b1aba57d2c/openfold/model/msa.py#L319](https://github.com/aqlaboratory/openfold/blob/b2d6bff691e0ad860d585ea6709b81b1aba57d2c/openfold/model/msa.py#L319)'
- en: The key point here is that MSA is an aligned version of all sequences related
    to the input sequences. This means that index X will correspond to the same area
    of the protein for each sequence.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的关键点是MSA是与输入序列相关的所有序列的对齐版本。这意味着索引X将对应于每个序列的蛋白质相同区域。
- en: By doing this operation column-wise, we ensure that we have a general understanding
    of which residues are more likely for each position. This also means the model
    would be robust should a similar sequence with small differences produce similar
    3D shapes.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 通过逐列执行此操作，我们确保对每个位置上哪些残基更可能有一个总体了解。这也意味着，如果一个类似的序列有微小的差异会产生相似的3D形状，模型将会更强健。
- en: '**2.1C MSA Transition**'
  id: totrans-79
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**2.1C MSA过渡**'
- en: '![](../Images/674760d15b95e0626fa7e7c90696968a.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/674760d15b95e0626fa7e7c90696968a.png)'
- en: MSA Transition in Evoformer. [1]
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: Evoformer中的MSA过渡。[1]
- en: 'Algorithm: 9'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 算法：9
- en: 'OpenFold: [https://github.com/aqlaboratory/openfold/blob/b2d6bff691e0ad860d585ea6709b81b1aba57d2c/openfold/model/evoformer.py#L45](https://github.com/aqlaboratory/openfold/blob/b2d6bff691e0ad860d585ea6709b81b1aba57d2c/openfold/model/evoformer.py#L45)'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'OpenFold: [https://github.com/aqlaboratory/openfold/blob/b2d6bff691e0ad860d585ea6709b81b1aba57d2c/openfold/model/evoformer.py#L45](https://github.com/aqlaboratory/openfold/blob/b2d6bff691e0ad860d585ea6709b81b1aba57d2c/openfold/model/evoformer.py#L45)'
- en: This is a simple 2-layer MLP that first increases the channel dimensions by
    a factor of 4 and then reduces it down to the original dimensions.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个简单的2层MLP，首先将通道维度增加4倍，然后再缩小到原始维度。
- en: '**2.1D Outer Product Mean**'
  id: totrans-85
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**2.1D 外积均值**'
- en: '![](../Images/80fb65e42778df2a915439bbd3f8b581.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/80fb65e42778df2a915439bbd3f8b581.png)'
- en: Outer Product Mean in Evoformer.[1]
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: Evoformer中的外积均值。[1]
- en: 'Algorithm: 10'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 算法：10
- en: 'OpenFold: [https://github.com/aqlaboratory/openfold/blob/b2d6bff691e0ad860d585ea6709b81b1aba57d2c/openfold/model/outer_product_mean.py#L27](https://github.com/aqlaboratory/openfold/blob/b2d6bff691e0ad860d585ea6709b81b1aba57d2c/openfold/model/outer_product_mean.py#L27)'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'OpenFold: [https://github.com/aqlaboratory/openfold/blob/b2d6bff691e0ad860d585ea6709b81b1aba57d2c/openfold/model/outer_product_mean.py#L27](https://github.com/aqlaboratory/openfold/blob/b2d6bff691e0ad860d585ea6709b81b1aba57d2c/openfold/model/outer_product_mean.py#L27)'
- en: This operation aims at keeping a continuous flow of information between the
    MSA and the pair representation. Each column in the MSA is an index position of
    a protein sequence.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 此操作旨在保持MSA和配对表示之间的信息连续流。MSA中的每一列是蛋白质序列的一个索引位置。
- en: Here, we select index i and j, which we independently send through a linear
    layer. This linear layer uses c=32, which is lower than c_m.
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在这里，我们选择索引i和j，并将它们独立地通过一个线性层。这个线性层使用c=32，这低于c_m。
- en: The outer product is then calculated, averaged, flattened, and again through
    another linear layer.
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然后计算外积，取均值，展平，再通过另一个线性层。
- en: We now have an updated entry for ij in the pair representation. We repeat this
    for all the pairs.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在在配对表示中有了更新的ij条目。我们对所有配对重复这一操作。
- en: 2.2 Pairs Stack
  id: totrans-94
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.2 配对堆叠
- en: '![](../Images/69ad5293cb38f8bd90d81997f7c2db00.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/69ad5293cb38f8bd90d81997f7c2db00.png)'
- en: Pairs Stack of EvoFormer. [1]
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: EvoFormer的配对堆叠。[1]
- en: Our pair representation can technically be interpreted as a distance matrix.
    Earlier, we saw how each amino acid starts with 32 neighbors. We can therefore
    build a triangle graph based on three indices of the pair representation.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 从技术上讲，我们的配对表示可以被解释为一个距离矩阵。之前，我们看到每种氨基酸都有32个邻居。因此，我们可以基于配对表示的三个索引构建一个三角图。
- en: For example, nodes i, j, and k will have edges ij, ik, and jk. Each edge is
    updated with information from the other two edges of all the triangles it is part
    of.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，节点i、j和k将有边ij、ik和jk。每条边都通过它所在的所有三角形的其他两条边的信息进行更新。
- en: '![](../Images/6c87f676eedaae962a2e875acd388fa2.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6c87f676eedaae962a2e875acd388fa2.png)'
- en: Triangle Multiplicative Update and Triangle Self-Attention. [1]
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 三角乘法更新和三角自注意力。[1]
- en: '**2.2A Triangular Multiplicative Update**'
  id: totrans-101
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**2.2A 三角乘法更新**'
- en: '![](../Images/14d748b403eb188c6ef59ff604b3be47.png)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/14d748b403eb188c6ef59ff604b3be47.png)'
- en: We have two types of updates, one for outgoing edges and one for incoming edges.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有两种类型的更新，一种用于输出边缘，一种用于输入边缘。
- en: '![](../Images/b3d2cb985f33e404ea6c08c22b405cf7.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b3d2cb985f33e404ea6c08c22b405cf7.png)'
- en: Triangular Multiplicative Update. [1]
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 三角乘法更新。[1]
- en: 'Algorithm: 11 and 12'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 算法：11和12
- en: 'OpenFold: [https://github.com/aqlaboratory/openfold/blob/f84eb0a113b6b8f9c0bb022ea0796aa90ff016bd/openfold/model/triangular_multiplicative_update.py#L28](https://github.com/aqlaboratory/openfold/blob/f84eb0a113b6b8f9c0bb022ea0796aa90ff016bd/openfold/model/triangular_multiplicative_update.py#L28)'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'OpenFold: [https://github.com/aqlaboratory/openfold/blob/f84eb0a113b6b8f9c0bb022ea0796aa90ff016bd/openfold/model/triangular_multiplicative_update.py#L28](https://github.com/aqlaboratory/openfold/blob/f84eb0a113b6b8f9c0bb022ea0796aa90ff016bd/openfold/model/triangular_multiplicative_update.py#L28)'
- en: For **outgoing edges**, the full row or pair representations i and j is first
    independently passed through a linear layer producing a representation of the
    left edges and right edges.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 对于**输出边缘**，完整的行或对表示 i 和 j 首先独立地通过一个线性层，生成左边缘和右边缘的表示。
- en: Then, we compute the dot product between the corresponding representation for
    the ij pair and the left and right edges independently.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们计算对应的 ij 对的表示与左边缘和右边缘的点积，分别进行。
- en: Finally, we take the dot product of the left and right edges representations
    and a final dot product with the ij pair representation.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们取左边缘和右边缘表示的点积，并与 ij 对表示进行最终点积。
- en: For **incoming edges**, the algorithm is very similar but bear in mind that
    if previously we were considering the edge as ik, we now go in the opposite direction
    ki. In the OpenFold code, this is implemented simply as a permute function.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 对于**输入边缘**，算法非常类似，但请注意，如果之前我们考虑的是边缘 ik，现在我们需要考虑相反的方向 ki。在OpenFold代码中，这简单地实现为一个排列函数。
- en: '**2.2B Triangular Self-Attention**'
  id: totrans-112
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**2.2B 三角自注意力**'
- en: '![](../Images/9e5b7c80c3e2433ed5980310d199b998.png)![](../Images/7a3e5c06c8fbffb72e53f35bc46bd2d5.png)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9e5b7c80c3e2433ed5980310d199b998.png)![](../Images/7a3e5c06c8fbffb72e53f35bc46bd2d5.png)'
- en: Triangular Self-Attention. [1]
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 三角自注意力。[1]
- en: 'Algorithm: 13 and 14'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 算法：13和14
- en: 'OpenFold: [https://github.com/aqlaboratory/openfold/blob/f84eb0a113b6b8f9c0bb022ea0796aa90ff016bd/openfold/model/triangular_attention.py#L31](https://github.com/aqlaboratory/openfold/blob/f84eb0a113b6b8f9c0bb022ea0796aa90ff016bd/openfold/model/triangular_attention.py#L31)'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'OpenFold: [https://github.com/aqlaboratory/openfold/blob/f84eb0a113b6b8f9c0bb022ea0796aa90ff016bd/openfold/model/triangular_attention.py#L31](https://github.com/aqlaboratory/openfold/blob/f84eb0a113b6b8f9c0bb022ea0796aa90ff016bd/openfold/model/triangular_attention.py#L31)'
- en: This operation aims at updating the pair representation by using self-attention.
    The main goal is to update the edge with the most relevant edges, ie. which amino
    acids in the protein are more likely to interact with the current node.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 这个操作旨在通过自注意力更新对表示。主要目标是用最相关的边缘更新边缘，即蛋白质中哪些氨基酸更可能与当前节点相互作用。
- en: 'With self-attention, we *learn* the best way to update the edge through:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 通过自注意力，我们*学习*更新边缘的最佳方法：
- en: (query-key) Similarity between edges that contain the node of interest. For
    instance for node i, all edges that share that node (eg. ij, ik).
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: （query-key）包含感兴趣节点的边缘之间的相似性。例如，对于节点 i，所有共享该节点的边缘（例如 ij，ik）。
- en: A third edge (eg. jk) which even if it does not directly connect to node i,
    is part of the triangle.
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第三条边（例如 jk），即使它不直接连接到节点 i，也属于三角形的一部分。
- en: This last operation is similar in style to a graph message-passing algorithm,
    where even if nodes are not directly connected, information from other nodes in
    the graph is weighted and passed on.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 这个最后的操作在风格上类似于图消息传递算法，即使节点没有直接连接，来自图中其他节点的信息也会被加权并传递。
- en: '**2.2C Transition Block**'
  id: totrans-122
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**2.2C 过渡块**'
- en: Equivalent to the transition block in the MSA trunk with a 2-Layer MLP where
    the channel is first expanded by a factor of 4 and then reduced to the original
    number.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 相当于MSA主干中的过渡块，具有一个2层的MLP，其中通道首先扩展4倍，然后减少到原始数量。
- en: The output of the EvoFormer block is an updated representation of both MSA and
    pairs (of the same dimensionality).
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: EvoFormer块的输出是更新后的MSA和对的表示（具有相同的维度）。
- en: 3\. Structure Module
  id: totrans-125
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 3. 结构模块
- en: '![](../Images/11b5abb19f9cb2dbdb7d2c93d698c4b9.png)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/11b5abb19f9cb2dbdb7d2c93d698c4b9.png)'
- en: Structure Module of AlphaFold. [1]
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: AlphaFold的结构模块。[1]
- en: 'Algorithm: 20'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 算法：20
- en: 'OpenFold: [https://github.com/aqlaboratory/openfold/blob/f84eb0a113b6b8f9c0bb022ea0796aa90ff016bd/openfold/model/structure_module.py#L515](https://github.com/aqlaboratory/openfold/blob/f84eb0a113b6b8f9c0bb022ea0796aa90ff016bd/openfold/model/structure_module.py#L515)'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'OpenFold: [https://github.com/aqlaboratory/openfold/blob/f84eb0a113b6b8f9c0bb022ea0796aa90ff016bd/openfold/model/structure_module.py#L515](https://github.com/aqlaboratory/openfold/blob/f84eb0a113b6b8f9c0bb022ea0796aa90ff016bd/openfold/model/structure_module.py#L515)'
- en: The structure module is the final part of the model and converts the pairs representations
    and the input sequence representation (corresponds to a row in the MSA representation)
    into a 3D structure. It consists of 8 layers with shared weights, and the pair
    representation is used to bias the attention operations in the Invariant Point
    Attention (IPA) module.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 结构模块是模型的最终部分，将配对表示和输入序列表示（对应MSA表示中的一行）转换为3D结构。它由8层共享权重构成，配对表示用于在不变点注意力（IPA）模块中偏置注意力操作。
- en: 'The outputs are:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '**Backbone Frames (r, 3x3)**: Frames represent a Euclidean transform for atomic
    positions to go from a local frame of reference to a global one. Free-floating
    body representation (blue triangles) composed of N-Cα-C; thus, each residue (r_i)
    has three sets of (x, y, z) coordinates'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**背骨框架（r, 3x3）**：框架表示将原子位置从局部参考系变换到全球参考系的欧几里得变换。由N-Cα-C组成的自由漂浮体表示（蓝色三角形）；因此，每个残基（r_i）有三组（x、y、z）坐标'
- en: '**χ** **angles of the sidechains** (r , 3): represents the angle of each rotatable
    atom of the side chain. The angles define the rotational isomer (rotamer) of a
    residue; therefore, one can derive the exact position of the atoms. Up to **χ1,
    χ2, χ3, χ4**.'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**χ** **侧链角度** (r , 3)：表示每个可旋转侧链原子的角度。这些角度定义了残基的旋转异构体（rotamer），因此可以推导出原子的确切位置。最多到**χ1、χ2、χ3、χ4**。'
- en: 'Note that **χ** refers to the dihedral angle of each of the rotatable bonds
    of the side chains. There are shorter amino acids that do not have all four χ
    angles as shown below:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 注意**χ**指的是每个可旋转键的二面角。有些较短的氨基酸没有所有四个χ角度，如下所示：
- en: '![](../Images/1dbf421f8a19c198e0bf7fa0959eccaf.png)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1dbf421f8a19c198e0bf7fa0959eccaf.png)'
- en: Side-chain angles for Lysine and Tyrosine. Tyrosine is shorter and does not
    have **χ3, χ4\.** (Illustration by the author)
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 赖氨酸和酪氨酸的侧链角度。酪氨酸较短，没有**χ3、χ4**。（由作者插图）
- en: '**3.1 Invariant Point Attention (IPA)**'
  id: totrans-137
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**3.1 不变点注意力（IPA）**'
- en: '![](../Images/8f868b6d1391f860e71b8e5de141cd74.png)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8f868b6d1391f860e71b8e5de141cd74.png)'
- en: Invariant Point Attention (IPA) of Structure Module. [1]
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 结构模块的不变点注意力（IPA）。 [1]
- en: 'Algorithm: 22'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 算法：22
- en: 'OpenFold: [https://github.com/aqlaboratory/openfold/blob/f84eb0a113b6b8f9c0bb022ea0796aa90ff016bd/openfold/model/structure_module.py#L161](https://github.com/aqlaboratory/openfold/blob/f84eb0a113b6b8f9c0bb022ea0796aa90ff016bd/openfold/model/structure_module.py#L161)'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'OpenFold: [https://github.com/aqlaboratory/openfold/blob/f84eb0a113b6b8f9c0bb022ea0796aa90ff016bd/openfold/model/structure_module.py#L161](https://github.com/aqlaboratory/openfold/blob/f84eb0a113b6b8f9c0bb022ea0796aa90ff016bd/openfold/model/structure_module.py#L161)'
- en: Generally, this type of attention is designed to be invariant to Euclidean transformations
    such as translations and rotations.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，这种类型的注意力设计为对欧几里得变换（如平移和旋转）不变。
- en: We first update the single representation with self-attention, as explained
    in previous sections.
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们首先用自注意力更新单一表示，如前面部分所述。
- en: We also feed information about the backbone frames of each residue to produce
    query points, key points, and value points for the local frame. These are then
    projected into a global frame where they interact with other residues and then
    projected back to the local frame.
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们还将每个残基的背骨框架信息输入以生成查询点、关键点和值点用于局部框架。这些点随后被投影到全球框架中，与其他残基交互后，再投影回局部框架。
- en: The word “invariant” refers to the fact that global and local reference points
    are enforced to be invariant by using squared distances and coordinate transformation
    in the 3D space.
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “不变”一词指的是通过在3D空间中使用平方距离和坐标变换，强制全局和局部参考点保持不变。
- en: '**3.2 Predict side chain and backbone torsion angles**'
  id: totrans-146
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**3.2 预测侧链和背骨扭转角度**'
- en: The single representation goes through a couple of MLPs and outputs the torsion
    angles ω, φ, ψ, χ1, χ2, χ3, χ4.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 单一表示经过几个MLP，并输出扭转角度ω、φ、ψ、χ1、χ2、χ3、χ4。
- en: '**3.3 Backbone Update**'
  id: totrans-148
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**3.3 背骨更新**'
- en: 'Algorithm: 23'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 算法：23
- en: 'OpenFold: [https://github.com/aqlaboratory/openfold/blob/f84eb0a113b6b8f9c0bb022ea0796aa90ff016bd/openfold/model/structure_module.py#L434](https://github.com/aqlaboratory/openfold/blob/f84eb0a113b6b8f9c0bb022ea0796aa90ff016bd/openfold/model/structure_module.py#L434)'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'OpenFold: [https://github.com/aqlaboratory/openfold/blob/f84eb0a113b6b8f9c0bb022ea0796aa90ff016bd/openfold/model/structure_module.py#L434](https://github.com/aqlaboratory/openfold/blob/f84eb0a113b6b8f9c0bb022ea0796aa90ff016bd/openfold/model/structure_module.py#L434)'
- en: 'There are two updates returned by this block: one is the **rotation** represented
    by a quaternion (1, a, b, c where the first value is fixed to 1 and a, b, and
    c correspond to the Euler axis predicted by the network) and a **translation**
    represented by a vector matrix.'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 此块返回两个更新：一个是由四元数表示的**旋转**（1, a, b, c，其中第一个值固定为1，a、b和c对应于网络预测的欧拉轴），另一个是由向量矩阵表示的**平移**。
- en: '**3.4 All Atom Coordinates**'
  id: totrans-152
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**3.4 所有原子坐标**'
- en: At this point, we have both the **backbone frames** and the **torsion angles**,
    and we would like to obtain the exact atom coordinates of the amino acid. Amino
    acids have a very specific structure of atoms, and we have the identity as the
    input sequence. We, therefore, apply the torsion angles to the atoms of the amino
    acid.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 此时，我们既有**骨架框架**又有**扭转角度**，我们希望获得氨基酸的确切原子坐标。氨基酸具有非常特定的原子结构，而我们有身份作为输入序列。因此，我们将扭转角度应用于氨基酸的原子。
- en: Note that many times you will find many structural violations in the output
    of AlphaFold, such as the ones depicted below. This is because the model itself
    does not enforce physical energy constraints. To alleviate this problem, we run
    an AMBER relaxation force field to minimize the energy of the protein.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，你会发现 AlphaFold 的输出中有许多结构违例，例如下面描述的情况。这是因为模型本身没有强制施加物理能量约束。为了解决这个问题，我们运行 AMBER
    松弛力场来最小化蛋白质的能量。
- en: '![](../Images/f5216d56c7db49314c97407df88cec78.png)'
  id: totrans-155
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f5216d56c7db49314c97407df88cec78.png)'
- en: Before AMBER relaxation, Methionine and Triptophan are predicted too close to
    each other forming an impossible bond. After AMBER relaxation, the rotamers are
    adjusted to minimize energy and steric clashes. (Illustration by the author)
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 在 AMBER 松弛之前，蛋氨酸和色氨酸被预测过于接近，形成了一个不可能的键。在 AMBER 松弛之后，旋转构象被调整以最小化能量和立体冲突。（插图作者提供）
- en: Other Optimisations
  id: totrans-157
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 其他优化
- en: 'The AlphaFold model contains several self-attention layers and large activations
    due to the sizes of the MSAs. Classical backpropagation is optimized to reduce
    the number of total computations per node. However, in the case of AlphaFold,
    it would require more than the available memory in a TPU core (16 GiB). Assuming
    a protein of 384 residues:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: AlphaFold 模型包含多个自注意力层和由于 MSA 尺寸而产生的大量激活。经典的反向传播优化是减少每个节点的总计算量。然而，在 AlphaFold
    的情况下，它会需要超过 TPU 核心的可用内存（16 GiB）。假设一个含有384个残基的蛋白质：
- en: '![](../Images/3e39ac8b417fff4f275a0d363aa0a8b9.png)'
  id: totrans-159
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3e39ac8b417fff4f275a0d363aa0a8b9.png)'
- en: Instead, AlphaFold used gradient checkpointing (also rematerialization). The
    activations are recomputed and calculated for one layer at the time, thus bringing
    memory consumption to around 0.4 GiB.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，AlphaFold 使用了梯度检查点（也称为再物化）。激活在每次一个层时被重新计算，从而将内存消耗降至约0.4 GiB。
- en: 'This GIF shows what backpropagation usually looks like:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 这个 GIF 显示了反向传播通常的样子：
- en: '![](../Images/e8d6816fa2e7013d2c1219385e99281b.png)'
  id: totrans-162
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e8d6816fa2e7013d2c1219385e99281b.png)'
- en: Backpropagation. (Illustration by the author based on [https://github.com/cybertronai/gradient-checkpointing](https://github.com/cybertronai/gradient-checkpointing))
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 反向传播。（插图作者基于 [https://github.com/cybertronai/gradient-checkpointing](https://github.com/cybertronai/gradient-checkpointing)
    提供）
- en: 'By checkpointing, we reduce memory usage, though this has the unfortunate side
    effect of increasing training time by 33%:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 通过检查点技术，我们减少了内存使用量，尽管这有一个不幸的副作用，即训练时间增加了 33%：
- en: '![](../Images/377a827f8ae0d036e482acd1be4fad13.png)'
  id: totrans-165
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/377a827f8ae0d036e482acd1be4fad13.png)'
- en: 'Fixing a layer for checkpointing. (Illustration by the author based on: [https://github.com/cybertronai/gradient-checkpointing](https://github.com/cybertronai/gradient-checkpointing))'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: '为检查点修复一个层。（插图作者基于: [https://github.com/cybertronai/gradient-checkpointing](https://github.com/cybertronai/gradient-checkpointing)
    提供）'
- en: '![](../Images/2fb60fef6b0d8c0864687165827c9803.png)'
  id: totrans-167
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2fb60fef6b0d8c0864687165827c9803.png)'
- en: Fixing a layer for checkpointing. (Illustration by the author based on [https://github.com/cybertronai/gradient-checkpointing](https://github.com/cybertronai/gradient-checkpointing))
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 为检查点修复一个层。（插图作者基于 [https://github.com/cybertronai/gradient-checkpointing](https://github.com/cybertronai/gradient-checkpointing)
    提供）
- en: What about Designing Proteins? The Inverse Folding Problem
  id: totrans-169
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设计蛋白质呢？逆折叠问题
- en: 'What if, rather than a sequence of amino acids, you had the model of a cool
    protein you designed with a dynamics simulation? Or one that you modeled to bind
    another protein like a COVID spike protein. Ideally, you would want to predict
    the sequence necessary to fold to an input 3D shape that may or may not exist
    in nature (i.e., it could be a completely new protein). Let me introduce you to
    the world of protein design, which is also my Ph.D. project TIMED (Three-dimensional
    Inference Method for Efficient Design):'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你有一个通过动态模拟设计的酷蛋白质模型，而不是一个氨基酸序列呢？或者一个你建模以结合另一个蛋白质（如 COVID 蛋白质刺突）的模型。理想情况下，你会想预测出折叠成一个输入的三维形状所需的序列，这个形状可能存在于自然界中，也可能是全新的蛋白质。我将向你介绍蛋白质设计的世界，这也是我博士项目
    TIMED（三维推断方法高效设计）的内容：
- en: '![](../Images/156deb08ea678962e49380f45319cf81.png)'
  id: totrans-171
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/156deb08ea678962e49380f45319cf81.png)'
- en: The Inverse Folding Problem (Illustration by the author)
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 逆折叠问题（作者插图）
- en: This problem is arguably harder than the folding problem, as multiple sequences
    can fold to the same shape. This is because there is redundancy in amino acid
    types, and there are also areas of a protein that are less critical for the actual
    fold.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 这个问题可以说比折叠问题更难，因为多个序列可以折叠成相同的形状。这是因为氨基酸类型存在冗余，并且蛋白质的某些区域对实际折叠的影响较小。
- en: The input of the model is a cube of gridded, voxelised space (a “Frame”) around
    each amino acid position of the backbone. The alpha Carbon is centered in the
    frame, and the frame is rotated so that the Alpha Carbon to Carbon bond lies along
    the x-axis. Each atom (C, N, O, alpha-C, beta-C) is one-hot-encoded in a different
    channel, thus producing a 4D array.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 模型的输入是围绕每个氨基酸位置的骨干的网格化体素空间的立方体（“框架”）。Alpha 碳中心位于框架中，框架旋转，使 Alpha 碳到碳的键沿 x 轴。每个原子（C、N、O、alpha-C、beta-C）在不同的通道中进行独热编码，从而产生一个
    4D 数组。
- en: 'The output of our models is a probability distribution over all amino acids
    at each position. For instance, at each position the models output a probability
    over every residue being at that position. For a 100-amino-acid protein will have
    an output of shape (100, 20) as there are twenty amino acids:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 我们模型的输出是每个位置上所有氨基酸的概率分布。例如，在每个位置，模型输出一个氨基酸在该位置上的概率。对于一个100氨基酸的蛋白质，输出的形状为 (100,
    20)，因为有二十种氨基酸：
- en: '![](../Images/21f8e109482c5a5353c658ed9d3dd1a5.png)'
  id: totrans-176
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/21f8e109482c5a5353c658ed9d3dd1a5.png)'
- en: Prediction pipeline for TIMED (Image by the author)
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: TIMED 的预测管道（作者插图）
- en: 'The cool aspect about AlphaFold is that we can use it to double-check whether
    our models work well:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: AlphaFold 的一个酷炫之处在于我们可以利用它来双重检查我们的模型是否有效：
- en: '![](../Images/679cf66e35f1e07b75ddd76cfc2e8130.png)'
  id: totrans-179
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/679cf66e35f1e07b75ddd76cfc2e8130.png)'
- en: The Folding and Inverse Folding Problem (Illustration by the author).
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 折叠和逆折叠问题（作者插图）。
- en: If you would like to know more about this model, have a look at my GitHub repository,
    which also includes a little UI Demo!
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想了解更多关于这个模型的信息，请查看我的 GitHub 仓库，那里还有一个小的 UI 演示！
- en: '![](../Images/92d5b7078415e1e58444f43dabc38af7.png)'
  id: totrans-182
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/92d5b7078415e1e58444f43dabc38af7.png)'
- en: 'UI Demo of TIMED for solving the Inverse Folding Problem. (Illustration by
    the author). GitHub: [https://github.com/wells-wood-research/timed-design](https://github.com/wells-wood-research/timed-design)'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 'TIMED 解决逆折叠问题的 UI 演示（作者插图）。GitHub: [https://github.com/wells-wood-research/timed-design](https://github.com/wells-wood-research/timed-design)'
- en: Conclusion
  id: totrans-184
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: In this article, we saw how AlphaFold (partially) solves a clear problem for
    biologists, mainly obtaining 3D structures from an amino acid sequence.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们看到 AlphaFold 如何（部分）解决了一个生物学家明确的问题，即从氨基酸序列中获得三维结构。
- en: We broke down the structure of the model into Input Embedder, EvoFormer, and
    Structure module. Each of these uses several self-attention layers, including
    many tricks to optimize the performance.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将模型的结构分解为输入嵌入器、EvoFormer 和结构模块。每个模块使用多个自注意力层，并包含许多优化性能的技巧。
- en: AlphaFold works well, but is this it for biology? No. AlphaFold is still computationally
    very expensive, and there isn’t an easy way to use it (No, Google Colab is not
    easy — it’s clunky). Several alternatives, like OmegaFold and ESMFold, attempt
    to solve these problems.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: AlphaFold 工作良好，但这就是生物学的全部吗？不。AlphaFold 仍然计算上非常昂贵，并且使用起来不容易（不，Google Colab 不容易——它很笨重）。一些替代方案，如
    OmegaFold 和 ESMFold，试图解决这些问题。
- en: These models still do not explain *how* a protein folds over time. There are
    also a lot of challenges that involve designing proteins where inverse folding
    models can use AlphaFold to double-check that designed proteins fold to a specific
    shape.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 这些模型仍未解释蛋白质如何随时间折叠。还有许多挑战涉及设计蛋白质，其中逆折叠模型可以利用 AlphaFold 进行双重检查，以确保设计的蛋白质折叠成特定的形状。
- en: In the next series of articles, we will look into OmegaFold and ESMFold!
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的系列文章中，我们将探讨 OmegaFold 和 ESMFold！
- en: References
  id: totrans-190
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] Jumper J, Evans R, Pritzel A, Green T, Figurnov M, Ronneberger O, Tunyasuvunakool
    K, Bates R, Žídek A, Potapenko A, et al. [Highly accurate protein structure prediction
    with AlphaFold](https://www.nature.com/articles/s41586-021-03819-2). Nature (2021)
    DOI: 10.1038/s41586–021–03819–2'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] Jumper J, Evans R, Pritzel A, Green T, Figurnov M, Ronneberger O, Tunyasuvunakool
    K, Bates R, Žídek A, Potapenko A, 等. [使用 AlphaFold 进行高精度蛋白质结构预测](https://www.nature.com/articles/s41586-021-03819-2)。
    《自然》(2021) DOI: 10.1038/s41586–021–03819–2'
- en: '[2] Alberts B. [Molecular biology of the cell](https://www.ncbi.nlm.nih.gov/books/NBK21054/).
    (2015) Sixth edition. New York, NY: Garland Science, Taylor and Francis Group.'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] Alberts B. [细胞的分子生物学](https://www.ncbi.nlm.nih.gov/books/NBK21054/)。 (2015)
    第六版。纽约，NY：Garland Science，Taylor 和 Francis Group。'
- en: '[3] Ahdritz G, Bouatta N, Kadyan S, Xia Q, Gerecke W, O’Donnell TJ, Berenberg
    D, Fisk I, Zanichelli N, Zhang B, et al. [OpenFold: Retraining AlphaFold2 yields
    new insights into its learning mechanisms and capacity for generalization](https://www.biorxiv.org/content/10.1101/2022.11.20.517210v1.full)
    (2022) Bioinformatics. DOI: 10.1101/2022.11.20.517210'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] Ahdritz G, Bouatta N, Kadyan S, Xia Q, Gerecke W, O’Donnell TJ, Berenberg
    D, Fisk I, Zanichelli N, Zhang B, 等. [OpenFold: 重新训练 AlphaFold2 提供了关于其学习机制和能力的新见解](https://www.biorxiv.org/content/10.1101/2022.11.20.517210v1.full)
    (2022) 生物信息学。DOI: 10.1101/2022.11.20.517210'
- en: '[4] Callaway E. [“It will change everything”: DeepMind’s AI makes gigantic
    leap in solving protein structures](https://www.nature.com/articles/d41586-020-03348-4)
    (2020). Nature 588(7837):203–204\. DOI: 10.1038/d41586–020–03348–4'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] Callaway E. [“这将改变一切”：DeepMind 的 AI 在解决蛋白质结构方面取得了巨大飞跃](https://www.nature.com/articles/d41586-020-03348-4)
    (2020)。《自然》588(7837):203–204\. DOI: 10.1038/d41586–020–03348–4'
- en: Further Reading
  id: totrans-195
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: 'The original AlphaFold Paper:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 原始的 AlphaFold 论文：
- en: '[](https://www.nature.com/articles/s41586-021-03819-2?source=post_page-----6c81faba670d--------------------------------)
    [## Highly accurate protein structure prediction with AlphaFold - Nature'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://www.nature.com/articles/s41586-021-03819-2?source=post_page-----6c81faba670d--------------------------------)
    [## 使用 AlphaFold 进行高精度蛋白质结构预测 - 自然'
- en: Proteins are essential to life, and understanding their structure can facilitate
    a mechanistic understanding of their…
  id: totrans-198
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 蛋白质对生命至关重要，理解它们的结构可以促进对其机制的理解…
- en: www.nature.com](https://www.nature.com/articles/s41586-021-03819-2?source=post_page-----6c81faba670d--------------------------------)
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: www.nature.com](https://www.nature.com/articles/s41586-021-03819-2?source=post_page-----6c81faba670d--------------------------------)
- en: 'OpenFold Paper and Code:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: OpenFold 论文和代码：
- en: '[](https://www.biorxiv.org/content/10.1101/2022.11.20.517210v2?source=post_page-----6c81faba670d--------------------------------)
    [## OpenFold: Retraining AlphaFold2 yields new insights into its learning mechanisms
    and capacity for…'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://www.biorxiv.org/content/10.1101/2022.11.20.517210v2?source=post_page-----6c81faba670d--------------------------------)
    [## OpenFold: 重新训练 AlphaFold2 提供了关于其学习机制和能力的新见解'
- en: AlphaFold2 revolutionized structural biology with the ability to predict protein
    structures with exceptionally high…
  id: totrans-202
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: AlphaFold2 通过能够以极高的精度预测蛋白质结构，彻底改变了结构生物学…
- en: 'www.biorxiv.org](https://www.biorxiv.org/content/10.1101/2022.11.20.517210v2?source=post_page-----6c81faba670d--------------------------------)
    [](https://github.com/aqlaboratory/openfold?source=post_page-----6c81faba670d--------------------------------)
    [## GitHub - aqlaboratory/openfold: Trainable, memory-efficient, and GPU-friendly
    PyTorch reproduction…'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 'www.biorxiv.org](https://www.biorxiv.org/content/10.1101/2022.11.20.517210v2?source=post_page-----6c81faba670d--------------------------------)
    [](https://github.com/aqlaboratory/openfold?source=post_page-----6c81faba670d--------------------------------)
    [## GitHub - aqlaboratory/openfold: 可训练、内存高效且支持 GPU 的 PyTorch 重现…'
- en: 'Figure: Comparison of OpenFold and AlphaFold2 predictions to the experimental
    structure of PDB 7KDX, chain B. A…'
  id: totrans-204
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 图：OpenFold 和 AlphaFold2 预测与 PDB 7KDX，链 B 的实验结构的比较。A…
- en: github.com](https://github.com/aqlaboratory/openfold?source=post_page-----6c81faba670d--------------------------------)
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: github.com](https://github.com/aqlaboratory/openfold?source=post_page-----6c81faba670d--------------------------------)
- en: 'Another well-written article on AlphaFold:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 关于 AlphaFold 的另一篇写得很好的文章：
- en: '[](https://dauparas.github.io/post/af2/?source=post_page-----6c81faba670d--------------------------------)
    [## AlphaFold 2 & Equivariance | Justas Dauparas'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://dauparas.github.io/post/af2/?source=post_page-----6c81faba670d--------------------------------)
    [## AlphaFold 2 & 等变性 | Justas Dauparas'
- en: Fabian Fuchs & Justas Dauparas A few weeks ago, in the latest CASP competition
    for protein structure prediction (…
  id: totrans-208
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Fabian Fuchs & Justas Dauparas 几周前，在最新的 CASP 蛋白质结构预测竞赛中（…
- en: dauparas.github.io](https://dauparas.github.io/post/af2/?source=post_page-----6c81faba670d--------------------------------)
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: dauparas.github.io](https://dauparas.github.io/post/af2/?source=post_page-----6c81faba670d--------------------------------)
- en: 'On Protein (sequence) Design:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 关于蛋白质（序列）设计：
- en: '[](https://github.com/wells-wood-research/timed-design?source=post_page-----6c81faba670d--------------------------------)
    [## GitHub - wells-wood-research/timed-design: Protein Sequence Design with Deep
    Learning and Tooling…'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://github.com/wells-wood-research/timed-design?source=post_page-----6c81faba670d--------------------------------)
    [## GitHub - wells-wood-research/timed-design：使用深度学习和工具进行蛋白质序列设计…'
- en: timed-design is a library to use protein sequence design models and analyse
    predictions. We feature retrained Keras…
  id: totrans-212
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: timed-design 是一个用于蛋白质序列设计模型和分析预测的库。我们展示了重新训练的 Keras…
- en: 'github.com](https://github.com/wells-wood-research/timed-design?source=post_page-----6c81faba670d--------------------------------)
    [](https://academic.oup.com/bioinformatics/article/39/1/btad027/6986968?source=post_page-----6c81faba670d--------------------------------)
    [## PDBench: evaluating computational methods for protein-sequence design'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: github.com](https://github.com/wells-wood-research/timed-design?source=post_page-----6c81faba670d--------------------------------)
    [](https://academic.oup.com/bioinformatics/article/39/1/btad027/6986968?source=post_page-----6c81faba670d--------------------------------)
    [## PDBench：评估蛋白质序列设计的计算方法
- en: The goal of protein design is to create novel amino acid sequences with useful
    properties and functions. An important…
  id: totrans-214
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 蛋白质设计的目标是创建具有有用特性和功能的新型氨基酸序列。一项重要的…
- en: academic.oup.com](https://academic.oup.com/bioinformatics/article/39/1/btad027/6986968?source=post_page-----6c81faba670d--------------------------------)
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: academic.oup.com](https://academic.oup.com/bioinformatics/article/39/1/btad027/6986968?source=post_page-----6c81faba670d--------------------------------)
- en: From the Author
  id: totrans-216
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 作者寄语
- en: '[](https://pub.towardsai.net/latent-diffusion-explained-simply-with-pok%C3%A9mon-3ebe15a3a019?source=post_page-----6c81faba670d--------------------------------)
    [## Latent Diffusion Explained Simply (with Pokémon)'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://pub.towardsai.net/latent-diffusion-explained-simply-with-pok%C3%A9mon-3ebe15a3a019?source=post_page-----6c81faba670d--------------------------------)
    [## 潜在扩散简单解释（以宝可梦为例）'
- en: From Text to Image, Image to Image and Inpainting — the Latent Diffusion revolution
  id: totrans-218
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 从文本到图像、图像到图像和修复——潜在扩散的革命
- en: pub.towardsai.net](https://pub.towardsai.net/latent-diffusion-explained-simply-with-pok%C3%A9mon-3ebe15a3a019?source=post_page-----6c81faba670d--------------------------------)
    [](https://betterhumans.pub/obsidian-tutorial-for-academic-writing-87b038060522?source=post_page-----6c81faba670d--------------------------------)
    [## Obsidian Tutorial for Academic Writing
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: pub.towardsai.net](https://pub.towardsai.net/latent-diffusion-explained-simply-with-pok%C3%A9mon-3ebe15a3a019?source=post_page-----6c81faba670d--------------------------------)
    [](https://betterhumans.pub/obsidian-tutorial-for-academic-writing-87b038060522?source=post_page-----6c81faba670d--------------------------------)
    [## Obsidian 学术写作教程
- en: Practical writing Tips for Manuscript Writing, Posters, and exporting citations
    to Word, PDF, and Latex.
  id: totrans-220
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 手稿写作、海报制作和将引用导出到 Word、PDF 和 LaTeX 的实用写作技巧。
- en: betterhumans.pub](https://betterhumans.pub/obsidian-tutorial-for-academic-writing-87b038060522?source=post_page-----6c81faba670d--------------------------------)
    [](https://betterhumans.pub/how-to-boost-your-productivity-for-scientific-research-using-obsidian-fe85c98c63c8?source=post_page-----6c81faba670d--------------------------------)
    [## How to Boost Your Productivity for Scientific Research Using Obsidian
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: betterhumans.pub](https://betterhumans.pub/obsidian-tutorial-for-academic-writing-87b038060522?source=post_page-----6c81faba670d--------------------------------)
    [](https://betterhumans.pub/how-to-boost-your-productivity-for-scientific-research-using-obsidian-fe85c98c63c8?source=post_page-----6c81faba670d--------------------------------)
    [## 如何利用 Obsidian 提升科研生产力
- en: Tools and workflows for managing your zettelkasten, projects, reading lists,
    notes, and inspiration during your PhD.
  id: totrans-222
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 管理你的 Zettelkasten、项目、阅读列表、笔记和在博士研究期间的灵感的工具和工作流。
- en: betterhumans.pub](https://betterhumans.pub/how-to-boost-your-productivity-for-scientific-research-using-obsidian-fe85c98c63c8?source=post_page-----6c81faba670d--------------------------------)
    [](https://betterhumans.pub/20-macos-apps-to-boost-your-productivity-74accb372c9c?source=post_page-----6c81faba670d--------------------------------)
    [## 20+ MacOS Apps to Boost Your Productivity
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: '[betterhumans.pub](https://betterhumans.pub/how-to-boost-your-productivity-for-scientific-research-using-obsidian-fe85c98c63c8?source=post_page-----6c81faba670d--------------------------------)
    [](https://betterhumans.pub/20-macos-apps-to-boost-your-productivity-74accb372c9c?source=post_page-----6c81faba670d--------------------------------)
    [## 20多个MacOS应用程序提升你的生产力'
- en: A collection of 20+ apps to significantly boost your productivity on MacOS
  id: totrans-224
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 一系列20多个应用程序，能显著提升你在MacOS上的生产力。
- en: betterhumans.pub](https://betterhumans.pub/20-macos-apps-to-boost-your-productivity-74accb372c9c?source=post_page-----6c81faba670d--------------------------------)
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: '[betterhumans.pub](https://betterhumans.pub/20-macos-apps-to-boost-your-productivity-74accb372c9c?source=post_page-----6c81faba670d--------------------------------)'
