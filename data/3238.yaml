- en: Data is the Foundation of Language Models
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据是语言模型的基础
- en: 原文：[https://towardsdatascience.com/data-is-the-foundation-of-language-models-52e9f48c07f5?source=collection_archive---------2-----------------------#2023-10-29](https://towardsdatascience.com/data-is-the-foundation-of-language-models-52e9f48c07f5?source=collection_archive---------2-----------------------#2023-10-29)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/data-is-the-foundation-of-language-models-52e9f48c07f5?source=collection_archive---------2-----------------------#2023-10-29](https://towardsdatascience.com/data-is-the-foundation-of-language-models-52e9f48c07f5?source=collection_archive---------2-----------------------#2023-10-29)
- en: How high-quality data impacts every aspect of the LLM training pipeline…
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 高质量数据如何影响 LLM 训练流程的每一个方面…
- en: '[](https://wolfecameron.medium.com/?source=post_page-----52e9f48c07f5--------------------------------)[![Cameron
    R. Wolfe, Ph.D.](../Images/52bb88d7cf1105501be2fae5ccbe7a03.png)](https://wolfecameron.medium.com/?source=post_page-----52e9f48c07f5--------------------------------)[](https://towardsdatascience.com/?source=post_page-----52e9f48c07f5--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----52e9f48c07f5--------------------------------)
    [Cameron R. Wolfe, Ph.D.](https://wolfecameron.medium.com/?source=post_page-----52e9f48c07f5--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://wolfecameron.medium.com/?source=post_page-----52e9f48c07f5--------------------------------)[![Cameron
    R. Wolfe, Ph.D.](../Images/52bb88d7cf1105501be2fae5ccbe7a03.png)](https://wolfecameron.medium.com/?source=post_page-----52e9f48c07f5--------------------------------)[](https://towardsdatascience.com/?source=post_page-----52e9f48c07f5--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----52e9f48c07f5--------------------------------)
    [Cameron R. Wolfe, Ph.D.](https://wolfecameron.medium.com/?source=post_page-----52e9f48c07f5--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F28aa6026c553&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdata-is-the-foundation-of-language-models-52e9f48c07f5&user=Cameron+R.+Wolfe%2C+Ph.D.&userId=28aa6026c553&source=post_page-28aa6026c553----52e9f48c07f5---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----52e9f48c07f5--------------------------------)
    ·16 min read·Oct 29, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F52e9f48c07f5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdata-is-the-foundation-of-language-models-52e9f48c07f5&user=Cameron+R.+Wolfe%2C+Ph.D.&userId=28aa6026c553&source=-----52e9f48c07f5---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F28aa6026c553&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdata-is-the-foundation-of-language-models-52e9f48c07f5&user=Cameron+R.+Wolfe%2C+Ph.D.&userId=28aa6026c553&source=post_page-28aa6026c553----52e9f48c07f5---------------------post_header-----------)
    发布于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----52e9f48c07f5--------------------------------)
    · 16 分钟阅读 · 2023 年 10 月 29 日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F52e9f48c07f5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdata-is-the-foundation-of-language-models-52e9f48c07f5&user=Cameron+R.+Wolfe%2C+Ph.D.&userId=28aa6026c553&source=-----52e9f48c07f5---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F52e9f48c07f5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdata-is-the-foundation-of-language-models-52e9f48c07f5&source=-----52e9f48c07f5---------------------bookmark_footer-----------)![](../Images/0cdc29d0203d7225c4541733960f9b92.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F52e9f48c07f5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdata-is-the-foundation-of-language-models-52e9f48c07f5&source=-----52e9f48c07f5---------------------bookmark_footer-----------)![](../Images/0cdc29d0203d7225c4541733960f9b92.png)'
- en: (Photo by [Joshua Sortino](https://unsplash.com/@sortino?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)
    on [Unsplash](https://unsplash.com/photos/worms-eye-view-photography-of-ceiling-LqKhnDzSF-8?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash))
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: (图片由 [Joshua Sortino](https://unsplash.com/@sortino?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)
    提供，[Unsplash](https://unsplash.com/photos/worms-eye-view-photography-of-ceiling-LqKhnDzSF-8?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash))
- en: Large Language Models (LLMs) have been around for quite some time, but only
    recently has their impressive performance warranted significant attention from
    the broader AI community. With this in mind, we might begin to question the origin
    of the current LLM movement. *What was it that actually made recent models so
    impressive compared to their predecessors?* Although some may argue a variety
    of different factors, one especially impactful advancement was the ability to
    perform alignment. In other words, we figured out how to train LLMs to not just
    output the most likely next word, but to output text will satisfy the goals of
    a human, whether it be by following an instruction or retrieving important information.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）已经存在了一段时间，但只有最近，它们的惊人表现才引起了更广泛的AI社区的显著关注。考虑到这一点，我们可能会开始质疑当前LLM运动的起源。*是什么让最近的模型相比其前身如此令人印象深刻？*
    尽管有人可能会争论各种不同的因素，但一个特别有影响力的进展是能够执行对齐。换句话说，我们找到了如何训练LLMs，不仅仅输出最可能的下一个词，而是输出能满足人类目标的文本，无论是通过遵循指令还是检索重要信息。
- en: “We hypothesize that alignment can be a simple process where the model learns
    the style or format for interacting with users, to expose the knowledge and capabilities
    that were already acquired during pretraining” *— from [1]*
  id: totrans-10
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “我们假设对齐可以是一个简单的过程，其中模型学习与用户交互的风格或格式，以展示在预训练期间已经获得的知识和能力” *— 引自 [1]*
- en: This overview will study the role and impact of alignment, as well as the interplay
    between alignment and pre-training. Interestingly, these ideas were explored by
    the recent LIMA model [1], which performs alignment by simply fine-tuning a pre-trained
    LLM over a semi-manually…
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 本综述将研究对齐的角色和影响，以及对齐与预训练之间的相互作用。有趣的是，这些观点在最近的LIMA模型[1]中得到了探讨，该模型通过简单地对预训练的LLM进行微调来实现对齐……
