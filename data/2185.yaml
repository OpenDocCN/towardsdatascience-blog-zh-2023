- en: Deploying Falcon-7B Into Production
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 部署 Falcon-7B 进入生产环境
- en: 原文：[https://towardsdatascience.com/deploying-falcon-7b-into-production-6dd28bb79373?source=collection_archive---------1-----------------------#2023-07-07](https://towardsdatascience.com/deploying-falcon-7b-into-production-6dd28bb79373?source=collection_archive---------1-----------------------#2023-07-07)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/deploying-falcon-7b-into-production-6dd28bb79373?source=collection_archive---------1-----------------------#2023-07-07](https://towardsdatascience.com/deploying-falcon-7b-into-production-6dd28bb79373?source=collection_archive---------1-----------------------#2023-07-07)
- en: A step-by-step tutorial
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 一步步教程
- en: Running Falcon-7B in the cloud as a microservice
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在云端以微服务形式运行 Falcon-7B
- en: '[](https://medium.com/@het.trivedi05?source=post_page-----6dd28bb79373--------------------------------)[![Het
    Trivedi](../Images/f6f11a66f60cacc6b553c7d1682b2fc6.png)](https://medium.com/@het.trivedi05?source=post_page-----6dd28bb79373--------------------------------)[](https://towardsdatascience.com/?source=post_page-----6dd28bb79373--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----6dd28bb79373--------------------------------)
    [Het Trivedi](https://medium.com/@het.trivedi05?source=post_page-----6dd28bb79373--------------------------------)'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@het.trivedi05?source=post_page-----6dd28bb79373--------------------------------)[![Het
    Trivedi](../Images/f6f11a66f60cacc6b553c7d1682b2fc6.png)](https://medium.com/@het.trivedi05?source=post_page-----6dd28bb79373--------------------------------)[](https://towardsdatascience.com/?source=post_page-----6dd28bb79373--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----6dd28bb79373--------------------------------)
    [Het Trivedi](https://medium.com/@het.trivedi05?source=post_page-----6dd28bb79373--------------------------------)'
- en: ·
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fce8ebd0c262c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeploying-falcon-7b-into-production-6dd28bb79373&user=Het+Trivedi&userId=ce8ebd0c262c&source=post_page-ce8ebd0c262c----6dd28bb79373---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----6dd28bb79373--------------------------------)
    ·16 min read·Jul 7, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F6dd28bb79373&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeploying-falcon-7b-into-production-6dd28bb79373&user=Het+Trivedi&userId=ce8ebd0c262c&source=-----6dd28bb79373---------------------clap_footer-----------)'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fce8ebd0c262c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeploying-falcon-7b-into-production-6dd28bb79373&user=Het+Trivedi&userId=ce8ebd0c262c&source=post_page-ce8ebd0c262c----6dd28bb79373---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----6dd28bb79373--------------------------------)
    ·16 分钟阅读·2023年7月7日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F6dd28bb79373&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeploying-falcon-7b-into-production-6dd28bb79373&user=Het+Trivedi&userId=ce8ebd0c262c&source=-----6dd28bb79373---------------------clap_footer-----------)'
- en: --
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F6dd28bb79373&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeploying-falcon-7b-into-production-6dd28bb79373&source=-----6dd28bb79373---------------------bookmark_footer-----------)![](../Images/10a0800c6f473b08d7985ccc8c5969f9.png)'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F6dd28bb79373&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeploying-falcon-7b-into-production-6dd28bb79373&source=-----6dd28bb79373---------------------bookmark_footer-----------)![](../Images/10a0800c6f473b08d7985ccc8c5969f9.png)'
- en: Image by author-Created using Midjourney
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供-使用 Midjourney 创建
- en: Background
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 背景
- en: By now, we’ve seen the capabilities of ChatGPT and what it has to offer. However,
    for enterprise use, closed-source models like ChatGPT may pose a risk as enterprises
    have no control over their data. OpenAI claims that user data is not stored or
    used for training models, but there is no guarantee that data will not be leaked
    in some way.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经了解了 ChatGPT 的能力及其提供的功能。然而，对于企业使用，像 ChatGPT 这样的闭源模型可能存在风险，因为企业无法控制其数据。OpenAI
    声称用户数据不会被存储或用于训练模型，但无法保证数据不会以某种方式泄露。
- en: To combat some of the issues related to closed-source models, researchers are
    rushing to build open-source LLMs that rival models like ChatGPT. With open-source
    models, enterprises can host the models themselves in a secure cloud environment,
    mitigating the risk of data leakage. On top of that, you get full transparency
    in the inner workings of the model, which helps build more trust with AI in general.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 为了应对与闭源模型相关的一些问题，研究人员正急于构建与ChatGPT等模型相媲美的开源LLM。使用开源模型，企业可以在安全的云环境中自行托管这些模型，从而降低数据泄露的风险。此外，你还可以完全透明地了解模型的内部工作，这有助于建立对AI的更多信任。
- en: With the recent advancements in open-source LLMs, it’s tempting to try out new
    models and see how they stack up against closed-source models like ChatGPT.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 随着开源LLM的最新进展，试用新的模型并查看它们如何与像ChatGPT这样的闭源模型竞争变得很有诱惑力。
- en: However, running open-source models today poses significant barriers. It’s so
    much easier to call the ChatGPT API than to figure out how to run an open-source
    LLM.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，今天运行开源模型存在显著的障碍。调用ChatGPT API要比弄清楚如何运行开源LLM容易得多。
- en: In this article, my goal is to break these barriers by showing how you can run
    open-source models like the Falcon-7B model in the cloud in a production-like
    setting. We will be able to access these models via an API endpoint similar to
    ChatGPT.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我的目标是通过展示如何在生产环境中运行像Falcon-7B这样的开源模型，来打破这些障碍。我们将能够通过类似于ChatGPT的API端点访问这些模型。
- en: Challenges
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 挑战
- en: One of the significant challenges to running open-source models is the lack
    of computing resources. Even a “small” model like the Falcon 7B requires a GPU
    to run.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 运行开源模型的一个重大挑战是缺乏计算资源。即使是像Falcon 7B这样的“小型”模型，也需要GPU才能运行。
- en: To solve this problem, we can leverage the GPUs in the cloud. But this poses
    another challenge. How do we containerize our LLM? How do we enable GPU support?
    Enabling GPU support can be tricky because it requires knowledge of CUDA. Working
    with CUDA can be a pain in the neck because you have to figure out how to install
    the proper CUDA dependencies and which versions are compatible.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这个问题，我们可以利用云中的GPU。但这带来了另一个挑战。我们如何容器化我们的LLM？我们如何启用GPU支持？启用GPU支持可能很棘手，因为这需要了解CUDA。处理CUDA可能会令人头痛，因为你需要弄清楚如何安装正确的CUDA依赖以及哪些版本是兼容的。
- en: So, to avoid the CUDA death trap, many companies have created solutions to easily
    containerize models while enabling GPU support. For this blog post, we’ll be using
    an open-source tool called [Truss](https://github.com/basetenlabs/truss) to help
    us easily containerize our LLM without much hassle.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，为了避免CUDA死循环，许多公司已经创建了解决方案来轻松容器化模型，同时支持GPU。对于这篇博客文章，我们将使用一个名为[Truss](https://github.com/basetenlabs/truss)的开源工具来帮助我们轻松容器化我们的LLM，而不费多少劲。
- en: Truss allows developers to easily containerize models built using any framework.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: Truss允许开发者轻松容器化使用任何框架构建的模型。
- en: Why use Truss?
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 为什么使用Truss？
- en: '![](../Images/994154aab9fb051480824ccacfa116e3.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/994154aab9fb051480824ccacfa116e3.png)'
- en: Truss — [https://truss.baseten.co/e2e](https://truss.baseten.co/e2e)
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: Truss — [https://truss.baseten.co/e2e](https://truss.baseten.co/e2e)
- en: 'Truss has a lot of useful features right out of the box such as:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: Truss拥有许多开箱即用的有用功能，例如：
- en: Turning your Python model into a microservice with a production-ready API endpoint
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将你的Python模型转换为具有生产就绪API端点的微服务
- en: Freezing dependencies via Docker
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过Docker冻结依赖
- en: Supporting inference on GPUs
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 支持GPU上的推理
- en: Simple pre-processing and post-processing for the model
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型的简单预处理和后处理
- en: Easy and secure secrets management
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 简单且安全的秘密管理
- en: I’ve used Truss before to deploy machine learning models and the process is
    quite smooth and simple. Truss automatically creates your dockerfile and manages
    Python dependencies. All we have to do is provide the code for our model.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 我之前使用过Truss来部署机器学习模型，过程非常顺利和简单。Truss自动创建你的dockerfile并管理Python依赖。我们要做的就是提供我们的模型代码。
- en: '![](../Images/53e388d9c34388bef656ff02ad0db9d9.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/53e388d9c34388bef656ff02ad0db9d9.png)'
- en: The main reason we want to use a tool like Truss is that it becomes much easier
    to deploy our model with GPU support.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望使用像Truss这样的工具的主要原因是，它使得部署支持GPU的模型变得更加容易。
- en: 'Note: I have not received any sponsorship from Baseten to promote their content
    nor am I associated with them in any way. I’m under no influence in any way from
    Baseten or Truss to write this article. I simply found their open source project
    to be cool and useful.'
  id: totrans-33
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 注意：我没有收到 Baseten 的赞助来推广他们的内容，也没有与他们有任何关联。我没有受到 Baseten 或 Truss 任何形式的影响来撰写这篇文章。我只是发现他们的开源项目很酷且有用。
- en: The plan
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 计划
- en: 'Here’s what I’ll be covering in this blog post:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇博客中，我将涵盖以下内容：
- en: Set up Falcon 7B locally using Truss
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 Truss 在本地设置 Falcon 7B
- en: Run the model locally if you have a GPU(I have an RTX 3080)
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果你有 GPU（我有一块 RTX 3080），可以在本地运行模型
- en: Containerize the model and run it using docker
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将模型容器化并使用 docker 运行
- en: Create a GPU-enabled kubernetes cluster in Google Cloud to run our model
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 Google Cloud 上创建一个启用 GPU 的 Kubernetes 集群来运行我们的模型
- en: Don’t worry if you don’t have a GPU for step 2, you will still be able to run
    the model in the cloud.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你在第二步没有 GPU，也不用担心，你仍然可以在云端运行模型。
- en: 'Here is the Github repo containing the code if you want to follow along:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想跟随，请查看包含代码的 Github 仓库：
- en: '[](https://github.com/htrivedi99/falcon-7b-truss?source=post_page-----6dd28bb79373--------------------------------)
    [## GitHub - htrivedi99/falcon-7b-truss'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '[## GitHub - htrivedi99/falcon-7b-truss](https://github.com/htrivedi99/falcon-7b-truss?source=post_page-----6dd28bb79373--------------------------------)'
- en: Contribute to htrivedi99/falcon-7b-truss development by creating an account
    on GitHub.
  id: totrans-43
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 通过在 GitHub 上创建账户，贡献于 htrivedi99/falcon-7b-truss 的开发。
- en: github.com](https://github.com/htrivedi99/falcon-7b-truss?source=post_page-----6dd28bb79373--------------------------------)
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '[github.com](https://github.com/htrivedi99/falcon-7b-truss?source=post_page-----6dd28bb79373--------------------------------)'
- en: Let’s get started!
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始吧！
- en: 'Step 1: Falcon 7B local setup using Truss'
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第一步：使用 Truss 本地设置 Falcon 7B
- en: First, we’ll need to create a project with Python version ≥ 3.8
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要创建一个 Python 版本 ≥ 3.8 的项目
- en: 'We’ll be downloading the model from hugging face and packaging it using Truss.
    Here are the dependencies we’ll need to install:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从 hugging face 下载模型，并使用 Truss 打包它。以下是我们需要安装的依赖项：
- en: '[PRE0]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Inside your Python project create a script called `main.py` . This is a temporary
    script we’ll be using to work with truss.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在你的 Python 项目中创建一个名为 `main.py` 的脚本。这是我们将用于与 truss 进行交互的临时脚本。
- en: 'Next, we’ll set up our Truss package by running the following command in the
    terminal:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将在终端中运行以下命令来设置 Truss 包：
- en: '[PRE1]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'If you’re prompted to create a new truss, press ‘y’. Once that’s complete,
    you should see a new directory called `falcon_7b_truss` . Inside that directory,
    there will be some autogenerated files and folders. There are a couple of things
    we need to fill out: `model.py` which is nested under the package `model` and
    `config.yaml` .'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你被提示创建一个新的 truss，按下‘y’。完成后，你应该会看到一个名为 `falcon_7b_truss` 的新目录。在该目录内，将有一些自动生成的文件和文件夹。我们需要填写几样东西：`model.py`，它嵌套在包
    `model` 下，以及 `config.yaml`。
- en: '[PRE2]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: As I mentioned previously, Truss only needs our model’s code, it takes care
    of all the other things automatically. We’ll be writing the code inside `model.py`
    , but it has to be written in a specific format.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 如我之前提到的，Truss 只需要我们模型的代码，它会自动处理所有其他事情。我们将在 `model.py` 内编写代码，但必须按照特定格式编写。
- en: 'Truss expects each model to support at least three functions: `__init__` ,
    `load` , and `predict` .'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: Truss 期望每个模型支持至少三个函数：`__init__` 、`load` 和 `predict`。
- en: '`__init__` is primarily used for creating class variables'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`__init__` 主要用于创建类变量'
- en: '`load` is where we’ll download the model from hugging face'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`load` 是我们将从 hugging face 下载模型的地方'
- en: '`predict` is where we’ll call our model'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`predict` 是我们调用模型的地方'
- en: 'Here’s the full code for `model.py` :'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 这是 `model.py` 的完整代码：
- en: '[PRE3]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'What’s happening here:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 这里发生了什么：
- en: '`MODEL_NAME` is the model we’ll be using which in our case is the `falcon-7b-instruct`
    model'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`MODEL_NAME` 是我们将使用的模型，在我们的情况下是 `falcon-7b-instruct` 模型'
- en: Inside `load`, we download the model from hugging face in 8bit. The reason we
    want 8bit is that the model uses significantly less memory on our GPU when quantized.
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 `load` 内，我们以 8bit 下载 hugging face 上的模型。我们选择 8bit 的原因是，当量化时，模型在 GPU 上占用的内存显著减少。
- en: Also, loading the model in 8-bit is necessary if you want to run the model locally
    on a GPU with less than 13GB VRAM.
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果你想在 VRAM 少于 13GB 的 GPU 上本地运行模型，则加载 8-bit 模型是必要的。
- en: The `predict` function accepts a JSON request as a parameter and calls the model
    using `self.pipeline` . The `torch.no_grad` tells Pytorch that we are in inference
    mode, not training mode.
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`predict` 函数接受一个 JSON 请求作为参数，并使用 `self.pipeline` 调用模型。`torch.no_grad` 告诉 Pytorch
    我们处于推理模式，而非训练模式。'
- en: Cool! That’s all we need to setup our model.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 太棒了！这就是我们设置模型所需的一切。
- en: 'Step 2: Running the model locally (Optional)'
  id: totrans-68
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第2步：在本地运行模型（可选）
- en: If you have an Nvidia GPU with more than 8GB of VRAM you will be able to run
    this model locally.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你有一块超过8GB VRAM的Nvidia GPU，你将能够在本地运行这个模型。
- en: If not, feel free to move on to the next step.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 如果没有，请随意继续下一步。
- en: We’ll need to download some more dependencies to run the model locally. Before
    you download the dependencies you need to make sure you have CUDA and the right
    CUDA drivers installed.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要下载一些更多的依赖项以在本地运行模型。在下载依赖项之前，你需要确保已安装CUDA和正确的CUDA驱动程序。
- en: Because we’re trying to run the model locally, Truss won’t be able to help us
    manage the CUDA madness.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们尝试在本地运行模型，Truss 将无法帮助我们管理 CUDA 问题。
- en: '[PRE4]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Next, inside `main.py` the script we created outside of the `falcon_7b_truss`
    directory, we need to load our truss.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，在`main.py`脚本中，我们需要加载我们的truss。
- en: 'Here is the code for `main.py` :'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 这是`main.py`的代码：
- en: '[PRE5]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'What’s happening here:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 这里发生了什么：
- en: If you recall, the `falcon_7b_truss` directory was created by truss. We can
    load that entire package, including the model and dependencies using `truss.load`
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果你记得，`falcon_7b_truss`目录是由truss创建的。我们可以使用`truss.load`加载整个包，包括模型和依赖项。
- en: Once, we’ve loaded our package we can simply call the `predict` method to get
    the models output
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一旦加载了包，我们可以简单地调用`predict`方法来获取模型的输出。
- en: Run `main.py` to get the output from the model.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 运行`main.py`以获取模型的输出。
- en: 'This model files are ~15 GB in size so it might take 5–10 minutes to download
    the model. After running the script you should see an output like this:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 这些模型文件的大小约为15 GB，因此下载模型可能需要5到10分钟。运行脚本后，你应该会看到类似的输出：
- en: '[PRE6]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Step 3: Containerizing the model using docker'
  id: totrans-83
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第3步：使用docker容器化模型
- en: Usually, when people containerize a model, they take the model binary and the
    Python dependencies and wrap it all up using a Flask or Fast API server.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，当人们容器化模型时，他们会将模型二进制文件和Python依赖项打包起来，使用Flask或Fast API服务器进行封装。
- en: A lot of this is boilerplate and we don’t want to do it ourselves. Truss will
    take care of it. We’ve already provided the model, Truss will create the server,
    so the only thing left to do is provide the Python dependencies.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 很多内容是样板代码，我们不想自己去做。Truss会处理这些。我们已经提供了模型，Truss会创建服务器，所以剩下的就是提供Python依赖项。
- en: The `config.yaml` holds the configuration for our model. This is where we can
    add the dependencies for our model. The config file already comes with most of
    the things we need, but we’ll need to add a few things.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '`config.yaml`保存了模型的配置。在这里，我们可以添加模型的依赖项。配置文件已经包含了大部分我们需要的内容，但我们还需要添加一些东西。'
- en: 'Here is what you need to add to `config.yaml`:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 这是你需要添加到`config.yaml`中的内容：
- en: '[PRE7]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: So the main thing we added is the `requirements` . All of the dependencies listed
    are required to download and run the model.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 我们添加的主要内容是`requirements`。列出的所有依赖项都是下载和运行模型所必需的。
- en: 'The other important thing we added is the `resources` . The `use_gpu: true`
    is essential, because this tells Truss to create a Dockerfile for us with GPU
    support enabled.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '我们添加的另一个重要内容是`resources`。`use_gpu: true`是必需的，因为这告诉Truss为我们创建一个启用GPU支持的Dockerfile。'
- en: That’s it for the configuration.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是配置的全部内容。
- en: Next up, we’ll containerize our model. If you don’t know how to containerize
    a model using Docker, don’t worry Truss has got you covered.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将容器化我们的模型。如果你不知道如何使用Docker容器化模型，别担心，Truss已经为你准备好了。
- en: 'Inside the `main.py` file, we’ll tell Truss to package everything together.
    Here is the code you need:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在`main.py`文件中，我们将告诉Truss将所有内容打包在一起。你需要的代码如下：
- en: '[PRE8]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'What’s happening:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 发生了什么：
- en: First, we load our `falcon_7b_truss`
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 首先，我们加载我们的`falcon_7b_truss`
- en: Next, the `docker_build_setup` function handles all of the complicated stuff
    like creating the Dockerfile and setting up the Fast API server.
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 接下来，`docker_build_setup`函数处理所有复杂的内容，如创建Dockerfile和设置Fast API服务器。
- en: If you take a look at your `falcon_7b_truss` directory, you’ll see that many
    more files got generated. We don’t need to worry about how these files work because
    it will all get managed behind the scene.
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果你查看`falcon_7b_truss`目录，你会发现生成了更多文件。我们不需要担心这些文件如何工作，因为一切都会在幕后管理。
- en: 'At the end of the run, we get a docker command to build our docker image: `docker
    build falcon_7b_truss -t falcon-7b-model:latest`'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在运行结束时，我们得到一个构建 Docker 镜像的命令：`docker build falcon_7b_truss -t falcon-7b-model:latest`
- en: '![](../Images/beb90d0e1302c0c426824fa199948d4d.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/beb90d0e1302c0c426824fa199948d4d.png)'
- en: 'If you want to build the docker image go ahead and run the build command. The
    image is ~ 9 GB in size , so it might take a while to build. If you don’t want
    to build it but want to follow along you can use my image: `htrivedi05/truss-falcon-7b:latest`
    .'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想构建 Docker 镜像，请运行构建命令。镜像大小约为 9 GB，所以可能需要一些时间来构建。如果你不想构建它但想跟随，可以使用我的镜像：`htrivedi05/truss-falcon-7b:latest`。
- en: 'If you’re building the image yourself, you will need to tag it and push it
    to docker hub so that our containers in the cloud can pull the image. Here are
    the commands you will need to run once the image is built:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你自己构建镜像，你需要标记并将其推送到 Docker Hub，以便我们在云中的容器可以拉取这个镜像。以下是镜像构建完成后需要运行的命令：
- en: '[PRE9]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '[PRE10]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Awesome! We are ready to run our model in the cloud!
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 太棒了！我们已经准备好在云中运行我们的模型了！
- en: '**(Optional steps below for running the image locally with a GPU)**'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '**（以下是可选步骤，用于在本地使用 GPU 运行镜像）**'
- en: If you have an Nvidia GPU and want to run your containerized model **locally**
    with GPU support, you need to ensure docker is configured to use your GPU.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你有一张 Nvidia GPU 并且希望在本地**使用** GPU 支持运行你的容器化模型，你需要确保 Docker 已配置为使用你的 GPU。
- en: 'Open up a terminal and run the following commands:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 打开终端并运行以下命令：
- en: '[PRE11]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '[PRE12]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '[PRE13]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Now that your docker has been configured to access your GPU, here is how to
    run your container:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你的 Docker 已经配置为访问你的 GPU，下面是如何运行你的容器：
- en: '[PRE14]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Again, it will take a while to download the model. To make sure everything
    is working correctly you can check the container logs and you should see “THE
    DEVICE INFERENCE IS RUNNING ON IS: cuda”.'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '再次提醒，下载模型需要一些时间。为了确保一切正常，你可以检查容器日志，你应该会看到“THE DEVICE INFERENCE IS RUNNING ON
    IS: cuda”。'
- en: 'You can call the model via an API endpoint like so:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过 API 端点调用模型，如下所示：
- en: '[PRE15]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Step 4: Deploying the model into production'
  id: totrans-117
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第4步：将模型部署到生产环境
- en: I’m using the word “production” quite loosely here. We’re going to run our model
    in kubernetes where it can easily scale and handle variable amounts of traffic.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 我在这里使用“生产”这个词比较宽泛。我们将把模型运行在 Kubernetes 中，那里可以轻松扩展并处理可变量的流量。
- en: With that being said, there are a **TON** of configurations kubernetes has such
    as network policies, storage, config maps, load balancing, secrets management,
    etc.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 话虽如此，Kubernetes 有大量的配置，例如网络策略、存储、配置映射、负载均衡、秘密管理等。
- en: Even though kubernetes is built to “scale” and run “production” workloads, a
    lot of the production-level configurations you need don’t come out of the box.
    Covering those advanced kubernetes topics is out of the scope of this article
    and takes away from what we’re trying to accomplish here. So for this blog post,
    we’ll create a minimal cluster without all the bells and whistles.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 即使 Kubernetes 是为了“扩展”和运行“生产”工作负载而构建的，很多生产级配置你需要的并不是开箱即用的。覆盖这些高级 Kubernetes 主题超出了本文的范围，也会偏离我们在这里想要实现的目标。因此，在这篇博客文章中，我们将创建一个最小化的集群，而没有所有的附加功能。
- en: Without further ado, let’s create our cluster!
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 不再多说，让我们创建我们的集群吧！
- en: 'Prerequisites:'
  id: totrans-122
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 先决条件：
- en: Have a Google Cloud Account with a project
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 拥有一个带有项目的 Google Cloud 帐户
- en: Have the **gcloud** CLI installed on your machine
  id: totrans-124
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在你的机器上安装**gcloud** CLI
- en: Make sure you have enough quota to run a GPU enabled machine. You can check
    your quotas under **IAM & Admin**.
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 确保你有足够的配额来运行一个启用 GPU 的机器。你可以在**IAM & Admin**下检查你的配额。
- en: '![](../Images/0b5f428850fadcfa0cc0346b2f5ab4a0.png)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0b5f428850fadcfa0cc0346b2f5ab4a0.png)'
- en: Creating our GKE cluster
  id: totrans-127
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建我们的 GKE 集群
- en: 'We’ll be using Google’s kubernetes engine to create and manage our cluster.
    Ok, time for some **IMPORTANT** information:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用 Google 的 Kubernetes 引擎来创建和管理我们的集群。好，现在是一些**重要的**信息：
- en: Google’s kubernetes engine is **NOT** free. Google will not allow us to use
    a powerful GPU free of charge. With that being said, we are creating a single-node
    cluster with a less powerful GPU. It should not cost you more than $1–$2 for this
    experiment.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: Google 的 Kubernetes 引擎**不是**免费的。Google 不允许我们免费使用强大的 GPU。话虽如此，我们正在创建一个单节点集群，配备较不强大的
    GPU。这个实验的费用不应超过 1-2 美元。
- en: 'Here’s the configuration for the kubernetes cluster we’ll be running:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们将运行的 Kubernetes 集群的配置：
- en: 1 node, standard kubernetes cluster (not autopilot)
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 节点，标准 Kubernetes 集群（非自动驾驶）
- en: 1 Nvidia T4 GPU
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 张 Nvidia T4 GPU
- en: n1-standard-4 machine (4 vCPU, 15 GB memory)
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: n1-standard-4 机器（4 vCPU，15 GB 内存）
- en: All of this will run on a spot instance
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所有这些都将在一个临时实例上运行
- en: 'Note: If you’re in another region and don’t have access to the exact same resources,
    feel free to make modifications.'
  id: totrans-135
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 注意：如果你在其他地区并且无法访问完全相同的资源，请随意进行修改。
- en: '**Steps for creating the cluster:**'
  id: totrans-136
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**创建集群的步骤：**'
- en: Head over to [google cloud console](https://console.cloud.google.com/) and search
    for the service called *Kubernetes Engine*
  id: totrans-137
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 前往[Google Cloud Console](https://console.cloud.google.com/)并搜索名为*Kubernetes
    Engine*的服务
- en: '![](../Images/09cff563a680d73a6527f4901dd44a00.png)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/09cff563a680d73a6527f4901dd44a00.png)'
- en: 2\. Click the *CREATE* button
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 2\. 点击*创建*按钮
- en: Make sure you are creating a standard cluster, not an autopilot cluster. It
    should say *Create a kubernetes cluster* at the top.
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 确保你创建的是标准集群，而不是自动驾驶集群。顶部应该显示*创建一个kubernetes集群*。
- en: 3\. Cluster basics
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 3\. 集群基本信息
- en: Inside the cluster basics tab, we don’t want to change much. Just give your
    cluster a name. You don’t need to change the zone or control plane.
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在集群基本信息选项卡中，我们不需要做太多更改。只需给你的集群命名。你不需要更改区域或控制平面。
- en: '![](../Images/a55be1fba664346c690ba7d0ec344ece.png)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a55be1fba664346c690ba7d0ec344ece.png)'
- en: 4\. Click on the **default-pool** tab and change the number of nodes to 1
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 4\. 点击**default-pool**选项卡，将节点数更改为1
- en: '![](../Images/ca67dee00ace8d272e0b9815bec29c55.png)'
  id: totrans-145
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ca67dee00ace8d272e0b9815bec29c55.png)'
- en: 5\. Under default-pool click the **Nodes** tab in the left-hand sidebar
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 5\. 在default-pool下，点击左侧边栏中的**节点**选项卡
- en: Change the **Machine Configuration** from **General Purpose** to **GPU**
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将**机器配置**从**通用型**更改为**GPU**
- en: Select the **Nvidia T4** as the **GPU type** and set **1** for the quantity
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择**Nvidia T4**作为**GPU类型**，并将数量设置为**1**
- en: Enable GPU timeshare (even though we won’t be using this feature)
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 启用GPU共享（尽管我们不会使用这个功能）
- en: Set the **Max shared clients per GPU** to 8
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将**每GPU最大共享客户端**设置为8
- en: For the **Machine Type**, select the **n1-standard-4 (4 vCPU, 15 GB memory)**
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于**机器类型**，选择**n1-standard-4（4 vCPU，15 GB 内存）**
- en: Change the **Boot disk size** to 50
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将**启动磁盘大小**更改为50
- en: 'Scroll down to the very bottom and check-mark where it says: **Enable nodes
    on spot VMs**'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 向下滚动到底部，勾选**启用临时虚拟机上的节点**
- en: '![](../Images/e26291a6c9e431d53f272eed05ba76ae.png)![](../Images/a23583fdadf90654922793c10114f036.png)![](../Images/210268f2273132cf85b94e48e7d4bfb4.png)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e26291a6c9e431d53f272eed05ba76ae.png)![](../Images/a23583fdadf90654922793c10114f036.png)![](../Images/210268f2273132cf85b94e48e7d4bfb4.png)'
- en: 'Here is a screen-shot of the estimated price I got for this cluster:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我为这个集群获得的估算价格的屏幕截图：
- en: '![](../Images/9d1d9c07e0380efa2b3be818df060653.png)'
  id: totrans-156
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9d1d9c07e0380efa2b3be818df060653.png)'
- en: Once you’ve configured the cluster go ahead and create it.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 配置完集群后，继续创建它。
- en: 'It’ll take a few minutes for Google to set everything up. After your cluster
    is up and running, we need to connect to it. Open up your terminal and run the
    following commands:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: Google设置一切需要几分钟时间。集群启动并运行后，我们需要连接到它。打开终端并运行以下命令：
- en: '[PRE16]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '[PRE17]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'If you used a different zone of cluster name, update those accordingly. To
    check that we’re connected run this command:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你使用了不同的区域或集群名称，请相应更新。要检查我们是否连接成功，运行以下命令：
- en: '[PRE18]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'You should see 1 node appear in your terminal. Even though our cluster has
    a GPU, it’s missing some Nvidia drivers which we’ll have to install. Thankfully,
    installing them is easy. Run the following command to install the drivers:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该会在终端中看到1个节点。尽管我们的集群有GPU，但缺少一些Nvidia驱动程序，我们需要安装它们。幸运的是，安装过程很简单。运行以下命令来安装驱动程序：
- en: '[PRE19]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Sweet! We are finally ready to deploy our model.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 太棒了！我们终于准备好部署模型了。
- en: Deploying the model
  id: totrans-166
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 部署模型
- en: To deploy our model onto our cluster we need to create a **kubernetes deployment**.
    A kubernetes deployment allows us to manage instances of our containerized model.
    I won’t go too deep into kubernetes or how to write yaml files as it’s out of
    scope.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 要将模型部署到集群中，我们需要创建一个**kubernetes部署**。一个kubernetes部署允许我们管理容器化模型的实例。我不会深入探讨kubernetes或如何编写yaml文件，因为这超出了范围。
- en: 'You need to create a file called `truss-falcon-deployment.yaml` . Open that
    file and paste in the following contents:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 你需要创建一个名为`truss-falcon-deployment.yaml`的文件。打开该文件并粘贴以下内容：
- en: '[PRE20]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'What’s happening:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 发生了什么：
- en: 'We are telling kubernetes that we want to create pods with our `falcon-7b-model`
    image. Make sure you replace `<your_docker_id>` with your actual id. If you didn’t
    create your own docker image and want to use mine instead, replace it with the
    following: `htrivedi05/truss-falcon-7b:latest`'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们告诉kubernetes我们想用`falcon-7b-model`镜像创建pods。确保将`<your_docker_id>`替换为你的实际id。如果你没有创建自己的docker镜像而想使用我的镜像，请将其替换为：`htrivedi05/truss-falcon-7b:latest`
- en: 'We are enabling GPU access for our container by setting a resource limit `nvidia.com/gpu:
    1` . This tells kubernetes to request only one GPU for our container'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '我们通过设置资源限制`nvidia.com/gpu: 1`来启用容器的GPU访问。这告诉kubernetes只请求一个GPU给我们的容器'
- en: To interact with our model, we need to create a kubernetes service that will
    run on port 8080
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 要与我们的模型交互，我们需要创建一个将在端口8080上运行的kubernetes服务。
- en: 'Create the deployment by running the following command in your terminal:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 在终端中运行以下命令以创建部署：
- en: '[PRE21]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'If you run the command:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你运行命令：
- en: '[PRE22]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'You should see something like this:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该看到如下内容：
- en: '[PRE23]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'It will take a few minutes for the deployment to change to the ready state.
    Remember the model has to get downloaded from hugging face each time the container
    restarts. You can check the progress of your container by running the following
    command:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 部署变为准备状态需要几分钟时间。请记住，每次容器重启时，模型都需要从hugging face下载。你可以通过运行以下命令来检查容器的进度：
- en: '[PRE24]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '[PRE25]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Change the pod name accordingly.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 相应地更改pod名称。
- en: 'There are a few things you want to look for in the logs:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 在日志中，你需要查看以下几个方面：
- en: 'Look for the print statement **THE DEVICE INFERENCE IS RUNNING ON IS: cuda**.
    This confirms that our container is properly connected to the GPU.'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '查找打印语句**THE DEVICE INFERENCE IS RUNNING ON IS: cuda**。这确认了我们的容器已正确连接到GPU。'
- en: Next, you should see some print statements regarding the model files being downloaded
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 接下来，你应该看到一些关于模型文件下载的打印语句。
- en: '[PRE26]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Once the model is downloaded and Truss has created the microservice you should
    see the following output at the end of your logs:'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一旦模型下载完成且Truss创建了微服务，你应该在日志末尾看到以下输出：
- en: '[PRE27]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: From this message, we can confirm the model is loaded and ready for inference.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 从此消息中，我们可以确认模型已加载并准备好进行推断。
- en: Model inference
  id: totrans-191
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型推断
- en: We can’t call the model directly, instead, we have to call the model’s service
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不能直接调用模型，而是必须调用模型的服务。
- en: 'Run the following command to get the name of your service:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 运行以下命令以获取服务名称：
- en: '[PRE28]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Output:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：
- en: '[PRE29]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'The `truss-falcon-7b-service` is the one we want to call. To make the service
    accessible we need to port-forward it using the following command:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 我们要调用的是`truss-falcon-7b-service`。为了使服务可访问，我们需要使用以下命令进行端口转发：
- en: '[PRE30]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Output:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：
- en: '[PRE31]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Sweet, our model is available as a REST API endpoint at `127.0.0.1:8080` .
    Open up any python script such as `main.py` and run the following code:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 太棒了，我们的模型现在作为REST API端点可用，地址是`127.0.0.1:8080`。打开任何Python脚本，比如`main.py`，并运行以下代码：
- en: '[PRE32]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Output:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：
- en: '[PRE33]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: Whoo-hoo! We have successfully containerized our Falcon 7B model and deployed
    it as a microservice in production!
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 哇！我们成功地将我们的Falcon 7B模型容器化，并将其作为微服务在生产环境中部署！
- en: Feel free to play with different prompts to see what the model returns.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 随意尝试不同的提示，看看模型返回什么。
- en: Winding down the cluster
  id: totrans-207
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 关闭集群
- en: 'Once you’ve had your fun messing with Falcon 7B you can delete your deployment
    by running this command:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 当你玩够了Falcon 7B后，你可以通过运行以下命令来删除你的部署：
- en: '[PRE34]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: Next, head over to the kubernetes engine in google cloud and delete the kubernetes
    cluster.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，前往Google Cloud中的kubernetes引擎并删除kubernetes集群。
- en: 'Note: All images unless otherwise noted are by the author'
  id: totrans-211
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 注意：除非另有说明，否则所有图像均为作者提供。
- en: Conclusion
  id: totrans-212
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结论
- en: Running and managing a production-grade model like ChatGPT is not easy. However,
    with time the tooling will get better for developers to deploy their own model
    into the cloud.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 运行和管理像ChatGPT这样的生产级模型并不容易。然而，随着时间的推移，工具将变得更好，开发者将能更轻松地将自己的模型部署到云端。
- en: In this blog post, we touched upon all the things needed to deploy an LLM into
    production at a basic level. We packaged the model using Truss, containerized
    it using Docker, and deployed it in the cloud using kubernetes. I know it’s a
    lot to unpack and it wasn’t the easiest thing to do in the world, but we did it
    anyway.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇博客文章中，我们讨论了在基本层面上将LLM部署到生产环境所需的所有事项。我们使用Truss打包模型，通过Docker进行容器化，并使用kubernetes将其部署到云端。我知道这要处理的内容很多，而且这并不是世界上最容易的事情，但我们还是完成了。
- en: I hope you learned something interesting from this blog post. Thanks for reading!
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 希望你从这篇博客文章中学到了一些有趣的东西。感谢阅读！
- en: Peace.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 平安。
