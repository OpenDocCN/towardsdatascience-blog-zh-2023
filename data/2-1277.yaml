- en: How we think about Data Pipelines is changing
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 我们对数据管道的思考正在改变
- en: 原文：[https://towardsdatascience.com/how-we-think-about-data-pipelines-is-changing-51c3bf6f34dc](https://towardsdatascience.com/how-we-think-about-data-pipelines-is-changing-51c3bf6f34dc)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/how-we-think-about-data-pipelines-is-changing-51c3bf6f34dc](https://towardsdatascience.com/how-we-think-about-data-pipelines-is-changing-51c3bf6f34dc)
- en: '![](../Images/7d79fb389159e7b29999cbc1ea81691a.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7d79fb389159e7b29999cbc1ea81691a.png)'
- en: Photo by [Ali Kazal](https://unsplash.com/@lureofadventure?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)
    on [Unsplash](https://unsplash.com/photos/a-mountain-range-with-trees-in-the-foreground-and-a-field-in-the-foreground-walahB6h_sU?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 照片由[Ali Kazal](https://unsplash.com/@lureofadventure?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)提供，来自[Unsplash](https://unsplash.com/photos/a-mountain-range-with-trees-in-the-foreground-and-a-field-in-the-foreground-walahB6h_sU?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)
- en: The goal is to reliably and efficiently release data into production
  id: totrans-4
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 目标是可靠且高效地将数据发布到生产环境
- en: '[](https://medium.com/@hugolu87?source=post_page-----51c3bf6f34dc--------------------------------)[![Hugo
    Lu](../Images/045de11463bb16ea70a816ba89118a9e.png)](https://medium.com/@hugolu87?source=post_page-----51c3bf6f34dc--------------------------------)[](https://towardsdatascience.com/?source=post_page-----51c3bf6f34dc--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----51c3bf6f34dc--------------------------------)
    [Hugo Lu](https://medium.com/@hugolu87?source=post_page-----51c3bf6f34dc--------------------------------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@hugolu87?source=post_page-----51c3bf6f34dc--------------------------------)[![Hugo
    Lu](../Images/045de11463bb16ea70a816ba89118a9e.png)](https://medium.com/@hugolu87?source=post_page-----51c3bf6f34dc--------------------------------)[](https://towardsdatascience.com/?source=post_page-----51c3bf6f34dc--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----51c3bf6f34dc--------------------------------)
    [Hugo Lu](https://medium.com/@hugolu87?source=post_page-----51c3bf6f34dc--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----51c3bf6f34dc--------------------------------)
    ·6 min read·Nov 8, 2023
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表在[Towards Data Science](https://towardsdatascience.com/?source=post_page-----51c3bf6f34dc--------------------------------)
    ·6分钟阅读·2023年11月8日
- en: --
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: Data Pipelines are series of tasks organised in a [directed acyclic graph](https://en.wikipedia.org/wiki/Directed_acyclic_graph)
    or “DAG”. Historically, these are run on open-source workflow orchestration packages
    like [Airflow](https://airflow.apache.org/) or [Prefect](https://www.prefect.io/?gclid=Cj0KCQjwqP2pBhDMARIsAJQ0CzoV5DrzqjyDqDJonPcBPT5lE2ih47H2LMSKBst2jh6mR6h3azCcRnwaAhOJEALw_wcB),
    and require i[nfrastructure](https://www.bhavaniravi.com/apache-airflow/deploying-airflow-on-kubernetes)
    managed by data engineers or platform teams. These data pipelines typically run
    on a [schedule](https://airflow.apache.org/docs/apache-airflow/1.10.1/scheduler.html),
    and allow data engineers to update data in locations such as data warehouses or
    data lakes.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 数据管道是一系列组织成[有向无环图](https://en.wikipedia.org/wiki/Directed_acyclic_graph)或“DAG”的任务。从历史上看，这些任务是在开源工作流编排软件（如[Airflow](https://airflow.apache.org/)或[Prefect](https://www.prefect.io/?gclid=Cj0KCQjwqP2pBhDMARIsAJQ0CzoV5DrzqjyDqDJonPcBPT5lE2ih47H2LMSKBst2jh6mR6h3azCcRnwaAhOJEALw_wcB)）上运行的，并且需要由数据工程师或平台团队管理的[基础设施](https://www.bhavaniravi.com/apache-airflow/deploying-airflow-on-kubernetes)。这些数据管道通常按[时间表](https://airflow.apache.org/docs/apache-airflow/1.10.1/scheduler.html)运行，并允许数据工程师更新数据仓库或数据湖等位置的数据。
- en: This is now changing. There is a [great shift in mentality](/what-data-engineers-can-learn-from-software-engineers-and-vice-versa-643cade3ef23)
    happening. As the data engineering industry matures, mindsets are shifting from
    a “move data to serve the business at all costs” mindset to “reliability and efficiency”
    / “software engineering” mindset.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 这正在发生变化。[心态发生了巨大的转变](/what-data-engineers-can-learn-from-software-engineers-and-vice-versa-643cade3ef23)。随着数据工程行业的成熟，心态正在从“不惜一切代价移动数据以服务业务”的心态转变为“可靠性和效率”/“软件工程”的心态。
- en: Continuous Data Integration and Delivery
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 持续数据集成和交付
- en: I’ve written before about how [Data Teams ship *data*](https://medium.com/orchestras-data-release-pipeline-blog/a-new-paradigm-for-data-continuous-data-integration-and-delivery-miniseries-part-5-a3338b3ffd03)
    whereas software teams ship *code.*
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 我之前写过关于[数据团队发布*数据*](https://medium.com/orchestras-data-release-pipeline-blog/a-new-paradigm-for-data-continuous-data-integration-and-delivery-miniseries-part-5-a3338b3ffd03)，而软件团队发布*代码*。
- en: This is a process called “Continuous Data Integration and Delivery”, and is
    the process of reliably and efficiently releasing data into production. There
    are subtle differences with the definition of “[CI/CD](https://aws.amazon.com/solutions/app-development/ci-cd/#:~:text=An%20integral%20part%20of%20development,with%20collaborative%20and%20automated%20processes.)”
    as used in Software Engineer, illustrated below.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 这个过程被称为“持续数据集成和交付”，是可靠高效地将数据发布到生产环境的过程。与软件工程中“[CI/CD](https://aws.amazon.com/solutions/app-development/ci-cd/#:~:text=An%20integral%20part%20of%20development,with%20collaborative%20and%20automated%20processes.)”的定义存在细微差异，如下所示。
- en: '![](../Images/5a9b3efdc44da8184627cefa728133c0.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5a9b3efdc44da8184627cefa728133c0.png)'
- en: Image the author’s
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 作者的图片
- en: In software engineering, Continuous Delivery is non-trivial because of the importance
    of having a [near exact replica](https://www.techtarget.com/searchsoftwarequality/definition/staging-environment#:~:text=A%20staging%20environment%20(stage)%20is,like%20environment%20before%20application%20deployment.)
    for code to operate in a staging environment.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在软件工程中，持续交付并不容易，因为在暂存环境中需要有一个[几乎完全相同的副本](https://www.techtarget.com/searchsoftwarequality/definition/staging-environment#:~:text=A%20staging%20environment%20(stage)%20is,like%20environment%20before%20application%20deployment.)来运行代码。
- en: Within Data Engineering, this is not necessary because the good we ship is *data*.
    If there is a table of data, and we *know* that as long as a few conditions are
    satisfied, the data *is* of a sufficient quality to be used, then that is sufficient
    for it to be “released” into production, so to speak.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据工程中，这是不必要的，因为我们发布的是*数据*。如果有一张数据表，并且我们*知道*只要满足一些条件，数据就*足够优质*可以使用，那么这就足够让它被“发布”到生产环境中。
- en: The process of releasing data into production — the analog for Continuous Delivery
    — is very simple, as it simply relates to copying or [cloning](https://docs.snowflake.com/en/sql-reference/sql/create-clone)
    a dataset.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 将数据发布到生产环境的过程——持续交付的类比——非常简单，因为它只涉及复制或[克隆](https://docs.snowflake.com/en/sql-reference/sql/create-clone)数据集。
- en: Furthermore, a *key pillar* of data engineering is *reacting* to new data *as
    it arrives* or checking to see if new data exists. There is no analog for this
    in software engineering — software applications do not need to poll APIs for the
    existence of new code, whereas data applications do.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，数据工程的一个*关键支柱*是对新数据的*及时反应*，或者检查新数据是否存在。在软件工程中没有类似的情况——软件应用程序不需要轮询API以检查新代码的存在，而数据应用程序需要。
- en: Given the analog of Continuous Delivery in data is so trivial, we can loosely
    define Continuous Data Integration as the process of reliably and efficiently
    releasing data into production in response to code changes. Code changes that
    govern the state of the data are “continuously integrated” via a process of cloning,
    materialising views, and running tests.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于数据中持续交付的类比如此微不足道，我们可以宽泛地定义持续数据集成为可靠高效地响应代码更改将数据发布到生产环境的过程。通过克隆、实体化视图和运行测试来“持续集成”控制数据状态的代码更改。
- en: We can also loosely define Continuous Data Delivery as the process of reliably
    and efficiently releasing *new data into production*. This covers the invocations
    of pipelines or operations in response to the existence of new data.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也可以宽泛地定义持续数据交付为可靠高效地将*新数据发布到生产环境*的过程。这包括对新数据的存在进行管道调用或操作。
- en: Thinking about these two processes as the same type of operation but in a different
    context is a fairly radical departure from how most data teams think about data
    pipelines, or *data release pipelines*.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 将这两个过程视为相同类型的操作但在不同的上下文中，这与大多数数据团队对数据管道或*数据发布管道*的思考方式有了相当大的改变。
- en: Additional considerations
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 额外的考虑因素
- en: There are lots of additional considerations to think about here, and that’s
    because there is *so much to consider* beyond merely releasing data in production.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有很多额外的考虑因素，这是因为除了仅仅在生产环境中发布数据之外，还有*很多要考虑的东西*。
- en: Data isn’t static. It doesn’t simply exist in a place where it can be manipulated.
    It arrives in locations sparsely distributed across an organisation. It gets moved
    between tools. It arrives at different frequencies and only after the laborious
    process of “ELT” does it finally arrive in a data lake or data warehouse.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 数据并非静态的。它不仅仅存在于可以操作的地方。它分散在组织中的各个地方。它在工具之间移动。它以不同的频率到达，只有经过“ELT”的繁琐过程后，它才最终到达数据湖或数据仓库。
- en: Furthermore, Github Actions isn’t a sufficient infrastructure for doing all
    of this work. Perhaps as an orchestration layer, but certainly not for doing heavy-compute
    and doing data management.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，Github操作并不足以支持所有这些工作。也许作为编排层，但肯定不适用于进行大量计算和数据管理。
- en: These factors lead to many additional considerations for how to design a system
    that’s capable of delivering Continuous Data Integration and Delivery, which I
    discuss here
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 这些因素导致了许多额外的考虑，关于如何设计一个能够提供持续数据集成和交付的系统，我在这里讨论
- en: User Interface
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 用户界面
- en: Having a single User Interface to view Data deployments is key. Data teams who
    just use the UIs from multiple cloud data providers for DataOps will be at a loss
    when it comes to aggregating metadata to do effective DataOps, but also [BizFinOps](https://aws.amazon.com/blogs/enterprise-strategy/introducing-finops-excuse-me-devsecfinbizops/).
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 拥有一个单一的用户界面来查看数据部署是关键的。只使用多个云数据提供商的用户界面进行数据运营的数据团队，在聚合元数据以进行有效的数据运营，以及[BizFinOps](https://aws.amazon.com/blogs/enterprise-strategy/introducing-finops-excuse-me-devsecfinbizops/)方面将会损失。
- en: 'There are also data deployments to aggregate, which typically arise due to:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 还有要聚合的数据部署，通常是由于：
- en: New data arriving
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 新数据到达
- en: A change in the logic for how to materialise the data
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 改变数据实现逻辑
- en: These are currently handled using a workflow orchestration tool and GitHub actions
    or something similar. This creates a disjoint — Data Teams need to inspect multiple
    tools to understand when data tables have been updated, what their definitions
    are, and so on. You could, of course, buy an observability tool. However this
    is *yet another UI, another tool, and another cost.*
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 目前使用工作流编排工具和GitHub操作或类似工具来处理这些。这会造成不连贯性 — 数据团队需要检查多个工具，以了解数据表何时已更新，它们的定义是什么等等。当然，你可以购买一个可观察性工具。然而，这又是*另一个用户界面，另一个工具，另一个成本*。
- en: Having a genuine single pane of glass for Orchestration, Observability, and
    some kind of Ops would be a killer feature and one I would have loved to use at
    Codat, where we’d stitched together a whole host of open-sourced and closed-source
    vendor SAAS tools.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 拥有一个真正的单一操作面板，用于编排、可观察性和某种运维功能，将是一个杀手级功能，我很想在Codat使用，我们在那里整合了一整套开源和闭源供应商SAAS工具。
- en: Observability or Metadata gathering
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 可观察性或元数据收集
- en: I alluded to this in the previous section, but observability and metadata gathering
    is fundamental to a strong Data Pipeline.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 我在前一节中提到了这一点，但可观察性和元数据收集对于强大的数据管道至关重要。
- en: The important thing here, which I believe Observability Platforms miss, is to
    place the Observation in the Pipeline itself. Otherwise, presenting data engineers
    with metadata, pieces of information like “this failed” or “that table is stale”
    is undesirable, since it’s A) ex-post (after it’s too late) and B) unrelated to
    pipeline runs. Sure — a table is broken. But is it broken because of a change
    someone just pushed or because of some new data that arrived?
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 这里重要的一点，我认为可观察性平台忽略了的是将观察放在管道本身。否则，向数据工程师呈现元数据、诸如“这个失败了”或“那个表已过时”的信息是不可取的，因为它们是A）事后（为时已晚）和B）与管道运行无关。当然
    —— 一个表是坏的。但它是因为某人刚刚推送了一个更改还是因为有新数据到达？
- en: There was another good talk from [Andrew Jones](https://andrew-jones.com/categories/data-contracts/)
    I attended recently where he spoke about the 1, 10, 100 pyramid.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 最近我参加了[安德鲁·琼斯](https://andrew-jones.com/categories/data-contracts/)的另一个精彩演讲，他谈到了1、10、100金字塔。
- en: '![](../Images/c9e90125d2e5e686ec0628f83a9afc09.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c9e90125d2e5e686ec0628f83a9afc09.png)'
- en: It costs $1 to prevent, $10 to mitigate and $100 once it’s too late. Image credit
    to Andrew Jones, posted with permission.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 预防成本为1美元，缓解成本为10美元，一旦为时已晚，成本为100美元。图片由安德鲁·琼斯提供，获得许可后发布。
- en: '**Prevention Cost** — Preventing an error in data at the point of extraction
    will cost you a $1.'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**预防成本** — 在数据提取点预防错误将花费你1美元。'
- en: '**Correction Cost** — Having someone correct an error post extraction will
    cost you $10.'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**更正成本** — 在提取后有人纠正错误将花费你10美元。'
- en: '**Failure Cost** — Letting bad data run through a process to its end resting
    place will cost you $100.'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**失败成本** — 让错误数据通过流程到达最终位置将花费你100美元。'
- en: Observability tools are in the yellow to red section. If they’re setup on your
    prod databases, the likelihood is you’re in a race against time to fix the issue
    before someone realises.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 可观察性工具处于黄色到红色区域。如果它们设置在你的生产数据库上，很可能你正在与时间赛跑，以在有人意识到之前解决问题。
- en: Having analysts patch bad data with hacky SQL sits in the yellow section.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 分析师使用hacky SQL修复错误数据位于黄色区域。
- en: Orchestration with observability combined is between the green and the yellow.
    If all your data pipelines have access to all your metadata, and can implement
    data quality tests as you materialise and update tables and views, then you can
    pause a pipeline *any time* these tests fail. This means [*no bad data ever gets
    into production*](https://medium.com/snowflake/avoid-bad-data-completely-continuous-delivery-architectures-in-the-modern-data-stack-part-1-22a0d48935f6)*.*
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 带有可观察性的编排位于绿色和黄色之间。如果您的所有数据管道都可以访问所有元数据，并且可以在您实现和更新表和视图时执行数据质量测试，那么当这些测试失败时，您可以随时暂停管道。这意味着[*不会有错误数据进入生产环境*](https://medium.com/snowflake/avoid-bad-data-completely-continuous-delivery-architectures-in-the-modern-data-stack-part-1-22a0d48935f6)*。*
- en: This is extremely powerful, which is why I believe having an Orchestration tool
    that executes data pipelines with access to granular metadata is the way forward
    (disclosure my start-up is doing just that).
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 这非常强大，这就是为什么我相信，拥有一个具有对细粒度元数据访问权限的编排工具是未来的发展方向（我创办的初创公司正在做这个）。
- en: Summary
  id: totrans-47
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: We are moving away from data-agnostic workflow orchestration tools plus janky
    or non-existent Continuous Integration to unified Continuous *DATA* Integration
    and Delivery.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 我们正在摆脱数据不可知的工作流编排工具加上不稳定或不存在的持续集成，转向统一的持续*数据*集成和交付。
- en: There will be platforms that enable data teams to get full, reliable, and efficient
    version-controlling of datasets and rock-solid data pipelines. These will have
    observability capabilities built-in, and while many are not end-to-end yet, it
    certainly feels like this is the way things are heading.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 将会有平台使数据团队能够对数据集进行完整、可靠和高效的版本控制，并拥有坚固的数据管道。这些平台将内置可观察性功能，虽然许多平台尚未实现端到端，但肯定感觉事情正在朝着这个方向发展。
- en: There are many mature data tools that are doing this. For example, for data
    warehousing CI and CD, Y42 have the [concept](https://www.y42.com/blog/virtual-data-builds-one-data-warehouse-environment-for-every-git-commit/)
    of “Virtual Data Builds” which is basically the same thing as part 1 of this article.
    For the Data Lake environment, Einat Orr over at Lake FS / Treeverse posted on
    this [recently](https://www.linkedin.com/feed/update/urn:li:activity:7126949272050106369/?commentUrn=urn%3Ali%3Acomment%3A%28activity%3A7126949272050106369%2C7126969193651924992%29&dashCommentUrn=urn%3Ali%3Afsd_comment%3A%287126969193651924992%2Curn%3Ali%3Aactivity%3A7126949272050106369%29&dashReplyUrn=urn%3Ali%3Afsd_comment%3A%287126970713164439552%2Curn%3Ali%3Aactivity%3A7126949272050106369%29&replyUrn=urn%3Ali%3Acomment%3A%28activity%3A7126949272050106369%2C7126970713164439552%29)
    — what they do is functionally pretty similar and easily implementable in Snowflake
    ([I wrote an article about that here](https://medium.com/snowflake/why-snowflakes-clone-command-changes-the-game-for-ci-cd-in-data-ccb6fb9955ba)).
    SQLMesh’ “enterprise” version (don’t believe the open source [happy clappyness](https://medium.com/@hugolu87/y-combinator-had-282-companies-this-winter-and-c-50-were-open-source-sunday-scaries-128e53318454),
    this *is* a venture backed business. They *will* try to make money, like all of
    us) has observability and version control built into it, and it’s pretty cool.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多成熟的数据工具正在做这个。例如，对于数据仓库的持续集成和交付，Y42拥有“虚拟数据构建”（Virtual Data Builds）的[概念](https://www.y42.com/blog/virtual-data-builds-one-data-warehouse-environment-for-every-git-commit/)，基本上与本文的第一部分相同。对于数据湖环境，Lake
    FS / Treeverse的Einat Orr最近在[LinkedIn上发表了文章](https://www.linkedin.com/feed/update/urn:li:activity:7126949272050106369/?commentUrn=urn%3Ali%3Acomment%3A%28activity%3A7126949272050106369%2C7126969193651924992%29&dashCommentUrn=urn%3Ali%3Afsd_comment%3A%287126969193651924992%2Curn%3Ali%3Aactivity%3A7126949272050106369%29&dashReplyUrn=urn%3Ali%3Afsd_comment%3A%287126970713164439552%2Curn%3Ali%3Aactivity%3A7126949272050106369%29&replyUrn=urn%3Ali%3Acomment%3A%28activity%3A7126949272050106369%2C7126970713164439552%29）-
    他们所做的在功能上非常相似，并且在Snowflake中很容易实现（[我在这里写了一篇文章](https://medium.com/snowflake/why-snowflakes-clone-command-changes-the-game-for-ci-cd-in-data-ccb6fb9955ba)）。SQLMesh的“企业”版本（不要相信开源的[快乐](https://medium.com/@hugolu87/y-combinator-had-282-companies-this-winter-and-c-50-were-open-source-sunday-scaries-128e53318454)，这是一家风险投资支持的企业。他们会试图赚钱，就像我们所有人一样）具有内置的可观察性和版本控制功能，非常酷。
- en: You can, of course, still do *all of this* using something like Airflow. Hell,
    you could brew your morning coffee with Airflow if you wanted to. I guess the
    question is — do you have the time, the patience, and the expertise to write all
    of that code? Or are you like me, and do you just want to get shit done? 🐠
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 你当然可以使用类似Airflow这样的工具来完成*所有这些*。该死的，如果你愿意，你甚至可以用Airflow煮早晨的咖啡。我想问题是——你有时间、耐心和专业知识来编写所有这些代码吗？还是像我一样，只是想把事情做好？🐠
