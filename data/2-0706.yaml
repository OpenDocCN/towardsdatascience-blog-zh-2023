- en: Democratizing Machine Learning with AWS SageMaker AutoML
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 AWS SageMaker AutoML 实现机器学习的民主化
- en: 原文：[https://towardsdatascience.com/democratizing-machine-learning-with-aws-sagemaker-automl-150299c70396](https://towardsdatascience.com/democratizing-machine-learning-with-aws-sagemaker-automl-150299c70396)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/democratizing-machine-learning-with-aws-sagemaker-automl-150299c70396](https://towardsdatascience.com/democratizing-machine-learning-with-aws-sagemaker-automl-150299c70396)
- en: An Overview
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 概述
- en: '[](https://brus-patrick63.medium.com/?source=post_page-----150299c70396--------------------------------)[![Patrick
    Brus](../Images/a252fe1c4f7a9ed2225d415571137e45.png)](https://brus-patrick63.medium.com/?source=post_page-----150299c70396--------------------------------)[](https://towardsdatascience.com/?source=post_page-----150299c70396--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----150299c70396--------------------------------)
    [Patrick Brus](https://brus-patrick63.medium.com/?source=post_page-----150299c70396--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://brus-patrick63.medium.com/?source=post_page-----150299c70396--------------------------------)[![Patrick
    Brus](../Images/a252fe1c4f7a9ed2225d415571137e45.png)](https://brus-patrick63.medium.com/?source=post_page-----150299c70396--------------------------------)[](https://towardsdatascience.com/?source=post_page-----150299c70396--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----150299c70396--------------------------------)
    [Patrick Brus](https://brus-patrick63.medium.com/?source=post_page-----150299c70396--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----150299c70396--------------------------------)
    ·16 min read·Apr 25, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----150299c70396--------------------------------)
    ·16 分钟阅读·2023年4月25日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/9d6177546bde28b57814053a64914628.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9d6177546bde28b57814053a64914628.png)'
- en: Photo by [Joshua Sortino](https://unsplash.com/@sortino?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由 [Joshua Sortino](https://unsplash.com/@sortino?utm_source=medium&utm_medium=referral)
    提供，来源于 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: Introduction
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: AI is still one of the hottest topics as of now, especially with the rise of
    ChatGPT. Many companies are now trying to make use of AI in order to extract useful
    insights from data that can be used to optimize their processes or bring out better
    products.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: AI 目前仍然是最热门的话题之一，尤其是随着 ChatGPT 的崛起。许多公司现在正尝试利用 AI 从数据中提取有用的见解，以优化他们的流程或开发更好的产品。
- en: However, building effective AI models requires a lot of expertise in different
    fields, like data preprocessing, model selection, hyperparameter tuning and many
    more. All these fields can be time-consuming and require specialized knowledge.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，构建有效的 AI 模型需要在不同领域具备大量专业知识，如数据预处理、模型选择、超参数调优等。所有这些领域都可能耗时且需要专业知识。
- en: This is where AutoML comes into play. AutoML automates many of the above-mentioned
    fields required for building an AI model.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是 AutoML 发挥作用的地方。AutoML 自动化了构建 AI 模型所需的许多领域。
- en: AutoML is rapidly becoming a popular solution for businesses and data scientists.
    It empowers organizations to leverage ML and AI to make informed decisions, without
    requiring them to be experts in data science. With the increasing demand for ML
    in businesses, AutoML provides an easy and efficient way to create accurate models,
    regardless of one’s expertise.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: AutoML 正迅速成为企业和数据科学家们喜爱的解决方案。它使组织能够利用 ML 和 AI 做出明智的决策，而无需成为数据科学专家。随着企业对 ML 需求的增加，AutoML
    提供了一种简单高效的方式来创建准确的模型，无论个人的专业知识水平如何。
- en: In this article, we’ll examine one very popular AutoML tool available in the
    market today, AWS SageMaker AutoML, and demonstrate how it can be used to solve
    complex ML use cases.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们将考察市场上一个非常流行的 AutoML 工具——AWS SageMaker AutoML，并展示如何利用它解决复杂的 ML 使用案例。
- en: I will train a model with the old-fashioned manual approach and compare the
    results to the ones that AWS SageMaker AutoML produces.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 我将用传统的手动方法训练一个模型，并将其结果与 AWS SageMaker AutoML 产生的结果进行比较。
- en: I will use the credit card fraud detection dataset from Kaggle for this comparison
    [1]. You can find the dataset [here](https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud).
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 我将使用 Kaggle 上的信用卡欺诈检测数据集进行对比 [1]。你可以在[这里](https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud)找到数据集。
- en: By the end of this article, you’ll have a clear understanding of how AutoML
    can help leverage ML to drive meaningful insights and make informed decisions.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文结束时，你将清楚了解 AutoML 如何帮助利用 ML 驱动有意义的见解并做出明智的决策。
- en: AWS SageMaker AutoML
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: AWS SageMaker AutoML
- en: '![](../Images/f89b70e37f4074e49b6a1784eb5d35ef.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f89b70e37f4074e49b6a1784eb5d35ef.png)'
- en: 'Figure 1: Overview of AWS SageMaker AutoML (Image by author, based on [2]).'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：AWS SageMaker AutoML 概述（作者提供的图像，基于 [2]）。
- en: Figure 1 gives an overview on the different steps that AWS SageMaker AutoML
    solves.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1 展示了 AWS SageMaker AutoML 解决的不同步骤概述。
- en: 'It includes the following steps:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 其包含以下步骤：
- en: '**Data Preparation:** You can easily upload your data to Amazon S3\. Once your
    data is uploaded, SageMaker AutoML automatically analyzes your data in order to
    detect any missing values, outliers or data types that need to be transformed.'
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**数据准备：** 你可以轻松地将数据上传到 Amazon S3。一旦数据上传完毕，SageMaker AutoML 会自动分析数据，以检测任何缺失值、异常值或需要转换的数据类型。'
- en: '**Automatic Model Creation:** AWS SageMaker AutoML automatically trains multiple
    machine learning models with different hyperparameters and algorithms to determine
    the best model for your data. It also provides automatic model tuning, which adjusts
    the hyperparameters of the selected models to further optimize their performance.
    It also creates the notebooks for running the model selection automatically for
    you, so that you have a full visibility about what is executed during this process.'
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**自动模型创建：** AWS SageMaker AutoML 自动训练多个机器学习模型，使用不同的超参数和算法，以确定最适合你数据的模型。它还提供了自动模型调优，调整所选模型的超参数以进一步优化其性能。它还为你创建运行模型选择的笔记本，以便你可以全面了解在此过程中执行了什么。'
- en: '**Model Deployment:** Once the best model has been selected, AWS SageMaker
    offers to deploy the model to a SageMaker endpoint or a batch transform job, where
    it can be used to make predictions on new data. On top of it, AWS SageMaker Model
    Monitor can be utilized in order to be alerted if any issues arise (like data
    drifts, concept drifts, …). It also provides tools for retraining the model with
    new data, as well as updating the model’s hyperparameters or algorithms to improve
    its performance.'
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**模型部署：** 一旦选择了最佳模型，AWS SageMaker 提供将模型部署到 SageMaker 端点或批处理转换作业的选项，在那里它可以用于对新数据进行预测。此外，AWS
    SageMaker Model Monitor 可以用于在出现问题（如数据漂移、概念漂移等）时发出警报。它还提供了重新训练模型的新数据、更新模型的超参数或算法以提升性能的工具。'
- en: AWS SageMaker AutoML offers a [Python SDK](https://sagemaker.readthedocs.io/en/stable/api/training/automl.html)
    that can be used for starting your AutoML job and a [GitHub repository](https://github.com/aws/amazon-sagemaker-examples/tree/main/autopilot)
    with various different notebook examples on how to utilize the AutoML SDK for
    concrete ML use cases.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: AWS SageMaker AutoML 提供了一个 [Python SDK](https://sagemaker.readthedocs.io/en/stable/api/training/automl.html)，可以用于启动
    AutoML 作业，并且有一个 [GitHub 仓库](https://github.com/aws/amazon-sagemaker-examples/tree/main/autopilot)，其中包含了各种不同的笔记本示例，展示了如何利用
    AutoML SDK 处理具体的机器学习用例。
- en: There are also other powerful and well-known AutoML tools available in the market,
    such as Google Cloud AutoML and H2O.ai, which also have their own unique strengths
    and weaknesses.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 市场上还提供了其他强大且知名的 AutoML 工具，如 Google Cloud AutoML 和 H2O.ai，它们也有各自独特的优势和劣势。
- en: Google Cloud AutoML is known for its ease of use and intuitive interface, which
    makes it perfect in case you are new to ML and also not that deep into coding.
    Google Cloud AutoML supports image data, video data, text data and tabluar data.
    You can read more about that [here](https://cloud.google.com/vertex-ai/docs/training-overview?hl=en#automl).
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: Google Cloud AutoML 以其易用性和直观的界面而闻名，这使其非常适合机器学习新手，并且对编码要求不高。Google Cloud AutoML
    支持图像数据、视频数据、文本数据和表格数据。你可以在 [这里](https://cloud.google.com/vertex-ai/docs/training-overview?hl=en#automl)
    了解更多信息。
- en: H20.ai is known for its speed and scalability, making it a good option for large
    datasets and complex models. H20.ai offers interfaces in R, Python, or a web GUI.
    You can read more about its features [here](https://h2o.ai/platform/h2o-automl/).
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: H2O.ai 以其速度和可扩展性而闻名，使其成为处理大数据集和复杂模型的良好选择。H2O.ai 提供 R、Python 或网页 GUI 的接口。你可以在
    [这里](https://h2o.ai/platform/h2o-automl/) 了解更多关于其功能的信息。
- en: Manual Training Approach
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 手动训练方法
- en: 'Before I’m going to use AWS SageMaker AutoML to come up with a classifier for
    the credit card dataset, I first train a model in the classic way: Doing everything
    myself from scratch.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用 AWS SageMaker AutoML 来生成信用卡数据集的分类器之前，我首先以传统方式训练一个模型：从头开始做所有事情。
- en: This later helps me to have a baseline and to compare my own approach to the
    AutoML approach from AWS, with the expectation that AWS SageMaker AutoML outperforms
    my manual, semi-optimal, approach.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 这将有助于我建立一个基准，并将我的方法与AWS的AutoML方法进行比较，期望AWS SageMaker AutoML能超越我的手动半优化方法。
- en: For the manual approach, I make use of Scikit-learn and I will run through the
    steps highlighted in the next chapters.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 对于手动方法，我使用了Scikit-learn，并将通过下一章中强调的步骤进行操作。
- en: You can also find the complete notebook in my GitHub repository [here](https://github.com/patrickbrus/aws-auto-ml-credit-fraud/blob/master/manual-approach/credit-fraud-ml.ipynb).
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以在我的GitHub仓库中找到完整的笔记本 [这里](https://github.com/patrickbrus/aws-auto-ml-credit-fraud/blob/master/manual-approach/credit-fraud-ml.ipynb)。
- en: '**Data preparation**'
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**数据准备**'
- en: I load the dataset from a CSV file and first check the distribution of the dataset.
    This shows that the dataset is highly imbalanced, with only **0.17%** of all samples
    being positive.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 我从CSV文件中加载数据集，首先检查数据集的分布。这显示数据集高度不平衡，只有**0.17%**的样本是正例。
- en: The dataset itself doesn’t contain any missing values.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集本身不包含任何缺失值。
- en: I then split the dataset with an 80/20 split into train and test and scale the
    data to be in the range 0–1, while the standard scaler is only trained on the
    training set to avoid some overly optimistic results.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我将数据集按80/20的比例拆分为训练集和测试集，并将数据缩放到0–1的范围内，同时标准化器仅在训练集上进行训练，以避免一些过于乐观的结果。
- en: You can find the code for these steps below.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 下面可以找到这些步骤的代码。
- en: '[PRE0]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Typically, extensive Exploratory Data Analysis (EDA) would also be part of the
    data preparation step. But for the sake of this experiment, I did not do extensive
    EDA, as the dataset is already well-prepared for ML.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，广泛的探索性数据分析（EDA）也会是数据准备步骤的一部分。但是为了这个实验，我没有进行广泛的EDA，因为数据集已经为ML做好了充分的准备。
- en: But just keep in mind that this part is also crucial to the success of your
    ML training and also takes some time typically.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 但请记住，这部分对于你的ML训练的成功也是至关重要的，并且通常需要一些时间。
- en: Model Selection
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型选择
- en: The next step is to figure out what ML algorithm is best suited for the data.
    For this purpose, I first train a very simple baseline model using logistic regression.
    This is to already have something simple that I can then compare more complex
    algorithms to.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是找出哪种ML算法最适合数据。为此，我首先使用逻辑回归训练一个非常简单的基准模型。这是为了有一个简单的模型，我可以将其与更复杂的算法进行比较。
- en: 'The goal should always be: Keep it simple! Don’t start with a Neural Network,
    which is harder to explain at the end, if also a simple algorithm, like logistic
    regression, could do the job.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 目标应始终是：保持简单！不要从神经网络开始，如果一个简单的算法，比如逻辑回归，能够完成任务，神经网络可能更难以解释。
- en: The logistic regression model achieved an F1-Score of **70.6**%. I am using
    the F1-Score for this dataset, as it is highly imbalanced and the accuracy would
    not deliver a meaningful measure of the model, as only predicting all classes
    as negative would already lead to an accuracy of more than **99%**!
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑回归模型的F1-Score为**70.6%**。我使用F1-Score来评估这个数据集，因为它高度不平衡，准确率不会提供有意义的模型评估，因为仅预测所有类别为负例就已经会导致超过**99%**的准确率！
- en: You can find the code for training the baseline model below.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 下面可以找到训练基准模型的代码。
- en: '[PRE1]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Okay, we now have a baseline. Let’s now try out different classification algorithms
    with their default hyperparameters, and let’s see what algorithm performs best
    on the data.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，我们现在有了基准。接下来尝试不同的分类算法及其默认超参数，看看哪个算法在数据上表现最好。
- en: 'I used a 5-fold cross-validation to train each of the following models:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 我使用了5折交叉验证来训练以下每个模型：
- en: decision tree
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 决策树
- en: supported vector machine
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 支持向量机
- en: k-nearest neighbors
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: k-近邻
- en: random forest
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随机森林
- en: ada-boost
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自适应提升
- en: '[PRE2]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The random forest algorithm delivered the best results with an F1-Score of **86.9%**,
    followed by the nearest neighbor algorithm with an F1-Score of **84.8%**. Not
    bad!
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 随机森林算法以**86.9%**的F1-Score取得了最佳结果，其次是最近邻算法，F1-Score为**84.8%**。不错！
- en: Next step is to fine tune the winner (random forest).
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是对获胜者（随机森林）进行微调。
- en: For this, I selected some values of the hyperparameters that I want to try out
    and used a randomized cross-validation search in order to find the best set of
    hyperparameters leading to the best model.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 为此，我选择了一些要尝试的超参数值，并使用随机化交叉验证搜索来找到最佳超参数组合，从而获得最佳模型。
- en: 'The code for this evaluation:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 这次评估的代码：
- en: '[PRE3]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The best model scored more or less the same as the one that I got without tuning
    the hyperparameters. What a waste of time ;)
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 最佳模型的得分多多少少与我在没有调整超参数的情况下获得的模型相同。真是浪费时间 ;)
- en: Model Evaluation
  id: totrans-62
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型评估
- en: Last but not least, I evaluate my final model on the hold-out test set to see
    how it would perform on real-world data.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 最后但同样重要的是，我在留出的测试集上评估我的最终模型，以查看它在真实世界数据上的表现。
- en: '[PRE4]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: This gives me an F1-Score of **82%,** so close to our validation results but
    a bit below.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 这给了我一个**82%**的F1分数，接近我们的验证结果，但略低一些。
- en: I know that one could get more out of the model by tuning it a bit more. But
    the goal in this article is to just do some basic ML and compare the results to
    AutoML to already see how good AutoML performs and how the effort is to run an
    AutoML job in comparison to only do the basic training myself.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 我知道通过更多地调整模型可以获得更多收益。但本文的目标只是做一些基本的机器学习，并将结果与AutoML进行比较，以查看AutoML的表现有多好，以及与仅自己做基础训练相比，运行AutoML作业的工作量有多大。
- en: Training with AWS SageMaker AutoML
  id: totrans-67
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用AWS SageMaker AutoML进行训练
- en: Okay, now that I have my baseline, I can try to get a better model with less
    effort using AWS SageMaker AutoML.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 好了，现在我有了基准，可以尝试使用AWS SageMaker AutoML以更少的努力获得更好的模型。
- en: I will again guide you through the different steps and provide the code snippets
    for all of these steps. You can also find the complete notebook in my GitHub repository
    [here](https://github.com/patrickbrus/aws-auto-ml-credit-fraud/blob/master/sagemaker-auto-ml/fraud-detection-auto-ml-sagemaker.ipynb).
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 我将再次指导你完成不同的步骤，并提供所有这些步骤的代码片段。你还可以在我的GitHub仓库[这里](https://github.com/patrickbrus/aws-auto-ml-credit-fraud/blob/master/sagemaker-auto-ml/fraud-detection-auto-ml-sagemaker.ipynb)找到完整的笔记本。
- en: Data Upload
  id: totrans-70
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据上传
- en: For SageMaker AutoML to work, the data needs to be stored in s3\. Therefore,
    I am first creating a bucket and then uploading the CSV file to this bucket (Gif
    1).
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使SageMaker AutoML正常工作，数据需要存储在s3中。因此，我首先创建一个桶，然后将CSV文件上传到这个桶中（Gif 1）。
- en: '![](../Images/e923a4bc225c20097934150620449b6e.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e923a4bc225c20097934150620449b6e.png)'
- en: 'Gif 1: Creating s3 bucket and uploading CSV file to this bucket (Gif by author).'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: Gif 1：创建s3桶并将CSV文件上传到此桶中（Gif由作者提供）。
- en: Setup SageMaker Notebook
  id: totrans-74
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 设置SageMaker笔记本
- en: The next step is to setup the environment where I can run the AutoML job in.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是设置我可以在其中运行AutoML作业的环境。
- en: To achieve this, I first create a notebook in SageMaker where I can then create
    and run the code in.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现这一点，我首先在SageMaker中创建一个笔记本，然后在其中创建和运行代码。
- en: In AWS, access to services by other services is handled with IAM roles. The
    SageMaker notebook gets an IAM role attached with some default access rights.
    But for accessing my before created s3 bucket, I first have to explicitly adapt
    the policy attached to this role.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在AWS中，其他服务对服务的访问由IAM角色处理。SageMaker笔记本附带了一个带有默认访问权限的IAM角色。但为了访问我之前创建的s3桶，我首先必须明确调整附加到该角色的策略。
- en: Gif 2 shows the complete process of creating the notebook and how I adapted
    the policy of the notebook role. The quality is unfortunately not that good, but
    I still think it is valuable to see the sequence of actions taken. But I also
    added some screenshots of the exact settings when creating the notebook (Figure
    2), and I added the complete policy I attached to the notebook role in my GitHub
    repository [here](https://github.com/patrickbrus/aws-auto-ml-credit-fraud/blob/master/sagemaker-auto-ml/notebook-role-policy.json).
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: Gif 2展示了创建笔记本的完整过程以及我如何调整笔记本角色的策略。质量不太好，但我仍然认为看到所采取的操作顺序是有价值的。此外，我还添加了一些创建笔记本时的具体设置截图（图2），并在我的GitHub仓库[这里](https://github.com/patrickbrus/aws-auto-ml-credit-fraud/blob/master/sagemaker-auto-ml/notebook-role-policy.json)添加了我附加到笔记本角色的完整策略。
- en: '![](../Images/e107018d2acca85d14be0000890bfe55.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e107018d2acca85d14be0000890bfe55.png)'
- en: 'Gif 2: Process of creating a notebook and adapting the policy of the attached
    IAM role (Gif by author).'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: Gif 2：创建笔记本的过程以及如何调整附加IAM角色的策略（Gif由作者提供）。
- en: '![](../Images/8e346589e66bbb7879cbd5d54085127e.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8e346589e66bbb7879cbd5d54085127e.png)'
- en: 'Figure 2: Settings of AWS SageMaker notebook creation (Image by author).'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：AWS SageMaker笔记本创建设置（图片由作者提供）。
- en: Run AWS SageMaker AutoML Job
  id: totrans-83
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 运行AWS SageMaker AutoML作业
- en: Now I can finally start running some code.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我终于可以开始运行一些代码了。
- en: The code I am running is mostly copied and adapted from [this](https://gitlab.com/juliensimon/aim307/-/blob/master/aim307.ipynb)
    AWS tutorial notebook.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 我运行的代码大多是从[这个](https://gitlab.com/juliensimon/aim307/-/blob/master/aim307.ipynb)AWS教程笔记本中复制并调整过来的。
- en: As a first step, I am loading the data from s3 into a Pandas dataframe and setup
    some general variables required for SageMaker AutoML later.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我将数据从 s3 加载到 Pandas 数据框中，并设置一些 SageMaker AutoML 之后所需的一般变量。
- en: '[PRE5]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Then I split the dataset into a training set and a hold-out test set. I will
    then use the latter to compare the AutoML model to my own trained model. I will
    then upload the data to the s3 bucket created by SageMaker so that the AutoML
    job can access it directly from s3.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我将数据集分成训练集和保留测试集。我将使用后者来比较 AutoML 模型和我自己训练的模型。然后，我将数据上传到 SageMaker 创建的 s3
    桶中，以便 AutoML 任务可以直接从 s3 访问它。
- en: '[PRE6]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Now I can set up the AutoML job and launch it. You can find more about the required
    input parameters and settings in the SageMaker SDK documentation [here](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/sagemaker/client/create_auto_ml_job.html#).
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我可以设置 AutoML 任务并启动它。你可以在 SageMaker SDK 文档[这里](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/sagemaker/client/create_auto_ml_job.html#)找到有关所需输入参数和设置的更多信息。
- en: '[PRE7]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: The job now runs in the background and creates the required AWS resources for
    you.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 任务现在在后台运行，为你创建所需的 AWS 资源。
- en: 'You can then run the following code in order to track the progress of the AutoML
    job:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，你可以运行以下代码来跟踪 AutoML 任务的进度：
- en: '[PRE8]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The job is running through the following stages:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 任务正在通过以下阶段：
- en: Analyzing Data
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 数据分析
- en: Feature Engineering
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 特征工程
- en: Model Tuning
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 模型调优
- en: Merging AutoML Tasks Reports
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 合并 AutoML 任务报告
- en: AWS SageMaker is generating two notebooks for you. One for exploring the data
    and one for defining the different candidates that are evaluated on the dataset.
    You can get these notebooks if you are interested in what code AWS SageMaker is
    running for these stages.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: AWS SageMaker 为你生成了两个笔记本。一个用于探索数据，另一个用于定义在数据集上进行评估的不同候选模型。如果你对 AWS SageMaker
    在这些阶段运行的代码感兴趣，可以查看这些笔记本。
- en: It is possible to list all the experiments that AWS SageMaker has executed and
    you can list all the explored candidates, too. I did not add the code in this
    article, but you can find it in my [notebook](https://github.com/patrickbrus/aws-auto-ml-credit-fraud/blob/master/sagemaker-auto-ml/fraud-detection-auto-ml-sagemaker.ipynb),
    if you are curios how this works.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 可以列出 AWS SageMaker 执行的所有实验，并且你也可以列出所有探讨过的候选模型。我在这篇文章中没有添加代码，但如果你对其工作原理感兴趣，可以在我的[笔记本](https://github.com/patrickbrus/aws-auto-ml-credit-fraud/blob/master/sagemaker-auto-ml/fraud-detection-auto-ml-sagemaker.ipynb)中找到代码。
- en: Evaluate Best Candidate on Testset
  id: totrans-102
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在测试集上评估最佳候选模型
- en: Now it is time to test the best candidate on the hold-out test set. This is
    the most interesting part to me, as it shows whether AutoML can score something
    better than my manual approach.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是时候在保留测试集上测试最佳候选模型了。对我来说，这是最有趣的部分，因为它显示了 AutoML 是否能比我的手动方法得分更高。
- en: First, I am retrieving the best candidate from the AWS SageMaker AutoML job.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我从 AWS SageMaker AutoML 任务中检索最佳候选模型。
- en: '[PRE9]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Next, I will host this model as an endpoint in AWS where I can send data to
    for inference.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我将在 AWS 中将此模型作为端点进行托管，以便我可以将数据发送给它进行推断。
- en: '[PRE10]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: And last but not least, read the CSV file with the test data and send the data
    to the final model for inference. I then compare the predictions to the ground
    truth and then use a quite manual approach to count up the true positives, true
    negatives, false positives and false negatives.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 最后但同样重要的是，读取包含测试数据的 CSV 文件，并将数据发送到最终模型进行推断。然后，我将预测结果与真实值进行比较，并使用相当手动的方法来计算真正例、假负例、假正例和真正例。
- en: I honestly was just too lazy to implement something on my own and just adated
    the code again from the AWS SageMaker tutorial notebook that you can find [here](https://gitlab.com/juliensimon/aim307/-/blob/master/aim307.ipynb).
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 老实说，我只是懒得自己实现一些东西，而是再次从 AWS SageMaker 教程笔记本中适配了代码，你可以在[这里](https://gitlab.com/juliensimon/aim307/-/blob/master/aim307.ipynb)找到。
- en: '[PRE11]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: The final model achieves an F1-Score of **96%** on the hold-out test set!
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 最终模型在保留测试集上达到了**96%**的 F1-Score！
- en: This is awesome! As comparison, the model I trained with Scikit-Learn only achieved
    an F1-Score of **82%**.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 这太棒了！相比之下，我用 Scikit-Learn 训练的模型仅达到了**82%**的 F1-Score。
- en: Shortcomings of AutoML
  id: totrans-113
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: AutoML 的不足之处
- en: AutoML is definetly powerful and can help to speed up the ML development cycle.
    But there are also shortcomings of using AutoML that should be recognized.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: AutoML 确实强大，可以帮助加快 ML 开发周期。但也存在一些使用 AutoML 的不足之处，需要认识到。
- en: One of the main limitations of AutoML is that it can be somewhat of a black
    box approach, as it automates much of the process of building a machine learning
    model. This can make it difficult for data scientists to fully understand how
    the model works and may limit their ability to fine-tune the model or debug any
    issues that arise.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: AutoML的主要限制之一是它可能是一种黑箱方法，因为它自动化了构建机器学习模型的大部分过程。这可能使数据科学家很难完全理解模型的工作原理，并可能限制他们微调模型或调试出现的问题的能力。
- en: AWS tries to combat this by providing jupyter notebooks that show the code for
    stages like data exploration or exploring different ML candidates. This already
    helps to get some insights, but if there are any issues on the code or the findings
    that the AutoML job had, then the data scientist has no influence on changing
    the code behind, as everything is auto-generated.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: AWS通过提供Jupyter笔记本来应对这一问题，展示了数据探索或探索不同ML候选模型等阶段的代码。这已经有助于获取一些见解，但如果代码或AutoML任务的结果出现任何问题，数据科学家无法对背后的代码进行更改，因为一切都是自动生成的。
- en: Another potential drawback of using AutoML is that it can be less flexible than
    a more traditional approach to building a machine learning model. AutoML is optimized
    for efficiency and ease of use, but this may come at the expense of customization
    options or the ability to work with specialized datasets or models.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 使用AutoML的另一个潜在缺点是，它可能比传统的机器学习模型构建方法灵活性差。AutoML优化了效率和易用性，但这可能以定制选项或处理专用数据集或模型的能力为代价。
- en: For this article, I have used a very simple dataset and AWS SageMaker AutoML
    was able to train a good candidate. But if there is a more challenging dataset,
    than it is not clear how well AutoML would perform on the dataset.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我使用了一个非常简单的数据集，AWS SageMaker AutoML能够训练出一个不错的候选模型。但如果遇到更具挑战性的数据集，AutoML在这些数据集上的表现如何还不清楚。
- en: Personal Findings
  id: totrans-119
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 个人发现
- en: In this chapter, I want to highlight my personal findings while using AWS SageMaker
    AutoML.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一章中，我想强调我在使用AWS SageMaker AutoML时的个人发现。
- en: The first thing I recognized is that it is quite complex to use and to set it
    up. I personally have some more experience in using AWS in general and also in
    coding, but if there is a person with not that mutch of prior knowledge, then
    the learning curve of using AWS SageMaker AutoML could be too steep.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 我首先意识到使用和设置AWS SageMaker AutoML相当复杂。我个人在使用AWS和编码方面有一定经验，但对于没有太多先前知识的人来说，使用AWS
    SageMaker AutoML的学习曲线可能过于陡峭。
- en: I also think that the documentation is not that well. I had a hard time in the
    beginning to find something about end-to-end usecases in the documentation. I
    could then find video trainings and GitHub repositories with example notebooks,
    but I personally like written documentation more than videos.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 我还认为文档质量不够好。我一开始在文档中很难找到关于端到端用例的内容。后来我找到了视频培训和GitHub上的示例笔记本，但我个人更喜欢书面的文档而非视频。
- en: And please be careful about the costs. I initially played around with SageMaker
    AutoML and had the AutoML job running more often, because I wanted to try out
    different things there. But that wasn’t as cheap as I hoped and I ended up paying
    more on this experiment than I planned.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 另外，请注意成本。我最初在SageMaker AutoML中进行了一些尝试，频繁运行AutoML任务，因为我想在其中尝试不同的东西。但这并没有像我预期的那样便宜，结果我在这个实验上的花费比计划的要多。
- en: Conclusion
  id: totrans-124
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: In this article, I compared AWS SageMaker AutoML to manually training a model
    using Scikit-Learn.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我将AWS SageMaker AutoML与使用Scikit-Learn手动训练模型进行了比较。
- en: As dataset, I decided to use the fraud detection dataset, as this has some difficulties
    in it because of the high degree of imbalance.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 作为数据集，我决定使用欺诈检测数据集，因为它由于高度不平衡而存在一些困难。
- en: I then manually tried to find a good classifier and did the same with AWS SageMaker
    AutoML.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我手动尝试找出一个好的分类器，并在AWS SageMaker AutoML中做了同样的尝试。
- en: I finally compared the results of both approaches on a hold-out test set, where
    AWS SageMaker AutoML outscored my manual approach, as it achieved an F1-Score
    of **96%**, compared to **82%**.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 我最终将两种方法的结果在一个保留测试集上进行了比较，其中AWS SageMaker AutoML的得分高于我的手动方法，达到了**96%**的F1分数，而我的手动方法为**82%**。
- en: This shows that it totally makes sense to use AWS SageMaker AutoML to train
    ML on your dataset in order to quickly come up with a classifier that you can
    use.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 这表明，使用AWS SageMaker AutoML在你的数据集上训练ML模型以快速生成一个可用的分类器是完全合理的。
- en: It is even not required to have expertise on the field of ML for making use
    of AWS SageMaker AutoML.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 甚至不需要在机器学习领域有专门的知识就可以使用AWS SageMaker AutoML。
- en: Of course, my manual approach has to be taken with a grain of salt.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，我的手动方法也需要谨慎对待。
- en: I did not invest a lot of time to fine tune my final classifier and I am pretty
    sure with a bit more invested time I would have scored also better results on
    the hold-out test set.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 我没有花很多时间来优化我的最终分类器，我相当确定，如果再多投入一些时间，我会在保留测试集上获得更好的结果。
- en: At least I hope so ;)
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 至少我希望如此 ;)
- en: But the idea of this article was to show how easy it can be to just make use
    of an AutoML library to come up with a classifier for your ML dataset.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 但这篇文章的目的就是展示如何轻松地利用AutoML库为你的机器学习数据集创建一个分类器。
- en: This at the end doesn’t need to be the final one that you put into a product,
    but you could at least do a quick initial proof-of-concept in order to see whether
    you can get something useful out of your data.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，这不一定是你投入到产品中的最终版本，但你可以至少做一个快速的初步概念验证，以查看你是否能从数据中获得有用的信息。
- en: Outlook
  id: totrans-136
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 展望
- en: I till now only took a deeper look into AWS SageMaker AutoML, but it would definetly
    be interesting to also check other offerings, like Google Cloud AutoML.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 迄今为止，我只深入了解了AWS SageMaker AutoML，但肯定也很有兴趣查看其他服务，例如Google Cloud AutoML。
- en: I am planing on doing a complete evaluation of Google Clouds Vertex AI offering
    in the near future and will then write about my findings in Medium as well.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 我计划在不久的将来对Google Cloud的Vertex AI进行全面评估，然后会在Medium上写出我的发现。
- en: Then I can also talk specifically about how Sagemaker performs compared to Google
    Clouds offer.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我还可以具体讨论Sagemaker与Google Cloud的表现相比如何。
- en: So follow me in case you don’t want to miss that!
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 所以如果你不想错过这些内容，请关注我！
- en: Thank you for reading my article to the end! I hope you enjoyed this article.
    If you want to read more articles like this in the future, follow me to stay updated.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 感谢你读到我的文章的最后！我希望你喜欢这篇文章。如果你想将来阅读更多类似的文章，关注我以保持更新。
- en: '[**Join my email list if you want to learn more about machine learning and
    the cloud.**](https://medium.com/subscribe/@brus-patrick63)'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '[**如果你想了解更多关于机器学习和云计算的信息，请加入我的邮件列表。**](https://medium.com/subscribe/@brus-patrick63)'
- en: Contact
  id: totrans-143
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 联系方式
- en: '[**LinkedIn**](https://www.linkedin.com/in/patrick-brus/) |[**GitHub**](https://github.com/patrickbrus)'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '[**LinkedIn**](https://www.linkedin.com/in/patrick-brus/) | [**GitHub**](https://github.com/patrickbrus)'
- en: References
  id: totrans-145
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1]: Machine Learning Group — ULB, [“Credit Card Fraud Detection”](https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud),
    Kaggle, 2018, [Database Contents License](https://opendatacommons.org/licenses/dbcl/1-0/)
    (DbCL) v1.0'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '[1]: 机器学习组 — ULB，[“信用卡欺诈检测”](https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud)，Kaggle，2018年，[数据库内容许可证](https://opendatacommons.org/licenses/dbcl/1-0/)
    (DbCL) v1.0'
- en: '[2]: AWS, [Amazon SageMaker Autopilot](https://aws.amazon.com/sagemaker/autopilot/?sagemaker-data-wrangler-whats-new.sort-by=item.additionalFields.postDateTime&sagemaker-data-wrangler-whats-new.sort-order=desc)
    (accessed 4/1/2023)'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '[2]: AWS，[Amazon SageMaker Autopilot](https://aws.amazon.com/sagemaker/autopilot/?sagemaker-data-wrangler-whats-new.sort-by=item.additionalFields.postDateTime&sagemaker-data-wrangler-whats-new.sort-order=desc)（访问于2023年4月1日）'
