- en: How Decision Trees Split Nodes, from Loss Function Perspective
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从损失函数的角度来看，决策树如何分裂节点
- en: 原文：[https://towardsdatascience.com/how-decision-trees-split-nodes-from-loss-function-perspective-60a2f2124b4e](https://towardsdatascience.com/how-decision-trees-split-nodes-from-loss-function-perspective-60a2f2124b4e)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/how-decision-trees-split-nodes-from-loss-function-perspective-60a2f2124b4e](https://towardsdatascience.com/how-decision-trees-split-nodes-from-loss-function-perspective-60a2f2124b4e)
- en: Learn how a decision tree splits nodes only to minimize its loss function
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 了解决策树如何通过分裂节点来最小化其损失函数
- en: '[](https://jasonweiyi.medium.com/?source=post_page-----60a2f2124b4e--------------------------------)[![Wei
    Yi](../Images/24b7a438912082519f24d18e11ac9638.png)](https://jasonweiyi.medium.com/?source=post_page-----60a2f2124b4e--------------------------------)[](https://towardsdatascience.com/?source=post_page-----60a2f2124b4e--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----60a2f2124b4e--------------------------------)
    [Wei Yi](https://jasonweiyi.medium.com/?source=post_page-----60a2f2124b4e--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://jasonweiyi.medium.com/?source=post_page-----60a2f2124b4e--------------------------------)[![魏毅](../Images/24b7a438912082519f24d18e11ac9638.png)](https://jasonweiyi.medium.com/?source=post_page-----60a2f2124b4e--------------------------------)[](https://towardsdatascience.com/?source=post_page-----60a2f2124b4e--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----60a2f2124b4e--------------------------------)
    [魏毅](https://jasonweiyi.medium.com/?source=post_page-----60a2f2124b4e--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----60a2f2124b4e--------------------------------)
    ·12 min read·May 15, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----60a2f2124b4e--------------------------------)
    ·阅读时间 12 分钟 ·2023年5月15日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/de317baa764ffb421a758d3453dbd4f5.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/de317baa764ffb421a758d3453dbd4f5.png)'
- en: Photo by [Ernest Brillo](https://unsplash.com/@ernest_brillo?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由 [Ernest Brillo](https://unsplash.com/@ernest_brillo?utm_source=medium&utm_medium=referral)
    提供，来源于 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: When talking about decision tree node splitting, I often hear phrases such as
    “variance reduction” and “information gain maximization”. And I’m always terrified
    by them. These are not something I am comfortable with using my every day vocabulary,
    until I realized that these phrases are synonyms for “minimizing the decision
    tree’s loss function”.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在谈论决策树节点分裂时，我经常听到诸如“方差减少”和“信息增益最大化”的术语。我总是对这些术语感到恐惧。这些并不是我日常用词中熟悉的内容，直到我意识到这些术语是“最小化决策树损失函数”的同义词。
- en: Which loss function? Well, every machine learning model needs a loss function.
    The loss function quantifies the *difference* between a model’s predictions and
    the actuals. An alternative term is the *error* in the model’s prediction. The
    decision tree model is of no exception.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 哪种损失函数？嗯，每个机器学习模型都需要一个损失函数。损失函数量化了模型预测与实际值之间的*差异*。另一种说法是模型预测中的*误差*。决策树模型也不例外。
- en: To me, understanding the loss function is the most important step in understanding
    a machine learning model. This is because a loss function encodes everything we
    want our model to be good at using a single number. Just like we quantify how
    good a man is by a single number; I recall that is a length measure of some sort,
    right, wink, wink?
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 对我来说，理解损失函数是理解机器学习模型的最重要一步。这是因为损失函数通过一个单一的数字编码了我们希望模型在各个方面表现良好的标准。就像我们用一个单一的数字来量化一个人的优秀程度；我记得那是某种长度的度量，对吧，眨眼，眨眼？
- en: Since the loss function measures the difference between a tree’s predictions
    and the actuals, to understand the loss function, we first need to understand
    how a decision tree makes predictions. Let’s start with the regression tree, and
    then move on to the classification tree.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 由于损失函数衡量的是树的预测与实际值之间的差异，为了理解损失函数，我们首先需要了解决策树是如何进行预测的。我们从回归树开始，然后再转到分类树。
- en: How a regression tree makes predictions
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 回归树如何进行预测
- en: 'A regression decision tree makes prediction in the form of a continuous value.
    For example, a regression tree that predicts a person’s salary from his or her
    age and gender. Here is a little dataset:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 回归决策树的预测形式是一个连续值。例如，一个回归树根据一个人的年龄和性别来预测其薪资。这里有一个小数据集：
- en: '![](../Images/649ffed19c626688ad7efad936a08972.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/649ffed19c626688ad7efad936a08972.png)'
- en: Example dataset for regression tree, with Salary as target variable, by author
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个用于回归树的示例数据集，以薪资作为目标变量，作者提供
- en: We want our regression tree to predict the *target variable* Salary, which is
    a continuous quantity, hence the name “regression tree” from two *features*, Age,
    which is continuous, and Gender, which is categorical. For Salary, I use *y₁*
    to *y₇* instead of real numbers to highlight the fact that we are not supposed
    to know other people’s salary. Let *sₒ={y₁, y₂, y₃, y₄, y₅, y₆, y₇}* represents
    all the salary data entries.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望我们的回归树预测*目标变量*薪资，它是一个连续数量，因此得名“回归树”，由两个*特征*组成，年龄是连续的，性别是分类的。对于薪资，我使用*y₁*到*y₇*而不是实际数字，以突出我们不应知道其他人的薪资。让*sₒ={y₁,
    y₂, y₃, y₄, y₅, y₆, y₇}*表示所有薪资数据条目。
- en: A regression tree uses some condition tuple (feature, value, comparison) to
    split the data into two parts. Note that decision trees are always doing binary
    splits.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 回归树使用一些条件元组（特征，值，比较）将数据分成两部分。请注意，决策树总是进行二分拆分。
- en: The following tree uses the splitting condition tuple (Age, 41, <) to split
    the full dataset *s₀* into two subsets *s1={y₁, y₃, y₅}* and *s2={y₂, y₄, y₆,
    y₇}*. The tuple (Age, 41, <) means that the tree uses the condition “Age < 41”
    to split all the data points that reaches the current root, which is the full
    dataset at the moment, into two non-overlapping subsets.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 以下树使用拆分条件元组（年龄，41，<）将完整数据集*s₀*拆分成两个子集*s1={y₁, y₃, y₅}*和*s2={y₂, y₄, y₆, y₇}*。元组（年龄，41，<）表示树使用条件“年龄
    < 41”将当前根节点（目前是完整数据集）中的所有数据点拆分成两个不重叠的子集。
- en: I *visualised* this split by placing the subset *s1* on the left branch and
    *s2* on the right branch. Note the splitting condition tuple (Age, 41, <) is for
    illustration purpose, it may not be optimal for the example dataset.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 我*可视化*了这个拆分，将子集*s1*放在左支，将*s2*放在右支。注意，拆分条件元组（年龄，41，<）仅为说明目的，可能并非示例数据集的最佳选择。
- en: '![](../Images/a99c3756d4a82ae0ad3f80d8e490c79d.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a99c3756d4a82ae0ad3f80d8e490c79d.png)'
- en: Regression tree with splitting condition Age < 41, by author
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的回归树，拆分条件为年龄 < 41
- en: Regression tree makes predictions by averaging
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 回归树通过取平均值来进行预测
- en: 'This little regression tree with one split, makes the salary prediction for
    a person data point in the following way:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 这个小的回归树通过一个拆分为一个数据点的薪资预测如下：
- en: 'If that person is younger than 41 years old, he reaches the left branch, i.e.
    the subset *s1*; the model then predicts the quantity *ŷₛ₁* which is the average
    of the salaries in the subset *s1*:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 如果该人年龄小于41岁，他将进入左支，即子集*s1*；模型随后预测的数量是*ŷₛ₁*，即子集*s1*中薪资的平均值：
- en: '![](../Images/13d5134b826e8011bec819644e396971.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/13d5134b826e8011bec819644e396971.png)'
- en: 'If that person is older or equal than 41 years, he reaches the right branch,
    i.e., the subset *s2*; the model predicts the quantity *ŷₛ₂*, which is the average
    of the salaries in the subset *s2*:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 如果该人年龄大于或等于41岁，他将进入右支，即子集*s2*；模型预测的数量是*ŷₛ₂*，即子集*s2*中薪资的平均值：
- en: '![](../Images/463e80026d9b07a4976f9b5108d7b5fa.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/463e80026d9b07a4976f9b5108d7b5fa.png)'
- en: 'In other words, for a data point, a regression tree makes prediction by:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，对于一个数据点，回归树的预测方法是：
- en: first letting the data point fall through the tree and reaches a leaf node
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先让数据点通过树，直到到达叶节点
- en: collecting the target variable values for the training data subset that are
    in that leaf node
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 收集目标变量值，以便用于位于该叶节点的训练数据子集
- en: using the average of those target values as the prediction.
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用这些目标值的平均值作为预测。
- en: Loss function for regression trees
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 回归树的损失函数
- en: Since the regression tree’s prediction is a continuous quantity, the most obvious
    choice of a loss function that measures the error between this prediction and
    the actual is the quadratic loss, or, equivalently, the mean squared loss (MSE).
    Since there are two branches, we define their loss terms respectively.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 由于回归树的预测是一个连续数量，最明显的损失函数选择是测量该预测与实际值之间误差的平方损失，或等效的均方误差（MSE）。由于有两个分支，我们分别定义它们的损失项。
- en: 'For the subset *s1*:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 对于子集*s1*：
- en: '![](../Images/3d483275c0143a369d527432d6912afb.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3d483275c0143a369d527432d6912afb.png)'
- en: 'For the subset *s2*:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 对于子集*s2*：
- en: '![](../Images/b36bcbd419fdd7e184553bb8647209b0.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b36bcbd419fdd7e184553bb8647209b0.png)'
- en: where *|s1|* denotes the size of the subset *s1*, that is, 3, and *|s2|* the
    size of the subset *s2*, that is 4\. The ⅓ and ¼ normalisation terms facts the
    size of the two subsets into the loss function. So *Lₛ₁* and *Lₛ₂* are averaged
    *per data point* loss.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *|s1|* 表示子集 *s1* 的大小，即3，*|s2|* 表示子集 *s2* 的大小，即4。⅓和¼的标准化项将两个子集的大小考虑到损失函数中。因此，*Lₛ₁*
    和 *Lₛ₂* 是 *每数据点* 的平均损失。
- en: 'The overall loss for the whole tree is then the sum of these two loss terms:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 整棵树的总体损失是这两个损失项的总和：
- en: '![](../Images/0e620014efe55ab107c7839e2355233f.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0e620014efe55ab107c7839e2355233f.png)'
- en: The fact that *Lₛ₁* and *Lₛ₂* are averaged per data point loss means we don’t
    need to put coefficients in front of them in the total loss *L* to account for
    the size difference in *s1* and *s2*. The subset size difference is already accounted
    for by the normalization constant ⅓ and ¼.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '*Lₛ₁* 和 *Lₛ₂* 每数据点损失的事实意味着我们不需要在总损失 *L* 前面加上系数来考虑 *s1* 和 *s2* 的大小差异。子集大小差异已由标准化常数⅓和¼考虑。'
- en: Finding splitting condition for a decision tree
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为决策树寻找分裂条件
- en: To learn a model from data, or equivalently, optimize a model, we change the
    model parameters such that the model’s predictions makes an error as small as
    possible. In other words, the model’s predictions should minimize the above loss
    function *L*.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 为了从数据中学习模型，或者说，优化模型，我们改变模型参数，使得模型的预测误差尽可能小。换句话说，模型的预测应该最小化上述损失函数 *L*。
- en: 'In the regression tree, at each branching, the model parameter is the splitting
    condition tuple, such as (Age, 40, <). The algorithm to decide which tuple to
    split the tree involves the following three steps:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在回归树中，每次分支时，模型参数是分裂条件元组，如（年龄，40，<）。决定分裂树的元组的算法包括以下三个步骤：
- en: 1\. Identify a suitable subset of splitting condition tuples
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1\. 确定适合的分裂条件元组子集
- en: 'Looking at features in data and find candidate splitting conditions among all
    possible splitting conditions:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 观察数据中的特征，并在所有可能的分裂条件中找到候选分裂条件：
- en: If a feature is continuous, such as Age, look at the range of this feature and
    decide on a short list of splitting values. For example, the Age in the data range
    from 20 to 70, then the short list of splitting values can be 30, 40, 50, 60\.
    This gives the following four tuples (Age, 30, <), (Age, 40, <), (Age, 50, <)
    and (Age, 60, <). For a continuous feature, there can be infinite number of splitting
    values, the algorithm can only afford to choose a *small subset* using heuristics.
    These subset may not be optimal, but it is a tradeoff between model precision
    and computation.
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果特征是连续的，比如年龄，则查看此特征的范围并决定一个短列表的分裂值。例如，数据中的年龄范围从20到70，则短列表的分裂值可以是30、40、50、60。这给出以下四个元组（年龄，30，<）、（年龄，40，<）、（年龄，50，<）和（年龄，60，<）。对于连续特征，可以有无限多个分裂值，算法只能使用启发式方法选择一个*小子集*。这些子集可能不是最优的，但这是模型精度和计算之间的折衷。
- en: If a feature is categorical, such as Gender, use its values as splitting values.
    This gives the two tuples (Gender, Male, =), (Gender, Female, =), which is reduced
    to just one tuple (Gender, Male, =) for this binary feature.
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果特征是类别型的，比如性别，则使用它的值作为分裂值。这给出两个元组（性别，男，=），（性别，女，=），对于这个二元特征，简化为一个元组（性别，男，=）。
- en: So we have five splitting conditional tuples in total.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们总共有五个分裂条件元组。
- en: 2\. Partition data using splitting condition tuples and evaluate loss
  id: totrans-50
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2\. 使用分裂条件元组对数据进行分区并评估损失
- en: Partition the dataset according to each of the splitting condition tuple and
    evaluate the loss function *L*. In the example, we have five tuples, so we partition
    the data five times according to each tuple, evaluate the loss function five times,
    each time based on a different partition of the data.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 根据每个分裂条件元组对数据集进行分区，并评估损失函数 *L*。在这个例子中，我们有五个元组，因此我们根据每个元组将数据分为五次，评估五次损失函数，每次基于数据的不同分区。
- en: 3\. Choose the splitting condition tuple with smallest loss
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3\. 选择最小损失的分裂条件元组
- en: Now the algorithm chooses the splitting condition tuple that yields the smallest
    loss and puts the two partitioned data subsets from this condition into the two
    leaf nodes created by the split.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，算法选择能产生最小损失的分裂条件元组，并将这个条件下的两个分区数据子集放入由分裂创建的两个叶节点中。
- en: It then treats each leaf node as the root of a new tree, and applies the same
    three steps for each new tree to do further splitting.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 然后它将每个叶节点视为新树的根，并对每棵新树应用相同的三个步骤进行进一步分裂。
- en: Quantifying model’s prediction improvement
  id: totrans-55
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 量化模型预测的改进
- en: That is how a regression tree splits nodes — it splits nodes to minimize its
    loss function, which is the mean squared loss. After the split, the model can
    hopefully make better predictions in the sense that the predictions produce smaller
    errors. The model can do that because it extracted information from the dataset.
    To quantify how much information has been extracted before and after the split,
    calculate the loss over the full dataset s₀, which is
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是回归树如何拆分节点——它拆分节点以最小化其损失函数，即均方损失。拆分后，模型可以在预测产生更小误差的意义上进行更好的预测。模型之所以能做到这一点，是因为它从数据集中提取了信息。为了量化拆分前后提取的信息量，请计算整个数据集
    s₀ 上的损失，公式为
- en: '![](../Images/c02aedc33c091b54b6e8af3cf511bca9.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c02aedc33c091b54b6e8af3cf511bca9.png)'
- en: 'Then do a differencing between the two:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 然后对两者进行差分：
- en: '![](../Images/3e5c46e4f03eeff12b03379ec00d76e3.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3e5c46e4f03eeff12b03379ec00d76e3.png)'
- en: Variance reduction
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 方差减少
- en: 'You sometimes hear the phrase “variance reduction” in the discussion of regression
    tree splitting. This is because if we look at the tree’s prediction and the corresponding
    loss, shown here again for just the left branch:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 你有时会在回归树拆分的讨论中听到“方差减少”这个短语。这是因为如果我们查看树的预测和相应的损失，如下所示，仅为左分支：
- en: 'Prediction:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 预测：
- en: '![](../Images/13d5134b826e8011bec819644e396971.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/13d5134b826e8011bec819644e396971.png)'
- en: 'Loss:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 损失：
- en: '![](../Images/3d483275c0143a369d527432d6912afb.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3d483275c0143a369d527432d6912afb.png)'
- en: 'You will realize that for a list of numbers *y₁, y₃, y₅*, the regression tree
    prediction *ŷₛ₁* is the mean of these numbers, which we usually denote as *ȳ*.
    And the mean squared error *Lₛ₁* has the same structure as the formula to compute
    the [variance](https://en.wikipedia.org/wiki/Variance) of this list of numbers:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 你会发现，对于一系列数字 *y₁, y₃, y₅*，回归树的预测 *ŷₛ₁* 是这些数字的均值，通常记作 *ȳ*。均方误差 *Lₛ₁* 具有与计算该数字列表的[方差](https://en.wikipedia.org/wiki/Variance)相同的结构：
- en: '![](../Images/eac424ac0616db88959295d9a3ee6236.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/eac424ac0616db88959295d9a3ee6236.png)'
- en: where *n=3* in our case. So the above splitting algorithm minimizes the loss,
    or equivalently, reduces the variance. That’s why we say that regression tree
    splits nodes by reducing the variance.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的例子中，*n=3*。因此，上述拆分算法通过最小化损失，或者说，减少方差来实现目标。这就是为什么我们说回归树通过减少方差来拆分节点。
- en: Now move on to classification tree, which makes predictions in the form a a
    categorical value, not a continuous value. You know the drill. First we show how
    a classification tree makes predictions, then define the loss function, and finally
    see how the above splitting algorithm applies to classification trees the same
    way as to regression trees.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 现在转到分类树，它以分类值的形式进行预测，而不是连续值。你知道怎么做的。首先我们展示分类树如何进行预测，然后定义损失函数，最后看看上述拆分算法如何像应用于回归树一样应用于分类树。
- en: How a classification tree makes predictions
  id: totrans-70
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分类树如何进行预测
- en: 'Let’s first create a dataset for the classification task by swapping the Gender
    and Salary columns from the previous dataset:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 首先通过交换之前数据集中性别和薪资列来创建一个分类任务的数据集：
- en: '![](../Images/bc3f7344feeaed1084dfabcb3eaf7ecf.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bc3f7344feeaed1084dfabcb3eaf7ecf.png)'
- en: Example dataset for classification tree, with Gender as target variable, by
    author
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 用于分类树的示例数据集，目标变量为性别，作者
- en: Here, the target variable to predict is Gender, which is a categorical variable,
    so we need a classification tree. The two features are Age and Salary.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，目标变量是性别（Gender），它是一个分类变量，因此我们需要一个分类树。两个特征是年龄（Age）和薪资（Salary）。
- en: 'Suppose we have the following classification tree:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有以下分类树：
- en: '![](../Images/577a8fb1f2f3356b294d3122b0d7d017.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/577a8fb1f2f3356b294d3122b0d7d017.png)'
- en: Classification tree with split condition Age<41, by author
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 拥有拆分条件 Age<41 的分类树，作者
- en: This tree needs to classify a data point into the Female class, or the Male
    class. Suppose this tree still use the condition Age < 41 to splits the dataset
    *s₀* into *s1={y₁F, y₃F, y₅M}* and *s2={y₂M, y₄F, y₆M, y₇F}*. I attached the gender
    symbol “F” or “M” to each data point, such as “*y1F”* to make the gender of each
    data point reaching a leaf node more clear.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 这个树需要将一个数据点分类到女性类别（Female）或男性类别（Male）。假设这个树仍然使用条件 Age < 41 来将数据集 *s₀* 拆分成 *s1={y₁F,
    y₃F, y₅M}* 和 *s2={y₂M, y₄F, y₆M, y₇F}*。我在每个数据点旁边附上性别符号“F”或“M”，例如“*y1F”*，以使每个数据点的性别在达到叶节点时更加清晰。
- en: So after the split, ⅔ of the data points arrived at the left leaf node is female,
    denoted by *P(F)=⅔*, which says the *occurrence probability* of female data points
    in this node is ⅔. Likewise, for male, it is *P(M)=⅓*.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 所以在分裂后，到达左叶节点的⅔数据点是女性，用*P(F)=⅔*表示，这说明该节点中女性数据点的*发生概率*是⅔。类似地，对于男性，它是*P(M)=⅓*。
- en: Classification trees make predictions by majority vote
  id: totrans-80
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分类树通过多数投票来做出预测
- en: We need a prediction mechanism other than averaging (like in the case of regression
    tree) because ⅔ of a woman plus *⅓* of a man is not a thing, despite the active
    LGBT movement in recent years.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要一种不同于平均的方法（如回归树中的情况），因为⅔的女性加上*⅓*的男性不是一个实际的情况，尽管近年来LGBT运动积极推进。
- en: A better choice is *majority vote —* predicting the class with the highest occurrence
    probability. For the left node, since there are more female data points than male
    data points, the model will predict the female class.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 更好的选择是*多数投票*——预测发生概率最高的类别。对于左节点，由于女性数据点多于男性数据点，模型将预测女性类别。
- en: What about the right leaf node, where the probability of female and male data
    points are equal, both are ½? In this case where the probabilities for the two
    classes are tied, the model can break the tied randomly, say, by predict the male
    class. In reality, the splitting algorithm discourages the tied situation to happen,
    as we will see later.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 那右侧叶节点呢，其中女性和男性数据点的概率相等，都是½？在这种情况下，当两个类别的概率相等时，模型可以随机打破平局，例如预测男性类别。实际上，分裂算法会防止平局情况的发生，正如我们稍后将看到的那样。
- en: Loss function for classification trees
  id: totrans-84
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分类树的损失函数
- en: 'Let’s work on the loss function that quantifies the error that the tree makes.
    Intuitively, in a leaf node, if the occurrence probability of the major class
    is larger, the model’s prediction for that node is better:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来处理量化树的错误的损失函数。直观地说，在一个叶节点中，如果主要类别的发生概率更大，则模型对该节点的预测更好：
- en: Our tree predicts Female for the left node, this prediction is right ⅔ of time,
    and wrong *⅓* of the time*.* In other words, if the distribution of different
    classes in a node is **less uniform**, the prediction is better.
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们的树对左节点预测为女性，这一预测在⅔的情况下是正确的，而在*⅓*的情况下是错误的。换句话说，如果一个节点中不同类别的分布**不够均匀**，预测会更好。
- en: It predicts Male for the right node, this prediction is correct only ½ of the
    time. In other words, if the distribution of different classes in a node is **more
    uniform**, the prediction is worse.
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它对右节点预测为男性，这一预测仅在½的情况下是正确的。换句话说，如果节点中不同类别的分布**更均匀**，预测效果会更差。
- en: 'So we need a measure for uniformity of the class labels in a set of data points.
    Many measures exist. A common choice is entropy:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们需要一个衡量数据点类别标签均匀性的指标。存在许多度量方法。常见的选择是熵：
- en: '![](../Images/6da67e07df847c987a8bca41fbcdc20b.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6da67e07df847c987a8bca41fbcdc20b.png)'
- en: where the sum is over all the classes *c*, Female and Male in our case. *P(c)*
    is the occurrence probability for the class *c*. And *log(P(c))* is the log of
    that occurrence probability.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 其中求和是对所有类别*c*，在我们的案例中是女性和男性。*P(c)* 是类别*c*的发生概率，而*log(P(c))* 是该发生概率的对数。
- en: Please note that entropy is one measure of uniformity; there are other common
    measures, such as Chi-Square and Gini Impurity. Recall that in economics, Gini
    index is used to quantify how wealth is unequally distributed among different
    classes of people in a society — it is a measure of un-uniformity.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，熵是衡量均匀性的一个指标；还有其他常见的衡量方法，如卡方和基尼 impurity。回想一下，在经济学中，基尼指数用于量化社会中财富在不同人群之间分配的不均等性——它是一个不均匀性的度量。
- en: If you want to know *why* entropy can measure uniformity, please refer to other
    [resources](https://stats.stackexchange.com/questions/66935/measure-for-the-uniformity-of-a-distribution).
    In this article, it is sufficient to know that entropy measures uniformity. But
    just for fun, we can plug in some number to see whether entropy tells us that
    our left branch is more uniform the the right branch.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想知道*为什么*熵可以衡量均匀性，请参阅其他[资源](https://stats.stackexchange.com/questions/66935/measure-for-the-uniformity-of-a-distribution)。在本文中，只需知道熵可以衡量均匀性就足够了。为了好玩，我们可以插入一些数字来看看熵是否告诉我们左分支比右分支更均匀。
- en: 'For the left leaf node:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 对于左侧叶节点：
- en: '![](../Images/c1831178dade93e44d71cee03b637f82.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c1831178dade93e44d71cee03b637f82.png)'
- en: 'For the right leaf node:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 对于右侧叶节点：
- en: '![](../Images/9daf06f993310e002d1c3cadf860b354.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9daf06f993310e002d1c3cadf860b354.png)'
- en: We can see that the loss for the left leaf node *Hₛ₁=0.637* is indeed smaller
    than the loss for the right leaf node *Hₛ*₂=0.693, conforming that the model’s
    prediction is better for data points in the left node.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，左叶节点的损失*Hₛ₁=0.637*确实小于右叶节点的损失*Hₛ*₂=0.693，这表明模型对左节点的数据点的预测更好。
- en: Now, just as in the case of regression tree, we need to combine *Hₛ₁* and *Hₛ*₂
    together to represent the overall loss for the whole tree.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，就像回归树的情况一样，我们需要将*Hₛ₁*和*Hₛ*₂结合起来，以表示整个树的总体损失。
- en: In the regression tree case, we added *Lₛ₁* and *Lₛ*₂ together because *Lₛ₁*
    and *Lₛ*₂ represents per data point loss, with the sizes of the different subset
    *s1* and *s2* normalized. But in classification tree case, this is not the case
    for the above entropy-based loss terms. If you look at the above definition of
    *Hₛ₁* and *Hₛ*₂, the size of the subset s1 and s2 is not mentioned.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 在回归树的情况下，我们将*Lₛ₁*和*Lₛ*₂相加，因为*Lₛ₁*和*Lₛ*₂代表每个数据点的损失，并且不同子集的大小*s1*和*s2*经过标准化。但是在分类树的情况下，上述基于熵的损失项并非如此。如果你查看*Hₛ₁*和*Hₛ*₂的定义，子集s1和s2的大小并没有提到。
- en: Intuitively, when a model makes a prediction error for a large subset of data,
    even if that error is small, the impact to the overall model performance measure
    can be big because of the large size of the data subset.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 从直观上看，当模型对大量数据子集的预测产生错误时，即使这个错误很小，对整体模型性能指标的影响也可能很大，因为数据子集的规模很大。
- en: 'To fact the data subset size in, we define the total loss H as the weighted
    sum of *Hₛ₁* and *Hₛ*₂, with weights belong the data subset proportions:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 为了考虑数据子集的大小，我们将总损失H定义为加权和，权重根据数据子集的比例确定：
- en: '![](../Images/5bac29b5ba8f09b9a75a22db5a77c1f7.png)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5bac29b5ba8f09b9a75a22db5a77c1f7.png)'
- en: Finding splitting condition for a classification tree
  id: totrans-103
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 寻找分类树的分裂条件
- en: With the loss function *H* defined for a classification tree, we can apply the
    same algorithm used for regression tree to split nodes. We just need to use the
    new loss *H* for classification, instead of the loss *L* for regression.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 使用为分类树定义的损失函数*H*，我们可以应用用于回归树的相同算法来分裂节点。我们只需要使用分类的新损失*H*，而不是回归的损失*L*。
- en: This makes sense because the weighted entropy *H* represents the error that
    our classification tree makes — more uniform, larger error, less uniform, smaller
    error. The above splitting algorithm will chooses a split condition that minimizes
    this error, hence encourages data points reaching every leaf node to be less uniform.
    So the tree can make less mistakes. Also, by encouraging less uniform data subsets
    in split leaf nodes, it is unlikely that a tie between two classes will happen.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 这很有意义，因为加权熵*H*表示我们分类树的错误——越均匀，错误越大；越不均匀，错误越小。上述分裂算法会选择最小化这种错误的分裂条件，从而鼓励每个叶节点的数据点变得更少均匀。因此，树可以减少错误。同时，通过在分裂叶节点中鼓励较少均匀的数据子集，不太可能出现两个类别之间的平局。
- en: How amazing!
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 太神奇了！
- en: Quantifying model’s prediction improvement
  id: totrans-107
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 量化模型的预测改进
- en: Just as the case in regression tree, you can measure the information extracted
    from data between and after the split by computing the loss on the root node *Hₛ₀*,
    which is before splitting, and do a differencing from *H*, that is *H-Hₛ₀*.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 就像回归树的情况一样，你可以通过计算分裂前的根节点*Hₛ₀*上的损失来衡量数据在分裂前后的信息提取，然后计算*H-Hₛ₀*。
- en: Information Gain
  id: totrans-109
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 信息增益
- en: Before I end, I want to mention information gain as you will see it everywhere
    when classification trees are in discussion.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 在结束之前，我想提到信息增益，因为你会在讨论分类树时到处看到它。
- en: '![](../Images/62a8cde34c07890200b246f66b772a3b.png)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/62a8cde34c07890200b246f66b772a3b.png)'
- en: You can see information gain is merely the negation of the entropy. So instead
    of finding a split condition that minimizes the entropy loss, we can equivalently
    find a split condition that maximizes the information gain. The constant 1 in
    the information gain formula makes no difference in the splitting algorithm. If
    the algorithm finds a splitting condition that maximizes *-Entropy*, that same
    condition also maximizes information gain, which is *1-Entropy*.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到信息增益实际上是熵的负值。因此，我们可以等效地找到一个最大化信息增益的分裂条件，而不是最小化熵损失的分裂条件。信息增益公式中的常数1在分裂算法中没有影响。如果算法找到一个最大化*-熵*的分裂条件，那么这个条件也会最大化信息增益，即*1-熵*。
- en: Conclusions
  id: totrans-113
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: This article explains how decision trees, both regression and classification
    trees, splits node from the loss function point of view. This view makes the notion
    such as reduction of variance, information gain, a lot more intuitive to understand.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 本文解释了决策树，包括回归树和分类树，从损失函数的角度如何进行节点分裂。这种视角使得如方差减少、信息增益等概念变得更加直观易懂。
