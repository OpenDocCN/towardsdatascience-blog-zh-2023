- en: LMQL — SQL for Language Models
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LMQL — 语言模型的 SQL
- en: 原文：[https://towardsdatascience.com/lmql-sql-for-language-models-d7486d88c541?source=collection_archive---------0-----------------------#2023-11-27](https://towardsdatascience.com/lmql-sql-for-language-models-d7486d88c541?source=collection_archive---------0-----------------------#2023-11-27)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/lmql-sql-for-language-models-d7486d88c541?source=collection_archive---------0-----------------------#2023-11-27](https://towardsdatascience.com/lmql-sql-for-language-models-d7486d88c541?source=collection_archive---------0-----------------------#2023-11-27)
- en: Yet another tool that could help you with LLM applications
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 另一个可能对你的 LLM 应用有帮助的工具
- en: '[](https://miptgirl.medium.com/?source=post_page-----d7486d88c541--------------------------------)[![Mariya
    Mansurova](../Images/b1dd377b0a1887db900cc5108bca8ea8.png)](https://miptgirl.medium.com/?source=post_page-----d7486d88c541--------------------------------)[](https://towardsdatascience.com/?source=post_page-----d7486d88c541--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----d7486d88c541--------------------------------)
    [Mariya Mansurova](https://miptgirl.medium.com/?source=post_page-----d7486d88c541--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://miptgirl.medium.com/?source=post_page-----d7486d88c541--------------------------------)[![Mariya
    Mansurova](../Images/b1dd377b0a1887db900cc5108bca8ea8.png)](https://miptgirl.medium.com/?source=post_page-----d7486d88c541--------------------------------)[](https://towardsdatascience.com/?source=post_page-----d7486d88c541--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----d7486d88c541--------------------------------)
    [Mariya Mansurova](https://miptgirl.medium.com/?source=post_page-----d7486d88c541--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F15a29a4fc6ad&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flmql-sql-for-language-models-d7486d88c541&user=Mariya+Mansurova&userId=15a29a4fc6ad&source=post_page-15a29a4fc6ad----d7486d88c541---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----d7486d88c541--------------------------------)
    ·17 min read·Nov 27, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fd7486d88c541&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flmql-sql-for-language-models-d7486d88c541&user=Mariya+Mansurova&userId=15a29a4fc6ad&source=-----d7486d88c541---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F15a29a4fc6ad&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flmql-sql-for-language-models-d7486d88c541&user=Mariya+Mansurova&userId=15a29a4fc6ad&source=post_page-15a29a4fc6ad----d7486d88c541---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----d7486d88c541--------------------------------)
    ·17分钟阅读·2023年11月27日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fd7486d88c541&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flmql-sql-for-language-models-d7486d88c541&user=Mariya+Mansurova&userId=15a29a4fc6ad&source=-----d7486d88c541---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fd7486d88c541&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flmql-sql-for-language-models-d7486d88c541&source=-----d7486d88c541---------------------bookmark_footer-----------)![](../Images/a3ba043a5a4b6836f2488f4c4b6759e7.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fd7486d88c541&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flmql-sql-for-language-models-d7486d88c541&source=-----d7486d88c541---------------------bookmark_footer-----------)![](../Images/a3ba043a5a4b6836f2488f4c4b6759e7.png)'
- en: Image by DALL-E 3
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来自 DALL-E 3
- en: I’m sure you’ve heard about SQL or even have mastered it. **SQL (Structured
    Query Language)** is a declarative language widely used to work with database
    data.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 我相信你一定听说过 SQL，甚至已经掌握了它。 **SQL（结构化查询语言）** 是一种广泛用于数据库数据操作的声明式语言。
- en: According to the annual [StackOverflow survey](https://survey.stackoverflow.co/2023/#technology-most-popular-technologies),
    SQL is still one of the most popular languages in the world. For professional
    developers, SQL is in the top-3 languages (after Javascript and HTML/CSS). More
    than a half of professionals use it. Surprisingly, SQL is even more popular than
    Python.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 根据年度 [StackOverflow 调查](https://survey.stackoverflow.co/2023/#technology-most-popular-technologies)，SQL
    仍然是全球最受欢迎的语言之一。对于专业开发人员来说，SQL 位列前三名（仅次于 Javascript 和 HTML/CSS）。超过一半的专业人员使用 SQL。令人惊讶的是，SQL
    甚至比 Python 更受欢迎。
- en: '![](../Images/72a34ba2e51238a06a33f6289c2d0bef.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/72a34ba2e51238a06a33f6289c2d0bef.png)'
- en: Graph by author, data from [StackOverflow survey](https://survey.stackoverflow.co/2023/#technology-most-popular-technologies)
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 图表由作者提供，数据来自 [StackOverflow 调查](https://survey.stackoverflow.co/2023/#technology-most-popular-technologies)
- en: SQL is a common way to talk to your data in a database. So, it is no surprise
    that there are attempts to use a similar approach for LLMs. In this article, I
    would like to tell you about one such approach called LMQL.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: SQL 是与数据库中的数据交互的常见方式。因此，使用类似的方法来处理 LLM 也就不足为奇了。在这篇文章中，我想告诉你一种名为 LMQL 的方法。
- en: What is LMQL?
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是 LMQL？
- en: '[**LMQL**](https://lmql.ai/) **(Language Model Query Language)** is an open-source
    programming language for language models. LMQL is released under [Apache 2.0 license](https://github.com/eth-sri/lmql/blob/main/LICENSE),
    which allows you to use it commercially.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '[**LMQL**](https://lmql.ai/) **（语言模型查询语言）** 是一种用于语言模型的开源编程语言。LMQL 在 [Apache
    2.0 许可证](https://github.com/eth-sri/lmql/blob/main/LICENSE)下发布，允许你商业使用。'
- en: 'LMQL was developed by ETH Zurich researchers. They proposed a novel idea of
    LMP (Language Model Programming). LMP combines natural and programming languages:
    text prompt and scripting instructions.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: LMQL 是由 ETH 苏黎世的研究人员开发的。他们提出了一种新的 LMP（语言模型编程）理念。LMP 结合了自然语言和编程语言：文本提示和脚本指令。
- en: 'In [the original paper](https://arxiv.org/abs/2212.06094), *“Prompting Is Programming:
    A Query Language for Large Language Models” by Luca Beurer-Kellner, Marc Fischer
    and Martin Vechev*, the authors flagged the following challenges of the current
    LLM usage:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '在 [原始论文](https://arxiv.org/abs/2212.06094) 中，*“Prompting Is Programming: A
    Query Language for Large Language Models” 由 Luca Beurer-Kellner、Marc Fischer 和
    Martin Vechev*，作者标记了当前 LLM 使用的以下挑战：'
- en: '**Interaction.** For example, we could use meta prompting, asking LM to expand
    the initial prompt. As a practical case, we could first ask the model to define
    the language of the initial question and then respond in that language. For such
    a task, we will need to send the first prompt, extract language from the output,
    add it to the second prompt template and make another call to the LM. There’s
    quite a lot of interactions we need to manage. With LMQL, you can define multiple
    input and output variables within one prompt. More than that, LMQL will optimise
    overall likelihood across numerous calls, which might yield better results.'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**交互。** 例如，我们可以使用元提示，让 LM 扩展初始提示。作为一个实际案例，我们可以首先要求模型定义初始问题的语言，然后用这种语言回答。对于这样的任务，我们需要发送第一个提示，从输出中提取语言，将其添加到第二个提示模板中，并再次调用
    LM。我们需要管理的交互非常多。使用 LMQL，你可以在一个提示中定义多个输入和输出变量。更重要的是，LMQL 将优化多个调用的整体可能性，这可能会产生更好的结果。'
- en: '**Constraint & token representation.** The current LMs don’t provide the functionality
    to constrain output, which is crucial if we use LMs in production. Imagine building
    a sentiment analysis in production to mark negative reviews in our interface for
    CS agents. Our program would expect to receive from the LLM “positive”, “negative”,
    or “neutral”. However, quite often, you could get something like “The sentiment
    for provided customer review is positive” from the LLM, which is not so easy to
    process in your API. That’s why constraints would be pretty helpful. LMQL allows
    you to control output using human-understandable words (not tokens that LMs operate
    with).'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**约束与令牌表示。** 当前的 LMs 不提供约束输出的功能，这在生产环境中使用 LMs 时至关重要。假设在生产环境中构建情感分析系统，以在我们界面上标记
    CS 代理的负面评论。我们的程序期望从 LLM 接收到“positive”，“negative”或“neutral”。然而，LLM 很可能返回诸如“提供的客户评论的情感是积极的”这样的内容，这在你的
    API 中处理起来并不容易。这就是为什么约束会非常有帮助。LMQL 允许你使用人类可理解的词（而不是 LMs 操作的令牌）来控制输出。'
- en: '**Efficiency and cost.** LLMs are large networks, so they are pretty expensive,
    regardless of whether you use them via API or in your local environment. LMQL
    can leverage predefined behaviour and the constraint of the search space (introduced
    by constraints) to reduce the number of LM invoke calls.'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**效率与成本。** LLM 是大型网络，因此无论你是通过 API 使用它们还是在本地环境中使用，它们的成本都相当高。LMQL 可以利用预定义的行为和搜索空间的约束（由约束引入）来减少
    LM 调用次数。'
- en: As you can see, LMQL can address these challenges. It allows you to combine
    multiple calls in one prompt, control your output and even reduce cost.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，LMQL 可以解决这些挑战。它允许你在一个提示中结合多个调用，控制输出，甚至降低成本。
- en: The impact on cost and efficiency could be pretty substantial. The limitations
    to the search space can significantly reduce costs for LLMs. For example, in the
    cases from [the LMQL paper](https://arxiv.org/abs/2212.06094), there were 75–85%
    fewer billable tokens with LMQL compared to standard decoding, which means it
    will significantly reduce your cost.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 对成本和效率的影响可能相当大。搜索空间的限制可以显著降低 LLM 的成本。例如，在 [LMQL 论文](https://arxiv.org/abs/2212.06094)
    中的案例中，LMQL 的可计费 tokens 比标准解码少了 75–85%，这意味着它将显著降低你的成本。
- en: '![](../Images/4767e98e5ab6f4ff82b8566c9e8f8a4f.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4767e98e5ab6f4ff82b8566c9e8f8a4f.png)'
- en: Image from [the paper by Beurer-Kellner et al. (2023)](https://arxiv.org/abs/2212.06094)
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来自 [Beurer-Kellner 等人 (2023) 的论文](https://arxiv.org/abs/2212.06094)
- en: 'I believe the most crucial benefit of LMQL is the complete control of your
    output. However, with such an approach, you will also have another layer of abstraction
    over LLM (similar to LangChain, which we discussed earlier). It will allow you
    to switch from one backend to another easily if you need to. LMQL can work with
    different backends: OpenAI, HuggingFace Transformers or `llama.cpp`.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 我认为 LMQL 最重要的好处是对输出的完全控制。然而，这种方法也会在 LLM 上增加另一层抽象（类似于我们之前讨论的 LangChain）。这将允许你在需要时轻松切换后端。LMQL
    可以与不同的后端一起使用：OpenAI、HuggingFace Transformers 或 `llama.cpp`。
- en: You can install LMQL locally or use a web-based [Playground](https://lmql.ai/playground/)
    online. Playground can be pretty handy for debugging, but you can only use the
    OpenAI backend here. For all other use cases, you will have to use local installation.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在本地安装 LMQL 或使用基于网页的 [Playground](https://lmql.ai/playground/) 在线工具。Playground
    对于调试非常方便，但你只能在这里使用 OpenAI 后端。对于所有其他用例，你需要使用本地安装。
- en: 'As usual, there are some limitations to this approach:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 像往常一样，这种方法有一些局限性：
- en: This library is not very popular yet, so the community is pretty small, and
    few external materials are available.
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这个库还不是很受欢迎，所以社区相对较小，外部材料也不多。
- en: In some cases, documentation might not be very detailed.
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在某些情况下，文档可能不是很详细。
- en: The most popular and best-performing OpenAI models have [some limitations](https://lmql.ai/docs/models/openai.html#openai-api-limitations),
    so you can’t use the full power of LMQL with ChatGPT.
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最受欢迎且表现最佳的 OpenAI 模型有 [一些限制](https://lmql.ai/docs/models/openai.html#openai-api-limitations)，因此你不能在
    ChatGPT 上充分利用 LMQL 的全部功能。
- en: I wouldn’t use LMQL in production since I can’t say that it’s a mature project.
    For example, distribution over tokens provides pretty poor accuracy.
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我不会在生产环境中使用 LMQL，因为我不能说它是一个成熟的项目。例如，token 的分布提供的准确性相当差。
- en: Somewhat close alternative to LMQL is [Guidance](https://github.com/guidance-ai/guidance).
    It also allows you to constrain generation and control the LM’s output.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 与 LMQL 相似的替代方案是 [Guidance](https://github.com/guidance-ai/guidance)。它也允许你约束生成并控制语言模型的输出。
- en: Despite all the limitations, I like the concept of Language Model Programming,
    and that’s why I’ve decided to discuss it in this article.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管存在所有限制，我仍然喜欢语言模型编程的概念，这也是我决定在本文中讨论它的原因。
- en: If you’re interested to learn more about LMQL from its authors, check [this
    video](https://www.youtube.com/watch?v=4StBzmb6OH0).
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你有兴趣了解更多关于 LMQL 的信息，可以查看 [这个视频](https://www.youtube.com/watch?v=4StBzmb6OH0)。
- en: LMQL syntax
  id: totrans-35
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LMQL 语法
- en: Now, we know a bit what LMQL is. Let’s look at the example of an LMQL query
    to get acquainted with its syntax.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们对 LMQL 有了一些了解。让我们看看 LMQL 查询的示例，以熟悉它的语法。
- en: '[PRE0]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: I hope you can guess its meaning. But let’s discuss it in detail.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 希望你能猜出它的含义。但让我们详细讨论一下。
- en: Here’s a scheme for a LMQL query
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 这是 LMQL 查询的示意图
- en: '![](../Images/adc58c40f3c91b3e4b8fe02965607dfa.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/adc58c40f3c91b3e4b8fe02965607dfa.png)'
- en: Image from [paper by Beurer-Kellner et al. (2023)](https://arxiv.org/abs/2212.06094)
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来自 [Beurer-Kellner 等人 (2023) 的论文](https://arxiv.org/abs/2212.06094)
- en: 'Any LMQL program consists of 5 parts:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 任何 LMQL 程序由 5 部分组成：
- en: '`Decoder` defines the decoding procedure used. In simple words, it describes
    the algorithm to pick up the next token. LMQL has three different types of decoders:
    argmax, beam and sample. You can learn about them in more detail from [the paper.](https://arxiv.org/pdf/2212.06094.pdf)'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Decoder` 定义了使用的解码过程。简单来说，它描述了选择下一个 token 的算法。LMQL 有三种不同类型的解码器：argmax、beam
    和 sample。你可以从 [论文](https://arxiv.org/pdf/2212.06094.pdf) 中详细了解它们。'
- en: Actual query is similar to the classic prompt but in Python syntax, which means
    that you could use such structures as loops or if-statements.
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实际查询类似于经典的提示，但使用 Python 语法，这意味着你可以使用像循环或 if 语句这样的结构。
- en: In `from` clause, we specified the model to use (`openai/text-davinci-003` in
    our example).
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 `from` 子句中，我们指定了要使用的模型（在我们的例子中是 `openai/text-davinci-003`）。
- en: '`Where` clause defines constraints.'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Where` 子句定义约束条件。'
- en: '`Distribution` is used when you want to see probabilities for tokens in the
    return. We haven’t used distribution in this query, but we will use it to get
    class probabilities for the sentiment analysis later.'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Distribution` 在你想查看返回中令牌的概率时使用。我们在这个查询中没有使用分布，但稍后我们将用它来获取情感分析的类别概率。'
- en: 'Also, you might have noticed special variables in our query `{name}` and `[RESPONSE]`.
    Let’s discuss how they work:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 另外，你可能已经注意到我们查询中的特殊变量 `{name}` 和 `[RESPONSE]`。我们来讨论一下它们是如何工作的：
- en: '`{name}` is an input parameter. It could be any variable from your scope. Such
    parameters help you create handy functions that could be easily re-used for different
    inputs.'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`{name}` 是一个输入参数。它可以是你范围内的任何变量。这些参数帮助你创建实用的函数，便于不同输入的重复使用。'
- en: '`[RESPONSE]` is a phrase that LM will generate. It can also be called a hole
    or placeholder. All the text before `[RESPONSE]` is sent to LM, and then the model’s
    output is assigned to the variable. It’s handy that you could easily re-use this
    output later in the prompt, referring to it as `{RESPONSE}`.'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`[RESPONSE]` 是一个由 LM 生成的短语。它也可以被称为占位符或占位符。所有在 `[RESPONSE]` 之前的文本都会发送给 LM，然后模型的输出将分配给这个变量。你可以很方便地在提示中重新使用这个输出，将其称为
    `{RESPONSE}`。'
- en: We’ve briefly covered the main concepts. Let’s try it ourselves. Practice makes
    perfect.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经简要介绍了主要概念。现在让我们自己试试。熟能生巧。
- en: Getting started
  id: totrans-52
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 入门指南
- en: Setting up environment
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 设置环境
- en: First of all, we need to set up our environment. To use LMQL in Python, we need
    to install a package first. No surprises, we can just use pip. You need an environment
    with Python ≥ 3.10.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要设置环境。要在 Python 中使用 LMQL，我们需要首先安装一个包。没有意外，我们可以直接使用 pip。你需要一个 Python ≥
    3.10 的环境。
- en: '[PRE1]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: If you want to use LMQL with local GPU, follow the instructions in [the documentation](https://lmql.ai/docs/installation.html).
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想在本地 GPU 上使用 LMQL，请参阅 [文档](https://lmql.ai/docs/installation.html)中的说明。
- en: To use OpenAI models, you need to set up APIKey to access OpenAI. The easiest
    way is to specify the `OPENAI_API_KEY` environment variable.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用 OpenAI 模型，你需要设置 APIKey 来访问 OpenAI。最简单的方法是指定 `OPENAI_API_KEY` 环境变量。
- en: '[PRE2]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: However, OpenAI models have many [limitations](https://lmql.ai/docs/models/openai.html#openai-api-limitations)
    (for example, you won’t be able to get distributions with more than five classes).
    So, we will use Llama.cpp to test LMQL with local models.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，OpenAI 模型有许多 [限制](https://lmql.ai/docs/models/openai.html#openai-api-limitations)（例如，你不能获取多于五个类别的分布）。因此，我们将使用
    Llama.cpp 来测试带有本地模型的 LMQL。
- en: First, you need to install Python binding for Llama.cpp in the same environment
    as LMQL.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，你需要在与 LMQL 相同的环境中安装 Python 绑定的 Llama.cpp。
- en: '[PRE3]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: If you want to use local GPU, specify the following parameters.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想使用本地 GPU，请指定以下参数。
- en: '[PRE4]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Then, we need to load model weights as `.gguf` files. You can find models on
    [HuggingFace Models Hub](https://huggingface.co/models).
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们需要将模型权重加载为 `.gguf` 文件。你可以在 [HuggingFace 模型中心](https://huggingface.co/models)
    找到模型。
- en: 'We will be using two models:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用两个模型：
- en: '`Llama-2-7B` ([link](https://huggingface.co/TheBloke/Llama-2-7B-GGUF))'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Llama-2-7B` ([链接](https://huggingface.co/TheBloke/Llama-2-7B-GGUF))'
- en: '`zephyr-7B-beta` ([link](https://huggingface.co/TheBloke/zephyr-7B-beta-GGUF))'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`zephyr-7B-beta` ([链接](https://huggingface.co/TheBloke/zephyr-7B-beta-GGUF))'
- en: Llama-2–7B is the smallest version of fine-tuned generative text models by Meta.
    It’s a pretty basic model, so we shouldn’t expect outstanding performance from
    it.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: Llama-2–7B 是 Meta 细调生成文本模型中最小的版本。它是一个相当基础的模型，因此我们不应期望它有卓越的表现。
- en: Zephyr is a fine-tuned version of the [Mistral](https://huggingface.co/mistralai/Mistral-7B-v0.1)
    model with decent performance. It performs better in some aspects than a 10x larger
    open-source model Llama-2–70b. However, there’s still some gap between Zephyr
    and proprietary models like ChatGPT or Claude.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: Zephyr 是一个经过微调的 [Mistral](https://huggingface.co/mistralai/Mistral-7B-v0.1)
    模型，性能不错。在某些方面，它的表现优于 10 倍大的开源模型 Llama-2–70b。然而，Zephyr 和像 ChatGPT 或 Claude 这样的专有模型之间仍存在差距。
- en: '![](../Images/ad8a00d37526988daaa9ea0e0f63cb1d.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ad8a00d37526988daaa9ea0e0f63cb1d.png)'
- en: Image from the paper by [Tunstall et al. (2023)](https://arxiv.org/abs/2310.16944)
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源于 [Tunstall 等人 (2023)](https://arxiv.org/abs/2310.16944) 的论文
- en: According to the [LMSYS ChatBot Arena leaderboard](https://huggingface.co/spaces/lmsys/chatbot-arena-leaderboard),
    Zephyr is the best-performing model with 7B parameters. It’s on par with much
    bigger models.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 根据[LMSYS ChatBot Arena排行榜](https://huggingface.co/spaces/lmsys/chatbot-arena-leaderboard)，Zephyr是表现最好的模型，拥有7B参数。它的表现与更大模型相当。
- en: '![](../Images/879e0f123b15262e8fdd22139726d698.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/879e0f123b15262e8fdd22139726d698.png)'
- en: Screenshot of leaderboard | [source](https://huggingface.co/spaces/lmsys/chatbot-arena-leaderboard)
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 排行榜截图 | [来源](https://huggingface.co/spaces/lmsys/chatbot-arena-leaderboard)
- en: Let’s load `.gguf` files for our models.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们加载`.gguf`文件以便使用我们的模型。
- en: '[PRE5]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: We need to download a few GBs so that it might take some time (10–15 minutes
    for each model). Luckily, you need to do it only once.
  id: totrans-77
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 我们需要下载几个GB的数据，所以可能需要一些时间（每个模型10到15分钟）。幸运的是，你只需做一次。
- en: 'You can interact with the local models in two different ways ([documentation](https://lmql.ai/docs/models/hf.html)):'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过两种不同方式与本地模型进行交互（[文档](https://lmql.ai/docs/models/hf.html)）：
- en: Two-process architecture when you have a separate long-running process with
    your model and short-running inference calls. This approach is more suitable for
    production.
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当你有一个独立的长时间运行进程和短时间运行的推断调用时的双进程架构。这种方法更适合生产环境。
- en: For ad-hoc tasks, we could use in-process model loading, specifying `local:`
    before the model name. We will be using this approach to work with the local models.
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于临时任务，我们可以使用进程内模型加载，在模型名称前指定`local:`。我们将使用这种方法来处理本地模型。
- en: Now, we’ve set up the environment, and it’s time to discuss how to use LMQL
    from Python.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们已经设置好了环境，接下来讨论如何从Python中使用LMQL。
- en: Python functions
  id: totrans-82
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Python函数
- en: Let’s briefly discuss how to use LMQL in Python. Playground can be handy for
    debugging, but if you want to use LM in production, you need an API.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 简要讨论如何在Python中使用LMQL。Playground对于调试很有帮助，但如果你想在生产环境中使用LM，您需要一个API。
- en: 'LMQL provides four main approaches to its functionality: `lmql.F` , `lmql.run`
    , `@lmql.query` decorator and [Generations API](https://lmql.ai/docs/lib/generations.html).'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: LMQL提供了四种主要的方法：`lmql.F`、`lmql.run`、`@lmql.query`装饰器和[Generations API](https://lmql.ai/docs/lib/generations.html)。
- en: '[Generations API](https://lmql.ai/docs/lib/generations.html) has been recently
    added. It’s a simple Python API that helps to do inference without writing LMQL
    yourself. Since I am more interested in the LMP concept, we won’t cover this API
    in this article.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '[Generations API](https://lmql.ai/docs/lib/generations.html)最近被添加。它是一个简单的Python
    API，帮助进行推断而不需要自己编写LMQL。由于我更关注LMP概念，本文不涵盖此API。'
- en: Let’s discuss the other three approaches in detail and try to use them.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们详细讨论其他三种方法并尝试使用它们。
- en: First, you could use `lmql.F`. It’s a lightweight functionality similar to lambda
    functions in Python that could allow you to execute part of LMQL code. `lmql.F`
    can have only one placeholder variable that will be returned from the lambda function.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，你可以使用`lmql.F`。它类似于Python中的lambda函数，允许你执行部分LMQL代码。`lmql.F`只能有一个占位符变量，该变量将从lambda函数返回。
- en: We could specify both prompt and constraint for the function. The constraint
    will be equivalent to the `where` clause in the LMQL query.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以为函数指定提示和约束。约束将等同于LMQL查询中的`where`子句。
- en: Since we haven’t specified any model, the OpenAI `text-davinci` will be used.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们没有指定任何模型，将使用OpenAI的`text-davinci`。
- en: '[PRE6]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: If you’re using Jupyter Notebooks, you might encounter some problems since Notebooks
    environments are asynchronous. You could enable nested event loops in your notebook
    to avoid such issues.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你使用的是Jupyter Notebook，你可能会遇到一些问题，因为Notebook环境是异步的。你可以在笔记本中启用嵌套事件循环以避免这些问题。
- en: '[PRE7]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: The second approach allows you to define more complex queries. You can use `lmql.run`
    to execute an LMQL query without creating a function. Let’s make our query a bit
    more complicated and use the answer from the model in the following question.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 第二种方法允许你定义更复杂的查询。你可以使用`lmql.run`来执行LMQL查询，而无需创建函数。让我们使查询更复杂，并在接下来的问题中使用模型的回答。
- en: In this case, we’ve defined constraints in the `where` clause of the query string
    itself.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们在查询字符串的`where`子句中定义了约束。
- en: '[PRE8]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Also, I’ve used `run_sync` instead of `run` to get a result synchronously.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我使用了`run_sync`而不是`run`来同步获取结果。
- en: 'As a result, we got an `LMQLResult` object with a set of fields:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 结果，我们得到了一个包含一组字段的`LMQLResult`对象：
- en: '`prompt` — include the whole prompt with the parameters and the model’s answers.
    We could see that the model answer was used for the second question.'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`prompt` — 包括带有参数和模型答案的整个提示。我们可以看到模型答案被用于第二个问题。'
- en: '`variables` — dictionary with all the variables we defined: `ANSWER` and `CAPITAL`
    .'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`variables` — 包含我们定义的所有变量的字典：`ANSWER`和`CAPITAL`。'
- en: '`distribution_variable` and `distribution_values` are `None` since we haven’t
    used this functionality.'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`distribution_variable`和`distribution_values`都是`None`，因为我们还没有使用这个功能。'
- en: '![](../Images/071065b1358f774d779849cc09cbc095.png)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/071065b1358f774d779849cc09cbc095.png)'
- en: Image by author
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片
- en: The third way to use Python API is the `[@lmql](http://twitter.com/lmql).query`
    decorator, which allows you to define a Python function that will be handy to
    use in the future. It’s more convenient if you plan to call this prompt several
    times.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Python API的第三种方法是`[@lmql](http://twitter.com/lmql).query`装饰器，它允许您定义一个Python函数，以便将来使用非常方便。如果您计划多次调用此提示，则更加方便。
- en: We could create a function for our previous query and get only the final answer
    instead of returning the whole `LMQLResult` object.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以为我们之前的查询创建一个函数，并且只获取最终的答案，而不是返回整个`LMQLResult`对象。
- en: '[PRE9]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Also, you could use LMQL in combination with LangChain:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以将LMQL与LangChain结合使用：
- en: LMQL queries are Prompt Templates on steroids and could be part of LangChain
    chains.
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LMQL查询是增强版的提示模板，可以成为LangChain链的一部分。
- en: You could leverage LangChain components from LMQL (for example, retrieval).
    You can find examples in [the documentation](https://lmql.ai/docs/lib/integrations/langchain.html).
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您可以利用LMQL中的LangChain组件（例如检索）。您可以在[文档](https://lmql.ai/docs/lib/integrations/langchain.html)中找到示例。
- en: Now, we know all the basics of LMQL syntax, and we are ready to move on to our
    task — to define sentiment for customer comments.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们知道了LMQL语法的所有基础知识，准备好开始我们的任务——为客户评论定义情感。
- en: Sentiment Analysis
  id: totrans-110
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 情感分析
- en: To see how LMQL is performing, we will use labelled Yelp reviews from the [UCI
    Machine Learning Repository](https://archive.ics.uci.edu/dataset/331/sentiment+labelled+sentences)
    and try to predict sentiment. All reviews in the dataset are positive or negative,
    but we will keep neutral as one of the possible options for classification.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 为了查看LMQL的性能，我们将使用来自[UCI机器学习库](https://archive.ics.uci.edu/dataset/331/sentiment+labelled+sentences)的带标签的Yelp评论，并尝试预测情感。数据集中的所有评论都是积极的或消极的，但我们将保留中性作为分类的一种可能选项。
- en: For this task, let’s use local models — `Zephyr` and `Llama-2`. To use them
    in LMQL, we need to specify the model and tokeniser when we are calling LMQL.
    For Llama-family models, we can use the default tokeniser.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个任务，让我们使用本地模型——`Zephyr`和`Llama-2`。在调用LMQL时，我们需要指定模型和标记器。对于Llama系列模型，我们可以使用默认的标记器。
- en: First attempts
  id: totrans-113
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第一次尝试
- en: Let’s pick one customer review `The food was very good.` and try to define its
    sentiment. We will use `lmql.run` for debugging since it’s convenient for such
    ad-hoc calls.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们选择一个客户评论`食物非常好。`并尝试定义其情感。由于对于这种临时调用来说，`lmql.run`非常方便，我们将使用它进行调试。
- en: I’ve started with a very naive approach.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 我一开始采用了非常天真的方法。
- en: '[PRE10]The food was very good.[PRE11]'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '[PRE10]食物非常好。[PRE11]'
- en: If your local model works exceptionally slowly, check whether your computer
    uses swap memory. Restart could be an excellent option to solve it.
  id: totrans-117
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 如果您的本地模型运行异常缓慢，请检查您的计算机是否使用了交换内存。重新启动可能是解决此问题的一个很好的选项。
- en: The code looks absolutely straightforward. Surprisingly, however, it doesn’t
    work and returns the following error.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 代码看起来非常简单。然而令人惊讶的是，它却不能正常工作，并返回以下错误。
- en: '[PRE12]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: From the message, we can guess that the output doesn’t fit the context size.
    Our prompt is about 20 tokens. So, it’s a bit weird that we’ve hit the threshold
    on the context size. Let’s try to constrain the number of tokens for `SENTIMENT`
    and see the output.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 从消息中，我们可以猜测输出与上下文大小不符。我们的提示大约是20个标记，所以击中上下文大小阈值有些奇怪。让我们尝试限制`SENTIMENT`的标记数量，看看输出。
- en: '[PRE13]The food was very good.[PRE14]The service was terrible.[PRE15]The hotel
    was amazing, the staff were friendly and the location was perfect.[PRE16]The product
    was a complete disappointment.[PRE17]The flight was delayed for 3 hours, the food
    was cold and the entertainment system didn''t work.[PRE18]The restaurant was packed,
    but the waiter was efficient and the food was delicious.[PRE19]'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '[PRE13]食物非常好。[PRE14]服务很糟糕。[PRE15]酒店很棒，员工友好，位置完美。[PRE16]产品令人完全失望。[PRE17]航班延误3小时，食物冷，娱乐系统不工作。[PRE18]餐厅座无虚席，但服务员效率高，食物美味。[PRE19]'
- en: Now, we could see the root cause of the problem — the model was stuck in a cycle,
    repeating the question variations and answers again and again. I haven’t seen
    such issues with OpenAI models (suppose they might control it), but they are pretty
    standard to open-source local models. We could use the `STOPS_AT` constraint to
    stop generation if we see `Q:` or a new line in the model response to avoid such
    cycles.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以看到问题的根源——模型陷入了一个循环，不断重复问题的变体和答案。我没有见过 OpenAI 模型出现这种问题（假设他们可能会控制它），但这种问题在开源本地模型中相当常见。我们可以使用
    `STOPS_AT` 约束来停止生成，如果在模型响应中看到 `Q:` 或新行，以避免这种循环。
- en: '[PRE20]The food was very good.[PRE21]'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '[PRE20]食物非常好。[PRE21]'
- en: 'Excellent, we’ve solved the issue and got the result. But since we will do
    classification, we would like the model to return one of the three outputs (class
    labels): `negative`, `neutral` or `positive`. We could add such a filter to the
    LMQL query to constrain the output.'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 很好，我们已经解决了问题并得到了结果。但由于我们将进行分类，我们希望模型返回三种输出之一（类别标签）：`negative`、`neutral` 或 `positive`。我们可以在
    LMQL 查询中添加这样的过滤器来限制输出。
- en: '[PRE22]The food was very good.[PRE23]'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '[PRE22]食物非常好。[PRE23]'
- en: We don’t need filters with stopping criteria since we are already limiting output
    to just three possible options, and LMQL doesn’t look at any other possibilities.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们已经将输出限制为仅三种可能的选项，并且 LMQL 不会考虑其他可能性，因此我们不需要带有停止标准的过滤器。
- en: Let’s try to use the chain of thoughts reasoning approach. Giving the model
    some time to think usually improves the results. Using LMQL syntax, we could quickly
    implement this approach.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试使用连锁思维推理方法。给模型一些思考时间通常会提高结果。使用 LMQL 语法，我们可以快速实现这种方法。
- en: '[PRE24]The food was very good.[PRE25]'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '[PRE24]食物非常好。[PRE25]'
- en: The output from the Zephyr model is pretty decent.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: Zephyr 模型的输出相当不错。
- en: '![](../Images/cebf97ad58f3d16d2b466e85c7e8171d.png)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/cebf97ad58f3d16d2b466e85c7e8171d.png)'
- en: Image by author
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：作者
- en: We can try the same prompt with Llama 2.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以尝试使用 Llama 2 进行相同的提示测试。
- en: '[PRE26]The food was very good.[PRE27]'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '[PRE26]食物非常好。[PRE27]'
- en: The reasoning doesn’t make much sense. We’ve already seen on the Leaderboard
    that the Zephyr model is much better than Llama-2–7b.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 推理没有多大意义。我们已经在排行榜上看到 Zephyr 模型比 Llama-2–7b 好得多。
- en: '![](../Images/6bc197c1f46bfd75046211535bc79a28.png)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6bc197c1f46bfd75046211535bc79a28.png)'
- en: Image by author
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：作者
- en: In classical Machine Learning, we usually get not only class labels but also
    their probability. We could get the same data using `distribution` in LMQL. We
    just need to specify the variable and possible values — `distribution SENTIMENT
    in [‘positive’, ‘negative’, ‘neutral’]`.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 在经典的机器学习中，我们通常不仅获得类别标签，还会获得它们的概率。我们可以使用 LMQL 中的 `distribution` 来获得相同的数据。我们只需要指定变量和可能的值——`distribution
    SENTIMENT in [‘positive’, ‘negative’, ‘neutral’]`。
- en: '[PRE28]The food was very good.[PRE29]'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '[PRE28]食物非常好。[PRE29]'
- en: Now, we got probabilities in the output, and we could see that the model is
    quite confident in the positive sentiment.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们得到了输出中的概率，我们可以看到模型对积极情绪的信心相当高。
- en: Probabilities could be helpful in practice if you want to use only decisions
    when the model is confident.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你只想在模型自信时使用决策，概率可能在实践中很有帮助。
- en: '![](../Images/7b8932e91d9ac23440ea5bd4066ac404.png)'
  id: totrans-141
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7b8932e91d9ac23440ea5bd4066ac404.png)'
- en: Image by author
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：作者
- en: Now, let’s create a function to use our sentiment analysis for various inputs.
    It would be interesting to compare results with and without distribution, so we
    need two functions.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们创建一个函数，用于各种输入的情感分析。比较有分布和没有分布的结果会很有趣，所以我们需要两个函数。
- en: '[PRE30]{review}[PRE31]{review}[PRE32]'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '[PRE30]{review}[PRE31]{review}[PRE32]'
- en: Then, we could use this function for the new review.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以使用这个函数进行新的评估。
- en: '[PRE33]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: The model decided that it was neutral.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 模型认为这是中性的。
- en: '![](../Images/60dead54440461f0e49fc8a134f63882.png)'
  id: totrans-148
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/60dead54440461f0e49fc8a134f63882.png)'
- en: Image by author
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：作者
- en: There’s a rationale behind this conclusion, but I would say this review is negative.
    Let’s see whether we could use other decoders and get better results.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 这个结论背后有一定的理由，但我认为这条评论是负面的。让我们看看是否可以使用其他解码器来获得更好的结果。
- en: 'By default, the `argmax` decoder is used. It’s the most straightforward approach:
    at each step, the model selects the token with the highest probability. We could
    try to play with other options.'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，使用的是 `argmax` 解码器。这是最直接的方法：在每一步，模型选择概率最高的标记。我们可以尝试使用其他选项。
- en: Let’s try to use [the beam search](https://en.wikipedia.org/wiki/Beam_search)
    approach with `n = 3` and a pretty high `tempreture = 0.8`. As a result, we would
    get three sequences sorted by likelihood, so we could just get the first one (with
    the highest likelihood).
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试使用 [beam search](https://en.wikipedia.org/wiki/Beam_search) 方法，`n = 3`
    和相当高的 `tempreture = 0.8`。结果，我们将得到三个按可能性排序的序列，因此我们可以选择第一个（可能性最高的）。
- en: '[PRE34]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: Now, the model was able to spot the negative sentiment in this review.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，模型能够识别到这条评论中的负面情感。
- en: '![](../Images/bab33e0feb447f962c2c333260c8b447.png)'
  id: totrans-155
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bab33e0feb447f962c2c333260c8b447.png)'
- en: Image by author
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供
- en: 'It’s worth saying that there’s a cost for beam search decoding. Since we are
    working on three sequences (beams), getting an LLM result takes 3 times more time
    on average: 39.55 secs vs 13.15 secs.'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 值得一提的是，beam search 解码是有成本的。由于我们处理的是三个序列（束），因此获取 LLM 结果的时间平均要多花 3 倍：39.55 秒对比
    13.15 秒。
- en: Now, we have our functions and can test them with our real data.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们有了函数，并可以用真实数据进行测试。
- en: Results on real-life data
  id: totrans-159
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 真实数据的结果
- en: 'I’ve run all the functions on a 10% sample of the 1K dataset of Yelp reviews
    with different parameters:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 我在 Yelp 评论的 1K 数据集的 10% 样本上运行了所有函数，并使用了不同的参数：
- en: '**models**: Llama 2 or Zephyr,'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型**：Llama 2 或 Zephyr，'
- en: '**approach**: using distribution or just constrained prompt,'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**方法**：使用分布或仅使用约束提示，'
- en: '**decoders**: argmax or beam search.'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**解码器**：argmax 或 beam search。'
- en: First, let’s compare accuracy — share of reviews with correct sentiment. We
    can see that Zephyr performs much better than the Llama 2 model. Also, for some
    reason, we get significantly poorer quality with distributions.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们比较准确性——即评论中正确情感的比例。我们可以看到，Zephyr 的表现远好于 Llama 2 模型。另外，由于某些原因，我们在分布上的质量显著下降。
- en: '![](../Images/20556862454d7b6648fa4bfcba8909ec.png)'
  id: totrans-165
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/20556862454d7b6648fa4bfcba8909ec.png)'
- en: Graph by author
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 作者绘图
- en: 'If we look a bit deeper, we could notice:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: '如果我们更深入地观察，会发现： '
- en: For positive reviews, accuracy is usually higher.
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于正面评论，准确性通常较高。
- en: The most common error is marking the review as neutral,
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最常见的错误是将评论标记为中性，
- en: For Llama 2 with prompt, we could see a high rate of critical issues (positive
    comments that were labelled as negatives).
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于带有提示的 Llama 2，我们可以看到有较高的关键问题率（将正面评论标记为负面）。
- en: In many cases, I suppose the model uses a similar rationale, scoring negative
    comments as neutral as we’ve seen earlier with the “dirty room” example. The model
    is unsure whether “dirty room” has a negative or neutral sentiment since we don’t
    know whether the customer expected a clean room.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多情况下，我认为模型使用类似的推理，将负面评论评分为中性，就像我们在“肮脏的房间”例子中看到的那样。模型不确定“肮脏的房间”是负面还是中性情感，因为我们不知道客户是否期望一个干净的房间。
- en: '![](../Images/61f60d4457987a2e4cc6f88f0d0bddbe.png)'
  id: totrans-172
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/61f60d4457987a2e4cc6f88f0d0bddbe.png)'
- en: Graph by author
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 作者绘图
- en: '![](../Images/236e21eb76dca04e4f8ff40a58ef5972.png)'
  id: totrans-174
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/236e21eb76dca04e4f8ff40a58ef5972.png)'
- en: Graph by author
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 作者绘图
- en: 'It’s also interesting to look at actual probabilities:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 观察实际概率也是很有趣的：
- en: 75% percentile of positive labels for positive comments is above 0.85 for the
    Zephyr model, while it is way lower for Llama 2.
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于正面评论，Zephyr 模型的正面标签的75%百分位数超过0.85，而 Llama 2 则远低于此值。
- en: All models show poor performance for negative comments, where the 75% percentile
    for negative labels for negative comments is way below even 0.5.
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所有模型在负面评论上的表现都很差，其中负面评论的负面标签的75%百分位数远低于0.5。
- en: '![](../Images/9fda378f7b359e789f197d5b33309843.png)'
  id: totrans-179
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9fda378f7b359e789f197d5b33309843.png)'
- en: Graph by author
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 作者绘图
- en: '![](../Images/ae745cfa031e4a2d4f0d4fafa41694f7.png)'
  id: totrans-181
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ae745cfa031e4a2d4f0d4fafa41694f7.png)'
- en: Graph by author
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 作者绘图
- en: Our quick research shows that a vanilla prompt with a Zephyr model and `argmax`
    decoder would be the best option for sentiment analysis. However, it’s worth checking
    different approaches for your use case. Also, you could often achieve better results
    by tweaking prompts.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的快速研究显示，使用 Zephyr 模型和 `argmax` 解码器的普通提示是情感分析的最佳选择。然而，值得根据你的用例检查不同的方法。此外，你通常可以通过调整提示来获得更好的结果。
- en: You can find the full code on [GitHub](https://github.com/miptgirl/miptgirl_medium/blob/main/lmql_intro/lmql_intro.ipynb).
  id: totrans-184
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 你可以在 [GitHub](https://github.com/miptgirl/miptgirl_medium/blob/main/lmql_intro/lmql_intro.ipynb)
    上找到完整代码。
- en: Summary
  id: totrans-185
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: Today, we’ve discussed a concept of LMP (Language Model Programming) that allows
    you to mix prompts in natural language and scripting instructions. We’ve tried
    using it for sentiment analysis tasks and got decent results using local open-source
    models.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 今天，我们讨论了LMP（语言模型编程）的概念，它允许你将自然语言和脚本指令混合使用。我们尝试将其用于情感分析任务，并在使用本地开源模型时获得了不错的结果。
- en: Even though LMQL is not widespread yet, this approach might be handy and gain
    popularity in the future since it combines natural and programming languages into
    a powerful tool for LMs.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管LMQL尚未普及，但这种方法可能会在未来变得非常有用，并获得广泛的关注，因为它将自然语言和编程语言结合成一个强大的工具用于语言模型。
- en: Thank you a lot for reading this article. I hope it was insightful to you. If
    you have any follow-up questions or comments, please leave them in the comments
    section.
  id: totrans-188
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 非常感谢你阅读这篇文章。希望它对你有所启发。如果你有任何后续问题或评论，请在评论区留言。
- en: Dataset
  id: totrans-189
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据集
- en: '*Kotzias,Dimitrios. (2015). Sentiment Labelled Sentences. UCI Machine Learning
    Repository (CC BY 4.0 license).* [*https://doi.org/10.24432/C57604*](https://doi.org/10.24432/C57604.)'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: '*Kotzias, Dimitrios. (2015). Sentiment Labelled Sentences. UCI Machine Learning
    Repository (CC BY 4.0 license).* [*https://doi.org/10.24432/C57604*](https://doi.org/10.24432/C57604.)'
