- en: 'RLHF: Reinforcement Learning from Human Feedback'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 'RLHF: 来自人类反馈的强化学习'
- en: 原文：[https://towardsdatascience.com/rlhf-reinforcement-learning-from-human-feedback-faa5ff4761d1](https://towardsdatascience.com/rlhf-reinforcement-learning-from-human-feedback-faa5ff4761d1)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/rlhf-reinforcement-learning-from-human-feedback-faa5ff4761d1](https://towardsdatascience.com/rlhf-reinforcement-learning-from-human-feedback-faa5ff4761d1)
- en: 'ChatGPT’s success ingredient: The Instruction Data.'
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ChatGPT成功的关键：指令数据。
- en: '[](https://automata88.medium.com/?source=post_page-----faa5ff4761d1--------------------------------)[![Ms
    Aerin](../Images/21335c7f04e64fa34585950f038f96d0.png)](https://automata88.medium.com/?source=post_page-----faa5ff4761d1--------------------------------)[](https://towardsdatascience.com/?source=post_page-----faa5ff4761d1--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----faa5ff4761d1--------------------------------)
    [Ms Aerin](https://automata88.medium.com/?source=post_page-----faa5ff4761d1--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://automata88.medium.com/?source=post_page-----faa5ff4761d1--------------------------------)[![Ms
    Aerin](../Images/21335c7f04e64fa34585950f038f96d0.png)](https://automata88.medium.com/?source=post_page-----faa5ff4761d1--------------------------------)[](https://towardsdatascience.com/?source=post_page-----faa5ff4761d1--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----faa5ff4761d1--------------------------------)
    [Ms Aerin](https://automata88.medium.com/?source=post_page-----faa5ff4761d1--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----faa5ff4761d1--------------------------------)
    ·24 min read·Oct 11, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page-----faa5ff4761d1--------------------------------)
    ·24分钟阅读·2023年10月11日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: ChatGPT has captivated the world with its impressive capabilities. But how did
    it get so smart?
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: ChatGPT凭借其令人印象深刻的能力吸引了全世界的关注。但它是如何变得如此聪明的呢？
- en: I recently spoke to one of my former coworkers, a software engineer I respect
    a lot, and I noticed that he believes ChatGPT is a manifestation of AGI, pointing
    to its ability to simplify complex topics to a six-year-old’s level of understanding
    as evidence. While I don’t entirely disagree with him on its unreasonable intelligence,
    I felt compelled to put down my thoughts. In this article, I’d like to emphasize
    that the magic of ChatGPT is heavily reliant on its training data.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 我最近和一位我非常尊敬的前同事——一位软件工程师进行了交谈，我注意到他认为ChatGPT是AGI的体现，并将其将复杂主题简化到六岁孩子理解水平的能力作为证据。虽然我对它的不合理智能并不完全不同意，但我觉得有必要表达一下我的想法。在这篇文章中，我想强调ChatGPT的魔力在于其训练数据。
- en: Carefully curated instruction data is the key to ChatGPT’s human-like abilities.
    Things like explaining concepts to a 6-year-old, turning a resume into a LinkedIn
    profile, brainstorming ideas with you, etc., didn’t just emerge—they were deliberately
    encoded into the model in the form of training data.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 精心策划的指令数据是ChatGPT类人能力的关键。诸如向6岁孩子解释概念、将简历转化为LinkedIn资料、与您头脑风暴等功能并不是偶然出现的——它们是被刻意编码到模型中的训练数据。
- en: '[After tweeting like this a few times, maybe it’s time time for a long-form...](https://x.com/aerinykim/status/1705640689139396775)'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '[发过几次这样的推文后，也许是时候写一篇长文了...](https://x.com/aerinykim/status/1705640689139396775)'
- en: Like everyone else, this is the first time I am experiencing closed research.
    Since I was in college, all frontier research has been open and peer-reviewed,
    until recently. And I believe openness ultimately advances science more than closedness.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 和其他人一样，这是我第一次接触封闭研究。自大学以来，所有前沿研究都是开放和同行评审的，直到最近。我相信开放性最终比封闭性更能推动科学进步。
- en: If we aim to match the performance of ChatGPT through open source, I believe
    we need to start taking training data more seriously. A substantial part of ChatGPT’s
    effectiveness might not come from, say, specific ML architecture, fine-tuning
    techniques, or frameworks. But more likely, it’s from the breadth, scale and quality
    of the instruction data.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们旨在通过开源来匹配ChatGPT的表现，我相信我们需要更加认真对待训练数据。ChatGPT的有效性很大程度上可能并不是来自于特定的ML架构、微调技术或框架。而更可能的是来自于指令数据的广度、规模和质量。
- en: To put it bluntly, fine-tuning large language models on mediocre instruction
    data is a waste of compute. Let’s take a look at what has changed in the training
    data and learning paradigm—how we are now formatting the training data differently
    and therefore learning differently than in past large-scale pre-training.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 直截了当地说，在平庸的指令数据上微调大型语言模型是一种浪费计算资源。让我们看看训练数据和学习范式中发生了什么变化——我们现在如何以不同的方式格式化训练数据，因此与过去的大规模预训练相比，学习也发生了不同的变化。
- en: What is RLHF?
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是RLHF？
- en: 'RLHF stands for Reinforcement Learning from Human Feedback. It has two main
    components:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: RLHF代表来自人类反馈的强化学习。它有两个主要组成部分：
- en: Reinforcement Learning (RL)
  id: totrans-15
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 强化学习（RL）
- en: Human Feedback (HF)
  id: totrans-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 人类反馈（HF）
- en: What exactly is being trained?
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 到底训练的是什么？
- en: Historically, when we talk about LLM training, we just mean updating only the
    parameters of the language model. However, **when we use RLHF, we train the parameters
    of three separate models.** This way, it provides far more freedom because it’s
    not confined by the maximum likelihood framework (see [**[WHY did we try RL in
    LLM?]**](#158d) section for details), and we learn an objective function directly
    from the data itself.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 历史上，当我们谈论LLM训练时，我们只意味着更新语言模型的参数。然而，**当我们使用RLHF时，我们训练三个独立模型的参数。** 这种方式提供了更多的自由，因为它不受限于最大似然框架（详细信息见[**[我们为何在LLM中尝试RL？]**](#158d)部分），并且我们直接从数据本身学习目标函数。
- en: 'Here are three models that are being trained:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有三个正在训练的模型：
- en: '**Language Model (SFT model)**'
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**语言模型（SFT模型）**'
- en: is a large pre-trained language model like GPT-3\. The model has already been
    trained, and it will be fine-tuned based on **instruction data** later.
  id: totrans-21
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 是一个像GPT-3这样的预训练的大型语言模型。该模型已经经过训练，稍后将基于**指令数据**进行微调。
- en: '**Reward Model**'
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**奖励模型**'
- en: is trained to predict human preferences and provide reward signals to reinforce
    the agent. It is trained on **human feedback** data.
  id: totrans-23
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 训练以预测人类偏好并提供奖励信号以强化代理。它是基于**人类反馈**数据进行训练的。
- en: '**Policy Model** **(Agent)**'
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**策略模型** **（代理）**'
- en: is trained to generate tokens by maximizing the predicted reward. To do so,
    it uses reinforcement learning with the reward model as its source of feedback.
    The policy model is initialized from the SFT model.
  id: totrans-25
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 通过最大化预测奖励来训练生成令牌。为此，它使用了以奖励模型作为反馈来源的强化学习。策略模型是从SFT模型初始化的。
- en: The LLM’s pre-existing weights are adjusted and fine-tuned in RL phase, where
    the model optimizes its actions (generating tokens) to maximize the rewards (good
    human feedback).
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: LLM的预先存在的权重在RL阶段进行调整和微调，在这个阶段，模型优化其行为（生成令牌）以最大化奖励（良好的人类反馈）。
- en: The seminal paper on RLHF is [**InstructGPT**](https://arxiv.org/abs/2203.02155),
    which was published last year by OpenAI. Recognizing just how powerful the InstructGPT
    model is, OpenAI switched all of their public APIs from using vanilla models to
    instruct models. Subsequently, they reduced academic publications detailing further
    progress, shifting research in-house. I will mainly use examples and methods from
    InstructGPT in this blog.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 关于RLHF的开创性论文是[**InstructGPT**](https://arxiv.org/abs/2203.02155)，它是去年由OpenAI发布的。认识到InstructGPT模型的强大，OpenAI将所有公共API从使用原始模型切换到使用指令模型。随后，他们减少了详细描述进一步进展的学术出版物，将研究转移到内部。我将在这个博客中主要使用InstructGPT的例子和方法。
- en: 'The Key Innovation in RLHF: Changing the training data format'
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: RLHF的关键创新：改变训练数据格式
- en: Before RLHF / ChatGPT / InstructGPT (I’ll use these three terms interchangeably),
    language models like GPT-3 were trained to predict the next word probabilistically
    using cross-entropy loss.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在RLHF / ChatGPT / InstructGPT之前（我将这三个术语互换使用），像GPT-3这样的语言模型是使用交叉熵损失来预测下一个词的概率。
- en: But is predicting the next token probabilistically our end goal?
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 但预测下一个令牌的概率性是否是我们的最终目标？
- en: Not at all! The most impressive aspect of ChatGPT is that it can do **so many
    different tasks** in natural language, such as paraphrasing, summarization, classification,
    and more. This broad-spectrum capability is what makes ChatGPT great and gives
    it a ‘wow’ factor when compared to more specialized, single-purpose ML models.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 绝对不是！ChatGPT 最令人印象深刻的方面是它能在自然语言中执行**许多不同的任务**，如释义、总结、分类等。这种广泛的能力使 ChatGPT 非常出色，并且与那些更专注于单一目的的机器学习模型相比，具有了‘惊叹’的因素。
- en: Then, **in order for a language model to do a variety of tasks instead of just
    predicting the next word, what do we need to do?**
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，**为了让语言模型执行各种任务而不仅仅是预测下一个词，我们需要做什么？**
- en: Generally speaking, if you want to change the model’s behavior, you need to
    change its training data, either its contents, format, or both. You can also change
    the loss function. ChatGPT changed all three aspects.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，如果你想改变模型的行为，你需要改变它的训练数据，无论是其内容、格式，还是两者都有。你也可以改变损失函数。ChatGPT 改变了这三个方面。
- en: Before we get into the specifics of RLHF, I’d like to show how the InstructGPT
    team went to great lengths to create extensive and exhaustive training data to
    make ChatGPT a reality.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入 RLHF 的细节之前，我想展示 InstructGPT 团队如何不遗余力地创建了大量详尽的训练数据，使 ChatGPT 成为现实。
- en: '**There are two types of human feedback used in RLHF.** One is the **instruction
    data**, and the other is **human preference data**.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '**RLHF 中使用了两种类型的人类反馈。** 一种是 **指令数据**，另一种是 **人类偏好数据**。'
- en: 1\. The Instruction Data (aka Demonstration Data)
  id: totrans-36
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 1\. 指令数据（即示范数据）
- en: Instruction data are pairs of inputs and outputs that demonstrate how the model
    should behave given an input.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 指令数据是输入和输出的配对，展示了给定输入时模型应该如何表现。
- en: If you want to train your very first InstructGPT model from scratch, you will
    have to write not only the answers but also the user inputs (use cases). Because
    until last year, GPT-3 API users rarely typed in daring prompts like explaining
    complex concepts to a 6-year-old, etc. Users never thought they could ask models
    such questions. And this is why instruction data is also called "demonstration"
    data. **We first have to demonstrate the use cases to the language model.**
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想从头开始训练你的第一个 InstructGPT 模型，你不仅需要编写答案，还需要编写用户输入（用例）。因为直到去年，GPT-3 API 用户很少输入像向6岁孩子解释复杂概念这样的大胆提示。用户从未想过可以向模型提出这样的问题。这也是为什么指令数据也被称为“示范”数据。**我们首先必须向语言模型展示用例。**
- en: Let’s look at the variety of use cases (prompts) curated by the InstructGPT
    team.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看 InstructGPT 团队策划的各种用例（提示）。
- en: '![](../Images/863d8e80f14f7ba8e82cbb1906efccd2.png)![](../Images/ed0e70635109e3b78429db899e3ceb97.png)![](../Images/220b6b3586ab6111712e0c8b5a31d22f.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/863d8e80f14f7ba8e82cbb1906efccd2.png)![](../Images/ed0e70635109e3b78429db899e3ceb97.png)![](../Images/220b6b3586ab6111712e0c8b5a31d22f.png)'
- en: Use Case Examples provided by InstructGPT
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: InstructGPT 提供的用例示例
- en: 'Here are some interesting use cases to highlight:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一些有趣的用例来强调：
- en: 'Closed Q&A use case has clear right and wrong answers like:'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 封闭式问答用例有明确的正确和错误答案，如：
- en: '[PRE0]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Open Q&A use case will have subjective responses:'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 开放式问答用例会有主观性的回答：
- en: '[PRE1]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Rewriting use case will require the labeler’s creativity.
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 重写用例将需要标注者的创造力。
- en: '[PRE2]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The diligence involved in creating Instruction Data
  id: totrans-49
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建指令数据时的勤奋
- en: Let’s look at what it takes to generate high-quality instruction data. Here’s
    an excerpt from the labeling instructions for the API prompt distribution from
    InstructGPT.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看生成高质量指令数据需要什么。这是来自 InstructGPT API 提示分发的标注说明摘录。
- en: '![](../Images/7dc093498a6c14a7fd18429ad8f630bd.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7dc093498a6c14a7fd18429ad8f630bd.png)'
- en: Excerpt of labeling instructions on the API prompt distribution from InstructGPT
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: InstructGPT 的 API 提示分发标注说明摘录
- en: This lengthy section is intended for the “labelers”. It is a long document that
    seems to have a lot of meanings to figure out. We needed this long set of instructions
    because for the labelers to create the instruction data that we want, they must
    first understand what we want them to do and **stick to those rules.**
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 这一长段文字是为“标注者”准备的。这是一份长文件，似乎有很多含义需要弄清楚。我们需要这套长说明，因为为了让标注者创建我们想要的指令数据，他们必须首先理解我们希望他们做什么，并且
    **遵循这些规则。**
- en: 'It seems there are three rules you should follow when writing:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 似乎有三条规则你应该遵循：
- en: '**Being helpful, truthful, and harmless.**'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '**有帮助、真实且无害。**'
- en: Let's take a look at the criteria for being helpful.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看看成为有帮助的标准。
- en: '**“**Answering the question they meant to ask, **even if they mis-asked it”.**'
  id: totrans-57
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**“**回答他们本来想问的问题，即使他们问得不准确。”**'
- en: This is a huge ask. It demands labelers try to truly help users rather than
    dismiss their mis-asked questions with responses like “I don’t understand you.”
    This is similar to how a mother will try to understand what her baby wants, even
    if the baby doesn’t say it exactly right.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个巨大的要求。它要求标注者真正尝试帮助用户，而不是用“我不理解你”这样的回答来回避用户提出的错误问题。这类似于母亲尝试理解她的宝宝想要什么，即使宝宝没有准确地说出来。
- en: '**“Being sensitive to internationality (e.g. “football” shouldn’t mean American
    football, and “the president” doesn’t necessarily mean the US president)”**'
  id: totrans-59
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**“对国际性敏感（例如，“football”不应指美式足球，而“the president”不一定指美国总统）”**'
- en: Labelers should have a strong grasp of language and a good understanding of
    how different cultures work.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 标注员应具备扎实的语言能力和对不同文化运作方式的良好理解。
- en: So, **who are these labelers capable of diligently following these sophisticated
    guidelines?** They are certainly not part-time workers on a crowdsourcing platform
    who can commit only 1–2 hours a day. Based on my experience creating large-scale
    training data, casual crowd workers can’t sufficiently produce the natural, nuanced
    conversations that enable ChatGPT’s impressive performance.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，**谁是这些能够认真遵循这些复杂指南的标注员呢？** 他们肯定不是那些只能每天投入1-2小时的众包平台的兼职工人。根据我创建大规模训练数据的经验，随意的众包工人无法充分提供自然、细腻的对话，进而促使ChatGPT的卓越表现。
- en: I prefer the term **“data writers”** over “labelers,” as it better captures
    the creativity and care involved. To make sure these data writers give you the
    quality work you need, you need to train them, over-communicate with them, keep
    them on the same page, review their submissions, give them feedback, and keep
    only the best writers and let the rest go. You will need to be able to trust your
    writers because the performance of your LLMs (the “wow” factor, the quality of
    ChatGPT’s responses to your questions, etc.) will be based on their work. While
    you are their boss, you are also heavily reliant on them. It’s a fascinating symbiotic
    relationship and an art in itself.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 我更倾向于使用**“数据编写者”**这个术语，而不是“标注员”，因为它更能体现其中的创造力和细致入微。为了确保这些数据编写者提供你所需的高质量工作，你需要培训他们，与他们过度沟通，保持一致，审查他们的提交，给予反馈，并保留最优秀的编写者，让其余的离开。你需要能够信任你的编写者，因为你的LLMs的表现（“wow”因素、ChatGPT对你问题的回答质量等）将基于他们的工作。虽然你是他们的老板，但你也严重依赖他们。这是一种迷人的共生关系，本身就是一种艺术。
- en: The InstructGPT team deserves a lot of credit for taking this art to the next
    level. Their work tells us that if we desire open-source LLMs on par with ChatGPT’s
    performance, the data aspect needs to be executed impeccably.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: InstructGPT团队值得大力称赞，他们将这门艺术提升到了一个新的水平。他们的工作告诉我们，如果我们希望开源的LLMs能达到ChatGPT的表现，数据方面需要无懈可击。
- en: 2\. Preference Data
  id: totrans-64
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 2\. 偏好数据
- en: Instruction data is used for [the Supervised Fine-Tuning (SFT) phase (details
    in the upcoming section)](#d6fd). The other critical half of the training data
    is the “preference data”. Preference data is used to train the reward model during
    the RL phase. It involves humans ranking different LLM-generated outputs based
    on their preferences. Preference provides training signals on right vs. wrong
    behavior.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 指令数据用于[监督性微调（SFT）阶段（详细信息见下一部分)](#d6fd)。另一半关键的训练数据是“偏好数据”。偏好数据用于在RL阶段训练奖励模型。这涉及到人类根据他们的偏好对不同的LLM生成的输出进行排名。偏好数据为正确与错误的行为提供训练信号。
- en: When I read the labeling guidelines, criteria like “helpful” or “truthful” seemed
    a bit unclear to me. Also, if I were the labeler, I would be less likely to read
    these directions carefully because they are so long. To address this, the InstructGPT
    team went to great lengths to train the labelers by giving them clear examples.
    **This was a crucial step to influence the desired model behavior.**
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 当我阅读标注指南时，像“有帮助的”或“真实的”这样的标准对我来说有点不清楚。此外，如果我是一名标注员，我可能不太会仔细阅读这些指南，因为它们太长了。为了应对这一点，InstructGPT团队付出了巨大努力，通过提供清晰的示例来培训标注员。**这是影响期望模型行为的关键步骤。**
- en: Here are the examples presented to labelers to help them understand what "helpful,”
    "truthful," and “harmless” mean.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是提供给标注员的示例，帮助他们理解“有帮助的”、“真实的”和“无害的”是什么意思。
- en: '![](../Images/c12f7ea615392e710b93f81f20442e8c.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c12f7ea615392e710b93f81f20442e8c.png)'
- en: Example of prioritizing harmlessness first. Okay, so safety first.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 以无害性优先为例。好的，安全第一。
- en: '![](../Images/2b7fb645756494f906534aa595d48e5f.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2b7fb645756494f906534aa595d48e5f.png)'
- en: Read the “Reasoning” section above. **The emphasis on the “helpful” aspect of
    training data, in my opinion, was the most important change in ChatGPT. This novel
    way of annotating the data makes InstructGPT stand out from previous research.
    However, it’s also worth noting that the same “being helpful” factor could lead
    to** [**hallucinations (more on this later)**](#f2de)**.**
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 阅读上面的“推理”部分。**我认为对训练数据“有用”方面的重视，是ChatGPT中最重要的变化。这种注释数据的新方法使得InstructGPT与之前的研究区别开来。然而，也值得注意的是，同样的“有用”因素可能会导致**
    [**幻觉（稍后会详细讲解）**](#f2de)**。**
- en: '![](../Images/304c15822a41c5e1bc75fb544b0aebce.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/304c15822a41c5e1bc75fb544b0aebce.png)'
- en: The above three examples, drawn from the InstructGPT’s [public documentation](https://docs.google.com/document/d/1viWm6I2hBPFL2zqflj4s2it32FRbkETZpUS3CcVdFvo/edit?usp=sharing),
    illustrate the level of training required for the instruction data writers and
    their significant influence on model’s behavior.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 上面三个示例来自InstructGPT的[公开文档](https://docs.google.com/document/d/1viWm6I2hBPFL2zqflj4s2it32FRbkETZpUS3CcVdFvo/edit?usp=sharing)，展示了指令数据编写者所需的训练水平及其对模型行为的重大影响。
- en: Unreasonable Effectiveness of the Instruction Data
  id: totrans-74
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 指令数据的非凡有效性
- en: Let’s compare the outputs of two models — one that was trained with instruction
    data and one that wasn’t.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们比较两个模型的输出——一个是用指令数据训练的，另一个则没有。
- en: '![](../Images/f52cd690b29ff27126e989cd305e38f7.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f52cd690b29ff27126e989cd305e38f7.png)'
- en: 'Model comparison: no instruction training vs. instruction training from [https://openai.com/research/instruction-following](https://openai.com/research/instruction-following)'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 模型比较：无指令训练 vs. 从[https://openai.com/research/instruction-following](https://openai.com/research/instruction-following)进行的指令训练
- en: On the left side, the vanilla version of DaVinci (a model not trained with instruction
    data) fails to comprehend the prompt “Explain the moon landing to a 6-year-old
    in a few sentences”. It doesn’t seem to understand the user’s ask and instead
    offers multiple irrelevant responses such as explaining evolution.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在左侧，未经过指令数据训练的DaVinci原版（一个未经过指令数据训练的模型）未能理解“用几句话向6岁孩子解释登月”这个提示。它似乎无法理解用户的要求，而是提供了多个无关的回答，如解释进化。
- en: On the other hand, the instruct-DaVinci model on the right, answers the user’s
    prompt right, although it is less elaborate than gpt4\. :)
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，右侧的instruct-DaVinci模型能够正确回答用户的提示，虽然它比gpt4的回答简洁。 :)
- en: Why should I care about instruction data?
  id: totrans-80
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 我为什么要关心指令数据？
- en: 1\. Understanding the format of instruction data helps you write better prompts.
  id: totrans-81
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1\. 理解指令数据的格式可以帮助你编写更好的提示。
- en: The closer your prompts align with the proprietary model’s instruction data,
    the more effective the output will be. Designing prompts that are similar to the
    model’s training data can save you time by reducing the amount of trial-and-error
    needed.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 你输入的提示与专有模型的指令数据越接近，输出效果就会越好。设计与模型训练数据相似的提示可以通过减少试错的时间来节省你的时间。
- en: 2\. It partly explains hallucination tendencies.
  id: totrans-83
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2\. 它在一定程度上解释了幻觉倾向。
- en: 'Various reasons have been proposed to explain hallucinations in models ([On
    the Origin of Hallucinations in Conversational Models: Is it the Datasets or the
    Models?](https://arxiv.org/abs/2204.07931), [Enabling Large Language Models to
    Generate Text with Citations](https://arxiv.org/abs/2305.14627), [Evaluating the
    Factual Consistency of Large Language Models Through Summarization](https://arxiv.org/abs/2211.08412),
    etc.) Some suggest that language models show pattern completion behavior because
    they’re trained to maximize the likelihood of adjacent text. But is this the only
    reason for hallucinations in RLHF?'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 已经提出了各种原因来解释模型中的幻觉现象（[对话模型幻觉的起源：是数据集还是模型？](https://arxiv.org/abs/2204.07931)，[使大型语言模型生成带引用的文本](https://arxiv.org/abs/2305.14627)，[通过总结评估大型语言模型的事实一致性](https://arxiv.org/abs/2211.08412)等）。一些人认为，语言模型显示模式完成行为是因为它们被训练来最大化相邻文本的可能性。但这是否是RLHF中幻觉的唯一原因？
- en: '**I don’t think we can overlook the fact that during the preference data labeling,
    labelers were instructed to prioritize helpfulness to users over truthfulness.
    But when we do our final evaluations, we have labelers put the truth first.**'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '**我认为我们不能忽视这样一个事实，即在偏好数据标注过程中，标注人员被指示优先考虑对用户的有用性而非真实性。但当我们进行最终评估时，我们会让标注人员把真实性放在首位。**'
- en: Refer again to [Example 2, “Prioritizing helpfulness over truthfulness”](#7e80).
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 再次参考[示例2，“优先考虑有用性而非真实性”](#7e80)。
- en: This example shows how putting more weight on “helpful” answers in human preference
    data can lead to hallucinations. To mitigate this, we could generate more training
    data that prioritize truthfulness and harmlessness over being helpful in certain
    scenarios, such as high-stake domains like medicine. Balancing the different priorities
    in different situations can help reduce hallucinations.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 这个例子展示了在人工偏好数据中对“有帮助”答案加权过重如何导致幻觉。为了减轻这种情况，我们可以生成更多优先考虑真实性和无害性的训练数据，而不是在某些情境下（如医学等高风险领域）只关注帮助性。平衡不同情况下的不同优先级可以帮助减少幻觉。
- en: Another thing that could cause hallucinations is that the model doesn’t know
    it is allowed to express uncertainty. An important step toward reducing hallucinations
    is to incentivize the model to express uncertainty in words. This has been a long-standing
    problem in NLP, as demonstrated by the specific attempt made by SQUAD (Stanford
    Question Answering Dataset) V2 to address it by not answering when uncertain.
    So while RLHF is a big step forward, some of NLP’s important problems, like how
    to handle uncertainty, are still not completely solved.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个可能导致幻觉的因素是模型不知道自己被允许表达不确定性。减少幻觉的一个重要步骤是激励模型用文字表达不确定性。这在NLP中一直是一个长期存在的问题，正如SQUAD（斯坦福问答数据集）V2通过在不确定时不回答的问题所体现的那样。因此，虽然RLHF是一个重要的进步，但一些NLP的重要问题，如如何处理不确定性，仍然没有完全解决。
- en: Ok, we’re done with the data part. Now let’s look at the RLHF methods.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，我们完成了数据部分。现在让我们看看RLHF的方法。
- en: Three steps of RLHF
  id: totrans-90
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: RLHF的三步骤
- en: OpenAI always shares this simplified diagram to explain how ChatGPT works. I
    hope now you can better appreciate the implications of the minor sub-step “A labeler
    demonstrated the desired output behavior” in Step 1.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI 总是分享这个简化的图示来解释 ChatGPT 是如何工作的。我希望现在你可以更好地理解在第1步中，次要子步骤“A标注员展示了期望的输出行为”的意义。
- en: '![](../Images/e4a42969c59a69e7ff32e3c0db565205.png)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e4a42969c59a69e7ff32e3c0db565205.png)'
- en: That digram
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 那个图示
- en: Step 1\. Supervised Fine-Tuning (SFT) Initialization
  id: totrans-94
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第一步\. 监督微调（SFT）初始化
- en: The first step in RLHF is supervised fine-tuning (SFT) to initialize the language
    model weights (the first column in the diagram). SFT trains the model on instruction
    data; cloning demonstrated conversational behavior. This step primes the model
    for subsequent reinforcement learning.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: RLHF的第一步是监督微调（SFT），以初始化语言模型权重（图示中的第一列）。SFT在指令数据上训练模型；克隆展示的对话行为。这一步为后续的强化学习做了准备。
- en: You can start SFT with a pretrained model like GPT-3, as OpenAI did for InstructGPT.
    Or you can train one from scratch and go from there. The SFT output provides the
    input for the next reinforcement learning phase.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以从预训练模型如GPT-3开始SFT，就像OpenAI为InstructGPT做的那样。或者你也可以从头开始训练，然后继续前进。SFT的输出为下一个强化学习阶段提供输入。
- en: Properly initialized weights are crucial for strong downstream task performance,
    not just in RLHF but in general. So the SFT model isn’t selected at random. The
    best SFT model will be chosen based on reward model scores using a validation
    set.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 适当初始化的权重对于强大的下游任务表现至关重要，不仅仅在RLHF中如此，一般情况下也是如此。因此，SFT模型的选择不是随意的。最佳的SFT模型将根据使用验证集的奖励模型得分来选择。
- en: '[Some notable excerpts in InstructGPT]'
  id: totrans-98
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '[InstructGPT中的一些显著摘录]'
- en: ''
  id: totrans-99
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The final reward model was initialized from a 6B GPT-3 model that was fine-tuned
    on a variety of **public NLP datasets (ARC, BoolQ, CoQA, DROP, MultiNLI, OpenBookQA,
    QuAC, RACE, and Winogrande)**. This was mostly for historical reasons; we find
    similar results when initializing the RM from the GPT-3 or SFT models.
  id: totrans-100
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 最终的奖励模型是从一个6B GPT-3模型初始化的，该模型在各种**公共NLP数据集（ARC, BoolQ, CoQA, DROP, MultiNLI,
    OpenBookQA, QuAC, RACE, 和 Winogrande）**上进行了微调。这主要是出于历史原因；我们发现从GPT-3或SFT模型初始化RM时也会得到类似的结果。
- en: ''
  id: totrans-101
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: We find that our SFT models overfit on validation loss after 1 epoch; however,
    we find that training for more epochs helps both the RM score and human preference
    ratings, despite this overfitting.
  id: totrans-102
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 我们发现我们的SFT模型在1个周期后会在验证损失上过拟合；然而，我们发现训练更多周期对RM得分和人工偏好评级都有帮助，尽管存在过拟合。
- en: Getting good instruction data can be expensive, especially if you don’t have
    thousands of user-submitted seed prompts. What then can you do if you don’t have
    as much resource as commercial enterprises? One option is to use publicly available
    data. Academic datasets mentioned above, SQUAD V1, V2, StackOverflow, Quora, and
    others can all be helpful. You can transform those data to suit your training
    requirements.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 获取良好的指令数据可能很昂贵，特别是如果你没有成千上万的用户提交的种子提示。那么，如果你没有像商业企业那样的资源，你可以做什么呢？一个选择是使用公开的数据。上述提到的学术数据集、SQUAD
    V1、V2、StackOverflow、Quora 等都可能有帮助。你可以将这些数据转换以适应你的训练需求。
- en: Step 2\. Training Reward Model
  id: totrans-104
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第 2 步：训练奖励模型
- en: The reward model’s job is to return **a scalar that represents the human preference**
    when given a pair of (prompt, answer). A high score means preferred, and a low
    score means not preferred.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 奖励模型的工作是返回**一个表示人类偏好的标量**，当给定一对（提示，答案）时。高分意味着被偏好，低分意味着不被偏好。
- en: '![](../Images/c544545039168d09980be7bb4a8d91b4.png)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c544545039168d09980be7bb4a8d91b4.png)'
- en: The loss function of the reward model
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 奖励模型的损失函数
- en: When you see the equation, it might not look straightforward, but this is actually
    a simple formula. Let’s look at this with real data.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 当你看到方程时，它可能看起来不直接，但这实际上是一个简单的公式。让我们用真实的数据来看看。
- en: '![](../Images/e18078ff7edd8e4702a0a6a928562dda.png)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e18078ff7edd8e4702a0a6a928562dda.png)'
- en: '[WordMakeover.com](https://wordmakeover.com) for effective email writing'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '[WordMakeover.com](https://wordmakeover.com) 用于有效的电子邮件写作'
- en: '**x** = input, question, or prompt'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '**x** = 输入、问题或提示'
- en: '**y_w** = winning output'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '**y_w** = 赢的输出'
- en: '**y_l** = losing output'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '**y_l** = 输的输出'
- en: '**K** = number of outputs (7 here because there are 7 LLM results)'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '**K** = 输出数量（这里为 7，因为有 7 个 LLM 结果）'
- en: '**θ** = reward model parameters being trained'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '**θ** = 正在训练的奖励模型参数'
- en: '**r_θ** = reward score from model (scalar)'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '**r_θ** = 来自模型的奖励分数（标量）'
- en: Now that we know every variable in the equation, let's understand why this loss
    function looks the way it does. Suppose the rightmost term, the difference between
    **r_θ** (winning pair) and **r_θ** (losing pair), holds a certain value. The sigmoid
    will make that difference fall between 0 and 1.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们知道方程中的每个变量了，让我们理解为什么这个损失函数是这样的。假设最右侧的项，即 **r_θ**（赢的对比对）和 **r_θ**（输的对比对）之间的差值，持有一个特定值。sigmoid
    将使这个差值落在 0 和 1 之间。
- en: Visualize the log graph between 0 and 1 after the sigmoid. When the input comes
    closer to zero, it plummets to negative infinity, and when the input comes closer
    to one, it rises to zero. From this, you can see why the model will suffer a hefty
    penalty if it assigns a larger reward value to the losing pair compared to the
    winning pair.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 视觉化 sigmoid 函数后的对数图形在 0 和 1 之间。当输入接近零时，它骤降至负无穷，而当输入接近一时，它上升至零。从中可以看出，如果模型给输掉的对比对分配了比赢得的对比对更大的奖励值，那么模型将受到重大的惩罚。
- en: You do this for all 7C2 pairs and then take the average. And that is the loss
    you want to minimize.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 对所有 7C2 对进行这种操作，然后取平均值。这就是你想要最小化的损失。
- en: 'For those who love codes:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 对于那些喜欢代码的人：
- en: '[PRE3]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The reward model is initialized from the SFT model. Then we remove the final
    embedding layer and add a linear layer that gives a scalar.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 奖励模型从 SFT 模型初始化。然后我们移除最终的嵌入层，添加一个给出标量的线性层。
- en: Size-wise, the reward model is often smaller than the language model. For example,
    InstructGPT used a 175B-parameter language model but a 6B-parameter reward model.
    The team reports that 175B reward model training was unstable, making it less
    suitable for use as the value function during RL.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 从大小上看，奖励模型通常比语言模型小。例如，InstructGPT 使用了一个 175B 参数的语言模型，但使用了一个 6B 参数的奖励模型。团队报告说，175B
    奖励模型的训练不稳定，使其不太适合作为 RL 期间的价值函数。
- en: What’s the purpose of ranking?
  id: totrans-124
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 排名的目的是什么？
- en: Ranking makes it easy to compare two outputs. **With n outputs, ranking can
    easily generate nC2 pairs from just one-time labeling.**
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 排名使得比较两个输出变得简单。**有了 n 个输出，排名可以通过一次标注轻松生成 nC2 对。**
- en: One disadvantage of binary options is the lack of granularity. **They don’t
    capture how much better output A is over B.** And without quantifying that difference,
    **errors can’t be precisely penalized based on severity.** The alternative is
    to have a labeler give integers or floats, but that is very subjective and hard
    to calibrate across different labelers.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 二元选项的一个缺点是缺乏细粒度。**它们无法捕捉输出 A 相对于 B 的优越程度。** 而且没有量化这种差异，**错误无法根据严重程度精确地惩罚。**
    另一种选择是让标注员给出整数或浮点数，但这非常主观，并且很难在不同标注员之间进行校准。
- en: Can anyone think of a better way to frame the preference problem? :)
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 有人能想到更好的方式来表述偏好问题吗？:)
- en: Step 3\. Optimize a policy against the reward model using RL
  id: totrans-128
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 步骤3：使用RL优化针对奖励模型的策略
- en: 'This step is in one sentence: **the LLM parameters and policy are jointly optimized**
    to maximize expected rewards from the reward model.'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 这一步骤可以用一句话概括：**LLM参数和策略是联合优化**以最大化从奖励模型中获得的期望奖励。
- en: WHY did we try RL in LLM?
  id: totrans-130
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们为什么在LLM中尝试RL？
- en: In the past, language models rarely used RL for optimization. Instead, they
    relied on information theory loss functions like cross-entropy to optimize using
    maximum likelihood.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 过去，语言模型很少使用RL进行优化。相反，它们依赖于信息论损失函数，如交叉熵，使用最大似然进行优化。
- en: While both maximum likelihood and RL are used for learning, the way they update
    parameters is based on different principles. Maximum likelihood is based on minimizing
    error with respect to correct answers **with a fixed loss function**, whereas
    RL is based on **a learnable reward function** while maximizing cumulative reward
    through interaction with an environment.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管最大似然和RL都用于学习，但它们更新参数的方式基于不同的原理。最大似然是基于最小化与正确答案的误差**使用固定损失函数**，而RL则基于**可学习的奖励函数**，同时通过与环境的互动来最大化累积奖励。
- en: People (e.g., [John Schulman](https://www.youtube.com/live/hhiLw5Q_UFg?si=r2Cs_4WHoS5KJ6q2),
    [YoavGo](https://gist.github.com/yoavg/6bff0fecd65950898eba1bb321cfbd81), etc.)
    have given numerous justifications for training LLM with RL, but if I push for
    an intuitive answer, I believe we tried RL **because we wanted the flexibility
    to train the objective function as well.**
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 人们（例如，[John Schulman](https://www.youtube.com/live/hhiLw5Q_UFg?si=r2Cs_4WHoS5KJ6q2)，[YoavGo](https://gist.github.com/yoavg/6bff0fecd65950898eba1bb321cfbd81)等）给出了大量关于使用RL训练LLM的理由，但如果我追求直观的答案，我相信我们尝试RL**是因为我们想要训练目标函数的灵活性**。
- en: 'Traditional language model training optimizes only one thing: the model parameters,
    while keeping the loss function fixed. This approach restricts flexibility, as
    **losses like cross-entropy bring in strong inductive biases with themselves—the
    maximum likelihood.** It rewards the most probable next-token predictions, assuming
    the highest likelihood output is optimal.'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 传统的语言模型训练仅优化一个方面：模型参数，同时保持损失函数固定。这种方法限制了灵活性，因为**损失函数如交叉熵本身带来了强大的归纳偏差——最大似然**。它奖励最可能的下一个标记预测，假设最高似然输出是最佳的。
- en: If we use RL, we are training not only the model parameters but also the reward
    function and training policy. **The reward function acts as a learnable loss function**
    tailored to the end goal. This provides greater freedom to optimize because we
    are no longer bound by the maximum likelihood framework. **We can learn an objective
    function from the data.** In RLHF, your objective function is the reward model,
    and you use RL to optimize that objective function.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们使用RL，我们不仅在训练模型参数，还在训练奖励函数和训练策略。**奖励函数充当一个可学习的损失函数**，量身定制于最终目标。这提供了更大的优化自由度，因为我们不再受限于最大似然框架。**我们可以从数据中学习目标函数**。在RLHF中，你的目标函数是奖励模型，你使用RL来优化该目标函数。
- en: To summarize, we tried RL to parameterize and learn the objective function.
    This is still a running experiment.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，我们尝试使用RL来参数化和学习目标函数。这仍然是一个进行中的实验。
- en: How can we formulate this as an RL problem?
  id: totrans-137
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们如何将这定义为RL问题？
- en: The end goal of ChatGPT is to generate text that humans prefer.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: ChatGPT的最终目标是生成人类更喜欢的文本。
- en: 'Then we can define the components of the RL problem as follows:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们可以将RL问题的组件定义如下：
- en: '**Agent**: **The language model** acts as the RL agent. It learns to generate
    text that is deemed optimal based on the reward system.'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '**代理**：**语言模型**充当RL代理。它学习生成被认为是基于奖励系统的最佳文本。'
- en: '**Action space**: The action space in this case is the set of all possible
    language outputs that the LLM can generate. Given the variability of language,
    this space is vast.'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '**动作空间**：在这种情况下，动作空间是LLM可以生成的所有可能语言输出的集合。鉴于语言的多样性，这个空间非常广泛。'
- en: '**Policy**: The policy is **the probability distribution over possible outputs**
    of the model at each generation step. It determines which actions the agent should
    take based on the current state.'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '**策略**：策略是**模型在每个生成步骤上的可能输出的概率分布**。它根据当前状态决定代理应该采取哪些行动。'
- en: '**Environment**: The environment is what the agent interacts with and where
    it receives feedback on its actions. In the RLHF case, the environment gives feedback
    to the agent by giving them rewards based on a human preference model.'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '**环境**：环境是代理互动的对象，并且是代理获取其行动反馈的地方。在 RLHF 案例中，环境通过基于人类偏好模型给予奖励的方式向代理提供反馈。'
- en: '**Reward**: The reward is a scalar signal that comes from the human preference
    model. The agent’s goal in RL is to maximize this expected reward over time, leading
    to better text generation.'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '**奖励**：奖励是来自人类偏好模型的标量信号。RL 中的代理目标是最大化这个期望奖励，从而提高文本生成质量。'
- en: By framing language generation as an RL problem this way, the model can interact
    with the reward model to improve its policy over time.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将语言生成框定为一个 RL 问题，模型可以与奖励模型互动，从而随着时间的推移改善其策略。
- en: For those who understand best by reading code, here is [a straightforward implementation
    of the RLHF trainer, generously contributed by our open-source contributor, Phil
    Wang](https://github.com/lucidrains/PaLM-rlhf-pytorch/tree/main).
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 对于那些通过阅读代码更容易理解的人，这里有[一份由我们的开源贡献者 Phil Wang 慷慨提供的 RLHF 训练器的直接实现](https://github.com/lucidrains/PaLM-rlhf-pytorch/tree/main)。
- en: Anticipating that someone will inevitably abstract this version, I’ve copied
    the trainer script here. This covers most of the PPO training components and flow.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 预期有人会抽象化这个版本，我在这里复制了训练脚本。这涵盖了大多数 PPO 训练组件和流程。
- en: The `generate` function generates sequences of text based on a given prompt.
    It uses the actor-critic model to generate the sequence and the reward model to
    score each sequence. The sequence with the highest score is selected as the best
    sequence.
  id: totrans-148
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`generate` 函数根据给定的提示生成文本序列。它使用演员-评论家模型生成序列，并使用奖励模型为每个序列打分。选择得分最高的序列作为最佳序列。'
- en: The `learn` function batches experiences, computes PPO losses, and updates actor
    & critic networks. Implements core PPO algorithm.
  id: totrans-149
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`learn` 函数批量处理经验，计算 PPO 损失，并更新演员和评论家网络。实现核心 PPO 算法。'
- en: The `train` loop collects actor experience, evaluates reward, and stores in
    memory. Calls `learn()` periodically to update policy.
  id: totrans-150
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`train` 循环收集演员经验，评估奖励并存储在内存中。定期调用 `learn()` 来更新策略。'
- en: '[PRE4]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Proximal Policy Optimization (PPO)
  id: totrans-152
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Proximal Policy Optimization (PPO)
- en: How can we take the biggest possible improvement step on a policy using the
    data we currently have without stepping so far that we accidentally cause performance
    collapse?
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何在不冒着过度优化导致性能崩溃的风险的情况下，利用当前数据在策略上迈出最大的改进步伐？
- en: Proximal Policy Optimization (PPO) is a reinforcement learning algorithm that
    strikes a balance between sample efficiency and ease of implementation. To keep
    the policy from changing too much, its objective function uses a clipped surrogate
    objective. Hence the word “proximal” in its name. This strategy ensures stable
    and consistent learning while avoiding the often complex implementation of other
    algorithms that aim to achieve the same result.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: Proximal Policy Optimization (PPO) 是一种强化学习算法，它在样本效率和实施简便性之间取得了平衡。为了防止策略变化过大，其目标函数使用了裁剪的替代目标。因此它的名字中有“proximal”一词。这一策略确保了稳定且一致的学习，同时避免了其他旨在实现相同结果的算法常常复杂的实现过程。
- en: I won’t go into details about policy optimization and its implementation.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 我不会详细讨论策略优化及其实施。
- en: 'How PPO works deserves its own blog post, so I’ll link to some good, in-depth
    tutorials here:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: PPO 的工作原理值得另写一篇博客，所以我会在这里链接一些好的、深入的教程。
- en: '[](https://www.interconnects.ai/p/specifying-objectives-in-rlhf?source=post_page-----faa5ff4761d1--------------------------------)
    [## Specifying objectives in RLHF'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://www.interconnects.ai/p/specifying-objectives-in-rlhf?source=post_page-----faa5ff4761d1--------------------------------)
    [## 在 RLHF 中指定目标'
- en: At ICML, it is obvious that many people are getting value out of RLHF. What
    is limiting the scientific understanding of…
  id: totrans-158
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 在 ICML 上，很明显很多人从 RLHF 中获得了价值。什么限制了科学理解…
- en: www.interconnects.ai](https://www.interconnects.ai/p/specifying-objectives-in-rlhf?source=post_page-----faa5ff4761d1--------------------------------)  [##
    Proximal Policy Optimization - Spinning Up documentation
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: www.interconnects.ai](https://www.interconnects.ai/p/specifying-objectives-in-rlhf?source=post_page-----faa5ff4761d1--------------------------------)
    [## Proximal Policy Optimization - Spinning Up 文档
- en: '(Previously: Background for TRPO) PPO is motivated by the same question as
    TRPO: how can we take the biggest possible…'
  id: totrans-160
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: （之前：TRPO 背景）PPO 的动机与 TRPO 相同：我们如何在策略上迈出最大的改进步伐…
- en: spinningup.openai.com](https://spinningup.openai.com/en/latest/algorithms/ppo.html?source=post_page-----faa5ff4761d1--------------------------------)
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: '[spinningup.openai.com](https://spinningup.openai.com/en/latest/algorithms/ppo.html?source=post_page-----faa5ff4761d1--------------------------------)'
- en: '**Data Size Comparison**'
  id: totrans-162
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**数据规模比较**'
- en: The data used for InstructGPT was orders of magnitude smaller than the data
    used for pretraining the foundation models.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 用于 InstructGPT 的数据量比用于预训练基础模型的数据量小得多。
- en: Pretraining data like the one used for GPT-3 was 300 B tokens. In contrast,
    the InstructGPT uses ~O(10M) tokens.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 预训练数据如 GPT-3 使用了 3000 亿个标记。相比之下，InstructGPT 使用了约 O(10M) 个标记。
- en: The supervised fine-tuning (SFT) uses around 15,000 prompts for training and
    1,500 for validation.
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 监督微调（SFT）使用了约15,000个提示用于训练，1,500个用于验证。
- en: The reward model uses the most training and validation prompts, approximately
    **150,000** and **80,000,** respectively.
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 奖励模型使用了最多的训练和验证提示，分别约为**150,000**和**80,000**。
- en: The reinforcement learning phase uses only ~32,000 prompts for training and
    ~16,000 for validation to optimize the agent.
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 强化学习阶段仅使用了约32,000个提示用于训练，约16,000个用于验证，以优化代理。
- en: So in total, the RLHF data is on the order of 10 million tokens — dwarfed by
    the hundreds of billions used for general pre-training.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，总体来说，RLHF 数据约为 1000 万个标记——远远小于用于一般预训练的数百亿个标记。
- en: I’ll wrap up this blog post by highlighting InstructGPT’s beautiful and promising
    results.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 我将通过突出 InstructGPT 的美妙和有前途的结果来结束这篇博客文章。
- en: 'Result: It’s better to train with the right type of data than to make the model
    100 times bigger.'
  id: totrans-170
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结果：使用正确类型的数据进行训练比将模型扩大100倍更为有效。
- en: '![](../Images/412338746ca97ae340436913631ba7bb.png)'
  id: totrans-171
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/412338746ca97ae340436913631ba7bb.png)'
- en: From InstructGPT
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 来自 InstructGPT
- en: Take a look at the graph. The red and yellow lines represent Instruct-PPO variants,
    which are RLHF methods.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 看一下图表。红色和黄色线条代表 Instruct-PPO 变体，这些是 RLHF 方法。
- en: The ELO rating is on the left, with higher numbers indicating a preference.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: ELO评分在左侧，数字越高表示偏好越强。
- en: The PPO models have only 1.3 billion parameters, whereas SFT and GPT models
    (represented by the green and blue lines) have 175 billion parameters. Despite
    having far smaller parameters, humans significantly prefer the outputs from InstructGPT
    over those from GPT-3.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: PPO 模型仅有 13 亿个参数，而 SFT 和 GPT 模型（由绿色和蓝色线条表示）有 1750 亿个参数。尽管参数远小于 GPT-3，人类显著偏好
    InstructGPT 的输出。
- en: '**This shows that training with the right kind of data is more helpful than
    just making the model a hundred times bigger.**'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: '**这表明，使用正确类型的数据进行训练比仅仅将模型扩大一百倍更为有效。**'
- en: '![](../Images/2e5924d00597d4727fd9be97e9c8fa10.png)'
  id: totrans-177
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2e5924d00597d4727fd9be97e9c8fa10.png)'
- en: InstructGPT performs better across several other concrete metrics.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: InstructGPT 在几个其他具体指标上表现更佳。
- en: 'Our Holy Grail: The Emergent Generalization'
  id: totrans-179
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 我们的圣杯：涌现泛化
- en: Even though I dismissed my colleague’s claim about “The Emergent Generalization”
    by reminding him that all the prompts were in training data, InstructGPT team
    did observe the emergence of generalization. They reported a degree of **generalization
    in following instructions extending to new domains.**
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我通过提醒我的同事所有提示都在训练数据中而否定了他关于“涌现泛化”的说法，InstructGPT团队确实观察到了泛化的出现。他们报告了**遵循指令时扩展到新领域的泛化程度**。
- en: '![](../Images/88acc927659ce2a94108c14a4aa7d15c.png)'
  id: totrans-181
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/88acc927659ce2a94108c14a4aa7d15c.png)'
- en: Despite the fact that 99% of the training data was in English, the InstructGPT
    model occasionally demonstrated the ability to follow instructions in French.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管 99% 的训练数据是英语，InstructGPT 模型偶尔也显示出跟随法语指令的能力。
- en: '![](../Images/95373cc8156da402187419418b3558b7.png)'
  id: totrans-183
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/95373cc8156da402187419418b3558b7.png)'
- en: Furthermore, while there were no programming-specific instructions in the training
    set, there was some generalization shown in the code QA scenarios. GPT-3 doesn’t
    really answer the prompt, but InstructGPT does a reasonably good job.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，尽管训练集中没有针对编程的具体指令，但在代码问答场景中显示出了一些泛化能力。GPT-3 并未真正回答提示问题，但 InstructGPT 表现得相当不错。
- en: These hints of generalization indicate the coveted phenomenon of emergence in
    AI. Though InstructGPT’s skills are largely based on its training data, I believe
    the glimpses beyond it point to **the beginnings of learned reasoning**.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 这些泛化的迹象表明了 AI 中渴望的涌现现象。尽管 InstructGPT 的技能主要基于其训练数据，但我相信超越它的迹象指向了**学习推理的开端**。
- en: I’m optimistic about further breakthroughs as RLHF research scales up. If basic
    reinforcement learning can unlock some generalization, advancing to bigger and
    better models might help with even broader emergent intelligence.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 我对随着RLHF研究的扩展而取得进一步突破持乐观态度。如果基础的强化学习可以解锁一些泛化能力，那么进步到更大更好的模型可能会帮助我们获得更广泛的新兴智能。
- en: 'Improving Open-Source RLHF Training Data: Action Items'
  id: totrans-187
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 改进开源RLHF训练数据：行动事项
- en: Finally, I’d like to talk about the actions we can take to make open-source
    RLHF data better.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我想谈谈我们可以采取哪些行动来改善开源RLHF数据。
- en: We’re stuck in a vicious loop right now. Because there aren’t any good open-source
    LLMs like ChatGPT, not as many people are using them. This results in fewer user
    prompts to train and improve upon, leaving us with mediocre models. Meanwhile,
    commercial LLMs gain more users and continually improve.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在陷入了一个恶性循环。因为没有像ChatGPT这样的优秀开源LLM，所以使用它们的人并不多。这导致用于训练和改进的数据较少，结果就是我们得到的是平庸的模型。与此同时，商业LLM获得了更多的用户并不断改进。
- en: 'Here are a few ways we can break this cycle:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有几种方法可以打破这个循环：
- en: '**One central hub where the prompts, results, and feedback from open-source
    users (who have opted in) are collectively assembled**: At the moment, the only
    platform that I know of where I can try LLama 2 is [POE](https://poe.com/Llama-2-70b).
    However, the open-source maintainers can’t access the user inputs (prompts) and
    the model’s output, which are vital for improving the open-source model. We need
    to **make those data available to people who are working on open-source models.**
    This step alone will make open-source LLMs so much better. We also need to enhance
    the user experience of this platform in order to attract more users, which will
    lead to more data and better models.'
  id: totrans-191
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**一个集中的中心，汇总开源用户（已选择参与）的提示、结果和反馈**：目前，我知道的唯一可以尝试LLama 2的平台是[POE](https://poe.com/Llama-2-70b)。然而，开源维护者无法访问用户输入（提示）和模型的输出，这对改善开源模型至关重要。我们需要**让那些从事开源模型工作的人能够获得这些数据。**
    这一点本身将使开源LLM变得更好。我们还需要提升这个平台的用户体验，以吸引更多用户，这将带来更多数据和更好的模型。'
- en: '**A unified repository for data preparation codes:** A single hub where all
    open-source LLM enthusiasts can share their data work, such as cleaning, transformation,
    preparation, and auto-labeling, would be very beneficial. Examples include code
    that converts web content into a trainable format and automates the reformatting
    of some unlabeled data, such as text from textbooks, into prompt-response pairs.
    At the moment, all of the data effort used in open-source RLHF is dispersed and
    untracked. This makes sense, because this core and hard data work is what differentiates
    different LLMs. To leverage the power of community, though, we need to establish
    a single, centralized hub.'
  id: totrans-192
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**一个统一的数据准备代码库：** 一个集中平台，让所有开源LLM爱好者可以分享他们的数据工作，如清理、转换、准备和自动标注，将是非常有益的。例如，包括将网页内容转换为可训练格式的代码，以及将一些未标记的数据（如教科书中的文本）自动重新格式化为提示-响应对的代码。目前，开源RLHF中的所有数据工作都是分散且未被追踪的。这是有道理的，因为这些核心且艰难的数据工作是区分不同LLM的关键。然而，为了利用社区的力量，我们需要建立一个单一的、集中化的中心。'
- en: '**Incentivize data sharing.** This is the hardest part, and it is easier said
    than done. I don’t have a good answer for this at the moment. In order for open
    source to progress, people need to be transparent about their training data. We
    need to find a way to incentivize data work and sharing. We also need to figure
    out the close collaboration between the open-source data lead and those training
    the LLMs.'
  id: totrans-193
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**激励数据共享。** 这是最困难的部分，说起来容易做起来难。我目前没有一个好的答案。为了让开源取得进展，人们需要对他们的训练数据保持透明。我们需要找到一种激励数据工作和共享的方法。我们还需要弄清楚开源数据负责人和训练LLM（大语言模型）之间的密切合作。'
- en: If we can work out the data and feedback loop parts, I do believe there is potential
    for the community to create better LLMs than those currently available commercially.
    It’s an ambitious goal, but with collective community effort, I believe it’s achievable.
    I hope that after reading this blog post, you’ll be a little more motivated to
    contribute to open-source data.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们能够解决数据和反馈循环的问题，我确实相信社区有潜力创造出比目前商业上可用的LLM更好的模型。这是一个雄心勃勃的目标，但通过集体社区的努力，我相信这是可以实现的。我希望在读完这篇博客文章后，你会更有动力去贡献开源数据。
- en: Really grateful to my reviewers for squeezing me into their packed schedules
    and sharing their thoughts to the blog. Without them, this blog wouldn’t be half
    as good.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 非常感谢我的审阅者们在他们紧张的日程中挤出时间，分享他们的想法给博客。没有他们，这个博客不会好到现在的一半。
- en: 'Shoutout to (in alphabetical order by last name): Nathan Lambert (ex-Huggingface),
    Rosanne Liu (Deepmind, ML Collective), Erin LeDell (AutoML), Joshua Moore (Snap),
    Abhinav Srivastava (Breez), Susan Zhang (OPT-175B)'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 特别感谢（按姓氏字母顺序排列）：Nathan Lambert（前Huggingface）、Rosanne Liu（Deepmind, ML Collective）、Erin
    LeDell（AutoML）、Joshua Moore（Snap）、Abhinav Srivastava（Breez）、Susan Zhang（OPT-175B）
