- en: A gentle introduction to Steerable Neural Networks (part 1)
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 可调整神经网络的温和介绍（第1部分）
- en: 原文：[https://towardsdatascience.com/a-gentle-introduction-to-steerable-neural-networks-part-1-32323d95b03f?source=collection_archive---------0-----------------------#2023-11-21](https://towardsdatascience.com/a-gentle-introduction-to-steerable-neural-networks-part-1-32323d95b03f?source=collection_archive---------0-----------------------#2023-11-21)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/a-gentle-introduction-to-steerable-neural-networks-part-1-32323d95b03f?source=collection_archive---------0-----------------------#2023-11-21](https://towardsdatascience.com/a-gentle-introduction-to-steerable-neural-networks-part-1-32323d95b03f?source=collection_archive---------0-----------------------#2023-11-21)
- en: What are Steerable Neural Networks and context
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 什么是可调整神经网络及其背景
- en: '[](https://medium.com/@mat.cip43?source=post_page-----32323d95b03f--------------------------------)[![Matteo
    Ciprian](../Images/61dab88069d99263e941a0e549473bdf.png)](https://medium.com/@mat.cip43?source=post_page-----32323d95b03f--------------------------------)[](https://towardsdatascience.com/?source=post_page-----32323d95b03f--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----32323d95b03f--------------------------------)
    [Matteo Ciprian](https://medium.com/@mat.cip43?source=post_page-----32323d95b03f--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@mat.cip43?source=post_page-----32323d95b03f--------------------------------)[![Matteo
    Ciprian](../Images/61dab88069d99263e941a0e549473bdf.png)](https://medium.com/@mat.cip43?source=post_page-----32323d95b03f--------------------------------)[](https://towardsdatascience.com/?source=post_page-----32323d95b03f--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----32323d95b03f--------------------------------)
    [Matteo Ciprian](https://medium.com/@mat.cip43?source=post_page-----32323d95b03f--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F975b976da56a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-gentle-introduction-to-steerable-neural-networks-part-1-32323d95b03f&user=Matteo+Ciprian&userId=975b976da56a&source=post_page-975b976da56a----32323d95b03f---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----32323d95b03f--------------------------------)
    ·15 min read·Nov 21, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F32323d95b03f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-gentle-introduction-to-steerable-neural-networks-part-1-32323d95b03f&user=Matteo+Ciprian&userId=975b976da56a&source=-----32323d95b03f---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F975b976da56a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-gentle-introduction-to-steerable-neural-networks-part-1-32323d95b03f&user=Matteo+Ciprian&userId=975b976da56a&source=post_page-975b976da56a----32323d95b03f---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----32323d95b03f--------------------------------)
    ·15分钟阅读·2023年11月21日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F32323d95b03f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-gentle-introduction-to-steerable-neural-networks-part-1-32323d95b03f&user=Matteo+Ciprian&userId=975b976da56a&source=-----32323d95b03f---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F32323d95b03f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-gentle-introduction-to-steerable-neural-networks-part-1-32323d95b03f&source=-----32323d95b03f---------------------bookmark_footer-----------)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F32323d95b03f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-gentle-introduction-to-steerable-neural-networks-part-1-32323d95b03f&source=-----32323d95b03f---------------------bookmark_footer-----------)'
- en: Introduction
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: Geometrical deep learning, as a branch of Deep Learning, aims to extend traditional
    AI frameworks such as Convolutional Neural Networks to process 3D or 2D geometric
    objects represented as graphs, manifolds, or point clouds. By incorporating geometric
    relationships and spatial dependencies directly into the learning framework, geometrical
    deep learning harnesses the inherent structural properties of data to eliminate
    the requirement for memory-intensive data augmentation techniques. For all these
    reasons, Geometrical Deep Learning can be seen as valuable tool for tackling complex
    data scenarios in domains such as computer vision, natural language processing,
    and beyond. Concerning the type of task and the type of transformation, a large
    variety of new CNN architectures have been proposed so far as “Spherical Neural
    Networks” ([link](https://arxiv.org/abs/1801.10130)), “Graph Neural Networks”
    (l[ink](https://arxiv.org/pdf/1812.08434.pdf%2523:~:text=Graph%252520neural%252520networks%252520%28GNNs%29%252520are,its%252520neighborhood%252520with%252520arbitrary%252520depth.)),
    and **“Steerable Neural Networks”.**
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 几何深度学习作为深度学习的一个分支，旨在扩展传统的AI框架，如卷积神经网络，以处理表示为图、流形或点云的三维或二维几何对象。通过直接将几何关系和空间依赖性整合到学习框架中，几何深度学习利用数据的固有结构特性，消除了对内存密集型数据增强技术的需求。出于所有这些原因，几何深度学习可以被视为在计算机视觉、自然语言处理等领域处理复杂数据场景的有价值工具。关于任务类型和转换类型，迄今已提出了大量新的CNN架构，如“球形神经网络”
    ([链接](https://arxiv.org/abs/1801.10130))， “图神经网络” ([链接](https://arxiv.org/pdf/1812.08434.pdf%2523:~:text=Graph%252520neural%252520networks%252520%28GNNs%2529%252520are,its%252520neighborhood%252520with%252520arbitrary%252520depth.))
    和 **“可转向神经网络”**。
- en: '***Steerable Neural Networks*** *have garnered significant interest due to
    their unique ability to extend the capabilities of regular Convolutional Neural
    Networks (CNNs). These networks can be viewed as an evolution of CNNs, where the
    kernel is conditioned to satisfy specific constraints. While CNNs excel at being*
    ***equivariant*** *to translation, Steerable Neural Networks take it a step further
    by offering enhanced flexibility and capturing a* ***wider range of transformations****,
    such as rotation.*'
  id: totrans-10
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '***可转向神经网络*** *因其将常规卷积神经网络（CNNs）的能力扩展到新的领域而引起了广泛关注。这些网络可以被视为CNNs的演变，其中核被条件化以满足特定约束条件。虽然CNNs在对平移的等变性方面表现出色，但可转向神经网络通过提供增强的灵活性和捕获更广泛的转换，如旋转，而更进一步。*'
- en: '**This tutorial** will present an introduction to “Steerable Neural Networks”
    (S-CNNs), trying to convey an intuitive understanding of the mathematical concepts
    behind them and a step-by-step explanation on how to design these networks.The
    tutorial is composed of two articles. This **first** article serves as an introduction
    to steerable neural networks (NNs), explaining their purpose and delving deeper
    into the concepts and formalism underlying S-CNNs. The **second** article ([here](https://medium.com/@mat.cip43/a-gentle-introduction-to-steerable-neural-networks-part-2-56dfc256b690))
    discusses at a high level the design of steerable filters and the steerable networks
    as overall.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '**本教程** 将介绍“可转向神经网络”（S-CNNs）的简介，试图传达对其背后数学概念的直观理解以及如何设计这些网络的逐步解释。本**第一篇**文章作为介绍可转向神经网络的起点，解释其目的并深入探讨支持S-CNNs的概念和形式化。**第二篇**文章（[这里](https://medium.com/@mat.cip43/a-gentle-introduction-to-steerable-neural-networks-part-2-56dfc256b690)）在高层次上讨论了可转向滤波器的设计和整体可转向网络。'
- en: This work aims at filling the gap between the current scientific literature
    and the wider data science audience. It is ideal for tech professionals as well
    as for researchers in this new branch of machine learning.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 本工作旨在填补当前科学文献与更广泛数据科学受众之间的差距。它非常适合技术专业人士以及这一新的机器学习分支的研究人员。
- en: '![](../Images/199602fecfa6cfb6243546f2201a27b0.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/199602fecfa6cfb6243546f2201a27b0.png)'
- en: Example of a simple Steerable Neural Network taken from the paper [[3]](https://arxiv.org/abs/1711.07289).
    As possible to see the effect of rotation in the input image is reflected to the
    the output response of the network.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 来自论文[[3]](https://arxiv.org/abs/1711.07289)的一个简单可转向神经网络的示例。可以看到输入图像的旋转效果反映在网络输出响应中。
- en: 'The following papers are taken as reference:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 以下论文作为参考：
- en: '[1] “3D Steerable CNNs: Learning Rotationally Equivariant Features in Volumetric
    Data”, Weilier et al., ([link](https://arxiv.org/abs/1807.02547));'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] “3D 可转向 CNN：在体积数据中学习旋转等变特征”，Weilier 等，([link](https://arxiv.org/abs/1807.02547));'
- en: '[2] “Steerable CNNs”, Cohen et al. [( link](https://arxiv.org/abs/1612.08498));'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] “可转向 CNN”，Cohen 等，([link](https://arxiv.org/abs/1612.08498));'
- en: '[3] “Learning Steerable Filters for Rotation Equivariant CNNs”,Weilier et al.,
    ([link](https://arxiv.org/abs/1711.07289))'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] “学习用于旋转等变 CNN 的可转向滤波器”，Weilier 等，([link](https://arxiv.org/abs/1711.07289))'
- en: '[4] “General E(2)-Equivariant Steerable CNNs” Weilier et al., ([link](https://arxiv.org/abs/1911.08251))'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] “通用 E(2)-等变可转向 CNN” Weilier 等，([link](https://arxiv.org/abs/1911.08251))'
- en: '[5] “Scale Steerable Filters for the Locally Scale-Invariant Convolutional
    Neural Network”, Ghosh et al. ([link](https://www.researchgate.net/publication/334480982_Scale_Steerable_Filters_for_the_Locally_Scale-Invariant_Convolutional_Neural_Network?enrichId=rgreq-7fc8b3654779eb94d36221a6e5fab2ff-XXX&enrichSource=Y292ZXJQYWdlOzMzNDQ4MDk4MjtBUzo3ODEyNTQ4ODI1MDg4MDBAMTU2MzI3NzA4NzY1NA%25253D%25253D&el=1_x_3&_esc=publicationCoverPdf))'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '[5] “适用于局部尺度不变卷积神经网络的尺度可转向滤波器”，Ghosh 等，([link](https://www.researchgate.net/publication/334480982_Scale_Steerable_Filters_for_the_Locally_Scale-Invariant_Convolutional_Neural_Network?enrichId=rgreq-7fc8b3654779eb94d36221a6e5fab2ff-XXX&enrichSource=Y292ZXJQYWdlOzMzNDQ4MDk4MjtBUzo3ODEyNTQ4ODI1MDg4MDBAMTU2MzI3NzA4NzY1NA%25253D%25253D&el=1_x_3&_esc=publicationCoverPdf))'
- en: '[6] “A program to build E(n)-equivariant steerable CNNs.” Cesa et al. ([link](https://openreview.net/pdf?id=WE4qe9xlnQw))'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '[6] “构建 E(n)-等变可转向 CNN 的程序。” Cesa 等，([link](https://openreview.net/pdf?id=WE4qe9xlnQw))'
- en: 'What are Steerable Neural Networks:'
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是可转向神经网络：
- en: Steerable neural networks take their name from the particular type of filters
    they use. Such filters are called g-*steerable filters* and they have been inspired
    by the steerable filters which gained popularity in the image recognition field
    for edge detection or oriented texture analysis at the [beginning of the 90’s](https://ieeexplore.ieee.org/document/93808).
    Steerable means commonly dirigible, manageable, capable of being managed or controlled.
    Following this convention, the response of a steerable filters is orientable and
    adaptable to a specific orientation of the input (an image for example). Steerability
    is related to another very important property which is called **Equivariance**.
    In an equivariant filter, if the INPUT to the filter is transformed according
    to a precise and well-defined geometric transformation *g* (translation, rotation,
    shift), the OUTPUT (which results from the convolution of the INPUT with the filter)
    is transformed by the same transformation *g*. In general, equivariance does not
    require that the transformations (the one at the input and the one at the output)
    are the same. This concept will be better addressed in the next paragraph but
    for now it allows us to provide a first definition of steerable filter and steerable
    CNN.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 可转向神经网络得名于它们使用的特定类型的滤波器。这些滤波器称为 g-*可转向滤波器*，它们的灵感来自于在图像识别领域中用于边缘检测或定向纹理分析的可转向滤波器，这些滤波器在[90年代初](https://ieeexplore.ieee.org/document/93808)获得了广泛的应用。可转向通常指的是可操控的、可管理的、能够被控制的。按照这种惯例，可转向滤波器的响应是可定向的，并且可以适应输入的特定方向（例如一张图像）。可转向性与另一个非常重要的属性相关，这就是**等变性**。在等变滤波器中，如果滤波器的输入经过了一个精确且明确的几何变换
    *g*（平移、旋转、移动），则输出（即输入与滤波器卷积的结果）也会经过相同的变换 *g*。通常，等变性并不要求变换（输入和输出的变换）是相同的。这个概念将在下一个段落中得到更好的阐述，但目前这使我们能够提供对可转向滤波器和可转向
    CNN 的初步定义。
- en: '*A* ***Steerable CNN filter*** *can be defined as a filter whose kernel is
    structured as a concatenation of different steerable filters. These filters show
    equivariance properties in relation to the* ***operation of convolution*** *with
    respect to a set of well-defined geometric transformations.*'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '*一个* ***可转向 CNN 滤波器*** *可以定义为一个其内核结构为不同可转向滤波器的串联的滤波器。这些滤波器在* ***卷积操作*** *相对于一组定义明确的几何变换方面显示出等变性特性。*'
- en: 'As we can see later, the condition of equivariance on the convolution operation
    leads to the imposition of specific constraints over the *structure of the kernel
    and on its weights*. From this definition it is now possible to define what a
    steerable CNN is: **Steerable Neural Networks** are neural networks composed of
    a series of steerable filters.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们稍后将看到的，卷积操作上的等变性条件导致对*内核结构及其权重*施加特定的约束。从这个定义中，现在可以定义什么是可转向 CNN：**可转向神经网络**是由一系列可转向滤波器组成的神经网络。
- en: 'What are S-CNN used for:'
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: S-CNN 的用途：
- en: The strength of a normal CNN is in its equivariance to translation. However,
    Steerable NN’s are more flexible and can show other types of transformations,
    rotation. In a rotationally equivariant problem, an unmodified CNN is compelled
    to learn rotated versions of the same filter introducing a redundant degree of
    freedom and increasing the risk of overfitting.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 普通 CNN 的优势在于其对平移的等变性。然而，可导神经网络更加灵活，可以展示其他类型的变换，例如旋转。在旋转等变问题中，未经修改的 CNN 被迫学习相同滤波器的旋转版本，从而引入了冗余的自由度，并增加了过拟合的风险。
- en: For this reason, Steerable CNN networks can outperform the classical CNN by
    directly incorporating information about the geometrical transformations acting
    at the input. This property makes S-CNNs particularly useful for several challenging
    tasks where we have to process inputs that have a geometrical description and
    representation such as images, manifolds, or vector fields.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，可导 CNN 网络可以通过直接整合输入处几何变换的信息，优于经典 CNN。这一特性使得 S-CNN 在处理具有几何描述和表示的输入（如图像、流形或向量场）时特别有用。
- en: 'Possible practical applications are for example :'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 可能的实际应用例如：
- en: '**Challenging 2D image segmentation:**predicting the cell boundaries given
    an input microscope image.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**挑战性的 2D 图像分割：** 给定输入显微镜图像预测细胞边界。'
- en: '**3D model classification:** classifying and recognizing 3D objects.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**3D 模型分类：** 对 3D 物体进行分类和识别。'
- en: '**3D chemical structure classification:** predicting the 3D chemical structure
    of a molecule given its chemical structure. A possible example is the prediction
    of spatial preferences of a group of amino acids given its sequence as explained
    in section 5.4 of the paper [[2]](https://arxiv.org/pdf/1807.02547.pdf).'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**3D 化学结构分类：** 预测给定化学结构的分子 3D 化学结构。一个可能的例子是根据氨基酸序列预测其空间偏好，具体见论文的第 5.4 节 [[2]](https://arxiv.org/pdf/1807.02547.pdf)。'
- en: '![](../Images/85b13d557a9a2caf33cfc0543a5ccf22.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/85b13d557a9a2caf33cfc0543a5ccf22.png)'
- en: Example of application of a a 3D steerable neural network for 3D object recognition.
    The input object (on the top) , and the representation of two different hidden
    layers feature maps. Taken from [Link](https://www.youtube.com/watch?v=ENLJACPHSEA)
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 3D 可导神经网络在 3D 物体识别中的应用示例。输入物体（在顶部）以及两个不同隐藏层特征图的表示。摘自 [Link](https://www.youtube.com/watch?v=ENLJACPHSEA)
- en: Preliminary definitions and Context
  id: totrans-35
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 初步定义和背景
- en: After introducing Steerable Neural Networks and their applications, let’s dive
    into the theory behind them. This section offers a more formal explanation of
    equivariance and steerability, providing essential definitions and a formal framework
    that will be instrumental in understanding the construction of steerable filters
    in the subsequent article.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在介绍了可导神经网络及其应用后，让我们深入探讨它们背后的理论。本节提供了等变性和可导性的更正式解释，提供了理解后续文章中可导滤波器构造所需的基本定义和正式框架。
- en: This article relies on an understanding of maps and geometrical transformations,
    for more information look on this other [article](/geometric-transformations-in-computer-vision-an-intuitive-explanation-with-python-examples-b0b6f06e1844).
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 本文依赖于对映射和几何变换的理解，更多信息请参考这篇 [文章](/geometric-transformations-in-computer-vision-an-intuitive-explanation-with-python-examples-b0b6f06e1844)。
- en: '1\. EQUIVARIANCE:'
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1. 等变性：
- en: 'Equivariance is a property of particular interest in symmetric problems. As
    stated before, in an equivariant model when the input is acted on by the transformation,
    it also acts on the output such that the application of the transformation can
    be applied before or after the model’s application with no change in overall behaviour.
    In an everyday setting there are many examples of equivariance. For example, when
    driving, the direction in which a car steers when the wheel is turned is equivariant
    with respect to the direction the car is pointing. Formally, if we have a map
    𝛙: *X → Y*, where *X*⊂ℝᵈ and *Y*⊂ℝᵈ¹ , and *g*, a geometrical transformation belonging
    to the group *G*, 𝛙 is equivariant to *G* if :'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '等变性是对称问题中特别感兴趣的特性。如前所述，在等变模型中，当输入经过变换作用时，输出也会受到相应作用，从而使得变换的应用可以在模型应用之前或之后进行，而整体行为不发生变化。在日常环境中有许多等变性的例子。例如，驾驶时，当转动方向盘时，汽车的转向方向与汽车所指方向是等变的。形式上，如果我们有一个映射
    𝛙: *X → Y*，其中 *X*⊂ℝᵈ 和 *Y*⊂ℝᵈ¹，以及 *g*，一个属于群体 *G* 的几何变换，𝛙 对 *G* 是等变的，如果：'
- en: '![](../Images/b17fdc06446a51dca6bd81149914f591.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b17fdc06446a51dca6bd81149914f591.png)'
- en: 'Eq.1: Math equation representing the equivariance of 𝛙 with respect to g.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 'Eq.1: 表示𝛙对g的等变性的数学方程。'
- en: 'where *Π₀(g) : X → X’* and *Π₁(g): Y→ Y’* are two linear maps ( e.g. often
    matrices applied by multiplication) determined by the application of *g* to x.
    A visual example is given by the picture below taken from the paper [[2](https://arxiv.org/abs/1612.08498)].
    In the image *g* is a rotation, specifically “*rotation of -90°”;* it is, therefore,
    denominated *r.* *Π₀(r)* operates in the domain of 𝛙 (=X), while *Π₁(r)* works
    in the codomain of 𝛙 (=Y).'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '其中*Π₀(g) : X → X’*和*Π₁(g): Y→ Y’*是由应用*g*到x确定的两个线性映射（例如，通常是通过乘法应用的矩阵）。下图提供了一个来自论文[[2](https://arxiv.org/abs/1612.08498)]的视觉示例。在图像中，*g*是旋转，具体为“*旋转-90°*”，因此被称为*r*。*Π₀(r)*在领域𝛙（=X）中操作，而*Π₁(r)*在𝛙（=Y）的值域中工作。'
- en: If *X=ℝ²*, 2-D cartesian space, and r is the transformation “clock-wise rotation
    of 90*°”,* the matrix *Π₀(r)* would be equal to a 2x2 Euler matrix with θ=π/2.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 如果*X=ℝ²*，2维笛卡尔空间，且r是“顺时针旋转90°”的变换，则矩阵*Π₀(r)*将等于θ=π/2的2x2欧拉矩阵。
- en: One should notice that if 𝛙 is equivariant with respect to G, applying a transformation
    and then computing the map produces the same result as calculating the map and
    then applying the transformation, a property formerly known as commutation.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 应注意，如果𝛙对G是等变的，那么施加变换后再计算映射会产生与先计算映射再施加变换相同的结果，这一属性以前称为交换性。
- en: '![](../Images/deec9ffe252abc3e89db765ba8e3730f.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/deec9ffe252abc3e89db765ba8e3730f.png)'
- en: 'Fig2A: A visual example of a function Ѱ equivariant to a transformation r.
    Taken from article [[2]](https://arxiv.org/abs/1612.08498).'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 'Fig2A: 函数Ѱ对变换r等变的视觉示例。摘自文章[[2]](https://arxiv.org/abs/1612.08498)。'
- en: '*At this point it is also worth mentioning a special case.* ***Invariance****,*
    a particular type of equivariance where *X=X’* and *Y=Y’*.In whatever way the
    input is transformed, the output always remains the same. From a deep learning
    prospective, invariant filter could be useful for example for object recognition:
    however an input image is rotated the output of the filter remains the same. One
    should note that the spaces *X* and *Y* may not necessarily have the same dimensionality,
    for example if we are trying to determine the orientation (*Y* as a 2-vector)
    of a car in a picture (*X* as a 2-d array of pixels), then the transformations
    *Π₁(g)* and *Π₀(g)* will be different as they apply to different spaces, even
    when they share the same g.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '*此时还值得提到一个特例。* ***不变性***，一种特殊类型的等变性，其中*X=X’*和*Y=Y’*。无论输入如何变换，输出始终保持不变。从深度学习的角度来看，不变滤波器例如在物体识别中可能有用：无论输入图像如何旋转，滤波器的输出始终保持不变。需要注意的是，*X*和*Y*的空间可能不具有相同的维度，例如，如果我们试图确定图片中汽车的方向（*Y*作为2维向量）而*X*作为像素的2维数组，则变换*Π₁(g)*和*Π₀(g)*将不同，因为它们适用于不同的空间，即使它们共享相同的g。'
- en: '2\. STEERABLE FILTERs:'
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2. 可操控滤波器：
- en: In contrast to the steerability of a car, steerable filters are a little more
    challenging to intuit. Both, however, share the underlying goal of achieving a
    consistent and predictable response to a specific parameter — a response that
    is intimately linked to the inherent transformations of the filter itself.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 与汽车的可操控性相比，可操控滤波器稍微难以直观理解。然而，两者都共享实现对特定参数一致和可预测响应的基本目标——这种响应与滤波器本身的固有变换密切相关。
- en: 'An intuitive example might be the following: think of a wind vane on a rooftop
    that shows the direction of the wind. Instead of installing a separate sensor
    for every possible wind direction, which would be impractical, you have a wind
    vane that rotates to align with the current wind direction.A steerable filter
    is like a wind vane. It adapts to directions encoded in input signals without
    needing a unique sensor for each possibility. In the same way, steerable filters
    in image processing adapt to different features or orientations in an image without
    requiring a separate filter for every possible orientation of the input. This
    approach offers an intelligent and effective method for modeling systems. In the
    context of machine learning, it enables us to concentrate on constructing valuable
    models without worrying about augmentation or incorporating additional weights
    to handle different orientations.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 一个直观的例子可能如下：想象一下屋顶上的风向标，显示风的方向。与其为每种可能的风向安装单独的传感器（这是不切实际的），不如安装一个可以旋转以与当前风向对齐的风向标。可转向滤波器就像一个风向标，它根据输入信号中编码的方向自适应，而无需为每种可能的输入方向使用独立的传感器。同样，在图像处理中，可转向滤波器适应图像中的不同特征或方向，而无需为每种可能的输入方向使用独立的滤波器。这种方法为建模系统提供了智能和有效的方法。在机器学习的背景下，它使我们能够专注于构建有价值的模型，而不必担心增强或增加额外的权重以处理不同的方向。
- en: While steerability can be applied generally to any set of transformations, we
    will here use rotations to introduce the idea in a more formal way.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管可转向性可以普遍应用于任何一组变换，我们将在此使用旋转来更正式地介绍这个概念。
- en: 'Let 𝛙: ℝᵈ →ℝᵈ¹ be a convolution map whose kernel function is **k**.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '让 𝛙: ℝᵈ →ℝᵈ¹ 成为其核函数为 **k** 的卷积映射。'
- en: 'Be *x*∈ℝⁿ , given an input signal depending on *x* , *f(x)* ∈ ℝᵈ, and outputsignal
    *, f* ₁(x) ∈ ℝᵈ¹ , we can write: *f* ₁(*x*)= 𝛙( *f(x)*) which means *f* ₁(x)=
    *k(x)* ∗ *f(x)* .'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 *x*∈ℝⁿ，给定一个依赖于 *x* 的输入信号 *f(x)* ∈ ℝᵈ，并且输出信号 *f*₁(*x*) ∈ ℝᵈ¹，我们可以写成：*f*₁(*x*)=
    𝛙(*f(x)*)，这意味着 *f*₁(*x*)= *k(x)* ∗ *f(x)*。
- en: 'This filter is defined *steerable* with respect to rotations if :'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 如果对旋转的转向滤波器定义如下：
- en: (1) its convolution kernel *k(x)* of each output element, can be expressed as
    a sum of basis functions *ψⱼ(x),* j*=1,..M*.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: (1) 每个输出元素的卷积核 *k(x)* 可以表示为基函数 *ψⱼ(x)* 的和，其中 *j*=1,..M*。
- en: '(2) the filter’s rotation by an arbitrary angle θ , ***g_θ*,** can be expressedin
    terms of rotations applied to each single basis function (this is valid for each
    θ). In mathematical terms it means:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: (2) 通过任意角度 θ 旋转滤波器的 **g_θ** 可以用每个基函数的旋转表示（对于每个 θ 均适用）。数学上来说，这意味着：
- en: '![](../Images/7f10890df339ac4edc24ab8c873cee97.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7f10890df339ac4edc24ab8c873cee97.png)'
- en: 'Eq.2: Definition of Steerable filter'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 'Eq.2: 可转向滤波器的定义'
- en: Thanks to this property, it is possible to uniquely orient the filter’s response
    to an input, by modifying the values of *wⱼ*. Let’s provide an example.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这一特性，可以通过修改 *wⱼ* 的值来唯一定向滤波器对输入的响应。我们来举个例子。
- en: 'The simplest illustration of a single steerable filter is in 2D space a filter
    whose kernel function is the directional derivative of a **two-dimensional Gaussian**.
    In this case the *k: ℝ² →ℝ* and *x = (x₁,x*₂*)* ∈ ℝ² :'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '在二维空间中，一个可定向单个可转向滤波器的最简单的例子是其核函数为 **二维高斯** 的方向导数。在这种情况下，*k: ℝ² →ℝ*，且 *x = (x₁,x₂)*
    ∈ ℝ²：'
- en: '![](../Images/85ffdee84540d25257d4f11173966e62.png)![](../Images/c211bd92826d4b82af2aab13c0061c44.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/85ffdee84540d25257d4f11173966e62.png)![](../Images/c211bd92826d4b82af2aab13c0061c44.png)'
- en: 'Eq.3: Directional derivative of a **two-dimensional Gaussian (above)** and
    transformation of a function k R² →R under gθ.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 'Eq.3: **二维高斯** 的方向导数（上）和函数 *k* R² →R 在 gθ 下的转换。'
- en: In the following lines we will show that this filter is steerable in the sense
    explained above.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的几行中，我们将展示该滤波器按上述方式是可转向的。
- en: From the theory we know that, given that *k* codomain is *ℝ*, we can write the
    rotated filter as Eq.3 (for more info look at Eq.3 in the next session).
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 从理论上我们知道，鉴于 *k* 的值域是 *ℝ*，我们可以将旋转后的滤波器写成 Eq.3（有关更多信息，请参见下一节中的 Eq.3）。
- en: 'By developing this equation we can show the steerability:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 通过推导这个方程，我们可以展示其可转向性：
- en: '![](../Images/124d0a45e88094b3bbcdb7ab13a1bfd3.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/124d0a45e88094b3bbcdb7ab13a1bfd3.png)'
- en: 'Eq.5: Mathematical proof of steerability of a the directional derivative of
    a **two-dimensional Gaussian**'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 'Eq.5: **二维高斯** 的方向导数可转向的数学证明'
- en: In this case we have applied the transformation ***g_θ*:** ℝ²→ℝ² and it is represented
    by the 2D euler matrix (see later induced representation). If we calculate ***k(g_θ***
    ⁻¹***(x₁,x₂))*,** we can see, after some algebra, that the generic rotated version
    of this impulsive filter can be expressed as a linear combination of two basis
    functions ѱ*₁(x₁,x*₂*)* and ѱ₂*(x₁,x*₂*)* with coefficient parameterized by θ.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们应用了变换 ***g_θ*:** ℝ²→ℝ²，并且它由二维欧拉矩阵表示（见下文诱导表示）。如果我们计算 ***k(g_θ*** ⁻¹***(x₁,x₂))*,**
    我们可以通过一些代数运算看到，这种冲激滤波器的通用旋转版本可以表示为两个基函数 ѱ*₁(x₁,x*₂*)* 和 ѱ₂*(x₁,x*₂*)* 的线性组合，系数由
    θ 参数化。
- en: As possible to see in the equation reported below (Eq.6) , given the linearity
    of convolution, it is always possible to express the convolution of an input function
    f with the θ-rotated impulsive response **g_θ(k(x,y))=**k_θas a linear combination
    of the convolutions of f with the single basis ѱ*₁*,ѱ₂ of k.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 如下方程（方程6）所示，由于卷积的线性特性，输入函数 f 与θ旋转的冲激响应 **g_θ(k(x,y))=**k_θ 的卷积始终可以表示为 f 与 k
    的单一基函数 ѱ*₁*、ѱ₂ 的卷积的线性组合。
- en: '![](../Images/ce56b68ee568798c988fc5128821e328.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ce56b68ee568798c988fc5128821e328.png)'
- en: 'Eq.6: Convolution of a steerable filter with f.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 方程6：一个可转向滤波器与 f 的卷积。
- en: This formula highlights the *power of steerable filters in a neural network*.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 这个公式突出了 *可转向滤波器在神经网络中的力量*。
- en: By incorporating these filters, we have the potential to construct a steerable
    kernel that ‘steers’ its responses to the orientation of the input. Each basis
    function acts like a versatile tool, permitting the network to efficiently blend
    these functions using learned weights ‘w₁’ and ‘w₂’ to respond accurately to varying
    orientations. For instance, when the network encounters data with varying orientations,
    such as a rotated object in an image, it configures these weights to align the
    kernel’s responses to the orientation of the input data. This adaptability enhances
    efficiency and effectiveness, leading to the same or better outcomes with fewer
    parameters. For this reason this approach can be used as the foundation for a
    more powerful CNN using steerable properties to handle diverse input orientations.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 通过引入这些滤波器，我们有可能构造一个可转向的核，它根据输入的方向“调整”其响应。每个基函数像一个多功能工具，允许网络使用学习到的权重‘w₁’和‘w₂’来高效地混合这些函数，以准确响应不同的方向。例如，当网络遇到具有不同方向的数据，如图像中的旋转物体时，它配置这些权重以使核的响应与输入数据的方向对齐。这种适应性提高了效率和效果，从而在参数更少的情况下达到相同或更好的结果。因此，这种方法可以作为使用可转向属性处理各种输入方向的更强大的
    CNN 的基础。
- en: Specifically, in next article, we’ll explore this further and see how a we can
    use the concept of steerable filter to build equivariant filters.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一篇文章中，我们将进一步探讨这个问题，并了解如何使用可转向滤波器的概念来构建等变滤波器。
- en: However, before we dive in, some definitions in this context will provide clarity
    and aid our discussion. For this reason in the next paragraph we introduce some
    formalism around convolution.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在深入之前，一些定义将提供清晰度并帮助我们的讨论。因此，在下一段中我们引入了一些关于卷积的形式化内容。
- en: '3\. FORMALISM:'
  id: totrans-76
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3\. 形式化：
- en: In this part, we try to give to the reader a schematic explanation of all the
    elements considered in the analysis. This formalism will allow us to define more
    formally a CNN and the geometrical transformations which operate at the input
    level. This will allow us in the next [article](https://medium.com/@mat.cip43/a-gentle-introduction-to-steerable-neural-networks-part-3-56dfc256b690)
    to understand how Steerable CNNs work.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们试图给读者提供一个所有分析元素的示意性解释。这种形式化将允许我们更正式地定义 CNN 及其在输入层操作的几何变换。这将使我们在下一篇 [文章](https://medium.com/@mat.cip43/a-gentle-introduction-to-steerable-neural-networks-part-3-56dfc256b690)
    中理解可转向 CNN 的工作原理。
- en: '*The elements:*'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '*元素：*'
- en: '**A space S**: The space on which the analysis takes place. Though S can extend
    to an arbitrary number of dimensions, it is easiest to visualize for two or three
    spatial dimensions. If for example we consider an image, the initial space is
    bidimensional and corresponds to the coordinate plane of the pixels (ℤ²). If we
    consider a “3D object” instead, the space S is tridimensional,ℤ³. A point *x*∈S
    identifies therefore a position.'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**一个空间 S**：分析发生的空间。虽然 S 可以扩展到任意数量的维度，但最容易在二维或三维空间中进行可视化。例如，如果我们考虑一幅图像，初始空间是二维的，对应于像素的坐标平面（ℤ²）。如果我们考虑一个“3D
    物体”，那么空间 S 是三维的，ℤ³。因此，一个点 *x*∈S 确定了一个位置。'
- en: '**An INPUT function *f*:** The function *f:* S → *F₀ =* ℝ ͨ which describes
    the input over our geometrical space (it can be a manifold or a vector field).
    This can be seen as a function from the space S to ℝ ͨ, *where each position x
    is connected to the “feature” f(x), also called* ***the fiber of f at x***. Let’s
    give some examples; a greyscale image can be seen as a function *f:* ℝ² → ℝ, with
    S=ℝ² *and* c=1\. If we consider a colored 3D manifold, the function will be *f:*
    ℝ³→ ℝ³, where each position is assigned an RGB color, S=ℝ³, c=3\.'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**一个输入函数 *f*:** 函数 *f:* S → *F₀ =* ℝ ͨ 描述了我们几何空间中的输入（它可以是流形或向量场）。这可以看作是从空间
    S 到 ℝ ͨ 的一个函数，*其中每个位置 x 与“特征” f(x) 相关联，也称为* ***x 点的 f 的纤维***。举些例子，一个灰度图像可以看作是一个函数
    *f:* ℝ² → ℝ，S=ℝ² *且* c=1。如果考虑一个彩色的 3D 流形，函数将是 *f:* ℝ³→ ℝ³，其中每个位置分配一个 RGB 颜色，S=ℝ³，c=3\。'
- en: In practice the function *f* is usually represented as a packed structure of
    *fibers* over some sampling space; for an image in the standard format the fibers
    would be regularly spaced horizontally and vertically (i.e. pixels). The function
    f constitutes the input layer of the neural network (see Fig. 2A, Fig. 2B). From
    now on, this starting layer will be called *F₀.*
  id: totrans-81
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 实际上，函数 *f* 通常表示为一些采样空间上的*纤维*的打包结构；对于标准格式的图像，纤维将水平和垂直地规则分布（即像素）。函数 f 构成了神经网络的输入层（见图
    2A，图 2B）。从现在起，这个起始层将被称为 *F₀*。
- en: '**A set of transformations G**: Once the objects of the analysis have been
    adequately defined, we can define the set of transformations the network should
    be equivariant to. A single transformation g∈G can be always described as a function
    in relation to the mathematical space on which it is applied. Given the input
    function *f:*S→ℝ ͨ*,* it is possible to characterize **π(g):** ℝ ͨ → ℝ ͨ, as “t*he
    induced transformation of g in* ℝ ͨ,”*.* The function *f exists in* ℝ ͨ, but the
    transformation g operates in S space. **π(g)** describes how *f* (in ℝ ͨ ) transforms
    under the application of g (in S). Considering *g* as a roto-translation composed
    of two components *r* (rotation) and translation *t*, in general, an input function
    *f(x)* transforms under the transformation g as described in Eq.7\.'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**一组变换 G**: 一旦分析对象被适当地定义，我们可以定义网络应该保持等变性的变换集。单个变换 g∈G 总是可以被描述为与应用它的数学空间相关的函数。给定输入函数
    *f:*S→ℝ ͨ*，* 可以表征 **π(g):** ℝ ͨ → ℝ ͨ，作为“*g 在 ℝ ͨ 中的诱导变换*”。*函数 *f 存在于 ℝ ͨ 中，但变换
    g 操作在 S 空间中。**π(g)** 描述了 *f*（在 ℝ ͨ 中）在应用 g（在 S 中）下的变换。考虑 *g* 作为由两个组件 *r*（旋转）和
    *t*（平移）组成的旋转-平移，一般来说，输入函数 *f(x)* 在变换 g 下的变换如 Eq.7 所述\。'
- en: In the image below, if f is a vector field ,**π(g)** is a matrix of dimension
    cxc while **,** If *f* is a scalar field ( *f :* ℝ² → ℝ ) , π(r) = 1.
  id: totrans-83
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在下图中，如果 f 是一个向量场，**π(g)** 是一个 cxc 维度的矩阵，而**，** 如果 *f* 是一个标量场（*f:* ℝ² → ℝ），π(r)
    = 1。
- en: The group G of the considered transformations are usually rotations (we will
    speak about ***SO(2)*** networks in this case) or even rotations + translations
    (we will speak about ***SE(2)***networks in this case). Similarly, on three-dimensional
    space, 3D rigid body motions are considered ( ***SO(3)*** or ***SE(3)***).
  id: totrans-84
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 所考虑的变换组 G 通常是旋转（在这种情况下我们将讨论***SO(2)*** 网络）或旋转 + 平移（在这种情况下我们将讨论 ***SE(2)*** 网络）。类似地，在三维空间中，考虑
    3D 刚体运动（***SO(3)*** 或 ***SE(3)***）。
- en: '![](../Images/128babacefc76ee3231664d01f052142.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/128babacefc76ee3231664d01f052142.png)'
- en: 'Fig 2B: graphical representation of the application of a transformation g on
    a scalar field (left) or on a vector field (right). Taken from paper [[3]](https://arxiv.org/abs/1911.08251).'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '图 2B: 变换 g 对标量场（左）或向量场（右）的应用的图形表示。摘自论文 [[3]](https://arxiv.org/abs/1911.08251)。'
- en: '![](../Images/64a14ddd551f8bc7c577b7ba069112f3.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/64a14ddd551f8bc7c577b7ba069112f3.png)'
- en: 'Eq.7: how f is transformed by the application of the transformation g to x'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 'Eq.7: f 如何通过变换 g 应用于 x 而被变换'
- en: '**Feature maps:** Following the definition of *f* given in the second point,
    the outputs of every layer of the neural network can be seen as the result of
    the application of a function *f ₙ* on the initial space S. Formally this can
    be represented as a function from S to the codomain space *Fₙ* , ( *f : S* → *Fₙ*),
    where *Fₙ*=ℝ ͨ ʿ*ⁿ* ʾ and c*ⁿ* is the number of features for the layer *n* .If
    we take fig. 2B as example we can see that the initial signal (input ) can be
    seen as a function *f :* S=ℝ² → *F₀*= ℝ³ while'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**特征图：** 根据第二点给出的 *f* 定义，神经网络每一层的输出可以看作是函数 *f ₙ* 在初始空间S上的应用结果。形式上可以表示为从S到对域空间
    *Fₙ* 的函数，（ *f : S* → *Fₙ*），其中 *Fₙ*=ℝ ͨ ʿ*ⁿ* ʾ 和 c*ⁿ* 是层 *n* 的特征数量。如果以图2B为例，我们可以看到初始信号（输入）可以看作是函数
    *f :* S=ℝ² → *F₀*= ℝ³。'
- en: '*f₁:* S=ℝ² → *F₁*= ℝ².'
  id: totrans-90
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*f₁:* S=ℝ² → *F₁*= ℝ²。'
- en: '**NN Filters φn**: A filter can be defined as a map between two contiguous
    layers in this way so **φ*:****Fₙ→ Fₙ₊₁.* The application of such a filter to
    a layer implies the convolution with the respective kernel *k*. How the convolution
    is defined in this case is crucial to the understanding of steerable NN. For this
    reason we dedicated a paragraph below.'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**NN滤波器 φn**: 滤波器可以定义为两个连续层之间的映射，如**φ*:****Fₙ→ Fₙ₊₁*。将这种滤波器应用于一层意味着与相应的内核*k*进行卷积。在这种情况下如何定义卷积对理解可导NN至关重要。因此，我们在下面专门讨论了这一点。'
- en: NN filter and convolution
  id: totrans-92
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: NN滤波器和卷积
- en: 'In this case, the kernel can be seen as a function *k: S →* ℝ ͨ ʿ*ⁿ* ʾ ˟ ͨ
    ʿ*ⁿ⁺ ¹* ʾ, where each position in *S is connected to a matrix of dimension c*ʿ*ⁿ*
    ʾ ˟ cʿ*ⁿ⁺ ¹* ʾ. For clarity, c*ⁿ* and c*ⁿ* ⁺ *¹* are respectively the dimension
    (number of features) of *Fₙ* and *Fₙ₊₁.*'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '在这种情况下，内核可以看作是一个函数 *k: S →* ℝ ͨ ʿ*ⁿ* ʾ ˟ ͨ ʿ*ⁿ⁺ ¹* ʾ，其中 *S 中的每个位置都连接到一个维度为
    c*ʿ*ⁿ* ʾ ˟ cʿ*ⁿ⁺ ¹* ʾ 的矩阵。为了清晰起见，c*ⁿ* 和 c*ⁿ* ⁺ *¹* 分别是 *Fₙ* 和 *Fₙ₊₁* 的维度（特征数量）。'
- en: '*We can define the convolution as following:*'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '*我们可以定义卷积如下：*'
- en: '![](../Images/bd6be8a28fb69390eda6f2f30eb0a5c4.png)![](../Images/2ff2e0ea7f32941591297fa3688147e4.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bd6be8a28fb69390eda6f2f30eb0a5c4.png)![](../Images/2ff2e0ea7f32941591297fa3688147e4.png)'
- en: 'Eq.8: Top: Relation which connects layer n and layer n+1\. Down: Definition
    of convolution in the space S'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 'Eq.8: 上方：连接层n和层n+1的关系。下方：空间S中的卷积定义'
- en: The top equation Eq.8 represents the function which connects the layer *n* and
    *n+1*; the one below is the definition of convolution in n-dimensional space S.
    The function σ*(x)* represents the non-linear function applied at the output of
    the convolution.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 上面的方程 Eq.8 代表连接层 *n* 和 *n+1* 的函数；下面的是n维空间S中的卷积定义。函数 σ*(x)* 代表应用于卷积输出的非线性函数。
- en: 'In Fig 2B, it is possible to see how, in a discrete domain, the convolution
    between the kernel and the input layer is calculated.Let’s illustrate this with
    a grayscale image denoted as *f ₀*: ℝ² -> ℝ. We can apply the filter discussed
    in Section 2, which was a steerable filter with the function'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '在图2B中，可以看到在离散域中，内核与输入层之间的卷积是如何计算的。我们用一个灰度图像 *f ₀*: ℝ² -> ℝ 来说明这一点。我们可以应用第2节中讨论的滤波器，这是一个具有函数的可导滤波器。'
- en: '*k(x₁, x₂)* being a 2D Gaussian filter defined as *k:* ℝ² -> ℝ¹˟¹=ℝ.'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '*k(x₁, x₂)* 是一个定义为 *k:* ℝ² -> ℝ¹˟¹=ℝ 的2D高斯滤波器。'
- en: '**In this case the application of the filter k on f*₀* is the classical 2D
    convolution and it can be written as:**'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '**在这种情况下，将滤波器k应用于*f*₀* 是经典的2D卷积，可以表示为：**'
- en: '![](../Images/53e163eb2326de5d1342bb6f4b99362f.png)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/53e163eb2326de5d1342bb6f4b99362f.png)'
- en: 'Eq.9: Definition of convolution'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 'Eq.9: 卷积的定义'
- en: 'Differently, in Fig. 2B you can see another example where *f ₀*: *ℝ²->* ℝ³(rgb
    image for example) and *f₁*: *ℝ²-> ℝ²* and *k₀: ℝ²->* ℝ³˟ *².*'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '不同的是，在图2B中，你可以看到另一个例子，其中 *f ₀*: *ℝ²->* ℝ³（例如rgb图像）和 *f₁*: *ℝ²-> ℝ²* 以及 *k₀:
    ℝ²->* ℝ³˟ *²*。'
- en: '![](../Images/edef4ef670f224a409fc844be0601fc4.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/edef4ef670f224a409fc844be0601fc4.png)'
- en: '**Fig 2B**: Visual example of filter convolution as defined above having S=R².
    F⁰ is the input space where the signal f⁰ exists, in this case R³. As possible
    to notice, the convolution operation has been substitute by correlation operation
    as suggested in [[4]](https://arxiv.org/abs/1911.08251)'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '**图2B**: 如上定义的滤波器卷积的视觉示例，S=R²。F⁰是信号f⁰存在的输入空间，在此案例中是R³。可以注意到，卷积操作已被相关操作替代，如[[4]](https://arxiv.org/abs/1911.08251)中所建议。'
- en: 'Combining all the points we’ve discussed so far, one can visualize a neural
    network within this formalism. Each individual feature map can be interpreted
    as a function *f: S → Fₙ*, where *Fₙ*= ℝʿⁿ ʾ and *f₀(x)* represents the network’s
    input. The filter’s application involves convolving it with its kernel function
    defined in Eq.8\. It’s worth noting that the main innovation thus far lies in
    the geometric representation of f as a function operating in a positional space
    S, and the definition of convolution within this space.'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '综合我们迄今讨论的所有要点，可以在这一形式化框架内可视化神经网络。每个单独的特征图可以被解释为一个函数 *f: S → Fₙ*，其中 *Fₙ*= ℝʿⁿ
    ʾ 和 *f₀(x)* 代表网络的输入。滤波器的应用涉及与其在Eq.8中定义的核函数卷积。值得注意的是，到目前为止，主要的创新在于将 *f* 作为在位置空间
    S 中操作的函数的几何表示，以及在这一空间内卷积的定义。'
- en: 'Below, we provide a representation of what a neural network in this context
    looks like:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是我们提供的神经网络在这一背景下的表示：
- en: '![](../Images/0080d2118a618cbfcbc3c6213d0a2c46.png)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0080d2118a618cbfcbc3c6213d0a2c46.png)'
- en: 'Eq.10: Symbolic representation of a neural network using the formalism expressed
    above.'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 'Eq.10: 使用上述形式化表达的神经网络的符号表示。'
- en: We will understand in the next [article](https://medium.com/@mat.cip43/a-gentle-introduction-to-steerable-neural-networks-part-3-56dfc256b690)
    how the definition of such formalism will help us in the design of a steerable
    CNN filter.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在下一篇[文章](https://medium.com/@mat.cip43/a-gentle-introduction-to-steerable-neural-networks-part-3-56dfc256b690)中了解这种形式化定义如何帮助我们设计可引导的CNN滤波器。
- en: Conclusions
  id: totrans-111
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: In this initial segment of our Steerable Neural Networks tutorial, we have established
    the fundamental concepts of Steerable Neural Networks, equivariance, and steerable
    filters. A mathematical framework has also been introduced to provide a rigorous
    foundation for understanding these concepts. Equivariance preserves behavior under
    transformations, while steerable filters adapt intelligently to input orientations.
    This groundwork paves the way for designing equivariant CNN filters, enhancing
    edge detection and orientation-based recognition. The next article will leverage
    these concepts for a deeper dive into Steerable CNN filters’ mechanics, completing
    our journey into this powerful neural network paradigm.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们《可引导神经网络》教程的初始部分，我们已经建立了可引导神经网络、等变性和可引导滤波器的基本概念。还介绍了一个数学框架，为理解这些概念提供了严格的基础。等变性在变换下保持行为不变，而可引导滤波器能够智能地适应输入的方向。这一基础工作为设计等变CNN滤波器铺平了道路，增强了边缘检测和基于方向的识别。下一篇文章将利用这些概念更深入地探讨可引导CNN滤波器的机制，完成我们对这一强大神经网络范式的探索。
- en: '✍️ 📄. About the authors:'
  id: totrans-113
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ✍️ 📄. 关于作者：
- en: '***1️⃣ Matteo Ciprian*,** Machine Learning Engineer/Researcher'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '***1️⃣ Matteo Ciprian***，机器学习工程师/研究员'
- en: MSc in Telecommunications Engineering at University of Padua. Currently working
    in the field of Sensor Fusion, Signal Processing and applied AI. Experience in
    projects related to AI applications in eHealth and wearable technologies (academic
    research and corporate domains). Specialized in developing Anomaly Detection algorithms,
    as well as advancing techniques in Deep Learning and Sensor Fusion.
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 帕多瓦大学电信工程硕士。当前从事传感器融合、信号处理和应用AI领域的工作。具有与AI在电子健康和可穿戴技术中的应用相关的项目经验（包括学术研究和企业领域）。专注于开发异常检测算法，以及推进深度学习和传感器融合技术。
- en: Passionate about Philosophy. Content creator in Youtube.
  id: totrans-116
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对哲学充满热情。YouTube内容创作者。
- en: '**🔗 Links:** 💼 [Linkedin](https://www.linkedin.com/in/matteo-ciprian-ba30ab122/)'
  id: totrans-117
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '**🔗 链接：** 💼 [Linkedin](https://www.linkedin.com/in/matteo-ciprian-ba30ab122/)'
- en: 📹 [Youtube](https://www.youtube.com/channel/UCF--7G3kkCmEsdPLm8wyPow)
  id: totrans-118
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 📹 [Youtube](https://www.youtube.com/channel/UCF--7G3kkCmEsdPLm8wyPow)
- en: 👨‍💻[Instagram](https://www.instagram.com/cip_mat/)
  id: totrans-119
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 👨‍💻[Instagram](https://www.instagram.com/cip_mat/)
- en: 2️⃣ ***Robert Schoonmaker*,** Signal Processing/Machine Learning Researcher
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 2️⃣ ***Robert Schoonmaker***，信号处理/机器学习研究员
- en: PhD in Computational Condensed Matter Physics from Durham University. Specializes
    in applied machine learning and nonlinear statistics, currently investigating
    the uses of GPU compute methods on synthetic aperture radar and similar systems.
    Experience includes developing symmetric ML methods for use in sensor fusion and
    positioning techniques.
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 杜伦大学计算凝聚态物理博士。专注于应用机器学习和非线性统计，目前研究GPU计算方法在合成孔径雷达及类似系统中的应用。经验包括开发用于传感器融合和定位技术的对称机器学习方法。
- en: '**🔗 Links:** 💼 [Linkedin](https://www.linkedin.com/in/robert-schoonmaker-951221b/)'
  id: totrans-122
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '**🔗 链接：** 💼 [Linkedin](https://www.linkedin.com/in/robert-schoonmaker-951221b/)'
