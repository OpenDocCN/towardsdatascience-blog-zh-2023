- en: Hands-On GenAI for Product & Engineering Leaders
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: é’ˆå¯¹äº§å“å’Œå·¥ç¨‹é¢†å¯¼è€…çš„åŠ¨æ‰‹ GenAI
- en: åŸæ–‡ï¼š[https://towardsdatascience.com/hands-on-genai-for-product-engineering-leaders-6ee6ad94e058?source=collection_archive---------10-----------------------#2023-11-28](https://towardsdatascience.com/hands-on-genai-for-product-engineering-leaders-6ee6ad94e058?source=collection_archive---------10-----------------------#2023-11-28)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡ï¼š[https://towardsdatascience.com/hands-on-genai-for-product-engineering-leaders-6ee6ad94e058?source=collection_archive---------10-----------------------#2023-11-28](https://towardsdatascience.com/hands-on-genai-for-product-engineering-leaders-6ee6ad94e058?source=collection_archive---------10-----------------------#2023-11-28)
- en: Make better product decisions by taking a peek under the hood of LLM-based products
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: é€šè¿‡æ·±å…¥äº†è§£åŸºäºLLMçš„äº§å“ï¼Œåšå‡ºæ›´å¥½çš„äº§å“å†³ç­–
- en: '[](https://medium.com/@ninadsohoni?source=post_page-----6ee6ad94e058--------------------------------)[![Ninad
    Sohoni](../Images/8d6ec40665bb85fb7b4ece99e6a40913.png)](https://medium.com/@ninadsohoni?source=post_page-----6ee6ad94e058--------------------------------)[](https://towardsdatascience.com/?source=post_page-----6ee6ad94e058--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----6ee6ad94e058--------------------------------)
    [Ninad Sohoni](https://medium.com/@ninadsohoni?source=post_page-----6ee6ad94e058--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@ninadsohoni?source=post_page-----6ee6ad94e058--------------------------------)[![Ninad
    Sohoni](../Images/8d6ec40665bb85fb7b4ece99e6a40913.png)](https://medium.com/@ninadsohoni?source=post_page-----6ee6ad94e058--------------------------------)[](https://towardsdatascience.com/?source=post_page-----6ee6ad94e058--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----6ee6ad94e058--------------------------------)
    [Ninad Sohoni](https://medium.com/@ninadsohoni?source=post_page-----6ee6ad94e058--------------------------------)'
- en: Â·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: Â·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F5ee93978501b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhands-on-genai-for-product-engineering-leaders-6ee6ad94e058&user=Ninad+Sohoni&userId=5ee93978501b&source=post_page-5ee93978501b----6ee6ad94e058---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----6ee6ad94e058--------------------------------)
    Â·35 min readÂ·Nov 28, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F6ee6ad94e058&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhands-on-genai-for-product-engineering-leaders-6ee6ad94e058&user=Ninad+Sohoni&userId=5ee93978501b&source=-----6ee6ad94e058---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[å…³æ³¨](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F5ee93978501b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhands-on-genai-for-product-engineering-leaders-6ee6ad94e058&user=Ninad+Sohoni&userId=5ee93978501b&source=post_page-5ee93978501b----6ee6ad94e058---------------------post_header-----------)
    å‘è¡¨åœ¨ [Towards Data Science](https://towardsdatascience.com/?source=post_page-----6ee6ad94e058--------------------------------)
    Â· 35 åˆ†é’Ÿé˜…è¯» Â· 2023å¹´11æœˆ28æ—¥ [](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F6ee6ad94e058&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhands-on-genai-for-product-engineering-leaders-6ee6ad94e058&user=Ninad+Sohoni&userId=5ee93978501b&source=-----6ee6ad94e058---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F6ee6ad94e058&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhands-on-genai-for-product-engineering-leaders-6ee6ad94e058&source=-----6ee6ad94e058---------------------bookmark_footer-----------)![](../Images/4b2f853b2bede6542039512f44e4c4c2.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F6ee6ad94e058&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhands-on-genai-for-product-engineering-leaders-6ee6ad94e058&source=-----6ee6ad94e058---------------------bookmark_footer-----------)![](../Images/4b2f853b2bede6542039512f44e4c4c2.png)'
- en: Image generated by [Bing Image Creator](https://www.bing.com/create) based on
    the prompt â€œproduct owner for a machine learning powered application working on
    a prototypeâ€
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ç‰‡ç”± [Bing Image Creator](https://www.bing.com/create) æ ¹æ®æç¤ºâ€œä¸ºæœºå™¨å­¦ä¹ é©±åŠ¨çš„åº”ç”¨ç¨‹åºå·¥ä½œä¸­çš„äº§å“æ‰€æœ‰è€…â€ç”Ÿæˆ
- en: Introduction
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ä»‹ç»
- en: If youâ€™re a regular driver, the hood of your car could be full of cotton for
    all you care. However, if you are anywhere in the design and execution chain responsible
    for building a better car, knowing what the different parts are and how they work
    together will help you build a better car.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœä½ æ˜¯ä¸€ä¸ªæ™®é€šå¸æœºï¼Œä½ å¯èƒ½ä¸åœ¨ä¹ä½ è½¦çš„å¼•æ“ç›–ä¸‹æ˜¯ä»€ä¹ˆã€‚ç„¶è€Œï¼Œå¦‚æœä½ æ˜¯è®¾è®¡å’Œæ‰§è¡Œé“¾æ¡ä¸­çš„ä¸€éƒ¨åˆ†ï¼Œè´Ÿè´£æ‰“é€ æ›´å¥½çš„æ±½è½¦ï¼Œäº†è§£ä¸åŒéƒ¨ä»¶æ˜¯ä»€ä¹ˆä»¥åŠå®ƒä»¬å¦‚ä½•ååŒå·¥ä½œï¼Œå°†å¸®åŠ©ä½ æ‰“é€ æ›´å¥½çš„æ±½è½¦ã€‚
- en: Similarly, as a product owner, business leader, or an engineer responsible for
    creating new Large Language Model (LLM) powered products, or for bringing LLMs
    / generative AI to existing products, an understanding of building blocks that
    go into an LLM-powered products will help you tackle strategic and tactical questions
    pertaining to technology, such as,
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: åŒæ ·ï¼Œä½œä¸ºäº§å“è´Ÿè´£äººã€ä¸šåŠ¡é¢†å¯¼æˆ–è´Ÿè´£åˆ›å»ºæ–°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é©±åŠ¨äº§å“çš„å·¥ç¨‹å¸ˆï¼Œæˆ–å°†LLM/ç”Ÿæˆæ€§AIå¼•å…¥ç°æœ‰äº§å“çš„å·¥ç¨‹å¸ˆï¼Œäº†è§£LLMé©±åŠ¨äº§å“çš„æ„å»ºæ¨¡å—å°†å¸®åŠ©ä½ è§£å†³ä¸æŠ€æœ¯ç›¸å…³çš„æˆ˜ç•¥å’Œæˆ˜æœ¯é—®é¢˜ï¼Œä¾‹å¦‚ï¼Œ
- en: Is our use case a good fit for LLM-powered solutions? Perhaps traditional analytics,
    supervised machine learning, or another approach is a better fit?
  id: totrans-12
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬çš„ä½¿ç”¨æ¡ˆä¾‹æ˜¯å¦é€‚åˆLLMé©±åŠ¨çš„è§£å†³æ–¹æ¡ˆï¼Ÿä¹Ÿè®¸ä¼ ç»Ÿçš„åˆ†æã€ç›‘ç£å¼æœºå™¨å­¦ä¹ æˆ–å…¶ä»–æ–¹æ³•æ›´åˆé€‚ï¼Ÿ
- en: If LLMs are the way to go, can our use case be addressed by an off-the-shelf
    product (say, ChatGPT Enterprise) now or in the near-future? Classic build-vs-buy
    decision.
  id: totrans-13
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å¦‚æœLLMæ˜¯å¯è¡Œçš„ï¼Œæˆ‘ä»¬çš„ä½¿ç”¨æ¡ˆä¾‹ç°åœ¨æˆ–åœ¨ä¸ä¹…çš„å°†æ¥æ˜¯å¦å¯ä»¥é€šè¿‡ç°æˆçš„äº§å“ï¼ˆæ¯”å¦‚ChatGPT Enterpriseï¼‰æ¥è§£å†³ï¼Ÿè¿™æ˜¯ç»å…¸çš„æ„å»ºä¸è´­ä¹°å†³ç­–ã€‚
- en: What are the different building blocks of our LLM-powered product? Which of
    these are commoditized, and which are likely to need more time to build and test?
  id: totrans-14
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬çš„LLMé©±åŠ¨äº§å“çš„ä¸åŒæ„å»ºæ¨¡å—æœ‰å“ªäº›ï¼Ÿå…¶ä¸­å“ªäº›å·²ç»å•†å“åŒ–ï¼Œå“ªäº›å¯èƒ½éœ€è¦æ›´å¤šæ—¶é—´æ¥æ„å»ºå’Œæµ‹è¯•ï¼Ÿ
- en: How do we measure the performance of our solution? What levers are available
    to improve the quality of outputs from our product?
  id: totrans-15
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¦‚ä½•è¡¡é‡è§£å†³æ–¹æ¡ˆçš„æ€§èƒ½ï¼Ÿæœ‰å“ªäº›æ æ†å¯ä»¥æé«˜æˆ‘ä»¬äº§å“è¾“å‡ºçš„è´¨é‡ï¼Ÿ
- en: Is our data quality acceptable for the use case? Are we organizing our data
    correctly, and passing relevant data to the LLM?
  id: totrans-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬çš„æ•°æ®è´¨é‡æ˜¯å¦ç¬¦åˆä½¿ç”¨æ¡ˆä¾‹çš„è¦æ±‚ï¼Ÿæˆ‘ä»¬æ˜¯å¦æ­£ç¡®åœ°ç»„ç»‡äº†æ•°æ®ï¼Œå¹¶å°†ç›¸å…³æ•°æ®ä¼ é€’ç»™äº†LLMï¼Ÿ
- en: Can we be confident that the LLMâ€™s responses will always be factually accurate.
    That is, will our solution â€˜hallucinateâ€™ when generating responses once in a while?
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬èƒ½å¦ç¡®ä¿¡LLMçš„å›ç­”å§‹ç»ˆæ˜¯äº‹å®å‡†ç¡®çš„ï¼Ÿä¹Ÿå°±æ˜¯è¯´ï¼Œæˆ‘ä»¬çš„è§£å†³æ–¹æ¡ˆæ˜¯å¦ä¼šåœ¨ç”Ÿæˆå›ç­”æ—¶å¶å°”å‡ºç°â€œå¹»è§‰â€ï¼Ÿ
- en: While these questions are answered later in the article, the objective with
    getting a little hands-on is to build an intuitive understanding of LLM-powered
    solutions, which should help you answer these questions on your own, or at least
    put you in a better position to research further.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: è™½ç„¶è¿™äº›é—®é¢˜åœ¨æ–‡ç« åé¢ä¼šå¾—åˆ°å›ç­”ï¼Œä½†é€šè¿‡åŠ¨æ‰‹å®è·µçš„ç›®æ ‡æ˜¯å»ºç«‹å¯¹LLMé©±åŠ¨è§£å†³æ–¹æ¡ˆçš„ç›´è§‚ç†è§£ï¼Œè¿™åº”è¯¥æœ‰åŠ©äºä½ è‡ªå·±å›ç­”è¿™äº›é—®é¢˜ï¼Œæˆ–è€…è‡³å°‘è®©ä½ æ›´å¥½åœ°è¿›è¡Œè¿›ä¸€æ­¥ç ”ç©¶ã€‚
- en: In a [previous article](/how-genai-solutions-revolutionize-business-automation-57747b0f11ce),
    I delved into some foundational concepts associated with building LLM-powered
    products. But you canâ€™t learn to drive just by reading blogs or watching videos
    â€” it requires you to get behind the wheel. Well, thanks to the age we live in,
    we have free-to-us tools (which [cost millions of dollars to create](https://lambdalabs.com/blog/demystifying-gpt-3))
    at our fingertips to build our own LLM solution in under an hour! So, in this
    article, I propose we do just that. Itâ€™s a much easier undertaking than learning
    to drive ğŸ˜.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä¸€ç¯‡[ä¸Šä¸€ç¯‡æ–‡ç« ](/how-genai-solutions-revolutionize-business-automation-57747b0f11ce)ä¸­ï¼Œæˆ‘**æ·±å…¥æ¢è®¨**äº†ä¸æ„å»ºLLMé©±åŠ¨äº§å“ç›¸å…³çš„ä¸€äº›åŸºç¡€æ¦‚å¿µã€‚ä½†ä½ ä¸èƒ½ä»…ä»…é€šè¿‡é˜…è¯»åšå®¢æˆ–è§‚çœ‹è§†é¢‘æ¥å­¦ä¼šé©¾é©¶â€”â€”è¿™éœ€è¦ä½ äº²è‡ªä¸Šè·¯ã€‚å¥½åœ¨æˆ‘ä»¬ç”Ÿæ´»çš„æ—¶ä»£æä¾›äº†å…è´¹çš„å·¥å…·ï¼ˆè¿™äº›å·¥å…·çš„åˆ›å»º[èŠ±è´¹äº†æ•°ç™¾ä¸‡ç¾å…ƒ](https://lambdalabs.com/blog/demystifying-gpt-3)ï¼‰ï¼Œæˆ‘ä»¬å¯ä»¥åœ¨ä¸åˆ°ä¸€å°æ—¶çš„æ—¶é—´é‡Œæ„å»ºè‡ªå·±çš„LLMè§£å†³æ–¹æ¡ˆï¼å› æ­¤ï¼Œåœ¨è¿™ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘å»ºè®®æˆ‘ä»¬å°±è¿™æ ·åšã€‚è¿™æ¯”å­¦ä¹ é©¾é©¶è¦å®¹æ˜“å¾—å¤šğŸ˜ã€‚
- en: '**Build a Chatbot that allows you to â€œchatâ€ with websites**'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '**æ„å»ºä¸€ä¸ªå…è®¸ä½ ä¸ç½‘ç«™â€œèŠå¤©â€çš„èŠå¤©æœºå™¨äºº**'
- en: 'Objective: Build a chatbot that answers questions based on information on a
    provided website, **to gain a better understanding of the building blocks** of
    popular GenAI solutions today'
  id: totrans-21
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ç›®æ ‡ï¼šæ„å»ºä¸€ä¸ªåŸºäºæä¾›çš„ç½‘ç«™ä¿¡æ¯å›ç­”é—®é¢˜çš„èŠå¤©æœºå™¨äººï¼Œ**ä»¥æ›´å¥½åœ°ç†è§£**å½“å‰æµè¡Œçš„GenAIè§£å†³æ–¹æ¡ˆçš„æ„å»ºæ¨¡å—
- en: We will create a question-answering chatbot that will answer questions based
    on information in a knowledge repository. This solution pattern, called Retrieval
    Augmented Generation (RAG), has become a go-to solution pattern in companies.
    One reason for the popularity of RAG is that rather than relying solely on the
    LLMs own knowledge, you can bring external information to the LLM in an automated
    manner. In real-world implementations, the external information can be from an
    organizationâ€™s own knowledge repository, holding proprietary information to enable
    the product to answer questions about the business, its products, business processes,
    etc. RAG also reduces LLM â€˜hallucinationsâ€™, in that the generated responses are
    grounded in the information provided to the LLM. According to a [recent talk](https://www.youtube.com/watch?v=xa7k9MUeIdk),
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†åˆ›å»ºä¸€ä¸ªåŸºäºçŸ¥è¯†åº“ä¿¡æ¯å›ç­”é—®é¢˜çš„é—®ç­”èŠå¤©æœºå™¨äººã€‚è¿™ç§è§£å†³æ–¹æ¡ˆæ¨¡å¼ï¼Œç§°ä¸ºæ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ï¼Œå·²æˆä¸ºå…¬å¸ä¸­çš„é¦–é€‰è§£å†³æ–¹æ¡ˆæ¨¡å¼ã€‚RAG ä¹‹æ‰€ä»¥å—æ¬¢è¿çš„ä¸€ä¸ªåŸå› æ˜¯ï¼Œå®ƒä¸ä»…ä¾èµ–äº
    LLM è‡ªèº«çš„çŸ¥è¯†ï¼Œè¿˜å¯ä»¥ä»¥è‡ªåŠ¨åŒ–çš„æ–¹å¼å°†å¤–éƒ¨ä¿¡æ¯å¸¦å…¥ LLMã€‚åœ¨å®é™…åº”ç”¨ä¸­ï¼Œå¤–éƒ¨ä¿¡æ¯å¯ä»¥æ¥è‡ªç»„ç»‡è‡ªå·±çš„çŸ¥è¯†åº“ï¼ŒåŒ…å«ä¸“æœ‰ä¿¡æ¯ï¼Œä»¥ä½¿äº§å“èƒ½å¤Ÿå›ç­”æœ‰å…³ä¸šåŠ¡ã€äº§å“ã€ä¸šåŠ¡æµç¨‹ç­‰é—®é¢˜ã€‚RAG
    è¿˜å‡å°‘äº† LLM çš„â€œå¹»è§‰â€ï¼Œå³ç”Ÿæˆçš„å“åº”æ˜¯åŸºäºæä¾›ç»™ LLM çš„ä¿¡æ¯çš„ã€‚æ ¹æ® [æœ€è¿‘çš„ä¸€æ¬¡æ¼”è®²](https://www.youtube.com/watch?v=xa7k9MUeIdk)ï¼Œ
- en: â€œRAG will the the default way enterprises use LLMsâ€
  id: totrans-23
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: â€œRAG å°†æ˜¯ä¼ä¸šä½¿ç”¨ LLM çš„é»˜è®¤æ–¹å¼â€
- en: -Dr. Waleed Kadous, Chief Scientist, AnyScale
  id: totrans-24
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: -Dr. Waleed Kadous, Chief Scientist, AnyScale
- en: For our hands-on exercise, we will let a user enter a website, which our solution
    will â€œreadâ€ into its knowledge repository. The solution will then be able to answer
    questions based on the information on the website. The website is a placeholder
    â€” in reality, this can be tweaked to consume text from any data source like PDFs,
    Excel, another product or internal system, etc. This approach works for other
    media â€” such as images â€” but they require a few different LLMs. For now, we will
    focus on text from websites.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æˆ‘ä»¬çš„å®æ“ç»ƒä¹ ä¸­ï¼Œæˆ‘ä»¬å°†å…è®¸ç”¨æˆ·è¾“å…¥ä¸€ä¸ªç½‘ç«™ï¼Œæˆ‘ä»¬çš„è§£å†³æ–¹æ¡ˆå°†â€œè¯»å–â€è¯¥ç½‘ç«™åˆ°å…¶çŸ¥è¯†åº“ä¸­ã€‚ç„¶åï¼Œè§£å†³æ–¹æ¡ˆå°†èƒ½å¤Ÿæ ¹æ®ç½‘ç«™ä¸Šçš„ä¿¡æ¯å›ç­”é—®é¢˜ã€‚è¿™ä¸ªç½‘ç«™æ˜¯ä¸€ä¸ªå ä½ç¬¦â€”â€”å®é™…ä¸Šï¼Œå¯ä»¥è°ƒæ•´ä¸ºä»ä»»ä½•æ•°æ®æºå¦‚
    PDFsã€Excelã€å…¶ä»–äº§å“æˆ–å†…éƒ¨ç³»ç»Ÿç­‰è·å–æ–‡æœ¬ã€‚è¿™ç§æ–¹æ³•ä¹Ÿé€‚ç”¨äºå…¶ä»–åª’ä½“â€”â€”å¦‚å›¾åƒâ€”â€”ä½†å®ƒä»¬éœ€è¦ä¸€äº›ä¸åŒçš„ LLMã€‚ç›®å‰ï¼Œæˆ‘ä»¬å°†é‡ç‚¹å…³æ³¨æ¥è‡ªç½‘ç«™çš„æ–‡æœ¬ã€‚
- en: 'For our example, we will use a sample book list webpage created for this blog:
    [Books Iâ€™d Pick Up â€” If There Were More Hours in the Day!](https://ninadsohoni.github.io/booklist/)
    You are welcome to use another website of your choice.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: ä½œä¸ºç¤ºä¾‹ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ä¸ºæœ¬åšå®¢åˆ›å»ºçš„ç¤ºä¾‹ä¹¦å•ç½‘é¡µï¼š[Books Iâ€™d Pick Up â€” If There Were More Hours in the
    Day!](https://ninadsohoni.github.io/booklist/) æ‚¨ä¹Ÿå¯ä»¥ä½¿ç”¨æ‚¨é€‰æ‹©çš„å…¶ä»–ç½‘ç«™ã€‚
- en: 'Hereâ€™s what our result will look like:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å°±æ˜¯æˆ‘ä»¬çš„ç»“æœçš„æ ·å­ï¼š
- en: '![](../Images/781fc874901946c9c92cb4c6ab0f0984.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/781fc874901946c9c92cb4c6ab0f0984.png)'
- en: LLM-powered chatbot to intelligently answer questions based on information on
    a website. (Image by the author)
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: LLM é©±åŠ¨çš„èŠå¤©æœºå™¨äººå¯ä»¥æ ¹æ®ç½‘ç«™ä¸Šçš„ä¿¡æ¯æ™ºèƒ½å›ç­”é—®é¢˜ã€‚ï¼ˆå›¾åƒç”±ä½œè€…æä¾›ï¼‰
- en: 'Here are the steps we will go through to build our solution:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: ä»¥ä¸‹æ˜¯æˆ‘ä»¬å°†éµå¾ªçš„æ­¥éª¤æ¥æ„å»ºæˆ‘ä»¬çš„è§£å†³æ–¹æ¡ˆï¼š
- en: 0\. Getting Set Up â€” Google Colaboratory & OpenAI API Key
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 0\. è®¾ç½® â€” Google Colaboratory å’Œ OpenAI API å¯†é’¥
- en: 1\. Create knowledge repository
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 1\. åˆ›å»ºçŸ¥è¯†åº“
- en: 2\. Search question-relevant context
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 2\. æœç´¢ä¸é—®é¢˜ç›¸å…³çš„ä¸Šä¸‹æ–‡
- en: 3\. Generate answer using LLM
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 3\. ä½¿ç”¨ LLM ç”Ÿæˆç­”æ¡ˆ
- en: 4\. Add â€œchatâ€ capability (optional)
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 4\. æ·»åŠ â€œèŠå¤©â€åŠŸèƒ½ï¼ˆå¯é€‰ï¼‰
- en: 5\. Add a simple pre-coded UI (optional)
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 5\. æ·»åŠ ä¸€ä¸ªç®€å•çš„é¢„ç¼–ç  UIï¼ˆå¯é€‰ï¼‰
- en: '**0.1\. Getting Set Up â€” Google Colaboratory & OpenAI API Key**'
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**0.1\. è®¾ç½® â€” Google Colaboratory å’Œ OpenAI API å¯†é’¥**'
- en: To build a LLM solution, we need a place to write and run code, and an LLM to
    generate responses to questions. We will use Google Colab for the code environment,
    and the model behind ChatGPT as our LLM.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: è¦æ„å»ºä¸€ä¸ª LLM è§£å†³æ–¹æ¡ˆï¼Œæˆ‘ä»¬éœ€è¦ä¸€ä¸ªç¼–å†™å’Œè¿è¡Œä»£ç çš„åœ°æ–¹ï¼Œä»¥åŠä¸€ä¸ªç”Ÿæˆé—®é¢˜å›ç­”çš„ LLMã€‚æˆ‘ä»¬å°†ä½¿ç”¨ Google Colab ä½œä¸ºä»£ç ç¯å¢ƒï¼Œå¹¶ä½¿ç”¨
    ChatGPT èƒŒåçš„æ¨¡å‹ä½œä¸ºæˆ‘ä»¬çš„ LLMã€‚
- en: Letâ€™s start with setting up [Google Colab](https://colab.google/), a free service
    by Google that enables running Python code in an easy-to-read format â€” no need
    to install anything on your computer. I find it convenient to add Colab to Google
    Drive so that I can later find Colab notebooks easily.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: é¦–å…ˆè®¾ç½® [Google Colab](https://colab.google/)ï¼Œè¿™æ˜¯ä¸€ä¸ªç”± Google æä¾›çš„å…è´¹æœåŠ¡ï¼Œå¯ä»¥ä»¥æ˜“äºé˜…è¯»çš„æ ¼å¼è¿è¡Œ
    Python ä»£ç  â€” æ— éœ€åœ¨è®¡ç®—æœºä¸Šå®‰è£…ä»»ä½•ä¸œè¥¿ã€‚æˆ‘å‘ç°å°† Colab æ·»åŠ åˆ° Google Drive ä¸­å¾ˆæ–¹ä¾¿ï¼Œè¿™æ ·æˆ‘å°±å¯ä»¥è½»æ¾æ‰¾åˆ° Colab ç¬”è®°æœ¬ã€‚
- en: To do so, navigate to **Google Drive** (using a browser) **> New > More > Connect
    More Apps >** Search **â€œColaboratoryâ€** in the Google Marketplace **> Install.**
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºæ­¤ï¼Œå¯¼èˆªè‡³ **Google Drive**ï¼ˆä½¿ç”¨æµè§ˆå™¨ï¼‰ **> æ–°å»º > æ›´å¤š > è¿æ¥æ›´å¤šåº”ç”¨ >** åœ¨ Google Marketplace
    ä¸­ **æœç´¢ â€œColaboratoryâ€** **> å®‰è£…ã€‚**
- en: To start using Colabobatory (â€œColabâ€), you can select **New** > **More** > **Google
    Colaboratory.** This will create a new notebook in your Google Drive so you can
    go back to it.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: è¦å¼€å§‹ä½¿ç”¨ Colabï¼Œä½ å¯ä»¥é€‰æ‹© **æ–°å»º** > **æ›´å¤š** > **Google Colaboratory**ã€‚è¿™å°†ä¼šåœ¨ä½ çš„ Google äº‘ç«¯ç¡¬ç›˜ä¸­åˆ›å»ºä¸€ä¸ªæ–°çš„ç¬”è®°æœ¬ï¼Œæ–¹ä¾¿ä½ è¿”å›ç»§ç»­ä½¿ç”¨ã€‚
- en: '![](../Images/ee514c154f891d1fd129b21195a698bd.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ee514c154f891d1fd129b21195a698bd.png)'
- en: Google Colaboratory accessible in Google Drive. (Image by the author)
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: Google Colaboratory å¯ä»¥åœ¨ Google äº‘ç«¯ç¡¬ç›˜ä¸­è®¿é—®ã€‚ï¼ˆå›¾ç‰‡ç”±ä½œè€…æä¾›ï¼‰
- en: Next, letâ€™s get access to an LLM. There are several open source and proprietary
    options available. While open source LLMs are free, powerful LLMs generally require
    powerful GPUs to process inputs and generate responses, and there is a nominal
    operate cost for GPUs. In our example, we will instead use OpenAIâ€™s service to
    use the LLM used by ChatGPT. To do so, you will require an API key, which is like
    a username/password rolled into one to let OpenAI know who is trying to access
    the LLM. As of this writing, OpenAI offered a $5 credit for new users, which should
    be sufficient for this hands-on tutorial. Here are the steps to get the API key,
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥ä¸‹æ¥ï¼Œè®©æˆ‘ä»¬è·å–å¯¹ LLM çš„è®¿é—®æƒé™ã€‚æœ‰å‡ ä¸ªå¼€æºå’Œä¸“æœ‰é€‰é¡¹å¯ä¾›é€‰æ‹©ã€‚è™½ç„¶å¼€æº LLM æ˜¯å…è´¹çš„ï¼Œä½†å¼ºå¤§çš„ LLM é€šå¸¸éœ€è¦å¼ºå¤§çš„ GPU æ¥å¤„ç†è¾“å…¥å’Œç”Ÿæˆå“åº”ï¼Œä¸”
    GPU çš„è¿è¡Œæˆæœ¬è¾ƒä½ã€‚åœ¨æˆ‘ä»¬çš„ç¤ºä¾‹ä¸­ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ OpenAI çš„æœåŠ¡æ¥ä½¿ç”¨ ChatGPT æ‰€ç”¨çš„ LLMã€‚ä¸ºæ­¤ï¼Œä½ éœ€è¦ä¸€ä¸ª API å¯†é’¥ï¼Œå®ƒç±»ä¼¼äºç”¨æˆ·å/å¯†ç çš„ç»„åˆï¼Œç”¨ä»¥è®©
    OpenAI çŸ¥é“æ˜¯è°åœ¨å°è¯•è®¿é—® LLMã€‚æ ¹æ®æ­¤æ—¶çš„ä¿¡æ¯ï¼ŒOpenAI ä¸ºæ–°ç”¨æˆ·æä¾›äº† $5 çš„ä¿¡ç”¨é¢åº¦ï¼Œè¶³ä»¥ç”¨äºæœ¬å®è·µæ•™ç¨‹ã€‚ä»¥ä¸‹æ˜¯è·å– API å¯†é’¥çš„æ­¥éª¤ï¼š
- en: Go to [**OpenAIâ€™s Platform website**](https://platform.openai.com/signup/)>
    **Get started > Sign up** with email & password or use Google or Microsoft account.
    You may also need a phone number to verification.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: è®¿é—®[**OpenAI å¹³å°ç½‘ç«™**](https://platform.openai.com/signup/)> **å¼€å§‹ä½¿ç”¨ > æ³¨å†Œ**ï¼Œä½¿ç”¨ç”µå­é‚®ä»¶å’Œå¯†ç è¿›è¡Œæ³¨å†Œï¼Œæˆ–ä½¿ç”¨
    Google æˆ– Microsoft å¸æˆ·æ³¨å†Œã€‚ä½ è¿˜å¯èƒ½éœ€è¦ä¸€ä¸ªç”µè¯å·ç è¿›è¡ŒéªŒè¯ã€‚
- en: Once logged in, click on your profile icon in the top right corner > **View
    API keys** > **Create new secret key**. The key will look something like the following
    (fake key for informational purposes only). Save it for use later.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: ç™»å½•åï¼Œç‚¹å‡»å³ä¸Šè§’çš„ä¸ªäººèµ„æ–™å›¾æ ‡ > **æŸ¥çœ‹ API å¯†é’¥** > **åˆ›å»ºæ–°å¯†é’¥**ã€‚å¯†é’¥å°†ç±»ä¼¼äºä»¥ä¸‹å†…å®¹ï¼ˆä»…ä¾›å‚è€ƒçš„å‡å¯†é’¥ï¼‰ã€‚è¯·ä¿å­˜ä»¥å¤‡åç”¨ã€‚
- en: '[PRE0]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Now, we are ready to build the solution.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ï¼Œæˆ‘ä»¬å·²ç»å‡†å¤‡å¥½æ„å»ºè§£å†³æ–¹æ¡ˆäº†ã€‚
- en: 0.2\. Prepare Notebook for Building Solution
  id: totrans-49
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 0.2\. å‡†å¤‡æ„å»ºè§£å†³æ–¹æ¡ˆçš„ç¬”è®°æœ¬
- en: 'We need to install some packages in the Colab environment to facilitate our
    solution. Just type the following code in the text box (called a â€œcellâ€) in Colab
    and press â€œShift + Return (enter)â€. Alternatively, just click the â€œplayâ€ button
    on the left of the cell or use the â€œRunâ€ menu at the top of the notebook. You
    may need to use the menu to insert new code cells for running subsequent code:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬éœ€è¦åœ¨ Colab ç¯å¢ƒä¸­å®‰è£…ä¸€äº›è½¯ä»¶åŒ…ä»¥æ–¹ä¾¿æˆ‘ä»¬çš„è§£å†³æ–¹æ¡ˆã€‚åªéœ€åœ¨ Colab ä¸­çš„æ–‡æœ¬æ¡†ï¼ˆç§°ä¸ºâ€œå•å…ƒæ ¼â€ï¼‰ä¸­è¾“å…¥ä»¥ä¸‹ä»£ç ï¼Œç„¶åæŒ‰â€œShift +
    Returnï¼ˆEnterï¼‰â€ã€‚æˆ–è€…ï¼Œç›´æ¥ç‚¹å‡»å•å…ƒæ ¼å·¦ä¾§çš„â€œæ’­æ”¾â€æŒ‰é’®æˆ–ä½¿ç”¨ç¬”è®°æœ¬é¡¶éƒ¨çš„â€œè¿è¡Œâ€èœå•ã€‚ä½ å¯èƒ½éœ€è¦ä½¿ç”¨èœå•æ’å…¥æ–°çš„ä»£ç å•å…ƒæ ¼ä»¥è¿è¡Œåç»­ä»£ç ï¼š
- en: '[PRE1]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Next, we should pull in code from the packages we installed so that the packages
    can be used in the code we write. You can use the new code cell and hit â€œShift
    + Returnâ€ again â€” and continue in this manner for each subsequent code block.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬åº”è¯¥ä»å·²å®‰è£…çš„è½¯ä»¶åŒ…ä¸­æå–ä»£ç ï¼Œä»¥ä¾¿åœ¨ç¼–å†™çš„ä»£ç ä¸­ä½¿ç”¨è¿™äº›è½¯ä»¶åŒ…ã€‚ä½ å¯ä»¥ä½¿ç”¨æ–°çš„ä»£ç å•å…ƒæ ¼å¹¶å†æ¬¡æŒ‰â€œShift + Returnâ€ â€” ä»¥è¿™ç§æ–¹å¼ç»§ç»­è¿›è¡Œæ¯ä¸ªåç»­çš„ä»£ç å—ã€‚
- en: '[PRE2]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Finally, add the OpenAI API key to a variable. Note that this key is like your
    password â€” do not share it. Also, do not share your Colab notebook without removing
    the API key first.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åï¼Œå°† OpenAI API å¯†é’¥æ·»åŠ åˆ°ä¸€ä¸ªå˜é‡ä¸­ã€‚è¯·æ³¨æ„ï¼Œè¿™ä¸ªå¯†é’¥ç±»ä¼¼äºä½ çš„å¯†ç  â€” è¯·å‹¿åˆ†äº«ã€‚æ­¤å¤–ï¼Œåœ¨åˆ†äº«ä½ çš„ Colab ç¬”è®°æœ¬å‰ï¼Œè¯·åŠ¡å¿…å…ˆåˆ é™¤
    API å¯†é’¥ã€‚
- en: '[PRE3]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Now we are ready to start building the solution. Here is a high-level view
    of the next steps:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬å‡†å¤‡å¼€å§‹æ„å»ºè§£å†³æ–¹æ¡ˆã€‚ä»¥ä¸‹æ˜¯æ¥ä¸‹æ¥æ­¥éª¤çš„é«˜çº§è§†å›¾ï¼š
- en: '![](../Images/f4773785ae6fd1047c8a8c724bb381bf.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f4773785ae6fd1047c8a8c724bb381bf.png)'
- en: Core steps to build the RAG solution (Image by the author)
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: æ„å»º RAG è§£å†³æ–¹æ¡ˆçš„æ ¸å¿ƒæ­¥éª¤ï¼ˆå›¾ç‰‡ç”±ä½œè€…æä¾›ï¼‰
- en: When coding, we will use LangChain, which has emerged as a popular framework
    to build solutions such as this one. It has packages for facilitating each of
    the steps from connecting to data sources to sending and receiving information
    from the LLM. LlamaIndex is another option to simplify building LLM-powered apps.
    While itâ€™s not strictly required to use LangChain (or LlamaIndex), and in some
    cases the high-level abstraction may make has the risk of leaving teams oblivious
    to whatâ€™s happening under the hood, we will use LangChain but still look under
    the hood often.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ç¼–ç æ—¶ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨LangChainï¼Œå®ƒå·²ç»æˆä¸ºæ„å»ºæ­¤ç±»è§£å†³æ–¹æ¡ˆçš„æµè¡Œæ¡†æ¶ã€‚å®ƒæœ‰åŠ©äºä»è¿æ¥æ•°æ®æºåˆ°å‘é€å’Œæ¥æ”¶LLMä¿¡æ¯çš„æ¯ä¸ªæ­¥éª¤ã€‚LlamaIndexæ˜¯å¦ä¸€ä¸ªç®€åŒ–æ„å»ºLLMé©±åŠ¨åº”ç”¨çš„é€‰é¡¹ã€‚è™½ç„¶å¹¶ä¸ä¸¥æ ¼è¦æ±‚ä½¿ç”¨LangChainï¼ˆæˆ–LlamaIndexï¼‰ï¼Œå¹¶ä¸”åœ¨æŸäº›æƒ…å†µä¸‹ï¼Œé«˜çº§æŠ½è±¡å¯èƒ½ä½¿å›¢é˜Ÿå¯¹å†…éƒ¨å‘ç”Ÿçš„äº‹æƒ…ä¸€æ— æ‰€çŸ¥ï¼Œä½†æˆ‘ä»¬å°†ä½¿ç”¨LangChainï¼Œä½†ä»ä¼šç»å¸¸æŸ¥çœ‹å†…éƒ¨æƒ…å†µã€‚
- en: Note that since the pace of innovation is so quick, it is likely that packages
    used in this code get updated, and some updates may cause the code to stop working
    unless updated accordingly. I do not intend to keep this code up-to-date. Nevertheless,
    the article is intended to serve as a demonstration, and the code could serve
    as reference, or a starting point that you may adapt to your needs.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: è¯·æ³¨æ„ï¼Œç”±äºåˆ›æ–°çš„é€Ÿåº¦å¦‚æ­¤ä¹‹å¿«ï¼Œå¯èƒ½ä¼šæœ‰ä»£ç ä¸­ä½¿ç”¨çš„åŒ…æ›´æ–°ï¼Œæœ‰äº›æ›´æ–°å¯èƒ½ä¼šå¯¼è‡´ä»£ç åœæ­¢å·¥ä½œï¼Œé™¤éç›¸åº”åœ°è¿›è¡Œæ›´æ–°ã€‚æˆ‘ä¸æ‰“ç®—ä¿æŒä»£ç çš„æœ€æ–°çŠ¶æ€ã€‚ç„¶è€Œï¼Œæœ¬æ–‡æ—¨åœ¨ä½œä¸ºæ¼”ç¤ºï¼Œä»£ç å¯ä»¥ä½œä¸ºå‚è€ƒæˆ–èµ·ç‚¹ï¼Œæ‚¨å¯ä»¥æ ¹æ®éœ€è¦è¿›è¡Œè°ƒæ•´ã€‚
- en: 1\. Create Knowledge Repository
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1\. åˆ›å»ºçŸ¥è¯†åº“
- en: '**1.1\. Identify & Read-in Documents** Letâ€™s access the book list and read
    the content into our Colab environment. The content is loaded as HTML originally,
    which is useful for web-browsers. However, we will convert it to a more human
    readable format using a HTML to Text convertor.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '**1.1\. ç¡®å®šå¹¶è¯»å–æ–‡æ¡£** è®©æˆ‘ä»¬è®¿é—®ä¹¦å•å¹¶å°†å†…å®¹è¯»å–åˆ°æˆ‘ä»¬çš„Colabç¯å¢ƒä¸­ã€‚å†…å®¹æœ€åˆä»¥HTMLæ ¼å¼åŠ è½½ï¼Œè¿™å¯¹ç½‘é¡µæµè§ˆå™¨å¾ˆæœ‰ç”¨ã€‚ç„¶è€Œï¼Œæˆ‘ä»¬å°†ä½¿ç”¨HTMLè½¬æ–‡æœ¬å·¥å…·å°†å…¶è½¬æ¢ä¸ºæ›´æ˜“è¯»çš„æ ¼å¼ã€‚'
- en: '[PRE4]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Here is what running the code generates on Google Colab:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: ä»¥ä¸‹æ˜¯è¿è¡Œä»£ç åœ¨Google Colabä¸Šç”Ÿæˆçš„å†…å®¹ï¼š
- en: '![](../Images/de6416c74c20602de4edbb11a39d23f5.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/de6416c74c20602de4edbb11a39d23f5.png)'
- en: Result of executing the code above . The website content is loaded into Colab
    environment. (Image by the author)
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰§è¡Œä¸Šè¿°ä»£ç çš„ç»“æœã€‚ç½‘ç«™å†…å®¹è¢«åŠ è½½åˆ°Colabç¯å¢ƒä¸­ã€‚ï¼ˆå›¾ç‰‡ç”±ä½œè€…æä¾›ï¼‰
- en: '**1.2\. Break Documents into Smaller Excerpts** There is one more step before
    we load the blogâ€™s information into our knowledge repository (which is essentially
    a database of our choice). The text should not be loaded into the database as-is.
    It should first be split into smaller chunks. This is for a few reasons:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '**1.2\. å°†æ–‡æ¡£åˆ†è§£ä¸ºè¾ƒå°çš„æ‘˜å½•** åœ¨æˆ‘ä»¬å°†åšå®¢çš„ä¿¡æ¯åŠ è½½åˆ°æˆ‘ä»¬çš„çŸ¥è¯†åº“ï¼ˆå³æˆ‘ä»¬é€‰æ‹©çš„æ•°æ®åº“ï¼‰ä¹‹å‰ï¼Œè¿˜æœ‰ä¸€æ­¥ã€‚æ–‡æœ¬ä¸åº”æŒ‰åŸæ ·åŠ è½½åˆ°æ•°æ®åº“ä¸­ã€‚å®ƒåº”é¦–å…ˆåˆ†å‰²æˆè¾ƒå°çš„å—ã€‚è¿™æœ‰å‡ ä¸ªåŸå› ï¼š'
- en: If our text is too long, it cannot be sent to the LLM due to exceeding the text
    length threshold (known as *â€œcontext sizeâ€*).
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å¦‚æœæˆ‘ä»¬çš„æ–‡æœ¬å¤ªé•¿ï¼Œç”±äºè¶…å‡ºæ–‡æœ¬é•¿åº¦é˜ˆå€¼ï¼ˆå³*â€œä¸Šä¸‹æ–‡å¤§å°â€*ï¼‰ï¼Œåˆ™ä¸èƒ½å‘é€ç»™LLMã€‚
- en: Longer text might have broad, loosely related information. We would be relying
    on the LLM to pick out the relevant portions â€” and this might not always work
    out as expected. With smaller chunks, we could use retrieval mechanisms to identify
    only the relevant pieces of information to send to the LLM, as we will see later.
  id: totrans-69
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: è¾ƒé•¿çš„æ–‡æœ¬å¯èƒ½åŒ…å«å¹¿æ³›ä¸”æ¾æ•£ç›¸å…³çš„ä¿¡æ¯ã€‚æˆ‘ä»¬å°†ä¾èµ–LLMæŒ‘é€‰å‡ºç›¸å…³éƒ¨åˆ†â€”â€”è¿™å¯èƒ½ä¸ä¼šæ€»æ˜¯å¦‚é¢„æœŸé‚£æ ·æœ‰æ•ˆã€‚ä½¿ç”¨è¾ƒå°çš„å—ï¼Œæˆ‘ä»¬å¯ä»¥åˆ©ç”¨æ£€ç´¢æœºåˆ¶æ¥è¯†åˆ«ä»…ç›¸å…³çš„ä¿¡æ¯å¹¶å‘é€ç»™LLMï¼Œå¦‚æˆ‘ä»¬åé¢å°†çœ‹åˆ°çš„ã€‚
- en: LLMs are prone to have stronger attention at the beginning and end of text,
    so longer chunks could lead the LLM to pay less attention to more content later
    (known as *â€œlost in the middleâ€*).
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: LLMå¯¹æ–‡æœ¬çš„å¼€å§‹å’Œç»“æŸéƒ¨åˆ†å…³æ³¨è¾ƒå¼ºï¼Œå› æ­¤è¾ƒé•¿çš„å—å¯èƒ½å¯¼è‡´LLMå¯¹åé¢æ›´å¤šçš„å†…å®¹å…³æ³¨è¾ƒå°‘ï¼ˆç§°ä¸º*â€œåœ¨ä¸­é—´è¿·å¤±â€*ï¼‰ã€‚
- en: The right chunk sizes for each use case will vary per the specifics of the use
    case, including the type of content, the LLM being used, and other factors. It
    is prudent to experiment with different chunk sizes and evaluate response quality
    before finalizing the solution. For this demonstration, letâ€™s use context-aware
    splitting where each book recommendation from the list gets its own chunk,
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: æ¯ä¸ªç”¨ä¾‹çš„é€‚å½“å—å¤§å°å°†æ ¹æ®ç”¨ä¾‹çš„å…·ä½“æƒ…å†µè€Œæœ‰æ‰€ä¸åŒï¼ŒåŒ…æ‹¬å†…å®¹ç±»å‹ã€ä½¿ç”¨çš„LLMåŠå…¶ä»–å› ç´ ã€‚æ˜æ™ºçš„åšæ³•æ˜¯å°è¯•ä¸åŒçš„å—å¤§å°å¹¶åœ¨ç¡®å®šè§£å†³æ–¹æ¡ˆä¹‹å‰è¯„ä¼°å“åº”è´¨é‡ã€‚åœ¨æœ¬æ¼”ç¤ºä¸­ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„åˆ†å—ï¼Œå…¶ä¸­ä¹¦å•ä¸­çš„æ¯ä¸ªä¹¦ç±æ¨èéƒ½æœ‰è‡ªå·±çš„å—ã€‚
- en: '[PRE5]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '![](../Images/ca21a3be9e3be635cf3f4fdb21f09bd4.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ca21a3be9e3be635cf3f4fdb21f09bd4.png)'
- en: One of many document chunks as a result of splitting the original content. (Image
    by the author)
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: åŸå§‹å†…å®¹æ‹†åˆ†åçš„ä¼—å¤šæ–‡æ¡£å—ä¹‹ä¸€ã€‚ï¼ˆå›¾ç‰‡ç”±ä½œè€…æä¾›ï¼‰
- en: Note that if the chunks created so far are still longer than desired, they can
    be split further using other text splitting algorithms, easily available via LangChain
    or LlamaIndex. For example, each bookâ€™s review could be split into paragraphs,
    if needed.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: è¯·æ³¨æ„ï¼Œå¦‚æœè¿„ä»Šä¸ºæ­¢åˆ›å»ºçš„æ–‡æœ¬å—ä»ç„¶æ¯”æ‰€éœ€çš„é•¿åº¦é•¿ï¼Œå¯ä»¥ä½¿ç”¨å…¶ä»–æ–‡æœ¬æ‹†åˆ†ç®—æ³•è¿›ä¸€æ­¥æ‹†åˆ†ï¼Œè¿™äº›ç®—æ³•å¯ä»¥é€šè¿‡ LangChain æˆ– LlamaIndex
    è½»æ¾è·å¾—ã€‚ä¾‹å¦‚ï¼Œæ¯æœ¬ä¹¦çš„è¯„è®ºå¯ä»¥æ ¹æ®éœ€è¦æ‹†åˆ†ä¸ºæ®µè½ã€‚
- en: '**1.3\. Load Excerpts into Knowledge Repository** The text chunks are now ready
    to be loaded to the knowledge repository. These are first passed through an embedding
    model to convert the text to a series of numbers that capture the meaning of the
    text. Then the actual text along with the numerical representation (i.e., embeddings)
    will be loaded to the vector database â€” our knowledge repository. Note that embeddings
    are also generated by LLMs, just a different kind than the chat LLM. If you wish
    to read more about embeddings, the [previous article](/how-genai-solutions-revolutionize-business-automation-57747b0f11ce)
    demonstrates the concept using examples.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '**1.3\. å°†æ‘˜å½•åŠ è½½åˆ°çŸ¥è¯†åº“** æ–‡æœ¬å—ç°åœ¨å‡†å¤‡å¥½åŠ è½½åˆ°çŸ¥è¯†åº“ä¸­ã€‚è¿™äº›æ–‡æœ¬å—é¦–å…ˆé€šè¿‡åµŒå…¥æ¨¡å‹è½¬æ¢ä¸ºä¸€ç³»åˆ—æ•æ‰æ–‡æœ¬æ„ä¹‰çš„æ•°å­—ã€‚ç„¶åï¼Œå®é™…çš„æ–‡æœ¬åŠå…¶æ•°å€¼è¡¨ç¤ºï¼ˆå³åµŒå…¥ï¼‰å°†åŠ è½½åˆ°å‘é‡æ•°æ®åº“â€”â€”æˆ‘ä»¬çš„çŸ¥è¯†åº“ä¸­ã€‚è¯·æ³¨æ„ï¼ŒåµŒå…¥ä¹Ÿç”±å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ç”Ÿæˆï¼Œåªæ˜¯ä¸èŠå¤©
    LLM çš„ç±»å‹ä¸åŒã€‚å¦‚æœä½ æƒ³äº†è§£æ›´å¤šå…³äºåµŒå…¥çš„å†…å®¹ï¼Œ[ä¸Šä¸€ç¯‡æ–‡ç« ](/how-genai-solutions-revolutionize-business-automation-57747b0f11ce)é€šè¿‡ç¤ºä¾‹å±•ç¤ºäº†è¿™ä¸€æ¦‚å¿µã€‚'
- en: We will use a vector database to store all the information. This will manifest
    our knowledge repository. Vector databases are purpose-built to enable searching
    by embedding similarity. If we want to search something from the database, the
    search term is converted to a numerical representation by running it through the
    embedding model first, and then the question embeddings are compared to all of
    the embeddings in the database. Records (in our case, text chunks about each book
    on the list) that are closest to the question embeddings are returned as search
    results, as long as they clear a threshold.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†ä½¿ç”¨å‘é‡æ•°æ®åº“æ¥å­˜å‚¨æ‰€æœ‰ä¿¡æ¯ã€‚è¿™å°†å®ç°æˆ‘ä»¬çš„çŸ¥è¯†åº“ã€‚å‘é‡æ•°æ®åº“æ˜¯ä¸“é—¨è®¾è®¡ç”¨äºé€šè¿‡åµŒå…¥ç›¸ä¼¼æ€§è¿›è¡Œæœç´¢çš„ã€‚å¦‚æœæˆ‘ä»¬æƒ³ä»æ•°æ®åº“ä¸­æœç´¢æŸäº›å†…å®¹ï¼Œæœç´¢è¯é¦–å…ˆé€šè¿‡åµŒå…¥æ¨¡å‹è½¬æ¢ä¸ºæ•°å€¼è¡¨ç¤ºï¼Œç„¶åå°†é—®é¢˜çš„åµŒå…¥ä¸æ•°æ®åº“ä¸­çš„æ‰€æœ‰åµŒå…¥è¿›è¡Œæ¯”è¾ƒã€‚ä¸é—®é¢˜åµŒå…¥æœ€æ¥è¿‘çš„è®°å½•ï¼ˆåœ¨æˆ‘ä»¬çš„ä¾‹å­ä¸­ï¼Œå°±æ˜¯å…³äºæ¯æœ¬ä¹¦çš„æ–‡æœ¬å—ï¼‰ä½œä¸ºæœç´¢ç»“æœè¿”å›ï¼Œåªè¦å®ƒä»¬è¶…è¿‡äº†ä¸€ä¸ªé˜ˆå€¼ã€‚
- en: '[PRE6]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '![](../Images/06255fabddafae0036919bdeaa6d2fdf.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/06255fabddafae0036919bdeaa6d2fdf.png)'
- en: A view of the first few text chunks loaded to the vector DB along with numerical
    representations (i.e., embeddings). (Image by the author)
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: åŠ è½½åˆ°å‘é‡æ•°æ®åº“ä¸­çš„å‰å‡ ä¸ªæ–‡æœ¬å—åŠå…¶æ•°å€¼è¡¨ç¤ºï¼ˆå³åµŒå…¥ï¼‰çš„è§†å›¾ã€‚ï¼ˆå›¾ç‰‡ç”±ä½œè€…æä¾›ï¼‰
- en: 2\. Search Question-Relevant Context
  id: totrans-81
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2\. æœç´¢é—®é¢˜ç›¸å…³çš„ä¸Šä¸‹æ–‡
- en: We ultimately want our solution to pick out relevant information from our vector
    DB knowledge corpus and pass it along to the LLM along with the question we want
    the LLM to answer. Letâ€™s try out the vector DB search, by asking the question
    â€œCan you recommend a few detective novels?â€
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬çš„ç»ˆæç›®æ ‡æ˜¯è®©æˆ‘ä»¬çš„è§£å†³æ–¹æ¡ˆä»å‘é‡æ•°æ®åº“çŸ¥è¯†åº“ä¸­æŒ‘é€‰ç›¸å…³ä¿¡æ¯ï¼Œå¹¶å°†å…¶ä¸æˆ‘ä»¬å¸Œæœ› LLM å›ç­”çš„é—®é¢˜ä¸€èµ·ä¼ é€’ç»™ LLMã€‚è®©æˆ‘ä»¬å°è¯•ä¸€ä¸‹å‘é‡æ•°æ®åº“æœç´¢ï¼Œè¯¢é—®é—®é¢˜â€œä½ èƒ½æ¨èå‡ æœ¬ä¾¦æ¢å°è¯´å—ï¼Ÿâ€
- en: '[PRE7]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '![](../Images/15bebbaf8b218ae61b888fe54953a1f3.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/15bebbaf8b218ae61b888fe54953a1f3.png)'
- en: Top search results for the question â€œCan you recommend a few detective novels?â€
    (Image by the author)
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: é—®é¢˜â€œä½ èƒ½æ¨èå‡ æœ¬ä¾¦æ¢å°è¯´å—ï¼Ÿâ€çš„æœç´¢ç»“æœå‰ 4 åï¼ˆå›¾ç‰‡ç”±ä½œè€…æä¾›ï¼‰
- en: We get top 4 results by default, unless we explicitly set the value to a different
    number. In this example, the top search, which is a Sherlock Holmes novel, mentions
    the term â€˜detectiveâ€™ directly. The second result (*The Day of the Jackal*) does
    not have the term â€˜detectiveâ€™, but mentions â€˜police agenciesâ€™ and â€˜uncover the
    plotâ€™, which bear semantic association with â€œdetective novelsâ€. The third result
    (*The Undercover Economist*) mentions the term â€˜undercoverâ€™, though it is about
    economics. I believe the last result was fetched due to its association with novels
    / books rather than â€œdetective novelsâ€ specifically, because four results were
    requested.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: é»˜è®¤æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬å¾—åˆ°å‰ 4 ä¸ªç»“æœï¼Œé™¤éæˆ‘ä»¬æ˜ç¡®è®¾ç½®ä¸åŒçš„å€¼ã€‚åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œæ’åç¬¬ä¸€çš„ç»“æœæ˜¯ä¸€æœ¬ç¦å°”æ‘©æ–¯å°è¯´ï¼Œç›´æ¥æåˆ°äº†â€œä¾¦æ¢â€ä¸€è¯ã€‚ç¬¬äºŒä¸ªç»“æœï¼ˆ*ã€Šæ°å…‹å°”çš„æ—¥å­ã€‹*ï¼‰è™½ç„¶æ²¡æœ‰â€œä¾¦æ¢â€ä¸€è¯ï¼Œä½†æåˆ°äº†â€œè­¦å¯Ÿæœºæ„â€å’Œâ€œæ­éœ²é˜´è°‹â€ï¼Œè¿™äº›ä¸â€œä¾¦æ¢å°è¯´â€åœ¨è¯­ä¹‰ä¸Šç›¸å…³ã€‚ç¬¬ä¸‰ä¸ªç»“æœï¼ˆ*ã€Šå§åº•ç»æµå­¦å®¶ã€‹*ï¼‰æåˆ°äº†â€œå§åº•â€ä¸€è¯ï¼Œå°½ç®¡å®ƒæ˜¯å…³äºç»æµå­¦çš„ã€‚æˆ‘è®¤ä¸ºæœ€åä¸€ä¸ªç»“æœè¢«è·å–æ˜¯å› ä¸ºå®ƒä¸å°è¯´/ä¹¦ç±æœ‰å…³ï¼Œè€Œä¸æ˜¯ç‰¹å®šçš„â€œä¾¦æ¢å°è¯´â€ï¼Œå› ä¸ºè¯·æ±‚äº†å››ä¸ªç»“æœã€‚
- en: Also, it is not strictly necessary to use a vector DB. You could load embeddings
    and facilitate search in other forms of storage. â€œNormalâ€ relational databases
    or even Excel can be used. But you would have to handle the â€œsimilarityâ€ calculation,
    which can be a dot product when using OpenAI embeddings, in your application logic.
    On the other hand, a vector DB does that for you.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: åŒæ ·ï¼Œä½¿ç”¨å‘é‡æ•°æ®åº“å¹¶ä¸æ˜¯ç»å¯¹å¿…è¦çš„ã€‚æ‚¨å¯ä»¥åŠ è½½åµŒå…¥å¹¶åœ¨å…¶ä»–å­˜å‚¨å½¢å¼ä¸­è¿›è¡Œæœç´¢ã€‚â€œæ™®é€šâ€å…³ç³»æ•°æ®åº“ç”šè‡³ Excel éƒ½å¯ä»¥ä½¿ç”¨ã€‚ä½†æ‚¨éœ€è¦åœ¨åº”ç”¨ç¨‹åºé€»è¾‘ä¸­å¤„ç†â€œç›¸ä¼¼æ€§â€è®¡ç®—ï¼Œå½“ä½¿ç”¨
    OpenAI åµŒå…¥æ—¶ï¼Œè¿™å¯ä»¥æ˜¯ç‚¹ç§¯ã€‚å¦ä¸€æ–¹é¢ï¼Œå‘é‡æ•°æ®åº“ä¸ºæ‚¨å¤„ç†äº†è¿™äº›ã€‚
- en: Note that if we wanted to pre-filter some search results by metadata, we could
    do so. For our demonstration, letâ€™s filter by the genre, which is under â€œHeader
    2â€ in the metadata we loaded from the book list.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: è¯·æ³¨æ„ï¼Œå¦‚æœæˆ‘ä»¬æƒ³é€šè¿‡å…ƒæ•°æ®é¢„ç­›é€‰ä¸€äº›æœç´¢ç»“æœï¼Œæˆ‘ä»¬å¯ä»¥è¿™æ ·åšã€‚ä¸ºäº†æ¼”ç¤ºï¼Œæˆ‘ä»¬å°†æ ¹æ®å…ƒæ•°æ®ä¸­ä»ä¹¦å•åŠ è½½çš„â€œHeader 2â€è¿‡æ»¤ã€‚
- en: '[PRE8]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '![](../Images/3e7381e8da847e76b51f202714d7e319.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3e7381e8da847e76b51f202714d7e319.png)'
- en: Search results based on applying a metadata pre-filter, showing only key columns.
    (Image by the author)
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: åŸºäºåº”ç”¨å…ƒæ•°æ®é¢„ç­›é€‰çš„æœç´¢ç»“æœï¼Œä»…æ˜¾ç¤ºå…³é”®åˆ—ã€‚ï¼ˆå›¾åƒç”±ä½œè€…æä¾›ï¼‰
- en: An interesting opportunity offered by LLMs is to use the LLM itself to inspect
    a user question, review available metadata, assess whether a metadata-based pre-filter
    is required and possible, and formulate the pre-filter query code, which can then
    be used on the vector DB to actually pre-filter data. See LangChainâ€™s [self-query
    retriever](https://python.langchain.com/docs/modules/data_connection/retrievers/self_query/)
    for more information about this.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: LLM æä¾›äº†ä¸€ä¸ªæœ‰è¶£çš„æœºä¼šï¼Œå³åˆ©ç”¨ LLM æœ¬èº«æ¥æ£€æŸ¥ç”¨æˆ·é—®é¢˜ï¼Œå®¡æŸ¥å¯ç”¨çš„å…ƒæ•°æ®ï¼Œè¯„ä¼°æ˜¯å¦éœ€è¦å’Œå¯èƒ½è¿›è¡ŒåŸºäºå…ƒæ•°æ®çš„é¢„ç­›é€‰ï¼Œå¹¶åˆ¶å®šé¢„ç­›é€‰æŸ¥è¯¢ä»£ç ï¼Œè¿™äº›ä»£ç å¯ä»¥åœ¨å‘é‡æ•°æ®åº“ä¸Šå®é™…é¢„ç­›é€‰æ•°æ®ã€‚æœ‰å…³æ›´å¤šä¿¡æ¯ï¼Œè¯·å‚è§
    LangChain çš„ [è‡ªæŸ¥è¯¢æ£€ç´¢å™¨](https://python.langchain.com/docs/modules/data_connection/retrievers/self_query/)ã€‚
- en: 3\. Generate Answer using LLM
  id: totrans-93
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3\. ä½¿ç”¨ LLM ç”Ÿæˆç­”æ¡ˆ
- en: Next, we will add instructions to the LLM that basically say â€œI am going to
    give you some information snippets, and a question. Please answer the question
    using the provided information snippetsâ€. Then, we bundle these instructions,
    the search results from the vector DB, and our question into a packet and send
    it to the LLM to respond. All these steps are facilitated by the following code.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å°†å‘ LLM æ·»åŠ æŒ‡ä»¤ï¼ŒåŸºæœ¬ä¸Šæ˜¯è¯´â€œæˆ‘å°†ç»™ä½ ä¸€äº›ä¿¡æ¯ç‰‡æ®µå’Œä¸€ä¸ªé—®é¢˜ã€‚è¯·ä½¿ç”¨æä¾›çš„ä¿¡æ¯ç‰‡æ®µå›ç­”é—®é¢˜â€ã€‚ç„¶åï¼Œæˆ‘ä»¬å°†è¿™äº›æŒ‡ä»¤ã€å‘é‡æ•°æ®åº“ä¸­çš„æœç´¢ç»“æœå’Œæˆ‘ä»¬çš„é—®é¢˜æ‰“åŒ…ï¼Œå¹¶å‘é€ç»™
    LLM è¿›è¡Œå›åº”ã€‚æ‰€æœ‰è¿™äº›æ­¥éª¤éƒ½ç”±ä»¥ä¸‹ä»£ç å®Œæˆã€‚
- en: Note that LangChain offers the opportunity to abstract some of this code, so
    your code doesnâ€™t have to be as verbose as the code that follows. However, the
    objective with the code below is to showcase the instructions sent to the language
    model. Hereâ€™s where you can customize them too â€” like in this case, the default
    instructions are changed to request the LLM to keep responses as concise as possible.
    If the default works for your use case, your code can skip the question template
    part altogether and LangChain will use the default prompt from its own package
    when sending a request to the LLM.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: è¯·æ³¨æ„ï¼ŒLangChain æä¾›äº†æŠ½è±¡ä¸€äº›ä»£ç çš„æœºä¼šï¼Œå› æ­¤æ‚¨çš„ä»£ç ä¸å¿…åƒä»¥ä¸‹ä»£ç é‚£æ ·å†—é•¿ã€‚ç„¶è€Œï¼Œä»¥ä¸‹ä»£ç çš„ç›®çš„æ˜¯å±•ç¤ºå‘é€åˆ°è¯­è¨€æ¨¡å‹çš„æŒ‡ä»¤ã€‚è¿™é‡Œä¹Ÿæ˜¯è‡ªå®šä¹‰è¿™äº›æŒ‡ä»¤çš„åœ°æ–¹â€”â€”ä¾‹å¦‚ï¼Œåœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œé»˜è®¤æŒ‡ä»¤è¢«æ›´æ”¹ä¸ºè¯·æ±‚
    LLM å°½å¯èƒ½ç®€æ´åœ°å›åº”ã€‚å¦‚æœé»˜è®¤è®¾ç½®é€‚ç”¨äºæ‚¨çš„ç”¨ä¾‹ï¼Œæ‚¨çš„ä»£ç å¯ä»¥å®Œå…¨è·³è¿‡é—®é¢˜æ¨¡æ¿éƒ¨åˆ†ï¼ŒLangChain ä¼šåœ¨å‘ LLM å‘é€è¯·æ±‚æ—¶ä½¿ç”¨å…¶åŒ…ä¸­çš„é»˜è®¤æç¤ºã€‚
- en: '[PRE9]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Now, letâ€™s ask for detective novel recommendations again and see what we get
    as a response.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ï¼Œè®©æˆ‘ä»¬å†æ¬¡è¯·æ±‚ä¾¦æ¢å°è¯´çš„æ¨èï¼Œçœ‹çœ‹æˆ‘ä»¬ä¼šå¾—åˆ°ä»€ä¹ˆå›åº”ã€‚
- en: '[PRE10]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '![](../Images/546144d38d406b1c88712edd0333f838.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/546144d38d406b1c88712edd0333f838.png)'
- en: Response from the solution recommending detective novels (Image by the author)
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: æ¨èä¾¦æ¢å°è¯´çš„è§£å†³æ–¹æ¡ˆå›åº”ï¼ˆå›¾åƒç”±ä½œè€…æä¾›ï¼‰
- en: Letâ€™s confirm whether the model reviewed all four of our previous search results
    from the vector DB, or did it just get the two results noted in the response?
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬ç¡®è®¤æ¨¡å‹æ˜¯å¦å›é¡¾äº†æˆ‘ä»¬ä»å‘é‡æ•°æ®åº“ä¸­è·å¾—çš„æ‰€æœ‰å››ä¸ªæœç´¢ç»“æœï¼Œè¿˜æ˜¯ä»…ä»…è·å–äº†å“åº”ä¸­æåˆ°çš„ä¸¤ä¸ªç»“æœï¼Ÿ
- en: '[PRE11]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '![](../Images/5f951dd421f7adbca2edefd17e925e95.png)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5f951dd421f7adbca2edefd17e925e95.png)'
- en: What was passed in as context to the LLM along with the question, to facilitate
    the response. (Image by the author)
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸é—®é¢˜ä¸€èµ·ä¼ é€’ç»™ LLM çš„ä¸Šä¸‹æ–‡ï¼Œä»¥ä¾¿äºå“åº”ã€‚ï¼ˆå›¾åƒç”±ä½œè€…æä¾›ï¼‰
- en: We can see that the LLM still had access to all four search results and reasoned
    that only the first two books were detective novels.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼ŒLLM ä»ç„¶è®¿é—®äº†æ‰€æœ‰å››ä¸ªæœç´¢ç»“æœï¼Œå¹¶æ¨æ–­å‡ºåªæœ‰å‰ä¸¤æœ¬ä¹¦æ˜¯ä¾¦æ¢å°è¯´ã€‚
- en: Be forewarned that the response from the LLM could change each time you ask
    a question, despite sending the same instructions, and the same information from
    the vector database. For example, on asking about fantasy book recommendations,
    the LLM sometimes gave three book recommendations, and sometimes more â€” though
    all from the book list. In all cases, the top recommended book stayed the same.
    Note that these variations were despite configuring the consistency â€” creativity
    spectrum â€” the â€˜temperatureâ€™ parameter â€” to 0 to minimize variance.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: è¯·æ³¨æ„ï¼ŒLLM çš„å“åº”æ¯æ¬¡æé—®æ—¶å¯èƒ½ä¼šæœ‰æ‰€ä¸åŒï¼Œå°½ç®¡å‘é€çš„æ˜¯ç›¸åŒçš„æŒ‡ä»¤å’Œæ¥è‡ªå‘é‡æ•°æ®åº“çš„ç›¸åŒä¿¡æ¯ã€‚ä¾‹å¦‚ï¼Œåœ¨è¯¢é—®å…³äºå¥‡å¹»ä¹¦ç±æ¨èæ—¶ï¼ŒLLM æœ‰æ—¶ç»™å‡ºä¸‰æœ¬ä¹¦ç±æ¨èï¼Œæœ‰æ—¶åˆ™æ›´å¤šâ€”â€”å°½ç®¡éƒ½æ˜¯æ¥è‡ªä¹¦å•ã€‚åœ¨æ‰€æœ‰æƒ…å†µä¸‹ï¼Œæœ€æ¨èçš„ä¹¦ç±ä¿æŒä¸å˜ã€‚è¯·æ³¨æ„ï¼Œè¿™äº›å˜åŒ–å‘ç”Ÿåœ¨å°†ä¸€è‡´æ€§â€”â€”åˆ›é€ åŠ›è°±â€”â€”å³â€œæ¸©åº¦â€å‚æ•°é…ç½®ä¸º
    0 ä»¥æœ€å°åŒ–å·®å¼‚çš„æƒ…å†µä¸‹ã€‚
- en: 4\. Add â€œchatâ€ capability (optional)
  id: totrans-107
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4\. æ·»åŠ â€œèŠå¤©â€åŠŸèƒ½ï¼ˆå¯é€‰ï¼‰
- en: 'The solution now has the necessary core functionality â€” it is able to read
    in information from a website and answer questions based on that information.
    But it currently does not offer a â€œconversationalâ€ user experience. Thanks to
    ChatGPT, the â€œchat interfaceâ€ has become the dominant design: we now expect this
    to be the â€œnaturalâ€ way to interact with generative AI, and LLMs in particular
    ğŸ˜…. The first steps towards getting to the chat interface involves adding â€œmemoryâ€
    to the solution.'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ï¼Œè§£å†³æ–¹æ¡ˆå…·å¤‡äº†å¿…è¦çš„æ ¸å¿ƒåŠŸèƒ½â€”â€”å®ƒèƒ½å¤Ÿä»ç½‘ç«™ä¸­è¯»å–ä¿¡æ¯å¹¶æ ¹æ®è¿™äº›ä¿¡æ¯å›ç­”é—®é¢˜ã€‚ä½†ç›®å‰å®ƒå¹¶æœªæä¾›â€œå¯¹è¯å¼â€çš„ç”¨æˆ·ä½“éªŒã€‚æ„Ÿè°¢ ChatGPTï¼Œâ€œèŠå¤©ç•Œé¢â€å·²æˆä¸ºä¸»æµè®¾è®¡ï¼šæˆ‘ä»¬ç°åœ¨æœŸæœ›è¿™æˆä¸ºä¸ç”Ÿæˆå¼
    AI å°¤å…¶æ˜¯ LLM äº’åŠ¨çš„â€œè‡ªç„¶â€æ–¹å¼ ğŸ˜…ã€‚å®ç°èŠå¤©ç•Œé¢çš„ç¬¬ä¸€æ­¥æ¶‰åŠå‘è§£å†³æ–¹æ¡ˆä¸­æ·»åŠ â€œå†…å­˜â€ã€‚
- en: â€œMemoryâ€ here is an illusion, in that the LLM is not actually remembering the
    conversation up to that point â€” it needs to be shown the full conversation history
    in each turn. So, if a user asks the LLM a follow-up question, the solution will
    package the original question, the LLMâ€™s original answer, and the follow-up question
    and send it to the LLM. The LLM reads the entire conversation and generates a
    meaningful response to continue the conversation.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é‡Œçš„â€œå†…å­˜â€æ˜¯ä¸€ç§å‡è±¡ï¼ŒLLM å®é™…ä¸Šå¹¶ä¸è®°ä½åˆ°ç›®å‰ä¸ºæ­¢çš„å¯¹è¯â€”â€”å®ƒéœ€è¦åœ¨æ¯æ¬¡å›åˆä¸­å±•ç¤ºå®Œæ•´çš„å¯¹è¯è®°å½•ã€‚å› æ­¤ï¼Œå¦‚æœç”¨æˆ·å‘ LLM æé—®åç»­é—®é¢˜ï¼Œè§£å†³æ–¹æ¡ˆå°†æ‰“åŒ…åŸå§‹é—®é¢˜ã€LLM
    çš„åŸå§‹ç­”æ¡ˆä»¥åŠåç»­é—®é¢˜ï¼Œå¹¶å°†å…¶å‘é€ç»™ LLMã€‚LLM é˜…è¯»æ•´ä¸ªå¯¹è¯å¹¶ç”Ÿæˆæœ‰æ„ä¹‰çš„å“åº”ä»¥ç»§ç»­å¯¹è¯ã€‚
- en: In question-answering chatbots, like the one weâ€™re building, this approach needs
    to be extended further because there is the interim step to reach out to the vector
    database and pull relevant information to formulate the response to a userâ€™s follow-up
    question. The way â€œmemoryâ€ is simulated in question-answering chatbots is,
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨é—®ç­”èŠå¤©æœºå™¨äººä¸­ï¼Œå¦‚æˆ‘ä»¬æ­£åœ¨æ„å»ºçš„è¿™ä¸ªï¼Œé€šå¸¸éœ€è¦è¿›ä¸€æ­¥æ‰©å±•æ­¤æ–¹æ³•ï¼Œå› ä¸ºå­˜åœ¨ä¸­é—´æ­¥éª¤éœ€è¦ä»å‘é‡æ•°æ®åº“ä¸­æå–ç›¸å…³ä¿¡æ¯ä»¥åˆ¶å®šå¯¹ç”¨æˆ·åç»­é—®é¢˜çš„å“åº”ã€‚åœ¨é—®ç­”èŠå¤©æœºå™¨äººä¸­ï¼Œâ€œå†…å­˜â€æ˜¯è¿™æ ·æ¨¡æ‹Ÿçš„ï¼š
- en: Retain all questions and responses (in a variable) as â€œchat historyâ€
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å°†æ‰€æœ‰é—®é¢˜å’Œå“åº”ä¿ç•™ï¼ˆåœ¨ä¸€ä¸ªå˜é‡ä¸­ï¼‰ä½œä¸ºâ€œèŠå¤©è®°å½•â€
- en: When the user asks a question, send the chat history and the new question to
    the LLM and ask it to generate a standalone question
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å½“ç”¨æˆ·æé—®æ—¶ï¼Œå°†èŠå¤©è®°å½•å’Œæ–°é—®é¢˜å‘é€ç»™ LLMï¼Œå¹¶è¦æ±‚ç”Ÿæˆä¸€ä¸ªç‹¬ç«‹çš„é—®é¢˜
- en: At this point, the chat history is no longer needed. Use the standalone question
    to run a new search on the vector DB
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æ­¤æ—¶ï¼ŒèŠå¤©è®°å½•ä¸å†éœ€è¦ã€‚ä½¿ç”¨ç‹¬ç«‹çš„é—®é¢˜åœ¨å‘é‡æ•°æ®åº“ä¸Šè¿›è¡Œæ–°çš„æœç´¢
- en: Pass the standalone question and search results, along with instructions to
    the LLM to get a final answer. This step is similar to what we implemented in
    the previous stage â€œGenerate Answer using LLMâ€
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å°†ç‹¬ç«‹é—®é¢˜å’Œæœç´¢ç»“æœä¼ é€’ç»™ LLMï¼Œå¹¶é™„ä¸ŠæŒ‡ä»¤ä»¥è·å¾—æœ€ç»ˆç­”æ¡ˆã€‚è¿™ä¸ªæ­¥éª¤ç±»ä¼¼äºæˆ‘ä»¬åœ¨ä¹‹å‰é˜¶æ®µâ€œä½¿ç”¨ LLM ç”Ÿæˆç­”æ¡ˆâ€ä¸­å®æ–½çš„æ­¥éª¤
- en: While we can keep a track of chat history in simpler variables, we will use
    one of LangChainâ€™s memory types. The particular memory object we will use offers
    the nice feature of automatically truncating older chat history when it reaches
    a size limit you specify, generally the size of the text that the selected LLM
    can accept. In our case, the LLM should be able to accept a little over 4,000
    â€œtokensâ€ (which are word parts), which should roughly be 3,000 words or ~5 pages
    from a Word document. OpenAI offers a 16k variant of the same ChatGPT LLM, which
    can accept 4x the input. Hence, the need to configure the memory size.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: è™½ç„¶æˆ‘ä»¬å¯ä»¥ä½¿ç”¨ç®€å•çš„å˜é‡æ¥è·Ÿè¸ªèŠå¤©è®°å½•ï¼Œä½†æˆ‘ä»¬å°†ä½¿ç”¨ LangChain çš„ä¸€ç§å†…å­˜ç±»å‹ã€‚æˆ‘ä»¬å°†ä½¿ç”¨çš„ç‰¹å®šå†…å­˜å¯¹è±¡æä¾›äº†ä¸€ä¸ªå¾ˆå¥½çš„åŠŸèƒ½ï¼Œå³åœ¨è¾¾åˆ°ä½ æŒ‡å®šçš„å¤§å°é™åˆ¶æ—¶è‡ªåŠ¨æˆªæ–­è¾ƒæ—§çš„èŠå¤©è®°å½•ï¼Œé€šå¸¸æ˜¯é€‰å®š
    LLM å¯ä»¥æ¥å—çš„æ–‡æœ¬å¤§å°ã€‚åœ¨æˆ‘ä»¬çš„æ¡ˆä¾‹ä¸­ï¼ŒLLM åº”è¯¥èƒ½å¤Ÿæ¥å—ç•¥è¶…è¿‡ 4,000 ä¸ªâ€œtokensâ€ï¼ˆå³è¯æ±‡éƒ¨åˆ†ï¼‰ï¼Œè¿™å¤§çº¦æ˜¯ 3,000 ä¸ªå•è¯æˆ– ~5
    é¡µçš„ Word æ–‡æ¡£ã€‚OpenAI æä¾›äº†ç›¸åŒ ChatGPT LLM çš„ 16k å˜ä½“ï¼Œå¯ä»¥æ¥å— 4 å€çš„è¾“å…¥ã€‚å› æ­¤ï¼Œéœ€è¦é…ç½®å†…å­˜å¤§å°ã€‚
- en: Here is the code to achieve these steps. Again, LangChain provides a higher-level
    abstraction and the code does not have to be so explicit. This version is just
    to expose the underlying instructions sent to the LLM â€” first to condense the
    chat history into a single standalone question, which will then be used for the
    vector DB search, and the second to generate a response to the generated standalone
    question based on vector DB search results.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯å®ç°è¿™äº›æ­¥éª¤çš„ä»£ç ã€‚åŒæ ·ï¼ŒLangChainæä¾›äº†æ›´é«˜å±‚æ¬¡çš„æŠ½è±¡ï¼Œä»£ç ä¸å¿…å¦‚æ­¤æ˜ç¡®ã€‚è¿™ä¸ªç‰ˆæœ¬åªæ˜¯ä¸ºäº†å±•ç¤ºå‘é€åˆ°LLMçš„åº•å±‚æŒ‡ä»¤â€”â€”é¦–å…ˆå°†èŠå¤©è®°å½•æµ“ç¼©æˆä¸€ä¸ªç‹¬ç«‹çš„å•ä¸€é—®é¢˜ï¼Œç„¶åç”¨äºå‘é‡æ•°æ®åº“æœç´¢ï¼Œå…¶æ¬¡æ ¹æ®å‘é‡æ•°æ®åº“æœç´¢ç»“æœç”Ÿæˆå¯¹ç”Ÿæˆçš„ç‹¬ç«‹é—®é¢˜çš„å“åº”ã€‚
- en: '[PRE12]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '![](../Images/e472c9c36dc137b348e73db60cfafa78.png)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e472c9c36dc137b348e73db60cfafa78.png)'
- en: Detective novel recommendations from the solution. Same response as the one
    received earlier using only â€œquestion-answeringâ€ capability, without â€œmemoryâ€
    (Image by the author)
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: è§£å†³æ–¹æ¡ˆçš„ä¾¦æ¢å°è¯´æ¨èã€‚ä¸ä¹‹å‰ä»…ä½¿ç”¨â€œé—®ç­”â€èƒ½åŠ›è€Œæ²¡æœ‰â€œè®°å¿†â€æ—¶æ”¶åˆ°çš„å“åº”ç›¸åŒï¼ˆå›¾ç‰‡ç”±ä½œè€…æä¾›ï¼‰
- en: 'Letâ€™s ask a follow-up question and look at the response to validate the solution
    now has â€œmemoryâ€ and can respond conversationally to follow-up questions:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬æå‡ºä¸€ä¸ªåç»­é—®é¢˜ï¼Œå¹¶æŸ¥çœ‹å“åº”ä»¥éªŒè¯è§£å†³æ–¹æ¡ˆç°åœ¨å…·æœ‰â€œè®°å¿†â€å¹¶ä¸”å¯ä»¥å¯¹åç»­é—®é¢˜è¿›è¡Œå¯¹è¯å¼å›ç­”ï¼š
- en: '[PRE13]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '![](../Images/fed2232a1253700a345d4e1bb406e967.png)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fed2232a1253700a345d4e1bb406e967.png)'
- en: Response to follow-up question asking more information about â€œthe second bookâ€.
    The solution responds back with more information about the same book as before
    (Image by the author)
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹â€œç¬¬äºŒæœ¬ä¹¦â€çš„æ›´å¤šä¿¡æ¯çš„åç»­é—®é¢˜çš„å“åº”ã€‚è§£å†³æ–¹æ¡ˆè¿”å›æ›´å¤šå…³äºåŒä¸€æœ¬ä¹¦çš„ä¿¡æ¯ï¼ˆå›¾ç‰‡ç”±ä½œè€…æä¾›ï¼‰
- en: 'Letâ€™s look at what is happening under the hood to validate that the solution
    does indeed go through the four steps outlined at the beginning of this section.
    Letâ€™s start with the chat history to verify that the solution does indeed log
    the conversation so far:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬çœ‹çœ‹åœ¨å¼•æ“ç›–ä¸‹å‘ç”Ÿäº†ä»€ä¹ˆï¼Œä»¥éªŒè¯è§£å†³æ–¹æ¡ˆç¡®å®ç»è¿‡äº†æœ¬èŠ‚å¼€å¤´æ¦‚è¿°çš„å››ä¸ªæ­¥éª¤ã€‚è®©æˆ‘ä»¬ä»èŠå¤©è®°å½•å¼€å§‹ï¼Œä»¥éªŒè¯è§£å†³æ–¹æ¡ˆç¡®å®è®°å½•äº†åˆ°ç›®å‰ä¸ºæ­¢çš„å¯¹è¯ï¼š
- en: '[PRE14]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '![](../Images/5eb9194b5133cbb28fd5070319810bf1.png)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5eb9194b5133cbb28fd5070319810bf1.png)'
- en: Chat history after asking the second question. Note that the response is also
    included in the conversation at this point. (Image by the author)
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: æé—®ç¬¬äºŒä¸ªé—®é¢˜åçš„èŠå¤©è®°å½•ã€‚æ³¨æ„æ­¤æ—¶å“åº”ä¹ŸåŒ…æ‹¬åœ¨å¯¹è¯ä¸­ã€‚ï¼ˆå›¾ç‰‡ç”±ä½œè€…æä¾›ï¼‰
- en: 'Letâ€™s look at what else is the solution tracking besides the chat history:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬çœ‹çœ‹è§£å†³æ–¹æ¡ˆé™¤äº†èŠå¤©è®°å½•å¤–è¿˜è·Ÿè¸ªäº†ä»€ä¹ˆï¼š
- en: '[PRE15]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '![](../Images/396886e2b0b33096284054126381fe35.png)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/396886e2b0b33096284054126381fe35.png)'
- en: Outputs, other than chat history, after asking the second question. (Image by
    the author)
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: æé—®ç¬¬äºŒä¸ªé—®é¢˜åï¼Œé™¤èŠå¤©è®°å½•ä¹‹å¤–çš„è¾“å‡ºã€‚ï¼ˆå›¾ç‰‡ç”±ä½œè€…æä¾›ï¼‰
- en: The solution internally uses the LLM to first convert the question *â€œTell me
    more about the second bookâ€* to *â€œWhat additional information can you provide
    about â€˜The Day of the Jackalâ€™ by Frederick Forsyth?â€*. Armed with this question,
    the solution is able to search the vector DB for any relevant information and
    retrieves The Day of the Jackal chunk first this time. Though note that some other
    irrelevant search results about other books are also included.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: è§£å†³æ–¹æ¡ˆå†…éƒ¨ä½¿ç”¨LLMå°†é—®é¢˜*â€œå‘Šè¯‰æˆ‘æ›´å¤šå…³äºç¬¬äºŒæœ¬ä¹¦çš„äº‹â€*è½¬æ¢ä¸º*â€œä½ èƒ½æä¾›æ›´å¤šå…³äºå¼—é›·å¾·é‡Œå…‹Â·ç¦èµ›æ–¯çš„ã€Šè´¼æ—¥ã€‹â€˜The Day of the Jackalâ€™çš„ä¿¡æ¯å—ï¼Ÿâ€*ã€‚æœ‰äº†è¿™ä¸ªé—®é¢˜ï¼Œè§£å†³æ–¹æ¡ˆèƒ½å¤Ÿåœ¨å‘é‡æ•°æ®åº“ä¸­æœç´¢ä»»ä½•ç›¸å…³ä¿¡æ¯ï¼Œå¹¶è¿™æ¬¡é¦–å…ˆæ£€ç´¢åˆ°ã€Šè´¼æ—¥ã€‹çš„ä¿¡æ¯ã€‚è™½ç„¶æ³¨æ„åˆ°ä¹ŸåŒ…æ‹¬äº†ä¸€äº›å…³äºå…¶ä»–ä¹¦ç±çš„æ— å…³æœç´¢ç»“æœã€‚
- en: Quick optional sidebar discussing potential issues
  id: totrans-133
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å¿«é€Ÿçš„å¯é€‰ä¾§è¾¹æ è®¨è®ºæ½œåœ¨é—®é¢˜
- en: '**Potential Issue #1 â€” Poor Standalone Question Generation:** In my tests,
    the chat solution wasnâ€™t always successful in generating a good standalone question,
    until the question generator prompt was tweaked. For example, for a follow-up
    question, â€œTell me about the second bookâ€, more often than not the generated follow-up
    question was â€œWhat can you tell me about the second book?â€ which is not particularly
    meaningful in itself and led to random search results and consequently a seemingly
    random generated response from the LLM.'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '**æ½œåœ¨é—®é¢˜ #1 â€” ç‹¬ç«‹é—®é¢˜ç”Ÿæˆä¸ä½³ï¼š** åœ¨æˆ‘çš„æµ‹è¯•ä¸­ï¼ŒèŠå¤©è§£å†³æ–¹æ¡ˆåœ¨ç”Ÿæˆä¸€ä¸ªå¥½çš„ç‹¬ç«‹é—®é¢˜æ—¶å¹¶ä¸æ€»æ˜¯æˆåŠŸï¼Œç›´åˆ°è°ƒæ•´äº†é—®é¢˜ç”Ÿæˆå™¨æç¤ºã€‚ä¾‹å¦‚ï¼Œå¯¹äºä¸€ä¸ªåç»­é—®é¢˜ï¼Œâ€œå‘Šè¯‰æˆ‘å…³äºç¬¬äºŒæœ¬ä¹¦çš„äº‹â€ï¼Œç”Ÿæˆçš„åç»­é—®é¢˜å¾€å¾€æ˜¯â€œä½ èƒ½å‘Šè¯‰æˆ‘å…³äºç¬¬äºŒæœ¬ä¹¦çš„ä»€ä¹ˆï¼Ÿâ€è¿™æœ¬èº«å¹¶æ²¡æœ‰ç‰¹åˆ«çš„æ„ä¹‰ï¼Œå¹¶å¯¼è‡´éšæœºçš„æœç´¢ç»“æœï¼Œå› æ­¤LLMçš„ç”Ÿæˆå“åº”çœ‹èµ·æ¥ä¹Ÿæ˜¯éšæœºçš„ã€‚'
- en: '**Potential Issue #2 â€” Changing Search Results Between Original & Follow-up
    Questions:** It is noteworthy that even though the second generated question specifically
    names the book of interest, the returned results from the vector DB search include
    other book results, and more importantly, these search results are different than
    from those for the original question! In this example, this change in search results
    was desirable since the question changed from â€œdetective novel recommendationsâ€
    to a particular novel. However, when a user is asking follow-up questions intending
    to dig deeper into a topic, variations in question formulation or LLM-generated
    standalone question may lead to different search results or a different ranking
    of search results, which might *not* be desirable.'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '**æ½œåœ¨é—®é¢˜ #2 â€” åŸå§‹é—®é¢˜ä¸è·Ÿè¿›é—®é¢˜ä¹‹é—´çš„æœç´¢ç»“æœå˜åŒ–ï¼š** å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œå³ä½¿ç¬¬äºŒä¸ªç”Ÿæˆçš„é—®é¢˜æ˜ç¡®æåˆ°æ„Ÿå…´è¶£çš„ä¹¦ç±ï¼Œå‘é‡æ•°æ®åº“æœç´¢è¿”å›çš„ç»“æœä¸­ä¹ŸåŒ…æ‹¬å…¶ä»–ä¹¦ç±çš„ç»“æœï¼Œæ›´é‡è¦çš„æ˜¯ï¼Œè¿™äº›æœç´¢ç»“æœä¸åŸå§‹é—®é¢˜çš„ç»“æœä¸åŒï¼åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œè¿™ç§æœç´¢ç»“æœçš„å˜åŒ–æ˜¯æœŸæœ›çš„ï¼Œå› ä¸ºé—®é¢˜ä»â€œä¾¦æ¢å°è¯´æ¨èâ€å˜æˆäº†ç‰¹å®šçš„å°è¯´ã€‚ç„¶è€Œï¼Œå½“ç”¨æˆ·æå‡ºè·Ÿè¿›é—®é¢˜ä»¥æ·±å…¥æ¢è®¨æŸä¸ªä¸»é¢˜æ—¶ï¼Œé—®é¢˜è¡¨è¿°çš„å˜åŒ–æˆ–LLMç”Ÿæˆçš„ç‹¬ç«‹é—®é¢˜å¯èƒ½ä¼šå¯¼è‡´ä¸åŒçš„æœç´¢ç»“æœæˆ–æœç´¢ç»“æœçš„ä¸åŒæ’åºï¼Œè¿™å¯èƒ½*ä¸*æ˜¯æœŸæœ›çš„ã€‚'
- en: This issue is possibly mitigated automatically, at least to a degree, by doing
    a broader initial search from the vector DB â€” returning many results instead of
    just 4â€“5 as with our example â€” and re-ranking them to ensure that the most relevant
    results bubble up to the top and are always sent to the LLM to generate the final
    response (see [Cohereâ€™s â€˜Rerankingâ€™](https://docs.cohere.com/docs/reranking)).
    Besides, it should be relatively straightforward for an app to recognize that
    search results have changed. It might be possible to apply some heuristics around
    whether the degree of change in search results (measured by ranking and overlap
    metrics), and the degree of change in the question (measured by distance metrics
    such as cosine similarity) are at parity. At least in cases where there are unexpected
    swings in search results over chat turns, the end user could be alerted and brought
    into the loop for a closer inspection, depending on the use case criticality and
    training or sophistication of end users.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªé—®é¢˜å¯èƒ½ä¼šåœ¨ä¸€å®šç¨‹åº¦ä¸Šè‡ªåŠ¨å¾—åˆ°ç¼“è§£ï¼Œè‡³å°‘æ˜¯é€šè¿‡å¯¹å‘é‡æ•°æ®åº“è¿›è¡Œæ›´å¹¿æ³›çš„åˆæ­¥æœç´¢â€”â€”è¿”å›æ›´å¤šçš„ç»“æœï¼Œè€Œä¸ä»…ä»…æ˜¯æˆ‘ä»¬ç¤ºä¾‹ä¸­çš„4-5ä¸ªâ€”â€”å¹¶å¯¹ç»“æœè¿›è¡Œé‡æ–°æ’åºï¼Œä»¥ç¡®ä¿æœ€ç›¸å…³çš„ç»“æœä¸Šå‡åˆ°é¡¶éƒ¨å¹¶å§‹ç»ˆå‘é€åˆ°LLMä»¥ç”Ÿæˆæœ€ç»ˆå“åº”ï¼ˆå‚è§[Cohereçš„â€œé‡æ–°æ’åºâ€](https://docs.cohere.com/docs/reranking)ï¼‰ã€‚æ­¤å¤–ï¼Œåº”ç”¨ç¨‹åºåº”ç›¸å¯¹å®¹æ˜“è¯†åˆ«æœç´¢ç»“æœæ˜¯å¦å‘ç”Ÿäº†å˜åŒ–ã€‚å¯èƒ½å¯ä»¥åº”ç”¨ä¸€äº›å¯å‘å¼æ–¹æ³•æ¥åˆ¤æ–­æœç´¢ç»“æœçš„å˜åŒ–ç¨‹åº¦ï¼ˆé€šè¿‡æ’åºå’Œé‡å åº¦é‡ï¼‰ä»¥åŠé—®é¢˜å˜åŒ–çš„ç¨‹åº¦ï¼ˆé€šè¿‡ä½™å¼¦ç›¸ä¼¼åº¦ç­‰è·ç¦»åº¦é‡ï¼‰æ˜¯å¦åŒ¹é…ã€‚è‡³å°‘åœ¨æœç´¢ç»“æœåœ¨èŠå¤©è½®æ¬¡ä¸­å‡ºç°æ„å¤–æ³¢åŠ¨çš„æƒ…å†µä¸‹ï¼Œå¯ä»¥æ ¹æ®ç”¨ä¾‹çš„å…³é”®æ€§ä»¥åŠæœ€ç»ˆç”¨æˆ·çš„åŸ¹è®­æˆ–ç´ å…»ï¼Œæé†’æœ€ç»ˆç”¨æˆ·å¹¶è®©ä»–ä»¬å‚ä¸æ›´è¯¦ç»†çš„æ£€æŸ¥ã€‚
- en: Another idea to control this behavior is to leverage the LLM to decide whether
    a follow-up question requires going to the vector DB again, or can the question
    be meaningfully answered with previously fetched results. Some use cases might
    want to generate two sets of search results and responses and let the LLM adjudicate
    between the answers, some others might be justified in passing the responsibility
    of controlling context to users by empowering them to freeze context (depending
    on the use case, user training or sophistication, and other considerations), and
    some others might simply be tolerant of changing search results over follow-up
    questions.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: æ§åˆ¶è¿™ç§è¡Œä¸ºçš„å¦ä¸€ä¸ªæƒ³æ³•æ˜¯åˆ©ç”¨LLMæ¥å†³å®šè·Ÿè¿›é—®é¢˜æ˜¯å¦éœ€è¦å†æ¬¡è®¿é—®å‘é‡æ•°æ®åº“ï¼Œæˆ–è€…è¿™ä¸ªé—®é¢˜æ˜¯å¦å¯ä»¥ç”¨ä¹‹å‰è·å–çš„ç»“æœæœ‰æ„ä¹‰åœ°å›ç­”ã€‚ä¸€äº›ç”¨ä¾‹å¯èƒ½å¸Œæœ›ç”Ÿæˆä¸¤ç»„æœç´¢ç»“æœå’Œå›ç­”ï¼Œè®©LLMåœ¨ç­”æ¡ˆä¹‹é—´è£å®šï¼Œå¦ä¸€äº›ç”¨ä¾‹å¯èƒ½é€šè¿‡èµ‹äºˆç”¨æˆ·å†»ç»“ä¸Šä¸‹æ–‡çš„èƒ½åŠ›æ¥å°†æ§åˆ¶ä¸Šä¸‹æ–‡çš„è´£ä»»è½¬äº¤ç»™ç”¨æˆ·ï¼ˆè¿™å–å†³äºç”¨ä¾‹ã€ç”¨æˆ·åŸ¹è®­æˆ–ç´ å…»ä»¥åŠå…¶ä»–è€ƒè™‘å› ç´ ï¼‰ï¼Œè¿˜æœ‰ä¸€äº›ç”¨ä¾‹å¯èƒ½å¯¹è·Ÿè¿›é—®é¢˜ä¸­æœç´¢ç»“æœçš„å˜åŒ–æŒå®½å®¹æ€åº¦ã€‚
- en: As you can probably tell, it is quite easy to get a basic solution working,
    but getting things just right â€” thatâ€˜s the hard part. The issues called out here
    is just scratching the surface. Alright, back to the main exercise â€¦
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: æ­£å¦‚ä½ å¯èƒ½å·²ç»çœ‹å‡ºï¼Œè·å¾—ä¸€ä¸ªåŸºæœ¬è§£å†³æ–¹æ¡ˆæ˜¯ç›¸å¯¹ç®€å•çš„ï¼Œä½†åšåˆ°å®Œç¾â€”â€”è¿™æ‰æ˜¯éš¾ç‚¹ã€‚è¿™é‡Œæåˆ°çš„é—®é¢˜åªæ˜¯å†°å±±ä¸€è§’ã€‚å¥½äº†ï¼Œå›åˆ°ä¸»è¦ä»»åŠ¡ â€¦
- en: 5\. Add a pre-coded UI
  id: totrans-139
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5\. æ·»åŠ é¢„ç¼–ç UI
- en: Finally, the chatbotâ€™s functionality is ready. Now, we can add a nice user-interface
    to improve user experience. This is (somewhat) easily possible due to Python libraries
    such as Gradio and Streamlit, which build front-end widgets based on instructions
    written in Python. Here, we will go with Gradio to quickly create a user interface.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åï¼ŒèŠå¤©æœºå™¨äººçš„åŠŸèƒ½å·²ç»å‡†å¤‡å¥½ã€‚ç°åœ¨ï¼Œæˆ‘ä»¬å¯ä»¥æ·»åŠ ä¸€ä¸ªæ¼‚äº®çš„ç”¨æˆ·ç•Œé¢ä»¥æ”¹å–„ç”¨æˆ·ä½“éªŒã€‚è¿™æ˜¯ï¼ˆæŸç§ç¨‹åº¦ä¸Šï¼‰ç”±äºPythonåº“å¦‚Gradioå’ŒStreamlitï¼Œä½¿å¾—åŸºäºPythonç¼–å†™çš„æŒ‡ä»¤æ„å»ºå‰ç«¯å°éƒ¨ä»¶æˆä¸ºå¯èƒ½ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬å°†ä½¿ç”¨Gradioå¿«é€Ÿåˆ›å»ºä¸€ä¸ªç”¨æˆ·ç•Œé¢ã€‚
- en: With dual objectives of catching anyone up in case they were not able to execute
    the code so far, and also to demonstrate some variations in getting to the same
    place, the following two blocks of code are self-contained and can be run in a
    completely new Colab notebook to generate the complete chatbot.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: ä»¥ä½¿ä»»ä½•å°šæœªæ‰§è¡Œä»£ç çš„äººèƒ½å¤Ÿè·Ÿä¸Šè¿›åº¦ï¼Œå¹¶æ¼”ç¤ºä¸€äº›è¾¾åˆ°ç›¸åŒç»“æœçš„å˜ä½“ï¼Œä»¥ä¸‹ä¸¤ä¸ªä»£ç å—æ˜¯è‡ªåŒ…å«çš„ï¼Œå¯ä»¥åœ¨å…¨æ–°çš„Colabç¬”è®°æœ¬ä¸­è¿è¡Œï¼Œä»¥ç”Ÿæˆå®Œæ•´çš„èŠå¤©æœºå™¨äººã€‚
- en: '[PRE16]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Before running the next set of code to render the chatbot UI, note that when
    rendered through Colab, the app becomes publicly accessible for 3 days for anyone
    with the link (the link is provided in the Colab notebook cell output). In theory,
    the app can be kept private by changing the last line in the code to *demo.launch(share=False)*,
    but I was not able to get the app to work at all then. Instead, I prefer running
    it in â€˜debugâ€™ mode in Colab, so the Colab cell stays â€œrunningâ€ until stopped,
    which then terminates the chatbot. Alternatively, run the code shown below in
    a different Colab cell to terminate the chatbot and delete content loaded to the
    Chroma vector DB within Colab.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿è¡Œä¸‹ä¸€ç»„ä»£ç ä»¥å‘ˆç°èŠå¤©æœºå™¨äººUIä¹‹å‰ï¼Œè¯·æ³¨æ„ï¼Œå½“é€šè¿‡Colabæ¸²æŸ“æ—¶ï¼Œåº”ç”¨ç¨‹åºå¯¹äºä»»ä½•æ‹¥æœ‰é“¾æ¥çš„äººå…¬å¼€å¯è®¿é—®3å¤©ï¼ˆé“¾æ¥åœ¨Colabç¬”è®°æœ¬å•å…ƒè¾“å‡ºä¸­æä¾›ï¼‰ã€‚ç†è®ºä¸Šï¼Œå¯ä»¥é€šè¿‡å°†ä»£ç ä¸­çš„æœ€åä¸€è¡Œæ›´æ”¹ä¸º*demo.launch(share=False)*æ¥ä¿æŒåº”ç”¨çš„ç§å¯†æ€§ï¼Œä½†é‚£æ—¶æˆ‘æ— æ³•ä½¿åº”ç”¨æ­£å¸¸å·¥ä½œã€‚ç›¸åï¼Œæˆ‘æ›´å€¾å‘äºåœ¨Colabä¸­ä»¥â€œè°ƒè¯•â€æ¨¡å¼è¿è¡Œï¼Œè¿™æ ·Colabå•å…ƒä¼šâ€œè¿è¡Œâ€ç›´åˆ°åœæ­¢ï¼Œç„¶åç»ˆæ­¢èŠå¤©æœºå™¨äººã€‚æˆ–è€…ï¼Œåœ¨ä¸åŒçš„Colabå•å…ƒä¸­è¿è¡Œä¸‹é¢çš„ä»£ç ï¼Œä»¥ç»ˆæ­¢èŠå¤©æœºå™¨äººå¹¶åˆ é™¤åœ¨Colabä¸­åŠ è½½åˆ°Chromaå‘é‡æ•°æ®åº“ä¸­çš„å†…å®¹ã€‚
- en: '[PRE17]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Below is the code to run the chatbot as an app. Most of this code reuses the
    code up to this point of the article, so should seem familiar. Note that there
    are some differences in the code below compared to the code earlier, including
    but not limited to there being no memory management using LangChainâ€™s â€˜tokenâ€™
    memory object that we used before. This means as the conversation continues for
    a while, the history will become too long to pass in to the language modelâ€™s context,
    and the app will need a restart.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: ä»¥ä¸‹æ˜¯å°†èŠå¤©æœºå™¨äººä½œä¸ºåº”ç”¨è¿è¡Œçš„ä»£ç ã€‚å¤§éƒ¨åˆ†ä»£ç é‡å¤äº†åˆ°ç›®å‰ä¸ºæ­¢çš„æ–‡ç« ä¸­çš„ä»£ç ï¼Œæ‰€ä»¥åº”è¯¥å¾ˆç†Ÿæ‚‰ã€‚è¯·æ³¨æ„ï¼Œä¸ä¹‹å‰çš„ä»£ç ç›¸æ¯”ï¼Œä¸‹é¢çš„ä»£ç å­˜åœ¨ä¸€äº›å·®å¼‚ï¼ŒåŒ…æ‹¬ä½†ä¸é™äºæ²¡æœ‰ä½¿ç”¨ä¹‹å‰ç”¨è¿‡çš„LangChainçš„â€˜tokenâ€™å†…å­˜å¯¹è±¡è¿›è¡Œå†…å­˜ç®¡ç†ã€‚è¿™æ„å‘³ç€éšç€å¯¹è¯çš„ç»§ç»­ï¼Œå†å²è®°å½•å°†å˜å¾—è¿‡é•¿ï¼Œæ— æ³•ä¼ é€’ç»™è¯­è¨€æ¨¡å‹çš„ä¸Šä¸‹æ–‡ï¼Œåº”ç”¨éœ€è¦é‡å¯ã€‚
- en: '[PRE18]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'You can play around with the app by giving it a different URL to load content
    from. This goes without saying: this is not a production-grade app, and was only
    created to demonstrate building blocks of RAG-based GenAI solutions. This is an
    early prototype at best and if it were to be converted into a regular product,
    most of the software engineering work will lie ahead.'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ å¯ä»¥é€šè¿‡ç»™åº”ç”¨æä¾›ä¸åŒçš„URLæ¥åŠ è½½å†…å®¹è¿›è¡Œå°è¯•ã€‚æ— éœ€å¤šè¨€ï¼šè¿™ä¸æ˜¯ä¸€ä¸ªç”Ÿäº§çº§åº”ç”¨ï¼Œåªæ˜¯ç”¨æ¥æ¼”ç¤ºåŸºäºRAGçš„GenAIè§£å†³æ–¹æ¡ˆçš„æ„å»ºå—ã€‚è¿™å……å…¶é‡æ˜¯ä¸€ä¸ªæ—©æœŸåŸå‹ï¼Œå¦‚æœè¦å°†å…¶è½¬æ¢ä¸ºå¸¸è§„äº§å“ï¼Œå¤§éƒ¨åˆ†çš„è½¯ä»¶å·¥ç¨‹å·¥ä½œè¿˜åœ¨å‰é¢ã€‚
- en: Revisiting FAQs from the Introduction
  id: totrans-148
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: é‡æ–°å®¡è§†ä»‹ç»ä¸­çš„å¸¸è§é—®é¢˜
- en: With the context and knowledge of the chatbot we created, letâ€™s revisit some
    of the questions posed in the *Introduction* and dive just a little bit deeper.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: æ ¹æ®æˆ‘ä»¬åˆ›å»ºçš„èŠå¤©æœºå™¨äººèƒŒæ™¯å’ŒçŸ¥è¯†ï¼Œè®©æˆ‘ä»¬é‡æ–°å®¡è§†åœ¨*ä»‹ç»*ä¸­æå‡ºçš„ä¸€äº›é—®é¢˜ï¼Œå¹¶æ·±å…¥æ¢è®¨ä¸€ä¸‹ã€‚
- en: '*Is our use case a good fit for LLM-powered solutions? Perhaps traditional
    analytics, supervised machine learning, or another approach is a better fit?*'
  id: totrans-150
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*æˆ‘ä»¬çš„ç”¨ä¾‹æ˜¯å¦é€‚åˆLLMé©±åŠ¨çš„è§£å†³æ–¹æ¡ˆï¼Ÿä¹Ÿè®¸ä¼ ç»Ÿçš„åˆ†ææ–¹æ³•ã€ç›‘ç£å­¦ä¹ æˆ–å…¶ä»–æ–¹æ³•æ›´åˆé€‚ï¼Ÿ*'
- en: LLMs are good at â€œunderstandingâ€ language related tasks as well as following
    instructions. So, the early use cases for LLMs have been question-answering, summarization,
    generation (text in this case), enabling better meaning-based search, sentiment
    analysis, coding, etc. LLMs have also picked up the ability to problem-solve and
    reason. For example, LLMs can act as automated graders for studentsâ€™ assignments
    if you provide it with an answer key, or sometimes even without.
  id: totrans-151
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: LLM åœ¨â€œç†è§£â€è¯­è¨€ç›¸å…³ä»»åŠ¡ä»¥åŠéµå¾ªæŒ‡ä»¤æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚å› æ­¤ï¼ŒLLM çš„æ—©æœŸä½¿ç”¨æ¡ˆä¾‹åŒ…æ‹¬é—®ç­”ã€æ€»ç»“ã€ç”Ÿæˆï¼ˆåœ¨è¿™é‡Œæ˜¯æ–‡æœ¬ï¼‰ã€æä¾›æ›´å¥½çš„åŸºäºæ„ä¹‰çš„æœç´¢ã€æƒ…æ„Ÿåˆ†æã€ç¼–ç ç­‰ã€‚LLM
    è¿˜è·å¾—äº†è§£å†³é—®é¢˜å’Œæ¨ç†çš„èƒ½åŠ›ã€‚ä¾‹å¦‚ï¼ŒLLM å¯ä»¥å……å½“å­¦ç”Ÿä½œä¸šçš„è‡ªåŠ¨è¯„åˆ†å‘˜ï¼Œåªè¦ä½ æä¾›ç­”æ¡ˆé”®ï¼Œç”šè‡³æœ‰æ—¶ä¸æä¾›ã€‚
- en: On the other hand, predictions or classifications based on a large number of
    data points, multi-armed bandit experiments for marketing optimization, recommender
    systems, reinforcement learning systems (Roomba, Nest thermostat, optimizing power
    consumption or inventory levels, etc.) are the forte of other types of analytics
    or machine learning â€¦ at least for the time being. Hybrid approaches where traditional
    ML models feed information to LLMs and vice-versa should also be considered as
    a holistic solution to a core business problem.
  id: totrans-152
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: å¦ä¸€æ–¹é¢ï¼ŒåŸºäºå¤§é‡æ•°æ®ç‚¹çš„é¢„æµ‹æˆ–åˆ†ç±»ã€ç”¨äºè¥é”€ä¼˜åŒ–çš„å¤šè‡‚èµŒåšæœºå®éªŒã€æ¨èç³»ç»Ÿã€å¼ºåŒ–å­¦ä¹ ç³»ç»Ÿï¼ˆå¦‚ Roombaã€Nest æ¸©æ§å™¨ã€ä¼˜åŒ–èƒ½æºæ¶ˆè€—æˆ–åº“å­˜æ°´å¹³ç­‰ï¼‰æ˜¯å…¶ä»–ç±»å‹åˆ†ææˆ–æœºå™¨å­¦ä¹ çš„å¼ºé¡¹â€¦â€¦è‡³å°‘ç›®å‰æ˜¯è¿™æ ·ã€‚ä¼ ç»Ÿ
    ML æ¨¡å‹å‘ LLM æä¾›ä¿¡æ¯å’Œåå‘ä¼ é€’ä¿¡æ¯çš„æ··åˆæ–¹æ³•ä¹Ÿåº”è€ƒè™‘ä½œä¸ºè§£å†³æ ¸å¿ƒä¸šåŠ¡é—®é¢˜çš„æ•´ä½“æ–¹æ¡ˆã€‚
- en: '*If LLMs are the way to go, can our use case be addressed by an off-the-shelf
    product (say, ChatGPT Enterprise) now or in the near-future? Classic build-vs-buy
    decision.*'
  id: totrans-153
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*å¦‚æœ LLM æ˜¯å¯è¡Œçš„è§£å†³æ–¹æ¡ˆï¼Œæˆ‘ä»¬çš„ä½¿ç”¨æ¡ˆä¾‹æ˜¯å¦å¯ä»¥é€šè¿‡ç°æˆçš„äº§å“ï¼ˆä¾‹å¦‚ï¼ŒChatGPT Enterpriseï¼‰ç°åœ¨æˆ–åœ¨ä¸ä¹…çš„å°†æ¥å¾—åˆ°è§£å†³ï¼Ÿç»å…¸çš„æ„å»ºä¸è´­ä¹°å†³ç­–ã€‚*'
- en: Services and products offered by OpenAI, AWS, and others are going to grow broader,
    better, and possibly cheaper. For example, ChatGPT letâ€™s users upload their files
    for analysis, Bing Chat and Googleâ€™s Bard let you point to external websites for
    question answering, AWS Kendra brings semantic search to an enterpriseâ€™s information,
    Microsoft Copilot lets you bring LLMs to Word, Powerpoint, Excel, etc. For the
    same reason that companies do not build their own operating systems or their own
    databases, companies should think about whether they need to build AI solutions
    that might possibly get obsolete by current and future off-the-shelf products.
    On the other hand, if a companyâ€™s use cases are specific, or restrictive in some
    sense â€” such as not being able to send their sensitive data to any vendor due
    to sensitivity, or due to regulatory guidance â€” then, it might be required to
    build generative AI products within the company to address use cases. Products
    that use an LLMâ€™s reasoning ability but undertake tasks or generate outputs too
    distinct than the vended solutions might warrant in-house development. For example,
    a system that is monitoring the factory floor, or a manufacturing processes, or
    inventory levels, etc. might warrant custom development, especially if there are
    no good domain-specific product offerings. Also, if the application requires specialized
    domain knowledge, then an LLM fine-tuned on domain-specific data is likely going
    to outperform a general-purpose LLM from OpenAI, and in-house development could
    be considered.
  id: totrans-154
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: OpenAIã€AWS å’Œå…¶ä»–å…¬å¸æä¾›çš„æœåŠ¡å’Œäº§å“å°†å˜å¾—æ›´åŠ å¹¿æ³›ã€æ›´å¥½ï¼Œå¯èƒ½è¿˜æ›´ä¾¿å®œã€‚ä¾‹å¦‚ï¼ŒChatGPT å…è®¸ç”¨æˆ·ä¸Šä¼ æ–‡ä»¶è¿›è¡Œåˆ†æï¼ŒBing Chat
    å’Œ Google çš„ Bard è®©ä½ æŒ‡å‘å¤–éƒ¨ç½‘ç«™è¿›è¡Œé—®ç­”ï¼ŒAWS Kendra å°†è¯­ä¹‰æœç´¢å¼•å…¥ä¼ä¸šçš„ä¿¡æ¯ä¸­ï¼ŒMicrosoft Copilot è®©ä½ åœ¨ Wordã€Powerpointã€Excel
    ç­‰åº”ç”¨ä¸­ä½¿ç”¨ LLMã€‚æ­£å¦‚å…¬å¸ä¸ä¼šè‡ªå·±æ„å»ºæ“ä½œç³»ç»Ÿæˆ–æ•°æ®åº“ä¸€æ ·ï¼Œå…¬å¸ä¹Ÿåº”è¯¥è€ƒè™‘æ˜¯å¦éœ€è¦æ„å»ºå¯èƒ½è¢«å½“å‰å’Œæœªæ¥çš„ç°æˆäº§å“æ‰€å–ä»£çš„ AI è§£å†³æ–¹æ¡ˆã€‚å¦ä¸€æ–¹é¢ï¼Œå¦‚æœå…¬å¸çš„ä½¿ç”¨æ¡ˆä¾‹éå¸¸å…·ä½“æˆ–åœ¨æŸç§ç¨‹åº¦ä¸Šå—é™ï¼Œä¾‹å¦‚ç”±äºæ•æ„Ÿæ€§æˆ–æ³•è§„æŒ‡å¯¼ä¸èƒ½å°†æ•æ„Ÿæ•°æ®å‘é€ç»™ä»»ä½•ä¾›åº”å•†ï¼Œé‚£ä¹ˆå¯èƒ½éœ€è¦åœ¨å…¬å¸å†…éƒ¨æ„å»ºç”Ÿæˆæ€§
    AI äº§å“ä»¥è§£å†³è¿™äº›ä½¿ç”¨æ¡ˆä¾‹ã€‚ä½¿ç”¨ LLM æ¨ç†èƒ½åŠ›ä½†æ‰¿æ‹…çš„ä»»åŠ¡æˆ–ç”Ÿæˆçš„è¾“å‡ºä¸ç°æˆè§£å†³æ–¹æ¡ˆå·®å¼‚å¤ªå¤§çš„äº§å“å¯èƒ½éœ€è¦å†…éƒ¨å¼€å‘ã€‚ä¾‹å¦‚ï¼Œç›‘æ§å·¥å‚è½¦é—´ã€åˆ¶é€ è¿‡ç¨‹æˆ–åº“å­˜æ°´å¹³çš„ç³»ç»Ÿå¯èƒ½éœ€è¦å®šåˆ¶å¼€å‘ï¼Œç‰¹åˆ«æ˜¯å½“æ²¡æœ‰å¥½çš„é¢†åŸŸç‰¹å®šäº§å“æ—¶ã€‚æ­¤å¤–ï¼Œå¦‚æœåº”ç”¨éœ€è¦ä¸“ä¸šé¢†åŸŸçŸ¥è¯†ï¼Œé‚£ä¹ˆåœ¨é¢†åŸŸç‰¹å®šæ•°æ®ä¸Šå¾®è°ƒçš„
    LLM å¯èƒ½ä¼šä¼˜äº OpenAI çš„é€šç”¨ LLMï¼Œå†…éƒ¨å¼€å‘ä¹Ÿå¯ä»¥è€ƒè™‘ã€‚
- en: '*What are the different building blocks of our LLM-powered product? Which of
    these are commoditized, and which are likely to need more time to build and test?*'
  id: totrans-155
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*æˆ‘ä»¬çš„ LLM é©±åŠ¨äº§å“çš„ä¸åŒæ„å»ºæ¨¡å—æœ‰å“ªäº›ï¼Ÿè¿™äº›æ¨¡å—ä¸­å“ªäº›å·²ç»å•†å“åŒ–ï¼Œå“ªäº›å¯èƒ½éœ€è¦æ›´å¤šæ—¶é—´æ¥æ„å»ºå’Œæµ‹è¯•ï¼Ÿ*'
- en: 'The high-level building blocks for a RAG solution like we built are the data
    pipeline, vector database, retrieval, generation, and the LLM of course. There
    are lots of great choices for LLMs and for vector databases. The data pipelines,
    retrieval, prompt engineering for generation will require some good old-fashioned
    data-scienc*y* experimentation to optimize for a use case. Once an initial solution
    is in place, productionization will require a lot of work, which is true of any
    data science / machine learning pipeline. This talk offers hard-earned wisdom
    on the topic of productionization: [LLMs in Production: Learning from Experience,
    Dr. Waleed Kadous, Chief Scientist, AnyScale](https://youtu.be/xa7k9MUeIdk?si=LQizYwFt4m-XOYpk)'
  id: totrans-156
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'åƒæˆ‘ä»¬æ„å»ºçš„RAGè§£å†³æ–¹æ¡ˆçš„é«˜çº§æ„å»ºå—åŒ…æ‹¬æ•°æ®ç®¡é“ã€å‘é‡æ•°æ®åº“ã€æ£€ç´¢ã€ç”Ÿæˆï¼Œå½“ç„¶è¿˜æœ‰LLMã€‚LLMå’Œå‘é‡æ•°æ®åº“æœ‰å¾ˆå¤šä¼˜ç§€çš„é€‰æ‹©ã€‚æ•°æ®ç®¡é“ã€æ£€ç´¢ã€ç”Ÿæˆçš„æç¤ºå·¥ç¨‹å°†éœ€è¦ä¸€äº›ä¼ ç»Ÿçš„æ•°æ®ç§‘å­¦å®éªŒæ¥é’ˆå¯¹å…·ä½“ç”¨ä¾‹è¿›è¡Œä¼˜åŒ–ã€‚ä¸€æ—¦åˆæ­¥è§£å†³æ–¹æ¡ˆåˆ°ä½ï¼Œç”Ÿäº§åŒ–å°†éœ€è¦å¤§é‡å·¥ä½œï¼Œè¿™åœ¨ä»»ä½•æ•°æ®ç§‘å­¦/æœºå™¨å­¦ä¹ ç®¡é“ä¸­éƒ½æ˜¯çœŸå®çš„ã€‚æœ¬è®²åº§æä¾›äº†æœ‰å…³ç”Ÿäº§åŒ–çš„å®è´µç»éªŒï¼š[LLMs
    in Production: Learning from Experience, Dr. Waleed Kadous, Chief Scientist, AnyScale](https://youtu.be/xa7k9MUeIdk?si=LQizYwFt4m-XOYpk)'
- en: '*How do we measure the performance of our solution? What levers are available
    to improve the quality of outputs from our product?* As with any technology (or
    non-technology ) solution, business impact should be measured using leading KPIs.
    Some direct metrics being difficult to measure get replaced by surrogate metrics
    such as average number of daily users (DAU) and other product metrics.'
  id: totrans-157
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*æˆ‘ä»¬å¦‚ä½•è¡¡é‡è§£å†³æ–¹æ¡ˆçš„æ€§èƒ½ï¼Ÿæœ‰å“ªäº›æ æ†å¯ä»¥æ”¹å–„æˆ‘ä»¬äº§å“çš„è¾“å‡ºè´¨é‡ï¼Ÿ* ä¸ä»»ä½•æŠ€æœ¯ï¼ˆæˆ–éæŠ€æœ¯ï¼‰è§£å†³æ–¹æ¡ˆä¸€æ ·ï¼Œå•†ä¸šå½±å“åº”ä½¿ç”¨é¢†å…ˆçš„å…³é”®ç»©æ•ˆæŒ‡æ ‡æ¥è¡¡é‡ã€‚ä¸€äº›ç›´æ¥åº¦é‡éš¾ä»¥æµ‹é‡ï¼Œé€šå¸¸ä¼šè¢«æ›¿ä»£æŒ‡æ ‡å¦‚æ¯æ—¥æ´»è·ƒç”¨æˆ·æ•°ï¼ˆDAUï¼‰å’Œå…¶ä»–äº§å“æŒ‡æ ‡æ‰€å–ä»£ã€‚'
- en: 'Business metrics should be complemented with technical metrics evaluating the
    performance of the RAG solution. The overall quality of the response â€” how good
    is the systemâ€™s response compared to the best generated response from an expert
    human or a state of the art frontier model such as GPT-4 (currently) could be
    evaluated using a range of metrics that test for informativeness, factualness,
    relevance, toxicity, etc. It will help to delve deeper into performance of individual
    components to iterate and improve each: the quality of information that the solution
    will use as context, retrieval, and generation.'
  id: totrans-158
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: å•†ä¸šæŒ‡æ ‡åº”è¡¥å……æŠ€æœ¯æŒ‡æ ‡ï¼Œä»¥è¯„ä¼°RAGè§£å†³æ–¹æ¡ˆçš„æ€§èƒ½ã€‚å¯ä»¥ä½¿ç”¨ä¸€ç³»åˆ—æµ‹è¯•ä¿¡æ¯é‡ã€äº‹å®å‡†ç¡®æ€§ã€ç›¸å…³æ€§ã€æ¯’æ€§ç­‰çš„æŒ‡æ ‡æ¥è¯„ä¼°å›åº”çš„æ•´ä½“è´¨é‡â€”â€”ç³»ç»Ÿçš„å›åº”ä¸ä¸“å®¶æˆ–å¦‚GPT-4ï¼ˆå½“å‰ï¼‰çš„æœ€å…ˆè¿›æ¨¡å‹çš„å›åº”ç›¸æ¯”å¦‚ä½•ã€‚è¿™æœ‰åŠ©äºæ·±å…¥äº†è§£å„ä¸ªç»„ä»¶çš„æ€§èƒ½ï¼Œä»¥ä¾¿è¿­ä»£å’Œæ”¹è¿›æ¯ä¸ªç»„ä»¶ï¼šè§£å†³æ–¹æ¡ˆå°†ç”¨ä½œä¸Šä¸‹æ–‡çš„ä¿¡æ¯è´¨é‡ã€æ£€ç´¢å’Œç”Ÿæˆã€‚
- en: i. How good is the **data quality**? If data available to an organization and
    stored in the vector database doesnâ€™t have the required information, no human
    or LLM can conjure a response based on it.
  id: totrans-159
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: i. **æ•°æ®è´¨é‡**å¦‚ä½•ï¼Ÿå¦‚æœç»„ç»‡å¯ç”¨çš„æ•°æ®å­˜å‚¨åœ¨å‘é‡æ•°æ®åº“ä¸­æ²¡æœ‰æ‰€éœ€çš„ä¿¡æ¯ï¼Œåˆ™æ²¡æœ‰äººæˆ–LLMèƒ½å¤ŸåŸºäºè¿™äº›ä¿¡æ¯æ„é€ å›åº”ã€‚
- en: ii. How good is the **retrieval**? Assuming the information is available, how
    successful is the system in finding and fetching the relevant bits?
  id: totrans-160
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ii. **æ£€ç´¢**æ•ˆæœå¦‚ä½•ï¼Ÿå‡è®¾ä¿¡æ¯å¯ç”¨ï¼Œç³»ç»Ÿåœ¨æ‰¾åˆ°å’Œæå–ç›¸å…³ä¿¡æ¯æ–¹é¢æœ‰å¤šæˆåŠŸï¼Ÿ
- en: iii. How good is the **generation (i.e. synthesis)**? Assuming the information
    is available, retrieved correctly, and passed on to the LLM to generate the final
    response, is the LLM using the information as expected?
  id: totrans-161
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: iii. **ç”Ÿæˆï¼ˆå³åˆæˆï¼‰**æ•ˆæœå¦‚ä½•ï¼Ÿå‡è®¾ä¿¡æ¯å¯ç”¨ã€æ£€ç´¢æ­£ç¡®ï¼Œå¹¶ä¼ é€’ç»™LLMç”Ÿæˆæœ€ç»ˆå›åº”ï¼ŒLLMæ˜¯å¦æŒ‰é¢„æœŸä½¿ç”¨è¿™äº›ä¿¡æ¯ï¼Ÿ
- en: Each of these areas could be evaluated separately and improved concurrently
    to improve the overall output.
  id: totrans-162
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: æ¯ä¸ªé¢†åŸŸéƒ½å¯ä»¥å•ç‹¬è¯„ä¼°å¹¶åŒæ—¶æ”¹è¿›ï¼Œä»¥æå‡æ•´ä½“è¾“å‡ºã€‚
- en: '**Improve Data quality:** Companies need to work on data pipelines to feed
    good information into the system. If there is bad quality of information in the
    vector database, having great LLMs will not improve the outputs drastically. In
    addition to employing traditional data quality and governance frameworks, companies
    should also consider improve the quality of chunking (more on this in the next
    questionâ€™s response).'
  id: totrans-163
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '**æ”¹å–„æ•°æ®è´¨é‡ï¼š** å…¬å¸éœ€è¦åœ¨æ•°æ®ç®¡é“ä¸Šè¿›è¡Œå·¥ä½œï¼Œä»¥å‘ç³»ç»Ÿæä¾›è‰¯å¥½çš„ä¿¡æ¯ã€‚å¦‚æœå‘é‡æ•°æ®åº“ä¸­çš„ä¿¡æ¯è´¨é‡å·®ï¼Œæ‹¥æœ‰ä¼˜ç§€çš„LLMä¹Ÿä¸ä¼šæ˜¾è‘—æ”¹å–„è¾“å‡ºã€‚é™¤äº†é‡‡ç”¨ä¼ ç»Ÿçš„æ•°æ®è´¨é‡å’Œæ²»ç†æ¡†æ¶å¤–ï¼Œå…¬å¸è¿˜åº”è€ƒè™‘æ”¹å–„åˆ†å—çš„è´¨é‡ï¼ˆä¸‹ä¸€é—®é¢˜çš„å›åº”ä¸­å°†æ›´å¤šè®¨è®ºæ­¤äº‹ï¼‰ã€‚'
- en: '**Improve Retrieval:** Retrieval could be improved through trying different
    retrieval algorithms, semantic re-ranking, hybrid search combining semantic search
    and keyword search, and fine-tuning embeddings. Improving instructions / prompt
    should also contribute to improving the quality of retrieval.'
  id: totrans-164
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '**æ”¹å–„æ£€ç´¢ï¼š** é€šè¿‡å°è¯•ä¸åŒçš„æ£€ç´¢ç®—æ³•ã€è¯­ä¹‰é‡æ’åºã€ç»“åˆè¯­ä¹‰æœç´¢å’Œå…³é”®è¯æœç´¢çš„æ··åˆæœç´¢ï¼Œä»¥åŠå¾®è°ƒåµŒå…¥ï¼Œå¯ä»¥æ”¹å–„æ£€ç´¢ã€‚æ”¹å–„æŒ‡ä»¤/æç¤ºä¹Ÿåº”æœ‰åŠ©äºæå‡æ£€ç´¢è´¨é‡ã€‚'
- en: '**Improve Generation:** As LLMs improve , the synthesis step will improve,
    and possibly retrieval too due to improved embedding models. Another option assuming
    resource & time availability is fine-tuning, which can improve the quality of
    responses for specific domains and tasks. For example, a smaller fine-tuned model
    on diagnosing specific medical conditions might be better at the task than a general
    purpose model like GPT-4, while also being faster and cheaper.'
  id: totrans-165
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '**æ”¹å–„ç”Ÿæˆï¼š** éšç€LLMçš„è¿›æ­¥ï¼Œåˆæˆæ­¥éª¤å°†å¾—åˆ°æ”¹å–„ï¼Œæ£€ç´¢å¯èƒ½ä¹Ÿä¼šå› ä¸ºåµŒå…¥æ¨¡å‹çš„æ”¹è¿›è€Œå¾—åˆ°æå‡ã€‚å¦ä¸€ç§é€‰æ‹©æ˜¯è¿›è¡Œå¾®è°ƒï¼Œå‰ææ˜¯èµ„æºå’Œæ—¶é—´å……è¶³ï¼Œè¿™å¯ä»¥æé«˜ç‰¹å®šé¢†åŸŸå’Œä»»åŠ¡çš„å“åº”è´¨é‡ã€‚ä¾‹å¦‚ï¼Œé’ˆå¯¹ç‰¹å®šåŒ»ç–—æ¡ä»¶è¿›è¡Œå¾®è°ƒçš„å°æ¨¡å‹å¯èƒ½åœ¨ä»»åŠ¡ä¸Šä¼˜äºåƒGPT-4è¿™æ ·çš„é€šç”¨æ¨¡å‹ï¼ŒåŒæ—¶ä¹Ÿæ›´å¿«ã€æ›´ä¾¿å®œã€‚'
- en: '*Is our data quality acceptable for the use case? Are we organizing our data
    correctly, and passing relevant data to the LLM?*'
  id: totrans-166
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*æˆ‘ä»¬çš„æ•°æ®è´¨é‡æ˜¯å¦é€‚åˆç”¨ä¾‹ï¼Ÿæˆ‘ä»¬æ˜¯å¦æ­£ç¡®ç»„ç»‡äº†æ•°æ®ï¼Œå¹¶å°†ç›¸å…³æ•°æ®ä¼ é€’ç»™LLMï¼Ÿ*'
- en: Data quality can be assessed with the traditional data quality & governance
    frameworks. Additionally for LLM-powered solutions, the information required by
    LLMs to answer user questions or carry out tasks should be available within the
    data available to the solution.
  id: totrans-167
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: æ•°æ®è´¨é‡å¯ä»¥é€šè¿‡ä¼ ç»Ÿçš„æ•°æ®è´¨é‡ä¸æ²»ç†æ¡†æ¶è¿›è¡Œè¯„ä¼°ã€‚æ­¤å¤–ï¼Œå¯¹äºLLMé©±åŠ¨çš„è§£å†³æ–¹æ¡ˆï¼ŒLLMå›ç­”ç”¨æˆ·é—®é¢˜æˆ–æ‰§è¡Œä»»åŠ¡æ‰€éœ€çš„ä¿¡æ¯åº”åœ¨è§£å†³æ–¹æ¡ˆå¯ç”¨çš„æ•°æ®ä¸­å­˜åœ¨ã€‚
- en: Assuming the data is available, the data should be chunked appropriately for
    the use case and LLM being used. Chunks shouldnâ€™t be too broad to dilute coherence
    with respect to a specific topic or too narrow to not include all the necessary
    context. Data shouldnâ€™t be split into chunks in a way that necessary context is
    split between chunks and meaningless when separated in this way. For example,
    if the two sentences below are split into two chunks,
  id: totrans-168
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: å‡è®¾æ•°æ®æ˜¯å¯ç”¨çš„ï¼Œæ•°æ®åº”æ ¹æ®ç”¨ä¾‹å’Œæ‰€ä½¿ç”¨çš„LLMè¿›è¡Œé€‚å½“çš„æ‹†åˆ†ã€‚å—ä¸åº”è¿‡äºå®½æ³›ï¼Œä»¥å…ç¨€é‡Šä¸ç‰¹å®šä¸»é¢˜ç›¸å…³çš„è¿è´¯æ€§ï¼Œä¹Ÿä¸åº”è¿‡äºç‹­çª„ï¼Œä»¥å…é—æ¼æ‰€æœ‰å¿…è¦çš„ä¸Šä¸‹æ–‡ã€‚æ•°æ®ä¸åº”ä»¥å°†å¿…è¦çš„ä¸Šä¸‹æ–‡åˆ†å‰²åœ¨å—ä¹‹é—´å¹¶ä¸”åœ¨è¿™ç§åˆ†éš”ä¸‹æ¯«æ— æ„ä¹‰çš„æ–¹å¼è¿›è¡Œæ‹†åˆ†ã€‚ä¾‹å¦‚ï¼Œå¦‚æœä¸‹é¢çš„ä¸¤ä¸ªå¥å­è¢«æ‹†åˆ†æˆä¸¤ä¸ªå—ï¼Œ
- en: '*â€œOpenAIâ€™s GPT-3.5 is a powerful LLM. It can support context sizes up to 16K
    tokens.â€*'
  id: totrans-169
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*â€œOpenAIçš„GPT-3.5æ˜¯ä¸€ä¸ªå¼ºå¤§çš„LLMã€‚å®ƒå¯ä»¥æ”¯æŒé«˜è¾¾16Kä»¤ç‰Œçš„ä¸Šä¸‹æ–‡å¤§å°ã€‚â€*'
- en: A question such as â€œTell me about GPT 3.5 LLMâ€ may not fetch the 2nd sentence
    as it doesnâ€™t mention GPT 3.5 and that information might not be provided to a
    user, just by virtue of suboptimal chunking. More dangerously, the sentence might
    still be fetched when asked about a completely different LLM due to semantic association
    of context sizes and tokens with LLMs, and the response might be that other model
    in focus has context sizes up to 16K, which would be factually inaccurate. This
    is a simplified example unlikely to be encountered in production, but the idea
    holds.
  id: totrans-170
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: åƒâ€œå‘Šè¯‰æˆ‘å…³äºGPT 3.5 LLMçš„æƒ…å†µâ€è¿™æ ·çš„é—®é¢˜å¯èƒ½ä¸ä¼šè·å–ç¬¬äºŒå¥ï¼Œå› ä¸ºå®ƒæ²¡æœ‰æåˆ°GPT 3.5ï¼Œè€Œè¿™æ¡ä¿¡æ¯å¯èƒ½ä¸ä¼šæä¾›ç»™ç”¨æˆ·ï¼Œä»…ä»…æ˜¯ç”±äºå­ä¼˜åŒ–å—çš„åŸå› ã€‚æ›´å±é™©çš„æ˜¯ï¼Œç”±äºä¸Šä¸‹æ–‡å¤§å°å’Œä»¤ç‰Œä¸LLMçš„è¯­ä¹‰å…³è”ï¼Œå½“è¯¢é—®å®Œå…¨ä¸åŒçš„LLMæ—¶ï¼Œå¯èƒ½ä»ä¼šè·å–è¯¥å¥å­ï¼Œä¸”å›ç­”å¯èƒ½æ˜¯å…¶ä»–é‡ç‚¹æ¨¡å‹çš„ä¸Šä¸‹æ–‡å¤§å°é«˜è¾¾16Kï¼Œè¿™å°†æ˜¯ä¸å‡†ç¡®çš„ã€‚è¿™æ˜¯ä¸€ä¸ªåœ¨ç”Ÿäº§ç¯å¢ƒä¸­ä¸å¤ªå¯èƒ½é‡åˆ°çš„ç®€åŒ–ç¤ºä¾‹ï¼Œä½†è¿™ä¸ªæƒ³æ³•æ˜¯æˆç«‹çš„ã€‚
- en: One possible approach to improve quality of chunks is to use context-aware text
    splitting, such as splitting by logical sections (as in our example of the book
    list). If any logical chunk is too big â€” such as Wikipedia pages on particular
    topics would be quite lengthy, they could be split further by logical sections
    or by semantic units such as by paragraphs, with a meaningful overlap between
    chunks, as well as ensuring the overall metadata and chunk specific metadata is
    passed to the LLM.
  id: totrans-171
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: æ”¹å–„å†…å®¹è´¨é‡çš„ä¸€ç§å¯èƒ½æ–¹æ³•æ˜¯ä½¿ç”¨ä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„æ–‡æœ¬æ‹†åˆ†ï¼Œä¾‹å¦‚æŒ‰é€»è¾‘éƒ¨åˆ†æ‹†åˆ†ï¼ˆå¦‚æˆ‘ä»¬ä¹¦å•çš„ç¤ºä¾‹ï¼‰ã€‚å¦‚æœä»»ä½•é€»è¾‘å—è¿‡å¤§â€”â€”ä¾‹å¦‚ï¼Œç»´åŸºç™¾ç§‘ä¸Šå…³äºæŸäº›ä¸»é¢˜çš„é¡µé¢å¯èƒ½éå¸¸é•¿â€”â€”å®ƒä»¬å¯ä»¥è¿›ä¸€æ­¥æŒ‰é€»è¾‘éƒ¨åˆ†æˆ–æŒ‰è¯­ä¹‰å•å…ƒï¼ˆå¦‚æ®µè½ï¼‰æ‹†åˆ†ï¼ŒåŒæ—¶åœ¨å—ä¹‹é—´ä¿æŒæœ‰æ„ä¹‰çš„é‡å ï¼Œå¹¶ç¡®ä¿å°†æ•´ä½“å…ƒæ•°æ®å’Œå—ç‰¹å®šçš„å…ƒæ•°æ®ä¼ é€’ç»™LLMã€‚
- en: '*Can we be confident that the LLMâ€™s responses will always be factually accurate.
    That is, will our solution â€˜hallucinateâ€™ when generating responses once in a while?*'
  id: totrans-172
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*æˆ‘ä»¬èƒ½å¦ç¡®ä¿¡LLMçš„å›ç­”æ€»æ˜¯äº‹å®å‡†ç¡®çš„ï¼Ÿä¹Ÿå°±æ˜¯è¯´ï¼Œæˆ‘ä»¬çš„è§£å†³æ–¹æ¡ˆä¼šä¸ä¼šåœ¨ç”Ÿæˆå›ç­”æ—¶å¶å°”â€˜å¹»è§‰â€™ï¼Ÿ*'
- en: 'A key selling point of RAG is to drive factuality. GPT 3.5 and GPT-4 are good
    at following this instruction: â€œrespond only from the provided context or say
    â€˜the question cannot be answered based on the information providedâ€™â€. This is
    hypothesized to be due to a lot of reinforcement learning from human feedback
    (RLHF) conducted by OpenAI. As a corollary, other LLMs might not currently be
    as good at following instructions. For a production application, especially an
    external facing one, it will be prudent to conduct a lot of testing aimed at validating
    that the generated output is faithful to the available context retrieved from
    the vector database, even though the LLM believes it to be the case. Approaches
    range from manual tests on samples, to using a powerful model such as GPT-4 to
    test samples of retrieved context and generated responses by other models, to
    using services and products such as [Galileo](https://www.rungalileo.io/) which
    focus on detecting LLM hallucinations in real-time.'
  id: totrans-173
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: RAGçš„ä¸€ä¸ªå…³é”®å–ç‚¹æ˜¯æ¨åŠ¨äº‹å®å‡†ç¡®æ€§ã€‚GPT 3.5å’ŒGPT-4åœ¨éµå¾ªæŒ‡ä»¤æ–¹é¢è¡¨ç°è‰¯å¥½ï¼šâ€œä»…ä»æä¾›çš„ä¸Šä¸‹æ–‡ä¸­å›åº”ï¼Œæˆ–è¯´â€˜åŸºäºæä¾›çš„ä¿¡æ¯æ— æ³•å›ç­”è¯¥é—®é¢˜â€™â€ã€‚è¿™è¢«æ¨æµ‹æ˜¯ç”±äºOpenAIè¿›è¡Œäº†å¤§é‡çš„åŸºäºäººç±»åé¦ˆçš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLHFï¼‰ã€‚ä½œä¸ºæ¨è®ºï¼Œå…¶ä»–LLMå¯èƒ½å½“å‰åœ¨éµå¾ªæŒ‡ä»¤æ–¹é¢è¡¨ç°ä¸ä½³ã€‚å¯¹äºç”Ÿäº§åº”ç”¨ï¼Œç‰¹åˆ«æ˜¯é¢å‘å¤–éƒ¨çš„åº”ç”¨ï¼Œè¿›è¡Œå¤§é‡æµ‹è¯•ä»¥éªŒè¯ç”Ÿæˆçš„è¾“å‡ºæ˜¯å¦å¿ å®äºä»å‘é‡æ•°æ®åº“æ£€ç´¢çš„å¯ç”¨ä¸Šä¸‹æ–‡ï¼Œå³ä½¿LLMç›¸ä¿¡è¿™æ˜¯æ­£ç¡®çš„ï¼Œä¹Ÿæ˜¯æ˜æ™ºçš„ã€‚æ–¹æ³•åŒ…æ‹¬å¯¹æ ·æœ¬è¿›è¡Œæ‰‹åŠ¨æµ‹è¯•ï¼Œä½¿ç”¨å¼ºå¤§çš„æ¨¡å‹å¦‚GPT-4æµ‹è¯•æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡æ ·æœ¬å’Œå…¶ä»–æ¨¡å‹ç”Ÿæˆçš„å“åº”ï¼Œæˆ–ä½¿ç”¨å¦‚[Galileo](https://www.rungalileo.io/)è¿™æ ·çš„æœåŠ¡å’Œäº§å“ï¼Œä¸“æ³¨äºå®æ—¶æ£€æµ‹LLMå¹»è§‰ã€‚
- en: Conclusion
  id: totrans-174
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ç»“è®º
- en: Had you known all of this 11 months ago, it would have justified a demonstration
    with the CEO of your company. Possibly even a TED talk to a wider audience. Today,
    this has become a part of AI literacy baseline, especially if you are involved
    in delivery of generative AI products. Hopefully, youâ€™re fairly caught up due
    to this exercise! ğŸ‘
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœä½ åœ¨11ä¸ªæœˆå‰å°±çŸ¥é“è¿™äº›å†…å®¹ï¼Œè¿™å°†å€¼å¾—ä¸ä½ å…¬å¸é¦–å¸­æ‰§è¡Œå®˜è¿›è¡Œä¸€æ¬¡æ¼”ç¤ºï¼Œç”šè‡³å¯èƒ½æœ‰ä¸€ä¸ªTEDæ¼”è®²å‘æ›´å¹¿æ³›çš„è§‚ä¼—ä»‹ç»ã€‚ä»Šå¤©ï¼Œè¿™å·²æˆä¸ºAIç´ å…»çš„åŸºæœ¬è¦æ±‚ï¼Œç‰¹åˆ«æ˜¯å¦‚æœä½ å‚ä¸ç”Ÿæˆå¼AIäº§å“çš„äº¤ä»˜ã€‚å¸Œæœ›é€šè¿‡è¿™æ¬¡ç»ƒä¹ ï¼Œä½ èƒ½æ¯”è¾ƒè·Ÿå¾—ä¸Šï¼ğŸ‘
- en: A few closing thoughts,
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€äº›ç»“æŸè¯­ï¼Œ
- en: There is serious promise in the technology â€” how many other technologies can
    â€œthinkâ€ to this degree, and can be used as â€œreasoning enginesâ€ (in the words of
    Dr. Andrew Ng [here](https://learn.deeplearning.ai/langchain/lesson/7/agents)).
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è¿™é¡¹æŠ€æœ¯å…·æœ‰å·¨å¤§çš„æ½œåŠ›â€”â€”è¿˜æœ‰å¤šå°‘å…¶ä»–æŠ€æœ¯èƒ½â€œæ€è€ƒâ€åˆ°è¿™ç§ç¨‹åº¦ï¼Œå¹¶ä¸”èƒ½ä½œä¸ºâ€œæ¨ç†å¼•æ“â€ï¼ˆç”¨å®‰å¾·é²Â·å´åšå£«çš„è¯è¯´ï¼Œå‚è§[è¿™é‡Œ](https://learn.deeplearning.ai/langchain/lesson/7/agents)ï¼‰ï¼Ÿ
- en: While frontier models (currently, GPT-4) will continue to advance, open source
    models and their domain-specific and task specific fine-tuned variants will be
    competitive on numerous tasks and will find many applications.
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è™½ç„¶å‰æ²¿æ¨¡å‹ï¼ˆç›®å‰ä¸ºGPT-4ï¼‰å°†ç»§ç»­è¿›æ­¥ï¼Œä½†å¼€æºæ¨¡å‹åŠå…¶é¢†åŸŸç‰¹å®šå’Œä»»åŠ¡ç‰¹å®šçš„å¾®è°ƒå˜ä½“åœ¨è®¸å¤šä»»åŠ¡ä¸­å°†å…·æœ‰ç«äº‰åŠ›ï¼Œå¹¶æ‰¾åˆ°è®¸å¤šåº”ç”¨ã€‚
- en: For better or worse, this cutting edge technology that took millions (hundreds
    of millions?) of dollars to develop is available for free â€” you could fill a form
    and download Metaâ€™s capable Llama2 model with a very permissive license. Nearly
    300,000 baseline LLMs or their fine-tuned variants are on HuggingFaceâ€™s model
    hub. Hardware is also commoditized.
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ— è®ºå¥½åï¼Œè¿™é¡¹è€—èµ„æ•°ç™¾ä¸‡ï¼ˆç”šè‡³æ•°äº¿ï¼Ÿï¼‰ç¾å…ƒå¼€å‘çš„å‰æ²¿æŠ€æœ¯ç°åœ¨æ˜¯å…è´¹çš„â€”â€”ä½ å¯ä»¥å¡«å†™ä¸€ä¸ªè¡¨æ ¼ï¼Œä¸‹è½½MetaåŠŸèƒ½å¼ºå¤§çš„Llama2æ¨¡å‹ï¼Œæ‹¥æœ‰éå¸¸å®½æ¾çš„è®¸å¯ã€‚HuggingFaceçš„æ¨¡å‹ä¸­å¿ƒå‡ ä¹æœ‰30ä¸‡ä¸ªåŸºç¡€LLMæˆ–å…¶å¾®è°ƒå˜ä½“ã€‚ç¡¬ä»¶ä¹Ÿå·²ç»å•†å“åŒ–ã€‚
- en: OpenAI models are now capable of being aware of and using â€œtoolsâ€ (functions,
    APIs, etc.), letting solutions interface with not just humans and databases, but
    with other programs. LangChain and other packages have already demonstrated using
    LLMs as the â€œbrainâ€ for autonomous agents that can accept input, decide what action
    to take, and follow through, repeating these steps until the agent reaches its
    goal. Our simple chatbot used two LLM calls in a deterministic sequence â€” generate
    standalone question, and synthesize search results into a coherent natural language
    response. Imagine what hundreds of calls to rapidly evolving LLMs with agentic
    autonomy can achieve!
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAIæ¨¡å‹ç°åœ¨èƒ½å¤Ÿè¯†åˆ«å¹¶ä½¿ç”¨â€œå·¥å…·â€ï¼ˆåŠŸèƒ½ã€APIç­‰ï¼‰ï¼Œä½¿å¾—è§£å†³æ–¹æ¡ˆä¸ä»…èƒ½ä¸äººç±»å’Œæ•°æ®åº“æ¥å£ï¼Œè¿˜èƒ½ä¸å…¶ä»–ç¨‹åºæ¥å£ã€‚LangChainå’Œå…¶ä»–è½¯ä»¶åŒ…å·²ç»å±•ç¤ºäº†å¦‚ä½•å°†LLMä½œä¸ºâ€œæ™ºèƒ½ä½“â€çš„â€œå¤§è„‘â€ï¼Œè¿™äº›æ™ºèƒ½ä½“èƒ½å¤Ÿæ¥å—è¾“å…¥ã€å†³å®šé‡‡å–çš„è¡ŒåŠ¨å¹¶æ‰§è¡Œï¼Œé‡å¤è¿™äº›æ­¥éª¤ç›´åˆ°æ™ºèƒ½ä½“å®ç°ç›®æ ‡ã€‚æˆ‘ä»¬çš„ç®€å•èŠå¤©æœºå™¨äººåœ¨ç¡®å®šæ€§åºåˆ—ä¸­ä½¿ç”¨äº†ä¸¤ä¸ªLLMè°ƒç”¨â€”â€”ç”Ÿæˆç‹¬ç«‹é—®é¢˜ï¼Œå¹¶å°†æœç´¢ç»“æœåˆæˆæˆè¿è´¯çš„è‡ªç„¶è¯­è¨€å“åº”ã€‚æƒ³è±¡ä¸€ä¸‹ï¼Œæ•°ç™¾æ¬¡å¯¹å¿«é€Ÿå‘å±•çš„LLMè¿›è¡Œçš„è°ƒç”¨ä¼šå–å¾—ä»€ä¹ˆæˆæœï¼
- en: These rapid advancements are a result of tremendous momentum around GenAI, and
    it will proliferate enterprises, and day-to-day life through our devices. First
    in simpler ways, but later on in increasingly sophisticated applications that
    leverage the reasoning and decision-making capability of the technology, blending
    it with traditional AI.
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è¿™äº›å¿«é€Ÿè¿›å±•æ˜¯ç”±äºGenAIå‘¨å›´çš„å·¨å¤§åŠ¨åŠ›ï¼Œå®ƒå°†é€šè¿‡æˆ‘ä»¬çš„è®¾å¤‡æ¸—é€åˆ°ä¼ä¸šå’Œæ—¥å¸¸ç”Ÿæ´»ä¸­ã€‚æœ€åˆæ˜¯ä»¥æ›´ç®€å•çš„æ–¹å¼ï¼Œä½†éšåå°†åœ¨åˆ©ç”¨æŠ€æœ¯çš„æ¨ç†å’Œå†³ç­–èƒ½åŠ›çš„è¶Šæ¥è¶Šå¤æ‚çš„åº”ç”¨ä¸­ä½“ç°ï¼Œä¸ä¼ ç»ŸAIèåˆã€‚
- en: Finally, now is a great time to get involved as the playing field is fairly
    level, at least for applying this technology â€” everyone is learning about this
    at more or less the same time since the ChatGPT boom in Dec 2022\. Things are
    of course different on the R&D side, with Big Tech companies that have spent years,
    and billions of dollars in developing this technology. Regardless, to build more
    sophisticated solutions later, itâ€™s the perfect time to get started now!
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æœ€ç»ˆï¼Œç°åœ¨æ˜¯ä¸€ä¸ªç»ä½³çš„æ—¶æœºæ¥å‚ä¸ï¼Œå› ä¸ºåº”ç”¨è¿™é¡¹æŠ€æœ¯çš„ç«äº‰ç¯å¢ƒç›¸å¯¹å…¬å¹³â€”â€”è‡ª2022å¹´12æœˆChatGPTçš„çˆ†å‘ä»¥æ¥ï¼Œæ¯ä¸ªäººåŸºæœ¬ä¸Šéƒ½åœ¨åŒä¸€æ—¶é—´å­¦ä¹ è¿™é¡¹æŠ€æœ¯ã€‚å½“ç„¶ï¼Œç ”å‘æ–¹é¢æƒ…å†µæœ‰æ‰€ä¸åŒï¼Œå¤§å‹ç§‘æŠ€å…¬å¸å·²ç»æŠ•å…¥äº†å¤šå¹´å’Œæ•°åäº¿ç¾å…ƒæ¥å¼€å‘è¿™é¡¹æŠ€æœ¯ã€‚å°½ç®¡å¦‚æ­¤ï¼Œä¸ºäº†å°†æ¥æ„å»ºæ›´å¤æ‚çš„è§£å†³æ–¹æ¡ˆï¼Œç°åœ¨æ­£æ˜¯å¼€å§‹çš„æœ€ä½³æ—¶æœºï¼
- en: Additional Resources
  id: totrans-183
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: é¢å¤–èµ„æº
- en: '**LangChain:** [Deeplearning.ai](http://deeplearning.ai/) course: [LangChain:
    Chat with Your Data](https://www.deeplearning.ai/short-courses/langchain-chat-with-your-data/)
    | LangChain [documentation](https://python.langchain.com/docs/get_started/introduction)'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**LangChainï¼š** [Deeplearning.ai](http://deeplearning.ai/)è¯¾ç¨‹ï¼š[LangChainï¼šä¸æ‚¨çš„æ•°æ®å¯¹è¯](https://www.deeplearning.ai/short-courses/langchain-chat-with-your-data/)
    | LangChain [æ–‡æ¡£](https://python.langchain.com/docs/get_started/introduction)'
- en: '**Gradio:** [Deeplearning.ai](https://www.deeplearning.ai/) course - [Building
    Generative AI Applications with Gradio](https://www.deeplearning.ai/short-courses/building-generative-ai-applications-with-gradio/)
    | Gradio documentation and [guides](https://www.gradio.app/guides)'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Gradioï¼š** [Deeplearning.ai](https://www.deeplearning.ai/)è¯¾ç¨‹ - [ä½¿ç”¨Gradioæ„å»ºç”Ÿæˆæ€§AIåº”ç”¨](https://www.deeplearning.ai/short-courses/building-generative-ai-applications-with-gradio/)
    | Gradioæ–‡æ¡£å’Œ[æŒ‡å—](https://www.gradio.app/guides)'
- en: I have found [Shawhin Talebi](https://medium.com/u/f3998e1cd186?source=post_page-----6ee6ad94e058--------------------------------)â€™s
    articles very instructive. See [Cracking Open the OpenAI (Python) API](/cracking-open-the-openai-python-api-230e4cae7971),
    [Cracking Open the Hugging Face Transformers Library](/cracking-open-the-hugging-face-transformers-library-350aa0ef0161),
    and other recent articles.
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æˆ‘å‘ç°[Shawhin Talebi](https://medium.com/u/f3998e1cd186?source=post_page-----6ee6ad94e058--------------------------------)çš„æ–‡ç« éå¸¸å…·æœ‰å¯å‘æ€§ã€‚è¯·å‚é˜…[ç ´è§£OpenAI
    (Python) API](/cracking-open-the-openai-python-api-230e4cae7971)ï¼Œ[ç ´è§£Hugging Face
    Transformersåº“](/cracking-open-the-hugging-face-transformers-library-350aa0ef0161)ä»¥åŠå…¶ä»–è¿‘æœŸæ–‡ç« ã€‚
- en: '[LLMs in Production: Learning from Experience, Dr. Waleed Kadous, Chief Scientist,
    AnyScale](https://youtu.be/xa7k9MUeIdk?si=LQizYwFt4m-XOYpk)'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[LLMsåœ¨ç”Ÿäº§ä¸­çš„åº”ç”¨ï¼šä»ç»éªŒä¸­å­¦ä¹ ï¼ŒAnyScaleé¦–å¸­ç§‘å­¦å®¶Dr. Waleed Kadous](https://youtu.be/xa7k9MUeIdk?si=LQizYwFt4m-XOYpk)'
- en: 'This talk by Jerry Liu, co-founfer of LlamaIndex outlines various approaches
    for output evaluation: [Practical Data Considerations for Building Production-Ready
    LLM Applications](https://youtu.be/xbeFAZl3uCk?si=XBpo6cWt0Z9P9v_w)'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç”±LlamaIndexçš„è”åˆåˆ›å§‹äººJerry Liuæ‰€åšçš„æ¼”è®²æ¦‚è¿°äº†è¾“å‡ºè¯„ä¼°çš„å„ç§æ–¹æ³•ï¼š[æ„å»ºç”Ÿäº§å°±ç»ªçš„LLMåº”ç”¨çš„å®ç”¨æ•°æ®è€ƒè™‘](https://youtu.be/xbeFAZl3uCk?si=XBpo6cWt0Z9P9v_w)
