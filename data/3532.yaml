- en: Hands-On GenAI for Product & Engineering Leaders
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 针对产品和工程领导者的动手 GenAI
- en: 原文：[https://towardsdatascience.com/hands-on-genai-for-product-engineering-leaders-6ee6ad94e058?source=collection_archive---------10-----------------------#2023-11-28](https://towardsdatascience.com/hands-on-genai-for-product-engineering-leaders-6ee6ad94e058?source=collection_archive---------10-----------------------#2023-11-28)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/hands-on-genai-for-product-engineering-leaders-6ee6ad94e058?source=collection_archive---------10-----------------------#2023-11-28](https://towardsdatascience.com/hands-on-genai-for-product-engineering-leaders-6ee6ad94e058?source=collection_archive---------10-----------------------#2023-11-28)
- en: Make better product decisions by taking a peek under the hood of LLM-based products
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 通过深入了解基于LLM的产品，做出更好的产品决策
- en: '[](https://medium.com/@ninadsohoni?source=post_page-----6ee6ad94e058--------------------------------)[![Ninad
    Sohoni](../Images/8d6ec40665bb85fb7b4ece99e6a40913.png)](https://medium.com/@ninadsohoni?source=post_page-----6ee6ad94e058--------------------------------)[](https://towardsdatascience.com/?source=post_page-----6ee6ad94e058--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----6ee6ad94e058--------------------------------)
    [Ninad Sohoni](https://medium.com/@ninadsohoni?source=post_page-----6ee6ad94e058--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@ninadsohoni?source=post_page-----6ee6ad94e058--------------------------------)[![Ninad
    Sohoni](../Images/8d6ec40665bb85fb7b4ece99e6a40913.png)](https://medium.com/@ninadsohoni?source=post_page-----6ee6ad94e058--------------------------------)[](https://towardsdatascience.com/?source=post_page-----6ee6ad94e058--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----6ee6ad94e058--------------------------------)
    [Ninad Sohoni](https://medium.com/@ninadsohoni?source=post_page-----6ee6ad94e058--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F5ee93978501b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhands-on-genai-for-product-engineering-leaders-6ee6ad94e058&user=Ninad+Sohoni&userId=5ee93978501b&source=post_page-5ee93978501b----6ee6ad94e058---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----6ee6ad94e058--------------------------------)
    ·35 min read·Nov 28, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F6ee6ad94e058&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhands-on-genai-for-product-engineering-leaders-6ee6ad94e058&user=Ninad+Sohoni&userId=5ee93978501b&source=-----6ee6ad94e058---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F5ee93978501b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhands-on-genai-for-product-engineering-leaders-6ee6ad94e058&user=Ninad+Sohoni&userId=5ee93978501b&source=post_page-5ee93978501b----6ee6ad94e058---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----6ee6ad94e058--------------------------------)
    · 35 分钟阅读 · 2023年11月28日 [](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F6ee6ad94e058&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhands-on-genai-for-product-engineering-leaders-6ee6ad94e058&user=Ninad+Sohoni&userId=5ee93978501b&source=-----6ee6ad94e058---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F6ee6ad94e058&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhands-on-genai-for-product-engineering-leaders-6ee6ad94e058&source=-----6ee6ad94e058---------------------bookmark_footer-----------)![](../Images/4b2f853b2bede6542039512f44e4c4c2.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F6ee6ad94e058&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhands-on-genai-for-product-engineering-leaders-6ee6ad94e058&source=-----6ee6ad94e058---------------------bookmark_footer-----------)![](../Images/4b2f853b2bede6542039512f44e4c4c2.png)'
- en: Image generated by [Bing Image Creator](https://www.bing.com/create) based on
    the prompt “product owner for a machine learning powered application working on
    a prototype”
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由 [Bing Image Creator](https://www.bing.com/create) 根据提示“为机器学习驱动的应用程序工作中的产品所有者”生成
- en: Introduction
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: If you’re a regular driver, the hood of your car could be full of cotton for
    all you care. However, if you are anywhere in the design and execution chain responsible
    for building a better car, knowing what the different parts are and how they work
    together will help you build a better car.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你是一个普通司机，你可能不在乎你车的引擎盖下是什么。然而，如果你是设计和执行链条中的一部分，负责打造更好的汽车，了解不同部件是什么以及它们如何协同工作，将帮助你打造更好的汽车。
- en: Similarly, as a product owner, business leader, or an engineer responsible for
    creating new Large Language Model (LLM) powered products, or for bringing LLMs
    / generative AI to existing products, an understanding of building blocks that
    go into an LLM-powered products will help you tackle strategic and tactical questions
    pertaining to technology, such as,
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，作为产品负责人、业务领导或负责创建新大型语言模型（LLM）驱动产品的工程师，或将LLM/生成性AI引入现有产品的工程师，了解LLM驱动产品的构建模块将帮助你解决与技术相关的战略和战术问题，例如，
- en: Is our use case a good fit for LLM-powered solutions? Perhaps traditional analytics,
    supervised machine learning, or another approach is a better fit?
  id: totrans-12
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们的使用案例是否适合LLM驱动的解决方案？也许传统的分析、监督式机器学习或其他方法更合适？
- en: If LLMs are the way to go, can our use case be addressed by an off-the-shelf
    product (say, ChatGPT Enterprise) now or in the near-future? Classic build-vs-buy
    decision.
  id: totrans-13
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果LLM是可行的，我们的使用案例现在或在不久的将来是否可以通过现成的产品（比如ChatGPT Enterprise）来解决？这是经典的构建与购买决策。
- en: What are the different building blocks of our LLM-powered product? Which of
    these are commoditized, and which are likely to need more time to build and test?
  id: totrans-14
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们的LLM驱动产品的不同构建模块有哪些？其中哪些已经商品化，哪些可能需要更多时间来构建和测试？
- en: How do we measure the performance of our solution? What levers are available
    to improve the quality of outputs from our product?
  id: totrans-15
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们如何衡量解决方案的性能？有哪些杠杆可以提高我们产品输出的质量？
- en: Is our data quality acceptable for the use case? Are we organizing our data
    correctly, and passing relevant data to the LLM?
  id: totrans-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们的数据质量是否符合使用案例的要求？我们是否正确地组织了数据，并将相关数据传递给了LLM？
- en: Can we be confident that the LLM’s responses will always be factually accurate.
    That is, will our solution ‘hallucinate’ when generating responses once in a while?
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们能否确信LLM的回答始终是事实准确的？也就是说，我们的解决方案是否会在生成回答时偶尔出现“幻觉”？
- en: While these questions are answered later in the article, the objective with
    getting a little hands-on is to build an intuitive understanding of LLM-powered
    solutions, which should help you answer these questions on your own, or at least
    put you in a better position to research further.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这些问题在文章后面会得到回答，但通过动手实践的目标是建立对LLM驱动解决方案的直观理解，这应该有助于你自己回答这些问题，或者至少让你更好地进行进一步研究。
- en: In a [previous article](/how-genai-solutions-revolutionize-business-automation-57747b0f11ce),
    I delved into some foundational concepts associated with building LLM-powered
    products. But you can’t learn to drive just by reading blogs or watching videos
    — it requires you to get behind the wheel. Well, thanks to the age we live in,
    we have free-to-us tools (which [cost millions of dollars to create](https://lambdalabs.com/blog/demystifying-gpt-3))
    at our fingertips to build our own LLM solution in under an hour! So, in this
    article, I propose we do just that. It’s a much easier undertaking than learning
    to drive 😝.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在一篇[上一篇文章](/how-genai-solutions-revolutionize-business-automation-57747b0f11ce)中，我**深入探讨**了与构建LLM驱动产品相关的一些基础概念。但你不能仅仅通过阅读博客或观看视频来学会驾驶——这需要你亲自上路。好在我们生活的时代提供了免费的工具（这些工具的创建[花费了数百万美元](https://lambdalabs.com/blog/demystifying-gpt-3)），我们可以在不到一小时的时间里构建自己的LLM解决方案！因此，在这篇文章中，我建议我们就这样做。这比学习驾驶要容易得多😝。
- en: '**Build a Chatbot that allows you to “chat” with websites**'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '**构建一个允许你与网站“聊天”的聊天机器人**'
- en: 'Objective: Build a chatbot that answers questions based on information on a
    provided website, **to gain a better understanding of the building blocks** of
    popular GenAI solutions today'
  id: totrans-21
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 目标：构建一个基于提供的网站信息回答问题的聊天机器人，**以更好地理解**当前流行的GenAI解决方案的构建模块
- en: We will create a question-answering chatbot that will answer questions based
    on information in a knowledge repository. This solution pattern, called Retrieval
    Augmented Generation (RAG), has become a go-to solution pattern in companies.
    One reason for the popularity of RAG is that rather than relying solely on the
    LLMs own knowledge, you can bring external information to the LLM in an automated
    manner. In real-world implementations, the external information can be from an
    organization’s own knowledge repository, holding proprietary information to enable
    the product to answer questions about the business, its products, business processes,
    etc. RAG also reduces LLM ‘hallucinations’, in that the generated responses are
    grounded in the information provided to the LLM. According to a [recent talk](https://www.youtube.com/watch?v=xa7k9MUeIdk),
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将创建一个基于知识库信息回答问题的问答聊天机器人。这种解决方案模式，称为检索增强生成（RAG），已成为公司中的首选解决方案模式。RAG 之所以受欢迎的一个原因是，它不仅依赖于
    LLM 自身的知识，还可以以自动化的方式将外部信息带入 LLM。在实际应用中，外部信息可以来自组织自己的知识库，包含专有信息，以使产品能够回答有关业务、产品、业务流程等问题。RAG
    还减少了 LLM 的“幻觉”，即生成的响应是基于提供给 LLM 的信息的。根据 [最近的一次演讲](https://www.youtube.com/watch?v=xa7k9MUeIdk)，
- en: “RAG will the the default way enterprises use LLMs”
  id: totrans-23
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “RAG 将是企业使用 LLM 的默认方式”
- en: -Dr. Waleed Kadous, Chief Scientist, AnyScale
  id: totrans-24
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: -Dr. Waleed Kadous, Chief Scientist, AnyScale
- en: For our hands-on exercise, we will let a user enter a website, which our solution
    will “read” into its knowledge repository. The solution will then be able to answer
    questions based on the information on the website. The website is a placeholder
    — in reality, this can be tweaked to consume text from any data source like PDFs,
    Excel, another product or internal system, etc. This approach works for other
    media — such as images — but they require a few different LLMs. For now, we will
    focus on text from websites.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的实操练习中，我们将允许用户输入一个网站，我们的解决方案将“读取”该网站到其知识库中。然后，解决方案将能够根据网站上的信息回答问题。这个网站是一个占位符——实际上，可以调整为从任何数据源如
    PDFs、Excel、其他产品或内部系统等获取文本。这种方法也适用于其他媒体——如图像——但它们需要一些不同的 LLM。目前，我们将重点关注来自网站的文本。
- en: 'For our example, we will use a sample book list webpage created for this blog:
    [Books I’d Pick Up — If There Were More Hours in the Day!](https://ninadsohoni.github.io/booklist/)
    You are welcome to use another website of your choice.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 作为示例，我们将使用为本博客创建的示例书单网页：[Books I’d Pick Up — If There Were More Hours in the
    Day!](https://ninadsohoni.github.io/booklist/) 您也可以使用您选择的其他网站。
- en: 'Here’s what our result will look like:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是我们的结果的样子：
- en: '![](../Images/781fc874901946c9c92cb4c6ab0f0984.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/781fc874901946c9c92cb4c6ab0f0984.png)'
- en: LLM-powered chatbot to intelligently answer questions based on information on
    a website. (Image by the author)
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: LLM 驱动的聊天机器人可以根据网站上的信息智能回答问题。（图像由作者提供）
- en: 'Here are the steps we will go through to build our solution:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是我们将遵循的步骤来构建我们的解决方案：
- en: 0\. Getting Set Up — Google Colaboratory & OpenAI API Key
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 0\. 设置 — Google Colaboratory 和 OpenAI API 密钥
- en: 1\. Create knowledge repository
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 1\. 创建知识库
- en: 2\. Search question-relevant context
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 2\. 搜索与问题相关的上下文
- en: 3\. Generate answer using LLM
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 3\. 使用 LLM 生成答案
- en: 4\. Add “chat” capability (optional)
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 4\. 添加“聊天”功能（可选）
- en: 5\. Add a simple pre-coded UI (optional)
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 5\. 添加一个简单的预编码 UI（可选）
- en: '**0.1\. Getting Set Up — Google Colaboratory & OpenAI API Key**'
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**0.1\. 设置 — Google Colaboratory 和 OpenAI API 密钥**'
- en: To build a LLM solution, we need a place to write and run code, and an LLM to
    generate responses to questions. We will use Google Colab for the code environment,
    and the model behind ChatGPT as our LLM.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 要构建一个 LLM 解决方案，我们需要一个编写和运行代码的地方，以及一个生成问题回答的 LLM。我们将使用 Google Colab 作为代码环境，并使用
    ChatGPT 背后的模型作为我们的 LLM。
- en: Let’s start with setting up [Google Colab](https://colab.google/), a free service
    by Google that enables running Python code in an easy-to-read format — no need
    to install anything on your computer. I find it convenient to add Colab to Google
    Drive so that I can later find Colab notebooks easily.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 首先设置 [Google Colab](https://colab.google/)，这是一个由 Google 提供的免费服务，可以以易于阅读的格式运行
    Python 代码 — 无需在计算机上安装任何东西。我发现将 Colab 添加到 Google Drive 中很方便，这样我就可以轻松找到 Colab 笔记本。
- en: To do so, navigate to **Google Drive** (using a browser) **> New > More > Connect
    More Apps >** Search **“Colaboratory”** in the Google Marketplace **> Install.**
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 为此，导航至 **Google Drive**（使用浏览器） **> 新建 > 更多 > 连接更多应用 >** 在 Google Marketplace
    中 **搜索 “Colaboratory”** **> 安装。**
- en: To start using Colabobatory (“Colab”), you can select **New** > **More** > **Google
    Colaboratory.** This will create a new notebook in your Google Drive so you can
    go back to it.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 要开始使用 Colab，你可以选择 **新建** > **更多** > **Google Colaboratory**。这将会在你的 Google 云端硬盘中创建一个新的笔记本，方便你返回继续使用。
- en: '![](../Images/ee514c154f891d1fd129b21195a698bd.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ee514c154f891d1fd129b21195a698bd.png)'
- en: Google Colaboratory accessible in Google Drive. (Image by the author)
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: Google Colaboratory 可以在 Google 云端硬盘中访问。（图片由作者提供）
- en: Next, let’s get access to an LLM. There are several open source and proprietary
    options available. While open source LLMs are free, powerful LLMs generally require
    powerful GPUs to process inputs and generate responses, and there is a nominal
    operate cost for GPUs. In our example, we will instead use OpenAI’s service to
    use the LLM used by ChatGPT. To do so, you will require an API key, which is like
    a username/password rolled into one to let OpenAI know who is trying to access
    the LLM. As of this writing, OpenAI offered a $5 credit for new users, which should
    be sufficient for this hands-on tutorial. Here are the steps to get the API key,
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们获取对 LLM 的访问权限。有几个开源和专有选项可供选择。虽然开源 LLM 是免费的，但强大的 LLM 通常需要强大的 GPU 来处理输入和生成响应，且
    GPU 的运行成本较低。在我们的示例中，我们将使用 OpenAI 的服务来使用 ChatGPT 所用的 LLM。为此，你需要一个 API 密钥，它类似于用户名/密码的组合，用以让
    OpenAI 知道是谁在尝试访问 LLM。根据此时的信息，OpenAI 为新用户提供了 $5 的信用额度，足以用于本实践教程。以下是获取 API 密钥的步骤：
- en: Go to [**OpenAI’s Platform website**](https://platform.openai.com/signup/)>
    **Get started > Sign up** with email & password or use Google or Microsoft account.
    You may also need a phone number to verification.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 访问[**OpenAI 平台网站**](https://platform.openai.com/signup/)> **开始使用 > 注册**，使用电子邮件和密码进行注册，或使用
    Google 或 Microsoft 帐户注册。你还可能需要一个电话号码进行验证。
- en: Once logged in, click on your profile icon in the top right corner > **View
    API keys** > **Create new secret key**. The key will look something like the following
    (fake key for informational purposes only). Save it for use later.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 登录后，点击右上角的个人资料图标 > **查看 API 密钥** > **创建新密钥**。密钥将类似于以下内容（仅供参考的假密钥）。请保存以备后用。
- en: '[PRE0]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Now, we are ready to build the solution.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们已经准备好构建解决方案了。
- en: 0.2\. Prepare Notebook for Building Solution
  id: totrans-49
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 0.2\. 准备构建解决方案的笔记本
- en: 'We need to install some packages in the Colab environment to facilitate our
    solution. Just type the following code in the text box (called a “cell”) in Colab
    and press “Shift + Return (enter)”. Alternatively, just click the “play” button
    on the left of the cell or use the “Run” menu at the top of the notebook. You
    may need to use the menu to insert new code cells for running subsequent code:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要在 Colab 环境中安装一些软件包以方便我们的解决方案。只需在 Colab 中的文本框（称为“单元格”）中输入以下代码，然后按“Shift +
    Return（Enter）”。或者，直接点击单元格左侧的“播放”按钮或使用笔记本顶部的“运行”菜单。你可能需要使用菜单插入新的代码单元格以运行后续代码：
- en: '[PRE1]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Next, we should pull in code from the packages we installed so that the packages
    can be used in the code we write. You can use the new code cell and hit “Shift
    + Return” again — and continue in this manner for each subsequent code block.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们应该从已安装的软件包中提取代码，以便在编写的代码中使用这些软件包。你可以使用新的代码单元格并再次按“Shift + Return” — 以这种方式继续进行每个后续的代码块。
- en: '[PRE2]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Finally, add the OpenAI API key to a variable. Note that this key is like your
    password — do not share it. Also, do not share your Colab notebook without removing
    the API key first.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，将 OpenAI API 密钥添加到一个变量中。请注意，这个密钥类似于你的密码 — 请勿分享。此外，在分享你的 Colab 笔记本前，请务必先删除
    API 密钥。
- en: '[PRE3]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Now we are ready to start building the solution. Here is a high-level view
    of the next steps:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们准备开始构建解决方案。以下是接下来步骤的高级视图：
- en: '![](../Images/f4773785ae6fd1047c8a8c724bb381bf.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f4773785ae6fd1047c8a8c724bb381bf.png)'
- en: Core steps to build the RAG solution (Image by the author)
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 构建 RAG 解决方案的核心步骤（图片由作者提供）
- en: When coding, we will use LangChain, which has emerged as a popular framework
    to build solutions such as this one. It has packages for facilitating each of
    the steps from connecting to data sources to sending and receiving information
    from the LLM. LlamaIndex is another option to simplify building LLM-powered apps.
    While it’s not strictly required to use LangChain (or LlamaIndex), and in some
    cases the high-level abstraction may make has the risk of leaving teams oblivious
    to what’s happening under the hood, we will use LangChain but still look under
    the hood often.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在编码时，我们将使用LangChain，它已经成为构建此类解决方案的流行框架。它有助于从连接数据源到发送和接收LLM信息的每个步骤。LlamaIndex是另一个简化构建LLM驱动应用的选项。虽然并不严格要求使用LangChain（或LlamaIndex），并且在某些情况下，高级抽象可能使团队对内部发生的事情一无所知，但我们将使用LangChain，但仍会经常查看内部情况。
- en: Note that since the pace of innovation is so quick, it is likely that packages
    used in this code get updated, and some updates may cause the code to stop working
    unless updated accordingly. I do not intend to keep this code up-to-date. Nevertheless,
    the article is intended to serve as a demonstration, and the code could serve
    as reference, or a starting point that you may adapt to your needs.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，由于创新的速度如此之快，可能会有代码中使用的包更新，有些更新可能会导致代码停止工作，除非相应地进行更新。我不打算保持代码的最新状态。然而，本文旨在作为演示，代码可以作为参考或起点，您可以根据需要进行调整。
- en: 1\. Create Knowledge Repository
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1\. 创建知识库
- en: '**1.1\. Identify & Read-in Documents** Let’s access the book list and read
    the content into our Colab environment. The content is loaded as HTML originally,
    which is useful for web-browsers. However, we will convert it to a more human
    readable format using a HTML to Text convertor.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '**1.1\. 确定并读取文档** 让我们访问书单并将内容读取到我们的Colab环境中。内容最初以HTML格式加载，这对网页浏览器很有用。然而，我们将使用HTML转文本工具将其转换为更易读的格式。'
- en: '[PRE4]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Here is what running the code generates on Google Colab:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是运行代码在Google Colab上生成的内容：
- en: '![](../Images/de6416c74c20602de4edbb11a39d23f5.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/de6416c74c20602de4edbb11a39d23f5.png)'
- en: Result of executing the code above . The website content is loaded into Colab
    environment. (Image by the author)
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 执行上述代码的结果。网站内容被加载到Colab环境中。（图片由作者提供）
- en: '**1.2\. Break Documents into Smaller Excerpts** There is one more step before
    we load the blog’s information into our knowledge repository (which is essentially
    a database of our choice). The text should not be loaded into the database as-is.
    It should first be split into smaller chunks. This is for a few reasons:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '**1.2\. 将文档分解为较小的摘录** 在我们将博客的信息加载到我们的知识库（即我们选择的数据库）之前，还有一步。文本不应按原样加载到数据库中。它应首先分割成较小的块。这有几个原因：'
- en: If our text is too long, it cannot be sent to the LLM due to exceeding the text
    length threshold (known as *“context size”*).
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果我们的文本太长，由于超出文本长度阈值（即*“上下文大小”*），则不能发送给LLM。
- en: Longer text might have broad, loosely related information. We would be relying
    on the LLM to pick out the relevant portions — and this might not always work
    out as expected. With smaller chunks, we could use retrieval mechanisms to identify
    only the relevant pieces of information to send to the LLM, as we will see later.
  id: totrans-69
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 较长的文本可能包含广泛且松散相关的信息。我们将依赖LLM挑选出相关部分——这可能不会总是如预期那样有效。使用较小的块，我们可以利用检索机制来识别仅相关的信息并发送给LLM，如我们后面将看到的。
- en: LLMs are prone to have stronger attention at the beginning and end of text,
    so longer chunks could lead the LLM to pay less attention to more content later
    (known as *“lost in the middle”*).
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: LLM对文本的开始和结束部分关注较强，因此较长的块可能导致LLM对后面更多的内容关注较少（称为*“在中间迷失”*）。
- en: The right chunk sizes for each use case will vary per the specifics of the use
    case, including the type of content, the LLM being used, and other factors. It
    is prudent to experiment with different chunk sizes and evaluate response quality
    before finalizing the solution. For this demonstration, let’s use context-aware
    splitting where each book recommendation from the list gets its own chunk,
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 每个用例的适当块大小将根据用例的具体情况而有所不同，包括内容类型、使用的LLM及其他因素。明智的做法是尝试不同的块大小并在确定解决方案之前评估响应质量。在本演示中，我们将使用上下文感知的分块，其中书单中的每个书籍推荐都有自己的块。
- en: '[PRE5]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '![](../Images/ca21a3be9e3be635cf3f4fdb21f09bd4.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ca21a3be9e3be635cf3f4fdb21f09bd4.png)'
- en: One of many document chunks as a result of splitting the original content. (Image
    by the author)
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 原始内容拆分后的众多文档块之一。（图片由作者提供）
- en: Note that if the chunks created so far are still longer than desired, they can
    be split further using other text splitting algorithms, easily available via LangChain
    or LlamaIndex. For example, each book’s review could be split into paragraphs,
    if needed.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，如果迄今为止创建的文本块仍然比所需的长度长，可以使用其他文本拆分算法进一步拆分，这些算法可以通过 LangChain 或 LlamaIndex
    轻松获得。例如，每本书的评论可以根据需要拆分为段落。
- en: '**1.3\. Load Excerpts into Knowledge Repository** The text chunks are now ready
    to be loaded to the knowledge repository. These are first passed through an embedding
    model to convert the text to a series of numbers that capture the meaning of the
    text. Then the actual text along with the numerical representation (i.e., embeddings)
    will be loaded to the vector database — our knowledge repository. Note that embeddings
    are also generated by LLMs, just a different kind than the chat LLM. If you wish
    to read more about embeddings, the [previous article](/how-genai-solutions-revolutionize-business-automation-57747b0f11ce)
    demonstrates the concept using examples.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '**1.3\. 将摘录加载到知识库** 文本块现在准备好加载到知识库中。这些文本块首先通过嵌入模型转换为一系列捕捉文本意义的数字。然后，实际的文本及其数值表示（即嵌入）将加载到向量数据库——我们的知识库中。请注意，嵌入也由大语言模型（LLMs）生成，只是与聊天
    LLM 的类型不同。如果你想了解更多关于嵌入的内容，[上一篇文章](/how-genai-solutions-revolutionize-business-automation-57747b0f11ce)通过示例展示了这一概念。'
- en: We will use a vector database to store all the information. This will manifest
    our knowledge repository. Vector databases are purpose-built to enable searching
    by embedding similarity. If we want to search something from the database, the
    search term is converted to a numerical representation by running it through the
    embedding model first, and then the question embeddings are compared to all of
    the embeddings in the database. Records (in our case, text chunks about each book
    on the list) that are closest to the question embeddings are returned as search
    results, as long as they clear a threshold.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用向量数据库来存储所有信息。这将实现我们的知识库。向量数据库是专门设计用于通过嵌入相似性进行搜索的。如果我们想从数据库中搜索某些内容，搜索词首先通过嵌入模型转换为数值表示，然后将问题的嵌入与数据库中的所有嵌入进行比较。与问题嵌入最接近的记录（在我们的例子中，就是关于每本书的文本块）作为搜索结果返回，只要它们超过了一个阈值。
- en: '[PRE6]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '![](../Images/06255fabddafae0036919bdeaa6d2fdf.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/06255fabddafae0036919bdeaa6d2fdf.png)'
- en: A view of the first few text chunks loaded to the vector DB along with numerical
    representations (i.e., embeddings). (Image by the author)
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 加载到向量数据库中的前几个文本块及其数值表示（即嵌入）的视图。（图片由作者提供）
- en: 2\. Search Question-Relevant Context
  id: totrans-81
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2\. 搜索问题相关的上下文
- en: We ultimately want our solution to pick out relevant information from our vector
    DB knowledge corpus and pass it along to the LLM along with the question we want
    the LLM to answer. Let’s try out the vector DB search, by asking the question
    “Can you recommend a few detective novels?”
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的终极目标是让我们的解决方案从向量数据库知识库中挑选相关信息，并将其与我们希望 LLM 回答的问题一起传递给 LLM。让我们尝试一下向量数据库搜索，询问问题“你能推荐几本侦探小说吗？”
- en: '[PRE7]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '![](../Images/15bebbaf8b218ae61b888fe54953a1f3.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/15bebbaf8b218ae61b888fe54953a1f3.png)'
- en: Top search results for the question “Can you recommend a few detective novels?”
    (Image by the author)
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 问题“你能推荐几本侦探小说吗？”的搜索结果前 4 名（图片由作者提供）
- en: We get top 4 results by default, unless we explicitly set the value to a different
    number. In this example, the top search, which is a Sherlock Holmes novel, mentions
    the term ‘detective’ directly. The second result (*The Day of the Jackal*) does
    not have the term ‘detective’, but mentions ‘police agencies’ and ‘uncover the
    plot’, which bear semantic association with “detective novels”. The third result
    (*The Undercover Economist*) mentions the term ‘undercover’, though it is about
    economics. I believe the last result was fetched due to its association with novels
    / books rather than “detective novels” specifically, because four results were
    requested.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，我们得到前 4 个结果，除非我们明确设置不同的值。在这个例子中，排名第一的结果是一本福尔摩斯小说，直接提到了“侦探”一词。第二个结果（*《杰克尔的日子》*）虽然没有“侦探”一词，但提到了“警察机构”和“揭露阴谋”，这些与“侦探小说”在语义上相关。第三个结果（*《卧底经济学家》*）提到了“卧底”一词，尽管它是关于经济学的。我认为最后一个结果被获取是因为它与小说/书籍有关，而不是特定的“侦探小说”，因为请求了四个结果。
- en: Also, it is not strictly necessary to use a vector DB. You could load embeddings
    and facilitate search in other forms of storage. “Normal” relational databases
    or even Excel can be used. But you would have to handle the “similarity” calculation,
    which can be a dot product when using OpenAI embeddings, in your application logic.
    On the other hand, a vector DB does that for you.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，使用向量数据库并不是绝对必要的。您可以加载嵌入并在其他存储形式中进行搜索。“普通”关系数据库甚至 Excel 都可以使用。但您需要在应用程序逻辑中处理“相似性”计算，当使用
    OpenAI 嵌入时，这可以是点积。另一方面，向量数据库为您处理了这些。
- en: Note that if we wanted to pre-filter some search results by metadata, we could
    do so. For our demonstration, let’s filter by the genre, which is under “Header
    2” in the metadata we loaded from the book list.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，如果我们想通过元数据预筛选一些搜索结果，我们可以这样做。为了演示，我们将根据元数据中从书单加载的“Header 2”过滤。
- en: '[PRE8]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '![](../Images/3e7381e8da847e76b51f202714d7e319.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3e7381e8da847e76b51f202714d7e319.png)'
- en: Search results based on applying a metadata pre-filter, showing only key columns.
    (Image by the author)
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 基于应用元数据预筛选的搜索结果，仅显示关键列。（图像由作者提供）
- en: An interesting opportunity offered by LLMs is to use the LLM itself to inspect
    a user question, review available metadata, assess whether a metadata-based pre-filter
    is required and possible, and formulate the pre-filter query code, which can then
    be used on the vector DB to actually pre-filter data. See LangChain’s [self-query
    retriever](https://python.langchain.com/docs/modules/data_connection/retrievers/self_query/)
    for more information about this.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: LLM 提供了一个有趣的机会，即利用 LLM 本身来检查用户问题，审查可用的元数据，评估是否需要和可能进行基于元数据的预筛选，并制定预筛选查询代码，这些代码可以在向量数据库上实际预筛选数据。有关更多信息，请参见
    LangChain 的 [自查询检索器](https://python.langchain.com/docs/modules/data_connection/retrievers/self_query/)。
- en: 3\. Generate Answer using LLM
  id: totrans-93
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3\. 使用 LLM 生成答案
- en: Next, we will add instructions to the LLM that basically say “I am going to
    give you some information snippets, and a question. Please answer the question
    using the provided information snippets”. Then, we bundle these instructions,
    the search results from the vector DB, and our question into a packet and send
    it to the LLM to respond. All these steps are facilitated by the following code.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将向 LLM 添加指令，基本上是说“我将给你一些信息片段和一个问题。请使用提供的信息片段回答问题”。然后，我们将这些指令、向量数据库中的搜索结果和我们的问题打包，并发送给
    LLM 进行回应。所有这些步骤都由以下代码完成。
- en: Note that LangChain offers the opportunity to abstract some of this code, so
    your code doesn’t have to be as verbose as the code that follows. However, the
    objective with the code below is to showcase the instructions sent to the language
    model. Here’s where you can customize them too — like in this case, the default
    instructions are changed to request the LLM to keep responses as concise as possible.
    If the default works for your use case, your code can skip the question template
    part altogether and LangChain will use the default prompt from its own package
    when sending a request to the LLM.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，LangChain 提供了抽象一些代码的机会，因此您的代码不必像以下代码那样冗长。然而，以下代码的目的是展示发送到语言模型的指令。这里也是自定义这些指令的地方——例如，在这种情况下，默认指令被更改为请求
    LLM 尽可能简洁地回应。如果默认设置适用于您的用例，您的代码可以完全跳过问题模板部分，LangChain 会在向 LLM 发送请求时使用其包中的默认提示。
- en: '[PRE9]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Now, let’s ask for detective novel recommendations again and see what we get
    as a response.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们再次请求侦探小说的推荐，看看我们会得到什么回应。
- en: '[PRE10]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '![](../Images/546144d38d406b1c88712edd0333f838.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/546144d38d406b1c88712edd0333f838.png)'
- en: Response from the solution recommending detective novels (Image by the author)
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 推荐侦探小说的解决方案回应（图像由作者提供）
- en: Let’s confirm whether the model reviewed all four of our previous search results
    from the vector DB, or did it just get the two results noted in the response?
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们确认模型是否回顾了我们从向量数据库中获得的所有四个搜索结果，还是仅仅获取了响应中提到的两个结果？
- en: '[PRE11]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '![](../Images/5f951dd421f7adbca2edefd17e925e95.png)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5f951dd421f7adbca2edefd17e925e95.png)'
- en: What was passed in as context to the LLM along with the question, to facilitate
    the response. (Image by the author)
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 与问题一起传递给 LLM 的上下文，以便于响应。（图像由作者提供）
- en: We can see that the LLM still had access to all four search results and reasoned
    that only the first two books were detective novels.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，LLM 仍然访问了所有四个搜索结果，并推断出只有前两本书是侦探小说。
- en: Be forewarned that the response from the LLM could change each time you ask
    a question, despite sending the same instructions, and the same information from
    the vector database. For example, on asking about fantasy book recommendations,
    the LLM sometimes gave three book recommendations, and sometimes more — though
    all from the book list. In all cases, the top recommended book stayed the same.
    Note that these variations were despite configuring the consistency — creativity
    spectrum — the ‘temperature’ parameter — to 0 to minimize variance.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，LLM 的响应每次提问时可能会有所不同，尽管发送的是相同的指令和来自向量数据库的相同信息。例如，在询问关于奇幻书籍推荐时，LLM 有时给出三本书籍推荐，有时则更多——尽管都是来自书单。在所有情况下，最推荐的书籍保持不变。请注意，这些变化发生在将一致性——创造力谱——即“温度”参数配置为
    0 以最小化差异的情况下。
- en: 4\. Add “chat” capability (optional)
  id: totrans-107
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4\. 添加“聊天”功能（可选）
- en: 'The solution now has the necessary core functionality — it is able to read
    in information from a website and answer questions based on that information.
    But it currently does not offer a “conversational” user experience. Thanks to
    ChatGPT, the “chat interface” has become the dominant design: we now expect this
    to be the “natural” way to interact with generative AI, and LLMs in particular
    😅. The first steps towards getting to the chat interface involves adding “memory”
    to the solution.'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，解决方案具备了必要的核心功能——它能够从网站中读取信息并根据这些信息回答问题。但目前它并未提供“对话式”的用户体验。感谢 ChatGPT，“聊天界面”已成为主流设计：我们现在期望这成为与生成式
    AI 尤其是 LLM 互动的“自然”方式 😅。实现聊天界面的第一步涉及向解决方案中添加“内存”。
- en: “Memory” here is an illusion, in that the LLM is not actually remembering the
    conversation up to that point — it needs to be shown the full conversation history
    in each turn. So, if a user asks the LLM a follow-up question, the solution will
    package the original question, the LLM’s original answer, and the follow-up question
    and send it to the LLM. The LLM reads the entire conversation and generates a
    meaningful response to continue the conversation.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的“内存”是一种假象，LLM 实际上并不记住到目前为止的对话——它需要在每次回合中展示完整的对话记录。因此，如果用户向 LLM 提问后续问题，解决方案将打包原始问题、LLM
    的原始答案以及后续问题，并将其发送给 LLM。LLM 阅读整个对话并生成有意义的响应以继续对话。
- en: In question-answering chatbots, like the one we’re building, this approach needs
    to be extended further because there is the interim step to reach out to the vector
    database and pull relevant information to formulate the response to a user’s follow-up
    question. The way “memory” is simulated in question-answering chatbots is,
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 在问答聊天机器人中，如我们正在构建的这个，通常需要进一步扩展此方法，因为存在中间步骤需要从向量数据库中提取相关信息以制定对用户后续问题的响应。在问答聊天机器人中，“内存”是这样模拟的：
- en: Retain all questions and responses (in a variable) as “chat history”
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将所有问题和响应保留（在一个变量中）作为“聊天记录”
- en: When the user asks a question, send the chat history and the new question to
    the LLM and ask it to generate a standalone question
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当用户提问时，将聊天记录和新问题发送给 LLM，并要求生成一个独立的问题
- en: At this point, the chat history is no longer needed. Use the standalone question
    to run a new search on the vector DB
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 此时，聊天记录不再需要。使用独立的问题在向量数据库上进行新的搜索
- en: Pass the standalone question and search results, along with instructions to
    the LLM to get a final answer. This step is similar to what we implemented in
    the previous stage “Generate Answer using LLM”
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将独立问题和搜索结果传递给 LLM，并附上指令以获得最终答案。这个步骤类似于我们在之前阶段“使用 LLM 生成答案”中实施的步骤
- en: While we can keep a track of chat history in simpler variables, we will use
    one of LangChain’s memory types. The particular memory object we will use offers
    the nice feature of automatically truncating older chat history when it reaches
    a size limit you specify, generally the size of the text that the selected LLM
    can accept. In our case, the LLM should be able to accept a little over 4,000
    “tokens” (which are word parts), which should roughly be 3,000 words or ~5 pages
    from a Word document. OpenAI offers a 16k variant of the same ChatGPT LLM, which
    can accept 4x the input. Hence, the need to configure the memory size.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们可以使用简单的变量来跟踪聊天记录，但我们将使用 LangChain 的一种内存类型。我们将使用的特定内存对象提供了一个很好的功能，即在达到你指定的大小限制时自动截断较旧的聊天记录，通常是选定
    LLM 可以接受的文本大小。在我们的案例中，LLM 应该能够接受略超过 4,000 个“tokens”（即词汇部分），这大约是 3,000 个单词或 ~5
    页的 Word 文档。OpenAI 提供了相同 ChatGPT LLM 的 16k 变体，可以接受 4 倍的输入。因此，需要配置内存大小。
- en: Here is the code to achieve these steps. Again, LangChain provides a higher-level
    abstraction and the code does not have to be so explicit. This version is just
    to expose the underlying instructions sent to the LLM — first to condense the
    chat history into a single standalone question, which will then be used for the
    vector DB search, and the second to generate a response to the generated standalone
    question based on vector DB search results.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 这是实现这些步骤的代码。同样，LangChain提供了更高层次的抽象，代码不必如此明确。这个版本只是为了展示发送到LLM的底层指令——首先将聊天记录浓缩成一个独立的单一问题，然后用于向量数据库搜索，其次根据向量数据库搜索结果生成对生成的独立问题的响应。
- en: '[PRE12]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '![](../Images/e472c9c36dc137b348e73db60cfafa78.png)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e472c9c36dc137b348e73db60cfafa78.png)'
- en: Detective novel recommendations from the solution. Same response as the one
    received earlier using only “question-answering” capability, without “memory”
    (Image by the author)
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 解决方案的侦探小说推荐。与之前仅使用“问答”能力而没有“记忆”时收到的响应相同（图片由作者提供）
- en: 'Let’s ask a follow-up question and look at the response to validate the solution
    now has “memory” and can respond conversationally to follow-up questions:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们提出一个后续问题，并查看响应以验证解决方案现在具有“记忆”并且可以对后续问题进行对话式回答：
- en: '[PRE13]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '![](../Images/fed2232a1253700a345d4e1bb406e967.png)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fed2232a1253700a345d4e1bb406e967.png)'
- en: Response to follow-up question asking more information about “the second book”.
    The solution responds back with more information about the same book as before
    (Image by the author)
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 对“第二本书”的更多信息的后续问题的响应。解决方案返回更多关于同一本书的信息（图片由作者提供）
- en: 'Let’s look at what is happening under the hood to validate that the solution
    does indeed go through the four steps outlined at the beginning of this section.
    Let’s start with the chat history to verify that the solution does indeed log
    the conversation so far:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看在引擎盖下发生了什么，以验证解决方案确实经过了本节开头概述的四个步骤。让我们从聊天记录开始，以验证解决方案确实记录了到目前为止的对话：
- en: '[PRE14]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '![](../Images/5eb9194b5133cbb28fd5070319810bf1.png)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5eb9194b5133cbb28fd5070319810bf1.png)'
- en: Chat history after asking the second question. Note that the response is also
    included in the conversation at this point. (Image by the author)
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 提问第二个问题后的聊天记录。注意此时响应也包括在对话中。（图片由作者提供）
- en: 'Let’s look at what else is the solution tracking besides the chat history:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看解决方案除了聊天记录外还跟踪了什么：
- en: '[PRE15]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '![](../Images/396886e2b0b33096284054126381fe35.png)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/396886e2b0b33096284054126381fe35.png)'
- en: Outputs, other than chat history, after asking the second question. (Image by
    the author)
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 提问第二个问题后，除聊天记录之外的输出。（图片由作者提供）
- en: The solution internally uses the LLM to first convert the question *“Tell me
    more about the second book”* to *“What additional information can you provide
    about ‘The Day of the Jackal’ by Frederick Forsyth?”*. Armed with this question,
    the solution is able to search the vector DB for any relevant information and
    retrieves The Day of the Jackal chunk first this time. Though note that some other
    irrelevant search results about other books are also included.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 解决方案内部使用LLM将问题*“告诉我更多关于第二本书的事”*转换为*“你能提供更多关于弗雷德里克·福赛斯的《贼日》‘The Day of the Jackal’的信息吗？”*。有了这个问题，解决方案能够在向量数据库中搜索任何相关信息，并这次首先检索到《贼日》的信息。虽然注意到也包括了一些关于其他书籍的无关搜索结果。
- en: Quick optional sidebar discussing potential issues
  id: totrans-133
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 快速的可选侧边栏讨论潜在问题
- en: '**Potential Issue #1 — Poor Standalone Question Generation:** In my tests,
    the chat solution wasn’t always successful in generating a good standalone question,
    until the question generator prompt was tweaked. For example, for a follow-up
    question, “Tell me about the second book”, more often than not the generated follow-up
    question was “What can you tell me about the second book?” which is not particularly
    meaningful in itself and led to random search results and consequently a seemingly
    random generated response from the LLM.'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '**潜在问题 #1 — 独立问题生成不佳：** 在我的测试中，聊天解决方案在生成一个好的独立问题时并不总是成功，直到调整了问题生成器提示。例如，对于一个后续问题，“告诉我关于第二本书的事”，生成的后续问题往往是“你能告诉我关于第二本书的什么？”这本身并没有特别的意义，并导致随机的搜索结果，因此LLM的生成响应看起来也是随机的。'
- en: '**Potential Issue #2 — Changing Search Results Between Original & Follow-up
    Questions:** It is noteworthy that even though the second generated question specifically
    names the book of interest, the returned results from the vector DB search include
    other book results, and more importantly, these search results are different than
    from those for the original question! In this example, this change in search results
    was desirable since the question changed from “detective novel recommendations”
    to a particular novel. However, when a user is asking follow-up questions intending
    to dig deeper into a topic, variations in question formulation or LLM-generated
    standalone question may lead to different search results or a different ranking
    of search results, which might *not* be desirable.'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '**潜在问题 #2 — 原始问题与跟进问题之间的搜索结果变化：** 值得注意的是，即使第二个生成的问题明确提到感兴趣的书籍，向量数据库搜索返回的结果中也包括其他书籍的结果，更重要的是，这些搜索结果与原始问题的结果不同！在这个例子中，这种搜索结果的变化是期望的，因为问题从“侦探小说推荐”变成了特定的小说。然而，当用户提出跟进问题以深入探讨某个主题时，问题表述的变化或LLM生成的独立问题可能会导致不同的搜索结果或搜索结果的不同排序，这可能*不*是期望的。'
- en: This issue is possibly mitigated automatically, at least to a degree, by doing
    a broader initial search from the vector DB — returning many results instead of
    just 4–5 as with our example — and re-ranking them to ensure that the most relevant
    results bubble up to the top and are always sent to the LLM to generate the final
    response (see [Cohere’s ‘Reranking’](https://docs.cohere.com/docs/reranking)).
    Besides, it should be relatively straightforward for an app to recognize that
    search results have changed. It might be possible to apply some heuristics around
    whether the degree of change in search results (measured by ranking and overlap
    metrics), and the degree of change in the question (measured by distance metrics
    such as cosine similarity) are at parity. At least in cases where there are unexpected
    swings in search results over chat turns, the end user could be alerted and brought
    into the loop for a closer inspection, depending on the use case criticality and
    training or sophistication of end users.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 这个问题可能会在一定程度上自动得到缓解，至少是通过对向量数据库进行更广泛的初步搜索——返回更多的结果，而不仅仅是我们示例中的4-5个——并对结果进行重新排序，以确保最相关的结果上升到顶部并始终发送到LLM以生成最终响应（参见[Cohere的“重新排序”](https://docs.cohere.com/docs/reranking)）。此外，应用程序应相对容易识别搜索结果是否发生了变化。可能可以应用一些启发式方法来判断搜索结果的变化程度（通过排序和重叠度量）以及问题变化的程度（通过余弦相似度等距离度量）是否匹配。至少在搜索结果在聊天轮次中出现意外波动的情况下，可以根据用例的关键性以及最终用户的培训或素养，提醒最终用户并让他们参与更详细的检查。
- en: Another idea to control this behavior is to leverage the LLM to decide whether
    a follow-up question requires going to the vector DB again, or can the question
    be meaningfully answered with previously fetched results. Some use cases might
    want to generate two sets of search results and responses and let the LLM adjudicate
    between the answers, some others might be justified in passing the responsibility
    of controlling context to users by empowering them to freeze context (depending
    on the use case, user training or sophistication, and other considerations), and
    some others might simply be tolerant of changing search results over follow-up
    questions.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 控制这种行为的另一个想法是利用LLM来决定跟进问题是否需要再次访问向量数据库，或者这个问题是否可以用之前获取的结果有意义地回答。一些用例可能希望生成两组搜索结果和回答，让LLM在答案之间裁定，另一些用例可能通过赋予用户冻结上下文的能力来将控制上下文的责任转交给用户（这取决于用例、用户培训或素养以及其他考虑因素），还有一些用例可能对跟进问题中搜索结果的变化持宽容态度。
- en: As you can probably tell, it is quite easy to get a basic solution working,
    but getting things just right — that‘s the hard part. The issues called out here
    is just scratching the surface. Alright, back to the main exercise …
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你可能已经看出，获得一个基本解决方案是相对简单的，但做到完美——这才是难点。这里提到的问题只是冰山一角。好了，回到主要任务 …
- en: 5\. Add a pre-coded UI
  id: totrans-139
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5\. 添加预编码UI
- en: Finally, the chatbot’s functionality is ready. Now, we can add a nice user-interface
    to improve user experience. This is (somewhat) easily possible due to Python libraries
    such as Gradio and Streamlit, which build front-end widgets based on instructions
    written in Python. Here, we will go with Gradio to quickly create a user interface.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，聊天机器人的功能已经准备好。现在，我们可以添加一个漂亮的用户界面以改善用户体验。这是（某种程度上）由于Python库如Gradio和Streamlit，使得基于Python编写的指令构建前端小部件成为可能。在这里，我们将使用Gradio快速创建一个用户界面。
- en: With dual objectives of catching anyone up in case they were not able to execute
    the code so far, and also to demonstrate some variations in getting to the same
    place, the following two blocks of code are self-contained and can be run in a
    completely new Colab notebook to generate the complete chatbot.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 以使任何尚未执行代码的人能够跟上进度，并演示一些达到相同结果的变体，以下两个代码块是自包含的，可以在全新的Colab笔记本中运行，以生成完整的聊天机器人。
- en: '[PRE16]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Before running the next set of code to render the chatbot UI, note that when
    rendered through Colab, the app becomes publicly accessible for 3 days for anyone
    with the link (the link is provided in the Colab notebook cell output). In theory,
    the app can be kept private by changing the last line in the code to *demo.launch(share=False)*,
    but I was not able to get the app to work at all then. Instead, I prefer running
    it in ‘debug’ mode in Colab, so the Colab cell stays “running” until stopped,
    which then terminates the chatbot. Alternatively, run the code shown below in
    a different Colab cell to terminate the chatbot and delete content loaded to the
    Chroma vector DB within Colab.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 在运行下一组代码以呈现聊天机器人UI之前，请注意，当通过Colab渲染时，应用程序对于任何拥有链接的人公开可访问3天（链接在Colab笔记本单元输出中提供）。理论上，可以通过将代码中的最后一行更改为*demo.launch(share=False)*来保持应用的私密性，但那时我无法使应用正常工作。相反，我更倾向于在Colab中以“调试”模式运行，这样Colab单元会“运行”直到停止，然后终止聊天机器人。或者，在不同的Colab单元中运行下面的代码，以终止聊天机器人并删除在Colab中加载到Chroma向量数据库中的内容。
- en: '[PRE17]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Below is the code to run the chatbot as an app. Most of this code reuses the
    code up to this point of the article, so should seem familiar. Note that there
    are some differences in the code below compared to the code earlier, including
    but not limited to there being no memory management using LangChain’s ‘token’
    memory object that we used before. This means as the conversation continues for
    a while, the history will become too long to pass in to the language model’s context,
    and the app will need a restart.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是将聊天机器人作为应用运行的代码。大部分代码重复了到目前为止的文章中的代码，所以应该很熟悉。请注意，与之前的代码相比，下面的代码存在一些差异，包括但不限于没有使用之前用过的LangChain的‘token’内存对象进行内存管理。这意味着随着对话的继续，历史记录将变得过长，无法传递给语言模型的上下文，应用需要重启。
- en: '[PRE18]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'You can play around with the app by giving it a different URL to load content
    from. This goes without saying: this is not a production-grade app, and was only
    created to demonstrate building blocks of RAG-based GenAI solutions. This is an
    early prototype at best and if it were to be converted into a regular product,
    most of the software engineering work will lie ahead.'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过给应用提供不同的URL来加载内容进行尝试。无需多言：这不是一个生产级应用，只是用来演示基于RAG的GenAI解决方案的构建块。这充其量是一个早期原型，如果要将其转换为常规产品，大部分的软件工程工作还在前面。
- en: Revisiting FAQs from the Introduction
  id: totrans-148
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 重新审视介绍中的常见问题
- en: With the context and knowledge of the chatbot we created, let’s revisit some
    of the questions posed in the *Introduction* and dive just a little bit deeper.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 根据我们创建的聊天机器人背景和知识，让我们重新审视在*介绍*中提出的一些问题，并深入探讨一下。
- en: '*Is our use case a good fit for LLM-powered solutions? Perhaps traditional
    analytics, supervised machine learning, or another approach is a better fit?*'
  id: totrans-150
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*我们的用例是否适合LLM驱动的解决方案？也许传统的分析方法、监督学习或其他方法更合适？*'
- en: LLMs are good at “understanding” language related tasks as well as following
    instructions. So, the early use cases for LLMs have been question-answering, summarization,
    generation (text in this case), enabling better meaning-based search, sentiment
    analysis, coding, etc. LLMs have also picked up the ability to problem-solve and
    reason. For example, LLMs can act as automated graders for students’ assignments
    if you provide it with an answer key, or sometimes even without.
  id: totrans-151
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: LLM 在“理解”语言相关任务以及遵循指令方面表现出色。因此，LLM 的早期使用案例包括问答、总结、生成（在这里是文本）、提供更好的基于意义的搜索、情感分析、编码等。LLM
    还获得了解决问题和推理的能力。例如，LLM 可以充当学生作业的自动评分员，只要你提供答案键，甚至有时不提供。
- en: On the other hand, predictions or classifications based on a large number of
    data points, multi-armed bandit experiments for marketing optimization, recommender
    systems, reinforcement learning systems (Roomba, Nest thermostat, optimizing power
    consumption or inventory levels, etc.) are the forte of other types of analytics
    or machine learning … at least for the time being. Hybrid approaches where traditional
    ML models feed information to LLMs and vice-versa should also be considered as
    a holistic solution to a core business problem.
  id: totrans-152
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 另一方面，基于大量数据点的预测或分类、用于营销优化的多臂赌博机实验、推荐系统、强化学习系统（如 Roomba、Nest 温控器、优化能源消耗或库存水平等）是其他类型分析或机器学习的强项……至少目前是这样。传统
    ML 模型向 LLM 提供信息和反向传递信息的混合方法也应考虑作为解决核心业务问题的整体方案。
- en: '*If LLMs are the way to go, can our use case be addressed by an off-the-shelf
    product (say, ChatGPT Enterprise) now or in the near-future? Classic build-vs-buy
    decision.*'
  id: totrans-153
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*如果 LLM 是可行的解决方案，我们的使用案例是否可以通过现成的产品（例如，ChatGPT Enterprise）现在或在不久的将来得到解决？经典的构建与购买决策。*'
- en: Services and products offered by OpenAI, AWS, and others are going to grow broader,
    better, and possibly cheaper. For example, ChatGPT let’s users upload their files
    for analysis, Bing Chat and Google’s Bard let you point to external websites for
    question answering, AWS Kendra brings semantic search to an enterprise’s information,
    Microsoft Copilot lets you bring LLMs to Word, Powerpoint, Excel, etc. For the
    same reason that companies do not build their own operating systems or their own
    databases, companies should think about whether they need to build AI solutions
    that might possibly get obsolete by current and future off-the-shelf products.
    On the other hand, if a company’s use cases are specific, or restrictive in some
    sense — such as not being able to send their sensitive data to any vendor due
    to sensitivity, or due to regulatory guidance — then, it might be required to
    build generative AI products within the company to address use cases. Products
    that use an LLM’s reasoning ability but undertake tasks or generate outputs too
    distinct than the vended solutions might warrant in-house development. For example,
    a system that is monitoring the factory floor, or a manufacturing processes, or
    inventory levels, etc. might warrant custom development, especially if there are
    no good domain-specific product offerings. Also, if the application requires specialized
    domain knowledge, then an LLM fine-tuned on domain-specific data is likely going
    to outperform a general-purpose LLM from OpenAI, and in-house development could
    be considered.
  id: totrans-154
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: OpenAI、AWS 和其他公司提供的服务和产品将变得更加广泛、更好，可能还更便宜。例如，ChatGPT 允许用户上传文件进行分析，Bing Chat
    和 Google 的 Bard 让你指向外部网站进行问答，AWS Kendra 将语义搜索引入企业的信息中，Microsoft Copilot 让你在 Word、Powerpoint、Excel
    等应用中使用 LLM。正如公司不会自己构建操作系统或数据库一样，公司也应该考虑是否需要构建可能被当前和未来的现成产品所取代的 AI 解决方案。另一方面，如果公司的使用案例非常具体或在某种程度上受限，例如由于敏感性或法规指导不能将敏感数据发送给任何供应商，那么可能需要在公司内部构建生成性
    AI 产品以解决这些使用案例。使用 LLM 推理能力但承担的任务或生成的输出与现成解决方案差异太大的产品可能需要内部开发。例如，监控工厂车间、制造过程或库存水平的系统可能需要定制开发，特别是当没有好的领域特定产品时。此外，如果应用需要专业领域知识，那么在领域特定数据上微调的
    LLM 可能会优于 OpenAI 的通用 LLM，内部开发也可以考虑。
- en: '*What are the different building blocks of our LLM-powered product? Which of
    these are commoditized, and which are likely to need more time to build and test?*'
  id: totrans-155
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*我们的 LLM 驱动产品的不同构建模块有哪些？这些模块中哪些已经商品化，哪些可能需要更多时间来构建和测试？*'
- en: 'The high-level building blocks for a RAG solution like we built are the data
    pipeline, vector database, retrieval, generation, and the LLM of course. There
    are lots of great choices for LLMs and for vector databases. The data pipelines,
    retrieval, prompt engineering for generation will require some good old-fashioned
    data-scienc*y* experimentation to optimize for a use case. Once an initial solution
    is in place, productionization will require a lot of work, which is true of any
    data science / machine learning pipeline. This talk offers hard-earned wisdom
    on the topic of productionization: [LLMs in Production: Learning from Experience,
    Dr. Waleed Kadous, Chief Scientist, AnyScale](https://youtu.be/xa7k9MUeIdk?si=LQizYwFt4m-XOYpk)'
  id: totrans-156
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '像我们构建的RAG解决方案的高级构建块包括数据管道、向量数据库、检索、生成，当然还有LLM。LLM和向量数据库有很多优秀的选择。数据管道、检索、生成的提示工程将需要一些传统的数据科学实验来针对具体用例进行优化。一旦初步解决方案到位，生产化将需要大量工作，这在任何数据科学/机器学习管道中都是真实的。本讲座提供了有关生产化的宝贵经验：[LLMs
    in Production: Learning from Experience, Dr. Waleed Kadous, Chief Scientist, AnyScale](https://youtu.be/xa7k9MUeIdk?si=LQizYwFt4m-XOYpk)'
- en: '*How do we measure the performance of our solution? What levers are available
    to improve the quality of outputs from our product?* As with any technology (or
    non-technology ) solution, business impact should be measured using leading KPIs.
    Some direct metrics being difficult to measure get replaced by surrogate metrics
    such as average number of daily users (DAU) and other product metrics.'
  id: totrans-157
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*我们如何衡量解决方案的性能？有哪些杠杆可以改善我们产品的输出质量？* 与任何技术（或非技术）解决方案一样，商业影响应使用领先的关键绩效指标来衡量。一些直接度量难以测量，通常会被替代指标如每日活跃用户数（DAU）和其他产品指标所取代。'
- en: 'Business metrics should be complemented with technical metrics evaluating the
    performance of the RAG solution. The overall quality of the response — how good
    is the system’s response compared to the best generated response from an expert
    human or a state of the art frontier model such as GPT-4 (currently) could be
    evaluated using a range of metrics that test for informativeness, factualness,
    relevance, toxicity, etc. It will help to delve deeper into performance of individual
    components to iterate and improve each: the quality of information that the solution
    will use as context, retrieval, and generation.'
  id: totrans-158
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 商业指标应补充技术指标，以评估RAG解决方案的性能。可以使用一系列测试信息量、事实准确性、相关性、毒性等的指标来评估回应的整体质量——系统的回应与专家或如GPT-4（当前）的最先进模型的回应相比如何。这有助于深入了解各个组件的性能，以便迭代和改进每个组件：解决方案将用作上下文的信息质量、检索和生成。
- en: i. How good is the **data quality**? If data available to an organization and
    stored in the vector database doesn’t have the required information, no human
    or LLM can conjure a response based on it.
  id: totrans-159
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: i. **数据质量**如何？如果组织可用的数据存储在向量数据库中没有所需的信息，则没有人或LLM能够基于这些信息构造回应。
- en: ii. How good is the **retrieval**? Assuming the information is available, how
    successful is the system in finding and fetching the relevant bits?
  id: totrans-160
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ii. **检索**效果如何？假设信息可用，系统在找到和提取相关信息方面有多成功？
- en: iii. How good is the **generation (i.e. synthesis)**? Assuming the information
    is available, retrieved correctly, and passed on to the LLM to generate the final
    response, is the LLM using the information as expected?
  id: totrans-161
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: iii. **生成（即合成）**效果如何？假设信息可用、检索正确，并传递给LLM生成最终回应，LLM是否按预期使用这些信息？
- en: Each of these areas could be evaluated separately and improved concurrently
    to improve the overall output.
  id: totrans-162
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 每个领域都可以单独评估并同时改进，以提升整体输出。
- en: '**Improve Data quality:** Companies need to work on data pipelines to feed
    good information into the system. If there is bad quality of information in the
    vector database, having great LLMs will not improve the outputs drastically. In
    addition to employing traditional data quality and governance frameworks, companies
    should also consider improve the quality of chunking (more on this in the next
    question’s response).'
  id: totrans-163
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '**改善数据质量：** 公司需要在数据管道上进行工作，以向系统提供良好的信息。如果向量数据库中的信息质量差，拥有优秀的LLM也不会显著改善输出。除了采用传统的数据质量和治理框架外，公司还应考虑改善分块的质量（下一问题的回应中将更多讨论此事）。'
- en: '**Improve Retrieval:** Retrieval could be improved through trying different
    retrieval algorithms, semantic re-ranking, hybrid search combining semantic search
    and keyword search, and fine-tuning embeddings. Improving instructions / prompt
    should also contribute to improving the quality of retrieval.'
  id: totrans-164
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '**改善检索：** 通过尝试不同的检索算法、语义重排序、结合语义搜索和关键词搜索的混合搜索，以及微调嵌入，可以改善检索。改善指令/提示也应有助于提升检索质量。'
- en: '**Improve Generation:** As LLMs improve , the synthesis step will improve,
    and possibly retrieval too due to improved embedding models. Another option assuming
    resource & time availability is fine-tuning, which can improve the quality of
    responses for specific domains and tasks. For example, a smaller fine-tuned model
    on diagnosing specific medical conditions might be better at the task than a general
    purpose model like GPT-4, while also being faster and cheaper.'
  id: totrans-165
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '**改善生成：** 随着LLM的进步，合成步骤将得到改善，检索可能也会因为嵌入模型的改进而得到提升。另一种选择是进行微调，前提是资源和时间充足，这可以提高特定领域和任务的响应质量。例如，针对特定医疗条件进行微调的小模型可能在任务上优于像GPT-4这样的通用模型，同时也更快、更便宜。'
- en: '*Is our data quality acceptable for the use case? Are we organizing our data
    correctly, and passing relevant data to the LLM?*'
  id: totrans-166
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*我们的数据质量是否适合用例？我们是否正确组织了数据，并将相关数据传递给LLM？*'
- en: Data quality can be assessed with the traditional data quality & governance
    frameworks. Additionally for LLM-powered solutions, the information required by
    LLMs to answer user questions or carry out tasks should be available within the
    data available to the solution.
  id: totrans-167
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 数据质量可以通过传统的数据质量与治理框架进行评估。此外，对于LLM驱动的解决方案，LLM回答用户问题或执行任务所需的信息应在解决方案可用的数据中存在。
- en: Assuming the data is available, the data should be chunked appropriately for
    the use case and LLM being used. Chunks shouldn’t be too broad to dilute coherence
    with respect to a specific topic or too narrow to not include all the necessary
    context. Data shouldn’t be split into chunks in a way that necessary context is
    split between chunks and meaningless when separated in this way. For example,
    if the two sentences below are split into two chunks,
  id: totrans-168
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 假设数据是可用的，数据应根据用例和所使用的LLM进行适当的拆分。块不应过于宽泛，以免稀释与特定主题相关的连贯性，也不应过于狭窄，以免遗漏所有必要的上下文。数据不应以将必要的上下文分割在块之间并且在这种分隔下毫无意义的方式进行拆分。例如，如果下面的两个句子被拆分成两个块，
- en: '*“OpenAI’s GPT-3.5 is a powerful LLM. It can support context sizes up to 16K
    tokens.”*'
  id: totrans-169
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*“OpenAI的GPT-3.5是一个强大的LLM。它可以支持高达16K令牌的上下文大小。”*'
- en: A question such as “Tell me about GPT 3.5 LLM” may not fetch the 2nd sentence
    as it doesn’t mention GPT 3.5 and that information might not be provided to a
    user, just by virtue of suboptimal chunking. More dangerously, the sentence might
    still be fetched when asked about a completely different LLM due to semantic association
    of context sizes and tokens with LLMs, and the response might be that other model
    in focus has context sizes up to 16K, which would be factually inaccurate. This
    is a simplified example unlikely to be encountered in production, but the idea
    holds.
  id: totrans-170
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 像“告诉我关于GPT 3.5 LLM的情况”这样的问题可能不会获取第二句，因为它没有提到GPT 3.5，而这条信息可能不会提供给用户，仅仅是由于子优化块的原因。更危险的是，由于上下文大小和令牌与LLM的语义关联，当询问完全不同的LLM时，可能仍会获取该句子，且回答可能是其他重点模型的上下文大小高达16K，这将是不准确的。这是一个在生产环境中不太可能遇到的简化示例，但这个想法是成立的。
- en: One possible approach to improve quality of chunks is to use context-aware text
    splitting, such as splitting by logical sections (as in our example of the book
    list). If any logical chunk is too big — such as Wikipedia pages on particular
    topics would be quite lengthy, they could be split further by logical sections
    or by semantic units such as by paragraphs, with a meaningful overlap between
    chunks, as well as ensuring the overall metadata and chunk specific metadata is
    passed to the LLM.
  id: totrans-171
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 改善内容质量的一种可能方法是使用上下文感知的文本拆分，例如按逻辑部分拆分（如我们书单的示例）。如果任何逻辑块过大——例如，维基百科上关于某些主题的页面可能非常长——它们可以进一步按逻辑部分或按语义单元（如段落）拆分，同时在块之间保持有意义的重叠，并确保将整体元数据和块特定的元数据传递给LLM。
- en: '*Can we be confident that the LLM’s responses will always be factually accurate.
    That is, will our solution ‘hallucinate’ when generating responses once in a while?*'
  id: totrans-172
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*我们能否确信LLM的回答总是事实准确的？也就是说，我们的解决方案会不会在生成回答时偶尔‘幻觉’？*'
- en: 'A key selling point of RAG is to drive factuality. GPT 3.5 and GPT-4 are good
    at following this instruction: “respond only from the provided context or say
    ‘the question cannot be answered based on the information provided’”. This is
    hypothesized to be due to a lot of reinforcement learning from human feedback
    (RLHF) conducted by OpenAI. As a corollary, other LLMs might not currently be
    as good at following instructions. For a production application, especially an
    external facing one, it will be prudent to conduct a lot of testing aimed at validating
    that the generated output is faithful to the available context retrieved from
    the vector database, even though the LLM believes it to be the case. Approaches
    range from manual tests on samples, to using a powerful model such as GPT-4 to
    test samples of retrieved context and generated responses by other models, to
    using services and products such as [Galileo](https://www.rungalileo.io/) which
    focus on detecting LLM hallucinations in real-time.'
  id: totrans-173
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: RAG的一个关键卖点是推动事实准确性。GPT 3.5和GPT-4在遵循指令方面表现良好：“仅从提供的上下文中回应，或说‘基于提供的信息无法回答该问题’”。这被推测是由于OpenAI进行了大量的基于人类反馈的强化学习（RLHF）。作为推论，其他LLM可能当前在遵循指令方面表现不佳。对于生产应用，特别是面向外部的应用，进行大量测试以验证生成的输出是否忠实于从向量数据库检索的可用上下文，即使LLM相信这是正确的，也是明智的。方法包括对样本进行手动测试，使用强大的模型如GPT-4测试检索到的上下文样本和其他模型生成的响应，或使用如[Galileo](https://www.rungalileo.io/)这样的服务和产品，专注于实时检测LLM幻觉。
- en: Conclusion
  id: totrans-174
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结论
- en: Had you known all of this 11 months ago, it would have justified a demonstration
    with the CEO of your company. Possibly even a TED talk to a wider audience. Today,
    this has become a part of AI literacy baseline, especially if you are involved
    in delivery of generative AI products. Hopefully, you’re fairly caught up due
    to this exercise! 👍
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你在11个月前就知道这些内容，这将值得与你公司首席执行官进行一次演示，甚至可能有一个TED演讲向更广泛的观众介绍。今天，这已成为AI素养的基本要求，特别是如果你参与生成式AI产品的交付。希望通过这次练习，你能比较跟得上！👍
- en: A few closing thoughts,
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 一些结束语，
- en: There is serious promise in the technology — how many other technologies can
    “think” to this degree, and can be used as “reasoning engines” (in the words of
    Dr. Andrew Ng [here](https://learn.deeplearning.ai/langchain/lesson/7/agents)).
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这项技术具有巨大的潜力——还有多少其他技术能“思考”到这种程度，并且能作为“推理引擎”（用安德鲁·吴博士的话说，参见[这里](https://learn.deeplearning.ai/langchain/lesson/7/agents)）？
- en: While frontier models (currently, GPT-4) will continue to advance, open source
    models and their domain-specific and task specific fine-tuned variants will be
    competitive on numerous tasks and will find many applications.
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 虽然前沿模型（目前为GPT-4）将继续进步，但开源模型及其领域特定和任务特定的微调变体在许多任务中将具有竞争力，并找到许多应用。
- en: For better or worse, this cutting edge technology that took millions (hundreds
    of millions?) of dollars to develop is available for free — you could fill a form
    and download Meta’s capable Llama2 model with a very permissive license. Nearly
    300,000 baseline LLMs or their fine-tuned variants are on HuggingFace’s model
    hub. Hardware is also commoditized.
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 无论好坏，这项耗资数百万（甚至数亿？）美元开发的前沿技术现在是免费的——你可以填写一个表格，下载Meta功能强大的Llama2模型，拥有非常宽松的许可。HuggingFace的模型中心几乎有30万个基础LLM或其微调变体。硬件也已经商品化。
- en: OpenAI models are now capable of being aware of and using “tools” (functions,
    APIs, etc.), letting solutions interface with not just humans and databases, but
    with other programs. LangChain and other packages have already demonstrated using
    LLMs as the “brain” for autonomous agents that can accept input, decide what action
    to take, and follow through, repeating these steps until the agent reaches its
    goal. Our simple chatbot used two LLM calls in a deterministic sequence — generate
    standalone question, and synthesize search results into a coherent natural language
    response. Imagine what hundreds of calls to rapidly evolving LLMs with agentic
    autonomy can achieve!
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAI模型现在能够识别并使用“工具”（功能、API等），使得解决方案不仅能与人类和数据库接口，还能与其他程序接口。LangChain和其他软件包已经展示了如何将LLM作为“智能体”的“大脑”，这些智能体能够接受输入、决定采取的行动并执行，重复这些步骤直到智能体实现目标。我们的简单聊天机器人在确定性序列中使用了两个LLM调用——生成独立问题，并将搜索结果合成成连贯的自然语言响应。想象一下，数百次对快速发展的LLM进行的调用会取得什么成果！
- en: These rapid advancements are a result of tremendous momentum around GenAI, and
    it will proliferate enterprises, and day-to-day life through our devices. First
    in simpler ways, but later on in increasingly sophisticated applications that
    leverage the reasoning and decision-making capability of the technology, blending
    it with traditional AI.
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这些快速进展是由于GenAI周围的巨大动力，它将通过我们的设备渗透到企业和日常生活中。最初是以更简单的方式，但随后将在利用技术的推理和决策能力的越来越复杂的应用中体现，与传统AI融合。
- en: Finally, now is a great time to get involved as the playing field is fairly
    level, at least for applying this technology — everyone is learning about this
    at more or less the same time since the ChatGPT boom in Dec 2022\. Things are
    of course different on the R&D side, with Big Tech companies that have spent years,
    and billions of dollars in developing this technology. Regardless, to build more
    sophisticated solutions later, it’s the perfect time to get started now!
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最终，现在是一个绝佳的时机来参与，因为应用这项技术的竞争环境相对公平——自2022年12月ChatGPT的爆发以来，每个人基本上都在同一时间学习这项技术。当然，研发方面情况有所不同，大型科技公司已经投入了多年和数十亿美元来开发这项技术。尽管如此，为了将来构建更复杂的解决方案，现在正是开始的最佳时机！
- en: Additional Resources
  id: totrans-183
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 额外资源
- en: '**LangChain:** [Deeplearning.ai](http://deeplearning.ai/) course: [LangChain:
    Chat with Your Data](https://www.deeplearning.ai/short-courses/langchain-chat-with-your-data/)
    | LangChain [documentation](https://python.langchain.com/docs/get_started/introduction)'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**LangChain：** [Deeplearning.ai](http://deeplearning.ai/)课程：[LangChain：与您的数据对话](https://www.deeplearning.ai/short-courses/langchain-chat-with-your-data/)
    | LangChain [文档](https://python.langchain.com/docs/get_started/introduction)'
- en: '**Gradio:** [Deeplearning.ai](https://www.deeplearning.ai/) course - [Building
    Generative AI Applications with Gradio](https://www.deeplearning.ai/short-courses/building-generative-ai-applications-with-gradio/)
    | Gradio documentation and [guides](https://www.gradio.app/guides)'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Gradio：** [Deeplearning.ai](https://www.deeplearning.ai/)课程 - [使用Gradio构建生成性AI应用](https://www.deeplearning.ai/short-courses/building-generative-ai-applications-with-gradio/)
    | Gradio文档和[指南](https://www.gradio.app/guides)'
- en: I have found [Shawhin Talebi](https://medium.com/u/f3998e1cd186?source=post_page-----6ee6ad94e058--------------------------------)’s
    articles very instructive. See [Cracking Open the OpenAI (Python) API](/cracking-open-the-openai-python-api-230e4cae7971),
    [Cracking Open the Hugging Face Transformers Library](/cracking-open-the-hugging-face-transformers-library-350aa0ef0161),
    and other recent articles.
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我发现[Shawhin Talebi](https://medium.com/u/f3998e1cd186?source=post_page-----6ee6ad94e058--------------------------------)的文章非常具有启发性。请参阅[破解OpenAI
    (Python) API](/cracking-open-the-openai-python-api-230e4cae7971)，[破解Hugging Face
    Transformers库](/cracking-open-the-hugging-face-transformers-library-350aa0ef0161)以及其他近期文章。
- en: '[LLMs in Production: Learning from Experience, Dr. Waleed Kadous, Chief Scientist,
    AnyScale](https://youtu.be/xa7k9MUeIdk?si=LQizYwFt4m-XOYpk)'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[LLMs在生产中的应用：从经验中学习，AnyScale首席科学家Dr. Waleed Kadous](https://youtu.be/xa7k9MUeIdk?si=LQizYwFt4m-XOYpk)'
- en: 'This talk by Jerry Liu, co-founfer of LlamaIndex outlines various approaches
    for output evaluation: [Practical Data Considerations for Building Production-Ready
    LLM Applications](https://youtu.be/xbeFAZl3uCk?si=XBpo6cWt0Z9P9v_w)'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由LlamaIndex的联合创始人Jerry Liu所做的演讲概述了输出评估的各种方法：[构建生产就绪的LLM应用的实用数据考虑](https://youtu.be/xbeFAZl3uCk?si=XBpo6cWt0Z9P9v_w)
