- en: How to Evaluate Representations
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何评估表示
- en: 原文：[https://towardsdatascience.com/how-to-evaluate-representations-886ce5ef7a66](https://towardsdatascience.com/how-to-evaluate-representations-886ce5ef7a66)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/how-to-evaluate-representations-886ce5ef7a66](https://towardsdatascience.com/how-to-evaluate-representations-886ce5ef7a66)
- en: From unsupervised to supervised metrics
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从无监督指标到监督指标
- en: '[](https://medium.com/@mina.ghashami?source=post_page-----886ce5ef7a66--------------------------------)[![Mina
    Ghashami](../Images/745f53b94f5667a485299b49913c7a21.png)](https://medium.com/@mina.ghashami?source=post_page-----886ce5ef7a66--------------------------------)[](https://towardsdatascience.com/?source=post_page-----886ce5ef7a66--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----886ce5ef7a66--------------------------------)
    [Mina Ghashami](https://medium.com/@mina.ghashami?source=post_page-----886ce5ef7a66--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@mina.ghashami?source=post_page-----886ce5ef7a66--------------------------------)[![Mina
    Ghashami](../Images/745f53b94f5667a485299b49913c7a21.png)](https://medium.com/@mina.ghashami?source=post_page-----886ce5ef7a66--------------------------------)[](https://towardsdatascience.com/?source=post_page-----886ce5ef7a66--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----886ce5ef7a66--------------------------------)
    [Mina Ghashami](https://medium.com/@mina.ghashami?source=post_page-----886ce5ef7a66--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----886ce5ef7a66--------------------------------)
    ·8 min read·Sep 14, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----886ce5ef7a66--------------------------------)
    ·8分钟阅读·2023年9月14日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/92a89b397bf306df6fcef24bd78eb2a7.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/92a89b397bf306df6fcef24bd78eb2a7.png)'
- en: 'credit: Image from unsplash.com'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 'credit: 图片来源于 unsplash.com'
- en: '*Embeddings*, also known as *representations*, are dense vector representations
    of entities such as words, documents, products, and more. They are designed to
    capture semantic meanings and highlight similarities among entities. A good set
    of representations should not only efficiently encode the essential features of
    entities but also exhibit properties like compactness, meaningfulness, and robustness
    across various tasks. In this article, we look into various evaluation metrics
    to assess the quality of representations. Let’s get started.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '*嵌入表示*，也称为*表示*，是诸如单词、文档、产品等实体的稠密向量表示。它们旨在捕捉语义含义并突出实体之间的相似性。一组好的表示不仅应有效地编码实体的基本特征，还应具有紧凑性、意义性和在各种任务中表现出的鲁棒性。本文将探讨评估表示质量的各种指标。让我们开始吧。'
- en: An Evaluation Framework
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 评估框架
- en: 'Any evaluation framework consists of three main components:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 任何评估框架由三个主要组件组成：
- en: '**A baseline method**: this serves as a benchmark against which new approaches
    or models are compared. It provides a reference point for evaluating the performance
    of the proposed methods.'
  id: totrans-11
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**基准方法**：这作为一个基准，与新方法或模型进行比较。它提供了评估提出的方法性能的参考点。'
- en: '**A set of evaluation metrics**: evaluation metrics are quantitative measures
    used to evaluate the performance of the models. These metrics can be supervised
    or unsupervised, and define how the success of the outputs is assessed.'
  id: totrans-12
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**一组评估指标**：评估指标是用于评估模型性能的定量度量。这些指标可以是监督的或无监督的，并定义了如何评估输出的成功。'
- en: '**An evaluation dataset**: the evaluation dataset is a collection of labeled/annotated
    or unlabelled data used to assess the performance of the models. This dataset
    should be representative of the real-world scenarios that the models are expected
    to handle. It needs to cover a diverse range of examples to ensure a comprehensive
    evaluation.'
  id: totrans-13
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**评估数据集**：评估数据集是一个标注/注释或未标注的数据集合，用于评估模型的性能。该数据集应能代表模型预期处理的现实场景。它需要覆盖各种各样的例子，以确保全面的评估。'
- en: Based on if evaluation metrics require ground truth labels, we can split them
    into *un-supervised metrics*, and *supervised metrics*. It is often more advantageous
    to employ un-supervised metrics, as they do not require labels, and the collection
    of labels is very expensive in practice.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 根据评估指标是否需要真实标签，我们可以将其分为*无监督指标*和*监督指标*。通常，使用无监督指标更为有利，因为它们不需要标签，而标签的收集在实践中非常昂贵。
- en: Below, we will look into state-of-the-art metrics. For each metric, pick a baseline
    method to compare your evaluations against. The baseline can be as simple as `*random
    embedding generator*`!
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 下面，我们将深入探讨最先进的指标。对于每个指标，选择一个基线方法以对比你的评估结果。基线可以简单到`*随机嵌入生成器*`！
- en: Supervised Evaluation Metrics
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 监督评估指标
- en: Supervised metrics require a labelled evaluation dataset. A common strategy
    is to choose a predictor such as a classifier or regressor. Then train the predictor
    on a limited set of labeled data from a specific task. Next, using a supervised
    metric measure the predictor’s performance on a held-out dataset.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 监督指标需要一个标记的评估数据集。一个常见的策略是选择一个预测器，例如分类器或回归器。然后在来自特定任务的有限标记数据集上训练预测器。接下来，使用监督指标测量预测器在持出数据集上的表现。
- en: 'It is worth it to mention two valuable points here:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里值得提到两点有价值的内容：
- en: 1️⃣ The `*validation accuracy`*, a commonly used metric, is shown to be sensitive
    to the size of the dataset used for training the probe[3]!! It is shown that *validation
    accuracy* is sensitive to the size of the training dataset; they may choose a
    different representations for the same task when given dataset of different sizes
    [3]! Ideally, this assessment must be independent of the dataset size and *be
    only dependent on the data distribution* [3].
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 1️⃣ `*验证准确率*`，一个常用的指标，被证明对训练探测器的数据集大小很敏感[3]！！研究表明，*验证准确率*对训练数据集的大小很敏感；当数据集大小不同时，他们可能会为相同任务选择不同的表示[3]！理想情况下，这种评估必须独立于数据集大小，并且*仅依赖于数据分布*[3]。
- en: 'The figure below shows this phenomena: two representations, one in red (representation
    A), and one in blue (representation B) are shown. The x-axis denotes the size
    of the training data used to train the predictor (referred to as probe). The y-axis
    shows the validation loss of the trained probe. Note the probe is trained on representations
    from each method. As we see, as training data size increases on the x-axis, the
    validation loss of the probe decreases on the y-axis. However, at some point,
    the two loss curves cross each other!! Therefore in small data sizes, representation
    A has lesser loss, while in larger data sizes the representation B has lesser
    loss. The paper in[3] names this curve the *loss-data curve* as it measures the
    loss of the predictor vs the training data size used to train the probe.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的图展示了这一现象：两个表示，一个是红色的（表示A），另一个是蓝色的（表示B）。x轴表示用于训练预测器（称为探测器）的训练数据的大小。y轴显示了训练好的探测器的验证损失。注意探测器是在每种方法的表示上训练的。正如我们所见，当x轴上的训练数据大小增加时，探测器的验证损失在y轴上减少。然而，在某些时候，这两条损失曲线交叉了！！因此，在小数据集上，表示A的损失较少，而在大数据集上，表示B的损失较少。文献[3]将这一曲线称为*损失-数据曲线*，因为它衡量了预测器的损失与用于训练探测器的训练数据大小的关系。
- en: '![](../Images/9a278072f2c134d332d32c902bb2bc3b.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9a278072f2c134d332d32c902bb2bc3b.png)'
- en: 'Figure 1: loss-data curve — image from [3] modified by the author'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：损失-数据曲线 — 来源于[3]，由作者修改
- en: 2️⃣ *Linear predictors* (linear classifiers/regressors) are widely criticized
    as a method to evaluate representations[4]; it is shown that models that perform
    strongly on linear classification tasks can perform weakly on more complex tasks
    [4].
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 2️⃣ *线性预测器*（线性分类器/回归器）被广泛批评为评估表示的方法[4]；研究表明，在线性分类任务上表现强劲的模型在更复杂的任务上可能表现较弱[4]。
- en: There are many supervised metrics proposed in literature; **Mutual Information
    (MI), F1 score, BLEU, Precision, Recall, Minimum Descriptor Length (MDL)** to
    name a few but the SOTA metrics [3,9] in supervised evaluation realm are **Surplus
    Description Length (SDL)** and **ϵ−sample complexity.**
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 文献中提出了许多监督指标；**互信息（MI）、F1得分、BLEU、精确度、召回率、最小描述长度（MDL）**等，但在监督评估领域的SOTA指标[3,9]是**多余描述长度（SDL）**和**ϵ−样本复杂度**。
- en: '**Surplus Description Length (SDL)** and **ϵ−sample complexity** are inspired
    by the idea that “*the best representation is the one which allows for the most
    efficient learning of a predictor to solve the task”* [3,9]*.* As [3] mentions
    *“this position is motivated by practical concerns; the more labels that are needed
    to solve a task in the deployment phase, the more expensive to use and the less
    widely applicable a representation will be”.*'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '**多余描述长度（SDL）**和**ϵ−样本复杂度**的灵感来源于“*最佳表示是允许最有效地学习预测器以解决任务的表示*” [3,9]*。正如[3]提到的*“这一立场受实际问题的驱动；在部署阶段，解决任务所需的标签越多，使用成本越高，表示的适用范围也越小。”*'
- en: '**Intuition**: To give an intuition, the **SDL** and **ϵ−sample complexity**
    metrics do not measure a quantity on a fixed data size, instead they estimate
    the loss-data curve and measure quantities with respect to the curve[3]. To that
    end, for both metrics the user specifies a tolerance ϵ so that a population loss
    of less than ϵ qualifies as solving the task. An ϵ-loss predictor is then any
    predictor which achieves loss lesser than ϵ [3]. Then the measures compute the
    cost of learning a predictor which achieves that loss.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '**直观理解**：为了给出直观的理解，**SDL**和**ϵ−样本复杂度**指标不是在固定数据大小上衡量一个量，而是估计损失数据曲线，并根据曲线测量量[3]。为此，对于这两个指标，用户指定一个容差ϵ，以便小于ϵ的总体损失被视为解决任务。一个ϵ损失预测器是任何实现损失小于ϵ的预测器[3]。然后，这些指标计算学习一个实现该损失的预测器的成本。'
- en: 'Let’s dive into the details of each metric:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们深入了解每个指标的细节：
- en: Surplus Description Length (SDL)
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 剩余描述长度 (SDL)
- en: As mentioned in [3], this metric corresponds to computing the area between the
    loss-data curve and a baseline set by y = ϵ. In other words, it measures the cost
    of re-creating an ϵ-loss predictor using the representation [3]. Mathematically,
    it is defined as following
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 如[3]中所述，该指标对应于计算损失数据曲线与y = ϵ所设基线之间的区域。换句话说，它衡量了使用表示[3]重新创建一个ϵ损失预测器的成本。从数学上讲，它定义如下
- en: '![](../Images/3936abce369eaef49193ac7562622fcd.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3936abce369eaef49193ac7562622fcd.png)'
- en: 'Figure 2: SDL definition — Image from [3]'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：SDL定义 — 图片来源于 [3]
- en: Here, A is an algorithm, it can be any predicting algorithm such as classification
    or regression, and L(A, i) refers to the loss of this predictor resulting from
    training algorithm A on the first i data points [3]. Naively computing this metric
    require unbounded data so in practice it is always estimated [3]. For an implementation
    of this metric refer to the Github repository at [8].
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，A是一个算法，它可以是任何预测算法，如分类或回归，L(A, i)指的是在前i个数据点上训练算法A所导致的预测器损失[3]。计算该指标通常需要无限的数据，因此在实践中总是进行估计[3]。有关此指标的实现，请参见GitHub仓库[8]。
- en: '![](../Images/23b716387cce185742bcab901a09a3db.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/23b716387cce185742bcab901a09a3db.png)'
- en: 'Figure 3: SDL metric corresponds to computing the area between the loss-data
    curve and a baseline — image by the author'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：SDL指标对应于计算损失数据曲线与基线之间的区域 — 图片由作者提供
- en: ϵ−sample complexity
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ϵ−样本复杂度
- en: 'As mentioned in [3], this metric measures the complexity of learning an ϵ-loss
    predictor by the number of samples it takes to find it. It is defined as:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 如[3]中所述，该指标通过找到ϵ损失预测器所需的样本数量来衡量学习一个ϵ损失预测器的复杂度。定义为：
- en: '![](../Images/f7f79555f51a31f88249fc2a37e52031.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f7f79555f51a31f88249fc2a37e52031.png)'
- en: 'Figure 4: epsilon-sample complexity definition — Image from [3]'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：epsilon-样本复杂度定义 — 图片来源于 [3]
- en: This measures allows the comparison of two representation by first picking a
    target function to learn (e.g., an epsilon loss 2 layer MLP classifier) then measuring
    which representations enables learning that function with less data [3].
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 该指标允许通过首先选择一个目标函数进行学习（例如，一个epsilon损失的2层MLP分类器），然后测量哪些表示可以使用更少的数据来学习该函数，从而比较两个表示[3]。
- en: '![](../Images/96e6347cb6ca74db82618336981fc74e.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/96e6347cb6ca74db82618336981fc74e.png)'
- en: 'Figure 5: epsilon-sample complexity corresponds to number of data points which
    achieve epsilon loss — image by the author'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 图5：epsilon-样本复杂度对应于实现epsilon损失所需的数据点数量 — 图片由作者提供
- en: '**Unsupervised Evaluation Metrics:**'
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**无监督评估指标：**'
- en: 'These metrics are every scientist’s favorite as they do not require labelled
    data. Some examples of common unsupervised metrics are the followings:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 这些指标是每个科学家最喜欢的，因为它们不需要标记数据。一些常见的无监督指标包括以下内容：
- en: '1) Cluster Learnability (CL):'
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1) 聚类可学习性 (CL)：
- en: This metric[1] assesses the learnability of representations. A higher Cluster
    Learnability (CL) score indicates that the learned representation is better at
    separating different clusters in the input data space. CL is measured as the learning
    speed of a K-Nearest Neighbor (KNN) classifier trained to predict labels obtained
    by clustering the representations with K-means.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 该指标[1]评估表示的可学习性。较高的聚类可学习性 (CL) 分数表示学到的表示在输入数据空间中更好地分离了不同的簇。CL的测量是训练一个K-最近邻（KNN）分类器的学习速度，该分类器用于预测通过K均值聚类表示获得的标签。
- en: '![](../Images/6c2f954130c618a34ea4faec71a58af0.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6c2f954130c618a34ea4faec71a58af0.png)'
- en: Image by the author
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供
- en: 'It consists of three main steps and works as following:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 它由三个主要步骤组成，工作如下：
- en: Pick hyper-parameters *k=number of clusters* and *k′ = number of neighbors*.
    Let x_i​ denote the representations of i−th data point.
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择超参数 *k=簇的数量* 和 *k′ = 邻居的数量*。设 x_i​ 表示第 i 个数据点的表示。
- en: Run k-means on the dataset to obtain k clusters; assign cluster id to each datapoint
    x_i​ as its label. Denote it as y^_i$​.
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对数据集运行 k-means 以获得 k 个簇；将簇 id 分配给每个数据点 x_i​ 作为其标签。记作 y^_i$​。
- en: Run KNN in a prequential manner on each datapoint to obtain a predicted label
    by majority rule. If a datapoint is {x_i​, y_​i}​ take all data points numbered
    lesser than i i.e {x_j​ | j<i } and run KNN on them to get majority vote of their
    class labels. This will be y~_i​.
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在每个数据点上以预检验方式运行 KNN，通过多数规则获得预测标签。如果一个数据点是 {x_i​, y_​i}​，则取所有编号小于 i 的数据点，即 {x_j​
    | j<i }，并对其运行 KNN 以获得其类别标签的多数投票。这将是 y~_i​。
- en: Then the Cluster Learnability (CL) metric would be
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 那么 Cluster Learnability (CL) 指标为
- en: '![](../Images/37c521a7bbf556cffbccfd27f821af29.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/37c521a7bbf556cffbccfd27f821af29.png)'
- en: 'Figure 6: Cluster-Learnability metric — image by the author'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6：Cluster-Learnability 指标 — 图片由作者提供
- en: 'The intuition for this metric is as following: The “prequential” approach is
    a way of evaluating the performance of a machine learning model, such as KNN,
    in an online learning setting. In this context, “prequential” combines the words
    “predictive” and “sequential.” In the prequential manner of KNN computation, the
    model is evaluated and updated sequentially, one instance at a time, using a stream
    of data. Since the data is unlabeled, this method uses clustering in step 2 to
    synthetically generate labels (i.e. cluster ids) and use them to compute the speed
    of cluster learnability in an online (prequential) manner.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 该指标的直观解释如下： “预检验”方法是一种在在线学习环境中评估机器学习模型性能的方法，例如 KNN。在这种情况下，“预检验”结合了“预测”和“序列”的词汇。在
    KNN 计算的预检验方式中，模型是按顺序评估和更新的，一次处理一个实例，使用数据流。由于数据是未标记的，这种方法使用第 2 步中的聚类来合成生成标签（即簇
    id）并使用它们以在线（预检验）方式计算簇的学习速度。
- en: 'The following code computes this metric :'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码计算该指标：
- en: 2) *Silhouette Coefficient*
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2) *轮廓系数*
- en: 'Similar to CL metric, silhouette coefficient metric measures clusterability
    of entities that inherently belong to one cluster. This metric measures the quality
    of clusters through their cohesion and separation. In other words, it measures
    how well each sample in a cluster is separated from samples in other clusters,
    and provides an indication of how compact and well-separated the clusters are.
    The Silhouette Coefficient ranges from -1 to 1:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于 CL 指标，轮廓系数指标测量固有属于一个簇的实体的簇聚合性。该指标通过簇的内聚性和分离性来衡量簇的质量。换句话说，它衡量簇内每个样本与其他簇中的样本的分离程度，并提供簇的紧凑性和良好分离性的指示。轮廓系数的范围从
    -1 到 1：
- en: A value close to 1 indicates that samples are well-clustered and far from samples
    in neighboring clusters.
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 值接近 1 表示样本在簇内聚合良好，并且远离邻近簇中的样本。
- en: A value close to 0 suggests overlapping clusters or samples that are on or very
    close to the decision boundary between clusters.
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 值接近 0 表示簇之间重叠或样本位于簇之间的决策边界上或非常接近。
- en: A value close to -1 indicates that samples are likely assigned to the wrong
    clusters.
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 值接近 -1 表示样本可能被分配到错误的簇中。
- en: 'The following code computes this metric :'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码计算该指标：
- en: 3) Cluster Purity
  id: totrans-63
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 3) 簇纯度
- en: 'This metric evaluates the quality of clusters obtained from representations.
    It measures the extent to which clusters contain data points from a single class.
    The steps to compute cluster purity are:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 该指标评估从表示中获得的簇的质量。它衡量簇中包含来自单一类别的数据点的程度。计算簇纯度的步骤如下：
- en: For each cluster, identify the majority class in that cluster. Count the number
    of data points of the majority class in each cluster.
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对每个簇，识别该簇中的多数类别。统计每个簇中多数类别的数据点数量。
- en: Sum these counts for all clusters. Divide the sum by the total number of data
    points.
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将所有簇的这些计数相加。将总和除以数据点的总数。
- en: The value ranges from 0 to 1\. Higher values indicate better clusters, with
    1 being perfect clustering where each cluster contains solely points from one
    class.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 该值的范围从 0 到 1\. 较高的值表示更好的簇，其中 1 表示完美的聚类，每个簇仅包含一个类别的点。
- en: 'The following code computes this metric :'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码计算该指标：
- en: Summary
  id: totrans-69
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: 'It’s imperative to assess quality of representations before feeding them into
    any machine learning model. To evaluate their quality we must have a framework
    consisting of three components: a baseline method, evaluation metrics, and an
    evaluation datasets. Metric divide into supervised and unsupervised, based on
    if they need a labelled dataset or not. Often unsupervised metrics are advantageous
    as they don’t require labelled dataset and collecting/curating label is difficult
    and time-consuming. The unsupervised metrics include cluster purity, cluster-ability
    and cluster-learnability. On the other hand, the state of the art supervised metrics
    are surplus descriptor length (SDL) and ϵ-sample complexity. Regardless of the
    choice of the metric, it’s important to stay consistent and use one single metric
    to compare two set of representations against each other.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在将表示输入任何机器学习模型之前，评估表示的质量是至关重要的。为了评估它们的质量，我们必须有一个由三个组件组成的框架：基线方法、评估指标和评估数据集。指标分为监督型和无监督型，取决于是否需要标记数据集。通常，无监督指标更具优势，因为它们不需要标记数据集，而收集/整理标签既困难又耗时。无监督指标包括集群纯度、集群能力和集群可学习性。另一方面，先进的监督指标是过剩描述符长度（SDL）和ϵ-样本复杂度。无论选择何种指标，保持一致并使用一个单一指标来比较两个表示集是重要的。
- en: 'If you have any questions or suggestions, feel free to reach out to me:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你有任何问题或建议，请随时与我联系：
- en: 'Email: mina.ghashami@gmail.com'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 邮箱：mina.ghashami@gmail.com
- en: 'LinkedIn: [https://www.linkedin.com/in/minaghashami/](https://www.linkedin.com/in/minaghashami/)'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 'LinkedIn: [https://www.linkedin.com/in/minaghashami/](https://www.linkedin.com/in/minaghashami/)'
- en: References
  id: totrans-74
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[Expressiveness and Learnability: A Unifying View for Evaluating Self-Supervised
    Learning](https://arxiv.org/pdf/2206.01251.pdf)'
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[表达能力与可学习性：评估自监督学习的统一视角](https://arxiv.org/pdf/2206.01251.pdf)'
- en: '[Self-Supervised Representation Learning: Introduction, Advances and Challenges](https://arxiv.org/pdf/2110.09327.pdf)'
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[自监督表示学习：简介、进展与挑战](https://arxiv.org/pdf/2110.09327.pdf)'
- en: '[Evaluating representations by the complexity of learning low-loss predictors](https://arxiv.org/abs/2009.07368)'
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[通过学习低损失预测器的复杂性来评估表示](https://arxiv.org/abs/2009.07368)'
- en: '[Probing the State of the Art: A Critical Look at Visual Representation Evaluation](https://arxiv.org/pdf/1912.00215.pdf)'
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[审视最前沿：对视觉表示评估的批判性观察](https://arxiv.org/pdf/1912.00215.pdf)'
- en: '[Representation learning with contrastive predictive coding](https://arxiv.org/abs/1807.03748)'
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[对比预测编码的表示学习](https://arxiv.org/abs/1807.03748)'
- en: '[Learning representations by maximizing mutual information across views](https://arxiv.org/abs/1906.00910)'
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[通过最大化视图间的互信息来学习表示](https://arxiv.org/abs/1906.00910)'
- en: '[On mutual information maximization for representation learning](https://arxiv.org/abs/1907.13625)'
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[关于互信息最大化的表示学习](https://arxiv.org/abs/1907.13625)'
- en: '[https://github.com/willwhitney/reprieve](https://github.com/willwhitney/reprieve)'
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[https://github.com/willwhitney/reprieve](https://github.com/willwhitney/reprieve)'
