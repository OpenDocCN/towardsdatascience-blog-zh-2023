- en: “Approximate-Predictions” Make Feature Selection Radically Faster
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: “近似预测”使特征选择极大地加快
- en: 原文：[https://towardsdatascience.com/approximate-predictions-make-feature-selection-radically-faster-0f9664877687](https://towardsdatascience.com/approximate-predictions-make-feature-selection-radically-faster-0f9664877687)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/approximate-predictions-make-feature-selection-radically-faster-0f9664877687](https://towardsdatascience.com/approximate-predictions-make-feature-selection-radically-faster-0f9664877687)
- en: Feature selection is so slow because it requires the creation of many models.
    Find out how to make it blazingly faster thanks to approximate-predictions
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 特征选择之所以如此缓慢，是因为它需要创建许多模型。了解如何利用近似预测让其速度极快。
- en: '[](https://medium.com/@mazzanti.sam?source=post_page-----0f9664877687--------------------------------)[![Samuele
    Mazzanti](../Images/432477d6418a3f79bf25dec42755d364.png)](https://medium.com/@mazzanti.sam?source=post_page-----0f9664877687--------------------------------)[](https://towardsdatascience.com/?source=post_page-----0f9664877687--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----0f9664877687--------------------------------)
    [Samuele Mazzanti](https://medium.com/@mazzanti.sam?source=post_page-----0f9664877687--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@mazzanti.sam?source=post_page-----0f9664877687--------------------------------)[![Samuele
    Mazzanti](../Images/432477d6418a3f79bf25dec42755d364.png)](https://medium.com/@mazzanti.sam?source=post_page-----0f9664877687--------------------------------)[](https://towardsdatascience.com/?source=post_page-----0f9664877687--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----0f9664877687--------------------------------)
    [Samuele Mazzanti](https://medium.com/@mazzanti.sam?source=post_page-----0f9664877687--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----0f9664877687--------------------------------)
    ·10 min read·Nov 17, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----0f9664877687--------------------------------)
    ·10分钟阅读·2023年11月17日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/966d9bfba9303c9a87213f6ba5d91239.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/966d9bfba9303c9a87213f6ba5d91239.png)'
- en: '[Image by Author]'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[作者提供的图片]'
- en: When developing a machine learning model, we usually start with a large set
    of features resulting from our feature engineering efforts.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在开发机器学习模型时，我们通常从一大堆特征开始，这些特征是我们特征工程工作结果。
- en: '**Feature selection is the process of choosing a smaller subset of features
    that are optimal for our ML model**.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '**特征选择是选择对我们的机器学习模型最优的小子集特征的过程**。'
- en: Why doing that and not just keeping all the features?
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么这样做而不是保持所有特征？
- en: '**Memory**. Big data take big space. Dropping features means that you need
    less memory to handle your data. Sometimes there are also external constraints.'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**内存**。大数据占用大空间。丢弃特征意味着你需要更少的内存来处理数据。有时也会有外部约束。'
- en: '**Time**. Retraining a model on less data can save you much time.'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**时间**。在较少的数据上重新训练模型可以节省大量时间。'
- en: '**Accuracy**. Less is more: this also goes for machine learning. Including
    redundant or irrelevant features means including unnecessary noise. Frequently,
    it happens that a model trained on less data performs better.'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**准确性**。少即是多：这对于机器学习也是如此。包含冗余或无关的特征意味着包含不必要的噪音。经常发生的情况是，使用较少数据训练的模型表现更好。'
- en: '**Explainability**. A smaller model is more easily explainable.'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可解释性**。较小的模型更容易解释。'
- en: '**Debugging**. A smaller model is easier to maintain and troubleshoot.'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**调试**。较小的模型更容易维护和排查故障。'
- en: Now, the main problem with feature selection is that it is **very slow because
    it requires training many models**.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，特征选择的主要问题是**非常慢，因为它需要训练许多模型**。
- en: In this article, we will see a trick that makes feature selection extremely
    faster thanks to “approximate-predictions”.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我们将看到一个技巧，它通过“近似预测”使特征选择极其快速。
- en: A very hard problem
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 一个非常困难的问题
- en: Let’s try to visualize the problem of feature selection. We start with *N* features,
    where *N* is typically hundreds or thousands.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试可视化特征选择的问题。我们从*N*个特征开始，其中*N*通常是数百或数千个。
- en: Thus, the output of feature selection can be seen as an array of length *N*
    made of “yes”/“no”, where each element of the array tells us whether the corresponding
    feature is selected or not.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，特征选择的输出可以看作是一个长度为*N*的由“是”/“否”组成的数组，其中数组的每个元素告诉我们相应的特征是否被选择。
- en: '![](../Images/25287df1dfeb78a203d6ca69447d2139.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/25287df1dfeb78a203d6ca69447d2139.png)'
- en: Output of feature selection. [Image by Author]
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 特征选择的输出。[作者提供的图片]
- en: The process of feature selection consists of trying different “candidates” and
    finally picking the best one (according to our performance metric).
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 特征选择的过程包括尝试不同的“候选”，最后选择最佳的一个（根据我们的性能指标）。
- en: '![](../Images/54bcc4d018d224e5cbbf1c7f0a6fe78e.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/54bcc4d018d224e5cbbf1c7f0a6fe78e.png)'
- en: Each candidate is a different set of features. [Image by Author]
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 每个候选都是一组不同的特征。 [作者提供的图像]
- en: Since we have *N* features and any of them can be either selected or not selected,
    this means that **we have 2^*N* possible candidates**.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们有*N*个特征，且每个特征可以被选择或不被选择，这意味着**我们有 2^*N* 个可能的候选**。
- en: This number becomes huge quickly. For example, with just 50 features and supposing
    that evaluating a candidate requires on average 1 second, trying all the possible
    candidates would require 35 million years!
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 这个数字很快会变得巨大。例如，仅有50个特征，并假设评估一个候选平均需要1秒，那么尝试所有可能的候选将需要3500万年！
- en: Thus, it should be clear why, in most practical cases, the number of candidates
    that are evaluated is just a tiny fraction of all the possible candidates.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，应该清楚为什么在大多数实际情况下，评估的候选数量只是所有可能候选的一个微小部分。
- en: Candidate proposal and evaluation
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 候选方案提议与评估。
- en: 'Many feature selection methods exist, but all of them can be framed as an iterative
    process made of two steps:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 存在许多特征选择方法，但所有这些方法都可以被框架为由两个步骤组成的迭代过程：
- en: Proposing a new candidate
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 提议一个新的候选。
- en: Evaluating the candidate
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 评估候选。
- en: '![](../Images/9f8669e0360e24c04bbcb19d1e04cf68.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9f8669e0360e24c04bbcb19d1e04cf68.png)'
- en: A very general framework for feature selection. [Image by Author]
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 一个非常通用的特征选择框架。 [作者提供的图像]
- en: Usually, all the attention is placed on the first step. The purpose of Step
    1 is usually to find candidates that will likely perform well, based on what we
    have learned so far.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，所有注意力都集中在第一步。第一步的目的是基于我们目前的学习，找到可能表现良好的候选。
- en: 'In this article, however, we will completely disregard Step 1 to focus solely
    on Step 2: candidate evaluation. To this aim, **we will propose new candidates
    completely at random**. In particular, we will use the following function to propose
    new candidates:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在本文中，我们将完全忽略第1步，专注于第2步：候选评估。为此，**我们将完全随机地提出新的候选**。特别是，我们将使用以下函数来提出新的候选：
- en: '[PRE0]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Regarding Step 2, we will compare two strategies of evaluation based on different
    types of predictions:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 关于第2步，我们将基于不同类型的预测比较两种评估策略：
- en: Exact-predictions.
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 精确预测。
- en: Approximate-predictions.
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 近似预测。
- en: Don’t worry if you are not familiar with these terms now, the next paragraphs
    will make things clearer.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你现在对这些术语不熟悉也不用担心，接下来的段落会让事情变得更清晰。
- en: Feature selection based on “exact-predictions”
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于“精确预测”的特征选择。
- en: 'Given a new candidate, all the feature selection methods you have seen so far
    probably follow this structure:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个新的候选，你到目前为止看到的所有特征选择方法可能都遵循这种结构：
- en: Train a model on the data frame made of the training observations and the candidate
    features.
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在由训练观察和候选特征组成的数据框上训练模型。
- en: Use the model to get the predictions on the validation set.
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用模型对验证集进行预测。
- en: Compute the performance metric on the validation set.
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在验证集上计算性能指标。
- en: 'Graphically, this is equivalent to:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 从图形上来看，这相当于：
- en: '![](../Images/6f840f9c38a08be9347d2a6f506e7905.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6f840f9c38a08be9347d2a6f506e7905.png)'
- en: Feature selection based on exact-predictions. [Image by Author]
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 基于精确预测的特征选择。 [作者提供的图像]
- en: This approach is based on **“exact-predictions” because we obtain the actual
    predictions made by the model trained exclusively on the candidate features**.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法基于**“精确预测”，因为我们获得的是仅在候选特征上训练的模型所做的实际预测**。
- en: 'This process, translated in Python, would look more or less like this:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 这个过程，用Python表示，大致如下：
- en: '[PRE1]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: As you can see, a new model is trained at each iteration, making this process
    very slow.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所见，每次迭代都会训练一个新的模型，使得这个过程非常缓慢。
- en: So, is there a way to exploit the information we have about the features, without
    having to train a new model at each iteration?
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，有没有办法在不必在每次迭代时训练新模型的情况下利用我们对特征的了解呢？
- en: This is where “approximate-predictions” come into play.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是“近似预测”发挥作用的地方。
- en: The intuition behind approximate-predictions
  id: totrans-56
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 近似预测的直觉。
- en: To help ourselves understand “approximate-predictions”, let’s use an example
    dataset from [Pycaret](https://github.com/pycaret/pycaret) (a Python library under
    [MIT license](https://github.com/pycaret/pycaret/blob/master/LICENSE)). The dataset
    is called “Concrete”, and the task is to predict the strength of concrete given
    some of its features.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 为了帮助我们理解“近似预测”，我们使用[Pycaret](https://github.com/pycaret/pycaret)（一个遵循[MIT许可](https://github.com/pycaret/pycaret/blob/master/LICENSE)的Python库）中的示例数据集。数据集名为“Concrete”，任务是根据一些特征预测混凝土的强度。
- en: Let’s start by dividing the observations into a training set and a validation
    set.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从将观察数据分成训练集和验证集开始。
- en: '[PRE2]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'We can train a model on the training dataset (I will use LightGBM, but any
    model will work):'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在训练数据集上训练一个模型（我将使用LightGBM，但任何模型都可以）：
- en: '[PRE3]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Since we have a trained model, we can compute the SHAP values (if you don’t
    know about the topic, you can read my [introduction to SHAP values](/shap-explained-the-way-i-wish-someone-explained-it-to-me-ab81cc69ef30)):'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们已经有了训练好的模型，我们可以计算SHAP值（如果你对这个主题不熟悉，可以阅读我的[SHAP值介绍](/shap-explained-the-way-i-wish-someone-explained-it-to-me-ab81cc69ef30)）：
- en: '[PRE4]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'We can easily display the SHAP values:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以轻松显示SHAP值：
- en: '![](../Images/fd2f8e390a89ae823310b0a91585e32e.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fd2f8e390a89ae823310b0a91585e32e.png)'
- en: SHAP values of the validation set of the Concrete dataset. [Image by Author]
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 混凝土数据集验证集的SHAP值。[图片来自作者]
- en: Each SHAP value represents the contribution brought by a feature to the final
    prediction for that observation. For example, if we take the first feature of
    the first row (-14.708), this means that that feature lowers the final prediction
    by -14.708.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 每个SHAP值代表了某个特征对该观察的最终预测的贡献。例如，如果我们取第一行的第一个特征（-14.708），这意味着该特征将最终预测值降低了-14.708。
- en: '**The most important property of SHAP values is that they are additive**. This
    means that if we sum the SHAP values of the first row (-14.708185 +7.572576 -0.366994
    +…), we obtain exactly the prediction made by the model for that row.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '**SHAP值最重要的特性是它们是加性的**。这意味着如果我们对第一行的SHAP值（-14.708185 +7.572576 -0.366994 +…）进行求和，我们可以准确得到模型对该行的预测。'
- en: 'This holds true for all the rows. Don’t believe me? You can check it yourself
    with the following piece of code:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 这对所有行都适用。不相信我？你可以用下面的代码自己检查：
- en: '[PRE5]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: This proves that by summing the SHAP values of any individual we obtain exactly
    the model prediction for that individual (there is actually a small rounding difference
    at the tenth decimal, which is negligible).
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 这证明了通过求和任何个体的SHAP值，我们可以准确得到该个体的模型预测（实际上在第十位小数处有一个小的四舍五入差异，但可以忽略）。
- en: '**We can exploit the additive property of SHAP values to simulate the predictions
    that a model trained on a particular subset of features would produce**.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '**我们可以利用SHAP值的加性属性来模拟训练于特定特征子集的模型所产生的预测**。'
- en: 'Let’s say that we want to answer the following question: “What predictions
    would the model make if it had just the features *Fly Ash*, *Water,* and *Age*?”'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们想回答以下问题：“如果模型只使用特征*Fly Ash*、*Water*和*Age*，它会做出什么预测？”
- en: '![](../Images/c19541414c0f94b2ede73aecc1600216.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c19541414c0f94b2ede73aecc1600216.png)'
- en: Selecting a subset of features from the SHAP values. [Image by Author]
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 从SHAP值中选择特征子集。[图片来自作者]
- en: SHAP values allow us to answer this question. In fact, since they are additive
    and keep into account the feature interactions, **it’s enough to sum the SHAP
    values relative to those features to obtain our estimate of the predictions that
    a model trained only on those features would produce**.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: SHAP值允许我们回答这个问题。实际上，由于它们是加性的，并且考虑了特征间的交互，**只需对那些特征的SHAP值进行求和，我们就能估计出一个仅在这些特征上训练的模型所做的预测**。
- en: '[PRE6]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Of course, this is just an approximation! If we wanted the exact-predictions,
    we would need to train a new model specialized on the candidate features only.
    This is why I am calling the **predictions obtained in this way “approximate predictions”**.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，这只是一个近似！如果我们想要准确的预测，我们需要训练一个仅专注于候选特征的新模型。这就是为什么我称**这种方式获得的预测为“近似预测”**。
- en: But how are approximate-predictions useful for feature selection?
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，近似预测对特征选择有什么用呢？
- en: Feature selection based on “approximate-predictions”
  id: totrans-80
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于“近似预测”的特征选择
- en: '**Approximate-predictions allow us to simulate any possible candidate of features
    without having to train a new model**. All we need is the SHAP values of the model
    trained on all the features.'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '**近似预测允许我们模拟任何可能的特征候选，而不必训练一个新模型**。我们只需要训练所有特征的模型的SHAP值。'
- en: Can you see why this is a game-changer? **With exact-predictions, we needed
    to train a new model at each iteration. Instead, to obtain approximate-predictions,
    we just need to sum some columns of a data frame!** This makes the process incredibly
    faster.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 你能看到这为什么是一个改变游戏规则的因素吗？**使用精确预测时，我们需要在每次迭代时训练一个新模型。相反，为了获得近似预测，我们只需对数据框的某些列进行求和！**
    这使得过程快得多。
- en: 'Graphically, this is what happens with approximate-predictions:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 从图形上看，这就是近似预测发生的情况：
- en: '![](../Images/1180e1811d72acf1928ee8ab6a3af466.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1180e1811d72acf1928ee8ab6a3af466.png)'
- en: Feature selection based on approximate-predictions. [Image by Author]
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 基于近似预测的特征选择。[作者提供的图片]
- en: 'Translated in Python:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 在Python中的翻译：
- en: '[PRE7]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: As you can see, in this case, just one model is trained at the beginning. Then,
    at each iteration, we just perform a simple sum of columns, which is of course
    much faster than training a brand-new model.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，在这种情况下，一开始只训练了一个模型。然后，在每次迭代中，我们只是执行简单的列求和，这显然比训练一个全新的模型要快得多。
- en: This seems amazing, but we must remember that summing the SHAP values of some
    features is just like obtaining a *proxy* of the real predictions that we would
    have if we trained a model exclusively on those features.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 这看起来很惊人，但我们必须记住，求和一些特征的SHAP值就像是获得一个*代理*，这个代理接近于如果我们只用这些特征训练模型时的实际预测。
- en: 'So, as with any approximation, the question becomes: **is the approximation
    good enough for our purposes?**'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，就像任何近似一样，问题变成了：**这个近似对我们的目的足够好吗？**
- en: Is the proxy good enough?
  id: totrans-91
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 代理是否足够好？
- en: To answer this question, let’s take an example dataset from [Pycaret](https://github.com/pycaret/pycaret)
    (a Python library under [MIT license](https://github.com/pycaret/pycaret/blob/master/LICENSE)).
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 为了回答这个问题，让我们从[Pycaret](https://github.com/pycaret/pycaret)（一个[MIT许可证](https://github.com/pycaret/pycaret/blob/master/LICENSE)下的Python库）中获取一个示例数据集。
- en: 'The dataset is called “heart” and contains 15 features:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集称为“心脏”，包含15个特征：
- en: AGE_50
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: AGE_50
- en: MD_50
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MD_50
- en: SBP_50
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SBP_50
- en: DBP_50
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DBP_50
- en: HT_50
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: HT_50
- en: WT_50
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: WT_50
- en: CHOL_50
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CHOL_50
- en: SES
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SES
- en: CL_STATUS
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CL_STATUS
- en: MD_62
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MD_62
- en: SBP_62
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SBP_62
- en: DBP_62
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DBP_62
- en: CHOL_62
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CHOL_62
- en: WT_62
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: WT_62
- en: IHD_DX
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: IHD_DX
- en: 'Using these features, I randomly generated 50 different candidates (i.e. 50
    different sets of features). As a performance metric, I used average precision.
    For each candidate, I tried both the Exact-Prediction and the Approximate-Prediction
    method and, for both of them, I computed:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这些特征，我随机生成了50个不同的候选（即50组不同的特征）。作为性能指标，我使用了平均精度。对于每个候选，我尝试了精确预测和近似预测方法，并且对于这两种方法，我计算了：
- en: 'Actual AP: the average precision computed using the exact-predictions.'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实际AP：这是使用精确预测计算的平均精度。
- en: 'Predicted AP: this is the average precision computed using the approximate-predictions.'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预测的AP：这是使用近似预测计算的平均精度。
- en: We can say that the proxy is good if the Predicted AP is very similar to the
    Actual AP.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 如果预测的AP与实际的AP非常相似，我们可以说代理是好的。
- en: Let’s visualize the 50 candidates with their Predicted AP and Actual AP.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们可视化50个候选的预测AP和实际AP。
- en: '![](../Images/86c7a14f63499a9544ebaea6c60596ce.png)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/86c7a14f63499a9544ebaea6c60596ce.png)'
- en: Heart dataset. Each dot represents a candidate set of features. [Image by Author]
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 心脏数据集。每个点表示一组候选特征。[作者提供的图片]
- en: By way of example, I put some labels showing which features are included in
    that candidate.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 作为例子，我添加了一些标签，显示了该候选特征中包含的特征。
- en: Out of curiosity, let’s also visualize how many features every candidate contains.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 出于好奇，我们还来可视化每个候选特征包含的特征数量。
- en: '![](../Images/662265618a0fd75b117741b91a23f1cb.png)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/662265618a0fd75b117741b91a23f1cb.png)'
- en: Heart dataset. Each dot represents a candidate set of features. The labels represent
    the number of features for that candidate. [Image by Author]
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 心脏数据集。每个点表示一组候选特征。标签表示该候选特征的数量。[作者提供的图片]
- en: Predicted AP and Actual AP seem very correlated, this is good news!
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 预测的AP与实际的AP似乎非常相关，这是好消息！
- en: Let’s measure it. I will use the Spearman correlation instead of the Pearson
    correlation because, in this case, we are more interested in the relative order
    of the candidates rather than in their linear relationship.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来测量一下。我将使用斯皮尔曼相关系数而不是皮尔逊相关系数，因为在这种情况下，我们更关注候选的相对顺序，而不是它们的线性关系。
- en: The correlation in this case is 89%, so really high. This is good news for us
    because it means that **if we select the best candidate using approximate predictions,
    this is probably also the best candidate (or one of the best candidates) according
    to exact predictions.**
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，相关性为89%，非常高。这对我们来说是好消息，因为这意味着**如果我们使用近似预测选择最佳候选特征，这个候选特征也很可能是准确预测中的最佳（或最佳之一）候选特征。**
- en: We can repeat the same procedure also for some other datasets that are in Pycaret.
    For each dataset, I randomly draw 50 feature set candidates and measure both the
    Predicted AP and the Actual AP.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也可以对Pycaret中的一些其他数据集重复相同的程序。对于每个数据集，我随机抽取50个特征集候选项，并测量预测AP和实际AP。
- en: 'These are the results:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是结果：
- en: '![](../Images/5e608150d988bda5efd353c851a16de8.png)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5e608150d988bda5efd353c851a16de8.png)'
- en: 13 datasets, 50 candidates for each dataset. Each dot represents a candidate.
    [Image by Author]
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 13个数据集，每个数据集50个候选特征。每个点代表一个候选特征。[作者提供的图片]
- en: At a glance, it seems that all the datasets have a strong correlation. This
    again confirms our intuition.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 一看之下，似乎所有数据集都有很强的相关性。这再次证实了我们的直觉。
- en: 'But let’s do it more rigorously and compute the Spearman correlation between
    Predicted AP and Actual AP, for each dataset separately:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 但让我们更严谨地进行计算，分别计算每个数据集的预测AP与实际AP之间的斯皮尔曼相关性：
- en: '![](../Images/04fb58f564d6f18a29f5c63cc4a05c2a.png)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/04fb58f564d6f18a29f5c63cc4a05c2a.png)'
- en: Correlation between Predicted AP and Actual AP, by dataset. [Image by Author]
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集之间预测AP和实际AP的相关性。[作者提供的图片]
- en: 'These numbers confirm our previous impression: the correlation coefficients
    are always very high, spanning between a minimum of 87% and a maximum of 100%.'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 这些数字证实了我们之前的印象：相关系数始终很高，介于87%和100%之间。
- en: So we can conclude that **approximate-predictions are actually a good proxy
    of exact predictions,** and we can use them to make feature selection much faster
    and, at the same time, reliable.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们可以得出结论：**近似预测实际上是准确预测的良好代理，** 我们可以利用它们使特征选择变得更快，同时保持可靠性。
- en: Conclusions
  id: totrans-133
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: 'Any feature selection method consists at least of two steps: proposing a new
    candidate set of features and evaluating that candidate.'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 任何特征选择方法至少包括两个步骤：提出一组新的候选特征和评估这些候选特征。
- en: In this article, we focused on the second step (evaluation) showing how to leverage
    SHAP values to obtain “approximate-predictions”. This approach allows to obtain
    an estimate of the “exact-predictions” that we would obtain if we trained a different
    model specialized on each set of features.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我们专注于第二步（评估），展示了如何利用SHAP值来获得“近似预测”。这种方法允许我们获得“准确预测”的估计值，即如果我们训练一个专注于每组特征的不同模型，我们将会得到的预测结果。
- en: The benefit is that approximate-predictions are obtained with a simple sum,
    thus making the evaluation step much faster and allowing to evaluate many more
    candidates.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 好处在于近似预测通过简单的加法获得，从而使评估步骤大大加快，并允许评估更多候选特征。
- en: We also showed that the approximate-predictions are reliable enough since the
    performance metric that we obtain with this method is very much correlated with
    the performance metric we would obtain using exact-predictions.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还展示了近似预测足够可靠，因为我们使用这种方法获得的性能指标与我们使用准确预测所获得的性能指标高度相关。
- en: '*You can reproduce all the code used for this article with* [*this notebook*](https://github.com/smazzanti/tds_approximate_predictions_for_feature_selection/blob/main/tds_approximate_predictions_for_feature_selection.ipynb)*.*'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '*你可以通过* [*这个笔记本*](https://github.com/smazzanti/tds_approximate_predictions_for_feature_selection/blob/main/tds_approximate_predictions_for_feature_selection.ipynb)*复制本文中使用的所有代码。*'
- en: '*Thank you for reading!*'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '*感谢你的阅读！*'
- en: '*If you find my work useful, you can subscribe to* [***get an email every time
    that I publish a new article***](https://medium.com/@mazzanti.sam/subscribe) *(usually
    once a month).*'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '*如果你觉得我的工作有用，你可以订阅* [***每次我发布新文章时接收电子邮件***](https://medium.com/@mazzanti.sam/subscribe)
    *(通常每月一次)。*'
- en: '*Want to show me your support for my work? You can* [***buy me a cappuccino***](https://ko-fi.com/samuelemazzanti)*.*'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '*想要支持我的工作？你可以* [***请我喝一杯卡布奇诺***](https://ko-fi.com/samuelemazzanti)*。*'
- en: '*If you’d like,* [***add me on Linkedin***](https://www.linkedin.com/in/samuelemazzanti/)*!*'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '*如果你愿意，* [***加我Linkedin***](https://www.linkedin.com/in/samuelemazzanti/)*！*'
