- en: 'MLX vs MPS vs CUDA: a Benchmark'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: MLX vs MPS vs CUDA：基准测试
- en: 原文：[https://towardsdatascience.com/mlx-vs-mps-vs-cuda-a-benchmark-c5737ca6efc9](https://towardsdatascience.com/mlx-vs-mps-vs-cuda-a-benchmark-c5737ca6efc9)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/mlx-vs-mps-vs-cuda-a-benchmark-c5737ca6efc9](https://towardsdatascience.com/mlx-vs-mps-vs-cuda-a-benchmark-c5737ca6efc9)
- en: A first benchmark of Apple’s new ML framework MLX
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 苹果新 ML 框架 MLX 的首次基准测试
- en: '[](https://tristanbilot.medium.com/?source=post_page-----c5737ca6efc9--------------------------------)[![Tristan
    Bilot](../Images/64c2628ae710042d80ca2ee2feb3da37.png)](https://tristanbilot.medium.com/?source=post_page-----c5737ca6efc9--------------------------------)[](https://towardsdatascience.com/?source=post_page-----c5737ca6efc9--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----c5737ca6efc9--------------------------------)
    [Tristan Bilot](https://tristanbilot.medium.com/?source=post_page-----c5737ca6efc9--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://tristanbilot.medium.com/?source=post_page-----c5737ca6efc9--------------------------------)[![Tristan
    Bilot](../Images/64c2628ae710042d80ca2ee2feb3da37.png)](https://tristanbilot.medium.com/?source=post_page-----c5737ca6efc9--------------------------------)[](https://towardsdatascience.com/?source=post_page-----c5737ca6efc9--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----c5737ca6efc9--------------------------------)
    [Tristan Bilot](https://tristanbilot.medium.com/?source=post_page-----c5737ca6efc9--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----c5737ca6efc9--------------------------------)
    ·6 min read·Dec 15, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page-----c5737ca6efc9--------------------------------)
    ·6分钟阅读·2023年12月15日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/e7da28e0049ecdc0858d1969c4856536.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e7da28e0049ecdc0858d1969c4856536.png)'
- en: Photo by [Javier Allegue Barros](https://unsplash.com/@soymeraki?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由[Javier Allegue Barros](https://unsplash.com/@soymeraki?utm_source=medium&utm_medium=referral)提供，[Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: If you’re a Mac user and a deep learning enthusiast, you’ve probably wished
    at some point that your Mac could handle those heavy models, right? Well, guess
    what? Apple just released [MLX](https://ml-explore.github.io/mlx/build/html/index.html),
    a framework for running ML models efficiently on Apple Silicon.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你是 Mac 用户且对深度学习充满热情，你可能曾希望你的 Mac 能够处理那些大型模型，对吧？好吧，猜猜怎么着？苹果刚刚发布了[MLX](https://ml-explore.github.io/mlx/build/html/index.html)，这是一个在
    Apple Silicon 上高效运行 ML 模型的框架。
- en: The recent introduction of the [MPS backend](https://developer.apple.com/metal/pytorch/)
    in PyTorch 1.12 was already a bold step, but with the announcement of MLX, it
    seems that Apple wants to make a significant leap into open source deep learning.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在 PyTorch 1.12 中引入的[MPS 后端](https://developer.apple.com/metal/pytorch/)已经是一个大胆的步骤，但随着
    MLX 的宣布，似乎苹果想要在开源深度学习领域迈出重要的一步。
- en: In this article, we’ll put these new approaches through their paces, benchmarking
    them against the traditional CPU backend on three different Apple Silicon chips,
    and two CUDA-enabled GPUs. By doing so, we aim to reveal just how much these novel
    Mac-compatible methods can be used in 2024 for deep learning experiments.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我们将对这些新方法进行实际测试，基于三个不同的 Apple Silicon 芯片和两个支持 CUDA 的 GPU 进行基准测试。通过这样做，我们旨在揭示这些新的
    Mac 兼容方法在 2024 年深度学习实验中的实际应用程度。
- en: As a GNN-oriented researcher, I’ll focus the benchmark on a Graph Convolutional
    Network (GCN) model. But since this model mainly consists of linear layers, our
    findings could be insightful even for those not specifically in the GNN sphere.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一名以 GNN 为导向的研究人员，我将把基准测试集中在图卷积网络（GCN）模型上。但由于该模型主要由线性层组成，我们的发现即使对那些不专注于 GNN
    领域的人也可能具有洞察力。
- en: Crafting an environment
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建环境
- en: 'To build an environment for MLX, we have to specify whether using the i386
    or arm architecture. With conda, this can be done using:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 要为 MLX 构建环境，我们必须指定使用 i386 还是 arm 架构。使用 conda 可以通过以下方式完成：
- en: '[PRE0]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'To check if your env is actually using arm, the output of the following command
    should be **arm**, not **i386**:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 要检查你的环境是否实际使用 arm，以下命令的输出应该是**arm**，而不是**i386**：
- en: '[PRE1]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Now simply install MLX using pip, and you’re all set to start exploring:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 现在只需使用 pip 安装 MLX，你就可以开始探索了：
- en: '[PRE2]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: GCN implementation
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: GCN 实现
- en: The GCN model, a type of Graph Neural Network (GNN), works with an adjacency
    matrix (representing the graph structure) and node features. It calculates node
    embeddings by gathering info from neighboring nodes. Specifically, each node gets
    the average of its neighbors’ features. This averaging is done by multiplying
    the node features with the normalized adjacency matrix, adjusted by node degree.
    To learn this process, the features are first projected into an embedding space
    via a linear layer.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: GCN 模型是一种图神经网络（GNN），它使用邻接矩阵（表示图结构）和节点特征。它通过收集邻居节点的信息来计算节点嵌入。具体来说，每个节点的特征是其邻居特征的平均值。这个平均过程是通过将节点特征与归一化的邻接矩阵相乘来完成的，矩阵经过了节点度的调整。为了学习这一过程，特征首先通过线性层投射到嵌入空间。
- en: 'In our version, we normalize the adjacency matrix just like in the original
    paper: during the preprocessing step. While this article won’t go into the preprocessing
    code, you can find with the full code in this GitHub repo:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的版本中，我们在预处理步骤中像原始论文中一样对邻接矩阵进行归一化。虽然这篇文章不会深入讨论预处理代码，但你可以在这个 GitHub 仓库中找到完整代码：
- en: '[](https://github.com/TristanBilot/mlx-GCN?source=post_page-----c5737ca6efc9--------------------------------)
    [## GitHub - TristanBilot/mlx-GCN'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '[## GitHub - TristanBilot/mlx-GCN](https://github.com/TristanBilot/mlx-GCN?source=post_page-----c5737ca6efc9--------------------------------)'
- en: Contribute to TristanBilot/mlx-GCN development by creating an account on GitHub.
  id: totrans-23
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 通过在 GitHub 上创建一个账户，你可以为 TristanBilot/mlx-GCN 的开发做出贡献。
- en: github.com](https://github.com/TristanBilot/mlx-GCN?source=post_page-----c5737ca6efc9--------------------------------)
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '[GitHub](https://github.com/TristanBilot/mlx-GCN?source=post_page-----c5737ca6efc9--------------------------------)'
- en: 'We’ll now walk through implementing a GCN layer and a GCN model using MLX:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将通过使用 MLX 实现一个 GCN 层和一个 GCN 模型：
- en: '[PRE3]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'At a glance, MLX code closely resembles PyTorch code, with a notable difference:
    here we instantiate `self.gcn_layers` as a list of modules, whereas in PyTorch,
    you would typically use `nn.Sequential` for such a purpose.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 一眼看去，MLX 代码与 PyTorch 代码非常相似，有一个显著的不同：在这里，我们将 `self.gcn_layers` 实例化为一个模块列表，而在
    PyTorch 中，通常会使用 `nn.Sequential` 来实现类似功能。
- en: 'The code starts to become quite different within the training loop:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 代码在训练循环中开始变得相当不同：
- en: '[PRE4]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Immediately apparent is the use of `mx.eval()`. In MLX, computations are lazy,
    meaning `eval()` is often used to actually compute new model parameters post-update.
    Another key function, `nn.value_and_grad()`, generates a function that calculates
    loss with respect to parameters. The first argument is the model holding the current
    parameters, and the second is a callable function for the forward pass and loss
    computation. The function it returns takes the same arguments as the forward function
    (in this case, `forward_fn`). We can define this function as follows:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 一眼就能看到 `mx.eval()` 的使用。在 MLX 中，计算是惰性的，这意味着 `eval()` 常用于在更新后实际计算新模型参数。另一个关键函数
    `nn.value_and_grad()` 生成一个函数，用于计算相对于参数的损失。第一个参数是包含当前参数的模型，第二个参数是用于前向传递和损失计算的可调用函数。它返回的函数与前向函数（在此情况下为
    `forward_fn`）具有相同的参数。我们可以这样定义这个函数：
- en: '[PRE5]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'It simply consists in computing a forward pass and calculating the loss. `loss_fn()`
    and `eval_fn()` are defined as follows:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 这仅仅包括计算前向传递和计算损失。`loss_fn()` 和 `eval_fn()` 定义如下：
- en: '[PRE6]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: You might observe that the loss function appears quite extensive, but it essentially
    calculates the cross-entropy between predictions and labels, and includes L2 regularization.
    Since L2 regularization isn’t a built-in feature yet, I’ve implemented it manually.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会注意到损失函数看起来相当复杂，但它实际上是计算预测与标签之间的交叉熵，并包括 L2 正则化。由于 L2 正则化还不是一个内置功能，我已经手动实现了它。
- en: One cool thing here is the elimination of the need to explicitly assign objects
    to a specific device, as we often do in PyTorch with .cuda() and .to(device).
    Thanks to the [unified memory](https://ml-explore.github.io/mlx/build/html/unified_memory.html)
    architecture of the Apple silicon chip, all variables coexist in the same space,
    eradicating slow data transfers between CPU and GPU and eliminating those pesky
    runtime errors related to device mismatches.
  id: totrans-35
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 一个很酷的特点是，不再需要像在 PyTorch 中那样显式地将对象分配到特定的设备上，如使用 `.cuda()` 和 `.to(device)`。得益于苹果芯片的[统一内存](https://ml-explore.github.io/mlx/build/html/unified_memory.html)架构，所有变量都共存于同一空间，消除了
    CPU 和 GPU 之间缓慢的数据传输，也解决了那些与设备不匹配相关的运行时错误。
- en: Benchmark
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 基准
- en: In our benchmark, we’ll be comparing MLX alongside MPS, CPU, and GPU devices,
    using a PyTorch implementation. Our testbed is a 2-layer GCN model, applied to
    the [Cora dataset](https://graphsandnetworks.com/the-cora-dataset/), which includes
    2708 nodes and 5429 edges.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的基准测试中，我们将 MLX 与 MPS、CPU 和 GPU 设备进行比较，使用的是 PyTorch 实现。我们的测试平台是一个 2 层的 GCN
    模型，应用于 [Cora 数据集](https://graphsandnetworks.com/the-cora-dataset/)，该数据集包括 2708
    个节点和 5429 条边。
- en: 'For MLX, MPS, and CPU tests, we benchmark the **M1 Pro**, **M2 Ultra** and
    **M3 Max** ships. Meanwhile, the GPU benchmarks are carried out on two NVIDIA
    Tesla models: the **V100 PCIe** and the **V100 NVLINK**.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 MLX、MPS 和 CPU 测试，我们对 **M1 Pro**、**M2 Ultra** 和 **M3 Max** 进行了基准测试。同时，GPU
    基准测试在两款 NVIDIA Tesla 模型上进行：**V100 PCIe** 和 **V100 NVLINK**。
- en: '![](../Images/e36ebb6ff7aa9c9b9cfe6e32c663110c.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e36ebb6ff7aa9c9b9cfe6e32c663110c.png)'
- en: 'Image by author: Benchmark of GCN running time on MLX and other backends (in
    ms)'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 作者图片：GCN 在 MLX 和其他后端上的基准测试（以毫秒为单位）
- en: '**MPS**: more than 2x faster than CPU on M1 Pro, not bad. On the two other
    chips, we notice 30–50% improvement compared to CPU.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '**MPS**：比 M1 Pro 上的 CPU 快超过 2 倍，表现不错。在其他两款芯片上，与 CPU 相比，改进幅度在 30% 到 50% 之间。'
- en: '**MLX**: 2.34x faster than MPS on M1 Pro. On M2 Ultra we get a 24% improvement
    compared to MPS. No real improvement between MPS and MLX on M3 Pro though.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '**MLX**：在 M1 Pro 上比 MPS 快 2.34 倍。在 M2 Ultra 上相比 MPS 提升了 24%。不过，在 M3 Pro 上 MPS
    和 MLX 之间没有实质性改进。'
- en: '**CUDA V100 PCIe & NVLINK**: only 23% and 34% faster than M3 Max with MLX,
    this is some serious stuff!'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '**CUDA V100 PCIe & NVLINK**：比 M3 Max with MLX 快 23% 和 34%，这确实很厉害！'
- en: MLX stands out as a game changer when compared to CPU and MPS, and it even comes
    close to the performance of a TESLA V100\. This initial benchmark highlights MLX’s
    significant potential to emerge as a popular Mac-based deep learning framework.
    It’s also worth noting that MLX has only recently been released to the public,
    and we can expect further enhancements from the open-source community in the coming
    years. We can also expect even more powerful Apple Silicon chips in the near future,
    taking performance of MLX to a whole new level.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 与 CPU 和 MPS 相比，MLX 突显出其作为游戏改变者的潜力，甚至接近 TESLA V100 的性能。这一初步基准测试突出了 MLX 在成为流行的
    Mac 基础深度学习框架方面的巨大潜力。值得注意的是，MLX 刚刚公开发布，我们可以期待未来几年开源社区带来的进一步改进。我们也期待在不久的将来，出现更强大的
    Apple Silicon 芯片，将 MLX 的性能提升到一个全新的水平。
- en: To recap
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 总结一下
- en: '**Cool things:**'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '**酷炫功能：**'
- en: We can now run deep learning models locally by leveraging the full power of
    Apple Silicon.
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们现在可以通过充分利用 Apple Silicon 的强大性能在本地运行深度学习模型。
- en: The syntax is pretty much similar as torch, with some inspirations from Jax.
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 语法与 torch 非常相似，受到了 Jax 的一些启发。
- en: No more device, everything lives in unified memory!
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不再需要设备，一切都存在于统一内存中！
- en: '**What’s missing:**'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '**缺少的功能：**'
- en: The framework is **very** young, many features are missing yet. Especially for
    Graph ML, all sparse operations and scattering APIs are not available at the moment,
    making it complicate to build Message Passing GNNs on top of MLX now.
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这个框架 **非常** 年轻，许多功能尚未完善。特别是对于 Graph ML，目前没有所有稀疏操作和散布 API，这使得在 MLX 上构建 Message
    Passing GNNs 变得复杂。
- en: As a new project, it’s worth noting that both the documentation and community
    discussions for MLX are somewhat limited at present.
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 作为一个新项目，目前 MLX 的文档和社区讨论相对有限。
- en: In conclusion, MLX made a surprisingly impactful entrance upon its release and
    demonstrates serious potential. I believe this framework could become a staple
    for daily research experiments. We’re also eager to see additional experiments,
    as the GCN tests primarily showcase MLX’s performance on basic linear layers.
    More comprehensive testing could reveal its full capabilities.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，MLX 的发布给人留下了意外的深刻印象，并展示了严肃的潜力。我相信这个框架可能会成为日常研究实验的基础工具。我们也期待更多的实验，因为 GCN 测试主要展示了
    MLX 在基本线性层上的表现。更全面的测试可能会揭示它的全部能力。
- en: '**Thx for reading!**'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '**感谢阅读！**'
