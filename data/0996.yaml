- en: Concurrently Train Multiple Time Series Models Over Spark with XGBoost
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 同时在Spark上训练多个时间序列模型，使用XGBoost
- en: 原文：[https://towardsdatascience.com/concurrently-train-multiple-time-series-models-over-spark-with-xgboost-c6d5ec4a6430?source=collection_archive---------5-----------------------#2023-03-17](https://towardsdatascience.com/concurrently-train-multiple-time-series-models-over-spark-with-xgboost-c6d5ec4a6430?source=collection_archive---------5-----------------------#2023-03-17)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/concurrently-train-multiple-time-series-models-over-spark-with-xgboost-c6d5ec4a6430?source=collection_archive---------5-----------------------#2023-03-17](https://towardsdatascience.com/concurrently-train-multiple-time-series-models-over-spark-with-xgboost-c6d5ec4a6430?source=collection_archive---------5-----------------------#2023-03-17)
- en: Take advantage of the distributive power of Apache Spark and concurrently train
    thousands of auto-regressive time-series models on big data
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 利用Apache Spark的分布式计算能力，同时在大数据上训练成千上万的自回归时间序列模型。
- en: '[](https://medium.com/@alon.agmon?source=post_page-----c6d5ec4a6430--------------------------------)[![Alon
    Agmon](../Images/c64e33ca886da4f4984c96acc7116a4d.png)](https://medium.com/@alon.agmon?source=post_page-----c6d5ec4a6430--------------------------------)[](https://towardsdatascience.com/?source=post_page-----c6d5ec4a6430--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----c6d5ec4a6430--------------------------------)
    [Alon Agmon](https://medium.com/@alon.agmon?source=post_page-----c6d5ec4a6430--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@alon.agmon?source=post_page-----c6d5ec4a6430--------------------------------)[![Alon
    Agmon](../Images/c64e33ca886da4f4984c96acc7116a4d.png)](https://medium.com/@alon.agmon?source=post_page-----c6d5ec4a6430--------------------------------)[](https://towardsdatascience.com/?source=post_page-----c6d5ec4a6430--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----c6d5ec4a6430--------------------------------)
    [Alon Agmon](https://medium.com/@alon.agmon?source=post_page-----c6d5ec4a6430--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fbcd1e3126cdc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fconcurrently-train-multiple-time-series-models-over-spark-with-xgboost-c6d5ec4a6430&user=Alon+Agmon&userId=bcd1e3126cdc&source=post_page-bcd1e3126cdc----c6d5ec4a6430---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----c6d5ec4a6430--------------------------------)
    ·12 min read·Mar 17, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fc6d5ec4a6430&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fconcurrently-train-multiple-time-series-models-over-spark-with-xgboost-c6d5ec4a6430&user=Alon+Agmon&userId=bcd1e3126cdc&source=-----c6d5ec4a6430---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fbcd1e3126cdc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fconcurrently-train-multiple-time-series-models-over-spark-with-xgboost-c6d5ec4a6430&user=Alon+Agmon&userId=bcd1e3126cdc&source=post_page-bcd1e3126cdc----c6d5ec4a6430---------------------post_header-----------)
    发表在[Towards Data Science](https://towardsdatascience.com/?source=post_page-----c6d5ec4a6430--------------------------------)
    ·12分钟阅读·2023年3月17日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fc6d5ec4a6430&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fconcurrently-train-multiple-time-series-models-over-spark-with-xgboost-c6d5ec4a6430&user=Alon+Agmon&userId=bcd1e3126cdc&source=-----c6d5ec4a6430---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc6d5ec4a6430&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fconcurrently-train-multiple-time-series-models-over-spark-with-xgboost-c6d5ec4a6430&source=-----c6d5ec4a6430---------------------bookmark_footer-----------)![](../Images/1987a9fe765a50a4ebf82e7533592137.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc6d5ec4a6430&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fconcurrently-train-multiple-time-series-models-over-spark-with-xgboost-c6d5ec4a6430&source=-----c6d5ec4a6430---------------------bookmark_footer-----------)![](../Images/1987a9fe765a50a4ebf82e7533592137.png)'
- en: Photo by [Ricardo Gomez Angel](https://unsplash.com/@rgaleriacom?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 照片由[Ricardo Gomez Angel](https://unsplash.com/@rgaleriacom?utm_source=medium&utm_medium=referral)拍摄，发布在[Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)上。
- en: 1\. Intro
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1\. 简介
- en: Suppose you have a large dataset consisting of your customers’ hourly transactions,
    and you were tasked with helping your company forecast and identify anomalies
    in their transaction patterns. “If the transaction rate of some customers is suddenly
    declining then we want to know about it”, the product manager explains, “the problem
    is that we have to automate this because we just have too many customers to keep
    track”. You have enough data to train a decent time-series model, but because
    transaction patterns differ quite a bit between customers, then you need to train
    a *model* *for each customer* in order to accurately forecast and detect anomalies
    in *their* specific usage patterns.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你有一个包含客户每小时交易的大型数据集，并且你的任务是帮助公司预测和识别其交易模式中的异常。产品经理解释道：“如果某些客户的交易率突然下降，我们希望了解这一点，问题是我们必须自动化这一过程，因为我们有太多客户需要跟踪。”你有足够的数据来训练一个不错的时间序列模型，但由于客户之间的交易模式差异较大，你需要为*每个客户*训练一个*模型*，以准确预测和检测*他们*特定的使用模式中的异常。
- en: I believe that this is quite a common task for many data scientists and machine
    learning engineers working with SaaS or retail customer data. From a machine learning
    perspective, this doesn’t seem like such a complicated task, but it can quickly
    turn into an engineering nightmare. What if we have thousands or even hundreds
    of thousands of customers? How should we train and manage thousands of models?
    What if we need to create this forecast relatively frequently? or even in real
    time? When data volume is constantly growing then even naive requirements can
    quickly get demanding and we need to ensure we have an infrastructure that can
    scale reliably as our data grows.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 我认为这对许多处理SaaS或零售客户数据的数据科学家和机器学习工程师来说是相当常见的任务。从机器学习的角度来看，这似乎不是一个复杂的任务，但它很快可能变成工程噩梦。如果我们有数千甚至数十万的客户怎么办？我们应该如何训练和管理数千个模型？如果我们需要相对频繁地创建这些预测，甚至是实时预测怎么办？当数据量不断增长时，即使是简单的需求也可能变得要求苛刻，我们需要确保拥有一个可以随着数据增长而可靠扩展的基础设施。
- en: Concurrently training multiple models on a huge dataset is actually one of the
    few cases that justifies training on a distributed cluster, such as Spark. I know
    that this is a controversial claim, but when it comes to structured tabular data
    then training on a distributed cluster (rather than on sampled data, for example)
    is often not justified. However, when the data we need to process is genuinely
    “big”, and we need to break it to many datasets and train a ML model on each,
    then Spark seems like the right direction.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 同时在巨大数据集上训练多个模型实际上是为分布式集群（如Spark）训练提供合理性的一种少数情况之一。我知道这是一种有争议的说法，但对于结构化的表格数据，通常在分布式集群上训练（而不是在抽样数据上，例如）往往并不合理。然而，当我们需要处理的数据确实是“庞大的”，并且我们需要将其拆分成多个数据集并在每个数据集上训练一个ML模型时，那么Spark似乎是正确的方向。
- en: Using Spark for model training provides a lot of capabilities but it also poses
    quite a few challenges, mostly around how data should be organized and formatted.
    The purpose of this post is to demonstrate at least one way in which this task
    can be accomplished, end to end, using Spark (and Scala)— from data formatting
    to model training and prediction.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Spark进行模型训练提供了很多功能，但也带来了相当多的挑战，主要集中在数据如何组织和格式化上。本文的目的是展示至少一种使用Spark（和Scala）从数据格式化到模型训练和预测的完整解决方案。
- en: Specifically, in what follows we are going to train an autoregressive (“AR”)
    time-series model using XGBoost over each of our customers time-series data. AR
    models, in short, take the value to be predicted as a linear function of its previous
    values. In other words, it models the number of transactions that a given customer
    will have at hour *h* as a function of the number of transactions they had on
    hours *h -1, h -2, h -3, h -n*. Such models are usually fairly reliable in giving
    a decent forecast for such tasks, and can be also implemented using boosting trees
    models which are widely available and easy to use. Indeed we will naively implement
    this using XGBoost regression.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 具体而言，在接下来的内容中，我们将使用XGBoost在每个客户的时间序列数据上训练一个自回归（“AR”）时间序列模型。简而言之，AR模型将待预测值作为其先前值的线性函数。换句话说，它将给定客户在小时*h*的交易数量建模为他们在小时*h
    -1, h -2, h -3, h -n*的交易数量的函数。这类模型通常能为此类任务提供相当可靠的预测，也可以使用广泛可用且易于使用的提升树模型来实现。实际上，我们将简单地使用XGBoost回归来实现这一点。
- en: The trickiest part in time-series training and prediction is to correctly “engineer”
    the features. Section 2 shortly explains how auto-regression works in the context
    of time-series, and shows how time-series data can be modeled for AR tasks using
    pure SQL. Section 3 focuses on how such dataset should be loaded to Spark, and
    shows how it can be “broken” into multiple training tasks and datasets. At last
    some of the complexity involved in training ML models over Spark is the use of
    Spark’s MLlib which some find tedious and counter-intuitive. I will demonstrate
    how this task can be accomplished without using MLlib API. Section 4 is dedicated
    to the prediction or forecast stage. Section 5 concludes.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 时间序列训练和预测中最棘手的部分是正确“工程化”特征。第 2 节简要说明了自回归在时间序列中的工作原理，并展示了如何使用纯 SQL 对时间序列数据进行
    AR 任务建模。第 3 节重点讲述了如何将数据集加载到 Spark 中，并展示了如何将其“拆分”成多个训练任务和数据集。最后，使用 Spark 的 MLlib
    进行机器学习模型训练所涉及的一些复杂性被认为是繁琐且直观不易的。我将演示如何在不使用 MLlib API 的情况下完成这一任务。第 4 节专注于预测或预报阶段。第
    5 节进行总结。
- en: 2\. Basic feature engineering for AR time-series models
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2\. AR 时间序列模型的基本特征工程
- en: The trickiest part in modelling time-series data as an auto-regressive problem
    is to properly organize and format it. A simplified and practical example might
    make the idea (and challenge) clearer.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 将时间序列数据建模为自回归问题中最棘手的部分是正确地组织和格式化数据。一个简化的实际示例可能会使这个想法（和挑战）更清晰。
- en: Suppose we have hourly transaction data collected over 6 hours — from 8AM to
    1PM, and we want to forecast the number of transactions each customer will have
    at 2PM.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有收集了 6 小时的每小时交易数据——从上午 8 点到下午 1 点，我们想要预测每个客户在下午 2 点的交易次数。
- en: '![](../Images/d45587374fff440ab8213bd8dca64ac7.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d45587374fff440ab8213bd8dca64ac7.png)'
- en: Dataset A — The data format we start with
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集 A — 我们开始使用的数据格式
- en: We decide to use 4 parameters in our regression, which means that we are going
    to use the number of transactions that the customer had on 10AM to 1PM in order
    to forecast the number of transactions they will have at 2PM. This reflects a
    more general intuition about our data —that 4 hours of data is enough to accurately
    forecast or explain the 5th hour (note that this is a very naive and simplified
    example, this is obviously not the case in the real world ). This means that if
    we have enough data samples then a well trained model should be able to learn
    patterns in the customer’s data that will enable it to accurately forecast the
    number of transaction in any hour given the 4 hours that preceded (I’m intentionally
    ignoring the idea of seasonality, which is an important concept in time-series
    analysis).
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 我们决定在回归中使用 4 个参数，这意味着我们将使用客户在上午 10 点到下午 1 点之间的交易次数来预测他们在下午 2 点的交易次数。这反映了我们对数据的一个更普遍的直觉——4
    小时的数据足以准确预测或解释第 5 小时（请注意，这是一个非常天真的简化示例，现实中显然并非如此）。这意味着，如果我们有足够的数据样本，那么一个训练良好的模型应该能够学习客户数据中的模式，从而准确预测给定前
    4 小时的任何时刻的交易数量（我故意忽略了季节性这一概念，它在时间序列分析中是一个重要的概念）。
- en: 'To train such model we need to create a training set with 4 features. Each
    row in our training set will consist of a target variable that represents the
    number of transactions at a given hour, and 4 parameters that capture the number
    of transactions in the 4 hours that preceded. By “pivoting” the table above, and
    creating a sliding window of the given shift ( 4 hours) we can create a dataset
    that will look something like this (per customer):'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 要训练这样的模型，我们需要创建一个具有 4 个特征的训练集。我们训练集中的每一行将包括一个目标变量，该变量表示给定小时的交易次数，以及 4 个捕捉前 4
    小时交易次数的参数。通过“透视”上表，并创建一个滑动窗口（4 小时），我们可以创建一个类似于下面的数据集（按客户划分）：
- en: '![](../Images/13363e9afd51e529d77e240ef3031f36.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/13363e9afd51e529d77e240ef3031f36.png)'
- en: Dataset B (the first 2 lines represent the actual data) and the lines below
    explain what this data represents)
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集 B（前两行表示实际数据），下面的行解释了这些数据代表什么）
- en: Ideally we will have more data, but the idea is the same, our model is supposed
    to “see” enough samples of 4 hours in order to learn how to correctly predict
    the 5th hour, which is our *y* or target variable. Because we want our model to
    detect patterns in our data we need to make sure it learns enough — sometimes
    6 hours will be enough to accurately forecast the 7th hour, and sometimes we will
    need at least a week.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 理想情况下，我们会有更多数据，但思想是相同的，我们的模型应该“看到”足够多的 4 小时样本，以便学会如何正确预测第 5 小时，即我们的 *y* 或目标变量。因为我们希望模型检测数据中的模式，所以需要确保它学习得足够
    — 有时 6 小时就足以准确预测第 7 小时，有时我们需要至少一周的数据。
- en: 'One simple way to create such a training set for this task is with an SQL query
    (that can be run using SparkSQL or some other query engine) that looks something
    like this:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 创建这样一个训练集的简单方法是使用 SQL 查询（可以使用 SparkSQL 或其他查询引擎运行），查询如下：
- en: '[PRE0]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The query starts with 2 WITH clauses: the first just extracts a list of customers
    we are interested in. Here you can add any condition that is supposed to filter
    in or out specific customers (perhaps you want to filter new customers or only
    include customers with sufficient traffic). The second WITH clause simply creates
    the first data set — *Dataset A*, which pulls a week of data for these customers
    and selects the customer id, date, hour, and number of transactions.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 查询以 2 个 WITH 子句开始：第一个子句仅提取我们感兴趣的客户列表。在这里，您可以添加任何条件以过滤特定客户（也许您想过滤新客户或仅包括流量足够的客户）。第二个
    WITH 子句简单地创建第一个数据集 — *数据集 A*，该数据集提取这些客户一周的数据，并选择客户 ID、日期、小时和交易次数。
- en: 'Finally, the last and most important SELECT clause generates *Dataset B,* by
    using SQL *lag()* function on each row in order to capture the number of transactions
    in each of the hours that preceded the hour in the row. Our result should look
    something like this:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，最重要的 SELECT 子句生成 *数据集 B*，通过对每一行使用 SQL *lag()* 函数，以捕获在该行时间之前的每小时的交易次数。我们的结果应该类似于：
- en: '[PRE1]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: As you can see, each row (that contains all the lagged values), has a customer
    id, the hour (as truncated date), the hour (represented as integer), the number
    of transactions that the customer had in that hour (this will be the target variable
    in our training set), and then 4 fields that capture the lagged number of transactions
    in the 4 hours that preceded the target variable (these will be features or parameters
    in which our autoregression model will learn to identify patterns).
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，每一行（包含所有滞后值），都有一个客户 ID、小时（作为截断日期）、小时（表示为整数）、客户在该小时的交易次数（这将是我们训练集中的目标变量），以及
    4 个字段，捕获目标变量之前 4 小时的滞后交易次数（这些将是特征或参数，我们的自回归模型将学习识别模式）。
- en: Now that we have our dataset ready, we can move to training with Spark
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经准备好了数据集，可以开始使用 Spark 进行训练。
- en: 3\. Data loading and model training over Spark
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3. 数据加载和模型训练使用 Spark
- en: '***3.1 Data Loading*** At this point we have our dataset almost ready for model
    training and prediction. After much of the heavy lifting involved in organizing
    the data in sliding windows was done using SQL. The next stage is to read the
    results using Spark and create a typed Spark Dataset that is ready for model training.
    This process or transformation will be implemented using the function below (explanation
    immediately follows)'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '***3.1 数据加载*** 此时，我们的数据集几乎已准备好进行模型训练和预测。大部分涉及数据在滑动窗口中组织的繁重工作已通过 SQL 完成。下一阶段是使用
    Spark 读取结果并创建一个准备好进行模型训练的类型化 Spark 数据集。这个过程或转换将通过下面的函数实现（解释紧随其后）。'
- en: 'The typed dataset will be based on a case class named *FeaturesRecord* (line
    1–2) that will represent a data sample. Each feature record has 4 properties:
    *key* is the customer id, *ts* is the time that this record captures, *label*
    is the target variable — the number of transactions in that specific time or *ts*,
    and *features* is a sequence of values that represent of the number of transactions
    the customer had in the preceding hours.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 类型化数据集将基于名为 *FeaturesRecord* 的案例类（第 1 行–2 行），它将表示一个数据样本。每个特征记录有 4 个属性：*key*
    是客户 ID，*ts* 是该记录捕获的时间，*label* 是目标变量 — 特定时间或 *ts* 的交易次数，*features* 是一个值序列，代表客户在前几小时的交易次数。
- en: The extraction of all the variables in the function above is pretty straightforward.
    The functional trick (that Scala makes possible) is the way we build the features
    vector (lines 11–12). Also, as you can see, other features can be added to the
    feature vector, such as the *day_of_week*, that will allow our model to learn
    some seasonality in our data.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 上述函数中所有变量的提取相当直接。功能性技巧（Scala 实现的）是我们构建特征向量的方式（第 11-12 行）。此外，如你所见，其他特征也可以添加到特征向量中，例如
    *day_of_week*，这将使我们的模型能够学习数据中的一些季节性。
- en: '***3.2 Model Training***'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '***3.2 模型训练***'
- en: The training of the model will proceed in 3 stages. Beforehand, recall that
    one requirement that complicates our task is that we need to train a model per
    customer, in order to capture the patterns and seasonality that characterize each.
    However, what we currently have is a dataset that contains data from *all* customers
    pooled together. So the first thing we need to do is “break” it in smaller datasets
    and then train our model on each.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 模型的训练将分为三个阶段。事先回顾一下，复杂化我们任务的一个要求是我们需要为每个客户训练一个模型，以捕捉每个客户的模式和季节性。然而，我们目前拥有的是一个包含*所有*客户数据的数据集。所以我们需要做的第一件事是“拆分”成更小的数据集，然后在每个数据集上训练我们的模型。
- en: Luckily, that can be done relatively easily using Spark’s (very useful) function
    *flatMapGroups()* which does exactly that.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，这可以通过使用 Spark 的（非常有用的）*flatMapGroups()* 函数相对容易地完成，它正是做这件事的。
- en: '[PRE2]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Recall that our dataset is typed and based on the class FeaturesRecord, which
    has a *key* property that represents the customer ID. Using Spark’s *groupByKey()*
    followed by *flatMapGroups()* , we are able to “break” our huge dataset, and call
    the function *predict()* on each customer’s data (the function actually does both
    training and prediction). Depending on Spark’s configuration, Spark will also
    be able to parallelize this call, which means that we would also be able to do
    so concurrently in a manner that will allow us to scale and do it fast.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，我们的数据集是类型化的，并基于类 FeaturesRecord，该类具有代表客户 ID 的 *key* 属性。使用 Spark 的 *groupByKey()*
    后跟 *flatMapGroups()*，我们能够“拆分”我们的庞大数据集，并在每个客户的数据上调用 *predict()* 函数（该函数实际上同时执行训练和预测）。根据
    Spark 的配置，Spark 也能并行化此调用，这意味着我们也能以允许扩展和快速处理的方式并发执行。
- en: 'Lets look at the predict() function line by line (though the next section will
    focus on the forecast and prediction staeg:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们逐行查看 predict() 函数（尽管下一节将重点关注预测和预测阶段）：
- en: Main flow
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 主要流程
- en: The predict function unfolds in 3 stages and eventually returns a case class
    of type *PredictionResult* (I will talk about this in the next section). In the
    first stage, the function *getForcastDatasets()* simply divides the records into
    2 sequences— one that is based on all records *but* the last 2 hours (that’s the
    dataset that our model is going to learn) and a sequence with just the last 2
    hours (that’s the sequence that our model is going to predict).
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: predict 函数展开为三个阶段，最终返回一个类型为*PredictionResult*的案例类（我将在下一节中讨论这个）。在第一阶段，函数 *getForcastDatasets()*
    仅仅将记录分为两个序列——一个基于所有记录*但*最后两小时（这是我们的模型将要学习的数据集），另一个仅包含最后两小时（这是我们的模型将要预测的序列）。
- en: In our case, I have to ignore the last datapoint as it might be incomplete,
    so training data will consist of all the records but the last 3, and the forecast
    data will be based on the records N-1 and N-2 (excluding N — the last one).
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的案例中，我必须忽略最后一个数据点，因为它可能不完整，因此训练数据将包括所有记录，但不包括最后三个，而预测数据将基于记录 N-1 和 N-2（排除
    N——最后一个）。
- en: In the second stage, we train our model on the training set. The train function
    is rather simple. We basically call XGBoost’s train function using our training
    dataset and a map with the regression parameters. The function returns a Booster
    object or a trained model. The Booster object will next be used to run prediction
    on the prediction sequence. (It is important to note that model selection needs
    to be done carefully, as not all time-series problems behave in a way that tree
    models can well capture.)
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在第二阶段，我们在训练集上训练我们的模型。训练函数相当简单。我们基本上调用 XGBoost 的训练函数，使用我们的训练数据集和一个包含回归参数的映射。该函数返回一个
    Booster 对象或一个训练好的模型。接下来，Booster 对象将用于在预测序列上运行预测。（需要注意的是，模型选择需要小心，因为并非所有时间序列问题都适合树模型的捕捉。）
- en: There is, however, one trick that should be mentioned. As you can see, before
    we pass the sequence of FeatureRecords to the train method, we call a function
    named *toDMatrix()*. XGBoost’s train method requires a DMatrix object to represent
    its sample data. To do so, we need to transform each sequence of FeatureRecords
    to a DMatrix. In order to so, I have created the *toDMatrix()* function using
    an implicit scala class that take a *Seq* as a parameter and offers a toDMatrix
    function that transforms each FeatureRecord into XBoost’s LabeledPoint which is
    then fed into a DMatrix and returned by the function.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，有一个技巧值得提及。如您所见，在我们将 FeatureRecords 序列传递给训练方法之前，我们调用了一个名为 *toDMatrix()* 的函数。XGBoost
    的训练方法需要一个 DMatrix 对象来表示其样本数据。为此，我们需要将每个 FeatureRecords 序列转换为 DMatrix。为此，我创建了 *toDMatrix()*
    函数，使用隐式 Scala 类，该类接受一个 *Seq* 作为参数，并提供一个 toDMatrix 函数，将每个 FeatureRecord 转换为 XBoost
    的 LabeledPoint，然后输入到 DMatrix 中，并由函数返回。
- en: Now that we have the functions required to transform a dataset to training dataset
    for XGBoost, we can run training and use the trained model to predict the next
    few hours.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经拥有将数据集转换为 XGBoost 训练数据集所需的函数，我们可以进行训练并使用训练好的模型预测接下来的几个小时。
- en: 4 Prediction and Forecast
  id: totrans-49
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 预测和预报
- en: 'Ideally, during training our model has “seen” many samples of 4 hours, and
    learned how to forecast the 5th. This can be useful for 2 kinds of tasks: simple
    forecast and anomaly detection. The latter task is an interesting and useful one
    in this context. The common technique is pretty simple: if we take time series
    data of the last few hours and use our model to forecast the last one, then a
    well trained model should give us a prediction that is pretty close to the actual
    number. However, if the predicted number is too high or too low, then this could
    mean 2 things: either the model is not accurate or the real data of the last hour
    is unexpected or anomalous. Imagine, for example, that we learn to identify traffic
    pressure over 24 hours in a specific junction, and one day there is an unusual
    traffic jam due to an accident. If we predict this hour and compare it to the
    actual data, then we will see that the predicted pressure is significantly lower
    than the actual one. This might mean that the model is simply wrong or that it
    accurately captures the fact that there is an anomaly at the moment.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 理想情况下，在训练过程中，我们的模型已经“看到”了许多4小时的样本，并学会了如何预测第5小时。这对于两种任务非常有用：简单预测和异常检测。后者在此背景下是一个有趣且有用的任务。常见的技术非常简单：如果我们利用最后几个小时的时间序列数据，并用我们的模型预测最后一个小时，那么一个训练良好的模型应该会给我们一个与实际数字非常接近的预测。然而，如果预测的数字过高或过低，则可能意味着两种情况：要么模型不准确，要么最后一个小时的真实数据是意外的或异常的。例如，假设我们学会了识别特定交汇点24小时内的交通压力，一天由于事故出现了异常交通拥堵。如果我们预测这一小时并与实际数据进行比较，我们会发现预测的压力明显低于实际压力。这可能意味着模型完全错误，或者它准确地捕捉到当前存在异常的事实。
- en: We will follow this logic and measure the difference between the predicted value
    and the actual value by dividing them. In this way, assuming that our model is
    accurate, a value of 1 will indicate that the actual data is indeed predicted
    and expected while a value of less than 1 will mean that actual data seems to
    be lower than expected and we might need to alert about this customer.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将遵循这个逻辑，通过将预测值与实际值进行除法运算来衡量它们之间的差异。这样，假设我们的模型是准确的，值为1将表明实际数据确实被预测并期望，而值小于1将意味着实际数据似乎低于预期，我们可能需要对此客户发出警报。
- en: For the purpose of prediction we will use a case class named *PredictionResult*
    that will consist of a *key* (customer), *ts* (timestamp), *label* (actual data),
    *prediction* (forecast value), and *ratio* (which is the diff between them).
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进行预测，我们将使用一个名为 *PredictionResult* 的 case 类，该类将包含一个 *key*（客户）、*ts*（时间戳）、*label*（实际数据）、*prediction*（预测值）和
    *ratio*（两者之间的差异）。
- en: 'We generate predictions by calling the Booster’s *predict()* method on the
    feature vector. Next, we *zip()* the feature record with the relevant forecast
    result in order to construct, for each prediction, a *PredictionResult* object
    (line 11) that will also calculate the ratio between the actual and predicted
    value. Finally we construct a typed dataset using the list of PredictionResult
    which will look as follows:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过在特征向量上调用 Booster 的 *predict()* 方法来生成预测。接下来，我们将特征记录与相关的预测结果 *zip()* 在一起，以构建每个预测的
    *PredictionResult* 对象（第11行），该对象还将计算实际值与预测值之间的比率。最后，我们使用 PredictionResult 的列表构建一个类型化的数据集，其样例如下：
- en: '![](../Images/0d279d2ab4efe6e6be71bbd13ff5eb16.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0d279d2ab4efe6e6be71bbd13ff5eb16.png)'
- en: As you can see, the ratio column can help us identify interesting anomalies.
    For most records, you can see that the value is pretty close to 1, which means
    that the actual value is almost identical to the predicted one, but we can certainly
    find some anomalies, mostly such that the actual value is 20%–40% higher than
    the predicted one.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所见，比例列可以帮助我们识别有趣的异常。对于大多数记录，你可以看到值非常接近1，这意味着实际值几乎与预测值相同，但我们确实可以找到一些异常，主要是实际值比预测值高出20%–40%。
- en: 5\. Conclusion
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5\. 结论
- en: Multiple model training in scale is an engineering challenge, especially where
    data is constantly growing and we want to benefit from more of it. ML pipelines
    that work great for one, five or ten models might start to choke when the number
    of concurrently trained models grows to hundreds or even thousands.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 大规模多模型训练是一项工程挑战，特别是当数据不断增长时，我们希望从更多数据中获益。对于一个、五个或十个模型表现良好的机器学习管道，当同时训练的模型数量增加到数百甚至上千时，可能会开始出现瓶颈。
- en: Apache Spark is a great tool for training in scale but using it for machine
    learning tasks is quite different than the common Python-based frameworks, and
    not suitable for every project. In this post, I demonstrated how we can create
    a training pipeline that will be able to scale. I picked time-series forecasting
    as an example, because it is considered a relatively complicated task from an
    engineering perspective, though it can be easily adapted to regression and classification
    problems using the same stack.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Spark是一个很棒的大规模训练工具，但用于机器学习任务时，它与常见的基于Python的框架有所不同，并且并不适合所有项目。在这篇文章中，我演示了如何创建一个能够扩展的训练管道。我选择了时间序列预测作为示例，因为从工程角度来看，它被认为是相对复杂的任务，尽管可以很容易地使用相同的技术栈适应回归和分类问题。
- en: We saw that much of the heavy lifting involved in creating the training set
    can (and should) be done using SQL over a query engine that supports SQL, such
    as Spark for example, for better readability and speed. Next, we used Spark’s
    Dataset APIs in order to split a huge dataset to many small training tasks. Finally,
    we used XGBoost’s booster API to create train an auto-regressive TS model, and
    use it to detect anomalies in our customers’ data. As mentioned earlier, multiple
    model training is a task that can quickly get very complicated. In this post,
    I tried to show that Spark’s API give us enough tools to make it relatively straightforward,
    and keep it simple, as much as we can.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到，创建训练集中的大量工作可以（也应该）通过SQL在支持SQL的查询引擎上完成，例如Spark，以获得更好的可读性和速度。接下来，我们使用Spark的Dataset
    APIs将一个巨大的数据集拆分成多个小的训练任务。最后，我们使用XGBoost的booster API来训练一个自回归时间序列模型，并用它来检测客户数据中的异常。如前所述，多模型训练是一项很快变得非常复杂的任务。在这篇文章中，我试图展示Spark的API为我们提供了足够的工具，使其相对简单，并尽可能保持简洁。
- en: Happy engineering! Hope this was helpful
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 祝工程愉快！希望这对你有帮助。
- en: '** You can find a link to the full sample code [here](https://github.com/a-agmon/spark-ml/blob/main/spark-ts-xb.scala)'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '** 你可以在[这里](https://github.com/a-agmon/spark-ml/blob/main/spark-ts-xb.scala)找到完整示例代码的链接'
- en: '*** there are many resources on time series analysis and auto-regression. I
    think that the resource that at I have learnt the most from is: *Forecasting:
    Principles and Practice (2nd ed)* by Rob J Hyndman and George Athanasopoulos (available
    online [here](https://otexts.com/fpp2/advanced.html))'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '*** 关于时间序列分析和自回归有很多资源。我认为我从中学到最多的资源是：*Forecasting: Principles and Practice
    (2nd ed)*，作者Rob J Hyndman和George Athanasopoulos（在线获取地址：[这里](https://otexts.com/fpp2/advanced.html)）'
