- en: Building A Graph Convolutional Network for Molecular Property Prediction
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建分子属性预测的图卷积网络
- en: 原文：[https://towardsdatascience.com/building-a-graph-convolutional-network-for-molecular-property-prediction-978b0ae10ec4?source=collection_archive---------2-----------------------#2023-12-23](https://towardsdatascience.com/building-a-graph-convolutional-network-for-molecular-property-prediction-978b0ae10ec4?source=collection_archive---------2-----------------------#2023-12-23)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/building-a-graph-convolutional-network-for-molecular-property-prediction-978b0ae10ec4?source=collection_archive---------2-----------------------#2023-12-23](https://towardsdatascience.com/building-a-graph-convolutional-network-for-molecular-property-prediction-978b0ae10ec4?source=collection_archive---------2-----------------------#2023-12-23)
- en: Artificial Intelligence
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 人工智能
- en: Tutorial to make molecular graphs and develop a simple PyTorch-based GCN
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 制作分子图和开发一个基于 PyTorch 的简单 GCN 的教程
- en: '[](https://medium.com/@ChemAndCode?source=post_page-----978b0ae10ec4--------------------------------)[![Gaurav
    Deshmukh](../Images/98433b1a256f160792a7b2b0874a2081.png)](https://medium.com/@ChemAndCode?source=post_page-----978b0ae10ec4--------------------------------)[](https://towardsdatascience.com/?source=post_page-----978b0ae10ec4--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----978b0ae10ec4--------------------------------)
    [Gaurav Deshmukh](https://medium.com/@ChemAndCode?source=post_page-----978b0ae10ec4--------------------------------)'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@ChemAndCode?source=post_page-----978b0ae10ec4--------------------------------)[![Gaurav
    Deshmukh](../Images/98433b1a256f160792a7b2b0874a2081.png)](https://medium.com/@ChemAndCode?source=post_page-----978b0ae10ec4--------------------------------)[](https://towardsdatascience.com/?source=post_page-----978b0ae10ec4--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----978b0ae10ec4--------------------------------)
    [Gaurav Deshmukh](https://medium.com/@ChemAndCode?source=post_page-----978b0ae10ec4--------------------------------)'
- en: ·
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F5a75283b2c71&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-a-graph-convolutional-network-for-molecular-property-prediction-978b0ae10ec4&user=Gaurav+Deshmukh&userId=5a75283b2c71&source=post_page-5a75283b2c71----978b0ae10ec4---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----978b0ae10ec4--------------------------------)
    ·17 min read·Dec 23, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F978b0ae10ec4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-a-graph-convolutional-network-for-molecular-property-prediction-978b0ae10ec4&user=Gaurav+Deshmukh&userId=5a75283b2c71&source=-----978b0ae10ec4---------------------clap_footer-----------)'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F5a75283b2c71&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-a-graph-convolutional-network-for-molecular-property-prediction-978b0ae10ec4&user=Gaurav+Deshmukh&userId=5a75283b2c71&source=post_page-5a75283b2c71----978b0ae10ec4---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----978b0ae10ec4--------------------------------)
    ·17分钟阅读·2023年12月23日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F978b0ae10ec4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-a-graph-convolutional-network-for-molecular-property-prediction-978b0ae10ec4&user=Gaurav+Deshmukh&userId=5a75283b2c71&source=-----978b0ae10ec4---------------------clap_footer-----------)'
- en: --
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F978b0ae10ec4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-a-graph-convolutional-network-for-molecular-property-prediction-978b0ae10ec4&source=-----978b0ae10ec4---------------------bookmark_footer-----------)![](../Images/44e7eb3195c6626cd9fc39e340695ed7.png)'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F978b0ae10ec4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-a-graph-convolutional-network-for-molecular-property-prediction-978b0ae10ec4&source=-----978b0ae10ec4---------------------bookmark_footer-----------)![](../Images/44e7eb3195c6626cd9fc39e340695ed7.png)'
- en: Photo by [BoliviaInteligente](https://unsplash.com/@boliviainteligente?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 照片由 [BoliviaInteligente](https://unsplash.com/@boliviainteligente?utm_source=medium&utm_medium=referral)
    提供，来源于 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: Artificial intelligence has taken the world by storm. Every week, new models,
    tools, and applications emerge that promise to push the boundaries of human endeavor.
    The availability of open-source tools that enable users to train and employ complex
    machine learning models in a modest number of lines of code have truly democratized
    AI; at the same time, while many of these off-the-shelf models may provide excellent
    predictive capabilities, their usage as black box models may deprive inquisitive
    students of AI of a deeper understanding of how they work and why they were developed
    in the first place. This understanding is particularly important in the natural
    sciences, where knowing that a model is accurate is not enough — it is also essential
    to know its connection to other physical theories, its limitations, and its generalizability
    to other systems. In this article, we will explore the basics of one particular
    ML model — a graph convolutional network — through the lens of chemistry. This
    is not meant to be a mathematically rigorous exploration; instead, we will try
    to compare features of the network with traditional models in the natural sciences
    and think about why it works as well as it does.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 人工智能在全球范围内引起了轰动。每周都会出现新的模型、工具和应用程序，承诺推动人类努力的边界。开放源代码工具的可用性使得用户能够在少量代码中训练和使用复杂的机器学习模型，真正实现了人工智能的民主化；同时，尽管许多这些现成的模型可能提供了出色的预测能力，但它们作为黑箱模型的使用可能会剥夺了对人工智能深入理解的好奇学生。特别是在自然科学中，这种理解尤为重要，因为知道一个模型准确是不够的——还必须了解它与其他物理理论的联系、其局限性以及它对其他系统的普遍适用性。在本文中，我们将通过化学的视角探讨一种特定的机器学习模型——图卷积网络。这并不是一个数学严格的探讨；相反，我们将尝试将网络的特征与传统自然科学模型进行比较，并思考它为何表现如此出色。
- en: 1\. The need for graphs and graph neural networks
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 1\. 对图形和图神经网络的需求
- en: A model in chemistry or physics is usually a continuous function, say *y=f(x₁,
    x₂, x₃, …, xₙ)*, in which *x₁, x₂, x₃, …, xₙ* are the inputs and *y* is the output.
    An example of such a model is the equation that determines the electrostatic interaction
    (or force) between two point charges *q₁* and *q₂* separated by a distance *r*
    present in a medium with relative permittivity *εᵣ*, commonly termed as Coulomb’s
    law.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在化学或物理学中，模型通常是一个连续函数，比如 *y=f(x₁, x₂, x₃, …, xₙ)*，其中 *x₁, x₂, x₃, …, xₙ* 是输入，*y*
    是输出。这样的模型的一个例子是决定两个点电荷 *q₁* 和 *q₂* 之间的静电相互作用（或力）的方程，这两个点电荷在相对介电常数为 *εᵣ* 的介质中，相隔距离为
    *r*，通常称为库仑定律。
- en: '![](../Images/5d08adea00f1d9c05c260a7dd4b77855.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5d08adea00f1d9c05c260a7dd4b77855.png)'
- en: 'Figure 1: The Coulomb equation as a model for electrostatic interactions between
    point charges (Image by author)'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：库仑方程作为点电荷之间静电相互作用的模型（图像来源：作者）
- en: If we did not know this relationship but, hypothetically, had multiple datapoints
    each including the interaction between point charges (the output) and the corresponding
    inputs, we could fit an artificial neural network to predict the interaction for
    any given point charges for any given separation in a medium with a specified
    permittivity. In the case of this problem, admittedly ignoring some important
    caveats, creating a data-driven model for a physical problem is relatively straightforward.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们不知道这种关系，但假设有多个数据点，每个数据点包括点电荷之间的相互作用（输出）和相应的输入，我们可以拟合一个人工神经网络来预测任何给定点电荷在指定介质中的任何给定分离下的相互作用。在这个问题的情况下，虽然忽略了一些重要的警告，但创建一个数据驱动的物理问题模型是相对简单的。
- en: Now consider the problem of prediction of a particular property, say solubility
    in water, from the structure of a molecule. First, there is no obvious set of
    inputs to describe a molecule. You could use various features, such as bond lengths,
    bond angles, number of different types of elements, number of rings, and so forth.
    However, there is no guarantee that any such arbitrary set is bound to work well
    for all molecules.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 现在考虑从分子的结构预测某一特定性质的问题，比如在水中的溶解度。首先，没有明显的输入集来描述一个分子。你可以使用各种特征，如键长、键角、不同类型元素的数量、环的数量等等。然而，没有保证任何这样的任意集合对所有分子都有效。
- en: Second, unlike the example of the point charges, the inputs may not necessarily
    reside in a continuous space. For example, we can think of methanol, ethanol,
    and propanol as a set of molecules with increasing chain lengths; there is no
    notion, however, of anything between them — chain length is a discrete parameter
    and there is no way to interpolate between methanol and ethanol to get other molecules.
    Having a continuous space of inputs is essential to calculate derivatives of the
    model, which can then be used for optimization of the chosen property.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，与点电荷的例子不同，输入可能不一定存在于连续空间中。例如，我们可以将甲醇、乙醇和丙醇视为一组链长逐渐增加的分子；然而，它们之间并不存在任何概念——链长是一个离散参数，没有办法在甲醇和乙醇之间进行插值以得到其他分子。拥有一个连续的输入空间对于计算模型的导数是至关重要的，这些导数随后可以用于优化所选属性。
- en: To overcome these problems, various methods for encoding molecules have been
    proposed. One such method is textual representation using schemes such as SMILES
    and SELFIES. There is a large body of literature on this representation, and I
    direct the interested reader to this [helpful review](https://www.cell.com/patterns/pdf/S2666-3899(22)00206-9.pdf).
    The second method involves representing molecules as graphs. While each method
    has its advantages and shortcomings, graph representations feel more intuitive
    for chemistry.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 为了克服这些问题，已经提出了各种编码分子的方法。其中一种方法是使用SMILES和SELFIES等方案进行文本表示。这种表示方法有大量文献资料，我推荐感兴趣的读者阅读这篇[有用的综述](https://www.cell.com/patterns/pdf/S2666-3899(22)00206-9.pdf)。第二种方法涉及将分子表示为图形。虽然每种方法都有其优点和缺点，但图形表示对化学更直观。
- en: A graph is a mathematical structure consisting of nodes connected by edges that
    represent relationships between nodes. Molecules fit naturally into this structure
    — atoms become nodes, and bonds become edges. Each node in the graph is represented
    by a vector that encodes properties of the corresponding atom. Usually, a one-hot
    encoding scheme suffices (more on this in the next section). These vectors can
    be stacked to create a *node matrix.* Relationships between nodes — denoted by
    edges — can be delineated through a square *adjacency matrix,* wherein every element
    *aᵢⱼ* is either 1 or 0 depending on whether the two nodes *i* and *j* are connected
    by an edge or not respectively. The diagonal elements are set to 1, indicating
    a self-connection, which makes the matrix amenable to convolutions (as you will
    see in the next section). More complex graph representations can be developed,
    in which edge properties are also one-hot encoded in a separate matrix, but we
    shall leave that for another article. These node and adjacency matrices will serve
    as inputs to our model.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 图是由节点通过边连接组成的数学结构，边表示节点之间的关系。分子自然适应这种结构——原子成为节点，键成为边。图中的每个节点由一个向量表示，该向量编码了相应原子的属性。通常，一位编码方案就足够了（更多内容见下一节）。这些向量可以堆叠起来形成一个*节点矩阵*。节点之间的关系——由边表示——可以通过一个方形的*邻接矩阵*来划分，其中每个元素*aᵢⱼ*
    取值为1或0，取决于两个节点*i* 和 *j* 是否由边连接。对角线上的元素设置为1，表示自连接，这使得矩阵适合卷积（如你将在下一节看到的）。可以开发更复杂的图形表示，其中边的属性也在一个单独的矩阵中进行一位编码，但我们将这些留待另一篇文章。这些节点和邻接矩阵将作为我们模型的输入。
- en: '![](../Images/1bb2c8b159493c6a4b83f4cae619305c.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1bb2c8b159493c6a4b83f4cae619305c.png)'
- en: 'Figure 2: Representation of an acetamide molecule as a graph with one-hot encodings
    of atomic numbers of nodes (Image by author)'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：将乙酰胺分子表示为图形，节点的原子序号通过一位编码表示（图片来源：作者）
- en: Typically, artificial neural network models accept a 1-dimensional vector of
    inputs. For multidimensional inputs, such as images, a class of models called
    convolutional neural networks was developed. In our case too we have 2-dimensional
    matrices as inputs, and therefore, need a modified network that can accept these
    as inputs. Graph neural networks were developed to operate on such node and adjacency
    matrices to convert them into appropriate 1-dimensional vectors that can then
    be passed through hidden layers of a vanilla artificial neural network to generate
    outputs. There are many types of graph neural networks, such as graph convolutional
    networks, message passing networks, graph attention networks, and so forth, which
    primarily differ in terms of the functions that exchange information between the
    nodes and edges in the graph. We shall take a closer look at graph convolutional
    networks due to their relative simplicity.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，人工神经网络模型接受的是一维输入向量。对于多维输入，比如图像，开发了一类叫做卷积神经网络的模型。在我们的情况下，我们有二维矩阵作为输入，因此需要一个修改过的网络来接受这些输入。图神经网络是为了处理这样的节点和邻接矩阵而开发的，它们将这些矩阵转换为适当的一维向量，这些向量可以通过普通的人工神经网络的隐藏层来生成输出。图神经网络有许多类型，比如图卷积网络、消息传递网络、图注意力网络等等，它们主要在于节点和边之间交换信息的函数上有所不同。由于图卷积网络相对简单，我们将更详细地了解它们。
- en: 2\. Graph convolution and pooling layers
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 2\. 图卷积和池化层
- en: 'Consider the initial state of your inputs. The node matrix represents the one-hot
    encoding of each atom in each row. For the sake of simplicity, let us consider
    a one-hot encoding of atomic numbers, wherein an atom with atomic number *n* will
    have a 1 at the *nᵗʰ* index and 0s everywhere else. The adjacency matrix represents
    the connections between the nodes. In its current state, the node matrix cannot
    be used as an input to an artificial neural network for the following reasons:
    (1) it is 2-dimensional, (2) it is not permutation-invariant, and (3) it is not
    unique. Permutation-invariance in this case implies that the input should remain
    the same no matter how you order the nodes; currently, the same molecule can be
    represented by multiple permutations of the same node matrix (assuming an appropriate
    permutation in the adjacency matrix as well). This is a problem since the network
    would treat different permutations as different inputs, when they should be treated
    as the same.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑你输入的初始状态。节点矩阵表示了每个原子的独热编码。为了简化起见，我们考虑原子序数的独热编码，其中原子序数为*n*的原子在*nᵗʰ*索引处有一个1，其余位置都是0。邻接矩阵表示节点之间的连接。在当前状态下，节点矩阵不能作为人工神经网络的输入，原因有以下几点：(1)
    它是二维的，(2) 它不是排列不变的，(3) 它不是唯一的。这里的排列不变性意味着无论你如何排列节点，输入应该保持不变；目前，相同的分子可以由相同节点矩阵的多个排列表示（假设邻接矩阵也有适当的排列）。这是一个问题，因为网络会将不同的排列视为不同的输入，而它们应该被视为相同的。
- en: There is an easy solution to the first two issues — pooling. If the node matrix
    is pooled along the column dimension, then it will be reduced to a 1-dimensional
    vector that is permutation-invariant. Typically, this pooling is a simple mean
    pooling, which means that the final pooled vector contains the means of every
    column in the node matrix. However, this still does not solve the third problem
    — pooling the node matrices of two isomers, such as n-pentane and neo-pentane,
    will produce the same pooled vector.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 对于前两个问题，有一个简单的解决方案——池化。如果节点矩阵沿列维度进行池化，那么它将被减少到一个排列不变的一维向量。通常，这种池化是简单的均值池化，这意味着最终池化后的向量包含节点矩阵中每一列的均值。然而，这仍然无法解决第三个问题——池化两个异构体的节点矩阵，例如正戊烷和新戊烷，将产生相同的池化向量。
- en: 'To make the final pooled vectors unique, we need to incorporate some neighbor
    information in the node matrix. In the case of isomers, while their chemical formula
    is the same, their structure is not. A simple way to incorporate neighbor information
    is to perform some operation, such as a sum, for each node with its neighbors.
    This can be represented as the multiplication of the node and adjacency matrices
    (try it out on paper: *the adjacency matrix times the node matrix produces an
    updated node matrix with each node vector equal to the sum of its neighbor node
    vectors with itself*). Often, this sum is normalized by the degree (or number
    of neighbors) of each node by pre-multiplying with the inverse of the diagonal
    degree matrix, making this a mean over neighbors. Lastly, this product is post-multiplied
    by a weight matrix to make this operation parameterizable. This whole operation
    is termed as a graph convolution. An intuitive and simple form of a graph convolution
    is shown in **Figure 3\.** Amore mathematically rigorous and numerically stable
    form was provided in [Thomas Kipf and Max Welling’s work](https://arxiv.org/abs/1609.02907),
    with a modified normalization of the adjacency matrix. The combination of convolution
    and pooling operations can also be interpreted as a *non-linear form of an empirical
    group contribution method.*'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使最终的池化向量具有唯一性，我们需要在节点矩阵中加入一些邻居信息。以同分异构体为例，虽然它们的化学式相同，但它们的结构却不同。加入邻居信息的一个简单方法是对每个节点及其邻居进行某种操作，例如求和。这可以表示为节点矩阵与邻接矩阵的乘法（试着在纸上计算：*邻接矩阵与节点矩阵的乘积生成一个更新后的节点矩阵，其中每个节点向量等于它自身与邻居节点向量的和*）。通常，通过用对角度矩阵的逆进行预乘，对每个节点的度（或邻居数量）进行归一化，从而使这一和值成为邻居的均值。最后，这个乘积会被一个权重矩阵后乘，以使这个操作具有参数化特性。这个完整的操作称为图卷积。**图
    3** 显示了一种直观而简单的图卷积形式。一个数学上更严格且数值上更稳定的形式可以在[Thomas Kipf 和 Max Welling 的研究](https://arxiv.org/abs/1609.02907)中找到，该研究对邻接矩阵进行了修改的归一化。卷积和池化操作的组合也可以解释为一种*非线性的经验群体贡献方法*。
- en: '![](../Images/a4f86992af1c3ad7b47d228fe44ebdb4.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a4f86992af1c3ad7b47d228fe44ebdb4.png)'
- en: 'Figure 3: Graph convolution for an acetamide molecule (Image by author)'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：用于乙酰胺分子的图卷积（作者提供的图片）
- en: The final structure of the graph convolutional network is as follows — first,
    node and adjacency matrices are calculated for a given molecule. Multiple graph
    convolutions are then applied to these followed by pooling to produce a single
    vector containing all the information regarding the molecule. This is then passed
    through the hidden layers of a standard artificial neural network to produce an
    output. The weights of the hidden layers, pooling layer, and convolution layers
    are simultaneously determined through backpropagation applied to a regression-based
    loss function like mean-squared loss.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 图卷积网络的最终结构如下——首先，为给定的分子计算节点和邻接矩阵。然后对这些矩阵应用多次图卷积，并进行池化以生成一个包含所有分子信息的单一向量。随后，这个向量通过标准人工神经网络的隐藏层产生输出。隐藏层、池化层和卷积层的权重通过对基于回归的损失函数（如均方误差）应用反向传播同时确定。
- en: 3\. Implementation in code
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 3\. 代码实现
- en: Having discussed all the key ideas related to graph convolutional networks,
    we are ready to start building one using PyTorch. While there exists a flexible,
    high-performance framework for GNNs called PyTorch Geometric, we shall not make
    use of it, since our goal is to look under the hood and develop our understanding.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在讨论了与图卷积网络相关的所有关键概念之后，我们准备开始使用 PyTorch 构建一个网络。虽然存在一个名为 PyTorch Geometric 的灵活且高性能的
    GNN 框架，但我们不会使用它，因为我们的目标是深入了解其内部机制并发展我们的理解。
- en: The tutorial is split into four major subsections — (1) creating graphs in an
    automated fashion using RDKit, (2) packaging the graphs into a PyTorch Dataset,
    (3) building the graph convolutional network architecture, and (4) training the
    network. The complete code, along with instructions to install and import the
    required packages, is provided in a GitHub repository with a link at the end of
    the article.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 本教程分为四个主要部分——（1）使用 RDKit 自动创建图形，（2）将图形打包成 PyTorch 数据集，（3）构建图卷积网络架构，以及（4）训练网络。完整的代码以及安装和导入所需包的说明可以在文章末尾提供的
    GitHub 仓库中找到链接。
- en: 3.1\. Creating graphs using RDKit
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.1\. 使用 RDKit 创建图形
- en: RDKit is a cheminformatics library that allows high-throughput access to properties
    of small molecules. We will need it for two tasks — getting the atomic number
    of each atom in a molecule to one-hot encode the node matrix and getting the adjacency
    matrix. We assume that molecules are provided in terms of their SMILES strings
    (which is true for most cheminformatics data). Additionally, to ensure that the
    sizes of node and adjacency matrices are uniform across all molecules — which
    they would not be by default, since the sizes of both are dependent on the number
    of atoms in a molecule — we pad the matrices with 0s. Finally, we shall try a
    small modification to the convolution that we have proposed above — we will replace
    the “1”s in the adjacency matrix with the reciprocals of the corresponding bond
    lengths. This way, the network will have more information regarding the geometry
    of the molecule, and it will also weight the convolutions around each node based
    on the bond lengths of the neighbors.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: RDKit是一个化学信息学库，允许高通量访问小分子的性质。我们将需要它来完成两个任务——获取分子中每个原子的原子序数以进行节点矩阵的独热编码，并获取邻接矩阵。我们假设分子是通过其SMILES字符串提供的（这对于大多数化学信息学数据来说是正确的）。此外，为了确保所有分子的节点和邻接矩阵的大小一致——默认情况下它们的大小不一致，因为它们的大小依赖于分子中的原子数——我们用0填充这些矩阵。最后，我们将对上面提出的卷积进行小修改——我们将邻接矩阵中的“1”替换为相应的键长的倒数。这样，网络将获得更多关于分子几何的信息，并且还会根据邻居的键长来加权每个节点周围的卷积。
- en: '[PRE0]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 3.2\. Packaging graphs in a Dataset
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.2\. 在Dataset中打包图
- en: 'PyTorch provides a handy *Dataset* class to store and access various kinds
    of data. We will use that to store the node and adjacency matrices and output
    for each molecule. Note that it is not mandatory to use this *Dataset* interface
    to handle data; nonetheless, using this abstraction makes subsequent steps simpler.
    We need to define two main methods for our class *GraphData* that inherits from
    the *Dataset* class: a *__len__* method to get the size of the dataset and crucially
    a *__getitem__* method to fetch the input and output for a given index.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch提供了一个便捷的*Dataset*类来存储和访问各种数据。我们将使用它来存储每个分子的节点和邻接矩阵及输出。请注意，使用这个*Dataset*接口来处理数据不是强制性的；不过，使用这个抽象会使后续步骤更加简单。我们需要为继承自*Dataset*类的*GraphData*类定义两个主要方法：一个是**__len__**方法来获取数据集的大小，另一个是**__getitem__**方法来获取给定索引的输入和输出。
- en: '[PRE1]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Since we have defined our own customized manner of returning the node and adjacency
    matrices, outputs, and SMILES strings, we need to define a custom function to
    collate the data, that is, package it into a batch, which is then passed on to
    the network. This ability to train neural networks by passing batches of data,
    rather than individual datapoints, and using mini-batch gradient descent provides
    a delicate balance between accuracy and compute efficiency. The collate function
    that we will define below will essentially collect all the data objects, stratify
    them into their categories, stack them in lists, convert them into PyTorch Tensors,
    and recombine these tensors such that they are returned in the same manner as
    that of our *GraphData* class.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们已经定义了自己定制的节点和邻接矩阵、输出以及SMILES字符串的返回方式，我们需要定义一个自定义函数来整理数据，即将数据打包成一个批次，然后传递给网络。通过传递数据批次而不是单个数据点，并使用小批量梯度下降来训练神经网络，可以在准确性和计算效率之间取得微妙的平衡。我们将在下面定义的整理函数本质上会收集所有数据对象，将它们按类别分层，堆叠在列表中，转换为PyTorch张量，并重新组合这些张量，以便以与我们的*GraphData*类相同的方式返回它们。
- en: '[PRE2]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 3.3\. Building the graph convolutional network architecture
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.3\. 构建图卷积网络架构
- en: Having completed the data processing aspects of the code, we now turn towards
    building the model itself. We shall build our own convolution and pooling layers
    for the sake of perspicuity, but more advanced developers among you can easily
    swap these out with more complex, pre-defined layers from the PyTorch Geometric
    module. The *ConvolutionLayer* essentially does three things — (1) calculation
    of the inverse diagonal degree matrix from the adjacency matrix, (2) multiplication
    of the four matrices (D⁻¹ANW), and (3) application of a non-linear activation
    function to the layer output. As with other PyTorch classes, we will inherit from
    the *Module* base class that already has definitions for methods like *forward.*
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 完成数据处理部分的代码后，我们现在转向构建模型本身。为了清晰起见，我们将构建自己的卷积层和池化层，但你们中更高级的开发者可以轻松地用PyTorch Geometric模块中更复杂的预定义层替换这些层。*ConvolutionLayer*本质上做三件事——（1）从邻接矩阵计算逆对角度矩阵，（2）对四个矩阵（D⁻¹ANW）进行乘法运算，以及（3）对层输出应用非线性激活函数。与其他PyTorch类一样，我们将从已经定义了*forward*方法等方法的*Module*基类继承。
- en: '[PRE3]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Next, let us construct the *PoolingLayer*. This layer only performs one operation,
    that is, a mean along the second dimension (number of nodes).
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们构造*PoolingLayer*。该层只执行一个操作，即沿第二维度（节点数量）计算均值。
- en: '[PRE4]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Finally, we shall define create the *ChemGCN* class containing the definitions
    of convolutional, pooling, and hidden layers. Typically, this class should have
    a constructor that defines the structure and ordering of each of these layers,
    and a *forward* method that accepts the input (in our case, the node and adjacency
    matrices) and produces the output. We will apply the *LeakyReLU* activation function
    to all of the layer outputs. Also, we shall use dropout to minimize overfitting.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将定义一个*ChemGCN*类，包含卷积层、池化层和隐藏层的定义。通常，这个类应该有一个构造函数来定义这些层的结构和顺序，以及一个*forward*方法，接受输入（在我们的情况下是节点和邻接矩阵）并生成输出。我们将对所有层的输出应用*LeakyReLU*激活函数。此外，我们还将使用dropout来减少过拟合。
- en: '[PRE5]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 3.4\. Training the network
  id: totrans-48
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 3.4\. 网络训练
- en: We have built the tools required to train our model and make predictions. In
    this section, we shall write helper functions to train and test our model, and
    a write script to run a workflow that makes graphs, builds the network, and trains
    the model.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经构建了训练模型和进行预测所需的工具。在这一部分，我们将编写辅助函数来训练和测试我们的模型，并编写脚本以运行生成图表、构建网络和训练模型的工作流程。
- en: First, we shall define a *Standardizer* class to standardize our outputs. Neural
    networks like to deal with relatively small numbers that do not vary wildly from
    each other. Standardization helps with that.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们定义一个*Standardizer*类来标准化我们的输出。神经网络更喜欢处理相对较小且相互之间变化不大的数字。标准化有助于达到这一点。
- en: '[PRE6]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Second, we define a function to perform the following steps per epoch:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，我们定义一个函数来执行每个epoch的以下步骤：
- en: Unpack inputs and outputs from the data loader and transfer them to the GPU
    (if available).
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从数据加载器中解包输入和输出，并将其传输到GPU（如果可用）。
- en: Pass the inputs through the network and get predictions.
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过网络传递输入并获得预测结果。
- en: Calculate the mean-squared loss between the predictions and outputs.
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算预测值与输出之间的均方误差。
- en: Perform backpropagation and update the weights of the network.
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 执行反向传播并更新网络的权重。
- en: Repeat the above steps for other batches.
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对其他批次重复上述步骤。
- en: The function returns the batch-averaged loss and mean absolute error that can
    be used to plot a loss curve. A similar function without the backpropagation is
    written to test the model.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 该函数返回批量平均损失和均值绝对误差，可用于绘制损失曲线。一个类似的没有反向传播的函数用于测试模型。
- en: '[PRE7]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Finally, let us write the overall workflow. This script will call everything
    we have defined above.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们编写整体工作流程。这个脚本将调用我们之前定义的所有内容。
- en: '[PRE8]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: That’s it! Running this script should output the training and test losses and
    errors.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 就这样！运行这个脚本应该会输出训练和测试的损失和错误。
- en: 4\. Results
  id: totrans-63
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 4\. 结果
- en: A network with the given architecture and hyperparameters was trained on the
    solubility dataset from the open-source [DeepChem repository](https://github.com/deepchem/deepchem)
    containing water solubilities of ~1000 small molecules. The figure below shows
    the training loss curve and parity plot for the test set for one particular train-test
    stratification. The mean absolute errors on the training and test sets are 0.59
    and 0.58 respectively (in log mol/l), lower than the 0.69 log mol/l for a linear
    model (based on predictions present in the dataset). It is no surprise that a
    neural network performs better than a linear regression model; nonetheless, this
    cursory comparison reassures us that the predictions made by our model are reasonable.
    Further, we accomplished this by only incorporating basic structural descriptors
    in the graphs — atomic numbers and bond lengths — and letting the convolutions
    and pooling functions build more complex relationships between these that led
    to the most accurate predictions of the molecular property.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 具有给定架构和超参数的网络在开源的[DeepChem库](https://github.com/deepchem/deepchem)上训练，该库包含约1000种小分子的水溶性。下图显示了一个特定训练-测试划分的训练损失曲线和测试集的对比图。训练集和测试集上的平均绝对误差分别为0.59和0.58（以log
    mol/l为单位），低于线性模型的0.69 log mol/l（基于数据集中的预测）。神经网络表现优于线性回归模型并不令人意外；尽管如此，这种粗略的比较使我们确信模型的预测是合理的。此外，我们仅通过在图中包含基本的结构描述符——原子序数和键长——来实现这一点，让卷积和池化函数建立这些描述符之间更复杂的关系，从而得出最准确的分子性质预测。
- en: '![](../Images/054b6b47d1d5ed507bd28da3eceef4e0.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/054b6b47d1d5ed507bd28da3eceef4e0.png)'
- en: 'Figure 4: Training loss curve (left) and parity plot (right) for the test set
    (Image by author)'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：测试集的训练损失曲线（左）和对比图（右）（图像由作者提供）
- en: 5\. Final remarks
  id: totrans-67
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 5\. 最后的说明
- en: 'This is by no means a definitive model for the chosen problem. There are many
    ways to improve the model including:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 这绝不是解决所选问题的最终模型。改进模型的方式有很多，包括：
- en: optimizing the hyperparameters
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 优化超参数
- en: using an early-stopping strategy to find a model with the lowest validation
    loss
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用早停策略找到具有最低验证损失的模型
- en: using more complex convolution and pooling functions
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用更复杂的卷积和池化函数
- en: collecting more data
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 收集更多数据
- en: Nevertheless, the goal of this tutorial was to expound on the fundamentals of
    graph convolutional networks for chemistry through a simple example. Having acquainted
    yourself with the basics, the sky is the limit in your GCN model-building journey.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管如此，本教程的目标是通过一个简单的例子阐述化学领域图卷积网络的基础知识。在掌握了基础知识后，你在GCN模型构建之旅中的可能性是无限的。
- en: Repository and helpful references
  id: totrans-74
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 仓库和有用的参考资料
- en: The complete code (including scripts to create figures) is provided in a [GitHub
    repository](https://github.com/gauravsdeshmukh/ChemGCN). Instructions to install
    requisite modules are also provided there. The dataset used to train the model
    is from the open-source [DeepChem repository](https://github.com/deepchem/deepchem)
    that is under the MIT license (commercial use permitted). The raw dataset filename
    in the repository is delaney_processed.csv.
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 完整的代码（包括创建图形的脚本）提供在[GitHub仓库](https://github.com/gauravsdeshmukh/ChemGCN)中。安装所需模块的说明也提供在那里。用于训练模型的数据集来自开源的[DeepChem库](https://github.com/deepchem/deepchem)，该库在MIT许可下（允许商业使用）。仓库中的原始数据集文件名为delaney_processed.csv。
- en: '[Review](https://www.cell.com/patterns/pdf/S2666-3899(22)00206-9.pdf) on string
    representations of molecules.'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[关于分子字符串表示的综述](https://www.cell.com/patterns/pdf/S2666-3899(22)00206-9.pdf)'
- en: '[Research article](https://arxiv.org/abs/1609.02907) on graph convolutional
    networks. The convolution function presented in this article is a simpler and
    more intuitive form of the function given in this article.'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[关于图卷积网络的研究文章](https://arxiv.org/abs/1609.02907)。本文介绍的卷积函数是本文中给出的函数的简化和更直观的形式。'
- en: '[Research article](https://arxiv.org/abs/1704.01212) on message passing neural
    networks. These are more generalized and expressive graph neural networks. It
    can be shown that a graph convolutional network is a message passing neural network
    with a specific type of message function.'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[关于消息传递神经网络的研究文章](https://arxiv.org/abs/1704.01212)。这些是更为通用和富有表现力的图神经网络。可以证明，图卷积网络是具有特定类型消息函数的消息传递神经网络。'
- en: '[Online book](https://dmol.pub/) on deep learning for molecules. This is a
    great resource to learn the basics of deep learning for chemistry and apply your
    learnings through hands-on coding exercises.'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[关于分子深度学习的在线书籍](https://dmol.pub/)。这是一个极好的资源，可以帮助你学习化学深度学习的基础知识，并通过动手编码练习应用所学。'
- en: If you have any questions, comments, or suggestions, please feel free to [email
    me](mailto:gauravsdeshmukh@outlook.com) or [contact me on X](https://twitter.com/ChemAndCode).
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你有任何问题、评论或建议，请随时通过[电子邮件联系我](mailto:gauravsdeshmukh@outlook.com)或通过[X联系我](https://twitter.com/ChemAndCode)。
