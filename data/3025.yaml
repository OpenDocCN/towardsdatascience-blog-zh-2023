- en: 'Simplifying Transformers: State of the Art NLP Using Words You Understand —
    Part 4 — Feed-Forward- Layer'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 简化 Transformers：用你理解的词汇解析最前沿的 NLP —— 第4部分 —— 前馈层
- en: 原文：[https://towardsdatascience.com/simplifying-transformers-state-of-the-art-nlp-using-words-you-understand-part-4-feed-foward-264bfee06d9?source=collection_archive---------4-----------------------#2023-10-04](https://towardsdatascience.com/simplifying-transformers-state-of-the-art-nlp-using-words-you-understand-part-4-feed-foward-264bfee06d9?source=collection_archive---------4-----------------------#2023-10-04)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/simplifying-transformers-state-of-the-art-nlp-using-words-you-understand-part-4-feed-foward-264bfee06d9?source=collection_archive---------4-----------------------#2023-10-04](https://towardsdatascience.com/simplifying-transformers-state-of-the-art-nlp-using-words-you-understand-part-4-feed-foward-264bfee06d9?source=collection_archive---------4-----------------------#2023-10-04)
- en: Plain old feed-forward layers and their role in Transformers.
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 传统的前馈层及其在 Transformers 中的作用。
- en: '[](https://medium.com/@chenmargalit?source=post_page-----264bfee06d9--------------------------------)[![Chen
    Margalit](../Images/fb37720654b3d1068b448d4d9ad624d5.png)](https://medium.com/@chenmargalit?source=post_page-----264bfee06d9--------------------------------)[](https://towardsdatascience.com/?source=post_page-----264bfee06d9--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----264bfee06d9--------------------------------)
    [Chen Margalit](https://medium.com/@chenmargalit?source=post_page-----264bfee06d9--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@chenmargalit?source=post_page-----264bfee06d9--------------------------------)[![Chen
    Margalit](../Images/fb37720654b3d1068b448d4d9ad624d5.png)](https://medium.com/@chenmargalit?source=post_page-----264bfee06d9--------------------------------)[](https://towardsdatascience.com/?source=post_page-----264bfee06d9--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----264bfee06d9--------------------------------)
    [Chen Margalit](https://medium.com/@chenmargalit?source=post_page-----264bfee06d9--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ff8e6113b0479&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsimplifying-transformers-state-of-the-art-nlp-using-words-you-understand-part-4-feed-foward-264bfee06d9&user=Chen+Margalit&userId=f8e6113b0479&source=post_page-f8e6113b0479----264bfee06d9---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----264bfee06d9--------------------------------)
    ·9 min read·Oct 4, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F264bfee06d9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsimplifying-transformers-state-of-the-art-nlp-using-words-you-understand-part-4-feed-foward-264bfee06d9&user=Chen+Margalit&userId=f8e6113b0479&source=-----264bfee06d9---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ff8e6113b0479&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsimplifying-transformers-state-of-the-art-nlp-using-words-you-understand-part-4-feed-foward-264bfee06d9&user=Chen+Margalit&userId=f8e6113b0479&source=post_page-f8e6113b0479----264bfee06d9---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----264bfee06d9--------------------------------)
    ·9分钟阅读·2023年10月4日'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F264bfee06d9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsimplifying-transformers-state-of-the-art-nlp-using-words-you-understand-part-4-feed-foward-264bfee06d9&source=-----264bfee06d9---------------------bookmark_footer-----------)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F264bfee06d9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsimplifying-transformers-state-of-the-art-nlp-using-words-you-understand-part-4-feed-foward-264bfee06d9&source=-----264bfee06d9---------------------bookmark_footer-----------)'
- en: 'As this is an ongoing series, if you haven’t done so yet, you might want to
    consider starting at one of the previous sections: [1st](/transformers-part-1-2a2755a2af0e),
    [2nd,](/transformers-part-2-input-2a8c3a141c7d) and [3rd](/transformers-part-3-attention-7b95881714df).'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这是一个持续更新的系列，如果你还没有开始，可能需要考虑从之前的部分开始：[第1部分](/transformers-part-1-2a2755a2af0e)、[第2部分](/transformers-part-2-input-2a8c3a141c7d)
    和 [第3部分](/transformers-part-3-attention-7b95881714df)。
- en: This fourth section will cover the essential Feed-Forward layer, a fundamental
    element found in most deep-learning architectures. While discussing vital topics
    common to deep learning, we’ll emphasize their significant role in shaping the
    Transformers architecture.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 本节将介绍基本的前馈层，这是大多数深度学习架构中的一个基本元素。在讨论深度学习中的重要主题时，我们将强调它们在塑造 Transformers 架构中的重要作用。
- en: '![](../Images/fc71898249eca35fab01290fd6179343.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fc71898249eca35fab01290fd6179343.png)'
- en: '[Images from the original paper](https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf)'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '[原始论文中的图片](https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf)'
- en: A feed-forward linear layer is basically a bunch of neurons, each of which is
    connected to a bunch of other neurons. Look at the image below. A, b, c, and d
    are neurons. They hold some input, some numbers representing some data we want
    to understand (pixels, word embeddings, etc.). They are connected to neuron number
    1\. Each neuron has a different connection strength. a-1 is 0.12, b-1 is -0.3,
    etc. In reality, all the neurons in the left column are connected to all the neurons
    in the right column. It makes the image unclear so I didn't make it this way,
    but it's important to note. Exactly in the same way we have a-1, we also have
    a-2, b-2, c-2, d-3, etc. Each of the connections between two neurons has a different
    “connection strength”.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 前馈线性层基本上是一堆神经元，每个神经元都与其他一堆神经元相连。请看下面的图片。A、b、c 和 d 是神经元。它们持有一些输入，即表示我们想要理解的数据的数字（像素、词嵌入等）。它们与神经元
    1 相连。每个神经元有不同的连接强度。a-1 是 0.12，b-1 是 -0.3，等等。实际上，左列中的所有神经元都与右列中的所有神经元相连。这样做会使图像变得不清晰，因此我没有这样做，但这点很重要。正如
    a-1 存在一样，我们也有 a-2、b-2、c-2、d-3 等。每两个神经元之间的连接都有不同的“连接强度”。
- en: '![](../Images/2bf9a5659c9f33bb35917fa86fa3964e.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2bf9a5659c9f33bb35917fa86fa3964e.png)'
- en: Image by Author
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片
- en: 'There are two important things to note in that architecture:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个架构中，有两点需要注意：
- en: 1\. As mentioned, Every node (neuron), is connected to every other node. All
    of the four a,b,c,d are connected to every other neuron (1,2,3). Think of this
    image as a chain of command. 1,2,3 are commanders. They get reports from soldiers
    a,b,c,d. A knows something, about something, but it doesn’t have a very wide view.
    1 knows more, as it gets reports from both a, b c, and d. The same goes for 2
    and 3 which are also commanders. These commanders (1,2,3) are also passing reports
    to higher-up commanders. Those commanders after them, get reports both from a,b,c,d,
    **and** from 1,2,3, as the next layer (each column of neurons is a layer) is also
    fully connected in exactly the same way. So the first important thing to understand
    is that 1 has a broader view than a and the commander in the next layer will have
    a broader view than 1\. When you have more dots, you can make more interesting
    connections.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 1. 如前所述，每个节点（神经元）都与每个其他节点相连。四个神经元 a、b、c 和 d 都与每个其他神经元（1、2、3）相连。将这张图片视为一个指挥链。1、2、3
    是指挥官。他们从士兵 a、b、c 和 d 那里收到报告。A 知道一些事情，但视野不广。1 知道得更多，因为它从 a、b、c 和 d 那里收到报告。2 和 3
    也是指挥官，它们的情况也是如此。这些指挥官（1、2、3）也将报告传递给更高层的指挥官。那些更高层的指挥官会同时收到来自 a、b、c、d 和 1、2、3 的报告，因为下一层（每一列神经元是一个层）也是以完全相同的方式连接的。因此，第一个重要的理解点是
    1 的视野比 a 广，而下一层的指挥官将比 1 的视野更广。当你有更多的点时，你可以建立更多有趣的连接。
- en: '2\. Each node has a different connection strength to every other Node in the
    next layer. a-1 is 0.12, b-1 is -0.3\. The numbers I put here are obviously made
    up, but they are of reasonable scale, and they are learned parameters (e.g. they
    change during training). Think of these numbers as of how much 1 counts on a,
    on b, etc. From the point of view of commander 1, a is a little bit trustable.
    You shouldn’t take everything he says for granted, but you can count on some of
    his words. B is very different. This node usually diminishes the importance of
    the input it gets. Like a laid-back person. Is this a tiger? Nah, just a big cat.
    This is an oversimplification of what happens but the important thing to note
    is this: Each neuron holds some input, whether it is raw input or a processed
    input, and passes it on with its own processing.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 2\. 每个节点与下一层的每个其他节点有不同的连接强度。a-1是0.12，b-1是-0.3。我这里给出的数字显然是虚构的，但它们的规模是合理的，并且它们是学习到的参数（例如，在训练过程中会发生变化）。把这些数字看作是1在a、b等上的权重。从指挥官1的角度来看，a稍微可信一点。你不应该完全相信他说的每句话，但你可以相信他的话中的一部分。B则非常不同。这个节点通常会减轻它收到输入的重视程度。就像一个随和的人。这是一个对发生事情的过度简化，但重要的是要注意：每个神经元持有一些输入，无论是原始输入还是处理过的输入，并以自己的处理方式传递它。
- en: 'Do you know the game Chinese Whispers? you sit in a row with 10 people and
    you whisper to the next person a word, say, “Pizza”. Person 2 has heard something
    like “Pazza” so they pass on “Pazza” to person 3\. Person 3 heard “Lassa” (it''s
    a whisper after all) so he passes on “Lassa”. The 4th person has heard “Batata”
    so he passes on Batata, and so on. When you ask the 10th person what did you head?
    he says: Shambala! How did we get from Pizza to Shambala? shit happens. The difference
    between that game and what a neural network does is that each such person will
    add its useful processing. Person 2 won’t say “Pazza”, he will say, “Pazza is
    an Italian dish, it''s great”. Person 3 will say “Lassa is an Italian dish, common
    all over the word”, etc. Each person (layer), adds something hopefully useful.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 你知道“传话游戏”吗？你和10个人坐成一排，你对下一个人耳语一个词，比如“Pizza”。第2个人听到的类似“Pazza”，于是他们把“Pazza”传给第3个人。第3个人听到的是“Lassa”（毕竟是耳语），于是他传递“Lassa”。第4个人听到的是“Batata”，所以他传递“Batata”，依此类推。当你问第10个人你听到的是什么，他会说：“Shambala！”我们怎么从Pizza变成Shambala的？事有凑巧。这种游戏和神经网络的区别在于，每个人都会添加自己的有用处理。第2个人不会说“Pazza”，他会说：“Pazza是意大利菜，非常棒”。第3个人会说：“Lassa是意大利菜，在世界各地都很常见”，等等。每个人（层）都添加了一些希望有用的东西。
- en: 'This is basically what happens. Each neuron gets an input, processes it, and
    moves it on. To match the fully connected layer, I suggest an upgrade to the Chinese
    Whisperers: from now on you play with multiple rows and each person whispers to
    any other person in every other line. The people in position 2 onwards get whispers
    from many people and need to understand how much “weight” (importance) they give
    to each person, This is a Feed Forward Layer.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 基本上就是这样发生的。每个神经元接收一个输入，处理它，然后传递出去。为了匹配完全连接层，我建议对“传话游戏”进行升级：从现在起，你与多个排进行游戏，每个人对其他行中的任何其他人耳语。第2位置及其后的人从许多人那里收到耳语，并需要理解他们给每个人多少“权重”（重要性）。这就是前馈层。
- en: Why do we use such layers? because they allow us to do add useful calculations.
    Think of it a bit like of the wisdom of the crowd. Do you know the story of guessing
    a steer’s weight? In 1906, someplace in England, someone brings a steer to an
    exhibition. The presenter asks 787 random people to guess its weight. What would
    you say? How much does the steer weigh?
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 我们为什么使用这些层？因为它们允许我们进行有用的计算。把它想象成群体智慧的一个例子。你知道猜测牛的体重的故事吗？1906年，在英国某地，有人把一头牛带到展览上。主持人让787个随机人猜它的体重。你会怎么说？这头牛重多少？
- en: The average of all their guesses was 1197 pounds (542 KG). These are guesses
    of random people. How far were they? 1 pound, 450 grams. The Steer’s weight is
    1198\. The story is taken from [here](https://www.wondriumdaily.com/the-wisdom-of-crowds/#:~:text=An%20Astonishing%20Example%20of%20the%20Wisdom%20of%20Crowds&text=The%20actual%20weight%20of%20the,that%20weight%20was%201%2C197%20pounds.)
    and I don't know if the details are right or not, but back to our business, you
    can think of linear layers as doing something like that. You add more parameters,
    more calculations (more guesses), and you get a better result.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 所有猜测的平均值是 1197 磅（542 千克）。这些是随机人的猜测。它们的误差有多大？1 磅，450 克。Steer 的重量是 1198。这个故事来源于
    [这里](https://www.wondriumdaily.com/the-wisdom-of-crowds/#:~:text=An%20Astonishing%20Example%20of%20the%20Wisdom%20of%20Crowds&text=The%20actual%20weight%20of%20the,that%20weight%20was%201%2C197%20pounds.)，我不知道细节是否正确，但回到我们的主题，你可以把线性层想象成类似的工作方式。你增加更多的参数，更多的计算（更多的猜测），你就能得到更好的结果。
- en: '![](../Images/3f260e9edd4b52bded85e69f5b62e601.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3f260e9edd4b52bded85e69f5b62e601.png)'
- en: Photo by [Larry Costales](https://unsplash.com/@larry3?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
    on [Unsplash](https://unsplash.com/photos/Ahf1ZmcKzgE?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由 [Larry Costales](https://unsplash.com/@larry3?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
    提供，来源于 [Unsplash](https://unsplash.com/photos/Ahf1ZmcKzgE?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)。
- en: 'Let''s try to imagine a real scenario. We give the network an image and we
    want to decide whether it''s an apple or an orange. The architecture is based
    on CNN layers, which I won''t get into as they are beyond the scope of this series,
    but basically, it''s a computation layer that is able to recognize specific patterns
    in an image. Each layer is able to recognize growing complications of patterns.
    For example, the first layer can’t notice almost anything, it just passes the
    raw pixels, the second layer recognizes vertical lines, the next layer has heard
    there are vertical lines, and from other neurons, it has heard there are vertical
    lines very near. It does 1+1 and thinks: Nice! It''s a corner. That is the benefit
    of getting inputs from multiple sources.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们试着想象一个真实场景。我们给网络一个图像，并且我们想决定它是一个苹果还是一个橘子。这个架构基于 CNN 层，我不会深入讨论，因为它超出了本系列的范围，但基本上，它是一个能够识别图像中特定模式的计算层。每一层都能识别逐渐复杂的模式。例如，第一层几乎不能察觉任何东西，它只是传递原始像素，第二层识别垂直线，下一层得知有垂直线，并且从其他神经元那里知道垂直线非常接近。它进行
    1+1 运算并认为：不错！这是一个角落。这就是从多个来源获取输入的好处。
- en: The more calculations we do, we might imagine, the better results we can get.
    In reality, it doesn’t really work this way, but it does have **some** truth in
    it. If I do more calculations and consult with more people (neurons), I can generally
    reach better results.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 我们做的计算越多，我们可以想象得到的结果就会更好。实际上，它并不完全是这样，但确实有 **一些** 真理。如果我进行更多的计算并咨询更多的人（神经元），我通常可以达到更好的结果。
- en: '**Activation Function** We will stack another vital building block of a basic
    and very important concept in deep learning as a whole, and then we’ll connect
    the dots to understand how this is related to Transformers.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '**激活函数** 我们将堆叠另一个深度学习中基本且非常重要的概念的关键构建块，然后我们将连接这些点以理解它与 Transformers 的关系。'
- en: Fully connected layers, great as they are, suffer from one big disadvantage.
    They are linear layers, they only do linear transformations, linear calculations.
    They add and multiply, but they can’t transform the input in “creative” ways.
    Sometimes adding more power doesn’t cut it, you need to think of the problem completely
    differently.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 全连接层虽然很棒，但有一个大缺点。它们是线性层，只进行线性变换和线性计算。它们只能进行加法和乘法，但不能以“创造性”的方式转换输入。有时候，增加更多的能力并不足够，你需要从完全不同的角度思考问题。
- en: If I make $10, I work 10 hours a day, and I want to save $10k faster, I can
    either work more days every week or work additional hours every day. But there
    are other solutions out there, aren’t there? So many banks to be robbed, other
    people not needing their money (I can spend it better), getting better-paid jobs,
    etc. The solution is not always more of the same.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我赚取 $10，每天工作 10 小时，并且我想更快地存到 $10k，我可以每周工作更多天数或每天工作额外的小时。但还有其他解决方案，不是吗？那么多的银行可以抢劫，其他人不需要他们的钱（我可以更好地使用），找到更高薪的工作等等。解决方案不总是更多的同样方法。
- en: Activation functions to the rescue. An activation function allows us to make
    a non-linear transformation. For example taking a list of numbers [1, 4, -3, 5.6]
    and transforming them into probabilities. This is exactly what the Softmax activation
    function does. It takes these numbers and transforms them to [8.29268754e-03,
    1.66563082e-01, 1.51885870e-04, 8.24992345e-01]. These 5 numbers sum to 1\. It
    is written a bit hectic but each e-03 means the first number (8) starts after
    3 zeros (e.g. 0.00 and then 82926\. The actual number is 0.00829268754). This
    Softmax activation function has taken integers and turned them into floats between
    0 and 1 in a way that preserves the gaps between them. You might imagine how extremely
    useful this is when wanting to use statistical methods on such values.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 激活函数来救场。激活函数允许我们进行非线性变换。例如，将一组数字 [1, 4, -3, 5.6] 转换成概率。这正是 Softmax 激活函数所做的。它将这些数字转换为
    [8.29268754e-03, 1.66563082e-01, 1.51885870e-04, 8.24992345e-01]。这五个数字的和为 1。虽然写法有些繁琐，但每个
    e-03 表示第一个数字（8）后面跟着 3 个零（例如 0.00 然后是 82926。实际数字是 0.00829268754）。这个 Softmax 激活函数将整数转变为
    0 到 1 之间的浮点数，同时保持它们之间的间距。当需要对这些值使用统计方法时，你可以想象这有多么有用。
- en: There are other types of activation functions, one of the most used ones is
    ReLU (Rectifies Linear Unit). It's an extremely simple (yet extremely useful)
    activation function that takes any negative number and turns it to 0, and any
    non-negative number and leaves it as it is. Very simple, very useful. If I give
    the list [1, -3, 2] to ReLU, I get [1, 0, 2] back.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 还有其他类型的激活函数，其中最常用的一种是 ReLU（修正线性单元）。它是一个极其简单（但极其有用）的激活函数，将任何负数变为 0，将任何非负数保持不变。非常简单，非常有用。如果我将列表
    [1, -3, 2] 传递给 ReLU，我得到 [1, 0, 2]。
- en: After scaring you with Softmax, you might have expected something more complicated,
    but as someone once told me “Luck is useful”. With this activation function, we
    got lucky.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在用 Softmax 吓到你之后，你可能会期待更复杂的东西，但正如有人曾告诉我的，“运气是有用的”。有了这个激活函数，我们很幸运。
- en: The reason we need these activation functions is that a nonlinear relationship
    cannot be represented by linear calculations (fully connected layers). If for
    every hour I work I get $10, the amount of money I get is linear. If for every
    5 hours of working straight, I get a 10% increase for the next 5 hours, the relationship
    is no longer linear. My salary won't be the number of hours I work * fixed hourly
    salary. The reason we carry the burden of deep learning for more complicated tasks
    such as text generation of computer recognition, is because the relationships
    we are looking for are highly unlinear. The word that comes after “I love” is
    not obvious and it's not constant.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要这些激活函数的原因是非线性关系无法通过线性计算（全连接层）来表示。如果每小时我赚 $10，那么我得到的金额是线性的。如果每工作 5 小时，我在接下来的
    5 小时中获得 10% 的增加，关系就不再是线性的了。我的工资不会是我工作的小时数 * 固定小时工资。我们之所以在更复杂的任务中，如计算机识别和文本生成中承担深度学习的负担，是因为我们寻找的关系是高度非线性的。“我爱”之后出现的词并不明显，也不是恒定的。
- en: A great benefit of ReLU, perhaps what made it so commonly used, is that it's
    very computationally cheap to calculate it on many numbers. When you have a small
    number of neurons (let's say tens of thousands), computation isn't super crucial.
    When you use hundreds of billions, like big LLMs do, a more computationally efficient
    way of crunching numbers might make the difference.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: ReLU 的一个重要好处，也可能是它如此常用的原因，是它在计算大量数字时非常便宜。当你有少量神经元（假设是几万个）时，计算并不是特别关键。但当你使用数百亿个神经元，如大型语言模型所做的那样，计算上的效率差异可能会产生显著影响。
- en: '**Regularization** The last concept we’ll introduce before explaining the (very
    simple) way it’s implemented in Transformers, is dropout. Dropout is a regularization
    technique. Regu- what? regularization. As algorithms are based on data and their
    task is to be as close to the training target as they can, it can be sometimes
    useful for someone with a big brain to just memorize stuff. As we are taught so
    skillfully in school, it isn’t always useful to learn complicated logic, we can
    sometimes just remember what we’ve seen, or remember something close to it. When
    was war world 2? well... it was affected by World War 1, economic crises, angry
    people, etc.... which was around 1917 .. so let''s say 1928\. Perhaps it’s just
    better to memorize the actual date.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '**正则化** 在解释它在 Transformers 中（非常简单）实现的方式之前，我们将介绍最后一个概念，即 dropout。Dropout 是一种正则化技术。正则化？正如我们所教的那样，学习复杂的逻辑并不总是有用，有时我们可以记住我们所见到的东西，或记住接近它的东西。第二次世界大战是什么时候？嗯……它受到第一次世界大战、经济危机、愤怒的人们等的影响……大约在1917年，所以我们说1928年。也许记住实际日期会更好。'
- en: As you might imagine, this isn’t good for Machine Learning. If we needed answers
    to questions we already had answers for, we wouldn’t need this crazy complicated
    field. We need a smart algorithm because we can’t memorize everything. We need
    it to make considerations on live inferences, we need it to kind of think. A general
    term for techniques used to make the algorithm learn, but not memorize, is called
    Regularization. Out of these regularization techniques, one commonly used is dropout.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你想象的，这对机器学习来说不好。如果我们需要回答我们已经知道的答案，我们就不需要这个复杂的领域了。我们需要一个聪明的算法，因为我们不能记住所有的东西。我们需要它对实时推断进行考虑，我们需要它有点像思考。用于让算法学习而不是记忆的技术的总称是正则化。在这些正则化技术中，一个常用的是
    dropout。
- en: '**Dropout** What’s dropout? a rather simple (lucky us again) technique. Remember
    we said fully connected layers are, fully connected? well, dropout shakes that
    logic. The dropout technique means turning the “connection strength” to 0, which
    means it won’t have any effect. Solder “a” becomes completely useless for commander
    1 as its input is turned to 0\. No answer, not positive, not negative. On every
    layer we add dropout, we randomly choose a number of neurons (configured by the
    developer) and turn their connection to other neurons to 0\. Each time the commander
    is forced to ignore different soldiers, hence not being able to memorize any of
    them as perhaps it won''t them next time.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '**Dropout** 什么是 dropout？一种相当简单的（幸运的是）技术。记住我们说全连接层是完全连接的？好吧，dropout 打破了这个逻辑。Dropout
    技术意味着将“连接强度”设置为 0，这意味着它不会产生任何效果。焊接的“a”对指挥官 1 完全无用，因为其输入被设置为 0。没有答案，没有正面，没有负面。在每一层中我们添加
    dropout 时，我们随机选择一定数量的神经元（由开发者配置），并将它们与其他神经元的连接设置为 0。每次指挥官都被迫忽略不同的士兵，因此无法记住他们，因为下次可能不会遇到他们。'
- en: '**Back to Transformers!** We now have all the building blocks needed to understand
    what happens specifically in the Feed Forward layer. It will now be very simple.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '**回到 Transformers！** 我们现在拥有了理解 Feed Forward 层中具体发生什么的所有构建块。这将变得非常简单。'
- en: '![](../Images/fc71898249eca35fab01290fd6179343.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fc71898249eca35fab01290fd6179343.png)'
- en: '[Image from the original paper](https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf)'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '[原始论文中的图像](https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf)'
- en: 'This layer simply does 3 things:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 这个层简单地做三件事：
- en: 1\. Position-wise linear calculation — every position in the text (represented
    as vectors) is passed through a linear layer.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 1\. 基于位置的线性计算——文本中的每个位置（以向量表示）都通过一个线性层。
- en: 2\. A ReLU calculation is done on the output of the linear calculation.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 2\. 在线性计算的输出上进行 ReLU 计算。
- en: 3\. Another Linear calculation is done on the output of the ReLU.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 3\. 在 ReLU 输出上进行另一个线性计算。
- en: 4\. Finally, we add to the output of layer 3.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 4\. 最后，我们将添加到第 3 层的输出中。
- en: Boom. That is all there is to it. If you’re experienced with deep learning,
    this section was probably very easy for you. If you aren’t, you may have struggled
    a bit, but you came to understand an extremely important moving piece of deep
    learning.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 就是这样。如果你对深度学习有经验，这部分可能对你来说非常简单。如果没有，你可能会遇到一些困难，但你已经理解了深度学习中的一个极其重要的移动部分。
- en: In the next part, we’ll be speaking about the Decoder! [You can find it here.](/simplifying-transformers-state-of-the-art-nlp-using-words-you-understand-part-5-decoder-and-cd2810c6ad40)
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的部分，我们将讨论解码器！[你可以在这里找到它。](/simplifying-transformers-state-of-the-art-nlp-using-words-you-understand-part-5-decoder-and-cd2810c6ad40)
