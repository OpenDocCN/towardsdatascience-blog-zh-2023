- en: An Introduction to Deep Learning for Sequential Data
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度学习在序列数据中的应用介绍
- en: 原文：[https://towardsdatascience.com/an-introduction-to-deep-learning-for-sequential-data-ac966b9b9b67](https://towardsdatascience.com/an-introduction-to-deep-learning-for-sequential-data-ac966b9b9b67)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/an-introduction-to-deep-learning-for-sequential-data-ac966b9b9b67](https://towardsdatascience.com/an-introduction-to-deep-learning-for-sequential-data-ac966b9b9b67)
- en: Highlighting the similarities between time series and NLP
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 突出时间序列与自然语言处理的相似性
- en: '[](https://donatoriccio.medium.com/?source=post_page-----ac966b9b9b67--------------------------------)[![Donato
    Riccio](../Images/0af2a026e72a023db4635522cbca50eb.png)](https://donatoriccio.medium.com/?source=post_page-----ac966b9b9b67--------------------------------)[](https://towardsdatascience.com/?source=post_page-----ac966b9b9b67--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----ac966b9b9b67--------------------------------)
    [Donato Riccio](https://donatoriccio.medium.com/?source=post_page-----ac966b9b9b67--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://donatoriccio.medium.com/?source=post_page-----ac966b9b9b67--------------------------------)[![Donato
    Riccio](../Images/0af2a026e72a023db4635522cbca50eb.png)](https://donatoriccio.medium.com/?source=post_page-----ac966b9b9b67--------------------------------)[](https://towardsdatascience.com/?source=post_page-----ac966b9b9b67--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----ac966b9b9b67--------------------------------)
    [Donato Riccio](https://donatoriccio.medium.com/?source=post_page-----ac966b9b9b67--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----ac966b9b9b67--------------------------------)
    ·8 min read·Nov 14, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----ac966b9b9b67--------------------------------)
    ·阅读时间8分钟·2023年11月14日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/9bd71d38282448e4edea2f653b4701a1.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9bd71d38282448e4edea2f653b4701a1.png)'
- en: How I imagine myself when I load a time series dataset. Image by the author.
    (AI assisted)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 当我加载时间序列数据集时的想象。图片由作者提供。（AI 辅助）
- en: Sequential data like time series and natural language require models that can
    capture ordering and context. While time series analysis focuses on forecasting
    based on temporal patterns, natural language processing aims to extract semantic
    meaning from word sequences.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 时间序列和自然语言等序列数据需要能够捕捉顺序和上下文的模型。时间序列分析侧重于基于时间模式进行预测，而自然语言处理旨在从单词序列中提取语义信息。
- en: Though distinct tasks, both data types have long-range dependencies where distant
    elements influence predictions. As deep learning has advanced, model architectures
    initially developed for one domain have been adapted to the other.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管任务各异，但这两种数据类型都有长程依赖关系，即远距离的元素会影响预测。随着深度学习的发展，最初为一个领域开发的模型架构已被调整以适用于另一个领域。
- en: Sequential data
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 序列数据
- en: Time series and natural language have both a sequential structure, where the
    position of an observation in the sequence matters greatly.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 时间序列和自然语言都具有顺序结构，其中观察值在序列中的位置至关重要。
- en: '![](../Images/9a269b83f4cb166a2c118aed8691e4ea.png)![](../Images/9f6877653538823f1d05aaebe893604c.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9a269b83f4cb166a2c118aed8691e4ea.png)![](../Images/9f6877653538823f1d05aaebe893604c.png)'
- en: A time series is a sequence of values. (left) Text is a sequence of words. (right)
    Image by author.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 时间序列是一系列数值。（左）文本是一系列单词。（右）图片由作者提供。
- en: 'A time series is a set of observations over time that are ordered chronologically
    and sampled at fixed time intervals. Some examples include:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 时间序列是一组按时间顺序排列的观察值，并以固定的时间间隔进行采样。一些例子包括：
- en: Stock prices every day
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每日股票价格
- en: Server metrics every hour
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每小时服务器指标
- en: Temperature readings every second
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每秒的温度读数
- en: The key attribute of time series data is that the ordering of observations is
    meaningful. Values nearby in time are usually highly dependent — knowing recent
    values gives insight into predicting the next value. Time series analysis aims
    to model these temporal dependencies to understand patterns and make forecasts.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 时间序列数据的关键属性是观察值的顺序是有意义的。时间上相邻的值通常高度相关——了解近期值有助于预测下一个值。时间序列分析旨在对这些时间依赖性进行建模，以理解模式并进行预测。
- en: 'Text data is also sequential — the order of words conveys meaning and context.
    For example:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 文本数据也是序列化的——单词的顺序传达了意义和上下文。例如：
- en: John threw the ball
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 约翰扔了球
- en: The ball threw John
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 球扔了约翰
- en: While both sentences contain the same words, their meaning changes entirely
    based on word order. These temporal relationships are represented in language
    models and are the key to natural language tasks like translation and summarization.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管两个句子包含相同的单词，但其含义完全取决于词序。这些时间关系在语言模型中表示，是自然语言任务如翻译和总结的关键。
- en: Both time series and text exhibit **long-range dependencies —** values far apart
    in the sequence still influence each other. Also, local patterns repeat across
    different locations.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 时间序列和文本都表现出**长距离依赖**——序列中相隔较远的值仍会相互影响。此外，本地模式在不同位置之间重复。
- en: Time series and text representation in neural network
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 神经网络中的时间序列和文本表示
- en: Text data need to be converted to embeddings to make them readable from a machine.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 文本数据需要转换为嵌入，以使其对机器可读。
- en: Vector representations called embeddings are learned from large datasets to
    capture semantic meaning and relationships between words or data points. The embedding
    vectors encode different semantic properties in each element, representing words/data
    in a dense, low-dimensional way for machine learning models. Embeddings can be
    pre-trained on large corpora and then fine-tuned for specific tasks.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 称为嵌入的向量表示从大型数据集中学习，以捕捉单词或数据点之间的语义意义和关系。嵌入向量在每个元素中编码不同的语义属性，以密集的低维方式表示单词/数据，供机器学习模型使用。嵌入可以在大型语料库上预训练，然后针对特定任务进行微调。
- en: '![](../Images/add12597b4b65fb74b6ec4c16c42731c.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/add12597b4b65fb74b6ec4c16c42731c.png)'
- en: From words to embeddings. Image by the author.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 从单词到嵌入。图片来源于作者。
- en: There are more things to keep in mind while analyzing time series, like trends
    and seasonality. But when it comes down to how these data are represented in a
    neural network, the difference between text and time series ultimately comes down
    to the fact that time series are a sequence of values, while text is a sequence
    of vectors.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在分析时间序列时，还需考虑趋势和季节性等因素。但在神经网络中，这些数据的表示方式的最终区别在于时间序列是值的序列，而文本是向量的序列。
- en: Tasks for sequential data
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 顺序数据任务
- en: When examining sequential data, the most intuitive next step would be to predict
    what comes next in the sequence.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在检查顺序数据时，最直观的下一步是预测序列中接下来会发生什么。
- en: '**In Time series forecasting** you’re trying to predict a continuous value
    (like tomorrow’s stock price or the temperature next week) based on past data.
    The model is trained to minimize the difference between its predictions and the
    actual values, a common characteristic of regression tasks.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '**在时间序列预测中**，你试图根据过去的数据预测一个连续值（例如明天的股价或下周的温度）。模型被训练以最小化其预测值与实际值之间的差异，这是回归任务的一个常见特征。'
- en: '**Text generation** — or, more appropriately, **next-token prediction —** consists
    in training a model to predict the next token given the previous ones. Autoregressive
    language modeling can be viewed as a multi-class classification problem, where
    you can think of each possible token as a separate class. The output is a probability
    distribution over all possible tokens in the vocabulary.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '**文本生成**——或更恰当地说，**下一个标记预测**——包括训练一个模型，以根据之前的标记预测下一个标记。自回归语言建模可以被视为一个多类分类问题，其中每个可能的标记可以看作一个单独的类别。输出是词汇表中所有可能标记的概率分布。'
- en: '![](../Images/6d0d2354dd7912f8cd312ff0ca365622.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6d0d2354dd7912f8cd312ff0ca365622.png)'
- en: Text classification (sentiment analysis). Image by the author.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 文本分类（情感分析）。图片来源于作者。
- en: Other tasks involve **sentence classification** — categorizing sentences or
    documents into predefined classes, and **time series classification.**
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 其他任务包括**句子分类**——将句子或文档分类到预定义的类别中，以及**时间序列分类**。
- en: An example is sentiment analysis, a task where each text is categorized in a
    *Positive* and *Negative* class. Time series can be classified too, for example
    one can classify heartbeats are *Healthy* or *Disease* to detect anomalies.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 一个例子是情感分析，这是一个将每个文本分类为*积极*或*消极*类别的任务。时间序列也可以进行分类，例如，可以将心跳分类为*健康*或*疾病*以检测异常。
- en: '![](../Images/76161ab75060a0fb57b9033c560bb7fd.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/76161ab75060a0fb57b9033c560bb7fd.png)'
- en: Time series classification (ECG). Image by the author.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 时间序列分类（ECG）。图片来源于作者。
- en: Here, the models require training on datasets of manually annotated examples
    to learn how to map textual or time series features to categorical labels.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，模型需要在手动标注的示例数据集上进行训练，以学习如何将文本或时间序列特征映射到分类标签。
- en: Modeling sequential data
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 顺序数据建模
- en: Before today’s powerful neural networks were created for time series forecasting
    and natural language processing, different models were typically used for these
    tasks.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在今天强大的神经网络用于时间序列预测和自然语言处理之前，通常使用不同的模型来处理这些任务。
- en: Statistical methods like autoregressive integrated moving average **(ARIMA)**
    and exponential smoothing models were popular for time series forecasting before
    2010s. These rely on mathematical relationships between past values in a time
    series to predict future values. While effective on some data, they make rigid
    assumptions that limit performance on complex real-world time series.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在2010年代之前，像自回归积分滑动平均**（ARIMA）**和指数平滑模型这样的统计方法在时间序列预测中非常流行。这些方法依赖于时间序列中过去值之间的数学关系来预测未来值。虽然在一些数据上有效，但它们做出的刚性假设限制了其在复杂现实世界时间序列上的表现。
- en: In NLP, tasks like language translation and speech recognition were historically
    addressed using rule-based systems. These encode human-crafted rules and grammar,
    requiring extensive manual effort and struggling with the nuance and variability
    of real human language. Alternately, naive Bayes, logistic regression, and other
    classical machine learning models were sometimes applied but could not effectively
    capture long-term context and dependencies in textual data.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在自然语言处理（NLP）中，语言翻译和语音识别等任务过去通常使用基于规则的系统来解决。这些系统编码了人为制定的规则和语法，需要大量的手工努力，并且在处理真实人类语言的细微差别和变化时存在困难。作为替代，朴素贝叶斯、逻辑回归以及其他经典机器学习模型有时也会被应用，但这些模型无法有效捕捉文本数据中的长期上下文和依赖关系。
- en: '![](../Images/38b2d5628855c440a291a0b147fb174f.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/38b2d5628855c440a291a0b147fb174f.png)'
- en: Models for sequential data. Image by the author.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 用于序列数据的模型。作者提供的图像。
- en: The introduction of **RNN** and **LSTM** networks allowed contextual learning
    for time series forecasting and NLP. Rather than relying on rigid statistical
    assumptions or simple input-output mappings, RNNs can learn long-range dependencies
    from sequential data. This breakthrough enabled them to excel on problems like
    language modeling, sentiment analysis, and non-linear forecasting where classical
    approaches wouldn’t work. Though introduced in the 1980s, these models have only
    become practical in the last decade, as computational power has dramatically increased.
    Google started using LSTM in Google Voice in 2015\. [1]
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '**RNN**和**LSTM**网络的引入使得时间序列预测和自然语言处理能够进行上下文学习。与依赖于刚性统计假设或简单的输入输出映射不同，RNN可以从顺序数据中学习长期依赖关系。这一突破使得它们在语言建模、情感分析和非线性预测等问题上表现出色，而这些问题的经典方法无效。尽管在1980年代就已引入，但这些模型直到过去十年才变得实用，因为计算能力显著提高。Google在2015年开始在Google
    Voice中使用LSTM。[1]'
- en: Recurrent Neural Networks
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 循环神经网络
- en: RNNs contain recursive connections that allow information to persist across
    timesteps.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: RNN包含递归连接，允许信息在时间步长之间持续存在。
- en: When working on forecasting, the RNN can be trained on past observations from
    a time series to learn the temporal patterns. The RNN processes the sequence by
    updating its hidden state at each time step based on the current input and previous
    hidden state.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在进行预测时，可以训练递归神经网络（RNN）使用时间序列中的历史观测数据来学习时间模式。RNN通过在每个时间步根据当前输入和先前的隐藏状态来更新其隐藏状态来处理序列。
- en: For next token prediction, the RNN is trained on textual sequences like sentences
    where each token is a word. The RNN learns to predict the next word based on the
    previous words. The hidden state maintains the context of earlier words to inform
    the next prediction. At each step, the RNN outputs a probability distribution
    over the next token.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 对于下一个标记预测，RNN在像句子这样的文本序列上进行训练，其中每个标记是一个词。RNN学习基于先前的词预测下一个词。隐藏状态保持早期词的上下文，以指导下一个预测。在每一步，RNN输出下一个标记的概率分布。
- en: '![](../Images/2ea1cb73f038ecc40b2030344a36e3f4.png)![](../Images/1cda611cdeffde3dbe484bf67a0748fd.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2ea1cb73f038ecc40b2030344a36e3f4.png)![](../Images/1cda611cdeffde3dbe484bf67a0748fd.png)'
- en: An LSTM model can be used both for forecasting and next-word prediction. Image
    by author.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: LSTM模型可用于预测和下一个词预测。作者提供的图像。
- en: The ability of RNNs to remember past context has been transformational for sequence
    tasks in NLP and time series analysis. However, they can struggle with long-term
    dependencies due to issues like vanishing and exploding gradients. This issue
    motivated architectural advances like **LSTMs** to improve gradient flow across
    many timesteps and has been further enhanced using attention-based models.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: RNNs 记住过去上下文的能力对 NLP 和时间序列分析中的序列任务产生了变革性影响。然而，由于梯度消失和爆炸等问题，它们在处理长期依赖时可能会遇到困难。这一问题促使了像**LSTM**这样的架构进展，以改善跨多个时间步的梯度流，并通过基于注意力的模型进一步增强。
- en: Transformers
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Transformers
- en: '**Attention** mechanisms made possible all the amazing LLMs we know today.
    They were initially introduced to augment RNNs by allowing models to focus on
    relevant parts of the input sequence when making predictions. Attention functions
    score the importance of each timestep and use these weights to extract relevant
    context.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意力**机制使得我们今天所知的所有惊人 LLM 成为可能。它们最初是为了增强 RNNs，通过允许模型在进行预测时关注输入序列的相关部分。注意力函数为每个时间步评分重要性，并利用这些权重提取相关上下文。'
- en: Attention has become an indispensable component for sequence tasks across NLP
    and time series modeling. It improves model accuracy and interpretability by focusing
    on relevant inputs.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 注意力已成为 NLP 和时间序列建模中序列任务不可或缺的组成部分。通过关注相关输入，它提高了模型的准确性和可解释性。
- en: '![](../Images/d36cd0e2b60f90673e27409a09c55aec.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d36cd0e2b60f90673e27409a09c55aec.png)'
- en: The original Transformer architecture. Positional encoding allows the capture
    of the order of the sequence. [2]
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 原始的 Transformer 架构。位置编码使得序列的顺序能够被捕捉到。[2]
- en: The Transformer architecture relying entirely on self-attention has led to breakthrough
    results in NLP and time series modeling. The self-attention layers allow the modeling
    of dependencies regardless of the distance between sequence elements as long as
    the sequence fits in the context length.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 完全依赖自注意力的 Transformer 架构在 NLP 和时间序列建模中取得了突破性成果。自注意力层允许建模序列元素之间的依赖关系，只要序列适合上下文长度，无论距离多远。
- en: Transformers have become state-of-the-art for sequential data, **with this architecture
    adapted to NLP as BERT and time series as Temporal Fusion Transformer.**
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: Transformers 已成为顺序数据的最先进技术，**这种架构被适配为 NLP 的 BERT 和时间序列的 Temporal Fusion Transformer。**
- en: Towards Foundation Models for Time Series
  id: totrans-62
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 面向时间序列的基础模型
- en: A **foundation model** is a large machine learning model that can be trained
    on vast amount of data and then adapted to various tasks. Foundation models differ
    from traditional machine learning models, which usually perform specific tasks.
    They are more general and flexible and can be used as a starting point for developing
    more specialized applications. Avoiding expensive training from scratch can significantly
    reduce the time and cost of building new applications.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '**基础模型**是一个大型机器学习模型，可以在大量数据上进行训练，然后适应各种任务。基础模型不同于传统的机器学习模型，后者通常执行特定任务。它们更为通用和灵活，可以作为开发更专业应用的起点。避免从头开始的昂贵训练可以显著减少构建新应用的时间和成本。'
- en: In NLP, Large Language Models allow ***in-context learning*** — they can perform
    new tasks they weren’t explicitly trained for. This revolutionary capability makes
    ChatGPT and other LLMs so powerful, as they can generalize to a wide variety of
    tasks.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在 NLP 中，大型语言模型允许***上下文学习***——它们可以执行未明确训练的新任务。这一革命性能力使 ChatGPT 和其他 LLM 如此强大，因为它们能够泛化到各种任务。
- en: Most current forecasting approaches must be individually fit to each new dataset.
    This process is time-consuming and requires domain expertise. To address this
    problem, the concept of foundation models has lately been applied to time series
    data.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 当前的大多数预测方法必须针对每个新数据集进行单独拟合。这一过程耗时且需要领域专业知识。为了解决这个问题，基础模型的概念最近被应用于时间序列数据。
- en: '![](../Images/4199e7c68d4e012ff1e51eaec8e63fd3.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4199e7c68d4e012ff1e51eaec8e63fd3.png)'
- en: TimeGPT is pre-trained on massive datasets and can generate new data. [3]
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: TimeGPT 在大量数据集上进行了预训练，并能够生成新数据。[3]
- en: '**TimeGPT i**s a Transformer-based neural network pre-trained on a diverse
    dataset of over 100 billion time series data points encompassing domains like
    economics, weather, transport, retail sales, etc. The key innovation is that,
    like GPT-3, TimeGPT can generalize to make accurate forecasts on new time series
    data without retraining on each new dataset. This *zero-shot* ability provides
    immense time and resource savings compared to traditional forecasting pipelines.
    A foundation model simplifies forecasting to a single model that can be applied
    to any time series with just a few lines of code. [2]'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '**TimeGPT**是一个基于Transformer的神经网络，预训练于超过1000亿个时间序列数据点的多样数据集，涵盖了经济学、天气、交通、零售销售等领域。其关键创新在于，像GPT-3一样，TimeGPT能够泛化，对新的时间序列数据进行准确预测，而无需对每个新数据集重新训练。这种*零样本*能力相比于传统预测管道节省了大量时间和资源。一个基础模型简化了预测过程，仅用几行代码即可应用于任何时间序列。[2]'
- en: Takeaways
  id: totrans-69
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 收获
- en: When doing deep learning, think outside the box. Data and models have more in
    common than they appear — everything is interconnected. Both time series analysis
    and NLP are rapidly innovating and sharing ideas.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在进行深度学习时，要跳出框框思考。数据和模型之间有更多共同之处——一切都是互联的。时间序列分析和自然语言处理（NLP）都在迅速创新和共享思想。
- en: Time series and NLP share many parallels as sequential data types. We model
    both with architectures such as RNN, LSTM, and Transformers. As deep learning
    advances, we expect techniques to continue crossing over between these domains.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 时间序列和自然语言处理作为序列数据类型有许多相似之处。我们使用RNN、LSTM和Transformer等架构对这两者进行建模。随着深度学习的进步，我们预计技术将继续在这些领域之间交叉。
- en: The 2010s were the decade of neural networks conquering domains once dominated
    by statistical models. The 2020s look set to be the decade of transformers cementing
    their dominance, and researchers continue pushing the boundaries of these formidable
    models.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 2010年代是神经网络征服曾经由统计模型主导的领域的十年。2020年代看起来将是Transformer巩固其主导地位的十年，研究人员不断推动这些强大模型的边界。
- en: '*Enjoyed this article? Get weekly data science interview questions delivered
    to your inbox by subscribing to my newsletter,* [*The Data Interview*](https://thedatainterview.substack.com/)*.*'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '*喜欢这篇文章吗？通过订阅我的新闻通讯，每周获取数据科学面试问题送到你的邮箱，* [*The Data Interview*](https://thedatainterview.substack.com/)*。*'
- en: '*Also, you can find me on* [*LinkedIn*](https://www.linkedin.com/in/driccio/)*.*'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '*另外，你可以在* [*LinkedIn*](https://www.linkedin.com/in/driccio/)*上找到我。*'
- en: References
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] [Long short-term memory — Wikipedia](https://en.wikipedia.org/wiki/Long_short-term_memory)'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] [长短期记忆 — 维基百科](https://en.wikipedia.org/wiki/Long_short-term_memory)'
- en: '[2][[1706.03762] Attention Is All You Need (arxiv.org)](https://arxiv.org/abs/1706.03762)'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '[2][[1706.03762] Attention Is All You Need (arxiv.org)](https://arxiv.org/abs/1706.03762)'
- en: '[3] [[2310.03589] TimeGPT-1 (arxiv.org)](https://arxiv.org/abs/2310.03589?ref=emergentmind)'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] [[2310.03589] TimeGPT-1 (arxiv.org)](https://arxiv.org/abs/2310.03589?ref=emergentmind)'
