- en: Augmenting LLMs with RAG
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 用 RAG 增强 LLM
- en: 原文：[https://towardsdatascience.com/augmenting-llms-with-rag-f79de914e672](https://towardsdatascience.com/augmenting-llms-with-rag-f79de914e672)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/augmenting-llms-with-rag-f79de914e672](https://towardsdatascience.com/augmenting-llms-with-rag-f79de914e672)
- en: An End to End Example Of Seeing How Well An LLM Model Can Answer Amazon SageMaker
    Related Questions
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 查看一个大型语言模型（LLM）模型如何回答 Amazon SageMaker 相关问题的端到端示例
- en: '[](https://ram-vegiraju.medium.com/?source=post_page-----f79de914e672--------------------------------)[![Ram
    Vegiraju](../Images/07d9334e905f710d9f3c6187cf69a1a5.png)](https://ram-vegiraju.medium.com/?source=post_page-----f79de914e672--------------------------------)[](https://towardsdatascience.com/?source=post_page-----f79de914e672--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----f79de914e672--------------------------------)
    [Ram Vegiraju](https://ram-vegiraju.medium.com/?source=post_page-----f79de914e672--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://ram-vegiraju.medium.com/?source=post_page-----f79de914e672--------------------------------)[![Ram
    Vegiraju](../Images/07d9334e905f710d9f3c6187cf69a1a5.png)](https://ram-vegiraju.medium.com/?source=post_page-----f79de914e672--------------------------------)[](https://towardsdatascience.com/?source=post_page-----f79de914e672--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----f79de914e672--------------------------------)
    [Ram Vegiraju](https://ram-vegiraju.medium.com/?source=post_page-----f79de914e672--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----f79de914e672--------------------------------)
    ·9 min read·Oct 10, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----f79de914e672--------------------------------)
    ·9 分钟阅读·2023年10月10日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/553cc94765b38c8f5437dbc15e3856a6.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/553cc94765b38c8f5437dbc15e3856a6.png)'
- en: Image from [Unsplash](https://unsplash.com/photos/lUSFeh77gcs)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源 [Unsplash](https://unsplash.com/photos/lUSFeh77gcs)
- en: I’ve written quite a few blogs on Medium around different technical topics,
    and more heavily around [Machine Learning (ML) Model Hosting on Amazon SageMaker](https://ram-vegiraju.medium.com/list/amazon-sagemaker-f1b06f720fba).
    I’ve also lately developed an interest for the growing Generative AI/Large Language
    Model ecosystem (like everyone else in the industry lol).
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 我在 Medium 上写了不少关于不同技术主题的博客，特别是关于 [在 Amazon SageMaker 上托管机器学习（ML）模型](https://ram-vegiraju.medium.com/list/amazon-sagemaker-f1b06f720fba)。最近，我也对日益增长的生成式
    AI/大型语言模型生态系统产生了兴趣（就像行业中的其他人一样，哈哈）。
- en: These two different verticals led me to an interesting question. **How good
    are my Medium articles at teaching Amazon SageMaker?** To answer this I decided
    to implement a Generative AI solution that utilizes [Retrieval Augmented Generation
    (RAG)](https://docs.aws.amazon.com/sagemaker/latest/dg/jumpstart-foundation-models-customize-rag.html)
    with access to some of my articles to see how well it could answer some SageMaker
    related questions.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个不同的领域让我产生了一个有趣的问题。**我的 Medium 文章在教授 Amazon SageMaker 方面表现如何？** 为了回答这个问题，我决定实现一个利用
    [检索增强生成（RAG）](https://docs.aws.amazon.com/sagemaker/latest/dg/jumpstart-foundation-models-customize-rag.html)
    的生成式 AI 解决方案，并访问我的一些文章，以查看它能够多好地回答一些与 SageMaker 相关的问题。
- en: 'In this article we’ll take a look at building an end to end Generative AI solution
    and utilize a few different popular tools to operationalize this workflow:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我们将构建一个端到端的生成式 AI 解决方案，并利用一些流行的工具来实现这个工作流：
- en: '[**LangChain**](https://www.langchain.com/): LangChain is a popular Python
    framework that helps simplify Generative AI applications by providing ready made
    modules that help with Prompt Engineering, RAG implementation, and LLM workflow
    orchestration.'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**LangChain**](https://www.langchain.com/): LangChain 是一个流行的 Python 框架，通过提供现成的模块来帮助简化生成式
    AI 应用程序，这些模块有助于提示工程、RAG 实现和 LLM 工作流编排。'
- en: '[**OpenAI**](https://openai.com/): LangChain will take care of the orchestration
    of our Generative AI App, the brains however is still the model. In this case
    we use an OpenAI provided LLM, but LangChain also integrates with different model
    sources such as SageMaker Endpoints, Cohere, etc.'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**OpenAI**](https://openai.com/): LangChain 将负责我们生成式 AI 应用程序的编排，但大脑仍然是模型。在这种情况下，我们使用了
    OpenAI 提供的 LLM，但 LangChain 也可以与不同的模型源集成，如 SageMaker 端点、Cohere 等。'
- en: '**NOTE**: This article assumes an intermediate understanding of Python and
    a basic understanding of LangChain in specific. I would suggest following this
    [article](/getting-started-with-langchain-a-beginners-guide-to-building-llm-powered-applications-95fc8898732c)
    for understanding LangChain and building Generative AI applications better.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意**：本文假设读者具有中级 Python 了解和对 LangChain 的基本了解。我建议参考这篇 [文章](/getting-started-with-langchain-a-beginners-guide-to-building-llm-powered-applications-95fc8898732c)
    来更好地理解 LangChain 和构建生成性 AI 应用。'
- en: '**DISCLAIMER**: I am a Machine Learning Architect at AWS and my opinions are
    my own.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '**免责声明**：我在 AWS 担任机器学习架构师，我的观点仅代表我个人。'
- en: Problem Overview
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题概述
- en: Large Language Models (LLMs) by themselves are incredibly powerful and can often
    answer many questions without assistance from fine-tuning or additional knowledge/context.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）本身非常强大，通常可以在不依赖微调或额外知识/上下文的情况下回答许多问题。
- en: This however can become a bottleneck when you need access to other specific
    sources of data and especially recent data. For example, while OpenAI has been
    trained on a large corpus of data it does not have knowledge of my recent Medium
    articles that I have written.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，当你需要访问其他特定的数据源，尤其是最新数据时，这可能会成为瓶颈。例如，虽然 OpenAI 已经在大量数据语料库上进行了训练，但它并不知道我最近在
    Medium 上写的文章。
- en: In this case we want to check how well my Medium articles can help assist with
    answering questions about Amazon SageMaker. OpenAI’s models already do have some
    knowledge of Amazon SageMaker from the corpus they have been trained on. What
    we want to see is how much performance we can gain by providing these LLMs with
    access to my Medium articles. These can serve almost as a cheat sheet of sort
    for LLMs that already have a large knowledge bank.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们想要检查我的 Medium 文章在回答关于 Amazon SageMaker 的问题方面能提供多大的帮助。OpenAI 的模型已经从它们训练过的语料库中获得了一些关于
    Amazon SageMaker 的知识。我们想要看到的是，通过让这些大型语言模型（LLMs）访问我的 Medium 文章，我们能提升多少性能。这些文章几乎可以作为一个为已经拥有大量知识库的
    LLMs 提供的速查表。
- en: How do we provide these LLMs access to this additional knowledge and information?
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何为这些 LLMs 提供访问这些额外知识和信息的途径？
- en: Why We Need RAG
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为什么我们需要 RAG
- en: 'This is where [Retrieval Augmented Generation (RAG)](https://aws.amazon.com/blogs/machine-learning/question-answering-using-retrieval-augmented-generation-with-foundation-models-in-amazon-sagemaker-jumpstart/)
    comes into play. With RAG we provide an information retrieval system that gives
    us access to the additional data that we need. This will help us answer more advanced
    SageMaker questions and augment our LLMs knowledge base. To implement a basic
    RAG system we need a few components:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是 [检索增强生成（RAG）](https://aws.amazon.com/blogs/machine-learning/question-answering-using-retrieval-augmented-generation-with-foundation-models-in-amazon-sagemaker-jumpstart/)
    发挥作用的地方。通过 RAG，我们提供了一个信息检索系统，使我们能够访问所需的额外数据。这将帮助我们回答更高级的 SageMaker 问题，并增强我们的 LLM
    知识库。要实现一个基本的 RAG 系统，我们需要几个组件：
- en: '[**Embeddings Model**](https://medium.com/@ryanntk/choosing-the-right-embedding-model-a-guide-for-llm-applications-7a60180d28e3):
    For the data we provide access to, this can’t simply be just a bunch of text or
    images, rather they need to be captured in a numeric/vector format for all NLP
    models (including LLMs) to understand. To transform our data, we utilize the [OpenAI
    Embeddings Model](https://platform.openai.com/docs/guides/embeddings), but there
    are a variety of different choices such as Cohere, Amazon Titan, etc that you
    can evaluate for performance.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**嵌入模型**](https://medium.com/@ryanntk/choosing-the-right-embedding-model-a-guide-for-llm-applications-7a60180d28e3)：对于我们提供访问的数据，这些数据不能仅仅是一些文本或图像，而是需要以数值/向量格式进行捕捉，以便所有自然语言处理模型（包括
    LLMs）能够理解。为了转换我们的数据，我们利用了 [OpenAI 嵌入模型](https://platform.openai.com/docs/guides/embeddings)，但也有多种不同的选择，如
    Cohere、Amazon Titan 等，你可以评估它们的性能。'
- en: '[**Vector Store**](https://aws.amazon.com/blogs/database/the-role-of-vector-datastores-in-generative-ai-applications/):
    Once we have our embeddings, we need to utilize a vector datastore that not only
    stores these vectors but also provides an efficient manner in which we can index
    and retrieve relevant data. When a user has a query, we want to return any relevant
    context that contains similarity to this input. Most of these vector stores are
    powered by KNN and other nearest neighbor algorithms to provide relevant context
    for the initial question. In the case of this solution we utilize the Facebook
    library [FAISS](https://github.com/facebookresearch/faiss), which can be utilized
    for efficient similarity search and clustering of vectors.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**向量存储**](https://aws.amazon.com/blogs/database/the-role-of-vector-datastores-in-generative-ai-applications/)：一旦我们拥有了嵌入，我们需要利用一个向量数据存储，不仅存储这些向量，还提供一种高效的方式来索引和检索相关数据。当用户有查询时，我们希望返回包含与该输入相似性的任何相关上下文。这些向量存储大多数由
    KNN 和其他最近邻算法提供支持，以为初始问题提供相关上下文。在这个解决方案中，我们使用了 Facebook 的 [FAISS](https://github.com/facebookresearch/faiss)
    库，该库可用于高效的相似性搜索和向量聚类。'
- en: '**LLM Model**: We have two models in this case, the embeddings model to create
    the embeddings, but we still also need the main LLM that takes these embeddings
    and the user input to return an output. In this case also we use the default [ChatOpenAI
    model](https://platform.openai.com/docs/guides/gpt).'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**LLM 模型**：在这种情况下，我们有两个模型，一个是用于创建嵌入的嵌入模型，但我们仍然需要主要的 LLM，它接收这些嵌入和用户输入以返回输出。在这种情况下，我们也使用默认的
    [ChatOpenAI 模型](https://platform.openai.com/docs/guides/gpt)。'
- en: '![](../Images/52f11376cdf9d37a40cb69cc03dc207c.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/52f11376cdf9d37a40cb69cc03dc207c.png)'
- en: RAG Flow (Created by Author)
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: RAG 流程（作者创建）
- en: Essentially you can think of RAG as a performance enhancer of LLMs by providing
    extra knowledge that the base LLM might not already have. In the next section
    we’ll take a look at how we can implement these concepts utilizing LangChain and
    OpenAI.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 从本质上讲，你可以将 RAG 视为 LLM 的性能增强器，通过提供基础 LLM 可能尚未拥有的额外知识。在下一部分中，我们将探讨如何利用 LangChain
    和 OpenAI 实现这些概念。
- en: Generative AI Application & Sample Inference
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 生成式 AI 应用与示例推理
- en: To get started you need an OpenAI API Key, which you can find and install at
    the following [link](https://platform.openai.com/docs/api-reference). Note the
    charges per rate/API limit so that you have an understanding of the pricing structure.
    For development we work in a [SageMaker Classic Notebook Instance](https://docs.aws.amazon.com/sagemaker/latest/dg/nbi.html),
    but any environment with OpenAI and LangChain installed should be sufficient.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 要开始，你需要一个 OpenAI API 密钥，你可以在以下 [链接](https://platform.openai.com/docs/api-reference)
    找到并安装。注意费用和 API 限制，以便了解定价结构。对于开发，我们在 [SageMaker Classic Notebook 实例](https://docs.aws.amazon.com/sagemaker/latest/dg/nbi.html)
    中工作，但任何安装了 OpenAI 和 LangChain 的环境都应足够。
- en: '[PRE0]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: After setting up LangChain and OpenAI, we create a local directory with ten
    of my popular Medium articles stored as PDFs. This will be the additional data/information
    we are making available to my LLM.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 设置 LangChain 和 OpenAI 后，我们创建一个本地目录，存储十篇我的热门 Medium 文章作为 PDF。这将是我们为 LLM 提供的额外数据/信息。
- en: '![](../Images/6236ef526322058a90cadb9927e65e01.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6236ef526322058a90cadb9927e65e01.png)'
- en: Medium Articles (Screenshot by Author)
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: Medium 文章（作者截图）
- en: As next steps we need to be able to load this data and also create a directory
    for where we can store the embeddings that we generate. LangChain has lots of
    utilities that will automatically load and also split/chunk your data for you.
    [Chunking](https://www.pinecone.io/learn/chunking-strategies/) is specifically
    important as we don’t want larger sets of data for the embeddings we generate.
    The larger the data the more potential noise that can be introduced.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步，我们需要能够加载这些数据，并创建一个目录以存储我们生成的嵌入。LangChain 提供了许多工具，可以自动加载并拆分/切块你的数据。 [切块](https://www.pinecone.io/learn/chunking-strategies/)
    尤其重要，因为我们不希望生成的嵌入数据集过大。数据越大，可能引入的噪声也越多。
- en: In this case we use the LangChain provided [PDF loader](https://python.langchain.com/docs/modules/data_connection/document_loaders/pdf)
    to load and split our data.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们使用 LangChain 提供的 [PDF 加载器](https://python.langchain.com/docs/modules/data_connection/document_loaders/pdf)
    来加载和拆分我们的数据。
- en: '[PRE1]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: We then instantiate our OpenAI Embeddings Model. We use the Embeddings model
    to create our embeddings and populate the local Cache directory we have created.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们实例化我们的 OpenAI 嵌入模型。我们使用嵌入模型创建我们的嵌入，并填充我们创建的本地缓存目录。
- en: '[PRE2]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '![](../Images/40cb3e81281f6644ffa9e9b5ee5447c0.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/40cb3e81281f6644ffa9e9b5ee5447c0.png)'
- en: Embeddings Generated (Screenshot by Author)
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 嵌入生成（作者截图）
- en: We then create our FAISS Vector Store and push our embedded documents.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们创建了 FAISS 向量存储并推送了嵌入的文档。
- en: '[PRE3]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: We then use a [RetrievalQA Chain](https://python.langchain.com/docs/use_cases/question_answering/how_to/vector_db_qa)
    to bring all these moving parts together. We specify our vector store we have
    created above and also pass in the ChatOpenAI default LLM as our model that will
    recive both the input and the relevant documents for context.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们使用[检索问答链](https://python.langchain.com/docs/use_cases/question_answering/how_to/vector_db_qa)来将这些移动部件汇聚在一起。我们指定了上面创建的向量存储，并将
    ChatOpenAI 默认 LLM 作为模型，该模型将接收输入和相关文档以获取上下文。
- en: '[PRE4]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: We then can compare performance of the model without RAG as opposed to our RAG
    based chain by passing in the same prompts and observing results. Let’s run a
    loop of sample prompts of varying difficulties.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们可以通过传入相同的提示并观察结果，比较模型在没有 RAG 的情况下与基于 RAG 的链的性能。让我们运行一系列不同难度的示例提示。
- en: '[PRE5]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: We see the first question itself is very specific to my writing. We know the
    OpenAI model does not have any access or knowledge of my articles, thus it outputs
    a pretty random and inaccurate description.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到第一个问题本身非常具体于我的写作。我们知道 OpenAI 模型没有访问或了解我的文章，因此它给出的描述非常随机且不准确。
- en: '![](../Images/877944bd4385e258faa94879d793f6b7.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/877944bd4385e258faa94879d793f6b7.png)'
- en: OpenAI Response (Screenshot by Author)
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI 回应（作者截图）
- en: Alternatively, our RAG chain has had access to some of my Medium articles and
    produces a somewhat accurate summary of my writing.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，我们的 RAG 链接访问了一些我的 Medium 文章，并生成了对我写作内容的相当准确的总结。
- en: '![](../Images/cf7aeab9513869d39110eec827a8a987.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/cf7aeab9513869d39110eec827a8a987.png)'
- en: RAG Response (Screenshot by Author)
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: RAG 回应（作者截图）
- en: 'We can then test both approaches by asking some SageMaker specific questions.
    We start with a very basic question: What is Amazon SageMaker? As the OpenAI LLM
    has knowledge of this matter it responds with a fairly accurate and comparable
    response to our RAG based approach.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过提出一些 SageMaker 相关的问题来测试这两种方法。我们从一个非常基本的问题开始：什么是 Amazon SageMaker？由于 OpenAI
    LLM 对此有了解，它会给出一个相当准确并且可以与我们基于 RAG 的方法相媲美的回答。
- en: '![](../Images/7afbc2d5452eb56315db8d8e37ce6e14.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7afbc2d5452eb56315db8d8e37ce6e14.png)'
- en: OpenAI vs RAG Response (Screenshot by Author)
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI 与 RAG 回应（作者截图）
- en: 'Where we start seeing the real benefits of RAG is as the questions start getting
    more specific and difficult. An example of this is the prompt comparing the two
    advanced hosting options: [Multi-Model Endpoints (MME)](https://docs.aws.amazon.com/sagemaker/latest/dg/multi-model-endpoints.html)
    and [Multi-Container Endpoints (MCE)](https://aws.amazon.com/blogs/machine-learning/part-5-model-hosting-patterns-in-amazon-sagemaker-cost-efficient-ml-inference-with-multi-framework-models-on-amazon-sagemaker/).'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 当问题开始变得更加具体和困难时，我们开始看到 RAG 的真正好处。一个例子是比较两种高级托管选项的提示：[多模型端点 (MME)](https://docs.aws.amazon.com/sagemaker/latest/dg/multi-model-endpoints.html)
    和 [多容器端点 (MCE)](https://aws.amazon.com/blogs/machine-learning/part-5-model-hosting-patterns-in-amazon-sagemaker-cost-efficient-ml-inference-with-multi-framework-models-on-amazon-sagemaker/)。
- en: '![](../Images/30fcd243656d7712a3246e49bb5f1451.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/30fcd243656d7712a3246e49bb5f1451.png)'
- en: MME vs MCE (Screenshot by Author)
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: MME 与 MCE（作者截图）
- en: Here we see that the Vanilla OpenAI response gives a completely inaccurate answer,
    it has no knowledge of these two recent capabilities. My [specific Medium article
    around MCE vs MME](/sagemaker-multi-model-vs-multi-container-endpoints-304f4c151540),
    however gives the model context around these offerings and it’s thus able to answer
    the query accurately.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里我们看到，Vanilla OpenAI 的回答给出了一个完全不准确的答案，它对这两个最近的功能没有了解。然而，我的[关于 MCE 与 MME 的具体
    Medium 文章](/sagemaker-multi-model-vs-multi-container-endpoints-304f4c151540)为模型提供了这些功能的背景，因此它能够准确回答查询。
- en: With RAG we can augment the basic knowledge our LLM already has around SageMaker.
    In the next section we can look at different methods to improve this prototype
    that we have built.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 借助 RAG，我们可以扩展我们 LLM 已经掌握的关于 SageMaker 的基础知识。在下一部分，我们可以查看不同的方法来改进我们所构建的原型。
- en: How Can We Improve Performance?
  id: totrans-61
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 我们如何提高性能？
- en: 'While this is a neat solution, there’s still a lot of room for improvement
    to scale this up. A few potential methods you can use to improve RAG based performance
    include the following:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这是一个很好的解决方案，但还有很多改进空间以便于扩展。你可以用来改进基于 RAG 的性能的一些潜在方法包括以下几点：
- en: '**Data Size and Quality**: In this case we’ve only provided ten Medium articles
    and still see solid performance. To boost this we could also provide access to
    my entire set of Medium articles or anything with the tag “SageMaker” for instance.
    We’ve also directly copied my articles without any formatting and the PDFs themselves
    are very unstructured, cleaning up the data format can help make chunking and
    therefore performance more optimal. **NOTE:** It’s also essential that the data
    you use should only rely on resources/articles that you are allowed to use for
    your purpose. In this example there’s no issue with my Medium articles as the
    source, but always make sure to ensure that you are using data in an authorized
    manner.'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据大小和质量**：在这个案例中，我们仅提供了十篇Medium文章，仍然看到了良好的性能。为了提升效果，我们还可以提供我所有Medium文章的访问权限，或者任何带有“
    SageMaker”标签的内容。我们还直接复制了我的文章而没有任何格式化，PDF本身也非常无结构，清理数据格式可以帮助更好地分块，从而优化性能。**注意：**
    使用的数据也必须仅依赖于你被允许用于目的的资源/文章。在这个例子中，以我的Medium文章作为来源没有问题，但始终确保你以授权的方式使用数据。'
- en: '**Vector Store Optimization**: In this case we’ve utilized the default FAISS
    vector store setup. Items you can tune are the speed of the vector store indexing
    as well as the number of documents to retrieve and provide to your LLM.'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**向量存储优化**：在这个案例中，我们使用了默认的FAISS向量存储设置。你可以调整的项包括向量存储索引的速度，以及检索和提供给LLM的文档数量。'
- en: '**Fine-Tuning vs RAG**: While RAG helps attain domain specific knowledge, fine-tuning
    is also another method to help an LLM attain a specific knowledge set. You want
    to evaluate your use-case here as well to see if fine-tuning makes more sense
    or a combination of both. Generally fine-tuning is very performant if you have
    quality data available. In this case with RAG we didn’t even necessarily format
    or shape our data, yet we were still able to yield strong results. With fine-tuning
    data availability AND quality is essential. For a full breakdown of both options
    please refer to the following [article](/rag-vs-finetuning-which-is-the-best-tool-to-boost-your-llm-application-94654b1eaba7).'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**微调与RAG**：虽然RAG有助于获得领域特定的知识，但微调也是帮助LLM获得特定知识集的另一种方法。你需要在这里评估你的使用案例，以确定是微调更有意义，还是两者的结合。一般来说，如果你有优质的数据，微调表现非常出色。在这种情况下，我们甚至没有对数据进行格式化或调整，但仍能取得良好的结果。对于微调，数据的可用性和质量至关重要。有关这两种选项的详细分析，请参阅以下[文章](/rag-vs-finetuning-which-is-the-best-tool-to-boost-your-llm-application-94654b1eaba7)。'
- en: Additional Resources & Conclusion
  id: totrans-66
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 附加资源与结论
- en: '[](https://github.com/RamVegiraju/LangChain-Samples/tree/master/Medium-SageMaker-Analyzer?source=post_page-----f79de914e672--------------------------------)
    [## LangChain-Samples/Medium-SageMaker-Analyzer at master · RamVegiraju/LangChain-Samples'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://github.com/RamVegiraju/LangChain-Samples/tree/master/Medium-SageMaker-Analyzer?source=post_page-----f79de914e672--------------------------------)
    [## LangChain-Samples/Medium-SageMaker-Analyzer在master · RamVegiraju/LangChain-Samples]'
- en: Examples integrating with LangChain for GenAI. Contribute to RamVegiraju/LangChain-Samples
    development by creating an…
  id: totrans-68
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 示例展示了如何将LangChain与GenAI集成。通过创建一个…贡献于RamVegiraju/LangChain-Samples的开发。
- en: github.com](https://github.com/RamVegiraju/LangChain-Samples/tree/master/Medium-SageMaker-Analyzer?source=post_page-----f79de914e672--------------------------------)
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: github.com](https://github.com/RamVegiraju/LangChain-Samples/tree/master/Medium-SageMaker-Analyzer?source=post_page-----f79de914e672--------------------------------)
- en: The code for the entire example can be found at the link above. This was a fun
    project to evaluate the value of my articles while also showing how to integrate
    RAG into your Generative AI applications. In coming articles we’ll continue to
    explore more Generative AI and LLM driven capabilities.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 整个示例的代码可以在上述链接中找到。这是一个有趣的项目，用于评估我的文章的价值，同时展示如何将RAG集成到你的生成式AI应用程序中。在接下来的文章中，我们将继续探索更多生成式AI和LLM驱动的功能。
- en: As always thank you for reading and feel free to leave any feedback.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 一如既往，感谢你的阅读，欢迎随时留下反馈。
- en: '*If you enjoyed this article feel free to connect with me on* [*LinkedIn*](https://www.linkedin.com/in/ram-vegiraju-81272b162/)
    *and subscribe to my Medium* [*Newsletter*](https://ram-vegiraju.medium.com/subscribe)*.*'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '*如果你喜欢这篇文章，请随时通过* [*LinkedIn*](https://www.linkedin.com/in/ram-vegiraju-81272b162/)
    *与我联系，并订阅我的Medium* [*新闻通讯*](https://ram-vegiraju.medium.com/subscribe)*。*'
