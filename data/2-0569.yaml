- en: Convolutional vs Feedforward Autoencoders for Image Denoising
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 卷积自编码器与前馈自编码器在图像去噪中的比较
- en: 原文：[https://towardsdatascience.com/convolutional-vs-feedforward-autoencoders-for-image-denoising-2fe2e9aed71d](https://towardsdatascience.com/convolutional-vs-feedforward-autoencoders-for-image-denoising-2fe2e9aed71d)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/convolutional-vs-feedforward-autoencoders-for-image-denoising-2fe2e9aed71d](https://towardsdatascience.com/convolutional-vs-feedforward-autoencoders-for-image-denoising-2fe2e9aed71d)
- en: KavetiAPPLICATIONS OF AUTOENCODERS
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Kaveti自编码器的应用
- en: Cleaning corrupted images using convolutional and feedforward autoencoders
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用卷积和前馈自编码器清理损坏的图像
- en: '[](https://rukshanpramoditha.medium.com/?source=post_page-----2fe2e9aed71d--------------------------------)[![Rukshan
    Pramoditha](../Images/b80426aff64ff186cb915795644590b1.png)](https://rukshanpramoditha.medium.com/?source=post_page-----2fe2e9aed71d--------------------------------)[](https://towardsdatascience.com/?source=post_page-----2fe2e9aed71d--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----2fe2e9aed71d--------------------------------)
    [Rukshan Pramoditha](https://rukshanpramoditha.medium.com/?source=post_page-----2fe2e9aed71d--------------------------------)'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://rukshanpramoditha.medium.com/?source=post_page-----2fe2e9aed71d--------------------------------)[![Rukshan
    Pramoditha](../Images/b80426aff64ff186cb915795644590b1.png)](https://rukshanpramoditha.medium.com/?source=post_page-----2fe2e9aed71d--------------------------------)[](https://towardsdatascience.com/?source=post_page-----2fe2e9aed71d--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----2fe2e9aed71d--------------------------------)
    [Rukshan Pramoditha](https://rukshanpramoditha.medium.com/?source=post_page-----2fe2e9aed71d--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----2fe2e9aed71d--------------------------------)
    ·9 min read·Jan 24, 2023
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page-----2fe2e9aed71d--------------------------------)
    ·阅读时间9分钟·2023年1月24日
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/3dcd4cfa1a5be3ca4f767f62e4438c48.png)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3dcd4cfa1a5be3ca4f767f62e4438c48.png)'
- en: Photo by [Pierre Bamin](https://unsplash.com/@bamin?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
    on [Unsplash](https://unsplash.com/photos/_EzTds6Fo44?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
    (Slightly modified by the author)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由[皮埃尔·巴敏](https://unsplash.com/@bamin?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)提供，来自[Unsplash](https://unsplash.com/photos/_EzTds6Fo44?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)（作者稍作修改）
- en: '*Do you want to find out how convolutional autoencoders outperform feedforward
    autoencoders in image denoising?*'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '*你想了解卷积自编码器如何在图像去噪中优于前馈自编码器吗？*'
- en: If ‘*Yes’*, just keep reading this article.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 如果‘*是*’，请继续阅读这篇文章。
- en: Different types of autoencoders
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 不同类型的自编码器
- en: There are many practical applications of autoencoders. Image denoising is one
    of them.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 自编码器有许多实际应用。图像去噪就是其中之一。
- en: Image denoising refers to removing noise from corrupted images to get clean
    images.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 图像去噪指的是从损坏的图像中去除噪声，以获得干净的图像。
- en: The autoencoders used for image denoising are especially known as **denoising
    autoencoders**.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 用于图像去噪的自编码器特别被称为**去噪自编码器**。
- en: We have already covered the fundamentals of autoencoders in the following articles.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经在以下文章中涵盖了自编码器的基础知识。
- en: '[PRE0]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'As a quick recap, an autoencoder is a type of neural network architecture that
    consists of three key elements: **Encoder**, **Decoder** and **Latent vector**.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 简单回顾一下，自编码器是一种神经网络架构，包含三个关键元素：**编码器**、**解码器**和**潜在向量**。
- en: The relationship between these key elements is shown in the following diagram.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 这些关键元素之间的关系如下图所示。
- en: '![](../Images/1ca0f75abe4d63c821ad0bd80e50d223.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1ca0f75abe4d63c821ad0bd80e50d223.png)'
- en: '**The structure of an autoencoder** (Image by author)'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '**自编码器的结构**（图片作者提供）'
- en: The encoder (function ***f***) takes the input, ***x*** and transforms it into
    a latent vector denoted by ***z***. The decoder (function ***g***) takes ***z***
    as its input and recovers the input, ***x*** from the latent vector. The recovered
    input is approximately the same as ***x*** and therefore denoted as ***x̄***.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 编码器（函数***f***) 接收输入***x*** 并将其转换为潜在向量***z***。解码器（函数***g***) 接收***z*** 作为输入，并从潜在向量中恢复输入***x***。恢复的输入大致与***x***
    相同，因此表示为***x̄***。
- en: The latent vector can have the same or a higher dimension than the input (in
    case of an **overcomplete autoencoder**) or a much lower dimension than the input
    (in case of an **undercomplete autoencoder**).
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 潜在向量可以具有与输入相同或更高的维度（在**超完备自编码器**的情况下），也可以具有远低于输入的维度（在**欠完备自编码器**的情况下）。
- en: However, for image-denoising applications, the latent vector should be lower-dimensional
    than the input.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，对于图像去噪应用，潜在向量应该比输入的维度更低。
- en: Complex autoencoders have many hidden layers. They are known as **deep (multilayer)
    autoencoders**. In contrast, an autoencoder with a single hidden layer is called
    a **shallow (vanilla) autoencoder**.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 复杂的自编码器有许多隐藏层。它们被称为**深层（多层）自编码器**。相比之下，具有单一隐藏层的自编码器称为**浅层（原始）自编码器**。
- en: We always use deep autoencoders in practical applications including image denoising
    as shallow autoencoders are not capable enough to capture the important relationships
    in the data.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在实际应用中总是使用深层自编码器，包括图像去噪，因为浅层自编码器无法有效捕捉数据中的重要关系。
- en: An autoencoder can be created using dense and convolutional layers.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 自编码器可以使用密集层和卷积层来创建。
- en: When both the encoder and decoder of an autoencoder consist of only dense (fully-connected
    or MLP) layers, such an autoencoder is known as a **feedforward autoencoder**.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 当自编码器的编码器和解码器部分仅由密集（全连接或MLP）层组成时，这种自编码器被称为**前馈自编码器**。
- en: When the encoder of an autoencoder consists of convolutional layers (downsampling
    layers) and the decoder consists of transposed convolutional layers (upsampling
    layers), such an autoencoder is known as a **convolutional autoencoder**.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 当自编码器的编码器部分由卷积层（下采样层）组成，解码器部分由转置卷积层（上采样层）组成时，这种自编码器被称为**卷积自编码器**。
- en: Here, we will compare the outputs of both types.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将比较这两种类型的输出。
- en: How autoencoders remove noise from corrupted images
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自编码器如何从受损图像中去除噪声
- en: 'In image generation applications using autoencoders, we usually train the autoencoder
    model as:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用自编码器的图像生成应用中，我们通常将自编码器模型训练为：
- en: '[PRE1]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: This is like telling the model “learn the relationships using the training data
    to generate new images that are almost identical to the original ones”. Then,
    we can call `autoencoder.predict(test_images)`to generate new images using the
    model.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 这就像告诉模型“使用训练数据学习关系，以生成几乎与原始图像相同的新图像”。然后，我们可以调用`autoencoder.predict(test_images)`来使用模型生成新图像。
- en: 'In image-denoising applications using autoencoders, we usually train the autoencoder
    model as:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用自编码器的图像去噪应用中，我们通常将自编码器模型训练为：
- en: '[PRE2]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: This is like telling the model “learn the relationships using the noisy (corrupted)
    training data to remove the noise from the images of the same domain”. Then, we
    can call `autoencoder.predict(test_images)`to denoise new corrupted images of
    the same domain.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 这就像告诉模型“使用有噪声（受损）的训练数据学习关系，以去除相同领域图像中的噪声”。然后，我们可以调用`autoencoder.predict(test_images)`来去噪相同领域的新受损图像。
- en: The noisy (corrupted) data is created by adding some stochastic (random) noise
    to the original data. The noise follows a Gaussian (normal) distribution centered
    (mean) at 0 with a standard deviation of 0.7.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 有噪声（受损）的数据是通过向原始数据中添加一些随机（随机）噪声来创建的。噪声遵循以0为均值、标准差为0.7的高斯（正态）分布。
- en: '[PRE3]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: When we train a denoising autoencoder, the encoder part keeps the most important
    information and removes any unnecessary noise from the training data and creates
    the lower-dimensional latent (compressed) representation of the same data. Then,
    the decoder part recovers the clean images from that compressed representation.
    The trained model can be used to clean new corrupted images of the same domain
    — by author
  id: totrans-39
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 当我们训练去噪自编码器时，编码器部分保留了最重要的信息，去除训练数据中的任何不必要的噪声，并创建该数据的低维潜在（压缩）表示。然后，解码器部分从该压缩表示中恢复干净的图像。训练好的模型可以用于清理相同领域的新受损图像
    — 作者
- en: 'Building the models: Convolutional and feedforward autoencoders'
  id: totrans-40
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 建立模型：卷积和前馈自编码器
- en: The dataset we use
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们使用的数据集
- en: We will use the **MNIST dataset** (see [Citation](#54a3) at the end) to build
    the two autoencoder models here. This dataset comes preloaded with tf.keras.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用**MNIST数据集**（见文末的[引用](#54a3)）来构建这两个自编码器模型。这个数据集已经预加载到tf.keras中。
- en: To learn how to acquire and prepare the MNIST dataset for deep learning applications,
    read my article, [*Acquire, Understand and Prepare the MNIST Dataset*](https://rukshanpramoditha.medium.com/acquire-understand-and-prepare-the-mnist-dataset-3d71a84e07e7).
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 要学习如何获取和准备MNIST数据集以用于深度学习应用，请阅读我的文章，[*获取、理解和准备MNIST数据集*](https://rukshanpramoditha.medium.com/acquire-understand-and-prepare-the-mnist-dataset-3d71a84e07e7)。
- en: Approach
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 方法
- en: We will build a convolutional and a feedforward autoencoder using the MNIST
    data and compare the outputs of both models. During the training, both modes have
    the same loss function, optimizer, batch size and number of epochs for comparison
    purposes.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用MNIST数据构建一个卷积自编码器和一个前馈自编码器，并比较这两个模型的输出。在训练过程中，为了比较目的，这两个模型具有相同的损失函数、优化器、批量大小和轮数。
- en: However, the two architectures differ significantly as feedforward autoencoders
    have dense layers while convolutional autoencoders have convolutional and transposed
    convolutional layers.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这两种架构有显著差异，因为前馈自编码器具有密集层，而卷积自编码器具有卷积层和反卷积层。
- en: Training the models and making predictions
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练模型并进行预测
- en: After running the following code, you will get the trained autoencoder models
    with their training history and also the predictions made on new data.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 运行以下代码后，你将获得训练好的自编码器模型及其训练历史记录，同时也会得到在新数据上的预测结果。
- en: '[PRE4]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: It will take a long time to run this code if you use only the CPU. To speed
    up the process, you can use the [Colab free GPU](https://rukshanpramoditha.medium.com/how-to-use-google-colab-free-gpu-to-run-deep-learning-code-incredibly-faster-760604d26c7e)
    or a [GPU on your own laptop](https://rukshanpramoditha.medium.com/setting-up-a-deep-learning-workplace-with-an-nvidia-graphics-card-gpu-for-windows-os-b6bff06eeec7),
    if any.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 如果仅使用CPU运行此代码，将需要很长时间。为了加快速度，你可以使用[Colab 免费 GPU](https://rukshanpramoditha.medium.com/how-to-use-google-colab-free-gpu-to-run-deep-learning-code-incredibly-faster-760604d26c7e)或[自己笔记本上的
    GPU](https://rukshanpramoditha.medium.com/setting-up-a-deep-learning-workplace-with-an-nvidia-graphics-card-gpu-for-windows-os-b6bff06eeec7)，如果有的话。
- en: I’m not going to explain the code line by line as I have already discussed all
    these things in my previous articles.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 我不会逐行解释代码，因为我已经在之前的文章中讨论了所有这些内容。
- en: Plotting the learning curves
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 绘制学习曲线
- en: Now, we can plot the learning curves for both models to analyze the training
    performance of both networks. For this, we can use the *history* objects of both
    models.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以绘制两个模型的学习曲线，以分析这两个网络的训练表现。为此，我们可以使用两个模型的*history*对象。
- en: '[PRE5]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '![](../Images/ab23dceefcfdd30c129803dcc44810d7.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ab23dceefcfdd30c129803dcc44810d7.png)'
- en: '**The learning curves** (Image by author)'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '**学习曲线**（图片由作者提供）'
- en: '**Left plot:** Shows the training performance of the feedforward autoencoder.
    The model seems to *slightly* overfit after the 35th epoch.'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**左图：** 显示了前馈自编码器的训练性能。模型在第35轮之后似乎*轻微*过拟合。'
- en: '**Right plot:** Shows the training performance of the convolutional autoencoder.
    The model is in the just-right condition where the model is neither underfitting
    nor overfitting.'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**右图：** 显示了卷积自编码器的训练性能。模型处于刚刚好的状态，即模型既没有欠拟合也没有过拟合。'
- en: Visualizing the outputs
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 可视化输出
- en: Now, we compare the outputs of both models. For comparison purposes, I will
    also add the original images and their corrupted (noisy) counterparts.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们比较两个模型的输出。为了比较，我还将添加原始图像及其损坏（噪声）版本。
- en: Let’s see the output.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一下输出结果。
- en: '[PRE6]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '![](../Images/993cae4901542d9ceedccf24d7544efa.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/993cae4901542d9ceedccf24d7544efa.png)'
- en: (Image by author)
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: （图片由作者提供）
- en: '**1st row:** Represents the corrupted images after adding Gaussian noise centered
    (mean) at 0 with a standard deviation of 0.7\. We feed these images to autoencoders
    to get clean images.'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**第1行：** 代表在添加中心（均值）为0，标准差为0.7的高斯噪声后损坏的图像。我们将这些图像输入到自编码器中，以获得干净的图像。'
- en: '**2nd row:** Represents the cleaned (denoised/predicted) images by the feedforward
    autoencoder. The contents of images can be identified and close enough to the
    original images.'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**第2行：** 代表由前馈自编码器清理（去噪/预测）后的图像。图像的内容可以被识别，并且与原始图像非常接近。'
- en: '**3rd row:** Represents the cleaned (denoised/predicted) images by the convolutional
    autoencoder. The contents of images can be easily identified and are almost identical
    to the original ones. The reason is that the convolutional layers can maintain
    the spatial structure between nearby pixels of the image. This is not possible
    with dense layers.'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**第3行：** 代表由卷积自编码器清理（去噪/预测）后的图像。图像的内容可以很容易地识别，并且几乎与原始图像相同。原因是卷积层可以保持图像中相邻像素之间的空间结构。这在密集层中是不可能实现的。'
- en: '**4th row:** Represents the original images added for comparison purposes.'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**第4行：** 代表了为比较目的而添加的原始图像。'
- en: Conclusions
  id: totrans-69
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: Convolutional autoencoders outperform feedforward autoencoders in image denoising
    as the convolutional layers in convolutional autoencoders can maintain the spatial
    structure between nearby pixels of the images — by author
  id: totrans-70
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 卷积自编码器在图像去噪方面优于前馈自编码器，因为卷积自编码器中的卷积层可以保持图像中邻近像素之间的空间结构 — 作者
- en: This is the end of today’s article.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 今天的文章到此结束。
- en: '**Please let me know if you’ve any questions or feedback.**'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '**如有任何问题或反馈，请告诉我。**'
- en: How about an AI course?
  id: totrans-73
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何进行人工智能课程？
- en: '[![](../Images/25706837376c0842cfc45cb6237dd72d.png)](https://rukshanpramoditha.medium.com/list/neural-networks-and-deep-learning-course-a2779b9c3f75)'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '[![](../Images/25706837376c0842cfc45cb6237dd72d.png)](https://rukshanpramoditha.medium.com/list/neural-networks-and-deep-learning-course-a2779b9c3f75)'
- en: (Screenshot by author)
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: （截图由作者提供）
- en: Support me as a writer
  id: totrans-76
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 支持我作为作者
- en: '*I hope you enjoyed reading this article. If you’d like to support me as a
    writer, kindly consider* [***signing up for a membership***](https://rukshanpramoditha.medium.com/membership)
    *to get unlimited access to Medium. It only costs $5 per month and I will receive
    a portion of your membership fee.*'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '*我希望你喜欢阅读这篇文章。如果你愿意支持我作为作者，请考虑* [***注册会员***](https://rukshanpramoditha.medium.com/membership)
    *以获得对 Medium 的无限制访问。每月仅需 $5，我将从你的会员费用中获得一部分。*'
- en: '[](https://rukshanpramoditha.medium.com/membership?source=post_page-----2fe2e9aed71d--------------------------------)
    [## Join Medium with my referral link - Rukshan Pramoditha'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://rukshanpramoditha.medium.com/membership?source=post_page-----2fe2e9aed71d--------------------------------)
    [## 使用我的推荐链接加入 Medium - Rukshan Pramoditha'
- en: Read every story from Rukshan Pramoditha (and thousands of other writers on
    Medium). Your membership fee directly…
  id: totrans-79
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 阅读 Rukshan Pramoditha（以及 Medium 上成千上万其他作者）的每一篇故事。您的会员费用将直接...
- en: rukshanpramoditha.medium.com](https://rukshanpramoditha.medium.com/membership?source=post_page-----2fe2e9aed71d--------------------------------)
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: rukshanpramoditha.medium.com](https://rukshanpramoditha.medium.com/membership?source=post_page-----2fe2e9aed71d--------------------------------)
- en: Join my private list of emails
  id: totrans-81
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 加入我的私人邮件列表
- en: '*Never miss a great story from me again. By* [***subscribing to my email list***](https://rukshanpramoditha.medium.com/subscribe)*,
    you will directly receive my stories as soon as I publish them.*'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '*再也不会错过我精彩的故事了。通过* [***订阅我的邮件列表***](https://rukshanpramoditha.medium.com/subscribe)*，你将会在我发布故事时直接收到。*'
- en: Thank you so much for your continuous support! See you in the next article.
    Happy learning to everyone!
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 非常感谢你的持续支持！下篇文章见。祝大家学习愉快！
- en: MNIST dataset info
  id: totrans-84
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: MNIST 数据集信息
- en: '**Citation:** Deng, L., 2012\. The mnist database of handwritten digit images
    for machine learning research. **IEEE Signal Processing Magazine**, 29(6), pp.
    141–142.'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**引用：** Deng, L., 2012. 用于机器学习研究的手写数字图像的 MNIST 数据库。**IEEE 信号处理杂志**, 29(6),
    第 141–142 页。'
- en: '**Source:** [http://yann.lecun.com/exdb/mnist/](http://yann.lecun.com/exdb/mnist/)'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**来源：** [http://yann.lecun.com/exdb/mnist/](http://yann.lecun.com/exdb/mnist/)'
- en: '**License:** *Yann LeCun* (Courant Institute, NYU) and *Corinna Cortes* (Google
    Labs, New York) hold the copyright of the MNIST dataset which is available under
    the *Creative Commons Attribution-ShareAlike 4.0 International License* ([**CC
    BY-SA**](https://creativecommons.org/licenses/by-sa/4.0/)). You can learn more
    about different dataset license types [here](https://rukshanpramoditha.medium.com/dataset-and-software-license-types-you-need-to-consider-d20965ca43dc#6ade).'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**许可：** *Yann LeCun*（纽约大学 Courant 研究所）和 *Corinna Cortes*（谷歌实验室，纽约）持有 MNIST
    数据集的版权，该数据集在 *创意共享署名-相同方式共享 4.0 国际许可证* 下提供 ([**CC BY-SA**](https://creativecommons.org/licenses/by-sa/4.0/))。你可以在
    [这里](https://rukshanpramoditha.medium.com/dataset-and-software-license-types-you-need-to-consider-d20965ca43dc#6ade)
    了解更多关于不同数据集许可证类型的信息。'
- en: '[Rukshan Pramoditha](https://medium.com/u/f90a3bb1d400?source=post_page-----2fe2e9aed71d--------------------------------)'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '[Rukshan Pramoditha](https://medium.com/u/f90a3bb1d400?source=post_page-----2fe2e9aed71d--------------------------------)'
- en: '**2023–01–24**'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '**2023–01–24**'
