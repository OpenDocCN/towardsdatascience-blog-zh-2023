- en: 'Tiny Audio Diffusion: Waveform Diffusion That Doesn’t Require Cloud Computing'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*Tiny Audio Diffusion：无需云计算的波形扩散*'
- en: 原文：[https://towardsdatascience.com/tiny-audio-diffusion-ddc19e90af9b?source=collection_archive---------3-----------------------#2023-06-29](https://towardsdatascience.com/tiny-audio-diffusion-ddc19e90af9b?source=collection_archive---------3-----------------------#2023-06-29)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/tiny-audio-diffusion-ddc19e90af9b?source=collection_archive---------3-----------------------#2023-06-29](https://towardsdatascience.com/tiny-audio-diffusion-ddc19e90af9b?source=collection_archive---------3-----------------------#2023-06-29)
- en: '![](../Images/cb135c405e7b88027dfc7c37b2d82f30.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/cb135c405e7b88027dfc7c37b2d82f30.png)'
- en: Generated with Stable Diffusion
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 由 Stable Diffusion 生成
- en: Exploring how to train models and generate sounds with audio waveform diffusion
    on a consumer laptop and GPU with less than 2GB VRAM
  id: totrans-4
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 探索如何在消费者级笔记本电脑和 VRAM 小于 2GB 的 GPU 上训练模型并生成声音，通过音频波形扩散实现。
- en: '[](https://medium.com/@crlandschoot?source=post_page-----ddc19e90af9b--------------------------------)[![Christopher
    Landschoot](../Images/99a2569f5a6a3a99fd1f72553aa3d634.png)](https://medium.com/@crlandschoot?source=post_page-----ddc19e90af9b--------------------------------)[](https://towardsdatascience.com/?source=post_page-----ddc19e90af9b--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----ddc19e90af9b--------------------------------)
    [Christopher Landschoot](https://medium.com/@crlandschoot?source=post_page-----ddc19e90af9b--------------------------------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@crlandschoot?source=post_page-----ddc19e90af9b--------------------------------)[![Christopher
    Landschoot](../Images/99a2569f5a6a3a99fd1f72553aa3d634.png)](https://medium.com/@crlandschoot?source=post_page-----ddc19e90af9b--------------------------------)[](https://towardsdatascience.com/?source=post_page-----ddc19e90af9b--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----ddc19e90af9b--------------------------------)
    [Christopher Landschoot](https://medium.com/@crlandschoot?source=post_page-----ddc19e90af9b--------------------------------)'
- en: ·
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fb64548f914a5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftiny-audio-diffusion-ddc19e90af9b&user=Christopher+Landschoot&userId=b64548f914a5&source=post_page-b64548f914a5----ddc19e90af9b---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----ddc19e90af9b--------------------------------)
    ·11 min read·Jun 29, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fddc19e90af9b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftiny-audio-diffusion-ddc19e90af9b&user=Christopher+Landschoot&userId=b64548f914a5&source=-----ddc19e90af9b---------------------clap_footer-----------)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fb64548f914a5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftiny-audio-diffusion-ddc19e90af9b&user=Christopher+Landschoot&userId=b64548f914a5&source=post_page-b64548f914a5----ddc19e90af9b---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----ddc19e90af9b--------------------------------)
    · 11 分钟阅读 · 2023年6月29日 [](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fddc19e90af9b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftiny-audio-diffusion-ddc19e90af9b&user=Christopher+Landschoot&userId=b64548f914a5&source=-----ddc19e90af9b---------------------clap_footer-----------)'
- en: --
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fddc19e90af9b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftiny-audio-diffusion-ddc19e90af9b&source=-----ddc19e90af9b---------------------bookmark_footer-----------)'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fddc19e90af9b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftiny-audio-diffusion-ddc19e90af9b&source=-----ddc19e90af9b---------------------bookmark_footer-----------)'
- en: Background
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 背景
- en: Diffusion models are all the rage currently, especially since [Stable Diffusion](https://stability.ai/blog/stable-diffusion-public-release)
    took the world by storm this past summer. Since then, countless variations and
    new diffusion models have been published in a wide variety of contexts. And while
    the stunning visuals have stolen the spotlight, there has been significant development
    with diffusion related to generative audio.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 扩散模型目前非常流行，特别是自从 [Stable Diffusion](https://stability.ai/blog/stable-diffusion-public-release)
    在这个夏天掀起热潮之后。此后，已经发布了无数种扩散模型的变体和新模型，涵盖了各种不同的背景。虽然令人惊叹的视觉效果抢占了风头，但在生成音频方面也有了显著的进展。
- en: Fueled by diffusion and other methods, generative music has seen many recent
    triumphs, as new models are published all the time. OpenAI wowed the world with
    the capabilities of [Jukebox](https://openai.com/research/jukebox) when it was
    released in 2020\. But Google said “Hold my model” when it produced the remarkable
    [MusicLM](https://google-research.github.io/seanet/musiclm/examples/) at the beginning
    of this year. Meta was not far behind when they released and open-sourced [MusicGen](https://ai.honu.io/papers/musicgen/)
    this past month. But large institutions are not the only ones joining in, as there
    have been very interesting contributions from independent researchers such as
    [Riffusion](https://www.riffusion.com/) (Forsgren & Martiros) and [Moûsai](https://arxiv.org/abs/2301.11757)
    (Schneider, et al.). On top of these, numerous other models have been released
    within the last few years, all having their benefits and drawbacks.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在扩散和其他方法的推动下，生成音乐取得了许多最近的胜利，因为新的模型不断发布。OpenAI在2020年发布的[Jukebox](https://openai.com/research/jukebox)让世界惊叹不已。然而，当Google在今年年初推出了卓越的[MusicLM](https://google-research.github.io/seanet/musiclm/examples/)时，却让人们惊呼“让我来展示我的模型”。Meta也紧随其后，上个月发布并开源了[MusicGen](https://ai.honu.io/papers/musicgen/)。但大型机构并不是唯一参与者，独立研究者如[Riffusion](https://www.riffusion.com/)（Forsgren
    & Martiros）和[Moûsai](https://arxiv.org/abs/2301.11757)（Schneider等）也做出了非常有趣的贡献。除此之外，过去几年还发布了许多其他模型，每个模型都有其优缺点。
- en: Diffusion models have captivated so many due to their remarkable creative ability;
    something that many other genres of machine learning (ML) lack. Most ML models
    are trained to perform a task and their success can be measured by being correct
    vs incorrect. But when we enter the realm of art and music, how can a model be
    optimized to what might be considered best? It could of course learn to reproduce
    famous art or music, but without novelty, there is no point. So how can this problem
    be solved — to inject creativity into a machine that only knows 1s and 0s? Diffusion
    is one method that offers an elegant solution to this quandary.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 扩散模型由于其非凡的创造力吸引了众多关注；这是许多其他机器学习（ML）领域所缺乏的。大多数ML模型被训练来执行特定任务，其成功可以通过正确与错误来衡量。但当我们进入艺术和音乐的领域时，如何优化模型以达到可能被认为是最佳的效果？当然，它可以学习重现著名的艺术作品或音乐，但没有新颖性是没有意义的。那么如何解决这个问题——将创造力注入到只懂得1和0的机器中呢？扩散是一种为这个难题提供优雅解决方案的方法。
- en: Diffusion — From 10,000 feet
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 扩散 — 从10,000英尺的视角
- en: At its core, diffusion in ML is simply the process of adding or removing noise
    from a signal (think of static from an old TV). *Forward diffusion* adds noise
    to a signal and *reverse diffusion* removes noise. The process we are most familiar
    with is the reverse diffusion process, where the model takes in noise and then
    “denoises” it into something that humans recognize (art, music, speech, etc.).
    This process can be manipulated in a myriad of ways to serve different purposes.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 从本质上讲，ML中的扩散只是从信号中添加或移除噪声的过程（想象一下旧电视的静态噪声）。*正向扩散*向信号添加噪声，而*反向扩散*则移除噪声。我们最熟悉的过程是反向扩散过程，在这个过程中，模型接收噪声，然后将其“去噪”成人类能够识别的东西（艺术、音乐、语音等）。这个过程可以通过多种方式进行操作，以服务于不同的目的。
- en: The “creativity” in diffusion comes from the random noise that initiates the
    denoising process. If you are providing the model with a different starting point
    every time to denoise into some form of art or music, this simulates creativity
    as the outputs will always be unique.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 扩散中的“创造力”来自于启动去噪过程的随机噪声。如果你每次都为模型提供不同的起点，以去噪成某种形式的艺术或音乐，这就模拟了创造力，因为输出结果总是独一无二的。
- en: '![](../Images/d390fdb920543ffc647552be467be026.png)![](../Images/da2a9802ace0b07cb4c850129bfa1f1e.png)![](../Images/69ab8f107bc959b1047d3b3e17dc73a3.png)![](../Images/6cfa650cde2b0f07f713f37780def651.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d390fdb920543ffc647552be467be026.png)![](../Images/da2a9802ace0b07cb4c850129bfa1f1e.png)![](../Images/69ab8f107bc959b1047d3b3e17dc73a3.png)![](../Images/6cfa650cde2b0f07f713f37780def651.png)'
- en: Images Generated with Stable Diffusion
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 使用稳定扩散生成的图像
- en: The method of teaching a model to perform this denoising process may actually
    be a bit counter-intuitive from an initial thought. The model actually learns
    to denoise a signal by doing the exact opposite — adding noise to a clean signal
    over and over again until only noise remains. The idea is that if the model can
    learn how to predict the noise added to a signal at each step, then it can also
    predict the noise removed at each step for the reverse process. The critical element
    to make this possible is that the noise being added/removed needs to be of a defined
    probabilistic distribution (typically Gaussian) so that the noising/denoising
    steps are predictable and repeatable.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 教授模型执行这种去噪过程的方法从最初的想法来看可能有点反直觉。模型实际上通过做完全相反的事情来学习去噪——不断地将噪声添加到干净的信号中，直到只剩下噪声。其思路是，如果模型能够学习如何预测每一步中添加的噪声，那么它也可以预测每一步中去除的噪声以进行反向过程。使这一切成为可能的关键因素是，添加/去除的噪声需要具有定义好的概率分布（通常是高斯分布），以便去噪/去噪步骤是可预测和可重复的。
- en: There is far more detail that goes into this process, but this should provide
    a sound conceptual understanding of what is happening under the hood. If you are
    interested in learning more about diffusion models (mathematical formulations,
    scheduling, latent space, etc.), I recommend reading this [blog post](https://www.assemblyai.com/blog/diffusion-models-for-machine-learning-introduction/)
    by AssemblyAI and these papers ([DDPM](https://arxiv.org/abs/2006.11239), [Improving
    DDPM](https://arxiv.org/abs/2102.09672), [DDIM](https://arxiv.org/abs/2010.02502),
    [Stable Diffusion](https://arxiv.org/abs/2112.10752)).
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 这个过程涉及的细节远不止这些，但这应该能提供一个关于背后发生了什么的合理概念。如果你有兴趣了解更多关于扩散模型（数学公式、调度、潜在空间等）的内容，我建议阅读[AssemblyAI的这篇博客文章](https://www.assemblyai.com/blog/diffusion-models-for-machine-learning-introduction/)以及这些论文（[DDPM](https://arxiv.org/abs/2006.11239)，[Improving
    DDPM](https://arxiv.org/abs/2102.09672)，[DDIM](https://arxiv.org/abs/2010.02502)，[Stable
    Diffusion](https://arxiv.org/abs/2112.10752)）。
- en: Tiny Audio Diffusion
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 微小音频扩散
- en: Understanding Audio for Machine Learning
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解音频以进行机器学习
- en: My interest in diffusion stems from the potential that it has shown with generative
    audio. Traditionally, to train ML algorithms, audio was converted into a spectrogram,
    which is basically a heatmap of sound energy over time. This was because a spectrogram
    representation was similar to an image, which computers are exceptional at working
    with, and it was a significant reduction in data size compared to a raw waveform.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 我对扩散的兴趣源于它在生成音频方面展示的潜力。传统上，为了训练机器学习算法，音频会被转换成频谱图，这基本上是声音能量随时间变化的热图。这是因为频谱图表示与图像相似，计算机在处理图像方面表现出色，而且与原始波形相比，数据大小大大减少。
- en: '![](../Images/6658a340cb4dbb4904b280d3dd4bd78c.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6658a340cb4dbb4904b280d3dd4bd78c.png)'
- en: Example Spectrogram of a Vocalist
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 声乐的示例频谱图
- en: However, with this transformation come some tradeoffs, including a reduction
    of resolution and a loss of *phase* information. The phase of an audio signal
    represents the position of multiple waveforms relative to one another. This can
    be demonstrated in the difference between a sine and a cosine function. They represent
    the same exact signal regarding amplitude, the only difference is a 90° (π/2 radians)
    phase shift between the two. For a more in-depth explanation of phase, check out
    this [video](https://www.youtube.com/watch?v=0XdJTipTnjM&t=318s) by [Akash Murthy](https://www.youtube.com/@akashmurthy).
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这种转换带来了一些权衡，包括分辨率的降低和*相位*信息的丧失。音频信号的相位表示多个波形彼此之间的位置。这可以通过正弦函数和余弦函数之间的差异来展示。它们在幅度上表示完全相同的信号，唯一的区别是两者之间有一个90°（π/2弧度）的相位偏移。有关相位的更深入解释，请查看这个[视频](https://www.youtube.com/watch?v=0XdJTipTnjM&t=318s)由[Akash
    Murthy](https://www.youtube.com/@akashmurthy)提供。
- en: '![](../Images/a7b7cb84b9dd5d4946d07ff57a005a8c.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a7b7cb84b9dd5d4946d07ff57a005a8c.png)'
- en: 90° phase shift between *sin* and cos
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '*sin*与cos之间的90°相位偏移'
- en: Phase is a perpetually challenging concept to grasp, even for those who work
    in audio, but it plays a critical role in creating the timbral qualities of sound.
    Suffice it to say that it should not be discarded so easily. Phase information
    can also technically be represented in spectrogram form (the complex portion of
    the transform), just like magnitude. However, the result is noisy and visually
    appears random, making it challenging for a model to learn any useful information
    from it. Because of this drawback, there has been recent interest in refraining
    from transforming audio into spectrograms and rather leaving it as a raw waveform
    to train models. While this brings its own set of challenges, both the amplitude
    and phase information are contained within the single signal of a waveform, providing
    a model with a more holistic picture of sound to learn from.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 相位是一个始终具有挑战性的概念，即使对于从事音频工作的人也是如此，但它在创造声音的音色特质中发挥着关键作用。可以说，它不应该被如此轻易地丢弃。相位信息也可以像幅度一样在频谱图中表示（变换的复杂部分）。然而，结果是噪声较多，视觉上显得随机，使得模型很难从中学习到有用的信息。由于这一缺陷，最近有兴趣避免将音频转换为频谱图，而是将其保留为原始波形以训练模型。虽然这带来了自己的一系列挑战，但波形信号中包含了幅度和相位信息，为模型提供了一个更全面的声音学习视角。
- en: '![](../Images/a041d57a2a78ae32cc291324b9302d9b.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a041d57a2a78ae32cc291324b9302d9b.png)'
- en: Example Waveform of a Vocalist
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 主唱的波形示例
- en: This is a key piece of my interest in waveform diffusion, and it has shown promise
    in yielding high-quality results for generative audio. Waveforms, however, are
    very dense signals requiring a significant amount of data to represent the range
    of frequencies humans can hear. For example, the music industry standard sampling
    rate is 44.1kHz, which means that 44,100 samples are required to represent just
    1 second of mono audio. Now double that for stereo playback. Because of this,
    most waveform diffusion models (that don’t leverage latent diffusion or other
    compression methods) require high GPU capacity (usually at least 16GB+ VRAM) to
    store all of the information while being trained.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我对波形扩散兴趣的一个关键点，它在生成音频方面显示出了良好的前景。然而，波形是非常密集的信号，需要大量数据来表示人类可以听到的频率范围。例如，音乐行业标准采样率为44.1kHz，这意味着需要44,100个样本来表示1秒钟的单声道音频。现在双倍考虑立体声播放。因此，大多数波形扩散模型（那些不利用潜在扩散或其他压缩方法的）需要高性能的GPU（通常至少需要16GB+
    VRAM）来存储所有信息并进行训练。
- en: Motivation
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 动机
- en: Many people do not have access to high-powered, high-capacity GPUs, or do not
    want to pay the fee to rent cloud GPUs for personal projects. Finding myself in
    this position, but still wanting to explore waveform diffusion models, I decided
    to develop a waveform diffusion system that could run on my meager local hardware.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 许多人没有高性能、大容量的GPU，或者不愿意为个人项目支付租用云GPU的费用。发现自己处于这种情况，但仍然希望探索波形扩散模型，我决定开发一个可以在我微不足道的本地硬件上运行的波形扩散系统。
- en: Hardware Setup
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 硬件设置
- en: I was equipped with an HP Spectre laptop from 2017 with an 8th Gen i7 processor
    and GeForce MX150 graphics card with 2GB VRAM — not what you would call a powerhouse
    for training ML models. My goal was to be able to create a model that could train
    and produce high-quality (44.1kHz) stereo outputs on this system.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 我使用的是2017年的HP Spectre笔记本电脑，配备了第8代i7处理器和2GB VRAM的GeForce MX150显卡——这并不算是训练机器学习模型的强大设备。我的目标是能够在这个系统上创建一个能够训练并生成高质量（44.1kHz）立体声输出的模型。
- en: Model Architecture
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型架构
- en: I leveraged Archinet’s [audio-diffusion-pytorch](https://github.com/archinetai/audio-diffusion-pytorch)
    library to build this model — thank you to [Flavio Schneider](https://github.com/flavioschneider)
    for his help working with this library that he largely built.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 我利用了Archinet的[audio-diffusion-pytorch](https://github.com/archinetai/audio-diffusion-pytorch)库来构建这个模型——感谢[Flavio
    Schneider](https://github.com/flavioschneider)在处理这个他大部分构建的库时的帮助。
- en: Attention U-Net
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 注意力U-Net
- en: The base model architecture consists of a U-Net with attention blocks which
    is standard for modern diffusion models. A U-Net is a neural network that was
    originally developed for image (2D) segmentation but has been adapted to audio
    (1D) for our uses with waveform diffusion. The U-Net architecture gets its name
    from its U-shaped design.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 基础模型架构由一个带有注意力块的U-Net组成，这在现代扩散模型中是标准配置。U-Net是一个最初为图像（2D）分割开发的神经网络，但已经被改编为用于波形扩散的音频（1D）。U-Net架构因其U形设计而得名。
- en: '![](../Images/342f2c88bc2f14bea027a44026df8a50.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/342f2c88bc2f14bea027a44026df8a50.png)'
- en: 'U-Net (Source: [*U-Net: Convolutional Networks for Biomedical Image Segmentation
    (Ronneberger, et. al)*](https://arxiv.org/abs/1505.04597v1)*)*'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 'U-Net（来源：[*U-Net: Convolutional Networks for Biomedical Image Segmentation
    (Ronneberger, et. al)*](https://arxiv.org/abs/1505.04597v1)）*'
- en: Very similar to an autoencoder, consisting of an encoder and a decoder, a U-Net
    also contains skip connections at each level of the network. These skip connections
    are direct connections between corresponding layers of the encoder and decoder,
    facilitating the transfer of fine-grained details from the encoder to the decoder.
    The encoder is responsible for capturing the important features of the input signal,
    while the decoder is responsible for generating the new audio sample. The encoder
    gradually reduces the resolution of the input audio, extracting features at different
    levels of abstraction. The decoder then takes these features and upsamples them,
    gradually increasing the resolution to generate the final audio sample.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 与自编码器非常相似，U-Net由编码器和解码器组成，还包含每一层的跳跃连接。这些跳跃连接是编码器和解码器对应层之间的直接连接，有助于将细粒度的细节从编码器传递到解码器。编码器负责捕捉输入信号的重要特征，而解码器负责生成新的音频样本。编码器逐渐减少输入音频的分辨率，在不同抽象层次上提取特征。然后解码器利用这些特征进行上采样，逐渐提高分辨率，以生成最终的音频样本。
- en: '![](../Images/1711cafeee3a27e21db179a32bc624f7.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1711cafeee3a27e21db179a32bc624f7.png)'
- en: 'Attention U-Net (Source: [Attention U-Net: Learning Where to Look for the Pancreas
    (](https://arxiv.org/abs/1804.03999v3)[Oktay, et al.)](https://arxiv.org/search/cs?searchtype=author&query=Oktay%2C+O))'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '注意力U-Net（来源：[注意力U-Net: 学习如何查看胰腺](https://arxiv.org/abs/1804.03999v3)（[Oktay,
    et al.](https://arxiv.org/search/cs?searchtype=author&query=Oktay%2C+O)））'
- en: This U-Net also contains self-attention blocks at the lower levels which help
    maintain the temporal consistency of the output. It is critical for the audio
    to be downsampled sufficiently to maintain efficiency for sampling during the
    diffusion process as well as avoid overloading the attention blocks. The model
    leverages [V-Diffusion](https://arxiv.org/abs/2202.00512) which is a diffusion
    technique inspired by [DDIM](https://arxiv.org/abs/2010.02502v4) sampling.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 这个U-Net还在较低层次包含自注意力块，这有助于保持输出的时间一致性。音频需要充分降采样，以维持在扩散过程中的采样效率，同时避免过载注意力块。模型利用了[V-Diffusion](https://arxiv.org/abs/2202.00512)，这是一种受到[DDIM](https://arxiv.org/abs/2010.02502v4)采样启发的扩散技术。
- en: To avoid running out of GPU VRAM, the length of the data that the base model
    was to be trained on needed to be short. Because of this, I decided to train one-shot
    drum samples due to their inherently short context lengths. After many iterations,
    the base model length was determined to be 32,768 samples @ 44.1kHz in stereo,
    which results in approximately 0.75 seconds. This may seem particularly short,
    but it is plenty of time for most drum samples.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 为了避免GPU VRAM耗尽，基础模型的训练数据长度需要较短。因此，我决定训练一次性鼓样本，因为它们的上下文长度固有地较短。经过多次迭代，确定基础模型长度为32,768个样本
    @ 44.1kHz立体声，相当于约0.75秒。这可能看起来特别短，但对于大多数鼓样本来说已经足够了。
- en: Transforms
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 变换
- en: To downsample the audio enough for the attention blocks, several pre-processing
    transforms were attempted. The hope was that if the audio data could be downsampled
    without losing significant information prior to training the model, then the number
    of nodes (neurons) and layers could be maximized without increasing the GPU memory
    load.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 为了将音频降采样到足够适合注意力模块的程度，尝试了几种预处理变换。希望如果在训练模型之前能够在不丢失重要信息的情况下降采样音频数据，那么可以在不增加GPU内存负担的情况下最大化节点（神经元）和层数。
- en: The first transform attempted was a version of “patching”. Originally proposed
    for [images](https://arxiv.org/abs/2207.04316v1), this process was adapted to
    audio for our purposes. The input audio sample is grouped by sequential time steps
    into chunks that are then transposed into channels. This process could then be
    reversed at the output of the U-Net to un-chunk the audio back to its full length.
    The un-chunking process created aliasing issues, however, resulting in undesirable
    high frequency artifacts in the generated audio.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个尝试的变换是“补丁”版本。最初是为[图像](https://arxiv.org/abs/2207.04316v1)提出的，这一过程被调整为适用于我们的音频处理。输入音频样本按时间步长分组为块，然后转置为通道。该过程可以在U-Net的输出端逆转，将音频块恢复到其完整长度。然而，解块过程产生了别名问题，导致生成的音频中出现了不希望有的高频伪影。
- en: The second transform attempted, proposed by [Schneider](https://arxiv.org/abs/2301.13267),
    is called a “Learned Transform” which consists of single convolutional blocks
    with large kernel sizes and strides at the start and end of the U-Net. Multiple
    kernel sizes and strides were attempted (16, 32, 64) coupled with accompanying
    model variations to appropriately downsample the audio. Again, however, this resulted
    in aliasing issues in the generated audio, though not as prevalent as the patching
    transform.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 第二种尝试的变换，由[Schneider](https://arxiv.org/abs/2301.13267)提出，被称为“学习变换”，它由单个卷积块组成，具有较大的卷积核尺寸和步幅，位于U-Net的开始和结束处。尝试了多种卷积核尺寸和步幅（16、32、64），并配合相应的模型变体来适当地降采样音频。然而，这仍然导致了生成音频中的混叠问题，尽管比打补丁变换少一些。
- en: Because of this, I decided that the model architecture would need to be adjusted
    to accommodate the raw audio with no pre-processing transforms to produce sufficient
    quality outputs.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我决定需要调整模型架构，以适应未经预处理变换的原始音频，从而产生足够质量的输出。
- en: This required extending the number of layers within the U-Net to avoid downsampling
    too quickly and losing important features along the way. After multiple iterations,
    the best architecture resulted in downsampling by only 2 at each layer. While
    this required a reduction in the number of nodes per layer, it ultimately produced
    the best results. Detailed information about the exact number of U-Net levels,
    layers, nodes, attention features, etc. can be found in the [configuration file](https://github.com/crlandsc/tiny-audio-diffusion/blob/main/exp/drum_diffusion.yaml)
    in the [tiny-audio-diffusion](https://github.com/crlandsc/tiny-audio-diffusion)
    repository on GitHub.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 这需要扩展U-Net中的层数，以避免过快地降采样并丢失重要特征。在多次迭代后，最佳的架构仅在每层降采样2次。虽然这要求每层节点数量减少，但最终产生了最佳的结果。有关U-Net的确切层级、层数、节点、注意力特征等的详细信息，可以在GitHub上的[tiny-audio-diffusion](https://github.com/crlandsc/tiny-audio-diffusion)库中的[配置文件](https://github.com/crlandsc/tiny-audio-diffusion/blob/main/exp/drum_diffusion.yaml)中找到。
- en: '**Conclusion**'
  id: totrans-54
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**结论**'
- en: Pre-Trained Models
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 预训练模型
- en: I trained 4 separate unconditional models to produce kicks, snare drums, hi-hats,
    and percussion (all drum sounds). The datasets used for training were small free
    one-shot samples that I had collected for my music production workflows (all open-source).
    Larger, more varied datasets would improve the quality and diversity of each model’s
    generated outputs. The models were trained for a various number of steps and epochs
    depending on the size of each dataset.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 我训练了4个独立的无条件模型，以生成踢鼓、军鼓、踩镲和打击乐（所有鼓声）。用于训练的数据集是我为音乐制作工作流程收集的小型免费单次样本（均为开源）。更大、更具多样性的数据集将提高每个模型生成输出的质量和多样性。这些模型根据每个数据集的大小进行了不同数量的步骤和周期训练。
- en: Pre-trained models are available for download on [Hugging Face](https://huggingface.co/crlandsc).
    See the training progress and output samples logged at [Weights & Biases](https://wandb.ai/crlandsc/unconditional-drum-diffusion?workspace=user-crlandsc).
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 预训练模型可以在[Hugging Face](https://huggingface.co/crlandsc)上下载。请查看在[Weights & Biases](https://wandb.ai/crlandsc/unconditional-drum-diffusion?workspace=user-crlandsc)上记录的训练进度和输出样本。
- en: Results
  id: totrans-58
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结果
- en: Overall, the quality of the output is quite high in spite of the reduced size
    of the models. However, there is still some slight high frequency “hiss” remaining,
    which is likely due to the limited size of the model. This can be seen in the
    small amount of noise remaining in the waveforms below. Most samples generated
    are crisp, maintaining transients and broadband timbral characteristics. Sometimes
    the models add extra noise toward the end of the sample, and this is likely a
    cost of the limit of layers and nodes of the model.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，尽管模型的规模较小，但输出质量仍然很高。然而，仍然存在一些轻微的高频“嘶嘶声”，这可能是由于模型的规模有限。这可以从下面波形中残留的少量噪音中看到。大多数生成的样本都很清晰，保持了瞬态和宽带音色特征。有时模型在样本的末尾添加额外的噪音，这可能是模型层数和节点数量限制的代价。
- en: Listen to some output samples from the models [here](https://github.com/crlandsc/tiny-audio-diffusion/tree/main/samples).
    Example outputs from each model are shown below.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 可以[在这里](https://github.com/crlandsc/tiny-audio-diffusion/tree/main/samples)收听来自模型的一些输出样本。每个模型的示例输出如下。
- en: '![](../Images/a245362428366e2fbeed7762e42cd95f.png)![](../Images/4904fb3abcd46cc221d80e835efb218a.png)![](../Images/fc39d4bcbd5ee7c8edf2c314b011e0e5.png)![](../Images/91ce2a034476c2b4afcf0ef16e6c2abd.png)![](../Images/8fb1eb610bebfa2703540c9a8d89086d.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a245362428366e2fbeed7762e42cd95f.png)![](../Images/4904fb3abcd46cc221d80e835efb218a.png)![](../Images/fc39d4bcbd5ee7c8edf2c314b011e0e5.png)![](../Images/91ce2a034476c2b4afcf0ef16e6c2abd.png)![](../Images/8fb1eb610bebfa2703540c9a8d89086d.png)'
- en: Discussion
  id: totrans-62
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 讨论
- en: Along with exploring waveform diffusion models on my local hardware, an important
    goal for this project was to be able to share that same opportunity with others.
    I wanted to offer an easy entry point for those with limited resources that were
    looking to experiment with audio waveform diffusion. Because of this, I structured
    the project repository to offer step-by-step instructions on how to train or fine-tune
    your own models as well as generate new samples from the [Inference.ipynb](https://github.com/crlandsc/tiny-audio-diffusion/blob/main/Inference.ipynb)
    notebook.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 除了在本地硬件上探索波形扩散模型外，这个项目的重要目标之一是能够将这一机会分享给其他人。我希望为那些资源有限但希望尝试音频波形扩散的人提供一个简单的入口。因此，我将项目库结构化，提供了如何训练或微调自己的模型以及如何从[Inference.ipynb](https://github.com/crlandsc/tiny-audio-diffusion/blob/main/Inference.ipynb)笔记本生成新样本的逐步说明。
- en: In addition, I recorded a [Tutorial Video](https://youtu.be/m6Eh2srtTro) that
    walks through setting up an Anaconda environment and demonstrates ways to generate
    unique samples with the pre-trained models.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我录制了一个[Tutorial Video](https://youtu.be/m6Eh2srtTro)，展示了如何设置Anaconda环境，并演示了使用预训练模型生成独特样本的方法。
- en: It is an exciting time for generative audio, especially with diffusion. I have
    learned an immense amount through building this project and have further expanded
    my optimism about what is to come in audio AI. I hope that this project can be
    of use to others looking to explore the world of audio AI as well.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 这是生成音频特别是扩散领域的激动人心的时刻。我在构建这个项目的过程中学到了很多，也进一步扩展了我对音频AI未来的乐观态度。我希望这个项目能对那些想探索音频AI世界的人有所帮助。
- en: '*All images, unless otherwise noted, are by the author.*'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '*所有图片，除非另有说明，均由作者提供。*'
- en: '**tiny-audio-diffusion code found here:** [**https://github.com/crlandsc/tiny-
    audio-diffusion**](https://github.com/crlandsc/tiny-audio-diffusion)'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '**tiny-audio-diffusion 代码可以在这里找到：** [**https://github.com/crlandsc/tiny-audio-diffusion**](https://github.com/crlandsc/tiny-audio-diffusion)'
- en: 'Tutorial video on setting up your environment to generate samples with tiny-audio-diffusion:
    [https://youtu.be/m6Eh2srtTro](https://youtu.be/m6Eh2srtTro)'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 设置环境以使用tiny-audio-diffusion生成样本的教程视频：[https://youtu.be/m6Eh2srtTro](https://youtu.be/m6Eh2srtTro)
- en: I am an audio scientist with a focus on AI/ML and spatial audio as well as a
    lifelong musician. If you are interested in more audio AI applications, see my
    recent article on [Music Demixing](https://medium.com/rock-nheavy/the-music-demixing-ai-revolution-9d1528c6ef7c).
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 我是一名专注于AI/ML和空间音频的音频科学家，同时也是一名终身音乐家。如果你对更多音频AI应用感兴趣，请参阅我最近的文章：[音乐去混响](https://medium.com/rock-nheavy/the-music-demixing-ai-revolution-9d1528c6ef7c)。
- en: 'Find me on [LinkedIn](https://www.linkedin.com/in/christopher-landschoot/)
    & [GitHub](https://github.com/crlandsc) and keep up to date with my current work
    and research here: [www.chrislandschoot.com](https://www.chrislandschoot.com/)'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在[LinkedIn](https://www.linkedin.com/in/christopher-landschoot/)和[GitHub](https://github.com/crlandsc)上找到我，并通过这里的[www.chrislandschoot.com](https://www.chrislandschoot.com/)跟进我当前的工作和研究。
- en: Find my music on [Spotify](https://open.spotify.com/artist/2i6noWJnJQPXPsudoiJuMS?si=fvLOxUPqTAWKB894WXMz5Q),
    [Apple Music](https://music.apple.com/us/artist/after-august/259370281), [YouTube](https://www.youtube.com/AfterAugust),
    [SoundCloud](https://soundcloud.com/after-august), and other streaming platforms
    as [After August](https://www.instagram.com/the_after_august/).
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在[Spotify](https://open.spotify.com/artist/2i6noWJnJQPXPsudoiJuMS?si=fvLOxUPqTAWKB894WXMz5Q)、[Apple
    Music](https://music.apple.com/us/artist/after-august/259370281)、[YouTube](https://www.youtube.com/AfterAugust)、[SoundCloud](https://soundcloud.com/after-august)以及其他流媒体平台上以[After
    August](https://www.instagram.com/the_after_august/)的名字找到我的音乐。
