- en: Train ImageNet without Hyperparameters with Automatic Gradient Descent
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用自动梯度下降训练 ImageNet，无需超参数
- en: 原文：[https://towardsdatascience.com/train-imagenet-without-hyperparameters-with-automatic-gradient-descent-31df80a4d249?source=collection_archive---------10-----------------------#2023-04-19](https://towardsdatascience.com/train-imagenet-without-hyperparameters-with-automatic-gradient-descent-31df80a4d249?source=collection_archive---------10-----------------------#2023-04-19)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/train-imagenet-without-hyperparameters-with-automatic-gradient-descent-31df80a4d249?source=collection_archive---------10-----------------------#2023-04-19](https://towardsdatascience.com/train-imagenet-without-hyperparameters-with-automatic-gradient-descent-31df80a4d249?source=collection_archive---------10-----------------------#2023-04-19)
- en: Towards architecture-aware optimisation
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 迈向面向架构的优化
- en: '[](https://chrismingard.medium.com/?source=post_page-----31df80a4d249--------------------------------)[![Chris
    Mingard](../Images/e2fdddc95d7d79b22f7fc5df6418df23.png)](https://chrismingard.medium.com/?source=post_page-----31df80a4d249--------------------------------)[](https://towardsdatascience.com/?source=post_page-----31df80a4d249--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----31df80a4d249--------------------------------)
    [Chris Mingard](https://chrismingard.medium.com/?source=post_page-----31df80a4d249--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://chrismingard.medium.com/?source=post_page-----31df80a4d249--------------------------------)[![Chris
    Mingard](../Images/e2fdddc95d7d79b22f7fc5df6418df23.png)](https://chrismingard.medium.com/?source=post_page-----31df80a4d249--------------------------------)[](https://towardsdatascience.com/?source=post_page-----31df80a4d249--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----31df80a4d249--------------------------------)
    [Chris Mingard](https://chrismingard.medium.com/?source=post_page-----31df80a4d249--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F3befb9679b10&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftrain-imagenet-without-hyperparameters-with-automatic-gradient-descent-31df80a4d249&user=Chris+Mingard&userId=3befb9679b10&source=post_page-3befb9679b10----31df80a4d249---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----31df80a4d249--------------------------------)
    ·8 min read·Apr 19, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F31df80a4d249&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftrain-imagenet-without-hyperparameters-with-automatic-gradient-descent-31df80a4d249&user=Chris+Mingard&userId=3befb9679b10&source=-----31df80a4d249---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F3befb9679b10&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftrain-imagenet-without-hyperparameters-with-automatic-gradient-descent-31df80a4d249&user=Chris+Mingard&userId=3befb9679b10&source=post_page-3befb9679b10----31df80a4d249---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----31df80a4d249--------------------------------)
    · 8分钟阅读 · 2023年4月19日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F31df80a4d249&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftrain-imagenet-without-hyperparameters-with-automatic-gradient-descent-31df80a4d249&user=Chris+Mingard&userId=3befb9679b10&source=-----31df80a4d249---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F31df80a4d249&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftrain-imagenet-without-hyperparameters-with-automatic-gradient-descent-31df80a4d249&source=-----31df80a4d249---------------------bookmark_footer-----------)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F31df80a4d249&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftrain-imagenet-without-hyperparameters-with-automatic-gradient-descent-31df80a4d249&source=-----31df80a4d249---------------------bookmark_footer-----------)'
- en: '**TL;DR** We’ve derived an optimiser called automatic gradient descent (AGD)
    that can train ImageNet without hyperparameters. This removes the need for expensive
    and time-consuming learning rate tuning, selection of learning rate decay schedulers,
    etc. Our paper can be found [here](https://arxiv.org/pdf/2304.05187.pdf).'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '**TL;DR** 我们提出了一种名为自动梯度下降（AGD）的优化器，可以在无需超参数的情况下训练 ImageNet。这消除了对昂贵且耗时的学习率调整、学习率衰减调度器选择等的需求。我们的论文可以在[这里](https://arxiv.org/pdf/2304.05187.pdf)找到。'
- en: I worked on this project with [Jeremy Bernstein](http://jeremybernste.in), [Kevin
    Huang](https://kevinhuang8.github.io/), [Navid Azizan](https://azizan.mit.edu/)
    and [Yisong Yue](http://www.yisongyue.com/). See Jeremy’s [GitHub](https://github.com/jxbz/agd)
    for a clean Pytorch implementation, or my [GitHub](https://github.com/C1510/agd_exp)
    for an experimental version with more features. **Figure 1** summarises the differences
    between AGD, Adam, and SGD.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 我与[Jeremy Bernstein](http://jeremybernste.in)、[Kevin Huang](https://kevinhuang8.github.io/)、[Navid
    Azizan](https://azizan.mit.edu/) 和[Yisong Yue](http://www.yisongyue.com/)一起工作。请查看
    Jeremy 的[GitHub](https://github.com/jxbz/agd)以获取干净的 Pytorch 实现，或者查看我的[GitHub](https://github.com/C1510/agd_exp)以获取更多功能的实验版本。**图
    1** 总结了 AGD、Adam 和 SGD 之间的区别。
- en: '![](../Images/ba2c442e73772f75c07a7801950c3813.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ba2c442e73772f75c07a7801950c3813.png)'
- en: '**Figure 1** Solid lines show train accuracy and dotted lines show test accuracy.
    **Left:** In contrast to our method, Adam and SGD with default hyperparameters
    perform poorly on a deep fully connected network (FCN) on CIFAR-10\. **Middle:**
    A learning rate grid search for Adam and SGD. Our optimiser performs about as
    well as fully-tuned Adam and SGD. **Right:** AGD trains ImageNet to a respectable
    test accuracy.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 1** 实线表示训练准确度，虚线表示测试准确度。**左侧：** 与我们的方法相比，Adam 和使用默认超参数的 SGD 在 CIFAR-10
    的深度全连接网络（FCN）上表现较差。**中间：** Adam 和 SGD 的学习率网格搜索。我们的优化器表现得与完全调优的 Adam 和 SGD 相当。**右侧：**
    AGD 在 ImageNet 上训练达到了令人满意的测试准确度。'
- en: Motivation
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 动机
- en: Anyone who has trained a deep neural network has likely had to tune the learning
    rate. This is (1) to ensure training is maximally efficient and (2) because finding
    the right learning rate can significantly improve overall generalisation. This
    is also a huge pain.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 任何训练过深度神经网络的人都可能需要调整学习率。这是为了 (1) 确保训练的最大效率，以及 (2) 因为找到合适的学习率可以显著提高整体泛化能力。这也是一件非常麻烦的事。
- en: '![](../Images/668e19e3a719c1746e92a214915a2dc8.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/668e19e3a719c1746e92a214915a2dc8.png)'
- en: '**Figure 2** Why learning rates are important for optimisation. To maximise
    the speed of convergence, you want to find the *Goldilocks* *learning rate:* large,
    but not so large where the non-linear terms in the objective function kick you
    around.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 2** 为什么学习率对优化如此重要。为了最大化收敛速度，你需要找到*恰到好处的* *学习率：* 大，但又不至于让目标函数中的非线性项使你偏离轨道。'
- en: However, with SGD the optimal learning rate highly depends on the architecture
    being trained. Finding it often requires a costly grid search procedure, sweeping
    over many orders of magnitude. Furthermore, other hyperparameters, like momentum
    and learning rate decay schedulers, also need to be selected and tuned.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，对于 SGD，最佳学习率高度依赖于正在训练的架构。找到它通常需要代价高昂的网格搜索程序，覆盖多个数量级。此外，其他超参数，如动量和学习率衰减调度器，也需要选择和调整。
- en: We present an optimiser called automatic gradient descent (AGD) that **does
    not need a learning rate to train a wide range of architectures and datasets,
    scaling all the way up to ResNet-50 on ImageNet**. This removes the need for any
    hyperparameter tuning (as both the effective learning rate and learning rate decay
    drop out of the analysis), saving on compute costs and massively speeding up the
    process of training a model.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提出了一种叫做自动梯度下降（AGD）的优化器，它**不需要学习率来训练各种架构和数据集，甚至可以扩展到 ImageNet 上的 ResNet-50**。这消除了任何超参数调整的需求（因为有效学习率和学习率衰减从分析中排除），节省了计算成本，并大大加快了模型训练的过程。
- en: Why do we need hyperparameters anyway?
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 我们为什么需要超参数呢？
- en: 'A deep learning system is composed of lots of interrelated components: architecture,
    data, loss function and gradients. There is a structure in the way these components
    interact, but as of yet nobody has exactly nailed down this structure, so we’re
    left with lots of tuning (e.g. learning rate, initialisation, schedulers) to ensure
    rapid convergence, and avoid overfitting.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习系统由许多相互关联的组件组成：架构、数据、损失函数和梯度。这些组件的交互方式有一定的结构，但至今没有人完全确定这个结构，因此我们仍需进行大量调优（如学习率、初始化、调度器），以确保快速收敛，并避免过拟合。
- en: But characterising these interactions perfectly could remove all degrees of
    freedom in the optimisation process — which are currently taken care of by manual
    hyperparameter tuning. Second-order methods currently characterise the sensitivity
    of the objective to weight perturbations using the Hessian, and remove degrees
    of freedom that way — however, such methods can be computationally intensive and
    thus not practical for large models.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，完美地表征这些交互作用可能会移除优化过程中的所有自由度——这些自由度目前由手动超参数调整处理。二阶方法目前使用Hessian表征目标对权重扰动的敏感性，并以这种方式移除自由度——然而，这些方法可能计算量大，因此在大型模型中不实用。
- en: 'We derive AGD by characterising these interactions analytically:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过分析这些交互作用来推导AGD：
- en: We bound the change in the **output** of the neural network in terms of the
    change in weights, for given **data** and **architecture**.
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们在给定**数据**和**架构**的情况下，将神经网络的**输出**变化与权重变化联系起来。
- en: We relate the change in **objective** (the total loss over all inputs in a batch)to
    the change in the **output** of the neural network
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将**目标**（批次中所有输入的总损失）的变化与神经网络的**输出**变化联系起来。
- en: We combine these results in a so-called *majorise-minimise* approach. We **majorise**
    the objective — that is, we derive an upper bound on the objective that lies tangent
    to the objective. We can then minimise this upper bound, knowing that this will
    move us downhill. This is visualised in **Figure 3**, where the **red curve**
    shows the majorisation of the objective function, shown by the **blue curve**.
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将这些结果结合在一种所谓的*主次优化*方法中。我们**主次优化**目标——即，我们推导出一个与目标相切的目标上界。然后我们可以最小化这个上界，知道这样做会使我们向下移动。这在**图3**中得到了可视化，其中**红色曲线**显示了目标函数的主次优化，如**蓝色曲线**所示。
- en: '![](../Images/52c80105691c91ae5b11f748058a8bb0.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/52c80105691c91ae5b11f748058a8bb0.png)'
- en: '**Figure 3** The **left panel** shows the basic idea behind majorise-minimise
    — minimising the objective function (blue) is done by minimising a series of upper
    bounds, or majorisations, (red). The **right panel** shows how a change in weights
    induces a change in the function, which in turn induces a change in the loss on
    a single datapoint, which in turn induces a change in the objective. We bound
    *∆L* in terms of *∆W,* and use this to construct our majorisation.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '**图3** **左面板**展示了主次优化的基本思想——通过最小化一系列上界或主次优化（红色），来最小化目标函数（蓝色）。**右面板**展示了权重的变化如何引起函数的变化，这进而引起单个数据点上的损失变化，最终引起目标的变化。我们将*∆L*与*∆W*相关联，并利用它来构建我们的主次优化。'
- en: AGD in Pytorch
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Pytorch中的AGD
- en: In this section, we go through all the key parts of the algorithm. See **Appendix
    A** for a sketch derivation.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将逐步讲解算法的所有关键部分。有关推导的简略内容，请参见**附录A**。
- en: '**On parameterisation**'
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**关于参数化**'
- en: We use a parameterisation that differs slightly from the conventional PyTorch
    defaults. AGD can be derived without assuming this parameterisation, but using
    it simplifies the analysis. For a fully connected layer *l*, we use orthogonal
    initialisation, scaled such that the singular values have magnitude sqrt((input
    dimension of *l* )/(output dimension of *l* )).
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用的参数化方法与传统的PyTorch默认设置略有不同。虽然可以在不假设这种参数化的情况下推导AGD，但使用这种参数化可以简化分析。对于完全连接层*l*，我们使用正交初始化，并将其缩放，以使奇异值的大小为sqrt（*l*的输入维度/
    *l*的输出维度）。
- en: We use this normalisation because it has nice properties that PyTorch default
    parameterisation does not, including stability with width, resistance to blow-ups
    in the activations, and promotion of feature learning. This is similar to [Greg
    Yang and Edward Hu’s muP](https://arxiv.org/pdf/2203.03466.pdf).
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用这种归一化，因为它具有PyTorch默认参数化所没有的一些良好特性，包括宽度的稳定性、对激活值爆炸的抵抗力以及促进特征学习。这类似于[Greg
    Yang和Edward Hu的muP](https://arxiv.org/pdf/2203.03466.pdf)。
- en: On the update
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 关于更新
- en: The step can be broken into two separate parts. The first is the calculation
    of eta (η), the “automatic learning rate”, which scales the update at all layers.
    Eta has a logarithmic dependence on the gradient norm — when the gradients are
    small, eta is approximately linear (like standard optimisers) but when they are
    very large, the logarithm automatically performs a type of gradient clipping.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 这一步可以分成两个独立的部分。第一部分是计算eta（η），即“自动学习率”，它会缩放所有层的更新。Eta对梯度范数有对数依赖——当梯度较小时，eta大致线性（像标准优化器一样），但当梯度非常大时，对数会自动执行一种梯度裁剪。
- en: Each layer is updated using eta multiplied by the layer’s weight norm, multiplied
    by normalised gradients, and divided by depth. The division by depth is responsible
    for scaling with depth. It is interesting that gradient normalisation drops out
    of the analysis, as other optimisers like Adam incorporate similar ideas heuristically.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 每层的更新使用 η 乘以层的权重范数，再乘以标准化梯度，最后除以深度。除以深度的操作负责与深度的缩放。有趣的是，梯度标准化在分析中消失了，因为其他优化器（如
    Adam）以启发式方式结合了类似的思想。
- en: Experiments
  id: totrans-35
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实验
- en: The goal for these experiments was to test AGD’s ability to (1) converge across
    a wide range of architectures and datasets, and (2) achieve comparable test accuracy
    to tuned Adam and SGD.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 这些实验的目标是测试 AGD 的能力：(1) 在广泛的架构和数据集上收敛，以及 (2) 实现与调整过的 Adam 和 SGD 相当的测试准确率。
- en: '**Figure 4** shows the learning curves of four architectures, from a fully-connected
    network (FCN) to ResNet-50, on datasets from CIFAR-10 to ImageNet. We compare
    AGD, shown with **solid lines**, to a standard optimiser, shown with **dotted
    lines** (SGD for ImageNet and tuned Adam for the other three). The **top row**
    shows the train objective (loss) and automatic learning rate η. The **bottom row**
    shows the train and test accuracies. **Figure 5** compares AGD vs tuned Adam vs
    tuned SGD on an 8-layer FCN. We see very similar performance from all three algorithms,
    reaching near identical test accuracy.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '**图4** 显示了从全连接网络（FCN）到 ResNet-50 的四种架构在 CIFAR-10 到 ImageNet 数据集上的学习曲线。我们将 AGD（用**实线**表示）与标准优化器（用**虚线**表示，ImageNet
    上为 SGD，其他三种数据集上为调整过的 Adam）进行了比较。**第一行** 显示了训练目标（损失）和自动学习率 η。**第二行** 显示了训练和测试准确率。**图5**
    比较了 AGD、调整过的 Adam 和调整过的 SGD 在一个 8 层 FCN 上的表现。我们看到这三种算法的性能非常相似，测试准确率几乎一致。'
- en: '**Figure 6** shows that AGD trains FCNs over a wide range of depths (2 to 32)
    and widths (64 to 2048). **Figure 7** shows the dependence of AGD on batch size
    (from 32 to 4096), on a 4-layer FCN. It seems to converge to a good optimum no
    matter the batch size!'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '**图6** 显示 AGD 可以在广泛的深度（2 到 32 层）和宽度（64 到 2048）上训练 FCNs。**图7** 显示了 AGD 对批量大小（从
    32 到 4096）的依赖性，测试了一个 4 层的 FCN。无论批量大小如何，AGD 似乎都能收敛到一个良好的最优解！'
- en: '![](../Images/534dbe32fff91aee2214eb18cd02b822.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/534dbe32fff91aee2214eb18cd02b822.png)'
- en: '**Figure 4** AGD vs Adam on four architectures: a depth-16 FCN on CIFAR-10,
    ResNet-18 on CIFAR-10, VGG-16 on CIFAR-100 and ResNet-50 on ImageNet-1k. AGD keeps
    a reasonable pace with hyperparameter-tuned Adam (which required grid searching
    several orders of magnitude)! These **solid lines** denote AGD and the **dashed
    lines** Adam (except for ImageNet, where we use SGD instead). The **top row**
    shows the train objective (i.e. loss) and the value of the automatic learning
    rate η during training. The **bottom row** shows the train and test accuracies.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '**图4** AGD 与 Adam 在四种架构上的比较：CIFAR-10 上的深度 16 FCN、CIFAR-10 上的 ResNet-18、CIFAR-100
    上的 VGG-16 和 ImageNet-1k 上的 ResNet-50。AGD 在超参数调整的 Adam（需要在多个数量级上进行网格搜索）中保持了合理的速度！这些**实线**表示
    AGD，**虚线**表示 Adam（除了 ImageNet，我们使用了 SGD）。**第一行** 显示了训练目标（即损失）和训练期间自动学习率 η 的值。**第二行**
    显示了训练和测试准确率。'
- en: '![](../Images/bb2588cbadb361d1ffdeb7818064c7a4.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bb2588cbadb361d1ffdeb7818064c7a4.png)'
- en: '**Figure 5** AGD vs Adam vs SGD on a depth-8 FCN with mean square error loss.
    Adam and SGD have their learning rates tuned. On the **left,** we plot the train
    and test objective functions (i.e. the loss). The **middle** shows the train and
    test accuracies. The **right** shows the average, minimum and maximum changes
    to the weights during each epoch.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '**图5** AGD 与 Adam 和 SGD 在一个深度 8 的 FCN 上的比较，损失为均方误差。Adam 和 SGD 的学习率经过调整。在**左侧**，我们绘制了训练和测试目标函数（即损失）。**中间**
    显示了训练和测试准确率。**右侧** 显示了每个周期中权重的平均、最小和最大变化。'
- en: '![](../Images/1f2c06ad2176c8aefe07a99904bceb3f.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1f2c06ad2176c8aefe07a99904bceb3f.png)'
- en: '**Figure 6** AGD converges out of the box for a huge range of depths and widths.
    Smaller architectures lack the capacity to achieve low loss, but AGD still trains
    them!'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '**图6** AGD 能够在极大的深度和宽度范围内开箱即用地收敛。较小的架构由于能力不足，无法实现低损失，但 AGD 仍能训练它们！'
- en: '![](../Images/e250e88fb0ebd094fe8057b033123abc.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e250e88fb0ebd094fe8057b033123abc.png)'
- en: '**Figure 7** And just to check AGD doesn’t only work for batch size 128, here
    is a selection of batch sizes for a depth-4 FCN.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '**图7** 为了确认 AGD 不仅仅适用于批量大小 128，这里展示了一个深度 4 FCN 的各种批量大小。'
- en: Conclusion
  id: totrans-47
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: 'To summarise, here is an “architecture-aware” optimiser: automatic gradient
    descent (AGD), capable of training small systems like an FCN on CIFAR-10 to large-scale
    systems like ResNet-50 on ImageNet, at a range of batch sizes, **without the need
    for manual hyperparameter tuning.**'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，这里有一个“架构感知”优化器：自动梯度下降（AGD），能够在各种批量大小下训练从CIFAR-10上的FCN等小型系统到ImageNet上的ResNet-50等大型系统，**无需手动调整超参数**。
- en: While using AGD has not removed all hyperparameters from machine learning, those
    that remain — batch size and architecture — typically fall under into the “make
    them as large as possible to fill up time/compute budget”.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然使用AGD并没有消除机器学习中的所有超参数，但剩下的超参数——批量大小和架构——通常属于“尽可能大以填满时间/计算预算”的类别。
- en: However, there is lots still to be done. We do not explicitly take into account
    stochasticity introduced into the gradient due to batch size. We also haven’t
    looked into regularisation like weight decay. While we’ve done a little bit of
    work in adding support for affine parameters (in batch norm layers) and bias terms,
    we haven’t tested it extensively, nor is it as well justified by theory as the
    rest of the results here.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，仍有许多工作要做。我们没有明确考虑由于批量大小引入的梯度的随机性。我们也没有研究像权重衰减这样的正则化。虽然我们在增加对仿射参数（在批量归一化层中）和偏置项的支持方面做了一些工作，但我们尚未进行广泛测试，也没有像这里的其他结果那样得到理论上的充分证明。
- en: Perhaps most importantly, we still need to do the analysis required for transformers,
    and test AGD on NLP tasks. Preliminary experiments with GPT-2 on OpenWebText2
    indicate that AGD works here too!
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 也许最重要的是，我们仍需进行对变压器的分析，并在NLP任务上测试AGD。与GPT-2在OpenWebText2上的初步实验表明，AGD在这里也有效！
- en: Finally, check out Jeremy’s [GitHub](https://github.com/jxbz/agd) for a clean
    version, or my [GitHub](https://github.com/C1510/agd_exp) for a developmental
    version with support for bias terms and affine parameters, if you want to try
    AGD! We hope you will find it useful.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，可以查看Jeremy的[GitHub](https://github.com/jxbz/agd)以获取干净版本，或者我的[GitHub](https://github.com/C1510/agd_exp)以获取支持偏置项和仿射参数的开发版本，如果你想尝试AGD！我们希望你会觉得它有用。
- en: Appendix A
  id: totrans-53
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 附录A
- en: We will go through a sketch of the important steps of the proof here. This is
    designed for anyone who wants to see how the main ideas come together, without
    going through the full proof, found in our paper [here](https://arxiv.org/abs/2304.05187).
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在这里简要介绍证明的重要步骤。这是为了那些想要了解主要思想如何汇聚的人，而无需查看我们论文中的完整证明，论文可以在[这里](https://arxiv.org/abs/2304.05187)找到。
- en: '![](../Images/bcc3225ca9db5c592c414009ba850e00.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bcc3225ca9db5c592c414009ba850e00.png)'
- en: Equation (1) specifies explicitly how the overall objective across the dataset
    *S* is decomposed into individual datapoints. *L* denotes loss, *x* the inputs,
    *y* the targets and *w* the weights. Equation (2) shows a decomposition of the
    linearisation error of the objective — the contributions of higher order terms
    given some change in weights, *Δw*,to the change in loss *ΔL(w)*. The linearisation
    error of the objective is important because it is equal to the contributions of
    higher order terms in the loss expanded at weights *w* — bounding this will tell
    us how far we can move before the higher order terms become important, and make
    sure we’re taking steps of sensible size, downhill.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 方程（1）明确规定了如何将数据集*S*上的总体目标分解为单个数据点。*L*表示损失，*x*为输入，*y*为目标，*w*为权重。方程（2）展示了目标的线性化误差的分解——在给定某些权重变化*Δw*时，高阶项对损失变化*ΔL(w)*的贡献。目标的线性化误差很重要，因为它等于在权重*w*处扩展的损失中高阶项的贡献——界定这一点将告诉我们可以移动多远，直到高阶项变得重要，并确保我们迈出的步伐是合理的、向下的。
- en: The first term on the RHS of Equation (2) is an inner product between two high-dimensional
    vectors, the linearisation error of the model, and the derivative of the loss
    with respect to *f(x)*. Since there is no clear reason why these two vectors should
    be aligned, we assume that their inner product is zero.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 方程（2）右侧的第一个项是两个高维向量的内积，即模型的线性化误差和关于*f(x)*的损失的导数。由于没有明确的理由说明这两个向量应该对齐，我们假设它们的内积为零。
- en: 'Adding *L(W+ΔW)* to each side of Equation (2), and noting that the linearisation
    error of the loss happens to be a Bregman divergence, we can simplify the notation:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 将*L(W+ΔW)*添加到方程（2）的两边，并注意到损失的线性化误差恰好是Bregman散度，我们可以简化符号：
- en: '![](../Images/a1909590e2acdd2cefaff88d476c4029.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a1909590e2acdd2cefaff88d476c4029.png)'
- en: A Bregman divergence is a measure of distance between two points (in this case,
    the outputs of two different parameter choices of a neural network), defined in
    terms of a strictly convex function — in this case, the loss.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: Bregman 散度是衡量两点之间距离的度量（在这种情况下是神经网络的两个不同参数选择的输出），其定义基于严格凸函数——在这里是损失函数。
- en: Calculating the Bregman divergence is actually quite straightforward for mean
    square error loss, and gives us
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 计算 Bregman 散度对于均方误差损失实际上非常简单，并给出了
- en: '![](../Images/a679271b0aa22b2c3c6e5a4a239ef696.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a679271b0aa22b2c3c6e5a4a239ef696.png)'
- en: Where dₗ is the output dimension of the network. We now assert the following
    scalings. All of these are somewhat arbitrary, but having them in this form will
    make the analysis much simpler.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 dₗ 是网络的输出维度。我们现在断言以下缩放。这些缩放有些任意，但将它们以这种形式表示将使分析变得更加简单。
- en: '![](../Images/c504076bba31b52a519b0521f722c85a.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c504076bba31b52a519b0521f722c85a.png)'
- en: We use the following two bounds on the size of the network output. Equation
    (5) bounds the magnitude of the network output, and comes from just applying the
    (input scaling) and (weight scaling) to a fully-connected network. Equation (6)
    bounds the maximum change in f(x) with a change in the weights W. The second inequality
    in (6) is tightest at large depth but holds for any depth.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用了对网络输出大小的以下两个界限。方程 (5) 对网络输出的幅度进行界限，这来自于对全连接网络应用（输入缩放）和（权重缩放）。方程 (6) 对权重
    W 的变化带来的 f(x) 的最大变化进行界限。方程 (6) 中的第二个不等式在大深度时最紧，但适用于任何深度。
- en: '![](../Images/0aab302a67de0bcde4e09cff1d47633f.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0aab302a67de0bcde4e09cff1d47633f.png)'
- en: Now, we substitute Equation (6) back into Equation (4), and expand all terms
    out explicitly to get Equation (7).
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将方程 (6) 代回方程 (4) 中，并将所有项展开以得到方程 (7)。
- en: '![](../Images/09eb87e0c958ceffc6d58ac21e8d3d62.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/09eb87e0c958ceffc6d58ac21e8d3d62.png)'
- en: We can substitute the sum in Equation (7) with G, defined in Equation (8) under
    an additional assumption about the gradient conditioning, which is discussed in
    detail in the paper. Finally, we get Equation (9) — this is the **majorisation**
    — the red line in **Figure 3**. We **minimize** the majorisation by differentiating
    with respect to η, and solve the resulting quadratic in exp(η), retaining the
    positive solution. This gives the following update
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将方程 (7) 中的总和替换为 G，这在方程 (8) 中定义，并在关于梯度条件的附加假设下讨论，详细内容请参阅论文。最后，我们得到方程 (9)——这就是**主化**——**图
    3**中的红线。我们通过对 η 求导来**最小化**主化，并求解结果中的二次方程，保留正解。这给出了以下更新
- en: '![](../Images/adaeada32696e409b4ceb79035a1e515.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/adaeada32696e409b4ceb79035a1e515.png)'
- en: And this concludes our derivation of automatic gradient descent. Please let
    us know if you have any comments, questions or other kinds of feedback.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 这就结束了我们对自动梯度下降的推导。如果你有任何评论、问题或其他反馈，请告知我们。
- en: All images in the blog are made by the authors of our [paper](https://arxiv.org/pdf/2304.05187.pdf).
    Figure 2 inspired by this [diagram](https://www.jeremyjordan.me/nn-learning-rate/).
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 博客中的所有图像均由我们[论文](https://arxiv.org/pdf/2304.05187.pdf)的作者制作。图 2 的灵感来源于这个[图表](https://www.jeremyjordan.me/nn-learning-rate/)。
