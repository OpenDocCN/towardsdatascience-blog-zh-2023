- en: Which Quantization Method is Right for You? (GPTQ vs. GGUF vs. AWQ)
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 哪种量化方法适合你？（GPTQ vs. GGUF vs. AWQ）
- en: 原文：[https://towardsdatascience.com/which-quantization-method-is-right-for-you-gptq-vs-gguf-vs-awq-c4cd9d77d5be](https://towardsdatascience.com/which-quantization-method-is-right-for-you-gptq-vs-gguf-vs-awq-c4cd9d77d5be)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/which-quantization-method-is-right-for-you-gptq-vs-gguf-vs-awq-c4cd9d77d5be](https://towardsdatascience.com/which-quantization-method-is-right-for-you-gptq-vs-gguf-vs-awq-c4cd9d77d5be)
- en: Exploring Pre-Quantized Large Language Models
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 探索预量化的大型语言模型
- en: '[](https://medium.com/@maartengrootendorst?source=post_page-----c4cd9d77d5be--------------------------------)[![Maarten
    Grootendorst](../Images/58e24b9cf7e10ff1cd5ffd75a32d1a26.png)](https://medium.com/@maartengrootendorst?source=post_page-----c4cd9d77d5be--------------------------------)[](https://towardsdatascience.com/?source=post_page-----c4cd9d77d5be--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----c4cd9d77d5be--------------------------------)
    [Maarten Grootendorst](https://medium.com/@maartengrootendorst?source=post_page-----c4cd9d77d5be--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@maartengrootendorst?source=post_page-----c4cd9d77d5be--------------------------------)[![Maarten
    Grootendorst](../Images/58e24b9cf7e10ff1cd5ffd75a32d1a26.png)](https://medium.com/@maartengrootendorst?source=post_page-----c4cd9d77d5be--------------------------------)[](https://towardsdatascience.com/?source=post_page-----c4cd9d77d5be--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----c4cd9d77d5be--------------------------------)
    [Maarten Grootendorst](https://medium.com/@maartengrootendorst?source=post_page-----c4cd9d77d5be--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----c4cd9d77d5be--------------------------------)
    ·11 min read·Nov 14, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----c4cd9d77d5be--------------------------------)
    ·11 分钟阅读·2023年11月14日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/c45770a07b8d0685c7ab123f15862789.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c45770a07b8d0685c7ab123f15862789.png)'
- en: Throughout the last year, we have seen the Wild West of Large Language Models
    (LLMs). The pace at which new technology and models were released was astounding!
    As a result, we have many different standards and ways of working with LLMs.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去一年里，我们见证了大型语言模型 (LLMs) 的狂野西部。新技术和模型的发布速度惊人！因此，我们有了许多不同的标准和处理 LLM 的方法。
- en: In this article, we will explore one such topic, namely loading your local LLM
    through several (quantization) standards. With sharding, quantization, and different
    saving and compression strategies, it is not easy to know which method is suitable
    for you.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我们将探讨一个话题，即通过几种（量化）标准加载你的本地 LLM。由于分片、量化以及不同的保存和压缩策略，很难知道哪种方法适合你。
- en: Throughout the examples, we will use [Zephyr 7B](https://huggingface.co/HuggingFaceH4/zephyr-7b-beta),
    a fine-tuned variant of Mistral 7B that was trained with [Direct Preference Optimization](https://arxiv.org/abs/2305.18290)
    (DPO).
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些例子中，我们将使用 [Zephyr 7B](https://huggingface.co/HuggingFaceH4/zephyr-7b-beta)，这是
    Mistral 7B 的一个微调变体，经过 [Direct Preference Optimization](https://arxiv.org/abs/2305.18290)
    (DPO) 训练。
- en: '🔥 **TIP**: After each example of loading an LLM, it is advised to restart your
    notebook to prevent OutOfMemory errors. Loading multiple LLMs requires significant
    RAM/VRAM. You can reset memory by deleting the models and resetting your cache
    like so:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 🔥 **提示**：在每个加载 LLM 的示例后，建议重启你的笔记本，以防止 OutOfMemory 错误。加载多个 LLM 需要大量 RAM/VRAM。你可以通过删除模型并重置缓存来重置内存：
- en: '[PRE0]'
  id: totrans-11
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: You can also follow along with the [**Google Colab Notebook**](https://colab.research.google.com/drive/1rt318Ew-5dDw21YZx2zK2vnxbsuDAchH?usp=sharing)
    to make sure everything works as intended.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 你还可以参考 [**Google Colab Notebook**](https://colab.research.google.com/drive/1rt318Ew-5dDw21YZx2zK2vnxbsuDAchH?usp=sharing)
    确保一切按预期工作。
- en: '**UPDATE**: I uploaded a video version to YouTube that goes more in-depth into
    how to use these quantization methods:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '**更新**：我上传了一个视频版本到 YouTube，深入探讨了如何使用这些量化方法：'
- en: 1\. HuggingFace
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 1\. HuggingFace
- en: The most straightforward, and vanilla, way of loading your LLM is through 🤗
    [Transformers](https://github.com/huggingface/transformers). HuggingFace has created
    a large suite of packages that allow us to do amazing things with LLMs!
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 加载你的 LLM 的最直接、最基本的方法是通过 🤗 [Transformers](https://github.com/huggingface/transformers)。HuggingFace
    创建了一套大型软件包，使我们能够对 LLM 进行惊人的操作！
- en: 'We will start by installing HuggingFace, among others, from its main branch
    to support newer models:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从安装 HuggingFace 开始，从其主分支中支持更新的模型：
- en: '[PRE1]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'After installation, we can use the following pipeline to easily load our LLM:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 安装后，我们可以使用以下管道轻松加载我们的 LLM：
- en: '[PRE2]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: This method of loading an LLM generally does not perform any compression tricks
    for saving VRAM or increasing efficiency.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 这种加载LLM的方法通常不会进行任何压缩技巧来节省VRAM或提高效率。
- en: 'To generate our prompt, we first have to create the necessary template. Fortunately,
    this can be done automatically if the chat template is saved in the underlying
    tokenizer:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 为了生成我们的提示，我们首先需要创建必要的模板。幸运的是，如果聊天模板保存在底层分词器中，这可以自动完成：
- en: '[PRE3]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The generated prompt, using the internal prompt template, is constructed like
    so:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 使用内部提示模板生成的提示构造如下：
- en: '![](../Images/1d2c11cd398ec50c0332607ab7df1fe8.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1d2c11cd398ec50c0332607ab7df1fe8.png)'
- en: The prompt template is automatically generated with the internal prompt template.
    Notice that there are different tags for differentiating between the user and
    the assistant.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 提示模板是使用内部提示模板自动生成的。注意到有不同的标签来区分用户和助手。
- en: 'Then, we can start passing the prompt to the LLM to generate our answer:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以开始将提示传递给LLM来生成我们的答案：
- en: '[PRE4]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'This gives us the following output:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 这给我们以下输出：
- en: Why did the Large Language Model go to the party?
  id: totrans-29
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 为什么大型语言模型去参加派对？
- en: ''
  id: totrans-30
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: To network and expand its vocabulary!
  id: totrans-31
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 为了网络和扩展其词汇量！
- en: The punchline may be a bit cheesy, but Large Language Models are all about expanding
    their vocabulary and networking with other models to improve their language skills.
    So, this joke is a perfect fit for them!
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 笑点可能有点老套，但大型语言模型的核心就是扩展词汇量和与其他模型建立联系以提高语言技能。因此，这个笑话非常适合它们！
- en: For pure inference, this method is generally the least efficient as we are loading
    the entire model without any compression or quantization strategies.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 对于纯推理来说，这种方法通常是效率最低的，因为我们在没有任何压缩或量化策略的情况下加载整个模型。
- en: It is, however, a great method to start with as it allows for easy loading and
    using the model!
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这是一个很好的起点方法，因为它便于加载和使用模型！
- en: 2\. Sharding
  id: totrans-35
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 2\. 分片
- en: Before we go into quantization strategies, there is another trick that we can
    employ to reduce the necessary VRAM for loading our model. With **sharding**,
    we are essentially splitting our model up into small pieces or **shards**.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们深入量化策略之前，还有另一个技巧可以用来减少加载模型所需的VRAM。通过**分片**，我们实际上是将模型拆分成小块或**分片**。
- en: '![](../Images/30e509c8abeecfb477aadd3c4ddc2e67.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/30e509c8abeecfb477aadd3c4ddc2e67.png)'
- en: Sharding an LLM is nothing more than breaking it up into pieces. Each individual
    piece is much easier to handle and might prevent memory issues.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 分片LLM不过是将其拆分成多个小块。每个小块更容易处理，可能避免内存问题。
- en: Each shard contains a smaller part of the model and aims to work around GPU
    memory limitations by distributing the model weights across different devices.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 每个分片包含模型的一小部分，旨在通过将模型权重分布到不同设备上来绕过GPU内存限制。
- en: Remember when I said we did not perform any compression tricks before?
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 记得我之前说过我们没有进行任何压缩技巧吗？
- en: That was not entirely true…
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 这并不完全正确……
- en: The model that we loaded, [Zephyr-7B-β](https://huggingface.co/HuggingFaceH4/zephyr-7b-beta),
    was actually already sharded for us! If you go to the model and click the *“Files
    and versions”* link, you will see that the model was split up into eight pieces.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 我们加载的模型，[Zephyr-7B-β](https://huggingface.co/HuggingFaceH4/zephyr-7b-beta)，实际上已经为我们进行了分片！如果你访问模型并点击*“Files
    and versions”*链接，你会看到模型被拆分成了八个部分。
- en: '![](../Images/ff7c4f272cf8764f683f5ede5f405b6f.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ff7c4f272cf8764f683f5ede5f405b6f.png)'
- en: The model was split up into eight small pieces or shards. This decreases the
    necessary VRAM as we only need to handle these small pieces.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 模型被拆分成了八个小块或分片。这减少了必要的VRAM，因为我们只需处理这些小块。
- en: Although we can shard a model ourselves, it is generally advised to be on the
    lookout for quantized models or even quantize them yourself.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们可以自己对模型进行分片，但通常建议寻找量化模型或自己进行量化。
- en: 'Sharding is quite straightforward using the [Accelerate](https://github.com/huggingface/accelerate)
    package:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 使用[Accelerate](https://github.com/huggingface/accelerate)包进行分片非常简单：
- en: '[PRE5]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'And that is it! Because we sharded the model into pieces of 4GB instead of
    2GB, we created fewer files to load:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 就这样！因为我们将模型分片成了4GB而不是2GB，所以我们创建了更少的文件来加载：
- en: '![](../Images/9f7666879f66be9ae9b7ca0e69885d74.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9f7666879f66be9ae9b7ca0e69885d74.png)'
- en: 3\. Quantize with Bitsandbytes
  id: totrans-50
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 3\. 使用Bitsandbytes进行量化
- en: A Large Language Model is represented by a bunch of weights and activations.
    These values are generally represented by the usual 32-bit floating point (`float32`)
    datatype.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型由一堆权重和激活值表示。这些值通常由常见的32位浮点数（`float32`）数据类型表示。
- en: The number of bits tells you something about how many values it can represent.
    **Float32** can represent values between 1.18e-38 and 3.4e38, quite a number of
    values! The lower the number of bits, the fewer values it can represent.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 位数告诉你它可以表示多少个值。**Float32**可以表示从1.18e-38到3.4e38之间的值，数量相当多！位数越少，能表示的值就越少。
- en: '![](../Images/69c7bf89aa06ada55340e1fb34e9d73f.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/69c7bf89aa06ada55340e1fb34e9d73f.png)'
- en: Common value representation methods. We aim to keep the number of bits as low
    as possible whilst maximizing both the range and precision of the representation.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 常见的值表示方法。我们旨在将位数保持在尽可能低的水平，同时最大化表示的范围和精度。
- en: As you might expect, if we choose a lower bit size, then the model becomes less
    accurate but it also needs to represent fewer values, thereby decreasing its size
    and memory requirements.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你可能预期的那样，如果我们选择较低的位大小，模型的准确性会降低，但它也需要表示更少的值，从而减小了模型的大小和内存需求。
- en: '![](../Images/e8fc103eac0e59d74fbd651678838516.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e8fc103eac0e59d74fbd651678838516.png)'
- en: A different representation method might negatively affect the precision with
    which to represent value. To the extent that some values are not even represented
    (too large values for float16 for example). Examples were calculated with PyTorch.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 不同的表示方法可能会对表示值的精度产生负面影响。以至于某些值甚至无法表示（例如，对于float16来说过大的值）。示例是使用PyTorch计算的。
- en: Quantization refers to converting an LLM from its original **Float32** representation
    to something smaller. However, we do not simply want to use a smaller bit variant
    but map a larger bit representation to a smaller bit without losing too much information.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 量化是指将大型语言模型从其原始的**Float32**表示转换为较小的表示。然而，我们不仅仅是使用较小的位变体，而是将较大的位表示映射到较小的位而不丢失太多信息。
- en: 'In practice, we see this often done with a new format, named **4bit-NormalFloat
    (NF4)**. This datatype does a few special tricks in order to efficiently represent
    a larger bit datatype. It consists of three steps:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，我们通常使用一种新的格式，称为**4bit-NormalFloat (NF4)**。这种数据类型通过一些特殊技巧有效地表示较大的位数据类型。它包括三个步骤：
- en: '**Normalization**: The weights of the model are normalized so that we expect
    the weights to fall within a certain range. This allows for more efficient representation
    of more common values.'
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**归一化**：模型的权重被归一化，以便我们期望权重落在一定范围内。这允许对更常见的值进行更高效的表示。'
- en: '**Quantization**: The weights are quantized to 4-bit. In NF4, the quantization
    levels are evenly spaced with respect to the normalized weights, thereby efficiently
    representing the original 32-bit weights.'
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**量化**：权重被量化为4位。在NF4中，量化水平相对于归一化的权重均匀分布，从而有效地表示原始的32位权重。'
- en: '**Dequantization**: Although the weights are stored in 4-bit, they are dequantized
    during computation which gives a performance boost during inference.'
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**去量化**：虽然权重以4位存储，但在计算过程中会进行去量化，这在推理过程中提升了性能。'
- en: 'To perform this quantization with HuggingFace, we need to define a configuration
    for the quantization with [Bitsandbytes](https://github.com/TimDettmers/bitsandbytes):'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用HuggingFace执行这种量化，我们需要定义一个量化配置，使用[Bitsandbytes](https://github.com/TimDettmers/bitsandbytes)：
- en: '[PRE6]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: This configuration allows us to specify which quantization levels we are going
    for. Generally, we want to represent the weights with 4-bit quantization but do
    the inference in 16-bit.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 这个配置允许我们指定我们要使用的量化水平。通常，我们希望使用4位量化来表示权重，但在推理时使用16位。
- en: 'Loading the model in a pipeline is then straightforward:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，加载模型到管道中是直接的：
- en: '[PRE7]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Next up, we can use the same prompt as we did before:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们可以使用之前的相同提示：
- en: '[PRE8]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'This will give us the following output:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 这将给我们如下输出：
- en: Why did the Large Language Model go to the party?
  id: totrans-71
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 大型语言模型为什么去参加派对？
- en: ''
  id: totrans-72
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: To network and expand its vocabulary!
  id: totrans-73
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 为了联网并扩展其词汇量！
- en: ''
  id: totrans-74
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The punchline may be a bit cheesy, but Large Language Models are all about expanding
    their vocabulary and networking with other models to improve their language skills.
    So, this joke is a perfect fit for them!
  id: totrans-75
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 这个笑话可能有点老套，但大型语言模型的核心就是扩展词汇量，并与其他模型建立联系以提高语言技能。所以，这个笑话非常适合它们！
- en: Quantization is a powerful technique to reduce the memory requirements of a
    model whilst keeping performance similar. It allows for faster loading, using,
    and fine-tuning LLMs even with smaller GPUs.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 量化是一种强大的技术，可以减少模型的内存需求，同时保持性能相似。它允许更快地加载、使用和微调 LLM，即使是在较小的 GPU 上。
- en: 4\. Pre-Quantization (GPTQ vs. AWQ vs. GGUF)
  id: totrans-77
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 4\. 预量化（GPTQ vs. AWQ vs. GGUF）
- en: Thus far, we have explored sharding and quantization techniques. Albeit useful
    techniques to have in your skillset, it seems rather wasteful to have to apply
    them every time you load the model.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经探讨了分片和量化技术。尽管这些技术在你的技能库中非常有用，但每次加载模型时都要应用它们似乎有些浪费。
- en: Instead, these models have often already been sharded and quantized for us to
    use. [TheBloke](https://huggingface.co/TheBloke) in particular is a user on HuggingFace
    that performs a bunch of quantizations for us to use.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，这些模型通常已经被分片和量化，供我们使用。[TheBloke](https://huggingface.co/TheBloke) 尤其是 HuggingFace
    上的一个用户，执行了一些量化供我们使用。
- en: '![](../Images/231b73427e165a5a7d33ab5162132e53.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/231b73427e165a5a7d33ab5162132e53.png)'
- en: At the moment of writing this, he has uploaded **more than 2000** quantized
    models for us!
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在撰写本文时，他已经上传了 **2000 多个** 量化模型供我们使用！
- en: These quantized models actually come in many different shapes and sizes. Most
    notably, the GPTQ, GGUF, and AWQ formats are most frequently used to perform 4-bit
    quantization.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 这些量化模型实际上有很多不同的形状和大小。最显著的是，GPTQ、GGUF 和 AWQ 格式最常用于执行 4 位量化。
- en: 'GPTQ: Post-Training Quantization for GPT Models'
  id: totrans-83
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 'GPTQ: 针对 GPT 模型的后训练量化'
- en: GPTQ is a [**p**ost-**t**raining **q**uantization](https://arxiv.org/abs/2210.17323)
    (PTQ) method for 4-bit quantization that focuses primarily on **GPU** inference
    and performance.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: GPTQ 是一种 [**后**-**训**练 **量**化](https://arxiv.org/abs/2210.17323) (PTQ) 方法，用于
    4 位量化，主要关注 **GPU** 推理和性能。
- en: The idea behind the method is that it will try to compress all weights to a
    4-bit quantization by minimizing the mean squared error to that weight. During
    inference, it will dynamically dequantize its weights to float16 for improved
    performance whilst keeping memory low.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 该方法背后的想法是通过最小化均方误差，将所有权重压缩到 4 位量化。在推理过程中，它会动态地将权重解量化为 float16，以提高性能，同时保持较低的内存占用。
- en: 'For a more detailed guide to the inner workings of GPTQ, definitely check out
    the following post:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解 GPTQ 内部工作的更详细指南，务必查看以下帖子：
- en: '[](/4-bit-quantization-with-gptq-36b0f4f02c34?source=post_page-----c4cd9d77d5be--------------------------------)
    [## 4-bit Quantization with GPTQ'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/4-bit-quantization-with-gptq-36b0f4f02c34?source=post_page-----c4cd9d77d5be--------------------------------)
    [## 4-bit Quantization with GPTQ'
- en: Quantize your own LLMs using AutoGPTQ
  id: totrans-88
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用 AutoGPTQ 量化你自己的 LLM
- en: towardsdatascience.com](/4-bit-quantization-with-gptq-36b0f4f02c34?source=post_page-----c4cd9d77d5be--------------------------------)
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/4-bit-quantization-with-gptq-36b0f4f02c34?source=post_page-----c4cd9d77d5be--------------------------------)
- en: 'We start with installing a number of packages we need to load in GPTQ-like
    models in HuggingFace Transformers:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 我们开始安装一些需要在 HuggingFace Transformers 中加载 GPTQ 类模型的包：
- en: '[PRE9]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: After doing so, we can navigate to the model that we want to load, namely “[TheBloke/zephyr-7B-beta-GPTQ](https://huggingface.co/TheBloke/zephyr-7B-beta-GPTQ)”
    and choose a specific revision.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 完成后，我们可以导航到我们想加载的模型，即“[TheBloke/zephyr-7B-beta-GPTQ](https://huggingface.co/TheBloke/zephyr-7B-beta-GPTQ)”，并选择一个特定的修订版。
- en: These revisions essentially indicate the quantization method, compression level,
    size of the model, etc.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 这些修订本质上表示量化方法、压缩级别、模型的大小等。
- en: 'For now, we are sticking with the “*main*” branch as that is generally a nice
    balance between compression and accuracy:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，我们仍然坚持使用“*main*”分支，因为它在压缩和准确性之间通常是一个不错的平衡：
- en: '[PRE10]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Although we installed a few additional dependencies, we could use the same pipeline
    as we used before which is a great benefit of using GPTQ.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们安装了一些额外的依赖项，但我们可以使用与之前相同的管道，这也是使用 GPTQ 的一个很大好处。
- en: 'After loading the model, we can run a prompt as follows:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 加载模型后，我们可以运行如下提示：
- en: '[PRE11]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'This gives us the following generated text:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 这给了我们以下生成的文本：
- en: Why did the Large Language Model go to the party?
  id: totrans-100
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 为什么大型语言模型去参加派对？
- en: ''
  id: totrans-101
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: To show off its wit and charm, of course!
  id: totrans-102
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 当然，是为了展示它的机智和魅力！
- en: ''
  id: totrans-103
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: But unfortunately, it got lost in the crowd and couldn't find its way back
  id: totrans-104
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 但不幸的是，它在人群中迷失了，找不到回去的路
- en: to its owner. The partygoers were impressed by its ability to blend in
  id: totrans-105
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 对它的主人。派对上的人们对它融入环境的能力感到印象深刻
- en: so seamlessly with the crowd, but the Large Language Model was just confused
  id: totrans-106
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 与人群无缝融合，但大型语言模型只是感到困惑
- en: and wanted to go home. In the end, it was found by a group of humans
  id: totrans-107
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 并想回家。最后，一群人发现了它
- en: who recognized its unique style and brought it back to its rightful place.
  id: totrans-108
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 他们认识到其独特的风格，并将其带回了应有的地方。
- en: From then on, the Large Language Model made sure to wear a name tag at
  id: totrans-109
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 从那时起，大型语言模型确保在
- en: all parties, just to be safe.
  id: totrans-110
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 为了安全起见，所有派对。
- en: GPTQ is the most often used compression method since it optimizes for GPU usage.
    It is definitely worth starting with GPTQ and switching over to a CPU-focused
    method, like GGUF if your GPU cannot handle such large models.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: GPTQ 是最常用的压缩方法，因为它优化了 GPU 的使用。如果你的 GPU 无法处理如此大的模型，绝对值得从 GPTQ 开始，然后切换到以 CPU 为重点的方法，如
    GGUF。
- en: 'GGUF: **GPT-Generated Unified Format**'
  id: totrans-112
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 'GGUF: **GPT-生成统一格式**'
- en: Although GPTQ does compression well, its focus on GPU can be a disadvantage
    if you do not have the hardware to run it.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管 GPTQ 处理压缩非常好，但它对 GPU 的关注可能在你没有硬件支持时成为一个缺点。
- en: GGUF, previously GGML, is a quantization method that allows users to use the
    CPU to run an LLM but also offload some of its layers to the GPU for a speed up.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: GGUF，之前称为 GGML，是一种量化方法，允许用户使用 CPU 运行 LLM，但也可以将一些层卸载到 GPU 上以加速。
- en: Although using the CPU is generally slower than using a GPU for inference, it
    is an incredible format for those running models on CPU or Apple devices. Especially
    since we are seeing smaller and more capable models appearing, like Mistral 7B,
    the GGUF format might just be here to stay!
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然使用 CPU 通常比使用 GPU 推理更慢，但对于在 CPU 或 Apple 设备上运行模型的人来说，它是一个了不起的格式。特别是随着我们看到更多较小但更强大的模型出现，如
    Mistral 7B，GGUF 格式可能会长久存在！
- en: 'Using GGUF is rather straightforward with the [ctransformers](https://github.com/marella/ctransformers)
    package which we will need to install first:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 GGUF 通过 [ctransformers](https://github.com/marella/ctransformers) 包相当简单，我们需要先安装这个包：
- en: '[PRE12]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: After doing so, we can navigate to the model that we want to load, namely “[TheBloke/zephyr-7B-beta-GGUF](https://huggingface.co/TheBloke/zephyr-7B-beta-GGUF)”
    and choose a specific file.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 完成后，我们可以导航到我们想要加载的模型，即“[TheBloke/zephyr-7B-beta-GGUF](https://huggingface.co/TheBloke/zephyr-7B-beta-GGUF)”并选择一个具体的文件。
- en: Like GPTQ, these files indicate the quantization method, compression, level,
    size of the model, etc.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 像 GPTQ 一样，这些文件指示了量化方法、压缩、级别、模型大小等。
- en: 'We are using “*zephyr-7b-beta.Q4_K_M.gguf*” since we focus on 4-bit quantization:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用“*zephyr-7b-beta.Q4_K_M.gguf*”因为我们专注于 4 位量化：
- en: '[PRE13]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'After loading the model, we can run a prompt as follows:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 加载模型后，我们可以运行如下提示：
- en: '[PRE14]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'This gives us the following output:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 这给出了以下输出：
- en: Why did the Large Language Model go to the party?
  id: totrans-125
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 为什么大型语言模型去参加派对？
- en: ''
  id: totrans-126
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: To impress everyone with its vocabulary!
  id: totrans-127
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 让大家对它的词汇印象深刻！
- en: ''
  id: totrans-128
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: But unfortunately, it kept repeating the same jokes over and over again, making
    everyone groan and roll their eyes. The partygoers soon realized that the Large
    Language Model was more of a party pooper than a party animal.
  id: totrans-129
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 但不幸的是，它不断重复相同的笑话，让大家 groan 和 roll their eyes。派对上的人们很快意识到，大型语言模型更像是个 party pooper
    而不是 party animal。
- en: ''
  id: totrans-130
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Moral of the story: Just because a Large Language Model can generate a lot
    of words, doesn’t mean it knows how to be funny or entertaining. Sometimes, less
    is more!'
  id: totrans-131
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 故事的寓意是：仅仅因为大型语言模型可以生成很多字词，并不意味着它知道如何搞笑或娱乐。有时候，少即是多！
- en: GGUF is an amazing format if you want to leverage both the CPU and GPU when
    you, like me, are GPU-poor and do not have the latest and greatest GPU available.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: GGUF 是一个很棒的格式，如果你像我一样缺乏 GPU，没有最新最强的 GPU，想要同时利用 CPU 和 GPU 的话。
- en: 'AWQ: Activation-aware Weight Quantization'
  id: totrans-133
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 'AWQ: Activation-aware Weight Quantization'
- en: A new format on the block is AWQ ([Activation-aware Weight Quantization](https://arxiv.org/abs/2306.00978))
    which is a quantization method similar to GPTQ. There are several differences
    between AWQ and GPTQ as methods but the most important one is that AWQ assumes
    that not all weights are equally important for an LLM’s performance.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 一个新格式是 AWQ ([Activation-aware Weight Quantization](https://arxiv.org/abs/2306.00978))，它是一种类似于
    GPTQ 的量化方法。AWQ 和 GPTQ 作为方法之间有几个区别，但最重要的一点是 AWQ 假设不是所有的权重对 LLM 的性能同样重要。
- en: In other words, there is a small fraction of weights that will be skipped during
    quantization which helps with the quantization loss.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，在量化过程中会跳过一小部分权重，这有助于减少量化损失。
- en: As a result, their paper mentions a significant speed-up compared to GPTQ whilst
    keeping similar, and sometimes even better, performance.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 结果，他们的论文提到，与 GPTQ 相比有显著的加速，同时保持类似，甚至有时更好的性能。
- en: The method is still relatively new and has not been adopted yet to the extent
    of GPTQ and GGUF, so it is interesting to see if all these methods can co-exist.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法仍然相对较新，尚未普及到像 GPTQ 和 GGUF 那样的程度，因此很有趣的是看看这些方法是否能够共存。
- en: 'For AWQ, we will use the [vLLM](https://github.com/vllm-project/vllm) package
    as that was, at least in my experience, the road of least resistance to using
    AWQ:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 AWQ，我们将使用 [vLLM](https://github.com/vllm-project/vllm) 包，因为根据我的经验，这是使用 AWQ
    的最简便途径：
- en: '[PRE15]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'With vLLM, loading and using our model becomes painless:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 vLLM，加载和使用我们的模型变得毫不费力：
- en: '[PRE16]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Then, we can easily run the model with `.generate`:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以轻松地使用 `.generate` 来运行模型：
- en: '[PRE17]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'This gives us the following output:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 这给我们带来了以下输出：
- en: Why did the Large Language Model go to the party?
  id: totrans-145
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 为什么大型语言模型去参加派对？
- en: ''
  id: totrans-146
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: To network and expand its vocabulary!
  id: totrans-147
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 为了扩展词汇量和建立网络！
- en: ''
  id: totrans-148
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Why did the Large Language Model blush?
  id: totrans-149
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 为什么大型语言模型脸红了？
- en: ''
  id: totrans-150
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Because it overheard another model saying it was a little too wordy!
  id: totrans-151
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 因为它听到另一个模型说它有点啰嗦！
- en: ''
  id: totrans-152
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Why did the Large Language Model get kicked out of the library?
  id: totrans-153
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 为什么大型语言模型被逐出了图书馆？
- en: ''
  id: totrans-154
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: It was being too loud and kept interrupting other models’ conversations with
    its endless chatter!
  id: totrans-155
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 它太吵了，一直打断其他模型的对话，喋喋不休！
- en: ''
  id: totrans-156
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: …
  id: totrans-157
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: …
- en: Although it is a new format, AWQ is gaining popularity due to its speed and
    quality of compression!
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这是一个新格式，但由于其速度和压缩质量，AWQ 正在获得越来越多的关注！
- en: '🔥 **TIP**: For a more detailed comparison between these techniques with respect
    to VRAM/Perplexity, I highly advise reading this [in-depth post](https://oobabooga.github.io/blog/posts/gptq-awq-exl2-llamacpp/)
    with a follow-up [here](https://github.com/ggerganov/llama.cpp/pull/3776#issuecomment-1781472687).'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 🔥 **提示**：对于这些技术在 VRAM/困惑度方面的详细比较，我强烈建议阅读这篇 [深入文章](https://oobabooga.github.io/blog/posts/gptq-awq-exl2-llamacpp/)
    和后续的 [讨论](https://github.com/ggerganov/llama.cpp/pull/3776#issuecomment-1781472687)。
- en: Thank you for reading!
  id: totrans-160
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 感谢阅读！
- en: If you are, like me, passionate about AI and/or Psychology, please feel free
    to add me on [**LinkedIn**](https://www.linkedin.com/in/mgrootendorst/), follow
    me on [**Twitter**](https://twitter.com/MaartenGr), or subscribe to my [**Newsletter**](http://maartengrootendorst.substack.com/).
    You can also find some of my content on my [**Personal Website**](https://maartengrootendorst.com/).
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你和我一样，对 AI 和/或心理学充满热情，请随时在 [**LinkedIn**](https://www.linkedin.com/in/mgrootendorst/)
    上添加我，在 [**Twitter**](https://twitter.com/MaartenGr) 上关注我，或者订阅我的 [**Newsletter**](http://maartengrootendorst.substack.com/)。你也可以在我的
    [**Personal Website**](https://maartengrootendorst.com/) 上找到我的一些内容。
- en: '*All images without a source credit were created by the author — Which means
    all of them, I like creating my own images ;)*'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '*所有没有来源信用的图片均由作者创作 —— 也就是说，全部都是我，我喜欢自己制作图片 ;)*'
