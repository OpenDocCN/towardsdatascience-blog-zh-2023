- en: Do Transformers Lose to Linear Models?
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Transformers 是否输给了线性模型？
- en: 原文：[https://towardsdatascience.com/transformers-lose-to-linear-models-902164ca5974?source=collection_archive---------11-----------------------#2023-01-23](https://towardsdatascience.com/transformers-lose-to-linear-models-902164ca5974?source=collection_archive---------11-----------------------#2023-01-23)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/transformers-lose-to-linear-models-902164ca5974?source=collection_archive---------11-----------------------#2023-01-23](https://towardsdatascience.com/transformers-lose-to-linear-models-902164ca5974?source=collection_archive---------11-----------------------#2023-01-23)
- en: '![](../Images/8d84a209dff6be6c22d99da7e5020a3b.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8d84a209dff6be6c22d99da7e5020a3b.png)'
- en: Photo by [Nicholas Cappello](https://unsplash.com/@bash__profile?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 照片由 [Nicholas Cappello](https://unsplash.com/@bash__profile?utm_source=medium&utm_medium=referral)
    提供，来自 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: Long-Term Forecasting using Transformers may not be the way to go
  id: totrans-4
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 Transformers 进行长期预测可能并非最佳选择
- en: '[](https://medium.com/@upadhyan?source=post_page-----902164ca5974--------------------------------)[![Nakul
    Upadhya](../Images/336cb21272e9b1f098177adbde50e92e.png)](https://medium.com/@upadhyan?source=post_page-----902164ca5974--------------------------------)[](https://towardsdatascience.com/?source=post_page-----902164ca5974--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----902164ca5974--------------------------------)
    [Nakul Upadhya](https://medium.com/@upadhyan?source=post_page-----902164ca5974--------------------------------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@upadhyan?source=post_page-----902164ca5974--------------------------------)[![Nakul
    Upadhya](../Images/336cb21272e9b1f098177adbde50e92e.png)](https://medium.com/@upadhyan?source=post_page-----902164ca5974--------------------------------)[](https://towardsdatascience.com/?source=post_page-----902164ca5974--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----902164ca5974--------------------------------)
    [Nakul Upadhya](https://medium.com/@upadhyan?source=post_page-----902164ca5974--------------------------------)'
- en: ·
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F4d9dddc62a80&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransformers-lose-to-linear-models-902164ca5974&user=Nakul+Upadhya&userId=4d9dddc62a80&source=post_page-4d9dddc62a80----902164ca5974---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----902164ca5974--------------------------------)
    ·9 min read·Jan 23, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F902164ca5974&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransformers-lose-to-linear-models-902164ca5974&user=Nakul+Upadhya&userId=4d9dddc62a80&source=-----902164ca5974---------------------clap_footer-----------)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F4d9dddc62a80&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransformers-lose-to-linear-models-902164ca5974&user=Nakul+Upadhya&userId=4d9dddc62a80&source=post_page-4d9dddc62a80----902164ca5974---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----902164ca5974--------------------------------)
    · 9分钟阅读 · 2023年1月23日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F902164ca5974&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransformers-lose-to-linear-models-902164ca5974&user=Nakul+Upadhya&userId=4d9dddc62a80&source=-----902164ca5974---------------------clap_footer-----------)'
- en: --
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F902164ca5974&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransformers-lose-to-linear-models-902164ca5974&source=-----902164ca5974---------------------bookmark_footer-----------)'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F902164ca5974&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransformers-lose-to-linear-models-902164ca5974&source=-----902164ca5974---------------------bookmark_footer-----------)'
- en: In recent years, Transformer-based solutions have been gaining incredible popularity.
    With the success of BERT, GTP, and other language transformers researchers started
    to apply this architecture to other sequential-modeling problems, specifically
    in the area of time series forecasting (also known as Long-Term Time Series Forecasting
    or LTSF). The attention mechanism seemed to be a perfect method to extract some
    of the long-term correlations present in long sequences.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，基于 Transformer 的解决方案获得了极大的关注。随着 BERT、GTP 以及其他语言 Transformer 的成功，研究人员开始将这种架构应用于其他序列建模问题，特别是在时间序列预测领域（也称为长期时间序列预测或
    LTSF）。注意力机制似乎是一种提取长序列中某些长期相关性的完美方法。
- en: 'However, researchers from the Chinese University of Hong Kong and the International
    Digital Economy Activity recently decided to question: [Are Transformers Effective
    for Time Series Forecasting](https://arxiv.org/abs/2205.13504) [1]? **They show
    that self-attention mechanisms (even with positional encoding) can result in temporal
    information loss. They then validate this claim with a set of one-layer linear
    models which outperform the transformer benchmarks in almost every experiment.**'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，最近来自香港中文大学和国际数字经济活动的研究人员决定质疑：[变形金刚是否适用于时间序列预测](https://arxiv.org/abs/2205.13504)
    [1]? **他们表明，自注意机制（即使加入了位置编码）可能会导致时间信息丢失。他们随后用一组单层线性模型验证了这一说法，在几乎每个实验中都优于变压器基准。**
- en: In simpler terms, *Transformers may not be the most ideal architecture for forecasting
    problems.*
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，*变压器可能并不是预测问题的最理想架构*。
- en: In this post, I aim to summarize the findings and experiments of Zeng et al.
    [1] that lead to this conclusion and discuss some potential implications of the
    work. All the experiments and models developed by the authors can be found in
    their [GitHub repository](https://github.com/cure-lab/LTSF-Linear) as well. Additionally,
    I highly encourage everyone to read [the original paper](https://arxiv.org/abs/2205.13504).
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我旨在总结曾等人[1]的发现和实验，得出这一结论，并讨论该工作的一些潜在影响。作者开发的所有实验和模型也可以在他们的[GitHub仓库](https://github.com/cure-lab/LTSF-Linear)中找到。此外，我强烈建议每个人都阅读[原始论文](https://arxiv.org/abs/2205.13504)。
- en: The Models and Data
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型和数据
- en: 'In their work, the authors evaluated 5 different SOTA Transformer models on
    the [Electricity Transformer Dataset (ETDataset)](https://github.com/zhouhaoyi/ETDataset)
    [3]. These models and some of their main features are as follows:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在他们的工作中，作者在[Electricity Transformer Dataset (ETDataset)](https://github.com/zhouhaoyi/ETDataset)
    [3]上评估了5种不同的SOTA变压器模型。这些模型及其主要特点如下：
- en: '**LogTrans** [2]: Proposes convolutional self-attention so local context can
    be better incorporated into the attention mechanism. The model also encodes a
    sparsity bias into the attention scheme. This helps improve the memory complexity'
  id: totrans-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**LogTrans** [2]:提出了卷积自注意力，以便更好地将本地上下文纳入到注意力机制中。该模型还在注意力方案中编码了一种稀疏偏差。这有助于提高内存复杂性。'
- en: '**Informer** [3]: Addresses memory/time complexity and error complexity issues
    caused by an auto-regressive decoder by proposing a new architecture and a direct-multi-step
    (DMS) forecasting strategy.'
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**Informer** [3]:通过提出一种新的架构和一种直接多步（DMS）预测策略，解决了由自回归解码器引起的内存/时间复杂性和误差复杂性问题。'
- en: '**Autoformer** [4]: Applies a seasonal-trend decomposition behind each neural
    block to extract the trend-cyclical components. Additionally, Autoformer designs
    a series-wise auto-correlation mechanism to replace vanilla self-attention.'
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**Autoformer** [4]: 在每个神经块后面应用季节趋势分解以提取趋势周期性成分。此外，Autoformer设计了一种系列式自动相关机制，以取代常规的自注意力。'
- en: '**Pyraformer** [5]: Implements a novel pyramidal attention mechanism that captures
    hierarchical multi-scale temporal dependencies. Like LogTrans, this model also
    explicitly encodes a sparsity bias into the attention scheme.'
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**Pyraformer** [5]:实现了一种新颖的金字塔式注意力机制，捕捉分层多尺度时间依赖关系。像LogTrans一样，该模型还在注意力方案中明确编码了一种稀疏偏差。'
- en: '**FEDFormer** [6]: Enhances the traditional transformer architecture by incorporating
    seasonal-trend decomposition methods into the architecture, effectively developing
    a *F*requency-*E*nhanced *D*ecomposed Trans*Former*.'
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**FEDFormer** [6]:通过将季节趋势分解方法合并到体系结构中，有效地开发了*F*requency-*E*nhanced *D*ecomposed
    *T*ransformer，增强了传统的变压器架构。'
- en: These models all make various changes to various pieces of the transformer architecture
    to address various different problems with traditional transformers (a full summary
    can be found in figure 1)
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 这些模型都对变压器架构的各个部分进行了各种改动，以解决传统变压器存在的各种不同问题（完整总结可在图1中找到）。
- en: '![](../Images/2761a09135aa7a0cbefbdbd88d36824a.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2761a09135aa7a0cbefbdbd88d36824a.png)'
- en: 'Figure 1: The pipeline of existing Transformer-based TSF solutions (Figure
    produced by Zeng et al. [1])'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：现有基于变压器的时间序列预测解决方案的流程图（由曾等人制作的图1）
- en: To compete against these transformer models, the authors proposed some “embarrassingly
    simple” models [1] that perform DMS predictions.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 为了与这些变压器模型竞争，作者提出了一些“令人尴尬地简单”的模型[1]来进行DMS预测。
- en: '![](../Images/ddc1833719a2931a311d6cda91bec641.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ddc1833719a2931a311d6cda91bec641.png)'
- en: 'Figure 2: Illustration of the basic DMS linear models (Figure produced by Zeng
    et al. [1])'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：基本DMS线性模型的示意图（图由Zeng等人[1]提供）
- en: 'These models and their properties are:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 这些模型及其属性如下：
- en: '**Decomposed Linear (D-Linear):** D-Linear uses a decomposition scheme to split
    raw data into a trend and seasonal component. Two single-layer linear networks
    are then applied to each component and the outputs are summed to get the final
    prediction.'
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**分解线性（D-Linear）：** D-Linear使用分解方案将原始数据拆分为趋势和季节性组件。然后对每个组件应用两个单层线性网络，并将输出求和以获得最终预测。'
- en: '**Normalized Linear (N-Linear):** N-Linear first subtracts the input by the
    last value of the sequence. The input is then passed into a single linear layer
    and the subtracted part is added in before making a final prediction. This helps
    address distribution shifts in the data.'
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**归一化线性（N-Linear）：** N-Linear首先从输入中减去序列的最后一个值。然后将输入传递到一个线性层中，最后将减去的部分加回，以进行最终预测。这有助于解决数据中的分布偏移。'
- en: '**Repeat**: Just repeat the last value in the look-back window.'
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**重复**：只需重复回顾窗口中的最后一个值。'
- en: These are some very simple baselines. The Linear models both involve a small
    amount of data preprocessing and a single-layer network. The Repeat is a trivial
    baseline.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是一些非常简单的基准线。线性模型都涉及少量的数据预处理和一个单层网络。重复是一个琐碎的基准。
- en: The experiments were performed with various widely-used datasets like the [Electricity
    Transformer](https://github.com/zhouhaoyi/ETDataset) (ETDataset)[3], Traffic,
    Electricity, Weather, ILI, and Exchange Rate [7] datasets.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 实验是在各种广泛使用的数据集上进行的，如[Electricity Transformer](https://github.com/zhouhaoyi/ETDataset)
    (ETDataset)[3]、交通、电力、天气、ILI和汇率[7]数据集。
- en: The Experiments
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实验
- en: On the 8 models above, the authors performed a series of experiments to evaluate
    the models’ performances and determine the impact of various components of each
    model on the end predictions.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述8个模型上，作者进行了一系列实验，以评估模型的性能并确定每个模型中各种组件对最终预测的影响。
- en: 'The first experiment was straightforward: each model was trained and used to
    forecast the data. The look-back periods were varied as well. The full testing
    results can be found in table 1 but in summary, **FEDFormer [6] was the best-performing
    *transformer* in most cases but was never the *overall* best performer**.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个实验很简单：每个模型都经过训练并用于预测数据。回顾期也有所变化。完整的测试结果见表1，但总的来说，**FEDFormer [6]在大多数情况下是表现最好的*transformer*，但从未是*总体*最佳的**。
- en: '![](../Images/7273ac9b84f4ae18e948f460a40b82ce.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7273ac9b84f4ae18e948f460a40b82ce.png)'
- en: 'Table 1: Experimental Errors. The best Transformer result is underlined, and
    the best overall result is bolded. (Figure produced by Zeng et al. [1])'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 表1：实验误差。最佳Transformer结果下划线标记，最佳总体结果加粗。（图由Zeng等人[1]提供）
- en: This embarrassing performance of transformers can be seen in the predictions
    for the Electricity, Exchange-Rate, and ETDataset in figure 3.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 这种Transformer令人尴尬的表现可以在图3中对电力、汇率和ETDataset的预测中看到。
- en: '![](../Images/2608bdfdad62c6fae76d68c87a33535e.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2608bdfdad62c6fae76d68c87a33535e.png)'
- en: 'Figure 3: LTSF output with input length = 96 and output length = 192 (Figure
    produced by Zeng et al. [1]).'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：输入长度=96，输出长度=192的LTSF输出（图由Zeng等人[1]提供）。
- en: 'Quoting the authors:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 引用作者的话：
- en: Transformers [28, 30, 31] fail to capture the scale and bias of the future data
    on Electricity and ETTh2\. Moreover, they can hardly predict a proper trend on
    aperiodic data such as Exchange-Rate. These phenomena further indicate the inadequacy
    of existing Transformer-based solutions for the LTSF task.
  id: totrans-42
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: Transformers [28, 30, 31]未能捕捉到电力和ETTh2上未来数据的规模和偏差。此外，它们几乎无法预测诸如汇率等非周期性数据的适当趋势。这些现象进一步表明现有的Transformer解决方案在LTSF任务中的不足。
- en: Many would argue however that this is unfair to transformers as attention mechanisms
    are usually good at preserving long-range information so Transformers should perform
    better with longer input sequences, and the authors test this hypothesis in their
    next experiment. They vary the look-back period between 24 and 720 time steps
    and evaluate the MSE. The authors found that in many cases, **the performance
    of the transformers did not improve and the error actually increases for a few
    models (view figure 4 for full results). In comparison, the performance of the
    Linear models significantly improved with the inclusion of more time steps**.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，许多人会认为这对变换器不公平，因为注意机制通常擅长保存长期信息，所以变换器应该在更长的输入序列中表现更好，作者在他们的下一个实验中测试了这个假设。他们将回溯期在24到720个时间步之间变化，并评估均方误差。作者发现，在许多情况下，**变换器的性能没有提高，实际上几个模型的误差还增加了（完整结果见图4）。相比之下，线性模型的性能在加入更多时间步后显著提高**。
- en: '![](../Images/8290012dc9797f84a645851454fad722.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8290012dc9797f84a645851454fad722.png)'
- en: 'Figure 4: MSE Results when varying the look-back period length (Figure produced
    by Zeng et al. [1])'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：回溯期长度变化的均方误差结果（图由Zeng等人[1]制作）
- en: There are still other factors to consider, however. Due to the complexity of
    transformers, they often require larger training data sets than other models in
    order to perform well and as a result, the authors decided to test whether or
    not training data size is a limiting factor for these transformer architectures.
    They leveraged the Traffic data [7] and trained Autoformer [4] and FEDformer [6]
    on the original set as well as a truncated set with the expectation that the errors
    will be higher with the smaller training set. **Surprisingly, the models trained
    on the smaller training set performed marginally better. While this doesn’t mean
    that one should use a smaller training set, this does mean that data set size
    is not a limiting factor for LTSF Transformers.**
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，仍然有其他因素需要考虑。由于变换器的复杂性，它们通常需要比其他模型更大的训练数据集才能表现良好，因此，作者决定测试训练数据集大小是否是这些变换器架构的限制因素。他们利用了交通数据[7]，并在原始数据集和截断数据集上训练了Autoformer
    [4]和FEDformer [6]，期望较小的训练集会导致误差更高。**令人惊讶的是，在较小训练集上训练的模型表现稍微更好。这并不意味着应该使用较小的训练集，但这确实意味着数据集大小不是LTSF变换器的限制因素。**
- en: Along with varying the training data size and look-back period size, the authors
    also experimented with varying what timesteps the lookback window started at.
    For example, if they were attempting to make a prediction for the period after
    *t*=196, instead of using *t* = 100, 101,…, 196 (the adjacent or “close” window)
    the authors tried using *t* = 4, 5,…, 100 (the “far” window). The idea is that
    forecasting should depend on whether the model can capture trend and periodicity
    well and the farther the horizon is, the worse the prediction should be. **The
    authors discovered that the performance of the transformers only drops slightly
    between the “close” and “far” windows.** This implies that the transformers may
    be overfitting to the provided data, which could explain why the Linear models
    perform better.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 除了改变训练数据大小和回溯期长度外，作者还尝试了不同的回溯窗口起始时间。例如，如果他们试图对*t*=196之后的时期进行预测，而不是使用*t*=100,
    101,…, 196（邻近或“接近”窗口），作者尝试了使用*t*=4, 5,…, 100（“远离”窗口）。这个想法是，预测应该依赖于模型是否能够很好地捕捉趋势和周期性，视野越远，预测效果应该越差。**作者发现变换器在“接近”窗口和“远离”窗口之间的性能仅略有下降。**这意味着变换器可能会对提供的数据过拟合，这可能解释了为什么线性模型表现更好。
- en: After evaluating the various transformer models, the authors also dived specifically
    into the effectiveness of self-attention and embedding strategies used by these
    models. Their first experiment involved disassembling existing transformers to
    analyze whether or not the complex design of the transformer was necessary. They
    broke the attention layer down into a simple linear layer, then removed auxiliary
    pieces apart from the embedding mechanisms, and finally reduced the transformer
    down to only linear layers. At each step, they recorded the MSE using various
    look-back period sizes and found that **the performance of the transformer grows
    with the gradual simplification.**
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在评估了各种变换器模型后，作者们还专门探讨了这些模型中自注意力和嵌入策略的有效性。他们的第一个实验涉及拆解现有的变换器，以分析变换器复杂设计是否必要。他们将注意力层拆解为简单的线性层，然后去除了除嵌入机制之外的辅助部分，最后将变换器简化为仅有的线性层。在每一步，他们记录了使用不同回溯期大小的均方误差，并发现**随着逐步简化，变换器的性能有所提升。**
- en: The authors also wanted to examine the impact of the transformers to preserve
    temporal order. They hypothesized that since self-attention is permutation-invariant
    (ignores order) and time series are permutation-sensitive, positional encoding
    and self-attention will not be enough to capture temporal information. To test
    this, the authors modified the sequences by shuffling the data and exchanging
    the first half of the input sequence with the second half. The more temporal information
    is captured by the model, the more the performance of the model should decrease
    with the modified sets. **The authors observed that the linear models had a higher
    performance drop than any of the transformer models, suggesting that the transformers
    are capturing less temporal information than the linear models. The full results
    can be found in the table below**
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 作者们还想研究变换器在保持时间顺序方面的影响。他们假设由于自注意力是排列不变的（忽略顺序），而时间序列对排列敏感，因此位置编码和自注意力不足以捕获时间信息。为了测试这一点，作者通过打乱数据和交换输入序列的前半部分与后半部分来修改序列。模型捕获的时间信息越多，修改数据集时模型性能的下降应该越大。**作者观察到，线性模型的性能下降比任何变换器模型都要大，这表明变换器捕获的时间信息少于线性模型。完整结果见下表**
- en: '![](../Images/bb699ce143b7bfdf66afae2bbff1477a.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bb699ce143b7bfdf66afae2bbff1477a.png)'
- en: 'Table 2: (Figure produced by Zeng et al. [1])'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2:（图由 Zeng 等人 [1] 制作）
- en: To further dive into the information-capturing capabilities of transformers,
    the authors examined the effectiveness of different encoding strategies by removing
    positional and temporal encoding from the transformers. These results were mixed
    depending on the model. For FEDFormer [6] and Autoformer [4], removing positional
    encoding improved the performance on the Traffic dataset on most look-back window
    sizes. However, Informer [3] did perform the best when it had all its positional
    encodings.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进一步探讨变换器的信息捕获能力，作者们通过去除变换器的位置信息和时间编码来检验不同编码策略的有效性。这些结果因模型而异。对于 FEDFormer [6]
    和 Autoformer [4]，去除位置编码在大多数回溯窗口大小上提高了交通数据集的性能。然而，Informer [3] 在具有所有位置编码时表现最好。
- en: Discussion and Conclusion
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 讨论与结论
- en: There are a few points to be careful of when understanding these results. Transformers
    are very sensitive to hyperparameters and often require a lot of tuning to effectively
    model the problem. However, the authors do not perform any kind of hyperparameter
    search when implementing these models, instead opting to use the default parameters
    used by the implementation of the models. There is an argument to be made that
    they also did not tune the linear models, so the comparison is fair. Additionally,
    tuning the linear models would take significantly less time than training the
    transformers due to the simplicity of the linear models. Despite this, there could
    be problems where transformers work incredibly well with the right hyperparameters,
    and cost and time can be ignored for accuracy.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 理解这些结果时需要注意几点。变换器对超参数非常敏感，通常需要大量的调优才能有效地建模问题。然而，作者在实现这些模型时没有进行任何超参数搜索，而是选择了模型实现中使用的默认参数。有观点认为，他们也没有调整线性模型，因此比较是公平的。此外，调整线性模型所需的时间远远少于训练变换器，因为线性模型的简单性。尽管如此，可能会出现变换器在正确的超参数下表现极其出色的情况，而成本和时间可以忽略，以提高准确性。
- en: Despite these critiques, the experiments done by the authors detail a clear
    breakdown of the flaws of transformers. These are large, very complex models that
    overfit easily on time series data. While they work well for language processing
    and other tasks, the permutation-invariant nature of self-attention does cause
    significant temporal loss. Additionally, a linear model is incredibly interpretable
    and explainable compared to the complicated architecture of a Transformer. If
    some modifications are made to these components of LTSF Transformers, we may see
    them eventually beat simple linear models or tackle problems linear models are
    bad at modeling (for example change point identification). In the meantime, however,
    data scientists and decision-makers should not blindly throw Transformers at a
    time-series forecasting problem without having very good reasons for leveraging
    this architecture.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管有这些批评，作者进行的实验详细地分解了变换器的缺陷。这些模型庞大且非常复杂，容易在时间序列数据上过拟合。虽然它们在语言处理和其他任务中表现良好，但自注意力的置换不变特性确实会导致显著的时间损失。此外，相比于复杂的变换器架构，线性模型在解释性和可解释性方面极其优越。如果对这些长期序列预测（LTSF）变换器的组件进行一些修改，我们可能会看到它们最终超越简单的线性模型或解决线性模型难以建模的问题（例如变点识别）。然而，与此同时，数据科学家和决策者不应盲目地将变换器应用于时间序列预测问题，除非有非常充分的理由来利用这种架构。
- en: Resources and References
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 资源和参考文献
- en: '[1] A. Zeng, M. Chen, L. Zhang, Q. Xu. [Are Transformers Effective for Time
    Series Forecasting?](https://arxiv.org/abs/2205.13504) (2022). Thirty-Seventh
    AAAI Conference on Artificial Intelligence.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] A. Zeng, M. Chen, L. Zhang, Q. Xu. [变换器对时间序列预测有效吗？](https://arxiv.org/abs/2205.13504)
    (2022). 第三十七届AAAI人工智能会议。'
- en: '[2] S. Li, X. Jin, Y. Xuan, X. Zhou, W. Chen, Y. Wang, X. Yan. [Enhancing the
    Locality and Breaking the Memory Bottleneck of Transformer on Time Series Forecasting](https://proceedings.neurips.cc/paper/2019/hash/6775a0635c302542da2c32aa19d86be0-Abstract.html)
    (2019). Advances in Neural Information Processing systems 32.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] S. Li, X. Jin, Y. Xuan, X. Zhou, W. Chen, Y. Wang, X. Yan. [提升变换器在时间序列预测中的局部性并打破记忆瓶颈](https://proceedings.neurips.cc/paper/2019/hash/6775a0635c302542da2c32aa19d86be0-Abstract.html)
    (2019). 神经信息处理系统进展 32。'
- en: '[3] H. Zhou, S. Zhang, J. Peng, S. Zhang, J. Li, H. Xiong, W. Zhang. [Informer:
    Beyond Efficient Transformer for Long Sequence Time-Series Forecasting](https://arxiv.org/abs/2012.07436)
    (2021). The Thirty-Fifth AAAI Conference on Artificial Intelligence, Virtual Conference.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] H. Zhou, S. Zhang, J. Peng, S. Zhang, J. Li, H. Xiong, W. Zhang. [Informer:
    超越高效变换器的长序列时间序列预测](https://arxiv.org/abs/2012.07436) (2021). 第三十五届AAAI人工智能会议，虚拟会议。'
- en: '[4] H. Wu, J. Xu, J. Wang, M. Long. [Autoformer: Decomposition Transformers
    with Auto-Correlation for Long-Term Series Forecasting](https://arxiv.org/abs/2106.13008)
    (2021). Advances in Neural Information Processing Systems 34.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] H. Wu, J. Xu, J. Wang, M. Long. [Autoformer: 用于长期序列预测的自相关分解变换器](https://arxiv.org/abs/2106.13008)
    (2021). 神经信息处理系统进展 34。'
- en: '[5] S. Liu, H. Yu, C. Liao, J. Li, W. Lin, A.X. Liu, S. Dustdar. [Pyraformer:
    Low-Complexity Pyramidal Attention for Long-Range Time Series Modeling and Forecasting](https://openreview.net/forum?id=0EXmFzUn5I)
    (2021). International Conference on Learning Representations 2021.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '[5] S. Liu, H. Yu, C. Liao, J. Li, W. Lin, A.X. Liu, S. Dustdar. [Pyraformer:
    低复杂度金字塔注意力用于长时间序列建模和预测](https://openreview.net/forum?id=0EXmFzUn5I) (2021). 国际学习表示会议
    2021。'
- en: '[6] T. Zhou, Z. Ma, Q. Wen, X. Wang, L. Sun, R. Jin. F[EDformer: Frequency
    Enhanced Decomposed Transformer for Long-term Series Forecasting](https://arxiv.org/abs/2201.12740)
    (2022). 39th International Conference on Machine Learning.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '[6] T. Zhou, Z. Ma, Q. Wen, X. Wang, L. Sun, R. Jin. [EDformer: 用于长期序列预测的频率增强分解变换器](https://arxiv.org/abs/2201.12740)
    (2022). 第39届国际机器学习会议。'
- en: '[7] G. Lai, W-C. Chang, Y. Yang, and H. Liu. [Modeling Long- and Short-Term
    Temporal Patterns with Deep Neural Networks](https://arxiv.org/abs/1703.07015)
    (2017). 41st International ACM SIGIR Conference on Research and Development in
    Information Retrieval.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '[7] G. Lai, W-C. Chang, Y. Yang, 和 H. Liu. [使用深度神经网络建模长短期时间模式](https://arxiv.org/abs/1703.07015)
    (2017). 第41届国际ACM SIGIR信息检索研究与发展会议。'
