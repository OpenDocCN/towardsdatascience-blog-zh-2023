- en: Multiple GPU training in PyTorch and Gradient Accumulation as an alternative
    to it
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**PyTorch中的多GPU训练及梯度累积作为其替代方案**'
- en: 原文：[https://towardsdatascience.com/multiple-gpu-training-in-pytorch-and-gradient-accumulation-as-an-alternative-to-it-e578b3fc5b91?source=collection_archive---------10-----------------------#2023-07-24](https://towardsdatascience.com/multiple-gpu-training-in-pytorch-and-gradient-accumulation-as-an-alternative-to-it-e578b3fc5b91?source=collection_archive---------10-----------------------#2023-07-24)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/multiple-gpu-training-in-pytorch-and-gradient-accumulation-as-an-alternative-to-it-e578b3fc5b91?source=collection_archive---------10-----------------------#2023-07-24](https://towardsdatascience.com/multiple-gpu-training-in-pytorch-and-gradient-accumulation-as-an-alternative-to-it-e578b3fc5b91?source=collection_archive---------10-----------------------#2023-07-24)
- en: Code and Theory
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**代码与理论**'
- en: '[](https://medium.com/@alexml0123?source=post_page-----e578b3fc5b91--------------------------------)[![Alexey
    Kravets](../Images/3b31f9b3c73c6c7ca709f845e6f70023.png)](https://medium.com/@alexml0123?source=post_page-----e578b3fc5b91--------------------------------)[](https://towardsdatascience.com/?source=post_page-----e578b3fc5b91--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----e578b3fc5b91--------------------------------)
    [Alexey Kravets](https://medium.com/@alexml0123?source=post_page-----e578b3fc5b91--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@alexml0123?source=post_page-----e578b3fc5b91--------------------------------)[![Alexey
    Kravets](../Images/3b31f9b3c73c6c7ca709f845e6f70023.png)](https://medium.com/@alexml0123?source=post_page-----e578b3fc5b91--------------------------------)[](https://towardsdatascience.com/?source=post_page-----e578b3fc5b91--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----e578b3fc5b91--------------------------------)
    [Alexey Kravets](https://medium.com/@alexml0123?source=post_page-----e578b3fc5b91--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fcf3e4a05b535&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmultiple-gpu-training-in-pytorch-and-gradient-accumulation-as-an-alternative-to-it-e578b3fc5b91&user=Alexey+Kravets&userId=cf3e4a05b535&source=post_page-cf3e4a05b535----e578b3fc5b91---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----e578b3fc5b91--------------------------------)
    ·7 min read·Jul 24, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fe578b3fc5b91&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmultiple-gpu-training-in-pytorch-and-gradient-accumulation-as-an-alternative-to-it-e578b3fc5b91&user=Alexey+Kravets&userId=cf3e4a05b535&source=-----e578b3fc5b91---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fcf3e4a05b535&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmultiple-gpu-training-in-pytorch-and-gradient-accumulation-as-an-alternative-to-it-e578b3fc5b91&user=Alexey+Kravets&userId=cf3e4a05b535&source=post_page-cf3e4a05b535----e578b3fc5b91---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----e578b3fc5b91--------------------------------)
    ·7分钟阅读·2023年7月24日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fe578b3fc5b91&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmultiple-gpu-training-in-pytorch-and-gradient-accumulation-as-an-alternative-to-it-e578b3fc5b91&user=Alexey+Kravets&userId=cf3e4a05b535&source=-----e578b3fc5b91---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fe578b3fc5b91&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmultiple-gpu-training-in-pytorch-and-gradient-accumulation-as-an-alternative-to-it-e578b3fc5b91&source=-----e578b3fc5b91---------------------bookmark_footer-----------)![](../Images/ed6fca5026469bec09b620c9620bc331.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fe578b3fc5b91&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmultiple-gpu-training-in-pytorch-and-gradient-accumulation-as-an-alternative-to-it-e578b3fc5b91&source=-----e578b3fc5b91---------------------bookmark_footer-----------)![](../Images/ed6fca5026469bec09b620c9620bc331.png)'
- en: '[https://unsplash.com/photos/vBzJ0UFOA70](https://unsplash.com/photos/vBzJ0UFOA70)'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://unsplash.com/photos/vBzJ0UFOA70](https://unsplash.com/photos/vBzJ0UFOA70)'
- en: In this article we are going to first see the differences between Data Parallelism
    (DP) and Distributed Data Parallelism (DDP) algorithms, then we will explain what
    Gradient Accumulation (GA) is to finally show how DDP and GA are implemented in
    PyTorch and how they lead to the same result.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我们将首先了解数据并行（DP）和分布式数据并行（DDP）算法之间的区别，然后解释什么是梯度累积（GA），最后展示如何在PyTorch中实现DDP和GA，并使它们得到相同的结果。
- en: Introduction
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**简介**'
- en: When training a deep neural network (DNN), one important hyperparameter is the
    batch size. Normally, the batch size should not be too big because the network
    would tend to overfit, but also not too small because it will result in slow convergence.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练深度神经网络（DNN）时，一个重要的超参数是批量大小。通常，批量大小不应过大，因为网络会倾向于过拟合，但也不应过小，因为这会导致收敛缓慢。
- en: 'When working with images of high resolution or other types of data that occupy
    a lot of memory, assuming that today most training of big DNN models are done
    on GPUs, fitting small batch size can be problematic depending on the memory of
    the available GPU. Because, as we said, small batch sizes result in slow convergence,
    there are three main methods we can use to increase the effective batch size:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 当处理高分辨率图像或其他占用大量内存的数据时，假设目前大多数大规模 DNN 模型的训练是在 GPU 上进行的，适应小批量大小可能会根据可用 GPU 的内存而变得有问题。因为，正如我们所说，小批量大小会导致收敛速度缓慢，我们可以使用三种主要方法来增加有效批量大小：
- en: Using multiple small GPUs running the model in parallel on mini-batches — DP
    or DDP algorithms
  id: totrans-13
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用多个小型 GPU 在 mini-batch 上并行运行模型——DP 或 DDP 算法
- en: Using a larger GPU (expensive)
  id: totrans-14
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用更大的 GPU（昂贵）
- en: Accumulate the gradient over multiple steps
  id: totrans-15
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在多个步骤中积累梯度
