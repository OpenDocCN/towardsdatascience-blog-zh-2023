- en: How to Optimize Your DL Data-Input Pipeline with a Custom PyTorch Operator
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何通过自定义 PyTorch 操作符优化你的深度学习数据输入管道
- en: 原文：[https://towardsdatascience.com/how-to-optimize-your-dl-data-input-pipeline-with-a-custom-pytorch-operator-7f8ea2da5206?source=collection_archive---------5-----------------------#2023-08-31](https://towardsdatascience.com/how-to-optimize-your-dl-data-input-pipeline-with-a-custom-pytorch-operator-7f8ea2da5206?source=collection_archive---------5-----------------------#2023-08-31)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/how-to-optimize-your-dl-data-input-pipeline-with-a-custom-pytorch-operator-7f8ea2da5206?source=collection_archive---------5-----------------------#2023-08-31](https://towardsdatascience.com/how-to-optimize-your-dl-data-input-pipeline-with-a-custom-pytorch-operator-7f8ea2da5206?source=collection_archive---------5-----------------------#2023-08-31)
- en: PyTorch Model Performance Analysis and Optimization — Part 5
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: PyTorch 模型性能分析与优化 — 第5部分
- en: '[](https://chaimrand.medium.com/?source=post_page-----7f8ea2da5206--------------------------------)[![Chaim
    Rand](../Images/c52659c389f167ad5d6dc139940e7955.png)](https://chaimrand.medium.com/?source=post_page-----7f8ea2da5206--------------------------------)[](https://towardsdatascience.com/?source=post_page-----7f8ea2da5206--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----7f8ea2da5206--------------------------------)
    [Chaim Rand](https://chaimrand.medium.com/?source=post_page-----7f8ea2da5206--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://chaimrand.medium.com/?source=post_page-----7f8ea2da5206--------------------------------)[![Chaim
    Rand](../Images/c52659c389f167ad5d6dc139940e7955.png)](https://chaimrand.medium.com/?source=post_page-----7f8ea2da5206--------------------------------)[](https://towardsdatascience.com/?source=post_page-----7f8ea2da5206--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----7f8ea2da5206--------------------------------)
    [Chaim Rand](https://chaimrand.medium.com/?source=post_page-----7f8ea2da5206--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F9440b37e27fe&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-optimize-your-dl-data-input-pipeline-with-a-custom-pytorch-operator-7f8ea2da5206&user=Chaim+Rand&userId=9440b37e27fe&source=post_page-9440b37e27fe----7f8ea2da5206---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----7f8ea2da5206--------------------------------)
    ·6 min read·Aug 31, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F7f8ea2da5206&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-optimize-your-dl-data-input-pipeline-with-a-custom-pytorch-operator-7f8ea2da5206&user=Chaim+Rand&userId=9440b37e27fe&source=-----7f8ea2da5206---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[查看](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F9440b37e27fe&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-optimize-your-dl-data-input-pipeline-with-a-custom-pytorch-operator-7f8ea2da5206&user=Chaim+Rand&userId=9440b37e27fe&source=post_page-9440b37e27fe----7f8ea2da5206---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----7f8ea2da5206--------------------------------)
    · 6分钟阅读 · 2023年8月31日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F7f8ea2da5206&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-optimize-your-dl-data-input-pipeline-with-a-custom-pytorch-operator-7f8ea2da5206&user=Chaim+Rand&userId=9440b37e27fe&source=-----7f8ea2da5206---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F7f8ea2da5206&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-optimize-your-dl-data-input-pipeline-with-a-custom-pytorch-operator-7f8ea2da5206&source=-----7f8ea2da5206---------------------bookmark_footer-----------)![](../Images/d20cad6621d94c0422ef9d63e6038ade.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F7f8ea2da5206&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-optimize-your-dl-data-input-pipeline-with-a-custom-pytorch-operator-7f8ea2da5206&source=-----7f8ea2da5206---------------------bookmark_footer-----------)![](../Images/d20cad6621d94c0422ef9d63e6038ade.png)'
- en: Photo by [Alexander Grey](https://unsplash.com/@sharonmccutcheon?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由 [Alexander Grey](https://unsplash.com/@sharonmccutcheon?utm_source=medium&utm_medium=referral)
    提供，发布在 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: This post is the fifth in a series of posts on the topic of performance analysis
    and optimization of GPU-based PyTorch workloads and a direct sequel to [part four](https://medium.com/towards-data-science/solving-bottlenecks-on-the-data-input-pipeline-with-pytorch-profiler-and-tensorboard-5dced134dbe9).
    In part four, we demonstrated how [PyTorch Profiler](https://pytorch.org/tutorials/recipes/recipes/profiler_recipe.html)
    and [TensorBoard](https://pytorch.org/tutorials/intermediate/tensorboard_profiler_tutorial.html)
    can be used to identify, analyze, and address performance bottlenecks in the data
    pre-processing pipeline of a DL training workload. In this post we discuss PyTorch’s
    support for [creating custom operators](https://pytorch.org/tutorials/advanced/cpp_extension.html)
    and demonstrate how it enables us to solve performance bottlenecks on the data
    input pipeline, accelerate DL workloads, and reduce the cost of training. Thanks
    go to [Yitzhak Levi](https://www.linkedin.com/in/yitzhak-levi-49a217201/) and
    [Gilad Wasserman](https://www.linkedin.com/in/gilad-wasserman-992530268/?originalSubdomain=il)
    for their contributions to this post. The code associated with this post can be
    found in [this GitHub repository](https://github.com/czmrand/jpeg-decode-and-crop/tree/main).
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 这篇文章是关于基于 GPU 的 PyTorch 工作负载性能分析和优化系列文章中的第五篇，直接续接了[第四部分](https://medium.com/towards-data-science/solving-bottlenecks-on-the-data-input-pipeline-with-pytorch-profiler-and-tensorboard-5dced134dbe9)。在第四部分中，我们演示了如何使用[PyTorch
    Profiler](https://pytorch.org/tutorials/recipes/recipes/profiler_recipe.html)和[TensorBoard](https://pytorch.org/tutorials/intermediate/tensorboard_profiler_tutorial.html)来识别、分析和解决
    DL 训练工作负载的数据预处理管道中的性能瓶颈。在这篇文章中，我们讨论了 PyTorch 对[创建自定义操作符](https://pytorch.org/tutorials/advanced/cpp_extension.html)的支持，并演示了它如何帮助我们解决数据输入管道中的性能瓶颈，加速
    DL 工作负载，并降低训练成本。感谢[伊扎克·莱维](https://www.linkedin.com/in/yitzhak-levi-49a217201/)和[吉拉德·瓦瑟曼](https://www.linkedin.com/in/gilad-wasserman-992530268/?originalSubdomain=il)对本文章的贡献。与本篇文章相关的代码可以在[这个
    GitHub 仓库](https://github.com/czmrand/jpeg-decode-and-crop/tree/main)中找到。
- en: Building PyTorch Extensions
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建 PyTorch 扩展
- en: PyTorch offers a number of ways for creating customized operations including
    [extending torch.nn](https://pytorch.org/docs/master/notes/extending.html#extending-torch-nn)
    with custom [Modules](https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module)
    and/or [Functions](https://pytorch.org/docs/master/autograd.html#function). In
    this post we are interested in PyTorch’s support for integrating customized C++
    code. This capability is important due to the fact that some operations can be
    implemented (much) more efficiently and/or easily in C++ than in Python. Using
    designated PyTorch utilities, such as [CppExtension](https://pytorch.org/docs/stable/cpp_extension.html),
    these operations can be easily included as “extensions” to PyTorch without needing
    to pull and recompile the entire PyTorch code base. For more on the motivation
    behind this feature and details of how to use it, please see [the official PyTorch
    tutorial on custom C++ and CUDA extensions](https://pytorch.org/tutorials/advanced/cpp_extension.html).
    Since our interest in this post is to accelerate the CPU-based data pre-processing
    pipeline, we will suffice with a C++ extension and not require CUDA code. In a
    future post we hope to demonstrate how to use this functionality to implement
    a custom CUDA extension in order to accelerate training code running on the GPU.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch 提供了多种创建自定义操作的方法，包括通过自定义[模块](https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module)扩展
    torch.nn 和/或自定义[函数](https://pytorch.org/docs/master/autograd.html#function)。在这篇文章中，我们关注
    PyTorch 对集成自定义 C++ 代码的支持。这一功能之所以重要，是因为某些操作在 C++ 中实现的效率和/或简便性可能远超 Python。通过指定的
    PyTorch 工具，如[CppExtension](https://pytorch.org/docs/stable/cpp_extension.html)，这些操作可以轻松地作为
    PyTorch 的“扩展”进行集成，而无需拉取和重新编译整个 PyTorch 代码库。有关这一功能的动机以及如何使用它的详细信息，请参见[官方 PyTorch
    自定义 C++ 和 CUDA 扩展教程](https://pytorch.org/tutorials/advanced/cpp_extension.html)。由于我们在本文中关注的是加速基于
    CPU 的数据预处理管道，因此我们将仅使用 C++ 扩展，不需要 CUDA 代码。在未来的文章中，我们希望展示如何使用这一功能实现自定义 CUDA 扩展，以加速在
    GPU 上运行的训练代码。
- en: Toy Example
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 示例
- en: 'In our [previous post](/solving-bottlenecks-on-the-data-input-pipeline-with-pytorch-profiler-and-tensorboard-5dced134dbe9#7fbd-af822198c08)
    we defined a data input pipeline that started with decoding a *533*x*800* JPEG
    image and then extracting a random *256*x*256* crop which, following a few additional
    transformations, is fed into the training loop. We used [PyTorch Profiler](https://pytorch.org/tutorials/recipes/recipes/profiler_recipe.html)
    and [TensorBoard](https://pytorch.org/tutorials/intermediate/tensorboard_profiler_tutorial.html)
    to measure the time associated with loading the image from file and **acknowledged
    the wastefulness of decoding.** For the sake of completeness, we copy in the code
    below:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们[之前的文章](/solving-bottlenecks-on-the-data-input-pipeline-with-pytorch-profiler-and-tensorboard-5dced134dbe9#7fbd-af822198c08)中，我们定义了一个数据输入管道，从解码一个*533*x*800*
    JPEG 图像开始，然后提取一个随机的*256*x*256*裁剪区域，经过一些额外的变换后，送入训练循环。我们使用了[PyTorch Profiler](https://pytorch.org/tutorials/recipes/recipes/profiler_recipe.html)和[TensorBoard](https://pytorch.org/tutorials/intermediate/tensorboard_profiler_tutorial.html)来测量从文件加载图像的时间，并**承认了解码的浪费**。为了完整性，我们在下面复制了代码：
- en: '[PRE0]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Recall from our previous post that the optimized average step time we reached
    was **0.72 seconds**. Presumably, were we able to decode only the crop in which
    we were interested, our pipeline would have run faster. Unfortunately, as of the
    time of this writing PyTorch does not include a function that supported this.
    However, using the tools for custom-op creation, we can define and implement our
    own function!
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 回顾我们之前的文章，我们达到的优化平均步骤时间是**0.72秒**。可以推测，如果我们能够仅解码感兴趣的裁剪区域，我们的管道将运行得更快。不幸的是，截至本文写作时，PyTorch
    并没有包含支持这一功能的函数。然而，利用自定义操作符创建工具，我们可以定义并实现自己的函数！
- en: Custom JPEG-Image-Decode-and-Crop Function
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自定义 JPEG 图像解码和裁剪函数
- en: 'The [libjpeg-turbo](https://libjpeg-turbo.org/) library is a JPEG image codec
    that includes a number of enhancements and optimizations compared to [libjpeg](http://www.ijg.org/).
    In particular, [libjpeg-turbo](https://libjpeg-turbo.org/) includes a number of
    functions that enable us to decode only a predefined crop within an image such
    as *jpeg_skip_scanlines* and *jpeg_crop_scanline*. If you are running in a conda
    environment you can install with the following command:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '[libjpeg-turbo](https://libjpeg-turbo.org/)库是一个 JPEG 图像编解码器，相比于[libjpeg](http://www.ijg.org/)，包含了许多增强和优化功能。特别是，[libjpeg-turbo](https://libjpeg-turbo.org/)包括许多功能，使我们能够仅解码图像中的预定义裁剪区域，例如*jpeg_skip_scanlines*和*jpeg_crop_scanline*。如果你在
    conda 环境中运行，可以使用以下命令进行安装：'
- en: '[PRE1]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Note that [libjpeg-turbo](https://libjpeg-turbo.org/) comes pre-installed in
    the official [AWS PyTorch 2.0 Deep Learning Docker image](https://github.com/aws/deep-learning-containers)
    that we will use in our experiments below.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，[libjpeg-turbo](https://libjpeg-turbo.org/)已预装在我们将在下面实验中使用的官方[AWS PyTorch
    2.0 深度学习 Docker 镜像](https://github.com/aws/deep-learning-containers)中。
- en: In the code block below we modify the [*decode_jpeg*](https://github.com/pytorch/vision/blob/release/0.15/torchvision/csrc/io/image/cpu/decode_jpeg.cpp#L72)
    function of [torchvision 0.15](https://pytorch.org/vision/stable/index.html) to
    decode and return a requested crop from an input JPEG encoded image.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的代码块中，我们修改了[*decode_jpeg*](https://github.com/pytorch/vision/blob/release/0.15/torchvision/csrc/io/image/cpu/decode_jpeg.cpp#L72)函数，来自[torchvision
    0.15](https://pytorch.org/vision/stable/index.html)，以解码并返回来自输入JPEG编码图像的请求裁剪。
- en: '[PRE2]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The full C++ file can be found [here](https://github.com/czmrand/jpeg-decode-and-crop/blob/main/custom_op/decode_and_crop_jpeg.cpp).
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 完整的 C++ 文件可以在[这里](https://github.com/czmrand/jpeg-decode-and-crop/blob/main/custom_op/decode_and_crop_jpeg.cpp)找到。
- en: In the next section, we will follow the steps in the PyTorch tutorial in order
    to convert this into a PyTorch operator that we can use in our pre-processing
    pipeline.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的部分中，我们将按照 PyTorch 教程中的步骤，将其转换为一个 PyTorch 操作符，以便在我们的预处理管道中使用。
- en: Deploying a PyTorch Extension
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 部署 PyTorch 扩展
- en: 'As described in the [PyTorch tutorial](https://pytorch.org/tutorials/advanced/cpp_extension.html),
    there are different ways of deploying a custom operator. There are a number of
    considerations that might factor into your deployment design. Here are a few examples
    of what we find important:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 如[PyTorch 教程](https://pytorch.org/tutorials/advanced/cpp_extension.html)中所述，有多种方式来部署自定义操作符。设计部署时需要考虑许多因素。以下是我们认为重要的一些例子：
- en: '**Just in time compilation**: In order to ensure that our C++ extension is
    compiled against the same version of PyTorch that we train with, we program our
    deployment script to compile the code right before training **within the training
    environment**.'
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**即时编译**：为了确保我们的C++扩展与我们训练时使用的PyTorch版本一致，我们编程部署脚本在训练**环境内**训练前编译代码。'
- en: '**Multi-process support**: The deployment script must support the possibility
    that our C++ extension will be loaded from multiple processes (e.g., multiple
    DataLoader workers).'
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**多进程支持**：部署脚本必须支持我们的C++扩展从多个进程（例如多个DataLoader工作者）加载的可能性。'
- en: '**Managed-training support**: Since we often train in managed training environments
    (such as [Amazon SageMaker](https://aws.amazon.com/sagemaker/)) we require that
    the deployment script support this option. (See [here](/customizing-your-cloud-based-machine-learning-training-environment-part-1-2622e10ed65a)
    for more on the topic of customizing a managed training environment.)'
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**托管训练支持**：由于我们经常在托管训练环境（如[Amazon SageMaker](https://aws.amazon.com/sagemaker/)）中进行训练，因此我们要求部署脚本支持此选项。（有关自定义托管训练环境的更多信息，请参见[这里](/customizing-your-cloud-based-machine-learning-training-environment-part-1-2622e10ed65a)。）'
- en: In the code block below we define a simple *setup.py* script that compiles and
    installs our custom function, as described [here](https://pytorch.org/tutorials/advanced/cpp_extension.html#building-with-setuptools).
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的代码块中，我们定义了一个简单的*setup.py*脚本，用于编译和安装我们的自定义函数，如[这里](https://pytorch.org/tutorials/advanced/cpp_extension.html#building-with-setuptools)所述。
- en: '[PRE3]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'We place our C++ file and the *setup.py* script in a folder named *custom_op*
    and define an *__init__.py* that ensures that the setup script is run a single
    time and by a single process:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将C++文件和*setup.py*脚本放在一个名为*custom_op*的文件夹中，并定义一个*__init__.py*，以确保安装脚本只运行一次且由单个进程执行。
- en: '[PRE4]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Last, we revise our data input pipeline to use our newly created customized
    function:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们修订了数据输入管道，使用我们新创建的自定义函数：
- en: '[PRE5]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Results
  id: totrans-35
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结果
- en: Following the optimization we have described, our step time drops to 0.48 seconds
    (from 0.72) for a 50% performance boost! Naturally, the impact of our optimization
    is directly related to the size of the raw JPEG images and our choice of crop
    size.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 根据我们描述的优化步骤，我们的步骤时间从0.72秒降到了0.48秒（提高了50%）！显然，我们优化的影响与原始JPEG图像的大小和我们选择的裁剪大小直接相关。
- en: Summary
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: Bottlenecks in the data pre-processing pipeline are common occurrences that
    can cause GPU starvation and slow down training. Given the potential cost implications,
    it is imperative that you have a variety of tools and techniques for analyzing
    and solving them. In this post we have reviewed the option of optimizing the data
    input pipeline by creating a custom C++ PyTorch extension, demonstrated its ease
    of use, and shown its potential impact. Of course, the potential gains from this
    kind of optimization mechanism will vary greatly based on the project and the
    details of the performance bottleneck.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 数据预处理管道中的瓶颈是常见的现象，可能导致GPU资源不足并减慢训练。考虑到潜在的成本影响，拥有多种工具和技术来分析和解决这些问题是至关重要的。在本文中，我们回顾了通过创建自定义C++
    PyTorch扩展来优化数据输入管道的选项，展示了其易用性，并说明了其潜在影响。当然，这种优化机制的潜在收益会根据项目和性能瓶颈的细节而大相径庭。
- en: The optimization technique discussed here joins a wide range of input pipeline
    optimization methods we have discussed in many of our blog posts. We encourage
    you to check them out (e.g., starting [here](/cloud-ml-performance-checklist-caa51e798002)).
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 这里讨论的优化技术加入了我们在许多博客文章中讨论的广泛的输入管道优化方法。我们鼓励你查看这些文章（例如，从[这里](/cloud-ml-performance-checklist-caa51e798002)开始）。
- en: '**What Next?** In [part 6](https://chaimrand.medium.com/pytorch-model-performance-analysis-and-optimization-part-6-b87412a0371b)
    of our series on performance analysis and optimization in PyTorch, we explore
    one of the more complicated types of performance issues to analyze — a bottleneck
    in the backward-propagation pass of a training step.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '**接下来做什么？** 在我们关于PyTorch性能分析和优化系列的[第6部分](https://chaimrand.medium.com/pytorch-model-performance-analysis-and-optimization-part-6-b87412a0371b)中，我们探讨了分析性能问题中较复杂的类型——训练步骤的反向传播过程中的瓶颈。'
