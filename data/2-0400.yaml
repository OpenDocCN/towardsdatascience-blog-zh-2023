- en: 'Boosting Model Accuracy: Techniques I Learned During My Machine Learning Thesis
    at Spotify (+Code Snippets)'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æå‡æ¨¡å‹å‡†ç¡®æ€§ï¼šæˆ‘åœ¨Spotifyæœºå™¨å­¦ä¹ è®ºæ–‡ä¸­å­¦åˆ°çš„æŠ€æœ¯ï¼ˆ+ä»£ç ç‰‡æ®µï¼‰
- en: åŸæ–‡ï¼š[https://towardsdatascience.com/boosting-model-accuracy-techniques-i-learned-during-my-machine-learning-thesis-at-spotify-code-8027f9c11e57](https://towardsdatascience.com/boosting-model-accuracy-techniques-i-learned-during-my-machine-learning-thesis-at-spotify-code-8027f9c11e57)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡ï¼š[https://towardsdatascience.com/boosting-model-accuracy-techniques-i-learned-during-my-machine-learning-thesis-at-spotify-code-8027f9c11e57](https://towardsdatascience.com/boosting-model-accuracy-techniques-i-learned-during-my-machine-learning-thesis-at-spotify-code-8027f9c11e57)
- en: A tech data scientistâ€™s stack to improve stubborn ML models
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ”¹å–„é¡½å›ºMLæ¨¡å‹çš„æŠ€æœ¯æ•°æ®ç§‘å­¦å®¶å·¥å…·æ ˆ
- en: '[](https://medium.com/@elalamik?source=post_page-----8027f9c11e57--------------------------------)[![Khouloud
    El Alami](../Images/58840bfe28a60892b51d40ad6ba7f5e8.png)](https://medium.com/@elalamik?source=post_page-----8027f9c11e57--------------------------------)[](https://towardsdatascience.com/?source=post_page-----8027f9c11e57--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----8027f9c11e57--------------------------------)
    [Khouloud El Alami](https://medium.com/@elalamik?source=post_page-----8027f9c11e57--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@elalamik?source=post_page-----8027f9c11e57--------------------------------)[![Khouloud
    El Alami](../Images/58840bfe28a60892b51d40ad6ba7f5e8.png)](https://medium.com/@elalamik?source=post_page-----8027f9c11e57--------------------------------)[](https://towardsdatascience.com/?source=post_page-----8027f9c11e57--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----8027f9c11e57--------------------------------)
    [Khouloud El Alami](https://medium.com/@elalamik?source=post_page-----8027f9c11e57--------------------------------)'
- en: Â·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----8027f9c11e57--------------------------------)
    Â·12 min readÂ·Aug 24, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: Â·å‘å¸ƒåœ¨[Towards Data Science](https://towardsdatascience.com/?source=post_page-----8027f9c11e57--------------------------------)
    Â·12åˆ†é’Ÿé˜…è¯»Â·2023å¹´8æœˆ24æ—¥
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '*This article is one of a two-part piece documenting my learnings from my Machine
    Learning Thesis at Spotify. Be sure to also check out* [*the second article on
    how I implemented Feature Importance in this research*](/feature-importance-analysis-with-shap-i-learned-at-spotify-aacd769831b4)*.*'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '*è¿™ç¯‡æ–‡ç« æ˜¯è®°å½•æˆ‘åœ¨Spotifyæœºå™¨å­¦ä¹ è®ºæ–‡ä¸­å­¦ä¹ å†…å®¹çš„ä¸¤éƒ¨åˆ†ä¹‹ä¸€ã€‚è¯·åŠ¡å¿…æŸ¥çœ‹* [*ç¬¬äºŒç¯‡å…³äºæˆ‘å¦‚ä½•åœ¨è¿™é¡¹ç ”ç©¶ä¸­å®ç°ç‰¹å¾é‡è¦æ€§*](/feature-importance-analysis-with-shap-i-learned-at-spotify-aacd769831b4)*.*'
- en: '[](/feature-importance-analysis-with-shap-i-learned-at-spotify-aacd769831b4?source=post_page-----8027f9c11e57--------------------------------)
    [## Feature Importance Analysis with SHAP I Learned at Spotify (with the Help
    of the Avengers)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/feature-importance-analysis-with-shap-i-learned-at-spotify-aacd769831b4?source=post_page-----8027f9c11e57--------------------------------)
    [## SHAPä¸­çš„ç‰¹å¾é‡è¦æ€§åˆ†æï¼Œæˆ‘åœ¨Spotifyå­¦ä¹ åˆ°çš„ï¼ˆåœ¨å¤ä»‡è€…çš„å¸®åŠ©ä¸‹ï¼‰'
- en: Identifying top features and understanding how they affect prediction outcomes
    of machine learning models with SHAP
  id: totrans-8
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ä½¿ç”¨SHAPè¯†åˆ«å…³é”®ç‰¹å¾å¹¶äº†è§£å®ƒä»¬å¦‚ä½•å½±å“æœºå™¨å­¦ä¹ æ¨¡å‹çš„é¢„æµ‹ç»“æœ
- en: towardsdatascience.com](/feature-importance-analysis-with-shap-i-learned-at-spotify-aacd769831b4?source=post_page-----8027f9c11e57--------------------------------)
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/feature-importance-analysis-with-shap-i-learned-at-spotify-aacd769831b4?source=post_page-----8027f9c11e57--------------------------------)
- en: In 2021, I spent 8 months building a predictive model to measure *user satisfaction*
    as part of my Thesis at Spotify.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨2021å¹´ï¼Œæˆ‘èŠ±äº†8ä¸ªæœˆæ—¶é—´æ„å»ºä¸€ä¸ªé¢„æµ‹æ¨¡å‹ï¼Œä»¥æµ‹é‡*ç”¨æˆ·æ»¡æ„åº¦*ï¼Œè¿™æ˜¯æˆ‘åœ¨Spotifyè®ºæ–‡çš„ä¸€éƒ¨åˆ†ã€‚
- en: '![](../Images/2106c3fd9d7bd76cadf0157aecf85277.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2106c3fd9d7bd76cadf0157aecf85277.png)'
- en: Image by Author
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ç‰‡ç”±ä½œè€…æä¾›
- en: 'My goal was to understand what made users satisfied with their music experience.
    To do so, I built a LightGBM classifier whose output was a binary response:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘çš„ç›®æ ‡æ˜¯ç†è§£æ˜¯ä»€ä¹ˆä½¿ç”¨æˆ·å¯¹ä»–ä»¬çš„éŸ³ä¹ä½“éªŒæ„Ÿåˆ°æ»¡æ„ã€‚ä¸ºæ­¤ï¼Œæˆ‘æ„å»ºäº†ä¸€ä¸ªLightGBMåˆ†ç±»å™¨ï¼Œå…¶è¾“å‡ºæ˜¯ä¸€ä¸ªäºŒå…ƒå“åº”ï¼š
- en: '*y = 1 â†’ the user is seemingly satisfied'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '*y = 1 â†’ ç”¨æˆ·ä¼¼ä¹æ»¡æ„'
- en: y = 0 â†’ not so much*
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: y = 0 â†’ ä¸æ€ä¹ˆæ»¡æ„*
- en: Predicting human satisfaction is a challenge because humans are by definition
    unsatisfied. Even a machine isnâ€™t so fit to decipher the mysteries of the human
    psyche. So naturally my model was as confused as one can be.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: é¢„æµ‹äººç±»æ»¡æ„åº¦æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ï¼Œå› ä¸ºäººç±»æœ¬è´¨ä¸Šæ˜¯ä¸æ»¡è¶³çš„ã€‚å³ä½¿æ˜¯æœºå™¨ä¹Ÿä¸é€‚åˆè§£è¯»äººç±»å¿ƒç†çš„å¥¥ç§˜ã€‚æ‰€ä»¥ï¼Œè‡ªç„¶åœ°ï¼Œæˆ‘çš„æ¨¡å‹ä¹Ÿé™·å…¥äº†å›°æƒ‘ã€‚
- en: From Human Predictor to Fortune Teller
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä»äººç±»é¢„æµ‹è€…åˆ°å åœå¸ˆ
- en: My accuracy score was around 0.5, which is the worst possible outcome you can
    get on a classifier. It means the algorithm has a 50% chance of predicting yes
    or no, and thatâ€™s as random as a human guess.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘çš„å‡†ç¡®ç‡çº¦ä¸º0.5ï¼Œè¿™åœ¨åˆ†ç±»å™¨ä¸­æ˜¯æœ€ç³Ÿç³•çš„ç»“æœã€‚è¿™æ„å‘³ç€ç®—æ³•æœ‰50%çš„æ¦‚ç‡é¢„æµ‹â€œæ˜¯â€æˆ–â€œå¦â€ï¼Œè¿™ä¸äººç±»çš„çŒœæµ‹ä¸€æ ·éšæœºã€‚
- en: So I spent 2 months trying and combining different techniques to improve the
    prediction of my model. In the end, I was finally able to improve my ROC score
    from 0.5 to 0.73, which was a big success!
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥æˆ‘èŠ±äº†2ä¸ªæœˆå°è¯•å’Œç»“åˆä¸åŒçš„æŠ€æœ¯æ¥æé«˜æ¨¡å‹çš„é¢„æµ‹èƒ½åŠ›ã€‚æœ€åï¼Œæˆ‘ç»ˆäºå°†ROCåˆ†æ•°ä»0.5æé«˜åˆ°0.73ï¼Œè¿™æ˜¯ä¸€ä¸ªå·¨å¤§çš„æˆåŠŸï¼
- en: In this post, I will share with you the techniques I used to significantly enhance
    the accuracy of my model. This article might come in handy whenever youâ€™re dealing
    with models that just wonâ€™t cooperate.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘å°†ä¸æ‚¨åˆ†äº«æˆ‘ç”¨æ¥æ˜¾è‘—æé«˜æ¨¡å‹å‡†ç¡®æ€§çš„æŠ€æœ¯ã€‚å½“ä½ çš„æ¨¡å‹æ— æ³•åˆä½œæ—¶ï¼Œè¿™ç¯‡æ–‡ç« å¯èƒ½ä¼šå¾ˆæœ‰ç”¨ã€‚
- en: '*Due to the confidentiality of this research, I cannot share sensitive information,
    but Iâ€™ll do my very best for it not to sound confusing.*'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '*ç”±äºè¿™é¡¹ç ”ç©¶çš„ä¿å¯†æ€§ï¼Œæˆ‘ä¸èƒ½åˆ†äº«æ•æ„Ÿä¿¡æ¯ï¼Œä½†æˆ‘ä¼šå°½åŠ›ç¡®ä¿å†…å®¹ä¸ä»¤äººå›°æƒ‘ã€‚*'
- en: But first, make sure to subscribe to my newsletter!
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ä½†é¦–å…ˆï¼Œç¡®ä¿è®¢é˜…æˆ‘çš„é€šè®¯ï¼
- en: Click on the link below & Iâ€™ll send you more **personalized content and insider
    tips** to help you on your journey to becoming a Data Scientist!
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: ç‚¹å‡»ä¸‹é¢çš„é“¾æ¥ï¼Œæˆ‘ä¼šå‘é€æ›´å¤š**ä¸ªæ€§åŒ–å†…å®¹å’Œå†…å¹•æŠ€å·§**ï¼Œå¸®åŠ©ä½ æˆä¸ºæ•°æ®ç§‘å­¦å®¶ï¼
- en: '[](https://medium.com/@elalamik/subscribe?source=post_page-----8027f9c11e57--------------------------------)
    [## Join +1k readers ğŸ’Œ that follow my journey as a Data Scientist in Tech + Spotify,
    donâ€™t miss out!'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '[## åŠ å…¥+1kè¯»è€… ğŸ’Œ å…³æ³¨æˆ‘åœ¨ç§‘æŠ€+Spotifyçš„æ•°æ®ç§‘å­¦ä¹‹æ—…ï¼Œåˆ«é”™è¿‡ï¼](https://medium.com/@elalamik/subscribe?source=post_page-----8027f9c11e57--------------------------------)'
- en: Join +1k readers ğŸ’Œ that follow my journey as a Data Scientist in Tech + Spotify,
    donâ€™t miss out! By signing up, youâ€¦
  id: totrans-25
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: åŠ å…¥+1kè¯»è€… ğŸ’Œ å…³æ³¨æˆ‘åœ¨ç§‘æŠ€+Spotifyçš„æ•°æ®ç§‘å­¦ä¹‹æ—…ï¼Œåˆ«é”™è¿‡ï¼é€šè¿‡æ³¨å†Œï¼Œä½ â€¦
- en: medium.com](https://medium.com/@elalamik/subscribe?source=post_page-----8027f9c11e57--------------------------------)
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '[medium.com](https://medium.com/@elalamik/subscribe?source=post_page-----8027f9c11e57--------------------------------)'
- en: '#0\. Data Preparation'
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '#0\. æ•°æ®å‡†å¤‡'
- en: Before diving into the methods I used, I just want to make sure you get the
    basics right first. Some of these methods rely on encoding your variables and
    preparing your data accordingly in order for them to work. Some of the code snippets
    Iâ€™ve included also reference user-defined functions I created in the data preparation
    section, so be sure to check them.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ·±å…¥æ¢è®¨æˆ‘ä½¿ç”¨çš„æ–¹æ³•ä¹‹å‰ï¼Œæˆ‘æƒ³ç¡®ä¿ä½ é¦–å…ˆæŒæ¡åŸºç¡€çŸ¥è¯†ã€‚å…¶ä¸­ä¸€äº›æ–¹æ³•ä¾èµ–äºå¯¹å˜é‡çš„ç¼–ç ä»¥åŠæ•°æ®çš„ç›¸åº”å‡†å¤‡ï¼Œä»¥ä¾¿å®ƒä»¬èƒ½å¤Ÿæ­£å¸¸å·¥ä½œã€‚æˆ‘åŒ…å«çš„ä¸€äº›ä»£ç ç‰‡æ®µä¹Ÿå¼•ç”¨äº†æˆ‘åœ¨æ•°æ®å‡†å¤‡éƒ¨åˆ†åˆ›å»ºçš„ç”¨æˆ·å®šä¹‰å‡½æ•°ï¼Œå› æ­¤è¯·åŠ¡å¿…æ£€æŸ¥å®ƒä»¬ã€‚
- en: '![](../Images/bba31f366c84e7be4e7fca230a55f625.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bba31f366c84e7be4e7fca230a55f625.png)'
- en: Hereâ€™s what my pipeline looked like in the order I implemented things
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯æˆ‘å®ç°æ­¥éª¤ä¸­çš„ç®¡é“é¡ºåº
- en: 1\. Encode Variables
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1\. ç¼–ç å˜é‡
- en: 'Make sure your variables are encoded:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: ç¡®ä¿ä½ çš„å˜é‡å·²ç¼–ç ï¼š
- en: '*Ordinal features,* so that the model preserves the ordinal information'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*åºæ•°ç‰¹å¾ï¼Œ* ä»¥ä¾¿æ¨¡å‹ä¿ç•™åºæ•°ä¿¡æ¯'
- en: '*Categorical features,* so that the model can interpret nominal data'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*ç±»åˆ«ç‰¹å¾ï¼Œ* ä»¥ä¾¿æ¨¡å‹èƒ½å¤Ÿè§£é‡Šåä¹‰æ•°æ®'
- en: 'So first, letâ€™s store our variables somewhere. Again, because the research
    is confidential, I cannot disclose the data I used, so letâ€™s use these instead:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥é¦–å…ˆï¼Œè®©æˆ‘ä»¬å°†å˜é‡å­˜å‚¨åœ¨æŸä¸ªåœ°æ–¹ã€‚åŒæ ·ï¼Œå› ä¸ºç ”ç©¶æ˜¯ä¿å¯†çš„ï¼Œæˆ‘ä¸èƒ½é€éœ²ä½¿ç”¨çš„æ•°æ®ï¼Œæ‰€ä»¥æˆ‘ä»¬å…ˆç”¨è¿™äº›æ•°æ®ï¼š
- en: '[PRE0]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Then, make sure to build the function that encodes the variables:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åï¼Œç¡®ä¿æ„å»ºç¼–ç å˜é‡çš„å‡½æ•°ï¼š
- en: '[PRE1]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Then apply that function to your list of variables. This means you need to create
    lists with strings of the name of your variables, i.e. a list for your *ordinal*
    variables, one for the *categorical* ones, and one for the *numerical* ones.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åå°†è¯¥å‡½æ•°åº”ç”¨äºä½ çš„å˜é‡åˆ—è¡¨ã€‚è¿™æ„å‘³ç€ä½ éœ€è¦åˆ›å»ºåŒ…å«å˜é‡åç§°çš„å­—ç¬¦ä¸²çš„åˆ—è¡¨ï¼Œå³ä¸º*åºæ•°*å˜é‡ã€*ç±»åˆ«*å˜é‡å’Œ*æ•°å€¼*å˜é‡åˆ†åˆ«åˆ›å»ºä¸€ä¸ªåˆ—è¡¨ã€‚
- en: '[PRE2]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 2\. Split the Data
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2\. åˆ’åˆ†æ•°æ®
- en: 'Split your dataframe to get your *train*, *validation*, and *test* sets:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: åˆ’åˆ†ä½ çš„æ•°æ®æ¡†ä»¥è·å¾—*è®­ç»ƒé›†*ã€*éªŒè¯é›†*å’Œ*æµ‹è¯•é›†*ï¼š
- en: '**Train Set** â€” to train the model on the algorithm you pick eg. LightGBM'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**è®­ç»ƒé›†** â€” ç”¨äºåœ¨ä½ é€‰æ‹©çš„ç®—æ³•ä¸Šè®­ç»ƒæ¨¡å‹ï¼Œä¾‹å¦‚ LightGBM'
- en: '**Validation Set** â€” to hyper-tune your parameters and optimize your prediction
    results'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**éªŒè¯é›†** â€” ç”¨äºè¶…è°ƒå‚æ•°å’Œä¼˜åŒ–é¢„æµ‹ç»“æœ'
- en: '**Test Set** â€” to make your final predictions'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**æµ‹è¯•é›†** â€” ç”¨äºè¿›è¡Œæœ€ç»ˆé¢„æµ‹'
- en: ğŸ”Š Keep in mind
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ğŸ”Š è®°ä½
- en: In my research, I split the data twice for two different purposes. The first
    split happens in the very beginning to create the train, validation, and test
    sets based on a user-level split. The other split happens much below when doing
    cross-validation and hyperparameter tuning.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æˆ‘çš„ç ”ç©¶ä¸­ï¼Œæˆ‘å°†æ•°æ®åˆ’åˆ†äº†ä¸¤æ¬¡ä»¥æ»¡è¶³ä¸åŒçš„ç›®çš„ã€‚ç¬¬ä¸€æ¬¡åˆ’åˆ†åœ¨ä¸€å¼€å§‹è¿›è¡Œï¼Œä»¥åŸºäºç”¨æˆ·çº§åˆ«çš„åˆ’åˆ†æ¥åˆ›å»ºè®­ç»ƒé›†ã€éªŒè¯é›†å’Œæµ‹è¯•é›†ã€‚å¦ä¸€ç§åˆ’åˆ†åˆ™åœ¨è¿›è¡Œäº¤å‰éªŒè¯å’Œè¶…å‚æ•°è°ƒæ•´æ—¶å‘ç”Ÿã€‚
- en: The initial split allows for a more flexible and randomized division of data,
    which ensures a good diversity of users in each set. The test set is set aside
    for final model evaluation, while the train and validation sets are used for model
    development and hyperparameter tuning.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: åˆå§‹åˆ’åˆ†å…è®¸æ•°æ®çš„æ›´çµæ´»å’Œéšæœºåˆ†å‰²ï¼Œç¡®ä¿æ¯ä¸ªé›†åˆä¸­ç”¨æˆ·çš„å¤šæ ·æ€§ã€‚æµ‹è¯•é›†ç•™ä½œæœ€ç»ˆæ¨¡å‹è¯„ä¼°ï¼Œè€Œè®­ç»ƒé›†å’ŒéªŒè¯é›†åˆ™ç”¨äºæ¨¡å‹å¼€å‘å’Œè¶…å‚æ•°è°ƒä¼˜ã€‚
- en: 'In my research, I used `**GroupShuffleSplit**` as follows:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æˆ‘çš„ç ”ç©¶ä¸­ï¼Œæˆ‘ä½¿ç”¨äº†`**GroupShuffleSplit**`ï¼Œå¦‚ä¸‹ï¼š
- en: '[PRE3]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '#1\. Feature Engineering'
  id: totrans-51
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '#1\. ç‰¹å¾å·¥ç¨‹'
- en: Feature engineering made a huge difference in improving the accuracy of my model.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: ç‰¹å¾å·¥ç¨‹åœ¨æé«˜æ¨¡å‹å‡†ç¡®æ€§æ–¹é¢äº§ç”Ÿäº†å·¨å¤§å·®å¼‚ã€‚
- en: When it comes to user listening satisfaction, I wanted to know whether it was
    more dependent on the user, their streaming behavior, or other factors. While
    the preliminary user data I had was meaningful, it lacked sufficient information
    gain and predictive power.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: å½“æ¶‰åŠåˆ°ç”¨æˆ·æ”¶å¬æ»¡æ„åº¦æ—¶ï¼Œæˆ‘æƒ³çŸ¥é“å®ƒæ˜¯æ›´ä¾èµ–äºç”¨æˆ·ã€ä»–ä»¬çš„æµåª’ä½“è¡Œä¸ºï¼Œè¿˜æ˜¯å…¶ä»–å› ç´ ã€‚è™½ç„¶æˆ‘æ‹¥æœ‰çš„åˆæ­¥ç”¨æˆ·æ•°æ®æœ‰æ„ä¹‰ï¼Œä½†ç¼ºä¹è¶³å¤Ÿçš„ä¿¡æ¯å¢ç›Šå’Œé¢„æµ‹èƒ½åŠ›ã€‚
- en: The most significant step in my optimization process became then to create new
    features that could better capture user satisfaction.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä¼˜åŒ–è¿‡ç¨‹ä¸­çš„æœ€é‡è¦æ­¥éª¤å˜æˆäº†åˆ›å»ºèƒ½å¤Ÿæ›´å¥½åœ°æ•æ‰ç”¨æˆ·æ»¡æ„åº¦çš„æ–°ç‰¹å¾ã€‚
- en: As the name suggests, creating new features is a *creative* process, so it means
    you need to sit down and put your domain knowledge to work, and think through
    novel ways to capture important information.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: æ­£å¦‚åå­—æ‰€ç¤ºï¼Œåˆ›å»ºæ–°ç‰¹å¾æ˜¯ä¸€ä¸ª*åˆ›é€ æ€§*çš„è¿‡ç¨‹ï¼Œè¿™æ„å‘³ç€ä½ éœ€è¦åä¸‹æ¥å‘æŒ¥ä½ çš„é¢†åŸŸçŸ¥è¯†ï¼Œæ€è€ƒæ•æ‰é‡è¦ä¿¡æ¯çš„æ–°é¢–æ–¹æ³•ã€‚
- en: 'The two main methods I used in this process were:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘åœ¨è¿™ä¸ªè¿‡ç¨‹ä¸­ä½¿ç”¨çš„ä¸¤ä¸ªä¸»è¦æ–¹æ³•æ˜¯ï¼š
- en: '**Feature Interaction.** The most important transformation I did was to combine
    already existing features together to create ratios.'
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**ç‰¹å¾äº¤äº’ã€‚** æˆ‘æ‰€åšçš„æœ€é‡è¦çš„å˜æ¢æ˜¯å°†å·²ç»å­˜åœ¨çš„ç‰¹å¾ç»„åˆåœ¨ä¸€èµ·ï¼Œä»¥åˆ›å»ºæ¯”ç‡ã€‚'
- en: '*Example: Letâ€™s say I have a feature measuring total minutes streamed, and
    another one tracking total minutes streamed when tracks are new releases. One
    thing I could do here would be to extract the minutes streamed from new releases
    and then divide it over the total minutes streamed to create a â€œnew music streams
    ratioâ€. This captures completely new information.*'
  id: totrans-58
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*ä¾‹å¦‚ï¼šå‡è®¾æˆ‘æœ‰ä¸€ä¸ªè¡¡é‡æ€»æ’­æ”¾åˆ†é’Ÿæ•°çš„ç‰¹å¾ï¼Œè¿˜æœ‰ä¸€ä¸ªè·Ÿè¸ªæ–°å‘è¡Œæ›²ç›®çš„æ€»æ’­æ”¾åˆ†é’Ÿæ•°çš„ç‰¹å¾ã€‚æˆ‘å¯ä»¥åœ¨è¿™é‡Œåšçš„äº‹æƒ…æ˜¯æå–æ¥è‡ªæ–°å‘è¡Œçš„æ’­æ”¾åˆ†é’Ÿæ•°ï¼Œç„¶åå°†å…¶é™¤ä»¥æ€»æ’­æ”¾åˆ†é’Ÿæ•°ï¼Œä»¥åˆ›å»ºâ€œæ–°éŸ³ä¹æ’­æ”¾æ¯”ç‡â€ã€‚è¿™æ•æ‰äº†å®Œå…¨æ–°çš„ä¿¡æ¯ã€‚*'
- en: '**Feature Aggregation.** Another thing I did was aggregate data over time and
    groups to create summarized features, such as the mean or standard deviation.
    This means you can create the same features but over different aggregates per
    time group.'
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**ç‰¹å¾èšåˆã€‚** æˆ‘åšçš„å¦ä¸€ä»¶äº‹æ˜¯å¯¹æ•°æ®è¿›è¡Œæ—¶é—´å’Œç»„çš„èšåˆï¼Œä»¥åˆ›å»ºæ±‡æ€»ç‰¹å¾ï¼Œå¦‚å‡å€¼æˆ–æ ‡å‡†å·®ã€‚è¿™æ„å‘³ç€ä½ å¯ä»¥åœ¨ä¸åŒçš„æ—¶é—´ç»„ä¸Šåˆ›å»ºç›¸åŒçš„ç‰¹å¾ï¼Œä½†è¦†ç›–ä¸åŒçš„èšåˆã€‚'
- en: '*Example: Averaging over the number of tracks streamed per day per playlist
    over the last 7 days, 14 days, and 30 days. And voilÃ , you just unlocked new information.*'
  id: totrans-60
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*ä¾‹å¦‚ï¼šè®¡ç®—è¿‡å»7å¤©ã€14å¤©å’Œ30å¤©æ¯ä¸ªæ’­æ”¾åˆ—è¡¨æ¯æ—¥æ’­æ”¾çš„æ›²ç›®æ•°é‡çš„å¹³å‡å€¼ã€‚ç§ï¼Œä½ åˆšåˆšè§£é”äº†æ–°ä¿¡æ¯ã€‚*'
- en: ğŸ”Š Keep in mind
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ğŸ”Š è¯·è®°ä½
- en: Feature engineering is also an iterative process. You may need to experiment
    with different combinations of features, transformations, and techniques to find
    the best set of features for your specific problem.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: ç‰¹å¾å·¥ç¨‹ä¹Ÿæ˜¯ä¸€ä¸ªè¿­ä»£çš„è¿‡ç¨‹ã€‚ä½ å¯èƒ½éœ€è¦å°è¯•ä¸åŒçš„ç‰¹å¾ç»„åˆã€å˜æ¢å’ŒæŠ€æœ¯ï¼Œä»¥æ‰¾åˆ°é€‚åˆä½ ç‰¹å®šé—®é¢˜çš„æœ€ä½³ç‰¹å¾é›†ã€‚
- en: Always validate the performance of your model with the new features on a separate
    validation set to ensure that the improvements are not due to overfitting.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: æ€»æ˜¯ç”¨æ–°çš„ç‰¹å¾åœ¨å•ç‹¬çš„éªŒè¯é›†ä¸ŠéªŒè¯æ¨¡å‹çš„è¡¨ç°ï¼Œä»¥ç¡®ä¿æ”¹è¿›ä¸æ˜¯ç”±äºè¿‡æ‹Ÿåˆã€‚
- en: '#2\. Feature Selection'
  id: totrans-64
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '#2\. ç‰¹å¾é€‰æ‹©'
- en: So I was feeding many features to my model without really knowing which ones
    were relevant. We may think that the more variables we have the better our model
    will learn, but if our model is learning from everything including garbage, this
    ends up being more harmful than anything.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: å› æ­¤ï¼Œæˆ‘å‘æ¨¡å‹è¾“å…¥äº†è®¸å¤šç‰¹å¾ï¼Œä½†å¹¶ä¸çœŸæ­£çŸ¥é“å“ªäº›æ˜¯ç›¸å…³çš„ã€‚æˆ‘ä»¬å¯èƒ½è®¤ä¸ºå˜é‡è¶Šå¤šï¼Œæ¨¡å‹å­¦ä¹ å¾—è¶Šå¥½ï¼Œä½†å¦‚æœæ¨¡å‹ä»æ‰€æœ‰å†…å®¹ä¸­å­¦ä¹ ï¼ŒåŒ…æ‹¬åƒåœ¾ï¼Œè¿™æœ€ç»ˆä¼šæ¯”ä»»ä½•ä¸œè¥¿éƒ½æ›´æœ‰å®³ã€‚
- en: 'Having too many features means that some of them could introduce noise to the
    model which is bad because it:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: ç‰¹å¾è¿‡å¤šæ„å‘³ç€å…¶ä¸­ä¸€äº›å¯èƒ½ä¼šç»™æ¨¡å‹å¼•å…¥å™ªå£°ï¼Œè¿™å¾ˆç³Ÿç³•ï¼Œå› ä¸ºå®ƒï¼š
- en: Hides the underlying patterns or relationships within the data.
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: éšè—æ•°æ®ä¸­çš„æ½œåœ¨æ¨¡å¼æˆ–å…³ç³»ã€‚
- en: Leads to overfitting as the model learns from the noise rather than the true
    relationships.
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å¯¼è‡´è¿‡æ‹Ÿåˆï¼Œå› ä¸ºæ¨¡å‹ä»å™ªå£°ä¸­å­¦ä¹ è€Œä¸æ˜¯ä»çœŸå®å…³ç³»ä¸­å­¦ä¹ ã€‚
- en: Increases complexity and slows down training.
  id: totrans-69
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å¢åŠ å¤æ‚æ€§å¹¶å‡æ…¢è®­ç»ƒé€Ÿåº¦ã€‚
- en: To avoid all these problems, we go chasing down the culprits using methods such
    as Pearsonâ€™s Correlation Coefficient, Recursive Feature Elimination, or Chi2 Test,
    amongst many others.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†é¿å…æ‰€æœ‰è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨è¯¸å¦‚çš®å°”é€Šç›¸å…³ç³»æ•°ã€é€’å½’ç‰¹å¾æ¶ˆé™¤æˆ–å¡æ–¹æ£€éªŒç­‰æ–¹æ³•è¿½è¸ªç½ªé­ç¥¸é¦–ã€‚
- en: In my case, I used the first two.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æˆ‘çš„æ¡ˆä¾‹ä¸­ï¼Œæˆ‘ä½¿ç”¨äº†å‰ä¸¤ç§æ–¹æ³•ã€‚
- en: Pearsonâ€™s Correlation Coefficient
  id: totrans-72
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: çš®å°”é€Šç›¸å…³ç³»æ•°
- en: This coef measures the **linear** relationship between two or more variables.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: è¯¥ç³»æ•°è¡¡é‡ä¸¤ä¸ªæˆ–æ›´å¤šå˜é‡ä¹‹é—´çš„**çº¿æ€§**å…³ç³»ã€‚
- en: It is the ratio between the covariance of two features and the product of their
    standard deviations. The final output is between -1 and 1 where 1 suggests a positive
    *linear* relationship between variables and -1 a negative one.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: å®ƒæ˜¯ä¸¤ä¸ªç‰¹å¾åæ–¹å·®ä¸å®ƒä»¬æ ‡å‡†å·®ä¹˜ç§¯çš„æ¯”ç‡ã€‚æœ€ç»ˆè¾“å‡ºåœ¨-1åˆ°1ä¹‹é—´ï¼Œå…¶ä¸­1è¡¨ç¤ºå˜é‡ä¹‹é—´çš„æ­£*çº¿æ€§*å…³ç³»ï¼Œè€Œ-1è¡¨ç¤ºè´Ÿå…³ç³»ã€‚
- en: 'Pearsonâ€™s correlation coefficient serves 2 purposes in feature selection:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: çš®å°”é€Šç›¸å…³ç³»æ•°åœ¨ç‰¹å¾é€‰æ‹©ä¸­æœ‰ä¸¤ä¸ªç”¨é€”ï¼š
- en: '**Filter out the least important features**, which tend to show a low correlation
    with the target variable.'
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**è¿‡æ»¤æ‰æœ€ä¸é‡è¦çš„ç‰¹å¾**ï¼Œè¿™äº›ç‰¹å¾å¾€å¾€ä¸ç›®æ ‡å˜é‡çš„ç›¸å…³æ€§è¾ƒä½ã€‚'
- en: '**Limit multicollinearity between variables** to avoid overfitting that may
    arise with data redundancy.'
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**é™åˆ¶å˜é‡ä¹‹é—´çš„å¤šé‡å…±çº¿æ€§**ä»¥é¿å…å› æ•°æ®å†—ä½™è€Œäº§ç”Ÿçš„è¿‡æ‹Ÿåˆã€‚'
- en: '**Why use it?** Itâ€™s a computationally cheap statistical method for picking
    up the intrinsic properties of dependent variables.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '**ä¸ºä»€ä¹ˆä½¿ç”¨ï¼Ÿ** è¿™æ˜¯ä¸€ä¸ªè®¡ç®—ä¾¿å®œçš„ç»Ÿè®¡æ–¹æ³•ï¼Œç”¨äºæ•æ‰å› å˜é‡çš„å†…åœ¨å±æ€§ã€‚'
- en: '**How to use it?** Correlation heatmaps point out the linear relationships
    existing between the variables'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '**å¦‚ä½•ä½¿ç”¨ï¼Ÿ** ç›¸å…³çƒ­å›¾æŒ‡å‡ºäº†å˜é‡ä¹‹é—´çš„çº¿æ€§å…³ç³»ã€‚'
- en: '[PRE4]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '[PRE5]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '[PRE6]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: ğŸš¨ Be careful with non-linear relationships!
  id: totrans-83
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ğŸš¨ å°å¿ƒéçº¿æ€§å…³ç³»ï¼
- en: Sometimes non-linear relationships between variables might also exist, which
    means you might want to be careful when filtering out multicollinear features.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: æœ‰æ—¶å˜é‡ä¹‹é—´ä¹Ÿå¯èƒ½å­˜åœ¨éçº¿æ€§å…³ç³»ï¼Œè¿™æ„å‘³ç€åœ¨è¿‡æ»¤å¤šé‡å…±çº¿æ€§ç‰¹å¾æ—¶éœ€è¦å°å¿ƒã€‚
- en: Detecting non-linear relationships can provide more nuanced and accurate insights
    into the data, which means you may want to keep them. To do so, you can use alternative
    methods such as Spearmanâ€™s Rank Correlation, Kendallâ€™s Tau, Scatter Plots, etcâ€¦
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: è¯†åˆ«éçº¿æ€§å…³ç³»å¯ä»¥æä¾›æ›´ç»†è‡´å’Œå‡†ç¡®çš„æ•°æ®æ´å¯Ÿï¼Œè¿™æ„å‘³ç€ä½ å¯èƒ½æƒ³ä¿ç•™è¿™äº›ç‰¹å¾ã€‚ä¸ºæ­¤ï¼Œä½ å¯ä»¥ä½¿ç”¨è¯¸å¦‚æ–¯çš®å°”æ›¼ç­‰çº§ç›¸å…³ã€è‚¯å¾·å°”ç§©ç›¸å…³ã€æ•£ç‚¹å›¾ç­‰æ›¿ä»£æ–¹æ³•ã€‚
- en: Recursive Feature Elimination
  id: totrans-86
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: é€’å½’ç‰¹å¾æ¶ˆé™¤
- en: It recursively narrows down features by weighting and ranking them using an
    importance algorithm. Starting with all features, it fits the chosen machine learning
    model, ranks the features, and iterates with smaller subsets until reaching the
    desired feature count (the one you initially set).
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: å®ƒé€šè¿‡ä½¿ç”¨é‡è¦æ€§ç®—æ³•å¯¹ç‰¹å¾è¿›è¡ŒåŠ æƒå’Œæ’åºï¼Œé€’å½’åœ°ç¼©å°ç‰¹å¾èŒƒå›´ã€‚ä»æ‰€æœ‰ç‰¹å¾å¼€å§‹ï¼Œå®ƒé€‚é…æ‰€é€‰çš„æœºå™¨å­¦ä¹ æ¨¡å‹ï¼Œå¯¹ç‰¹å¾è¿›è¡Œæ’åºï¼Œå¹¶ç”¨æ›´å°çš„å­é›†è¿­ä»£ï¼Œç›´åˆ°è¾¾åˆ°æ‰€éœ€çš„ç‰¹å¾æ•°é‡ï¼ˆä½ æœ€åˆè®¾å®šçš„æ•°é‡ï¼‰ã€‚
- en: '**Why use it?** The result is a ranking of features by importance, which allows
    us to kick out features with the least predictive power from the party.'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '**ä¸ºä»€ä¹ˆä½¿ç”¨ï¼Ÿ** ç»“æœæ˜¯æŒ‰é‡è¦æ€§æ’åºçš„ç‰¹å¾ï¼Œè¿™ä½¿æˆ‘ä»¬èƒ½å¤Ÿå°†é¢„æµ‹èƒ½åŠ›æœ€å·®çš„ç‰¹å¾è¸¢å‡ºå±€ã€‚'
- en: ğŸš¨ Be careful with encoding!
  id: totrans-89
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ğŸš¨ å°å¿ƒç¼–ç ï¼
- en: RFE requires prior numerical encoding of categorical variables in order to work,
    so refer back to the initial section for encoding variables.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: RFEéœ€è¦å¯¹åˆ†ç±»å˜é‡è¿›è¡Œå…ˆå‰çš„æ•°å€¼ç¼–ç æ‰èƒ½å·¥ä½œï¼Œæ‰€ä»¥è¯·å‚è€ƒåˆå§‹éƒ¨åˆ†è¿›è¡Œå˜é‡ç¼–ç ã€‚
- en: '[PRE7]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'I combined the results of these 2 methods when filtering out the least important
    features:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘åœ¨ç­›é€‰æœ€ä¸é‡è¦çš„ç‰¹å¾æ—¶ï¼Œç»“åˆäº†è¿™ä¸¤ç§æ–¹æ³•çš„ç»“æœï¼š
- en: Using Pearsonâ€™s Correlation Coefficient, I found no strong linearity between
    the dependent features and the target variable. So I kept all of them *(I was
    also scared of removing non-linear relationships).*
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä½¿ç”¨çš®å°”é€Šç›¸å…³ç³»æ•°ï¼Œæˆ‘æ²¡æœ‰å‘ç°å› å˜é‡å’Œç›®æ ‡å˜é‡ä¹‹é—´æœ‰å¼ºçº¿æ€§å…³ç³»ã€‚å› æ­¤ï¼Œæˆ‘ä¿ç•™äº†æ‰€æœ‰ç‰¹å¾ *(æˆ‘ä¹Ÿå®³æ€•åˆ é™¤éçº¿æ€§å…³ç³»)ã€‚*
- en: Using Recursive Feature Elimination, I removed the lowest-ranked features *(because
    why not)*.
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä½¿ç”¨é€’å½’ç‰¹å¾æ¶ˆé™¤ï¼Œæˆ‘ç§»é™¤äº†æœ€ä½æ’åçš„ç‰¹å¾ *(å› ä¸ºä¸ºä»€ä¹ˆä¸å‘¢)*ã€‚
- en: '#3\. Hyperparameter Tuning'
  id: totrans-95
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '#3\. è¶…å‚æ•°è°ƒæ•´'
- en: Hyperparameter tuning is a mandatory stop when optimizing a machine learning
    model. Itâ€™s basically the part where you look for one of the best combinations
    of parameters that can give you great performance for your model.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: è¶…å‚æ•°è°ƒæ•´æ˜¯ä¼˜åŒ–æœºå™¨å­¦ä¹ æ¨¡å‹æ—¶çš„å¼ºåˆ¶æ­¥éª¤ã€‚åŸºæœ¬ä¸Šï¼Œè¿™æ˜¯å¯»æ‰¾èƒ½å¤Ÿä¸ºä½ çš„æ¨¡å‹æä¾›æœ€ä½³æ€§èƒ½çš„å‚æ•°ç»„åˆçš„è¿‡ç¨‹ã€‚
- en: 'In my research, I used a two-step strategy combining `**GroupKFold**` cross-validation
    with `**RandomizedSearchCV**`for hyperparameter tuning, which was the best combination
    given that:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æˆ‘çš„ç ”ç©¶ä¸­ï¼Œæˆ‘ä½¿ç”¨äº†ä¸€ç§ç»“åˆ`**GroupKFold**`äº¤å‰éªŒè¯å’Œ`**RandomizedSearchCV**`çš„ä¸¤æ­¥ç­–ç•¥è¿›è¡Œè¶…å‚æ•°è°ƒä¼˜ï¼Œè¿™æ˜¯ä¸€ç§æœ€ä½³ç»„åˆï¼Œå› ä¸ºï¼š
- en: The sample data was very large *(300k users).*
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ç¤ºä¾‹æ•°æ®é‡éå¸¸å¤§*(30ä¸‡ç”¨æˆ·)*ã€‚
- en: The user data needed to be split appropriately *(we donâ€™t want to find Kâ€™s streaming
    data in all splits, no no)*.
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ç”¨æˆ·æ•°æ®éœ€è¦é€‚å½“åœ°æ‹†åˆ†*(æˆ‘ä»¬ä¸æƒ³åœ¨æ‰€æœ‰åˆ†å‰²ä¸­éƒ½æ‰¾åˆ°Kçš„æµæ•°æ®ï¼Œä¸ä¸)*ã€‚
- en: '**Step 1:** Preventing Data Leakage with GroupKFold'
  id: totrans-100
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**ç¬¬1æ­¥ï¼š** ä½¿ç”¨GroupKFoldé˜²æ­¢æ•°æ®æ³„æ¼'
- en: My data consisted of multiple records for individual users. Because data gets
    split for hyperparameter tuning, I needed to prevent data leakage by ensuring
    that information from the same user was not split between the training and validation
    sets.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘çš„æ•°æ®åŒ…å«äº†å¤šä¸ªç”¨æˆ·çš„è®°å½•ã€‚ç”±äºæ•°æ®ä¼šè¢«ç”¨äºè¶…å‚æ•°è°ƒä¼˜ï¼Œæˆ‘éœ€è¦é˜²æ­¢æ•°æ®æ³„æ¼ï¼Œç¡®ä¿åŒä¸€ç”¨æˆ·çš„ä¿¡æ¯ä¸ä¼šåœ¨è®­ç»ƒé›†å’ŒéªŒè¯é›†ä¸­è¢«åˆ†å¼€ã€‚
- en: The best method to do so is `**GroupKFold**`, which divides the data over a
    training and validating set randomly using different portions of the dataset at
    each iteration. This creates separate sets with distinct and non-overlapping users.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€ä½³çš„æ–¹æ³•æ˜¯`**GroupKFold**`ï¼Œå®ƒé€šè¿‡åœ¨æ¯æ¬¡è¿­ä»£ä¸­ä½¿ç”¨æ•°æ®é›†çš„ä¸åŒéƒ¨åˆ†ï¼Œéšæœºå°†æ•°æ®åˆ†ä¸ºè®­ç»ƒé›†å’ŒéªŒè¯é›†ã€‚è¿™ä¼šåˆ›å»ºå…·æœ‰ä¸åŒä¸”ä¸é‡å ç”¨æˆ·çš„ç‹¬ç«‹é›†ã€‚
- en: This is crucial for achieving a reliable performance assessment, as you want
    your model to be tested on entirely unseen users, not just new data from users
    it has seen during training.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å¯¹äºå®ç°å¯é çš„æ€§èƒ½è¯„ä¼°è‡³å…³é‡è¦ï¼Œå› ä¸ºä½ å¸Œæœ›æ¨¡å‹åœ¨å®Œå…¨æœªè§è¿‡çš„ç”¨æˆ·ä¸Šè¿›è¡Œæµ‹è¯•ï¼Œè€Œä¸ä»…ä»…æ˜¯è®­ç»ƒè¿‡ç¨‹ä¸­è§è¿‡çš„ç”¨æˆ·çš„æ–°æ•°æ®ã€‚
- en: '**Step 2:** Efficient Hyperparameter Tuning with RandomizedSearchCV'
  id: totrans-104
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**ç¬¬2æ­¥ï¼š** ä½¿ç”¨RandomizedSearchCVè¿›è¡Œé«˜æ•ˆçš„è¶…å‚æ•°è°ƒä¼˜'
- en: My sample data was around 300k users, which was the largest one I could afford
    without triggering a system crash, given my computational capabilities. Using
    `**RandomizedSearchCV**` is much more efficient when your sample is this large.
    It works wonders.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘çš„æ ·æœ¬æ•°æ®å¤§çº¦æœ‰30ä¸‡ç”¨æˆ·ï¼Œè¿™æ˜¯åœ¨æˆ‘çš„è®¡ç®—èƒ½åŠ›ä¸‹ä¸ä¼šå¼•å‘ç³»ç»Ÿå´©æºƒçš„æœ€å¤§è§„æ¨¡ã€‚ä½¿ç”¨`**RandomizedSearchCV**`åœ¨è¿™ç§å¤§æ ·æœ¬æƒ…å†µä¸‹è¦é«˜æ•ˆå¾—å¤šã€‚æ•ˆæœæ˜¾è‘—ã€‚
- en: Instead of searching through all possible hyperparameter combinations like a
    traditional grid search would do, it randomly samples a subset of the hyperparameter
    space. Then it evaluates the performance of the selected combinations using cross-validation.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: å®ƒä¸ä¼šåƒä¼ ç»Ÿçš„ç½‘æ ¼æœç´¢é‚£æ ·æœç´¢æ‰€æœ‰å¯èƒ½çš„è¶…å‚æ•°ç»„åˆï¼Œè€Œæ˜¯éšæœºæŠ½å–è¶…å‚æ•°ç©ºé—´çš„ä¸€ä¸ªå­é›†ã€‚ç„¶åä½¿ç”¨äº¤å‰éªŒè¯è¯„ä¼°æ‰€é€‰ç»„åˆçš„æ€§èƒ½ã€‚
- en: âœ¨Results
  id: totrans-107
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: âœ¨ç»“æœ
- en: 'By combining these two, I performed hyperparameter tuning on multiple data
    subsets with non-overlapping users. This way I was able to:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: é€šè¿‡ç»“åˆè¿™ä¸¤è€…ï¼Œæˆ‘å¯¹å¤šä¸ªæ•°æ®å­é›†è¿›è¡Œäº†è¶…å‚æ•°è°ƒä¼˜ï¼Œè¿™äº›å­é›†çš„ç”¨æˆ·ä¸é‡å ã€‚è¿™æ ·æˆ‘èƒ½å¤Ÿï¼š
- en: Address data leakage concerns
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: è§£å†³æ•°æ®æ³„æ¼é—®é¢˜
- en: Ensure computational efficiency
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ç¡®ä¿è®¡ç®—æ•ˆç‡
- en: Implement a robust basis for hyperparameter selection
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å®æ–½ç¨³å¥çš„è¶…å‚æ•°é€‰æ‹©åŸºç¡€
- en: '[PRE8]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: After weâ€™re done identifying the best hyperparameters through `**RandomizedSearchCV**`
    and `**GroupKFold**`, we use the initial train and validation sets from `**GroupShuffleSplit**`
    to train the final model with the selected hyperparameters.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨é€šè¿‡`**RandomizedSearchCV**`å’Œ`**GroupKFold**`ç¡®å®šæœ€ä½³è¶…å‚æ•°åï¼Œæˆ‘ä»¬ä½¿ç”¨`**GroupShuffleSplit**`çš„åˆå§‹è®­ç»ƒé›†å’ŒéªŒè¯é›†æ¥è®­ç»ƒæœ€ç»ˆæ¨¡å‹ã€‚
- en: Remember that `split_df()` function we created at the very beginning of this
    article? Weâ€™re using it in this step to get our data split.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: è¿˜è®°å¾—æˆ‘ä»¬åœ¨è¿™ç¯‡æ–‡ç« ä¸€å¼€å§‹åˆ›å»ºçš„`split_df()`å‡½æ•°å—ï¼Ÿæˆ‘ä»¬åœ¨è¿™ä¸€æ­¥ä¸­ä½¿ç”¨å®ƒæ¥æ‹†åˆ†æ•°æ®ã€‚
- en: '[PRE9]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Then we plug in the best parameters found with hyperparameter tuning.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åï¼Œæˆ‘ä»¬æ’å…¥é€šè¿‡è¶…å‚æ•°è°ƒä¼˜æ‰¾åˆ°çš„æœ€ä½³å‚æ•°ã€‚
- en: '[PRE10]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: ğŸ”Š Keep in mind
  id: totrans-118
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ğŸ”Š è¯·è®°ä½
- en: Iâ€™m mentioning this because it confused me a lot while I was working on this
    research. The `eval_set` is used for monitoring the model's performance on a specific
    validation set during training. This is different than cross-validation, which
    evaluates the model's ability to generalize across multiple training-validation
    splits.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘æåˆ°è¿™ä¸€ç‚¹æ˜¯å› ä¸ºåœ¨ç ”ç©¶è¿‡ç¨‹ä¸­è¿™è®©æˆ‘å¾ˆå›°æƒ‘ã€‚`eval_set`ç”¨äºåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ç›‘æ§æ¨¡å‹åœ¨ç‰¹å®šéªŒè¯é›†ä¸Šçš„è¡¨ç°ã€‚è¿™ä¸äº¤å‰éªŒè¯ä¸åŒï¼Œäº¤å‰éªŒè¯è¯„ä¼°æ¨¡å‹åœ¨å¤šä¸ªè®­ç»ƒ-éªŒè¯åˆ†å‰²ä¸Šçš„æ³›åŒ–èƒ½åŠ›ã€‚
- en: '#4\. Data Generation'
  id: totrans-120
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '#4. æ•°æ®ç”Ÿæˆ'
- en: After implementing all the previous steps, my model still needed an extra boost.
    Because some groups in my data were more underrepresented than others, my model
    had a wee bit of a struggle to generalize through them.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å®æ–½æ‰€æœ‰ä¹‹å‰çš„æ­¥éª¤åï¼Œæˆ‘çš„æ¨¡å‹ä»ç„¶éœ€è¦é¢å¤–çš„æå‡ã€‚ç”±äºæˆ‘çš„æ•°æ®ä¸­æŸäº›ç»„çš„ä»£è¡¨æ€§è¾ƒå¼±ï¼Œæˆ‘çš„æ¨¡å‹åœ¨è¿™äº›ç»„ä¸Šçš„æ³›åŒ–èƒ½åŠ›æœ‰äº›åƒåŠ›ã€‚
- en: So I made sure to generate a larger random sample of users for all of the underrepresented
    sets. This last step gave my model exactly what it needed to properly generalize
    all that beautiful wisdom from the data and make reliable predictions.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥æˆ‘ç¡®ä¿ä¸ºæ‰€æœ‰ä»£è¡¨æ€§ä¸è¶³çš„ç”¨æˆ·ç»„ç”Ÿæˆäº†ä¸€ä¸ªæ›´å¤§çš„éšæœºæ ·æœ¬ã€‚è¿™ä¸€æ­¥éª¤æ­£å¥½ç»™äº†æˆ‘çš„æ¨¡å‹æ‰€éœ€çš„å†…å®¹ï¼Œä½¿å…¶èƒ½å¤Ÿæ­£ç¡®åœ°æ³›åŒ–æ•°æ®ä¸­çš„æ‰€æœ‰ç¾å¥½æ™ºæ…§ï¼Œå¹¶åšå‡ºå¯é çš„é¢„æµ‹ã€‚
- en: Last Word
  id: totrans-123
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æœ€åçš„ä¸€å¥è¯
- en: Keep in mind that the process of optimizing a model is an iterative one, which
    means that you may have to combine and repeat some of these methods until you
    reach a satisfying performance.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: è¯·è®°ä½ï¼Œä¼˜åŒ–æ¨¡å‹çš„è¿‡ç¨‹æ˜¯ä¸€ä¸ªè¿­ä»£çš„è¿‡ç¨‹ï¼Œè¿™æ„å‘³ç€ä½ å¯èƒ½éœ€è¦ç»“åˆå’Œé‡å¤ä¸€äº›æ–¹æ³•ï¼Œç›´åˆ°è¾¾åˆ°ä»¤äººæ»¡æ„çš„æ€§èƒ½ã€‚
- en: Optimization Methods Recap
  id: totrans-125
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä¼˜åŒ–æ–¹æ³•å›é¡¾
- en: '**Feature Engineering** â€” Creating new features using different methods such
    as feature aggregation, transformation, temporal data encoding, standardization
    and more can introduce new information to the data.'
  id: totrans-126
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**ç‰¹å¾å·¥ç¨‹** â€” ä½¿ç”¨ä¸åŒçš„æ–¹æ³•åˆ›å»ºæ–°ç‰¹å¾ï¼Œä¾‹å¦‚ç‰¹å¾èšåˆã€è½¬æ¢ã€æ—¶é—´æ•°æ®ç¼–ç ã€æ ‡å‡†åŒ–ç­‰ï¼Œå¯ä»¥ä¸ºæ•°æ®å¼•å…¥æ–°çš„ä¿¡æ¯ã€‚'
- en: '**Feature Selection** â€” After creating new features, evaluate their importance
    and remove irrelevant or redundant features that do not contribute to model performance.
    Some methods include Pearsonâ€™s Correlation Coefficient, Recursive Feature Elimination,
    or Chi2.'
  id: totrans-127
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**ç‰¹å¾é€‰æ‹©** â€” åœ¨åˆ›å»ºæ–°ç‰¹å¾åï¼Œè¯„ä¼°å®ƒä»¬çš„é‡è¦æ€§ï¼Œå¹¶ç§»é™¤é‚£äº›å¯¹æ¨¡å‹æ€§èƒ½æ²¡æœ‰è´¡çŒ®çš„æ— å…³æˆ–å†—ä½™ç‰¹å¾ã€‚ä¸€äº›æ–¹æ³•åŒ…æ‹¬çš®å°”é€Šç›¸å…³ç³»æ•°ã€é€’å½’ç‰¹å¾æ¶ˆé™¤æˆ–å¡æ–¹æ£€éªŒã€‚'
- en: '**Hyperparameter Tuning** â€” Preventing Data Leakage with GroupKFold then searching
    for the best parameters with RandomisedSearchCV in a computationally efficient
    way.'
  id: totrans-128
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**è¶…å‚æ•°è°ƒä¼˜** â€” ä½¿ç”¨GroupKFoldé˜²æ­¢æ•°æ®æ³„æ¼ï¼Œç„¶åä»¥è®¡ç®—ä¸Šé«˜æ•ˆçš„æ–¹å¼ä½¿ç”¨RandomisedSearchCVå¯»æ‰¾æœ€ä½³å‚æ•°ã€‚'
- en: '**Data Generation â€”** Make sure groups are equally represented in the sample
    and if needed and possible, increase the sample size to cover a larger sample
    of data points.'
  id: totrans-129
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**æ•°æ®ç”Ÿæˆ** â€” ç¡®ä¿æ ·æœ¬ä¸­çš„å„ç»„å¾—åˆ°å‡ç­‰ä»£è¡¨ï¼Œå¦‚æœéœ€è¦ä¸”å¯èƒ½çš„è¯ï¼Œå¢åŠ æ ·æœ¬å¤§å°ä»¥æ¶µç›–æ›´å¤šçš„æ•°æ®ç‚¹ã€‚'
- en: I have GIFTS for you ğŸ!
  id: totrans-130
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æˆ‘ä¸ºä½ å‡†å¤‡äº†ç¤¼ç‰© ğŸï¼
- en: Sign up to my [**newsletter**](https://levelupwithk.substack.com/) **Kâ€™s DataLadder**
    and youâ€™ll automatically get my **ultimate SQL cheat sheet** with all the queries
    I use every day in my job in big tech + another secret gift!
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: æ³¨å†Œæˆ‘çš„ [**æ–°é—»é€šè®¯**](https://levelupwithk.substack.com/) **Kçš„DataLadder**ï¼Œä½ å°†è‡ªåŠ¨è·å¾—æˆ‘çš„
    **ç»ˆæSQLå¤‡å¿˜å•**ï¼Œå…¶ä¸­åŒ…å«æˆ‘æ¯å¤©åœ¨å¤§ç§‘æŠ€å…¬å¸å·¥ä½œä¸­ä½¿ç”¨çš„æ‰€æœ‰æŸ¥è¯¢+å¦ä¸€ä¸ªç§˜å¯†ç¤¼ç‰©ï¼
- en: I share each week what itâ€™s like to be a Data Scientist in Tech, alongside practical
    tips, skills, and stories all meant to help you level up â€” because no one really
    knows until theyâ€™re in it!
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘æ¯å‘¨åˆ†äº«åœ¨ç§‘æŠ€è¡Œä¸šä½œä¸ºæ•°æ®ç§‘å­¦å®¶çš„ç»å†ï¼ŒåŒ…æ‹¬å®ç”¨æŠ€å·§ã€æŠ€èƒ½å’Œæ•…äº‹ï¼Œæ‰€æœ‰è¿™äº›éƒ½æ˜¯ä¸ºäº†å¸®åŠ©ä½ æå‡æ°´å¹³â€”â€”å› ä¸ºæ²¡æœ‰äººçœŸæ­£çŸ¥é“ï¼Œç›´åˆ°ä»–ä»¬äº²èº«ä½“éªŒï¼
- en: If you havenâ€™t done that already
  id: totrans-133
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å¦‚æœä½ è¿˜æ²¡æœ‰è¿™æ ·åšçš„è¯
- en: Subscribe to my[**YouTube**](https://rebrand.ly/tdf62uv)channel. New video coming
    up very soon!
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è®¢é˜…æˆ‘çš„ [**YouTube**](https://rebrand.ly/tdf62uv)é¢‘é“ã€‚æ–°è§†é¢‘å³å°†ä¸Šçº¿ï¼
- en: Follow meon[**Instagram**](https://www.instagram.com/elalamikhouloud/), [**LinkedIn**](https://www.linkedin.com/in/elalamik/),
    [**X**](https://twitter.com/elalamik), whatever works for you
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å…³æ³¨æˆ‘çš„ [**Instagram**](https://www.instagram.com/elalamikhouloud/)ã€[**LinkedIn**](https://www.linkedin.com/in/elalamik/)ã€[**X**](https://twitter.com/elalamik)ï¼Œéšä½ å–œæ¬¢ã€‚
- en: See you soon!
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ä¹…è§ï¼
