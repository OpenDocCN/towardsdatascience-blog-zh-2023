- en: Challenges of Detecting AI-Generated Text
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 检测 AI 生成文本的挑战
- en: 原文：[https://towardsdatascience.com/challenges-of-detecting-ai-generated-text-6d85bf779448?source=collection_archive---------1-----------------------#2023-09-27](https://towardsdatascience.com/challenges-of-detecting-ai-generated-text-6d85bf779448?source=collection_archive---------1-----------------------#2023-09-27)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/challenges-of-detecting-ai-generated-text-6d85bf779448?source=collection_archive---------1-----------------------#2023-09-27](https://towardsdatascience.com/challenges-of-detecting-ai-generated-text-6d85bf779448?source=collection_archive---------1-----------------------#2023-09-27)
- en: We’ll take an in-depth look at the challenges of detecting AI-generated text,
    and the effectiveness of the techniques used in practice.
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们将深入探讨检测 AI 生成文本的挑战，以及在实际应用中所使用技术的有效性。
- en: '[](https://medium.com/@dhruvbird?source=post_page-----6d85bf779448--------------------------------)[![Dhruv
    Matani](../Images/d63bf7776c28a29c02b985b1f64abdd3.png)](https://medium.com/@dhruvbird?source=post_page-----6d85bf779448--------------------------------)[](https://towardsdatascience.com/?source=post_page-----6d85bf779448--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----6d85bf779448--------------------------------)
    [Dhruv Matani](https://medium.com/@dhruvbird?source=post_page-----6d85bf779448--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@dhruvbird?source=post_page-----6d85bf779448--------------------------------)[![Dhruv
    Matani](../Images/d63bf7776c28a29c02b985b1f64abdd3.png)](https://medium.com/@dhruvbird?source=post_page-----6d85bf779448--------------------------------)[](https://towardsdatascience.com/?source=post_page-----6d85bf779448--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----6d85bf779448--------------------------------)
    [Dhruv Matani](https://medium.com/@dhruvbird?source=post_page-----6d85bf779448--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F63f5d5495279&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fchallenges-of-detecting-ai-generated-text-6d85bf779448&user=Dhruv+Matani&userId=63f5d5495279&source=post_page-63f5d5495279----6d85bf779448---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----6d85bf779448--------------------------------)
    ·15 min read·Sep 27, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F6d85bf779448&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fchallenges-of-detecting-ai-generated-text-6d85bf779448&user=Dhruv+Matani&userId=63f5d5495279&source=-----6d85bf779448---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F63f5d5495279&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fchallenges-of-detecting-ai-generated-text-6d85bf779448&user=Dhruv+Matani&userId=63f5d5495279&source=post_page-63f5d5495279----6d85bf779448---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----6d85bf779448--------------------------------)
    ·15 min read·Sep 27, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F6d85bf779448&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fchallenges-of-detecting-ai-generated-text-6d85bf779448&user=Dhruv+Matani&userId=63f5d5495279&source=-----6d85bf779448---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F6d85bf779448&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fchallenges-of-detecting-ai-generated-text-6d85bf779448&source=-----6d85bf779448---------------------bookmark_footer-----------)![](../Images/b9dfe546990ad9a0e1f0ed38878e5470.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F6d85bf779448&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fchallenges-of-detecting-ai-generated-text-6d85bf779448&source=-----6d85bf779448---------------------bookmark_footer-----------)![](../Images/b9dfe546990ad9a0e1f0ed38878e5470.png)'
- en: Photo by [Houcine Ncib](https://unsplash.com/@houcinencibphotography?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由 [Houcine Ncib](https://unsplash.com/@houcinencibphotography?utm_source=medium&utm_medium=referral)
    提供，来源于 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: Co-authored with [Naresh Singh](https://medium.com/@brocolishbroxoli).
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 与 [Naresh Singh](https://medium.com/@brocolishbroxoli) 共同撰写。
- en: Table of contents
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 目录
- en: '[Introduction](#86be)'
  id: totrans-11
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[介绍](#86be)'
- en: '[Building an intuition for text source detection](#64d4)'
  id: totrans-12
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[建立文本来源检测的直觉](#64d4)'
- en: '[What is the perplexity of a language model?](#542b)'
  id: totrans-13
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[语言模型的困惑度是什么？](#542b)'
- en: '[Computing the perplexity of a language model’s prediction](#dc6a)'
  id: totrans-14
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[计算语言模型预测的困惑度](#dc6a)'
- en: '[Detecting AI-generated text](#f256)'
  id: totrans-15
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[检测 AI 生成文本](#f256)'
- en: '[Misinformation](#b56a)'
  id: totrans-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[错误信息](#b56a)'
- en: '[What’s next?](#a778)'
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[接下来是什么？](#a778)'
- en: '[Conclusion](#8446)'
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[结论](#8446)'
- en: Introduction
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: AI-assisted technologies for writing articles or posts are everywhere now! ChatGPT
    has unlocked numerous applications of language-based AI, and the use of AI for
    any kind of content generation has gone through the roof.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 现在写文章或帖子用的AI辅助技术无处不在！ChatGPT开启了基于语言的AI的众多应用，AI在任何类型内容生成中的使用已经达到了空前的高度。
- en: However, in school assignments such as creative writing, students are expected
    to create their own content. However, due to the popularity and effectiveness
    of AI for such tasks, they might be tempted to use it. In such cases, it is important
    for teachers to have access to reliable and trustworthy tools to detect AI-generated
    content.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在诸如创意写作之类的学校作业中，学生需要创建自己的内容。然而，由于AI在这些任务中的流行和有效性，学生可能会被诱惑去使用它。在这种情况下，教师拥有能够检测AI生成内容的可靠工具就显得尤为重要。
- en: This article aims to provide an intuition as well as the technical specification
    of building such a tool. It is intended for the readers who want to understand
    how AI detection works intuitively as well as the technical audience who wants
    to build such a tool.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 本文旨在提供对构建此类工具的直观理解以及技术规格。它面向那些希望直观理解AI检测工作原理的读者以及希望构建此类工具的技术观众。
- en: Let’s jump straight in!
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们直接进入主题！
- en: Building an intuition for text source detection
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 建立文本来源检测的直观理解
- en: At a high level, we’re trying to answer the question, *“How likely is it that
    an AI language model such as GPT-3 has generated all or part of this text?”*
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 从高层次来看，我们试图回答的问题是，*“AI语言模型（如GPT-3）生成全部或部分文本的可能性有多大？”*
- en: If you step back, you will realize this is a typical daily scenario. For example,
    how likely would it be for your mother to say the following sentence to you?
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你退一步看，你会意识到这是一种典型的日常情境。例如，你母亲对你说以下句子的可能性有多大？
- en: Dear, please go to bed before 8:00 pm.
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 亲爱的，请在晚上8点之前上床睡觉。
- en: Dear, please go to bed after 11:00 pm.
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 亲爱的，请在晚上11点后上床睡觉。
- en: We would guess that the former is much more likely than the latter because you
    have built an understanding of the world around you and have a sense of which
    events are more likely to occur.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 我们猜测前者的可能性远高于后者，因为你对周围世界已经有了一定的理解，并且对哪些事件更可能发生有了感觉。
- en: This is exactly how a language model works. Language models learn something
    about the world around them, specifically language. They learn to predict the
    next token or word given an incomplete sentence.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 这正是语言模型的工作原理。语言模型学习有关周围世界的知识，特别是语言。它们学习在给定不完整句子的情况下预测下一个标记或单词。
- en: In the example above, if you’re told that your mother is saying something, and
    what has been said so far is *“Dear, please go to bed”*, then the most likely
    continuation of the sentence is going to be “before 8:00 pm”, and not “after 11:00
    pm”. In technical terms, we say that you’d be more *perplexed* to hear the 2nd
    sentence as opposed to the 1st one.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在上面的例子中，如果你被告知你母亲正在说话，而迄今为止所说的是*“亲爱的，请在睡觉”*，那么这个句子的最可能的继续就是“在晚上8点之前”，而不是“在晚上11点之后”。用技术术语来说，我们说你会对听到第二句话而非第一句话感到更多的*困惑*。
- en: Let’s dig deeper into what perplexity means in the context of a language mode.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们深入探讨一下在语言模型的背景下困惑度的含义。
- en: What is the perplexity of a language model?
  id: totrans-33
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 语言模型的困惑度是什么？
- en: According to [dictionary.com](https://www.dictionary.com/browse/perplexity),
    perplexity is defined as
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 根据 [dictionary.com](https://www.dictionary.com/browse/perplexity)，困惑度被定义为
- en: the state of being perplexed; confusion; uncertainty.
  id: totrans-35
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 困惑的状态；混乱；不确定性。
- en: In the real world, if you encounter a situation that you don’t expect, you will
    be more perplexed than if you encounter a situation that you expect. For example,
    when driving on a road, if you see a traffic light, then you’re less likely to
    be perplexed as opposed to if you see a goat crossing the street.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在现实世界中，如果你遇到一个你没有预料到的情况，你会比遇到一个你预料到的情况感到更多的困惑。例如，当你在路上行驶时，如果你看到一个交通信号灯，那么你会比看到一只穿过街道的山羊时感到更少的困惑。
- en: Similarly, for a language model that is trying to predict the next word in a
    sentence, we say that the model will perplex us if it completes the sentence using
    a word that we don’t expect v/s if it uses a word that we expect. Some examples.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，对于一个试图预测句子中下一个单词的语言模型来说，我们说如果模型用一个我们没想到的单词来完成句子，它会让我们感到困惑，相比之下，如果它使用我们期待的单词。以下是一些例子。
- en: '**Sentences with a low perplexity would look like the following**'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '**低困惑度的句子看起来会是这样的**'
- en: It’s a sunny day outside.
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 外面的天气晴朗。
- en: I’m sorry I missed the flight and was unable to reach the national park in time.
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对不起，我错过了航班，未能及时到达国家公园。
- en: '**Sentences with a high perplexity would look like the following**'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '**具有高困惑度的句子可能如下所示**'
- en: It’s a bread day outside.
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 外面的天气很好。
- en: I’m convenient missed light outside and could not reach national park.
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我错过了光线，无法到达国家公园。
- en: Next, let’s look at how we can compute the perplexity of a prediction made by
    the language model.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们看看如何计算语言模型做出的预测的困惑度。
- en: Computing the perplexity of a language model’s prediction
  id: totrans-45
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 计算语言模型预测的困惑度
- en: The perplexity of a language model is related to the probability of being able
    to unsurprisingly predict the next token (word) of a sentence.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 语言模型的困惑度与能够意料之外地预测句子下一个标记（单词）的概率相关。
- en: Suppose we train a language model with a vocabulary of 6600 tokens and run a
    single prediction step to have the model predict the next token in a sentence.
    Let’s assume that the likelihood of picking this token is 5/6600 (i.e., this token
    was not very likely). Its perplexity is the inverse of the probability which is
    6600/5 = 1320, which suggests that *we are highly perplexed* by this suggestion.
    If the likelihood of picking this token was 6000/6600 instead, then the perplexity
    would be 6600/6000 = 1.1 which suggests that we are just slightly perplexed by
    this suggestion.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们用一个包含 6600 个标记的词汇表训练语言模型，并运行一个预测步骤以让模型预测句子中的下一个标记。假设选择该标记的概率是 5/6600（即，该标记的概率不是很高）。其困惑度是概率的倒数，即
    6600/5 = 1320，这表明 *我们对这个建议感到非常困惑*。如果选择该标记的概率是 6000/6600，那么困惑度将是 6600/6000 = 1.1，这表明我们对这个建议感到仅仅是稍微困惑。
- en: Hence, the perplexity of our model on a more likely prediction is lower than
    the perplexity of our model on a less likely prediction.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，模型在更可能的预测上的困惑度低于模型在不太可能的预测上的困惑度。
- en: The perplexity of predicting all tokens in a sentence “x” is formally defined
    as the Nth root of the inverse of the product of the token probabilities.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 预测句子“x”中所有标记的困惑度形式上定义为标记概率乘积的倒数的 N 次根。
- en: '![](../Images/426c37041a0cb753755ed645420b9bae.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/426c37041a0cb753755ed645420b9bae.png)'
- en: However, to ensure numerical stability, we can define it in terms of the log
    function.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，为了确保数值稳定性，我们可以用对数函数来定义它。
- en: '![](../Images/525b453fe476fc340eb4bd4bd4712c9c.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/525b453fe476fc340eb4bd4bd4712c9c.png)'
- en: Which is e (2.71828) to the power of the average negative log-likelihood of
    the predicted token being the ground truth token.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 这是 e（2.71828）到预测标记为真实标记的平均负对数似然的幂。
- en: Training and validation perplexity
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练和验证困惑度
- en: The model’s training and validation perplexity can be computed directly from
    the batch or epoch loss.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 模型的训练和验证困惑度可以直接从批次或时期的损失计算得出。
- en: Prediction perplexity
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 预测困惑度
- en: One can’t compute the prediction perplexity since it requires having a set of
    ground truth labels for each prediction.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 由于需要一组每个预测的真实标签，因此无法计算预测困惑度。
- en: PyTorch code to compute perplexity
  id: totrans-58
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 计算困惑度的 PyTorch 代码
- en: Let’s assume that the variable *probs* is a *torch.Tensor* of shape *(sequence_length,)*,
    which contains the probability of the ground-truth token being predicted by the
    language model at that position in the sequence.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 假设变量 *probs* 是一个形状为 *(sequence_length,)* 的 *torch.Tensor*，它包含了语言模型在序列中该位置上预测的真实标记的概率。
- en: The per-token perplexity can be computed using this code.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用以下代码计算每个标记的困惑度。
- en: '[PRE0]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The sample perplexity can be computed using this code.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用以下代码计算样本困惑度。
- en: '[PRE1]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Next, let’s look at the code that computes this per-token probability given
    a sentence.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们查看一下计算给定句子的每个标记概率的代码。
- en: '[PRE2]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Now that we know something about how language models function, and how we can
    compute the per-token as well as per-sentence perplexity, let’s try to tie it
    all together and take a look at how one can leverage this information to build
    a tool that can detect if some text was AI-generated.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们对语言模型的功能以及如何计算每个标记和每个句子的困惑度有了一些了解，让我们试着把这些信息结合起来，看看如何利用这些信息构建一个可以检测文本是否由
    AI 生成的工具。
- en: Detecting AI-generated text
  id: totrans-67
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 检测 AI 生成的文本
- en: 'We have all the ingredients we need to check if a piece of text is AI-generated.
    Here’s everything we need:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经拥有检查文本是否由 AI 生成所需的所有成分。这里是我们需要的一切：
- en: The text (sentence or paragraph) we wish to check.
  id: totrans-69
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们希望检查的文本（句子或段落）。
- en: The tokenized version of this text, tokenized using the tokenizer that was used
    to tokenize the training dataset for this model.
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 该文本的分词版本，使用与该模型的训练数据集相同的分词器进行分词。
- en: The trained language model.
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 经过训练的语言模型。
- en: 'Using 1, 2, and 3 above, we can compute the following:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 利用以上 1、2 和 3，我们可以计算以下内容：
- en: Per-token probability as predicted by the model.
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 模型预测的每个 token 的概率。
- en: Per-token perplexity using the per-token probability.
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用每个 token 的概率计算的困惑度。
- en: Total perplexity for the entire sentence.
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 整个句子的总困惑度。
- en: The perplexity of the model on the training dataset.
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 模型在训练数据集上的困惑度。
- en: To check if a text is AI-generated, we need to compare the sentence perplexity
    with the model’s perplexity scaled by a fudge-factor, alpha. If the sentence perplexity
    is more than the model’s perplexity with scaling, then it’s probably human-written
    text (i.e. not AI-generated). Otherwise, it’s probably AI-generated. The reason
    for this is that we expect the model to not be perplexed by text it would generate
    itself, so if it encounters some text that it itself would not generate, then
    there’s reason to believe that the text isn’t AI-generated. If the perplexity
    of the sentence is less than or equal to the model’s training perplexity with
    scaling, then it’s likely that it was generated using this language model, but
    we can’t be very sure. This is because it’s possible for a human to have written
    that text, and it just happens to be something that the model could also have
    generated. After all, the model was trained on a lot of human-written text so
    in some sense, the model represents an “average human’s writing”.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 要检查文本是否由 AI 生成，我们需要将句子的困惑度与模型的困惑度（经过调整的系数 alpha）进行比较。如果句子的困惑度高于调整后的模型困惑度，则可能是人类撰写的文本（即不是
    AI 生成的）。否则，可能是 AI 生成的。原因在于，我们期望模型不会对它自己生成的文本感到困惑，因此如果它遇到一些自己不会生成的文本，那么就有理由相信这些文本不是
    AI 生成的。如果句子的困惑度小于或等于经过调整的模型训练困惑度，那么很可能是使用该语言模型生成的，但我们不能非常确定。这是因为一个人也有可能写出这样的文本，而且这正好是模型也可能生成的内容。毕竟，模型是在大量人类写作的文本上训练的，所以在某种意义上，模型代表了一种“普通人类的写作”。
- en: '![](../Images/d515e2e4323c1603bd5fc0416815b800.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d515e2e4323c1603bd5fc0416815b800.png)'
- en: '*ppx(x)* in the formula above means the perplexity of the input “x”.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 上述公式中的*ppx(x)*表示输入“x”的困惑度。
- en: Next, let’s take a look at examples of human-written v/s AI-generated text.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们查看一些人类撰写的文本与 AI 生成的文本的例子。
- en: Examples of AI-generated v/s human written text
  id: totrans-81
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: AI 生成的文本与人类撰写的文本的例子
- en: We’ve written some Python code that colors each token in a sentence based on
    its perplexity relative to the model’s perplexity. The first token is always coloured
    black if we don’t consider its perplexity. Tokens that have a perplexity that
    is less than or equal to the model’s perplexity with scaling are coloured red,
    indicating that they may be AI-generated, whereas the tokens with higher perplexity
    are coloured green, indicating that they were definitely not AI-generated.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 我们编写了一些 Python 代码，根据每个 token 相对于模型困惑度的困惑度对其着色。如果我们不考虑其困惑度，第一个 token 总是呈黑色。困惑度小于或等于模型困惑度的
    token 被着色为红色，表明它们可能是 AI 生成的，而困惑度较高的 token 被着色为绿色，表明它们肯定不是 AI 生成的。
- en: '![](../Images/88d5070960ec7b91dce50880f51f986b.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/88d5070960ec7b91dce50880f51f986b.png)'
- en: The numbers in the square brackets before the sentence indicate the perplexity
    of the sentence as computed using the language model. Note that some words are
    part red and part blue. This is due to the fact that we used a subword tokenizer.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 句子前方方括号中的数字表示使用语言模型计算的句子困惑度。注意某些单词有部分红色和部分蓝色。这是因为我们使用了子词分词器。
- en: Here’s the code that generates the HTML above.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 这是生成上述 HTML 的代码。
- en: '[PRE3]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: As we can see from the examples above, if a model detects some text as human-generated,
    it’s definitely human-generated, but if it detects the text as AI-generated, there’s
    a chance that it’s not AI-generated. So why does this happen? Let’s take a look
    next!
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 从上面的例子可以看出，如果模型将某些文本检测为人类生成的，那么这些文本确实是人类生成的，但如果模型将文本检测为 AI 生成的，那么有可能并非 AI 生成的。那么为什么会发生这种情况呢？让我们接着看！
- en: False positives
  id: totrans-88
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 假阳性
- en: Our language model is trained on a LOT of text written by humans. It’s generally
    hard to detect if something was written (digitally) by a specific person. The
    model’s inputs for training comprise many, many different styles of writing, likely
    written by a large number of people. This causes the model to learn many different
    writing styles and content. It’s very likely that your writing style very closely
    matches the writing style of some text the model was trained on. This is the result
    of false positives and why the model can’t be sure that some text is AI-generated.
    However, the model can be sure that some text was human-generated.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的语言模型经过大量由人类编写的文本训练。通常很难检测到某个特定人是否（数字化）编写了某些内容。模型的训练输入包含了许多不同的写作风格，很可能是由大量不同的人编写的。这使得模型学习了许多不同的写作风格和内容。很可能你的写作风格与模型训练中某些文本的写作风格非常接近。这就是假阳性出现的原因，也是模型不能确定某些文本是否是
    AI 生成的原因。然而，模型可以确定某些文本是人类生成的。
- en: '**OpenAI:** OpenAI recently announced that it would discontinue its tools for
    detecting AI-generated text, citing a low accuracy rate (Source: [Hindustan Times](https://tech.hindustantimes.com/tech/news/openai-kills-off-its-own-ai-text-detection-tool-shocking-reason-behind-it-71690364760759.html)).'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '**OpenAI：** OpenAI 最近宣布将停止其用于检测 AI 生成文本的工具，理由是准确率较低（来源：[Hindustan Times](https://tech.hindustantimes.com/tech/news/openai-kills-off-its-own-ai-text-detection-tool-shocking-reason-behind-it-71690364760759.html)）。'
- en: The original version of the AI classifier tool had certain limitations and inaccuracies
    from the outset. Users were required to input at least 1,000 characters of text
    manually, which OpenAI then analyzed to classify as either AI or human-written.
    Unfortunately, the tool’s performance fell short, as it properly identified only
    26 percent of AI-generated content and mistakenly labeled human-written text as
    AI about 9 percent of the time.
  id: totrans-91
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: AI 分类器工具的原始版本从一开始就存在某些局限性和不准确性。用户需要手动输入至少 1,000 个字符的文本，然后 OpenAI 会分析这些文本以分类为
    AI 或人类编写。不幸的是，该工具的表现不尽如人意，因为它仅正确识别了 26% 的 AI 生成内容，并且错误地将 9% 的人类编写文本标记为 AI。
- en: Here’s the [blog post from OpenAI](https://openai.com/blog/new-ai-classifier-for-indicating-ai-written-text).
    It seems like they used a different approach compared to the one mentioned in
    this article.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 这是 [OpenAI 的博客文章](https://openai.com/blog/new-ai-classifier-for-indicating-ai-written-text)。看起来他们使用了与本文中提到的不同的方法。
- en: Our classifier is a language model fine-tuned on a dataset of pairs of human-written
    text and AI-written text on the same topic. We collected this dataset from a variety
    of sources that we believe to be written by humans, such as the pretraining data
    and human demonstrations on prompts submitted to InstructGPT. We divided each
    text into a prompt and a response. On these prompts, we generated responses from
    a variety of different language models trained by us and other organizations.
    For our web app, we adjust the confidence threshold to keep the false positive
    rate low; in other words, we only mark text as likely AI-written if the classifier
    is very confident.
  id: totrans-93
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 我们的分类器是一个语言模型，经过微调，使用的是同一主题的人工编写文本和 AI 编写文本的对照数据集。我们从多个我们认为是由人类编写的来源收集了这个数据集，比如预训练数据和提交给
    InstructGPT 的人类演示。我们将每篇文本分为提示和回应。在这些提示上，我们生成了来自我们和其他组织训练的各种语言模型的回应。对于我们的网页应用，我们调整了置信度阈值，以保持低的假阳性率；换句话说，我们只有在分类器非常确信的情况下才将文本标记为可能的
    AI 编写。
- en: '**GPTZero:** Another popular AI-generated text detection tool is [GPTZero](https://gptzero.me/).
    It seems like GPTZero uses [perplexity and burstiness](https://ecoagi.ai/topics/ChatGPT/high-perplexity-score-gpt-zero)
    to detect AI-generated text. *“Burstiness refers to the phenomenon where certain
    words or phrases appear in bursts within a text. In other words if a word appears
    once in a text, it’s likely to appear again in close proximity”* ([source](https://ecoagi.ai/topics/ChatGPT/high-perplexity-score-gpt-zero)).'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '**GPTZero：** 另一个流行的 AI 生成文本检测工具是 [GPTZero](https://gptzero.me/)。看起来 GPTZero
    使用 [困惑度和突发性](https://ecoagi.ai/topics/ChatGPT/high-perplexity-score-gpt-zero)
    来检测 AI 生成的文本。*“突发性指的是某些词语或短语在文本中以突发的形式出现。换句话说，如果一个词在文本中出现了一次，它很可能会在接近的地方再次出现”*（[来源](https://ecoagi.ai/topics/ChatGPT/high-perplexity-score-gpt-zero)）。'
- en: GPTZero claims to have a very high success rate. According to the [GPTZero FAQ](https://gptzero.me/faq),
    *“At a threshold of 0.88, 85% of AI documents are classified as AI, and 99% of
    human documents are classified as human.”*
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: GPTZero 宣称具有非常高的成功率。根据 [GPTZero FAQ](https://gptzero.me/faq)，*“在 0.88 的阈值下，85%
    的 AI 文档被分类为 AI，99% 的人工文档被分类为人工。”*
- en: The generality of this approach
  id: totrans-96
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 这种方法的普遍性
- en: The approach mentioned in this article doesn’t generalize well. What we mean
    by this is that if you have 3 language models, for example, GPT3, GPT3.5, and
    GPT4, then you must run the input text through all the 3 models and check perplexity
    on all of them to see if the text was generated by any one of them. This is because
    each model generates text slightly differently, and they all need to independently
    evaluate text to see if any of them may have generated the text.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 这篇文章提到的方法不具有很好的一般化性。我们的意思是，如果你有三个语言模型，比如 GPT3、GPT3.5 和 GPT4，那么你必须将输入文本通过这三个模型，并检查它们的困惑度，以确定文本是否由其中任何一个模型生成。这是因为每个模型生成文本的方式略有不同，它们都需要独立评估文本，以确定是否有模型生成了该文本。
- en: With the proliferation of large language models in the world as of August 2023,
    it seems unlikely that one can check any piece of text as having originated from
    any of the language models in the world.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 随着到 2023 年 8 月世界上大量语言模型的普及，似乎不太可能检查任何文本是否来自世界上的任何语言模型。
- en: In fact, new models are being trained every day, and trying to keep up with
    this rapid progress seems hard at best.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，每天都有新的模型在训练，跟上这种快速进展似乎非常困难。
- en: The example below shows the result of asking our model to predict if the sentences
    generated by ChatGPT are AI-generated or not. As you can see, the results are
    mixed.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 以下示例显示了让我们的模型预测 ChatGPT 生成的句子是否为 AI 生成的结果。如你所见，结果是混合的。
- en: '![](../Images/dfc4b3b7f610dcb435455a04baf59c02.png)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/dfc4b3b7f610dcb435455a04baf59c02.png)'
- en: The sentences in the purple box are correctly identified as AI-generated by
    our model, whereas the rest are incorrectly identified as human written.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 紫色框中的句子被我们的模型正确识别为 AI 生成的，而其余的则被错误识别为人工撰写的。
- en: There are many reasons why this may happen.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 这可能发生的原因有很多。
- en: '**Train corpus size:** Our model is trained on very little text, whereas ChatGPT
    was trained on terabytes of text.'
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**训练语料库规模：** 我们的模型在非常少的文本上进行训练，而 ChatGPT 在数TB 的文本上进行训练。'
- en: '**Data distribution:** Our model is trained on a different data distribution
    as compared to ChatGPT.'
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**数据分布：** 我们的模型在与 ChatGPT 不同的数据分布上进行训练。'
- en: '**Fine-tuning:** Our model is just a GPT model, whereas ChatGPT was fine-tuned
    for chat-like responses, making it generate text in a slightly different tone.
    If you had a model that generates legal text or medical advice, then our model
    would perform poorly on text generated by those models as well.'
  id: totrans-106
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**微调：** 我们的模型只是一个 GPT 模型，而 ChatGPT 是针对聊天类响应进行微调的，使其生成的文本具有略微不同的语调。如果你有一个生成法律文本或医学建议的模型，那么我们的模型在这些模型生成的文本上也会表现不佳。'
- en: '**Model size:** Our model is very small (less than 100M parameters compared
    to > 200B parameters for ChatGPT-like models).'
  id: totrans-107
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**模型规模：** 我们的模型非常小（少于 100M 参数，而 ChatGPT 类模型有超过 200B 参数）。'
- en: It’s clear that we need a better approach if we hope to provide a reasonably
    high-quality result to check if any text is AI-generated.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 很明显，如果我们希望提供合理高质量的结果以检查文本是否为 AI 生成的，我们需要一种更好的方法。
- en: Next, let’s take a look at some misinformation about this topic circulating
    around the internet.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们来看一下关于这个话题在互联网中流传的一些错误信息。
- en: Misinformation
  id: totrans-110
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 错误信息
- en: Some articles interpret perplexity incorrectly. For example, if you search google.com
    for [“does human written content have high or low perplexity?”](https://www.google.com/search?q=does+human+written+content+have+high+or+low+perplexity%3F),
    you’ll get the [following result](https://www.linkedin.com/pulse/how-write-content-using-chatgpt-outsmarts-ai-tools-99-dasun-sucharith/)
    in the first position.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 一些文章对困惑度的解释不正确。例如，如果你在 google.com 搜索 [“人工撰写内容的困惑度高还是低？”](https://www.google.com/search?q=does+human+written+content+have+high+or+low+perplexity%3F)，你会在第一个位置看到
    [以下结果](https://www.linkedin.com/pulse/how-write-content-using-chatgpt-outsmarts-ai-tools-99-dasun-sucharith/)。
- en: '![](../Images/51bd737f2d520f0aa9f71677734a6cf7.png)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/51bd737f2d520f0aa9f71677734a6cf7.png)'
- en: This is incorrect since human-written content typically has higher perplexity
    compared with AI-generated content.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 这是不正确的，因为人工撰写的内容通常比 AI 生成的内容具有更高的困惑度。
- en: Let’s take a look at techniques that researchers in this area are exploring
    to do better than where we are.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看看研究人员在这一领域探索的技术，以期比目前的情况做得更好。
- en: What’s next?
  id: totrans-115
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 接下来是什么？
- en: We’ve established that detecting AI-generated text is a hard problem, and that
    success rates are so low that they aren’t any better than guessing. Let’s look
    at what state of the art techniques researchers in this area are exploring to
    get a better handle on things.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经确定，检测AI生成文本是一个困难的问题，成功率低到不比猜测更好。让我们看看这一领域的最先进技术，研究人员如何探索以更好地处理这个问题。
- en: '**Watermarking:** [OpenAI and Google have promised to watermark AI generated
    text](https://arstechnica.com/ai/2023/07/openai-google-will-watermark-ai-generated-content-to-hinder-deepfakes-misinfo/)
    so that it’s possible to identify it programmatically.'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '**水印：** [OpenAI和Google已承诺为AI生成的文本添加水印](https://arstechnica.com/ai/2023/07/openai-google-will-watermark-ai-generated-content-to-hinder-deepfakes-misinfo/)，以便可以程序化识别。'
- en: The technical details of how this watermark might work are unclear and neither
    company has disclosed any details related to it.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 关于这种水印如何工作的技术细节尚不清楚，两家公司都没有披露相关细节。
- en: Even if OpenAI and Google adopt a watermarking technique, we can’t be certain
    that every language model that is deployed out there will have a watermark included.
    It would still be possible for people to deploy their own models to generate text
    and put it out there in the wild. Even if companies decide to watermark generated
    text, it’s not clear if this will be a standard or if every company will have
    their own proprietary strategy and potentially paid tool to check if any text
    was generated using their AI-based text-generation tools. If it’s an open standard
    there’s a chance that people will be able to work around it unless it’s something
    like a cryptographic cipher that requires a ton of computation to undo. If it’s
    not an open standard, then people will be at the mercy of these companies to provide
    open and free access to the tools and APIs needed to perform these checks. There’s
    also the question of how effective these will be in the long run since it may
    even be possible to train models to ingest the AI-generated text with a watermark,
    and return AI-generated text without a watermark.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 即使OpenAI和Google采用了水印技术，我们也不能确定所有部署的语言模型都会包含水印。人们仍然可能会部署自己的模型来生成文本并将其发布到公共平台。即使公司决定给生成的文本加水印，也不清楚这是否会成为标准，还是每家公司会有自己的专有策略和可能收费的工具来检查文本是否由其AI文本生成工具生成。如果这是一个开放标准，人们有可能绕过它，除非它像加密密码一样需要大量计算来解密。如果不是开放标准，那么人们将依赖这些公司提供开放和免费的工具及API来进行检查。此外，这些工具在长期中的有效性也是一个问题，因为甚至可能训练模型来接收带水印的AI生成文本，并返回没有水印的AI生成文本。
- en: '[This article](https://www.nytimes.com/interactive/2023/02/17/business/ai-text-detection.html)
    discusses a possible technique for adding a watermark for AI-generated text, and
    mentions the significant challenges with the approach.'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '[这篇文章](https://www.nytimes.com/interactive/2023/02/17/business/ai-text-detection.html)讨论了一种为AI生成文本添加水印的可能技术，并提到这种方法的重大挑战。'
- en: '**Personalization:** In our opinion, this problem of detecting AI-generated
    text is going to remain challenging in the near term. We believe that strategies
    will need to get more intrusive and personalized to be more effective. For example,
    instead of asking if some text is AI-generated it may be more reasonable to ask
    if this text was written by a specific person. However, this would require the
    system to have access to large amounts of text written by that specific person.
    Additionally, the problem becomes more complex if something was written by more
    than one person, such as this article.'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '**个性化：** 在我们看来，检测AI生成文本的问题在短期内仍将具有挑战性。我们相信，策略需要变得更加侵入性和个性化才能更有效。例如，与其询问某些文本是否由AI生成，不如询问这些文本是否由特定的人撰写。然而，这将要求系统能够访问大量该特定人撰写的文本。此外，如果某些文本由多人撰写，如本文所示，问题会变得更加复杂。'
- en: Let’s look at the impact such a personalized system for detecting human-written
    text would have on educators and students.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看这种个性化的系统在检测人工撰写文本方面对教育工作者和学生的影响。
- en: If such a solution existed, **educators** would be more inclined to hand out
    individual assignments to students instead of group assignments. This would also
    require every student to first provide a large amount of text that they themselves
    have written. This could mean spending several hours in-person at a university
    before enrolling for classes. Surely, this would have a negative effect on the
    ability to teach students the importance of working together as a team to accomplish
    a common goal.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 如果存在这样的解决方案，**教育工作者**将更倾向于给学生布置个人作业而不是小组作业。这还将要求每个学生首先提供大量他们自己撰写的文本。这可能意味着在入学前需要在大学里亲自花费几个小时。这无疑会对教授学生团队合作以实现共同目标的重要性产生负面影响。
- en: On the other hand, having access to AI-based text generation could free students
    in some cases to focus on the actual problem at hand such as performing the research
    or literature study instead of spending time writing out their learnings in a
    polished manner. One can imagine that students will end up spending more of their
    time learning concepts and techniques in math or science class as opposed to writing
    about it. That part can be taken care of by the AI.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，在某些情况下，访问基于AI的文本生成技术可以让学生集中精力解决实际问题，例如进行研究或文献研究，而不是花时间以完善的方式撰写他们的学习成果。可以想象，学生们会在数学或科学课程中花更多时间学习概念和技术，而不是写作。那部分可以由AI处理。
- en: Conclusion
  id: totrans-125
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: In this article, we built intuition around how one can detect AI-generated text.
    The main metric we can use is the perplexity of the generated text. We saw some
    PyTorch code to check if a given text may be AI-generated using the perplexity
    of that text. We also saw some of the drawbacks of this approach, including the
    possibility of false positives. We hope this helps you understand and appreciate
    the nitty-gritty details behind detecting AI-generated text.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们建立了如何检测AI生成文本的直觉。我们可以使用的主要指标是生成文本的困惑度。我们看到了一些PyTorch代码，用于检查给定文本是否可能是AI生成的，方法是利用文本的困惑度。我们也看到了这种方法的一些缺点，包括可能出现假阳性。希望这能帮助你理解和欣赏检测AI生成文本的细节。
- en: This is a constantly evolving space and researchers are trying hard to figure
    out a way to detect AI-generated text with higher accuracy. The impact of this
    technology to our lives promises to be significant, and in many ways unknown.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个不断发展的领域，研究人员正努力寻找一种更高准确率检测AI生成文本的方法。这项技术对我们生活的影响承诺将是显著的，并且在许多方面仍然未知。
- en: While we discussed the techniques for detecting AI-generated text, we assumed
    that the entire text is either human-written or AI-generated. In practice, text
    tends to be partially human-written and partially AI-generated, and this complicates
    things considerably.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们讨论了检测AI生成文本的技术，但我们假设整个文本要么是人类撰写的，要么是AI生成的。实际上，文本往往是部分人类撰写的，部分AI生成的，这使得问题变得更加复杂。
- en: If you want to read about additional approaches to detect AI-generated text,
    such as the use of the burstiness metric, you can read about them [here](/how-do-we-know-if-a-text-is-ai-generated-82e710ea7b51).
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想了解更多检测AI生成文本的方法，比如使用突发性指标，你可以在[这里](/how-do-we-know-if-a-text-is-ai-generated-82e710ea7b51)阅读。
- en: All the images in this article (except for the first one) were created by the
    author(s).
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 本文中的所有图片（除了第一张）均由作者（们）创作。
