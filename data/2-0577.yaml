- en: 'Courage to Learn ML: An In-Depth Guide to the Most Common Loss Functions'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 勇敢学习机器学习：最常见损失函数的深入指南
- en: 原文：[https://towardsdatascience.com/courage-to-learn-ml-an-in-depth-guide-to-the-most-common-loss-functions-84a6b07cca17](https://towardsdatascience.com/courage-to-learn-ml-an-in-depth-guide-to-the-most-common-loss-functions-84a6b07cca17)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/courage-to-learn-ml-an-in-depth-guide-to-the-most-common-loss-functions-84a6b07cca17](https://towardsdatascience.com/courage-to-learn-ml-an-in-depth-guide-to-the-most-common-loss-functions-84a6b07cca17)
- en: MSE, Log Loss, Cross Entropy, RMSE, and the Foundational Principles of Popular
    Loss Functions
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: MSE、对数损失、交叉熵、RMSE以及流行损失函数的基础原理
- en: '[](https://amyma101.medium.com/?source=post_page-----84a6b07cca17--------------------------------)[![Amy
    Ma](../Images/2edf55456a1f92724535a1441fa2bef5.png)](https://amyma101.medium.com/?source=post_page-----84a6b07cca17--------------------------------)[](https://towardsdatascience.com/?source=post_page-----84a6b07cca17--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----84a6b07cca17--------------------------------)
    [Amy Ma](https://amyma101.medium.com/?source=post_page-----84a6b07cca17--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://amyma101.medium.com/?source=post_page-----84a6b07cca17--------------------------------)[![Amy
    Ma](../Images/2edf55456a1f92724535a1441fa2bef5.png)](https://amyma101.medium.com/?source=post_page-----84a6b07cca17--------------------------------)[](https://towardsdatascience.com/?source=post_page-----84a6b07cca17--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----84a6b07cca17--------------------------------)
    [Amy Ma](https://amyma101.medium.com/?source=post_page-----84a6b07cca17--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----84a6b07cca17--------------------------------)
    ·13 min read·Dec 28, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page-----84a6b07cca17--------------------------------)
    ·13分钟阅读·2023年12月28日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/847cb932c67abb07544b56bac23b87a8.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/847cb932c67abb07544b56bac23b87a8.png)'
- en: Photo by [William Warby](https://unsplash.com/@wwarby?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 由[William Warby](https://unsplash.com/@wwarby?utm_source=medium&utm_medium=referral)拍摄，[Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: 'Welcome back! In the ‘[Courage to Learn ML](http://towardsdatascience.com/tagged/courage-to-learn-ml)’
    series, where we conquer machine learning fears one challenge at a time. Today,
    we’re diving headfirst into the world of loss functions: the silent superheroes
    guiding our models to learn from mistakes. In this post, we’d cover the following
    topics:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 欢迎回来！在‘[勇敢学习机器学习](http://towardsdatascience.com/tagged/courage-to-learn-ml)’系列中，我们一步一步征服机器学习的恐惧。今天，我们将深入探讨损失函数的世界：这些默默无闻的超级英雄引导我们的模型从错误中学习。在这篇文章中，我们将涵盖以下主题：
- en: What is a loss function?
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 什么是损失函数？
- en: Difference between loss functions and metrics
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 损失函数与指标之间的区别
- en: Explaining MSE and MAE from two perspectives
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从两个角度解释MSE和MAE
- en: Three basic ideas when designing loss functions
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设计损失函数时的三个基本概念
- en: Using those three basic ideas to interpret MSE, log loss, and cross-entropy
    loss
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 运用这三个基本概念来解释MSE、对数损失和交叉熵损失
- en: Connection between log loss and cross-entropy loss
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对数损失与交叉熵损失之间的关系
- en: How to handle multiple loss functions (objectives) in practice
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何在实践中处理多个损失函数（目标）
- en: Difference between MSE and RMSE
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MSE和RMSE的区别
- en: What are loss functions, and why are they important in machine learning models?
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是损失函数，它们在机器学习模型中为什么重要？
- en: Loss functions are crucial in evaluating a model’s effectiveness during its
    learning process, akin to an exam or a set of criteria. They serve as indicators
    of how far the model’s predictions deviate from the true labels ( the ‘correct’
    answers). Typically, loss functions assess performance by measuring the discrepancy
    between the predictions made by the model and the actual labels. This evaluation
    of the gap informs the model about the extent of adjustments needed in its parameters,
    such as weights or coefficients, to more accurately capture the underlying patterns
    in the data.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 损失函数在评估模型在学习过程中的有效性方面至关重要，类似于考试或标准。它们作为模型预测与真实标签（即‘正确’答案）之间偏差的指标。通常，损失函数通过衡量模型预测和实际标签之间的差异来评估性能。这种差距的评估向模型提供了关于其参数（如权重或系数）需要调整的程度，以便更准确地捕捉数据中的潜在模式。
- en: There are different loss functions in machine learning. These factors include
    the nature of the predictive task at hand, whether it’s regression or classification,
    the distribution of the target variable, as illustrated by the use of Focal Loss
    for handling imbalanced datasets, and the specific learning methodology of the
    algorithm, such as the application of hinge loss in SVMs. Understanding and selecting
    the appropriate loss function is quite important, since it directly influences
    how a model learns from the data.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习中存在不同的损失函数。这些因素包括预测任务的性质，如回归或分类，目标变量的分布，例如使用Focal Loss处理不平衡数据集，以及算法的具体学习方法，如在SVM中应用铰链损失。理解和选择合适的损失函数非常重要，因为它直接影响模型如何从数据中学习。
- en: To learn machine learning, one should know the most popular ones. For example,
    (Mean Squared Error) MSE and (Mean Absolute Error) MAE are commonly used in regression
    problems, while cross entropy is the most common loss function for classification
    tasks.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 学习机器学习时，应了解最流行的损失函数。例如，均方误差（MSE）和平均绝对误差（MAE）在回归问题中常用，而交叉熵是分类任务中最常用的损失函数。
- en: How do loss functions differ from metrics, and in what ways can a loss function
    also serve as a metric?
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 损失函数和度量指标有何不同，损失函数在何种情况下也可以作为度量指标？
- en: 'Your statement about loss function can also be metrics is misleading. Loss
    functions and metrics both assess model performance, but in different stages and
    for different purpose:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 关于损失函数也可以作为度量指标的说法可能会误导。损失函数和度量指标都用于评估模型性能，但在不同的阶段和目的下：
- en: '**Loss Functions**: These are **used during the model’s learning process**
    to guide its adjustments. They need to be **differentiable** to facilitate optimization.
    For instance, Mean Squared Error (MSE) and Mean Absolute Error (MAE) are common
    loss functions in regression models.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**损失函数**：这些函数在**模型学习过程中使用**，以指导模型的调整。它们需要是**可微分的**，以便进行优化。例如，均方误差（MSE）和平均绝对误差（MAE）是回归模型中常见的损失函数。'
- en: '**Metrics**: These **evaluate the model’s performance after training**. Metrics
    should be **interpretable** and provide clear insights into model effectiveness.
    While some metrics, like accuracy, can be straightforward, others like F1 score
    involve threshold decisions and are non-differentiable, making them less suitable
    for guiding learning.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**度量指标**：这些**评估模型训练后的表现**。度量指标应该是**可解释的**，并提供关于模型有效性的清晰见解。虽然一些度量指标，如准确率，可能比较直接，但其他如F1得分则涉及阈值决策且不可微分，使其不太适合作为学习指导。'
- en: Notably, some measures, such as MSE and MAE, can serve both as loss functions
    and metrics due to their differentiability and interpretability. However, **not
    all metrics are suitable as loss functions**, primarily due to the need for differentiability
    in loss functions for optimization purposes.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，一些度量指标，如MSE和MAE，由于其可微分性和可解释性，可以既作为损失函数也作为度量指标。然而，**并非所有的度量指标都适合作为损失函数**，主要是因为损失函数需要可微分性以用于优化。
- en: In practice, one should always carefully choose the loss function and metrics
    **together** for learning, and ensure that the learning and evaluation are **aligned
    in the same direction**. This alignment ensures that the model is optimized and
    evaluated **based on the same criteria** that reflect the end goals of the application.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 实际中，应始终仔细选择损失函数和度量指标**一起**用于学习，并确保学习和评估**朝着相同方向对齐**。这种对齐确保了模型的优化和评估**基于相同标准**，这些标准反映了应用的最终目标。
- en: '**Author’s Note:** It’s important to clarify that using the F1 score as a loss
    function in machine learning models isn’t entirely infeasible. In my ongoing study,
    I’ve encountered innovative methods that address the non-differentiability issue
    commonly associated with the F1 score. For instance, Ashref Maiza’s [post](https://medium.com/r?url=https%3A%2F%2Ftowardsdatascience.com%2Fthe-unknown-benefits-of-using-a-soft-f1-loss-in-classification-systems-753902c0105d)
    introduces a differentiable approximation of the F1 score. This approach involves
    “softening” precision and recall using likelihood concepts, rather than setting
    arbitrary thresholds. Additionally, some online discussions like [the one](https://datascience.stackexchange.com/questions/66581/is-it-possible-to-make-f1-score-differentiable-and-use-it-directly-as-a-loss-fun)
    explore similar themes.'
  id: totrans-27
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**作者注：** 需要明确的是，使用 F1 分数作为机器学习模型的损失函数并非完全不可行。在我持续的研究中，我遇到了一些创新方法，解决了通常与 F1
    分数相关的不可微分性问题。例如，Ashref Maiza 的 [帖子](https://medium.com/r?url=https%3A%2F%2Ftowardsdatascience.com%2Fthe-unknown-benefits-of-using-a-soft-f1-loss-in-classification-systems-753902c0105d)
    介绍了一种 F1 分数的可微分近似方法。这种方法通过使用似然概念“软化”精确度和召回率，而不是设置任意阈值。此外，一些在线讨论，如 [这个讨论](https://datascience.stackexchange.com/questions/66581/is-it-possible-to-make-f1-score-differentiable-and-use-it-directly-as-a-loss-fun)，也探讨了类似的主题。'
- en: ''
  id: totrans-28
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The challenge lies in the inherent nature of the F1 score. While it’s a highly
    informative metric, selecting an appropriate loss function to effectively optimize
    the model under the same criteria can be complex. Moreover, tuning thresholds
    adds another layer of complexity. I’m really interested into this topic. If you
    have insights or experiences to share, please feel free to connect with me. I’m
    eager to expand my understanding and engage in further discussions.
  id: totrans-29
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 挑战在于 F1 分数的固有特性。尽管它是一个高度信息量的指标，但选择合适的损失函数来在相同标准下有效优化模型可能会很复杂。此外，调整阈值增加了另一层复杂性。我对这个话题非常感兴趣。如果你有见解或经验可以分享，请随时与我联系。我渴望扩展我的理解并参与进一步的讨论。
- en: You said MSE and MAE as typical metrics in regression problems. What are they
    and when to use them?
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 你提到 MSE 和 MAE 是回归问题中的典型指标。它们是什么以及何时使用它们？
- en: In regression problems, where the predictions are continuous values, the goal
    is to minimize the difference between the model’s predictions and the actual values.
    To assess the model’s effectiveness in grasping the underlying pattern, we use
    metrics like Mean Squared Error (MSE) and Mean Absolute Error (MAE). Both these
    metrics quantify the gap between predictions and actual values, but they do so
    using different evaluation approaches.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在回归问题中，预测值是连续值，目标是最小化模型预测值与实际值之间的差异。为了评估模型在把握潜在模式方面的有效性，我们使用均方误差 (MSE) 和平均绝对误差
    (MAE) 等指标。这两个指标都量化了预测值和实际值之间的差距，但它们使用不同的评估方法。
- en: 'MSE is defined as:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: MSE 的定义为：
- en: '![](../Images/de58a0aad13be06d5cafd5a6fb9d0843.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/de58a0aad13be06d5cafd5a6fb9d0843.png)'
- en: Here, y_i is the actual value, y_hat_i is the predicted value, and n is the
    number of observations.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，y_i 是实际值，y_hat_i 是预测值，n 是观察值的数量。
- en: MSE calculates the average of the squared differences between predictions and
    actual values, which is the Euclidean distance (l2 norm) of predictions and the
    true labels.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: MSE 计算预测值和实际值之间平方差的平均值，这相当于预测值与真实标签之间的欧几里得距离（l2 范数）。
- en: 'On the other hand, MAE is defined as:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，MAE 的定义为：
- en: '![](../Images/0f683d27af0ac3affd9a6c0f7b0730dd.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0f683d27af0ac3affd9a6c0f7b0730dd.png)'
- en: Here, the absolute differences between the actual and predicted values are averaged,
    corresponding to the Manhattan distance (l1 norm). In the other words, MAE calculates
    the average distance between the estimated values and the actual value **without
    considering the direction** (positive or negative).
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，实际值和预测值之间的绝对差异被平均，这对应于曼哈顿距离（l1 范数）。换句话说，MAE 计算估计值与实际值之间的平均距离，**不考虑方向**（正或负）。
- en: We talked about Lp norm and different distances in our discussion on l1, l2
    regularizations [https://medium.com/p/1bb171e43b35](https://medium.com/p/1bb171e43b35),
  id: totrans-39
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 我们在讨论 l1 和 l2 正则化时谈到了 Lp 范数和不同的距离 [https://medium.com/p/1bb171e43b35](https://medium.com/p/1bb171e43b35)。
- en: The primary distinction between MSE and MAE is their response to **outliers**.
    MSE, by squaring the errors, amplifies and gives more weight to larger errors,
    making it sensitive to outliers. **This is useful if larger errors are more significant
    in your problem context.** However, MAE assigns equal weight to all errors, making
    it more robust to outliers and non-normal error distributions.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: MSE和MAE的主要区别在于它们对**异常值**的响应。MSE通过平方错误放大并赋予较大错误更多权重，使其对异常值敏感。**如果较大错误在你的问题上下文中更为重要，这很有用。**
    然而，MAE对所有错误赋予相等的权重，使其对异常值和非正态误差分布更为鲁棒。
- en: The choice between MSE and MAE should based on the properties of the training
    data and the implications of larger errors in the model. MSE is preferable when
    we want to heavily penalize larger errors, while MAE is better when we want to
    treat all errors equally.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 选择MSE还是MAE应基于训练数据的特性和模型中较大错误的影响。当我们希望对较大错误进行重罚时，MSE更为合适，而当我们希望平等对待所有错误时，MAE则更为适用。
- en: I get that squaring the differences in MSE amplifies the errors, leading to
    a greater emphasis on outliers. Are there other perspectives or aspects that help
    differentiate between these two metrics?
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 我明白MSE中平方差异会放大错误，从而对异常值给予更多关注。还有其他观点或方面可以帮助区分这两种指标吗？
- en: Certainly, there’s another perspective to understand the differences between
    Mean Squared Error (MSE) and Mean Absolute Error (MAE) beyond their handling of
    outliers. Imagine you’re tasked with predicting a value ‘y’ without any additional
    features (no ‘Xs’). In this scenario, the simplest model would predict a constant
    value for all inputs.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，还有另一种视角可以理解MSE（均方误差）和MAE（平均绝对误差）之间的差异，超越它们对异常值的处理。假设你需要预测一个值‘y’而没有任何额外的特征（没有‘X’）。在这种情况下，最简单的模型将对所有输入预测一个常数值。
- en: When using MSE as the loss function, t**he constant that minimizes the MSE is
    the mean of the target values**. This is because the mean is the central point
    that minimizes the sum of squared differences from all other points. On the other
    hand, if you use MAE, the median of the target values is the minimizing constant.
    The median, unlike the mean, is less influenced by extreme values or outliers.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 使用MSE作为损失函数时，**最小化MSE的常数是目标值的均值**。这是因为均值是使所有点的平方差之和最小的中心点。另一方面，如果使用MAE，目标值的中位数就是最小化常数。与均值不同，中位数对极端值或异常值的影响较小。
- en: '![](../Images/4893042d90453b7b0eabac2dfed8982b.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4893042d90453b7b0eabac2dfed8982b.png)'
- en: In the universe of Douglas Adams’ ‘The Hitchhiker’s Guide to the Galaxy,’ 42
    is the ultimate answer to life, the universe, and everything. Who knows, maybe
    42 is also the magic number to shrink your loss function — but hey, it all depends
    on what your loss function is! Image created by ChatGPT
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在道格拉斯·亚当斯的《银河系漫游指南》中，42是生命、宇宙和一切的**终极答案**。谁知道呢，也许42还是缩小你的损失函数的魔法数字——但这完全取决于你的损失函数是什么！图像由ChatGPT创建
- en: This difference in sensitivity to outliers stems from how the mean and median
    are calculated. The mean takes into account the magnitude of each value, making
    it easier to being skewed by outliers. The median, however, is only concerned
    with the order of the values, thus maintaining its position regardless of the
    extremities in the dataset. This intrinsic property of the median contributes
    to MAE’s robustness to outliers, providing an alternative interpretation of the
    distinct behaviors of MSE and MAE in modeling contexts.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 这种对异常值敏感度的差异源于均值和中位数的计算方式。均值考虑了每个值的大小，使其更容易受到异常值的影响。而中位数只关心值的顺序，因此无论数据集的极端值如何，都能保持其位置。中位数的这种内在特性使得MAE对异常值具有更强的鲁棒性，提供了MSE和MAE在建模环境下不同表现的另一种解释。
- en: You can find an explanation of why the mean minimizes MSE and the median minimizes
    MAE in Shubham Dhingra’s [post](/evaluation-metrics-ii-e6f09ded4981).
  id: totrans-48
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 你可以在Shubham Dhingra的[文章](/evaluation-metrics-ii-e6f09ded4981)中找到关于均值如何最小化MSE以及中位数如何最小化MAE的解释。
- en: 'We’ve talked about how MSE and MAE measure errors, but there’s more to the
    story. Different tasks need different ways to measure how good our models are
    doing. This is where loss functions come in, and there are three basic ideas behind
    them. Understanding these ideas will help you pick the right loss function for
    any job. So, let’s get started with the most important question:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 我们讨论了MSE和MAE如何测量误差，但事情远不止于此。不同的任务需要不同的方式来衡量我们的模型表现如何。这就是损失函数的作用所在，背后有三个基本思想。理解这些思想将帮助你为任何工作选择合适的损失函数。那么，让我们从最重要的问题开始：
- en: What are the 3 basic ideas that guide the design of any loss function?
  id: totrans-50
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设计任何损失函数的三个基本思想是什么？
- en: 'In designing loss functions, three basic ideas generally guide the process:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在设计损失函数时，通常有三个基本思想指导这一过程：
- en: '**Minimizing Residuals**: The key is to reduce the residuals, which are the
    differences between predicted and actual values. To address both negative and
    positive discrepancies, we often square these residuals, as seen in the **least
    squares method**. This approach, which sums the squared residuals, is a staple
    in regression problems for its simplicity and effectiveness.'
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**最小化残差**：关键是减少残差，即预测值与实际值之间的差异。为了处理负误差和正误差，我们通常会对这些残差进行平方，如**最小二乘法**中所见。这种方法，通过求和平方残差，在回归问题中因其简单性和有效性而成为主流。'
- en: '**Maximizing Likelihood (MLE)**: Here, the goal is to adjust the model parameters
    to maximize the likelihood of the observed data, making the model as representative
    of the underlying process as possible. This **probabilistic approach** is fundamental
    in models like logistic regression and neural networks, where fitting the model
    to the data distribution is crucial.'
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**最大化似然（MLE）**：这里的目标是调整模型参数以最大化观察到的数据的似然，使模型尽可能代表潜在过程。这种**概率方法**在逻辑回归和神经网络等模型中至关重要，在这些模型中，将模型拟合到数据分布上是关键。'
- en: '**Distinguishing Signal from Noise**: This principle, rooted in **information
    theory**, involves separating valuable data (signal) from irrelevant data (noise).
    Methods based on this idea, focusing on entropy and impurity, are essential in
    classification tasks and form the basis for algorithms like decision trees.'
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**区分信号与噪声**：这一原则源于**信息论**，涉及将有价值的数据（信号）与无关的数据（噪声）分开。基于这一思想的方法，专注于熵和不纯度，在分类任务中至关重要，并构成了决策树等算法的基础。'
- en: Additionally, it’s important to recognize that **some loss functions are tailored
    to specific algorithms**, such as the hinge loss for SVM, indicating that the
    nature of the algorithm also plays a role in loss function design. Additionally,
    **the nature of the data impacts the selection of a loss function**. For instance,
    in cases of imbalanced training data, we might adjust our loss function to a class-balanced
    loss or opt for focal loss.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，重要的是要认识到**某些损失函数是针对特定算法量身定制的**，例如SVM的铰链损失，这表明算法的性质也在损失函数设计中发挥作用。此外，**数据的性质会影响损失函数的选择**。例如，在训练数据不平衡的情况下，我们可能会将损失函数调整为类平衡损失或选择焦点损失。
- en: 'Now, equipped with these fundamental concepts, let’s apply them for interpretive
    analysis to enhance our comprehension. With this approach, we can attempt to address
    the following question:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，掌握了这些基本概念后，让我们应用它们进行解释性分析，以增强我们的理解。通过这种方法，我们可以尝试解决以下问题：
- en: How might we apply MLE and the least squares method to enhance our comprehension
    of MSE?
  id: totrans-57
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 我们如何应用最大似然估计（MLE）和最小二乘法来增强对MSE的理解？
- en: First, let’s break down MSE with the least squares method. The LSE approach
    finds the best model fit by **minimizing the sum of the squares of the residuals**.
    In linear regression (which deals with continuous outputs), a residual is the
    difference between the predicted value and the actual label. MSE, or Mean Squared
    Error, is essentially the average of these squared differences. Therefore, the
    least squares method aims to minimize MSE (factoring in this averaging step),
    making MSE an appropriate loss function for this method.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们用最小二乘法来分解均方误差（MSE）。最小二乘估计（LSE）方法通过**最小化残差的平方和**来找到最佳的模型拟合。在线性回归（处理连续输出）中，残差是预测值与实际标签之间的差异。MSE，即均方误差，本质上是这些平方差异的平均值。因此，最小二乘法旨在最小化MSE（考虑到这个平均步骤），使得MSE成为该方法的合适损失函数。
- en: Next, looking at MSE from a Maximum Likelihood Estimation (MLE) perspective,
    **under the assumption of linear regression, we typically assume that residuals
    follow a normal distribution**. This allows us to model the likelihood of observing
    our data as a product of individual probability density functions (PDFs). For
    simplification, we take the natural logarithm of this likelihood, transforming
    it into a sum of the logs of individual PDFs. It’s important to note that we use
    density functions for continuous variables, as opposed to probability mass functions
    for discrete variables.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，从最大似然估计（MLE）的角度看，**在假设线性回归的情况下，我们通常假设残差服从正态分布**。这允许我们将观察数据的似然性建模为单个概率密度函数（PDF）的乘积。为了简化，我们对这种似然性取自然对数，将其转化为单个
    PDF 对数的和。需要注意的是，我们使用密度函数用于连续变量，而不是概率质量函数用于离散变量。
- en: '![](../Images/b547842b5c3ce036820918533fac0f26.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b547842b5c3ce036820918533fac0f26.png)'
- en: 'Note: Likelihood calculations differ for discrete and continuous variables.
    Discrete variables use a probability mass function, while continuous variables
    employ a probability density function. For more on MLE, refer to my [previous
    post](https://medium.com/p/65218b2c2b99).'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：似然计算在离散变量和连续变量之间有所不同。离散变量使用概率质量函数，而连续变量使用概率密度函数。有关 MLE 的更多信息，请参阅我的 [上一篇文章](https://medium.com/p/65218b2c2b99)。
- en: 'When we examine the log likelihood, it comprises two parts: a constant component
    and a variable component that calculates the squared differences between the true
    labels and predictions. **To maximize this log likelihood, we focus on minimizing
    the variable component, which is essentially the sum of squared residuals.** In
    the context of linear regression, this minimization equates to minimizing MSE,
    especially when we consider the scaling factor 1/2*σ*² that arises from the normal
    distribution assumption.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们检查对数似然时，它包含两个部分：一个常数成分和一个变量成分，后者计算真实标签与预测之间的平方差。**为了最大化这种对数似然，我们专注于最小化变量成分，这本质上是平方残差的总和**。在实际线性回归的背景下，这种最小化等同于最小化
    MSE，尤其是当我们考虑到由正态分布假设引起的缩放因子 1/2*σ*² 时。
- en: In summary, MSE can be derived and understood from both the perspectives of
    the Least Squares Estimation (LSE) and MLE, with each approach providing a unique
    lens into the significance and application of MSE in regression analysis.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，MSE 可以从最小二乘估计（LSE）和 MLE 两种角度推导和理解，每种方法提供了一个独特的视角来探讨 MSE 在回归分析中的重要性和应用。
- en: So MSE is a common loss function for regression problem. But can I use it for
    classification problem? Such as logistic regression?
  id: totrans-64
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 所以 MSE 是回归问题中常见的损失函数。但我可以将它用于分类问题吗？比如逻辑回归？
- en: 'MSE, while common in regression, isn’t ideal for classification tasks, such
    as logistic regression. **The primary reason is the mismatch in the nature of
    outputs**: logistic regression predicts probabilities, whereas MSE assumes continuous
    numerical values. This misalignment leads to theoretical and practical challenges.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: MSE 虽然在回归中很常见，但对于分类任务，如逻辑回归，并不理想。**主要原因是输出的性质不匹配**：逻辑回归预测的是概率，而 MSE 假设的是连续数值。这种不一致导致了理论和实际应用中的挑战。
- en: Practically, **MSE creates a non-convex loss surface when combined with logistic
    regression, which often uses a sigmoid activation function**. This non-convexity
    means the error surface has multiple local minima, making it difficult for optimization
    algorithms like gradient descent to find the global minimum. Essentially, the
    algorithm might get ‘stuck’ in a local minimum, leading to suboptimal model performance.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，**MSE 在与逻辑回归结合时会产生非凸的损失面，逻辑回归通常使用 sigmoid 激活函数**。这种非凸性意味着误差面有多个局部最小值，使得像梯度下降这样的优化算法难以找到全局最小值。基本上，算法可能会在局部最小值处“卡住”，导致模型性能次优。
- en: Moreover, **combining MSE with the sigmoid function can cause the gradients
    to become very small,** particularly for extreme input values. This leads to the
    ‘gradient vanishing’ problem, where the model stops learning or learns very slowly
    because the updates to the model parameters become insignificantly small.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，**将 MSE 与 sigmoid 函数结合可能导致梯度变得非常小**，特别是对于极端输入值。这会导致“梯度消失”问题，使得模型停止学习或学习非常缓慢，因为模型参数的更新变得微不足道。
- en: Therefore, for classification problems, especially binary ones like logistic
    regression, MSE is not a idea loss function.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，对于分类问题，尤其是像逻辑回归这样的二分类问题，MSE 并不是一个理想的损失函数。
- en: So what is a good loss function for logistic regression or more general classification
    problem?
  id: totrans-69
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 那么，什么是适用于逻辑回归或更一般分类问题的良好损失函数呢？
- en: Alright, diving into the world of loss functions for logistic regression, let’s
    see how we can apply some basic design ideas to understand them better.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，让我们深入探讨逻辑回归的损失函数世界，看看如何应用一些基本的设计理念来更好地理解它们。
- en: First off, let’s look at the **least squares method**. The core idea here is
    to minimize the gap between our model’s output and the true labels. A straightforward
    approach is setting a threshold to convert logistic regression’s probability outputs
    into binary labels, and then comparing these with the true labels. If we choose,
    say, a 0.5 threshold for classifying donuts and bagels, we label predictions above
    0.5 as donuts and below as bagels, then tally up the mismatches. This approach,
    known as the **0–1 loss**, is directly corresponds to accuracy but isn’t used
    as a loss function for training due to its non-differentiability and non-convex
    nature, making it impractical for optimization methods like gradient descent.
    It’s more of a conceptual approach than a practical loss function.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们看看**最小二乘法**。这里的核心思想是最小化模型输出与真实标签之间的差距。一种简单的方法是设定一个阈值，将逻辑回归的概率输出转换为二元标签，然后将这些标签与真实标签进行比较。如果我们选择0.5作为分类甜甜圈和百吉饼的阈值，我们将大于0.5的预测标记为甜甜圈，小于0.5的标记为百吉饼，然后计算不匹配的数量。这种方法称为**0–1损失**，它直接对应于准确度，但由于其不可导性和非凸性，未被用作训练的损失函数，使得其在梯度下降等优化方法中不够实用。它更多的是一种概念上的方法，而非实际的损失函数。
- en: '![](../Images/216a6e4af686aacf01557f47f5b07b25.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/216a6e4af686aacf01557f47f5b07b25.png)'
- en: When I first visited America, I couldn’t tell the difference between a donut
    and a bagel. A classifier to distinguish between donuts and bagels could be useful.
    Image created by ChatGPT
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 当我第一次访问美国时，我无法分辨甜甜圈和百吉饼之间的区别。一个区分甜甜圈和百吉饼的分类器可能会很有用。图像由 ChatGPT 创建
- en: Moving on, let’s use the **MLE (Maximum Likelihood Estimation)** idea. In logistic
    regression, MLE tries to find the weights and bias that maximize the probability
    of seeing the actual observed data. Imagine our goal is to find a set of weights
    and bias that maximize the log likelihood, where the likelihood *L* is the product
    of individual probabilities of observing each outcome. We’re **assuming our data
    points are independent and each follows a Bernoulli distribution**.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们使用**最大似然估计（MLE）**的概念。在逻辑回归中，MLE试图找到最大化实际观察数据概率的权重和偏置。假设我们的目标是找到一组最大化对数似然的权重和偏置，其中似然
    *L* 是观察每个结果的个体概率的乘积。我们**假设我们的数据点是独立的，并且每个点都遵循伯努利分布**。
- en: '![](../Images/82b771c76bb5c0ae7e38e6403b785ea2.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/82b771c76bb5c0ae7e38e6403b785ea2.png)'
- en: 'So we’d have the log loss as:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们有了对数损失函数：
- en: '![](../Images/6526f99b664f1ec4f63f58d351f4b3e6.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6526f99b664f1ec4f63f58d351f4b3e6.png)'
- en: Finally, let’s bring in some **information theory**, treating logistic regression
    as a signal capture machine. In this approach, we employ concepts like entropy
    and cross-entropy to assess the information our model captures. Entropy measures
    the amount of uncertainty or surprise in an event. Cross-entropy gauge how well
    our model’s predicted probability distribution lines up with the actual, true
    distribution. The goal here is to minimize cross-entropy, which is kind of like
    minimizing the KL divergence. Though not exactly a ‘distance’ in the strict sense,
    KL divergence represents how far off our model’s predictions are from the actual
    labels.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们引入一些**信息论**的概念，将逻辑回归视为一个信号捕获机器。在这种方法中，我们使用熵和交叉熵等概念来评估我们模型捕获的信息。熵度量事件的不确定性或惊讶程度。交叉熵衡量我们模型的预测概率分布与实际真实分布的匹配程度。这里的目标是最小化交叉熵，这类似于最小化KL散度。虽然KL散度在严格意义上不是一种“距离”，但它表示我们的模型预测与实际标签之间的偏差程度。
- en: '![](../Images/7149620e9ba95c8fb5a023ac6ed58826.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7149620e9ba95c8fb5a023ac6ed58826.png)'
- en: 'Softmax is another topic on my writing list. Source: [*https://towardsdatascience.com/cross-entropy-loss-function-f38c4ec8643e*](/cross-entropy-loss-function-f38c4ec8643e)'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: Softmax是我写作列表中的另一个主题。来源：[*https://towardsdatascience.com/cross-entropy-loss-function-f38c4ec8643e*](/cross-entropy-loss-function-f38c4ec8643e)
- en: So, through the application of three distinct design principles for loss functions,
    we’ve crafted various types of loss functions suitable for logistic regression
    and broader classification challenges.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，通过应用三种不同的设计原则，我们已经制定了各种适合逻辑回归和更广泛分类挑战的损失函数。
- en: 'It’s particularly fascinating to observe that, despite originating from different
    perspectives**, log loss and cross-entropy loss are essentially the same in the
    context of binary classification**. This convergence occurs in situations where
    only two possible outcomes exist; under these conditions, cross-entropy effortlessly
    simplifies and transforms into log loss. Comprehending this shift is vital for
    understanding the interplay and practical application of these theoretical concepts:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 特别有趣的是，尽管起源于不同的角度，**对数损失和交叉熵损失在二分类的背景下本质上是相同的**。这种收敛发生在只有两个可能结果的情况下；在这些条件下，交叉熵轻松简化并转化为对数损失。理解这种转变对于理解这些理论概念的相互作用和实际应用至关重要：
- en: '![](../Images/b0ede840635bb7f2759b59856a52f920.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b0ede840635bb7f2759b59856a52f920.png)'
- en: 'Derive log loss from cross-entropy loss. Source: [*https://towardsdatascience.com/cross-entropy-loss-function-f38c4ec8643e*](/cross-entropy-loss-function-f38c4ec8643e)'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 从交叉熵损失推导对数损失。来源：[*https://towardsdatascience.com/cross-entropy-loss-function-f38c4ec8643e*](/cross-entropy-loss-function-f38c4ec8643e)
- en: '**Author’s Note:** In the future, I’m considering delving into the fascinating
    world of information theory — a topic that, surprisingly, is both intuitive and
    practical in real-world applications. Until then, I highly recommend [Kiprono
    Elijah Koech’s post](/cross-entropy-loss-function-f38c4ec8643e) as an excellent
    resource on the subject. Stay tuned for more!'
  id: totrans-85
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**作者注：** 未来，我计划深入研究信息理论这一迷人的领域——这一主题令人惊讶地既直观又在现实应用中非常实用。在此之前，我强烈推荐[Kiprono
    Elijah Koech的文章](/cross-entropy-loss-function-f38c4ec8643e)作为该主题的优秀资源。敬请关注更多内容！'
- en: In practical scenarios, how should one approach the situation where multiple
    loss functions need to be minimized?
  id: totrans-86
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在实际场景中，应该如何处理需要最小化多个损失函数的情况？
- en: When managing multiple loss functions in a model, balancing them can be challenging,
    as they may conflict. One common approach is to create a weighted sum of these
    loss functions, assigning specific weights to each. However, this introduces new
    hyperparameters (the weights), necessitating careful tuning. Adjusting these weights
    means retraining the model, which can be time-consuming and may affect interpretability
    and performance.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 在模型中管理多个损失函数时，平衡它们可能会面临挑战，因为它们可能会冲突。一种常见的方法是创建这些损失函数的加权和，为每个损失函数分配特定的权重。然而，这会引入新的超参数（权重），需要仔细调整。调整这些权重意味着需要重新训练模型，这可能是耗时的，并且可能会影响模型的可解释性和性能。
- en: Alternatively, a constraint-based approach can be effective. For instance, in
    SVM, we aim to maximize the margin (reducing variance) while minimizing classification
    error (reducing bias). This can be achieved by treating the margin maximization
    as a constraint, using techniques like Lagrange multipliers, and focusing on minimizing
    the classification error. This method requires a strong mathematical foundation
    and thoughtful formulation of constraints.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，一种基于约束的方法也可以有效。例如，在SVM中，我们的目标是最大化间隔（减少方差）同时最小化分类误差（减少偏差）。这可以通过将间隔最大化作为约束，使用拉格朗日乘数等技术，并专注于最小化分类误差来实现。这种方法需要坚实的数学基础和深思熟虑的约束制定。
- en: A third option is to decouple the objectives, building separate models for each
    and then combining their results. This approach simplifies model development and
    maintenance, as each model can be independently monitored and retrained. It also
    offers flexibility in responding to changes in objectives or business goals. However,
    it’s important to consider how the combined results of these models align with
    the overall objective.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 第三个选项是解耦目标，为每个目标构建单独的模型，然后将它们的结果进行组合。这种方法简化了模型开发和维护，因为每个模型可以独立监控和重新训练。它还提供了响应目标或业务目标变化的灵活性。然而，重要的是要考虑这些模型的组合结果如何与整体目标对齐。
- en: However, it’s important to understand that adversarial loss in GANs isn’t just
    a combination of the discriminator’s and generator’s losses. This is because the
    two networks are engaged in a responsive interaction, learning and adapting in
    response to each other, rather than optimizing their losses independently.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，重要的是要理解GAN中的对抗性损失不仅仅是鉴别器和生成器损失的组合。这是因为两个网络处于响应性互动中，相互学习和适应，而不是独立优化它们的损失。
- en: 'Before we conclude, I’d like to address a straightforward yet practical query:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们结束之前，我想解决一个简单但实用的问题：
- en: Why do we sometimes prefer using RMSE (Root Mean Squared Error) instead of MSE
    ?
  id: totrans-92
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为什么我们有时更倾向于使用RMSE（均方根误差）而不是MSE？
- en: MSE (Root Mean Squared Error) is often preferred over MSE (Mean Squared Error)
    in certain situations due to its interpretability. By taking the square root of
    MSE, RMSE converts the error units back to the original units of the data. This
    makes RMSE more intuitive and directly comparable to the scale of the data being
    analyzed. For instance, if you’re predicting housing prices, RMSE provides an
    error metric in the same unit as the prices themselves, making it easier to understand
    the magnitude of the errors.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: MSE（均方误差）在某些情况下由于其可解释性，通常优于MSE（均方误差）。通过取MSE的平方根，RMSE将误差单位转换回数据的原始单位。这使得RMSE更加直观，并且可以直接与分析数据的尺度进行比较。例如，如果你在预测房价，RMSE提供了一个与价格本身相同单位的误差指标，使得理解误差的大小更为容易。
- en: Additionally, **RMSE is more sensitive to larger errors than RMAE (Root Mean
    Absolute Error) due to the square root transformation**, emphasizing significant
    deviations more than RMAE. This can be particularly useful in scenarios where
    larger errors are more undesirable.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，**RMSE对较大误差比RMAE（均方绝对误差）更为敏感，因为平方根转换**，强调了比RMAE更显著的偏差。这在较大误差更不受欢迎的情况下尤其有用。
- en: '*(Unless otherwise noted, all images are by the author)*'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '*(除非另有说明，所有图片均由作者提供)*'
- en: If you’re enjoying t[his series](http://towardsdatascience.com/tagged/courage-to-learn-ml),
    remember that your interactions — claps, comments, and follows — do more than
    just support; they’re the driving force that keeps this series going and inspires
    my continued sharing.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你喜欢[这个系列](http://towardsdatascience.com/tagged/courage-to-learn-ml)，记住你的互动——点赞、评论和关注——不仅仅是支持；它们是推动这个系列持续更新和激励我继续分享的动力。
- en: 'Other posts in this series:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 本系列的其他文章：
- en: '[Courage to Learn ML: Demystifying L1 & L2 Regularization (part 1)](/understanding-l1-l2-regularization-part-1-9c7affe6f920)'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[勇敢学习机器学习：解密L1和L2正则化（第1部分）](/understanding-l1-l2-regularization-part-1-9c7affe6f920)'
- en: '[Courage to Learn ML: Decoding Likelihood, MLE, and MAP](/courage-to-learn-ml-decoding-likelihood-mle-and-map-65218b2c2b99)'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[勇敢学习机器学习：解码似然性、MLE和MAP](/courage-to-learn-ml-decoding-likelihood-mle-and-map-65218b2c2b99)'
- en: '[Courage to Learn ML: A Deeper Dive into F1, Recall, Precision, and ROC Curves](https://medium.com/p/d5c0a46e5eb7)'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[勇敢学习机器学习：深入探讨F1、召回率、精确度和ROC曲线](https://medium.com/p/d5c0a46e5eb7)'
- en: '***If you liked the article, you can find me on*** [***LinkedIn***](https://www.linkedin.com/in/amyma101/)***.***'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '***如果你喜欢这篇文章，可以在*** [***LinkedIn***](https://www.linkedin.com/in/amyma101/)***找到我。***'
