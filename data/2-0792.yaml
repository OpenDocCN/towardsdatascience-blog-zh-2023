- en: Efficiently Serving Open Source LLMs
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: é«˜æ•ˆæœåŠ¡å¼€æº LLM
- en: åŸæ–‡ï¼š[https://towardsdatascience.com/efficiently-serving-open-source-llms-5f0bf5d8fd59](https://towardsdatascience.com/efficiently-serving-open-source-llms-5f0bf5d8fd59)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡ï¼š[https://towardsdatascience.com/efficiently-serving-open-source-llms-5f0bf5d8fd59](https://towardsdatascience.com/efficiently-serving-open-source-llms-5f0bf5d8fd59)
- en: '[](https://medium.com/@ryanshrott?source=post_page-----5f0bf5d8fd59--------------------------------)[![Ryan
    Shrott](../Images/186524066383b4b02c994692aebb3ea5.png)](https://medium.com/@ryanshrott?source=post_page-----5f0bf5d8fd59--------------------------------)[](https://towardsdatascience.com/?source=post_page-----5f0bf5d8fd59--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----5f0bf5d8fd59--------------------------------)
    [Ryan Shrott](https://medium.com/@ryanshrott?source=post_page-----5f0bf5d8fd59--------------------------------)'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@ryanshrott?source=post_page-----5f0bf5d8fd59--------------------------------)[![Ryan
    Shrott](../Images/186524066383b4b02c994692aebb3ea5.png)](https://medium.com/@ryanshrott?source=post_page-----5f0bf5d8fd59--------------------------------)[](https://towardsdatascience.com/?source=post_page-----5f0bf5d8fd59--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----5f0bf5d8fd59--------------------------------)
    [Ryan Shrott](https://medium.com/@ryanshrott?source=post_page-----5f0bf5d8fd59--------------------------------)'
- en: Â·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----5f0bf5d8fd59--------------------------------)
    Â·5 min readÂ·Aug 14, 2023
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: Â·å‘å¸ƒåœ¨ [Towards Data Science](https://towardsdatascience.com/?source=post_page-----5f0bf5d8fd59--------------------------------)
    Â·é˜…è¯»æ—¶é•¿ 5 åˆ†é’ŸÂ·2023 å¹´ 8 æœˆ 14 æ—¥
- en: --
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/c16d4589aafa99416a26da8bff1b5afe.png)'
  id: totrans-5
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c16d4589aafa99416a26da8bff1b5afe.png)'
- en: Photo by [Mariia Shalabaieva](https://unsplash.com/@maria_shalabaieva?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ç‰‡æ¥æºï¼š[Mariia Shalabaieva](https://unsplash.com/@maria_shalabaieva?utm_source=medium&utm_medium=referral)
    äº [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: 'This article explains my personal experiences using 6 common methods for serving
    open source LLMs: AWS Sage Maker, Hugging Face, Together.AI, VLLM and Petals.ml.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬æ–‡è§£é‡Šäº†æˆ‘ä¸ªäººä½¿ç”¨ 6 ç§å¸¸è§æ–¹æ³•æœåŠ¡å¼€æº LLM çš„ç»éªŒï¼šAWS Sage Makerã€Hugging Faceã€Together.AIã€VLLM
    å’Œ Petals.mlã€‚
- en: The struggleâ€¦
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æŒ£æ‰â€¦
- en: 'Youâ€™ve felt the pain, struggle and glory of serving your own fine-tuned open
    source LLM, however, you ultimately decided to return to Open AI or Anthropic
    due to cost, inference time, reliability and technology challenges :( Youâ€™ve also
    given up on renting a A100 GPU (many providers have GPUs fully booked until the
    end of 2023!). And you donâ€™t have 100K to shell out for a 2 tier A100 server box.
    Still, youâ€™re dreaming, and you really want to get open source to work for your
    solution. Perhaps your firm does not want to send itâ€™s private data to Open AI
    or you want a fine tuned model for a very specific task? In this article, I will
    outline and compare some of the most effective inference methods/platforms for
    serving open source LLMs in 2023\. I will compare and contrast 6 methods and explain
    when you should use one or the other. I have personally tried all 6 of these and
    will detail my personal experience with these solutions: **AWS Sage Maker, Hugging
    Face Inference endpoints, Together.AI, VLLM and Petals.ml**. I donâ€™t have all
    the answers, but I will do my best to detail my experiences. I have no monetary
    connection with any of these providers and am simply sharing my experiences for
    the benefit of others. Please tell about your experiences!'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ å·²ç»æ„Ÿå—åˆ°äº†æœåŠ¡è‡ªå·±å¾®è°ƒçš„å¼€æº LLM çš„ç—›è‹¦ã€æŒ£æ‰å’Œè£è€€ï¼Œä½†ä½ æœ€ç»ˆå› ä¸ºæˆæœ¬ã€æ¨ç†æ—¶é—´ã€å¯é æ€§å’ŒæŠ€æœ¯æŒ‘æˆ˜è€Œå†³å®šå›åˆ° Open AI æˆ– Anthropic
    :( ä½ ä¹Ÿæ”¾å¼ƒäº†ç§Ÿç”¨ A100 GPUï¼ˆè®¸å¤šä¾›åº”å•†çš„ GPU å·²ç»è¢«é¢„è®¢åˆ° 2023 å¹´åº•ï¼ï¼‰ã€‚ä½ ä¹Ÿæ²¡æœ‰ 10 ä¸‡ç¾å…ƒå»è´­ä¹°ä¸€ä¸ª 2 çº§ A100 æœåŠ¡å™¨ã€‚å°½ç®¡å¦‚æ­¤ï¼Œä½ ä»åœ¨æ¢¦æƒ³ï¼Œä½ çœŸçš„å¸Œæœ›å¼€æºèƒ½å¤Ÿä¸ºä½ çš„è§£å†³æ–¹æ¡ˆæœåŠ¡ã€‚ä¹Ÿè®¸ä½ çš„å…¬å¸ä¸æ„¿æ„å°†ç§äººæ•°æ®å‘é€ç»™
    Open AIï¼Œæˆ–è€…ä½ éœ€è¦ä¸€ä¸ªé’ˆå¯¹ç‰¹å®šä»»åŠ¡å¾®è°ƒçš„æ¨¡å‹ï¼Ÿåœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘å°†æ¦‚è¿°å¹¶æ¯”è¾ƒä¸€äº›æœ€æœ‰æ•ˆçš„æ¨ç†æ–¹æ³•/å¹³å°ï¼Œç”¨äºæœåŠ¡å¼€æº LLMã€‚åœ¨ 2023 å¹´ï¼Œæˆ‘å°†å¯¹ 6
    ç§æ–¹æ³•è¿›è¡Œæ¯”è¾ƒï¼Œå¹¶è§£é‡Šä½•æ—¶åº”è¯¥ä½¿ç”¨å…¶ä¸­ä¸€ç§æˆ–å¦ä¸€ç§ã€‚æˆ‘äº²è‡ªå°è¯•äº†è¿™ 6 ç§æ–¹æ³•ï¼Œå¹¶å°†è¯¦ç»†ä»‹ç»æˆ‘çš„ä¸ªäººç»éªŒï¼š**AWS Sage Makerã€Hugging
    Face æ¨ç†ç«¯ç‚¹ã€Together.AIã€VLLM å’Œ Petals.ml**ã€‚æˆ‘æ²¡æœ‰æ‰€æœ‰çš„ç­”æ¡ˆï¼Œä½†æˆ‘ä¼šå°½åŠ›è¯¦ç»†è¯´æ˜æˆ‘çš„ç»éªŒã€‚æˆ‘ä¸è¿™äº›ä¾›åº”å•†æ²¡æœ‰ä»»ä½•ç»æµè”ç³»ï¼Œä»…ä»…æ˜¯åˆ†äº«æˆ‘çš„ç»éªŒä»¥é€ ç¦ä»–äººã€‚è¯·åˆ†äº«ä½ çš„ç»éªŒï¼
- en: Why open source?
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä¸ºä»€ä¹ˆé€‰æ‹©å¼€æºï¼Ÿ
- en: Open source models have a plethora of advantages including control, privacy
    and potential cost reductions. For example, you could fine tune a smaller open
    source model for your particular use case, resulting in accurate results and fast
    inference time. Privacy control means that inference can be done on your own servers.
    On the other hand, cost reduction is much harder than you might think. Open AI
    has economies of scale and has competitive pricing. Their pricing model for GPT-3.5
    turbo is very hard to compete with, and has been shown to be similar to the cost
    of electricity. Still, there are methods and techniques you can use to save money
    and get excellent results with open source models. For example, my fine tuned
    model of [Stable Beluga 2](https://huggingface.co/stabilityai/StableBeluga2) is
    currently outperforming GPT-3.5 Turbo significantly, and is cheaper for my application.
    So I strongly suggest giving open source a shot for your application.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: å¼€æºæ¨¡å‹æœ‰è®¸å¤šä¼˜ç‚¹ï¼ŒåŒ…æ‹¬æ§åˆ¶ã€éšç§å’Œæ½œåœ¨çš„æˆæœ¬é™ä½ã€‚ä¾‹å¦‚ï¼Œä½ å¯ä»¥é’ˆå¯¹ç‰¹å®šçš„ä½¿ç”¨æ¡ˆä¾‹å¾®è°ƒä¸€ä¸ªè¾ƒå°çš„å¼€æºæ¨¡å‹ï¼Œä»è€Œè·å¾—å‡†ç¡®çš„ç»“æœå’Œå¿«é€Ÿçš„æ¨ç†æ—¶é—´ã€‚éšç§æ§åˆ¶æ„å‘³ç€æ¨ç†å¯ä»¥åœ¨è‡ªå·±çš„æœåŠ¡å™¨ä¸Šå®Œæˆã€‚å¦ä¸€æ–¹é¢ï¼Œæˆæœ¬é™ä½æ¯”ä½ æƒ³è±¡çš„è¦å›°éš¾å¾—å¤šã€‚OpenAI
    æ‹¥æœ‰è§„æ¨¡ç»æµï¼Œå®šä»·å…·æœ‰ç«äº‰åŠ›ã€‚ä»–ä»¬çš„ GPT-3.5 Turbo å®šä»·æ¨¡å¼å¾ˆéš¾ä¸ä¹‹ç«äº‰ï¼Œå¹¶ä¸”å·²è¢«è¯æ˜ç±»ä¼¼äºç”µåŠ›æˆæœ¬ã€‚ä¸è¿‡ï¼Œä½ ä»ç„¶å¯ä»¥é‡‡ç”¨ä¸€äº›æ–¹æ³•å’ŒæŠ€å·§æ¥èŠ‚çœå¼€æ”¯ï¼Œå¹¶ç”¨å¼€æºæ¨¡å‹è·å¾—ä¼˜ç§€çš„ç»“æœã€‚ä¾‹å¦‚ï¼Œæˆ‘çš„å¾®è°ƒæ¨¡å‹
    [Stable Beluga 2](https://huggingface.co/stabilityai/StableBeluga2) ç›®å‰æ˜¾è‘—ä¼˜äº GPT-3.5
    Turboï¼Œå¹¶ä¸”åœ¨æˆ‘çš„åº”ç”¨ä¸­æ›´ä¾¿å®œã€‚å› æ­¤ï¼Œæˆ‘å¼ºçƒˆå»ºè®®ä½ å°è¯•ä½¿ç”¨å¼€æºæ¨¡å‹ã€‚
- en: Hugging Face Inference Endpoints
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Hugging Face æ¨ç†ç«¯ç‚¹
- en: This is the most common and simplest method for serving an open source LLM.
    It only takes a couple clicks and is foolproof. After all, Hugging Face was originally
    an NLP company. Your model probably already exists on hugging face as well, so
    this should be the go-to option for quickly testing your model. The GPU server
    costs due tend to run on the higher side. For example, if you simply used RunPod.io
    to deploy your model, you will have more options for providers and lower costs.
    Hugging Face has open sourced their transformers inference library and have provided
    a docker image that is easy to modify. So if you want more control go with a custom
    solution on RunPod. [Here is a](https://www.youtube.com/watch?v=FdcXJ7d3WQU&t=224s&ab_channel=VenelinValkov)
    tutorial on how to do it on RunPod.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯æœåŠ¡å¼€æº LLM æœ€å¸¸è§ä¸”æœ€ç®€å•çš„æ–¹æ³•ã€‚åªéœ€ç‚¹å‡»å‡ ä¸‹å³å¯ï¼Œä¸”å‡ ä¹æ²¡æœ‰é”™è¯¯ã€‚æ¯•ç«Ÿï¼ŒHugging Face æœ€åˆæ˜¯ä¸€å®¶ NLP å…¬å¸ã€‚ä½ çš„æ¨¡å‹å¾ˆå¯èƒ½å·²ç»å­˜åœ¨äº
    Hugging Face ä¸Šï¼Œå› æ­¤è¿™æ˜¯å¿«é€Ÿæµ‹è¯•æ¨¡å‹çš„é¦–é€‰é€‰é¡¹ã€‚GPU æœåŠ¡å™¨æˆæœ¬å¾€å¾€è¾ƒé«˜ã€‚ä¾‹å¦‚ï¼Œå¦‚æœä½ ä»…ä½¿ç”¨ RunPod.io éƒ¨ç½²æ¨¡å‹ï¼Œä½ å°†æœ‰æ›´å¤šçš„æä¾›å•†é€‰æ‹©ï¼Œå¹¶ä¸”æˆæœ¬æ›´ä½ã€‚Hugging
    Face å·²å¼€æºäº†ä»–ä»¬çš„ Transformers æ¨ç†åº“ï¼Œå¹¶æä¾›äº†æ˜“äºä¿®æ”¹çš„ Docker é•œåƒã€‚å› æ­¤ï¼Œå¦‚æœä½ éœ€è¦æ›´å¤šæ§åˆ¶ï¼Œå¯ä»¥é€‰æ‹©åœ¨ RunPod ä¸Šçš„è‡ªå®šä¹‰è§£å†³æ–¹æ¡ˆã€‚
    [è¿™æ˜¯ä¸€ä¸ª](https://www.youtube.com/watch?v=FdcXJ7d3WQU&t=224s&ab_channel=VenelinValkov)å…³äºå¦‚ä½•åœ¨
    RunPod ä¸Šæ“ä½œçš„æ•™ç¨‹ã€‚
- en: VLLM
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: VLLM
- en: '[This solution](https://github.com/vllm-project/vllm) is very interesting due
    to the inference speed. They claim to be 24 times faster than hugging face transformers!
    Using it personally, I found the speed to be about 10 times faster than Hugging
    Face transformers. I found there to be a few bugs here and there though. This
    project is being actively worked on, and is not foolproof yet. Still, I strongly
    suggest you try it for your application. Due to the faster inference speed, the
    cost will be significantly lower in comparison to HF transformers.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '[è¿™ä¸ªè§£å†³æ–¹æ¡ˆ](https://github.com/vllm-project/vllm)ç”±äºå…¶æ¨ç†é€Ÿåº¦è€Œéå¸¸æœ‰è¶£ã€‚ä»–ä»¬å£°ç§°æ¯” Hugging Face
    çš„ Transformers å¿« 24 å€ï¼åœ¨æˆ‘ä¸ªäººä½¿ç”¨æ—¶ï¼Œå‘ç°é€Ÿåº¦å¤§çº¦æ˜¯ Hugging Face Transformers çš„ 10 å€ã€‚ä¸è¿‡ï¼Œæˆ‘å‘ç°è¿™é‡Œæœ‰ä¸€äº›å°
    bugã€‚è¿™ä¸ªé¡¹ç›®æ­£åœ¨ç§¯æå¼€å‘ä¸­ï¼Œå°šæœªå®Œå–„ã€‚ä¸è¿‡ï¼Œæˆ‘ä»ç„¶å¼ºçƒˆå»ºè®®ä½ è¯•è¯•è¿™ä¸ªè§£å†³æ–¹æ¡ˆã€‚ç”±äºæ¨ç†é€Ÿåº¦æ›´å¿«ï¼Œç›¸è¾ƒäº HF Transformersï¼Œæˆæœ¬å°†æ˜¾è‘—é™ä½ã€‚'
- en: '![](../Images/f05be35a29639954eae27b280d6dc2af.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f05be35a29639954eae27b280d6dc2af.png)'
- en: 'Source: [https://github.com/vllm-project/vllm](https://github.com/vllm-project/vllm)'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 'æ¥æº: [https://github.com/vllm-project/vllm](https://github.com/vllm-project/vllm)'
- en: Petals.ml
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Petals.ml
- en: '[This](https://github.com/bigscience-workshop/petals) is the most intereresting
    solution. The developers at Petals.ml have discovered a way to run LLMs at home,
    BitTorrent-style. This allows you to do fine-tuning and inference up to 10 times
    faster than offloading. In practice, this means that only a small part of the
    model will be loaded on your own GPU, and the rest will exist on a GPU network
    swarm. In other words, a network of GPUs will work together in order to do the
    compute. This is very interesting because it democratizes LLM usage to some extent,
    i.e. anyone has the ability to run huge LLMs without paying a cent! The paper
    of the technology used can be [found here](https://arxiv.org/abs/2209.01188).
    I strongly suggest you give Petals.ml a try!'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '[è¿™ä¸ª](https://github.com/bigscience-workshop/petals)æ˜¯æœ€æœ‰è¶£çš„è§£å†³æ–¹æ¡ˆã€‚Petals.mlçš„å¼€å‘è€…å‘ç°äº†ä¸€ç§åœ¨å®¶è¿è¡ŒLLMsçš„æ–¹æ³•ï¼Œç±»ä¼¼äºBitTorrentã€‚è¿™ä½¿å¾—å¾®è°ƒå’Œæ¨ç†çš„é€Ÿåº¦æ¯”å¸è½½å¿«æœ€å¤š10å€ã€‚å®é™…ä¸Šï¼Œè¿™æ„å‘³ç€æ¨¡å‹çš„åªæœ‰ä¸€å°éƒ¨åˆ†ä¼šåŠ è½½åˆ°ä½ è‡ªå·±çš„GPUä¸Šï¼Œå…¶ä½™éƒ¨åˆ†ä¼šå­˜åœ¨äºGPUç½‘ç»œç¾¤ä¸­ã€‚æ¢å¥è¯è¯´ï¼Œä¸€ä¸ªGPUç½‘ç»œå°†åä½œè¿›è¡Œè®¡ç®—ã€‚è¿™éå¸¸æœ‰è¶£ï¼Œå› ä¸ºå®ƒåœ¨ä¸€å®šç¨‹åº¦ä¸Šä½¿LLMçš„ä½¿ç”¨å¾—åˆ°æ°‘ä¸»åŒ–ï¼Œå³ä»»ä½•äººéƒ½å¯ä»¥è¿è¡Œå¤§å‹LLMè€Œä¸èŠ±ä¸€åˆ†é’±ï¼ç›¸å…³æŠ€æœ¯çš„è®ºæ–‡å¯ä»¥[åœ¨è¿™é‡Œæ‰¾åˆ°](https://arxiv.org/abs/2209.01188)ã€‚æˆ‘å¼ºçƒˆå»ºè®®ä½ è¯•è¯•Petals.mlï¼'
- en: Together.AI
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Together.AI
- en: '[They](https://together.ai/) provide an API for open source models with excellent
    pricing. You can fine tune and deploy open source models using Together.AI compute
    cluster. Their pricing is 20% of AWS. Their platform is straightforward and easy
    to get started. Therefore, I would highly suggest this platform. Their API is
    about [1/10th the price](https://together.ai/pricing) of GPT-3.5 turbo. This is
    my new favorite way to deploy open source models!'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '[ä»–ä»¬](https://together.ai/)æä¾›äº†ä¸€ä¸ªå…·æœ‰å‡ºè‰²å®šä»·çš„å¼€æºæ¨¡å‹APIã€‚ä½ å¯ä»¥ä½¿ç”¨Together.AIè®¡ç®—é›†ç¾¤å¯¹å¼€æºæ¨¡å‹è¿›è¡Œå¾®è°ƒå’Œéƒ¨ç½²ã€‚ä»–ä»¬çš„å®šä»·æ˜¯AWSçš„20%ã€‚ä»–ä»¬çš„å¹³å°ç®€å•ç›´è§‚ï¼Œå®¹æ˜“ä¸Šæ‰‹ã€‚å› æ­¤ï¼Œæˆ‘å¼ºçƒˆæ¨èè¿™ä¸ªå¹³å°ã€‚ä»–ä»¬çš„APIä»·æ ¼å¤§çº¦æ˜¯[GPT-3.5
    turboçš„1/10](https://together.ai/pricing)ã€‚è¿™æ˜¯æˆ‘ç°åœ¨æœ€å–œæ¬¢çš„å¼€æºæ¨¡å‹éƒ¨ç½²æ–¹å¼ï¼'
- en: AWS Sagemaker
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: AWS Sagemaker
- en: 'The tried and true method for deploying ML models. Sagemaker is not particular
    beginner friendly and is probably the most expensive method in comparison to the
    methods provided above. Itâ€™s also the most complex. However, if your business
    is already using AWS, this may be your only option. Also, if you have free compute
    on AWS like me, why not give it a shot? Here is a tutorial on how to do it by
    AI Anytime: [https://www.youtube.com/watch?v=A9Pu4xg-Nas&ab_channel=AIAnytime](https://www.youtube.com/watch?v=A9Pu4xg-Nas&ab_channel=AIAnytime).'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: éƒ¨ç½²MLæ¨¡å‹çš„æˆç†Ÿæ–¹æ³•ã€‚Sagemakerå¯¹åˆå­¦è€…ä¸å¤ªå‹å¥½ï¼Œè€Œä¸”ä¸ä¸Šè¿°æ–¹æ³•ç›¸æ¯”ï¼Œå®ƒå¯èƒ½æ˜¯æœ€æ˜‚è´µçš„ã€‚å®ƒä¹Ÿæ˜¯æœ€å¤æ‚çš„ã€‚ç„¶è€Œï¼Œå¦‚æœä½ çš„ä¸šåŠ¡å·²ç»åœ¨ä½¿ç”¨AWSï¼Œè¿™å¯èƒ½æ˜¯ä½ å”¯ä¸€çš„é€‰æ‹©ã€‚æ­¤å¤–ï¼Œå¦‚æœä½ åƒæˆ‘ä¸€æ ·åœ¨AWSä¸Šæœ‰å…è´¹çš„è®¡ç®—èµ„æºï¼Œä¸ºä»€ä¹ˆä¸å°è¯•ä¸€ä¸‹å‘¢ï¼Ÿè¿™æ˜¯AI
    Anytimeçš„æ•™ç¨‹ï¼š[https://www.youtube.com/watch?v=A9Pu4xg-Nas&ab_channel=AIAnytime](https://www.youtube.com/watch?v=A9Pu4xg-Nas&ab_channel=AIAnytime)ã€‚
- en: 'Conclusion:'
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ç»“è®ºï¼š
- en: In conclusion, I highly suggest trying Together.AI and Petals.ml due to the
    many upsides in using these platforms. If you require privacy and very fast inference
    speeds, I suggest using VLLM. If you are forced to use AWS, then go with SageMaker.
    Finally, if you want something simple and efficient (especially for testing),
    go with HF transformers endpoints.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: æ€»ç»“æ¥è¯´ï¼Œæˆ‘å¼ºçƒˆå»ºè®®å°è¯•Together.AIå’ŒPetals.mlï¼Œå› ä¸ºä½¿ç”¨è¿™äº›å¹³å°æœ‰è®¸å¤šä¼˜åŠ¿ã€‚å¦‚æœä½ éœ€è¦éšç§å’Œéå¸¸å¿«çš„æ¨ç†é€Ÿåº¦ï¼Œæˆ‘å»ºè®®ä½¿ç”¨VLLMã€‚å¦‚æœä½ è¢«è¿«ä½¿ç”¨AWSï¼Œé‚£ä¹ˆé€‰æ‹©SageMakerã€‚å¦‚æœä½ æƒ³è¦ç®€å•é«˜æ•ˆçš„æ–¹æ¡ˆï¼ˆç‰¹åˆ«æ˜¯ç”¨äºæµ‹è¯•ï¼‰ï¼Œå¯ä»¥é€‰æ‹©HF
    transformersç«¯ç‚¹ã€‚
- en: 'ğŸ“¢ Hey there! If you found this article helpful, please consider:'
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ğŸ“¢ å—¨ï¼å¦‚æœä½ è§‰å¾—è¿™ç¯‡æ–‡ç« æœ‰å¸®åŠ©ï¼Œè¯·è€ƒè™‘ï¼š
- en: ğŸ‘ Clapping 50 times (this helps a lot!)
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ‘ é¼“æŒ50æ¬¡ï¼ˆè¿™å¾ˆæœ‰å¸®åŠ©ï¼ï¼‰
- en: âœï¸ Leaving a comment
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: âœï¸ ç•™ä¸‹è¯„è®º
- en: ğŸŒŸ Highlighting parts you found insightful
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸŒŸ çªå‡ºä½ è§‰å¾—æœ‰è§åœ°çš„éƒ¨åˆ†
- en: ğŸ‘£ Following me
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ‘£ å…³æ³¨æˆ‘
- en: Any questions? ğŸ¤” Donâ€™t hesitate to ask. Supporting me this way is a free and
    easy way to show appreciation for my detailed tutorial articles! ğŸ˜Š
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: æœ‰ä»»ä½•é—®é¢˜å—ï¼ŸğŸ¤” ä¸è¦çŠ¹è±«ï¼Œå°½ç®¡é—®ã€‚ä»¥è¿™ç§æ–¹å¼æ”¯æŒæˆ‘æ˜¯å¯¹æˆ‘çš„è¯¦ç»†æ•™ç¨‹æ–‡ç« çš„ä¸€ç§å…è´¹è€Œç®€å•çš„æ„Ÿè°¢æ–¹å¼ï¼ğŸ˜Š
- en: Final Notes
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æœ€ç»ˆè¯´æ˜
- en: If youâ€™re interested in learning more about developing full stack AI apps only
    with Python, please feel free to [sign up here](https://saas.mixo.io/). As always,
    please leave your own experiences and comments below. I look forward to reading
    them all.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœä½ å¯¹ä»…ä½¿ç”¨Pythonå¼€å‘å…¨æ ˆAIåº”ç”¨æ„Ÿå…´è¶£ï¼Œè¯·éšæ—¶[æ³¨å†Œè¿™é‡Œ](https://saas.mixo.io/)ã€‚ä¸€å¦‚æ—¢å¾€ï¼Œè¯·åœ¨ä¸‹é¢ç•™ä¸‹ä½ çš„ç»éªŒå’Œè¯„è®ºã€‚æˆ‘æœŸå¾…é˜…è¯»æ‰€æœ‰è¯„è®ºã€‚
- en: Thatâ€™s all folks â€” if youâ€™ve made it this far, please comment below and add
    me on [LinkedIn](https://www.linkedin.com/in/ryanshrott/).
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: å°±è¿™äº›äº†ï¼Œå¦‚æœä½ è¯»åˆ°è¿™é‡Œï¼Œè¯·åœ¨ä¸‹é¢è¯„è®ºå¹¶åœ¨[LinkedIn](https://www.linkedin.com/in/ryanshrott/)ä¸Šæ·»åŠ æˆ‘ã€‚
- en: My Github is [here](https://github.com/ryanshrott).
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘çš„Githubåœ¨[è¿™é‡Œ](https://github.com/ryanshrott)ã€‚
- en: '**Other Deep Learning Blogs**'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '**å…¶ä»–æ·±åº¦å­¦ä¹ åšå®¢**'
- en: '[Sequence Models by Andrew Ng â€” 11 Lessons Learned](https://medium.com/towards-data-science/sequence-models-by-andrew-ng-11-lessons-learned-c62fb1d3485b)'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '[å®‰å¾·é²Â·å´çš„åºåˆ—æ¨¡å‹ â€” 11ä¸ªç»éªŒæ•™è®­](https://medium.com/towards-data-science/sequence-models-by-andrew-ng-11-lessons-learned-c62fb1d3485b)'
- en: '[Computer Vision by Andrew Ng â€” 11 Lessons Learned](/computer-vision-by-andrew-ng-11-lessons-learned-7d05c18a6999)'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '[å®‰å¾·é²Â·å´çš„è®¡ç®—æœºè§†è§‰ â€” 11ä¸ªç»éªŒæ•™è®­](/computer-vision-by-andrew-ng-11-lessons-learned-7d05c18a6999)'
- en: '[Deep Learning Specialization by Andrew Ng â€” 21 Lessons Learned](/deep-learning-specialization-by-andrew-ng-21-lessons-learned-15ffaaef627c)'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '[å®‰å¾·é²Â·å´çš„æ·±åº¦å­¦ä¹ ä¸“ä¸šåŒ– â€” 21ä¸ªç»éªŒæ•™è®­](/deep-learning-specialization-by-andrew-ng-21-lessons-learned-15ffaaef627c)'
