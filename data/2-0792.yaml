- en: Efficiently Serving Open Source LLMs
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 高效服务开源 LLM
- en: 原文：[https://towardsdatascience.com/efficiently-serving-open-source-llms-5f0bf5d8fd59](https://towardsdatascience.com/efficiently-serving-open-source-llms-5f0bf5d8fd59)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/efficiently-serving-open-source-llms-5f0bf5d8fd59](https://towardsdatascience.com/efficiently-serving-open-source-llms-5f0bf5d8fd59)
- en: '[](https://medium.com/@ryanshrott?source=post_page-----5f0bf5d8fd59--------------------------------)[![Ryan
    Shrott](../Images/186524066383b4b02c994692aebb3ea5.png)](https://medium.com/@ryanshrott?source=post_page-----5f0bf5d8fd59--------------------------------)[](https://towardsdatascience.com/?source=post_page-----5f0bf5d8fd59--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----5f0bf5d8fd59--------------------------------)
    [Ryan Shrott](https://medium.com/@ryanshrott?source=post_page-----5f0bf5d8fd59--------------------------------)'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@ryanshrott?source=post_page-----5f0bf5d8fd59--------------------------------)[![Ryan
    Shrott](../Images/186524066383b4b02c994692aebb3ea5.png)](https://medium.com/@ryanshrott?source=post_page-----5f0bf5d8fd59--------------------------------)[](https://towardsdatascience.com/?source=post_page-----5f0bf5d8fd59--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----5f0bf5d8fd59--------------------------------)
    [Ryan Shrott](https://medium.com/@ryanshrott?source=post_page-----5f0bf5d8fd59--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----5f0bf5d8fd59--------------------------------)
    ·5 min read·Aug 14, 2023
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----5f0bf5d8fd59--------------------------------)
    ·阅读时长 5 分钟·2023 年 8 月 14 日
- en: --
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/c16d4589aafa99416a26da8bff1b5afe.png)'
  id: totrans-5
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c16d4589aafa99416a26da8bff1b5afe.png)'
- en: Photo by [Mariia Shalabaieva](https://unsplash.com/@maria_shalabaieva?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：[Mariia Shalabaieva](https://unsplash.com/@maria_shalabaieva?utm_source=medium&utm_medium=referral)
    于 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: 'This article explains my personal experiences using 6 common methods for serving
    open source LLMs: AWS Sage Maker, Hugging Face, Together.AI, VLLM and Petals.ml.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 本文解释了我个人使用 6 种常见方法服务开源 LLM 的经验：AWS Sage Maker、Hugging Face、Together.AI、VLLM
    和 Petals.ml。
- en: The struggle…
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 挣扎…
- en: 'You’ve felt the pain, struggle and glory of serving your own fine-tuned open
    source LLM, however, you ultimately decided to return to Open AI or Anthropic
    due to cost, inference time, reliability and technology challenges :( You’ve also
    given up on renting a A100 GPU (many providers have GPUs fully booked until the
    end of 2023!). And you don’t have 100K to shell out for a 2 tier A100 server box.
    Still, you’re dreaming, and you really want to get open source to work for your
    solution. Perhaps your firm does not want to send it’s private data to Open AI
    or you want a fine tuned model for a very specific task? In this article, I will
    outline and compare some of the most effective inference methods/platforms for
    serving open source LLMs in 2023\. I will compare and contrast 6 methods and explain
    when you should use one or the other. I have personally tried all 6 of these and
    will detail my personal experience with these solutions: **AWS Sage Maker, Hugging
    Face Inference endpoints, Together.AI, VLLM and Petals.ml**. I don’t have all
    the answers, but I will do my best to detail my experiences. I have no monetary
    connection with any of these providers and am simply sharing my experiences for
    the benefit of others. Please tell about your experiences!'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 你已经感受到了服务自己微调的开源 LLM 的痛苦、挣扎和荣耀，但你最终因为成本、推理时间、可靠性和技术挑战而决定回到 Open AI 或 Anthropic
    :( 你也放弃了租用 A100 GPU（许多供应商的 GPU 已经被预订到 2023 年底！）。你也没有 10 万美元去购买一个 2 级 A100 服务器。尽管如此，你仍在梦想，你真的希望开源能够为你的解决方案服务。也许你的公司不愿意将私人数据发送给
    Open AI，或者你需要一个针对特定任务微调的模型？在本文中，我将概述并比较一些最有效的推理方法/平台，用于服务开源 LLM。在 2023 年，我将对 6
    种方法进行比较，并解释何时应该使用其中一种或另一种。我亲自尝试了这 6 种方法，并将详细介绍我的个人经验：**AWS Sage Maker、Hugging
    Face 推理端点、Together.AI、VLLM 和 Petals.ml**。我没有所有的答案，但我会尽力详细说明我的经验。我与这些供应商没有任何经济联系，仅仅是分享我的经验以造福他人。请分享你的经验！
- en: Why open source?
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 为什么选择开源？
- en: Open source models have a plethora of advantages including control, privacy
    and potential cost reductions. For example, you could fine tune a smaller open
    source model for your particular use case, resulting in accurate results and fast
    inference time. Privacy control means that inference can be done on your own servers.
    On the other hand, cost reduction is much harder than you might think. Open AI
    has economies of scale and has competitive pricing. Their pricing model for GPT-3.5
    turbo is very hard to compete with, and has been shown to be similar to the cost
    of electricity. Still, there are methods and techniques you can use to save money
    and get excellent results with open source models. For example, my fine tuned
    model of [Stable Beluga 2](https://huggingface.co/stabilityai/StableBeluga2) is
    currently outperforming GPT-3.5 Turbo significantly, and is cheaper for my application.
    So I strongly suggest giving open source a shot for your application.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 开源模型有许多优点，包括控制、隐私和潜在的成本降低。例如，你可以针对特定的使用案例微调一个较小的开源模型，从而获得准确的结果和快速的推理时间。隐私控制意味着推理可以在自己的服务器上完成。另一方面，成本降低比你想象的要困难得多。OpenAI
    拥有规模经济，定价具有竞争力。他们的 GPT-3.5 Turbo 定价模式很难与之竞争，并且已被证明类似于电力成本。不过，你仍然可以采用一些方法和技巧来节省开支，并用开源模型获得优秀的结果。例如，我的微调模型
    [Stable Beluga 2](https://huggingface.co/stabilityai/StableBeluga2) 目前显著优于 GPT-3.5
    Turbo，并且在我的应用中更便宜。因此，我强烈建议你尝试使用开源模型。
- en: Hugging Face Inference Endpoints
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Hugging Face 推理端点
- en: This is the most common and simplest method for serving an open source LLM.
    It only takes a couple clicks and is foolproof. After all, Hugging Face was originally
    an NLP company. Your model probably already exists on hugging face as well, so
    this should be the go-to option for quickly testing your model. The GPU server
    costs due tend to run on the higher side. For example, if you simply used RunPod.io
    to deploy your model, you will have more options for providers and lower costs.
    Hugging Face has open sourced their transformers inference library and have provided
    a docker image that is easy to modify. So if you want more control go with a custom
    solution on RunPod. [Here is a](https://www.youtube.com/watch?v=FdcXJ7d3WQU&t=224s&ab_channel=VenelinValkov)
    tutorial on how to do it on RunPod.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 这是服务开源 LLM 最常见且最简单的方法。只需点击几下即可，且几乎没有错误。毕竟，Hugging Face 最初是一家 NLP 公司。你的模型很可能已经存在于
    Hugging Face 上，因此这是快速测试模型的首选选项。GPU 服务器成本往往较高。例如，如果你仅使用 RunPod.io 部署模型，你将有更多的提供商选择，并且成本更低。Hugging
    Face 已开源了他们的 Transformers 推理库，并提供了易于修改的 Docker 镜像。因此，如果你需要更多控制，可以选择在 RunPod 上的自定义解决方案。
    [这是一个](https://www.youtube.com/watch?v=FdcXJ7d3WQU&t=224s&ab_channel=VenelinValkov)关于如何在
    RunPod 上操作的教程。
- en: VLLM
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: VLLM
- en: '[This solution](https://github.com/vllm-project/vllm) is very interesting due
    to the inference speed. They claim to be 24 times faster than hugging face transformers!
    Using it personally, I found the speed to be about 10 times faster than Hugging
    Face transformers. I found there to be a few bugs here and there though. This
    project is being actively worked on, and is not foolproof yet. Still, I strongly
    suggest you try it for your application. Due to the faster inference speed, the
    cost will be significantly lower in comparison to HF transformers.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '[这个解决方案](https://github.com/vllm-project/vllm)由于其推理速度而非常有趣。他们声称比 Hugging Face
    的 Transformers 快 24 倍！在我个人使用时，发现速度大约是 Hugging Face Transformers 的 10 倍。不过，我发现这里有一些小
    bug。这个项目正在积极开发中，尚未完善。不过，我仍然强烈建议你试试这个解决方案。由于推理速度更快，相较于 HF Transformers，成本将显著降低。'
- en: '![](../Images/f05be35a29639954eae27b280d6dc2af.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f05be35a29639954eae27b280d6dc2af.png)'
- en: 'Source: [https://github.com/vllm-project/vllm](https://github.com/vllm-project/vllm)'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '来源: [https://github.com/vllm-project/vllm](https://github.com/vllm-project/vllm)'
- en: Petals.ml
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Petals.ml
- en: '[This](https://github.com/bigscience-workshop/petals) is the most intereresting
    solution. The developers at Petals.ml have discovered a way to run LLMs at home,
    BitTorrent-style. This allows you to do fine-tuning and inference up to 10 times
    faster than offloading. In practice, this means that only a small part of the
    model will be loaded on your own GPU, and the rest will exist on a GPU network
    swarm. In other words, a network of GPUs will work together in order to do the
    compute. This is very interesting because it democratizes LLM usage to some extent,
    i.e. anyone has the ability to run huge LLMs without paying a cent! The paper
    of the technology used can be [found here](https://arxiv.org/abs/2209.01188).
    I strongly suggest you give Petals.ml a try!'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '[这个](https://github.com/bigscience-workshop/petals)是最有趣的解决方案。Petals.ml的开发者发现了一种在家运行LLMs的方法，类似于BitTorrent。这使得微调和推理的速度比卸载快最多10倍。实际上，这意味着模型的只有一小部分会加载到你自己的GPU上，其余部分会存在于GPU网络群中。换句话说，一个GPU网络将协作进行计算。这非常有趣，因为它在一定程度上使LLM的使用得到民主化，即任何人都可以运行大型LLM而不花一分钱！相关技术的论文可以[在这里找到](https://arxiv.org/abs/2209.01188)。我强烈建议你试试Petals.ml！'
- en: Together.AI
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Together.AI
- en: '[They](https://together.ai/) provide an API for open source models with excellent
    pricing. You can fine tune and deploy open source models using Together.AI compute
    cluster. Their pricing is 20% of AWS. Their platform is straightforward and easy
    to get started. Therefore, I would highly suggest this platform. Their API is
    about [1/10th the price](https://together.ai/pricing) of GPT-3.5 turbo. This is
    my new favorite way to deploy open source models!'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '[他们](https://together.ai/)提供了一个具有出色定价的开源模型API。你可以使用Together.AI计算集群对开源模型进行微调和部署。他们的定价是AWS的20%。他们的平台简单直观，容易上手。因此，我强烈推荐这个平台。他们的API价格大约是[GPT-3.5
    turbo的1/10](https://together.ai/pricing)。这是我现在最喜欢的开源模型部署方式！'
- en: AWS Sagemaker
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: AWS Sagemaker
- en: 'The tried and true method for deploying ML models. Sagemaker is not particular
    beginner friendly and is probably the most expensive method in comparison to the
    methods provided above. It’s also the most complex. However, if your business
    is already using AWS, this may be your only option. Also, if you have free compute
    on AWS like me, why not give it a shot? Here is a tutorial on how to do it by
    AI Anytime: [https://www.youtube.com/watch?v=A9Pu4xg-Nas&ab_channel=AIAnytime](https://www.youtube.com/watch?v=A9Pu4xg-Nas&ab_channel=AIAnytime).'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 部署ML模型的成熟方法。Sagemaker对初学者不太友好，而且与上述方法相比，它可能是最昂贵的。它也是最复杂的。然而，如果你的业务已经在使用AWS，这可能是你唯一的选择。此外，如果你像我一样在AWS上有免费的计算资源，为什么不尝试一下呢？这是AI
    Anytime的教程：[https://www.youtube.com/watch?v=A9Pu4xg-Nas&ab_channel=AIAnytime](https://www.youtube.com/watch?v=A9Pu4xg-Nas&ab_channel=AIAnytime)。
- en: 'Conclusion:'
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结论：
- en: In conclusion, I highly suggest trying Together.AI and Petals.ml due to the
    many upsides in using these platforms. If you require privacy and very fast inference
    speeds, I suggest using VLLM. If you are forced to use AWS, then go with SageMaker.
    Finally, if you want something simple and efficient (especially for testing),
    go with HF transformers endpoints.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，我强烈建议尝试Together.AI和Petals.ml，因为使用这些平台有许多优势。如果你需要隐私和非常快的推理速度，我建议使用VLLM。如果你被迫使用AWS，那么选择SageMaker。如果你想要简单高效的方案（特别是用于测试），可以选择HF
    transformers端点。
- en: '📢 Hey there! If you found this article helpful, please consider:'
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 📢 嗨！如果你觉得这篇文章有帮助，请考虑：
- en: 👏 Clapping 50 times (this helps a lot!)
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 👏 鼓掌50次（这很有帮助！）
- en: ✏️ Leaving a comment
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: ✏️ 留下评论
- en: 🌟 Highlighting parts you found insightful
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 🌟 突出你觉得有见地的部分
- en: 👣 Following me
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 👣 关注我
- en: Any questions? 🤔 Don’t hesitate to ask. Supporting me this way is a free and
    easy way to show appreciation for my detailed tutorial articles! 😊
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 有任何问题吗？🤔 不要犹豫，尽管问。以这种方式支持我是对我的详细教程文章的一种免费而简单的感谢方式！😊
- en: Final Notes
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 最终说明
- en: If you’re interested in learning more about developing full stack AI apps only
    with Python, please feel free to [sign up here](https://saas.mixo.io/). As always,
    please leave your own experiences and comments below. I look forward to reading
    them all.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你对仅使用Python开发全栈AI应用感兴趣，请随时[注册这里](https://saas.mixo.io/)。一如既往，请在下面留下你的经验和评论。我期待阅读所有评论。
- en: That’s all folks — if you’ve made it this far, please comment below and add
    me on [LinkedIn](https://www.linkedin.com/in/ryanshrott/).
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 就这些了，如果你读到这里，请在下面评论并在[LinkedIn](https://www.linkedin.com/in/ryanshrott/)上添加我。
- en: My Github is [here](https://github.com/ryanshrott).
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 我的Github在[这里](https://github.com/ryanshrott)。
- en: '**Other Deep Learning Blogs**'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '**其他深度学习博客**'
- en: '[Sequence Models by Andrew Ng — 11 Lessons Learned](https://medium.com/towards-data-science/sequence-models-by-andrew-ng-11-lessons-learned-c62fb1d3485b)'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '[安德鲁·吴的序列模型 — 11个经验教训](https://medium.com/towards-data-science/sequence-models-by-andrew-ng-11-lessons-learned-c62fb1d3485b)'
- en: '[Computer Vision by Andrew Ng — 11 Lessons Learned](/computer-vision-by-andrew-ng-11-lessons-learned-7d05c18a6999)'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '[安德鲁·吴的计算机视觉 — 11个经验教训](/computer-vision-by-andrew-ng-11-lessons-learned-7d05c18a6999)'
- en: '[Deep Learning Specialization by Andrew Ng — 21 Lessons Learned](/deep-learning-specialization-by-andrew-ng-21-lessons-learned-15ffaaef627c)'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '[安德鲁·吴的深度学习专业化 — 21个经验教训](/deep-learning-specialization-by-andrew-ng-21-lessons-learned-15ffaaef627c)'
