- en: Solving Unity Environment with Deep Reinforcement Learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用深度强化学习解决 Unity 环境
- en: 原文：[https://towardsdatascience.com/solving-unity-environment-with-deep-reinforcement-learning-836dc181ee3b?source=collection_archive---------10-----------------------#2023-02-20](https://towardsdatascience.com/solving-unity-environment-with-deep-reinforcement-learning-836dc181ee3b?source=collection_archive---------10-----------------------#2023-02-20)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/solving-unity-environment-with-deep-reinforcement-learning-836dc181ee3b?source=collection_archive---------10-----------------------#2023-02-20](https://towardsdatascience.com/solving-unity-environment-with-deep-reinforcement-learning-836dc181ee3b?source=collection_archive---------10-----------------------#2023-02-20)
- en: End to End Project with code of a PyTorch implementation of Deep Reinforcement
    Learning Agent
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 深度强化学习代理的 PyTorch 实现的端到端项目
- en: '[](https://gabrielcassimiro17.medium.com/?source=post_page-----836dc181ee3b--------------------------------)[![Gabriel
    Cassimiro](../Images/2cf8a09a706236059c46c7f0f20d4365.png)](https://gabrielcassimiro17.medium.com/?source=post_page-----836dc181ee3b--------------------------------)[](https://towardsdatascience.com/?source=post_page-----836dc181ee3b--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----836dc181ee3b--------------------------------)
    [Gabriel Cassimiro](https://gabrielcassimiro17.medium.com/?source=post_page-----836dc181ee3b--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://gabrielcassimiro17.medium.com/?source=post_page-----836dc181ee3b--------------------------------)[![Gabriel
    Cassimiro](../Images/2cf8a09a706236059c46c7f0f20d4365.png)](https://gabrielcassimiro17.medium.com/?source=post_page-----836dc181ee3b--------------------------------)[](https://towardsdatascience.com/?source=post_page-----836dc181ee3b--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----836dc181ee3b--------------------------------)
    [Gabriel Cassimiro](https://gabrielcassimiro17.medium.com/?source=post_page-----836dc181ee3b--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F3692fb93d7e5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsolving-unity-environment-with-deep-reinforcement-learning-836dc181ee3b&user=Gabriel+Cassimiro&userId=3692fb93d7e5&source=post_page-3692fb93d7e5----836dc181ee3b---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----836dc181ee3b--------------------------------)
    ·6 min read·Feb 20, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F836dc181ee3b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsolving-unity-environment-with-deep-reinforcement-learning-836dc181ee3b&user=Gabriel+Cassimiro&userId=3692fb93d7e5&source=-----836dc181ee3b---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[查看](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F3692fb93d7e5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsolving-unity-environment-with-deep-reinforcement-learning-836dc181ee3b&user=Gabriel+Cassimiro&userId=3692fb93d7e5&source=post_page-3692fb93d7e5----836dc181ee3b---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----836dc181ee3b--------------------------------)
    ·6 min read·2023年2月20日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F836dc181ee3b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsolving-unity-environment-with-deep-reinforcement-learning-836dc181ee3b&user=Gabriel+Cassimiro&userId=3692fb93d7e5&source=-----836dc181ee3b---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F836dc181ee3b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsolving-unity-environment-with-deep-reinforcement-learning-836dc181ee3b&source=-----836dc181ee3b---------------------bookmark_footer-----------)![](../Images/407363663dbe1f188dd99ddb58cb70e2.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F836dc181ee3b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsolving-unity-environment-with-deep-reinforcement-learning-836dc181ee3b&source=-----836dc181ee3b---------------------bookmark_footer-----------)![](../Images/407363663dbe1f188dd99ddb58cb70e2.png)'
- en: Image by [Arseny Togulev](https://unsplash.com/@tetrakiss?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
    on [Unsplash](https://unsplash.com/pt-br/s/fotografias/robot?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由 [Arseny Togulev](https://unsplash.com/@tetrakiss?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
    提供，来源于 [Unsplash](https://unsplash.com/pt-br/s/fotografias/robot?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
- en: Unity is a popular game development engine that allows developers to create
    games with stunning graphics and immersive gameplay. It is widely used for developing
    games across various platforms, including mobile, PC, and consoles. However, creating
    intelligent and challenging game environments is a challenging task for game developers.
    This is where Deep Reinforcement Learning (DRL) comes into play.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: Unity 是一个流行的游戏开发引擎，允许开发者创建具有惊人图形和沉浸式游戏玩法的游戏。它被广泛用于开发各种平台上的游戏，包括移动设备、PC 和控制台。然而，创建智能且具有挑战性的游戏环境对游戏开发者来说是一项艰巨的任务。这就是深度强化学习（DRL）的作用所在。
- en: DRL is a subset of machine learning that combines deep learning and reinforcement
    learning. It is a powerful technique that has been used to solve complex tasks
    in various domains, including robotics, finance, and gaming. In recent years,
    DRL has become a popular approach to building intelligent game agents that can
    learn from experience and adapt to new situations.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: DRL 是一种机器学习的子集，结合了深度学习和强化学习。它是一种强大的技术，已被用于解决各种领域中的复杂任务，包括机器人技术、金融和游戏。近年来，DRL
    已成为构建智能游戏代理的流行方法，这些代理可以从经验中学习并适应新情况。
- en: In this post, we will explore how DRL can be used to solve Unity game environments.
    We will go through an implementation of DRL in the Unity environment to collect
    Bananas. We will also explore some of the challenges associated with using DRL
    in game development and how these challenges can be overcome.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我们将探讨如何使用 DRL 解决 Unity 游戏环境中的问题。我们将通过在 Unity 环境中实现 DRL 来收集香蕉。我们还将探讨使用
    DRL 进行游戏开发的一些挑战，以及如何克服这些挑战。
- en: This was a project for the Deep Reinforcement Learning specialization from Udacity.
    The full project and code can be found on this [Github repo](https://github.com/gabrielcassimiro17/rl-dqn-collect-bananas).
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 这是 Udacity 深度强化学习专业课程中的一个项目。完整项目和代码可以在这个[Github 仓库](https://github.com/gabrielcassimiro17/rl-dqn-collect-bananas)中找到。
- en: Objective
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 目标
- en: This project has the objective to train an Agent using Deep Q Learning. The
    agent will be trained to collect yellow bananas while avoiding blue bananas from
    Unity’s Banana Collector environment.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 该项目的目标是使用深度 Q 学习训练一个代理。代理将被训练以收集黄色香蕉，同时避免 Unity 的香蕉收集器环境中的蓝色香蕉。
- en: More information about the Unix environment can be found [here](https://github.com/Unity-Technologies/ml-agents/blob/main/docs/Learning-Environment-Examples.md#banana-collector).
    The agent was trained using a Deep Q Learning algorithm and was able to solve
    the environment in 775 episodes.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 有关 Unix 环境的更多信息可以在[这里](https://github.com/Unity-Technologies/ml-agents/blob/main/docs/Learning-Environment-Examples.md#banana-collector)找到。代理使用深度
    Q 学习算法进行了训练，并在 775 个回合中成功解决了环境。
- en: Enviroment & Task
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 环境与任务
- en: 'The environment consists in a square world with yellow and blue bananas. The
    agent has the objective to collect as many yellow bananas as possible while avoiding
    the blue ones. The agent has 4 possible actions: move forward, move backward,
    turn left and turn right.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 该环境由一个方形世界组成，其中有黄色和蓝色香蕉。代理的目标是尽可能多地收集黄色香蕉，同时避免蓝色香蕉。代理有 4 个可能的动作：向前移动、向后移动、左转和右转。
- en: The state space has 37 dimensions and contains the agent’s velocity, along with
    ray-based perception of objects around the agent’s forward direction. A reward
    of +1 is provided for collecting a yellow banana, and a reward of -1 is provided
    for collecting a blue banana.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 状态空间具有 37 维，包括代理的速度以及基于射线的周围对象感知。收集一个黄色香蕉会获得 +1 奖励，收集一个蓝色香蕉会获得 -1 奖励。
- en: The task is episodic, and in order to solve the environment, the agent must
    get an average score of +13 over 100 consecutive episodes.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 任务是阶段性的，为了解决环境，代理必须在 100 个连续的回合中获得平均 +13 的分数。
- en: 'The env looks like this:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 环境如下所示：
- en: '![](../Images/03b8b6f99cd8f3437e3a8c7c5fc4a448.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/03b8b6f99cd8f3437e3a8c7c5fc4a448.png)'
- en: Image by author
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片
- en: '**The Agent**'
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**代理**'
- en: To solve the problem given by the environment it was implemented a Deep Q Learning
    algorithm. The algorithm is based on the paper [Human-level control through deep
    reinforcement learning](https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf)
    by DeepMind.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决环境所给出的问题，实现了一种深度 Q 学习算法。该算法基于 DeepMind 的论文[通过深度强化学习实现人类水平的控制](https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf)。
- en: 'The algorithm works by using a neural network to approximate the Q-Function.
    The neural network receives the state as input and outputs the Q-Value for each
    action. Then it uses the Q-Value to select the best action to be taken by the
    agent. The algorithm learns by using the Q-Learning algorithm to train the neural
    network. There are also two problems with a simple implementation of the algorithm:
    correlated experiences and correlated targets. The algorithm uses two techniques
    to solve these problems: Experience Replay and Fixed Q-Targets.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 该算法通过使用神经网络来近似 Q-函数。神经网络接收状态作为输入，并为每个动作输出 Q-值。然后它使用 Q-值来选择代理需要采取的最佳动作。该算法通过使用
    Q-Learning 算法来训练神经网络。简单实现算法时还存在两个问题：相关经验和相关目标。该算法使用了两种技术来解决这些问题：经验回放和固定 Q-目标。
- en: '**Correlated experiences**'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '**相关经验**'
- en: Correlated experiences refer to a situation where the experiences (or transitions)
    of an agent are correlated with each other, meaning they are not independent and
    identically distributed. This can lead to an overestimation of the expected reward
    of a particular state or action, resulting in poor performance or convergence
    to suboptimal policies.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 相关经验是指代理的经验（或转移）彼此之间相关，即它们不是独立同分布的。这可能导致对特定状态或动作的期望奖励的高估，从而导致性能差或收敛到次优策略。
- en: To solve this problem it is used a technique called **Experience Replay**. The
    technique consists in storing the experiences of the agent in a replay buffer
    and sampling randomly from it to train the neural network.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这个问题，使用了一种叫做 **经验回放** 的技术。该技术包括将代理的经验存储在回放缓冲区中，并从中随机抽样以训练神经网络。
- en: '**Correlated targets**'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '**相关目标**'
- en: Correlated targets refer to a situation where the target values used to update
    the policy are not independent of each other, leading to correlation in the learning
    signal. This can slow down or prevent convergence to the optimal policy.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 相关目标是指用于更新策略的目标值彼此之间不是独立的，导致学习信号的相关性。这可能会减慢或阻止收敛到最佳策略。
- en: 'To solve this problem it is used a technique called **Fixed Q-Targets**. The
    technique consists in using two neural networks: the local network and the target
    network. The local network is used to select the best action to be taken by the
    agent while the target network is used to calculate the target value for the Q-Learning
    algorithm. The target network is updated every 4 steps with the weights of the
    local network.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这个问题，使用了一种叫做 **固定 Q-目标** 的技术。该技术包括使用两个神经网络：本地网络和目标网络。本地网络用于选择代理需要采取的最佳动作，而目标网络用于计算
    Q-Learning 算法的目标值。目标网络每 4 步更新一次，与本地网络的权重同步。
- en: 'This is the implementation in Python:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 这是 Python 中的实现：
- en: '**Neural network architecture**'
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**神经网络架构**'
- en: The neural network architecture used in the algorithm is a simple fully connected
    neural network with 2 hidden layers. The input layer has 37 neurons, the output
    layer has 4 neurons and the hidden layers have 64 neurons each. The activation
    function used in the hidden layers is ReLU and the activation function used in
    the output layer is the identity function.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 算法中使用的神经网络架构是一个简单的全连接神经网络，具有 2 层隐藏层。输入层有 37 个神经元，输出层有 4 个神经元，隐藏层每层有 64 个神经元。隐藏层使用的激活函数是
    ReLU，输出层使用的激活函数是恒等函数。
- en: The optimizer used for this implementation is Adam with a learning rate of 0.0005.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 此实现中使用的优化器是 Adam，学习率为 0.0005。
- en: The library used to implement the neural network was PyTorch.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 实现神经网络所使用的库是 PyTorch。
- en: 'This was the implementation of the neural network:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 这是神经网络的实现：
- en: Training Task
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练任务
- en: To train the agent we used a loop to interact with the environment, collect
    and learn from the experiences. One of the hyperparameters used in the training
    task was the number of episodes. This first hyperparameter was tuned manually
    trying to optimize the training time and the performance of the agent. The number
    of episodes used in the final implementation was 1200 however the env was solved
    in 775.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 为了训练代理，我们使用了一个循环与环境交互，收集并从经验中学习。训练任务中使用的超参数之一是回合数。第一个超参数是手动调整的，目的是优化训练时间和代理的性能。最终实现中使用的回合数为
    1200，但环境在 775 回合中被解决。
- en: The second hyperparameter used in the training task was the number of steps
    per episode. This hyperparameter was also tuned manually trying to optimize the
    training time and the performance of the agent. The bigger the number of steps
    the more the agent can explore the environment but it increases a lot the training
    time. The number of steps per episode used in the final implementation was 1000.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 训练任务中使用的第二个超参数是每集的步数。这个超参数也通过手动调整来优化训练时间和智能体的性能。步数越多，智能体可以越多地探索环境，但训练时间也会大幅增加。最终实现中每集的步数是1000。
- en: 'Some other hyperparameters used:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 其他一些使用的超参数：
- en: 'Replay buffer size: 1000'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 回放缓冲区大小：1000
- en: 'Batch size: 32'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 批量大小：32
- en: 'Update every: 4'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更新频率：4
- en: 'Gamma: 0.99'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gamma：0.99
- en: 'Tau: 1e-3'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tau：1e-3
- en: 'Learning rate: 0.0005'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习率：0.0005
- en: With this, we were able to solve the environment in 775 episodes. The plot below
    shows the progress of the agent in obtaining higher rewards.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这些措施，我们能够在775集内解决环境问题。下面的图表显示了智能体在获得更高奖励方面的进展。
- en: '![](../Images/0c871cf37146a7d3bd8546b3624108df.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0c871cf37146a7d3bd8546b3624108df.png)'
- en: Image by Author
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片
- en: Here we can see the rewards increase as the agent improves. The tradeoff between
    **exploration and exploitation** is also visible in the plot, where the agent
    explores more in the first 200 episodes and then starts to exploit the environment
    and get higher rewards.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们可以看到随着智能体的改进，奖励增加。在图中也可以看到**探索与开发**之间的权衡，智能体在前200集时更多地进行探索，然后开始开发环境并获得更高的奖励。
- en: The full implementation can be found on this [GitHub repo](https://github.com/gabrielcassimiro17/rl-dqn-collect-bananas).
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 完整的实现可以在这个[GitHub仓库](https://github.com/gabrielcassimiro17/rl-dqn-collect-bananas)中找到。
- en: While we were able to solve the environment, there are a few improvements that
    can be applied to speed up the solution.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们能够解决环境问题，但仍有一些改进可以应用，以加快解决速度。
- en: '**Future improvements**'
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**未来改进**'
- en: 'The algorithm can be improved by using the following techniques:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 算法可以通过以下技术进行改进：
- en: Dueling DQN — [paper](https://arxiv.org/pdf/1511.06581.pdf)
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对抗DQN — [论文](https://arxiv.org/pdf/1511.06581.pdf)
- en: Prioritized Experience Replay — [paper](https://arxiv.org/pdf/1511.05952.pdf)
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 优先经验回放 — [论文](https://arxiv.org/pdf/1511.05952.pdf)
- en: Another possible improvement is to work with pixel data from the environment.
    These improvements will likely be a topic for a new article, and I intend on going
    deeper into the core concepts and implementing the NN with TensorFlow.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个可能的改进是使用环境中的像素数据。这些改进可能会成为新文章的主题，我打算深入探讨核心概念并用TensorFlow实现神经网络。
- en: Thanks for Reading!
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 感谢阅读！
- en: 'Here are a few other articles you might be interested:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一些你可能感兴趣的其他文章：
- en: '[](/object-detection-with-tensorflow-model-and-opencv-d839f3e42849?source=post_page-----836dc181ee3b--------------------------------)
    [## Object detection with Tensorflow model and OpenCV'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '[## 对象检测与Tensorflow模型和OpenCV](https://towardsdatascience.com/object-detection-with-tensorflow-model-and-opencv-d839f3e42849?source=post_page-----836dc181ee3b--------------------------------)'
- en: Using a trained model to identify objects on static images and live video
  id: totrans-62
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用训练好的模型识别静态图像和实时视频中的对象
- en: towardsdatascience.com](/object-detection-with-tensorflow-model-and-opencv-d839f3e42849?source=post_page-----836dc181ee3b--------------------------------)
    [](/how-to-prepare-for-the-gcp-professional-machine-learning-engineer-exam-b1c59967355f?source=post_page-----836dc181ee3b--------------------------------)
    [## How to prepare for the GCP Professional Machine Learning Engineer exam
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '[## 如何为GCP专业机器学习工程师考试做好准备](https://towardsdatascience.com/how-to-prepare-for-the-gcp-professional-machine-learning-engineer-exam-b1c59967355f?source=post_page-----836dc181ee3b--------------------------------)
    [## 对象检测与Tensorflow模型和OpenCV](https://towardsdatascience.com/object-detection-with-tensorflow-model-and-opencv-d839f3e42849?source=post_page-----836dc181ee3b--------------------------------)'
- en: Courses review, study tips, and how I did it
  id: totrans-64
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 课程评审、学习技巧以及我的实践经历
- en: towardsdatascience.com](/how-to-prepare-for-the-gcp-professional-machine-learning-engineer-exam-b1c59967355f?source=post_page-----836dc181ee3b--------------------------------)
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '[## 如何为GCP专业机器学习工程师考试做好准备](https://towardsdatascience.com/how-to-prepare-for-the-gcp-professional-machine-learning-engineer-exam-b1c59967355f?source=post_page-----836dc181ee3b--------------------------------)'
