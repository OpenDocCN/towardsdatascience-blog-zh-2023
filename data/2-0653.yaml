- en: Data Pipeline Orchestration
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ•°æ®ç®¡é“ç¼–æ’
- en: åŸæ–‡ï¼š[https://towardsdatascience.com/data-pipeline-orchestration-9887e1b5eb7a](https://towardsdatascience.com/data-pipeline-orchestration-9887e1b5eb7a)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡ï¼š[https://towardsdatascience.com/data-pipeline-orchestration-9887e1b5eb7a](https://towardsdatascience.com/data-pipeline-orchestration-9887e1b5eb7a)
- en: Data pipeline management done right simplifies deployment and increases the
    availability and accessibility of data for analytics
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ­£ç¡®çš„æ•°æ®ç®¡é“ç®¡ç†ç®€åŒ–äº†éƒ¨ç½²ï¼Œå¢åŠ äº†æ•°æ®åˆ†æçš„å¯ç”¨æ€§å’Œå¯åŠæ€§ã€‚
- en: '[](https://mshakhomirov.medium.com/?source=post_page-----9887e1b5eb7a--------------------------------)[![ğŸ’¡Mike
    Shakhomirov](../Images/bc6895c7face3244d488feb97ba0f68e.png)](https://mshakhomirov.medium.com/?source=post_page-----9887e1b5eb7a--------------------------------)[](https://towardsdatascience.com/?source=post_page-----9887e1b5eb7a--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----9887e1b5eb7a--------------------------------)
    [ğŸ’¡Mike Shakhomirov](https://mshakhomirov.medium.com/?source=post_page-----9887e1b5eb7a--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://mshakhomirov.medium.com/?source=post_page-----9887e1b5eb7a--------------------------------)[![ğŸ’¡Mike
    Shakhomirov](../Images/bc6895c7face3244d488feb97ba0f68e.png)](https://mshakhomirov.medium.com/?source=post_page-----9887e1b5eb7a--------------------------------)[](https://towardsdatascience.com/?source=post_page-----9887e1b5eb7a--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----9887e1b5eb7a--------------------------------)
    [ğŸ’¡Mike Shakhomirov](https://mshakhomirov.medium.com/?source=post_page-----9887e1b5eb7a--------------------------------)'
- en: Â·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----9887e1b5eb7a--------------------------------)
    Â·7 min readÂ·Apr 3, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: Â·å‘å¸ƒäº [Towards Data Science](https://towardsdatascience.com/?source=post_page-----9887e1b5eb7a--------------------------------)
    Â·7åˆ†é’Ÿé˜…è¯»Â·2023å¹´4æœˆ3æ—¥
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/6ab99cb7a0920a4d1f69a66bee5f00b7.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6ab99cb7a0920a4d1f69a66bee5f00b7.png)'
- en: Photo by [Manuel NÃ¤geli](https://unsplash.com/@gwundrig?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ç‰‡ç”± [Manuel NÃ¤geli](https://unsplash.com/@gwundrig?utm_source=medium&utm_medium=referral)
    æä¾›ï¼Œæ¥æºäº [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: What is data orchestration?
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä»€ä¹ˆæ˜¯æ•°æ®ç¼–æ’ï¼Ÿ
- en: DataOps teams use **Data Pipeline Orchestration** as a solution to centralize
    administration and oversight of end-to-end data pipelines.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: DataOpså›¢é˜Ÿä½¿ç”¨**æ•°æ®ç®¡é“ç¼–æ’**ä½œä¸ºé›†ä¸­ç®¡ç†å’Œç›‘ç£ç«¯åˆ°ç«¯æ•°æ®ç®¡é“çš„è§£å†³æ–¹æ¡ˆã€‚
- en: The process of automating the data pipeline is known as data orchestration.
  id: totrans-10
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: è‡ªåŠ¨åŒ–æ•°æ®ç®¡é“çš„è¿‡ç¨‹è¢«ç§°ä¸ºæ•°æ®ç¼–æ’ã€‚
- en: It is important to manage data pipelines right as it affects almost everything,
    i.e. data quality, process speed and data governance.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: æ­£ç¡®ç®¡ç†æ•°æ®ç®¡é“å¾ˆé‡è¦ï¼Œå› ä¸ºå®ƒå‡ ä¹å½±å“ä¸€åˆ‡ï¼Œå³æ•°æ®è´¨é‡ã€è¿‡ç¨‹é€Ÿåº¦å’Œæ•°æ®æ²»ç†ã€‚
- en: What makes an effective data pipeline management?
  id: totrans-12
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ä»€ä¹ˆæ˜¯æœ‰æ•ˆçš„æ•°æ®ç®¡é“ç®¡ç†ï¼Ÿ
- en: '- **Transparency and visibility.** It is important that everyone in the team
    know how exactly the data is being transformed, where it comes from and where
    the process of data transformation ends.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '- **é€æ˜æ€§å’Œå¯è§æ€§ã€‚** è®©å›¢é˜Ÿä¸­çš„æ¯ä¸ªäººéƒ½çŸ¥é“æ•°æ®æ˜¯å¦‚ä½•è½¬æ¢çš„ï¼Œå®ƒæ¥è‡ªå“ªé‡Œï¼Œä»¥åŠæ•°æ®è½¬æ¢è¿‡ç¨‹çš„ç»“æŸåœ°ç‚¹æ˜¯å¾ˆé‡è¦çš„ã€‚'
- en: '- **Faster deployments.** It is crucial to be able to reproduce the elements
    of the data pipeline continuously. Consider these elements as building blocks
    of the data pipeline. So, when there is a requirement to create a new data pipeline,
    it is important to replicate building blocks with ease for each new data process
    instead of creating them from scratch.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '- **æ›´å¿«çš„éƒ¨ç½²ã€‚** èƒ½å¤ŸæŒç»­é‡ç°æ•°æ®ç®¡é“çš„å„ä¸ªå…ƒç´ è‡³å…³é‡è¦ã€‚å°†è¿™äº›å…ƒç´ è§†ä¸ºæ•°æ®ç®¡é“çš„æ„å»ºæ¨¡å—ã€‚å› æ­¤ï¼Œå½“éœ€è¦åˆ›å»ºæ–°çš„æ•°æ®ç®¡é“æ—¶ï¼Œé‡è¦çš„æ˜¯èƒ½å¤Ÿè½»æ¾å¤åˆ¶è¿™äº›æ„å»ºæ¨¡å—ï¼Œè€Œä¸æ˜¯ä»å¤´å¼€å§‹åˆ›å»ºã€‚'
- en: '- **Efficient data governance.** By creating a structured data flow, processes
    are managed and source controlled by relevant teams and data is accessible and
    manageable with ease.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '- **é«˜æ•ˆçš„æ•°æ®æ²»ç†ã€‚** é€šè¿‡åˆ›å»ºç»“æ„åŒ–çš„æ•°æ®æµï¼Œç›¸å…³å›¢é˜Ÿèƒ½å¤Ÿç®¡ç†å’Œæºæ§æµç¨‹ï¼Œæ•°æ®æ˜“äºè®¿é—®å’Œç®¡ç†ã€‚'
- en: Common problems while handling data workflows
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å¤„ç†æ•°æ®å·¥ä½œæµæ—¶çš„å¸¸è§é—®é¢˜
- en: ETL workflow complexity
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ETLå·¥ä½œæµå¤æ‚æ€§
- en: 'My experience suggests that the most typical issue while managing a data pipeline
    is the ETL process complexity. The data pipeline is usually defined by a set of
    data transformation steps moving the data from its source to its destination.
    There are numerous tools, even frameworks, approaches and techniques to do it
    and I previously wrote about it here:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘çš„ç»éªŒè¡¨æ˜ï¼Œç®¡ç†æ•°æ®ç®¡é“æ—¶æœ€å…¸å‹çš„é—®é¢˜æ˜¯**ETLè¿‡ç¨‹çš„å¤æ‚æ€§**ã€‚æ•°æ®ç®¡é“é€šå¸¸ç”±ä¸€ç³»åˆ—æ•°æ®è½¬æ¢æ­¥éª¤å®šä¹‰ï¼Œè¿™äº›æ­¥éª¤å°†æ•°æ®ä»æºå¤´è½¬ç§»åˆ°ç›®çš„åœ°ã€‚æœ‰è®¸å¤šå·¥å…·ï¼Œç”šè‡³æ˜¯æ¡†æ¶ã€æ–¹æ³•å’ŒæŠ€æœ¯å¯ä»¥åšåˆ°è¿™ä¸€ç‚¹ï¼Œæˆ‘ä¹‹å‰åœ¨è¿™é‡Œå†™è¿‡ï¼š
- en: '[](/data-pipeline-design-patterns-100afa4b93e3?source=post_page-----9887e1b5eb7a--------------------------------)
    [## Data pipeline design patterns'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/data-pipeline-design-patterns-100afa4b93e3?source=post_page-----9887e1b5eb7a--------------------------------)
    [## æ•°æ®ç®¡é“è®¾è®¡æ¨¡å¼'
- en: Choosing the right architecture with examples
  id: totrans-20
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: é€‰æ‹©æ­£ç¡®çš„æ¶æ„ç¤ºä¾‹
- en: towardsdatascience.com](/data-pipeline-design-patterns-100afa4b93e3?source=post_page-----9887e1b5eb7a--------------------------------)
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/data-pipeline-design-patterns-100afa4b93e3?source=post_page-----9887e1b5eb7a--------------------------------)
- en: So it does make sense to source control data pipelines and the steps they consist
    of. Documenting everything is very important so the rest of the team knows exactly
    what the data solution does.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: å› æ­¤ï¼Œå¯¹æ•°æ®ç®¡é“åŠå…¶åŒ…å«çš„æ­¥éª¤è¿›è¡Œæºä»£ç æ§åˆ¶æ˜¯æœ‰æ„ä¹‰çš„ã€‚è®°å½•æ‰€æœ‰å†…å®¹éå¸¸é‡è¦ï¼Œä»¥ä¾¿å›¢é˜Ÿå…¶ä»–æˆå‘˜å‡†ç¡®äº†è§£æ•°æ®è§£å†³æ–¹æ¡ˆçš„åŠŸèƒ½ã€‚
- en: It is always nice to have some visual representation of the data pipeline.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: æœ‰ä¸€ä¸ªæ•°æ®ç®¡é“çš„å¯è§†åŒ–è¡¨ç¤ºæ€»æ˜¯å¾ˆæœ‰å¸®åŠ©çš„ã€‚
- en: If you deploy pipelines with Airflow, DBT, Dataform, Jinja or AWS Step functions,
    it is great and usually these tools provide a great dependency graph functionality.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœä½ ä½¿ç”¨ Airflowã€DBTã€Dataformã€Jinja æˆ– AWS Step Functions éƒ¨ç½²ç®¡é“ï¼Œé‚£å¾ˆå¥½ï¼Œé€šå¸¸è¿™äº›å·¥å…·æä¾›äº†å¾ˆå¥½çš„ä¾èµ–å…³ç³»å›¾åŠŸèƒ½ã€‚
- en: '![](../Images/9aea09abe9e5292450751d0bcf44658f.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9aea09abe9e5292450751d0bcf44658f.png)'
- en: Example dependency graph. Image by author.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: ç¤ºä¾‹ä¾èµ–å…³ç³»å›¾ã€‚å›¾åƒç”±ä½œè€…æä¾›ã€‚
- en: Hard to replicate and deploy changes
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: éš¾ä»¥å¤åˆ¶å’Œéƒ¨ç½²æ›´æ”¹
- en: Very often data pipelines are complex and it might take a lot of time to change
    the associated resources and deploy them. In other words, it might become a very
    time-consuming task for data and machine learning engineers to do so.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: æ•°æ®ç®¡é“é€šå¸¸å¾ˆå¤æ‚ï¼Œå¯èƒ½éœ€è¦å¤§é‡æ—¶é—´æ¥æ›´æ”¹ç›¸å…³èµ„æºå¹¶è¿›è¡Œéƒ¨ç½²ã€‚æ¢å¥è¯è¯´ï¼Œè¿™å¯èƒ½æˆä¸ºæ•°æ®å’Œæœºå™¨å­¦ä¹ å·¥ç¨‹å¸ˆéå¸¸è€—æ—¶çš„ä»»åŠ¡ã€‚
- en: It is also essential to cover all parts of the data pipeline and not only the
    data transformation part. For example, Airflow is a great tool to orchestrate
    the data pipeline and we might want to use an S3 bucket connector for the data
    lake there but you also might want to describe the S3 resource and keep it in
    Github.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: è¿˜å¿…é¡»æ¶µç›–æ•°æ®ç®¡é“çš„æ‰€æœ‰éƒ¨åˆ†ï¼Œè€Œä¸ä»…ä»…æ˜¯æ•°æ®è½¬æ¢éƒ¨åˆ†ã€‚ä¾‹å¦‚ï¼ŒAirflow æ˜¯ä¸€ä¸ªå¾ˆå¥½çš„å·¥å…·æ¥åè°ƒæ•°æ®ç®¡é“ï¼Œæˆ‘ä»¬å¯èƒ½æƒ³åœ¨è¿™é‡Œä½¿ç”¨ S3 æ¡¶è¿æ¥å™¨æ¥è¿æ¥æ•°æ®æ¹–ï¼Œä½†ä½ å¯èƒ½è¿˜éœ€è¦æè¿°
    S3 èµ„æºå¹¶å°†å…¶ä¿å­˜åœ¨ Github ä¸Šã€‚
- en: This is where Infrastructure as Code (IAC) becomes useful.
  id: totrans-30
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: è¿™å°±æ˜¯åŸºç¡€è®¾æ–½å³ä»£ç ï¼ˆIACï¼‰å˜å¾—æœ‰ç”¨çš„åœ°æ–¹ã€‚
- en: With IAC tools like Terraform and AWS Cloudformaion, we can describe all resources
    we might need for our data pipeline and not only the ones that actually perform
    the data transformation. For example, we can describe not just the pipeline resources
    that transform the data, i.e. AWS Lambda functions and other services, but also
    data storage resources, notification settings and alarms for those microservices.
    Some IAC solutions are platform agnostic (Terraform), some are not (AWS Cloudformation)
    and all have their pros and cons **depending on the data stack in hand and DataOps
    team skills.**
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨åƒ Terraform å’Œ AWS CloudFormation è¿™æ ·çš„ IAC å·¥å…·ï¼Œæˆ‘ä»¬å¯ä»¥æè¿°æˆ‘ä»¬æ•°æ®ç®¡é“å¯èƒ½éœ€è¦çš„æ‰€æœ‰èµ„æºï¼Œè€Œä¸ä»…ä»…æ˜¯å®é™…æ‰§è¡Œæ•°æ®è½¬æ¢çš„èµ„æºã€‚ä¾‹å¦‚ï¼Œæˆ‘ä»¬å¯ä»¥æè¿°ä¸ä»…æ˜¯è½¬æ¢æ•°æ®çš„ç®¡é“èµ„æºï¼Œå³
    AWS Lambda å‡½æ•°å’Œå…¶ä»–æœåŠ¡ï¼Œè¿˜å¯ä»¥æè¿°æ•°æ®å­˜å‚¨èµ„æºã€é€šçŸ¥è®¾ç½®å’Œè¿™äº›å¾®æœåŠ¡çš„è­¦æŠ¥ã€‚ä¸€äº› IAC è§£å†³æ–¹æ¡ˆæ˜¯å¹³å°æ— å…³çš„ï¼ˆTerraformï¼‰ï¼Œä¸€äº›åˆ™ä¸æ˜¯ï¼ˆAWS
    CloudFormationï¼‰ï¼Œå®ƒä»¬éƒ½æœ‰å…¶ä¼˜ç¼ºç‚¹**æ ¹æ®æ‰‹å¤´çš„æ•°æ®å †æ ˆå’Œ DataOps å›¢é˜Ÿçš„æŠ€èƒ½è€Œå®šã€‚**
- en: '[](https://levelup.gitconnected.com/infrastructure-as-code-for-beginners-a4e36c805316?source=post_page-----9887e1b5eb7a--------------------------------)
    [## Infrastructure as Code for Beginners'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://levelup.gitconnected.com/infrastructure-as-code-for-beginners-a4e36c805316?source=post_page-----9887e1b5eb7a--------------------------------)
    [## åˆå­¦è€…çš„åŸºç¡€è®¾æ–½å³ä»£ç '
- en: Deploy Data Pipelines like a pro with these templates
  id: totrans-33
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ä½¿ç”¨è¿™äº›æ¨¡æ¿åƒä¸“ä¸šäººå£«ä¸€æ ·éƒ¨ç½²æ•°æ®ç®¡é“
- en: levelup.gitconnected.com](https://levelup.gitconnected.com/infrastructure-as-code-for-beginners-a4e36c805316?source=post_page-----9887e1b5eb7a--------------------------------)
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: levelup.gitconnected.com](https://levelup.gitconnected.com/infrastructure-as-code-for-beginners-a4e36c805316?source=post_page-----9887e1b5eb7a--------------------------------)
- en: Data pipeline solutions that can be continuously reproduced and deployed in
    different environments are great because they are source controlled and have great
    **CI/CD features**. All these things help to **avoid any potential human errors
    and to decrease data engineering time and costs.**
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: å¯ä»¥åœ¨ä¸åŒç¯å¢ƒä¸­æŒç»­é‡å¤å’Œéƒ¨ç½²çš„æ•°æ®ç®¡é“è§£å†³æ–¹æ¡ˆå¾ˆæ£’ï¼Œå› ä¸ºå®ƒä»¬æ˜¯æºä»£ç æ§åˆ¶çš„ï¼Œå¹¶å…·æœ‰å¾ˆå¥½çš„**CI/CD ç‰¹æ€§**ã€‚æ‰€æœ‰è¿™äº›æœ‰åŠ©äº**é¿å…æ½œåœ¨çš„äººä¸ºé”™è¯¯ï¼Œå¹¶å‡å°‘æ•°æ®å·¥ç¨‹æ—¶é—´å’Œæˆæœ¬**ã€‚
- en: So the modern approach for data pipeline management and orchestration is the
    one to reduce all potential issues mentioned above, i.e.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: ç°ä»£çš„æ•°æ®ç®¡é“ç®¡ç†å’Œåè°ƒæ–¹æ³•æ˜¯å‡å°‘ä¸Šè¿°æ‰€æœ‰æ½œåœ¨é—®é¢˜çš„ï¼Œå³
- en: To reduce human errors, be able to easily replicate data pipeline resources,
    visualize the dependencies and improve data quality.
  id: totrans-37
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ä¸ºäº†å‡å°‘äººä¸ºé”™è¯¯ï¼Œèƒ½å¤Ÿè½»æ¾å¤åˆ¶æ•°æ®ç®¡é“èµ„æºï¼Œå¯è§†åŒ–ä¾èµ–å…³ç³»å¹¶æé«˜æ•°æ®è´¨é‡ã€‚
- en: '**Data orchestration done right increases the availability and accessibility
    of data for analytics.**'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '**æ­£ç¡®çš„æ•°æ®åè°ƒå¯ä»¥å¢åŠ æ•°æ®çš„å¯ç”¨æ€§å’Œåˆ†æçš„å¯åŠæ€§ã€‚**'
- en: Typical data flow that needs to be managed
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: éœ€è¦ç®¡ç†çš„å…¸å‹æ•°æ®æµ
- en: Typically by data pipeline we mean a collection of data-related resources that
    help us to deliver and transform the data from point A to point B.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: é€šå¸¸ï¼Œæ•°æ®ç®¡é“æŒ‡çš„æ˜¯ä¸€ç»„æ•°æ®ç›¸å…³çš„èµ„æºï¼Œå¸®åŠ©æˆ‘ä»¬å°†æ•°æ®ä»Aç‚¹ä¼ é€åˆ°Bç‚¹å¹¶è¿›è¡Œè½¬æ¢ã€‚
- en: 'By resources we mean tools and at the conceptual level we have three main data
    processing tasks that must be able to perform effectively:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: èµ„æºæŒ‡çš„æ˜¯å·¥å…·ï¼Œåœ¨æ¦‚å¿µå±‚é¢ä¸Šï¼Œæˆ‘ä»¬æœ‰ä¸‰ä¸ªä¸»è¦çš„æ•°æ®å¤„ç†ä»»åŠ¡ï¼Œå¿…é¡»èƒ½å¤Ÿæœ‰æ•ˆæ‰§è¡Œï¼š
- en: '**- Data Storage**'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '**- æ•°æ®å­˜å‚¨**'
- en: This can be data lakes in Google Cloud Storage, AWS S3 data lakes, etc. or any
    Relational and non-relational databases or even third-party resources available
    via APIs. they all serve one purpose to store the data. And in the majority of
    cases, this is where the data will be coming from on our data pipeline design
    diagram.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å¯ä»¥æ˜¯Google Cloud Storageä¸­çš„æ•°æ®æ¹–ã€AWS S3æ•°æ®æ¹–ç­‰ï¼Œæˆ–è€…ä»»ä½•å…³ç³»å‹å’Œéå…³ç³»å‹æ•°æ®åº“ï¼Œç”šè‡³æ˜¯é€šè¿‡APIæä¾›çš„ç¬¬ä¸‰æ–¹èµ„æºã€‚å®ƒä»¬çš„å…±åŒç›®çš„æ˜¯å­˜å‚¨æ•°æ®ã€‚åœ¨å¤§å¤šæ•°æƒ…å†µä¸‹ï¼Œè¿™å°±æ˜¯æˆ‘ä»¬æ•°æ®ç®¡é“è®¾è®¡å›¾ä¸­æ•°æ®çš„æ¥æºã€‚
- en: '**- Data loading or ingestion.**'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '**- æ•°æ®åŠ è½½æˆ–æ‘„å–ã€‚**'
- en: This can be managed tools (Fivetran, Stitch, etc.) or something bespoke like
    serverless microservices and various AWS Lambda or GCP Cloud Functions. They all
    serve one purpose to perform an ETL and to load data into the destination, i.e.
    data warehouse or another data lake depending on our data platform architecture
    type. Sometimes it might be more efficient to use tools that scale well, i.e.
    Spark.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™äº›å¯ä»¥æ˜¯ç®¡ç†å·¥å…·ï¼ˆå¦‚Fivetranã€Stitchç­‰ï¼‰ï¼Œä¹Ÿå¯ä»¥æ˜¯å®šåˆ¶çš„æ— æœåŠ¡å™¨å¾®æœåŠ¡å’Œå„ç§AWS Lambdaæˆ–GCP Cloud Functionsã€‚å®ƒä»¬çš„å…±åŒç›®çš„æ˜¯æ‰§è¡ŒETLå¹¶å°†æ•°æ®åŠ è½½åˆ°ç›®æ ‡ä¸­ï¼Œä¾‹å¦‚æ•°æ®ä»“åº“æˆ–æ ¹æ®æˆ‘ä»¬çš„æ•°æ®å¹³å°æ¶æ„ç±»å‹çš„å…¶ä»–æ•°æ®æ¹–ã€‚æœ‰æ—¶ä½¿ç”¨èƒ½å¤Ÿè‰¯å¥½æ‰©å±•çš„å·¥å…·ï¼ˆä¾‹å¦‚Sparkï¼‰å¯èƒ½æ›´é«˜æ•ˆã€‚
- en: '[](/data-platform-architecture-types-f255ac6e0b7?source=post_page-----9887e1b5eb7a--------------------------------)
    [## Data Platform Architecture Types'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/data-platform-architecture-types-f255ac6e0b7?source=post_page-----9887e1b5eb7a--------------------------------)
    [## æ•°æ®å¹³å°æ¶æ„ç±»å‹'
- en: How well does it answer your business needs? Dilemma of a choice.
  id: totrans-47
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: å®ƒåœ¨å¤šå¤§ç¨‹åº¦ä¸Šæ»¡è¶³äº†ä½ çš„ä¸šåŠ¡éœ€æ±‚ï¼Ÿé€‰æ‹©çš„å›°å¢ƒã€‚
- en: towardsdatascience.com](/data-platform-architecture-types-f255ac6e0b7?source=post_page-----9887e1b5eb7a--------------------------------)
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/data-platform-architecture-types-f255ac6e0b7?source=post_page-----9887e1b5eb7a--------------------------------)
- en: '**- Data transformation tools**'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '**- æ•°æ®è½¬æ¢å·¥å…·**'
- en: Historically the most natural way to transform data is SQL. This is a common
    data manipulation language recognised by all teams, Business Intelligence (BI)
    and data analysts, software engineers and data scientists. So there is a variety
    of tools at the moment that can offer reliable source-controlled data transformation
    with SQL queries, i.e. Jinja, DBT, Dataform, AWS Step Functions etc.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: å†å²ä¸Šï¼Œæœ€è‡ªç„¶çš„æ•°æ®è½¬æ¢æ–¹å¼æ˜¯SQLã€‚è¿™æ˜¯ä¸€ç§è¢«æ‰€æœ‰å›¢é˜Ÿã€å•†ä¸šæ™ºèƒ½ï¼ˆBIï¼‰å’Œæ•°æ®åˆ†æå¸ˆã€è½¯ä»¶å·¥ç¨‹å¸ˆåŠæ•°æ®ç§‘å­¦å®¶è®¤å¯çš„å¸¸ç”¨æ•°æ®æ“ä½œè¯­è¨€ã€‚å› æ­¤ï¼Œç›®å‰æœ‰å¤šç§å·¥å…·å¯ä»¥æä¾›å¯é çš„æºæ§æ•°æ®è½¬æ¢ï¼Œä¾‹å¦‚Jinjaã€DBTã€Dataformã€AWS
    Step Functionsç­‰ã€‚
- en: '**- Business intelligence**'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '**- å•†ä¸šæ™ºèƒ½**'
- en: These tools help to deliver analyses and insights. Some of them a free community
    tools, i.e. Looker Studio and others are subscription-based paid only. All have
    pros and cons and it might be wise to choose one based on the company size. For
    example, SMEs donâ€™t usually need to pay extra for BI OLAP cube features where
    data is being additionally analyzed and transformed. All they need is to email
    the daily dashboard with the main KPIs.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™äº›å·¥å…·å¸®åŠ©æä¾›åˆ†æå’Œè§è§£ã€‚å…¶ä¸­ä¸€äº›æ˜¯å…è´¹çš„ç¤¾åŒºå·¥å…·ï¼Œå¦‚Looker Studioï¼Œå…¶ä»–çš„æ˜¯ä»…é™è®¢é˜…çš„ä»˜è´¹å·¥å…·ã€‚æ¯ç§å·¥å…·éƒ½æœ‰ä¼˜ç¼ºç‚¹ï¼Œé€‰æ‹©æ—¶å¯èƒ½éœ€è¦æ ¹æ®å…¬å¸è§„æ¨¡æ¥å†³å®šã€‚ä¾‹å¦‚ï¼Œä¸­å°ä¼ä¸šé€šå¸¸ä¸éœ€è¦ä¸ºBI
    OLAPç«‹æ–¹ä½“åŠŸèƒ½æ”¯ä»˜é¢å¤–è´¹ç”¨ï¼Œè¿™äº›åŠŸèƒ½ç”¨äºé¢å¤–åˆ†æå’Œè½¬æ¢æ•°æ®ã€‚ä»–ä»¬åªéœ€é€šè¿‡ç”µå­é‚®ä»¶å‘é€åŒ…å«ä¸»è¦KPIçš„æ¯æ—¥ä»ªè¡¨ç›˜ã€‚
- en: The list of tools is massive and some great BI solutions are also available
    with free features.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: å·¥å…·åˆ—è¡¨åºå¤§ï¼Œä¸€äº›å‡ºè‰²çš„BIè§£å†³æ–¹æ¡ˆä¹Ÿæä¾›å…è´¹åŠŸèƒ½ã€‚
- en: '**Not an extensive list:**'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '**ä¸æ˜¯è¯¦å°½çš„åˆ—è¡¨ï¼š**'
- en: AWS Quicksight
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: AWS Quicksight
- en: Mode
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mode
- en: Sisense
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sisense
- en: Looker
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Looker
- en: Metabase
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Metabase
- en: PowerBI
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PowerBI
- en: '**The problem is that data tools with fully integrated features donâ€™t exist.**'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '**é—®é¢˜åœ¨äºæ²¡æœ‰å®Œå…¨é›†æˆåŠŸèƒ½çš„æ•°æ®å·¥å…·ã€‚**'
- en: How do we link and connect data services together?
  id: totrans-62
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¦‚ä½•å°†æ•°æ®æœåŠ¡è¿æ¥åœ¨ä¸€èµ·ï¼Ÿ
- en: Wrapping our data processes **into one solution** can also be achieved with
    a little bit of programming and Infrastructure as Code techniques.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: å°†æˆ‘ä»¬çš„æ•°æ®å¤„ç†è¿‡ç¨‹**æ•´åˆåˆ°ä¸€ä¸ªè§£å†³æ–¹æ¡ˆä¸­**ä¹Ÿå¯ä»¥é€šè¿‡ä¸€ç‚¹ç¼–ç¨‹å’ŒåŸºç¡€è®¾æ–½å³ä»£ç æŠ€æœ¯æ¥å®ç°ã€‚
- en: 'Conceptually we might want to deploy a data pipeline like this:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: ä»æ¦‚å¿µä¸Šè®²ï¼Œæˆ‘ä»¬å¯èƒ½å¸Œæœ›åƒè¿™æ ·éƒ¨ç½²æ•°æ®ç®¡é“ï¼š
- en: '![](../Images/b999ac569c1399f8eacbfa0d66c53a15.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b999ac569c1399f8eacbfa0d66c53a15.png)'
- en: Image by author.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ç‰‡ç”±ä½œè€…æä¾›ã€‚
- en: How do we make this solution robust and cost-effective?
  id: totrans-67
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¦‚ä½•ä½¿è¿™ä¸ªè§£å†³æ–¹æ¡ˆæ—¢å¼ºå¤§åˆå…·æœ‰æˆæœ¬æ•ˆç›Šï¼Ÿ
- en: With no doubt, the most cost-effective way would be to use **serverless architecture**.
    Consider this data pipeline below, for example. It can be deployed with IAC (AWS
    Cloudformation or Terraform) and all main parts are integrated into one complete
    data solution.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: æ¯‹åº¸ç½®ç–‘ï¼Œæœ€å…·æˆæœ¬æ•ˆç›Šçš„æ–¹æ³•æ˜¯ä½¿ç”¨**æ— æœåŠ¡å™¨æ¶æ„**ã€‚ä¾‹å¦‚ï¼Œè€ƒè™‘ä¸‹é¢çš„æ•°æ®ç®¡é“ã€‚å®ƒå¯ä»¥é€šè¿‡IACï¼ˆAWS Cloudformationæˆ–Terraformï¼‰è¿›è¡Œéƒ¨ç½²ï¼Œå¹¶ä¸”æ‰€æœ‰ä¸»è¦éƒ¨åˆ†éƒ½é›†æˆåœ¨ä¸€ä¸ªå®Œæ•´çš„æ•°æ®è§£å†³æ–¹æ¡ˆä¸­ã€‚
- en: We have AWS Lambda functions, AWS Step functions, a relational database, data
    lake storage buckets and a data lake house solution (AWS Athena).
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬æœ‰AWS Lambdaå‡½æ•°ã€AWS Stepå‡½æ•°ã€ä¸€ä¸ªå…³ç³»å‹æ•°æ®åº“ã€æ•°æ®æ¹–å­˜å‚¨æ¡¶å’Œä¸€ä¸ªæ•°æ®æ¹–æˆ¿è§£å†³æ–¹æ¡ˆï¼ˆAWS Athenaï¼‰ã€‚
- en: '![](../Images/3d6fa66e126398bfe0bd267f57bf7059.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3d6fa66e126398bfe0bd267f57bf7059.png)'
- en: Example DAG. Image by author.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: ç¤ºä¾‹DAGã€‚å›¾ç‰‡ç”±ä½œè€…æä¾›ã€‚
- en: Consider this data pipeline example which consists of an AWS Lambda function.
    Standalone, microservice like this is just a lambda function but there is so much
    more we can do with it, i.e. extract data from APIs, export data from relational
    databases, invoke and trigger other services, etc.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: è€ƒè™‘è¿™ä¸ªç”±AWS Lambdaå‡½æ•°ç»„æˆçš„æ•°æ®ç®¡é“ç¤ºä¾‹ã€‚ä½œä¸ºä¸€ä¸ªç‹¬ç«‹çš„å¾®æœåŠ¡ï¼Œå®ƒåªæ˜¯ä¸€ä¸ªlambdaå‡½æ•°ï¼Œä½†æˆ‘ä»¬å¯ä»¥ç”¨å®ƒåšæ›´å¤šçš„äº‹æƒ…ï¼Œä¾‹å¦‚ä»APIæå–æ•°æ®ã€ä»å…³ç³»å‹æ•°æ®åº“å¯¼å‡ºæ•°æ®ã€è°ƒç”¨å’Œè§¦å‘å…¶ä»–æœåŠ¡ç­‰ã€‚
- en: 'We can deploy the pipeline with just one AWS CLI command using infrastructure
    as code:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥é€šè¿‡ä¸€ä¸ªAWS CLIå‘½ä»¤ä½¿ç”¨åŸºç¡€è®¾æ–½å³ä»£ç æ¥éƒ¨ç½²ç®¡é“ï¼š
- en: '![](../Images/a53bea5cc9239238a4bb2fce87a101e8.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a53bea5cc9239238a4bb2fce87a101e8.png)'
- en: Deploy result. Image by author.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: éƒ¨ç½²ç»“æœã€‚å›¾ç‰‡ç”±ä½œè€…æä¾›ã€‚
- en: With this, we should be able to create a simple Step Function and can use it
    as a template to extend and improve our ETL pipeline.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: æœ‰äº†è¿™ä¸ªï¼Œæˆ‘ä»¬åº”è¯¥èƒ½å¤Ÿåˆ›å»ºä¸€ä¸ªç®€å•çš„Step Functionï¼Œå¹¶å¯ä»¥å°†å…¶ç”¨ä½œæ¨¡æ¿æ¥æ‰©å±•å’Œæ”¹è¿›æˆ‘ä»¬çš„ETLç®¡é“ã€‚
- en: Should we choose to **be platform agnostic** we can deploy the complete pipeline
    with **Terraform** and use various data services across different cloud platforms.
    Often I use this data pipeline design pattern with AWS Lambda functions to run
    SQL queries in my BigQuery data warehouse solution or to perform any other ETL
    task. Very useful.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæˆ‘ä»¬é€‰æ‹©**å¹³å°æ— å…³**ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨**Terraform**æ¥éƒ¨ç½²å®Œæ•´çš„ç®¡é“ï¼Œå¹¶åœ¨ä¸åŒçš„äº‘å¹³å°ä¸Šä½¿ç”¨å„ç§æ•°æ®æœåŠ¡ã€‚æˆ‘ç»å¸¸ä½¿ç”¨è¿™ç§æ•°æ®ç®¡é“è®¾è®¡æ¨¡å¼ä¸AWS
    Lambdaå‡½æ•°ç»“åˆä½¿ç”¨ï¼Œä»¥åœ¨æˆ‘çš„BigQueryæ•°æ®ä»“åº“è§£å†³æ–¹æ¡ˆä¸­è¿è¡ŒSQLæŸ¥è¯¢æˆ–æ‰§è¡Œå…¶ä»–ETLä»»åŠ¡ï¼Œéå¸¸æœ‰ç”¨ã€‚
- en: Conclusion
  id: totrans-78
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ç»“è®º
- en: Wrapping our data processes into one solution might be challenging and would
    require some **imagination**.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: å°†æˆ‘ä»¬çš„æ•°æ®å¤„ç†è¿‡ç¨‹æ‰“åŒ…æˆä¸€ä¸ªè§£å†³æ–¹æ¡ˆå¯èƒ½ä¼šå¾ˆå…·æŒ‘æˆ˜æ€§ï¼Œå¹¶ä¸”éœ€è¦ä¸€äº›**æƒ³è±¡åŠ›**ã€‚
- en: Infrastructure as code sounds like the right way to go. Indeed, it simplifies
    data solution deployments and replication, eliminates human errors and helps to
    perform data engineering and MLOps tasks faster. Although it might look complex
    and sophisticated, it all comes with experience and requires time to learn. Donâ€™t
    hesitate to invest a bit of yours in learning it. This is a very rewarding skill.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: åŸºç¡€è®¾æ–½å³ä»£ç å¬èµ·æ¥æ˜¯æ­£ç¡®çš„é€‰æ‹©ã€‚ç¡®å®ï¼Œå®ƒç®€åŒ–äº†æ•°æ®è§£å†³æ–¹æ¡ˆçš„éƒ¨ç½²å’Œå¤åˆ¶ï¼Œæ¶ˆé™¤äº†äººä¸ºé”™è¯¯ï¼Œå¹¶æœ‰åŠ©äºæ›´å¿«åœ°æ‰§è¡Œæ•°æ®å·¥ç¨‹å’ŒMLOpsä»»åŠ¡ã€‚å°½ç®¡å®ƒå¯èƒ½çœ‹èµ·æ¥å¤æ‚å’Œç²¾è‡´ï¼Œä½†è¿™éƒ½éœ€è¦ç»éªŒï¼Œå¹¶ä¸”éœ€è¦æ—¶é—´æ¥å­¦ä¹ ã€‚ä¸è¦çŠ¹è±«ï¼ŒæŠ•å…¥ä¸€ç‚¹æ—¶é—´æ¥å­¦ä¹ å®ƒã€‚è¿™æ˜¯ä¸€é¡¹éå¸¸æœ‰ä»·å€¼çš„æŠ€èƒ½ã€‚
- en: It can offer multitudes of options for data pipeline orchestration.
  id: totrans-81
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: å®ƒå¯ä»¥æä¾›å¤šç§æ•°æ®ç®¡é“ç¼–æ’é€‰é¡¹ã€‚
- en: With just a little bit of coding and knowing how APIs work opportunities for
    data pipeline design and management become endless.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: åªéœ€ä¸€ç‚¹ç¼–ç å’Œäº†è§£APIçš„å·¥ä½œæ–¹å¼ï¼Œæ•°æ®ç®¡é“è®¾è®¡å’Œç®¡ç†çš„æœºä¼šå°±å˜å¾—æ— ç©·æ— å°½ã€‚
- en: Recommended read
  id: totrans-83
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ¨èé˜…è¯»
- en: '[](/create-mysql-and-postgres-instances-using-aws-cloudformation-d3af3c46c22a?source=post_page-----9887e1b5eb7a--------------------------------)
    [## Create MySQL and Postgres instances using AWS Cloudformation'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/create-mysql-and-postgres-instances-using-aws-cloudformation-d3af3c46c22a?source=post_page-----9887e1b5eb7a--------------------------------)
    [## ä½¿ç”¨AWS Cloudformationåˆ›å»ºMySQLå’ŒPostgreså®ä¾‹'
- en: Infrastructure as Code for database practitioners
  id: totrans-85
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: æ•°æ®åº“ä»ä¸šè€…çš„åŸºç¡€è®¾æ–½å³ä»£ç 
- en: towardsdatascience.com](/create-mysql-and-postgres-instances-using-aws-cloudformation-d3af3c46c22a?source=post_page-----9887e1b5eb7a--------------------------------)
    [](https://aws.amazon.com/cloudformation/?source=post_page-----9887e1b5eb7a--------------------------------)
    [## Provision Infrastructure as Code - AWS CloudFormation - AWS
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '[towardsdatascience.com](/create-mysql-and-postgres-instances-using-aws-cloudformation-d3af3c46c22a?source=post_page-----9887e1b5eb7a--------------------------------)
    [](https://aws.amazon.com/cloudformation/?source=post_page-----9887e1b5eb7a--------------------------------)
    [## ä½œä¸ºä»£ç çš„åŸºç¡€è®¾æ–½ - AWS CloudFormation - AWS'
- en: AWS CloudFormation Scale your infrastructure worldwide and manage resources
    across all AWS accounts and regions throughâ€¦
  id: totrans-87
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: AWS CloudFormation é€šè¿‡ AWS CloudFormationï¼Œæ‚¨å¯ä»¥åœ¨å…¨çƒèŒƒå›´å†…æ‰©å±•åŸºç¡€è®¾æ–½ï¼Œå¹¶ç®¡ç†æ‰€æœ‰ AWS è´¦æˆ·å’ŒåŒºåŸŸä¸­çš„èµ„æºâ€¦
- en: aws.amazon.com](https://aws.amazon.com/cloudformation/?source=post_page-----9887e1b5eb7a--------------------------------)
    [](https://aws.amazon.com/step-functions/?source=post_page-----9887e1b5eb7a--------------------------------)
    [## Serverless Workflow Orchestration - AWS Step Functions - Amazon Web Services
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '[aws.amazon.com](https://aws.amazon.com/cloudformation/?source=post_page-----9887e1b5eb7a--------------------------------)
    [](https://aws.amazon.com/step-functions/?source=post_page-----9887e1b5eb7a--------------------------------)
    [## æ— æœåŠ¡å™¨å·¥ä½œæµç¼–æ’ - AWS Step Functions - äºšé©¬é€Šç½‘ç»œæœåŠ¡'
- en: AWS Step Functions 4,000 state transitions per month Use code to process data
    on demand with large-scale parallelâ€¦
  id: totrans-89
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: AWS Step Functions æ¯æœˆæ”¯æŒ 4,000 æ¬¡çŠ¶æ€è½¬æ¢ ä½¿ç”¨ä»£ç æŒ‰éœ€å¤„ç†æ•°æ®ï¼Œè¿›è¡Œå¤§è§„æ¨¡å¹¶è¡Œå¤„ç†â€¦
- en: aws.amazon.com](https://aws.amazon.com/step-functions/?source=post_page-----9887e1b5eb7a--------------------------------)
    [](https://www.terraform.io/?source=post_page-----9887e1b5eb7a--------------------------------)
    [## Terraform by HashiCorp
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '[aws.amazon.com](https://aws.amazon.com/step-functions/?source=post_page-----9887e1b5eb7a--------------------------------)
    [](https://www.terraform.io/?source=post_page-----9887e1b5eb7a--------------------------------)
    [## HashiCorp çš„ Terraform'
- en: Terraform is an open-source infrastructure as code software tool that enables
    you to safely and predictably createâ€¦
  id: totrans-91
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Terraform æ˜¯ä¸€æ¬¾å¼€æºçš„åŸºç¡€è®¾æ–½å³ä»£ç è½¯ä»¶å·¥å…·ï¼Œèƒ½å¤Ÿå¸®åŠ©æ‚¨å®‰å…¨ã€å¯é¢„æµ‹åœ°åˆ›å»ºâ€¦
- en: www.terraform.io](https://www.terraform.io/?source=post_page-----9887e1b5eb7a--------------------------------)
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '[www.terraform.io](https://www.terraform.io/?source=post_page-----9887e1b5eb7a--------------------------------)'
