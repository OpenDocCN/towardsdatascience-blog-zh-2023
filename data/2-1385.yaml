- en: Kaiming He Initialization in Neural Networks — Math Proof
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Kaiming He 初始化在神经网络中的数学证明
- en: 原文：[https://towardsdatascience.com/kaiming-he-initialization-in-neural-networks-math-proof-73b9a0d845c4](https://towardsdatascience.com/kaiming-he-initialization-in-neural-networks-math-proof-73b9a0d845c4)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/kaiming-he-initialization-in-neural-networks-math-proof-73b9a0d845c4](https://towardsdatascience.com/kaiming-he-initialization-in-neural-networks-math-proof-73b9a0d845c4)
- en: Deriving optimal initial variance of weight matrices in neural network layers
    with ReLU activation function
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 推导具有 ReLU 激活函数的神经网络层中权重矩阵的最优初始方差
- en: '[](https://medium.com/@EsterHlav?source=post_page-----73b9a0d845c4--------------------------------)[![Ester
    Hlav](../Images/5fe679b93c42d568d0d5b331a8bf92b9.png)](https://medium.com/@EsterHlav?source=post_page-----73b9a0d845c4--------------------------------)[](https://towardsdatascience.com/?source=post_page-----73b9a0d845c4--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----73b9a0d845c4--------------------------------)
    [Ester Hlav](https://medium.com/@EsterHlav?source=post_page-----73b9a0d845c4--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@EsterHlav?source=post_page-----73b9a0d845c4--------------------------------)[![Ester
    Hlav](../Images/5fe679b93c42d568d0d5b331a8bf92b9.png)](https://medium.com/@EsterHlav?source=post_page-----73b9a0d845c4--------------------------------)[](https://towardsdatascience.com/?source=post_page-----73b9a0d845c4--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----73b9a0d845c4--------------------------------)
    [Ester Hlav](https://medium.com/@EsterHlav?source=post_page-----73b9a0d845c4--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----73b9a0d845c4--------------------------------)
    ·10 min read·Feb 15, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----73b9a0d845c4--------------------------------)
    ·阅读时间约 10 分钟·2023 年 2 月 15 日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/0e072a98caece16e7471a537515610cc.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0e072a98caece16e7471a537515610cc.png)'
- en: Initialization techniques are one of the prerequisites for successfully training
    a deep learning architecture. Traditionally, weight initialization methods need
    to be compatible with the choice of an activation function as a mismatch can potentially
    affect training negatively.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 初始化技术是成功训练深度学习架构的前提条件之一。传统上，权重初始化方法需要与激活函数的选择兼容，因为不匹配可能会对训练产生负面影响。
- en: ReLU is one of the most commonly used activation functions in deep learning.
    Its properties make it a very convenient choice for scaling to large neural networks.
    On one hand, it is inexpensive to calculate the derivative during backpropagation
    because it is a linear function with a step-function derivative. On the other
    hand, ReLU helps reduce feature correlation as it is a non-negative activation
    function, *i.e.* features can only contribute positively to subsequent layers.
    It is a prevalent choice in convolutional architectures where the input dimension
    is large and neural networks tend to be very deep.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: ReLU 是深度学习中最常用的激活函数之一。它的特性使其成为扩展到大型神经网络的非常方便的选择。一方面，由于它是一个线性函数，具有步进函数导数，在反向传播过程中计算导数的开销较小。另一方面，ReLU
    有助于减少特征相关性，因为它是一个非负激活函数，*即* 特征只能对后续层产生积极贡献。它在输入维度较大且神经网络往往非常深的卷积架构中是一种流行的选择。
- en: 'In *“Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet
    Classification”* ⁽*¹* ⁾ by He *et al.* (2015), the authors present a methodology
    to optimally initialize neural network layers using a ReLU activation function.
    This technique allows the neural network to start in a regime with constant variance
    between inputs and outputs both in terms of forward and backward passes, which
    empirically showed meaningful improvement in training stability and speed. In
    the following sections, we provide a detailed and complete derivation behind the
    He initialization technique.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在 He *et al.*（2015）的*“深入探讨整流器：超越人类水平的 ImageNet 分类”* ⁽*¹* ⁾ 中，作者提出了一种使用 ReLU
    激活函数来最优初始化神经网络层的方法。这种技术使神经网络在前向和反向传播过程中在输入和输出之间保持常数方差，这在经验上显示出训练稳定性和速度的显著改善。在接下来的章节中，我们将提供
    He 初始化技术背后的详细和完整的推导过程。
- en: Notation
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 符号
- en: A layer in a neural network, composed of a weight matrix *Wₖ* and bias vector
    *bₖ*, undergoes two consecutive transformations. The first transformation is *yₖ
    = xₖ Wₖ + bₖ*, and the second is *xₖ ₊ ₁ = f(yₖ)*
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 神经网络中的一层，由权重矩阵 *Wₖ* 和偏置向量 *bₖ* 组成，经过两个连续的变换。第一个变换是 *yₖ = xₖ Wₖ + bₖ*，第二个变换是
    *xₖ ₊ ₁ = f(yₖ)*
- en: '*xₖ* is the actual layer and *yₖ* is the pre-activation layer'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*xₖ* 是实际层，*yₖ* 是预激活层'
- en: A layer has *nₖ* units, thus *xₖ ∈ ℝⁿ⁽* ᵏ*⁾, Wₖ ∈ ℝⁿ⁽* ᵏ*⁾*˙*ⁿ⁽* ᵏ ⁺ ¹*⁾, bₖ
    ∈ℝⁿ⁽* ᵏ ⁺ ¹*⁾*
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一层有*nₖ*个单元，因此*xₖ ∈ ℝⁿ⁽* ᵏ*⁾, Wₖ ∈ ℝⁿ⁽* ᵏ*⁾*˙*ⁿ⁽* ᵏ ⁺ ¹*⁾, bₖ ∈ℝⁿ⁽* ᵏ ⁺ ¹*⁾*
- en: '*xₖWₖ + bₖ* has dimension ( *1 × nₖ ) × ( nₖ × nₖ ₊* ₁*) + 1 × nₖ ₊* ₗ *= 1
    × nₖ ₊* ₁'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*xₖWₖ + bₖ* 的维度是（*1 × nₖ*）×（*nₖ × nₖ ₊* ₁*）+ 1 × nₖ ₊* ₗ* = 1 × nₖ ₊* ₁'
- en: The activation function *f* is applied element-wise and does not change the
    shape of a vector. As a result, *xₖ ₊* ₁*= f(xₖ Wₖ+ bₖ)∈ ℝⁿ⁽* ᵏ ⁺ ¹*⁾*
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 激活函数*f*按元素应用，不改变向量的形状。因此，*xₖ ₊* ₁*= f(xₖ Wₖ+ bₖ)∈ ℝⁿ⁽* ᵏ ⁺ ¹*⁾*
- en: For a neural network of depth *n*, the input layer is represented by *x₀* and
    the output layer by *xₙ*
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于深度为*n*的神经网络，输入层由*x₀*表示，输出层由*xₙ*表示
- en: The loss function of the network is represented by *L*
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 网络的损失函数由*L*表示
- en: '*Δx = ∂L/∂x* denotes gradients of the loss function with respect to vector
    *x*'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Δx = ∂L/∂x* 表示损失函数关于向量*x*的梯度'
- en: Assumptions
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 假设
- en: '*Assumption 1*:'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*假设 1*：'
- en: We assume for this initialization setup a *non-linear* activation function ReLU
    defined as *f(x) =* *ReLU(x) = max(0, x).* As a function defined separately on
    two intervals, its derivative has a value of 1 on the strictly positive half of
    *ℝ* and 0 on the strictly negative half. Technically, the derivative of ReLU is
    not defined in 0 due to the limits of both sides not being equal, that is *f’(0⁻*⁻*)
    = 0 ≠ 1 = f’(0⁺).* In practice, for backpropagation’s purpose, ReLU’(0) is assumed
    to be 0.
  id: totrans-21
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对于这个初始化设置，我们假设一个*非线性*激活函数ReLU定义为*f(x) =* *ReLU(x) = max(0, x).* 作为在两个区间上分别定义的函数，它的导数在*ℝ*的严格正半轴上值为1，在严格负半轴上值为0。技术上，由于两侧的极限不相等，ReLU在0处的导数未定义，即*f’(0⁻*⁻*)
    = 0 ≠ 1 = f’(0⁺).* 实际上，为了反向传播的目的，假设ReLU’(0)为0。
- en: '*Assumption 2:*'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*假设 2*：'
- en: It is assumed that all inputs, weights, and layers in the neural network are
    independent and identically distributed (*iid*) at initialization, as well as
    the gradients.
  id: totrans-23
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 假设神经网络中的所有输入、权重和层在初始化时都是独立同分布的（*iid*），梯度也是如此。
- en: '*Assumption 3:*'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*假设 3*：'
- en: The inputs are assumed to be normalized with zero mean and the weights and biases
    are initialized from a symmetric distribution centered at zero, *i.e. 𝔼[x₀] =
    𝔼[Wₖ] = 𝔼[bₖ] = 0*. This means that both *xₖ* and yₖ have an expectation of zero
    at initialization, and *yₖ* has a symmetric distribution at initialization due
    to *f(0) = 0*.
  id: totrans-25
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 假设输入已归一化为零均值，权重和偏置从以零为中心的对称分布中初始化，即*𝔼[x₀] = 𝔼[Wₖ] = 𝔼[bₖ] = 0*。这意味着*xₖ*和yₖ在初始化时都有零的期望，由于*f(0)
    = 0*，*yₖ*在初始化时具有对称分布。
- en: Motivation
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 动机
- en: 'The aim of this proof is to determine the distribution of the weight matrix
    by finding *Var[W]* given two constraints:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 本证明的目的是通过在两个约束条件下找到*Var[W]*来确定权重矩阵的分布：
- en: ∀*k, Var[yₖ] = Var[yₖ ₋* ₁*]*, *i.e.* constant variance in the forward signal
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ∀*k, Var[yₖ] = Var[yₖ ₋* ₁*]*，*即*前向信号中的方差是恒定的
- en: ∀*k, Var[Δxₖ] = Var[Δxₖ ₊* ₁*], i.e.* constant variance in the backward signal
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ∀*k, Var[Δxₖ] = Var[Δxₖ ₊* ₁*]，即*反向信号中的方差是恒定的
- en: Ensuring that the variance of both layers and gradients is constant throughout
    the network at initialization helps prevent exploding and vanishing gradients
    in neural networks. If the gain is *above* one, it will result in exploding gradients
    and optimization divergence, while if the gain is *below* one, it will result
    in vanishing gradients and halt learning. The above two equations ensure that
    the signal gain is precisely one.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 确保网络中所有层和梯度的方差在初始化时保持恒定，有助于防止神经网络中的梯度爆炸和梯度消失。如果增益*大于* 一，则会导致梯度爆炸和优化发散；如果增益*小于*
    一，则会导致梯度消失并停止学习。上述两个方程确保信号增益恰好为一。
- en: The motivation as well as the derivations in this paper are following the Xavier
    Glorot initialization⁽²⁾ paper published five years prior. While the previous
    work uses post-activation layers for constant variance in the forward signal,
    the He initialization proof uses pre-activation layers. Similarly, for the backward
    signal, He’s derivation uses post-activation layers instead of pre-activation
    layers in Glorot’s initialization. Given that these two proofs share some similarities,
    looking at both helps gain insights into why controlling for weights’ variance
    is so important in *any* neural network. (See “[Xavier Glorot Initialization in
    Neural Networks — Math Proof](https://medium.com/towards-data-science/xavier-glorot-initialization-in-neural-networks-math-proof-4682bf5c6ec3)”
    for more details)
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 本文的动机及推导基于五年前发表的 Xavier Glorot 初始化⁽²⁾ 论文。虽然前述工作使用了后激活层来保持前向信号的恒定方差，但 He 初始化证明使用了前激活层。同样地，对于反向信号，He
    的推导使用了后激活层，而非 Glorot 初始化中的前激活层。鉴于这两个证明有一些相似之处，比较这两者有助于理解为什么控制权重的方差在*任何*神经网络中如此重要。（更多细节见
    “[Xavier Glorot Initialization in Neural Networks — Math Proof](https://medium.com/towards-data-science/xavier-glorot-initialization-in-neural-networks-math-proof-4682bf5c6ec3)”）
- en: '[](/xavier-glorot-initialization-in-neural-networks-math-proof-4682bf5c6ec3?source=post_page-----73b9a0d845c4--------------------------------)
    [## Xavier Glorot Initialization in Neural Networks — Math Proof'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/xavier-glorot-initialization-in-neural-networks-math-proof-4682bf5c6ec3?source=post_page-----73b9a0d845c4--------------------------------)
    [## Xavier Glorot Initialization in Neural Networks — Math Proof'
- en: Detailed derivation for finding optimal initial distributions of weight matrices
    in deep learning layers with tanh…
  id: totrans-33
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 详细推导了在深度学习层中使用 tanh 激活函数时寻找权重矩阵的最优初始分布……
- en: towardsdatascience.com](/xavier-glorot-initialization-in-neural-networks-math-proof-4682bf5c6ec3?source=post_page-----73b9a0d845c4--------------------------------)
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/xavier-glorot-initialization-in-neural-networks-math-proof-4682bf5c6ec3?source=post_page-----73b9a0d845c4--------------------------------)
- en: 'Math Proof: Kaiming He Initialization'
  id: totrans-35
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数学证明：Kaiming He 初始化
- en: I. Forward Pass
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: I. 前向传播
- en: We are looking for *Wₖ* such that the variance of each subsequent pre-activation
    layer *y* is equal, *i.e.* *Var[yₖ] = Var[yₖ ₋* ₁*].*
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 我们正在寻找 *Wₖ* 使得每个后续前激活层 *y* 的方差相等，*即* *Var[yₖ] = Var[yₖ ₋* ₁*].*
- en: We know that *yₖ = xₖ Wₖ+ bₖ.*
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 我们知道 *yₖ = xₖ Wₖ+ bₖ*。
- en: For simplicity, we look at the *i-th* element of the pre-activation layer *yₖ*
    and apply the variance operator on both sides of the previous equation.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简化，我们查看前激活层的*i-th* 元素 *yₖ* 并对前面的方程两边应用方差算子。
- en: '![](../Images/16cb584b879d9662cbbfbfc8a3ac57bc.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/16cb584b879d9662cbbfbfc8a3ac57bc.png)'
- en: In the first step, we remove *bₖ* entirely, as following *Assumption 1* it is
    initialized at value zero. Additionally, we leverage the independence of *W* and
    *x* to transform the variance of the sum into a sum of variances, following *Var[X+Y]
    = Var[X] + Var[Y] with X* ⟂ *Y*.
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在第一步中，我们完全去除 *bₖ*，因为根据*假设1*，它初始化为零值。此外，我们利用 *W* 和 *x* 的独立性，将和的方差转换为方差之和，依据*Var[X+Y]
    = Var[X] + Var[Y]，其中 X* ⟂ *Y*。
- en: In the second step, as *W* and *x* are *i.i.d.,* each term in the sum is equal,
    hence the sum is simply a *nₖ* times repetition of *Var[xW]*.
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在第二步中，由于 *W* 和 *x* 是 *i.i.d.*，因此和中的每一项都是相等的，因此和仅仅是 *Var[xW]* 的 *nₖ* 次重复。
- en: In the third step, we follow the formula for *X* ⟂ *Y* which implies that *Var[XY]
    = E[X²]E[Y²] - E[X]²E[Y]²*. This allows us to separate *W* and *x* contributions
    to the pre-activation layer’s variance.
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在第三步中，我们遵循 *X* ⟂ *Y* 的公式，这意味着 *Var[XY] = E[X²]E[Y²] - E[X]²E[Y]²*。这使我们能够将 *W*
    和 *x* 对前激活层方差的贡献分开。
- en: '![](../Images/dd865a2fc54dec78238b3aa7a6ccab66.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/dd865a2fc54dec78238b3aa7a6ccab66.png)'
- en: In the fourth step, we leverage *Assumption 3* of zero expectation for weights
    and layers at initialization. This leaves us with a single term involving a squared
    expectation.
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在第四步中，我们利用了权重和层在初始化时的零期望的*假设3*。这使我们剩下一个涉及平方期望的单一项。
- en: In the fifth step, we transform the squared expectation into a variance since
    *Var[X] = E[( X - E[X])²] = E[X²]* if *X* has a zero mean. Now we can express
    the pre-activation layer’s variance as a separate product of layer and weight
    variance.
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在第五步中，我们将平方期望转换为方差，因为*Var[X] = E[( X - E[X])²] = E[X²]*，如果*X*的均值为零。现在我们可以将前激活层的方差表示为层方差和权重方差的单独乘积。
- en: Finally, in order to link *Var[yₖ] to Var[yₖ ₋* ₁*],* we express the squared
    expectation *E[xₖ²]* in terms of *Var[yₖ ₋* ₁*]* in the following steps using
    the *Law of the Unconscious Statistician*.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，为了将 *Var[yₖ] 与 Var[yₖ ₋* ₁*]* 关联起来，我们使用 *无意识统计学家定律* 将平方期望 *E[xₖ²]* 表达为 *Var[yₖ
    ₋* ₁*]*。
- en: '![](../Images/438ceff83f96e7fda4ee45850bfbeb9e.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/438ceff83f96e7fda4ee45850bfbeb9e.png)'
- en: The theorem states that we can formulate any expectation of the function of
    a random variable as an integral of its function and probability density *p*.
    As we know that *xₖ = max(0, yₖ ₋* ₁*)*, we can rewrite the squared expectation
    of *xₖ* as an integral on *ℝ* of *y*.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 定理指出，我们可以将随机变量函数的任何期望表示为其函数和概率密度 *p* 的积分。由于我们知道 *xₖ = max(0, yₖ ₋* ₁*)*，我们可以将
    *xₖ* 的平方期望重写为在 *ℝ* 上关于 *y* 的积分。
- en: '![](../Images/ff34a215683d295eb7d4e630f9029d68.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ff34a215683d295eb7d4e630f9029d68.png)'
- en: In the sixth step, we simplify the integral using that *y* is zero on ℝ⁻.
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在第六步中，我们利用 *y* 在 ℝ⁻ 上为零来简化积分。
- en: In the seventh step, we leverage the statistical property of *y* as a symmetric
    random variable, which hence has a symmetric density function *p*, and note that
    the entire integral’s term is an even function. Even functions are symmetric with
    respect to 0 on ℝ, which means that integrating from 0 to *a* is the same as from
    *-a* to 0\. We use this trick to reformulate back the integral as an integral
    over ℝ.
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在第七步中，我们利用 *y* 作为对称随机变量的统计特性，因此它具有对称的密度函数 *p*，并注意到整个积分项是偶函数。偶函数相对于 ℝ 上的 0 是对称的，这意味着从
    0 到 *a* 的积分与从 *-a* 到 0 的积分是相同的。我们使用这个技巧将积分重新表述为在 ℝ 上的积分。
- en: '![](../Images/f0613a7a42c13e34c3e7a65b014cffbb.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f0613a7a42c13e34c3e7a65b014cffbb.png)'
- en: In the ninth and tenth steps, we rewrite this integral as an integral of a function
    of a random variable. By applying the LOTUS — this time from right to left — we
    can change this integral to an expectation of the function over the random variable
    *y*. As a squared expectation of a zero mean variable, this is essentially a variance.
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在第九和第十步中，我们将这个积分重写为随机变量的函数的积分。通过应用 LOTUS——这次是从右到左——我们可以将此积分转换为关于随机变量 *y* 的函数的期望。作为零均值变量的平方期望，这本质上是方差。
- en: '![](../Images/8a86c6fac095d23f1346be8be0d7bfa2.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8a86c6fac095d23f1346be8be0d7bfa2.png)'
- en: We finally get to put it all together using the results of steps five and ten
    — the variance of a pre-activation layer is directly linked to its previous pre-activation
    variance as well as the variance of the layer’s weights. Since we require that
    *Var[yₖ] = Var[yₖ ₋* ₁*]*, it allows us to confirm that a layer’s weights variance
    *Var[Wₖ]* should be 2/*nₖ* .
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，我们将步骤五和步骤十的结果综合起来——前置激活层的方差与其前置激活方差以及该层权重的方差直接相关。由于我们要求 *Var[yₖ] = Var[yₖ
    ₋* ₁*]*，这使我们可以确认该层权重的方差 *Var[Wₖ]* 应该是 2/*nₖ*。
- en: 'In summary, here is again the whole derivation of the forward propagation reviewed
    in this section:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 总结一下，这里是本节回顾的前向传播的完整推导：
- en: '![](../Images/b3525ec2d5fdd6d851a28475d664d725.png)![](../Images/d09f218de73be6e53ae2a95eafb5b2b2.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b3525ec2d5fdd6d851a28475d664d725.png)![](../Images/d09f218de73be6e53ae2a95eafb5b2b2.png)'
- en: II. Backward Pass
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: II. 反向传播
- en: We are looking for *Wₖ* such that *Var[Δxₖ] = Var[Δxₖ ₊* ₁*].*
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 我们正在寻找 *Wₖ*，使得 *Var[Δxₖ] = Var[Δxₖ ₊* ₁*]*。
- en: Here*, xₖ ₊* ₁*= f (yₖ)* and *yₖ = xₖ Wₖ + bₖ.*
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里*， xₖ ₊* ₁*= f (yₖ)* 和 *yₖ = xₖ Wₖ + bₖ*。
- en: 'Before applying the variance operator, let us first calculate the partial derivatives
    of the loss *L* with respect to *x* and *y* : *Δxₖ* and *Δyₖ* .'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在应用方差运算符之前，让我们首先计算损失 *L* 相对于 *x* 和 *y* 的偏导数：*Δxₖ* 和 *Δyₖ*。
- en: '![](../Images/9c17c3bc893544ffa5202e2901d6cead.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9c17c3bc893544ffa5202e2901d6cead.png)'
- en: First, we use the chain rule and the fact that the derivative of a linear product
    is its linear coefficient — in this case, *Wₖ*.
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 首先，我们使用链式法则和线性乘积的导数是其线性系数的事实——在这种情况下是 *Wₖ*。
- en: Second, we leverage *Assumption 2* stating that gradients and weights are independent
    of each other. Using independence, the variance of the product becomes the product
    of variances, which is equal to zero since the weights are assumed to be initialized
    with zero means. Hence, the expectation of the gradient of *L w.r.t. x* is zero.
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 其次，我们利用 *假设 2* 说明梯度和权重彼此独立。利用独立性，积的方差等于方差的积，因为权重假设为零均值初始化。因此，*L w.r.t. x* 的梯度期望为零。
- en: Third, we use the chain rule to link *Δyₖ* and *Δxₖ ₊* ₁ as the partial derivative
    of *x* *w.r.t. y* is ReLU’s derivative taken in *y*.
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第三，我们使用链式法则将 *Δyₖ* 和 *Δxₖ ₊* ₁ 关联起来，因为 *x* 相对于 *y* 的偏导数是 ReLU 对 *y* 的导数。
- en: '![](../Images/9b88943311d1a917e2bc3bc18529d10c.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9b88943311d1a917e2bc3bc18529d10c.png)'
- en: 'Fourth, recalling the derivative of ReLU, we compute the expectation of *Δyₖ*
    using the previous equation. As *f’(x)* is split into two parts with an equal
    probability of *½*, we can write it as a sum of two terms: expectation over ℝ⁺
    and ℝ⁻, respectively. From previous calculations, we know that the expectation
    of *Δxₖ* is zero, and we can thus confirm that both gradients have a mean of 0.'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第四，回顾ReLU的导数，我们使用之前的方程计算*Δyₖ*的期望。由于*f’(x)*被分为两个等概率的部分*½*，我们可以将其写作两个项的和：分别对ℝ⁺和ℝ⁻的期望。从之前的计算中，我们知道*Δxₖ*的期望为零，因此可以确认两个梯度的均值均为0。
- en: '![](../Images/197febab4ff1f2ccdf31069750903908.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/197febab4ff1f2ccdf31069750903908.png)'
- en: Fifth, we use the same rule as before to write a squared expectation as a variance,
    here with *Δyₖ* .
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第五，我们使用之前相同的规则，将平方期望写作方差，这里是*Δyₖ*。
- en: Sixth, we leverage *Assumption 2* stating gradients are independent at initialization
    to split the variance of the two gradients *Δxₖ ₊* ₁ and *f’(yₖ).* Further simplification
    stems from *Assumption 3* and we can finally compute ReLU’s squared expectation
    given its even split between positive and negative intervals.
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第六，我们利用*假设2*指出梯度在初始化时是独立的，以此分离两个梯度*Δxₖ ₊* ₁和*f’(yₖ)*的方差。进一步简化源自*假设3*，我们最终可以计算ReLU的平方期望，因为它在正负区间之间是均匀分布的。
- en: '![](../Images/313e8009108a1814e9b693539fcf9548.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/313e8009108a1814e9b693539fcf9548.png)'
- en: Finally, using the gathered results from the above sections, and reapplying
    the assumption of *iid,* we conclude the result of the backpropagation pass to
    be similar to the forward pass, *i.e.* given *Var[Δxₖ] = Var[Δxₖ ₊* ₁*],* the
    variance of any layer’s weights *Var[Wₖ]* is equal to 2/*nₖ .*
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，利用上述各节中收集到的结果，并重新应用*iid*假设，我们得出反向传播的结果与正向传播类似，即给定*Var[Δxₖ] = Var[Δxₖ ₊* ₁*]*，任何一层的权重的方差*Var[Wₖ]*等于2/*nₖ*。
- en: 'To summarize, here is a reminder of the important step-by-step calculations
    included within this backward pass section:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 总结一下，这里是反向传播部分包含的重要逐步计算的提醒：
- en: '![](../Images/be426dc6afa5c2a15f929bc66627b32c.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/be426dc6afa5c2a15f929bc66627b32c.png)'
- en: III. Weight Distribution
  id: totrans-76
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: III. 权重分布
- en: 'In the two previous sections, we concluded the following for both backward
    and forward setups:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在前两节中，我们对正向和反向设置得出了以下结论：
- en: '![](../Images/0f745c22499c3eca5a1fafb02cc475cb.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0f745c22499c3eca5a1fafb02cc475cb.png)'
- en: It is interesting to note that this result is different from the Glorot initialization⁽²⁾,
    where the authors essentially have to average the *two* distinct results obtained
    in the forward and backward passes. Furthermore, we observe that the variance
    in the He method is doubled, which, intuitively, is due to the fact that ReLU’s
    zero negative section reduces variance by a factor of two.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是，这个结果与Glorot初始化方法不同，其中作者实际上必须对正向和反向传播中得到的*两个*不同结果进行平均。此外，我们观察到He方法中的方差是翻倍的，这直观上是因为ReLU的零负区域将方差减少了一半。
- en: Subsequently, knowing the variance of the distribution, we can now initialize
    the weights with either normal distribution *N(0, 𝜎²)* or uniform distribution
    *U(-a, a)*. Empirically, there is no evidence that one distribution is superior
    to the other, and it seems that the performance improvement comes down solely
    to the symmetry and scale properties of a chosen distribution. Furthermore, we
    do need to keep in mind *Assumption 3*, restricting the distribution choice to
    be symmetric and centered in 0.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 随后，了解了分布的方差后，我们可以使用正态分布*N(0, 𝜎²)*或均匀分布*U(-a, a)*来初始化权重。根据经验，没有证据表明一种分布优于另一种分布，似乎性能的提升完全归结为所选分布的对称性和尺度属性。此外，我们确实需要记住*假设3*，限制分布选择为对称且以0为中心。
- en: '**For Normal distribution *N(0, 𝜎²)***'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**对于正态分布 *N(0, 𝜎²)***'
- en: 'If *X ~ N(0, 𝜎²),* then *Var[X] = 𝜎²*, thus the variance and standard deviation
    of the weight matrix can be written as:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 如果*X ~ N(0, 𝜎²)*，则*Var[X] = 𝜎²*，因此权重矩阵的方差和标准差可以写作：
- en: '![](../Images/e1085f0b569de97721c8dd5bb2bc15f7.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e1085f0b569de97721c8dd5bb2bc15f7.png)'
- en: 'We can therefore conclude that *Wₖ* follows a normal distribution with coefficients:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 因此我们可以得出结论，*Wₖ*遵循以下系数的正态分布：
- en: '![](../Images/2ba793a077bfd84a7661bbde1ee61e1b.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2ba793a077bfd84a7661bbde1ee61e1b.png)'
- en: As a reminder, *nₖ* is the number of inputs of the layer *k*.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 作为提醒，*nₖ*是层*k*的输入数量。
- en: '**For Uniform distribution *U(-a, a)***'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**对于均匀分布 *U(-a, a)***'
- en: 'If *X ~ U(-a, a)*, then using the below formula of a variance for a random
    variable following a uniform distribution, we can find the bound *a*:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 如果*X ~ U(-a, a)*，则使用以下均匀分布随机变量方差的公式，我们可以找到界限 *a*：
- en: '![](../Images/2adb9197e433a65dc09424df786bfce6.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2adb9197e433a65dc09424df786bfce6.png)'
- en: 'Finally, we can conclude that *Wₖ* follows a uniform distribution with coefficients:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以得出结论，*Wₖ* 遵循具有系数的均匀分布：
- en: '![](../Images/a689c2c60b45301651c2fd13a4d62a25.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a689c2c60b45301651c2fd13a4d62a25.png)'
- en: Conclusion
  id: totrans-92
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: This article provides a step-by-step derivation of why *He initialization method*
    is optimal for neural networks that use ReLU activation functions, given the constraints
    on forward and backward passes to have constant variances.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 本文提供了为什么*He 初始化方法*对于使用 ReLU 激活函数的神经网络是最优的的逐步推导，前提是前向和反向传播的方差保持恒定。
- en: The methodology of this proof also extends to the broader family of linear rectifiers,
    like PReLU (discussed in (1) by He *et al.*) or Leaky ReLU (allowing for a minuscule
    gradient to flow in the negative interval). Similar optimal variance formulas
    can be derived for these variants of the ReLU activation function.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 该证明的方法论还扩展到线性激活函数的更广泛家族，如 PReLU（在 (1) 中由 He *等人* 讨论）或 Leaky ReLU（允许在负区间中有微小的梯度流动）。对于这些
    ReLU 激活函数的变体，也可以推导出类似的最优方差公式。
- en: '**Citations**'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '**引用**'
- en: '(1)[*Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet
    Classification*](https://arxiv.org/abs/1502.01852), He *et al.* (2015)'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: (1)[*深入探讨激活函数：超越人类水平的 ImageNet 分类表现*](https://arxiv.org/abs/1502.01852)，He *等人*
    (2015)
- en: (2)[*Understanding the difficulty of training deep feedforward neural networks*](https://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf),
    Glorot *et al.* (2010)
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: (2)[*理解训练深度前馈神经网络的难度*](https://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf)，Glorot
    *等人* (2010)
- en: '*Source: All the above equations and images are my own.*'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '*来源：以上所有方程和图像均为我个人制作。*'
