- en: Understand Policy Gradient by Building Cross Entropy from Scratch
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通过从零开始构建交叉熵来理解策略梯度
- en: 原文：[https://towardsdatascience.com/understand-policy-gradient-by-building-cross-entropy-from-scratch-75ca18b53e94?source=collection_archive---------2-----------------------#2023-06-11](https://towardsdatascience.com/understand-policy-gradient-by-building-cross-entropy-from-scratch-75ca18b53e94?source=collection_archive---------2-----------------------#2023-06-11)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/understand-policy-gradient-by-building-cross-entropy-from-scratch-75ca18b53e94?source=collection_archive---------2-----------------------#2023-06-11](https://towardsdatascience.com/understand-policy-gradient-by-building-cross-entropy-from-scratch-75ca18b53e94?source=collection_archive---------2-----------------------#2023-06-11)
- en: A unified view of how we train models
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们如何训练模型的统一视角
- en: '[](https://tonychenxyz.medium.com/?source=post_page-----75ca18b53e94--------------------------------)[![Tony
    Chen](../Images/37a65cea36bc332c8777c4f599a592f5.png)](https://tonychenxyz.medium.com/?source=post_page-----75ca18b53e94--------------------------------)[](https://towardsdatascience.com/?source=post_page-----75ca18b53e94--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----75ca18b53e94--------------------------------)
    [Tony Chen](https://tonychenxyz.medium.com/?source=post_page-----75ca18b53e94--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://tonychenxyz.medium.com/?source=post_page-----75ca18b53e94--------------------------------)[![Tony
    Chen](../Images/37a65cea36bc332c8777c4f599a592f5.png)](https://tonychenxyz.medium.com/?source=post_page-----75ca18b53e94--------------------------------)[](https://towardsdatascience.com/?source=post_page-----75ca18b53e94--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----75ca18b53e94--------------------------------)
    [Tony Chen](https://tonychenxyz.medium.com/?source=post_page-----75ca18b53e94--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F1c86c3ef2039&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstand-policy-gradient-by-building-cross-entropy-from-scratch-75ca18b53e94&user=Tony+Chen&userId=1c86c3ef2039&source=post_page-1c86c3ef2039----75ca18b53e94---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----75ca18b53e94--------------------------------)
    ·16 min read·Jun 11, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F75ca18b53e94&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstand-policy-gradient-by-building-cross-entropy-from-scratch-75ca18b53e94&user=Tony+Chen&userId=1c86c3ef2039&source=-----75ca18b53e94---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F1c86c3ef2039&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstand-policy-gradient-by-building-cross-entropy-from-scratch-75ca18b53e94&user=Tony+Chen&userId=1c86c3ef2039&source=post_page-1c86c3ef2039----75ca18b53e94---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----75ca18b53e94--------------------------------)
    · 16 分钟阅读 · 2023年6月11日 [](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F75ca18b53e94&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstand-policy-gradient-by-building-cross-entropy-from-scratch-75ca18b53e94&user=Tony+Chen&userId=1c86c3ef2039&source=-----75ca18b53e94---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F75ca18b53e94&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstand-policy-gradient-by-building-cross-entropy-from-scratch-75ca18b53e94&source=-----75ca18b53e94---------------------bookmark_footer-----------)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F75ca18b53e94&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstand-policy-gradient-by-building-cross-entropy-from-scratch-75ca18b53e94&source=-----75ca18b53e94---------------------bookmark_footer-----------)'
- en: '**Reinforcement learning (RL)** can do amazing stuff. Most recently, [ChatGPT
    is fine-tuned on human feedback with PPO](https://www.assemblyai.com/blog/how-chatgpt-actually-works/),
    a variant of a class of reinforcement learning algorithm called **Policy Gradient
    (PG)**. Understanding RL, especially policy gradient, could be non-trivial, particularly
    if you like grasping intuitions like I do. In this post, I will walk through a
    thread of thoughts that really helped me understand PG by starting from more familiar
    supervised learning setting.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '**强化学习 (RL)** 可以做出令人惊叹的事情。最近，[ChatGPT 通过 PPO 进行微调](https://www.assemblyai.com/blog/how-chatgpt-actually-works/)，PPO
    是一种叫做 **策略梯度 (PG)** 的强化学习算法的变种。理解 RL，特别是策略梯度，可能并不简单，特别是如果你像我一样喜欢把握直觉的话。在这篇文章中，我将探讨一系列思路，这些思路确实帮助我从更熟悉的监督学习环境出发，深入理解
    PG。'
- en: TL;DR
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: We will start by designing a simple supervised training procedure of a binary
    classification robot by rewarding it +1 for correct answers
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们将从设计一个简单的监督训练程序开始，通过奖励+1来对二分类机器人进行正确答案的训练
- en: We will formulate the objective for the procedure
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们将为该过程制定目标
- en: We will derive the gradient ascent formulation for the procedure (which will
    turn out to be the same procedure as gradient descent with Cross Entropy)
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们将推导出该过程的梯度上升公式（这将与使用交叉熵的梯度下降过程相同）
- en: We will compare our procedure to RL settings and relate our gradient ascent
    to policy gradient
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们将把我们的过程与RL设置进行比较，并将我们的梯度上升与策略梯度联系起来
- en: '**Who should read this?**'
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**谁应该阅读这个？**'
- en: My goal is to provide a friendly, intuitive aid to understand PG. It is helpful
    if you have a general understanding of RL problem setting and know on high level
    what PG is.
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我的目标是提供一种友好且直观的方式来理解PG。如果你对RL问题设置有一个大致了解，并且知道PG的高级概念，将会很有帮助。
- en: I hope to help you better understand the relationship between RL with PG and
    supervised ML. So it is helpful if you know about how to train a supervised ML
    algorithm with cross entropy loss function.
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我希望帮助你更好地理解RL与PG以及监督ML之间的关系。因此，如果你了解如何用交叉熵损失函数训练一个监督ML算法，将会非常有帮助。
- en: Why this post?
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为什么写这篇文章？
- en: Policy Gradient
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 策略梯度
- en: In an RL problem, an **agent** interacts with an **environment** to learn a
    **policy.** The policy tells the agent what to do in different **states** to maximize
    **reward**.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在RL问题中，**代理**与**环境**互动以学习**策略**。策略告诉代理在不同**状态**下该做什么以最大化**奖励**。
- en: '![](../Images/c713f4a5b047c8393283f44de994e836.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c713f4a5b047c8393283f44de994e836.png)'
- en: Image by Author
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图像
- en: The idea of PG seems straightforward.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: PG的想法似乎很简单明了。
- en: The **policy** that guides agent behavior at time *t* is *π_θ(a_t|s_t)*.
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 指导时间*t*上代理行为的**策略**是*π_θ(a_t|s_t)*。
- en: This is **some kind of function** (often a neural network) with parameter *θ*.
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这是一种**函数**（通常是神经网络），具有参数*θ*。
- en: It **takes in information of states** *s_t* and spits out a **probability distribution
    of action to take** *a_t*.
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它**接收状态信息** *s_t* 并输出一个**采取行动的概率分布** *a_t*。
- en: Then it **receives a reward** *r(s_t, a_t)*.
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然后它**接收奖励** *r(s_t, a_t)*。
- en: When we have history of many of such cycles of action and reward, we can update
    parameter *θ* in order to maximize expected reward produced by actions generated
    from *π_θ*.
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当我们拥有许多这样的动作和奖励周期的历史时，我们可以更新参数*θ*以最大化由*π_θ*生成的动作所带来的预期奖励。
- en: How do we do the update? Through…**gradient**! We update the model producing
    *π_θ* with the following gradient
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何进行更新？通过…**梯度**！我们通过以下梯度更新生成*π_θ*的模型
- en: '![](../Images/33d815377e9ba0a3076a657fab933349.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/33d815377e9ba0a3076a657fab933349.png)'
- en: Something Feels Off
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 有些东西感觉不对劲
- en: This looks very familiar. When we train a neural network model in good old supervised
    learning, we also update the model parameters by doing the operation in the second
    line aka gradient descent (technically in PG case, since we are maximizing an
    objective, it’s gradient ascent).
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 这看起来非常熟悉。当我们在传统的监督学习中训练神经网络模型时，我们也通过执行第二行操作即梯度下降来更新模型参数（在PG情况下，技术上是梯度上升，因为我们在最大化目标）。
- en: 'But this also feels very different. If you look at its [derivation process](https://www.youtube.com/watch?v=GKoKNYaBvM0&list=PL_iWQOsE6TfXxKgI1GgyV1B_Xa0DxE5eH&index=23),
    you can see it **takes a bit of effort** to derive this equation. That’s very
    different from the more intuitive way of how we do supervised learning: feed an
    input into neural network, get an output, compare it with target and calculate
    a loss function, hit backprop button, and we are done!'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 但这也感觉非常不同。如果你查看它的[推导过程](https://www.youtube.com/watch?v=GKoKNYaBvM0&list=PL_iWQOsE6TfXxKgI1GgyV1B_Xa0DxE5eH&index=23)，你会发现推导这个方程**需要一点努力**。这与我们在监督学习中更直观的做法非常不同：将输入提供给神经网络，得到输出，与目标进行比较并计算损失函数，点击反向传播按钮，就完成了！
- en: Also, for me, the **log term** always seems coming out of nowhere. Although
    the same online course in the link above walks through how do we get to the log
    term, the process seems to just be a bunch of math that’s correct but lacking
    motivation.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 对我来说，**对数项**总是似乎突然出现。尽管上述链接中的同一在线课程讲解了如何得到对数项，但过程似乎只是一堆正确但缺乏动机的数学。
- en: What is exactly the difference from supervised learning? It turns out diving
    into this question provides a great way of understanding PG. Furthermore, it is
    a good reminder for the **nature of some familiar things in supervised learning**
    that we do everyday.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 从监督学习中具体的区别是什么？深入探讨这个问题可以很好地理解策略梯度。此外，它也是对我们每天做的**一些熟悉的监督学习本质**的良好提醒。
- en: Let’s Build Cross Entropy from Scratch
  id: totrans-35
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从头开始构建交叉熵
- en: 'If we are thrown with some loss functions used in supervised ML, they would
    “make sense” immediately. But it takes some more efforts to understand where it
    comes from. For example, the good old mean square loss intuitively make sense:
    it just minimizes the distance between the prediction and target. But there’re
    so many distance metrics; why square distance? Y[ou have to look deeper to see
    that mean square error is the byproduct of doing maximum likelihood and assuming
    the underlying population distribution to be normal](https://tivadardanka.com/blog/mean-squared-error-explained).'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们用一些在监督学习中使用的损失函数来分析，它们会立即“显得合理”。但要理解它们的来源则需要更多的努力。例如，经典的均方误差直观上很合理：它只是最小化预测与目标之间的距离。但有这么多距离度量，为什么选择平方距离？[你必须深入了解均方误差是做最大似然估计并假设基础总体分布为正态分布的副产品](https://tivadardanka.com/blog/mean-squared-error-explained)。
- en: 'Same with another good old loss function that we use everyday: Cross Entropy.
    While there are many [good interpretations of what Cross Entropy does](/entropy-cross-entropy-and-kl-divergence-17138ffab87b#:~:text=KL%20divergence%20is%20the%20relative,as%20the%20actual%20probability%20distribution.),
    **let’s try to build it from the most rudimentary manner**.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 同样地，我们日常使用的另一个经典损失函数是交叉熵。虽然有很多[关于交叉熵的良好解释](/entropy-cross-entropy-and-kl-divergence-17138ffab87b#:~:text=KL%20divergence%20is%20the%20relative,as%20the%20actual%20probability%20distribution.)，**让我们尝试从最基本的方式构建它**。
- en: Let’s Train a Classification Bot!
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 让我们训练一个分类机器人！
- en: 'Say you want to train a robot to classify dog and cat image. It’s intuitive
    To train it by rewarding the right answer and punish it (or not reward it) for
    wrong answers. Here’s how it’s done:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你想训练一个机器人来分类狗和猫的图像。直观上，通过奖励正确答案并惩罚（或不奖励）错误答案来训练它是合理的。具体方法如下：
- en: You **give the robot an image**. Let’s call it *s.* This image is sampled from
    a population distribution *D_s*
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你**给机器人一张图片**。我们称之为*s*。这张图片是从总体分布*D_s*中采样的。
- en: '![](../Images/90151735fcf9c358929bba56a796f060.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/90151735fcf9c358929bba56a796f060.png)'
- en: 'Dog Image Source: Unsplash; Other Parts By Author'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 狗图像来源：Unsplash；其他部分由作者提供
- en: The robot will **give you an answer** if it think it’s a dog image (action *a_dog*)
    or it’s a cat image (action *a_cat*)*.*
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果机器人认为这是狗的图像（动作*a_dog*）或这是猫的图像（动作*a_cat*），它将**给你一个答案**。
- en: 'The robot has **its own prediction** given the image about the probability
    of the image being dog or cat: *π_θ(a|s) = (a_dog, a_cat)*. For example, *π_θ(a|s)
    = (0.9, 0.1)* means it thinks there’s 0.9 probability it’s a dog, and 0.1 probably
    it’s a cat.'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器人根据图像有**自己的预测**，即图像是狗还是猫的概率：*π_θ(a|s) = (a_dog, a_cat)*。例如，*π_θ(a|s) = (0.9,
    0.1)*意味着它认为有0.9的概率是狗，0.1的概率是猫。
- en: '![](../Images/31518402fb49d27bc9d2ced9334b2d01.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/31518402fb49d27bc9d2ced9334b2d01.png)'
- en: 'Dog Image Source: Unsplash; Other Parts By Author'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 狗图像来源：Unsplash；其他部分由作者提供
- en: 'But every time the robot will only give you a solid answer. It either says
    “it’s a dog” (*a_dog*) or “it’s a cat” *(a_cat)*. Every time **it gives you a
    response**, the responses (actions) are **randomly sampled from distribution**
    produced by *π_θ(a|s)*: *a = (a_dog, a_cat) ~ π_θ(a|s)*.'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 但每次机器人只会给你一个明确的答案。它要么说“这是狗” (*a_dog*)，要么说“这是猫” *(a_cat)*。每次**它给你一个回应**时，回应（动作）是**从分布中随机采样**得到的，由*π_θ(a|s)*产生：*a
    = (a_dog, a_cat) ~ π_θ(a|s)*。
- en: '![](../Images/89e8436eefd10f6faca3a83c91f37826.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/89e8436eefd10f6faca3a83c91f37826.png)'
- en: 'Dog Image Source: Unsplash; Other Parts By Author'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 狗图像来源：Unsplash；其他部分由作者提供
- en: Then you will **reward** the robot (maybe give it a treat?) with reward of 1
    when robot answers correctly. (*r(s,a) = 1*) There’s **no reward** (0 reward)
    when the answer is wrong. (*r(s,a) = 0*)
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当机器人正确回答时，你将**奖励**它（可能给它一个小奖励？），奖励值为1。（*r(s,a) = 1*）。当回答错误时，则**没有奖励**（0奖励）。(*r(s,a)
    = 0*)
- en: '![](../Images/32a9c703d976a3082f0b2b7452894982.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/32a9c703d976a3082f0b2b7452894982.png)'
- en: 'Dog Image Source: Unsplash; Other Parts By Author'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 狗图像来源：Unsplash；其他部分由作者提供
- en: '![](../Images/cf7ff12b91caebf06f6324d5860acff4.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/cf7ff12b91caebf06f6324d5860acff4.png)'
- en: 'Cat Image Source: Unsplash; Other Parts By Author'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 猫图像来源：Unsplash；其他部分由作者提供
- en: This process is what I had in mind when first learning about supervised ML.
    Reward it when it’s correct. Punish it (or simply no reward in this training process
    we designed) for it’s wrong. Probably the most intuitive way to train something.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我在第一次学习监督学习时想到的过程。当它正确时给予奖励。当它错误时（或在我们设计的训练过程中没有奖励）给予惩罚。这可能是训练某物最直观的方式。
- en: Maximizing Objective
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 最大化目标
- en: 'What’s our objective for the robot? We want its response to be correct as often
    as possible. More precisely, we want to find optimal parameter *θ**, that produces
    a *π_θ(a|s),* suchthat over all possible occurrences of *s* (sampled from image
    population distribution *D_s*)and *a* (sampled from distribution given *s* produced
    by model *π_θ(a|s)*), we get **maximum average reward weighted by probability**
    of each occurence of *(s,a)*:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的目标是什么？我们希望它的响应尽可能正确。更准确地说，我们希望找到最优参数*θ*，使得生成的*π_θ(a|s)*，在所有可能的*s*（从图像总体分布*D_s*中采样）和*a*（从由模型*π_θ(a|s)*生成的分布中采样）中，能够获得**每对*(s,a)*出现的概率加权的最大平均奖励**：
- en: '![](../Images/51cddd132910509e631770148af36d81.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/51cddd132910509e631770148af36d81.png)'
- en: In other words, we are **maximizing** the objective function *J(θ)* defined
    as
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，我们在**最大化**定义为
- en: '![](../Images/68e8b3d62384b11eb799409224934b10.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/68e8b3d62384b11eb799409224934b10.png)'
- en: Gradient of Objective
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 目标的梯度
- en: Now we have an objective function, we could probably try to maximize it through…gradient
    ascent! Namely, we can optimize the function by iteratively do
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了一个目标函数，我们可以尝试通过…梯度上升来最大化它！也就是说，我们可以通过迭代进行
- en: '![](../Images/31073f9fbb212dcb9a46f8b235dde998.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/31073f9fbb212dcb9a46f8b235dde998.png)'
- en: But how should we calculate the gradient, i.e. derivative of *J* respect to
    *θ*? It’s a bit tricky in this case since
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 但我们应如何计算梯度，即*J*对*θ*的导数？这在这种情况下有点棘手，因为
- en: The function that we want to take derivative of is an **expectation**.
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们希望对其求导的函数是一个**期望**。
- en: If the expectation is not over a distribution dependent on *θ,* then by linearity
    of expectation, we can just take the derivative of what’s inside of expectation
    and leave expectation there. However, in this case,tThe expectation is over *(s,a)
    ~ (D_s, π_θ(a|s)),* which is dependent *θ*. So the derivative is not obvious.
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果期望不是关于依赖于*θ*的分布，那么通过期望的线性性，我们可以直接对期望内部的内容进行求导，并将期望保留在那里。然而，在这种情况下，期望是关于*(s,a)
    ~ (D_s, π_θ(a|s))*的，这依赖于*θ*。因此，导数并不明显。
- en: Another way of thinking about this is that *J*(*θ*) changes value as the frequency
    changes for how often we sample *(s,a)* from a distribution **partially determined
    by *θ****.* We want more frequent occurrences of *s=dog image* and *a=a_dog* (similar
    pair for cat). How can we capture **changes of *θ* towards this direction** when
    we do gradient ascent?
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 另一种思考方式是，*J*(*θ*)的值随着我们从由**部分由*θ*决定的分布**中采样*(s,a)*的频率变化而变化。我们希望更频繁地出现*s=dog
    image*和*a=a_dog*（猫的类似对）。当我们进行梯度上升时，我们如何捕捉**向这个方向变化的*θ***？
- en: In addition, **ideally, we want gradient to be in the form**
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，**理想情况下，我们希望梯度呈现以下形式**
- en: '![](../Images/df2aff2595ad8db90cba253e1fd6529d.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/df2aff2595ad8db90cba253e1fd6529d.png)'
- en: 'This is because you train the bot from samples of interactions between robot
    and you. Every sample consists of a triplet of *(s,a,r)*. We can therefore approximate
    this gradient by taking average of *f(θ,s,a,r)* with *N* samples that we collect
    (by law of large number, i.e. do Stochastic Gradient Ascent):'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 这是因为你通过机器人与您的交互样本来训练机器人。每个样本包含一个*(s,a,r)*三元组。因此，我们可以通过对收集到的*N*个样本进行平均来近似这个梯度（根据大数法则，即进行随机梯度上升）：
- en: '![](../Images/20b0441122d80c37d43bb51c281ffe07.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/20b0441122d80c37d43bb51c281ffe07.png)'
- en: We can then do gradient ascent by doing
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们可以通过进行梯度上升来进行优化
- en: '![](../Images/b7d2290df9fb82d565f032c8fadfb1e5.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b7d2290df9fb82d565f032c8fadfb1e5.png)'
- en: Now let’s find *f.*
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们找到*f*。
- en: Finding Gradient
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 寻找梯度
- en: To recap, we want to start from (1) to get (2) for some *f(θ,s,a,r)*.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 总结一下，我们希望从（1）开始，得到（2），对于某个*f(θ,s,a,r)*。
- en: '![](../Images/be0cf74430c73014af3e3081d153a191.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/be0cf74430c73014af3e3081d153a191.png)'
- en: 'Let’s first rewrite (1) with the **definition of expectation**:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们用**期望的定义**重写（1）：
- en: '![](../Images/7acfd07aa0eb750ddb7697d47f03b45d.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7acfd07aa0eb750ddb7697d47f03b45d.png)'
- en: This is basically the integration of reward weighted by probability over all
    possible *(s,a)* pair.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 这基本上是对所有可能的*(s,a)*对的奖励按概率加权的积分。
- en: Now, what’s exactly the joint probability *P(s,a)* for one pair of *(s,a)* to
    appear? We can decompose it into the probability of image sample (*s*) appearing,
    and the probability that the robot randomly select action *a*.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，一个*(s,a)*对的联合概率*P(s,a)*究竟是多少？我们可以将其分解为图像样本（*s*）出现的概率和机器人随机选择动作*a*的概率。
- en: '![](../Images/bc1e000dca4f52ae85ddd28428ab0330.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bc1e000dca4f52ae85ddd28428ab0330.png)'
- en: Since the robot randomly select action *a* from the robot’s internal model of
    prediction *π_θ(a|s)*, we have
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 由于机器人从其内部预测模型 *π_θ(a|s)* 中随机选择动作 *a*，我们有
- en: '![](../Images/3ae5aed9db62c8211b8a17c33f7a77a7.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3ae5aed9db62c8211b8a17c33f7a77a7.png)'
- en: Out of all terms inside of the parentheses, only *π_θ(a|s)* is dependent on
    *θ*. The other terms are all constant. So we can **move the gradient operation
    inside of the integral sign** next to this term and get
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在括号内的所有项中，只有 *π_θ(a|s)* 依赖于 *θ*。其他项都是常数。因此，我们可以**将梯度操作移动到积分符号内**，并得到
- en: '![](../Images/4947001861b23bf63fb5bc7e2581012c.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4947001861b23bf63fb5bc7e2581012c.png)'
- en: Note that we can also write the following. Nothing big here. Just multiplying
    the original lefthand side by 1 written as a fraction and rearranging the terms.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们也可以写出以下内容。这里没什么大不了的。只是将原始左边的内容乘以以分数形式写出的1，并调整项。
- en: '![](../Images/6052d91540c72ca207601583ea3cd79b.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6052d91540c72ca207601583ea3cd79b.png)'
- en: Replacing it back, and rearranging a bit, we get
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 替换回去，并稍微调整一下，我们得到
- en: '![](../Images/e83df6f3f4d826ed91a7760403e76b72.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e83df6f3f4d826ed91a7760403e76b72.png)'
- en: '*P(s)π_θ(a|s)* looks familiar. It’s *P(s,a)* that we decomposed earlier! Putting
    it back, we have'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '*P(s)π_θ(a|s)* 看起来很熟悉。这正是我们之前分解的 *P(s,a)*！将其放回去，我们得到'
- en: '![](../Images/b5532069617ea101e52c2021c02c0e9c.png)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b5532069617ea101e52c2021c02c0e9c.png)'
- en: Now we have an integral and *P(s,a)*, we can…fit it back into **the definition
    of expectation**!
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有一个积分和 *P(s,a)*，我们可以…将其适配回**期望的定义**！
- en: '![](../Images/b3bb809839bdcaf9aafc99d278fe540a.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b3bb809839bdcaf9aafc99d278fe540a.png)'
- en: Which is exactly The form we want to get in (2), where *f* is the terms inside
    of the bracket!
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 这正是我们在（2）中想要得到的形式，其中 *f* 是括号内的项！
- en: You may have wondered why we rewrite gradient of *π_θ(a|s)* in the clunky fraction
    earlier? The idea was to create a *π_θ(a|s)* term (which we lost earlier by taking
    derivative of it), so we could **produce a *P(s,a)* term again**, and turn the
    integration back into an expectation!
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会想，为什么我们在之前的繁琐分数中重写了*π_θ(a|s)*的梯度？其目的是创建一个*π_θ(a|s)*项（我们在求导时丢失了它），以便我们可以**再次生成一个
    *P(s,a)* 项**，并将积分重新转化为期望！
- en: Building Cross Entropy
  id: totrans-97
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 构建交叉熵
- en: Now it’s magic time.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是魔法时刻。
- en: '![](../Images/a5366bbd4ec40454be4bf04c966b6033.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a5366bbd4ec40454be4bf04c966b6033.png)'
- en: 'Don’t believe me? Work from right hand side to left hand side with chain rule.
    ([Optional] Side thoughts: So if you are also confused about the motivation of
    the log term in policy gradient formula, it is a side product of simplifying a
    clunky equation we get, with the intention of extracting a *π_θ(a|s)* term to
    transform things back to expectation.)'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 不相信我？使用链式法则从右手边到左手边进行工作。（[可选] 旁注：如果你对策略梯度公式中对数项的动机感到困惑，这实际上是简化我们得到的繁琐方程的副产品，旨在提取一个
    *π_θ(a|s)* 项，将事物转回期望。）
- en: 'So we can simplify gradient of *J(θ)* a bit:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们可以稍微简化*J(θ)*的梯度：
- en: '![](../Images/a9e1880f242614f1079f6fb15b8c37fa.png)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a9e1880f242614f1079f6fb15b8c37fa.png)'
- en: So each time we have a batch of *(s,a)* as samples, we can do gradient ascent
    by
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 所以每次我们有一批*(s,a)*作为样本时，可以通过
- en: '![](../Images/1640cda6d2fd5a21a65a02d0a6e8afd5.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1640cda6d2fd5a21a65a02d0a6e8afd5.png)'
- en: To bring it to a more familiar form, Moving the gradient sign outside of summation,
    we have
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 为了将其转化为更熟悉的形式，将梯度符号移到求和外部，我们有
- en: '![](../Images/18707025a5e1fd5d9ff9c696371f86ad.png)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/18707025a5e1fd5d9ff9c696371f86ad.png)'
- en: We will also invert the sign by doing
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还会通过进行以下操作来反转符号
- en: '![](../Images/a709db8808ec96d0725e5a23dcbc114c.png)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a709db8808ec96d0725e5a23dcbc114c.png)'
- en: Does it ring a bell? Let’s compare it with what we do when doing **gradient
    descent on cross entropy loss**.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 这让你想起什么吗？让我们将其与**在交叉熵损失上进行梯度下降**时所做的事情进行比较。
- en: Recall that cross entropy loss is
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，交叉熵损失是
- en: '![](../Images/46bc5ce54fb4dc9bc83393022d14bef5.png)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/46bc5ce54fb4dc9bc83393022d14bef5.png)'
- en: where **y_i** is true label, a one hot vector (*y_i_1, y_i_2*) that describes
    whether an image is cat and dog (either (0,1) or (1,0)). **y_hat_i** is prediction
    from a model, a vector (*y_hat_i_1, y_hat_i_2*) where the two entries some up
    two one.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 其中**y_i**是真实标签，是一个描述图像是猫还是狗的独热向量（*y_i_1, y_i_2*，要么是(0,1)要么是(1,0)）。**y_hat_i**是模型的预测，是一个向量（*y_hat_i_1,
    y_hat_i_2*），其中两个条目的和为1。
- en: 'When we do **gradient descent** on this loss function, we calculate the cross
    entropy loss function for the batch, and hit the backprop button:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们对这个损失函数进行**梯度下降**时，我们计算批次的交叉熵损失函数，并点击反向传播按钮：
- en: '![](../Images/8f7faf6a722147b90275b2e04cdc4556.png)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8f7faf6a722147b90275b2e04cdc4556.png)'
- en: The differences between this expression and the gradient ascent express we derived
    earlier are
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 这个表达式与我们之前推导出的梯度上升表达式之间的区别是
- en: '![](../Images/847bf691725e8e3dad2692bc2c512639.png)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/847bf691725e8e3dad2692bc2c512639.png)'
- en: 'To bring the relationship into words, it basically means: on sample *x_i,*
    y_i'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 用语言描述，就是：在样本*x_i*上，y_i
- en: The model **makes prediction** (*y_hat_i_1, y_hat_i_2*) given *x_i*
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型**做出预测**（*y_hat_i_1, y_hat_i_2*）给定*x_i*
- en: The model **randomly samples a response** from the predicted distribution
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型**从预测分布中随机采样响应**
- en: We **reward** response 1 with *y_i_1* and response 2 with *y_i_2*.
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们**奖励**响应1的*y_i_1*，并且对响应2的*y_i_2*进行奖励。
- en: Since when the label is class 1, *y_i_1 = 1, y_i_2 = 0*, we **reward the model
    with 1** when model correctly responds 1 and there’s **0 reward** when it incorrectly
    responds 0\. Similar with class 2.
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于当标签为类别1时，*y_i_1 = 1, y_i_2 = 0*，我们在模型正确响应1时**奖励模型1分**，而在模型错误响应0时**没有奖励**。类别2的情况也是如此。
- en: And that’s exactly what we’ve been doing!
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 这正是我们一直在做的事情！
- en: So to recap,
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 所以总结一下，
- en: We **designed a simple training** setting where we **reward** **robot** with
    1 point when it answers correctly and 0 point when it answers incorrectly
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们**设计了一个简单的训练**设置，在这个设置中，我们**奖励** **机器人**当其正确回答时得1分，当其回答错误时得0分。
- en: We summerize what we want to achieve in an **objective function** that describes
    reward robot gets weighted by chances of its response
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们总结了我们希望在**目标函数**中实现的内容，该目标函数描述了机器人根据其响应的机会加权所获得的奖励。
- en: We find the gradient descent procedure to **maximize this objective function**
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们找到梯度下降过程以**最大化这个目标函数**
- en: And we get… the exact procedure that we use when **training a model by calculating
    Cross Entropy loss first and backproping through it**!
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然后我们得到……我们在**通过计算交叉熵损失然后进行反向传播训练模型**时使用的确切过程！
- en: Coming Back to Reinforcement Learning
  id: totrans-128
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 回到强化学习
- en: Now let’s turn our focus back to reinforcement learning setting. What are the
    differences between RL and the supervised ML setting.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们把焦点重新放回到强化学习设置上。RL与监督学习设置之间的区别是什么？
- en: Multiple Timesteps
  id: totrans-130
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 多个时间步长
- en: The first difference is that RL usually involves in **multiple states and multiple
    episodes**. In our setting, the robot starts with input of the image, i.e. state
    *s.* After the robot gives you back an answer based on its prediction and collects
    the reward, the interactions between the robot and you are done.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个区别是RL通常涉及**多个状态和多个回合**。在我们的设置中，机器人从图像输入开始，即状态*s*。在机器人基于预测给出答案并收集奖励后，机器人与您的互动就结束了。
- en: In contrary, in RL problems, agents often interact with environment in **multiple
    episodes**, and it might transition to other states after the initial state.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，在RL（强化学习）问题中，智能体通常在**多个回合**中与环境互动，且在初始状态后可能过渡到其他状态。
- en: The objective function then becomes
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 目标函数变为
- en: '![](../Images/0db0ebac8f528b86c6f438801036b294.png)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0db0ebac8f528b86c6f438801036b294.png)'
- en: Putting in words, we maximize **average sum of rewards** **of all timesteps**
    **over all possible sequence (trajectory) of states and actions**, **weighted
    by the probability of each trajectory** occurring when actions are decided by
    parameter *θ*.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 用语言描述，我们最大化**所有时间步长**的**平均奖励总和**，**对所有可能的状态和动作序列（轨迹）加权**，**加权由每个轨迹发生的概率决定**，当动作由参数*θ*决定时。
- en: Notes that *p_θ* is the joint distribution of a sequence of states and actions
    when actions are decided by agent’s model parameter *θ*. At each time step, the
    agent’s action is decided by *π_θ(a_t|s_t)*, where *π_θ* is a model parameterized
    by *θ*. *p_θ* is a **high level abstraction** of how probable it is for a sequence
    of states and actions to occur when the agent makes decision based on *π_θ* (i.e.
    *p_θ* is a placeholder for theoretically how often the agent takes on the trajectory.
    On the other hand, *π_θ(a|s)* is the probability that the agent will take an action
    at a specific timestep. We don’t actually easily know the value *p_θ*, so later
    we will rewrite it with model output *π_θ(a|s)* that we actually know).
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，*p_θ*是一个状态和动作序列的联合分布，当动作由代理的模型参数*θ*决定时。在每个时间步，代理的动作由*π_θ(a_t|s_t)*决定，其中*π_θ*是一个以*θ*为参数的模型。*p_θ*是一个**高级抽象**，表示当代理根据*π_θ*做出决策时，状态和动作序列发生的概率（即*p_θ*是理论上代理在轨迹上采取行动的频率的占位符。另一方面，*π_θ(a|s)*是代理在特定时间步采取某个动作的概率。我们实际上不容易知道*p_θ*的值，因此稍后我们将用实际知道的模型输出*π_θ(a|s)*来重写它）。
- en: 'Let’s compare it with the objective we had earlier:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们与之前的目标进行比较：
- en: '![](../Images/68e8b3d62384b11eb799409224934b10.png)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/68e8b3d62384b11eb799409224934b10.png)'
- en: The **main differences** are
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '**主要区别**如下：'
- en: We calculate expectation over a sequence of *s* and *a* instead of just one
    pair.
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们计算一个* s *和*a*序列上的期望，而不是仅仅一个对。
- en: We maximize the sum of rewards from all timesteps in the trajectory instead
    of only the one-timestep reward from the image and answer.
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们最大化轨迹中所有时间步的奖励总和，而不仅仅是来自图像和回答的单一时间步奖励。
- en: Comparing the Gradient Formula
  id: totrans-142
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 比较梯度公式：
- en: We can do the similar manipulations to this objective to derive the gradient
    that we can use to update *θ* at every timestep.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以对这个目标做类似的操作，推导出我们可以在每个时间步更新*θ*的梯度。
- en: To recap, our goal is to find gradient of *J(θ)* in the following form for some
    *f*
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 回顾一下，我们的目标是以以下形式找到某些*f*的*J(θ)*的梯度。
- en: '![](../Images/51c03df091a50c12a0c49fd25857a142.png)'
  id: totrans-145
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/51c03df091a50c12a0c49fd25857a142.png)'
- en: 'When we obtain a batch of sampled sequence of *s_1, a_1, r_1, … s_T, a_T, r_T*,
    we can then update *θ* via Stochastic Gradient Ascent:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们获得一批样本序列*s_1, a_1, r_1, … s_T, a_T, r_T*时，我们可以通过随机梯度上升更新*θ*：
- en: '![](../Images/dcf0c87b185cef2d80e89fe317052566.png)'
  id: totrans-147
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/dcf0c87b185cef2d80e89fe317052566.png)'
- en: To simplify things, let’s denote the sequence of state and with one variable
    *τ*.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简化，我们将状态序列记作一个变量*τ*。
- en: '![](../Images/b5550a0ddf26d2a80114c2964221b459.png)'
  id: totrans-149
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b5550a0ddf26d2a80114c2964221b459.png)'
- en: So we hope to maximize the following objective function
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们希望最大化以下目标函数：
- en: '![](../Images/3845823a7665d560b48d15d159a05a05.png)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3845823a7665d560b48d15d159a05a05.png)'
- en: 'We can do the similar manipulations that we did:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以做类似的操作：
- en: Write **expectation in terms of integration**
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**用积分表示期望**。'
- en: '![](../Images/32f0fda37bbd20c49cd30d7d635828e1.png)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/32f0fda37bbd20c49cd30d7d635828e1.png)'
- en: '**Take derivative** with respect to *θ* onthe only term involving *θ: p_θ(τ)*'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对仅涉及*θ*的项*p_θ(τ)* **求导**。
- en: '![](../Images/dfad136479b7d13f40f32b4e33e58d29.png)'
  id: totrans-156
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/dfad136479b7d13f40f32b4e33e58d29.png)'
- en: Rewrite the gradient of *p_θ(τ)* as **product of *p_θ(τ)* and something else**
    to recover the form that **defines an expectation**
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将*p_θ(τ)*的梯度重写为** *p_θ(τ)*和其他东西的乘积**，以恢复定义期望的形式。
- en: '![](../Images/6038a11e5a858b4a7dc55648e5dabef6.png)'
  id: totrans-158
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6038a11e5a858b4a7dc55648e5dabef6.png)'
- en: So we obtain
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们得到：
- en: '![](../Images/814cbdc34ccc9e58fd5ddf31e88a3f0b.png)'
  id: totrans-160
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/814cbdc34ccc9e58fd5ddf31e88a3f0b.png)'
- en: Voila! It is exactly what We want to find. To put in words, it means we are
    updating *θ* **to the direction of gradient of log probability** of samples *τ*
    under the actions determined by *θ,* **weighted by the total reward** along the
    sampled *τ*. **This is exactly the formulation of Policy Gradients.**
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 看！这正是我们想要找到的。换句话说，这意味着我们正在将*θ* **更新为样本*τ*的对数概率梯度的方向**，权重是沿样本*τ*的**总奖励**。**这正是策略梯度的公式**。
- en: If we extend the cross entropy analogy from earlier, sum of rewards are basically
    labels for the trajectory, and *p_θ(τ)* is how likely *τ* happensunder model’s
    prediction. The training process **encourages the model to predict distributions
    similar to the distribution of rewards over different trajectories *τ****.* (This
    is in fact a mathematically accurate statement [correct me if I’m wrong]. If you
    know about KL-Divergence, compare what’s being taken gradient of to KL-Divergence).
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们从早期的交叉熵类比延伸过来，奖励的总和基本上是轨迹的标签，而 *p_θ(τ)* 是模型预测下 *τ* 发生的可能性。训练过程 **鼓励模型预测与不同轨迹
    *τ* 上的奖励分布相似的分布**。（这实际上是一个数学上准确的陈述 [如果我错了请纠正我]。如果你知道 KL 散度，可以将所计算的梯度与 KL 散度进行比较）。
- en: 'We can do some more manipulations with conditional probabilities and definition
    of *p_θ(τ)*. This process is well explained by [this video](https://www.youtube.com/watch?v=GKoKNYaBvM0&list=PL_iWQOsE6TfXxKgI1GgyV1B_Xa0DxE5eH&index=22&ab_channel=RAIL)
    (around 9:27). We finally obtains the following, which rewrites *p_θ(τ)* as *π_θ(a_t|s_t)*
    that we actually knows value of:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以对条件概率和 *p_θ(τ)* 的定义进行更多的操作。这个过程在[这个视频](https://www.youtube.com/watch?v=GKoKNYaBvM0&list=PL_iWQOsE6TfXxKgI1GgyV1B_Xa0DxE5eH&index=22&ab_channel=RAIL)（大约在
    9:27）中讲解得很好。我们最终得到以下内容，将 *p_θ(τ)* 重新表示为 *π_θ(a_t|s_t)*，这是我们实际知道其值的：
- en: '![](../Images/02b546318170e0f0cec5b21354e454c4.png)'
  id: totrans-164
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/02b546318170e0f0cec5b21354e454c4.png)'
- en: Note That **when T = 1** (single episode), **this is same as** **the gradient
    that we obtained in our setting before**. In other words, supervised ML is a special
    case of RL where there’s only one episode, and reward is non-stochastic (see the
    next section).
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 注意 **当 T = 1**（单次实验），**这与我们之前设置中获得的梯度是一样的**。换句话说，监督学习是强化学习的一个特殊情况，其中只有一个实验，奖励是非随机的（见下一节）。
- en: 'Another Difference: Estimating Rewards'
  id: totrans-166
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 另一个区别：奖励的估计
- en: Another difference between RL and supervised ML is how much can we trust the
    rewards. In supervised ML, the reward are ground truth label that come with the
    image samples. We are usually 100% sure that the rewards are correct, and our
    robot will adjust its behaviors toward those labels.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习与监督学习之间的另一个区别是我们可以多大程度上相信奖励。在监督学习中，奖励是与图像样本一起提供的真实标签。我们通常 100% 确定奖励是正确的，我们的机器人会根据这些标签调整其行为。
- en: However, in RL problems, the rewards could be **more stochastic** (imagine when
    you play a game, you could be in the same place twice but get different scores).
    So we have to **estimate the reward** for a particular state-action pair with
    historical reward as we interact with environment.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在强化学习问题中，奖励可能 **更具随机性**（想象一下你玩游戏时，可能在同一个地方两次但得到不同的分数）。因此，我们必须 **估计特定状态-动作对的奖励**，通过与环境互动并利用历史奖励来进行估计。
- en: '***[Optional]*** *Side thoughts: I was also thinking if there’s middle territory
    between supervised ML (where labels/rewards are 100% trustable ) and RL (where
    rewards are more stochastic). It seems like when labels are noisy (contains some
    wrong labels), we are kind of in the middle? So would* [*psuedo-labeling method*](https://www.kaggle.com/code/cdeotte/pseudo-labeling-qda-0-969)
    *share some flavor as RL problem? Let me know your thoughts.*'
  id: totrans-169
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '***[可选]*** *附带想法：我还在思考是否存在监督学习（标签/奖励是 100% 可相信的）和强化学习（奖励更具随机性）之间的中间领域。当标签有噪声（包含一些错误标签）时，我们是否有点像处于中间？所以，*
    [*伪标签方法*](https://www.kaggle.com/code/cdeotte/pseudo-labeling-qda-0-969) *是否与强化学习问题有一些相似之处？请告诉我你的想法。*'
- en: Technically, in the long run, we should have enough historical reward to understand
    the average reward behavior, but in the short run, small sample number might produce
    **unstable**, biased estimate about it.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 从长远来看，我们应该有足够的历史奖励来理解平均奖励行为，但在短期内，小样本数量可能会产生 **不稳定** 的偏差估计。
- en: Worse, since agent behavior is updated by the reward collected, if we collect
    low-quality rewards, we might go into and stuck at a bad policy. And it will take
    a long time to get out of there and back on right track.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 更糟糕的是，由于代理行为是通过收集的奖励来更新的，如果我们收集到低质量的奖励，我们可能会陷入并停留在一个糟糕的策略中。要从那里走出来并重新回到正确的轨道上需要很长时间。
- en: This is one the challenges in RL that is still an ongoing research area. [Doing
    some manipulations to rewards](https://www.youtube.com/watch?v=VgdSubQN35g&list=PL_iWQOsE6TfXxKgI1GgyV1B_Xa0DxE5eH&index=25&ab_channel=RAIL)
    and variants of policy gradient such as [TRPO](https://arxiv.org/abs/1502.05477)
    and [PPO](https://arxiv.org/abs/1707.06347) are designed to address this issue
    better, and have become more commonly used than vanilla PG.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 这是强化学习中的一个挑战，仍然是一个正在进行的研究领域。 [对奖励进行一些操作](https://www.youtube.com/watch?v=VgdSubQN35g&list=PL_iWQOsE6TfXxKgI1GgyV1B_Xa0DxE5eH&index=25&ab_channel=RAIL)
    和变体，如 [TRPO](https://arxiv.org/abs/1502.05477) 和 [PPO](https://arxiv.org/abs/1707.06347)，旨在更好地解决这个问题，并且比普通PG使用得更为广泛。
- en: '[Optional] Another Side Thought: Comparison to Sequential Supervised ML'
  id: totrans-173
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '[可选] 另一种思考：与序列监督机器学习的比较'
- en: 'One difference between our supervised ML setting and RL is that RL often involves
    multiple timesteps. I immediately had the question: then how does RL differ from
    training a sequential model like Transformer or LSTM?'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的监督机器学习设置与RL之间的一个区别是RL通常涉及多个时间步。我立刻有一个问题：那么RL与训练像Transformer或LSTM这样的序列模型有什么不同？
- en: The answer to this question definitely depends on the exact loss design of the
    training your favorite sequential model.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 这个问题的答案绝对取决于你最喜欢的序列模型的训练损失设计。
- en: For now, let’s say you train a sequence model *f(***x_1,x_2,…x_T***)* to predict
    **y_1, y_2…y_T**For example, in a machine translation task, **x**’s could be words
    of input English sentence, and **y***’s* are words output French sentence (each
    of **x_t, y_t** is a one hot vector representation of the word).
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，假设你训练一个序列模型 *f(***x_1,x_2,…x_T***)* 以预测 **y_1, y_2…y_T**。例如，在机器翻译任务中，**x**
    可能是输入英文句子的单词，而 **y** 是输出法文句子的单词（每个 **x_t, y_t** 是单词的一个独热向量表示）。
- en: We calculate the loss function on each sample by taking sum of cross entropy
    between each word output prediction and truth label. We the average it over a
    batch of samples and do backprop like following
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过对每个样本的每个单词输出预测与真实标签之间的交叉熵之和来计算损失函数。然后，我们对一批样本进行平均，并像下面这样进行反向传播。
- en: '![](../Images/fc39808c28ea708c98fd3999d99a357a.png)'
  id: totrans-178
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fc39808c28ea708c98fd3999d99a357a.png)'
- en: Putting back into the Policy Gradient formulation, this to me is same as calculating
    gradient of objective function as
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 放回到策略梯度公式中，对我来说，这与计算目标函数的梯度相同
- en: '![](../Images/341af3d224e7b768f617589c55d9830e.png)'
  id: totrans-180
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/341af3d224e7b768f617589c55d9830e.png)'
- en: The difference between this formulation and PG’s formulation is that we are
    not multiplying sum log probability of all timestep’s prediction with sum of rewards
    from all steps. Instead, we take pairwise product of log probability and reward
    of each timestep and sum them up.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 这种公式与PG公式的区别在于，我们没有将所有时间步的预测的对数概率之和与所有步骤的奖励之和相乘。相反，我们取每个时间步的对数概率与奖励的成对乘积并将它们相加。
- en: This removes a lot of terms thus greatly reduce variance of gradients, which
    might be what make training a Transformer/LSTM in supervised setting easier than
    RL algorithm? (in addition to the non-stochastic rewards in supervised setting).
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 这去除了很多项，因此大大减少了梯度的方差，这可能是使得在监督设置中训练Transformer/LSTM比RL算法更容易的原因？（除了监督设置中的非随机奖励）。
- en: 'A technique of reducing variance of PG is introduced in [this video](https://www.youtube.com/watch?v=VgdSubQN35g&list=PL_iWQOsE6TfXxKgI1GgyV1B_Xa0DxE5eH&index=25&ab_channel=RAIL):
    Change the sum of reward of all time steps in PG to rewards to go (i.e. sum from
    *t’ = t to t’ = T*). This shares a similar flavor as what different between PG
    and training a Transformer/LSTM in supervised setting. While rewards to go method
    make the agent to evaluate each state by possible future rewards, could we say
    that supervised sequential training make model only focus on current timestep’s
    correctness?'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: '[这个视频](https://www.youtube.com/watch?v=VgdSubQN35g&list=PL_iWQOsE6TfXxKgI1GgyV1B_Xa0DxE5eH&index=25&ab_channel=RAIL)
    中介绍了一种减少PG方差的技术：将PG中所有时间步的奖励总和更改为未来奖励（即从 *t’ = t 到 t’ = T* 的总和）。这与PG与在监督设置中训练Transformer/LSTM之间的不同具有相似的风味。虽然未来奖励方法使得代理能够通过可能的未来奖励评估每个状态，但我们是否可以说监督序列训练使得模型仅关注当前时间步的正确性？'
- en: Also, I tried to work backwards from this gradient expression and find the original
    *J(θ)* that results this gradient expression, so we can more directly interpret
    the objective of supervised sequential training. But I got stuck in the halfway.
    Let me know if you have any thoughts.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我尝试从这个梯度表达式中倒推，找到导致这个梯度表达式的原始 *J(θ)*，以便我们可以更直接地解释监督序列训练的目标。但我在半途中卡住了。如果你有任何想法，请告诉我。
- en: Acknowledgment
  id: totrans-185
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 致谢
- en: The connection between policy gradient and cross entropy is not my own original
    idea. Thanks [this post](https://amoudgl.github.io/blog/blog/policy-gradient/)
    for giving me thoughts on expanding it to more fundamentally understand what cross
    entropy and policy gradient are doing.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 策略梯度与交叉熵之间的联系并非我自己原创的想法。感谢[这篇文章](https://amoudgl.github.io/blog/blog/policy-gradient/)给了我拓展思路的启发，让我从更根本的角度理解交叉熵和策略梯度的作用。
