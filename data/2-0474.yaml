- en: Catch Up On Large Language Models
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 了解大型语言模型
- en: 原文：[https://towardsdatascience.com/catch-up-on-large-language-models-8daf784f46f8](https://towardsdatascience.com/catch-up-on-large-language-models-8daf784f46f8)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/catch-up-on-large-language-models-8daf784f46f8](https://towardsdatascience.com/catch-up-on-large-language-models-8daf784f46f8)
- en: A practical guide to large language models without the hype
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实用的无炒作大型语言模型指南
- en: '[](https://medium.com/@marcopeixeiro?source=post_page-----8daf784f46f8--------------------------------)[![Marco
    Peixeiro](../Images/7cf0a81d87281d35ff47f51e3026a3e9.png)](https://medium.com/@marcopeixeiro?source=post_page-----8daf784f46f8--------------------------------)[](https://towardsdatascience.com/?source=post_page-----8daf784f46f8--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----8daf784f46f8--------------------------------)
    [Marco Peixeiro](https://medium.com/@marcopeixeiro?source=post_page-----8daf784f46f8--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@marcopeixeiro?source=post_page-----8daf784f46f8--------------------------------)[![Marco
    Peixeiro](../Images/7cf0a81d87281d35ff47f51e3026a3e9.png)](https://medium.com/@marcopeixeiro?source=post_page-----8daf784f46f8--------------------------------)[](https://towardsdatascience.com/?source=post_page-----8daf784f46f8--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----8daf784f46f8--------------------------------)
    [Marco Peixeiro](https://medium.com/@marcopeixeiro?source=post_page-----8daf784f46f8--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----8daf784f46f8--------------------------------)
    ·15 min read·Sep 5, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----8daf784f46f8--------------------------------)
    ·15分钟阅读·2023年9月5日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/adb61491bc3d90e8c3f4676819450577.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/adb61491bc3d90e8c3f4676819450577.png)'
- en: Photo by [Gary Bendig](https://unsplash.com/@kris_ricepees?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由 [Gary Bendig](https://unsplash.com/@kris_ricepees?utm_source=medium&utm_medium=referral)
    提供，来源于 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: If you are here, it means that like me you were overwhelmed by the constant
    flow of information, and hype posts surrounding **large language models** (LLMs).
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你在这里，这意味着像我一样，你被围绕 **大型语言模型** (LLMs) 的不断信息流和炒作文章所压倒。
- en: This article is my attempt at helping you catch up on the subject of large language
    models without the hype. After all, it is a transformative technology, and I believe
    it is important for us to understand it, hopefully making you curious to learn
    even more and build something with it.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 这篇文章是我试图帮助你了解大型语言模型的努力，没有炒作。毕竟，这是一个变革性的技术，我相信了解它很重要，希望这会激发你更深入地学习并用它创建一些东西。
- en: In the following sections, we will define what LLMs are and how they work, of
    course covering the Transformer architecture. We also explore the different methods
    of training LLMs and conclude the article with a hands-on project where we use
    Flan-T5 for sentiment analysis using Python.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的部分中，我们将定义什么是 LLM 及其工作原理，当然会涵盖 Transformer 架构。我们还将探讨不同的 LLM 训练方法，并通过一个动手项目结束文章，在这个项目中，我们将使用
    Flan-T5 进行 Python 情感分析。
- en: Let’s get started!
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 开始吧！
- en: 'LLMs and generative AI: are they the same thing?'
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LLM 和生成式 AI：它们是一样的吗？
- en: 'Generative AI is a subset of machine learning that focuses on models who’s
    primary function is to generate *something*: text, images, video, code, etc.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 生成式 AI 是机器学习的一个子集，专注于那些主要功能是生成 *某物* 的模型：文本、图像、视频、代码等。
- en: Generative models train on enormous amounts of data created by humans to learn
    patterns and structure which allow them to create new data.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 生成模型通过在大量由人类创建的数据上进行训练，以学习模式和结构，从而能够生成新数据。
- en: 'Examples of generative models include:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 生成模型的示例包括：
- en: '**Image generation**: DALL-E, Midjourney'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**图像生成**：DALL-E，Midjourney'
- en: '**Code generation**: OpenAI Codex'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**代码生成**：OpenAI Codex'
- en: '**Text generation**: GPT-3, Flan-T5, LLaMA'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**文本生成**：GPT-3，Flan-T5，LLaMA'
- en: Large language models are part of the generative AI landscape, since they take
    an input text and repeatedly predict the next word until the output is complete.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型是生成式 AI 领域的一部分，因为它们会接收输入文本并反复预测下一个单词，直到输出完成。
- en: However, as language models grew larger, they were able to perform other tasks
    in natural language processing, like summarization, sentiment analysis, named
    entity recognition, translation and more.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，随着语言模型的规模不断扩大，它们能够执行其他自然语言处理任务，如摘要、情感分析、命名实体识别、翻译等。
- en: With that in mind, let’s now focus our attention on how LLMs work.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 既然如此，现在让我们关注 LLM 的工作原理。
- en: How LLMs work
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LLM 的工作原理
- en: One of the reasons why we now have large language models is because of the seminal
    work of Google and University of Toronto when they released the paper [Attention
    Is All You Need](https://arxiv.org/abs/1706.03762) in 2017.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在拥有大规模语言模型的原因之一是 Google 和多伦多大学的开创性工作，他们在 2017 年发布了论文 [Attention Is All You
    Need](https://arxiv.org/abs/1706.03762)。
- en: This paper introduced the **Transformer** architecture, which is behind the
    LLMs we know and use today.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 本文介绍了 **Transformer** 架构，它是我们今天所知和使用的 LLM 背后的基础。
- en: This architecture unlocked large scale models, making it possible to train very
    large models on multiple GPUs, and the models are able to process the inputs in
    parallel, giving them the opportunity to treat very large sequences of data.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 这种架构解锁了大规模模型，使得在多个 GPU 上训练非常大的模型成为可能，这些模型能够并行处理输入，给它们处理非常大数据序列的机会。
- en: Overview of the Transformer architecture
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Transformer 架构概述
- en: The following is meant to be a high-level overview of the Transformer architecture.
    There are many resources that dive deeper into it, but the goal here is just to
    understand the way it works so we can understand how different LLMs specialize
    in different tasks.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 以下内容旨在对 Transformer 架构进行高层次的概述。虽然有许多资源对其进行了更深入的探讨，但这里的目标只是理解其工作原理，以便理解不同的 LLM
    如何在不同任务中发挥作用。
- en: At any time, for more details, I suggest you read the [original paper](https://arxiv.org/pdf/1706.03762.pdf).
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 如需更多详细信息，我建议阅读 [原始论文](https://arxiv.org/pdf/1706.03762.pdf)。
- en: So, let’s start with a simplified visualization of the Transformer architecture.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，让我们从 Transformer 架构的简化可视化开始。
- en: '![](../Images/f8b3a35183236257fa4fe5247b56a9a5.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f8b3a35183236257fa4fe5247b56a9a5.png)'
- en: A simplified visualization of the Transformer architecture. Image by the author.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer 架构的简化可视化。图片由作者提供。
- en: From the figure above, we can see that the main components of the Transformer
    are the encoder and decoder. Inside each, we also find the **attention** component.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 从上图中，我们可以看到 Transformer 的主要组件是编码器和解码器。在每个组件内部，我们还可以找到 **attention** 组件。
- en: Let’s explore each component in more detail to understand how the Transformer
    architecture works.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更详细地探讨每个组件，以理解 Transformer 架构的工作原理。
- en: Tokenize the inputs
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分词输入
- en: We know LLMs work with text, but computers work with numbers, not letters. Therefore,
    the input must be *tokenized*.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 我们知道 LLMs 使用文本，但计算机处理的是数字而非字母。因此，输入必须进行 *分词*。
- en: Tokenization is the process in which words of a sentence are represented as
    numbers.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 分词是将句子的单词表示为数字的过程。
- en: Basically, every possible word a model can work with is in a dictionary with
    a number associated to it. With tokenization, we can retrieve the number associated
    with the word, to represent a sentence as a sequence of numbers, as shown below.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 基本上，模型可以处理的每个可能单词都在一个字典中，并且与之关联一个编号。通过分词，我们可以检索与单词相关的编号，将句子表示为数字序列，如下所示。
- en: '![](../Images/ba7e371a044bec54d4a3dce16126b05a.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ba7e371a044bec54d4a3dce16126b05a.png)'
- en: Example of tokenization. The sentence is tokenized and then sent to the embedding
    of the Transformer. Image by the author.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 分词示例。句子被分词后送入 Transformer 的嵌入层。图片由作者提供。
- en: In the figure above, we see an example of how the sentence “It rained this morning”
    can be tokenized before being set to the embedding layer of the Transformer.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在上图中，我们看到一个例子，展示了如何在将句子“It rained this morning”发送到 Transformer 的嵌入层之前进行分词。
- en: Note that there are many ways of tokenizing a sentence. In the example above,
    the tokenizer can represents parts of a word, which is why *rained* is separated
    into *rain* and *ed*. Other tokenizers would have a number for full words only.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 注意到分词句子的方式有很多种。在上面的例子中，分词器可以将一个词的部分表示出来，这就是为什么 *rained* 被分成 *rain* 和 *ed*。其他分词器可能只会为完整的单词分配一个编号。
- en: The word embedding layer
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 词嵌入层
- en: At this point, we have a series of numbers that represent words, but how can
    the computer understand their meaning?
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 此时，我们有一系列表示单词的数字，但计算机如何理解这些数字的含义呢？
- en: This is achieved by the word embedding layers.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 这是通过词嵌入层实现的。
- en: Word embedding is a learned representation of words, such that words with a
    similar meaning will have a similar representation. The model will learn different
    properties of words and represent them in a fixed space, where each axis can represent
    the property of a word.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 词嵌入是一种对单词的学习表示，使得具有相似意义的单词具有相似的表示。模型将学习单词的不同属性，并在一个固定空间中表示它们，其中每个轴可以表示单词的属性。
- en: '![](../Images/6a404c0f4ae58847ff43d149d786629d.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6a404c0f4ae58847ff43d149d786629d.png)'
- en: Visualization of word embeddings. We can see that “morning” and “sunrise” have
    a similar representation since the angle in the 3D space is smaller. Similarly,
    “rain” and “thunder” are closer to one another. Image by the author.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 词嵌入的可视化。我们可以看到“morning”和“sunrise”有相似的表示，因为它们在3D空间中的角度较小。同样，“rain”和“thunder”彼此更近。图片由作者提供。
- en: In the figure above, we can see how a 3D word embedding can look like. We see
    that “morning” and “sunrise” are closer to one another, and therefore have a similar
    representation. This can be can be computed using cosine similarity.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在上图中，我们可以看到3D词嵌入的样子。我们看到“morning”和“sunrise”彼此更近，因此具有相似的表示。这可以通过余弦相似度计算得出。
- en: On the other hand, “rain” and “thunder” are close to each other, and far from
    “morning” and “sunrise”.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，“rain”和“thunder”彼此较近，而与“morning”和“sunrise”相距较远。
- en: Now, we can only show a 3D space, but in reality, embeddings can have hundreds
    of dimensions. In fact, the original Transformer architecture used an embedding
    space of 512 dimensions. This means that the model could learn 512 different properties
    of words to represent them in a space of 512 dimensions.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们只能展示一个3D空间，但实际上，嵌入可以有数百个维度。事实上，原始的Transformer架构使用了512维的嵌入空间。这意味着模型可以学习512个不同的词属性，将它们表示在一个512维的空间中。
- en: What about word order?
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 那么词序呢？
- en: You may have noticed that by representing words in embeddings, we lose their
    order in the sentence.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能已经注意到，通过表示词的嵌入，我们会丧失它们在句子中的顺序。
- en: Of course, with natural language, word order is very important, and so that’s
    why we use positional encoding, so the model knows the order of the words in a
    sentence.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，在自然语言中，词序非常重要，因此我们使用位置编码，以便模型了解句子中单词的顺序。
- en: It is the combination of the word embeddings and the positional encoding that
    gets sent to the encoder.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 是将词嵌入和位置编码结合在一起并发送给编码器。
- en: Inside the encoder
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在编码器内部
- en: Our inputs travel inside the encoder where they will go through the **self-attention**
    mechanism.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的输入在编码器内部传递，在那里它们会经过**自注意力**机制。
- en: This is where the model can learn the dependencies between each token in a sentence.
    It learns the importance of each word in relation to all other words in a sentence.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是模型可以学习句子中每个标记之间依赖关系的地方。它学习了每个词相对于句子中所有其他词的重要性。
- en: '![](../Images/f6c0661476ae1f98f0b96aa065648efa.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f6c0661476ae1f98f0b96aa065648efa.png)'
- en: Example of an attention map for the word “rained”. The stroke width is representative
    of the importance. Here, we can see that “rained” is strongly connected to “this”
    and “morning”. Image by the author.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 单词“rained”的注意力图示例。笔划宽度代表重要性。在这里，我们可以看到“rained”与“this”和“morning”紧密连接。图片由作者提供。
- en: In the figure above, we have a stylized example of an attention map for the
    word “rained”. The stroke width represents the importance.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在上图中，我们展示了单词“rained”的注意力图的风格化示例。笔划宽度表示重要性。
- en: In this example, we can see that self-attention captures the importance of “rained”
    with “this” and “morning”, meaning that it understands the context of this sentence.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们可以看到自注意力捕捉了“rained”与“this”和“morning”的重要性，这意味着它理解了这个句子的上下文。
- en: While this example remains simple, since we have a very short sentence, the
    self-attention mechanism works very well on longer sentences, effectively capturing
    context and the overall meaning of a sentence.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这个例子很简单，因为我们只有一个非常短的句子，自注意力机制在较长的句子中效果很好，能够有效地捕捉上下文和句子的整体含义。
- en: Furthermore, the model does not have a single attention head. In fact, it has
    multiple attention heads, also called *multi-headed self-attention*, where each
    head can learn a different aspect of language.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，模型并没有一个单一的注意力头。事实上，它有多个注意力头，也称为*多头自注意力*，每个头部可以学习语言的不同方面。
- en: For example, in the paper [Attention Is All You Need](https://arxiv.org/pdf/1706.03762.pdf),
    the authors found that one head was involved in *anaphora resolution*, which is
    identifying the link between an entity and its repeated references.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在论文[Attention Is All You Need](https://arxiv.org/pdf/1706.03762.pdf)中，作者发现一个头部涉及到*指代消解*，即识别实体与其重复引用之间的联系。
- en: '![](../Images/0027b908a4028d654238ec0de6e4052f.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0027b908a4028d654238ec0de6e4052f.png)'
- en: Example of anaphora resolution. Here, the word “keys” is referenced again in
    the sentence as “they”. Image by the author.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 指代消解的例子。在这里，单词“keys”在句子中再次被引用为“they”。图片由作者提供。
- en: Above, we see an example of anaphora resolution, where the word “keys” is later
    referenced as “they”, and so one attention head can specialize in identifying
    those links.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 上面，我们看到一个指代解析的示例，其中单词“keys”后来被提及为“they”，因此一个注意力头可以专门识别这些链接。
- en: Note that we do not decide what aspect of language each attention head will
    learn.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们并未决定每个注意力头将学习语言的哪个方面。
- en: At this point, the model has a deep representation of the structure of meaning
    of a sentence. This is sent to the decoder.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 此时，模型已经对句子的意义结构有了深层次的表示。这被发送到解码器。
- en: Inside the decoder
  id: totrans-70
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解码器内部
- en: The decoder accepts a deep representation of the input tokens. This informs
    the self-attention mechanism inside the decoder.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 解码器接受输入令牌的深层表示。这为解码器内部的自注意机制提供信息。
- en: As a reminder, here is the Transformer architecture again, so we can remember
    what it looks like.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 作为提醒，这里再次展示了Transformer架构，以便我们记住它的样子。
- en: '![](../Images/f8b3a35183236257fa4fe5247b56a9a5.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f8b3a35183236257fa4fe5247b56a9a5.png)'
- en: A simplified visualization of the Transformer architecture. Image by the author.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer架构的简化可视化。图片由作者提供。
- en: A **start-of-sequence** token is inserted as an input of the decoder, to signal
    it to start generating new tokens.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '**序列开始**令牌被插入作为解码器的输入，以指示其开始生成新令牌。'
- en: New tokens are generated according to the understanding of the input sequence
    generated by the encoder and its self-attention mechanism.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 新令牌是根据编码器生成的输入序列的理解及其自注意机制生成的。
- en: In the figure above, we can see that the output of the decoder gets sent to
    a softmax layer. This generates a vector of probabilities for each possible token.
    The one with the largest probability is then output by the model.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在上图中，我们可以看到解码器的输出被送到一个softmax层。这生成了每个可能令牌的概率向量。具有最大概率的令牌随后由模型输出。
- en: That output token is then sent back to the embeddings as an input to the decoder,
    until an **end-of-sequence** token is generated by the model. At that point, the
    output sequence is complete.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 该输出令牌随后被送回嵌入层作为解码器的输入，直到模型生成**序列结束**令牌。此时，输出序列完成。
- en: This concludes the basic architecture behind large language models. With the
    Transformer architecture and its ability to process data in parallel, it was possible
    to train models on huge amounts of data, making LLMs a reality.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 这总结了大型语言模型背后的基本架构。通过Transformer架构及其并行处理数据的能力，使得在大量数据上训练模型成为可能，使LLMs成为现实。
- en: Now, there is more to this, as LLMs do not all use the full Transformer architecture,
    and that influences the way they are trained. Let’s explore this in more detail.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，情况更复杂，因为LLMs并非都使用完整的Transformer架构，这影响了它们的训练方式。让我们更详细地探讨这一点。
- en: How LLMs are trained
  id: totrans-81
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LLM的训练方式
- en: We have seen the underlying mechanisms that power large language models, and
    as mentioned, not all models use the full Transformer architecture.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到了支撑大型语言模型的基本机制，如前所述，并非所有模型都使用完整的Transformer架构。
- en: In fact, some models may use the encoder portion only, while others use the
    decoder portion only.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，一些模型可能只使用编码器部分，而其他模型只使用解码器部分。
- en: This means that the models are also trained differently and will therefore specialize
    in particular tasks.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着模型的训练方式也不同，因此会专注于特定任务。
- en: Encoder-only models
  id: totrans-85
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 仅编码器模型
- en: Encoder-only models, also called **autoencoding** models are best suited for
    tasks like sentiment analysis, named entity recognition, and word classification
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 仅编码器模型，也称为**自编码**模型，最适合用于情感分析、命名实体识别和词汇分类等任务。
- en: Popular examples of autoencoding models are BERT and ROBERTA.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 自编码模型的流行示例有BERT和ROBERTA。
- en: Those models are trained using **masked language modeling** (MLM). With that
    training method, words in an input sentence are randomly masked, and the objective
    of the model is then to reconstruct the original text.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 这些模型使用**掩码语言建模**（MLM）进行训练。通过这种训练方法，输入句子中的单词会被随机掩盖，模型的目标是重建原始文本。
- en: '![](../Images/7593549a23d59ccb9ddcef964b473e80.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7593549a23d59ccb9ddcef964b473e80.png)'
- en: Illustrating masked language modeling (MLM) for autoencoding models. Here, a
    random word was masked in the input sentence, and the model must reconstruct the
    original sentence. Image by the author.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 说明了用于自编码模型的掩码语言建模（MLM）。在这里，输入句子中的一个随机单词被掩盖，模型必须重建原始句子。图片由作者提供。
- en: In the figure above, we can see what masked language modeling looks like. A
    word is hidden and the sentence is fed to the model, which must then learn to
    predict the right word to get the correct original sentence.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 在上图中，我们可以看到掩蔽语言建模的样子。一个词被隐藏，句子被输入到模型中，模型必须学习预测正确的词以得到正确的原始句子。
- en: With that method, autoencoding models develop **bidrectional context**, since
    they see what precedes and follows the token they must predict, and not just what
    comes before.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 使用该方法，自编码模型发展了**双向上下文**，因为它们可以看到需要预测的标记前后的内容，而不仅仅是前面的内容。
- en: Again, in the figure above, the model sees “it rained” and “morning”, so it
    sees both the beginning and the end of the sentence, allowing it to predict the
    word “this” to reconstruct the sentence correctly.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 再次如上图所示，模型看到“it rained”和“morning”，因此它看到句子的开头和结尾，这使得它能够预测“this”这个词，从而正确重构句子。
- en: Note that with autoencoding models, the input and output sequences have the
    same length.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，对于自编码模型，输入和输出序列的长度是相同的。
- en: Decoder-only models
  id: totrans-95
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 仅解码器模型
- en: Decoder-only models are also called *autoregressive* models. These models are
    best suited for text generation, but new functions arise when the models get very
    large.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 仅解码器模型也称为*自回归*模型。这些模型最适合文本生成，但当模型变得非常大时，新的功能就会出现。
- en: Example of autoregressive models are GPT and BLOOM.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 自回归模型的例子有GPT和BLOOM。
- en: These models are trained using **causal language modeling** (CLM). With causal
    language modeling, the model only sees the tokens preceding the mask, meaning
    that it does not see the end of the sequence.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 这些模型使用**因果语言建模**（CLM）进行训练。使用因果语言建模时，模型只看到掩蔽之前的标记，这意味着它看不到序列的结尾。
- en: '![](../Images/dd4503e7d11be727715a89d105e4629a.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/dd4503e7d11be727715a89d105e4629a.png)'
- en: Illustrating causal language modeling. Here, the model only sees the tokens
    leading to the mask. Then, it must infer the next tokens until the sentence is
    complete. Image by the author.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 说明因果语言建模。在这里，模型只看到导致掩蔽的标记。然后，它必须推断下一个标记直到句子完整。图像由作者提供。
- en: As we see above, with causal language modeling, the model only sees the tokens
    leading to the mask, and not what comes after. Then, it must predict the next
    tokens until the sentence is complete.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 如上所示，使用因果语言建模时，模型只看到导致掩蔽的标记，而看不到掩蔽之后的内容。然后，它必须预测下一个标记直到句子完整。
- en: In the example above, the model would output “this”, and that token would be
    fed back as an input, so the model can then predict “morning”.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 在上面的例子中，模型会输出“this”，然后该标记会被反馈作为输入，因此模型可以预测“morning”。
- en: Unlike masked language modeling, model build unidirectional context, since they
    do not see what comes after the mask.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 与掩蔽语言建模不同，模型建立了单向上下文，因为它们看不到掩蔽之后的内容。
- en: Of course, with decoder-only models, the output sequence can have a different
    length than the input sequence.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，对于仅解码器模型，输出序列的长度可能与输入序列的长度不同。
- en: Encoder-decoder models
  id: totrans-105
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 编码器-解码器模型
- en: Encoder-decoder models are also called *sequence-to-sequence* models, and they
    use the full Transformer architecture.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 编码器-解码器模型也称为*序列到序列*模型，并且它们使用完整的Transformer架构。
- en: Those models are often used for translation, text summarization and question
    answering.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 这些模型通常用于翻译、文本摘要和问答。
- en: Popular examples of sequence-to-sequence models are T5 and BART.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 流行的序列到序列模型的例子有T5和BART。
- en: To train these models, the **span corruption** method is used. Here, a random
    sequence of tokens is masked and designated as a *sentinel* token. Then, the model
    must reconstruct the masked sequence autoregressively.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 为了训练这些模型，使用了**跨度破坏**方法。在这里，一个随机的标记序列被掩蔽并指定为*哨兵*标记。然后，模型必须自回归地重构被掩蔽的序列。
- en: '![](../Images/aa279015ee0385588f31e1a9586423cf.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/aa279015ee0385588f31e1a9586423cf.png)'
- en: Illustration of span corruption. Here, a sequence of tokens is masked and replaced
    by a sentinel token. The model must then reconstructed the masked sequence autoregressively.
    Image by the author.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 跨度破坏的说明。在这里，一系列标记被掩蔽并用哨兵标记替代。然后，模型必须自回归地重构被掩蔽的序列。图像由作者提供。
- en: In the figure above, we can see that a sequence of two tokens were masked and
    replaced by a sentinel token. The model is then trained to reconstruct the sentinel
    token to obtain the original sentence.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 在上图中，我们可以看到两个标记的序列被掩蔽并用哨兵标记替代。然后，模型被训练以重构哨兵标记以获得原始句子。
- en: Here, the masked input is sent to the encoder, and the decoder is responsible
    for reconstructing the masked sequence.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，掩码输入被发送到编码器，而解码器负责重建掩码序列。
- en: A note on model size
  id: totrans-114
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 关于模型大小的说明
- en: While we have specified certain tasks for which certain models perform best,
    researchers have observed that large models are capable of various tasks.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们已指定了某些模型表现最佳的任务，研究人员观察到大型模型能够执行各种任务。
- en: Therefore, very large decoder-only models can be very good at translation, even
    though encoder-decoder models specialize in that task.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，虽然编码-解码模型专门用于翻译，但非常大的仅解码模型在翻译方面也表现出色。
- en: With all of that in mind, let’s now start working with a large language in Python.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到这些，让我们现在开始在 Python 中使用大型语言模型。
- en: Work with a large language model
  id: totrans-118
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 与大型语言模型合作
- en: Before we get hands-on experience with a large language model, let’s just cover
    some technical terms involved when working with LLMs.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们实际操作大型语言模型之前，让我们先了解一些与 LLM 相关的技术术语。
- en: First, the text that we feed the LLM is called *prompt*, and the output of the
    model is called *completion*.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们提供给 LLM 的文本称为*提示（prompt）*，模型的输出称为*完成（completion）*。
- en: '![](../Images/a54b6eb231931eccc8e824df03e110e6.png)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a54b6eb231931eccc8e824df03e110e6.png)'
- en: The prompt is the text we feed to the model with the instructions. The output
    of the model is called completion. Image by the author.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 提示是我们向模型提供的包含指令的文本。模型的输出称为完成。图片由作者提供。
- en: Inside the prompt is where we give the instructions to the LLM to achieve the
    task that we want.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 在提示中，我们向 LLM 提供指令，以实现我们希望完成的任务。
- en: This is also where **prompt engineering** is performed. With prompt engineering,
    we can perform **in-context learning**, which is when we give examples to the
    model of how certain tasks should be performed. We will see an example of that
    later on.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 这也是进行**提示工程**的地方。通过提示工程，我们可以进行**上下文学习**，即向模型提供如何执行某些任务的示例。稍后我们将看到一个例子。
- en: For now, let’s interact with an LLM using Python for sentiment analysis.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，让我们使用 Python 与 LLM 进行情感分析的互动。
- en: 'Hands-on project: sentiment analysis with Flan-T5'
  id: totrans-126
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实践项目：使用 Flan-T5 进行情感分析
- en: For this mini project, we use Flan-T5 for sentiment analysis of various financial
    news.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个迷你项目，我们使用 Flan-T5 对各种金融新闻进行情感分析。
- en: Flan-T5 is an improved version of the T5 model, which is a sequence-to-sequence
    model. Researchers basically took the T5 model and fine-tuned it on different
    tasks covering more languages. For more details, you can refer to the [original
    paper](https://arxiv.org/pdf/2210.11416.pdf).
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: Flan-T5 是 T5 模型的改进版，T5 是一个序列到序列模型。研究人员基本上对 T5 模型进行了微调，使其覆盖更多语言。有关更多详细信息，请参见[原始论文](https://arxiv.org/pdf/2210.11416.pdf)。
- en: As for the dataset, we will use the *financial_phrasebank* dataset published
    by Pekka Malo and Ankur Sinha under the Creative Commons Attribute license.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 对于数据集，我们将使用由 Pekka Malo 和 Ankur Sinha 在 Creative Commons 属性许可下发布的*financial_phrasebank*
    数据集。
- en: The dataset contains a total of 4840 sentences from English language financial
    news that were categorized as positive, negative or neutral. A group of five to
    eight annotators classified each sentence, and depending on the agreement rate,
    the size of the dataset will vary (4850 rows for a 50% agreement rate, and 2260
    rows for a 100% agreement rate).
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集包含来自英语金融新闻的共4840个句子，这些句子被分类为积极、消极或中立。五到八名注释员对每个句子进行分类，根据一致性率，数据集的大小会有所不同（50%一致率为4850行，100%一致率为2260行）。
- en: For more information on the dataset and how it was compiled, refer to the [full
    dataset details page.](https://huggingface.co/datasets/financial_phrasebank)
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 有关数据集及其编制方式的更多信息，请参见[完整数据集详情页面。](https://huggingface.co/datasets/financial_phrasebank)
- en: Of course, all code show below is available on [GitHub](https://github.com/marcopeix/learn_llm/blob/main/1_llm_get_started.ipynb).
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，下面显示的所有代码都可以在[GitHub](https://github.com/marcopeix/learn_llm/blob/main/1_llm_get_started.ipynb)上找到。
- en: Setup your environment
  id: totrans-133
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 设置你的环境
- en: 'For the following experiment to work, make sure to have a virtual environment
    with the following packages installed:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使以下实验有效，确保有一个虚拟环境，并安装了以下软件包：
- en: torch
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: torch
- en: torchdata
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: torchdata
- en: transformers
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: transformers
- en: datasets
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: datasets
- en: pandas
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: pandas
- en: matplotlib
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: matplotlib
- en: scikit-learn
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: scikit-learn
- en: Note that the libraries *transformers* and *datasets* are from HuggingFace,
    making it super easy for us to access and experiment with LLMs.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，库*transformers*和*datasets*来自 HuggingFace，使我们可以轻松访问和实验 LLM。
- en: Once the environment is setup, we can start by importing the required libraries.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦环境设置好，我们可以开始导入所需的库。
- en: '[PRE0]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Load the data
  id: totrans-145
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 加载数据
- en: Then, we can load our dataset. Here, we use the dataset with 100% agreement
    rate.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以加载我们的数据集。在这里，我们使用了具有 100% 一致性率的数据集。
- en: '[PRE1]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: This dataset contains a total of 2264 sentences.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 该数据集包含总共 2264 个句子。
- en: Note that the label is encoded. 1 means neutral, 0 means negative and 2 means
    positive. The count of each label is shown below.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意标签已被编码。1 代表中性，0 代表负面，2 代表正面。每种标签的计数如下所示。
- en: '![](../Images/fe864adf8af515e6c1bc3fc401dd6ccc.png)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fe864adf8af515e6c1bc3fc401dd6ccc.png)'
- en: Frequency of each sentiment in the dataset. Image by the author.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集中每种情感的频率。图片由作者提供。
- en: Let’s store the actual label of each sentence in a DataFrame, making it easier
    for us to evaluate the model later on.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将每个句子的实际标签存储在一个 DataFrame 中，以便后续更容易评估模型。
- en: '[PRE2]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Load the model
  id: totrans-154
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 加载模型
- en: Now, let’s load the model as well as the tokenizer. As mentioned above, we will
    load the Flan-T5 model. Note that the model is available in different sizes, but
    I decided to use the base version.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们加载模型和分词器。如前所述，我们将加载 Flan-T5 模型。请注意，该模型有不同的大小版本，但我决定使用基础版。
- en: '[PRE3]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: That’s it! We can now use this LLM to perform sentiment analysis on our dataset.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 就这样！我们现在可以使用这个 LLM 对我们的数据集进行情感分析。
- en: Prompt the model for sentiment analysis
  id: totrans-158
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 向模型提出情感分析的提示
- en: For the model to perform sentiment analysis, we need to do prompt engineering
    to specify that task.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 为了让模型进行情感分析，我们需要进行提示工程以指定该任务。
- en: In this case we simply use “*Is the following sentence positive, negative or
    neutral?*”. We then pass the sentence of our dataset and let the model infer.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们简单地使用“*以下句子是正面、负面还是中性？*”。然后我们将数据集中的句子传递给模型，让模型进行推断。
- en: Note that this is called **zero-shot inference**, since the model was not specifically
    trained for this particular task on this specific dataset.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，这被称为**零-shot 推断**，因为模型没有特别针对这个特定任务和数据集进行训练。
- en: '[PRE4]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: In the Python code block above, we loop over each sentence in the dataset and
    pass it in our prompt. The prompt is tokenized and set to the model. We then decode
    the output to obtain a natural language response. Finally, we store the prediction
    of the model in a list.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 在上面的 Python 代码块中，我们循环遍历数据集中的每个句子，并将其传递到我们的提示中。提示被分词并设置给模型。然后，我们解码输出以获得自然语言响应。最后，我们将模型的预测存储在列表中。
- en: Then, let’s add these predictions to our DataFrame.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，让我们将这些预测添加到我们的 DataFrame 中。
- en: '[PRE5]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Evaluate the model
  id: totrans-166
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 评估模型
- en: To evaluate our model, let’s display the confusion matrix of the predictions,
    as well as the classification report.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 为了评估我们的模型，让我们展示预测的混淆矩阵以及分类报告。
- en: '[PRE6]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '![](../Images/82cccc41147fd11fd4f0fd6cce0e2532.png)'
  id: totrans-169
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/82cccc41147fd11fd4f0fd6cce0e2532.png)'
- en: Confusion matrix of zero-shot sentiment analysis on financial news using Flan-T5\.
    Image by the author.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 Flan-T5 对金融新闻进行零-shot 情感分析的混淆矩阵。图片由作者提供。
- en: '![](../Images/b9909c03e18bb8479578ad762a17ba9b.png)'
  id: totrans-171
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b9909c03e18bb8479578ad762a17ba9b.png)'
- en: Classification report for zero-shot sentiment analysis. Image by the author.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 零-shot 情感分析的分类报告。图片由作者提供。
- en: From the figure above, we can see that the model found all negative sentences,
    at the cost of precision since it mislabelled 611 neutral sentences and 92 positive
    sentences. Also, we can see a clear problem with identifying neutral sentences,
    as it mislabelled the vast majority.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 从上面的图中，我们可以看到模型找到了所有负面句子，但代价是精确度，因为它错误标记了 611 个中性句子和 92 个正面句子。此外，我们还可以看到识别中性句子存在明显的问题，因为它错误标记了绝大多数句子。
- en: Therefore, let’s try to change our prompt to see if we can improve the model’s
    performance.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，让我们尝试更改提示，以查看是否可以提高模型的性能。
- en: One-shot inference with in-context learning
  id: totrans-175
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 带有上下文学习的一次性推断
- en: Here, we modify our prompt to include an example of a neutral sentence. This
    technique is called **in-context learning**, as we pass an example of how the
    model should behave inside the prompt.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们修改了我们的提示，加入了一个中性句子的示例。这种技术称为**上下文学习**，因为我们在提示中传递了模型应如何表现的示例。
- en: Passing one example is called **one-shot inference**. It is possible to pass
    more examples, in which case it becomes **few-shot inference**.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 传递一个示例称为**一次性推断**。可以传递更多示例，这种情况称为**少量样本推断**。
- en: It is normal to show up to five examples to the LLM. If the performance does
    not improve, then it is likely that we need to fine-tune the model.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 向 LLM 展示最多五个示例是正常的。如果性能没有提高，那么很可能需要对模型进行微调。
- en: For now, let’s see how one example impacts the performance.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看一个示例如何影响性能。
- en: '[PRE7]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: In the code block above, we see that we give an example of a neutral sentence
    to help the model identify them. Then, we pass each sentence for the model to
    classify.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 在上面的代码块中，我们看到我们给出了一个中性句子的示例，以帮助模型识别它们。然后，我们将每个句子传递给模型进行分类。
- en: Afterwards, we follow the same steps of adding a new columns containing the
    new predictions, and displaying the confusion matrix.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们按照相同的步骤添加包含新预测的新列，并显示混淆矩阵。
- en: '[PRE8]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '![](../Images/086bcf3eb3efc80555f4344687861b6f.png)'
  id: totrans-184
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/086bcf3eb3efc80555f4344687861b6f.png)'
- en: Confusion matrix of one-shot sentiment analysis of financial news using Flan-T5\.
    Image by the author.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Flan-T5进行金融新闻的单次情感分析的混淆矩阵。图片由作者提供。
- en: '![](../Images/2ba2fe6cfd211f5f71a249c7c4d5a826.png)'
  id: totrans-186
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2ba2fe6cfd211f5f71a249c7c4d5a826.png)'
- en: Classification report for one-shot sentiment analysis. Image by the author.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 单次情感分析的分类报告。图片由作者提供。
- en: From the figure above, we can see a slight improvement. The weighted F1-score
    increased from 0.40 to 0.44\. The model did better on the neutral class, but at
    the cost of a worse performance on the positive class.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 从上图可以看出，略有改善。加权F1分数从0.40提高到了0.44。模型在中性类别上的表现更好，但以牺牲对正面类别的表现为代价。
- en: Adding examples of positive, negative, and neutral sentences may help, but I
    did not test it out. Otherwise, fine-tuning the model would be necessary, but
    that is the subject of another article.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 添加正面、负面和中性句子的示例可能会有帮助，但我没有进行测试。否则，就需要对模型进行微调，但那是另一篇文章的主题。
- en: Conclusion
  id: totrans-190
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: A lot of concepts were covered in this article, from the understanding the basics
    of LLMs, to actually using Flan-T5 for sentiment analysis in Python.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 本文涵盖了许多概念，从理解LLM的基础知识，到实际使用Flan-T5进行Python中的情感分析。
- en: You now have the foundational knowledge to explore this world on your own and
    see how we can fine-tune LLMs, how we can train one, and how we can build applications
    around them.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你拥有了探索这个领域的基础知识，可以自己看看如何微调LLM，如何训练LLM，以及如何围绕它们构建应用程序。
- en: I hope that you learned something new, and that you are curious to learn even
    more.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 希望你学到了新东西，并且对学习更多充满好奇。
- en: Cheers 🍻
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 干杯 🍻
- en: Support me
  id: totrans-195
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 支持我
- en: Enjoying my work? Show your support with [Buy me a coffee](http://buymeacoffee.com/dswm),
    a simple way for you to encourage me, and I get to enjoy a cup of coffee! If you
    feel like it, just click the button below 👇
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 喜欢我的工作吗？通过[请我喝咖啡](http://buymeacoffee.com/dswm)来支持我，这是你鼓励我的简单方式，而我能享受一杯咖啡！如果你愿意，只需点击下面的按钮
    👇
- en: '[![](../Images/7ad9438bd50b1698fdd722fa6661b16c.png)](http://buymeacoffee.com/dswm)'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: '[![](../Images/7ad9438bd50b1698fdd722fa6661b16c.png)](http://buymeacoffee.com/dswm)'
- en: References
  id: totrans-198
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考资料
- en: '[Attention is All You Need](https://arxiv.org/abs/1706.03762) — Ashish Vaswani,
    Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz
    Kaiser, Illia Polosukhin'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: '[注意力机制](https://arxiv.org/abs/1706.03762) — Ashish Vaswani, Noam Shazeer, Niki
    Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin'
- en: '[Generative AI with LLM](https://www.deeplearning.ai/courses/generative-ai-with-llms/)s
    — deeplearning.ai'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: '[生成式AI与LLM](https://www.deeplearning.ai/courses/generative-ai-with-llms/) —
    deeplearning.ai'
