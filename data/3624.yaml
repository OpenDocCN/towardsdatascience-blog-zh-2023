- en: Create Many-To-One relationships Between Columns in a Synthetic Table with PySpark
    UDFs
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 PySpark UDFs 在合成表中创建多对一关系
- en: 原文：[https://towardsdatascience.com/create-many-to-one-relationships-between-columns-in-a-synthetic-table-with-pyspark-udfs-41e39d97c936?source=collection_archive---------3-----------------------#2023-12-09](https://towardsdatascience.com/create-many-to-one-relationships-between-columns-in-a-synthetic-table-with-pyspark-udfs-41e39d97c936?source=collection_archive---------3-----------------------#2023-12-09)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/create-many-to-one-relationships-between-columns-in-a-synthetic-table-with-pyspark-udfs-41e39d97c936?source=collection_archive---------3-----------------------#2023-12-09](https://towardsdatascience.com/create-many-to-one-relationships-between-columns-in-a-synthetic-table-with-pyspark-udfs-41e39d97c936?source=collection_archive---------3-----------------------#2023-12-09)
- en: Leverage some simple equations to generate related columns in test tables.
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 利用一些简单的方程在测试表中生成相关列。
- en: '[](https://medium.com/@mc12338?source=post_page-----41e39d97c936--------------------------------)[![Matt
    Collins](../Images/b28ac8100d6fb287e3fa6926eec7939a.png)](https://medium.com/@mc12338?source=post_page-----41e39d97c936--------------------------------)[](https://towardsdatascience.com/?source=post_page-----41e39d97c936--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----41e39d97c936--------------------------------)
    [Matt Collins](https://medium.com/@mc12338?source=post_page-----41e39d97c936--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@mc12338?source=post_page-----41e39d97c936--------------------------------)[![Matt
    Collins](../Images/b28ac8100d6fb287e3fa6926eec7939a.png)](https://medium.com/@mc12338?source=post_page-----41e39d97c936--------------------------------)[](https://towardsdatascience.com/?source=post_page-----41e39d97c936--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----41e39d97c936--------------------------------)
    [Matt Collins](https://medium.com/@mc12338?source=post_page-----41e39d97c936--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fd1970f1605f1&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcreate-many-to-one-relationships-between-columns-in-a-synthetic-table-with-pyspark-udfs-41e39d97c936&user=Matt+Collins&userId=d1970f1605f1&source=post_page-d1970f1605f1----41e39d97c936---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----41e39d97c936--------------------------------)
    ·7 min read·Dec 9, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F41e39d97c936&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcreate-many-to-one-relationships-between-columns-in-a-synthetic-table-with-pyspark-udfs-41e39d97c936&user=Matt+Collins&userId=d1970f1605f1&source=-----41e39d97c936---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fd1970f1605f1&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcreate-many-to-one-relationships-between-columns-in-a-synthetic-table-with-pyspark-udfs-41e39d97c936&user=Matt+Collins&userId=d1970f1605f1&source=post_page-d1970f1605f1----41e39d97c936---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----41e39d97c936--------------------------------)
    ·7 分钟阅读·2023年12月9日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F41e39d97c936&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcreate-many-to-one-relationships-between-columns-in-a-synthetic-table-with-pyspark-udfs-41e39d97c936&user=Matt+Collins&userId=d1970f1605f1&source=-----41e39d97c936---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F41e39d97c936&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcreate-many-to-one-relationships-between-columns-in-a-synthetic-table-with-pyspark-udfs-41e39d97c936&source=-----41e39d97c936---------------------bookmark_footer-----------)![](../Images/8f6ff1c8009d98db1eaa04ccfe19ff4e.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F41e39d97c936&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcreate-many-to-one-relationships-between-columns-in-a-synthetic-table-with-pyspark-udfs-41e39d97c936&source=-----41e39d97c936---------------------bookmark_footer-----------)![](../Images/8f6ff1c8009d98db1eaa04ccfe19ff4e.png)'
- en: Image generated with DALL-E 3
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 DALL-E 3 生成的图像
- en: I’ve recently been playing around with Databricks Labs Data Generator to create
    completely synthetic datasets from scratch. As part of this, I’ve looked at building
    sales data around different stores, employees, and customers. As such, I wanted
    to create relationships between the columns I was artificially populating — such
    as mapping employees and customers to a certain store.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 我最近一直在使用 Databricks Labs 数据生成器从头开始创建完全合成的数据集。在此过程中，我研究了围绕不同商店、员工和客户构建销售数据。因此，我想在我人为填充的列之间创建关系，例如将员工和客户映射到某个商店。
- en: Through using PySpark UDFs and a bit of logic, we can generate related columns
    which follow a many-to-one relationship. With a bit of magic, we are even able
    to extend the logic to give some variance in this mapping — like a customer generally
    buying from their local store but sometimes from a different store.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用 PySpark UDF 和一些逻辑，我们可以生成遵循多对一关系的相关列。通过一点魔法，我们甚至可以扩展逻辑，为这种映射提供一些变化——比如客户通常在本地店铺购物，但有时会在其他店铺购物。
- en: '**Use Databricks Labs Data Generator to generate our basic DataFrame**'
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**使用 Databricks Labs 数据生成器生成我们的基本 DataFrame**'
- en: '***Note: You can skip this section if not required!***'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '***注意：如果不需要，可以跳过此部分！***'
- en: First up, we need to create a DataFrame with our first randomly-generated column.
    In our case, we’re going to start with the store, as logically we will have “many
    employees per store” and “many customers repeatedly shopping at a store”.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要创建一个包含我们第一个随机生成列的 DataFrame。在我们的情况下，我们将从店铺开始，因为从逻辑上讲，我们将有“每家店铺多个员工”和“许多客户重复在店铺购物”。
- en: With a Star Schema Data Model in mind, we’re going to start with our Sales Fact
    table — a transactional table which will contain key values for the Sale Id, Store
    Id, Employee Id and Customer Id, the Sale Amount along with some datetime data
    for the purchase. We can then fill out the specifics about the Store, Employee
    and Customer in dimension tables further down the line.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 以星型模式数据模型为基础，我们将从销售事实表开始——这是一个事务表，包含 Sale Id、Store Id、Employee Id 和 Customer
    Id 的关键值，销售金额以及一些购买的日期时间数据。然后我们可以在后续的维度表中填写有关 Store、Employee 和 Customer 的详细信息。
- en: 'We’ll start small — a table with 1000 sales will do. We now need to decide
    how to split these sales up between stores, employees and customers. Let’s suggest
    the following:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从小开始——一个包含 1000 笔销售的表格就可以了。现在我们需要决定如何将这些销售分配到店铺、员工和客户之间。我们建议如下：
- en: Stores = 20
  id: totrans-16
  prefs:
  - PREF_UL
  - PREF_H1
  type: TYPE_NORMAL
  zh: 店铺数 = 20
- en: Employees = 100
  id: totrans-17
  prefs:
  - PREF_UL
  - PREF_H1
  type: TYPE_NORMAL
  zh: 员工数 = 100
- en: Customers = 700
  id: totrans-18
  prefs:
  - PREF_UL
  - PREF_H1
  type: TYPE_NORMAL
  zh: 客户数 = 700
- en: 'We can also say that the sales will be recorded over the course of last month:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以说，销售将记录在上个月的时间段内：
- en: First Sale Date = 2023–11–01
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 首次销售日期 = 2023–11–01
- en: Last Sale Date = 2023–11–30
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后销售日期 = 2023–11–30
- en: The Sale Id needs to be a unique column so we can generate an Id column for
    this. We now need to distribute the 1000 sales across the 20 stores. For simplicity
    we will assume this is random.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: Sale Id 需要是唯一的列，以便我们可以为此生成一个 Id 列。现在我们需要将 1000 笔销售分配到 20 家店铺中。为了简单起见，我们假设这是随机的。
- en: 'Using Databricks Lab Generator we can do this with the following code:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 Databricks Lab 生成器，我们可以用以下代码完成：
- en: Now add some code to record when the sales were made and their amount. To keep
    things simple, we’ll round the timestamp of the sale to the nearest hour.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 现在添加一些代码来记录销售的时间和金额。为了保持简单，我们将销售的时间戳四舍五入到最接近的小时。
- en: To calculate the sale amount, we can use the “expr” parameter in our withColumn
    expression to allow us to generate a random number, with some rules/boundaries.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 为了计算销售金额，我们可以在 withColumn 表达式中使用“expr”参数，以允许我们生成一个随机数，并设定一些规则/边界。
- en: 'In this case, the expression is quite straight-forward: produce a random number
    (between 0 and 1), add 0.1 (ensuring sale values are not 0) and multiply by 350.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，表达式非常简单：生成一个随机数（在 0 和 1 之间），加上 0.1（确保销售值不为 0），然后乘以 350。
- en: 'We’ve got our basic shape for the DataFrame now, so put it all together:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在有了 DataFrame 的基本结构，所以把它们整合在一起：
- en: 'We can create a quick Data Profile to look at the distribution of values in
    the columns:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以创建一个快速的数据概况来查看列中的值分布：
- en: '![](../Images/a82e88e7cd291b288a0986f984fac0b1.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a82e88e7cd291b288a0986f984fac0b1.png)'
- en: 'Image by Author: Data profile generated in Databricks'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片：在 Databricks 中生成的数据概况
- en: We can see that the StoreId distribution is relatively even across the 20 stores,
    with no missing values and averages around the centre as we would expect. The
    same follows for the Timestamps and amount values.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到 StoreId 的分布在 20 家店铺中相对均匀，没有缺失值，且平均值围绕中心，符合预期。时间戳和金额值也是如此。
- en: '**Add a dependent Many-To-One column**'
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**添加一个依赖的多对一列**'
- en: Now we can add our Employee Id column to the DataFrame. We’re done with Databricks
    Lab Data Generator now, so will just use PySpark operations to add columns to
    the DataFrame.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以将 Employee Id 列添加到 DataFrame 中。我们现在完成了 Databricks Lab 数据生成器，所以将只使用 PySpark
    操作来向 DataFrame 中添加列。
- en: 'Stepping back from the code, we want to model this as the following statements:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 从代码中退一步，我们希望将其建模为以下语句：
- en: There are 20 stores.
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有 20 家店铺。
- en: Each store has more than 1 employee.
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每家店铺都有超过 1 名员工。
- en: Each employee works at a single store only.
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个员工仅在一个商店工作。
- en: 'First we need to split the employees between the stores. The following python
    function can be used to do so:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要在商店之间分配员工。可以使用以下 Python 函数来完成：
- en: Now that we have our distribution of employees for each store, let’s start assigning
    Ids!
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经为每个商店分配了员工分布，让我们开始分配 ID 吧！
- en: 'The employeesPerStore list ensures that the employee Ids per store do not overlap.
    We can use this to randomly assign an employee Id to a sale in the table with
    the following equation:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: employeesPerStore 列表确保每个商店的员工 ID 不会重叠。我们可以使用这个列表用以下公式随机分配员工 ID 到表中的销售记录中：
- en: This function currently only works for a single value — we need to put this
    into something that a PySpark DataFrame can work with (functionally, and quickly!)
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 这个函数目前只适用于单个值——我们需要将它放入 PySpark DataFrame 可以使用的形式（功能上，并且要快速！）
- en: 'We can pass PySpark UDFs to the *withColumn* method, so let’s reformat this
    logic into a function and set it to a UDF:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将 PySpark UDF 传递给 *withColumn* 方法，因此让我们将此逻辑重格式化为一个函数，并将其设置为 UDF：
- en: 'Now call this as a new column in the DataFrame:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 现在在 DataFrame 中调用它作为一个新列：
- en: We can quickly test this looks correct by using the Visualisation tool in Databricks
    to see the distinct count of Employee Ids per Store Id. This is my preference
    but you could also use group by logic or other plotting modules, if desired.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过使用 Databricks 中的可视化工具来快速测试这是否正确，查看每个商店 ID 的员工 ID 不同计数。这是我的首选方法，但你也可以使用分组逻辑或其他绘图模块，如果需要的话。
- en: '![](../Images/159ea138f058375b07f161e8b09a7df3.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/159ea138f058375b07f161e8b09a7df3.png)'
- en: 'Image by Author: Distinct count of Employee Ids per Store'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 作者图片：每个商店的员工 ID 不同计数
- en: '**Important Note:** This logic allows for employees to be **missed** from the
    results. This means that it is possible for an employee to make 0 sales, and thus
    not be included in the DataFrame. We’ll look at how to ensure all customers have
    sales recorded against them in the next section.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '**重要提示：** 这个逻辑可能会导致员工在结果中**遗漏**。这意味着可能会有员工没有销售记录，从而没有被包含在 DataFrame 中。我们将在下一节中查看如何确保所有客户都有销售记录。'
- en: Add the customers column
  id: totrans-48
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 添加客户列
- en: The customers column is a bit different… while our use-case suggests it is common
    for a customer to shop at a single store multiple times, it is entirely possible
    that they got to a different store one day. How do we model this?
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 客户列稍有不同……虽然我们的使用案例表明客户在一个商店多次购物是常见的，但他们有一天去不同的商店也是完全可能的。我们如何建模这一点？
- en: 'We’ve got the starting points with the work done for our employees column,
    so can repeat the *get_employees* function and UDF logic for customers as below:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经完成了员工列的起始工作，因此可以重复使用 *get_employees* 函数和 UDF 逻辑来处理客户，如下所示：
- en: 'We’ve again potentially missed a few customers off here. Here are a few approaches
    to rectify this:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 我们再次可能遗漏了一些客户。以下是一些纠正方法：
- en: Recalculate in *while* loop until you converge on a DataFrame which contains
    all customers (inefficient, costly, could run indefinitely)
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 *while* 循环中重新计算，直到收敛到一个包含所有客户的 DataFrame（效率低，成本高，可能会无限期运行）
- en: Randomly update customer Ids in *while* loop until all customers in DataFrame
    (requires logic to only overwrite same stores, could also run indefinitely)
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 *while* 循环中随机更新客户 ID 直到 DataFrame 中所有客户都有（需要逻辑只覆盖相同商店，可能也会无限期运行）
- en: Return a list of all customer Ids with more than 1 record in the sales table,
    and randomly overwrite until all missing Ids are added (also needs logic for overwriting
    customers in same store, may also require *while* loop logic)
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 返回销售表中记录超过 1 条的所有客户 ID 列表，并随机覆盖直到所有缺失的 ID 都被添加（还需要逻辑覆盖相同商店的客户，可能还需要 *while*
    循环逻辑）
- en: '**Reverse the process and start with employees. This ensures each employee
    is randomly assigned to rows. We can then use the mapping and apply the store
    Id.**'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**反转过程，从员工开始。这确保每个员工被随机分配到行中。然后我们可以使用映射并应用商店 ID。**'
- en: Hopefully it is clear why the last option is the lowest effort to compute —
    we have all the code required so just need to reformat things slightly.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 希望你能明白最后一个选项为什么是计算工作量最小的——我们拥有所需的所有代码，只需要稍微重新格式化一下。
- en: 'Our new scripts looks as follows:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的新脚本如下：
- en: '![](../Images/b5bc57b56200b1ece8e679b6be856c58.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b5bc57b56200b1ece8e679b6be856c58.png)'
- en: 'Image by Author: Databricks Data Profile for the new DataFrame'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 作者图片：新 DataFrame 的 Databricks 数据概况
- en: Adding Randomness to the customers
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 向客户添加随机性
- en: 'What we now need is a bit of randomness, which we need to define. For our example,
    let’s say that each customer has a 90% chance of shopping at the usual store (the
    “local” store). If we do not need all customers to be returned in the results
    set, we can simply adjust our *customers_udf* as follows, and use *df2*:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在需要的是一些随机性，我们需要定义它。以我们的例子为例，假设每个客户有90%的机会在通常的商店（“本地”商店）购物。如果我们不需要将所有客户返回在结果集中，我们可以简单地调整我们的*customers_udf*，并使用*df2*：
- en: The logic involves using the *random.choices* function to supply a weighted
    list and return a single value.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 该逻辑涉及使用*random.choices*函数来提供加权列表并返回单一值。
- en: To compute the weighted list, we have the weight of our “local” store for the
    customer, in this case 90%, so need to assign the remaining 10% to the other stores,
    in this case 19 stores. The probability of each other store being selected will
    therefore be 10/19 = 0.526%. We can populate an array with these percentages,
    which would look something like the following:[0.526,0.526,0.526,…,90,0.526,…0.526]
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 要计算加权列表，我们需要将客户的“本地”商店的权重设置为90%，因此需要将剩余的10%分配给其他商店，这里有19家商店。因此，每个其他商店被选择的概率将是10/19
    = 0.526%。我们可以用这些百分比填充一个数组，数组看起来类似于：[0.526,0.526,0.526,…,90,0.526,…0.526]
- en: Passing this into random.choices, we then randomly select a store Id from the
    list with the corresponding weights and use this as the input for the *customer_id*
    variable, as before.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 将其传递给`random.choices`，我们就可以根据对应的权重从列表中随机选择一个商店ID，并将其用作*customer_id*变量的输入，如前所述。
- en: '**Note:** The output of random.choices returns a list (as you can request k
    results), so access the 0th element of the list to get the store_id as an integer
    value.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意：** `random.choices`的输出返回一个列表（因为你可以请求k个结果），所以访问列表的第0个元素以获取store_id作为整数值。'
- en: 'If we need to combine this logic with a DataFrame including all customers,
    we can reverse the process slightly. The weights logic is still valid so we can
    just plug this into randomly select a store and return this as the result:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们需要将此逻辑与包含所有客户的 DataFrame 结合使用，我们可以稍微逆转该过程。权重逻辑仍然有效，所以我们可以将其插入以随机选择商店并将其作为结果返回：
- en: '![](../Images/abaf37bd9c9efc775dda58e0503258df.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/abaf37bd9c9efc775dda58e0503258df.png)'
- en: 'Image by Author: Sample of final DataFrame in Databricks'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图像：Databricks中的最终 DataFrame 示例
- en: Conclusion
  id: totrans-69
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: There we have it! A synthetically created DataFrame with both strict and loose
    mappings between columns. You can now progress the next steps to populate related
    tables which may contain more descriptive information, such as dimension tables
    of store names, addresses, employee names, roles, etc. This can also be done using
    Databricks Labs Data Generator or any other tool/process you are comfortable with.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 就这样！一个合成创建的 DataFrame，其中列之间有严格和松散的映射。你现在可以进行下一步，填充可能包含更多描述性信息的相关表，例如商店名称、地址、员工姓名、角色等的维度表。这也可以使用
    Databricks Labs 数据生成器或任何其他你熟悉的工具/过程来完成。
- en: There are some great examples on the Databricks Labs Data Generator [GitHub
    Repo](https://github.com/databrickslabs/dbldatagen) along with documentation,
    so please do take a look at this if you are curious to learn more.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Databricks Labs 数据生成器的[GitHub Repo](https://github.com/databrickslabs/dbldatagen)上有一些很棒的示例和文档，如果你想了解更多内容，请查看一下。
- en: All of my code can be accessed from the following [GitHub Repo](https://github.com/MattPCollins/Dbldatagen-Related-Tables).
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 我所有的代码可以从以下[GitHub Repo](https://github.com/MattPCollins/Dbldatagen-Related-Tables)访问。
- en: If you have any thoughts, comments or alternatives to this demo, please reach
    out in the comments. Thanks!
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你对这个演示有任何想法、评论或替代方案，请在评论中联系我。谢谢！
