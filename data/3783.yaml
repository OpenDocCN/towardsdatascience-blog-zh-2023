- en: 'Understanding Deep Learning Optimizers: Momentum, AdaGrad, RMSProp & Adam'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解深度学习优化器：动量、AdaGrad、RMSProp与Adam
- en: 原文：[https://towardsdatascience.com/understanding-deep-learning-optimizers-momentum-adagrad-rmsprop-adam-e311e377e9c2?source=collection_archive---------0-----------------------#2023-12-30](https://towardsdatascience.com/understanding-deep-learning-optimizers-momentum-adagrad-rmsprop-adam-e311e377e9c2?source=collection_archive---------0-----------------------#2023-12-30)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/understanding-deep-learning-optimizers-momentum-adagrad-rmsprop-adam-e311e377e9c2?source=collection_archive---------0-----------------------#2023-12-30](https://towardsdatascience.com/understanding-deep-learning-optimizers-momentum-adagrad-rmsprop-adam-e311e377e9c2?source=collection_archive---------0-----------------------#2023-12-30)
- en: Gain intuition behind acceleration training techniques in neural networks
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 了解神经网络中的加速训练技术的直观感受
- en: '[](https://medium.com/@slavahead?source=post_page-----e311e377e9c2--------------------------------)[![Vyacheslav
    Efimov](../Images/db4b02e75d257063e8e9d3f1f75d9d6d.png)](https://medium.com/@slavahead?source=post_page-----e311e377e9c2--------------------------------)[](https://towardsdatascience.com/?source=post_page-----e311e377e9c2--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----e311e377e9c2--------------------------------)
    [Vyacheslav Efimov](https://medium.com/@slavahead?source=post_page-----e311e377e9c2--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@slavahead?source=post_page-----e311e377e9c2--------------------------------)[![Vyacheslav
    Efimov](../Images/db4b02e75d257063e8e9d3f1f75d9d6d.png)](https://medium.com/@slavahead?source=post_page-----e311e377e9c2--------------------------------)[](https://towardsdatascience.com/?source=post_page-----e311e377e9c2--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----e311e377e9c2--------------------------------)
    [Vyacheslav Efimov](https://medium.com/@slavahead?source=post_page-----e311e377e9c2--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fc8a0ca9d85d8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-deep-learning-optimizers-momentum-adagrad-rmsprop-adam-e311e377e9c2&user=Vyacheslav+Efimov&userId=c8a0ca9d85d8&source=post_page-c8a0ca9d85d8----e311e377e9c2---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----e311e377e9c2--------------------------------)
    ·8 min read·Dec 30, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fe311e377e9c2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-deep-learning-optimizers-momentum-adagrad-rmsprop-adam-e311e377e9c2&user=Vyacheslav+Efimov&userId=c8a0ca9d85d8&source=-----e311e377e9c2---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fc8a0ca9d85d8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-deep-learning-optimizers-momentum-adagrad-rmsprop-adam-e311e377e9c2&user=Vyacheslav+Efimov&userId=c8a0ca9d85d8&source=post_page-c8a0ca9d85d8----e311e377e9c2---------------------post_header-----------)
    发布于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----e311e377e9c2--------------------------------)
    ·8 min read·Dec 30, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fe311e377e9c2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-deep-learning-optimizers-momentum-adagrad-rmsprop-adam-e311e377e9c2&user=Vyacheslav+Efimov&userId=c8a0ca9d85d8&source=-----e311e377e9c2---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fe311e377e9c2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-deep-learning-optimizers-momentum-adagrad-rmsprop-adam-e311e377e9c2&source=-----e311e377e9c2---------------------bookmark_footer-----------)![](../Images/665f9f91827ad58ea605c3ef0704e2e8.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fe311e377e9c2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-deep-learning-optimizers-momentum-adagrad-rmsprop-adam-e311e377e9c2&source=-----e311e377e9c2---------------------bookmark_footer-----------)![](../Images/665f9f91827ad58ea605c3ef0704e2e8.png)'
- en: Introduction
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: 'D**eep learning** made a gigantic step in the world of artificial intelligence.
    At the current moment, neural networks outperform other types of algorithms on
    non-tabular data: images, videos, audio, etc. Deep learning models usually have
    a strong complexity and come up with millions or even billions of trainable parameters.
    That is why it is essential in the modern era to use acceleration techniques to
    reduce training time.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '**深度学习**在人工智能领域迈出了巨大的一步。目前，神经网络在非表格数据（如图像、视频、音频等）上优于其他类型的算法。深度学习模型通常具有较强的复杂性，并且拥有数百万甚至数十亿个可训练的参数。因此，在现代时代，使用加速技术来减少训练时间是至关重要的。'
- en: One of the most common algorithms performed during training is **backpropagation**
    consisting of changing weights of a neural network in respect to a given loss
    function. Backpropagation is usually performed via **gradient descent** which
    tries to converge loss function to a local minimum step by step.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练过程中最常见的算法之一是**反向传播**，它包括根据给定的损失函数调整神经网络的权重。反向传播通常通过**梯度下降**进行，该方法试图一步一步地将损失函数收敛到局部最小值。
- en: As it turns out, naive gradient descent is not usually a preferable choice for
    training a deep network because of its slow convergence rate. This became a motivation
    for researchers to develop optimization algorithms which accelerate gradient descent.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 事实证明，简单的梯度下降通常不是训练深度网络的首选，因为其收敛速度较慢。这激励了研究人员开发加速梯度下降的优化算法。
- en: Before reading this article, it is highly recommended that you are familiar
    with the **exponentially moving average** concept which is used in optimization
    algorithms. If not, you can refer to the article below.
  id: totrans-12
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 在阅读本文之前，强烈建议你了解**指数移动平均**的概念，它在优化算法中被使用。如果不了解，可以参考下面的文章。
- en: '[](/intuitive-explanation-of-exponential-moving-average-2eb9693ea4dc?source=post_page-----e311e377e9c2--------------------------------)
    [## Intuitive Explanation of Exponential Moving Average'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/intuitive-explanation-of-exponential-moving-average-2eb9693ea4dc?source=post_page-----e311e377e9c2--------------------------------)
    [## 指数移动平均的直观解释'
- en: Understand the logic behind the fundamental algorithm used inside the gradient
    descent
  id: totrans-14
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 理解梯度下降中使用的基本算法的逻辑
- en: towardsdatascience.com](/intuitive-explanation-of-exponential-moving-average-2eb9693ea4dc?source=post_page-----e311e377e9c2--------------------------------)
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '[towardsdatascience.com](/intuitive-explanation-of-exponential-moving-average-2eb9693ea4dc?source=post_page-----e311e377e9c2--------------------------------)'
- en: Gradient descent
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 梯度下降
- en: 'Gradient descent is the simplest optimization algorithm which computes gradients
    of loss function with respect to model weights and updates them by using the following
    formula:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度下降是最简单的优化算法，它计算损失函数相对于模型权重的梯度，并使用以下公式更新它们。
- en: '![](../Images/d32e845ebbb9169b66dfac2195651c04.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d32e845ebbb9169b66dfac2195651c04.png)'
- en: Gradient descent equation. w is the weight vector, dw is the gradient of w,
    α is the learning rate, t is the iteration number
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度下降方程。w 是权重向量，dw 是 w 的梯度，α 是学习率，t 是迭代次数。
- en: To understand why gradient descent converges slowly, let us look at the example
    below of a **ravine** where a given function of two variables should be minimised.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解为什么梯度下降收敛缓慢，让我们看下面的**峡谷**示例，其中一个包含两个变量的函数应该被最小化。
- en: '![](../Images/45fe2806f8f0dbaf9ec84762d1c2f2cf.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/45fe2806f8f0dbaf9ec84762d1c2f2cf.png)'
- en: Example of an optimization problem with gradient descent in a ravine area. The
    starting point is depicted in blue and the local minimum is shown in black.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在峡谷区域中使用梯度下降的优化问题示例。起始点用蓝色表示，局部最小值用黑色表示。
- en: '**A ravine** is an area where the surface is much more steep in one dimension
    than in another'
  id: totrans-23
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**峡谷**是一个在一个维度上表面比另一个维度更陡峭的区域。'
- en: From the image, we can see that the starting point and the local minima have
    different horizontal coordinates and are almost equal vertical coordinates. Using
    gradient descent to find the local minima will likely make the loss function slowly
    oscillate towards vertical axes. These bounces occur because gradient descent
    does not store any history about its previous gradients making gradient steps
    more undeterministic on each iteration. This example can be generalized to a higher
    number of dimensions.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 从图像中可以看出，起始点和局部最小值具有不同的横坐标，并且几乎相等的纵坐标。使用梯度下降寻找局部最小值可能会导致损失函数沿垂直轴慢慢振荡。这些跳跃发生是因为梯度下降不存储任何关于先前梯度的历史，使得每次迭代的梯度步长更加不确定。这个例子可以推广到更高维度。
- en: As a consequence, it would be risky to use a large learning rate as it could
    lead to disconvergence.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，使用较大的学习率是有风险的，因为这可能导致不收敛。
- en: Momentum
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 动量
- en: Based on the example above, it would be desirable to make a loss function performing
    larger steps in the horizontal direction and smaller steps in the vertical. This
    way, the convergence would be much faster. This effect is exactly achieved by
    Momentum.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 基于上述示例，期望在水平方向上使损失函数执行较大的步长，在垂直方向上执行较小的步长。这样，收敛速度会更快。这正是动量所实现的效果。
- en: 'Momentum uses a pair of equations at each iteration:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 动量在每次迭代时使用一对方程：
- en: '![](../Images/1deb4e4c81fb9ccb3870e1987c5e3597.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1deb4e4c81fb9ccb3870e1987c5e3597.png)'
- en: Momentum equations
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 动量法公式
- en: The first formula uses an exponentially moving average for gradient values *dw*.
    Basically, it is done to store trend information about a set of previous gradient
    values. The second equation performs the normal gradient descent update using
    the computed moving average value on the current iteration. α is the learning
    rate of the algorithm.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个公式使用指数移动平均来处理梯度值*dw*。基本上，这是为了存储有关一组先前梯度值的趋势信息。第二个公式在当前迭代中使用计算出的移动平均值执行正常的梯度下降更新。α是算法的学习率。
- en: Momentum can be particularly useful for cases like the above. Imagine we have
    computed gradients on every iteration like in the picture above. Instead of simply
    using them for updating weights, we take several past values and literaturally
    perform update in the averaged direction.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 动量法特别适用于上述情况。假设我们在每次迭代中计算了梯度，如上图所示。我们不仅简单地使用这些梯度来更新权重，还取几个过去的值，并在平均方向上进行更新。
- en: '![](../Images/027d92c3a7351463a50ea5572741f48d.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/027d92c3a7351463a50ea5572741f48d.png)'
- en: 'Sebastian Ruder concisely describes the effect of Momentum in his [paper](https://arxiv.org/pdf/1609.04747.pdf):
    “The momentum term increases for dimensions whose gradients point in the same
    directions and reduces updates for dimensions whose gradients change directions.
    As a result, we gain faster convergence and reduced oscillation”.'
  id: totrans-34
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: Sebastian Ruder 在他的 [论文](https://arxiv.org/pdf/1609.04747.pdf) 中简洁地描述了动量法的效果：“动量项对于梯度方向一致的维度增加，对于梯度方向改变的维度减少更新。因此，我们获得了更快的收敛速度和减少的振荡。”
- en: As a result, updates performed by Momentum might look like in the figure below.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，动量法执行的更新可能如下图所示。
- en: '![](../Images/fa9de8bee2c37be9dba9294007916497.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fa9de8bee2c37be9dba9294007916497.png)'
- en: Optimization with Momentum
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 动量法优化
- en: In practice, Momentum usually converges much faster than gradient descent. With
    Momentum, there are also fewer risks in using larger learning rates, thus accelerating
    the training process.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，动量法通常比梯度下降法收敛更快。使用动量法时，使用较大学习率的风险也较小，从而加速了训练过程。
- en: In Momentum, it is recommended to choose β close to 0.9.
  id: totrans-39
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 在动量法中，建议选择接近0.9的β值。
- en: AdaGrad (Adaptive Gradient Algorithm)
  id: totrans-40
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: AdaGrad（自适应梯度算法）
- en: AdaGrad is another optimizer with the motivation to adapt the learning rate
    to computed gradient values. There might occur situations when during training,
    one component of the weight vector has very large gradient values while another
    one has extremely small. **This happens especially in cases when an infrequent
    model parameter appears to have a low influence on predictions**. It is worth
    noting that with frequent parameters such problems do not usually occur as, for
    their update, the model uses a lot of prediction signals. Since lots of information
    from signals is taken into account for gradient computation, gradients are usually
    adequate and represent a correct direction towards the local minimum. However,
    this is not the case for rare parameters which can lead to extremely large and
    unstable gradients. The same problem can occur with sparse data where there is
    too little information about certain features.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: AdaGrad 是另一种优化器，其动机是根据计算的梯度值调整学习率。在训练过程中，可能会出现权重向量的一个组件具有非常大的梯度值，而另一个组件具有极小的梯度值的情况。**这种情况尤其发生在不常见的模型参数对预测的影响较小时**。值得注意的是，对于频繁出现的参数，这类问题通常不会发生，因为模型在更新它们时会使用大量的预测信号。由于在梯度计算中考虑了大量信号的信息，梯度通常是适当的，并且代表了朝向局部最小值的正确方向。然而，对于稀有参数，这种情况并非如此，可能导致极大的不稳定梯度。相同的问题也可能出现在稀疏数据中，其中某些特征的信息过少。
- en: AdaGrad deals with the aforementioned problem **by independently adapting the
    learning rate for each weight component**. If gradients corresponding to a certain
    weight vector component are large, then the respective learning rate will be small.
    Inversely, for smaller gradients, the learning rate will be bigger. This way,
    Adagrad deals with vanishing and exploding gradient problems.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: AdaGrad 通过**为每个权重组件独立调整学习率**来解决上述问题。如果与某个权重向量组件相关的梯度较大，则相应的学习率会较小。相反，对于较小的梯度，学习率会较大。通过这种方式，Adagrad
    处理了梯度消失和爆炸的问题。
- en: Under the hood, Adagrad accumulates element-wise squares *dw²* of gradients
    from all previous iterations. During weight update, instead of using normal learning
    rate α, AdaGrad scales it by dividing α by the square root of the accumulated
    gradients *√vₜ*. Additionally, a small positive term ε is added to the denominator
    to prevent potential division by zero.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在内部，Adagrad 累积所有先前迭代的梯度的元素平方 *dw²*。在权重更新期间，AdaGrad 不使用正常的学习率 α，而是通过将 α 除以累积梯度的平方根
    *√vₜ* 来缩放它。此外，还向分母中添加了一个小的正数 ε，以防止潜在的除零错误。
- en: '![](../Images/8e0f659992ad29fb50e113b0daf3571d.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8e0f659992ad29fb50e113b0daf3571d.png)'
- en: AdaGrad equations
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: AdaGrad 方程
- en: 'The greatest advantage of AdaGrad is that there is no longer a need to manually
    adjust the learning rate as it adapts itself during training. Nevertheless, there
    is a negative side of AdaGrad: the learning rate constantly decays with the increase
    of iterations (the learning rate is always divided by a positive cumulative number).
    Therefore, the algorithm tends to converge slowly during the last iterations where
    it becomes very low.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: AdaGrad 的最大优点是不再需要手动调整学习率，因为它在训练过程中会自行适应。然而，AdaGrad 也有负面的一面：学习率随着迭代次数的增加不断衰减（学习率总是被一个正的累积数除）。因此，该算法在最后几次迭代中趋于慢收敛，因为学习率变得非常低。
- en: '![](../Images/b0fc1e109cdcd4288a12b6abe3d8f08e.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b0fc1e109cdcd4288a12b6abe3d8f08e.png)'
- en: Optimization with AdaGrad
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 AdaGrad 进行优化
- en: RMSProp (Root Mean Square Propagation)
  id: totrans-49
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: RMSProp（均方根传播）
- en: RMSProp was elaborated as an improvement over AdaGrad which tackles the issue
    of learning rate decay. Similarly to AdaGrad, RMSProp uses a pair of equations
    for which the weight update is absolutely the same.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: RMSProp 被详细阐述为对 AdaGrad 的改进，解决了学习率衰减的问题。与 AdaGrad 类似，RMSProp 使用一对方程，其权重更新完全相同。
- en: '![](../Images/bdd2b8950a31350fc6aeb1330e1aaf2d.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bdd2b8950a31350fc6aeb1330e1aaf2d.png)'
- en: RMSProp equations
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: RMSProp 方程
- en: However, instead of storing a cumulated sum of squared gradients *dw²* for vₜ,
    the exponentially moving average is calculated for squared gradients *dw²*. Experiments
    show that RMSProp generally converges faster than AdaGrad because, with the exponentially
    moving average, it puts more emphasis on recent gradient values rather than equally
    distributing importance between all gradients by simply accumulating them from
    the first iteration. Furthermore, compared to AdaGrad, the learning rate in RMSProp
    does not always decay with the increase of iterations making it possible to adapt
    better in particular situations.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，与其为 vₜ 存储平方梯度的累积和 *dw²*，不如计算平方梯度 *dw²* 的指数移动平均。实验表明，由于指数移动平均，RMSProp 通常比
    AdaGrad 收敛更快，因为它更重视最近的梯度值，而不是通过从第一次迭代开始简单地累积所有梯度来平均分配重要性。此外，与 AdaGrad 相比，RMSProp
    的学习率并不总是随着迭代次数的增加而衰减，这使得它在特定情况下能够更好地适应。
- en: '![](../Images/9d0dd019424624f624dd0940c1a21e0a.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9d0dd019424624f624dd0940c1a21e0a.png)'
- en: Optimization with RMSProp
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 RMSProp 进行优化
- en: In RMSProp, it is recommended to choose β close to 1.
  id: totrans-56
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 在 RMSProp 中，建议选择接近 1 的 β。
- en: '**Why not to simply use a squared gradient for v**ₜ **instead of the exponentially
    moving average?**'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '**为什么不简单地使用平方梯度 v**ₜ **而不是指数移动平均？**'
- en: 'It is known that the exponentially moving average distributes higher weights
    to recent gradient values. This is one of the reasons why RMSProp adapts quickly.
    But would not it be better if instead of the moving average we only took into
    account the last square gradient at every iteration (vₜ *= dw²*)? As it turns
    out, the update equation would transform in the following manner:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 已知指数移动平均将更高的权重分配给最近的梯度值。这是 RMSProp 快速适应的原因之一。但如果我们只考虑每次迭代的最后一个平方梯度（vₜ *= dw²*），而不是使用移动平均，是否会更好？事实证明，更新方程将变成以下形式：
- en: '![](../Images/921431c4faa6b96309222b853c12f650.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/921431c4faa6b96309222b853c12f650.png)'
- en: Transformation of RMSProp equations when using a squared gradient instead of
    the exponentially moving average
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 使用平方梯度代替指数移动平均时 RMSProp 方程的转换
- en: 'As we can see, the resulting formula looks very similar to the one used in
    the gradient descent. However, instead of using a normal gradient value for the
    update, we are now using the sign of the gradient:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，结果公式与梯度下降中使用的公式非常相似。然而，我们现在使用的是梯度的符号，而不是正常的梯度值来进行更新：
- en: If *dw > 0*, then a weight *w* is decreased by α.
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果 *dw > 0*，则权重 *w* 由 α 减少。
- en: If *dw < 0*, then a weight *w* is increased by α.
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果 *dw < 0*，则权重 *w* 由 α 增加。
- en: To sum it up, if vₜ *= dw²*, then model weights can only be changed by ±α. Though
    this approach works sometimes, it is still not flexible the algorithm becomes
    extremely sensitive to the choice of α and absolute magnitudes of gradient are
    ignored which can make the method tremendously slow to converge. A positive aspect
    about this algorithm is the fact only a single bit is required to store signs
    of gradietns which can be handy in distributed computations with strict memory
    requirements.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 总而言之，如果 vₜ *= dw²*，那么模型权重只能通过 ±α 来改变。尽管这种方法在某些情况下有效，但它的灵活性较差，算法对 α 的选择变得极其敏感，并且忽略了梯度的绝对大小，这可能导致方法收敛速度极其缓慢。该算法的一个积极方面是仅需一个位来存储梯度的符号，这在有严格内存要求的分布式计算中非常方便。
- en: Adam (Adaptive Moment Estimation)
  id: totrans-65
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Adam（自适应矩估计）
- en: For the moment, Adam is the most famous optimization algorithm in deep learning.
    At a high level, Adam combines Momentum and RMSProp algorithms. To achieve it,
    it simply keeps track of the exponentially moving averages for computed gradients
    and squared gradients respectively.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，Adam 是深度学习中最著名的优化算法。从高层次来看，Adam 结合了动量和 RMSProp 算法。为了实现这一点，它分别跟踪计算梯度和平方梯度的指数移动平均值。
- en: '![](../Images/36f3227cbddd28917346c2d951b40e27.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/36f3227cbddd28917346c2d951b40e27.png)'
- en: Adam equations
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: Adam 方程
- en: Furthermore, it is possible to use bias correction for moving averages for a
    more precise approximation of gradient trend during the first several iterations.
    The experiments show that Adam adapts well to almost any type of neural network
    architecture taking the advantages of both Momentum and RMSProp.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，可以对移动平均进行偏差修正，以更精确地估算前几次迭代中的梯度趋势。实验表明，Adam 对几乎任何类型的神经网络架构适应良好，兼具动量和 RMSProp
    的优点。
- en: '![](../Images/820a86fef66786198d018b7f3bd3b4de.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/820a86fef66786198d018b7f3bd3b4de.png)'
- en: Optimization with Adam
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: Adam优化
- en: According to the [Adam paper](https://arxiv.org/pdf/1412.6980.pdf), good default
    values for hyperparameters are β₁ = 0.9, β₂ = 0.999, ε = 1e-8.
  id: totrans-72
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 根据 [Adam 论文](https://arxiv.org/pdf/1412.6980.pdf)，超参数的良好默认值为 β₁ = 0.9，β₂ = 0.999，ε
    = 1e-8。
- en: Conclusion
  id: totrans-73
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: We have looked at different optimization algorithms in neural networks. Considered
    as a combination of Momentum and RMSProp, Adam is the most superior of them which
    robustly adapts to large datasets and deep networks. Moreover, it has a straightforward
    implementation and little memory requirements making it a preferable choice in
    the majority of situations.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经研究了神经网络中的不同优化算法。Adam 被认为是动量（Momentum）和 RMSProp 的结合体，它在适应大规模数据集和深层网络方面表现最为出色。此外，它实现简单且内存需求少，使其在大多数情况下成为首选。
- en: Resources
  id: totrans-75
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 资源
- en: '[An overview of gradient descent optimization algorithms](https://arxiv.org/pdf/1609.04747.pdf)'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[梯度下降优化算法概述](https://arxiv.org/pdf/1609.04747.pdf)'
- en: '[Adam: A Method For Stochastic Optimization](https://arxiv.org/pdf/1412.6980.pdf)'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Adam：一种随机优化方法](https://arxiv.org/pdf/1412.6980.pdf)'
- en: '*All images unless otherwise noted are by the author*'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '*除非另有说明，否则所有图片均由作者提供*'
