- en: 'Streamlining Serverless ML Inference: Unleashing Candle Framework’s Power in
    Rust'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 精简无服务器 ML 推理：释放 Candle 框架在 Rust 中的力量
- en: 原文：[https://towardsdatascience.com/streamlining-serverless-ml-inference-unleashing-candle-frameworks-power-in-rust-c6775d558545?source=collection_archive---------4-----------------------#2023-12-21](https://towardsdatascience.com/streamlining-serverless-ml-inference-unleashing-candle-frameworks-power-in-rust-c6775d558545?source=collection_archive---------4-----------------------#2023-12-21)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/streamlining-serverless-ml-inference-unleashing-candle-frameworks-power-in-rust-c6775d558545?source=collection_archive---------4-----------------------#2023-12-21](https://towardsdatascience.com/streamlining-serverless-ml-inference-unleashing-candle-frameworks-power-in-rust-c6775d558545?source=collection_archive---------4-----------------------#2023-12-21)
- en: Building a lean and robust model serving layer for vector embedding and search
    with Hugging Face’s new Candle Framework
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 Hugging Face 的新 Candle 框架构建一个精简且强大的模型服务层，用于向量嵌入和搜索
- en: '[](https://medium.com/@alon.agmon?source=post_page-----c6775d558545--------------------------------)[![Alon
    Agmon](../Images/c64e33ca886da4f4984c96acc7116a4d.png)](https://medium.com/@alon.agmon?source=post_page-----c6775d558545--------------------------------)[](https://towardsdatascience.com/?source=post_page-----c6775d558545--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----c6775d558545--------------------------------)
    [Alon Agmon](https://medium.com/@alon.agmon?source=post_page-----c6775d558545--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@alon.agmon?source=post_page-----c6775d558545--------------------------------)[![Alon
    Agmon](../Images/c64e33ca886da4f4984c96acc7116a4d.png)](https://medium.com/@alon.agmon?source=post_page-----c6775d558545--------------------------------)[](https://towardsdatascience.com/?source=post_page-----c6775d558545--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----c6775d558545--------------------------------)
    [Alon Agmon](https://medium.com/@alon.agmon?source=post_page-----c6775d558545--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fbcd1e3126cdc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fstreamlining-serverless-ml-inference-unleashing-candle-frameworks-power-in-rust-c6775d558545&user=Alon+Agmon&userId=bcd1e3126cdc&source=post_page-bcd1e3126cdc----c6775d558545---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----c6775d558545--------------------------------)
    ·14 min read·Dec 21, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fc6775d558545&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fstreamlining-serverless-ml-inference-unleashing-candle-frameworks-power-in-rust-c6775d558545&user=Alon+Agmon&userId=bcd1e3126cdc&source=-----c6775d558545---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fbcd1e3126cdc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fstreamlining-serverless-ml-inference-unleashing-candle-frameworks-power-in-rust-c6775d558545&user=Alon+Agmon&userId=bcd1e3126cdc&source=post_page-bcd1e3126cdc----c6775d558545---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----c6775d558545--------------------------------)
    · 14分钟阅读 · 2023年12月21日 [](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fc6775d558545&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fstreamlining-serverless-ml-inference-unleashing-candle-frameworks-power-in-rust-c6775d558545&user=Alon+Agmon&userId=bcd1e3126cdc&source=-----c6775d558545---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc6775d558545&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fstreamlining-serverless-ml-inference-unleashing-candle-frameworks-power-in-rust-c6775d558545&source=-----c6775d558545---------------------bookmark_footer-----------)![](../Images/b1a030ea4ca7bbcfd13b2a181a02c506.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc6775d558545&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fstreamlining-serverless-ml-inference-unleashing-candle-frameworks-power-in-rust-c6775d558545&source=-----c6775d558545---------------------bookmark_footer-----------)![](../Images/b1a030ea4ca7bbcfd13b2a181a02c506.png)'
- en: Photo by [Clay Banks](https://unsplash.com/@claybanks?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由 [Clay Banks](https://unsplash.com/@claybanks?utm_source=medium&utm_medium=referral)
    提供，发布于 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: 1\. Intro
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 1\. 介绍
- en: The incredible progress in AI research and tooling over the past decade has
    given rise to more accurate and more reliable machine learning models, as well
    as to libraries and frameworks that make it increasingly simpler to integrate
    AI capabilities into existing applications.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去十年中，AI 研究和工具的惊人进展催生了更准确、更可靠的机器学习模型，以及使将 AI 功能集成到现有应用程序中变得越来越简单的库和框架。
- en: However, one area that still remains a considerable challenge in demanding production
    environments is inference at scale. Suppose, for example, that we have a simple
    search service that receives a few keywords, and then uses a language model in
    order to embed them in a vector and search for similar texts or documents in some
    vector database. This is quite a popular use case, and also a central part of
    RAG architecture which is commonly used to apply generative AI on domain specific
    knowledge and data.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在要求高的生产环境中，推理规模仍然是一个相当大的挑战。例如，假设我们有一个简单的搜索服务，它接收几个关键词，然后使用语言模型将其嵌入到向量中，并在某个向量数据库中搜索类似的文本或文档。这是一个相当流行的用例，也是
    RAG 架构的核心部分，RAG 架构通常用于将生成型 AI 应用于特定领域的知识和数据。
- en: 'In itself, this seems like a relatively straightforward use case to implement.
    There are many open-source language models and model hubs that we can use as embedding
    models in a few lines of code. If we further assume that the number of vectors
    we need to store and query is relatively moderate (e.g., less than 1M) then there
    are plenty of simple options for vector storage and search: from plain in-memory
    storage to databases such as Postgres, Redis, or Elastic.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 从本身来看，这似乎是一个相对简单的实现用例。我们可以使用许多开源语言模型和模型中心，几行代码就可以用作嵌入模型。如果我们进一步假设需要存储和查询的向量数量相对适中（例如少于
    1M），那么有很多简单的向量存储和搜索选项：从纯内存存储到数据库如 Postgres、Redis 或 Elastic。
- en: But, what if our service is required to serve thousands or hundreds of thousands
    of requests per second? what if we need to maintain a relatively low — sub-second
    — latency on every request? What if we need to scale out fast to serve bursts
    of requests?
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 但，如果我们的服务需要每秒处理数千或数十万的请求怎么办？如果我们需要在每个请求上保持相对较低的 — 毫秒级 — 延迟呢？如果我们需要快速扩展以应对请求的高峰怎么办？
- en: Although our use case is indeed quite simple, the scale and load requirements
    certainly make it a challenge. Scalable high throughput systems are usually based
    on multiple instances of small and performant binaries that can quickly bootstrap,
    scale, and serve requests. This creates a challenge in the context of machine
    learning systems, and specifically in deep learning, because the common libraries
    are usually heavy and clunky, partly because most are implemented in Python which
    is not easy to scale in demanding environments.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们的使用案例确实相当简单，但规模和负载要求无疑使其成为一项挑战。可扩展的高吞吐量系统通常基于多个小而高效的二进制文件实例，这些实例可以快速启动、扩展并处理请求。在机器学习系统，尤其是深度学习的背景下，这带来了挑战，因为常见的库通常比较笨重，部分原因是大多数库是用
    Python 实现的，而 Python 在要求高的环境中扩展性较差。
- en: Therefore, when facing such challenges, we will either choose to use some paid
    serving platform that will take care of the scale, or we will have to create our
    own dedicated serving layer using a number of technologies.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，面对这些挑战时，我们要么选择使用一些付费服务平台来处理规模问题，要么不得不使用多种技术创建自己的专用服务层。
- en: In response to these challenges, Hugging Face has introduced the [Candle](https://github.com/huggingface/candle)
    framework, described as “a minimalist ML framework for Rust with a focus on performance…and
    ease of use”. Candle empowers us to build robust and lightweight model inference
    services in Rust, using a torch-like API. Inference services based on Candle will
    easily scale, rapidly bootstrap, and process requests blazing fast in a manner
    that makes it more suitable for cloud native serverless environments that are
    aimed to deal with challenges of scale and resilience.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 针对这些挑战，Hugging Face 引入了[Candle](https://github.com/huggingface/candle)框架，它被描述为“一个注重性能和易用性的
    Rust 轻量级 ML 框架”。Candle 使我们能够在 Rust 中构建稳健且轻量的模型推理服务，使用类似 torch 的 API。基于 Candle
    的推理服务将能够轻松扩展、快速启动，并以极快的速度处理请求，使其更适合于旨在应对规模和弹性挑战的云原生无服务器环境。
- en: The purpose of this post is to take the popular use case described earlier and
    show how it can be implemented, end to end, using the Candle framework. We will
    delve into a relatively straightforward but robust implementation of a vector
    embedding and search REST service based on Candle and Axum (as our web framework).
    We will use a specific news headlines dataset but the code can be very easily
    extended to any textual dataset.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 本帖的目的是展示如何使用Candle框架实现之前描述的流行用例，从头到尾。我们将深入探讨基于Candle和Axum（作为我们的Web框架）的相对简单但强大的向量嵌入和搜索REST服务实现。我们将使用一个特定的新闻头条数据集，但代码可以很容易扩展到任何文本数据集。
- en: This will be a very hands-on and practical post. Section 2 lays out the main
    design or flow of our service, as well as the involved components we will develop
    and work with. Section 3 focuses on the Candle framework and shows how to implement
    a vector embedding and search functionality using a Bert model. Section 4 shows
    how to wrap the model inference functionality in a REST web service using Axum.
    Section 5 explains how to create the actual embedding and artifacts that our service
    will need. Section 6 concludes.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 这将是一个非常实践和实用的帖子。第2节展示了我们服务的主要设计或流程，以及我们将开发和使用的相关组件。第3节聚焦于Candle框架，并展示如何使用Bert模型实现向量嵌入和搜索功能。第4节展示了如何使用Axum将模型推断功能封装在REST
    Web服务中。第5节解释了如何创建我们服务所需的实际嵌入和工件。第6节总结。
- en: 2\. High Level Service Design
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 2\. 高级服务设计
- en: I will start with the main building blocks of the inference service that we
    are going to implement. The main requirement is to create an HTTP REST endpoint
    that will receive a textual query consisting of a few key words, and will respond
    with the top 5 news headlines that are the most similar to the search query.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 我将从我们要实现的推断服务的主要构建块开始。主要要求是创建一个HTTP REST端点，该端点将接收一个由几个关键字组成的文本查询，并响应与搜索查询最相似的前5条新闻头条。
- en: For this task, we will use *Bert* as a language model because it usually performs
    well on document embedding tasks. In an offline batch process (that will be explained
    in Section 5), we will use Bert to embed about 20K news headlines or documents
    and create a vector embedding for each headline that the service will use to search
    for matches. Both the embedding and the textual headlines will be serialized to
    a binary format that each service instance will load in order to serve query requests.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个任务，我们将使用*Bert*作为语言模型，因为它通常在文档嵌入任务中表现良好。在一个离线批处理过程中（将在第5节中解释），我们将使用Bert来嵌入大约20K条新闻头条或文档，并为每个头条创建一个向量嵌入，服务将使用这些嵌入来搜索匹配项。嵌入和文本头条将被序列化为二进制格式，每个服务实例将加载这些格式以服务查询请求。
- en: '![](../Images/1527f77a4e20c9011e559b24398e9fd2.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1527f77a4e20c9011e559b24398e9fd2.png)'
- en: Image by author
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片
- en: As illustrated above, upon receiving a request with a search query, the service
    will first use a language model in order to embed the search query in a vector.
    Next, it will search the pre-loaded *embedding* for N most similar vectors (each
    vector representing a news headline). Finally, it will use the index of the most
    similar vectors in order to fetch the actual textual headline it represents using
    a *mapping file*.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 如上所示，在接收到带有搜索查询的请求后，服务将首先使用语言模型将搜索查询嵌入到一个向量中。接下来，它将搜索预加载的*embedding*以找到N个最相似的向量（每个向量代表一个新闻头条）。最后，它将使用最相似向量的索引通过*映射文件*提取它所代表的实际文本头条。
- en: 'We will implement this by creating a module or a rust library with one struct
    named *BertInferenceModel,* that will provide the main functions: model loading,
    sentence embedding, and vector search. The module will be used by a REST service
    implemented using Axum web framework. The next section will focus on implementing
    the module while the Section that follows will be dedicated to the web service
    itself.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将通过创建一个名为*BertInferenceModel*的模块或rust库来实现这一点，该模块将提供主要功能：模型加载、句子嵌入和向量搜索。该模块将被一个使用Axum
    Web框架实现的REST服务使用。下一节将专注于实现该模块，而后续的部分将专注于Web服务本身。
- en: Please note that the next sections include many code examples, though for the
    sake of clarity they only show the main functionality implemented. For a complete
    implementation of the solution please refer to the companion git repo linked below.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，接下来的部分包括许多代码示例，但为了清晰起见，它们仅展示了实现的主要功能。有关解决方案的完整实现，请参阅下面链接的配套git仓库。
- en: 3\. Model Serving and Embedding using Candle
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 3\. 使用Candle进行模型服务和嵌入
- en: 'This section will focus on the implementation of the module that will serve
    as an abstraction layer on top of the Candle library API. We will implement this
    as a struct named *BertInferenceModel*, that will consist of 3 main functions:
    model loading, inference (or sentence embedding), and just a simple vector search
    using Cosine similarity.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 本节将重点介绍将作为 Candle 库 API 上的抽象层的模块的实现。我们将实现一个名为*BertInferenceModel*的结构体，该结构体包含三个主要功能：模型加载、推理（或句子嵌入）以及使用余弦相似度进行简单的向量搜索。
- en: '[PRE0]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '*BertInferenceModel* will encapsulate the Bert model and tokenizer that we
    download from Hugging Face repository and will essentially wrap their functionality.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '*BertInferenceModel* 将封装我们从 Hugging Face 仓库下载的 Bert 模型和分词器，并基本上包装它们的功能。'
- en: '**Loading Models from Hugging Face Hub**'
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**从 Hugging Face Hub 加载模型**'
- en: '*BertInferenceModel* will be instantiated with a *load()* function that will
    return a new instance of the struct, loaded with the relevant model and tokenizer,
    and ready for inference tasks. The load function’s parameters include the name
    and revision of the model we wish to load (we will use Bert sentence transformers)
    and the embedding file path.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '*BertInferenceModel* 将通过 *load()* 函数实例化，该函数将返回一个新实例的结构体，加载了相关模型和分词器，并准备进行推理任务。加载函数的参数包括我们希望加载的模型的名称和修订版（我们将使用
    Bert 句子转换器）以及嵌入文件路径。'
- en: '[PRE1]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: As shown in the load function code below, loading a model involves creating
    a *Repo* struct which contains the properties of the relevant repo in Hugging
    Face (e.g. name and revision), and then creating an *API* struct in order to actually
    connect to the repo and download the relevant files (the model weights that are
    required to create the model are represented using HuggingFace’s safetensors format).
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 如下加载函数代码所示，加载模型涉及创建一个包含 Hugging Face 仓库相关属性的*Repo*结构体（例如名称和修订版），然后创建一个*API*结构体以实际连接到仓库并下载相关文件（用于创建模型的模型权重使用
    HuggingFace 的 safetensors 格式表示）。
- en: The *api.get* function returns the local name of the relevant file (whether
    it was downloaded or just read from cache). Files will be downloaded only once
    while subsequent calls to *api.get* will simply use the cached version. We instantiate
    a tokenizer struct using the tokenizer config file, and build our model using
    the weights file (in safetensors format) and a config file.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '*api.get*函数返回相关文件的本地名称（无论是下载的还是仅从缓存中读取的）。文件将只下载一次，而随后的*api.get*调用将仅使用缓存版本。我们使用分词器配置文件实例化一个分词器结构体，并使用权重文件（以
    safetensors 格式）和配置文件来构建我们的模型。'
- en: After we have a model and tokenizer loaded, we can finally load the actual embedding
    file, which we will use to search for matches. I will later show how we generate
    it using the same model and then serialize it to a file. Loading embeddings as
    a Tensor using HuggingFace’s safetensors module is fairly simple, all we need
    is the file name and a key that was given to the tensor when it was saved.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在加载了模型和分词器之后，我们最终可以加载实际的嵌入文件，用于搜索匹配项。我将稍后展示如何使用相同的模型生成嵌入文件，然后将其序列化为文件。使用 HuggingFace
    的 safetensors 模块将嵌入加载为 Tensor 相对简单，我们只需要文件名和保存 Tensor 时给予的密钥。
- en: Now that we have a model and tokenizer loaded, as well as an embedding vectors
    in memory, we finished the initialization of the *BertInferenceModel* that is
    returned to the calling function, and can move on to implement the inference method.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经加载了模型和分词器，并且内存中有了嵌入向量，我们完成了对返回给调用函数的 *BertInferenceModel* 的初始化，可以继续实现推理方法。
- en: '**Sentence Inference and Embedding**'
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**句子推理和嵌入**'
- en: The inference function is also fairly straightforward. We start by using the
    loaded tokenizer in order to encode the sentence (line 5). The *encode()* function
    returns an Encoding struct with a *get_ids()* function that returns an array or
    a numerical representation of the words in the sentence we want to embed. Next,
    we wrap the array of token ids in a tensor that we can can feed to our embedding
    model and use the model’s forward function in order to get the embedding vectors
    that represent the sentence (line 10).
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 推理函数也相当简单。我们首先使用加载的分词器对句子进行编码（第 5 行）。*encode()*函数返回一个 Encoding 结构体，该结构体具有一个
    *get_ids()* 函数，返回一个数组或句子中单词的数值表示。接下来，我们将令牌 ID 数组包装在一个 Tensor 中，以便将其输入到我们的嵌入模型中，并使用模型的前向函数获取表示句子的嵌入向量（第
    10 行）。
- en: The dimensions of the vectors we get in line 12 from the embedding model are
    [128, 384]. This is so because Bert represents each token or word with a vector
    of size 384 and the maximum input length of a sentence vector is 128 (because
    our input has just a few words then most of it is padding). In other words, we
    essentially get a vector of size 384 per token or word in our sentence, besides
    padding and other instruction tokens.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从嵌入模型中在第12行得到的向量维度是[128, 384]。这是因为Bert用大小为384的向量表示每个标记或词，且句子向量的最大输入长度为128（因为我们的输入只有几个单词，所以大部分是填充）。换句话说，除了填充和其他指令标记外，我们基本上获得了每个标记或词的大小为384的向量。
- en: Next, we need to *compress* the sentence vectors from a tensor of size [128,
    384] to a single vector of size [1, 384] that will represent or capture “the essence”
    of the sentence so that we can match it to other sentences in our embedding and
    find sentences that are similar to it. To do so, and partly because the inputs
    we are dealing with are short keywords rather than long sentences, we will use
    max pooling which essentially creates a new vector by taking the max values of
    each dimension of a given tensor in order to capture the most salient features
    in each dimension. As you can see below, this is fairly simple to implement using
    Candle’s API. Finally, we use L2 normalization in order to avoid skew and improve
    the cosine similarity measure by ensuring all vectors have the same magnitude.
    You can see the actual implementation of both pooling and normalization functions
    below.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要将句子向量从大小为[128, 384]的张量压缩成一个大小为[1, 384]的单一向量，该向量将代表或捕捉句子的“精髓”，以便我们可以将其与嵌入中的其他句子进行匹配，并找到与之相似的句子。为此，并且部分因为我们处理的输入是短关键字而不是长句子，我们将使用最大池化，它本质上通过取给定张量每个维度的最大值来创建一个新向量，以捕捉每个维度中最显著的特征。正如您在下方看到的，这使用Candle的API实现起来相当简单。最后，我们使用L2归一化以避免偏差，并通过确保所有向量具有相同的幅度来改善余弦相似性度量。您可以在下方看到池化和归一化函数的实际实现。
- en: '**Measuring Vector Similarity**'
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**测量向量相似性**'
- en: Although this is not directly related to the Candle library, our module will
    also offer a vector search utility method that will receive a vector and use its
    internal embedding in order to return the indices of the most similar vectors.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这与Candle库没有直接关系，但我们的模块也将提供一个向量搜索实用方法，该方法将接收一个向量并利用其内部嵌入以返回最相似向量的索引。
- en: 'This is implemented pretty naively: we first create a collection of tuples
    (line 7), where the first member of the tuple will represent the index of the
    relevant text and the second member will represent the cosine similarity score.
    We then iterate over all indices, and measure the cosine similarity between each
    to the given vector we need to match. Finally, we add the tuple with the (index,
    similarity score) to the collection, sort it, and return the top N requested.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 这实现得相当简单：我们首先创建一个元组集合（第7行），其中元组的第一个成员将表示相关文本的索引，第二个成员将表示余弦相似性评分。然后，我们遍历所有索引，测量每个与我们需要匹配的给定向量之间的余弦相似性。最后，我们将（索引，相似性评分）的元组添加到集合中，对其进行排序，并返回前N个请求的结果。
- en: 4\. Embed and Search Web Service
  id: totrans-45
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 4\. 嵌入和搜索Web服务
- en: Now that we have a struct that wraps our main model functionality, we need to
    encapsulate it in a REST service. We will create a REST endpoint with one POST
    route that will receive a few keywords in a JSON payload and return the index
    of the most similar vectors in the preloaded embedding. Upon request, the service
    will embed the keywords in a vector, search for similarities in its in-memory
    embedding, and return the index of the most similar vectors. The service will
    use the index to find the corresponding headlines in the text mapping file.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了一个封装主要模型功能的结构体，我们需要将其封装在一个REST服务中。我们将创建一个REST端点，包含一个POST路由，该路由将接收JSON有效负载中的几个关键字，并返回在预加载嵌入中的最相似向量的索引。根据请求，服务将把关键字嵌入到一个向量中，搜索其内存中嵌入的相似性，并返回最相似向量的索引。该服务将使用索引在文本映射文件中找到相应的标题。
- en: We will implement the service using the excellent Axum web framework. Most of
    the relevant code is the typical Axum boilerplate so I won’t get too much into
    the details of how to create a REST endpoint using Axum. As in many web frameworks,
    building a REST endpoint typically involves creating a Router and registering
    a handler function on some route to process the request. However, an ML model
    serving layer has the additional complexity of managing the state and persistence
    of the model itself. Model loading can be expensive in terms of performance as
    it involves the IO of loading model files (whether its from Hugging Face’s repo
    or local). Likewise, we need to find a way to cache and reuse the model across
    multiple requests.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用优秀的Axum web框架来实现这个服务。相关代码大部分是典型的Axum模板代码，所以我不会详细讲解如何使用Axum创建REST端点。与许多web框架一样，构建REST端点通常涉及创建一个Router并在某个路由上注册一个处理函数来处理请求。然而，ML模型服务层具有额外的复杂性，即管理模型本身的状态和持久性。模型加载可能在性能上很昂贵，因为它涉及加载模型文件的IO操作（无论是从Hugging
    Face的仓库还是本地）。同样，我们需要找到一种方法来缓存和重用模型以应对多个请求。
- en: To address such requirements, Axum provides the application State feature that
    we can use to initialize and persist any asset we want to be injected into the
    context of every request. Lets look at the entire initialization code of the service
    first, line by line, and see how this works.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 为了满足这些要求，Axum提供了应用状态功能，我们可以用来初始化和持久化任何我们想要注入到每个请求上下文中的资产。首先让我们逐行查看服务的整个初始化代码，看看它是如何工作的。
- en: Each service instance will create and load a model wrapper and then cache it
    for reuse by each request it receives. In line 3 we are creating the model wrapper
    by calling the *load()* function to bootstrap and load the model. Besides the
    name and version of the Bert model we load from HF, we also need to specify the
    location of the embedding file that we load to memory in order to search for similar
    vectors, as well as the key that we used when creating the embedding.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 每个服务实例都会创建并加载一个模型包装器，然后缓存它以供每个接收到的请求重用。在第3行，我们通过调用*load()*函数来创建模型包装器，以引导并加载模型。除了从HF加载的Bert模型的名称和版本，我们还需要指定嵌入文件的位置，该文件被加载到内存中以便搜索相似的向量，以及在创建嵌入时使用的密钥。
- en: In addition to the actual model, we also need to cache the mapping file for
    reuse by each request. After the service uses the model to embed the keywords,
    it searches for the most similar vectors in its embedding file and then returns
    their indices. The service then uses the mapping file in order to extract the
    actual texts corresponding to the indices of the most similar vectors. In a more
    robust production system, upon receiving the indices of the most similar vectors
    from the model wrapper, the service would fetch the actual texts from some fast
    access database, though in our case it will suffice to read it from a preloaded
    list of strings stored in a file. In line 10, we load the list that was pre-saved
    as a binary file.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 除了实际的模型，我们还需要缓存映射文件以供每个请求重用。服务在使用模型嵌入关键词后，会在其嵌入文件中搜索最相似的向量，然后返回它们的索引。服务接着使用映射文件提取与最相似向量的索引对应的实际文本。在更稳健的生产系统中，服务会从某个快速访问的数据库中获取实际文本，但在我们的案例中，从存储在文件中的预加载字符串列表中读取就足够了。在第10行，我们加载了之前保存为二进制文件的列表。
- en: Now we have 2 assets that we need to cache and reuse — the model (wrapper) and
    the mapping file. Axum enables us to do this using an *Arc* or a thread-safe reference-counting
    pointer that each request will share. As you can see in line 15, we create a new
    Arc around a tuple that consists of the model wrapper and the mapping file. In
    line 17–19, we create a new HTTP route to the function that will handle each request.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有两个需要缓存和重用的资产——模型（包装器）和映射文件。Axum使我们能够使用*Arc*，即线程安全的引用计数指针，每个请求都会共享。正如第15行所示，我们在包含模型包装器和映射文件的元组周围创建了一个新的Arc。在第17–19行，我们创建了一个新的HTTP路由来处理每个请求的函数。
- en: '[PRE2]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: In order to cache the tuple and make it available to each request we use the
    *with_state(state)* function in order to inject it to the relevant request context.
    Let’s see exactly how.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 为了缓存元组并使其对每个请求可用，我们使用*with_state(state)*函数将其注入到相关的请求上下文中。我们来看看具体是如何操作的。
- en: '**Handling Requests**'
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**处理请求**'
- en: Our service will serve HTTP POST requests that carry the following payload that
    will consist of the keywords and number of similar vectors or headlines we want
    to receive.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的服务将处理HTTP POST请求，这些请求包含以下有效负载，这些有效负载包括关键词和我们想要接收的相似向量或标题的数量。
- en: '[PRE3]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: We will implement the corresponding request and response structs of the handler
    function and Axum will take care of serialization and de-serialization when required.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将实现处理函数的相应请求和响应结构体，Axum将在需要时处理序列化和反序列化。
- en: '[PRE4]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Next, we can move to the handler function itself. The handler will take 2 parameters:
    the application state that we initialized before (Axum will take care of injecting
    it to each function call), and the request struct we defined earlier.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们可以进入处理函数本身。处理函数将接受2个参数：我们之前初始化的应用程序状态（Axum将负责将其注入到每个函数调用中），以及我们之前定义的请求结构体。
- en: Processing each request will consist of 4 main stages that should be straightforward
    by now. In line 5, we first extract a reference to the state tuple that holds
    a reference to the model and mapping file. In line 6, we embed the keywords in
    a vector using the model. Next, in line 9, we search for N most similar vectors.
    The *score_vector_similarity()* function returns a vector of tuples, each consists
    of an index and cosine similarity score. Finally, we iterate over the indices
    tuples, extract the string corresponding to the index from the mapping file, and
    wrap it in the response payload struct.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 处理每个请求将包括4个主要阶段，这些阶段现在应该已经很清楚了。在第5行，我们首先提取一个指向状态元组的引用，该元组持有对模型和映射文件的引用。在第6行，我们使用模型将关键词嵌入到一个向量中。接下来，在第9行，我们搜索N个最相似的向量。*score_vector_similarity()*函数返回一个由元组组成的向量，每个元组包含一个索引和余弦相似度分数。最后，我们遍历这些索引元组，从映射文件中提取对应索引的字符串，并将其封装到响应有效负载结构中。
- en: And.. we are good to go! Although it doesn’t say much necessarily, I have tested
    this on my Mac with an embedding of about 20K vectors and got a nice avg response
    time of 100ms. Not bad for Bert based vector embedding + vector search.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 然后……我们就可以开始了！虽然这可能并没有具体说明太多，但我在我的Mac上进行了测试，使用了大约20K向量的嵌入，并获得了100ms的良好平均响应时间。对于基于Bert的向量嵌入和向量搜索来说，这算是不坏。
- en: '[PRE5]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: (this example was created using an embedding generated over Arxiv papers abstracts
    dataset. The actual dataset is available [here](https://www.kaggle.com/datasets/spsayakpaul/arxiv-paper-abstracts/)
    under public domain license.)
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: （这个示例是使用在Arxiv论文摘要数据集上生成的嵌入创建的。实际数据集可以在[这里](https://www.kaggle.com/datasets/spsayakpaul/arxiv-paper-abstracts/)以公共领域许可证获取。）
- en: 5\. Generating the Embedding
  id: totrans-64
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 5\. 生成嵌入
- en: Just before we conclude, there is one last component in this flow that we need
    to cover. Until now we have assumed the existence of an embedding file in which
    we search for similar vectors. However, I haven’t explained how to create the
    embedding file itself.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们结束之前，还有一个最后的组件需要覆盖。到目前为止，我们假设了一个嵌入文件的存在，在其中我们搜索相似的向量。然而，我还没有解释如何创建嵌入文件本身。
- en: Recall that the struct created in the last section *— BertInferenceModel,* already
    contains a function that embeds a set of keywords into a vector. When we create
    a function that is required to embed *multiple* sets of keywords, all we need
    to do is process them as a batch.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，在上一节中创建的结构体*— BertInferenceModel*，已经包含了一个将一组关键词嵌入到向量中的函数。当我们创建一个需要嵌入*多个*关键词集的函数时，我们只需将它们作为批量处理即可。
- en: The main difference in the way we use *BertInferenceModel* involve using the
    tokenizer *encode_batch* function rather than *encode*, which takes a vector of
    strings rather than a string. Then we simply stack all the vectors into a single
    tensor and feed it to the model *forward()* function just as we did with a single
    vector embedding (You can see the full source code of the function in the companion
    repo linked below).
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用*BertInferenceModel*的主要区别在于使用tokenizer的*encode_batch*函数而不是*encode*，前者接受一个字符串向量而不是单个字符串。然后，我们将所有向量堆叠成一个单一的张量，并将其输入到模型的*forward()*函数中，就像我们处理单个向量嵌入时一样（你可以在下面链接的辅助仓库中查看函数的完整源代码）。
- en: Once we have such function that can embed multiple strings, then the embedding
    generator itself is pretty straightforward. It uses the rayon crate in order to
    parallel the embedding of the text file, and afterwards it stack the results together
    to create a single tensor. Finally, it writes the embedding to disk using the
    safetensors format. The embedding is an important asset in this pipeline as it
    needs to be copied to every service instance.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们拥有能够嵌入多个字符串的函数，嵌入生成器本身就相当简单。它使用 rayon crate 来并行处理文本文件的嵌入，然后将结果堆叠在一起，创建一个单一的张量。最后，它使用
    safetensors 格式将嵌入写入磁盘。嵌入是这个管道中的重要资产，因为它需要被复制到每个服务实例。
- en: Now we can conclude :-)
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以得出结论 :-)
- en: 6\. Conclusion
  id: totrans-70
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 6\. 结论
- en: One of the greatest challenges in ML engineering is running inference in scale.
    AI is anything but lightweight, and therefore scaling inference workloads is a
    great pain that often ends up being very expensive or over engineered. This is
    exactly the challenge that Hugging Face’s Candle library tries to address. Using
    a Torch like API in Rust, it enables us to create a lean and fast model serving
    layer that can easily scale and run in a serverless environment.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习工程中，最大的挑战之一是大规模推断。人工智能绝非轻量级，因此，扩展推断工作负载往往是一项非常昂贵或过度设计的痛苦挑战。这正是 Hugging
    Face 的 Candle 库试图解决的难题。它使用类似 Torch 的 Rust API，使我们能够创建一个精简且快速的模型服务层，能够轻松扩展并在无服务器环境中运行。
- en: In this post, I showed how we can use Candle to create an end to end model inference
    layer that can serve requests for vector embedding and search. I explained how
    we can wrap Bert / sentence transformers model in a library with a small memory
    footprint and use it in a REST service based on Axum.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我展示了如何使用 Candle 创建一个端到端的模型推断层，能够处理向量嵌入和搜索请求。我解释了如何将 Bert / sentence transformers
    模型包装成一个内存占用小的库，并在基于 Axum 的 REST 服务中使用它。
- en: The true value of Hugging Face’s Candle library lies in its ability to bridge
    the gap between powerful ML capabilities and efficient resource utilization. By
    leveraging Rust’s performance and safety features, Candle paves the way for more
    sustainable and cost-effective ML solutions. This is particularly beneficial for
    organizations seeking to deploy AI at scale without the overheads. I am hopeful
    that with Candle, we will see a new wave of ML applications that are not only
    high-performing but also lighter and more adaptable to various environments.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: Hugging Face 的 Candle 库的真正价值在于其能够弥合强大机器学习能力与高效资源利用之间的差距。通过利用 Rust 的性能和安全特性，Candle
    为更可持续和成本效益高的机器学习解决方案铺平了道路。这对那些希望在不增加开销的情况下大规模部署 AI 的组织特别有利。我希望借助 Candle，我们将看到一波不仅性能高效，而且更轻量且适应各种环境的机器学习应用。
- en: Some resources on Candle
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 一些关于 Candle 的资源
- en: '[https://github.com/huggingface/candle](https://github.com/huggingface/candle)'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://github.com/huggingface/candle](https://github.com/huggingface/candle)'
- en: '[https://medium.com/@Aaron0928/hugging-face-has-written-a-new-ml-framework-in-rust-now-open-sourced-1afea2113410](https://medium.com/@Aaron0928/hugging-face-has-written-a-new-ml-framework-in-rust-now-open-sourced-1afea2113410)'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://medium.com/@Aaron0928/hugging-face-has-written-a-new-ml-framework-in-rust-now-open-sourced-1afea2113410](https://medium.com/@Aaron0928/hugging-face-has-written-a-new-ml-framework-in-rust-now-open-sourced-1afea2113410)'
- en: '[https://pub.towardsai.net/candle-and-falcon-a-guide-to-large-language-models-in-rust-3f0a4369df03](https://pub.towardsai.net/candle-and-falcon-a-guide-to-large-language-models-in-rust-3f0a4369df03)'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://pub.towardsai.net/candle-and-falcon-a-guide-to-large-language-models-in-rust-3f0a4369df03](https://pub.towardsai.net/candle-and-falcon-a-guide-to-large-language-models-in-rust-3f0a4369df03)'
- en: All source code for this post can be found in my github repo [here](https://github.com/a-agmon/candle_demo_1-1)
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 本文的所有源代码可以在我的 GitHub 仓库中找到 [这里](https://github.com/a-agmon/candle_demo_1-1)
