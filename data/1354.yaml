- en: The Map Of Transformers
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 变换器的地图
- en: 原文：[https://towardsdatascience.com/the-map-of-transformers-e14952226398?source=collection_archive---------4-----------------------#2023-04-18](https://towardsdatascience.com/the-map-of-transformers-e14952226398?source=collection_archive---------4-----------------------#2023-04-18)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/the-map-of-transformers-e14952226398?source=collection_archive---------4-----------------------#2023-04-18](https://towardsdatascience.com/the-map-of-transformers-e14952226398?source=collection_archive---------4-----------------------#2023-04-18)
- en: Transformers
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 变换器
- en: A broad overview of Transformers research
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 变换器研究的广泛概述
- en: '[](https://soran-ghaderi.medium.com/?source=post_page-----e14952226398--------------------------------)[![Soran
    Ghaderi](../Images/49d2b0022c2962d8ad7af5017383374f.png)](https://soran-ghaderi.medium.com/?source=post_page-----e14952226398--------------------------------)[](https://towardsdatascience.com/?source=post_page-----e14952226398--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----e14952226398--------------------------------)
    [Soran Ghaderi](https://soran-ghaderi.medium.com/?source=post_page-----e14952226398--------------------------------)'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://soran-ghaderi.medium.com/?source=post_page-----e14952226398--------------------------------)[![Soran
    Ghaderi](../Images/49d2b0022c2962d8ad7af5017383374f.png)](https://soran-ghaderi.medium.com/?source=post_page-----e14952226398--------------------------------)[](https://towardsdatascience.com/?source=post_page-----e14952226398--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----e14952226398--------------------------------)
    [Soran Ghaderi](https://soran-ghaderi.medium.com/?source=post_page-----e14952226398--------------------------------)'
- en: ·
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fd2b75b0bb761&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-map-of-transformers-e14952226398&user=Soran+Ghaderi&userId=d2b75b0bb761&source=post_page-d2b75b0bb761----e14952226398---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----e14952226398--------------------------------)
    ·25 min read·Apr 18, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fe14952226398&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-map-of-transformers-e14952226398&user=Soran+Ghaderi&userId=d2b75b0bb761&source=-----e14952226398---------------------clap_footer-----------)'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fd2b75b0bb761&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-map-of-transformers-e14952226398&user=Soran+Ghaderi&userId=d2b75b0bb761&source=post_page-d2b75b0bb761----e14952226398---------------------post_header-----------)
    在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----e14952226398--------------------------------)
    上发表 · 25 分钟阅读 · 2023年4月18日 [](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fe14952226398&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-map-of-transformers-e14952226398&user=Soran+Ghaderi&userId=d2b75b0bb761&source=-----e14952226398---------------------clap_footer-----------)'
- en: --
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fe14952226398&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-map-of-transformers-e14952226398&source=-----e14952226398---------------------bookmark_footer-----------)![](../Images/4c821282032da4ddc63dc6024fcab1a0.png)'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fe14952226398&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-map-of-transformers-e14952226398&source=-----e14952226398---------------------bookmark_footer-----------)![](../Images/4c821282032da4ddc63dc6024fcab1a0.png)'
- en: Fig. 1\. Isometric map. Designed by [vectorpocket / Freepik](http://www.freepik.com).
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1\. 等距地图。由 [vectorpocket / Freepik](http://www.freepik.com) 设计。
- en: 1\. Introduction
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 1\. 介绍
- en: 'The pace of research in deep learning has accelerated significantly in recent
    years, making it increasingly difficult to keep abreast of all the latest developments.
    Despite this, there is a particular direction of investigation that has garnered
    significant attention due to its demonstrated success across a diverse range of
    domains, including natural language processing, computer vision, and audio processing.
    This is due in large part to its highly adaptable architecture. The model is called
    Transformer, and it makes use of an array of mechanisms and techniques in the
    field (i.e., attention mechanisms). You can read more about the building blocks
    and their implementation along with multiple illustrations in the following articles:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，深度学习的研究进展显著加快，使得跟踪所有最新发展的难度越来越大。尽管如此，有一个特定的研究方向由于其在自然语言处理、计算机视觉和音频处理等多个领域的显著成功，受到了广泛关注。这在很大程度上归功于其高度适应的架构。这个模型被称为Transformer，它利用了该领域的一系列机制和技术（即注意力机制）。你可以在以下文章中深入了解这些构建块及其实现，并查看多个插图：
- en: '[](/transformers-in-action-attention-is-all-you-need-ac10338a023a?source=post_page-----e14952226398--------------------------------)
    [## Transformers in Action: Attention Is All You Need'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/transformers-in-action-attention-is-all-you-need-ac10338a023a?source=post_page-----e14952226398--------------------------------)
    [## Transformer的应用：注意力即是全部'
- en: A brief survey, illustration, and implementation
  id: totrans-13
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 简要调查、插图和实现
- en: towardsdatascience.com](/transformers-in-action-attention-is-all-you-need-ac10338a023a?source=post_page-----e14952226398--------------------------------)
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '[towardsdatascience.com](/transformers-in-action-attention-is-all-you-need-ac10338a023a?source=post_page-----e14952226398--------------------------------)'
- en: 'This article provides more details about the attention mechanisms that I will
    be talking about throughout this article:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 这篇文章提供了更多关于注意力机制的细节，我将在本文中讨论这些机制：
- en: '[](/rethinking-thinking-how-do-attention-mechanisms-actually-work-a6f67d313f99?source=post_page-----e14952226398--------------------------------)
    [## Rethinking Thinking: How Do Attention Mechanisms Actually Work?'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/rethinking-thinking-how-do-attention-mechanisms-actually-work-a6f67d313f99?source=post_page-----e14952226398--------------------------------)
    [## 重新思考：注意力机制究竟是如何工作的？'
- en: The brain, the mathematics, and DL — research frontiers
  id: totrans-17
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 大脑、数学与深度学习——研究前沿
- en: towardsdatascience.com](/rethinking-thinking-how-do-attention-mechanisms-actually-work-a6f67d313f99?source=post_page-----e14952226398--------------------------------)
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '[towardsdatascience.com](/rethinking-thinking-how-do-attention-mechanisms-actually-work-a6f67d313f99?source=post_page-----e14952226398--------------------------------)'
- en: 2\. Taxonomy of the Transformers
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 2\. Transformer的分类
- en: 'A comprehensive range of models has been explored based on the vanilla Transformer
    to date, which can broadly be broken down into three categories:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，基于原始Transformer已经探索了广泛的模型，这些模型大致可以分为三类：
- en: Architectural modifications
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 架构修改
- en: Pretraining methods
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预训练方法
- en: Applications
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应用
- en: '![](../Images/7491b188970493b11bc9aac813510f92.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7491b188970493b11bc9aac813510f92.png)'
- en: Fig. 2\. Transformer variants modifications. Photo by [author](https://github.com/soran-ghaderi).
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 图2. Transformer变体修改。照片由[作者](https://github.com/soran-ghaderi)提供。
- en: Each category above contains several other sub-categories, which I will investigate
    thoroughly in the next sections. Fig. 2\. illustrates the categories researchers
    have modified Transformers.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 上述每个类别包含几个子类别，我将在接下来的部分中深入探讨。图2展示了研究人员对Transformer的修改类别。
- en: 3\. Attention
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 3\. 注意力
- en: Self-attention plays an elemental role in Transformer, although, it suffers
    from two main disadvantages in practice [1].
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 自注意力在Transformer中发挥了基本作用，但在实践中存在两个主要缺点[1]。
- en: '**Complexity**: As for long sequences, this module turns into a bottleneck
    since its computational complexity is O(T²·D).'
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**复杂性**：对于长序列，这个模块变成了瓶颈，因为其计算复杂度为O(T²·D)。'
- en: '**Structural prior:** It does not tackle the structural bias of the inputs
    and requires additional mechanisms to be injected into the training data which
    later it can learn (i.e. learning the order information of the input sequences).'
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**结构先验**：它没有处理输入的结构偏差，需要在训练数据中注入额外的机制，模型才能后续学习（例如学习输入序列的顺序信息）。'
- en: '![](../Images/56cdb5c915c769faa741c72e1dd844bc.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/56cdb5c915c769faa741c72e1dd844bc.png)'
- en: Fig. 3\. Categories of attention modifications and example papers. Photo by
    [author](https://www.linkedin.com/in/soran-ghaderi/).
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 图3. 注意力修改类别及示例论文。照片由[作者](https://www.linkedin.com/in/soran-ghaderi/)提供。
- en: Therefore, researchers have explored various techniques to overcome these drawbacks.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，研究人员探索了各种技术来克服这些缺陷。
- en: '**Sparse attention:** This technique tries to lower the computation time and
    the memory requirements of the attention mechanism by taking a smaller portion
    of the inputs into account instead of the entire input sequence, producing a sparse
    matrix in contrast to a full matrix.'
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**稀疏注意力：** 该技术通过只考虑输入序列的一小部分而不是整个输入序列来降低注意力机制的计算时间和内存需求，从而产生稀疏矩阵，相较于全矩阵。'
- en: '**Linearized attention:** Disentangling the attention matrix using kernel feature
    maps, this method tries to compute the attention in the reverse order to reduce
    the resource requirements to linear complexity.'
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**线性化注意力：** 通过使用核特征映射解构注意力矩阵，这种方法尝试以反向顺序计算注意力，从而将资源需求降低到线性复杂度。'
- en: '**Prototype and memory compression:** This line of modification tries to decrease
    the queries and key-value pairs to achieve a smaller attention matrix which in
    turn reduces the time and computational complexity.'
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**原型和内存压缩：** 这种修改线试图减少查询和键-值对，以实现较小的注意力矩阵，从而减少时间和计算复杂度。'
- en: '**Low-rank self-attention:** By explicitly modeling the low-rank property of
    the self-attention matrix using parameterization or replacing it with a low-rank
    approximation tries to improve the performance of the transformer.'
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**低秩自注意力：** 通过显式建模自注意力矩阵的低秩特性，使用参数化或用低秩近似替代，以期提高变换器的性能。'
- en: '**Attention with prior:** Leveraging the prior attention distribution from
    other sources, this approach, combines other attention distributions with the
    one obtained from the inputs.'
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**带有先验的注意力：** 利用来自其他来源的先验注意力分布，这种方法将其他注意力分布与从输入中获得的注意力分布结合起来。'
- en: '**Modified multi-head mechanism:** There are various ways to modify and improve
    the performance of the multi-head mechanism which can be categorized under this
    research direction.'
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**修改的多头机制：** 有多种方法可以修改和提高多头机制的性能，这些方法可以归类于这一研究方向。'
- en: 3.1\. Sparse attention
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.1. 稀疏注意力
- en: The standard self-attention mechanism in a transformer requires every token
    to attend to all other tokens. However, it has been observed that in many cases,
    the attention matrix is often very sparse, meaning that only a small number of
    tokens actually attend to each other [2]. This suggests that it is possible to
    reduce the computational complexity of the self-attention mechanism by limiting
    the number of query-key pairs that each query attends to. By only computing the
    similarity scores for pre-defined patterns of query-key pairs, it is possible
    to significantly reduce the amount of computation required without sacrificing
    performance.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 变换器中的标准自注意力机制要求每个标记都关注所有其他标记。然而，已经观察到在许多情况下，注意力矩阵通常非常稀疏，这意味着只有少量标记实际上彼此关注[2]。这表明，可以通过限制每个查询关注的查询-键对的数量来减少自注意力机制的计算复杂度。通过仅计算预定义模式的查询-键对的相似性分数，可以在不牺牲性能的情况下显著减少所需的计算量。
- en: '![](../Images/91697d5a6d8416a50cbc58e164a206d9.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/91697d5a6d8416a50cbc58e164a206d9.png)'
- en: Eq. 1
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: Eq. 1
- en: In the un-normalized attention matrix Â, the -∞ items are not typically stored
    in memory in order to reduce the memory footprint. This is done to decrease the
    amount of memory required to implement the matrix, which can improve the efficiency
    and performance of the system.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在非规范化的注意力矩阵Â中，-∞项通常不会存储在内存中，以减少内存占用。这是为了降低实现矩阵所需的内存量，从而提高系统的效率和性能。
- en: We can map the attention matrix to a bipartite graph where the standard attention
    mechanism can be thought of as a complete bipartite graph, where each query receives
    information from all of the nodes in the memory and uses this information to update
    its representation. In this way, the attention mechanism allows each query to
    attend to all of the other nodes in the memory and incorporate their information
    into its representation. This allows the model to capture complex relationships
    and dependencies between the nodes in the memory. The sparse attention mechanism,
    on the other hand, can be thought of as a sparse graph. This means that not all
    of the nodes in the graph are connected, which can reduce the computational complexity
    of the system and improve its efficiency and performance. By limiting the number
    of connections between nodes, the sparse attention mechanism can still capture
    important relationships and dependencies, but with less computational overhead.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将注意力矩阵映射到一个二分图，其中标准注意力机制可以被认为是一个完整的二分图，每个查询从记忆中的所有节点接收信息，并利用这些信息更新其表示。这样，注意力机制允许每个查询关注记忆中的所有其他节点，并将它们的信息纳入其表示中。这使得模型能够捕捉记忆节点之间复杂的关系和依赖性。另一方面，稀疏注意力机制可以被认为是一个稀疏图。这意味着图中的并非所有节点都相互连接，这可以减少系统的计算复杂性，提高其效率和性能。通过限制节点之间的连接数量，稀疏注意力机制仍然可以捕捉重要的关系和依赖性，但计算开销较小。
- en: There are two main classes of approaches to sparse attention, based on the metrics
    used to determine the sparse connections between nodes [1]. These are **position-based**
    and **content-based** sparse attention.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 稀疏注意力方法主要有两类，基于用于确定节点之间稀疏连接的指标[1]。这两类是**基于位置**和**基于内容**的稀疏注意力。
- en: 3.1.1\. Position-based sparse attention
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.1.1\. 基于位置的稀疏注意力
- en: In this type of attention, the connections in the attention matrix are limited
    according to predetermined patterns. They can be expressed as combinations of
    simpler patterns, which can be useful for understanding and analyzing the behavior
    of the attention mechanism.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种类型的注意力中，注意力矩阵中的连接是根据预定模式进行限制的。它们可以表示为更简单模式的组合，这对理解和分析注意力机制的行为非常有用。
- en: '![](../Images/654f80182028882725b4412ae9ed3394.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/654f80182028882725b4412ae9ed3394.png)'
- en: Fig. 4\. Main atomic sparse attention patterns. The colored squares demonstrate
    correspondent calculated attention scores. Image from [[1]](https://arxiv.org/abs/2106.04554v2).
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4\. 主要的原子稀疏注意力模式。彩色方块展示了相应的计算注意力分数。图片来源于[[1]](https://arxiv.org/abs/2106.04554v2)。
- en: '**3.1.1.1\. Atomic sparse attention:** There are five basic atomic sparse attention
    patterns that can be used to construct a variety of different sparse attention
    mechanisms that have different trade-offs between computational complexity and
    performance as shown in Fig. 4.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '**3.1.1.1\. 原子稀疏注意力：** 有五种基本的原子稀疏注意力模式可以用来构建各种不同的稀疏注意力机制，这些机制在计算复杂性和性能之间有不同的权衡，如图
    4 所示。'
- en: '**Global attention:** Global nodes can be used as an information hub across
    all other nodes that can attend to all other nodes in the sequence and vice versa
    as in Fig. 4 (a).'
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**全局注意力：** 全局节点可以作为信息中心，能够关注序列中的所有其他节点，反之亦然，如图 4 (a) 所示。'
- en: '**Band attention (also sliding window attention or local attention):** The
    relationships and dependencies between different parts of the data are often local
    rather than global. In the band attention, the attention matrix is a band matrix,
    with the queries only attending to a certain number of neighboring nodes on either
    side as shown in Fig. 4 (b).'
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**带状注意力（也称为滑动窗口注意力或局部注意力）：** 数据不同部分之间的关系和依赖性通常是局部的而非全局的。在带状注意力中，注意力矩阵是一个带状矩阵，查询仅关注两侧一定数量的邻近节点，如图
    4 (b) 所示。'
- en: '**Dilated attention:** Similar to how dilated convolutional neural networks
    (CNNs) can increase the receptive field without increasing computational complexity,
    it is possible to do the same with band attention by using a dilated window with
    gaps of dilation *w_d* >= 1, as shown in Fig. 4 (c). Also, it can be extended
    to strided attention where the dilation 𝑤 𝑑 is assumed to be a large value.'
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**扩张注意力：** 类似于扩张卷积神经网络（CNNs）可以在不增加计算复杂性的情况下扩大感受野，通过使用扩张窗口（扩张*𝑤_d* >= 1）可以实现带状注意力的类似效果，如图
    4 (c) 所示。此外，它也可以扩展到步幅注意力，其中扩张𝑤 𝑑 被认为是一个较大的值。'
- en: '**Random attention:** To improve the ability of the attention mechanism to
    capture non-local interactions, a few edges can be randomly sampled for each query,
    as depicted in Fig. 4 (d).'
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**随机注意力：** 为了提高注意力机制捕捉非局部交互的能力，可以为每个查询随机采样一些边，如图4(d)所示。'
- en: '**Block local attention:** The input sequence is segmented into several non-intersecting
    query blocks, each of which is associated with a local memory block. The queries
    within each query block only attend to the keys in the corresponding memory block,
    shown in 3(e).'
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**块级局部注意力：** 输入序列被分割成几个互不交叉的查询块，每个查询块都关联一个局部内存块。每个查询块中的查询仅关注相应内存块中的键，如图3(e)所示。'
- en: '**3.1.1.2\. Compound sparse attention:** As illustrated in Fig. 5, many existing
    sparse attention mechanisms are composed of more than one of the atomic patterns
    described above.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '**3.1.1.2\. 复合稀疏注意力：** 如图5所示，许多现有的稀疏注意力机制由上述描述的多个原子模式组成。'
- en: '![](../Images/fc2ed23f12fc57d7c3cda43bcedf436c.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fc2ed23f12fc57d7c3cda43bcedf436c.png)'
- en: Fig. 5\. Four different compound sparse attention patterns. Image from [[1]](https://arxiv.org/abs/2106.04554v2).
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 图5\. 四种不同的复合稀疏注意力模式。图片来自[[1]](https://arxiv.org/abs/2106.04554v2)。
- en: '**3.1.1.3\. Extended sparse attention:** There are also other types of patterns
    that have been explored for specific data types. By way of example, BP-Transformer
    [3] uses a binary tree to capture a combination of global and local attention
    across the input sequence. Tokens are leaf nodes and the internal nodes are span
    nodes containing multiple tokens. Fig. 6 shows a number of extended sparse attention
    patterns.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '**3.1.1.3\. 扩展稀疏注意力：** 还有其他类型的模式已被探索用于特定数据类型。例如，BP-Transformer [3] 使用二叉树来捕捉输入序列中全局和局部注意力的组合。令牌是叶节点，内部节点是包含多个令牌的跨度节点。图6展示了多种扩展稀疏注意力模式。'
- en: '![](../Images/33205e4eb8177520d005fdfc0cac64b5.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/33205e4eb8177520d005fdfc0cac64b5.png)'
- en: Fig. 6\. Different extended sparse attention patterns. Image from [[1]](https://arxiv.org/abs/2106.04554v2).
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 图6\. 不同的扩展稀疏注意力模式。图片来自[[1]](https://arxiv.org/abs/2106.04554v2)。
- en: 3.1.2\. Content-based sparse attention
  id: totrans-63
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.1.2\. 基于内容的稀疏注意力
- en: In this approach, a sparse graph is constructed where the sparse connections
    are based on the inputs. It selects the keys that have high similarity scores
    with the given query. An efficient way to build this graph is to use Maximum Inner
    Product Search (MIPS) which finds the maximum dot-product between keys and the
    query without calculating all dot-products.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种方法中，构建一个稀疏图，其中稀疏连接基于输入。它选择与给定查询具有高相似度的键。构建此图的高效方法是使用最大内积搜索（MIPS），该方法在不计算所有点积的情况下找到键与查询之间的最大点积。
- en: '![](../Images/ef21f3839a71e0eaac3848b36f39baca.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ef21f3839a71e0eaac3848b36f39baca.png)'
- en: Fig. 7\. 2-D attention schemes for the Routing Transformer compared to local
    attention and strided attention. Image from [[4](https://arxiv.org/abs/2003.05997)]
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 图7\. Routing Transformer的2-D注意力方案，与局部注意力和跨步注意力相比。图片来自[[4](https://arxiv.org/abs/2003.05997)]
- en: Routing Transformer [4] as shown in Fig. 7, equips the self-attention mechanism
    with a sparse routing module by using online k-means clustering to cluster keys
    and queries on the same centroid vectors. It isolates the queries to only attend
    keys within the same cluster. Reformer [5] uses locality-sensitive hashing (LSH)
    instead of dot-product attention to select keys and values for each query. It
    enables the queries to only attend to tokens within the same bucket which are
    derived from the queries and keys using LSH. Using the LSTM edge predictor, Sparse
    Adaptive Connection (SAC) [6] constructs a graph from the input sequence and achieves
    attention edges to enhance the tasks-specific performance by leveraging an adaptive
    sparse connection.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: Routing Transformer [4] 如图7所示，通过使用在线k-means聚类对键和值进行同心心向量聚类，为自注意力机制配备了稀疏路由模块。它将查询隔离，只关注同一簇内的键。Reformer
    [5] 使用局部敏感哈希（LSH）代替点积注意力，为每个查询选择键和值。它使查询仅关注来自LSH生成的同一桶中的令牌。使用LSTM边预测器，稀疏自适应连接（SAC）
    [6] 从输入序列中构建图，并通过利用自适应稀疏连接来增强任务特定的性能。
- en: 3.2\. Linearized attention
  id: totrans-68
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.2\. 线性化注意力
- en: The computational complexity of the dot-product attention mechanism (softmax(QK^⊤)V)
    increases quadratically with the spatiotemporal size (length) of the input. Therefore,
    it impedes its usage when exposed to large inputs such as videos, long sequences,
    or high-resolution images. By disentangling softmax(QK^⊤) to Q′ K′^⊤, the (Q′
    K′^⊤ V) can be computed in reverse order, resulting in a linear complexity O(𝑇
    ).
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 点积注意力机制的计算复杂度（softmax(QK^⊤)V）随着输入的空间时间大小（长度）的增加而呈二次增加。因此，当暴露于大输入（如视频、长序列或高分辨率图像）时，它阻碍了其使用。通过将
    softmax(QK^⊤) 解开成 Q′ K′^⊤，(Q′ K′^⊤ V) 可以按相反顺序计算，结果是线性复杂度 O(𝑇)。
- en: '![](../Images/4dc671f6875350317d9d0f7768061ba3.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4dc671f6875350317d9d0f7768061ba3.png)'
- en: Fig. 8\. Standard self-attention and linearized self-attention complexity difference.
    Image from [[1](https://arxiv.org/abs/2106.04554v2)].
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8\. 标准自注意力和线性化自注意力的复杂度差异。图片来自 [[1](https://arxiv.org/abs/2106.04554v2)]。
- en: Assuming Â = exp(QK^⊤) denotes an un-normalized attention matrix, where exp(.)
    is applied element-wise, Linearized attention is a technique that approximates
    the un-normalized attention matrix exp(QK^⊤) with 𝜙(Q) 𝜙(K)^⊤ where 𝜙 is a row-wise
    feature map. By applying this technique, we can do 𝜙(Q) (𝜙(K)^⊤ V) which is a
    linearized computation of an un-normalized attention matrix as illustrated in
    Fig. 8.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 假设 Â = exp(QK^⊤) 表示未归一化的注意力矩阵，其中 exp(.) 逐元素应用，线性化注意力是一种近似未归一化注意力矩阵 exp(QK^⊤)
    的技术，其形式为 𝜙(Q) 𝜙(K)^⊤，其中 𝜙 是逐行特征映射。通过应用这种技术，我们可以执行 𝜙(Q) (𝜙(K)^⊤ V)，这是未归一化注意力矩阵的线性化计算，如图
    8 所示。
- en: To achieve a deeper understanding of linearized attention, I will explore the
    formulation in vector form. I will examine the general form of attention in order
    to gain further insight.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更深入地理解线性化注意力，我将探索向量形式的公式。我将检查注意力的一般形式，以获得进一步的洞见。
- en: '![](../Images/a652c4b566db4a5e99dd1caa2935f3b5.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a652c4b566db4a5e99dd1caa2935f3b5.png)'
- en: Eq. 2
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 公式 2
- en: In this context, sim(·, ·) is a scoring function that measures the similarity
    between input vectors. In the vanilla Transformer, the scoring function is the
    exponential of the inner product, exp(⟨·, ·⟩). A suitable selection for sim(·,
    ·) is a kernel function, K(x, y) = 𝜙(x)𝜙(y)^⊤ , which leads to further insights
    into the linearized attention.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个背景下，sim(·, ·) 是衡量输入向量相似性的评分函数。在标准 Transformer 中，评分函数是内积的指数形式，exp(⟨·, ·⟩)。一个适合的选择是核函数
    sim(·, ·) = K(x, y) = 𝜙(x)𝜙(y)^⊤ ，这进一步揭示了线性化注意力的洞见。
- en: '![](../Images/c3d9b54c2ac39325feb71330701caa5d.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c3d9b54c2ac39325feb71330701caa5d.png)'
- en: Eq. 3
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 公式 3
- en: in this formulation, the outer product of vectors is denoted by ⊗. Attention
    can be linearized by first computing the highlighted terms which allow the autoregressive
    models i.e. transformer decoders to run like RNNs.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个公式中，向量的外积用 ⊗ 表示。注意力可以通过首先计算突出显示的术语来线性化，这允许自回归模型，即 Transformer 解码器像 RNN 一样运行。
- en: Eq. 2 shows that it keeps a memory matrix by aggregating associations from outer
    products of (feature-mapped) keys and queries. It later retrieves it by multiplying
    the memory matrix with the feature-mapped query with proper normalization.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 公式 2 表明，通过聚合（特征映射后的）键和查询的外积，它保留了一个内存矩阵。稍后通过将内存矩阵与特征映射后的查询乘以适当的归一化来检索它。
- en: 'This approach consists of two foundational components:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法由两个基础组件组成：
- en: '**Feature map 𝜙 (·):** the kernel feature map for each attention implementation
    (ex. 𝜙𝑖(x) = elu(𝑥 𝑖 )+1 proposed in Linear Transformer'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**特征映射 𝜙 (·)：** 每种注意力实现的核特征映射（例如，Linear Transformer 提出的𝜙𝑖(x) = elu(𝑥 𝑖 )+1）。'
- en: '**Aggregation rule:** aggregating the associations {𝜙 (k)𝑗 ⊗ v𝑗} into the memory
    matrix by simple summation.'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**聚合规则：** 通过简单求和将关联 {𝜙 (k)𝑗 ⊗ v𝑗} 聚合到内存矩阵中。'
- en: 3.3\. Query prototyping and memory compression
  id: totrans-84
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.3\. 查询原型化和内存压缩
- en: Aside from employing the utilization of sparse attention or kernel-based linearized
    attention, it is also feasible to mitigate the intricacy of attention through
    a decrease in the quantity of queries or key-value pairs, thereby resulting in
    the initiation of query prototypes and the implementation of memory compression
    techniques, respectively.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 除了使用稀疏注意力或基于核的线性化注意力，还可以通过减少查询或键值对的数量来缓解注意力的复杂性，从而引入查询原型和内存压缩技术。
- en: '![](../Images/7c0ff26beef060b9c57b31245f7634db.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7c0ff26beef060b9c57b31245f7634db.png)'
- en: Fig. 9\. Qurey prototyping and memory compression. Photo from [[1](https://arxiv.org/abs/2106.04554v2)]
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9\. 查询原型化和内存压缩。照片来自 [[1](https://arxiv.org/abs/2106.04554v2)]
- en: '**3.3.1\. Attention with prototype queries:** The implementation of Attention
    with Prototype Queries involves the utilization of a set of query prototypes as
    the primary basis for computing attention distributions. The model employs two
    distinct methodologies, either by copying the computed distributions to the positions
    occupied by the represented queries, or by filling those positions with discrete
    uniform distributions. The flow of computation in this process is depicted in
    Figure 9(a).'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '**3.3.1\. 带原型查询的注意力：** 实施带原型查询的注意力涉及使用一组查询原型作为计算注意力分布的主要依据。模型采用两种不同的方法，要么将计算得到的分布复制到代表查询的位置，要么在这些位置填充离散均匀分布。该过程的计算流程如图9(a)所示。'
- en: Clustered Attention, as described in [7], involves the aggregation of queries
    into several clusters, with attention distributions being computed for the centroids
    of these clusters. All queries within a cluster are assigned the attention distribution
    calculated for its corresponding centroid.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类注意力，如[7]所述，涉及将查询聚合到几个簇中，注意力分布则是针对这些簇的质心计算的。簇内所有查询都被分配给相应质心计算出的注意力分布。
- en: Informer, as outlined in [8], employs a methodology of explicit query sparsity
    measurement, derived from an approximation of the Kullback-Leibler divergence
    between the query’s attention distribution and the discrete uniform distribution,
    to select query prototypes. Attention distributions are then calculated only for
    the top-𝑢 queries as determined by the query sparsity measurement, with the remaining
    queries being assigned discrete uniform distributions.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: Informer，如[8]所述，采用了显式查询稀疏度测量的方法，该方法源自于对查询的注意力分布与离散均匀分布之间Kullback-Leibler散度的近似，以选择查询原型。然后，仅对由查询稀疏度测量确定为前𝑢个的查询计算注意力分布，而其余查询则分配离散均匀分布。
- en: '**3.3.2\. Attention with compressed key-value memory:** This technique reduces
    the complexity of the attention mechanism in the Transformer by reducing the number
    of key-value pairs before applying attention as shown in Fig. 9(b). This is achieved
    by compressing the key-value memory. The compressed memory is then used to compute
    attention scores. This technique can significantly reduce the computational cost
    of attention while maintaining good performance on various NLP tasks.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '**3.3.2\. 压缩键值内存的注意力：** 这种技术通过在应用注意力之前减少键-值对的数量来降低Transformer中注意力机制的复杂性，如图9(b)所示。这通过压缩键值内存来实现。压缩内存然后用于计算注意力分数。这种技术可以显著降低注意力的计算成本，同时在各种自然语言处理任务上保持良好的性能。'
- en: '*Liu et al. [9]* suggest a technique called *Memory Compressed Attention (MCA)*
    in their paper. *MCA* involves using strided convolution to decrease the number
    of keys and values. *MCA* is utilized alongside local attention, which is also
    proposed in the same paper. By reducing the number of keys and values by a factor
    of the kernel size, *MCA* is able to capture global context and process longer
    sequences than the standard Transformer model with the same computational resources.'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '*Liu et al. [9]* 在其论文中提出了一种名为 *Memory Compressed Attention (MCA)* 的技术。*MCA*
    使用跨步卷积来减少键和值的数量。*MCA* 与本文中也提出的局部注意力一起使用。通过将键和值的数量减少到卷积核大小的因子，*MCA* 能够捕获全局上下文并处理比标准Transformer模型更长的序列，同时保持相同的计算资源。'
- en: '*Set Transformer* [10] and *Luna* [11] are two models that utilize external
    trainable global nodes to condense information from inputs. The condensed representations
    then function as a compressed memory that the inputs attend to, effectively reducing
    the quadratic complexity of self-attention to linear complexity concerning the
    length of the input sequence.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '*Set Transformer* [10] 和 *Luna* [11] 是两个利用外部可训练的全局节点来压缩输入信息的模型。压缩表示然后作为输入的注意力的压缩内存，有效地将自注意力的二次复杂度降低到与输入序列长度线性相关的复杂度。'
- en: '*Linformer* [12] reduces the computational complexity of self-attention to
    linear by linearly projecting keys and values from the length *n* to a smaller
    length *n_k.* The setback with this approach is the pre-assumed input sequence
    length, making it unsuitable for autoregressive attention.'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '*Linformer* [12]将自注意力的计算复杂度线性降低，通过将键和值从长度为 *n* 线性投影到更小的长度 *n_k*。这种方法的缺点是预设的输入序列长度，因此不适合自回归注意力模型。'
- en: '*Poolingformer* [13] employs a two-level attention mechanism that combines
    sliding window attention with compressed memory attention. Compressed memory attention
    helps with enlarging the receptive field. To reduce the number of keys and values,
    several pooling operations are explored, including max pooling and Dynamic Convolution-based
    pooling.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '*Poolingformer* [13] 采用了一个两级注意力机制，将滑动窗口注意力与压缩内存注意力相结合。压缩内存注意力有助于扩大感受野。为了减少键和值的数量，探索了几种池化操作，包括最大池化和基于动态卷积的池化。'
- en: 3.4\. Low-rank self-attention
  id: totrans-96
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.4\. 低秩自注意力
- en: 'According to empirical and theoretical analyses conducted by various researchers
    [14, 12], the self-attention matrix A ∈ R𝑇 ×𝑇 exhibits low-rank characteristics
    in many cases. This observation offers two implications: Firstly, the low-rank
    nature can be explicitly modeled using parameterization. This could lead to the
    development of new models that leverage this property to improve performance.
    Secondly, instead of using the full self-attention matrix, a low-rank approximation
    could be used in its place. This approach could enable more efficient computations
    and further enhance the scalability of self-attention-based models.'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 根据各种研究者 [14, 12] 进行的实证和理论分析，自注意力矩阵 A ∈ R𝑇 ×𝑇 在许多情况下表现出低秩特性。这一观察提供了两个含义：首先，可以使用参数化显式建模低秩特性。这可能会导致开发利用这一特性以提高性能的新模型。其次，可以用低秩近似代替完整的自注意力矩阵。这种方法可以实现更高效的计算，并进一步提高基于自注意力的模型的可扩展性。
- en: '**3.4.1\. Low-rank parameterization:** When the rank of the attention matrix
    is lower than the sequence length, it suggests that over-parameterizing the model
    by setting 𝐷𝑘 > 𝑇 would lead to overfitting in situations where the input is typically
    short. Therefore, it is sensible to restrict the dimension of 𝐷𝑘 and leverage
    the low-rank property as an inductive bias. To this end, Guo et al. [14] propose
    decomposing the self-attention matrix into a low-rank attention module with a
    small 𝐷𝑘 that captures long-range non-local interactions, and a band attention
    module that captures local dependencies. This approach can be beneficial in scenarios
    where the input is short and requires effective modeling of both local and non-local
    dependencies.'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '**3.4.1\. 低秩参数化：** 当注意力矩阵的秩低于序列长度时，这表明通过设置 𝐷𝑘 > 𝑇 来过度参数化模型会导致在输入通常较短的情况下出现过拟合。因此，限制
    𝐷𝑘 的维度并利用低秩特性作为归纳偏差是明智的。为此，Guo 等人 [14] 提出了将自注意力矩阵分解为一个小的 𝐷𝑘 低秩注意力模块，用于捕捉长距离的非局部交互，以及一个带状注意力模块，用于捕捉局部依赖。这种方法在输入较短且需要有效建模局部和非局部依赖的场景中可能会有所帮助。'
- en: '**3.4.2\. Low-rank approximation:** The low-rank property of the attention
    matrix can also be leveraged to reduce the complexity of self-attention by using
    a low-rank matrix approximation. This methodology is closely related to the low-rank
    approximation of kernel matrices, and some existing works are inspired by kernel
    approximation. For instance, Performer, as discussed in Section 3.2, uses a random
    feature map originally proposed to approximate Gaussian kernels to decompose the
    attention distribution matrix A into C𝑄 GC𝐾, where G is a Gaussian kernel matrix
    and the random feature map approximates G.'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '**3.4.2\. 低秩近似：** 也可以利用注意力矩阵的低秩特性，通过使用低秩矩阵近似来降低自注意力的复杂性。这种方法与核矩阵的低秩近似密切相关，一些现有工作受到核近似的启发。例如，Performer（如第
    3.2 节所讨论的）使用了一种最初用于近似高斯核的随机特征映射，将注意力分布矩阵 A 分解为 C𝑄 GC𝐾，其中 G 是高斯核矩阵，随机特征映射近似 G。'
- en: 'An alternative approach to dealing with the low-rank property of attention
    matrices is to use Nyström-based methods [15, 16]. In these methods, a subset
    of landmark nodes is selected from the input sequence using down-sampling techniques
    such as strided average pooling. The selected landmarks are then used as queries
    and keys to approximate the attention matrix. Specifically, the attention computation
    involves softmax normalization of the product of the original queries with the
    selected keys, followed by the product of the selected queries with the normalized
    result. This can be expressed as:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 处理注意力矩阵低秩特性的一种替代方法是使用基于 Nyström 的方法 [15, 16]。在这些方法中，从输入序列中选择一部分地标节点，使用下采样技术，如步长平均池化。选择的地标节点被用作查询和键，以近似注意力矩阵。具体而言，注意力计算包括对原始查询与选择的键的乘积进行
    softmax 归一化，然后计算选择的查询与归一化结果的乘积。这可以表示为：
- en: '![](../Images/262220a7ba22b64149ec700c89c9b3a4.png)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/262220a7ba22b64149ec700c89c9b3a4.png)'
- en: Eq. 4
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 方程 4
- en: Note that the inverse of the matrix **M**^-1 = (softmax(Q̃K̃^T))^-1 may not
    always exist, but this issue can be mitigated in various ways. For example, CSALR
    [15] adds an identity matrix to **M** to ensure the inverse always exists, while
    Nyström-former [16] uses the Moore-Penrose pseudoinverse of **M** to handle singular
    cases.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，矩阵 **M**^-1 = (softmax(Q̃K̃^T))^-1 的逆可能并不总是存在，但可以通过多种方式减轻此问题。例如，CSALR [15]
    向 **M** 添加单位矩阵以确保逆矩阵始终存在，而 Nyström-former [16] 使用 **M** 的 Moore-Penrose 伪逆来处理奇异情况。
- en: 3.5\. Attention with prior
  id: totrans-104
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.5\. 带先验的注意力
- en: The attention mechanism is a way of focusing on specific parts of an input sequence.
    It does this by generating a weighted sum of the vectors in the sequence, where
    the weights are determined by an attention distribution. The attention distribution
    can be generated from the inputs, or it can come from other sources, such as prior
    knowledge. In most cases, the attention distribution from the inputs and the prior
    attention distribution are combined by computing a weighted sum of their scores
    before applying softmax, thus, allowing the neural network to learn from both
    the inputs and the prior knowledge.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 注意力机制是一种关注输入序列中特定部分的方法。它通过生成序列中向量的加权和来实现，其中权重由注意力分布决定。注意力分布可以从输入中生成，也可以来自其他来源，如先验知识。在大多数情况下，输入的注意力分布和先验注意力分布通过计算它们分数的加权和后结合，然后应用
    softmax，从而使神经网络能够从输入和先验知识中学习。
- en: '![](../Images/f3d9acc3d75dd3afcdc64f246b3f0bdb.png)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f3d9acc3d75dd3afcdc64f246b3f0bdb.png)'
- en: Fig. 10\. Attention with prior combines generated and prior attention scores
    to compute the final attention scores. Image from [[1]](https://arxiv.org/abs/2106.04554v2).
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10\. 带先验的注意力将生成的注意力分数和先验注意力分数结合起来计算最终的注意力分数。图片来自 [[1]](https://arxiv.org/abs/2106.04554v2)。
- en: '**3.5.1\. Prior that models locality:** To model the locality of certain types
    of data like text, a Gaussian distribution over positions can be used as prior
    attention. This involves multiplying the generated attention distribution with
    a Gaussian density and renormalizing or adding a bias term G to the generated
    attention scores, where higher G indicates a higher prior probability of attending
    to a specific input.'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '**3.5.1\. 建模局部性的先验：** 为了建模某些类型数据的局部性，如文本，可以使用位置上的高斯分布作为先验注意力。这涉及到将生成的注意力分布与高斯密度相乘，并对生成的注意力分数进行归一化或添加偏置项
    G，其中更高的 G 表示对特定输入的先验概率更高。'
- en: 'Yang et al. [17] propose a method of predicting a central position for each
    input and defining the Gaussian bias accordingly:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: Yang 等人 [17] 提出了预测每个输入的中心位置并相应地定义高斯偏置的方法：
- en: '![](../Images/af94364ef136d40e6cb207f656871ecb.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/af94364ef136d40e6cb207f656871ecb.png)'
- en: Eq. 5
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 方程 5
- en: where 𝜎 denotes the standard deviation for the Gaussian. The Gaussian bias is
    defined as the negative of the squared distance between the central position and
    the input position, divided by the standard deviation of the Gaussian distribution.
    The standard deviation can be determined as a hyperparameter or predicted from
    the inputs.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 𝜎 表示高斯的标准差。高斯偏置定义为中心位置与输入位置之间的平方距离的负值，除以高斯分布的标准差。标准差可以作为超参数确定，也可以从输入中预测。
- en: '![](../Images/fbbdad900d98f2d1f54606ee25c4d9bc.png)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fbbdad900d98f2d1f54606ee25c4d9bc.png)'
- en: Fig. 11\. The proposed approach in [17] is illustrated, using a window size
    of 2 (D = 2). Photo from [[17](https://aclanthology.org/D18-1475/)].
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11\. 使用窗口大小为 2 (D = 2) 的[17]提出的方法进行说明。图片来自 [[17](https://aclanthology.org/D18-1475/)]。
- en: The Gaussian Transformer [18] model assumes that the central position for each
    input query 𝑞𝑖 is 𝑖, and defines the bias term 𝐺𝑖 𝑗 for the generated attention
    scores as
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 高斯 Transformer [18] 模型假设每个输入查询 𝑞𝑖 的中心位置为 𝑖，并将生成的注意力分数的偏置项 𝐺𝑖 𝑗 定义为
- en: '![](../Images/00d462bccc16e5a6fdbe1ea8805bdf2b.png)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/00d462bccc16e5a6fdbe1ea8805bdf2b.png)'
- en: Eq. 6
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 方程 6
- en: where 𝑤 is a non-negative scalar parameter controlling the deviation and 𝑏 is
    a negative scalar parameter reducing the weight for the central position.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 𝑤 是一个非负标量参数，控制偏差，𝑏 是一个负标量参数，减少中心位置的权重。
- en: '**3.5.2\. Prior from lower modules:** In Transformer architecture, attention
    distributions between adjacent layers are often found to be similar. Therefore,
    it is reasonable to use the attention distribution from a lower layer as a prior
    for computing attention in a higher layer. This can be achieved by combining the
    attention scores from the current layer with a weighted sum of the previous layer’s
    attention scores and a translation function that maps the previous scores to the
    prior to be applied.'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '**3.5.2\. 来源于低层模块：** 在 Transformer 架构中，相邻层之间的注意力分布通常被发现是相似的。因此，使用来自低层的注意力分布作为计算高层注意力的先验是合理的。这可以通过将当前层的注意力分数与前一层注意力分数的加权和以及将前一层分数映射到要应用的先验的转换函数相结合来实现。'
- en: '![](../Images/97d7e1995b33ec4e42a969251bb4f0eb.png)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/97d7e1995b33ec4e42a969251bb4f0eb.png)'
- en: Eq. 7
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: Eq. 7
- en: 'where A(𝑙) represents the *l-*th layer attention scores while *w*1​ and *w*2​
    control the relative importance of the previous attention scores and the current
    attention scores. Also, the function 𝑔: R𝑛×𝑛 → R𝑛×𝑛 translates the previous attention
    scores into a prior to be applied to the current attention scores.'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '其中 A(𝑙) 代表 *l-* 层注意力分数，而 *w*1 和 *w*2 控制之前的注意力分数和当前注意力分数的相对重要性。此外，函数 𝑔: R𝑛×𝑛
    → R𝑛×𝑛 将之前的注意力分数转化为应用于当前注意力分数的先验。'
- en: The *Predictive Attention Transformer* proposed in the paper [19] suggests using
    a 2D-convolutional layer on the previous attention scores to compute the final
    attention scores as a convex combination of the generated attention scores and
    the convolved scores. In other words, the weight parameters for the generated
    and convolved scores are set to 𝛼 and 1-𝛼, respectively, and the function 𝑔(·)
    in Eq. (6) is a convolutional layer. The paper presents experiments showing that
    training the model from scratch and fine-tuning it after adapting a pre-trained
    BERT model both lead to improvements over baseline models.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 论文 [19] 中提出的 *Predictive Attention Transformer* 建议在之前的注意力分数上使用 2D 卷积层来计算最终的注意力分数，该分数是生成的注意力分数和卷积分数的凸组合。换句话说，生成和卷积分数的权重参数分别设置为
    𝛼 和 1-𝛼，而 Eq. (6) 中的函数 𝑔(·) 是一个卷积层。论文中的实验表明，无论是从头开始训练模型还是在适配预训练 BERT 模型后进行微调，都比基线模型有了改进。
- en: The *Realformer* model proposed in [20] introduces a residual skip connection
    on attention maps by directly adding the previous attention scores to the newly
    generated ones. This can be seen as setting 𝑤 1 = 𝑤 2 = 1 and 𝑔(·) to be the identity
    map in Eq. (6). The authors conduct pre-training experiments on this model and
    report that it outperforms the baseline BERT model in multiple datasets, even
    with significantly lower pre-training budgets.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 论文 [20] 中提出的 *Realformer* 模型通过直接将之前的注意力分数添加到新生成的分数中，引入了对注意力图的残差跳跃连接。这可以视为在 Eq.
    (6) 中将 𝑤 1 = 𝑤 2 = 1 和 𝑔(·) 设置为恒等映射。作者在该模型上进行的预训练实验报告称，该模型在多个数据集上优于基线 BERT 模型，即使在显著降低预训练预算的情况下。
- en: '*Lazyformer* [21] proposes an innovative approach where attention maps are
    shared between adjacent layers to reduce computational costs. This is achieved
    by setting 𝑔(·) to identity and alternately switching between the settings of
    𝑤 1 = 0, 𝑤 2 = 1 and 𝑤 1 = 1, 𝑤 2 = 0\. This method enables the computation of
    attention maps only once and reuses them in succeeding layers. The pre-training
    experiments conducted by Lazyformer show that their model is not only efficient
    but also effective, outperforming the baseline models with significantly lower
    computation budgets.'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '*Lazyformer* [21] 提出了一个创新方法，通过在相邻层之间共享注意力图来降低计算成本。这是通过将 𝑔(·) 设为恒等映射并交替切换 𝑤
    1 = 0, 𝑤 2 = 1 和 𝑤 1 = 1, 𝑤 2 = 0 的设置来实现的。这种方法使得只需计算一次注意力图，并在后续层中重复使用。Lazyformer
    进行的预训练实验表明，他们的模型不仅高效，而且有效，超越了基线模型，并显著降低了计算预算。'
- en: '**3.5.3\. Prior as multi-task adapters:** The Prior as Multi-task Adapters
    approach uses trainable attention priors that enable efficient parameter sharing
    across tasks [22]. The Conditionally Adaptive Multi-Task Learning (CAMTL) [23]
    framework is a technique for multi-task learning that enables the efficient sharing
    of pre-trained models between tasks. CAMTL uses trainable attention prior, which
    depends on task encoding, to act as an adapter for multi-task inductive knowledge
    transfer. Specifically, the attention prior is represented as a block diagonal
    matrix that is added to the attention scores of upper layers in pre-trained Transformers:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '**3.5.3\. 作为多任务适配器的先验：** 作为多任务适配器的先验方法使用可训练的注意力先验，这些先验使得跨任务的参数共享更为高效 [22]。条件自适应多任务学习（CAMTL）[23]
    框架是一种多任务学习技术，它使得在任务之间高效共享预训练模型成为可能。CAMTL 使用依赖于任务编码的可训练注意力先验，作为多任务诱导知识转移的适配器。具体来说，注意力先验被表示为块对角矩阵，添加到预训练
    Transformer 的上层注意力分数中：'
- en: '![](../Images/cca478938fd9c353b29199a52fe294f5.png)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/cca478938fd9c353b29199a52fe294f5.png)'
- en: Eq. 8
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: Eq. 8
- en: in which, ⊕ represents direct sum, 𝐴𝑗 are trainable parameters with dimensions
    (𝑛/𝑚)×(𝑛/𝑚) and 𝛾𝑗 and 𝛽𝑗 are Feature Wise Linear Modulation functions with input
    and output dimensions of R𝐷𝑧 and (𝑛/𝑚)×(𝑛/𝑚), respectively [24]. The CAMTL framework
    specifies a maximum sequence length 𝑛𝑚𝑎𝑥 in implementation. The attention prior,
    which is a trainable matrix, is added to the attention scores of the upper layers
    in pre-trained Transformers. This addition creates an adapter that allows for
    parameter-efficient multi-task inductive knowledge transfer. The prior is organized
    as a block diagonal matrix for efficient computation.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 在其中，⊕ 表示直接和，𝐴𝑗 是具有 (𝑛/𝑚)×(𝑛/𝑚) 维度的可训练参数，𝛾𝑗 和 𝛽𝑗 是具有输入和输出维度为 R𝐷𝑧 和 (𝑛/𝑚)×(𝑛/𝑚)
    的特征线性调制函数 [24]。CAMTL 框架在实现中规定了最大序列长度 𝑛𝑚𝑎𝑥。注意力先验是一个可训练矩阵，它被添加到预训练 Transformer 的上层注意力分数中。这种添加创建了一个适配器，使得多任务诱导知识转移在参数上更高效。先验被组织为块对角矩阵以提高计算效率。
- en: '**3.5.4\. Attention with only prior:** Zhang et al. [25] have developed an
    alternative approach to attention distribution that does not rely on pair-wise
    interaction between inputs. Their method is called the “average attention network,”
    and it uses a discrete uniform distribution as the sole source of attention distribution.
    The values are then aggregated as a cumulative average of all values. To enhance
    the network’s expressiveness, a feed-forward gating layer is added on top of the
    average attention module. The benefit of this approach is that the modified Transformer
    decoder can be trained in a parallel manner, and it can decode like an RNN, avoiding
    the O(𝑇²) complexity associated with decoding.'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '**3.5.4\. 仅使用先验的注意力：** Zhang 等人 [25] 开发了一种替代的注意力分布方法，该方法不依赖于输入之间的成对交互。他们的方法称为“平均注意力网络”，它使用离散均匀分布作为注意力分布的唯一来源。然后将这些值聚合为所有值的累积平均值。为了增强网络的表达能力，在平均注意力模块上添加了一个前馈门控层。这种方法的好处是，修改后的
    Transformer 解码器可以以并行方式进行训练，并且能够像 RNN 一样解码，避免了与解码相关的 O(𝑇²) 复杂性。'
- en: similar to Yang et al. [17] and Guo et al. [18], which use a fixed local window
    for attention distribution, You et al. [26] incorporate a hardcoded Gaussian distribution
    attention for attention calculation. However, They completely ignore the calculated
    attention and solely use the Gaussian distribution for attention computation in
    which, the mean and variance are the hyperparameters. Provided it is implemented
    on self-attention, it can produce results close to the baseline models in machine
    translation tasks.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于 Yang 等人 [17] 和 Guo 等人 [18]，它们使用固定的局部窗口进行注意力分布，You 等人 [26] 将硬编码的高斯分布注意力用于注意力计算。然而，他们完全忽略了计算得到的注意力，只使用高斯分布进行注意力计算，其中均值和方差是超参数。只要在自注意力上实现，它就可以在机器翻译任务中产生接近基线模型的结果。
- en: 'Synthesizer [27] has proposed a novel way of generating attention scores in
    Transformers. Instead of using the traditional method of generating attention
    scores, they replace them with two variants: (1) learnable, randomly initialized
    attention scores, and (2) attention scores output by a feed-forward network that
    is only conditioned on the input being queried. The results of their experiments
    on machine translation and language modeling tasks demonstrate that these variants
    perform comparably to the standard Transformer model. However, the reason why
    these variants work is not fully explained, leaving room for further investigation.'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: Synthesizer [27] 提出了一种在 Transformers 中生成注意力分数的新方法。他们用两种变体替代传统的生成注意力分数的方法：(1)
    可学习的、随机初始化的注意力分数，以及 (2) 由仅对输入进行条件处理的前馈网络输出的注意力分数。他们在机器翻译和语言建模任务上的实验结果表明，这些变体的表现与标准
    Transformer 模型相当。然而，这些变体为何有效尚未完全解释，仍有进一步研究的空间。
- en: 3.6\. Improved multi-head mechanism
  id: totrans-133
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.6\. 改进的多头机制
- en: Multi-head attention is a powerful technique because it allows a model to attend
    to different parts of the input simultaneously. However, it is not guaranteed
    that each attention head will learn unique and complementary features. As a result,
    some researchers have explored methods to ensure that each attention head captures
    distinct information.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 多头注意力是一种强大的技术，因为它允许模型同时关注输入的不同部分。然而，不能保证每个注意力头都会学习到独特且互补的特征。因此，一些研究人员探索了确保每个注意力头捕捉到不同信息的方法。
- en: '**3.6.1\. Head behavior modeling:** Multi-head attention is a useful tool in
    natural language processing models as it enables the simultaneous processing of
    multiple inputs and feature representations [28]. However, the vanilla Transformer
    model lacks a mechanism to ensure that different attention heads capture distinct
    and non-redundant features. Additionally, there is no provision for interaction
    among the heads. To address these limitations, recent research has focused on
    introducing novel mechanisms that guide the behavior of attention heads or enable
    interaction between them.'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '**3.6.1\. 头部行为建模：** 多头注意力是自然语言处理模型中的一个有用工具，因为它允许同时处理多个输入和特征表示[28]。然而，传统的 Transformer
    模型缺乏确保不同注意力头捕捉到不同且非冗余特征的机制。此外，也没有头部之间相互作用的规定。为了解决这些局限性，近期的研究集中于引入新颖的机制来指导注意力头的行为或使它们之间能够进行交互。'
- en: 'In order to promote diversity among different attention heads, Li et al. [29]
    propose an additional regularization term in the loss function. This regularization
    consists of two parts: the first two aim to maximize the cosine distances between
    input subspaces and output representations, while the latter encourages dispersion
    of the positions attended by multiple heads through element-wise multiplication
    of their corresponding attention matrices. By adding this auxiliary term, the
    model is encouraged to learn a more diverse set of attention patterns across different
    heads, which can improve its performance on various tasks.'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 为了促进不同注意力头之间的多样性，Li 等人 [29] 在损失函数中提出了额外的正则化项。这个正则化由两部分组成：前两部分旨在最大化输入子空间和输出表示之间的余弦距离，而后者通过元素级乘法鼓励多个头部关注的位置的分散。通过添加这一辅助项，模型被鼓励在不同头部之间学习到更多样的注意力模式，从而提高在各种任务上的性能。
- en: Numerous studies have shown that pre-trained Transformer models exhibit certain
    self-attention patterns that do not align well with natural language processing.
    Kovaleva et al. [30] identify several of these patterns in BERT, including attention
    heads that focus exclusively on the special tokens [CLS] and [SEP]. To improve
    training, Deshpande and Narasimhan [31] suggest using an auxiliary loss function
    that measures the Frobenius norm between the attention distribution maps and predefined
    attention patterns. This approach introduces constraints to encourage more meaningful
    attention patterns.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 许多研究表明，预训练的 Transformer 模型展示了某些自注意力模式，这些模式与自然语言处理并不完全契合。Kovaleva 等人 [30] 在 BERT
    中识别出其中的几种模式，包括专注于特殊标记 [CLS] 和 [SEP] 的注意力头。为了改进训练，Deshpande 和 Narasimhan [31] 提出了使用辅助损失函数，该函数测量注意力分布图与预定义注意力模式之间的
    Frobenius 范数。这种方法引入了约束，以鼓励更有意义的注意力模式。
- en: In the paper by Shen et al. [32], a new mechanism called Talking-head Attention
    is introduced, which aims to encourage the model to transfer information between
    different attention heads in a learnable manner. This mechanism involves linearly
    projecting the generated attention scores from the hidden dimension to a new space
    with h_k heads, applying softmax in this space, and then projecting the results
    to another space with h_v heads for value aggregation. This way, the attention
    mechanism can learn to dynamically transfer information between the different
    attention heads, leading to improved performance in various natural language processing
    tasks.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 在Shen等人[32]的论文中，提出了一种名为“Talking-head Attention”的新机制，该机制旨在鼓励模型以可学习的方式在不同的注意力头之间传递信息。该机制包括将生成的注意力分数从隐藏维度线性投影到具有h_k个头的新空间，在该空间中应用softmax，然后将结果投影到另一个具有h_v个头的空间以进行值聚合。通过这种方式，注意力机制可以学习在不同注意力头之间动态传递信息，从而提高各种自然语言处理任务的性能。
- en: Collaborative Multi-head Attention is a mechanism proposed in [33] that involves
    the use of shared query and key projections, denoted as W𝑄 and W𝐾, respectively,
    along with a mixing vector m𝑖. This mixing vector is used to filter the projection
    parameters for the 𝑖-th head. Specifically, the attention computation is adapted
    to reflect this mechanism, resulting in a modified equation (3).
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 协作多头注意力是一种在[33]中提出的机制，涉及使用共享的查询和键投影，分别记作W𝑄和W𝐾，以及一个混合向量m𝑖。该混合向量用于过滤𝑖-th头的投影参数。具体而言，注意力计算被调整以反映这一机制，从而得出修改后的方程（3）。
- en: '![](../Images/bc4e7c6e5518925240d2d2681e63b0a7.png)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bc4e7c6e5518925240d2d2681e63b0a7.png)'
- en: Eq. 9
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: Eq. 9
- en: where all heads share W^q and W^k.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 其中所有头共享W^q和W^k。
- en: '**3.6.2\. Multi-head with restricted spans:**'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '**3.6.2\. 具有限制范围的多头注意力：**'
- en: 'The vanilla attention mechanism typically assumes full attention spans, allowing
    a query to attend to all key-value pairs. However, it has been observed that some
    attention heads tend to focus more on local contexts, while others attend to broader
    contexts. As a result, it may be advantageous to impose constraints on attention
    spans for specific purposes:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 标准的注意力机制通常假设全范围注意力，允许查询对所有键值对进行注意。然而，已经观察到一些注意力头更倾向于关注局部上下文，而其他注意力头则关注更广泛的上下文。因此，对特定目的施加注意力范围的约束可能是有利的：
- en: 'Locality: Restricting attention spans can explicitly impose local constraints,
    which can be beneficial in scenarios where locality is an important consideration.'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 局部性：限制注意力范围可以明确施加局部约束，这在局部性是重要考虑因素的情况下尤为有益。
- en: 'Efficiency: Appropriately implemented, such a model can scale to longer sequences
    without introducing additional memory usage or computational time.'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 效率：如果实施得当，这种模型可以在不增加额外内存使用或计算时间的情况下扩展到更长的序列。
- en: Restricting attention spans involves multiplying each attention distribution
    value with a mask value, followed by re-normalization. The mask value can be determined
    by a non-increasing function that maps a distance to a value in the range [0,
    1]. In vanilla attention, a mask value of 1 is assigned for all distances, as
    illustrated in Figure 12(a).
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 限制注意力范围涉及将每个注意力分布值与掩码值相乘，然后进行重新归一化。掩码值可以由一个非递增函数确定，该函数将距离映射到[0, 1]范围内的一个值。在标准注意力中，对于所有距离分配掩码值为1，如图12(a)所示。
- en: '![](../Images/6398180ac8234a6b560c662fad7c4831.png)'
  id: totrans-148
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6398180ac8234a6b560c662fad7c4831.png)'
- en: Fig. 12 showcases three distinct types of span masking functions denoted as
    𝑚(𝑥). The horizontal axis represents the distance 𝑥, while the vertical axis represents
    the corresponding mask value. This visual representation offers insights into
    the diverse behaviors and patterns exhibited by these masking functions, providing
    a clear visualization of how the mask values change with respect to the distance
    between key-value pairs. Image from [[1]](https://arxiv.org/abs/2106.04554v2).
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 图12展示了三种不同类型的范围掩码函数，记作𝑚(𝑥)。水平轴表示距离𝑥，而垂直轴表示相应的掩码值。这一视觉表示提供了这些掩码函数展示的不同行为和模式的洞见，清晰地展示了掩码值如何随着键值对之间的距离变化。图片来源于[[1]](https://arxiv.org/abs/2106.04554v2)。
- en: In a study by Sukhbaatar et al. [34], a novel approach was proposed, introducing
    a learnable attention span that is depicted in the intriguing Figure 12(b). This
    innovative technique utilizes a mask parameterized by a learnable scalar 𝑧, combined
    with a hyperparameter 𝑅, to adaptively modulate the attention span. Remarkably,
    experimental results on character-level language modeling demonstrated that these
    adaptive-span models outperformed the baseline models while requiring significantly
    fewer FLOPS. Notably, an interesting observation was made that lower layers of
    the model tended to exhibit smaller learned spans, whereas higher layers displayed
    larger spans. This intriguing finding suggests that the model can autonomously
    learn a hierarchical composition of features, showcasing its exceptional ability
    to capture complex patterns and structures in the data.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 在Sukhbaatar等人 [34] 的研究中，提出了一种新颖的方法，引入了一个可学习的注意力范围，如有趣的图12(b)所示。这种创新技术利用了由可学习标量
    𝑧 参数化的掩码，并结合超参数 𝑅，自适应地调节注意力范围。实验结果表明，这些自适应范围模型在字符级语言建模上优于基线模型，同时需要显著更少的FLOPS。值得注意的是，一个有趣的观察是模型的低层通常显示出较小的学习范围，而高层则表现出较大的范围。这一有趣发现表明模型可以自主学习特征的层次组合，展示了其捕捉数据中复杂模式和结构的卓越能力。
- en: The *Multi-Scale Transformer* [35] presents a novel approach to attention spans
    that challenges the traditional paradigm. Unlike vanilla attention, which assumes
    a uniform attention span across all heads, this innovative model introduces a
    fixed attention span with dynamic scaling in different layers. Illustrated in
    Fig. 12(c), the fixed attention span acts as a window that can be scaled up or
    down, controlled by a scale value denoted as 𝑤.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '*多尺度变换器* [35] 提出了一个新的注意力范围方法，挑战了传统范式。与假设所有头部具有统一注意力范围的普通注意力不同，该创新模型引入了一个具有动态缩放的固定注意力范围。如图12(c)所示，固定注意力范围作为一个可以缩放的窗口，由标记为
    𝑤 的尺度值控制。'
- en: '![](../Images/6ac644da107d37c3434399808e584639.png)'
  id: totrans-152
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6ac644da107d37c3434399808e584639.png)'
- en: Fig. 13\. Multi-Scale Multi-Head Self-Attention, where three heads are depicted,
    each representing a different scale. The blue, green, and red boxes illustrate
    the scales of ω = 1, ω = 3, and ω = 5, respectively. Photo by [author](https://www.linkedin.com/in/soran-ghaderi/).
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 图13\. 多尺度多头自注意力，其中描绘了三个头部，每个头部代表不同的尺度。蓝色、绿色和红色框分别表示尺度 ω = 1、ω = 3 和 ω = 5。照片由
    [作者](https://www.linkedin.com/in/soran-ghaderi/) 提供。
- en: The scale values vary, with higher layers favoring larger scales for broader
    contextual dependencies and lower layers opting for smaller scales for more localized
    attention as shown in Figure 13\. The experimental results of the Multi-Scale
    Transformer demonstrate its superior performance over baseline models on various
    tasks, showcasing its potential for more efficient and effective language processing.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 尺度值各异，高层倾向于使用较大的尺度以获取更广泛的上下文依赖，而低层则选择较小的尺度以获得更局部的关注，如图13所示。多尺度变换器的实验结果显示，它在各种任务上表现出优于基线模型的性能，展示了其在语言处理上的更高效和有效的潜力。
- en: '**3.6.3\. Multi-head with refined aggregation:**'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '**3.6.3\. 多头与精细化聚合：**'
- en: The vanilla multi-head attention mechanism, as proposed by Vaswani et al. [28],
    involves the computation of multiple attention heads that operate in parallel
    to generate individual output representations. These representations are then
    concatenated and subjected to a linear transformation, as defined in Eq. (11),
    to obtain the final output representation. By combining Eqs. (10), (11), and (12),
    it can be observed that this concatenate-and-project formulation is equivalent
    to a summation over re-parameterized attention outputs. This approach allows for
    efficient aggregation of the diverse attention head outputs, enabling the model
    to capture complex dependencies and relationships in the input data.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 普通的多头注意力机制，如Vaswani等人 [28] 提出的，涉及计算多个并行操作的注意力头，以生成各自的输出表示。这些表示然后被串联，并进行线性变换，如Eq.
    (11) 定义，以获得最终的输出表示。通过结合 Eq. (10)、(11) 和 (12)，可以观察到这种串联与投影的形式等同于对重新参数化的注意力输出进行求和。这种方法允许高效地聚合多样的注意力头输出，使模型能够捕捉输入数据中的复杂依赖关系和关系。
- en: '![](../Images/07b6abd9b22daba4413bd081f9b09d01.png)'
  id: totrans-157
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/07b6abd9b22daba4413bd081f9b09d01.png)'
- en: Eq. 10
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: Eq. 10
- en: and
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 和
- en: '![](../Images/c6fe42c1c28247a1f9ad3f0c572be9dc.png)'
  id: totrans-160
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c6fe42c1c28247a1f9ad3f0c572be9dc.png)'
- en: Eq. 11
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: Eq. 11
- en: where
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 其中
- en: '![](../Images/9d6b8fb6740b786dda46a499be5a1038.png)'
  id: totrans-163
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9d6b8fb6740b786dda46a499be5a1038.png)'
- en: Eq. 12
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: Eq. 12
- en: To facilitate the aggregation process, the weight matrix W𝑂 ∈ R𝐷𝑚 ×𝐷𝑚 used for
    the linear transformation is partitioned into 𝐻 blocks, where 𝐻 represents the
    number of attention heads.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 为了便于聚合过程，线性变换使用的权重矩阵W𝑂 ∈ R𝐷𝑚 ×𝐷𝑚 被划分为𝐻个块，其中𝐻表示注意力头的数量。
- en: '![](../Images/eef0efcb393851c8ccb50c23f836825a.png)'
  id: totrans-166
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/eef0efcb393851c8ccb50c23f836825a.png)'
- en: Eq. 13
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: Eq. 13
- en: 'The weight matrix W𝑂_𝑖, with dimension 𝐷𝑣 × 𝐷𝑚, is used for the linear transformation
    in each attention head, allowing for re-parameterized attention outputs through
    the concatenate-and-project formulation, as defined in Eq. (14):'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 权重矩阵W𝑂_𝑖，维度为𝐷𝑣 × 𝐷𝑚，用于每个注意力头中的线性变换，通过连接-投影公式重新参数化注意力输出，如Eq. (14)所定义：
- en: '![](../Images/f2ecb33a4229e43be136ee5f3e0da112.png)'
  id: totrans-169
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f2ecb33a4229e43be136ee5f3e0da112.png)'
- en: Eq. 14
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: Eq. 14
- en: Some researchers may argue that the straightforward aggregate-by-summation approach
    may not fully leverage the expressive power of multi-head attention and that a
    more complex aggregation scheme could be more desirable.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 一些研究人员可能会争辩说，简单的加和聚合方法可能无法充分利用多头注意力的表达能力，更复杂的聚合方案可能更为理想。
- en: Gu and Feng [36] and Li et al. [37] propose employing routing methods originally
    conceived for capsule networks [38] as a means to further aggregate information
    derived from distinct attention heads. Through a process of transforming the outputs
    of attention heads into input capsules and subsequently undergoing an iterative
    routing procedure, output capsules are obtained. These output capsules are then
    concatenated to serve as the final output of the multi-head attention mechanism.
    Notably, the dynamic routing [38] and EM routing [39] mechanisms employed in these
    works introduce additional parameters and computational overhead. Nevertheless,
    Li et al. [37] empirically demonstrate that selectively applying the routing mechanism
    to the lower layers of the model achieves an optimal balance between translation
    performance and computational efficiency.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: Gu 和 Feng [36] 以及 Li 等人 [37] 提出了使用最初为胶囊网络 [38] 设计的路由方法，作为进一步聚合来自不同注意力头信息的一种手段。通过将注意力头的输出转化为输入胶囊并随后经历迭代路由过程，获得输出胶囊。这些输出胶囊随后被连接作为多头注意力机制的最终输出。值得注意的是，这些工作中使用的动态路由
    [38] 和 EM 路由 [39] 机制引入了额外的参数和计算开销。然而，Li 等人 [37] 实证表明，选择性地将路由机制应用于模型的较低层次能在翻译性能和计算效率之间取得最佳平衡。
- en: '**3.6.4\. Other multi-head modifications:**'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: '**3.6.4\. 其他多头修改：**'
- en: In addition to the aforementioned modifications, several other approaches have
    been proposed to enhance the performance of the multi-head attention mechanism.
    Shazeer [40] introduced the concept of multi-query attention, where key-value
    pairs are shared among all attention heads. This reduces the memory bandwidth
    requirements during decoding and leads to faster decoding, albeit with minor quality
    degradation compared to the baseline. On the other hand, Bhojanapalli et al. [41]
    identified that the size of attention keys could impact their ability to represent
    arbitrary distributions. To address this, they proposed disentangling the head
    size from the number of heads, contrary to the conventional practice of setting
    the head size as 𝐷𝑚/ℎ, where 𝐷𝑚 is the model dimension and ℎ is the number of
    heads.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 除了上述修改之外，还提出了几种其他方法来增强多头注意力机制的性能。Shazeer [40] 引入了多查询注意力的概念，其中键值对在所有注意力头之间共享。这减少了解码过程中的内存带宽需求，并使解码速度更快，尽管与基线相比存在轻微的质量下降。另一方面，Bhojanapalli
    等人 [41] 发现注意力键的大小可能会影响它们表示任意分布的能力。为了解决这个问题，他们提出将头的大小与头的数量分开，这与传统上将头的大小设定为𝐷𝑚/ℎ的做法相反，其中𝐷𝑚是模型维度，ℎ是头的数量。
- en: 4\. Summary
  id: totrans-175
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 4\. 总结
- en: In conclusion, the taxonomy of transformers and the various advancements in
    attention mechanisms have significantly expanded the capabilities and efficiency
    of transformer-based models. Sparse attention techniques, such as position-based
    and content-based sparse attention, along with linearized attention, have addressed
    the computational limitations of traditional dense attention. Query prototyping
    and memory compression methods have introduced innovative ways to improve the
    efficiency of attention mechanisms. Low-rank self-attention has enabled parameterization
    and approximation techniques for more efficient attention computations. Incorporating
    priors, such as locality modeling, lower module priors, and multi-task adapters,
    has shown promising results in improving attention mechanisms. Lastly, modifications
    to the multi-head mechanism, such as head behavior modeling, restricted spans,
    refined aggregation, and other variations, have shown the potential to further
    enhance the performance of transformer-based models.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，变换器的分类及注意机制的各种进展显著扩展了基于变换器的模型的能力和效率。稀疏注意技术，如基于位置和基于内容的稀疏注意，以及线性化注意，解决了传统稠密注意的计算限制。查询原型和内存压缩方法引入了创新的方法来提高注意机制的效率。低秩自注意使得参数化和近似技术用于更高效的注意计算。引入先验，如局部建模、较低模块先验和多任务适配器，在提高注意机制方面表现出令人鼓舞的结果。最后，对多头机制的修改，如头行为建模、受限跨度、精细聚合及其他变体，展示了进一步提升基于变换器的模型性能的潜力。
- en: These advancements in attention mechanisms offer exciting prospects for future
    research and applications in various domains, such as natural language processing,
    computer vision, and machine translation. By leveraging these innovative techniques,
    transformer-based models can continue to push the boundaries of performance and
    efficiency, opening up new possibilities for advanced machine-learning applications.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 注意机制的这些进展为未来在自然语言处理、计算机视觉和机器翻译等多个领域的研究和应用提供了令人兴奋的前景。通过利用这些创新技术，基于变换器的模型可以继续突破性能和效率的界限，为高级机器学习应用打开新的可能性。
- en: '**Please share your thoughts, questions, and opinions in the comments section
    below.**'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: '**请在下方评论区分享你的想法、问题和意见。**'
- en: '5\. Upcoming Topics: Unveiling the Next Chapters in the Transformer Journey'
  id: totrans-179
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 5\. 即将到来的主题：揭示变换器旅程中的下一个章节
- en: 'In future articles, the following topics will be discussed in more detail:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 在未来的文章中，将更详细地讨论以下主题：
- en: '**Other Module-Level Modifications:** These cover additional modifications
    at the module level, such as position representations, layer normalization, and
    position-wise feed-forward networks (FFN), which play crucial roles in the performance
    and efficiency of transformer-based models.'
  id: totrans-181
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**其他模块级修改：** 这些涵盖了模块级的其他修改，如位置表示、层归一化和位置-wise 前馈网络（FFN），它们在基于变换器的模型的性能和效率中发挥了至关重要的作用。'
- en: '**Architecture-Level Variants:** This section will explore various architecture-level
    variants of transformers, including adapting transformers to be lightweight, strengthening
    cross-block connectivity, adaptive computation time, transformers with divide-and-conquer
    strategies, and exploring alternative architecture designs to further improve
    the capabilities and efficiency of transformers.'
  id: totrans-182
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**架构级变体：** 本节将探讨变换器的各种架构级变体，包括将变换器调整为轻量级、增强跨块连接、适应性计算时间、采用分而治之策略的变换器，以及探索替代架构设计以进一步提升变换器的能力和效率。'
- en: '**Pre-trained Transformers:** delving into pre-trained transformers, which
    have gained significant attention in recent years for their ability to leverage
    large-scale pre-training data for improving the performance of downstream tasks,
    this section will discuss different pre-training techniques, such as BERT, GPT,
    and T5.'
  id: totrans-183
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**预训练变换器：** 深入探讨预训练变换器，这些变换器近年来因其利用大规模预训练数据提高下游任务性能的能力而受到广泛关注，本节将讨论不同的预训练技术，如
    BERT、GPT 和 T5。'
- en: '**Applications of Transformers:** A diverse range of applications will be highlighted
    in this part, where transformers have shown remarkable performance, including
    natural language processing, computer vision, speech recognition, and recommendation
    systems, among others. The potential and versatility of transformers in various
    domains will be discussed.'
  id: totrans-184
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**变压器的应用：** 在这一部分中将突出显示变压器在各种领域展示出卓越性能的多样化应用，包括自然语言处理、计算机视觉、语音识别和推荐系统等。将讨论变压器在各个领域中的潜力和多样性。'
- en: '**Research Directions:** Providing insights into the future directions of research
    and development in the field of transformers, this component discusses emerging
    trends, challenges, and opportunities for further advancements in transformer-based
    models, offering a glimpse into the exciting possibilities of transformers in
    the years to come.'
  id: totrans-185
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**研究方向：** 提供关于变压器研究和开发未来方向的见解，讨论变压器模型在新兴趋势、挑战和机会方面的进一步发展，展示变压器在未来几年里的令人兴奋的可能性。'
- en: By covering these topics, the article aims to provide a comprehensive overview
    of the advancements, modifications, applications, and future directions of transformers,
    shedding light on the potential of these powerful models in driving the next generation
    of machine learning and artificial intelligence applications.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 通过涵盖这些主题，本文旨在全面介绍变压器的进展、修改、应用和未来发展方向，揭示这些强大模型在推动下一代机器学习和人工智能应用中的潜力。
- en: 'Contributing to TransformerX: Ways to Get Involved'
  id: totrans-187
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参与TransformerX项目：参与方式
- en: '[](https://github.com/tensorops/TransformerX?source=post_page-----e14952226398--------------------------------)
    [## GitHub - tensorops/TransformerX: Flexible Python library providing building
    blocks (layers) for…'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://github.com/tensorops/TransformerX?source=post_page-----e14952226398--------------------------------)
    [## GitHub - tensorops/TransformerX: Flexible Python library providing building
    blocks (layers) for…'
- en: Flexible Python library providing building blocks (layers) for reproducible
    Transformers research (Tensorflow ✅…
  id: totrans-189
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 灵活的Python库，为可重现的Transformer研究提供构建块（层）（Tensorflow ✅…
- en: github.com](https://github.com/tensorops/TransformerX?source=post_page-----e14952226398--------------------------------)
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: '[github.com](https://github.com/tensorops/TransformerX?source=post_page-----e14952226398--------------------------------)'
- en: 'I am grateful to those who have been inspiring and encouraging in developing
    the TransformerX library. We always look for your contributions in the following
    forms:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 我感谢那些在开发TransformerX库中给予启发和鼓励的人。我们一直期待您以以下形式做出贡献：
- en: '**Contributing code and developing new layers:** Check out [specific issues](https://github.com/tensorops/TransformerX/issues?q=is%3Aissue+is%3Aopen+label%3A%22issue+list%22)
    labeled `issue_list`for ideas, and start implementing them with the help of our
    guides.'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**贡献代码和开发新层次：** 查看[特定问题](https://github.com/tensorops/TransformerX/issues?q=is%3Aissue+is%3Aopen+label%3A%22issue+list%22)标记为`issue_list`的想法，并在我们的指南的帮助下开始实施它们。'
- en: '**Suggesting new features or reporting bugs:** Create issues to share your
    suggestions or report any issues you encounter.'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**建议新功能或报告错误：** 创建问题来分享你的建议或报告你遇到的任何问题。'
- en: '**Writing documentation and tutorial resources:** Help us improve documentation
    for the library.'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**编写文档和教程资源：** 帮助我们改进库的文档。'
- en: '**Sharing and writing about TransformerX on social media:** Mention us on Twitter
    for a reshare or on LinkedIn.'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**在社交媒体上分享和撰写TransformerX相关内容：** 在Twitter上提及我们，进行转发，或在LinkedIn上分享。'
- en: Don’t worry if you are not sure about coding, We **will help you** through every
    step to make your first contribution.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你不确定编码，不用担心，我们会在每一步都**帮助你**进行第一次贡献。
- en: 🌟 By giving the TransformerX a star on GitHub you show your support for the
    project! Your contribution helps us continue to improve and develop this library.
    Thank you! 🚀
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 🌟 在GitHub上为TransformerX点星，展示你对项目的支持！你的贡献帮助我们不断改进和发展这个库。谢谢！🚀
- en: Like to see more related content?
  id: totrans-198
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 喜欢看更多相关内容吗？
- en: Follow me on [**Twitter**](https://twitter.com/soranghadri)🐦, [**GitHub**](https://github.com/soran-ghaderi)🚀
    (where I’m most active), and let’s connect on [**LinkedIn**](https://www.linkedin.com/in/soran-ghaderi/)💼
    and of course, follow my [Medium](http://soran-ghaderi.medium.com) 📝, and [subscribe](https://soran-ghaderi.medium.com/subscribe)
    to my posts.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 关注我的[**Twitter**](https://twitter.com/soranghadri)🐦，[**GitHub**](https://github.com/soran-ghaderi)🚀（我最活跃的平台），让我们在[**LinkedIn**](https://www.linkedin.com/in/soran-ghaderi/)💼上连接，当然，关注我的[Medium](http://soran-ghaderi.medium.com)
    📝，并[订阅](https://soran-ghaderi.medium.com/subscribe)我的文章。
- en: Also, here is my [**website**](http://soran-ghaderi.github.io/).
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，这里是我的[**网站**](http://soran-ghaderi.github.io/)。
- en: References
  id: totrans-201
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: For the list of references, please visit this [gist](https://gist.github.com/soran-ghaderi/41abae1310007bd962c7b7bb5406556a).
    This will provide you with a comprehensive list of sources that were referenced
    in this article, for further reading and in-depth information on the topic. Happy
    exploring!
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 有关参考文献的列表，请访问这个[gist](https://gist.github.com/soran-ghaderi/41abae1310007bd962c7b7bb5406556a)。这将为您提供一份全面的参考资料列表，以便进一步阅读和深入了解该主题。祝您探索愉快！
