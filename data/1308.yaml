- en: Deploy Your Local GPT Server With Triton
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Triton部署本地GPT服务器
- en: 原文：[https://towardsdatascience.com/deploy-your-local-gpt-server-with-triton-a825d528aa5d?source=collection_archive---------3-----------------------#2023-04-14](https://towardsdatascience.com/deploy-your-local-gpt-server-with-triton-a825d528aa5d?source=collection_archive---------3-----------------------#2023-04-14)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/deploy-your-local-gpt-server-with-triton-a825d528aa5d?source=collection_archive---------3-----------------------#2023-04-14](https://towardsdatascience.com/deploy-your-local-gpt-server-with-triton-a825d528aa5d?source=collection_archive---------3-----------------------#2023-04-14)
- en: How to run large language models on your local server
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何在本地服务器上运行大型语言模型
- en: '[](https://medium.com/@bnjmn_marie?source=post_page-----a825d528aa5d--------------------------------)[![Benjamin
    Marie](../Images/3ea1ad230cb1e67610418a8e36a5e5dd.png)](https://medium.com/@bnjmn_marie?source=post_page-----a825d528aa5d--------------------------------)[](https://towardsdatascience.com/?source=post_page-----a825d528aa5d--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----a825d528aa5d--------------------------------)
    [Benjamin Marie](https://medium.com/@bnjmn_marie?source=post_page-----a825d528aa5d--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@bnjmn_marie?source=post_page-----a825d528aa5d--------------------------------)[![Benjamin
    Marie](../Images/3ea1ad230cb1e67610418a8e36a5e5dd.png)](https://medium.com/@bnjmn_marie?source=post_page-----a825d528aa5d--------------------------------)[](https://towardsdatascience.com/?source=post_page-----a825d528aa5d--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----a825d528aa5d--------------------------------)
    [Benjamin Marie](https://medium.com/@bnjmn_marie?source=post_page-----a825d528aa5d--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fad2a414578b3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeploy-your-local-gpt-server-with-triton-a825d528aa5d&user=Benjamin+Marie&userId=ad2a414578b3&source=post_page-ad2a414578b3----a825d528aa5d---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----a825d528aa5d--------------------------------)
    ·8 min read·Apr 14, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fa825d528aa5d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeploy-your-local-gpt-server-with-triton-a825d528aa5d&user=Benjamin+Marie&userId=ad2a414578b3&source=-----a825d528aa5d---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[查看](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fad2a414578b3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeploy-your-local-gpt-server-with-triton-a825d528aa5d&user=Benjamin+Marie&userId=ad2a414578b3&source=post_page-ad2a414578b3----a825d528aa5d---------------------post_header-----------)
    发表在[Towards Data Science](https://towardsdatascience.com/?source=post_page-----a825d528aa5d--------------------------------)
    ·8分钟阅读·2023年4月14日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fa825d528aa5d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeploy-your-local-gpt-server-with-triton-a825d528aa5d&user=Benjamin+Marie&userId=ad2a414578b3&source=-----a825d528aa5d---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fa825d528aa5d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeploy-your-local-gpt-server-with-triton-a825d528aa5d&source=-----a825d528aa5d---------------------bookmark_footer-----------)![](../Images/aa5dcc0e3ae51b6c33eee3078226b497.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fa825d528aa5d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeploy-your-local-gpt-server-with-triton-a825d528aa5d&source=-----a825d528aa5d---------------------bookmark_footer-----------)![](../Images/aa5dcc0e3ae51b6c33eee3078226b497.png)'
- en: Image from [Pixabay](https://pixabay.com/photos/nvidia-graphic-card-bitcoin-gpu-5264921/)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来自[Pixabay](https://pixabay.com/photos/nvidia-graphic-card-bitcoin-gpu-5264921/)
- en: Using OpenAI GPT models is possible only through OpenAI API. In other words,
    you must share your data with OpenAI to use their GPT models.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 使用OpenAI GPT模型仅通过OpenAI API是可能的。换句话说，你必须与OpenAI分享数据才能使用其GPT模型。
- en: Data confidentiality is at the center of many businesses and a priority for
    most individuals. Sending or receiving highly private data on the Internet to
    a private corporation is often not an option.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 数据机密性是许多企业的核心，也是大多数个人的优先事项。在互联网上向私人公司发送或接收高度私人数据通常不是一种选择。
- en: For these reasons, you may be interested in running your own GPT models to process
    locally your personal or business data.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 出于这些原因，你可能会对在本地运行自己的GPT模型以处理个人或业务数据感兴趣。
- en: Fortunately, there are many open-source alternatives to OpenAI GPT models. They
    are not as good as GPT-4, yet, but can compete with GPT-3.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，有许多开源替代品可供选择，虽然它们尚不如GPT-4，但能够与GPT-3竞争。
- en: 'For instance, EleutherAI proposes several GPT models: GPT-J, GPT-Neo, and GPT-NeoX.
    They are all fully documented, open, and under a license permitting commercial
    use.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，EleutherAI 提供了几个 GPT 模型：GPT-J、GPT-Neo 和 GPT-NeoX。它们都有详细的文档，开源，并且在允许商业使用的许可证下。
- en: These models are also big. The smallest, GPT-J, takes almost 10 Gb of disk space
    when compressed (6 billion parameters). On some machines, loading such models
    can take a lot of time. Ideally, we would need a local server that would keep
    the model fully loaded in the background and ready to be used.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 这些模型也很大。最小的 GPT-J 在压缩后占用近 10 GB 的磁盘空间（60 亿个参数）。在某些机器上，加载这些模型可能需要很长时间。理想情况下，我们需要一个本地服务器来保持模型在后台完全加载并随时准备使用。
- en: 'One way to do that is to run GPT on a local server using a dedicated framework
    such as nVidia Triton ([BSD-3 Clause license](https://github.com/triton-inference-server/server/blob/main/LICENSE)).
    *Note: By “server” I don’t mean a physical machine. Triton is just a framework
    that can you install on any machine.*'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 其中一种方法是使用专用框架（如 nVidia Triton）在本地服务器上运行 GPT ([BSD-3 Clause 许可证](https://github.com/triton-inference-server/server/blob/main/LICENSE))。*注意：所谓“服务器”并不是指物理机器。Triton
    只是一个可以安装在任何机器上的框架。*
- en: Triton with a FasterTransformer ([Apache 2.0 license](https://github.com/NVIDIA/FasterTransformer/blob/main/LICENSE))
    backend manages CPU and GPU loads during all the steps of prompt processing. Once
    Triton hosts your GPT model, each one of your prompts will be preprocessed and
    post-processed by FastTransformer in an optimal way based on your hardware configuration.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 FasterTransformer ([Apache 2.0 许可证](https://github.com/NVIDIA/FasterTransformer/blob/main/LICENSE))
    后端的 Triton 在整个提示处理过程中管理 CPU 和 GPU 的负载。一旦 Triton 托管了你的 GPT 模型，你的每一个提示都会由 FastTransformer
    根据你的硬件配置以最佳方式进行预处理和后处理。
- en: In this article, I will show you how to serve a GPT-J model for your applications
    using Triton Inference Server. I chose GPT-J because it is one of the smallest
    GPT models which is both performant and exploitable for commercial use ([Apache
    2.0 license](https://huggingface.co/EleutherAI/gpt-j-6b)).
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我将向你展示如何使用 Triton Inference Server 为你的应用程序提供 GPT-J 模型。我选择 GPT-J 是因为它是最小的
    GPT 模型之一，性能良好且可用于商业用途 ([Apache 2.0 许可证](https://huggingface.co/EleutherAI/gpt-j-6b))。
- en: Requirements
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 需求
- en: Hardware
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 硬件
- en: You need at least one GPU supporting CUDA 11 or higher. We will run a large
    model, GPT-J, so your GPU should have at least 12 GB of VRAM.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 你需要至少一张支持 CUDA 11 或更高版本的 GPU。我们将运行一个大型模型 GPT-J，因此你的 GPU 应该至少有 12 GB 的 VRAM。
- en: Setting up the Triton server and processing the model take also a significant
    amount of hard drive space. You should have at least 50 GB available.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 设置 Triton 服务器和处理模型也需要相当大的硬盘空间。你应该至少有 50 GB 的可用空间。
- en: OS
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 操作系统
- en: You need a UNIX OS, preferably Ubuntu or Debian. If you have another UNIX OS,
    it will work as well but you will have to adapt all the commands that download
    and install packages to the package manager of your OS.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 你需要一个 UNIX 操作系统，最好是 Ubuntu 或 Debian。如果你使用其他 UNIX 操作系统也可以，但你需要将所有下载和安装软件包的命令调整为适合你操作系统的包管理器。
- en: 'I ran all the commands presented in this tutorial on Ubuntu 20.04 under WSL2\.
    *Note: I ran into some issues with WSL2 that I will explain but that you may not
    have if you are running a native Ubuntu.*'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 我在 WSL2 的 Ubuntu 20.04 上运行了本教程中呈现的所有命令。*注意：我遇到了一些 WSL2 的问题，我会进行说明，但如果你运行的是原生
    Ubuntu，可能不会遇到这些问题。*
- en: For some of the commands, you will need “sudo” privileges.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 对于一些命令，你需要“sudo”权限。
- en: Dependencies
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 依赖项
- en: FasterTransformer requires [CMAKE](https://cmake.org/install/) for compilation.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: FasterTransformer 需要 [CMAKE](https://cmake.org/install/) 进行编译。
- en: There are other dependencies but I’ll provide a guide to install them, inside
    this tutorial, when necessary.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 还有其他依赖项，但在必要时，我会在本教程中提供安装指南。
- en: Setting up a docker container for Triton
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为 Triton 设置 Docker 容器
- en: We will use a docker image already prepared by nVidia. It is provided in a specific
    branch of the “fastertransformer_backend”.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用 nVidia 已经准备好的 Docker 镜像。它提供在“fastertransformer_backend”的特定分支中。
- en: So first we have to clone this repository and get this branch.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 所以首先我们需要克隆这个仓库并获取这个分支。
- en: '[PRE0]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: If you don’t have Docker, jump to the end of this article where you will find
    a short tutorial to install it.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你没有 Docker，请跳到本文末尾，你将找到一个简短的安装教程。
- en: The following command builds the docker for the Triton server.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 以下命令用于构建 Triton 服务器的 Docker 镜像。
- en: '[PRE1]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'It should run smoothly. *Note: In my case, I had several problems with GPG
    keys that were missing or not properly installed. If you have a similar issue,
    drop a message in the comments. I’ll be happy to help you!*'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 它应该顺利运行。*注意：在我的情况下，我遇到了几个缺失或未正确安装的GPG密钥的问题。如果你有类似的问题，请在评论中留言。我很乐意帮助你！*
- en: 'Then, we can run the docker image:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以运行docker镜像：
- en: '[PRE2]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'If it succeeds, you will see something like this:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 如果成功，你将看到类似这样的内容：
- en: '![](../Images/6bb49a3d37fffe75041ab4e8961fcb87.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6bb49a3d37fffe75041ab4e8961fcb87.png)'
- en: Image by author
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图像
- en: '*Note: If you see the error “docker: Error response from daemon: could not
    select device driver “” with capabilities: [[gpu]].”, it may mean that you don’t
    have the nVidia container installed. I provide installation instructions at the
    end of the article.*'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '*注意：如果你看到错误“docker: Error response from daemon: could not select device driver
    “” with capabilities: [[gpu]].”，这可能意味着你没有安装nVidia容器。我在文章末尾提供了安装说明。*'
- en: '**Don’t leave or close this container: All the remaining steps must be performed
    inside it.**'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '**不要离开或关闭此容器：所有剩余的步骤必须在其中完成。**'
- en: Prepare GPT-J for FasterTransformer
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为FasterTransformer准备GPT-J
- en: The next steps prepare the GPT-J model.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来的步骤准备GPT-J模型。
- en: 'We need to get and configure FasterTransformer. *Note: You will need CMAKE
    for this step.*'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要获取并配置FasterTransformer。*注意：你需要CMAKE来完成这一步。*
- en: '[PRE3]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: It may take enough time to drink a coffee ☕.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 喝杯咖啡可能需要一点时间 ☕。
- en: FasterTransformer is used to run the model inside the Triton server. It can
    manage preprocessing and post-processing of input/output.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: FasterTransformer用于在Triton服务器内部运行模型。它可以管理输入/输出的预处理和后处理。
- en: 'Now, we can get GPT-J:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以获取GPT-J：
- en: '[PRE4]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: This command first downloads the model and then extracts it.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 此命令首先下载模型，然后提取它。
- en: It may take enough time to drink two coffees ☕☕or to take a nap if you don’t
    have high-speed Internet. There is around 10 GB to download.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你的互联网速度不高，可能需要一点时间喝两杯咖啡 ☕☕或者打个盹。下载内容大约为10 GB。
- en: The next step is the conversion of the model weights to FasterTransformer format.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是将模型权重转换为FasterTransformer格式。
- en: '[PRE5]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'I put “1” for “ — n-inference-gpus” because I have only 1 GPU but if you have
    more you can put a higher number. *Note: I added “nvidia-cuda-nvcc” because it
    was needed in my environment. It may already be installed in yours. If you have
    other issues with another library called “ptxas” drop a comment I’ll answer it.*'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 我将“1”设置为“ — n-inference-gpus”，因为我只有1个GPU，但如果你有更多，你可以设置更高的数字。*注意：我添加了“nvidia-cuda-nvcc”，因为在我的环境中需要它。你的环境中可能已经安装了。如果你遇到其他名为“ptxas”的库的问题，请留言，我会回答。*
- en: 'If you are running into an error with the previous command about “jax” or “jaxlib”,
    the following command solved it for me:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你在运行之前的命令时遇到关于“jax”或“jaxlib”的错误，以下命令对我有帮助：
- en: '[PRE6]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Optimization
  id: totrans-59
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 优化
- en: 'Before starting the server, it is also recommended to run the kernel auto-tuning.
    It will find among all the low-level algorithms the best one given the architecture
    of GPT-J and your machine hardware. gpt_gemm will do that:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在启动服务器之前，还建议运行内核自动调优。它会在所有低级算法中找到最适合GPT-J架构和你的机器硬件的算法。`gpt_gemm`会完成这项工作：
- en: '[PRE7]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: It should produce a file named “gemm_config.in”.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 它应该生成一个名为“gemm_config.in”的文件。
- en: Configuration of FasterTransformer
  id: totrans-63
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: FasterTransformer配置
- en: 'Now, we want to configure the server to run GPT-J. Find and open the following
    file:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们要配置服务器以运行GPT-J。找到并打开以下文件：
- en: '*fastertransformer_backend/all_models/gptj/fastertransformer/config.pbtxt*'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '*fastertransformer_backend/all_models/gptj/fastertransformer/config.pbtxt*'
- en: 'Set up the parameter for your number of GPUs. *Note: It must be the same as
    you have indicated with “n-inference-gpus” when converting the weights of GPT-J.*'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 设置你GPU的数量的参数。*注意：它必须与转换GPT-J权重时用“n-inference-gpus”指示的数量相同。*
- en: '[PRE8]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Then, indicate where to find GPT-J:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，指明GPT-J的位置：
- en: '[PRE9]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Run the Triton server
  id: totrans-70
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 运行Triton服务器
- en: 'To launch the Triton server, run the following command: *Note: Change “CUDA_VISIBLE_DEVICES”
    to set the IDs of your GPUs, e.g., “0,1” if you have two GPUs that you would like
    to use.*'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 启动Triton服务器，请运行以下命令：*注意：更改“CUDA_VISIBLE_DEVICES”以设置你的GPU的ID，例如，如果你有两个GPU想要使用，可以设置为“0,1”。*
- en: '[PRE10]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: If everything works correctly, you will see in your terminal the server waiting
    with 1 model loaded by FasterTransformer.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一切正常，你将看到终端中显示服务器等待加载1个由FasterTransformer加载的模型。
- en: It remains to create a client that queries the server. It can be for instance
    your application that will exploit the GPT-J model.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 还需要创建一个客户端来查询服务器。例如，可以是你的应用程序，它将利用GPT-J模型。
- en: 'nVidia provides an example of a client in:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: nVidia提供了一个客户端示例：
- en: '*fastertransformer_backend/tools/end_to_end_test.py*'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '*fastertransformer_backend/tools/end_to_end_test.py*'
- en: This script may look quite complicated but it only prepares all the arguments
    and batch your prompts, and then send everything to the server which is in charge
    of everything else.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 这个脚本可能看起来很复杂，但它只是准备所有参数和批处理你的提示，然后将一切发送到负责其他所有操作的服务器。
- en: 'Modify the variable *input0* toinclude your prompts. It is located here:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 修改变量*input0*以包含你的提示。它的位置如下：
- en: '![](../Images/f001212d5e8e6084f23345dbf55feb7f.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f001212d5e8e6084f23345dbf55feb7f.png)'
- en: Image by author
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片
- en: Finally, you can run this script to prompt your Triton server. You should get
    the response quickly since the model is already fully loaded and optimized.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，你可以运行这个脚本来提示你的 Triton 服务器。你应该能很快得到响应，因为模型已经完全加载并优化过。
- en: And that’s it! You have now everything you need to start exploiting your local
    GPT model in your applications.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 就这样！你现在拥有了开始在应用程序中利用本地 GPT 模型所需的一切。
- en: Conclusion
  id: totrans-83
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: The steps explained in this article are also applicable to all other models
    supported by FasterTransformer (except for specific parts that you will have to
    adapt). You can find [the list here](https://github.com/NVIDIA/FasterTransformer#support-matrix).
    If the model you want to use is not in the list, it may work as well or you may
    have to modify some of the commands I provide.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 本文中解释的步骤也适用于 FasterTransformer 支持的所有其他模型（除了你需要调整的特定部分）。你可以在[此处查看列表](https://github.com/NVIDIA/FasterTransformer#support-matrix)。如果你想使用的模型不在列表中，它可能也能正常工作，或者你可能需要修改一些我提供的命令。
- en: 'If you have many GPUs at your disposal, you can straightforwardly apply the
    same steps to GPT-Neo* models. You would only have to modify the “*config.pbtxt*”
    to adapt to these models. *Note: nVidia may have already prepared these configuration
    files, so look in the* [*FasterTransformer repository*](https://github.com/NVIDIA/FasterTransformer)
    *before making your own configuration files.*'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你有多个 GPU 可用，你可以直接将相同的步骤应用于 GPT-Neo* 模型。你只需修改“*config.pbtxt*”以适应这些模型。*注意：nVidia
    可能已经准备好了这些配置文件，因此在创建自己的配置文件之前，请查看* [*FasterTransformer 仓库*](https://github.com/NVIDIA/FasterTransformer)
    *。*
- en: 'If you want to use T5 instead of a GPT model, you can have a look at [this
    tutorial written by nVidia](https://developer.nvidia.com/blog/deploying-gpt-j-and-t5-with-fastertransformer-and-triton-inference-server/#entry-content-comments).
    *Note: nVidia’s tutorial is outdated, you will have to modify some commands.*'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想使用 T5 而不是 GPT 模型，你可以查看[nVidia 编写的教程](https://developer.nvidia.com/blog/deploying-gpt-j-and-t5-with-fastertransformer-and-triton-inference-server/#entry-content-comments)。*注意：nVidia
    的教程已经过时，你需要修改一些命令。*
- en: Successfully installing and running a Triton inference server by just following
    these steps is very much dependent on your machine configuration. If you have
    any issues, feel free to drop a comment and I’ll try to help.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 成功安装和运行 Triton 推理服务器只需遵循这些步骤，但非常依赖于你的机器配置。如果遇到任何问题，请随时留言，我会尽力帮助。
- en: Further instructions to install missing dependencies
  id: totrans-88
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 安装缺失依赖项的进一步说明
- en: 'Installation of Docker (Ubuntu):'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 安装 Docker（Ubuntu）：
- en: '[PRE11]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Installation of nvidia-container-toolkit (Ubuntu):'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 安装 nvidia-container-toolkit（Ubuntu）：
- en: '[PRE12]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
