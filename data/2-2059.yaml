- en: The Rise of Two-Tower Models in Recommender Systems
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 推荐系统中的两塔模型崛起
- en: 原文：[https://towardsdatascience.com/the-rise-of-two-tower-models-in-recommender-systems-be6217494831](https://towardsdatascience.com/the-rise-of-two-tower-models-in-recommender-systems-be6217494831)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/the-rise-of-two-tower-models-in-recommender-systems-be6217494831](https://towardsdatascience.com/the-rise-of-two-tower-models-in-recommender-systems-be6217494831)
- en: A deep-dive into the latest technology used to debias ranking models
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 深入探讨用于消除排序模型偏差的最新技术
- en: '[](https://medium.com/@samuel.flender?source=post_page-----be6217494831--------------------------------)[![Samuel
    Flender](../Images/390d82a673de8a8bb11cef66978269b5.png)](https://medium.com/@samuel.flender?source=post_page-----be6217494831--------------------------------)[](https://towardsdatascience.com/?source=post_page-----be6217494831--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----be6217494831--------------------------------)
    [Samuel Flender](https://medium.com/@samuel.flender?source=post_page-----be6217494831--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@samuel.flender?source=post_page-----be6217494831--------------------------------)[![Samuel
    Flender](../Images/390d82a673de8a8bb11cef66978269b5.png)](https://medium.com/@samuel.flender?source=post_page-----be6217494831--------------------------------)[](https://towardsdatascience.com/?source=post_page-----be6217494831--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----be6217494831--------------------------------)
    [Samuel Flender](https://medium.com/@samuel.flender?source=post_page-----be6217494831--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----be6217494831--------------------------------)
    ·7 min read·Oct 29, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----be6217494831--------------------------------)
    ·7分钟阅读·2023年10月29日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/c2ea8cba8c6d13e4bf5926fa05096305.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c2ea8cba8c6d13e4bf5926fa05096305.png)'
- en: Photo by [Evgeny Smirnov](https://unsplash.com/@smirik)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 照片由 [Evgeny Smirnov](https://unsplash.com/@smirik) 提供
- en: Recommender systems are among the most ubiquitous Machine Learning applications
    in the world today. However, the underlying ranking models are plagued by [numerous
    biases](/biases-in-recommender-systems-top-challenges-and-recent-breakthroughs-edcda59d30bf)
    that can severely limit the quality of the resulting recommendations. The problem
    of building unbiased rankers — also known as unbiased learning to rank, ULTR —
    remains one of the most important research problems within ML and is still far
    from being solved.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 推荐系统是当今世界上最普遍的机器学习应用之一。然而，底层的排序模型存在 [许多偏差](/biases-in-recommender-systems-top-challenges-and-recent-breakthroughs-edcda59d30bf)，这可能严重限制推荐的质量。构建无偏排序器的问题——也称为无偏排序学习（ULTR）——仍然是机器学习领域最重要的研究问题之一，且远未解决。
- en: 'In this post, we’ll take a deep-dive into one particular modeling approach
    that has relatively recently enabled the industry to control biases very effectively
    and thus build vastly superior recommender systems: the two-tower model, where
    one tower learns relevance and another (shallow) tower learns biases.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我们将深入探讨一种相对较新的建模方法，这种方法已经使行业能够非常有效地控制偏差，从而构建出极其优越的推荐系统：两塔模型，其中一个塔学习相关性，另一个（浅层）塔学习偏差。
- en: While two-tower models have probably been used in the industry for several years,
    the first paper to formally introduce them to the broader ML community was Huawei’s
    2019 PAL paper.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然两塔模型可能在行业中使用了好几年，但第一篇正式向更广泛的机器学习社区介绍它们的论文是华为2019年的PAL论文。
- en: PAL (Huawei, 2019) — the OG two-tower model
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: PAL（华为，2019）——首个两塔模型
- en: Huawei’s paper [PAL](https://github.com/tangxyw/RecSysPapers/blob/main/Debias/%5B2019%5D%5BHuawei%5D%5BPAL%5D%20a%20position-bias%20aware%20learning%20framework%20for%20CTR%20prediction%20in%20live%20recommender%20systems.pdf)
    (“position-aware learning to rank”) considers the problem of position bias within
    the context of the Huawei app store.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 华为的论文 [PAL](https://github.com/tangxyw/RecSysPapers/blob/main/Debias/%5B2019%5D%5BHuawei%5D%5BPAL%5D%20a%20position-bias%20aware%20learning%20framework%20for%20CTR%20prediction%20in%20live%20recommender%20systems.pdf)
    (“位置感知排序学习”) 考虑了在华为应用商店背景下的位置偏差问题。
- en: 'Position bias has been observed over and over again in ranking models across
    the industry. It simply means that users are more likely to click on items that
    are shown first. This may be because they’re in a hurry, because they blindly
    trust the ranking algorithm, or other reasons. Here’s a plot demonstrating position
    bias in Huawei’s data:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在整个行业的排序模型中，一再观察到位置偏差。这意味着用户更有可能点击首先展示的项目。这可能是因为他们赶时间、盲目相信排序算法，或其他原因。以下是展示华为数据中位置偏差的图表：
- en: '![](../Images/39763b8fc1bce8500988b90981c66c3b.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/39763b8fc1bce8500988b90981c66c3b.png)'
- en: 'Position bias. Source: Huawei’s paper [PAL](https://github.com/tangxyw/RecSysPapers/blob/main/Debias/%5B2019%5D%5BHuawei%5D%5BPAL%5D%20a%20position-bias%20aware%20learning%20framework%20for%20CTR%20prediction%20in%20live%20recommender%20systems.pdf)'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 位置偏差。来源：华为的论文 [PAL](https://github.com/tangxyw/RecSysPapers/blob/main/Debias/%5B2019%5D%5BHuawei%5D%5BPAL%5D%20a%20position-bias%20aware%20learning%20framework%20for%20CTR%20prediction%20in%20live%20recommender%20systems.pdf)
- en: Position bias is a problem because we simply can’t know whether users clicked
    on the first item because it was indeed the most relevant for them or because
    it was shown first — and in recommender systems we aim to solve the former learning
    objective, not the latter.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 位置偏差是一个问题，因为我们无法确定用户点击第一个项目是因为它确实最相关，还是因为它首先展示——而在推荐系统中，我们旨在解决前者的学习目标，而不是后者。
- en: The solution proposed in the PAL paper is to factorize the learning problem
    as
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: PAL 论文提出的解决方案是将学习问题分解为
- en: '[PRE0]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: where x is the feature vector, and `seen` is a binary variable indicating whether
    the users has seen the impression or not. In PAL, `seen` depends only on the position
    of the item, but we can add other variables as well (as we’ll see later).
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 x 是特征向量，`seen` 是一个二进制变量，指示用户是否已经看到该展示。在 PAL 中，`seen` 仅依赖于项的位置，但我们也可以添加其他变量（稍后我们会看到）。
- en: 'Based on this framework, we can then build a model with two towers, each of
    which outputs one of the two probabilities on the right hand side, and then simply
    multiply the two probabilities:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 基于这一框架，我们可以构建一个具有两个塔的模型，每个塔输出右侧的两个概率之一，然后简单地将这两个概率相乘：
- en: '![](../Images/26a05297dd86a7206de4b19d8c4c6a6c.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/26a05297dd86a7206de4b19d8c4c6a6c.png)'
- en: 'Source: Huawei’s paper [PAL](https://github.com/tangxyw/RecSysPapers/blob/main/Debias/%5B2019%5D%5BHuawei%5D%5BPAL%5D%20a%20position-bias%20aware%20learning%20framework%20for%20CTR%20prediction%20in%20live%20recommender%20systems.pdf)'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 来源：华为的论文 [PAL](https://github.com/tangxyw/RecSysPapers/blob/main/Debias/%5B2019%5D%5BHuawei%5D%5BPAL%5D%20a%20position-bias%20aware%20learning%20framework%20for%20CTR%20prediction%20in%20live%20recommender%20systems.pdf)
- en: 'The clouds in this image are simply neural networks: a shallow one for the
    position tower (because it only needs to process a single features), and a deep
    one for the CTR tower (because it needs to process a large number of features
    and create interactions between those features). We also call these two towers
    the bias tower and engagement tower, respectively.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 图中的云层实际上是神经网络：一个用于位置塔的浅层网络（因为它只需要处理单一特征），另一个用于 CTR 塔的深层网络（因为它需要处理大量特征并在这些特征之间创建交互）。我们也将这两个塔分别称为偏差塔和参与塔。
- en: Notably, at inference time, where positions aren’t available, we run a forward
    pass only using the engagement tower, not the bias tower. Similar to Dropout,
    the model thus behaves differently at training time at inference time.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，在推理时，位置不可用时，我们仅使用参与塔进行前向传递，而不使用偏差塔。类似于 Dropout，因此模型在训练和推理时的行为有所不同。
- en: 'Does it work? Yes, indeed, PAL works remarkably well. The authors build 2 different
    version of the DeepFM ranking model (which I wrote about [here](/deep-learning-in-recommender-systems-a-primer-96e4b07b54ca)),
    one version with PAL and baseline version with a naive way of treating item position:
    simply passing it as a feature into the engagement tower. Their online A/B test
    shows that PAL improves both click-through rates and conversion rates by around
    25%, a huge lift!'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 它有效吗？是的，PAL 的效果非常好。作者建立了两个不同版本的 DeepFM 排名模型（我在 [这里](/deep-learning-in-recommender-systems-a-primer-96e4b07b54ca)
    写过），一个版本使用 PAL，另一个版本使用一种简单的处理项位置的方法：将其作为特征传递到参与塔中。他们的在线 A/B 测试显示，PAL 将点击率和转化率分别提高了大约
    25%，这是一个巨大的提升！
- en: PAL showed that positions themselves can be used as inputs to a ranking model,
    but they need to be passed through a dedicated tower, not the main model (a rule
    that has also been added as [Rule 36](https://developers.google.com/machine-learning/guides/rules-of-ml#rule_36_avoid_feedback_loops_with_positional_features)
    to Google’s “Rules of ML”). The two-tower model was officially born — even though
    it has been likely used across the industry prior to the PAL paper.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: PAL 显示位置本身可以作为排名模型的输入，但需要通过专门的塔进行处理，而不是主模型（这一规则也被添加为 [Rule 36](https://developers.google.com/machine-learning/guides/rules-of-ml#rule_36_avoid_feedback_loops_with_positional_features)
    到 Google 的“机器学习规则”中）。双塔模型正式诞生——尽管在 PAL 论文之前，这种方法可能已经在业界使用过。
- en: “Watch Next” (YouTube, 2019) — the additive two-tower model
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: “接下来观看”（YouTube，2019）——加性双塔模型
- en: 'YouTube’s paper “[Recommending What Video to Watch Next](https://daiwk.github.io/assets/youtube-multitask.pdf)”
    — commonly known simply as the “Watch Next” paper — came out around the same time
    as PAL and tried to solve the same problem: debiasing of recommender systems using
    a two-tower model. However, compared to PAL, YouTube used an *additive* two-tower
    model instead of a multiplicative one.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: YouTube的论文“[Recommending What Video to Watch Next](https://daiwk.github.io/assets/youtube-multitask.pdf)”——通常被简单称为“Watch
    Next”论文——发布的时间与PAL相近，并试图解决相同的问题：使用双塔模型去偏差推荐系统。然而，与PAL相比，YouTube使用的是*加性*双塔模型，而不是乘性模型。
- en: 'To see why this works, consider again the factorization of the learning objective:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解这为何有效，再次考虑学习目标的分解：
- en: '[PRE1]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Because probabilities are sigmoids of logits, sigmoids are simply weighted
    exponential functions, and the exponential function follows the product rule for
    exponents, we can also write:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 由于概率是logits的sigmoid函数，sigmoid函数本质上是加权指数函数，而指数函数遵循指数的乘法规则，因此我们也可以写作：
- en: '[PRE2]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: which is, alas, the two-tower additive model.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是，遗憾的是，加性双塔模型。
- en: '![](../Images/9b68128643857a9aa4c3ef64089a6406.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9b68128643857a9aa4c3ef64089a6406.png)'
- en: 'The two-tower additive model. Source: “[Recommending What Video to Watch Next](https://daiwk.github.io/assets/youtube-multitask.pdf)”'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 双塔加性模型。来源：“[Recommending What Video to Watch Next](https://daiwk.github.io/assets/youtube-multitask.pdf)”
- en: 'Another notable innovation compared to PAL is the use of other features besides
    position in the shallow tower. One example is the user device type. This makes
    sense because different devices are expected to exhibit different forms of position
    bias: for example, position bias may be more severe on phones with smaller screens
    where the user can see fewer positions at the same time and has to swipe more.
    As such, the shallow tower in YouTube’s model doesn’t just learn position bias,
    it learns all sorts of selection biases, depending on which features are being
    passed into the shallow tower.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 相比于PAL，另一个显著的创新是使用除了位置以外的其他特征在浅层塔中。例如，用户设备类型。这是有道理的，因为不同设备预计会表现出不同形式的位置偏差：例如，位置偏差可能在屏幕较小的手机上更为严重，因为用户可以同时看到的位置更少，并且需要更多地滑动。因此，YouTube模型中的浅层塔不仅学习位置偏差，还学习各种选择偏差，具体取决于传入浅层塔的特征。
- en: In order to ensure the shallow tower actually leverages all of these features,
    the authors add a Dropout layer with a dropout probability of 10% on top of the
    position feature alone. Without it, the model may over-rely on the position feature
    and not learn any other selection biases.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确保浅层塔实际利用了所有这些特征，作者在仅位置特征上添加了一个10%丢弃率的Dropout层。没有它，模型可能会过度依赖位置特征，而无法学习其他选择偏差。
- en: Using A/B testing, the authors show that adding the shallow tower improves their
    “engagement metric” (unfortunately no details were given on what that metric is)
    by 0.24%.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 通过A/B测试，作者展示了添加浅层塔提高了他们的“参与度指标”（不幸的是没有给出该指标的详细信息）0.24%。
- en: Disentangling relevance and bias in ULTR (Google, 2023)
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在ULTR中解开相关性和偏差（谷歌，2023）
- en: Up until this point, we’ve made the assumption that the two towers in ULTR can
    learn independently during model training. Alas, this assumption is not true,
    show the authors of recent Google paper, “[Towards Disentangling Relevance and
    Bias in Unbiased Learning to Rank](https://arxiv.org/abs/2212.13937)”.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们假设ULTR中的两个塔可以在模型训练期间独立学习。不幸的是，最近的谷歌论文“[Towards Disentangling Relevance
    and Bias in Unbiased Learning to Rank](https://arxiv.org/abs/2212.13937)”的作者表明，这个假设并不成立。
- en: 'Here’s a [Gedankenexperiment](https://www.britannica.com/science/Gedankenexperiment)
    to see why: suppose there’s a perfect relevance model, that is, a model that can
    perfectly explain clicks. In such a case, once we re-train the model on new data,
    all the bias tower needs to do is learn how to map positions to clicks. The relevance
    tower wouldn’t have to learn anything — it would be entirely useless!'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一个[思想实验](https://www.britannica.com/science/Gedankenexperiment)来说明原因：假设存在一个完美的相关性模型，即一个能够完美解释点击的模型。在这种情况下，一旦我们用新数据重新训练模型，所有的偏差塔需要做的就是学习如何将位置映射到点击。相关性塔则无需学习任何东西——它将完全无用！
- en: In other words, relevance has a confounding effect on position, as shown in
    the red arrow in the causal graph below.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，相关性对位置有干扰效应，如下图因果图中的红色箭头所示。
- en: '![](../Images/7b8e236dd5723ff1974126bedc060891.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7b8e236dd5723ff1974126bedc060891.png)'
- en: 'The confounding effect of the relevance tower on the bias tower. Source: [Zhang
    et al 2023](https://arxiv.org/abs/2212.13937)'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 相关性塔对偏置塔的混淆效应。来源：[Zhang et al 2023](https://arxiv.org/abs/2212.13937)
- en: 'But that’s not what we want: we want the relevance tower to keep learning as
    new data comes in. One simple method to solve this problem — eliminate the red
    edge in the causal graph — is by adding a Dropout layer on top of the bias logit,
    as shown in the plot below:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 但这不是我们想要的：我们希望相关性塔在新数据到来时继续学习。解决这个问题的一种简单方法——消除因果图中的红色边缘——是添加一个 Dropout 层到偏置对数值上，如下图所示：
- en: '![](../Images/bc33470348cb5b182aa65c9b8011bc5c.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bc33470348cb5b182aa65c9b8011bc5c.png)'
- en: 'Adding dropout on top of the bias tower. Source: [Zhang et al 2023](https://arxiv.org/abs/2212.13937).'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在偏置塔上添加 Dropout。来源：[Zhang et al 2023](https://arxiv.org/abs/2212.13937)。
- en: Why Dropout? The key idea is that by randomly dropping out the bias logit, we
    “nudge” the model to rely a little bit more on the relevance tower instead of
    simply learning the mapping between historic positions and relevance. Note that
    this idea is very similar to YouTube’s Watch Next paper, however here we drop
    out the entire bias logit instead of only the position feature.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么使用 Dropout？关键思想是通过随机丢弃偏置对数值（bias logit），我们“推动”模型更多地依赖于相关性塔（relevance tower），而不是仅仅学习历史位置和相关性之间的映射。请注意，这一思想与
    YouTube 的 Watch Next 论文非常相似，但这里我们丢弃的是整个偏置对数值，而不仅仅是位置特征。
- en: 'In order for this to work, the logit dropout probability needs to be high:
    with a dropout of 0.5 (that is, randomly zeroing out the bias logit in 50% of
    the training examples) the authors beat PAL by 1% click NDGC on production data
    from the Chrome App store — a strong demonstration of this simple technique.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使其有效，偏置对数值的 Dropout 概率需要很高：在 0.5 的 Dropout 下（即在 50% 的训练样本中随机将偏置对数值置零），作者在
    Chrome 应用商店的生产数据上击败了 PAL 1% 的点击 NDGC——这是这种简单技术的有力证明。
- en: Summary
  id: totrans-50
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 总结
- en: 'Let’s recap:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回顾一下：
- en: 'The two-tower model powerful approach to build unbiased ranking models: one
    tower learns relevance while the other tower learns position bias (and, optionally,
    other biases).'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 双塔模型是一种强大的方法来构建无偏排名模型：一个塔学习相关性，而另一个塔学习位置偏置（以及，选择性地，其他偏置）。
- en: We can combine the output from the two towers either by multiplying their probabilities
    (as done in Huawei’s PAL), or by adding their logits (as done in YouYube’s Watch
    Next). We call the latter the additive two-tower model.
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以通过乘以两个塔的概率（如华为的 PAL 所做），或者通过加上它们的对数值（如 YouTube 的 Watch Next 所做）来组合两个塔的输出。我们将后者称为加性双塔模型（additive
    two-tower model）。
- en: The input to the bias tower is usually the position of the item, but we can
    also add other features that introduce bias such as the user device type.
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 偏置塔的输入通常是项目的位置，但我们也可以添加其他引入偏置的特征，如用户设备类型。
- en: Dropout (either on the positions or on the entire bias logit) prevents the model
    from over-relying on historic positions and has been shown to improve generalization.
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dropout（无论是在位置上还是在整个偏置对数值上）可以防止模型过度依赖历史位置，并且已被证明可以提高泛化能力。
- en: 'And that’s just the tip of the iceberg. Given the importance of recommender
    systems in today’s tech industry, and the discovery that two-tower modeling can
    substantially improve predictive performance, this research domain is probably
    still far from its full potential: we’re really just getting started!'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 这仅仅是冰山一角。鉴于推荐系统在今天的科技行业中的重要性，以及发现双塔建模可以显著提高预测性能，这一研究领域可能仍远未发挥其全部潜力：我们真的只是刚刚开始！
- en: '*If you like this content and want to stay on top of the latest technologies
    in ML, make sure to* [*subscribe*](https://samflender.substack.com)*.*'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '*如果你喜欢这些内容并想保持对最新 ML 技术的了解，确保* [*订阅*](https://samflender.substack.com)*。*'
