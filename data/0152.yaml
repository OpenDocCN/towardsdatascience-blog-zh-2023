- en: 'Optimization, Newton’s Method, & Profit Maximization: Part 1 — Basic Optimization
    Theory'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 优化、牛顿法与利润最大化：第1部分 — 基本优化理论
- en: 原文：[https://towardsdatascience.com/optimization-newtons-method-profit-maximization-part-1-basic-optimization-theory-ff7c5f966565?source=collection_archive---------10-----------------------#2023-01-10](https://towardsdatascience.com/optimization-newtons-method-profit-maximization-part-1-basic-optimization-theory-ff7c5f966565?source=collection_archive---------10-----------------------#2023-01-10)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/optimization-newtons-method-profit-maximization-part-1-basic-optimization-theory-ff7c5f966565?source=collection_archive---------10-----------------------#2023-01-10](https://towardsdatascience.com/optimization-newtons-method-profit-maximization-part-1-basic-optimization-theory-ff7c5f966565?source=collection_archive---------10-----------------------#2023-01-10)
- en: '![](../Images/e6f766d329c212d0721d49d9bd28830e.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e6f766d329c212d0721d49d9bd28830e.png)'
- en: All Images by Author
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 所有图片由作者提供
- en: Learn how to solve and utilize Newton’s Method for multi-dimensional optimization
    problems
  id: totrans-4
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 学习如何解决和利用牛顿法解决多维优化问题
- en: '[](https://medium.com/@jakepenzak?source=post_page-----ff7c5f966565--------------------------------)[![Jacob
    Pieniazek](../Images/2d9c6295d39fcaaec4e62f11c359cb29.png)](https://medium.com/@jakepenzak?source=post_page-----ff7c5f966565--------------------------------)[](https://towardsdatascience.com/?source=post_page-----ff7c5f966565--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----ff7c5f966565--------------------------------)
    [Jacob Pieniazek](https://medium.com/@jakepenzak?source=post_page-----ff7c5f966565--------------------------------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@jakepenzak?source=post_page-----ff7c5f966565--------------------------------)[![Jacob
    Pieniazek](../Images/2d9c6295d39fcaaec4e62f11c359cb29.png)](https://medium.com/@jakepenzak?source=post_page-----ff7c5f966565--------------------------------)[](https://towardsdatascience.com/?source=post_page-----ff7c5f966565--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----ff7c5f966565--------------------------------)
    [Jacob Pieniazek](https://medium.com/@jakepenzak?source=post_page-----ff7c5f966565--------------------------------)'
- en: ·
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F6f0948d99b1c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Foptimization-newtons-method-profit-maximization-part-1-basic-optimization-theory-ff7c5f966565&user=Jacob+Pieniazek&userId=6f0948d99b1c&source=post_page-6f0948d99b1c----ff7c5f966565---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----ff7c5f966565--------------------------------)
    ·14 min read·Jan 10, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fff7c5f966565&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Foptimization-newtons-method-profit-maximization-part-1-basic-optimization-theory-ff7c5f966565&user=Jacob+Pieniazek&userId=6f0948d99b1c&source=-----ff7c5f966565---------------------clap_footer-----------)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F6f0948d99b1c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Foptimization-newtons-method-profit-maximization-part-1-basic-optimization-theory-ff7c5f966565&user=Jacob+Pieniazek&userId=6f0948d99b1c&source=post_page-6f0948d99b1c----ff7c5f966565---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----ff7c5f966565--------------------------------)
    ·14分钟阅读·2023年1月10日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fff7c5f966565&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Foptimization-newtons-method-profit-maximization-part-1-basic-optimization-theory-ff7c5f966565&user=Jacob+Pieniazek&userId=6f0948d99b1c&source=-----ff7c5f966565---------------------clap_footer-----------)'
- en: --
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fff7c5f966565&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Foptimization-newtons-method-profit-maximization-part-1-basic-optimization-theory-ff7c5f966565&source=-----ff7c5f966565---------------------bookmark_footer-----------)'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fff7c5f966565&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Foptimization-newtons-method-profit-maximization-part-1-basic-optimization-theory-ff7c5f966565&source=-----ff7c5f966565---------------------bookmark_footer-----------)'
- en: This article is the **1st** in a 3 part series. In the 1st part, we will be
    studying basic optimization theory. Then, in [pt. 2](/optimization-newtons-method-profit-maximization-part-2-constrained-optimization-theory-dc18613c5770),
    we will be extending this theory to constrained optimization problems. Lastly,
    in pt. 3, we will apply the optimization theory covered, as well as econometric
    and economic theory, to solve a profit maximization problem.
  id: totrans-10
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 本文是3部分系列中的**第1部分**。在第1部分中，我们将学习基本的优化理论。然后，在[第2部分](/optimization-newtons-method-profit-maximization-part-2-constrained-optimization-theory-dc18613c5770)，我们将扩展这些理论到约束优化问题。最后，在第3部分中，我们将应用所涵盖的优化理论，以及计量经济学和经济理论，来解决一个利润最大化问题。
- en: Mathematical optimization is an extremely powerful field of mathematics the
    underpins much of what we, as data scientists, implicitly, or explicitly, utilize
    on a regular basis — in fact, nearly all machine learning algorithms make use
    of optimization theory to obtain model convergence. Take, for example, a classification
    problem, we seek to *minimize* log-loss by choosing the *optimal* parameters or
    weights of the model. *In general, mathematical optimization can be thought of
    as the primary theoretical mechanism by which machines learn.* A robust understanding
    of mathematical optimization is an extremely beneficial skillset to have in the
    data scientists toolbox — it enables the data scientist to have a deeper understanding
    of many of the algorithms used today and, furthermore, to solve a *vast array
    of unique optimization problems*.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 数学优化是一个极其强大的数学领域，支撑了我们数据科学家在日常工作中隐性或显性使用的许多工具——实际上，几乎所有的机器学习算法都利用优化理论来实现模型收敛。例如，在分类问题中，我们试图通过选择模型的*最优*参数或权重来*最小化*对数损失。*一般来说，数学优化可以被视为机器学习的主要理论机制*。对数学优化的深刻理解是数据科学家工具箱中非常有用的技能——它使数据科学家能够更深入地理解许多当前使用的算法，并且进一步解决*各种独特的优化问题*。
- en: Many of the readers may be familiar with gradient descent, or related optimization
    algorithms such as stochastic gradient descent. However, this post will discuss
    in more depth the classical Newton method for optimization, sometimes referred
    to as the Newton-Raphson method. We will, nevertheless, develop the mathematics
    behind optimization theory from the basics to gradient descent and then dive more
    into Newton’s method with implementations in python. This will serve as the necessary
    preliminaries for our excursion into constrained optimization in [**part 2**](/optimization-newtons-method-profit-maximization-part-2-constrained-optimization-theory-dc18613c5770)
    and an econometric profit-maximization problem in **part 3** of this series.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 许多读者可能对梯度下降或相关的优化算法，如随机梯度下降，已经有所了解。然而，本文将更深入地讨论经典的牛顿优化方法，有时称为牛顿-拉夫森方法。我们将从基础数学知识开始，逐步讲解优化理论，到梯度下降，然后深入探讨牛顿方法及其在
    Python 中的实现。这将为我们进入[**第2部分**](/optimization-newtons-method-profit-maximization-part-2-constrained-optimization-theory-dc18613c5770)的约束优化和本系列的**第3部分**中的计量经济学利润最大化问题提供必要的前期准备。
- en: Optimization Basics — A Simple Quadratic Function
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 优化基础——一个简单的二次函数
- en: 'Mathematical optimization can be defined “as the science of determining the
    best solutions to mathematically defined problems.”[1] This may be conceptualized
    in some real-world examples as: choosing the parameters to minimize a loss function
    for a machine learning algorithm, choosing price and advertising to maximize profit,
    choosing stocks to maximize risk-adjusted financial return, etc. Formally, any
    mathematical optimization problem can be formulated abstractly as such:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 数学优化可以被定义为“确定数学定义问题的最佳解决方案的科学。”[1] 在一些实际例子中，这可以被概念化为：选择参数以最小化机器学习算法的损失函数，选择价格和广告以最大化利润，选择股票以最大化风险调整后的财务回报等等。形式上，任何数学优化问题都可以抽象地表述如下：
- en: '![](../Images/91d0602859b3778af388eb855d77cd2a.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/91d0602859b3778af388eb855d77cd2a.png)'
- en: (1)
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: (1)
- en: 'This can be read as follows: Choose real values of the vector **x** that minimize
    the *objective function* *f*(**x**) (or maximize -*f*(**x**)) subject to the *inequality
    constraints* *g*(**x**) and *equality constraints* *h*(**x**).We will be addressing
    how to solve for constrained optimization problems in [part 2](/optimization-newtons-method-profit-maximization-part-2-constrained-optimization-theory-dc18613c5770)
    of this series — as they can make the optimization problems particularly non-trivial.
    For now, let’s look at an unconstrained single variable example — consider the
    following optimization problem:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 这可以理解为：选择向量**x**的实值，以最小化*目标函数* *f*(**x**)（或最大化-*f*(**x**))，并满足*不等式约束* *g*(**x**)和*等式约束*
    *h*(**x**)。我们将在本系列的[第二部分](/optimization-newtons-method-profit-maximization-part-2-constrained-optimization-theory-dc18613c5770)中讨论如何求解约束优化问题——因为它们使优化问题变得特别复杂。现在，让我们来看一个无约束的单变量示例——考虑以下优化问题：
- en: '![](../Images/776e66032cdf1167135d13f03ac59c6c.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/776e66032cdf1167135d13f03ac59c6c.png)'
- en: (2)
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: （2）
- en: In this case, we want to choose the value of x that minimizes the above quadratic
    function. There are multiple ways we can go about this — first, a naïve approach
    would be to do a grid search iterating over a large range of x values and choose
    x where *f*(x) has the lowest functional value. However, this approach can quickly
    lose computational tractability as the search space increases, the function becomes
    more complex, or the dimensions increase.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们想选择使上面的二次函数最小化的x值。我们可以采用多种方法——首先，一种简单的方法是对x值的大范围进行网格搜索，并选择使*f*(x)具有最低函数值的x。然而，随着搜索空间的增加、函数变得更加复杂或维度增加，这种方法可能很快失去计算上的可行性。
- en: 'Alternatively, we can solve directly using calculus if a closed-form solution
    exists. That is, we can solve analytically for the value of x via calculus. By
    taking the derivative (or, as covered later, the gradient in higher dimensions)
    and setting it equal to 0 — *the first order necessary condition for a relative
    minimum —* we can solve for the relative extrema of the function. We can then
    take the second derivate (or, as covered later, the Hessian in higher dimensions)
    to determine whether this extrema is a maximum or minimum. A second derivative
    greater than 0 (or a positive definite Hessian) — *the second order necessary
    condition for a relative minimum —* implies a minimum and vice-versa. Observe:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 如果存在封闭形式的解，我们可以直接使用微积分来求解。也就是说，我们可以通过微积分解析地求解x的值。通过取导数（或在高维中，如后面所述的梯度）并将其设置为0——*相对最小值的必要一阶条件*——我们可以求解函数的相对极值。然后我们可以取第二导数（或在高维中，如后面所述的Hessian矩阵）来确定这个极值是最大值还是最小值。第二导数大于0（或正定Hessian矩阵）——*相对最小值的必要二阶条件*——意味着是最小值，反之亦然。观察：
- en: '![](../Images/b356d5910e63261dd7e34294b0bcc246.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b356d5910e63261dd7e34294b0bcc246.png)'
- en: (3)
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: （3）
- en: 'We can verify this graphically for (2) above:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过图形验证上述（2）：
- en: '![](../Images/6b090480c2e5931b739e8fb4f3c0d907.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6b090480c2e5931b739e8fb4f3c0d907.png)'
- en: Notethat when multiple extrema of a function exist (i.e., multiple minimums
    or maximums), care must be taken to determine which is the global extrema — we
    will briefly discuss this issue further in this article.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，当一个函数存在多个极值点（即多个最小值或最大值）时，必须小心确定哪个是全局极值——我们将在本文中简要讨论这个问题。
- en: The analytical approach demonstrated above can be extended into higher dimensions
    utilizing gradients and Hessians — however, we will not be solving the closed-form
    solutions in higher dimensions — the intuition, however, remains the same. We
    will, nevertheless, be solving higher dimensional problems utilizing *iterative
    schemes.* What do I mean by *iterative schemes*? In general, a closed form (or
    analytical) solution may not exist, and certainly need not exist for a maximum
    or minimum to exist. Thus, we require a methodology to numerically solve the optimization
    problem. This leads us to the more generalized *iterative schemes* including gradient
    descent and the Newton methods.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 上述分析方法可以扩展到更高维度，利用梯度和 Hessian 矩阵——然而，我们不会在高维中求解封闭形式的解——但直觉依然相同。我们仍然会利用*迭代方案*来解决更高维的问题。我说的*迭代方案*是什么意思？通常，可能不存在封闭形式（或解析）的解，并且为了存在最大值或最小值，封闭形式的解确实不一定存在。因此，我们需要一种数值方法来解决优化问题。这引导我们到更广泛的*迭代方案*，包括梯度下降法和牛顿方法。
- en: Iterative Optimization Schemes
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 迭代优化方案
- en: In general, there are three main categories of iterative optimization schemes.
    Namely, *zero-order*, *first-order*, and *second-order*, which make use of local
    information about the function from no derivatives, first derivatives, or second
    derivatives, respectively.[1] In order to use each iterative scheme, the function
    f(**x**) must be a continuously differentiable function to the respective degree.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，迭代优化方案主要分为三类，即 *零阶*、*一阶* 和 *二阶*，它们分别利用函数的零阶、一阶或二阶导数的局部信息。[1] 要使用每种迭代方案，函数
    f(**x**) 必须是相应阶数上连续可微的函数。
- en: '**Zero-order iterative schemes**'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '**零阶迭代方案**'
- en: '*Zero-order iterative schemes* are closely aligned with the grid-search as
    mentioned above — simply, you search over a certain range possible values of the
    value **x** to obtain the minimum functional value. As you likely suspect, these
    methods tend to be much more computationally expensive than methods that utilize
    higher orders. Needless to say, they can be reliable and easy to program. There
    are methodologies out there that improve upon the simple grid-search, see [1]
    for more information; however, we will be focusing more-so on the higher-order
    schemes.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '*零阶迭代方案* 与上述的网格搜索紧密相关——简单来说，你在一定范围内搜索可能的 **x** 值以获得最小的函数值。正如你可能猜到的，这些方法往往比使用高阶方法的计算开销大得多。不用说，它们可以是可靠的并且容易编程。市场上有一些方法可以改进简单的网格搜索，参见[1]了解更多信息；然而，我们将更多关注高阶方案。'
- en: '**First-order iterative schemes**'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '**一阶迭代方案**'
- en: '*First-order iterative schemes* are iterative schemes that utilize local information
    of the first derivatives of the objective function. Most notably, gradient descent
    methods fall under this category. For a single variable function as above, the
    gradient is just the first derivative. Generalizing this to *n* dimensions, for
    a function *f*(**x**), the gradient is the vector of first order partial derivatives:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '*一阶迭代方案* 是利用目标函数一阶导数的局部信息的迭代方案。最显著的例子是梯度下降方法。对于如上所述的单变量函数，梯度就是一阶导数。将此推广到 *n*
    维度，对于一个函数 *f*(**x**)，梯度是一阶偏导数的向量：'
- en: '![](../Images/d43e3d17e6b500055468db4db576bd0b.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d43e3d17e6b500055468db4db576bd0b.png)'
- en: (4) Gradient of a continuously differentiable function *f*(**x**)
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: (4) 连续可微函数 *f*(**x**)
- en: 'Gradient descent begins by choosing a random starting point and iteratively
    taking steps in the direction of the negative gradient of *f*(**x**) — *the steepest
    direction of the function*. Each iterative step can be represented as follows:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度下降从选择一个随机的起点开始，并在 *f*(**x**) 的负梯度方向上迭代进行——*函数的最陡方向*。每次迭代步骤可以表示如下：
- en: '![](../Images/d0c20a695835779d9d20b310cf778c7d.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d0c20a695835779d9d20b310cf778c7d.png)'
- en: (5) Gradient Descent Iterative Scheme
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: (5) 梯度下降迭代方案
- en: 'where *γ* is the respective learning rate, which controls how fast or slow
    the gradient descent algorithm “learns” at each iteration. Too large and our iterations
    can diverge uncontrollably. Too small and the iterations can take forever to converge.
    This scheme is conducted iteratively until any one or more convergence criteria
    is achieved, such as:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *γ* 是相应的学习率，控制梯度下降算法在每次迭代中“学习”的快慢。学习率过大会导致我们的迭代不受控制地发散。学习率过小则迭代可能需要很长时间才能收敛。此方案会迭代进行，直到达到一个或多个收敛准则，如：
- en: '![](../Images/855991698a9b3c03d179ffcba7c2ef8c.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/855991698a9b3c03d179ffcba7c2ef8c.png)'
- en: (6) Convergence Criteria for Iterative Optimization Schemes
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: (6) 迭代优化方案的收敛准则
- en: 'for some small epsilon threshold. Referring back to our quadratic example,
    setting our initial guess to x = 3 and the learning rate to 0.1, the steps would
    look as follows:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 对于某个小的 epsilon 阈值。回到我们的二次例子，将初始猜测设置为 x = 3 和学习率设置为 0.1，步骤如下：
- en: '![](../Images/a1ec26982129d237011ea7c103ff6bb3.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a1ec26982129d237011ea7c103ff6bb3.png)'
- en: (7)
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: (7)
- en: 'And visually:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 视觉化如下：
- en: '![](../Images/eb6bbeaa05e96ebae221a06c3e6933e0.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/eb6bbeaa05e96ebae221a06c3e6933e0.png)'
- en: Gradient descent and first-order iterative schemes are notably reliable in their
    performance. In fact, gradient descent algorithms are primarily utilized for optimization
    of loss functions in Neural Networks and ML models, and many developments have
    improved the efficacy of these algorithms. Nevertheless, they are still using
    limited local information about the function (only the first derivative). Thus,
    in higher dimension and depending on the nature of the objective function & the
    learning rate, these schemes 1) can have a slow convergence rate as they maintain
    a linear convergence rate and 2) may fail entirely to converge. Because of this,
    it is beneficial for the data scientist to expand their optimization arsenal!
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度下降和一阶迭代方案在性能上显著可靠。实际上，梯度下降算法主要用于神经网络和机器学习模型中的损失函数优化，许多发展已提高了这些算法的效能。然而，它们仍然仅使用关于函数的有限局部信息（仅一阶导数）。因此，在高维情况下，根据目标函数的性质和学习率，这些方案
    1) 可能具有较慢的收敛速度，因为它们保持线性收敛率，并且 2) 可能完全无法收敛。由于这个原因，数据科学家扩大优化工具库是有益的！
- en: '**Second-order iterative schemes**'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '**二阶迭代方案**'
- en: 'As you have likely now pieced together, *Second-order iterative schemes* are
    iterative schemes that utilize local information of the first derivatives ***and***
    thesecond derivatives of the objective function. Most notably, we have the Newton
    method (NM), which makes use of the Hessian of the objective function. For a single
    variable function, the Hessian is simply the second derivative. Similar to the
    gradient, generalizing this to *n* dimensions, the Hessian is an *n* x *n symmetrical*
    matrix of the second order partial derivatives of a twice continuously differentiable
    function *f*(**x**):'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 如你现在可能已经明白，*二阶迭代方案* 是利用目标函数的一阶导数和二阶导数的局部信息的迭代方案。最显著的是，我们有牛顿法 (NM)，它使用目标函数的海森矩阵。对于单变量函数，海森矩阵仅仅是二阶导数。类似于梯度，将其推广到
    *n* 维度，海森矩阵是一个 *n* x *n 对称* 矩阵，包含一个两次连续可微函数 *f*(**x**）的二阶偏导数。
- en: '![](../Images/02d6b85d3d647d76e0d3a4440e2afba7.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/02d6b85d3d647d76e0d3a4440e2afba7.png)'
- en: (8) Hessian of a twice continuously differentiable function *f*(**x**)
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: (8) 二次连续可微函数 *f*(**x**）的海森矩阵
- en: 'Now moving on to derive the NM, first recall the first order necessary condition
    for a minimum:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 现在转到导出 NM，首先回忆最小值的一阶必要条件：
- en: '![](../Images/9716e1afa505ae541eed48d13afd769a.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9716e1afa505ae541eed48d13afd769a.png)'
- en: (9) First-Order Necessary Condition for Relative Minimum at x*
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: (9) x* 处相对最小值的一级必要条件
- en: 'We can approximate ***x**** using a Taylor Series expansion:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用泰勒级数展开来近似 ***x****：
- en: '![](../Images/f671020420ba3b2e88ede14734e5cb88.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f671020420ba3b2e88ede14734e5cb88.png)'
- en: (10)
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: (10)
- en: 'Each iterative addition of delta, **Δ,** is an expected better approximation
    of **x***. Thus, each iterative step using the NM can be represented as follows:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 每次迭代的增量 **Δ** 是对 **x*** 的一个更好的预期近似。因此，使用 NM 的每次迭代步骤可以表示如下：
- en: '![](../Images/aeea601b1cde8c66b5fd09c5d76f1710.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/aeea601b1cde8c66b5fd09c5d76f1710.png)'
- en: (11) Newton Method Iterative Scheme
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: (11) 牛顿法迭代方案
- en: 'Referring back to our quadratic example, setting our initial guess to x = 3,
    the steps would look as follows:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 回到我们的二次函数示例，将初始猜测设置为 x = 3，步骤如下：
- en: '![](../Images/a0c7a8dfd976ea5a79692fc8ba4b574c.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a0c7a8dfd976ea5a79692fc8ba4b574c.png)'
- en: (12)
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: (12)
- en: And we, elegantly, converge to the optimal solution on our first iteration.
    Note, the convergence criteria is the same regardless of scheme.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 我们优雅地在第一次迭代时就收敛到最优解。注意，无论方案如何，收敛标准都是相同的。
- en: Note that all of the optimization schemes suffer from the possibility of getting
    caught in a relative extremum, rather than the global extremum (i.e., think a
    higher order polynomial with multiple extrema (min’s and/or max’s)— we could get
    stuck in one relative extrema when, in reality, another extrema may be globally
    more optimal for our problem). There are methods developed, and always being developed,
    for dealing with global optimization, which we will not dive too deep into. You
    can use prior knowledge of the functional form to set expectations of what results
    you anticipate (i.e., If a strictly convex function has a critical point, then
    it must be a global minimum). **Nevertheless, as a general rule of thumb, it is
    always wise to iterate optimization schemes over different possible starting values
    of x and then study the stability of results, usually picking the results with
    the most optimal functional values for the problem at hand.**
  id: totrans-65
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 请注意，所有优化方案都可能陷入局部极值，而不是全局极值（即，考虑具有多个极值（最小值和/或最大值）的高阶多项式——我们可能会陷入一个局部极值，而实际上，另一个极值可能在全球范围内对我们的实际问题更为优化）。已有的方法，并且仍在不断开发，用于处理全局优化，我们将不会深入探讨。你可以利用函数形式的先验知识来设置对结果的预期（即，如果一个严格凸函数有一个临界点，则它必须是全局最小值）。**然而，作为一般经验法则，通常明智的做法是对不同的初始值x迭代优化方案，然后研究结果的稳定性，通常选择具有最优函数值的结果。**
- en: Multi-Dimensional Example — Rosenbrock’s Parabolic Valley
  id: totrans-66
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 多维示例——罗森布罗克的抛物线谷
- en: 'Let’s now consider the following optimization problem of two variables:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们考虑以下两个变量的优化问题：
- en: '![](../Images/932d986307b137d667c91cffb7f002e0.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/932d986307b137d667c91cffb7f002e0.png)'
- en: (13) Rosenbrock’s Parabolic Valley
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: (13) 罗森布罗克的抛物线谷
- en: '![](../Images/74f4e8929ed07a5c8fb08b9506e49b66.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/74f4e8929ed07a5c8fb08b9506e49b66.png)'
- en: We will first solve the above optimization problem first by hand and then in
    python, both utilizing Newton’s Method.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先通过手动求解上述优化问题，然后在 Python 中进行求解，均使用牛顿法。
- en: '**Solving via Hand**'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '**通过手动求解**'
- en: 'To solve by hand, we will need to solve for the gradient, solve for the Hessian,
    choose our initial guess **Γ** = [x,y], and then iterate plugging this information
    into the NM algorithm until convergence is achieved. First, solving for the gradient,
    we have:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 要手动求解，我们需要求解梯度，求解 Hessian，选择我们的初始猜测 **Γ** = [x,y]，然后迭代将这些信息输入到 NM 算法中，直到收敛为止。首先，求解梯度，我们得到：
- en: '![](../Images/cedfd948a0bb2401ef6ccb4831579ccf.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/cedfd948a0bb2401ef6ccb4831579ccf.png)'
- en: (14)
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: (14)
- en: 'Solving for the Hessian, we have:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 求解 Hessian，我们得到：
- en: '![](../Images/420358b7e714700813e65ad39ff22643.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/420358b7e714700813e65ad39ff22643.png)'
- en: (15)
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: (15)
- en: 'Setting our initial guess to **Γ** = [-1.2,1], we have:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 将我们的初始猜测设置为 **Γ** = [-1.2,1]，我们得到：
- en: '![](../Images/c34850f961e3220718d01cfeb43d05b9.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c34850f961e3220718d01cfeb43d05b9.png)'
- en: (16)
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: (16)
- en: Thus, we successfully solve for the optimal minimum of our objective function
    at **Γ*** = [1,1].
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们成功地求解了目标函数的最优最小值为 **Γ*** = [1,1]。
- en: '**Solving via Python**'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '**通过 Python 求解**'
- en: 'We will now turn to solving this problem, and generalizing it to any function,
    in python using S[ymPy](https://www.sympy.org/en/index.html) — a python library
    for symbolic mathematics. First, let’s walk through defining Rosenbrock’s parabolic
    valley and calculating the gradient & Hessian of the function:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将转向用 Python 求解这个问题，并将其推广到任何函数，使用 S[ymPy](https://www.sympy.org/en/index.html)
    —— 一个用于符号数学的 Python 库。首先，让我们定义罗森布罗克的抛物线谷，并计算该函数的梯度和 Hessian：
- en: '[PRE0]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'SymPy allows us to investigate the symbolic representation of our equations.
    For example, if we call `objective` , we will see the corresponding output:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: SymPy 允许我们调查方程的符号表示。例如，如果我们调用 `objective` ，我们将看到相应的输出：
- en: '![](../Images/9df35cc55695fea8ebbfb88b4c9ee3c8.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9df35cc55695fea8ebbfb88b4c9ee3c8.png)'
- en: Symbolic representation of function by SymPy
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: SymPy 的函数符号表示
- en: 'Additionally, SymPy allows us take the derivatives of the respective function
    utilizing the `sm.diff()` command. If we run our defined functions to obtain the
    gradient `get_gradient(objective,Gamma)` , we obtain a numpy array representing
    the gradient:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，SymPy 允许我们利用 `sm.diff()` 命令对相关函数进行求导。如果我们运行定义的函数以获得梯度 `get_gradient(objective,Gamma)`
    ，我们得到一个表示梯度的 numpy 数组：
- en: '![](../Images/21ab38bf204a0bc7b70ba2ed3655f69c.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/21ab38bf204a0bc7b70ba2ed3655f69c.png)'
- en: Gradient as solved by SymPy
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: SymPy 求解的梯度
- en: 'Accessing a specific element, we can see the symbolic representation `get_gradient(objective,
    Gamma)[0]` :'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 访问特定元素时，我们可以看到符号表示 `get_gradient(objective, Gamma)[0]`：
- en: '![](../Images/6f149886b0c1fe9de8b3c3b61e564d05.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6f149886b0c1fe9de8b3c3b61e564d05.png)'
- en: df(**Γ**)/dx as solved by SymPy
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: SymPy 解出的 df(**Γ**)/dx
- en: 'Similarly, for the Hessian we can call `get_hessian(objective, Gamma)` :'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，对于 Hessian 矩阵，我们可以调用 `get_hessian(objective, Gamma)`：
- en: '![](../Images/b8ce56d342c7cbe4e30c8e69748f863d.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b8ce56d342c7cbe4e30c8e69748f863d.png)'
- en: Hessian as solved by SymPy
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: SymPy 解出的 Hessian 矩阵
- en: Accessing a specific element `get_hessian(objective,Gamma)[0][1]`
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 访问特定元素 `get_hessian(objective,Gamma)[0][1]`
- en: '![](../Images/a4359f5c2a62c74d4e04d877482c8e8e.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a4359f5c2a62c74d4e04d877482c8e8e.png)'
- en: df(**Γ**)/dxdy as solved by SymPy
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: SymPy 解出的 df(**Γ**)/dxdy
- en: 'One can easily verify that the gradient and Hessian are identical to the ones
    we solved out by hand. SymPy allows for the evaluation of any function given specified
    values for the symbols. For example, we can evaluate the gradient at our initial
    guess by tweaking the function as follows:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 可以轻松验证梯度和 Hessian 矩阵与我们手动计算得到的结果是相同的。SymPy 允许对给定符号的指定值评估任何函数。例如，我们可以通过如下调整函数来评估初始猜测处的梯度：
- en: '[PRE1]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'We can now compute our gradient given our starting point by calling `get_gradient(objective,
    Gamma, {x:-1.2,y:1.0})` :'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以通过调用 `get_gradient(objective, Gamma, {x:-1.2,y:1.0})` 来计算给定起始点的梯度：
- en: '![](../Images/b7bf3a05cba745cf7b866255c08fe451.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b7bf3a05cba745cf7b866255c08fe451.png)'
- en: Gradient evaluated at initial point
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 初始点处的梯度
- en: 'Similarly, for the Hessian `get_hessian(objective, Gamma, {x:-1.2,y:1.0})`
    :'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，对于 Hessian 矩阵 `get_hessian(objective, Gamma, {x:-1.2,y:1.0})`：
- en: '![](../Images/f285e3836dc3ecf69ad2b37843093ca7.png)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f285e3836dc3ecf69ad2b37843093ca7.png)'
- en: Hessian evaluated at initial point
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 初始点处的 Hessian 矩阵
- en: Again, we can verify that these values are correct from our work by hand above.
    *Now we have all the ingredients necessary to code Newton’s method* (the code
    for gradient descent is given at the end of this article as well)*:*
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，我们可以通过以上手动计算的工作来验证这些值是否正确。*现在我们拥有了编写牛顿法所需的所有要素*（梯度下降的代码在本文末尾也提供）*：*
- en: '[PRE2]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'We can now run the code via `newton_method(objective,Gamma,{x:-1.2,y:1})` :'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以通过 `newton_method(objective,Gamma,{x:-1.2,y:1})` 来运行代码：
- en: '![](../Images/e02ab685850059438bb587571f23aa35.png)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e02ab685850059438bb587571f23aa35.png)'
- en: Conclusion
  id: totrans-113
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结论
- en: There you have it! If you have made it up to this point, you now have a robust
    understanding of how to think about and abstractly formulate unconstrained mathematical
    optimization problems, along with the basic analytical approach and the more complex
    iterative methods for solving such problems. It is clear that the more information
    that we can incorporate about the function in the iterative schemes (i.e., higher
    order derivatives), the more efficient the convergence rate. ***Note that we are
    just brushing the surface of the complex world that is mathematical optimization.***
    Nevertheless, the tools we have discussed today can absolutely be utilized in
    practice and extended to higher dimensional optimization problems.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 就这样！如果你已经阅读到这一步，你现在对如何思考和抽象地制定无约束数学优化问题有了扎实的理解，包括基本的分析方法和更复杂的迭代方法。显然，我们在迭代方案中可以融入的信息越多（即更高阶的导数），收敛速度就越高效。***请注意，我们只是触及了数学优化复杂世界的表面。***
    尽管如此，我们今天讨论的工具在实践中绝对可以使用，并可以扩展到更高维的优化问题。
- en: Stay tuned for [**Part 2**](/optimization-newtons-method-profit-maximization-part-2-constrained-optimization-theory-dc18613c5770)
    **of this series** where we will extend what we have learned here to solving constrained
    optimization problems — which is an extremely practical extension on unconstrained
    optimization. In fact, most real world optimization problems will have some form
    of constraints on the choice variables. Then we will shift to **Part 3 of this
    series** where we will apply the optimization theory learned and additional econometric
    & economic theory to solve a simple example of profit maximization problem. I
    hope you have enjoyed reading this as much as I have enjoyed writing it!
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 请关注 [**第 2 部分**](/optimization-newtons-method-profit-maximization-part-2-constrained-optimization-theory-dc18613c5770)
    **的系列文章**，我们将在其中扩展我们在这里学到的内容，解决有约束的优化问题——这是无约束优化的一个极其实用的扩展。实际上，大多数实际优化问题都会对选择变量有某种形式的约束。然后我们将转向**本系列的第
    3 部分**，在其中我们将应用学到的优化理论和额外的计量经济学与经济理论来解决一个简单的利润最大化问题。希望你和我一样喜欢阅读这篇文章！
- en: Bonus— The Pitfalls of Newton’s Method
  id: totrans-116
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 奖励——牛顿法的陷阱
- en: 'Despite the attractiveness of Newton’s method, it is not without its own pitfalls.
    Notably, two main pitfalls exists — 1) NM is not always convergent even when choosing
    starting points near the solution & 2) NM requires the computation of the Hessian
    matrix at each step which can be computationally very expensive in higher dimensions.
    For pitfall #1), a respective solution is the Modified Newton method (MNM), which
    can be loosely thought of as gradient descent where the search direction is given
    by the Newton step, **Δ**. For pitfall #2), quasi-Newton methods, such as DFP
    or BFGS, have been proposed that approximate the inverse-Hessian used at each
    step to improve computational burden. For more information see, [1].'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '尽管牛顿法具有吸引力，但它也有自身的陷阱。尤其是，存在两个主要陷阱——1）即使在选择接近解的起始点时，NM 并不总是收敛；2）NM 在每一步都需要计算
    Hessian 矩阵，这在高维情况下计算开销非常大。对于陷阱 #1），一种相应的解决方案是改进牛顿法（MNM），可以粗略地认为它是梯度下降法，其中搜索方向由牛顿步长**Δ**给出。对于陷阱
    #2），已经提出了准牛顿方法，如 DFP 或 BFGS，这些方法通过近似每一步使用的逆 Hessian 矩阵来减轻计算负担。有关更多信息，请参见[1]。'
- en: Gradient Descent Function
  id: totrans-118
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 梯度下降函数
- en: '[PRE3]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Resources
  id: totrans-120
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 资源
- en: '[1] Snyman, J. A., & Wilke, D. N. (2019). *Practical mathematical optimization:
    Basic optimization theory and gradient-based algorithms* (2nd ed.). Springer.'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] Snyman, J. A., & Wilke, D. N. (2019). *实用数学优化：基本优化理论与基于梯度的算法*（第2版）。Springer。'
- en: '[2] [https://en.wikipedia.org/wiki/Gradient_descent](https://en.wikipedia.org/wiki/Gradient_descent)'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] [https://en.wikipedia.org/wiki/Gradient_descent](https://en.wikipedia.org/wiki/Gradient_descent)'
- en: '[3] [https://en.wikipedia.org/wiki/Newton%27s_method#:~:text=In%20numerical%20analysis%2C%20Newton%27s%20method%2C%20also%20known%20as,roots%20%28or%20zeroes%29%20of%20a%20real%20-valued%20function](https://en.wikipedia.org/wiki/Newton%27s_method#:~:text=In%20numerical%20analysis%2C%20Newton%27s%20method%2C%20also%20known%20as,roots%20%28or%20zeroes%29%20of%20a%20real%20-valued%20function).'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] [https://en.wikipedia.org/wiki/Newton%27s_method#:~:text=In%20numerical%20analysis%2C%20Newton%27s%20method%2C%20also%20known%20as,roots%20%28or%20zeroes%29%20of%20a%20real%20-valued%20function](https://en.wikipedia.org/wiki/Newton%27s_method#:~:text=In%20numerical%20analysis%2C%20Newton%27s%20method%2C%20also%20known%20as,roots%20%28or%20zeroes%29%20of%20a%20real%20-valued%20function).'
- en: '*Access all the code via this GitHub Repo:* [https://github.com/jakepenzak/Blog-Posts](https://github.com/jakepenzak/Blog-Posts)'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '*通过此 GitHub Repo 访问所有代码:* [https://github.com/jakepenzak/Blog-Posts](https://github.com/jakepenzak/Blog-Posts)'
- en: '*I appreciate you reading my post! My posts on Medium seek to explore real-world
    and theoretical applications utilizing* ***econometric*** *and* ***statistical/machine
    learning*** *techniques. Additionally, I seek to provide posts on the theoretical
    underpinnings of certain methodologies via theory and simulations. Most importantly,
    I write to learn! I hope to make complex topics slightly more accessible to all.
    If you enjoyed this post, please consider* [***following me on Medium***](https://medium.com/@jakepenzak)*!*'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '*感谢你阅读我的帖子！我在 Medium 上的帖子旨在探讨利用* ***计量经济学*** *和* ***统计/机器学习*** *技术的现实世界和理论应用。此外，我还希望通过理论和模拟提供某些方法论的理论基础。最重要的是，我写作是为了学习！我希望让复杂的话题对所有人稍微更易于理解。如果你喜欢这篇文章，请考虑*
    [***在 Medium 上关注我***](https://medium.com/@jakepenzak)*！*'
