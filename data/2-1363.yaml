- en: Is ChatGPT Actually Intelligent?
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ChatGPT çœŸçš„æ™ºèƒ½å—ï¼Ÿ
- en: åŸæ–‡ï¼š[https://towardsdatascience.com/is-chatgpt-actually-intelligent-42d07462fe59](https://towardsdatascience.com/is-chatgpt-actually-intelligent-42d07462fe59)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡ï¼š[https://towardsdatascience.com/is-chatgpt-actually-intelligent-42d07462fe59](https://towardsdatascience.com/is-chatgpt-actually-intelligent-42d07462fe59)
- en: Maybe notâ€¦
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä¹Ÿè®¸ä¸æ˜¯â€¦â€¦
- en: '[](https://huonglanchu.medium.com/?source=post_page-----42d07462fe59--------------------------------)[![Lan
    Chu](../Images/813b24f60d6cfe2c9273e064d850c7fe.png)](https://huonglanchu.medium.com/?source=post_page-----42d07462fe59--------------------------------)[](https://towardsdatascience.com/?source=post_page-----42d07462fe59--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----42d07462fe59--------------------------------)
    [Lan Chu](https://huonglanchu.medium.com/?source=post_page-----42d07462fe59--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://huonglanchu.medium.com/?source=post_page-----42d07462fe59--------------------------------)[![å…°æ¥š](../Images/813b24f60d6cfe2c9273e064d850c7fe.png)](https://huonglanchu.medium.com/?source=post_page-----42d07462fe59--------------------------------)[](https://towardsdatascience.com/?source=post_page-----42d07462fe59--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----42d07462fe59--------------------------------)
    [å…°æ¥š](https://huonglanchu.medium.com/?source=post_page-----42d07462fe59--------------------------------)'
- en: Â·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----42d07462fe59--------------------------------)
    Â·12 min readÂ·Jul 21, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: Â·å‘è¡¨äº [Towards Data Science](https://towardsdatascience.com/?source=post_page-----42d07462fe59--------------------------------)
    Â·é˜…è¯»æ—¶é—´ 12 åˆ†é’ŸÂ·2023å¹´7æœˆ21æ—¥
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: If you have been on any social media platform in the past months, I am sure
    you have heard about ChatGPT, Google Bard, Microsoft Bing and a myriad of new
    language models. All these new models, some can argue, are better writers than
    you and me and their English is definitely much better than mine **ğŸ¥²** Every few
    years, somebody just invents something crazy that makes you totally reconsider
    what is possible. And in this article, we will be talking about the kind of invention
    that is rocking everyone in the world â€” yes, you guessed it â€” ChatGPT.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœä½ åœ¨è¿‡å»å‡ ä¸ªæœˆé‡Œä½¿ç”¨è¿‡ä»»ä½•ç¤¾äº¤åª’ä½“å¹³å°ï¼Œæˆ‘ç›¸ä¿¡ä½ ä¸€å®šå¬è¯´è¿‡ ChatGPTã€Google Bardã€Microsoft Bing å’Œè®¸å¤šæ–°çš„è¯­è¨€æ¨¡å‹ã€‚æ‰€æœ‰è¿™äº›æ–°æ¨¡å‹ï¼Œæœ‰äººå¯èƒ½ä¼šäº‰è¾©è¯´ï¼Œæ¯”ä½ æˆ‘å†™å¾—æ›´å¥½ï¼Œä»–ä»¬çš„è‹±è¯­ä¹Ÿç¡®å®æ¯”æˆ‘çš„å¥½
    **ğŸ¥²** æ¯éš”å‡ å¹´ï¼Œå°±ä¼šæœ‰äººå‘æ˜ä¸€äº›ç–¯ç‹‚çš„ä¸œè¥¿ï¼Œè®©ä½ å®Œå…¨é‡æ–°è€ƒè™‘å¯èƒ½æ€§ã€‚è€Œåœ¨è¿™ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘ä»¬å°†è®¨è®ºä¸€ç§è®©å…¨ä¸–ç•Œéƒ½åœ¨éœ‡æ’¼çš„å‘æ˜â€”â€”æ˜¯çš„ï¼Œä½ çŒœå¯¹äº†â€”â€”ChatGPTã€‚
- en: '![](../Images/e9402ce0a1ff589cf90aaf22d6fcd633.png)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e9402ce0a1ff589cf90aaf22d6fcd633.png)'
- en: Image generated with Bing image creator. Artistic representation of human intelligence.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ç‰‡ç”± Bing å›¾åƒåˆ›å»ºå™¨ç”Ÿæˆã€‚å¯¹äººç±»æ™ºèƒ½çš„è‰ºæœ¯è¡¨ç°ã€‚
- en: As we will increasingly rely on AI to do things for us, to make decisions for
    us, it is natural to ask whether AI is truly intelligent in the sense that its
    understanding of language mirrors our own, or that it is fundamentally different?
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: éšç€æˆ‘ä»¬è¶Šæ¥è¶Šä¾èµ– AI ä¸ºæˆ‘ä»¬åšäº‹å’Œåšå†³ç­–ï¼Œæå‡º AI æ˜¯å¦çœŸæ­£æ™ºèƒ½çš„é—®é¢˜æ˜¯è‡ªç„¶çš„ï¼Œç‰¹åˆ«æ˜¯å®ƒå¯¹è¯­è¨€çš„ç†è§£æ˜¯å¦åæ˜ äº†æˆ‘ä»¬è‡ªå·±çš„ç†è§£ï¼Œè¿˜æ˜¯åœ¨æœ¬è´¨ä¸Šæœ‰æ‰€ä¸åŒï¼Ÿ
- en: To make sense of it all, we are going to first look into how the Generative-Pretrained
    Transformer (GPT) and ChatGPT works, and then discuss what it means for an AI
    to be intelligent.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†ç†è§£è¿™ä¸€åˆ‡ï¼Œæˆ‘ä»¬å°†é¦–å…ˆæ¢è®¨ç”Ÿæˆé¢„è®­ç»ƒå˜æ¢å™¨ï¼ˆGPTï¼‰å’Œ ChatGPT çš„å·¥ä½œåŸç†ï¼Œç„¶åè®¨è®º AI æ™ºèƒ½çš„å«ä¹‰ã€‚
- en: Understanding GPT Model
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç†è§£ GPT æ¨¡å‹
- en: The GPT model, first proposed by OpenAI in their paper [Improving Language Understanding
    by Generative Pre-Training](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf),
    uses unsupervised pre-training followed by supervised fine-tuning on various language
    tasks.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: GPT æ¨¡å‹æœ€åˆç”± OpenAI åœ¨å…¶è®ºæ–‡ [æ”¹è¿›è¯­è¨€ç†è§£çš„ç”Ÿæˆé¢„è®­ç»ƒ](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf)
    ä¸­æå‡ºï¼Œä½¿ç”¨æ— ç›‘ç£é¢„è®­ç»ƒï¼Œç„¶ååœ¨å„ç§è¯­è¨€ä»»åŠ¡ä¸Šè¿›è¡Œç›‘ç£å¾®è°ƒã€‚
- en: '![](../Images/46fbb43138db57f871a9d50d3d8a1fb9.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/46fbb43138db57f871a9d50d3d8a1fb9.png)'
- en: 'Source: [language_understanding_paper.pdf](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf)'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 'æ¥æº: [language_understanding_paper.pdf](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf)'
- en: The model architecture is based on Transformers, which has demonstrated robust
    performance in tasks such as machine translation and document generation. This
    model architecture was first introduced in the paper â€œ[Attention is all you need](https://arxiv.org/pdf/1706.03762.pdf)â€
    by Google researchers. It provides an organized memory for managing long-term
    dependencies in text compared to [recurrent neural networks](https://en.wikipedia.org/wiki/Recurrent_neural_network)
    and [convolutional neural network](https://en.wikipedia.org/wiki/Convolutional_neural_network),
    leading to better performance across a wide range of tasks.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: è¯¥æ¨¡å‹æ¶æ„åŸºäº Transformersï¼Œå·²åœ¨æœºå™¨ç¿»è¯‘å’Œæ–‡æ¡£ç”Ÿæˆç­‰ä»»åŠ¡ä¸­å±•ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ã€‚è¯¥æ¨¡å‹æ¶æ„é¦–æ¬¡åœ¨ Google ç ”ç©¶äººå‘˜çš„è®ºæ–‡ â€œ[Attention
    is all you need](https://arxiv.org/pdf/1706.03762.pdf)â€ ä¸­æå‡ºã€‚ä¸[é€’å½’ç¥ç»ç½‘ç»œ](https://en.wikipedia.org/wiki/Recurrent_neural_network)å’Œ[å·ç§¯ç¥ç»ç½‘ç»œ](https://en.wikipedia.org/wiki/Convolutional_neural_network)ç›¸æ¯”ï¼Œå®ƒæä¾›äº†ä¸€ä¸ªæœ‰ç»„ç»‡çš„å†…å­˜ï¼Œç”¨äºç®¡ç†æ–‡æœ¬ä¸­çš„é•¿æœŸä¾èµ–å…³ç³»ï¼Œä»è€Œåœ¨å¹¿æ³›çš„ä»»åŠ¡ä¸­å®ç°äº†æ›´å¥½çš„æ€§èƒ½ã€‚
- en: '**Itâ€™s a prediction game**'
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**è¿™æ˜¯ä¸€ä¸ªé¢„æµ‹æ¸¸æˆ**'
- en: You can think of GPT model as a machine that is good at guessing what comes
    next. For example, if you give it the phrase â€œ**Instead of turning right, she
    turns**â€¦â€, GPT might predict â€œleftâ€ or â€œbackâ€ or something else as the next word.
    How did it learn this? It does this by reading a large amount of existing text
    and learning how words tend to appear in context with other words and predicting
    the next most likely word that might appear in response to a request from users.
    Or to be precise, it learns how to interpret the positional encoding from the
    data. This means that we slap a number of each word in a sentence and as you train
    the model on a lot of text, it learns how to interpret the positional encoding
    of each word in a sentence.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ å¯ä»¥å°† GPT æ¨¡å‹è§†ä¸ºä¸€ç§æ“…é•¿é¢„æµ‹ä¸‹ä¸€ä¸ªå†…å®¹çš„æœºå™¨ã€‚ä¾‹å¦‚ï¼Œå¦‚æœä½ ç»™å®ƒçŸ­è¯­â€œ**å¥¹æ²¡æœ‰å³è½¬ï¼Œè€Œæ˜¯è½¬**â€¦â€¦â€ï¼ŒGPT å¯èƒ½ä¼šé¢„æµ‹â€œå·¦è½¬â€æˆ–â€œå›è½¬â€æˆ–å…¶ä»–ä½œä¸ºä¸‹ä¸€ä¸ªå•è¯ã€‚å®ƒæ˜¯å¦‚ä½•å­¦ä¹ åˆ°è¿™ä¸€ç‚¹çš„ï¼Ÿå®ƒé€šè¿‡é˜…è¯»å¤§é‡ç°æœ‰æ–‡æœ¬ï¼Œå­¦ä¹ å•è¯åœ¨ä¸å…¶ä»–å•è¯çš„ä¸Šä¸‹æ–‡ä¸­å¦‚ä½•å‡ºç°ï¼Œå¹¶é¢„æµ‹å¯èƒ½å‡ºç°çš„ä¸‹ä¸€ä¸ªæœ€å¯èƒ½çš„å•è¯ã€‚æˆ–è€…æ›´å‡†ç¡®åœ°è¯´ï¼Œå®ƒå­¦ä¹ å¦‚ä½•è§£è¯»æ•°æ®ä¸­çš„ä½ç½®ç¼–ç ã€‚è¿™æ„å‘³ç€æˆ‘ä»¬ç»™å¥å­ä¸­çš„æ¯ä¸ªå•è¯æ‰“ä¸Šä¸€ä¸ªç¼–å·ï¼Œå½“ä½ åœ¨å¤§é‡æ–‡æœ¬ä¸Šè®­ç»ƒæ¨¡å‹æ—¶ï¼Œå®ƒå­¦ä¹ å¦‚ä½•è§£è¯»å¥å­ä¸­æ¯ä¸ªå•è¯çš„ä½ç½®ç¼–ç ã€‚
- en: '![](../Images/8070cda8dd03456a317466de5d392c79.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8070cda8dd03456a317466de5d392c79.png)'
- en: Image by author.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ç‰‡ç”±ä½œè€…æä¾›ã€‚
- en: '![](../Images/22381e723a004acaaec9ddddca29a381.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/22381e723a004acaaec9ddddca29a381.png)'
- en: Image by author
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ç‰‡ç”±ä½œè€…æä¾›ã€‚
- en: '**Transformer**'
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**Transformer**'
- en: But it is not just guessing one word at a time, it is thinking about whole sentences
    and even paragraphs â€” and this is where the transformer architecture comes into
    the picture.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†è¿™ä¸ä»…ä»…æ˜¯ä¸€æ¬¡çŒœä¸€ä¸ªè¯ï¼Œå®ƒæ˜¯åœ¨è€ƒè™‘æ•´ä¸ªå¥å­ç”šè‡³æ®µè½â€”â€”è¿™å°±æ˜¯ transformer æ¶æ„å‘æŒ¥ä½œç”¨çš„åœ°æ–¹ã€‚
- en: The â€œmagic ingredientâ€ that helps GPT make these predictions is the self-attention
    mechanism in transformer architecture. This mechanism helps the model determine
    the importance of each word in a sentence when predicting the next word. For instance,
    in the sentence â€œI walked my dog every day because it makes meâ€¦â€, the word â€œdogâ€
    might indicate to GPT that the next word is likely related to dogs and feelings,
    such as â€œhappyâ€ or â€œrelaxedâ€.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: å¸®åŠ© GPT åšå‡ºè¿™äº›é¢„æµ‹çš„â€œé­”æ³•æˆåˆ†â€æ˜¯ transformer æ¶æ„ä¸­çš„è‡ªæ³¨æ„åŠ›æœºåˆ¶ã€‚è¯¥æœºåˆ¶å¸®åŠ©æ¨¡å‹åœ¨é¢„æµ‹ä¸‹ä¸€ä¸ªå•è¯æ—¶ç¡®å®šå¥å­ä¸­æ¯ä¸ªå•è¯çš„é‡è¦æ€§ã€‚ä¾‹å¦‚ï¼Œåœ¨å¥å­â€œæˆ‘æ¯å¤©éƒ½å¸¦ç€æˆ‘çš„ç‹—æ•£æ­¥ï¼Œå› ä¸ºå®ƒè®©æˆ‘â€¦â€¦â€ä¸­ï¼Œå•è¯â€œç‹—â€å¯èƒ½ä¼šæç¤º
    GPT ä¸‹ä¸€ä¸ªå•è¯å¾ˆå¯èƒ½ä¸ç‹—å’Œæƒ…æ„Ÿç›¸å…³ï¼Œå¦‚â€œå¿«ä¹â€æˆ–â€œæ”¾æ¾â€ã€‚
- en: '![](../Images/7cb0aa616f35cb69351873f681a480a4.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7cb0aa616f35cb69351873f681a480a4.png)'
- en: 'Source: [language_understanding_paper.pdf](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf)'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥æºï¼š[language_understanding_paper.pdf](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf)
- en: '**Self-attention mechanism**'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '**è‡ªæ³¨æ„åŠ›æœºåˆ¶**'
- en: What is self-attention anyway? Letâ€™s imagine a group of people (words) playing
    a game (sentence). In this game, each player represents a word in a sentence.
    Now, imagine that each player needs to determine how much attention they should
    pay to every other player on the field, including themselves, to predict the next
    move in the game.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: è‡ªæ³¨æ„åŠ›ç©¶ç«Ÿæ˜¯ä»€ä¹ˆï¼Ÿè®©æˆ‘ä»¬æƒ³è±¡ä¸€ç¾¤äººï¼ˆå•è¯ï¼‰æ­£åœ¨ç©ä¸€ä¸ªæ¸¸æˆï¼ˆå¥å­ï¼‰ã€‚åœ¨è¿™ä¸ªæ¸¸æˆä¸­ï¼Œæ¯ä¸ªç©å®¶ä»£è¡¨ä¸€ä¸ªå¥å­ä¸­çš„å•è¯ã€‚ç°åœ¨ï¼Œæƒ³è±¡æ¯ä¸ªç©å®¶éœ€è¦ç¡®å®šä»–ä»¬åº”è¯¥å¤šå…³æ³¨åœºä¸Šå…¶ä»–æ¯ä¸ªç©å®¶ï¼ŒåŒ…æ‹¬è‡ªå·±ï¼Œä»¥é¢„æµ‹æ¸¸æˆä¸­çš„ä¸‹ä¸€ä¸ªåŠ¨ä½œã€‚
- en: In this context, self-attention is like each player communicating with every
    other player in the game. This communication allows them to assess the importance
    of every other player. Some players may seem highly significant (high attention)
    to you, while others may seem less so (low attention).
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ä¸ªä¸Šä¸‹æ–‡ä¸­ï¼Œè‡ªæ³¨æ„åŠ›æœºåˆ¶å°±åƒæ¯ä¸ªç©å®¶ä¸æ¸¸æˆä¸­çš„å…¶ä»–æ‰€æœ‰ç©å®¶è¿›è¡Œäº¤æµã€‚è¿™ç§äº¤æµä½¿ä»–ä»¬èƒ½å¤Ÿè¯„ä¼°å…¶ä»–æ¯ä¸ªç©å®¶çš„é‡è¦æ€§ã€‚æœ‰äº›ç©å®¶å¯¹ä½ å¯èƒ½æ˜¾å¾—éå¸¸é‡è¦ï¼ˆé«˜æ³¨æ„åŠ›ï¼‰ï¼Œè€Œå…¶ä»–ç©å®¶å¯èƒ½æ˜¾å¾—ä¸é‚£ä¹ˆé‡è¦ï¼ˆä½æ³¨æ„åŠ›ï¼‰ã€‚
- en: '![](../Images/8396ac02a5a2d75f61400a4406ce0d6d.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8396ac02a5a2d75f61400a4406ce0d6d.png)'
- en: Image by author. The thickness of the arrow represents the strong attention
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: ä½œè€…æä¾›çš„å›¾ç‰‡ã€‚ç®­å¤´çš„ç²—ç»†ä»£è¡¨äº†å¼ºæ³¨æ„åŠ›ã€‚
- en: The level of importance is determined by how relevant each player is to the
    individualâ€™s understanding of the game. For instance, if youâ€™re a forward in a
    soccer game, you might find the midfielders highly relevant because they support
    your attacks, while the goalkeeper might seem less important to your immediate
    actions.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: é‡è¦æ€§çš„çº§åˆ«ç”±æ¯ä¸ªç©å®¶ä¸ä¸ªäººå¯¹æ¸¸æˆç†è§£çš„ç›¸å…³æ€§å†³å®šã€‚ä¾‹å¦‚ï¼Œå¦‚æœä½ åœ¨è¶³çƒæ¯”èµ›ä¸­æ˜¯å‰é”‹ï¼Œä½ å¯èƒ½ä¼šå‘ç°ä¸­åœºçƒå‘˜éå¸¸é‡è¦ï¼Œå› ä¸ºä»–ä»¬æ”¯æŒä½ çš„è¿›æ”»ï¼Œè€Œé—¨å°†å¯èƒ½å¯¹ä½ çš„å³æ—¶è¡ŒåŠ¨çœ‹èµ·æ¥ä¸é‚£ä¹ˆé‡è¦ã€‚
- en: Now, imagine all of this happening simultaneously, with every single player
    communicating with all others to understand their relevance to the game. From
    this, each player calculates the significance score of every other player, including
    themselves.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ï¼Œæƒ³è±¡è¿™ä¸€åˆ‡åŒæ—¶å‘ç”Ÿï¼Œæ¯ä¸€ä¸ªç©å®¶ä¸æ‰€æœ‰å…¶ä»–ç©å®¶è¿›è¡Œæ²Ÿé€šï¼Œä»¥äº†è§£ä»–ä»¬åœ¨æ¸¸æˆä¸­çš„ç›¸å…³æ€§ã€‚ç”±æ­¤ï¼Œæ¯ä¸ªç©å®¶è®¡ç®—å‡ºæ¯ä¸ªå…¶ä»–ç©å®¶çš„æ˜¾è‘—æ€§åˆ†æ•°ï¼ŒåŒ…æ‹¬ä»–ä»¬è‡ªå·±ã€‚
- en: In other words, each player isnâ€™t just considering one other player at a time,
    but all other players on the field simultaneously. This simultaneous consideration
    is a key feature of the self-attention mechanism, and itâ€™s what enables the model
    to capture complex relationships and dependencies between players in a game.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: æ¢å¥è¯è¯´ï¼Œæ¯ä¸ªç©å®¶ä¸ä»…ä»…æ˜¯è€ƒè™‘ä¸€ä¸ªå…¶ä»–ç©å®¶ï¼Œè€Œæ˜¯åŒæ—¶è€ƒè™‘åœºä¸Šæ‰€æœ‰å…¶ä»–ç©å®¶ã€‚è¿™ç§åŒæ—¶è€ƒè™‘æ˜¯è‡ªæ³¨æ„åŠ›æœºåˆ¶çš„ä¸€ä¸ªå…³é”®ç‰¹æ€§ï¼Œå®ƒä½¿æ¨¡å‹èƒ½å¤Ÿæ•æ‰æ¸¸æˆä¸­ç©å®¶ä¹‹é—´å¤æ‚çš„å…³ç³»å’Œä¾èµ–ã€‚
- en: '![](../Images/29147d03b9d9b5ae6bb8f3d6a7274e4b.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/29147d03b9d9b5ae6bb8f3d6a7274e4b.png)'
- en: Image by author.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: ä½œè€…æä¾›çš„å›¾ç‰‡ã€‚
- en: In the above example, the â€œwordâ€ server means very different things. Self-attention
    allows a model to understand a word in the context of the words around it. When
    the model processed the word â€œserverâ€ in the first sentence, it might be attending
    to the word â€œcheckâ€ while in the second sentence, the server might be attending
    to the word â€œcrashedâ€ because the only way for the model to understand that the
    word â€œserverâ€ in the second sentence is a computer server/system is because of
    the word â€œcrashedâ€. Meanwhile in the first sentence, the word server is attending
    itself to the word â€œcheckâ€ because that is the only way the model can understand
    that the word â€œserverâ€ in the first sentence refers to a human.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä¸Šè¿°ä¾‹å­ä¸­ï¼Œâ€œwordâ€è¿™ä¸ªè¯çš„æ„æ€éå¸¸ä¸åŒã€‚è‡ªæ³¨æ„åŠ›æœºåˆ¶å…è®¸æ¨¡å‹åœ¨å‘¨å›´è¯è¯­çš„ä¸Šä¸‹æ–‡ä¸­ç†è§£ä¸€ä¸ªè¯ã€‚å½“æ¨¡å‹å¤„ç†ç¬¬ä¸€ä¸ªå¥å­ä¸­çš„â€œserverâ€æ—¶ï¼Œå®ƒå¯èƒ½ä¼šå…³æ³¨â€œcheckâ€è¿™ä¸ªè¯ï¼Œè€Œåœ¨ç¬¬äºŒä¸ªå¥å­ä¸­ï¼ŒæœåŠ¡å™¨å¯èƒ½ä¼šå…³æ³¨â€œcrashedâ€è¿™ä¸ªè¯ï¼Œå› ä¸ºæ¨¡å‹ç†è§£ç¬¬äºŒä¸ªå¥å­ä¸­â€œserverâ€æŒ‡çš„æ˜¯è®¡ç®—æœºæœåŠ¡å™¨/ç³»ç»Ÿçš„å”¯ä¸€æ–¹å¼æ˜¯é€šè¿‡â€œcrashedâ€è¿™ä¸ªè¯ã€‚åŒæ—¶ï¼Œåœ¨ç¬¬ä¸€ä¸ªå¥å­ä¸­ï¼Œè¯â€œserverâ€å…³æ³¨â€œcheckâ€è¿™ä¸ªè¯ï¼Œå› ä¸ºè¿™æ˜¯æ¨¡å‹ç†è§£ç¬¬ä¸€ä¸ªå¥å­ä¸­â€œserverâ€æŒ‡çš„æ˜¯ä¸€ä¸ªäººçš„å”¯ä¸€æ–¹å¼ã€‚
- en: The self-attention mechanism allows the GPT model to weigh the importance of
    each word in the sentence when predicting the next word by outputting the weights
    of each word as a vector, which will be used as the input for the prediction layer.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: è‡ªæ³¨æ„åŠ›æœºåˆ¶å…è®¸GPTæ¨¡å‹åœ¨é¢„æµ‹ä¸‹ä¸€ä¸ªè¯æ—¶æƒè¡¡å¥å­ä¸­æ¯ä¸ªè¯çš„é‡è¦æ€§ï¼Œé€šè¿‡å°†æ¯ä¸ªè¯çš„æƒé‡è¾“å‡ºä¸ºä¸€ä¸ªå‘é‡ï¼Œè¿™äº›å‘é‡å°†ä½œä¸ºé¢„æµ‹å±‚çš„è¾“å…¥ã€‚
- en: '**The prediction layer**'
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**é¢„æµ‹å±‚**'
- en: The final part on top of the transformer layer is a linear layer that transforms
    the vector output of the Transformer layer into a set of logits â€” scores that
    the machine learning model assigns to predict the next word in a sentence. Each
    possible next token gets a logit (a score). These logits are then passed through
    a SoftMax function to get probabilities, and the word with the highest probability
    is selected as the prediction.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: Transformerå±‚é¡¶éƒ¨çš„æœ€ç»ˆéƒ¨åˆ†æ˜¯ä¸€ä¸ªçº¿æ€§å±‚ï¼Œå®ƒå°†Transformerå±‚çš„å‘é‡è¾“å‡ºè½¬æ¢ä¸ºä¸€ç»„logitsâ€”â€”æœºå™¨å­¦ä¹ æ¨¡å‹åˆ†é…çš„ç”¨äºé¢„æµ‹å¥å­ä¸­ä¸‹ä¸€ä¸ªè¯çš„åˆ†æ•°ã€‚æ¯ä¸ªå¯èƒ½çš„ä¸‹ä¸€ä¸ªè¯æ±‡éƒ½è·å¾—ä¸€ä¸ªlogitï¼ˆä¸€ä¸ªåˆ†æ•°ï¼‰ã€‚è¿™äº›logitséšåé€šè¿‡SoftMaxå‡½æ•°è·å¾—æ¦‚ç‡ï¼Œå…·æœ‰æœ€é«˜æ¦‚ç‡çš„è¯è¢«é€‰æ‹©ä¸ºé¢„æµ‹è¯ã€‚
- en: '**This process is akin to a prediction game.** And because there are many possible
    words that could come next in our example sentence â€œInstead of turning right,
    she turns...â€, there is an element of randomness in the way a model can respond.
    In many cases, the GPT models can answer the same question in different ways â€”
    which I think all of us have experienced while playing with ChatGPT.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '**è¿™ä¸ªè¿‡ç¨‹ç±»ä¼¼äºä¸€ä¸ªé¢„æµ‹æ¸¸æˆã€‚** ç”±äºåœ¨æˆ‘ä»¬çš„ç¤ºä¾‹å¥å­â€œInstead of turning right, she turns...â€ä¸­ï¼Œå¯èƒ½å‡ºç°å¾ˆå¤šç§æ¥ä¸‹æ¥çš„å•è¯ï¼Œå› æ­¤æ¨¡å‹å“åº”çš„æ–¹å¼ä¸­å­˜åœ¨ä¸€å®šçš„éšæœºæ€§ã€‚åœ¨è®¸å¤šæƒ…å†µä¸‹ï¼ŒGPT
    æ¨¡å‹å¯ä»¥ç”¨ä¸åŒçš„æ–¹å¼å›ç­”ç›¸åŒçš„é—®é¢˜â€”â€”æˆ‘è®¤ä¸ºæˆ‘ä»¬æ‰€æœ‰äººéƒ½åœ¨ä¸ ChatGPT äº’åŠ¨æ—¶ç»å†è¿‡è¿™ç§æƒ…å†µã€‚'
- en: How ChatGPT Works
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ChatGPT çš„å·¥ä½œåŸç†
- en: Fine-tuning GPT model for dialogue
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å¾®è°ƒ GPT æ¨¡å‹ä»¥è¿›è¡Œå¯¹è¯
- en: ChatGPT is a fine-tuned version of GPT-3.5 and has been developed in a way that
    allows it to understand and respond to user questions and instructions.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: ChatGPT æ˜¯ GPT-3.5 çš„ä¸€ä¸ªå¾®è°ƒç‰ˆæœ¬ï¼Œå¹¶ä¸”å·²ç»ä»¥ä¸€ç§å…è®¸å®ƒç†è§£å’Œå“åº”ç”¨æˆ·é—®é¢˜å’ŒæŒ‡ä»¤çš„æ–¹å¼è¿›è¡Œäº†å¼€å‘ã€‚
- en: Training data
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è®­ç»ƒæ•°æ®
- en: The three primary sources of information are (1) information that is publicly
    available on the internet, (2) information that is licensed from third parties,
    and (3) information that users or human trainers provide.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸‰ä¸ªä¸»è¦çš„ä¿¡æ¯æ¥æºæ˜¯ï¼ˆ1ï¼‰äº’è”ç½‘ä¸Šå…¬å¼€å¯ç”¨çš„ä¿¡æ¯ï¼Œï¼ˆ2ï¼‰ä»ç¬¬ä¸‰æ–¹å¤„è·å¾—çš„è®¸å¯ä¿¡æ¯ï¼Œä»¥åŠï¼ˆ3ï¼‰ç”¨æˆ·æˆ–äººç±»è®­ç»ƒå¸ˆæä¾›çš„ä¿¡æ¯ã€‚
- en: Training process
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è®­ç»ƒè¿‡ç¨‹
- en: In this section, I will give a high-level explanation of the training process
    of ChatGPT, which involves supervised fine-tuning, the creation of a reward model
    for reinforcement learning, and fine-tuning with Proximal Policy Optimisation
    (PPO).
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ä¸€éƒ¨åˆ†ï¼Œæˆ‘å°†å¯¹ ChatGPT çš„è®­ç»ƒè¿‡ç¨‹è¿›è¡Œé«˜å±‚æ¬¡çš„è§£é‡Šï¼Œå…¶ä¸­åŒ…æ‹¬ç›‘ç£å¾®è°ƒã€åˆ›å»ºå¥–åŠ±æ¨¡å‹ç”¨äºå¼ºåŒ–å­¦ä¹ ä»¥åŠä½¿ç”¨é‚»è¿‘ç­–ç•¥ä¼˜åŒ–ï¼ˆPPOï¼‰è¿›è¡Œå¾®è°ƒã€‚
- en: '![](../Images/0abe4e197e41c0f7bf301ff4fc4960b8.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0abe4e197e41c0f7bf301ff4fc4960b8.png)'
- en: 'Source: Training language models to follow instructions with human feedback
    OpenAI et al., 2022 [https://arxiv.org/pdf/2203.02155.pdf](https://arxiv.org/pdf/2203.02155.pdf).'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥æºï¼šè®­ç»ƒè¯­è¨€æ¨¡å‹ä»¥éµå¾ªäººç±»åé¦ˆçš„æŒ‡ä»¤ OpenAI ç­‰ï¼Œ2022 [https://arxiv.org/pdf/2203.02155.pdf](https://arxiv.org/pdf/2203.02155.pdf)ã€‚
- en: '**Step 1: Train a supervised learning model**'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '**ç¬¬ 1 æ­¥ï¼šè®­ç»ƒä¸€ä¸ªç›‘ç£å­¦ä¹ æ¨¡å‹**'
- en: In this step, the goal is to provide the model with examples of the desired
    behavior. This is done by having human AI trainers interact with the model.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ä¸€æ­¥ä¸­ï¼Œç›®æ ‡æ˜¯å‘æ¨¡å‹æä¾›æ‰€éœ€è¡Œä¸ºçš„ç¤ºä¾‹ã€‚è¿™æ˜¯é€šè¿‡è®©äººç±» AI è®­ç»ƒå¸ˆä¸æ¨¡å‹è¿›è¡Œäº’åŠ¨æ¥å®ç°çš„ã€‚
- en: This starts with collecting prompts from users. These prompts are then given
    to the AI trainers who generate responses that demonstrate the desired output
    behavior. This prompt-response labelled data is then used to fine-tune GPT-3.5,
    with the goal of learning to generate similar responses when presented with similar
    prompts in the future.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä»æ”¶é›†ç”¨æˆ·çš„æç¤ºå¼€å§‹ã€‚è¿™äº›æç¤ºç„¶åäº¤ç»™ AI è®­ç»ƒå¸ˆï¼ŒAI è®­ç»ƒå¸ˆç”Ÿæˆå±•ç¤ºæ‰€éœ€è¾“å‡ºè¡Œä¸ºçš„å“åº”ã€‚ç„¶åï¼Œè¿™äº›æç¤º-å“åº”æ ‡æ³¨æ•°æ®è¢«ç”¨æ¥å¾®è°ƒ GPT-3.5ï¼Œç›®æ ‡æ˜¯åœ¨æœªæ¥é¢å¯¹ç±»ä¼¼æç¤ºæ—¶ç”Ÿæˆç±»ä¼¼çš„å“åº”ã€‚
- en: '**Step 2: Train a reward model for Reinforcement learning**'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '**ç¬¬ 2 æ­¥ï¼šè®­ç»ƒä¸€ä¸ªå¥–åŠ±æ¨¡å‹ä»¥è¿›è¡Œå¼ºåŒ–å­¦ä¹ **'
- en: The goal is to train a reward model that gives a score to the Supervised fine-tuned
    model outputs based on how desirable these outputs are for AI trainers. In other
    words, this reward model says something about the quality of the output based
    on the AI trainerâ€™s preference.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: ç›®æ ‡æ˜¯è®­ç»ƒä¸€ä¸ªå¥–åŠ±æ¨¡å‹ï¼Œè¯¥æ¨¡å‹æ ¹æ®ç›‘ç£å¾®è°ƒæ¨¡å‹è¾“å‡ºçš„ç»“æœå¯¹å…¶è¿›è¡Œè¯„åˆ†ï¼Œè¿™äº›è¾“å‡ºå¯¹äº AI è®­ç»ƒå¸ˆæ¥è¯´æ˜¯å¤šä¹ˆä»¤äººæ»¡æ„ã€‚æ¢å¥è¯è¯´ï¼Œè¿™ä¸ªå¥–åŠ±æ¨¡å‹æ ¹æ® AI è®­ç»ƒå¸ˆçš„åå¥½å¯¹è¾“å‡ºçš„è´¨é‡è¿›è¡Œè¯„ä¼°ã€‚
- en: 'Hereâ€™s how it works:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å°±æ˜¯å®ƒçš„å·¥ä½œåŸç†ï¼š
- en: A list of prompts is selected, and the supervised fine-tuned model generates
    multiple outputs for each prompt.
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: é€‰æ‹©ä¸€ä¸ªæç¤ºåˆ—è¡¨ï¼Œç›‘ç£å¾®è°ƒæ¨¡å‹ä¸ºæ¯ä¸ªæç¤ºç”Ÿæˆå¤šä¸ªè¾“å‡ºã€‚
- en: AI trainers rank the outputs from best to worst. The result is a new labeled
    dataset, where the rankings are the labels.
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: AI è®­ç»ƒå¸ˆå¯¹è¾“å‡ºç»“æœè¿›è¡Œä»æœ€å¥½åˆ°æœ€å·®çš„æ’åã€‚ç»“æœæ˜¯ä¸€ä¸ªæ–°çš„æ ‡æ³¨æ•°æ®é›†ï¼Œå…¶ä¸­çš„æ’åå°±æ˜¯æ ‡ç­¾ã€‚
- en: This new data is used to train a reward model that takes the output of the SFT
    model and ranks them in order of preference.
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è¿™äº›æ–°æ•°æ®ç”¨äºè®­ç»ƒä¸€ä¸ªå¥–åŠ±æ¨¡å‹ï¼Œè¯¥æ¨¡å‹æ¥å— SFT æ¨¡å‹çš„è¾“å‡ºå¹¶æŒ‰åå¥½é¡ºåºå¯¹å…¶è¿›è¡Œæ’åã€‚
- en: '**Step 3: Optimize the policy using the PPO algorithm**'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '**ç¬¬ 3 æ­¥ï¼šä½¿ç”¨ PPO ç®—æ³•ä¼˜åŒ–ç­–ç•¥**'
- en: '[**Reinforcement Learning**](https://www.assemblyai.com/blog/reinforcement-learning-with-deep-q-learning-explained/)
    (RL) is now applied to improve the policy by letting it optimize the reward model.
    In RL, a policy is a strategy or a set of rules that an agent follows to make
    decisions in an environment. It is a way for the agent to determine which action
    to take based on the current state.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '[**å¼ºåŒ–å­¦ä¹ **](https://www.assemblyai.com/blog/reinforcement-learning-with-deep-q-learning-explained/)ï¼ˆRLï¼‰ç°åœ¨è¢«åº”ç”¨äºé€šè¿‡ä¼˜åŒ–å¥–åŠ±æ¨¡å‹æ¥æ”¹è¿›ç­–ç•¥ã€‚åœ¨RLä¸­ï¼Œç­–ç•¥æ˜¯ä»£ç†åœ¨ç¯å¢ƒä¸­åšå‡ºå†³ç­–æ—¶éµå¾ªçš„ç­–ç•¥æˆ–è§„åˆ™é›†åˆã€‚è¿™æ˜¯ä¸€ç§è®©ä»£ç†æ ¹æ®å½“å‰çŠ¶æ€ç¡®å®šé‡‡å–ä½•ç§è¡ŒåŠ¨çš„æ–¹æ³•ã€‚'
- en: How Reinforcement learning works â€” Example using Hide and Seek game
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: å¼ºåŒ–å­¦ä¹ å¦‚ä½•å·¥ä½œâ€”â€”ä»¥æ‰è¿·è—æ¸¸æˆä¸ºä¾‹
- en: RL is about learning from sequential interactions with the environment to optimize
    a long-term goal. Given the current input, you make a decision and the next input
    depends on your decisions. [PPO](https://openai.com/research/openai-baselines-ppo)
    (Proximal policy optimization) is the algorithm that is used by openAI to train
    agents in RL. In RL, **the agent/AI** **learns from and updates the current policy
    directly**, rather than learning only from past experiences (only training data).
    This means that PPO is continuously updating the current policy based on the actions
    that the agent is taking and the rewards it is receiving.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: RLæ˜¯é€šè¿‡ä¸ç¯å¢ƒçš„è¿ç»­äº¤äº’æ¥ä¼˜åŒ–é•¿æœŸç›®æ ‡çš„å­¦ä¹ è¿‡ç¨‹ã€‚æ ¹æ®å½“å‰è¾“å…¥ï¼Œä½ åšå‡ºå†³ç­–ï¼Œä¸‹ä¸€æ¬¡è¾“å…¥å–å†³äºä½ çš„å†³ç­–ã€‚[PPO](https://openai.com/research/openai-baselines-ppo)ï¼ˆè¿‘ç«¯ç­–ç•¥ä¼˜åŒ–ï¼‰æ˜¯OpenAIç”¨æ¥è®­ç»ƒRLä»£ç†çš„ç®—æ³•ã€‚åœ¨RLä¸­ï¼Œ**ä»£ç†/AI**
    **ç›´æ¥ä»å’Œæ›´æ–°å½“å‰ç­–ç•¥ä¸­å­¦ä¹ **ï¼Œè€Œä¸æ˜¯ä»…ä»…ä»è¿‡å»çš„ç»éªŒï¼ˆä»…è®­ç»ƒæ•°æ®ï¼‰ä¸­å­¦ä¹ ã€‚è¿™æ„å‘³ç€PPOæ ¹æ®ä»£ç†é‡‡å–çš„è¡ŒåŠ¨å’Œæ”¶åˆ°çš„å¥–åŠ±ä¸æ–­æ›´æ–°å½“å‰ç­–ç•¥ã€‚
- en: First, the trained supervised fine-tuned model in step 1 is used to initialize
    the PPO model. It will generate a response given a prompt. In the next stage,
    the reward model (built in step 2) produces a reward score for the response. Finally,
    the reward is fed back to the baseline model to update the parameters using the
    PPO algorithm that is supposed to increase the likelihood of better responses
    in the future. This process is iterated multiple times, with the model improving
    as it receives more feedback from the reward model and adjusts its parameters
    accordingly using PPO. This allows the model to learn from human feedback and
    improve its ability to generate future better responses.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: é¦–å…ˆï¼Œåœ¨ç¬¬1æ­¥ä¸­è®­ç»ƒè¿‡çš„ç›‘ç£å¾®è°ƒæ¨¡å‹ç”¨äºåˆå§‹åŒ–PPOæ¨¡å‹ã€‚å®ƒä¼šåœ¨ç»™å®šæç¤ºæ—¶ç”Ÿæˆä¸€ä¸ªå“åº”ã€‚åœ¨ä¸‹ä¸€é˜¶æ®µï¼Œå¥–åŠ±æ¨¡å‹ï¼ˆåœ¨ç¬¬2æ­¥ä¸­å»ºç«‹ï¼‰ä¸ºå“åº”ç”Ÿæˆå¥–åŠ±åˆ†æ•°ã€‚æœ€åï¼Œå¥–åŠ±åé¦ˆç»™åŸºå‡†æ¨¡å‹ï¼Œä»¥ä½¿ç”¨PPOç®—æ³•æ›´æ–°å‚æ•°ï¼Œæ—¨åœ¨æé«˜æœªæ¥æ›´å¥½å“åº”çš„å¯èƒ½æ€§ã€‚è¿™ä¸ªè¿‡ç¨‹ä¼šé‡å¤å¤šæ¬¡ï¼Œæ¨¡å‹éšç€ä»å¥–åŠ±æ¨¡å‹æ”¶åˆ°æ›´å¤šåé¦ˆè€Œä¸æ–­æ”¹è¿›ï¼Œå¹¶ä½¿ç”¨PPOè°ƒæ•´å…¶å‚æ•°ã€‚è¿™ä½¿å¾—æ¨¡å‹èƒ½å¤Ÿä»äººç±»åé¦ˆä¸­å­¦ä¹ ï¼Œå¹¶æé«˜ç”Ÿæˆæœªæ¥æ›´å¥½å“åº”çš„èƒ½åŠ›ã€‚
- en: Is GPT model actually â€œintelligentâ€?
  id: totrans-65
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: GPTæ¨¡å‹çœŸçš„â€œæ™ºèƒ½â€å—ï¼Ÿ
- en: While GPT modelâ€™s ability to generate human-like text is impressive, is its
    understanding of language the same or fundamentally different from human understanding?
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: å°½ç®¡GPTæ¨¡å‹ç”Ÿæˆç±»äººæ–‡æœ¬çš„èƒ½åŠ›ä»¤äººå°è±¡æ·±åˆ»ï¼Œä½†å®ƒå¯¹è¯­è¨€çš„ç†è§£æ˜¯å¦ä¸äººç±»çš„ç†è§£ç›¸åŒæˆ–æœ‰æ ¹æœ¬åŒºåˆ«ï¼Ÿ
- en: '![](../Images/0fe9f4fcf8c5dce0ca73eddb65ba65e6.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0fe9f4fcf8c5dce0ca73eddb65ba65e6.png)'
- en: Image generated with Dall-E 2\. Classic idea of intelligence.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: ç”±Dall-E 2ç”Ÿæˆçš„å›¾åƒã€‚ç»å…¸çš„æ™ºèƒ½æ¦‚å¿µã€‚
- en: A [study](https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1224/reports/custom_116874134.pdf)
    from Stanford University attempt to understand whether pre-trained language models
    (PTLM) such as BERT or RobBERTa can understand definitions by finding the semantic
    relevance between contexts and definitions. The approach is that given a target
    word **w** in a context sentence **c**, the models are required to detect if a
    sentence **g** can be considered as a description of the definition of **w**.
    The PTLMs calculate the semantic similarity between the context embedding and
    the candidate definition embedding by using cosine similarity, which calculates
    the cosine of the angle between the two embeddings. The closer the cosine similarity
    is to 1, the more semantically similar the two embeddings. The studies found that
    PTLM struggles to understand when the definitions are abstract.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€é¡¹æ¥è‡ªæ–¯å¦ç¦å¤§å­¦çš„[ç ”ç©¶](https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1224/reports/custom_116874134.pdf)å°è¯•äº†è§£é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ï¼ˆPTLMï¼‰ï¼Œå¦‚BERTæˆ–RobBERTaï¼Œæ˜¯å¦èƒ½å¤Ÿé€šè¿‡æ‰¾å‡ºä¸Šä¸‹æ–‡ä¸å®šä¹‰ä¹‹é—´çš„è¯­ä¹‰ç›¸å…³æ€§æ¥ç†è§£å®šä¹‰ã€‚è¯¥æ–¹æ³•æ˜¯ï¼Œåœ¨ç»™å®šä¸Šä¸‹æ–‡å¥å­**c**ä¸­çš„ç›®æ ‡è¯**w**æ—¶ï¼Œæ¨¡å‹éœ€è¦æ£€æµ‹ä¸€ä¸ªå¥å­**g**æ˜¯å¦å¯ä»¥è¢«è§†ä¸º**w**å®šä¹‰çš„æè¿°ã€‚PTLMé€šè¿‡ä½¿ç”¨ä½™å¼¦ç›¸ä¼¼åº¦è®¡ç®—ä¸Šä¸‹æ–‡åµŒå…¥ä¸å€™é€‰å®šä¹‰åµŒå…¥ä¹‹é—´çš„è¯­ä¹‰ç›¸ä¼¼æ€§ï¼Œä½™å¼¦ç›¸ä¼¼åº¦è®¡ç®—ä¸¤ä¸ªåµŒå…¥ä¹‹é—´è§’åº¦çš„ä½™å¼¦å€¼ã€‚ä½™å¼¦ç›¸ä¼¼åº¦è¶Šæ¥è¿‘1ï¼Œä¸¤è€…çš„è¯­ä¹‰ç›¸ä¼¼æ€§è¶Šé«˜ã€‚ç ”ç©¶å‘ç°ï¼ŒPTLMåœ¨å®šä¹‰æŠ½è±¡æ—¶éš¾ä»¥ç†è§£ã€‚
- en: So, I guess the first question to address is what â€œunderstandingâ€ and â€œintelligenceâ€
    actually mean.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥ï¼Œæˆ‘æƒ³ç¬¬ä¸€ä¸ªéœ€è¦è§£å†³çš„é—®é¢˜æ˜¯â€œç†è§£â€å’Œâ€œæ™ºèƒ½â€å®é™…ä¸Šæ˜¯ä»€ä¹ˆæ„æ€ã€‚
- en: 'In this regard, the authors of [On the Dangers of Stochastic Parrots: Can Language
    Models Be Too Big?](https://dl.acm.org/doi/10.1145/3442188.3445922) argue that
    **the text, which is generated by large language models, lacks actual meaning
    and intention.**'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™æ–¹é¢ï¼Œ[ã€Šéšæœºé¹¦é¹‰çš„å±é™©ï¼šè¯­è¨€æ¨¡å‹æ˜¯å¦è¿‡å¤§ï¼Ÿã€‹](https://dl.acm.org/doi/10.1145/3442188.3445922) çš„ä½œè€…è®¤ä¸º**ç”±å¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆçš„æ–‡æœ¬ç¼ºä¹å®é™…çš„æ„ä¹‰å’Œæ„å›¾**ã€‚
- en: Traditionally, when two people engage in a conversation, they both try to understand
    each otherâ€™s beliefs, knowledge, experiences, and perspectives. The words we choose
    to use and **what we say depend on our mental â€œpictureâ€ or understanding of the
    other person**. Whether it is a child or an adult, a colleague or a family member,
    a partner, or someone we just met at the bus stop, we have certain assumptions
    about their thoughts and characteristics. Similarly, when we listen to someone
    else speaking, we automatically place their words in context based on our mental
    understanding of them, including their beliefs, knowledge, understanding, and
    intentions.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: ä¼ ç»Ÿä¸Šï¼Œå½“ä¸¤ä¸ªäººè¿›è¡Œå¯¹è¯æ—¶ï¼Œä»–ä»¬éƒ½ä¼šå°½åŠ›ç†è§£å¯¹æ–¹çš„ä¿¡ä»°ã€çŸ¥è¯†ã€ç»éªŒå’Œè§‚ç‚¹ã€‚æˆ‘ä»¬é€‰æ‹©ä½¿ç”¨çš„è¯è¯­å’Œ**æˆ‘ä»¬æ‰€è¯´çš„å†…å®¹å–å†³äºæˆ‘ä»¬å¯¹å¯¹æ–¹çš„å¿ƒç†â€œå›¾æ™¯â€æˆ–ç†è§£**ã€‚æ— è®ºæ˜¯å­©å­è¿˜æ˜¯æˆäººï¼ŒåŒäº‹è¿˜æ˜¯å®¶åº­æˆå‘˜ï¼Œä¼´ä¾£ï¼Œè¿˜æ˜¯æˆ‘ä»¬åœ¨å…¬äº¤è½¦ç«™åˆšè®¤è¯†çš„äººï¼Œæˆ‘ä»¬å¯¹ä»–ä»¬çš„æ€æƒ³å’Œç‰¹å¾éƒ½æœ‰ä¸€å®šçš„å‡è®¾ã€‚åŒæ ·ï¼Œå½“æˆ‘ä»¬å¬åˆ«äººè®²è¯æ—¶ï¼Œæˆ‘ä»¬ä¼šæ ¹æ®å¯¹ä»–ä»¬çš„å¿ƒç†ç†è§£ï¼ŒåŒ…æ‹¬ä»–ä»¬çš„ä¿¡ä»°ã€çŸ¥è¯†ã€ç†è§£å’Œæ„å›¾ï¼Œè‡ªåŠ¨å°†ä»–ä»¬çš„è¯è¯­ç½®äºèƒŒæ™¯ä¸­ã€‚
- en: '![](../Images/a169efa88221b1cb4bcf7d50bcc6c999.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a169efa88221b1cb4bcf7d50bcc6c999.png)'
- en: Photo by [Priscilla Du Preez](https://unsplash.com/@priscilladupreez?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
    on [Unsplash](https://unsplash.com/photos/nF8xhLMmg0c?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ç‰‡æ¥æºï¼š[Priscilla Du Preez](https://unsplash.com/@priscilladupreez?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
    äº [Unsplash](https://unsplash.com/photos/nF8xhLMmg0c?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
- en: '**When a language model generates text, it doesnâ€™t have any intention to communicate
    or consider the beliefs or thoughts of the person reading it**. The training data
    used to create the model didnâ€™t involve interacting with listeners or understanding
    their perspectives. Essentially, the language model can not understand and engage
    in meaningful communication as humans do.'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '**å½“è¯­è¨€æ¨¡å‹ç”Ÿæˆæ–‡æœ¬æ—¶ï¼Œå®ƒæ²¡æœ‰ä»»ä½•æ²Ÿé€šçš„æ„å›¾ï¼Œä¹Ÿä¸ä¼šè€ƒè™‘é˜…è¯»è€…çš„ä¿¡ä»°æˆ–æ€æƒ³**ã€‚ç”¨äºåˆ›å»ºæ¨¡å‹çš„è®­ç»ƒæ•°æ®å¹¶æœªæ¶‰åŠä¸å¬ä¼—äº’åŠ¨æˆ–ç†è§£ä»–ä»¬çš„è§‚ç‚¹ã€‚å®é™…ä¸Šï¼Œè¯­è¨€æ¨¡å‹ä¸èƒ½åƒäººç±»ä¸€æ ·ç†è§£å’Œè¿›è¡Œæœ‰æ„ä¹‰çš„æ²Ÿé€šã€‚'
- en: Our human brains however are so influenced by our language that we interpret
    our communication with large language models as if they are trying to convey meaningful
    intent. And if one side of the communication lacks actual meaning, our understanding
    of deeper/inferred meaning becomes an illusion.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶è€Œï¼Œæˆ‘ä»¬çš„äººè„‘å—åˆ°è¯­è¨€çš„å½±å“å¾ˆå¤§ï¼Œæˆ‘ä»¬å°†ä¸å¤§å‹è¯­è¨€æ¨¡å‹çš„æ²Ÿé€šè§£é‡Šä¸ºå®ƒä»¬è¯•å›¾ä¼ è¾¾æœ‰æ„ä¹‰çš„æ„å›¾ã€‚å¦‚æœæ²Ÿé€šçš„ä¸€æ–¹ç¼ºä¹å®é™…æ„ä¹‰ï¼Œæˆ‘ä»¬å¯¹æ›´æ·±å±‚æ¬¡/æ¨æ–­æ„ä¹‰çš„ç†è§£å°±å˜æˆäº†ä¸€ç§å¹»è§‰ã€‚
- en: Looking at how the GPT model was built, it is obvious that **these language
    models do not process information and generate text consciously**. It was all
    a prediction game that foresees which words are coming next based on the patterns
    the models have seen from the training data. In addition, large language model
    like ChatGPT sometimes writes plausible-sounding but incorrect or nonsensical
    answers because it does not have the common sense reasoning as we â€” humans do.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: ä» GPT æ¨¡å‹çš„æ„å»ºæ–¹å¼æ¥çœ‹ï¼Œæ˜¾ç„¶**è¿™äº›è¯­è¨€æ¨¡å‹å¹¶ä¸ä»¥è‡ªè§‰çš„æ–¹å¼å¤„ç†ä¿¡æ¯å’Œç”Ÿæˆæ–‡æœ¬**ã€‚è¿™å®Œå…¨æ˜¯ä¸€ä¸ªé¢„æµ‹æ¸¸æˆï¼Œæ ¹æ®æ¨¡å‹ä»è®­ç»ƒæ•°æ®ä¸­çœ‹åˆ°çš„æ¨¡å¼æ¥é¢„æµ‹æ¥ä¸‹æ¥ä¼šå‡ºç°å“ªäº›è¯ã€‚æ­¤å¤–ï¼Œåƒ
    ChatGPT è¿™æ ·çš„è¯­è¨€æ¨¡å‹æœ‰æ—¶ä¼šå†™å‡ºçœ‹ä¼¼åˆç†ä½†å®é™…ä¸Šé”™è¯¯æˆ–æ— æ„ä¹‰çš„ç­”æ¡ˆï¼Œå› ä¸ºå®ƒå¹¶ä¸åƒæˆ‘ä»¬â€”â€”äººç±»ä¸€æ ·æ‹¥æœ‰å¸¸è¯†æ¨ç†èƒ½åŠ›ã€‚
- en: Can AI gain consciousness?
  id: totrans-78
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: äººå·¥æ™ºèƒ½èƒ½è·å¾—æ„è¯†å—ï¼Ÿ
- en: '**Human intelligence and understanding go hand in hand with consciousness**
    â€” which is the ability to feel things, pain, joy, love and angers. Consciousness
    is linked to an organic body so a natural question to ask is would it be possible
    that non-organic systems can obtain consciousness? Because we know so little about
    human consciousness, we canâ€™t rule out the possibility of computer developing
    consciousness. It is still an ongoing research, and whether AI can have consciousness
    or not, we might see in the coming decades. But perhaps one piece of good news
    is that at least for now we wonâ€™t have to yet deal with the science-fiction nightmare
    of AI obtaining consciousness and deciding to enslave and wipe out human :).'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '**äººç±»æ™ºèƒ½å’Œç†è§£ä¸æ„è¯†æ˜¯å¯†ä¸å¯åˆ†çš„**â€”â€”æ„è¯†æ˜¯æ„ŸçŸ¥äº‹ç‰©ã€ç—›è‹¦ã€å¿«ä¹ã€çˆ±å’Œæ„¤æ€’çš„èƒ½åŠ›ã€‚æ„è¯†ä¸æœ‰æœºä½“ç›¸å…³è”ï¼Œå› æ­¤è‡ªç„¶ä¼šæœ‰ä¸€ä¸ªé—®é¢˜ï¼šéæœ‰æœºç³»ç»Ÿæ˜¯å¦å¯èƒ½è·å¾—æ„è¯†ï¼Ÿå› ä¸ºæˆ‘ä»¬å¯¹äººç±»æ„è¯†çŸ¥ä¹‹ç”šå°‘ï¼Œæ‰€ä»¥ä¸èƒ½æ’é™¤è®¡ç®—æœºå‘å±•æ„è¯†çš„å¯èƒ½æ€§ã€‚è¿™ä»ç„¶æ˜¯ä¸€ä¸ªæ­£åœ¨è¿›è¡Œçš„ç ”ç©¶ï¼Œæ— è®ºAIæ˜¯å¦èƒ½å¤Ÿæ‹¥æœ‰æ„è¯†ï¼Œæˆ‘ä»¬å¯èƒ½åœ¨æœªæ¥å‡ åå¹´ä¸­ä¼šçœ‹åˆ°ã€‚ä¸è¿‡ï¼Œä¹Ÿè®¸æœ‰ä¸€ä¸ªå¥½æ¶ˆæ¯æ˜¯ï¼Œè‡³å°‘ç›®å‰æˆ‘ä»¬ä¸å¿…é¢å¯¹ç§‘å¹»å°è¯´ä¸­çš„å™©æ¢¦â€”â€”AIè·å¾—æ„è¯†å¹¶å†³å®šå¥´å½¹å’Œæ¶ˆç­äººç±»çš„æƒ…æ™¯
    :).'
- en: I think there is still a long way to go before machine intelligence becomes
    similar to human intelligence. â€œIt is no longer in the realm of science fiction
    to imagine AI systems having feelings and even human-level consciousness,â€ says
    the [BBC](https://www.bbc.com/news/technology-65401783). Most experts agree that
    AI is nowhere near this level of sophistication but it is evolving at lightning
    speed and it is difficult to imagine how it will look like in the next few years.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘è®¤ä¸ºï¼Œæœºå™¨æ™ºèƒ½è¦è¾¾åˆ°ç±»ä¼¼äºäººç±»æ™ºèƒ½çš„æ°´å¹³è¿˜éœ€è¦å¾ˆé•¿æ—¶é—´ã€‚â€œæƒ³è±¡AIç³»ç»Ÿæ‹¥æœ‰æƒ…æ„Ÿç”šè‡³äººç±»æ°´å¹³çš„æ„è¯†å·²ç»ä¸å†æ˜¯ç§‘å¹»å°è¯´ä¸­çš„åœºæ™¯ï¼Œâ€[BBC](https://www.bbc.com/news/technology-65401783)è¯´é“ã€‚å¤§å¤šæ•°ä¸“å®¶åŒæ„ï¼ŒAIç¦»è¿™ç§å¤æ‚ç¨‹åº¦è¿˜å¾ˆé¥è¿œï¼Œä½†å®ƒæ­£ä»¥é—ªç”µèˆ¬çš„é€Ÿåº¦å‘å±•ï¼Œéš¾ä»¥æƒ³è±¡æœªæ¥å‡ å¹´ä¼šæ˜¯ä»€ä¹ˆæ ·å­ã€‚
- en: What I believe is that we can say with reasonable confidence that current artificial
    intelligence models have developed a set of skills that are closely related to
    certain aspects of human intelligence. It might be the time for AI creators to
    invest more in understanding consciousness.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ç›¸ä¿¡æˆ‘ä»¬å¯ä»¥ç›¸å½“æœ‰ä¿¡å¿ƒåœ°è¯´ï¼Œå½“å‰çš„äººå·¥æ™ºèƒ½æ¨¡å‹å·²ç»å‘å±•å‡ºä¸€å¥—ä¸äººç±»æ™ºèƒ½æŸäº›æ–¹é¢å¯†åˆ‡ç›¸å…³çš„æŠ€èƒ½ã€‚ä¹Ÿè®¸æ˜¯æ—¶å€™è®©AIåˆ›é€ è€…æ›´å¤šåœ°æŠ•èµ„äºç†è§£æ„è¯†ã€‚
- en: References
  id: totrans-82
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å‚è€ƒæ–‡çŒ®
- en: '[Introducing ChatGPT (openai.com)](https://openai.com/blog/chatgpt)'
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[ä»‹ç»ChatGPT (openai.com)](https://openai.com/blog/chatgpt)'
- en: '[How ChatGPT and Our Language Models Are Developed | OpenAI Help Center](https://help.openai.com/en/articles/7842364-how-chatgpt-and-our-language-models-are-developed)'
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[ChatGPTå’Œæˆ‘ä»¬çš„è¯­è¨€æ¨¡å‹å¦‚ä½•å¼€å‘ | OpenAIå¸®åŠ©ä¸­å¿ƒ](https://help.openai.com/en/articles/7842364-how-chatgpt-and-our-language-models-are-developed)'
- en: '[The inside story of how ChatGPT was built from the people who made it | MIT
    Technology Review](https://www.technologyreview.com/2023/03/03/1069311/inside-story-oral-history-how-chatgpt-built-openai/)'
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[ChatGPTçš„å¹•åæ•…äº‹ï¼šç”±åˆ›å»ºè€…è®²è¿° | MITæŠ€æœ¯è¯„è®º](https://www.technologyreview.com/2023/03/03/1069311/inside-story-oral-history-how-chatgpt-built-openai/)'
- en: Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., â€¦
    & Amodei, D. (2020). Language models are few-shot learners. *Advances in neural
    information processing systems*, *33*, 1877â€“1901.
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., â€¦
    & Amodei, D. (2020). è¯­è¨€æ¨¡å‹æ˜¯å°‘æ ·æœ¬å­¦ä¹ è€…ã€‚*ç¥ç»ä¿¡æ¯å¤„ç†ç³»ç»Ÿçš„è¿›å±•*, *33*, 1877â€“1901ã€‚
- en: Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C., Mishkin, P., â€¦ &
    Lowe, R. (2022). Training language models to follow instructions with human feedback.
    *Advances in Neural Information Processing Systems*, *35*, 27730â€“27744.
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C., Mishkin, P., â€¦ &
    Lowe, R. (2022). é€šè¿‡äººç±»åé¦ˆè®­ç»ƒè¯­è¨€æ¨¡å‹ä»¥éµå¾ªæŒ‡ä»¤ã€‚*ç¥ç»ä¿¡æ¯å¤„ç†ç³»ç»Ÿçš„è¿›å±•*, *35*, 27730â€“27744ã€‚
- en: Radford, A., Narasimhan, K., Salimans, T., & Sutskever, I. (2018). Improving
    language understanding by generative pre-training.
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Radford, A., Narasimhan, K., Salimans, T., & Sutskever, I. (2018). é€šè¿‡ç”Ÿæˆé¢„è®­ç»ƒæé«˜è¯­è¨€ç†è§£èƒ½åŠ›ã€‚
- en: 'Bender, E. M., Gebru, T., McMillan-Major, A., & Shmitchell, S. (2021, March).
    On the dangers of stochastic parrots: Can language models be too big?ğŸ¦œ. In *Proceedings
    of the 2021 ACM conference on fairness, accountability, and transparency* (pp.
    610â€“623).'
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Bender, E. M., Gebru, T., McMillan-Major, A., & Shmitchell, S. (2021å¹´3æœˆ). å…³äºéšæœºé¹¦é¹‰çš„å±é™©ï¼šè¯­è¨€æ¨¡å‹æ˜¯å¦å¯èƒ½è¿‡å¤§ï¼ŸğŸ¦œ.
    æ”¶å½•äº*2021å¹´ACMå…¬å¹³æ€§ã€é—®è´£åˆ¶ä¸é€æ˜åº¦ä¼šè®®è®ºæ–‡é›†*ï¼ˆç¬¬610â€“623é¡µï¼‰ã€‚
- en: '[Are AIs actually intelligent?. If you have been on any social mediaâ€¦ | by
    Telmo Subira Rodriguez | Jul, 2023 | Medium](https://medium.com/@telmosubirar/are-ais-actually-intelligent-caacfb2fa7e8)'
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[äººå·¥æ™ºèƒ½çœŸçš„æ™ºèƒ½å—ï¼Ÿå¦‚æœä½ æ›¾ç»åœ¨ä»»ä½•ç¤¾äº¤åª’ä½“ä¸Šâ€¦ | ä½œè€…ï¼šTelmo Subira Rodriguez | 2023å¹´7æœˆ | Medium](https://medium.com/@telmosubirar/are-ais-actually-intelligent-caacfb2fa7e8)'
- en: Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N.,
    â€¦ & Polosukhin, I. (2017). Attention is all you need. *Advances in neural information
    processing systems*, *30*
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N.,
    â€¦ & Polosukhin, I. (2017). æ³¨æ„åŠ›æœºåˆ¶å°±æ˜¯ä½ æ‰€éœ€çš„ä¸€åˆ‡ã€‚*ç¥ç»ä¿¡æ¯å¤„ç†ç³»ç»Ÿè¿›å±•*, *30*
