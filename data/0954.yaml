- en: Running a Stable Diffusion Cluster on GCP with tensorflow-serving (Part 2)
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在GCP上使用tensorflow-serving运行稳定扩散集群（第2部分）
- en: 原文：[https://towardsdatascience.com/running-a-stable-diffusion-cluster-on-gcp-with-tensorflow-serving-part-2-c421ecb7472a?source=collection_archive---------9-----------------------#2023-03-14](https://towardsdatascience.com/running-a-stable-diffusion-cluster-on-gcp-with-tensorflow-serving-part-2-c421ecb7472a?source=collection_archive---------9-----------------------#2023-03-14)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/running-a-stable-diffusion-cluster-on-gcp-with-tensorflow-serving-part-2-c421ecb7472a?source=collection_archive---------9-----------------------#2023-03-14](https://towardsdatascience.com/running-a-stable-diffusion-cluster-on-gcp-with-tensorflow-serving-part-2-c421ecb7472a?source=collection_archive---------9-----------------------#2023-03-14)
- en: Creating the artifacts and deploying the model on the cluster
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建工件并在集群上部署模型
- en: '[](https://thushv89.medium.com/?source=post_page-----c421ecb7472a--------------------------------)[![Thushan
    Ganegedara](../Images/3fabfa37132f7d3a9e7679c3b8d7e061.png)](https://thushv89.medium.com/?source=post_page-----c421ecb7472a--------------------------------)[](https://towardsdatascience.com/?source=post_page-----c421ecb7472a--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----c421ecb7472a--------------------------------)
    [Thushan Ganegedara](https://thushv89.medium.com/?source=post_page-----c421ecb7472a--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://thushv89.medium.com/?source=post_page-----c421ecb7472a--------------------------------)[![Thushan
    Ganegedara](../Images/3fabfa37132f7d3a9e7679c3b8d7e061.png)](https://thushv89.medium.com/?source=post_page-----c421ecb7472a--------------------------------)[](https://towardsdatascience.com/?source=post_page-----c421ecb7472a--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----c421ecb7472a--------------------------------)
    [Thushan Ganegedara](https://thushv89.medium.com/?source=post_page-----c421ecb7472a--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F6f0b045d5681&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frunning-a-stable-diffusion-cluster-on-gcp-with-tensorflow-serving-part-2-c421ecb7472a&user=Thushan+Ganegedara&userId=6f0b045d5681&source=post_page-6f0b045d5681----c421ecb7472a---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----c421ecb7472a--------------------------------)
    ·14 min read·Mar 14, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fc421ecb7472a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frunning-a-stable-diffusion-cluster-on-gcp-with-tensorflow-serving-part-2-c421ecb7472a&user=Thushan+Ganegedara&userId=6f0b045d5681&source=-----c421ecb7472a---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F6f0b045d5681&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frunning-a-stable-diffusion-cluster-on-gcp-with-tensorflow-serving-part-2-c421ecb7472a&user=Thushan+Ganegedara&userId=6f0b045d5681&source=post_page-6f0b045d5681----c421ecb7472a---------------------post_header-----------)
    发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----c421ecb7472a--------------------------------)
    ·14分钟阅读·2023年3月14日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fc421ecb7472a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frunning-a-stable-diffusion-cluster-on-gcp-with-tensorflow-serving-part-2-c421ecb7472a&user=Thushan+Ganegedara&userId=6f0b045d5681&source=-----c421ecb7472a---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc421ecb7472a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frunning-a-stable-diffusion-cluster-on-gcp-with-tensorflow-serving-part-2-c421ecb7472a&source=-----c421ecb7472a---------------------bookmark_footer-----------)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc421ecb7472a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frunning-a-stable-diffusion-cluster-on-gcp-with-tensorflow-serving-part-2-c421ecb7472a&source=-----c421ecb7472a---------------------bookmark_footer-----------)'
- en: In [part 1](/running-a-stable-diffusion-cluster-on-gcp-with-tensorflow-serving-part-1-4f7a8e2f66df),
    we learned how to use `terraform` to set up and manage our infrastructure conveniently.
    In this part, we will continue on our journey to deploy a running Stable Diffusion
    model on the provisioned cluster.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [第1部分](/running-a-stable-diffusion-cluster-on-gcp-with-tensorflow-serving-part-1-4f7a8e2f66df)
    中，我们学习了如何使用`terraform`方便地设置和管理基础设施。在这一部分中，我们将继续我们的旅程，将运行中的稳定扩散模型部署到提供的集群上。
- en: '**Note**: You can follow this tutorial end-to-end even if you’re a free user
    (as long as you have some of free tier credits left).'
  id: totrans-9
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**注意**：即使你是免费用户，也可以完整地跟随本教程（只要你还有一些免费层积分）。'
- en: ''
  id: totrans-10
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: All images, unless otherwise noted, are by the author
  id: totrans-11
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 除非另有说明，所有图片均由作者提供
- en: 'Github: [https://github.com/thushv89/tf-serving-gke](https://github.com/thushv89/tf-serving-gke)'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 'Github: [https://github.com/thushv89/tf-serving-gke](https://github.com/thushv89/tf-serving-gke)'
- en: Let’s take a look at what the final result would be.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看最终结果会是什么。
- en: '![](../Images/4f57aa3e47f1793b54600176ea1a7c53.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4f57aa3e47f1793b54600176ea1a7c53.png)'
- en: Some images generated by the deployed Stable Diffusion model
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 部署的稳定扩散模型生成的一些图像。
- en: Preparing the model artifacts
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备模型工件
- en: What is Stable Diffusion anyway?
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 稳定扩散到底是什么？
- en: 'There are five main components that builds up the [Stable Diffusion model](https://jalammar.github.io/illustrated-stable-diffusion/):'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 构建[稳定扩散模型](https://jalammar.github.io/illustrated-stable-diffusion/)有五个主要组件：
- en: Tokenizer — Tokenizes a given string to a list of tokens (numerical IDs)
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分词器——将给定字符串分词为令牌列表（数值ID）。
- en: Text encoder — Takes in the tokenized text and produces a text embedding
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文本编码器——接受分词后的文本并生成文本嵌入。
- en: Diffusion model — Takes in the text embedding and a latent image (initially
    noise) as an input and incrementally refine the latent image to encoder more and
    more useful information (visually pleasing)
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 扩散模型——接受文本嵌入和潜在图像（最初是噪声）作为输入，并逐步优化潜在图像以编码越来越多有用的信息（视觉上令人愉悦）。
- en: Decoder — Takes in the final latent image and produces an actual image
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 解码器——接受最终的潜在图像并生成实际图像。
- en: Image encoder (used for the in-painting feature — we’ll be ignoring this for
    this exercise)
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图像编码器（用于修复功能——在本练习中我们将忽略这一点）。
- en: The principal ground-shattering idea behind stable diffusion (diffusion models)
    is,
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 稳定扩散（扩散模型）的核心突破性理念是，
- en: If you add a bit of noise to an image gradually for many steps, you will end
    up with an image containing noise. By reversing the order of the process, you
    can have an input (noise) and a target (original image). Then a model is trained
    to predict the original image from noise.
  id: totrans-25
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 如果你在多次步骤中逐渐向图像添加一点噪声，最后你会得到一个包含噪声的图像。通过反转这个过程，你可以得到一个输入（噪声）和一个目标（原始图像）。然后训练一个模型从噪声中预测原始图像。
- en: All of the above components work cohesively to achieve this idea.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 上述所有组件协同工作以实现这一理念。
- en: Storing the Stable Diffusion model
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 存储稳定扩散模型
- en: 'Code: [https://github.com/thushv89/tf-serving-gke/blob/master/notebooks/savedmodel_stable_diffusion.ipynb](https://github.com/thushv89/tf-serving-gke/blob/master/notebooks/savedmodel_stable_diffusion.ipynb)'
  id: totrans-28
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 代码：[https://github.com/thushv89/tf-serving-gke/blob/master/notebooks/savedmodel_stable_diffusion.ipynb](https://github.com/thushv89/tf-serving-gke/blob/master/notebooks/savedmodel_stable_diffusion.ipynb)
- en: In order to construct a [Stable Diffusion model](https://jalammar.github.io/illustrated-stable-diffusion/),
    we’ll be using the `keras_cv` library, which includes a collection of popular
    deep learning vision models for image classification, segmentation, generative
    AI, etc. You can find a tutorial [here](https://keras.io/guides/keras_cv/generate_images_with_stable_diffusion/),
    which explains how to use the `StableDiffusion` in `keras_cv`. You can open up
    a notebook and play with the model to familiarize yourself.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 为了构建[稳定扩散模型](https://jalammar.github.io/illustrated-stable-diffusion/)，我们将使用`keras_cv`库，该库包括用于图像分类、分割、生成AI等的流行深度学习视觉模型集合。你可以在[这里](https://keras.io/guides/keras_cv/generate_images_with_stable_diffusion/)找到一个教程，讲解如何在`keras_cv`中使用`StableDiffusion`。你可以打开一个笔记本并与模型一起玩以熟悉它。
- en: Our goal here is to save the `StableDiffusion`model in the `SavedModel` format;
    the go-to standard for serializing TensorFlow models. One crucial requirement
    to do this is making sure all operations used are TensorFlow graph compatible.
    Unfortunately, this is not the case.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的目标是将`StableDiffusion`模型保存为`SavedModel`格式；这是序列化TensorFlow模型的标准方法。做到这一点的一个关键要求是确保所有使用的操作都是TensorFlow图兼容的。不幸的是，情况并非如此。
- en: The current version of the model uses a TensorFlow graph incompatible tokenizer,
    so it needs to be brought out of the packaged model and used in a separate step.
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当前版本的模型使用与TensorFlow图不兼容的分词器，因此需要将其从打包模型中提取出来，并在单独的步骤中使用。
- en: The current version uses `predict_on_batch` in order to generate an image, which
    is not supported by TensorFlow graph building.
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当前版本使用`predict_on_batch`来生成图像，但TensorFlow图构建不支持此功能。
- en: Fixing the model
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 修正模型
- en: In order to patch up the eager mode `StableDiffusion` model, we’ll create a
    new model called `StableDiffusionNoTokenizer`. Through out this new model, we’ll
    replace all `predict_on_batch()` calls with graph compatible `__call__()` calls.
    We’ll also be decoupling the tokenization process from the model as the name suggests.
    Additionally, in the `generate_image()` function, we’ll be replacing,
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 为了修补急切模式的 `StableDiffusion` 模型，我们将创建一个名为 `StableDiffusionNoTokenizer` 的新模型。通过这个新模型，我们将用图形兼容的
    `__call__()` 替换所有 `predict_on_batch()` 调用。正如名字所示，我们还将把标记化过程与模型解耦。此外，在 `generate_image()`
    函数中，我们将替换，
- en: '[PRE0]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: with,
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 与，
- en: '[PRE1]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: where,
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，
- en: '[PRE2]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Two main changes I’ve done are:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 我做的两个主要更改是：
- en: Instead of a Python `for` loop, I’ve used the `tf.while_loop` which is more
    performant in TensorFlow.
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我使用了 `tf.while_loop` 代替 Python `for` 循环，因为在 TensorFlow 中它的性能更优。
- en: Combined the two separate calls to the `diffusion_model` to a single call and
    later split the outputs.
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将两个独立的 `diffusion_model` 调用合并为一个调用，然后再拆分输出。
- en: There are other changes such as replacing various operations with TensorFlow
    equivalent (e.g. `np.clip()` -> `tf.clip_by_value()`), you can compare and contrast
    the [original model](https://github.com/keras-team/keras-cv/blob/master/keras_cv/models/stable_diffusion/stable_diffusion.py#L43),
    with [this version](https://github.com/thushv89/tf-serving-gke/blob/master/notebooks/savedmodel_stable_diffusion.ipynb)
    to compare and contrast.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 还有其他更改，如用 TensorFlow 等效函数替换各种操作（例如 `np.clip()` -> `tf.clip_by_value()`），你可以对比
    [原始模型](https://github.com/keras-team/keras-cv/blob/master/keras_cv/models/stable_diffusion/stable_diffusion.py#L43)
    和 [此版本](https://github.com/thushv89/tf-serving-gke/blob/master/notebooks/savedmodel_stable_diffusion.ipynb)
    来进行比较。
- en: When working with TensorFlow in the graph execution mode, you can use `tf.print()`
    statements in order to ensure the validity of the code during execution. Please
    refer to the Appendix for more information about `tf.print()`.
  id: totrans-44
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 在 TensorFlow 的图执行模式下，你可以使用 `tf.print()` 语句以确保代码在执行过程中的有效性。有关 `tf.print()` 的更多信息，请参考附录。
- en: Once the underlying model is fixed, we can create the following model, which
    can be executed in graph mode without a hiccup.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦底层模型修复完成，我们可以创建以下模型，该模型可以在图模式下无缝执行。
- en: '[PRE3]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'This model takes in the following inputs:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型接受以下输入：
- en: '`input_tokens`: Tokenized representation of the input string'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_tokens`: 输入字符串的标记化表示'
- en: '`negative_prompt_tokens`: Tokenized representation of the negative prompt (more
    about negative prompting: here)'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`negative_prompt_tokens`: 负面提示的标记化表示（有关负面提示的更多信息：这里）'
- en: '`num_steps`: Number of steps to run the diffusion process for'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_steps`: 执行扩散过程的步骤数'
- en: '`batch_size`: Number of images to generate per image'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`batch_size`: 每张图片生成的图片数量'
- en: 'Here’s an example usage of this model:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 这是这个模型的一个使用示例：
- en: '[PRE4]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Remember that, I’m (i.e. Free user tier) heavily restricted by quotas on this
    project.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，我（即免费用户等级）在这个项目中受到配额的严重限制。
- en: No GPU quota at all
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 完全没有 GPU 配额
- en: Max 8 N2 CPUs (If you choose N1 CPUs, you can go up to 12)
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最多 8 个 N2 CPU（如果选择 N1 CPU，可以达到 12 个）
- en: Therefore, I cannot use any GPU instances or more than 2 `n2-standard-4`instances.
    Stable Diffusion models are quite slow therefore we’ll be challenged by latency
    using CPU instances.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我不能使用任何 GPU 实例或超过 2 个 `n2-standard-4` 实例。由于稳定扩散模型较慢，因此使用 CPU 实例时我们将面临延迟问题。
- en: Here are some details about how long it takes under different parameters. The
    tests were don on a `n2-standard-8` machine on [Vertex AI workbench](https://cloud.google.com/vertex-ai-workbench).
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是不同参数下所需时间的详细信息。测试是在 `n2-standard-8` 机器上，在 [Vertex AI workbench](https://cloud.google.com/vertex-ai-workbench)
    上进行的。
- en: Image size (`num_steps = 40`)
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图像大小（`num_steps = 40`）
- en: '— 512x512 image: 474s'
  id: totrans-60
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: — 512x512 图像：474s
- en: '— 384x384 image: 233s'
  id: totrans-61
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: — 384x384 图像：233s
- en: '`batch_size` and `num_steps`'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`batch_size` 和 `num_steps`'
- en: '— `batch size = 1`: 21.6s (`num_steps=5`), 67.7s (`num_steps=20`) and 99.5s
    (`num_steps=30`)'
  id: totrans-63
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: — `batch size = 1`：21.6s（`num_steps=5`），67.7s（`num_steps=20`）和 99.5s（`num_steps=30`）
- en: — `batch size = 2`, 55.6s (`num_steps=5`), 121.1s (`num_steps=20`) and 180.2s
    (`num_steps=30`)
  id: totrans-64
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: — `batch size = 2`，55.6s（`num_steps=5`），121.1s（`num_steps=20`）和 180.2s（`num_steps=30`）
- en: — `batch size=4`, 21.6s (`num_steps=5`), 67.7s (`num_steps=20`) and 99.5s (`num_steps=30`)
  id: totrans-65
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: — `batch size=4`，21.6s（`num_steps=5`），67.7s（`num_steps=20`）和 99.5s（`num_steps=30`）
- en: As you can see, increasing the `image_size` , `batch_size` , `num_steps` lead
    to increased time consumption. Therefore balancing the computational cost with
    the image quality, we chose the following parameters for our deployed model.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，增加 `image_size`、`batch_size` 和 `num_steps` 会导致时间消耗增加。因此，在平衡计算成本和图像质量后，我们为部署的模型选择了以下参数。
- en: '`image_size`: `384x384`'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`image_size`: `384x384`'
- en: '`num_steps`: `30`'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_steps`: `30`'
- en: '`batch_size`: `1`'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`batch_size`: `1`'
- en: Once the model is created, upload the model to the created GCS bucket.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 模型创建后，将模型上传到创建的 GCS 存储桶中。
- en: '[PRE5]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: This will be the data source we’ll be using in order to deploy our model as
    a prediction service.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 这将是我们用来将模型部署为预测服务的数据源。
- en: Let’s again take a moment to appreciate some images generated by the model,
    before continuing on to the next section.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们再次欣赏一些模型生成的图像，然后继续下一个部分。
- en: '![](../Images/f665634a061194fc8674cc778a31ab4e.png)![](../Images/ce241c77d0e034e567fa853ae515e930.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f665634a061194fc8674cc778a31ab4e.png)![](../Images/ce241c77d0e034e567fa853ae515e930.png)'
- en: Images generated by the deployed model
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 部署模型生成的图像
- en: Deploying and serving up the model
  id: totrans-76
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 部署和提供模型
- en: 'Code: [https://github.com/thushv89/tf-serving-gke/tree/master/infrastrcture](https://github.com/thushv89/tf-serving-gke/tree/master/infrastrcture)'
  id: totrans-77
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '代码: [https://github.com/thushv89/tf-serving-gke/tree/master/infrastrcture](https://github.com/thushv89/tf-serving-gke/tree/master/infrastrcture)'
- en: 'To deploy our model and setup a prediction service, we need 3 configurations:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 要部署我们的模型并设置预测服务，我们需要 3 个配置：
- en: '`configmap.yaml` — This defines various variables that are required during
    the deployment. For example this would encompass the location of the SavedModel
    on GCS (i.e. accessible through the environment variable `MODEL_PATH`)'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`configmap.yaml` — 这定义了部署过程中所需的各种变量。例如，这将包括在 GCS 上保存模型的位置（即通过环境变量 `MODEL_PATH`
    访问）。'
- en: '`deployment.yaml` — [Deployment](https://kubernetes.io/docs/concepts/workloads/controllers/deployment/)
    defines the pod specifications (e.g. CPU) and containers it should be running.
    In this case, we’ll be running a single container running `tensorflow-serving`
    serving the model located at `MODEL_PATH` .'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`deployment.yaml` — [Deployment](https://kubernetes.io/docs/concepts/workloads/controllers/deployment/)
    定义了 Pod 的规格（例如 CPU）和应该运行的容器。在这种情况下，我们将运行一个单独的容器，运行 `tensorflow-serving` 来提供位于
    `MODEL_PATH` 的模型。'
- en: '`service.yaml` — [Service](https://kubernetes.io/docs/concepts/services-networking/service/)
    is the mechanism with which we expose our `tensorflow-serving` app running in
    our pods. For example we can tell it to expose our pod(s) through a load balancer.'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`service.yaml` — [Service](https://kubernetes.io/docs/concepts/services-networking/service/)
    是我们暴露在 Pod 中运行的 `tensorflow-serving` 应用的机制。例如，我们可以让它通过负载均衡器暴露我们的 Pod。'
- en: The deployment
  id: totrans-82
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 部署
- en: 'Let’s first look at the spec of the `deployment` :'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先查看 `deployment` 的规格：
- en: '[PRE6]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'We can make few interesting observations:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以做一些有趣的观察：
- en: We’re only declaring a single replica in the script, scaling will be setup separately
    in and will be controlled through an autoscaling policy
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们在脚本中只声明了一个副本，缩放将在其他地方设置，并通过自动缩放策略进行控制。
- en: We provide a `selector`, which the service will look for in a deployment to
    ensure it’s serving on the correct deployment
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们提供一个 `selector`，服务将会在部署中查找它，以确保它在正确的部署上提供服务。
- en: We expose two ports; 8501 (HTTP traffic) and 8500 (GRPC traffic)
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们暴露了两个端口：8501（HTTP 流量）和 8500（GRPC 流量）。
- en: We’ll be requesting 3 “CPU time” and 12Gi per container.
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们将为每个容器请求 3 个“CPU 时间”和 12Gi 的内存。
- en: '**Note 1**:A node will be typically running other pods necessitated by Kubernetes
    (i.e. DNS, monitoring, etc.). Therefore such factors need to be taken into account
    when stipulating compute resources for the pod. You can see that, though we have
    a 4 CPUs in a node, we only request 3 (you could request fractional CPU resources
    as well — e.g. 3.5). You can see the allocatable CPU/Memory of each node in GCP
    (GCP Console → Clusters → Nodes → Click on a node) or using `kubectl describe
    nodes`.'
  id: totrans-90
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**注意 1**: 节点通常会运行 Kubernetes 需要的其他 Pods（例如 DNS、监控等）。因此，在规定 Pod 的计算资源时需要考虑这些因素。您可以看到，尽管节点上有
    4 个 CPU，我们只请求了 3 个（您也可以请求分数的 CPU 资源 — 例如 3.5）。您可以在 GCP 上查看每个节点的可分配 CPU/内存（GCP
    控制台 → 集群 → 节点 → 单击节点）或使用 `kubectl describe nodes`。'
- en: If your node is unable to fulfill the compute resources you specify Kubernetes
    will not be able to run the pods and throw an error (e.g. PodUnschedulable).
  id: totrans-91
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 如果您的节点无法满足您指定的计算资源，Kubernetes 将无法运行 Pods 并抛出错误（例如 PodUnschedulable）。
- en: ''
  id: totrans-92
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**Note 2**: One of the most crucial arguments you need to be careful about
    is `--rest_api_timeout_in_ms=720000` . It takes about 250s to serve up a single
    request, so here, we’re setting roughly **three** times that time as the timeout,
    to account for any enqueued requests, when we send parallel requests. If you set
    this to a value that’s too small, your requests will timeout before they are complete.'
  id: totrans-93
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**注意 2**：你需要特别注意的一个关键参数是`--rest_api_timeout_in_ms=720000`。处理一个请求大约需要250秒，所以我们这里将超时时间设置为大约**三倍**的时间，以应对并行请求时任何排队的请求。如果你将其设置为过小的值，你的请求将在完成之前超时。'
- en: Defining the service
  id: totrans-94
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 定义服务
- en: Here we’re defining a `LoadBalancer` type service, where we will expose the
    `stable-diffusion` app through the GCP load balancer. In this approach, you will
    be provided with the load balancer’s IP address, where the load balancer will
    route the traffic to the pods coming on to it. Users will be making requests against
    the IP address of the load balancer.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们定义了一个`LoadBalancer`类型的服务，我们将通过GCP负载均衡器暴露`stable-diffusion`应用。在这种方法中，你将获得负载均衡器的IP地址，负载均衡器将把流量路由到到达它的副本。用户将向负载均衡器的IP地址发起请求。
- en: '[PRE7]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Autoscaling
  id: totrans-97
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 自动扩展
- en: There’s an imperative topic that we’ve been putting off; scaling up our service.
    In real world, you may need to serve thousands, millions or even billions of customers.
    In order to do so, your service needs to be able to scale up/down the number of
    nodes/pods in the cluster, based on the demand. Fortunately GCP provides a variety
    of options, from fully managed autoscaling to semi/fully user managed autoscaling.
    You can learn more about these [in this video](https://www.youtube.com/watch?v=cFhch7hozRg).
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 我们一直拖延的一个重要话题是：扩展我们的服务。在现实世界中，你可能需要服务数千、数百万甚至数十亿的客户。为了做到这一点，你的服务需要能够根据需求上下扩展集群中的节点/副本数量。幸运的是，GCP提供了多种选项，从完全托管的自动扩展到半托管/完全用户管理的自动扩展。你可以通过[这个视频](https://www.youtube.com/watch?v=cFhch7hozRg)了解更多信息。
- en: Here we’ll be using a horizontal pod autoscaler (HPA). The horizontal pod autoscaler
    will scale up the number of pods, based on some threshold you provide (e.g. CPU
    or memory usage). Here’s an example.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将使用水平副本自动扩展器（HPA）。水平副本自动扩展器将根据你提供的一些阈值（例如CPU或内存使用情况）扩展副本的数量。这是一个示例。
- en: '[PRE8]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Here we’re giving the HPA a minimum of 1, maximum of 2 pods, and asking it to
    add more pods if the average CPU across current set of pods go above 60%.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将HPA的最小副本数设置为1，最大副本数设置为2，并要求它在当前副本集的平均CPU超过60%时添加更多副本。
- en: Applying the changes
  id: totrans-102
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 应用更改
- en: We now got all the building blocks ready to start our service. Simply run the
    following commands.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在已经准备好所有的构建块来启动我们的服务。只需运行以下命令。
- en: '[PRE9]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Predicting from the served model
  id: totrans-105
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从服务模型中预测
- en: In order to predict you simply need to make a POST request to the correct URL,
    with a payload containing the input to the model.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进行预测，你只需向正确的URL发起一个POST请求，负载中包含模型的输入。
- en: Sequential predictions
  id: totrans-107
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 顺序预测
- en: As the first example, we show how you can make a series of requests one after
    the other.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 作为第一个示例，我们展示了如何一个接一个地发起一系列请求。
- en: '[PRE10]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: This took over 1600s when I ran the experiment. As you might imagine, this setup
    is quite inefficient, and is unable to leverage the cluster’s ability to scale
    up.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 当我运行实验时，这花费了超过1600秒。正如你想象的，这种设置相当低效，无法利用集群的扩展能力。
- en: Parallel predictions
  id: totrans-111
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 并行预测
- en: You can use Python’s multiprocessing library to make parallel requests, which
    is more remnant of real-world user requests.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用Python的多处理库来进行并行请求，这更贴近真实用户请求的情况。
- en: '[PRE11]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: This ran in 900s. Therefore, we have a ~180% speed up, by scaling up the cluster
    to a maximum of 2 pods.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 这运行了900秒。因此，通过将集群扩展到最多2个副本，我们实现了约180%的加速。
- en: '**Note about setting timeouts**'
  id: totrans-115
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**关于设置超时的说明**'
- en: Be careful when setting up the parallel requests. If you send the parallel requests
    all at once (since this is only 6 requests), **they will likely timeout**. This
    is because it takes time to create a new node and initialize a new pod. So if
    all requests are made instantly, the load balancer might not even have time to
    see the 2nd node and end up trying to serve all requests to the single node.
  id: totrans-116
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 在设置并行请求时要小心。如果你一次性发送所有并行请求（因为这只有6个请求），**它们可能会超时**。这是因为创建新节点和初始化新副本需要时间。所以如果所有请求瞬间发出，负载均衡器可能甚至没有时间看到第二个节点，最终会尝试将所有请求服务于单个节点。
- en: Your timeout defined above is counted from the time the request is received
    (i.e. enter the `*tensorflow-serving*` queue), not from the time it starts serving
    the request. So if the request waits too long in the queue that also counts for
    the timeout.
  id: totrans-117
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 上述定义的超时时间是从请求接收的时间（即进入`*tensorflow-serving*`队列）开始计算的，而不是从开始处理请求的时间开始计算。因此，如果请求在队列中等待时间过长，也会计入超时。
- en: You can monitor the compute metrics such as the CPU usage and memory consumption
    on GCP (GCP → Kubernetes Engine → Services & Ingress → Select your service).
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在 GCP 上监控计算指标，如 CPU 使用率和内存消耗（GCP → Kubernetes Engine → Services & Ingress
    → 选择你的服务）。
- en: '![](../Images/a93688cd809d2f3f3fb3ee6a342fc3c2.png)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a93688cd809d2f3f3fb3ee6a342fc3c2.png)'
- en: Usage graph for sequential requests (top) Usage for parallel requests (bottom)
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 顺序请求的使用图（上）并行请求的使用图（下）
- en: Conclusion
  id: totrans-121
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: In this 2 part tutorial, we,
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个两部分的教程中，我们，
- en: Setup the infrastructure using terraform (an IaaS tool), which mainly consisted
    of a cluster and a node-pool (Part 1)
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用terraform（一个IaaS工具）设置基础设施，主要包括一个集群和一个节点池（第1部分）
- en: Deployed a model and created a prediction service to serve user requests using
    a Stable Diffusion model (Part 2)
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 部署了一个模型，并创建了一个预测服务来处理用户请求，使用了一个Stable Diffusion模型（第2部分）
- en: We setup this tutorial in a way that it can be run by even a free tier user.
    We setup a cluster with 2 nodes and created 1 pod per node. Then we made both
    sequential and parallel predictions and saw that parallel predictions lead to
    ~180% gains in higher throughput.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 我们设置了这个教程，使得即使是免费用户也能运行。我们设置了一个包含2个节点的集群，并为每个节点创建了1个pod。然后我们进行了顺序和并行预测，发现并行预测带来了约180%的吞吐量提升。
- en: 'Next steps:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步：
- en: '[Model warmup](https://www.tensorflow.org/tfx/serving/serving_config) — `tensorflow-serving`
    offers an easy way to warm up a model. You can parse example requests and they
    will be loaded and sent to the model, prior to serving actual user requests. This
    will reduce the latency for the initial user requests.'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[模型预热](https://www.tensorflow.org/tfx/serving/serving_config) — `tensorflow-serving`
    提供了一种简单的方法来预热模型。你可以解析示例请求，它们将被加载并发送到模型中，实际处理用户请求之前。这将减少初始用户请求的延迟。'
- en: '[Dynamic batching](https://www.tensorflow.org/tfx/serving/serving_config#batching_configuration)
    of requests — You can choose to dynamically batch the incoming requests. This
    will allow the model to make predictions on a batch of inputs than predicting
    on each input. Given enough memory, this will likely to provide throughput gains,
    allowing you to serve lot of requests within reasonable time bounds.'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[动态批处理](https://www.tensorflow.org/tfx/serving/serving_config#batching_configuration)
    请求 — 你可以选择动态批处理传入的请求。这将允许模型对一批输入进行预测，而不是对每个输入进行预测。只要有足够的内存，这可能会提供吞吐量的提升，让你在合理的时间范围内处理大量请求。'
- en: Appendix
  id: totrans-129
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 附录
- en: Debugging within pods
  id: totrans-130
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在pods中进行调试
- en: When I’m trying to get this up and running, a painstaking issue I faced was
    running into the following wall of brick.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 当我尝试启动它时，遇到的一个痛苦问题是遇到了以下砖墙。
- en: '![](../Images/4e40e615e61954e8f711aaa3d0525c6d.png)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4e40e615e61954e8f711aaa3d0525c6d.png)'
- en: The errors that were shown in the Workloads → Deployment section
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Workloads → Deployment 部分显示的错误
- en: and when I go into one of the pods in the deployment, I get a more sensible
    (still inconspicuous) error. But it was still inadequate to put a finger on what
    exactly was wrong.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 当我进入部署中的一个pod时，我得到了一个更合理的（仍然不显眼的）错误。但仍然不足以明确指出到底哪里出了问题。
- en: '![](../Images/d6c31012d4b7079d9bf28cfcbf84b2c5.png)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d6c31012d4b7079d9bf28cfcbf84b2c5.png)'
- en: Events produced by a single pod
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 单个pod产生的事件
- en: So I had to find a way to microscopically investigate the root cause. For that
    I first logged into the pod’s container in question,
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我必须找到一种方法来微观地调查根本原因。为此，我首先登录到相关pod的容器中，
- en: '[PRE12]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Once I’m in, all I got to do is capitalize on the “[everything is a file](https://en.wikipedia.org/wiki/Everything_is_a_file)”
    paradigm Linux thrives on. In other words, you can simply tap into a file to see
    the output/error stream of a process. For example, in my case, `tensorflow-serving`
    process had the PID 7, therefore, `/proc/7/fd/2` gives the error stream of that
    process.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我进入，就可以利用 Linux 所赖以生存的“[一切皆文件](https://en.wikipedia.org/wiki/Everything_is_a_file)”这一范式。换句话说，你可以简单地访问一个文件以查看进程的输出/错误流。例如，在我的情况下，`tensorflow-serving`
    进程的PID是7，因此，`/proc/7/fd/2` 给出了该进程的错误流。
- en: '[PRE13]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: In here, I was able to see exactly why this wasn’t kick-starting. It was because
    the container didn’t have the necessary permission to access the GCS bucket specified
    in `MODEL_PATH` .
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我能够准确地看到为什么这没有启动。这是因为容器没有必要的权限来访问`MODEL_PATH`中指定的GCS桶。
- en: Using `tf.print` for debugging
  id: totrans-142
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用`tf.print`进行调试
- en: As you know, TensorFlow offers two styles of execution; imperative and declarative.
    Since we use the `__call__()` to call the model (i.e. `self.model(<inputs>)`,
    these calls are executed as graph operations. You may know already that graph
    execution is notoriously difficult to debug, due to obscurities caused by the
    internal graph. One solution TensorFlow offers is the usage of `tf.print` statements.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所知，TensorFlow 提供了两种执行风格：命令式和声明式。由于我们使用`__call__()`来调用模型（即`self.model(<inputs>)`），这些调用作为图操作执行。你可能已经知道，图执行因内部图造成的模糊性而难以调试。TensorFlow
    提供的一种解决方案是使用`tf.print`语句。
- en: You can place `tf.print` statements in your model calls, and those print statements
    are added as operations to the graph, so you can see values of executed tensors,
    etc. allowing you to debug the code than throwing darts and hoping for the best.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在模型调用中放置`tf.print`语句，这些打印语句会作为操作添加到图中，因此你可以查看执行的张量的值等，这样你可以更好地调试代码，而不是盲目尝试。
- en: Make sure your `tf.print` statement is printing an input that appears immediately
    before the time you want it to be printed. If you add independent/dummy `tf.print`
    statements, they will not get embedded in the graph in the correct position. This
    may give you the deceptive feeling that some computation is happening very quickly,
    due to the incorrect placement of the graph.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 确保你的`tf.print`语句打印的输入出现在你希望它被打印的时间之前。如果你添加了独立/虚拟的`tf.print`语句，它们不会被正确地嵌入到图中。这可能会给你一种误导性的感觉，认为某些计算正在非常快速地进行，这是由于图的错误放置所导致的。
- en: Note about machine types
  id: totrans-146
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 关于机器类型的说明
- en: There are two main [types of machines](https://cloud.google.com/compute/docs/machine-resource#recommendations_for_machine_types)
    you can use for this exercise; `n1` and `n2`. [N2 instances use 3rd generation
    Xeon processors](https://cloud.google.com/blog/products/compute/compute-engine-n2-vms-now-available-with-intel-ice-lake)
    that are equipped special instruction sets (`AVX-512`) to speed up operations
    such as matrix multiplication. Therefore, CPU bound TensorFlow code runs faster
    on n2 machines than on n1.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个练习，你可以使用两种主要的[机器类型](https://cloud.google.com/compute/docs/machine-resource#recommendations_for_machine_types)；`n1`和`n2`。[N2
    实例使用第三代 Xeon 处理器](https://cloud.google.com/blog/products/compute/compute-engine-n2-vms-now-available-with-intel-ice-lake)，这些处理器配备了特殊的指令集（`AVX-512`），以加速诸如矩阵乘法等操作。因此，CPU
    密集型 TensorFlow 代码在 n2 机器上运行得比在 n1 上更快。
- en: Acknowledgement
  id: totrans-148
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 致谢
- en: I’d like to acknowledge the [ML Developer Programs](https://developers.google.com/community/experts)
    and the team for the GCP credits provided to make this tutorial a success.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 我想感谢[ML Developer Programs](https://developers.google.com/community/experts)及其团队提供的GCP积分，使这次教程得以成功。
