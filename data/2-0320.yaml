- en: 'Applied Reinforcement Learning IV: Implementation of DQN'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 应用强化学习 IV：DQN 的实现
- en: 原文：[https://towardsdatascience.com/applied-reinforcement-learning-iv-implementation-of-dqn-7a9cb2c12f97](https://towardsdatascience.com/applied-reinforcement-learning-iv-implementation-of-dqn-7a9cb2c12f97)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/applied-reinforcement-learning-iv-implementation-of-dqn-7a9cb2c12f97](https://towardsdatascience.com/applied-reinforcement-learning-iv-implementation-of-dqn-7a9cb2c12f97)
- en: Implementation of the DQN algorithm, and application to OpenAI Gym’s CartPole-v1
    environment
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: DQN 算法的实现，以及将其应用于 OpenAI Gym 的 CartPole-v1 环境
- en: '[](https://medium.com/@JavierMtz5?source=post_page-----7a9cb2c12f97--------------------------------)[![Javier
    Martínez Ojeda](../Images/5b5df4220fa64c13232c29de9b4177af.png)](https://medium.com/@JavierMtz5?source=post_page-----7a9cb2c12f97--------------------------------)[](https://towardsdatascience.com/?source=post_page-----7a9cb2c12f97--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----7a9cb2c12f97--------------------------------)
    [Javier Martínez Ojeda](https://medium.com/@JavierMtz5?source=post_page-----7a9cb2c12f97--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@JavierMtz5?source=post_page-----7a9cb2c12f97--------------------------------)[![哈维尔·马丁内斯·奥赫达](../Images/5b5df4220fa64c13232c29de9b4177af.png)](https://medium.com/@JavierMtz5?source=post_page-----7a9cb2c12f97--------------------------------)[](https://towardsdatascience.com/?source=post_page-----7a9cb2c12f97--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----7a9cb2c12f97--------------------------------)
    [哈维尔·马丁内斯·奥赫达](https://medium.com/@JavierMtz5?source=post_page-----7a9cb2c12f97--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----7a9cb2c12f97--------------------------------)
    ·8 min read·Jan 10, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----7a9cb2c12f97--------------------------------)
    ·8分钟阅读·2023年1月10日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/e9381b94512b701bbdf635b646b1733b.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e9381b94512b701bbdf635b646b1733b.png)'
- en: Photo by [DeepMind](https://unsplash.com/@deepmind?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由 [DeepMind](https://unsplash.com/@deepmind?utm_source=medium&utm_medium=referral)
    提供，来源于 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: If you want to read this article without a Premium Medium account, you can do
    it from this friend link :)
  id: totrans-8
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 如果你想阅读这篇文章而不使用高级 Medium 账户，可以通过这个朋友链接来阅读 :)
- en: '[https://www.learnml.wiki/applied-reinforcement-learning-iv-implementation-of-dqn/](https://www.learnml.wiki/applied-reinforcement-learning-iv-implementation-of-dqn/)'
  id: totrans-9
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '[https://www.learnml.wiki/applied-reinforcement-learning-iv-implementation-of-dqn/](https://www.learnml.wiki/applied-reinforcement-learning-iv-implementation-of-dqn/)'
- en: 'In the previous article of this series, [*Applied Reinforcement Learning III:
    Deep Q-Networks (DQN)*](https://medium.com/@JavierMtz5/applied-reinforcement-learning-iii-deep-q-networks-dqn-8f0e38196ba9),
    the Deep Q-Networks algorithm was introduced and explained, as well as the advantages
    and disadvantages of its application with respect to its predecessor: [Q-Learning](https://medium.com/towards-data-science/applied-reinforcement-learning-i-q-learning-d6086c1f437).
    In this article everything previously explained will be put into practice by applying
    DQN to a real use case. If you are not familiar with the basic concepts of the
    DQN algorithm or its rationale, I recommend you take a look at the previous article
    before continuing with this one.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '在本系列的上一篇文章中，[*应用强化学习 III: 深度 Q 网络 (DQN)*](https://medium.com/@JavierMtz5/applied-reinforcement-learning-iii-deep-q-networks-dqn-8f0e38196ba9)，介绍并解释了深度
    Q 网络算法，以及与其前身 [Q-Learning](https://medium.com/towards-data-science/applied-reinforcement-learning-i-q-learning-d6086c1f437)
    相比的优缺点。在本文中，将把之前讲解的内容付诸实践，通过将 DQN 应用到实际用例中。如果你对 DQN 算法的基本概念或其原理不熟悉，我建议你在继续阅读本文之前先查看上一篇文章。'
- en: '[](https://medium.com/@JavierMtz5/applied-reinforcement-learning-iii-deep-q-networks-dqn-8f0e38196ba9?source=post_page-----7a9cb2c12f97--------------------------------)
    [## Applied Reinforcement Learning III: Deep Q-Networks (DQN)'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@JavierMtz5/applied-reinforcement-learning-iii-deep-q-networks-dqn-8f0e38196ba9?source=post_page-----7a9cb2c12f97--------------------------------)
    [## 应用强化学习 III: 深度 Q 网络 (DQN)'
- en: Learn the behavior of the DQN algorithm step by step, as well as its improvements
    compared to previous Reinforcement…
  id: totrans-12
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 逐步学习 DQN 算法的行为，以及与之前的强化学习方法相比的改进…
- en: medium.com](https://medium.com/@JavierMtz5/applied-reinforcement-learning-iii-deep-q-networks-dqn-8f0e38196ba9?source=post_page-----7a9cb2c12f97--------------------------------)
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: medium.com](https://medium.com/@JavierMtz5/applied-reinforcement-learning-iii-deep-q-networks-dqn-8f0e38196ba9?source=post_page-----7a9cb2c12f97--------------------------------)
- en: Environment — CartPole-v1
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 环境 — CartPole-v1
- en: The simulation environment to be used will be the [**gym CartPole environment**](https://www.gymlibrary.dev/environments/classic_control/cart_pole/),
    which consists of a cart that moves along a horizontal guide with a vertical pole
    mounted on it, as shown in *Figure 1*. The objective is that, by means of the
    inertia generated by the cart in its horizontal displacements, the pole remains
    vertical as long as possible. An episode is considered successful when the pole
    remains vertical for more than 500 timesteps, and an episode is considered unsuccessful
    if the cart runs off the horizontal guide on the right or left side, or if the
    pole tilts more than 12 degrees (~0.2095 radians) with respect to the vertical
    axis.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 使用的仿真环境将是 [**gym CartPole 环境**](https://www.gymlibrary.dev/environments/classic_control/cart_pole/)，它由一个沿水平导轨移动的推车和一个竖直安装在其上的杆子组成，如*图
    1*所示。目标是通过推车在水平位移中产生的惯性，使杆子尽可能保持竖直。当杆子保持竖直超过 500 个时间步骤时，认为一集成功；如果推车在右侧或左侧偏离水平导轨，或者杆子相对于垂直轴倾斜超过
    12 度（约 0.2095 弧度），则认为一集失败。
- en: '![](../Images/fdeab548cbd53c60384ae10bdf7d5e05.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fdeab548cbd53c60384ae10bdf7d5e05.png)'
- en: '**Figure 1**. Image of the CartPole environment. Image extracted from the rendering
    of the [gym environment](https://www.gymlibrary.dev/environments/classic_control/cart_pole/)'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 1**. CartPole 环境的图像。图片摘自 [gym 环境](https://www.gymlibrary.dev/environments/classic_control/cart_pole/)的渲染'
- en: The environment has a **discrete action space**, since the DQN algorithm is
    not applicable to environments with a continuous action space, and the **state
    space is continuous**, because the main advantage of DQN over Q-Learning is the
    possibility of working with continuous or high-dimensional states, which is to
    be proved.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 环境具有**离散动作空间**，因为 DQN 算法不适用于具有连续动作空间的环境，而**状态空间是连续的**，因为 DQN 相对于 Q-Learning
    的主要优势是能够处理连续或高维状态，这一点还需要证明。
- en: Actions
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 动作
- en: '![](../Images/563e1e434fd290e3b681f283021b5d99.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/563e1e434fd290e3b681f283021b5d99.png)'
- en: Actions for the CartPole environment. Image by author
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: CartPole 环境的动作。作者提供的图像
- en: States
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 状态
- en: 'The states are represented as an array of 4 elements, where the meaning of
    each element, as well as their allowed value ranges are:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 状态表示为一个包含 4 个元素的数组，每个元素的意义及其允许的值范围如下：
- en: '![](../Images/d0952fee0aa07fbbe3e693b151cafb36.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d0952fee0aa07fbbe3e693b151cafb36.png)'
- en: States for the CartPole environment. Image by author
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: CartPole 环境的状态。作者提供的图像
- en: 'It should be noted that, although the allowed value ranges are those shown
    in the table, an episode will be terminated if:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 应注意，尽管允许的值范围如表中所示，但如果发生以下情况，则一集将被终止：
- en: The Cart position on the x axis leaves the (-2.4, 2.4) range
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cart 在 x 轴上的位置离开了 (-2.4, 2.4) 范围
- en: The Pole angle leaves the (-0.2095, 0.2095) range
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 杆子角度离开了 (-0.2095, 0.2095) 范围
- en: Rewards
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 奖励
- en: The agent receives a reward of +1 for each time step, with the intention of
    keeping the pole standing for as long as possible.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 代理在每个时间步骤中获得 +1 的奖励，旨在尽可能长时间保持杆子竖立。
- en: DQN Implementation
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: DQN 实现
- en: The pseudocode extracted from the paper presented by Mnih et al. **[1]** will
    be used as a reference to support the implementation of the algorithm.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 从 Mnih 等人**[1]** 提出的论文中提取的伪代码将作为参考以支持算法的实现。
- en: '![](../Images/17a2556d0c81cad2169e3ea55a8b7795.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/17a2556d0c81cad2169e3ea55a8b7795.png)'
- en: DQN Pseudocode. Extracted from **[1]**
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: DQN 伪代码。摘自**[1]**
- en: 1\. Initialize Replay Buffer
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1\. 初始化 Replay Buffer
- en: 'The Replay Buffer is implemented as a double-ended queue (deque), and two methods
    are created to interact with it: ***save_experience()*** and ***sample_experience_batch()***.
    ***save_experience()*** allows adding an experience to the replay buffer, while
    ***sample_experience_batch()*** randomly picks up a batch of experiences, which
    will be used to train the agent. The batch of experiences is returned as a tuple
    of ***(states, actions, rewards, next_states, terminals)***, where each element
    in the tuple corresponds to an array of {batch_size} items.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: Replay Buffer 被实现为双端队列（deque），并创建了两个方法与之交互：***save_experience()*** 和 ***sample_experience_batch()***。***save_experience()***
    允许将经验添加到重放缓冲区，而 ***sample_experience_batch()*** 随机挑选一批经验，这些经验将用于训练代理。经验批次作为一个***(states,
    actions, rewards, next_states, terminals)*** 的元组返回，其中元组中的每个元素对应一个 {batch_size}
    项的数组。
- en: '**Deques**, according to [Python’s documentation](https://docs.python.org/3/library/collections.html#collections.deque),
    support thread-safe, memory efficient appends and pops from either side of the
    deque with approximately the same O(1) performance in either direction. Python
    **lists**, on the other hand, have a complexity of O(n) when inserting or popping
    elements on the left-side of the list, which makes deques a much more efficient
    option when you need to work on the left-side elements.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '**双端队列（Deques）**，根据[Python 的文档](https://docs.python.org/3/library/collections.html#collections.deque)，支持线程安全、内存高效的从队列任一侧追加和弹出元素，性能在任一方向上大致为
    O(1)。而 **Python 列表** 在从列表的左侧插入或弹出元素时，复杂度为 O(n)，这使得双端队列在需要操作左侧元素时是一个更高效的选择。'
- en: 2\. Initialize Main and Target Neural Networks
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2\. 初始化主神经网络和目标神经网络
- en: Both the Main and Target Neural Networks are initialized with the ***create_nn()***
    method, which creates a keras model with 3 layers of 64, 64 and 2 (action size
    for CartPole environment) neurons each, and whose input is the state size of the
    environment. The loss function used is the Mean Squared Error (MSE), as stated
    in the algorithm’s peudocode (and shown in *Figure 2*), and the optimizer is Adam.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 主神经网络和目标神经网络都通过***create_nn()*** 方法初始化，该方法创建一个包含 64、64 和 2（CartPole 环境的动作大小）层的
    keras 模型，每层神经元数量分别为 64、64 和 2，输入为环境的状态大小。使用的损失函数是均方误差（MSE），如算法伪代码中所述（如*图 2*所示），优化器为
    Adam。
- en: If the DQN algorithm is used to teach an agent how to play an Atari game as
    mentioned in the [previous article](https://medium.com/towards-data-science/applied-reinforcement-learning-i-q-learning-d6086c1f437),
    the neural network should be convolutional, since for this type of training the
    states are the frames of the video game.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 如果使用 DQN 算法来教代理如何玩 Atari 游戏，如[上一篇文章](https://medium.com/towards-data-science/applied-reinforcement-learning-i-q-learning-d6086c1f437)所提到的，神经网络应为卷积神经网络，因为这种训练类型的状态是视频游戏的帧。
- en: '![](../Images/5b0c9a869249e0327a6cab00632d52fc.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5b0c9a869249e0327a6cab00632d52fc.png)'
- en: '**Figure 2**. Extracted from the DQN Pseudocode in **[1]**'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 2**。摘自 DQN 伪代码中的 **[1]**'
- en: 3\. Execute loop for each timestep, within each episode
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3\. 在每个时间步长中执行循环，每个回合内
- en: The following steps are repeated for each timestep, and form the main loop of
    the DQN agent training.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 以下步骤会在每个时间步长上重复，并形成 DQN 代理训练的主要循环。
- en: 3.1 Pick action following Epsilon-Greedy Policy
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.1 按照 Epsilon-Greedy 策略选择动作
- en: The action with the best Q-Value, which is the one with highest value on the
    output of the main neural network, is chosen with probability 1-ε, and a random
    action is chosen otherwise.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 选择具有最佳 Q-值的动作，即主神经网络输出中值最高的动作，概率为 1-ε，其余情况下选择一个随机动作。
- en: The full algorithm implementation, which is available in [my GitHub repository](https://github.com/JavierMtz5/ArtificialIntelligence),
    updates the epsilon value in each episode, making it lower and lower. This makes
    the exploration phase happen mainly at the beginning of the training, and the
    exploitation phase in the remaining episodes. The evolution of the epsilon values
    during a training are in *Figure 3*.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 完整的算法实现可以在[我的 GitHub 仓库](https://github.com/JavierMtz5/ArtificialIntelligence)中找到，它在每个回合中更新
    epsilon 值，使其逐渐变小。这使得探索阶段主要发生在训练的开始阶段，而利用阶段则发生在剩余的回合中。训练过程中 epsilon 值的演变见*图 3*。
- en: '![](../Images/2be1d80158a5c96427cab34620e87f8c.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2be1d80158a5c96427cab34620e87f8c.png)'
- en: '**Figure 3**. Epsilon decay during training. Image by author'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 3**。训练中的 epsilon 衰减。图片作者提供'
- en: 3.2 Perform action on environment and store experience in Replay Buffer
  id: totrans-50
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.2 在环境中执行动作并将经验存储到回放缓冲区
- en: The action extracted from the previous step is applied on the gym environment
    via the ***env.step()*** method, which receives the action to be applied as parameter.
    This method returns a tuple **(next_state, reward, terminated, truncated, info)**,
    from which the next state, reward and terminated fields are used together with
    the current state and the action chosen for saving the experience in the Replay
    Buffer with the ***save_experience()*** method defined before.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 从上一步提取的动作通过***env.step()*** 方法应用到 gym 环境中，该方法接收要应用的动作作为参数。此方法返回一个元组 **(next_state,
    reward, terminated, truncated, info)**，其中的下一个状态、奖励和终止字段与当前状态和选择的动作一起用于使用之前定义的***save_experience()***方法将经验保存到回放缓冲区。
- en: 3.3 Train agent
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.3 训练代理
- en: Finally, the agent is trained with the experiences stored along the episodes.
    As explained in [the previous article](https://medium.com/towards-data-science/applied-reinforcement-learning-iii-deep-q-networks-dqn-8f0e38196ba9),
    the output of the main neural network is taken as predicted value, and the target
    value is calculated from the reward and the output of the target network for the
    action with highest Q-Value on the next state. The loss is then calculated as
    the squared difference between the predicted value and the target value, and gradient
    descent is performed on the main neural network from this loss.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，智能体通过沿回合存储的经验进行训练。如[上一篇文章](https://medium.com/towards-data-science/applied-reinforcement-learning-iii-deep-q-networks-dqn-8f0e38196ba9)所解释，主神经网络的输出被视为预测值，目标值则根据奖励和目标网络在下一个状态中具有最高Q值的动作的输出计算。然后，计算预测值与目标值之间的平方差作为损失，并在主神经网络上执行梯度下降。
- en: 4\. Main Loop
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4. 主循环
- en: After having defined the behavior of the algorithm in each of the previous steps,
    as well as its implementation in code, all the pieces can be put together and
    the DQN algorithm can be built, as shown in the code below.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在定义了算法在前述步骤中的行为以及其代码实现之后，所有组件可以组合在一起，构建DQN算法，如下代码所示。
- en: 'There are certain behaviors of the code worth mentioning:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 代码中有一些行为值得提及：
- en: The ***update_target_network()*** method, which has not been mentioned in this
    article, copies the weights from the main neural network to the target neural
    network. This process is repeated every {update_rate} timesteps.
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***update_target_network()*** 方法，本文中未提及，将主神经网络的权重复制到目标神经网络。这个过程每{update_rate}时间步重复一次。'
- en: Training of the main neural network is only performed when there are enough
    experiences in the Replay Buffer to fill a batch.
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 只有当Replay Buffer中有足够的经验以填满一个批次时，才会进行主神经网络的训练。
- en: The epsilon value is reduced in each episode as long as a minimum value has
    not been reached, by multiplying the previous value by a reducing factor less
    than 1.
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 只要未达到最低值，epsilon值在每个回合中会减少，通过将前一个值乘以小于1的减少因子。
- en: Training stops when a cumulative reward of more than 499 has been reached in
    each of the last 10 episodes, as the agent is considered to have learned to perform
    the task successfully.
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当在最后10个回合中的每一回合都达到了499以上的累积奖励时，训练停止，因为此时智能体被认为已经成功学会了执行任务。
- en: Test the DQN Agent
  id: totrans-61
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 测试DQN智能体
- en: The performance of the trained DQN agent is evaluated by plotting the rewards
    obtained during training, and by running a test episode.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 训练好的DQN智能体的表现通过绘制训练期间获得的奖励图来评估，并通过运行一个测试回合来进行。
- en: The reward plot is used to see if the agent is able to make the rewards converge
    towards the maximum, or if on the contrary it is not able to make the rewards
    increase with each training episode, which would mean that the training has failed.
    As for the execution of the test episode, it is a realistic way to evaluate the
    agent, since it shows in a practical way whether the agent has really learned
    to perform the task for which he has been trained.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 奖励图用于查看智能体是否能够使奖励收敛到最大值，或者相反，如果智能体不能使奖励在每个训练回合中增加，这将意味着训练失败。至于测试回合的执行，这是评估智能体的一种现实方式，因为它以实际的方式展示了智能体是否真的学会了执行其训练的任务。
- en: Reward Plot
  id: totrans-64
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 奖励图
- en: The graph clearly shows how the rewards converge towards the maximum, despite
    the fact that in some episodes the agent fails in a few timesteps. This may occur
    because in those episodes the agent is initialized in a position that it does
    not know well, since it has not been initialized that way many times throughout
    the training. Likewise, the tendency of the rewards to maximum values is an indicator
    that the agent has trained correctly.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 图表清晰地显示了尽管在一些回合中智能体在若干时间步内失败，但奖励仍然收敛到最大值。这可能是因为在这些回合中，智能体初始化在一个不熟悉的位置，因为在训练过程中这种初始化并不多见。同样，奖励趋向最大值的趋势是智能体训练正确的一个指标。
- en: '![](../Images/6a0b6ca05c2d705221fdebdfa5787ab3.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6a0b6ca05c2d705221fdebdfa5787ab3.png)'
- en: Reward Plot. Image by author
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 奖励图。作者提供的图片
- en: Execution of Test Episode
  id: totrans-68
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 执行测试回合
- en: The agent succeeds in executing the test episode with the maximum score, 500,
    which is unequivocal proof that he has learned to perform the task perfectly,
    as the reward plot suggested.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 智能体成功地以最高分500执行了测试回合，这无疑证明了他已经完美地学会了执行任务，正如奖励图所示。
- en: '![](../Images/23faa34cb2bde64cf1068398928a1343.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/23faa34cb2bde64cf1068398928a1343.png)'
- en: Code and execution of test episode. Image by author
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 代码和测试回合的执行。图像由作者提供。
- en: Conclusion
  id: totrans-72
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: The DQN algorithm has managed to learn the task perfectly, so its application
    to environments with a continuous state space can be considered valid. Likewise,
    the algorithm manages to solve in a reasonable time the problem of working with
    a continuous and complex state space through the use of two neural networks. As
    only one of the two networks is trained, the computation time is significantly
    reduced, although it should be noted that the execution times of the algorithm
    are still long even for simple tasks, when compared to the Q-Learning algorithm.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: DQN 算法已经成功地完美地学习了任务，因此它在具有连续状态空间的环境中的应用可以被认为是有效的。同样，算法通过使用两个神经网络，在合理的时间内解决了处理连续和复杂状态空间的问题。由于只有两个网络中的一个被训练，计算时间大大减少，但值得注意的是，即使是简单任务，相比于
    Q-Learning 算法，算法的执行时间仍然较长。
- en: Regarding the execution times of the algorithm, this article shows only the
    implementation of DQN for a simple and easy-to-learn environment using dense neural
    networks, but it must be taken into account that the application of DQN in environments
    such as Atari games, which feed convolutional neural networks with images of the
    game, need much more time both to reach the convergence to the optimal Q-Values
    and to perform the forward and backpropagation processes, due to the greater complexity
    of the environment and the neural networks.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 关于算法的执行时间，本文仅展示了在简单且易于学习的环境中使用密集神经网络实现 DQN，但需要考虑的是，DQN 在如 Atari 游戏这样的环境中的应用，这些环境通过游戏图像供给卷积神经网络，需要更长的时间来达到最佳
    Q 值的收敛，以及进行前向和反向传播过程，因为环境和神经网络的复杂性更高。
- en: Finally, the limitations of using a Replay Buffer, in terms of memory cost,
    must be taken into account. Although the maximum length of the queue (deque) can
    be specified, thus avoiding excessive memory consumption, it is still a huge RAM
    consumption. This problem, however, can be solved by loading most of the Replay
    Buffer on disk, and keeping a small fraction in memory, or by using a more efficient
    buffer for the task, instead of the double-ended queue.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，必须考虑使用 Replay Buffer 的限制，特别是内存成本。虽然可以指定队列（deque）的最大长度，从而避免过度的内存消耗，但仍然会消耗大量的
    RAM。然而，这个问题可以通过将大部分 Replay Buffer 加载到磁盘上，同时将小部分保留在内存中，或者使用更高效的缓冲区来解决，而不是使用双端队列。
- en: Code
  id: totrans-76
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 代码
- en: The complete implementation of the DQN algorithm, both as Jupyter Notebook and
    Python script can be found in my [ArtificialIntelligence GitHub repository](https://github.com/JavierMtz5/ArtificialIntelligence).
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 完整实现的 DQN 算法，无论是 Jupyter Notebook 还是 Python 脚本，都可以在我的[人工智能 GitHub 仓库](https://github.com/JavierMtz5/ArtificialIntelligence)中找到。
- en: '[](https://github.com/JavierMtz5/ArtificialIntelligence?source=post_page-----7a9cb2c12f97--------------------------------)
    [## GitHub - JavierMtz5/ArtificialIntelligence'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '[## GitHub - JavierMtz5/ArtificialIntelligence](https://github.com/JavierMtz5/ArtificialIntelligence?source=post_page-----7a9cb2c12f97--------------------------------)'
- en: You can't perform that action at this time. You signed in with another tab or
    window. You signed out in another tab or…
  id: totrans-79
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 目前无法执行该操作。您在另一个标签或窗口中登录。您在另一个标签中注销了…
- en: github.com](https://github.com/JavierMtz5/ArtificialIntelligence?source=post_page-----7a9cb2c12f97--------------------------------)
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '[github.com](https://github.com/JavierMtz5/ArtificialIntelligence?source=post_page-----7a9cb2c12f97--------------------------------)'
- en: REFERENCES
  id: totrans-81
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: '**[1]** MNIH, Volodymyr, et al. Playing atari with deep reinforcement learning.
    *arXiv preprint arXiv:1312.5602*, 2013.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '**[1]** MNIH, Volodymyr 等人。使用深度强化学习玩 Atari 游戏。*arXiv 预印本 arXiv:1312.5602*，2013。'
