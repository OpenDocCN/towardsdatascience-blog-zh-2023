- en: Why Is Feature Scaling Important in Machine Learning? Discussing 6 Feature Scaling
    Techniques
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为什么特征缩放在机器学习中很重要？讨论 6 种特征缩放技术
- en: 原文：[https://towardsdatascience.com/why-is-feature-scaling-important-in-machine-learning-discussing-6-feature-scaling-techniques-2773bda5be30](https://towardsdatascience.com/why-is-feature-scaling-important-in-machine-learning-discussing-6-feature-scaling-techniques-2773bda5be30)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/why-is-feature-scaling-important-in-machine-learning-discussing-6-feature-scaling-techniques-2773bda5be30](https://towardsdatascience.com/why-is-feature-scaling-important-in-machine-learning-discussing-6-feature-scaling-techniques-2773bda5be30)
- en: Standardization, Normalization, Robust Scaling, Mean Normalization, Maximum
    Absolute Scaling and Vector Unit Length Scaling
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 标准化、归一化、鲁棒缩放、均值归一化、最大绝对缩放和向量单位长度缩放
- en: '[](https://rukshanpramoditha.medium.com/?source=post_page-----2773bda5be30--------------------------------)[![Rukshan
    Pramoditha](../Images/b80426aff64ff186cb915795644590b1.png)](https://rukshanpramoditha.medium.com/?source=post_page-----2773bda5be30--------------------------------)[](https://towardsdatascience.com/?source=post_page-----2773bda5be30--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----2773bda5be30--------------------------------)
    [Rukshan Pramoditha](https://rukshanpramoditha.medium.com/?source=post_page-----2773bda5be30--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://rukshanpramoditha.medium.com/?source=post_page-----2773bda5be30--------------------------------)[![Rukshan
    Pramoditha](../Images/b80426aff64ff186cb915795644590b1.png)](https://rukshanpramoditha.medium.com/?source=post_page-----2773bda5be30--------------------------------)[](https://towardsdatascience.com/?source=post_page-----2773bda5be30--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----2773bda5be30--------------------------------)
    [Rukshan Pramoditha](https://rukshanpramoditha.medium.com/?source=post_page-----2773bda5be30--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----2773bda5be30--------------------------------)
    ·13 min read·Aug 15, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----2773bda5be30--------------------------------)
    ·13 分钟阅读·2023年8月15日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/fd0c135d0ae75a274f8063f5a34b65b7.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fd0c135d0ae75a274f8063f5a34b65b7.png)'
- en: Photo by [Mediamodifier](https://unsplash.com/@mediamodifier?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
    on [Unsplash](https://unsplash.com/photos/TuZAl7v4TCM?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 照片由 [Mediamodifier](https://unsplash.com/@mediamodifier?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
    提供，来自 [Unsplash](https://unsplash.com/photos/TuZAl7v4TCM?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
- en: Many machine-learning algorithms need to have features on the same scale.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 许多机器学习算法需要特征在相同的尺度上。
- en: There are diffident types of feature scaling methods that we can choose in various
    scenarios. They have different (technical) names. The term ***Feature Scaling***
    simply refers to any of those methods.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在不同场景中选择不同类型的特征缩放方法。它们有不同的（技术）名称。术语***特征缩放***仅指这些方法中的任何一种。
- en: '[PRE0]'
  id: totrans-10
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Feature scaling in different scenarios
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 不同场景下的特征缩放
- en: '**Feature scaling in PCA:** In principal component analysis, PCA components
    are highly sensitive to the relative ranges of the original features, if they
    are not measured on the same scale. PCA tries to choose the components that maximize
    the variance of the data. If the maximization of various occurs due to higher
    ranges of some features, those features may tend to dominate the PCA process.
    In this case, the true variance may not be captured by the components. To avoid
    this, we generally perform feature scaling before PCA. However, there are two
    exceptions. If there is no significant difference in the scale between the features,
    for example, one feature ranges between 0 and 1 and another ranges between 0 and
    1.2, we do not need to perform feature scaling although there will be no harm
    if we do! If you perform PCA by decomposing the correlation matrix instead of
    the covariance matrix, you do not need to do feature scaling even though the features
    are not measured on the same scale.'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**主成分分析中的特征缩放：** 在主成分分析中，如果原始特征的相对范围不同，PCA 组件对这些范围非常敏感。PCA 尝试选择能最大化数据方差的组件。如果由于某些特征的范围较大而导致方差最大化，这些特征可能会主导
    PCA 过程。在这种情况下，组件可能无法捕捉真实的方差。为避免这种情况，我们通常在 PCA 之前进行特征缩放。然而，有两个例外。如果特征之间的尺度差异不显著，例如一个特征范围在
    0 和 1 之间，另一个特征范围在 0 和 1.2 之间，虽然进行特征缩放不会有害，但我们不需要执行特征缩放！如果你通过分解相关矩阵而不是协方差矩阵来进行
    PCA，即使特征不是在相同的尺度上测量的，也不需要进行特征缩放。'
- en: '**Feature scaling in k-means clustering:** One of the main assumptions in k-means
    clustering is that all features are measured on the same scale. If not, we should
    perform feature scaling. The k-means algorithm calculates the distance between
    data points. Features with a higher range may dominate the calculations and those
    calculations may not be accurate. To avoid this, we need to perform feature scaling
    before k-means. Scaling features will also improve the training speed of k-means
    models.'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**k-means聚类中的特征缩放：** k-means聚类的主要假设之一是所有特征都在相同的尺度上进行测量。如果不是，我们应该进行特征缩放。k-means算法计算数据点之间的距离。范围较高的特征可能主导计算，这些计算可能不准确。为避免这种情况，我们需要在k-means之前进行特征缩放。缩放特征还会提高k-means模型的训练速度。'
- en: '**Feature scaling in KNN and SVM algorithms:** In general, algorithms that
    calculate the distance between data points are mostly affected by the relative
    ranges of features. KNN and SVM are not exceptions. Features with a higher range
    may contribute more because of the higher range, but not because of its importance.
    We do not want to algorithm to be biased in that way. Therefore, we need to scale
    features to contribute equally to distance calculations.'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**KNN和SVM算法中的特征缩放：** 通常，计算数据点之间距离的算法大多受到特征相对范围的影响。KNN和SVM也不例外。范围较高的特征可能会由于其范围较高而贡献更多，但并不是因为其重要性。我们不希望算法在这种方式上产生偏差。因此，我们需要缩放特征，以便它们在距离计算中贡献均等。'
- en: '**Feature scaling in linear models:** The parameter values of linear models
    such as linear regression are highly dependent on the scale of the input features.
    Therefore, it is better to use the features measured on the same scale. That will
    also improve the training speed of linear models.'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**线性模型中的特征缩放：** 线性模型如线性回归的参数值高度依赖于输入特征的尺度。因此，最好使用在相同尺度上测量的特征。这也会提高线性模型的训练速度。'
- en: '**Feature scaling in neural networks:** We usually apply feature scaling methods
    to input data. But, it is also possible to apply feature scaling to the activation
    values of hidden layers in a neural network! The scaled output values then become
    the inputs to the next layers. This is called *batch normalization* which can
    effectively eliminate the vanishing gradient problem and covariate shift problem
    and enhance the stability of the network during the training. It also speeds up
    the training process of neural network models.'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**神经网络中的特征缩放：** 我们通常将特征缩放方法应用于输入数据。然而，也可以将特征缩放应用于神经网络中隐藏层的激活值！经过缩放的输出值随后成为下一层的输入。这被称为*批量归一化*，它能有效消除梯度消失问题和协变量偏移问题，并提高网络在训练过程中的稳定性。它还加快了神经网络模型的训练速度。'
- en: '**Feature scaling in the convergence of algorithms:** The learning rate is
    the main factor that decides the speed of the convergence of deep learning and
    machine learning algorithms. Feature scaling also has an effect on this! When
    the features are measured on the same scale, the calculations can be performed
    much faster and the algorithms converge faster!'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**算法收敛中的特征缩放：** 学习率是决定深度学习和机器学习算法收敛速度的主要因素。特征缩放也对这一点有影响！当特征在相同的尺度上进行测量时，计算可以更快地执行，算法也会更快地收敛！'
- en: '**Feature scaling in tree-based algorithms:** Feature scaling is not necessary
    for tree-based algorithms as they are not much sensitive to the relative scale
    of features. Popular tree-based algorithms are: Decision Tree, Random Forest,
    AdaBoost, Gradient Boosting, XGBoost, LightGBM, and CatBoot.'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基于树的算法中的特征缩放：** 对于基于树的算法，不需要特征缩放，因为它们对特征的相对尺度不太敏感。流行的基于树的算法包括：决策树、随机森林、AdaBoost、梯度提升、XGBoost、LightGBM和CatBoost。'
- en: '**Feature scaling in LDA:** Linear discriminant analysis (LDA) is a linear
    dimensionality reduction technique that performs dimensionality reduction by maximizing
    the class separability of classification datasets, not by maximizing the variance
    of the data. Therefore, LDA is not sensitive to the relative ranges of the features
    and feature scaling is not necessary for LDA.'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**LDA中的特征缩放：** 线性判别分析（LDA）是一种线性降维技术，通过最大化分类数据集的类别可分离性来进行降维，而不是通过最大化数据的方差。因此，LDA对特征的相对范围不敏感，因此不需要对LDA进行特征缩放。'
- en: Feature scaling methods
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 特征缩放方法
- en: 1\. Standardization
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1\. 标准化
- en: Standardization is the most popular feature scaling method in which we center
    values at 0 and a unit standard deviation. The result is the z-score and therefore,
    this scaling method is also known as z-score standardization or normalization.
    After applying standardization to a feature, the data has a distribution with
    a mean of 0 and a standard deviation of 1\. That kind of distribution is called
    a standard normal distribution.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 标准化是最流行的特征缩放方法，其中我们将值居中于0并使标准差为1。结果是z-score，因此这种缩放方法也称为z-score标准化或归一化。应用标准化到特征后，数据的分布具有均值0和标准差1。这样的分布称为标准正态分布。
- en: To apply standardization to a variable, first, we need to calculate the mean
    and the standard deviation of that variable. Then, we subtract the mean from each
    value and divide the result by the standard deviation. For a set of features,
    these calculations are performed feature-wise simultaneously.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 要对变量应用标准化，首先，我们需要计算该变量的均值和标准差。然后，从每个值中减去均值，并将结果除以标准差。对于一组特征，这些计算是同时在特征级别上进行的。
- en: '![](../Images/20200f7ce28d119a83b9739b47a3ad48.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/20200f7ce28d119a83b9739b47a3ad48.png)'
- en: '**Standardization formula** (Image by author)'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '**标准化公式**（图片由作者提供）'
- en: The z-score or the output of standardization practically means how many standard
    deviations a value deviates from the mean!
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: z-score或标准化的输出实际上表示一个值偏离均值多少个标准差！
- en: The z-score values are not bounded to a certain range. The standardization process
    is not affected by the presence of outliers in the data.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: z-score值不受限于某个特定范围。标准化过程不会受到数据中异常值的影响。
- en: Standardization is particularly useful when the data follows a Gaussian or normal
    distribution or the distribution is unknown.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 标准化在数据遵循高斯分布或正态分布，或分布未知时特别有用。
- en: In Scikit-learn, standardization can be performed using the **StandardScaler()**
    function.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在Scikit-learn中，可以使用**StandardScaler()**函数进行标准化。
- en: '[PRE1]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: When calling the fit() method, the mean and standard deviation of each variable
    are calculated. If we need to apply the scaling process to data, we should also
    call the transform() method.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 调用fit()方法时，会计算每个变量的均值和标准差。如果我们需要对数据应用缩放过程，也应调用transform()方法。
- en: 2\. Min-Max Scaling (Normalization)
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2\. 最小-最大缩放（归一化）
- en: The min-max scaling or normalization is the process of scaling data into a specific
    range of your choice. The most commonly used range is (0, 1).
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 最小-最大缩放或归一化是将数据缩放到您选择的特定范围的过程。最常用的范围是（0, 1）。
- en: To apply min-max scaling to a variable, first, we need to find the minimum and
    maximum values of that variable. Then, we subtract the minimum from each data
    value and divide the result by the range (the difference between the maximum and
    minimum). For a set of features, these calculations are performed feature-wise
    simultaneously.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 要对变量应用最小-最大缩放，首先，我们需要找到该变量的最小值和最大值。然后，我们从每个数据值中减去最小值，并将结果除以范围（最大值与最小值的差）。对于一组特征，这些计算是同时在特征级别上进行的。
- en: '![](../Images/ad772b292f7677ce3ce25c495643af04.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ad772b292f7677ce3ce25c495643af04.png)'
- en: '**Min-max scaling formula** (Image by author)'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '**最小-最大缩放公式**（图片由作者提供）'
- en: The min-max scaling is particularly useful when the distribution of data is
    not known or does not follow the normal distribution.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 最小-最大缩放在数据分布未知或不遵循正态分布时特别有用。
- en: The min-max scaling process is highly sensitive to the outlier in the data.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 最小-最大缩放过程对数据中的异常值非常敏感。
- en: In Scikit-learn, min-max scaling can be performed using the **MinMaxScaler()**
    function.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在Scikit-learn中，可以使用**MinMaxScaler()**函数进行最小-最大缩放。
- en: '[PRE2]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: When calling the fit() method, the minimum and maximum values of each variable
    are found. If we need to apply the scaling process to data, we should also call
    the transform() method.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 调用fit()方法时，会找到每个变量的最小值和最大值。如果我们需要对数据应用缩放过程，也应调用transform()方法。
- en: The **MinMaxScaler()** function also provides an option to change the range
    of your choice. The default is set to (0, 1).
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '**MinMaxScaler()**函数还提供了更改您选择的范围的选项。默认设置为（0, 1）。'
- en: 3\. Robust Scaling
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3\. 鲁棒缩放
- en: The robust scaling is also known as **quantile scaling** in which we scale data
    based on 1st, 2nd and 3rd quantiles. The 2nd quantile is the median as you already
    know.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 鲁棒缩放也称为**分位数缩放**，其中我们根据第1、第2和第3分位数缩放数据。第2分位数是中位数，如您所知。
- en: To apply robust scaling to a variable, first, we need to find the quantiles
    of that variable. Then, we subtract the median (2nd quantile or Q2) from each
    data value and divide the result by the IQR (the difference between 3rd and 1st
    quantiles). For a set of features, these calculations are performed feature-wise
    simultaneously.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 要对变量应用鲁棒缩放，首先需要找到该变量的分位数。然后，我们从每个数据值中减去中位数（第2个分位数或Q2），并将结果除以IQR（第3个分位数与第1个分位数之间的差异）。对于一组特征，这些计算会同时在特征层面进行。
- en: '![](../Images/961dc3dee5397083a8f3eed848ee03fd.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/961dc3dee5397083a8f3eed848ee03fd.png)'
- en: '**Robust scaling formula** (Image by author)'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '**鲁棒缩放公式**（图片由作者提供）'
- en: '![](../Images/409816a59e781bb1f4c21765290dd059.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/409816a59e781bb1f4c21765290dd059.png)'
- en: '**Robust scaling alternative formula** (Image by author)'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '**鲁棒缩放替代公式**（图片由作者提供）'
- en: The robust scaling is particularly useful when there are outliers in the data.
    This is because quantiles are robust to outliers (hence the name, *robust scaling*!).
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 当数据中存在异常值时，鲁棒缩放尤其有用。这是因为分位数对异常值具有鲁棒性（因此命名为*鲁棒缩放*）。
- en: In Scikit-learn, robust scaling can be performed using the **RobustScaler()**
    function.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在Scikit-learn中，可以使用**RobustScaler()**函数执行鲁棒缩放。
- en: '[PRE3]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: When calling the fit() method, the quantile values of each variable are found.
    If we need to apply the scaling process to data, we should also call the transform()
    method.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 调用fit()方法时，找到每个变量的分位数值。如果我们需要对数据应用缩放过程，还应调用transform()方法。
- en: 4\. Mean Normalization
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4\. 均值归一化
- en: Mean normalization is another popular feature scaling technique in which we
    subtract the mean from each data value and divide the result by the range (the
    difference between the maximum and minimum).
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 均值归一化是另一种流行的特征缩放技术，我们从每个数据值中减去均值，并将结果除以范围（最大值与最小值之间的差异）。
- en: The formula is quite similar to the min-max scaling formula, except we subtract
    the mean from each data value, instead of subtracting the minimum from each data
    value.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 这个公式与最小-最大缩放公式非常相似，只是我们从每个数据值中减去了均值，而不是从每个数据值中减去最小值。
- en: '![](../Images/e27c49cc6caa7118a2d28239cbf1db23.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e27c49cc6caa7118a2d28239cbf1db23.png)'
- en: '**Mean normalization formula** (Image by author)'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '**均值归一化公式**（图片由作者提供）'
- en: Mean normalization cannot be directly implemented in Scikit-learn as there is
    no dedicated function to do that. But, we can create a Scikit-learn pipeline by
    combing **StandardScaler()** and **RobustScaler()** transformers to perform mean
    normalization. A pipeline *sequentially* applies multiple transformers to data.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 均值归一化无法直接在Scikit-learn中实现，因为没有专门的函数来执行这一操作。但我们可以通过将**StandardScaler()**和**RobustScaler()**转换器结合创建一个Scikit-learn流水线来执行均值归一化。流水线*顺序地*将多个转换器应用于数据。
- en: '[PRE4]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: In the above code sample, I have customized both **StandardScaler()** and **RobustScaler()**
    transformers by changing their default hyperparameter values!
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在上面的代码示例中，我通过更改默认超参数值自定义了**StandardScaler()**和**RobustScaler()**转换器！
- en: '[PRE5]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: This code line does not perform typical standardization. Instead, it subtracts
    the mean from each value and divides the result by 1 because **with_std=False**.
    The output of this will become the input to the next transformer.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 这行代码并没有执行典型的标准化。相反，它从每个值中减去均值，并将结果除以1，因为**with_std=False**。这将成为下一转换器的输入。
- en: '[PRE6]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: By setting **with_centering=False**, it does not subtract the median from the
    input values. In addition to that, it divides the input values by range (the difference
    between maximum and minimum) with **quantile_range=(0, 100)**. The 0 represents
    the minimum and 100 represents the maximum.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 通过设置**with_centering=False**，不会从输入值中减去中位数。此外，它还通过**quantile_range=(0, 100)**将输入值除以范围（最大值与最小值之间的差异）。0表示最小值，100表示最大值。
- en: The pipeline applies the above-customized transformers to data sequentially.
    When the data goes through the above pipeline, the mean normalization will be
    performed!
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 流水线将上述自定义的转换器顺序地应用于数据。当数据经过上述流水线时，将执行均值归一化！
- en: 5\. Maximum Absolute Scaling
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5\. 最大绝对缩放
- en: Maximum absolute scaling can be performed by dividing every data value by the
    maximum value of the feature. Its formula is very simple as compared to previous
    methods.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 最大绝对缩放可以通过将每个数据值除以特征的最大值来执行。与之前的方法相比，它的公式非常简单。
- en: '![](../Images/2defdd59966037f875b4aa8efdc4217d.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2defdd59966037f875b4aa8efdc4217d.png)'
- en: '**Maximum absolute scaling formula** (Image by author)'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '**最大绝对缩放公式**（图片由作者提供）'
- en: The previous methods center the data by subtracting either mean, minimum or
    median from each data value. But, maximum absolute scaling does not center the
    data in that way. Therefore, this method works well with ***sparse data*** in
    whichmost of the values are zero.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的方法通过从每个数据值中减去均值、最小值或中位数来中心化数据。但最大绝对值缩放不会以这种方式中心化数据。因此，该方法适用于 ***稀疏数据***，其中大多数值为零。
- en: In Scikit-learn, maximum absolute scaling can be performed using the **MaxAbsScaler()**
    function.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Scikit-learn 中，可以使用 **MaxAbsScaler()** 函数执行最大绝对值缩放。
- en: '[PRE7]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: When calling the fit() method, the maximum values of each variable are found.
    If we need to apply the scaling process to data, we should also call the transform()
    method.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 调用 fit() 方法时，会找到每个变量的最大值。如果我们需要对数据应用缩放过程，也应该调用 transform() 方法。
- en: 6\. Vector Unit-Length Scaling
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6\. 向量单位长度缩放
- en: A unit vector has a magnitude of 1\. To convert a non-zero vector to a unit
    vector, you need to divide that vector by its length which is calculated by using
    either Manhattan distance (L1 norm) or Euclidean distance (L2 norm) of that vector.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 单位向量的大小为 1。要将非零向量转换为单位向量，需要将该向量除以其长度，该长度可以通过使用曼哈顿距离（L1 范数）或欧几里得距离（L2 范数）来计算。
- en: Now, consider the following non-zero vector.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，考虑以下非零向量。
- en: '![](../Images/0da2b1c33694d41973c2ccf48e9884c6.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0da2b1c33694d41973c2ccf48e9884c6.png)'
- en: '**Vector** (Image by author)'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '**向量**（图片来源：作者）'
- en: The length (magnitude) of this vector can be calculated by using the Manhattan
    distance (L1 norm).
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 该向量的长度（大小）可以通过曼哈顿距离（L1 范数）来计算。
- en: '![](../Images/702c37258c0ee787fb3327cad7a836a5.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/702c37258c0ee787fb3327cad7a836a5.png)'
- en: '**Manhattan distance (L1 norm) of the vector** (Image by author)'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '**向量的曼哈顿距离（L1 范数）**（图片来源：作者）'
- en: The length (magnitude) of the vector can also be calculated by using the Euclidean
    distance (L2 norm) which is the most commonly used method.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 向量的长度（大小）也可以通过使用最常用的方法，即欧几里得距离（L2 范数）来计算。
- en: '![](../Images/3a87ec302d7ab1fe174439dccc6bc7a8.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3a87ec302d7ab1fe174439dccc6bc7a8.png)'
- en: '**Euclidean distance (L2 norm) of the vector** (Image by author)'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '**向量的欧几里得距离（L2 范数）**（图片来源：作者）'
- en: To convert ***x*** to a unit vector, we need to divide it by the length which
    is a real number as shown above. This is called *normalizing the vector*. After
    normalizing, the vector becomes a unit vector of magnitude (length) 1 and has
    the same direction as ***x***.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 要将 ***x*** 转换为单位向量，我们需要将其除以长度，如上所示。这称为 *归一化向量*。归一化后，向量变成一个大小（长度）为 1 的单位向量，并且具有与
    ***x*** 相同的方向。
- en: '![](../Images/7859b9423798bb5c7496f161b151afbd.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7859b9423798bb5c7496f161b151afbd.png)'
- en: '**Unit vector formula** (Image by author)'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '**单位向量公式**（图片来源：作者）'
- en: The previous feature scaling methods performed calculations feature-wise, i.e.,
    by considering each feature across all observations. However, in unit length scaling,
    the calculations are performed observation-wise, i.e., by considering each observation
    across all features.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的特征缩放方法是按特征进行计算，即考虑所有观测值中的每个特征。然而，在单位长度缩放中，计算是按观测值进行的，即考虑所有特征中的每个观测值。
- en: To illustrate this, consider the following tabular data.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明这一点，请考虑以下表格数据。
- en: '![](../Images/25e4c98ccac57707a62025af7341ff29.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/25e4c98ccac57707a62025af7341ff29.png)'
- en: (Image by author)
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: （图片来源：作者）
- en: There are three observation vectors in the data. For example, the first observation
    can be represented by the following vector,
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 数据中有三个观测向量。例如，第一个观测可以由以下向量表示，
- en: '**Ob1 = (2, 3, 5)**'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '**Ob1 = (2, 3, 5)**'
- en: '[PRE8]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: All other observations can be represented in a similar format.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 所有其他观测值可以用类似的格式表示。
- en: The unit length scaling is applied to observation vectors in this way.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 单位长度缩放以这种方式应用于观测向量。
- en: In Scikit-learn, unit length scaling is performed using the **Normalizer()**
    function.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Scikit-learn 中，单位长度缩放是通过 **Normalizer()** 函数进行的。
- en: '[PRE9]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: When calling the fit() method, nothing happens this time. It only validates
    the model’s parameters. In previous cases, the parameters were learned. Calling
    the transform() method will divide each observation vector by its length, i.e.
    perform unit length scaling.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 调用 fit() 方法时，这次没有任何变化。它仅验证模型的参数。在之前的情况下，参数是通过学习获得的。调用 transform() 方法将除以每个观测向量的长度，即执行单位长度缩放。
- en: The **Normalizer()** function also provides an option to change the distance
    type, Manhattan (‘l1’ norm) or Euclidean (‘l2’ norm). By default, ‘l2’ is used.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '**Normalizer()** 函数还提供了更改距离类型的选项，曼哈顿（‘l1’ 范数）或欧几里得（‘l2’ 范数）。默认情况下使用‘l2’。'
- en: Feature scaling and distribution of data
  id: totrans-102
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 特征缩放与数据分布
- en: Any feature scaling method discussed above does not change the underlying distribution
    of data. Before and after applying feature scaling, the variable’s distribution
    remains unchanged! Only the range of values will be changed!
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 以上讨论的任何特征缩放方法都不会改变数据的基础分布。在应用特征缩放前后，变量的分布保持不变！只有值的范围会发生变化！
- en: To illustrate this, I will create two histograms of the same feature before
    and after applying z-score standardization.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明这一点，我将创建同一特征的两个直方图，一个是在应用 z-score 标准化之前，另一个是在应用之后。
- en: '[PRE10]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '[PRE11]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '![](../Images/f933e18ebb5a3067c1b8d46cf6bb9c01.png)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f933e18ebb5a3067c1b8d46cf6bb9c01.png)'
- en: '**Histogram of unscaled variable** (Image by author)'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '**未缩放变量的直方图**（图像作者提供）'
- en: '[PRE12]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '![](../Images/6d1bf69f2c91d110a625e167f97ec6f9.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6d1bf69f2c91d110a625e167f97ec6f9.png)'
- en: '**Histogram of scaled variable** (Image by author)'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '**缩放变量的直方图**（图像作者提供）'
- en: The variable’s distribution remains unchanged! But, the range of values will
    be changed!
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 变量的分布保持不变！但是，值的范围会发生变化！
- en: '[PRE13]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '![](../Images/c0710f92da4be6bbdd0f16d7c4c170fd.png)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c0710f92da4be6bbdd0f16d7c4c170fd.png)'
- en: '**The range before scaling** (Image by author)'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '**缩放前的范围**（图像作者提供）'
- en: '[PRE14]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '![](../Images/584b5f5e69e442239f46930515825899.png)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/584b5f5e69e442239f46930515825899.png)'
- en: '**The range after scaling** (Image by author)'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '**缩放后的范围**（图像作者提供）'
- en: Data leakage when feature scaling
  id: totrans-119
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 特征缩放时的数据泄漏
- en: In data preprocessing, data leakage happens when some information in the train
    set leaks to the test set.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据预处理过程中，当训练集中的某些信息泄漏到测试集中时，就会发生数据泄漏。
- en: 'When training a model, the data used for training should not be used for testing.
    That’s why we split a dataset into two parts: train and test sets. The train and
    test sets should be independent and should not be mixed up.'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 训练模型时，用于训练的数据不应用于测试。这就是为什么我们将数据集划分为训练集和测试集的原因。训练集和测试集应该是独立的，不应混合使用。
- en: In feature scaling, data leakage easily happens in two ways.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 在特征缩放中，数据泄漏主要有两种方式。
- en: '**When performing feature scaling before splitting data into train and test
    sets**'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**在将数据划分为训练集和测试集之前进行特征缩放**'
- en: '[PRE15]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: When calling the fit() method of the scaler (sc), the parameters (means and
    standard deviations of each variable) are learned from the ***entire*** dataset.
    Some information in train and test sets may be mixed up because splitting is done
    after scaling the data.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 当调用缩放器 (sc) 的 fit() 方法时，参数（每个变量的均值和标准差）是从***整个***数据集中学习得到的。由于在数据缩放后再进行划分，训练集和测试集中的一些信息可能会混淆。
- en: To avoid this, you should do feature scaling after splitting data.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 为避免这种情况，应该在划分数据后进行特征缩放。
- en: '[PRE16]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '**When calling the fit() method of the scaler (sc) twice on both training and
    test sets**'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**在对训练集和测试集分别调用缩放器 (sc) 的 fit() 方法两次时**'
- en: '[PRE17]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: When calling the fit() method twice on both training and test sets, the parameters
    are learned twice. The parameters should only be learned on the training test,
    not on the test set. The learned parameters on the train set can also be applied
    to transform the test set too. In other words, you need to call the fit() method
    only once on the train set. In this way, we can avoid leaking data from the train
    set to the test set.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 当对训练集和测试集分别调用 fit() 方法两次时，参数会被学习两次。参数应该只在训练集上学习，而不是在测试集上学习。训练集上学习到的参数也可以应用于转换测试集。换句话说，你只需要在训练集上调用一次
    fit() 方法。这样，我们可以避免将数据从训练集泄漏到测试集。
- en: '[PRE18]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Summary of feature scaling methods
  id: totrans-132
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 特征缩放方法总结
- en: '**Standardization:** *StandardScaler()*, Usage — When the data follows a Gaussian
    or normal distribution or the distribution is unknown | Less sensitive to outliers
    in data'
  id: totrans-133
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**标准化：** *StandardScaler()*，用法 — 当数据遵循高斯或正态分布或分布未知时 | 对数据中的异常值不太敏感'
- en: '**Min-Max Scaling (Normalization):** *MinMaxScaler()*, Usage — When the distribution
    of data is not known or does not follow the normal distribution | Highly sensitive
    to the outlier in data'
  id: totrans-134
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**最小-最大缩放（归一化）：** *MinMaxScaler()*，用法 — 当数据分布未知或不遵循正态分布时 | 对数据中的异常值高度敏感'
- en: '**Robust Scaling:** *RobustScaler()*, Usage — Robust to outliers in data'
  id: totrans-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**鲁棒缩放：** *RobustScaler()*，用法 — 对数据中的异常值鲁棒'
- en: '**Mean Normalization:** *StandardScaler()* and *RobustScaler()*, Usage — Sensitive
    to outliers in data'
  id: totrans-136
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**均值归一化：** *StandardScaler()* 和 *RobustScaler()*，用法 — 对数据中的异常值敏感'
- en: '**Maximum Absolute Scaling:** *MaxAbsScaler(),* Usage — Does not center data
    unlike other methods | Works well with sparse data'
  id: totrans-137
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**最大绝对值缩放：** *MaxAbsScaler()*，用法 — 与其他方法不同，不会对数据进行中心化 | 与稀疏数据效果良好'
- en: '**Vector Unit-Length Scaling:** *Normalizer()*, Usage — Performs calculations
    observation-wise, i.e., by considering each observation across all features (all
    other methods perform calculations feature-wise, i.e., by considering each feature
    across all observations)'
  id: totrans-138
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**向量单位长度缩放：** *Normalizer()*，使用方法——逐观测进行计算，即考虑所有特征下的每个观测值（所有其他方法按特征进行计算，即考虑所有观测下的每个特征）'
- en: This is the end of today’s article.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 今天的文章到此结束。
- en: '**Please let me know if you’ve any questions or feedback.**'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '**如果您有任何问题或反馈，请告诉我。**'
- en: How about an AI course?
  id: totrans-141
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: AI课程怎么样？
- en: '[**Neural Networks and Deep Learning Course**](https://rukshanpramoditha.medium.com/list/neural-networks-and-deep-learning-course-a2779b9c3f75)'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**神经网络与深度学习课程**](https://rukshanpramoditha.medium.com/list/neural-networks-and-deep-learning-course-a2779b9c3f75)'
- en: Join my private list of emails
  id: totrans-143
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 加入我的私人邮件列表
- en: '*Never miss a great story from me again. By* [***subscribing to my email list***](https://rukshanpramoditha.medium.com/subscribe)*,
    you will directly receive my stories as soon as I publish them.*'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '*再也不会错过我的精彩故事。通过*[***订阅我的邮件列表***](https://rukshanpramoditha.medium.com/subscribe)*，您将直接在我发布新故事时立即收到。*'
- en: Thank you so much for your continuous support! See you in the next article.
    Happy learning to everyone!
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 非常感谢您的持续支持！我们在下一篇文章中见。祝大家学习愉快！
- en: References
  id: totrans-146
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: Special credits go to the author of the following book which I read to get some
    knowledge on the feature scaling methods.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 特别感谢以下书籍的作者，我通过阅读该书获得了有关特征缩放方法的一些知识。
- en: '*Python Feature Engineering Cookbook (2nd Edition 2022) by Soledad Galli*'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Python特征工程实用手册（第2版 2022）作者：索莱达德·加利*'
- en: Iris dataset info
  id: totrans-149
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Iris数据集信息
- en: '**Source:** You can download the original Iris dataset [here](https://archive.ics.uci.edu/dataset/53/iris).'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**来源：** 您可以[在这里](https://archive.ics.uci.edu/dataset/53/iris)下载原始的Iris数据集。'
- en: '**Creator:** R. A. Fisher'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**创作者：** R. A. Fisher'
- en: '**Citation:** Fisher,R. A.. (1988). Iris. UCI Machine Learning Repository.
    [https://doi.org/10.24432/C56C76](https://doi.org/10.24432/C56C76)'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**引用：** Fisher, R. A.. (1988). Iris. UCI机器学习库。[https://doi.org/10.24432/C56C76](https://doi.org/10.24432/C56C76)'
- en: '**License:** This dataset is licensed under a [Creative Commons Attribution
    4.0 International](https://creativecommons.org/licenses/by/4.0/) (CC BY 4.0) license.'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**许可证：** 本数据集采用[知识共享署名 4.0 国际](https://creativecommons.org/licenses/by/4.0/)（CC
    BY 4.0）许可证。'
- en: This post was originally published on [Substack](https://datasciencemasterclass.substack.com/p/why-is-feature-scaling-important-in-ml)
    by me.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 本文最初由我在[Substack](https://datasciencemasterclass.substack.com/p/why-is-feature-scaling-important-in-ml)上发布。
- en: '**Designed and written by:** [**Rukshan Pramoditha**](https://medium.com/u/f90a3bb1d400?source=post_page-----2773bda5be30--------------------------------)'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '**设计与撰写：** [**Rukshan Pramoditha**](https://medium.com/u/f90a3bb1d400?source=post_page-----2773bda5be30--------------------------------)'
- en: '**2023–08–15**'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: '**2023–08–15**'
