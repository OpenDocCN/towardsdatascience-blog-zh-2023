- en: Building LLMs-Powered Apps with OPL Stack
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用OPL堆栈构建LLMs驱动的应用程序
- en: 原文：[https://towardsdatascience.com/building-llms-powered-apps-with-opl-stack-c1d31b17110f?source=collection_archive---------2-----------------------#2023-04-03](https://towardsdatascience.com/building-llms-powered-apps-with-opl-stack-c1d31b17110f?source=collection_archive---------2-----------------------#2023-04-03)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/building-llms-powered-apps-with-opl-stack-c1d31b17110f?source=collection_archive---------2-----------------------#2023-04-03](https://towardsdatascience.com/building-llms-powered-apps-with-opl-stack-c1d31b17110f?source=collection_archive---------2-----------------------#2023-04-03)
- en: 'OPL: OpenAI, Pinecone, and Langchain for knowledge-based AI assistant'
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: OPL：OpenAI、Pinecone 和 Langchain 用于知识驱动的AI助手
- en: '[](https://medium.com/@wen_yang?source=post_page-----c1d31b17110f--------------------------------)[![Wen
    Yang](../Images/5eac438762d015a0ab128757cc951967.png)](https://medium.com/@wen_yang?source=post_page-----c1d31b17110f--------------------------------)[](https://towardsdatascience.com/?source=post_page-----c1d31b17110f--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----c1d31b17110f--------------------------------)
    [Wen Yang](https://medium.com/@wen_yang?source=post_page-----c1d31b17110f--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@wen_yang?source=post_page-----c1d31b17110f--------------------------------)[![Wen
    Yang](../Images/5eac438762d015a0ab128757cc951967.png)](https://medium.com/@wen_yang?source=post_page-----c1d31b17110f--------------------------------)[](https://towardsdatascience.com/?source=post_page-----c1d31b17110f--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----c1d31b17110f--------------------------------)
    [Wen Yang](https://medium.com/@wen_yang?source=post_page-----c1d31b17110f--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fcbb5383bd438&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-llms-powered-apps-with-opl-stack-c1d31b17110f&user=Wen+Yang&userId=cbb5383bd438&source=post_page-cbb5383bd438----c1d31b17110f---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----c1d31b17110f--------------------------------)
    ·12 min read·Apr 3, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fc1d31b17110f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-llms-powered-apps-with-opl-stack-c1d31b17110f&user=Wen+Yang&userId=cbb5383bd438&source=-----c1d31b17110f---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fcbb5383bd438&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-llms-powered-apps-with-opl-stack-c1d31b17110f&user=Wen+Yang&userId=cbb5383bd438&source=post_page-cbb5383bd438----c1d31b17110f---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----c1d31b17110f--------------------------------)
    ·12 min 阅读·2023年4月3日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fc1d31b17110f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-llms-powered-apps-with-opl-stack-c1d31b17110f&user=Wen+Yang&userId=cbb5383bd438&source=-----c1d31b17110f---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc1d31b17110f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-llms-powered-apps-with-opl-stack-c1d31b17110f&source=-----c1d31b17110f---------------------bookmark_footer-----------)![](../Images/017e9316f7ecbd4f794d51802ab23255.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc1d31b17110f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-llms-powered-apps-with-opl-stack-c1d31b17110f&source=-----c1d31b17110f---------------------bookmark_footer-----------)![](../Images/017e9316f7ecbd4f794d51802ab23255.png)'
- en: 'Midjourney Prompt: a girl building a lego bridge from multiple blocks'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: Midjourney 提示：一个女孩用多个积木块建造乐高桥梁
- en: 'I remember a month ago, Eugene Yan posted a [poll](https://www.linkedin.com/posts/eugeneyan_activity-7029289248209977344-oteC?utm_source=share&utm_medium=member_desktop)
    on Linkedin:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 我记得一个月前，Eugene Yan 在 LinkedIn 上发布了一项 [投票](https://www.linkedin.com/posts/eugeneyan_activity-7029289248209977344-oteC?utm_source=share&utm_medium=member_desktop)：
- en: Are you feeling the FOMO from not working on LLMs/Generative AI?
  id: totrans-10
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 你是否感到因为没有参与LLMs/生成型AI而错失良机？
- en: 'Most answered “Yes”. It’s easy to understand why, given the sweeping attention
    generated by chatGPT and now the release of gpt-4\. People describe the rise of
    Large Language Models (LLMs) feels like the iPhone moment. Yet I think there’s
    really no need to feel the FOMO. Consider this: missing out on the opportunity
    to develop iPhones doesn’t preclude the ample potential for creating innovative
    iPhone apps. So too with LLMs. We have just entered the dawn of a new era and
    now it’s the perfect time to harness the magic of integrating LLMs to build powerful
    applications.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数人回答了“是”。考虑到chatGPT引发的广泛关注以及现在gpt-4的发布，很容易理解为什么会这样。人们形容大型语言模型（LLMs）的崛起就像iPhone的时刻。然而，我认为真的没有必要感到FOMO。考虑一下：错过了开发iPhones的机会并不排除创造创新iPhone应用程序的巨大潜力。LLMs也是如此。我们刚刚进入了一个新时代，现在正是利用LLMs整合构建强大应用程序的绝佳时机。
- en: 'In this post, I’ll cover below topics:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我将涵盖以下主题：
- en: What is the OPL stack?
  id: totrans-13
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 什么是OPL技术栈？
- en: How to use the OPL to build chatGPT with domain knowledge? (Essential components
    with code walkthrough)
  id: totrans-14
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如何使用OPL构建具有领域知识的chatGPT？（包含代码演示的关键组件）
- en: Production considerations
  id: totrans-15
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 生产考虑因素
- en: Common misconceptions
  id: totrans-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 常见误解
- en: 1\. What is the OPL stack?
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 1\. 什么是OPL技术栈？
- en: '![](../Images/5efe1d72069d166bbe99a2cee6c8a240.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5efe1d72069d166bbe99a2cee6c8a240.png)'
- en: Image created by the author
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 作者创建的图像
- en: '**OPL stands for OpenAI, Pinecone, and Langchain,** which has increasingly
    become the industry solution toovercome the two limitations of LLMs:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '**OPL代表OpenAI、Pinecone和Langchain，** 它已逐渐成为克服LLMs两个局限性的行业解决方案：'
- en: '**LLMs hallucination:** chatGPT will sometimes provide wrong answers with overconfidence.
    One of the underlying causes is that those language models are trained to predict
    the next word very effectively, or the next token to be precise. Given an input
    text, chatGPT will return words with high probability, which doesn’t mean that
    chatGPT has reasoning ability.'
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**LLMs幻觉：** chatGPT有时会提供过度自信的错误回答。一个潜在的原因是这些语言模型被训练得非常有效地预测下一个词，或者更准确地说是下一个标记。给定输入文本，chatGPT会返回高概率的词，这并不意味着chatGPT具有推理能力。'
- en: '**Less up-to-date knowledge:** chatGPT’s training data is limited to internet
    data prior to Sep 2021\. Therefore, it will produce less desirable answers if
    your questions are about recent trends or topics.'
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**知识更新不够及时：** chatGPT的训练数据仅限于2021年9月之前的互联网数据。因此，如果你的问题涉及最近的趋势或话题，它可能会产生不太理想的回答。'
- en: 'The common solution is to add a knowledge base on top of LLMs and use Langchain
    as a framework to build the pipeline. The essential components of each technology
    can be summarized below:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 常见的解决方案是在LLMs上添加知识库，并使用Langchain作为构建流水线的框架。每项技术的关键组件可以总结如下：
- en: '**OpenAI**:'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**OpenAI**：'
- en: '- provides API access to powerful LLMs such as chatGPT and gpt-4'
  id: totrans-25
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '- 提供对强大LLMs如chatGPT和gpt-4的API访问'
- en: '- provides embedding models to convert text to embeddings.'
  id: totrans-26
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '- 提供将文本转换为嵌入的模型。'
- en: '**Pinecone**: it provides embedding vector storage, semantic similarity comparison,
    and fast retrieval.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Pinecone**：提供嵌入向量存储、语义相似度比较和快速检索。'
- en: '**Langchain**: it comprises 6 modules (`Models`, `Prompts`, `Indexes`, `Memory`,
    `Chains` and `Agents`).'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Langchain**：它包含6个模块（`Models`、`Prompts`、`Indexes`、`Memory`、`Chains`和`Agents`）。'
- en: '- `Models` offers flexibility in embedding models, chat models, and LLMs, including
    but not limited to OpenAI’s offerings. You can also use other models from Hugging
    Face like BLOOM and FLAN-T5.'
  id: totrans-29
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '- `Models` 提供了灵活的嵌入模型、聊天模型和LLMs，包括但不限于OpenAI的产品。你还可以使用Hugging Face上的其他模型，如BLOOM和FLAN-T5。'
- en: '- `Memory` : there are a variety of ways to allow chatbots to remember past
    conversation memory. From my experience, entity memory works well and is efficient.'
  id: totrans-30
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '- `Memory` : 有多种方式可以让聊天机器人记住过去的对话记录。根据我的经验，实体记忆效果好且高效。'
- en: '- `Chains` : If you’re new to Langchain, Chains is a great starting point.
    It follows a pipeline-like structure to process user input, select the LLM model,
    apply a Prompt template, and search the relevant context from the knowledge base.'
  id: totrans-31
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '- `Chains` : 如果你是Langchain的新手，Chains是一个很好的起点。它遵循类似流水线的结构来处理用户输入，选择LLM模型，应用Prompt模板，并从知识库中搜索相关上下文。'
- en: Next, I’ll walk through the app I built using the OPL stack.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我将介绍我使用OPL技术栈构建的应用程序。
- en: 2\. How to use the OPL to build chatGPT with domain knowledge? (Essential components
    with code walkthrough)
  id: totrans-33
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 2\. 如何使用OPL构建具有领域知识的chatGPT？（包含代码演示的关键组件）
- en: 'The app I built is called [chatOutside](https://outsidechat.streamlit.app/)
    , which has two primary sections:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 我构建的应用程序称为[chatOutside](https://outsidechat.streamlit.app/)，它有两个主要部分：
- en: '**chatGPT**: lets you chat with chatGPT directly, and the format is similar
    to a Q&A app, where you receive a single input and output at a time.'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**chatGPT**：让你直接与chatGPT聊天，格式类似于问答应用，每次接收一个输入和输出。'
- en: '**chatOutside**: allows you to chat with a version of chatGPT with expert knowledge
    of Outdoor activities and trends. The format is more like a chatbot style, where
    all messages are recorded as the conversation progresses. I’ve also included a
    section that provides source links, which can boost user confidence and is always
    useful to have.'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**chatOutside**：让你与具有户外活动及趋势专业知识的chatGPT版本进行对话。格式更类似于聊天机器人的风格，所有消息在对话过程中都会被记录。我还包含了一个提供源链接的部分，这可以增强用户信心，并且总是有用的。'
- en: 'As you can see, if you ask the same question: “What’re the best running shoes
    in 2023? My budget is around $200”. chatGPT will say “as an AI language model,
    I don’t have access to information from the future.” While chatOutside will provide
    you with more up-to-date answers, along with source links.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，如果你问同样的问题：“2023年最好的跑鞋是什么？我的预算在$200左右。”chatGPT会说“作为一个AI语言模型，我无法访问未来的信息。”而chatOutside会为你提供更及时的答案，并附上源链接。
- en: '![](../Images/90480e9dfc8f81b47e0fe6bf4e2e3f4b.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/90480e9dfc8f81b47e0fe6bf4e2e3f4b.png)'
- en: 'There are three major steps involved in the development process:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 开发过程涉及三个主要步骤：
- en: 'Step 1: Build an Outside Knowledge Base in Pinecone'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第一步：在Pinecone中构建外部知识库
- en: 'Step 2: Use Langchain for Question & Answering Service'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第二步：使用Langchain进行问答服务
- en: 'Step 3: Build our app in Streamlit'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第三步：在Streamlit中构建我们的应用程序
- en: Implementation details for each step are discussed below.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 每个步骤的实施细节将在下面讨论。
- en: '**Step 1:** Build an Outside Knowledge Base in Pinecone'
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**步骤1：** 在Pinecone中构建外部知识库'
- en: '**Step 1.1**: I connected to our Outside catalog database and selected articles
    published between January 1st, 2022, and March 29th, 2023\. This provided us with
    approximately 20,000 records.'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**步骤1.1：** 我连接到我们的外部目录数据库，并选择了在2022年1月1日至2023年3月29日之间发布的文章。这为我们提供了大约20,000条记录。'
- en: '![](../Images/a55bed6eacdefdd3ad7e8a6bb57cae7c.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a55bed6eacdefdd3ad7e8a6bb57cae7c.png)'
- en: sample data preview from Outside
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 外部的示例数据预览
- en: Next, we need to perform two data transformations.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要进行两个数据转换。
- en: '**Step 1.2:** convert the above dataframe to a list of dictionaries to ensure
    data can be upserted correctly into Pinecone.'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**步骤1.2：** 将上述数据框转换为字典列表，以确保数据可以正确地插入到Pinecone中。'
- en: '[PRE0]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '**Step 1.3:** Split the `content`into smaller chunks using Langchain’s `RecursiveCharacterTextSplitter`
    . The benefit of breaking down documents into smaller chunks is twofold:'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**步骤1.3：** 使用Langchain的`RecursiveCharacterTextSplitter`将`content`拆分成更小的块。将文档拆分为更小的块有两个好处：'
- en: '- A typical article might be more than 1000 characters, which is very long.
    Imagine we want to retrieve top-3 articles as context to prompt the chatGPT, we
    could easily hit the 4000 token limit.'
  id: totrans-52
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '- 一篇典型文章可能超过1000个字符，这非常长。想象一下我们想检索前3篇文章作为提示给chatGPT，我们很容易就会超过4000个字元限制。'
- en: '- Smaller chunks provide more relevant information, resulting in better context
    to prompt chatGPT.'
  id: totrans-53
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '- 更小的块提供更相关的信息，从而为chatGPT提供更好的提示上下文。'
- en: '[PRE1]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: After splitting, each record’s content was broken down into multiple chunks,
    each having less than 400 tokens.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 分割后，每条记录的内容被拆分成多个部分，每个部分少于400个字元。
- en: '![](../Images/309b912e454adbd6526dcc71bbd2aab6.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/309b912e454adbd6526dcc71bbd2aab6.png)'
- en: Break the content into multiple chunks
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 将内容拆分为多个块
- en: One thing worth noting is that the text splitter used is called `RecursiveCharacterTextSplitter`
    , which is recommended use by Harrison Chase, the creator of Langchain. The basic
    idea is to first split by the paragraph, then split by sentence, with overlapping
    (20 tokens). This helps preserve meaningful information and context from the surrounding
    sentences.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，使用的文本分割器称为`RecursiveCharacterTextSplitter`，这是Langchain的创建者Harrison Chase推荐使用的。基本思路是首先按段落拆分，然后按句子拆分，重叠（20字元）。这有助于保留来自周围句子的有意义信息和上下文。
- en: '**Step 1.4:** Upsert data to Pinecone. The below code is adapted from [James
    Briggs](https://medium.com/u/b9d77a4ca1d1?source=post_page-----c1d31b17110f--------------------------------)’s
    wonderful [tutorial](https://github.com/pinecone-io/examples/blob/master/generation/langchain/handbook/05-langchain-retrieval-augmentation.ipynb).'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**步骤 1.4：** 将数据插入 Pinecone。下面的代码改编自 [James Briggs](https://medium.com/u/b9d77a4ca1d1?source=post_page-----c1d31b17110f--------------------------------)
    的精彩 [教程](https://github.com/pinecone-io/examples/blob/master/generation/langchain/handbook/05-langchain-retrieval-augmentation.ipynb)。'
- en: '[PRE2]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: We batch upload and embed all articles. which took about 20 minutes to upsert
    20k records. Be sure to adjust the `tqdm`import accordingly based on your env
    (you don’t need to import both!)
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 我们批量上传并嵌入所有文章，这花费了大约 20 分钟来插入 2 万条记录。请根据你的环境调整 `tqdm` 导入（你不需要同时导入两个！）
- en: '[PRE3]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: After upserting the Outside articles data, we can inspect our pinecone index
    by using `index.describe_index_stats()` . One of the stats to pay attention to
    is `index_fullness`, which was 0.2 in our case. This means the Pinecone pod was
    20% full, suggesting that a single p1 pod can store approximately 100k articles.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在将 Outside 文章数据插入后，我们可以通过使用 `index.describe_index_stats()` 检查我们的 Pinecone 索引。需要注意的统计数据之一是
    `index_fullness`，在我们的案例中为 0.2。这意味着 Pinecone pod 已满 20%，暗示一个 p1 pod 大约可以存储 10 万篇文章。
- en: '![](../Images/e58bbd39a40d1f5ca391d0328c3206d7.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e58bbd39a40d1f5ca391d0328c3206d7.png)'
- en: After upserting data into Pinecone
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在将数据插入 Pinecone 后
- en: 'Step 2: Use Langchain for Question & Answering Service'
  id: totrans-66
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 步骤 2：使用 Langchain 进行问答服务
- en: '*Note: Langchain updates so fast these days, the version used below code is*
    `*0.0.118*` *.*'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '*注意：Langchain 最近更新非常快，下面代码使用的版本是* `*0.0.118*` *。*'
- en: '![](../Images/84f252a1857dc61124514ae1c921ecfe.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/84f252a1857dc61124514ae1c921ecfe.png)'
- en: Data flow in OPL stack
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: OPL 堆栈中的数据流
- en: 'The above sketchnote illustrates how data flows during the inference stage:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 上面的草图说明了推理阶段数据流动的方式：
- en: 'The user asks a question: “What are the best running shoes in 2023?”.'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用户提问：“2023 年最好的跑鞋是什么？”
- en: The question is converted into embedding using the `ada-002`model.
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 问题使用 `ada-002` 模型转换为嵌入。
- en: The user question embedding is compared with all vectors stored in Pinecone
    using `similarity_search`function, which retrieves the top 3 text chunks that
    are most likely to answer the question.
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用户问题嵌入与 Pinecone 中存储的所有向量通过 `similarity_search` 函数进行比较，该函数检索最有可能回答问题的前三个文本块。
- en: Langchain then passes the top 3 text chunks as `context` , along with the user
    question to gpt-3.5 ( `ChatCompletion` ) to generate the answers.
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Langchain 然后将前三个文本块作为 `context`，与用户问题一起传递给 gpt-3.5（`ChatCompletion`）生成答案。
- en: 'All can be achieved with less than 30 lines of code:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些都可以通过不到 30 行代码实现：
- en: '[PRE4]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Now we can test by asking a hiking-related question: “Can you recommend some
    advanced hiking trails with views of water in California bay area?”'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以通过提出一个与徒步旅行相关的问题进行测试：“你能推荐一些加州湾区带水景的高级徒步旅行路线吗？”
- en: '![](../Images/20894ccdec7880545e25efa3b0beb363.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/20894ccdec7880545e25efa3b0beb363.png)'
- en: Langchain VectorDBQA with source
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: Langchain VectorDBQA 带来源
- en: 'Step 3: Build our app in Streamlit'
  id: totrans-80
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 步骤 3：在 Streamlit 中构建我们的应用
- en: 'After verifying the logic is working in Jupyter notebook, we can assemble everything
    together and build a frontend using streamlit. In our streamlit app, there are
    two python files:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Jupyter notebook 中验证逻辑有效后，我们可以将所有内容整合在一起，并使用 streamlit 构建前端。在我们的 streamlit
    应用中，有两个 python 文件：
- en: '- `app.py` : the main python file for frontend and power the app'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '- `app.py`：前端的主要 python 文件，驱动应用'
- en: '- `utils.py` : the supporting function which will be called by `app.py`'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '- `utils.py`：由 `app.py` 调用的支持函数'
- en: 'Here’s what my `utils.py` looks like:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是我的 `utils.py` 看起来的样子：
- en: '[PRE5]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'And finally, here’s what my `app.py`looks like:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，这就是我的 `app.py` 看起来的样子：
- en: '[PRE6]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 3\. Production considerations
  id: totrans-88
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 3\. 生产考虑
- en: Alrighty, enough coding!
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 好了，够了！
- en: 'The app is actually pretty nice already as it is. But if we want to move to
    production, there are a few additional things to consider:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 应用实际上已经很不错了。但是如果我们想进入生产环境，还有一些额外的事项需要考虑：
- en: 'Ingesting new and updated data in Pinecone: we did a one-time batch upsert
    for article data. In reality, new articles are added to our websites every day,
    and some fields may get updated for data already ingested into Pinecone. This
    is not a machine learning problem but it’s ever-present for media companies: how
    to keep your data updated in every service. The potential solution is to set up
    a cron job to run the upsert and update the job periodically. There’s an instruction
    on how to send upserts in parallel, which might be quite useful if we can use
    asynchronous tasks with Django and Celery.'
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 Pinecone 中摄取新的和更新的数据：我们对文章数据进行了单次批量插入。实际上，每天都有新的文章添加到我们的网站中，而且一些字段可能会更新已经摄取到
    Pinecone 的数据。这不是机器学习的问题，而是对媒体公司而言常常存在的问题：如何保持每个服务中的数据更新。潜在的解决方案是设置一个定时任务来定期执行插入和更新任务。有关如何并行发送插入操作的指令，如果我们可以使用
    Django 和 Celery 的异步任务，这可能会非常有用。
- en: 'Limitation for [Pinecone pod storage](https://docs.pinecone.io/docs/limits#:~:text=Pod%20storage%20capacity,5M%20vectors%20with%20768%20dimensions.):
    the app is currently using p1 pod, which can store up to 1M vectors with 768 dimensions,
    or roughly 500k vectors if we use OpenAI’s `ada-002`embedding model (which has
    a dimension of 1536).'
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[Pinecone pod 存储](https://docs.pinecone.io/docs/limits#:~:text=Pod%20storage%20capacity,5M%20vectors%20with%20768%20dimensions.)的限制：当前应用使用的是
    p1 pod，能够存储最多 1M 个 768 维向量，或者如果我们使用 OpenAI 的 `ada-002` 嵌入模型（其维度为 1536），则大约能存储
    50 万个向量。'
- en: 'Stream capability for faster response times: To reduce the perceived latency
    for users, it may be helpful to add stream capability to the app. This would mimic
    the chatGPT by returning generated output token by token, rather than showing
    the entire response at once. While this functionality works for REST API using
    LangChain function, it poses a unique challenge for us since we use GraphQL instead
    of REST.'
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 流式处理功能以提高响应速度：为了减少用户感知的延迟，可能有助于向应用添加流式处理功能。这将模拟 chatGPT 按 token 返回生成的输出，而不是一次性显示整个响应。虽然这种功能在使用
    LangChain 函数的 REST API 中有效，但对于我们来说，因为我们使用 GraphQL 而不是 REST，这带来了独特的挑战。
- en: 4\. Common misconceptions and questions
  id: totrans-94
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 4\. 常见误解和问题
- en: '**chatGPT remembers the internet data up to Sep 2021\. And it is retrieve answers
    based on memory.**'
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**chatGPT 记住了截至 2021 年 9 月的互联网数据，并且基于记忆检索答案。**'
- en: '- This is not how it works. After training, chatGPT deletes the data from memory
    and uses its 175 billion parameters (weights) to predict what’s the most probable
    token (text). It doesn’t retrieve answers based on memory. That’s why if you just
    copy the answer generated by chatGPT, it’s unlikely that you can find any source
    from the internet.'
  id: totrans-96
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '- 这并不是它的工作方式。训练之后，chatGPT 从内存中删除数据，并使用其 1750 亿个参数（权重）来预测最可能的 token（文本）。它不会基于记忆检索答案。这就是为什么如果你只是复制
    chatGPT 生成的答案，你不太可能找到来自互联网的任何来源。'
- en: '**We can train/fine-tune/prompt-engineering chatGPT.**'
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**我们可以训练/微调/提示工程 chatGPT。**'
- en: '- Training and fine-tuning Large Language Models meant actually changing the
    model parameters. You need to have access to the actual model file and guide the
    model for your specific use cases. In most cases, we wouldn’t train or fine-tune
    chatGPT. Prompt engineering is all we need: providing extra context to chatGPT
    and allowing it to answer based on the contexts.'
  id: totrans-98
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '- 训练和微调大型语言模型意味着实际更改模型参数。你需要访问实际的模型文件，并根据你的具体使用情况指导模型。在大多数情况下，我们不会训练或微调 chatGPT。我们所需要的只是提示工程：为
    chatGPT 提供额外的上下文，并让它根据这些上下文回答问题。'
- en: '**What’s the difference between a token and a word?'
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**token 和词有什么区别？**'
- en: -** Token is a word piece. 100 tokens are roughly equal to 75 words. For example,
    “Unbelievable” is one word but 3 tokens (un, belie, able).
  id: totrans-100
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: -** Token 是一个词片段。100 个 token 大约等于 75 个词。例如，“Unbelievable” 是一个词，但包含 3 个 token（un,
    belie, able）。**
- en: '**What does the 4000-token limitation mean?'
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**4000-token 限制意味着什么？**'
- en: -** OpenAI gpt-3.5 has a token limitation of 4096 for combining user input,
    context, and response. When using Langchain’s memory, the total number of words
    used in (user question + context + memory + chatGPT response) needs to be less
    than 3000 words (4000 tokens).
  id: totrans-102
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: -** OpenAI gpt-3.5 的 token 限制为 4096，用于结合用户输入、上下文和响应。当使用 Langchain 的内存时，（用户问题
    + 上下文 + 内存 + chatGPT 响应）使用的总词数需要少于 3000 个词（4000 个 token）。**
- en: '- gpt-4 has a higher token limit but it’s also 20X more expensive! (gpt-3.5:
    $0.002/1K tokens; gpt-4: $0.045/1K tokens assuming 500 for prompt and 500 for
    completion).'
  id: totrans-103
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '- gpt-4 有更高的 token 限制，但也贵了 20 倍！（gpt-3.5：$0.002/1000 tokens；gpt-4：$0.045/1000
    tokens，假设 500 个用于提示和 500 个用于完成）。'
- en: '**Do I have to use Vector Store like Pinecone?'
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**我是否必须使用像 Pinecone 这样的向量存储？**'
- en: -** No. Pinecone is not the only option for vector storage. Other vector store
    options include Chroma, FAISS, Redis, and more. Additionally, you don’t always
    need a vector store. For example, if you want to build a Q&A for a specific website,
    you can crawl the web page and follow this [openai-cookbook-recipe](https://github.com/openai/openai-cookbook/blob/main/apps/web-crawl-q-and-a/web-qa.ipynb).
  id: totrans-105
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: -** 不，Pinecone 不是唯一的向量存储选项。其他向量存储选项包括 Chroma、FAISS、Redis 等。此外，你并不总是需要向量存储。例如，如果你想为特定网站构建一个问答系统，你可以爬取网页并参考这个
    [openai-cookbook-recipe](https://github.com/openai/openai-cookbook/blob/main/apps/web-crawl-q-and-a/web-qa.ipynb)。
- en: Parting Words
  id: totrans-106
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 告别的话
- en: Thank you for reading this lengthy post! If you have any questions or tips on
    using Langchain, please feel free to reach out.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 感谢你阅读这篇长文！如果你对使用 Langchain 有任何问题或建议，请随时联系我。
- en: Also, I will be going to the [LLM Bootcamp](https://fullstackdeeplearning.com/llm-bootcamp/)
    where I hope to learn more best practices on productionize LLMs-powered apps.
    If you are interested in this topic, stay tuned for my future posts! 😃
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我将参加 [LLM Bootcamp](https://fullstackdeeplearning.com/llm-bootcamp/)，希望能学习到更多关于将
    LLMs 应用到生产中的最佳实践。如果你对这个话题感兴趣，请关注我未来的帖子！😃
