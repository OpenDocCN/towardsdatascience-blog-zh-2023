- en: 'Logistic Regression: Deceptively Flawed'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 逻辑回归：看似有缺陷
- en: 原文：[https://towardsdatascience.com/logistic-regression-deceptively-flawed-2c3e7f77eac9?source=collection_archive---------15-----------------------#2023-05-23](https://towardsdatascience.com/logistic-regression-deceptively-flawed-2c3e7f77eac9?source=collection_archive---------15-----------------------#2023-05-23)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/logistic-regression-deceptively-flawed-2c3e7f77eac9?source=collection_archive---------15-----------------------#2023-05-23](https://towardsdatascience.com/logistic-regression-deceptively-flawed-2c3e7f77eac9?source=collection_archive---------15-----------------------#2023-05-23)
- en: When can large odds ratios and perfectly separated data bite you?
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 大的赔率比和完美分离的数据什么时候会让你吃亏？
- en: '[](https://medium.com/@igor-s?source=post_page-----2c3e7f77eac9--------------------------------)[![Igor
    Šegota](../Images/17c592b71fef9526a0679d47937837f6.png)](https://medium.com/@igor-s?source=post_page-----2c3e7f77eac9--------------------------------)[](https://towardsdatascience.com/?source=post_page-----2c3e7f77eac9--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----2c3e7f77eac9--------------------------------)
    [Igor Šegota](https://medium.com/@igor-s?source=post_page-----2c3e7f77eac9--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@igor-s?source=post_page-----2c3e7f77eac9--------------------------------)[![Igor
    Šegota](../Images/17c592b71fef9526a0679d47937837f6.png)](https://medium.com/@igor-s?source=post_page-----2c3e7f77eac9--------------------------------)[](https://towardsdatascience.com/?source=post_page-----2c3e7f77eac9--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----2c3e7f77eac9--------------------------------)
    [Igor Šegota](https://medium.com/@igor-s?source=post_page-----2c3e7f77eac9--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fe5f8ebca4ad8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flogistic-regression-deceptively-flawed-2c3e7f77eac9&user=Igor+%C5%A0egota&userId=e5f8ebca4ad8&source=post_page-e5f8ebca4ad8----2c3e7f77eac9---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----2c3e7f77eac9--------------------------------)
    ·8 min read·May 23, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F2c3e7f77eac9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flogistic-regression-deceptively-flawed-2c3e7f77eac9&user=Igor+%C5%A0egota&userId=e5f8ebca4ad8&source=-----2c3e7f77eac9---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fe5f8ebca4ad8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flogistic-regression-deceptively-flawed-2c3e7f77eac9&user=Igor+%C5%A0egota&userId=e5f8ebca4ad8&source=post_page-e5f8ebca4ad8----2c3e7f77eac9---------------------post_header-----------)
    发表在[Towards Data Science](https://towardsdatascience.com/?source=post_page-----2c3e7f77eac9--------------------------------)
    ·8分钟阅读·2023年5月23日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F2c3e7f77eac9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flogistic-regression-deceptively-flawed-2c3e7f77eac9&user=Igor+%C5%A0egota&userId=e5f8ebca4ad8&source=-----2c3e7f77eac9---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F2c3e7f77eac9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flogistic-regression-deceptively-flawed-2c3e7f77eac9&source=-----2c3e7f77eac9---------------------bookmark_footer-----------)![](../Images/fd6846838f4d97d1fe123837c8fb36d1.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F2c3e7f77eac9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flogistic-regression-deceptively-flawed-2c3e7f77eac9&source=-----2c3e7f77eac9---------------------bookmark_footer-----------)![](../Images/fd6846838f4d97d1fe123837c8fb36d1.png)'
- en: Photo by [Alvan Nee](https://unsplash.com/es/@alvannee?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 照片由[Alvan Nee](https://unsplash.com/es/@alvannee?utm_source=medium&utm_medium=referral)拍摄，发布在[Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: 'This is a second part to a previous post on conceptual understanding of logistic
    regression:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 这是上一篇关于逻辑回归概念理解的文章的第二部分：
- en: '[](/logistic-regression-faceoff-67560de4f492?source=post_page-----2c3e7f77eac9--------------------------------)
    [## Logistic regression: Faceoff and Conceptual Understanding'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/logistic-regression-faceoff-67560de4f492?source=post_page-----2c3e7f77eac9--------------------------------)
    [## 逻辑回归：对决与概念理解'
- en: What log-losses and perfectly separated data have to do with hockey sticks?
  id: totrans-11
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 对数损失和完美分离的数据与冰球棒有什么关系？
- en: towardsdatascience.com](/logistic-regression-faceoff-67560de4f492?source=post_page-----2c3e7f77eac9--------------------------------)
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/logistic-regression-faceoff-67560de4f492?source=post_page-----2c3e7f77eac9--------------------------------)
- en: Last time we visualized and explained fitting log-losses in logistic regression.
    We also showed that this process cannot fit perfectly separated data. In other
    words, unlike linear regression with ordinary least square fit, logistic regression
    actually works better if the data is a little bit noisy!
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 上次我们可视化并解释了逻辑回归中的对数损失拟合。我们还展示了这个过程无法完美拟合完全分离的数据。换句话说，与普通最小二乘法的线性回归不同，逻辑回归实际上在数据稍微有些噪声时效果更好！
- en: In practice, does this actually matter? *It depends:*
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，这是否重要？*这要看情况：*
- en: It matters if our goal is to use the output for statistical inference. For example,
    accurately estimating model coefficients, calculating confidence intervals and
    to test hypotheses using p-values.
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果我们的目标是将输出用于统计推断，这就很重要。例如，准确估计模型系数、计算置信区间并使用 p 值测试假设。
- en: It does not matter much or at all, if our goal is to use the output of a logistic
    model to create a predictive classification model.
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果我们的目标是使用逻辑模型的输出创建一个预测分类模型，那么这并没有太大关系。
- en: 'Statistical inference: statsmodels'
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 统计推断：statsmodels
- en: Sample data
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 示例数据
- en: 'For this part, we will use Python’s statsmodels library. Keep in mind that
    statsmodels and scikit-learn (used later) parametrize the probability using *β*s
    instead of *k* and *x₀*:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们将使用 Python 的 statsmodels 库。请注意，statsmodels 和 scikit-learn（稍后使用）使用 *β*
    来参数化概率，而不是 *k* 和 *x₀*：
- en: '![](../Images/6b56227f9d6f2bd6640839f01a3d90a6.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6b56227f9d6f2bd6640839f01a3d90a6.png)'
- en: 'where the relationship between *k*, *x₀* and *β₁*, *β₀* is:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *k*、*x₀* 和 *β₁*、*β₀* 之间的关系是：
- en: '![](../Images/b9226107d5bd05e8238a5d138809310b.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b9226107d5bd05e8238a5d138809310b.png)'
- en: 'We will continue with datasets we generated in the [first part on logistic
    regression](/logistic-regression-faceoff-67560de4f492), first with the “imperfect”
    data `sample_df` , using statsmodels’ formula API:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将继续使用我们在 [逻辑回归第一部分](/logistic-regression-faceoff-67560de4f492) 中生成的数据集，首先使用“非完美”数据
    `sample_df` ，使用 statsmodels 的公式 API：
- en: '[PRE0]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '![](../Images/0c76474446a04887204ce5aebabe589a.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0c76474446a04887204ce5aebabe589a.png)'
- en: 'Our model parameters are *k = 3* and *x₀ = 2.5*, so those translate to *β₁
    = 3* and *β₀ = -7.5\.* We can compare those with fitted parameters by reading
    them out from the `coef` column of the bottom table:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的模型参数是 *k = 3* 和 *x₀ = 2.5*，因此它们对应于 *β₁ = 3* 和 *β₀ = -7.5*。我们可以通过从底部表格的 `coef`
    列中读取这些参数，与拟合的参数进行比较：
- en: '![](../Images/6e4bba19fadc12707c46ebbaba5aa5c7.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6e4bba19fadc12707c46ebbaba5aa5c7.png)'
- en: We have very few data points and the seed was intentionally chosen to showcase
    outliers, so the fit is a little bit off, but it is still in the right ballpark.
    The total log-loss is here reported as “Log-Likelihood”, which is just the negative
    of total log-loss and equals -6.911.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的数据点非常少，而且种子是故意选择的，以展示离群值，因此拟合略有偏差，但仍在合理范围内。这里报告的总对数损失为“对数似然”，即总对数损失的负值，等于
    -6.911。
- en: Perfectly separated data
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 完全分离的数据
- en: What happens when we run our perfectly separated dataset in statsmodels? Again,
    it depends!
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们在 statsmodels 中运行我们完全分离的数据集时，会发生什么？答案是，这要看情况！
- en: '[PRE1]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'In this case we got an error — no results are output. In other cases of perfect
    separation, we may get a warning. For example, if we use the same parameters but
    different random seed:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下我们得到了一个错误——没有输出结果。在其他完美分离的情况下，我们可能会得到一个警告。例如，如果我们使用相同的参数但不同的随机种子：
- en: '[PRE2]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '![](../Images/29d9a85d58eaf5d3be965f408b515d2b.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/29d9a85d58eaf5d3be965f408b515d2b.png)'
- en: 'Since the second model did not converge either, we can argue that it probably
    should have also returned an error, not an innocent warning. Logistic regression
    function in R, `glm(..., family=binomial)` does the same. To quote [R Inferno](https://www.burns-stat.com/pages/Tutor/R_inferno.pdf),
    Circle 5, Consistency:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 由于第二个模型也没有收敛，我们可以认为它可能也应该返回一个错误，而不是一个无害的警告。R 中的逻辑回归函数 `glm(..., family=binomial)`
    也是这样。引用 [R Inferno](https://www.burns-stat.com/pages/Tutor/R_inferno.pdf) 的第五圈，一致性：
- en: There is a problem with warnings. No one reads them. People have to read error
    messages because no food pellet falls into the tray after they push the button.
    With a warning the machine merely beeps at them but they still get their food
    pellet. Never mind that it might be poison.
  id: totrans-36
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 警告的问题在于，没有人会阅读它们。人们必须阅读错误信息，因为在按下按钮后没有食物颗粒掉入托盘中。而警告只会使机器发出警报，但他们仍然会得到食物颗粒。即使它可能是毒药也无所谓。
- en: Therefore, be careful when doing inference using effect sizes and p-values!
    In this case the data is perfectly separable — we have a perfect predictor — while
    the reported p-value (P > |z|) is 0.998\. Ignoring or misunderstanding these warnings
    may get you miss some obvious features in the data.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在使用效应大小和 p 值进行推断时要小心！在这种情况下，数据是完全可分的——我们有一个完美的预测器——而报告的 p 值（P > |z|）为 0.998。忽视或误解这些警告可能会使你错过数据中的一些明显特征。
- en: Alternatively, consider using a different model. There is a brave new world
    outside of logistic regression! 🌍
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 另外，考虑使用其他模型。逻辑回归之外还有一个崭新的世界！🌍
- en: 'Prediction: scikit-learn'
  id: totrans-39
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 预测：scikit-learn
- en: Sample data
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 样本数据
- en: scikit-learn automates and abstracts much of the calculations while providing
    a simple and consistent way to run many different models. However, in doing so,
    it hides a number of things happening in the background. When it is not obvious
    what is happening under the hood, learning data science concepts from scikit-learn
    documentation can lead to misunderstanding.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: scikit-learn 自动化和抽象了许多计算，同时提供了一种简单而一致的方式来运行许多不同的模型。然而，在这样做时，它隐藏了许多在后台发生的事情。当后台发生的事情不明显时，从
    scikit-learn 文档中学习数据科学概念可能会导致误解。
- en: 'One of these is misunderstandings is that when we run `LogisticRegression()`
    from `sklearn.linear_model` and use a `.predict()` method, it *makes it seem*
    as if we are running a classification model:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 其中一个误解是，当我们从 `sklearn.linear_model` 运行 `LogisticRegression()` 并使用 `.predict()`
    方法时，它*看起来像*我们在运行一个分类模型：
- en: '[PRE3]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: However, we can verify that this is still the same old logistic regression by
    either calling its `.predict_proba()` method or by digging out the model parameters
    from the model object, which we printed out here.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我们可以通过调用其 `.predict_proba()` 方法或从模型对象中挖掘模型参数来验证这仍然是老旧的逻辑回归，我们在这里打印出来了。
- en: 'Under the hood, `.predict()` predicts outcomes based on the larger of two probabilities:
    *p* or *1-p*. If *p > 1-p* then it predicts 1, otherwise it predicts 0\. It does
    not calculate confidence intervals, does not have p-values and does not do any
    kind of statistical inference or hypothesis testing. It is still a regression
    model, just with another thresholding layer on top of it.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在幕后，`.predict()` 根据两个概率中的较大者进行预测：*p* 或 *1-p*。如果 *p > 1-p*，则预测为 1，否则预测为 0。它不会计算置信区间，也没有
    p 值，也不会进行任何类型的统计推断或假设检验。它仍然是一个回归模型，只是在其上方有一个额外的阈值层。
- en: 'Note: `LogisticRegression()` by default uses L2 regularization, which adds
    an extra term to the log-loss function. To make it comparable with statsmodels,
    use `LogisticRegression(penalty=None)`.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：`LogisticRegression()` 默认使用 L2 正则化，它在对数损失函数中添加了一个额外的项。为了使其与 statsmodels 可比，使用
    `LogisticRegression(penalty=None)`。
- en: Perfectly separable data
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 完全可分的数据
- en: The advantage of ignoring the accuracy of fitted coefficients is that even if
    the coefficients are not accurate, or just plain wrong, the model can still be
    good enough! As the saying goes, “let not the perfect be the enemy of the good”.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 忽略拟合系数的准确性的好处在于，即使系数不准确或完全错误，模型仍然可能足够好！正如谚语所说，“不要让完美成为优秀的敌人”。
- en: 'scikit-learn by default uses a different fitting method (called “[lbfgs](https://en.wikipedia.org/wiki/Limited-memory_BFGS)”)
    than statsmodels. In my very limited set of tests, lbfgs did not error out on
    perfectly separated data. To see what happens here, let us run the same two datasets
    we used previously for statsmodels:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: scikit-learn 默认使用与 statsmodels 不同的拟合方法（称为 “[lbfgs](https://en.wikipedia.org/wiki/Limited-memory_BFGS)”）。在我非常有限的测试中，lbfgs
    在完全分离的数据上没有出现错误。为了查看这里发生了什么，让我们运行之前用于 statsmodels 的两个相同数据集：
- en: '[PRE4]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: This algorithm also stops at some point, giving finite values of *β₁* (or *k*),
    without an error, but it still predicts the correct outcomes. These are perfectly
    usable model fits for classification!
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 该算法在某个点也会停止，给出有限的 *β₁*（或 *k*）值，没有错误，但仍然能够预测正确的结果。这些都是用于分类的完全可用的模型拟合！
- en: Log-odds and loose ends
  id: totrans-52
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 对数几率和未解决的问题
- en: 'There is one last topic we did not touch too much. Coefficient *k* (or *β₁*)
    which multiplies *x*, has another interpretation — a *log odds ratio*. Odds ratio
    describes the multiplicative change in odds, when *x* increases by 1:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 还有一个我们没有深入讨论的话题。系数 *k*（或 *β₁*）乘以 *x*，有另一种解释——*对数几率比*。几率比描述了当 *x* 增加 1 时几率的乘法变化：
- en: '![](../Images/e8f72fc1b29d7d37a5de1eee98f6643a.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e8f72fc1b29d7d37a5de1eee98f6643a.png)'
- en: This definition states that **odds ratio is a ratio of ratios of probabilities**.
    If increasing *x* by one unit, increases the probability of *y = 1* from 0.1 (odds
    0.1 / 0.9 = 0.11) to 0.2 (odds = 0.2 / 0.8 = 0.25), that is represented by odds
    ratio of 0.25 / 0.1 = 2.27.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 这个定义表明**赔率比是概率比的比率**。如果将 *x* 增加一个单位，使得 *y = 1* 的概率从 0.1（赔率 0.1 / 0.9 = 0.11）增加到
    0.2（赔率 = 0.2 / 0.8 = 0.25），则赔率比为 0.25 / 0.1 = 2.27。
- en: When probabilities are small, large odds ratio does not reflect large absolute
    probabilities. If increasing *x* by one unit, increases the probability of *y
    = 1:*
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 当概率很小时，大的赔率比并不反映大的绝对概率。如果将 *x* 增加一个单位，使得 *y = 1:*
- en: from *p(x) = 0.0001* (odds = 0.0001 / (1–0.0001) ≈ 0.0001)
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从 *p(x) = 0.0001*（赔率 = 0.0001 / (1–0.0001) ≈ 0.0001）
- en: to *p(x + 1) = 0.001* (odds = 0.001 / (1–0.001) ≈ 0.001)
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 到 *p(x + 1) = 0.001*（赔率 = 0.001 / (1–0.001) ≈ 0.001）
- en: 'then this gives us the odds ratio of 0.001/0.0001 = 10\. Even though the odds
    ratio is 10, the final absolute probability is still very small: 0.001\. The good
    news is that **when probability is small, odds ratio becomes easier to interpret**
    — it is approximately equal to a ratio of two probabilities *p(x + 1) / p(x)*.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 那么这给出了赔率比为 0.001/0.0001 = 10。即使赔率比为 10，最终的绝对概率仍然很小：0.001。好消息是**当概率很小时，赔率比变得更容易解释**——它大致等于两个概率
    *p(x + 1) / p(x)* 的比率。
- en: 'Compare this to another example where probability increases by a substantial
    amount, say by 25% from *p(x) = 0.5* to *p(x + 1) = 0.75*: in this case odds ratio
    is only 3!'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 将此与另一个例子进行比较，其中概率大幅增加，例如从 *p(x) = 0.5* 增加到 *p(x + 1) = 0.75*，在这种情况下赔率比仅为 3！
- en: Therefore, high odds ratio needs to be used with a grain of salt when the probability
    of *y = 1* is small. There are alternatives to odds ratio, such as [relative risk](https://www.statology.org/interpret-relative-risk/)
    which are more interpretable, but may not be simple to calculate.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，当 *y = 1* 的概率很小的时候，高赔率比需要谨慎使用。还有其他可替代的赔率比，例如 [相对风险](https://www.statology.org/interpret-relative-risk/)，这些方法更具可解释性，但计算可能不简单。
- en: Conclusion
  id: totrans-62
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: In the last two posts, we gave an intuitive explanation to logistic regression
    and show how to run regression models in Python’s statsmodels and scikit-learn
    libraries.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在前两篇文章中，我们对逻辑回归进行了直观的解释，并展示了如何在 Python 的 statsmodels 和 scikit-learn 库中运行回归模型。
- en: 'Take home points:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 关键要点：
- en: Logistic regression outputs a probability — therefore it is a regression algorithm.
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 逻辑回归输出一个概率——因此它是一种回归算法。
- en: Logistic regression is often used for classification. For example, by predicting
    the outcome with a larger probability — or by setting a custom probability threshold.
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 逻辑回归通常用于分类。例如，通过预测具有更大概率的结果——或者通过设置自定义概率阈值。
- en: Parameters are estimated numerically using the difference between data and two
    **crossed hockey sticks log-loss curves that serve as a cost function**.
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 参数通过数据之间的差异和两个**交叉曲棍球棒对数损失曲线作为成本函数**的差异来进行数值估计。
- en: Use **statsmodels** to run a logistic regression when you are interested in
    statistical inference (care about accuracy of coefficients, hypothesis testing,
    p-values, ...).
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当你对统计推断感兴趣（关注系数的准确性、假设检验、p 值等）时，使用 **statsmodels** 进行逻辑回归。
- en: Use **scikit-learn** to run a logistic regression when you just want to predict
    the outcome or when you need to run a larger model.
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当你只是想预测结果或需要运行更大模型时，使用 **scikit-learn** 进行逻辑回归。
- en: 'Perfectly separable data breaks statistical inference: best-fit coefficients
    do not exist or “are infinite” and p-values will be large (because of large confidence
    intervals). If we only care about using the model for classification/prediction,
    then scikit-learn’s implementation works better for perfectly separated data.'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 完美可分的数据破坏了统计推断：最佳拟合系数不存在或“是无限的”，而 p 值将很大（因为置信区间很大）。如果我们只关心使用模型进行分类/预测，那么 scikit-learn
    的实现对完美分离的数据效果更好。
- en: '**Do not blindly trust large (log-)odds ratios**. Consider calculating additional
    metrics, such as absolute probabilities.'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**不要盲目相信大的（对数）赔率比**。考虑计算额外的指标，如绝对概率。'
- en: I hope this helps you in your data science journey!
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 希望这对你的数据科学之旅有所帮助！
