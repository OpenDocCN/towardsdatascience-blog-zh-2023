- en: Train your ML models on GPU changing just one line of code
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通过更改仅一行代码，在GPU上训练你的ML模型
- en: 原文：[https://towardsdatascience.com/train-your-ml-models-on-gpu-changing-just-one-line-of-code-5f8ff96a91ba?source=collection_archive---------12-----------------------#2023-03-20](https://towardsdatascience.com/train-your-ml-models-on-gpu-changing-just-one-line-of-code-5f8ff96a91ba?source=collection_archive---------12-----------------------#2023-03-20)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/train-your-ml-models-on-gpu-changing-just-one-line-of-code-5f8ff96a91ba?source=collection_archive---------12-----------------------#2023-03-20](https://towardsdatascience.com/train-your-ml-models-on-gpu-changing-just-one-line-of-code-5f8ff96a91ba?source=collection_archive---------12-----------------------#2023-03-20)
- en: Utilize cuML and ATOM to make your machine learning pipelines blazingly fast
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 利用cuML和ATOM让你的机器学习管道快速如闪电
- en: '[](https://tvdboom.medium.com/?source=post_page-----5f8ff96a91ba--------------------------------)[![Marco
    vd Boom](../Images/3fc053efda1c23dd84a6418ded2603ca.png)](https://tvdboom.medium.com/?source=post_page-----5f8ff96a91ba--------------------------------)[](https://towardsdatascience.com/?source=post_page-----5f8ff96a91ba--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----5f8ff96a91ba--------------------------------)
    [Marco vd Boom](https://tvdboom.medium.com/?source=post_page-----5f8ff96a91ba--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://tvdboom.medium.com/?source=post_page-----5f8ff96a91ba--------------------------------)[![Marco
    vd Boom](../Images/3fc053efda1c23dd84a6418ded2603ca.png)](https://tvdboom.medium.com/?source=post_page-----5f8ff96a91ba--------------------------------)[](https://towardsdatascience.com/?source=post_page-----5f8ff96a91ba--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----5f8ff96a91ba--------------------------------)
    [Marco vd Boom](https://tvdboom.medium.com/?source=post_page-----5f8ff96a91ba--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fe2091b627921&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftrain-your-ml-models-on-gpu-changing-just-one-line-of-code-5f8ff96a91ba&user=Marco+vd+Boom&userId=e2091b627921&source=post_page-e2091b627921----5f8ff96a91ba---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----5f8ff96a91ba--------------------------------)
    ·5 min read·Mar 20, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F5f8ff96a91ba&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftrain-your-ml-models-on-gpu-changing-just-one-line-of-code-5f8ff96a91ba&user=Marco+vd+Boom&userId=e2091b627921&source=-----5f8ff96a91ba---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fe2091b627921&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftrain-your-ml-models-on-gpu-changing-just-one-line-of-code-5f8ff96a91ba&user=Marco+vd+Boom&userId=e2091b627921&source=post_page-e2091b627921----5f8ff96a91ba---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----5f8ff96a91ba--------------------------------)
    ·5分钟阅读·2023年3月20日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F5f8ff96a91ba&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftrain-your-ml-models-on-gpu-changing-just-one-line-of-code-5f8ff96a91ba&user=Marco+vd+Boom&userId=e2091b627921&source=-----5f8ff96a91ba---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F5f8ff96a91ba&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftrain-your-ml-models-on-gpu-changing-just-one-line-of-code-5f8ff96a91ba&source=-----5f8ff96a91ba---------------------bookmark_footer-----------)![](../Images/47cda21ca8cd9c5fe8ecf85bd2b81072.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F5f8ff96a91ba&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftrain-your-ml-models-on-gpu-changing-just-one-line-of-code-5f8ff96a91ba&source=-----5f8ff96a91ba---------------------bookmark_footer-----------)![](../Images/47cda21ca8cd9c5fe8ecf85bd2b81072.png)'
- en: Photo by [Thomas Foster](https://unsplash.com/de/@thomasfos?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由 [Thomas Foster](https://unsplash.com/de/@thomasfos?utm_source=medium&utm_medium=referral)
    提供，来源于 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: Introduction
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 简介
- en: Graphics Processing Units (GPUs) can significantly accelerate calculations for
    preprocessing steps or training machine learning models. Training models typically
    involves compute-intensive matrix multiplications and other operations that can
    take advantage of a GPU’s massively parallel architecture. Training on large datasets
    can take hours to run on a single processor. However, if you offload those tasks
    to a GPU, you can reduce training time to minutes instead.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 图形处理单元 (GPU) 可以显著加速预处理步骤或训练机器学习模型的计算。训练模型通常涉及计算密集型矩阵乘法和其他可以利用 GPU 大规模并行架构的操作。在单个处理器上训练大型数据集可能需要几个小时。然而，如果将这些任务转移到
    GPU 上，你可以将训练时间减少到几分钟。
- en: In this story, we’ll show you how to use the [ATOM](https://github.com/tvdboom/ATOM)
    library to easily train your machine learning pipeline on a GPU. ATOM is an open-source
    Python package designed to help data scientists fasten the exploration of machine
    learning pipelines. Read [this story](/atom-a-python-package-for-fast-exploration-of-machine-learning-pipelines-653956a16e7b)
    if you want a gentle introduction to the library.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个故事中，我们将展示如何使用 [ATOM](https://github.com/tvdboom/ATOM) 库轻松在 GPU 上训练你的机器学习管道。ATOM
    是一个开源 Python 包，旨在帮助数据科学家加速机器学习管道的探索。如果你想了解库的温和介绍，请阅读 [这个故事](/atom-a-python-package-for-fast-exploration-of-machine-learning-pipelines-653956a16e7b)。
- en: Set up
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 设置
- en: ATOM uses [cuML](https://github.com/rapidsai/cuml) as backend library for GPU
    training. cuML is a suite of fast, GPU-accelerated machine learning algorithms
    designed for data science and analytical tasks. Unfortunately, cuML can not be
    installed through pip, and is therefore not installed as a dependency of ATOM.
    Read [here](https://docs.rapids.ai/install) how to install it.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: ATOM 使用 [cuML](https://github.com/rapidsai/cuml) 作为 GPU 训练的后端库。cuML 是一套快速、GPU
    加速的机器学习算法，旨在数据科学和分析任务中使用。不幸的是，cuML 不能通过 pip 安装，因此未作为 ATOM 的依赖项安装。请阅读 [这里](https://docs.rapids.ai/install)
    了解如何安装它。
- en: 'Requirements for cuML to take into account:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: cuML 的要求需要考虑：
- en: 'Operating System: Ubuntu 18.04/20.04 or CentOS 7/8 with gcc/++ 9.0+ or Windows
    10+ with WSL2'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 操作系统：Ubuntu 18.04/20.04 或 CentOS 7/8，使用 gcc/++ 9.0+，或 Windows 10+，使用 WSL2
- en: 'GPU: NVIDIA Pascal™ or better with [Compute capability](https://developer.nvidia.com/cuda-gpus)
    6.0+'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GPU：NVIDIA Pascal™ 或更高版本，具有 [计算能力](https://developer.nvidia.com/cuda-gpus) 6.0+
- en: 'Drivers: CUDA & NVIDIA Drivers of versions 11.0, 11.2, 11.4 or 11.5'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 驱动程序：CUDA 和 NVIDIA 驱动程序版本 11.0、11.2、11.4 或 11.5
- en: '**Tip:** See [this repo](https://github.com/rapidsai-community/rapids-smsl)
    to install cuML on [SageMaker Studio Lab](https://studiolab.sagemaker.aws/).'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '**提示：** 查看 [这个仓库](https://github.com/rapidsai-community/rapids-smsl) 以在 [SageMaker
    Studio Lab](https://studiolab.sagemaker.aws/) 上安装 cuML。'
- en: Example
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 示例
- en: Training transformers and models in atom using a GPU is as easy as
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用 GPU 的 atom 中训练变换器和模型就像
- en: 'initializing *atom* with parameter `device="gpu"`. The `device` parameter accepts
    any string that follows the [SYCL_DEVICE_FILTER](https://github.com/intel/llvm/blob/sycl/sycl/doc/EnvironmentVariables.md#sycl_device_filter)
    filter selector. Examples are:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 使用参数 `device="gpu"` 初始化 *atom*。`device` 参数接受任何遵循 [SYCL_DEVICE_FILTER](https://github.com/intel/llvm/blob/sycl/sycl/doc/EnvironmentVariables.md#sycl_device_filter)
    过滤器选择器的字符串。示例包括：
- en: device=”cpu” (use CPU)
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: device=”cpu” （使用 CPU）
- en: device=”gpu” (use default GPU)
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: device=”gpu” （使用默认 GPU）
- en: device=”gpu:0" (use first GPU)
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: device=”gpu:0" （使用第一个 GPU）
- en: device=”gpu:1" (use second GPU)
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: device=”gpu:1" （使用第二个 GPU）
- en: '**Note:** ATOM does not support multi-GPU training. If there is more than one'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意：** ATOM 不支持多GPU训练。如果有多个'
- en: GPU on the machine and the `device` parameter does not specify which
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 机器上的 GPU 和 `device` 参数未指定使用哪个
- en: one to use, the first one is used by default.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 一个可用时，默认使用第一个。
- en: Use the `engine` parameter to choose between the cuML and sklearnex execution
    engines. In this story, we will focus on cuML. The [XGBoost](https://tvdboom.github.io/ATOM/v5.1/API/models/xgb/),
    [LightGBM](https://tvdboom.github.io/ATOM/v5.1/API/models/lgb/) and [CatBoost](https://tvdboom.github.io/ATOM/v5.1/API/models/catb/)
    models come with their own GPU engine. Setting device=”gpu” is sufficient to accelerate
    them with GPU, regardless of the engine parameter. Click [here](https://tvdboom.github.io/ATOM/v5.1/user_guide/accelerating/#supported-estimators_1)
    for an overview of the transformers and models that support GPU acceleration.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`engine`参数在cuML和sklearnex执行引擎之间进行选择。在这个故事中，我们将专注于cuML。[XGBoost](https://tvdboom.github.io/ATOM/v5.1/API/models/xgb/)、[LightGBM](https://tvdboom.github.io/ATOM/v5.1/API/models/lgb/)和[CatBoost](https://tvdboom.github.io/ATOM/v5.1/API/models/catb/)模型配备了自己的GPU引擎。设置device="gpu"即可通过GPU加速这些模型，无论engine参数如何。点击[这里](https://tvdboom.github.io/ATOM/v5.1/user_guide/accelerating/#supported-estimators_1)查看支持GPU加速的变换器和模型概述。
- en: '**Tip:** If you don’t have access to a GPU, you can use online cloud services
    like [Google Colab](https://colab.research.google.com/) or [Sagemaker Studio Lab](https://studiolab.sagemaker.aws/)
    to try it out. Make sure to choose the GPU compute type. See [this notebook](https://studiolab.sagemaker.aws/import/github/tvdboom/ATOM/blob/master/examples/accelerating_cuml.ipynb)
    to get you started.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '**提示：** 如果你没有GPU访问权限，可以使用在线云服务，如[Google Colab](https://colab.research.google.com/)或[Sagemaker
    Studio Lab](https://studiolab.sagemaker.aws/)来尝试。请确保选择GPU计算类型。查看[这个笔记本](https://studiolab.sagemaker.aws/import/github/tvdboom/ATOM/blob/master/examples/accelerating_cuml.ipynb)以开始使用。'
- en: Let’s get started with the example.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始这个例子。
- en: '[PRE0]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '![](../Images/55acf33fb5b7d0954041f4858c0ca1db.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/55acf33fb5b7d0954041f4858c0ca1db.png)'
- en: Not only models, but also transformers can benefit from GPU acceleration. For
    example, to scale the features to mean=0 and std=1.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 不仅模型，变换器也可以从GPU加速中受益。例如，将特征缩放到mean=0和std=1。
- en: '[PRE1]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '![](../Images/8c063e8ed093c866d452bc5a70af3b9b.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8c063e8ed093c866d452bc5a70af3b9b.png)'
- en: Since we stated that we want to use the cuML engine, ATOM automatically selects
    the transformer from that library whenever available.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们声明要使用cuML引擎，ATOM会在有可用时自动从该库中选择变换器。
- en: '[PRE2]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '![](../Images/c9ddb00955323c040c092a1c234a8b0f.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c9ddb00955323c040c092a1c234a8b0f.png)'
- en: 'Let’s train three models: the [Random Forest](https://tvdboom.github.io/ATOM/v5.1/API/models/rf/)
    is available in cuML, the [Stochastig Gradient Descent](https://tvdboom.github.io/ATOM/v5.1/API/models/sgd/)
    is not, and [XGBoost](https://tvdboom.github.io/ATOM/v5.1/API/models/xgb/) has
    its own GPU implementation.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们训练三个模型：[随机森林](https://tvdboom.github.io/ATOM/v5.1/API/models/rf/)在cuML中可用，[随机梯度下降](https://tvdboom.github.io/ATOM/v5.1/API/models/sgd/)不可用，[XGBoost](https://tvdboom.github.io/ATOM/v5.1/API/models/xgb/)有自己的GPU实现。
- en: '[PRE3]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '![](../Images/33a1e0871c3a4ffb7d8b45b5e06c1e5f.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/33a1e0871c3a4ffb7d8b45b5e06c1e5f.png)'
- en: '[PRE4]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '![](../Images/c33a9703e165fc7293d750a134ff0b51.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c33a9703e165fc7293d750a134ff0b51.png)'
- en: Note the massive difference in training time between the models!
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 注意模型之间训练时间的巨大差异！
- en: If we check the underlying estimators, we’ll see that the the RF model was indeed
    trained on GPU, the SGD wasn’t (since it’s not available on cuML, ATOM falls back
    to the default sklearn implementation), and the XGB model did train on GPU using
    its native module.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们检查底层估算器，会发现RF模型确实是在GPU上训练的，SGD没有（因为它在cuML中不可用，ATOM回退到默认的sklearn实现），而XGB模型确实使用其本地模块在GPU上训练。
- en: '[PRE5]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '![](../Images/361899e33da3925a1afae09cd6ebc382.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/361899e33da3925a1afae09cd6ebc382.png)'
- en: Lastly, analyzing the results is as easy as always.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，分析结果像往常一样简单。
- en: '[PRE6]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '![](../Images/02f1c905f80dca2a6d6bc8c29145edf8.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/02f1c905f80dca2a6d6bc8c29145edf8.png)'
- en: Conclusion
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结论
- en: We have shown how to use the ATOM package to train your machine learning pipelines
    on GPU. ATOM is also capable of acceleration on CPU. Read [this story](https://medium.com/towards-data-science/make-your-sklearn-models-up-to-100-times-faster-563bb682665e)
    to learn how.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经展示了如何使用ATOM包在GPU上训练你的机器学习管道。ATOM也能在CPU上加速。阅读[这个故事](https://medium.com/towards-data-science/make-your-sklearn-models-up-to-100-times-faster-563bb682665e)了解如何操作。
- en: For further information about ATOM, have a look at the package’s [documentation](https://tvdboom.github.io/ATOM/).
    For bugs or feature requests, don’t hesitate to open an issue on [GitHub](https://github.com/tvdboom/ATOM)
    or send me an email.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 欲了解更多关于ATOM的信息，请查看该包的[文档](https://tvdboom.github.io/ATOM/)。对于错误或功能请求，请随时在[GitHub](https://github.com/tvdboom/ATOM)上提出问题或发邮件给我。
- en: 'References:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 参考文献：
- en: All plots and images (except the featured image) are created by the author.
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所有图表和图片（除封面图外）均由作者创建。
- en: 'Related stories:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 相关故事：
- en: '[](/atom-a-python-package-for-fast-exploration-of-machine-learning-pipelines-653956a16e7b?source=post_page-----5f8ff96a91ba--------------------------------)
    [## ATOM: A Python package for fast exploration of machine learning pipelines'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '[ATOM: 一个用于快速探索机器学习管道的 Python 包](https://towardsdatascience.com/atom-a-python-package-for-fast-exploration-of-machine-learning-pipelines-653956a16e7b?source=post_page-----5f8ff96a91ba--------------------------------)'
- en: Automated Tool for Optimized Modelling (ATOM) is an open-source Python package
    designed to help data scientists perform…
  id: totrans-59
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 自动化优化建模工具（ATOM）是一个开源 Python 包，旨在帮助数据科学家进行…
- en: towardsdatascience.com](/atom-a-python-package-for-fast-exploration-of-machine-learning-pipelines-653956a16e7b?source=post_page-----5f8ff96a91ba--------------------------------)
    [](/exploration-of-deep-learning-pipelines-made-easy-e1cf649892bc?source=post_page-----5f8ff96a91ba--------------------------------)
    [## Exploration of Deep Learning pipelines made easy
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '[深度学习管道探索变得简单](https://towardsdatascience.com/exploration-of-deep-learning-pipelines-made-easy-e1cf649892bc?source=post_page-----5f8ff96a91ba--------------------------------)'
- en: A simple guide on how to use the right package to perform fast DL experimentations
    in Python
  id: totrans-61
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 如何使用正确的包在 Python 中进行快速深度学习实验的简单指南
- en: towardsdatascience.com](/exploration-of-deep-learning-pipelines-made-easy-e1cf649892bc?source=post_page-----5f8ff96a91ba--------------------------------)
    [](/make-your-sklearn-models-up-to-100-times-faster-563bb682665e?source=post_page-----5f8ff96a91ba--------------------------------)
    [## Make your sklearn models up to 100 times faster
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '[使你的 sklearn 模型快上 100 倍](https://towardsdatascience.com/make-your-sklearn-models-up-to-100-times-faster-563bb682665e?source=post_page-----5f8ff96a91ba--------------------------------)'
- en: How to considerable reduce training time changing only 1 line of code
  id: totrans-63
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 如何通过仅更改一行代码显著减少训练时间
- en: towardsdatascience.com](/make-your-sklearn-models-up-to-100-times-faster-563bb682665e?source=post_page-----5f8ff96a91ba--------------------------------)
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '[使你的 sklearn 模型快上 100 倍](https://towardsdatascience.com/make-your-sklearn-models-up-to-100-times-faster-563bb682665e?source=post_page-----5f8ff96a91ba--------------------------------)'
