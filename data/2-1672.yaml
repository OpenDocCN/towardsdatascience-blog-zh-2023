- en: Pre-Training Context is All You Need
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 预训练上下文是你所需的一切
- en: 原文：[https://towardsdatascience.com/pre-training-context-is-all-you-need-f457ffa8a358](https://towardsdatascience.com/pre-training-context-is-all-you-need-f457ffa8a358)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/pre-training-context-is-all-you-need-f457ffa8a358](https://towardsdatascience.com/pre-training-context-is-all-you-need-f457ffa8a358)
- en: The driving force behind modern transformer models stems to a large extent from
    its pertaining data, allowing for strong in-context learning capabilities.
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 现代变换模型背后的驱动力在很大程度上来源于其相关数据，从而允许强大的上下文学习能力。
- en: '[](https://medium.com/@benjamin.thuerer?source=post_page-----f457ffa8a358--------------------------------)[![Benjamin
    Thürer](../Images/b4c49698c7270c592bf992fc47f75765.png)](https://medium.com/@benjamin.thuerer?source=post_page-----f457ffa8a358--------------------------------)[](https://towardsdatascience.com/?source=post_page-----f457ffa8a358--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----f457ffa8a358--------------------------------)
    [Benjamin Thürer](https://medium.com/@benjamin.thuerer?source=post_page-----f457ffa8a358--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@benjamin.thuerer?source=post_page-----f457ffa8a358--------------------------------)[![本杰明·图尔雷](../Images/b4c49698c7270c592bf992fc47f75765.png)](https://medium.com/@benjamin.thuerer?source=post_page-----f457ffa8a358--------------------------------)[](https://towardsdatascience.com/?source=post_page-----f457ffa8a358--------------------------------)[![走向数据科学](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----f457ffa8a358--------------------------------)
    [本杰明·图尔雷](https://medium.com/@benjamin.thuerer?source=post_page-----f457ffa8a358--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----f457ffa8a358--------------------------------)
    ·6 min read·Nov 27, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----f457ffa8a358--------------------------------)
    ·6 分钟阅读·2023年11月27日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: Generative Artificial Intelligence and its popular transformer models are advertised
    everywhere these days and new models are being released every hour (see [the inflation
    of AI](https://medium.com/towards-data-science/the-inflation-of-ai-is-more-always-better-8ea1be75e0aa)).
    In this rapidly evolving field of AI, the possibilities of values these models
    could bring seem to be endless. Large Language Models (LLM) like [chatGPT](http://chat.openai.com)
    already made it into every Engineers' pile of resources, writers use them to support
    their articles, and designers create the first visuals or seek inspiration from
    the outcome of computer vision models.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 生成型人工智能及其流行的变换模型如今随处可见，每小时都有新模型发布（见[人工智能的膨胀](https://medium.com/towards-data-science/the-inflation-of-ai-is-more-always-better-8ea1be75e0aa)）。在这个迅速发展的人工智能领域，这些模型可能带来的价值似乎是无穷无尽的。像[chatGPT](http://chat.openai.com)这样的“大型语言模型”（LLM）已经成为每位工程师资源的组成部分，作家们用它们来支持他们的文章，设计师们则用计算机视觉模型的结果来创建初步的视觉效果或寻找灵感。
- en: If it is not magic, what really powers these impressive transformer models?
  id: totrans-7
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 如果这不是魔法，那究竟是什么驱动了这些令人印象深刻的变换模型？
- en: 'However, even though the achievements and usefulness are great and generative
    AI enhances productivity, it is important to recall that modern Machine Learning
    models (like LLMs or VisionTransformers) are not performing any magic at all (similar
    to the fact that ML, or statistical models in general, never have been magical).
    Even though the remarkable abilities of models might be perceived as *magic-like*
    and some experts in the field even talk about things like *hallucinations* of
    models, still, the foundation of every model is just math and statistical probabilities
    (sometimes complex, but still math). This leads to the fundamental question: If
    it is not magic, what really powers these impressive transformer models?'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，即使成就和实用性都很出色，生成型人工智能提升了生产力，但重要的是要记住，现代机器学习模型（如 LLM 或 Vision Transformers）并没有进行任何魔法（类似于
    ML 或统计模型本身从未有过神奇的事实）。尽管模型的卓越能力可能被视为*类似魔法*，一些领域专家甚至谈到模型的*幻觉*，但每个模型的基础仍然只是数学和统计概率（虽然有时复杂，但仍是数学）。这引出了一个根本性的问题：如果这不是魔法，那究竟是什么驱动了这些令人印象深刻的变换模型？
- en: '![](../Images/034acf90a6d7f81e53b4a668d97bd141.png)'
  id: totrans-9
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/034acf90a6d7f81e53b4a668d97bd141.png)'
- en: 'Figure 1: Showcasing that ChatGPT (using GPT4) points towards its “advanced
    technology” and “extensive training” as the main performance drivers.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：展示了 ChatGPT（使用 GPT4）将其“先进技术”和“广泛训练”视为主要性能驱动因素。
- en: The Fundament of Every Model is Data
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 每个模型的基础是数据
- en: 'As with any model (statistical or ML), it is the training data that has the
    largest impact on the later model performance. If you don’t have a high volume
    of quality data reflecting the relationships you would like the model to learn,
    there is *nothing* to train on and the resulting model will perform poorly (the
    famous GIGO principle: Garbage In Garbage Out). This fundamental principle of
    data modeling has not changed at all over the years. Behind every revolutionary
    new transformer model stands first of all just one thing: **data**. It is the
    **amount**, **quality**, and **context** of that data that will drive the subsequent
    performance of the model. Recent studies (see further below) support this by showcasing
    that the latest generative AI models generalize well when the provided context
    is part of the training distribution but poorly for out-of-distribution learning.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 与任何模型（统计模型或机器学习模型）一样，训练数据对模型的最终性能影响最大。如果没有大量高质量的数据来反映你希望模型学习的关系，那么就*没有*可以训练的内容，最终的模型表现会很差（著名的GIGO原则：垃圾进垃圾出）。这一数据建模的基本原则多年来没有改变。每个革命性的新型变换模型背后首先只有一个因素：**数据**。是数据的**数量**、**质量**和**上下文**决定了模型的后续表现。近期研究（见下文）通过展示最新的生成性人工智能模型在提供的上下文属于训练分布时能够很好地进行泛化，而在异质分布学习中表现不佳来支持这一点。
- en: In-Distribution vs. Out-Of-Distribution Learning
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 同质分布与异质分布学习
- en: It is important to keep in mind that a model is nothing else than a huge network,
    tree, or graph of relationships. What an ML model basically learns is how to transform
    a given input into a desired output (see Figure 2).
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是要记住，模型不过是一个巨大的网络、树或关系图。机器学习模型基本上学习的是如何将给定的输入转换为所需的输出（见图2）。
- en: '![](../Images/9181ca0727d831aefc6a5acce38a704b.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9181ca0727d831aefc6a5acce38a704b.png)'
- en: 'Figure 2: Schema of a super simple neural network predicting foot traffic based
    on weather and other context. On the left-hand side is the input during training
    (features) while on the right-hand side is the output (target). In between can
    be several transformations (layers) which learn a complex input-output relationship.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：一个超级简单的神经网络结构图，它根据天气和其他上下文预测人流量。在左侧是训练过程中的输入（特征），而右侧是输出（目标）。在它们之间可以有多个转换（层），这些层学习复杂的输入输出关系。
- en: 'When the model is trained (or in other words: when those relationships are
    updated), the context of the input and its informativeness of the output will
    define what the model is good at. Similar to humans being good at responding to
    questions in their native language, ML models are good at responding to input
    data they have seen a lot. That is called **in-distribution** Learning. If, during
    training, the model has been provided with large amounts of rich context, it can
    rely on this acquired knowledge later and the resulting predictions show an accurate
    performance.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 当模型被训练时（换句话说：当这些关系被更新时），输入的上下文和输出的信息量将决定模型的擅长领域。类似于人类能够很好地回答母语中的问题，机器学习模型在处理它们见过的数据时表现较好。这被称为**同质分布**学习。如果在训练过程中，模型接收到大量丰富的上下文，它可以依赖于这种获得的知识，最终的预测结果会显示出准确的表现。
- en: '**Out-of-distribution** learning, however, describes the situation where a
    model is supposed to predict based on context that it has *not* seen before. You
    can picture a human who never learned Norwegian suddenly responding to a question
    asked in Norwegian. Please inspect Figure 3 for an overview of in- and out-of-distribution
    learning.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，**异质分布**学习描述的是模型需要基于它*未曾*见过的上下文进行预测的情况。你可以想象一个从未学过挪威语的人突然回应用挪威语提出的问题。请查看图3以了解同质分布和异质分布学习的概况。
- en: '![](../Images/9271f018d29e44fd241a22c72b9e8a15.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9271f018d29e44fd241a22c72b9e8a15.png)'
- en: 'Figure 3: Showing in-distribution (left) vs. out-of-distribution (right) Learning.
    The model on the left-hand side performs poorly for new context that was **not**
    part of the original training data (in this case “Politics”), while the model
    on the right-hand side does perform well for unseen context. ML models generally
    fall under the left category and perform poorly for out-of-distribution learning.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：展示了同质分布（左）与异质分布（右）学习。左侧的模型在面对不属于原始训练数据的新的上下文（在这种情况下是“政治”）时表现较差，而右侧的模型在处理未见过的上下文时表现良好。机器学习模型通常属于左侧类别，并且在异质分布学习中表现不佳。
- en: The impressive performance of modern LLMs and other ML models comes from a vast
    volume and amount of context in the original training data. Due to this extensive
    pre-training of models, the range of questions that can fall inside of in-distribution
    learning is huge. That allows a model to have answers to a variety of questions
    and that might appear to a user as *magical* or as *human-level intelligence,*
    but it is not. Similarly, a wrong or unexpected answer by the model is also not
    a true *hallucination*, it basically highlights context gaps in the original training
    data and, thus, leads to out-of-distribution learning. In general, machine learning
    models are very limited in their out-of-distribution learning capabilities, requiring
    extensive training for foundational models.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 现代LLMs和其他ML模型的出色性能来源于原始训练数据中的大量上下文。由于模型的广泛预训练，能够处理的内分布学习的问题范围非常广泛。这使得模型能够回答各种问题，这可能对用户而言看起来像是*魔法*或*人类级智能*，但实际上并非如此。同样，模型的错误或意外回答也并不是真正的*幻觉*，它基本上突出了原始训练数据中的上下文缺口，从而导致分布外学习。总的来说，机器学习模型在其分布外学习能力上非常有限，需要对基础模型进行广泛训练。
- en: The Power of Pretraining in Language Models
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 语言模型中的预训练力量
- en: In a recent paper by Google DeepMind members, the authors strengthen the argument
    that the in-context learning performance of modern LLMs is mostly derived from
    their pre-training distribution. The paper “[Pretraining Data Mixtures Enable
    Narrow Model Selection Capabilities in Transformer Models](https://doi.org/10.48550/arXiv.2311.00871)”
    by Steve Yadlowsky, Lyric Doshi, and Nilesh Tripuraneni (2023) focuses on how
    modern transformer models, acquire their impressive in-context learning abilities
    (their abilities to have answers for any context prompted to them).
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在谷歌DeepMind成员最近的一篇论文中，作者进一步支持了现代LLMs的上下文学习性能主要源自其预训练分布的观点。论文“[Pretraining Data
    Mixtures Enable Narrow Model Selection Capabilities in Transformer Models](https://doi.org/10.48550/arXiv.2311.00871)”由**Steve
    Yadlowsky**、**Lyric Doshi**和**Nilesh Tripuraneni**（2023）撰写，专注于现代变换器模型如何获得其令人印象深刻的上下文学习能力（即应对任何提供的上下文的能力）。
- en: '[](https://doi.org/10.48550/arXiv.2311.00871?source=post_page-----f457ffa8a358--------------------------------)
    [## Pretraining Data Mixtures Enable Narrow Model Selection Capabilities in Transformer
    Models'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://doi.org/10.48550/arXiv.2311.00871?source=post_page-----f457ffa8a358--------------------------------)
    [## 预训练数据混合使变换器模型具备狭窄的模型选择能力'
- en: Transformer models, notably large language models (LLMs), have the remarkable
    ability to perform in-context learning…
  id: totrans-25
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 变换器模型，特别是大型语言模型（LLMs），具有在上下文中学习的卓越能力…
- en: doi.org](https://doi.org/10.48550/arXiv.2311.00871?source=post_page-----f457ffa8a358--------------------------------)
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: doi.org](https://doi.org/10.48550/arXiv.2311.00871?source=post_page-----f457ffa8a358--------------------------------)
- en: The findings are very insightful. When transformer models are pre-trained on
    data covering a wide range of contexts, they demonstrate an impressive performance
    in learning new tasks that fall within the pre-training context. This capability
    is near-optimal, showcasing an impressive degree of generalization and adaptability
    within the training distribution. However, when these models encounter context
    outside of their pre-training domain, the performance is limited and failures
    occur. This showcases a reduced generalization and clear limitations for out-of-distribution
    contexts.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 这些发现非常有启发性。当变换器模型在覆盖广泛上下文的数据上进行预训练时，它们在学习新的任务时表现出令人印象深刻的能力，这些任务都属于预训练上下文。这种能力接近最优，展示了在训练分布内的显著泛化和适应能力。然而，当这些模型遇到超出其预训练领域的上下文时，性能受到限制且会出现失败。这突显了在分布外上下文中的减少泛化能力和明显限制。
- en: 'Vision Transformers: A Case Study in Scale'
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 视觉变换器：规模案例研究
- en: 'In another study (also by Google DeepMind in 2023) with the title: “[ConvNets
    Match Vision Transformers at Scale](https://doi.org/10.48550/arXiv.2310.16764)”
    the authors Samuel L. Smith, Andrew Brock, Leonard Berrada, and Soham De challenge
    a widespread belief for computer vision that, at scale, modern Vision Transformer
    models outperform traditional models like Convolutional Neural Networks (CNNs).
    The study trains both CNNs and Vision Transformers with a similar computing budget
    and compares their performance.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在另一项研究中（也是由谷歌DeepMind于2023年进行）题为：“[ConvNets Match Vision Transformers at Scale](https://doi.org/10.48550/arXiv.2310.16764)”的论文中，作者**Samuel
    L. Smith**、**Andrew Brock**、**Leonard Berrada**和**Soham De**挑战了一个在计算机视觉领域广泛存在的观念，即在大规模下，现代的视觉变换器模型超越了传统的模型，如卷积神经网络（CNNs）。该研究使用类似的计算预算训练了CNNs和视觉变换器，并比较了它们的性能。
- en: '[](https://doi.org/10.48550/arXiv.2310.16764?source=post_page-----f457ffa8a358--------------------------------)
    [## ConvNets Match Vision Transformers at Scale'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '[## ConvNets 在规模上匹敌视觉 Transformer](https://doi.org/10.48550/arXiv.2310.16764?source=post_page-----f457ffa8a358--------------------------------)'
- en: Many researchers believe that ConvNets perform well on small or moderately sized
    datasets, but are not competitive with…
  id: totrans-31
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 许多研究人员认为，卷积神经网络（ConvNets）在小型或中型数据集上表现良好，但在更大的数据集上不具竞争力……
- en: doi.org](https://doi.org/10.48550/arXiv.2310.16764?source=post_page-----f457ffa8a358--------------------------------)
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '[doi.org](https://doi.org/10.48550/arXiv.2310.16764?source=post_page-----f457ffa8a358--------------------------------)'
- en: The results indicate a scaling law between the compute budget used for pretraining
    and the subsequent performance. After fine-tuning on ImageNet, the pre-trained
    CNNs matched the performance of Vision Transformers at comparable budgets.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 结果表明，预训练所使用的计算预算与随后的性能之间存在规模法则。在 ImageNet 上进行微调后，预训练的 CNN 在相当的预算下达到了与视觉 Transformer
    相匹配的性能。
- en: Summary
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: Together, these two studies show an interesting picture of the impressive performance
    of modern Transformer models. First, the performance is not just driven by the
    model architecture, it is more driven by the amount of pre-training conducted.
    Second, when the pre-training context covers a wide range, the resulting model
    will also show a wide range of in-context learning capabilities.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 这两项研究共同描绘了现代 Transformer 模型令人印象深刻的性能。首先，性能不仅仅受到模型架构的驱动，更主要是由预训练的量驱动。其次，当预训练的上下文涵盖广泛时，生成的模型也会展示出广泛的上下文学习能力。
- en: These studies underscore the critical principle that volume, quality, and context
    of the training data is the most essential part of any foundational ML model.
    Without knowing the context covered by the pre-training it is hard to determine
    upfront the areas in which a model will perform well. Benchmark tests can help
    indicate potential context caps. Those tests do not showcase how good a model
    performs in general, they mostly showcase which context has been part of the model's
    training distribution.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 这些研究强调了一个关键原则：训练数据的量、质量和上下文是任何基础机器学习模型最关键的部分。在不知道预训练涵盖的上下文的情况下，很难事先确定模型在哪些领域表现良好。基准测试可以帮助指示潜在的上下文限制。这些测试并不展示模型的一般性能，它们主要展示了哪些上下文曾经成为模型训练分布的一部分。
- en: In conclusion, as it is the age of AI and the number of Data Scientists and
    Engineers developing ML models is increasing, it is becoming even more evident
    that pre-training with a wide range of contexts isn’t just a part of the process;
    in many ways, it’s all you need.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，随着人工智能时代的到来和数据科学家及工程师在机器学习模型开发中的人数增加，预训练涵盖广泛上下文的重要性变得越来越明显；在许多方面，它不仅是过程的一部分，而且是你所需要的全部。
- en: '*All images, unless otherwise noted, are by the author.*'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '*所有图像，除非另有说明，均由作者提供。*'
- en: '***Please check out*** [***my profile page***](https://medium.com/@benjamin.thuerer/about)***,
    follow me, or*** [***subscribe to my email list***](https://medium.com/@benjamin.thuerer/subscribe)
    ***if you would like to know what I write about or if you want to be updated on
    new stories.***'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '***请查看*** [***我的个人主页***](https://medium.com/@benjamin.thuerer/about)***，关注我，或***
    [***订阅我的邮件列表***](https://medium.com/@benjamin.thuerer/subscribe) ***，如果你想了解我写的内容或希望及时获得新故事的更新。***'
