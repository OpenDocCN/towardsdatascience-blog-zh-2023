- en: How I Turned ChatGPT into an SQL-Like Translator for Image and Video Datasets
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 我是如何将 ChatGPT 转变为类似 SQL 的图像和视频数据集翻译器
- en: 原文：[https://towardsdatascience.com/how-i-turned-chatgpt-into-an-sql-like-translator-for-image-and-video-datasets-7b22b318400a?source=collection_archive---------1-----------------------#2023-06-08](https://towardsdatascience.com/how-i-turned-chatgpt-into-an-sql-like-translator-for-image-and-video-datasets-7b22b318400a?source=collection_archive---------1-----------------------#2023-06-08)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/how-i-turned-chatgpt-into-an-sql-like-translator-for-image-and-video-datasets-7b22b318400a?source=collection_archive---------1-----------------------#2023-06-08](https://towardsdatascience.com/how-i-turned-chatgpt-into-an-sql-like-translator-for-image-and-video-datasets-7b22b318400a?source=collection_archive---------1-----------------------#2023-06-08)
- en: A process of prompt engineering, software engineering, trial and error, and
    elbow grease
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 这是一个涉及提示工程、软件工程、试验和错误、以及辛勤工作的过程
- en: '[](https://medium.com/@jacob_marks?source=post_page-----7b22b318400a--------------------------------)[![Jacob
    Marks, Ph.D.](../Images/94d9832b8706d1044e3195386613bfab.png)](https://medium.com/@jacob_marks?source=post_page-----7b22b318400a--------------------------------)[](https://towardsdatascience.com/?source=post_page-----7b22b318400a--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----7b22b318400a--------------------------------)
    [Jacob Marks, Ph.D.](https://medium.com/@jacob_marks?source=post_page-----7b22b318400a--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@jacob_marks?source=post_page-----7b22b318400a--------------------------------)[![Jacob
    Marks, Ph.D.](../Images/94d9832b8706d1044e3195386613bfab.png)](https://medium.com/@jacob_marks?source=post_page-----7b22b318400a--------------------------------)[](https://towardsdatascience.com/?source=post_page-----7b22b318400a--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----7b22b318400a--------------------------------)
    [Jacob Marks, Ph.D.](https://medium.com/@jacob_marks?source=post_page-----7b22b318400a--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ff7dc0c0eae92&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-i-turned-chatgpt-into-an-sql-like-translator-for-image-and-video-datasets-7b22b318400a&user=Jacob+Marks%2C+Ph.D.&userId=f7dc0c0eae92&source=post_page-f7dc0c0eae92----7b22b318400a---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----7b22b318400a--------------------------------)
    ·21 min read·Jun 8, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F7b22b318400a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-i-turned-chatgpt-into-an-sql-like-translator-for-image-and-video-datasets-7b22b318400a&user=Jacob+Marks%2C+Ph.D.&userId=f7dc0c0eae92&source=-----7b22b318400a---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ff7dc0c0eae92&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-i-turned-chatgpt-into-an-sql-like-translator-for-image-and-video-datasets-7b22b318400a&user=Jacob+Marks%2C+Ph.D.&userId=f7dc0c0eae92&source=post_page-f7dc0c0eae92----7b22b318400a---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----7b22b318400a--------------------------------)
    ·21分钟阅读·2023年6月8日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F7b22b318400a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-i-turned-chatgpt-into-an-sql-like-translator-for-image-and-video-datasets-7b22b318400a&user=Jacob+Marks%2C+Ph.D.&userId=f7dc0c0eae92&source=-----7b22b318400a---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F7b22b318400a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-i-turned-chatgpt-into-an-sql-like-translator-for-image-and-video-datasets-7b22b318400a&source=-----7b22b318400a---------------------bookmark_footer-----------)![](../Images/4a82e88fc3efdca9105841c859778660.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F7b22b318400a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-i-turned-chatgpt-into-an-sql-like-translator-for-image-and-video-datasets-7b22b318400a&source=-----7b22b318400a---------------------bookmark_footer-----------)![](../Images/4a82e88fc3efdca9105841c859778660.png)'
- en: VoxelGPT using natural language to query an image dataset. Image courtesy of
    the author.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: VoxelGPT 使用自然语言查询图像数据集。图片由作者提供。
- en: Unlike tabular data, datasets for *computer vision tasks* are unstructured —
    think gobs of pixels, heaps of labels, bags of tags, and some sometimes-structured
    metadata. Nevertheless, those of us working in computer vision still need to be
    able to efficiently sift through those troves of data in order to *understand*
    the dataset, prepare train and test sets, find patterns, identify edge cases,
    and evaluate model performance.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 与表格数据不同，*计算机视觉任务*的数据集是非结构化的——可以想象成一堆像素、大量标签、标签包以及一些有时结构化的元数据。然而，那些从事计算机视觉工作的人仍然需要能够高效地筛选这些数据宝库，以*理解*数据集，准备训练集和测试集，发现模式，识别边缘案例，并评估模型性能。
- en: When I need to understand visual data, (which is basically all the time), I
    use the open source library FiftyOne, which defines a powerful Python syntax for
    querying computer vision data. It’s kind of like SQL for the unstructured world
    of computer vision data, and it allows me to programmatically filter, sort, and
    semantically slice datasets consisting of images, videos, and even 3D point clouds.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 当我需要理解视觉数据时（这基本上是一直需要的），我使用开源库 FiftyOne，它定义了一种强大的 Python 语法来查询计算机视觉数据。这有点像计算机视觉数据的
    SQL，它允许我以编程方式筛选、排序和语义切片由图像、视频甚至 3D 点云组成的数据集。
- en: 'A few months back, with the ChatGPT craze in full swing, I saw an example application
    on OpenAI’s website of [translating natural language input into SQL queries](https://platform.openai.com/examples/default-sql-translate).
    While this application was quite basic, and computer vision data is *much* more
    complex, it got me thinking: could something similar be done for datasets of images
    and videos? In other words:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 几个月前，随着 ChatGPT 热潮的全面爆发，我在 OpenAI 的网站上看到一个例子应用，[将自然语言输入转换为 SQL 查询](https://platform.openai.com/examples/default-sql-translate)。虽然这个应用程序相当基础，而计算机视觉数据则*复杂得多*，但这让我思考：是否可以为图像和视频数据集做类似的事情？换句话说：
- en: '*Can we harness the versatility of large language models (LLMs) to translate
    natural language queries into filtered views of unstructured computer vision datasets?*'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '*我们能否利用大型语言模型 (LLM) 的多功能性，将自然语言查询转换为对非结构化计算机视觉数据集的过滤视图？*'
- en: The answer? Yes we can!
  id: totrans-13
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 答案是？是的，我们可以！
- en: 'Combining the general linguistic and inference capabilities of LLMs with FiftyOne’s
    query language, our team at Voxel51 built [**VoxelGPT**](https://github.com/voxel51/voxelgpt):
    an open source AI Assistant which lets you comprehensively query your computer
    vision datasets *without writing a single line of code*!'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 将 LLM 的通用语言和推理能力与 FiftyOne 的查询语言相结合，我们在 Voxel51 的团队构建了 [**VoxelGPT**](https://github.com/voxel51/voxelgpt)：一个开源
    AI 助手，可以让你*无需编写一行代码*即可全面查询计算机视觉数据集！
- en: We did it using [langchain](https://github.com/hwchase17/langchain), [tiktoken](https://github.com/openai/tiktoken),
    and [fiftyone](https://github.com/voxel51/fiftyone).
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用了 [langchain](https://github.com/hwchase17/langchain)、[tiktoken](https://github.com/openai/tiktoken)
    和 [fiftyone](https://github.com/voxel51/fiftyone) 完成了这个任务。
- en: You can try it for free at [gpt.fiftyone.ai](https://gpt.fiftyone.ai)!
  id: totrans-16
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 你可以在 [gpt.fiftyone.ai](https://gpt.fiftyone.ai) 免费试用！
- en: This post will walk you through the prompt engineering, software engineering,
    and copious trial and error that went into building a domain-specific LLM application.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 这篇文章将带你了解构建领域特定 LLM 应用程序的提示工程、软件工程和大量试错过程。
- en: Many times throughout the process, we hit a wall, and feared that the task was
    infeasible. If you are trying and struggling to build an LLM-powered application,
    I hope this post gives you the inspiration you need to break through your own
    walls!
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个过程中，我们多次遇到瓶颈，并担心任务是否不可行。如果你正在尝试并挣扎于构建一个 LLM 驱动的应用程序，我希望这篇文章能给你突破自己困境的灵感！
- en: 'The post is structured as follows:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 这篇文章的结构如下：
- en: '[SQL-like queries for images and videos](#6677)'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[类似 SQL 的图像和视频查询](#6677)'
- en: '[Defining the overarching task](#2386)'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[定义总体任务](#2386)'
- en: '[Giving the model context](#ccee)'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[为模型提供上下文](#ccee)'
- en: '[Generating and utilizing examples](#61fc)'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[生成和利用示例](#61fc)'
- en: '[Breaking down the problem](#547d)'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[拆解问题](#547d)'
- en: '[Getting to production](#62a5)'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[投入生产](#62a5)'
- en: The Query Language
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 查询语言
- en: '![](../Images/77fa4667a6edcd4ac1d4c93c8e576db4.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/77fa4667a6edcd4ac1d4c93c8e576db4.png)'
- en: VoxelGPT using natural language to query an image dataset. Image courtesy of
    the author.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: VoxelGPT 使用自然语言查询图像数据集。图片由作者提供。
- en: Before we dive into how we used an LLM to generate queries, it’s worth taking
    a moment to describe the query language we want the model to translate. Here’s
    the bare bones info you will need. For a more thorough overview, see the [FiftyOne
    User Guide](https://docs.voxel51.com/user_guide/basics.html). If you’re already
    familiar with the language, feel free to skip to the next section.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们深入讨论如何使用LLM生成查询之前，值得花一点时间描述一下我们希望模型翻译的查询语言。以下是你需要的基本信息。有关更全面的概述，请参阅[FiftyOne用户指南](https://docs.voxel51.com/user_guide/basics.html)。如果你已经熟悉这些语言，可以直接跳到下一部分。
- en: '**Dataset:** A `Dataset` is the computer vision analog of a `Table` in SQL,
    or a `DataFrame` in pandas. It contains all of the information related to a collection
    of media files.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据集：** `Dataset`是SQL中的`Table`或pandas中的`DataFrame`的计算机视觉类比。它包含与一组媒体文件相关的所有信息。'
- en: '**Sample**: A `Sample` is analogous to a row in a data table. Samples are the
    atomic elements of a `Dataset`. Each sample has a `filepath`, which points to
    a media file, and stores all other info associated with that piece of data.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**样本：** `Sample`类似于数据表中的一行。样本是`Dataset`的基本元素。每个样本都有一个`filepath`，指向一个媒体文件，并存储与该数据片段相关的所有其他信息。'
- en: '**Field:** A `Field` is analogous to a column in a data table in that it defines
    a property of samples (for example, image width, height, and filepath). However,
    fields are flexible in that they can contain other fields (see `Label` below).'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**字段：** `Field`类似于数据表中的列，因为它定义了样本的属性（例如，图像宽度、高度和文件路径）。然而，字段是灵活的，它们可以包含其他字段（见下文的`Label`）。'
- en: '**Label:** A `Label` is a `Field` that stores semantic ground truth or prediction
    info. For instance, object detections are stored in `Detections` label fields,
    and classifications are stored in `Classification` label fields.'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**标签：** `Label`是一个存储语义真实或预测信息的`Field`。例如，对象检测存储在`Detections`标签字段中，而分类存储在`Classification`标签字段中。'
- en: 'As in SQL or pandas, you can filter your data with query operations. For example,
    you may want to query a data table for:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 与SQL或pandas一样，你可以使用查询操作来过滤数据。例如，你可能想要查询数据表以获取：
- en: '*All rows with an entry in column “A” greater than 0.5*'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '*所有在“A”列中条目大于0.5的行*'
- en: 'However, whereas data tables lend themselves to numeric slicing and filtering,
    unstructured computer vision data lends itself to *semantic slicing*, such as:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，与数据表适合进行数字切片和过滤不同，非结构化计算机视觉数据更适合进行*语义切片*，例如：
- en: '*Retrieving the high confidence “dog” predictions with large bounding boxes
    in images with at least 3 non-”dog” ground truth detections*'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '*在至少有3个非“狗”地面真实检测的图像中，检索高置信度的“大”边界框“狗”预测*'
- en: Semantically slicing unstructured data requires far more flexibility.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 对非结构化数据进行语义切片需要更大的灵活性。
- en: To achieve this level of flexibility, FiftyOne defines a set of `ViewStage`methods,
    which encapsulate allowed query operations like filtering, matching, selecting,
    sorting, and excluding. These methods are loosely analogous to `SELECT`, `WHERE`,
    and `ORDER BY` in SQL, but because the space of queries is much larger, there
    are far more methods, and each of these methods has many use cases. See this [Views
    Cheat Sheet](https://docs.voxel51.com/cheat_sheets/views_cheat_sheet.html) for
    a comprehensive discussion.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现这种灵活性，FiftyOne定义了一组`ViewStage`方法，这些方法封装了允许的查询操作，如过滤、匹配、选择、排序和排除。这些方法大致类似于SQL中的`SELECT`、`WHERE`和`ORDER
    BY`，但由于查询的空间更大，方法数量也多得多，每个方法都有许多使用场景。有关详细讨论，请参见这个[视图备忘单](https://docs.voxel51.com/cheat_sheets/views_cheat_sheet.html)。
- en: You can obtain an arbitrary subset of a `Dataset`, called a `DatasetView`, by
    composing multiple `ViewStage` operations in sequence.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过将多个`ViewStage`操作按顺序组合来获取`Dataset`的任意子集，这个子集称为`DatasetView`。
- en: 'Here’s what the query language looks like in action: given a dataset named
    `my_dataset`, if we wanted to get all “dog” detections in the `"predictions”`
    label field of 10 random images in that contain a “cat” (in their `"ground_truth”`
    label field), we could use the following query:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是查询语言在实际操作中的样子：给定一个名为`my_dataset`的数据集，如果我们想要获取所有在包含“猫”的10张随机图片的`"predictions"`标签字段中的“狗”检测，我们可以使用以下查询：
- en: '[PRE0]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '`match()` restricts to images with a “cat” in their `"ground_truth”` detections
    field, then `filter_labels()` reaches inside the `"predictions”` field and retrieves
    only the “dog” detections, and finally `take()` randomly selects a subset.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '`match()`限制为在`"ground_truth"`检测字段中包含“猫”的图像，然后`filter_labels()`深入到`"predictions"`字段中，仅检索“狗”检测，最后`take()`随机选择一个子集。'
- en: Defining the Task
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 定义任务
- en: 'When we started this journey, we didn’t know what was possible with LLMs. Back
    in December 2022, I had played around with ChatGPT and even wrote an article [Tunnel
    Vision in Computer Vision: Can ChatGPT See?](https://medium.com/voxel51/tunnel-vision-in-computer-vision-can-chatgpt-see-e6ef037c535)
    probing the chatbot’s understanding of computer vision concepts. At the time,
    it was impressive that a general purpose language model could write *mostly* correct
    Python code for basic image processing tasks, but the thought of using GPT-generated
    code (*without human verification*) in computer vision applications seemed far-fetched.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们开始这个旅程时，我们并不知道LLMs能做什么。早在2022年12月，我曾玩过ChatGPT，并撰写了一篇文章[计算机视觉中的隧道视野：ChatGPT能看见吗？](https://medium.com/voxel51/tunnel-vision-in-computer-vision-can-chatgpt-see-e6ef037c535)，探讨了聊天机器人对计算机视觉概念的理解。当时，一个通用语言模型能为基本的图像处理任务编写*大部分*正确的Python代码已经令人印象深刻，但在计算机视觉应用中使用GPT生成的代码（*没有人工验证*）的想法似乎不切实际。
- en: After seeing the text-to-SQL example, and reading about GPT-4 reportedly passing
    the BAR (among other exams) with flying colors, I decided to give GPT-for-CV another
    try. I asked GPT-4 to generate the Python code for a few simple queries and was
    pleasantly surprised by its responses. The veracity of GPT-4’s out-of-the-box
    results left much to be desired, which was understandable given that *I had not
    fed in any information about the FiftyOne library*. But the prospects of making
    this work were tantalizing.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在看到文本到SQL的示例后，并阅读了GPT-4 reportedly以优异的成绩通过BAR考试（以及其他考试）的消息后，我决定再次尝试GPT-for-CV。我让GPT-4生成了一些简单查询的Python代码，对其回应感到惊喜。GPT-4的即用结果的真实性还有待提高，这是可以理解的，因为*我没有提供任何关于FiftyOne库的信息*。但使其有效的前景令人兴奋。
- en: Require Uniformity
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 需要统一性
- en: As we experimented with prompting techniques (we’ll get to that later), it became
    immediately apparent that if we wanted to incorporate this into larger pipelines,
    we’d need to *clearly define the task*.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们实验不同的提示技术时（我们稍后会讨论），如果我们想将其纳入更大的管道中，立刻就显现出我们需要*明确任务*。
- en: Left to its own devices, the GPT model’s responses varied substantially in their
    format from prompt to prompt.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 如果任由其自行运作，GPT模型的回应在格式上会因提示而异。
- en: 'Here are just two examples, which we will analyze shortly:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 这里只是两个例子，我们将很快进行分析：
- en: '**Prompt 1:**'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '**提示 1:**'
- en: '*Your task is to convert input natural language queries into Python code to
    generate ViewStages for the computer vision library FiftyOne.*'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '*你的任务是将输入的自然语言查询转换为Python代码，以生成计算机视觉库FiftyOne的ViewStages。*'
- en: '*Here is your first natural language query: “Images that only contain dogs”*'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '*这是你的第一个自然语言查询：“仅包含狗的图像”*'
- en: '*Give me the FiftyOne code.*'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '*给我FiftyOne代码。*'
- en: '**Response 1:**'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '**回应 1:**'
- en: '*Sure, here’s the FiftyOne code that generates a ViewStage for images that
    only contain dogs:*'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '*当然，这里是生成仅包含狗的图像的FiftyOne代码：*'
- en: '[PRE1]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '*This code loads the Open Images v6 validation dataset with only the “detections”
    label type for dogs. It then creates a view stage that filters out any images
    that do not have any dog detections. Finally, the view stage is applied to the
    dataset and launched in the FiftyOne App.*'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '*这段代码加载了Open Images v6验证数据集，仅包含“检测”标签类型的狗。然后，它创建了一个视图阶段，过滤掉没有狗检测的图像。最后，将视图阶段应用到数据集中，并在FiftyOne应用中启动。*'
- en: '**Prompt 2:**'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '**提示 2:**'
- en: '*Your task is to convert input natural language queries into Python code to
    generate ViewStages for the machine learning library FiftyOne.*'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '*你的任务是将输入的自然语言查询转换为Python代码，以生成FiftyOne机器学习库的ViewStages。*'
- en: '*Here is your first natural language query: “most unique images with a false
    positive detection”*'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '*这是你的第一个自然语言查询：“最独特的假阳性检测图像”*'
- en: '*Give me the FiftyOne code.*'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '*给我FiftyOne代码。*'
- en: '**Response 2:**'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '**回应 2:**'
- en: '*Sure, here’s the FiftyOne code to generate ViewStages for the natural language
    query “most unique images with a false positive detection” using the FiftyOne
    library in Python:*'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '*当然，这里是使用FiftyOne库在Python中生成“最独特的假阳性检测图像”自然语言查询的FiftyOne代码：*'
- en: '[PRE2]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '*In this code, we first load the FiftyOne library and any necessary dependencies.
    We then load your dataset using fo.load_dataset().*'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '*在这段代码中，我们首先加载FiftyOne库和任何必要的依赖项。然后，我们使用fo.load_dataset()加载你的数据集。*'
- en: '*Next, we define your false positive detection model using foz.load_zoo_model().
    This function loads a pre-trained model from the FiftyOne Zoo Model Zoo, which
    includes a wide range of state-of-the-art models for computer vision.*'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '*接下来，我们使用foz.load_zoo_model()定义你的假阳性检测模型。该函数从FiftyOne Zoo Model Zoo加载一个预训练模型，其中包括广泛的计算机视觉最先进模型。*'
- en: '*After that, we define our pipeline using the $set, $match, $group, $count,
    and $sort aggregation stages. The pipeline first adds a field to each sample in
    your dataset that contains the model predictions using $set. We then filter the
    samples to only include those with a false positive detection using $match.*'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '*之后，我们使用$set、$match、$group、$count和$sort聚合阶段定义我们的管道。管道首先使用$set为数据集中每个样本添加一个包含模型预测的字段。然后，我们使用$match过滤样本，仅包括那些检测到假阳性的样本。*'
- en: '*Next, we group the samples by their file paths using $group, and count the
    number of images in each group using $count. Finally, we sort the groups in descending
    order of the number of images using $sort.*'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '*接下来，我们使用$group按文件路径对样本进行分组，并使用$count计算每个组中的图像数量。最后，我们使用$sort按图像数量的降序对组进行排序。*'
- en: '*Finally, we run the pipeline on your dataset and limit the results to the
    top 10 images with the most false positive detections using limit(). We then display
    the results in a view stage using display().*'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '*最后，我们在你的数据集上运行管道，并使用limit()将结果限制为检测到最多假阳性的前10张图像。然后，我们使用display()在视图阶段展示结果。*'
- en: Even though the prompt for these two examples was structured in the same way,
    the responses differed in a few key ways. Response 1 attempts to create a `DatasetView`
    by adding `ViewStage` to the dataset. Response 2 defines and applies a [MongoDB
    aggregation pipeline](https://www.mongodb.com/docs/manual/core/aggregation-pipeline/),
    followed by the `limit()` method (applying `Limit` stage) to limit the view to
    10 samples, as well as a non-existent (AKA hallucinated) `display()` method. Additionally,
    while Response 1 loads in an actual dataset (Open Images V6), Response 2 is effectively
    template code, as `"your_dataset_name"` and `"your_model_name”` need to be filled
    in.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这两个示例的提示结构相同，但响应在几个关键方面有所不同。响应1尝试通过向数据集添加`ViewStage`来创建`DatasetView`。响应2定义并应用了一个[MongoDB聚合管道](https://www.mongodb.com/docs/manual/core/aggregation-pipeline/)，随后使用`limit()`方法（应用`Limit`阶段）将视图限制为10个样本，并使用了一个不存在（即幻觉）`display()`方法。此外，尽管响应1加载了一个实际的数据集（Open
    Images V6），响应2实际上是模板代码，因为`"your_dataset_name"`和`"your_model_name"`需要填写。
- en: 'These examples also highlighted the following issues:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 这些示例还突出了以下问题：
- en: '**Boilerplate code**: some responses contained code for importing modules,
    instantiating datasets (and models), and visualizing the view (`session = fo.launch_app(dataset)`).'
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**模板代码**：一些响应包含了导入模块、实例化数据集（和模型）以及可视化视图（`session = fo.launch_app(dataset)`）的代码。'
- en: '**Explanatory text**: in many cases — including educational contexts — the
    fact that the model explains its “reasoning” is a positive. If we want to perform
    queries on the user’s behalf, however, this explanatory text just gets in the
    way. Some queries even resulted in multiple code blocks, split up by text.'
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**解释性文本**：在许多情况下——包括教育背景——模型解释其“推理”是一个积极的。如果我们希望代表用户执行查询，这些解释性文本就会成为障碍。一些查询甚至导致多个被文本分隔的代码块。'
- en: 'What we really wanted was for the LLM to respond with code that could be copied
    and pasted into a Python process, without all of the extra baggage. As a first
    attempt at prompting the model, I started to give the following text as prefix
    to any natural language query I wanted it to translate:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 我们真正想要的是让LLM回应可以直接复制粘贴到Python进程中的代码，而不附带额外的负担。作为对模型进行提示的第一次尝试，我开始将以下文本作为前缀添加到任何自然语言查询中：
- en: '[PRE3]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Crucially, I defined a *task*, and set *rules*, instructing the model what it
    was allowed and not allowed to do.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 关键是，我定义了一个*任务*，并设置了*规则*，指导模型允许和不允许做的事情。
- en: '*Note: with responses coming in a more uniform format, it was at this point
    that I moved from the ChatGPT chat interface to using GPT-4 via OpenAI’s API.*'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '*注意：随着响应格式的更加统一，我在这时从ChatGPT聊天界面转向使用OpenAI的API中的GPT-4。*'
- en: Limiting Scope
  id: totrans-79
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 限制范围
- en: Our team also decided that, at least to start, we would limit the scope of what
    we were asking the LLM to do. While the fiftyone query language itself is full-bodied,
    asking a pre-trained model to do arbitrarily complex tasks without any fine-tuning
    is a recipe for disappointment. Start simple, and iteratively add in complexity.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的团队还决定，至少在开始时，我们将限制我们要求 LLM 执行的范围。尽管 fiftyone 查询语言本身功能全面，但要求预训练模型在没有任何微调的情况下执行任意复杂的任务是令人失望的。开始时要简单，然后逐步增加复杂性。
- en: 'For this experiment, we imposed the following bounds:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 对于此实验，我们施加了以下限制：
- en: '**Just images and videos**: don’t expect the LLM to query 3D point clouds or
    grouped datasets.'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**仅限图像和视频**：不要指望 LLM 查询 3D 点云或分组数据集。'
- en: '**Ignore fickle** `**ViewStages**`: most `ViewStages` abide by the same basic
    rules, but a few buck the trend. `Concat` is the only `ViewStages`` that takes
    in a second `DatasetView`; `Mongo` uses MongoDB Aggregation syntax; `GeoNear`
    has a `query` argument, which takes in a `fiftyone.utils.geojson.geo_within()`
    object; and `GeoWithin` requires a 2D array to define the region to which the
    “within” applies. We decided to ignore `Concat`, `Mongo`, and `GeoWithin`, and
    to support all `GeoNear` usage *except* for the `query` argument.'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**忽略变化的** `**ViewStages**`：大多数 `ViewStages` 遵循相同的基本规则，但有一些例外。`Concat` 是唯一一个接受第二个
    `DatasetView` 的 `ViewStages`；`Mongo` 使用 MongoDB 聚合语法；`GeoNear` 具有一个 `query` 参数，该参数接受一个
    `fiftyone.utils.geojson.geo_within()` 对象；而 `GeoWithin` 需要一个 2D 数组来定义“within”适用的区域。我们决定忽略
    `Concat`、`Mongo` 和 `GeoWithin`，并支持所有 `GeoNear` 的使用 *除了* `query` 参数。'
- en: '**Stick to two stages**: while it would be great for the model to compose an
    arbitrary number of stages, in most workflows I’ve seen, one or two `ViewStages`
    suffice to create the desired `DatasetView`. The goal of this project was not
    to get caught in the weeds, but to build something useful for computer vision
    practitioners.'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**坚持两个阶段**：虽然模型能够组合任意数量的阶段是很好的，但在我见过的大多数工作流程中，一两个 `ViewStages` 就足以创建所需的 `DatasetView`。该项目的目标不是陷入细节，而是为计算机视觉从业者构建一些有用的东西。'
- en: Providing Context
  id: totrans-85
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 提供上下文
- en: '![](../Images/b6238ba569dacd151b120983c5bafd56.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b6238ba569dacd151b120983c5bafd56.png)'
- en: VoxelGPT using natural language to query an image dataset. Image courtesy of
    the author.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: VoxelGPT 使用自然语言查询图像数据集。图片由作者提供。
- en: In addition to giving the model an explicit “task” and providing clear instructions,
    we found that we could improve performance by giving the model more information
    about how FiftyOne’s query language works. Without this information, the LLM is
    flying blind. It is just grasping, reaching out into the darkness.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 除了给模型一个明确的“任务”和提供清晰的指示外，我们发现通过提供有关FiftyOne查询语言如何工作的更多信息，可以提高性能。如果没有这些信息，LLM
    就像在黑暗中盲目飞行，只是在抓取和伸手进入黑暗中。
- en: For example, in Prompt 2, when I asked for false positive predictions, the response
    attempted to reference these false positives with `predictions.mistakes.false_positive`.
    As far as ChatGPT was concerned, this seemed like a reasonable way to store and
    access information about false positives.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在提示 2 中，当我要求获取假阳性预测时，响应尝试用`predictions.mistakes.false_positive`来引用这些假阳性。就
    ChatGPT 而言，这似乎是存储和访问假阳性信息的合理方法。
- en: 'The model didn’t know that in FiftyOne, the truth/falsity of detection predictions
    is evaluated with `dataset.evaluate_detections()` and after running said evaluation,
    you can retrieve all images with a false positive by matching for `eval_fp>0`
    with:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 模型不知道在 FiftyOne 中，检测预测的真实性/虚假性是通过 `dataset.evaluate_detections()` 评估的，运行该评估后，你可以通过匹配
    `eval_fp>0` 来检索所有具有假阳性的图像：
- en: '[PRE4]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'I tried to clarify the task by providing additional rules, such as:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 我尝试通过提供额外的规则来澄清任务，例如：
- en: '[PRE5]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'I also provided information about label types:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 我还提供了有关标签类型的信息：
- en: '[PRE6]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Additionally, while by providing the model with a list of allowed view stages,
    I was able to nudge it towards using them, it didn’t know
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，虽然通过提供允许的视图阶段列表，我能够推动模型使用这些阶段，但它并不知道
- en: '*When* a given stage was relevant, or'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*当*给定阶段相关时，或者'
- en: '*How* to use the stage in a syntactically correct manner'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*如何*以语法正确的方式使用阶段'
- en: To fill this gap, I wanted to give the LLM information about each of the view
    stages. I wrote code to loop through view stages (which you can list with `fiftyone.list_view_stages()`),
    store the docstring, and then split the text of the docstring into description
    and inputs/arguments.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 为了填补这一空白，我想给 LLM 提供有关每个视图阶段的信息。我编写了代码来遍历视图阶段（你可以使用 `fiftyone.list_view_stages()`
    列出这些阶段），存储文档字符串，然后将文档字符串的文本拆分为描述和输入/参数。
- en: '*However, I soon ran into a problem: context length.*'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '*然而，我很快遇到了一个问题：上下文长度。*'
- en: Using the base GPT-4 model via the OpenAI API, I was already bumping up against
    the 8,192 token context length. And this was before adding in examples, or any
    information about the dataset itself!
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 使用OpenAI API的基础GPT-4模型时，我已经接近了8,192令牌的上下文长度。这是在添加示例或任何有关数据集的信息之前！
- en: OpenAI does have a GPT-4 model with a 32,768 token context which in theory I
    could have used, but a back-of-the-envelope calculation convinced me that this
    could get expensive. If we filled the entire 32k token context, given [OpenAI’s
    pricing](https://openai.com/pricing), it would cost about $2 per query!
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI确实有一个具有32,768令牌上下文的GPT-4模型，理论上我可以使用，但一个粗略的计算让我相信这可能会很贵。如果我们填满了整个32k令牌上下文，根据[OpenAI的定价](https://openai.com/pricing)，每次查询大约需要2美元！
- en: 'Instead, our team rethought our approach and did the following:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，我们的团队重新考虑了我们的方法，并进行了以下操作：
- en: Switch to GPT-3.5
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 切换到GPT-3.5
- en: Minimize token count
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最小化令牌数
- en: Be more selective with input info
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更加挑剔地选择输入信息
- en: Switching to GPT-3.5
  id: totrans-107
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 切换到GPT-3.5
- en: There’s no such thing as a free lunch — this did lead to slightly lower performance,
    at least initially. Over the course of the project, we were able to recover and
    far surpass this through prompt engineering! In our case, the effort was worth
    the cost savings. In other cases, it might not be.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 没有什么是免费的——这确实导致了稍微较低的性能，至少在初期。在项目过程中，我们通过提示工程恢复并远远超越了这一点！在我们的案例中，这个努力是值得的。在其他情况下，可能就不是这样了。
- en: Minimizing Token Count
  id: totrans-109
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 最小化令牌数
- en: 'With context length becoming a limiting factor, I employed the following simple
    trick: *use ChatGPT to optimize prompts!*'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 由于上下文长度成为限制因素，我使用了以下简单的技巧：*使用ChatGPT优化提示！*
- en: One `ViewStage` at a time, I took the original description and list of inputs,
    and fed this information into ChatGPT, along with a prompt *asking the LLM to
    minimize the token count* of that text while *retaining all semantic information*.
    Using [tiktoken](https://github.com/openai/tiktoken) to count the tokens in the
    original and compressed versions, I was able to reduce the number of tokens by
    about 30%.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 一次处理一个`ViewStage`，我将原始描述和输入列表提供给ChatGPT，并附上一个*要求LLM最小化文本的令牌数*的提示，同时*保留所有语义信息*。使用[tiktoken](https://github.com/openai/tiktoken)来计算原始和压缩版本中的令牌数，我能够将令牌数减少约30%。
- en: Being More Selective
  id: totrans-112
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 更加挑剔
- en: While it’s great to provide the model with context, some information is more
    helpful than other information, depending on the task at hand. If the model only
    needs to generate a Python query involving two `ViewStages`, it probably won’t
    benefit terribly from information about what inputs the other `ViewStages` take.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管提供上下文对模型很有帮助，但某些信息比其他信息更有用，具体取决于任务。如果模型仅需要生成涉及两个`ViewStages`的Python查询，它可能不会从其他`ViewStages`的输入信息中获得太大帮助。
- en: We knew that we needed a way to select relevant information depending on the
    input natural language query. However, it wouldn’t be as simple as performing
    a similarity search on the descriptions and input parameters, because the former
    often comes in very different language than the latter. We needed a way to link
    input and information selection.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 我们知道需要一种方法来根据输入的自然语言查询选择相关信息。然而，这不会像在描述和输入参数上执行相似性搜索那样简单，因为前者通常以与后者非常不同的语言出现。我们需要一种将输入和信息选择联系起来的方法。
- en: That link, as it turns out, was *examples*.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 结果发现，那个链接是*示例*。
- en: Examples
  id: totrans-116
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 示例
- en: Generating Examples
  id: totrans-117
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 生成示例
- en: If you’ve ever played around with ChatGPT or another LLM, you’ve probably experienced
    first-hand how providing the model with even just a single relevant example can
    drastically improve performance.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你曾经使用过ChatGPT或其他LLM，你可能亲身体验过，即使只提供一个相关的示例，也能大幅提高性能。
- en: 'As a starting point, I came up with 10 completely synthetic examples and passed
    these along to GPT-3.5 by adding this below the task rules and `ViewStage` descriptions
    in my input prompt:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 作为起点，我想出了10个完全合成的示例，并将这些示例通过在任务规则和`ViewStage`描述下方添加以下内容传递给GPT-3.5：
- en: 'Here are a few examples of Input-Output Pairs in A, B form:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是A、B形式的输入-输出对的一些示例：
- en: '[PRE7]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: With just these 10 examples, there was a noticeable improvement in the quality
    of the model’s responses, so our team decided to be systematic about it.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 仅凭这10个示例，模型响应的质量有了显著提高，因此我们的团队决定对此采取系统化的方式。
- en: First, we combed through our docs, finding any and all examples of views created
    through combinations of `ViewStages`.
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们仔细检查了我们的文档，找出了通过`ViewStages`组合创建的所有视图示例。
- en: We then went through the list of `ViewStages` and added examples so that we
    had as close to complete coverage as possible over usage syntax. To this, we made
    sure that there was at least one example for each argument or keyword, to give
    the model a pattern to follow.
  id: totrans-124
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们查看了`ViewStages`的列表，并添加了示例，以便尽可能全面地覆盖用法语法。为此，我们确保每个参数或关键字至少有一个示例，以便为模型提供一个可遵循的模式。
- en: With usage syntax covered, we varied the names of fields and classes in the
    examples so that the model wouldn’t generate any false assumptions about names
    correlating with stages. For instance, we don’t want the model to strongly associate
    the “person” class with the `match_labels()` method just because all of the examples
    for `match_labels()` happen to include a “person” class.
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在覆盖了用法语法之后，我们更改了示例中字段和类的名称，以防止模型对名称与阶段之间的关联产生错误的假设。例如，我们不希望模型仅仅因为所有`match_labels()`的示例都包含“person”类，就强烈关联“person”类与`match_labels()`方法。
- en: Selecting Similar Examples
  id: totrans-126
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 选择类似的示例
- en: At the end of this example generation process, we already had hundreds of examples
    — far more than could fit in the context length. Fortunately, these examples contained
    (as input) natural language queries that we could directly compare with the user’s
    input natural language query.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例生成过程结束时，我们已经有了数百个示例——远远超过了可以容纳的上下文长度。幸运的是，这些示例包含了（作为输入）自然语言查询，我们可以直接与用户的自然语言查询进行比较。
- en: To perform this comparison, we pre-computed embeddings for these example queries
    with OpenAI’s [text-embedding-ada–002](https://openai.com/blog/new-and-improved-embedding-model)
    model. At run-time, the user’s query is embedded with the same model, and the
    examples with the most similar natural language queries — by cosine distance —
    are selected. Initially, we used [ChromaDB](https://www.trychroma.com/) to construct
    an in-memory vector database. However, given that we were dealing with hundreds
    or thousands of vectors, rather than hundreds of thousands or millions, it actually
    made more sense to switch to an exact vector search (plus we limited dependencies).
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进行这种比较，我们使用OpenAI的[text-embedding-ada–002](https://openai.com/blog/new-and-improved-embedding-model)模型预计算了这些示例查询的嵌入。在运行时，用户的查询会使用相同的模型进行嵌入，然后选择与自然语言查询最相似的示例——通过余弦距离来确定。最初，我们使用了[ChromaDB](https://www.trychroma.com/)来构建内存中的向量数据库。然而，由于我们处理的是数百或数千个向量，而不是数十万或数百万个向量，实际上转而使用精确向量搜索更为合理（此外，我们还减少了依赖）。
- en: It was becoming difficult to manage these examples and the components of the
    prompt, so it was at this point that we started to use [LangChain’s Prompts module](https://python.langchain.com/en/latest/modules/prompts.html).
    Initially, we were able to use their [Similarity ExampleSelector](https://python.langchain.com/en/latest/modules/prompts/example_selectors/examples/similarity.html)
    to select the most relevant examples, but eventually we had to write a custom
    `ExampleSelector` so that we had more control over the pre-filtering.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 管理这些示例和提示的组件变得越来越困难，因此我们在这时开始使用[LangChain的Prompts模块](https://python.langchain.com/en/latest/modules/prompts.html)。最初，我们能够使用他们的[相似性示例选择器](https://python.langchain.com/en/latest/modules/prompts/example_selectors/examples/similarity.html)来选择最相关的示例，但最终我们不得不编写一个自定义的`ExampleSelector`，以便对预过滤有更多控制。
- en: Filtering for Appropriate Examples
  id: totrans-130
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 过滤适当的示例
- en: 'In the computer vision query language, the appropriate syntax for a query can
    depend on the media type of the samples in the dataset: videos, for example, sometimes
    need to be treated differently than images. Rather than confuse the model by giving
    seemingly conflicting examples, or complicating the task by forcing the model
    to infer based on media type, we decided to only give examples that would be syntactically
    correct for a given dataset. In the context of vector search, this is known as
    [pre-filtering](https://weaviate.io/developers/weaviate/concepts/prefiltering).'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 在计算机视觉查询语言中，查询的适当语法可能取决于数据集中样本的媒体类型：例如，视频有时需要与图像不同地处理。为了避免通过给出看似矛盾的示例而使模型感到困惑，或通过迫使模型基于媒体类型推断而使任务复杂化，我们决定仅给出对给定数据集语法正确的示例。在向量搜索的上下文中，这被称为[预过滤](https://weaviate.io/developers/weaviate/concepts/prefiltering)。
- en: 'This idea worked so well that we eventually applied the same considerations
    to other features of the dataset. In some cases, the differences were merely syntactic
    — when querying labels, the syntax for accessing a `Detections` label is different
    from that of a `Classification` label. Other filters were more strategic: sometimes
    we didn’t want the model to know about a certain feature of the query language.'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 这个想法效果很好，以至于我们最终将相同的考虑应用于数据集的其他特性。在某些情况下，差异仅仅是语法上的——在查询标签时，访问`Detections`标签的语法与访问`Classification`标签的语法不同。其他过滤器则更具战略性：有时我们不希望模型了解查询语言的某些特性。
- en: For instance, we didn’t want to give the LLM examples utilizing computations
    it would not have access to. If a text similarity index had not been constructed
    for a specific dataset, it would not make sense to feed the model examples of
    searching for the best visual matches to a natural language query. In a similar
    vein, if the dataset did not have any evaluation runs, then querying for true
    positives and false positives would yield either errors or null results.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，我们不想给LLM提供使用它无法访问的计算的示例。如果没有为特定数据集构建文本相似性索引，则向模型提供查找与自然语言查询最佳视觉匹配的示例是没有意义的。类似地，如果数据集中没有任何评估运行，则查询真实正例和假阳性将产生错误或空结果。
- en: You can see the complete example pre-filtering pipeline in [view_stage_example_selector.py](https://github.com/voxel51/voxelgpt/blob/main/links/view_stage_example_selector.py)
    in the GitHub repo.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在GitHub仓库中的[view_stage_example_selector.py](https://github.com/voxel51/voxelgpt/blob/main/links/view_stage_example_selector.py)中查看完整的示例预处理管道。
- en: Choosing Contextual Info Based on Examples
  id: totrans-135
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 根据示例选择上下文信息
- en: For a given natural language query, we then use the examples selected by our
    `ExampleSelector` to decide what additional information to provide in the context.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 对于给定的自然语言查询，我们然后使用`ExampleSelector`选择的示例来决定在上下文中提供哪些额外信息。
- en: In particular, we count the occurrences of each `ViewStage` in these selected
    examples, identify the five most frequent ``ViewStages`, and add the descriptions
    and information about the input parameters for these `ViewStages` as context in
    our prompt. The rationale for this is that if a stage frequently occurs in similar
    queries, it is likely (but not guaranteed) to be relevant to this query.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 特别是，我们统计了这些选择示例中每个`ViewStage`的出现次数，确定了出现频率最高的五个``ViewStages`，并在我们的提示中添加了关于这些`ViewStages`的描述和输入参数信息。这么做的理由是，如果一个阶段在类似的查询中频繁出现，它很可能（但不能保证）与该查询相关。
- en: If it is not relevant, then the description will help the model to determine
    that it is not relevant. If it is relevant, then information about input parameters
    will help the model generate a syntactically correct `ViewStage` operation.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 如果不相关，那么描述将帮助模型确定它不相关。如果相关，那么关于输入参数的信息将帮助模型生成语法正确的`ViewStage`操作。
- en: Divide and Conquer
  id: totrans-139
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分而治之
- en: '![](../Images/6e1df4b3d2bfed3c2e4862decc4ad83d.png)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6e1df4b3d2bfed3c2e4862decc4ad83d.png)'
- en: VoxelGPT using natural language to query an image dataset. Image courtesy of
    the author.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: VoxelGPT使用自然语言查询图像数据集。图片由作者提供。
- en: Up until this point, we had focused on squeezing as much relevant information
    as possible — and just relevant information — into a single prompt. But this approach
    was reaching its limits.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们专注于将尽可能多的相关信息——而且只是相关信息——挤进一个提示中。但这种方法已接近其极限。
- en: Even without accounting for the fact that every dataset has its own names for
    fields and classes, the space of possible Python queries was just too large.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 即使不考虑每个数据集都有自己的字段和类名，可能的Python查询空间也太大了。
- en: To make progress, we needed to break the problem down into smaller pieces. Taking
    inspiration from recent approaches, including [Chain-of-thought prompting](https://arxiv.org/abs/2201.11903)
    and [Selection-inference prompting](https://arxiv.org/abs/2205.09712), we divided
    the problem of generating a `DatasetView` into four distinct selection subproblems
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 为了取得进展，我们需要将问题分解成更小的部分。借鉴近期的方法，包括[链式思考提示](https://arxiv.org/abs/2201.11903)和[选择推理提示](https://arxiv.org/abs/2205.09712)，我们将生成`DatasetView`的问题分成了四个不同的选择子问题。
- en: Algorithms
  id: totrans-145
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 算法
- en: Runs of algorithms
  id: totrans-146
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 算法运行
- en: Relevant fields
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 相关字段
- en: Relevant class names
  id: totrans-148
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 相关的类名
- en: We then chained these selection “links” together, and passed their outputs along
    to the model in the final prompt for `DatasetView` inference.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们将这些选择“链接”串联起来，并将它们的输出传递给模型，在最终提示中用于`DatasetView`推理。
- en: For each of these subtasks, the same principles of uniformity and simplicity
    apply. We tried to recycle the natural language queries from existing examples
    wherever possible, but made a point to simplify the formats of all inputs and
    outputs for each selection task. What is simplest for one link may not be simplest
    for another!
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这些子任务，统一性和简洁性的原则适用。我们尽可能地回收现有示例中的自然语言查询，但明确简化了每个选择任务的所有输入和输出的格式。对一个链接来说最简单的东西可能对另一个链接并不是最简单的！
- en: Algorithms
  id: totrans-151
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 算法
- en: In FiftyOne, information resulting from a computation on a dataset is stored
    as a “run”. This includes computations like `uniqueness`, which measures how unique
    each image is relative to the rest of the images in the dataset, and `hardness`,
    which quantifies the difficulty a model will experience when trying to learn on
    this sample. It also includes computations of `similarity`, which involve generating
    a vector index for embeddings associated with each sample, and even `evaluation`
    computations, which we touched upon earlier.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 在 FiftyOne 中，来自数据集计算的信息被存储为一个“运行”（“run”）。这包括诸如 `uniqueness` 这样的计算，它衡量每张图像相对于数据集中其他图像的独特性，以及
    `hardness`，它量化模型在尝试学习此样本时会遇到的难度。它还包括 `similarity` 的计算，这涉及为与每个样本相关联的嵌入生成向量索引，甚至包括我们之前提到的
    `evaluation` 计算。
- en: Each of these computations generates a different type of results object, which
    has its own API. Furthermore, there is not any one-to-one correspondence between
    `ViewStages` and these computations. Let’s take uniqueness as an example.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 每个这些计算生成不同类型的结果对象，每种结果对象都有自己的 API。此外，`ViewStages` 和这些计算之间没有一一对应的关系。以独特性（uniqueness）为例。
- en: 'A uniqueness computation result is stored in a float-valued field (`"uniqueness”`
    by default) on each image. This means that depending on the situation, you may
    want to sort by uniqueness:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 独特性计算结果存储在每张图像的一个浮点值字段中（默认值为 `"uniqueness"`）。这意味着根据情况，你可能希望按独特性排序：
- en: '[PRE8]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Retrieve samples with uniqueness above a certain threshold:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 检索独特性高于某个阈值的样本：
- en: '[PRE9]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Or even just show the uniqueness field:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 或者仅显示独特性字段：
- en: '[PRE10]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'In this selection step, we task the LLM with predicting which of the possible
    computations might be relevant to the user’s natural language query. An example
    for this task looks like:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个选择步骤中，我们任务 LLM 预测可能与用户自然语言查询相关的计算。这个任务的一个示例是：
- en: '[PRE11]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Runs of Algorithms
  id: totrans-162
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 算法运行
- en: Once potentially relevant computational algorithms have been identified, we
    task the LLM with selecting the most appropriate run of each computation. This
    is essential because some computations can be run multiple times on the same dataset
    with different configurations, and a `ViewStage` may only make sense with the
    right “run”.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦识别出可能相关的计算算法，我们会任务 LLM 选择每个计算的最合适运行。这是必需的，因为某些计算可以在同一数据集上使用不同配置运行多次，而 `ViewStage`
    可能仅在正确的“运行”下才有意义。
- en: 'A great example of this is similarity runs. Suppose you are testing out two
    models (InceptionV3 and CLIP) on your data, and you have generated a vector similarity
    index on the dataset for each model. When using the `SortBySimilarity` view stage,
    which images are determined to be most similar to which other images can depend
    quite strongly on the embedding model, so the following two queries would need
    to generate different results:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 一个很好的例子是相似度运行。假设你在你的数据上测试两个模型（InceptionV3 和 CLIP），并且你已经为每个模型在数据集中生成了向量相似度索引。当使用
    `SortBySimilarity` 视图阶段时，哪些图像被确定为与哪些其他图像最相似可能会强烈依赖于嵌入模型，因此以下两个查询可能会生成不同的结果：
- en: '[PRE12]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: This run selection process is handled separately for each type of computation,
    as each requires a modified set of task rules and examples.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 这个运行选择过程对每种计算类型分别处理，因为每种计算需要修改后的任务规则和示例。
- en: Relevant Fields
  id: totrans-167
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 相关字段
- en: This link in the chain involves identifying all field names relevant to the
    natural language query that are *not* related to a computational run. For instance
    not all datasets with predictions have those labels stored under the name `"predictions”`.
    Depending on the person, dataset, and application, predictions might be stored
    in a field named `"pred"`, `"resnet"`, `"fine-tuned"`, `"predictions_05_16_2023"`,
    or something else entirely.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 链中的这个链接涉及识别所有与自然语言查询相关的字段名称，这些字段名称 *不* 与计算运行相关。例如，并非所有包含预测的 数据集 都将这些标签存储在名为
    `"predictions"` 的字段下。根据个人、数据集和应用的不同，预测可能存储在名为 `"pred"`、`"resnet"`、`"fine-tuned"`、`"predictions_05_16_2023"`
    或完全不同的名称的字段中。
- en: 'Examples for this task included the query, the names and types of all fields
    in the dataset, and the names of relevant fields:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 此任务的示例包括查询、数据集中所有字段的名称和类型，以及相关字段的名称：
- en: '[PRE13]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Relevant Class Names
  id: totrans-171
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 相关的类名
- en: For label fields like classifications and detections, translating a natural
    language query into Python code requires using the names of actual classes in
    the dataset. To accomplish this, I tasked GPT-3.5 with performing [named entity
    recognition](https://paperswithcode.com/task/named-entity-recognition-ner) for
    label classes in input queries.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 对于标签字段如分类和检测，将自然语言查询转换为Python代码需要使用数据集中实际类的名称。为此，我让GPT-3.5执行[命名实体识别](https://paperswithcode.com/task/named-entity-recognition-ner)以识别输入查询中的标签类。
- en: In the query “samples with at least one cow prediction and no horses”, the model’s
    job is to identify `"horse"` and `"cow"`. These identified names are then compared
    against the class names for label fields selected in the prior step — first case
    sensitive, then case insensitive, then plurality insensitive.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 在查询“样本中至少有一个牛预测且没有马”的情况下，模型的任务是识别`"horse"`和`"cow"`。这些识别出的名称随后与在之前步骤中选择的标签字段的类名进行比较——首先是区分大小写的，然后是不区分大小写的，最后是不区分复数的。
- en: 'If no matches are found between named entities and the class names in the dataset,
    we fall back to semantic matching: `"people"` → `"person"`, `"table"` → `"dining
    table"`, and `"animal"` → `[“cat”, “dog", “horse", …]`.'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 如果在数据集中未找到命名实体和类名之间的匹配，我们将退回到语义匹配：`"people"` → `"person"`，`"table"` → `"dining
    table"`，和`"animal"` → `[“cat”, “dog", “horse", …]`。
- en: 'Whenever the match is not identical, we use the names of the matched classes
    to update the query that is passed into the final inference step:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 每当匹配不完全相同时，我们使用匹配的类名来更新传递给最终推断步骤的查询：
- en: '[PRE14]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: ViewStage Inference
  id: totrans-177
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ViewStage推断
- en: Once all of these selections have been made, the similar examples, relevant
    descriptions, and relevant dataset info (selected algorithmic runs, fields, and
    classes) are passed in to the model, along with the (potentially modified) query.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦所有这些选择完成，类似的示例、相关的描述和相关的数据集信息（选择的算法运行、字段和类）将与（可能修改过的）查询一起传递给模型。
- en: Rather than instruct the model to return code to me in the form `dataset.view1().view2()…viewn()`
    as we were doing initially, we ended up nixing the `dataset` part, and instead
    asking the model to return the `ViewStages` as a list. At the time, I was surprised
    to see this improve performance, but in hindsight, it fits with the insight that
    the more you split the task up, the better an LLM can do.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 与其指示模型以`dataset.view1().view2()…viewn()`的形式返回代码，我们最终去掉了`dataset`部分，而是要求模型将`ViewStages`作为列表返回。当时，我对这种改进性能感到惊讶，但回想起来，它与任务拆分越多，LLM表现越好的见解相一致。
- en: Making it Usable
  id: totrans-180
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使其可用
- en: Creating an LLM-powered toy is cool, but turning the same kernel into an LLM-power
    application is much cooler. Here’s a brief overview of how we did it.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个LLM驱动的玩具很酷，但将相同的内核转变为LLM驱动的应用程序则更酷。以下是我们如何做到这一点的简要概述。
- en: Unit Testing
  id: totrans-182
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 单元测试
- en: As we turned this from a proof-of-principle into a robustly engineered system,
    we used unit testing to stress test the pipeline and identify weak points. The
    modular nature of links in the chain means that each step can individually be
    unit tested, validated, and iterated on without needing to run the entire chain.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们将其从一个原则证明转变为一个稳健的工程系统时，我们使用单元测试来压力测试管道并识别薄弱点。链中链接的模块化特性意味着每一步可以单独进行单元测试、验证和迭代，而无需运行整个链。
- en: This leads to *faster improvement*, because different individuals or groups
    of people within a prompt-engineering team can work on different links in the
    chain in parallel. Additionally, it results in *reduced costs*, as in theory,
    you should only need to run a single step of LLM inference to optimize a single
    link in the chain.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 这导致了*更快的改进*，因为在提示工程团队中，不同的个人或小组可以并行处理链中的不同链接。此外，这还导致了*成本减少*，因为理论上，你只需要运行LLM推断的单个步骤来优化链中的单个链接。
- en: Evaluating LLM-Generated Code
  id: totrans-185
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 评估LLM生成的代码
- en: We used Python’s `eval()` function to turn GPT-3.5’s response into a `DatasetView`.
    We then set the state of the FiftyOne App `session` to display this view.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用Python的`eval()`函数将GPT-3.5的响应转换为`DatasetView`。然后，我们设置FiftyOne App的`session`状态以显示该视图。
- en: Input Validation
  id: totrans-187
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 输入验证
- en: Garbage input → garbage output. To avoid this, we run validation to make sure
    that the user’s natural language query is sensible.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 垃圾输入 → 垃圾输出。为了避免这种情况，我们运行验证以确保用户的自然语言查询是合理的。
- en: 'First, we use [OpenAI’s moderation endpoint](https://platform.openai.com/docs/guides/moderation/quickstart).
    Then we categorize any prompt into one of the following four cases:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们使用 [OpenAI 的审核端点](https://platform.openai.com/docs/guides/moderation/quickstart)。然后，我们将任何提示分类到以下四种情况之一：
- en: '**1:** Sensible and complete: the prompt can reasonably be translated into
    Python code for querying a dataset.'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: '**1:** 合理且完整：该提示可以合理地转换为用于查询数据集的 Python 代码。'
- en: '*All images with dog detections*'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: '*所有带狗检测的图像*'
- en: '**2:** Sensible and incomplete: the prompt is reasonable, but cannot be converted
    into a DatasetView without additional information. For example, if we have two
    models with predictions on our data, then the following prompt, which just refers
    to “my model” is insufficient:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: '**2:** 合理但不完整：该提示合理，但在没有额外信息的情况下无法转换为 DatasetView。例如，如果我们有两个模型对数据进行预测，那么仅提到“我的模型”的提示是不足够的：'
- en: '*Retrieve my model’s incorrect predictions*'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: '*检索我的模型的错误预测*'
- en: '**3:** Out of scope: we are building an application that generates queried
    views into computer vision datasets. While the underlying GPT-3.5 model is a general
    purpose LLM, our application should not turn into a disconnected ChatGPT session
    *next* *to* your dataset. Prompts like the following should be snuffed out:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: '**3:** 超出范围：我们正在构建一个生成查询视图的应用程序，应用于计算机视觉数据集。虽然底层的 GPT-3.5 模型是通用 LLM，但我们的应用不应变成一个与数据集*相邻*的离线
    ChatGPT 会话。如下提示应被拒绝：'
- en: '*Explain quantum computing like I’m five*'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: '*像我五岁一样解释量子计算*'
- en: '**4:** Not sensible: given a random string, it would not make sense to attempt
    to generate a view of the dataset — where would one even start?!'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: '**4:** 不合理：给定一个随机字符串，尝试生成数据集视图是不合适的——从哪里开始呢？！'
- en: '*Azlsakjdbiayervbg*'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: '*Azlsakjdbiayervbg*'
- en: Before attempting to turn the user’s input query into a sequence of view stages,
    we pass the input into the model with validation instructions and examples. Depending
    on the response, we either prompt the user for more information or a more sensible
    query, or proceed with the dataset view generation pipeline.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 在尝试将用户的输入查询转换为视图阶段序列之前，我们将输入传递到带有验证指令和示例的模型中。根据响应，我们要么提示用户提供更多信息或更合理的查询，要么继续进行数据集视图生成流程。
- en: Recap
  id: totrans-199
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: VoxelGPT in action! Video courtesy of the author.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: VoxelGPT 实战！视频由作者提供。
- en: A simple idea led to a crazy idea, and this journey brought that crazy idea
    to life. With prompt engineering, some genuine software engineering, a lot of
    elbow grease, and a healthy dose of black magic, our small team created an LLM-powered
    application that translates natural language queries into filtered views of computer
    vision datasets.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 一个简单的想法引发了一个疯狂的想法，这段旅程将那个疯狂的想法变成了现实。通过提示工程、一些真正的软件工程、大量的辛勤工作以及健康的黑魔法，我们的小团队创建了一个
    LLM 驱动的应用程序，将自然语言查询转换为计算机视觉数据集的过滤视图。
- en: 'The takeaways are simple: define a task, instruct with rules, limit scope,
    simplify, opt for uniformity, break down problems, and keep it relevant.'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 要点很简单：定义任务、规定规则、限制范围、简化、选择一致性、分解问题，并保持相关性。
- en: 'When we put all the pieces together, it looks something like this:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们将所有部分组合在一起时，它看起来像这样：
- en: Validate query
  id: totrans-204
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 验证查询
- en: Find similar examples
  id: totrans-205
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 查找类似的示例
- en: Retrieve pertinent documentation
  id: totrans-206
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检索相关文档
- en: Identify potential algorithms/computations
  id: totrans-207
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 确定潜在的算法/计算
- en: Select most likely runs of those algorithms
  id: totrans-208
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择这些算法最可能的运行结果
- en: Determine which fields (properties) are accessed/utilized in queries
  id: totrans-209
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 确定在查询中访问/利用的字段（属性）
- en: Infer the class names for each label field
  id: totrans-210
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 推断每个标签字段的类名称
- en: Generate list of view stages for query
  id: totrans-211
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 生成查询的视图阶段列表
- en: Compose view stages and return filtered view into dataset
  id: totrans-212
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 组成视图阶段并返回过滤后的数据集视图
- en: Win
  id: totrans-213
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 胜利
- en: VoxelGPT is far from perfect. But that was never the goal. There’s room for
    improvement at every step — not to mention more examples! One could also use this
    as a foundation for building an [AutoGPT](https://github.com/Significant-Gravitas/Auto-GPT)-style
    delegator for computer vision tasks.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: VoxelGPT 远非完美。但这从未是目标。每一步都有改进的空间——更不用说更多的示例了！还可以将其作为构建 [AutoGPT](https://github.com/Significant-Gravitas/Auto-GPT)
    风格的计算机视觉任务委托器的基础。
- en: Any and all contributions to [VoxelGPT](https://github.com/voxel51/fiftyone-gpt)
    are welcome. It’s free and open source! 🙂
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 对 [VoxelGPT](https://github.com/voxel51/fiftyone-gpt) 的任何贡献都欢迎。它是免费的开源软件！🙂
