- en: 'Harnessing the Power of Knowledge Graphs: Enriching an LLM with Structured
    Data'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 利用知识图谱的力量：用结构化数据丰富LLM
- en: 原文：[https://towardsdatascience.com/harnessing-the-power-of-knowledge-graphs-enriching-an-llm-with-structured-data-997fabc62386](https://towardsdatascience.com/harnessing-the-power-of-knowledge-graphs-enriching-an-llm-with-structured-data-997fabc62386)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/harnessing-the-power-of-knowledge-graphs-enriching-an-llm-with-structured-data-997fabc62386](https://towardsdatascience.com/harnessing-the-power-of-knowledge-graphs-enriching-an-llm-with-structured-data-997fabc62386)
- en: '![](../Images/e32186b4eaa2e36dee4c8c50e5110b8f.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e32186b4eaa2e36dee4c8c50e5110b8f.png)'
- en: A step-by-step guide to creating a knowledge graph and exploring its potential
    to enhance an LLM
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建知识图谱并探索其增强LLM潜力的逐步指南
- en: '[](https://stevehedden.medium.com/?source=post_page-----997fabc62386--------------------------------)[![Steve
    Hedden](../Images/af7bec4a191ab857eccd885dd89e88b4.png)](https://stevehedden.medium.com/?source=post_page-----997fabc62386--------------------------------)[](https://towardsdatascience.com/?source=post_page-----997fabc62386--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----997fabc62386--------------------------------)
    [Steve Hedden](https://stevehedden.medium.com/?source=post_page-----997fabc62386--------------------------------)'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://stevehedden.medium.com/?source=post_page-----997fabc62386--------------------------------)[![Steve
    Hedden](../Images/af7bec4a191ab857eccd885dd89e88b4.png)](https://stevehedden.medium.com/?source=post_page-----997fabc62386--------------------------------)[](https://towardsdatascience.com/?source=post_page-----997fabc62386--------------------------------)[![数据科学的前沿](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----997fabc62386--------------------------------)
    [Steve Hedden](https://stevehedden.medium.com/?source=post_page-----997fabc62386--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----997fabc62386--------------------------------)
    ·20 min read·Jul 10, 2023
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于[数据科学的前沿](https://towardsdatascience.com/?source=post_page-----997fabc62386--------------------------------)
    ·20分钟阅读·2023年7月10日
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '*For accompanying code, see notebook* [*here.*](https://github.com/SteveHedden/kg_llm/blob/main/SDKG.ipynb)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '*有关代码，请参见笔记本* [*这里。*](https://github.com/SteveHedden/kg_llm/blob/main/SDKG.ipynb)'
- en: In recent years, [large language models](https://snorkel.ai/large-language-models-llms/)
    (LLMs), have become ubiquitous. Perhaps the most famous LLM is ChatGPT, which
    was released by OpenAI in November 2022\. ChatGPT is able to [generate ideas](https://www.linkedin.com/pulse/generate-100-content-ideas-chat-gpt-mfon-akpan/),
    [give personalized recommendations](https://bootcamp.uxdesign.cc/how-to-use-chatgpt-for-personalized-recommendations-840e01dcad89),
    [understand complicated topics](https://medium.com/101-innovation-hacks/using-chatgpt-to-explain-complex-concepts-2ea6aba97cf3),
    [act as a writing assistant](https://chatgptwriter.ai/), or [help you build a
    model to predict the Academy Awards.](https://medium.com/design-bootcamp/using-chatgpt-to-predict-the-oscars-c6d8cdb6b3a0)
    Meta has announced their own LLM called [LLaMA](https://ai.meta.com/blog/large-language-model-llama-meta-ai/),
    Google has [LaMDA](https://blog.google/technology/ai/lamda/), and there is even
    an open-source alternative, [BLOOM.](https://huggingface.co/bigscience/bloom)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，[大型语言模型](https://snorkel.ai/large-language-models-llms/)（LLMs）变得无处不在。也许最著名的LLM是ChatGPT，它由OpenAI于2022年11月发布。ChatGPT能够[生成创意](https://www.linkedin.com/pulse/generate-100-content-ideas-chat-gpt-mfon-akpan/)、[给出个性化推荐](https://bootcamp.uxdesign.cc/how-to-use-chatgpt-for-personalized-recommendations-840e01dcad89)、[理解复杂话题](https://medium.com/101-innovation-hacks/using-chatgpt-to-explain-complex-concepts-2ea6aba97cf3)、[充当写作助手](https://chatgptwriter.ai/)，或者[帮助你建立一个预测奥斯卡奖的模型。](https://medium.com/design-bootcamp/using-chatgpt-to-predict-the-oscars-c6d8cdb6b3a0)
    Meta宣布了他们自己的LLM叫做[LLaMA](https://ai.meta.com/blog/large-language-model-llama-meta-ai/)，Google有[LaMDA](https://blog.google/technology/ai/lamda/)，甚至还有一个开源替代品，[BLOOM。](https://huggingface.co/bigscience/bloom)
- en: 'LLMs have excelled in natural language processing (NLP) tasks like the ones
    listed above because LLMs have historically focused on [unstructured data](https://en.wikipedia.org/wiki/Unstructured_data)
    — data that does not have a pre-defined structure, and is usually text-heavy.
    I asked ChatGPT, “why have LLMs historically focused on unstructured data?” The
    reply was:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: LLM在自然语言处理（NLP）任务中表现出色，如上述所列，因为LLM历史上专注于[非结构化数据](https://en.wikipedia.org/wiki/Unstructured_data)——没有预定义结构的数据，通常以文本为主。我问ChatGPT，“为什么LLM历史上专注于非结构化数据？”回答是：
- en: “LLMs have historically focused on unstructured data due to its abundance, availability,
    and the challenges it poses. Unstructured data provides a vast source for training
    language models, allowing them to learn patterns, context, and semantics. LLMs
    excel in handling the complexities and nuances of unstructured data, capturing
    the subtleties of language and leveraging the rich context, opinions, and creative
    expressions it contains. While efforts are underway to bridge the gap between
    LLMs and structured data, the historical emphasis on unstructured data stems from
    its unique characteristics and the valuable information it offers, allowing LLMs
    to provide more human-like responses.”
  id: totrans-10
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “LLMs历史上专注于非结构化数据，因其丰富性、可获得性以及所带来的挑战。非结构化数据为训练语言模型提供了广泛的来源，使其能够学习模式、上下文和语义。LLMs在处理非结构化数据的复杂性和细微差别方面表现出色，捕捉语言的细微之处，并利用其中丰富的上下文、观点和创造性表达。虽然正在努力弥合LLMs与结构化数据之间的差距，但对非结构化数据的历史性重视源于其独特特征和所提供的有价值信息，使LLMs能够提供更具人性化的回应。”
- en: '[Structured data](https://www.ibm.com/cloud/blog/structured-vs-unstructured-data)
    is usually quantitative and well-organized, usually into rows and columns, and,
    as ChatGPT notes, there is still a gap between LLMs and structured data.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '[结构化数据](https://www.ibm.com/cloud/blog/structured-vs-unstructured-data)通常是定量的，并且组织良好，通常是以行和列的形式呈现的。正如ChatGPT指出的那样，LLMs与结构化数据之间仍存在差距。'
- en: Knowledge graphs (KG), on the other hand, are excellent at querying structured
    data. A knowledge graph is,
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 知识图谱（KG）则在查询结构化数据方面表现出色。知识图谱是，
- en: “directed labeled graph in which domain specific meanings are associated with
    nodes and edges. A node could represent any real-world entity, for example, people,
    company, computer, etc. An edge label captures the relationship of interest between
    the two nodes, for example, a friendship relationship between two people, a customer
    relationship between a company and person, or a network connection between two
    computers, etc.” [(Chaudhri et al., 2022)](https://onlinelibrary.wiley.com/doi/10.1002/aaai.12033).
  id: totrans-13
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “有向标记图，其中领域特定的意义与节点和边缘相关联。一个节点可以代表任何现实世界的实体，例如，人、公司、计算机等。边缘标签捕捉两个节点之间的兴趣关系，例如，两个人之间的友谊关系，公司和个人之间的客户关系，或两个计算机之间的网络连接等。”
    [(Chaudhri et al., 2022)](https://onlinelibrary.wiley.com/doi/10.1002/aaai.12033)。
- en: KGs allow one to integrate heterogenous data sources, including combinations
    of structured, semi-structured, and unstructured data. KGs are used for organizing
    data, drawing inferences, [creating recommendations](/introduction-to-knowledge-graph-based-recommender-systems-34254efd1960),
    and [semantic search](https://www.stardog.com/blog/how-to-build-a-semantic-search-engine-using-a-knowledge-graph/).
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: KGs允许集成异构数据源，包括结构化、半结构化和非结构化数据的组合。KGs用于组织数据、推导推论、[创建推荐](/introduction-to-knowledge-graph-based-recommender-systems-34254efd1960)和[语义搜索](https://www.stardog.com/blog/how-to-build-a-semantic-search-engine-using-a-knowledge-graph/)。
- en: 'As Shirui Pan et al. point out in their paper, “[Unifying Large Language Models
    and Knowledge Graphs: A Roadmap](https://arxiv.org/abs/2306.08302),” the two models
    can be complementary. Some of the main weaknesses of LLMs, that they are black-box
    models and struggle with factual knowledge, are some of the KGs’ greatest strengths.
    KGs are, essentially, collections of facts, and they are fully interpretable.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 正如Shirui Pan等人在他们的论文“[统一大语言模型和知识图谱：路线图](https://arxiv.org/abs/2306.08302)”中指出的那样，这两种模型可以互补。LLMs的一些主要弱点，如黑箱模型和对事实知识的困难，正是KGs的主要优势。KGs本质上是事实的集合，它们是完全可解释的。
- en: '![](../Images/7b67d913513771a3e703d2426924b8a5.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7b67d913513771a3e703d2426924b8a5.png)'
- en: How LLMs and KGs can be complementary. From Shirui Pan et al., 2023\. <[https://arxiv.org/abs/2306.08302](https://arxiv.org/abs/2306.08302)>
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: LLMs和KGs如何互补。来源：Shirui Pan等人，2023年。<[https://arxiv.org/abs/2306.08302](https://arxiv.org/abs/2306.08302)>
- en: Shirui et al. propose many potential ways LLMs and KGs can complement each other.
    In this tutorial, I will show how to create a KG from structured data, and then
    use that KG as part of the input prompt to the LLM, something called in-context
    learning. I will compare the LLM’s responses when using the KG as part of the
    input with the LLM’s responses when using the original structured data as part
    of the input prompt.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: Shirui等人提出了LLMs和KGs互补的多种潜在方式。在本教程中，我将展示如何从结构化数据创建一个KG，然后将这个KG作为LLM输入提示的一部分，这被称为上下文学习。我将比较在使用KG作为输入的一部分时LLM的回应与使用原始结构化数据作为输入提示时LLM的回应。
- en: 'The methodology for this tutorial is as follows:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 本教程的方法如下：
- en: Download some metadata on World Bank documents using the [World Bank API](https://documents.worldbank.org/en/publication/documents-reports/api)
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用[World Bank API](https://documents.worldbank.org/en/publication/documents-reports/api)下载一些关于世界银行文档的元数据
- en: Build an ontology using the metadata for the documents
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用文档的元数据构建本体
- en: Populate the ontology with instances of documents
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 用文档实例填充本体
- en: Pull in additional entities and relationships into the graph using [Wikidata](https://www.wikidata.org/wiki/Wikidata:Main_Page)
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用[Wikidata](https://www.wikidata.org/wiki/Wikidata:Main_Page)将额外的实体和关系引入图谱
- en: Query the KG directly using [SPARQL](https://www.ontotext.com/knowledgehub/fundamentals/what-is-sparql/)
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 直接使用[SPARQL](https://www.ontotext.com/knowledgehub/fundamentals/what-is-sparql/)查询知识图谱
- en: 'Compare the different ways of interacting with data: using SPARQL queries on
    the RDF, putting raw metadata into [LlamaIndex](https://www.llamaindex.ai/), and
    putting the RDF data into LlamaIndex'
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 比较与数据交互的不同方式：使用SPARQL查询RDF，将原始元数据放入[LlamaIndex](https://www.llamaindex.ai/)，以及将RDF数据放入LlamaIndex
- en: 'If you don’t want to read any more, my overall takeaways are that:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你不想再阅读更多内容，我的总体收获是：
- en: 'While it is possible to use structured data (in the form of CSV or RDF files)
    directly, using in-context learning, to enhance an LLM, the results are not great.
    The LLM suffers the same problems: it sometimes gives correct answers but sometimes
    suffers from hallucinations (incorrect facts) and it is impossible to know how/why
    these are occurring.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 尽管可以直接使用结构化数据（以CSV或RDF文件形式），但利用上下文学习来增强LLM的结果并不理想。LLM会遇到相同的问题：有时给出正确的答案，但有时出现幻觉（错误的事实），而且无法知道这些问题是如何/为什么发生的。
- en: Turning your structured data into a knowledge graph by building out an ontology
    and assigning each instance of data appropriate classes and properties can improve
    the results, but hallucinations and inexplicable inaccuracies persist.
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将结构化数据转换为知识图谱，通过构建本体并为每个数据实例分配适当的类别和属性，可以改善结果，但仍然存在幻觉和难以解释的不准确性。
- en: If you are interested in specific queries across your personal structured dataset
    and need accurate and verifiable answers, you should use formal queries either
    using SPARQL or something else. A SPARQL query can answer a question like, “what
    are all of the projects associated with documents that this author has written?”
    much better than an LLM, even when enhanced through in-context learning with the
    KG.
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果你对个人结构化数据集中的特定查询感兴趣，并需要准确和可验证的答案，你应该使用正式查询，无论是使用SPARQL还是其他工具。SPARQL查询可以比LLM更好地回答诸如“与该作者撰写的文档相关的所有项目是什么？”这样的问题，即使在通过KG进行上下文学习时也如此。
- en: An LLM can help write and refine the SPARQL query, however. If an LLM can translate
    a prompt into a SPARQL query, then a user can still ‘chat’ directly with structured
    data without writing their own code/queries.
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然而，LLM可以帮助编写和完善SPARQL查询。如果LLM能够将提示翻译成SPARQL查询，那么用户仍然可以直接“聊天”与结构化数据，而无需编写自己的代码/查询。
- en: LLMs are great at understanding and interpreting unstructured data. This capability
    extends even to structured data *when unstructured information is embedded within
    it.* For instance, if the structured data includes a column labeled ‘abstracts’
    containing unstructured text, the LLM can leverage that data to generate insightful
    results.
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LLM在理解和解释非结构化数据方面表现出色。这种能力甚至扩展到*当非结构化信息嵌入其中时*的结构化数据。例如，如果结构化数据包含一个名为“摘要”的列，其中包含非结构化文本，LLM可以利用这些数据生成有见地的结果。
- en: Using both the raw metadata and the KG to enhance the LLM did not improve results.
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用原始元数据和知识图谱（KG）来增强LLM并没有改善结果。
- en: 'Some other ways Shirui et al. suggest KGs and LLMs can work together: use LLMs
    to translate prompt into formal query, use KGs to validate the LLMs responses,
    [use LLMs to build KGs](https://gpt-index.readthedocs.io/en/latest/reference/indices/kg.html),
    and use KGs to train LLMs.'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shirui等人建议KG和LLM可以共同工作的其他方法：使用LLM将提示翻译成正式查询，使用KG验证LLM的响应，[使用LLM构建KG](https://gpt-index.readthedocs.io/en/latest/reference/indices/kg.html)，以及使用KG训练LLM。
- en: 1\. Download some metadata on World Bank documents using the World Bank API
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 1\. 使用世界银行API下载一些关于世界银行文档的元数据
- en: First we need the metadata for some World Bank docs. For full documentation
    on the World Bank Documents and Reports API, go [here.](https://documents.worldbank.org/en/publication/documents-reports/api)
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 首先我们需要一些世界银行文档的元数据。有关世界银行文档和报告API的完整文档，请访问[此处。](https://documents.worldbank.org/en/publication/documents-reports/api)
- en: I selected World Bank document metadata as the foundation for our knowledge
    graph for several reasons. Firstly, the World Bank provides an API that enables
    access to their data. Additionally, the metadata associated with World Bank documents
    is comprehensive and offers valuable information. Lastly, my familiarity with
    this type of data ensures a better understanding of its structure and attributes.
    It’s important to note that the flexibility of building a knowledge graph applies
    to any data source, as long as one possesses sufficient domain knowledge to structure
    it effectively.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 我选择世界银行文档元数据作为我们知识图谱的基础有几个原因。首先，世界银行提供了一个API，可以访问他们的数据。此外，与世界银行文档相关的元数据是全面的，提供了有价值的信息。最后，我对这种数据类型的熟悉程度确保了对其结构和属性的更好理解。值得注意的是，构建知识图谱的灵活性适用于任何数据源，只要具备足够的领域知识以有效地构建它。
- en: The following code gives us the metadata for the most recent 20 reports that
    have the phrase “sustainable development” in their title.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码提供了最近20份标题中包含“可持续发展”一词的报告的元数据。
- en: '[PRE0]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Now we have our metadata saved as a pandas dataframe (df).
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经将元数据保存为 pandas 数据框（df）。
- en: 2\. Build an ontology using the metadata from the documents
  id: totrans-40
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 2\. 使用文档的元数据构建本体
- en: Now we can set up our ontology. What even is an ontology?
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以建立我们的本体。那么，本体到底是什么？
- en: '*“An ontology defines a common vocabulary for researchers who need to share
    information in a domain. It includes machine-interpretable definitions of basic
    concepts in the domain and relations among them,”* [*(Noy and McGuinness, 2001).*](https://protege.stanford.edu/publications/ontology_development/ontology101.pdf)'
  id: totrans-42
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*“本体定义了一个共同的词汇，以便需要在某一领域共享信息的研究人员使用。它包括该领域基本概念及其相互关系的机器可解释定义，”* [*(Noy 和 McGuinness,
    2001).*](https://protege.stanford.edu/publications/ontology_development/ontology101.pdf)'
- en: By adopting an ontology, we gain the ability to connect diverse data sources.
    In this tutorial, our main focus lies in World Bank data, however, if we were
    to integrate UN data, we would face challenges such as varying document type categories,
    date formats, and country names. An ontology serves as a valuable tool in bridging
    these differences and establishing connections between the datasets. Moreover,
    we can expand the scope of our knowledge graph by incorporating data from Wikidata,
    a widely used public knowledge graph closely related to Wikipedia. Aligning our
    data ontology with Wikidata’s ontology enables seamless integration of information
    from Wikidata into our own knowledge graph.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 通过采用本体，我们能够连接不同的数据源。在本教程中，我们的主要关注点是世界银行数据，然而，如果我们要整合联合国数据，我们将面临诸如文档类型类别、日期格式和国家名称的差异等挑战。本体作为弥合这些差异和建立数据集之间连接的有价值工具。此外，我们可以通过整合来自
    Wikidata 的数据来扩展我们的知识图谱，Wikidata 是一个广泛使用的公共知识图谱，与 Wikipedia 密切相关。将我们的数据本体与 Wikidata
    的本体对齐，可以实现信息的无缝集成。
- en: The following code sets up our graph
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码设置了我们的图谱
- en: '[PRE1]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: There is a column in the metadata called, “count”, which represents the country
    of origin for a given document. We want to use this column to create a ‘country’
    class in our ontology, along with subclasses for each unique country in this column.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 元数据中有一列叫做“count”，表示给定文档的原产国。我们希望利用这一列在我们的本体中创建一个“country”类，并为这一列中的每个独特国家创建子类。
- en: '[PRE2]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: I use [protégé](https://protege.stanford.edu/), a free, open-source ontology
    editor, to view and sometimes manually adjust my ontologies. If you want to view
    the ontology as you create it, just save the graph as a ttl file and open with
    protégé.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 我使用 [protégé](https://protege.stanford.edu/)，一个免费的开源本体编辑器，来查看和有时手动调整我的本体。如果你想在创建本体时查看它，只需将图谱保存为
    ttl 文件，并用 Protégé 打开。
- en: '[PRE3]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Once you open the file in protégé, it should look something like this:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你在 Protégé 中打开文件，它应该类似于这样：
- en: '![](../Images/1be512ad9ab9802089b5314d6efcffc7.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1be512ad9ab9802089b5314d6efcffc7.png)'
- en: Screenshot from Protégé. Image by author.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 从 Protégé 截图。图片由作者提供。
- en: 'Each of the unique countries in the ‘count’ column of the World Bank data will
    have created a unique subclass in our ontology under the ‘country’ class. You
    can see that each country also has a label and a Wikidata URI. In this case, Argentina’s
    Wikidata URI is: [https://www.wikidata.org/entity/Q414](https://www.wikidata.org/wiki/Q414).'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在世界银行数据的“count”列中，每个独特的国家将会在我们的本体下的“country”类中创建一个独特的子类。你可以看到每个国家还具有一个标签和一个Wikidata
    URI。在这种情况下，阿根廷的Wikidata URI是：[https://www.wikidata.org/entity/Q414](https://www.wikidata.org/wiki/Q414)。
- en: That is also the link to the Wikidata page for the country Argentina.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 这也是阿根廷国家的Wikidata页面的链接。
- en: Now we have to create classes and subclasses for everything else. I made classes
    for document type, World Bank project, World Bank trustfund, country, and authors.
    I’m not putting all of that code in this tutorial, but see my [notebook](https://github.com/SteveHedden/kg_llm/blob/main/SDKG.ipynb)
    for the code to create all of it.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们需要为其他所有内容创建类和子类。我为文档类型、世界银行项目、世界银行信托基金、国家和作者创建了类。我没有把所有代码放在这个教程中，但请参见我的[笔记本](https://github.com/SteveHedden/kg_llm/blob/main/SDKG.ipynb)以获取创建所有这些类的代码。
- en: 3\. Populate the ontology with instances of documents
  id: totrans-56
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 3\. 用文档实例填充本体
- en: The ontology is the backbone of the KG, but now we need to populate it with
    data. The ontology so far defines classes, subclasses, properties of classes and
    subclasses, and the relations between them.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 本体是知识图谱的骨干，但现在我们需要用数据填充它。目前，本体定义了类、子类、类和子类的属性以及它们之间的关系。
- en: For example, we now have a class for World Bank documents and a subclass for
    working papers. Our ontology establishes that a working paper is a subclass of
    (or a type of) a World Bank document. Our ontology establishes labels and other
    properties for these entities. For example, [working paper](https://www.wikidata.org/wiki/Q1228945)
    is an entity in Wikidata, and so we include the Wikidata URI in our ontology as
    a property.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，我们现在有一个世界银行文档的类和一个工作文件的子类。我们的本体确定了工作文件是世界银行文档的子类（或一种类型）。我们的本体为这些实体确定了标签和其他属性。例如，[工作文件](https://www.wikidata.org/wiki/Q1228945)是Wikidata中的一个实体，因此我们将Wikidata
    URI作为属性包含在本体中。
- en: '![](../Images/925aa55e53d0d825ac2f393d27d1b9a6.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/925aa55e53d0d825ac2f393d27d1b9a6.png)'
- en: Conceptualization of several entities in an ontology. Image by author.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 对本体中几个实体的概念化。图像由作者提供。
- en: In the example above, however, there are no actual World Bank working papers
    i.e. there are no instances of working papers. We have established the class,
    not the instances.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在上面的示例中，并没有实际的世界银行工作文件，即没有工作文件的实例。我们已经建立了类，但尚未建立实例。
- en: Here is the code to go through our DataFrame row by row, and for each row, create
    an instance of a document, and assign that instance appropriate properties. Note
    that to run this code you will need to have created all of these classes already
    (again, see the notebook for full code).
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 这是逐行遍历我们的DataFrame的代码，并为每一行创建一个文档实例，并分配适当的属性。请注意，要运行这段代码，你需要先创建所有这些类（再一次，请参见笔记本以获取完整代码）。
- en: '[PRE4]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Now we have an actual knowledge graph. An instance of a working paper, with
    its associated properties, can be visualized the following way:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了一个实际的知识图谱。可以用以下方式可视化一个工作文件实例及其相关属性：
- en: '![](../Images/e32186b4eaa2e36dee4c8c50e5110b8f.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e32186b4eaa2e36dee4c8c50e5110b8f.png)'
- en: Conceptualization of ontology populated with some instances of data. Image by
    author.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 对填充了一些数据实例的本体的概念化。图像由作者提供。
- en: You can save this new file and open in protégé to ensure that all of the entities
    have been incorporated appropriately.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以保存这个新文件，并在protégé中打开，以确保所有实体都已被适当地纳入。
- en: 4\. Pull in additional entities and relationships into the graph using Wikidata
  id: totrans-68
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 4\. 使用Wikidata将额外的实体和关系引入图谱
- en: Because many of the entities in our KG have Wikidata URIs associated with them,
    we can import additional data from Wikidata into our KG. For this demo, I have
    only done this for countries. In the following code, we iterate over the country
    column, query Wikidata for the country entity, and import all properties and their
    values for the given country. *Note that this code can take a while to run — it
    has to query Wikidata for all of the properties for each country, and their values,
    and put them all into our graph.*
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 因为我们知识图谱中的许多实体都与Wikidata的URI关联，我们可以从Wikidata导入额外的数据到我们的知识图谱中。在这个演示中，我只对国家做了这些操作。在以下代码中，我们遍历国家列，查询Wikidata中的国家实体，并导入给定国家的所有属性及其值。*请注意，这段代码可能需要一段时间来运行——它必须查询Wikidata中每个国家的所有属性及其值，并将它们全部放入我们的图谱中。*
- en: '[PRE5]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Now we have an ontology populated with data from the World Bank and additional
    data from Wikidata! If you open your KG in protégé you can explore all of the
    properties we have imported. Below is a screenshot of some of the properties we’ve
    imported for Argentina. All of this data can also be viewed directly on Wikidata
    [here](https://www.wikidata.org/wiki/Q414).
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了一个用世界银行数据和来自Wikidata的额外数据填充的本体！如果你在protégé中打开你的知识图谱，你可以探索我们导入的所有属性。下面是我们为阿根廷导入的一些属性的屏幕截图。这些数据也可以直接在Wikidata上查看，[这里](https://www.wikidata.org/wiki/Q414)。
- en: '![](../Images/5b517e5545d10f0f0b76a12c538b25e8.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5b517e5545d10f0f0b76a12c538b25e8.png)'
- en: Screenshot from Protégé. Image by author.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 从Protégé截取的屏幕截图。作者提供的图片。
- en: 'Wikidata has data like: form of government, head of state, diplomatic relations,
    life expectancy (and many other development indicators) over time, subregions/territories,
    and many more.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: Wikidata有如下数据：政府形式、国家元首、外交关系、预期寿命（以及许多其他发展指标）随时间变化、子区域/领土等。
- en: 5\. Query the KG directly using SPARQL
  id: totrans-75
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 5\. 直接使用SPARQL查询知识图谱
- en: We can query this KG directly using [SPARQL](https://www.ontotext.com/knowledgehub/fundamentals/what-is-sparql/)
    queries, the standard query language for RDF databases (what we have created).
    There is a SPARQL wrapper in Python that we can use.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用[SPARQL](https://www.ontotext.com/knowledgehub/fundamentals/what-is-sparql/)查询直接查询这个知识图谱（我们创建的RDF数据库的标准查询语言）。我们可以使用Python中的SPARQL封装器。
- en: 'For example, if want to see all documents about Brazil, we can run the following
    code:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果想查看所有关于巴西的文档，我们可以运行以下代码：
- en: '[PRE6]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: This will output all World Bank documents in our KG that have ‘Brazil’ listed
    as the countryOfOrigin.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 这将输出我们知识图谱中所有将‘巴西’列为国家来源的世界银行文档。
- en: We can make SPARQL queries as complicated as we like and include any properties
    from either the World Bank metadata or the Wikidata that we imported. For example,
    what if we want to know which author has written the most documents about countries
    that are federal republics?
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将SPARQL查询做得尽可能复杂，并包含来自世界银行元数据或我们导入的Wikidata的任何属性。例如，如果我们想知道哪位作者写了最多关于联邦共和国国家的文档呢？
- en: '[PRE7]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The output will look something like this:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 输出将类似于下面这样：
- en: '![](../Images/2067ed15f4ae4e05dce8921af8c7fb46.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2067ed15f4ae4e05dce8921af8c7fb46.png)'
- en: We can see that the author “World Bank” has written five documents about countries
    that Wikidata has listed as federal republics. In this case, the countries are
    Brazil, Comoros, and Ethiopia. This is significant because the World Bank data
    does not tell us the form of government for any country, that came from Wikidata.
    Since we built an ontology that aligns with Wikidata, we can incorporate additional
    data from Wikidata easily.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到“世界银行”作者已撰写了五篇关于Wikidata列出的联邦共和国国家的文档。在这种情况下，这些国家是巴西、科摩罗和埃塞俄比亚。这很重要，因为世界银行数据并未告诉我们任何国家的政府形式，这些信息来自Wikidata。由于我们构建了与Wikidata对齐的本体，我们可以轻松地从Wikidata中纳入额外数据。
- en: 6\. Compare the different ways of interacting with data
  id: totrans-85
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 6\. 比较与数据交互的不同方式
- en: 'We know that we can use SPARQL to query the KG and get accurate results. But
    that requires that we write SPARQL queries, which requires some technical capabilities.
    Can we combine this accuracy with the ease of use of an LLM so that we can ‘chat’
    directly with the data? [LlamaIndex](https://www.llamaindex.ai/) is a powerful
    tool that allows you to enhance an LLM using your own data, what they call context
    information. [Here](https://gpt-index.readthedocs.io/en/latest/getting_started/starter_example.html)
    is a LlamaIndex tutorial to get started. I will use LlamaIndex to incorporate
    this World Bank data into an LLM in two ways: using the raw CSV file that we got
    directly from the World Bank API, and using the KG that we’ve built and populated
    using the World Bank data.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 我们知道可以使用SPARQL查询知识图谱并获得准确结果。但这要求我们编写SPARQL查询，这需要一些技术能力。我们能否将这种准确性与LLM的易用性相结合，从而直接与数据进行“对话”？[LlamaIndex](https://www.llamaindex.ai/)是一个强大的工具，允许你使用自己的数据（他们称之为上下文信息）来增强LLM。[这里](https://gpt-index.readthedocs.io/en/latest/getting_started/starter_example.html)是一个LlamaIndex入门教程。我将使用LlamaIndex以两种方式将世界银行数据纳入LLM：使用我们直接从世界银行API获得的原始CSV文件，以及使用我们用世界银行数据构建和填充的知识图谱。
- en: Use raw CSV file we got directly from World Bank API
  id: totrans-87
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用我们直接从世界银行API获得的原始CSV文件
- en: As a starting point, let’s load the raw metadata from the World Bank into LlamaIndex.
    This will serve as a benchmark to which we can compare the results after incorporating
    the KG. The raw data that comes from the World Bank is in CSV format and does
    not have any ontology associated with it. Here is all the code you need to get
    LlamaIndex set up. You’ll need an OpenAI API key, which you can get from the OpenAI
    website. This code reads data directly from a local data folder, I have named
    my folder ‘data’. You can just put a csv file directly into this folder and LlamaIndex
    will index it.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 作为起点，让我们将来自世界银行的原始元数据加载到LlamaIndex中。这将作为一个基准，用来比较在整合知识图谱后结果的变化。来自世界银行的原始数据是CSV格式的，并且没有任何关联的本体。以下是设置LlamaIndex所需的所有代码。你需要一个OpenAI
    API密钥，可以从OpenAI网站获取。此代码直接从本地数据文件夹读取数据，我将我的文件夹命名为‘data’。你只需将CSV文件放入此文件夹中，LlamaIndex将对其进行索引。
- en: '[PRE8]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Now we can ask it some questions, using basic human English, just like we would
    ChatGPT.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以用基本的英语提问，就像我们对待ChatGPT一样。
- en: '[PRE9]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Here is the response:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 这是响应：
- en: '`Brazil — LATIN AMERICA AND CARIBBEAN — P126452 — Rio Grande do Norte: Regional
    Development and Governance — Audited Financial Statement Brazil — LATIN AMERICA
    AND CARIBBEAN- P158000- Amazon Sustainable Landscapes Project — Procurement Plan`'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '`Brazil — LATIN AMERICA AND CARIBBEAN — P126452 — Rio Grande do Norte: Regional
    Development and Governance — Audited Financial Statement Brazil — LATIN AMERICA
    AND CARIBBEAN- P158000- Amazon Sustainable Landscapes Project — Procurement Plan`'
- en: These documents are, in fact, World Bank documents about Brazil that are in
    the context information. There are, however, many other documents that are not
    listed here.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 这些文档实际上是关于巴西的世界银行文档，包含在上下文信息中。然而，还有许多其他文档未列出。
- en: Now let’s try to get all of the documents that Anna Corsi has written.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们尝试获取Anna Corsi编写的所有文档。
- en: '[PRE10]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The response is:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 响应是：
- en: '`Corsi,Anna has not written any documents based on the context information.`'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '`Corsi, Anna没有根据上下文信息编写任何文档。`'
- en: This is incorrect. Anna Corsi is an author in the data.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 这不正确。Anna Corsi是数据中的一位作者。
- en: 'Let’s try one more time:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们再试一次：
- en: '[PRE11]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'And the response is:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 响应是：
- en: '`Anna Corsi is not mentioned in the given context information`'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '`Anna Corsi在给定的上下文信息中没有提及`'
- en: Again, this is incorrect. Anna Corsi is one of the World Bank authors in our
    data.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，这不正确。Anna Corsi是我们数据中的世界银行作者之一。
- en: Use the knowledge graph as input
  id: totrans-105
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用知识图谱作为输入
- en: Now the big question — does using a knowledge graph as our context information
    improve these results? Rather than load the RDF data into LlamaIndex the same
    way, we will use the [RDFReader](https://github.com/emptycrown/llama-hub/tree/main/llama_hub/file/rdf).
    I have found this method of loading data into LlamaIndex to be a bit more problematic
    and it takes longer, but it is recommended way of incorporating RDF data into
    your input prompt. RDFReader requires that every entity in the KG have a label.
    So even if you add a comment on an entity you also need to add a label to the
    comment.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 现在大问题是——使用知识图谱作为我们的上下文信息是否改善了这些结果？我们将不再以相同的方式将RDF数据加载到LlamaIndex中，而是使用[ RDFReader](https://github.com/emptycrown/llama-hub/tree/main/llama_hub/file/rdf)。我发现这种将数据加载到LlamaIndex中的方法有些问题，所需时间较长，但它是将RDF数据纳入输入提示的推荐方式。RDFReader要求知识图谱中的每个实体都必须有一个标签。因此，即使你在实体上添加了评论，也需要为评论添加一个标签。
- en: '[PRE12]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '***Note:*** *that I am still using the GPTVectorStoreIndex for the RDF data,
    rather than the KnowledgeGraphIndex. I didn’t find that the KnowledgeGraphIndex
    produced any better results but it took a lot more time and was a lot more costly.*
    [*Others*](https://www.mikulskibartosz.name/llama-index-which-index-should-you-use/)
    *seem to have had similar experiences. My understanding is the the RDFReader is
    for using a pre-made KG and the KGIndex is for creating a KG with LlamaIndex.*'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '***注意：*** *我仍在使用GPTVectorStoreIndex处理RDF数据，而不是KnowledgeGraphIndex。我发现KnowledgeGraphIndex没有产生更好的结果，但它花费了更多的时间和成本。*
    [*其他人*](https://www.mikulskibartosz.name/llama-index-which-index-should-you-use/)
    *似乎也有类似的经历。我的理解是RDFReader用于使用预制的知识图谱，而KGIndex用于使用LlamaIndex创建知识图谱。*'
- en: 'Once initialized, we can ask this LLM the same questions to see if the answers
    are any better. For the first prompt, “Show me all of the World Bank documents
    in the context information about Brazil,” the response is:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦初始化，我们可以向这个LLM提出相同的问题，看看答案是否有所改进。对于第一个提示，“展示上下文信息中所有关于巴西的世界银行文档”，响应是：
- en: '[PRE13]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: This is partially correct. Two of these entities are documents about Brazil,
    but two are about Argentina, and one is about Ethiopia. There are also additional
    documents about Brazil that are not listed here.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 这部分是正确的。这些实体中有两个是关于巴西的文档，但有两个是关于阿根廷的，还有一个是关于埃塞俄比亚的。还有一些关于巴西的额外文档未在此列出。
- en: 'Now let’s ask about a specific author again:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们再问一个具体的作者：
- en: '[PRE14]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The response is:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 回复是：
- en: '[PRE15]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: This is partially correct. Anna Corsi did write this document, but she wrote
    three other documents in the context information that are not listed here.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 这部分是正确的。安娜·科尔西确实写了这份文档，但她还在背景信息中写了另外三份未在此列出的文档。
- en: '[PRE16]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'The response is:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 回复是：
- en: '`‘Anna Corsi is an author of the document “Concept Project Information Document
    (PID) — Land administration infrastructure for green and sustainable development
    — P179217”. This document is about a project to support the development of a mass
    valuation system in Turkey and generate market values for individual property
    units. Corsi is also the author of “The Time is Now : How Can Uzbekistan Leverage
    Urbanization as a Driver of Sustainable Development?”, a document about how Uzbekistan
    can use urbanization to promote sustainable development.’`'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '`‘安娜·科尔西是文档“概念项目信息文档（PID）——绿色和可持续发展的土地管理基础设施——P179217”的作者。该文档涉及一个支持土耳其大规模评估系统发展的项目，并为单个财产单位生成市场价值。科尔西还是“现在就是时候：乌兹别克斯坦如何利用城市化推动可持续发展？”的作者，该文档讨论了乌兹别克斯坦如何利用城市化促进可持续发展。’`'
- en: This is partially correct. The first part, about the land administration project
    in Turkey, is correct. Anna Corsi was not, however, an author of the Uzbekistan
    report.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 这部分是正确的。关于土耳其土地管理项目的第一部分是正确的。然而，安娜·科尔西并不是乌兹别克斯坦报告的作者。
- en: 'The LLM clearly struggles with specific structured data questions. But, as
    we said at the beginning, LLMs excel at understanding and generating human-like
    text, capturing intricate language patterns, and producing contextually relevant
    responses. Here is an example of how we can use the strengths of the LLM on this
    dataset. Suppose we already know that the World Bank has a land management infrastructure
    project in Turkey. We can ask the LLM to describe the project to us using the
    following prompt:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 语言模型明显在处理特定结构化数据问题时有困难。不过，正如我们开始时所说，语言模型擅长理解和生成类似人类的文本，捕捉复杂的语言模式，并产生上下文相关的回答。以下是如何利用语言模型的优势来处理这一数据集的示例。假设我们已经知道世界银行在土耳其有一个土地管理基础设施项目。我们可以要求语言模型用以下提示描述该项目：
- en: '[PRE17]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The response is:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 回复是：
- en: '`The objective of the Land Management Infrastructure for Green and Sustainable
    Development Project is to improve the accuracy and accessibility of land administration
    information in Turkiye. There are three components to the project, the first component
    being creating 3D city models and updating cadastre data. This component will
    support: (i) the creation of 3D city models based on the proven approach tested
    in the Amasya pilot; and (ii) the completion of the update and verification of
    cadastral data for 6 million parcels (out of the total remaining 11 million parcels19
    not covered by LRCMP), in both urban and rural areas. As part of the cadastre
    updating activities, capacity building programs for addressing challenges concerning
    women’s land rights and ownership will be discussed with TKGM to determine how
    to better address these issues as part of the public consultation step during
    the surveying process. While activities on the update and verification of cadastral
    data will be carried out in both urban and rural areas, for the creation of 3D
    city models, the Project will cover major urban areas (approximately 40,000 km,
    almost all urban areas in the country) in all 81 provinces in Turkiye. As part
    of these activities, the project will finance the completion and renewal`'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '`绿色和可持续发展土地管理基础设施项目的目标是提高土耳其土地管理信息的准确性和可及性。该项目有三个组成部分，第一个组成部分是创建3D城市模型和更新土地登记数据。该部分将支持：（i）基于在阿马斯亚试点中测试的成熟方法创建3D城市模型；（ii）完成对600万块地块的更新和验证（剩余的1100万块地块中有19块未包含在LRCMP中），包括城市和农村地区。作为土地登记数据更新活动的一部分，将与TKGM讨论如何更好地解决女性土地权利和所有权方面的挑战，以便在测量过程中作为公众咨询步骤的一部分进行。尽管土地登记数据的更新和验证活动将在城市和农村地区进行，但在创建3D城市模型时，项目将覆盖土耳其所有81个省份的主要城市区域（约40,000平方公里，几乎覆盖全国所有城市区域）。作为这些活动的一部分，项目将资助完成和更新`'
- en: This is an accurate summary of the document in our database. This information
    largely comes from the ‘abstract’ field in the data.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们数据库中文档的准确摘要。这些信息主要来自数据中的‘abstract’字段。
- en: Conclusion
  id: totrans-126
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: In conclusion, incorporating structured data directly into LLMs through in-context
    learning yields suboptimal results, and challenges such as occasional hallucinations
    and lack of transparency persist. While transforming structured data into knowledge
    graphs improves LLM performance to some extent, formal query languages like SPARQL
    outperform LLMs in specific queries across structured datasets. However, LLMs
    can assist in formulating SPARQL queries, providing a way for users to ‘chat’
    with structured data without writing formal queries.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，通过上下文学习将结构化数据直接纳入LLMs会产生次优结果，且偶尔出现幻觉和缺乏透明度等挑战仍然存在。虽然将结构化数据转换为知识图谱在一定程度上改善了LLM的表现，但像SPARQL这样的正式查询语言在特定查询中比LLMs更具优势。然而，LLMs可以协助制定SPARQL查询，为用户提供无需编写正式查询的方式与结构化数据进行‘聊天’。
- en: LLMs have demonstrated their prowess in understanding and interpreting unstructured
    data, allowing them to provide relevant responses. This capability extends even
    to structured data when unstructured information is embedded within it. For instance,
    if the structured data includes a column labeled ‘abstracts’ containing unstructured
    text, the LLM can leverage that data to generate insightful results.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: LLMs在理解和解释非结构化数据方面展现了其强大能力，使其能够提供相关的响应。这种能力甚至扩展到了结构化数据，只要其中嵌入了非结构化信息。例如，如果结构化数据包含一个标记为‘abstracts’的列，其中包含非结构化文本，则LLM可以利用这些数据生成有洞察力的结果。
- en: Alternative synergies between LLMs and knowledge graphs, such as using LLMs
    to translate prompts into formal queries or leveraging knowledge graphs for LLM
    validation, offer potential avenues for exploration and advancement in the field.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 变革性协同作用如将LLMs用于将提示翻译成正式查询，或利用知识图谱对LLMs进行验证，为该领域的探索和进步提供了潜在途径。
