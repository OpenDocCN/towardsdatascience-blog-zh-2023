- en: The Infinite Babel Library of LLMs
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LLMs 的无限巴别图书馆
- en: 原文：[https://towardsdatascience.com/the-infinite-babel-library-of-llms-90e203b2f6b0](https://towardsdatascience.com/the-infinite-babel-library-of-llms-90e203b2f6b0)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/the-infinite-babel-library-of-llms-90e203b2f6b0](https://towardsdatascience.com/the-infinite-babel-library-of-llms-90e203b2f6b0)
- en: '| ARTIFICIAL INTELLIGENCE | FUTURE |TRANSFORMERS |'
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_TB
  zh: '| 人工智能 | 未来 | 变压器 |'
- en: 'Open-source, data, and attention: How the future of LLMs will change'
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 开源、数据和关注：LLMs 未来将如何改变
- en: '[](https://salvatore-raieli.medium.com/?source=post_page-----90e203b2f6b0--------------------------------)[![Salvatore
    Raieli](../Images/6bb4520e2df40d20283e7283141b5e06.png)](https://salvatore-raieli.medium.com/?source=post_page-----90e203b2f6b0--------------------------------)[](https://towardsdatascience.com/?source=post_page-----90e203b2f6b0--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----90e203b2f6b0--------------------------------)
    [Salvatore Raieli](https://salvatore-raieli.medium.com/?source=post_page-----90e203b2f6b0--------------------------------)'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://salvatore-raieli.medium.com/?source=post_page-----90e203b2f6b0--------------------------------)[![Salvatore
    Raieli](../Images/6bb4520e2df40d20283e7283141b5e06.png)](https://salvatore-raieli.medium.com/?source=post_page-----90e203b2f6b0--------------------------------)[](https://towardsdatascience.com/?source=post_page-----90e203b2f6b0--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----90e203b2f6b0--------------------------------)
    [Salvatore Raieli](https://salvatore-raieli.medium.com/?source=post_page-----90e203b2f6b0--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----90e203b2f6b0--------------------------------)
    ·16 min read·May 8, 2023
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----90e203b2f6b0--------------------------------)
    ·16 分钟阅读·2023 年 5 月 8 日
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/662b901cf99f5b6eb661b1c961669369.png)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/662b901cf99f5b6eb661b1c961669369.png)'
- en: image by the author using OpenAI DALL-E
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 作者使用 OpenAI DALL-E 创建的图像
- en: “‘[The Godfather of A.I.’ Leaves Google and Warns of Danger Ahead](https://www.nytimes.com/2023/05/01/technology/ai-google-chatbot-engineer-quits-hinton.html)”,
    is the title of the New York Times. How we can know if LMs are a threat to humanity
    if they are not open-source? What is actually happening? How the world of language
    models is on the brink of Changement.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: “‘[人工智能的教父’ 离开谷歌并警告前方危险](https://www.nytimes.com/2023/05/01/technology/ai-google-chatbot-engineer-quits-hinton.html)”
    是纽约时报的标题。如果语言模型没有开源，我们怎么知道它们是否对人类构成威胁？实际上发生了什么？语言模型的世界如何在变化的边缘。
- en: The calling for the open-source crusade
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 号召开源运动
- en: '![](../Images/6d9b96a4ae8fdb8e31a90144a6a87261.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6d9b96a4ae8fdb8e31a90144a6a87261.png)'
- en: image by [Nik Shuliahin](https://unsplash.com/it/@tjump) on Unsplash.com
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 图像由 [Nik Shuliahin](https://unsplash.com/it/@tjump) 提供，来自 Unsplash.com
- en: A short while ago [GPT-4](https://openai.com/research/gpt-4) was revealed to
    the public, and I think we all went to read the technical report and were disappointed.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 不久前 [GPT-4](https://openai.com/research/gpt-4) 向公众公开，我认为我们都去阅读了技术报告，但感到失望。
- en: '![](../Images/7a0bed6c4021763586fbc5da9961724a.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7a0bed6c4021763586fbc5da9961724a.png)'
- en: 'Technical report GPT-4\. screenshot by the author, image source: [here](https://arxiv.org/pdf/2303.08774.pdf)'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 技术报告 GPT-4。由作者截屏，图像来源：[这里](https://arxiv.org/pdf/2303.08774.pdf)
- en: 'Recently, [Nature also addressed the issue](https://www.nature.com/articles/d41586-023-01295-4):
    we need [large language models](https://en.wikipedia.org/wiki/Large_language_model)
    (LLMs) to be open-source.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，[《自然》也讨论了这个问题](https://www.nature.com/articles/d41586-023-01295-4)：我们需要 [大型语言模型](https://en.wikipedia.org/wiki/Large_language_model)
    (LLMs) 开源。
- en: Many of the LLMs are proprietary, not released, and we don’t know what data
    they were trained on. This does not allow them to be inspected and tested for
    limitations, especially with regard to bias.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 许多 LLMs 是专有的，没有发布，我们不知道它们是用什么数据训练的。这不允许对它们进行检查和测试，特别是关于偏见的方面。
- en: In addition, sharing information and code with ChatGPT is at risk of leakage
    [as discovered by Samsung](https://gizmodo.com/chatgpt-ai-samsung-employees-leak-data-1850307376).
    Not to mention that some states believe that data storage by [these companies
    violates the GDPR](https://blog.davidlibeau.fr/chatgpt-will-probably-never-comply-with-gdpr/).
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，与 ChatGPT 分享信息和代码存在泄漏的风险 [如三星所发现](https://gizmodo.com/chatgpt-ai-samsung-employees-leak-data-1850307376)。更不用说，一些州认为
    [这些公司的数据存储违反了 GDPR](https://blog.davidlibeau.fr/chatgpt-will-probably-never-comply-with-gdpr/)。
- en: This is why we need LLMs to be open-source, and there should be more investment
    in the development of new LLMs, such as the [BLOOM](https://pub.towardsai.net/a-new-bloom-in-ai-why-the-bloom-model-can-be-a-gamechanger-380a15b1fba7)
    consortium (a 170 B parameter LLM that was developed by an academic consortium).
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是为什么我们需要大语言模型开源，并且应当更多地投资于新大语言模型的开发，比如[BLOOM](https://pub.towardsai.net/a-new-bloom-in-ai-why-the-bloom-model-can-be-a-gamechanger-380a15b1fba7)
    联盟（一个由学术联盟开发的 1700 亿参数的大语言模型）。
- en: There has often been sensationalism in recent months, both about the real capabilities
    of these LLMs and the risks of artificial intelligence. If researchers cannot
    test the models, they cannot really assess their capabilities, and the same for
    analyzing the risks. In addition, an open-source model is much more transparent
    and the community can also try to identify the source of problematic behavior.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，经常出现对这些大语言模型实际能力和人工智能风险的夸张报道。如果研究人员无法测试模型，他们就无法真正评估其能力，分析风险也是如此。此外，开源模型要透明得多，社区也可以尝试找出问题行为的源头。
- en: Moreover, it is not a demand by academia, institution are alarmed by AI. European
    Union is discussing these days the EU AI act that can reshape the future of LLMs.
    At the same time, the [White House is pushing tech CEO](https://www.nytimes.com/2023/05/04/technology/us-ai-research-regulation.html?utm_source=tldrai)
    to limit the risk of AI. Thus, open source could be actually a future requirement
    for language models.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，这并不是学术界的要求，机构对人工智能感到警惕。欧盟这些天正在讨论可能重塑 LLM 未来的欧盟人工智能法案。与此同时，[白宫正在施压科技首席执行官](https://www.nytimes.com/2023/05/04/technology/us-ai-research-regulation.html?utm_source=tldrai)以限制人工智能的风险。因此，开源可能实际上成为语言模型的未来要求。
- en: Why ChatGPT is that good?
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为什么 ChatGPT 如此优秀？
- en: '![](../Images/318475e0d08ae720f761191b50879a3f.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/318475e0d08ae720f761191b50879a3f.png)'
- en: We have all heard about ChatGPT, and how it seemed revolutionary. But how was
    it trained?
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 我们都听说过 ChatGPT，以及它如何看起来具有革命性。但它是如何训练的呢？
- en: '[](https://medium.com/data-driven-fiction/everything-but-everything-you-need-to-know-about-chatgpt-546af7153ee2?source=post_page-----90e203b2f6b0--------------------------------)
    [## Everything but everything you need to know about ChatGPT'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/data-driven-fiction/everything-but-everything-you-need-to-know-about-chatgpt-546af7153ee2?source=post_page-----90e203b2f6b0--------------------------------)
    [## 关于 ChatGPT 的一切，你需要知道的一切'
- en: what is known, the latest news, what it is impacting, and what is changing.
    all in one article
  id: totrans-26
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 了解现状，获取最新新闻，了解其影响和变化，一篇文章全涵盖。
- en: medium.com](https://medium.com/data-driven-fiction/everything-but-everything-you-need-to-know-about-chatgpt-546af7153ee2?source=post_page-----90e203b2f6b0--------------------------------)
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: medium.com](https://medium.com/data-driven-fiction/everything-but-everything-you-need-to-know-about-chatgpt-546af7153ee2?source=post_page-----90e203b2f6b0--------------------------------)
- en: Let us start with the fact that ChatGPT was trained on the basis of an LLM (GPT
    3.5 to be precise). Typically, these GPT-like language models are trained using
    the prediction of the [next token in a sequence](https://en.wikipedia.org/wiki/Language_model)
    (from a sequence of tokens w, the model must predict the next token w+1).
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从一个事实开始，ChatGPT 是基于一种大语言模型（准确地说是 GPT 3.5）进行训练的。通常，这些类似 GPT 的语言模型是通过预测[序列中的下一个标记](https://en.wikipedia.org/wiki/Language_model)进行训练的（从标记序列
    w 中，模型必须预测下一个标记 w+1）。
- en: 'The model typically is a transformer: consisting of an encoder that receives
    input as a sequence and a decoder that generates the output sequence. The heart
    of this system is [multi-head self-attention](https://en.wikipedia.org/wiki/Attention_(machine_learning)),
    which allows the model to learn information about the context and dependencies
    between the various parts of the sequence.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型通常是一个变换器：由一个接收输入序列的编码器和一个生成输出序列的解码器组成。这个系统的核心是[多头自注意力机制](https://en.wikipedia.org/wiki/Attention_(machine_learning))，它允许模型学习有关上下文和序列各部分之间依赖关系的信息。
- en: '![](../Images/b2da300060a6991f8bfb378428c961b5.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b2da300060a6991f8bfb378428c961b5.png)'
- en: 'image source: [here](https://arxiv.org/pdf/1706.03762.pdf)'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：[这里](https://arxiv.org/pdf/1706.03762.pdf)
- en: '[GPT-3](https://en.wikipedia.org/wiki/GPT-3) was trained with this principle
    (like the other models in the Generative Pre-training Transformer, GPT, family),
    only with many more parameters and much more data (570 GB of data and 176 B of
    parameters).'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '[GPT-3](https://en.wikipedia.org/wiki/GPT-3) 就是采用这种原则进行训练的（像其他生成预训练变换器系列中的模型一样），只不过有更多的参数和数据（570GB
    的数据和 1760 亿个参数）。'
- en: '[GPT3](https://en.wikipedia.org/wiki/GPT-3) has tremendous capabilities however
    when it comes to generating text it often hallucinates, lacks helpfulness, is
    uninterpretable, and often contains biases. This means that the model is not aligned
    with what we expect from a model that generates text like a human'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '[GPT3](https://en.wikipedia.org/wiki/GPT-3)具有巨大的能力，但在生成文本时，它经常出现幻觉、缺乏帮助、不可解释，并且常常包含偏见。这意味着模型与我们期望的像人类一样生成文本的模型并不对齐。'
- en: How do we obtain ChatGPT from GPT-3?
  id: totrans-34
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 我们如何从GPT-3中获得ChatGPT？
- en: 'The process is called [Reinforcement Learning from Human Feedback](https://huggingface.co/blog/rlhf)
    (RHLF), and was described by the authors in this article:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 这个过程称为[来自人类反馈的强化学习](https://huggingface.co/blog/rlhf)（RHLF），作者在这篇文章中进行了描述：
- en: '[](https://arxiv.org/abs/2203.02155?source=post_page-----90e203b2f6b0--------------------------------)
    [## Training language models to follow instructions with human feedback'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://arxiv.org/abs/2203.02155?source=post_page-----90e203b2f6b0--------------------------------)
    [## 通过人类反馈训练语言模型以遵循指令'
- en: Making language models bigger does not inherently make them better at following
    a user's intent. For example, large…
  id: totrans-37
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 让语言模型变得更大并不会自动提高它们遵循用户意图的能力。例如，大型…
- en: arxiv.org](https://arxiv.org/abs/2203.02155?source=post_page-----90e203b2f6b0--------------------------------)
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '[arxiv.org](https://arxiv.org/abs/2203.02155?source=post_page-----90e203b2f6b0--------------------------------)'
- en: 'Here I will describe it very generally and succinctly. Specifically, it consists
    of three steps:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我将非常简要地描述它。具体来说，它包括三个步骤：
- en: '**Supervised fine-tuning**, is the first step in which the LLM is fine-tuned
    to learn a supervised policy (baseline model or SFT model).'
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**监督微调**是第一步，其中LLM被微调以学习监督策略（基线模型或SFT模型）。'
- en: '**Mimic human preferences**, in this step, the annotators must vote on a set
    of outputs from the baseline model. This curated dataset is used to train a new
    model, the reward model.'
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**模仿人类偏好**，在这一步中，注释员必须对基线模型的一组输出进行投票。这些经过整理的数据集用于训练新模型，即奖励模型。'
- en: '[**Proximal Policy Optimization (PPO)**](/proximal-policy-optimization-ppo-explained-abed1952457b?gi=9f8ba523b43c),
    here the reward model is used to fine-tune the SFT model and obtain the policy
    model'
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[**近端策略优化（PPO）**](/proximal-policy-optimization-ppo-explained-abed1952457b?gi=9f8ba523b43c)，在这里，奖励模型用于微调SFT模型并获得策略模型。'
- en: '![](../Images/3e1261510ad93d65caeb90c9526f8059.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3e1261510ad93d65caeb90c9526f8059.png)'
- en: 'image source: [here](https://arxiv.org/pdf/2203.02155.pdf)'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：[这里](https://arxiv.org/pdf/2203.02155.pdf)
- en: To prepare for the first step OpenAI collected a series of prompts and asked
    human annotators to write down the expected response (12–15 K prompts). Some of
    these prompts were collected from GPT-3 users, so probably what one writes on
    ChatGPT will be used for the next model.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 为了准备第一步，OpenAI收集了一系列提示，并要求人类注释员写下预期的响应（12–15K提示）。其中一些提示来自GPT-3用户，因此在ChatGPT上写的内容可能会用于下一个模型。
- en: The authors used as a model GPT-3.5 that had already been fine-tuned on programming
    code, this also explains the code capabilities of ChatGPT.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 作者使用了作为模型的GPT-3.5，该模型已经在编程代码上进行了微调，这也解释了ChatGPT的代码能力。
- en: Now this step however is not exactly scalable since it is supervised learning.
    In any case, the model thus obtained is not yet aligned.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这一步并不完全具有可扩展性，因为它是监督学习。无论如何，由此获得的模型尚未对齐。
- en: '![](../Images/da31c4b18f10f3ac161b16a6c788db60.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/da31c4b18f10f3ac161b16a6c788db60.png)'
- en: 'image source: [here](https://arxiv.org/pdf/2203.02155.pdf)'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：[这里](https://arxiv.org/pdf/2203.02155.pdf)
- en: The annotators noted a range of responses from the SFT model, according to how
    desirable such a response is (from worst to best). We now have a much larger dataset
    (10 x) and provide the SFT model responses to the new model, which must rank in
    order of preference.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 注释员根据SFT模型的响应范围进行标注，根据响应的期望程度（从最差到最好）。我们现在拥有一个更大的数据集（10倍），并将SFT模型的响应提供给新模型，新模型必须按偏好顺序进行排名。
- en: During this stage, the model is learning a general policy about the data, and
    how to maximize its reward (when he is able to rank well the outputs).
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一阶段，模型正在学习关于数据的一般策略，以及如何最大化其奖励（当它能够很好地排名输出时）。
- en: '![](../Images/74cd14b951a1055a96b3f03687234a47.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/74cd14b951a1055a96b3f03687234a47.png)'
- en: 'image source: [here](https://arxiv.org/pdf/2203.02155.pdf)'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：[这里](https://arxiv.org/pdf/2203.02155.pdf)
- en: So we have the SFT model, and we use its weights to initialize a new PPO model.
    This model is fine-tuned using Proximal Policy Optimization (PPO).
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们有了SFT模型，并使用其权重来初始化一个新的PPO模型。该模型使用近端策略优化（PPO）进行微调。
- en: In other words, we use a reinforcement learning algorithm. The PPO model receives
    a random prompt and responds to the prompt, after which it receives a penalty
    or reward. Instead of classical [Q-learning](https://it.wikipedia.org/wiki/Q-learning),
    here the model policy is updated to each response (the model learns directly from
    experience, on policy).
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，我们使用了强化学习算法。PPO 模型接收到一个随机提示并对其做出响应，然后它会收到惩罚或奖励。与经典的 [Q-learning](https://it.wikipedia.org/wiki/Q-learning)
    相比，这里模型策略会在每次响应后更新（模型直接从经验中学习，在策略上）。
- en: In addition, the authors use the per-token [Kullback-Leibler (KL)](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence)
    penalty to make the model’s response distribution similar to that of the SFT model.
    This is because we want to optimize the model with the RL (due to the reward model)
    but we still do not want it to forget what it learned in step 1, which are prompts
    curated by humans.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，作者使用每个 token 的 [Kullback-Leibler (KL)](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence)
    惩罚，使模型的响应分布类似于 SFT 模型的分布。这是因为我们希望通过 RL（由于奖励模型）优化模型，但我们仍然不希望它忘记在步骤 1 中学到的内容，这些是由人类策划的提示。
- en: 'Finally, the model is evaluated on three aspects: helpfulness, truthfulness,
    and harmlessness. After all, these were exactly the aspects we wanted to optimize.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，该模型在三个方面进行评估：有用性、真实性和无害性。毕竟，这正是我们希望优化的方面。
- en: A curious note is that the model when evaluated on classic benchmarks (question
    answering, summarization, classification) has lower performance than GPT-3\. This
    is the cost of alignment.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 一个值得注意的事实是，当在经典基准测试（问答、摘要、分类）上评估时，该模型的表现低于 GPT-3。这就是对齐的代价。
- en: Alpaca, a revolutionary animal
  id: totrans-59
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Alpaca，一种革命性的动物
- en: '![](../Images/46194285bf0755a3f0a2eb2df10dab3c.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/46194285bf0755a3f0a2eb2df10dab3c.png)'
- en: image by [Dong Cheng](https://unsplash.com/it/@dongcheng) on Unsplash
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由 [Dong Cheng](https://unsplash.com/it/@dongcheng) 在 Unsplash 提供
- en: As mentioned there is a real need to study the behavior of these models and
    this is only possible if they are open source. On the other hand, any LM can be
    aligned using RHLF.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，确实需要研究这些模型的行为，只有在它们是开源的情况下才能做到这一点。另一方面，任何语言模型都可以使用 RHLF 进行对齐。
- en: RHLF is much less expensive and computationally intensive than training a model
    from scratch. On the other hand, it requires that there be annotators (you do
    indeed need a dataset with instructions). **But can’t these steps be automated?**
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: RHLF 比从头开始训练模型要便宜且计算密集度低得多。另一方面，它要求有标注者（你确实需要一个包含指令的数据集）。**但这些步骤不能自动化吗？**
- en: The first step was [Self-instruct](https://arxiv.org/pdf/2212.10560.pdf), in
    this 2022 article, the authors propose a semi-automated method. In fact, the general
    idea is to start with a set of manually-written instructions. This set of instructions
    serves both as seeds and to be sure that most [NLP](https://en.wikipedia.org/wiki/Natural_language_processing)
    tasks are covered.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步是 [Self-instruct](https://arxiv.org/pdf/2212.10560.pdf)，在这篇 2022 年的文章中，作者提出了一种半自动化的方法。实际上，一般的想法是从一组手动编写的指令开始。这些指令集既作为种子，又确保涵盖了大多数
    [NLP](https://en.wikipedia.org/wiki/Natural_language_processing) 任务。
- en: Starting then with only 175 instructions prompted the model to generate the
    dataset (50k instructions). The dataset was then used for instruction tuning.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 从仅有的 175 个指令开始，模型生成了数据集（50k 指令）。然后，该数据集用于指令调整。
- en: '![](../Images/3614a712dd4be63ad2280e789d181ed2.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3614a712dd4be63ad2280e789d181ed2.png)'
- en: 'A high-level overview of SELF-INSTRUCT. image source: [here](https://arxiv.org/pdf/2212.10560.pdf)'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: SELF-INSTRUCT 的高级概览。图片来源：[这里](https://arxiv.org/pdf/2212.10560.pdf)
- en: Having a method needed only a model. ChatGPT is based on OpenAI GPT-3.5, but
    can’t a smaller model be used? **Does it necessarily need more than 100 B parameters?**
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 仅有一个模型需要的方法。ChatGPT 基于 OpenAI GPT-3.5，但难道不能使用更小的模型吗？**它真的需要超过 100 B 的参数吗？**
- en: 'Instead, the Stanford researchers used LLaMA and specifically the 7B version
    and 52 K instructions generated following the self-instruct method (instructions
    generated using OpenAI’s text-davinci-003). The real value of Alpaca is that the
    authors simplified the pipeline and greatly reduced costs in a way that any academic
    lab could replicate the process (which is in [this repository](https://github.com/tatsu-lab/stanford_alpaca)).
    As in fact stated:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，斯坦福研究人员使用了 LLaMA，特别是 7B 版本，并按照 self-instruct 方法生成了 52 K 指令（使用 OpenAI 的 text-davinci-003
    生成的指令）。Alpaca 的真正价值在于作者简化了流程，大大降低了成本，使任何学术实验室都能复制这个过程（在 [这个库](https://github.com/tatsu-lab/stanford_alpaca)
    中）。实际上如所述：
- en: For our initial run, fine-tuning a 7B LLaMA model took 3 hours on 8 80GB A100s,
    which costs less than $100 on most cloud compute providers. ([source](https://crfm.stanford.edu/2023/03/13/alpaca.html))
  id: totrans-70
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 在我们的初步运行中，微调一个 7B 的 LLaMA 模型在 8 台 80GB 的 A100 上花费了 3 小时，这在大多数云计算提供商上花费不到 100
    美元。([source](https://crfm.stanford.edu/2023/03/13/alpaca.html))
- en: The initial model evaluation showed that Alpaca is almost good at GPT-3.5 (in
    some cases even exceeding it). This may seem surprising given that this is a model
    that is 20 times smaller. On the other hand, the model behaved like GPT in a series
    of inputs (so the training acts as a kind of knowledge distillation). On the other
    hand, the model has the same limitations as typical language models, showing hallucinations,
    toxicity, and stereotypes.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 初步模型评估显示，Alpaca 的表现几乎与 GPT-3.5 一致（在某些情况下甚至超过了 GPT-3.5）。这可能让人感到惊讶，因为这是一个体积小了
    20 倍的模型。另一方面，该模型在一系列输入下表现得像 GPT（因此训练相当于一种知识蒸馏）。另一方面，该模型也具有典型语言模型的相同局限性，表现出幻觉、毒性和刻板印象。
- en: Alpaca then demonstrates that any academic laboratory can train its own version
    of ChatGPT (using [LLaMA](https://medium.com/mlearning-ai/metas-llama-a-small-language-model-beating-giants-5065948e0b7f),
    which is available only for research). On the other hand, any company using another
    model can align and create its own version of ChatGPT. In addition, similar models
    could still even be deployed on [cell phones](https://twitter.com/thiteanish/status/1635188333705043969)
    or [Raspberry Pi computers](https://msproul.github.io/AlpacaPi/).
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: Alpaca 证明了任何学术实验室都可以训练自己的 ChatGPT 版本（使用[ LLaMA](https://medium.com/mlearning-ai/metas-llama-a-small-language-model-beating-giants-5065948e0b7f)，该模型仅供研究使用）。另一方面，任何使用其他模型的公司都可以调整并创建自己的
    ChatGPT 版本。此外，类似的模型甚至可以部署在[手机](https://twitter.com/thiteanish/status/1635188333705043969)或[树莓派计算机](https://msproul.github.io/AlpacaPi/)上。
- en: The authors released a demo, but it was [shut down](https://www.theregister.com/2023/03/21/stanford_ai_alpaca_taken_offline/)
    after a short time (as a matter of security). Also, although one had to apply
    to use LLaMA (and access the model weights), a few days later the model [was leaked
    online](https://www.theverge.com/2023/3/8/23629362/meta-ai-language-model-llama-leak-online-misuse).
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 作者发布了一个演示，但在短时间内被[关闭](https://www.theregister.com/2023/03/21/stanford_ai_alpaca_taken_offline/)（出于安全考虑）。此外，尽管使用
    LLaMA 需要申请（并访问模型权重），但几天后模型[被泄露到网上](https://www.theverge.com/2023/3/8/23629362/meta-ai-language-model-llama-leak-online-misuse)。
- en: Are LLMs at the border of a revolution?
  id: totrans-74
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LLM 是否处于革命的边缘？
- en: '![](../Images/b409e6e6cbb3677fc3eb3fe8c02c99f3.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b409e6e6cbb3677fc3eb3fe8c02c99f3.png)'
- en: 'image source: [here](https://arxiv.org/pdf/2304.13712.pdf)'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：[这里](https://arxiv.org/pdf/2304.13712.pdf)
- en: It seems like it has been years since ChatGPT was released but instead, it was
    only a few months. Up to that time we were talking about the power law, how it
    was necessary for a model to have more parameters, more data, and more training
    in order to allow for the origin of emergent behaviors.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来自从 ChatGPT 发布以来已经过了好几年，但实际上仅仅只有几个月。到那个时候，我们在讨论幂律法则，如何让一个模型拥有更多的参数、更多的数据和更多的训练，以便使新兴行为得以出现。
- en: These ideas led to the idea that we could define a kind of [Moore’s law](https://en.wikipedia.org/wiki/Moore%27s_law)
    for language models. In a sense, in recent years we have seen almost an exponential
    law (we have gone from 1.5 B parameters for GPT-2 to 175 B for GPT-3).
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 这些想法促成了我们可以为语言模型定义一种[摩尔定律](https://en.wikipedia.org/wiki/Moore%27s_law)的想法。在某种意义上，近年来我们几乎看到了一个指数法则（我们从
    GPT-2 的 15 亿参数发展到 GPT-3 的 1750 亿参数）。
- en: What has changed?
  id: totrans-79
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 有什么变化？
- en: The first blow to this doctrine could be called, the arrival of [Chinchilla](https://en.wikipedia.org/wiki/Chinchilla_AI).
    DeepMind’s model showed that it is not only a matter of data quantity but also
    of data quality. Second, META’s LLaMA showed that even smaller models using a
    curated data set can achieve similar if not better results than huge models.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 对这个理论的第一次冲击可以称之为，[Chinchilla](https://en.wikipedia.org/wiki/Chinchilla_AI)的到来。DeepMind
    的模型表明，这不仅仅是数据量的问题，还有数据质量的问题。其次，META 的 LLaMA 表明，即使是使用精心挑选的数据集的小型模型也能取得与大型模型相似甚至更好的结果。
- en: It is not just a matter of models. The data is the other issue. Humans do not
    produce enough data, probably not enough data to support any GPT-5 according to
    when required by the power law. Second, the data will not be as accessible as
    before.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 这不仅仅是模型的问题。数据是另一个问题。人类生产的数据不足，可能不够支持任何需要按照幂律法则所需的 GPT-5。其次，数据将不再像以前那样容易获取。
- en: In fact, Reddit (a popular data resource) has announced that [AI developers
    will have to pay](https://www.reddit.com/r/reddit/comments/12qwagm/an_update_regarding_reddits_api/)
    to access its content. Even [Wikipedia has thought the same](https://voicebot.ai/2021/03/17/wikipedia-will-start-charging-tech-giants-and-their-voice-assistants-for-data-access/)
    and now [StackOverflow](https://www.zdnet.com/article/stack-overflow-joins-reddit-and-twitter-in-charging-ai-companies-for-training-data/)
    is moving in the same way, it will require companies to pay.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，Reddit（一个受欢迎的数据资源）已宣布[AI 开发者将需付费](https://www.reddit.com/r/reddit/comments/12qwagm/an_update_regarding_reddits_api/)才能访问其内容。即使是[维基百科也有相同的想法](https://voicebot.ai/2021/03/17/wikipedia-will-start-charging-tech-giants-and-their-voice-assistants-for-data-access/)，现在[StackOverflow](https://www.zdnet.com/article/stack-overflow-joins-reddit-and-twitter-in-charging-ai-companies-for-training-data/)也在朝着这个方向发展，它将要求公司支付费用。
- en: “Community platforms that fuel LLMs absolutely should be compensated for their
    contributions so that companies like us can reinvest back into our communities
    to continue to make them thrive,” Stack Overflow’s Chandrasekar says. “We’re very
    supportive of Reddit’s approach.” ([source](https://www.wired.com/story/stack-overflow-will-charge-ai-giants-for-training-data/))
  id: totrans-83
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: Stack Overflow的Chandrasekar说：“社区平台为LLMs提供支持，绝对应该得到补偿，以便像我们这样的公司可以重新投资于我们的社区，使其继续繁荣。”他说：“我们非常支持Reddit的做法。”
    ([来源](https://www.wired.com/story/stack-overflow-will-charge-ai-giants-for-training-data/))
- en: And even if one manages to get the data, it may not be safe the same for a company.
    [Getty has sued an AI art generator](https://www.theverge.com/2023/2/6/23587393/ai-art-copyright-lawsuit-getty-images-stable-diffusion),
    but the artists themselves have also filed lawsuits. Not to mention, that programmers
    have done the same [with GitHub Copilot](https://www.euronews.com/culture/2023/03/27/from-lawsuits-to-tech-hacks-heres-how-artists-are-fighting-back-against-ai-image-generatio)
    which has been trained with code in the repositories. In addition, the music industry
    (notoriously litigious) has [spoken out against AI-generated music](https://www.ft.com/content/aec1679b-5a34-4dad-9fc9-f4d8cdd124b9)
    and urged against streaming services. If even AI companies [appeal to fair use](https://www.theverge.com/2023/1/28/23575919/microsoft-openai-github-dismiss-copilot-ai-copyright-lawsuit),
    it is by no means a given that they will have the same access to data in the future.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 即使有人设法获得数据，对于公司而言，数据的安全性也未必得到保障。[Getty 已经起诉了一款 AI 艺术生成器](https://www.theverge.com/2023/2/6/23587393/ai-art-copyright-lawsuit-getty-images-stable-diffusion)，但艺术家们也提出了诉讼。更不用说，程序员们也对[GitHub
    Copilot](https://www.euronews.com/culture/2023/03/27/from-lawsuits-to-tech-hacks-heres-how-artists-are-fighting-back-against-ai-image-generatio)提起了诉讼，GitHub
    Copilot 是基于代码库中的代码进行训练的。此外，音乐行业（以诉讼著称）[对AI生成的音乐表示反对](https://www.ft.com/content/aec1679b-5a34-4dad-9fc9-f4d8cdd124b9)，并呼吁反对流媒体服务。如果即使AI公司[援引合理使用](https://www.theverge.com/2023/1/28/23575919/microsoft-openai-github-dismiss-copilot-ai-copyright-lawsuit)进行上诉，未来它们是否能继续获得相同的数据访问权限仍然不确定。
- en: There is another factor to consider, apart from extending models by hetero modality,
    the transformer architecture has not changed since 2017\. All language models
    are based on the dogma that only multi-head self-attention is needed and nothing
    more. Until recently Sam Altman was convinced that the scalability of the architecture
    was the key to AGI. But as he said at a recent [MIT event](https://www.imaginationinaction.co/),
    the key to AGI is not in more layers and more parameters.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 除了通过异质模态扩展模型之外，还需考虑另一个因素，自2017年起，transformer架构没有发生变化。所有语言模型都基于这样一种信条：只需多头自注意力机制即可，无需更多。直到最近，Sam
    Altman仍然认为架构的可扩展性是AGI的关键。但正如他在最近的[MIT活动](https://www.imaginationinaction.co/)上所说，AGI的关键不在于更多的层和参数。
- en: '![](../Images/e452bf39507b9ec47868c33d02c1b4a7.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e452bf39507b9ec47868c33d02c1b4a7.png)'
- en: 'image source: [here](https://arxiv.org/pdf/1706.03762.pdf)'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 图像来源：[这里](https://arxiv.org/pdf/1706.03762.pdf)
- en: 'The transformer has definite limitations and this is reflected in the LMs:
    hallucinations, toxicity, and bias. Modern LLMs are not capable of critical thinking.
    Techniques such as chain of thoughts and prompt engineering serve as patches to
    try to mitigate the problem.'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: transformer存在明确的局限性，这在语言模型中得到了反映：幻觉、毒性和偏见。现代大型语言模型（LLMs）不具备批判性思维能力。诸如思维链和提示工程等技术作为补丁，试图缓解这些问题。
- en: Moreover, multi-head self-attention has been shown to be capable of solving
    RNN-derived problems and allowing behaviors to emerge as in-context learning has
    a quadratic cost. Recently, it has been seen that one cannot replace self-attention
    with non-quadratic variants of attention without losing expressiveness. However,
    work such as [Spike-GPT](https://levelup.gitconnected.com/spikegpt-a-260-m-only-parameters-lm-not-afraid-of-competition-e262431d67aa)
    and [Hyena](https://levelup.gitconnected.com/welcome-back-80s-transformers-could-be-blown-away-by-convolution-21ff15f6d1cc)
    show that less expensive alternatives not based on self-attention exist and allow
    for comparable results in the construction of language models.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，已经证明，多头自注意力能够解决 RNN 派生的问题，并且允许行为的出现，因为上下文学习具有二次成本。最近，已发现不能用非二次的注意力变体替代自注意力，否则会失去表达力。然而，像
    [Spike-GPT](https://levelup.gitconnected.com/spikegpt-a-260-m-only-parameters-lm-not-afraid-of-competition-e262431d67aa)
    和 [Hyena](https://levelup.gitconnected.com/welcome-back-80s-transformers-could-be-blown-away-by-convolution-21ff15f6d1cc)
    这样的工作表明，存在不基于自注意力的成本较低的替代方案，并且在构建语言模型时可以获得类似的结果。
- en: Also as shown aligning a model using RHLF has a cost with respect to performance
    in the various tasks. Therefore, LMs will not replace the “expert model” but in
    the future will perhaps be orchestrators of other models (as for example suggested
    by [HuggingGPT](https://levelup.gitconnected.com/hugginggpt-give-your-chatbot-an-ai-army-cfadf5647f98)).
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，使用 RHLF 对齐模型在各种任务中的表现也有一定的成本。因此，语言模型不会取代“专家模型”，但未来可能会成为其他模型的协调者（如 [HuggingGPT](https://levelup.gitconnected.com/hugginggpt-give-your-chatbot-an-ai-army-cfadf5647f98)
    所建议的）。
- en: You cannot stop Open-source and why it is always winning
  id: totrans-91
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 你无法阻止开源，为什么它总是能胜出
- en: '![](../Images/3a6b25bb1fc139ea22e046a0242924a8.png)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3a6b25bb1fc139ea22e046a0242924a8.png)'
- en: image by [Steven Lelham](https://unsplash.com/it/@slelham)
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由 [Steven Lelham](https://unsplash.com/it/@slelham) 提供
- en: is MidJourney or DALL-E better? it is difficult perhaps to say. What is certain
    is that stable diffusion is the winning technology. Stable diffusion by the fact
    that it has been open-source has spawned so many applications and has been the
    inspiration for so much derivative research (ControlNet, synthetic data for medical
    imaging, parallels to the brain).
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 是 MidJourney 还是 DALL-E 更好？也许很难说。可以肯定的是，stable diffusion 是制胜的技术。由于 stable diffusion
    是开源的，它催生了许多应用，并且成为了许多衍生研究的灵感（ControlNet、医学成像的合成数据、大脑的类比）。
- en: Through the work of the community, Stable diffusion in its various versions
    has been improved and there are endless variations. On the other hand, there is
    no application of DALL-E that does not have a counterpart based on stable diffusion
    (but the reverse is true).
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 通过社区的努力，Stable diffusion 在其各种版本中得到了改进，并且有无尽的变体。另一方面，没有一个 DALL-E 的应用没有基于 stable
    diffusion 的对应物（但反之则不成立）。
- en: Why then has the same not happened for language models?
  id: totrans-96
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 那么，为什么语言模型没有发生同样的情况呢？
- en: So far the main problem is that training a language model was a prohibitive
    undertaking. BigScience’s BLOOM is indeed a huge consortium. But LLaMA has shown
    that much smaller models can compete with monsters of more than 100 B parameters.
    Alpaca showed that LM alignment can also be done with little cost (less than $1,000
    total cost). These are the elements that allowed Simon Willson to say “[Large
    language models are having their Stable Diffusion moment.](https://simonwillison.net/2023/Mar/11/llama/)”
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，主要问题是训练语言模型是一项费用高昂的任务。BigScience 的 BLOOM 确实是一个巨大的联盟。然而，LLaMA 已经表明，较小的模型也能与超过
    100 B 参数的巨型模型竞争。Alpaca 表明，语言模型的对齐也可以以较低的成本（总成本少于 1,000 美元）完成。这些因素使 Simon Willson
    能够说“[大型语言模型正在迎来它们的 Stable Diffusion 时刻。](https://simonwillison.net/2023/Mar/11/llama/)”
- en: 'From Alpaca to the present day, a lot of models have come out that [are open-source](https://twitter.com/dctanner/status/1643263959263322115).
    Not only has [Stability AI released a number of models](https://stability.ai/blog/stability-ai-launches-the-first-of-its-stablelm-suite-of-language-models)
    that are competitive with giants and can be used by everyone, but other companies
    have also released chatbots and models. In just a few weeks we have seen: [Dolly](https://github.com/databrickslabs/dolly),
    [HuggingChat](https://huggingface.co/chat/), Koala, and many more'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 从 Alpaca 到今天，出现了许多 [开源](https://twitter.com/dctanner/status/1643263959263322115)
    模型。不仅 [Stability AI 发布了多个与巨头竞争的模型](https://stability.ai/blog/stability-ai-launches-the-first-of-its-stablelm-suite-of-language-models)，并且可以被所有人使用，而且其他公司也发布了聊天机器人和模型。在短短几周内，我们见证了：[Dolly](https://github.com/databrickslabs/dolly)、[HuggingChat](https://huggingface.co/chat/)、Koala
    等众多新模型。
- en: '![](../Images/2dc4d41faad26b91c012f8ac3afb3148.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2dc4d41faad26b91c012f8ac3afb3148.png)'
- en: 'screenshots by the author. image source: [here](https://huggingface.co/chat/)'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的截图。图片来源：[这里](https://huggingface.co/chat/)
- en: Now, some of the models mentioned are yes open-source however they are for non-commercial
    use. although they are open to academic research this means that they cannot be
    exploited by interested companies.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，提到的一些模型确实是开源的，但它们仅用于非商业用途。虽然这些模型开放用于学术研究，但这意味着感兴趣的公司不能利用它们。
- en: 'This is only part of the story. In fact, there are already models on HuggingFace
    that can be easily trained (models, datasets, and pipelines) and there are to
    date several models that are commercially available (to date [more than 10](https://github.com/eugeneyan/open-llms)):'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 这只是故事的一部分。实际上，HuggingFace上已经有可以轻松训练的模型（模型、数据集和管道），目前已有多个商业化的模型（至今[超过10个](https://github.com/eugeneyan/open-llms)）：
- en: '![](../Images/d374e3ba279d7afd606a009bf17ab876.png)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d374e3ba279d7afd606a009bf17ab876.png)'
- en: 'screenshot by the author. source: [here](https://github.com/eugeneyan/open-llms)'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的截图。来源：[这里](https://github.com/eugeneyan/open-llms)
- en: Open-source model, private data, and new applications
  id: totrans-105
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 开源模型、私人数据和新应用
- en: '![](../Images/6138c6a0e3c35ffb7b83692a0a099541.png)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6138c6a0e3c35ffb7b83692a0a099541.png)'
- en: image by [Muhammad Zaqy Al Fattah](https://unsplash.com/it/@dizzydizz) on Unsplash
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由[Muhammad Zaqy Al Fattah](https://unsplash.com/it/@dizzydizz)在Unsplash提供
- en: Dario Amodei, [CEO of Anthropic is seeking billions](https://venturebeat.com/ai/as-anthropic-seeks-billions-to-take-on-openai-industrial-capture-is-nigh-or-is-it/)
    to beat OpenAI on the bigger model of the world. However, the rest of the world
    is moving in another direction. For example, Bloomberg, which is not a known player
    in AI [has released a LLM for finance](https://arxiv.org/abs/2303.17564v1) (trained
    on 363 billion tokens from finance sources).
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: Dario Amodei，[Anthropic的首席执行官正在寻求数十亿资金](https://venturebeat.com/ai/as-anthropic-seeks-billions-to-take-on-openai-industrial-capture-is-nigh-or-is-it/)以击败OpenAI的大型模型。然而，世界的其他地方却在朝另一个方向发展。例如，Bloomberg虽然在AI领域并不知名，[已发布了一个金融LLM](https://arxiv.org/abs/2303.17564v1)（训练于来自金融来源的3630亿个标记）。
- en: Why do we want an LLM for finance? Why do not use just ChatGPT?
  id: totrans-109
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 为什么我们需要用于金融的LLM？为什么不直接使用ChatGPT？
- en: Google MedPalm showed that a generalist model has poor performance compared
    to a model that is fine-tuned on a specific topic (in this case it was datasets
    of medical, scientific, and so on articles).
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: Google MedPalm显示，通用模型的表现不如针对特定主题（在此案例中为医学、科学等文章数据集）微调的模型。
- en: Fine-tuning an LLM is clearly expensive. Especially if we are talking about
    models with hundreds of billions of parameters. Smaller models are much less expensive,
    however still not indifferent. META’s LLaMA with being open-source has partly
    solved this problem. In fact, the authors of [LLaMA-Adapter showed that only 1.2
    million parameters](https://arxiv.org/pdf/2303.16199.pdf) need to be added in
    order to do fine-tuning (the training took less than an hour).
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 微调LLM显然成本很高，尤其是当我们谈论参数达到数百亿的模型时。较小的模型成本较低，但仍然不便宜。META的LLaMA作为开源模型在一定程度上解决了这个问题。事实上，[LLaMA-Adapter的作者展示了仅需增加1.2百万参数](https://arxiv.org/pdf/2303.16199.pdf)即可进行微调（训练时间少于一小时）。
- en: While it is true that LLaMA is not commercially available, there are many other
    models that are available (from small to large). What will obviously enable a
    successful application in a given field is data.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管LLaMA确实不可商业化，但仍有许多其他模型可以使用（从小到大）。显然，在特定领域成功应用的关键是数据。
- en: '[As Samsung discovered unpleasantly](https://www.engadget.com/three-samsung-employees-reportedly-leaked-sensitive-data-to-chatgpt-190221114.html?guccounter=1&guce_referrer=aHR0cHM6Ly93d3cuZ29vZ2xlLmNvbS8&guce_referrer_sig=AQAAAJvMJ0sb8_IHUybp8w3JmFkI5VkTKP1MgcrB9zWTaUKcBrEm8Fs-D18iKE4b9jOIFn_s-1p86ZmF0fG3V-LEFDiHRvBAlqBIDOMFFraTYxYbXzdcBNMfh8ppicFde-u_RCNbZLZGq7cgfor-7u9h-MLCn1cxhNHMVug6WJrcutz0),
    it is a risk to use ChatGPT inside a company. Even if [ChatGPT now allows people
    to disable chat history or decline to use their data](https://arstechnica.com/information-technology/2023/04/chatgpt-users-can-now-opt-out-of-chat-history-and-model-training/)
    to train the model, companies will consider it risky to concede their data.'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '[正如三星不愉快地发现的](https://www.engadget.com/three-samsung-employees-reportedly-leaked-sensitive-data-to-chatgpt-190221114.html?guccounter=1&guce_referrer=aHR0cHM6Ly93d3cuZ29vZ2xlLmNvbS8&guce_referrer_sig=AQAAAJvMJ0sb8_IHUybp8w3JmFkI5VkTKP1MgcrB9zWTaUKcBrEm8Fs-D18iKE4b9jOIFn_s-1p86ZmF0fG3V-LEFDiHRvBAlqBIDOMFFraTYxYbXzdcBNMfh8ppicFde-u_RCNbZLZGq7cgfor-7u9h-MLCn1cxhNHMVug6WJrcutz0)，在公司内部使用
    ChatGPT 是有风险的。即使[ChatGPT 现在允许用户禁用聊天记录或拒绝使用他们的数据](https://arstechnica.com/information-technology/2023/04/chatgpt-users-can-now-opt-out-of-chat-history-and-model-training/)来训练模型，公司也会认为分享他们的数据存在风险。'
- en: Many companies will consider it possible to train their own chatbot, a model
    that is fine-tuned on their own corporate data and will remain internal. After
    all, the technology is available and affordable even for companies with small
    budgets. Moreover, the low cost allows them to be able to fine-tune regularly
    as new data arrives or if a better open-source model is released. Companies that
    now have the data will be much more reluctant to grant it.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 许多公司会考虑训练自己的聊天机器人，这是一种基于自身企业数据进行微调的模型，并且将保持内部使用。毕竟，这项技术即使对于预算较小的公司也变得可用和负担得起。此外，低成本使他们能够在新数据到来时或如果有更好的开源模型发布时定期进行微调。现在拥有数据的公司会更不愿意将数据分享出去。
- en: Moreover, we have seen how important is to have quality data. Data in medicine
    and many other fields are difficult to collect (expensive, regulated, scarce)
    and companies that possess them have a vantage. OpenAI could spend billions trying
    to collect for example medical data, but beyond the cost, patient recruitment
    requires years and an established network (which it has not). Companies that have
    the data now will be more restrictive in sharing these data with models that can
    store what they are exposed.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们已经看到拥有优质数据的重要性。医学和许多其他领域的数据收集困难（昂贵、受规管、稀缺），而拥有这些数据的公司具有优势。OpenAI 可能会花费数十亿试图收集医学数据，但除了成本之外，患者招募需要多年时间和已建立的网络（而
    OpenAI 并不具备）。现在拥有数据的公司在分享这些数据时会更加严格。
- en: '![](../Images/1d2779573a760ce361c54cce016342cf.png)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1d2779573a760ce361c54cce016342cf.png)'
- en: image by [Petrebels](https://unsplash.com/it/@petrebels) on Unsplash
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源 [Petrebels](https://unsplash.com/it/@petrebels) 在 Unsplash
- en: In addition, works such as HuggingGPT and [AudioGPT](https://github.com/AIGC-Audio/AudioGPT)
    are showing the LLM is an interface for the user to interact with expert models
    (text-to-image, audio model, and much more). In the last years, many companies
    have hired data scientists and have developed different specialized models for
    their needs (pharmaceutical companies' models for drug discovery and design, manufacturing
    companies for component design and predictive maintenance, and so on). Thus, now
    data scientists can instruct LLMs to connect with their previously trained models
    and allow internal non-technical users to interact with them through textual prompts.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，像 HuggingGPT 和 [AudioGPT](https://github.com/AIGC-Audio/AudioGPT) 这样的作品表明
    LLM 是用户与专家模型（文本到图像、音频模型等）互动的界面。在过去几年中，许多公司已经聘请数据科学家并开发了不同的专业模型以满足其需求（制药公司用于药物发现和设计的模型、制造公司用于组件设计和预测性维护的模型等）。因此，现在数据科学家可以指导
    LLM 连接到他们之前训练的模型，并允许内部非技术用户通过文本提示与其互动。
- en: There is also another element that points toward such a scenario, the regulations
    on generative AI are unclear (for example, Google has not released its generative
    music model for fear of copyright infringement). In addition to the copyright
    issue, questions about liability remain open. Therefore, many companies may internalize
    the technology and create their own AI assistant in the coming months.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 还有另一个因素指向这样的情景，即生成式 AI 的规章不明确（例如，谷歌未发布其生成音乐模型以避免版权侵权）。除了版权问题，关于责任的问题仍然悬而未决。因此，许多公司可能会在接下来的几个月里内化这项技术，创建自己的
    AI 助手。
- en: Parting thoughts
  id: totrans-120
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 告别的思考
- en: '![](../Images/19a35058c4bd9cf5b03a62422a64c50e.png)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/19a35058c4bd9cf5b03a62422a64c50e.png)'
- en: image by [Saif71.com](https://unsplash.com/it/@saif71) on Unsplash.com
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源于 [Saif71.com](https://unsplash.com/it/@saif71) 在 Unsplash.com
- en: 'Dr. Hinton said that when people used to ask him how he could work on technology
    that was potentially dangerous, he would paraphrase Robert Oppenheimer, who led
    the U.S. effort to build the atomic bomb: “When you see something that is technically
    sweet, you go ahead and do it.”'
  id: totrans-123
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 辛顿博士说，当人们曾经问他如何能从事潜在危险的技术工作时，他会引用罗伯特·奥本海默的话，奥本海默领导了美国的原子弹研制工作：“当你看到一些技术上很甜美的东西时，你就去做它。”
- en: ''
  id: totrans-124
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: He does not say that anymore. ([source](https://www.nytimes.com/2023/05/01/technology/ai-google-chatbot-engineer-quits-hinton.html))
  id: totrans-125
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 他现在不再这样说了。 ([source](https://www.nytimes.com/2023/05/01/technology/ai-google-chatbot-engineer-quits-hinton.html))
- en: Hinton recently stated that we need to discuss the risks of artificial intelligence.
    But we cannot study the risks of a bomb exploding if it is inside a black box.
    That is why it is increasingly urgent for models to be Open source.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 辛顿最近表示，我们需要讨论人工智能的风险。但我们不能在一个黑箱中研究炸弹爆炸的风险。这就是为什么模型越来越迫切需要开源。
- en: LLMs are in a phase of change anyway. Creating bigger and bigger models is unsustainable
    and does not give the same advantage as it once did. The future of the next LLMs
    will lie in data and probably in new architectures no longer based on self-attention.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: LLMs 正处于变革的阶段。创建越来越大的模型是不可持续的，并且不再提供曾经的优势。下一代LLM的未来将取决于数据，并且可能基于不再依赖自注意力的新架构。
- en: However, data will not be as accessible as it once was; companies are beginning
    to stop access to it. [Microsoft says it is willing to allow companies](https://www.cnbc.com/2023/02/07/microsoft-will-offer-chatgpt-tech-for-companies-to-customize-source.html)
    to create their own version of ChatGPT. But companies will be skeptical.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，数据不会像以前那样容易获取；公司开始限制对数据的访问。 [微软表示愿意允许公司](https://www.cnbc.com/2023/02/07/microsoft-will-offer-chatgpt-tech-for-companies-to-customize-source.html)
    创建自己版本的 ChatGPT。但公司会持怀疑态度。
- en: Some companies fear for their business (it seems ChatGPT has already claimed
    [its first victim](https://www.greatandhra.com/articles/special-articles/edtech-firm-says-chatgpt-killing-its-business-128974)),
    and others are afraid of data leakage. Or simplyment the technology is finally
    within reach of almost all companies, and each will create a chatbot tailored
    to its needs.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 一些公司担心他们的业务（似乎 ChatGPT 已经宣称 [其第一个受害者](https://www.greatandhra.com/articles/special-articles/edtech-firm-says-chatgpt-killing-its-business-128974)），而其他公司则担心数据泄漏。或者，仅仅是因为技术最终几乎触手可及，每家公司都将创建一个符合自己需求的聊天机器人。
- en: 'In conclusion, we can see different trends (which in part they already happening):'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，我们可以看到不同的趋势（这些趋势在某种程度上已经在发生）：
- en: A mounting fear of AI is pushing for open-source models
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对人工智能日益增长的恐惧正在推动开源模型的发展
- en: This is leading to an increasing publication of open-source LLMs models. Which
    in turn, it is showing you can use smaller models and reduce the cost of their
    alignment.
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这导致了开源LLM模型的不断发布。这反过来显示，你可以使用更小的模型并降低对其对齐的成本。
- en: LLM models are a threat to different businesses and companies fear that these
    models could menace their business. Thus, different companies are reducing access
    to their data or asking for payment from AI companies.
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 大型语言模型（LLM）对不同的企业构成威胁，公司担心这些模型可能威胁到他们的业务。因此，不同的公司正在减少对其数据的访问或要求人工智能公司支付费用。
- en: Reduction in cost, fear of competition, a new relevance for proprietary data,
    and the new availability of open-source models are leading companies to train
    their own chatbots on their own data using open-source models.
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 成本降低、对竞争的担忧、对专有数据的新相关性以及开源模型的新可用性正促使公司使用开源模型在自己的数据上训练自己的聊天机器人。
- en: '**What do you think about the future of LLMs? Let me know in the comments**'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '**你对LLM的未来有什么看法？在评论中告诉我**'
- en: 'If you have found this interesting:'
  id: totrans-136
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如果你觉得这个话题有趣：
- en: '*You can look for my other articles, you can also* [***subscribe***](https://salvatore-raieli.medium.com/subscribe)
    *to get notified when I publish articles, you can* [***become a Medium member***](https://medium.com/@salvatore-raieli/membership)
    *to access all its stories (affiliate links of the platform for which I get small
    revenues without cost to you) and you can also connect or reach me on*[***LinkedIn***](https://www.linkedin.com/in/salvatore-raieli/)***.***'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '*你可以查看我的其他文章，也可以* [***订阅***](https://salvatore-raieli.medium.com/subscribe)
    *以在我发布文章时获得通知，你也可以* [***成为 Medium 会员***](https://medium.com/@salvatore-raieli/membership)
    *以访问所有故事（这是平台的附属链接，我可以从中获得少量收入，但不会对你产生费用），你还可以通过*[***LinkedIn***](https://www.linkedin.com/in/salvatore-raieli/)***与我联系或找到我。***'
- en: '*Here is the link to my GitHub repository, where I am planning to collect code
    and many resources related to machine learning, artificial intelligence, and more.*'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '*这是我 GitHub 仓库的链接，我计划在这里收集与机器学习、人工智能及其他相关的代码和许多资源。*'
- en: '[](https://github.com/SalvatoreRa/tutorial?source=post_page-----90e203b2f6b0--------------------------------)
    [## GitHub - SalvatoreRa/tutorial: Tutorials on machine learning, artificial intelligence,
    data science…'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '[## GitHub - SalvatoreRa/tutorial: 机器学习、人工智能、数据科学的教程…](https://github.com/SalvatoreRa/tutorial?source=post_page-----90e203b2f6b0--------------------------------)'
- en: Tutorials on machine learning, artificial intelligence, data science with math
    explanation and reusable code (in python…
  id: totrans-140
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 机器学习、人工智能、数据科学的教程，包含数学解释和可重用代码（用 Python…
- en: github.com](https://github.com/SalvatoreRa/tutorial?source=post_page-----90e203b2f6b0--------------------------------)
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '[GitHub - SalvatoreRa/tutorial: 机器学习、人工智能、数据科学的教程…](https://github.com/SalvatoreRa/tutorial?source=post_page-----90e203b2f6b0--------------------------------)'
- en: '*or you may be interested in one of my recent articles:*'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '*或者你可能对我最近的一篇文章感兴趣：*'
- en: '[](https://levelup.gitconnected.com/welcome-back-80s-transformers-could-be-blown-away-by-convolution-21ff15f6d1cc?source=post_page-----90e203b2f6b0--------------------------------)
    [## Welcome Back 80s: Transformers Could Be Blown Away by Convolution'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '[## 欢迎回到80年代：变压器可能会被卷积所超越](https://levelup.gitconnected.com/welcome-back-80s-transformers-could-be-blown-away-by-convolution-21ff15f6d1cc?source=post_page-----90e203b2f6b0--------------------------------)'
- en: The Hyena model shows how convolution could be faster than self-attention
  id: totrans-144
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Hyena 模型展示了卷积如何可能比自注意力更快
- en: 'levelup.gitconnected.com](https://levelup.gitconnected.com/welcome-back-80s-transformers-could-be-blown-away-by-convolution-21ff15f6d1cc?source=post_page-----90e203b2f6b0--------------------------------)
    [](https://levelup.gitconnected.com/meta-dino-how-self-supervised-learning-is-changing-computer-vision-1666a5e43dbb?source=post_page-----90e203b2f6b0--------------------------------)
    [## META DINO: how self-supervised learning is changing computer vision'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '[## META DINO: 自监督学习如何改变计算机视觉](https://levelup.gitconnected.com/meta-dino-how-self-supervised-learning-is-changing-computer-vision-1666a5e43dbb?source=post_page-----90e203b2f6b0--------------------------------)'
- en: 'Curated data, visual features, and knowledge distillation: the foundations
    of next computer vision models'
  id: totrans-146
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 筛选数据、视觉特征和知识蒸馏：下一代计算机视觉模型的基础
- en: 'levelup.gitconnected.com](https://levelup.gitconnected.com/meta-dino-how-self-supervised-learning-is-changing-computer-vision-1666a5e43dbb?source=post_page-----90e203b2f6b0--------------------------------)
    [](https://levelup.gitconnected.com/looking-into-your-eyes-how-google-ai-model-can-predict-your-age-from-the-eye-857979339da9?source=post_page-----90e203b2f6b0--------------------------------)
    [## Looking into Your Eyes: How Google AI Model Can Predict Your Age from the
    Eye'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '[## 透过你的眼睛：谷歌 AI 模型如何通过眼睛预测你的年龄](https://levelup.gitconnected.com/meta-dino-how-self-supervised-learning-is-changing-computer-vision-1666a5e43dbb?source=post_page-----90e203b2f6b0--------------------------------)'
- en: The new model can unlock secrets of aging by analyzing eye photos
  id: totrans-148
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 新模型通过分析眼睛照片可以揭示衰老的秘密
- en: 'levelup.gitconnected.com](https://levelup.gitconnected.com/looking-into-your-eyes-how-google-ai-model-can-predict-your-age-from-the-eye-857979339da9?source=post_page-----90e203b2f6b0--------------------------------)
    [](https://levelup.gitconnected.com/the-mechanical-symphony-will-ai-displace-the-human-workforce-9baeb786efa4?source=post_page-----90e203b2f6b0--------------------------------)
    [## The Mechanical Symphony: Will AI Displace the Human Workforce?'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '[levelup.gitconnected.com](https://levelup.gitconnected.com/looking-into-your-eyes-how-google-ai-model-can-predict-your-age-from-the-eye-857979339da9?source=post_page-----90e203b2f6b0--------------------------------)
    [](https://levelup.gitconnected.com/the-mechanical-symphony-will-ai-displace-the-human-workforce-9baeb786efa4?source=post_page-----90e203b2f6b0--------------------------------)
    [## 机械交响乐：人工智能会取代人类劳动力吗？'
- en: 'GPT-4 shows impressive skills: what will be the impact on the labor market?'
  id: totrans-150
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: GPT-4展示了令人印象深刻的技能：这将对劳动市场产生什么影响？
- en: levelup.gitconnected.com](https://levelup.gitconnected.com/the-mechanical-symphony-will-ai-displace-the-human-workforce-9baeb786efa4?source=post_page-----90e203b2f6b0--------------------------------)
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '[levelup.gitconnected.com](https://levelup.gitconnected.com/the-mechanical-symphony-will-ai-displace-the-human-workforce-9baeb786efa4?source=post_page-----90e203b2f6b0--------------------------------)'
