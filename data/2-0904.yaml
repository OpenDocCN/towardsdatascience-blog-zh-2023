- en: First Steps in the World Of Reinforcement Learning using Python
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Python 探索强化学习的第一步
- en: 原文：[https://towardsdatascience.com/first-steps-in-the-world-of-reinforcement-learning-using-python-b843b76538e3](https://towardsdatascience.com/first-steps-in-the-world-of-reinforcement-learning-using-python-b843b76538e3)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/first-steps-in-the-world-of-reinforcement-learning-using-python-b843b76538e3](https://towardsdatascience.com/first-steps-in-the-world-of-reinforcement-learning-using-python-b843b76538e3)
- en: Original Python implementation of how to find the best places to be in one of
    the fundamental worlds of reinforcement learning — the grid world
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 原始的 Python 实现，展示了如何在强化学习的基本世界之一——网格世界中找到最佳位置
- en: '[](https://eligijus-bujokas.medium.com/?source=post_page-----b843b76538e3--------------------------------)[![Eligijus
    Bujokas](../Images/061fd30136caea2ba927140e8b3fae3c.png)](https://eligijus-bujokas.medium.com/?source=post_page-----b843b76538e3--------------------------------)[](https://towardsdatascience.com/?source=post_page-----b843b76538e3--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----b843b76538e3--------------------------------)
    [Eligijus Bujokas](https://eligijus-bujokas.medium.com/?source=post_page-----b843b76538e3--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://eligijus-bujokas.medium.com/?source=post_page-----b843b76538e3--------------------------------)[![Eligijus
    Bujokas](../Images/061fd30136caea2ba927140e8b3fae3c.png)](https://eligijus-bujokas.medium.com/?source=post_page-----b843b76538e3--------------------------------)[](https://towardsdatascience.com/?source=post_page-----b843b76538e3--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----b843b76538e3--------------------------------)
    [Eligijus Bujokas](https://eligijus-bujokas.medium.com/?source=post_page-----b843b76538e3--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----b843b76538e3--------------------------------)
    ·15 min read·Jan 13, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----b843b76538e3--------------------------------)
    ·15 分钟阅读·2023年1月13日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/b476cbb9f0b2f5a0f39114a7b0ebca24.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b476cbb9f0b2f5a0f39114a7b0ebca24.png)'
- en: Gridworld matrices; Photo by author
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 网格世界矩阵；作者提供的照片
- en: The purpose of this article is to present fundamental concepts and definitions
    in Reinforcement Learning (from here on — **RL**) using Python code and comments.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 本文的目的是使用 Python 代码和注释介绍强化学习（以下简称**RL**）中的基本概念和定义。
- en: 'The article was heavily inspired by the great RL course: [https://www.coursera.org/learn/fundamentals-of-reinforcement-learning](https://www.coursera.org/learn/fundamentals-of-reinforcement-learning)'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 这篇文章深受以下伟大强化学习课程的启发：[https://www.coursera.org/learn/fundamentals-of-reinforcement-learning](https://www.coursera.org/learn/fundamentals-of-reinforcement-learning)
- en: 'The theory is laid out in the book¹: [http://www.incompleteideas.net/book/RLbook2020.pdf](http://www.incompleteideas.net/book/RLbook2020.pdf)'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 理论见书¹：[http://www.incompleteideas.net/book/RLbook2020.pdf](http://www.incompleteideas.net/book/RLbook2020.pdf)
- en: 'The code for all my RL experiments can be seen in my Gitlab repo: [https://github.com/Eligijus112/rl-snake-game](https://github.com/Eligijus112/rl-snake-game)'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 我所有的强化学习实验代码可以在我的 Gitlab 仓库中查看：[https://github.com/Eligijus112/rl-snake-game](https://github.com/Eligijus112/rl-snake-game)
- en: The grid world problem is a classic problem in RL where we want to create an
    optimal strategy for an agent to traverse a grid.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 网格世界问题是强化学习中的一个经典问题，我们希望为代理创建一个优化的策略以穿越网格。
- en: A grid is a square matrix of cells, and the agent can move in any of the four
    directions (up, down, left, right) in each cell. The agent receives a reward of
    -1 for each step it takes, and a reward of +10 if it reaches the goal cell. The
    numbers for the rewards are arbitrary and can be defined by the user.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 网格是一个方形的单元格矩阵，代理可以在每个单元格中向任意四个方向（上、下、左、右）移动。代理每移动一步会获得-1的奖励，若达到目标单元格则获得+10的奖励。奖励的数值是任意的，可以由用户定义。
- en: Formally, an agent in RL frameworks is defined as **the component that makes
    the decision of what action to take.** The agent takes action in concrete time
    steps.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在强化学习框架中，代理被正式定义为**做出采取何种行动的决策的组件**。代理在具体的时间步骤中采取行动。
- en: 'In the grid world setting, the whole action set is defined by the following
    set:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在网格世界设置中，整个动作集合由以下集合定义：
- en: '![](../Images/b336858f0f5eab063a10922cc7926120.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b336858f0f5eab063a10922cc7926120.png)'
- en: Action set
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 动作集合
- en: Or
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 或
- en: '![](../Images/4f396e4084c317ccd1fa466164aecb21.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4f396e4084c317ccd1fa466164aecb21.png)'
- en: Action set
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 动作集合
- en: 'No matter where our agent is, it can only move either left, right up or down.
    Now let us define and visualize our grid world:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 无论我们的代理处于何处，它只能向左、向右、向上或向下移动。现在让我们定义并可视化我们的网格世界：
- en: '[PRE0]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '[PRE1]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '![](../Images/ddb3ae05c5ca79f13def722c6674a83b.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ddb3ae05c5ca79f13def722c6674a83b.png)'
- en: Gridworld; Photo by author
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 网格世界；作者照片
- en: In the above example, we have defined our first needed matrix — the **R** matrix
    or the **reward matrix**. The goals are at the centre and at the corners of the
    grid world. When the agent goes into one of the cells, it receives a reward of
    the value of that cell.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述示例中，我们定义了第一个所需的矩阵——**R**矩阵或**奖励矩阵**。目标位于网格世界的中心和角落。当代理进入其中一个单元格时，它会获得该单元格的奖励值。
- en: 'Let us define another key matrix - the state matrix **S**:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们定义另一个关键矩阵——状态矩阵**S**：
- en: '[PRE2]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '![](../Images/51f7e9d43aef3204ba3682c32376d982.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/51f7e9d43aef3204ba3682c32376d982.png)'
- en: The state space; Photo by author
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 状态空间；作者照片
- en: In the grid world that we defined, there are in total **49 states** that an
    agent can be in. Each state can be identified by the integer from the matrix.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们定义的网格世界中，总共有**49个状态**，代理可以处于其中的任何一个状态。每个状态可以通过矩阵中的整数来识别。
- en: 'Let us assume that our agent is in state 17 and moves down. That action value**,
    denoted as q,** is:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们的代理在状态17并向下移动。该动作值**，表示为q，**是：
- en: '![](../Images/4e9be0dceaf7766eb7616a668122640c.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4e9be0dceaf7766eb7616a668122640c.png)'
- en: The action value for action ‘down’ in state 17
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 状态17中动作‘down’的动作值
- en: 'The action value is 10 because the reward at grid 24 is equal to 10\. Thus,
    when we are using action values, we need to keep note of the grid **S** index
    and the reward matrix **G**. One can easily guess, that moving from the same state
    to the right will have a reward of -1:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 动作值为10，因为网格24的奖励等于10。因此，当我们使用动作值时，需要注意网格**S**索引和奖励矩阵**G**。可以很容易猜到，从同一状态向右移动的奖励是-1：
- en: '![](../Images/0938faa5a0b3ab7968d7ffc4391a783f.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0938faa5a0b3ab7968d7ffc4391a783f.png)'
- en: The action value for action ‘right’ in state 17
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 状态17中动作‘right’的动作值
- en: 'In general, the function q, called action—value, maps a number to a state—action
    pair:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，函数q，称为动作值，将一个数字映射到状态—动作对：
- en: '![](../Images/5e7166d1a7357025897ef15cf4d3a83a.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5e7166d1a7357025897ef15cf4d3a83a.png)'
- en: Action value function
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 动作值函数
- en: The higher the number, the bigger the “reward” for the agent and thus, the agent
    always wants to make an action which **maximizes** **q from the current state**.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 数字越高，对代理的“奖励”就越大，因此，代理总是希望采取**最大化****当前状态下的q**的动作。
- en: 'So far, we have defined the matrices **R** **(rewards)**, and **S (states)**.
    Another key matrix is the state value matrix **V**. The dimensions of the **V**
    matrix are the same as the S and G matrices and each element in the **V** matrix
    evaluates the “goodness” of the given state. The “goodness” here refers to the
    equation:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经定义了矩阵**R**（奖励）和**S**（状态）。另一个关键矩阵是状态值矩阵**V**。**V**矩阵的维度与S和G矩阵相同，每个**V**矩阵中的元素评估给定状态的“好坏”。这里的“好坏”指的是方程：
- en: '![](../Images/4c220ff25b15ac721c1892e297056cbf.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4c220ff25b15ac721c1892e297056cbf.png)'
- en: Value of state s [1]
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 状态s的值[1]
- en: 'We read the above equation as follows:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将上述方程读作：
- en: The value of state s given policy pi is equal to the expected returns at time
    step t given that the state at time t was s.
  id: totrans-46
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 给定策略pi的状态s的值等于在时间步t给定状态为s时的期望回报。
- en: We calculate the above value for all the states and store it in the matrix **V**.
    We have introduced new variables here so let us define them.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 我们计算所有状态的上述值并将其存储在矩阵**V**中。我们在这里引入了新的变量，所以让我们定义它们。
- en: '![](../Images/f046ada703904fdc3e0bf22f7cadd61a.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f046ada703904fdc3e0bf22f7cadd61a.png)'
- en: The total return at time step t [1]
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 时间步t的总回报[1]
- en: '![](../Images/a87ffee384e210ada1ee944a256fde51.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a87ffee384e210ada1ee944a256fde51.png)'
- en: Discount factor [1]
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 折扣因子[1]
- en: The index **K** is called the terminal state where the agent reaches any of
    the goals in our grid world. In other words, **the value of G in each state shows
    the discounted reward sum when starting the agent path towards the goal from the
    given state. The bigger the value, the more desirable the state is.**
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '**K**索引称为终端状态，其中代理达到网格世界中的任意目标。换句话说，**每个状态中G的值表示从给定状态开始朝向目标的代理路径的折扣奖励总和。值越大，状态越受欢迎。**'
- en: 'The pi term in the value of state equation is called **policy** and is a probability
    of taking an action in state s:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 状态方程中的pi项称为**策略**，是采取状态s中某一动作的概率：
- en: '![](../Images/16502cff059ff88102a32e427a8ce0a8.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/16502cff059ff88102a32e427a8ce0a8.png)'
- en: Policy
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 策略
- en: 'Let us initiate the initial value matrix **V**:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们初始化初始值矩阵**V**：
- en: '[PRE3]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '![](../Images/faa47e2889ea96314d3c7fe5782f0445.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/faa47e2889ea96314d3c7fe5782f0445.png)'
- en: Initial state value matrix; Photo by author
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 初始状态值矩阵；作者照片
- en: As we have not yet explored our created world of grids, all the returns of the
    states are 0.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们尚未探索我们创建的网格世界，因此所有状态的回报都是0。
- en: The last matrix we will need is the policy matrix **P**.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要的最后一个矩阵是策略矩阵**P**。
- en: '[PRE4]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '[PRE5]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '![](../Images/ca4fcca11a20fa8491331e3f6beaa831.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ca4fcca11a20fa8491331e3f6beaa831.png)'
- en: Initial policy matrix; Photo by author
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 初始策略矩阵；作者照片
- en: Each arrow in a grid represents the available actions that an agent can take.
    The probabilities in the initial matrix are uniform and there are no available
    moves in the goal states.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 网格中的每个箭头代表代理可以采取的可用动作。初始矩阵中的概率是均匀的，目标状态中没有可用的移动。
- en: Having the **R, P, S** and **V** matrices, we can finally start calculating
    the answer for our RL problem. But we have yet to define, what is the RL objective.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 拥有**R, P, S**和**V**矩阵后，我们可以最终开始计算我们RL问题的答案。但我们还需定义RL目标。
- en: '**The objective of an RL algorithm is for the agent to find the optimal policy
    P that maximizes the returns in each state.**'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '**RL算法的目标是使代理找到最优策略P，以最大化每个状态的回报。**'
- en: Another formulation is that **the objective is to calculate the optimal state
    values in the matrix V.**
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种表述是**目标是计算矩阵V中的最优状态值。**
- en: 'Let us say we have a 5 by 5 grid with the goal at the centre:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有一个5乘5的网格，目标在中央：
- en: '![](../Images/3276db0f4e58cee158234d9b46fcec5c.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3276db0f4e58cee158234d9b46fcec5c.png)'
- en: Example grid world; Photo by author
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 示例网格世界；作者照片
- en: 'To build intuition, I have calculated the optimal values and the policies.
    We will find out how to do it in the next section of this article but for now
    let us interpret the following **V** and **P** matrices:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 为了建立直观理解，我已经计算了最佳值和策略。我们将在本文章的下一部分找到如何实现，但现在让我们解释以下**V**和**P**矩阵：
- en: '![](../Images/522e430faa9e355649fec4444715db3f.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/522e430faa9e355649fec4444715db3f.png)'
- en: Solved Value and Policy matrices; Photo by author
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 已解决的值和策略矩阵；作者照片
- en: 'Recall that every value in the V matrix is the total accumulated discounted
    reward. Thus, our agent would want to always go to the state which has the highest
    value. To put it mathematically, at every state the agent is in, he will choose
    the next state by the following equation:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，V矩阵中的每个值是总的累计折扣奖励。因此，我们的代理将始终希望前往具有最高值的状态。用数学表达就是，在每个状态下，代理将根据以下方程选择下一个状态：
- en: '![](../Images/a1d5be7b12aebedcd66dfb4efcf11f0f.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a1d5be7b12aebedcd66dfb4efcf11f0f.png)'
- en: The optimal choice at every state [1]
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 每个状态的最佳选择 [1]
- en: At every state, we will choose an action whichleads us to the state **s prime**
    wherethe **r + gamma * (new state value)** is the highest.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在每个状态下，我们将选择一个动作，使我们进入状态**s prime**，在这里**r + gamma * (new state value)**是最高的。
- en: To put an even simpler intuition, we can just list out all the available actions
    from the current state, check which available state has the highest **V(s)** value
    and go there. Looking at the matrices above, we can see that, for example, state
    8 has two optimal choices — **down** and **left**. This is because these actions
    will lead the agent into equally good states. Thus having the **V** matrix, we
    will always infer the P matrix.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 更简单的直观理解是，我们可以列出当前状态下所有可用的动作，检查哪个可用状态具有最高的**V(s)**值，然后前往那里。从上面的矩阵中，我们可以看到，例如，状态8有两个最佳选择——**下**和**左**。这是因为这些动作将使代理进入同样好的状态。因此，拥有**V**矩阵后，我们将始终推断出P矩阵。
- en: 'Now, how to go from the V matrix with all zeroes to a matrix with values? We
    need to define the Bellman equation for each state:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，如何从全零的V矩阵转换为具有值的矩阵？我们需要为每个状态定义贝尔曼方程：
- en: '![](../Images/ad0993137593b7dfc5db6303504664b5.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ad0993137593b7dfc5db6303504664b5.png)'
- en: Bellman equation for state s with policy pi [1]
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 状态s的贝尔曼方程与策略pi [1]
- en: The above equation is intimidating and has a recursive property. For the grid
    world example, we can simplify the equation and write it without the conditional
    probabilities in the middle.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 上面的方程令人望而生畏，且具有递归特性。对于网格世界示例，我们可以简化方程，并将其写出，而无需中间的条件概率。
- en: '![](../Images/66808347cf33d040418b7e026674b640.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/66808347cf33d040418b7e026674b640.png)'
- en: Simplified equation [1]
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 简化方程 [1]
- en: We can do this because when we make an action in a state s, we are guaranteed
    to go to just one next state.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以这样做，因为当我们在状态s中执行一个动作时，我们可以保证只会进入一个下一个状态。
- en: In the solved matrix example, recall that
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在已解决的矩阵示例中，回顾一下
- en: '![](../Images/06df8d53e68b9b0497e8073ac5968a3f.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/06df8d53e68b9b0497e8073ac5968a3f.png)'
- en: Value of state 0
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 状态0的值
- en: 'This means that in the long run our agent if starting from position 0, will
    accumulate a -6.07 total reward. In order to estimate this and go from a recursive
    formula to a formula we can evaluate with a simple for loop, we will use the following
    algorithm:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着从位置 0 开始，我们的代理在长期内将累计 -6.07 的总奖励。为了估计这一点并将递归公式转换为可以通过简单循环评估的公式，我们将使用以下算法：
- en: '![](../Images/036a51aa40f92331d08f1e0c65fddf33.png)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/036a51aa40f92331d08f1e0c65fddf33.png)'
- en: Value iteration algorithm [1]
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 价值迭代算法 [1]
- en: 'We will simplify the middle part of the algorithm for the grid world problem:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将简化网格世界问题算法的中间部分：
- en: '![](../Images/f0ab254caf5769c503bc05223296f649.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f0ab254caf5769c503bc05223296f649.png)'
- en: Value iteration algorithm simplified; Photo by author
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 价值迭代算法简化；作者拍摄
- en: Now let us transfer everything to Python code.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们将一切转到 Python 代码中。
- en: '[PRE6]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '[PRE7]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: The above function finds the Belman equation value for the state **s**.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 上述函数找到状态 **s** 的贝尔曼方程值。
- en: '[PRE8]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The above code chunk implements the value iteration algorithm for finding the
    optimal (or close to optimal) **V** matrix.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码块实现了价值迭代算法，用于寻找最佳（或接近最佳的） **V** 矩阵。
- en: '[PRE9]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'We now have all the theory and code for us to start evaluating all the states
    in our grid world. Recall, that we have the initial grid world as:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在有了所有理论和代码，开始评估网格世界中的所有状态。回顾一下，我们的初始网格世界如下：
- en: '![](../Images/07a9e06cbad806a751627013cd488f21.png)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/07a9e06cbad806a751627013cd488f21.png)'
- en: State space; Photo by author
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 状态空间；作者拍摄
- en: '![](../Images/ddb3ae05c5ca79f13def722c6674a83b.png)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ddb3ae05c5ca79f13def722c6674a83b.png)'
- en: Gridworld; Photo by author
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 网格世界；作者拍摄
- en: '![](../Images/a8da08bce9e812777606c180c4a48507.png)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a8da08bce9e812777606c180c4a48507.png)'
- en: Initial value and policy matrices; Photo by author
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 初始值和策略矩阵；作者拍摄
- en: Now let us update one state — the first or **s = 1**.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们更新一个状态——第一个或 **s = 1**。
- en: '[PRE10]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The value matrix and the policy matrix now look like the following:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 价值矩阵和策略矩阵现在看起来如下：
- en: '![](../Images/555818070687f34a555a1516f51a5011.png)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/555818070687f34a555a1516f51a5011.png)'
- en: One value iteration for one state; Photo by author
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 一个状态的价值迭代；作者拍摄
- en: Being in states 2 or 8 the optimal policy would be to move to state 1 because
    0 < 2.66 and thus state 1 is more valuable than its neighbours.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 在状态 2 或 8 中，最佳策略是移动到状态 1，因为 0 < 2.66，因此状态 1 比其邻居更有价值。
- en: 'Let us now update state 3 and see what is happening:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们更新状态 3，看看会发生什么：
- en: '[PRE11]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '![](../Images/d7e22b1776ea0246a92ec6717ab79788.png)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d7e22b1776ea0246a92ec6717ab79788.png)'
- en: Updating the third state; Photo by author
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 更新第三状态；作者拍摄
- en: The value for being in state 3 is -1 and thus, the agent, as of right now in
    our grid world, would want to avoid this state when compared to its neighbours.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 在状态 3 中的值为 -1，因此，目前在我们的网格世界中，代理会倾向于避免这个状态，相比于其邻居。
- en: 'The value iteration algorithm works in the exact same fashion as above just
    for all the states (in our case — from states 0 to 48). To implement it use the
    following code:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 价值迭代算法以与上述相同的方式工作，只是针对所有状态（在我们的案例中——从状态 0 到 48）。要实现它，请使用以下代码：
- en: '[PRE12]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '![](../Images/fb7237cf07e6f7665cd3700e36645e2b.png)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fb7237cf07e6f7665cd3700e36645e2b.png)'
- en: Solved grid world; Photo by author
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 已解决的网格世界；作者拍摄
- en: The agent could start in any non-terminal state and move along the arrows in
    the policy matrix. If there are two or more arrows in the same state, we can move
    with the same probability into each of the states that the arrows are pointing
    to.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 代理可以从任何非终结状态开始，并沿着策略矩阵中的箭头移动。如果同一状态中有两个或更多箭头，我们可以以相同的概率移动到箭头指向的每一个状态。
- en: 'To summarize, in a simple RL problem, we have 4 main matrices:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 总结一下，在一个简单的强化学习问题中，我们有 4 个主要矩阵：
- en: Reward matrix **R**
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 奖励矩阵 **R**
- en: State value function **V**
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 状态值函数 **V**
- en: Policy matrix **P**
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 策略矩阵 **P**
- en: State matrix **S**.
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 状态矩阵 **S**。
- en: Additionally, we need a finite set of actions **A**.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们需要一个有限的动作集合 **A**。
- en: 'To evaluate each state theoretically, we use the Bellman equation:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理论上评估每个状态，我们使用贝尔曼方程：
- en: '![](../Images/ad0993137593b7dfc5db6303504664b5.png)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ad0993137593b7dfc5db6303504664b5.png)'
- en: Bellman equation for state s with policy pi
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 状态 **s** 的贝尔曼方程与策略 pi
- en: 'The evaluate the state value practically, we use the value iteration algorithm:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实际评估状态值，我们使用价值迭代算法：
- en: '![](../Images/f0ab254caf5769c503bc05223296f649.png)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f0ab254caf5769c503bc05223296f649.png)'
- en: Value iteration algorithm simplified; Photo by author
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 价值迭代算法简化；作者拍摄
- en: The goal of an RL assignment is to find the optimal policy that our agent can
    follow.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习任务的目标是找到我们的代理可以遵循的最佳策略。
- en: 'Feel free to use the code and make adjustments to it here: [https://github.com/Eligijus112/rl-snake-game](https://github.com/Eligijus112/rl-snake-game).'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 随意使用代码并在此处进行调整：[https://github.com/Eligijus112/rl-snake-game](https://github.com/Eligijus112/rl-snake-game)。
- en: Happy learning!
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 祝学习愉快！
- en: '[1]'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '[1]'
- en: 'Author: **Richard S. Sutton, Andrew G. Barto**'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 作者：**理查德·S·萨顿，安德鲁·G·巴托**
- en: 'Year: 2018'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 年份：2018
- en: 'Title: **Reinforcement Learning: An Introduction**'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 标题：**强化学习：导论**
- en: URL:[**http://archive.ics.uci.edu/ml**](https://web.stanford.edu/class/psych209/Readings/SuttonBartoIPRLBook2ndEd.pdf)
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 链接：[**http://archive.ics.uci.edu/ml**](https://web.stanford.edu/class/psych209/Readings/SuttonBartoIPRLBook2ndEd.pdf)
