- en: 'Sentence Transformers: Meanings in Disguise'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 句子变换器：伪装中的意义
- en: 原文：[https://towardsdatascience.com/sentence-transformers-meanings-in-disguise-323cf6ac1e52](https://towardsdatascience.com/sentence-transformers-meanings-in-disguise-323cf6ac1e52)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/sentence-transformers-meanings-in-disguise-323cf6ac1e52](https://towardsdatascience.com/sentence-transformers-meanings-in-disguise-323cf6ac1e52)
- en: '[NLP For Semantic Search](https://jamescalam.medium.com/list/nlp-for-semantic-search-d3a4b96a52fe)'
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '[NLP语义搜索](https://jamescalam.medium.com/list/nlp-for-semantic-search-d3a4b96a52fe)'
- en: How modern language models capture meaning
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 现代语言模型如何捕捉意义
- en: '[](https://jamescalam.medium.com/?source=post_page-----323cf6ac1e52--------------------------------)[![James
    Briggs](../Images/cb34b7011748e4d8607b7ff4a8510a93.png)](https://jamescalam.medium.com/?source=post_page-----323cf6ac1e52--------------------------------)[](https://towardsdatascience.com/?source=post_page-----323cf6ac1e52--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----323cf6ac1e52--------------------------------)
    [James Briggs](https://jamescalam.medium.com/?source=post_page-----323cf6ac1e52--------------------------------)'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://jamescalam.medium.com/?source=post_page-----323cf6ac1e52--------------------------------)[![James
    Briggs](../Images/cb34b7011748e4d8607b7ff4a8510a93.png)](https://jamescalam.medium.com/?source=post_page-----323cf6ac1e52--------------------------------)[](https://towardsdatascience.com/?source=post_page-----323cf6ac1e52--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----323cf6ac1e52--------------------------------)
    [James Briggs](https://jamescalam.medium.com/?source=post_page-----323cf6ac1e52--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----323cf6ac1e52--------------------------------)
    ·12 min read·Jan 3, 2023
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page-----323cf6ac1e52--------------------------------)
    ·阅读时间12分钟·2023年1月3日
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/edabd3baac27a7fd707b7855ea93c5c2.png)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/edabd3baac27a7fd707b7855ea93c5c2.png)'
- en: Photo by [Brian Suh](https://unsplash.com/@_briansuh?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral). Originally
    posted in the [NLP for Semantic Search ebook](https://www.pinecone.io/learn/sentence-embeddings/)
    at Pinecone (where the author is employed).
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：[Brian Suh](https://unsplash.com/@_briansuh?utm_source=medium&utm_medium=referral)于[Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)。最初发布在[Pinecone的NLP语义搜索电子书](https://www.pinecone.io/learn/sentence-embeddings/)中（作者在此工作）。
- en: Transformers have wholly rebuilt the landscape of natural language processing
    (NLP). Before transformers, we had *okay* translation and language classification
    thanks to recurrent neural nets (RNNs) — their language comprehension was limited
    and led to many minor mistakes, and coherence over larger chunks of text was practically
    impossible.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 变换器完全重塑了自然语言处理（NLP）的格局。在变换器出现之前，得益于递归神经网络（RNNs），我们有了*还不错*的翻译和语言分类——它们的语言理解能力有限，导致许多小错误，大块文本的一致性几乎是不可能的。
- en: Since the introduction of the first transformer model in the 2017 paper *‘Attention
    is all you need’* [1], NLP has moved from RNNs to models like BERT and GPT. These
    new models can answer questions, write articles *(maybe GPT-3 wrote this)*, enable
    incredibly intuitive semantic search — and much more.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 自2017年论文*《Attention is all you need》* [1]首次引入变换器模型以来，NLP从RNNs发展到BERT和GPT等模型。这些新模型可以回答问题、撰写文章（*也许是GPT-3写的*）、实现非常直观的语义搜索——还有更多。
- en: The funny thing is, for many tasks, the latter parts of these models are the
    same as those in RNNs — often a couple of feedforward NNs that output model predictions.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是，对于许多任务，这些模型的后期部分与RNN中的部分相同——通常是几个前馈神经网络，用于输出模型预测结果。
- en: It’s the *input* to these layers that changed. The [dense embeddings](https://www.pinecone.io/learn/dense-vector-embeddings-nlp/)
    created by transformer models are so much richer in information that we get massive
    performance benefits despite using the same final outward layers.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 改变的是这些层的*输入*。变换器模型创建的[密集嵌入](https://www.pinecone.io/learn/dense-vector-embeddings-nlp/)信息量要丰富得多，尽管使用相同的最终外层，我们仍然获得了巨大的性能提升。
- en: 'These increasingly rich sentence embeddings can be used to quickly compare
    sentence similarity for various use cases. Such as:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 这些越来越丰富的句子嵌入可以用来快速比较各种用例中的句子相似性。例如：
- en: '**Semantic textual similarity (STS)** — comparison of sentence pairs. We may
    want to identify patterns in datasets, but this is most often used for benchmarking.'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**语义文本相似度 (STS)** — 比较句子对。我们可能想要在数据集中识别模式，但这通常用于基准测试。'
- en: '**Semantic search** — information retrieval (IR) using semantic meaning. Given
    a set of sentences, we can search using a *‘query’* sentence and identify the
    most similar records. Enables search to be performed on concepts (rather than
    specific words).'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**语义搜索**——使用语义意义的信息检索（IR）。给定一组句子，我们可以使用*‘查询’*句子进行搜索，并识别最相似的记录。使搜索能够基于概念（而非特定词汇）进行。'
- en: '**Clustering** — we can cluster our sentences, useful for topic modeling.'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**聚类**——我们可以对句子进行聚类，这对主题建模很有用。'
- en: In this article, we will explore how these embeddings have been adapted and
    applied to a range of semantic similarity applications by using a new breed of
    transformers called *‘sentence transformers’*.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们将探讨这些嵌入是如何被调整和应用于各种语义相似性应用的，通过使用一种叫做*‘句子变换器’*的新型变换器。
- en: Some “Context”
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 一些“上下文”
- en: Before we dive into sentence transformers, it might help to piece together why
    transformer embeddings are so much richer — and where the difference lies between
    a vanilla *transformer* and a *sentence transformer*.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们深入探讨句子变换器之前，理解变换器嵌入为何如此丰富可能会有所帮助——以及普通*变换器*与*句子变换器*之间的差异所在。
- en: Transformers are indirect descendants of the previous RNN models. These old
    recurrent models were typically built from many recurrent *units* like [LSTMs
    or GRUs](/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21).
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 变换器是之前RNN模型的间接后代。这些旧的递归模型通常由许多递归*单元*构成，如[LSTMs或GRUs](/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21)。
- en: In *machine translation*, we would find [encoder-decoder networks](https://machinelearningmastery.com/encoder-decoder-recurrent-neural-network-models-neural-machine-translation/).
    The first model for *encoding* the original language to a *context vector*, and
    a second model for *decoding* this into the target language.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在*机器翻译*中，我们会找到[编码器-解码器网络](https://machinelearningmastery.com/encoder-decoder-recurrent-neural-network-models-neural-machine-translation/)。第一个模型用于*编码*原始语言到*上下文向量*，第二个模型用于将其*解码*成目标语言。
- en: '![](../Images/ecd770c80c2df0afeb2b3944472fea4c.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ecd770c80c2df0afeb2b3944472fea4c.png)'
- en: Encoder-decoder architecture with the single context vector shared between the
    two models, this acts as an information bottleneck, as *all* information must
    be passed through this point.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 具有单一上下文向量共享的编码器-解码器架构在两个模型之间，这作为一个信息瓶颈，因为*所有*信息必须通过这一点传递。
- en: The problem here is that we create an *information bottleneck* between the two
    models. We’re creating a massive amount of information over multiple time steps
    and trying to squeeze it all through a single connection. This limits the encoder-decoder
    performance because much of the information produced by the encoder is lost before
    reaching the decoder.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的问题是我们在两个模型之间创建了一个*信息瓶颈*。我们在多个时间步骤中创建了大量信息，并试图通过一个连接将所有信息挤压过来。这限制了编码器-解码器的性能，因为编码器产生的许多信息在到达解码器之前就已经丢失。
- en: The *attention mechanism* provided a solution to the bottleneck issue. It offered
    another route for information to pass through. Still, it didn’t overwhelm the
    process because it focused *attention* only on the most relevant information.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '*注意机制*为瓶颈问题提供了一个解决方案。它提供了信息传递的另一条途径。然而，它并没有让过程变得复杂，因为它仅仅*关注*最相关的信息。'
- en: The information bottleneck is removed by passing a context vector from each
    timestep into the attention mechanism (producing annotation vectors), and there
    is better information retention across longer sequences.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将每个时间步的上下文向量传递到注意机制中（生成注释向量），去除了信息瓶颈，并在较长序列中有更好的信息保留。
- en: '![](../Images/7c95a9edcf3ca64a8faa0df2e7d08200.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7c95a9edcf3ca64a8faa0df2e7d08200.png)'
- en: Encoder-decoder with the attention mechanism. The attention mechanism considered
    all encoder output activations and each timestep’s activation in the decoder,
    which modifies the decoder outputs.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 带有注意机制的编码器-解码器。注意机制考虑了所有编码器输出激活和解码器中每个时间步的激活，这些都会修改解码器输出。
- en: During decoding, the model decodes one word/timestep at a time. An alignment
    (e.g., similarity) between the word and all encoder annotations is calculated
    for each step.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在解码过程中，模型一次解码一个单词/时间步。每一步都会计算单词与所有编码器注释之间的对齐度（例如，相似度）。
- en: Higher alignment resulted in greater weighting to the encoder annotation on
    the output of the decoder step. Meaning the mechanism calculated which encoder
    words to pay *attention* to.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 更高的对齐度导致了对解码器步骤输出的编码器注释的更大加权。这意味着机制计算了需要*关注*哪些编码器单词。
- en: '![](../Images/48a331d23b9489d304f128804266edf9.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/48a331d23b9489d304f128804266edf9.png)'
- en: Attention between an English-French encoder and decoder, source [2].
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 英法编码器和解码器之间的注意力，来源 [2]。
- en: The best-performing RNN encoder-decoders all used this attention mechanism.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 所有表现最好的 RNN 编码器-解码器都使用了这种注意力机制。
- en: Attention is All You Need
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Attention is All You Need
- en: In 2017, a paper titled *Attention Is All You Need* was published. This marked
    a turning point in NLP. The authors demonstrated that we could remove the RNN
    networks and get superior performance using *just* the attention mechanism — with
    a few changes.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在 2017 年，一篇题为*Attention Is All You Need*的论文发表了。这标志着 NLP 的一个转折点。作者们展示了我们可以去除
    RNN 网络，并仅使用*注意力*机制——经过一些修改，就能获得更好的性能。
- en: This new attention-based model was named a *‘transformer’*. Since then, the
    NLP ecosystem has entirely shifted from RNNs to transformers thanks to their vastly
    superior performance and incredible capability for generalization.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 这个基于注意力的新模型被称为*‘transformer’*。从那时起，由于其极其优越的性能和出色的泛化能力，NLP 生态系统完全从 RNN 转向了 transformers。
- en: 'The first transformer removed the need for RNNs through the use of *three*
    key components:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个 transformer 通过使用*三个*关键组件去除了对 RNN 的需求：
- en: Positional Encoding
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 位置编码
- en: Self-attention
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自注意力
- en: Multi-head attention
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多头注意力
- en: '**Positional encoding** replaced the key advantage of RNNs in NLP — the ability
    to consider the order of a sequence (they were *recurrent*). It worked by adding
    a set of varying sine wave activations to each input embedding based on position.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '**位置编码**取代了 RNN 在 NLP 中的关键优势——考虑序列顺序的能力（它们是*递归的*）。它通过根据位置向每个输入嵌入添加一组变化的正弦波激活来工作。'
- en: '**Self-attention** is where the attention mechanism is applied between a word
    and all of the other words in its own context (sentence/paragraph). This is different
    from vanilla attention which specifically focused on attention between encoders
    and decoders.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '**自注意力**是指在一个词与其自身上下文（句子/段落）中的所有其他词之间应用注意力机制。这不同于普通的注意力，它专注于编码器和解码器之间的注意力。'
- en: '**Multi-head attention** can be seen as several *parallel* attention mechanisms
    working together. Using several attention *heads* allowed the representation of
    several sets of relationships (rather than a single set).'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '**多头注意力**可以看作是几个*并行*的注意力机制共同工作。使用多个注意力*头*允许表示多个关系集（而不是单一的关系集）。'
- en: Pretrained Models
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 预训练模型
- en: The new transformer models generalized much better than previous RNNs, which
    were often built specifically for each use case.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 新的 transformer 模型的泛化能力远远超过了以前的 RNN，这些 RNN 通常是为每个用例特别构建的。
- en: With transformer models, it is possible to use the same *‘core’* of a model
    and simply swap the last few layers for different use cases (without retraining
    the *core*).
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 transformer 模型，可以使用相同的*‘核心’*模型，并仅替换最后几层以适应不同的用例（而无需重新训练*核心*）。
- en: This new property resulted in the rise of *pretrained* models for NLP. Pretrained
    transformer models are trained on vast amounts of training data — often at high
    costs by the likes of Google or OpenAI, then released for the public to use for
    free.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 这种新特性导致了*NLP*中*预训练*模型的兴起。预训练的 transformer 模型是在大量训练数据上训练的——通常由谷歌或 OpenAI 等公司高成本训练，然后免费提供给公众使用。
- en: One of the most widely used of these pretrained models is BERT, or **B**idirectional
    **E**ncoder **R**epresentations from **T**ransformers by Google AI.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 这些预训练模型中最广泛使用的之一是 BERT，或谷歌 AI 的**B**idirectional **E**ncoder **R**epresentations
    from **T**ransformers。
- en: BERT spawned a whole host of further models and derivations such as distilBERT,
    RoBERTa, and ALBERT, covering tasks such as classification, Q&A, POS-tagging,
    and more.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: BERT 产生了一系列进一步的模型和变种，如 distilBERT、RoBERTa 和 ALBERT，涵盖分类、问答、词性标注等任务。
- en: BERT for Sentence Similarity
  id: totrans-50
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: BERT 用于句子相似度
- en: 'So far, so good, but these transformer models had one issue when building sentence
    vectors: Transformers work using word or *token*-level embeddings, *not* sentence-level
    embeddings.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，一切都很好，但这些 transformer 模型在构建句子向量时存在一个问题：Transformers 使用的是词或*token*级别的嵌入，而*不是*句子级别的嵌入。
- en: Before sentence transformers, the approach to calculating *accurate* sentence
    similarity with BERT was to use a cross-encoder structure. This meant that we
    would pass two sentences to BERT, add a classification head to the top of BERT
    — and use this to output a similarity score.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在句子 transformers 出现之前，使用 BERT 计算*准确的*句子相似度的方法是使用交叉编码器结构。这意味着我们将两个句子传递给 BERT，在
    BERT 顶部添加一个分类头——并用它来输出相似度评分。
- en: '![](../Images/2f05f889b220b0df300e2979b6e1f4b1.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2f05f889b220b0df300e2979b6e1f4b1.png)'
- en: The BERT cross-encoder architecture consists of a BERT model which consumes
    sentences A and B. Both are processed in the same sequence, separated by a `[SEP]`
    token. All of this is followed by a feedforward NN classifier that outputs a similarity
    score.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: BERT 交叉编码器架构包括一个 BERT 模型，该模型处理句子 A 和 B。这两个句子在同一序列中处理，由 `[SEP]` 标记分隔。随后是一个前馈神经网络分类器，输出相似度评分。
- en: The cross-encoder network does produce very accurate similarity scores (better
    than SBERT), but it’s *not scalable*. If we wanted to perform a similarity search
    through a small 100K sentence dataset, we would need to complete the cross-encoder
    inference computation 100K times.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 交叉编码器网络确实生成非常准确的相似度评分（比 SBERT 更好），但它的*可扩展性差*。如果我们想在一个 100K 句子的数据库中进行相似度搜索，我们需要完成
    100K 次交叉编码器推断计算。
- en: To cluster sentences, we would need to compare all sentences in our 100K dataset,
    resulting in just under 500M comparisons — this is simply not realistic.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 要对句子进行聚类，我们需要比较我们 100K 数据集中的所有句子，结果将产生接近 5 亿次比较 —— 这显然是不现实的。
- en: Ideally, we need to pre-compute sentence vectors that can be stored and then
    used whenever required. If these vector representations are good, all we need
    to do is calculate the cosine similarity between each.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 理想情况下，我们需要预先计算句子向量，这些向量可以被存储，然后在需要时使用。如果这些向量表示良好，我们只需计算每对句子之间的余弦相似度。
- en: With the original BERT (and other transformers), we can build a sentence embedding
    by averaging the values across all token embeddings output by BERT (if we input
    512 tokens, we output 512 embeddings). Alternatively, we can use the output of
    the first `[CLS]` token (a BERT-specific token whose output embedding is used
    in classification tasks).
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 使用原始的 BERT（及其他变换器），我们可以通过对 BERT 输出的所有标记嵌入进行平均来构建句子嵌入（如果我们输入 512 个标记，则输出 512
    个嵌入）。另一种方法是使用第一个 `[CLS]` 标记的输出（一个特定于 BERT 的标记，其输出嵌入用于分类任务）。
- en: Using one of these two approaches gives us our sentence embeddings that can
    be stored and compared much faster, shifting search times from 65 hours to around
    5 seconds (see below). However, the accuracy is not good, and is worse than using
    averaged GloVe embeddings (which were developed in 2014).
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这两种方法中的一种可以得到我们的句子嵌入，这些嵌入可以被存储并且比较速度更快，将搜索时间从 65 小时缩短到大约 5 秒（见下文）。然而，准确性不佳，甚至比使用平均的
    GloVe 嵌入（该方法开发于 2014 年）更差。
- en: '**The solution** to this lack of an accurate model *with* reasonable latency
    was designed by Nils Reimers and Iryna Gurevych in 2019 with the introduction
    of sentence-BERT (SBERT) and the `sentence-transformers` library.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '**这个解决方案** 旨在解决缺乏准确模型的*合理延迟*问题，由 Nils Reimers 和 Iryna Gurevych 在 2019 年设计，推出了
    sentence-BERT（SBERT）和 `sentence-transformers` 库。'
- en: SBERT outperformed the previous state-of-the-art (SOTA) models for all common
    semantic textual similarity (STS) tasks — more on these later — except a single
    dataset (SICK-R).
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: SBERT 在所有常见的语义文本相似性（STS）任务中表现优于之前的最先进（SOTA）模型 —— 更多关于这些任务的内容见下文 —— 唯一的例外是一个数据集（SICK-R）。
- en: Thankfully for scalability, SBERT produces sentence embeddings — so we do *not*
    need to perform a whole inference computation for every sentence-pair comparison.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，为了实现可扩展性，SBERT 生成句子嵌入 —— 因此我们*不*需要对每个句子对比较执行整个推断计算。
- en: Reimers and Gurevych demonstrated the dramatic speed increase in 2019\. Finding
    the most similar sentence pair from 10K sentences took 65 hours with BERT. With
    SBERT, embeddings are created in ~5 seconds and compared with cosine similarity
    in ~0.01 seconds.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: Reimers 和 Gurevych 在 2019 年展示了显著的速度提升。从 10K 个句子中找到最相似的句子对，使用 BERT 需要 65 小时。使用
    SBERT，嵌入的创建时间约为 5 秒，与余弦相似度的比较时间约为 0.01 秒。
- en: Since the SBERT paper, many more sentence transformer models have been built
    using similar concepts that went into training the original SBERT. They’re all
    trained on many similar and dissimilar sentence pairs.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 自 SBERT 论文发布以来，已经构建了许多使用类似概念的句子变换器模型，这些概念用于训练原始的 SBERT。它们都在许多相似和不相似的句子对上进行了训练。
- en: Using a loss function such as softmax loss, multiple negatives ranking loss,
    or MSE margin loss, these models are optimized to produce similar embeddings for
    similar sentences and dissimilar embeddings otherwise.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 使用诸如 softmax 损失、多负样本排序损失或 MSE 边际损失等损失函数，这些模型被优化以生成相似句子的相似嵌入和不相似句子的不同嵌入。
- en: Now you have some context behind sentence transformers, where they come from,
    and why they’re needed. Let’s dive into how they work.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经了解了一些关于句子变换器的背景知识，包括它们的来源及其必要性。让我们深入探讨它们是如何工作的。
- en: '**[3] The SBERT paper covers many of this section''s statements, techniques,
    and numbers.*'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '**[3] SBERT论文涵盖了本节中的许多陈述、技术和数据。*'
- en: Sentence Transformers
  id: totrans-68
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 句子转换器
- en: We explained the cross-encoder architecture for sentence similarity with BERT.
    SBERT is similar but drops the final classification head, and processes one sentence
    at a time. SBERT then uses mean pooling on the final output layer to produce a
    sentence embedding.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 我们解释了使用BERT的跨编码器架构来衡量句子相似性。SBERT类似，但去掉了最终的分类头，并且一次处理一个句子。SBERT然后在最终输出层上使用均值池化来生成句子嵌入。
- en: Unlike BERT, SBERT is fine-tuned on sentence pairs using a *siamese* architecture.
    We can think of this as having two identical BERTs in parallel that share the
    exact same network weights.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 与BERT不同，SBERT在句子对上使用*siamese*架构进行微调。我们可以将其视为两个并行的完全相同的BERT，分享完全相同的网络权重。
- en: '![](../Images/4878afbc9ff8eb4032ac87ca0262425a.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4878afbc9ff8eb4032ac87ca0262425a.png)'
- en: An SBERT model applied to a sentence pair *sentence A* and *sentence B*. Note
    that the BERT model outputs token embeddings (consisting of 512 768-dimensional
    vectors). We then compress that data into a single 768-dimensional sentence vector
    using a pooling function.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: SBERT模型应用于句子对*句子A*和*句子B*。注意，BERT模型输出的是令牌嵌入（由512个768维向量组成）。我们随后使用池化函数将这些数据压缩成一个768维的句子向量。
- en: In reality, we are using a single BERT model. However, because we process sentence
    A followed by sentence B as *pairs* during training, it is easier to think of
    this as two models with tied weights.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，我们使用的是单个BERT模型。然而，由于我们在训练期间将句子A和句子B作为*对*进行处理，因此更容易将其视为具有相同权重的两个模型。
- en: Siamese BERT Pre-Training
  id: totrans-74
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Siamese BERT预训练
- en: There are different approaches to training sentence transformers. We will describe
    the original process featured most prominently in the original SBERT that optimizes
    on *softmax-loss*. Note that this is a high-level explanation, we will save the
    in-depth walkthrough for another article.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 训练句子转换器有不同的方法。我们将描述最初的SBERT过程，该过程主要优化*softmax-loss*。请注意，这是一个高层次的解释，我们将把深入讲解留到另一篇文章中。
- en: The softmax-loss approach used the *‘siamese’* architecture fine-tuned on the
    Stanford Natural Language Inference (SNLI) and Multi-Genre NLI (MNLI) corpora.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: softmax-loss方法使用*‘siamese’*架构，在斯坦福自然语言推理（SNLI）和多领域NLI（MNLI）语料库上进行微调。
- en: 'SNLI contains 570K sentence pairs, and MNLI contains 430K. The pairs in both
    corpora include a `premise` and a `hypothesis`. Each pair is assigned one of three
    labels:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: SNLI包含570K句子对，MNLI包含430K。这两个语料库中的句子对都包含一个`前提`和一个`假设`。每对句子被分配一个三种标签之一：
- en: '**0** — *entailment*, e.g. the `premise` suggests the `hypothesis`.'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**0** — *蕴含*，例如，`前提`暗示了`假设`。'
- en: '**1** — *neutral*, the `premise` and `hypothesis` could both be true, but they
    are not necessarily related.'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**1** — *中立*，`前提`和`假设`都可能为真，但它们不一定相关。'
- en: '**2** — *contradiction*, the `premise` and `hypothesis` contradict each other.'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**2** — *矛盾*，`前提`和`假设`互相矛盾。'
- en: Given this data, we feed sentence A (let’s say the `premise`) into siamese BERT
    A and sentence B (`hypothesis`) into siamese BERT B.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 给定这些数据，我们将句子A（假设为`前提`）输入到siamese BERT A中，将句子B（`假设`）输入到siamese BERT B中。
- en: The siamese BERT outputs our pooled sentence embeddings. There were the results
    of *three* different pooling methods in the SBERT paper. Those are *mean*, *max*,
    and *[CLS]*-pooling. The *mean*-pooling approach was best performing for both
    NLI and STSb datasets.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: siamese BERT输出我们的池化句子嵌入。SBERT论文中有*三种*不同的池化方法结果。这些方法是*均值*、*最大值*和*[CLS]*-池化。*均值*池化方法在NLI和STSb数据集中表现最佳。
- en: 'There are now two sentence embeddings. We will call embeddings A `u` and embeddings
    B `v`. The next step is to concatenate `u` and `v`. Again, several concatenation
    approaches were tested, but the highest performing was a `(u, v, |u-v|)` operation:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 现在有两个句子嵌入。我们将嵌入A称为`u`，嵌入B称为`v`。下一步是拼接`u`和`v`。再次，测试了几种拼接方法，但表现最好的方法是`(u, v, |u-v|)`操作：
- en: '![](../Images/8742d79bf3ddda40f9b682351e32673e.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8742d79bf3ddda40f9b682351e32673e.png)'
- en: We concatenate the embeddings **u**, **v**, and **|u — v|**.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将嵌入**u**、**v**和**|u — v|**进行拼接。
- en: '`|u-v|` is calculated to give us the element-wise difference between the two
    vectors. Alongside the original two embeddings (`u` and `v`), these are all fed
    into a feedforward neural net (FFNN) that has *three* outputs.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '`|u-v|`的计算结果给出两个向量的逐元素差异。除了原始的两个嵌入（`u`和`v`），这些都被输入到一个具有*三个*输出的前馈神经网络（FFNN）中。'
- en: These three outputs align to our NLI similarity labels **0**, **1**, and **2**.
    We need to calculate the softmax from our FFNN, which is done within the [cross-entropy
    loss function](https://www.pinecone.io/learn/cross-entropy-loss/). The softmax
    and labels are used to optimize on this *‘softmax-loss’*.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 这三个输出与我们的 NLI 相似性标签**0**、**1**和**2**对齐。我们需要从我们的 FFNN 计算 softmax，这在[交叉熵损失函数](https://www.pinecone.io/learn/cross-entropy-loss/)中完成。softmax
    和标签用于优化这个*‘softmax-loss’*。
- en: '![](../Images/78e112539f7a39c8d0831cf39290667a.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/78e112539f7a39c8d0831cf39290667a.png)'
- en: The operations were performed during training on two sentence embeddings, `u`
    and `v`. Note that *softmax-loss* refers cross-entropy loss (which contains a
    softmax function by default).
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 这些操作是在训练过程中对两个句子嵌入`u`和`v`进行的。注意，*softmax-loss* 指的是交叉熵损失（默认情况下包含一个 softmax 函数）。
- en: This results in our pooled sentence embeddings for similar sentences (label
    **0**) becoming *more similar*, and embeddings for dissimilar sentences (label
    **2**) becoming *less similar*.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 这导致我们对于相似句子（标签**0**）的池化句子嵌入变得*更加相似*，而对于不相似句子（标签**2**）的嵌入变得*不那么相似*。
- en: Remember we are using *siamese* BERTs **not** *dual* BERTs. Meaning we don’t
    use two independent BERT models but a single BERT that processes sentence A followed
    by sentence B.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，我们使用的是*siamese* BERT，而不是*dual* BERT。这意味着我们不使用两个独立的 BERT 模型，而是使用一个处理句子 A
    然后处理句子 B 的单个 BERT。
- en: This means that when we optimize the model weights, they are pushed in a direction
    that allows the model to output more similar vectors where we see an *entailment*
    label and more dissimilar vectors where we see a *contradiction* label.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着当我们优化模型权重时，它们会朝着一个方向推动，使模型在看到*蕴含*标签时输出更多相似的向量，而在看到*矛盾*标签时输出更多不相似的向量。
- en: The fact that this training approach works is not particularly intuitive and
    indeed has been described by Reimers as *coincidentally* producing good sentence
    embeddings [5].
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 这种训练方法有效的事实并不是特别直观，实际上 Reimers 曾描述过它*偶然*产生了良好的句子嵌入[5]。
- en: Since the original paper, further work has been done in this area. Many more
    models such as the [latest MPNet and RoBERTa models trained on 1B+ samples](https://huggingface.co/spaces/flax-sentence-embeddings/sentence-embeddings)
    (producing much better performance) have been built. We will be exploring some
    of these in future articles, and the superior training approaches they use.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 自原始论文以来，这个领域有了进一步的研究。已经建立了许多更多的模型，如[最新的 MPNet 和 RoBERTa 模型（在超过 1B 样本上训练）](https://huggingface.co/spaces/flax-sentence-embeddings/sentence-embeddings)（表现更佳）。我们将在未来的文章中探讨其中的一些模型及其使用的优越训练方法。
- en: For now, let’s look at how we can initialize and use some of these sentence-transformer
    models.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看如何初始化和使用一些这些句子变换器模型。
- en: Getting Started with Sentence Transformers
  id: totrans-96
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 开始使用句子变换器
- en: The fastest and easiest way to begin working with sentence transformers is through
    the `sentence-transformers` library created by the creators of SBERT. We can install
    it with `pip`.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 开始使用句子变换器的最快和最简单的方法是通过 SBERT 创建的`sentence-transformers`库。我们可以通过`pip`安装它。
- en: '[PRE0]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: We will start with the original SBERT model `bert-base-nli-mean-tokens`. First,
    we download and initialize the model.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从原始的 SBERT 模型`bert-base-nli-mean-tokens`开始。首先，我们下载并初始化模型。
- en: 'The output we can see here is the `SentenceTransformer` object which contains
    *three* components:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到的输出是`SentenceTransformer`对象，它包含了*三个*组件：
- en: The **transformer** itself, here we can see the max sequence length of `128`
    tokens and whether to lowercase any input (in this case, the model does *not*).
    We can also see the model class, `BertModel`.
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**变换器**本身，我们可以看到最大序列长度为`128`个标记，并且是否对任何输入进行小写处理（在这种情况下，模型*不*进行小写处理）。我们还可以看到模型类`BertModel`。'
- en: The **pooling** operation, here we can see that we are producing a `768`-dimensional
    sentence embedding. We are doing this using the *mean pooling* method.
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**池化**操作，在这里我们可以看到我们正在生成一个`768`维的句子嵌入。我们使用的是*均值池化*方法。'
- en: Once we have the model, building sentence embeddings is quickly done using the
    `encode` method.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们有了模型，使用`encode`方法可以迅速构建句子嵌入。
- en: We now have sentence embeddings that we can use to quickly compare sentence
    similarity for the use cases introduced at the start of the article; STS, semantic
    search, and clustering.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了句子嵌入，可以用来快速比较句子相似性，用于文章开头介绍的用例：STS、语义搜索和聚类。
- en: We can put together a fast STS example using nothing more than a cosine similarity
    function and Numpy.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以仅使用余弦相似度函数和 Numpy 快速编写一个 STS 示例。
- en: '![](../Images/bfec024273597cb93ea09e5ccf87b5b4.png)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bfec024273597cb93ea09e5ccf87b5b4.png)'
- en: Heatmap showing cosine similarity values between all sentence-pairs.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 热图显示了所有句子对之间的余弦相似度值。
- en: 'Here we have calculated the cosine similarity between every combination of
    our five sentence embeddings. Which are:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们计算了五个句子嵌入之间每种组合的余弦相似度。它们是：
- en: We can see the highest similarity score in the bottom-right corner with `0.64`.
    As we would hope, this is for sentences `4` and `3`, which both describe poor
    dental practices using construction materials.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到右下角的最高相似度分数为 `0.64`。正如我们所希望的，这一结果是针对描述使用建筑材料进行不良牙科实践的句子 `4` 和 `3` 的。
- en: Other sentence-transformers
  id: totrans-110
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 其他句子转换器
- en: Although we returned good results from the SBERT model, many more sentence transformer
    models have since been built. Many of which we can find in the `sentence-transformers`
    library.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们从 SBERT 模型中得到了良好的结果，但自那以后已经构建了许多其他句子转换器模型。我们可以在 `sentence-transformers`
    库中找到许多这样的模型。
- en: These newer models can significantly outperform the original SBERT. In fact,
    SBERT is no longer listed as an available model on the [SBERT.net models page](https://www.sbert.net/docs/pretrained_models.html).
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 这些更新的模型在性能上可以显著超过原始的 SBERT。事实上，SBERT 不再列为 [SBERT.net 模型页面](https://www.sbert.net/docs/pretrained_models.html)上的可用模型。
- en: A few of the top-performing models on the sentence transformers model page.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 在句子转换器模型页面上的一些顶级表现模型。
- en: We will cover some of these later models in more detail in future articles.
    For now, let’s compare one of the highest performers and run through our STS task.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在未来的文章中更详细地介绍一些这些较新的模型。现在，让我们比较一下表现最好的模型，并运行我们的 STS 任务。
- en: 'Here we have the `SentenceTransformer` model for `all-mpnet-base-v2`. The components
    are very similar to the `bert-base-nli-mean-tokens` model, with some small differences:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 这里我们有 `SentenceTransformer` 模型 `all-mpnet-base-v2`。这些组件与 `bert-base-nli-mean-tokens`
    模型非常相似，只是有一些小差异：
- en: '`max_seq_length` has increased from `128` to `384`. Meaning we can process
    sequences that are *three* times longer than we could with SBERT.'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_seq_length` 从 `128` 增加到了 `384`。这意味着我们可以处理的序列长度是使用 SBERT 时的 *三倍*。'
- en: The base model is now `MPNetModel` [4] not `BertModel`.
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基础模型现在是 `MPNetModel` [4] 而不是 `BertModel`。
- en: There is an additional normalization layer applied to sentence embeddings.
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对句子嵌入应用了额外的归一化层。
- en: Let’s compare the STS results of `all-mpnet-base-v2` against SBERT.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们比较一下 `all-mpnet-base-v2` 和 SBERT 的 STS 结果。
- en: '![](../Images/f3e67a466911193a2c3c56562143bdf1.png)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f3e67a466911193a2c3c56562143bdf1.png)'
- en: Heatmaps for both SBERT and the MPNet sentence transformer.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: SBERT 和 MPNet 句子转换器的热图。
- en: The semantic representation of later models is apparent. Although SBERT correctly
    identifies `4` and `3` as the most similar pair, it also assigns reasonably high
    similarity to other sentence pairs.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 后期模型的语义表示非常明显。虽然 SBERT 正确地识别 `4` 和 `3` 为最相似的对，但它也对其他句子对赋予了相当高的相似度。
- en: On the other hand, the MPNet model makes a *very* clear distinction between
    similar and dissimilar pairs, with most pairs scoring less than 0.1 and the `4`-`3`
    pair scored at *0.52*.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，MPNet 模型在相似对和不相似对之间做出了 *非常* 清晰的区分，大多数对的得分低于 0.1，而 `4`-`3` 对的得分为 *0.52*。
- en: 'By increasing the separation between dissimilar and similar pairs, we’re:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 通过增加不相似对和相似对之间的分离，我们正在：
- en: Making it easier to automatically identify relevant pairs.
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使得自动识别相关对变得更加容易。
- en: Pushing predictions closer to the *0* and *1* target scores for *dissimilar*
    and *similar* pairs used during training. This is something we will see more of
    in our future articles on fine-tuning these models.
  id: totrans-126
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将预测结果推向训练期间使用的 *0* 和 *1* 目标分数，使 *不相似* 和 *相似* 对的分数更加接近。这是我们将在未来的文章中关于微调这些模型时看到的内容。
- en: That’s it for this article introducing sentence embeddings and the current SOTA
    sentence transformer models for building these incredibly useful embeddings.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是本文介绍句子嵌入和当前 SOTA 句子转换器模型的全部内容，这些模型用于构建这些极其有用的嵌入。
- en: Sentence embeddings, although only recently popularized, were produced from
    a long range of fantastic innovations. We described some of the mechanics applied
    to create the first sentence transformer, SBERT.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 句子嵌入，虽然最近才流行开来，但却是从一系列出色的创新中产生的。我们描述了一些应用于创建第一个句子转换器 SBERT 的机制。
- en: We also demonstrated that despite SBERT’s very recent introduction in 2019,
    other sentence transformers already outperform the model. Fortunately for us,
    it’s easy to switch out SBERT for one of these newer models with the `sentence-transformers`
    library.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还展示了尽管SBERT于2019年才刚刚推出，但其他句子变换器已经超越了该模型。幸运的是，通过`sentence-transformers`库，我们可以轻松地将SBERT替换为这些更新的模型。
- en: References
  id: totrans-130
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] A. Vashwani, et al., [Attention Is All You Need](https://arxiv.org/abs/1706.03762)
    (2017), NeurIPS'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] A. Vashwani 等，[注意力机制全在于此](https://arxiv.org/abs/1706.03762)（2017年），NeurIPS'
- en: '[2] D. Bahdanau, et al., [Neural Machine Translation by Jointly Learning to
    Align and Translate](https://arxiv.org/abs/1409.0473) (2015), ICLR'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] D. Bahdanau 等，[通过共同学习对齐和翻译的神经机器翻译](https://arxiv.org/abs/1409.0473)（2015年），ICLR'
- en: '[3] N. Reimers, I. Gurevych, [Sentence-BERT: Sentence Embeddings using Siamese
    BERT-Networks](https://arxiv.org/abs/1908.10084) (2019), ACL'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] N. Reimers, I. Gurevych，[Sentence-BERT：使用Siamese BERT网络的句子嵌入](https://arxiv.org/abs/1908.10084)（2019年），ACL'
- en: '[4] [MPNet Model](https://huggingface.co/transformers/model_doc/mpnet.html),
    Hugging Face Docs'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] [MPNet模型](https://huggingface.co/transformers/model_doc/mpnet.html)，Hugging
    Face文档'
- en: '[5] N. Reimers, [Natural Language Inference](https://github.com/UKPLab/sentence-transformers/blob/master/examples/training/nli/README.md),
    sentence-transformers on GitHub'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '[5] N. Reimers，[自然语言推断](https://github.com/UKPLab/sentence-transformers/blob/master/examples/training/nli/README.md)，GitHub上的sentence-transformers'
- en: '**All images are by the author except where stated otherwise*'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '**所有图片均为作者提供，除非另有说明**'
