- en: Say Once! Repeating Words Is Not Helping AI
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 说一遍！重复的话语并未帮助AI
- en: 原文：[https://towardsdatascience.com/say-once-repeating-words-is-not-helping-ai-58f38035f66e](https://towardsdatascience.com/say-once-repeating-words-is-not-helping-ai-58f38035f66e)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/say-once-repeating-words-is-not-helping-ai-58f38035f66e](https://towardsdatascience.com/say-once-repeating-words-is-not-helping-ai-58f38035f66e)
- en: '| ARTIFICIAL INTELLIGENCE | NLP | LLMs'
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '| 人工智能 | 自然语言处理 | 大语言模型'
- en: How and why is repeating tokens harming LLMs? Why is this a problem?
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 重复使用标记如何以及为什么会对LLM造成伤害？这是什么问题？
- en: '[](https://salvatore-raieli.medium.com/?source=post_page-----58f38035f66e--------------------------------)[![Salvatore
    Raieli](../Images/6bb4520e2df40d20283e7283141b5e06.png)](https://salvatore-raieli.medium.com/?source=post_page-----58f38035f66e--------------------------------)[](https://towardsdatascience.com/?source=post_page-----58f38035f66e--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----58f38035f66e--------------------------------)
    [Salvatore Raieli](https://salvatore-raieli.medium.com/?source=post_page-----58f38035f66e--------------------------------)'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://salvatore-raieli.medium.com/?source=post_page-----58f38035f66e--------------------------------)[![Salvatore
    Raieli](../Images/6bb4520e2df40d20283e7283141b5e06.png)](https://salvatore-raieli.medium.com/?source=post_page-----58f38035f66e--------------------------------)[](https://towardsdatascience.com/?source=post_page-----58f38035f66e--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----58f38035f66e--------------------------------)
    [Salvatore Raieli](https://salvatore-raieli.medium.com/?source=post_page-----58f38035f66e--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----58f38035f66e--------------------------------)
    ·14 min read·Jun 20, 2023
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布在[Towards Data Science](https://towardsdatascience.com/?source=post_page-----58f38035f66e--------------------------------)
    ·14分钟阅读·2023年6月20日
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/40a23dee61acd2fe912d9497b6a07d5f.png)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/40a23dee61acd2fe912d9497b6a07d5f.png)'
- en: image by [Kristina Flour](https://unsplash.com/it/@tinaflour) on Unsplash
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由[Kristina Flour](https://unsplash.com/it/@tinaflour)提供，来源于Unsplash
- en: '[Large Language Models (LLMs)](https://en.wikipedia.org/wiki/Large_language_model)
    have shown their capabilities and have taken the world by storm. Every big company
    now has a model with a fancy name. But, under the hood, they are all [transformers](https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)).
    **Everyone dreams of the trillion parameters, but is there no limit?**'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '[大语言模型（LLMs）](https://en.wikipedia.org/wiki/Large_language_model)已经展示了它们的能力，并且在全球引起了轰动。每个大公司现在都有一个名字花哨的模型。但实际上，它们都是[变换器](https://en.wikipedia.org/wiki/Transformer_(machine_learning_model))。**每个人都梦想拥有万亿参数，但难道没有限制吗？**'
- en: 'In this article, we discuss that:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我们讨论了以下内容：
- en: Is it guaranteed that a bigger model has better performance than a small model?
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更大的模型是否保证比小模型性能更好？
- en: Do we have the data for huge models?
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们是否有关于巨大模型的数据？
- en: What happens if instead of collecting new data you use the data again?
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果不收集新数据而是重复使用已有数据，会发生什么？
- en: 'Scaling over the sky: what is hurting the wing?'
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在天空中扩展：是什么伤害了机翼？
- en: '![](../Images/4c30898f3c8683fc888bf2424f1a20f8.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4c30898f3c8683fc888bf2424f1a20f8.png)'
- en: Image by [Sean Pollock](https://unsplash.com/it/@seanpollock) on Unsplash
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由[Sean Pollock](https://unsplash.com/it/@seanpollock)提供，来源于Unsplash
- en: '[OpenAI has defined the scaling law](https://arxiv.org/abs/2001.08361), stating
    that model performance follows a power law according to how many parameters are
    used and the number of data points. This along with the search for emergent properties
    has created the parameter race: **the bigger the model, the better.**'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '[OpenAI定义了规模定律](https://arxiv.org/abs/2001.08361)，指出模型性能遵循一个幂律，取决于使用了多少参数和数据点。这与对新兴属性的探索一起，催生了参数竞赛：**模型越大，性能越好。**'
- en: Is that true? Are bigger models giving better performance?
  id: totrans-18
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 这是真的吗？更大的模型是否会提供更好的性能？
- en: Recently, emergent properties have come into crisis. As shown by Stanford researchers,
    the concept of emergent property may not exist.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，新兴属性面临危机。斯坦福研究人员表明，新兴属性的概念可能并不存在。
- en: '[](/emergent-abilities-in-ai-are-we-chasing-a-myth-fead754a1bf9?source=post_page-----58f38035f66e--------------------------------)
    [## Emergent Abilities in AI: Are We Chasing a Myth?'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/emergent-abilities-in-ai-are-we-chasing-a-myth-fead754a1bf9?source=post_page-----58f38035f66e--------------------------------)
    [## 人工智能的新兴能力：我们是在追逐一个神话吗？'
- en: Changing Perspective on Large Language Models emerging properties
  id: totrans-21
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 对大语言模型（LLMs）新兴属性的观点改变
- en: towardsdatascience.com](/emergent-abilities-in-ai-are-we-chasing-a-myth-fead754a1bf9?source=post_page-----58f38035f66e--------------------------------)
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/emergent-abilities-in-ai-are-we-chasing-a-myth-fead754a1bf9?source=post_page-----58f38035f66e--------------------------------)
- en: The scaling law probably assigns much less value to the dataset than is actually
    thought. [DeepMind has shown with Chinchilla](https://www.deepmind.com/publications/an-empirical-analysis-of-compute-optimal-large-language-model-training),
    that one should not only think to scale the parameters but also the data. In fact,
    Chinchilla shows that it is superior in capacity to [Gopher](https://arxiv.org/abs/2112.11446)
    (70 B vs. 280 B parameters)
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 缩放法则可能赋予数据集的价值远低于实际认为的价值。[DeepMind通过Chinchilla](https://www.deepmind.com/publications/an-empirical-analysis-of-compute-optimal-large-language-model-training)表明，人们不仅要考虑参数的规模，还要考虑数据的规模。事实上，Chinchilla显示出它在容量上优于[Gopher](https://arxiv.org/abs/2112.11446)（70
    B与280 B参数）
- en: '![](../Images/b7da5b5a3d95d2a77649d3981baacb1d.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b7da5b5a3d95d2a77649d3981baacb1d.png)'
- en: '“Overlaid predictions. We overlay the predictions from our three different
    approaches, along with projections from Kaplan et al. (2020). We find that all
    three methods predict that current large models should be substantially smaller
    and therefore trained much longer than is currently done.” Image source: [here](https://arxiv.org/pdf/2203.15556.pdf)'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: “叠加预测。我们叠加了三种不同方法的预测，以及Kaplan等（2020年）的预测。我们发现所有三种方法都预测当前的大型模型应该小得多，因此训练时间也应该比现在的时间长。”
    图片来源：[这里](https://arxiv.org/pdf/2203.15556.pdf)
- en: Recently, the machine learning community got excited about LLaMA not only because
    it is open source, but because the 65 B version of parameters outperformed [OPT
    175 B](https://ai.facebook.com/blog/democratizing-access-to-large-scale-language-models-with-opt-175b/).
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，机器学习社区对LLaMA感到兴奋，不仅因为它是开源的，还因为65 B版本的参数超越了[OPT 175 B](https://ai.facebook.com/blog/democratizing-access-to-large-scale-language-models-with-opt-175b/)。
- en: '[](https://medium.com/mlearning-ai/metas-llama-a-small-language-model-beating-giants-5065948e0b7f?source=post_page-----58f38035f66e--------------------------------)
    [## META’s LLaMA: A small language model beating giants'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/mlearning-ai/metas-llama-a-small-language-model-beating-giants-5065948e0b7f?source=post_page-----58f38035f66e--------------------------------)
    [## META的LLaMA：一个小型语言模型战胜巨头'
- en: META open-source model will help us to understand how LMs biases arise
  id: totrans-28
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: META开源模型将帮助我们理解语言模型偏差的产生
- en: medium.com](https://medium.com/mlearning-ai/metas-llama-a-small-language-model-beating-giants-5065948e0b7f?source=post_page-----58f38035f66e--------------------------------)
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '[medium.com](https://medium.com/mlearning-ai/metas-llama-a-small-language-model-beating-giants-5065948e0b7f?source=post_page-----58f38035f66e--------------------------------)'
- en: 'As DeepMind states in the Chinchilla article, one can estimate how many tokens
    are required to fully train a state-of-the-art LLM. On the other hand, one can
    also estimate how many high-quality tokens exist. Recent research has wondered
    about this topic. [They concluded](https://arxiv.org/pdf/2211.04325.pdf):'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 正如DeepMind在Chinchilla文章中所述，可以估计完全训练一个最先进的LLM所需的tokens数量。另一方面，也可以估计存在多少高质量的tokens。最近的研究对此话题产生了疑问。[他们得出结论](https://arxiv.org/pdf/2211.04325.pdf)：
- en: Language datasets have grown exponentially, with a 50% yearly growth in language
    dataset publication (up to 2e12 words by the end of 2022). This is showing the
    research and publication of new language datasets is a very active field.
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 语言数据集呈指数增长，语言数据集出版的年增长率达到50%（到2022年底达到2e12个单词）。这表明新语言数据集的研究和出版是一个非常活跃的领域。
- en: On the other hand, the number of words on the internet (stock of words) is growing
    (and the authors estimate it between 7e13 and 7e16 words, so 1.5 -4.5 orders of
    magnitude).
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 另一方面，互联网上的单词数量（单词库存）在增长（作者估计在7e13到7e16个单词之间，因此是1.5到4.5个数量级）。
- en: Since, however, they try to use a stock of words that is of high quality, actually
    the authors estimate the quality stock to be between n 4.6e12 and 1.7e13 words.
    **The authors state that between 2023–2027 we will have exhausted the number of
    quality words and between 2030 and 2050 the entire stock.**
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然而，由于他们尝试使用高质量的单词库存，实际上作者估计高质量库存在4.6e12到1.7e13个单词之间。**作者表示，在2023年至2027年间，我们将耗尽质量单词的数量，而在2030年至2050年之间将耗尽全部库存。**
- en: The stock of images is not much better off either (three to four orders of magnitude)
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图像库存的情况也没有好多少（三到四个数量级）
- en: '![](../Images/b40e2e8fedec8ae3213af29a06693e90.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b40e2e8fedec8ae3213af29a06693e90.png)'
- en: 'projection of data usage. image source: [here](https://arxiv.org/pdf/2211.04325.pdf)'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 数据使用的预测。图片来源：[这里](https://arxiv.org/pdf/2211.04325.pdf)
- en: Why is this happening?
  id: totrans-37
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 为什么会发生这种情况？
- en: 'Well, because we humans are not infinite and do not produce text as much as
    ChatGPT. In fact, projections of the number of Internet users (real and predicted)
    speak volumes:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 好吧，因为我们人类并非无限制地生成文本，不能像ChatGPT那样大量生产。事实上，互联网用户数量的预测（真实与预测）说明了一切：
- en: '![](../Images/02944a3a9c4df3a096c2b764ee797c18.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/02944a3a9c4df3a096c2b764ee797c18.png)'
- en: 'Real and projected evolution of internet users. image source: [here](https://arxiv.org/pdf/2211.04325.pdf)'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 互联网用户的真实和预测演变。图片来源：[这里](https://arxiv.org/pdf/2211.04325.pdf)
- en: In fact, not everyone is happy about texts, code, and other sources being used
    to train artificial intelligence models. In fact, [Wikipedia, Reddit, and other
    sources historically used to train models would like companies to pay to use their
    data](/the-infinite-babel-library-of-llms-90e203b2f6b0). In contrast, companies
    are invoking fair use, and at present the regulation landscape is unclear.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，并非所有人都对用文本、代码和其他来源来训练人工智能模型感到满意。实际上，[维基百科、Reddit和其他用于训练模型的来源希望公司付费使用他们的数据](/the-infinite-babel-library-of-llms-90e203b2f6b0)。相比之下，公司则援引公平使用条款，目前的法规环境尚不明确。
- en: Combining the data together, a trend can be clearly seen. The number of tokens
    required to optimally train an LLM is growing faster than the tokens in stock.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 将数据整合在一起，可以清晰地看到一个趋势。为了最佳训练LLM所需的令牌数量增长速度超过了现有的令牌库存。
- en: '![](../Images/7a7d8cfc715fd75d345833f62d899c07.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7a7d8cfc715fd75d345833f62d899c07.png)'
- en: 'image source: [here](https://arxiv.org/pdf/2305.13230.pdf)'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：[这里](https://arxiv.org/pdf/2305.13230.pdf)
- en: According to the scaling law defined by Chinchilla (number of tokens required
    for optimal LLM training), we have already exceeded the limit. From the graph,
    we can see that according to these estimates with [PaLM-540 B](https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.html),
    we have reached the limit (10.8 trillion tokens required vs. 9 trillion in stock).
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 根据Chinchilla定义的扩展法则（用于最佳LLM训练所需的令牌数量），我们已经超过了限制。从图表中可以看出，根据这些估计，使用[PaLM-540
    B](https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.html)，我们已达到极限（需要10.8万亿个令牌，而库存为9万亿）。
- en: '**Some authors have called this problem with the “token crisis.”** Moreover,
    so far we have considered only English-language tokens, but there are seven thousand
    other languages. [Fifty-six percent of the entire web is in English](https://w3techs.com/technologies/overview/content_language),
    and the remaining 44 percent belongs to only 100 other languages. And this is
    reflected in the performance of models in other languages.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '**一些作者称这个问题为“令牌危机”。** 此外，到目前为止，我们仅考虑了英语令牌，但还有七千种其他语言。[整个网络的56%是英语](https://w3techs.com/technologies/overview/content_language)，剩下的44%则属于仅100种其他语言。这也反映在其他语言模型的表现中。'
- en: Can we get more data?
  id: totrans-47
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 我们能获取更多的数据吗？
- en: '![](../Images/9f98ef16a2dbf8efd323a627e748fd11.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9f98ef16a2dbf8efd323a627e748fd11.png)'
- en: image by [Karen Vardazaryan](https://unsplash.com/it/@bright) on Unsplash
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由[Karen Vardazaryan](https://unsplash.com/it/@bright)提供，来源于Unsplash
- en: As we have seen more parameters do not equate to better performance. For better
    performance, we need quality tokens (texts), but these are in short supply. **How
    can we obtain them? Can we help ourselves with artificial intelligence?**
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所见，更多的参数并不等于更好的性能。为了获得更好的性能，我们需要优质的令牌（文本），但这些资源稀缺。**我们如何获得这些资源？我们能依靠人工智能来帮助自己吗？**
- en: Why we are not using Chat-GPT to produce text?
  id: totrans-51
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 为什么我们不使用Chat-GPT来生成文本？
- en: '**If we humans are not producing enough text, why not automate this process?**
    A recent study shows [how this process is not optimal](https://arxiv.org/pdf/2305.15717.pdf).
    Stanford Alpaca was trained using 52,000 examples derived from [GPT-3](https://en.wikipedia.org/wiki/GPT-3),
    but only apparently achieved similar performance. [**In reality, the model learns
    the style of the target model but not its knowledge.**](https://levelup.gitconnected.com/the-imitation-game-taming-the-gap-between-open-source-and-proprietary-models-627374b390e5)'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '**如果我们人类生成的文本不足，为什么不自动化这个过程呢？** 最近的研究显示了[这个过程如何不尽如人意](https://arxiv.org/pdf/2305.15717.pdf)。斯坦福Alpaca使用52,000个从[GPT-3](https://en.wikipedia.org/wiki/GPT-3)中衍生的示例进行训练，但显然只达到了类似的性能。[**实际上，该模型学习了目标模型的风格，但未能掌握其知识。**](https://levelup.gitconnected.com/the-imitation-game-taming-the-gap-between-open-source-and-proprietary-models-627374b390e5)'
- en: Why not train longer?
  id: totrans-53
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 为什么不进行更长时间的训练？
- en: 'For both PaLM, Gopher, and LLaMA (also for the other LLMs) it is clearly written
    that the models were trained for a few epochs (one or however few). This is not
    a limitation of the [Transformer](https://en.wikipedia.org/wiki/Transformer_(machine_learning_model))
    because, for example, the [Vision Transformers (ViT)](https://en.wikipedia.org/wiki/Vision_transformer)
    have been trained for 300 epochs on ImageNet (1 million images), as shown in the
    table:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 对于PaLM、Gopher和LLaMA（以及其他LLMs），清楚地写明了这些模型训练了几个时期（一个或几个）。这不是[Transformer](https://en.wikipedia.org/wiki/Transformer_(machine_learning_model))的限制，因为例如，[视觉Transformer（ViT）](https://en.wikipedia.org/wiki/Vision_transformer)在ImageNet（100万张图片）上训练了300个时期，如下表所示：
- en: '![](../Images/bc5ebefbcba1ba2a15990ccedb965a4a.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bc5ebefbcba1ba2a15990ccedb965a4a.png)'
- en: 'image source: [here](https://arxiv.org/pdf/2010.11929.pdf)'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：[这里](https://arxiv.org/pdf/2010.11929.pdf)
- en: 'Because it is beyond expensive. [In the LLaMA article](https://medium.com/mlearning-ai/metas-llama-a-small-language-model-beating-giants-5065948e0b7f),
    the authors trained for only one epoch (and two epochs for only part of the dataset).
    Nevertheless, the authors report:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 因为这实在太昂贵了。在[LLaMA文章](https://medium.com/mlearning-ai/metas-llama-a-small-language-model-beating-giants-5065948e0b7f)中，作者只训练了一个时期（而数据集的一部分训练了两个时期）。尽管如此，作者报告称：
- en: When training a 65B-parameter model, our code processes around 380 tokens/sec/GPU
    on 2048 A100 GPU with 80GB of RAM. This means that training over our dataset containing
    1.4T tokens takes approximately 21 days. ([source](https://arxiv.org/pdf/2302.13971.pdf))
  id: totrans-58
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 当训练一个65B参数的模型时，我们的代码在2048张80GB RAM的A100 GPU上处理约380个令牌/秒。这意味着在包含1.4T令牌的数据集上训练大约需要21天。
    ([source](https://arxiv.org/pdf/2302.13971.pdf))
- en: Training an LLM for even a few epochs is extremely expensive. As calculated
    by [Dmytro Nikolaiev (Dimid)](https://medium.com/u/97b5279dad26?source=post_page-----58f38035f66e--------------------------------)
    this is meaning [4.0 million dollars](/behind-the-millions-estimating-the-scale-of-large-language-models-97bd7287fb6b)
    if you train a model similar to META’s LLaMA on the Google Cloud Platform.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 训练一个大型语言模型（LLM）即使只训练几个时期也极其昂贵。如[德米特罗·尼古拉耶夫（Dimid）](https://medium.com/u/97b5279dad26?source=post_page-----58f38035f66e--------------------------------)计算，这相当于[400万美元](/behind-the-millions-estimating-the-scale-of-large-language-models-97bd7287fb6b)，如果你在谷歌云平台上训练一个类似于META的LLaMA的模型。
- en: 'So training for other epochs would lead to an exponential increase in costs.
    **Also, we don’t know if this additional training is really useful: we haven’t
    tested it yet.**'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 所以训练其他的时期将导致成本的指数增加。**此外，我们不知道这些额外的训练是否真的有用：我们还没有测试过。**
- en: 'Recently a group of researchers at the University of Singapore studied what
    happens if we train an LLM for multiple epochs:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，新加坡大学的一组研究人员研究了如果我们训练一个LLM多个时期会发生什么：
- en: '[](https://arxiv.org/abs/2305.13230?source=post_page-----58f38035f66e--------------------------------)
    [## To Repeat or Not To Repeat: Insights from Scaling LLM under Token-Crisis'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://arxiv.org/abs/2305.13230?source=post_page-----58f38035f66e--------------------------------)
    [## 是否重复：从扩展LLM中的令牌危机中获得的见解'
- en: Recent research has highlighted the importance of dataset size in scaling language
    models. However, large language…
  id: totrans-63
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 最近的研究突显了数据集规模在扩展语言模型中的重要性。然而，大型语言模型...
- en: arxiv.org](https://arxiv.org/abs/2305.13230?source=post_page-----58f38035f66e--------------------------------)
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: arxiv.org](https://arxiv.org/abs/2305.13230?source=post_page-----58f38035f66e--------------------------------)
- en: Repetita iuvant aut continuata secant
  id: totrans-65
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Repetita iuvant aut continuata secant
- en: '![](../Images/bad195651655fa3252688bea96be3549.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bad195651655fa3252688bea96be3549.png)'
- en: Image by [Unseen Studio](https://unsplash.com/it/@craftedbygc) on Unsplash
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由[Unseen Studio](https://unsplash.com/it/@craftedbygc)提供，来自Unsplash
- en: Until now we know that the performance of a model is derived not only by the
    number of parameters but also by the number of quality tokens used to train. On
    the other hand, these quality tokens are not infinite and we are approaching the
    limit. **If we cannot find enough quality tokens and it is an option to generate
    with AI, what could we do?**
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 直到现在，我们知道模型的表现不仅由参数数量决定，还由用于训练的优质令牌数量决定。另一方面，这些优质令牌不是无限的，我们正接近极限。**如果我们找不到足够的优质令牌，而生成它们是一个选项，我们该怎么办？**
- en: Can we use the same training set and train longer?
  id: totrans-69
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 我们可以使用相同的训练集并延长训练时间吗？
- en: There is a Latin locution that states that repeating things benefits (r*epetita
    iuvant*), but over time someone added “but continuing bores” (*continuata secant*).
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 有一句拉丁语说，重复有益（*repetita iuvant*），但随着时间的推移，有人加上了“但持续的无聊”（*continuata secant*）。
- en: 'The same is true for neural networks: increasing the number of epochs improves
    network performance (decrease in loss); at some point, however, while the loss
    in the training set continues to fall, the loss in the validation set begins to
    rise. The neural network went into [overfitting](https://en.wikipedia.org/wiki/Overfitting),
    beginning to consider patterns that are only present in the training set and losing
    the ability to generalize.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络也是如此：增加训练轮数会提高网络性能（减少损失）；然而，在某个时刻，当训练集中的损失继续下降时，验证集中的损失开始上升。神经网络进入了[过拟合](https://en.wikipedia.org/wiki/Overfitting)状态，开始考虑仅存在于训练集中的模式，失去了泛化能力。
- en: '![](../Images/66cd1d33ca987237ac5987a905023782.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/66cd1d33ca987237ac5987a905023782.png)'
- en: 'Overfitting/overtraining in supervised learning. Image source: [here](https://en.wikipedia.org/wiki/Overfitting)'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 监督学习中的过拟合/过度训练。图片来源：[here](https://en.wikipedia.org/wiki/Overfitting)
- en: Ok, this has been studied extensively for small neural networks, but what about
    huge transformers?
  id: totrans-74
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 好的，这在小型神经网络中已经进行了广泛研究，但在大型变压器中情况如何呢？
- en: The authors of this study used the [T5 model](https://ai.googleblog.com/2020/02/exploring-transfer-learning-with-t5.html)
    (encoder-decoder model) on the C4 dataset. The authors trained several versions
    of the model, increasing the number of parameters until the larger model outperformed
    the smaller model (indicating that the larger model received a sufficient number
    of tokens, as Chinchilla’s law). The authors noted that there was a linear relationship
    between the number of tokens required and the size of the model (confirming what
    DeepMind saw with Chinchilla).
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 本研究的作者在 C4 数据集上使用了[T5模型](https://ai.googleblog.com/2020/02/exploring-transfer-learning-with-t5.html)（编码器-解码器模型）。作者训练了几个版本的模型，增加了参数数量，直到较大的模型超过了较小的模型（表明较大的模型获得了足够的
    tokens，如 Chinchilla 定律所示）。作者指出，所需 tokens 的数量与模型的大小之间存在线性关系（证实了 DeepMind 对 Chinchilla
    的观察）。
- en: '![](../Images/00720fa825e60895d6743c561eab6428.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/00720fa825e60895d6743c561eab6428.png)'
- en: 'Image source: [here](https://arxiv.org/pdf/2305.13230.pdf)'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：[here](https://arxiv.org/pdf/2305.13230.pdf)
- en: 'The C4 dataset is limited (does not have infinite tokens) so to increase the
    number of parameters the authors found themselves in a tokens-scarcity condition.
    Thus they decided to simulate what happens if an LLM sees repeated data. They
    sampled a certain number of tokens, so the model found itself seeing them again
    in tokens training. This showed:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: C4 数据集是有限的（没有无限的 tokens），因此为了增加参数数量，作者发现自己处于 tokens 短缺的条件下。因此，他们决定模拟 LLM 看到重复数据的情况。他们抽取了一定数量的
    tokens，因此模型发现自己在 tokens 训练中再次看到它们。这表明：
- en: Repeated tokens lead to degraded performance.
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 重复的 tokens 导致性能下降。
- en: Larger models are more susceptible to overfitting under tokens-crisis conditions
    (so even though it theoretically consumes more computational resources this leads
    to degraded performance).
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 tokens 短缺条件下，大型模型更容易发生过拟合（因此尽管理论上它消耗了更多的计算资源，但这会导致性能下降）。
- en: '![](../Images/940085b94c885d0037fba10b6bf27663.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/940085b94c885d0037fba10b6bf27663.png)'
- en: 'Image source: [here](https://arxiv.org/pdf/2305.13230.pdf)'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：[here](https://arxiv.org/pdf/2305.13230.pdf)
- en: In addition, these models are used for downstream tasks. Often an LLM is trained
    unsupervised on a large amount of text and then fine-tuned on a smaller dataset
    for a downstream task. Or it may go through a process called alignment (as in
    the case of ChatGPT).
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，这些模型还用于下游任务。通常，一个大语言模型（LLM）在大量文本上进行无监督训练，然后在较小的数据集上进行微调以完成下游任务。或者，它可能会经历称为对齐的过程（如
    ChatGPT 的情况）。
- en: When an LLM is trained on repeated data even though it is then fine-tuned on
    another dataset, performance is degraded. So the downstream tasks are also impacted.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 当一个 LLM 在重复的数据上训练，即使之后在另一个数据集上进行微调，性能也会下降。因此，下游任务也会受到影响。
- en: '![](../Images/ac9c68a16e51f162ca1bc64c95b3d321.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ac9c68a16e51f162ca1bc64c95b3d321.png)'
- en: 'Image source: [here](https://arxiv.org/pdf/2305.13230.pdf)'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：[here](https://arxiv.org/pdf/2305.13230.pdf)
- en: Why repeated tokens are not a good idea
  id: totrans-87
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为什么重复的 tokens 不是一个好主意
- en: '![](../Images/2c4a3f3cca1e4bbf67e2653d55867c12.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2c4a3f3cca1e4bbf67e2653d55867c12.png)'
- en: Image by [Brett Jordan](https://unsplash.com/it/@brett_jordan) on Unsplash
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由[Brett Jordan](https://unsplash.com/it/@brett_jordan)在 Unsplash 提供
- en: We just saw that repeated tokens harm training. But why does this happen?
  id: totrans-90
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 我们刚刚看到重复的 tokens 会损害训练。但是为什么会发生这种情况呢？
- en: The authors decided to investigate by keeping the number of repeated tokens
    fixed and increasing the number of total tokens in the dataset. The results show
    that a larger dataset alleviates multi-epoch degradation issues.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 作者决定通过固定重复标记的数量并增加数据集中总标记的数量来进行调查。结果表明，更大的数据集缓解了多轮训练降级的问题。
- en: '![](../Images/41965687e7b13654ad2741d3ed1689db.png)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/41965687e7b13654ad2741d3ed1689db.png)'
- en: 'Image source: [here](https://arxiv.org/pdf/2305.13230.pdf)'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：[这里](https://arxiv.org/pdf/2305.13230.pdf)
- en: '[Last year Galactica](https://arxiv.org/pdf/2211.09085.pdf) was published (a
    model that was supposed to help scientists but [lasted only three days](https://www.technologyreview.com/2022/11/18/1063487/meta-large-language-model-ai-only-survived-three-days-gpt-3-science/)).
    Apart from the spectacular debacle, the article suggested that part of their results
    was from the quality of the data. According to the authors, data quality reduced
    the risk of overfitting:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '[去年，Galactica](https://arxiv.org/pdf/2211.09085.pdf) 发布了（一个原本旨在帮助科学家的模型，但[仅存活了三天](https://www.technologyreview.com/2022/11/18/1063487/meta-large-language-model-ai-only-survived-three-days-gpt-3-science/)）。除了那次惊人的失败之外，文章还指出，他们的部分结果来源于数据的质量。根据作者的说法，数据质量降低了过拟合的风险：'
- en: We are able to train on it for multiple epochs without overfitting, where upstream
    and downstream performance improves with use of repeated tokens. ([source](https://arxiv.org/pdf/2211.09085.pdf))
  id: totrans-95
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 我们能够在其上进行多轮训练而不会过拟合，其中上游和下游性能随着重复标记的使用而提高。 ([来源](https://arxiv.org/pdf/2211.09085.pdf))
- en: '![](../Images/973eb580593c47877e6f117fff7207e2.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/973eb580593c47877e6f117fff7207e2.png)'
- en: 'image source: [here](https://arxiv.org/pdf/2211.09085.pdf)'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：[这里](https://arxiv.org/pdf/2211.09085.pdf)
- en: For the authors, the repeated tokens actually not only do not harm the model
    training but actually improved downstream performance.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 对于作者来说，重复标记实际上不仅没有损害模型训练，反而提高了下游性能。
- en: In this new study, the authors use the Wikipedia dataset which is considered
    a higher quality dataset than C4, and add repeated tokens. The results show that
    there is a similar level of degradation, which is against what is stated in Galactica’s
    article.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 在这项新研究中，作者使用了被认为质量高于C4的维基百科数据集，并添加了重复标记。结果显示，降级水平相似，这与Galactica文章中的说法相反。
- en: '![](../Images/c6635e936469851fb038cd398a69c00b.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c6635e936469851fb038cd398a69c00b.png)'
- en: 'image source: [here](https://arxiv.org/pdf/2305.13230.pdf)'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：[这里](https://arxiv.org/pdf/2305.13230.pdf)
- en: 'The authors also tried to investigate whether it was also due to model scaling.
    During the scaling of a model, both the number of parameters and the computational
    cost increase. The authors decided to study these two factors individually:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 作者还尝试调查是否也由于模型扩展。在模型扩展过程中，参数数量和计算成本都会增加。作者决定分别研究这两个因素：
- en: '[**Mixture-of-Experts (MoE)**](https://ai.googleblog.com/2022/11/mixture-of-experts-with-expert-choice.html)
    because although it increases the number of parameters it maintains a similar
    computational cost.'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**专家混合模型（MoE）**](https://ai.googleblog.com/2022/11/mixture-of-experts-with-expert-choice.html)
    因为虽然它增加了参数数量，但保持了类似的计算成本。'
- en: '[**ParamShare**](https://ai.googleblog.com/2022/11/mixture-of-experts-with-expert-choice.html),
    on the other hand, reduces the number of parameters but maintains the same computational
    cost.'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**ParamShare**](https://ai.googleblog.com/2022/11/mixture-of-experts-with-expert-choice.html)
    则减少了参数数量，但保持了相同的计算成本。'
- en: '![](../Images/a1c97c42cba2c96288f2c14b2d4e632d.png)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a1c97c42cba2c96288f2c14b2d4e632d.png)'
- en: 'image source: [here](https://arxiv.org/pdf/2305.13230.pdf)'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：[这里](https://arxiv.org/pdf/2305.13230.pdf)
- en: The results show that the model with fewer parameters is less affected by repeated
    tokens. In contrast, the MoE model (greater number of parameters) is more prone
    to overfitting. The result is interesting because MoE has been used successfully
    in many AI models, so the authors suggest that although MoE is a useful technique
    when there is enough data, it can hurt performance when there are not enough tokens.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 结果表明，参数较少的模型受重复标记的影响较小。相比之下，MoE模型（参数较多）更容易过拟合。这个结果很有趣，因为MoE在许多AI模型中已经成功使用，所以作者建议，虽然MoE是一个在数据充足时有用的技术，但在标记不足时可能会损害性能。
- en: 'The authors also explored whether objective training impacts performance degradation.
    In general, there are two training objectives:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 作者还探讨了目标训练是否影响性能降级。通常，有两个训练目标：
- en: '[**Next token prediction**](https://arxiv.org/abs/2212.11281) (given a sequence
    of tokens predict the next in the sequence).'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**下一个标记预测**](https://arxiv.org/abs/2212.11281)（给定一系列标记，预测序列中的下一个标记）。'
- en: '[**Masked language modeling**](https://huggingface.co/docs/transformers/main/tasks/masked_language_modeling),
    where one or more tokens are masked and need to predict them.'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**掩码语言建模**](https://huggingface.co/docs/transformers/main/tasks/masked_language_modeling)，即掩盖一个或多个标记并需要预测它们。'
- en: Recently, with [PaLM2–2](https://ai.google/discover/palm2/), Google introduced
    UL2 which is a mix between these two training objectives. UL2 has been shown to
    accelerate model training however interestingly, UL2 is more prone to overfitting
    and has greater multi-epoch degradation.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，谷歌推出了 [PaLM2–2](https://ai.google/discover/palm2/)，并引入了 UL2，这是一种这两种训练目标的混合。虽然
    UL2 显示出加速模型训练的效果，但有趣的是，UL2 更容易过拟合，并且有更大的多轮次退化。
- en: '![](../Images/714c09520e9e558a53f6cf1acd00586a.png)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/714c09520e9e558a53f6cf1acd00586a.png)'
- en: 'image source: [here](https://arxiv.org/pdf/2305.13230.pdf)'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：[这里](https://arxiv.org/pdf/2305.13230.pdf)
- en: The authors next explored how they could try to alleviate multi-epoch degradation.
    Since regularization techniques are used precisely to prevent overfitting, the
    authors tested whether these techniques had a beneficial effect here as well.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 作者接着探索了如何尝试缓解多轮次退化。由于正则化技术的使用正是为了防止过拟合，作者测试了这些技术是否在这里也有有益的效果。
- en: Dropout shows to be one of the most efficient techniques to alleviate the problem.
    This is not surprising because one of the most efficient regularization techniques,
    it is easily parallelized and used by most of the models.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: Dropout 被证明是缓解这个问题的最有效技术之一。这并不令人惊讶，因为作为一种最有效的正则化技术之一，它容易并行化，并被大多数模型使用。
- en: '![](../Images/819f3826fd343df6ba93c90da456a499.png)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/819f3826fd343df6ba93c90da456a499.png)'
- en: 'image source: [here](https://arxiv.org/pdf/2305.13230.pdf)'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：[这里](https://arxiv.org/pdf/2305.13230.pdf)
- en: Moreover, it works best for the authors to start without dropout and only at
    a later point in the training to add dropout.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，作者发现最好从不使用 dropout 开始，并仅在训练的较晚阶段添加 dropout。
- en: '![](../Images/f0cf3be64f13de793f9c2298a8097df7.png)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f0cf3be64f13de793f9c2298a8097df7.png)'
- en: 'image source: [here](https://arxiv.org/pdf/2305.13230.pdf)'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：[这里](https://arxiv.org/pdf/2305.13230.pdf)
- en: On the other hand, the authors note that using Dropout in some models, especially
    the larger ones, can lead to a slight reduction in performance. So although it
    may have beneficial effects against overfitting it could lead to unexpected behaviors
    in other contexts. So much that models GPT-3, PaLM, LLaMA, Chinchilla, and Gopher
    do not use it in their architecture.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，作者指出，在某些模型，尤其是较大的模型中，使用 Dropout 可能会导致性能轻微下降。因此，尽管它可能在防止过拟合方面有益，但在其他环境中可能会导致意外的行为。因此，GPT-3、PaLM、LLaMA、Chinchilla
    和 Gopher 等模型在其架构中不使用它。
- en: '![](../Images/a005e0d3460b7fb717c34dbbf3e6c013.png)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a005e0d3460b7fb717c34dbbf3e6c013.png)'
- en: 'image source: [here](https://arxiv.org/pdf/2305.13230.pdf)'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：[这里](https://arxiv.org/pdf/2305.13230.pdf)
- en: 'As described in the table below, the authors used for their experiments what
    are now considered almost small models. Thus, it is expensive to test different
    hyperparameters when designing an LLM:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 如下表所述，作者在实验中使用的模型现在被认为几乎是小型模型。因此，在设计大型语言模型（LLM）时，测试不同的超参数非常昂贵：
- en: For instance, in our specific scenario, training T5-XL five times would require
    approximately $37,000 USD for renting Google Cloud TPUs. Considering even larger
    models like PaLM and GPT-4, trained on even larger datasets, this cost becomes
    unmanageable ([source](https://arxiv.org/pdf/2305.13230.pdf))
  id: totrans-125
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 例如，在我们特定的场景中，训练 T5-XL 五次大约需要 $37,000 USD 来租用 Google Cloud TPUs。考虑到更大的模型如 PaLM
    和 GPT-4，在更大的数据集上训练，这个成本变得不可控（[来源](https://arxiv.org/pdf/2305.13230.pdf)）
- en: '![](../Images/eac5745319c497f6992c6d1f88ec8d78.png)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/eac5745319c497f6992c6d1f88ec8d78.png)'
- en: 'image source: [here](https://arxiv.org/pdf/2305.13230.pdf)'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：[这里](https://arxiv.org/pdf/2305.13230.pdf)
- en: Since in their experiments, a Sparse MoE model approximates the behavior of
    a dense model (which is more computationally expensive), one can use it to search
    for the best hyperparameters.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 由于在他们的实验中，稀疏 MoE 模型近似于密集模型（后者计算开销更大）的行为，因此可以使用它来搜索最佳超参数。
- en: 'For example, the authors show that one can test different learning rates for
    the MoE model and it exhibits the same performance as the equivalent dense model.
    So for the authors, one can test different hyperparameters with the MoE model
    and then train with the chosen parameters the dense model, thus saving cost:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，作者展示了可以测试MoE模型的不同学习率，并且它展现出与等效的密集模型相同的性能。因此，对作者来说，可以用MoE模型测试不同的超参数，然后用选择的参数训练密集模型，从而节省成本：
- en: sweeping the MoE Large model incurred an expenditure of approximately 10.6K
    USD on the Google Cloud Platform. Conversely, training the Dense XL model only
    once required 7.4K USD. Consequently, the entire development process, including
    sweeping, amounted to a total cost of 18K USD, which is only 0.48 times the expense
    of directly tuning the Dense XL model ([source](https://arxiv.org/pdf/2305.13230.pdf))
  id: totrans-130
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 对MoE大型模型的全面调整在Google Cloud Platform上花费了大约10.6K USD。相比之下，只训练一次Dense XL模型只需7.4K
    USD。因此，整个开发过程，包括调整，总成本达到了18K USD，这仅为直接调整Dense XL模型的费用的0.48倍 ([source](https://arxiv.org/pdf/2305.13230.pdf))
- en: '![](../Images/1d7936a9edc9c0c727a734e00b06da96.png)'
  id: totrans-131
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1d7936a9edc9c0c727a734e00b06da96.png)'
- en: 'image source: [here](https://arxiv.org/pdf/2305.13230.pdf)'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 图像来源：[here](https://arxiv.org/pdf/2305.13230.pdf)
- en: Parting thoughts
  id: totrans-133
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 思考总结
- en: In recent years there has been a race to have the biggest model. On the one
    hand, this race has been motivated by the fact that at a certain scale, properties
    were emerging that were impossible to predict with smaller models. On the other
    hand, the scaling law of OpenAI stated that performance is a function of the number
    of model parameters.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，出现了争夺最大模型的竞赛。一方面，这场竞赛的动机在于在某一规模下，会出现一些无法用更小模型预测的特性。另一方面，OpenAI的缩放定律指出，性能是模型参数数量的函数。
- en: In the past year this paradigm has come into crisis.
  id: totrans-135
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 在过去一年中，这一范式陷入了危机。
- en: Recently LlaMA has shown the importance of data quality. Also, Chinchilla showed
    a new rule for calculating the number of tokens needed to train a model optimally.
    In fact, a model of a certain number of parameters requires a number of data in
    order to perform optimally.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，LlaMA显示了数据质量的重要性。同时，Chinchilla展示了一个用于计算训练模型所需的标记数量的新规则。实际上，具有一定数量参数的模型需要相应的数据量才能达到最佳性能。
- en: Subsequent studies have shown that the number of quality tokens is not infinite.
    On the other hand, the number of model parameters grows more than how many tokens
    we humans can generate.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 随后的研究表明，优质标记的数量不是无限的。另一方面，模型参数的数量增长快于我们人类能够生成的标记数量。
- en: This led to the question of how we can solve the tokens crisis. Recent studies
    show that using LLM to generate tokens is not a viable way. This new work shows
    how using the same tokens for multiple epochs can actually deteriorate performance.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 这引出了如何解决标记危机的问题。最近的研究表明，使用LLM生成标记并不是一个可行的方法。这项新工作显示了在多个周期内使用相同标记实际上会降低性能。
- en: 'Work like this is important because although we are training and using LLM
    more and more, there are many even basic aspects that we do not know about. This
    work answers a question that seems basic but which the authors answer with experimental
    data: **what happens when training an LLM for multiple epochs?**'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 这样的工作很重要，因为尽管我们越来越多地训练和使用LLM，但仍有许多基本方面我们不了解。这项工作回答了一个看似基本的问题，但作者通过实验数据给出了答案：**训练LLM多个时期会发生什么？**
- en: Moreover, this article is part of a growing slice of literature that shows how
    an uncritical increase in the number of parameters is unnecessary. On the other
    hand, bigger and bigger models are more and more expensive and also consume more
    and more electricity. Considering that [we need to optimize resources](/how-ai-could-fuel-global-warming-8f6e1dda6711),
    this article suggests that training a huge model without enough data is just a
    waste.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，本文是不断增长的文献的一部分，这些文献展示了不加批判地增加参数数量是多么不必要。另一方面，越来越大的模型变得越来越昂贵，同时也消耗越来越多的电力。考虑到[我们需要优化资源](/how-ai-could-fuel-global-warming-8f6e1dda6711)，本文建议，在没有足够数据的情况下训练一个巨大的模型只是浪费。
- en: This article still shows how we need new architectures that can replace the
    transformer. So it is time to focus research on new ideas instead of continuing
    to scale models.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 本文仍然展示了我们需要新的架构来替代transformer。因此，是时候将研究重点放在新想法上，而不是继续扩大模型规模。
- en: 'If you have found this interesting:'
  id: totrans-142
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如果你觉得这很有趣：
- en: '*You can look for my other articles, you can also* [***subscribe***](https://salvatore-raieli.medium.com/subscribe)
    *to get notified when I publish articles, you can* [***become a Medium member***](https://medium.com/@salvatore-raieli/membership)
    *to access all its stories (affiliate links of the platform for which I get small
    revenues without cost to you) and you can also connect or reach me on*[***LinkedIn***](https://www.linkedin.com/in/salvatore-raieli/)***.***'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '*你可以查看我的其他文章，你还可以* [***订阅***](https://salvatore-raieli.medium.com/subscribe)
    *以便在我发布新文章时获得通知，你还可以* [***成为 Medium 会员***](https://medium.com/@salvatore-raieli/membership)
    *访问所有故事（这些是平台的推广链接，我从中获得少量收入，不会对你产生额外费用），你还可以通过*[***LinkedIn***](https://www.linkedin.com/in/salvatore-raieli/)***与我联系或找到我。***'
- en: '*Here is the link to my GitHub repository, where I am planning to collect code
    and many resources related to machine learning, artificial intelligence, and more.*'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '*这是我 GitHub 仓库的链接，我计划在这里收集与机器学习、人工智能等相关的代码和许多资源。*'
- en: '[](https://github.com/SalvatoreRa/tutorial?source=post_page-----58f38035f66e--------------------------------)
    [## GitHub - SalvatoreRa/tutorial: Tutorials on machine learning, artificial intelligence,
    data science…'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '[## GitHub - SalvatoreRa/tutorial：机器学习、人工智能、数据科学的教程…](https://github.com/SalvatoreRa/tutorial?source=post_page-----58f38035f66e--------------------------------)
    [## GitHub - SalvatoreRa/tutorial：关于机器学习、人工智能、数据科学的教程…'
- en: Tutorials on machine learning, artificial intelligence, data science with math
    explanation and reusable code (in python…
  id: totrans-146
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 关于机器学习、人工智能、数据科学的教程，包括数学解释和可重用代码（用 Python 编写…
- en: github.com](https://github.com/SalvatoreRa/tutorial?source=post_page-----58f38035f66e--------------------------------)
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '[## GitHub - SalvatoreRa/tutorial：机器学习、人工智能、数据科学的教程…](https://github.com/SalvatoreRa/tutorial?source=post_page-----58f38035f66e--------------------------------)'
- en: '*or you may be interested in one of my recent articles:*'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '*或者你可能对我最近的一篇文章感兴趣：*'
- en: '[](https://salvatore-raieli.medium.com/scaling-isnt-everything-how-bigger-models-fail-harder-d64589be4f04?source=post_page-----58f38035f66e--------------------------------)
    [## Scaling Isn’t Everything: How Bigger Models Fail Harder'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '[## 扩展并非一切：更大的模型为何失败得更惨](https://salvatore-raieli.medium.com/scaling-isnt-everything-how-bigger-models-fail-harder-d64589be4f04?source=post_page-----58f38035f66e--------------------------------)
    [## 扩展并非一切：更大的模型为何失败得更惨'
- en: Are Large Language Models really understanding programming languages?
  id: totrans-150
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 大型语言模型真的能理解编程语言吗？
- en: 'salvatore-raieli.medium.com](https://salvatore-raieli.medium.com/scaling-isnt-everything-how-bigger-models-fail-harder-d64589be4f04?source=post_page-----58f38035f66e--------------------------------)
    [](https://levelup.gitconnected.com/metas-lima-maria-kondo-s-way-for-llms-training-8411e3907fed?source=post_page-----58f38035f66e--------------------------------)
    [## META’S LIMA: Maria Kondo’s way for LLMs training'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '[## META 的 LIMA：玛丽亚·近藤的 LLM 训练方式](https://salvatore-raieli.medium.com/scaling-isnt-everything-how-bigger-models-fail-harder-d64589be4f04?source=post_page-----58f38035f66e--------------------------------)
    [](https://levelup.gitconnected.com/metas-lima-maria-kondo-s-way-for-llms-training-8411e3907fed?source=post_page-----58f38035f66e--------------------------------)
    [## META’S LIMA：玛丽亚·近藤的 LLM 训练方式'
- en: Less and tidy data to create a model capable to rival ChatGPT
  id: totrans-152
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更少而整洁的数据来创建一个能够与 ChatGPT 竞争的模型
- en: 'levelup.gitconnected.com](https://levelup.gitconnected.com/metas-lima-maria-kondo-s-way-for-llms-training-8411e3907fed?source=post_page-----58f38035f66e--------------------------------)
    [](https://levelup.gitconnected.com/google-med-palm-2-is-ai-ready-for-medical-residency-e37907115bd0?source=post_page-----58f38035f66e--------------------------------)
    [## Google Med-PaLM 2: is AI ready for medical residency?'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: '[## META 的 LIMA：玛丽亚·近藤的 LLM 训练方式](https://levelup.gitconnected.com/metas-lima-maria-kondo-s-way-for-llms-training-8411e3907fed?source=post_page-----58f38035f66e--------------------------------)
    [](https://levelup.gitconnected.com/google-med-palm-2-is-ai-ready-for-medical-residency-e37907115bd0?source=post_page-----58f38035f66e--------------------------------)
    [## 谷歌 Med-PaLM 2：AI 是否准备好进入医学住院医生培训？'
- en: Google's new model achieves impressive results in the medical domain
  id: totrans-154
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 谷歌的新模型在医学领域取得了令人印象深刻的成果
- en: 'levelup.gitconnected.com](https://levelup.gitconnected.com/google-med-palm-2-is-ai-ready-for-medical-residency-e37907115bd0?source=post_page-----58f38035f66e--------------------------------)
    [](https://levelup.gitconnected.com/to-ai-or-not-to-ai-how-to-survive-f5e853aebd5b?source=post_page-----58f38035f66e--------------------------------)
    [## To AI or not to AI: how to survive?'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '[## 谷歌 Med-PaLM 2：AI 是否准备好进入医学住院医生培训？](https://levelup.gitconnected.com/google-med-palm-2-is-ai-ready-for-medical-residency-e37907115bd0?source=post_page-----58f38035f66e--------------------------------)
    [](https://levelup.gitconnected.com/to-ai-or-not-to-ai-how-to-survive-f5e853aebd5b?source=post_page-----58f38035f66e--------------------------------)
    [## AI 还是非 AI：如何生存？'
- en: With generative AI threatening businesses and side hustles, how you can find
    space?
  id: totrans-156
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 随着生成性 AI 对企业和副业的威胁，你如何找到自己的空间？
- en: levelup.gitconnected.com](https://levelup.gitconnected.com/to-ai-or-not-to-ai-how-to-survive-f5e853aebd5b?source=post_page-----58f38035f66e--------------------------------)
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '[levelup.gitconnected.com](https://levelup.gitconnected.com/to-ai-or-not-to-ai-how-to-survive-f5e853aebd5b?source=post_page-----58f38035f66e--------------------------------)'
- en: References
  id: totrans-158
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'A list of the principal references consulted for this article:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 本文参考的主要文献列表：
- en: 'Fuzhao Xue et al, 2023, To Repeat or Not To Repeat: Insights from Scaling LLM
    under Token-Crisis, [link](https://arxiv.org/abs/2305.13230)'
  id: totrans-160
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Fuzhao Xue 等，2023，《重复还是不重复：在令牌危机下扩展 LLM 的见解》，[链接](https://arxiv.org/abs/2305.13230)
- en: 'Hugo Touvron et all. 2023, LLaMA: Open and Efficient Foundation Language Models.
    [link](https://arxiv.org/abs/2302.13971)'
  id: totrans-161
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Hugo Touvron 等，2023，《LLaMA：开放且高效的基础语言模型》。 [链接](https://arxiv.org/abs/2302.13971)
- en: Arnav Gudibande et all, 2023, The False Promise of Imitating Proprietary LLMs.
    [link](https://arxiv.org/abs/2305.15717)
  id: totrans-162
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Arnav Gudibande 等，2023，《模仿专有 LLM 的虚假承诺》。 [链接](https://arxiv.org/abs/2305.15717)
- en: PaLM 2, google blog, [link](https://ai.google/discover/palm2/)
  id: totrans-163
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: PaLM 2，谷歌博客，[链接](https://ai.google/discover/palm2/)
- en: 'Pathways Language Model (PaLM): Scaling to 540 Billion Parameters for Breakthrough
    Performance. Google Blog, [link](https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.html)'
  id: totrans-164
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Pathways Language Model (PaLM)：扩展至540亿参数以实现突破性性能。谷歌博客，[链接](https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.html)
- en: Buck Shlegeris et all, 2022, Language models are better than humans at next-token
    prediction, [link](https://arxiv.org/abs/2212.11281)
  id: totrans-165
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Buck Shlegeris 等，2022，《语言模型在下一个令牌预测上优于人类》，[链接](https://arxiv.org/abs/2212.11281)
- en: Pablo Villalobos et. all, 2022, Will we run out of data? An analysis of the
    limits of scaling datasets in Machine Learning. [link](https://arxiv.org/abs/2211.04325)
  id: totrans-166
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Pablo Villalobos 等，2022，《我们会用完数据吗？对机器学习中数据集扩展极限的分析》。 [链接](https://arxiv.org/abs/2211.04325)
- en: 'Susan Zhang et al. 2022, OPT: Open Pre-trained Transformer Language Models.
    [link](https://arxiv.org/abs/2205.01068)'
  id: totrans-167
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Susan Zhang 等，2022，《OPT：开放预训练变换器语言模型》。 [链接](https://arxiv.org/abs/2205.01068)
- en: Jordan Hoffmann et all, 2022, An empirical analysis of compute-optimal large
    language model training. [link](https://arxiv.org/abs/2203.15556)
  id: totrans-168
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Jordan Hoffmann 等，2022，《计算最优大型语言模型训练的实证分析》。 [链接](https://arxiv.org/abs/2203.15556)
- en: 'Ross Taylor et al, 2022, Galactica: A Large Language Model for Science, [link](https://arxiv.org/abs/2211.09085)'
  id: totrans-169
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Ross Taylor 等，2022，《Galactica：一种用于科学的大型语言模型》，[链接](https://arxiv.org/abs/2211.09085)
- en: Zixiang Chen et al, 2022, Towards Understanding Mixture of Experts in Deep Learning,
    [link](https://arxiv.org/abs/2208.02813)
  id: totrans-170
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Zixiang Chen 等，2022，《朝着理解深度学习中的专家混合模型前进》，[链接](https://arxiv.org/abs/2208.02813)
- en: Jared Kaplan et all, 2020, Scaling Laws for Neural Language Models. [link](https://arxiv.org/abs/2001.08361)
  id: totrans-171
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Jared Kaplan 等，2020，《神经语言模型的规模定律》。 [链接](https://arxiv.org/abs/2001.08361)
- en: How AI could fuel global warming, TDS, [link](/how-ai-could-fuel-global-warming-8f6e1dda6711)
  id: totrans-172
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 人工智能如何助长全球变暖，TDS，[链接](/how-ai-could-fuel-global-warming-8f6e1dda6711)
- en: Masked language modeling, HuggingFace blog, [link](https://huggingface.co/docs/transformers/main/tasks/masked_language_modeling)
  id: totrans-173
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 掩码语言建模，HuggingFace 博客，[链接](https://huggingface.co/docs/transformers/main/tasks/masked_language_modeling)
- en: Mixture-of-Experts with Expert Choice Routing, Google Blog, [link](https://ai.googleblog.com/2022/11/mixture-of-experts-with-expert-choice.html)
  id: totrans-174
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 专家混合模型与专家选择路由，谷歌博客，[链接](https://ai.googleblog.com/2022/11/mixture-of-experts-with-expert-choice.html)
- en: Why Meta’s latest large language model survived only three days online, MIT
    review, [link](https://www.technologyreview.com/2022/11/18/1063487/meta-large-language-model-ai-only-survived-three-days-gpt-3-science/)
  id: totrans-175
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为什么 Meta 最新的大型语言模型在线仅存活了三天，MIT 评审，[链接](https://www.technologyreview.com/2022/11/18/1063487/meta-large-language-model-ai-only-survived-three-days-gpt-3-science/)
- en: 'Exploring Transfer Learning with T5: the Text-To-Text Transfer Transformer,
    Google Blog, [link](https://ai.googleblog.com/2020/02/exploring-transfer-learning-with-t5.html)'
  id: totrans-176
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 探索使用 T5 的迁移学习：文本到文本的迁移变换器，谷歌博客，[链接](https://ai.googleblog.com/2020/02/exploring-transfer-learning-with-t5.html)
- en: Scaling laws for reward model overoptimization, OpenAI blog, [link](https://openai.com/research/scaling-laws-for-reward-model-overoptimization)
  id: totrans-177
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 奖励模型过度优化的规模定律，OpenAI 博客，[链接](https://openai.com/research/scaling-laws-for-reward-model-overoptimization)
- en: An empirical analysis of compute-optimal large language model training, DeepMind
    blog, [link](https://www.deepmind.com/blog/an-empirical-analysis-of-compute-optimal-large-language-model-training)
  id: totrans-178
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算最优大型语言模型训练的实证分析，DeepMind 博客，[链接](https://www.deepmind.com/blog/an-empirical-analysis-of-compute-optimal-large-language-model-training)
- en: 'Xiaonan Nie et al, 2022, EvoMoE: An Evolutional Mixture-of-Experts Training
    Framework via Dense-To-Sparse Gate. [link](https://arxiv.org/abs/2112.14397)'
  id: totrans-179
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Xiaonan Nie 等, 2022, **EvoMoE: 一种通过稠密到稀疏门控的进化专家混合模型训练框架**。 [link](https://arxiv.org/abs/2112.14397)'
- en: Tianyu Chen et al, 2022, Task-Specific Expert Pruning for Sparse Mixture-of-Experts,
    [link](https://arxiv.org/abs/2206.00277)
  id: totrans-180
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Tianyu Chen 等, 2022, **任务特定专家剪枝用于稀疏专家混合模型**, [link](https://arxiv.org/abs/2206.00277)
- en: Bo Li et al, 2022, Sparse Mixture-of-Experts are Domain Generalizable Learners,
    [link](https://arxiv.org/abs/2206.04046)
  id: totrans-181
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Bo Li 等, 2022, **稀疏专家混合模型是领域通用的学习者**, [link](https://arxiv.org/abs/2206.04046)
