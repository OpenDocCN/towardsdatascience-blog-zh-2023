- en: 'Anatomy of LLM-Based Chatbot Applications: Monolithic vs. Microservice Architectural
    Patterns'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于LLM的聊天机器人应用程序的结构：单体架构与微服务架构模式
- en: 原文：[https://towardsdatascience.com/anatomy-of-llm-based-chatbot-applications-monolithic-vs-microservice-architectural-patterns-77796216903e](https://towardsdatascience.com/anatomy-of-llm-based-chatbot-applications-monolithic-vs-microservice-architectural-patterns-77796216903e)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/anatomy-of-llm-based-chatbot-applications-monolithic-vs-microservice-architectural-patterns-77796216903e](https://towardsdatascience.com/anatomy-of-llm-based-chatbot-applications-monolithic-vs-microservice-architectural-patterns-77796216903e)
- en: A Practical Guide to Building Monolithic and Microservice Chatbot Applications
    with Streamlit, Huggingface, and FastAPI
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实用指南：使用Streamlit、Huggingface和FastAPI构建单体和微服务聊天机器人应用程序
- en: '[](https://stephen-leo.medium.com/?source=post_page-----77796216903e--------------------------------)[![Marie
    Stephen Leo](../Images/c5669d884da5ff5c965f98904257d379.png)](https://stephen-leo.medium.com/?source=post_page-----77796216903e--------------------------------)[](https://towardsdatascience.com/?source=post_page-----77796216903e--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----77796216903e--------------------------------)
    [Marie Stephen Leo](https://stephen-leo.medium.com/?source=post_page-----77796216903e--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://stephen-leo.medium.com/?source=post_page-----77796216903e--------------------------------)[![Marie
    Stephen Leo](../Images/c5669d884da5ff5c965f98904257d379.png)](https://stephen-leo.medium.com/?source=post_page-----77796216903e--------------------------------)[](https://towardsdatascience.com/?source=post_page-----77796216903e--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----77796216903e--------------------------------)
    [Marie Stephen Leo](https://stephen-leo.medium.com/?source=post_page-----77796216903e--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----77796216903e--------------------------------)
    ·9 min read·May 8, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page-----77796216903e--------------------------------)
    ·9分钟阅读·2023年5月8日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/5b4d2660c860f9c8391e33823a5f1824.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5b4d2660c860f9c8391e33823a5f1824.png)'
- en: 'Image generated by Author using Midjourney V5.1 using the prompt: “isometric
    highly realistic view of a laptop, screen has the picture of a bright, multi colored
    rubik’s cube that is illuminated from within, bright, warm, cheerful lighting.
    8k, hdr, unreal engine”'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者使用Midjourney V5.1生成，提示词为：“等距的高现实感笔记本电脑视图，屏幕上显示一个明亮的、多彩的魔方，内部被照亮，明亮、温暖、愉快的光线。8k，hdr，虚幻引擎”
- en: With the advent of OpenAI’s ChatGPT, chatbots are exploding in popularity! Every
    business seeks ways to incorporate ChatGPT into its customer-facing and internal
    applications. Further, with open-source chatbots catching up so rapidly that even
    [Google engineers seem to conclude they and OpenAI have “no moat,”](https://www.semianalysis.com/p/google-we-have-no-moat-and-neither)
    there’s never been a better time to be in the AI industry!
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 随着OpenAI的ChatGPT的出现，聊天机器人正在迅速流行！每个企业都在寻求将ChatGPT融入其面向客户和内部的应用程序中。此外，由于开源聊天机器人进展如此迅速，以至于即使[谷歌工程师似乎也得出结论认为他们和OpenAI“没有护城河”，](https://www.semianalysis.com/p/google-we-have-no-moat-and-neither)
    现在正是进入AI行业的最佳时机！
- en: As a Data Scientist building such an application, one of the critical decisions
    is choosing between a monolithic and microservices architecture. Both architectures
    have pros and cons; ultimately, the choice depends on the business’s needs, such
    as scalability and ease of integration with existing systems. In this blog post,
    we will explore the differences between these two architectures with live code
    examples using Streamlit, Huggingface, and FastAPI!
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 作为构建这种应用程序的数据科学家，关键的决策之一是选择单体架构还是微服务架构。这两种架构各有优缺点；最终的选择取决于业务的需求，例如可扩展性和与现有系统的集成方便性。在这篇博客文章中，我们将使用Streamlit、Huggingface和FastAPI的实时代码示例来探讨这两种架构之间的区别！
- en: First, create a new conda environment and install the necessary libraries.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，创建一个新的conda环境并安装必要的库。
- en: '[PRE0]'
  id: totrans-11
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Monolithic architecture
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 单体架构
- en: '![](../Images/3c56f65e305be824fdbe283044fd1416.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3c56f65e305be824fdbe283044fd1416.png)'
- en: In a monolithic application, all the code related to the application is tightly
    coupled in a single, self-contained unit. Image by Author
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在单体应用程序中，与应用程序相关的所有代码都紧密耦合在一个独立的单元中。 图片由作者提供
- en: Monolithic architecture is an approach that involves building the entire application
    as a single, self-contained unit. This approach is simple and easy to develop
    but can become complex as the application grows. All application components, including
    the user interface, business logic, and data storage, are tightly coupled in a
    monolithic architecture. Any changes made to one part of the app can ripple effect
    on the entire application.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 单体架构是一种将整个应用程序构建为一个自包含的单元的方法。这种方法简单且易于开发，但随着应用程序的增长，可能会变得复杂。在单体架构中，包括用户界面、业务逻辑和数据存储在内的所有应用程序组件都是紧密耦合的。对应用程序某一部分所做的任何更改都可能对整个应用程序产生连锁反应。
- en: Let’s use Huggingface and Streamlit to build a monolithic chatbot application
    below. We’ll use Streamlit to build the frontend user interface, while Huggingface
    provides an extremely easy-to-use, high-level abstraction to various open-source
    LLM models called [pipelines](https://huggingface.co/docs/transformers/main_classes/pipelines#natural-language-processing).
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 下面我们使用 Huggingface 和 Streamlit 来构建一个单体聊天机器人应用程序。我们将使用 Streamlit 来构建前端用户界面，而
    Huggingface 提供了一种非常易于使用的高层抽象，称为 [pipelines](https://huggingface.co/docs/transformers/main_classes/pipelines#natural-language-processing)，它可以访问各种开源
    LLM 模型。
- en: First, let’s create a file utils.py containing three helper functions common
    to the front end in monolithic and microservices architectures.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们创建一个包含三个在单体和微服务架构的前端中常用的助手函数的文件 utils.py。
- en: '`clear_conversation()`: This function deletes all the stored session_state
    variables in the Streamlit frontend. We use it to clear the entire chat history
    and start a new chat thread.'
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`clear_conversation()`: 此函数删除 Streamlit 前端中所有存储的 session_state 变量。我们用它来清除整个聊天记录并开始一个新的聊天线程。'
- en: '`display_conversation()`: This function uses the streamlit_chat library to
    create a beautiful chat interface frontend with our entire chat thread displayed
    on the screen from the latest to the oldest message. Since the Huggingface pipelines
    API stores user_inputs and generate_responses in separate lists, we also use this
    function to create a single interleaved_conversation list that contains the entire
    chat thread so we can download it if needed.'
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`display_conversation()`: 此函数使用 streamlit_chat 库创建一个漂亮的聊天界面前端，将我们的整个聊天线程从最新到最旧的消息显示在屏幕上。由于
    Huggingface pipelines API 将 user_inputs 和 generate_responses 存储在不同的列表中，我们还使用此函数创建一个包含整个聊天线程的单一
    interleaved_conversation 列表，以便在需要时可以下载它。'
- en: '`download_conversation()`: This function converts the whole chat thread to
    a pandas dataframe and downloads it as a csv file to your local computer.'
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`download_conversation()`: 此函数将整个聊天线程转换为 pandas dataframe 并下载为 csv 文件到您的本地计算机。'
- en: '[PRE1]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Next, let’s create a single monolith.py file containing our entire monolithic
    application.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们创建一个包含整个单体应用程序的 monolith.py 文件。
- en: OpenAI’s ChatGPT API costs money for every token in both the question and response.
    Hence for this small demo, I chose to use an open-source model from Huggingface
    called “facebook/blenderbot-400M-distill”. You can find the entire list of over
    2000 open-source models trained for the conversational task at the [Huggingface
    model hub](https://huggingface.co/models?pipeline_tag=conversational). For more
    details on the conversational task pipeline, refer to [Huggingface’s official
    documentation](https://huggingface.co/docs/transformers/v4.28.1/en/main_classes/pipelines#transformers.Conversation).
    When open-source models inevitably catch up to the proprietary models from OpenAI
    and Google, I’m sure Huggingface will be THE platform for researchers to share
    those models, given how much they’ve revolutionized the field of NLP over the
    past few years!
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: OpenAI 的 ChatGPT API 对问题和回答中的每个令牌都收费。因此，在这个小演示中，我选择使用 Huggingface 的一个开源模型，名为“facebook/blenderbot-400M-distill”。您可以在
    [Huggingface 模型中心](https://huggingface.co/models?pipeline_tag=conversational)
    找到超过 2000 个经过对话任务训练的开源模型的完整列表。有关对话任务 pipeline 的更多细节，请参考 [Huggingface 官方文档](https://huggingface.co/docs/transformers/v4.28.1/en/main_classes/pipelines#transformers.Conversation)。当开源模型不可避免地赶上
    OpenAI 和 Google 的专有模型时，我相信 Huggingface 将成为研究人员分享这些模型的平台，考虑到他们在过去几年中如何彻底改变了 NLP
    领域！
- en: '`main()`: This function builds the frontend app’s layout using Streamlit. We’ll
    have a button to clear the conversation and one to download. We’ll also have a
    text box where the user can type their question, and upon pressing enter, we’ll
    call the `monolith_llm_response` function with the user’s input. Finally, we’ll
    display the entire conversation on the front end using the `display_conversation`
    function from utils.'
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`main()`: 此函数使用 Streamlit 构建前端应用的布局。我们将有一个按钮来清除对话，另一个按钮来下载。我们还会有一个文本框，用户可以在其中输入问题，按下回车后，我们将调用
    `monolith_llm_response` 函数处理用户的输入。最后，我们将使用来自 `utils` 的 `display_conversation`
    函数在前端显示整个对话。'
- en: '`monolith_llm_response()`: This function is responsible for the chatbot logic
    using Huggingface pipelines. First, we create a new Conversation object and initialize
    it to the entire conversation history up to that point. Then, we add the latest
    user_input to that object, and finally, we pass this conversation object to the
    Huggingface pipeline that we created two steps back. Huggingface automatically
    adds the user input and response generated to the conversation history!'
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`monolith_llm_response()`: 此函数负责使用 Huggingface 管道处理聊天机器人逻辑。首先，我们创建一个新的 `Conversation`
    对象，并将其初始化为到目前为止的整个对话历史。然后，我们将最新的 `user_input` 添加到该对象中，最后，我们将此对话对象传递给我们之前创建的 Huggingface
    管道。Huggingface 会自动将用户输入和生成的响应添加到对话历史中！'
- en: '[PRE2]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: That’s it! We can run this monolithic application by running `streamlit run
    monolith.py` and interacting with the application on a web browser! We could quickly
    deploy this application as such to a cloud service like Google Cloud Run, as described
    in [my previous blog post,](https://medium.com/towards-artificial-intelligence/make-extra-money-on-the-side-with-data-science-984a623c53f5)
    and interact with it over the internet too!
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 就这些了！我们可以通过运行 `streamlit run monolith.py` 来运行这个单体应用，并在网页浏览器上与应用进行交互！我们还可以将此应用快速部署到像
    Google Cloud Run 这样的云服务中，正如 [我之前的博客文章](https://medium.com/towards-artificial-intelligence/make-extra-money-on-the-side-with-data-science-984a623c53f5)
    中所描述的那样，并通过互联网进行交互！
- en: '![](../Images/a7252eb9c1b0038f1c6afa067858db11.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a7252eb9c1b0038f1c6afa067858db11.png)'
- en: Monolithic Streamlit App interface. Image by Author
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 单体 Streamlit 应用界面。图片由作者提供。
- en: Microservices architecture
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 微服务架构
- en: '![](../Images/66daf7b302656ef96086a711dee6949f.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/66daf7b302656ef96086a711dee6949f.png)'
- en: In a microservices application, each component is split up into its own smaller,
    independent service. Image by Author
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在微服务应用中，每个组件被拆分为自己的较小的独立服务。图片由作者提供。
- en: Microservices architecture is an approach that involves breaking down the application
    into smaller, independent services. Each application component, such as the user
    interface, business logic, and data storage, is developed and deployed independently.
    This approach offers flexibility and scalability as we can modularly add more
    capabilities and horizontally scale each service independently of others by adding
    more instances.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 微服务架构是一种将应用程序拆分成更小的独立服务的方法。每个应用程序组件，如用户界面、业务逻辑和数据存储，都是独立开发和部署的。这种方法提供了灵活性和可扩展性，因为我们可以模块化地添加更多功能，并通过添加更多实例来水平扩展每个服务，而不会影响其他服务。
- en: Let’s split the Huggingface model inference from our monolithic app into a separate
    microservice using FastAPI and the Streamlit frontend into another microservice
    below. Since the backend in this demo only has the LLM model, our backend API
    server is the same as the LLM model from the picture above. We can directly re-use
    the utils.py file we created above in the frontend microservice!
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用 FastAPI 将 Huggingface 模型推理从我们的单体应用中拆分为一个单独的微服务，并将 Streamlit 前端拆分为另一个微服务。由于这个演示中的后端仅有
    LLM 模型，我们的后端 API 服务器与上图中的 LLM 模型相同。我们可以直接在前端微服务中重用我们之前创建的 `utils.py` 文件！
- en: First, let’s create a backend.py file that will serve as our FastAPI microservice
    that runs the Huggingface pipeline inference.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们创建一个 `backend.py` 文件，这将作为我们的 FastAPI 微服务，运行 Huggingface 管道推理。
- en: We first create the pipeline object with the same model that we chose earlier,
    “facebook/blenderbot-400M-distill”
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们首先使用之前选择的相同模型，“facebook/blenderbot-400M-distill”创建管道对象。
- en: We then create a ConversationHistory Pydantic model so that we can receive the
    inputs required for the pipeline as a payload to the FastAPI service. For more
    information on the FastAPI request body, please look at [the FastAPI documentation](https://fastapi.tiangolo.com/tutorial/body/).
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们创建了一个 `ConversationHistory` Pydantic 模型，以便将管道所需的输入作为负载传递给 FastAPI 服务。如需更多有关
    FastAPI 请求主体的信息，请参见 [FastAPI 文档](https://fastapi.tiangolo.com/tutorial/body/)。
- en: It’s a good practice to reserve the root route in APIs for a health check. So
    we define that route first.
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为 APIs 保留根路由用于健康检查是一个好习惯。所以我们首先定义这个路由。
- en: Finally, we define a route called `/chat`, which accepts the API payload as
    a ConversationHistory object and converts it to a dictionary. Then we create a
    new Conversation object and initialize it with the conversation history received
    in the payload. Next, we add the latest user_input to that object and pass this
    conversation object to the Huggingface pipeline. Finally, we must return the latest
    generated response to the front end.
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们定义一个名为 `/chat` 的路由，该路由接受 API 有效负载作为 ConversationHistory 对象，并将其转换为字典。然后我们创建一个新的
    Conversation 对象，并用有效负载中接收到的对话历史进行初始化。接着，我们将最新的 user_input 添加到该对象中，并将这个对话对象传递给
    Huggingface pipeline。最后，我们必须将最新生成的响应返回给前端。
- en: '[PRE3]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: We can run this FastAPI app locally using `uvicorn backend:app --reload`, or
    deploy it to a cloud service like Google Cloud Run, as described in [my previous
    blog post,](https://medium.com/towards-artificial-intelligence/make-extra-money-on-the-side-with-data-science-984a623c53f5)
    and interact with it over the internet! You can test the backend using the API
    docs that FastAPI automatically generates at the `/docs` route by navigating to
    [http://127.0.0.1:8000/docs](http://127.0.0.1:8000/docs).
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用 `uvicorn backend:app --reload` 本地运行这个 FastAPI 应用，也可以将其部署到 Google Cloud
    Run 等云服务中，如[我之前的博客文章](https://medium.com/towards-artificial-intelligence/make-extra-money-on-the-side-with-data-science-984a623c53f5)中所述，并通过互联网进行交互！你可以通过访问
    [http://127.0.0.1:8000/docs](http://127.0.0.1:8000/docs) 来使用 FastAPI 自动生成的 `/docs`
    路由中的 API 文档测试后端。
- en: '![](../Images/beb421a5268c38e1ed6301ea58c47aef.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/beb421a5268c38e1ed6301ea58c47aef.png)'
- en: FastAPI docs for the backend. Image by Author
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 后端的 FastAPI 文档。图片由作者提供
- en: Finally, let’s create a frontend.py file that contains the frontend code.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们创建一个包含前端代码的 `frontend.py` 文件。
- en: '`main()`: This function is precisely similar to `main()` in the monolithic
    application, except for one change that we call the `microservice_llm_response()`
    function when the user enters any input.'
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`main()`：这个函数与单体应用中的 `main()` 函数非常相似，唯一的变化是当用户输入任何内容时，我们调用 `microservice_llm_response()`
    函数。'
- en: '`microservice_llm_response()`: Since we split out the LLM logic into a separate
    FastAPI microservice, this function uses the conversation history stored in the
    session_state to post a request to the backend FastAPI service and then appends
    both the user’s input and the response from the FastAPI backend to the conversation
    history to continue the memory of the entire chat thread.'
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`microservice_llm_response()`：由于我们将 LLM 逻辑拆分到一个单独的 FastAPI 微服务中，这个函数利用存储在 session_state
    中的对话历史记录，向后端 FastAPI 服务发送请求，然后将用户输入和 FastAPI 后端的响应都附加到对话历史中，以便继续记住整个聊天线程。'
- en: '[PRE4]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: That’s it! We can run this frontend application by running `streamlit run frontend.py`
    and interacting with the application on a web browser! As my [previous blog post](https://medium.com/towards-artificial-intelligence/make-extra-money-on-the-side-with-data-science-984a623c53f5)
    described, we could quickly deploy to a cloud service like Google Cloud Run and
    interact with it over the internet too!
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 就这样！我们可以通过运行 `streamlit run frontend.py` 来启动这个前端应用，并在网页浏览器上与应用进行交互！正如我的[上一篇博客文章](https://medium.com/towards-artificial-intelligence/make-extra-money-on-the-side-with-data-science-984a623c53f5)中所描述的，我们也可以快速部署到云服务如
    Google Cloud Run，并通过互联网与之互动！
- en: Which architecture to choose?
  id: totrans-49
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 选择哪个架构？
- en: The answer depends on the requirements of your application. A monolithic architecture
    can be a great starting point for a Data Scientist to build an initial proof-of-concept
    quickly and get it in front of business stakeholders. But, inevitably, if you
    plan to productionize the application, a microservices architecture is generally
    a better bet over a monolithic one because it allows for more flexibility and
    scalability and allows different specialized developers to focus on building the
    various components. For example, a frontend developer might use React to build
    the frontend, a Data Engineer might use Airflow to write the data pipelines, and
    an ML engineer might use [FastAPI](https://fastapi.tiangolo.com/) or [BentoML](https://github.com/bentoml/BentoML)
    to deploy the model serving API with custom business logic.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 答案取决于你应用的需求。单体架构可以是数据科学家快速构建初步概念验证并向业务干系人展示的一个很好的起点。但不可避免地，如果你计划将应用投入生产，微服务架构通常比单体架构更好，因为它提供了更多的灵活性和可扩展性，并允许不同的专业开发人员专注于构建各种组件。例如，前端开发人员可能会使用
    React 构建前端，数据工程师可能会使用 Airflow 编写数据管道，而 ML 工程师可能会使用 [FastAPI](https://fastapi.tiangolo.com/)
    或 [BentoML](https://github.com/bentoml/BentoML) 来部署具有自定义业务逻辑的模型服务 API。
- en: Additionally, with microservices, chatbot developers can easily incorporate
    new features or change existing ones without affecting the entire application.
    This level of flexibility and scalability is crucial for businesses that want
    to integrate the chatbot into existing applications. Dedicated UI/UX, data engineers,
    data scientists, and ML engineers can each focus on their areas of expertise to
    deliver a polished product!
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，通过微服务，聊天机器人开发人员可以轻松地加入新功能或更改现有功能，而不会影响整个应用程序。这种灵活性和可扩展性对那些希望将聊天机器人集成到现有应用中的企业至关重要。专注的
    UI/UX、数据工程师、数据科学家和 ML 工程师可以各自专注于他们的专业领域，以交付一个精致的产品！
- en: Conclusion
  id: totrans-52
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: In conclusion, monolithic and microservices architectures have pros and cons,
    and the choice between the two depends on the business’s specific needs. However,
    I prefer microservices architecture for chatbot applications due to its flexibility,
    scalability, and the fact that I can delegate frontend development to more qualified
    UI/UX folk 🤩.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，单体架构和微服务架构各有优缺点，两者之间的选择取决于业务的具体需求。然而，我更倾向于微服务架构用于聊天机器人应用，因为它的灵活性、可扩展性，以及我可以将前端开发委托给更合格的
    UI/UX 专家 🤩。
