- en: Training an Agent to Master a Simple Game Through Self-Play
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通过自我对弈训练一个代理以掌握简单游戏
- en: 原文：[https://towardsdatascience.com/training-an-agent-to-master-a-simple-game-through-self-play-88bdd0d60928?source=collection_archive---------5-----------------------#2023-09-06](https://towardsdatascience.com/training-an-agent-to-master-a-simple-game-through-self-play-88bdd0d60928?source=collection_archive---------5-----------------------#2023-09-06)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/training-an-agent-to-master-a-simple-game-through-self-play-88bdd0d60928?source=collection_archive---------5-----------------------#2023-09-06](https://towardsdatascience.com/training-an-agent-to-master-a-simple-game-through-self-play-88bdd0d60928?source=collection_archive---------5-----------------------#2023-09-06)
- en: Simulate games and predict the outcomes.
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模拟游戏并预测结果。
- en: '[](https://sebastiengilbert.medium.com/?source=post_page-----88bdd0d60928--------------------------------)[![Sébastien
    Gilbert](../Images/380f6588c3ef718947bcf82061f190eb.png)](https://sebastiengilbert.medium.com/?source=post_page-----88bdd0d60928--------------------------------)[](https://towardsdatascience.com/?source=post_page-----88bdd0d60928--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----88bdd0d60928--------------------------------)
    [Sébastien Gilbert](https://sebastiengilbert.medium.com/?source=post_page-----88bdd0d60928--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://sebastiengilbert.medium.com/?source=post_page-----88bdd0d60928--------------------------------)[![Sébastien
    Gilbert](../Images/380f6588c3ef718947bcf82061f190eb.png)](https://sebastiengilbert.medium.com/?source=post_page-----88bdd0d60928--------------------------------)[](https://towardsdatascience.com/?source=post_page-----88bdd0d60928--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----88bdd0d60928--------------------------------)
    [Sébastien Gilbert](https://sebastiengilbert.medium.com/?source=post_page-----88bdd0d60928--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F975aef8c496a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftraining-an-agent-to-master-a-simple-game-through-self-play-88bdd0d60928&user=S%C3%A9bastien+Gilbert&userId=975aef8c496a&source=post_page-975aef8c496a----88bdd0d60928---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----88bdd0d60928--------------------------------)
    ·8 min read·Sep 6, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F88bdd0d60928&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftraining-an-agent-to-master-a-simple-game-through-self-play-88bdd0d60928&user=S%C3%A9bastien+Gilbert&userId=975aef8c496a&source=-----88bdd0d60928---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F975aef8c496a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftraining-an-agent-to-master-a-simple-game-through-self-play-88bdd0d60928&user=S%C3%A9bastien+Gilbert&userId=975aef8c496a&source=post_page-975aef8c496a----88bdd0d60928---------------------post_header-----------)
    发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----88bdd0d60928--------------------------------)
    ·8 min read·2023年9月6日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F88bdd0d60928&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftraining-an-agent-to-master-a-simple-game-through-self-play-88bdd0d60928&user=S%C3%A9bastien+Gilbert&userId=975aef8c496a&source=-----88bdd0d60928---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F88bdd0d60928&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftraining-an-agent-to-master-a-simple-game-through-self-play-88bdd0d60928&source=-----88bdd0d60928---------------------bookmark_footer-----------)![](../Images/d109dfb5700af262af5a3d2b43e7547e.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F88bdd0d60928&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftraining-an-agent-to-master-a-simple-game-through-self-play-88bdd0d60928&source=-----88bdd0d60928---------------------bookmark_footer-----------)![](../Images/d109dfb5700af262af5a3d2b43e7547e.png)'
- en: A robot computing some additions. Image by the author, with help from DALL-E
    2.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 一个计算一些加法的机器人。图片由作者提供，借助 DALL-E 2。
- en: Introduction
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: Isn’t it amazing that everything you need to excel in a perfect information
    game is there for everyone to see in the rules of the game?
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 难道不令人惊叹吗？在完美信息游戏中，你需要的一切都在游戏规则中可见。
- en: Unfortunately, for mere mortals like me, reading the rules of a new game is
    only a tiny fraction of the journey to learn to play a complex game. Most of the
    time is spent playing, ideally against a player of comparable strength (or a better
    player who is patient enough to help us expose our weaknesses). Losing often and
    hopefully winning sometimes provides the psychological punishments and rewards
    that steer us towards playing incrementally better.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，对于像我这样的普通人来说，阅读新游戏的规则只是学习玩复杂游戏旅程的一小部分。大部分时间都花在玩游戏上，理想情况下是与实力相当的玩家（或一个足够耐心的更强的玩家，帮助我们暴露我们的弱点）对战。经常失败并希望偶尔获胜提供了心理上的惩罚和奖励，促使我们逐步提高游戏水平。
- en: 'Perhaps, in a not-too-far future, a language model will read the rules of a
    complex game such as chess and, right from the start, play at the highest possible
    level. In the meantime, I propose a more modest challenge: learning by self-play.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 或许，在不久的将来，语言模型将能够阅读像国际象棋这样的复杂游戏的规则，并从一开始就以最高水平进行游戏。在此期间，我提出一个更为谦虚的挑战：通过自我对弈学习。
- en: In this project, we’ll train an agent to learn to **play perfect information,
    two player games** by observing the results of matches played by previous versions
    of itself. The agent will approximate a value (the game expected result) for any
    game state. As an additional challenge, our agent won’t be allowed to maintain
    a lookup table of the state space, as this approach would not be manageable for
    complex games.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个项目中，我们将训练一个代理，通过观察之前版本的比赛结果来学习**完美信息的两人游戏**。该代理将为任何游戏状态近似一个值（游戏期望结果）。作为额外挑战，我们的代理不允许维持状态空间的查找表，因为这种方法对于复杂游戏而言是不切实际的。
- en: Solving SumTo100
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 解决SumTo100
- en: The game
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 游戏
- en: 'The game that we are going to discuss is SumTo100\. The game goal is to reach
    a sum of 100 by adding numbers between 1 and 10\. Here are the rules:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 我们要讨论的游戏是SumTo100。游戏的目标是通过将1到10之间的数字相加来达到100。以下是规则：
- en: Initialize sum = 0.
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化和 = 0。
- en: Choose a first player. The two players take turns.
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择一个首位玩家。两位玩家轮流进行。
- en: 'While sum < 100:'
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当和 < 100 时：
- en: The player chooses a number between 1 and 10 inclusively. The selected number
    gets added to the sum without exceeding 100.
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 玩家选择一个1到10之间的数字（包括1和10）。选定的数字会被加到总和中，但不会超过100。
- en: If sum < 100, the other player plays (i.e., we go back to the top of point 3).
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果和 < 100，另一位玩家继续进行（即，我们返回到第3点的顶部）。
- en: 4\. The player that added the last number (reaching 100) wins.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 4. 加上最后一个数字（达到100）的玩家获胜。
- en: '![](../Images/795b2aff5a3db4ea402f215dfb555414.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/795b2aff5a3db4ea402f215dfb555414.png)'
- en: Two snails minding their own business. Image by the author, with help from DALL-E
    2.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 两只蜗牛各自忙着自己的事。图像由作者提供，DALL-E 2协助完成。
- en: 'Starting with such a simple game has many advantages:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 从这样一个简单的游戏开始有许多好处：
- en: The state space has only 101 possible values.
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 状态空间只有101个可能的值。
- en: The states can get plotted on a 1D grid. This peculiarity will allow us to represent
    the state value function learned by the agent as a 1D bar graph.
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 状态可以被绘制在一维网格上。这一特点将使我们能够将代理学习到的状态值函数表示为一维条形图。
- en: 'The optimal strategy is known:'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最佳策略已知：
- en: '- Reach a sum of 11n + 1, where n ∈ {0, 1, 2, …, 9}'
  id: totrans-29
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '- 达到一个和为11n + 1，其中n ∈ {0, 1, 2, …, 9}'
- en: 'We can visualize the state value of the optimal strategy:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以可视化最佳策略的状态值：
- en: '![](../Images/9fa578e690e7ca71bc5d28f13819cf2c.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9fa578e690e7ca71bc5d28f13819cf2c.png)'
- en: 'Figure 1: The optimal state values for SumTo100\. Image by the author.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：SumTo100的最佳状态值。图像由作者提供。
- en: The game state is the sum after an agent has completed its turn. A value of
    1.0 means that the agent is sure to win (or has won), while a value of -1.0 means
    that the agent is sure to lose (assuming the opponent plays optimally). An intermediary
    value represents the estimated return. For example, a state value of 0.2 means
    a slightly positive state, while a state value of -0.8 represents a likely loss.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 游戏状态是代理完成其回合后的总和。值为1.0意味着代理肯定会赢（或已经赢了），而值为-1.0则意味着代理肯定会输（假设对手进行最佳玩法）。中间值代表估计回报。例如，状态值为0.2表示稍微积极的状态，而状态值为-0.8表示可能的失败。
- en: If you want to dive in the code, the script that performs the whole training
    procedure is *learn_sumTo100.sh*, [in this repository](https://github.com/sebastiengilbert73/tutorial_learnbyplay).
    Otherwise, bear with me as we’ll go through a high level description of how our
    agent learns by self-play.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想深入了解代码，执行整个训练过程的脚本是*learn_sumTo100.sh*，[在这个仓库](https://github.com/sebastiengilbert73/tutorial_learnbyplay)。否则，请耐心等待，我们将通过高层次的描述来介绍我们的代理如何通过自我对弈进行学习。
- en: Generation of games played by random players
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 由随机玩家进行的游戏生成
- en: We want our agent to learn from games played by previous versions of itself,
    but in the first iteration, since the agent has not learned anything yet, we’ll
    have to simulate games played by *random players*. At each turn, the players will
    get the list of legal moves from the game authority (the class that encodes the
    game rules), given the current game state. The random players will select a move
    randomly from this list.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望代理从其先前版本所玩的游戏中学习，但在第一轮中，由于代理尚未学习任何东西，我们将不得不模拟*随机玩家*进行的游戏。在每一步，玩家将从游戏权威（编码游戏规则的类）处获得合法移动的列表，基于当前的游戏状态。随机玩家将从此列表中随机选择一个动作。
- en: 'Figure 2 is an example of a game played by two random players:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 图2是两个随机玩家进行游戏的示例：
- en: '![](../Images/621b06f67e6b39bb1b66c0de6fe48c53.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/621b06f67e6b39bb1b66c0de6fe48c53.png)'
- en: 'Figure 2: Example of a game played by random players. Image by the author.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：随机玩家进行游戏的示例。图像由作者提供。
- en: In this case, the second player won the game by reaching a sum of 100.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，第二个玩家通过达到总和100赢得了游戏。
- en: We’ll implement an agent that has access to a neural network that takes as input
    a game state (after the agent has played) and outputs the expected return of this
    game. For any given state (before the agent has played), the agent gets the list
    of legal actions and their corresponding candidate states (we only consider games
    having deterministic transitions).
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将实现一个代理，该代理访问一个神经网络，该网络以游戏状态（在代理已经玩过之后）作为输入，并输出该游戏的预期回报。对于任何给定的状态（在代理尚未玩过之前），代理将获得合法动作列表及其对应的候选状态（我们仅考虑具有确定性转移的游戏）。
- en: 'Figure 3 shows the interactions between the agent, the opponent (whose move
    selection mechanism is unknown), and the game authority:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 图3展示了代理、对手（其移动选择机制未知）和游戏权威之间的交互：
- en: '![](../Images/73c3f1248a7589182a154460b1ec050c.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/73c3f1248a7589182a154460b1ec050c.png)'
- en: 'Figure 3: Interactions between the agent, the opponent, and the game authority.
    Image by the author.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：代理、对手和游戏权威之间的交互。图像由作者提供。
- en: In this setting, the agent relies on its regression neural network to predict
    the expected return of game states. The better the neural network can predict
    which candidate move yields the highest return, the better the agent will play.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种设置下，代理依赖于其回归神经网络来预测游戏状态的预期回报。神经网络预测哪个候选动作产生最高回报的能力越强，代理的表现就会越好。
- en: 'Our list of randomly played matches will provide us with the dataset for our
    first pass of training. Taking the example game from Figure 2, we want to punish
    the moves made by player 1 since its behaviour led to a loss. The state resulting
    from the last action gets a value of -1.0 since it allowed the opponent to win.
    The other states get discounted negative values by a factor of γᵈ , where d is
    the distance with respect to the last state reached by the agent. γ (gamma) is
    the discount factor, a number ∈ [0, 1], that expresses the uncertainty in the
    evolution of a game: we don’t want to punish early decisions as hard as the last
    decisions. Figure 4 shows the state values associated with the decisions made
    by player 1:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的随机游戏列表将为我们第一次训练提供数据集。以图2中的示例游戏为例，我们希望惩罚玩家1的移动，因为其行为导致了失败。由于允许对手获胜，最后一个动作产生的状态值为-1.0。其他状态通过折扣因子γᵈ获得负值，其中d是相对于代理达到的最后状态的距离。γ（gamma）是折扣因子，一个∈
    [0, 1]的数字，表示游戏演变中的不确定性：我们不希望像对最后决策那样严厉地惩罚早期决策。图4展示了玩家1做出的决策相关的状态值：
- en: '![](../Images/fd36467f54afe8bb0ebd7d69304985b0.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fd36467f54afe8bb0ebd7d69304985b0.png)'
- en: 'Figure 4: The state values, from the point of view of player 1\. Image by the
    author.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：从玩家1的角度看状态值。图像由作者提供。
- en: The random games generate states with their target expected return. For example,
    reaching a sum of 97 has a target expected return of -1.0, and a sum of 73 has
    a target expected return of -γ³. Half the states take the point of view of player
    1, and the other half take the point of view of player 2 (although it doesn’t
    matter in the case of the game SumTo100). When a game ends with a win for the
    agent, the corresponding states get similarly discounted positive values.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 随机游戏生成具有目标预期回报的状态。例如，达到97的总和有一个目标预期回报为-1.0，而达到73的总和有一个目标预期回报为-γ³。状态的一半以玩家1的视角为准，另一半以玩家2的视角为准（虽然在游戏SumTo100的情况下并不重要）。当游戏以代理获胜结束时，相应的状态将获得类似折扣的正值。
- en: Training an agent to predict the return of games
  id: totrans-50
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练一个代理以预测游戏的回报
- en: 'We have all we need to start our training: a neural network (we’ll use a two-layers
    perceptron) and a dataset of (state, expected return) pairs. Let’s see how the
    loss on the predicted expected return evolves:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经拥有了开始训练所需的一切：一个神经网络（我们将使用一个两层感知机）和一个 (状态, 期望回报) 对的数据集。让我们看看预测的期望回报的损失如何演变：
- en: '![](../Images/aed34ba90cca092159f457e82761b0e2.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/aed34ba90cca092159f457e82761b0e2.png)'
- en: 'Figure 5: Evolution of the loss as a function of the epoch. Image by the author.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：损失随轮次演变的情况。图片由作者提供。
- en: We shouldn’t be surprised that the neural network doesn’t show much predicting
    power over the outcome of games played by random players.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 不足为奇的是，神经网络在预测由随机玩家进行的游戏结果方面并没有表现出太大的预测能力。
- en: Did the neural network learn anything at all?
  id: totrans-55
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 神经网络究竟学到了什么？
- en: 'Fortunately, because the states can get represented as a 1D grid of numbers
    between 0 and 100, we can plot the predicted returns of the neural network after
    the first training round and compare them with the optimal state values of Figure
    1:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，由于状态可以表示为 0 到 100 之间的 1D 数字网格，我们可以绘制神经网络在第一次训练轮次后的预测回报，并将其与图 1 中的最佳状态值进行比较：
- en: '![](../Images/7e29ae6d8508e4435fde819a26e55cba.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7e29ae6d8508e4435fde819a26e55cba.png)'
- en: 'Figure 6: The predicted returns after training on a dataset of games played
    by random players. Image by the author.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6：在由随机玩家进行的游戏数据集上训练后的预测回报。图片由作者提供。
- en: 'As it turns out, through the chaos of random games, the neural network learned
    two things:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 事实证明，通过随机游戏的混乱，神经网络学到了两件事：
- en: If you can reach a sum of 100, do it. That’s good to know, considering it is
    the goal of the game.
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果你能达到 100，就去做吧。考虑到这是游戏的目标，知道这一点是好的。
- en: If you reach a sum of 99, you’re sure to lose. Indeed, in this situation, the
    opponent has only one legal action and that action yields to a loss for the agent.
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果你达到 99，你肯定会输。实际上，在这种情况下，对手只有一个合法的行动，而该行动会导致代理的失败。
- en: The neural network learned essentially to finish the game.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络基本上学会了如何完成游戏。
- en: To learn to play a little better, we must rebuild the dataset by simulating
    games played between copies of the agent with their freshly trained neural network.
    To avoid generating identical games, the players play a bit randomly. An approach
    that works well is choosing moves with the epsilon-greedy algorithm, using ε =
    0.5 for each players first move, then ε = 0.1 for the rest of the game.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 为了学会稍微好一点的游戏玩法，我们必须通过模拟在其新训练神经网络下的代理之间进行的游戏来重建数据集。为了避免生成相同的游戏，玩家会有些随机。一个有效的方法是使用ε-贪婪算法选择行动，玩家的第一步使用ε
    = 0.5，然后游戏的其余部分使用ε = 0.1。
- en: Repeating the training loop with better and better players
  id: totrans-64
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用越来越好的玩家重复训练循环
- en: 'Since both players now know that they must reach 100, reaching a sum between
    90 and 99 should be punished, because the opponent would jump on the opportunity
    to win the match. This phenomenon is visible in the predicted state values after
    the second round of training:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 由于两名玩家现在都知道他们必须达到 100，达到 90 到 99 之间的总和应该受到惩罚，因为对手会抓住机会赢得比赛。这种现象在第二轮训练后的预测状态值中是明显的：
- en: '![](../Images/56665c735e94d184694716d505e7211b.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/56665c735e94d184694716d505e7211b.png)'
- en: 'Figure 7: Predicted state values after two rounds of training. Sums from 90
    to 99 show values close to -1\. Image by the author.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7：经过两轮训练后的预测状态值。90 到 99 的总和显示出接近 -1 的值。图片由作者提供。
- en: We see a pattern emerging. The first training round informs the neural network
    about the last action; the second training round informs about the penultimate
    action, and so on. We need to repeat the cycle of games generation and training
    on prediction at least as many times as there are actions in a game.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到一种模式正在出现。第一次训练轮次将最后一个行动的信息传递给神经网络；第二次训练轮次传递倒数第二个行动的信息，以此类推。我们需要至少重复生成游戏和预测训练的周期多次，以覆盖游戏中的所有行动。
- en: 'The following animation shows the evolution of the predicted state values after
    25 training rounds:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 以下动画展示了经过 25 轮训练后的预测状态值的演变：
- en: '![](../Images/77fdfa0b81871f7545cb49bdda1617c7.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/77fdfa0b81871f7545cb49bdda1617c7.png)'
- en: 'Figure 8: Animation of the state values learned along the training rounds.
    Image by the author.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8：训练轮次中学到的状态值的动画。图片由作者提供。
- en: The envelope of the predicted returns decays exponentially, as we go from the
    end towards the beginning of the game. Is this a problem?
  id: totrans-72
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 预测回报的包络线随着游戏的进行从结尾向开始方向呈指数衰减。这是个问题吗？
- en: 'Two factors contribute to this phenomenon:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 两个因素导致了这种现象：
- en: γ directly damps the target expected returns, as we move away from the end of
    the game.
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: γ 直接抑制目标预期回报，因为我们离游戏结束越来越远。
- en: The epsilon-greedy algorithm injects randomness in the player behaviours, making
    the outcomes harder to predict. There is an incentive to predict a value close
    to zero to protect against cases of extremely high losses. However, the randomness
    is desirable because we don’t want the neural network to learn a single line of
    play. We want the neural network to witness blunders and unexpected good moves,
    both from the agent and the opponent.
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: epsilon-贪婪算法在玩家行为中注入了随机性，使结果更难预测。存在一种激励机制来预测接近零的值，以防止极高损失的情况。然而，这种随机性是可取的，因为我们不希望神经网络学习单一的游戏策略。我们希望神经网络能见证到错误和意外的好动作，这些都可能来自代理和对手。
- en: In practice, it shouldn’t be a problem because in any situation, we will compare
    values among the legal moves in a given state, which share comparable scales,
    at least for the game SumTo100\. The scale of the values doesn’t matter when we
    choose the greedy move.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，这不应该是问题，因为在任何情况下，我们将比较给定状态下所有合法移动的值，这些值在规模上是相似的，至少对于SumTo100游戏而言。选择贪婪移动时，值的规模并不重要。
- en: Conclusion
  id: totrans-77
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: 'We challenged ourselves to create an agent that can learn to master a game
    of perfect information involving two players, with deterministic transitions from
    a state to the next, given an action. No hand coded strategies nor tactics were
    allowed: everything had to be learned by self-play.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 我们挑战自己创建一个可以学习掌握完美信息游戏的代理，这个游戏涉及两个玩家，状态到状态之间有确定的过渡，给定一个动作。不得使用手动编码的策略或战术：一切都必须通过自我对弈来学习。
- en: We could solve the simple game of SumTo100 by running multiple rounds of pitching
    copies of the agent against each other, and training a regression neural network
    to predict the expected return of the generated games.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过多轮对抗性的代理拷贝来解决简单的SumTo100游戏，并训练一个回归神经网络来预测生成游戏的预期回报。
- en: The gained insight prepares us well for the next ladder in game complexity,
    but that will be for my next post! 😊
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 获得的见解为我们做好了迎接下一个游戏复杂性阶梯的准备，但那将是我下一个帖子中的内容！😊
- en: Thank you for your time.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 感谢你的时间。
