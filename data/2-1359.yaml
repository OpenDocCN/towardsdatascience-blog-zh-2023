- en: 'Introduction to Speech Enhancement: Part 1 — Concepts and Task Definition'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 语音增强简介：第一部分 — 概念与任务定义
- en: 原文：[https://towardsdatascience.com/introduction-to-speech-enhancement-part-1-df6098b47b91](https://towardsdatascience.com/introduction-to-speech-enhancement-part-1-df6098b47b91)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/introduction-to-speech-enhancement-part-1-df6098b47b91](https://towardsdatascience.com/introduction-to-speech-enhancement-part-1-df6098b47b91)
- en: An introduction into the concepts, methods, and algorithms that allow us to
    improve the quality of degraded speech or suppress noise
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 对改善退化语音质量或抑制噪声的概念、方法和算法的介绍
- en: '[](https://medium.com/@mattiadigangi?source=post_page-----df6098b47b91--------------------------------)[![Mattia
    Di Gangi](../Images/ccd89021df6724797d45cc3c655a38a5.png)](https://medium.com/@mattiadigangi?source=post_page-----df6098b47b91--------------------------------)[](https://towardsdatascience.com/?source=post_page-----df6098b47b91--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----df6098b47b91--------------------------------)
    [Mattia Di Gangi](https://medium.com/@mattiadigangi?source=post_page-----df6098b47b91--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@mattiadigangi?source=post_page-----df6098b47b91--------------------------------)[![Mattia
    Di Gangi](../Images/ccd89021df6724797d45cc3c655a38a5.png)](https://medium.com/@mattiadigangi?source=post_page-----df6098b47b91--------------------------------)[](https://towardsdatascience.com/?source=post_page-----df6098b47b91--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----df6098b47b91--------------------------------)
    [Mattia Di Gangi](https://medium.com/@mattiadigangi?source=post_page-----df6098b47b91--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----df6098b47b91--------------------------------)
    ·6 min read·Jan 17, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----df6098b47b91--------------------------------)
    ·阅读时间 6 分钟·2023年1月17日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/fb50f60823ea39daa9134c875473e489.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fb50f60823ea39daa9134c875473e489.png)'
- en: Photo by [Wan San Yip](https://unsplash.com/ja/@wansan_99?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由 [Wan San Yip](https://unsplash.com/ja/@wansan_99?utm_source=medium&utm_medium=referral)
    提供，来源于 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: 'This article is part of a series:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 本文是系列文章的一部分：
- en: '**Introduction to Speech Enhancement: Part 1 — Concepts and Task Definition**'
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**语音增强简介：第一部分 — 概念与任务定义**'
- en: '[Introduction to Speech Enhancement: Part 2 — Signal Representation](https://medium.com/towards-data-science/introduction-to-speech-enhancement-part-2-signal-representation-ab1deca2fa74)'
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[语音增强简介：第二部分 — 信号表示](https://medium.com/towards-data-science/introduction-to-speech-enhancement-part-2-signal-representation-ab1deca2fa74)'
- en: Speech enhancement is a set of methods and techniques aiming at improving speech
    quality in terms of intelligibility and/or perceptive quality using speech audio
    signal processing techniques [[Wikipedia](https://en.wikipedia.org/wiki/Speech_enhancement)].
    It has many practical use cases, including cleaning speech signal from noise in
    hearing aids, or recovering a hard-to-understand speech signal from a noisy channel/environment.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 语音增强是一套方法和技术，旨在通过语音音频信号处理技术提高语音质量，包括清除助听器中的噪声，或从嘈杂的信道/环境中恢复难以理解的语音信号 [[Wikipedia](https://en.wikipedia.org/wiki/Speech_enhancement)]。它有许多实际应用，包括从噪声中清除语音信号或从嘈杂的信道/环境中恢复难以理解的语音信号。
- en: It is similar but different from **speech separation** [1], that is the algorithmic
    separation of one audio signal into two different channels for, respectively,
    the speech and the background. It is possible to apply speech enhancement to get
    the speech signal and then apply additional algorithms to compute the **residual
    signal**, or the *difference* between the original signal and the enhanced speech**.**
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 它类似于但不同于**语音分离**[1]，即将一个音频信号算法性地分离成两个不同的通道，分别用于语音和背景。可以应用语音增强来获取语音信号，然后应用额外的算法来计算**残差信号**，或原始信号与增强语音之间的*差异*。
- en: Different speech enhancement methods are applied to reduce the effect of different
    [noise models](https://aishack.in/tutorials/noise-models-1/) (e.g. stationary
    vs non-stationary). While this field of research is not new at all, deep learning
    approaches flourished in recent years and improved the quality of speech enhancement
    in the most challenging scenario of non-stationary noise, even with a low signal-to-noise
    ratio (SNR).
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 不同的语音增强方法被应用于减少不同[噪声模型](https://aishack.in/tutorials/noise-models-1/)（例如，平稳与非平稳）的影响。虽然这个研究领域并不新鲜，但近年来深度学习方法蓬勃发展，提升了在最具挑战性的非平稳噪声场景中的语音增强质量，即使在低信噪比（SNR）下也是如此。
- en: Terminology and Background
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 术语与背景
- en: '**Signal:** observation of a physical quantity that varies and is measurable.
    Audio, video, images, all fall under this definition.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '**信号：** 观测一个变化且可测量的物理量。音频、视频、图像，都属于这个定义。'
- en: '**Noise model:** it is a mathematic model of the **stochastic** process underlying
    a noise signal. Since the process is stochastic, it is usually described in terms
    of a probability distribution.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '**噪声模型：** 这是描述噪声信号潜在**随机**过程的数学模型。由于该过程是随机的，通常用概率分布来描述。'
- en: '**(Non-)Stationary process:** a signal is assumed generated by an underlying
    process. It can be deterministic or stochastic. Since we know everything about
    a deterministic process, the interest lies on stochastic processes. A stochastic
    process is said *stationary* if its unconditional joint distribution does **not**
    depend on time. In other words, given a model for the process, the probability
    of an event **X** occurring at time **t** does not depend on **t** itself**.**
    By extension, a signal produced by a stationary process is also called stationary.
    In the audio domain, a signal is stationary when its frequency or spectral content
    does not change over time. This implies that, for speech enhancement, the real
    challenge is given by non-stationary noise, since it is more unpredictable and
    more hardly distinguishable from speech, as the human voice is also non-stationary:
    the frequencies in our voice signals change all the time.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '**（非）平稳过程：** 假设信号由一个潜在的过程生成。它可以是确定性的或随机的。由于我们对确定性过程了解一切，因此兴趣在于随机过程。如果一个随机过程的无条件联合分布**不**依赖于时间，则称其为*平稳的*。换句话说，给定一个过程的模型，事件
    **X** 在时间 **t** 发生的概率不依赖于 **t** 本身。通过扩展，由平稳过程生成的信号也称为平稳。在音频领域，当信号的频率或谱内容随时间变化时，信号就是平稳的。这意味着，对于语音增强，真正的挑战来自于非平稳噪声，因为它更不可预测且更难与语音区分，而人声也同样是非平稳的：我们声音信号中的频率一直在变化。'
- en: Experimental Setup
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实验设置
- en: Similarly to most empirical research fields, in speech enhancement we need datasets
    for both training our models and evaluating them, metrics to evaluate the results,
    as well as hardware and software tools to run our algorithms.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 与大多数实证研究领域类似，在语音增强中，我们需要数据集来训练我们的模型和评估它们，还需要评估结果的指标，以及运行算法的硬件和软件工具。
- en: As hardware requirements, we definitely need a modern computer equipped with
    (at least) one NVIDIA GPU for training, while a modern multicore CPU can be enough
    during inference, although a high CPU usage is expected.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 作为硬件要求，我们绝对需要一台配备有（至少）一个 NVIDIA GPU 的现代计算机进行训练，而现代多核 CPU 在推断过程中也可能足够，尽管预计会有较高的
    CPU 使用率。
- en: As per software, we need a codebase that uses one deep learning library like
    Tensorflow, Pytorch, or one of those derived from Jax. The FullSubnet+ repository
    linked above can be a good starting point.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在软件方面，我们需要一个使用如 Tensorflow、Pytorch 或 Jax 派生库的代码库。上面链接的 FullSubnet+ 存储库可以是一个很好的起点。
- en: Then, we come to data. The easiest way to measure quality and enable supervised
    learning is to have a reference signal that the network is supposed to reconstruct.
    For this reason, the training sets are usually built by having a collection of
    clean speech signals and a collection of noise signals. Then, both are combined
    by adding them with different levels of SNR. This way, during the training it
    is possible to generate a really high number of speech/noise combinations that
    the network must learn to discriminate to obtain enhanced speech signal. Speech
    enhancement is encouraged in the research community by means of shared tasks,
    and as such it is possible to find public datasets from those sources. The [Deep
    Noise Suppression (DNS) Challenge](https://github.com/microsoft/DNS-Challenge)
    is held annually and presented at Interspeech. In the related repo you can find
    links to the provided datasets as well as additional resources.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们谈到数据。测量质量和实现监督学习的最简单方法是拥有一个网络应该重建的参考信号。因此，训练集通常通过收集干净的语音信号和噪声信号来构建。然后，通过将它们添加不同级别的信噪比（SNR）来将这两者结合起来。这样，在训练过程中，可以生成大量的语音/噪声组合，网络必须学习这些组合以获得增强的语音信号。研究社区通过共享任务来推动语音增强，因此可以从这些来源找到公开数据集。每年举办的[Deep
    Noise Suppression (DNS) Challenge](https://github.com/microsoft/DNS-Challenge)就在Interspeech上展示。在相关的代码库中，您可以找到提供的数据集链接以及其他资源。
- en: 'The evaluation is usually performed by means of manual and automatic assessment.
    The trade-off between the two has been stated many times but it may be worth to
    repeat it. Manual evaluation is performed by humans: it is costly and slow, because
    humans need to be found, instructed and payed, and of course they need to listen
    to the audios before evaluating them. Automatic metrics are fast and cheap, so
    they are repeatable during a development cycle, but they can capture only some
    aspects of the evaluation and not be much reliable in some cases.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 评估通常通过手动和自动评估来进行。两者之间的权衡已被多次提及，但可能值得重复一下。手动评估由人工执行：它既昂贵又缓慢，因为需要找到、指导和支付人力，当然他们还需要在评估之前听取音频。自动指标则快速且便宜，因此在开发周期中可以重复使用，但它们只能捕捉评估的某些方面，并且在某些情况下可能不够可靠。
- en: 'In the case of speech enhancement, there are 4 main automatic metrics that
    are commonly used: perceptual evaluation of speech quality (PESQ), log-likelihood
    ratio (LLR), cepstral distance (CD), and weighted spectral slope distance (WSS).
    All of them compute a distance between the clean speech and the enhanced speech.
    PESQ compares the quality between the samples of the two signals and produces
    a score between -0.5 and 4.5, the higher the better. The other three metrics compute
    distances between different properties of the two spectra: the formants, the log
    spectral distance, or a weighted distance of the spectral slopes. When it is important,
    human evaluation can be added to assess the perceived quality or to count the
    recognizable words from the enhanced signal. Speech recognition may be used to
    recognize the words, but then we need to cope its own errors.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在语音增强的情况下，通常使用四种主要的自动评估指标：语音质量的感知评估（PESQ）、对数似然比（LLR）、倒谱距离（CD）和加权谱斜率距离（WSS）。所有这些指标都计算清晰语音和增强语音之间的距离。PESQ比较两个信号样本之间的质量，产生一个在-0.5到4.5之间的分数，分数越高越好。其他三个指标计算两个谱之间不同属性的距离：共振峰、对数谱距离或谱斜率的加权距离。当需要时，可以增加人工评估，以评估感知质量或统计增强信号中的可识别单词。语音识别可以用来识别单词，但这时我们需要处理它自身的错误。
- en: For what concerns the signal representation, there are speech enhancement models
    that work on the time domain and others that work on the frequency domain. The
    frequency domain is obtained by applying the [Fourier transform](https://www.thefouriertransform.com/)
    to the signal and is an *atemporal* representation of the signal that gives us
    information about its frequency components. In practice, the representation in
    the frequency domain is more useful than the representation in the time domain
    for speech enhancement, and all state-of-the-art methods use it.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 关于信号表示，有些语音增强模型在时间域中工作，而其他模型则在频域中工作。频域是通过对信号应用[傅里叶变换](https://www.thefouriertransform.com/)获得的，是对信号的*非时间性*表示，提供了关于其频率成分的信息。在实践中，频域表示比时间域表示更适用于语音增强，所有最先进的方法都使用它。
- en: Example
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 示例
- en: Let’s show an example to get a hearing understanding of the topic. We start
    from a video commercial containing loud music that may make the speech hard to
    hear.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们展示一个例子，以便对这个主题有听觉上的理解。我们从一个包含响亮音乐的视频广告开始，这可能会使语音难以听清。
- en: Released with a Creative Commons license
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 以创作共享许可证发布
- en: 'We first extract the audio track by using the famous [ffmpeg](https://ffmpeg.org/index.html)
    tool:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先使用著名的[ffmpeg](https://ffmpeg.org/index.html)工具提取音频轨道：
- en: '[## original_audio.wav](https://drive.google.com/file/d/1Nu2_OxkkIdIOsrT_e_E910t1bi5cxgXb/preview?source=post_page-----df6098b47b91--------------------------------)'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '[## original_audio.wav](https://drive.google.com/file/d/1Nu2_OxkkIdIOsrT_e_E910t1bi5cxgXb/preview?source=post_page-----df6098b47b91--------------------------------)'
- en: 'Then, we apply the FullSubNet+ algorithm [2], a deep-learning based method
    with [open-source code](https://github.com/hit-thusz-RookieCJ/FullSubNet-plus)
    and a [pre-trained model](https://github.com/hit-thusz-RookieCJ/FullSubNet-plus#readme)
    available. It is very easy to reproduce this result by following the instructions
    in the repo:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们应用FullSubNet+算法[2]，这是一种基于深度学习的方法，具有[开源代码](https://github.com/hit-thusz-RookieCJ/FullSubNet-plus)和[预训练模型](https://github.com/hit-thusz-RookieCJ/FullSubNet-plus#readme)可用。按照仓库中的说明，很容易复现这一结果：
- en: '[## enhanced_audio.wav](https://drive.google.com/file/d/1-IRilAqcVQ8qZn2fTg66a8ZS-i_DgqKf/preview?source=post_page-----df6098b47b91--------------------------------)'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '[## enhanced_audio.wav](https://drive.google.com/file/d/1-IRilAqcVQ8qZn2fTg66a8ZS-i_DgqKf/preview?source=post_page-----df6098b47b91--------------------------------)'
- en: 'And finally, to satisfy your curiosity, the residual audio obtained by the
    enhanced speech from the original signal:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，为了满足你的好奇心，展示从原始信号中增强语音得到的残余音频：
- en: '[## residual_audio.wav](https://drive.google.com/file/d/1ZHSn9LtAQYUWPzQDxi_cAzDIYrALoTio/preview?source=post_page-----df6098b47b91--------------------------------)'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '[## residual_audio.wav](https://drive.google.com/file/d/1ZHSn9LtAQYUWPzQDxi_cAzDIYrALoTio/preview?source=post_page-----df6098b47b91--------------------------------)'
- en: The speech quality in *enhanced_audio.wav* is notably improved and the music
    is, according to the time, totally removed or its volume is lowered considerably*.*
    The result shows the effectiveness of modern speech enhancement, but it also shows
    that the results are not perfect and more research is going on in this field.
    Also, by running the code, you will notice that this operation is quite expensive
    in computational terms. The availability of a GPU makes the process much faster,
    but reducing the computational complexity while keeping or improving the quality
    is an important research goal.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '*enhanced_audio.wav*中的语音质量显著提高，并且音乐的音量根据时间被完全去除或大幅降低。结果显示了现代语音增强的有效性，但也表明结果并不完美，该领域还有更多研究在进行。此外，通过运行代码，你会发现这一操作在计算上相当昂贵。拥有GPU会使过程更快，但在保持或改善质量的同时减少计算复杂性是一个重要的研究目标。'
- en: Conclusion
  id: totrans-36
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: That’s all for the first part of this series! We have discussed the ideas and
    the reasons for speech enhancement, and given some background about digital signals.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 本系列的第一部分到此为止！我们讨论了语音增强的理念和原因，并提供了一些关于数字信号的背景信息。
- en: In the second part of this series will go into the details to explain a publicly
    available state-of-the-art model for speech enhancement.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在本系列的第二部分，我们将详细介绍一个公开的最先进语音增强模型。
- en: In the meanwhile, I encourage you to read through the linked resources in the
    article, and to go through teaching material for digital signal processing if
    the topic is of interest for you. One freely-available book on the topic is [Think
    DSP](https://github.com/AllenDowney/ThinkDSP). It is possible to read it online
    or download the pdf, and of course to buy a physical copy to support its author
    (I do not receive any money from it).
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 与此同时，我鼓励你阅读文章中的链接资源，如果你对这一主题感兴趣，可以查看数字信号处理的教学资料。其中一本免费提供的书籍是[Think DSP](https://github.com/AllenDowney/ThinkDSP)。你可以在线阅读或下载pdf，当然也可以购买纸质版以支持作者（我不会从中获得任何收益）。
- en: Thank you for reading so far and I hope to see you in the next part!
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 感谢你一直阅读，希望在下一部分中见到你！
- en: '[**Introduction to Speech Enhancement: Part 2 — Signal Representation**](/introduction-to-speech-enhancement-part-2-signal-representation-ab1deca2fa74)'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '[**语音增强简介：第二部分 — 信号表示**](/introduction-to-speech-enhancement-part-2-signal-representation-ab1deca2fa74)'
- en: '[](/automatic-evaluation-of-synthesized-speech-354f7a2a20d7?source=post_page-----df6098b47b91--------------------------------)
    [## Automatic Evaluation of Synthesized Speech'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/automatic-evaluation-of-synthesized-speech-354f7a2a20d7?source=post_page-----df6098b47b91--------------------------------)
    [## 自动评价合成语音'
- en: Your guide to understanding how deep learning and pre-trained models can be
    used for evaluating automatic voices
  id: totrans-43
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 了解深度学习和预训练模型如何用于自动语音评估的指南
- en: towardsdatascience.com](/automatic-evaluation-of-synthesized-speech-354f7a2a20d7?source=post_page-----df6098b47b91--------------------------------)
    [](/data-processing-automation-with-inotifywait-663aba0c560a?source=post_page-----df6098b47b91--------------------------------)
    [## Data Processing Automation with inotifywait
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/automatic-evaluation-of-synthesized-speech-354f7a2a20d7?source=post_page-----df6098b47b91--------------------------------)
    [](/data-processing-automation-with-inotifywait-663aba0c560a?source=post_page-----df6098b47b91--------------------------------)
    [## 使用inotifywait进行数据处理自动化
- en: How to automate before having a production-ready MLOps platfotm
  id: totrans-45
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 在拥有生产就绪的MLOps平台之前如何进行自动化
- en: towardsdatascience.com](/data-processing-automation-with-inotifywait-663aba0c560a?source=post_page-----df6098b47b91--------------------------------)
    [](/3-common-bug-sources-and-how-to-avoid-them-182f9974d2ab?source=post_page-----df6098b47b91--------------------------------)
    [## 3 Common Bug Sources and How to Avoid Them
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/data-processing-automation-with-inotifywait-663aba0c560a?source=post_page-----df6098b47b91--------------------------------)
    [](/3-common-bug-sources-and-how-to-avoid-them-182f9974d2ab?source=post_page-----df6098b47b91--------------------------------)
    [## 3种常见的错误来源及如何避免它们
- en: Some coding patterns are more prone to hide bugs. Writing high quality code
    and knowing how our brain works can help to…
  id: totrans-47
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 某些编码模式更容易隐藏错误。编写高质量代码并了解大脑的工作原理可以帮助…
- en: towardsdatascience.com](/3-common-bug-sources-and-how-to-avoid-them-182f9974d2ab?source=post_page-----df6098b47b91--------------------------------)
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/3-common-bug-sources-and-how-to-avoid-them-182f9974d2ab?source=post_page-----df6098b47b91--------------------------------)
- en: References
  id: totrans-49
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] Bahmaninezhad, Fahimeh, et al. “A unified framework for speech separation.”
    *arXiv preprint arXiv:1912.07814* (2019).'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] Bahmaninezhad, Fahimeh等. “语音分离的统一框架。” *arXiv预印本 arXiv:1912.07814* (2019)。'
- en: '[2] Chen, Jun, et al. “FullSubNet+: Channel Attention FullSubNet with Complex
    Spectrograms for Speech Enhancement.” *ICASSP 2022–2022 IEEE International Conference
    on Acoustics, Speech and Signal Processing (ICASSP)*. IEEE, 2022\. [https://arxiv.org/abs/2203.12188](https://arxiv.org/abs/2203.12188)'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] 陈军等. “FullSubNet+: 基于通道注意力的FullSubNet与复杂谱图用于语音增强。” *ICASSP 2022–2022 IEEE国际声学、语音与信号处理会议（ICASSP）*。IEEE，2022。
    [https://arxiv.org/abs/2203.12188](https://arxiv.org/abs/2203.12188)'
- en: Medium Membership
  id: totrans-52
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Medium会员
- en: Do you like my writing and are considering subscribing for a Medium Membership
    for having unlimited access to the articles?
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 你喜欢我的写作并考虑订阅Medium会员以无限访问文章吗？
- en: If you decide to subscribe through this link you will support me through your
    subscription with no additional cost for you [https://medium.com/@mattiadigangi/membership](https://medium.com/@mattiadigangi/membership)
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你通过此链接订阅，你将支持我而不会增加额外费用 [https://medium.com/@mattiadigangi/membership](https://medium.com/@mattiadigangi/membership)
