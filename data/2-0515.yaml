- en: CLIP — Intuitively and Exhaustively Explained
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: CLIP — 直观且详尽的解释
- en: 原文：[https://towardsdatascience.com/clip-intuitively-and-exhaustively-explained-1d02c07dbf40](https://towardsdatascience.com/clip-intuitively-and-exhaustively-explained-1d02c07dbf40)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/clip-intuitively-and-exhaustively-explained-1d02c07dbf40](https://towardsdatascience.com/clip-intuitively-and-exhaustively-explained-1d02c07dbf40)
- en: Creating strong image and language representations for general machine learning
    tasks.
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 为一般机器学习任务创建强大的图像和语言表示。
- en: '[](https://medium.com/@danielwarfield1?source=post_page-----1d02c07dbf40--------------------------------)[![Daniel
    Warfield](../Images/c1c8b4dd514f6813e08e401401324bca.png)](https://medium.com/@danielwarfield1?source=post_page-----1d02c07dbf40--------------------------------)[](https://towardsdatascience.com/?source=post_page-----1d02c07dbf40--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----1d02c07dbf40--------------------------------)
    [Daniel Warfield](https://medium.com/@danielwarfield1?source=post_page-----1d02c07dbf40--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@danielwarfield1?source=post_page-----1d02c07dbf40--------------------------------)[![Daniel
    Warfield](../Images/c1c8b4dd514f6813e08e401401324bca.png)](https://medium.com/@danielwarfield1?source=post_page-----1d02c07dbf40--------------------------------)[](https://towardsdatascience.com/?source=post_page-----1d02c07dbf40--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----1d02c07dbf40--------------------------------)
    [Daniel Warfield](https://medium.com/@danielwarfield1?source=post_page-----1d02c07dbf40--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----1d02c07dbf40--------------------------------)
    ·17 min read·Oct 20, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----1d02c07dbf40--------------------------------)
    ·阅读时间17分钟·2023年10月20日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/bb84737af3ff56dd6f70ef16d6a1e9ce.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bb84737af3ff56dd6f70ef16d6a1e9ce.png)'
- en: “Contrasting Modes” by Daniel Warfield using MidJourney. All images by the author
    unless otherwise specified.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 由Daniel Warfield使用MidJourney制作的“对比模式”。除非另有说明，所有图片均由作者提供。
- en: In this post you’ll learn about “contrastive language-image pre-training” (CLIP),
    A strategy for creating vision and language representations so good they can be
    used to make highly specific and performant classifiers without any training data.
    We’ll go over the theory, how CLIP differs from more conventional methods, then
    we’ll walk through the architecture step by step.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，你将学习到“对比语言-图像预训练”（CLIP），一种创建视觉和语言表示的策略，使得这些表示足够好，可以用于创建高度特定且性能优越的分类器，而无需任何训练数据。我们将探讨理论，了解CLIP与更传统方法的不同，然后逐步讲解其架构。
- en: '![](../Images/b518de3e7bc648a9fde1be23489a9594.png)'
  id: totrans-9
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b518de3e7bc648a9fde1be23489a9594.png)'
- en: CLIP predicting highly specific labels for classification tasks it was never
    directly trained on. [Source](https://arxiv.org/pdf/2103.00020.pdf)
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: CLIP为分类任务预测高度特定的标签，这些任务从未直接进行过训练。[来源](https://arxiv.org/pdf/2103.00020.pdf)
- en: '**Who is this useful for?** Anyone interested in computer vision, natural language
    processing (NLP), or multimodal modeling.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '**谁会觉得这篇文章有用？** 任何对计算机视觉、自然语言处理（NLP）或多模态建模感兴趣的人。'
- en: '**How advanced is this post?** This post should be approachable to novice data
    scientists. Some of the later sections are a bit more advanced (particularly when
    we dig into the loss function).'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '**这篇文章有多高级？** 这篇文章对初学数据科学的读者应该是容易理解的。后面的一些部分稍微复杂一点（特别是当我们深入探讨损失函数时）。'
- en: '**Pre-requisites:** Some cursory knowledge of computer vision and natural language
    processing.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '**前提条件：** 对计算机视觉和自然语言处理有一些基础了解。'
- en: The Typical Image Classifier
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 典型的图像分类器
- en: When training a model to detect if an image is of a cat or a dog, a common approach
    is to present a model with images of both cats and dogs, then incrementally adjust
    the model based on it’s errors until it learns to distinguish between the two.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练一个模型以检测一张图片是猫还是狗时，常见的方法是向模型展示猫和狗的图片，然后根据模型的错误逐步调整，直到模型学会区分这两者。
- en: '![](../Images/8deeb5ca639e4ccb7a872fc8152a773a.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8deeb5ca639e4ccb7a872fc8152a773a.png)'
- en: A conceptual diagram of what supervised learning might look like. Imagine we
    have a new model which doesn’t know anything about images. We can feed it an image,
    ask it to predict the class of the image, then update the parameters of the model
    based on how wrong it is. We can then do this numerous times until the model starts
    performing well at the task. I explore back propagation in [this post](https://medium.com/towards-data-science/what-are-gradients-and-why-do-they-explode-add23264d24b),
    which is the mechanism which makes this generally possible.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 监督学习可能是什么样子的概念图。假设我们有一个对图像一无所知的新模型。我们可以将一张图像输入给它，让它预测图像的类别，然后根据预测的错误程度来更新模型的参数。我们可以重复这个过程多次，直到模型开始在任务上表现良好。我在[这篇文章](https://medium.com/towards-data-science/what-are-gradients-and-why-do-they-explode-add23264d24b)中探讨了反向传播，这是使这种情况一般可能的机制。
- en: This traditional form of supervised learning is perfectly acceptable for many
    use cases, and is known to perform well in a variety of tasks. However, this strategy
    is also known to result in highly specialized models which only perform well within
    the bounds of their initial training.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 这种传统的监督学习形式在许多使用场景中完全可以接受，并且在各种任务中表现良好。然而，这种策略也会导致高度专业化的模型，这些模型只在其初始训练的范围内表现良好。
- en: '![](../Images/b43e355283d899f7213b73064d845db7.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b43e355283d899f7213b73064d845db7.png)'
- en: Comparing CLIP with a more traditional supervised model. Each of the models
    were trained on and perform well on ImageNet (a popular image classification dataset),
    but when exposed to similar datasets containing the same classes in different
    representations, the supervised model experiences a large degradation in performance,
    while CLIP does not. This implies that the representations in CLIP are more robust
    and generalizable than other methods. [Source](https://arxiv.org/pdf/2103.00020.pdf)
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 将CLIP与更传统的监督模型进行比较。每个模型都在ImageNet（一个流行的图像分类数据集）上进行训练并表现良好，但当暴露于包含相同类别但不同表示形式的类似数据集时，监督模型的性能会显著下降，而CLIP则不会。这表明CLIP中的表示比其他方法更具鲁棒性和可泛化性。[来源](https://arxiv.org/pdf/2103.00020.pdf)
- en: To resolve the issue of over-specialization, CLIP approaches classification
    in a fundamentally different way; by trying to learn the association between images
    and their annotation through contrastive learning. We’ll explore what that means
    in the next section.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决过度专业化的问题，CLIP以一种根本不同的方式处理分类；通过尝试通过对比学习来学习图像及其注释之间的关联。我们将在下一部分探讨这意味着什么。
- en: CLIP, in a Nutshell
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: CLIP，简而言之
- en: What if, instead of creating a model that can predict if an image belongs to
    one of some list of classes, we create a model which predicts if an image belongs
    to some arbitrary caption? This is a subtle shift in thinking which opens the
    doors to completely new training strategies and model applications.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们不是创建一个可以预测图像是否属于某个类别的模型，而是创建一个预测图像是否属于某个任意说明的模型呢？这是一个微妙的思维转变，它为完全新的训练策略和模型应用打开了大门。
- en: '![](../Images/5cd531d59e0a4f2a30d3b910ff38bdb6.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5cd531d59e0a4f2a30d3b910ff38bdb6.png)'
- en: The end result of CLIP, a model which can predict if some arbitrary text belongs
    with an arbitrary image
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: CLIP的最终结果是一个可以预测任意文本是否与任意图像匹配的模型
- en: The core idea of CLIP is to use captioned images scraped from the internet to
    create a model which can predict if text is compatible with an image or not.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: CLIP的核心思想是利用从互联网上抓取的带有说明文字的图像来创建一个模型，该模型可以预测文本是否与图像兼容。
- en: '![](../Images/e00007ef6a051cc7b46a92bce917026b.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e00007ef6a051cc7b46a92bce917026b.png)'
- en: An example of CLIP applied to various data sets it hasn’t seen before. While
    not completely perfect (it predicted the wrong type of aeroplane), CLIP displays
    a remarkable ability to understand a variety of different classification problems
    out of the box. [Source](https://arxiv.org/pdf/2103.00020.pdf)
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: CLIP应用于它之前未见过的各种数据集的示例。虽然并不完全完美（它预测了错误类型的飞机），但CLIP表现出一种令人瞩目的能力，能够理解各种不同的分类问题。[来源](https://arxiv.org/pdf/2103.00020.pdf)
- en: CLIP does this by learning how to encode images and text in such a way that,
    when the text and image encodings are compared to each other, matching images
    have a high value and non-matching images have a low value. **In essence, the
    model learns to map images and text into a landscape such that matching pairs
    are close together, and not matching pairs are far apart.** This strategy of learning
    to predict if things belong or don’t belong together is commonly referred to as
    “contrastive learning”.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: CLIP 通过学习如何对图像和文本进行编码，使得当比较文本和图像的编码时，匹配的图像具有高值而不匹配的图像具有低值。**本质上，该模型学习将图像和文本映射到一个空间，使得匹配的对接近在一起，而不匹配的对则远离。**
    这种学习预测事物是否属于同一组的策略通常被称为“对比学习”。
- en: '![](../Images/4f3214a9b539161246cb390212a143d5.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4f3214a9b539161246cb390212a143d5.png)'
- en: A conceptual diagram of contrastive learning from the perspective of CLIP. In
    essence, we put every image and every caption in some arbitrary space. We then
    learn to place those images and captions within that space such that matching
    pairs are close together, and non-matching pairs are far apart.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 从 CLIP 的角度来看，对比学习的概念图。本质上，我们将每个图像和每个标题放置在某个任意空间中。然后我们学习将这些图像和标题放置在该空间中，使得匹配的对接近在一起，而不匹配的对则远离。
- en: In CLIP, contrastive learning is done by learning a text encoder and an image
    encoder, which learns to put an input into some position in a vector space. CLIP
    then compares these positions during training and tries to maximize the closeness
    of positive pairs, and minimize the closeness of negative pairs.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在 CLIP 中，通过学习一个文本编码器和一个图像编码器来进行对比学习，这两个编码器学习将输入放置在向量空间中的某个位置。CLIP 然后在训练过程中比较这些位置，并尝试最大化正对的接近度，同时最小化负对的接近度。
- en: '![](../Images/93403bdef38eb08b9e3361d371707c74.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/93403bdef38eb08b9e3361d371707c74.png)'
- en: A diagram of CLIP. We take a bunch of images and their corresponding description
    and learn how to encode them such that matching values are large, and not matching
    values are small. In the diagram above the blue highlighted region corresponds
    to positive matching pairs, and the rest of the matrix corresponds to negative
    pairs which need to be minimized. [Source](https://arxiv.org/pdf/2103.00020.pdf)
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: CLIP 的示意图。我们将一堆图像及其相应的描述进行编码，使得匹配值较大，不匹配值较小。在上面的示意图中，蓝色高亮区域对应正匹配对，而矩阵的其余部分对应需要最小化的负匹配对。
    [来源](https://arxiv.org/pdf/2103.00020.pdf)
- en: 'The general strategy CLIP employs allows us to do all sorts of things:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: CLIP 采用的总体策略允许我们做各种事情：
- en: We can build image classifiers by just asking the model which text, like “a
    photo of a cat” and “a photo of a dog” are most likely to be associated with an
    image
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以通过只询问模型哪些文本，如“猫的照片”和“狗的照片”，最有可能与图像相关，来构建图像分类器。
- en: We can build an image search system which can be used to find the image which
    is most related to input text. For instance, we can look at a variety of images
    and find which image is most likely to correspond to the text “a photo of a dog”
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以构建一个图像搜索系统，用于找到与输入文本最相关的图像。例如，我们可以查看各种图像，并找到最可能与文本“狗的照片”对应的图像。
- en: We can use the image encoder by itself to extract abstract information about
    an image which is relevant to text. The encoder can position images in space dependent
    on the content of the image, that information can be used by other machine learning
    models.
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以单独使用图像编码器来提取与文本相关的图像的抽象信息。编码器可以根据图像的内容将图像定位在空间中，这些信息可以被其他机器学习模型使用。
- en: We can use the text encoder by itself to extract abstract information about
    text which is relevant to images. The encoder can position text in space dependent
    on the entire content of the text, that information can be used by other machine
    learning models.
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以单独使用文本编码器来提取与图像相关的文本的抽象信息。编码器可以根据文本的整体内容将文本定位在空间中，这些信息可以被其他机器学习模型使用。
- en: '![](../Images/9625dd56853f5ef34bb3101fefea36a7.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9625dd56853f5ef34bb3101fefea36a7.png)'
- en: Recall that we’re learning to position images and text in such a way that similar
    things are close together. In doing this process we’ve found a way to put both
    text and images in meaningful locations. To do this effectively we have to build
    encoders which have a strong understanding of images and text. We need to be able
    to understand “catness” in an image, or understand that the word “fabulous” is
    modifying the word “jacket”. As a result, the encoders used in CLIP can be used
    in isolation to be able to extract meaning from certain inputs.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下，我们在学习如何将图像和文本定位到相似的东西靠近在一起。通过这个过程，我们找到了一种将文本和图像放置在有意义位置的方法。为了有效地做到这一点，我们必须构建对图像和文本有强大理解的编码器。我们需要能够理解图像中的“猫性”，或者理解“fabulous”这个词在修饰“jacket”这个词。因此，CLIP中使用的编码器可以单独使用以从特定输入中提取意义。
- en: 'While zero-shot classification is pretty cool (zero-shot meaning the ability
    to perform well on an unseen type of data. For instance, asking the model “is
    this person happy” when it was never trained explicitly to detect happiness),
    extracting and using just the text or image encoder within CLIP has become even
    more popular. Because CLIP models are trained to create subtle and powerful encodings
    of text and images which can represent complex relationships, the high quality
    embeddings from the CLIP encoders can be co-opted for other tasks; I have an article
    which uses the image encoder from CLIP to enable language models to understand
    images, for instance:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然零样本分类相当酷（零样本指的是能够在未见过的数据类型上表现良好。例如，询问模型“这个人快乐吗”而模型从未被明确训练去检测快乐），提取和使用CLIP中的文本或图像编码器变得更加流行。由于CLIP模型被训练来创建文本和图像的微妙而强大的编码，这些编码可以表示复杂的关系，因此CLIP编码器生成的高质量嵌入可以被用于其他任务；例如，我有一篇文章使用CLIP的图像编码器来使语言模型理解图像：
- en: '[](/visual-question-answering-with-frozen-large-language-models-353d42791054?source=post_page-----1d02c07dbf40--------------------------------)
    [## Visual Question Answering with Frozen Large Language Models'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/visual-question-answering-with-frozen-large-language-models-353d42791054?source=post_page-----1d02c07dbf40--------------------------------)
    [## 使用冻结的大型语言模型进行视觉问答'
- en: Talking with LLMs about images, without training LLMs on images.
  id: totrans-44
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 与LLMs谈论图像，而不对LLMs进行图像训练。
- en: towardsdatascience.com](/visual-question-answering-with-frozen-large-language-models-353d42791054?source=post_page-----1d02c07dbf40--------------------------------)
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/visual-question-answering-with-frozen-large-language-models-353d42791054?source=post_page-----1d02c07dbf40--------------------------------)
- en: So, now we have a high level understanding of CLIP. Don’t worry if you don’t
    completely understand; in the next section we’ll break down CLIP component by
    component to build an intuitive understanding of how it functions.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，现在我们对CLIP有了一个高层次的理解。如果你还没完全明白也没关系；在下一节中，我们将逐个组件拆解CLIP，以建立对其功能的直观理解。
- en: The Components of CLIP
  id: totrans-47
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: CLIP的组件
- en: CLIP is a high level architecture which can use a variety of different sub components
    to achieve the same general results. We’ll be following the [CLIP paper](https://arxiv.org/pdf/2103.00020.pdf)
    and break down one of the possible approaches.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: CLIP是一个高层次的架构，可以使用各种不同的子组件来实现相同的一般结果。我们将遵循[CLIP论文](https://arxiv.org/pdf/2103.00020.pdf)，并拆解其中一种可能的方法。
- en: The Text Encoder
  id: totrans-49
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 文本编码器
- en: '![](../Images/247352f6c2bfdeb48bd18300440aba99.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/247352f6c2bfdeb48bd18300440aba99.png)'
- en: The Text Encoder within CLIP, [source](https://arxiv.org/pdf/2103.00020.pdf)
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: CLIP中的文本编码器，[source](https://arxiv.org/pdf/2103.00020.pdf)
- en: At its highest level, the text encoder converts input text into a vector (a
    list of numbers) that represents the text’s meaning.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在最高层次上，文本编码器将输入文本转换为一个表示文本含义的向量（一个数字列表）。
- en: '![](../Images/098f3763c93ebf5b7ea31e7747b16bad.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/098f3763c93ebf5b7ea31e7747b16bad.png)'
- en: The purpose of a text encoder, essentially
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 文本编码器的目的，本质上
- en: The text encoder within CLIP is a standard transformer encoder, which I cover
    intuitively and exhaustively in [another post](https://medium.com/towards-data-science/transformers-intuitively-and-exhaustively-explained-58a5c5df8dbb).
    For the purposes of this article, a transformer can be thought of as a system
    which takes an entire input sequence of words, then re-represents and compares
    those words to create an abstract, contextualized representation of the entire
    input. The self attention mechanism within a transformer is the main mechanism
    that creates that contextualized representation.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: CLIP中的文本编码器是标准的变压器编码器，我在[另一篇文章](https://medium.com/towards-data-science/transformers-intuitively-and-exhaustively-explained-58a5c5df8dbb)中直观而详尽地介绍了它。为了本文的目的，变压器可以被认为是一个系统，它接收整个输入序列的词，然后重新表示和比较这些词，以创建整个输入的抽象化、情境化表示。变压器中的自注意机制是创建这种情境化表示的主要机制。
- en: '![](../Images/d1e7ecefd0deb63387cdc360144073b5.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d1e7ecefd0deb63387cdc360144073b5.png)'
- en: Multi Headed Self Attention, the main operation in a transformer, takes an input
    sequence of words and converts them into an abstract representation. The end result
    can be conceptualized as containing “contextualized meaning” rather than a list
    of words. If you don’t know what a word vector embedding is, I cover them in [this
    post](https://medium.com/towards-data-science/transformers-intuitively-and-exhaustively-explained-58a5c5df8dbb).
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 多头自注意力，变压器中的主要操作，将输入序列的词转换为抽象表示。最终结果可以被概念化为包含“情境化意义”，而不是一个词列表。如果你不知道什么是词向量嵌入，我在[这篇文章](https://medium.com/towards-data-science/transformers-intuitively-and-exhaustively-explained-58a5c5df8dbb)中进行了介绍。
- en: One modification CLIP makes to the general transformer strategy is that it results
    in a vector, not a matrix, which is meant to represent the entire input sequence.
    It does this by simply extracting the vector for the last token in the input sequence.
    This works because the self attention mechanism is designed to contextualize each
    input with every other input. Thus, after passing through multiple layers of self
    attention, the transformer can learn to encode all necessary meaning into a single
    vector.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: CLIP对通用变压器策略的一项修改是，它生成的是向量而不是矩阵，旨在表示整个输入序列。它通过简单地提取输入序列中最后一个标记的向量来实现。这是有效的，因为自注意机制旨在将每个输入与其他输入进行情境化。因此，经过多层自注意力机制后，变压器可以学习将所有必要的意义编码到一个单一的向量中。
- en: '![](../Images/f70fe3977a7f5a744d47f043aa20209f.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f70fe3977a7f5a744d47f043aa20209f.png)'
- en: A conceptual diagram of CLIP’s text encoder; multiple multi-headed self attention
    layers manipulating the output into a final vector which represents the entire
    input. This final vector is used to represent the text input as a point in space,
    which is ultimately used to calculate the “closeness” between a caption and an
    image.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: CLIP文本编码器的概念图；多个多头自注意力层将输出操控为最终的向量，该向量表示整个输入。这个最终向量被用来将文本输入表示为空间中的一个点，这个点最终用于计算字幕与图像之间的“接近度”。
- en: Feel free to refer to my article on transformers for more in-depth information.
    In the next section we’ll talk about Image encoders, which convert an image into
    a representative vector.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 随时参阅我关于变压器的文章，以获得更深入的信息。在下一节中，我们将讨论图像编码器，它将图像转换为代表性向量。
- en: '[](/transformers-intuitively-and-exhaustively-explained-58a5c5df8dbb?source=post_page-----1d02c07dbf40--------------------------------)
    [## Transformers — Intuitively and Exhaustively Explained'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/transformers-intuitively-and-exhaustively-explained-58a5c5df8dbb?source=post_page-----1d02c07dbf40--------------------------------)
    [## 变压器 — 直观而详尽的解释'
- en: 'Exploring the modern wave of machine learning: taking apart the transformer
    step by step'
  id: totrans-63
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 探索现代机器学习的潮流：一步步拆解变压器
- en: towardsdatascience.com](/transformers-intuitively-and-exhaustively-explained-58a5c5df8dbb?source=post_page-----1d02c07dbf40--------------------------------)
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/transformers-intuitively-and-exhaustively-explained-58a5c5df8dbb?source=post_page-----1d02c07dbf40--------------------------------)
- en: The Image Encoder
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 图像编码器
- en: '![](../Images/b0b82f2d400dbf3227ddd7e78c187490.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b0b82f2d400dbf3227ddd7e78c187490.png)'
- en: The Image Encoder within CLIP, [source](https://arxiv.org/pdf/2103.00020.pdf)
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: CLIP中的图像编码器，[source](https://arxiv.org/pdf/2103.00020.pdf)
- en: At its highest level, the image encoder converts an image into a vector (a list
    of numbers) that represents the images meaning.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 在最高层次上，图像编码器将图像转换为代表图像含义的向量（数字列表）。
- en: '![](../Images/fccfa62c3ff527799061db0c4510c4cb.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fccfa62c3ff527799061db0c4510c4cb.png)'
- en: The purpose of an image encoder, essentially
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 图像编码器的目的，基本上
- en: There are a few approaches to image encoders which are discussed in the CLIP
    paper. In this article we’ll consider ResNET-50, a time tested convolutional approach
    which has been applied to several general image tasks. I’ll be covering ResNET
    in a future post, but for the purposes of this article we can just think of ResNET
    as a classic convolutional neural network.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: CLIP 论文中讨论了几种图像编码器的方法。在这篇文章中，我们将考虑 ResNET-50，这是一种经过时间考验的卷积方法，已应用于多个通用图像任务。我将在未来的文章中详细介绍
    ResNET，但在本文中，我们可以将 ResNET 视为经典的卷积神经网络。
- en: A convolutional neural network is a strategy of image modeling which filters
    an image with a small matrix of values called a kernel. It sweeps the kernel through
    the image and calculates a new value for each pixel based on the kernel and the
    input image.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积神经网络是一种图像建模策略，通过一个称为卷积核的小矩阵对图像进行滤波。它将卷积核在图像中滑动，并根据卷积核和输入图像计算每个像素的新值。
- en: '![](../Images/03a190777a4022dcf4c2a2f16693379b.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/03a190777a4022dcf4c2a2f16693379b.png)'
- en: A conceptual diagram of transforming an image via a Kernel. The kernel is placed
    in a certain spot within an image and multiplies the surrounding pixels by some
    value. It then adds those multiplied values together to calculate a new value
    for the image at that location. The values of each kernel are learnable, and a
    convolutional network uses many kernels. The end result is a network which learns
    to re-represent an input numerous different ways to extract meaning from an image.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 通过卷积核转换图像的概念图。卷积核被放置在图像的某个位置，并将周围的像素乘以某个值。然后，它将这些乘积值相加，以计算图像在该位置的新值。每个卷积核的值是可学习的，卷积网络使用许多卷积核。最终结果是一个网络，通过多种不同方式重新表示输入图像，以从中提取意义。
- en: The whole idea behind a convolutional network is, by doing a combination of
    convolutions and downsampling of an image, you can extract more and more subtle
    feature representations. Once an image has been condensed down to a small number
    of high quality abstract features, a dense network can then be used to turn those
    features into some final output. I talk about this more in depth in [another post](https://medium.com/towards-data-science/self-supervised-learning-using-projection-heads-b77af3911d33),
    specifically the role of the dense network at the end, called a projection head.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积网络的整个想法是，通过对图像进行卷积和下采样的组合，你可以提取越来越微妙的特征表示。一旦图像被压缩成少量高质量的抽象特征，就可以使用密集网络将这些特征转换为最终输出。我在[另一篇文章](https://medium.com/towards-data-science/self-supervised-learning-using-projection-heads-b77af3911d33)中深入讨论了这个问题，特别是最后的密集网络的角色，即投影头。
- en: '![](../Images/6638f4c22de25e4676e6472982139588.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6638f4c22de25e4676e6472982139588.png)'
- en: A classic convolutional architecture from the YOLO paper, an landmark object
    detection model. Each of these boxes describe an input images horizontal and vertical
    size, as well as their “depth”, in terms of number of features. The input image
    is an RGB image, so it has a depth of 3\. The following box has a “depth” of 192,
    which corresponds to having 192 kernels extracting different information from
    the input image. By extracting more and more features, and downsampling the image
    via maxpooling, the network distills the image into an abstract representation
    which is trained to hold some meaning about the image. [Source](https://arxiv.org/pdf/1506.02640.pdf)
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: YOLO 论文中的经典卷积架构，一个具有里程碑意义的目标检测模型。这些框描述了输入图像的水平和垂直尺寸，以及它们的“深度”，以特征数量来衡量。输入图像是
    RGB 图像，因此它的深度为 3。下一个框的“深度”为 192，这对应于有 192 个卷积核从输入图像中提取不同的信息。通过提取越来越多的特征，并通过最大池化对图像进行下采样，网络将图像提炼成一个抽象表示，并训练其对图像的某些含义。[Source](https://arxiv.org/pdf/1506.02640.pdf)
- en: From the perspective of CLIP, the end result ends up being a vector which can
    be thought of as a summary of the input image. This vector, along with the summary
    vectors for text, are then used in the next section to construct a multi-modal
    embedding space, which we’ll discuss in the next section.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 从 CLIP 的角度来看，最终结果是一个向量，可以被视为输入图像的摘要。这个向量以及文本的摘要向量将用于下一部分，以构建多模态嵌入空间，我们将在下一部分中讨论。
- en: '![](../Images/ea0083f794e7fb81368402142d0b087e.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ea0083f794e7fb81368402142d0b087e.png)'
- en: The image encoder used in CLIP in a nutshell. The image encoder converts an
    image into an abstract set of features with a convolutional network, then uses
    a dense, fully connected neural network to create the final output. In this case,
    the final output vector can be thought of as a summary of the entire input
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: CLIP 中使用的图像编码器简要说明。图像编码器通过卷积网络将图像转换为一组抽象特征，然后使用密集的全连接神经网络生成最终输出。在这种情况下，最终输出向量可以被认为是对整个输入的总结。
- en: The Multi-Modal Embedding Space, and CLIP Training
  id: totrans-81
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 多模态嵌入空间与 CLIP 训练
- en: '![](../Images/60c9abe0abfae206acbf7500d0508226.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/60c9abe0abfae206acbf7500d0508226.png)'
- en: The component of the CLIP training process which jointly aligns the embedding
    for both text and images, [source](https://arxiv.org/pdf/2103.00020.pdf)
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: CLIP 训练过程中的一个组件，它将文本和图像的嵌入进行联合对齐，[来源](https://arxiv.org/pdf/2103.00020.pdf)
- en: In the previous two sections we went over modeling strategies which can summarize
    text and images into vectors. In this section we’ll go over how CLIP uses those
    vectors to build strong image and language representations.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在前两节中，我们讨论了可以将文本和图像总结为向量的建模策略。在这一节中，我们将讨论 CLIP 如何利用这些向量构建强大的图像和语言表示。
- en: The idea of summarizing something complicated into an abstract vector is generally
    referred to as an “embedding”. We “embed” things like images and text into a vector
    as a way to summarize their general meaning.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 将复杂事物总结为抽象向量的想法通常被称为“嵌入”。我们将图像和文本等事物“嵌入”到向量中，以总结它们的一般意义。
- en: '![](../Images/349b9ad3f67390495d26ab333046ca39.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/349b9ad3f67390495d26ab333046ca39.png)'
- en: 'The image and text encoder “embedding” each input. Note: the length of these
    embeddings is typically very long, maybe 256 elements long.'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 图像和文本编码器将每个输入“嵌入”。注意：这些嵌入的长度通常非常长，可能长达256个元素。
- en: We can think of these embedding vectors as representing the input as some point
    in high dimensional space. For demonstrative purposes we can imagine creating
    encoders which embed their input into a vector of length two. These vectors could
    then be considered as points in a two dimensional space, and we could draw their
    positions.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将这些嵌入向量视为高维空间中某一点的表示。为了演示的目的，我们可以想象创建将输入嵌入到长度为二的向量的编码器。这些向量可以被视为二维空间中的点，我们可以绘制它们的位置。
- en: '![](../Images/5762566dcf10fb0ec4a3aa29757bd9f8.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5762566dcf10fb0ec4a3aa29757bd9f8.png)'
- en: an example of CLIP before it’s trained, with 2 dimensional embeddings for demonstrative
    purposes. Each of the images is passed through the image encoder to create a vector
    of length 2, and each of the input text is passed through the text encoder to
    create a vector of length 2
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: CLIP 训练前的示例，使用二维嵌入进行演示。每张图像都通过图像编码器生成一个长度为2的向量，每个输入文本都通过文本编码器生成一个长度为2的向量。
- en: We can think of this space as the **Multi-Modal Embedding Space**, and we can
    train CLIP (by training the image and text encoders) to put these points in spots
    such that positive pairs are close to each other.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将这个空间视为**多模态嵌入空间**，我们可以训练 CLIP（通过训练图像和文本编码器）使这些点的位置安排得使得正样本对彼此接近。
- en: '![](../Images/c622aeed706fa4a1869a766a6d032d74.png)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c622aeed706fa4a1869a766a6d032d74.png)'
- en: an example of CLIP after it’s trained, with 2 dimensional embeddings for demonstrative
    purposes. Notice how, once the encoders are trained, positive pairs end up close
    together
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: CLIP 训练后的示例，使用二维嵌入进行演示。注意，一旦编码器经过训练，正样本对最终会接近在一起。
- en: There are a lot of ways to define “close” in machine learning. Arguably the
    most common approach is cosine similarity, which is what CLIP employs. The idea
    behind cosine similarity is that we can say two vectors are similar if the angle
    between them is small.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习中定义“接近”的方式有很多种。可以说最常见的方法是余弦相似度，这是 CLIP 使用的方法。余弦相似度的思想是，如果两个向量之间的角度很小，我们可以说它们是相似的。
- en: '![](../Images/db9f4699f5656f690ee9e277c07db25c.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/db9f4699f5656f690ee9e277c07db25c.png)'
- en: If calculating similarity based on cosine similarity, the angle between A and
    B is small, and thus A and B are similar. C would be considered very different
    from both A and B
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 如果基于余弦相似度计算相似性，则 A 和 B 之间的角度较小，因此 A 和 B 是相似的。C 将被认为与 A 和 B 都非常不同。
- en: 'The term “cosine” comes from the cosine function, a trigonometry function which
    calculates the ratio of the adjacent leg of a right triangle with the hypotenuse
    based on some angle. If that sounds like gibberish, no big deal: if the angle
    is small between two vectors, the cosine between the two vectors will be close
    to 1\. If the vectors are 90 degrees apart, the cosine will be zero. If the vectors
    are pointing in opposite directions, the cosine will be -1\. As a result, with
    cosine, you get big numbers when the vectors point in the same direction, and
    you get small numbers when they don''t.'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: “余弦”这个术语来源于余弦函数，它是一种三角函数，根据某个角度计算直角三角形中邻边与斜边的比值。如果这听起来像是胡言乱语，也没关系：如果两个向量之间的角度很小，则它们之间的余弦值接近1。如果向量之间的角度为90度，余弦值为零。如果向量指向相反的方向，余弦值为-1。结果是，当向量朝同一方向时，你会得到大的数值，而当它们不朝同一方向时，你会得到小的数值。
- en: '![](../Images/51c9fc4ff63cb8475e38af90feb9e307.png)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/51c9fc4ff63cb8475e38af90feb9e307.png)'
- en: A conceptual diagram for how the cosine wave (the wave on the bottom) relates
    to the anglue within a right triangle. It’s really not that important to understand
    the cosine function completely for this article, only that it’s biggest when the
    angle between things is 0, and smallest when things point in opposite directions.
    If you’re curious about learning more about trigonometry and waves, you can refer
    to [this article](https://medium.com/towards-data-science/use-frequency-more-frequently-14715714de38)
    I wrote on how frequency analysis relates to machine learning
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个关于余弦波（底部的波形）如何与直角三角形中的角度相关的概念图。对本文而言，完全理解余弦函数并不是非常重要，只需了解当两个事物之间的角度为0时，余弦值最大，而当两个事物朝相反方向时，余弦值最小。如果你对学习更多关于三角学和波动的内容感兴趣，可以参考我写的[这篇文章](https://medium.com/towards-data-science/use-frequency-more-frequently-14715714de38)，讲述了频率分析如何与机器学习相关。
- en: 'The cosine of the angle between two vectors can be calculated by measuring
    the angle between them and then passing that angle through the cosine function.
    Printing out all the vectors and measuring the angle between them using a protractor
    might slow down our training time, though. Luckily, we can use the following identity
    to calculate the cosine of the angle between two vectors:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 两个向量之间的夹角的余弦值可以通过测量它们之间的角度，然后将该角度通过余弦函数计算来得到。不过，打印出所有向量并使用量角器测量它们之间的角度可能会拖慢我们的训练时间。幸运的是，我们可以使用以下恒等式来计算两个向量之间夹角的余弦值：
- en: '![](../Images/946b0df176cdf7fb2d3d1ba4d762ff0f.png)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/946b0df176cdf7fb2d3d1ba4d762ff0f.png)'
- en: '[source](/cosine-similarity-how-does-it-measure-the-similarity-maths-behind-and-usage-in-python-50ad30aad7db)'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '[来源](/cosine-similarity-how-does-it-measure-the-similarity-maths-behind-and-usage-in-python-50ad30aad7db)'
- en: 'If you were already daunted by math, you might feel even more daunted now.
    But I’ll break it down:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你已经觉得数学很复杂，你现在可能会觉得更复杂。但是我会将其拆解：
- en: The phrase **A•B** represents the dot product between vector A and B. The dot
    product is what you get when you multiply every element in A by the corresponding
    element in B, then sum all the results. So if A=[1,2,3], and B=[2,3,4], then A•B
    = (1x2) + (2x3) + (3x4).
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 短语**A•B**表示向量A和B之间的点积。点积是指将A中的每个元素与B中对应的元素相乘，然后将所有结果相加。所以如果A=[1,2,3]，B=[2,3,4]，则A•B
    = (1x2) + (2x3) + (3x4)。
- en: The phrase“||(some vector)||”, as in **||A||** or **||B||**, represents the
    calculation of the norm of a vector. This is simply the magnitude, or length,
    of the vector. The length of a vector can be calculated by taking the square root
    of the sum of squares in a vector. So, for A=[1,2,3], ||A|| = sqrt(1² + 2² + 3²)
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 短语“||(某个向量)||”，如**||A||**或**||B||**，表示向量的范数计算。这只是向量的大小或长度。向量的长度可以通过计算向量中各个分量的平方和的平方根来得到。所以，对于A=[1,2,3]，||A||
    = sqrt(1² + 2² + 3²)。
- en: You can, conceptually, think of the numerator **A•B** as the similarity, while
    the denominator **||A||||B||** divides the similarity by the lengths of the vectors.
    This division makes the cosine similarity only change based on the angle between
    vectors, and not their size. Without the denominator reigning things in, **A•B**
    would be bigger if A and B got longer, regardless of their direction.
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从概念上讲，可以将分子**A•B**视为相似性，而分母**||A||||B||**将相似性除以向量的长度。这种除法使得余弦相似性仅根据向量之间的角度变化，而不依赖于它们的大小。如果没有分母进行调整，**A•B**
    的值会随着A和B的长度增加而增大，而不管它们的方向。
- en: If we look back at our original diagram, we might notice that we’re calculating
    dot products between the embedding of images and text
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们回顾原始图示，可能会注意到我们正在计算图像和文本的嵌入之间的点积。
- en: '![](../Images/60c9abe0abfae206acbf7500d0508226.png)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/60c9abe0abfae206acbf7500d0508226.png)'
- en: The calculation of distance between image and text representations in CLIP.
    notice how the dot product, the numerator of the cosine similarity, is calculated
    between each embedding for text and images [source](https://arxiv.org/pdf/2103.00020.pdf).
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: CLIP中图像和文本表示之间距离的计算。注意点积，即余弦相似度的分子，是如何在文本和图像的每个嵌入之间计算的。[来源](https://arxiv.org/pdf/2103.00020.pdf)。
- en: Because of the way loss is calculated, all the image and text vectors will have
    a length of 1 and, as a result, we can forgo dividing by the magnitude of the
    vectors. So, while we’re not dividing by the denominator, this is still conceptually
    trained using cosine similarity (we’ll touch on this more in the next section).
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 由于损失的计算方式，所有图像和文本向量的长度都将为1，因此我们可以省略除以向量大小的步骤。因此，虽然我们没有除以分母，但这仍然是通过余弦相似度进行概念上的训练（我们将在下一部分详细讨论）。
- en: Now that we have an idea of how we go from image and text to embedding vectors,
    and how those embedding vectors get used to calculate similarity, we can look
    a bit more in-depth at how training practically shakes out by exploring how CLIP
    uses contrastive loss.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们对如何将图像和文本转换为嵌入向量以及如何使用这些嵌入向量来计算相似度有了一定了解，我们可以更深入地探讨训练实际情况，了解CLIP如何使用对比损失。
- en: CLIP and contrastive loss
  id: totrans-112
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: CLIP和对比损失
- en: The whole idea of contrastive loss is, instead of looking at individual examples
    to try to boost performance on a per-pair basis, you can instead think of the
    problem as incrementally improving the closeness of positive pairs while, simultaneously,
    preserving the distance of negative pairs.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 对比损失的整体思想是，不是单独查看每个示例以尝试在每对样本上提升性能，而是将问题视为逐步提高正样本对的相似度，同时保持负样本对的距离。
- en: '![](../Images/b88d381d67f59b3480cb94fedd4dfd12.png)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b88d381d67f59b3480cb94fedd4dfd12.png)'
- en: A traditional supervised approach on the left, and contrastive approach on the
    right. The supervised approach works on individual input-output pairs (usually
    in the form of a mini-batch, but still a grouping of one to one pairs), while
    the contrastive approach pairs every possible pair together and tries to raise
    the closeness of positive pairs (highlighted in blue) and reduce the closeness
    of negative pairs.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 左侧是传统的监督方法，右侧是对比方法。监督方法处理单个输入-输出对（通常以小批量的形式出现，但仍然是一对一的分组），而对比方法将所有可能的对配对在一起，并尝试提高正样本对的接近性（用蓝色突出显示），同时减少负样本对的接近性。
- en: In CLIP this is done by computing the dot product between the encoded text and
    image representations which, as we discussed previously, can be used to quantify
    “closeness”.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 在CLIP中，这是通过计算编码文本和图像表示之间的点积来完成的，正如我们之前讨论的，这可以用来量化“接近性”。
- en: This shift in thinking towards contrastive learning is really what makes CLIP
    so powerful. There are a lot of ways the caption for an image can be generated;
    a picture can be captioned “A cat lying down”, “feline relaxing”, “little cutie
    george taking a nap”, whatever. It’s difficult to predict the actual caption of
    an image, but by using “closeness” and “farness”, CLIP can deal with this problem
    elegantly.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 这种转变的思维方式使得对比学习真正赋予CLIP强大的能力。图像的描述可以有很多种方式；一张图片可以被描述为“猫躺下了”、“猫咪放松中”、“小可爱乔治在打盹”等等。虽然很难预测图像的实际描述，但通过使用“接近性”和“远离性”，CLIP能够优雅地处理这个问题。
- en: CLIP uses 32,768 image-text pairs every single batch, which is a pretty big
    batch size compared to a more traditional approach which might see a batch size
    of 16–64\. As a result, CLIP is forced to be really good at getting positive pairs
    far away from a large number of negative pairs, which is what makes CLIP so robust.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: CLIP在每个批次中使用32,768对图像-文本，这比传统方法的批次大小（通常为16–64）要大得多。因此，CLIP必须非常擅长将正样本对从大量负样本对中分离开来，这也是CLIP如此强大的原因。
- en: 'in order to train a neural network you need a single value for performance,
    called the “loss”, which you can use to update the model. Each training step you
    update the model’s parameters such that the model would output a smaller loss
    value at that step. The CLIP paper includes the following pseudo code which describes
    how loss is calculated within CLIP:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 为了训练神经网络，你需要一个性能的单一值，称为“损失”，你可以用它来更新模型。在每次训练步骤中，你更新模型的参数，使得模型在该步骤输出一个更小的损失值。CLIP论文中包括了以下伪代码，描述了CLIP中损失的计算方式：
- en: '[PRE0]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Breaking this down into components:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 将这一点分解成组件：
- en: First we get a batch of images of dimension `[batch size, image height, image
    width, number of colors]` and a batch of text of dimension `[batch size, sequence
    length]`. These batches are aligned with each other, such that every image in
    the batch of images corresponds with every piece of text in the batch of text.
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们获取一批维度为 `[batch size, image height, image width, number of colors]` 的图像和一批维度为
    `[batch size, sequence length]` 的文本。这些批次彼此对齐，使得图像批次中的每个图像与文本批次中的每一段文本对应。
- en: We pass these through our encoder, which results in some vector for each image
    and each piece of text in their respective batches.
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将这些通过我们的编码器，这样每个图像和每段文本在各自的批次中就会生成一个向量。
- en: In order for the images and text to be placed in the same embedding space, the
    vectors are passed through a linear projection. This can be thought of as a single
    layer dense network without biases or an activation function. The CLIP paper talks
    about how the details of this aren’t super important, just that both image and
    text vectors end up being the same length. These vectors are normalized using
    l2 normalization, which keeps the vectors pointing in the same direction but compresses
    all vectors to a length of one.
  id: totrans-124
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了将图像和文本放置在相同的嵌入空间中，这些向量通过线性投影。这可以被视为一个没有偏差或激活函数的单层全连接网络。CLIP 论文提到，这些细节并不是特别重要，重要的是图像和文本向量的长度最终相同。这些向量使用
    l2 归一化进行归一化，这使得向量保持指向相同的方向，但将所有向量压缩为长度为一。
- en: Because all embedding vectors are of length 1, there’s no need to divide by
    their magnitude to calculate cosine similarity. Thus, the dot product between
    the embedding vectors is equivalent to cosine similarity. The cosine similarity
    is multiplied by a temperature parameter, which controls how intensely the similarities
    effect a given training epoch.
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 由于所有嵌入向量的长度为 1，因此不需要通过其大小来计算余弦相似度。因此，嵌入向量之间的点积等同于余弦相似度。余弦相似度乘以一个温度参数，该参数控制相似度在给定训练周期中的影响强度。
- en: loss is calculated across text and images symmetrically via cross entropy loss.
    This is mentioned offhandedly in the CLIP paper, but the details can be a bit
    tricky.
  id: totrans-126
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 损失是通过交叉熵损失在文本和图像之间对称地计算的。这在 CLIP 论文中有提到，但细节可能有些复杂。
- en: 'In researching for this article, I found the following expression for cross
    entropy loss:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 在研究这篇文章时，我发现了以下交叉熵损失的表达式：
- en: '![](../Images/483ce6a39b09a29d86e7d37622c0077c.png)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/483ce6a39b09a29d86e7d37622c0077c.png)'
- en: Cross entropy loss, where “t” is the value of some true label and “p” is the
    value of some prediction
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 交叉熵损失，其中“t”是某个真实标签的值，“p”是某个预测的值。
- en: 'This idea is all well and good, but cosin similarity goes from -1 to 1 as we
    previously discussed, and you can’t take the log of a negative number. The CLIP
    paper mentions the following:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 这个想法很好，但正如我们之前讨论的，余弦相似度的范围是 -1 到 1，而你不能对负数取对数。CLIP 论文提到如下内容：
- en: The cosine similarity of these embeddings is then calculated, scaled by a temperature
    parameter τ , and normalized into a probability distribution via a softmax. —
    [CLIP](https://arxiv.org/pdf/2103.00020.pdf)
  id: totrans-131
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 这些嵌入的余弦相似度随后被计算，通过温度参数 τ 进行缩放，并通过 softmax 归一化为概率分布。— [CLIP](https://arxiv.org/pdf/2103.00020.pdf)
- en: 'Using this information, as well as information from the pseudo code:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 利用这些信息，以及伪代码中的信息：
- en: '[PRE1]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: We can infer that the `cross_entropy_loss` function (as a function specified
    in the pseudo code) contains a softmax operation over the specified axis. This
    is a fiddly detail, but it’s important in making CLIP train effectively.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以推断出 `cross_entropy_loss` 函数（作为伪代码中指定的函数）包含在指定轴上的 softmax 操作。这是一个细节，但它在使
    CLIP 有效训练中很重要。
- en: For those who might not be aware, a softmax function takes a vector of some
    values and turns it into a positive vector with a sum of components equal to one.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 对于那些可能不太了解的人，softmax 函数将一组值的向量转换为一个正向量，其组件的总和等于一。
- en: '![](../Images/0ebfce23fb63b16cbecdc3883720c1c5.png)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0ebfce23fb63b16cbecdc3883720c1c5.png)'
- en: The softmax function. The use of exp has all sorts of nice properties, but essentially
    the softmax function squashes down an input vector such that elements in the vector
    are between 0 and 1, and the sum of all elements in the vector is 1
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: softmax 函数。使用 exp 具有各种良好的属性，但本质上，softmax 函数将输入向量压缩，使得向量中的元素在 0 和 1 之间，且向量中所有元素的总和为
    1。
- en: 'This is already getting a bit math heavy, and I don’t think a complete mathematical
    understanding of softmax is fundamentally crucial. Let’s just look at a few examples:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 这已经有点数学复杂了，我认为对 softmax 的完整数学理解并不是根本性的关键。让我们看看几个例子：
- en: '![](../Images/61aef7e167267b475ec7ae07d701cb9e.png)'
  id: totrans-139
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/61aef7e167267b475ec7ae07d701cb9e.png)'
- en: Four example applications of the softmax function. The softmax function maps
    all elements in a vector to between 0 and 1, and ensures that the sum of all mapped
    elements is equal to 1\. The result of a the softmax function can be thought of
    as a set of probabilities. (there is a typo in the third example. It should be
    [0.67, 0.24, 0.09] thanks for catching that and letting me know in the comments,
    Anna)
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: Softmax 函数的四个示例应用。Softmax 函数将向量中的所有元素映射到 0 和 1 之间，并确保所有映射元素的总和等于 1。softmax 函数的结果可以看作是一组概率。（第三个例子中有一个错字，应为
    [0.67, 0.24, 0.09]，感谢 Anna 提醒我并在评论中告知我。）
- en: The softmax function allows us to take our general idea of “closeness” from
    cosin distance values between -1 and 1, and turn them into a vector of probabilities,
    which can be interpreted as “belongs together”, between 0 and 1.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: Softmax 函数允许我们将余弦距离值在 -1 和 1 之间的“接近度”转换为概率向量，概率值在 0 和 1 之间，可以解释为“属于一组”。
- en: 'This “belongs together” probability can be calculated two ways:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 这种“属于一组”的概率可以通过两种方式计算：
- en: we can take the softmax of the cosin distance across the text axis, thus calculating
    the probability that text belongs to an image
  id: totrans-143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以对文本轴上的余弦距离进行 softmax，从而计算文本属于图像的概率。
- en: we can take the softmax of the cosin distance across the image axis, thus calculating
    the probability that an image belongs to text
  id: totrans-144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以对图像轴上的余弦距离进行 softmax，从而计算图像属于文本的概率。
- en: '![](../Images/0c20901ea6045acd327640fc5b7d3887.png)'
  id: totrans-145
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0c20901ea6045acd327640fc5b7d3887.png)'
- en: Two approaches to calculating probabilities. We can calculate the probability
    that a certain piece of text belongs to an image (top), or we can calculate the
    probability that a certain image belongs to a piece of text (bottom)
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 计算概率的两种方法。我们可以计算某段文本属于图像的概率（上图），或者我们可以计算某张图像属于一段文本的概率（下图）。
- en: This is why the `cross_entropy_loss` function in CLIP’s pseudo code contains
    an `axis` argument; the softmax can either be calculated horizontally or vertically
    to calculate one of two loss calculations.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是 CLIP 伪代码中的 `cross_entropy_loss` 函数包含 `axis` 参数的原因；softmax 可以水平或垂直计算，以进行两种损失计算之一。
- en: Now that we have probabilities ranging between 0 and 1, we can use our cross
    entropy function to calculate loss. This will be the training objective which
    we will attempt to minimize.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们得到的概率范围在 0 和 1 之间，我们可以使用交叉熵函数来计算损失。这将是我们的训练目标，我们将尝试最小化它。
- en: '![](../Images/483ce6a39b09a29d86e7d37622c0077c.png)'
  id: totrans-149
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/483ce6a39b09a29d86e7d37622c0077c.png)'
- en: Cross entropy loss, where “t” is the value of some true label and “p” is the
    value of some prediction. This particular expression is for a vector of true values
    and a vector of predictions. Just as easily, this expression could be expanded
    as the sum of losses across a matrix, or two matrices as in our case.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 交叉熵损失，其中“t”是某个真实标签的值，“p”是某个预测的值。这个特定的表达式适用于真实值向量和预测向量。这个表达式也可以扩展为矩阵或两个矩阵的损失之和，正如我们在这里的情况。
- en: We can go through, element by element in each matrix, calculating the loss for
    that element. Then we can sum all the losses across both matrices to compute the
    total loss.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以逐个计算每个矩阵中的元素的损失。然后我们可以将两个矩阵中的所有损失加起来以计算总损失。
- en: '![](../Images/3e630310e5a983a139e04785fad3f72e.png)'
  id: totrans-152
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3e630310e5a983a139e04785fad3f72e.png)'
- en: Going from probabilities via the two softmax approaches to a single loss value
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 通过两种 softmax 方法将概率转换为单一损失值。
- en: The astute among you might realize a peculiar incompatibility here. The whole
    point of contrastive learning is that you’re learning to optimize both positive
    and negative pairs. We want to push positive pairs close together, while pushing
    negative pairs apart. How can we learn to push negative pairs apart if their loss
    (the thing we’re optimizing) is zero no matter what we do? (negative pairs are
    identified as having a true value of zero, which sets the log loss of negative
    pairs to always be zero)
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 聪明的人可能会意识到这里的一个特殊不兼容性。对比学习的整个要点是你正在学习优化正对和负对。我们希望将正对推得更近，同时将负对推得更远。如果负对的损失（我们优化的内容）无论我们做什么都是零，我们怎么能学习将负对推得更远呢？（负对被识别为真实值为零，这使得负对的对数损失总是为零）
- en: 'This is a sneaky but incredibly important characteristic of the softmax function:
    **when the probability of negative pairs get bigger, the probability of positive
    pairs get smaller as a direct consequence.** As a result, by optimizing for positive
    pairs to be as close to 1 as possible, we’re also optimizing for the cosine distance
    between negative pairs to be as small as possible.'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 这是 softmax 函数的一个巧妙但极其重要的特性：**当负对的概率增大时，正对的概率会直接减少。** 结果，通过优化正对的概率尽可能接近 1，我们也在优化负对之间的余弦距离尽可能小。
- en: Using CLIP
  id: totrans-156
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 CLIP
- en: I touched on these use cases of CLIP previously, but I wanted to reiterate now
    that we have a more thorough understanding of CLIP.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 我之前提到过 CLIP 的这些使用场景，但现在我们对 CLIP 有了更深入的了解，我想重申一下。
- en: '**Use 1: Image Classifier**'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: '**用法 1: 图像分类器**'
- en: Given an input image, we can pass a variety of textual descriptions to CLIP
    and calculate which description best represents the image. We can do this by passing
    the image through the image encoder, all text through the text encoder, and calculate
    the cosine similarity between the image and all text inputs via a dot product
    of their embedding. We can then compute the softmax across all similarity values
    to compute a probability that a piece of text belongs to an image.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个输入图像，我们可以将各种文本描述传递给 CLIP，计算哪个描述最能代表图像。我们可以通过将图像传递通过图像编码器，将所有文本传递通过文本编码器，并通过它们的嵌入的点积计算图像和所有文本输入之间的余弦相似度。然后，我们可以计算所有相似度值的
    softmax，以计算一段文本属于某个图像的概率。
- en: '**Use 2: Image Search**'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: '**用法 2: 图像搜索**'
- en: Similarly to building an image classifier, we can pass some phrase into the
    text encoder, multiple images to the image encoder, compute the dot product and
    softmax, and thus get a probability of which image is most relevant to a piece
    of text.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于构建图像分类器，我们可以将一些短语传递到文本编码器，将多张图像传递到图像编码器，计算点积和 softmax，从而获得哪个图像与一段文本最相关的概率。
- en: '**Use 3: Image Encoder**'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '**用法 3: 图像编码器**'
- en: Because CLIP is good at representing the content of images generally, we can
    use the image encoder for downstream tasks. I cover an example of this [here](https://medium.com/towards-data-science/visual-question-answering-with-frozen-large-language-models-353d42791054).
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 CLIP 在一般情况下能够很好地表示图像内容，我们可以将图像编码器用于下游任务。我在[这里](https://medium.com/towards-data-science/visual-question-answering-with-frozen-large-language-models-353d42791054)介绍了一个示例。
- en: '**Use 4: Text Encoder**'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: '**用法 4: 文本编码器**'
- en: Because CLIP is good at representing understanding which aspects of a linguistic
    phrase are relevant to images, we can use the text encoder for downstream tasks.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 CLIP 在理解语言短语的哪些方面与图像相关方面表现出色，我们可以将文本编码器用于下游任务。
- en: Companion Piece
  id: totrans-166
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 附件
- en: Check out the companion piece for this article, where I use a CLIP style model
    to implement two types of image search
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 查看这篇文章的附件，其中我使用 CLIP 风格的模型实现了两种类型的图像搜索
- en: '[](/image-search-in-5-minutes-9bc4f903b22a?source=post_page-----1d02c07dbf40--------------------------------)
    [## Image Search in 5 Minutes'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/image-search-in-5-minutes-9bc4f903b22a?source=post_page-----1d02c07dbf40--------------------------------)
    [## 5分钟内的图像搜索'
- en: Cutting-edge image search, simply and quickly
  id: totrans-169
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 前沿的图像搜索，简单而迅速
- en: towardsdatascience.com](/image-search-in-5-minutes-9bc4f903b22a?source=post_page-----1d02c07dbf40--------------------------------)
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: '[towardsdatascience.com](/image-search-in-5-minutes-9bc4f903b22a?source=post_page-----1d02c07dbf40--------------------------------)'
- en: Conclusion
  id: totrans-171
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: And that’s it! Good job for sticking in there. CLIP is fascinating and incredibly
    powerful, but because it’s so fundamentally different than more straightforward
    approaches it can be difficult to wrap your head around.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 就这些了！做得好，坚持下来了。CLIP 是非常迷人和强大的，但因为它在本质上与更直接的方法差异很大，所以可能很难理解。
- en: In this post we covered the high level reasons of why CLIP exists and what it
    generally does. Then we broke CLIP down into three components; the image encoder,
    the text encoder, and the jointly aligned embedding space used to bridge the two
    together. We went over the high level intuition of how CLIP uses large batches
    to create numerous positive and negative pairs, then we drilled down into the
    loss function used to optimize CLIP.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我们讨论了CLIP存在的高层原因以及它的一般功能。然后我们将CLIP分解为三个组件：图像编码器、文本编码器以及用于将两者连接在一起的共同对齐嵌入空间。我们讲解了CLIP如何利用大量的批次创建众多正负样本的高层直觉，然后深入探讨了用于优化CLIP的损失函数。
- en: Follow For More!
  id: totrans-174
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 关注获取更多内容！
- en: I describe papers and concepts in the ML space, with an emphasis on practical
    and intuitive explanations. I plan on implementing CLIP from scratch in a future
    post.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 我描述了机器学习领域的论文和概念，重点在于实用和直观的解释。我计划在未来的文章中从头实现CLIP。
- en: '[](https://medium.com/@danielwarfield1/subscribe?source=post_page-----1d02c07dbf40--------------------------------)
    [## Get an email whenever Daniel Warfield publishes'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@danielwarfield1/subscribe?source=post_page-----1d02c07dbf40--------------------------------)
    [## 每当**丹尼尔·沃菲尔德**发布新文章时，获取邮件'
- en: High quality data science articles straight to your inbox. Get an email whenever
    Daniel Warfield publishes. By signing up, you…
  id: totrans-177
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 高质量的数据科学文章直接送到你的邮箱。每当**丹尼尔·沃菲尔德**发布新文章时，你都会收到邮件。通过注册，你…
- en: medium.com](https://medium.com/@danielwarfield1/subscribe?source=post_page-----1d02c07dbf40--------------------------------)
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: medium.com](https://medium.com/@danielwarfield1/subscribe?source=post_page-----1d02c07dbf40--------------------------------)
- en: '**Attribution:** All of the resources in this document were created by Daniel
    Warfield, unless a source is otherwise provided. You can use any resource in this
    post for your own non-commercial purposes, so long as you reference this article,
    [https://danielwarfield.dev](https://danielwarfield.dev/), or both. An explicit
    commercial license may be granted upon request.'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: '**版权声明：** 本文档中的所有资源均由**丹尼尔·沃菲尔德**创建，除非另有来源说明。你可以将本文中的任何资源用于个人非商业用途，只要你引用了这篇文章，[https://danielwarfield.dev](https://danielwarfield.dev/)，或两者都引用。应要求可以提供明确的商业许可。'
