- en: 'Curse of Dimensionality: An Intuitive Exploration'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**维度诅咒：直观探索**'
- en: 原文：[https://towardsdatascience.com/curse-of-dimensionality-an-intuitive-exploration-1fbf155e1411?source=collection_archive---------3-----------------------#2023-12-30](https://towardsdatascience.com/curse-of-dimensionality-an-intuitive-exploration-1fbf155e1411?source=collection_archive---------3-----------------------#2023-12-30)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/curse-of-dimensionality-an-intuitive-exploration-1fbf155e1411?source=collection_archive---------3-----------------------#2023-12-30](https://towardsdatascience.com/curse-of-dimensionality-an-intuitive-exploration-1fbf155e1411?source=collection_archive---------3-----------------------#2023-12-30)
- en: '[](https://medium.com/@salih.salih?source=post_page-----1fbf155e1411--------------------------------)[![Salih
    Salih](../Images/220f3c5363989d94c5593eca7ff72c67.png)](https://medium.com/@salih.salih?source=post_page-----1fbf155e1411--------------------------------)[](https://towardsdatascience.com/?source=post_page-----1fbf155e1411--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----1fbf155e1411--------------------------------)
    [Salih Salih](https://medium.com/@salih.salih?source=post_page-----1fbf155e1411--------------------------------)'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@salih.salih?source=post_page-----1fbf155e1411--------------------------------)[![Salih
    Salih](../Images/220f3c5363989d94c5593eca7ff72c67.png)](https://medium.com/@salih.salih?source=post_page-----1fbf155e1411--------------------------------)[](https://towardsdatascience.com/?source=post_page-----1fbf155e1411--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----1fbf155e1411--------------------------------)
    [Salih Salih](https://medium.com/@salih.salih?source=post_page-----1fbf155e1411--------------------------------)'
- en: ·
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F2037cbb08e24&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcurse-of-dimensionality-an-intuitive-exploration-1fbf155e1411&user=Salih+Salih&userId=2037cbb08e24&source=post_page-2037cbb08e24----1fbf155e1411---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----1fbf155e1411--------------------------------)
    ·11 min read·Dec 30, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F1fbf155e1411&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcurse-of-dimensionality-an-intuitive-exploration-1fbf155e1411&user=Salih+Salih&userId=2037cbb08e24&source=-----1fbf155e1411---------------------clap_footer-----------)'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F2037cbb08e24&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcurse-of-dimensionality-an-intuitive-exploration-1fbf155e1411&user=Salih+Salih&userId=2037cbb08e24&source=post_page-2037cbb08e24----1fbf155e1411---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----1fbf155e1411--------------------------------)
    ·11 分钟阅读·2023年12月30日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F1fbf155e1411&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcurse-of-dimensionality-an-intuitive-exploration-1fbf155e1411&user=Salih+Salih&userId=2037cbb08e24&source=-----1fbf155e1411---------------------clap_footer-----------)'
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F1fbf155e1411&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcurse-of-dimensionality-an-intuitive-exploration-1fbf155e1411&source=-----1fbf155e1411---------------------bookmark_footer-----------)![](../Images/8e1fab29575e7aec7aa57966239b2c2f.png)'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F1fbf155e1411&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcurse-of-dimensionality-an-intuitive-exploration-1fbf155e1411&source=-----1fbf155e1411---------------------bookmark_footer-----------)![](../Images/8e1fab29575e7aec7aa57966239b2c2f.png)'
- en: Photo by Mathew Schwartz on Unsplash
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 照片由 Mathew Schwartz 提供，来源于 Unsplash
- en: '**Introduction**'
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**介绍**'
- en: In the previous article, we discussed the [surprising behavior of data in higher
    dimensions](/the-surprising-behavior-of-data-in-higher-dimensions-1c49bca9bbee).
    We found that volume tends to accumulate in the corners of spaces in a strange
    way, and we simulated a hypersphere inscribed inside a hypercube to investigate
    this, observing an interesting decrease in their volume ratio as the dimensions
    grew. Examples that demonstrated the advantages of multi-dimensional thinking
    were the DVD-paper experiment and the kernel trick in support vector machines(SVMs).
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一篇文章中，我们讨论了[数据在高维空间中的惊人行为](/the-surprising-behavior-of-data-in-higher-dimensions-1c49bca9bbee)。我们发现体积在空间的角落里以一种奇怪的方式累积，我们模拟了一个内切在超立方体中的超球体来研究这一点，观察到随着维度的增加，它们的体积比有趣地减少了。展示多维思维优势的例子有
    DVD 纸实验和支持向量机(SVM)中的核技巧。
- en: Today, we will be looking at some of the difficult aspects of high-dimensional
    data which is referred to as curse of dimensionality. Our goal is to have an intuitive
    understanding of this concept and its practical implications. The diagram below
    outlines how our article is structured.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 今天，我们将探讨一些高维数据的难点，这些难点被称为维度诅咒。我们的目标是对这一概念及其实际影响有一个直观的理解。下图概述了我们文章的结构。
- en: '![](../Images/0bcf5fff1009e705ae4a9da30c8ce22e.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0bcf5fff1009e705ae4a9da30c8ce22e.png)'
- en: Image by Author
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 作者插图
- en: Understanding the Curse of Dimensionality
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解维度诅咒
- en: “Curse of dimensionality” is a term that was first used by Richard E. Bellman
    back in the 1960s. It began as Bellman’s idea from dynamic optimization and it
    turned out to be a fundamental concept for understanding complexity in high-dimensional
    spaces.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: “维度诅咒”是由**理查德·E·贝尔曼**在1960年代首次使用的术语。它起初源于贝尔曼对动态优化的观点，后来成为理解高维空间复杂性的一个基本概念。
- en: Good, but what is “curse of dimensionality”?
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 很好，但“维度诅咒”是什么？
- en: It is at its core the difficulties and unique characteristics one faces when
    working with data in high-dimensional spaces( in our case this refers to having
    many features, columns or attributes in datasets). These spaces go far beyond
    our experience of everyday life in three-dimensional space.
  id: totrans-16
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 这本质上是指在处理高维数据时遇到的困难和独特特性（在我们的案例中，这指的是数据集中具有许多特征、列或属性）。这些空间远远超出了我们在三维空间中的日常经验。
- en: When we increase the number of dimensions on a dataset, the volume it occupies
    expands exponentially. This might appear initially as an advantage — more space
    could mean more data and probably more insights? However, that is not the case
    because having many dimensions comes with a number of challenges which change
    how we need to deal with and understand these high-dimensional data.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们增加数据集的维度时，它所占据的体积会以指数级扩展。这最初可能看起来是一个优势——更多的空间可能意味着更多的数据和可能更多的见解？然而，事实并非如此，因为维度的增加带来了许多挑战，这些挑战改变了我们处理和理解这些高维数据的方式。
- en: Key Challenges
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 主要挑战
- en: 'The shift from low-dimensional to high-dimensional data faces several harsh
    challenges. There are two, which stand out because they have the most significant
    effects: 1) sparsity of data; 2) the issue with distance metric. Each of them
    makes analysis in higher dimensions even more complex.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 从低维数据到高维数据的转变面临着几个严峻的挑战。其中两个尤为突出，因为它们的影响最大：1) 数据稀疏性；2) 距离度量问题。它们每一个都使高维数据的分析变得更加复杂。
- en: 'Data Sparsity: Islands in an Ocean of Emptiness'
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据稀疏性：空旷海洋中的小岛
- en: Data sparsity in highly dimensional spaces is like few small islands lost within
    a vast ocean. When dimensionality increases, data points that were close together
    in lower dimensions become increasingly separated. This is due to the fact that
    the amount of space expands exponentially with each new addition of another dimension.
    Just imagine a cube becoming a hypercube; its corners move further away from its
    center and make it more empty inside. This growing emptiness is what we refer
    to as data sparsity.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 高维空间中的数据稀疏性就像是辽阔海洋中的几个小岛。当维度增加时，在低维空间中彼此接近的数据点会变得越来越分离。这是因为每新增一个维度，空间的扩展量是指数级增长的。想象一下一个立方体变成一个超立方体；它的角落远离中心，使得内部变得更加空旷。这种不断增长的空旷感就是我们所称的数据稀疏性。
- en: Many data analysis techniques struggle with sparsity. For example, many clustering
    algorithms depend on closely situated data points to form meaningful clusters.
    However, when data points become too dispersed, these algorithms face difficulties.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 许多数据分析技术在面对稀疏性时都会遇到困难。例如，许多聚类算法依赖于相互接近的数据点来形成有意义的簇。然而，当数据点变得过于分散时，这些算法就会面临困难。
- en: 'Distance Metric Problems: When Proximity Loses Meaning'
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 距离度量问题：当接近失去意义时
- en: In high-dimensional spaces, distance metrics encounter significant challenges.
    Metrics like Euclidean or Manhattan distances, which are useful for measuring
    proximity between data points in lower dimensions, lose their effectiveness. In
    these expanded spaces, distances start to converge. This means that most pairs
    of points become nearly equidistant from each other and from a reference point.
    This convergence makes it harder to distinguish between close neighbors and distant
    ones.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在高维空间中，距离度量遇到了显著的挑战。像欧几里得或曼哈顿距离这样的度量，在低维空间中用于测量数据点之间的接近度，但在这些扩展空间中，它们的效果减弱。距离开始趋于收敛。这意味着大多数点对之间的距离变得几乎相等，也与参考点的距离接近。这种收敛使得区分近邻和远点变得更加困难。
- en: In tasks like classification, where distance measurements are crucial for categorizing
    new data points, these metrics become less effective. As a result, algorithm performance
    drops, leading to less accurate predictions and analyses.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在分类等任务中，距离测量对新数据点的分类至关重要，这些度量在这种情况下效果会降低。因此，算法的性能下降，导致预测和分析的准确性降低。
- en: To better understand how distance behavior changes in higher dimensions, let’s
    perform a simple simulation. We will generate random points in both low and high-dimensional
    spaces. This will allow us to observe and compare the distribution of distances,
    showing us how these distances evolve as we move to higher dimensions.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地理解在高维空间中距离行为的变化，让我们进行一个简单的模拟。我们将生成低维和高维空间中的随机点。这将使我们能够观察和比较距离的分布，展示这些距离如何随着维度的增加而演变。
- en: '[PRE0]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '![](../Images/bf5c048e94e5f1e2a0de989d9924256c.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bf5c048e94e5f1e2a0de989d9924256c.png)'
- en: Image by Author
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供
- en: The code output shows how distances change across dimensions. In 3D, there are
    different distances between points. In 100D, distances between points tend to
    be similar. Graphs to the right also show that as dimensions increase, the mean
    distance between points gets bigger, but the standard deviation stays roughly
    the same as it was on 2D or 3D space.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 代码输出显示了维度间距离的变化。在 3D 中，点之间有不同的距离。在 100D 中，点之间的距离趋于相似。右侧的图表也显示，随着维度的增加，点之间的平均距离增大，但标准差与
    2D 或 3D 空间中的大致相同。
- en: Another note here is that as dimensions increase, the mean distance between
    points gets bigger and approaches the maximum distance. This happens because most
    of the space is concentrated in the corners.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个需要注意的是，随着维度的增加，点之间的平均距离变得更大，并接近最大距离。这是因为大部分空间集中在角落里。
- en: To better understand this, we can simulate random points in dimensions up to
    100D. This will let us compare the average distance to the maximum distance.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地理解这一点，我们可以模拟高达 100 维的随机点。这将使我们能够比较平均距离与最大距离。
- en: '[PRE1]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '![](../Images/dd0ddb65e50f1495c27b26f232ce9068.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/dd0ddb65e50f1495c27b26f232ce9068.png)'
- en: Image by Author
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供
- en: The graph shows that as we go into higher dimensions, the average distance gets
    closer to the maximum distance. We used normalization in here to make sure the
    scales were accurate.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 图表显示，随着维度的增加，平均距离越来越接近最大距离。我们在这里使用了标准化以确保尺度的准确性。
- en: It’s important to understand the difference between absolute and relative distances.
    While absolute distances generally increase with more dimensions, it’s the relative
    differences that matter more. Clustering algorithms like K-means or DBSCAN work
    by looking at how points are positioned compared to each other, not their exact
    distances. This lets us find patterns and relationships that we might miss if
    we only looked at the distances.
  id: totrans-37
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 理解绝对距离和相对距离之间的区别非常重要。虽然绝对距离通常随着维度的增加而增加，但相对差异更为重要。像 K-means 或 DBSCAN 这样的聚类算法通过查看点相对于彼此的位置来工作，而不是它们的确切距离。这使我们能够发现如果仅仅看距离可能会遗漏的模式和关系。
- en: 'But this leads to an interesting question: why do pairs of points in high-dimensional
    spaces tend to be roughly the same distance apart as we add more dimensions? What
    causes this to happen?'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 但这引出了一个有趣的问题：为什么在高维空间中，点对之间的距离在增加维度时趋于相同？是什么导致了这种情况的发生？
- en: '![](../Images/00dea8767aeac0aefd50ef6f96d1b33d.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/00dea8767aeac0aefd50ef6f96d1b33d.png)'
- en: Photo by Aakash Dhage on Unsplash
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由 Aakash Dhage 提供，来源于 Unsplash
- en: To understand why pairs of points in high-dimensional spaces become equidistant,
    we can look at the Law of Large Numbers (LLN). This statistical principle suggests
    that as we increase our sample size or the number of dimensions, the average of
    our observations gets closer to the expected value.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解为什么高维空间中的点对变得等距，我们可以查看大数法则（LLN）。这一统计原理表明，随着样本大小或维度的增加，我们的观察平均值会越来越接近期望值。
- en: Let’s consider the example of rolling a fair six-sided dice. The expected mean
    of a roll is 3.5, which is the average of all possible outcomes. Initially, with
    just a few rolls, like 5 or 10, the average might be significantly different from
    3.5 due to randomness. But as we increase the number of rolls to hundreds or thousands,
    the average roll value gets closer to 3.5\. This phenomenon, where the average
    of many trials aligns with the expected value, shows the essence of the LLN. It
    demonstrates that while individual outcomes are unpredictable, the average becomes
    highly predictable over many trials.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑一个公平的六面骰子的掷骰子例子。掷骰子的期望均值是3.5，这是所有可能结果的平均值。最初，仅有几次掷骰子，比如5次或10次，平均值可能会因为随机性而与3.5有显著差异。但当我们将掷骰子的次数增加到几百次或几千次时，平均掷骰子值会逐渐接近3.5。这一现象，即多次试验的平均值与期望值对齐，展示了大数法则（LLN）的本质。它表明，尽管单个结果是不可预测的，但在多次试验中，平均值变得非常可预测。
- en: Now, how does this relate to distances in high-dimensional spaces?
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，这与高维空间中的距离有什么关系呢？
- en: The Euclidean distance between two points in an n-dimensional space is calculated
    by summing the squared differences across each dimension. We can think of each
    squared difference as a random variable, similar to a roll of a dice. As the number
    of dimensions (or rolls) increases, the sum of these ‘rolls’ gets closer to an
    expected value.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在n维空间中，两点之间的欧几里得距离是通过对每个维度的平方差进行求和来计算的。我们可以将每个平方差看作一个随机变量，类似于掷骰子。随着维度（或掷骰子次数）的增加，这些‘掷骰子’的总和会越来越接近一个期望值。
- en: 'A crucial requirement for the LLN is the independence of random variables.
    In high-dimensional vectors, this independence might be shown through an interesting
    geometric property: **the vectors tend to be almost orthogonal to each other.**'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 大数法则的一个关键要求是随机变量的独立性。在高维向量中，这种独立性可以通过一个有趣的几何属性来显示：**这些向量往往几乎是正交的。**
- en: '[PRE2]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Try running the code above and editing the number of dimensions/ trials, and
    you can notice that vectors in higher dimensions are almost orthogonal.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 尝试运行上面的代码并调整维度/试验次数，你会发现高维空间中的向量几乎是正交的。
- en: 'The angle between two vectors, A and B, is determined by the cosine of the
    angle, which is derived from their dot product and magnitudes. The formula is
    expressed as:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 两个向量A和B之间的角度由角度的余弦值决定，这个余弦值来源于它们的点积和大小。公式表示为：
- en: '![](../Images/c4ac1aade2a563f24149ae4c11fd32de.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c4ac1aade2a563f24149ae4c11fd32de.png)'
- en: Here, *A*⋅*B* represents the dot product of vectors A and B, and ∥*A*∥ and ∥*B*∥
    are their respective magnitudes. For two vectors to be orthogonal, the angle between
    them must be 90 degrees, making cos(*θ*) equal to zero. Typically, this is achieved
    when the dot product *A*⋅*B* is zero, a condition familiar in lower dimensions.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，*A*⋅*B*表示向量A和B的点积，∥*A*∥和∥*B*∥是它们各自的大小。为了使两个向量正交，它们之间的角度必须是90度，这使得cos(*θ*)等于零。通常，这在低维空间中通过点积*A*⋅*B*等于零来实现。
- en: However, in high-dimensional spaces, another phenomenon emerges. **The ratio
    of the dot product to the magnitude of the vectors becomes so small that we can
    consider the vectors to be ‘almost orthogonal.’**
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在高维空间中，出现了另一种现象。**点积与向量大小的比率变得如此之小，以至于我们可以认为这些向量是‘几乎正交’的。**
- en: But what does it mean for two vectors to be ‘independent’ in this context?
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，在这个背景下，两个向量‘独立’是什么意思呢？
- en: 'Navigating a Grid City: An Analogy for Independence in High Dimensions'
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在网格城市中导航：高维独立性的类比
- en: Imagine you are in a city laid out in a grid, like Manhattan’s streets. Picture
    yourself at an intersection, trying to reach another point in this city. In this
    analogy, each street represents a dimension in a high-dimensional space. Moving
    along a street is like changing the value in one dimension of a high-dimensional
    vector. Moving along one street doesn’t affect your position on another street,
    just like changing one dimension doesn’t affect the others.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 想象你在一个像曼哈顿街道那样的网格布局的城市中。设想你站在一个交叉口，试图到达城市中的另一个点。在这个类比中，每条街道代表高维空间中的一个维度。沿着街道移动就像在改变高维向量中的一个维度。沿着一条街道移动不会影响你在另一条街道上的位置，就像改变一个维度不会影响其他维度一样。
- en: To reach a specific intersection, you make a series of independent decisions,
    like calculating distance in high-dimensional space. Each decision contributes
    independently but leads you to your destination.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 要到达一个特定的交叉口，你需要做出一系列独立的决策，就像在高维空间中计算距离一样。每个决策独立地做出，但最终将你引导到目的地。
- en: This analogy also applies to the concept of orthogonality in high-dimensional
    vectors. When vectors are almost orthogonal, they follow their own paths without
    significantly influencing each other. This condition complements the need for
    statistical independence for the LLN.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 这个类比也适用于高维向量中的正交性概念。当向量几乎正交时，它们沿着各自的路径移动，互相之间没有显著影响。这种条件补充了LLN对统计独立性的需求。
- en: 'An important note: while this analogy of LLN offers a helpful perspective,
    it may not capture all the idea or causes behind this behavior. However, it serves
    as a useful proxy, providing an understanding of what the reason **might** be
    for pairs of point to be almost equidistant.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 一个重要的说明：虽然这个LLN类比提供了一个有用的视角，但它可能无法捕捉到这种行为背后的所有思想或原因。然而，它作为一个有用的代理，提供了对点对几乎等距原因的理解**可能**。
- en: Practical Implications
  id: totrans-58
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实际意义
- en: One way the curse of dimensionality problems show up is overfitting. Overfitting
    happens when a complex model learns noise instead of the patterns in the data.
    This is especially true in high-dimensional spaces where there are many features.
    The model can make false connections or correlations and perform poorly when it
    sees new data(failing to generalize).
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 维度诅咒问题的一种表现是过拟合。过拟合发生在一个复杂的模型学习到的是噪声而不是数据中的模式。这在高维空间中特别明显，因为那里有很多特征。模型可能会做出虚假的连接或相关性，并在看到新数据时表现不佳（无法进行泛化）。
- en: The curse also makes it hard to find patterns in big datasets. High-dimensional
    data is spread out and sparse, so it’s challenging for traditional analysis methods
    to find meaningful insights. Some changes or specialized methods are needed to
    navigate and understand this type of data.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 维度诅咒还使得在大数据集中寻找模式变得困难。高维数据分散且稀疏，因此传统分析方法难以找到有意义的见解。需要一些改变或专门的方法来导航和理解这种类型的数据。
- en: Another implication is that processing high-dimensional data takes a lot of
    computational power and memory. Algorithms that work well in lower dimensions
    become much more complex and resource-heavy as the number of dimensions increases.
    This means either having more powerful hardware or optimizing algorithms to handle
    the increased computational load efficiently.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个含义是处理高维数据需要大量的计算能力和内存。在低维度下表现良好的算法在维度增加时变得更加复杂和资源密集。这意味着要么需要更强大的硬件，要么需要优化算法以高效处理增加的计算负载。
- en: '**How to address curse of dimensionality?**'
  id: totrans-62
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**如何应对维度诅咒？**'
- en: There are several strategies to deal with the curse of dimensionality. One way
    is to reduce the dimensionality while keeping the important information(ex. PCA
    algorithm). Another method is manifold learning(can be considered as a type of
    dimensionality reduction).which uncovers the structure within the high-dimensional
    data. The key idea behind manifold learning is that many high-dimensional datasets
    actually lie on a lower-dimensional manifold within the high-dimensional space(ex.
    Isomaps)
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 有几种策略可以应对维度诅咒。一种方法是降低维度，同时保留重要的信息（例如，PCA算法）。另一种方法是流形学习（可以看作是一种降维方法），它揭示了高维数据中的结构。流形学习的关键思想是，许多高维数据集实际上位于高维空间中的低维流形上（例如，Isomaps）。
- en: Note here that -generally speaking- traditional dimensionality reduction techniques
    like PCA (Principal Component Analysis) focus on preserving global data structure
    and variance in a linear fashion. In contrast, manifold learning techniques like
    Isomap(Isometric Mapping) emphasize uncovering the underlying non-linear structure(manifold)
    of data, aiming to preserve local relationships and geometrical features.
  id: totrans-64
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 在这里需要注意的是，-一般来说- 传统的降维技术，如主成分分析（PCA），侧重于以线性方式保留全局数据结构和方差。相比之下，流形学习技术，如等距映射（Isomap），强调揭示数据的潜在非线性结构（流形），旨在保留局部关系和几何特征。
- en: Feature selection is also an option, where relevant features are chosen to improve
    model performance. Regularization techniques prevent overfitting by shrinking
    less important features. Increasing the sample size can also help, although it
    may not always be possible. These methods can help us analyze high-dimensional
    data more accurately and efficiently.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 特征选择也是一个选项，通过选择相关特征来提高模型性能。正则化技术通过缩减不重要的特征来防止过拟合。增加样本量也可以有所帮助，尽管这可能并不总是可能的。这些方法可以帮助我们更准确和高效地分析高维数据。
- en: Conclusion
  id: totrans-66
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: The curse of dimensionality is one of the most important problems in data science
    and machine learning. It happens when dealing with high-dimensional spaces. Two
    significant challenges that arise are data sparsity and issues with distance metrics.
    These challenges can cause overfitting in machine learning models and make computations
    more complex. To tackle these challenges, strategies like dimensionality reduction,
    feature selection, and regularization techniques can be used.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 维度诅咒是数据科学和机器学习中最重要的问题之一。它发生在处理高维空间时。两个显著的挑战是数据稀疏性和距离度量的问题。这些挑战可能导致机器学习模型过拟合，并使计算变得更加复杂。为了解决这些挑战，可以使用降维、特征选择和正则化技术等策略。
- en: If you have made it this far, I would like to thank you for spending time reading
    this! I hope you found the topic enjoyable and at least inspiring enough to delve
    deeper into the world of high-dimensional data. Please feel free to suggest any
    edits or point out any mistakes or inaccuracies.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你已经读到这里，我想感谢你花时间阅读这篇文章！我希望你觉得这个话题有趣，并且至少激发了你更深入了解高维数据的兴趣。请随时建议任何修改或指出任何错误或不准确之处。
