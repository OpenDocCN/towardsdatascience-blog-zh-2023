- en: 'Enhancing RAG Pipelines in Haystack: Introducing DiversityRanker and LostInTheMiddleRanker'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 提升 Haystack 中的 RAG 流水线：引入 DiversityRanker 和 LostInTheMiddleRanker
- en: 原文：[https://towardsdatascience.com/enhancing-rag-pipelines-in-haystack-45f14e2bc9f5?source=collection_archive---------1-----------------------#2023-08-08](https://towardsdatascience.com/enhancing-rag-pipelines-in-haystack-45f14e2bc9f5?source=collection_archive---------1-----------------------#2023-08-08)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/enhancing-rag-pipelines-in-haystack-45f14e2bc9f5?source=collection_archive---------1-----------------------#2023-08-08](https://towardsdatascience.com/enhancing-rag-pipelines-in-haystack-45f14e2bc9f5?source=collection_archive---------1-----------------------#2023-08-08)
- en: How the latest rankers optimize LLM context window utilization in Retrieval-Augmented
    Generation (RAG) pipelines
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 最新的排名器如何优化 LLM 上下文窗口在检索增强生成（RAG）流水线中的利用
- en: '[](https://medium.com/@dovlex?source=post_page-----45f14e2bc9f5--------------------------------)[![Vladimir
    Blagojevic](../Images/ed7e836d5fd7e6484d90858dcc4afe5e.png)](https://medium.com/@dovlex?source=post_page-----45f14e2bc9f5--------------------------------)[](https://towardsdatascience.com/?source=post_page-----45f14e2bc9f5--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----45f14e2bc9f5--------------------------------)
    [Vladimir Blagojevic](https://medium.com/@dovlex?source=post_page-----45f14e2bc9f5--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@dovlex?source=post_page-----45f14e2bc9f5--------------------------------)[![Vladimir
    Blagojevic](../Images/ed7e836d5fd7e6484d90858dcc4afe5e.png)](https://medium.com/@dovlex?source=post_page-----45f14e2bc9f5--------------------------------)[](https://towardsdatascience.com/?source=post_page-----45f14e2bc9f5--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----45f14e2bc9f5--------------------------------)
    [Vladimir Blagojevic](https://medium.com/@dovlex?source=post_page-----45f14e2bc9f5--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ff1fc40f79c2f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fenhancing-rag-pipelines-in-haystack-45f14e2bc9f5&user=Vladimir+Blagojevic&userId=f1fc40f79c2f&source=post_page-f1fc40f79c2f----45f14e2bc9f5---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----45f14e2bc9f5--------------------------------)
    ·8 min read·Aug 8, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F45f14e2bc9f5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fenhancing-rag-pipelines-in-haystack-45f14e2bc9f5&user=Vladimir+Blagojevic&userId=f1fc40f79c2f&source=-----45f14e2bc9f5---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ff1fc40f79c2f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fenhancing-rag-pipelines-in-haystack-45f14e2bc9f5&user=Vladimir+Blagojevic&userId=f1fc40f79c2f&source=post_page-f1fc40f79c2f----45f14e2bc9f5---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----45f14e2bc9f5--------------------------------)
    · 8分钟阅读 · 2023年8月8日 [](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F45f14e2bc9f5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fenhancing-rag-pipelines-in-haystack-45f14e2bc9f5&user=Vladimir+Blagojevic&userId=f1fc40f79c2f&source=-----45f14e2bc9f5---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F45f14e2bc9f5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fenhancing-rag-pipelines-in-haystack-45f14e2bc9f5&source=-----45f14e2bc9f5---------------------bookmark_footer-----------)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F45f14e2bc9f5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fenhancing-rag-pipelines-in-haystack-45f14e2bc9f5&source=-----45f14e2bc9f5---------------------bookmark_footer-----------)'
- en: The recent improvements in Natural Language Processing (NLP) and Long-Form Question
    Answering (LFQA) would have, just a few years ago, sounded like something from
    the domain of science fiction. Who could have thought that nowadays we would have
    systems that can answer complex questions with the precision of an expert, all
    while synthesizing these answers on the fly from a vast pool of sources? LFQA
    is a type of Retrieval-Augmented Generation (RAG) which has recently made significant
    strides, utilizing the best retrieval and generation capabilities of Large Language
    Models (LLMs).
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 最近在自然语言处理（NLP）和长文问答（LFQA）方面的改进，几年前听起来简直像科幻小说中的内容。谁能想到现在我们会有能够像专家一样精确回答复杂问题的系统，同时从庞大的资源库中即时合成这些答案？LFQA
    是一种检索增强生成（RAG），最近取得了显著进展，利用了大型语言模型（LLMs）最优秀的检索和生成能力。
- en: But what if we could refine this setup even further? What if we could optimize
    how RAG selects and utilizes information to enhance its performance? This article
    introduces two innovative components aiming to improve RAG with concrete examples
    drawn from LFQA, based on the latest research and our experience — the DiversityRanker
    and the LostInTheMiddleRanker.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 但如果我们能进一步优化这个设置呢？如果我们可以优化 RAG 如何选择和利用信息以提升其性能呢？本文介绍了两个创新组件，旨在通过基于最新研究和我们的经验，从
    LFQA 中提取具体示例来改善 RAG——DiversityRanker 和 LostInTheMiddleRanker。
- en: Consider the LLM’s context window as a gourmet meal, where each paragraph is
    a unique, flavorful ingredient. Just as a culinary masterpiece requires diverse,
    high-quality ingredients, LFQA question-answering demands a context window filled
    with high-quality, varied, relevant, and non-repetitive paragraphs.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 将 LLM 的上下文窗口视为一顿美味的餐点，其中每段文字都是一种独特且富有风味的配料。就像一件精美的烹饪作品需要多样化的高质量配料一样，LFQA 问答也要求上下文窗口充满高质量、多样化、相关且不重复的段落。
- en: In the intricate world of LFQA and RAG, making the most of the LLM’s context
    window is paramount. Any wasted space or repetitive content limits the depth and
    breadth of the answers we can extract and generate. It’s a delicate balancing
    act to lay out the content of the context window appropriately. This article presents
    new approaches to mastering this balancing act, which will enhance RAG’s capacity
    for delivering precise, comprehensive responses.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在 LFQA 和 RAG 的复杂世界中，最大限度地利用 LLM 的上下文窗口至关重要。任何浪费的空间或重复的内容都会限制我们提取和生成答案的深度和广度。合理布置上下文窗口的内容是一项微妙的平衡工作。本文提出了掌握这一平衡的全新方法，这将提升
    RAG 提供准确、全面响应的能力。
- en: Let’s explore these exciting advancements and how they improve LFQA and RAG.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们探索这些令人兴奋的进展以及它们如何改善 LFQA 和 RAG。
- en: '**Background**'
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**背景**'
- en: Haystack is an open-source framework providing end-to-end solutions for practical
    NLP builders. It supports a wide range of use cases, from question-answering and
    semantic document search all the way to LLM agents. Its modular design allows
    the integration of state-of-the-art NLP models, document stores, and various other
    components required in today’s NLP toolbox.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: Haystack 是一个开源框架，提供实践 NLP 构建者的端到端解决方案。它支持从问答和语义文档搜索到 LLM 代理等广泛的用例。其模块化设计允许集成最先进的
    NLP 模型、文档存储和当今 NLP 工具箱中所需的各种其他组件。
- en: One of the key concepts in Haystack is the idea of a pipeline. A pipeline represents
    a sequence of processing steps that a specific component executes. These components
    can perform various types of text processing, allowing users to easily create
    powerful and customizable systems by defining how data flows through the pipeline
    and the order of nodes that perform their processing steps.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: Haystack 中的一个关键概念是管道的思想。管道表示一个特定组件执行的一系列处理步骤。这些组件可以执行各种类型的文本处理，允许用户通过定义数据如何在管道中流动以及执行处理步骤的节点顺序，轻松创建强大且可定制的系统。
- en: The pipeline plays a crucial role in web-based long-form question answering.
    It starts with a WebRetriever component, which searches and retrieves query-relevant
    documents from the web, automatically stripping HTML content into raw text. But
    once we fetch query-relevant documents, how do we make the most of them? How do
    we fill the LLM’s context window to maximize the quality of the answers? And what
    if these documents, although highly relevant, are repetitive and numerous, sometimes
    overflowing the LLM context window?
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在基于网络的长篇问答中，管道发挥了关键作用。它从 WebRetriever 组件开始，该组件在网络上搜索和检索与查询相关的文档，自动将 HTML 内容剥离成原始文本。但一旦我们获取了与查询相关的文档，我们如何充分利用它们？我们如何填充
    LLM 的上下文窗口以最大化答案的质量？如果这些文档尽管高度相关，但却重复且数量众多，有时会溢出 LLM 上下文窗口，该怎么办？
- en: This is where the components we’ll introduce today come into play — the DiversityRanker
    and the LostInTheMiddleRanker. Their aim is to address these challenges and improve
    the answers generated by the LFQA/RAG pipelines.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 今天我们将介绍的组件正是在这里发挥作用的——DiversityRanker 和 LostInTheMiddleRanker。它们的目标是解决这些挑战并改善
    LFQA/RAG 管道生成的答案。
- en: The DiversityRanker enhances the diversity of the paragraphs selected for the
    context window. LostInTheMiddleRanker, usually positioned after DiversityRanker
    in the pipeline, helps to mitigate the LLM performance degradation observed when
    models must access relevant information in the middle of a long context window.
    The following sections will delve deeper into these two components and demonstrate
    their effectiveness in a practical use case.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: DiversityRanker 增强了上下文窗口中选定段落的多样性。LostInTheMiddleRanker 通常在流水线中位于 DiversityRanker
    之后，有助于减轻模型在长上下文窗口中必须访问相关信息时观察到的性能下降。接下来的部分将深入探讨这两个组件，并展示它们在实际使用案例中的效果。
- en: '**DiversityRanker**'
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**DiversityRanker**'
- en: The DiversityRanker is a novel component designed to enhance the diversity of
    the paragraphs selected for the context window in the RAG pipeline. It operates
    on the principle that a diverse set of documents can increase the LLM’s ability
    to generate answers with more breadth and depth.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: DiversityRanker 是一个新型组件，旨在增强 RAG 流水线中上下文窗口选定段落的多样性。它基于一个原则，即多样化的文档集可以提高 LLM
    生成答案的广度和深度。
- en: '![](../Images/8d5d8883d306689625739a1271204ec6.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8d5d8883d306689625739a1271204ec6.png)'
- en: 'Figure 1: An artistic interpretation of the DiversityRanker algorithm’s document
    ordering process, courtesy of MidJourney. Please note that this visualization
    is more illustrative than precise.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：DiversityRanker 算法文档排序过程的艺术表现，由 MidJourney 提供。请注意，此可视化更多是说明性的，而非精确的。
- en: The DiversityRanker uses sentence transformers to calculate the similarity between
    documents. The sentence transformers library offers powerful embedding models
    for creating meaningful representations of sentences, paragraphs, and even whole
    documents. These representations, or embeddings, capture the semantic content
    of the text, allowing us to measure how similar two pieces of text are.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: DiversityRanker 使用句子变换器来计算文档之间的相似度。句子变换器库提供了强大的嵌入模型，用于创建句子、段落甚至整个文档的有意义表示。这些表示或嵌入捕捉了文本的语义内容，使我们能够测量两段文本的相似性。
- en: 'DiversityRanker processes the documents using the following algorithm:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: DiversityRanker 使用以下算法处理文档：
- en: 1\. It starts by calculating the embeddings for each document and the query
    using a sentence-transformer model.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 1\. 它首先使用句子变换器模型计算每个文档和查询的嵌入。
- en: 2\. It then selects the document semantically closest to the query as the first
    selected document.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 2\. 然后选择语义上最接近查询的文档作为第一个选择的文档。
- en: 3\. For each remaining document, it calculates the average similarity to the
    already selected documents.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 3\. 对于每个剩余的文档，它计算与已经选择的文档的平均相似度。
- en: 4\. It then selects the document that is, on average, least similar to the already
    selected documents.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 4\. 然后选择平均上与已经选择的文档最不相似的文档。
- en: 5\. This selection process continues until all documents are selected, resulting
    in a list of documents ordered from the document contributing the most to the
    overall diversity to the document that contributes the least.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 5\. 这个选择过程持续进行，直到所有文档都被选择，结果是一个文档列表，从对整体多样性贡献最大的文档到贡献最小的文档。
- en: 'A technical note to keep in mind: the DiversityRanker uses a greedy local approach
    to select the next document in order, which might not find the most optimal overall
    order for the documents. DiversityRanker focuses on diversity more than relevance,
    so it should be placed in the pipeline after another component like TopPSampler
    or another similarity ranker that focuses more on relevance. By using it after
    a component that selects the most relevant documents, we ensure that we select
    diverse documents from a pool of already relevant documents.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 需要记住的技术说明：DiversityRanker 使用贪婪的局部方法来选择下一个文档，这可能无法找到文档的最优整体顺序。DiversityRanker
    更注重多样性而非相关性，因此它应该在 TopPSampler 或其他更注重相关性的相似性排序器等组件之后放置在流水线中。通过在选择最相关文档的组件之后使用它，我们确保从已经相关的文档池中选择多样化的文档。
- en: '**LostInTheMiddleRanker**'
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**LostInTheMiddleRanker**'
- en: 'The LostInTheMiddleRanker optimizes the layout of the selected documents in
    the LLM’s context window. This component is a way to work around a problem identified
    in recent research [1] that suggests LLMs struggle to focus on relevant passages
    in the middle of a long context. The LostInTheMiddleRanker alternates placing
    the best documents at the beginning and end of the context window, making it easy
    for the LLM’s attention mechanism to access and use them. To understand how LostInTheMiddleRanker
    orders the given documents, imagine a simple example where documents consist of
    a single digit from 1 to 10 in ascending order. LostInTheMiddleRanker will order
    these ten documents in the following order: [1 3 5 7 9 10 8 6 4 2].'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: LostInTheMiddleRanker优化了所选文档在LLM上下文窗口中的布局。这个组件是解决最近研究中识别出的一个问题的一种方法[1]，该研究表明LLM难以关注长上下文中间的相关段落。LostInTheMiddleRanker将最佳文档交替放置在上下文窗口的开头和结尾，使LLM的注意力机制更容易访问和使用它们。为了理解LostInTheMiddleRanker如何排序给定的文档，可以想象一个简单的例子，其中文档由从1到10的单个数字按升序排列。LostInTheMiddleRanker将这十个文档按以下顺序排序：[1
    3 5 7 9 10 8 6 4 2]。
- en: Although the authors of this research focused on a question-answering task —
    extracting the relevant spans of the answer from the text — we are speculating
    that the LLM’s attention mechanism will also have an easier time focusing on the
    paragraphs in the beginning and the end of the context window when generating
    answers.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这项研究的作者专注于一个问答任务——从文本中提取相关的答案片段——我们推测LLM的注意力机制在生成答案时，也会更容易关注上下文窗口开头和结尾的段落。
- en: '![](../Images/4c89795f5fc907acca68b02a2cc441a7.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4c89795f5fc907acca68b02a2cc441a7.png)'
- en: Figure 2\. LLMs struggle to extract answers from the middle of the context,
    adapted from Liu et al. (2023)[1]
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 图2\. LLM在提取上下文中间的答案时遇到困难，改编自Liu等（2023）[1]
- en: LostInTheMiddleRanker is best positioned as the last ranker in the RAG pipeline
    as the given documents are already selected based on similarity (relevance) and
    ordered by diversity.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: LostInTheMiddleRanker最适合作为RAG管道中的最后一个排名器，因为所给的文档已经基于相似性（相关性）进行选择，并按多样性排序。
- en: '**Using the new rankers in pipelines**'
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**在管道中使用新的排名器**'
- en: In this section, we’ll look into the practical use case of the LFQA/RAG pipeline,
    focusing on how to integrate the DiversityRanker and LostInTheMiddleRanker. We’ll
    also discuss how these components interact with each other and the other components
    in the pipeline.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将探讨LFQA/RAG管道的实际应用案例，重点是如何集成DiversityRanker和LostInTheMiddleRanker。我们还将讨论这些组件如何相互作用以及与管道中的其他组件的互动。
- en: The first component in the pipeline is a WebRetriever which retrieves query
    relevant documents from the web using a programmatic search engine API (SerperDev,
    Google, Bing etc). The retrieved documents are first stripped of HTML tags, converted
    to raw text, and optionally preprocessed into shorter paragraphs. They are then,
    in turn passed to a TopPSampler component, which selects the most relevant paragraphs
    based on their similarity to the query.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 管道中的第一个组件是WebRetriever，它使用程序化搜索引擎API（如SerperDev、Google、Bing等）从网络中检索查询相关的文档。检索到的文档首先去除HTML标签，转换为原始文本，并可选择性地预处理成较短的段落。然后，这些文档会传递给TopPSampler组件，该组件根据与查询的相似性选择最相关的段落。
- en: After TopPSampler selects the set of relevant paragraphs, they are passed to
    the DiversityRanker. DiversityRanker, in turn, orders the paragraphs based on
    their diversity, reducing the repetitiveness of the TopPSampler-ordered documents.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在TopPSampler选择相关段落集之后，它们会传递给DiversityRanker。DiversityRanker则根据段落的多样性对其进行排序，减少TopPSampler排序文档的重复性。
- en: The selected documents are then passed to the LostInTheMiddleRanker. As we previously
    mentioned, LostInTheMiddleRanker places the most relevant paragraphs at the beginning
    and the end of the context window, while pushing the worst-ranked documents to
    the middle.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 选定的文档随后传递给LostInTheMiddleRanker。如前所述，LostInTheMiddleRanker将最相关的段落放置在上下文窗口的开头和结尾，同时将排名最差的文档推到中间。
- en: Finally, the merged paragraphs are passed to a PromptNode, which conditions
    an LLM to answer the question based on these selected paragraphs.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，合并的段落会传递给PromptNode，PromptNode将LLM调整为基于这些选定的段落回答问题。
- en: '![](../Images/82e967a7370d98025712457debeb5125.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/82e967a7370d98025712457debeb5125.png)'
- en: Figure 3\. LFQA/RAG pipeline — Image by author
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 图3\. LFQA/RAG管道 — 作者提供的图像
- en: The new rankers are already merged into Haystack’s main branch and will be available
    in the upcoming 1.20 release slated for the end of August 2023\. We included a
    new LFQA/RAG pipeline demo in the project’s examples folder.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 新排序器已经合并到Haystack的主分支中，并将在2023年8月底的1.20版本中发布。我们在项目的示例文件夹中添加了新的LFQA/RAG管道演示。
- en: The demo shows how DiversityRanker and LostInTheMiddleRanker can be easily integrated
    into a RAG pipeline to improve the quality of the generated answers.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 演示展示了DiversityRanker和LostInTheMiddleRanker如何轻松集成到RAG管道中，以提高生成答案的质量。
- en: '**Case study**'
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**案例研究**'
- en: 'To demonstrate the effectiveness of the LFQA/RAG pipelines that include the
    two new rankers, we’ll use a small sample of half a dozen questions requiring
    detailed answers. The questions include: “What are the main reasons for long-standing
    animosities between Russia and Poland?”, “What are the primary causes of climate
    change on both global and local scales?”, and more. To answer these questions
    well, LLMs require a wide range of historical, political, scientific, and cultural
    sources, making them ideal for our use case.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 为了展示包含两个新排序器的LFQA/RAG管道的有效性，我们将使用一个包含半打问题的小样本，这些问题需要详细的回答。问题包括：“俄罗斯和波兰长期敌对的主要原因是什么？”，“全球和地方尺度上气候变化的主要原因是什么？”等等。要很好地回答这些问题，LLM需要广泛的历史、政治、科学和文化来源，这使得它们非常适合我们的用例。
- en: Comparing the generated answers of the RAG pipeline with two new rankers (optimized
    pipeline) and a pipeline without them (non-optimized) would require complex evaluation
    involving human expert judgment. To simplify evaluation and to evaluate the effect
    of the DiversityRanker primarily, we calculated the average pairwise cosine distance
    of the context documents injected into the LLM context instead. We limited the
    context window size in both pipelines to 1024 words. By running these sample Python
    scripts [2], we have found that the optimized pipeline has an average 20–30% increase
    in pairwise cosine distance [3] for the documents injected into the LLM context.
    This increase in the pairwise cosine distance essentially means that the documents
    used are more diverse (and less repetitive), thus giving the LLM a wider and richer
    range of paragraphs to draw upon for its answers. We’ll leave the evaluation of
    LostInTheMiddleRanker and its effect on generated answers for one of our upcoming
    articles.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 比较RAG管道生成的答案与两个新排序器（优化管道）以及没有这些排序器的管道（未优化管道）会涉及复杂的评估，需要人工专家的判断。为了简化评估并主要评估DiversityRanker的效果，我们改为计算注入LLM上下文中的上下文文档的平均成对余弦距离。我们将两个管道的上下文窗口大小限制为1024个词。通过运行这些示例Python脚本[2]，我们发现优化管道中注入LLM上下文的文档的平均成对余弦距离增加了约20–30%[3]。这种成对余弦距离的增加本质上意味着所使用的文档更具多样性（而且重复性更少），从而为LLM提供了更广泛和丰富的段落供其回答参考。我们将把对LostInTheMiddleRanker及其对生成答案的影响的评估留到我们即将发布的文章中。
- en: '**Conclusion**'
  id: totrans-50
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**结论**'
- en: 'We’ve explored how Haystack users can enhance their RAG pipelines by using
    two innovative rankers: DiversityRanker and LostInTheMiddleRanker.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 我们探索了Haystack用户如何通过使用两个创新排序器：DiversityRanker和LostInTheMiddleRanker来增强他们的RAG管道。
- en: DiversityRanker ensures that the LLM’s context window is filled with diverse,
    non-repetitive documents, providing a broader range of paragraphs for the LLM
    to synthesize the answer from. At the same time, the LostInTheMiddleRanker optimizes
    the placement of the most relevant paragraphs in the context window, making it
    easier for the model to access and utilize the best-supporting documents.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: DiversityRanker确保LLM的上下文窗口中填充了多样且不重复的文档，为LLM提供了更广泛的段落以便综合答案。同时，LostInTheMiddleRanker优化了最相关段落在上下文窗口中的位置，使得模型更容易访问和利用最支持的文档。
- en: Our small case study confirmed the effectiveness of the DiversityRanker by calculating
    the average pairwise cosine distance of the documents injected into the LLM’s
    context window in the optimized RAG pipeline (with two new rankers) and the non-optimized
    pipeline (no rankers used). The results showed that an optimized RAG pipeline
    increased the average pairwise cosine distance by approximately 20–30%.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的小案例研究通过计算注入LLM上下文窗口中的文档的平均成对余弦距离，确认了DiversityRanker的有效性，比较了优化的RAG管道（使用两个新排序器）和未优化管道（未使用排序器）。结果显示，优化的RAG管道使得平均成对余弦距离增加了约20–30%。
- en: We have demonstrated how these new rankers can potentially enhance Long-Form
    Question-Answering and other RAG pipelines. By continuing to invest in and expand
    on these and similar ideas, we can further improve the capabilities of Haystack’s
    RAG pipelines, bringing us closer to crafting NLP solutions that seem more like
    magic than reality.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经展示了这些新型排名器如何有可能增强长篇问答和其他RAG管道。通过继续投资并扩展这些及类似的想法，我们可以进一步提升Haystack的RAG管道的能力，使我们更接近于打造看起来更像魔法而非现实的NLP解决方案。
- en: '**References**:'
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**参考文献**：'
- en: '[1] “Lost in the Middle: How Language Models Use Long Contexts” at [https://arxiv.org/abs/2307.03172](https://arxiv.org/abs/2307.03172)'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] “迷失在中间：语言模型如何使用长上下文” [https://arxiv.org/abs/2307.03172](https://arxiv.org/abs/2307.03172)'
- en: '[2] Script: [https://gist.github.com/vblagoje/430def6cda347c0b65f5f244bc0f2ede](https://gist.github.com/vblagoje/430def6cda347c0b65f5f244bc0f2ede)'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] 脚本： [https://gist.github.com/vblagoje/430def6cda347c0b65f5f244bc0f2ede](https://gist.github.com/vblagoje/430def6cda347c0b65f5f244bc0f2ede)'
- en: '[3] Script output (answers): [https://gist.github.com/vblagoje/738253f87b7590b1c014e3d598c8300b](https://gist.github.com/vblagoje/738253f87b7590b1c014e3d598c8300b)'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] 脚本输出（答案）： [https://gist.github.com/vblagoje/738253f87b7590b1c014e3d598c8300b](https://gist.github.com/vblagoje/738253f87b7590b1c014e3d598c8300b)'
