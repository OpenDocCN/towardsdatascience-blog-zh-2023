- en: 'GPTQ or bitsandbytes: Which Quantization Method to Use for LLMs — Examples
    with Llama 2'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: GPTQ 或 bitsandbytes：对于LLMs 应使用哪种量化方法 — 以 Llama 2 为例
- en: 原文：[https://towardsdatascience.com/gptq-or-bitsandbytes-which-quantization-method-to-use-for-llms-examples-with-llama-2-f79bc03046dc](https://towardsdatascience.com/gptq-or-bitsandbytes-which-quantization-method-to-use-for-llms-examples-with-llama-2-f79bc03046dc)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/gptq-or-bitsandbytes-which-quantization-method-to-use-for-llms-examples-with-llama-2-f79bc03046dc](https://towardsdatascience.com/gptq-or-bitsandbytes-which-quantization-method-to-use-for-llms-examples-with-llama-2-f79bc03046dc)
- en: Large language model quantization for affordable fine-tuning and inference on
    your computer
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 针对个人计算机上可承受的微调和推断进行大型语言模型量化
- en: '[](https://medium.com/@bnjmn_marie?source=post_page-----f79bc03046dc--------------------------------)[![Benjamin
    Marie](../Images/3ea1ad230cb1e67610418a8e36a5e5dd.png)](https://medium.com/@bnjmn_marie?source=post_page-----f79bc03046dc--------------------------------)[](https://towardsdatascience.com/?source=post_page-----f79bc03046dc--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----f79bc03046dc--------------------------------)
    [Benjamin Marie](https://medium.com/@bnjmn_marie?source=post_page-----f79bc03046dc--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@bnjmn_marie?source=post_page-----f79bc03046dc--------------------------------)[![Benjamin
    Marie](../Images/3ea1ad230cb1e67610418a8e36a5e5dd.png)](https://medium.com/@bnjmn_marie?source=post_page-----f79bc03046dc--------------------------------)[](https://towardsdatascience.com/?source=post_page-----f79bc03046dc--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----f79bc03046dc--------------------------------)
    [Benjamin Marie](https://medium.com/@bnjmn_marie?source=post_page-----f79bc03046dc--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----f79bc03046dc--------------------------------)
    ·7 min read·Aug 25, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----f79bc03046dc--------------------------------)
    ·7分钟阅读·2023年8月25日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/ff5993f4f2ee78297a0c1cd107099ea9.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ff5993f4f2ee78297a0c1cd107099ea9.png)'
- en: Image by the author — Made with an illustration from [Pixabay](https://pixabay.com/vectors/llama-alpaca-animal-mammal-zoo-297668/)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片 — 制作自[Pixabay](https://pixabay.com/vectors/llama-alpaca-animal-mammal-zoo-297668/)
- en: As large language models (LLM) got bigger with more and more parameters, new
    techniques to reduce their memory usage have also been proposed.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 随着大型语言模型（LLM）变得越来越大，参数也越来越多，新的技术也被提出以减少它们的内存使用。
- en: One of the most effective methods to reduce the model size in memory is **quantization**.
    You can see quantization as a compression technique for LLMs. In practice, the
    main goal of quantization is to lower the precision of the LLM’s weights, typically
    from 16-bit to 8-bit, 4-bit, or even 3-bit, with minimal performance degradation.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 减少内存中模型大小的最有效方法之一是**量化**。你可以将量化视为LLM的压缩技术。在实践中，量化的主要目标是降低LLM权重的精度，通常从16位降低到8位、4位，甚至3位，同时尽量减少性能下降。
- en: 'There are two popular quantization methods for LLMs: GPTQ and bitsandbytes.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 有两种流行的LLM量化方法：GPTQ和bitsandbytes。
- en: In this article, I discuss what the main differences between these two approaches
    are. They both have their own advantages and disadvantages that make them suitable
    for different use cases. I present a comparison of their memory usage and inference
    speed using Llama 2\. I also discuss their performance based on experiments from
    previous work.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我讨论了这两种方法之间的主要差异。它们各有优缺点，使它们适用于不同的使用场景。我展示了它们在使用 Llama 2 时的内存使用情况和推断速度的比较。我还根据以往的实验讨论了它们的性能。
- en: '*Note: If you want to know more about quantization, I recommend reading this
    excellent introduction by* [*Maxime Labonne*](https://medium.com/u/dc89da634938?source=post_page-----f79bc03046dc--------------------------------)*:*'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '*注意：如果你想了解更多关于量化的内容，我推荐阅读* [*Maxime Labonne*](https://medium.com/u/dc89da634938?source=post_page-----f79bc03046dc--------------------------------)*的这篇出色的介绍：*'
- en: '[](/introduction-to-weight-quantization-2494701b9c0c?source=post_page-----f79bc03046dc--------------------------------)
    [## Introduction to Weight Quantization'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/introduction-to-weight-quantization-2494701b9c0c?source=post_page-----f79bc03046dc--------------------------------)
    [## 权重量化介绍'
- en: Reducing the size of Large Language Models with 8-bit quantization
  id: totrans-14
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用8位量化减少大型语言模型的大小
- en: towardsdatascience.com](/introduction-to-weight-quantization-2494701b9c0c?source=post_page-----f79bc03046dc--------------------------------)
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '[原文链接](https://towardsdatascience.com/introduction-to-weight-quantization-2494701b9c0c?source=post_page-----f79bc03046dc--------------------------------)'
- en: 'GPTQ: Post-training quantization for lightweight storage and fast inference'
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: GPTQ：用于轻量存储和快速推理的后训练量化
- en: '[GPTQ (Frantar et al., 2023)](https://arxiv.org/abs/2210.17323) was first applied
    to models ready to deploy. In other words, once the model is fully fine-tuned,
    GPTQ will be applied to reduce its size.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '[GPTQ (Frantar et al., 2023)](https://arxiv.org/abs/2210.17323)首次应用于准备部署的模型。换句话说，一旦模型完全微调，GPTQ将被应用于减少其大小。'
- en: GPTQ can lower the weight precision to 4-bit or 3-bit. In practice, GPTQ is
    mainly used for 4-bit quantization. 3-bit has been shown very unstable ([Dettmers
    and Zettlemoyer, 2023](https://arxiv.org/abs/2212.09720)).
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: GPTQ可以将权重精度降低到4-bit或3-bit。实际上，GPTQ主要用于4-bit量化。3-bit显示出非常不稳定（[Dettmers和Zettlemoyer,
    2023](https://arxiv.org/abs/2212.09720)）。
- en: It quantizes without loading the entire model into memory. Instead, GPTQ loads
    and quantizes the LLM module by module. Quantization also requires a small sample
    of data for calibration which can take more than one hour on a consumer GPU.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 它在量化时不需要将整个模型加载到内存中。相反，GPTQ逐模块加载和量化LLM。量化还需要少量数据进行校准，这可能需要超过一小时的消费者GPU时间。
- en: In a previous article, I did experiments to quantize Llama 2 7B with 4-bit precision
    using GPTQ. The original model weighs 13.5 GB on the hard drive but after quantization,
    the model size is reduced to 3.9 GB (28.9% of its original size).
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一篇文章中，我进行了实验，使用GPTQ对Llama 2 7B进行4-bit精度量化。原始模型在硬盘上的大小为13.5 GB，但经过量化后，模型大小减少到3.9
    GB（原始大小的28.9%）。
- en: '[](https://kaitchup.substack.com/p/quantization-of-llama-2-with-gtpq?source=post_page-----f79bc03046dc--------------------------------)
    [## Quantization of Llama 2 with GTPQ for Fast Inference on Your Computer'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://kaitchup.substack.com/p/quantization-of-llama-2-with-gtpq?source=post_page-----f79bc03046dc--------------------------------)
    [## 使用GTPQ对Llama 2进行量化，以便在您的计算机上快速推理'
- en: Llama 2 but 75% smaller
  id: totrans-22
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Llama 2但减少了75%的大小
- en: kaitchup.substack.com](https://kaitchup.substack.com/p/quantization-of-llama-2-with-gtpq?source=post_page-----f79bc03046dc--------------------------------)
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: kaitchup.substack.com](https://kaitchup.substack.com/p/quantization-of-llama-2-with-gtpq?source=post_page-----f79bc03046dc--------------------------------)
- en: Once quantized, the model can run on a much smaller GPU. For instance, the original
    Llama 2 7B wouldn’t run on a 12 GB of VRAM (which is about what you get on a free
    Google Colab instance), but it would easily run once quantized. Not only it would
    run, but it would also leave a significant amount of VRAM unused allowing inference
    with bigger batches.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦量化，模型可以在更小的GPU上运行。例如，原始的Llama 2 7B无法在12 GB VRAM的GPU上运行（这大约是您在免费Google Colab实例上得到的），但量化后就能轻松运行。不仅能运行，而且还能显著减少VRAM的使用量，允许更大批量的推理。
- en: The quantization of bitsandbytes is also optimized for inference as I will show
    you in the next sections of this article.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: bitsandbytes的量化也针对推理进行了优化，正如我将在本文的下一部分展示的那样。
- en: 'bitsandbytes: On-the-fly quantization for super simple fine-tuning and efficient
    inference'
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: bitsandbytes：即时量化，便于简单的微调和高效推理
- en: bitsandbytes also supports quantization but with a different kind of 4-bit precision
    denoted [NormalFloat (NF)](https://huggingface.co/blog/4bit-transformers-bitsandbytes).
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: bitsandbytes也支持量化，但使用不同类型的4-bit精度，即[NormalFloat (NF)](https://huggingface.co/blog/4bit-transformers-bitsandbytes)。
- en: It was proposed at the same time as QLoRa for fine-tuning quantized LLMs with
    adapters, which at that time was not possible with GPTQ LLMs (this is possible
    since June 2023). The convenient integration of 4-bit NF (nf4) in QLoRa is the
    main advantage of bitsandbytes over GPTQ.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 它与QLoRa同时提出，用于通过适配器对量化的LLM进行微调，当时GPTQ LLMs尚不支持这种方法（自2023年6月以来已可实现）。QLoRa中4-bit
    NF（nf4）的便捷集成是bitsandbytes相对于GPTQ的主要优势。
- en: I showed how to fine-tune Llama 2 7B with nf4 in a previous article.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 我在上一篇文章中展示了如何使用nf4对Llama 2 7B进行微调。
- en: '[](https://kaitchup.substack.com/p/fine-tune-llama-2-on-your-computer?source=post_page-----f79bc03046dc--------------------------------)
    [## Fine-tune Llama 2 on Your Computer with QLoRa and TRL'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://kaitchup.substack.com/p/fine-tune-llama-2-on-your-computer?source=post_page-----f79bc03046dc--------------------------------)
    [## 使用QLoRa和TRL在您的计算机上微调Llama 2'
- en: On Guanaco and with the correct padding
  id: totrans-31
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 在Guanaco上，并使用正确的填充
- en: kaitchup.substack.com](https://kaitchup.substack.com/p/fine-tune-llama-2-on-your-computer?source=post_page-----f79bc03046dc--------------------------------)
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: kaitchup.substack.com](https://kaitchup.substack.com/p/fine-tune-llama-2-on-your-computer?source=post_page-----f79bc03046dc--------------------------------)
- en: With bitsandbytes, the model is transparently quantized on-the-fly when loaded.
    If you use Hugging Face transformers, it only requires the “load_in_4bit” to be
    set to “True” when you call the “from_pretrained” method.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 使用bitsandbytes时，模型在加载时会被透明地实时量化。如果你使用Hugging Face transformers，只需在调用“from_pretrained”方法时将“load_in_4bit”设置为“True”即可。
- en: A significant difference with GPTQ is that at the time of writing this article,
    bitsandbytes can’t serialize nf4 quantized models. The model has to be quantized
    at loading time, every time.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 与GPTQ的一个显著区别是，在撰写本文时，bitsandbytes无法序列化nf4量化模型。模型必须在每次加载时进行量化。
- en: Software and hardware requirements for QLoRa and GPTQ
  id: totrans-35
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: QLoRa和GPTQ的软件和硬件要求
- en: Hardware
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 硬件
- en: NVIDIA introduced the support for [4-bit precision with its Turing GPUs](https://developer.nvidia.com/blog/int4-for-ai-inference/)
    in 2018\. Free Google Colab instances run on T4 GPUs which are based on the Turing
    architecture. I confirmed that you can run 4-bit quantization on Google Colab.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: NVIDIA在2018年为其Turing GPU引入了[4位精度支持](https://developer.nvidia.com/blog/int4-for-ai-inference/)。免费的Google
    Colab实例运行在基于Turing架构的T4 GPU上。我确认你可以在Google Colab上运行4位量化。
- en: 'As for consumer GPUs, I can only say with certainty that it is supported by
    the RTX 30xx GPUs (I tried it on my RTX 3060), or more recent ones. In theory,
    it should also work with the GTX 16xx and RTX 20xx since they also exploit the
    Turing architecture but I didn’t try it and couldn’t find any evidence that GPTQ
    or bitsandbytes nf4 would work on these GPUs. *Note: If you know such work, please
    drop a link in the comments and I’ll update this paragraph.*'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 关于消费级GPU，我只能确定RTX 30xx系列GPU（我在我的RTX 3060上进行了测试）或更近期的型号支持这一点。理论上，它也应该适用于GTX 16xx和RTX
    20xx系列，因为它们也利用了Turing架构，但我没有进行测试，也没有找到GPTQ或bitsandbytes nf4在这些GPU上工作的证据。*注意：如果你知道这样的工作，请在评论中留下链接，我会更新这一段。*
- en: Software
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 软件
- en: '[GPTQ''s official repository is on GitHub](https://github.com/IST-DASLab/gptq)
    (Apache 2.0 License). It can be directly used to quantize OPT, BLOOM, or LLaMa,
    with 4-bit and 3-bit precision. However, you will find that most quantized LLMs
    available online, for instance, on the Hugging Face Hub, were quantized with [AutoGPTQ](https://github.com/PanQiWei/AutoGPTQ)
    (Apache 2.0 License).'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '[GPTQ的官方仓库在GitHub上](https://github.com/IST-DASLab/gptq)（Apache 2.0许可）。它可以直接用于量化OPT、BLOOM或LLaMa，支持4位和3位精度。然而，你会发现大多数在线可用的量化LLM，例如在Hugging
    Face Hub上，都是用[AutoGPTQ](https://github.com/PanQiWei/AutoGPTQ)（Apache 2.0许可）进行量化的。'
- en: AutoGPTQ is user-friendly and uses interfaces from Hugging Face transformers.
    It is also well-documented and maintained.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: AutoGPTQ用户友好，使用Hugging Face transformers中的接口。它也有很好的文档和维护。
- en: 'To install AutoGPTQ, I recommend following the instructions given in the GitHub:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 要安装AutoGPTQ，我建议按照GitHub上提供的说明进行操作：
- en: '[PRE0]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '[bitsandbytes library (MIT license) is defined by its repository](https://github.com/TimDettmers/bitsandbytes)
    as a “wrapper around CUDA custom functions”. In practice, it is directly supported
    by Hugging Face transformers, very much like Accelerate and Datasets, even though
    bitsandbytes is not an official Hugging Face library. For instance, if you try
    to quantize the model when loading it, transformers will tell you to install bitsandbytes.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '[bitsandbytes库（MIT许可）由其仓库定义](https://github.com/TimDettmers/bitsandbytes)为“CUDA自定义函数的封装器”。实际上，它被Hugging
    Face transformers直接支持，非常类似于Accelerate和Datasets，尽管bitsandbytes不是官方的Hugging Face库。例如，如果你在加载模型时尝试量化，transformers会告诉你安装bitsandbytes。'
- en: 'You can get it with pip:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过pip安装：
- en: '[PRE1]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '*Note: 1 day after I submitted this article to Towards Data Science, Hugging
    Face added the support of AutoGPTQ models in the transformers library. Using GPTQ
    inside transformers may enable even better performance than the performance I
    report in the following section.*'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '*注意：在我将这篇文章提交给Towards Data Science的一天后，Hugging Face在transformers库中增加了对AutoGPTQ模型的支持。在transformers中使用GPTQ可能会比我在下一节中报告的性能更好。*'
- en: Comparison between GPTQ and bitsandbytes
  id: totrans-48
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: GPTQ和bitsandbytes之间的比较
- en: 'In this section, I present comparisons between models quantized using both
    quantization methods, according to:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我将展示使用两种量化方法的模型之间的比较。
- en: The perplexity they reach on some datasets
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它们在一些数据集上的困惑度
- en: Their GPU VRAM consumption
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 他们的GPU VRAM消耗
- en: Their inference speed
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 他们的推理速度
- en: For the perplexity evaluation, I rely on numbers already published. For VRAM
    consumption, I rely on my own experiments, also supported by numbers already published.
    For the inference speed, I couldn’t easily find results already published online,
    so I present my own results obtained with Llama 2 7B.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 对于困惑度评估，我依赖于已经发布的数字。对于VRAM消耗，我依赖于我自己的实验，同时也得到了已经发布的数字的支持。对于推理速度，我没有轻易找到已经在线发布的结果，因此我展示了我使用Llama
    2 7B获得的结果。
- en: 'You can reproduce my results using my notebook published in The Kaitchup ([notebook
    #11](https://kaitchup.substack.com/p/notebooks)), my substack newsletter.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '你可以使用我在The Kaitchup上发布的笔记本（[notebook #11](https://kaitchup.substack.com/p/notebooks)）来重现我的结果，这是我的substack通讯。'
- en: 'GPTQ vs. bitsandbytes: Perplexity'
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: GPTQ与bitsandbytes：困惑度
- en: The main author of AutoGPTQ evaluated LLaMa (the first version) quantized with
    GPTQ and bitsandbytes by computing the perplexity on the C4 dataset.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: AutoGPTQ的主要作者通过计算C4数据集上的困惑度来评估了量化为GPTQ和bitsandbytes的LLaMa（第一版）。
- en: 'The results are released in the “GPTQ-for-LLaMA” repository (Apache 2.0 License):'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 结果发布在“GPTQ-for-LLaMA”仓库（Apache 2.0许可证）中：
- en: '![](../Images/020cc9ed138d3b49aa43648a37811609.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/020cc9ed138d3b49aa43648a37811609.png)'
- en: Results provided by [GPTQ-for-LLaMA](https://github.com/qwopqwop200/GPTQ-for-LLaMa)
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 结果由 [GPTQ-for-LLaMA](https://github.com/qwopqwop200/GPTQ-for-LLaMa) 提供
- en: There is a lot of information to unpack here. Each table presents the results
    for a different size of LLaMA. Let’s focus on the last column, c4(ppl), for each
    table.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有很多信息需要解读。每个表格展示了不同大小的LLaMA的结果。让我们关注每个表格中的最后一列c4(ppl)。
- en: We will compare GPTQ-128g, which is GPTQ 4-bit, with nf4-double_quant and nf4
    which are both bitsandbytes quantization algorithms. “nf4-double_quant” is a variant
    that quantizes the quantization constants.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将比较GPTQ-128g（GPTQ 4-bit）与nf4-double_quant和nf4（这两者都是bitsandbytes量化算法）。 “nf4-double_quant”是一个变体，量化了量化常数。
- en: For the 7B version, they all perform the same, at 5.30 ppl.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 对于7B版本，它们的表现相同，都为5.30 ppl。
- en: We see a difference for the 13B and 33B versions where GPTQ yields a lower perplexity.
    The results suggest that GPTQ seems better, compared to nf4, as the model gets
    bigger.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到13B和33B版本存在差异，其中GPTQ产生了较低的困惑度。结果表明，相对于nf4，GPTQ似乎在模型变大时表现更好。
- en: GPTQ seems to have a small advantage here over bitsandbytes’ nf4.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: GPTQ似乎在这里比bitsandbytes的nf4略有优势。
- en: 'GPTQ vs. bitsandbytes: VRAM Usage'
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: GPTQ与bitsandbytes：VRAM使用情况
- en: In the table above, the author also reports on VRAM usage. We can see that nf4-double_quant
    and GPTQ use almost the same amount of memory.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在上面的表格中，作者还报告了VRAM使用情况。我们可以看到，nf4-double_quant和GPTQ使用的内存几乎相同。
- en: nf4 without double quantization significantly uses more memory than GPTQ. The
    difference for LLaMA 33B is greater than 1 GB. Double quantization is necessary
    to match GPTQ quantization performance.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: nf4在没有双重量化的情况下显著使用了比GPTQ更多的内存。LLaMA 33B的差异大于1 GB。双重量化是匹配GPTQ量化性能所必需的。
- en: 'In my own experiments with Llama 2 7B and using 3 different GPUs, I also observed
    that GPTQ and nf4-double_quant consume a very similar amount of VRAM. *Note: I
    used the T4, V100, and A100 40 GB GPUs that are all available on Google Colab
    PRO.*'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 在我自己使用3种不同GPU的Llama 2 7B实验中，我也观察到GPTQ和nf4-double_quant消耗的VRAM量非常相似。*注意：我使用了T4、V100和A100
    40 GB GPU，这些都可以在Google Colab PRO上使用。*
- en: 'GPTQ vs. bitsandbytes: Inference Speed'
  id: totrans-69
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: GPTQ与bitsandbytes：推理速度
- en: To get an idea about the inference speed, I have run 5 different prompts with
    both quantized models, without batching, generating outputs of up to 1,000 tokens
    ([see the notebook](https://kaitchup.substack.com/p/notebooks)). For each prompt,
    I counted how many tokens per second were generated. Then, I averaged over the
    5 prompts.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 为了了解推理速度，我运行了5个不同的提示，对两个量化模型进行测试，没有进行批处理，生成了最多1,000个token的输出（[查看笔记本](https://kaitchup.substack.com/p/notebooks)）。对于每个提示，我计算了每秒生成的token数量。然后，我对5个提示的结果取了平均值。
- en: '![](../Images/2d78940a365267e6ff2635a76f6847a2.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2d78940a365267e6ff2635a76f6847a2.png)'
- en: Table by the author.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的表格。
- en: The clear winner here is GPTQ. It is twice faster as bitsandbytes nf4 with double
    quantization.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 这里明显的赢家是GPTQ。它的速度是bitsandbytes nf4双重量化的两倍。
- en: The results also show that almost the same speed is achieved by the 3 GPUs.
    Getting more expensive GPUs won’t make the inference faster if you don’t do batching.
    However, even with small batches, I would expect to see some significant differences
    between the T4 and the A100.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 结果还显示，三种GPU几乎实现了相同的速度。如果不进行批处理，购买更昂贵的GPU不会加快推理速度。然而，即使在小批量的情况下，我也预期看到T4与A100之间的一些显著差异。
- en: 'Conclusion: GPTQ vs. bitsandbytes'
  id: totrans-75
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论：GPTQ与bitsandbytes
- en: To sum up, GPTQ 4-bit is better than bitsandbytes nf4 if you are looking for
    better performance. It achieves a lower perplexity and a faster inference for
    similar VRAM consumption.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，如果你追求更好的性能，GPTQ 4位比bitsandbytes nf4更好。它在相似的VRAM消耗下实现了更低的困惑度和更快的推理速度。
- en: However, if you are looking for fine-tuning quantized models, bitsandbytes is
    a better and more convenient alternative thanks to its support by Hugging Face
    libraries. bitsandbytes quantization is the algorithm behind QLoRA which allows
    fine-tuning of quantized models with adapters.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，如果你希望对量化模型进行微调，bitsandbytes是一个更好且更方便的选择，因为它得到了Hugging Face库的支持。bitsandbytes量化是QLoRA背后的算法，QLoRA允许使用适配器对量化模型进行微调。
- en: 'If you are developing/deploying LLMs on consumer hardware, I would recommend
    the following:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你在消费级硬件上开发/部署LLM，我建议你考虑以下几点：
- en: Fine-tune the LLM with bitsandbytes nf4 and QLoRa
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用bitsandbytes nf4和QLoRa对LLM进行微调
- en: Merge the adapter into the LLM
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将适配器合并到LLM中
- en: Quantize the resulting model with GPTQ 4-bit
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用GPTQ 4位量化结果模型
- en: Deploy
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 部署
- en: 'Here is a summary of the pros and cons of both quantization methods:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是两种量化方法的优缺点总结：
- en: '**bitsandbytes pros**:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '**bitsandbytes优点：**'
- en: Supports QLoRa
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 支持QLoRa
- en: On-the-fly quantization
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实时量化
- en: '**bitsandbytes cons**:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '**bitsandbytes缺点：**'
- en: Slow inference
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 推理较慢
- en: Quantized models can’t be serialized
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 量化模型不能被序列化
- en: '**GPTQ pros**:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '**GPTQ优点：**'
- en: Serialization
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 序列化
- en: Supports 3-bit precision
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 支持3位精度
- en: Fast
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 快速
- en: '**GPTQ cons:**'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '**GPTQ缺点：**'
- en: Model quantization is slow
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型量化较慢
- en: Fine-tuning GPTQ models is possible but understudied
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对GPTQ模型进行微调是可能的，但研究不足
