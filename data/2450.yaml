- en: 4-bit Quantization with GPTQ
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 4位量化与GPTQ
- en: 原文：[https://towardsdatascience.com/4-bit-quantization-with-gptq-36b0f4f02c34?source=collection_archive---------0-----------------------#2023-07-31](https://towardsdatascience.com/4-bit-quantization-with-gptq-36b0f4f02c34?source=collection_archive---------0-----------------------#2023-07-31)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/4-bit-quantization-with-gptq-36b0f4f02c34?source=collection_archive---------0-----------------------#2023-07-31](https://towardsdatascience.com/4-bit-quantization-with-gptq-36b0f4f02c34?source=collection_archive---------0-----------------------#2023-07-31)
- en: Quantize your own LLMs using AutoGPTQ
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用AutoGPTQ量化你自己的LLM
- en: '[](https://medium.com/@mlabonne?source=post_page-----36b0f4f02c34--------------------------------)[![Maxime
    Labonne](../Images/a7efdd305e3cc77d5509bbb1076d57d8.png)](https://medium.com/@mlabonne?source=post_page-----36b0f4f02c34--------------------------------)[](https://towardsdatascience.com/?source=post_page-----36b0f4f02c34--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----36b0f4f02c34--------------------------------)
    [Maxime Labonne](https://medium.com/@mlabonne?source=post_page-----36b0f4f02c34--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@mlabonne?source=post_page-----36b0f4f02c34--------------------------------)[![Maxime
    Labonne](../Images/a7efdd305e3cc77d5509bbb1076d57d8.png)](https://medium.com/@mlabonne?source=post_page-----36b0f4f02c34--------------------------------)[](https://towardsdatascience.com/?source=post_page-----36b0f4f02c34--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----36b0f4f02c34--------------------------------)
    [Maxime Labonne](https://medium.com/@mlabonne?source=post_page-----36b0f4f02c34--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fdc89da634938&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F4-bit-quantization-with-gptq-36b0f4f02c34&user=Maxime+Labonne&userId=dc89da634938&source=post_page-dc89da634938----36b0f4f02c34---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----36b0f4f02c34--------------------------------)
    ·10 min read·Jul 31, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F36b0f4f02c34&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F4-bit-quantization-with-gptq-36b0f4f02c34&user=Maxime+Labonne&userId=dc89da634938&source=-----36b0f4f02c34---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fdc89da634938&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F4-bit-quantization-with-gptq-36b0f4f02c34&user=Maxime+Labonne&userId=dc89da634938&source=post_page-dc89da634938----36b0f4f02c34---------------------post_header-----------)
    发表在[Towards Data Science](https://towardsdatascience.com/?source=post_page-----36b0f4f02c34--------------------------------)
    · 10分钟阅读 · 2023年7月31日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F36b0f4f02c34&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F4-bit-quantization-with-gptq-36b0f4f02c34&user=Maxime+Labonne&userId=dc89da634938&source=-----36b0f4f02c34---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F36b0f4f02c34&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F4-bit-quantization-with-gptq-36b0f4f02c34&source=-----36b0f4f02c34---------------------bookmark_footer-----------)![](../Images/c14c3bcded8e89b92f862c1114cba7ff.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F36b0f4f02c34&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F4-bit-quantization-with-gptq-36b0f4f02c34&source=-----36b0f4f02c34---------------------bookmark_footer-----------)![](../Images/c14c3bcded8e89b92f862c1114cba7ff.png)'
- en: Image by author
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供
- en: Recent advancements in weight quantization allow us to run massive large language
    models on consumer hardware, like a LLaMA-30B model on an RTX 3090 GPU. This is
    possible thanks to novel 4-bit quantization techniques with minimal performance
    degradation, like [GPTQ](https://arxiv.org/abs/2210.17323), [GGML](https://github.com/ggerganov/ggml),
    and [NF4](https://huggingface.co/blog/4bit-transformers-bitsandbytes).
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 最近的权重量化进展使我们能够在消费者硬件上运行大规模语言模型，例如在RTX 3090 GPU上运行LLaMA-30B模型。这得益于新颖的4位量化技术，性能降级最小，如[GPTQ](https://arxiv.org/abs/2210.17323)、[GGML](https://github.com/ggerganov/ggml)和[NF4](https://huggingface.co/blog/4bit-transformers-bitsandbytes)。
- en: In the [previous article](https://medium.com/towards-data-science/introduction-to-weight-quantization-2494701b9c0c),
    we introduced naïve 8-bit quantization techniques and the excellent LLM.int8().
    In this article, we will explore the popular **GPTQ algorithm** to understand
    how it works and implement it using the [AutoGPTQ](https://github.com/PanQiWei/AutoGPTQ)
    library.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在[上一篇文章](https://medium.com/towards-data-science/introduction-to-weight-quantization-2494701b9c0c)中，我们介绍了朴素的8位量化技术和优秀的LLM.int8()。在本文中，我们将深入探讨流行的**GPTQ算法**，了解其工作原理，并使用[AutoGPTQ](https://github.com/PanQiWei/AutoGPTQ)库进行实现。
- en: You can find the code on [Google Colab](https://colab.research.google.com/drive/1lSvVDaRgqQp_mWK_jC9gydz6_-y6Aq4A?usp=sharing)
    and [GitHub](https://github.com/mlabonne/llm-course/tree/main).
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在 [Google Colab](https://colab.research.google.com/drive/1lSvVDaRgqQp_mWK_jC9gydz6_-y6Aq4A?usp=sharing)
    和 [GitHub](https://github.com/mlabonne/llm-course/tree/main) 上找到代码。
- en: 🧠 Optimal Brain Quantization
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 🧠 最优脑量化
- en: 'Let’s start by introducing the problem we’re trying to solve. For every layer
    ℓ in the network, we want to find a quantized version **Ŵₗ** *of the original
    weights* **Wₗ**. This is called the **layer-wise compression problem**. More specifically,
    to minimize performance degradation, we want the outputs (**Ŵ**ᵨ**X**ᵨ) of these
    new weights to be as close as possible to the original ones (**W**ᵨ**X**ᵨ). In
    other words, we want to find:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 首先介绍我们要解决的问题。对于网络中的每一层 ℓ，我们希望找到原始权重 **Wₗ** 的量化版本 **Ŵₗ**。这称为 **逐层压缩问题**。更具体地，为了最小化性能降级，我们希望这些新权重的输出
    (**Ŵ**ᵨ**X**ᵨ) 尽可能接近原始权重的输出 (**W**ᵨ**X**ᵨ)。换句话说，我们希望找到：
- en: '![](../Images/bea35e882148e389c0c3d23f956d46d2.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bea35e882148e389c0c3d23f956d46d2.png)'
- en: Different approaches have been proposed to solve this problem, but we’re interested
    in the [**Optimal Brain Quantizer**](https://arxiv.org/abs/2208.11580) (OBQ) framework
    here.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 已提出了不同的方法来解决这个问题，但我们在这里关注的是 [**最优脑量化器**](https://arxiv.org/abs/2208.11580)（OBQ）框架。
- en: 'This method is inspired by a **pruning technique** to carefully remove weights
    from a fully trained dense neural network (Optimal Brain Surgeon). It uses an
    approximation technique and provides explicit formulas for the best single weight
    *w𐞥* to remove and optimal update *δ*ꟳ to adjust the set of remaining non-quantized
    weights *F* to make up for the removal:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 该方法的灵感来源于一种 **剪枝技术**，用于从完全训练的密集神经网络中小心地去除权重（最优脑外科医生）。它使用了一种近似技术，并提供了最佳单个权重 *w𐞥*
    的显式公式以去除，并通过最佳更新 *δ*ꟳ 来调整剩余未量化权重 *F*，以弥补去除的影响：
- en: '![](../Images/c28f4ed29652a4fba9e7411172489f2a.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c28f4ed29652a4fba9e7411172489f2a.png)'
- en: where quant(*w*) is the weight rounding given by the quantization and **H**ꟳ
    is the Hessian.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 quant(*w*) 是由量化给出的权重舍入，**H**ꟳ 是 Hessian。
- en: Using OBQ, we can quantize the easiest weight first and then adjust all remaining
    non-quantized weights to **compensate for this precision loss**. Then we pick
    the next weight to quantize, and so on.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 OBQ，我们可以先量化最简单的权重，然后调整所有剩余的未量化权重以 **补偿这种精度损失**。然后我们选择下一个要量化的权重，依此类推。
- en: 'A potential issue with this approach is when there are outlier weights, which
    can result in high **quantization error**. Usually, these outliers would be quantized
    last, when there are few non-quantized weights left that could be adjusted to
    compensate for the large error. This effect can worsen when some weights are pushed
    further outside the grid by intermediate updates. A simple heuristic is applied
    to prevent this: outliers are quantized as soon as they appear.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法的一个潜在问题是，当存在异常权重时，可能导致高 **量化误差**。通常，这些异常值会在量化最后处理，当剩下的未量化权重较少时，可以调整以弥补大误差。当一些权重通过中间更新被推到网格之外时，这种效应可能会加剧。一个简单的启发式方法是尽快量化出现的异常值。
- en: 'This process could be computationally heavy, especially for LLMs. To deal with
    this, the OBQ method uses a trick that avoids redoing the entire computation each
    time a weight is simplified. After quantizing a weight, it adjusts the matrix
    used in calculations (the Hessian) by **removing the row and column** associated
    with that weight (using Gaussian elimination):'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 这个过程可能计算量很大，特别是对于 LLM。为了解决这个问题，OBQ 方法使用了一种技巧，避免了每次简化权重时重新进行整个计算。量化一个权重后，它通过
    **去除与该权重相关的行和列**（使用高斯消元法）来调整计算中使用的矩阵（Hessian）：
- en: '![](../Images/04c24c3d6a72907b1b3508009c8a72d8.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/04c24c3d6a72907b1b3508009c8a72d8.png)'
- en: The method also employs vectorization to process multiple rows of the weight
    matrix at once. Despite its efficiency, the OBQ’s computation time increases significantly
    as the size of the weight matrix increases. This cubic growth makes it difficult
    to use OBQ on very large models with billions of parameters.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 该方法还采用了向量化处理一次处理多个权重矩阵的行。尽管效率很高，但随着权重矩阵大小的增加，OBQ 的计算时间显著增加。这种立方增长使得在具有数十亿参数的大型模型上使用
    OBQ 变得困难。
- en: 🧮 The GPTQ Algorithm
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 🧮 GPTQ 算法
- en: Introduced by Frantar et al. (2023), the [GPTQ algorithm](https://arxiv.org/abs/2210.17323)
    takes inspiration from the OBQ method, but with significant improvements to scale
    it for (very) large language models.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 由Frantar等人（2023年）提出的[GPTQ算法](https://arxiv.org/abs/2210.17323)从OBQ方法中获得灵感，但对其进行了重大改进，以使其适用于（非常）大的语言模型。
- en: 'Step 1: Arbitrary Order Insight'
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第一步：任意顺序洞察
- en: The OBQ method selects weights (parameters in a model) for quantization in a
    certain order, determined by which will **add the least additional error**. However,
    GPTQ observes that for large models, quantizing weights in any fixed order can
    perform just as well. This is because even though some weights might introduce
    more error individually, they are quantized later in the process when there are
    few other weights left that could increase the error. So the order doesn’t matter
    as much as we thought.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: OBQ方法按一定顺序选择要量化的权重（模型中的参数），该顺序由**增加最少额外误差**的权重决定。然而，GPTQ观察到，对于大型模型，按任何固定顺序量化权重的效果都一样。这是因为即使某些权重可能单独引入更多误差，它们会在处理过程中较晚的阶段进行量化，此时可能没有其他权重可以增加误差。因此，顺序并不像我们想象的那样重要。
- en: Based on this insight, GPTQ aims to quantize all weights in the **same order
    for all rows** of a matrix. This makes the process faster because certain computations
    have to be done only once for each column, rather than once for each weight.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 基于这一洞察，GPTQ旨在对矩阵的**所有行使用相同的顺序量化所有权重**。这使得过程更快，因为某些计算只需要对每一列进行一次，而不是对每个权重进行一次。
- en: '![](../Images/dd7b329f93ed489449b8f138e55b63b5.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/dd7b329f93ed489449b8f138e55b63b5.png)'
- en: Image by author
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供
- en: 'Step 2: Lazy Batch-Updates'
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第二步：懒惰批量更新
- en: This scheme won’t be fast because it requires updating a **huge matrix** with
    very few computations for each entry. This type of operation can’t utilize the
    full compute capabilities of GPUs and will be slowed down by memory limitations
    (memory throughput bottleneck).
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 这个方案不会很快，因为它需要更新一个**巨大的矩阵**，每个条目的计算量非常少。这种操作无法利用GPU的全部计算能力，并且会受到内存限制（内存吞吐瓶颈）的影响。
- en: To resolve this, GPTQ introduces “lazy batch” updates. It turns out that the
    final rounding decisions for a given column are only affected by updates performed
    on that column, not on later columns. Therefore, GPTQ can apply the algorithm
    to a **batch of columns at a time** (like 128 columns), updating only those columns
    and a corresponding block of the matrix. After a block is fully processed, the
    algorithm performs global updates on the entire matrix.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这一问题，GPTQ引入了“懒惰批量”更新。结果发现，给定列的最终舍入决策只受该列上的更新影响，而不受后续列的影响。因此，GPTQ可以将算法应用于**一次处理一批列**（如128列），仅更新这些列及矩阵中的相应块。一个块完全处理后，算法对整个矩阵进行全局更新。
- en: '![](../Images/8fe33b573159cfae9d1247d3073088cf.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8fe33b573159cfae9d1247d3073088cf.png)'
- en: 'Step 3: Cholesky Reformulation'
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第三步：Cholesky重构
- en: However, there’s one more issue to address. When the algorithm scales up to
    very large models, numerical inaccuracies can become a problem. Specifically,
    repeated applications of a certain operation can **accumulate numerical errors**.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 不过，还有一个问题需要解决。当算法扩展到非常大的模型时，数值不准确可能成为问题。具体来说，某些操作的重复应用可能会**积累数值误差**。
- en: To tackle this, GPTQ uses a [Cholesky decomposition](https://en.wikipedia.org/wiki/Cholesky_decomposition),
    a numerically stable method for solving certain mathematical problems. It involves
    precomputing some required information from the matrix using the Cholesky method.
    This approach, combined with a slight “dampening” (adding a small constant to
    diagonal elements of the matrix), helps the algorithm to avoid numerical issues.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这个问题，GPTQ使用了[Cholesky分解](https://en.wikipedia.org/wiki/Cholesky_decomposition)，这是一种数值稳定的数学问题解决方法。它涉及使用Cholesky方法从矩阵中预计算一些所需的信息。这种方法，加上轻微的“减震”（向矩阵对角线元素中添加一个小常数），有助于算法避免数值问题。
- en: 'The full algorithm can be summarized in a few steps:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 完整算法可以总结为几个步骤：
- en: The GPTQ algorithm begins with a Cholesky decomposition of the Hessian inverse
    (a matrix that helps decide how to adjust the weights)
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: GPTQ算法以Hessian逆矩阵的Cholesky分解开始（该矩阵有助于决定如何调整权重）。
- en: It then runs in loops, handling batches of columns at a time.
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后它以循环的方式运行，每次处理一批列。
- en: For each column in a batch, it quantizes the weights, calculates the error,
    and updates the weights in the block accordingly.
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于每个批次中的每一列，它对权重进行量化，计算误差，并相应地更新块中的权重。
- en: After processing the batch, it updates all remaining weights based on the block’s
    errors.
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 处理批次后，它会根据块的错误更新所有剩余的权重。
- en: The GPTQ algorithm was tested on various language generation tasks. It was compared
    with other quantization methods, like rounding all weights to the nearest quantized
    value (RTN). GPTQ was used with the BLOOM (176B parameters) and OPT (175B parameters)
    model families, and models were quantized using a **single NVIDIA A100 GPU**.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: GPTQ算法在各种语言生成任务中进行了测试。它与其他量化方法进行了比较，比如将所有权重舍入到最近的量化值（RTN）。GPTQ与BLOOM（176B参数）和OPT（175B参数）模型系列一起使用，并且模型是使用**单个NVIDIA
    A100 GPU**进行量化的。
- en: 💻 Quantize an LLM with AutoGPTQ
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 💻 使用AutoGPTQ对LLM进行量化
- en: GPTQ has been very popular to create models in 4-bit precision that can efficiently
    run on GPUs. You can find many examples on the Hugging Face Hub, especially from
    [TheBloke](https://huggingface.co/TheBloke). If you’re looking for an approach
    that is more CPU-friendly, [GGML](https://github.com/ggerganov/ggml) is currently
    your best option. Finally, the `transformers` library with `bitsandbytes` allows
    you to quantize a model when it's loaded using the `load_in_4bit=true` argument,
    which requires downloading full models and storing them in your RAM.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: GPTQ在创建可以高效运行于GPU上的4位精度模型方面非常受欢迎。你可以在Hugging Face Hub上找到许多示例，特别是来自[TheBloke](https://huggingface.co/TheBloke)。如果你在寻找更适合CPU的方案，[GGML](https://github.com/ggerganov/ggml)目前是你的最佳选择。最后，`transformers`库与`bitsandbytes`允许你在加载模型时通过`load_in_4bit=true`参数进行量化，这需要下载完整的模型并将其存储在你的RAM中。
- en: Let’s implement the GPTQ algorithm using the AutoGPTQ library and quantize a
    GPT-2 model. This requires a GPU, but a free T4 on Google Colab will do. We start
    by loading the libraries and defining the model we want to quantize (in this case,
    GPT-2).
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用AutoGPTQ库实现GPTQ算法，并对GPT-2模型进行量化。这需要一个GPU，但Google Colab上的免费T4就足够了。我们从加载库和定义要量化的模型（在这种情况下是GPT-2）开始。
- en: '[PRE0]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '[PRE1]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: We now want to load the model and the tokenizer. The tokenizer is loaded using
    the classic `AutoTokenizer` class from the `transformers` library. On the other
    hand, we need to pass a specific configuration (`BaseQuantizeConfig`) to load
    the model.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在要加载模型和分词器。分词器使用`transformers`库中的经典`AutoTokenizer`类加载。另一方面，我们需要传递一个特定的配置（`BaseQuantizeConfig`）来加载模型。
- en: 'In this configuration, we can specify the number of bits to quantize (here,
    `bits=4`) and the group size (size of the lazy batch). Note that this group size
    is optional: we could also use **one set of parameters** for the entire weight
    matrix. In practice, these groups generally improve the quality of the quantization
    at a very low cost (especially with `group_size=1024`). The `damp_percent` value
    is here to help the Cholesky reformulation and should not be changed.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在此配置中，我们可以指定要量化的位数（这里是`bits=4`）和组大小（懒批次的大小）。请注意，这个组大小是可选的：我们也可以为整个权重矩阵使用**一组参数**。实际上，这些组通常能以非常低的成本提高量化的质量（特别是使用`group_size=1024`时）。`damp_percent`值用于帮助Cholesky重整化，不应更改。
- en: Finally, the `desc_act` (also called act order) is a tricky parameter. It allows
    you to **process rows based on decreasing activation**, meaning the most important
    or impactful rows (determined by sampled inputs and outputs) are processed first.
    This method aims to place most of the quantization error (inevitably introduced
    during quantization) on less significant weights. This approach improves the overall
    accuracy of the quantization process by ensuring the most significant weights
    are processed with greater precision. However, when used alongside group size,
    `desc_act` can lead to performance slowdowns due to the need to frequently reload
    quantization parameters. For this reason, we won't use it here (it will probably
    be fixed in the future, however).
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，`desc_act`（也称为act order）是一个棘手的参数。它允许你**根据递减的激活处理行**，意味着最重要或影响最大的行（由采样的输入和输出决定）首先处理。这种方法旨在将大部分量化误差（不可避免地在量化过程中引入）放在不太重要的权重上。通过确保最重要的权重以更高的精度处理，这种方法可以提高量化过程的整体准确性。然而，当与组大小一起使用时，`desc_act`可能会导致性能下降，因为需要频繁重新加载量化参数。因此，我们在这里不会使用它（不过它可能会在未来得到修复）。
- en: '[PRE2]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The quantization process **relies heavily on samples** to evaluate and enhance
    the quality of the quantization. They provide a means of comparison between the
    outputs produced by the origina and the newly quantized model. The larger the
    number of samples provided, the greater the potential for more accurate and effective
    comparisons, leading to improved quantization quality.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 量化过程**严重依赖样本**来评估和提升量化质量。样本提供了一个比较原始模型和新量化模型输出的方法。提供的样本数量越多，比较的潜力就越大，从而提高量化质量。
- en: In the context of this article, we utilize the [**C4 (Colossal Clean Crawled
    Corpus) dataset**](https://huggingface.co/datasets/c4) to generate our samples.
    The C4 dataset is a large-scale, multilingual collection of web text gathered
    from the Common Crawl project. This expansive dataset has been cleaned and prepared
    specifically for training large-scale language models, making it a great resource
    for tasks such as this. The WikiText dataset is another popular option.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文的背景下，我们使用了[**C4 (Colossal Clean Crawled Corpus) 数据集**](https://huggingface.co/datasets/c4)来生成样本。C4数据集是一个大规模、多语言的网页文本集合，来自Common
    Crawl项目。这个庞大的数据集已经被清理和准备好，专门用于训练大规模语言模型，使其成为类似任务的极佳资源。WikiText数据集是另一个受欢迎的选择。
- en: In the following code block, we load 1024 samples from the C4 dataset, tokenize
    them, and format them.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下代码块中，我们从C4数据集中加载1024个样本，对其进行分词和格式化。
- en: '[PRE3]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Now that dataset is ready, we can start the quantization process with a batch
    size of 1\. Optionally, we also use [OpenAI Triton](https://github.com/openai/triton),
    a CUDA alternative, to communicate with the GPU. Once this is done, we save the
    tokenizer and the model in a safetensors format.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 现在数据集已经准备好，我们可以开始量化过程，批量大小为1。可选地，我们还使用了[OpenAI Triton](https://github.com/openai/triton)，这是CUDA的替代方案，用于与GPU进行通信。一旦完成，我们将分词器和模型保存为safetensors格式。
- en: '[PRE4]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: As per usual, the model and tokenizer can then be loaded from the output directory
    using the `AutoGPTQForCausalLM` and `AutoTokenizer` classes.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 像往常一样，模型和分词器可以从输出目录中加载，使用`AutoGPTQForCausalLM`和`AutoTokenizer`类。
- en: '[PRE5]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Let’s check that the model is working correctly. The AutoGPTQ model (mostly)
    works as a normal `transformers` model, which makes it compatible with inference
    pipelines, as shown in the following example:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们检查模型是否正常工作。AutoGPTQ模型（大多数情况下）作为一个普通的`transformers`模型工作，这使得它与推理管道兼容，如以下示例所示：
- en: '[PRE6]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '[PRE7]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: We managed to get a convincing completion from our quantized GPT-2 model. A
    more in-depth evaluation would require **measuring the perplexity** of the quantized
    model versus the original one. However, we will leave it out of the scope of this
    article.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 我们成功地从量化后的GPT-2模型中获得了令人信服的结果。更深入的评估将需要**测量量化模型与原始模型的困惑度**。不过，这超出了本文的范围。
- en: Conclusion
  id: totrans-65
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: In this article, we introduced the GPTQ algorithm, a state-of-the-art quantization
    technique to run LLMs on consumer-grade hardware. We showed how it addresses the
    layer-wise compression problem, based on an improved OBS technique with arbitrary
    order insight, lazy batch updates, and Cholesky reformulation. This novel approach
    **significantly reduces memory and computation requirements**, making LLMs accessible
    to a broader audience.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们介绍了GPTQ算法，这是一种最先进的量化技术，用于在消费者级硬件上运行LLMs。我们展示了它如何解决基于改进的OBS技术的层级压缩问题，结合了任意顺序洞察、惰性批量更新和Cholesky重构。这种新颖的方法**显著减少了内存和计算需求**，使LLMs可以被更广泛的受众使用。
- en: 'In addition, we **quantized our own LLM model** on a free T4 GPU and ran it
    to generate text. You can push your own version of a GPTQ 4-bit quantized model
    on the Hugging Face Hub. As mentioned in the introduction, GPTQ is not the only
    4-bit quantization algorithm: [GGML](https://github.com/ggerganov/ggml) and [NF4](https://huggingface.co/blog/4bit-transformers-bitsandbytes)
    are excellent alternatives with slightly different scopes. I encourage you to
    learn more about them and give them a shot!'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们在免费的T4 GPU上**量化了我们自己的LLM模型**并运行它以生成文本。你可以在Hugging Face Hub上发布你自己的GPTQ 4-bit量化模型。正如引言中提到的，GPTQ并不是唯一的4-bit量化算法：[GGML](https://github.com/ggerganov/ggml)和[NF4](https://huggingface.co/blog/4bit-transformers-bitsandbytes)是具有稍微不同范围的优秀替代方案。我鼓励你了解更多，并尝试一下！
- en: If you’re interested in more technical content around LLMs, follow me on Twitter
    [@maximelabonne](https://twitter.com/maximelabonne).
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你对更多关于LLMs的技术内容感兴趣，可以在Twitter上关注我 [@maximelabonne](https://twitter.com/maximelabonne)。
- en: References
  id: totrans-69
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'B. Hassibi, D. G. Stork and G. J. Wolff, [“Optimal Brain Surgeon and general
    network pruning,”](https://ieeexplore.ieee.org/document/298572) IEEE International
    Conference on Neural Networks, San Francisco, CA, USA, 1993, pp. 293–299 vol.1,
    doi: 10.1109/ICNN.1993.298572.'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'B. Hassibi, D. G. Stork 和 G. J. Wolff, [**“优化脑外科医生和通用网络修剪”**](https://ieeexplore.ieee.org/document/298572)，IEEE
    国际神经网络大会，旧金山，美国，1993年，页码 293–299，doi: 10.1109/ICNN.1993.298572。'
- en: 'Elias Frantar, Sidak Pal Singh, & Dan Alistarh. (2023). [Optimal Brain Compression:
    A Framework for Accurate Post-Training Quantization and Pruning](https://arxiv.org/abs/2208.11580).'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Elias Frantar, Sidak Pal Singh 和 Dan Alistarh. (2023). [**优化脑压缩：准确的训练后量化和修剪框架**](https://arxiv.org/abs/2208.11580)。
- en: 'Elias Frantar, Saleh Ashkboos, Torsten Hoefler, & Dan Alistarh. (2023). [GPTQ:
    Accurate Post-Training Quantization for Generative Pre-trained Transformers](https://arxiv.org/abs/2210.17323).'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Elias Frantar, Saleh Ashkboos, Torsten Hoefler 和 Dan Alistarh. (2023). [**GPTQ：生成预训练变换器的准确训练后量化**](https://arxiv.org/abs/2210.17323)。
- en: Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael
    Matena, Yanqi Zhou, Wei Li, & Peter J. Liu. (2020). [Exploring the Limits of Transfer
    Learning with a Unified Text-to-Text Transformer](https://arxiv.org/abs/1910.10683v3).
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael
    Matena, Yanqi Zhou, Wei Li 和 Peter J. Liu. (2020). [**探索统一的文本到文本变换器的迁移学习极限**](https://arxiv.org/abs/1910.10683v3)。
- en: Related articles
  id: totrans-74
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 相关文章
- en: '[](/introduction-to-weight-quantization-2494701b9c0c?source=post_page-----36b0f4f02c34--------------------------------)
    [## Introduction to Weight Quantization'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/introduction-to-weight-quantization-2494701b9c0c?source=post_page-----36b0f4f02c34--------------------------------)
    [## **权重量化简介**](https://towardsdatascience.com/introduction-to-weight-quantization-2494701b9c0c?source=post_page-----36b0f4f02c34--------------------------------)'
- en: Reducing the size of Large Language Models with 8-bit quantization
  id: totrans-76
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用 8 位量化减少大型语言模型的大小
- en: towardsdatascience.com](/introduction-to-weight-quantization-2494701b9c0c?source=post_page-----36b0f4f02c34--------------------------------)
    [](/fine-tune-your-own-llama-2-model-in-a-colab-notebook-df9823a04a32?source=post_page-----36b0f4f02c34--------------------------------)
    [## Fine-Tune Your Own Llama 2 Model in a Colab Notebook
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '[towardsdatascience.com](/introduction-to-weight-quantization-2494701b9c0c?source=post_page-----36b0f4f02c34--------------------------------)
    [](/fine-tune-your-own-llama-2-model-in-a-colab-notebook-df9823a04a32?source=post_page-----36b0f4f02c34--------------------------------)
    [## **在 Colab Notebook 中微调你的 Llama 2 模型**](https://towardsdatascience.com/fine-tune-your-own-llama-2-model-in-a-colab-notebook-df9823a04a32?source=post_page-----36b0f4f02c34--------------------------------)'
- en: A practical introduction to LLM fine-tuning
  id: totrans-78
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 关于 LLM 微调的实用介绍
- en: towardsdatascience.com](/fine-tune-your-own-llama-2-model-in-a-colab-notebook-df9823a04a32?source=post_page-----36b0f4f02c34--------------------------------)
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '[towardsdatascience.com](/fine-tune-your-own-llama-2-model-in-a-colab-notebook-df9823a04a32?source=post_page-----36b0f4f02c34--------------------------------)'
- en: '*Learn more about machine learning and support my work with one click — become
    a Medium member here:*'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '*了解更多关于机器学习的内容，并通过一键支持我的工作 — 现在成为 Medium 会员：*'
- en: '[](https://medium.com/@mlabonne/membership?source=post_page-----36b0f4f02c34--------------------------------)
    [## Join Medium with my referral link — Maxime Labonne'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@mlabonne/membership?source=post_page-----36b0f4f02c34--------------------------------)
    [## **通过我的推荐链接加入 Medium — Maxime Labonne**](https://medium.com/@mlabonne/membership?source=post_page-----36b0f4f02c34--------------------------------)'
- en: As a Medium member, a portion of your membership fee goes to writers you read,
    and you get full access to every story…
  id: totrans-82
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 作为 Medium 会员，你的会员费用的一部分将会分配给你阅读的作者，并且你可以全面访问每个故事……
- en: medium.com](https://medium.com/@mlabonne/membership?source=post_page-----36b0f4f02c34--------------------------------)
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '[medium.com](https://medium.com/@mlabonne/membership?source=post_page-----36b0f4f02c34--------------------------------)'
- en: '*If you’re already a member, you can* [*follow me on Medium*](https://medium.com/@mlabonne)*.*'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '*如果你已经是会员，你可以* [*在 Medium 上关注我*](https://medium.com/@mlabonne)*。*'
