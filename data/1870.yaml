- en: 'Transformer Models 101: Getting Started — Part 2'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Transformer 模型 101：入门指南 — 第2部分
- en: 原文：[https://towardsdatascience.com/transformer-models-101-getting-started-part-2-4c9a45bf8b81?source=collection_archive---------13-----------------------#2023-06-06](https://towardsdatascience.com/transformer-models-101-getting-started-part-2-4c9a45bf8b81?source=collection_archive---------13-----------------------#2023-06-06)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/transformer-models-101-getting-started-part-2-4c9a45bf8b81?source=collection_archive---------13-----------------------#2023-06-06](https://towardsdatascience.com/transformer-models-101-getting-started-part-2-4c9a45bf8b81?source=collection_archive---------13-----------------------#2023-06-06)
- en: The complex math behind transformer models, in simple words
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Transformer 模型背后的复杂数学，用简单的词汇解释
- en: '[](https://nandinibansal1811.medium.com/?source=post_page-----4c9a45bf8b81--------------------------------)[![Nandini
    Bansal](../Images/a813ac8be6f89b2e856651fda66ab078.png)](https://nandinibansal1811.medium.com/?source=post_page-----4c9a45bf8b81--------------------------------)[](https://towardsdatascience.com/?source=post_page-----4c9a45bf8b81--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----4c9a45bf8b81--------------------------------)
    [Nandini Bansal](https://nandinibansal1811.medium.com/?source=post_page-----4c9a45bf8b81--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://nandinibansal1811.medium.com/?source=post_page-----4c9a45bf8b81--------------------------------)[![Nandini
    Bansal](../Images/a813ac8be6f89b2e856651fda66ab078.png)](https://nandinibansal1811.medium.com/?source=post_page-----4c9a45bf8b81--------------------------------)[](https://towardsdatascience.com/?source=post_page-----4c9a45bf8b81--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----4c9a45bf8b81--------------------------------)
    [Nandini Bansal](https://nandinibansal1811.medium.com/?source=post_page-----4c9a45bf8b81--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fc41c26af0465&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransformer-models-101-getting-started-part-2-4c9a45bf8b81&user=Nandini+Bansal&userId=c41c26af0465&source=post_page-c41c26af0465----4c9a45bf8b81---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----4c9a45bf8b81--------------------------------)
    ·7 min read·Jun 6, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F4c9a45bf8b81&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransformer-models-101-getting-started-part-2-4c9a45bf8b81&user=Nandini+Bansal&userId=c41c26af0465&source=-----4c9a45bf8b81---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fc41c26af0465&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransformer-models-101-getting-started-part-2-4c9a45bf8b81&user=Nandini+Bansal&userId=c41c26af0465&source=post_page-c41c26af0465----4c9a45bf8b81---------------------post_header-----------)
    发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----4c9a45bf8b81--------------------------------)
    · 7 分钟阅读 · 2023年6月6日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F4c9a45bf8b81&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransformer-models-101-getting-started-part-2-4c9a45bf8b81&user=Nandini+Bansal&userId=c41c26af0465&source=-----4c9a45bf8b81---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4c9a45bf8b81&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransformer-models-101-getting-started-part-2-4c9a45bf8b81&source=-----4c9a45bf8b81---------------------bookmark_footer-----------)![](../Images/b598c04052b566c5fdd8e24dde8376ae.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4c9a45bf8b81&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransformer-models-101-getting-started-part-2-4c9a45bf8b81&source=-----4c9a45bf8b81---------------------bookmark_footer-----------)![](../Images/b598c04052b566c5fdd8e24dde8376ae.png)'
- en: Image by [Dariusz Sankowski](https://pixabay.com/users/dariuszsankowski-1441456/?utm_source=link-attribution&utm_medium=referral&utm_campaign=image&utm_content=1052010)
    from [Pixabay](https://pixabay.com//?utm_source=link-attribution&utm_medium=referral&utm_campaign=image&utm_content=1052010)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由[Dariusz Sankowski](https://pixabay.com/users/dariuszsankowski-1441456/?utm_source=link-attribution&utm_medium=referral&utm_campaign=image&utm_content=1052010)提供，来源于[Pixabay](https://pixabay.com//?utm_source=link-attribution&utm_medium=referral&utm_campaign=image&utm_content=1052010)
- en: 'In the previous article, we looked at how the Encoder block of the Transformer
    model works in detail. If you haven’t read that article, I would recommend you
    to read it before starting with this one as the concepts covered there are carried
    forward in this article. You may head to:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一篇文章中，我们详细介绍了 Transformer 模型的编码器（Encoder）块的工作原理。如果你还没有阅读那篇文章，我建议你在阅读本文之前先阅读它，因为那里的概念在本文中有所延续。你可以前往：
- en: '[](/transformer-models-101-getting-started-part-1-b3a77ccfa14d?source=post_page-----4c9a45bf8b81--------------------------------)
    [## Transformer Models 101: Getting Started — Part 1'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/transformer-models-101-getting-started-part-1-b3a77ccfa14d?source=post_page-----4c9a45bf8b81--------------------------------)
    [## 变换器模型 101：入门 — 第 1 部分'
- en: Complex maths behind transformer models in simple words...
  id: totrans-11
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 用简单的语言解释变换器模型背后的复杂数学...
- en: towardsdatascience.com](/transformer-models-101-getting-started-part-1-b3a77ccfa14d?source=post_page-----4c9a45bf8b81--------------------------------)
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/transformer-models-101-getting-started-part-1-b3a77ccfa14d?source=post_page-----4c9a45bf8b81--------------------------------)
- en: If you have already read it, awesome! Let’s get started with a deep dive into
    the Decoder block and the complex maths associated with it.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你已经阅读了这些内容，太棒了！让我们开始深入了解解码器块及其相关的复杂数学。
- en: Decoder of Transformer
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 变换器的解码器
- en: 'Like the Encoder block of the Transformer models, the Decoder block consists
    of N stacked decoders that function sequentially and accept the input from the
    previous decoder. However, that is not the only input accepted by the decoder.
    The sentence representation generated by the Encoder block is fed to every decoder
    in the Decoder block. Therefore, we can conclude that each decoder accepts two
    different inputs:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 像变换器模型的编码器块一样，解码器块由 N 个堆叠的解码器组成，这些解码器按顺序工作，并接受来自前一个解码器的输入。然而，这并不是解码器唯一接受的输入。由编码器块生成的句子表示会被传递给解码器块中的每一个解码器。因此，我们可以得出结论，每个解码器接受两个不同的输入：
- en: Sentence representation from the Encoder Block
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 来自编码器块的句子表示
- en: The output of the previous Decoder
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 之前解码器的输出
- en: '![](../Images/1144b091a8021ae2198f3598dc9f9afe.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1144b091a8021ae2198f3598dc9f9afe.png)'
- en: Fig 1\. Encoder & Decoder blocks functioning together (Image by Author)
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1\. 编码器和解码器块的协同工作（图片由作者提供）
- en: Before we delve any deeper into the different components that make up a Decoder,
    it is essential to have an intuition of how the decoder in general generates the
    output sentence or target sentence.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们进一步探讨解码器的不同组件之前，了解解码器一般是如何生成输出句子或目标句子的直观感受是很重要的。
- en: How is the target sentence generated?
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 目标句子是如何生成的？
- en: At timestep t=1, only <sos> token or the ***start of the sentence*** is passed
    as input to the decoder block. Based on the <sos>, the first word of the target
    sentence is generated by the decoder block.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在时间步 t=1 时，只有 <sos> 标记或 ***句子开始*** 被传递给解码器块作为输入。基于 <sos>，解码器块生成目标句子的第一个单词。
- en: In the next timestamp i.e. t=2, the input to the decoder block includes the
    <sos> token as well as the first word generated by the decoder block. The next
    word is generated based on this input.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一个时间戳，即 t=2，解码器块的输入包括 <sos> 标记以及解码器块生成的第一个单词。下一个单词是基于这个输入生成的。
- en: Similarly, with every timestamp increment, the length of the input to the decoder
    block increases as the word generated in the previous timestamp is added to the
    current input sentence.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，每次时间戳增加时，解码器块的输入长度也会增加，因为在前一个时间戳生成的单词会被添加到当前输入句子中。
- en: When the decoder block completes the generation of the entire target sentence,
    <eos> or the ***end of the sentence*** token is generated.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 当解码器块完成整个目标句子的生成时，会生成 <eos> 或 ***句子结束*** 标记。
- en: You can think of it as a recursive process!
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以将其视为一个递归过程！
- en: '![](../Images/c4f83c9c11b7f68dfa0f7df38ef85dfc.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c4f83c9c11b7f68dfa0f7df38ef85dfc.png)'
- en: Fig. 2 Recursive generation of output tokens using Decoder (Image by Author)
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2 递归生成输出标记使用解码器（图片由作者提供）
- en: Now, this is what is supposed to happen when input is given to the transformer
    model and we are expecting an output. But at the time of training/finetuning the
    transformer model, we already have the target sentence in the training dataset.
    So how does it work?
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，这就是当输入给定到变换器模型时我们期望的输出情况。然而，在训练/微调变换器模型时，我们已经在训练数据集中拥有目标句子。那么它是如何工作的呢？
- en: 'It brings us to an extremely important concept of Decoders: ***Masked Multi-head
    Attention***. Sounds familiar? Of course, it does. In the previous part, we understood
    the concept of ***Multi-head attention*** which is used in the Encoder block.
    Let us now understand how these two are different.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 这使我们接触到解码器的一个极其重要的概念：***掩蔽多头注意力***。听起来很熟悉？当然了。在之前的部分，我们理解了在编码器块中使用的 ***多头注意力***
    的概念。现在让我们了解这两者的不同之处。
- en: Masked Multi-head Attention
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 掩蔽多头注意力
- en: The decoder block generates the target sentence word by word and hence, the
    model has to be trained similarly so that it can make accurate predictions even
    with a limited set of tokens.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 解码器块逐字生成目标句子，因此，模型必须以类似的方式进行训练，以便即使在有限的标记集下也能做出准确预测。
- en: Hence, as the name suggests, we mask all the tokens to the right of the sentence
    which have not been predicted yet before calculating the self-attention matrix.
    This will ensure that the self-attention mechanism only considers the tokens that
    will be available to the model at each recursive step of prediction.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，正如名字所示，我们在计算自注意力矩阵之前会掩蔽句子右侧尚未预测的所有标记。这将确保自注意力机制仅考虑模型在每次递归预测步骤中将可用的标记。
- en: 'Let us take a simple example to understand it:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们举一个简单的例子来理解它：
- en: '![](../Images/f70b358e28c8796b99a4b8f0b931c107.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f70b358e28c8796b99a4b8f0b931c107.png)'
- en: Fig 3\. Masked Multi-head attention matrix representation (Image by Author)
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3\. 掩蔽多头注意力矩阵表示（图像来源：作者）
- en: The steps and formula to calculate the self-attention matrix will be the same
    as we do in the Encoder block. We will cover the steps on a high level in this
    article. For a deeper understanding, please feel free to head to the previous
    part of this article series.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 计算自注意力矩阵的步骤和公式与我们在编码器块中做的相同。我们将在本文中高层次地覆盖这些步骤。欲了解更深入的内容，请随时前往本文系列的上一部分。
- en: '*Generate embeddings for the target sentence and obtain* ***target matrix Y***'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*为目标句子生成嵌入并获得* ***目标矩阵 Y***'
- en: '*Transform the target sentence into* ***Q, K & V*** *by multiplying random
    weight matrices* ***Wq, Wk & Wv with target matrix Y***'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*通过将随机权重矩阵* ***Wq、Wk 和 Wv*** *与目标矩阵 Y 相乘，将目标句子转换为* ***Q、K 和 V***'
- en: '*Calculate the dot product of* ***Q and K-transpose***'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*计算* ***Q 和 K 的转置*** *的点积*'
- en: '*Scale the dot product by dividing it by the* ***square root*** *of the embedding
    dimension (****dk****)*'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*通过将点积除以* ***平方根*** *的嵌入维度 (****dk****) 来缩放*'
- en: '*Apply masking on the scaled matrix by replacing all the cells with* ***<mask>
    with — inf***'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*通过将所有* ***<mask>*** *单元替换为* ***— inf*** *来对缩放矩阵应用掩蔽*'
- en: '*Now* ***apply the softmax function*** *on the matrix and multiply it with
    the* ***Vi matrix*** *to generate the attention matrix Zi*'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*现在* ***对矩阵应用 softmax 函数***，*并将其与* ***Vi 矩阵*** *相乘以生成注意力矩阵 Zi*'
- en: '*Concatenate multiple attention matrices Zi into a single* ***attention matrix
    M***'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*将多个注意力矩阵 Zi 连接成一个单一的* ***注意力矩阵 M***'
- en: This attention matrix will be fed to the next component of the Decoder block
    along with the input sentence representation generated by the Encoder block. Let
    us now understand how both of these matrices are consumed by the Decoder block.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 这个注意力矩阵将与编码器块生成的输入句子表示一起传递到解码器块的下一个组件。现在让我们理解这两个矩阵是如何被解码器块使用的。
- en: Multi-head Attention
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 多头注意力
- en: This sublayer of the Decoder block is also known as the ***“Encoder-Decoder
    Attention Layer”*** as it accepts both **masked attention matrix (M)** and **sentence
    representation by Encoder (R)**.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 解码器块的这个子层也被称为***“编码器-解码器注意力层”***，因为它接收**掩蔽注意力矩阵 (M)**和**由编码器生成的句子表示 (R)**。
- en: '![](../Images/42c44b268b7f30a8506a35f82570c2be.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/42c44b268b7f30a8506a35f82570c2be.png)'
- en: Fig 4\. Multihead Attention Mechanism (Image by Author)
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4\. 多头注意力机制（图像来源：作者）
- en: 'The calculation of the self-attention matrix is very similar to how it is done
    in the previous step with a small twist. Since we have two input matrices for
    this layer, they are transformed into Q, K & V as follows:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 自注意力矩阵的计算与前一步骤中的计算非常相似，只是有一点小变化。由于这个层有两个输入矩阵，它们被转化为 Q、K 和 V，如下所示：
- en: Q is generated using Wq and M
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Q 是使用 Wq 和 M 生成的
- en: K & V matrices are generated using Wk & Wv with R
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: K 和 V 矩阵是使用 Wk 和 Wv 与 R 生成的
- en: By now you must’ve understood that every step and calculation that goes behind
    the Transformer model has a very specific reason. Similarly, there is also a reason
    why each of these matrices is generated using a different input matrix. Can you
    guess?
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 到现在为止，你一定已经明白了变换器模型中的每一步和计算都有非常具体的理由。类似地，每个矩阵使用不同的输入矩阵生成也是有原因的。你能猜到吗？
- en: '***Quick Hint: The answer lies in how the self-attention matrix is calculated...***'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '***快速提示：答案在于自注意力矩阵是如何计算的…***'
- en: Yes, you got it right!
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 是的，你答对了！
- en: If you recall, when we understood the concept of self-attention using an input
    sentence, we talked about how it calculates attention scores while mapping the
    source sentence to itself. Every word in the source sentence is compared against
    every other word in the same sentence to quantify the relationships and understand
    the context.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你还记得，当我们通过一个输入句子理解自注意力概念时，我们谈到了它如何计算注意力分数，同时将源句子映射到自身。源句子中的每个词都与同一句子中的每个其他词进行比较，以量化关系并理解上下文。
- en: Here also we are doing the same thing, the only difference being, we are comparing
    each word of the input sentence (K-transpose) to the target sentence words (Q).
    It will help us quantify how similar both of these sentences are to each other
    and understand the relationships between the words.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们也做着相同的事情，唯一的区别是，我们将输入句子（K-转置）的每个词与目标句子词（Q）进行比较。这将帮助我们量化这两个句子彼此的相似程度，并理解单词之间的关系。
- en: '![](../Images/a2c20a503086986041db5f326a124c28.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a2c20a503086986041db5f326a124c28.png)'
- en: Fig 5\. Attention Matrix Representation with input sentence & target sentence
    (Image by Author)
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5\. 输入句子和目标句子的注意力矩阵表示（作者提供的图像）
- en: In the end, the attention matrix Zi generated will be of dimension N X 1 where
    N = word count of the target sentence.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，生成的注意力矩阵 Zi 的维度将是 N X 1，其中 N = 目标句子的词数。
- en: Since this is also a multi-head attention layer, to generate the final attention
    matrix, multiple attention matrices are concatenated.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这也是一个多头注意力层，为了生成最终的注意力矩阵，多个注意力矩阵被拼接在一起。
- en: 'With this, we have covered all the unique components of the Decoder block.
    However, some other components function the same as in Encoder Block. Let us also
    look at them briefly:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这些，我们已经涵盖了 Decoder 块的所有独特组件。然而，某些其他组件的功能与 Encoder 块中的相同。我们也简要看看它们：
- en: '***Positional Encoding*** — Just like the encoder block, to preserve the word
    order of the target sentence, we add positional encoding to the target embedding
    before feeding it to the Masked Multi-attention layer.'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***位置编码*** — 就像编码器块一样，为了保留目标句子的词序，我们在将其输入到 Masked Multi-attention 层之前，将位置编码添加到目标嵌入中。'
- en: '***Feedforward Network*** — This sublayer in the decoder block is the classic
    neural network with two dense layers and ReLU activations. It accepts input from
    the multi-head attention layer, performs some non-linear transformations on the
    same and finally generates contextualised vectors.'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***前馈网络*** — 这个解码器块中的子层是经典的神经网络，具有两个全连接层和 ReLU 激活。它接受来自多头注意力层的输入，对其进行一些非线性变换，最后生成上下文化的向量。'
- en: '***Add & Norm Component*** — This is a *residual layer* followed by *layer
    normalisation.* It helps faster model training while ensuring no information from
    sub-layers is lost.'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***Add & Norm 组件*** — 这是一个*残差层*，后面跟着*层归一化*。它有助于加快模型训练，同时确保不会丢失来自子层的信息。'
- en: We have covered these concepts in detail in [Part 1](/transformer-models-101-getting-started-part-1-b3a77ccfa14d).
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在[第 1 部分](/transformer-models-101-getting-started-part-1-b3a77ccfa14d)中详细介绍了这些概念。
- en: With this, we have wrapped up the internal working of the Decoder block as well.
    As you might have guessed, both Encoder & Decoder blocks are used to process and
    generate contextualized vectors for the input sentence. So who does the actual
    next-word prediction task? Let’s find out.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这些，我们也总结了 Decoder 块的内部工作原理。正如你可能猜到的，编码器和解码器块用于处理和生成输入句子的上下文化向量。那么实际的下一个词预测任务由谁来完成呢？让我们找出答案。
- en: Linear & Softmax Layer
  id: totrans-68
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 线性与 Softmax 层
- en: Sitting on top of the Decoder network, it accepts the output matrix generated
    by the last decoder in the stack as input. This output matrix is transformed into
    a logit vector of the same size as the vocabulary size. We then apply the softmax
    function on this logit vector to generate probabilities corresponding to each
    word. The word with the highest probability is predicted as the next word. The
    model is optimized for cross-entropy loss using **Adam Optimizer**.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 它位于 Decoder 网络的顶部，接受由堆栈中最后一个解码器生成的输出矩阵作为输入。这个输出矩阵被转换为与词汇表大小相同的 logit 向量。然后我们对这个
    logit 向量应用 softmax 函数，以生成与每个词对应的概率。具有最高概率的词被预测为下一个词。该模型使用**Adam 优化器**进行交叉熵损失优化。
- en: To avoid overfitting, **dropout layers** are added after every sub-layer of
    the encoder/decoder network.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 为了避免过拟合，**dropout 层**在每个编码器/解码器网络的子层后面添加。
- en: That’s all about the entire Transformer Model. With this, we have completed
    the in-depth walk-through of Transformer Model Architecture in the simplest language
    possible.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是整个 Transformer 模型的全部内容。通过这个，我们已经用尽可能简单的语言完成了对 Transformer 模型架构的深入讲解。
- en: Conclusion
  id: totrans-72
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结论
- en: Now that you know all about Transformer models, it shouldn’t be difficult for
    you to build your knowledge on top of this and delve into more complex LLM model
    architectures such as BERT, GPT, etc.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经了解了所有关于 Transformer 模型的内容，基于此构建你的知识并深入研究更复杂的 LLM 模型架构，如 BERT、GPT 等，不应该会很困难。
- en: 'You may refer to the below resources for the same:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以参考以下资源：
- en: '[https://huggingface.co/blog/bert-101](https://huggingface.co/blog/bert-101)'
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[https://huggingface.co/blog/bert-101](https://huggingface.co/blog/bert-101)'
- en: '[https://jalammar.github.io/illustrated-gpt2/](https://jalammar.github.io/illustrated-gpt2/)'
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[https://jalammar.github.io/illustrated-gpt2/](https://jalammar.github.io/illustrated-gpt2/)'
- en: I hope this 2-part article would make the Transformer Models a little less intimidating
    to understand. If you found it useful, please spread the good word.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 希望这篇两部分的文章能让 Transformer 模型的理解变得稍微不那么令人生畏。如果你觉得这篇文章有用，请传播这个好消息。
- en: Until next time!
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 下次见！
