- en: 'The Secret to Improved NLP: An In-Depth Look at the nn.Embedding Layer in PyTorch'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 提升 NLP 性能的秘诀：深入了解 PyTorch 中的 nn.Embedding 层
- en: 原文：[https://towardsdatascience.com/the-secret-to-improved-nlp-an-in-depth-look-at-the-nn-embedding-layer-in-pytorch-6e901e193e16](https://towardsdatascience.com/the-secret-to-improved-nlp-an-in-depth-look-at-the-nn-embedding-layer-in-pytorch-6e901e193e16)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/the-secret-to-improved-nlp-an-in-depth-look-at-the-nn-embedding-layer-in-pytorch-6e901e193e16](https://towardsdatascience.com/the-secret-to-improved-nlp-an-in-depth-look-at-the-nn-embedding-layer-in-pytorch-6e901e193e16)
- en: Dissecting the `nn.Embedding` layer in PyTorch and a complete guide on how it
    works
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解剖 PyTorch 中的 `nn.Embedding` 层及其工作原理的完整指南
- en: '[](https://medium.com/@will.badr?source=post_page-----6e901e193e16--------------------------------)[![Will
    Badr](../Images/46a4737eaedc1cb30f4f11ceb8373528.png)](https://medium.com/@will.badr?source=post_page-----6e901e193e16--------------------------------)[](https://towardsdatascience.com/?source=post_page-----6e901e193e16--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----6e901e193e16--------------------------------)
    [Will Badr](https://medium.com/@will.badr?source=post_page-----6e901e193e16--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@will.badr?source=post_page-----6e901e193e16--------------------------------)[![Will
    Badr](../Images/46a4737eaedc1cb30f4f11ceb8373528.png)](https://medium.com/@will.badr?source=post_page-----6e901e193e16--------------------------------)[](https://towardsdatascience.com/?source=post_page-----6e901e193e16--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----6e901e193e16--------------------------------)
    [Will Badr](https://medium.com/@will.badr?source=post_page-----6e901e193e16--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----6e901e193e16--------------------------------)
    ·8 min read·Jan 24, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----6e901e193e16--------------------------------)
    ·阅读时间 8 分钟·2023 年 1 月 24 日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/78883177665ed8578c3715ef39211129.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/78883177665ed8578c3715ef39211129.png)'
- en: OpenAI DALL-E Generated Image
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI DALL-E 生成的图像
- en: You might have seen the famous PyTorch *nn.Embedding()* layer in multiple neural
    network architectures that involves natural language processing (NLP). This is
    one of the simplest and most important layers when it comes to designing advanced
    NLP architectures. Let me explain what it is, in simple terms.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能在涉及自然语言处理（NLP）的多个神经网络架构中见过著名的 PyTorch *nn.Embedding()* 层。这是设计先进 NLP 架构时最简单且最重要的层之一。让我用简单的术语来解释它是什么。
- en: After spending some time looking into its C++ source code, here is what I found.
    The *nn.Embedding* layer is a simple lookup table that maps an index value to
    a weight matrix of a certain dimension. This simple operation is the foundation
    of many advanced NLP architectures, allowing for the processing of discrete input
    symbols in a continuous space. During the training the parameters of the *nn.Embedding*
    layer in a neural network are adjusted in order to optimize the performance of
    the model. Specifically, the embedding matrix is updated via backpropagation to
    minimize the loss function. This can be thought of as learning a mapping from
    discrete input tokens (such as words) to continuous embedding vectors in a high-dimensional
    space, where the vectors are optimised to represent the meaning or context of
    the input tokens in relation to the task the model is trained for (e.g. text generation,
    language translation).
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在花了一些时间研究其 C++ 源代码后，我发现了以下内容。*nn.Embedding* 层是一个简单的查找表，将索引值映射到具有特定维度的权重矩阵。这一简单操作是许多先进
    NLP 架构的基础，允许在连续空间中处理离散输入符号。在训练过程中，神经网络中的 *nn.Embedding* 层的参数会被调整，以优化模型的性能。具体来说，通过反向传播更新嵌入矩阵，以最小化损失函数。这可以被视为学习将离散输入标记（如单词）映射到高维空间中的连续嵌入向量，其中向量被优化以表示输入标记的意义或上下文，相关于模型训练任务（例如文本生成、语言翻译）。
- en: 'Now let’s look at some concrete examples with code:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们通过一些具体的代码示例来了解：
- en: The *nn.Embedding* layer takes in two arguments as a minimum. the vocabulary
    size and the size of the encoded representation for each word. For example, if
    you have a vocabulary of 10,000 words, then the value of the first argument would
    be 10,000\. Each word in the vocabulary will be represented by a vector of fixed
    size. The second argument is the size of the learned embedding for each word
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '*nn.Embedding* 层至少需要两个参数：词汇表大小和每个单词编码表示的大小。例如，如果你的词汇表有 10,000 个单词，那么第一个参数的值将是
    10,000。词汇表中的每个单词将由一个固定大小的向量表示。第二个参数是每个单词学习到的嵌入的大小。'
- en: '[PRE0]'
  id: totrans-12
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'What happened here is that PyTorch created a lookup table called `embedding`.
    This table has 10 rows and 50 columns. Each row represents a single word embedding
    that is initialized randomly drawn from a uniform distribution. They are initialized
    using the `nn.init.uniform_()` function from the `torch.nn.init` module and the
    weights are initialized with random values between -1 and 1\. To examine the embeddings
    for a given word (eg. first word in the table), you can run:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 这里发生的事情是 PyTorch 创建了一个名为 `embedding` 的查找表。这个表有 10 行和 50 列。每一行代表一个单词的嵌入，初始化时是从均匀分布中随机抽取的。它们是使用
    `torch.nn.init` 模块中的 `nn.init.uniform_()` 函数进行初始化的，权重被初始化为在 -1 到 1 之间的随机值。要检查给定单词的嵌入（例如，表中的第一个单词），你可以运行：
- en: '[PRE1]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The output is a vector of size 50:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 输出是一个大小为 50 的向量：
- en: '![](../Images/a202b2c7cd8622681f8fdafe67a55e6c.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a202b2c7cd8622681f8fdafe67a55e6c.png)'
- en: These are the numbers that gets tuned and optimised during the training process
    to convey the meaning of a certain word. The initialization method can have a
    significant impact on the performance of model. Different initialization methods
    can lead to different starting points for the optimization process, and this can
    affect how quickly or easily the network converges to a good solution. For example,
    if the weights are initialized to very small or very large values, the gradients
    during backpropagation will also be small or large, which can slow down or even
    prevent convergence. On the other hand, if the weights are initialized to values
    that are closer to zero, the gradients will be more reasonable, and the network
    is more likely to converge quickly.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 这些数字在训练过程中被调整和优化，以传达特定单词的含义。初始化方法对模型的性能有显著影响。不同的初始化方法可能导致优化过程的起始点不同，这会影响网络收敛到良好解的速度或难易程度。例如，如果权重初始化为非常小或非常大的值，反向传播过程中梯度也会很小或很大，这可能会减慢甚至阻止收敛。另一方面，如果权重初始化为接近零的值，梯度将更为合理，网络更可能快速收敛。
- en: 'Also, different initialization methods are designed to work well with different
    types of activation functions. For example, the Xavier initialization is designed
    to work well with sigmoid and tanh activation functions, whereas other methods
    may work better with ReLU and its variants. Now, let’s see how to initialize the
    *nn.Embedding* layer using different methods:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，不同的初始化方法被设计成与不同类型的激活函数配合良好。例如，Xavier 初始化设计用于与 sigmoid 和 tanh 激活函数配合使用，而其他方法可能与
    ReLU 及其变体效果更佳。现在，让我们看看如何使用不同的方法初始化 *nn.Embedding* 层：
- en: '`nn.init.normal_()`: which initializes the weights with random values drawn
    from a normal distribution with a mean of 0 and a standard deviation of 1\. It
    is also known as Gaussian initialization.'
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`nn.init.normal_()`：它用从均值为 0 和标准差为 1 的正态分布中抽取的随机值初始化权重。这也被称为高斯初始化。'
- en: '[PRE2]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '`nn.init.constant_()`: This function initializes the weights with a specific
    constant value. For example, you can use `nn.init.constant_(my_layer.weight, 0)`
    to initialize the weights of a layer to 0.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '`nn.init.constant_()`：这个函数用特定的常数值初始化权重。例如，你可以使用 `nn.init.constant_(my_layer.weight,
    0)` 将层的权重初始化为 0。'
- en: '`nn.init.xavier_uniform_()` and `nn.init.xavier_normal_()`: These functions
    are based on the work of [Xavier Glorot](https://scholar.google.com/citations?user=_WnkXlkAAAAJ&hl=en)
    and [Yoshua Bengio](https://yoshuabengio.org/), and they are designed to work
    well with sigmoid and tanh activation functions. They initialize the weights to
    values that are close to zero, but not too small.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '`nn.init.xavier_uniform_()` 和 `nn.init.xavier_normal_()`：这些函数基于 [Xavier Glorot](https://scholar.google.com/citations?user=_WnkXlkAAAAJ&hl=en)
    和 [Yoshua Bengio](https://yoshuabengio.org/) 的研究，设计用于与 sigmoid 和 tanh 激活函数配合使用。它们将权重初始化为接近零但不太小的值。'
- en: '[PRE3]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '`nn.init.kaiming_uniform_()` and `nn.init.kaiming_normal_()`: These functions
    are based on the work of He et al., and they are designed to work well with ReLU
    and its variants (LeakyReLU, PReLU, RReLU, etc.). They also initialize the weights
    to values that are close to zero, but not too small.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '`nn.init.kaiming_uniform_()` 和 `nn.init.kaiming_normal_()`：这些函数基于 He 等人的研究，设计用于与
    ReLU 及其变体（LeakyReLU、PReLU、RReLU 等）配合使用。它们还将权重初始化为接近零但不太小的值。'
- en: '[PRE4]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'These weights can also be initialized using pre-trained word vectors such as
    GloVe or word2vec, which have been trained on large corpora and have been shown
    to be useful for many natural language processing tasks. The process of using
    a pre-trained word vectors is called — Fine-tuning. Using pre-trained word embeddings
    with the `nn.Embedding` layer can be very useful for a variety of natural language
    processing (NLP) tasks. There are a few reasons why this is the case:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 这些权重还可以使用预训练的词向量，如GloVe或word2vec进行初始化，这些向量在大型语料库上进行过训练，并且已被证明对许多自然语言处理任务有用。使用预训练词向量的过程称为——微调。将预训练的词嵌入与`nn.Embedding`层结合使用，对各种自然语言处理（NLP）任务非常有用。这是因为：
- en: '**Improves model performance:** Pre-trained word embeddings have been trained
    on massive amounts of text data and have been shown to be useful for a variety
    of NLP tasks. When used as input to a neural network, they can help to improve
    the performance of the model by providing it with a good set of initial weights
    that capture the meaning of words.'
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**提高模型性能：** 预训练的词嵌入已经在大量文本数据上进行训练，并且已被证明对各种NLP任务有用。当作为神经网络的输入使用时，它们可以通过提供一组好的初始权重来提高模型的性能，这些权重捕捉了词语的意义。'
- en: '**Saves computation time and resources:** Training a neural network to learn
    word embeddings from scratch can be a time-consuming and computationally expensive
    task, especially if you are working with large corpora. By using pre-trained word
    embeddings, you can save a significant amount of computation time and resources,
    as the embeddings have already been learned on a large corpus.'
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**节省计算时间和资源：** 从头开始训练神经网络以学习词嵌入可能是一个耗时且计算开销大的任务，尤其是当你处理大语料库时。通过使用预训练的词嵌入，你可以节省大量计算时间和资源，因为这些嵌入已经在大语料库上学习过。'
- en: '**Allows transfer learning:** Pre-trained word embeddings can be used for transfer
    learning, which means that you can use the embeddings learned on one task as a
    starting point for a different but related task. This can be particularly useful
    for NLP tasks where labeled data is scarce or expensive to obtain.'
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**允许迁移学习：** 预训练的词嵌入可以用于迁移学习，这意味着你可以将学到的嵌入作为起点，用于不同但相关的任务。这在标签数据稀缺或获取成本高的NLP任务中特别有用。'
- en: 'Let’s see how it can be implemented:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何实现它：
- en: '[PRE5]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'You can also use the `from_pretrained()` method to load the pre-trained embeddings
    directly:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 你还可以使用`from_pretrained()`方法直接加载预训练的嵌入：
- en: '[PRE6]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'You can also use pre-trained embeddings from popular libraries like GloVe or
    fastText, for example:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 你还可以使用来自流行库如GloVe或fastText的预训练嵌入，例如：
- en: '[PRE7]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: In some cases when performing transfer learning, you may need to freeze the
    pre-trained embeddings during training process, so that they are not updated during
    the backpropagation step and only the last dense layer is updated. To do this,
    set `embedding_layer.weight.requiresGrad = False` to prevent this layer from being
    updated.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在进行迁移学习时，你可能需要在训练过程中冻结预训练的嵌入，以防在反向传播步骤中更新它们，并且只有最后一个全连接层被更新。要做到这一点，设置`embedding_layer.weight.requiresGrad
    = False`以防止此层被更新。
- en: The rise of Transformers
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 变换器的兴起
- en: One other common place where you will find this layer is in the transformer
    architecture. The `nn.Embedding` layer is a key component of the transformer architecture,
    which is a type of neural network architecture that has been widely used for natural
    language processing tasks such as language translation, text summarization, question
    answering and creating large language models like GPT3
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个常见的地方是变换器架构中。`nn.Embedding`层是变换器架构的关键组件，它是一种广泛用于自然语言处理任务的神经网络架构，如语言翻译、文本摘要、问答以及创建大型语言模型如GPT3。
- en: In a transformer architecture, the `nn.Embedding` layer is used to convert the
    input sequence of tokens (such as words or subwords) into a continuous representation.
    This is done by looking up the embedding vector for each token in the input sequence
    in a learned embedding matrix.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在变换器架构中，`nn.Embedding`层用于将输入的令牌序列（例如单词或子词）转换为连续表示。这是通过在学习的嵌入矩阵中查找每个令牌的嵌入向量来完成的。
- en: The output of the embedding layer is then passed through several layers of multi-head
    self-attention and feed-forward neural networks, which are used to process and
    understand the input sequence in a contextually-aware manner. The self-attention
    mechanism is the key component of transformers, which allows the model to weigh
    the importance of each token in the input sequence when making predictions. This
    is all built in `nn.Transformer` layer in PyTorch.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 嵌入层的输出随后通过几个多头自注意力和前馈神经网络层，这些层用于在上下文感知的方式中处理和理解输入序列。自注意力机制是变换器的关键组件，它允许模型在进行预测时衡量输入序列中每个标记的重要性。这些都在PyTorch中的`nn.Transformer`层内实现。
- en: 'After passing through the transformer layers, the output of the model is typically
    passed through a final linear layer, which is used to make predictions for the
    task at hand. For example, in a language translation model, the final linear layer
    would be used to predict the probability of each word in the target language given
    the input sequence in the source language. Let’s look at what the `Transformer`class
    looks like in Python:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 经过变换器层处理后，模型的输出通常会通过一个最终的线性层，用于进行任务预测。例如，在语言翻译模型中，最终的线性层会用于预测目标语言中每个单词的概率，给定源语言中的输入序列。让我们看看`Transformer`类在Python中的样子：
- en: '[PRE8]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: It is important to note that this is a simple example for demonstration purposes
    and to show how embedding layer is being used in `Transformers`, and a real transformer
    model would typically have additional components such as positional encoding which
    is a technique that provides the model with information about the relative position
    of each token in the input sequence. There is also layer normalization to normalize
    the activations of a layer, in order to improve the stability and performance
    of the model. Also, it is common practice to use pre-trained embeddings to initialize
    the embedding layer, in order to leverage the knowledge learned from large corpora
    of text data.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的是，这只是一个用于演示目的的简单示例，展示了嵌入层在`Transformers`中的使用，而真实的变换器模型通常会有额外的组件，如位置编码，这是一种提供每个标记相对位置的信息的技术。还有层归一化，用于规范化层的激活，以提高模型的稳定性和性能。此外，常见做法是使用预训练的嵌入来初始化嵌入层，以利用从大量文本数据中学到的知识。
- en: 'Interesting Facts:'
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 有趣的事实：
- en: 'The transformer layer above with only 10 vocabularies, 50 dimensional vector
    embedding, 2 multi-head attention and 2 layers in the encoder and decoder has
    2,018,692 trainable parameters. These are the number of parameters we optimize
    during the training process. To get this number, I run the code below:'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 上述具有仅10个词汇、50维向量嵌入、2个多头注意力机制以及编码器和解码器中的2层的变换器层共有2,018,692个可训练参数。这些是我们在训练过程中优化的参数数量。要得到这个数字，我运行了下面的代码：
- en: '[PRE9]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: The model dimension or d_model, must be divisible by the number of heads in
    the multi-head self-attention mechanism, because the multi-head attention mechanism
    divides the model dimension into several smaller subspaces. Each subspace is then
    used to calculate attention weights for a different set of tokens.
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型维度或d_model必须能够被多头自注意力机制中的头数整除，因为多头注意力机制将模型维度分割成几个较小的子空间。每个子空间用于计算不同标记集的注意力权重。
- en: Transformer models have also been used in computer vision tasks such as object
    detection and image segmentation by using self-attention mechanism to process
    image pixels as tokens.
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 变换器模型也被用于计算机视觉任务，如物体检测和图像分割，通过使用自注意力机制将图像像素处理为标记。
- en: In conclusion, the `nn.Embedding` layer is a fundamental asset in many NLP models,
    and it plays a critical role in the transformer architecture. The `nn.Embedding`
    layer is used to convert the input sequence of tokens into a continuous representation
    that can be effectively processed by the model. The use of pre-trained embeddings
    allows transformer models to leverage the knowledge learned from large corpora
    of text data, which can improve their performance on a wide range of natural language
    processing tasks. The `nn.Embedding` layer also has several parameters that we
    did not cover in this post, such as `sparse` option, `padding_idx`, `max_norm`
    and `norm_type` that can be used to customize the embedding layer to the specific
    requirements of the task at hand. Understanding the `nn.Embedding` layer and how
    it works is an important step in building effective natural language processing
    models with PyTorch.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，`nn.Embedding` 层是许多 NLP 模型中的基本资产，并且在 transformer 架构中扮演着关键角色。`nn.Embedding`
    层用于将输入的标记序列转换为模型可以有效处理的连续表示。使用预训练的嵌入允许 transformer 模型利用从大量文本数据中学到的知识，这可以提高它们在各种自然语言处理任务上的表现。`nn.Embedding`
    层还具有一些我们在这篇文章中没有涉及的参数，如 `sparse` 选项、`padding_idx`、`max_norm` 和 `norm_type`，这些参数可以用来根据具体任务的需求自定义嵌入层。理解
    `nn.Embedding` 层及其工作原理是使用 PyTorch 构建有效自然语言处理模型的重要步骤。
