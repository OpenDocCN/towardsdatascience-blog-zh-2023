- en: Using SHAP to Debug a PyTorch Image Regression Model
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用SHAP调试PyTorch图像回归模型
- en: 原文：[https://towardsdatascience.com/using-shap-to-debug-a-pytorch-image-regression-model-4b562ddef30d](https://towardsdatascience.com/using-shap-to-debug-a-pytorch-image-regression-model-4b562ddef30d)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/using-shap-to-debug-a-pytorch-image-regression-model-4b562ddef30d](https://towardsdatascience.com/using-shap-to-debug-a-pytorch-image-regression-model-4b562ddef30d)
- en: Using DeepShap to understand and improve the model powering an autonomous car
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用DeepShap来理解和改进驱动自动驾驶汽车的模型
- en: '[](https://conorosullyds.medium.com/?source=post_page-----4b562ddef30d--------------------------------)[![Conor
    O''Sullivan](../Images/2dc50a24edb12e843651d01ed48a3c3f.png)](https://conorosullyds.medium.com/?source=post_page-----4b562ddef30d--------------------------------)[](https://towardsdatascience.com/?source=post_page-----4b562ddef30d--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----4b562ddef30d--------------------------------)
    [Conor O''Sullivan](https://conorosullyds.medium.com/?source=post_page-----4b562ddef30d--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://conorosullyds.medium.com/?source=post_page-----4b562ddef30d--------------------------------)[![Conor
    O''Sullivan](../Images/2dc50a24edb12e843651d01ed48a3c3f.png)](https://conorosullyds.medium.com/?source=post_page-----4b562ddef30d--------------------------------)[](https://towardsdatascience.com/?source=post_page-----4b562ddef30d--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----4b562ddef30d--------------------------------)
    [Conor O''Sullivan](https://conorosullyds.medium.com/?source=post_page-----4b562ddef30d--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----4b562ddef30d--------------------------------)
    ·11 min read·Jan 10, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----4b562ddef30d--------------------------------)
    ·阅读时间11分钟·2023年1月10日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/965f5964b6c98ef14db3694ad7588c2f.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/965f5964b6c98ef14db3694ad7588c2f.png)'
- en: '(source: author)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: （来源：作者）
- en: Autonomous cars terrify me. Big hunks of metal flying around with no humans
    to stop them if something goes wrong. To reduce this risk it is not enough to
    evaluate the models powering these beasts. We also need to understand how they
    are making predictions. This is to avoid any edge cases that would cause unforeseen
    accidents.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 自动驾驶汽车让我感到恐惧。巨大的金属块在空中飞驰，如果出现问题没有人可以阻止它们。为了降低风险，仅仅评估这些“怪物”背后的模型是不够的。我们还需要了解它们是如何进行预测的。这是为了避免任何可能导致意外的边缘情况。
- en: Okay, so our application is not so consequential. We will be debugging the model
    used to power a mini-automated car (the worst you could expect is a bruised ankle).
    Still, IML methods can be useful. We will see how they can even improve the performance
    of the model.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 好吧，我们的应用程序并没有那么重要。我们将调试用于驱动迷你自动驾驶汽车的模型（最坏的情况可能是一个瘀伤的脚踝）。尽管如此，IML方法仍然有用。我们将看看它们如何甚至能提高模型的性能。
- en: 'Specifically, we will:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，我们将：
- en: Fine-tune ResNet-18 using PyTorch with image data and a continuous target variable
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用PyTorch和图像数据以及连续目标变量来微调ResNet-18
- en: Evaluate the model using MSE and scatter plots
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用MSE和散点图来评估模型
- en: Interpret the model using DeepSHAP
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用DeepSHAP解释模型
- en: Correct the model through better data collection
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过更好的数据收集来纠正模型
- en: Discuss how image augmentation could further improve the model
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 讨论图像增强如何进一步改善模型
- en: Along the way, we will discuss some key pieces of Python code. You can also
    find the full project on [GitHub](https://github.com/conorosully/SHAP-tutorial/blob/main/src/image_data.ipynb).
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个过程中，我们将讨论一些关键的Python代码。你还可以在[GitHub](https://github.com/conorosully/SHAP-tutorial/blob/main/src/image_data.ipynb)上找到完整的项目。
- en: If you are new to SHAP, then check out the **video** below**.** If you want
    more, then check out my [**SHAP course**](https://adataodyssey.com/courses/shap-with-python/)**.**
    You can get free access if you sign up for my [**Newsletter**](https://mailchi.mp/aa82a5ce1dc0/signup)
    :)
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你是SHAP的新手，请查看下面的**视频**。如果你想要更多内容，可以查看我的[**SHAP课程**](https://adataodyssey.com/courses/shap-with-python/)**。**
    如果你注册我的[**新闻通讯**](https://mailchi.mp/aa82a5ce1dc0/signup)，你可以免费访问 :)
- en: Python packages
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Python包
- en: '[PRE0]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Dataset
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据集
- en: We start the project by collecting data in one room only (this will come back
    to haunt us). As mentioned, we use images to power an automated car. You can find
    examples of these on [Kaggle](https://www.kaggle.com/datasets/conorsully1/jatracer-images?select=object_detection).
    These images are all 224 x 224 pixels.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从在一个房间里收集数据开始这个项目（这将对我们造成困扰）。如前所述，我们使用图像来驱动自动化汽车。你可以在[Kaggle](https://www.kaggle.com/datasets/conorsully1/jatracer-images?select=object_detection)上找到这些图像的例子。这些图像的尺寸均为224
    x 224像素。
- en: We display one of them with the code below. Take note of the image name (line
    2). The first two numbers are x and y coordinates within the 224 x 224 frame.
    In **Figure 1**, you can see we have displayed these coordinates using a green
    circle (line 8).
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 我们用下面的代码显示了其中一个。注意图像名称（第2行）。前两个数字是224 x 224帧中的x和y坐标。在**图1**中，你可以看到我们使用绿色圆圈显示了这些坐标（第8行）。
- en: '[PRE1]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '![](../Images/480d8a4bf80557fca51cf4ce598573c9.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/480d8a4bf80557fca51cf4ce598573c9.png)'
- en: 'Figure 1: example of input image of track (source: author)'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：赛道输入图像示例（来源：作者）
- en: These coordinates are the target variable. The model predicts them using the
    image as input. This prediction is then used to direct the car. In this case,
    you can see the car is coming up to a left turn. The ideal direction is to go
    towards the coordinates given by the green circle.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 这些坐标是目标变量。模型使用图像作为输入来预测它们。然后，这一预测用于引导汽车。在这种情况下，你可以看到汽车正在接近左转。理想的方向是朝向绿色圆圈所给的坐标。
- en: Training the PyTorch model
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练PyTorch模型
- en: I want to focus on SHAP so we won’t go into too much depth on the modelling
    code. If you have any questions, feel free to ask them in the comments.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 我想专注于SHAP，因此我们不会深入讨论建模代码。如果你有任何问题，随时在评论中提问。
- en: We start by creating the **ImageDataset** class. This is used to load our image
    data and target variables. It does this using the **paths** to our images. One
    thing to point out is how the target variables are scaled — both **x** and **y**
    will be between **-1** and **1**.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先创建**ImageDataset**类。这个类用于加载我们的图像数据和目标变量。它通过**paths**来完成这一任务。值得指出的是目标变量的缩放方式——**x**和**y**的范围都在**-1**和**1**之间。
- en: '[PRE2]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: In fact, when the model is deployed only the x predictions are used to direct
    the car. Because of scaling, the sign of the x prediction will determine the car’s
    direction. When **x < 0**, the car should turn left. Similarly, when **x > 0**
    the car should turn right. The larger the x value the sharper the turn.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，当模型部署时，仅使用x预测来引导汽车。由于缩放，x预测的符号将决定汽车的方向。当**x < 0**时，汽车应该左转。同样，当**x > 0**时，汽车应该右转。x值越大，转弯越急。
- en: We use the ImageDataset class to create training and validation data loaders.
    This is done by doing a random **80/20** split of all the image paths from room
    1\. In the end, we have **1,217** and **305** images in the training and validation
    set respectively.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用ImageDataset类创建训练和验证数据加载器。这是通过对所有来自房间1的图像路径进行随机**80/20**拆分来完成的。最终，我们在训练集和验证集中分别有**1,217**和**305**张图像。
- en: '[PRE3]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Notice the **batch_size** of the **valid_loader**. We are using the length of
    the validation dataset (i.e. 305). This allows us to load all validation data
    in one iteration. If you are working with larger datasets you may need to use
    a smaller batch size.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 注意**valid_loader**的**batch_size**。我们使用验证数据集的长度（即305）。这使我们可以在一次迭代中加载所有验证数据。如果你处理的是更大的数据集，可能需要使用较小的批量大小。
- en: We load a pretrained ResNet18 model (line 5). By setting **model.fc**, we update
    the final layer (line 6). It is a fully connected layer from 512 nodes to our
    2 target variable nodes. We will be using the Adam optimizer to fine-tune this
    model (line 9).
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 我们加载一个预训练的ResNet18模型（第5行）。通过设置**model.fc**，我们更新了最后一层（第6行）。这是一个从512个节点到我们2个目标变量节点的全连接层。我们将使用Adam优化器来微调这个模型（第9行）。
- en: '[PRE4]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: I’ve trained the model using a GPU (line 2). You will still be able to run the
    code on a CPU. Fine-tuning is not as computationally expensive as training from
    scratch!
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 我已经使用GPU训练了模型（第2行）。你仍然可以在CPU上运行代码。微调不像从头开始训练那样计算密集！
- en: Finally, we have our model training code. We train for 10 epochs using MSE as
    our loss function. Our final model is the one that has the lowest MSE on the validation
    set.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们有了模型训练代码。我们使用MSE作为损失函数训练10个周期。我们的最终模型是验证集上MSE最低的模型。
- en: '[PRE5]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Evaluation metrics
  id: totrans-40
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 评估指标
- en: At this point, we want to understand how our model is doing. We look at MSE
    and scatter plots of actual vs predicted x values. We ignore y for now as it does
    not impact the direction of the car.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 此时，我们想了解我们的模型表现如何。我们查看MSE和实际与预测x值的散点图。暂时忽略y，因为它不会影响汽车的方向。
- en: Training and validation set
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练和验证集
- en: '**Figure 2** gives these metrics on the training and validation set. The diagonal
    red line gives perfect predictions. There is a similar variation around this line
    for **x < 0** and **x > 0**. In other words, the model is able to predict left
    and right turns with similar accuracy. Similar performance on the training and
    validation set also indicates that the model is not overfitted.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '**图2**提供了训练和验证集的这些指标。对角线红线表示完美预测。对于**x < 0**和**x > 0**，此线周围有类似的变异。换句话说，模型能够以类似的准确性预测左转和右转。在训练和验证集上的类似表现也表明模型没有过拟合。'
- en: '![](../Images/d1244d02c4608d8292bb5c47f8904a7d.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d1244d02c4608d8292bb5c47f8904a7d.png)'
- en: 'Figure 2: model evaluation on training and validation set (source: author)'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：模型在训练和验证集上的评估（来源：作者）
- en: To create the above plot, we use the **model_evaluation** function. Note, the
    data loaders should be created so that they will load all data in the first iteration.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 要创建上述图，我们使用**model_evaluation**函数。注意，数据加载器应创建为在第一次迭代中加载所有数据。
- en: '[PRE6]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: You can see what we mean when we use the function below. We have created a new
    **train_loader** setting the batch size to the length of the training dataset.
    It is also important to load the saved model (line 2). Otherwise, you will end
    up using the model trained during the last epoch.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 使用下面的函数可以看出我们的意思。我们创建了一个新的**train_loader**，将批量大小设置为训练数据集的长度。加载保存的模型也很重要（第2行）。否则，你将使用上一个纪元训练的模型。
- en: '[PRE7]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Moving to a new locations
  id: totrans-50
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 移动到新位置
- en: 'The results look good! We would expect the car to perform well and it did.
    That is until we moved it to a new location:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 结果看起来不错！我们预计汽车会表现良好，它确实如此。直到我们将它移动到新位置：
- en: '![](../Images/ac24f769db9587ade7b762bf46f95e4f.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ac24f769db9587ade7b762bf46f95e4f.png)'
- en: 'Figure 3: model going wrong in a new location (source: author)'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：模型在新位置表现不佳（来源：作者）
- en: We collect some data from new locations (room 2 and room 3). Running the evaluation
    on these images, you can see that our model does not perform as well. This is
    strange! The car is on the exact same track so why does the room matter?
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从新位置（房间2和房间3）收集了一些数据。对这些图像进行评估时，你会发现我们的模型表现得不如预期。这很奇怪！汽车在完全相同的轨道上，为什么房间会有影响呢？
- en: '![](../Images/5fd30bf321e46ac5eb3649df59d68097.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5fd30bf321e46ac5eb3649df59d68097.png)'
- en: 'Figure 3: model evaluation on room 2 and room 3 (source: author)'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：模型在房间2和房间3上的评估（来源：作者）
- en: Debugging the model using SHAP
  id: totrans-57
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用SHAP调试模型
- en: We look to SHAP for the answer. It can be used to understand which pixels are
    important for a given prediction. We start by loading our saved model (line 2).
    SHAP has not been implemented for GPU so we set the device to CPU (lines 5–6).
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 我们寻求SHAP的答案。它可以用来理解哪些像素对给定的预测重要。我们首先加载保存的模型（第2行）。SHAP尚未实现GPU支持，所以我们将设备设置为CPU（第5-6行）。
- en: '[PRE8]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: To calculate SHAP values, we need to get some background images. SHAP will integrate
    over these images when calculating values. We are using a **batch_size** of 100
    images. This should give us reasonable approximations. Increasing the number of
    images will improve the approximation but it will also increase the computation
    time.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 计算SHAP值时，我们需要获取一些背景图像。SHAP将在计算值时对这些图像进行集成。我们使用**batch_size**为100的图像。这应该能给我们合理的近似值。增加图像数量将提高近似精度，但也会增加计算时间。
- en: '[PRE9]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: We create an explainer object by passing our model and background images into
    the **DeepExplainer** function. This function approximates SHAP values efficiently
    for neural networks. As an alternative, you could replace it with the **GradientExplainer**
    function.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过将模型和背景图像传入**DeepExplainer**函数来创建一个解释器对象。这个函数有效地为神经网络近似SHAP值。作为替代，你可以用**GradientExplainer**函数替换它。
- en: '[PRE10]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: We load 2 example images — a right and left turn (line 2) and transform them
    (line 6). This is important as the images should be in the same format as used
    to train the model. We then calculate the SHAP values for the predictions made
    using these images (line 10).
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 我们加载2张示例图像——一张右转和一张左转（第2行），并进行变换（第6行）。这很重要，因为图像应该与训练模型时使用的格式相同。然后，我们计算这些图像预测的SHAP值（第10行）。
- en: '[PRE11]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Finally, we can display the SHAP values using the **image_plot** function.
    But, we first need to restructure them. The SHAP values are returned with dimensions:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以使用**image_plot**函数来显示SHAP值。但我们首先需要重新构造这些值。SHAP值的返回维度是：
- en: '**( #targets, #images, #channels, #width, #height)**'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '**( #targets, #images, #channels, #width, #height)**'
- en: 'We use the transpose function so we have dimensions:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用转置函数，所以我们有维度：
- en: '**( #targets, #images, #width, #height, #channels)**'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '**（#targets, #images, #width, #height, #channels）**'
- en: Note, we have also passed the original images into the **image_plot** function.
    The **test_input** images would look strange due to the transformations.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们也将原始图像传入了**image_plot**函数。由于变换，**test_input**图像可能会显得很奇怪。
- en: '[PRE12]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: You can see the result in **Figure 4**. The first column gives the original
    images. The second and third columns are the SHAP values for the x and y prediction
    respectively. Blue pixels have decreased the prediction. In comparison, red pixels
    have increased the prediction. In other words, for the x prediction, red pixels
    have resulted in a sharper right turn.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在**图4**中看到结果。第一列给出原始图像。第二列和第三列分别是x和y预测的SHAP值。蓝色像素降低了预测值。相比之下，红色像素增加了预测值。换句话说，对于x预测，红色像素导致了更尖锐的右转。
- en: '![](../Images/996afc6e687106197c27ae1f796ff6d3.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/996afc6e687106197c27ae1f796ff6d3.png)'
- en: 'Figure 4: example shap values on a left an right turn (source: author)'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：左转和右转的示例shap值（来源：作者）
- en: Now we are getting somewhere. The important result is that the model is using
    background pixels. You can see this in Figure 5 where we zoom in on the x prediction
    for the right turn. In other words, the background is important to the prediction.
    That explains the poor performance! When we moved to a new room, the background
    changed and our predictions became unreliable.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了进展。重要的结果是模型正在使用背景像素。你可以在图5中看到这一点，我们对右转的x预测进行了放大。换句话说，背景对预测很重要。这解释了表现不佳的原因！当我们转到一个新房间时，背景发生了变化，我们的预测变得不可靠。
- en: '![](../Images/3c10a5ffc40ef7b10fd59e4240b4705b.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3c10a5ffc40ef7b10fd59e4240b4705b.png)'
- en: 'Figure 5: shap values for x prediction of right turn (source: author)'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 图5：右转x预测的shap值（来源：作者）
- en: The model is overfitted to the data from room 1\. The same objects and background
    are present in every image. As a result, the model associates these with left
    and right turns. We couldn’t identify this in our evaluation as we have the same
    background in both the training and validation images.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 模型对房间1的数据过拟合。每张图像中都有相同的对象和背景。因此，模型将这些与左转和右转关联起来。由于训练和验证图像中都有相同的背景，我们在评估中无法识别出这一点。
- en: '![](../Images/c310b18d34dbc7beff0814b3f6ef991f.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c310b18d34dbc7beff0814b3f6ef991f.png)'
- en: 'Figure 6: overfitting to training data (source: author)'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 图6：对训练数据的过拟合（来源：作者）
- en: Improving the model
  id: totrans-81
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 改进模型
- en: We want our model to perform well under all conditions. To achieve this, we
    would expect it to only use pixels from the track. So, let’s discuss some ways
    of making the model more robust.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望我们的模型在所有条件下表现良好。为此，我们期望它只使用轨迹上的像素。那么，让我们讨论一些提高模型鲁棒性的方法。
- en: Collecting new data
  id: totrans-83
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 收集新数据
- en: The best solution is to simply collect more data. We already have some from
    room 2 and 3\. Following the same process, we train a new model using data from
    all 3 rooms. Looking at **Figure 7**, it now has a better performance on images
    from the new rooms.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 最好的解决方案是简单地收集更多的数据。我们已经有了一些来自房间2和3的数据。按照相同的过程，我们使用来自所有3个房间的数据训练一个新模型。查看**图7**，它现在在新房间的图像上表现更好。
- en: '![](../Images/10716511b78e52460d55d1b4b3ac4ea0.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/10716511b78e52460d55d1b4b3ac4ea0.png)'
- en: 'Figure 7: evaluation of the new model on rooms 2 and 3 (source: author)'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 图7：在房间2和3上评估新模型（来源：作者）
- en: The hope is that by training on data from multiple rooms we break the associations
    between turns and the background. Different objects are now present on left and
    right turns but the track remains the same. The model should learn that the track
    is what is important to the prediction.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 希望通过在多个房间的数据上训练，我们可以打破转弯与背景之间的关联。现在，左转和右转上存在不同的对象，但轨迹保持不变。模型应该学会轨迹才是预测的重要因素。
- en: We can confirm this by looking at the SHAP values for the new model. These are
    for the same turns we saw in **Figure 4**. There is now less weight put on the
    background pixels. Okay, it’s not perfect but we are getting somewhere.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过查看新模型的SHAP值来确认这一点。这些值对应于我们在**图4**中看到的相同转弯。现在，背景像素的权重较少。好吧，虽然不完美，但我们在进步。
- en: '![](../Images/7eaf7b8a5080b75e89c7e6b8ac901e88.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7eaf7b8a5080b75e89c7e6b8ac901e88.png)'
- en: 'Figure 8: shap values from model trained on all 3 rooms (source: author)'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 图8：在所有3个房间上训练的模型的shap值（来源：作者）
- en: We could continue to collect data. The more locations we collect data the more
    robust our model will be. However, data collection can be time-consuming (and
    boring!). Instead, we can look to data augmentation.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以继续收集数据。我们收集的数据地点越多，我们的模型就会越强大。然而，数据收集可能是耗时的（而且无聊！）。相反，我们可以考虑数据增强。
- en: Data Augmentations
  id: totrans-92
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据增强
- en: Data augmentation is when we systematically or randomly alter images using code.
    This allows us to artificially introduce noise and increase the size of our dataset.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 数据增强是指我们使用代码系统地或随机地改变图像。这使我们能够人为地引入噪声并增加数据集的大小。
- en: For example, we could double the size of our dataset by **flipping images**
    on the vertical axis. We can do this because our track is symmetrical. As seen
    in Figure 9, **deletion** could also be a useful method. This involves including
    images where objects or the entire background have been removed.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，我们可以通过**翻转图像**在垂直轴上来将数据集的大小翻倍。我们之所以可以这样做，是因为我们的轨道是对称的。如图9所示，**删除**也可能是一个有用的方法。这涉及到包含那些对象或整个背景被去除的图像。
- en: '![](../Images/c7eeb92d54e15f3bc7b7d973e68ef02d.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c7eeb92d54e15f3bc7b7d973e68ef02d.png)'
- en: 'Figure 9: example of image augmentation using deletion (source: author)'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 图9：使用删除进行图像增强的示例（来源：作者）
- en: When building a robust model, you should also consider factors like lighting
    conditions and image quality. We can simulate these using color jitter or by adding
    noise. If you want to learn about all of these methods check out the article below.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在构建强大的模型时，您还应该考虑诸如光照条件和图像质量等因素。我们可以通过颜色抖动或添加噪声来模拟这些因素。如果您想了解所有这些方法，请查看下面的文章。
- en: '[](/augmenting-images-for-deep-learning-3f1ea92a891c?source=post_page-----4b562ddef30d--------------------------------)
    [## Augmenting Images for Deep Learning'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/augmenting-images-for-deep-learning-3f1ea92a891c?source=post_page-----4b562ddef30d--------------------------------)
    [## 用于深度学习的图像增强'
- en: Using Python to augment data by flipping, adjusting brightness, color jitter
    and random noise
  id: totrans-99
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用 Python 进行数据增强，包括翻转、调整亮度、颜色抖动和随机噪声
- en: towardsdatascience.com](/augmenting-images-for-deep-learning-3f1ea92a891c?source=post_page-----4b562ddef30d--------------------------------)
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/augmenting-images-for-deep-learning-3f1ea92a891c?source=post_page-----4b562ddef30d--------------------------------)
- en: In the above article, we also discuss why it is difficult to tell if these augmentations
    have made the model more robust. We could deploy the model in many environments
    but this is time-consuming. Thankfully, SHAP can be used as an alternative. Like
    with data collection, it can give us insight into how the augmentations have changed
    the way the model makes predictions.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述文章中，我们还讨论了为什么很难判断这些增强是否使模型更强大。我们可以在许多环境中部署模型，但这很耗时。幸运的是，SHAP 可以作为一种替代方案。与数据收集一样，它可以帮助我们了解增强如何改变模型的预测方式。
- en: I hope you enjoyed this article! You can support me by becoming one of my [**referred
    members**](https://conorosullyds.medium.com/membership) **:)**
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 希望您喜欢这篇文章！您可以通过成为我的[**推荐会员**](https://conorosullyds.medium.com/membership) **:)**
    来支持我。
- en: '[](https://conorosullyds.medium.com/membership?source=post_page-----4b562ddef30d--------------------------------)
    [## Join Medium with my referral link — Conor O’Sullivan'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://conorosullyds.medium.com/membership?source=post_page-----4b562ddef30d--------------------------------)
    [## 通过我的推荐链接加入 Medium — Conor O’Sullivan'
- en: As a Medium member, a portion of your membership fee goes to writers you read,
    and you get full access to every story…
  id: totrans-104
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 作为 Medium 会员，您的部分会员费会分配给您阅读的作者，同时您可以完全访问每一个故事……
- en: conorosullyds.medium.com](https://conorosullyds.medium.com/membership?source=post_page-----4b562ddef30d--------------------------------)
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: conorosullyds.medium.com](https://conorosullyds.medium.com/membership?source=post_page-----4b562ddef30d--------------------------------)
- en: '| [Twitter](https://twitter.com/conorosullyDS) | [YouTube](https://www.youtube.com/channel/UChsoWqJbEjBwrn00Zvghi4w)
    | [Newsletter](https://mailchi.mp/aa82a5ce1dc0/signup) — sign up for FREE access
    to a [Python SHAP course](https://adataodyssey.com/courses/shap-with-python/)'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '| [Twitter](https://twitter.com/conorosullyDS) | [YouTube](https://www.youtube.com/channel/UChsoWqJbEjBwrn00Zvghi4w)
    | [Newsletter](https://mailchi.mp/aa82a5ce1dc0/signup) — 免费注册以获得[Python SHAP 课程](https://adataodyssey.com/courses/shap-with-python/)'
- en: Dataset
  id: totrans-107
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据集
- en: '**JatRacer Images** (CC0: Public Domain) [https://www.kaggle.com/datasets/conorsully1/jatracer-images](https://www.kaggle.com/datasets/conorsully1/jatracer-images)'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '**JatRacer 图像**（CC0: 公共领域） [https://www.kaggle.com/datasets/conorsully1/jatracer-images](https://www.kaggle.com/datasets/conorsully1/jatracer-images)'
- en: References
  id: totrans-109
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考资料
- en: SHAP, **PyTorch Deep Explainer MNIST example** [https://shap.readthedocs.io/en/latest/example_notebooks/image_examples/image_classification/PyTorch%20Deep%20Explainer%20MNIST%20example.html](https://shap.readthedocs.io/en/latest/example_notebooks/image_examples/image_classification/PyTorch%20Deep%20Explainer%20MNIST%20example.html)
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: SHAP，**PyTorch 深度解释器 MNIST 示例** [https://shap.readthedocs.io/en/latest/example_notebooks/image_examples/image_classification/PyTorch%20Deep%20Explainer%20MNIST%20example.html](https://shap.readthedocs.io/en/latest/example_notebooks/image_examples/image_classification/PyTorch%20Deep%20Explainer%20MNIST%20example.html)
