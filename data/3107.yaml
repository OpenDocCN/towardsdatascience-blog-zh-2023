- en: Activation Functions For Neural Networks & Deep Learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 神经网络与深度学习的激活函数
- en: 原文：[https://towardsdatascience.com/activation-functions-non-linearity-neural-networks-101-ab0036a2e701?source=collection_archive---------8-----------------------#2023-10-12](https://towardsdatascience.com/activation-functions-non-linearity-neural-networks-101-ab0036a2e701?source=collection_archive---------8-----------------------#2023-10-12)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/activation-functions-non-linearity-neural-networks-101-ab0036a2e701?source=collection_archive---------8-----------------------#2023-10-12](https://towardsdatascience.com/activation-functions-non-linearity-neural-networks-101-ab0036a2e701?source=collection_archive---------8-----------------------#2023-10-12)
- en: Explaining why neural networks can learn (nearly) anything and everything
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解释神经网络为什么能学习（几乎）任何事情
- en: '[](https://medium.com/@egorhowell?source=post_page-----ab0036a2e701--------------------------------)[![Egor
    Howell](../Images/1f796e828f1625440467d01dcc3e40cd.png)](https://medium.com/@egorhowell?source=post_page-----ab0036a2e701--------------------------------)[](https://towardsdatascience.com/?source=post_page-----ab0036a2e701--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----ab0036a2e701--------------------------------)
    [Egor Howell](https://medium.com/@egorhowell?source=post_page-----ab0036a2e701--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@egorhowell?source=post_page-----ab0036a2e701--------------------------------)[![Egor
    Howell](../Images/1f796e828f1625440467d01dcc3e40cd.png)](https://medium.com/@egorhowell?source=post_page-----ab0036a2e701--------------------------------)[](https://towardsdatascience.com/?source=post_page-----ab0036a2e701--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----ab0036a2e701--------------------------------)
    [Egor Howell](https://medium.com/@egorhowell?source=post_page-----ab0036a2e701--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F1cac491223b2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Factivation-functions-non-linearity-neural-networks-101-ab0036a2e701&user=Egor+Howell&userId=1cac491223b2&source=post_page-1cac491223b2----ab0036a2e701---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----ab0036a2e701--------------------------------)
    ·8 min read·Oct 12, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fab0036a2e701&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Factivation-functions-non-linearity-neural-networks-101-ab0036a2e701&user=Egor+Howell&userId=1cac491223b2&source=-----ab0036a2e701---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F1cac491223b2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Factivation-functions-non-linearity-neural-networks-101-ab0036a2e701&user=Egor+Howell&userId=1cac491223b2&source=post_page-1cac491223b2----ab0036a2e701---------------------post_header-----------)
    发布于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----ab0036a2e701--------------------------------)
    ·8 min read·Oct 12, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fab0036a2e701&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Factivation-functions-non-linearity-neural-networks-101-ab0036a2e701&user=Egor+Howell&userId=1cac491223b2&source=-----ab0036a2e701---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fab0036a2e701&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Factivation-functions-non-linearity-neural-networks-101-ab0036a2e701&source=-----ab0036a2e701---------------------bookmark_footer-----------)![](../Images/f20f48d260492d8ecb7fb2bf39ec6862.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fab0036a2e701&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Factivation-functions-non-linearity-neural-networks-101-ab0036a2e701&source=-----ab0036a2e701---------------------bookmark_footer-----------)![](../Images/f20f48d260492d8ecb7fb2bf39ec6862.png)'
- en: Machine learning icons created by Becris — Flatico. [https://www.flaticon.com/free-icons/machine-learning](https://www.flaticon.com/free-icons/machine-learning)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 由 Becris 创作的机器学习图标 — Flatico。[https://www.flaticon.com/free-icons/machine-learning](https://www.flaticon.com/free-icons/machine-learning)
- en: Background
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 背景
- en: 'In my previous article, we introduced the [***multi-layer perceptron***](https://en.wikipedia.org/wiki/Multilayer_perceptron)
    ***(MLP)****,* which is just a set of stacked interconnected [***perceptrons***](https://en.wikipedia.org/wiki/Perceptron).
    I highly recommend you check my previous post if you are unfamiliar with the perceptron
    and MLP as will discuss it quite a bit in this article:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在我的前一篇文章中，我们介绍了[***多层感知器***](https://zh.wikipedia.org/wiki/%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E5%99%A8)
    ***(MLP)***，它只是一组堆叠在一起的互相连接的[***感知器***](https://zh.wikipedia.org/wiki/%E6%84%9F%E7%9F%A5%E5%99%A8)。如果你对感知器和
    MLP 不熟悉，我强烈建议你查看我之前的文章，因为我们在本文中将会大量讨论：
- en: '[](https://levelup.gitconnected.com/intro-perceptron-architecture-neural-networks-101-2a487062810c?source=post_page-----ab0036a2e701--------------------------------)
    [## Intro, Perceptron & Architecture: Neural Networks 101'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://levelup.gitconnected.com/intro-perceptron-architecture-neural-networks-101-2a487062810c?source=post_page-----ab0036a2e701--------------------------------)
    [## 介绍，感知机与架构：神经网络101'
- en: An introduction to Neural Networks and their building blocks
  id: totrans-12
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 神经网络及其构建模块的介绍
- en: levelup.gitconnected.com](https://levelup.gitconnected.com/intro-perceptron-architecture-neural-networks-101-2a487062810c?source=post_page-----ab0036a2e701--------------------------------)
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: levelup.gitconnected.com](https://levelup.gitconnected.com/intro-perceptron-architecture-neural-networks-101-2a487062810c?source=post_page-----ab0036a2e701--------------------------------)
- en: 'An example MLP with two hidden layers is shown below:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 下图展示了一个具有两个隐藏层的MLP示例：
- en: '![](../Images/94cdac9897a5e852d9cd6edd425fcba0.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/94cdac9897a5e852d9cd6edd425fcba0.png)'
- en: A basic two-hidden multi-layer perceptron. Diagram by author.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 一个基础的双隐藏层多层感知机。图示由作者提供。
- en: 'However, the problem with the MLP is that it can only fit a [***linear classifier***](https://en.wikipedia.org/wiki/Linear_separability).
    This is because the individual perceptrons have a [***step function***](https://en.wikipedia.org/wiki/Step_function)
    as their [***activation function***](https://en.wikipedia.org/wiki/Activation_function),
    which is linear:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，MLP的问题在于它只能拟合一个[***线性分类器***](https://en.wikipedia.org/wiki/Linear_separability)。这是因为单个感知机具有一个[***阶跃函数***](https://en.wikipedia.org/wiki/Step_function)作为其[***激活函数***](https://en.wikipedia.org/wiki/Activation_function)，这是线性的：
