- en: 'Random Forests in 2023: Modern Extensions of a Powerful Method'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 2023年的随机森林：一种强大方法的现代扩展
- en: 原文：[https://towardsdatascience.com/random-forests-in-2023-modern-extensions-of-a-powerful-method-b62debaf1d62?source=collection_archive---------9-----------------------#2023-11-07](https://towardsdatascience.com/random-forests-in-2023-modern-extensions-of-a-powerful-method-b62debaf1d62?source=collection_archive---------9-----------------------#2023-11-07)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/random-forests-in-2023-modern-extensions-of-a-powerful-method-b62debaf1d62?source=collection_archive---------9-----------------------#2023-11-07](https://towardsdatascience.com/random-forests-in-2023-modern-extensions-of-a-powerful-method-b62debaf1d62?source=collection_archive---------9-----------------------#2023-11-07)
- en: Random Forests came a long way
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 随机森林取得了长足的发展
- en: '[](https://medium.com/@jeffrey_85949?source=post_page-----b62debaf1d62--------------------------------)[![Jeffrey
    Näf](../Images/0ce6db85501192cdebeeb910eb81a688.png)](https://medium.com/@jeffrey_85949?source=post_page-----b62debaf1d62--------------------------------)[](https://towardsdatascience.com/?source=post_page-----b62debaf1d62--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----b62debaf1d62--------------------------------)
    [Jeffrey Näf](https://medium.com/@jeffrey_85949?source=post_page-----b62debaf1d62--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@jeffrey_85949?source=post_page-----b62debaf1d62--------------------------------)[![Jeffrey
    Näf](../Images/0ce6db85501192cdebeeb910eb81a688.png)](https://medium.com/@jeffrey_85949?source=post_page-----b62debaf1d62--------------------------------)[](https://towardsdatascience.com/?source=post_page-----b62debaf1d62--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----b62debaf1d62--------------------------------)
    [Jeffrey Näf](https://medium.com/@jeffrey_85949?source=post_page-----b62debaf1d62--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fca780798011a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frandom-forests-in-2023-modern-extensions-of-a-powerful-method-b62debaf1d62&user=Jeffrey+N%C3%A4f&userId=ca780798011a&source=post_page-ca780798011a----b62debaf1d62---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----b62debaf1d62--------------------------------)
    ·12 min read·Nov 7, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fb62debaf1d62&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frandom-forests-in-2023-modern-extensions-of-a-powerful-method-b62debaf1d62&user=Jeffrey+N%C3%A4f&userId=ca780798011a&source=-----b62debaf1d62---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fca780798011a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frandom-forests-in-2023-modern-extensions-of-a-powerful-method-b62debaf1d62&user=Jeffrey+N%C3%A4f&userId=ca780798011a&source=post_page-ca780798011a----b62debaf1d62---------------------post_header-----------)
    发布于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----b62debaf1d62--------------------------------)
    ·12 min read·2023年11月7日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fb62debaf1d62&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frandom-forests-in-2023-modern-extensions-of-a-powerful-method-b62debaf1d62&user=Jeffrey+N%C3%A4f&userId=ca780798011a&source=-----b62debaf1d62---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fb62debaf1d62&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frandom-forests-in-2023-modern-extensions-of-a-powerful-method-b62debaf1d62&source=-----b62debaf1d62---------------------bookmark_footer-----------)![](../Images/11d2719adb78db10fac42a7884ac2918.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fb62debaf1d62&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frandom-forests-in-2023-modern-extensions-of-a-powerful-method-b62debaf1d62&source=-----b62debaf1d62---------------------bookmark_footer-----------)![](../Images/11d2719adb78db10fac42a7884ac2918.png)'
- en: 'Features of modern Random Forest methods. Source: Author.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 现代随机森林方法的特点。来源：作者。
- en: In terms of Machine Learning timelines, Random Forests (RFs), introduced in
    the seminal paper of Breimann ([1]), are ancient. Despite their age, they keep
    impressing with their performance and are a topic of active research. The goal
    of this article is to highlight what a versatile toolbox Random Forest methods
    have become, focussing on **Generalized Random Forest (GRF)** and **Distributional
    Random Forest (DRF)**.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习的时间线中，随机森林（RFs），在Breimann的开创性论文中提出（[1]），可以算得上古老了。尽管它们已经有些年头，但凭借其卓越的性能仍然令人印象深刻，并且仍是活跃研究的主题。本文旨在突出随机森林方法已成为多功能工具箱的特点，重点关注**广义随机森林（GRF）**和**分布随机森林（DRF）**。
- en: In short, the main idea underlying both methods is that the weights implicitly
    produced by RF can be used to estimate targets other than the conditional expectation.
    The idea of GRF is to use a Random Forest with a splitting criterion that is adapted
    to the target one has in mind (e.g., conditional mean, conditional quantiles,
    or the conditional treatment effect). The idea of DRF is to adapt the splitting
    criterion such that the whole conditional distribution can be estimated. From
    this object, many different targets can then be derived in a second step. In fact,
    I mostly talk about DRF in this article, as I am more familiar with this method
    and it is somewhat more streamlined (only one forest has to be fitted for a wide
    range of targets). However, all the advantages, indicated in the figure above,
    also apply to GRF and in fact, the DRF package in R is built upon the [professional
    implementation of GRF](https://grf-labs.github.io/grf/). Moreover, the fact that
    the splitting criterion of GRF forests is adapted to the target means it can have
    better performance than DRF. This is particularly true for binary *Y*, where probability_forests()
    should be used. So, even though I talk mostly about DRF, GRF should be kept in
    mind throughout this article.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，这两种方法的主要思想是，RF隐式产生的权重可以用于估计除条件期望之外的其他目标。GRF的理念是使用一个随机森林，分裂准则适应于所设想的目标（例如条件均值、条件分位数或条件处理效应）。DRF的理念是调整分裂准则，以便能够估计整个条件分布。从这个对象中，可以在第二步派生出许多不同的目标。事实上，本文主要讨论DRF，因为我对这种方法更为熟悉，而且它在处理广泛目标时更加简洁（只需为广泛的目标拟合一个森林）。然而，图中指出的所有优点也适用于GRF，实际上，R中的DRF包是建立在[GRF的专业实现](https://grf-labs.github.io/grf/)之上的。此外，GRF森林的分裂准则适应于目标，这意味着它可能比DRF表现更好。这对于二元*Y*尤其如此，此时应使用probability_forests()。因此，尽管我主要讨论DRF，但在整篇文章中应始终记住GRF。
- en: 'The goal of this article is to provide an overview with links to deeper reading
    in the corresponding sections. We will go through each of the points in the above
    figure clock-wise, reference the corresponding articles, and highlight it with
    a little example. I first quickly summarize the most important links to further
    reading below:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 本文的目标是提供一个概述，并提供相应部分的深入阅读链接。我们将按图中的时钟方向逐一介绍每个要点，引用相关的文章，并通过一个小示例进行强调。我首先快速总结了下面的最重要的进一步阅读链接：
- en: 'Versatility/Performance: [Medium Article](/drf-a-random-forest-for-almost-everything-625fa5c3bcb8)
    and Original Papers ([DRF](https://jmlr.org/papers/v23/21-0585.html)/[GRF](https://projecteuclid.org/journals/annals-of-statistics/volume-47/issue-2/Generalized-random-forests/10.1214/18-AOS1709.full))'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 多样性/性能： [Medium Article](/drf-a-random-forest-for-almost-everything-625fa5c3bcb8)
    和原始论文（[DRF](https://jmlr.org/papers/v23/21-0585.html)/[GRF](https://projecteuclid.org/journals/annals-of-statistics/volume-47/issue-2/Generalized-random-forests/10.1214/18-AOS1709.full)）
- en: 'Missing Values Incorporated: [Medium Article](/random-forests-and-missing-values-3daaea103db0)'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 缺失值纳入： [Medium Article](/random-forests-and-missing-values-3daaea103db0)
- en: 'Uncertainty Measures: [Medium Article](/inference-for-distributional-random-forests-64610bbb3927)'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 不确定性度量： [Medium Article](/inference-for-distributional-random-forests-64610bbb3927)
- en: 'Variable Importance: [Medium Article](https://medium.com/towards-data-science/variable-importance-in-random-forests-20c6690e44e0)'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 变量重要性： [Medium Article](https://medium.com/towards-data-science/variable-importance-in-random-forests-20c6690e44e0)
- en: The Example
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 示例
- en: We take *X_1, X_2, X_4, …, X_10* independently uniform between (-1,1) and create
    dependence between *X_1* and *X_3* by taking *X_3=X_1 + uniform error*. Then we
    simulate *Y* as
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 我们取 *X_1, X_2, X_4, …, X_10* 独立地均匀分布在（-1,1）之间，并通过 *X_3=X_1 + uniform error*
    来创建 *X_1* 和 *X_3* 之间的依赖关系。然后我们模拟 *Y* 为
- en: '![](../Images/db8b95d3d53203e739c5509eb1aa992e.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/db8b95d3d53203e739c5509eb1aa992e.png)'
- en: '[PRE0]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Notice that with the function ampute from the [mice package](https://cran.r-project.org/web/packages/mice/index.html),
    we put **Missing not at Random (MAR)** missing values on ***X*** to highlight
    the ability of GRF/DRF to deal with missing values. Moreover, in the above process
    only *X_1* and *X_2* are relevant for predicting *Y*, all other variables are
    “noise” variables. Such a “sparse” setting might actually be common in real-life
    datasets.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，通过[mice包](https://cran.r-project.org/web/packages/mice/index.html)中的ampute函数，我们将**缺失值非随机（MAR）**设置在***X***上，以突出GRF/DRF处理缺失值的能力。此外，在上述过程中，仅
    *X_1* 和 *X_2* 对预测 *Y* 是相关的，所有其他变量都是“噪声”变量。这种“稀疏”设置在实际数据集中可能很常见。
- en: 'We now choose a test point for this example that we will use throughout:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们选择一个测试点作为这个示例，将在整个过程中使用：
- en: '[PRE1]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Versatility
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 多样性
- en: 'DRF estimates the conditional distribution *P_{Y|****X****=****x****}*in the
    form of simple weights:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: DRF 估计条件分布 *P_{Y|****X****=****x****}* 以简单权重的形式：
- en: '![](../Images/206d5127ad181f1e1458ee40d1784285.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/206d5127ad181f1e1458ee40d1784285.png)'
- en: From these weights, a wide range of targets can be calculated, or they can be
    used to simulate from the conditional distribution. A good reference for its versatility
    is the original [research article](https://jmlr.org/papers/v23/21-0585.html),
    where a lot of examples were used, as well as the corresponding [medium article](/drf-a-random-forest-for-almost-everything-625fa5c3bcb8).
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 从这些权重中，可以计算出广泛的目标，或者可以用来从条件分布中进行模拟。它的多功能性一个很好的参考是原始的 [研究文章](https://jmlr.org/papers/v23/21-0585.html)，其中使用了很多例子，还有相应的
    [medium 文章](/drf-a-random-forest-for-almost-everything-625fa5c3bcb8)。
- en: 'In the example, we first simulate from this distribution:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们首先从这个分布中进行模拟：
- en: '[PRE2]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '![](../Images/707d8f363302b0a531f4172989c20161.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/707d8f363302b0a531f4172989c20161.png)'
- en: 'Histogram of the simulated conditional distribution overlaid with the true
    density (in red). Source: Author.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 模拟条件分布的直方图与真实密度（红色）叠加。来源：作者。
- en: The plot shows the approximate draws from the conditional distribution overlaid
    with the true density in red. We now use this to estimate the **conditional expectation**
    and the **conditional (0.05, 0.95) quantiles** at ***x:***
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 图表显示了从条件分布中大致抽取的样本，并与真实密度（红色）叠加。我们现在使用此数据来估计 **条件期望** 和 **条件 (0.05, 0.95) 分位数**
    在 ***x:***
- en: '[PRE3]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '![](../Images/b011f4a2998015c8a6f07661ea241c2c.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b011f4a2998015c8a6f07661ea241c2c.png)'
- en: 'Histogram of the simulated conditional distribution overlaid with the true
    density (in red). Additionally, the estimated conditional expectation and the
    conditional (0.05, 0.95) quantiles are in blue, with true values in red. Source:
    Author.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 模拟条件分布的直方图与真实密度（红色）叠加。此外，估计的条件期望和条件 (0.05, 0.95) 分位数为蓝色，真实值为红色。来源：作者。
- en: Likewise, many targets can be calculated with GRF, only in this case for each
    of those two targets a different forest would need to be fit. In particular, *regression_forest()*
    for the conditional expectation and *quantile_forest()* for the quantiles.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，GRF 可以计算许多目标，但在这种情况下，每个目标需要拟合不同的森林。特别是，*regression_forest()* 用于条件期望，*quantile_forest()*
    用于分位数。
- en: What GRF cannot do is deal with multivariate targets, which is possible with
    DRF as well.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: GRF 无法处理多变量目标，而 DRF 可以做到这一点。
- en: Performance
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 性能
- en: Despite all the work on powerful new (nonparametric) methods, such as neural
    nets, tree-based methods are consistently able to beat competitors on tabular
    data. See e.g., this [fascinating paper](https://arxiv.org/abs/2207.08815), or
    this older paper on the [strength of RF in classification](https://dl.acm.org/doi/pdf/10.5555/2627435.2697065).
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管有大量关于强大新（非参数）方法的工作，如神经网络，但基于树的方法在表格数据上始终能够击败竞争者。请参见，例如，这篇 [引人入胜的论文](https://arxiv.org/abs/2207.08815)，或这篇关于
    [RF 在分类中的强度](https://dl.acm.org/doi/pdf/10.5555/2627435.2697065) 的旧论文。
- en: To be fair, with parameter tuning, [boosted tree methods](/an-overview-of-boosting-methods-catboost-xgboost-adaboost-lightboost-histogram-based-gradient-407447633ac1),
    such as XGboost, often take the lead, at least when it comes to classical prediction
    (which corresponds to conditional expectation estimation). Nonetheless, the robust
    performance RF methods tend to have without any tuning is remarkable. Moreover,
    there has also been work on improving the performance of Random Forests, for example,
    the [hedged Random Forest approach](https://arxiv.org/pdf/2308.15384.pdf).
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 公平地说，通过参数调优，[提升树方法](/an-overview-of-boosting-methods-catboost-xgboost-adaboost-lightboost-histogram-based-gradient-407447633ac1)，如
    XGboost，通常在经典预测（对应于条件期望估计）中占据主导地位。然而，RF 方法在没有任何调优下展现的强大性能是值得注意的。此外，还有关于改进随机森林性能的研究，例如，[对冲随机森林方法](https://arxiv.org/pdf/2308.15384.pdf)。
- en: Missing Values Incorporated
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 缺失值整合
- en: '“Missing incorporated in attributes criterion” (MIA) from [this paper](https://www.sciencedirect.com/science/article/abs/pii/S0167865508000305)
    is a very simple but very powerful idea that allows tree-based methods to handle
    missing data. This was implemented in the GRF R package and so it is also available
    in DRF. The details are also explained in this [medium article](/random-forests-and-missing-values-3daaea103db0).
    As simple as the concept is, it works remarkably well in practice: In the above
    example, DRF had no trouble handling substantial MAR missingness in the training
    data ***X*** (!)'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 来自 [这篇论文](https://www.sciencedirect.com/science/article/abs/pii/S0167865508000305)
    的“缺失数据合并属性标准”（MIA）是一个非常简单但非常强大的想法，它允许基于树的方法处理缺失数据。这已在 GRF R 包中实现，因此 DRF 中也可以使用。详细信息也在这篇
    [medium 文章](/random-forests-and-missing-values-3daaea103db0) 中解释。尽管这一概念很简单，但在实践中效果非常好：在上述示例中，DRF
    在处理训练数据中的大量 MAR 缺失值时毫不费力 ***X*** (!)
- en: '**Uncertainty Measures**'
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**不确定性度量**'
- en: As a statistician I don’t just want point estimates (even of a distribution),
    but also a measure of **estimation uncertainty** of my parameters (even if the
    “parameter” is my whole distribution). Turns out that a simple additional subsampling
    baked into DRF/GRF allows for a principled uncertainty quantification for large
    sample sizes. The theory behind this in the case of DRF is derived in this [research
    article](https://arxiv.org/abs/2302.05761), but I also explain it in this [medium
    article](/inference-for-distributional-random-forests-64610bbb3927). GRF has all
    the theory in the [original paper](https://projecteuclid.org/journals/annals-of-statistics/volume-47/issue-2/Generalized-random-forests/10.1214/18-AOS1709.full).
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 作为统计学家，我不仅希望得到点估计（即使是分布的点估计），还希望得到我的参数（即使“参数”是我的整个分布）的 **估计不确定性**。事实证明，DRF/GRF
    中内置的简单额外子抽样可以为大样本量提供原则性的不确定性量化。关于 DRF 的理论在这篇 [研究文章](https://arxiv.org/abs/2302.05761)
    中得以推导，我在这篇 [medium 文章](/inference-for-distributional-random-forests-64610bbb3927)
    中也进行了解释。GRF 的所有理论在 [原始论文](https://projecteuclid.org/journals/annals-of-statistics/volume-47/issue-2/Generalized-random-forests/10.1214/18-AOS1709.full)
    中都有描述。
- en: 'We adapt this for the above example:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将其应用于上述示例：
- en: '[PRE4]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '![](../Images/bb22e3661f626b01f57210c507fef1c2.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bb22e3661f626b01f57210c507fef1c2.png)'
- en: 'Histogram of the simulated conditional distribution overlaid with the true
    density (in red). Additionally, the estimated conditional expectation and the
    conditional (0.05, 0.95) quantiles are in blue, with true values in red. Moreover,
    the dashed red lines are the confidence intervals for the estimates as calculated
    by DRF. Source: Author.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 模拟条件分布的直方图叠加了真实密度（红色）。此外，估计的条件期望和条件 (0.05, 0.95) 分位数为蓝色，真实值为红色。此外，虚线红色的线条是由
    DRF 计算出的估计置信区间。来源：作者。
- en: As can be seen from the above code, we essentially have *B* subtrees that can
    be used to calculate the measure each time. From these *B* samples of mean and
    quantiles, we can then calculate variances and use a normal approximation to obtain
    (asymptotic) confidence intervals seen in the dashed line in the Figure. *Again,
    all of this can be done despite the missing values in* ***X***(!)
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 从上述代码可以看出，我们实际上有 *B* 个子树可以用来每次计算度量。通过这些 *B* 个均值和分位数样本，我们可以计算方差，并使用正态近似来获得图中虚线所示的（渐近）置信区间。*尽管存在缺失值，这一切依然可以完成*
    ***X***(!)
- en: '**Variable Importance**'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '**变量重要性**'
- en: 'A final important aspect of Random Forests is efficiently calculated variable
    importance measures. While traditional measures are somewhat ad hoc, for the traditional
    RF and for DRF, there are now principled measures available, as explained in this
    [medium article](https://medium.com/towards-data-science/variable-importance-in-random-forests-20c6690e44e0).
    For RF, the Sobol-MDA method reliably identifies the variables most important
    for conditional expectation estimation, whereas for DRF, the MMD-MDA identifies
    the variables most important for the estimation of the distribution overall. As
    discussed in the article, using the idea of *projected Random Forests*, these
    measures can be very efficiently implemented. We demonstrate this in the example
    with a less efficient implementation of the MMD variable importance measure:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 随机森林的一个重要方面是有效计算变量重要性度量。尽管传统的度量方法有些随意，但对于传统 RF 和 DRF，现在已经有了有原则的度量方法，如在这篇 [medium
    article](https://medium.com/towards-data-science/variable-importance-in-random-forests-20c6690e44e0)
    中解释的那样。对于 RF，Sobol-MDA 方法可靠地识别出对条件期望估计最重要的变量，而对于 DRF，MMD-MDA 识别出对整体分布估计最重要的变量。如文章中讨论的那样，使用
    *投影随机森林* 的理念，这些度量可以非常高效地实现。我们在例子中展示了 MMD 变量重要性度量的一个较低效实现：
- en: '[PRE5]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Here both *X_1* and *X_2* are correctly identified as being the most relevant
    variable when trying to estimate the distribution. Remarkably, despite the dependence
    of *X_3* and *X_1*, the measure correctly quantifies that *X_3* is not important
    for the prediction of the distribution of *Y*. This is something that the original
    MDA measure of Random Forests tends to do wrong, as demonstrated in the medium
    article. Moreover, notice again that the missing values in ***X*** are no problem
    here.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*X_1* 和 *X_2* 在尝试估计分布时被正确地识别为最相关的变量。值得注意的是，尽管 *X_3* 和 *X_1* 之间存在依赖关系，但测量方法正确地量化了
    *X_3* 对于预测 *Y* 分布的重要性。这是随机森林原始 MDA 测量往往做错的事情，如中等文章所示。此外，请再次注意，***X*** 中的缺失值在这里没有问题。
- en: Conclusion
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结论
- en: GRF/DRF and also the traditional Random Forest should not be missing in the
    toolbox of any data scientist. While methods like XGboost can have a better performance
    in traditional prediction, the many strengths of modern RF-based approaches render
    them an incredibly versatile tool.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: GRF/DRF 以及传统的随机森林在任何数据科学家的工具箱中都不应缺少。尽管像 XGboost 这样的方法在传统预测中可能表现更好，但现代基于 RF 的方法的许多优点使它们成为一个极其多功能的工具。
- en: Of course, one should keep in mind that these methods are still fully nonparametric,
    and a lot of data points are needed for the fit to make sense. This is in particularly
    true for the uncertainty quantification, which is only valid asymptotically, i.e.
    for “large” samples.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，需要记住的是这些方法仍然完全是非参数的，并且需要大量的数据点才能使拟合有意义。这一点在不确定性量化中尤其真实，它仅在渐近情况下有效，即对于“大”样本。
- en: Literature
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 文献
- en: '[1] Breiman, L. (2001). Random forests. Machine learning, 45(1):5–32.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] Breiman, L. (2001). Random forests. Machine learning, 45(1):5–32.'
- en: 'Appendix: Code'
  id: totrans-58
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录：代码
- en: '[PRE6]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
