- en: Fine-Tune Your Own Open-Source LLM Using the Latest Techniques
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ä½¿ç”¨æœ€æ–°æŠ€æœ¯å¾®è°ƒä½ çš„å¼€æºLLM
- en: åŸæ–‡ï¼š[https://towardsdatascience.com/how-to-efficiently-fine-tune-your-own-open-source-llm-using-novel-techniques-code-provided-03a4e67d1b48](https://towardsdatascience.com/how-to-efficiently-fine-tune-your-own-open-source-llm-using-novel-techniques-code-provided-03a4e67d1b48)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡ï¼š[https://towardsdatascience.com/how-to-efficiently-fine-tune-your-own-open-source-llm-using-novel-techniques-code-provided-03a4e67d1b48](https://towardsdatascience.com/how-to-efficiently-fine-tune-your-own-open-source-llm-using-novel-techniques-code-provided-03a4e67d1b48)
- en: In this article, I tune a base LLama2 LLM to output SQL code. I use Parameter
    Efficient Fine-Tuning techniques to optimise the process.
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: åœ¨è¿™ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘è°ƒæ•´äº†ä¸€ä¸ªåŸºç¡€çš„LLama2 LLMï¼Œä»¥è¾“å‡ºSQLä»£ç ã€‚æˆ‘ä½¿ç”¨äº†å‚æ•°é«˜æ•ˆå¾®è°ƒæŠ€æœ¯æ¥ä¼˜åŒ–è¿™ä¸€è¿‡ç¨‹ã€‚
- en: '[](https://medium.com/@christopher_karg?source=post_page-----03a4e67d1b48--------------------------------)[![Christopher
    Karg](../Images/9d163d59e0c3167732f55d497caf9db2.png)](https://medium.com/@christopher_karg?source=post_page-----03a4e67d1b48--------------------------------)[](https://towardsdatascience.com/?source=post_page-----03a4e67d1b48--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----03a4e67d1b48--------------------------------)
    [Christopher Karg](https://medium.com/@christopher_karg?source=post_page-----03a4e67d1b48--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@christopher_karg?source=post_page-----03a4e67d1b48--------------------------------)[![Christopher
    Karg](../Images/9d163d59e0c3167732f55d497caf9db2.png)](https://medium.com/@christopher_karg?source=post_page-----03a4e67d1b48--------------------------------)[](https://towardsdatascience.com/?source=post_page-----03a4e67d1b48--------------------------------)[![æ•°æ®ç§‘å­¦å‰æ²¿](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----03a4e67d1b48--------------------------------)
    [Christopher Karg](https://medium.com/@christopher_karg?source=post_page-----03a4e67d1b48--------------------------------)'
- en: Â·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----03a4e67d1b48--------------------------------)
    Â·13 min readÂ·Dec 15, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: Â·å‘è¡¨äº[æ•°æ®ç§‘å­¦å‰æ²¿](https://towardsdatascience.com/?source=post_page-----03a4e67d1b48--------------------------------)
    Â·13åˆ†é’Ÿé˜…è¯»Â·2023å¹´12æœˆ15æ—¥
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/389987bb764f9aceba43b821ecf0128e.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/389987bb764f9aceba43b821ecf0128e.png)'
- en: 'Source: [https://www.pexels.com/photo/calm-body-of-lake-between-mountains-346529/](https://www.pexels.com/photo/calm-body-of-lake-between-mountains-346529/)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 'æ¥æº: [https://www.pexels.com/photo/calm-body-of-lake-between-mountains-346529/](https://www.pexels.com/photo/calm-body-of-lake-between-mountains-346529/)'
- en: 'In [a previous article](https://medium.com/towards-data-science/quantisation-and-co-reducing-inference-times-on-llms-by-80-671db9349bdb),
    I began to make a case for why you would consider training your own LLM. I also
    provided a brief introduction to the hardware requirements, as well as methods
    for optimising the training and inference. In this article, I will cover exactly
    how to fine-tune an open-source LLM and provide code snippets for you to follow
    along and reproduce the results. We will tune a Llama2â€“7B model to provide us
    with SQL output based on natural language input â€” in other words, the model will
    convert a question we ask in natural language:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨[ä¸Šä¸€ç¯‡æ–‡ç« ](https://medium.com/towards-data-science/quantisation-and-co-reducing-inference-times-on-llms-by-80-671db9349bdb)ä¸­ï¼Œæˆ‘å¼€å§‹é˜è¿°ä¸ºä½•ä½ åº”è¯¥è€ƒè™‘è®­ç»ƒè‡ªå·±çš„LLMã€‚æˆ‘è¿˜ç®€è¦ä»‹ç»äº†ç¡¬ä»¶è¦æ±‚ä»¥åŠä¼˜åŒ–è®­ç»ƒå’Œæ¨ç†çš„æ–¹æ³•ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘å°†è¯¦ç»†ä»‹ç»å¦‚ä½•å¾®è°ƒä¸€ä¸ªå¼€æºLLMï¼Œå¹¶æä¾›ä»£ç ç‰‡æ®µä¾›ä½ è·Ÿéšå’Œé‡ç°ç»“æœã€‚æˆ‘ä»¬å°†è°ƒæ•´ä¸€ä¸ªLlama2â€“7Bæ¨¡å‹ï¼Œä½¿å…¶æ ¹æ®è‡ªç„¶è¯­è¨€è¾“å…¥æä¾›SQLè¾“å‡ºâ€”â€”æ¢å¥è¯è¯´ï¼Œæ¨¡å‹å°†æŠŠæˆ‘ä»¬ç”¨è‡ªç„¶è¯­è¨€æå‡ºçš„é—®é¢˜è½¬æ¢ä¸ºSQLæŸ¥è¯¢ï¼š
- en: '*â€œHow many customers decided to buy eggs in the month of November?â€*'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '*â€œåœ¨11æœˆä»½ï¼Œæœ‰å¤šå°‘å®¢æˆ·å†³å®šè´­ä¹°é¸¡è›‹ï¼Ÿâ€*'
- en: 'To a SQL query that fetches the corresponding result:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: åˆ°ä¸€ä¸ªSQLæŸ¥è¯¢ï¼Œè¯¥æŸ¥è¯¢è·å–ç›¸åº”çš„ç»“æœï¼š
- en: '[PRE0]'
  id: totrans-11
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'In each case, the schema of the database (DB) will be provided as the context
    for the LLM to work with:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ¯ç§æƒ…å†µä¸‹ï¼Œæ•°æ®åº“ï¼ˆDBï¼‰çš„æ¶æ„å°†ä½œä¸ºLLMçš„å·¥ä½œä¸Šä¸‹æ–‡æä¾›ï¼š
- en: '[PRE1]'
  id: totrans-13
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: We will be using [this dataset](https://huggingface.co/datasets/knowrohit07/know_sql)
    in the tuning process. Whilst this article is focussed primarily on achieving
    the above task, the methodology will be provided in such a way that you can adapt
    the tuning process to suit your requirements.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†åœ¨è°ƒæ•´è¿‡ç¨‹ä¸­ä½¿ç”¨[è¿™ä¸ªæ•°æ®é›†](https://huggingface.co/datasets/knowrohit07/know_sql)ã€‚è™½ç„¶è¿™ç¯‡æ–‡ç« ä¸»è¦é›†ä¸­åœ¨å®ç°ä¸Šè¿°ä»»åŠ¡ä¸Šï¼Œä½†æ–¹æ³•å°†ä»¥ä¸€ç§ä½ å¯ä»¥è°ƒæ•´ä»¥æ»¡è¶³ä½ è¦æ±‚çš„æ–¹å¼æä¾›ã€‚
- en: In this article I will be using [Google Colab](https://colab.google/) to fine-tune
    the LLM. We will be using the [know_sql dataset](https://huggingface.co/datasets/knowrohit07/know_sql)
    (OpenRAIL license) that I mentioned previously. We will also be using the [axolotl
    framework](https://github.com/OpenAccess-AI-Collective/axolotl) to handle the
    fine-tuning process. They have some great documentation on their GitHub page.
    Rather than writing the ~100 lines of code to manually handle the fine-tuning
    process, axolotl allows us to simply edit a YAML config file for the respective
    model we are looking to fine-tune. I will be running through the exact process
    in this article but I would suggest reading through the axolotl documentation
    if anything is unclear.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘å°†ä½¿ç”¨[Google Colab](https://colab.google/)æ¥å¾®è°ƒLLMã€‚æˆ‘ä»¬å°†ä½¿ç”¨ä¹‹å‰æåˆ°çš„[know_sqlæ•°æ®é›†](https://huggingface.co/datasets/knowrohit07/know_sql)ï¼ˆOpenRAILè®¸å¯è¯ï¼‰ã€‚æˆ‘ä»¬è¿˜å°†ä½¿ç”¨[axolotlæ¡†æ¶](https://github.com/OpenAccess-AI-Collective/axolotl)æ¥å¤„ç†å¾®è°ƒè¿‡ç¨‹ã€‚ä»–ä»¬åœ¨GitHubé¡µé¢ä¸Šæœ‰ä¸€äº›å¾ˆå¥½çš„æ–‡æ¡£ã€‚ä¸å…¶ç¼–å†™çº¦100è¡Œä»£ç æ¥æ‰‹åŠ¨å¤„ç†å¾®è°ƒè¿‡ç¨‹ï¼Œaxolotlå…è®¸æˆ‘ä»¬ç®€å•åœ°ç¼–è¾‘ç›¸åº”æ¨¡å‹çš„YAMLé…ç½®æ–‡ä»¶ã€‚æˆ‘å°†åœ¨æœ¬æ–‡ä¸­è¯¦ç»†ä»‹ç»å…·ä½“è¿‡ç¨‹ï¼Œä½†å¦‚æœæœ‰ä»»ä½•ä¸æ¸…æ¥šçš„åœ°æ–¹ï¼Œå»ºè®®æŸ¥çœ‹axolotlæ–‡æ¡£ã€‚
- en: The process of fine-tuning an LLM is more computationally expensive than simply
    running inference on an existing LLM (again [here is the link](https://medium.com/towards-data-science/quantisation-and-co-reducing-inference-times-on-llms-by-80-671db9349bdb)
    to my previous article going over the various stages of running inference and
    optimising an existing LLM). As such, we are unable to use the free tier of Google
    Colab as the task requires [NVIDIAâ€™s Ampere GPU architecture](https://www.nvidia.com/en-us/data-center/ampere-architecture/)
    (or later). This architecture is available via the â€œA100 GPUâ€ runtime type on
    Google Colab. For further info on interacting with runtime types, I would suggest
    checking out my [previous article](https://medium.com/towards-data-science/quantisation-and-co-reducing-inference-times-on-llms-by-80-671db9349bdb).
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: å¾®è°ƒLLMçš„è¿‡ç¨‹æ¯”ä»…åœ¨ç°æœ‰LLMä¸Šè¿è¡Œæ¨ç†è¦è®¡ç®—å¯†é›†å¾—å¤šï¼ˆå†æ­¤æä¾›[é“¾æ¥](https://medium.com/towards-data-science/quantisation-and-co-reducing-inference-times-on-llms-by-80-671db9349bdb)åˆ°æˆ‘çš„ä¸Šä¸€ç¯‡æ–‡ç« ï¼Œæ¶µç›–äº†è¿è¡Œæ¨ç†å’Œä¼˜åŒ–ç°æœ‰LLMçš„å„ä¸ªé˜¶æ®µï¼‰ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æ— æ³•ä½¿ç”¨Google
    Colabçš„å…è´¹ç‰ˆï¼Œå› ä¸ºä»»åŠ¡éœ€è¦[NVIDIAçš„Ampere GPUæ¶æ„](https://www.nvidia.com/en-us/data-center/ampere-architecture/)ï¼ˆæˆ–æ›´é«˜ç‰ˆæœ¬ï¼‰ã€‚æ­¤æ¶æ„å¯é€šè¿‡Google
    Colabä¸Šçš„â€œA100 GPUâ€è¿è¡Œæ—¶ç±»å‹è·å¾—ã€‚æœ‰å…³ä¸è¿è¡Œæ—¶ç±»å‹äº¤äº’çš„æ›´å¤šä¿¡æ¯ï¼Œæˆ‘å»ºè®®æŸ¥çœ‹æˆ‘çš„[ä¸Šä¸€ç¯‡æ–‡ç« ](https://medium.com/towards-data-science/quantisation-and-co-reducing-inference-times-on-llms-by-80-671db9349bdb)ã€‚
- en: At this stage, I should mention that Google offers the A100 GPU on a first come,
    first served basis or â€œsubject to availabilityâ€. I have at times found it tedious
    to get access to the A100 GPUâ€™s as they do not often become available. If unavailable,
    you will be automatically switched to an older architecture that is not sufficient
    for completing this task. Your mileage may vary. If you do not wish to wait for
    the correct GPUs to become available, I would suggest exploring some of the hardware
    suggestions I made in my [previous article](https://medium.com/towards-data-science/quantisation-and-co-reducing-inference-times-on-llms-by-80-671db9349bdb).
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ä¸ªé˜¶æ®µï¼Œæˆ‘åº”è¯¥æåˆ°Googleæä¾›A100 GPUæ˜¯â€œå…ˆåˆ°å…ˆå¾—â€æˆ–â€œè§†æƒ…å†µè€Œå®šâ€ã€‚æˆ‘å‘ç°æœ‰æ—¶è·å–A100 GPUçš„è®¿é—®æƒé™æ¯”è¾ƒéº»çƒ¦ï¼Œå› ä¸ºå®ƒä»¬ä¸å¸¸æœ‰ã€‚å¦‚æœä¸å¯ç”¨ï¼Œä½ å°†è‡ªåŠ¨åˆ‡æ¢åˆ°ä¸€ä¸ªè¾ƒæ—§çš„æ¶æ„ï¼Œè¿™ä¸è¶³ä»¥å®Œæˆè¿™ä¸ªä»»åŠ¡ã€‚ä½ çš„ä½“éªŒå¯èƒ½ä¼šæœ‰æ‰€ä¸åŒã€‚å¦‚æœä½ ä¸æƒ³ç­‰å¾…æ­£ç¡®çš„GPUå˜å¾—å¯ç”¨ï¼Œæˆ‘å»ºè®®ä½ æŸ¥çœ‹æˆ‘åœ¨[ä¸Šä¸€ç¯‡æ–‡ç« ](https://medium.com/towards-data-science/quantisation-and-co-reducing-inference-times-on-llms-by-80-671db9349bdb)ä¸­æåˆ°çš„ä¸€äº›ç¡¬ä»¶å»ºè®®ã€‚
- en: 'The workflow will run as follows:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: å·¥ä½œæµç¨‹å¦‚ä¸‹ï¼š
- en: Â· Set up Google Colab and mount GDrive (we need to mount the GDrive as we need
    to access certain files from within the Colab notebook).
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: Â· è®¾ç½®Google Colabå¹¶æŒ‚è½½GDriveï¼ˆæˆ‘ä»¬éœ€è¦æŒ‚è½½GDriveï¼Œå› ä¸ºéœ€è¦ä»Colabç¬”è®°æœ¬ä¸­è®¿é—®æŸäº›æ–‡ä»¶ï¼‰ã€‚
- en: Â· Install dependencies
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: Â· å®‰è£…ä¾èµ–é¡¹
- en: Â· Authenticate via HuggingFace CLI (if you have never received authorisation
    from Meta to use the Llama-2 LLMs, I again suggest reviewing my [previous article](https://medium.com/towards-data-science/quantisation-and-co-reducing-inference-times-on-llms-by-80-671db9349bdb)
    that covers the process of gaining access).
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: Â· é€šè¿‡HuggingFace CLIè¿›è¡Œèº«ä»½éªŒè¯ï¼ˆå¦‚æœä½ ä»æœªè·å¾—Metaæˆæƒä½¿ç”¨Llama-2 LLMsï¼Œæˆ‘å†æ¬¡å»ºè®®ä½ æŸ¥çœ‹æˆ‘çš„[ä¸Šä¸€ç¯‡æ–‡ç« ](https://medium.com/towards-data-science/quantisation-and-co-reducing-inference-times-on-llms-by-80-671db9349bdb)ï¼Œè¯¥æ–‡ç« æ¶µç›–äº†è·å–è®¿é—®æƒé™çš„è¿‡ç¨‹ï¼‰ã€‚
- en: Â· Load dataset
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: Â· åŠ è½½æ•°æ®é›†
- en: Â· Clone axolotl repo into Gdrive
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: Â· å°†axolotlä»“åº“å…‹éš†åˆ°Gdrive
- en: Â· Update axolotl config file
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: Â· æ›´æ–°axolotlé…ç½®æ–‡ä»¶
- en: Â· Fine-tune Llama-7B model (this took about 2hrs. Again your mileage may vary).
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: Â· å¾®è°ƒLlama-7Bæ¨¡å‹ï¼ˆè¿™å¤§çº¦éœ€è¦2å°æ—¶ã€‚ä½ çš„ä½“éªŒå¯èƒ½ä¼šæœ‰æ‰€ä¸åŒï¼‰ã€‚
- en: Â· Run inference on fine-tuned model
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: Â· åœ¨å¾®è°ƒåçš„æ¨¡å‹ä¸Šè¿è¡Œæ¨ç†
- en: Before jumping into the code, I wish to clarify exactly what fine-tuning is
    and when you may want to consider fine-tuning your LLM.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿›å…¥ä»£ç ä¹‹å‰ï¼Œæˆ‘å¸Œæœ›æ¾„æ¸…å¾®è°ƒåˆ°åº•æ˜¯ä»€ä¹ˆï¼Œä»¥åŠä½ å¯èƒ½ä½•æ—¶è€ƒè™‘å¾®è°ƒä½ çš„ LLMã€‚
- en: Fine-tuning can be carried out in a variety of ways. The method you choose comes
    down to your available resources and the task you want to achieve. It could be
    that a â€˜baseâ€™ model yields satisfactory results, in which case you can skip the
    process entirely.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: å¾®è°ƒå¯ä»¥é€šè¿‡å¤šç§æ–¹å¼è¿›è¡Œã€‚ä½ é€‰æ‹©çš„æ–¹æ³•å–å†³äºä½ å¯ç”¨çš„èµ„æºå’Œä½ æƒ³è¦å®ç°çš„ä»»åŠ¡ã€‚å¯èƒ½â€˜åŸºç¡€â€™æ¨¡å‹å·²ç»èƒ½äº§ç”Ÿä»¤äººæ»¡æ„çš„ç»“æœï¼Œè¿™ç§æƒ…å†µä¸‹ä½ å¯ä»¥å®Œå…¨è·³è¿‡å¾®è°ƒè¿‡ç¨‹ã€‚
- en: 'Again, my [previous article](https://medium.com/towards-data-science/quantisation-and-co-reducing-inference-times-on-llms-by-80-671db9349bdb)
    goes over why you may want to use an open-source LLM rather than offerings such
    as OpenAIâ€™s ChatGPT. A general rule of thumb I use when considering if I should
    fine-tune an LLM for my task is as follows:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: å†æ¬¡ï¼Œæˆ‘çš„ [ä¸Šä¸€ç¯‡æ–‡ç« ](https://medium.com/towards-data-science/quantisation-and-co-reducing-inference-times-on-llms-by-80-671db9349bdb)
    è®¨è®ºäº†ä¸ºä»€ä¹ˆä½ å¯èƒ½ä¼šé€‰æ‹©ä½¿ç”¨å¼€æºçš„ LLMï¼Œè€Œä¸æ˜¯åƒ OpenAI çš„ ChatGPT è¿™æ ·çš„äº§å“ã€‚è€ƒè™‘æ˜¯å¦åº”å¾®è°ƒ LLM çš„ä¸€ä¸ªé€šç”¨åŸåˆ™å¦‚ä¸‹ï¼š
- en: Â· Does my task require domain specific knowledge/expertise?
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: Â· æˆ‘çš„ä»»åŠ¡æ˜¯å¦éœ€è¦é¢†åŸŸç‰¹å®šçš„çŸ¥è¯†/ä¸“ä¸šæŠ€èƒ½ï¼Ÿ
- en: Â· Will I be feeding proprietary data to my LLM?
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: Â· æˆ‘æ˜¯å¦ä¼šå‘æˆ‘çš„ LLM è¾“å…¥ä¸“æœ‰æ•°æ®ï¼Ÿ
- en: Â· Do I have a significant amount of domain specific data to train/tune my model?
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: Â· æˆ‘æ˜¯å¦æœ‰è¶³å¤Ÿçš„é¢†åŸŸç‰¹å®šæ•°æ®æ¥è®­ç»ƒ/è°ƒæ•´æˆ‘çš„æ¨¡å‹ï¼Ÿ
- en: Â· Do I have the time and compute resources available to complete the task?
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: Â· æˆ‘æ˜¯å¦æœ‰è¶³å¤Ÿçš„æ—¶é—´å’Œè®¡ç®—èµ„æºæ¥å®Œæˆä»»åŠ¡ï¼Ÿ
- en: 'If you answered yes to one or more of the above questions, you should consider
    fine-tuning an LLM. If you answered yes to all of the above questions, it may
    be worth training an LLM from scratch. It may be worth using a step-by-step approach:
    initially use a â€˜baseâ€™ LLM, then try a fine-tuned LLM, then try an LLM that you
    have trained from scratch. Each step becomes increasingly expensive (in terms
    of time and compute requirements), where you would only move to the next step
    if your current LLM does not yield satisfactory results. The process of training
    an LLM from scratch is outside the scope of this article but Iâ€™m happy to answer
    any questions you have on the topic.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœä½ å¯¹ä¸€ä¸ªæˆ–å¤šä¸ªä»¥ä¸Šçš„é—®é¢˜å›ç­”æ˜¯ï¼Œä½ åº”è¯¥è€ƒè™‘å¾®è°ƒ LLMã€‚å¦‚æœä½ å¯¹æ‰€æœ‰é—®é¢˜éƒ½å›ç­”æ˜¯ï¼Œå¯èƒ½å€¼å¾—ä»å¤´å¼€å§‹è®­ç»ƒä¸€ä¸ª LLMã€‚å¯ä»¥é‡‡ç”¨é€æ­¥çš„æ–¹æ³•ï¼šåˆå§‹ä½¿ç”¨ä¸€ä¸ªâ€˜åŸºç¡€â€™
    LLMï¼Œç„¶åå°è¯•ä¸€ä¸ªå¾®è°ƒè¿‡çš„ LLMï¼Œå†å°è¯•ä¸€ä¸ªä½ ä»å¤´è®­ç»ƒçš„ LLMã€‚æ¯ä¸€æ­¥çš„æˆæœ¬ï¼ˆæ—¶é—´å’Œè®¡ç®—èµ„æºï¼‰é€æ¸å¢åŠ ï¼Œåªæœ‰å½“ä½ å½“å‰çš„ LLM æœªèƒ½äº§ç”Ÿä»¤äººæ»¡æ„çš„ç»“æœæ—¶ï¼Œæ‰ä¼šè¿›å…¥ä¸‹ä¸€æ­¥ã€‚è®­ç»ƒ
    LLM ä»é›¶å¼€å§‹çš„è¿‡ç¨‹è¶…å‡ºäº†æœ¬æ–‡çš„èŒƒå›´ï¼Œä½†æˆ‘å¾ˆä¹æ„å›ç­”ä½ å…³äºè¿™ä¸ªè¯é¢˜çš„ä»»ä½•é—®é¢˜ã€‚
- en: Now that we have decided whether to fine-tune our LLM, Iâ€™ll cover what the process
    actually entails.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬å·²ç»å†³å®šæ˜¯å¦å¾®è°ƒæˆ‘ä»¬çš„ LLMï¼Œæˆ‘å°†è¯¦ç»†ä»‹ç»è¿™ä¸ªè¿‡ç¨‹å®é™…ä¸ŠåŒ…æ‹¬ä»€ä¹ˆã€‚
- en: Our â€˜baseâ€™ LLM is a pre-trained model that has an existing set of weights and
    biases from its initial training. These weights and biases allow the model to
    have â€˜learnedâ€™ a broad range of basic language patterns and general knowledge.
    In the fine-tuning process we are updating these weights and biases using our
    labelled dataset.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬çš„â€˜åŸºç¡€â€™ LLM æ˜¯ä¸€ä¸ªé¢„è®­ç»ƒæ¨¡å‹ï¼Œæ‹¥æœ‰æ¥è‡ªåˆå§‹è®­ç»ƒçš„ä¸€ç»„æƒé‡å’Œåç½®ã€‚è¿™äº›æƒé‡å’Œåç½®ä½¿å¾—æ¨¡å‹èƒ½å¤Ÿâ€˜å­¦ä¹ â€™åˆ°å¹¿æ³›çš„åŸºæœ¬è¯­è¨€æ¨¡å¼å’Œä¸€èˆ¬çŸ¥è¯†ã€‚åœ¨å¾®è°ƒè¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨æ ‡è®°çš„æ•°æ®é›†æ¥æ›´æ–°è¿™äº›æƒé‡å’Œåç½®ã€‚
- en: 'Previously, a typical fine-tuning process looked like this:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: ä¹‹å‰ï¼Œå…¸å‹çš„å¾®è°ƒè¿‡ç¨‹å¦‚ä¸‹ï¼š
- en: '**Forward Pass**'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '**å‰å‘ä¼ æ’­**'
- en: Â· The model tries to make predictions based on input data
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: Â· æ¨¡å‹å°è¯•åŸºäºè¾“å…¥æ•°æ®è¿›è¡Œé¢„æµ‹
- en: '**Loss calculation** Â· The model calculates how far off the predictions were.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '**æŸå¤±è®¡ç®—** Â· æ¨¡å‹è®¡ç®—é¢„æµ‹ç»“æœçš„åå·®ã€‚'
- en: '**Backward Pass (backpropagation)** Â· The model is essentially figuring out
    how much each of the modelâ€™s parameters contributed to the loss.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '**åå‘ä¼ æ’­** Â· æ¨¡å‹åŸºæœ¬ä¸Šæ˜¯åœ¨å¼„æ¸…æ¥šæ¯ä¸ªå‚æ•°å¯¹æŸå¤±çš„è´¡çŒ®ç¨‹åº¦ã€‚'
- en: '**Update Parameters** Â· In this step, the parameters will be updated and adjusted
    to reduce the loss. The size of the update (learning rate) is a hyperparameter
    that can be adjusted.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '**æ›´æ–°å‚æ•°** Â· åœ¨è¿™ä¸€æ­¥ï¼Œå‚æ•°å°†è¢«æ›´æ–°å’Œè°ƒæ•´ä»¥å‡å°‘æŸå¤±ã€‚æ›´æ–°çš„å¤§å°ï¼ˆå­¦ä¹ ç‡ï¼‰æ˜¯ä¸€ä¸ªå¯ä»¥è°ƒæ•´çš„è¶…å‚æ•°ã€‚'
- en: '**Repeat** Â· The above steps are repeated for a given number of iterations
    (epochs). Often this is very low. Common LLMâ€™s such as BERT perform best after
    a fine-tuning of 2 epochs. In this article we will only be using 1 epoch to fine-tune
    a Llama2â€“7B model.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '**é‡å¤** Â· ä¸Šè¿°æ­¥éª¤ä¼šé‡å¤è¿›è¡Œä¸€å®šæ¬¡æ•°ï¼ˆè®­ç»ƒè½®æ¬¡ï¼‰ã€‚é€šå¸¸è¿™ä¸ªæ¬¡æ•°éå¸¸å°‘ã€‚åƒ BERT è¿™æ ·çš„å¸¸è§ LLM åœ¨ç»è¿‡ 2 ä¸ªè½®æ¬¡çš„å¾®è°ƒåè¡¨ç°æœ€ä½³ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å°†ä»…ä½¿ç”¨
    1 ä¸ªè½®æ¬¡æ¥å¾®è°ƒ Llama2â€“7B æ¨¡å‹ã€‚'
- en: With the advent of very large LLMs, new strategies have been identified to make
    this process much more efficient. Introducing LoRA (Low Rank Adaptation) and QLoRA
    (Quantised LoRA). Weâ€™ll be using LoRA in our SQL example later on so Iâ€™ll concentrate
    on that. Both strategies can be grouped under the umbrella of Parameter-Efficient
    Fine-Tuning (PEFT). The techniques allow us to successfully fine-tune very large
    language models but also minimise the computational requirements to do so. It
    does this by reducing the number of trainable parameters during the tuning process.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: éšç€éå¸¸å¤§å‹ LLM çš„å‡ºç°ï¼Œå·²ç»ç¡®å®šäº†ä½¿è¿™ä¸€è¿‡ç¨‹æ›´åŠ é«˜æ•ˆçš„æ–°ç­–ç•¥ã€‚å¼•å…¥ LoRAï¼ˆä½ç§©é€‚é…ï¼‰å’Œ QLoRAï¼ˆé‡åŒ– LoRAï¼‰ã€‚æˆ‘ä»¬å°†åœ¨åç»­çš„ SQL
    ç¤ºä¾‹ä¸­ä½¿ç”¨ LoRAï¼Œæ‰€ä»¥æˆ‘å°†é‡ç‚¹ä»‹ç»å®ƒã€‚è¿™ä¸¤ç§ç­–ç•¥å¯ä»¥å½’å…¥å‚æ•°é«˜æ•ˆå¾®è°ƒï¼ˆPEFTï¼‰çš„èŒƒç•´ã€‚è¿™äº›æŠ€æœ¯ä½¿æˆ‘ä»¬èƒ½å¤ŸæˆåŠŸåœ°å¾®è°ƒéå¸¸å¤§çš„è¯­è¨€æ¨¡å‹ï¼ŒåŒæ—¶æœ€å°åŒ–æ‰§è¡Œæ‰€éœ€çš„è®¡ç®—è¦æ±‚ã€‚å®ƒé€šè¿‡å‡å°‘å¾®è°ƒè¿‡ç¨‹ä¸­çš„å¯è®­ç»ƒå‚æ•°æ•°é‡æ¥å®ç°è¿™ä¸€ç‚¹ã€‚
- en: Whilst reduced memory usage, storage cost and inference latency are benefits
    of using a PEFT approach, it is important to note the training time may be increased
    during this process and performance can be more sensitive to hyperparameter choices.
  id: totrans-45
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: å°½ç®¡å‡å°‘å†…å­˜ä½¿ç”¨ã€å­˜å‚¨æˆæœ¬å’Œæ¨ç†å»¶è¿Ÿæ˜¯ä½¿ç”¨ PEFT æ–¹æ³•çš„å¥½å¤„ï¼Œä½†éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œåœ¨æ­¤è¿‡ç¨‹ä¸­è®­ç»ƒæ—¶é—´å¯èƒ½ä¼šå¢åŠ ï¼Œæ€§èƒ½å¯èƒ½å¯¹è¶…å‚æ•°é€‰æ‹©æ›´åŠ æ•æ„Ÿã€‚
- en: There are various PEFT techniques available to us. For further reading on these
    techniques, I highly recommend [this blog post](https://www.leewayhertz.com/parameter-efficient-fine-tuning/).
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥ä½¿ç”¨å„ç§ PEFT æŠ€æœ¯ã€‚æœ‰å…³è¿™äº›æŠ€æœ¯çš„æ›´å¤šé˜…è¯»ï¼Œæˆ‘å¼ºçƒˆæ¨è[è¿™ç¯‡åšå®¢æ–‡ç« ](https://www.leewayhertz.com/parameter-efficient-fine-tuning/)ã€‚
- en: In using LoRA, an assumption is made that existing modelâ€™s weights are close
    to the optimum solution. These weights are then frozen so they can no longer be
    updated in the fine-tuning process. Instead, low-rank matrices are introduced
    into each layer of our LLM. These matrices act as adapters that replace the trainable
    parameters we froze. The computational requirements for updating these low-rank
    matrices are significantly lower than if we were still using the original parameters.
    In other words, the original parameters can be approximated well by the low-rank
    matrices due to redundancy in the original parameters and correlation between
    the original parameters and low-rank matrices. This is identified in what is called
    â€˜Rank-Deficiencyâ€™ â€” the paper to which can be found [here](https://arxiv.org/abs/2106.09685).
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨ LoRA æ—¶ï¼Œå‡è®¾ç°æœ‰æ¨¡å‹çš„æƒé‡æ¥è¿‘æœ€ä¼˜è§£ã€‚è¿™äº›æƒé‡éšåè¢«å›ºå®šï¼Œå› æ­¤åœ¨å¾®è°ƒè¿‡ç¨‹ä¸­ä¸èƒ½å†æ›´æ–°ã€‚ç›¸åï¼Œä½ç§©çŸ©é˜µè¢«å¼•å…¥åˆ°æˆ‘ä»¬çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¯ä¸€å±‚ä¸­ã€‚è¿™äº›çŸ©é˜µä½œä¸ºé€‚é…å™¨ï¼Œæ›¿ä»£äº†æˆ‘ä»¬å†»ç»“çš„å¯è®­ç»ƒå‚æ•°ã€‚æ›´æ–°è¿™äº›ä½ç§©çŸ©é˜µçš„è®¡ç®—è¦æ±‚æ˜¾è‘—ä½äºå¦‚æœæˆ‘ä»¬ä»ä½¿ç”¨åŸå§‹å‚æ•°æ—¶çš„è®¡ç®—è¦æ±‚ã€‚æ¢å¥è¯è¯´ï¼Œç”±äºåŸå§‹å‚æ•°ä¸­çš„å†—ä½™ä»¥åŠåŸå§‹å‚æ•°ä¸ä½ç§©çŸ©é˜µä¹‹é—´çš„ç›¸å…³æ€§ï¼ŒåŸå§‹å‚æ•°å¯ä»¥è¢«ä½ç§©çŸ©é˜µå¾ˆå¥½åœ°è¿‘ä¼¼ã€‚è¿™åœ¨æ‰€è°“çš„â€˜Rank-Deficiencyâ€™ä¸­å¾—åˆ°äº†ä½“ç°â€”â€”ç›¸å…³è®ºæ–‡å¯ä»¥åœ¨[è¿™é‡Œ](https://arxiv.org/abs/2106.09685)æ‰¾åˆ°ã€‚
- en: All this results in reduced parameter overhead which leads to the advantages
    I listed earlier. A big one to note is the reduced inference latency. We are essentially
    creating a linear design by introducing these low-rank matrices into our model.
    The inference latency is significantly reduced when compared to a fully fine-tuned
    model. This makes the approach suitable for real-time applications.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€æœ‰è¿™äº›ç»“æœå‡å°‘äº†å‚æ•°å¼€é”€ï¼Œä»è€Œå¸¦æ¥äº†æˆ‘ä¹‹å‰åˆ—å‡ºçš„ä¼˜åŠ¿ã€‚å€¼å¾—æ³¨æ„çš„ä¸€ç‚¹æ˜¯å‡å°‘äº†æ¨ç†å»¶è¿Ÿã€‚é€šè¿‡å°†è¿™äº›ä½ç§©çŸ©é˜µå¼•å…¥æˆ‘ä»¬çš„æ¨¡å‹ï¼Œæˆ‘ä»¬å®é™…ä¸Šåˆ›å»ºäº†ä¸€ä¸ªçº¿æ€§è®¾è®¡ã€‚ä¸å®Œå…¨å¾®è°ƒçš„æ¨¡å‹ç›¸æ¯”ï¼Œæ¨ç†å»¶è¿Ÿæ˜¾è‘—å‡å°‘ã€‚è¿™ä½¿å¾—è¯¥æ–¹æ³•é€‚ç”¨äºå®æ—¶åº”ç”¨ã€‚
- en: We can further optimise inference times by introducing quantisation â€” a topic
    I covered extensively in my [previous article](https://medium.com/towards-data-science/quantisation-and-co-reducing-inference-times-on-llms-by-80-671db9349bdb).
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥é€šè¿‡å¼•å…¥é‡åŒ–è¿›ä¸€æ­¥ä¼˜åŒ–æ¨ç†æ—¶é—´â€”â€”è¿™æ˜¯æˆ‘åœ¨[ä¸Šä¸€ç¯‡æ–‡ç« ](https://medium.com/towards-data-science/quantisation-and-co-reducing-inference-times-on-llms-by-80-671db9349bdb)ä¸­å¹¿æ³›è®¨è®ºçš„ä¸»é¢˜ã€‚
- en: '[Axolotl](https://github.com/OpenAccess-AI-Collective/axolotl) provides a very
    easy to use tool that allows us to implement all the above features.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '[Axolotl](https://github.com/OpenAccess-AI-Collective/axolotl)æä¾›äº†ä¸€ä¸ªéå¸¸æ˜“äºä½¿ç”¨çš„å·¥å…·ï¼Œä½¿æˆ‘ä»¬èƒ½å¤Ÿå®ç°ä¸Šè¿°æ‰€æœ‰åŠŸèƒ½ã€‚'
- en: Letâ€™s get to it.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬å¼€å§‹å§ã€‚
- en: First weâ€™ll set up our [colab notebook](https://colab.google/). You now want
    to select the A100 GPU runtime type. If you are running the free version, you
    will need to upgrade to a paid tier. Please note, the A100 GPUs may be unavailable.
    Iâ€™d advise running a cell with a â€œhello worldâ€ example to ensure you donâ€™t get
    moved to an older V100 GPU. If the A100 GPU is unavailable, you can either wait
    for one to become available (I unfortunately could not establish peak and off-peak
    times for when GPU availability was high). Or you can explore the other hardware
    options listed in my [previous article](https://medium.com/towards-data-science/quantisation-and-co-reducing-inference-times-on-llms-by-80-671db9349bdb).
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: é¦–å…ˆæˆ‘ä»¬å°†è®¾ç½®æˆ‘ä»¬çš„ [colab notebook](https://colab.google/)ã€‚ä½ ç°åœ¨éœ€è¦é€‰æ‹© A100 GPU è¿è¡Œæ—¶ç±»å‹ã€‚å¦‚æœä½ ä½¿ç”¨çš„æ˜¯å…è´¹ç‰ˆæœ¬ï¼Œä½ éœ€è¦å‡çº§åˆ°ä»˜è´¹ç‰ˆæœ¬ã€‚è¯·æ³¨æ„ï¼ŒA100
    GPUs å¯èƒ½ä¸å¯ç”¨ã€‚æˆ‘å»ºè®®è¿è¡Œä¸€ä¸ªåŒ…å«â€œhello worldâ€ç¤ºä¾‹çš„å•å…ƒï¼Œä»¥ç¡®ä¿ä½ ä¸ä¼šè¢«è½¬ç§»åˆ°è¾ƒæ—§çš„ V100 GPUã€‚å¦‚æœ A100 GPU ä¸å¯ç”¨ï¼Œä½ å¯ä»¥ç­‰å¾…ä¸€ä¸ªå¯ç”¨çš„
    GPUï¼ˆæˆ‘ä¸å¹¸çš„æ˜¯æ— æ³•ç¡®å®š GPU å¯ç”¨æ€§çš„é«˜å³°å’Œä½å³°æ—¶é—´ï¼‰ã€‚æˆ–è€…ä½ å¯ä»¥æŸ¥çœ‹æˆ‘[ä¸Šä¸€ç¯‡æ–‡ç« ](https://medium.com/towards-data-science/quantisation-and-co-reducing-inference-times-on-llms-by-80-671db9349bdb)ä¸­åˆ—å‡ºçš„å…¶ä»–ç¡¬ä»¶é€‰é¡¹ã€‚
- en: 'Next letâ€™s mount our Gdrive. You can do so by entering the following code:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥ä¸‹æ¥è®©æˆ‘ä»¬æŒ‚è½½ Gdriveã€‚ä½ å¯ä»¥é€šè¿‡è¾“å…¥ä»¥ä¸‹ä»£ç æ¥å®Œæˆï¼š
- en: '[PRE2]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: You will then receive a pop-up where you have to authenticate via your google
    account. We are mounting the drive as we need to work with files that weâ€™ll be
    storing within our Gdrive.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åä½ ä¼šæ”¶åˆ°ä¸€ä¸ªå¼¹å‡ºçª—å£ï¼Œéœ€è¦é€šè¿‡ä½ çš„ Google è´¦æˆ·è¿›è¡Œèº«ä»½éªŒè¯ã€‚æˆ‘ä»¬æ­£åœ¨æŒ‚è½½é©±åŠ¨å™¨ï¼Œå› ä¸ºæˆ‘ä»¬éœ€è¦å¤„ç†å­˜å‚¨åœ¨ Gdrive ä¸­çš„æ–‡ä»¶ã€‚
- en: Next, weâ€™ll authenticate our access to the Llama2â€“7B via the HuggingFace CLI.
    This will be our base model that we will fine-tune. Again, if you have never before
    requested access to the model via Metaâ€™s official page, please follow instructions
    in my previous article.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å°†é€šè¿‡ HuggingFace CLI è¿›è¡Œ Llama2â€“7B çš„è®¿é—®è®¤è¯ã€‚è¿™å°†æ˜¯æˆ‘ä»¬å°†è¦å¾®è°ƒçš„åŸºç¡€æ¨¡å‹ã€‚å¦‚æœä½ ä¹‹å‰ä»æœªé€šè¿‡ Meta çš„å®˜æ–¹ç½‘ç«™ç”³è¯·è¿‡è¯¥æ¨¡å‹çš„è®¿é—®æƒé™ï¼Œè¯·å‚é˜…æˆ‘ä¹‹å‰çš„æ–‡ç« ä¸­çš„è¯´æ˜ã€‚
- en: 'To authenticate, enter the following code in a new cell:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: è¦è¿›è¡Œè®¤è¯ï¼Œè¯·åœ¨ä¸€ä¸ªæ–°å•å…ƒä¸­è¾“å…¥ä»¥ä¸‹ä»£ç ï¼š
- en: '[PRE3]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: You will then be asked to enter your access key (I explain how to access this
    key in my [previous article](https://medium.com/towards-data-science/quantisation-and-co-reducing-inference-times-on-llms-by-80-671db9349bdb)).
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åç³»ç»Ÿä¼šè¦æ±‚ä½ è¾“å…¥è®¿é—®å¯†é’¥ï¼ˆæˆ‘åœ¨æˆ‘çš„[ä¸Šä¸€ç¯‡æ–‡ç« ](https://medium.com/towards-data-science/quantisation-and-co-reducing-inference-times-on-llms-by-80-671db9349bdb)ä¸­è§£é‡Šäº†å¦‚ä½•è·å–æ­¤å¯†é’¥ï¼‰ã€‚
- en: Next, weâ€™ll handle our installs. I have found this to often be the biggest hurdle
    in running experiments with LLMs. Certain packages are built to run on specific
    hardware, if you do not have that specific hardware, it can be tedious to unpick
    your installs and update as required. Thankfully the axolotl error outputs are
    very descriptive so you should be able to track down any compatibility issues
    fairly easily. If you follow the instructions on the axolotl readme you should
    be good to go. Let me know if you get stuck.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å°†å¤„ç†æˆ‘ä»¬çš„å®‰è£…ã€‚æˆ‘å‘ç°è¿™é€šå¸¸æ˜¯è¿è¡Œ LLM å®éªŒæ—¶æœ€å¤§çš„éšœç¢ã€‚æŸäº›åŒ…æ˜¯ä¸ºç‰¹å®šç¡¬ä»¶æ„å»ºçš„ï¼Œå¦‚æœä½ æ²¡æœ‰è¯¥ç‰¹å®šç¡¬ä»¶ï¼Œå¯èƒ½ä¼šå¾ˆéº»çƒ¦åœ°å¤„ç†å®‰è£…å’Œæ›´æ–°ã€‚å¹¸è¿çš„æ˜¯ï¼Œaxolotl
    é”™è¯¯è¾“å‡ºéå¸¸è¯¦ç»†ï¼Œä½ åº”è¯¥èƒ½å¤Ÿç›¸å¯¹å®¹æ˜“åœ°è·Ÿè¸ªä»»ä½•å…¼å®¹æ€§é—®é¢˜ã€‚å¦‚æœä½ æŒ‰ç…§ axolotl readme ä¸­çš„è¯´æ˜æ“ä½œï¼Œä½ åº”è¯¥å¯ä»¥é¡ºåˆ©è¿›è¡Œã€‚å¦‚æœä½ é‡åˆ°å›°éš¾ï¼Œè¯·å‘Šè¯‰æˆ‘ã€‚
- en: 'Lets run the installs, weâ€™ll be installing the correct version of the cuda
    drivers as well as dependencies for the axolotl library. Weâ€™ll also be installing
    HuggingFaceâ€™s datasets library to gain access to our SQL training set:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬è¿è¡Œå®‰è£…ï¼Œæˆ‘ä»¬å°†å®‰è£…æ­£ç¡®ç‰ˆæœ¬çš„ CUDA é©±åŠ¨ç¨‹åºä»¥åŠ axolotl åº“çš„ä¾èµ–é¡¹ã€‚æˆ‘ä»¬è¿˜å°†å®‰è£… HuggingFace çš„æ•°æ®é›†åº“ï¼Œä»¥è®¿é—®æˆ‘ä»¬çš„
    SQL è®­ç»ƒé›†ï¼š
- en: '[PRE4]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Next weâ€™ll clone the axolotl repo into our Gdrive. If you get errors stating
    the directories do not exist, simply create them using the Gdrive UI:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥ä¸‹æ¥æˆ‘ä»¬å°†æŠŠ axolotl ä»“åº“å…‹éš†åˆ°æˆ‘ä»¬çš„ Gdriveã€‚å¦‚æœå‡ºç°ç›®å½•ä¸å­˜åœ¨çš„é”™è¯¯ï¼Œåªéœ€ä½¿ç”¨ Gdrive UI åˆ›å»ºè¿™äº›ç›®å½•å³å¯ï¼š
- en: '[PRE5]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Run some further installs:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: è¿è¡Œä¸€äº›è¿›ä¸€æ­¥çš„å®‰è£…ï¼š
- en: '[PRE6]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Next weâ€™ll grab our dataset and check it is loaded correctly:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥ä¸‹æ¥æˆ‘ä»¬å°†è·å–æ•°æ®é›†å¹¶æ£€æŸ¥å…¶æ˜¯å¦æ­£ç¡®åŠ è½½ï¼š
- en: '[PRE7]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Great, at this point we have installed all dependencies, weâ€™ve cloned the axolotl
    repo, and are authorised to access the Llama-2 7B model. At this stage we need
    to do some further configuration in the form of updating our YAML file. Axolotl
    uses this YAML file as a set of instructions for how we wish to fine-tune our
    model. Iâ€™d suggest looking through some of the examples provided by axolotl within
    the axolotl directory that was created when you cloned the repo. It will give
    you a feel for what settings we can change and what hyperparameters we can work
    with. Again, the readme of the repo is very helpful here.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: å¾ˆå¥½ï¼Œç›®å‰æˆ‘ä»¬å·²ç»å®‰è£…äº†æ‰€æœ‰ä¾èµ–é¡¹ï¼Œå…‹éš†äº† axolotl ä»“åº“ï¼Œå¹¶è·å¾—äº†è®¿é—® Llama-2 7B æ¨¡å‹çš„æˆæƒã€‚åœ¨è¿™ä¸ªé˜¶æ®µï¼Œæˆ‘ä»¬éœ€è¦è¿›ä¸€æ­¥é…ç½®ï¼Œæ›´æ–°
    YAML æ–‡ä»¶ã€‚Axolotl ä½¿ç”¨è¿™ä¸ª YAML æ–‡ä»¶ä½œä¸ºå¾®è°ƒæ¨¡å‹çš„æŒ‡ä»¤é›†ã€‚æˆ‘å»ºè®®æŸ¥çœ‹ä¸€äº› Axolotl æä¾›çš„ç¤ºä¾‹ï¼Œè¿™äº›ç¤ºä¾‹ä½äºå…‹éš†ä»“åº“æ—¶åˆ›å»ºçš„ axolotl
    ç›®å½•ä¸­ã€‚è¿™å°†å¸®åŠ©ä½ äº†è§£å¯ä»¥æ›´æ”¹å“ªäº›è®¾ç½®ä»¥åŠå¯ä»¥ä½¿ç”¨å“ªäº›è¶…å‚æ•°ã€‚åŒæ ·ï¼Œä»“åº“çš„ readme å¯¹è¿™é‡Œéå¸¸æœ‰å¸®åŠ©ã€‚
- en: 'Below is a copy of my final YAML config file:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: ä»¥ä¸‹æ˜¯æˆ‘æœ€ç»ˆçš„ YAML é…ç½®æ–‡ä»¶å‰¯æœ¬ï¼š
- en: '[PRE8]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Youâ€™ll notice we are able to update parameters that were discussed in my previous
    article (fp16: true) as a means of optimising the training process and limiting
    memory requirements for the tuning of this model. If you are using different hardware,
    Iâ€™d suggest reading through the docs and checking any error messages you get from
    when we initialise the tuning process later on. Depending on your goal, axolotl
    has lots of example config files for you to use and adapt.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 'ä½ ä¼šæ³¨æ„åˆ°æˆ‘ä»¬å¯ä»¥æ›´æ–°ä¹‹å‰æ–‡ç« ä¸­è®¨è®ºçš„å‚æ•°ï¼ˆfp16: trueï¼‰ï¼Œä»¥ä¼˜åŒ–è®­ç»ƒè¿‡ç¨‹å¹¶é™åˆ¶è¯¥æ¨¡å‹å¾®è°ƒæ‰€éœ€çš„å†…å­˜ã€‚å¦‚æœä½ ä½¿ç”¨çš„æ˜¯ä¸åŒçš„ç¡¬ä»¶ï¼Œå»ºè®®æŸ¥çœ‹æ–‡æ¡£å’Œåˆå§‹åŒ–å¾®è°ƒè¿‡ç¨‹æ—¶çš„ä»»ä½•é”™è¯¯ä¿¡æ¯ã€‚æ ¹æ®ä½ çš„ç›®æ ‡ï¼Œaxolotl
    æä¾›äº†è®¸å¤šç¤ºä¾‹é…ç½®æ–‡ä»¶ä¾›ä½ ä½¿ç”¨å’Œè°ƒæ•´ã€‚'
- en: 'Save the config file into the fine_tune_llm directory that you created earlier
    as *sql.yml*. A further python script is required for handling tokenisation strategies.
    This should also be saved in your fine_tune_llm directory as *context_qa2.py*.
    Here is the script:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: å°†é…ç½®æ–‡ä»¶ä¿å­˜åˆ°ä¹‹å‰åˆ›å»ºçš„ fine_tune_llm ç›®å½•ä¸­ï¼Œå‘½åä¸º *sql.yml*ã€‚è¿˜éœ€è¦ä¸€ä¸ªé¢å¤–çš„ Python è„šæœ¬æ¥å¤„ç†åˆ†è¯ç­–ç•¥ã€‚è¿™ä¸ªè„šæœ¬ä¹Ÿåº”è¯¥ä¿å­˜åœ¨
    fine_tune_llm ç›®å½•ä¸­ï¼Œå‘½åä¸º *context_qa2.py*ã€‚è¿™æ˜¯è„šæœ¬ï¼š
- en: '[PRE9]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Great â€” now we should have everything ready to initialise the fine-tuning process.
    Our dependencies are installed, our config files are in their respective places
    in our Gdrive. You should have a fine_tune_llm folder that looks like this:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: å¾ˆå¥½â€”â€”ç°åœ¨æˆ‘ä»¬åº”è¯¥å·²ç»å‡†å¤‡å¥½åˆå§‹åŒ–å¾®è°ƒè¿‡ç¨‹ã€‚æˆ‘ä»¬çš„ä¾èµ–é¡¹å·²ç»å®‰è£…å¥½ï¼Œé…ç½®æ–‡ä»¶ä¹Ÿéƒ½åœ¨ Gdrive çš„å„è‡ªä½ç½®ä¸­ã€‚ä½ åº”è¯¥æœ‰ä¸€ä¸ªçœ‹èµ·æ¥åƒè¿™æ ·çš„ fine_tune_llm
    æ–‡ä»¶å¤¹ï¼š
- en: '![](../Images/7b32d36201feb8026a300dcb86f28e5c.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7b32d36201feb8026a300dcb86f28e5c.png)'
- en: Directory layout for fine-tuning LLama2â€“7B using Axolotl
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨ Axolotl å¾®è°ƒ LLama2â€“7B çš„ç›®å½•å¸ƒå±€
- en: The *.yml* file is our config file. The *.py* file is our script that handles
    tokenisation. The axolotl directory is the one we cloned from the repo earlier.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '*.yml* æ–‡ä»¶æ˜¯æˆ‘ä»¬çš„é…ç½®æ–‡ä»¶ã€‚*.py* æ–‡ä»¶æ˜¯å¤„ç†åˆ†è¯çš„è„šæœ¬ã€‚axolotl ç›®å½•æ˜¯æˆ‘ä»¬ä¹‹å‰ä»ä»“åº“å…‹éš†çš„ç›®å½•ã€‚'
- en: 'Now all we need to do is run:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬éœ€è¦åšçš„å°±æ˜¯è¿è¡Œï¼š
- en: '[PRE10]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'This will initialise the process of fine-tuning our specified model using the
    *sql.yml* config file. This process took circa 2hrs for me. Your mileage may vary.
    If you receive any errors at this stage, it will most likely be due to dependency
    errors. I ran into an issue a few times where I had to manually install the correct
    cuda drivers and flash_attn again:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å°†åˆå§‹åŒ–ä½¿ç”¨ *sql.yml* é…ç½®æ–‡ä»¶æ¥å¾®è°ƒæˆ‘ä»¬æŒ‡å®šçš„æ¨¡å‹ã€‚è¿™ä¸ªè¿‡ç¨‹å¯¹æˆ‘æ¥è¯´å¤§çº¦èŠ±äº† 2 å°æ—¶ã€‚ä½ çš„æƒ…å†µå¯èƒ½ä¼šæœ‰æ‰€ä¸åŒã€‚å¦‚æœæ­¤é˜¶æ®µå‡ºç°ä»»ä½•é”™è¯¯ï¼Œå¾ˆå¯èƒ½æ˜¯ç”±äºä¾èµ–é”™è¯¯ã€‚æˆ‘é‡åˆ°è¿‡å‡ æ¬¡é—®é¢˜ï¼Œéœ€è¦æ‰‹åŠ¨å®‰è£…æ­£ç¡®çš„
    cuda é©±åŠ¨ç¨‹åºå’Œ flash_attnï¼š
- en: '[PRE11]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Great â€” we used fairly straightforward configurations provided by axolotl to
    initialise the fine-tuning of our LLM.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: å¾ˆå¥½â€”â€”æˆ‘ä»¬ä½¿ç”¨äº† Axolotl æä¾›çš„ç›¸å½“ç®€å•çš„é…ç½®æ¥åˆå§‹åŒ–æˆ‘ä»¬ LLM çš„å¾®è°ƒã€‚
- en: Letâ€™s run inference on our tuned model to see how it performs.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬å¯¹å¾®è°ƒåçš„æ¨¡å‹è¿›è¡Œæ¨ç†ï¼Œçœ‹çœ‹å®ƒçš„è¡¨ç°å¦‚ä½•ã€‚
- en: 'As the data we used to tune the model has a fairly specific layout, we will
    need to manually create some prompts that the model can work with. Letâ€™s create
    a question that we want answered based on an existing DB schema. Weâ€™ll use the
    same example as when we checked our dataset loaded correctly but weâ€™ll overwrite
    the question:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: ç”±äºç”¨äºè°ƒæ•´æ¨¡å‹çš„æ•°æ®å…·æœ‰ç›¸å½“ç‰¹å®šçš„å¸ƒå±€ï¼Œæˆ‘ä»¬éœ€è¦æ‰‹åŠ¨åˆ›å»ºä¸€äº›æ¨¡å‹å¯ä»¥å¤„ç†çš„æç¤ºè¯ã€‚è®©æˆ‘ä»¬åˆ›å»ºä¸€ä¸ªåŸºäºç°æœ‰æ•°æ®åº“æ¨¡å¼çš„é—®é¢˜ã€‚æˆ‘ä»¬å°†ä½¿ç”¨ä¹‹å‰æ£€æŸ¥æ•°æ®é›†æ˜¯å¦æ­£ç¡®åŠ è½½æ—¶çš„ç›¸åŒç¤ºä¾‹ï¼Œä½†æˆ‘ä»¬å°†è¦†ç›–é—®é¢˜ï¼š
- en: '[PRE12]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Now we need to ensure the formatting is correct:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬éœ€è¦ç¡®ä¿æ ¼å¼æ­£ç¡®ï¼š
- en: '[PRE13]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Weâ€™ll also create a quick function to handle this formatting:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬è¿˜å°†åˆ›å»ºä¸€ä¸ªå¿«é€Ÿå‡½æ•°æ¥å¤„ç†è¿™äº›æ ¼å¼ï¼š
- en: '[PRE14]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Letâ€™s check the above:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬æ£€æŸ¥ä¸€ä¸‹ä»¥ä¸Šå†…å®¹ï¼š
- en: '[PRE15]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Great, now our prompt is ready to be fed into the model.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: å¾ˆå¥½ï¼Œç°åœ¨æˆ‘ä»¬çš„æç¤ºè¯å·²ç»å‡†å¤‡å¥½å¯ä»¥è¾“å…¥åˆ°æ¨¡å‹ä¸­ã€‚
- en: 'Now we need to run inference on our model that was saved in our qlora-out directory
    (as specified in our yaml config file). Letâ€™s first install the dependencies required
    to run inference on our model and tokenise our prompt so the model can work with
    it. Weâ€™ll need to the Llama2â€“7B tokeniser from HuggingFace. The workflow will
    be very similar to what we covered in my [previous article](https://medium.com/towards-data-science/quantisation-and-co-reducing-inference-times-on-llms-by-80-671db9349bdb).
    Here is the code:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬éœ€è¦å¯¹ä¿å­˜åœ¨qlora-outç›®å½•ä¸­çš„æ¨¡å‹è¿›è¡Œæ¨æ–­ï¼ˆå¦‚yamlé…ç½®æ–‡ä»¶ä¸­æŒ‡å®šçš„ï¼‰ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬éœ€è¦å®‰è£…è¿è¡Œæ¨æ–­æ‰€éœ€çš„ä¾èµ–é¡¹ï¼Œå¹¶å¯¹æç¤ºè¿›è¡Œåˆ†è¯ï¼Œä»¥ä¾¿æ¨¡å‹å¯ä»¥å¤„ç†å®ƒã€‚æˆ‘ä»¬éœ€è¦ä»HuggingFaceè·å–Llama2â€“7Båˆ†è¯å™¨ã€‚å·¥ä½œæµç¨‹å°†ä¸æˆ‘åœ¨[ä¸Šä¸€ç¯‡æ–‡ç« ](https://medium.com/towards-data-science/quantisation-and-co-reducing-inference-times-on-llms-by-80-671db9349bdb)ä¸­ä»‹ç»çš„éå¸¸ç›¸ä¼¼ã€‚ä»¥ä¸‹æ˜¯ä»£ç ï¼š
- en: '[PRE16]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Awesome! As you can see from the ASSISTANT output, it provided the correct answer.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: å¤ªæ£’äº†ï¼æ­£å¦‚ä½ ä»åŠ©æ‰‹çš„è¾“å‡ºä¸­çœ‹åˆ°çš„ï¼Œå®ƒæä¾›äº†æ­£ç¡®çš„ç­”æ¡ˆã€‚
- en: Whilst this is arguably a very straightforward query, you can see weâ€™ve successfully
    trained a base LLM that could only be used for general language queries on a very
    specific task. It now writes SQL code!
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: è™½ç„¶è¿™å¯ä»¥è¯´æ˜¯ä¸€ä¸ªéå¸¸ç®€å•çš„é—®é¢˜ï¼Œä½†ä½ å¯ä»¥çœ‹åˆ°æˆ‘ä»¬æˆåŠŸè®­ç»ƒäº†ä¸€ä¸ªåŸºç¡€LLMï¼Œè¿™ä¸ªLLMä¹‹å‰åªèƒ½ç”¨äºä¸€èˆ¬è¯­è¨€æŸ¥è¯¢ï¼Œç°åœ¨å®ƒå¯ä»¥å†™SQLä»£ç äº†ï¼
- en: By familiarising yourself with the above and checking through the axolotl documentation,
    youâ€™ll hopefully be able to see how one may adapt my approach to different LLM
    tasks such as Q&A for example. Simply feed in a dataset of questions on a specific
    topic along with their respective answers, update your yaml config file using
    the examples provided by axolotl and begin another fine-tuning experiment ïŠ
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: é€šè¿‡ç†Ÿæ‚‰ä»¥ä¸Šå†…å®¹å¹¶æŸ¥çœ‹axolotlæ–‡æ¡£ï¼Œä½ å°†èƒ½å¤Ÿäº†è§£å¦‚ä½•å°†æˆ‘çš„æ–¹æ³•é€‚åº”åˆ°ä¸åŒçš„LLMä»»åŠ¡ä¸­ï¼Œä¾‹å¦‚é—®ç­”ã€‚åªéœ€è¾“å…¥ä¸€ä¸ªç‰¹å®šä¸»é¢˜çš„é—®é¢˜æ•°æ®é›†åŠå…¶ç›¸åº”ç­”æ¡ˆï¼Œä½¿ç”¨axolotlæä¾›çš„ç¤ºä¾‹æ›´æ–°ä½ çš„yamlé…ç½®æ–‡ä»¶ï¼Œç„¶åå¼€å§‹å¦ä¸€ä¸ªå¾®è°ƒå®éªŒğŸ˜Š
- en: I hope you enjoyed reading the article as much as I did writing it. As always,
    please contact me if you have any questions.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘å¸Œæœ›ä½ é˜…è¯»è¿™ç¯‡æ–‡ç« æ—¶çš„æ„Ÿå—ä¸æˆ‘å†™ä½œæ—¶ä¸€æ ·æ„‰å¿«ã€‚åƒå¾€å¸¸ä¸€æ ·ï¼Œå¦‚æœä½ æœ‰ä»»ä½•é—®é¢˜ï¼Œè¯·éšæ—¶è”ç³»æˆ‘ã€‚
- en: '*All images belong to the author unless otherwise stated.*'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '*é™¤éå¦æœ‰è¯´æ˜ï¼Œå¦åˆ™æ‰€æœ‰å›¾åƒå‡å±äºä½œè€…ã€‚*'
