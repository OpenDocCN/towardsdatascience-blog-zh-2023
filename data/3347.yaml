- en: 'Precision Clustering Made Simple: kscorerâ€™s Guide to Auto-Selecting Optimal
    K-means Clusters'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç²¾å‡†èšç±»ç®€åŒ–ç‰ˆï¼škscorer çš„è‡ªåŠ¨é€‰æ‹©æœ€ä½³ K-means èšç±»æŒ‡å—
- en: åŸæ–‡ï¼š[https://towardsdatascience.com/precision-clustering-made-simple-kscorers-guide-to-auto-selecting-optimal-k-means-clusters-51fb39fde44c?source=collection_archive---------3-----------------------#2023-11-10](https://towardsdatascience.com/precision-clustering-made-simple-kscorers-guide-to-auto-selecting-optimal-k-means-clusters-51fb39fde44c?source=collection_archive---------3-----------------------#2023-11-10)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡ï¼š[https://towardsdatascience.com/precision-clustering-made-simple-kscorers-guide-to-auto-selecting-optimal-k-means-clusters-51fb39fde44c?source=collection_archive---------3-----------------------#2023-11-10](https://towardsdatascience.com/precision-clustering-made-simple-kscorers-guide-to-auto-selecting-optimal-k-means-clusters-51fb39fde44c?source=collection_archive---------3-----------------------#2023-11-10)
- en: kscorer streamlines the process of clustering and provides practical approach
    to data analysis through advanced scoring and parallelization
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: kscorer ç®€åŒ–äº†èšç±»è¿‡ç¨‹ï¼Œé€šè¿‡å…ˆè¿›çš„è¯„åˆ†å’Œå¹¶è¡ŒåŒ–æä¾›äº†å®ç”¨çš„æ•°æ®åˆ†ææ–¹æ³•
- en: '[](https://wldmrgml.medium.com/?source=post_page-----51fb39fde44c--------------------------------)[![Volodymyr
    Holomb](../Images/ff4a34f4dc4ee397d4d30512aa8f177c.png)](https://wldmrgml.medium.com/?source=post_page-----51fb39fde44c--------------------------------)[](https://towardsdatascience.com/?source=post_page-----51fb39fde44c--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----51fb39fde44c--------------------------------)
    [Volodymyr Holomb](https://wldmrgml.medium.com/?source=post_page-----51fb39fde44c--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://wldmrgml.medium.com/?source=post_page-----51fb39fde44c--------------------------------)[![Volodymyr
    Holomb](../Images/ff4a34f4dc4ee397d4d30512aa8f177c.png)](https://wldmrgml.medium.com/?source=post_page-----51fb39fde44c--------------------------------)[](https://towardsdatascience.com/?source=post_page-----51fb39fde44c--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----51fb39fde44c--------------------------------)
    [Volodymyr Holomb](https://wldmrgml.medium.com/?source=post_page-----51fb39fde44c--------------------------------)'
- en: Â·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: Â·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F95923fba037b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fprecision-clustering-made-simple-kscorers-guide-to-auto-selecting-optimal-k-means-clusters-51fb39fde44c&user=Volodymyr+Holomb&userId=95923fba037b&source=post_page-95923fba037b----51fb39fde44c---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----51fb39fde44c--------------------------------)
    Â·7 min readÂ·Nov 10, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F51fb39fde44c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fprecision-clustering-made-simple-kscorers-guide-to-auto-selecting-optimal-k-means-clusters-51fb39fde44c&user=Volodymyr+Holomb&userId=95923fba037b&source=-----51fb39fde44c---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[å…³æ³¨](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F95923fba037b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fprecision-clustering-made-simple-kscorers-guide-to-auto-selecting-optimal-k-means-clusters-51fb39fde44c&user=Volodymyr+Holomb&userId=95923fba037b&source=post_page-95923fba037b----51fb39fde44c---------------------post_header-----------)
    å‘è¡¨åœ¨ [Towards Data Science](https://towardsdatascience.com/?source=post_page-----51fb39fde44c--------------------------------)
    Â· 7åˆ†é’Ÿé˜…è¯»Â·2023å¹´11æœˆ10æ—¥'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F51fb39fde44c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fprecision-clustering-made-simple-kscorers-guide-to-auto-selecting-optimal-k-means-clusters-51fb39fde44c&source=-----51fb39fde44c---------------------bookmark_footer-----------)![](../Images/0d4ed07360aaaec601e971efa3c3fee4.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F51fb39fde44c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fprecision-clustering-made-simple-kscorers-guide-to-auto-selecting-optimal-k-means-clusters-51fb39fde44c&source=-----51fb39fde44c---------------------bookmark_footer-----------)![](../Images/0d4ed07360aaaec601e971efa3c3fee4.png)'
- en: Made by DALL-E-2 according to the authorâ€™s description
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: ç”± DALL-E-2 æ ¹æ®ä½œè€…çš„æè¿°åˆ¶ä½œ
- en: Unsupervised machine learning, particularly clustering, is a challenging task
    in data science. It is crucial to a wide range of practical business analytics
    projects. Clustering can operate on its own, but it is also a valuable component
    in complex data processing pipelines that enhance the efficiency of other algorithms.
    For instance, clustering plays a crucial role when developing a [recommender system](https://medium.com/towards-data-science/building-memory-efficient-meta-hybrid-recommender-engine-back-to-front-part-2-51a7d4546e90).
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: æ— ç›‘ç£æœºå™¨å­¦ä¹ ï¼Œå°¤å…¶æ˜¯èšç±»ï¼Œåœ¨æ•°æ®ç§‘å­¦ä¸­æ˜¯ä¸€é¡¹å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ã€‚å®ƒå¯¹å¹¿æ³›çš„å®é™…å•†ä¸šåˆ†æé¡¹ç›®è‡³å…³é‡è¦ã€‚èšç±»å¯ä»¥ç‹¬ç«‹è¿è¡Œï¼Œä½†å®ƒä¹Ÿæ˜¯å¤æ‚æ•°æ®å¤„ç†ç®¡é“ä¸­çš„ä¸€ä¸ªæœ‰ä»·å€¼çš„ç»„æˆéƒ¨åˆ†ï¼Œè¿™äº›ç®¡é“æå‡äº†å…¶ä»–ç®—æ³•çš„æ•ˆç‡ã€‚ä¾‹å¦‚ï¼Œèšç±»åœ¨å¼€å‘[æ¨èç³»ç»Ÿ](https://medium.com/towards-data-science/building-memory-efficient-meta-hybrid-recommender-engine-back-to-front-part-2-51a7d4546e90)æ—¶èµ·ç€å…³é”®ä½œç”¨ã€‚
- en: Well, Scikit-Learn notoriously offers various proven clustering algorithms.
    Nonetheless, most of them are parametric and require setting the number of clusters,
    which is one of the most significant challenges in clustering.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: å¥½å§ï¼ŒScikit-Learnè‘—ååœ°æä¾›äº†å„ç§ç»è¿‡éªŒè¯çš„èšç±»ç®—æ³•ã€‚å°½ç®¡å¦‚æ­¤ï¼Œå…¶ä¸­å¤§å¤šæ•°éƒ½æ˜¯å‚æ•°åŒ–çš„ï¼Œéœ€è¦è®¾ç½®ç°‡æ•°ï¼Œè¿™æ˜¯èšç±»ä¸­æœ€é‡è¦çš„æŒ‘æˆ˜ä¹‹ä¸€ã€‚
- en: Commonly, an iterative method is used to decide on the optimal number of clusters
    when working with data. It means that you carry out clustering multiple times,
    each time with a different number of clusters, and evaluate the corresponding
    result. While this technique is useful, it does have limitations.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: é€šå¸¸ï¼Œä½¿ç”¨è¿­ä»£æ–¹æ³•æ¥å†³å®šæœ€ä½³çš„ç°‡æ•°ã€‚è¿™æ„å‘³ç€ä½ å¤šæ¬¡è¿›è¡Œèšç±»ï¼Œæ¯æ¬¡ä½¿ç”¨ä¸åŒçš„ç°‡æ•°ï¼Œå¹¶è¯„ä¼°ç›¸åº”çš„ç»“æœã€‚è™½ç„¶è¿™ç§æŠ€æœ¯å¾ˆæœ‰ç”¨ï¼Œä½†ä¹Ÿæœ‰å…¶å±€é™æ€§ã€‚
- en: The [yellowbrick package](https://www.scikit-yb.org/en/latest/api/cluster/index.html)
    is a commonly employed tool that makes it easy to identify the optimal number
    of clusters. However, it also has some drawbacks. One significant drawback is
    the possibility of conflicting outcomes when evaluating multiple metrics and the
    challenge of identifying an [elbow](https://en.wikipedia.org/wiki/Elbow_method_(clustering))
    on the diagram.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '[yellowbrickåŒ…](https://www.scikit-yb.org/en/latest/api/cluster/index.html)æ˜¯ä¸€ä¸ªå¸¸ç”¨å·¥å…·ï¼Œå¯ä»¥è½»æ¾è¯†åˆ«æœ€ä½³ç°‡æ•°ã€‚ç„¶è€Œï¼Œå®ƒä¹Ÿæœ‰ä¸€äº›ç¼ºç‚¹ã€‚ä¸€ä¸ªæ˜¾è‘—çš„ç¼ºç‚¹æ˜¯è¯„ä¼°å¤šä¸ªæŒ‡æ ‡æ—¶å¯èƒ½ä¼šå‡ºç°å†²çªçš„ç»“æœï¼Œä»¥åŠåœ¨å›¾è¡¨ä¸Šè¯†åˆ«[è‚˜éƒ¨](https://en.wikipedia.org/wiki/Elbow_method_(clustering))çš„æŒ‘æˆ˜ã€‚'
- en: In addition, the dataset size poses another problem, regardless of the package
    used. When working with large datasets, resource consumption difficulties may
    impede your ability to efficiently iterate through a wide range of clusters. If
    this is the case, consider exploring techniques such as [MiniBatchKMeans](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.MiniBatchKMeans.html),
    which can afford parallel clustering.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤å¤–ï¼Œæ•°æ®é›†çš„å¤§å°ä¹Ÿæ˜¯ä¸€ä¸ªé—®é¢˜ï¼Œä¸è®ºä½¿ç”¨ä½•ç§åŒ…ã€‚å½“å¤„ç†å¤§æ•°æ®é›†æ—¶ï¼Œèµ„æºæ¶ˆè€—å›°éš¾å¯èƒ½ä¼šå¦¨ç¢ä½ æœ‰æ•ˆåœ°è¿­ä»£å¤šä¸ªç°‡ã€‚å¦‚æœæ˜¯è¿™ç§æƒ…å†µï¼Œå¯ä»¥è€ƒè™‘æ¢ç´¢è¯¸å¦‚[MiniBatchKMeans](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.MiniBatchKMeans.html)ç­‰æŠ€æœ¯ï¼Œå®ƒå¯ä»¥å®ç°å¹¶è¡Œèšç±»ã€‚
- en: But advanced optimization of your clustering routine may require lesser-known
    techniques, described further. You will also get to know the [kscorer package](https://pypi.org/project/kscorer/),
    which streamlines these techniques, offering a more robust and efficient approach
    to determining the optimal number of clusters.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†å¯¹ä½ çš„èšç±»ç¨‹åºè¿›è¡Œé«˜çº§ä¼˜åŒ–å¯èƒ½éœ€è¦ä¸€äº›é²œä¸ºäººçŸ¥çš„æŠ€æœ¯ï¼Œç¨åä¼šè¯¦ç»†ä»‹ç»ã€‚ä½ è¿˜å°†äº†è§£[kscoreråŒ…](https://pypi.org/project/kscorer/)ï¼Œå®ƒç®€åŒ–äº†è¿™äº›æŠ€æœ¯ï¼Œæä¾›äº†ä¸€ç§æ›´å¼ºå¤§å’Œé«˜æ•ˆçš„æ–¹æ³•æ¥ç¡®å®šæœ€ä½³ç°‡æ•°ã€‚
- en: 'Without a hitch, those techniques are:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™äº›æŠ€æœ¯æ¯«æ— ç–‘é—®æ˜¯ï¼š
- en: '**Dimensionality Reduction.** It may be beneficial to perform a Principal Component
    Analysis (PCA) on the data before applying the clustering algorithm. This will
    reduce data interference and lead to a more reliable clustering process.'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**é™ç»´ã€‚** åœ¨åº”ç”¨èšç±»ç®—æ³•ä¹‹å‰ï¼Œå¯¹æ•°æ®è¿›è¡Œä¸»æˆåˆ†åˆ†æï¼ˆPCAï¼‰å¯èƒ½ä¼šæœ‰ç›Šã€‚è¿™å°†å‡å°‘æ•°æ®å¹²æ‰°ï¼Œå¹¶å¯¼è‡´æ›´å¯é çš„èšç±»è¿‡ç¨‹ã€‚'
- en: '**Cosine Similarity.** There is a straightforward way to use (approx.) cosine
    distances in K-means via applying Euclidean normalisation to the data. So that
    you do not need to pre-calculate distance matrix, such as while performing agglomerative
    clustering.'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ä½™å¼¦ç›¸ä¼¼åº¦ã€‚** æœ‰ä¸€ç§ç®€å•çš„æ–¹æ³•å¯ä»¥åœ¨K-meansä¸­ä½¿ç”¨ï¼ˆè¿‘ä¼¼ï¼‰ä½™å¼¦è·ç¦»ï¼Œå³é€šè¿‡å¯¹æ•°æ®è¿›è¡Œæ¬§å‡ é‡Œå¾—å½’ä¸€åŒ–ã€‚è¿™æ ·ä½ å°±ä¸éœ€è¦é¢„å…ˆè®¡ç®—è·ç¦»çŸ©é˜µï¼Œæ¯”å¦‚åœ¨è¿›è¡Œå‡èšèšç±»æ—¶ã€‚'
- en: '**Many-Metrics-At-Hand.** To find the optimal number of clusters, one should
    rely on a multi-metric assessment instead of depending on a single metric.'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**å¤šæŒ‡æ ‡æ‰‹å¤´ã€‚** ä¸ºäº†æ‰¾åˆ°æœ€ä½³ç°‡æ•°ï¼Œåº”è¯¥ä¾èµ–å¤šæŒ‡æ ‡è¯„ä¼°ï¼Œè€Œä¸æ˜¯ä»…ä»…ä¾é å•ä¸€æŒ‡æ ‡ã€‚'
- en: '**Data Sampling.** To address resource consumption issues and improve clustering
    results one can get random samples from data to perform clustering operations
    and asses metrics. Averaging scores from multiple iterations can reduce the effect
    of randomness and produce more consistent results.'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**æ•°æ®é‡‡æ ·ã€‚** ä¸ºäº†è§£å†³èµ„æºæ¶ˆè€—é—®é¢˜å¹¶æ”¹è¿›èšç±»ç»“æœï¼Œå¯ä»¥ä»æ•°æ®ä¸­è·å–éšæœºæ ·æœ¬ä»¥æ‰§è¡Œèšç±»æ“ä½œå¹¶è¯„ä¼°æŒ‡æ ‡ã€‚é€šè¿‡å¤šæ¬¡è¿­ä»£çš„å¹³å‡å¾—åˆ†å¯ä»¥å‡å°‘éšæœºæ€§çš„å½±å“ï¼Œä»è€Œäº§ç”Ÿæ›´ä¸€è‡´çš„ç»“æœã€‚'
- en: This workflow is illustrated below.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸‹é¢å±•ç¤ºäº†è¿™ä¸ªå·¥ä½œæµç¨‹ã€‚
- en: '![](../Images/c39dd7640649cc00f95f26c863aab0f2.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c39dd7640649cc00f95f26c863aab0f2.png)'
- en: Image by an author
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ç‰‡ç”±ä½œè€…æä¾›
- en: Luckily, there is no need to build this entire pipeline from scratch as an implementation
    is currently available in the [kscorer package](https://pypi.org/project/kscorer/).
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: å¹¸è¿çš„æ˜¯ï¼Œæ— éœ€ä»å¤´å¼€å§‹æ„å»ºæ•´ä¸ªç®¡é“ï¼Œå› ä¸º [kscorer åŒ…](https://pypi.org/project/kscorer/)ä¸­å·²ç»æœ‰ç°æˆçš„å®ç°ã€‚
- en: Now, letâ€™s delve a bit deeper
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç°åœ¨ï¼Œè®©æˆ‘ä»¬æ·±å…¥æ¢è®¨ä¸€ä¸‹
- en: 'I once heard a data scientist state at a conference talk: â€œBasically, **you
    can do what you want, as long as you know what you are doing**.â€ Â© [Alex2006](https://datascience.stackexchange.com/a/36003/101016)'
  id: totrans-25
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: æˆ‘æ›¾åœ¨ä¸€æ¬¡ä¼šè®®æ¼”è®²ä¸­å¬åˆ°ä¸€ä½æ•°æ®ç§‘å­¦å®¶è¯´ï¼šâ€œåŸºæœ¬ä¸Šï¼Œåªè¦ä½ çŸ¥é“è‡ªå·±åœ¨åšä»€ä¹ˆï¼Œ**ä½ å¯ä»¥åšä»»ä½•ä½ æƒ³åšçš„äº‹æƒ…**ã€‚â€ Â© [Alex2006](https://datascience.stackexchange.com/a/36003/101016)
- en: It is recommended to scale your data before clustering to ensure all features
    are on an equal footing and none dominate due to their magnitude. **Standardisation**
    (centred around the mean and scaled by the standard deviation) or **Min-Max scaling**
    (scaling values to a specified range) are common techniques used for scaling.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨èšç±»ä¹‹å‰å»ºè®®å¯¹æ•°æ®è¿›è¡Œç¼©æ”¾ï¼Œä»¥ç¡®ä¿æ‰€æœ‰ç‰¹å¾å¤„äºåŒä¸€æ°´å¹³ï¼Œé¿å…å› ç‰¹å¾çš„å¤§å°è€Œä¸»å¯¼ã€‚**æ ‡å‡†åŒ–**ï¼ˆä»¥å‡å€¼ä¸ºä¸­å¿ƒå¹¶æŒ‰æ ‡å‡†å·®ç¼©æ”¾ï¼‰æˆ–**æœ€å°-æœ€å¤§ç¼©æ”¾**ï¼ˆå°†å€¼ç¼©æ”¾åˆ°æŒ‡å®šèŒƒå›´ï¼‰æ˜¯å¸¸ç”¨çš„ç¼©æ”¾æŠ€æœ¯ã€‚
- en: Itâ€™s worth noting that the significance of feature scaling, perfectly [illustrated
    here](https://scikit-learn.org/stable/auto_examples/preprocessing/plot_scaling_importance.html),
    isnâ€™t restricted to just KNeighbors models but also applies to various data science
    methodologies. Standardising features through z-score normalisation ensures that
    all features are on the same scale, preventing any feature from dominating the
    model adjustment due to their magnitudes. This scaling procedure can significantly
    impact the modelâ€™s performance, resulting in different model adjustments when
    compared to using unscaled data.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œç‰¹å¾ç¼©æ”¾çš„é‡è¦æ€§åœ¨[è¿™é‡Œ](https://scikit-learn.org/stable/auto_examples/preprocessing/plot_scaling_importance.html)å¾—åˆ°äº†å®Œç¾çš„è¯´æ˜ï¼Œå®ƒä¸ä»…é™äº
    KNeighbors æ¨¡å‹ï¼Œè¿˜é€‚ç”¨äºå„ç§æ•°æ®ç§‘å­¦æ–¹æ³•ã€‚é€šè¿‡ z-score å½’ä¸€åŒ–å¯¹ç‰¹å¾è¿›è¡Œæ ‡å‡†åŒ–å¯ä»¥ç¡®ä¿æ‰€æœ‰ç‰¹å¾åœ¨ç›¸åŒçš„å°ºåº¦ä¸Šï¼Œé˜²æ­¢ä»»ä½•ç‰¹å¾å› å…¶å¤§å°è€Œä¸»å¯¼æ¨¡å‹è°ƒæ•´ã€‚è¿™ä¸ªç¼©æ”¾è¿‡ç¨‹å¯èƒ½ä¼šæ˜¾è‘—å½±å“æ¨¡å‹çš„æ€§èƒ½ï¼Œä¸ä½¿ç”¨æœªç¼©æ”¾æ•°æ®æ—¶è¿›è¡Œçš„æ¨¡å‹è°ƒæ•´ç›¸æ¯”ï¼Œå¯èƒ½ä¼šå¯¼è‡´ä¸åŒçš„æ¨¡å‹è°ƒæ•´ã€‚
- en: Furthermore, there is a fundamental link between K-means clustering and **PCA**,
    explored in Ding and Heâ€™s paper [â€œK-means Clustering via Principal Component Analysisâ€](https://ranger.uta.edu/%7Echqding/papers/KmeansPCA1.pdf).
    Although initially serving distinct purposes, these techniques ultimately aim
    to efficiently represent data whilst minimising reconstruction errors. PCA aims
    to represent data vectors as a combination of a reduced number of eigenvectors.
    In contrast, K-means clustering aims to represent data vectors as a combination
    of cluster centroid vectors. Both approaches strive to minimise the mean-squared
    reconstruction error.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤å¤–ï¼ŒK-means èšç±»ä¸**ä¸»æˆåˆ†åˆ†æï¼ˆPCAï¼‰**ä¹‹é—´å­˜åœ¨åŸºæœ¬è”ç³»ï¼Œè¿™åœ¨ä¸å’Œèµ«çš„è®ºæ–‡ [â€œK-means Clustering via Principal
    Component Analysisâ€](https://ranger.uta.edu/%7Echqding/papers/KmeansPCA1.pdf)ä¸­è¿›è¡Œäº†æ¢è®¨ã€‚å°½ç®¡æœ€åˆè¿™ä¸¤ç§æŠ€æœ¯çš„ç›®çš„ä¸åŒï¼Œä½†æœ€ç»ˆå®ƒä»¬éƒ½æ—¨åœ¨æœ‰æ•ˆåœ°è¡¨ç¤ºæ•°æ®ï¼ŒåŒæ—¶æœ€å°åŒ–é‡æ„è¯¯å·®ã€‚PCA
    æ—¨åœ¨å°†æ•°æ®å‘é‡è¡¨ç¤ºä¸ºå‡å°‘æ•°é‡çš„ç‰¹å¾å‘é‡çš„ç»„åˆã€‚è€Œ K-means èšç±»åˆ™æ—¨åœ¨å°†æ•°æ®å‘é‡è¡¨ç¤ºä¸ºç°‡ä¸­å¿ƒå‘é‡çš„ç»„åˆã€‚è¿™ä¸¤ç§æ–¹æ³•éƒ½åŠ›æ±‚æœ€å°åŒ–å‡æ–¹é‡æ„è¯¯å·®ã€‚
- en: After applying PCA, weâ€™ll scale again our data due to computational issues that
    might arise (some values may be close to zero while others will be quite large).
    This perfectly makes sense as we already have lost track of our initial features
    (after PCA), so there will be no interpretation of data.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨åº”ç”¨ PCA åï¼Œç”±äºå¯èƒ½å‡ºç°çš„è®¡ç®—é—®é¢˜ï¼ˆæœ‰äº›å€¼å¯èƒ½æ¥è¿‘é›¶ï¼Œè€Œå…¶ä»–å€¼å¯èƒ½éå¸¸å¤§ï¼‰ï¼Œæˆ‘ä»¬éœ€è¦å†æ¬¡å¯¹æ•°æ®è¿›è¡Œç¼©æ”¾ã€‚è¿™å®Œå…¨æœ‰æ„ä¹‰ï¼Œå› ä¸ºæˆ‘ä»¬åœ¨ PCA åå·²ç»å¤±å»äº†å¯¹åˆå§‹ç‰¹å¾çš„è·Ÿè¸ªï¼Œå› æ­¤æ•°æ®å°†æ— æ³•è§£é‡Šã€‚
- en: Another interesting correlation that may not be commonly known is between **Cosine
    Similarity** and Euclidean Distance. [Understanding the relationship](https://medium.com/ai-for-real/relationship-between-cosine-similarity-and-euclidean-distance-7e283a277dff)
    between these measures is crucial when they are used interchangeably, albeit indirectly.
    This knowledge has practical application in transforming the traditional K-means
    Clustering Algorithm into the Spherical K-means Clustering Algorithm, where cosine
    similarity is a vital metric for clustering data. As mentioned previously, we
    can â€œestablishâ€ the connection between cosine similarity and Euclidean distance
    by applying Euclidean normalisation to data.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: å¦ä¸€ä¸ªå¯èƒ½ä¸ä¸ºäººçŸ¥çš„æœ‰è¶£ç›¸å…³æ€§æ˜¯**ä½™å¼¦ç›¸ä¼¼åº¦**ä¸æ¬§å‡ é‡Œå¾—è·ç¦»ä¹‹é—´çš„å…³ç³»ã€‚[ç†è§£è¿™ç§å…³ç³»](https://medium.com/ai-for-real/relationship-between-cosine-similarity-and-euclidean-distance-7e283a277dff)åœ¨è¿™äº›åº¦é‡é—´æ¥äº’æ¢ä½¿ç”¨æ—¶è‡³å…³é‡è¦ã€‚è¿™äº›çŸ¥è¯†åœ¨å°†ä¼ ç»Ÿçš„K-meansèšç±»ç®—æ³•è½¬æ¢ä¸ºçƒé¢K-meansèšç±»ç®—æ³•æ—¶å…·æœ‰å®é™…åº”ç”¨ï¼Œå…¶ä¸­ä½™å¼¦ç›¸ä¼¼åº¦æ˜¯èšç±»æ•°æ®çš„å…³é”®æŒ‡æ ‡ã€‚å¦‚å‰æ‰€è¿°ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡å¯¹æ•°æ®åº”ç”¨æ¬§å‡ é‡Œå¾—å½’ä¸€åŒ–æ¥â€œå»ºç«‹â€ä½™å¼¦ç›¸ä¼¼åº¦ä¸æ¬§å‡ é‡Œå¾—è·ç¦»ä¹‹é—´çš„è”ç³»ã€‚
- en: 'In the absence of ground truth cluster labels, the evaluation of clustering
    models must rely on intrinsic measures, and the [kscorer package](https://pypi.org/project/kscorer/)
    offers a comprehensive set of indicators to assess the quality of clustering.
    These indicators suggest valuable insight into the degree of separation among
    recognised clusters:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ç¼ºä¹çœŸå®ç°‡æ ‡ç­¾çš„æƒ…å†µä¸‹ï¼Œèšç±»æ¨¡å‹çš„è¯„ä¼°å¿…é¡»ä¾èµ–äºå†…åœ¨åº¦é‡ï¼Œè€Œ[kscoreråŒ…](https://pypi.org/project/kscorer/)æä¾›äº†ä¸€å¥—å…¨é¢çš„æŒ‡æ ‡æ¥è¯„ä¼°èšç±»è´¨é‡ã€‚è¿™äº›æŒ‡æ ‡æä¾›äº†æœ‰å…³è¯†åˆ«ç°‡ä¹‹é—´åˆ†ç¦»ç¨‹åº¦çš„æœ‰ä»·å€¼è§è§£ï¼š
- en: '[**Silhouette Coefficient**](https://scikit-learn.org/stable/modules/clustering.html#silhouette-coefficient).
    It quantifies the separation of clusters by calculating the difference between
    the mean distance to the nearest cluster that a data point does not belong to
    and the mean intra-cluster distance for each data point. The outcome is standardised
    and expressed as the ratio between the two, with elevated values indicating superior
    cluster separation.'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**è½®å»“ç³»æ•°**](https://scikit-learn.org/stable/modules/clustering.html#silhouette-coefficient)ã€‚å®ƒé€šè¿‡è®¡ç®—æ•°æ®ç‚¹åˆ°å…¶ä¸å±äºçš„æœ€è¿‘ç°‡çš„å¹³å‡è·ç¦»ä¸æ¯ä¸ªæ•°æ®ç‚¹çš„ç°‡å†…å¹³å‡è·ç¦»ä¹‹é—´çš„å·®å¼‚æ¥é‡åŒ–ç°‡çš„åˆ†ç¦»ç¨‹åº¦ã€‚ç»“æœç»è¿‡æ ‡å‡†åŒ–ï¼Œå¹¶è¡¨ç¤ºä¸ºä¸¤è€…ä¹‹é—´çš„æ¯”ä¾‹ï¼Œå€¼è¶Šé«˜è¡¨ç¤ºç°‡åˆ†ç¦»è¶Šä¼˜è¶Šã€‚'
- en: '[**Calinski-Harabasz Index**](https://scikit-learn.org/stable/modules/clustering.html#calinski-harabasz-index).
    It calculates the ratio of between-cluster scattering to within-cluster scattering.
    A higher score on the Calinski-Harabasz test indicates better clustering performance,
    indicating well-defined clusters.'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**Calinski-HarabaszæŒ‡æ•°**](https://scikit-learn.org/stable/modules/clustering.html#calinski-harabasz-index)ã€‚å®ƒè®¡ç®—ç°‡é—´æ•£åº¦ä¸ç°‡å†…æ•£åº¦çš„æ¯”ç‡ã€‚Calinski-Harabaszæµ‹è¯•å¾—åˆ†è¶Šé«˜ï¼Œè¡¨ç¤ºèšç±»æ€§èƒ½è¶Šå¥½ï¼Œç°‡å®šä¹‰è¶Šæ¸…æ™°ã€‚'
- en: '[**Davies-Bouldin Index**](https://scikit-learn.org/stable/modules/clustering.html#davies-bouldin-index).
    It measures the ratio of between-cluster dispersion to within-cluster dispersion,
    with a lower value indicating superior clustering performance and more distinct
    clusters.'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**Davies-BouldinæŒ‡æ•°**](https://scikit-learn.org/stable/modules/clustering.html#davies-bouldin-index)ã€‚å®ƒè¡¡é‡ç°‡é—´ç¦»æ•£åº¦ä¸ç°‡å†…ç¦»æ•£åº¦çš„æ¯”ç‡ï¼Œå€¼è¶Šä½è¡¨ç¤ºèšç±»æ€§èƒ½è¶Šä¼˜è¶Šï¼Œç°‡çš„åŒºåˆ†åº¦è¶Šé«˜ã€‚'
- en: '[**Dunn Index.**](https://en.wikipedia.org/wiki/Dunn_index) It assesses cluster
    quality by comparing intercluster distance (the smallest distance between any
    two cluster centroids) to intracluster distance (the largest distance between
    any two points within a cluster). A higher Dunn Index indicates more well-defined
    clusters.'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**é‚“æ©æŒ‡æ•°**](https://en.wikipedia.org/wiki/Dunn_index)ã€‚å®ƒé€šè¿‡æ¯”è¾ƒç°‡é—´è·ç¦»ï¼ˆä»»æ„ä¸¤ä¸ªç°‡è´¨å¿ƒä¹‹é—´çš„æœ€å°è·ç¦»ï¼‰ä¸ç°‡å†…è·ç¦»ï¼ˆç°‡å†…ä»»æ„ä¸¤ç‚¹ä¹‹é—´çš„æœ€å¤§è·ç¦»ï¼‰æ¥è¯„ä¼°ç°‡çš„è´¨é‡ã€‚é‚“æ©æŒ‡æ•°è¶Šé«˜ï¼Œè¡¨ç¤ºç°‡å®šä¹‰è¶Šæ¸…æ™°ã€‚'
- en: 'The Python calculation for the index utilized in the package is outlined as
    follows:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: åŒ…ä¸­ä½¿ç”¨çš„æŒ‡æ ‡çš„Pythonè®¡ç®—æ–¹æ³•å¦‚ä¸‹ï¼š
- en: '[**Bayesian Information Criterion (BIC)**](https://stackoverflow.com/a/35379657/6025592).
    BIC serves as an additional and to some extent an independent metric. While K-means
    does not offer a direct probabilistic model, BIC can help estimate the dataâ€™s
    distribution after applying a K-means model. This approach provides a more comprehensive
    assessment of the cluster quality.'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**è´å¶æ–¯ä¿¡æ¯å‡†åˆ™ (BIC)**](https://stackoverflow.com/a/35379657/6025592)ã€‚BICä½œä¸ºä¸€ä¸ªé¢å¤–çš„ã€åœ¨æŸç§ç¨‹åº¦ä¸Šæ˜¯ç‹¬ç«‹çš„åº¦é‡ã€‚è™½ç„¶K-meansæ²¡æœ‰æä¾›ç›´æ¥çš„æ¦‚ç‡æ¨¡å‹ï¼Œä½†BICå¯ä»¥å¸®åŠ©ä¼°è®¡åº”ç”¨K-meansæ¨¡å‹åçš„æ•°æ®åˆ†å¸ƒã€‚è¿™ç§æ–¹æ³•æä¾›äº†å¯¹ç°‡è´¨é‡æ›´å…¨é¢çš„è¯„ä¼°ã€‚'
- en: All metrics are standardised, guaranteeing that higher scores consistently indicate
    well-defined clusters. This thorough evaluation is crucial in identifying the
    optimal number of clusters in a dataset.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€æœ‰æŒ‡æ ‡éƒ½ç»è¿‡æ ‡å‡†åŒ–ï¼Œç¡®ä¿è¾ƒé«˜çš„è¯„åˆ†å§‹ç»ˆè¡¨ç¤ºå®šä¹‰æ˜ç¡®çš„èšç±»ã€‚è¿™ç§å½»åº•çš„è¯„ä¼°å¯¹äºè¯†åˆ«æ•°æ®é›†ä¸­æœ€ä½³èšç±»æ•°è‡³å…³é‡è¦ã€‚
- en: To overcome memory limitations and execute data preprocessing and scoring operations
    expediently for K-means clustering, the [kscorer package](https://pypi.org/project/kscorer/)
    utilises N random data samples. This approach ensures seamless execution and adapts
    to datasets of different sizes and structures. Similar to cross-validation techniques,
    it maintains robust results, even though each iteration focuses on a limited subset
    of the data.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†å…‹æœè®°å¿†é™åˆ¶ï¼Œå¹¶è¿…é€Ÿæ‰§è¡Œæ•°æ®é¢„å¤„ç†å’Œè¯„åˆ†æ“ä½œä»¥è¿›è¡ŒK-meansèšç±»ï¼Œ[kscoreråŒ…](https://pypi.org/project/kscorer/)åˆ©ç”¨Nä¸ªéšæœºæ•°æ®æ ·æœ¬ã€‚è¿™ç§æ–¹æ³•ç¡®ä¿äº†æ— ç¼æ‰§è¡Œï¼Œå¹¶èƒ½é€‚åº”ä¸åŒå¤§å°å’Œç»“æ„çš„æ•°æ®é›†ã€‚ç±»ä¼¼äºäº¤å‰éªŒè¯æŠ€æœ¯ï¼Œå®ƒèƒ½å¤Ÿä¿æŒç¨³å¥çš„ç»“æœï¼Œå³ä½¿æ¯æ¬¡è¿­ä»£åªå…³æ³¨æ•°æ®çš„ä¸€ä¸ªæœ‰é™å­é›†ã€‚
- en: Hands-on with kscorer
  id: totrans-40
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ä½¿ç”¨kscoreråŠ¨æ‰‹æ“ä½œ
- en: So, we have some data for clustering. Please note that we pretend not to know
    the exact number of clusters in this scenario.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: å› æ­¤ï¼Œæˆ‘ä»¬æœ‰ä¸€äº›æ•°æ®ç”¨äºèšç±»ã€‚è¯·æ³¨æ„ï¼Œæˆ‘ä»¬åœ¨æ­¤åœºæ™¯ä¸­å‡è®¾ä¸çŸ¥é“ç¡®åˆ‡çš„èšç±»æ•°ã€‚
- en: 'Moving forward, we will split our dataset into train and test sets and fit
    a model to detect the optimal number of clusters. The model will automatically
    search for the optimal number of clusters between 3 and 15\. This can be effortlessly
    achieved as follows:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å°†æŠŠæ•°æ®é›†åˆ†æˆè®­ç»ƒé›†å’Œæµ‹è¯•é›†ï¼Œå¹¶æ‹Ÿåˆä¸€ä¸ªæ¨¡å‹æ¥æ£€æµ‹æœ€ä½³èšç±»æ•°ã€‚è¯¥æ¨¡å‹å°†è‡ªåŠ¨åœ¨3åˆ°15ä¹‹é—´æœç´¢æœ€ä½³èšç±»æ•°ã€‚è¿™å¯ä»¥è½»æ¾å®ç°ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š
- en: After completing the fitting process, we can review the scaled scores for all
    the metrics applied. This will help us to determine the best number of clusters
    for our available data. When checking the plot, you will notice that some clusters
    are highlighted with corresponding scores. These labelled points correspond with
    the local maxima in the average scores across all metrics and thus represent the
    best options for selecting the optimal number of clusters.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: å®Œæˆæ‹Ÿåˆè¿‡ç¨‹åï¼Œæˆ‘ä»¬å¯ä»¥æŸ¥çœ‹æ‰€æœ‰åº”ç”¨æŒ‡æ ‡çš„æ ‡å‡†åŒ–è¯„åˆ†ã€‚è¿™å°†å¸®åŠ©æˆ‘ä»¬ç¡®å®šé€‚åˆæˆ‘ä»¬æ•°æ®çš„æœ€ä½³èšç±»æ•°ã€‚å½“æŸ¥çœ‹å›¾è¡¨æ—¶ï¼Œä½ ä¼šæ³¨æ„åˆ°ä¸€äº›èšç±»è¢«çªå‡ºæ˜¾ç¤ºï¼Œå¹¶å¸¦æœ‰ç›¸åº”çš„è¯„åˆ†ã€‚è¿™äº›æ ‡è®°ç‚¹å¯¹åº”äºæ‰€æœ‰æŒ‡æ ‡çš„å¹³å‡è¯„åˆ†ä¸­çš„å±€éƒ¨æœ€å¤§å€¼ï¼Œå› æ­¤ä»£è¡¨äº†é€‰æ‹©æœ€ä½³èšç±»æ•°çš„æœ€ä½³é€‰é¡¹ã€‚
- en: Now, we can evaluate how well our new cluster labels match the true labels.
    Be sure that this option is usually not available in practical business scenarios
    ğŸ˜‰
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ï¼Œæˆ‘ä»¬å¯ä»¥è¯„ä¼°æˆ‘ä»¬æ–°çš„èšç±»æ ‡ç­¾ä¸çœŸå®æ ‡ç­¾çš„åŒ¹é…ç¨‹åº¦ã€‚è¯·ç¡®ä¿è¿™ç§é€‰é¡¹åœ¨å®é™…å•†ä¸šåœºæ™¯ä¸­é€šå¸¸æ˜¯ä¸å¯ç”¨çš„ğŸ˜‰
- en: In an unusual move in clustering, you could try to cluster data that hasnâ€™t
    been seen before. But note that this isnâ€™t a typical clustering task. A different
    and often more useful strategy would be to make a classifier using the cluster
    labels as targets. This will make it easier to assign cluster labels to new data.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨èšç±»ä¸­ï¼Œä½ å¯ä»¥å°è¯•å¯¹ä¹‹å‰æœªè§è¿‡çš„æ•°æ®è¿›è¡Œèšç±»ã€‚ä½†è¯·æ³¨æ„ï¼Œè¿™å¹¶ä¸æ˜¯ä¸€ä¸ªå…¸å‹çš„èšç±»ä»»åŠ¡ã€‚ä¸€ç§ä¸åŒä¸”é€šå¸¸æ›´æœ‰ç”¨çš„ç­–ç•¥æ˜¯ä½¿ç”¨èšç±»æ ‡ç­¾ä½œä¸ºç›®æ ‡æ¥æ„å»ºåˆ†ç±»å™¨ã€‚è¿™å°†ä½¿å¾—å°†èšç±»æ ‡ç­¾åˆ†é…ç»™æ–°æ•°æ®å˜å¾—æ›´å®¹æ˜“ã€‚
- en: And, finally, a fresh [interactive perspective](https://medium.com/analytics-vidhya/visualizing-data-made-easy-with-prosphera-40f8994ee60f)
    on our data.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åï¼Œæä¾›ä¸€ä¸ªæ–°çš„[äº’åŠ¨è§†è§’](https://medium.com/analytics-vidhya/visualizing-data-made-easy-with-prosphera-40f8994ee60f)æ¥è§‚å¯Ÿæˆ‘ä»¬çš„æ•°æ®ã€‚
- en: So, that is how weâ€™ve delved into K-means clustering using the [kscorer package](https://pypi.org/project/kscorer/),
    which streamlines the process of finding the optimal number of clusters. Due to
    its complex metrics and parallel processing, it has proved to be a practical tool
    for data analysis.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: å› æ­¤ï¼Œè¿™å°±æ˜¯æˆ‘ä»¬å¦‚ä½•åˆ©ç”¨[kscoreråŒ…](https://pypi.org/project/kscorer/)æ·±å…¥ç ”ç©¶K-meansèšç±»ï¼Œè¯¥åŒ…ç®€åŒ–äº†å¯»æ‰¾æœ€ä½³èšç±»æ•°çš„è¿‡ç¨‹ã€‚ç”±äºå…¶å¤æ‚çš„æŒ‡æ ‡å’Œå¹¶è¡Œå¤„ç†ï¼Œå®ƒå·²è¢«è¯æ˜æ˜¯ä¸€ä¸ªå®ç”¨çš„æ•°æ®åˆ†æå·¥å…·ã€‚
