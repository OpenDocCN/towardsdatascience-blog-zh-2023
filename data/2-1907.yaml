- en: How I Achieved Top 10% in Europe’s Largest Machine Learning Competition with
    Stacked Ensembles
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**我如何通过堆叠集成模型在欧洲最大机器学习竞赛中获得前 10%**'
- en: 原文：[https://towardsdatascience.com/stacked-ensembles-for-advanced-predictive-modeling-with-h2o-ai-and-optuna-8c339f8fb602](https://towardsdatascience.com/stacked-ensembles-for-advanced-predictive-modeling-with-h2o-ai-and-optuna-8c339f8fb602)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/stacked-ensembles-for-advanced-predictive-modeling-with-h2o-ai-and-optuna-8c339f8fb602](https://towardsdatascience.com/stacked-ensembles-for-advanced-predictive-modeling-with-h2o-ai-and-optuna-8c339f8fb602)
- en: A conceptual and hands-on coding guide to training Stacked Ensembles with H2O.ai
    and Optuna
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 关于使用 H2O.ai 和 Optuna 训练堆叠集成模型的概念性和实践编码指南
- en: '[](https://medium.com/@sheilateozy?source=post_page-----8c339f8fb602--------------------------------)[![Sheila
    Teo](../Images/de3e697ba84d4896bdd869a9367049f4.png)](https://medium.com/@sheilateozy?source=post_page-----8c339f8fb602--------------------------------)[](https://towardsdatascience.com/?source=post_page-----8c339f8fb602--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----8c339f8fb602--------------------------------)
    [Sheila Teo](https://medium.com/@sheilateozy?source=post_page-----8c339f8fb602--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@sheilateozy?source=post_page-----8c339f8fb602--------------------------------)[![Sheila
    Teo](../Images/de3e697ba84d4896bdd869a9367049f4.png)](https://medium.com/@sheilateozy?source=post_page-----8c339f8fb602--------------------------------)[](https://towardsdatascience.com/?source=post_page-----8c339f8fb602--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----8c339f8fb602--------------------------------)
    [Sheila Teo](https://medium.com/@sheilateozy?source=post_page-----8c339f8fb602--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----8c339f8fb602--------------------------------)
    ·13 min read·Dec 18, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----8c339f8fb602--------------------------------)
    ·阅读时间 13 分钟·2023 年 12 月 18 日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/4aa1fd2dabd2834494a342ca18c09ddd.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4aa1fd2dabd2834494a342ca18c09ddd.png)'
- en: Image generated by DALL·E 3
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图像由 DALL·E 3 生成
- en: We all know that ensemble models outperform any singular model at predictive
    modeling. You’ve probably heard all about Bagging and Boosting as common ensemble
    methods, with Random Forests and Gradient Boosting Machines as respective examples.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 我们都知道，集成模型在预测建模中优于任何单一模型。你可能听说过 Bagging 和 Boosting 作为常见的集成方法，以随机森林和梯度提升机作为各自的例子。
- en: But what about ensembling different models together under a separate higher-level
    model? This is where stacked ensembles comes in. **This article is step-by-step
    guide on how to train stacked ensembles using the popular machine learning library,
    H2O.**
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，如何在一个独立的更高层次模型下组合不同的模型呢？这就是堆叠集成模型的作用所在。**本文是如何使用流行的机器学习库 H2O 训练堆叠集成模型的逐步指南。**
- en: To demonstrate the power of stacked ensembles, I will provide a walk-through
    of my full code for training a stacked ensemble of 40 Deep Neural Network, XGBoost
    and LightGBM models for the prediction task posed in the 2023 Cloudflight Coding
    Competition (AI Category), one of the largest coding competitions in Europe, where
    I placed top 10% on the competition leaderboard within a training time of 1 hour!
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 为了展示堆叠集成模型的强大功能，我将提供一个完整的代码演示，介绍如何训练一个由 40 个深度神经网络、XGBoost 和 LightGBM 模型组成的堆叠集成模型，以完成
    2023 年 Cloudflight 编程竞赛（AI 类别）中的预测任务。这是欧洲最大的编程竞赛之一，我在其中的训练时间为 1 小时内获得了前 10% 的名次！
- en: '**This guide will cover:**'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '**本指南将涵盖：**'
- en: '[**What are stacked ensembles and how do they work?**](#97e3)'
  id: totrans-12
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[**什么是堆叠集成模型，它们是如何工作的？**](#97e3)'
- en: '[**How to train stacked ensembles with H2O.ai**](#ee48) **—'
  id: totrans-13
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[**如何使用 H2O.ai 训练堆叠集成模型**](#ee48) **—'
- en: With a full code walk-through in Python**
  id: totrans-14
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 通过 Python 的完整代码演示**
- en: '[**Comparing the performance of a stacked ensemble versus standalone models**](#f414)'
  id: totrans-15
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[**比较堆叠集成模型与单独模型的性能**](#f414)'
- en: 1\. What are Stacked Ensembles and how do they work?
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 1\. 什么是堆叠集成模型，它们是如何工作的？
- en: 'A stacked ensemble combines predictions from multiple models through another,
    higher-level model, with the aim being to increase overall predictive performance
    by capitalizing on the unique strengths of each constituent model. It involves
    2 stages:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 堆叠集成模型通过另一个更高层次的模型将多个模型的预测结果进行结合，旨在通过利用每个组成模型的独特优势来提高整体预测性能。它包括两个阶段：
- en: '**Stage 1: Multiple Base Models**'
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**阶段 1：多个基础模型**'
- en: First, multiple base models are independently trained on the same training dataset.
    These models should ideally be diverse, ranging from simple linear regressions
    to complex deep learning models. The key is that they should differ from each
    other in some way, either in terms of using a different algorithm or using the
    same algorithm but with different hyperparameter settings.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，多个基础模型在相同的训练数据集上独立训练。这些模型应当尽可能多样化，从简单的线性回归到复杂的深度学习模型都可以。关键是它们在某些方面应有所不同，无论是使用不同的算法还是使用相同的算法但具有不同的超参数设置。
- en: '**The more diverse the base models are, the more powerful the eventual stacked
    ensemble.** This is because different models are able to capture different patterns
    in the data. For example, a tree-based model might be good at capturing non-linear
    relationships, while a linear model excels at understanding linear trends. When
    these diverse base models are combined, the stacked ensemble can then leverage
    the different strengths of each base model, increasing predictive performance.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '**基础模型的多样性越高，最终的堆叠集成模型越强大。** 这是因为不同的模型能够捕捉数据中的不同模式。例如，基于树的模型可能擅长捕捉非线性关系，而线性模型擅长理解线性趋势。当这些多样化的基础模型结合在一起时，堆叠集成模型可以利用每个基础模型的不同优势，从而提高预测性能。'
- en: 'Stage 2: One Meta-Model'
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第二阶段：一个元模型
- en: After all the base models are trained, each base model’s predictions for the
    target is used as a feature for training a higher-level model, termed a meta-model.
    This means that the meta-model is not trained on the original dataset’s features,
    but instead on the predictions of the base models. If there are `n` base models,
    there are `n` predictions generated, and these are the `n` features used for training
    the meta-model.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在所有基础模型训练完成后，每个基础模型对目标的预测将用作训练一个更高层次模型（称为元模型）的特征。这意味着元模型不是在原始数据集的特征上训练，而是在基础模型的预测上进行训练。如果有`n`个基础模型，则生成`n`个预测，这些预测就是用于训练元模型的`n`个特征。
- en: While the training features differ between the base models and the meta-model,
    the target however stays the same, which is the original target from the dataset.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然训练特征在基础模型和元模型之间有所不同，但目标保持不变，即数据集中的原始目标。
- en: The meta-model learns how to best combine the predictions from the base models
    to make a final, more accurate prediction.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 元模型学习如何最佳地结合基础模型的预测，以做出最终的、更准确的预测。
- en: Detailed Steps for Training a Stacked Ensemble
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 堆叠集成训练的详细步骤
- en: '**For each base model:**'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '**对于每个基础模型：**'
- en: 1\. Pick an algorithm (eg. Random Forest).
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 1\. 选择一个算法（例如，随机森林）。
- en: 2\. Use cross-validation to obtain the best set of hyperparameters for the algorithm.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 2\. 使用交叉验证获得算法的最佳超参数集合。
- en: 3\. Obtain cross-validation predictions for the target in the training set.
    These will be used to train the meta-model subsequently.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 3\. 获取训练集中目标的交叉验证预测。这些将随后用于训练元模型。
- en: '*To illustrate this, say a Random Forest algorithm was chosen in Step 1, and
    its optimal hyperparameters were determined as* `*h*` *in Step 2.*'
  id: totrans-30
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*为了说明这一点，假设在步骤1中选择了随机森林算法，并且在步骤2中确定了其最优超参数为* `*h*` *。*'
- en: ''
  id: totrans-31
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*The cross-validation predictions are obtained through the following, assuming
    5-fold cross-validation:'
  id: totrans-32
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*交叉验证预测是通过以下方式获得的，假设使用5折交叉验证：'
- en: 1\. Train a Random Forest with hyperparamters* `*h*` *on Folds 1–4.
  id: totrans-33
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 1\. 在第1到第4折上训练具有超参数* `*h*` *的随机森林。
- en: 2\. Used the trained Random Forest to make predictions for Fold 5\. These are
    the cross-validation predictions for Fold 5.
  id: totrans-34
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 2\. 使用训练好的随机森林对第5折进行预测。这些是第5折的交叉验证预测。
- en: 3\. Repeat the above to obtain cross-validation predictions for each fold. After
    which, cross-validation predictions for the target will be obtained for the entire
    training set.*
  id: totrans-35
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 3\. 重复上述步骤以获取每一折的交叉验证预测。之后，将获得整个训练集的目标交叉验证预测。*
- en: '**For the meta-model:**'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '**对于元模型：**'
- en: 1\. Obtain the features for training the meta-model. These are the predictions
    of each of the base models.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 1\. 获取用于训练元模型的特征。这些是每个基础模型的预测。
- en: 2\. Obtain the target for training the meta-model. This is the original target
    from the training set.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 2\. 获取用于训练元模型的目标。这是来自训练集的原始目标。
- en: 3\. Pick an algorithm (eg. Linear Regression).
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 3\. 选择一个算法（例如，线性回归）。
- en: 4\. Use cross-validation to obtain the best set of hyperparameters for the algorithm.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 4\. 使用交叉验证获得算法的最佳超参数集合。
- en: 'And voila! You now have:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 看，完成了！你现在拥有：
- en: '- Multiple base models that are trained with optimal hyperparameters'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '- 多个经过优化超参数训练的基础模型'
- en: '- One meta-model that is also trained with optimal hyperparameters'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '- 一个也经过优化超参数训练的元模型'
- en: Which means you have successfully trained a stacked ensemble!
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着你已经成功训练了一个堆叠集成模型！
- en: 2\. How to Train Stacked Ensembles with H2O.ai
  id: totrans-45
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 2\. 如何使用 H2O.ai 训练堆叠集成模型
- en: Now, let’s jump into coding it out!
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们开始编写代码吧！
- en: As mentioned, this section covers my full code for training a stacked ensemble
    for the prediction task posed in the 2023 Cloudflight Coding Competition (AI Category),
    which is a regression task using tabular data. Within the competition’s time constraints,
    I created a stacked ensemble from 40 base models of 3 algorithm types — Deep Neural
    Network, XGBoost, and LightGBM, with these specific algorithms chosen as they
    often achieve superior performance in practice.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，本节包括我为 2023 Cloudflight 编码竞赛（AI 类别）中的预测任务训练堆叠集成模型的完整代码，这是一个使用表格数据的回归任务。在竞赛时间限制下，我从
    40 个基础模型中创建了一个堆叠集成模型，涉及 3 种算法类型——深度神经网络、XGBoost 和 LightGBM，这些特定算法被选择是因为它们在实践中通常表现优异。
- en: 2.1\. Data Preparation
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.1\. 数据准备
- en: First, let’s import the necessary libraries.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们导入必要的库。
- en: '[PRE0]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: And initialize the H2O cluster.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 并初始化 H2O 集群。
- en: '[PRE1]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Next, load in the dataset.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，加载数据集。
- en: '[PRE2]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Before moving on to model building using H2O, let’s first understand the following
    traits of H2O models:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用 H2O 进行模型构建之前，让我们首先了解 H2O 模型的以下特性：
- en: H2O models cannot take in Pandas DataFrame objects, so `data` must be converted
    from a Pandas DataFrame to its H2O equivalent, which is a H2OFrame.
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: H2O 模型不能接收 Pandas DataFrame 对象，因此 `data` 必须从 Pandas DataFrame 转换为 H2O 对应的 H2OFrame。
- en: H2O models can encode categorical features automatically, which is great as
    it takes this preprocessing step out of our hands. To ensure that such features
    are understood by the models to be categorical, they must be explicitly converted
    into the factor (categorical) data type.
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: H2O 模型可以自动编码分类特征，这很好，因为这一步预处理被省略了。为了确保这些特征被模型识别为分类特征，它们必须显式转换为因子（分类）数据类型。
- en: '[PRE3]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Now we can proceed to split our dataset into train (90%) and validation (10%)
    sets, using the `split_frame()` method of H2OFrame objects.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以使用 H2OFrame 对象的 `split_frame()` 方法将数据集拆分为训练集（90%）和验证集（10%）。
- en: '[PRE4]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Lastly, let’s obtain the features and target for modelling. Unlike Scikit-Learn
    models which take as input the *values* of the features and the target, H2O models
    take as input the *names* of the features and the target.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们获取建模的特征和目标。与接受*特征*和目标的*值*的 Scikit-Learn 模型不同，H2O 模型接受的是*特征*和目标的*名称*。
- en: '[PRE5]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Now, let the model training fun begin!
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让模型训练的乐趣开始吧！
- en: 2.2\. Training Deep Neural Networks (DNN) as Base Models
  id: totrans-64
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.2\. 训练深度神经网络（DNN）作为基础模型
- en: Let’s start by training the DNNs that will form our set of base models for the
    stacked ensemble, using H2O’s `H2ODeepLearningEstimator`.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 我们先从训练将组成堆叠集成模型的 DNN 开始，使用 H2O 的 `H2ODeepLearningEstimator`。
- en: '**Aside: Why train DNNs in H2O, instead of Tensorflow, Keras, or PyTorch?**'
  id: totrans-66
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**附注：为什么在 H2O 中训练 DNN，而不是使用 Tensorflow、Keras 或 PyTorch？**'
- en: ''
  id: totrans-67
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*Before jumping into the code for this, you might be wondering why I chose
    to train DNNs using H2O’s* `*H2ODeepLearningEstimator*`*, as opposed to using
    Tensorflow, Keras, or PyTorch, which are the common libraries used to build DNNs.*'
  id: totrans-68
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*在跳入代码之前，你可能会好奇为什么我选择使用 H2O 的* `*H2ODeepLearningEstimator*`* 来训练 DNN，而不是使用
    Tensorflow、Keras 或 PyTorch 这些常用的 DNN 构建库。*'
- en: ''
  id: totrans-69
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*The straightforward answer is that building a stacked ensemble in H2O uses
    the* `*H2OStackedEnsembleEstimator*`*, which can only accept base models that
    are part of the H2O model family. However, the more critical reason is that H2O’s*
    `*H2ODeepLearningEstimator*` *enables far easier tuning of DNNs than these other
    frameworks, and here’s why.*'
  id: totrans-70
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*简单的答案是，在 H2O 中构建堆叠集成模型使用的是* `*H2OStackedEnsembleEstimator*`*，它只能接受 H2O 模型家族中的基础模型。然而，更关键的原因是
    H2O 的* `*H2ODeepLearningEstimator*` *比其他框架更容易调整 DNN，这就是原因。*'
- en: ''
  id: totrans-71
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*In TensorFlow, Keras, or PyTorch, regularization effects like dropout layers
    must be manually added into the model architecture, such as using* `*keras.layers.Dropout()*`*.
    This allows for greater customization, but also requires more detailed knowledge
    and effort. For example, you have to decide where and how many times to include
    the* `*keras.layers.Dropout()*` *layer within your model architecture.*'
  id: totrans-72
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*在TensorFlow、Keras或PyTorch中，像dropout层这样的正则化效果必须手动添加到模型架构中，例如使用* `*keras.layers.Dropout()*`*。这允许更大的自定义，但也需要更多的详细知识和精力。例如，你必须决定在模型架构中在哪里以及多少次包括*
    `*keras.layers.Dropout()*` *层。*'
- en: ''
  id: totrans-73
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*On the other hand, H2O''s* `*H2ODeepLearningEstimator*` *is more abstracted
    and accessible to the layman. Regularization can be enabled in a straightforward
    manner through model hyperparameters, reducing the need for manual setup of these
    components as layers. Furthermore, the* default *model hyperparameters already
    includes regularization. The common feature preprocessing steps, such as scaling
    of numerical features and encoding of categorical features, are also included
    as model hyperparameters for automatic feature preprocessing. These enable the
    tuning of DNNs to be a far more straightforward and easy process, without having
    to dive into the complexities of deep learning model architecture. In the context
    of a time crunch in the competition, this was extremely useful for me!*'
  id: totrans-74
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*另一方面，H2O的* `*H2ODeepLearningEstimator*` *更加抽象，易于普通人使用。可以通过模型超参数以简单的方式启用正则化，从而减少手动设置这些组件为层的需求。此外，*
    默认 *的模型超参数已包含正则化。常见的特征预处理步骤，例如数值特征的缩放和分类特征的编码，也作为模型超参数包含在内，以实现自动特征预处理。这些功能使得调整深度神经网络（DNN）的过程变得更加直接和简单，而无需深入了解深度学习模型架构的复杂性。在比赛的时间紧迫背景下，这对我来说非常有用！*'
- en: But which set of hyperparameters should we train `H2ODeepLearningEstimator`
    with? This is where *optuna* comes in. *Optuna* is a hyperparameter optimization
    framework, similar to the traditional grid search and random search approaches,
    but better in that it employs a more sophisticated approach.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，我们应该使用哪一组超参数来训练`H2ODeepLearningEstimator`呢？这就是*optuna*的作用所在。*Optuna*是一个超参数优化框架，类似于传统的网格搜索和随机搜索方法，但它采用了更复杂的方法。
- en: Grid search systematically explores a predefined range of hyperparameter values,
    while random search selects random combinations within these specified limits.
    However, *optuna* uses Bayesian optimization to learn from previous searches to
    propose better-performing hyperparameter sets in each subsequent search, increasing
    the efficiency of its search for the optimal model hyperparameters. This is especially
    effective in complex and large hyperparameter spaces where traditional search
    methods can be prohibitively time-consuming and may eventually still fail to locate
    the optimal set of hyperparameters.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 网格搜索系统地探索预定义范围内的超参数值，而随机搜索则在这些指定的限制范围内选择随机组合。然而，*optuna*利用贝叶斯优化，从之前的搜索中学习，提出每次后续搜索中表现更好的超参数集，提高了寻找最佳模型超参数的效率。这在复杂且大型的超参数空间中尤其有效，在这些空间中，传统的搜索方法可能会耗时极长，最终仍可能无法找到最佳的超参数集。
- en: Now, let’s get into the code. We’ll use *optuna* to tune the hyperparameters
    of H2O’s `H2ODeepLearningEstimator`, and keep track of all the trained models
    inside the list `dnn_models`.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们进入代码部分。我们将使用*optuna*来调整H2O的`H2ODeepLearningEstimator`的超参数，并将所有训练过的模型跟踪到列表`dnn_models`中。
- en: '[PRE6]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Above, an *optuna* study is created to search for the best set of `H2ODeepLearningEstimator`
    hyperparameters that minimizes the cross-validation RMSE (as this is a regression
    task), with the optimization process running for 20 trials using the parameter
    `n_trials=20`. This means that 20 DNNs are trained and stored in the list `dnn_models`
    for usage as base models for the stacked ensemble later on, each with a different
    set of hyperparameters. In the interest of time under the competition’s time constraints,
    I chose to train 20 DNNs, but you can set `n_trials` to be however many DNNs you
    wish to train for your stacked ensemble.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 上述*optuna*研究用于寻找能够最小化交叉验证RMSE的`H2ODeepLearningEstimator`超参数集（因为这是一个回归任务），优化过程运行20次试验，使用参数`n_trials=20`。这意味着训练了20个DNN，并将它们存储在列表`dnn_models`中，以供稍后作为堆叠集成的基模型使用，每个模型都有不同的超参数集。考虑到比赛的时间限制，我选择训练20个DNN，但你可以根据自己的需求设置`n_trials`为你希望训练的DNN数量。
- en: Importantly, the `H2ODeepLearningEstimator` must be trained with `keep_cross_validation_predictions=True`,
    as these cross-validation predictions will be used as features for training the
    meta-model later.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是，`H2ODeepLearningEstimator`必须使用`keep_cross_validation_predictions=True`进行训练，因为这些交叉验证预测将被用作训练元模型的特征。
- en: 2.3\. Training XGBoost and LightGBM as Base Models
  id: totrans-81
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.3\. 将XGBoost和LightGBM作为基模型进行训练
- en: Next, let’s train the XGBoost and LightGBM models that will also form our set
    of base models for the stacked ensemble. We’ll again use *optuna* to tune the
    hyperparameters of H2O’s `H2OXGBoostEstimator`, and keep track of all the trained
    models inside the list `xgboost_lightgbm_models`.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们训练将形成堆叠集成基模型集合的XGBoost和LightGBM模型。我们将再次使用*optuna*来调优H2O的`H2OXGBoostEstimator`的超参数，并将所有训练过的模型记录在`xgboost_lightgbm_models`列表中。
- en: Before diving into the code for this, we must first understand that `H2OXGBoostEstimator`
    is the integration of the XGBoost framework from the popular *xgboost* library
    into H2O. On the other hand, H2O does not integrate the *lightgbm* library. However,
    it does provide a method for emulating the LightGBM framework using a certain
    set of parameters within `H2OXGBoostEstimator`- and this is exactly what we will
    implement in order to train both XGBoost and LightGBM models using `H2OXGBoostEstimator`.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入代码之前，我们必须首先了解`H2OXGBoostEstimator`是将流行的*xgboost*库中的XGBoost框架集成到H2O中。另一方面，H2O并未集成*lightgbm*库。然而，它确实提供了一种方法，通过在`H2OXGBoostEstimator`中使用特定参数集来模拟LightGBM框架——这正是我们将实现的，以便使用`H2OXGBoostEstimator`来训练XGBoost和LightGBM模型。
- en: '[PRE7]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Similarly, 20 XGBoost and LightGBM models are trained and stored in the list
    `xgboost_lightgbm_models` for usage as base models for the stacked ensemble later
    on, each with a different set of hyperparameters. You can set `n_trials` to be
    however many XGBoost/LightGBM models you wish to train for your stacked ensemble.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，20个XGBoost和LightGBM模型被训练并存储在`xgboost_lightgbm_models`列表中，以备后续作为堆叠集成的基模型使用，每个模型有一组不同的超参数。你可以设置`n_trials`为任何你希望训练的XGBoost/LightGBM模型的数量。
- en: Importantly, the `H2OXGBoostEstimator` must also be trained with `keep_cross_validation_predictions=True`,
    as these cross-validation predictions will be used as features for training the
    meta-model later.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是，`H2OXGBoostEstimator`还必须使用`keep_cross_validation_predictions=True`进行训练，因为这些交叉验证预测将被用作训练元模型的特征。
- en: 2.4\. Training the Meta-Model
  id: totrans-87
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.4\. 训练元模型
- en: We will use *all* of the Deep Neural Network, XGBoost and LightGBM models trained
    above as base models. However, this does not mean that all of them will be used
    in the stacked ensemble, as we will perform automatic base model selection when
    tuning our meta-model (more on this later)!
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用上述训练的*所有*深度神经网络、XGBoost和LightGBM模型作为基模型。然而，这并不意味着所有这些模型都会被用于堆叠集成，因为在调优我们的元模型时，我们将进行自动基模型选择（稍后会详细介绍）！
- en: Recall that we had stored each trained base model inside the lists `dnn_models`
    (20 models) and `xgboost_lightgbm_models` (20 models), giving a total of 40 base
    models for our stacked ensemble. Let’s combine them into a final list of base
    models, `base_models`.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，我们将每个训练过的基模型存储在`dnn_models`（20个模型）和`xgboost_lightgbm_models`（20个模型）列表中，总共为我们的堆叠集成提供了40个基模型。接下来，我们将它们合并成一个最终的基模型列表`base_models`。
- en: '[PRE8]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Now, we are ready to train the meta-model using these base models. But first,
    we have to decide on the meta-model algorithm, where a few concepts come into
    play:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们已经准备好使用这些基模型来训练元模型。但首先，我们必须决定元模型算法，在此过程中涉及一些概念：
- en: Most academic papers on stacked ensembles recommend choosing a **simple** linear-based
    algorithm for the meta-model. This is to avoid the meta-model overfitting to the
    predictions from the base models.
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 大多数关于堆叠集成的学术论文建议为元模型选择**简单**的线性算法。这是为了避免元模型对基模型的预测结果过拟合。
- en: H2O recommends the usage of a Generalized Linear Model (GLM) over a Linear Regression
    (for regression tasks) or Logistic Regression (for classification tasks). This
    is because the GLM is a flexible linear model that does not impose the key assumptions
    of normality and homoscedasticity that the latter do, allowing it to model the
    true behavior of the target values better, since such assumptions can be difficult
    to be met in practice. Further explanations on this can be found in [this academic
    thesis](https://github.com/ledell/phd-thesis/blob/main/ledell-phd-thesis.pdf),
    on which H2O’s work was based upon.
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: H2O 推荐在回归任务中使用广义线性模型（GLM）而非线性回归，或在分类任务中使用广义线性模型而非逻辑回归。这是因为 GLM 是一种灵活的线性模型，不像后者那样强加正态性和同方差性的关键假设，这使得它能够更好地建模目标值的真实行为，因为这些假设在实际操作中可能很难满足。关于这一点的进一步解释可以在[这篇学术论文](https://github.com/ledell/phd-thesis/blob/main/ledell-phd-thesis.pdf)中找到，H2O
    的工作就是基于这篇论文的。
- en: As such, we will instantiate the meta-model using `H2OStackedEnsembleEstimator`
    with `metalearner_algorithm='glm'`, and use *optuna* to tune the hyperparameters
    of the GLM meta-model to optimize performance.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们将使用 `H2OStackedEnsembleEstimator` 并设置 `metalearner_algorithm='glm'` 来实例化元模型，并使用
    *optuna* 调整 GLM 元模型的超参数以优化性能。
- en: '[PRE9]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Notice that the cross-validation predictions of each base model were not explicitly
    passed into `H2OStackedEnsembleEstimator`. This is because H2O does this automatically
    under the hood, making things easier for us! All we had to do was set `keep_cross_validation_predictions=True`
    when training our base models previously, and instantiate `H2OStackedEnsembleEstimator`
    with the parameter `base_models=base_models`.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，每个基础模型的交叉验证预测没有被显式地传递给 `H2OStackedEnsembleEstimator`。这是因为 H2O 在后台自动完成了这一步骤，使我们更加轻松！我们只需在之前训练基础模型时设置
    `keep_cross_validation_predictions=True`，并使用参数 `base_models=base_models` 实例化 `H2OStackedEnsembleEstimator`
    即可。
- en: Now, we can finally build the `best_ensemble` model, using the optimal hyperparameters
    found by *optuna*.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以最终构建 `best_ensemble` 模型，使用 *optuna* 找到的最佳超参数。
- en: '[PRE10]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: And voila, we have successfully trained a stacked ensemble in H2O! Let’s take
    a look at it.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 完成了，我们成功地在 H2O 中训练了一个堆叠集成模型！让我们来看看吧。
- en: '[PRE11]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '![](../Images/f03397fcef61f99f6cb428a7698d2f5d.png)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f03397fcef61f99f6cb428a7698d2f5d.png)'
- en: Image by author
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片
- en: Notice that the stacked ensemble uses only 16 out of the 40 base models we passed
    to it, of which 3 are XGBoost/LightGBM and 13 are Deep Neural Networks. This is
    due to the hyperparameter `alpha` that we tuned for the GLM meta-model, which
    represents the distribution of regularization between L1 (LASSO) and L2 (Ridge).
    A value of `1` entails only L1 regularization, while a value of `0` entails only
    L2 regularization.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 注意到堆叠集成模型只使用了我们提供的 40 个基础模型中的 16 个，其中 3 个是 XGBoost/LightGBM 模型，13 个是深度神经网络模型。这是因为我们为
    GLM 元模型调整的超参数`alpha`，它代表了 L1（LASSO）和 L2（Ridge）之间的正则化分布。`1` 的值仅涉及 L1 正则化，而 `0`
    的值仅涉及 L2 正则化。
- en: As reflected above, its optimal value was found to be `alpha=0.16`, thus a mix
    of L1 and L2 was employed. Some of the base models’ predictions had their coefficients
    in the regression set to `0` under L1 regularization, meaning that these base
    models were not used in the stacked ensemble at all, therefore fewer than 40 base
    models ended up being used.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 如上所述，最佳值被发现为 `alpha=0.16`，因此使用了 L1 和 L2 的混合。一些基础模型的预测在 L1 正则化下的回归中系数被设置为 `0`，这意味着这些基础模型在堆叠集成中完全没有被使用，因此实际使用的基础模型数量少于
    40 个。
- en: The key takeaway here is that our setup above also performs automatic selection
    of which base models to use for optimal performance, through the meta-model’s
    regularization hyperparameters, instead of simply using all 40 base models provided.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的关键要点是，我们上面的设置还通过元模型的正则化超参数自动选择了用于最佳性能的基础模型，而不是简单地使用所有提供的 40 个基础模型。
- en: '3\. Comparing Performance: Stacked Ensemble Versus Standalone Base Models'
  id: totrans-106
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 3\. 性能比较：堆叠集成与独立基础模型
- en: To demonstrate the power of stacked ensembles, let’s use it to generate predictions
    for the validation set, which was held out from the beginning. The RMSE figures
    below are specific only to the dataset I am using, but feel free to run this article’s
    codes on your own dataset too, and see the difference in model performance for
    yourself!
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 为了展示堆叠集成的威力，让我们使用它为从一开始就被保留的验证集生成预测。下面的 RMSE 数字仅针对我使用的数据集，但也可以在自己的数据集上运行本文的代码，亲自查看模型性能的差异！
- en: '[PRE12]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: The stacked ensemble produces an RMSE of 0.31 on the validation set.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 堆叠集成在验证集上产生了0.31的RMSE。
- en: Next, let’s dig into the performance of each of the base models on this same
    validation set.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们深入分析每个基础模型在这个相同验证集上的表现。
- en: '[PRE13]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '![](../Images/853f94aa62127c3dce0a3c3c83c84219.png)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/853f94aa62127c3dce0a3c3c83c84219.png)'
- en: Image by author
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供
- en: Compared to the stacked ensemble which achieved an RMSE of 0.31, the best-performing
    standalone base model achieved an RMSE of 0.35.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 与实现了0.31 RMSE的堆叠集成模型相比，表现最佳的独立基础模型的RMSE为0.35。
- en: This means that Stacking was able to improve predictive performance by 11% on
    unseen data!
  id: totrans-115
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 这意味着堆叠方法能够在未见数据上提高预测性能11%！
- en: Now that you’ve witnessed the power of stacked ensembles, it’s your turn to
    try them out!
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经见证了堆叠集成的威力，轮到你亲自尝试了！
- en: I had a lot of fun writing this, and if you had fun reading, I would really
    appreciate if you took a second to leave some claps and a follow!
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 写这篇文章我感到非常愉快，如果你阅读起来也觉得有趣，我会非常感激你花一点时间留下点赞和关注！
- en: See you in the next one!
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 下次见！
- en: Sheila
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: Sheila
