- en: Finding Our Way Through a Random Forest
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通过随机森林找到我们的路
- en: 原文：[https://towardsdatascience.com/finding-our-way-through-a-random-forest-5ff6c1382572](https://towardsdatascience.com/finding-our-way-through-a-random-forest-5ff6c1382572)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/finding-our-way-through-a-random-forest-5ff6c1382572](https://towardsdatascience.com/finding-our-way-through-a-random-forest-5ff6c1382572)
- en: Or how, in a hypothetical world plagued by zombies, decision trees can make
    the difference between being out of the woods or not
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 或者在一个被僵尸困扰的假想世界里，决策树如何能决定你是否能够脱身
- en: '[](https://medium.com/@manfred.james?source=post_page-----5ff6c1382572--------------------------------)[![Diego
    Manfre](../Images/2189d8e63df449a869526bf8b6c50440.png)](https://medium.com/@manfred.james?source=post_page-----5ff6c1382572--------------------------------)[](https://towardsdatascience.com/?source=post_page-----5ff6c1382572--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----5ff6c1382572--------------------------------)
    [Diego Manfre](https://medium.com/@manfred.james?source=post_page-----5ff6c1382572--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@manfred.james?source=post_page-----5ff6c1382572--------------------------------)[![Diego
    Manfre](../Images/2189d8e63df449a869526bf8b6c50440.png)](https://medium.com/@manfred.james?source=post_page-----5ff6c1382572--------------------------------)[](https://towardsdatascience.com/?source=post_page-----5ff6c1382572--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----5ff6c1382572--------------------------------)
    [Diego Manfre](https://medium.com/@manfred.james?source=post_page-----5ff6c1382572--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----5ff6c1382572--------------------------------)
    ·17 min read·Apr 19, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----5ff6c1382572--------------------------------)
    ·阅读时长17分钟·2023年4月19日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/42de49ee660a66ca0f4cef619e194d77.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/42de49ee660a66ca0f4cef619e194d77.png)'
- en: Image made by the author using Midjourney
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者使用 Midjourney 制作
- en: '*Outside the garage, the growls and snarls did not stop. He could not believe
    that the Zombie Apocalypse he had watched many times in series and movies was
    finally on his front porch. He could wait hidden in the garage for some time but
    had to come out eventually. Should he take an axe with him or would the rifle
    be enough? He could try to find some food but, should he go alone? He tried to
    remember all the zombie movies he had seen but could not agree on a single strategy.
    If he only had a way of remembering every scene where a character is killed by
    zombies, would that be enough to increase his chances of survival? If he just
    had a decision guide everything would be simpler…*'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '*车库外面，咆哮和咆哮声不断。他简直不敢相信自己在系列剧和电影中多次看到的僵尸末日最终出现在了自己门前。他可以在车库里藏一段时间，但最终还是得出来。他该带斧头还是仅用步枪就够了？他可以试着找些食物，但应该一个人去吗？他试图回忆起所有看过的僵尸电影，却无法达成一致的策略。如果他能记住每个角色被僵尸杀死的场景，这是否能增加他的生存几率？如果他有一个决策指南，一切都会更简单……*'
- en: Introduction
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: Have you ever watched one of those zombie apocalypse movies where there is one
    character that always seems to know where the zombies are hidden or if it is better
    to fight with them or run away? Does this person really know what is going to
    happen? Did someone tell him/her beforehand? Maybe there is nothing magical about
    this. Maybe this person has read a lot of comics about zombies and it is really
    good at knowing what to do in each case and learning from others’ mistakes. How
    important it is to find the best way of using the events of the past as a guide
    for our decisions! This guide, also known as a decision tree, is a widely used
    supervised learning algorithm. This article is an introductory discussion about
    decision trees, how to build them and why many of them create a random forest.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 你是否看过那些僵尸末日电影，其中总有一个角色似乎总是知道僵尸藏在哪里，或者知道是该战斗还是逃跑？这个人真的知道接下来会发生什么吗？有人事先告诉过他/她吗？也许这并没有什么神奇的地方。也许这个人读过很多关于僵尸的漫画，并且非常擅长根据每种情况做出正确的决策，学习别人的错误。找到使用过去事件作为我们决策指南的最佳方式是多么重要！这个指南，也称为决策树，是一种广泛使用的监督学习算法。本文是关于决策树的入门讨论，讲述了如何构建它们以及为什么许多决策树会创建一个随机森林。
- en: A simple decision
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 一个简单的决策
- en: You are in the middle of the zombie mayhem and you want to know how to increase
    your chances of survival. At this point, you only have information from 15 of
    your friends. For each one of them you know if they were alone, if they had a
    vehicle or a weapon or if they were trained to fight. Most importantly, you know
    if they were able to survive or not. How can you use this information to your
    advantage?
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 你处在僵尸混乱之中，你想知道如何提高生存的机会。在这一点上，你只有15个朋友的信息。你知道每一个朋友是否孤身一人，是否有车辆或武器，或者是否经过战斗训练。最重要的是，你知道他们是否能够生存下来。你如何利用这些信息来为自己争取优势？
- en: 'Table 1 summarizes the results and characteristics of your 15 friends. You
    want to be like the 3 of them that survived in the end. What do these 3 friends
    have in common? A simple inspection of the table will tell us that the three survivors
    had all these things in common: they were not alone, they were trained to fight,
    and they had a vehicle and a weapon. So, will you be able to survive if you had
    all these four things? Past experiences are telling us that you might! If you
    had to decide what to take with you and whether to be on your own or not, at least
    you now have some historical data to support your decision.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 表1总结了你15个朋友的结果和特征。你想成为最终幸存的那3个人中的一员。这3个朋友有什么共同点？简单检查表格会告诉我们这三位幸存者有这些共同点：他们并不孤单，他们经过了战斗训练，他们有车辆和武器。那么，如果你拥有这四样东西，你能幸存吗？过去的经验告诉我们，你有可能！如果你必须决定带上什么以及是否独自一人，至少现在你有一些历史数据来支持你的决定。
- en: '![](../Images/e946a0470d4ed9093000bc63ba4b35cc.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e946a0470d4ed9093000bc63ba4b35cc.png)'
- en: '*Table 1\. Features of 15 individuals and their final outcome in the Zombie
    Apocalypse for Example 1 (Table made by the author).*'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '*表1\. 15个个体的特征及其在僵尸末日中的最终结果示例1（表由作者制作）。*'
- en: A harder decision
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 更困难的决定
- en: 'Zombie apocalypses are never as simple as they look. Let’s say that instead
    of the 15 friends from the previous example, you have the following friends:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 僵尸末日永远不像它看起来那么简单。假设这次你有以下这些朋友，而不是前面例子中的15个朋友：
- en: '![](../Images/1d866c4ed9a2ae935240193408c2876b.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1d866c4ed9a2ae935240193408c2876b.png)'
- en: Table 2\. Features of 15 individuals and their final outcome in the Zombie Apocalypse
    for Example 2 *(Table made by the author).*
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 表2\. 15个个体的特征及其在僵尸末日中的最终结果示例2 *(表由作者制作)。*
- en: This time, reaching a conclusion only by visual inspection is not that simple.
    The one thing we know for sure is that if you want to survive, you better have
    someone by your side. The 5 people that survived were not alone (Figure 1). Besides
    this, it is difficult to see if there is a particular combination of things that
    will lead you to survival. Some people were able to survive although they were
    alone. How did they do? If you know you will be alone, what else can you do to
    increase your chances of surviving? Is there anything like a decision roadmap?
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 这一次，仅通过视觉检查得出结论并不那么简单。我们唯一可以确定的是，如果你想要生存，最好身边有一个人。幸存的5个人并不是孤身一人（图1）。除此之外，很难看出是否存在某种特定的组合可以让你存活下来。有些人虽然孤身一人仍然能够生存。他们是怎么做到的？如果你知道自己会孤身一人，还有什么可以做的来增加生存的机会？是否存在类似决策路线图的东西？
- en: '![](../Images/c9c5ff10bafc7a9f30f9053c72a669a1.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c9c5ff10bafc7a9f30f9053c72a669a1.png)'
- en: Figure 1\. Effects of each of the features on the final outcome from the group
    of 15 individuals in Example 2 *(Table made by the author).*
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 图1\. 各特征对示例2中15个人群体最终结果的影响 *(图由作者制作)。*
- en: The Decision Tree
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 决策树
- en: We can find some answers to the previous questions in a [decision tree](https://en.wikipedia.org/wiki/Decision_tree_learning).
    A decision tree is a model of the predicted outcome we can find according to the
    decisions we make. This model is built using previous experiences. In our example,
    we can build a decision tree using the characteristics of the 15 friends and their
    outcomes. A decision tree consists of multiple decision nodes or branches. In
    each one of these nodes, we make a decision that will take us to the following
    node until we reach an outcome.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在[决策树](https://en.wikipedia.org/wiki/Decision_tree_learning)中找到一些对前面问题的答案。决策树是一个根据我们做出的决定来预测结果的模型。这个模型是通过之前的经验构建的。在我们的例子中，我们可以利用15个朋友的特征和他们的结果来构建决策树。决策树由多个决策节点或分支组成。在这些节点中，我们做出一个决策，决策将引导我们到下一个节点，直到我们得到一个结果。
- en: Growing a decision tree
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 构建决策树
- en: If someone asks you to draw a genealogical tree, you would probably start with
    your grandparents or great-grandparents. From there, the tree will grow through
    your parents, uncles and cousins, until it reaches you. In a similar way, to grow
    a decision tree you always start from a node that does the best separation of
    your data. From that point, the tree will start growing according to the feature
    that best divides the data. There are [many algorithms](https://en.wikipedia.org/wiki/Decision_tree_learning#Metrics)
    you can use to grow a decision tree. This article explains how to use the [information
    gain](https://en.wikipedia.org/wiki/Information_gain_in_decision_trees) and the
    [Shannon entropy](https://en.wikipedia.org/wiki/Entropy_(information_theory)).
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 如果有人让你画一个家谱，你可能会从你的祖父母或曾祖父母开始。从那里，家谱将通过你的父母、叔叔和表亲逐渐展开，直到到达你。类似地，要构建决策树，你总是从一个能够最好地分离数据的节点开始。从这一点开始，树将根据最佳分割数据的特征开始生长。有许多[算法](https://en.wikipedia.org/wiki/Decision_tree_learning#Metrics)可以用来构建决策树。本文解释了如何使用[信息增益](https://en.wikipedia.org/wiki/Information_gain_in_decision_trees)和[香农熵](https://en.wikipedia.org/wiki/Entropy_(information_theory))。
- en: 'Let’s focus on Table 2\. We can see that there are 5 people who survived and
    10 who died. This means that the probability of surviving is 5/15 = ⅓ and the
    probability of dying is ⅔. With this information, we can calculate the entropy
    of this distribution. In this case, entropy refers to the average level of surprise
    or uncertainty in this distribution. To calculate the entropy we use the following
    equation:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们关注表2\. 我们可以看到有 5 人幸存，10 人死亡。这意味着幸存的概率是 5/15 = ⅓，死亡的概率是 ⅔。利用这些信息，我们可以计算这种分布的熵。在这种情况下，熵指的是这种分布的平均惊讶或不确定性水平。为了计算熵，我们使用以下方程：
- en: '![](../Images/078fa06d576037b251bd37f05462f00f.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/078fa06d576037b251bd37f05462f00f.png)'
- en: Note that this equation can also be expressed in terms of one of the probabilities
    since *p(surv)*+*p(die)*=1\. If we plot this function you can see how the entropy
    has the highest value of one when both *p(surv)* and *p(die)* are equal to 0.5\.
    On the contrary, if the whole distribution corresponds to people who all survived
    or all died, the entropy is zero. So, the highest the entropy, the highest the
    uncertainty. The lowest the entropy, the more homogeneous the distribution is
    and the less “surprised” we will be about the outcome.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，这个方程也可以用其中一个概率来表示，因为 *p(surv)*+*p(die)*=1。如果我们绘制这个函数，你会看到当 *p(surv)* 和 *p(die)*
    都等于 0.5 时，熵的值达到 1 的最高值。相反，如果整个分布对应于所有人都生存或所有人都死亡的情况，则熵为零。因此，熵越高，*不确定性*越高。熵越低，分布越均匀，我们对结果的*惊讶*程度就越小。
- en: '![](../Images/454fc1395d6c1b5c4bb4519ab9fb3e8d.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/454fc1395d6c1b5c4bb4519ab9fb3e8d.png)'
- en: Figure 2\. Plot of the entropy as a function of the probability of each event.
    The green curve represents the probability of dying, the orange curve represents
    the probability of surviving and the blue curve is the sum of both probabilities
    since p(surv)+p(die)=1 *(Image made by the author).*
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 图2\. 熵作为每个事件概率的函数的图示。绿色曲线代表死亡的概率，橙色曲线代表生存的概率，蓝色曲线是这两种概率的总和，因为 p(surv)+p(die)=1
    *(由作者制作的图像)。*
- en: 'In our case, the number of survivors is less than half of the entire population.
    It would be reasonable to think that most people do not survive the zombie apocalypse.
    The entropy in this case is 0.92 which is what you get in the blue curve of Figure
    2 when you search for x=⅓ or ⅔ or when you apply the following equation:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的案例中，幸存者的数量不到总人口的一半。合理的推测是大多数人没有幸存于僵尸末日。在这种情况下，熵为 0.92，这就是你在图2的蓝色曲线中搜索 x=⅓
    或 ⅔ 时得到的值，或者当你应用以下方程时得到的值：
- en: '![](../Images/fcb683d3e34c60d007b7423cc2c49769.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fcb683d3e34c60d007b7423cc2c49769.png)'
- en: Now that we know the entropy or the degree of uncertainty of the entire distribution,
    what should we do? The next step is to find how to divide the data so we can keep
    that level of uncertainty.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们知道了整个分布的熵或不确定性的程度，我们应该怎么做？下一步是找到如何划分数据，以保持这种不确定性水平。
- en: The premise of the information gain consists of choosing the decision node that
    will reduce the less the level of entropy of the previous node. At this point,
    we are trying to find which is the best first separation of the data. Is it the
    fact that we are alone, that we know how to fight or that we have a vehicle or
    a weapon? To know the answer we can calculate what is the information gain of
    each one of these choices and then decide which one of them has the biggest gain.
    Remember that we are trying to minimize the change in the entropy which is the
    heterogeneity or level of surprise in the outcome distribution.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 信息增益的前提是选择能够最小化前一节点熵水平的决策节点。在这个阶段，我们正试图找出数据的最佳首次分离点。是我们单独存在、知道如何战斗，还是拥有交通工具或武器？为了知道答案，我们可以计算每个选择的信息增益，然后决定哪一个具有最大的增益。记住，我们试图最小化熵的变化，即结果分布中的异质性或惊讶程度。
- en: Are you trained to fight?
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 你受过战斗训练吗？
- en: 'Is this the first question you should ask yourself in this case? Will this
    question minimize the change in entropy of the outcome distribution? To know this,
    let’s calculate the entropy of each one of these two cases: we know how to fight
    and we do not know how to fight. Figure 3 shows that of the 9 people that knew
    how to fight, only 5 of them survived. On the contrary, all of the 6 people who
    were not trained to fight did not survive.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 这是你在这种情况下应该问自己的第一个问题吗？这个问题会最小化结果分布中熵的变化吗？为了知道这一点，让我们计算这两个案例的熵：我们知道如何战斗和我们不知道如何战斗。图3显示，在9名知道如何战斗的人中，只有5名幸存了。相反，所有6名没有接受战斗训练的人都没有生存下来。
- en: '![](../Images/2068323863de0c8549517166433366e1.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2068323863de0c8549517166433366e1.png)'
- en: Figure 3\. Results from Example 2 according to the ability to fight *(Image
    made by the author).*
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 图3\. 根据战斗能力的示例2结果（*图像由作者制作*）。
- en: To calculate the entropy of the previous cases we can apply the same equation
    we used before. Figure 3 shows how the entropy for the case in which people were
    trained to fight is 0.99 whereas in the other case, the entropy is zero. Remember
    that an entropy of zero means no surprise, homogeneous distribution, this is what
    is actually happening in this case since all the people who were not trained to
    fight, did not survive. At this point, it is important to note that the calculation
    of the entropy in this second scenario contains an undefined calculation since
    we will end up with a logarithm of zero. In these cases, you can always apply
    [L’Hôpital’s rule](https://en.wikipedia.org/wiki/L%27H%C3%B4pital%27s_rule) as
    it is explained in this [article](https://sefiks.com/2018/08/25/indeterminate-forms-and-lhospitals-rule-in-decision-trees/).
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 要计算之前案例的熵，我们可以应用之前使用的相同公式。图3显示，经过战斗训练的情况下，熵为0.99，而在其他情况下，熵为零。请记住，熵为零意味着没有惊讶，分布均匀，这实际上是因为所有没有接受战斗训练的人都没有生存下来。在此阶段，重要的是要注意，在第二种情况下的熵计算包含一个未定义的计算，因为我们最终会得到一个零的对数。在这些情况下，你可以始终应用[洛必达法则](https://en.wikipedia.org/wiki/L%27H%C3%B4pital%27s_rule)，正如在这篇[文章](https://sefiks.com/2018/08/25/indeterminate-forms-and-lhospitals-rule-in-decision-trees/)中所解释的那样。
- en: We now need to calculate the information gain from this decision. This is the
    same as asking how much the uncertainty in all my decisions would change if I
    decided to split all the outcomes according to this question. The information
    gain is calculated by subtracting the entropy of each decision from the entropy
    of the main node. An important thing to notice is how this operation is weighted
    according to the number of individuals on each decision. So a big entropy can
    have a small effect on the information gain calculation if the number of people
    that took that decision is small. For this example, the information gain from
    the ability to fight is 0.32 as it is shown in Figure 3.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在需要计算这个决策的信息增益。这与问如果我决定根据这个问题划分所有结果，所有决策的不确定性会改变多少是一样的。信息增益通过从主要节点的熵中减去每个决策的熵来计算。一个重要的事情是注意到，这个操作根据每个决策的个体数量加权。因此，即使熵很大，如果做出该决策的人数量较少，也可能对信息增益计算的影响较小。对于这个例子，战斗能力的信息增益为0.32，如图3所示。
- en: Can you survive the zombies all by yourself?
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 你能单独幸存下来吗？
- en: We can do a similar analysis of the possibility of surviving the zombie apocalypse
    alone or with someone else. Figure 4 shows the calculation. In this case, the
    information gain is 0.52\. Note how in this case, the possibility of being alone
    never led to survival, whereas in cases where the person was not alone, it survived
    in 5 of the 7 cases.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以对单独生存或与他人一起生存的可能性进行类似的分析。图4展示了计算过程。在这种情况下，信息增益为0.52。注意到在这种情况下，单独一人从未导致生存，而在不孤单的情况下，7个案例中有5个幸存下来。
- en: '![](../Images/cc2775684cc573d759716b0ad3129970.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/cc2775684cc573d759716b0ad3129970.png)'
- en: Figure 4\. Results from Example 2 according to if the person was alone or not
    *(Image made by the author).*
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 图4\. 根据是否独自一人，示例2的结果*（图像由作者制作）。*
- en: What about having a vehicle or a weapon?
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 拥有车辆或武器如何？
- en: For these two cases, we can calculate the information gain as we did before
    (Figure 5). You can see how the information gains are smaller than the ones calculated
    previously. This means that, at this point, it is better to divide our data according
    to the two previous features than to these ones. Remember that the biggest information
    gain corresponds to the feature in which the entropy reduction is the smallest.
    Once we have calculated all the information gains for every feature we can decide
    what the first node of the decision tree will be.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这两种情况，我们可以像之前一样计算信息增益（图5）。你可以看到这些信息增益比之前计算的要小。这意味着，此时根据前两个特征来划分数据比按照这两个特征划分要更好。记住，最大的增益对应于熵减少最小的特征。一旦我们计算了所有特征的信息增益，就可以决定决策树的第一个节点是什么。
- en: '![](../Images/3abf9611a00f9fe133e37fa18b493716.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3abf9611a00f9fe133e37fa18b493716.png)'
- en: Figure 4\. Results from Example 2 according to if the person had a vehicle or
    a weapon *(Image made by the author).*
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 图4\. 根据是否拥有车辆或武器，示例2的结果*（图像由作者制作）。*
- en: The first node
  id: totrans-50
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第一个节点
- en: 'Table 3 shows the information gains for each feature. The biggest information
    gain corresponds to the fact of being alone or having a companion. This node takes
    us to the first decision in our tree: you will not survive alone. The 8 people
    that were alone did not make it regardless of whether they had a weapon, a car
    or were trained to fight. So this is the first thing we can infer from our analysis
    which supports what we had concluded by just inspecting the data.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 表3显示了每个特征的信息增益。最大的增益对应于是否独自一人或有伴侣。这一节点将我们带到决策树中的第一个决策：你将无法单独生存。8个独自一人的人无论是否有武器、汽车或接受过战斗训练都无法生存。因此，这是我们从分析中可以得出的第一个结论，这也支持了我们仅通过检查数据得出的结论。
- en: '![](../Images/46786788bacafa94bc6ef937a9a6398f.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/46786788bacafa94bc6ef937a9a6398f.png)'
- en: Table 3\. Information gain for each feature after the analysis of the first
    node *(Table made by the author).*
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 表3\. 第一个节点分析后每个特征的信息增益*（表格由作者制作）。*
- en: At this point, the decision tree looks like Figure 5\. We know that there are
    no possibilities of surviving if we are alone (taking into account the data we
    have). If we are not alone, then we might survive but not in all cases. Since
    we can calculate the entropy at the right node in Figure 5, which is 0.86 (this
    calculation is shown in Figure 4), then we can also calculate the information
    gain from the other three features and decide which the next decision node will
    be.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 此时，决策树的样子如图5所示。我们知道如果我们独自一人是没有生存的可能性（考虑到我们拥有的数据）。如果我们不独自一人，那么我们可能会生存下来，但并非所有情况下都如此。由于我们可以计算图5中右侧节点的熵，即0.86（该计算过程在图4中展示），我们也可以计算其他三个特征的信息增益，并决定下一个决策节点是什么。
- en: '![](../Images/9119add05e8c42cfc5e8b35438f29889.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9119add05e8c42cfc5e8b35438f29889.png)'
- en: Figure 5\. Decision tree after the first decision node *(Image made by the author).*
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 图5\. 第一个决策节点后的决策树*（图像由作者制作）。*
- en: The second node
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第二个节点
- en: Figure 5 shows that the biggest information gain at this point comes from the
    weapon feature so that is the next decision node as is shown in Figure 6\. Note
    how all the people that were not alone and had a weapon survived and that is why
    the left side of the weapon node finishes in a survival decision.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 图5显示，此时最大的增益来自于武器特征，因此这是下一个决策节点，如图6所示。注意到所有未独自一人且拥有武器的人都幸存下来，这就是为什么武器节点的左侧以生存决策结束。
- en: '![](../Images/eec0d3452a656a0a25ac9ab861f9663e.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/eec0d3452a656a0a25ac9ab861f9663e.png)'
- en: Figure 6\. Decision tree after the second decision node *(Image made by the
    author).*
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 图6\. 第二个决策节点后的决策树*（图像由作者制作）。*
- en: The tree is complete
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 树是完整的
- en: There are still 3 people that were not alone and did not have a weapon that
    we need to categorize. If we follow the same process explained previously, we
    will find that the next feature with the biggest information gain is the vehicle.
    So we can add an additional node to our tree in which we ask if a particular person
    had a vehicle. That will divide the remaining 3 people into a group of 2 people
    that did have a vehicle but did not survive and one single person without a vehicle
    who survived. The final decision tree is presented in Figure 7.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 还有3个人没有单独存在且没有武器，我们需要对他们进行分类。如果我们遵循之前解释的相同过程，我们会发现下一个信息增益最大的特征是车辆。因此，我们可以在树上添加一个额外的节点，询问某个人是否拥有车辆。这将把剩下的3个人分成一个有2人拥有车辆但未幸存的组和一个没有车辆但幸存的单独个体。最终的决策树如图7所示。
- en: '![](../Images/9bdac9825f161f8090edf8ec1ebfb3a1.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9bdac9825f161f8090edf8ec1ebfb3a1.png)'
- en: Figure 7\. Final decision tree for Example 2 *(Image made by the author).*
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 图7\. 示例2的最终决策树*(作者制作的图像)。*
- en: The problem with decision trees
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 决策树的问题
- en: As you can see, the decision tree is a model built with previous experiences.
    Depending on the number of features in your data you will encounter multiple questions
    that will guide you to the final answer. It is important to notice how in this
    case one of the features is not represented in the decision tree. The ability
    to fight was never selected as a decision node since the other features always
    had a bigger information gain. This means that, according to the input data, being
    trained on how to fight is not important to survive a zombie apocalypse. However,
    this could also mean that we did not have enough samples to determine if the ability
    to fight was important or not. The key here is to remember that a decision tree
    is as good as the input data we are using to build it. In this case, a sample
    of 15 people might not be enough to have a good estimation of the importance of
    being trained to fight. This is one of the problems of the decision trees.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，决策树是一个基于以往经验构建的模型。根据数据中的特征数量，你会遇到多个问题，这些问题将引导你到最终答案。值得注意的是，在这种情况下，某些特征在决策树中并未体现。战斗能力从未被选择为决策节点，因为其他特征总是有更大的信息增益。这意味着，根据输入数据，训练战斗能力对于在僵尸
    apocalypse 中生存并不重要。然而，这也可能意味着我们没有足够的样本来确定战斗能力是否重要。关键在于记住，决策树的好坏取决于我们用于构建它的输入数据。在这种情况下，15个人的样本可能不足以对战斗训练的重要性进行准确估计。这是决策树的一个问题。
- en: As with other supervised learning approaches, decision trees are not perfect.
    On the one hand, they rely heavily on the input data. This means that a small
    change in the input data can lead to important changes in the final tree. Decision
    trees are not really good at generalizing. On the other hand, they tend to have
    overfitting problems. In other words, we can end up with a complex decision tree
    that works perfectly with the input data but it will dramatically fail when we
    use a test set. This could also affect the results if we are using the decision
    tree with continuous variables instead of categorical ones like the example presented.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 与其他监督学习方法一样，决策树并不完美。一方面，它们非常依赖输入数据。这意味着输入数据的微小变化可能会导致最终树的重要变化。决策树并不擅长于概括。另一方面，它们往往存在过拟合问题。换句话说，我们可能会得到一个与输入数据完全匹配但在测试集上表现极差的复杂决策树。如果我们使用带有连续变量的决策树而非像示例中那样的分类变量，这也可能影响结果。
- en: 'One way of making decision trees more efficient is to prune them. This means
    stopping the algorithm before reaching a pure node just as the ones we reached
    in our example. This could lead to the removal of a branch that is not providing
    any improvement in the accuracy of the decision tree. Pruning gives the decision
    tree more generalization power. However, if we decide to prune our decision tree
    then we might start asking additional questions such as: when is the right moment
    to stop the algorithm? Should we stop after we reach a minimum number of samples?
    Or after a predefined number of nodes? How to determine these numbers? Pruning
    can definitely help us to avoid overfitting but it may lead to additional questions
    that are not so simple to answer.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 提高决策树效率的一种方法是修剪它们。这意味着在达到纯节点之前停止算法，就像我们在示例中所达到的那样。这可能导致删除那些对决策树准确性没有改善的分支。修剪赋予决策树更多的概括能力。然而，如果我们决定修剪决策树，那么我们可能会开始提出额外的问题，比如：何时是停止算法的正确时机？我们应该在达到最小样本数后停止？还是在预定义的节点数后停止？如何确定这些数字？修剪确实可以帮助我们避免过拟合，但它可能带来一些不易回答的额外问题。
- en: What about a full forest instead of a tree?
  id: totrans-69
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 那么，如何看待整个森林而不是单棵树呢？
- en: What if instead of a single decision tree, we have multiple decision trees?
    They will change according to the portion of the input data they take, the features
    they read and their pruning characteristics. We will end up with many decision
    trees and many different answers but we can always go with the majority in the
    case of a classification task or an average if we are working on regressions.
    This can help us to generalize the distribution of our data better. We might think
    that one decision tree is misclassifying but if we find 10 or 20 trees that reach
    the same conclusion, then this is telling us that there might be no misclassification
    after all. Basically, we are letting the majority decide instead of guiding ourselves
    by one single decision tree. This methodology is called [Random Forest](https://en.wikipedia.org/wiki/Random_forest).
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们不是使用单一的决策树，而是多个决策树呢？它们将根据所处理的输入数据部分、读取的特征以及修剪特性而变化。最终我们将得到许多决策树和不同的答案，但在分类任务中我们可以选择多数结果，或在回归任务中取平均。这可以帮助我们更好地概括数据的分布。我们可能会认为某棵决策树存在误分类，但如果我们找到10或20棵树得出相同的结论，那么这可能表明实际上没有误分类。基本上，我们是让多数决定，而不是依赖单棵决策树。这种方法称为[随机森林](https://en.wikipedia.org/wiki/Random_forest)。
- en: The concept of Random Forests is usually related to the concept of [Bagging](https://www.ibm.com/topics/bagging)
    which is a process where a random sample of data in a training set is selected
    with replacement. This means that individual data points can be chosen more than
    once. In the Random Forest methodology, we can choose a random number of points,
    build a decision tree, and then do this again until we have multiple trees. Then,
    the final decision will come from all the answers that were obtained from the
    trees.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 随机森林的概念通常与[袋装法](https://www.ibm.com/topics/bagging)相关，袋装法是一个从训练集中随机抽取数据样本的过程，并且有放回地选择。这意味着个别数据点可以被选择多次。在随机森林方法中，我们可以选择一个随机数量的点，建立一棵决策树，然后重复这一过程直到得到多棵树。最后的决策将来自所有树的答案。
- en: Random Forests is a well-known ensemble method used for classification and regression
    problems. This method has been applied in many industries such as finance, healthcare
    and e-commerce [1]. Although the original idea of Random Forests was slowly developed
    by many researchers, Leo Breiman is commonly known as the creator of this methodology
    [2]. His [personal webpage](https://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm)
    contains a detailed description of Random Forests and an extensive explanation
    of how and why it works. It is a long but worthy read.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 随机森林是一种著名的集成方法，用于分类和回归问题。该方法已在金融、医疗保健和电子商务等多个行业中应用[1]。虽然随机森林的原始思想是由许多研究人员逐步发展的，但莱奥·布雷曼通常被认为是这一方法的创始人[2]。他的[个人网页](https://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm)中包含了随机森林的详细描述和其工作原理的广泛解释。这是篇较长但值得阅读的文章。
- en: An additional and important thing to understand about random forests is the
    way in which they work with the features of the dataset. At each node, the random
    forest will randomly select a pre-defined number of features instead of all of
    them to decide how to split each node. Remember that in the previous example,
    we analyzed the information gain from each feature at each level of the decision
    tree. On the contrary, a random forest will only analyze the information gain
    from a subset of the features at each node. So, the random forest mixes Bagging
    with the random variable selection at each node.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 关于随机森林，一个重要的理解点是它如何处理数据集的特征。在每个节点，随机森林会随机选择一个预定义的特征数量，而不是所有特征，来决定如何分裂每个节点。记住在之前的例子中，我们分析了决策树每个层级的每个特征的信息增益。相反，随机森林会在每个节点只分析特征子集的信息增益。因此，随机森林将Bagging与每个节点的随机变量选择混合在一起。
- en: A true zombie apocalypse
  id: totrans-74
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 一个真正的僵尸末日
- en: Let’s go back to the zombies! The previous example was really simple, we had
    data from 15 people and we only knew 4 things about each one of them. Let’s make
    this harder! Let’s say that now we have a dataset with more than one thousand
    entries and for each one of them we have 10 features. This dataset was randomly
    generated in Excel and does not belong to any commercial or private repository,
    you can access it from this [GitHub page](https://github.com/manfrezord/MediumArticles/tree/main/RF).
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 回到僵尸的问题！之前的例子非常简单，我们有15个人的数据，每个人只有4个特征。让我们把难度加大！假设现在我们有一个超过一千条记录的数据集，每条记录有10个特征。这个数据集是在Excel中随机生成的，不属于任何商业或私人库，你可以从这个[GitHub页面](https://github.com/manfrezord/MediumArticles/tree/main/RF)访问它。
- en: As is usual with these types of methodologies, it is a good idea to separate
    the entire dataset into a training and testing set. We will use the training set
    to build the decision tree and random forest models and then we will evaluate
    them with the test set. For this purpose, we will use the [scikit-learn libraries](https://scikit-learn.org/stable/).
    This [Jupyter Notebook](https://github.com/manfrezord/MediumArticles/blob/main/RF/Random_Forests_Zombie_Apocalypse.ipynb)
    contains a detailed explanation of the dataset, how to load it and how to build
    the models using the library.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这些类型的方法论，通常建议将整个数据集分成训练集和测试集。我们将使用训练集来构建决策树和随机森林模型，然后使用测试集来评估它们。为此，我们将使用[scikit-learn库](https://scikit-learn.org/stable/)。这个[Jupyter
    Notebook](https://github.com/manfrezord/MediumArticles/blob/main/RF/Random_Forests_Zombie_Apocalypse.ipynb)包含了数据集的详细说明、如何加载数据以及如何使用该库构建模型。
- en: The entire dataset contains 1024 entries of which 212 (21%) correspond to survivals
    and 812 (79%) to deaths. We divided this dataset into a training set that corresponds
    to 80% of the data (819 entries) and a testing set which contains 205 entries.
    Figure 8 shows how the relation between survivals and deaths is maintained in
    all sets.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 整个数据集包含1024条记录，其中212条（21%）对应幸存者，812条（79%）对应死亡。我们将数据集分为一个训练集，占数据的80%（819条记录），以及一个包含205条记录的测试集。图8显示了在所有数据集中幸存者与死亡者之间的关系是如何保持的。
- en: '![](../Images/cb94acf47c63441fd40234e370b55774.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/cb94acf47c63441fd40234e370b55774.png)'
- en: Figure 8\. Separation of the dataset for Example 3 into a training and testing
    dataset *(Image made by the author).*
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 图8. 示例3中数据集的训练集和测试集分离*(图片由作者制作)。*
- en: 'Regarding the features, this time we have 6 additional characteristics for
    each individual:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 关于特征，这次我们为每个个体增加了6个额外的特征：
- en: Do you have a radio?
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你有收音机吗？
- en: Do you have food?
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你有食物吗？
- en: Have you taken a course in outdoor survival?
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你上过户外生存课程吗？
- en: Have you taken a first aid course?
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你上过急救课程吗？
- en: Have you had a zombie encounter before?
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你以前遇到过僵尸吗？
- en: Do you have a GPS?
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你有GPS吗？
- en: These 6 features combined with the 4 features we already had, represent 10 different
    characteristics for each individual or entry. With this information, we can build
    a decision tree following the previously explained steps. The [Jupyter Notebook](https://github.com/manfrezord/MediumArticles/blob/main/RF/Random_Forests_Zombie_Apocalypse.ipynb)
    uses the function DecisionTreeClassifier to generate a Decision Tree. Note that
    this function is not meant to work for categorical variables. In this case, we
    have converted all the answers for each category to -1 or +1\. This means that
    every time we see a -1 in the results it means “No” whereas a +1 means “Yes”.
    This is better explained in the [Jupyter Notebook](https://github.com/manfrezord/MediumArticles/blob/main/RF/Random_Forests_Zombie_Apocalypse.ipynb).
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 这 6 个特征加上我们已有的 4 个特征，共同代表了每个个体或条目的 10 个不同特征。有了这些信息，我们可以按照之前解释的步骤构建决策树。[Jupyter
    Notebook](https://github.com/manfrezord/MediumArticles/blob/main/RF/Random_Forests_Zombie_Apocalypse.ipynb)
    使用 DecisionTreeClassifier 函数生成决策树。请注意，此函数不适用于分类变量。在这种情况下，我们已将每个类别的所有答案转换为 -1 或
    +1。这意味着每次我们在结果中看到 -1 时，表示“否”，而 +1 表示“是”。这一点在[Jupyter Notebook](https://github.com/manfrezord/MediumArticles/blob/main/RF/Random_Forests_Zombie_Apocalypse.ipynb)中有更详细的说明。
- en: The [Notebook](https://github.com/manfrezord/MediumArticles/blob/main/RF/Random_Forests_Zombie_Apocalypse.ipynb)
    explains how to load the data, call the decision tree function and plot the results.
    Figure 9 shows the decision tree that was built with the 819 entries that corresponded
    to the training set (click [here](https://raw.githubusercontent.com/manfrezord/MediumArticles/main/RF/Decision_Tree_Big.png)
    for a bigger picture). The dark blue boxes correspond to final decision nodes
    in which the answer was survival whereas the dark orange boxes represent final
    decision nodes where the final answer was not survival. You can see how the first
    decision node corresponds to the vehicle and from there, the tree starts growing
    according to different features.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '[笔记本](https://github.com/manfrezord/MediumArticles/blob/main/RF/Random_Forests_Zombie_Apocalypse.ipynb)
    解释了如何加载数据、调用决策树函数并绘制结果。图 9 显示了使用 819 个训练集条目构建的决策树（点击[这里](https://raw.githubusercontent.com/manfrezord/MediumArticles/main/RF/Decision_Tree_Big.png)查看更大图）。深蓝色框对应于最终的决策节点，其中答案为幸存，而深橙色框表示最终答案为未幸存。你可以看到，第一个决策节点对应于车辆，从那里开始，树根据不同的特征开始生长。'
- en: '![](../Images/bcca9dc17f11ea61f069296f72acd20a.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bcca9dc17f11ea61f069296f72acd20a.png)'
- en: Figure 9\. Decision tree for the training set of Example 3 ([here](https://raw.githubusercontent.com/manfrezord/MediumArticles/main/RF/Decision_Tree_Big.png)
    is a zoomable version of this diagram made by the author).
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9\. 示例 3 的训练集决策树（[这里](https://raw.githubusercontent.com/manfrezord/MediumArticles/main/RF/Decision_Tree_Big.png)是作者制作的可缩放版本）。
- en: We can evaluate how good this tree is if we use the test set inputs to predict
    the final categories and then compare these results with the original results.
    Table 4 shows a [confusion matrix](https://en.wikipedia.org/wiki/Confusion_matrix)
    with the number of times the decision tree misclassified an entry. We can see
    that the test set had 40 cases that represented survival and the decision tree
    only classified correctly 25 of them. On the other hand, from the 165 cases that
    did not survive, the decision tree misclassified 11\. The relation between the
    correct classifications and the entire dataset of 205 points is 0.87 which is
    usually known as the prediction accuracy score.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以评估这棵树的好坏，如果我们使用测试集输入预测最终类别，然后将这些结果与原始结果进行比较。表 4 显示了一个[混淆矩阵](https://en.wikipedia.org/wiki/Confusion_matrix)，其中列出了决策树将条目误分类的次数。我们可以看到，测试集中有
    40 个代表幸存的案例，而决策树仅正确分类了其中的 25 个。另一方面，在 165 个未幸存的案例中，决策树误分类了 11 个。正确分类与 205 个点的整个数据集的关系为
    0.87，这通常被称为预测准确率得分。
- en: '![](../Images/73c39df49bcc4ee87e3beea94ed94313.png)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/73c39df49bcc4ee87e3beea94ed94313.png)'
- en: Table 4\. Confusion matrix for the testing dataset after running the decision
    tree *(Image made by the author).*
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 表 4\. 决策树运行后的测试数据集混淆矩阵*（图片由作者制作）。*
- en: '87% of accuracy does not look bad but, can we improve this using a random forest?
    The next section of the [Jupyter Notebook](https://github.com/manfrezord/MediumArticles/blob/main/RF/Random_Forests_Zombie_Apocalypse.ipynb)
    contains an implementation of a random forest using the sklearn function RandomForestClassifier.
    This random forest will contain 10 decision trees that are built using all the
    entries but only considering 3 features at each split. Each of the decision trees
    in the random forest considers 682 entries which represent 84% of the full training
    set. So, just to be clear, the random forest process will:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 87%的准确率看起来还不错，但我们能通过随机森林提高这个结果吗？下一部分的[Jupyter Notebook](https://github.com/manfrezord/MediumArticles/blob/main/RF/Random_Forests_Zombie_Apocalypse.ipynb)包含了使用sklearn函数RandomForestClassifier实现的随机森林。这棵随机森林将包含10棵决策树，这些决策树使用所有条目进行构建，但每次分裂时只考虑3个特征。随机森林中的每棵决策树考虑682个条目，这些条目代表了完整训练集的84%。因此，明确来说，随机森林过程将：
- en: Take a random subset of 682 entries from the training set
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从训练集中随机抽取682个条目
- en: Build a decision tree that considers 3 randomly selected features at each node
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 构建一个决策树，在每个节点考虑3个随机选择的特征
- en: Repeat the previous steps 9 additional times
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复上述步骤9次
- en: The predictions will correspond to the majority vote over the 10 decision trees
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 预测将对应于10棵决策树中的多数投票
- en: Table 5 shows the confusion matrix for the results coming from the random forest.
    We can see that these results are better than what we were getting before with
    a single decision tree. This random forest misclassifies 11 entries and has a
    prediction accuracy score of 0.95 which is higher than the decision tree.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 表 5 显示了来自随机森林的结果的混淆矩阵。我们可以看到，这些结果比我们之前用单棵决策树得到的要好。这个随机森林误分类了11个条目，预测准确率为0.95，高于决策树。
- en: '![](../Images/ea782a068d65ae2e0ff9bcbc7eaf38f3.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ea782a068d65ae2e0ff9bcbc7eaf38f3.png)'
- en: Table 5\. Confusion matrix for the testing dataset after running the random
    forest *(Image made by the author).*
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 表 5\. 运行随机森林后测试数据集的混淆矩阵 *(图像由作者制作)。*
- en: It is important to take into account that the random forest methodology is not
    only as good as the input data we have but also as good as the selection of parameters
    that we use. The number of decision trees we build and the number of parameters
    we analyze at each split will have an important effect on the final result. So,
    as is the case of many other supervised learning algorithms, it is necessary to
    spend some time tuning the parameters until we found the best possible result.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的是，随机森林方法不仅取决于我们拥有的输入数据的质量，还取决于我们使用的参数选择。我们构建的决策树数量以及每次分裂时分析的参数数量会对最终结果产生重要影响。因此，与许多其他监督学习算法一样，有必要花时间调整参数，直到找到最佳结果。
- en: Conclusions
  id: totrans-103
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: Going through this article is just like that guy in the movies that managed
    to escape from the zombie that was chasing him because a tree branch fell on the
    zombie’s head just at the right time! This is not the only zombie he will encounter
    and he is definitely not out of the woods yet! There are many things about Random
    Forests and Decision Trees that were not even mentioned in this article. However,
    it is enough to understand the usage and applicability of this method. Currently,
    there are multiple libraries and programs that build these models in seconds.
    So you probably don’t need to go through the entropy and information gain calculation
    again. Still, it is important to understand what is happening behind the curtain
    and how to correctly interpret the results. In a world where topics such as “Machine
    Learning”, “Ensemble Methods” and “Data Analytics” are every day more common,
    it is important to have a clear idea of what these methods are and how to apply
    them to everyday problems. Unlike the zombie apocalypse survival movies, being
    ready does not happen by chance.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 阅读这篇文章就像电影里那个成功逃脱追赶他的僵尸的家伙，因为一个树枝恰好掉在了僵尸的头上！这并不是他遇到的唯一僵尸，他显然还没有脱离困境！关于随机森林和决策树的许多内容在这篇文章中并未提及。然而，了解这种方法的使用和适用性已经足够。目前，有多个库和程序能够在几秒钟内构建这些模型。所以你可能不需要再重新计算熵和信息增益。不过，理解幕后发生的事情以及如何正确解读结果仍然很重要。在“机器学习”、“集成方法”和“数据分析”等话题日益普及的今天，清楚了解这些方法及其应用于日常问题是很重要的。与僵尸末日求生电影不同，准备好不是偶然发生的。
- en: References
  id: totrans-105
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: IBM. [What is random forest?](https://www.ibm.com/topics/random-forest)
  id: totrans-106
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: IBM. [什么是随机森林？](https://www.ibm.com/topics/random-forest)
- en: Louppe, Gilles (2014). Understanding Random Forests. PhD dissertation. University
    of Liege
  id: totrans-107
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Louppe, Gilles (2014). 《理解随机森林》。博士论文。列日大学
