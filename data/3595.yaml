- en: Object Detection using RetinaNet and KerasCV
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 RetinaNet 和 KerasCV 的目标检测
- en: 原文：[https://towardsdatascience.com/object-detection-using-retinanet-and-kerascv-b07940327b6c?source=collection_archive---------3-----------------------#2023-12-06](https://towardsdatascience.com/object-detection-using-retinanet-and-kerascv-b07940327b6c?source=collection_archive---------3-----------------------#2023-12-06)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/object-detection-using-retinanet-and-kerascv-b07940327b6c?source=collection_archive---------3-----------------------#2023-12-06](https://towardsdatascience.com/object-detection-using-retinanet-and-kerascv-b07940327b6c?source=collection_archive---------3-----------------------#2023-12-06)
- en: Object detection using the power and simplicity of the KerasCV library.
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 KerasCV 库的力量和简便性进行目标检测。
- en: '[](https://medium.com/@ed.izaguirre?source=post_page-----b07940327b6c--------------------------------)[![Ed
    Izaguirre](../Images/c9eded1f06c47571baa662107428483f.png)](https://medium.com/@ed.izaguirre?source=post_page-----b07940327b6c--------------------------------)[](https://towardsdatascience.com/?source=post_page-----b07940327b6c--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----b07940327b6c--------------------------------)
    [Ed Izaguirre](https://medium.com/@ed.izaguirre?source=post_page-----b07940327b6c--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@ed.izaguirre?source=post_page-----b07940327b6c--------------------------------)[![Ed
    Izaguirre](../Images/c9eded1f06c47571baa662107428483f.png)](https://medium.com/@ed.izaguirre?source=post_page-----b07940327b6c--------------------------------)[](https://towardsdatascience.com/?source=post_page-----b07940327b6c--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----b07940327b6c--------------------------------)
    [Ed Izaguirre](https://medium.com/@ed.izaguirre?source=post_page-----b07940327b6c--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F33a47cfa4187&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fobject-detection-using-retinanet-and-kerascv-b07940327b6c&user=Ed+Izaguirre&userId=33a47cfa4187&source=post_page-33a47cfa4187----b07940327b6c---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----b07940327b6c--------------------------------)
    ·21 min read·Dec 6, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fb07940327b6c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fobject-detection-using-retinanet-and-kerascv-b07940327b6c&user=Ed+Izaguirre&userId=33a47cfa4187&source=-----b07940327b6c---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F33a47cfa4187&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fobject-detection-using-retinanet-and-kerascv-b07940327b6c&user=Ed+Izaguirre&userId=33a47cfa4187&source=post_page-33a47cfa4187----b07940327b6c---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----b07940327b6c--------------------------------)
    · 21 分钟阅读 · 2023年12月6日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fb07940327b6c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fobject-detection-using-retinanet-and-kerascv-b07940327b6c&user=Ed+Izaguirre&userId=33a47cfa4187&source=-----b07940327b6c---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fb07940327b6c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fobject-detection-using-retinanet-and-kerascv-b07940327b6c&source=-----b07940327b6c---------------------bookmark_footer-----------)![](../Images/8393284e99efaccbc4492dd3f58d6a02.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fb07940327b6c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fobject-detection-using-retinanet-and-kerascv-b07940327b6c&source=-----b07940327b6c---------------------bookmark_footer-----------)![](../Images/8393284e99efaccbc4492dd3f58d6a02.png)'
- en: An image of leaves on a plant. Created in [DALL·E 2](https://openai.com/dall-e-2).
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 一张植物叶子的图像。创建于 [DALL·E 2](https://openai.com/dall-e-2)。
- en: '**Table of Contents**'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '**目录**'
- en: '[Wait, what’s KerasCV?](#4bc2)'
  id: totrans-10
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[等等，什么是 KerasCV？](#4bc2)'
- en: '[Inspecting the Data](#043c)'
  id: totrans-11
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[检查数据](#043c)'
- en: '[Pre-Processing Images](#4e89)'
  id: totrans-12
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[图像预处理](#4e89)'
- en: '[RetinaNet Model Background](#c7dd)'
  id: totrans-13
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[RetinaNet 模型背景](#c7dd)'
- en: '[Training RetinaNet](#b137)'
  id: totrans-14
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[训练 RetinaNet](#b137)'
- en: '[Making Predictions](#c1fe)'
  id: totrans-15
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[做出预测](#c1fe)'
- en: '[Conclusion](#0b07)'
  id: totrans-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[结论](#0b07)'
- en: '[References](#fdd7)'
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[参考文献](#fdd7)'
- en: '**Relevant Links**'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '**相关链接**'
- en: '[Working Kaggle Notebook](https://www.kaggle.com/code/edizaguirre/plant-object-detection):
    Feel free to make a copy of the notebook, play around with the code, and use that
    free GPU.'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Kaggle 实验笔记](https://www.kaggle.com/code/edizaguirre/plant-object-detection):
    随意复制笔记本，试验代码，并使用免费的 GPU。'
- en: '[PlantDoc Dataset](https://public.roboflow.com/object-detection/plantdoc?ref=blog.roboflow.com):
    This is the dataset used in this notebook, hosted on Roboflow. The dataset is
    published under the CC BY 4.0 DEED license, which means you can copy and redistribute
    the material in any medium or format for any purpose, even commercially.'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[PlantDoc 数据集](https://public.roboflow.com/object-detection/plantdoc?ref=blog.roboflow.com)：这是本笔记本中使用的数据集，托管在
    Roboflow 上。该数据集在 CC BY 4.0 DEED 许可证下发布，这意味着你可以在任何媒介或格式中复制和重新分发该材料，甚至用于商业目的。'
- en: Wait, what’s KerasCV?
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 等等，什么是 KerasCV？
- en: 'After finishing a mini-project based on image segmentation ([see here](https://medium.com/towards-data-science/image-segmentation-an-in-depth-guide-5e56512eea2e)),
    I was ready to move into another common task under the computer vision umbrella:
    **object detection.** Object detection refers to taking an image and producing
    boxes around objects of interest, as well as classifying the objects the boxes
    contain. As a simple example, take a look at the image below:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在完成基于图像分割的小项目后（[参见这里](https://medium.com/towards-data-science/image-segmentation-an-in-depth-guide-5e56512eea2e)），我准备转入计算机视觉领域下另一个常见任务：**物体检测**。物体检测指的是对图像进行处理，产生围绕感兴趣对象的框，并分类这些框中的对象。作为一个简单的例子，看看下面的图片：
- en: '![](../Images/45a73eb52e043b13700a464177192f03.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/45a73eb52e043b13700a464177192f03.png)'
- en: Example of object detection. Notice the bounding box and class label. Image
    by author.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 物体检测的示例。请注意边界框和类标签。图片由作者提供。
- en: 'The blue box is referred to as a **bounding box** and the **class name** is
    placed right above it. Object detection can thus be broken down into two mini-problems:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 蓝色的框被称为**边界框**，**类名**放置在其正上方。因此，物体检测可以分解为两个小问题：
- en: A *regression* problem where the model must predict *x* and *y* coordinates
    for the both the upper left corner and the lower right corner of the box.
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一个*回归*问题，模型必须预测盒子左上角和右下角的*x*和*y*坐标。
- en: A *classification* problem where the model must predict what class of object
    the box is observing.
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一个*分类*问题，模型必须预测盒子正在观察的物体类别。
- en: In this example, the bounding box was created and labeled by a human. We would
    like to automate this process, and a well-trained object detection model can do
    just that.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，边界框是由人类创建和标记的。我们希望自动化这个过程，而一个训练良好的物体检测模型正可以做到这一点。
- en: 'I sat down to review my study material regarding object detection, and was
    promptly disappointed. Unfortunately, most introductory material scarcely mention
    object detection. François Chollet in *Deep Learning with Python* [1] states:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 我坐下来回顾我关于物体检测的学习资料，很快就感到失望。不幸的是，大多数介绍性的资料几乎没有提到物体检测。François Chollet 在*Python
    深度学习* [1] 中提到：
- en: Note that we won’t cover object detection, because it would be too specialized
    and too complicated for an introductory book.
  id: totrans-30
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 请注意，我们不会涵盖物体检测，因为它对于介绍性书籍来说过于专业和复杂。
- en: Aurélion Géron [2] provides a lot of textual content covering the ideas behind
    object detection, but provides only a few lines of code covering an object detection
    task with dummy bounding boxes, far from the end-to-end pipeline I was looking
    for. Andrew Ng’s [3] famous *Deep Learning Specialization* course goes the deepest
    on object detection, but even he ends the coding lab by loading a pre-trained
    object detection model and just doing inference.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: Aurélion Géron [2] 提供了许多关于物体检测背后思想的文本内容，但只提供了几行代码来处理带有虚拟边界框的物体检测任务，远未达到我所期望的端到端流水线。Andrew
    Ng [3] 的著名*深度学习专项课程*在物体检测方面涵盖最深入，但甚至他在编码实验室中也只是加载了一个预训练的物体检测模型进行推理。
- en: 'Looking to go deeper, I started to sketch out the outline of an object detection
    pipeline. Just to do pre-processing for a RetinaNet model, one would have to do
    the following (*note: other object detection models such as YOLO would require
    different steps*):'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 想要更深入地研究，我开始勾勒出一个物体检测流水线的大纲。仅仅为了为 RetinaNet 模型进行预处理，你需要执行以下步骤（*注：其他物体检测模型如 YOLO
    需要不同的步骤*）：
- en: Take input images and resize them all to be the same size, with padding to prevent
    the aspect ratio from getting messed up. Oh, don’t forget about the bounding boxes;
    these also need to be appropriately reshaped or you will ruin your data.
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将输入图片都调整为相同的大小，并进行填充以防止长宽比混乱。哦，不要忘记边界框；这些也需要适当地重新调整形状，否则你会破坏你的数据。
- en: Generate anchor boxes at different scales and aspect ratios based on the ground
    truth bounding boxes in the training set. These anchor boxes act as reference
    points for the model during training.
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 根据训练集中的真实边界框生成不同尺度和纵横比的锚框。这些锚框在训练过程中作为模型的参考点。
- en: Assign labels to the anchor boxes based on their overlap with ground truth boxes.
    Anchor boxes with high overlap are labeled as positive examples, while those with
    low overlap are labeled as negative examples.
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 根据与真实框的重叠情况为锚框分配标签。重叠度高的锚框标记为正例，而重叠度低的锚框标记为负例。
- en: There are multiple ways to describe the same bounding box. You would need to
    implement functions for converting between these different formats. More on this
    in a moment.
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 描述相同的边界框有多种方法。你需要实现函数来在这些不同格式之间进行转换。稍后会详细介绍。
- en: Implement data augmentation, taking care to not only augment the images but
    also the boxes. In theory you can omit this, but in practice this is necessary
    to help our models generalize well.
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现数据增强时，不仅要增强图像，还要增强框。理论上你可以省略这一步，但在实践中这是必要的，以帮助我们的模型更好地泛化。
- en: '[Take a look at this example](https://keras.io/examples/vision/retinanet/)
    on the Keras website. **Yikes**. Post-processing of our model predictions would
    take even more work. To paraphrase the Keras team: this is a technically complex
    problem.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '[看看这个例子](https://keras.io/examples/vision/retinanet/) 在 Keras 网站上。**哎呀**。我们模型预测的后处理将需要更多工作。借用
    Keras 团队的话：这是一个技术上复杂的问题。'
- en: 'As I was beginning to despair, I started desperately browsing the internet
    and stumbled upon a library I had never heard of before: [KerasCV](https://keras.io/keras_cv/).
    As I read the documentation, it began to dawn on me that this is the **future
    of computer vision in TensorFlow/Keras.** From their introduction:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 当我开始绝望时，我开始急切地浏览互联网，偶然发现了一个我从未听说过的库：[KerasCV](https://keras.io/keras_cv/)。当我阅读文档时，我开始意识到这是**TensorFlow/Keras
    计算机视觉的未来**。根据他们的介绍：
- en: 'KerasCV can be understood as a horizontal extension of the Keras API: the components
    are new first-party Keras objects that are too specialized to be added to core
    Keras. They receive the same level of polish and backwards compatibility guarantees
    as the core Keras API, and they are maintained by the Keras team.'
  id: totrans-40
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: KerasCV 可以被理解为 Keras API 的横向扩展：这些组件是新的第一方 Keras 对象，过于专业化而无法添加到核心 Keras 中。它们与核心
    Keras API 享有相同级别的打磨和向后兼容保证，并由 Keras 团队维护。
- en: '*“But why did none of my study materials even mention this?”* I wondered. The
    answer is simple: this is a fairly new library. The first commit on GitHub was
    on April 13th, 2022, too new to show up even in the latest editions of my textbooks.
    In fact, the 1.0 version of the library hasn’t even been released yet (as of November
    10th, 2023 it is on 0.6.4). I expect KerasCV will be discussed in detail by the
    next editions of my textbooks and online courses (to be fair, Gèron does mention
    in passing a “new Keras NLP project” and Keras CV project that the reader may
    be interested in).'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '*“但为什么我的学习材料中没有提到这个？”* 我想。答案很简单：这是一个相当新的库。GitHub 上的第一次提交是在 2022 年 4 月 13 日，太新了，甚至还未出现在我教科书的最新版本中。事实上，该库的
    1.0 版本尚未发布（截至 2023 年 11 月 10 日，它是 0.6.4）。我预计 KerasCV 会在我教科书的下一版和在线课程中详细讨论（公平地说，Gèron
    确实提到过“新的 Keras NLP 项目”和 Keras CV 项目，读者可能会感兴趣）。'
- en: Being so new, KerasCV doesn’t have many tutorials aside from those published
    by the Keras team themselves ([see here](https://keras.io/guides/keras_cv/)).
    In this tutorial I will demonstrate an end-to-end object detection pipeline to
    recognize healthy and diseased leaves using techniques inspired by but distinct
    from the official Keras guides. **With KerasCV, even beginners can take labeled
    datasets and use them to build effective object detection pipelines.**
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: KerasCV 刚刚推出，除了 Keras 团队自己发布的教程外，还没有很多教程（[见这里](https://keras.io/guides/keras_cv/)）。在本教程中，我将演示一个端到端的目标检测流程，使用受官方
    Keras 指南启发但又不同于这些指南的技术来识别健康和病变叶片。**有了 KerasCV，即使是初学者也可以利用标记数据集来构建有效的目标检测管道。**
- en: 'A few notes before we begin. KerasCV is a fast changing library, with the codebase
    and documentation being updated on a regular basis. The implementation shown here
    will work with KerasCV version 0.6.4\. The Keras team has stated that: *“there
    is no backwards compatibility contract until KerasCV reaches v1.0.0.”* This implies
    that there is no guarantee the methods used in this tutorial will continue to
    work as KerasCV gets updated. I have hard coded the KerasCV version number in
    the linked Kaggle notebook to prevent these sorts of issues.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始之前需要注意几点。KerasCV 是一个快速变化的库，其代码库和文档会定期更新。这里展示的实现将适用于 KerasCV 版本 0.6.4。Keras
    团队已声明：*“在 KerasCV 达到 v1.0.0 之前，没有向后兼容的承诺。”* 这意味着无法保证本教程中使用的方法在 KerasCV 更新时仍然有效。我已在链接的
    Kaggle notebook 中硬编码了 KerasCV 版本号，以防止这些问题。
- en: KerasCV has quite a few bugs that are already noted in the [Issues tab on GitHub](https://github.com/keras-team/keras-cv/issues).
    In addition, the documentation is lacking in some areas (I’m looking at you, *MultiClassNonMaxSuppression*).
    As you play around with KerasCV, try not to be discouraged by these issues. In
    fact, this is a great opportunity to become a contributor to the KerasCV codebase!
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: KerasCV 有很多已知的错误，可以在 [GitHub 的问题标签页](https://github.com/keras-team/keras-cv/issues)
    中查看。此外，文档在一些领域也有所欠缺（我看着你，*MultiClassNonMaxSuppression*）。在使用 KerasCV 时，尽量不要被这些问题气馁。事实上，这是一个成为
    KerasCV 代码库贡献者的绝佳机会！
- en: This tutorial will focus on implementation details of KerasCV. I will briefly
    review some high-level concepts in object detection, but I will assume the reader
    has some background knowledge on concepts such as the RetinaNet architecture.
    The code shown here has been edited and rearranged for clarity, please see the
    Kaggle notebook linked above for the complete code.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 本教程将重点介绍 KerasCV 的实现细节。我将简要回顾一些目标检测的高级概念，但假设读者对如 RetinaNet 架构等概念有一定背景知识。这里展示的代码已进行编辑和调整以提高清晰度，完整代码请参见上面链接的
    Kaggle notebook。
- en: Finally, a note on safety. The model created here is not intended to be state-of-the-art;
    treat this as a high-level tutorial. Further fine-tuning and data cleaning would
    be expected before this plant disease detection model could be implemented in
    production. It would be a good idea to run any predictions a model makes by a
    human expert to confirm a diagnosis.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，关于安全的提示。这里创建的模型并非最先进的技术；请将其视为一个高层次的教程。在将此植物疾病检测模型投入生产之前，需要进一步的微调和数据清理。最好将模型做出的任何预测交由人工专家确认诊断。
- en: Inspecting the Data
  id: totrans-47
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 检查数据
- en: 'The PlantDoc dataset contains 2,569 images across 13 plant species and 30 classes.
    The goal of the dataset is set out in the abstract of the paper *PlantDoc: A Dataset
    for Visual Plant Disease Detection* by Singh et. al [4].'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 'PlantDoc 数据集包含 2,569 张图像，涵盖 13 种植物和 30 个类别。数据集的目标在 Singh 等人撰写的论文 *PlantDoc:
    A Dataset for Visual Plant Disease Detection* 的摘要中进行了阐述 [4]。'
- en: India loses 35% of the annual crop yield due to plant diseases. Early detection
    of plant diseases remains difficult due to the lack of lab infrastructure and
    expertise. In this paper, we explore the possibility of computer vision approaches
    for scalable and early plant disease detection.
  id: totrans-49
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 印度由于植物疾病每年损失 35% 的作物产量。由于缺乏实验室基础设施和专业知识，植物疾病的早期检测仍然很困难。本文探讨了计算机视觉方法在可扩展和早期植物疾病检测中的可能性。
- en: This is a noble goal, and an area where computer vision can do a lot of good
    for farmers.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个崇高的目标，也是计算机视觉可以为农民做出很多贡献的领域。
- en: 'Roboflow allows us to download the dataset in a variety of different formats.
    Since we are using TensorFlow, let’s download the dataset as a **TFRecord**. A
    TFRecord is a specific format used in TensorFlow that is designed to store large
    amounts of data efficiently. The data is represented by a sequence of records,
    where each record is a key-value pair. Each key is a referred to as a *feature*.
    The downloaded zip file contains four files, two for training and two for validation:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: Roboflow 允许我们以多种不同格式下载数据集。由于我们使用 TensorFlow，建议将数据集下载为 **TFRecord** 格式。TFRecord
    是 TensorFlow 中一种特定格式，旨在高效地存储大量数据。数据由一系列记录表示，每个记录是一个键值对。每个键称为 *feature*。下载的压缩文件包含四个文件，其中两个用于训练，两个用于验证：
- en: '`leaves_label_map.pbtxt` : This is a Protocol Buffers text format file, which
    is used to describe the structure of the data. Opening the file in a text editor,
    I see that there are thirty classes. There are a mixture of healthy leaves such
    as `Apple leaf` and unhealthy leaves such as `Apple Scab Leaf` .'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`leaves_label_map.pbtxt` : 这是一个 Protocol Buffers 文本格式文件，用于描述数据的结构。打开文件时，我看到有三十个类别。既有健康叶子如
    `Apple leaf`，也有不健康叶子如 `Apple Scab Leaf`。'
- en: '`leaves.tfrecord` : This is the TFRecord file that contains all of our data.'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`leaves.tfrecord` : 这是包含我们所有数据的 TFRecord 文件。'
- en: Our first step is to inspect `leaves.tfrecord`. What features do our records
    contain? Unfortunately this is not specified by Roboflow.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的第一步是检查 `leaves.tfrecord`。我们的记录包含哪些特征？不幸的是，Roboflow 并未指定这一点。
- en: '[PRE0]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'I see the following features printed:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 我看到以下打印的特征：
- en: '`image/encoded` : This is the encoded binary representation of an image. In
    the case of this dataset the images are encoded in the jpeg format.'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`image/encoded` : 这是图像的编码二进制表示。在这个数据集中，图像是以 jpeg 格式编码的。'
- en: '`image/height` : This is the height of each image.'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`image/height` : 这是每个图像的高度。'
- en: '`image/width` : This is the width of each image.'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`image/width` : 这是每个图像的宽度。'
- en: '`image/object/bbox/xmin` : This is the x-coordinate of the top-left corner
    of our bounding box.'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`image/object/bbox/xmin` : 这是边界框左上角的 x 坐标。'
- en: '`image/object/bbox/xmax` : This is the x-coordinate of the bottom-right corner
    of our bounding box.'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`image/object/bbox/xmax` : 这是边界框右下角的 x 坐标。'
- en: '`image/object/bbox/ymin` : This is the y-coordinate of the top-left corner
    of our bounding box.'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`image/object/bbox/ymin` : 这是边界框左上角的 y 坐标。'
- en: '`image/object/bbox/ymax` : This is the y-coordinate of the bottom-right corner
    of our bounding box.'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`image/object/bbox/ymax` : 这是边界框右下角的 y 坐标。'
- en: '`image/object/class/label` : These are the labels associated with each bounding
    box.'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`image/object/class/label` : 这些是与每个边界框关联的标签。'
- en: Now we want to take all of the images and associated bounding boxes and put
    them together in a TensorFlow **Dataset object**. Dataset objects allow you to
    store large amounts of data without overwhelming your system’s memory. This is
    accomplished through features such as **lazy loading** and **batching**. Lazy
    loading means that the data is not loaded into memory until its explicitly requested
    (for example when performing transformations or during training). Batching means
    that only a select number of images (usually 8, 16, 32, etc.) get loaded into
    memory at once. In short, I recommend always converting your data into Dataset
    objects, especially when you are dealing with large amounts of data (typical in
    object detection).
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们想把所有图像及其关联的边界框整合到一个 TensorFlow **Dataset 对象**中。Dataset 对象允许你存储大量数据而不会使系统内存超载。这是通过**延迟加载**和**批处理**等功能实现的。延迟加载意味着数据不会被加载到内存中，直到它被显式请求（例如在执行转换或训练时）。批处理意味着一次只加载选择数量的图像（通常为8、16、32等）。简而言之，我建议你始终将数据转换为
    Dataset 对象，特别是在处理大量数据时（在目标检测中很常见）。
- en: To convert a TFRecord to a Dataset object in TensorFlow, you can use the `tf.data.TFRecordDataset`
    class to create a dataset from our TFRecord file, and then apply parsing functions
    using the `map` method to extract and preprocess features. The parsing code is
    shown below.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 要将 TFRecord 转换为 TensorFlow 中的 Dataset 对象，你可以使用 `tf.data.TFRecordDataset` 类从
    TFRecord 文件创建数据集，然后使用 `map` 方法应用解析函数来提取和预处理特征。解析代码如下所示。
- en: '[PRE1]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Let’s break this down:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们详细拆解一下：
- en: '`feature_description` : This is a dictionary that describes the expected format
    of each of our features. We use `tf.io.FixedLenFeature` when the length of a feature
    is fixed across all examples in the dataset, and `tf.io.VarLenFeature` when some
    variability in the length is expected. Since the number of bounding boxes is not
    constant across our dataset (some images have more boxes, others have less), we
    use `tf.io.VarLenFeature` for anything related to bounding boxes.'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`feature_description` : 这是一个描述每个特征预期格式的字典。当特征在数据集中所有示例中的长度是固定时，我们使用 `tf.io.FixedLenFeature`，当长度存在某些变动时，我们使用
    `tf.io.VarLenFeature`。由于边界框的数量在数据集中并不固定（有些图像有更多框，有些则较少），因此我们对所有与边界框相关的内容使用 `tf.io.VarLenFeature`。'
- en: We decode the image files using `tf.image.decode_jpeg` , since our images are
    encoded in the JPEG format.
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们使用 `tf.image.decode_jpeg` 解码图像文件，因为我们的图像是以 JPEG 格式编码的。
- en: Note the use of `tf.sparse.to_dense` used for the bounding box coordinates and
    labels. When we use `tf.io.VarLenFeature` the information comes back as a sparse
    matrix. A sparse matrix is a matrix in which most of the elements are zero, resulting
    in a data structure that efficiently stores only the non-zero values along with
    their indices. Unfortunately, many pre-processing functions in TensorFlow require
    dense matrices. This includes `tf.stack` , which we use to horizontally stack
    information from multiple bounding boxes together. To fix this issue, we use `tf.sparse.to_dense`
    to convert the sparse matrices to dense matrices.
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 请注意用于边界框坐标和标签的 `tf.sparse.to_dense` 的使用。当我们使用 `tf.io.VarLenFeature` 时，信息会以稀疏矩阵的形式返回。稀疏矩阵是大多数元素为零的矩阵，结果是一个只有效存储非零值及其索引的数据结构。不幸的是，TensorFlow
    中的许多预处理函数要求使用稠密矩阵。这包括 `tf.stack`，我们用来水平堆叠来自多个边界框的信息。为了解决这个问题，我们使用 `tf.sparse.to_dense`
    将稀疏矩阵转换为稠密矩阵。
- en: After stacking the boxes, we use KerasCV’s `keras_cv.bounding_box.convert_format`
    function. When inspecting the data, I noticed that the bounding box coordinates
    were normalized between 0 and 1\. This means that the numbers represent percentages
    of the images total width/height. So a value of 0.5 represents 50% * image_width,
    as an example. This is a **relative format**, which Keras refers to as `REL_XYXY`
    , rather than the **absolute format** `XYXY`. In theory converting to the absolute
    format is not necessary, but I was running into bugs when training my model with
    relative coordinates. See the [KerasCV documentation](https://keras.io/api/keras_cv/bounding_box/formats/#rel_xyxy-class)
    for some other supported bounding box formats.
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在堆叠框之后，我们使用 KerasCV 的 `keras_cv.bounding_box.convert_format` 函数。检查数据时，我注意到边界框坐标被归一化在
    0 和 1 之间。这意味着这些数字表示图像总宽度/高度的百分比。例如，值为 0.5 表示 50% * image_width。这是一种 **相对格式**，Keras
    称之为 `REL_XYXY`，而不是 **绝对格式** `XYXY`。理论上，转换为绝对格式不是必要的，但当我使用相对坐标训练模型时遇到了错误。有关其他支持的边界框格式，请参见
    [KerasCV 文档](https://keras.io/api/keras_cv/bounding_box/formats/#rel_xyxy-class)。
- en: 'Finally, we take the images and bounding boxes and convert them into the format
    that KerasCV wants: **dictionaries**. A Python dictionary is a data type that
    contains key-value pairs. Specifically, KerasCV expects the following format:'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，我们将图像和边界框转换为 KerasCV 所需的格式：**字典**。Python 字典是一种包含键值对的数据类型。具体来说，KerasCV 期望以下格式：
- en: '[PRE2]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: This is actually a “dictionary within a dictionary”, since `bounding_boxes`
    is also a dictionary.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 这实际上是一个“字典中的字典”，因为 `bounding_boxes` 也是一个字典。
- en: Finally use the `.map` function to apply the parsing function to our TFRecord.
    You may then inspect the Dataset object. Everything checks out.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 最后使用 `.map` 函数将解析函数应用于我们的 TFRecord。然后可以检查 Dataset 对象。一切正常。
- en: '[PRE3]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '**Congratulations, the hardest part is now over with.** Creating the “dictionary
    within a dictionary” that KerasCV wants is the most difficult task in my opinion.
    The rest is more straightforward.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '**恭喜，最困难的部分现在已经完成了。** 在我看来，创建 KerasCV 所需的“字典中的字典”是最具挑战性的任务。其余部分更为直接。'
- en: Pre-Processing Images
  id: totrans-79
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 图像预处理
- en: Our data is already split into training and validation sets. So we will begin
    by batching our datasets.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的数据已经分为训练集和验证集。所以我们将开始对数据集进行批处理。
- en: '[PRE4]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'A few notes:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 一些说明：
- en: 'We are using `ragged_batch` for the same reason we used `VarLenFeature` : we
    don’t know in advance how many bounding boxes we will have for each image. If
    all of the images had the same number of bounding boxes, then we would just use
    `batch` instead.'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们使用 `ragged_batch` 是因为我们不知道每个图像将有多少个边界框。如果所有图像都有相同数量的边界框，那么我们可以直接使用 `batch`。
- en: We set `BBOX_FORMAT=“xyxy”` . Recall that earlier when loading in data we converted
    the bounding box format from the relative XYXY format to the absolute XYXY format.
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们设置了 `BBOX_FORMAT=“xyxy”` 。回忆一下，之前在加载数据时，我们将边界框格式从相对的 XYXY 格式转换为绝对的 XYXY 格式。
- en: 'Now we can implement **data augmentation**. Data augmentation is a common technique
    in computer vision problems. It modifies your training images slightly e.g. a
    slight rotation, horizontally flipping the images, etc. This helps solve the problem
    of having too little data and also helps with regularization. Here we will introduce
    the following augmentations:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以实现 **数据增强**。数据增强是计算机视觉问题中的一种常见技术。它对训练图像进行轻微的修改，例如轻微旋转、水平翻转图像等。这有助于解决数据不足的问题，并且有助于正则化。在这里，我们将引入以下增强方法：
- en: KerasCV’s `JitteredResize` function. This function is designed for object detection
    pipelines and implements an image augmentation technique that involves randomly
    scaling, resizing, cropping, and padding images along with corresponding bounding
    boxes. This process introduces variability in scale and local features, enhancing
    the diversity of the training data for improved generalization.
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: KerasCV的`JitteredResize`函数。这个函数旨在用于目标检测管道，实现了一种图像增强技术，涉及随机缩放、调整大小、裁剪和填充图像及相应的边界框。这一过程引入了尺度和局部特征的变异，提高了训练数据的多样性，从而改善了模型的泛化能力。
- en: We then add horizontal and vertical `RandomFlips` as well as a `RandomRotation`.
    Here the `factor` is a float that represents a fraction of 2*π.* We use.25, which
    means that our augmenter will rotate our images by some number between -25% of
    *π* to 25% of *π.* In degrees this means between -45° to 45° rotations.
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然后我们添加了水平和垂直的`RandomFlips`以及`RandomRotation`。这里的`factor`是一个表示2*π*分数的浮点数。我们使用0.25，这意味着我们的增强器会将图像旋转-25%到25%*π*之间的某个角度。以度数表示，这意味着旋转范围在-45°到45°之间。
- en: Finally we add in `RandomSaturation` and `RandomHue` . A saturation of 0.0 would
    leave a grayscale image, while 1.0 would be fully saturated. A factor of 0.5 would
    leave no change, so choosing a range of 0.4–0.6 results in a subtle change. A
    hue factor of 0.0 would leave no change. Putting `factor=0.2` implies a range
    of 0.0–0.2, another subtle change.
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，我们添加了`RandomSaturation`和`RandomHue`。饱和度为0.0会留下灰度图像，而1.0则完全饱和。0.5的因子不会造成任何变化，因此选择0.4–0.6的范围会产生细微的变化。色调因子为0.0不会产生变化。设置`factor=0.2`表示范围为0.0–0.2，这是另一种细微变化。
- en: '[PRE5]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: We typically only augment the *training* set because we want the model to avoid
    “memorizing” patterns and instead make sure the model learns general patterns
    that will be found in the real world. This increases the diversity of what the
    model sees during training.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通常只对*训练*集进行增强，因为我们希望模型避免“记忆”模式，而是确保模型学习到在现实世界中会遇到的通用模式。这增加了模型在训练过程中看到的多样性。
- en: 'We also want to resize the *validation* images to be the same size (with padding).
    These will be resized without any distortion. The bounding boxes must also be
    reshaped accordingly. KerasCV can handle this difficult task with ease:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还希望将*验证*图像调整为相同的大小（带有填充）。这些图像将在不失真的情况下调整大小。边界框也必须相应地重新调整。KerasCV可以轻松处理这一困难任务：
- en: '[PRE6]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Finally we can visualize our images and bounding boxes with the pre-processing
    included:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以可视化我们的图像和包含预处理的边界框：
- en: '[PRE7]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: This type of visualization function is common in KerasCV. It plots a grid of
    images and boxes with the rows and columns specified in the arguments. We see
    that our training images have been slightly rotated, some have been horizontally
    or vertically flipped, they may have been zoomed in or out., and subtle changes
    in hue/saturation can be seen. **With all augmentation layers in KerasCV, the
    bounding boxes also get augmented if necessary.** Note that `class_mapping` is
    a simple dictionary. I got both keys and labels from the `leaves_label_map.pbtxt`
    text file mentioned earlier.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 这种类型的可视化函数在KerasCV中很常见。它绘制了一组图像和框，行和列由参数指定。我们看到我们的训练图像有些被轻微旋转，有些被水平或垂直翻转，可能还进行了放大或缩小，并且色调/饱和度的细微变化也可以看到。**在KerasCV中，所有增强层也会在必要时增强边界框。**
    请注意，`class_mapping`是一个简单的字典。我从之前提到的`leaves_label_map.pbtxt`文本文件中获得了键和标签。
- en: '![](../Images/ba2e39bc14c00635799df9b0585f3e84.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ba2e39bc14c00635799df9b0585f3e84.png)'
- en: Examples of the original images on the left (validation set) and the augmented
    images on the right (training set). Images by author.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 左侧是原始图像（验证集）的示例，右侧是增强图像（训练集）。图片由作者提供。
- en: 'One last thing before looking at the RetinaNet model. Earlier we had to create
    the “dictionary within a dictionary” to get the data into a format compatible
    with KerasCV pre-processing, but now we need to convert it to a tuple of numbers
    to feed to our model for training. This is fairly straight forward to do:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 在查看RetinaNet模型之前最后要说的一件事是，之前我们需要创建“字典中的字典”以将数据转换为与KerasCV预处理兼容的格式，但现在我们需要将其转换为数字元组以供模型训练。这相当直接：
- en: '[PRE8]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: RetinaNet Model Background
  id: totrans-100
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: RetinaNet模型背景
- en: 'One popular model for conducting object detection is called **RetinaNet**.
    A detailed description of the model is beyond the scope of this article. In brief,
    RetinaNet is a single-stage detector, meaning it only looks at the image once
    before predicting bounding boxes and classes. This is similar to the famous YOLO
    (You Only Look Once) model, but with some important differences. What I want to
    highlight here is the novel classification loss function used: the **focal loss**.
    This solves the issue of *class imbalance* in an image.'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 一个用于进行目标检测的流行模型叫做**RetinaNet**。该模型的详细描述超出了本文的范围。简而言之，RetinaNet 是一个单阶段检测器，意味着它在预测边界框和类别之前只查看一次图像。这类似于著名的
    YOLO（You Only Look Once）模型，但有一些重要的不同之处。我在这里要强调的是使用的创新分类损失函数：**focal loss**。它解决了图像中的*类别不平衡*问题。
- en: 'To understand why this is important, consider the following analogy: imagine
    you are a teacher in a room of 100 students. 95 students are loud and rambunctious,
    always yelling and raising their hands. 5 students are quiet and don’t say much.
    As the teacher you need to pay attention to everyone equally, but the loud students
    are crowding out the quiet ones. *Here you have a problem of class imbalance*.
    To fix the issue, you develop a special hearing aid that boosts the quiet students
    and deemphasizes the loud students. In this analogy, the loud students are the
    vast majority of background pixels in our images that do not contain leaves, while
    the quiet students are those small regions that do. The “hearing aid” is the focal
    loss, which allows us to focus our model on those pixels that contain leaves,
    without paying too much attention to those that do not.'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解这点的重要性，可以考虑以下类比：假设你是一名教室里有 100 个学生的老师。95 个学生吵闹且喧哗，总是喊叫和举手。5 个学生安静，不怎么说话。作为老师，你需要平等关注每个人，但吵闹的学生正在挤走安静的学生。*这里你遇到了类别不平衡的问题*。为了解决这个问题，你开发了一种特殊的助听器，它增强了安静学生的声音并弱化了吵闹学生的声音。在这个类比中，吵闹的学生是我们图像中不包含叶子的背景像素的大多数，而安静的学生是那些包含叶子的少量区域。这个“助听器”就是
    focal loss，它使我们可以将模型集中在包含叶子的像素上，而不会过多关注那些不包含叶子的像素。
- en: 'There are three important components of the RetinaNet model:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: RetinaNet 模型有三个重要组件：
- en: '**A** **backbone**. This forms the base of the model. We also call this a **feature
    extractor**. As the name suggests, it takes an image and scans for features. Low-level
    layers extract low-level features (e.g. lines and curves) while higher-level layers
    extract high-level features (e.g. lips and eyes). In this project the backbone
    will be a YOLOv8 model that has been pre-trained on the [COCO dataset](https://cocodataset.org/#home).
    We are only using YOLO only as a feature extractor, **not as an object detector.**'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**一个** **骨干网络**。这构成了模型的基础。我们也称之为**特征提取器**。顾名思义，它接收图像并扫描特征。低层提取低级特征（例如线条和曲线），而高层提取高级特征（例如嘴唇和眼睛）。在这个项目中，骨干网络将是一个在[COCO
    数据集](https://cocodataset.org/#home)上进行过预训练的 YOLOv8 模型。我们只将 YOLO 用作特征提取器，**而不是作为目标检测器。**'
- en: '**Feature pyramid network (FPN)**. This is a model architecture that generates
    a “pyramid” of feature maps at different scales to detect objects of various sizes.
    It does this by combining low-resolution, semantically strong features with high-resolution,
    semantically weak features via a top-down pathway and lateral connections. Take
    a look at [this video](https://www.youtube.com/watch?v=FKsgO0U7CUw) for a detailed
    explanation or take a [look at the paper](https://arxiv.org/pdf/1612.03144.pdf)
    [5] that introduced the FPN.'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**特征金字塔网络（FPN）**。这是一种模型架构，在不同的尺度上生成“金字塔”特征图，以检测各种大小的对象。它通过通过自上而下的路径和横向连接将低分辨率的语义强特征与高分辨率的语义弱特征结合起来。查看[这个视频](https://www.youtube.com/watch?v=FKsgO0U7CUw)以获取详细解释，或查看[这篇论文](https://arxiv.org/pdf/1612.03144.pdf)
    [5]，该论文介绍了 FPN。'
- en: '**Two task-specific subnetworks.** These subnetworks take each level of the
    pyramid and detects objects in each. One subnetwork identifies classes (classification)
    while the other identifies bounding boxes (regression). These subnetworks are
    untrained.'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**两个任务特定的子网络。** 这些子网络处理金字塔的每一层，并检测每层中的对象。一个子网络用于识别类别（分类），另一个用于识别边界框（回归）。这些子网络尚未训练。'
- en: '![](../Images/fea3f7047b7ed2e45adb64c47906a800.png)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fea3f7047b7ed2e45adb64c47906a800.png)'
- en: Simplified RetinaNet architecture. Image by author.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 简化的 RetinaNet 架构。图片由作者提供。
- en: Earlier we resized the images to be of size 416 by 416\. This is a somewhat
    arbitrary choice, although the object detection model you pick will often specify
    a desired minimum size. For the YOLOv8 backbone we are using, the image size should
    be divisible by 32\. This is because the maximum stride of the backbone is 32
    and it is a fully convolutional network. Do your homework on any model you use
    to figure out this factor for your own projects.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 之前我们将图像调整为 416x416 的大小。这是一个有点随意的选择，尽管你选择的目标检测模型通常会指定一个所需的最小大小。对于我们使用的 YOLOv8
    主干，图像大小应该是 32 的倍数。这是因为主干的最大步幅是 32，而且它是一个完全卷积网络。对于你自己项目中使用的任何模型，请进行调研以找出这个因素。
- en: Training RetinaNet
  id: totrans-110
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练 RetinaNet
- en: 'Let’s begin by setting up some basic parameters, such as the optimizer and
    the metrics we will be using. Here we will be using Adam as our optimizer. Note
    the `global_clip_norm` argument. According to the [KerasCV object detection guide](https://keras.io/guides/keras_cv/object_detection_keras_cv/):'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从设置一些基本参数开始，比如优化器和我们将使用的指标。这里我们将使用 Adam 作为优化器。请注意`global_clip_norm`参数。根据[KerasCV
    目标检测指南](https://keras.io/guides/keras_cv/object_detection_keras_cv/)：
- en: You will always want to include a `global_clipnorm` when training object detection
    models. This is to remedy exploding gradient problems that frequently occur when
    training object detection models.
  id: totrans-112
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 在训练目标检测模型时，你总是希望包含`global_clipnorm`。这是为了修复在训练目标检测模型时经常出现的梯度爆炸问题。
- en: '[PRE9]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: We will follow their advice. For our metrics we will be using the **BoxCOCOMetrics**.
    These are popular metrics for object detection. They essentially consist of the
    **mean average precision (mAP)** and the **mean average recall (mAR).** In summary,
    mAP quantifies how effectively the model locates and identifies objects by measuring
    the average area of correct object detections relative to the total area covered
    by the model’s predictions. mAR is a different score that assesses the model's
    ability to capture the complete extent of objects by calculating the average ratio
    of the correctly identified object area to the actual object area. See [this article](https://cocodataset.org/#detection-eval)
    for exact details on the metrics. [This video](https://www.youtube.com/watch?v=qWfzIYCvBqo)
    does a great explanation of the basics of precision and recall.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将遵循他们的建议。对于我们的指标，我们将使用**BoxCOCOMetrics**。这些是目标检测中流行的指标。它们基本上包括**平均精度 (mAP)**
    和**平均召回率 (mAR)**。总的来说，mAP 通过测量正确对象检测的平均面积与模型预测覆盖的总面积的比率来量化模型定位和识别对象的有效性。mAR 是一个不同的分数，通过计算正确识别的对象区域与实际对象区域的平均比例来评估模型捕获对象全部范围的能力。有关指标的详细信息，请参见[这篇文章](https://cocodataset.org/#detection-eval)。
    [这段视频](https://www.youtube.com/watch?v=qWfzIYCvBqo) 对精度和召回率的基本知识进行了很好的解释。
- en: '[PRE10]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Because the box metrics are computationally expensive to compute, we pass the
    `evaluate_freq=5` argument to tell our model to compute the metrics after every
    five batches rather than every single batch during training. I noticed that with
    too high a number the validation metrics weren’t being printed out at all.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 由于框的指标计算开销很大，我们传递`evaluate_freq=5`参数，以告知我们的模型在每五个批次后计算指标，而不是在训练期间每个批次后计算。我注意到，当数字设置得过高时，验证指标根本没有被打印出来。
- en: 'Let’s continue by looking at the callbacks we will be using:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们继续查看我们将使用的回调：
- en: '[PRE11]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '**Early stopping.** If the validation loss has not improved after six epochs,
    we will stop training.'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**早停**。如果验证损失在六个周期后没有改善，我们将停止训练。'
- en: '**Model checkpoint**. We will be checking the `val_loss` after every epoch,
    saving the model weights if it bests an earlier epoch.'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型检查点**。我们将在每个周期后检查`val_loss`，如果它优于早期的周期，将保存模型权重。'
- en: '**Lambda callback.** A lambda callback is a custom callback that allows you
    to define and execute arbitrary Python functions during training at different
    points in each epoch. In this case we are using it to print custom metrics after
    each epoch. If you just print the COCOMetrics out it is a mess of numbers. For
    simplicity, we will only print out the training and validation loss and mAP.'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Lambda 回调**。Lambda 回调是一个自定义回调，允许你在训练过程中于每个周期的不同点定义并执行任意 Python 函数。在这种情况下，我们用它来在每个周期后打印自定义指标。如果直接打印
    COCOMetrics，会是一堆杂乱的数字。为了简化，我们只打印训练和验证的损失和 mAP。'
- en: '**Visualization of detections.** This will print out a 4 by 8 grid of images
    along with predicted bounding boxes after every five epochs. This will give us
    insight into how good (or terrible) our model is. If all goes well these visualizations
    should get better as training progresses.'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**检测的可视化。** 这将在每五个周期后打印出一个 4x8 的图像网格以及预测的边界框。这将使我们洞察我们的模型有多好（或多糟）。如果一切顺利，这些可视化效果应该随着训练的进行而变得更好。'
- en: Finally we create our model. Recall that the backbone is a YOLOv8 model. We
    must pass the `num_classes` we will be using, as well as the `bounding_box_format`.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 最终我们创建了我们的模型。回顾一下，主干是一个 YOLOv8 模型。我们必须传递我们将使用的 `num_classes`，以及 `bounding_box_format`。
- en: '[PRE12]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'We also have to customize the **non-max suppression** parameter of our model.
    Non-max suppression is used in object detection to filter out multiple overlapping
    predicted bounding boxes that correspond to the same object. It only keeps the
    box with the highest confidence score and removes redundant boxes, ensuring that
    each object is detected only once. It incorporates two parameters: the `iou_threshold`
    and the `confidence_threshold:`'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还必须自定义模型的 **非极大值抑制** 参数。非极大值抑制用于目标检测中，以过滤掉多个重叠的预测边界框，这些框对应于同一对象。它只保留置信度分数最高的框，并删除冗余的框，确保每个对象只被检测一次。它包含两个参数：`iou_threshold`
    和 `confidence_threshold`。
- en: IoU, or **intersection over union**, is a number between 0 and 1 that measures
    how much overlap there is between one predicted box and another predicted box.
    If the overlap is *higher* than the `iou_threshold` then the predicted box with
    the lower confidence score is thrown away.
  id: totrans-126
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: IoU，或 **交并比**，是一个介于 0 和 1 之间的数字，衡量一个预测框与另一个预测框之间的重叠程度。如果重叠*高于* `iou_threshold`，则置信度较低的预测框会被丢弃。
- en: The **confidence score** reflects the model’s confidence in its predicted bounding
    box. If the confidence score for a predicted box is *lowe*r than the `confidence_threshold`
    the box is thrown away.
  id: totrans-127
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**置信度分数**反映了模型对其预测的边界框的信心。如果预测框的置信度分数*低于* `confidence_threshold`，则该框会被丢弃。'
- en: Although these parameters do not affect training, they still need to be tuned
    to your particular application for prediction purposes. Setting `iou_threshold=0.5`
    and `confidence_threshold=0.5` is a good starting place.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这些参数不会影响训练，但它们仍需根据您的特定应用进行调整以用于预测。设置 `iou_threshold=0.5` 和 `confidence_threshold=0.5`
    是一个好的起点。
- en: 'One note before beginning training: we discussed why it is helpful for the
    *classification loss* to be the focal loss, but we have not discussed a suitable
    *regression loss* to define the error on our predicted bounding boxes coordinates.
    A popular regression loss (or `box_loss`) is the **smooth L1 loss.** I think of
    smooth L1 as a “best of both worlds” loss. It incorporates both the L1 loss (absolute
    error) and the L2 loss (mean squared error). The loss is quadratic for small error
    values, and linear for large error values ([see this link](https://en.wikipedia.org/wiki/Huber_loss)).
    KerasCV has a built-in smooth L1 loss for our convenience. The loss that will
    be displayed during training will be the sum of `box_loss` and `classification_loss`
    .'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始训练之前有一点需要注意：我们讨论了为什么将 *分类损失* 设置为焦点损失是有帮助的，但我们还没有讨论定义预测边界框坐标误差的合适 *回归损失*。一种流行的回归损失（或
    `box_loss`）是 **平滑 L1 损失**。我认为平滑 L1 是一种“兼顾两全”的损失。它结合了 L1 损失（绝对误差）和 L2 损失（均方误差）。当误差值较小时，损失是二次的，当误差值较大时，损失是线性的（[查看此链接](https://en.wikipedia.org/wiki/Huber_loss)）。KerasCV
    为我们的便利提供了内置的平滑 L1 损失。训练期间显示的损失将是 `box_loss` 和 `classification_loss` 的总和。
- en: '[PRE13]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Training on an NVIDIA Tesla P100 GPU takes about one hour and 12 minutes.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 在 NVIDIA Tesla P100 GPU 上训练大约需要一个小时 12 分钟。
- en: Making Predictions
  id: totrans-132
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进行预测
- en: '[PRE14]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Now we can load the best model seen during training and use it to make some
    predictions on the validation set:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以加载在训练过程中看到的最佳模型，并用它对验证集进行一些预测：
- en: '![](../Images/c2ba62fd10557e95258f2a613654b162.png)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c2ba62fd10557e95258f2a613654b162.png)'
- en: Sample visual of validation set predictions. Image by author.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 验证集预测的样本视觉效果。图片由作者提供。
- en: 'The metrics on our best model are:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 我们最佳模型的指标是：
- en: 'Loss: 0.4185'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '损失: 0.4185'
- en: 'mAP: 0.2182'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'mAP: 0.2182'
- en: 'Validation Loss: 0.4584'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '验证损失: 0.4584'
- en: 'Validation mAP: 0.2916'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '验证集 mAP: 0.2916'
- en: 'Respectable, but this can be improved. More on this in the conclusion. (Note:
    I noticed that `MultiClassNonMaxSuppression` does not seem to be working correctly.
    The bottom left image shown above clearly has boxes that overlap with more than
    20% of their area, yet the lower confidence box is not suppressed. This is something
    I will have to look more into.)'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 值得尊敬，但还有改进的空间。更多内容将在结论中讨论。（注意：我发现`MultiClassNonMaxSuppression`似乎没有正常工作。上面显示的左下角图像明显有超过20%重叠的框，但较低置信度的框没有被抑制。这是我需要进一步研究的问题。）
- en: Here is a plot of our training and validation losses per epoch. Some overfitting
    is seen. Also, it may be wise to add in a learning rate schedule to decrease the
    learning rate over time. This may help resolve the issue of large jumps being
    made near the end of training.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是我们每个训练周期和验证周期的损失图。可以看到有些过拟合现象。此外，增加一个学习率调度器以逐渐降低学习率可能是明智的。这可能有助于解决在训练结束时出现的大幅跳跃问题。
- en: '![](../Images/18a89c5dacade58fccdc3cc8b6b73220.png)'
  id: totrans-144
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/18a89c5dacade58fccdc3cc8b6b73220.png)'
- en: A plot of our training and validation losses per epoch. We are seeing signs
    of overfitting. Image by author.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 每个训练周期和验证周期的损失图。我们看到了一些过拟合的迹象。图片由作者提供。
- en: Conclusion
  id: totrans-146
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: 'If you have made it this far give yourself a pat on the back! Object detection
    is among the more difficult tasks in computer vision. Luckily for us we have the
    new KerasCV library to make our lives easier. To summarize the workflow for creating
    an object detection pipeline:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你已经做到这一步，给自己一个赞美吧！目标检测是计算机视觉中较为困难的任务之一。幸运的是，我们有新的KerasCV库来简化我们的工作。总结一下创建目标检测管道的工作流程：
- en: 'Begin by visualizing your dataset. Ask yourself questions like: “What is my
    bounding box format? Is it *xyxy*? *Rel*x*yxy*? How many classes am I dealing
    with?” Make sure to create a function similar to `visualize_dataset` to look at
    your images and bounding boxes.'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 开始时可视化你的数据集。问自己一些问题：“我的边界框格式是什么？是*xyxy*？*Rel*x*yxy*？我处理多少个类别？”确保创建一个类似于`visualize_dataset`的函数来查看你的图像和边界框。
- en: Convert whatever format of data you have into the “dictionary within a dictionary”
    format that KerasCV wants. Using a TensorFlow Dataset object to hold the data
    is especially helpful.
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将你拥有的任何格式的数据转换为KerasCV所需的“字典中的字典”格式。使用TensorFlow Dataset对象来存储数据特别有帮助。
- en: Do some basic pre-processing, such as image re-sizing and data augmentation.
    KerasCV makes this fairly simple. Take care to read the literature on your model
    of choice to make sure the image sizes are appropriate.
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 进行一些基本的预处理，例如图像缩放和数据增强。KerasCV使这些操作相对简单。请注意查阅你选择的模型的文献，以确保图像尺寸适当。
- en: Convert the dictionaries back into tuples for training.
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将字典转换回元组以用于训练。
- en: Select an **optimizer** (*Adam* is an easy choice), **two loss functions** (*focal*
    for the class loss and *L1 smooth* for the box loss are easy choices), and **metrics**
    (*COCO metrics* are an easy choice).
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择一个**优化器**（*Adam*是一个简单的选择），**两个损失函数**（*focal*用于类别损失，*L1 smooth*用于框损失是简单的选择），以及**指标**（*COCO
    metrics*是一个简单的选择）。
- en: Visualizing your detections during training can be instructive to see what sorts
    of objects your model is missing.
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在训练期间可视化你的检测结果可以帮助了解你的模型遗漏了哪些对象。
- en: '![](../Images/df0a7bd248b9052967fbb97a513d3f7f.png)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/df0a7bd248b9052967fbb97a513d3f7f.png)'
- en: Example of a problematic label in the dataset. Image by author.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集中问题标签的示例。图片由作者提供。
- en: 'One of the primary next steps would be to clean up the dataset. For example,
    take a look at the image above. The labelers correctly identified the *potato
    leaf late blight*, but what about all of the other healthy potato leaves? Why
    were these not labeled as *potato leaf*? Looking at the health check tab on the
    Roboflow website, you can see that some classes are vastly underrepresented in
    the dataset:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 主要的下一步之一是清理数据集。例如，查看上面的图像。标注者正确地识别了*马铃薯叶晚疫病*，但其他所有健康的马铃薯叶子呢？为什么这些没有标注为*马铃薯叶*？查看Roboflow网站上的健康检查标签，你可以看到某些类别在数据集中严重不足：
- en: '![](../Images/0c473c3d4a8b36a21cf0de2c22302f06.png)'
  id: totrans-157
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0c473c3d4a8b36a21cf0de2c22302f06.png)'
- en: Chart showing the class imbalance. [From Roboflow’s website](https://public.roboflow.com/object-detection/plantdoc/health).
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 显示类别不平衡的图表。[来自Roboflow的网站](https://public.roboflow.com/object-detection/plantdoc/health)。
- en: Try fixing these issues before tweaking any hyperparameters. Best of luck on
    your object detection tasks!
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 在调整任何超参数之前，先尝试修复这些问题。祝你在目标检测任务中好运！
- en: References
  id: totrans-160
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] F. Chollet, [Deep Learning with Pytho](https://www.manning.com/books/deep-learning-with-python-second-edition)n
    (2021), Manning Publications Co.'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] F. Chollet, [用Python进行深度学习](https://www.manning.com/books/deep-learning-with-python-second-edition)(2021),
    Manning Publications Co.'
- en: '[2] A. Géron, [Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow](https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/)
    (2022), O’Reily Media Inc.'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] A. Géron, [动手实践机器学习：使用Scikit-Learn、Keras和TensorFlow](https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/)
    (2022), O’Reily Media Inc.'
- en: '[3] A. Ng, [Deep Learning Specialization](https://www.coursera.org/specializations/deep-learning),
    DeepLearning.AI'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] A. Ng, [深度学习专项课程](https://www.coursera.org/specializations/deep-learning),
    DeepLearning.AI'
- en: '[4] D. Singh, N. Jain, P. Jain, P. Kayal, S. Kumawat, N. Batra, [PlantDoc:
    A Dataset for Visual Plant Disease Detection](https://arxiv.org/pdf/1911.10317.pdf)
    (2019), CoDS COMAD 2020'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] D. Singh, N. Jain, P. Jain, P. Kayal, S. Kumawat, N. Batra, [PlantDoc：用于视觉植物疾病检测的数据集](https://arxiv.org/pdf/1911.10317.pdf)
    (2019), CoDS COMAD 2020'
- en: '[5] T. Lin, P. Dollár, R. Girshick, K. He, B. Hariharan, S. Belongie, [Feature
    Pyramid Networks for Object Detection](https://arxiv.org/abs/1612.03144)(2017),
    CVPR 2017'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: '[5] T. Lin, P. Dollár, R. Girshick, K. He, B. Hariharan, S. Belongie, [用于目标检测的特征金字塔网络](https://arxiv.org/abs/1612.03144)(2017),
    CVPR 2017'
- en: '[6] T. Lin, P. Goyal, R. Girshick, K. He, P. Dollar, [Focal Loss for Object
    Detection](https://arxiv.org/abs/1708.02002)(2020), IEEE Transactions on Pattern
    Analysis and Machine Intelligence'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: '[6] T. Lin, P. Goyal, R. Girshick, K. He, P. Dollar, [目标检测中的焦点损失](https://arxiv.org/abs/1708.02002)(2020),
    IEEE模式分析与机器智能学报'
