- en: Vision-Based Rep Counting in the Wild
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 视觉基础的重复计数在实际应用中的探索
- en: 原文：[https://towardsdatascience.com/vision-based-rep-counting-in-the-wild-cb9a4d1bdb7e?source=collection_archive---------10-----------------------#2023-02-21](https://towardsdatascience.com/vision-based-rep-counting-in-the-wild-cb9a4d1bdb7e?source=collection_archive---------10-----------------------#2023-02-21)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/vision-based-rep-counting-in-the-wild-cb9a4d1bdb7e?source=collection_archive---------10-----------------------#2023-02-21](https://towardsdatascience.com/vision-based-rep-counting-in-the-wild-cb9a4d1bdb7e?source=collection_archive---------10-----------------------#2023-02-21)
- en: A review of different approaches to vision-based rep counting
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 对视觉基础重复计数的不同方法进行回顾
- en: '[](https://medium.com/@aakashagrawal?source=post_page-----cb9a4d1bdb7e--------------------------------)[![Aakash
    Agrawal](../Images/29c88586046f4b51d40cc0336f696cef.png)](https://medium.com/@aakashagrawal?source=post_page-----cb9a4d1bdb7e--------------------------------)[](https://towardsdatascience.com/?source=post_page-----cb9a4d1bdb7e--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----cb9a4d1bdb7e--------------------------------)
    [Aakash Agrawal](https://medium.com/@aakashagrawal?source=post_page-----cb9a4d1bdb7e--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@aakashagrawal?source=post_page-----cb9a4d1bdb7e--------------------------------)[![Aakash
    Agrawal](../Images/29c88586046f4b51d40cc0336f696cef.png)](https://medium.com/@aakashagrawal?source=post_page-----cb9a4d1bdb7e--------------------------------)[](https://towardsdatascience.com/?source=post_page-----cb9a4d1bdb7e--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----cb9a4d1bdb7e--------------------------------)
    [Aakash Agrawal](https://medium.com/@aakashagrawal?source=post_page-----cb9a4d1bdb7e--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F93ce827b6548&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fvision-based-rep-counting-in-the-wild-cb9a4d1bdb7e&user=Aakash+Agrawal&userId=93ce827b6548&source=post_page-93ce827b6548----cb9a4d1bdb7e---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----cb9a4d1bdb7e--------------------------------)
    ·9 min read·Feb 21, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fcb9a4d1bdb7e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fvision-based-rep-counting-in-the-wild-cb9a4d1bdb7e&user=Aakash+Agrawal&userId=93ce827b6548&source=-----cb9a4d1bdb7e---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F93ce827b6548&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fvision-based-rep-counting-in-the-wild-cb9a4d1bdb7e&user=Aakash+Agrawal&userId=93ce827b6548&source=post_page-93ce827b6548----cb9a4d1bdb7e---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----cb9a4d1bdb7e--------------------------------)
    ·9 分钟阅读·2023年2月21日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fcb9a4d1bdb7e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fvision-based-rep-counting-in-the-wild-cb9a4d1bdb7e&user=Aakash+Agrawal&userId=93ce827b6548&source=-----cb9a4d1bdb7e---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fcb9a4d1bdb7e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fvision-based-rep-counting-in-the-wild-cb9a4d1bdb7e&source=-----cb9a4d1bdb7e---------------------bookmark_footer-----------)![](../Images/a982fa9483f349cd3eb00878266278f6.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fcb9a4d1bdb7e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fvision-based-rep-counting-in-the-wild-cb9a4d1bdb7e&source=-----cb9a4d1bdb7e---------------------bookmark_footer-----------)![](../Images/a982fa9483f349cd3eb00878266278f6.png)'
- en: 'src: photo by [@paragdmehta](https://unsplash.com/@paragdmehta), illustrating
    a repetitive pattern.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 'src: 图片由 [@paragdmehta](https://unsplash.com/@paragdmehta) 提供，展示了一个重复的模式。'
- en: '*In this article, I try to explain my exploration of different vision-based
    repetition counting techniques and discuss their pros and cons. Specifically,
    I highlight five major ways in which computer vision has been employed for rep
    counting.*'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '*在这篇文章中，我尝试解释我对不同视觉基础的重复计数技术的探索，并讨论它们的优缺点。具体而言，我重点介绍了计算机视觉在重复计数中应用的五种主要方式。*'
- en: Wearable sensors have been quite popular for reps and set counting. Owing to
    the fact that these sensors are expensive and, in most cases, are only limited
    to tracking a particular body part, lately, a lot of focus has been on using vision-based
    approaches for rep counting.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 穿戴式传感器在重复和组计数中非常受欢迎。由于这些传感器价格昂贵，而且在大多数情况下仅限于跟踪特定的身体部位，近年来，越来越多的关注转向了使用视觉基础的方法进行重复计数。
- en: From countless applications in activity monitoring, sports, and gaming to helping
    gain insight into the number of times a biological event (heartbeat, pulse count,
    etc.) occurs, Rep counting is a problem actively being solved in both academia
    and industry.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 从活动监控、运动和游戏中的无数应用，到帮助了解生物事件（心跳、脉搏计数等）发生的次数，重复计数是学术界和工业界积极解决的问题。
- en: '**Keywords:** *Rep Counting, Computer Vision, Pose Estimation.*'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '**关键词：** *重复计数、计算机视觉、姿态估计。*'
- en: Table of Contents
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 目录
- en: '***RepNet: Class Agnostic rep counting in the Wild***'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***RepNet: 无类别的重复计数应用***'
- en: '***Rule-based exercise rep counting using Pose Estimation***'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***基于规则的姿态估计运动重复计数***'
- en: '***Exercise rep counting using ideas from Signal Processing***'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***基于信号处理的运动重复计数***'
- en: '***GymCam***'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***GymCam***'
- en: '***Rep counting using a DL-based Optical Flow Approach***'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***基于深度学习的光流方法进行重复计数***'
- en: Most of the techniques we discuss in the blog won’t be generic but rather exclusive
    to a specific problem (for example, workouts). Also, for a deeper understanding
    of the technique, please refer to the references provided.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在博客中讨论的大多数技术不会是通用的，而是专门针对特定问题（例如锻炼）。此外，若要深入了解该技术，请参阅提供的参考文献。
- en: '1\. RepNet: Class Agnostic Rep Counting in the Wild'
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '1. RepNet: 无类别的重复计数应用'
- en: '*paper:* [Counting Out Time: Class Agnostic Video Repetition Counting in the
    Wild](https://arxiv.org/pdf/2006.15418.pdf) [1]'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '*论文：* [Counting Out Time: 无类别的视频重复计数](https://arxiv.org/pdf/2006.15418.pdf)
    [1]'
- en: '![](../Images/2df6017bc6e76280436c625e2a0d26ef.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2df6017bc6e76280436c625e2a0d26ef.png)'
- en: 'Fig: RepNet architecture. src: [https://arxiv.org/pdf/2006.15418.pdf](https://arxiv.org/pdf/2006.15418.pdf)'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 图：RepNet 体系结构。源：[https://arxiv.org/pdf/2006.15418.pdf](https://arxiv.org/pdf/2006.15418.pdf)
- en: One of the most prominent works around Rep Counting has been the **RepNet**,an
    end-to-end deep learning model that can accurately predict counts on a broad range
    of repetitive movements.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在重复计数方面最突出的工作之一是 **RepNet**，这是一个端到端的深度学习模型，可以准确预测广泛的重复运动计数。
- en: 'The RepNet model takes in a video stream as input and predicts two outputs:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: RepNet 模型接收视频流作为输入，并预测两个输出：
- en: '***Per-frame period length:*** For each frame that is a part of repetitive
    action, we want to know the period length (in time units) of that action.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '***每帧周期长度：*** 对于每一个重复动作的帧，我们希望知道该动作的周期长度（以时间单位表示）。'
- en: '***Per-frame periodicity:*** a score indicating whether the current frame is
    a part of repetition or not.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '***每帧周期性：*** 表示当前帧是否为重复的一部分的分数。'
- en: 'Some of the key highlights of the RepNet model include: A **Temporal Self-similarity
    Matrix (TSM):**'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: RepNet 模型的几个关键亮点包括：**时间自相似矩阵（TSM）：**
- en: '![](../Images/7c65b7735474f0ed7e3f153b5d1e4c0d.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7c65b7735474f0ed7e3f153b5d1e4c0d.png)'
- en: 'Fig: RepNet leverages a Temporal Self-similarity Matrix (TSM). src: [https://arxiv.org/pdf/2006.15418.pdf](https://arxiv.org/pdf/2006.15418.pdf)'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 图：RepNet 利用时间自相似矩阵（TSM）。源：[https://arxiv.org/pdf/2006.15418.pdf](https://arxiv.org/pdf/2006.15418.pdf)
- en: '**TSM** is the highlight of this rep counting technique. It is the information
    bottleneck of the RepNet architecture. This matrix helps relate the frames to
    each other by computing a pairwise similarity function between two embeddings.'
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**TSM** 是这种重复计数技术的亮点。它是 RepNet 体系结构的信息瓶颈。该矩阵通过计算两个嵌入之间的成对相似性函数来帮助将帧彼此关联起来。'
- en: One can also infer (using heuristics) the number of repetitions from these TSMs,
    which makes predictions from the RepNet model interpretable.
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 还可以通过（使用启发式方法）从这些 TSM 推断重复次数，这使得 RepNet 模型的预测具有解释性。
- en: Diverse real-world repetition videos ensure these TSMs are quite diverse, and
    hence RepNet has a pool of applications besides just rep counting.
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 多样化的真实世界重复视频确保这些 TSM 足够多样化，因此 RepNet 除了重复计数之外还有一系列应用。
- en: One of the most impressive things about this rep counting method is that it
    is *class agnostic* (generic) and useful to a wide range of repetitive motions.
    RepNet model is a classical application of popular Transformers in Computer Vision.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 这种重复计数方法最令人印象深刻的一点是它是*无类别的*（通用的），适用于广泛的重复动作。RepNet 模型是流行的 Transformers 在计算机视觉中应用的经典实例。
- en: '![](../Images/346dd57079b5be82f05679d01f38f351.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/346dd57079b5be82f05679d01f38f351.png)'
- en: 'src: [https://arxiv.org/pdf/2006.15418.pdf](https://arxiv.org/pdf/2006.15418.pdf)'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 源：[https://arxiv.org/pdf/2006.15418.pdf](https://arxiv.org/pdf/2006.15418.pdf)
- en: However, the model is constrained in the sense that the number of frames in
    the input video has to be limited. This can be attributed to the fact that the
    size of the TSMs is equal to the number of input frames.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，该模型存在一定限制，因为输入视频中的帧数必须有限。这可以归因于TSM的大小等于输入帧的数量。
- en: The model is quite heavy and complex; hence deploying this on a mobile app or
    any production environment would be quite challenging and might have latency issues.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型相当庞大且复杂，因此在移动应用程序或任何生产环境中部署这一模型将是相当具有挑战性的，并可能存在延迟问题。
- en: 2\. Rule-based rep counting using Pose Estimation
  id: totrans-39
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 2\. 基于规则的姿态估计重复计数
- en: '*blog:* [Winning Interactivity Using Computer Vision](https://blog.cult.fit/posts/winning-interactivity-using-computer-vision)
    [2]'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '*博客：* [使用计算机视觉赢得互动](https://blog.cult.fit/posts/winning-interactivity-using-computer-vision)
    [2]'
- en: This is the most common idea used in industry. A number of health and fitness
    startups have been working on building accurate, lightweight, state-of-the-art
    pose estimation models which can be used to accurately count the reps during exercise
    and provide posture correction feedback, etc.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 这是行业中最常见的想法。许多健康和健身初创公司一直在努力构建准确、轻量化的最先进姿态估计模型，这些模型可以用来准确计算运动中的重复次数，并提供姿势纠正反馈等。
- en: 'Major Steps involved:'
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 涉及的主要步骤：
- en: Given a specific exercise, you first come up with definitions (rules) for states
    in that exercise. There can be multiple states in an exercise. A **squat** exercise,
    for example, can be broken into two states, say a lower state and an upper state.
    During the course of movement, the person doing exercise will shift from one state
    to the other. These state rules can be thought of as representing **activation**
    regions during movement.
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 给定一个特定的练习，你首先需要为该练习中的状态制定定义（规则）。一个练习中可以有多个状态。例如，**深蹲**练习可以分为两个状态，即下半部状态和上半部状态。在运动过程中，做练习的人将从一个状态转移到另一个状态。这些状态规则可以看作是运动过程中**激活**区域的表示。
- en: 'E.g., for a squat, these rules can be (*th* refers to threshold values):'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 例如，对于深蹲，这些规则可以是（*th*指阈值）：
- en: '[PRE0]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: During inference, we start by computing the metrics (angles, distances normalized)
    using pose-keypoints from the model in real-time and check whether a particular
    rule gets activated or not, and perform rep counting using the flag.
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在推理过程中，我们首先实时计算模型中姿态关键点的度量（角度、归一化的距离），并检查某个特定规则是否被激活，然后使用标志进行重复计数。
- en: '![](../Images/6ef09f548a6a452264ff2de80aa44c8c.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6ef09f548a6a452264ff2de80aa44c8c.png)'
- en: 'src: [Winning Interactivity Using Computer Vision](https://blog.cult.fit/posts/winning-interactivity-using-computer-vision).
    Image by the author.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 来源：[使用计算机视觉赢得互动](https://blog.cult.fit/posts/winning-interactivity-using-computer-vision)。图片由作者提供。
- en: 'One of the major upsides of the approach is that rep counting is **fast** and
    **accurate**, and **latency** is very low. However, some major downsides include
    the following:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 该方法的主要优点之一是重复计数**快速**且**准确**，**延迟**非常低。然而，一些主要的缺点包括：
- en: It is not a generic rep counting.
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这不是通用的重复计数。
- en: The pose estimation model is highly sensitive to background noise and hence
    rep counting as well.
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 姿态估计模型对背景噪音非常敏感，因此对重复计数也有影响。
- en: '***Scalability Issues:*** writing rules manually is a time-intensive process.
    We also need to test the rules with different variations in angle, orientation,
    posture, etc. Imagine writing rules for 100s of exercises in the corpus.'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***可扩展性问题：*** 手动编写规则是一个耗时的过程。我们还需要测试不同角度、方向、姿势等的规则变体。想象一下为语料库中的数百种练习编写规则。'
- en: 3\. Rep Counting using ideas from Signal Processing
  id: totrans-53
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 3\. 使用信号处理思想进行重复计数
- en: '*blog:* [Building an Exercise Rep Counter Using Ideas from Signal Processing](https://medium.com/towards-data-science/building-an-exercise-rep-counter-using-ideas-from-signal-processing-fcdf14e76f81)
    [3]'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '*博客：* [使用信号处理思想构建运动重复计数器](https://medium.com/towards-data-science/building-an-exercise-rep-counter-using-ideas-from-signal-processing-fcdf14e76f81)
    [3]'
- en: '**Goal:** *Use Signal Processing ideas like zero-crossing and peak detection
    to make an exercise rep counter.*'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '**目标：** *使用零交叉和峰值检测等信号处理思想来制作一个运动重复计数器。*'
- en: This approach is very similar to rule-based rep counting except for the hassle
    of manually writing the rules for different states during the rep. This approach
    ***semi-automates*** the state calculation approach by inferring a reference line
    (which can be thought of as a state boundary) for a specific movement/exercise
    using a trainer's video and then using the reference line for counting reps of
    any video of that exercise.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法与基于规则的重复计数非常相似，只是需要手动编写不同状态的规则。这种方法通过推断参考线（可以视为状态边界）来***半自动化***状态计算方法，使用训练师的视频来进行特定运动/练习，然后使用该参考线来计数任何视频中的该练习的重复次数。
- en: Here, we consider exercise as a set of waves of metrics of keypoints. These
    Metrics include angles and distances between a combination of different body keypoints,
    and the keypoints are computed using a pose estimation model (Tensorflow's [**Movenet**](https://www.tensorflow.org/hub/tutorials/movenet)
    pose estimation model).
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将运动视为关键点度量的波形集合。这些度量包括不同身体关键点之间的角度和距离，关键点是通过姿态估计模型（Tensorflow 的 [**Movenet**](https://www.tensorflow.org/hub/tutorials/movenet)
    姿态估计模型）计算的。
- en: 'Major Steps involved:'
  id: totrans-58
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 主要步骤：
- en: We first compute metrics (distances and angles) between a combination of keypoints
    using a trainer reference video (as input). These metrics represent a signal temporally.
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们首先使用训练师参考视频（作为输入）计算关键点之间的度量（距离和角度）。这些度量在时间上表示为信号。
- en: We filter out all the stationary signals and create a combined signal of the
    non-stationary ones. Then we compute the reference line using the mean of the
    summed-up signal.
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们过滤掉所有静态信号，并创建非静态信号的组合信号。然后，我们使用汇总信号的均值计算参考线。
- en: During inference, we start by again computing the metrics on the test user input
    video and compute an overall combined signal in real-time.
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在推断过程中，我们首先在测试用户输入视频上计算度量，并实时计算整体组合信号。
- en: We create a fixed-size moving window and check for the intersection of the overall
    signal (from 3) with the reference line (from 2). This intersection gives an indication
    that the rep is complete.
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们创建一个固定大小的移动窗口，并检查整体信号（来自 3）与参考线（来自 2）的交点。这个交点表明重复动作已经完成。
- en: '![](../Images/8f8451118dd7510832560f3606fe8be2.png)![](../Images/a6de37774f7ccc8281b538bebb04c207.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8f8451118dd7510832560f3606fe8be2.png)![](../Images/a6de37774f7ccc8281b538bebb04c207.png)'
- en: Results using the idea of zero-crossing. Image by the Author. [src](/building-an-exercise-rep-counter-using-ideas-from-signal-processing-fcdf14e76f81)
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 使用零交叉思想的结果。图片由作者提供。 [源]( /building-an-exercise-rep-counter-using-ideas-from-signal-processing-fcdf14e76f81)
- en: 'This approach is fast, easy to implement, and fairly accurate. However, some
    major downsides include the following:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法快速、易于实施且相当准确。然而，主要的缺点包括以下几点：
- en: Rep counting is ***exclusive*** and ***non-generic***.
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 重复计数是 ***独占性的*** 和 ***非通用的***。
- en: Highly sensitive to background noise.
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对背景噪音高度敏感。
- en: '***Scaling issues***: One needs to calculate the zero-crossing line using a
    reference video for any activity (also ensuring the video does not have any noise).'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***缩放问题***：需要使用参考视频计算零交叉线来进行任何活动（同时确保视频没有噪音）。'
- en: 4\. GymCam
  id: totrans-69
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 4\. GymCam
- en: '*paper:* [Detecting, recognizing, and tracking simultaneous exercises in unconstrained
    scenes](https://smashlab.io/publications/gymcam/)[4]'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '*论文：* [在不受约束的场景中检测、识别和跟踪同时进行的运动](https://smashlab.io/publications/gymcam/)[4]'
- en: GymCam is a vision-based system used for automated exercise rep counting and
    tracking. It is based on the assumption that any repetitive motion inside the
    gym is some sort of exercise. Again, here the input to the system is a video stream
    from the camera, and the output is several exercise-related metrics, including
    rep count.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: GymCam 是一个基于视觉的系统，用于自动化的运动重复计数和跟踪。它基于这样的假设：任何在健身房内的重复运动都属于某种运动。在这里，系统的输入是来自摄像头的视频流，输出是多个与运动相关的度量，包括重复计数。
- en: Summary of the Steps Involved
  id: totrans-72
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 步骤总结
- en: '![](../Images/fd2b6054cced0a51916651510e7459bc.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fd2b6054cced0a51916651510e7459bc.png)'
- en: 'GymCam: major steps involved. Image by the author.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: GymCam：主要步骤。图片由作者提供。
- en: '*Detect all potential motion trajectories in a video using a dense* [***optical
    flow***](https://docs.opencv.org/3.4/d4/dee/tutorial_optical_flow.html) *algorithm.*
    A motion trajectory might be a result of non-exercise activities, too, for example,
    warm-up, users’ gait, roaming here and there, etc.'
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*使用密集的* [***光流***](https://docs.opencv.org/3.4/d4/dee/tutorial_optical_flow.html)
    *算法检测视频中的所有潜在运动轨迹。* 运动轨迹也可能是非运动活动的结果，例如热身、用户的步态、到处走动等。'
- en: '*Detect all* ***exercise*** *motion trajectories in a scene.* How do they do
    so?Firstly, they perform a feature extraction step that involves extracting handcrafted
    features from a 5-sec window of any trajectory. They use an [MLP](https://en.wikipedia.org/wiki/Multilayer_perceptron)-based
    binary classifier model, which takes in the input feature and outputs a probability
    of whether that input trajectory (feature) is an exercise-related activity or
    not.'
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*检测所有* ***运动*** *轨迹在场景中。* 他们是怎么做到的？首先，他们执行一个特征提取步骤，从任何轨迹的5秒窗口中提取手工特征。他们使用一个[MLP](https://en.wikipedia.org/wiki/Multilayer_perceptron)
    基于的二分类器模型，该模型输入特征并输出该输入轨迹（特征）是否与运动相关的概率。'
- en: '***Clustering*** *exercise motion trajectories in space and time.* After clustering,
    an average motion trajectory is generated by combining all trajectories belonging
    to a given cluster. Note here that the number of clusters is pre-defined. These
    average trajectories are then used for exercise rep counting and tracking.'
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '***聚类*** *运动轨迹在空间和时间上的练习。* 聚类后，通过将所有属于给定聚类的轨迹合并生成一个平均运动轨迹。请注意，聚类的数量是预定义的。这些平均轨迹随后用于计数和跟踪练习的重复次数。'
- en: '***Rep Counting*** *and* ***Exercise Recognition:*** Average trajectories are
    then converted into feature vectors, which are then fed to an MLP Regressor and
    an MLP Classifier model to infer rep counts and exercise labels, respectively.'
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '***重复计数*** *和* ***运动识别：*** 平均轨迹然后被转换为特征向量，这些特征向量被输入到MLP回归模型和MLP分类模型中，以分别推断重复计数和运动标签。'
- en: '![](../Images/6e5d4c861207cef83b2048458baf8e30.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6e5d4c861207cef83b2048458baf8e30.png)'
- en: Rep Counting and exercise recognition from combined trajectories. Image by the
    author.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 从组合轨迹中进行重复计数和练习识别。图片由作者提供。
- en: 'Some of the noteworthy features of this system are: It is an ***end-to-end***
    system that performs rep counting in a real-world setting. ***Optical Flow***
    identifies all movements, and hence it would be sufficient to track the exercise
    and perform rep counting even if the user is **barely** visible.'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 该系统的一些显著特点是：它是一个***端到端*** 系统，可以在现实世界环境中执行重复计数。***光流*** 识别所有运动，因此即使用户**几乎**不可见，也足以跟踪运动并执行重复计数。
- en: 'Issues with this system:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 系统存在的问题：
- en: '***Multiple users overlap*** in a video while doing the exercise. And hence
    it becomes very difficult to figure out the exact boundaries of these users and
    infer the rep counts.'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***多个用户重叠*** 在视频中进行运动。因此，很难确定这些用户的确切边界并推断重复次数。'
- en: '***Noise Sensitive:*** noisy human behavior such as warming up, rest, user’s
    gait, etc., might exhibit periodicity and hence, can have an undesired contribution
    to the rep count.'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***噪声敏感：*** 噪声人类行为，如热身、休息、用户的步态等，可能会表现出周期性，因此可能对重复计数产生不必要的贡献。'
- en: 'Rep counting is not generic: the system is limited to just the exercise rep
    counting.'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 重复计数不是通用的：该系统仅限于运动重复计数。
- en: 5\. DL-based Optical Flow Approach
  id: totrans-86
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 5\. 基于DL的光流方法
- en: '*blog:* [Workout Movement Counting App using Deep Learning and Optical Flow
    Algorithm](/how-i-created-the-workout-movement-counting-app-using-deep-learning-and-optical-flow-89f9d2e087ac)
    [5]'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '*博客:* [使用深度学习和光流算法的运动计数应用](/how-i-created-the-workout-movement-counting-app-using-deep-learning-and-optical-flow-89f9d2e087ac)
    [5]'
- en: Another interesting idea employing vision to solve rep counting is the Optical
    flow approach.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个有趣的思路是利用视觉解决重复计数问题，即光流方法。
- en: Major Steps Involved
  id: totrans-89
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 主要步骤
- en: '*Find color-coded representations of video frames in a repetitive activity
    using a dense optical flow algorithm.* Here, the catch basically lies in the idea
    that different states of a repetitive movement will have different color codings.'
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*使用密集光流算法查找重复活动中视频帧的颜色编码表示。* 这里的关键在于不同的重复运动状态将有不同的颜色编码。'
- en: '![](../Images/a46957c2b74135d6fa3782c731f082ba.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a46957c2b74135d6fa3782c731f082ba.png)'
- en: Dense Optical flow encodes downward movement as the green color and upward movement
    as the purple color. Gif by the Author.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 密集光流将向下运动编码为绿色，将向上运动编码为紫色。Gif由作者提供。
- en: For details about the optical flow algorithm, please refer to the **opencv**
    doc [here](https://docs.opencv.org/3.4/d4/dee/tutorial_optical_flow.html) (along
    with the implementation).
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 关于光流算法的详细信息，请参见**opencv**文档 [这里](https://docs.opencv.org/3.4/d4/dee/tutorial_optical_flow.html)（以及实现）。
- en: '2\. *Dataset Creation*: Next step is to generate a dataset of color-coded images
    and videos and label them with different states of the movement (say up or down).'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 2\. *数据集创建*：下一步是生成一个带有颜色编码的图像和视频数据集，并将其标注为不同的运动状态（如向上或向下）。
- en: 3\. *Model Training:* Next step involves training a vanilla CNN model to perform
    a multiclass classification of the frames. At test time, color-coded frames from
    optical flow are then fed to the model, which predicts one of the movement states
    and also captures the class label. This is basically a color-matching problem
    but via a model, as the model is more robust.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 3\. *模型训练*：下一步涉及训练一个普通的 CNN 模型，以对帧进行多类分类。在测试时，将来自光流的颜色编码帧输入到模型中，模型预测运动状态之一并捕获类别标签。这基本上是一个通过模型进行的颜色匹配问题，因为模型更为健壮。
- en: 'The approach is accurate and easily deployable in production. However, the
    cons easily outweigh the pros of the approach:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 该方法准确且容易在生产环境中部署。然而，缺点往往超过优点：
- en: Rep counting is ***exclusive*** and ***class-dependent***.
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 重复计数是 ***独占性*** 和 ***类别依赖性*** 的。
- en: '***Scaling issues***: one needs to annotate the dataset and train a model each
    time a new exercise gets added to the corpus.'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***扩展问题***：每次将新运动添加到语料库时，都需要标注数据集并训练模型。'
- en: '***Orientation Sensitive:*** Same movements in different orientations will
    have different color encodings resulting in a wrong model prediction. This is
    one of the major limitations of the approach.'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***方向敏感性***：相同的动作在不同的方向上会有不同的颜色编码，从而导致错误的模型预测。这是该方法的主要限制之一。'
- en: '***Noise Sensitive***: Any slight noise in the background would change these
    color encodings and hence the model’s prediction.'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***噪声敏感性***：背景中的任何轻微噪声都会改变这些颜色编码，从而影响模型的预测。'
- en: References
  id: totrans-101
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1]. Dwibedi, Debidatta and Aytar, Yusuf and Tompson, Jonathan and Sermanet,
    Pierre and Zisserman, Andrew. Counting Out Time: Class Agnostic Video Repetition
    Counting in the Wild. IEEE/CVF Conference on Computer Vision and Pattern Recognition
    (CVPR). DOI: [https://doi.org/10.48550/arxiv.1902.09868](https://doi.org/10.48550/arxiv.1902.09868)'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '[1]. Dwibedi, Debidatta 和 Aytar, Yusuf 和 Tompson, Jonathan 和 Sermanet, Pierre
    和 Zisserman, Andrew. 计时：在实际环境中进行类无关的视频重复计数。IEEE/CVF计算机视觉与模式识别会议（CVPR）。DOI: [https://doi.org/10.48550/arxiv.1902.09868](https://doi.org/10.48550/arxiv.1902.09868)'
- en: '[2]. Aakash Agrawal. [Winning Interactivity Using Computer Vision](https://blog.cult.fit/posts/winning-interactivity-using-computer-vision).
    [The .fit way](https://blog.cult.fit/).'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '[2]. Aakash Agrawal. [利用计算机视觉赢得交互性](https://blog.cult.fit/posts/winning-interactivity-using-computer-vision)。
    [The .fit方式](https://blog.cult.fit/)。'
- en: '[3]. Aakash Agrawal. [Building an Exercise Rep Counter Using Ideas from Signal
    Processing](https://medium.com/towards-data-science/building-an-exercise-rep-counter-using-ideas-from-signal-processing-fcdf14e76f81).
    Towards Data Science.'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '[3]. Aakash Agrawal. [利用信号处理中的想法构建一个运动重复计数器](https://medium.com/towards-data-science/building-an-exercise-rep-counter-using-ideas-from-signal-processing-fcdf14e76f81)。Towards
    Data Science。'
- en: '[4]. Rushil Khurana, Karan Ahuja, Zac Yu, Jennifer Mankoff, Chris Harrison,
    and Mayank Goel. 2019\. GymCam: Detecting, Recognizing, and Tracking Simultaneous
    Exercises in Unconstrained Scenes. Proc. ACM Interact. Mob. Wearable Ubiquitous
    Technol. 2, 4, Article 185\. DOI: [https://doi.org/10.1145/3287063](https://doi.org/10.1145/3287063)'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '[4]. Rushil Khurana, Karan Ahuja, Zac Yu, Jennifer Mankoff, Chris Harrison,
    和 Mayank Goel. 2019\. GymCam：在不受约束的场景中检测、识别和跟踪同时进行的运动。Proc. ACM Interact. Mob.
    Wearable Ubiquitous Technol. 2, 4, Article 185\. DOI: [https://doi.org/10.1145/3287063](https://doi.org/10.1145/3287063)'
- en: '[5]. Art Kulakov. [How I created the Workout Movement Counting App using Deep
    Learning and Optical Flow Algorithm](/how-i-created-the-workout-movement-counting-app-using-deep-learning-and-optical-flow-89f9d2e087ac).
    Towards Data Science.'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '[5]. Art Kulakov. [我如何使用深度学习和光流算法创建健身动作计数应用程序](/how-i-created-the-workout-movement-counting-app-using-deep-learning-and-optical-flow-89f9d2e087ac)。Towards
    Data Science。'
- en: I hope you enjoyed exploring a few techniques on vision-based rep counting.
    Most of the ideas are easy to implement and deploy as well. I would like to know
    the feedback of anyone reading this blog. I would be happy to answer any doubts/questions
    on any of the concepts mentioned above. Feedback is greatly welcomed. You can
    reach me via [Linkedin](https://www.linkedin.com/in/akash2016123/).
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 我希望你喜欢探索一些基于视觉的重复计数技术。大多数想法都很容易实现和部署。我希望了解任何阅读这篇博客的人的反馈。我很乐意回答有关上述任何概念的疑问/问题。欢迎反馈。你可以通过
    [Linkedin](https://www.linkedin.com/in/akash2016123/) 联系我。
- en: Thanks for reading!
  id: totrans-108
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 感谢阅读！
