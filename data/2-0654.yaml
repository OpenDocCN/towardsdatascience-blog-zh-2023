- en: 'Data Pipelines with Polars: Step-by-Step Guide'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Polars 构建的数据管道：逐步指南
- en: 原文：[https://towardsdatascience.com/data-pipelines-with-polars-step-by-step-guide-f5474accacc4](https://towardsdatascience.com/data-pipelines-with-polars-step-by-step-guide-f5474accacc4)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/data-pipelines-with-polars-step-by-step-guide-f5474accacc4](https://towardsdatascience.com/data-pipelines-with-polars-step-by-step-guide-f5474accacc4)
- en: Build scalable and fast data pipelines with Polars
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 Polars 构建可扩展且快速的数据管道
- en: '[](https://medium.com/@antonsruberts?source=post_page-----f5474accacc4--------------------------------)[![Antons
    Tocilins-Ruberts](../Images/363a4f32aa793cca7a67dea68e76e3cf.png)](https://medium.com/@antonsruberts?source=post_page-----f5474accacc4--------------------------------)[](https://towardsdatascience.com/?source=post_page-----f5474accacc4--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----f5474accacc4--------------------------------)
    [Antons Tocilins-Ruberts](https://medium.com/@antonsruberts?source=post_page-----f5474accacc4--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@antonsruberts?source=post_page-----f5474accacc4--------------------------------)[![Antons
    Tocilins-Ruberts](../Images/363a4f32aa793cca7a67dea68e76e3cf.png)](https://medium.com/@antonsruberts?source=post_page-----f5474accacc4--------------------------------)[](https://towardsdatascience.com/?source=post_page-----f5474accacc4--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----f5474accacc4--------------------------------)
    [Antons Tocilins-Ruberts](https://medium.com/@antonsruberts?source=post_page-----f5474accacc4--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----f5474accacc4--------------------------------)
    ·14 min read·Jul 24, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----f5474accacc4--------------------------------)
    ·阅读时间 14分钟·2023年7月24日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/bfaae191cbbf33d6ba469a6bffdb83bf.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bfaae191cbbf33d6ba469a6bffdb83bf.png)'
- en: Photo by [Filippo Vicini](https://unsplash.com/@filippo_vicini?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由 [Filippo Vicini](https://unsplash.com/@filippo_vicini?utm_source=medium&utm_medium=referral)
    提供，来源于 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: Introduction
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: The aim of this post is to explain and show you how to build data pipelines
    with Polars. It puts together and uses all the knowledge you’ve got from the previous
    two parts of this series, so if you haven’t gone through them yet, I highly recommend
    you start there and come back here later.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 本文的目的是解释并展示如何使用 Polars 构建数据管道。它整合并使用了你从本系列前两部分获得的所有知识，因此如果你还没有阅读这些内容，我强烈建议你先去阅读，然后再回来这里。
- en: '[](/eda-with-polars-step-by-step-guide-for-pandas-users-part-1-b2ec500a1008?source=post_page-----f5474accacc4--------------------------------)
    [## EDA with Polars: Step-by-Step Guide for Pandas Users (Part 1)'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/eda-with-polars-step-by-step-guide-for-pandas-users-part-1-b2ec500a1008?source=post_page-----f5474accacc4--------------------------------)
    [## 使用 Polars 进行EDA：Pandas 用户的逐步指南（第1部分）'
- en: Level up your data analysis with Polars
  id: totrans-11
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提升你在 Polars 上的数据分析技能
- en: 'towardsdatascience.com](/eda-with-polars-step-by-step-guide-for-pandas-users-part-1-b2ec500a1008?source=post_page-----f5474accacc4--------------------------------)
    [](/eda-with-polars-step-by-step-guide-to-aggregate-and-analytic-functions-part-2-a22d986315aa?source=post_page-----f5474accacc4--------------------------------)
    [## EDA with Polars: Step-by-Step Guide to Aggregate and Analytic Functions (Part
    2)'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/eda-with-polars-step-by-step-guide-for-pandas-users-part-1-b2ec500a1008?source=post_page-----f5474accacc4--------------------------------)
    [](/eda-with-polars-step-by-step-guide-to-aggregate-and-analytic-functions-part-2-a22d986315aa?source=post_page-----f5474accacc4--------------------------------)
    [## 使用 Polars 进行EDA：汇总和分析函数的逐步指南（第2部分）
- en: Advanced aggregates and rolling averages at lightning speed with Polars
  id: totrans-13
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用 Polars 以闪电般的速度进行高级聚合和滚动平均
- en: towardsdatascience.com](/eda-with-polars-step-by-step-guide-to-aggregate-and-analytic-functions-part-2-a22d986315aa?source=post_page-----f5474accacc4--------------------------------)
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/eda-with-polars-step-by-step-guide-to-aggregate-and-analytic-functions-part-2-a22d986315aa?source=post_page-----f5474accacc4--------------------------------)
- en: Setup
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设置
- en: You can find all the code in this [repository](https://github.com/aruberts/tutorials/tree/main/polars),
    so don’t forget to clone/pull and star it. In particular, we’ll be exploring this
    [file](https://github.com/aruberts/tutorials/blob/main/polars/data_preparation_pipeline.py)
    which means that we’ll finally move away from notebooks into the real world!
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在这个 [repository](https://github.com/aruberts/tutorials/tree/main/polars)
    中找到所有的代码，所以不要忘记克隆/拉取并给它加星。特别是，我们将探索这个 [file](https://github.com/aruberts/tutorials/blob/main/polars/data_preparation_pipeline.py)，这意味着我们最终将从笔记本走向实际应用！
- en: 'Data used in this project can be downloaded from [Kaggle](https://www.kaggle.com/datasets/datasnaek/youtube-new?resource=download&sort=published)
    (CC0: Public Domain). It’s the same YouTube trending dataset that was used in
    the previous two parts. I assume that you already have Polars installed, so just
    make sure to update it to the latest version using `pip install -U polars` .'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '本项目中使用的数据可以从 [Kaggle](https://www.kaggle.com/datasets/datasnaek/youtube-new?resource=download&sort=published)
    下载（CC0: Public Domain）。这与前两部分中使用的 YouTube 趋势数据集相同。我假设你已经安装了 Polars，因此只需确保通过 `pip
    install -U polars` 更新到最新版本。'
- en: Data Pipelines
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据管道
- en: Put simply, a data pipeline is an automated sequence of steps that pulls the
    data from one or multiple locations, applies processing steps and saves the processed
    data elsewhere making it available for further use.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 简单来说，数据管道是一个自动化的步骤序列，它从一个或多个位置提取数据，应用处理步骤，并将处理后的数据保存到其他地方，使其可以用于进一步的使用。
- en: Pipelines in Polars
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Polars 中的管道
- en: Polars way of working with data lends itself quite nicely to building scalable
    data pipelines. First of all, the fact that we can chain the methods so easily
    allows for some fairly complicated pipelines to be written quite elegantly.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: Polars 处理数据的方式非常适合构建可扩展的数据管道。首先，我们可以如此轻松地链式调用方法，这使得一些相当复杂的管道可以优雅地编写。
- en: For example, let’s say we want to find out ***which trending videos had the
    most views in each month of 2018***. Below you can see a full pipeline to calculate
    this metric and to save it as a parquet file.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设我们想找出 ***2018 年每个月中哪个趋势视频的观看次数最多***。下面你可以看到一个完整的管道，用于计算这个指标并将其保存为 parquet
    文件。
- en: '[PRE0]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Quite neat, right? If you know SQL this is very easy to read and understand.
    But can we make it even better? Of course, with Polars `.pipe()` method. This
    method gives us a structured way to apply sequential functions to the Data Frame.
    For this to work, let’s refactor the code above into functions.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 相当不错，对吧？如果你了解 SQL，这很容易阅读和理解。但我们可以做得更好吗？当然可以，使用 Polars 的 `.pipe()` 方法。这种方法为我们提供了一种有结构的方式来将顺序函数应用于
    DataFrame。为了使其有效，让我们将上述代码重构成函数。
- en: '[PRE1]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Notice that these functions take a Polars DataFrame as input (together with
    some other arguments) and output already altered Polars Data Frame. Chaining these
    methods together is a piece of cake with `.pipe()` .
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 注意这些函数接受一个 Polars DataFrame 作为输入（以及一些其他参数），并输出已经修改的 Polars DataFrame。通过 `.pipe()`
    方法将这些方法链在一起是一件轻而易举的事。
- en: '[PRE2]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: First of all, the reformatted code is much easier to understand. Second, separation
    of concerns is in general a good programming principle to follow as it allows
    easier debugging and cleaner code. For this simple example, it might be an overkill
    to modularise the pipeline but you’ll see how useful it is in the larger example
    in the next section. Now, let’s make the whole thing faster using **lazy mode**.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，重格式化后的代码更容易理解。其次，关注点分离通常是一个很好的编程原则，因为它可以更容易地调试和保持代码整洁。对于这个简单的示例，将管道模块化可能有些过于复杂，但你会看到它在下一个部分的更大示例中的有用之处。现在，让我们使用
    **懒模式** 来加快整体速度。
- en: Lazy mode allows us to write the queries and pipelines, put them all together,
    and let the backend engine do its optimisation magic. For example, the code written
    above is definitely not optimal. I’ve put the column selection as the last step
    which means that the size of processed data is unnecessarily large. Lucky for
    us, Polars is smart enough to realise this, so it will optimise the code. Also,
    we only need to change two small things in the code to get the speed up which
    is incredible. First of all, we change `pl.read_csv` to `pl.scan_csv` to read
    the data in lazy mode. Then, at the end of the query, we put `.collect()` to tell
    Polars that we want to execute the optimised query
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 懒模式允许我们编写查询和管道，将它们全部组合在一起，然后让后台引擎进行优化。例如，上述代码显然不是最优的。我把列选择作为最后一步，这意味着处理的数据量不必要地大。幸运的是，Polars
    足够聪明，能够识别这一点，因此它会优化代码。此外，我们只需在代码中做两个小的更改即可获得速度提升，这点非常不可思议。首先，我们将 `pl.read_csv`
    更改为 `pl.scan_csv` 以在懒模式下读取数据。然后，在查询的末尾添加 `.collect()`，以告知 Polars 我们希望执行优化后的查询。
- en: '[PRE3]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: On my machine, I’ve got the ~ 3x speedup which is impressive given we’ve made
    two very simple edits. Now that you’ve got the concept of piping and lazy evaluations,
    let’s move on to a more complicated example.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在我的机器上，我得到了约 3 倍的速度提升，考虑到我们仅做了两个非常简单的编辑，这点非常令人印象深刻。现在你已经掌握了管道和延迟评估的概念，让我们进入一个更复杂的示例。
- en: Data Pipeline for Machine Learning Features
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习特征的数据管道
- en: 'Warning: There’s a lot of text and a lot of code! Sections should be followed
    sequentially since they build up the pipeline.'
  id: totrans-33
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 警告：有很多文本和代码！各部分应该按顺序跟随，因为它们构建了管道。
- en: Given the dataset that we have at hand (YouTube Trending Videos), let’s build
    features that can be used in predicting **how long a video will be in trending.**
    Even though this might sound simple, the pipeline to create them is going to be
    quite complex. The final format of dataset should have one row per video ID, features
    that are available as soon as the video gets into trending, and the actual number
    of days that the video stayed in trending (target).
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 根据我们手头的数据集（YouTube Trending Videos），我们来构建用于预测**视频在流行中持续时间的特征**。虽然这听起来很简单，但创建这些特征的过程将会相当复杂。数据集的最终格式应该是每个视频ID一行，视频进入流行时可用的特征，以及视频在流行中停留的实际天数（目标）。
- en: '![](../Images/6bf7ae837c8dac8debee9bcf95439939.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6bf7ae837c8dac8debee9bcf95439939.png)'
- en: Mock final dataset format. Created by author.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 模拟最终数据集格式。作者创建。
- en: 'The features that could be useful in our prediction task are:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的预测任务中可能有用的特征包括：
- en: Video characteristics (e.g. category)
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 视频特征（例如，类别）
- en: Number of views, like, comments, etc. at the moment of entry into Trending
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 进入流行时的观看次数、点赞数、评论数等
- en: Past performance of the channel in Trending (e.g. number of trending videos
    in the last 7 days)
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 频道在流行中的过去表现（例如，过去7天的流行视频数量）
- en: General Trending characteristics (e.g. average time in trending for all the
    videos in the last 30 days)
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一般流行特征（例如，过去30天所有视频的平均流行时间）
- en: Below you can see a diagram representation of the required pipeline to create
    this dataset (make sure to zoom in).
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 下面你可以看到创建该数据集所需的管道的图示表示（确保放大查看）。
- en: '![](../Images/afa2b618abfc31e1c1b02b0c5ba2c9f9.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/afa2b618abfc31e1c1b02b0c5ba2c9f9.png)'
- en: Data Pipeline Flow. Created by author.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 数据管道流程。作者创建。
- en: I know it’s a lot to take in, so let’s eat this elephant a bite at a time. Below
    you can find the descriptions and code for every pipeline step. In addition, this
    pipeline will be parametrised using a YAML configuration file, so you’ll find
    the configuration parameters for every steps as well. Here’s how you’d read a
    YAML file named `pipe_config.yaml` which you can find in the repo.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 我知道这信息量很大，所以我们一口一口地消化它。下面你可以找到每个管道步骤的描述和代码。此外，这个管道将使用YAML配置文件进行参数化，所以你也会找到每个步骤的配置参数。这是如何读取名为
    `pipe_config.yaml` 的YAML文件的示例，你可以在仓库中找到。
- en: '[PRE4]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'So, for every step of the pipeline you’ll find:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，对于管道的每一步，你会发现：
- en: Description of the step
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 步骤描述
- en: Relevant functions
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 相关功能
- en: Relevant configuration parameters
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 相关配置参数
- en: Code to run the pipeline up to this step
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 运行到此步骤的管道代码
- en: This way, we’ll slowly build up to the full pipeline and you’ll have an in-depth
    understanding of what’s going on and how to create something similar for your
    own data.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 这样，我们将逐步建立完整的管道，你将对发生的事情有深入了解，并学会如何为自己的数据创建类似的内容。
- en: Read Data
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 读取数据
- en: 'The aim of this step is self explanatory — to read in the dataset for further
    processing. We have two inputs — a csv file with the main data and a json with
    the data about category mappings. The parameters for this step are as follows:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 这一步的目标不言自明——读取数据集以进行进一步处理。我们有两个输入——一个包含主要数据的csv文件和一个包含类别映射数据的json文件。此步骤的参数如下：
- en: '[PRE5]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: No need to write a function to read the csv data (since it alreday exists in
    Polars), so the only function we write is for reading in the category mappings.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 不需要编写读取csv数据的函数（因为在Polars中已经存在），所以我们只编写了读取类别映射的函数。
- en: '[PRE6]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: With this function, the code to read in the required files is quite simple.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 使用此函数，读取所需文件的代码非常简单。
- en: '[PRE7]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Now, let’s move on to a very unexciting yet crucial step — data cleaning.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们进入一个非常不令人兴奋但至关重要的步骤——数据清理。
- en: Data Cleaning
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据清理
- en: This dataset is already quite clean but we need to do some additional pre-processing
    to the dates and the category columns.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 这个数据集已经相当干净，但我们需要对日期和类别列做一些额外的预处理。
- en: '`trending_date` and `publish_time` need to be formatted as `pl.datetime`'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`trending_date` 和 `publish_time` 需要格式化为 `pl.datetime`'
- en: '`category_id` needs to be mapped from ID to the actual category name'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`category_id` 需要从ID映射到实际的类别名称'
- en: Polars need to know in which format the dates will be provided, so it’s best
    to encode the data formats with corresponding date columns in the `pipe_config.yaml`
    file to make it clear and easy to change.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: Polars 需要知道日期将以何种格式提供，因此最好在 `pipe_config.yaml` 文件中对数据格式进行编码，并在相应的日期列中进行说明，以使其清晰且易于更改。
- en: '[PRE8]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Since we want to use Polars pipelines to modularise the code, we need to create
    two functions — `parse_dates` and `map_dict_columns` which will perform two required
    transformations. However, here’s the catch — splitting these operations into two
    steps makes the code slower because Polars can’t use parallelisation efficiently.
    You can test this yourself by timing the execution of these two Polars expressions.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们希望使用 Polars 管道来模块化代码，我们需要创建两个函数——`parse_dates` 和 `map_dict_columns`，它们将执行两个所需的转换。然而，有个问题——将这些操作分成两个步骤会使代码变得更慢，因为
    Polars 无法有效地使用并行化。你可以通过计时这两个 Polars 表达式的执行来自己测试一下。
- en: '[PRE9]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'For me, the first expression was ~2x slower which is very significant. So what
    do we do then? Well, here’s the secret:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 对我来说，第一个表达式慢了 ~2 倍，这非常显著。那么我们该怎么办呢？好吧，这里有个秘密：
- en: We should build up the expressions before passing them through the `.with_columns`
    method.
  id: totrans-70
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 我们应该在将表达式传递给 `.with_columns` 方法之前构建它们。
- en: Hence, the functions `parse_dates` and `map_dict_columns` should return lists
    of expressions instead of the transformed Data Frames. These expressions can be
    combined and applied in the final cleaning function that we’ll call `clean_data`
    .
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，函数 `parse_dates` 和 `map_dict_columns` 应该返回表达式列表，而不是转换后的数据帧。这些表达式可以在最终清理函数中组合并应用，我们将称之为
    `clean_data`。
- en: '[PRE10]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: As you can see, now we only have one `.with_columns` operation which makes the
    code more optimised. Note that all the arguments to the functions are provided
    as dictionaries. This is because YAML gets read in as a dictionary. Now, let’s
    add the cleaning step to the pipeline.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，我们现在只有一个 `.with_columns` 操作，这使得代码更优化。请注意，所有函数的参数都作为字典提供。这是因为 YAML 被读取为字典。现在，让我们将清理步骤添加到管道中。
- en: '[PRE11]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Clean, modular, fast — what not to like? Let’s move on to the next step.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 干净、模块化、快速——还有什么不喜欢的？让我们继续下一步。
- en: Basic Feature Engineering
  id: totrans-76
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 基础特征工程
- en: 'This step does some basic feature engineering on top of the clean data, namely:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 这一步在清理的数据上做一些基本的特征工程，即：
- en: Calculates ratio features — likes to dislikes, likes to views and comments to
    views
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算比率特征——点赞与踩的比率、点赞与观看的比率以及评论与观看的比率
- en: Calculates difference in days between publishing and trending
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算发布和趋势之间的天数差异
- en: Extracts weekdays from `trending_date` column
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从 `trending_date` 列中提取工作日
- en: Let’s parametrise the calculation of these features in the configuration file.
    We want to specify the name of a feature we want to create and the corresponding
    columns in the dataset that should be used for calculation.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在配置文件中对这些特征的计算进行参数化。我们要指定一个要创建的特征的名称以及数据集中用于计算的相应列。
- en: '[PRE12]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Logic for the functions is still the same — build up expressions and pass them
    to `.with_columns` method. Hence, the functions `ratio_features` , `diff_features`
    and `date_features` are all called within the main function named `basic_feature_engineering`
    .
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 函数的逻辑仍然是一样的——构建表达式并将其传递给 `.with_columns` 方法。因此，函数 `ratio_features`、`diff_features`
    和 `date_features` 都在名为 `basic_feature_engineering` 的主函数中调用。
- en: '[PRE13]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Similar to the previous step, we just pass the main function to the `pipe` and
    provide to it all the required configurations as arguments.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于前一步，我们只需将主函数传递给 `pipe` 并将所有所需的配置作为参数提供。
- en: '[PRE14]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Nice, we’re half way through the pipeline! Now, let’s transform the dataset
    into the right format and finally calculate our target — days in trending.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 很好，我们已经完成了一半的管道！现在，让我们将数据集转换为正确的格式，并最终计算我们的目标——趋势中的天数。
- en: Data Transformation
  id: totrans-88
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据转换
- en: Just a reminder that the original dataset has multiple entries per video since
    it details information for every day in trending. If a video stayed five days
    in trending, this video would appear five times in this dataset. Our goal is to
    end up with the dataset that has just one entry per video (refer to the image
    below).
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 提醒一下，原始数据集中每个视频有多个条目，因为它详细记录了每一天的趋势。如果一个视频在趋势中停留了五天，这个视频在数据集中会出现五次。我们的目标是得到一个每个视频只有一个条目的数据集（请参见下图）。
- en: '![](../Images/ce3629ab065fa04ef5204cfe10f3af84.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ce3629ab065fa04ef5204cfe10f3af84.png)'
- en: Data transformation step mockup. Created by author.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 数据转换步骤示例。作者创建。
- en: We can achieve this using a combination of `.groupby` and `.agg` methods. The
    only configuration parameter to specify here is for filtering the videos that
    took too long to get into trending since these videos are the outliers identified
    in [the previous part](/eda-with-polars-step-by-step-guide-to-aggregate-and-analytic-functions-part-2-a22d986315aa)
    of this series. After we get the table with `video_ids` and the corresponding
    target (days in trending) we also need to not forget to join the features from
    the original dataset since they won’t be carried over during the `groupby` operation.
    Hence, we’ll also need to specify which features to join and which columns should
    be the joining keys.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过 `.groupby` 和 `.agg` 方法的组合来实现这一点。这里唯一需要配置的参数是过滤那些由于视频花费太长时间才进入流行榜的视频，因为这些视频是[前一部分](/eda-with-polars-step-by-step-guide-to-aggregate-and-analytic-functions-part-2-a22d986315aa)中识别出的离群点。在得到包含
    `video_ids` 和相应目标（流行天数）的表后，我们还需要记得从原始数据集中联接特征，因为这些特征不会在 `groupby` 操作中传递。因此，我们还需要指定要联接的特征和作为联接键的列。
- en: '[PRE15]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: To perform the required step, we’ll design two functions — `join_original_features`
    and `create_target_df` .
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 为了执行所需的步骤，我们将设计两个函数——`join_original_features` 和 `create_target_df`。
- en: '[PRE16]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Notice that the `groupby` operation to create the target and the `join_original_features`
    function are both run in `create_target_df` function since they both use the original
    dataset as input. This means that even though we have an intermediate output (`target`
    variable), we can still run this function in a `pipe` method with no issues.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，在 `create_target_df` 函数中，`groupby` 操作以创建目标和 `join_original_features` 函数都是运行的，因为它们都使用原始数据集作为输入。这意味着即使我们有一个中间输出（`target`
    变量），我们仍然可以在 `pipe` 方法中运行此函数，而不会出现问题。
- en: '[PRE17]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: For the final step, let’s generate more advanced features using dynamic and
    rolling aggregates (covered in-depth in the [last post](/eda-with-polars-step-by-step-guide-to-aggregate-and-analytic-functions-part-2-a22d986315aa)).
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 对于最后一步，让我们使用动态和滚动聚合生成更高级的特征（在[上一篇文章](/eda-with-polars-step-by-step-guide-to-aggregate-and-analytic-functions-part-2-a22d986315aa)中详细介绍）。
- en: Advanced Aggregates
  id: totrans-99
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 高级聚合
- en: This step is responsible to generating time-based aggregates. The only configuration
    that we need to provide are the windows for the aggregates.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 这一步负责生成基于时间的聚合。我们需要提供的唯一配置是聚合的窗口期。
- en: '[PRE18]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '**Rolling Aggregates**'
  id: totrans-102
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**滚动聚合**'
- en: Let’s starting with rolling features. Below you can see an example with two
    lagged rolling features for a channel `abc` for a window of two days.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从滚动特征开始。下方是一个示例，展示了一个 `abc` 频道在两天窗口期内的两个滞后滚动特征。
- en: '![](../Images/e9a06003999e420e769cbdf361befe5a.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e9a06003999e420e769cbdf361befe5a.png)'
- en: Rolling features example. Image by author.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 滚动特征示例。图片由作者提供。
- en: 'Rolling features are extremely easy in Polars, all you need is `.groupby_rolling()`
    method and some aggregates within the `.agg()` namespace. The aggregates that
    can be useful are:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Polars 中，滚动特征非常简单，你只需要 `.groupby_rolling()` 方法和 `.agg()` 命名空间中的一些聚合。可能有用的聚合包括：
- en: Number of previous videos in trending
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 先前流行视频的数量
- en: The average number of days in trending for the previously trending videos
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 先前流行视频的平均流行天数
- en: The maximum number of days in trending for the previously trending videos
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 先前流行视频的最大流行天数
- en: With this in mind, let’s build a function named `build_channel_rolling` that
    can take the required period as input, this way we’ll be able to to easily create
    rolling features for any period that we want, and outputs these required aggregates.
    The `by` argument should be `channel_title` because we want to create aggregates
    by channel and the index column should be `first_day_in_trending` since this is
    our main date columns. These two columns will also be used to join these rolling
    aggregates to the original Data Frame.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于此，让我们构建一个名为 `build_channel_rolling` 的函数，该函数可以将所需的周期作为输入，这样我们就可以轻松创建任何我们想要的滚动特征，并输出这些所需的聚合。`by`
    参数应设置为 `channel_title`，因为我们希望按频道创建聚合，而索引列应为 `first_day_in_trending`，因为这是我们的主要日期列。这两列也将用于将这些滚动聚合联接到原始数据框中。
- en: '[PRE19]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: The `add_rolling_features` is a wrapper function that can be passed to our pipeline.
    It generates and joins the aggregates for the periods specified in the config.
    Now, let’s move on to the final feature generation step.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '`add_rolling_features` 是一个包装函数，可以传递到我们的管道中。它生成并联接配置中指定的周期的聚合。现在，让我们进入最终的特征生成步骤。'
- en: Period Aggregates
  id: totrans-113
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 周期聚合
- en: These aggregates are similar to the rolling ones, but they aim to measure general
    behaviour in the Trending tab.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 这些聚合类似于滚动聚合，但它们旨在衡量“流行”标签中的一般行为。
- en: '![](../Images/7c8d82746db65b1ec3b36821024b3ae7.png)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7c8d82746db65b1ec3b36821024b3ae7.png)'
- en: Period features example. Image by author.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 周期特征示例。图片由作者提供。
- en: 'If rolling aggregates aimed to capture the past behaviour of a channel, these
    aggregates will capture general trends. This might be useful since the algorithm
    that’s used to determine who gets into Trending and for how long changes constantly.
    Hence, the aggregates that we want to create are:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 如果滚动聚合旨在捕捉频道的过去行为，这些聚合将捕捉一般趋势。这可能是有用的，因为决定谁进入流行和停留多长时间的算法不断变化。因此，我们想要创建的聚合是：
- en: Number videos in trending last period
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最近一段时间内流行的视频数量
- en: Average days in trending in the last period
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最近一段时间内的平均流行天数
- en: Maximum days in trending in the last period
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最近一段时间内的最大流行天数
- en: The logic for functions is the same — we’ll create a function that builds these
    aggregates and a wrapper function that will build and join the aggregates for
    all the periods. Notice that we don’t specify `by` parameter because we want to
    calculate these features for all the videos per day. Also notice that we need
    to use `shift` on the aggregates since we want to use the features for the last
    period and not the current one.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 函数的逻辑是相同的 — 我们将创建一个函数来构建这些聚合，以及一个包装函数来构建并连接所有时期的聚合。请注意，我们不指定`by`参数，因为我们想要计算每天所有视频的这些特征。还要注意，我们需要在聚合上使用`shift`，因为我们希望使用最后一段时间的特征，而不是当前的。
- en: '[PRE20]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Finally, let’s put this all together into our pipeline!
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们将这些都整合到我们的管道中吧！
- en: '[PRE21]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: I hope you’re as excited as I am because we’re almost there! The final step
    — writing out data.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 我希望你和我一样激动，因为我们快到了！最后一步 — 写出数据。
- en: Write Data
  id: totrans-126
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 写数据
- en: Saving transformed data is a piece of cake since we can use `.save_parquet()`
    right after the `collect()` operation. Below you can see full code that the file
    `data_preparation_pipeline.py` contains.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 保存转换后的数据非常简单，因为我们可以在`collect()`操作之后直接使用`.save_parquet()`。下面你可以看到文件`data_preparation_pipeline.py`中包含的完整代码。
- en: '[PRE22]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: We can run this pipeline like any other Python file.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以像运行其他 Python 文件一样运行这个管道。
- en: '[PRE23]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '[PRE24]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: On my laptop these steps take less than a half a second which is impressive
    given how many operations we’ve chained together and how many features are generated.
    Most importantly, the pipeline looks clean, is very easy to debug, and can be
    extended/changed/cut in no time. Great job us!
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 在我的笔记本电脑上，这些步骤不到半秒钟完成，这很令人印象深刻，考虑到我们链在一起的操作数量和生成的特征数量。最重要的是，管道看起来很干净，非常容易调试，并且可以在很短时间内扩展/更改/裁剪。我们做得很棒！
- en: Conclusion
  id: totrans-133
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: 'If you’ve followed through the steps and got here — amazing job! Here’s a brief
    summary of what you should’ve learned in this post:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你按照这些步骤走到了这里 — 做得好！以下是你应该在这篇文章中学到的内容的简要总结：
- en: How to chain multiple operations together into a pipeline
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何将多个操作链在一起形成管道
- en: How to make this pipeline efficient
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何使这个管道高效
- en: How to structure your pipeline project and parametrise it using YAML file
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何结构化你的管道项目并使用 YAML 文件对其进行参数化
- en: Make sure to apply these learnings to your own data. I recommend starting small
    (2–3 steps) and then expanding the pipeline as your needs grow. Make sure to keep
    it modular, lazy, and group as many operations into `.with_columns()` as possible
    to ensure proper parallelisation.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 确保将这些学习应用到你自己的数据中。我建议从小处着手（2-3步），然后随着需求的增长扩展管道。确保保持模块化，惰性，并将尽可能多的操作组合到`.with_columns()`中，以确保适当的并行处理。
- en: Not a Medium Member yet?
  id: totrans-139
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还不是 Medium 会员？
- en: '[](https://medium.com/@antonsruberts/membership?source=post_page-----f5474accacc4--------------------------------)
    [## Join Medium with my referral link — Antons Tocilins-Ruberts'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@antonsruberts/membership?source=post_page-----f5474accacc4--------------------------------)
    [## 使用我的推荐链接加入 Medium — Antons Tocilins-Ruberts'
- en: Read every story from Antons Tocilins-Ruberts (and thousands of other writers
    on Medium). Your membership fee directly…
  id: totrans-141
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 阅读 Antons Tocilins-Ruberts 的每个故事（以及 Medium 上成千上万其他作家的故事）。你的会员费直接…
- en: medium.com](https://medium.com/@antonsruberts/membership?source=post_page-----f5474accacc4--------------------------------)
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: medium.com](https://medium.com/@antonsruberts/membership?source=post_page-----f5474accacc4--------------------------------)
