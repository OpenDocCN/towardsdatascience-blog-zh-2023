- en: Apple M2 Max GPU vs Nvidia V100, P100 and T4
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Apple M2 Max GPU 对比 Nvidia V100、P100 和 T4
- en: 原文：[https://towardsdatascience.com/apple-m2-max-gpu-vs-nvidia-v100-p100-and-t4-8b0d18d08894](https://towardsdatascience.com/apple-m2-max-gpu-vs-nvidia-v100-p100-and-t4-8b0d18d08894)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/apple-m2-max-gpu-vs-nvidia-v100-p100-and-t4-8b0d18d08894](https://towardsdatascience.com/apple-m2-max-gpu-vs-nvidia-v100-p100-and-t4-8b0d18d08894)
- en: Compare Apple Silicon M2 Max GPU performances to Nvidia V100, P100, and T4 for
    training MLP, CNN, and LSTM models with TensorFlow.
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 比较 Apple Silicon M2 Max GPU 的性能与 Nvidia V100、P100 和 T4，在 TensorFlow 中训练 MLP、CNN
    和 LSTM 模型。
- en: '[](https://fabrice-daniel.medium.com/?source=post_page-----8b0d18d08894--------------------------------)[![Fabrice
    Daniel](../Images/c13598211944934b5565fad97f152700.png)](https://fabrice-daniel.medium.com/?source=post_page-----8b0d18d08894--------------------------------)[](https://towardsdatascience.com/?source=post_page-----8b0d18d08894--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----8b0d18d08894--------------------------------)
    [Fabrice Daniel](https://fabrice-daniel.medium.com/?source=post_page-----8b0d18d08894--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://fabrice-daniel.medium.com/?source=post_page-----8b0d18d08894--------------------------------)[![Fabrice
    Daniel](../Images/c13598211944934b5565fad97f152700.png)](https://fabrice-daniel.medium.com/?source=post_page-----8b0d18d08894--------------------------------)[](https://towardsdatascience.com/?source=post_page-----8b0d18d08894--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----8b0d18d08894--------------------------------)
    [Fabrice Daniel](https://fabrice-daniel.medium.com/?source=post_page-----8b0d18d08894--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----8b0d18d08894--------------------------------)
    ·6 min read·Nov 2, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----8b0d18d08894--------------------------------)
    ·阅读时长 6 分钟·2023年11月2日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/9c4460ec6c7c0b9a501cae279c02a10f.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9c4460ec6c7c0b9a501cae279c02a10f.png)'
- en: Image by the author
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供
- en: Launched in November 2020, [**Apple M1**](https://en.wikipedia.org/wiki/Apple_M1)
    was a revolution in the world of computers dominated by Intel. These new M1 Macs
    showed impressive performance in many benchmarks as M1 was faster than most high-end
    desktop computers for only a fraction of their energy consumption.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 2020年11月发布的 [**Apple M1**](https://en.wikipedia.org/wiki/Apple_M1) 是在被 Intel
    主导的计算机世界中的一场革命。这些新的 M1 Mac 在许多基准测试中表现出色，因为 M1 在能耗的极小部分下，比大多数高端台式计算机更快。
- en: 'Here are my previous benchmarks for the M1:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是我之前对 M1 的基准测试：
- en: '[**Benchmark M1 vs Xeon® vs Core i5 vs K80 and T4**](https://medium.com/towards-data-science/benchmark-m1-vs-xeon-vs-core-i5-vs-k80-and-t4-e3802f27421c)'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '[**M1 与 Xeon®、Core i5、K80 和 T4 的基准测试**](https://medium.com/towards-data-science/benchmark-m1-vs-xeon-vs-core-i5-vs-k80-and-t4-e3802f27421c)'
- en: '[**M1 competes with 20 cores Xeon® on TensorFlow training**](https://medium.com/towards-data-science/benchmark-m1-part-2-vs-20-cores-xeon-vs-amd-epyc-16-and-32-cores-8e394d56003d)'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '[**M1 在 TensorFlow 训练中与 20 核 Xeon® 竞争**](https://medium.com/towards-data-science/benchmark-m1-part-2-vs-20-cores-xeon-vs-amd-epyc-16-and-32-cores-8e394d56003d)'
- en: In January 2023, Apple announced the new [**M2 Pro and M2 Max**](https://en.wikipedia.org/wiki/Apple_M2).
    Their specs let us expect good performance increases, especially regarding the
    GPU.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 2023年1月，苹果宣布了新的 [**M2 Pro 和 M2 Max**](https://en.wikipedia.org/wiki/Apple_M2)。它们的规格让我们期待性能的显著提升，特别是在
    GPU 方面。
- en: '![](../Images/e66a1ec49c63c10d2ba6765b0f8cefc6.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e66a1ec49c63c10d2ba6765b0f8cefc6.png)'
- en: Compared M2 Max with M1 specs
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 将 M2 Max 与 M1 规格进行比较
- en: '*This M2 Max has 30 GPU cores, so we estimated the 10.7 TFLOPS from the 13.6
    TFLOPS of the 38 GPU cores version.*'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '*这个 M2 Max 具有 30 个 GPU 核心，因此我们根据 38 核 GPU 版本的 13.6 TFlops 估算了 10.7 TFLOPS。*'
- en: '![](../Images/a1c7ee55fcac13bdff32d65895caf633.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a1c7ee55fcac13bdff32d65895caf633.png)'
- en: GPU Compared
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: GPU 比较
- en: By comparison, the M2 Max 38 Cores GPU reaches 13.6 TFlops. My test below will
    show that the TFlops alone cannot be used to estimate the actual performances
    of these GPUs.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，M2 Max 38核 GPU 达到 13.6 TFlops。我的测试将显示，仅凭 TFlops 无法估计这些 GPU 的实际性能。
- en: To get comparable results, I run every test with the default TensorFlow FP32
    floating point precision.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 为了获得可比较的结果，我在默认的 TensorFlow FP32 浮点精度下运行每个测试。
- en: 'You can verify this precision by running :'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过运行来验证这一精确度：
- en: '[PRE0]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '**Setup**'
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**设置**'
- en: In this article, I benchmark the [**M2 Max**](https://en.wikipedia.org/wiki/Apple_M2)
    GPU against Nvidia [**V100**](https://www.nvidia.com/en-us/data-center/v100/),
    [**P100**](https://www.nvidia.com/en-us/data-center/tesla-p100/), and [**T4**](https://www.nvidia.com/en-us/data-center/tesla-t4/)
    for MLP, CNN, and LSTM [**TensorFlow**](https://www.tensorflow.org) models.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我对[**M2 Max**](https://en.wikipedia.org/wiki/Apple_M2) GPU与Nvidia的[**V100**](https://www.nvidia.com/en-us/data-center/v100/)、[**P100**](https://www.nvidia.com/en-us/data-center/tesla-p100/)、[**T4**](https://www.nvidia.com/en-us/data-center/tesla-t4/)在MLP、CNN和LSTM[**TensorFlow**](https://www.tensorflow.org)模型上的表现进行了基准测试。
- en: '![](../Images/f3ecb7d91531c5b327914f809fbca04f.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f3ecb7d91531c5b327914f809fbca04f.png)'
- en: Benchmark setup
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 基准测试设置
- en: 'On M1 and M2 Max computers, the environment was created under [**miniforge**](https://github.com/conda-forge/miniforge).
    Only the following packages were installed:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在M1和M2 Max计算机上，环境是在[**miniforge**](https://github.com/conda-forge/miniforge)下创建的。仅安装了以下软件包：
- en: '[PRE1]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Unlike in my previous articles, TensorFlow is now directly working with Apple
    Silicon, no matter if you install it from pip, anaconda, or miniforge.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 与我以前的文章不同，TensorFlow现在可以直接与Apple Silicon兼容，无论是从pip、anaconda还是miniforge安装。
- en: To enable GPU usage, install the [**tensorflow-metal**](https://developer.apple.com/metal/tensorflow-plugin/)
    package distributed by Apple using [**TensorFlow PluggableDevices**](https://blog.tensorflow.org/2021/06/pluggabledevice-device-plugins-for-TensorFlow.html).
    Note that [**Metal**](https://developer.apple.com/metal/) acceleration is also
    available for [**PyTorch**](https://developer.apple.com/metal/pytorch/) and [**JAX**](https://developer.apple.com/metal/jax/).
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 要启用GPU使用，请安装由苹果公司分发的[**tensorflow-metal**](https://developer.apple.com/metal/tensorflow-plugin/)包，并使用[**TensorFlow
    PluggableDevices**](https://blog.tensorflow.org/2021/06/pluggabledevice-device-plugins-for-TensorFlow.html)。请注意，[**Metal**](https://developer.apple.com/metal/)加速也适用于[**PyTorch**](https://developer.apple.com/metal/pytorch/)和[**JAX**](https://developer.apple.com/metal/jax/)。
- en: Apple says
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 苹果表示
- en: With updates to Metal backend support, you can train a wider set of networks
    faster with new features like custom kernels and mixed-precision training.
  id: totrans-31
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 随着对Metal后端支持的更新，你可以利用新功能如自定义内核和混合精度训练，更快地训练更广泛的网络。
- en: Models
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型
- en: This benchmark consists of a Python program running a sequence of [**MLP**](https://en.wikipedia.org/wiki/Multilayer_perceptron),
    [**CNN**](https://en.wikipedia.org/wiki/Convolutional_neural_network), and [**LSTM**](https://en.wikipedia.org/wiki/Long_short-term_memory)
    models training on [**Fashion MNIST**](https://github.com/zalandoresearch/fashion-mnist)**¹**
    for batch sizes of 32, 128, 512, and 1024 samples.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 这个基准测试包含一个运行[**MLP**](https://en.wikipedia.org/wiki/Multilayer_perceptron)、[**CNN**](https://en.wikipedia.org/wiki/Convolutional_neural_network)和[**LSTM**](https://en.wikipedia.org/wiki/Long_short-term_memory)模型的Python程序，这些模型在[**Fashion
    MNIST**](https://github.com/zalandoresearch/fashion-mnist)**¹**数据集上进行训练，批量大小为32、128、512和1024样本。
- en: It also uses a validation set to be consistent with the way most model training
    is performed in real-life applications. Then, a test set is used to evaluate the
    model after the training to make sure everything works well. So, the training,
    validation, and test set sizes are respectively 50000, 10000, and 10000.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 它还使用验证集以保持与实际应用中大多数模型训练方法的一致性。然后，使用测试集在训练后评估模型，以确保一切正常。所以，训练集、验证集和测试集的大小分别为50000、10000和10000。
- en: I use the same test program as described in this [previous article](https://medium.com/towards-data-science/benchmark-m1-vs-xeon-vs-core-i5-vs-k80-and-t4-e3802f27421c).
    As a reminder, the three models are simple and summarized below.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 我使用与[上一篇文章](https://medium.com/towards-data-science/benchmark-m1-vs-xeon-vs-core-i5-vs-k80-and-t4-e3802f27421c)中描述的相同测试程序。提醒一下，这三个模型是简单的，下面总结了它们的特点。
- en: '**MLP**'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '**MLP**'
- en: '[PRE2]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '**CNN**'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '**CNN**'
- en: '[PRE3]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '**LSTM**'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '**LSTM**'
- en: '[PRE4]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: They are all using the following optimizer and loss function.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 它们都使用以下优化器和损失函数。
- en: '[PRE5]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Note that, unlike in the previous benchmark articles, the eager mode is now
    working for GPU.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，与以前的基准测试文章不同，现在的急切模式对GPU有效。
- en: Results
  id: totrans-45
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结果
- en: Let’s first compare M2 Max with M1.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们首先比较M2 Max与M1。
- en: '![](../Images/633645acf4ea7899e96db3182146af59.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/633645acf4ea7899e96db3182146af59.png)'
- en: M2 Max vs M1
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: M2 Max与M1的比较
- en: As expected, the M2 Max is always faster than the M1\. The following shows how
    many times.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 正如预期的那样，M2 Max始终比M1更快。以下显示了具体的倍数。
- en: '![](../Images/a72117e77c3d3ce486b11e1fcd9d6d3e.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a72117e77c3d3ce486b11e1fcd9d6d3e.png)'
- en: How many times M2 Max is faster than M1 for GPU Training
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: M2 Max在GPU训练中的速度是M1的多少倍
- en: Here are the average, minimum, and maximum increases in speed. We found that
    Convnet training with M2 Max can be up to 4.38 times faster than with M1 GPU.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是速度的平均、最小和最大增幅。我们发现使用 M2 Max 进行 Convnet 训练的速度可以比使用 M1 GPU 快多达 4.38 倍。
- en: '![](../Images/cf56c6b28fb1b238f344010927a7a520.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/cf56c6b28fb1b238f344010927a7a520.png)'
- en: How many times M2 Max is faster than M1 for GPU Training
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: M2 Max 在 GPU 训练中比 M1 快多少倍
- en: Now let’s compare M2 Max with the other Nvidia High-End GPUs found on Google
    Cloud, AWS, Google Colaboratory, or Kaggle platforms.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们比较一下 M2 Max 与 Google Cloud、AWS、Google Colaboratory 或 Kaggle 平台上其他 Nvidia
    高端 GPU 的表现。
- en: '![](../Images/de94672cc997005e5fe3ba7975a16a9f.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/de94672cc997005e5fe3ba7975a16a9f.png)'
- en: M2 Max vs Nvidia T4, V100 and P100
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: M2 Max 与 Nvidia T4、V100 和 P100 的比较
- en: While training performances look quite similar for batch sizes 32 and 128, M2
    Max is showing the best performances over all the GPUs for batch sizes 512 and
    1024.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然在 batch size 为 32 和 128 时训练性能相似，但 M2 Max 在 batch size 为 512 和 1024 时显示出所有
    GPU 中最好的性能。
- en: The P100 is the fastest of the other GPUs while when looking at the specifications
    V100 was expected. This can be due to the instance itself as the characteristics
    of the three different versions of each card cannot explain it. When checking
    the details, Kaggle P100 instance has 4 vCPU while Colab V100 proposes 2 vCPU
    only. Maybe this can impact global performances and should be checked further.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: P100 是其他 GPU 中速度最快的，而从规格来看 V100 是预期中的最优选择。这可能与实例本身有关，因为每张卡的三个不同版本的特性无法解释这一点。检查细节时，Kaggle
    P100 实例有 4 个 vCPU，而 Colab V100 只有 2 个 vCPU。也许这会影响整体性能，需要进一步检查。
- en: Here we compare how many times M2 Max is faster than Nvidia P100.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们比较 M2 Max 与 Nvidia P100 的速度差异。
- en: '![](../Images/efa2db05d34bf6c8f42c6a78008f7635.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/efa2db05d34bf6c8f42c6a78008f7635.png)'
- en: How many times M2 Max is faster than P100 for GPU Training
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: M2 Max 在 GPU 训练中比 P100 快多少倍
- en: Here are the average, minimum, and maximum increases in speed. We found that
    on average the M2 Max is identical to a P100 and that while it’s slower for a
    batch size of 32 and 128 it can be faster up to 1.77 times for MLP, 1.43 times
    for LSTM or 1.24 times for CNN with a batch size of 1024.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是速度的平均、最小和最大增幅。我们发现，平均而言，M2 Max 与 P100 相同，并且虽然在 batch size 为 32 和 128 时较慢，但在
    batch size 为 1024 时，MLP 的速度可以快达 1.77 倍，LSTM 快达 1.43 倍，CNN 快达 1.24 倍。
- en: '![](../Images/26a1ac1af363faec738f73f479eac797.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/26a1ac1af363faec738f73f479eac797.png)'
- en: How many times M2 Max is faster than P100 for GPU Training
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: M2 Max 在 GPU 训练中比 P100 快多少倍
- en: Benefit of GPU vs CPU on M2 Max
  id: totrans-66
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: M2 Max 上 GPU 与 CPU 的好处
- en: In the past, with the previous version of TensorFlow, it was often observed
    that MLP and LSTM were more efficiently trained on the CPU than GPU. Only CNN
    benefits from GPU. But now, with the recent version of TensorFlow things have
    changed for LSTM.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 过去，使用旧版本的 TensorFlow 时，常常观察到 MLP 和 LSTM 在 CPU 上的训练效率高于 GPU。只有 CNN 受益于 GPU。但现在，随着
    TensorFlow 的最新版本，LSTM 的情况发生了变化。
- en: So a last interesting test is to check when models training benefit from the
    GPU compared to the CPU.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 所以最后一个有趣的测试是检查模型训练在 GPU 与 CPU 的比较中是否有好处。
- en: '![](../Images/97b57ea5b1725f773b9fd58c2c84afa7.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/97b57ea5b1725f773b9fd58c2c84afa7.png)'
- en: How many times training on M2 Max GPU is faster than CPU
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: M2 Max GPU 的训练速度比 CPU 快多少倍
- en: MLP does not benefit from GPU acceleration. As expected CNN benefits for every
    case but from a batch size of 128 the difference is huge and goes up to about
    5 times faster.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: MLP 并未从 GPU 加速中受益。正如预期的那样，CNN 在每种情况下都受益，但从 batch size 为 128 开始，差异巨大，速度提高到约 5
    倍。
- en: But the really good surprise comes from LSTM which shows by far the biggest
    benefit from training on GPU which is up to 6.77 times faster for a batch size
    of 1024
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 但真正的惊喜来自 LSTM，其训练速度比 GPU 快多达 6.77 倍，尤其是在 batch size 为 1024 时。
- en: Conclusion
  id: totrans-73
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: From these tests, it appears that
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 从这些测试来看，
- en: M2 Max is by far faster than M1, so Mac users can benefit from such an upgrade
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: M2 Max 比 M1 快得多，因此 Mac 用户可以从这一升级中受益。
- en: Compared to T4, P100, and V100 M2 Max is always faster for a batch size of 512
    and 1024
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与 T4、P100 和 V100 相比，M2 Max 在 batch size 为 512 和 1024 时始终更快。
- en: Performance differences are not only a TFlops concern. M2 Max is theoretically
    15% faster than P100 but in the true test for a batch size of 1024 it shows performances
    higher by 24% for CNN, 43% for LSTM, and 77% for MLP
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 性能差异不仅仅是 TFlops 的问题。M2 Max 理论上比 P100 快 15%，但在真实测试中，对于 batch size 为 1024 的情况，CNN
    的性能提高了 24%，LSTM 提高了 43%，MLP 提高了 77%。
- en: Of course, these metrics can only be considered for similar neural network types
    and depths as used in this test.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，这些指标只能用于与本测试中使用的神经网络类型和深度相似的情况。
- en: A new test is needed with bigger models and datasets.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 需要对更大模型和数据集进行新的测试。
- en: Now TensorFlow is easy to install and run efficiently on Apple Silicon. The
    GPU can now be used for any model and increase a lot the training performances
    for any type of model. That’s especially true for LSTM.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 现在TensorFlow可以在Apple Silicon上轻松安装和高效运行。GPU现在可以用于任何模型，大大提高了任何类型模型的训练性能，特别是LSTM模型。
- en: We can conclude that M2 Max is a very good platform for machine learning engineers.
    It enables training models on GPU with very good performances, even better than
    a T4, P100, or V100 commonly found on cloud instances. It also includes 12 CPU
    cores, making it more flexible than these GPU instances, and all of this for only
    a fraction of their energy consumption.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以得出结论，M2 Max 是一个非常好的机器学习工程师平台。它能够以非常好的性能在GPU上训练模型，甚至比常见的T4、P100或V100云实例更好。它还包含12个CPU核心，使其比这些GPU实例更灵活，且能耗仅为它们的一小部分。
- en: '**Update (12/11/2023)**: The last versions of tensorflow-metal (> 0.8.0) has
    a bug making some models unable to converge. To avoid this issue, I’ve updated
    the installation instructions in the article by setting the TensorFlow and tensorflow-metal
    versions. Then, I replayed the test with this new package combination to ensure
    the benchmark was unchanged. I confirm that it does not change anything to the
    performance results.'
  id: totrans-82
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**更新 (2023年12月11日)**：最新版本的tensorflow-metal（> 0.8.0）存在一个错误，使得一些模型无法收敛。为避免此问题，我已通过设置TensorFlow和tensorflow-metal版本来更新了文章中的安装说明。随后，我使用这个新软件包组合重新进行了测试，以确保基准测试结果没有变化。我确认这不会改变性能结果。'
- en: Sources
  id: totrans-83
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考资料
- en: '**Images and Code** : All images and codes in this work are by the author unless
    explicitly stated otherwise.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '**图像和代码**：除非另有明确说明，否则本文中的所有图像和代码均由作者提供。'
- en: '**Datasets Licences** : [Fashion-MNIST](https://github.com/zalandoresearch/fashion-mnist)
    is licenced under the [MIT Licence](https://github.com/zalandoresearch/fashion-mnist/blob/master/LICENSE)'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '**数据集许可**：[Fashion-MNIST](https://github.com/zalandoresearch/fashion-mnist)
    按照 [MIT 许可证](https://github.com/zalandoresearch/fashion-mnist/blob/master/LICENSE)
    进行许可。'
- en: '[1] Han Xiao and Kashif Rasul and Roland Vollgraf, [Fashion-MNIST: a Novel
    Image Dataset for Benchmarking Machine Learning Algorithms](https://arxiv.org/abs/1708.07747)
    (2017)'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] Han Xiao 和 Kashif Rasul 和 Roland Vollgraf，[Fashion-MNIST：用于基准测试机器学习算法的新型图像数据集](https://arxiv.org/abs/1708.07747)（2017年）'
