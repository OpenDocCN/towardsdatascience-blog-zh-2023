- en: Scene Graph Generation and its Application in Robotics
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: åœºæ™¯å›¾ç”ŸæˆåŠå…¶åœ¨æœºå™¨äººå­¦ä¸­çš„åº”ç”¨
- en: åŸæ–‡ï¼š[https://towardsdatascience.com/scene-graph-generation-and-its-application-in-robotics-f9ba864aa572?source=collection_archive---------6-----------------------#2023-10-10](https://towardsdatascience.com/scene-graph-generation-and-its-application-in-robotics-f9ba864aa572?source=collection_archive---------6-----------------------#2023-10-10)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡ï¼š[https://towardsdatascience.com/scene-graph-generation-and-its-application-in-robotics-f9ba864aa572?source=collection_archive---------6-----------------------#2023-10-10](https://towardsdatascience.com/scene-graph-generation-and-its-application-in-robotics-f9ba864aa572?source=collection_archive---------6-----------------------#2023-10-10)
- en: Letâ€™s have a *small* talk about visualizing images with interactive graphical
    representation!
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬æ¥è¿›è¡Œä¸€æ¬¡å…³äºç”¨äº¤äº’å¼å›¾å½¢è¡¨ç¤ºæ³•å¯è§†åŒ–å›¾åƒçš„*ç®€çŸ­*è®¨è®ºå§ï¼
- en: '[](https://medium.com/@ritanshiagarwal?source=post_page-----f9ba864aa572--------------------------------)[![Ritanshi
    Agarwal](../Images/68c99a95cade2a64988dc1f84f008076.png)](https://medium.com/@ritanshiagarwal?source=post_page-----f9ba864aa572--------------------------------)[](https://towardsdatascience.com/?source=post_page-----f9ba864aa572--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----f9ba864aa572--------------------------------)
    [Ritanshi Agarwal](https://medium.com/@ritanshiagarwal?source=post_page-----f9ba864aa572--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@ritanshiagarwal?source=post_page-----f9ba864aa572--------------------------------)[![Ritanshi
    Agarwal](../Images/68c99a95cade2a64988dc1f84f008076.png)](https://medium.com/@ritanshiagarwal?source=post_page-----f9ba864aa572--------------------------------)[](https://towardsdatascience.com/?source=post_page-----f9ba864aa572--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----f9ba864aa572--------------------------------)
    [Ritanshi Agarwal](https://medium.com/@ritanshiagarwal?source=post_page-----f9ba864aa572--------------------------------)'
- en: Â·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: Â·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fb2a8c7a68d54&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fscene-graph-generation-and-its-application-in-robotics-f9ba864aa572&user=Ritanshi+Agarwal&userId=b2a8c7a68d54&source=post_page-b2a8c7a68d54----f9ba864aa572---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----f9ba864aa572--------------------------------)
    Â·12 min readÂ·Oct 10, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Ff9ba864aa572&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fscene-graph-generation-and-its-application-in-robotics-f9ba864aa572&user=Ritanshi+Agarwal&userId=b2a8c7a68d54&source=-----f9ba864aa572---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[å…³æ³¨](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fb2a8c7a68d54&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fscene-graph-generation-and-its-application-in-robotics-f9ba864aa572&user=Ritanshi+Agarwal&userId=b2a8c7a68d54&source=post_page-b2a8c7a68d54----f9ba864aa572---------------------post_header-----------)
    å‘è¡¨åœ¨ [Towards Data Science](https://towardsdatascience.com/?source=post_page-----f9ba864aa572--------------------------------)
    Â· 12åˆ†é’Ÿé˜…è¯» Â· 2023å¹´10æœˆ10æ—¥'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ff9ba864aa572&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fscene-graph-generation-and-its-application-in-robotics-f9ba864aa572&source=-----f9ba864aa572---------------------bookmark_footer-----------)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ff9ba864aa572&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fscene-graph-generation-and-its-application-in-robotics-f9ba864aa572&source=-----f9ba864aa572---------------------bookmark_footer-----------)'
- en: 'Scene graph generation is the process of generating scene graphs and a scene
    graph contains the visual understanding of an image in the form of a graph. It
    has nodes and edges representing the objects and their relationships, respectively.
    Contextual information about the scenes can help in semantic scene understanding.
    Although there are certain challenges such as uncertainty of real-world scenarios
    or unavailability of a standard dataset, researchers are trying to apply the scene
    graphs in the field of robotics. This write-up contains two major parts: one is
    about how scene graphs can be obtained by having an in-depth discussion about
    two state-of-the-art approaches for scene graph generation based on region-based
    convolutional neural networks, and the other part deals with the application of
    scene graphs in robot planning. In its former part, this article further explains
    that only having a numerical metric for model evaluation is insufficient for proper
    performance analysis. The latter part includes a thorough explanation of a method
    that uses the concept of scene graphs for visual context-aware robot planning.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: åœºæ™¯å›¾ç”Ÿæˆæ˜¯ç”Ÿæˆåœºæ™¯å›¾çš„è¿‡ç¨‹ï¼Œåœºæ™¯å›¾ä»¥å›¾å½¢çš„å½¢å¼åŒ…å«å›¾åƒçš„è§†è§‰ç†è§£ã€‚å®ƒå…·æœ‰è¡¨ç¤ºå¯¹è±¡å’Œå®ƒä»¬å…³ç³»çš„èŠ‚ç‚¹å’Œè¾¹ã€‚å…³äºåœºæ™¯çš„ä¸Šä¸‹æ–‡ä¿¡æ¯å¯ä»¥å¸®åŠ©è¯­ä¹‰åœºæ™¯ç†è§£ã€‚å°½ç®¡å­˜åœ¨å¦‚ç°å®ä¸–ç•Œåœºæ™¯çš„ä¸ç¡®å®šæ€§æˆ–ç¼ºä¹æ ‡å‡†æ•°æ®é›†ç­‰æŒ‘æˆ˜ï¼Œç ”ç©¶äººå‘˜æ­£å°è¯•å°†åœºæ™¯å›¾åº”ç”¨äºæœºå™¨äººé¢†åŸŸã€‚æœ¬æ–‡åŒ…å«ä¸¤ä¸ªä¸»è¦éƒ¨åˆ†ï¼šä¸€éƒ¨åˆ†è®¨è®ºäº†å¦‚ä½•é€šè¿‡å¯¹åŸºäºåŒºåŸŸçš„å·ç§¯ç¥ç»ç½‘ç»œçš„ä¸¤ç§æœ€å…ˆè¿›æ–¹æ³•çš„æ·±å…¥è®¨è®ºæ¥è·å–åœºæ™¯å›¾ï¼Œå¦ä¸€éƒ¨åˆ†æ¶‰åŠåœºæ™¯å›¾åœ¨æœºå™¨äººè§„åˆ’ä¸­çš„åº”ç”¨ã€‚åœ¨å‰ä¸€éƒ¨åˆ†ä¸­ï¼Œæ–‡ç« è¿›ä¸€æ­¥è§£é‡Šäº†ä»…æœ‰æ•°å€¼åº¦é‡æ¥è¯„ä¼°æ¨¡å‹æ˜¯ä¸å¤Ÿçš„ï¼Œæ— æ³•è¿›è¡Œé€‚å½“çš„æ€§èƒ½åˆ†æã€‚åä¸€éƒ¨åˆ†åŒ…æ‹¬å¯¹ä¸€ç§åˆ©ç”¨åœºæ™¯å›¾æ¦‚å¿µè¿›è¡Œè§†è§‰ä¸Šä¸‹æ–‡æ„ŸçŸ¥æœºå™¨äººè§„åˆ’çš„æ–¹æ³•çš„è¯¦ç»†è§£é‡Šã€‚
- en: '![](../Images/bd1ee322371527cfd647250bb3bdcf12.png)'
  id: totrans-9
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bd1ee322371527cfd647250bb3bdcf12.png)'
- en: 'Figure 1a: Nodes and Edges depiction to create scene graphs (Background Image
    by [TruckRun](https://unsplash.com/@truckrun_ebike_systems?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)
    on [Unsplash](https://unsplash.com/photos/a-couple-of-people-riding-bikes-through-a-forest-g1YAFw6_lHo?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash))'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾1aï¼šåˆ›å»ºåœºæ™¯å›¾çš„èŠ‚ç‚¹å’Œè¾¹çš„æç»˜ï¼ˆèƒŒæ™¯å›¾ç‰‡æ¥è‡ª[TruckRun](https://unsplash.com/@truckrun_ebike_systems?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)äº[Unsplash](https://unsplash.com/photos/a-couple-of-people-riding-bikes-through-a-forest-g1YAFw6_lHo?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)ï¼‰
- en: 1\. Introduction
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 1\. ä»‹ç»
- en: When considering the description of image elements in words, well-known active
    techniques such as image segmentation, object classification, activity identification,
    etc. play a significant role in the process. Scene Graph Generation (SGG) is a
    method to generate scene graphs (SGs), by depicting objects (e.g., humans, animals,
    etc.) and their attributes (e.g., color, clothes, vehicles, etc.) as nodes, and
    the relationships among objects as the edges. Fig.1a and 1b shows how a scene
    graph generated from an RGB image usually looks like. It can be seen that the
    distinct colored nodes depict different major objects in the image (a male, a
    female, and a vehicle), the information about these objects is shown by the nodes
    having the variant of the same node color, and the edges are labeled with verbs
    showing a connection, or we can say relation among the nodes. The research has
    been going on for past years to make the SGs possible even for small details in
    the image.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è€ƒè™‘ç”¨æ–‡å­—æè¿°å›¾åƒå…ƒç´ æ—¶ï¼Œå›¾åƒåˆ†å‰²ã€å¯¹è±¡åˆ†ç±»ã€æ´»åŠ¨è¯†åˆ«ç­‰çŸ¥åçš„æ´»è·ƒæŠ€æœ¯åœ¨è¿‡ç¨‹ä¸­å‘æŒ¥äº†é‡è¦ä½œç”¨ã€‚åœºæ™¯å›¾ç”Ÿæˆï¼ˆSGGï¼‰æ˜¯ä¸€ç§ç”Ÿæˆåœºæ™¯å›¾ï¼ˆSGsï¼‰çš„æ–¹æ³•ï¼Œé€šè¿‡å°†å¯¹è±¡ï¼ˆå¦‚äººç±»ã€åŠ¨ç‰©ç­‰ï¼‰åŠå…¶å±æ€§ï¼ˆå¦‚é¢œè‰²ã€è¡£ç‰©ã€è½¦è¾†ç­‰ï¼‰æç»˜ä¸ºèŠ‚ç‚¹ï¼Œå¹¶å°†å¯¹è±¡ä¹‹é—´çš„å…³ç³»æç»˜ä¸ºè¾¹ã€‚å›¾1aå’Œ1bå±•ç¤ºäº†ä»RGBå›¾åƒç”Ÿæˆçš„åœºæ™¯å›¾é€šå¸¸æ˜¯ä»€ä¹ˆæ ·çš„ã€‚å¯ä»¥çœ‹åˆ°ï¼Œä¸åŒé¢œè‰²çš„èŠ‚ç‚¹æç»˜äº†å›¾åƒä¸­çš„ä¸»è¦å¯¹è±¡ï¼ˆä¸€ä¸ªç”·æ€§ï¼Œä¸€ä¸ªå¥³æ€§å’Œä¸€è¾†è½¦è¾†ï¼‰ï¼Œå…³äºè¿™äº›å¯¹è±¡çš„ä¿¡æ¯ç”±å…·æœ‰ç›¸åŒèŠ‚ç‚¹é¢œè‰²å˜ä½“çš„èŠ‚ç‚¹æ˜¾ç¤ºï¼Œè¾¹ç¼˜åˆ™ç”¨åŠ¨è¯æ ‡æ³¨ï¼Œæ˜¾ç¤ºè¿æ¥ï¼Œæˆ–è€…è¯´èŠ‚ç‚¹ä¹‹é—´çš„å…³ç³»ã€‚ç ”ç©¶äººå‘˜åœ¨è¿‡å»å‡ å¹´ä¸­ä¸€ç›´è‡´åŠ›äºä½¿SGså³ä½¿å¯¹å›¾åƒä¸­çš„å°ç»†èŠ‚ä¹Ÿèƒ½å®ç°ã€‚
- en: '![](../Images/4c2ab148202efa281bd3da401c6fb11e.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4c2ab148202efa281bd3da401c6fb11e.png)'
- en: 'Figure 1b: Process and Major Challenges in Scene Graph Generation'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾1bï¼šåœºæ™¯å›¾ç”Ÿæˆä¸­çš„è¿‡ç¨‹å’Œä¸»è¦æŒ‘æˆ˜
- en: This article extensively discusses two approaches in the upcoming sections for
    a better understanding of generating scene graphs. The write-up structure comprises
    six sections. The brief start of SGG and its prior applications including image
    and video captioning is explained in section 2\. Section 3 describes the state-of-the-art
    methods with their model description. The experimental analysis of these methods
    is discussed and compared in section 4\. Section 5 discusses some possible applications
    of SGs in the field of robotics with a main focus on robot planning, and also
    how having accurate SGs will make applications like telepresence robots, human-robot
    collaboration, etc. much easier to implement. Finally, section 6 concludes the
    entire literature.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬æ–‡å°†åœ¨æ¥ä¸‹æ¥çš„ç« èŠ‚ä¸­å¹¿æ³›è®¨è®ºç”Ÿæˆåœºæ™¯å›¾çš„ä¸¤ç§æ–¹æ³•ï¼Œä»¥ä¾¿æ›´å¥½åœ°ç†è§£ã€‚å†™ä½œç»“æ„åŒ…æ‹¬å…­ä¸ªéƒ¨åˆ†ã€‚ç¬¬2èŠ‚ç®€è¦ä»‹ç»äº†SGGåŠå…¶å…ˆå‰çš„åº”ç”¨ï¼ŒåŒ…æ‹¬å›¾åƒå’Œè§†é¢‘å­—å¹•ã€‚ç¬¬3èŠ‚æè¿°äº†æœ€å…ˆè¿›çš„æ–¹æ³•åŠå…¶æ¨¡å‹æè¿°ã€‚ç¬¬4èŠ‚è®¨è®ºå¹¶æ¯”è¾ƒäº†è¿™äº›æ–¹æ³•çš„å®éªŒåˆ†æã€‚ç¬¬5èŠ‚è®¨è®ºäº†SGsåœ¨æœºå™¨äººé¢†åŸŸçš„ä¸€äº›å¯èƒ½åº”ç”¨ï¼Œä¸»è¦é›†ä¸­åœ¨æœºå™¨äººè§„åˆ’æ–¹é¢ï¼Œè¿˜è®¨è®ºäº†å‡†ç¡®çš„SGså¦‚ä½•ä½¿è¿œç¨‹æœºå™¨äººã€äººæœºåä½œç­‰åº”ç”¨æ›´æ˜“äºå®ç°ã€‚æœ€åï¼Œç¬¬6èŠ‚æ€»ç»“äº†æ•´ä¸ªæ–‡çŒ®ã€‚
- en: 2\. Related Work
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 2\. ç›¸å…³å·¥ä½œ
- en: The SGs have a history of distinct applications, for instance, image and video
    captioning, visual question answering, 3D scene understanding, many applications
    in robotics, etc. [10] [3]. Most of the recent approaches include machine learning
    methods like Region-based Convolutional Neural Networks (R-CNN) [8], faster R-CNN
    [9], Recurrent Neural Networks (RNN) [7], etc. [7] proposed a structure that takes
    the image as input, then passes messages containing contextual information, and
    refines the prediction using RNN. Another method [6] uses the energy model of
    the image to generate comprehensive scene graphs. The datasets for SGG are also
    obtained using different real-world scenarios, images, and videos. The dataset
    mentioned here is a subset [7] of Visual Genome [4] containing 108,073 human-annotated
    images.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: SGsæœ‰ç€ä¸åŒçš„åº”ç”¨å†å²ï¼Œä¾‹å¦‚å›¾åƒå’Œè§†é¢‘å­—å¹•ã€è§†è§‰é—®ç­”ã€3Dåœºæ™¯ç†è§£ã€æœºå™¨äººé¢†åŸŸçš„è®¸å¤šåº”ç”¨ç­‰ã€‚[10] [3]ã€‚æœ€è¿‘çš„å¤§å¤šæ•°æ–¹æ³•åŒ…æ‹¬æœºå™¨å­¦ä¹ æ–¹æ³•ï¼Œå¦‚åŸºäºåŒºåŸŸçš„å·ç§¯ç¥ç»ç½‘ç»œï¼ˆR-CNNï¼‰[8]ã€æ›´å¿«çš„R-CNN
    [9]ã€é€’å½’ç¥ç»ç½‘ç»œï¼ˆRNNï¼‰[7]ç­‰ã€‚[7]æå‡ºäº†ä¸€ç§ç»“æ„ï¼Œå°†å›¾åƒä½œä¸ºè¾“å…¥ï¼Œç„¶åä¼ é€’åŒ…å«ä¸Šä¸‹æ–‡ä¿¡æ¯çš„æ¶ˆæ¯ï¼Œå¹¶ä½¿ç”¨RNNè¿›è¡Œé¢„æµ‹çš„ç»†åŒ–ã€‚å¦ä¸€ç§æ–¹æ³•[6]ä½¿ç”¨å›¾åƒçš„èƒ½é‡æ¨¡å‹ç”Ÿæˆç»¼åˆçš„åœºæ™¯å›¾ã€‚SGGçš„æ•°æ®é›†ä¹Ÿæ˜¯é€šè¿‡ä¸åŒçš„çœŸå®ä¸–ç•Œåœºæ™¯ã€å›¾åƒå’Œè§†é¢‘è·å¾—çš„ã€‚è¿™é‡Œæåˆ°çš„æ•°æ®é›†æ˜¯Visual
    Genome [4]çš„ä¸€ä¸ªå­é›†[7]ï¼ŒåŒ…å«108,073å¼ äººç±»æ³¨é‡Šçš„å›¾åƒã€‚
- en: 3\. State-of-the-Art Approaches
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 3\. æœ€å…ˆè¿›çš„æ–¹æ³•
- en: 3.1 Graph R-CNN Approach
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.1 å›¾å½¢ R-CNN æ–¹æ³•
- en: J. Yang et. al. [8] proposes a model that first considers all the objects connected
    by an edge, relationship, then nullifies the unlikely relationship using a parameter
    called *relatedness*. For instance, it is more likely to have a relation between
    *car* and *wheel*, than a *building* and a *wheel*.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: J. Yangç­‰[8]æå‡ºäº†ä¸€ç§æ¨¡å‹ï¼Œé¦–å…ˆè€ƒè™‘æ‰€æœ‰ç”±è¾¹è¿æ¥çš„å¯¹è±¡å’Œå…³ç³»ï¼Œç„¶åä½¿ç”¨ç§°ä¸º*ç›¸å…³æ€§*çš„å‚æ•°æ¥æ’é™¤ä¸å¤ªå¯èƒ½çš„å…³ç³»ã€‚ä¾‹å¦‚ï¼Œ*è½¦*å’Œ*è½®å­*ä¹‹é—´çš„å…³ç³»æ¯”*å»ºç­‘ç‰©*å’Œ*è½®å­*ä¹‹é—´çš„å…³ç³»æ›´æœ‰å¯èƒ½å­˜åœ¨ã€‚
- en: The paper shows a novel framework, namely Graph R-CNN with the introduction
    of a new evaluation metric, SGGen+. The model consists of three blocks, first
    to extract object nodes, second to remove unlikely edges, and finally to propagate
    graph context throughout the remaining graph so that the final scene graph can
    be produced. The novelty resides in the second and third block of the model, having
    a new relation proposal network (RePN) which computes *relatedness* that further
    helps in removing unlikely relationships, also called relation edge pruning and
    an attentional graph convolutional network (aGCN) that constitutes the propagation
    of higher-order context that leads to an update in scene graph giving it a final
    touch, respectively. Basically, the final block labeled the generated scene graph.
    The probability distribution of the whole idea can be depicted by the equation
    below, where for image *I*, *O* is the set of objects, *R* is the relation among
    objects, and *Rl* and *Ol* are relation and object labels respectively.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: è®ºæ–‡å±•ç¤ºäº†ä¸€ä¸ªæ–°é¢–çš„æ¡†æ¶ï¼Œå³Graph R-CNNï¼Œå¼•å…¥äº†ä¸€ç§æ–°çš„è¯„ä¼°æŒ‡æ ‡SGGen+ã€‚è¯¥æ¨¡å‹ç”±ä¸‰ä¸ªæ¨¡å—ç»„æˆï¼šé¦–å…ˆæå–å¯¹è±¡èŠ‚ç‚¹ï¼Œå…¶æ¬¡ç§»é™¤ä¸å¤ªå¯èƒ½çš„è¾¹ï¼Œæœ€ååœ¨å‰©ä½™å›¾ä¸­ä¼ æ’­å›¾ä¸Šä¸‹æ–‡ï¼Œä»¥ä¾¿ç”Ÿæˆæœ€ç»ˆçš„åœºæ™¯å›¾ã€‚åˆ›æ–°ç‚¹åœ¨äºæ¨¡å‹çš„ç¬¬äºŒä¸ªå’Œç¬¬ä¸‰ä¸ªæ¨¡å—ï¼Œå…·æœ‰ä¸€ä¸ªæ–°çš„å…³ç³»æè®®ç½‘ç»œï¼ˆRePNï¼‰ï¼Œè¯¥ç½‘ç»œè®¡ç®—*ç›¸å…³æ€§*ï¼Œè¿›ä¸€æ­¥å¸®åŠ©ç§»é™¤ä¸å¤ªå¯èƒ½çš„å…³ç³»ï¼Œä¹Ÿç§°ä¸ºå…³ç³»è¾¹ä¿®å‰ªï¼Œä»¥åŠä¸€ä¸ªæ³¨æ„åŠ›å›¾å·ç§¯ç½‘ç»œï¼ˆaGCNï¼‰ï¼Œå®ƒæ„æˆäº†é«˜é˜¶ä¸Šä¸‹æ–‡çš„ä¼ æ’­ï¼Œä»è€Œæ›´æ–°åœºæ™¯å›¾ï¼Œèµ‹äºˆå…¶æœ€ç»ˆä¿®é¥°ã€‚åŸºæœ¬ä¸Šï¼Œæœ€ç»ˆçš„æ¨¡å—æ ‡è®°ç”Ÿæˆçš„åœºæ™¯å›¾ã€‚æ•´ä¸ªæ¦‚å¿µçš„æ¦‚ç‡åˆ†å¸ƒå¯ä»¥é€šè¿‡ä¸‹é¢çš„æ–¹ç¨‹è¡¨ç¤ºï¼Œå…¶ä¸­å¯¹å›¾åƒ
    *I*ï¼Œ*O* æ˜¯å¯¹è±¡çš„é›†åˆï¼Œ*R* æ˜¯å¯¹è±¡ä¹‹é—´çš„å…³ç³»ï¼Œ*Rl* å’Œ *Ol* åˆ†åˆ«æ˜¯å…³ç³»å’Œå¯¹è±¡æ ‡ç­¾ã€‚
- en: Pr(G|I) = Pr(O|I) * Pr(R|O, I) * Pr(Rl, Ol|O, R, I)
  id: totrans-22
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: Pr(G|I) = Pr(O|I) * Pr(R|O, I) * Pr(Rl, Ol|O, R, I)
- en: 'Fig. 2 shows the entire algorithm in pictorial form, and how all three stages
    work consecutively. The pipeline can be described as: using faster R-CNN for object
    detection, then generating an initial graph with all defined relationships, then
    pruning unlikely edges based on *relatedness* score, and then finally using aGCN
    for refining.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 2 å±•ç¤ºäº†æ•´ä¸ªç®—æ³•çš„å›¾ç¤ºå½¢å¼ï¼Œä»¥åŠæ‰€æœ‰ä¸‰ä¸ªé˜¶æ®µå¦‚ä½•ä¾æ¬¡å·¥ä½œã€‚è¯¥æµç¨‹å¯ä»¥æè¿°ä¸ºï¼šä½¿ç”¨ Faster R-CNN è¿›è¡Œå¯¹è±¡æ£€æµ‹ï¼Œç„¶åç”Ÿæˆä¸€ä¸ªåŒ…å«æ‰€æœ‰å®šä¹‰å…³ç³»çš„åˆå§‹å›¾ï¼Œç„¶ååŸºäº
    *ç›¸å…³æ€§* åˆ†æ•°ä¿®å‰ªä¸å¤ªå¯èƒ½çš„è¾¹ç¼˜ï¼Œæœ€åä½¿ç”¨ aGCN è¿›è¡Œç²¾ç‚¼ã€‚
- en: '![](../Images/e95853623c01823b7da7f73796aa1bb6.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e95853623c01823b7da7f73796aa1bb6.png)'
- en: 'Figure 2: Graph R-CNN [8] Working Explained'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 'å›¾ 2: å›¾å½¢ R-CNN [8] å·¥ä½œè¯´æ˜'
- en: 3.2 Stacked Motif Network (MotifNet)
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.2 å †å  Motif ç½‘ç»œï¼ˆMotifNetï¼‰
- en: R. Zellers et. al. [9] proposes a network that performs bounding box collection,
    object identification, and relation identification consecutively. The method shows
    the intense use of faster R-CNN and Long Short-term Memory (LSTM) networks. In
    contrast with Graph R-CNN, this method is based on detecting the relationships
    among objects and adding the relationship edges.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: R. Zellers ç­‰äºº[9] æå‡ºäº†ä¸€ä¸ªç½‘ç»œï¼Œè¯¥ç½‘ç»œä¾æ¬¡æ‰§è¡Œè¾¹ç•Œæ¡†æ”¶é›†ã€å¯¹è±¡è¯†åˆ«å’Œå…³ç³»è¯†åˆ«ã€‚è¿™ç§æ–¹æ³•å±•ç¤ºäº†å¯¹ Faster R-CNN å’Œé•¿çŸ­æœŸè®°å¿†ï¼ˆLSTMï¼‰ç½‘ç»œçš„å¯†é›†ä½¿ç”¨ã€‚ä¸å›¾å½¢
    R-CNN ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•åŸºäºæ£€æµ‹å¯¹è±¡ä¹‹é—´çš„å…³ç³»å¹¶æ·»åŠ å…³ç³»è¾¹ã€‚
- en: The idea here can be seen as a conditional probability equation, as shown in
    the equation below, which states the probability to find graph *G* of image *I*
    is given as the product of the probability of finding bounding box array *B* given
    *I*, probability of obtaining object *O* given *B* and *I*, and the probability
    of getting relations *R* among *O* given *B*, *O* and *I*.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é‡Œçš„æƒ³æ³•å¯ä»¥çœ‹ä½œæ˜¯ä¸€ä¸ªæ¡ä»¶æ¦‚ç‡æ–¹ç¨‹ï¼Œå¦‚ä¸‹æ–¹ç¨‹æ‰€ç¤ºï¼Œè¡¨ç¤ºåœ¨ç»™å®š *I* çš„æƒ…å†µä¸‹æ‰¾åˆ°å›¾ *G* çš„æ¦‚ç‡ä¸ºæ‰¾åˆ°è¾¹ç•Œæ¡†æ•°ç»„ *B* çš„æ¦‚ç‡ã€åœ¨ç»™å®š *B*
    å’Œ *I* çš„æƒ…å†µä¸‹è·å¾—å¯¹è±¡ *O* çš„æ¦‚ç‡ï¼Œä»¥åŠåœ¨ç»™å®š *B*ã€*O* å’Œ *I* çš„æƒ…å†µä¸‹è·å¾—å…³ç³» *R* çš„æ¦‚ç‡çš„ä¹˜ç§¯ã€‚
- en: Pr(G|I) = Pr(B|I) * Pr(O|B,I) * Pr(R|B,O,I)
  id: totrans-29
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: Pr(G|I) = Pr(B|I) * Pr(O|B,I) * Pr(R|B,O,I)
- en: Firstly, to find the set of the bounding box, they also use faster R-CNN, as
    in [8], shown on the left side of Fig. 3\. It uses the VGG backbone structure
    for object identification while determining bounding boxes in *I*. Then, the model
    proceeds further with bidirectional LSTM layers depicted by the object context
    layer in Fig. 3\. This block of layers generates the object labels for each bounding
    box region, *bi* âˆˆ *B*. Another bidirectional LSTM layers block is again used
    for determining relations as edge context, portrayed in the same figure. The output
    is computed as determining the edge labels which clearly depict the outer product
    with objects and relations among them. The detailed model structure can be learned
    in [9].
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: é¦–å…ˆï¼Œä¸ºäº†æ‰¾åˆ°è¾¹ç•Œæ¡†çš„é›†åˆï¼Œä»–ä»¬ä¹Ÿä½¿ç”¨ Faster R-CNNï¼Œå¦‚ [8] æ‰€ç¤ºï¼Œè§å›¾ 3 çš„å·¦ä¾§ã€‚å®ƒä½¿ç”¨ VGG ä¸»å¹²ç»“æ„è¿›è¡Œå¯¹è±¡è¯†åˆ«ï¼ŒåŒæ—¶ç¡®å®š *I*
    ä¸­çš„è¾¹ç•Œæ¡†ã€‚ç„¶åï¼Œæ¨¡å‹ç»§ç»­ä½¿ç”¨å›¾ 3 ä¸­çš„å¯¹è±¡ä¸Šä¸‹æ–‡å±‚æ‰€ç¤ºçš„åŒå‘ LSTM å±‚ã€‚è¯¥å±‚å—ç”Ÿæˆæ¯ä¸ªè¾¹ç•Œæ¡†åŒºåŸŸçš„å¯¹è±¡æ ‡ç­¾ï¼Œ*bi* âˆˆ *B*ã€‚å¦ä¸€ä¸ªåŒå‘ LSTM
    å±‚å—ç”¨äºç¡®å®šä½œä¸ºè¾¹ç¼˜ä¸Šä¸‹æ–‡çš„å…³ç³»ï¼Œå¦‚åŒä¸€å›¾ä¸­æ‰€ç¤ºã€‚è¾“å‡ºç»“æœè®¡ç®—ä¸ºç¡®å®šè¾¹ç¼˜æ ‡ç­¾ï¼Œè¿™äº›æ ‡ç­¾æ¸…æ™°åœ°æè¿°äº†å¯¹è±¡åŠå…¶ä¹‹é—´çš„å…³ç³»çš„å¤–ç§¯ã€‚è¯¦ç»†çš„æ¨¡å‹ç»“æ„å¯ä»¥åœ¨ [9] ä¸­å­¦ä¹ ã€‚
- en: '![](../Images/418fe06b2b6453556f0f2ede61b2c4d7.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/418fe06b2b6453556f0f2ede61b2c4d7.png)'
- en: 'Figure 3: MotifNet [9] Structure Flow Diagram Explained (Dog Image by [Justin
    Veenema](https://unsplash.com/@justinveenema?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)
    on [Unsplash](https://unsplash.com/photos/white-and-black-american-pit-bull-terrier-at-daytime-NH1d0xX6Ldk?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash))'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 'å›¾ 3: MotifNet [9] ç»“æ„æµç¨‹å›¾è¯´æ˜ï¼ˆç‹—çš„å›¾ç‰‡ç”± [Justin Veenema](https://unsplash.com/@justinveenema?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)
    æä¾›ï¼Œæ¥æºäº [Unsplash](https://unsplash.com/photos/white-and-black-american-pit-bull-terrier-at-daytime-NH1d0xX6Ldk?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)ï¼‰'
- en: 4\. Experiments in State-of-the-Art Approaches
  id: totrans-33
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 4\. æœ€å…ˆè¿›æ–¹æ³•ä¸­çš„å®éªŒ
- en: 4.1 Experimental Setup
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.1 å®éªŒè®¾ç½®
- en: 'The setup for both methods is described below:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸¤ç§æ–¹æ³•çš„è®¾ç½®å¦‚ä¸‹æ‰€è¿°ï¼š
- en: '**Graph R-CNN:** The use of faster R-CNN with VGG16 backbone to ensure object
    detection is implemented via PyTorch. For the RePN implementation, a multi-layer
    perceptron structure is used to analyze the relatedness score using two projection
    functions, each for subject and object relation. Two aGCN layers are used, one
    for the feature level, the result of which is sent to the other one at the semantic
    level. The training is done in two stages, first only the object detector is trained,
    and then the whole model is trained jointly.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '**å›¾å½¢ R-CNNï¼š** ä½¿ç”¨å¸¦æœ‰ VGG16 ä¸»å¹²çš„æ›´å¿« R-CNN æ¥ç¡®ä¿å¯¹è±¡æ£€æµ‹ï¼Œé€šè¿‡ PyTorch å®ç°ã€‚å¯¹äº RePN å®ç°ï¼Œä½¿ç”¨å¤šå±‚æ„ŸçŸ¥æœºç»“æ„é€šè¿‡ä¸¤ä¸ªæŠ•å½±å‡½æ•°ï¼ˆåˆ†åˆ«é’ˆå¯¹ä¸»é¢˜å’Œå¯¹è±¡å…³ç³»ï¼‰åˆ†æç›¸å…³æ€§å¾—åˆ†ã€‚ä½¿ç”¨ä¸¤ä¸ª
    aGCN å±‚ï¼Œä¸€ä¸ªç”¨äºç‰¹å¾çº§åˆ«ï¼Œå…¶ç»“æœä¼ é€’ç»™å¦ä¸€ä¸ªè¯­ä¹‰çº§åˆ«çš„å±‚ã€‚è®­ç»ƒåˆ†ä¸¤ä¸ªé˜¶æ®µè¿›è¡Œï¼Œé¦–å…ˆè®­ç»ƒå¯¹è±¡æ£€æµ‹å™¨ï¼Œç„¶åè”åˆè®­ç»ƒæ•´ä¸ªæ¨¡å‹ã€‚'
- en: '**MotifNet:** The images that are fed into the bounding box detector are made
    to be of size 592x592, by using the zero-padding method. All the LSTM layers undergo
    highway connections. Two and four alternating highway LSTM layers are used for
    object and edge context respectively. The ordering of the bounding box regions
    can be done in several ways using central x-coordinate, maximum non-background
    prediction, size of the bounding box, or just random shuffling.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '**MotifNetï¼š** é€šè¿‡ä½¿ç”¨é›¶å¡«å……æ–¹æ³•ï¼Œå°†è¾“å…¥åˆ°è¾¹ç•Œæ¡†æ£€æµ‹å™¨çš„å›¾åƒè°ƒæ•´ä¸º 592x592 çš„å¤§å°ã€‚æ‰€æœ‰ LSTM å±‚éƒ½ç»è¿‡é«˜é€Ÿå…¬è·¯è¿æ¥ã€‚åˆ†åˆ«ä¸ºå¯¹è±¡å’Œè¾¹ç¼˜ä¸Šä¸‹æ–‡ä½¿ç”¨äº†ä¸¤ä¸ªå’Œå››ä¸ªäº¤æ›¿çš„é«˜é€Ÿå…¬è·¯
    LSTM å±‚ã€‚è¾¹ç•Œæ¡†åŒºåŸŸçš„æ’åºå¯ä»¥é€šè¿‡ä¸­å¤® x åæ ‡ã€æœ€å¤§éèƒŒæ™¯é¢„æµ‹ã€è¾¹ç•Œæ¡†å¤§å°æˆ–éšæœºæ´—ç‰Œæ¥å®Œæˆã€‚'
- en: The main challenge is to analyze the model with a common dataset framework,
    as different approaches use different data preprocessing, split, and evaluation.
    However, the discussed approaches, Graph R-CNN and MotifNet uses the publicly
    available data processing scheme and split from [7]. There are 150 object classes
    and 50 classes for relations in this Visual Genome dataset [4].
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸»è¦æŒ‘æˆ˜æ˜¯ä½¿ç”¨å…±åŒçš„æ•°æ®é›†æ¡†æ¶åˆ†ææ¨¡å‹ï¼Œå› ä¸ºä¸åŒçš„æ–¹æ³•ä½¿ç”¨ä¸åŒçš„æ•°æ®é¢„å¤„ç†ã€æ‹†åˆ†å’Œè¯„ä¼°ã€‚ç„¶è€Œï¼Œè®¨è®ºçš„æ–¹æ³•ï¼ŒGraph R-CNN å’Œ MotifNet
    ä½¿ç”¨äº†[7]ä¸­å…¬å¼€çš„æ•°æ®å¤„ç†æ–¹æ¡ˆå’Œæ‹†åˆ†ã€‚è¿™ä¸ª Visual Genome æ•°æ®é›†ä¸­æœ‰ 150 ä¸ªå¯¹è±¡ç±»åˆ«å’Œ 50 ä¸ªå…³ç³»ç±»åˆ«[4]ã€‚
- en: '**Visual Genome Dataset [4] in a nutshell:**'
  id: totrans-39
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**Visual Genome æ•°æ®é›† [4] æ€»ç»“ï¼š**'
- en: ''
  id: totrans-40
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Human Annotated Images
  id: totrans-41
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: äººå·¥æ ‡æ³¨çš„å›¾åƒ
- en: ''
  id: totrans-42
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: More than 100,000 images
  id: totrans-43
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: è¶…è¿‡ 100,000 å¼ å›¾åƒ
- en: ''
  id: totrans-44
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 150 Object Classes
  id: totrans-45
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 150 ä¸ªå¯¹è±¡ç±»åˆ«
- en: ''
  id: totrans-46
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 50 Relation Classes
  id: totrans-47
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 50 ä¸ªå…³ç³»ç±»åˆ«
- en: ''
  id: totrans-48
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Each image has around 11.5 objects and 6.2 relationships in scene graph
  id: totrans-49
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: æ¯å¼ å›¾åƒåœ¨åœºæ™¯å›¾ä¸­å¤§çº¦æœ‰ 11.5 ä¸ªå¯¹è±¡å’Œ 6.2 ä¸ªå…³ç³»
- en: 4.2 Experimental Results
  id: totrans-50
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.2 å®éªŒç»“æœ
- en: '![](../Images/045f9e42cf75bc160863454ce696921b.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/045f9e42cf75bc160863454ce696921b.png)'
- en: 'Table 1: Performance Comparison'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: è¡¨ 1ï¼šæ€§èƒ½æ¯”è¾ƒ
- en: '**Quantitative Comparison:** Both methods evaluated their model using the recall
    metric. Table 1 shows the comparison of both methods via different quantitative
    indicators. (1) Predicate Classification (PredCls) denotes the performance to
    recognize the relation between objects, (2) Phrase Classification (PhrCls) or
    scene graph classification in [9] depicts the ability to observe the categories
    of both objects and relations, (3) Scene Graph Generation (SGGen) or scene graph
    detection in [9] represents the performance to combine the objects with detected
    relations among them. In [8], they enhance the latter metric with a comprehensive
    SGGen (SGGen+) that includes the possibility of having a certain scenario like
    detecting a *man* as *boy*, technically it is a failed detection, but qualitatively
    if all the relations to this object is detected successfully then it should be
    considered as a successful result, hence increasing the SGGen metric value.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '**å®šé‡æ¯”è¾ƒï¼š** ä¸¤ç§æ–¹æ³•éƒ½ä½¿ç”¨å¬å›ç‡æŒ‡æ ‡è¯„ä¼°äº†å®ƒä»¬çš„æ¨¡å‹ã€‚è¡¨ 1 é€šè¿‡ä¸åŒçš„å®šé‡æŒ‡æ ‡å±•ç¤ºäº†è¿™ä¸¤ç§æ–¹æ³•çš„æ¯”è¾ƒã€‚(1) è°“è¯åˆ†ç±»ï¼ˆPredClsï¼‰è¡¨ç¤ºè¯†åˆ«å¯¹è±¡ä¹‹é—´å…³ç³»çš„æ€§èƒ½ï¼Œ(2)
    çŸ­è¯­åˆ†ç±»ï¼ˆPhrClsï¼‰æˆ–[9]ä¸­çš„åœºæ™¯å›¾åˆ†ç±»è¡¨ç¤ºè§‚å¯Ÿå¯¹è±¡å’Œå…³ç³»ç±»åˆ«çš„èƒ½åŠ›ï¼Œ(3) åœºæ™¯å›¾ç”Ÿæˆï¼ˆSGGenï¼‰æˆ–[9]ä¸­çš„åœºæ™¯å›¾æ£€æµ‹è¡¨ç¤ºå°†å¯¹è±¡ä¸æ£€æµ‹åˆ°çš„å…³ç³»ç»“åˆçš„æ€§èƒ½ã€‚åœ¨[8]ä¸­ï¼Œä»–ä»¬é€šè¿‡å…¨é¢çš„SGGenï¼ˆSGGen+ï¼‰æ¥æå‡åè€…çš„æŒ‡æ ‡ï¼Œè¯¥æŒ‡æ ‡åŒ…æ‹¬äº†è¯†åˆ«*ç”·äºº*ä¸º*ç”·å­©*çš„å¯èƒ½æ€§ï¼Œä»æŠ€æœ¯ä¸Šè®²ï¼Œè¿™æ˜¯ä¸€ä¸ªå¤±è´¥çš„æ£€æµ‹ï¼Œä½†å¦‚æœå¯¹è¯¥å¯¹è±¡çš„æ‰€æœ‰å…³ç³»éƒ½è¢«æˆåŠŸæ£€æµ‹åˆ°ï¼Œé‚£ä¹ˆåº”è§†ä¸ºæˆåŠŸç»“æœï¼Œä»è€Œæé«˜SGGenæŒ‡æ ‡å€¼ã€‚'
- en: According to table 1, MotifNet [9] performs comparatively better when analyzing
    objects, edges, and relation labels separately. However, the generation of the
    entire graph of a given image is more accurate using the second approach, Graph
    R-CNN [8]. It also shows that having the comprehensive output metric shows a better
    analysis of the scene graph model.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: æ ¹æ®è¡¨1ï¼ŒMotifNet [9] åœ¨åˆ†åˆ«åˆ†æå¯¹è±¡ã€è¾¹ç¼˜å’Œå…³ç³»æ ‡ç­¾æ—¶è¡¨ç°ç›¸å¯¹æ›´å¥½ã€‚ç„¶è€Œï¼Œä½¿ç”¨ç¬¬äºŒç§æ–¹æ³•ï¼Œå³ Graph R-CNN [8]ï¼Œç”Ÿæˆç»™å®šå›¾åƒçš„æ•´ä¸ªå›¾å½¢æ›´ä¸ºå‡†ç¡®ã€‚å®ƒè¿˜è¡¨æ˜ï¼Œæ‹¥æœ‰å…¨é¢çš„è¾“å‡ºæŒ‡æ ‡å¯ä»¥æ›´å¥½åœ°åˆ†æåœºæ™¯å›¾æ¨¡å‹ã€‚
- en: '**Qualitative Comparison:** In neural motifs structure [9], they consider the
    qualitative results separately. For instance, the detection of relation edge *wearing*
    as *wears* falls under the category of failed detection. It shows that the model
    [9] performs better than what the output metric number shows. On the other hand,
    [8] includes this understanding of result in their comprehensive SGGen (SGGen+)
    metric which already takes possible not-so-failed detections into consideration.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '**å®šæ€§æ¯”è¾ƒï¼š** åœ¨ç¥ç»ç»“æ„ [9] ä¸­ï¼Œä»–ä»¬åˆ†åˆ«è€ƒè™‘å®šæ€§ç»“æœã€‚ä¾‹å¦‚ï¼Œå…³ç³»è¾¹ç¼˜çš„æ£€æµ‹ *wearing* ä½œä¸º *wears* è¢«å½’ä¸ºå¤±è´¥æ£€æµ‹ç±»åˆ«ã€‚è¿™è¡¨æ˜è¯¥æ¨¡å‹
    [9] çš„è¡¨ç°ä¼˜äºè¾“å‡ºæŒ‡æ ‡æ‰€æ˜¾ç¤ºçš„æ•ˆæœã€‚å¦ä¸€æ–¹é¢ï¼Œ[8] å°†è¿™ç§ç»“æœç†è§£çº³å…¥ä»–ä»¬çš„å…¨é¢SGGen (SGGen+) æŒ‡æ ‡ä¸­ï¼Œè¯¥æŒ‡æ ‡å·²è€ƒè™‘äº†å¯èƒ½ä¸æ˜¯å®Œå…¨å¤±è´¥çš„æ£€æµ‹ã€‚'
- en: 5\. Applications in Robotics
  id: totrans-56
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 5\. æœºå™¨äººå­¦ä¸­çš„åº”ç”¨
- en: This article will overview the understanding of scenes that helps in robotic
    applications, especially robot planning. The wide area of robotics including automatic
    indoor mapping, teleoperation robots, human-robot control for purposes like telemedicine,
    and many more results in a very deep area of SGs applications.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬æ–‡å°†æ¦‚è¿°æœ‰åŠ©äºæœºå™¨äººåº”ç”¨çš„åœºæ™¯ç†è§£ï¼Œç‰¹åˆ«æ˜¯æœºå™¨äººè§„åˆ’ã€‚æœºå™¨äººå­¦çš„å¹¿æ³›é¢†åŸŸåŒ…æ‹¬è‡ªåŠ¨å®¤å†…åˆ¶å›¾ã€é¥æ§æœºå™¨äººã€ç”¨äºè¿œç¨‹åŒ»ç–—ç­‰ç›®çš„çš„äººæœºæ§åˆ¶ï¼Œä»¥åŠæ›´å¤šåº”ç”¨ï¼Œå½¢æˆäº†ä¸€ä¸ªéå¸¸æ·±å¥¥çš„åœºæ™¯å›¾åº”ç”¨é¢†åŸŸã€‚
- en: When both humans and robots collaborated in a similar workspace to complete
    a specified task, then it is called Human-Robot Collaboration (HRC), and the collaborative
    robots are called *cobots*. Having semantic information along with object detection
    makes the tasks even easier to do. [5] shows a method that uses SGG for safety
    analysis in HRC. Another application involves teleoperation robots that include
    social interaction skills like a meeting chat room on a mobile stick controlled
    by a human from a remote location. The applications include elderly care, attending
    events even if a person is disabled, etc. [2] discusses a method where scene understanding
    helps the user to analyze and control remote environment clearly.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: å½“äººç±»å’Œæœºå™¨äººåœ¨ç±»ä¼¼çš„å·¥ä½œç©ºé—´ä¸­åˆä½œå®ŒæˆæŒ‡å®šä»»åŠ¡æ—¶ï¼Œè¿™ç§°ä¸ºäººæœºåä½œï¼ˆHRCï¼‰ï¼Œè€Œåä½œæœºå™¨äººè¢«ç§°ä¸º*cobots*ã€‚æ‹¥æœ‰è¯­ä¹‰ä¿¡æ¯ä»¥åŠå¯¹è±¡æ£€æµ‹å¯ä»¥ä½¿ä»»åŠ¡å˜å¾—æ›´å®¹æ˜“ã€‚
    [5] å±•ç¤ºäº†ä¸€ç§ä½¿ç”¨SGGè¿›è¡ŒHRCå®‰å…¨åˆ†æçš„æ–¹æ³•ã€‚å¦ä¸€ä¸ªåº”ç”¨æ¶‰åŠé¥æ§æœºå™¨äººï¼ŒåŒ…æ‹¬ç¤¾ä¼šäº’åŠ¨æŠ€èƒ½ï¼Œä¾‹å¦‚ç”±è¿œç¨‹ä½ç½®çš„äººæ“æ§çš„ç§»åŠ¨æ†ä¸Šçš„ä¼šè®®èŠå¤©å®¤ã€‚åº”ç”¨é¢†åŸŸåŒ…æ‹¬è€å¹´æŠ¤ç†ã€å³ä½¿åœ¨æ®‹ç–¾æƒ…å†µä¸‹ä¹Ÿèƒ½å‚åŠ æ´»åŠ¨ç­‰ã€‚[2]
    è®¨è®ºäº†ä¸€ç§æ–¹æ³•ï¼Œå…¶ä¸­åœºæ™¯ç†è§£å¸®åŠ©ç”¨æˆ·æ¸…æ™°åœ°åˆ†æå’Œæ§åˆ¶è¿œç¨‹ç¯å¢ƒã€‚
- en: 5.1 Robot Planning
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.1 æœºå™¨äººè§„åˆ’
- en: Assigning certain motion tasks to a robot, such as moving one object from one
    place to another, and expecting smooth service from a machine requires a highly
    accurate level of planning and development. SGG plays an important role in producing
    service robots as it gives the robot an in-depth abstracted picture of the scene
    which further helps it to locate objects precisely. [1] uses local scene graphs
    to perceive the global view to reach its target. It describes robot planning for
    a common task that includes searching for an object, such as a fruit, in an indoor
    environment.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: å°†ç‰¹å®šçš„è¿åŠ¨ä»»åŠ¡åˆ†é…ç»™æœºå™¨äººï¼Œå¦‚å°†ä¸€ä¸ªç‰©ä½“ä»ä¸€ä¸ªåœ°æ–¹ç§»åŠ¨åˆ°å¦ä¸€ä¸ªåœ°æ–¹ï¼Œå¹¶æœŸæœ›æœºå™¨æä¾›å¹³ç¨³æœåŠ¡ï¼Œéœ€è¦é«˜åº¦å‡†ç¡®çš„è§„åˆ’å’Œå¼€å‘ã€‚SGG åœ¨ç”Ÿäº§æœåŠ¡æœºå™¨äººä¸­å‘æŒ¥é‡è¦ä½œç”¨ï¼Œå› ä¸ºå®ƒä¸ºæœºå™¨äººæä¾›äº†åœºæ™¯çš„æ·±å…¥æŠ½è±¡å›¾åƒï¼Œè¿™è¿›ä¸€æ­¥å¸®åŠ©æœºå™¨äººç²¾ç¡®å®šä½ç‰©ä½“ã€‚[1]
    ä½¿ç”¨å±€éƒ¨åœºæ™¯å›¾æ¥æ„ŸçŸ¥å…¨çƒè§†å›¾ä»¥è¾¾åˆ°ç›®æ ‡ã€‚å®ƒæè¿°äº†ä¸€ä¸ªåŒ…æ‹¬åœ¨å®¤å†…ç¯å¢ƒä¸­å¯»æ‰¾ç‰©ä½“ï¼ˆå¦‚æ°´æœï¼‰çš„å¸¸è§ä»»åŠ¡çš„æœºå™¨äººè§„åˆ’ã€‚
- en: '**Scene analysis for robot planning (SARP)** [1] is an algorithm designed for
    robots to use visual contextual information to complete the planned task. This
    method takes advantage of scene graphs to model the global scene understanding
    using MotifNet [9], the algorithm discussed in section 3.2\. The block diagram
    of the model can be accessed in [1]. The model uses MotifNet to generate local
    scene graphs, then the contextual information to analyze uncertainty between observations
    and actions, and update the robotâ€™s belief. The latter process is done using the
    Partially Observable Markov Decision Process (PO-MDP) framework. The MDPs are
    known for sequential decision-making. In the block diagram, the offline trained
    scene graph network generates the global scene graphs and feeds the result into
    the Markov process.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '**æœºå™¨äººè§„åˆ’çš„åœºæ™¯åˆ†æ (SARP)** [1] æ˜¯ä¸€ç§ç®—æ³•ï¼Œæ—¨åœ¨ä½¿æœºå™¨äººåˆ©ç”¨è§†è§‰ä¸Šä¸‹æ–‡ä¿¡æ¯æ¥å®Œæˆè®¡åˆ’ä»»åŠ¡ã€‚è¯¥æ–¹æ³•åˆ©ç”¨åœºæ™¯å›¾é€šè¿‡MotifNet [9]
    å»ºæ¨¡å…¨å±€åœºæ™¯ç†è§£ï¼ŒMotifNetç®—æ³•åœ¨ç¬¬3.2èŠ‚ä¸­è®¨è®ºã€‚æ¨¡å‹çš„æ¡†å›¾å¯ä»¥åœ¨[1]ä¸­æŸ¥çœ‹ã€‚è¯¥æ¨¡å‹ä½¿ç”¨MotifNetç”Ÿæˆæœ¬åœ°åœºæ™¯å›¾ï¼Œç„¶ååˆ©ç”¨ä¸Šä¸‹æ–‡ä¿¡æ¯æ¥åˆ†æè§‚å¯Ÿå’Œè¡ŒåŠ¨ä¹‹é—´çš„ä¸ç¡®å®šæ€§ï¼Œå¹¶æ›´æ–°æœºå™¨äººçš„ä¿¡å¿µã€‚åç»­è¿‡ç¨‹ä½¿ç”¨éƒ¨åˆ†å¯è§‚å¯Ÿé©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼ˆPO-MDPï¼‰æ¡†æ¶å®Œæˆã€‚MDPä»¥é¡ºåºå†³ç­–è‘—ç§°ã€‚åœ¨æ¡†å›¾ä¸­ï¼Œç¦»çº¿è®­ç»ƒçš„åœºæ™¯å›¾ç½‘ç»œç”Ÿæˆå…¨å±€åœºæ™¯å›¾ï¼Œå¹¶å°†ç»“æœè¾“å…¥åˆ°é©¬å°”å¯å¤«è¿‡ç¨‹ã€‚'
- en: '***Multiple Local SGs â€” â€” â€” â€” â€” â†’Global Context SGs â€” â€” â€” â€” â†’Target Search***'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '***å¤šä¸ªæœ¬åœ°SGs â€” â€” â€” â€” â€” â†’å…¨å±€ä¸Šä¸‹æ–‡SGs â€” â€” â€” â€” â†’ç›®æ ‡æœç´¢***'
- en: The method is experimented for locating objects scattered through the area with
    precision and in minimal time. The robot is fed with a scene graph dataset, an
    already trained scene graph network, and a domain map for directive mobility.
    As a result, the robot navigates through the area for target search, and to do
    so, it creates scene graphs for contextual information every step of the way,
    as clearly shown in figure 2 and 3 of the [published IEEE article](https://ieeexplore.ieee.org/abstract/document/9730031)
    [1]. It is the demonstration where the robot is assigned the task of locating
    a *banana*. It increments the global scene graph at every time instant except
    at T=4 because there are no new objectsâ€™ instances in that frame. It can be seen
    that having local scene graphs helped the robot to determine the global context
    for the target search. According to the performance comparison in [1], SARP outperforms
    the baseline methods both in terms of success rate and action cost. However, the
    model can be further expanded with facial recognition of humans, and also analyzed
    when changing or adding objects during the test process.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: è¯¥æ–¹æ³•åœ¨ç²¾å‡†ä¸”æœ€å°æ—¶é—´å†…å®šä½æ•£å¸ƒåœ¨åŒºåŸŸå†…çš„ç‰©ä½“ã€‚æœºå™¨äººè¢«æä¾›äº†ä¸€ä¸ªåœºæ™¯å›¾æ•°æ®é›†ã€ä¸€ä¸ªå·²ç»è®­ç»ƒå¥½çš„åœºæ™¯å›¾ç½‘ç»œå’Œä¸€ä¸ªç”¨äºæŒ‡ä»¤ç§»åŠ¨çš„é¢†åŸŸåœ°å›¾ã€‚ç»“æœæ˜¯ï¼Œæœºå™¨äººåœ¨åŒºåŸŸå†…è¿›è¡Œç›®æ ‡æœç´¢ï¼Œä¸ºæ­¤ï¼Œå®ƒåœ¨æ¯ä¸€æ­¥éƒ½åˆ›å»ºåœºæ™¯å›¾ä»¥è·å–ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œå¦‚[å‘è¡¨çš„IEEEæ–‡ç« ](https://ieeexplore.ieee.org/abstract/document/9730031)
    [1]ä¸­çš„å›¾2å’Œå›¾3æ‰€ç¤ºã€‚è¿™æ˜¯ä¸€ä¸ªæ¼”ç¤ºï¼Œå±•ç¤ºäº†æœºå™¨äººè¢«åˆ†é…ä»»åŠ¡ä»¥å®šä½*é¦™è•‰*ã€‚å®ƒåœ¨æ¯ä¸ªæ—¶é—´ç‚¹éƒ½æ›´æ–°å…¨å±€åœºæ™¯å›¾ï¼Œé™¤äº†T=4ï¼Œå› ä¸ºåœ¨é‚£ä¸ªå¸§ä¸­æ²¡æœ‰æ–°çš„ç‰©ä½“å®ä¾‹ã€‚å¯ä»¥çœ‹å‡ºï¼Œæ‹¥æœ‰æœ¬åœ°åœºæ™¯å›¾å¸®åŠ©æœºå™¨äººç¡®å®šç›®æ ‡æœç´¢çš„å…¨å±€ä¸Šä¸‹æ–‡ã€‚æ ¹æ®[1]ä¸­çš„æ€§èƒ½æ¯”è¾ƒï¼ŒSARPåœ¨æˆåŠŸç‡å’Œè¡ŒåŠ¨æˆæœ¬æ–¹é¢éƒ½ä¼˜äºåŸºçº¿æ–¹æ³•ã€‚ç„¶è€Œï¼Œè¯¥æ¨¡å‹å¯ä»¥è¿›ä¸€æ­¥æ‰©å±•ï¼ŒåŠ å…¥äººè„¸è¯†åˆ«åŠŸèƒ½ï¼Œå¹¶ä¸”åœ¨æµ‹è¯•è¿‡ç¨‹ä¸­æ›´æ¢æˆ–æ·»åŠ ç‰©ä½“æ—¶ä¹Ÿéœ€è¦è¿›è¡Œåˆ†æã€‚
- en: 6\. Conclusion
  id: totrans-64
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 6\. ç»“è®º
- en: The article discusses the different methods for SGG, the generation of graphs
    having semantic information about scenes, and some of its robotic applications.
    The object and relation contextual information gives us a very descriptive understanding
    of a scene. MotifNet detects and adds the relation among objects as edge context
    using the LSTM network, while Graph R-CNN forms the scene graphs by eliminating
    the unlikely relation edges using *relatedness* score. There are several versions
    of the Visual Genome dataset used in recent approaches, however, MotifNet and
    Graph R-CNN evaluate their model on the same dataset model, which makes their
    quantitative comparison reasonable. Overall, both the quantitative and qualitative
    measurements are analyzed and it can be seen that numbers donâ€™t tell the whole
    story, the not-so-failed scenarios, such as *man* instead of *boy*, should also
    be considered as successful outcome, if all of its relation edges are detected
    correctly, while evaluating a method for SGG. The applications in robotics is
    growing rapidly over the past decades and having an in-depth semantic description
    of the image, or scene will help in several image and video-related applications.
    One method is extensively discussed for the robot planning application that uses
    scene graphs for better visual scene understanding so that the robot can perform
    tasks (e.g., locating objects in indoor environment) in the long run.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ç¯‡æ–‡ç« è®¨è®ºäº† SGG çš„ä¸åŒæ–¹æ³•ï¼Œå³ç”ŸæˆåŒ…å«åœºæ™¯è¯­ä¹‰ä¿¡æ¯çš„å›¾å½¢ï¼Œä»¥åŠå…¶ä¸€äº›æœºå™¨äººåº”ç”¨ã€‚å¯¹è±¡å’Œå…³ç³»çš„ä¸Šä¸‹æ–‡ä¿¡æ¯è®©æˆ‘ä»¬å¯¹åœºæ™¯æœ‰äº†éå¸¸è¯¦ç»†çš„ç†è§£ã€‚MotifNet
    ä½¿ç”¨ LSTM ç½‘ç»œæ£€æµ‹å¹¶æ·»åŠ å¯¹è±¡ä¹‹é—´çš„å…³ç³»ä½œä¸ºè¾¹ç¼˜ä¸Šä¸‹æ–‡ï¼Œè€Œ Graph R-CNN é€šè¿‡ä½¿ç”¨*ç›¸å…³æ€§*åˆ†æ•°æ¥æ¶ˆé™¤ä¸å¤ªå¯èƒ½çš„å…³ç³»è¾¹ç¼˜ï¼Œä»è€Œå½¢æˆåœºæ™¯å›¾ã€‚è™½ç„¶æœ€è¿‘çš„æ–¹æ³•ä¸­ä½¿ç”¨äº†å‡ ç§ç‰ˆæœ¬çš„
    Visual Genome æ•°æ®é›†ï¼Œä½† MotifNet å’Œ Graph R-CNN åœ¨ç›¸åŒçš„æ•°æ®é›†æ¨¡å‹ä¸Šè¯„ä¼°äº†å®ƒä»¬çš„æ¨¡å‹ï¼Œè¿™ä½¿å¾—å®ƒä»¬çš„å®šé‡æ¯”è¾ƒæ˜¯åˆç†çš„ã€‚æ€»ä½“è€Œè¨€ï¼Œå¯¹å®šé‡å’Œå®šæ€§æµ‹é‡è¿›è¡Œäº†åˆ†æï¼Œå¯ä»¥çœ‹å‡ºï¼Œæ•°å­—å¹¶ä¸èƒ½å®Œå…¨è¯´æ˜é—®é¢˜ï¼Œåƒ*ç”·äºº*è€Œä¸æ˜¯*ç”·å­©*è¿™æ ·çš„å¹¶éå¤±è´¥çš„åœºæ™¯ï¼Œå¦‚æœå…¶æ‰€æœ‰å…³ç³»è¾¹ç¼˜éƒ½æ­£ç¡®æ£€æµ‹åˆ°ï¼Œä¹Ÿåº”è¢«è§†ä¸ºæˆåŠŸç»“æœã€‚åœ¨è¯„ä¼°
    SGG æ–¹æ³•æ—¶ï¼Œè€ƒè™‘å›¾åƒæˆ–åœºæ™¯çš„æ·±å…¥è¯­ä¹‰æè¿°å°†æœ‰åŠ©äºå›¾åƒå’Œè§†é¢‘ç›¸å…³åº”ç”¨ã€‚ä¸€ä¸ªæ–¹æ³•åœ¨æœºå™¨äººè§„åˆ’åº”ç”¨ä¸­å¾—åˆ°äº†å¹¿æ³›è®¨è®ºï¼Œè¯¥æ–¹æ³•ä½¿ç”¨åœºæ™¯å›¾æ¥æ›´å¥½åœ°ç†è§£è§†è§‰åœºæ™¯ï¼Œä»¥ä¾¿æœºå™¨äººåœ¨é•¿è¿œä¸­æ‰§è¡Œä»»åŠ¡ï¼ˆä¾‹å¦‚ï¼Œåœ¨å®¤å†…ç¯å¢ƒä¸­å®šä½ç‰©ä½“ï¼‰ã€‚
- en: Well, this is so much for a *small* research talk about converting images into
    interactive graphical texts. I hope you guys had fun (maybe a little!). Iâ€™ll be
    back with another research article summing up recent cool researches (probably
    on any topic from communication networking, Network Simulator (NS3), or maybe
    Wi-Fi 7ğŸ˜‰).
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: å¥½äº†ï¼Œè¿™å°±æ˜¯å…³äºå°†å›¾åƒè½¬æ¢ä¸ºäº’åŠ¨å›¾å½¢æ–‡æœ¬çš„*ç®€çŸ­*ç ”ç©¶è®²åº§çš„å…¨éƒ¨å†…å®¹ã€‚æˆ‘å¸Œæœ›å¤§å®¶ç©å¾—æ„‰å¿«ï¼ˆå¯èƒ½æœ‰ä¸€ç‚¹ç‚¹ï¼ï¼‰ã€‚æˆ‘ä¼šå¸¦ç€å¦ä¸€ç¯‡ç ”ç©¶æ–‡ç« å›æ¥ï¼Œæ€»ç»“è¿‘æœŸçš„ä¸€äº›æœ‰è¶£ç ”ç©¶ï¼ˆå¯èƒ½æ¶‰åŠé€šä¿¡ç½‘ç»œã€ç½‘ç»œæ¨¡æ‹Ÿå™¨ï¼ˆNS3ï¼‰ï¼Œæˆ–è€…Wi-Fi
    7ğŸ˜‰ï¼‰ã€‚
- en: Have fun researching!!
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: ç¥ä½ ä»¬ç ”ç©¶æ„‰å¿«ï¼ï¼
- en: Regards,
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤è‡´ï¼Œ
- en: Ritanshi
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: Ritanshi
- en: REFERENCES
  id: totrans-70
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å‚è€ƒæ–‡çŒ®
- en: S. Amiri, K. Chandan, and S. Zhang. Reasoning with scene graphs for robot planning
    under partial observability. IEEE Robotics and Automation Letters, 7(2):5560â€“5567,
    2022.
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: S. Amiri, K. Chandan å’Œ S. Zhangã€‚ã€ŠReasoning with scene graphs for robot planning
    under partial observabilityã€‹ã€‚IEEE Robotics and Automation Lettersï¼Œ7(2):5560â€“5567ï¼Œ2022å¹´ã€‚
- en: 'F. Amodeo, F. Caballero, N. DÃ­az-RodrÃ­guez, and L. Merino. Og-sgg: Ontology-guided
    scene graph generation. a case study in transfer learning for telepresence robotics.
    IEEE Access, pages 1â€“1, 2022.'
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'F. Amodeo, F. Caballero, N. DÃ­az-RodrÃ­guez å’Œ L. Merinoã€‚ã€ŠOg-sgg: Ontology-guided
    scene graph generation. A case study in transfer learning for telepresence roboticsã€‹ã€‚IEEE
    Accessï¼Œé¡µç  1â€“1ï¼Œ2022å¹´ã€‚'
- en: 'X. Chang, P. Ren, P. Xu, Z. Li, X. Chen, and A. Hauptmann. A comprehensive
    survey of scene graphs: Generation and application. IEEE Transactions on Pattern
    Analysis and Machine Intelligence, 45(1):1â€“26, 2023.'
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'X. Chang, P. Ren, P. Xu, Z. Li, X. Chen å’Œ A. Hauptmannã€‚ã€ŠA comprehensive survey
    of scene graphs: Generation and applicationã€‹ã€‚IEEE Transactions on Pattern Analysis
    and Machine Intelligenceï¼Œ45(1):1â€“26ï¼Œ2023å¹´ã€‚'
- en: 'R. Krishna, Y. Zhu, O. Groth, J. Johnson, K. Hata, J. Kravitz, S. Chen, Y.
    Kalantidis, L.-J. Li, D. A. Shamma, et al. Visual genome: Connecting language
    and vision using crowdsourced dense image annotations. International journal of
    computer vision, 123(1):32â€“73, 2017.'
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'R. Krishna, Y. Zhu, O. Groth, J. Johnson, K. Hata, J. Kravitz, S. Chen, Y.
    Kalantidis, L.-J. Li, D. A. Shamma ç­‰ã€‚ã€ŠVisual genome: Connecting language and vision
    using crowdsourced dense image annotationsã€‹ã€‚International Journal of Computer
    Visionï¼Œ123(1):32â€“73ï¼Œ2017å¹´ã€‚'
- en: H. Riaz, A. Terra, K. Raizer, R. Inam, and A. Hata. Scene understanding for
    safety analysis in human-robot collaborative operations. In 2020 6th International
    Conference on Control, Automation and Robotics (ICCAR), pages 722â€“731, 2020.
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: H. Riaz, A. Terra, K. Raizer, R. Inam å’Œ A. Hataã€‚ã€ŠScene understanding for safety
    analysis in human-robot collaborative operationsã€‹ã€‚åœ¨ 2020å¹´ç¬¬å…­å±Šæ§åˆ¶ã€è‡ªåŠ¨åŒ–ä¸æœºå™¨äººå›½é™…ä¼šè®®ï¼ˆICCARï¼‰ï¼Œé¡µç 
    722â€“731ï¼Œ2020å¹´ã€‚
- en: M. Suhail, A. Mittal, B. Siddiquie, C. Broaddus, J. Eledath, G. Medioni, and
    L. Sigal. Energy-based learning for scene graph generation. In 2021 IEEE/CVF Conference
    on Computer Vision and Pattern Recognition (CVPR), pages 13931â€“13940, 2021.
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: M. Suhail, A. Mittal, B. Siddiquie, C. Broaddus, J. Eledath, G. Medioni, å’Œ L.
    Sigalã€‚åŸºäºèƒ½é‡çš„åœºæ™¯å›¾ç”Ÿæˆå­¦ä¹ ã€‚å‘è¡¨äº2021å¹´IEEE/CVFè®¡ç®—æœºè§†è§‰ä¸æ¨¡å¼è¯†åˆ«ä¼šè®®ï¼ˆCVPRï¼‰ï¼Œç¬¬13931â€“13940é¡µï¼Œ2021å¹´ã€‚
- en: D. Xu, Y. Zhu, C. B. Choy, and L. Fei-Fei. Scene graph generation by iterative
    message passing. In Proceedings of the IEEE conference on computer vision and
    pattern recognition, pages 5410â€“5419, 2017.
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: D. Xu, Y. Zhu, C. B. Choy, å’Œ L. Fei-Feiã€‚é€šè¿‡è¿­ä»£æ¶ˆæ¯ä¼ é€’ç”Ÿæˆåœºæ™¯å›¾ã€‚å‘è¡¨äºIEEEè®¡ç®—æœºè§†è§‰ä¸æ¨¡å¼è¯†åˆ«ä¼šè®®è®ºæ–‡é›†ï¼Œç¬¬5410â€“5419é¡µï¼Œ2017å¹´ã€‚
- en: J. Yang, J. Lu, S. Lee, D. Batra, and D. Parikh. Graph r-cnn for scene graph
    generation. In Proceedings of the European conference on computer vision (ECCV),
    pages 670â€“685, 2018.
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: J. Yang, J. Lu, S. Lee, D. Batra, å’Œ D. Parikhã€‚ç”¨äºåœºæ™¯å›¾ç”Ÿæˆçš„å›¾å½¢r-cnnã€‚å‘è¡¨äºæ¬§æ´²è®¡ç®—æœºè§†è§‰ä¼šè®®ï¼ˆECCVï¼‰è®ºæ–‡é›†ï¼Œç¬¬670â€“685é¡µï¼Œ2018å¹´ã€‚
- en: 'R. Zellers, M. Yatskar, S. Thomson, and Y. Choi. Neural motifs: Scene graph
    parsing with global context. In 2018 IEEE/CVF Conference on Computer Vision and
    Pattern Recognition, pages 5831â€“5840, 2018.'
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: R. Zellers, M. Yatskar, S. Thomson, å’Œ Y. Choiã€‚ç¥ç»æ¨¡å¼ï¼šå…·æœ‰å…¨å±€ä¸Šä¸‹æ–‡çš„åœºæ™¯å›¾è§£æã€‚å‘è¡¨äº2018å¹´IEEE/CVFè®¡ç®—æœºè§†è§‰ä¸æ¨¡å¼è¯†åˆ«ä¼šè®®ï¼Œç¬¬5831â€“5840é¡µï¼Œ2018å¹´ã€‚
- en: 'G. Zhu, L. Zhang, Y. Jiang, Y. Dang, H. Hou, P. Shen, M. Feng, X. Zhao, Q.
    Miao, S. A. A. Shah, et al. Scene graph generation: A comprehensive survey. arXiv
    preprint arXiv:2201.00443, 2022.'
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: G. Zhu, L. Zhang, Y. Jiang, Y. Dang, H. Hou, P. Shen, M. Feng, X. Zhao, Q. Miao,
    S. A. A. Shahç­‰ã€‚åœºæ™¯å›¾ç”Ÿæˆï¼šä¸€é¡¹ç»¼åˆè°ƒæŸ¥ã€‚arXivé¢„å°æœ¬arXiv:2201.00443ï¼Œ2022å¹´ã€‚
