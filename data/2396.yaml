- en: Speech and Natural Language Input for Your Mobile App Using LLMs
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 LLMs 为你的移动应用提供语音和自然语言输入
- en: 原文：[https://towardsdatascience.com/speech-and-natural-language-input-for-your-mobile-app-using-llms-e79e23d3c5fd?source=collection_archive---------5-----------------------#2023-07-25](https://towardsdatascience.com/speech-and-natural-language-input-for-your-mobile-app-using-llms-e79e23d3c5fd?source=collection_archive---------5-----------------------#2023-07-25)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/speech-and-natural-language-input-for-your-mobile-app-using-llms-e79e23d3c5fd?source=collection_archive---------5-----------------------#2023-07-25](https://towardsdatascience.com/speech-and-natural-language-input-for-your-mobile-app-using-llms-e79e23d3c5fd?source=collection_archive---------5-----------------------#2023-07-25)
- en: How to leverage OpenAI GPT-4 Functions to navigate your GUI
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何利用 OpenAI GPT-4 功能来导航你的 GUI
- en: '[](https://medium.com/@hgwvandam?source=post_page-----e79e23d3c5fd--------------------------------)[![Hans
    van Dam](../Images/52846b7417b271767597c468edfaec46.png)](https://medium.com/@hgwvandam?source=post_page-----e79e23d3c5fd--------------------------------)[](https://towardsdatascience.com/?source=post_page-----e79e23d3c5fd--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----e79e23d3c5fd--------------------------------)
    [Hans van Dam](https://medium.com/@hgwvandam?source=post_page-----e79e23d3c5fd--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@hgwvandam?source=post_page-----e79e23d3c5fd--------------------------------)[![Hans
    van Dam](../Images/52846b7417b271767597c468edfaec46.png)](https://medium.com/@hgwvandam?source=post_page-----e79e23d3c5fd--------------------------------)[](https://towardsdatascience.com/?source=post_page-----e79e23d3c5fd--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----e79e23d3c5fd--------------------------------)
    [Hans van Dam](https://medium.com/@hgwvandam?source=post_page-----e79e23d3c5fd--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F6ce6c6116a37&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fspeech-and-natural-language-input-for-your-mobile-app-using-llms-e79e23d3c5fd&user=Hans+van+Dam&userId=6ce6c6116a37&source=post_page-6ce6c6116a37----e79e23d3c5fd---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----e79e23d3c5fd--------------------------------)
    ·14 min read·Jul 25, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fe79e23d3c5fd&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fspeech-and-natural-language-input-for-your-mobile-app-using-llms-e79e23d3c5fd&user=Hans+van+Dam&userId=6ce6c6116a37&source=-----e79e23d3c5fd---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F6ce6c6116a37&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fspeech-and-natural-language-input-for-your-mobile-app-using-llms-e79e23d3c5fd&user=Hans+van+Dam&userId=6ce6c6116a37&source=post_page-6ce6c6116a37----e79e23d3c5fd---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----e79e23d3c5fd--------------------------------)
    · 14 分钟阅读 · 2023年7月25日 [](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fe79e23d3c5fd&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fspeech-and-natural-language-input-for-your-mobile-app-using-llms-e79e23d3c5fd&user=Hans+van+Dam&userId=6ce6c6116a37&source=-----e79e23d3c5fd---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fe79e23d3c5fd&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fspeech-and-natural-language-input-for-your-mobile-app-using-llms-e79e23d3c5fd&source=-----e79e23d3c5fd---------------------bookmark_footer-----------)![](../Images/e9ebbfb4cc7e7ae96821d7ba97ed5661.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fe79e23d3c5fd&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fspeech-and-natural-language-input-for-your-mobile-app-using-llms-e79e23d3c5fd&source=-----e79e23d3c5fd---------------------bookmark_footer-----------)![](../Images/e9ebbfb4cc7e7ae96821d7ba97ed5661.png)'
- en: Photo by [Kelly Sikkema](https://unsplash.com/@kellysikkema?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
    on [Unsplash](https://unsplash.com/photos/v9FQR4tbIq8?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由 [Kelly Sikkema](https://unsplash.com/@kellysikkema?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
    提供，发布于 [Unsplash](https://unsplash.com/photos/v9FQR4tbIq8?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
- en: Introduction
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: A Large Language Model (LLM) is a machine learning system that can effectively
    process natural language. The most advanced LLM available at the moment is GPT-4,
    which powers the paid version of ChatGPT. In this article, you will learn how
    to give your app highly flexible speech interpretation using GPT-4 function calling,
    in full synergy with your app’s Graphical User Interface (GUI). It is intended
    for product owners, UX designers, and mobile developers.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLM）是一个可以有效处理自然语言的机器学习系统。目前最先进的LLM是GPT-4，它为付费版ChatGPT提供支持。在这篇文章中，你将学习如何通过GPT-4功能调用，为你的应用程序提供高度灵活的语音解释，与应用程序的图形用户界面（GUI）完全协同。这篇文章旨在为产品负责人、用户体验设计师和移动开发者提供指导。
- en: '![](../Images/dd3be11e362fdfc3a7b672287aec81ad.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/dd3be11e362fdfc3a7b672287aec81ad.png)'
- en: Background
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 背景
- en: Digital assistants on mobile phones (Android and iOS) have failed to catch on
    for several reasons, among which they are faulty, limited, and often tedious to
    use. LLMs, and now especially OpenAI GPT-4, hold the potential to make a difference
    here, with their ability to more deeply grasp the user’s intention instead of
    trying to coarsely pattern match a spoken expression.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 移动电话（Android和iOS）上的数字助手未能普及，有几个原因，其中包括它们有缺陷、功能有限且使用起来往往很麻烦。LLM，特别是OpenAI GPT-4，拥有更深入理解用户意图的潜力，而不是粗略地模式匹配口语表达，从而有可能带来改变。
- en: 'Android has Google Assistant’s ‘app actions’, and iOS has SiriKit intents.
    These provide simple templates to register speech requests that your app can handle.
    Google Assistant and Siri have already improved quite a bit over the past few
    years — even more than you probably realize. Their coverage is greatly determined,
    however, by which apps implement support for them. Nevertheless, you can, for
    instance, play your favorite song on Spotify using speech. The natural language
    interpretation of these OS-provided services, however, predates the huge advances
    in this field that LLMs have brought about — so it is time for the next step:
    to harness the power of LLMs to make speech input more reliable and flexible.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: Android有Google Assistant的“应用操作”，iOS有SiriKit意图。这些提供了简单的模板来注册你的应用可以处理的语音请求。Google
    Assistant和Siri在过去几年中已经有了很大改进——甚至超出你的想象。然而，它们的覆盖范围在很大程度上取决于哪些应用实现了对它们的支持。尽管如此，你仍然可以通过语音在Spotify上播放你喜欢的歌曲。然而，这些操作系统提供的服务的自然语言解释早于LLM在这一领域带来的巨大进步——所以是时候迈出下一步：利用LLM的力量使语音输入更可靠和灵活。
- en: Although we can expect that the operating system services (like Siri and Google
    Assistant) will adapt their strategies soon to take advantage of LLMs, we can
    already enable our apps to use speech without being limited by these services.
    Once you have adopted the concepts in this article, your app will also be ready
    to tap into [new assistants](https://www.axios.com/2023/07/31/google-assistant-artificial-intelligence-news),
    once they become available.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们可以预期操作系统服务（如Siri和Google Assistant）会很快调整策略，以利用LLM，但我们已经可以使我们的应用程序在不受这些服务限制的情况下使用语音。一旦你掌握了本文中的概念，你的应用也将准备好接入[新助手](https://www.axios.com/2023/07/31/google-assistant-artificial-intelligence-news)，一旦它们上线。
- en: The choice of your LLM (GPT, PaLM, LLama2, MPT, Falcon etc.) does have an impact
    on reliability, but the core principles you will learn here can be applied to
    any of them. We will let the user access the entirety of the app’s functionality
    by saying what they want in a single expression. The LLM maps a natural language
    expression into a function call on the navigation structure and functionality
    of our app. And it need not be a sentence spoken like a robot. **LLM’s interpretation
    powers allow users to speak like a human, using their own words or language; hesitate,
    make mistakes, and correct mistakes**. Where users have rejected voice assistants
    because they often fail to understand what they mean, the flexibility of an LLM
    can make the interaction feel much more natural and reliable, leading to higher
    user adoption.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 你选择的LLM（GPT、PaLM、LLama2、MPT、Falcon等）确实会影响可靠性，但你将在这里学到的核心原理可以应用于任何LLM。我们将让用户通过一句话表达他们的需求，从而访问应用程序的全部功能。LLM将自然语言表达映射到我们应用的导航结构和功能上的函数调用上。这不一定要像机器人一样说出一句完整的句子。**LLM的解释能力允许用户像人类一样说话，使用他们自己的词汇或语言；犹豫、犯错并纠正错误**。用户之所以拒绝语音助手，是因为它们经常无法理解他们的意思，而LLM的灵活性可以让交互变得更加自然和可靠，从而提高用户的接受度。
- en: Why speech input in your app, and why now?
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为什么现在在你的应用中使用语音输入？
- en: '**Pros:**'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '**优点：**'
- en: Navigate to a screen and provide all parameters in one speech expression
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过一句语音表达来导航到一个界面并提供所有参数
- en: 'Shallow Learning Curve: No need for the user to find where in your app the
    data is or how to operate the GUI'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 浅层学习曲线：用户无需找到数据在应用中的位置或如何操作 GUI
- en: Hands-free
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 免提
- en: 'Complementary and not unconnected (like in a voice user interface or VUI):
    speech and GUI work in harmony.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 互补而非不相关（如语音用户界面或 VUI）：语音和 GUI 和谐工作。
- en: Accessibility for visually impaired
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 视力障碍的可及性
- en: 'Now: because interpretation of natural language has risen to a new level through
    LLMs, responses are much more reliable'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 现在：由于自然语言的解释通过 LLM 达到了一个新水平，回应更加可靠
- en: '**Cons:**'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '**缺点：**'
- en: Privacy when speaking
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 说话时的隐私
- en: accuracy/misinterpretations
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 准确性/误解
- en: still relatively slow
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 仍然相对较慢
- en: 'Knowledge in the head vs. in the world (What can I say?): the user does not
    know what spoken expressions the system understands and has answers to'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 头脑中的知识与世界中的知识（我能说什么？）：用户不知道系统理解和回答哪些口语表达
- en: Examples of apps that can benefit from speech input include those used for car
    or bicycle driving assistance. In general, users may not want to engage in the
    precision of navigating an app by touch when they cannot easily use their hands,
    for instance, when they are on the move, wearing gloves, or busy working with
    their hands.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 受益于语音输入的应用示例包括用于汽车或自行车驾驶辅助的应用。一般来说，当用户不能轻松使用双手时，例如在移动中、戴着手套或忙于用手工作的情况下，他们可能不愿意通过触摸精确导航应用。
- en: Shopping apps may also benefit from this feature, as users can verbalize their
    desires in their own words rather than navigate through shopping screens and set
    filters.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 购物应用也可以受益于此功能，因为用户可以用自己的话表达需求，而不是通过购物界面和设置过滤器来导航。
- en: When applying this approach to increase accessibility for visually impaired
    individuals, you might consider adding natural language output and text-to-speech
    features to the mix.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 当将这种方法应用于提高视力障碍人士的可及性时，您可能考虑加入自然语言输出和文本转语音功能。
- en: Your app
  id: totrans-33
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 您的应用
- en: The following figure shows the navigation structure of a typical app, exemplified
    by a train trip planner you may be familiar with. At the top, you see the default
    navigation structure for touch navigation. This structure is governed by the Navigation
    Component. All navigation clicks are delegated to the Navigation Component, which
    then executes the navigation action. The bottom depicts how we can tap into this
    structure using speech input.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 下图展示了一个典型应用的导航结构，以您可能熟悉的火车旅行规划器为例。在顶部，您可以看到触摸导航的默认导航结构。该结构由导航组件控制。所有导航点击都委托给导航组件，后者执行导航操作。底部展示了我们如何利用语音输入来接入这一结构。
- en: '![](../Images/20896ca68632952ea9be1a1ee07da5d4.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/20896ca68632952ea9be1a1ee07da5d4.png)'
- en: speech enabling your app using LLM function calling
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 LLM 功能调用来启用您的应用的语音功能
- en: 'The users say what they want; then a speech recognizer transforms the speech
    into text. The system constructs a prompt containing this text and sends it to
    the LLM. The LLM responds to the app with data, telling it which screen to activate
    with which parameters. This data object is turned into a deep link and given to
    the navigation component. The navigation component activates the right screen
    with the right parameters: in this example, the ‘Outings’ screen with ‘Amsterdam’
    as a parameter. Please note that this is a simplification. We will elaborate on
    the details below.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 用户说出他们的需求，然后语音识别器将语音转换为文本。系统构建一个包含这些文本的提示并发送给 LLM。LLM 以数据的形式回应应用，告诉它哪个界面需要激活以及使用哪些参数。这个数据对象被转换为深层链接并提供给导航组件。导航组件用正确的参数激活正确的界面：在这个例子中，就是用‘阿姆斯特丹’作为参数的‘外出’界面。请注意，这只是一个简化版。我们将在下面详细说明。
- en: Many modern apps have a centralized navigation component under the hood. Android
    has Jetpack Navigation, Flutter has the Router, and iOS has NavigationStack. Centralized
    navigation components allow deep linking, which is a technique that allows users
    to navigate directly to a specific screen within a mobile application rather than
    going through the app’s main screen or menu. For the concepts in this article
    to work, a navigation component and centralized deep linking are not necessary,
    but they make implementing the concepts easier.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 许多现代应用程序在底层有一个集中式导航组件。Android 有 Jetpack Navigation，Flutter 有 Router，而 iOS 有
    NavigationStack。集中式导航组件支持深度链接，这是一种技术，允许用户直接导航到移动应用中的特定屏幕，而无需经过应用的主屏幕或菜单。为了使本文中的概念有效，导航组件和集中式深度链接并非必需，但它们使实现这些概念更为简单。
- en: Deep linking involves creating a unique (URI) path that points to a specific
    piece of content or a specific section within an app. Moreover, this path can
    contain parameters that control the states of GUI elements on the screen that
    the deep link points to.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 深度链接涉及创建一个独特的 (URI) 路径，该路径指向应用中的特定内容或特定部分。此外，这个路径可以包含控制屏幕上深度链接所指向的 GUI 元素状态的参数。
- en: Function calling for your app
  id: totrans-40
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 你的应用程序的函数调用
- en: 'We tell the LLM to map a natural language expression to a navigation function
    call through prompt engineering techniques. The prompt reads something like: ‘Given
    the following function templates with parameters, map the following natural language
    question onto one of these function templates and return it’.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过提示工程技术指示 LLM 将自然语言表达映射到导航功能调用。提示的内容类似于：‘给定以下带参数的函数模板，将以下自然语言问题映射到这些函数模板之一并返回’。
- en: Most LLMs are capable of this. LangChain has leveraged it effectively through
    Zero Shot ReAct Agents, and the functions to be called are called [Tools](https://python.langchain.com/docs/modules/agents/tools/).
    OpenAI has fine-tuned their GPT-3.5 and GPT-4 models with special versions (currently
    gpt-3.5-turbo-0613 and gpt-4–0613) that are very good at this, and they have made
    specific API entries for this purpose. In this article, we will take the OpenAI
    notation, but the concepts can be applied to any LLM, e.g. using the ReAct mechanism
    mentioned. Moreover, LangChain has a specific agent type (AgentType.OPENAI_FUNCTIONS)
    that translates Tools into OpenAI function templates under the hood. For LLama2
    you will be able to use [llama-api](https://www.llama-api.com/) with the same
    syntax as OpenAI.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数 LLM 都能做到这一点。LangChain 通过 Zero Shot ReAct Agents 有效地利用了这一点，待调用的函数称为 [Tools](https://python.langchain.com/docs/modules/agents/tools/)。OpenAI
    已经用特别版本（当前为 gpt-3.5-turbo-0613 和 gpt-4–0613）对其 GPT-3.5 和 GPT-4 模型进行了微调，非常擅长此任务，并为此目的设置了特定的
    API 条目。本文将采用 OpenAI 的符号表示，但这些概念可以应用于任何 LLM，例如使用提到的 ReAct 机制。此外，LangChain 有一个特定的代理类型
    (AgentType.OPENAI_FUNCTIONS)，在幕后将 Tools 转换为 OpenAI 函数模板。对于 LLama2，你将能够使用 [llama-api](https://www.llama-api.com/)
    并使用与 OpenAI 相同的语法。
- en: 'Function calling for LLMs works as follows:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: LLM 的函数调用工作如下：
- en: You insert a JSON schema of function templates into your prompt along with the
    user’s natural language expression as a user message.
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你将函数模板的 JSON 架构与用户的自然语言表达作为用户消息一起插入到提示中。
- en: The LLM attempts to map the user’s natural language expression onto one of these
    templates.
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: LLM 尝试将用户的自然语言表达映射到这些模板之一。
- en: The LLM returns the resulting JSON object so your code can make a function call.
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: LLM 返回结果 JSON 对象，以便你的代码可以进行函数调用。
- en: In this article, the function definitions are direct mappings of the graphical
    user interface (GUI) of a (mobile) app, where each function corresponds to a screen
    and each parameter to a GUI element on that screen. A natural language expression
    sent to the LLM returns a JSON object containing a function name and its parameters
    that you can use to navigate to the right screen and trigger the right function
    in your view model, such that the right data is fetched. The values of the relevant
    GUI elements on that screen are set according to the parameters.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，函数定义是 (移动) 应用程序图形用户界面 (GUI) 的直接映射，其中每个函数对应于一个屏幕，每个参数对应于该屏幕上的一个 GUI 元素。发送到
    LLM 的自然语言表达返回一个包含函数名称及其参数的 JSON 对象，你可以用来导航到正确的屏幕并在视图模型中触发正确的函数，以便获取正确的数据。该屏幕上相关
    GUI 元素的值根据参数进行设置。
- en: 'This is illustrated in the following figure:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 这在下图中进行了说明：
- en: '![](../Images/b8ae7bc16f62cd81d969e202d45f53c9.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b8ae7bc16f62cd81d969e202d45f53c9.png)'
- en: mapping LLM functions onto your mobile app’s GUI
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 将 LLM 功能映射到你的移动应用程序的 GUI
- en: 'It shows a stripped version of the function templates as added to the prompt
    for the LLM. To see the full-length prompt for the user message: ‘What things
    can I do in Amsterdam?’, [click here (Github Gist](https://gist.github.com/hansvdam/8b9269390e16fa0bf394d7656bec1ea5)).
    It contains a full curl request that you can use from the command line or import
    into Postman. You need to put your own O[penAI-key](https://platform.openai.com/account/api-keys)
    in the placeholder to run it.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 它展示了添加到LLM提示中的函数模板的精简版本。要查看用户消息‘我在阿姆斯特丹可以做些什么？’的完整提示，[点击这里 (Github Gist)](https://gist.github.com/hansvdam/8b9269390e16fa0bf394d7656bec1ea5)。它包含了你可以从命令行使用或导入到Postman中的完整curl请求。你需要将你自己的O[penAI-key](https://platform.openai.com/account/api-keys)放入占位符中以运行它。
- en: Screens without parameters
  id: totrans-52
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 没有参数的屏幕
- en: 'Some screens in your app don’t have any parameters, or at least not the ones
    that the LLM needs to be aware of. To reduce token usage and clutter, we can combine
    a number of these screen triggers in a single function with one parameter: the
    screen to open.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 你应用中的一些屏幕没有任何参数，或者至少没有LLM需要了解的参数。为了减少令牌使用和杂乱，我们可以将这些屏幕触发器合并到一个单一的函数中，并使用一个参数：要打开的屏幕。
- en: '[PRE0]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The Criterion as to whether a triggering function needs parameters is whether
    the user has a choice: there is some form of search or navigation going on on
    the screen, i.e. are there any search (like) fields or tabs to choose from?'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 判断触发函数是否需要参数的标准是用户是否有选择：屏幕上是否进行某种形式的搜索或导航，即是否有可以选择的搜索（类似）字段或标签？
- en: If not, then the LLM does not need to know about it, and screen triggering may
    be added to the generic screen triggering function of your app. It is mostly a
    matter of experimentation with the descriptions of the screen purpose. If you
    need a longer description, consider giving it its own function definition to put
    more separate emphasis on its description than the enum of the generic parameter
    does.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 如果没有，那么LLM不需要知道这些信息，并且屏幕触发可以添加到你应用的通用屏幕触发函数中。这主要是一个关于屏幕目的描述的实验问题。如果你需要更长的描述，考虑给它一个自己的函数定义，以便比通用参数的枚举更分开地强调它的描述。
- en: 'Prompt instruction guidance and repair:'
  id: totrans-57
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 提示指令指导和修复：
- en: 'In the system message of your prompt, you give generic steering information.
    In our example, it can be important for the LLM to know what date and time it
    is now, for instance, if you want to plan a trip for tomorrow. Another important
    thing is to steer its presumptiveness. Often, we would rather have the LLM be
    overconfident than bother the user with its uncertainty. A good system message
    for our example app is:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在你提示的系统消息中，你提供了一般性的引导信息。在我们的示例中，了解当前的日期和时间可能很重要，例如，如果你想为明天计划一个旅行。另一个重要的方面是引导其假设性。我们通常更希望LLM表现得过于自信，而不是因为不确定性而打扰用户。对于我们的示例应用，一个好的系统消息是：
- en: '[PRE1]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Function parameter descriptions can require quite a bit of tuning. An example
    is the trip_date_time when planning a train trip. A reasonable parameter description
    is:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 函数参数描述可能需要相当多的调整。例如，在计划火车旅行时，`trip_date_time`就是一个例子。一个合理的参数描述是：
- en: '[PRE2]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'So if it is now 15:00 and users say they want to leave at 8, they mean 20:00
    unless they specifically mention the time of the day. The above instruction works
    reasonably well for GPT-4\. But in some edge cases, it still fails. We can then
    e.g. add extra parameters to the function template that we can use to make further
    repairs in our own code. For instance, we can add:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 所以如果现在是15:00，而用户说他们想在8点离开，他们实际上指的是20:00，除非他们特别提到一天中的时间。上述指令对GPT-4的效果相当好。但在某些极端情况下，它仍然会失败。我们可以例如添加额外的参数到函数模板中，以便在我们自己的代码中进行进一步修正。例如，我们可以添加：
- en: '[PRE3]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: In your app, you will likely find parameters that require post-processing to
    enhance their success ratio.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在你的应用中，你可能会发现一些参数需要后处理以提高其成功率。
- en: System requests for clarification
  id: totrans-65
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 系统请求澄清
- en: Sometimes, the user’s request lacks information to proceed. There may not be
    a function suitable to handle the user’s request. In that case, the LLM will respond
    in natural language that you can show to the user, e.g. by means of a Toast.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，用户的请求缺乏继续处理所需的信息。可能没有适合处理用户请求的函数。在这种情况下，LLM会用自然语言响应，你可以通过例如Toast的方式展示给用户。
- en: It may also be the case that the LLM does recognize a potential function to
    call, but information is lacking to fill all required function parameters. In
    that case, consider making parameters optional. But if that is not possible, the
    LLM may send a request in natural language for the missing parameters in the language
    of the user. You should show this text to the users, e.g. through a Toast or text-to-speech,
    so they can give the missing information (in speech). For instance, when the user
    says ‘I want to go to Amsterdam’ (and your app has not provided a default or current
    location through the system message) the LLM might respond with ‘I understand
    you want to make a train trip, from where do you want to depart?’.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 也可能存在这样一种情况，即大型语言模型（LLM）确实识别出了一个潜在的函数调用，但缺乏填充所有必需函数参数的信息。在这种情况下，可以考虑将参数设置为可选。但如果这不可行，LLM可能会用用户的语言发送自然语言请求，询问缺失的参数。你应该将这段文本展示给用户，例如通过吐司提示或文本转语音，让他们提供缺失的信息（通过语音）。例如，当用户说“我想去阿姆斯特丹”（而你的应用没有通过系统消息提供默认或当前位置）时，LLM可能会回应“我知道你想要乘火车旅行，你想从哪里出发？”。
- en: This brings up the issue of conversational history. I recommend you always include
    the last 4 messages from the user in the prompt so a request for information can
    be spread over multiple turns. To simplify things, omit the system’s responses
    from the history because, in this use case, they tend to do more harm than good.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 这提出了对话历史的问题。我建议你始终在提示中包含用户的最后4条消息，以便信息请求可以分多次进行。为了简化起见，可以省略系统的响应，因为在这种用例中，它们往往弊大于利。
- en: Speech recognition
  id: totrans-69
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 语音识别
- en: Speech recognition is a crucial part of the transformation from speech to a
    parametrized navigation action in the app. When the quality of interpretation
    is high, bad speech recognition may very well be the weakest link. Mobile phones
    have onboard speech recognition with reasonable quality, but LLM-based speech
    recognition like [Whisper](https://openai.com/research/whisper), Google [Chirp/USM](https://cloud.google.com/speech-to-text/v2/docs/chirp-model),
    Meta [MMS](https://ai.meta.com/blog/multilingual-model-speech-recognition/?utm_source=twitter&utm_medium=organic_social&utm_campaign=blog&utm_content=card),
    or [DeepGram](https://developers.deepgram.com/reference/streaming) tends to lead
    to better results, especially when you can [tune them](https://cloud.google.com/speech-to-text/docs/adaptation-model)
    for your use case.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 语音识别是将语音转换为应用中的参数化导航动作的关键部分。当解释的质量较高时，语音识别的质量较差可能会成为最薄弱的环节。手机具有合理质量的内置语音识别，但基于LLM的语音识别如[Whisper](https://openai.com/research/whisper)、谷歌的[Chirp/USM](https://cloud.google.com/speech-to-text/v2/docs/chirp-model)、Meta的[MMS](https://ai.meta.com/blog/multilingual-model-speech-recognition/?utm_source=twitter&utm_medium=organic_social&utm_campaign=blog&utm_content=card)或[DeepGram](https://developers.deepgram.com/reference/streaming)往往会取得更好的结果，特别是当你可以为你的用例[调整](https://cloud.google.com/speech-to-text/docs/adaptation-model)它们时。
- en: Architecture
  id: totrans-71
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 架构
- en: 'It is probably best to store the function definitions on the server, but they
    can also be managed by the app and sent with every request. Both have their pros
    and cons. Having them sent with every request is more flexible, and the alignment
    of functions and screens may be easier to maintain. However, the function templates
    not only contain the function name and parameters but also their descriptions
    that we might want to update quicker than the update flow in the app stores. These
    descriptions are more or less LLM-dependent and crafted for what works. It is
    not unlikely that you want to swap out the LLM for a better or cheaper one or
    even swap dynamically at some point. Having the function templates on the server
    may also have the advantage of maintaining them in one place if your app is native
    on iOS and Android. If you use OpenAI services for both speech recognition and
    natural language processing, the technical big picture of the flow looks as follows:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 最好将函数定义存储在服务器上，但它们也可以由应用管理，并随每个请求发送。这两种方式各有利弊。随每个请求发送函数定义更灵活，函数和界面的对齐也可能更容易维护。然而，函数模板不仅包含函数名称和参数，还包含我们可能希望比应用商店更新流程更快更新的描述。这些描述或多或少依赖于LLM，并且根据实际效果进行设计。你可能会想要用更好的或更便宜的LLM来替换当前的LLM，或者甚至在某些时候动态切换。将函数模板存储在服务器上也可能有一个好处，即如果你的应用在iOS和Android上都是原生的，那么可以在一个地方维护它们。如果你同时使用OpenAI的服务进行语音识别和自然语言处理，那么整个流程的技术大图如下：
- en: '![](../Images/ca4debb6ef8e0f616fcb21be2a997df9.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ca4debb6ef8e0f616fcb21be2a997df9.png)'
- en: architecture for speech enabling your mobile app using Whisper and OpenAI function
    calling
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 Whisper 和 OpenAI 函数调用为你的移动应用启用语音的架构
- en: The users speak their request; it is recorded into an m4a buffer/file (or mp3
    if you like), which is sent to your server, which relays it to [Whisper](https://openai.com/blog/introducing-chatgpt-and-whisper-apis).
    Whisper responds with the transcription, and your server combines it with your
    system message and function templates into a prompt for the LLM. Your server receives
    back the raw function call JSON, which it then processes into a function call
    JSON object for your app.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 用户说出他们的请求；它被录制到 m4a 缓冲区/文件（如果你愿意，也可以是 mp3），然后发送到你的服务器，服务器将其转发到 [Whisper](https://openai.com/blog/introducing-chatgpt-and-whisper-apis)。Whisper
    响应转录内容，你的服务器将其与系统消息和函数模板结合成 LLM 的提示。你的服务器收到原始函数调用 JSON，然后将其处理成应用程序所需的函数调用 JSON
    对象。
- en: From function call to deep link
  id: totrans-76
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从函数调用到深度链接
- en: 'To illustrate how a function call translates into a deep link, we take the
    function call response from the initial example:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明函数调用如何转换为深度链接，我们取初始示例中的函数调用响应：
- en: '[PRE4]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'On different platforms, this is handled quite differently, and over time, many
    different navigation mechanisms have been used and are often still in use. It
    is beyond the scope of this article to go into implementation details, but roughly
    speaking, the platforms in their most recent incarnation can employ deep linking
    as follows:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在不同的平台上，这一过程处理得相当不同，并且随着时间的推移，使用了许多不同的导航机制，并且这些机制仍然在使用中。详细的实现细节超出了本文的范围，但大致来说，这些平台在其最新版本中可以采用如下的深度链接：
- en: 'On Android:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Android 上：
- en: '[PRE5]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'On Flutter:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Flutter 上：
- en: '[PRE6]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'On iOS, things are a little less standardized, but using NavigationStack:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在 iOS 上，事情有些不够标准化，但使用 NavigationStack：
- en: '[PRE7]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'And then issuing:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 然后发出：
- en: '[PRE8]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'More on deep linking can be found here: [for Android](https://blog.appcircle.io/article/jetpack-compose-navigation-deeplink),
    [for Flutter](https://docs.flutter.dev/cookbook/navigation/navigate-with-arguments),
    [for iOS](https://www.swiftyplace.com/blog/better-navigation-in-swiftui-with-navigation-stack)'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 更多关于深度链接的信息可以在这里找到：[Android](https://blog.appcircle.io/article/jetpack-compose-navigation-deeplink)，[Flutter](https://docs.flutter.dev/cookbook/navigation/navigate-with-arguments)，[iOS](https://www.swiftyplace.com/blog/better-navigation-in-swiftui-with-navigation-stack)
- en: Free text field for apps
  id: totrans-89
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 应用程序的自由文本字段
- en: 'There are two modes of free text input: voice and typing. We’ve mainly talked
    about speech, but a text field for typing input is also an option. Natural language
    is usually quite lengthy, so it may be difficult to compete with GUI interaction.
    However, GPT-4 tends to be quite good at guessing parameters from abbreviations,
    so even very short abbreviated typing can often be interpreted correctly.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 有两种自由文本输入模式：语音和打字。我们主要讨论了语音，但打字输入的文本字段也是一个选项。自然语言通常相当冗长，因此可能很难与 GUI 交互竞争。然而，GPT-4
    通常很擅长从缩写中猜测参数，因此即使是非常简短的缩写打字也常常能被正确解释。
- en: The use of functions with parameters in the prompt often dramatically narrows
    the interpretation context for an LLM. Therefore, it needs very little, and even
    less if you instruct it to be presumptive. This is a new phenomenon that holds
    promise for mobile interaction. In the case of the train station to train station
    planner, the LLM made the following interpretations when used with the exemplary
    prompt structure in this article. You can try it out for yourself using the [prompt
    gist](https://gist.github.com/hansvdam/8b9269390e16fa0bf394d7656bec1ea5) mentioned
    above.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 在提示中使用带有参数的函数通常会大大缩小 LLM 的解释上下文。因此，它需要非常少的内容，如果你指示它进行假设则更少。这是一个新的现象，对移动交互具有很大的潜力。在车站到车站规划器的案例中，LLM
    在使用本文中示例提示结构时做出了以下解释。你可以使用上述的 [prompt gist](https://gist.github.com/hansvdam/8b9269390e16fa0bf394d7656bec1ea5)
    亲自尝试。
- en: '**Examples:**'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '**示例：**'
- en: '‘ams utr’: show me a list of train itineraries from Amsterdam Central Station
    to Utrecht Central Station departing from now'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: ‘ams utr’：给我显示从阿姆斯特丹中央车站到乌特勒支中央车站的列车时刻表，从现在起出发。
- en: '‘utr ams arr 9’: (Given that it is 13:00 at the moment). Show me a list of
    train itineraries from Utrecht Central Station to Amsterdam Central Station, arriving
    before 21:00'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: ‘utr ams arr 9’：（假设现在是13:00）。给我显示从乌特勒支中央车站到阿姆斯特丹中央车站的列车时刻表，要求到达时间在21:00之前。
- en: '**Follow up interaction**'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '**后续交互**'
- en: 'Just like in ChatGPT, you can refine your query if you send a short piece of
    the interaction history along:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 就像在 ChatGPT 中一样，你可以通过发送一小段交互历史来细化你的查询：
- en: 'Using the history feature, the following also works very well (presume it is
    9:00 in the morning now):'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 使用历史记录功能，以下内容也非常有效（假设现在是早上9:00）：
- en: 'Type: ‘ams utr’ and get the answer as above. Then type ‘arr 7’ in the next
    turn. And yes, it can actually translate that into a trip being planned from Amsterdam
    Central to Utrecht Central, arriving before 19:00.'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 输入：‘ams utr’ 并获得上述答案。然后在下一轮输入‘arr 7’。是的，它实际上可以将其翻译为从阿姆斯特丹中央到乌特勒支中央的旅行，预计在19:00之前到达。
- en: I made an example web app about this that you can find a video about [here](https://www.youtube.com/watch?v=XBjiqLD578I).
    The link to the actual app is in the description.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 我制作了一个关于此的示例网页应用程序，你可以在[这里](https://www.youtube.com/watch?v=XBjiqLD578I)找到相关视频。实际应用程序的链接在描述中。
- en: 'Update: a successor to this article incorporating text input can be found [here](https://medium.com/towards-data-science/synergy-of-llm-and-gui-beyond-the-chatbot-c8b0e08c6801)
    and a demo video [here](https://youtu.be/vJy0HI_mH7w).'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 更新：可以在[这里](https://medium.com/towards-data-science/synergy-of-llm-and-gui-beyond-the-chatbot-c8b0e08c6801)找到这篇文章的继任者，其中包含文本输入，演示视频请见[这里](https://youtu.be/vJy0HI_mH7w)。
- en: Future
  id: totrans-101
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 未来
- en: You can expect this deep link structure to handle functions within your app
    to become an integral part of your phone’s OS (Android or iOS). A global assistant
    on the phone will handle speech requests, and apps can expose their functions
    to the OS so that they can be triggered in a deep-linking fashion. This parallels
    how plugins are made available for ChatGPT. Now a coarse form of this is already
    available through the intents in the AndroidManifest and [App Actions](https://developers.google.com/assistant/app)
    on Android and on iOS through [SiriKit intents](https://medium.com/simform-engineering/how-to-integrate-siri-shortcuts-and-design-custom-intents-tutorial-e53285b550cf).
    The amount of control you have over these is limited, and the user has to speak
    like a robot to activate them reliably. Undoubtedly, this will improve over time
    when [LLM-powered assistants](https://www.axios.com/2023/07/31/google-assistant-artificial-intelligence-news)
    take over.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以期待这种深度链接结构处理应用内功能，成为你手机操作系统（Android 或 iOS）的一个重要组成部分。手机上的全球助手将处理语音请求，应用程序可以将其功能暴露给操作系统，以便以深度链接的方式触发。这与
    ChatGPT 插件的可用性类似。目前，通过 AndroidManifest 中的意图和 [App Actions](https://developers.google.com/assistant/app)
    以及 iOS 上的 [SiriKit intents](https://medium.com/simform-engineering/how-to-integrate-siri-shortcuts-and-design-custom-intents-tutorial-e53285b550cf)
    已经可以粗略实现。你对这些功能的控制有限，用户需要像机器人一样说话才能可靠地激活它们。毫无疑问，当[LLM 驱动的助手](https://www.axios.com/2023/07/31/google-assistant-artificial-intelligence-news)接管时，这种情况会随着时间的推移而改善。
- en: VR and AR (XR) offer great opportunities for speech recognition because the
    user's hands are often engaged in other activities.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: VR 和 AR（XR）为语音识别提供了极好的机会，因为用户的双手通常参与其他活动。
- en: It will probably not take long before anyone can run their own high-quality
    LLM. The cost will decrease, and speed will increase rapidly over the next year.
    Soon LoRA LLMs will become available on smartphones, so inference can take place
    on your phone, reducing cost and speed. Also, more and more competition will come,
    both open source like [Llama2](https://ai.meta.com/llama/), and closed source
    like [PaLM](https://developers.generativeai.google/products/palm).
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 可能很快任何人都能运行自己的高质量 LLM。成本将降低，速度在接下来的一年里将迅速增加。LoRA LLMs 很快将出现在智能手机上，这样推理可以在你的手机上进行，从而降低成本并提高速度。此外，竞争也会越来越激烈，包括像
    [Llama2](https://ai.meta.com/llama/) 这样的开源项目，以及像 [PaLM](https://developers.generativeai.google/products/palm)
    这样的闭源项目。
- en: 'Finally, the synergy of modalities can be driven further than providing random
    access to the GUI of your entire app. It is the power of LLMs to combine multiple
    sources that hold the promise for better assistance to emerge. Some interesting
    articles: [multimodal dialog](https://masterofcode.com/blog/multimodal-conversation-design-tutorial-part-2-best-practices-use-cases-and-future-outlook),
    [google blog on GUIs and LLMs](https://ai.googleblog.com/2023/05/enabling-conversational-interaction-on.html),
    [interpreting GUI interaction as language](https://pure.tue.nl/ws/files/3655721/200612175.pdf),
    [LLM Powered Assistants](https://nickarner.com/notes/llm-powered-assistants-for-complex-interfaces-february-26-2023/).'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，模态的协同效应可以超越提供对整个应用程序 GUI 的随机访问。LLM（大语言模型）结合多个来源的能力预示着更好的帮助将会出现。一些有趣的文章：[多模态对话](https://masterofcode.com/blog/multimodal-conversation-design-tutorial-part-2-best-practices-use-cases-and-future-outlook)，[谷歌关于
    GUI 和 LLM 的博客](https://ai.googleblog.com/2023/05/enabling-conversational-interaction-on.html)，[将
    GUI 交互解释为语言](https://pure.tue.nl/ws/files/3655721/200612175.pdf)，[LLM 驱动的助手](https://nickarner.com/notes/llm-powered-assistants-for-complex-interfaces-february-26-2023/)。
- en: Conclusion
  id: totrans-106
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: In this article, you learned how to apply function calling to speech enable
    your app. Using the provided [Gist](https://gist.github.com/hansvdam/8b9269390e16fa0bf394d7656bec1ea5)
    as a point of departure, you can experiment in Postman or from the command line
    to get an idea of how powerful function calling is. If you want to run a POC on
    speech enabling your app, I would recommend putting the server bit from the architecture
    section directly into your app. It all boils down to 2 HTTP calls, some prompt
    construction, and implementing microphone recording. Depending on your skill and
    codebase, you will have your POC up and running in several days.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，你学会了如何应用函数调用来为你的应用程序启用语音功能。以提供的 [Gist](https://gist.github.com/hansvdam/8b9269390e16fa0bf394d7656bec1ea5)
    为出发点，你可以在 Postman 或命令行中进行实验，了解函数调用的强大功能。如果你想在你的应用上运行一个语音启用的POC（概念验证），我建议将架构部分的服务器代码直接集成到你的应用中。整体来说，这归结为
    2 次 HTTP 调用，一些提示构建和实现麦克风录音。根据你的技能和代码库，你将在几天内完成 POC 的搭建。
- en: Happy coding!
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 编程愉快！
- en: Follow me on [LinkedIn](https://www.linkedin.com/in/hans-van-dam-71a7866/) or
    [UXX.AI](https://uxx.ai)
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [LinkedIn](https://www.linkedin.com/in/hans-van-dam-71a7866/) 或 [UXX.AI](https://uxx.ai)
    上关注我
- en: '*All images in this article, unless otherwise noted, are by the author.*'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '*除非另有说明，本文中的所有图片均由作者提供。*'
