- en: 'Tree Ensembles: Bagging, Boosting and Gradient Boosting'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 树集成：自助法、提升法和梯度提升
- en: 原文：[https://towardsdatascience.com/tree-ensembles-theory-and-practice-1cf9eb27781](https://towardsdatascience.com/tree-ensembles-theory-and-practice-1cf9eb27781)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/tree-ensembles-theory-and-practice-1cf9eb27781](https://towardsdatascience.com/tree-ensembles-theory-and-practice-1cf9eb27781)
- en: Theory and practice explained in detail
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理论和实践详尽解释
- en: '[](https://jorgemartinlasaosa.medium.com/?source=post_page-----1cf9eb27781--------------------------------)[![Jorge
    Martín Lasaosa](../Images/21b4e500b7d14204ea76f579c3e2433f.png)](https://jorgemartinlasaosa.medium.com/?source=post_page-----1cf9eb27781--------------------------------)[](https://towardsdatascience.com/?source=post_page-----1cf9eb27781--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----1cf9eb27781--------------------------------)
    [Jorge Martín Lasaosa](https://jorgemartinlasaosa.medium.com/?source=post_page-----1cf9eb27781--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://jorgemartinlasaosa.medium.com/?source=post_page-----1cf9eb27781--------------------------------)[![Jorge
    Martín Lasaosa](../Images/21b4e500b7d14204ea76f579c3e2433f.png)](https://jorgemartinlasaosa.medium.com/?source=post_page-----1cf9eb27781--------------------------------)[](https://towardsdatascience.com/?source=post_page-----1cf9eb27781--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----1cf9eb27781--------------------------------)
    [Jorge Martín Lasaosa](https://jorgemartinlasaosa.medium.com/?source=post_page-----1cf9eb27781--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----1cf9eb27781--------------------------------)
    ·10 min read·Jan 26, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----1cf9eb27781--------------------------------)
    ·10分钟阅读·2023年1月26日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/ab39fad56deee0716f620ca35605c141.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ab39fad56deee0716f620ca35605c141.png)'
- en: Photo by [Arnaud Mesureur](https://unsplash.com/@tbzr?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 照片由 [Arnaud Mesureur](https://unsplash.com/@tbzr?utm_source=medium&utm_medium=referral)
    提供，来源于 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: 'A **tree ensemble** is a machine learning technique for supervised learning
    that consists of a set of individually trained decision trees defined as *weak*
    or *base* learners, that may not perform well individually. The aggregation of
    the *weak* learners produce a new *strong* model, which is often more accurate
    than the former ones. There are three main types of ensemble learning methods:
    **bagging**, **boosting,** and **gradient boosting**. Every method can be used
    with other *weak* learners, but in this post, only trees are going to be taken
    into account.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '**树集成** 是一种监督学习的机器学习技术，由一组单独训练的决策树组成，这些决策树被定义为*弱*或*基础*学习器，单独表现可能不佳。将这些*弱*学习器聚合成一个新的*强*模型，通常比之前的模型更准确。集成学习方法主要有三种类型：**自助法**、**提升法**
    和 **梯度提升**。每种方法都可以与其他*弱*学习器结合使用，但在这篇文章中，仅考虑决策树。'
- en: 'The rest of the article is divided into two sections:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 文章的其余部分分为两个部分：
- en: '**Intuition & History.** It explains the origin and gives a short description
    of each ensemble learning method.'
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**直觉与历史。** 解释了每种集成学习方法的起源，并对其进行了简要描述。'
- en: '**Practical Demonstration.** Develops every ensemble learning method step by
    step. For this purpose, a small synthetic dataset is also presented to help with
    the explanations.'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**实际演示。** 逐步展开每种集成学习方法。为此，还提供了一个小的合成数据集，以帮助解释。'
- en: All images unless otherwise noted are by the author.
  id: totrans-12
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 除非另有说明，否则所有图像均由作者提供。
- en: '**Intuition & History**'
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**直觉与历史**'
- en: Bagging
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 自助法
- en: The term was first defined by Breiman (1996) [1], and it is an acronym for **B**ootstrap
    **Agg**regation. For this ensemble, each decision tree uses as input data a bootstrap
    sample of the training dataset. A bootstrap sample is a randomly selected sample
    with replacement, which means that observations can appear once, more than once,
    or never. Then, all the predictions are combined using a statistic such as the
    average. *Random Forest* [2] uses this technique (among others) and is one of
    the most successful and widely used ensemble methods.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 该术语首次由 Breiman (1996) [1] 定义，是 **B**ootstrap **Agg**regation 的缩写。对于这种集成，每个决策树都使用训练数据集的自助样本作为输入数据。自助样本是随机选择的样本，允许重复选择，这意味着观察值可能出现一次、多次或从不出现。然后，所有预测都使用统计量（如平均值）进行组合。*随机森林*
    [2] 使用了这种技术（以及其他技术），并且是最成功和广泛使用的集成方法之一。
- en: '![](../Images/2965a6381d0e0c93c62fec49c6a33635.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2965a6381d0e0c93c62fec49c6a33635.png)'
- en: Bagging visual explanation. Each sample with replacement serves as input data
    for the weak learner. [3]
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: Bagging可视化解释。每个带替换的样本作为弱学习器的输入数据。[3]
- en: '**Boosting**'
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**提升**'
- en: 'Keans and Valiant (1988, 1989) [4][5] left open the following question: *Can
    a set of weak learners create a single strong learner?* In 1990, Schapire‘s affirmative
    answer [6] led to the development of boosting. Boosting, as opposed to bagging,
    takes a more iterative approach in which trees are trained sequentially. Observations
    have assigned a weight, and each tree is built in an additive manner, assigning
    greater weights (more importance) to misclassified observations in the previous
    learners. There are many boosting algorithms, but the first one to take full advantage
    of *weak* learners was *AdaBoost* [7], formulated by Freund and Schapire in 1995.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 'Keans和Valiant (1988, 1989) [4][5] 提出了以下问题: *一组弱学习器能否创建一个强学习器？* 1990年，Schapire的肯定回答[6]
    导致了提升算法的发展。与bagging不同，提升采取了更迭代的方法，其中树木按顺序训练。观察被赋予权重，每棵树都是以加法的方式构建的，为前一学习器中的分类错误观察分配更大的权重（更重要）。有许多提升算法，但第一个充分利用*弱*学习器的是*AdaBoost*
    [7]，由Freund和Schapire于1995年提出。'
- en: '![](../Images/f5b7de4b3ffa9d305b5b18dbf5a7fce6.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f5b7de4b3ffa9d305b5b18dbf5a7fce6.png)'
- en: AdaBoost visual explanation. Each learning is focused on misclassified observations
    of the previous tree [8]
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: AdaBoost可视化解释。每次学习都集中在前一棵树的分类错误观察上。[8]
- en: Gradient Boosting
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 梯度提升
- en: Breiman proposed a “Gradient Boosting Machine” [9] that also takes an iterative
    approach in which trees are trained sequentially. However, instead of having weights
    to update, the trees are fitting the *pseudo residuals* of the previous tree.
    The first *pseudo residuals* are the result of subtracting the real value from
    the average of the output feature. Many algorithms such as *XGBoost* [10], *CatBoost*
    [11] or *LightGBM* [12] are based on this technique.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: Breiman提出了一种“梯度提升机器”[9]，也采用了迭代方法，其中树木按顺序训练。然而，不同于更新权重，树木适应的是前一棵树的*伪残差*。第一组*伪残差*是通过从输出特征的平均值中减去真实值得到的。许多算法，如*XGBoost*
    [10]、*CatBoost* [11]或*LightGBM* [12]，都基于这一技术。
- en: '![](../Images/637a9996edfdb9386d8f9079846291e0.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/637a9996edfdb9386d8f9079846291e0.png)'
- en: Gradient Boosting Visual Explanation [13]
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度提升可视化解释 [13]
- en: Practical Demonstration
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实际演示
- en: The practical demonstration is highly inspired by [Josh Starmer’s Youtube channel](https://www.youtube.com/channel/UCtYLUTtgS3k1Fg4y5tAhLbw).
    If you want a visual and extremely concise explanation of almost any machine learning
    model or technique, just check it out!
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 实际演示深受[Josh Starmer的YouTube频道](https://www.youtube.com/channel/UCtYLUTtgS3k1Fg4y5tAhLbw)的启发。如果你想要几乎所有机器学习模型或技术的视觉和极其简洁的解释，可以去看看！
- en: 'Disclaimer: the data have been tailor-made so that the results show what is
    expected. This does not mean that the techniques are infallible or that they can
    be compared with each other.'
  id: totrans-28
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 免责声明：数据是量身定制的，以便结果符合预期。这并不意味着这些技术是万无一失的或可以相互比较。
- en: Data
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据
- en: When I learn new algorithms or methods, I really like to test them in small
    datasets where I can focus on the details. So for the practical demonstration,
    we will use a small synthetic dataset containing made-up information about houses
    and their *price*. Bagging and boosting models are going to be explained with
    the price feature transformed into a categorical feature, while gradient boosting
    is going to be explained with the price as a numerical feature.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 当我学习新算法或方法时，我非常喜欢在小数据集上进行测试，以便我能专注于细节。因此，在实际演示中，我们将使用一个包含关于房屋及其*价格*的虚构信息的小型合成数据集。Bagging和boosting模型将用价格特征转换为分类特征来解释，而梯度提升则用价格作为数值特征来解释。
- en: All the Jupyter Notebooks developed for the article can be found in this [GitHub
    Repository](https://github.com/jomartla/medium_articles/tree/main/TreeEnsembles),
    but the main pieces of code are still shown along the reading. For example, the
    data creation process can be seen below.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 为本文开发的所有Jupyter笔记本可以在这个[GitHub代码库](https://github.com/jomartla/medium_articles/tree/main/TreeEnsembles)中找到，但主要代码片段仍会在阅读过程中展示。例如，数据创建过程可以在下面看到。
- en: 'Code used for creating the data. It can be found also in the repository: TreeEnsembles/synthetic_data.py.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 创建数据的代码。也可以在代码库中找到：TreeEnsembles/synthetic_data.py。
- en: 'The data created have 10 customers and 6 features:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 创建的数据包含10个客户和6个特征：
- en: '**Square Meters**: Numerical'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**平方米**: 数值型'
- en: '**If the house has a garage**: Boolean'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**如果房子有车库**: 布尔值'
- en: '**If the house has a garden**: Boolean'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**如果房子有花园**: 布尔值'
- en: '**Number of rooms**: Numerical'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**房间数量**：数值型'
- en: '**Price**: Categorical or Numerical'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**价格**：分类或数值型'
- en: As it is said before, the price feature will be categorical or numerical depending
    on the tree ensemble explained.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，价格特征将根据解释的树集合体是分类的还是数值的。
- en: '![](../Images/9f25d714245c006fab798fdc72d190e6.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9f25d714245c006fab798fdc72d190e6.png)'
- en: DataFrame used in the practical demonstration of bagging and boosting. The price
    feature is a categorical feature.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在袋装法和提升法的实际演示中使用的DataFrame。价格特征是一个分类特征。
- en: '![](../Images/afeeb23b6ca3a1ef67c9b8bcad127de3.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/afeeb23b6ca3a1ef67c9b8bcad127de3.png)'
- en: DataFrame used in the practical demonstration of gradient boosting. The price
    feature is a numerical feature.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在梯度提升法的实际演示中使用的DataFrame。价格特征是一个数值特征。
- en: Bagging
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 袋装法
- en: 'First, remember that bagging is an acronym for **B**ootstrap **Agg**regation.
    These two words lead the way, so let’s start with the first one: **bootstrap**.
    The following code builds 5bootstrap samples of 7 observations from the data generated
    in the previous section.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，请记住，袋装法是**B**ootstrap **Agg**regation的缩写。这两个词引领了方向，所以我们从第一个词开始：**bootstrap**。以下代码从前一节生成的数据中构建了5个具有7个观测值的自举样本。
- en: 'Code used for creating the samples. The code can be found also in the repository:
    TreeEnsembles/Bagging.ipynb.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 创建样本的代码。代码也可以在仓库中找到：TreeEnsembles/Bagging.ipynb。
- en: '![](../Images/1802cb50bdbacecd5ae00130ceb41211.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1802cb50bdbacecd5ae00130ceb41211.png)'
- en: Each sample contains exactly seven indexes. Repeated indexes are allowed.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 每个样本包含确切的七个索引。允许重复的索引。
- en: As defined before, the randomly selected samples are done with replacement,
    that’s why the samples have repeated indexes. The next step trains a *weak* learner
    for each sample. In this case, the learner chosen is the Decision Tree Classifier
    from scikit-learn [15]
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所定义，随机选择的样本是带有替换的，因此样本有重复的索引。接下来的步骤是为每个样本训练一个*弱*学习者。在这种情况下，选择的学习者是来自scikit-learn的决策树分类器
    [15]
- en: 'Code used for developing bagging. It can be found also in the repository: TreeEnsembles/bagging.ipynb.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 开发袋装法的代码。代码也可以在仓库中找到：TreeEnsembles/bagging.ipynb。
- en: Once every tree is trained, let’s dive into the first sample and its corresponding
    tree. The first bootstrapped sample had the observations *[6, 0, 2, 2, 7, 6, 8]*,
    so these are the observations used by the tree to train. The induced tree shows
    that can correctly classify every observation but one (observation = 0). If you
    follow the tree, it is clear that it is going to be classified as LOW, while the
    price is actually MEDIUM. Nevertheless, we can say the performance of the tree
    is somewhat good.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦每棵树都被训练完成，我们就来深入了解第一个样本及其对应的树。第一个自举样本包含了观测值*[6, 0, 2, 2, 7, 6, 8]*，这些就是树用来训练的观测值。诱导的树显示它能正确分类每个观测值，除了一个（观测值
    = 0）。如果你跟随这棵树，很明显它将被分类为低，而实际价格是中等的。尽管如此，我们可以说这棵树的表现还是有一定的好的。
- en: '![](../Images/37489faa9b654e803a8e95bfbf20da65.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/37489faa9b654e803a8e95bfbf20da65.png)'
- en: First tree and sample for bagging practical demonstration.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个树和样本用于袋装法的实际演示。
- en: 'However, take a look at the observation indexes again, there are only 5 unique
    values *[0, 2, 6, 7, 8]*, so the first tree is taking only 50% of the sample for
    training. Thus, the resulting accuracy can be misleading. To understand better
    the decision tree performance, we are going to predict the whole dataset (10 observations)
    with each decision tree. Furthermore, let’s make use of the second word that makes
    up bagging: **aggregation**. Our *strong* learner (called bagging) will take the
    most common prediction from the five trained trees.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，再次查看观测值的索引，只有5个独特的值*[0, 2, 6, 7, 8]*，因此第一棵树只用了50%的样本进行训练。因此，结果的准确率可能会具有误导性。为了更好地理解决策树的表现，我们将用每棵决策树对整个数据集（10个观测值）进行预测。此外，让我们利用袋装法的第二个词：**聚合**。我们的*强*学习者（称为袋装法）将从五棵训练树中获取最常见的预测。
- en: '![](../Images/4ddb50703f30b671200db59936f85845.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4ddb50703f30b671200db59936f85845.png)'
- en: Predictions of individual trees and bagging aggregation.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 单棵树和袋装法聚合的预测。
- en: As we can see below, in terms of accuracy the *weak* learners (trees) perform
    worse individually than the *strong* learner (bagging).
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 如下所示，从准确率来看，*弱*学习者（决策树）单独表现不如*强*学习者（袋装法）。
- en: '![](../Images/6bcf5c50e69bae93a9b0668f0ac220f9.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6bcf5c50e69bae93a9b0668f0ac220f9.png)'
- en: Accuracy of bagging and each trained tree.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 袋装法和每棵训练树的准确率。
- en: Boosting
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 提升方法
- en: Boosting, as opposed to bagging, trains trees sequentially. A first tree is
    trained with all the data. In the following tree, misclassified observations are
    given a higher weight (scikit learn’s decision tree classifier has a weights parameter).
    That weight allows the tree to *focus* more on certain observations. Here we have
    the code used to implement a boosting ensemble.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 提升与袋装不同，按顺序训练树。第一棵树用所有数据进行训练。在随后的树中，误分类观察值会赋予更高的权重（scikit learn 的决策树分类器具有权重参数）。该权重使树能够更*专注*于某些观察值。这里是用于实现提升集成的代码。
- en: 'Code used for developing boosting. It can be found also in the repository:
    TreeEnsembles/boosting.ipynb.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 用于开发提升的代码。它也可以在仓库中找到：TreeEnsembles/boosting.ipynb。
- en: As we said, the first step is to train the first tree with all the observations.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所说，第一步是用所有观察值训练第一棵树。
- en: '![](../Images/621f11904ba1caec6415ae80700056ac.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/621f11904ba1caec6415ae80700056ac.png)'
- en: First trained tree of boosting.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 提升的第一棵训练树。
- en: Note that the tree is not able to learn properly the observations *[0, 6, 9]*.
    So, according to boosting theory and the code, these observations will have higher
    weights in the second tree.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 注意到树无法正确学习观察值 *[0, 6, 9]*。因此，根据提升理论和代码，这些观察值在第二棵树中将具有更高的权重。
- en: '![](../Images/23abeb6dc11903a7b7560b5014ea146e.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/23abeb6dc11903a7b7560b5014ea146e.png)'
- en: Second trained tree of boosting
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 提升的第二棵训练树
- en: As we can see, the second tree trained with the updated weights is able to learn
    properly the previously misclassified observations *[0, 6, 9].* Higher weights
    assigned to these indexes have forced the tree to learn them properly. However,
    the tree learning process has changed also for the rest of the observations. It
    fails now to learn observations *[3, 4, 7]*.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所见，使用更新权重训练的第二棵树能够正确学习之前误分类的观察值 *[0, 6, 9]*。分配给这些索引的更高权重迫使树正确学习它们。然而，树的学习过程也改变了其余观察值。现在，它未能学习观察值
    *[3, 4, 7]*。
- en: Therefore, the third tree would double up the weights on these misclassified
    observations and, tree after tree, they will correct the errors made by previous
    learners. In the next picture, it is shown how each tree improves the errors of
    the previous one. Besides, there is a column called boosting that chooses the
    most common classification of all the trees.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，第三棵树会将这些误分类观察值的权重加倍，并且每棵树会纠正之前学习者所犯的错误。在下一张图中，展示了每棵树如何改进前一棵树的错误。此外，还有一列叫做提升的列，它选择了所有树中最常见的分类。
- en: '![](../Images/c2dbf25b27cd7cdd31ae066c5e1709ae.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c2dbf25b27cd7cdd31ae066c5e1709ae.png)'
- en: Predictions of the sequentially trained trees and predictions of the boosting
    aggregation.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 顺序训练的树的预测值以及提升聚合的预测值。
- en: If we calculate the accuracy of each tree and the boosting aggregation, the
    results clearly show that the boosting technique also improves the results of
    each individual tree.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们计算每棵树的准确性以及提升聚合的准确性，结果清楚地表明，提升技术也改善了每棵单独树的结果。
- en: '![](../Images/1faafb90bac46fa07cc31117e38e7823.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1faafb90bac46fa07cc31117e38e7823.png)'
- en: Accuracy of boosting and each trained tree.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 提升和每棵训练树的准确性。
- en: Gradient Boosting
  id: totrans-76
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 梯度提升
- en: Remember that for gradient boosting we are going to use the DataFrame whose
    target variable price is numeric. Also, remember that gradient boosting, like
    boosting, has an iterative approach. The difference is that the trees fit the
    *pseudo residuals* of the previous tree, instead of the same observations with
    different weights.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，对于梯度提升，我们将使用目标变量价格为数值型的 DataFrame。同时，请记住，梯度提升与提升类似，采用迭代的方法。不同之处在于，树适应的是前一棵树的
    *伪残差*，而不是相同观察值的不同权重。
- en: The first thing we are going to do is to calculate the first *pseudo residuals*
    (*residuals_0*), whichare the result of subtracting the mean of the price from
    the real value of the price.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 我们要做的第一件事是计算第一个 *伪残差*（*residuals_0*），这是通过从价格的实际值中减去价格的均值得到的。
- en: '![](../Images/4fc0a29e8501d0921adc33b0a03a0d12.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4fc0a29e8501d0921adc33b0a03a0d12.png)'
- en: DataFrame with the first pseudo residuals (residuals_0) calculated.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 计算出的首个伪残差（residuals_0）的 DataFrame。
- en: The Mean Absolute Error (MAE) of predicting the values using the mean to every
    observation is *100397.68*. This is the metric that is going to be improved with
    each trained tree. That being told, let’s train the first tree with the *pseudo
    residuals* shown before as the target.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 使用均值预测每个观察值的平均绝对误差（MAE）为 *100397.68*。这是每棵训练树都会改进的指标。说完这些，我们来用之前显示的 *伪残差* 作为目标训练第一棵树。
- en: '![](../Images/4919e79037b95f33644890a5ac8c8441.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4919e79037b95f33644890a5ac8c8441.png)'
- en: The first tree of gradient boosting was trained with pseudo residuals (residuals_0).
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度提升的第一棵树使用伪残差（residuals_0）进行训练。
- en: Observe that each final node has a different number of samples. In the first
    one on the left, there is only one observation. It corresponds to observation
    with index *1* whose *pseudo residual* is -*138461.4*. In the second final node
    from left to right, there are 5 samples which correspond to observations with
    indexes *[0, 2, 3, 6, 7]*. The predicted residual value by the tree (*-72705*)
    is the average value of the 5 residuals mentioned. These values will be known
    in advance as *predicted pseudo residuals.*
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 注意每个最终节点有不同数量的样本。在最左边的第一个节点中，只有一个观测值。它对应于索引为 *1* 的观测值，其 *pseudo residual* 为
    -*138461.4*。在从左到右的第二个最终节点中，有5个样本，它们对应于索引为 *[0, 2, 3, 6, 7]* 的观测值。树预测的残差值 (*-72705*)
    是这5个残差值的平均值。这些值将预先被称为 *predicted pseudo residuals*。
- en: To predict real price values with this first tree we should do the following
    operation.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使用第一棵树预测实际价格值，我们应执行以下操作。
- en: '![](../Images/102ba401f3720c391226f35026434f22.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/102ba401f3720c391226f35026434f22.png)'
- en: Formula for predicting price values after the first tree.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 第一棵树后的价格预测公式。
- en: The predictions shown below achieve a MAE of *70347.53,* which improves the
    MAE achieved before by predicting only with the mean (*100397.68*). With these
    predictions, we can calculate the next *pseudo residuals* (*residuals_1*), which
    are the result of subtracting the predictions of the first tree (*predictions_0*)
    from the real value of the price.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 下面显示的预测达到 *70347.53* 的MAE，这比之前仅用均值预测所达到的MAE (*100397.68*) 更有改善。利用这些预测，我们可以计算下一个
    *pseudo residuals* (*residuals_1*)，这是通过将第一棵树的预测值 (*predictions_0*) 从实际价格值中减去得到的。
- en: '![](../Images/bfb585c003a3ac58073a9be147cffe82.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bfb585c003a3ac58073a9be147cffe82.png)'
- en: Predictions results after the first tree.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 第一棵树后的预测结果。
- en: Note that *residuals_1* are always closer to zero than *residuals_0*. This is
    because we are using a portion of the information given by the tree to improve
    the prediction made by the mean. This portion percentage is defined by the *learning
    rate.*
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，*residuals_1* 总是比 *residuals_0* 更接近零。这是因为我们使用了树所提供的部分信息来改善由均值做出的预测。这部分信息的比例由
    *learning rate* 定义。
- en: 'Until now, only one tree has been involved, but we said before that gradient
    boosting use several trees sequentially. It’s time to train the second tree, and
    for this purpose, the residuals previously calculated (*residuals_1*) are going
    to be used as the target. The second trained tree looks as follows:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，仅涉及了第一棵树，但我们之前提到过梯度提升是顺序使用多棵树。现在是时候训练第二棵树了，为此目的，之前计算的残差 (*residuals_1*)
    将作为目标使用。第二棵训练树如下：
- en: '![](../Images/bc9fa6e030df96a20d1c8e1253935baf.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bc9fa6e030df96a20d1c8e1253935baf.png)'
- en: Second tree of gradient boosting is trained with the pseudo residuals (residuals_1).
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 第二棵梯度提升树使用伪残差（residuals_1）进行训练。
- en: If we follow the same steps as in the first tree, we get the following results.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们按照第一棵树的相同步骤操作，将得到以下结果。
- en: '![](../Images/78ae276fc780b3d415e66c282708d877.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/78ae276fc780b3d415e66c282708d877.png)'
- en: Predictions results after second tree.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 第二棵树后的预测结果。
- en: The only difference with the first tree is that we used both the first and second
    trees' pseudo residuals predictions (*residual_predictions_1* and *residual_predictions_1*)
    for making the predictions(*predictions_1*)*.* Instead of doing an aggregation
    of the predictions as it is done in bagging and boosting, gradient boosting adds
    to the mean price small portions of information of each tree (*learning rate *
    residuals preds tree x*).
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 与第一棵树唯一的不同是，我们使用了第一棵和第二棵树的伪残差预测 (*residual_predictions_1* 和 *residual_predictions_1*)
    来进行预测 (*predictions_1*)。梯度提升不是像装袋和提升中那样进行预测聚合，而是将每棵树的少量信息添加到均值价格中 (*learning rate
    * residuals preds tree x*)。
- en: '![](../Images/b02723756bbd999a120eba66209b2723.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b02723756bbd999a120eba66209b2723.png)'
- en: Formula for predicting price values after the second tree is trained.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 第二棵树训练后的价格预测公式。
- en: After training 5 trees, we can clearly see how the MAE has been reduced in the
    following results. Each iteration provides better learning, and it is clear that
    the union of all the trees provides better metrics than using them individually.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 训练了5棵树后，我们可以清晰地看到MAE在以下结果中的减少情况。每次迭代都提供了更好的学习效果，显然所有树的联合提供了比单独使用它们更好的指标。
- en: '![](../Images/f8ec212491b8d53c02dc15374513eab7.png)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f8ec212491b8d53c02dc15374513eab7.png)'
- en: MAE achieved after each iteration.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 每次迭代后的MAE值。
- en: Conclusion
  id: totrans-104
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: 'This article is intended to be a step-by-step guide which explains how different
    ensembling learning methods work: **Bagging**, **Boosting** and **Gradient Boosting**.'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '-   本文旨在提供一个逐步指南，解释不同的集成学习方法如何工作：**装袋法**、**提升法**和**梯度提升法**。'
- en: In the first section, we have seen the history, description and uses of these
    techniques, while in the second one, we have developed in a practical way all
    the models, showing how the union of different *weak* learners can create a *strong*
    learner with a higher predictive power.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '-   在第一部分，我们回顾了这些技术的历史、描述和应用；而在第二部分，我们实际地发展了所有模型，展示了不同的*弱*学习器如何结合成一个具有更高预测能力的*强*学习器。'
- en: I hope you have found the reading useful and enjoyable. And above all, I am
    happy to receive any kind of feedback. So feel free to share your thoughts!
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '-   我希望你觉得阅读有用且愉快。最重要的是，我很乐意接受任何形式的反馈。请随时分享你的想法！'
- en: References
  id: totrans-108
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] Breiman, L. «Bagging Predictors». *Machine Learning* 24, (1996): 123–40\.
    [https://doi.org/10.1007/BF00058655](https://doi.org/10.1007/BF00058655).'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] Breiman, L. 《装袋预测器》。*机器学习* 24 (1996): 123–40\. [https://doi.org/10.1007/BF00058655](https://doi.org/10.1007/BF00058655).'
- en: '[2] Breiman, L. «Random Forests». *Machine Learning* 45, (2001): 5–32\. [https://doi.org/10.1023/A:1010933404324](https://doi.org/10.1023/A:1010933404324).'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] Breiman, L. 《随机森林》。*机器学习* 45 (2001): 5–32\. [https://doi.org/10.1023/A:1010933404324](https://doi.org/10.1023/A:1010933404324).'
- en: '[3] Image Source: [https://www.researchgate.net/figure/The-bagging-approach-Several-classifier-are-trained-on-bootstrap-samples-of-the-training_fig4_322179244](https://www.researchgate.net/figure/The-bagging-approach-Several-classifier-are-trained-on-bootstrap-samples-of-the-training_fig4_322179244)'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] 图像来源: [https://www.researchgate.net/figure/The-bagging-approach-Several-classifier-are-trained-on-bootstrap-samples-of-the-training_fig4_322179244](https://www.researchgate.net/figure/The-bagging-approach-Several-classifier-are-trained-on-bootstrap-samples-of-the-training_fig4_322179244)'
- en: '[4] Kearns, M. «[Thoughts on Hypothesis Boosting](http://www.cis.upenn.edu/~mkearns/papers/boostnote.pdf)»,
    Unpublished manuscript (Machine Learning class project) (1988)'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] Kearns, M. 《[关于假设提升的思考](http://www.cis.upenn.edu/~mkearns/papers/boostnote.pdf)》，未出版手稿（机器学习课程项目）(1988)'
- en: '[5] Kearns, M; Valiant, L. «Cryptographic limitations on learning Boolean formulae
    and finite automata». *Symposium on Theory of Computing* 21, (1989): 433–444 [https://dl.acm.org/doi/10.1145/73007.73049](https://dl.acm.org/doi/10.1145/73007.73049)'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '[5] Kearns, M; Valiant, L. 《关于学习布尔公式和有限自动机的密码学限制》。*计算理论研讨会* 21 (1989): 433–444
    [https://dl.acm.org/doi/10.1145/73007.73049](https://dl.acm.org/doi/10.1145/73007.73049)'
- en: '[6] Schapire, R.E. «The Strength of Weak Learnability». *Machine Learning*
    5, (1990): 197–227 [https://doi.org/10.1007/BF00116037](https://doi.org/10.1007/BF00116037)'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '[6] Schapire, R.E. 《弱学习能力的力量》。*机器学习* 5 (1990): 197–227 [https://doi.org/10.1007/BF00116037](https://doi.org/10.1007/BF00116037)'
- en: '[7] Freund, Y.; Schapire, R.E. (1995). «A decision-theoretic generalization
    of online learning and an application to boosting». *Computational Learning Theory,*
    (1995) [https://doi.org/10.1007/3-540-59119-2_166](https://doi.org/10.1007/3-540-59119-2_166)'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '[7] Freund, Y.; Schapire, R.E. (1995). 《在线学习的决策理论推广及其在提升中的应用》。*计算学习理论,* (1995)
    [https://doi.org/10.1007/3-540-59119-2_166](https://doi.org/10.1007/3-540-59119-2_166)'
- en: '[8] Image Source: [https://www.researchgate.net/figure/Training-of-an-AdaBoost-classifier-The-first-classifier-trains-on-unweighted-data-then_fig3_306054843](https://www.researchgate.net/figure/Training-of-an-AdaBoost-classifier-The-first-classifier-trains-on-unweighted-data-then_fig3_306054843)'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '[8] 图像来源: [https://www.researchgate.net/figure/Training-of-an-AdaBoost-classifier-The-first-classifier-trains-on-unweighted-data-then_fig3_306054843](https://www.researchgate.net/figure/Training-of-an-AdaBoost-classifier-The-first-classifier-trains-on-unweighted-data-then_fig3_306054843)'
- en: '[9] Friedman, J.H. «Greedy Function Approximation: A Gradient Boosting Machine»
    *The Annals of Statistics* 29 (2001). [https://doi.org/10.1214/aos/1013203451](https://doi.org/10.1214/aos/1013203451).'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '[9] Friedman, J.H. 《贪婪函数近似：一种梯度提升机》 *统计年鉴* 29 (2001). [https://doi.org/10.1214/aos/1013203451](https://doi.org/10.1214/aos/1013203451).'
- en: '[10] Chen, Tianqi, and Carlos Guestrin. «XGBoost: A Scalable Tree Boosting
    System» *Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge
    Discovery and Data Mining* (2016). [https://doi.org/10.1145/2939672.2939785](https://doi.org/10.1145/2939672.2939785).'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '[10] Chen, Tianqi, and Carlos Guestrin. 《XGBoost: 一种可扩展的树提升系统》 *第22届ACM SIGKDD国际知识发现与数据挖掘会议论文集*
    (2016). [https://doi.org/10.1145/2939672.2939785](https://doi.org/10.1145/2939672.2939785).'
- en: '[11] Dorogush, A.V.; Ershov, V.; Gulin. A. CatBoost: Gradient Boosting with
    Categorical Features Support». *ArXiv:1810.11363*, 24 (2018). [http://arxiv.org/abs/1810.11363](http://arxiv.org/abs/1810.11363).'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '[11] Dorogush, A.V.; Ershov, V.; Gulin. A. CatBoost: 支持分类特征的梯度提升». *ArXiv:1810.11363*,
    24 (2018). [http://arxiv.org/abs/1810.11363](http://arxiv.org/abs/1810.11363).'
- en: '[12] Ke, G.; Meng, Q.; Finley, T; Wang, T; Chen, W; Ma, W; Ye, Q; Liu, T. «LightGBM:
    A Highly Efficient Gradient Boosting Decision Tree». *Advances in Neural Information
    Processing Systems*, 20 (2017). [https://proceedings.neurips.cc/paper/2017/hash/6449f44a102fde848669bdd9eb6b76fa-Abstract.html](https://proceedings.neurips.cc/paper/2017/hash/6449f44a102fde848669bdd9eb6b76fa-Abstract.html).'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '[12] Ke, G.; Meng, Q.; Finley, T; Wang, T; Chen, W; Ma, W; Ye, Q; Liu, T. «LightGBM:
    高效的梯度提升决策树». *神经信息处理系统进展*, 20 (2017). [https://proceedings.neurips.cc/paper/2017/hash/6449f44a102fde848669bdd9eb6b76fa-Abstract.html](https://proceedings.neurips.cc/paper/2017/hash/6449f44a102fde848669bdd9eb6b76fa-Abstract.html).'
- en: '[13] Image source: [https://www.researchgate.net/profile/Karem-Abdelmohsen/publication/339077244/figure/fig3/AS:855596286877696@1581001459188/Schematic-diagram-of-a-tree-based-gradient-boosting-method.png](https://www.researchgate.net/profile/Karem-Abdelmohsen/publication/339077244/figure/fig3/AS:855596286877696@1581001459188/Schematic-diagram-of-a-tree-based-gradient-boosting-method.png)'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '[13] 图片来源: [https://www.researchgate.net/profile/Karem-Abdelmohsen/publication/339077244/figure/fig3/AS:855596286877696@1581001459188/Schematic-diagram-of-a-tree-based-gradient-boosting-method.png](https://www.researchgate.net/profile/Karem-Abdelmohsen/publication/339077244/figure/fig3/AS:855596286877696@1581001459188/Schematic-diagram-of-a-tree-based-gradient-boosting-method.png)'
- en: '[14] StatQuest with Josh Starmer (Youtube Channel): [https://www.youtube.com/c/joshstarmer](https://www.youtube.com/c/joshstarmer)'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '[14] StatQuest with Josh Starmer (YouTube频道): [https://www.youtube.com/c/joshstarmer](https://www.youtube.com/c/joshstarmer)'
- en: '[15] [https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html)'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '[15] [https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html)'
