- en: 'Temporal Differences with Python: First Sample-Based Reinforcement Learning
    Algorithm'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Python中的时序差分：第一个基于样本的强化学习算法
- en: 原文：[https://towardsdatascience.com/temporal-differences-with-python-first-sample-based-reinforcement-learning-algorithm-54c11745a0ee](https://towardsdatascience.com/temporal-differences-with-python-first-sample-based-reinforcement-learning-algorithm-54c11745a0ee)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/temporal-differences-with-python-first-sample-based-reinforcement-learning-algorithm-54c11745a0ee](https://towardsdatascience.com/temporal-differences-with-python-first-sample-based-reinforcement-learning-algorithm-54c11745a0ee)
- en: Coding up and understanding the TD(0) algorithm using Python
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用Python编写并理解TD(0)算法
- en: '[](https://eligijus-bujokas.medium.com/?source=post_page-----54c11745a0ee--------------------------------)[![Eligijus
    Bujokas](../Images/061fd30136caea2ba927140e8b3fae3c.png)](https://eligijus-bujokas.medium.com/?source=post_page-----54c11745a0ee--------------------------------)[](https://towardsdatascience.com/?source=post_page-----54c11745a0ee--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----54c11745a0ee--------------------------------)
    [Eligijus Bujokas](https://eligijus-bujokas.medium.com/?source=post_page-----54c11745a0ee--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://eligijus-bujokas.medium.com/?source=post_page-----54c11745a0ee--------------------------------)[![Eligijus
    Bujokas](../Images/061fd30136caea2ba927140e8b3fae3c.png)](https://eligijus-bujokas.medium.com/?source=post_page-----54c11745a0ee--------------------------------)[](https://towardsdatascience.com/?source=post_page-----54c11745a0ee--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----54c11745a0ee--------------------------------)
    [Eligijus Bujokas](https://eligijus-bujokas.medium.com/?source=post_page-----54c11745a0ee--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----54c11745a0ee--------------------------------)
    ·13 min read·Jan 27, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----54c11745a0ee--------------------------------)
    ·13分钟阅读·2023年1月27日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/36f274e9692bf335a901977271968109.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/36f274e9692bf335a901977271968109.png)'
- en: Photo by [Kurt Cotoaga](https://unsplash.com/@kydroon?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[Kurt Cotoaga](https://unsplash.com/@kydroon?utm_source=medium&utm_medium=referral)在[Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)上的照片'
- en: 'This is a continuation article from my previous article:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我之前文章的续集：
- en: '[](/first-steps-in-the-world-of-reinforcement-learning-using-python-b843b76538e3?source=post_page-----54c11745a0ee--------------------------------)
    [## First Steps in the World Of Reinforcement Learning using Python'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/first-steps-in-the-world-of-reinforcement-learning-using-python-b843b76538e3?source=post_page-----54c11745a0ee--------------------------------)
    [## Python强化学习中的第一步'
- en: Original Python implementation of how to find the best places to be in one of
    the fundamental worlds of reinforcement…
  id: totrans-10
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Python的原始实现，展示了如何在强化学习的基本世界之一中找到最佳位置……
- en: towardsdatascience.com](/first-steps-in-the-world-of-reinforcement-learning-using-python-b843b76538e3?source=post_page-----54c11745a0ee--------------------------------)
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/first-steps-in-the-world-of-reinforcement-learning-using-python-b843b76538e3?source=post_page-----54c11745a0ee--------------------------------)
- en: In this article, I want to familiarize the reader with the sample-based algorithm
    logic in Reinforcement Learning (**RL**). To do this, we will create a grid world
    with holes (much like the one in the thumbnail) and let our agent freely traverse
    our created world.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我想让读者熟悉强化学习中的基于样本的算法逻辑（**RL**）。为此，我们将创建一个带有洞的网格世界（类似于缩略图中的那个），并让我们的代理在创建的世界中自由遍历。
- en: Hopefully, by the end of the agent's journey, he will have learnt where in the
    world is a good place to be and which locations should be avoided. To help our
    agent in the learning process we will use the famous **TD(0)** algorithm.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 希望在代理的旅程结束时，他能学会在世界上哪个地方是好的地方，哪些位置应该避免。为了帮助我们的代理学习，我们将使用著名的**TD(0)**算法。
- en: Before diving into the algorithms, let us define the objective that we want
    to solve.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入算法之前，让我们定义一下我们想要解决的目标。
- en: 'In this article, we will create a grid world with 5 rows and 7 columns, meaning,
    our agent will be able to be in one of 35 states. The rules of movement are:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我们将创建一个5行7列的网格世界，这意味着我们的代理将能够处于35个状态中的一个。移动规则如下：
- en: The agent cannot go outside the grid world boundaries.
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 代理不能离开网格世界的边界。
- en: The agent, at each time step, can only move up, down, left or right.
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在每个时间步，代理只能向上、向下、向左或向右移动。
- en: The agent starts in the top left corner of our grid world.
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 代理从我们网格世界的左上角开始。
- en: If the agent reaches his goal or falls into the hole, the game ends and he is
    returned to the starting state.
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果代理达到目标或掉入洞里，游戏结束，代理会被返回到起始状态。
- en: Each movement earns a reward of -1.
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每次移动都会获得-1的奖励。
- en: Falling into a hole earns a reward of -10.
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 掉入洞里会获得-10的奖励。
- en: Reaching the goal earns a reward of 10.
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 达到目标会获得10的奖励。
- en: The ultimate goal of our agent is to evaluate each state he can be in as well
    as possible. In other words, **our agent wants to evaluate the value of each state
    given a certain policy of movement.**
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 我们代理的**终极目标是尽可能准确地评估它可能处于的每一个状态。换句话说，**我们代理希望根据给定的移动策略评估每个状态的价值。**
- en: 'The bellow code snipped initializes the environment stated in the previous
    section:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码片段初始化了前一节中描述的环境：
- en: '[PRE0]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The objects that we need to start learning are:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要开始学习的对象是：
- en: The state matrix S
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 状态矩阵S
- en: The value matrix V
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 值矩阵V
- en: The reward matrix R
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 奖励矩阵R
- en: The policy dictionary P
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 策略字典P
- en: By default, the code snippet above initiates a world with a random policy.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，上述代码片段初始化了一个随机策略的世界。
- en: '**A random policy means that our agent chooses to go from one state to another
    by a uniform probability distribution.**'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '**随机策略意味着我们的代理通过均匀概率分布选择从一个状态转移到另一个状态。**'
- en: 'Let us create our world and explore in more detail the matrices:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们创建我们的世界，更详细地探索这些矩阵：
- en: '[PRE1]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The bellow code snippet is used to plot the matrices:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码片段用于绘制矩阵：
- en: '[PRE2]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Let us first visualize the state matrix:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 首先让我们可视化状态矩阵：
- en: '[PRE3]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '![](../Images/a010780e57982499b646f6fd03f4aad8.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a010780e57982499b646f6fd03f4aad8.png)'
- en: State matrix; Photo by author
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 状态矩阵；作者拍摄的照片
- en: The red states indicate the hole coordinates — these are the state our agent
    wants to avoid.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 红色状态表示洞的坐标——这些是我们的代理想要避免的状态。
- en: The grey state indicates the goal — this is where our agent wants to be.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 灰色状态表示目标——这是我们的代理想要到达的地方。
- en: Our agent will always start its journey from state 0.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的代理总是从状态0开始它的旅程。
- en: 'The reward matrix is as follows:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 奖励矩阵如下：
- en: '[PRE4]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '![](../Images/1aa2a275f8ba0f3ab10a817dbbfb6caa.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1aa2a275f8ba0f3ab10a817dbbfb6caa.png)'
- en: Reward matrix; Photo by author
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 奖励矩阵；作者拍摄的照片
- en: The reward matrix for transitioning into a certain state is visualized above.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 转移到某个状态的奖励矩阵在上面可视化。
- en: 'For example:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 例如：
- en: Going from state 1 -> 8 would give a reward of -1
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从状态1到8会获得-1的奖励
- en: Going from state 9 -> 10 would give a reward of -10
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从状态9到10会获得-10的奖励
- en: Going from state 33 -> 34 would give a reward of 10
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从状态33到34会获得10的奖励
- en: And so on.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 依此类推。
- en: 'The policy that our agent will follow is a random policy — transitioning into
    each of the states is uniform:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的代理将遵循的策略是随机策略——进入每个状态的概率均等：
- en: '[PRE5]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '![](../Images/2555fd4214edb5815094a9072e2b5f46.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2555fd4214edb5815094a9072e2b5f46.png)'
- en: The policy matrix; Photo by author
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 策略矩阵；作者拍摄的照片
- en: 'The grey states in the policy matrix represent the terminal states: if the
    agent chooses to go to that state, the episode will end and the agent will be
    reset to state 0.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 策略矩阵中的灰色状态表示终端状态：如果代理选择进入该状态，剧集将结束，代理将被重置到状态0。
- en: '**The goal of the TD(0) algorithm is to evaluate each state’s value for the
    given policy.**'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '**TD(0)算法的目标是评估给定策略下每个状态的价值。**'
- en: 'In other words, we want to fill out the value matrix values:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，我们想要填充值矩阵的值：
- en: '[PRE6]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '![](../Images/a3440272edf8dca009a125014db17b4c.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a3440272edf8dca009a125014db17b4c.png)'
- en: Initial value matrix; Photo by author
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 初始值矩阵；作者拍摄的照片
- en: The **TD(0)** algorithm is short for **one step temporal differences** algorithm.
    In order to start building intuition and putting it broadly, in this algorithm
    our agent makes one step following the given policy, observes the reward and updates
    the estimates of the state value after such a step.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '**TD(0)**算法是**单步时序差分**算法的简称。为了开始建立直觉并广泛地说，在此算法中，我们的代理按照给定的策略执行一步，观察奖励，并在这种步骤后更新状态价值的估计。'
- en: 'To put it mathematically, the update step is the following:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 从数学上讲，更新步骤如下：
- en: '![](../Images/bc019e96ebe04404524df5596f5c0a45.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bc019e96ebe04404524df5596f5c0a45.png)'
- en: TD(0) update equation
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: TD(0) 更新方程
- en: 'Here:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 这里：
- en: '**s prime** — the state to which our agent goes from the current state s.'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**s prime** — 我们的代理从当前状态s转移到的状态。'
- en: The reward **r** is equal to the reward of transitioning to s prime.
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 奖励 **r** 等于转移到s prime的奖励。
- en: '**Gamma** is the discount rate (more than 0, less than or equal to 1).'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Gamma** 是折扣率（大于0，小于或等于1）。'
- en: '**Alpha** is the size (more than 0, less than or equal to 1).'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Alpha** 是大小（大于0，小于或等于1）。'
- en: 'The full algorithm¹ is the following:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 完整算法¹如下：
- en: '![](../Images/77bdaf7b738af3153323baee021c0110.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/77bdaf7b738af3153323baee021c0110.png)'
- en: Full TD(0); Photo by author
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 完整TD(0)；作者照片
- en: The TD(0) algorithm is a **prediction** algorithm. In RL, a prediction algorithm
    refers to an algorithm that tries to estimate the state values while **not** changing
    the given policy (probabilities of transition).
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: TD(0)算法是一种**预测**算法。在强化学习中，预测算法指的是一种尝试估计状态值的算法，同时**不**改变给定的策略（转移概率）。
- en: It is also a **bootstrap** algorithm because we are using the current estimate
    of the value function to estimate the value function for the next state.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 这也是一种**自助**算法，因为我们使用当前的价值函数估计来估计下一个状态的价值函数。
- en: 'Thus, we are only interested in the state values — the total expected accumulated
    reward if the agent moves from the current state:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们只关心状态值——智能体从当前状态移动的总期望累计奖励：
- en: '![](../Images/c69146c3676f9e8d5c3aae5cc0b9a450.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c69146c3676f9e8d5c3aae5cc0b9a450.png)'
- en: State value
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 状态价值
- en: Now let us start the implementation of the algorithm.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们开始实现算法。
- en: 'The first thing that our agent needs is to make a move based on our created
    policy:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的智能体首先需要根据我们创建的策略进行移动：
- en: '[PRE7]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'When an agent is in state s, he can only go to states that are present in the
    policy matrix dictionary. For example, all the actions in state 1 are:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 当智能体处于状态s时，它只能前往策略矩阵字典中存在的状态。例如，状态1中的所有动作是：
- en: '![](../Images/f28b8b65ac45770c6f867938c3719b6c.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f28b8b65ac45770c6f867938c3719b6c.png)'
- en: All the possible actions from state 1
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 状态1的所有可能动作
- en: The sum of all the probabilities is equal to 1 and our agent chooses randomly
    from the actions **right, left or down** (refer to the state matrix plot to see
    where the state is).
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 所有概率的总和等于1，我们的智能体随机选择**右、左或下**（请参阅状态矩阵图以查看状态位置）。
- en: 'The above action is all that is needed to start updating the value function.
    When our agent makes a move, it transitions to another state and collects the
    reward of that state. We then apply the equation:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 上述动作是开始更新价值函数所需的全部。当智能体进行移动时，它转移到另一个状态并收集该状态的奖励。然后我们应用方程：
- en: '![](../Images/bc019e96ebe04404524df5596f5c0a45.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bc019e96ebe04404524df5596f5c0a45.png)'
- en: TD(0) update equation
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: TD(0)更新方程
- en: '[PRE8]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The final thing to do is to wrap everything into a **while loop** and only
    stop exploring when our agent transitions into a terminal state:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一步是将所有内容封装到一个**while循环**中，只有当我们的智能体转移到终止状态时才停止探索：
- en: '[PRE9]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: We now have everything we need to implement the full TD(0) algorithm.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在拥有了实施完整TD(0)算法所需的一切。
- en: Let us define 10000 episodes and let our agent learn!
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们定义10000次实验，让我们的智能体进行学习吧！
- en: '[PRE10]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The number of actions our agent took before termination:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的智能体在终止之前所采取的动作数量：
- en: '[PRE11]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '![](../Images/43bc9ae571c1200b08fa3af801a9413e.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/43bc9ae571c1200b08fa3af801a9413e.png)'
- en: Number of moves; Graph by author
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 移动次数；作者绘图
- en: On average, our agent took 10 moves before bumping into a terminal state.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 平均而言，我们的智能体在碰到终止状态之前进行了10次移动。
- en: 'The final evaluated state value matrix:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 最终评估的状态价值矩阵：
- en: '![](../Images/1394931721b63e545ef719d8b174129b.png)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1394931721b63e545ef719d8b174129b.png)'
- en: Evaluated V using TD(0) and a random policy; Graph by author
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 使用TD(0)和随机策略评估的V；作者绘图
- en: As we can see, following the given policy, it is a very bad state to be in where
    our agent starts our journey. On average, starting from that state, the agent
    accumulates only a -9.96 reward. As we move closer to the goal state, however,
    the value increases.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所见，按照给定的策略，智能体开始旅程的状态非常糟糕。平均而言，从该状态开始，智能体仅获得-9.96的奖励。然而，随着我们接近目标状态，价值会增加。
- en: Note that the goal state and the hole states have a value of 0 because there
    is no exploration from these states — every time the agent transitions there the
    game ends.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，目标状态和洞穴状态的值为0，因为这些状态没有探索——每次智能体转移到这些状态，游戏就结束了。
- en: 'What would happen if we chose another policy? For example, more often chose
    the direction to the ‘right’:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们选择了另一种策略会发生什么？例如，更频繁地选择“向右”方向：
- en: '[PRE12]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '![](../Images/66602a243f165f8344cf6d401141b74b.png)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/66602a243f165f8344cf6d401141b74b.png)'
- en: Number of moves with a different policy
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 不同策略下的移动次数
- en: '![](../Images/3cbab1ef6cc6bcd0ba876784e9803804.png)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3cbab1ef6cc6bcd0ba876784e9803804.png)'
- en: Value matrix of the different policy
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 不同策略的价值矩阵
- en: The random policies state value matrix sum is equal to **-249.29** while the
    policy with a bigger probability to go to the right has a sum of **-213.51**.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 随机策略的状态价值矩阵总和为**-249.29**，而更高概率向右的策略总和为**-213.51**。
- en: In that sense, we can say that moving more often to the right is a better policy!
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 从这个意义上说，我们可以说更频繁地向右移动是一种更好的策略！
- en: In this article, I presented the first sample-based algorithm in RL — the one-step
    temporal difference algorithm or TD(0).
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我介绍了RL中的第一个基于样本的算法——一步时序差分算法或TD(0)。
- en: It is a prediction algorithm, meaning, it is only used to evaluate the states
    for the given policy. Changing a policy will yield different state-value results.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一种预测算法，即仅用于评估给定策略的状态。改变策略会得到不同的状态价值结果。
- en: Happy learning and happy coding everyone!
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 祝大家学习愉快，编程快乐！
- en: '[1]'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '[1]'
- en: 'Author: **Richard S. Sutton, Andrew G. Barto**'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '作者: **理查德·S·萨顿，安德鲁·G·巴托**'
- en: 'Year: **2018**'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '年份: **2018**'
- en: 'Page: **120**'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '页码: **120**'
- en: 'Title: **Reinforcement Learning: An Introduction**'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '书名: **强化学习：一种介绍**'
- en: URL:[**http://archive.ics.uci.edu/ml**](https://web.stanford.edu/class/psych209/Readings/SuttonBartoIPRLBook2ndEd.pdf)
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: URL:[**http://archive.ics.uci.edu/ml**](https://web.stanford.edu/class/psych209/Readings/SuttonBartoIPRLBook2ndEd.pdf)
