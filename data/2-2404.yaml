- en: 'XGBoost: How Deep Learning Can Replace Gradient Boosting and Decision Trees
    — Part 1'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: XGBoost：深度学习如何取代梯度提升和决策树 — 第 1 部分
- en: 原文：[https://towardsdatascience.com/xgboost-how-deep-learning-can-replace-gradient-boosting-and-decision-trees-291dc9365656](https://towardsdatascience.com/xgboost-how-deep-learning-can-replace-gradient-boosting-and-decision-trees-291dc9365656)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/xgboost-how-deep-learning-can-replace-gradient-boosting-and-decision-trees-291dc9365656](https://towardsdatascience.com/xgboost-how-deep-learning-can-replace-gradient-boosting-and-decision-trees-291dc9365656)
- en: '[](https://medium.com/@guillaume.saupin?source=post_page-----291dc9365656--------------------------------)[![Saupin
    Guillaume](../Images/d9112d3cdfe6f335b6ff2c875fba6bb5.png)](https://medium.com/@guillaume.saupin?source=post_page-----291dc9365656--------------------------------)[](https://towardsdatascience.com/?source=post_page-----291dc9365656--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----291dc9365656--------------------------------)
    [Saupin Guillaume](https://medium.com/@guillaume.saupin?source=post_page-----291dc9365656--------------------------------)'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@guillaume.saupin?source=post_page-----291dc9365656--------------------------------)[![Saupin
    Guillaume](../Images/d9112d3cdfe6f335b6ff2c875fba6bb5.png)](https://medium.com/@guillaume.saupin?source=post_page-----291dc9365656--------------------------------)[](https://towardsdatascience.com/?source=post_page-----291dc9365656--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----291dc9365656--------------------------------)
    [Saupin Guillaume](https://medium.com/@guillaume.saupin?source=post_page-----291dc9365656--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----291dc9365656--------------------------------)
    ·7 min read·May 29, 2023
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----291dc9365656--------------------------------)
    ·阅读时长 7 分钟·2023年5月29日
- en: --
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/d836a60a327155b897054d6830cd01dc.png)'
  id: totrans-5
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d836a60a327155b897054d6830cd01dc.png)'
- en: Photo by [Preethi Viswanathan](https://unsplash.com/@sallybrad2016?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由 [Preethi Viswanathan](https://unsplash.com/@sallybrad2016?utm_source=medium&utm_medium=referral)
    提供，来源于 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: In this article, you will learn about rewriting decision trees using a Differentiable
    Programming approach, as proposed by the NODE paper, enabling the reformulation
    of this model in a manner similar to Neural Networks.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，你将了解如何使用可微编程方法重写决策树，这种方法由 NODE 论文提出，使得该模型可以以类似于神经网络的方式进行重新表述。
- en: 'Deriving this formulation is an excellent exercise, as it raises many issues
    common when building and training Custom Neural Networks:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 推导这种表述是一个很好的练习，因为它提出了在构建和训练自定义神经网络时常见的许多问题：
- en: How to avoid the vanishing gradient problem?
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何避免梯度消失问题？
- en: What is a good choice for initial weights?
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 什么是良好的初始权重选择？
- en: Why use batch normalization?
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为什么使用批量归一化？
- en: 'But before answering these questions, let’s see how to rephrase Decision Tree
    in the mathematical framework of Neural Networks: Differentiable Programming.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在回答这些问题之前，让我们看看如何将决策树重新表述为神经网络的数学框架：可微编程。
- en: 'Update — the second article is online:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 更新 — 第二篇文章已上线：
- en: '[](/xgboost-how-deep-learning-can-replace-gradient-boosting-and-decision-trees-part-2-training-b432620750f8?source=post_page-----291dc9365656--------------------------------)
    [## XGBoost: How Deep Learning Can Replace Gradient Boosting and Decision Trees
    — Part 2: Training'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/xgboost-how-deep-learning-can-replace-gradient-boosting-and-decision-trees-part-2-training-b432620750f8?source=post_page-----291dc9365656--------------------------------)
    [## XGBoost：深度学习如何取代梯度提升和决策树 — 第 2 部分：训练'
- en: A world without if
  id: totrans-15
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 一个没有 if 的世界
- en: towardsdatascience.com](/xgboost-how-deep-learning-can-replace-gradient-boosting-and-decision-trees-part-2-training-b432620750f8?source=post_page-----291dc9365656--------------------------------)
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/xgboost-how-deep-learning-can-replace-gradient-boosting-and-decision-trees-part-2-training-b432620750f8?source=post_page-----291dc9365656--------------------------------)
- en: XGBoost is highly efficient, but…
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: XGBoost 非常高效，但…
- en: If you have read my previous articles on Gradient Boosting and Decision Trees,
    you are aware that Gradient Boosting, combined with Ensembles of Decision Trees,
    has achieved excellent performance in classification or regression tasks involving
    tabular data.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你读过我之前关于梯度提升和决策树的文章，你会知道梯度提升结合决策树集成在分类或回归任务中表现出色，尤其是在涉及表格数据时。
- en: '[](https://amzn.to/3IKyqTF?source=post_page-----291dc9365656--------------------------------)
    [## Practical Gradient Boosting: An deep dive into Gradient Boosting in Python'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://amzn.to/3IKyqTF?source=post_page-----291dc9365656--------------------------------)
    [## 实用的梯度提升：深入探讨Python中的梯度提升'
- en: This book on Gradient Boosting methods is intended for students, academics,
    engineers, and data scientists who wish to…
  id: totrans-20
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 本书关于梯度提升方法，旨在为希望深入了解的学生、学者、工程师和数据科学家提供参考…
- en: amzn.to](https://amzn.to/3IKyqTF?source=post_page-----291dc9365656--------------------------------)
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: amzn.to](https://amzn.to/3IKyqTF?source=post_page-----291dc9365656--------------------------------)
- en: However, despite the efforts made by XGBoost, LightGBM, or CatBoost to enhance
    the construction of Ensemble of Decision Trees, training such a model still relies
    on a brute force approach. It involves systematically evaluating the gain associated
    with splitting values and features.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管XGBoost、LightGBM或CatBoost在增强决策树集成构建方面做出了努力，但训练这样的模型仍然依赖于蛮力方法。它涉及系统地评估拆分值和特征相关的增益。
- en: 'Refer to my previous article on the subject:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 请参考我之前关于此主题的文章：
- en: '[](/diy-xgboost-library-in-less-than-200-lines-of-python-69b6bf25e7d9?source=post_page-----291dc9365656--------------------------------)
    [## DIY XGBoost library in less than 200 lines of python'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/diy-xgboost-library-in-less-than-200-lines-of-python-69b6bf25e7d9?source=post_page-----291dc9365656--------------------------------)
    [## 在不到200行Python代码中实现XGBoost库'
- en: XGBoost explained as well as gradient boosting method and HP tuning by building
    your own gradient boosting library for…
  id: totrans-25
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: XGBoost的解释以及梯度提升方法和HP调优，通过构建你自己的梯度提升库来实现…
- en: towardsdatascience.com](/diy-xgboost-library-in-less-than-200-lines-of-python-69b6bf25e7d9?source=post_page-----291dc9365656--------------------------------)
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/diy-xgboost-library-in-less-than-200-lines-of-python-69b6bf25e7d9?source=post_page-----291dc9365656--------------------------------)
- en: 'The modern approach to training a Machine Learning model, and programs in general,
    involves using Differentiable Programming. If you are unfamiliar with the concept,
    I have written an introductory article on the subject:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 现代机器学习模型及程序的训练方法涉及使用可微分编程。如果你对这一概念不熟悉，我已经撰写了一篇介绍性文章：
- en: '[](/differentiable-programming-from-scratch-abba0ebebc1c?source=post_page-----291dc9365656--------------------------------)
    [## Differentiable Programming from Scratch'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/differentiable-programming-from-scratch-abba0ebebc1c?source=post_page-----291dc9365656--------------------------------)
    [## 从零开始的可微分编程'
- en: Last year I had the great opportunity to attend a talk with Yann Lecun, at the
    Facebook Artificial Intelligence…
  id: totrans-29
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 去年我有幸参加了Yann Lecun在Facebook人工智能部门的讲座…
- en: towardsdatascience.com](/differentiable-programming-from-scratch-abba0ebebc1c?source=post_page-----291dc9365656--------------------------------)
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/differentiable-programming-from-scratch-abba0ebebc1c?source=post_page-----291dc9365656--------------------------------)
- en: Essentially, the idea is to identify the parameters to optimize in your model/program
    and use a gradient-based approach to find the optimal parameters. However, for
    this to work, your model must be differentiable with respect to the parameters
    of interest.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 本质上，思想是识别模型/程序中需要优化的参数，并使用基于梯度的方法来找到最优参数。然而，为了使其工作，你的模型必须对感兴趣的参数是可微分的。
- en: Although Ensemble of Decision Trees employ a Gradient-Based approach for training,
    their construction process is not differentiable.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管决策树集成方法采用了基于梯度的方法进行训练，但它们的构建过程却不可微分。
- en: Building Decision Trees is not a differentiable process
  id: totrans-33
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建决策树不是一个可微分的过程
- en: 'A decision tree is a simple yet powerful data structure with numerous applications.
    When training such a structure for regression or classification tasks, we aim
    to identify three main parameters:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树是一种简单而强大的数据结构，具有广泛的应用。当为回归或分类任务训练这样的结构时，我们的目标是识别三个主要参数：
- en: The feature to use for each decision node to minimize the error, which involves
    feature selection.
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 用于每个决策节点的特征，以最小化错误，这涉及特征选择。
- en: The threshold used to split the input data at each node. Data with a selected
    feature value greater than the threshold will be assigned to the right child node,
    while the rest will be assigned to the left child node.
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 用于在每个节点拆分输入数据的阈值。具有所选特征值大于阈值的数据将分配到右子节点，而其余的数据将分配到左子节点。
- en: The value assigned to the leaves of the tree.
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 分配给树叶的值。
- en: The first two steps are non-differentiable. In current implementations of Gradient
    Boosted Trees, the first step requires iterating over features and is non-smooth.
    The second step involves evaluating the gain using the resulting data partition
    and the lists of possible values. This process is also non-differentiable.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 前两步是不可微分的。在当前的梯度提升树实现中，第一步需要对特征进行迭代，并且是不光滑的。第二步涉及使用结果数据分区和可能值列表来评估增益。这个过程也是不可微分的。
- en: Fortunately, in 2019, Popov, Morozov, and Babenko introduced a differentiable
    formulation of Decision Trees.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，2019年，Popov、Morozov 和 Babenko 提出了可微分的决策树公式。
- en: '[](https://arxiv.org/abs/1909.06312?source=post_page-----291dc9365656--------------------------------)
    [## Neural Oblivious Decision Ensembles for Deep Learning on Tabular Data'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://arxiv.org/abs/1909.06312?source=post_page-----291dc9365656--------------------------------)
    [## 神经盲决策集成用于表格数据的深度学习'
- en: Nowadays, deep neural networks (DNNs) have become the main instrument for machine
    learning tasks within a wide range of…
  id: totrans-41
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 如今，深度神经网络（DNNs）已成为广泛范围内机器学习任务的主要工具...
- en: arxiv.org](https://arxiv.org/abs/1909.06312?source=post_page-----291dc9365656--------------------------------)
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: arxiv.org](https://arxiv.org/abs/1909.06312?source=post_page-----291dc9365656--------------------------------)
- en: I highly recommend reading their paper, as it is always fruitful to gather information
    from the source. However, to facilitate comprehension, I have written some Python
    code using Jax.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 我强烈推荐阅读他们的论文，因为从源头获取信息总是有益的。不过，为了方便理解，我用 Jax 编写了一些 Python 代码。
- en: 'The questions addressed in the paper are:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 论文中讨论的问题包括：
- en: How can we reformulate feature selection to make it smooth and differentiable?
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们如何重新表述特征选择，使其平滑且可微分？
- en: How can we reformulate comparison to make it smooth and differentiable?
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们如何重新表述比较，使其平滑且可微分？
- en: Feature selection regularization
  id: totrans-47
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 特征选择正则化
- en: 'As mentioned earlier, feature selection in standard Gradient Boosting algorithms
    is performed using an exhaustive search: we try each feature and keep the one
    that provides the highest gain.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，标准梯度提升算法中的特征选择是通过穷举搜索来进行的：我们尝试每一个特征，并保留提供最高增益的特征。
- en: Instead of this iterative process, we need to reformulate the selection as a
    function that returns the value of the feature ensuring maximal gain.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要将选择重新表述为一个函数，该函数返回确保最大增益的特征值，而不是这种迭代过程。
- en: 'First, let’s assume that the features are gathered in a vector. We need a way
    to extract the value of the selected feature from this array, ideally using vector
    arithmetic. Starting with the assumption that we already know the feature to retain,
    we can achieve this with a simple dot product:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，假设特征被收集在一个向量中。我们需要一种方法来从该数组中提取所选特征的值，理想情况下使用向量运算。从我们已经知道要保留的特征开始，我们可以通过简单的点积来实现：
- en: '[PRE0]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Hence, we can retrieve a specific value from an array of features by multiplying
    all of them by 0, except the one we need to keep, and summing all the products.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们可以通过将所有特征乘以0（除了需要保留的特征），然后求和所有乘积，来从特征数组中提取特定值。
- en: Learning the feature that achieves the highest performance involves learning
    the vector `selection_weights` with the additional constraint that the sum of
    its elements must be equal to 1\. Ideally, this 1 should be concentrated in a
    single element of the vector.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 学习实现最高性能的特征涉及学习向量`selection_weights`，并附加约束条件：其元素之和必须等于1。理想情况下，这1应该集中在向量的单一元素上。
- en: Enforcing this constraint on the norm of the selection vector can be achieved
    using a function. An obvious candidate is the softmax function, which enforces
    the norm of the weight vector to be 1\. However, softmax generates a smooth distribution
    and cannot generate a 1 for the highest weight and 0 for the others.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 对选择向量的范数施加此约束可以通过函数来实现。一个显然的候选者是 softmax 函数，它将权重向量的范数强制为1。然而，softmax 生成的是光滑分布，无法生成1作为最高权重而其他为0。
- en: 'The following code snippet illustrates this:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码片段演示了这一点：
- en: '[PRE1]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Not only is the norm not exactly equal to 1.0, but the distribution is spread
    across all dimensions.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 不仅范数不完全等于1.0，而且分布分布在所有维度上。
- en: 'Therefore, the NODE paper proposes using the entmax function instead:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，NODE 论文建议使用 entmax 函数：
- en: '[PRE2]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: This appears to be a good choice, as it generates the expected result!
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 这似乎是一个不错的选择，因为它生成了预期的结果！
- en: By utilizing the dot product in conjunction with the `entmax` function, we can
    retrieve the value of a given feature.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将点积与`entmax`函数结合使用，我们可以检索给定特征的值。
- en: 'Differentiable feature selection can be as simple as:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 可微特征选择可以简单到如下：
- en: '[PRE3]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: During the training phase, the weights will be trained to learn which feature
    to keep for a given decision.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练阶段，权重将被训练以学习在给定决策中保留哪个特征。
- en: Introducing a threshold
  id: totrans-65
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 引入阈值
- en: The other parameter used in a decision node, which must be learned, is a threshold.
    If the value of the selected feature is greater than or equal to this threshold,
    the prediction follows the right path of the tree. Conversely, if the value is
    strictly lower, the left path is taken.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在决策节点中使用的另一个参数是阈值，这必须被学习。如果所选特征的值大于或等于该阈值，则预测沿着树的右路径进行。相反，如果值严格低于阈值，则采取左路径。
- en: A similar approach can be used to generate a 1 if the value is above the threshold
    or below it, by using a dot product and the *entmax* function, but with a vector
    of size 2\. The first element represents the difference between the selected value
    and the threshold, while the second element is 0.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用类似的方法生成一个1，如果值高于阈值或低于阈值，通过使用点积和*entmax*函数，但使用一个大小为2的向量。第一个元素表示选定值与阈值之间的差异，而第二个元素是0。
- en: Thanks to the *entmax* function, if the value is below the threshold, the first
    element of the vector is negative. When applying the *entmax* function, the 0
    is transformed into a 1, and the negative part becomes equal to 0\. Conversely,
    when the value is greater than the threshold, the first element is 1 and the second
    element is 0.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 借助*entmax*函数，如果值低于阈值，向量的第一个元素是负数。当应用*entmax*函数时，0会被转化为1，负数部分变为0。相反，当值大于阈值时，第一个元素是1，第二个元素是0。
- en: 'Here is an example implementation:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个示例实现：
- en: '[PRE4]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: During the training process, the parameters to be learned are not only the threshold
    but also the weights attached to the right and left leaves of this simple 1-level
    tree.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练过程中，需要学习的参数不仅是阈值，还有附加在这个简单1级树的左右叶子上的权重。
- en: '[![](../Images/646ded91846ff6aa7fa8814985c9837d.png)](https://www.buymeacoffee.com/guillaumes0)'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '[![](../Images/646ded91846ff6aa7fa8814985c9837d.png)](https://www.buymeacoffee.com/guillaumes0)'
- en: '[https://www.buymeacoffee.com/guillaumes0](https://www.buymeacoffee.com/guillaumes0)'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://www.buymeacoffee.com/guillaumes0](https://www.buymeacoffee.com/guillaumes0)'
- en: Next steps
  id: totrans-74
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 下一步
- en: In this first article, we have seen how to regularize the highly non-smooth
    structure of Gradient Boosting, namely the decision tree. This is the minimum
    requirement to transition from the Gradient Boosting method to the Differentiable
    Programming paradigm that underlies Neural Networks.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我们已经看到了如何对梯度提升的高度非光滑结构进行正则化，即决策树。这是从梯度提升方法过渡到支撑神经网络的可微编程范式的最低要求。
- en: 'However, there are still three problems that need to be addressed:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，仍然有三个问题需要解决：
- en: How to extend the method to support multi-level decision trees?
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如何扩展该方法以支持多级决策树？
- en: How to learn the parameters of such trees?
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如何学习这种树的参数？
- en: How can we ensure that we stay in the linear part of the `entmax`function, and
    ensure that gradient does not vanish?
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们如何确保保持在`entmax`函数的线性部分，并确保梯度不会消失？
- en: I will provide further details on these topics in a forthcoming article.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 我将在即将发布的文章中提供这些主题的更多细节。
- en: 'Update — the second article is online:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 更新 — 第二篇文章已上线：
- en: '[](/xgboost-how-deep-learning-can-replace-gradient-boosting-and-decision-trees-part-2-training-b432620750f8?source=post_page-----291dc9365656--------------------------------)
    [## XGBoost: How Deep Learning Can Replace Gradient Boosting and Decision Trees
    — Part 2: Training'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/xgboost-how-deep-learning-can-replace-gradient-boosting-and-decision-trees-part-2-training-b432620750f8?source=post_page-----291dc9365656--------------------------------)
    [## XGBoost: 深度学习如何取代梯度提升和决策树 — 第2部分：训练'
- en: A world without if
  id: totrans-83
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 没有if的世界
- en: towardsdatascience.com](/xgboost-how-deep-learning-can-replace-gradient-boosting-and-decision-trees-part-2-training-b432620750f8?source=post_page-----291dc9365656--------------------------------)
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '[towardsdatascience.com](/xgboost-how-deep-learning-can-replace-gradient-boosting-and-decision-trees-part-2-training-b432620750f8?source=post_page-----291dc9365656--------------------------------)'
- en: 'In the meantime, I you want to improve your mastery of Gradient Boosting, check
    out my book on the subject:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 与此同时，如果你想提高对梯度提升的掌握，可以查看我关于该主题的书籍：
- en: '[](https://amzn.to/3IKyqTF?source=post_page-----291dc9365656--------------------------------)
    [## Practical Gradient Boosting: An deep dive into Gradient Boosting in Python'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://amzn.to/3IKyqTF?source=post_page-----291dc9365656--------------------------------)
    [## 实用的梯度提升：深入了解 Python 中的梯度提升'
- en: This book on Gradient Boosting methods is intended for students, academics,
    engineers, and data scientists who wish to…
  id: totrans-87
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 本书关于梯度提升方法，旨在为希望深入了解这一领域的学生、学者、工程师和数据科学家提供指导…
- en: amzn.to](https://amzn.to/3IKyqTF?source=post_page-----291dc9365656--------------------------------)
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: amzn.to](https://amzn.to/3IKyqTF?source=post_page-----291dc9365656--------------------------------)
