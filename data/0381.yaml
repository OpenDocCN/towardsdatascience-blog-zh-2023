- en: Streaming Big Data Files from Cloud Storage
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从云存储中流式传输大数据文件
- en: 原文：[https://towardsdatascience.com/streaming-big-data-files-from-cloud-storage-634e54818e75?source=collection_archive---------9-----------------------#2023-01-25](https://towardsdatascience.com/streaming-big-data-files-from-cloud-storage-634e54818e75?source=collection_archive---------9-----------------------#2023-01-25)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/streaming-big-data-files-from-cloud-storage-634e54818e75?source=collection_archive---------9-----------------------#2023-01-25](https://towardsdatascience.com/streaming-big-data-files-from-cloud-storage-634e54818e75?source=collection_archive---------9-----------------------#2023-01-25)
- en: Methods for efficient consumption of large files
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 高效处理大型文件的方法
- en: '[](https://chaimrand.medium.com/?source=post_page-----634e54818e75--------------------------------)[![Chaim
    Rand](../Images/c52659c389f167ad5d6dc139940e7955.png)](https://chaimrand.medium.com/?source=post_page-----634e54818e75--------------------------------)[](https://towardsdatascience.com/?source=post_page-----634e54818e75--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----634e54818e75--------------------------------)
    [Chaim Rand](https://chaimrand.medium.com/?source=post_page-----634e54818e75--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://chaimrand.medium.com/?source=post_page-----634e54818e75--------------------------------)[![Chaim
    Rand](../Images/c52659c389f167ad5d6dc139940e7955.png)](https://chaimrand.medium.com/?source=post_page-----634e54818e75--------------------------------)[](https://towardsdatascience.com/?source=post_page-----634e54818e75--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----634e54818e75--------------------------------)
    [Chaim Rand](https://chaimrand.medium.com/?source=post_page-----634e54818e75--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F9440b37e27fe&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fstreaming-big-data-files-from-cloud-storage-634e54818e75&user=Chaim+Rand&userId=9440b37e27fe&source=post_page-9440b37e27fe----634e54818e75---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----634e54818e75--------------------------------)
    ·13 min read·Jan 25, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F634e54818e75&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fstreaming-big-data-files-from-cloud-storage-634e54818e75&user=Chaim+Rand&userId=9440b37e27fe&source=-----634e54818e75---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F9440b37e27fe&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fstreaming-big-data-files-from-cloud-storage-634e54818e75&user=Chaim+Rand&userId=9440b37e27fe&source=post_page-9440b37e27fe----634e54818e75---------------------post_header-----------)
    发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----634e54818e75--------------------------------)
    ·13分钟阅读·2023年1月25日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F634e54818e75&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fstreaming-big-data-files-from-cloud-storage-634e54818e75&user=Chaim+Rand&userId=9440b37e27fe&source=-----634e54818e75---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F634e54818e75&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fstreaming-big-data-files-from-cloud-storage-634e54818e75&source=-----634e54818e75---------------------bookmark_footer-----------)![](../Images/d4e67f787dac3bd4e6d982b7bfd56796.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F634e54818e75&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fstreaming-big-data-files-from-cloud-storage-634e54818e75&source=-----634e54818e75---------------------bookmark_footer-----------)![](../Images/d4e67f787dac3bd4e6d982b7bfd56796.png)'
- en: Photo by [Aron Visuals](https://unsplash.com/@aronvisuals?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由 [Aron Visuals](https://unsplash.com/@aronvisuals?utm_source=medium&utm_medium=referral)
    提供，来源于 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: 'Working with very large files can pose challenges to application developers
    related to efficient resource management and runtime performance. Text file editors,
    for example, can be divided into those that can handle large files, and those
    that make your CPU choke, make your PC freeze, and make you want to scream. These
    challenges are exacerbated when the large files reside in a remote storage location.
    In such cases one must consider the manner in which the files will be pulled to
    the application while taking into account: bandwidth capacity, network latency,
    and the application’s file access pattern. In this post we consider the case in
    which our data application requires access to one or more large files that reside
    in **cloud object storage**. This continues a series of posts on the topic of
    efficient ingestion of data from the cloud (e.g., [here](/training-from-cloud-storage-with-s5cmd-5c8fb5c06056),
    [here](/training-in-pytorch-from-amazon-s3-6156d5342d1), and [here](https://julsimon.medium.com/deep-dive-on-tensorflow-training-with-amazon-sagemaker-and-amazon-s3-12038828075c)).'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 处理非常大的文件可能会给应用程序开发人员带来与高效资源管理和运行时性能相关的挑战。例如，文本文件编辑器可以分为能够处理大文件的编辑器和那些让你的CPU卡顿、让你的PC冻结、让你想尖叫的编辑器。当大型文件存储在远程存储位置时，这些挑战会更加严重。在这种情况下，必须考虑文件如何被拉取到应用程序中，同时考虑：带宽容量、网络延迟和应用程序的文件访问模式。在这篇文章中，我们考虑了我们的数据应用程序需要访问存储在**云对象存储**中的一个或多个大文件的情况。这是关于高效从云中获取数据的一系列文章中的一篇（例如，[这里](/training-from-cloud-storage-with-s5cmd-5c8fb5c06056)，[这里](/training-in-pytorch-from-amazon-s3-6156d5342d1)
    和 [这里](https://julsimon.medium.com/deep-dive-on-tensorflow-training-with-amazon-sagemaker-and-amazon-s3-12038828075c)）。
- en: Before we get started, let’s be clear…when using cloud storage, it is usually
    **not** recommended to work with files that are particularly large. If you are
    working with multiple data files, it is also preferred **not** to choose a file
    size that is particularly small (due to the overhead incurred by numerous requests
    to the storage service). The sweet spot varies across platforms but is usually
    somewhere between a few MBs to a few hundred MBs. (If you rely heavily on cloud
    storage, you may want to design a simple experiment to test this out.) Unfortunately,
    we don’t always have control over the data design process and sometimes have no
    choice but to work with the cards we’ve been dealt.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始之前，让我们明确一下……在使用云存储时，通常**不**建议处理特别大的文件。如果你正在处理多个数据文件，也建议**不要**选择特别小的文件大小（因为对存储服务发出的多个请求会产生额外开销）。最佳文件大小因平台而异，但通常在几MB到几百MB之间。（如果你严重依赖云存储，可能需要设计一个简单的实验来测试这一点。）不幸的是，我们并不总是能控制数据设计过程，有时只能接受现有的条件。
- en: 'Another good practice, especially when working with large files, is to choose
    a format that supports partial file reads — that is, a format that does not require
    ingesting the entire file in order to process any part of it. A few examples of
    such files are:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个好的实践，特别是在处理大文件时，是选择支持部分文件读取的格式——也就是说，选择一种不需要加载整个文件即可处理其任何部分的格式。这类文件的几个例子包括：
- en: a simple text file,
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个简单的文本文件，
- en: a sequential dataset — a dataset containing individual records that are grouped
    into a single file in a sequential manner,
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个顺序数据集——一个包含按顺序分组到单个文件中的单独记录的数据集，
- en: a dataset that is stored in a columnar format, such as [Apache Parquet](https://parquet.apache.org/),
    that is specifically designed to enable loading only selected columns of the data,
    and
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 以列式格式存储的数据集，例如 [Apache Parquet](https://parquet.apache.org/)，这种格式专门设计用于仅加载选定的列，
- en: a video file that is stored in a format that allows playback from any time offset
    (as most formats do).
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 允许从任意时间偏移播放的视频文件（大多数格式都支持这种功能）。
- en: In this post, we assume that our large files allow for partial reads. We will
    consider several options for ingesting the file contents in Python and measure
    how they perform in different application use cases. Although our demonstrations
    will be based on AWS’s object storage service, [Amazon S3](https://aws.amazon.com/s3/),
    most of what we write pertains equally to any other object storage service. Please
    note that our choice of specific services, APIs, libraries, etc., should not be
    viewed as an endorsement of these over any other option. The best solution for
    streaming data from the cloud is very much dependent on the details of your project
    and your environment, and it is highly recommended that you conduct your own in-depth
    analysis before drawing any conclusions.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我们假设我们的大文件允许部分读取。我们将考虑几种在 Python 中读取文件内容的选项，并测量它们在不同应用场景下的表现。尽管我们的演示将基于
    AWS 的对象存储服务 [Amazon S3](https://aws.amazon.com/s3/)，但我们所写的内容同样适用于任何其他对象存储服务。请注意，我们选择的具体服务、API、库等，不应被视为对这些选项的偏好。云端数据流的最佳解决方案很大程度上依赖于项目和环境的细节，强烈建议在得出任何结论之前进行深入分析。
- en: Direct Download from Amazon S3
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从 Amazon S3 直接下载
- en: In this post, we will assume that we are downloading files directly from Amazon
    S3\. However, it should be noted that there a number of services and/or solutions
    that include an intermediate step between the object storage and the application.
    AWS, for example, offers services such as [Amazon FSx](https://aws.amazon.com/fsx/)
    and [Amazon EFS](https://aws.amazon.com/efs/) for mirroring your data in a high-performance
    file system in the cloud. [AI Store](https://github.com/NVIDIA/aistore) offers
    a [kubernetes](https://kubernetes.io/)-based solution for a lightweight storage
    stack adjacent to the data consuming applications. These kinds of solutions might
    alleviate some of the challenges associated with using large files, e.g., they
    may reduce latency and support higher bandwidth. On the other hand, they often
    introduce a bunch of new challenges related to deployment, maintenance, scalability,
    etc. Plus, they cost extra money.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我们将假设我们直接从 Amazon S3 下载文件。然而，需要注意的是，有许多服务和/或解决方案在对象存储和应用程序之间包括一个中间步骤。例如，AWS
    提供了 [Amazon FSx](https://aws.amazon.com/fsx/) 和 [Amazon EFS](https://aws.amazon.com/efs/)
    等服务，将数据镜像到云中的高性能文件系统中。[AI Store](https://github.com/NVIDIA/aistore) 提供了一种基于 [kubernetes](https://kubernetes.io/)
    的解决方案，用于与数据消费应用程序相邻的轻量级存储堆栈。这些解决方案可能会缓解使用大文件时的一些挑战，例如，它们可能会减少延迟并支持更高的带宽。另一方面，它们通常会引入一系列与部署、维护、扩展性等相关的新挑战。此外，它们还会增加额外的费用。
- en: Metrics for Comparative Performance Measurement
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 比较性能测量指标
- en: 'In the next few sections, we will describe different ways of pulling large
    files from Amazon S3 in Python. To compare the behavior of these methods, we will
    use the following metrics:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的几节中，我们将描述在 Python 中从 Amazon S3 拉取大文件的不同方法。为了比较这些方法的行为，我们将使用以下指标：
- en: '**Time to first sample** — how much time does it take until the first sample
    in the file is read.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**首次采样时间** — 读取文件中第一个样本需要多长时间。'
- en: '**Average sequential read time** — what is the average read time per sample
    when iterating sequentially over all of the samples.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**平均顺序读取时间** — 在顺序遍历所有样本时，每个样本的平均读取时间是多少。'
- en: '**Total processing time** — what is the total processing time of the entire
    data file.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**总处理时间** — 整个数据文件的总处理时间是多少。'
- en: '**Average random read time**— what is the average read time when reading samples
    at arbitrary offsets.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**平均随机读取时间** — 在读取任意偏移量的样本时的平均读取时间是多少。'
- en: Different applications will have different preferences as to which of these
    metrics to prioritize. For example, a video streaming application might prioritize
    a low time-to-first-sample in order to improve viewer experience. It will also
    require efficient reads at arbitrary offsets to support features such as fast-forward.
    On the other hand, as long as the average time-per-sample passes a certain threshold
    (e.g., 30 frames per second) optimizing this metric is not important. In contrast,
    a deep learning training application might prioritize reducing the average sequential
    read and total processing time in order to minimize the potential for performance
    bottlenecks in the training flow.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 不同的应用程序将对这些指标的优先级有不同的偏好。例如，视频流应用可能会优先考虑较低的首次样本时间，以提高观众体验。它还需要在任意偏移量处进行高效读取，以支持快进等功能。另一方面，只要平均每样本时间超过某个阈值（例如，每秒
    30 帧），优化此指标就不那么重要。相比之下，深度学习训练应用可能会优先考虑减少平均顺序读取时间和总处理时间，以最小化训练流程中的潜在性能瓶颈。
- en: Toy Example
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 玩具示例
- en: 'To facilitate our discussion, we create a 2 GB binary file and assume that
    the file contains 2,048 data samples, each one being 1 MB in size. The code block
    below includes snippets for: creating a file with random data and uploading it
    to Amazon S3 (using boto3), iterating over all of the samples sequentially, and
    sampling the data at non-sequential file offsets. For this and all subsequent
    code snippets, we assume that your AWS account and local environment have been
    appropriately [configured](https://boto3.amazonaws.com/v1/documentation/api/latest/guide/credentials.html)
    to access Amazon S3.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 为了方便讨论，我们创建了一个 2 GB 的二进制文件，并假设该文件包含 2,048 个数据样本，每个样本大小为 1 MB。下面的代码块包括以下片段：创建一个包含随机数据的文件并将其上传到
    Amazon S3（使用 boto3），按顺序遍历所有样本，以及在非顺序文件偏移量处采样数据。对于此及所有后续代码片段，我们假设您的 AWS 账户和本地环境已被适当地
    [配置](https://boto3.amazonaws.com/v1/documentation/api/latest/guide/credentials.html)
    以访问 Amazon S3。
- en: '[PRE0]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Downloading from S3 to Local Disk
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从 S3 下载到本地磁盘
- en: 'The first option we consider is to download the large file to a local disk
    and then read it from there in the same manner that we would read any other local
    file. There a number of methods for downloading a file to a local disk. The three
    we will evaluate here are: Python boto3 API, AWS CLI, and S5cmd.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 我们考虑的第一个选项是将大型文件下载到本地磁盘，然后以读取任何其他本地文件的方式从那里读取它。有多种方法可以将文件下载到本地磁盘。我们在这里评估的三种方法是：Python
    boto3 API、AWS CLI 和 S5cmd。
- en: '**Boto3 File Download**'
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**Boto3 文件下载**'
- en: The most straightforward way to pull files from Amazon S3 in Python is to use
    the dedicated [Boto3](https://boto3.amazonaws.com/v1/documentation/api/latest/index.html)
    Python library. In the code block below, we show how to define an *S3* *client*
    and use the [download file](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/s3.html#S3.Client.download_file)
    API to pull our file from S3 to a local path. The API takes a [TransferConfig](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/customizations/s3.html#boto3.s3.transfer.TransferConfig)
    object which includes controls for tuning the download behavior. In our example,
    we have left the settings with their default values.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Python 中从 Amazon S3 拉取文件的最直接方法是使用专用的 [Boto3](https://boto3.amazonaws.com/v1/documentation/api/latest/index.html)
    Python 库。在下面的代码块中，我们展示了如何定义一个 *S3* *客户端* 并使用 [下载文件](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/s3.html#S3.Client.download_file)
    API 将文件从 S3 拉取到本地路径。该 API 接受一个 [TransferConfig](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/customizations/s3.html#boto3.s3.transfer.TransferConfig)
    对象，其中包含调节下载行为的控制项。在我们的示例中，我们将设置保留为默认值。
- en: '[PRE1]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: We ran this script (and all subsequent scripts) in our local environment and
    averaged the results over 10 trials. Not surprisingly, the average time-to-first-sample
    was relatively high, ~21.3 seconds. This is due to the fact that we needed to
    wait until the entire file was downloaded before opening it. Once the download
    was completed, the average read time for both sequential and arbitrary samples
    was miniscule, as we would expect from any other local file.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在本地环境中运行了这个脚本（以及所有后续脚本），并将结果在 10 次试验中取了平均值。不出所料，平均首次样本时间相对较高，约为 21.3 秒。这是因为我们需要等待整个文件下载完成后才能打开。一旦下载完成，顺序和任意样本的平均读取时间都非常微小，就像我们从其他本地文件中预期的一样。
- en: Boto3 includes a similar API, [download_fileobj](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/s3.html#S3.Client.download_fileobj),
    for downloading the file directly into memory (e.g., using an [io.BytesIO](https://docs.python.org/3/library/io.html#io.BytesIO)
    object). However, this would generally **not** be recommended when working with
    large files.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: Boto3包含一个类似的API，[download_fileobj](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/s3.html#S3.Client.download_fileobj)，用于将文件直接下载到内存中（例如，使用[io.BytesIO](https://docs.python.org/3/library/io.html#io.BytesIO)对象）。然而，这在处理大文件时通常**不**推荐。
- en: AWS CLI
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: AWS CLI
- en: The [AWS CLI](https://docs.aws.amazon.com/cli/latest/index.html) utility offers
    similar functionality for command line use. AWS CLI is written in Python and uses
    the same underlying APIs as Boto3\. Some developers feel more comfortable with
    this mode of use. The download configuration settings are controlled via the [AWS
    config file](https://docs.aws.amazon.com/cli/latest/topic/s3-config.html).
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '[AWS CLI](https://docs.aws.amazon.com/cli/latest/index.html)工具提供了类似的命令行功能。AWS
    CLI是用Python编写的，使用与Boto3相同的底层API。一些开发者对这种使用方式感到更加舒适。下载配置设置通过[AWS配置文件](https://docs.aws.amazon.com/cli/latest/topic/s3-config.html)进行控制。'
- en: '[PRE2]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Not surprisingly, the results of running this script (with the default configuration
    settings) were nearly identical to the previous Boto3 results.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 不出所料，运行此脚本（使用默认配置设置）的结果与之前的Boto3结果几乎完全相同。
- en: '**S5cmd**'
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**S5cmd**'
- en: We covered the [S5cmd](https://github.com/peak/s5cmd) command line utility at
    length in a [previous post](/training-from-cloud-storage-with-s5cmd-5c8fb5c06056)
    in which we showed its value in downloading hundreds of small files from cloud
    storage in parallel. Contrary to the previous method, S5cmd is written in the
    [Go Programming Language](https://go.dev/) and is thus able to better utilize
    the underlying resources (e.g., CPU cores and TCP connections). Check out [this
    informative blog](https://joshua-robinson.medium.com/s5cmd-for-high-performance-object-storage-7071352cc09d)
    for more details on how S5cmd works and its significant performance advantages.
    The S5cmd [*concurrency*](https://github.com/peak/s5cmd#concurrency)flag allows
    for controlling the download speed. The code block below demonstrates the use
    of S5cmd with the *concurrency* set to 10.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在[之前的文章](/training-from-cloud-storage-with-s5cmd-5c8fb5c06056)中详细介绍了[S5cmd](https://github.com/peak/s5cmd)命令行工具，展示了它在并行从云存储中下载数百个小文件的价值。与之前的方法不同，S5cmd是用[Go编程语言](https://go.dev/)编写的，因此能够更好地利用底层资源（例如，CPU核心和TCP连接）。有关S5cmd如何工作及其显著性能优势的更多细节，请查看[这篇信息丰富的博客](https://joshua-robinson.medium.com/s5cmd-for-high-performance-object-storage-7071352cc09d)。S5cmd的[*concurrency*](https://github.com/peak/s5cmd#concurrency)标志允许控制下载速度。下面的代码块演示了将S5cmd的*concurrency*设置为10的使用方法。
- en: '[PRE3]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Sadly, we were not able to reproduce the overwhelming performance benefit of
    S5cmd that was [previously demonstrated on a 50 GB file](https://joshua-robinson.medium.com/s5cmd-for-high-performance-object-storage-7071352cc09d).
    The average time-to-first-sample was ~23.1 seconds, slightly higher than our previous
    results.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 很遗憾，我们未能再现S5cmd在[50 GB文件上的卓越性能提升](https://joshua-robinson.medium.com/s5cmd-for-high-performance-object-storage-7071352cc09d)。平均首次样本时间约为23.1秒，稍高于我们之前的结果。
- en: '**Multi-part vs. Single-threaded Download**'
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**多线程与单线程下载**'
- en: Each of the previous methods employed multi-part downloading under the hood.
    In multi-part downloading, multiple threads are run in parallel, each of which
    is responsible for downloading a disjoint chunk of the file. Multi-part downloading
    is critical for pulling large files from the cloud in a timely fashion. To demonstrate
    its importance, we reran the Boto3 experiment with the *use_threads* flag set
    to *False*, effectively disabling multi-part downloading. This caused the resultant
    average time-to-first-sample to jump to a whopping 156 seconds.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的每种方法都在后台使用了多线程下载。在多线程下载中，多个线程并行运行，每个线程负责下载文件的一个不重叠的块。多线程下载对于及时从云端拉取大文件至关重要。为了演示其重要性，我们重新进行了Boto3实验，将*use_threads*标志设置为*False*，实际上禁用了多线程下载。这导致结果的平均首次样本时间飙升至156秒。
- en: '**The Art of Prefetching Data Files**'
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**数据文件预取的艺术**'
- en: Prefetching is a common technique that is employed when iterating over multiple
    large files. When prefetching, an application will start to download one or more
    subsequent files in parallel to its processing the current file. In this way the
    application can avoid the download delay for all but the very first file. Effective
    prefetching requires appropriate tuning to reach the most optimal results. Prefetching
    data from the cloud is employed by many frameworks for accelerating the speed
    of data ingestion. For example, both [PyTorch](https://www.google.com/url?sa=t&rct=j&q=&esrc=s&cd=&cad=rja&uact=8&ved=2ahUKEwiHkcyujc_8AhV0g_0HHTphBe8QFnoECAkQAQ&url=https%3A%2F%2Fpytorch.org%2F&usg=AOvVaw2mABY6VbqZdRJYnleMzDSb)
    and [TensorFlow](https://www.google.com/url?sa=t&rct=j&q=&esrc=s&cd=&cad=rja&uact=8&ved=2ahUKEwjzjMS_jc_8AhVvi_0HHWgpBPQQFnoECAkQAQ&url=https%3A%2F%2Fwww.tensorflow.org%2F&usg=AOvVaw0TGZBeXHx2CVPI2FiDZclR)
    support prefetching training-data files for optimizing deep learning training.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 预取是一种常用技术，用于遍历多个大型文件时。当进行预取时，应用程序会开始并行下载一个或多个后续文件，同时处理当前文件。通过这种方式，应用程序可以避免除第一个文件之外的所有文件的下载延迟。有效的预取需要适当的调整以达到最优结果。许多框架通过从云中预取数据来加速数据摄取速度。例如，[PyTorch](https://www.google.com/url?sa=t&rct=j&q=&esrc=s&cd=&cad=rja&uact=8&ved=2ahUKEwiHkcyujc_8AhV0g_0HHTphBe8QFnoECAkQAQ&url=https%3A%2F%2Fpytorch.org%2F&usg=AOvVaw2mABY6VbqZdRJYnleMzDSb)和[TensorFlow](https://www.google.com/url?sa=t&rct=j&q=&esrc=s&cd=&cad=rja&uact=8&ved=2ahUKEwjzjMS_jc_8AhVvi_0HHWgpBPQQFnoECAkQAQ&url=https%3A%2F%2Fwww.tensorflow.org%2F&usg=AOvVaw0TGZBeXHx2CVPI2FiDZclR)都支持预取训练数据文件以优化深度学习训练。
- en: Streaming Data from S3
  id: totrans-48
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从S3流式传输数据
- en: 'Some applications may be willing to compromise on the average read-time-per-sample
    in exchange for a low time-to-first-sample. In this section we demonstrate a Boto3
    option for streaming the file from S3 in such a way that allows us to begin processing
    it before completing the file download. The method we describe involves creating
    a [Linux FIFO pipe](https://www.gnu.org/software/libc/manual/html_node/Pipes-and-FIFOs.html)
    and feeding it into the Boto3 [download_fileobj](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/s3.html#S3.Client.download_fileobj)
    API:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 一些应用程序可能愿意在平均每个样本的读取时间上做出妥协，以换取较低的首样本时间。在这一节中，我们演示了一种Boto3选项，用于从S3流式传输文件，以便在完成文件下载之前就开始处理它。我们描述的方法涉及创建一个[Linux
    FIFO管道](https://www.gnu.org/software/libc/manual/html_node/Pipes-and-FIFOs.html)，并将其传递给Boto3的[download_fileobj](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/s3.html#S3.Client.download_fileobj)
    API：
- en: '[PRE4]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Sure enough, the average time-to-first-sample dropped all the way down to ~2.31
    seconds (from more than 20). On the other hand, both the average time-per-sample
    and the total file processing time increased to ~0.01 seconds and ~24.7 seconds,
    respectively.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 确实，平均首样本时间降至约2.31秒（从超过20秒降下）。另一方面，平均每样本时间和总文件处理时间分别增加到约0.01秒和24.7秒。
- en: Reading Data from Arbitrary Offsets
  id: totrans-52
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从任意偏移量读取数据
- en: Some applications require the ability to read only select portions of the file
    at arbitrary offsets. For these use cases, downloading the entire file can be
    extremely wasteful. Here we show how to download specific [byte-ranges](https://docs.aws.amazon.com/whitepapers/latest/s3-optimizing-performance-best-practices/use-byte-range-fetches.html)
    of the file using the Boto3 [get_object](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/s3.html#S3.Client.get_object)
    data streaming API. The code block below demonstrates the use of the API for both
    streaming the entire file and reading arbitrary data chunks.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 一些应用程序要求能够在任意偏移量处仅读取文件的特定部分。对于这些用例，下载整个文件可能是极其浪费的。在这里，我们展示了如何使用Boto3的[get_object](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/s3.html#S3.Client.get_object)数据流
    API 下载文件的特定[字节范围](https://docs.aws.amazon.com/whitepapers/latest/s3-optimizing-performance-best-practices/use-byte-range-fetches.html)。下面的代码块演示了API在流式传输整个文件和读取任意数据块时的使用。
- en: '[PRE5]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: While this solution results in a time-to-first-sample result of ~1.37 seconds,
    the total file processing time (~119 seconds) renders it unusable for sequential
    reading. Its value is demonstrated by the average time to read arbitrary samples
    — ~0.191 seconds.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管此解决方案的首样本时间结果约为1.37秒，但总文件处理时间（约119秒）使其不适合顺序读取。其价值体现在读取任意样本的平均时间上——约0.191秒。
- en: Note that our example did not take advantage of the fact that the arbitrary
    offsets were predetermined. A real-world application would use this information
    to prefetch file segments and boost performance.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们的示例没有利用任意偏移量是预先确定的这一事实。现实世界中的应用程序将使用这些信息来预取文件段并提升性能。
- en: As mentioned above, pulling large files from the cloud efficiently relies on
    parallel multi-part downloading. One way to implement this is by using the Boto3
    [get_object](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/s3.html#S3.Client.get_object)
    API to read disjoint file chunks in the manner just shown.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 如上所述，从云中高效拉取大型文件依赖于并行多部分下载。实现这一点的一种方法是使用Boto3的[get_object](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/s3.html#S3.Client.get_object)
    API以刚刚展示的方式读取不连续的文件块。
- en: Filtering Data with Amazon S3 Select
  id: totrans-58
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用Amazon S3 Select过滤数据
- en: Sometimes the partial data we are seeking is a small number of rows and/or columns
    from a large file that is stored in a CSV, JSON, or Apache Parquet file format.
    In such cases, we can simply apply an SQL filter using a dedicated service such
    as [Amazon S3 Select](https://docs.aws.amazon.com/AmazonS3/latest/userguide/selecting-content-from-objects.html).
    To run an SQL filter to multiple files, you may consider a service such as [Amazon
    Athena](https://aws.amazon.com/athena/). Both services allow you to limit your
    data retrieval to the specific information you need and avoid high overhead of
    having to pull one or more large files. Be sure to check out the documentation
    to learn more.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，我们所寻求的部分数据是从存储在CSV、JSON或Apache Parquet文件格式的大文件中提取的少量行和/或列。在这种情况下，我们可以简单地使用专用服务，如[Amazon
    S3 Select](https://docs.aws.amazon.com/AmazonS3/latest/userguide/selecting-content-from-objects.html)来应用SQL过滤器。要对多个文件运行SQL过滤器，你可以考虑使用[Amazon
    Athena](https://aws.amazon.com/athena/)。这两种服务都允许你将数据检索限制为所需的特定信息，避免了拉取一个或多个大型文件的高开销。请务必查看文档以了解更多信息。
- en: Mounting S3 Data Using Goofys
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Goofys挂载S3数据
- en: All of the solutions we have covered until now have involved explicitly pulling
    data directly from cloud storage. Other solutions expose the cloud storage access
    to the application as a (POSIX-like) file system. [Goofys](https://github.com/kahing/goofys)
    is a popular [FUSE](https://en.wikipedia.org/wiki/Filesystem_in_Userspace) based
    library for reading from Amazon S3\. The command below demonstrates how to mount
    an S3 bucket to a local file path.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们讨论的所有解决方案都涉及直接从云存储中提取数据。其他解决方案则将云存储访问暴露给应用程序，作为（类似POSIX的）文件系统。[Goofys](https://github.com/kahing/goofys)是一个流行的基于[FUSE](https://en.wikipedia.org/wiki/Filesystem_in_Userspace)的库，用于从Amazon
    S3中读取数据。下面的命令演示了如何将S3桶挂载到本地文件路径。
- en: '[PRE6]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Once the goofys mount has been configured, the application can access the large
    files by pointing to them via the local path, as shown in the code block below:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦配置了goofys挂载，应用程序可以通过指向本地路径的方式访问大型文件，如下面的代码块所示：
- en: '[PRE7]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: The goofys-based solution resulted in a time-to-first-sample of ~1.36 seconds,
    total file processing time of ~27.6 seconds, and an average time of ~0.149 seconds
    for reading arbitrary samples.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 基于goofys的解决方案导致了约1.36秒的首次样本时间、约27.6秒的总文件处理时间和约0.149秒的读取任意样本的平均时间。
- en: It should be noted that under the hood, goofys attempts to optimize the response
    time even at the expense of additional calls to Amazon S3 (e.g., by prefetching
    data chunks even before they are requested). Depending on your setup and details
    of your application’s data access pattern, this could result in slightly higher
    Amazon S3 costs relative to other solutions. Goofys includes a number of settings
    for controlling its behavior, such as the “ — cheap” flag for trading performance
    for potentially lower costs. Note that these controls are applied when defining
    the mount. Contrary to the Boto3 based solutions, goofys does not enable you to
    tune the controls (e.g., chunk size, prefetching behavior, etc.) during runtime
    to support varying data ingestion patterns.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 应该注意的是，在底层，goofys尝试优化响应时间，即使是以增加对Amazon S3的额外调用为代价（例如，预取数据块，即使在它们被请求之前）。根据你的设置和应用程序的数据访问模式的细节，这可能导致相对于其他解决方案，Amazon
    S3成本略有增加。Goofys包括多个设置来控制其行为，例如，“ — cheap”标志用于在性能和潜在较低成本之间进行权衡。请注意，这些控制是在定义挂载时应用的。与基于Boto3的解决方案相反，goofys不允许你在运行时调整控制（例如，块大小、预取行为等），以支持不同的数据摄取模式。
- en: Another point to be aware of is that the portions of data read by goofys are
    cached. Thus, if you read the same portion of data a second time while it is still
    in the cache, the response will be immediate. Keep this in mind when running your
    tests and be sure to reset the goofys mount before each experiment.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个需要注意的点是，goofys读取的数据部分会被缓存。因此，如果在缓存中仍然存在相同的数据部分时再次读取，该响应将是即时的。在运行测试时请记住这一点，并确保在每次实验之前重置goofys挂载。
- en: Be sure to check out the [documentation](https://github.com/kahing/goofys) for
    more details on how goofys works, how to control its behavior, its limitations,
    and more.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 确保查看[文档](https://github.com/kahing/goofys)以获取更多关于goofys如何工作、如何控制其行为、其限制等方面的详细信息。
- en: Comparative Results
  id: totrans-69
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 比较结果
- en: 'The following table summarizes the results of the experiments that we ran:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 下表总结了我们进行的实验结果：
- en: '![](../Images/c94ab478e7b7e84f96c6bf7beb61754c.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c94ab478e7b7e84f96c6bf7beb61754c.png)'
- en: Comparative Results of Pulling 2 GB File from S3 (by Author)
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 从S3拉取2GB文件的比较结果（作者提供）
- en: The results indicate that the best method for ingesting a large data file is
    likely to depend on the specific needs of the application. Based on these results,
    an application that seeks to minimize the time-to-first-sample would benefit most
    from a goofys based solution, while an application that seeks to minimize the
    total processing time would opt for downloading the file to a local disk using
    Boto3.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 结果表明，处理大数据文件的最佳方法可能取决于应用程序的具体需求。根据这些结果，旨在最小化首次样本时间的应用程序将从基于goofys的解决方案中获益最大，而旨在最小化总处理时间的应用程序则会选择使用Boto3将文件下载到本地磁盘。
- en: 'We strongly caution against relying on this table to make any decisions for
    your own applications. The tests were run in a very specific setup with the versions
    of the tools available at the time of this writing. The comparative results are
    likely to vary greatly based on: the setup, the network load, the distance from
    the cloud storage facility, the versions of the utilities used, the resource requirements
    of the application, and more.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 我们强烈建议不要依赖此表格来为你的应用程序做出任何决策。测试是在一个非常特定的环境下进行的，使用的是本文撰写时可用的工具版本。比较结果可能会因以下因素而有很大差异：设置、网络负载、与云存储设施的距离、所用工具的版本、应用程序的资源需求等。
- en: Our results did not account for differences in the utilization of CPU, memory,
    and other system resources between the different solutions. In practice, the load
    on the system resources should be taken into consideration as they may impact
    the application behavior.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的结果没有考虑不同解决方案之间CPU、内存和其他系统资源的使用差异。在实际应用中，系统资源的负载应予以考虑，因为它们可能会影响应用程序的行为。
- en: Be sure to run your own in-depth, use-case driven experiments before making
    any design decisions.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在做出任何设计决策之前，请务必进行自己的深入、基于用例的实验。
- en: Summary
  id: totrans-77
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this post we have covered the topic of pulling big data files from cloud
    storage. We have seen that the best approach will likely vary based on the particular
    needs of the data application in question. Our discussion has highlighted a theme
    that is common across all of our posts covering cloud based services — while *the
    cloud* opens up a wide range of new opportunities for technological development
    and advancement, it also introduces quite a number of unique and exciting challenges,
    and compels us to rethink common application design principles.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我们讨论了从云存储中提取大型数据文件的话题。我们看到，最佳的方法可能会根据数据应用的具体需求而有所不同。我们的讨论突显了一个在所有关于云服务的文章中都存在的主题——尽管*云*为技术发展和进步开辟了广泛的新机会，但它也带来了许多独特而令人兴奋的挑战，并促使我们重新思考常见的应用设计原则。
- en: As usual, comments, questions, and corrections are more than welcome.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 一如既往，欢迎提出评论、问题和更正。
