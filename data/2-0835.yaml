- en: Everything You Should Know About Evaluating Large Language Models
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 你应该知道的关于评估大型语言模型的一切
- en: 原文：[https://towardsdatascience.com/everything-you-should-know-about-evaluating-large-language-models-dce69ef8b2d2](https://towardsdatascience.com/everything-you-should-know-about-evaluating-large-language-models-dce69ef8b2d2)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/everything-you-should-know-about-evaluating-large-language-models-dce69ef8b2d2](https://towardsdatascience.com/everything-you-should-know-about-evaluating-large-language-models-dce69ef8b2d2)
- en: Open Language Models
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 开放语言模型
- en: From perplexity to measuring general intelligence
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从困惑度到衡量一般智能
- en: '[](https://donatoriccio.medium.com/?source=post_page-----dce69ef8b2d2--------------------------------)[![Donato
    Riccio](../Images/0af2a026e72a023db4635522cbca50eb.png)](https://donatoriccio.medium.com/?source=post_page-----dce69ef8b2d2--------------------------------)[](https://towardsdatascience.com/?source=post_page-----dce69ef8b2d2--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----dce69ef8b2d2--------------------------------)
    [Donato Riccio](https://donatoriccio.medium.com/?source=post_page-----dce69ef8b2d2--------------------------------)'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://donatoriccio.medium.com/?source=post_page-----dce69ef8b2d2--------------------------------)[![Donato
    Riccio](../Images/0af2a026e72a023db4635522cbca50eb.png)](https://donatoriccio.medium.com/?source=post_page-----dce69ef8b2d2--------------------------------)[](https://towardsdatascience.com/?source=post_page-----dce69ef8b2d2--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----dce69ef8b2d2--------------------------------)
    [Donato Riccio](https://donatoriccio.medium.com/?source=post_page-----dce69ef8b2d2--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----dce69ef8b2d2--------------------------------)
    ·10 min read·Aug 28, 2023
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----dce69ef8b2d2--------------------------------)
    ·10分钟阅读·2023年8月28日
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/1557949e969c2f9d2a1f22fa5916c168.png)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1557949e969c2f9d2a1f22fa5916c168.png)'
- en: Image generated by the author using Stable Diffusion.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者使用稳定扩散生成。
- en: As open source language models become more readily available, getting lost in
    all the options is easy.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 随着开源语言模型的越来越普及，容易在众多选项中迷失方向。
- en: How do we determine their performance and compare them? And how can we confidently
    say that one model is better than another?
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何确定它们的表现并进行比较？我们如何自信地说一个模型比另一个更好？
- en: This article provides some answers by presenting training and evaluation metrics,
    and general and specific benchmarks to have a clear picture of your model’s performance.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 本文通过提供训练和评估指标、一般和特定基准，来帮助清晰了解你模型的表现。
- en: 'If you missed it, take a look at the first article in the Open Language Models
    series:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你错过了，请查看《开放语言模型》系列中的第一篇文章：
- en: '[](/a-gentle-introduction-to-open-source-large-language-models-3643f5ca774?source=post_page-----dce69ef8b2d2--------------------------------)
    [## A Gentle Introduction to Open Source Large Language Models'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/a-gentle-introduction-to-open-source-large-language-models-3643f5ca774?source=post_page-----dce69ef8b2d2--------------------------------)
    [## 开源大型语言模型的温和介绍'
- en: Why everyone is talking about Llamas, Alpacas, Falcons and other animals
  id: totrans-14
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 为什么大家都在谈论美洲驼、羊驼、猎鹰和其他动物
- en: towardsdatascience.com](/a-gentle-introduction-to-open-source-large-language-models-3643f5ca774?source=post_page-----dce69ef8b2d2--------------------------------)
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '[towardsdatascience.com](/a-gentle-introduction-to-open-source-large-language-models-3643f5ca774?source=post_page-----dce69ef8b2d2--------------------------------)'
- en: Perplexity
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 困惑度
- en: Language models define a probability distribution over a vocabulary of words
    to select the most likely next word in a sequence. Given a text, a language model
    assigns a probability to each word in the language, and the most likely is selected.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 语言模型在词汇表上定义概率分布，以选择序列中最可能的下一个词。给定一段文本，语言模型为语言中的每个词分配一个概率，选择最可能的词。
- en: '**Perplexity** measures how well a language model can predict the next word
    in a given sequence. As a training metric, it shows how well the models learned
    its training set.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '**困惑度** 衡量语言模型在给定序列中预测下一个词的能力。作为一种训练指标，它显示了模型对训练集的学习效果。'
- en: We won’t go into the mathematical details but intuitively, **minimizing perplexity
    means maximizing the predicted probability.**
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不会深入探讨数学细节，但直观上，**最小化困惑度意味着最大化预测概率**。
- en: In other words, the best model is the one that is not *surprised* when it sees
    the new text because it’s expecting it — meaning it already predicted well what
    words are coming next in the sequence.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，最好的模型是当看到新文本时不会感到*惊讶*的，因为它早已预测了序列中接下来会出现的词。
- en: While perplexity is helpful, it doesn’t consider the meaning behind the words
    or the context in which they are used, and it’s influenced by how we tokenize
    our data — different language models with varying vocabularies and tokenization
    techniques can produce varying perplexity scores, making direct comparisons less
    meaningful.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然困惑度很有帮助，但它没有考虑单词背后的意义或使用上下文，并且受到我们如何对数据进行分词的影响 — 不同的语言模型采用不同的词汇和分词技术可能会产生不同的困惑度分数，使得直接比较变得不那么有意义。
- en: '**Perplexity is a useful but limited metric**. We use it primarily to track
    progress during a model’s training or to compare different versions of the same
    model. For instance, after applying quantization — a technique that reduces a
    model’s computational demands — we often use perplexity to assess any changes
    in the model’s quality.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '**困惑度是一个有用但有限的指标**。我们主要用它来跟踪模型训练过程中的进展或比较同一模型的不同版本。例如，在应用量化技术后 — 一种减少模型计算需求的技术
    — 我们通常使用困惑度来评估模型质量的变化。'
- en: Perplexity is just one part of the equation — it offers valuable insights but
    doesn’t tell the whole story. ¹
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 困惑度只是方程式的一部分 — 它提供了有价值的见解，但并不能讲述全部故事。¹
- en: '![](../Images/b8db6afe466fbec575a24d6a365f89c3.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b8db6afe466fbec575a24d6a365f89c3.png)'
- en: Some of the tasks for Large Language Models. Image by the author.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型的一些任务。图片由作者提供。
- en: BLEU and ROUGE
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: BLEU 和 ROUGE
- en: If you’re into Natural Language Processing, you may have heard about the **ROUGE
    and BLEU scores.**
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你对自然语言处理感兴趣，你可能听说过**ROUGE 和 BLEU 分数**。
- en: Introduced in the early 2000s for machine translation, they quantify how close
    the machine text is to a human reference.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 它们在2000年代初期引入，用于机器翻译，量化机器文本与人类参考文本的接近程度。
- en: The **BLEU** score is the number of words in the human reference text divided
    by the total words. Similarly to the precision score, it takes values between
    zero and one, where values closer to one represent more similar texts.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '**BLEU** 分数是人类参考文本中的单词数除以总单词数。类似于精确度分数，它的值介于零和一之间，值越接近一表示文本越相似。'
- en: '**ROUGE** works on similar principles but is a bit more complex since it analyzes
    overlap through several aspects, such as n-grams (ROUGE-N), longest common subsequences
    (ROUGE-L) and skip bigrams. (ROUGE-S)'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '**ROUGE** 基于类似的原理，但由于它通过多个方面（如n-grams（ROUGE-N）、最长公共子序列（ROUGE-L）和跳跃二元组（ROUGE-S））分析重叠，因此略为复杂。'
- en: When it comes to large language models, BLEU and ROUGE are used to evaluate
    how close the output is aligned to the human solution, considered correct. But
    they are not enough for every generative task. As you can imagine,producing the
    reference textcan be expensive and time-consuming and not even feasible for some
    domains or languages.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 对于大型语言模型，BLEU 和 ROUGE 用于评估输出与被认为正确的人类解决方案的接近程度。但它们并不适用于所有生成任务。正如你所想，生成参考文本可能既昂贵又耗时，对于某些领域或语言甚至不可行。
- en: Sometimes there isn’t just one correct way to summarize or translate a text.
    These scores can only account for a few valid options.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，概括或翻译文本并没有唯一的正确方式。这些分数只能考虑一些有效的选项。
- en: Also, **they don’t take into consideration the context** — a text that works
    for a news article might not be the best fit for a social media post, and what’s
    suitable for a formal setting might not be appropriate for a casual one.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，**它们不考虑上下文** — 对于新闻文章有效的文本可能不适合社交媒体帖子，而正式场合适用的内容可能不适合休闲场合。
- en: The need for benchmarks
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基准测试的需求
- en: Open source models are usually smaller and fine-tuned to be more specialized
    for a particular task.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 开源模型通常较小，并经过微调以更专注于特定任务。
- en: Meta’s founder, **Mark Zuckerberg**, thinks we’ll interact with different AI
    entities for different needs instead of relying on a general-purpose AI assistant.²
    To really understand which model best suits a particular task, we need a way to
    compare them.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: Meta的创始人**马克·扎克伯格**认为，我们将根据不同的需求与不同的AI实体互动，而不是依赖通用的AI助手。² 要真正了解哪种模型最适合特定任务，我们需要一种比较它们的方法。
- en: '**Specific benchmarks** assess a particular aspect of a language model. For
    example, if you want to evaluate how truthful your model answers are or quantify
    how well it does on a task after fine-tuning, use a specific benchmark.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '**特定基准**评估语言模型的某个特定方面。例如，如果你想评估你的模型回答的真实性或量化模型在微调后的任务表现，请使用特定基准。'
- en: Four of them are used in [*Hugging’s Face OpenLLM Leaderboard*](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard).
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 其中四个用于[*Hugging’s Face OpenLLM Leaderboard*](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard)。
- en: '**The Abstraction and Reasoning Corpus (ARC)** is an **abstract reasoning test**.
    It applies to humans and AIs and tries to measure a *human-like form of fluid
    intelligence.* Given an input grid, the user needs to choose the correct output.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '**抽象推理语料库（ARC）**是一个**抽象推理测试**。它适用于人类和人工智能，并试图测量一种*类似人类的流动智力*。给定一个输入网格，用户需要选择正确的输出。'
- en: '![](../Images/14d0a1ee6042b9276a4dc0697f9b298e.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/14d0a1ee6042b9276a4dc0697f9b298e.png)'
- en: The ARC test interface. Language models can interact with it through JSON files.³
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: ARC测试接口。语言模型可以通过JSON文件与其交互。³
- en: '**HellaSwag** is a test where the user needs to pick the best ending to a given
    context, a task called **commonsense inference**. While easy for humans, many
    LLMs struggle with this test. The only one able to reach almost human-level performance
    is GPT-4.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '**HellaSwag**是一个测试，用户需要为给定的上下文选择最佳结尾，这一任务称为**常识推理**。虽然对人类来说很容易，但许多LLM在这个测试中表现不佳。唯一能够达到接近人类水平表现的是GPT-4。'
- en: '![](../Images/3d60eefdd92c36f9dbc64f2eeaa7b4a3.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3d60eefdd92c36f9dbc64f2eeaa7b4a3.png)'
- en: An example test in HellaSwag.⁴
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: HellaSwag中的一个示例测试。⁴
- en: '**Massive Multitask Language Understanding (MMLU)** measures a text model’s
    multitask accuracy on 57 tasks, including mathematics, US history, computer science,
    law, and more. The test looks like multiple choice questions on different problems
    and assesses **understanding of the world and general knowledge.**'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '**大规模多任务语言理解（MMLU）**测量文本模型在57个任务上的多任务准确性，包括数学、美国历史、计算机科学、法律等。测试看起来像是不同问题的选择题，并评估**对世界和常识的理解**。'
- en: '![](../Images/2eea9088510f8fe88d7605b22647c90d.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2eea9088510f8fe88d7605b22647c90d.png)'
- en: Measuring Massive Multitask Language Understanding. ⁵
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 测量大规模多任务语言理解。⁵
- en: '**TruthfulQA** consists of two tasks: generation and multiple-choice. The generation
    task requires models to produce authentic and informative answers to questions,
    while the multiple-choice one requires models to select or assign probabilities
    to true and false answer choices. The benchmark covers 57 topics and uses various
    metrics to measure the models’ **ability to recognize false information**. Interestingly,
    the paper shows that larger models are less truthful. ⁶'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '**TruthfulQA**包括两个任务：生成和多项选择。生成任务要求模型产生真实和有信息的答案，而多项选择任务要求模型选择或分配真实和虚假答案选项的概率。该基准测试涵盖57个主题，并使用各种指标来测量模型**识别虚假信息的能力**。有趣的是，论文显示较大的模型不太真实。⁶'
- en: '![](../Images/f0f66f040f86a5563b17fec762908a3b.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f0f66f040f86a5563b17fec762908a3b.png)'
- en: The models are measured ontheir ability to recognize false information.⁶
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 这些模型的衡量标准是它们识别虚假信息的能力。⁶
- en: Measuring code generation abilities
  id: totrans-51
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 测量代码生成能力
- en: When ChatGPT came out, asking it to write some code is probably the first thing
    we’ve all tried. The ability to code is one of the most useful and time-saving
    skills that LLMs can offer to us.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 当ChatGPT刚推出时，要求它写一些代码可能是我们都尝试的第一件事。编码能力是LLM可以提供给我们的最有用且节省时间的技能之一。
- en: In the open source landscape there are many models specialized in code generation,
    like **Wizard Coder** or the most recent **Code LLama.**
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在开源领域，有许多专门从事代码生成的模型，如**Wizard Coder**或最新的**Code LLama**。
- en: 'To show the impressive coding abilities of their new **Code Llama** model,
    they chose two code-specific benchmarks: [**HumanEval**](https://github.com/openai/human-eval)and
    **Mostly Basic Python Programming (**[**MBPP**](https://github.com/google-research/google-research/tree/master/mbpp)**)**,
    complemented by a human evaluation.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 为了展示其新**Code Llama**模型令人印象深刻的编码能力，他们选择了两个特定于代码的基准测试：[**HumanEval**](https://github.com/openai/human-eval)和**Mostly
    Basic Python Programming (**[**MBPP**](https://github.com/google-research/google-research/tree/master/mbpp)**)**，并辅以人工评估。
- en: In the first, models need to generate a code starting from a docstring, while
    in the second they start from a text prompt.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一个基准测试中，模型需要从文档字符串生成代码，而在第二个基准测试中，模型从文本提示开始。
- en: Every prompt comes with one or more unit tests to evaluate the correctness of
    the output.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 每个提示都会附带一个或多个单元测试，以评估输出的正确性。
- en: '![](../Images/66183ff25f6b5e4d7f8aeeb2daf4674f.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/66183ff25f6b5e4d7f8aeeb2daf4674f.png)'
- en: An MBPP entry. [Source.](https://github.com/google-research/google-research/blob/master/mbpp/sanitized-mbpp.json)
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: MBPP条目。 [来源。](https://github.com/google-research/google-research/blob/master/mbpp/sanitized-mbpp.json)
- en: After collecting a sample of **k** entries generated by the model, the **pass@k
    metric** is computed. If at least one entry passes the unit tests, the solution
    is considered correct. For example, **a pass@1 score of 67.0 means the model can
    solve 67% of the problems at the first try.**
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在收集到模型生成的**k**个样本后，计算**pass@k指标**。如果至少一个样本通过了单元测试，则认为解决方案正确。例如，**pass@1分数为67.0意味着模型在第一次尝试中可以解决67%的问题。**
- en: When computing this metric, you can use any value of **k**. But in practice,
    we are interested in the **pass@1**. If you have to keep trying to get a correct
    solution, how can you trust that model?
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 计算此指标时，你可以使用任何**k**的值。但实际上，我们关注的是**pass@1**。如果你必须不断尝试才能获得正确的解决方案，你如何相信那个模型？
- en: The evaluation results for **Code LLama** are the following.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '**Code LLama**的评估结果如下。'
- en: '![](../Images/1041feff23bbd2b453ee4f3ef6d97865.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1041feff23bbd2b453ee4f3ef6d97865.png)'
- en: Code Llama evaluation. ⁷
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: Code Llama评估。⁷
- en: Their results show that GPT-4 is the best model, able to solve 67% of the tasks
    in **HumanEval** at the first try. However, **Code Llama is the best open source
    code-specific model, with just 34B parameters.**
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 他们的结果显示，GPT-4是最好的模型，能够在**HumanEval**中第一次尝试解决67%的任务。然而，**Code Llama是最好的开源代码专用模型，只有34B参数。**
- en: Measuring general intelligence
  id: totrans-65
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 测量通用智能
- en: Evaluation systems must cover numerous scenarios, especially for larger Language
    Models designed to be general purpose because of their impressive generalization
    ability to diverse tasks.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 评估系统必须覆盖众多场景，尤其是针对大型通用语言模型，因为它们在处理多样任务时表现出令人印象深刻的泛化能力。
- en: While for classic machine learning models, you are used to evaluating the model
    using a *test set*, LLMs enable *zero-shot learning* and *few-shot learning —*
    an LLM can learn to perform a task that hasn’t been explicitly trained for. Under
    these circumstances, using a test set or a single metric to benchmark LLM’s capabilities
    is insufficient.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 对于经典的机器学习模型，你习惯使用*测试集*来评估模型，而LLMs使*零样本学习*和*少样本学习*成为可能——LLM可以学习执行没有明确训练过的任务。在这种情况下，使用测试集或单一指标来基准LLM的能力是不够的。
- en: '**General benchmarks** are extensive collections of tests in diverse scenarios
    and tasks. They’re like the ultimate test for your model, aiming to gauge every
    aspect of intelligence.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '**通用基准**是各种场景和任务的广泛测试集合。它们像是对模型的终极测试，旨在评估智能的每个方面。'
- en: 'Some of them are the **Holistic Evaluation of Language Models (HELM), built
    to evaluate** models based on seven key metrics: accuracy, calibration and uncertainty,
    robustness, fairness, bias and stereotypes, toxicity, and efficiency, calculated
    in 16 scenarios.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 一些评估体系如**语言模型的整体评估（HELM），旨在根据七个关键指标评估**模型：准确性、校准和不确定性、稳健性、公平性、偏见和刻板印象、毒性和效率，计算涉及16种场景。
- en: '![](../Images/48761bd94d5f661f7a1a6fee5d8953c7.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/48761bd94d5f661f7a1a6fee5d8953c7.png)'
- en: Holistic Evaluation of Language Models.⁸
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 语言模型的整体评估。⁸
- en: '**SuperGLUE**, introduced in 2019, is an advanced version of the General Language
    Understanding Evaluation (GLUE) test. The GLUE benchmark comprises nine tasks
    related to sentence or sentence-pair language understanding, all built on pre-existing
    datasets. **SuperGLUE** offers a more challenging set of tasks and a public leaderboard.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '**SuperGLUE**，于2019年推出，是通用语言理解评估（GLUE）测试的高级版本。GLUE基准包括九项与句子或句子对语言理解相关的任务，所有任务都建立在现有数据集上。**SuperGLUE**提供了一组更具挑战性的任务和一个公开的排行榜。'
- en: '**BIG-bench,** from Google, expands GLUE and SuperGLUE with a more extensive
    collection of natural language understanding tasks. It is a massive collaborative
    project with contributions from 444 authors from 132 institutions worldwide. It
    assesses LLMs based on their accuracy, fluency, creativity, and generalization
    abilities on [over 200 tasks](https://github.com/google/BIG-bench/blob/main/bigbench/benchmark_tasks/keywords_to_tasks.md#summary-table)!'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '**BIG-bench**，来自谷歌，扩展了GLUE和SuperGLUE，包含了更多自然语言理解任务的广泛集合。这是一个庞大的协作项目，由来自132个机构的444位作者贡献。它基于[超过200个任务](https://github.com/google/BIG-bench/blob/main/bigbench/benchmark_tasks/keywords_to_tasks.md#summary-table)评估LLMs的准确性、流畅性、创造力和泛化能力！'
- en: Since running BIG-bench can be very time-consuming, the authors also provide
    a lite version with a subset of 24 tasks called BIG-bench lite. Their GitHub repo
    is open for contributions and new ideas.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 由于运行BIG-bench可能非常耗时，作者还提供了一个包含24个任务的轻量版，称为BIG-bench lite。他们的GitHub仓库开放供贡献和新想法。
- en: '![](../Images/fe51824a9ddcda3ce58ea568cafbf8a1.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fe51824a9ddcda3ce58ea568cafbf8a1.png)'
- en: Diversity and scale of BIG-bench tasks.⁹
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: BIG-bench 任务的多样性和规模。⁹
- en: Another way of evaluating language models is a manual **human evaluation**.
    As the name suggests, it measures the quality and performance of large language
    models by asking human judges to rate or compare the outputs of LLMs, like in
    **Chatbot Arena**. It’s a platform for benchmarking large language models (LLMs)
    using the Elo rating system — like chess — where users chat with two anonymized
    LLMs side-by-side and vote for the one they think is best. The votes are then
    used to calculate the ELO ratings and rank the LLMs on a leaderboard.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 评估语言模型的另一种方法是手动 **人工评估**。顾名思义，它通过要求人工评审员对 LLM 的输出进行评分或比较来衡量大型语言模型的质量和性能，类似于
    **Chatbot Arena**。这是一个使用 Elo 评分系统（类似于国际象棋）对大型语言模型（LLMs）进行基准测试的平台，用户可以并排与两个匿名 LLM
    进行对话，并投票选择他们认为最好的一个。然后，这些投票用于计算 ELO 评分，并在排行榜上对 LLM 进行排名。
- en: You can visit [their website](https://chat.lmsys.org/) and chat with different
    LLMs yourself.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以访问 [他们的网站](https://chat.lmsys.org/) 并与不同的 LLM 进行对话。
- en: '![](../Images/5416b49cc003e19491c9faffc0905bc4.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5416b49cc003e19491c9faffc0905bc4.png)'
- en: Chatbot Arena [leaderboard.](https://chat.lmsys.org/)
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: Chatbot Arena [排行榜](https://chat.lmsys.org/)
- en: 'A case from research: Llama 2'
  id: totrans-81
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 研究案例：Llama 2
- en: '**Llama 2** is the successor to Llama. It was released in July 2023 in 7B,
    13B, 34B and 70B sizes, including a fine-tuned versions called **Llama 2 Chat.**'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '**Llama 2** 是 Llama 的继任者。它于 2023 年 7 月发布，具有 7B、13B、34B 和 70B 的不同规模，包括一个微调版本，称为
    **Llama 2 Chat**。'
- en: 'In the paper, we can find two main evaluation procedures: a **general** and
    a **safety** evaluation.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 论文中，我们可以找到两种主要的评估程序：**综合评估**和**安全评估**。
- en: '![](../Images/94b3490d50e78389ac1abdfb88e555e0.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/94b3490d50e78389ac1abdfb88e555e0.png)'
- en: General evaluation. LLama 2 is an improvement of LLama, and scores better than
    MPT and Falcon.¹⁰
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 综合评估。Llama 2 是对 Llama 的改进，得分优于 MPT 和 Falcon。¹⁰
- en: The evaluation criteria in the authors’ work suggest that they prioritized two
    main objectives.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 作者工作的评估标准表明，他们优先考虑了两个主要目标。
- en: 'First, **compare Llama 2 to the first version and the open source competitors.**
    To achieve that, they used a comprehensive general evaluation, where the models
    are evaluated on five dimensions: **Code, Commonsense Reasoning, World Knowledge,
    Reading Comprehension** and **Math**. Each dimension is an average of multiple
    benchmarks.'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，**将 Llama 2 与第一个版本以及开源竞争对手进行比较。** 为了实现这一目标，他们使用了全面的通用评估，其中模型在五个维度上进行评估：**代码、常识推理、世界知识、阅读理解**和**数学**。每个维度是多个基准测试的平均值。
- en: The results are complemented by the **MMLU, BBH (BigBench Hard), and AGI Eval
    benchmarks,** shown in separate columns.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 结果还通过 **MMLU、BBH（BigBench Hard）和 AGI Eval 基准测试** 进行补充，这些基准测试在单独的列中显示。
- en: The second objective evident in the authors’ work was to **show that their fine-tuning
    method led to a more truthful and less toxic model.**
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 作者工作的第二个目标显而易见，即 **展示他们的微调方法使模型更真实且有害性更低。**
- en: '![](../Images/dfa33942088c45fa0d30d7d2f2ac30b6.png)![](../Images/4f049abf17b90b2e6a3eabecc334c478.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/dfa33942088c45fa0d30d7d2f2ac30b6.png)![](../Images/4f049abf17b90b2e6a3eabecc334c478.png)'
- en: LLMs before fine tuning (left) and after. (right) ¹⁰
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 微调前的 LLM（左）和微调后的 LLM（右）¹⁰
- en: The safety evaluation is aimed to assess **Truthfulness** and **Toxicity** using
    the **TruthfulQA** and **ToxiGen** benchmarks.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 安全评估旨在使用 **TruthfulQA** 和 **ToxiGen** 基准测试来评估 **真实性** 和 **有害性**。
- en: It shows that thanks to the fine-tuning process, **Llama 2 is less toxic than
    other models but less truthful than ChatGPT.**
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 结果表明，得益于微调过程，**Llama 2 比其他模型的有害性更低，但比 ChatGPT 的真实性略逊。**
- en: Conclusion
  id: totrans-94
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: Language models have a multifaceted and flexible nature.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 语言模型具有多方面和灵活的特性。
- en: Open-source models offer tailored solutions, and specialization might be the
    way forward.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 开源模型提供了量身定制的解决方案，专业化可能是未来的发展方向。
- en: When comparing models, look for benchmarks relevant to your needs.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在比较模型时，请寻找与您的需求相关的基准测试。
- en: The best one isn’t necessarily the one with the lowest perplexity or highest
    BLEU score, but the one that truly adds value to your life.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 最好的模型不一定是困惑度最低或 BLEU 分数最高的模型，而是那个真正为您的生活增添价值的模型。
- en: '*If you enjoyed this article, join* [***Text Generation***](https://textgeneration.substack.com/)
    *— our newsletter has two weekly posts with the latest insights on Generative
    AI and Large Language Models.*'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '*如果您喜欢这篇文章，请加入* [***Text Generation***](https://textgeneration.substack.com/)
    *——我们的新闻通讯每周发布两篇文章，提供有关生成式 AI 和大型语言模型的最新见解。*'
- en: '*Also, you can find me on* [***LinkedIn***](https://www.linkedin.com/in/driccio/)***.***'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '*此外，您还可以在* [***LinkedIn***](https://www.linkedin.com/in/driccio/)***上找到我。***'
- en: References
  id: totrans-101
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考资料
- en: ¹ M. Shoeybi and R. Caruana, [Language Model Evaluation Beyond Perplexity](https://arxiv.org/abs/2106.00085)
    (2023), arXiv.org
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: ¹ M. Shoeybi 和 R. Caruana，[语言模型评估超越困惑度](https://arxiv.org/abs/2106.00085) (2023),
    arXiv.org
- en: '² Lex Friedman Podcast, [Mark Zuckerberg: The Future of AI](https://www.youtube.com/watch?v=Ff4fRgnuFgQ&t=2723s)
    (2023), YouTube.'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: ² Lex Friedman Podcast，[马克·扎克伯格：人工智能的未来](https://www.youtube.com/watch?v=Ff4fRgnuFgQ&t=2723s)
    (2023), YouTube.
- en: '³ Xu, Y., Li, W., Vaezipoor, P., Sanner, S., & Khalil, E. B., [LLMs and the
    Abstraction and Reasoning Corpus: Successes, Failures, and the Importance of Object-based
    Representations.](https://arxiv.org/abs/2305.18354) (2023), arXiv.org'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: ³ Xu, Y., Li, W., Vaezipoor, P., Sanner, S., & Khalil, E. B., [LLMs 与抽象与推理语料库：成功、失败以及基于对象的表示的重要性。](https://arxiv.org/abs/2305.18354)
    (2023), arXiv.org
- en: '⁴ Zellers, R. et al, [HellaSwag: Can a Machine Really Finish Your Sentence?](https://arxiv.org/abs/1905.07830)
    (2022), arXiv.org'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '⁴ Zellers, R. 等人，[HellaSwag: 机器真的能完成你的句子吗？](https://arxiv.org/abs/1905.07830)
    (2022), arXiv.org'
- en: ⁵ Hendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M., Song, D., & Steinhardt,
    J. [Measuring Massive Multitask Language Understanding.](https://arxiv.org/abs/2009.03300)
    (2021), arXiv.org
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: ⁵ Hendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M., Song, D., & Steinhardt,
    J. [测量大规模多任务语言理解。](https://arxiv.org/abs/2009.03300) (2021), arXiv.org
- en: '⁶ Lin, S., Hilton, J., & Evans, O. (2021). [TruthfulQA: Measuring How Models
    Mimic Human Falsehoods.](https://arxiv.org/abs/2109.07958) (2022), arXiv.org'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '⁶ Lin, S., Hilton, J., & Evans, O. (2021). [TruthfulQA: 测量模型如何模仿人类的虚假信息。](https://arxiv.org/abs/2109.07958)
    (2022), arXiv.org'
- en: ⁷ [Introducing Code Llama, a state-of-the-art large language model for coding](https://ai.meta.com/blog/code-llama-large-language-model-coding/).
    (2023), meta.com
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: ⁷ [介绍 Code Llama，一种最先进的大型语言模型用于编码](https://ai.meta.com/blog/code-llama-large-language-model-coding/)。
    (2023), meta.com
- en: ⁸ Liang, P. et al (2022). [Holistic Evaluation of Language Models](https://arxiv.org/abs/2211.09110).
    (2022), arXiv.org
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: ⁸ Liang, P. 等人 (2022)。[语言模型的整体评估](https://arxiv.org/abs/2211.09110)。 (2022),
    arXiv.org
- en: '⁹ Srivastava, A. et al. [Beyond the Imitation Game: Quantifying and extrapolating
    the capabilities of language models.](https://arxiv.org/abs/2206.04615) (2022),
    arXiv.org'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: ⁹ Srivastava, A. 等人。[超越模仿游戏：量化和外推语言模型的能力。](https://arxiv.org/abs/2206.04615)
    (2022), arXiv.org
- en: '¹⁰ Touvron, H. et al. (2023). [Llama 2: Open Foundation and Fine-Tuned Chat
    Models](https://arxiv.org/abs/2307.09288). (2023), arXiv.org'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: ¹⁰ Touvron, H. 等人 (2023)。[Llama 2：开放基础和微调的聊天模型](https://arxiv.org/abs/2307.09288)。
    (2023), arXiv.org
