- en: How to Chat With Any PDFs and Image Files Using Large Language Models — With
    Code
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何使用大型语言模型与任何PDF和图像文件进行聊天 — 带代码
- en: 原文：[https://towardsdatascience.com/how-to-chat-with-any-file-from-pdfs-to-images-using-large-language-models-with-code-4bcfd7e440bc](https://towardsdatascience.com/how-to-chat-with-any-file-from-pdfs-to-images-using-large-language-models-with-code-4bcfd7e440bc)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/how-to-chat-with-any-file-from-pdfs-to-images-using-large-language-models-with-code-4bcfd7e440bc](https://towardsdatascience.com/how-to-chat-with-any-file-from-pdfs-to-images-using-large-language-models-with-code-4bcfd7e440bc)
- en: Complete guide to building an AI assistant that can answer questions about any
    file
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 完整指南，教你如何构建一个可以回答任何文件问题的AI助手
- en: '[](https://zoumanakeita.medium.com/?source=post_page-----4bcfd7e440bc--------------------------------)[![Zoumana
    Keita](../Images/34a15c1d03687816dbdbc065f5719f80.png)](https://zoumanakeita.medium.com/?source=post_page-----4bcfd7e440bc--------------------------------)[](https://towardsdatascience.com/?source=post_page-----4bcfd7e440bc--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----4bcfd7e440bc--------------------------------)
    [Zoumana Keita](https://zoumanakeita.medium.com/?source=post_page-----4bcfd7e440bc--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://zoumanakeita.medium.com/?source=post_page-----4bcfd7e440bc--------------------------------)[![Zoumana
    Keita](../Images/34a15c1d03687816dbdbc065f5719f80.png)](https://zoumanakeita.medium.com/?source=post_page-----4bcfd7e440bc--------------------------------)[](https://towardsdatascience.com/?source=post_page-----4bcfd7e440bc--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----4bcfd7e440bc--------------------------------)
    [Zoumana Keita](https://zoumanakeita.medium.com/?source=post_page-----4bcfd7e440bc--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----4bcfd7e440bc--------------------------------)
    ·9 min read·Aug 5, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----4bcfd7e440bc--------------------------------)
    ·9分钟阅读·2023年8月5日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: Introduction
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: So much valuable information is trapped in PDF and image files. Luckily, we
    have these powerful brains capable of processing those files to find specific
    information, which in fact is great.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: PDF和图像文件中困藏着如此宝贵的信息。幸运的是，我们有这些强大的大脑，能够处理这些文件以找到特定信息，这实际上非常棒。
- en: But how many of us, deep inside wouldn’t like to have a tool that can answer
    any question about a given document?
  id: totrans-8
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 但我们中有多少人，内心深处并不希望拥有一个能回答关于给定文档的任何问题的工具呢？
- en: That is the whole purpose of this article. I will explain step-by-step how to
    build a system that can chat with any PDFs and image files.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是本文的全部目的。我将逐步解释如何构建一个能够与任何PDF和图像文件聊天的系统。
- en: 'If you prefer to watch video instead, check the link below:'
  id: totrans-10
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 如果你更愿意观看视频，请查看下面的链接：
- en: The video format of the article
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 文章的视频格式
- en: General Workflow of the project
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 项目的总体工作流程
- en: It’s always good to have a clear understanding of the main components of the
    system being built. So let’s get started.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 清楚了解系统的主要组件总是好的。那么，让我们开始吧。
- en: '![](../Images/84a8f95dccd53c58aa61e24bcae40112.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/84a8f95dccd53c58aa61e24bcae40112.png)'
- en: End-to-end workflow of the overall chat system (Image by Author)
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 整个聊天系统的端到端工作流程（作者提供的图像）
- en: First, the user submits the document to be processed, which can be in PDF or
    image format.
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 首先，用户提交需要处理的文档，可以是PDF或图像格式。
- en: A second module is used to detect the format of the file so that the relevant
    content extraction function is applied.
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第二个模块用于检测文件格式，以便应用相关的内容提取功能。
- en: The content of the document is then split into multiple chunks using the `Data
    Splitter` module.
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然后，文档的内容使用`Data Splitter`模块被拆分成多个块。
- en: Those chunks are finally transformed into embeddings using the `Chunk Transformer`
    before they are stored in the vector store.
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这些块最终通过`Chunk Transformer`转换为嵌入，然后存储在向量存储中。
- en: At the end of the process, the user’s query is used to find relevant chunks
    containing the answer to that query, and the result is returned as a JSON to the
    user.
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在处理结束时，用户的查询被用来找到包含该查询答案的相关块，结果以JSON格式返回给用户。
- en: 1\. Detect document type
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1\. 检测文档类型
- en: For each input document, specific processing is applied depending on its type,
    whether it is `PDF` , or `image.`
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个输入文档，根据其类型应用特定的处理，无论是`PDF`还是`image.`
- en: This can be achieved with the helper function `detect_document_type` combined
    with the `guess` function from the built-in Python module.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 这可以通过结合使用`detect_document_type`的辅助函数和内置Python模块中的`guess`函数来实现。
- en: '[PRE0]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Now we can test the function on two types of documents:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以在两种类型的文档上测试这个功能：
- en: '`transformer_paper.pdf` is the Transformers research paper [from Arxiv](https://arxiv.org/pdf/1706.03762.pdf).'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`transformer_paper.pdf` 是 Transformers 研究论文 [来自 Arxiv](https://arxiv.org/pdf/1706.03762.pdf)。'
- en: '`zoumana_article_information.png` is the image document containing information
    about the main topics I have covered on Medium.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`zoumana_article_information.png` 是包含有关我在 Medium 上所涵盖的主要主题信息的图像文档。'
- en: '[PRE1]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Output:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：
- en: '![](../Images/0f6456b0157ceb2106c561fe6d04c038.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0f6456b0157ceb2106c561fe6d04c038.png)'
- en: Files types successfully detected (Image by Author)
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 文件类型成功检测（图片由作者提供）
- en: Both file type is successfully detected by the `detect_document_type` function.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '`detect_document_type` 函数成功检测了这两种文件类型。'
- en: 2\. Extract content based on document type
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2\. 基于文档类型提取内容
- en: The `[langchain](https://python.langchain.com/docs/get_started/introduction.html)`
    library provides different modules to extract the content of a given type of document.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '`[langchain](https://python.langchain.com/docs/get_started/introduction.html)`
    库提供了不同的模块来提取特定类型文档的内容。'
- en: '`UnstructuredImageLoader` extracts image content.'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`UnstructuredImageLoader` 提取图像内容。'
- en: '`UnstructuredFileLoader` extracts the content of any pdf and Txt files.'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`UnstructuredFileLoader` 提取任何 pdf 和 Txt 文件的内容。'
- en: We can combine these modules and the above `detect_document_type` function to
    implement the text extraction logic.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将这些模块与上述`detect_document_type`函数结合起来，实现文本提取逻辑。
- en: These modules can be used to implement end-to-end text extraction logic within
    the `extract_file_content` function.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 这些模块可以用于在`extract_file_content`函数中实现端到端的文本提取逻辑。
- en: Let’s see them in action! 🔥
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看它们的实际效果！ 🔥
- en: '[PRE2]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Now, let’s print the first `400` characters of each file content.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们打印每个文件内容的前 `400` 个字符。
- en: '[PRE3]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Output:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：
- en: 'The first 400 characters of each of the above documents are shown below:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 以上每个文档的前 400 个字符如下：
- en: The research paper content starts with `Provided proper attribution is provided`
    and ends with `Jacod Uszkoreit* Google Research usz@google.com.`
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 研究论文的内容以`Provided proper attribution is provided` 开始，以`Jacod Uszkoreit* Google
    Research usz@google.com.` 结束。
- en: The image document’s content starts with `This document provides a quick summary`
    and ends with `Data Science section covers basic to advance concepts.`
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图像文档的内容以`This document provides a quick summary` 开始，以`Data Science section covers
    basic to advance concepts.` 结束。
- en: '![](../Images/c03c271f7d3bd2cb53d17f3989ac0dd5.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c03c271f7d3bd2cb53d17f3989ac0dd5.png)'
- en: First 400 characters of the Transformers paper and the Article Information document
    (Image by Author)
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: Transformers 论文和文章信息文档的前 400 个字符（图片由作者提供）
- en: 3\. Chat Implementation
  id: totrans-49
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3\. 聊天实现
- en: The input document is broken into chunks, then an embedding is created for each
    chunk before implementing the question-answering logic.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 输入文档被分成块，然后为每个块创建嵌入，之后实现问答逻辑。
- en: '**a. Document chunking**'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '**a. 文档分块**'
- en: The chunks represent smaller segments of a larger piece of text. This process
    is essential to ensure that a piece of content is represented with as little noise
    as possible, making it semantically relevant.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 这些块代表了较大文本的一小段。这一过程对于确保内容尽可能少的噪音，并保持语义相关性至关重要。
- en: Multiple chunking strategies can be applied. For instance, we have the `NLTKTextSplitter`
    , `SpacyTextSplitter` , `RecursiveCharacterTextSplitter` , `CharacterTextSplitter`
    and more.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 可以应用多种分块策略。例如，我们有 `NLTKTextSplitter`、`SpacyTextSplitter`、`RecursiveCharacterTextSplitter`、`CharacterTextSplitter`
    等。
- en: Each one of these strategies has its own pros and cons.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 这些策略各有优缺点。
- en: The main focus of this article is made on the `CharacterTextSplitter` which
    creates chunks from the input documents based on `\n\n` , and measure each chunk's
    length (`length_function` ) by its number of characters.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 本文的重点是 `CharacterTextSplitter`，它根据 `\n\n` 从输入文档中创建块，并通过字符数量（`length_function`）来衡量每个块的长度。
- en: '[PRE4]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The `chunk_size` tells that we want a maximum of 1000 characters in each chunk,
    and a smaller value will result in more chunks, while a larger one will generate
    fewer chunks.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '`chunk_size` 指定我们希望每个块最多包含 1000 个字符，而较小的值将导致更多的块，而较大的值将生成更少的块。'
- en: It is important to note that the way the `chunk_size` is chosen can affect the
    overall result. So, a good approach is to try different values and chose the one
    that better fits one's use case.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的是，`chunk_size` 的选择方式会影响整体结果。因此，一个好的方法是尝试不同的值，选择最适合自己用例的那个。
- en: Also, the `chunk_overlap` means that we want a maximum of 200 overlapping characters
    between consecutive chunks.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，`chunk_overlap` 表示我们希望连续块之间有最多 200 个重叠字符。
- en: For instance, imagine that we have a document containing the text `Chat with
    your documents using LLMs` and want to apply the chunking using the `Chunk Size
    = 10` and `Chunk overlap = 5.`
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设我们有一个包含文本 `Chat with your documents using LLMs` 的文档，并想使用 `Chunk Size =
    10` 和 `Chunk overlap = 5` 来应用块化。
- en: 'The process is explained in the image below:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 该过程在下面的图像中进行了说明：
- en: '![](../Images/0c513e3c8a79c4e85371ebed619bb1ca.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0c513e3c8a79c4e85371ebed619bb1ca.png)'
- en: Document chunking illustration (Image by Author)
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 文档块化示例（作者提供的图片）
- en: We can see that we ended up with a total of 7 chunks for an input document of
    35 characters (spaces included).
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，对于一个包含 35 个字符（包括空格）的输入文档，我们最终得到了 7 个块。
- en: But, why do we use these overlaps in the first place?
  id: totrans-65
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 但是，我们为什么要使用这些重叠部分呢？
- en: Including these overlaps, the `CharacterTextSplitter` ensures that the underlying
    context is maintained between the chunks, which is especially useful when working
    with long pieces of documents.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 包括这些重叠部分，`CharacterTextSplitter` 确保在块之间保持底层上下文，这在处理长文档时特别有用。
- en: Similarly to the `chunk_size` there is no fixed value of `chunk_overlap` . Different
    values need to be tested to choose the one with better results.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于 `chunk_size`，`chunk_overlap` 没有固定值。需要测试不同的值以选择效果更好的值。
- en: 'Now, let’s see their application in our scenario:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看它们在我们的场景中的应用：
- en: '[PRE5]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Output:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：
- en: '![](../Images/15039c16589eb887d710631a85066443.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/15039c16589eb887d710631a85066443.png)'
- en: Number of chunks in each document (Image by Author)
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 每个文档中的块数（作者提供的图片）
- en: For a larger document like the research paper, we have a lot more chunks (51)
    compared to the one-page article document, which is only 2.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 对于像研究论文这样的较大文档，我们有更多的块（51 个），而一页文章文档只有 2 个块。
- en: '**b. Create embeddings of the chunks**'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '**b. 创建块的嵌入**'
- en: We can use the `OpenAIEmbeddings` module, which uses `text-embedding-ada-002`
    model by default to create the embedding of the chunks.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用 `OpenAIEmbeddings` 模块，该模块默认使用 `text-embedding-ada-002` 模型来创建块的嵌入。
- en: 'Instead of using the `text-embedding-ada-002` can use a different model (e.g.
    `gpt-3.5-turbo-0301`) by changing the following parameters:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 可以通过更改以下参数，使用不同的模型（例如 `gpt-3.5-turbo-0301`）来代替 `text-embedding-ada-002`：
- en: model = “`gpt-3.5-turbo-0301`”
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: model = “`gpt-3.5-turbo-0301`”
- en: deployment = "`<DEPLOYMENT-NAME>` “ which corresponds to the name given during
    the deployment of the model. The default value is also `text-embedding-ada-002`
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: deployment = "`<DEPLOYMENT-NAME>` "，这对应于在模型部署期间给出的名称。默认值也是 `text-embedding-ada-002`
- en: For simplicity’s sake, we will stick to using the default parameters’ value
    in this tutorial. But before that, we need to acquire the OpenAI credentials,
    and all the steps are provided in the [following article](https://medium.com/geekculture/how-to-fine-tune-gpt3-using-openai-api-and-python-9ef813879af4).
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简单起见，在本教程中我们将坚持使用默认参数值。但在此之前，我们需要获取 OpenAI 凭据，所有步骤在 [以下文章](https://medium.com/geekculture/how-to-fine-tune-gpt3-using-openai-api-and-python-9ef813879af4)
    中提供。
- en: '[PRE6]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '**c. Create document search**'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '**c. 创建文档搜索**'
- en: To get the answer to a given query, we need to create a vector store that finds
    the closest matching chunk to that query.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 要回答给定的查询，我们需要创建一个向量存储，以找到与该查询最匹配的块。
- en: 'Such vector store can be created using the `from_texts` function from `FAISS`
    module and the function takes two main parameters: `text_splitter` and `embeddings`
    which are both defined previously.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 这样的向量存储可以使用 `FAISS` 模块中的 `from_texts` 函数创建，该函数需要两个主要参数：`text_splitter` 和 `embeddings`，这两者都已定义。
- en: '[PRE7]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: By running the `get_doc_search` on the research paper chunks, we can see that
    the result is of a `vectorstores` . The result would have been the same if we
    used the article_information_chunks.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 通过在研究论文块上运行 `get_doc_search`，我们可以看到结果是 `vectorstores`。如果我们使用 article_information_chunks，结果也会相同。
- en: '[PRE8]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Output:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：
- en: '![](../Images/83925a3f53662ad0e178de1561b3efcb.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/83925a3f53662ad0e178de1561b3efcb.png)'
- en: Vector store of the research paper (Image by Author)
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 研究论文的向量存储（作者提供的图片）
- en: '**d. Start chatting with your documents**'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '**d. 开始与你的文档聊天**'
- en: Congrats on making it that far! 🎉
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜你走到这一步！ 🎉
- en: The `chat_with_file` function is used to implement the end-to-end logic of the
    chat by combining all the above functions, along with the with `similarity_search`
    function.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '`chat_with_file` 函数用于实现聊天的端到端逻辑，将所有上述函数与 `similarity_search` 函数结合使用。'
- en: 'The final function takes two parameters:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 最终函数需要两个参数：
- en: The file we want to chat with, and
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们想要聊天的文件，以及
- en: The query provided by the user
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用户提供的查询
- en: '[PRE9]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Let’s take a step back to properly understand what is happening in the above
    code block.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们退一步，以正确理解上述代码块中发生的事情。
- en: The `load_qa_chain` provides an interface to perform question-answering over
    a set of documents. In this specific case, we are using the default `OpenAI GPT-3`
    large language model.
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`load_qa_chain` 提供了一个接口，用于在一组文档上执行问答。在这个特定的案例中，我们使用的是默认的 `OpenAI GPT-3` 大型语言模型。'
- en: The `chain_type` is `map_rerank` . By doing so, the `load_qa_chain` function
    returns the answers based on a confidence score given by the chain. There are
    other `chain_type` that can be used such as `map_reduce` , `stuff` , `refine`
    and more. Each one has its own pros and cons.
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`chain_type` 是 `map_rerank` 。通过这样做，`load_qa_chain` 函数根据链提供的置信度分数返回答案。还有其他可以使用的
    `chain_type`，如 `map_reduce`、`stuff`、`refine` 等。每种都有其优缺点。'
- en: By setting `return_intermediate_steps=True` , we can access the metadata such
    as the above confidence score.
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过设置 `return_intermediate_steps=True`，我们可以访问诸如上述置信度分数等元数据。
- en: 'Its output is a dictionary of two keys: the **answer** to the query, and the
    confidence **score**.'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 它的输出是一个包含两个键的字典：查询的 **答案** 和置信度 **分数**。
- en: 'We can finally chat with the our files, starting with the image document:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 我们终于可以开始与我们的文件聊天，从图像文档开始：
- en: '**Chat with the image document**'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**与图像文档聊天**'
- en: To chat with the image document, we provide the path to the document, and the
    question we want the model to answer.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 要与图像文档聊天，我们提供文档路径和我们希望模型回答的问题。
- en: '[PRE10]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Output:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：
- en: '![](../Images/34fd452b4430151ca4a7a5bea6dbceb1.png)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/34fd452b4430151ca4a7a5bea6dbceb1.png)'
- en: Result of a query on the image document (Image by Author)
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 对图像文档的查询结果（图像来源：作者）
- en: The model is 100% confident in its response. By looking at the first paragraph
    of the original document below, we can see that the model response is indeed correct.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 模型对其响应有 100% 的信心。通过查看下面的原始文档的第一段，我们可以看到模型的响应确实是正确的。
- en: '![](../Images/110f996f430c1236dcc24ef9b5b2b9b4.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/110f996f430c1236dcc24ef9b5b2b9b4.png)'
- en: First two paragraphs of the original article image document (Image by Author)
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 原始文章图像文档的前两段（图像来源：作者）
- en: One of the most interesting parts is that it provided a brief summary of the
    main topics covered in the document ( statistics, model evaluation metrics, SQL
    queries, etc.).
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 最有趣的部分之一是它提供了文档中主要主题的简要总结（统计、模型评估指标、SQL 查询等）。
- en: '**Chat with the PDF file**'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**与 PDF 文件聊天**'
- en: The process with the PDF file is similar to the one in the above section.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: PDF 文件的处理过程类似于上述部分。
- en: '[PRE11]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Output:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：
- en: Once again we are getting a 100% confidence score from the model. The answer
    to the question looks pretty correct!
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 我们再次从模型中获得了 100% 的置信度分数。问题的答案看起来非常正确！
- en: '![](../Images/6947fe479ba82c3cde792c3bf82f6981.png)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6947fe479ba82c3cde792c3bf82f6981.png)'
- en: Result of a query on the PDF document (Image by Author)
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 对 PDF 文档的查询结果（图像来源：作者）
- en: In both cases, the model was able to provide a human-like response in a few
    seconds. Making a human go through the same process would take minutes, even hours
    depending on the length of the document.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 在这两种情况下，模型都能够在几秒钟内提供类似人类的响应。让一个人经历相同的过程将需要几分钟，甚至几小时，具体取决于文档的长度。
- en: Conclusion
  id: totrans-121
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: Congratulations!!!🎉
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜！！！🎉
- en: I hope this article provided enough tools to help you take your knowledge to
    the next level. The code is available on [my GitHub](https://github.com/keitazoumana/Medium-Articles-Notebooks/blob/main/Chat_With_Any_Document.ipynb).
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 我希望这篇文章提供了足够的工具来帮助你提升知识水平。代码可在 [我的 GitHub](https://github.com/keitazoumana/Medium-Articles-Notebooks/blob/main/Chat_With_Any_Document.ipynb)
    上获取。
- en: In my next article, I will explain how to integrate this system into a nice
    user interface. Stay tuned!
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 在我的下一篇文章中，我将解释如何将这个系统集成到一个漂亮的用户界面中。敬请期待！
- en: Also, If you enjoy reading my stories and wish to support my writing, consider
    becoming a Medium member. It’s $5 a month, giving you unlimited access to thousands
    of Python guides and Data science articles.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你喜欢阅读我的故事并希望支持我的写作，考虑成为 Medium 会员。每月 $5，你将获得对成千上万的 Python 指南和数据科学文章的无限访问。
- en: By signing up using [my link](https://zoumanakeita.medium.com/membership), I
    will earn a small commission at no extra cost to you.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用 [我的链接](https://zoumanakeita.medium.com/membership)，我将获得少量佣金，而你无需额外支付。
- en: '[](https://zoumanakeita.medium.com/membership?source=post_page-----4bcfd7e440bc--------------------------------)
    [## Join Medium with my referral link - Zoumana Keita'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://zoumanakeita.medium.com/membership?source=post_page-----4bcfd7e440bc--------------------------------)
    [## 通过我的推荐链接加入 Medium - Zoumana Keita]'
- en: As a Medium member, a portion of your membership fee goes to writers you read,
    and you get full access to every story…
  id: totrans-128
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 作为 Medium 会员，你的会员费用的一部分会用于支持你阅读的作者，同时你可以完全访问每个故事……
- en: zoumanakeita.medium.com](https://zoumanakeita.medium.com/membership?source=post_page-----4bcfd7e440bc--------------------------------)
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '[zoumanakeita.medium.com](https://zoumanakeita.medium.com/membership?source=post_page-----4bcfd7e440bc--------------------------------)'
- en: Feel free to follow me on [Twitter](https://twitter.com/zoumana_keita_), and
    [YouTube](https://www.youtube.com/channel/UC9xKdy8cz6ZuJU5FTNtM_pQ), or say Hi
    on [LinkedIn](https://www.linkedin.com/in/zoumana-keita/).
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 欢迎通过 [Twitter](https://twitter.com/zoumana_keita_) 和 [YouTube](https://www.youtube.com/channel/UC9xKdy8cz6ZuJU5FTNtM_pQ)
    关注我，或者在 [LinkedIn](https://www.linkedin.com/in/zoumana-keita/) 上打个招呼。
- en: '[Let’s connect here for a 1–1 discussion](https://topmate.io/zoumanakeita)'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '[在这里与我联系进行一对一讨论](https://topmate.io/zoumanakeita)'
- en: Before you leave, there are more great resources below you might be interested
    in reading!
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 离开之前，下面还有更多你可能感兴趣的优质资源！
- en: '[Introduction to Text Embeddings with the OpenAI API](https://medium.com/geekculture/introduction-to-text-embeddings-with-the-openai-api-1f83d2a15fda)'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '[使用 OpenAI API 进行文本嵌入介绍](https://medium.com/geekculture/introduction-to-text-embeddings-with-the-openai-api-1f83d2a15fda)'
- en: '[How to Extract Text from Any PDF and Image for Large Language Model](https://medium.com/towards-data-science/how-to-extract-text-from-any-pdf-and-image-for-large-language-model-2d17f02875e6)'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '[如何从任何 PDF 和图像中提取文本以供大型语言模型使用](https://medium.com/towards-data-science/how-to-extract-text-from-any-pdf-and-image-for-large-language-model-2d17f02875e6)'
