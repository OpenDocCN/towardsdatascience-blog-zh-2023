- en: 'Retro Data Science: Testing the First Versions of YOLO'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 复古数据科学：测试YOLO的首个版本
- en: 原文：[https://towardsdatascience.com/retro-data-science-testing-the-first-versions-of-yolo-799b9c1835d7](https://towardsdatascience.com/retro-data-science-testing-the-first-versions-of-yolo-799b9c1835d7)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/retro-data-science-testing-the-first-versions-of-yolo-799b9c1835d7](https://towardsdatascience.com/retro-data-science-testing-the-first-versions-of-yolo-799b9c1835d7)
- en: Let's travel 8 years back in time
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 让我们回到8年前
- en: '[](https://dmitryelj.medium.com/?source=post_page-----799b9c1835d7--------------------------------)[![Dmitrii
    Eliuseev](../Images/7c48f0c016930ead59ddb785eaf3e0e6.png)](https://dmitryelj.medium.com/?source=post_page-----799b9c1835d7--------------------------------)[](https://towardsdatascience.com/?source=post_page-----799b9c1835d7--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----799b9c1835d7--------------------------------)
    [Dmitrii Eliuseev](https://dmitryelj.medium.com/?source=post_page-----799b9c1835d7--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://dmitryelj.medium.com/?source=post_page-----799b9c1835d7--------------------------------)[![Dmitrii
    Eliuseev](../Images/7c48f0c016930ead59ddb785eaf3e0e6.png)](https://dmitryelj.medium.com/?source=post_page-----799b9c1835d7--------------------------------)[](https://towardsdatascience.com/?source=post_page-----799b9c1835d7--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----799b9c1835d7--------------------------------)
    [Dmitrii Eliuseev](https://dmitryelj.medium.com/?source=post_page-----799b9c1835d7--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----799b9c1835d7--------------------------------)
    ·8 min read·Jun 22, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于[Towards Data Science](https://towardsdatascience.com/?source=post_page-----799b9c1835d7--------------------------------)
    ·8分钟阅读·2023年6月22日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/6868311449632a8f33870a1ec215e355.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6868311449632a8f33870a1ec215e355.png)'
- en: Objects detection with YOLO, Image by author
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 使用YOLO进行物体检测，图像由作者提供
- en: The world of data science is constantly changing. Often, we cannot see these
    changes just because they are going slowly, but after some time, it is easy to
    watch back and see that the landscape became drastically different. Tools and
    libraries, which were at the cutting edge of progress only 10 years ago, can be
    completely forgotten today.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 数据科学的世界不断变化。通常，我们无法看到这些变化，因为它们进展缓慢，但过了一段时间，我们很容易回顾，发现景象已发生了巨大的变化。仅仅10年前处于前沿的工具和库，今天可能已经完全被遗忘。
- en: YOLO (You Only Look Once) is a popular object detection library. Its first version
    was released a pretty long time ago, in 2015\. YOLO was working fast, it provided
    good results, and the pre-trained models were publicly available. The model quickly
    became popular, and the project is still actively improving nowadays. This gives
    us the opportunity to see how data science tools and libraries have evolved over
    the years. In this article, I will test different YOLO versions, from the very
    first V1 up to the latest V8.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: YOLO（You Only Look Once）是一个流行的物体检测库。它的第一个版本在2015年发布。YOLO运行速度快，效果好，预训练模型也公开可用。该模型迅速流行起来，项目至今仍在积极改进。这使我们有机会看到数据科学工具和库多年来是如何演变的。在这篇文章中，我将测试不同的YOLO版本，从最初的V1到最新的V8。
- en: 'For further testing, I will use this image:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 进行进一步测试时，我将使用这张图像：
- en: '![](../Images/c80b38ffa792b0fd9df9cd624b0d83d4.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c80b38ffa792b0fd9df9cd624b0d83d4.png)'
- en: Test image, made by author
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 测试图像，由作者制作
- en: Let’s get started.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 开始吧。
- en: YOLO V1..V3
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: YOLO V1..V3
- en: 'The very first paper, “You Only Look Once: Unified, Real-Time Object Detection,”
    about YOLO [was released in 2015](https://arxiv.org/abs/1506.02640). And surprisingly,
    YOLO v1 is still [available for download](https://pjreddie.com/darknet/yolov1/).
    As Mr.Redmon, one of the authors of the original paper, [wrote](https://pjreddie.com/darknet/yolov1/),
    he is keeping this version “for historical purposes”, and this is really nice
    indeed. But can we run it today? The model is distributed in the form of two files.
    The configuration file “`yolo.cfg`" contains details about the neural network
    model:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '关于YOLO的第一篇论文，“You Only Look Once: Unified, Real-Time Object Detection”，[发布于2015年](https://arxiv.org/abs/1506.02640)。令人惊讶的是，YOLO
    v1 仍然可以[下载](https://pjreddie.com/darknet/yolov1/)。正如原始论文的作者之一Mr.Redmon，[所写](https://pjreddie.com/darknet/yolov1/)，他保留这个版本是“为了历史目的”，这确实很棒。但我们今天还能运行它吗？该模型以两个文件的形式分发。配置文件“`yolo.cfg`”包含有关神经网络模型的详细信息：'
- en: '[PRE0]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: And the second file “`yolov1.weights`", as the name suggests, contains the weights
    of the pre-trained model.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个文件“`yolov1.weights`”，顾名思义，包含预训练模型的权重。
- en: This type of format is not from PyTorch or Keras. It turned out that the model
    was created using **Darknet**, an open-source neural network framework written
    in C. This project is still [available on GitHub](https://github.com/pjreddie/darknet),
    but it looks abandoned. At the moment of writing this article, there are 164 pull
    requests and 1794 open issues; the last commits were made in 2018, and later only
    README.md was changed (well, this is probably how the death of the project looks
    in the modern digital world).
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 这种格式并不是来自 PyTorch 或 Keras。结果发现该模型是使用 **Darknet** 创建的，这是一种用 C 编写的开源神经网络框架。这个项目仍然
    [可在 GitHub 上获取](https://github.com/pjreddie/darknet)，但看起来已经被弃用。在撰写本文时，共有 164 个拉取请求和
    1794 个未解决问题；最后一次提交是在 2018 年，之后只有 README.md 文件进行了更改（这可能就是现代数字世界中项目死亡的样子）。
- en: 'The original Darknet project is abandoned; this is bad news. The good news
    is that the *readNetFromDarknet* method is still available in OpenCV, and it is
    [present](https://docs.opencv.org/5.x/d6/d0f/group__dnn.html) even in the latest
    OpenCV versions. So, we can easily try to load the original YOLO v1 model using
    the modern Python environment:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 原始 Darknet 项目已被弃用，这真是坏消息。好消息是 *readNetFromDarknet* 方法在 OpenCV 中仍然可用，并且在最新版本的
    OpenCV 中也 [存在](https://docs.opencv.org/5.x/d6/d0f/group__dnn.html)。因此，我们可以很容易地尝试在现代
    Python 环境中加载原始 YOLO v1 模型：
- en: '[PRE1]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Alas, it did not work; I only got an error:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 可惜，没有成功；我只得到了一个错误：
- en: '[PRE2]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'It turned out that “yolo.cfg” has a layer named “local”, which is not supported
    by OpenCV, and I don’t know if there is a workaround for that. Anyway, the [YOLO
    v2 config](https://pjreddie.com/darknet/yolov2/) does not have this layer anymore,
    and this model can be successfully loaded in OpenCV:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 结果发现“yolo.cfg”有一个名为“local”的层，OpenCV 不支持这个层，而且我不知道是否有解决方法。无论如何，[YOLO v2 配置](https://pjreddie.com/darknet/yolov2/)中不再有这个层，该模型可以成功加载到
    OpenCV 中：
- en: '[PRE3]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Using the model is not as easy as we might expect. First, we need to find the
    output layers of the model:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 使用模型并不像我们预期的那么简单。首先，我们需要找到模型的输出层：
- en: '[PRE4]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Then we need to load the image and convert it into binary format, which the
    model can understand:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们需要加载图像并将其转换为模型可以理解的二进制格式：
- en: '[PRE5]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Finally, we can run forward propagation. A “forward” method will run the calculations
    and return the requested layer outputs:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，我们可以进行前向传播。一个“forward”方法将进行计算并返回所请求的层输出：
- en: '[PRE6]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Making the forward propagation is straightforward, but parsing the output can
    be a bit tricky. The model is producing 85-dimensional feature vectors as an output,
    where the first 4 digits represent object rectangles, the 5th digit is a probability
    of the presence of an object, and the last 80 digits contain the probability information
    for the 80 categories the model was trained on. Having this information, we can
    draw the labels over the original image:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 前向传播是直接的，但解析输出可能有点棘手。模型输出 85 维特征向量，其中前 4 个数字表示物体矩形，第 5 个数字表示物体存在的概率，最后 80 个数字包含模型训练的
    80 个类别的概率信息。拥有这些信息后，我们可以在原始图像上绘制标签：
- en: '[PRE7]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Here I use *np.argmax* to find the class ID with the maximum probability. The
    YOLO model was trained using the [COCO](https://cocodataset.org/) (Common Objects
    in Context, Creative Commons Attribution 4.0 License) dataset, and for simplicity
    reasons, I placed all 80 label names directly in the code. I also used the OpenCV
    [NMSBoxes](https://docs.opencv.org/4.7.0/d6/d0f/group__dnn.html#ga9d118d70a1659af729d01b10233213ee)
    method to combine embedded rectangles together.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我使用 *np.argmax* 来找到概率最大的类别 ID。YOLO 模型是使用 [COCO](https://cocodataset.org/)（Common
    Objects in Context，创作共享 4.0 许可）数据集训练的，为了简化，我将所有 80 个标签名称直接放入代码中。我还使用了 OpenCV 的
    [NMSBoxes](https://docs.opencv.org/4.7.0/d6/d0f/group__dnn.html#ga9d118d70a1659af729d01b10233213ee)
    方法来将嵌套的矩形合并在一起。
- en: 'The final result looks like this:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 最终结果如下：
- en: '![](../Images/1fc1bfd1fb37ff086f372456ea726128.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1fc1bfd1fb37ff086f372456ea726128.png)'
- en: YOLO v2 results, Image by author
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: YOLO v2 结果，图片由作者提供
- en: We successfully ran a model released in 2016 in a modern environment!
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 我们成功地在现代环境中运行了一个 2016 年发布的模型！
- en: 'The next version, YOLO v3, was released two years later, in 2018, and we can
    also run it using the same code (the weights and config files [are available online](https://pjreddie.com/darknet/yolo/)).
    As the authors [wrote in the paper](https://arxiv.org/abs/1804.02767), the new
    model is more accurate, and we can easily verify this:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 下一版本 YOLO v3 于 2018 年发布，我们也可以使用相同的代码运行它（权重和配置文件 [在线提供](https://pjreddie.com/darknet/yolo/)）。正如作者
    [在论文中写的](https://arxiv.org/abs/1804.02767)，新模型更准确，我们可以轻松验证这一点：
- en: '![](../Images/9d0b929f5bdc80b6c4ecca60c513ce1c.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9d0b929f5bdc80b6c4ecca60c513ce1c.png)'
- en: YOLO v3 results, Image by author
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: YOLO v3 结果，图像作者提供
- en: Indeed, a V3 model was able to find more objects on the same image. Those readers
    who are interested in technical details can read this [TDS article](/yolo-v3-object-detection-53fb7d3bfe6b)
    written in 2018.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 确实，V3模型能够在同一图像上找到更多的对象。对技术细节感兴趣的读者可以阅读这篇2018年的[TDS文章](/yolo-v3-object-detection-53fb7d3bfe6b)。
- en: YOLO V5..V7
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: YOLO V5..V7
- en: 'As we can see, the model loaded with the *readNetFromDarknet* method works,
    but the required code is pretty “low-level” and cumbersome. OpenCV developers
    decided to make life easier, and in 2019, a new [*DetectionModel*](https://docs.opencv.org/4.1.2/d3/df1/classcv_1_1dnn_1_1DetectionModel.html)class
    was added to version 4.1.2\. We can load the YOLO model this way; the general
    logic remains the same, but the required amount of code is much smaller. The model
    directly returns class IDs, confidence values, and rectangles in one method call:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，使用*readNetFromDarknet*方法加载的模型可以工作，但所需的代码相当“低级”和繁琐。OpenCV开发者决定简化工作，在2019年，为4.1.2版本添加了一个新的[*DetectionModel*](https://docs.opencv.org/4.1.2/d3/df1/classcv_1_1dnn_1_1DetectionModel.html)类。我们可以这样加载YOLO模型；整体逻辑保持不变，但所需的代码量要小得多。模型在一次方法调用中直接返回类ID、置信值和矩形：
- en: '[PRE8]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: As we can see, all the low-level code needed for extracting boxes and confidence
    values from the model output is not needed anymore.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，提取模型输出中的框和置信值所需的所有低级代码现在不再需要了。
- en: 'The result of running YOLO v7 is, in general, the same:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 运行YOLO v7的结果总体上是相同的：
- en: '![](../Images/6ec0648d9b747deb482d6832dc877002.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6ec0648d9b747deb482d6832dc877002.png)'
- en: YOLO v7 results, Image by author
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: YOLO v7 结果，图像作者提供
- en: YOLO V8
  id: totrans-49
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: YOLO V8
- en: 'The [8th version](https://github.com/ultralytics/ultralytics) was released
    in 2023, so I cannot consider it “retro”, at least at the moment of writing this
    text. But just to compare the results, let’s see the code required nowadays to
    run YOLO:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '[第8版](https://github.com/ultralytics/ultralytics)发布于2023年，所以我不能认为它是“复古”的，至少在写这篇文章的时候。为了比较结果，我们来看看现在运行YOLO所需的代码：'
- en: '[PRE9]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: As we can see, the code became even more compact. We don’t need to take care
    of dataset label names (the model provides a “names” property) or how to draw
    rectangles and labels on the image (there is a special *BoxAnnotator* class for
    that). We even don’t need to download model weights anymore; the library will
    do it automatically for us. Compared to 2016, the program from 2023 “shrunk” from
    about 50 to about 5 lines of code! It is obviously a nice improvement, and modern
    developers don’t need to know about forward propagation or the output level format
    anymore. The model just works as a black box with some “magic” inside. Is it good
    or bad? I don’t know :)
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，代码变得更加简洁。我们不需要处理数据集标签名称（模型提供了一个“names”属性）或如何在图像上绘制矩形和标签（有一个专门的*BoxAnnotator*类）。我们甚至不需要再下载模型权重；库会自动为我们完成这项工作。与2016年相比，2023年的程序从大约50行缩减到约5行！显然，这是一个很大的改进，现代开发者不再需要了解前向传播或输出级别格式。模型就像一个内含“魔法”的黑箱。这个现象是好还是坏？我不知道
    :)
- en: 'As for the result itself, it’s pretty accurate:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 至于结果本身，它相当准确：
- en: '![](../Images/a034b88e25cc58d7056f681b6e27d499.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a034b88e25cc58d7056f681b6e27d499.png)'
- en: YOLO v8 results, Image by author
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: YOLO v8 结果，图像作者提供
- en: The model works well, and at least on my computer, the calculation speed improved
    compared to v7, maybe because of the better use of the GPU.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 模型工作良好，至少在我的计算机上，相比v7，计算速度有所提高，也许是因为更好地利用了GPU。
- en: Conclusion
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结论
- en: 'In this article, we were able to test almost all YOLO models, made from 2016
    up to 2023\. At first glance, attempts to run a model, released almost 10 years
    ago, may look like a waste of time. But as for me, I learned a lot while doing
    these tests:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我们能够测试几乎所有的YOLO模型，从2016年到2023年。乍一看，运行一个发布近10年的模型可能显得浪费时间。但对我来说，我在做这些测试时学到了很多：
- en: It was interesting to see how popular data science tools and libraries have
    evolved over the years. The trend of moving from low-level code to high-level
    methods, which do everything and even download the pre-trained model before execution
    (at least for now, without asking for a subscription key yet, but who knows what
    will be next 10 years later?), looks clear. Is it good or bad? This is an interesting
    and open question.
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 看到流行的数据科学工具和库随着时间的推移而演变是很有趣的。从低级代码转向高级方法的趋势越来越明显，这些方法可以做所有事情，甚至在执行前下载预训练模型（至少现在，还不需要订阅密钥，但谁知道未来10年会怎样？）看起来很清楚。这是好还是坏？这是一个有趣且开放的问题。
- en: It was important to know that OpenCV is “natively” capable of running deep learning
    models. This allows using neural network models not only in large frameworks like
    PyTorch or Keras but also in pure Python or even C++ applications. Not every application
    is running in a cloud with virtually unlimited resources. The IoT market is growing,
    and this is especially important for running neural networks on low-power devices
    like robots, surveillance cameras, or smart doorbells.
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 重要的是要知道OpenCV“本地”支持运行深度学习模型。这使得不仅可以在像PyTorch或Keras这样的大型框架中使用神经网络模型，还可以在纯Python甚至C++应用程序中使用。并不是所有的应用程序都在资源几乎无限的云端运行。物联网市场在增长，这对在像机器人、监控摄像头或智能门铃这样的低功耗设备上运行神经网络尤其重要。
- en: In the next article, I will test it in more detail and show how YOLO v8 runs
    on a low-powered board like a Raspberry Pi, and we will be able to test both Python
    and C++ versions. Stay tuned.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一篇文章中，我将更详细地测试，并展示YOLO v8在像树莓派这样的低功耗板上的运行情况，我们将能够测试Python和C++版本。敬请期待。
- en: If you enjoyed this story, feel free [to subscribe](https://medium.com/@dmitryelj/membership)
    to Medium, and you will get notifications when my new articles will be published,
    as well as full access to thousands of stories from other authors.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你喜欢这个故事，欢迎[订阅](https://medium.com/@dmitryelj/membership)Medium，你将收到我新文章发布的通知，并且可以完全访问其他作者的成千上万篇故事。
