- en: Why Do We Even Have Neural Networks?
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为什么我们还需要神经网络？
- en: 原文：[https://towardsdatascience.com/why-do-we-even-have-neural-networks-72410cb9348e](https://towardsdatascience.com/why-do-we-even-have-neural-networks-72410cb9348e)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/why-do-we-even-have-neural-networks-72410cb9348e](https://towardsdatascience.com/why-do-we-even-have-neural-networks-72410cb9348e)
- en: 'Alternatives to Neural Networks: Taylor Series & Fourier Series'
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 神经网络的替代方案：泰勒级数与傅里叶级数
- en: '[](https://medium.com/@egorhowell?source=post_page-----72410cb9348e--------------------------------)[![Egor
    Howell](../Images/1f796e828f1625440467d01dcc3e40cd.png)](https://medium.com/@egorhowell?source=post_page-----72410cb9348e--------------------------------)[](https://towardsdatascience.com/?source=post_page-----72410cb9348e--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----72410cb9348e--------------------------------)
    [Egor Howell](https://medium.com/@egorhowell?source=post_page-----72410cb9348e--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@egorhowell?source=post_page-----72410cb9348e--------------------------------)[![Egor
    Howell](../Images/1f796e828f1625440467d01dcc3e40cd.png)](https://medium.com/@egorhowell?source=post_page-----72410cb9348e--------------------------------)[](https://towardsdatascience.com/?source=post_page-----72410cb9348e--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----72410cb9348e--------------------------------)
    [Egor Howell](https://medium.com/@egorhowell?source=post_page-----72410cb9348e--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----72410cb9348e--------------------------------)
    ·7 min read·Dec 16, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----72410cb9348e--------------------------------)
    ·阅读时间7分钟·2023年12月16日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/25f1853ad21413f753796df43b84dfb0.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/25f1853ad21413f753796df43b84dfb0.png)'
- en: ”[https://www.flaticon.com/free-icons/neural-network](https://www.flaticon.com/free-icons/neural-network)."
    title=”neural network icons” Neural network icons created by imaginationlol —
    Flaticon.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: ”[https://www.flaticon.com/free-icons/neural-network](https://www.flaticon.com/free-icons/neural-network)。"
    标题=”神经网络图标” 神经网络图标由 imaginationlol 创建 — Flaticon。
- en: 'I have recently been writing a series of articles explaining the key concepts
    behind modern-day neural networks:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 我最近一直在撰写一系列文章，解释现代神经网络背后的关键概念：
- en: '![Egor Howell](../Images/e969a9f3c3357e1c80dcd0092d9a1288.png)'
  id: totrans-9
  prefs: []
  type: TYPE_IMG
  zh: '![Egor Howell](../Images/e969a9f3c3357e1c80dcd0092d9a1288.png)'
- en: '[Egor Howell](https://medium.com/@egorhowell?source=post_page-----72410cb9348e--------------------------------)'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '[Egor Howell](https://medium.com/@egorhowell?source=post_page-----72410cb9348e--------------------------------)'
- en: Neural Networks
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 神经网络
- en: '[View list](https://medium.com/@egorhowell/list/neural-networks-616db722dbbb?source=post_page-----72410cb9348e--------------------------------)9
    stories![](../Images/66f86f14ddf20d9da9cac53dee54bae3.png)![](../Images/d968b0bd4358bb0d60bd296aef08a720.png)![](../Images/30f3f4354836e6d3d56175fe20cf6b7c.png)'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '[查看列表](https://medium.com/@egorhowell/list/neural-networks-616db722dbbb?source=post_page-----72410cb9348e--------------------------------)
    9个故事![](../Images/66f86f14ddf20d9da9cac53dee54bae3.png)![](../Images/d968b0bd4358bb0d60bd296aef08a720.png)![](../Images/30f3f4354836e6d3d56175fe20cf6b7c.png)'
- en: One reason why neural networks are so powerful and popular is that they exhibit
    the [***universal approximation theorem***](https://en.wikipedia.org/wiki/Universal_approximation_theorem)***.***
    This means that a neural network can “learn” any function no matter how complex.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络如此强大和受欢迎的一个原因是它们展示了 [***通用逼近定理***](https://en.wikipedia.org/wiki/Universal_approximation_theorem)***。***
    这意味着神经网络可以“学习”任何函数，无论其复杂程度如何。
- en: '[***“Functions describe the world.”***](https://www.youtube.com/watch?v=PAZTIAfaNr8)'
  id: totrans-14
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '[***“函数描述世界。”***](https://www.youtube.com/watch?v=PAZTIAfaNr8)'
- en: 'A function, ***f(x)***, takes some input, ***x***, and gives an output ***y***:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 一个函数，***f(x)***，接收一些输入，***x***，并给出输出 ***y***：
- en: '![](../Images/9c132aa6dab2a0b9da872eb5ead558c9.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9c132aa6dab2a0b9da872eb5ead558c9.png)'
- en: How a mathematical function works. Diagram by author.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 数学函数如何工作。图示作者提供。
- en: This function defines the relationship between the input and output. In most
    cases, we have the inputs and the corresponding outputs with the goal of the neural
    network to learn, or approximate, the function that maps between them.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 这个函数定义了输入与输出之间的关系。在大多数情况下，我们拥有输入和相应的输出，神经网络的目标是学习或逼近将它们之间映射的函数。
- en: Neural networks were invented around the 1950s and 1960s. Yet, at that time
    there were other known universal approximaters out there. So, why do we even have
    neural networks …
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络在1950年代和1960年代左右被发明。然而，当时还有其他已知的通用逼近器。那么，为什么我们还需要神经网络呢……
- en: Taylor Series
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 泰勒级数
- en: The [***Taylor Series***](https://en.wikipedia.org/wiki/Taylor_series) represents
    a function as an infinite sum of terms calculated from the values of its derivatives
    at a single point. In other words, it’s a sum of infinite polynomials to approximate
    a function.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '[***泰勒级数***](https://en.wikipedia.org/wiki/Taylor_series) 将一个函数表示为从其在单点的导数值计算出的无限项之和。换句话说，它是一个无限多项式的和，用于近似一个函数。'
- en: '![](../Images/8f4f8b3545ec098a95d29e2aa431229e.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8f4f8b3545ec098a95d29e2aa431229e.png)'
- en: Taylor Series. Equation by author in LaTeX.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 泰勒级数。方程由作者在 LaTeX 中给出。
- en: The above expression represents a function ***f*(*x*)** as an infinite sum,
    where ***f^n*** is the ***n-th*** derivative or order of ***f*** at the point
    ***a***, and ***n*!** denotes the factorial of ***n***.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 上述表达式将一个函数 ***f*(*x*)** 表示为一个无限和，其中 ***f^n*** 是在点 ***a*** 处的 ***n-th*** 导数或阶数，***n*!**
    表示 ***n*** 的阶乘。
- en: See [here](https://math.stackexchange.com/questions/218421/what-are-the-practical-applications-of-the-taylor-series)
    if you are interested in learning why we use Taylor Series. Long story short,
    they are used to make ugly functions nice to work with!
  id: totrans-25
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 如果你有兴趣了解我们为什么使用泰勒级数，请见 [这里](https://math.stackexchange.com/questions/218421/what-are-the-practical-applications-of-the-taylor-series)。简而言之，它们用于将复杂的函数变得易于处理！
- en: There exists a simplification of the Taylor Series called the [***Maclaurin
    series***](https://brilliant.org/wiki/maclaurin-series/) where ***a = 0***.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 存在一种泰勒级数的简化形式，称为 [***麦克劳林级数***](https://brilliant.org/wiki/maclaurin-series/)，其中
    ***a = 0***。
- en: '![](../Images/3d32e8122e1d96f78970db6ccf3c91ac.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3d32e8122e1d96f78970db6ccf3c91ac.png)'
- en: Maclaurin Series. Equation by author in LaTeX.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 麦克劳林级数。方程由作者在 LaTeX 中给出。
- en: Where in this case ***a_0***, ***a_1***, etc are the coefficients for the corresponding
    polynomials. The goal of the Taylor and Maclaurin series is to find the best values
    of the coefficients to approximate a given target function.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，***a_0***、***a_1*** 等是对应多项式的系数。泰勒级数和麦克劳林级数的目标是找到最优系数值，以近似给定的目标函数。
- en: '*Sounds familiar?*'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '*听起来很熟悉？*'
- en: We can even express the Maclaurin series in matrix notation.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 我们甚至可以用矩阵表示麦克劳林级数。
- en: '![](../Images/01e86c2f0b9fc6ffa4bde3fb7eb0367b.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/01e86c2f0b9fc6ffa4bde3fb7eb0367b.png)'
- en: Maclaurin Series in matrix notation. Equation by author in LaTeX.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 麦克劳林级数的矩阵表示。方程由作者在 LaTeX 中给出。
- en: This is pretty much a single-layer neural network! ***a_0*** is the bias term,
    ***a_1*** to ***a_n*** are the weights, and ***x*** to ***x^n*** are our features.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 这几乎是一个单层神经网络！ ***a_0*** 是偏置项，***a_1*** 到 ***a_n*** 是权重，***x*** 到 ***x^n*** 是我们的特征。
- en: I like to think of the Taylor series as (loosely) [**polynomial regression**](https://medium.com/analytics-vidhya/understanding-polynomial-regression-5ac25b970e18#:~:text=Polynomial%20Regression%20is%20a%20form,the%20method%20of%20least%20squares.)**!**
  id: totrans-35
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 我喜欢把泰勒级数视为（宽松地）[**多项式回归**](https://medium.com/analytics-vidhya/understanding-polynomial-regression-5ac25b970e18#:~:text=Polynomial%20Regression%20is%20a%20form,the%20method%20of%20least%20squares.)**！**
- en: In machine learning problems, we don’t actually have the whole function but
    rather a sample of data points. This is where we would pass in the ***x^n*** features
    as Taylor features to a neural network to learn the coefficients using backpropagation.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习问题中，我们实际上没有整个函数，而只是数据点的样本。这时，我们会将 ***x^n*** 特征作为泰勒特征传入神经网络，通过反向传播来学习系数。
- en: One last interesting property relating the Taylor series to machine learning
    is gradient descent. The general gradient descent formula comes from applying
    the Taylor series to the loss function. See [here](https://www.reddit.com/r/MachineLearning/comments/o01ox7/d_unfair_comparison_neural_networks_vs_taylor/)
    for the proof of this concept.
  id: totrans-37
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 另一个有趣的特性是将泰勒级数与机器学习相关联，即梯度下降。一般梯度下降公式来自将泰勒级数应用于损失函数。有关这一概念的证明，请见 [这里](https://www.reddit.com/r/MachineLearning/comments/o01ox7/d_unfair_comparison_neural_networks_vs_taylor/)。
- en: Fourier Series
  id: totrans-38
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 傅里叶级数
- en: '[**The Fourier series**](https://www.youtube.com/watch?v=Zgcry0SPUY8) is very
    similar to the Taylor series, but instead uses sines and cosines waves instead
    of polynomials. It’s defined as:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '[**傅里叶级数**](https://www.youtube.com/watch?v=Zgcry0SPUY8) 与泰勒级数非常相似，但使用的是正弦和余弦波，而不是多项式。它定义为：'
- en: Any periodic function can be decomposed into a sum of sine and cosine waves
  id: totrans-40
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 任何周期函数都可以分解为正弦波和余弦波的和。
- en: This is a very simple statement but its implications are significant.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一条非常简单的声明，但其含义却非常深远。
- en: Supplemental Video on Fourier Series.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 关于傅里叶级数的补充视频。
- en: 'For example, shown below are the functions ***sin(2x)*** and ***cos(3x)***
    and their corresponding summation:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，下面显示了***sin(2x)***和***cos(3x)***的函数及其对应的求和：
- en: '[PRE0]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '![](../Images/a183f5e7a6b960ebee5a7c0da7b0f444.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a183f5e7a6b960ebee5a7c0da7b0f444.png)'
- en: Example sine waves and their sum. Plot generated by author in Python.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 示例正弦波及其和。图表由作者在Python中生成。
- en: The ***sin(2x)*** and ***cos(3x)*** functionsare simple functions yet their
    summation (red line) leads to a more complex pattern. This is the main idea behind
    the Fourier series using multiple simple functions to build a complex one.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '***sin(2x)***和***cos(3x)***函数是简单函数，但它们的和（红线）会导致更复杂的模式。这是傅里叶级数的主要思想，即使用多个简单函数来构建复杂的函数。'
- en: 'One of the most interesting results from the Fourier series is being able to
    construct a [***square wave***](https://en.wikipedia.org/wiki/Square_wave) by
    summing infinite sines ([***harmonics***](https://electronics.stackexchange.com/questions/32310/what-exactly-are-harmonics-and-how-do-they-appear))
    of different *odd number* (orders)frequencies and amplitudes:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 傅里叶级数的一个有趣结果是通过对不同*奇数*（阶数）频率和振幅的无限正弦函数（[***谐波***](https://electronics.stackexchange.com/questions/32310/what-exactly-are-harmonics-and-how-do-they-appear)）进行求和来构造[***方波***](https://en.wikipedia.org/wiki/Square_wave)：
- en: '![](../Images/0371b9c3f215a468de4e9619c39073fb.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0371b9c3f215a468de4e9619c39073fb.png)'
- en: Summation of odd sine waves. Equation by author in LaTeX.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 奇数正弦波的求和。公式由作者在LaTeX中编写。
- en: '[PRE1]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '![](../Images/ec07ecb0dd2cbc496d12451f330adb28.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ec07ecb0dd2cbc496d12451f330adb28.png)'
- en: Using sine waves to create a square wave. Plot generated by author in Python.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 使用正弦波创建方波。图表由作者在Python中生成。
- en: What’s amazing about this result is that we have generated a sharp and straight-line
    plot from smooth sine functions. This shows the true power of the Fourier series
    to construct any periodic function.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 这个结果令人惊讶，因为我们从光滑的正弦函数生成了一个尖锐的直线图。这展示了傅里叶级数构造任何周期函数的真正能力。
- en: The Fourier series is often applied to time series to model complex [**seasonal**](/seasonality-of-time-series-5b45b4809acd?sk=aaa7e3e31c0d2ec75e151a95ecce3098)
    patterns. This is called [**harmonic regression**](https://medium.com/towards-data-science/take-your-forecasting-to-the-next-level-with-harmonic-regression-5a8515f63295?sk=8c5a869f9825ce001f337cf5f478338f).
  id: totrans-55
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 傅里叶级数通常应用于时间序列，以建模复杂的[**季节性**](/seasonality-of-time-series-5b45b4809acd?sk=aaa7e3e31c0d2ec75e151a95ecce3098)模式。这被称为[**谐波回归**](https://medium.com/towards-data-science/take-your-forecasting-to-the-next-level-with-harmonic-regression-5a8515f63295?sk=8c5a869f9825ce001f337cf5f478338f)。
- en: 'As declared earlier, the Fourier series states that any periodic function can
    be broken down into a sum of sine and cosine waves. Mathematically, this is written
    as:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，傅里叶级数声明任何周期函数都可以分解为正弦和余弦波的总和。数学上，这可以写作：
- en: '![](../Images/b5d85fb61a2151e425aa2b03b2994768.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b5d85fb61a2151e425aa2b03b2994768.png)'
- en: Fourier series. Equation by author in LaTeX.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 傅里叶级数。公式由作者在LaTeX中编写。
- en: 'Where:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 其中：
- en: '***A_0:*** *average value of the given periodic function*'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***A_0:*** *给定周期函数的平均值*'
- en: '***A_n:*** *coefficients of the cosine components*'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***A_n:*** *余弦分量的系数*'
- en: '***B_n:*** *coefficients of the sine components*'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***B_n:*** *正弦分量的系数*'
- en: '***n:*** *the**order which is the frequency of the sine or cosine wave, this
    is referred to as the ‘*[***harmonics***](https://electronics.stackexchange.com/questions/32310/what-exactly-are-harmonics-and-how-do-they-appear)*’*'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***n:*** *“*频率的顺序，称为‘*[***谐波***](https://electronics.stackexchange.com/questions/32310/what-exactly-are-harmonics-and-how-do-they-appear)*’”*'
- en: '***P:*** *period of the function*'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***P:*** *函数的周期*'
- en: Likewise, with the Taylor series, our aim with the Fourier Series is to find
    the coefficients ***A_n*** and ***B_n*** to our features, which in this case is
    the sine and cosine function.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 同样地，使用泰勒级数，我们的目标是通过傅里叶级数找到***A_n***和***B_n***系数，这些系数对应于正弦和余弦函数。
- en: Why Use Neural Networks Then?
  id: totrans-66
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 那么为什么使用神经网络呢？
- en: Both the Taylor and Fourier series can be viewed as universal function approximators
    and they both predate the neural network. So, why on earth do we have neural networks?
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 泰勒级数和傅里叶级数可以被视为通用函数逼近器，并且它们早于神经网络。那么，为什么我们还需要神经网络呢？
- en: Well, the answer is not straightforward as there are many intricacies between
    the three methods. I have been fairly liberal when describing how the Taylor and
    Fourier series work, otherwise this article would be very, very exhaustive.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 答案并不简单，因为这三种方法之间存在许多复杂性。我在描述泰勒和傅里叶级数的工作原理时较为宽松，否则这篇文章将非常繁琐。
- en: Let’s break down some reasons why the Taylor or Fourier series can’t replace
    a neural network.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们分解一些泰勒级数或傅里叶级数无法替代神经网络的原因。
- en: Taylor Series
  id: totrans-70
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 泰勒级数
- en: The main issue with the Taylor series is that they approximate around a single
    point. They are estimating a function over one value and its local region. We
    want to know what the whole function looks like over a large range. This means
    the Taylor series (polynomials) fails to generalise outside the training set.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 泰勒级数的主要问题是它们围绕一个点进行近似。它们是在一个值及其局部区域上估计一个函数。我们想知道整个函数在大范围内是什么样的。这意味着泰勒级数（多项式）在训练集之外无法泛化。
- en: Fourier Series
  id: totrans-72
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 傅里叶级数
- en: One issue with the Fourier series is that it needs to *see* the function it’s
    going to approximate. For example, in time series it’s used to find the complex
    seasonal pattern in the data. But, it knows what the data looks like. A neural
    network aims to *learn* this function.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 傅里叶级数的一个问题是它需要*看到*要近似的函数。例如，在时间序列中，它用于寻找数据中的复杂季节性模式。但它知道数据是什么样的。神经网络的目标是*学习*这个函数。
- en: However, the main problem is the complexity of the Fourier series. The number
    of coefficients increases exponentially with the number of variables in the function
    we are trying to estimate. However, for a neural network, this is not necessarily
    the case.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，主要的问题在于傅里叶级数的复杂性。系数的数量随着我们尝试估计的函数中的变量数量呈指数增长。然而，对于神经网络来说，情况不一定如此。
- en: Let’s say we have a function ***f(x)***, that we can approximate well with 100
    coefficients. Now suppose we want to approximate ***f(x,y).*** Instead of having
    100 coefficients, we now have 100² = 10,000\. For ***f(x,y,z***), we have 100³.
    And this process goes on, increasing exponentially.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有一个函数***f(x)***，我们可以用100个系数很好地近似它。现在假设我们想要近似***f(x,y)***。我们现在有100² = 10,000个系数。对于***f(x,y,z)***，我们有100³。这个过程持续下去，指数增长。
- en: What I am describing here is the [***curse of dimensionality***](https://en.wikipedia.org/wiki/Curse_of_dimensionality)***.***
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 我在这里描述的是[***维度诅咒***](https://en.wikipedia.org/wiki/Curse_of_dimensionality)***。***
- en: Neural networks on the other hand can accurately model (some) of these high
    dimensional functions without increasing their input dimensions too much.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，神经网络可以准确建模（一些）这些高维函数，而不需要过多增加输入维度。
- en: No Free Lunch Theorem
  id: totrans-78
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 无免费午餐定理
- en: It is important to mention that neural networks will not always be better than
    the Taylor series and Fourier series. The beauty of machine learning is that it
    is the science of maths. You have to play around when fitting your model to find
    the best one. It may well be that adding Taylor or Fourier features will improve
    it. However, it may also make it worse. The goal is to find the best one, but
    this is different for every dataset.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是要提到，神经网络并不总是比泰勒级数和傅里叶级数更好。机器学习的美在于它是数学的科学。你需要在拟合模型时进行尝试，以找到最佳模型。添加泰勒或傅里叶特征可能会改善它，但也可能会使其变得更差。目标是找到最佳模型，但这对于每个数据集都不同。
- en: Another Thing!
  id: totrans-80
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 另一个事情！
- en: I have a free newsletter, [**Dishing the Data**](https://dishingthedata.substack.com/),
    where I share weekly tips for becoming a better Data Scientist. There is no “fluff”
    or “clickbait,” just pure actionable insights from a practicing Data Scientist.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 我有一个免费的通讯，[**Dishing the Data**](https://dishingthedata.substack.com/)，每周分享成为更好的数据科学家的技巧。没有“空洞的”或“点击诱饵”，只有来自实践数据科学家的纯粹可操作的见解。
- en: '[](https://newsletter.egorhowell.com/?source=post_page-----72410cb9348e--------------------------------)
    [## Dishing The Data | Egor Howell | Substack'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://newsletter.egorhowell.com/?source=post_page-----72410cb9348e--------------------------------)
    [## Dishing The Data | Egor Howell | Substack'
- en: How To Become A Better Data Scientist. Click to read Dishing The Data, by Egor
    Howell, a Substack publication with…
  id: totrans-83
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 如何成为更好的数据科学家。点击阅读Dishing The Data，由Egor Howell编写，是一个Substack出版物，包含……
- en: newsletter.egorhowell.com](https://newsletter.egorhowell.com/?source=post_page-----72410cb9348e--------------------------------)
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: newsletter.egorhowell.com](https://newsletter.egorhowell.com/?source=post_page-----72410cb9348e--------------------------------)
- en: Connect With Me!
  id: totrans-85
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 与我联系！
- en: '[**YouTube**](https://www.youtube.com/@egorhowell)'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**YouTube**](https://www.youtube.com/@egorhowell)'
- en: '[**LinkedIn**](https://www.linkedin.com/in/egor-howell-092a721b3/)'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**LinkedIn**](https://www.linkedin.com/in/egor-howell-092a721b3/)'
- en: '[**Twitter**](https://twitter.com/EgorHowell)'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**Twitter**](https://twitter.com/EgorHowell)'
- en: '[**GitHub**](https://github.com/egorhowell)'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**GitHub**](https://github.com/egorhowell)'
- en: References and Further Reading
  id: totrans-90
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献和进一步阅读
- en: '*Forecasting: Principles and Practice:* [https://otexts.com/fpp2/](https://otexts.com/fpp3/arima.html)'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*预测：原则与实践：* [https://otexts.com/fpp2/](https://otexts.com/fpp3/arima.html)'
- en: '[*Excellent video explaining a similar concept*](https://www.youtube.com/watch?v=TkwXa7Cvfr8&t=923s).'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*解释类似概念的优秀视频*](https://www.youtube.com/watch?v=TkwXa7Cvfr8&t=923s)'
- en: '[*Great discussion on reddit regarding Neural Networks vs Taylor Series vs
    Fourier Series.*](https://www.reddit.com/r/MachineLearning/comments/o01ox7/d_unfair_comparison_neural_networks_vs_taylor/)'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*关于神经网络与泰勒级数与傅里叶级数的精彩讨论*](https://www.reddit.com/r/MachineLearning/comments/o01ox7/d_unfair_comparison_neural_networks_vs_taylor/)'
