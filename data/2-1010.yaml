- en: Gradient Boosting from Theory to Practice (Part 1)
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从理论到实践的梯度提升（第1部分）
- en: 原文：[https://towardsdatascience.com/gradient-boosting-from-theory-to-practice-part-1-940b2c9d8050](https://towardsdatascience.com/gradient-boosting-from-theory-to-practice-part-1-940b2c9d8050)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/gradient-boosting-from-theory-to-practice-part-1-940b2c9d8050](https://towardsdatascience.com/gradient-boosting-from-theory-to-practice-part-1-940b2c9d8050)
- en: Understand the math behind the popular gradient boosting algorithm and how to
    use it in practice
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解流行梯度提升算法背后的数学原理以及如何在实践中使用它
- en: '[](https://medium.com/@roiyeho?source=post_page-----940b2c9d8050--------------------------------)[![Dr.
    Roi Yehoshua](../Images/905a512ffc8879069403a87dbcbeb4db.png)](https://medium.com/@roiyeho?source=post_page-----940b2c9d8050--------------------------------)[](https://towardsdatascience.com/?source=post_page-----940b2c9d8050--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----940b2c9d8050--------------------------------)
    [Dr. Roi Yehoshua](https://medium.com/@roiyeho?source=post_page-----940b2c9d8050--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@roiyeho?source=post_page-----940b2c9d8050--------------------------------)[![Dr.
    Roi Yehoshua](../Images/905a512ffc8879069403a87dbcbeb4db.png)](https://medium.com/@roiyeho?source=post_page-----940b2c9d8050--------------------------------)[](https://towardsdatascience.com/?source=post_page-----940b2c9d8050--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----940b2c9d8050--------------------------------)
    [Dr. Roi Yehoshua](https://medium.com/@roiyeho?source=post_page-----940b2c9d8050--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----940b2c9d8050--------------------------------)
    ·19 min read·Jul 11, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----940b2c9d8050--------------------------------)
    ·阅读时间19分钟·2023年7月11日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/24fe276d9c34eff1399a9e28bbbe6c98.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/24fe276d9c34eff1399a9e28bbbe6c98.png)'
- en: Photo by [Jens Lelie](https://unsplash.com/@madebyjens?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
    on [Unsplash](https://unsplash.com/photos/u0vgcIOQG08?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由[Jens Lelie](https://unsplash.com/@madebyjens?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)提供，出处：[Unsplash](https://unsplash.com/photos/u0vgcIOQG08?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
- en: Gradient boosting is a widely used machine learning technique that is based
    on a combination of **boosting** and **gradient descent**.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度提升是一种广泛使用的机器学习技术，基于**提升**和**梯度下降**的结合。
- en: Boosting is an [ensemble method](https://medium.com/towards-artificial-intelligence/introduction-to-ensemble-methods-226a5a421687)
    that combines multiple weak learners (or base learners) to create a strong predictive
    model. The base models are trained sequentially, where each model focuses on correcting
    the errors made by the previous models.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: Boosting 是一种[集成方法](https://medium.com/towards-artificial-intelligence/introduction-to-ensemble-methods-226a5a421687)，通过将多个弱学习者（或基本学习者）结合起来创建强预测模型。基本模型是按顺序训练的，每个模型侧重于纠正前一个模型的错误。
- en: In **gradient boosting**, each base model is trained to predict the negative
    gradients of the loss function with respect to the predictions of the previous
    models. As a result, adding the newly trained base learner to the ensemble makes
    a step in the steepest descent direction towards the minimum of the loss. This
    process is similar to gradient descent, but it operates in the function space
    rather than the parameter space. Therefore, it is known as **functional gradient
    descent.**
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在**梯度提升**中，每个基本模型被训练以预测相对于前一个模型预测的损失函数的负梯度。因此，将新训练的基本学习者添加到集成中，会朝着损失最小值的最陡下降方向前进。这个过程类似于梯度下降，但它在函数空间中操作而不是参数空间中。因此，它被称为**函数梯度下降**。
- en: When the weak learners are [decision trees](https://medium.com/@roiyeho/decision-trees-part-1-da4e613d2369),
    the resulting method is known as **gradient-boosted decision trees** (GBDT) or
    **gradient boosting machine** (GBM).
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 当弱学习者是[决策树](https://medium.com/@roiyeho/decision-trees-part-1-da4e613d2369)时，所得的方法称为**梯度提升决策树**（GBDT）或**梯度提升机**（GBM）。
- en: Gradient boosting is one of the best algorithms that exist today for dealing
    with structured (tabular) data, and provides state-of-the-art results on many
    standard classification benchmarks. Alongside deep learning, it is one of the
    most commonly used algorithms in Kaggle competitions.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度提升是当前处理结构化（表格）数据的最佳算法之一，并在许多标准分类基准上提供了最先进的结果。与深度学习一起，它是Kaggle竞赛中最常用的算法之一。
- en: The gradient boosting algorithm was originally developed by Jerome Freidman
    in 2001 [1]. Since then, it has evolved into a family of algorithms that includes
    XGBoost, CatBoost and LightGBM. These variants of the algorithm incorporate various
    enhancements that further improve the performance and scalability of gradient
    boosting.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度提升算法最初由Jerome Freidman于2001年开发[1]。从那时起，它演变成了一系列算法，包括XGBoost、CatBoost和LightGBM。这些算法的变体包含了各种改进，进一步提升了梯度提升的性能和可扩展性。
- en: This article covers in depth the theory and implementation of gradient boosting.
    In the first part of the article we will focus on the theoretical concepts of
    gradient boosting, present the algorithm in pseudocode, and demonstrate its usage
    on a small numerical example. In the [second part](/gradient-boosting-from-theory-to-practice-part-2-25c8b7ca566b),
    we will explore the classes in Scikit-Learn that implement gradient boosting,
    and use them to solve different regression and classification tasks.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 本文将深入探讨梯度提升的理论和实现。在文章的第一部分，我们将关注梯度提升的理论概念，展示算法的伪代码，并在一个小的数值示例中演示其使用。在[第二部分](/gradient-boosting-from-theory-to-practice-part-2-25c8b7ca566b)中，我们将探索Scikit-Learn中实现梯度提升的类，并使用它们来解决不同的回归和分类任务。
- en: Intuitive Introduction
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 直观介绍
- en: 'As a reminder, in [supervised machine learning](https://medium.com/@roiyeho/introduction-to-supervised-machine-learning-313730eb5aa2)
    problems, we are given a training set of *n* labeled samples: *D* = {(**x**₁,
    *y*₁), (**x**₂, *y*₂), … , (**x***ₙ, yₙ*)}, where **x***ᵢ* is a *m*-dimensional
    vector that contains the features of sample *i*, and *yᵢ* represents the label
    of that sample. Our goal is to build a model whose predictions are as close as
    possible to the true labels.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 作为提醒，在[监督机器学习](https://medium.com/@roiyeho/introduction-to-supervised-machine-learning-313730eb5aa2)问题中，我们给定了一个
    *n* 个标记样本的训练集： *D* = {(**x**₁, *y*₁), (**x**₂, *y*₂), … , (**x***ₙ, yₙ*)}，其中 **x***ᵢ*
    是一个 *m* 维向量，包含样本 *i* 的特征， *yᵢ* 表示该样本的标签。我们的目标是建立一个模型，其预测尽可能接近真实标签。
- en: For our initial discussion, we will assume that the learning problem is a regression
    problem, i.e., the targets *yᵢ* are continuous values.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的初步讨论中，我们假设学习问题是回归问题，即目标 *yᵢ* 是连续值。
- en: 'The basic idea of gradient boosting is to build an ensemble of weak models,
    where each model is trained to predict the residuals of the previous model. This
    process can be described as follows:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度提升的基本思想是构建一个弱模型的集合，每个模型都被训练来预测前一个模型的残差。这个过程可以描述如下：
- en: Fit a base model *h*₁(**x**) to the given labels *y*.
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将基础模型 *h*₁(**x**) 拟合到给定标签 *y* 上。
- en: Set the initial ensemble to *F*₁(**x**) = *h*₁(**x**).
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将初始集合设置为 *F*₁(**x**) = *h*₁(**x**)。
- en: Fit a base model *h*₂(**x**) to the residuals *y* − *F*₁(**x**).
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将基础模型 *h*₂(**x**) 拟合到残差 *y* − *F*₁(**x**) 上。
- en: 'Combine the two models into a new ensemble: *F*₂(**x**) = *h*₁(**x**) + *h*₂(**x**).
    The predictions of *F*₂(**x**) should be closer to the targets than *F*₁(**x**).'
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将这两个模型组合成一个新的集合： *F*₂(**x**) = *h*₁(**x**) + *h*₂(**x**)。 *F*₂(**x**) 的预测应比
    *F*₁(**x**) 更接近目标。
- en: Fit a base model *h*₃(**x**) to the residuals *y* − *F*₂(**x**).
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将基础模型 *h*₃(**x**) 拟合到残差 *y* − *F*₂(**x**) 上。
- en: 'Combine the three models into a new ensemble: *F*₃(**x**) = *h*₁(**x**) + *h*₂(**x**)
    + *h*₃(**x**). The predictions of *F*₃(**x**) should be closer to the targets
    than *F*₂(**x**).'
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将这三个模型组合成一个新的集合： *F*₃(**x**) = *h*₁(**x**) + *h*₂(**x**) + *h*₃(**x**)。 *F*₃(**x**)
    的预测应比 *F*₂(**x**) 更接近目标。
- en: Continue this process for *M* steps.
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 继续这个过程 *M* 步。
- en: Return *F*ₘ(**x**) as the final hypothesis.
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 返回 *F*ₘ(**x**) 作为最终假设。
- en: We can demonstrate this process in Python by manually building a sequence of
    regression trees, where each tree is trained to predict the residuals of the previous
    trees.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过手动构建一系列回归树来在Python中演示这个过程，每棵树都被训练来预测前面树的残差。
- en: 'Let’s first generate some noisy quadratic training set:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 首先生成一些有噪声的二次训练集：
- en: '[PRE0]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '![](../Images/ac1b75586f81cf18ffd9070201568fae.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ac1b75586f81cf18ffd9070201568fae.png)'
- en: The training data
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 训练数据
- en: 'Our base learners will be decision trees with maximum depth of 2\. The first
    decision tree *h*₁ is fit to the given data set:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的基础学习器将是最大深度为2的决策树。第一个决策树 *h*₁ 适合给定的数据集：
- en: '[PRE1]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The first ensemble *F*₁ consists of this single tree, and its *R*² score on
    the training set is:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个集合 *F*₁ 由这棵单独的树组成，它在训练集上的 *R*² 分数为：
- en: '[PRE2]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '[PRE3]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The second tree *h*₂ is fitted to the residual errors made by the first tree:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 第二棵树 *h*₂ 被拟合到第一棵树的残差错误上：
- en: '[PRE4]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '*h*₂ is added to the ensemble to create the second ensemble *F*₂. We can use
    *F*₂ to make predictions by simply adding up the predictions of both trees:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '*h*₂ 被添加到集成中以创建第二个集成 *F*₂。我们可以通过简单地将两棵树的预测结果相加来使用 *F*₂ 进行预测：'
- en: '[PRE5]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '[PRE6]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Lastly, a third tree *h*₃ is fitted to the residuals of the second ensemble
    *F*₂, and then added to the ensemble:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，一个第三棵树 *h*₃ 被拟合到第二个集成 *F*₂ 的残差上，然后添加到集成中：
- en: '[PRE7]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '[PRE8]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Notice how the *R*² score of the model gradually increases as we add more trees
    to the ensemble.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，随着我们将更多树添加到集成中，模型的 *R*² 分数是如何逐渐增加的。
- en: 'Let’s plot the residuals and the ensemble’s predictions after every iteration:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在每次迭代后绘制残差和集成的预测结果：
- en: '[PRE9]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '![](../Images/455b3f8898987bccc3b27c35ff280fc4.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/455b3f8898987bccc3b27c35ff280fc4.png)'
- en: The base learners and the ensemble after each boosting iteration
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 每次提升迭代后的基础学习器和集成
- en: We can see that the ensemble’s predictions gradually improve as more trees are
    added to the ensemble.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，随着更多树的加入，集成的预测逐渐改进。
- en: 'The generalization of gradient boosting to other types of problems (e.g., classification
    problems) and other loss functions follows from the observation that the residuals
    *hₘ*(**x***ᵢ*) are proportional to the negative gradients of the squared loss
    function with respect to *Fₘ*₋₁(**x***ᵢ*):'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度提升对其他类型问题（例如分类问题）和其他损失函数的推广来源于观察到残差 *hₘ*(**x***ᵢ*) 与平方损失函数关于 *Fₘ*₋₁(**x***ᵢ*)
    的负梯度成比例：
- en: '![](../Images/ea7a34294581ed5b91f8c0ba8de7fd5c.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ea7a34294581ed5b91f8c0ba8de7fd5c.png)'
- en: Therefore, we can generalize this technique to any differentiable loss function
    by using the negative gradients of the loss function instead of the residuals.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们可以通过使用损失函数的负梯度而不是残差，将此技术推广到任何可微损失函数。
- en: The Gradient Boosting Algorithm
  id: totrans-54
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 梯度提升算法
- en: We will now derive the general gradient boosting algorithm for any differentiable
    loss function.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将推导出适用于任何可微损失函数的一般梯度提升算法。
- en: 'Boosting approximates the true mapping from the features to the labels *y*
    = *f*(**x**) using an **additive expansion** (ensemble) of the form:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 提升通过 **加法扩展**（集成）的形式来逼近特征到标签 *y* = *f*(**x**) 的真实映射：
- en: '![](../Images/47ad176295cdd5cd2f8169f5bfa1f5a2.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/47ad176295cdd5cd2f8169f5bfa1f5a2.png)'
- en: where *hₘ*(**x**) are base learners from some class *H* (usually decision trees
    of a fixed size), and *M* is the number of learners.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *hₘ*(**x**) 是来自某个类别 *H* 的基础学习器（通常是固定大小的决策树），*M* 是学习器的数量。
- en: 'Given a loss function *L*(*y*, *F*(**x**)), our goal is to find an approximation
    *F*(**x**) that minimizes the total loss on the training set:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 给定损失函数 *L*(*y*, *F*(**x**))，我们的目标是找到一个近似 *F*(**x**) 以最小化训练集上的总损失：
- en: '![](../Images/540abe44b692b785270dd4f79572703a.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/540abe44b692b785270dd4f79572703a.png)'
- en: The objective function *J* includes functions as parameters (the functions *hₘ*),
    thus it cannot be optimized using traditional optimization methods such as gradient
    descent. Instead, the model is trained in an additive manner, by adding one base
    learner at a time.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 目标函数 *J* 包括作为参数的函数（函数 *hₘ*），因此不能使用传统优化方法如梯度下降进行优化。相反，模型以加法方式训练，每次添加一个基础学习器。
- en: 'We start from a model *F*₀ of a constant function that minimizes the objective:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从一个最小化目标的常数函数模型 *F*₀ 开始：
- en: '![](../Images/5feb064623ac2dffdf1bac92a97177eb.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5feb064623ac2dffdf1bac92a97177eb.png)'
- en: For example, if the loss function is squared loss (used in regression problems),
    *F*₀(**x**) would simply be the mean of the target values.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果损失函数是平方损失（用于回归问题），*F*₀(**x**) 将简单地是目标值的均值。
- en: 'Then, we incrementally expand the model in a greedy fashion:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们以贪婪的方式逐步扩展模型：
- en: '![](../Images/45be628c78c5f0eb51d14eff1fef9cdc.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/45be628c78c5f0eb51d14eff1fef9cdc.png)'
- en: 'where the newly added base learner *hₘ* is fitted to minimize the expected
    loss of the ensemble *Fₘ*:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 新添加的基础学习器 *hₘ* 被拟合以最小化集成 *Fₘ* 的期望损失：
- en: '![](../Images/d351924b182a411b349e8d50e65b1a51.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d351924b182a411b349e8d50e65b1a51.png)'
- en: 'Finding the best function *hₘ* for an arbitrary loss function *L* is computationally
    infeasible, since it would require us to enumerate all the possible functions
    in *H* and pick the best one. Instead, we use an iterative approach: in every
    iteration we choose a base learner *hₘ* that points in the negative gradient direction
    of the loss function. As a result, adding *hₘ* to the ensemble will get us one
    step closer to the minimum loss.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 寻找适用于任意损失函数 *L* 的最佳函数 *hₘ* 在计算上是不可行的，因为这需要我们枚举 *H* 中的所有可能函数并挑选最佳函数。相反，我们使用迭代方法：在每次迭代中，我们选择一个基学习器
    *hₘ*，使其指向损失函数的负梯度方向。因此，将 *hₘ* 添加到集成中将使我们更接近最小损失。
- en: This process is similar to gradient descent, but it operates in the function
    space rather than the parameter space, since in every iteration we move to a different
    function in the hypothesis space *H*, rather than making a step in the parameter
    space of a specific function *h*. This allows *h* to be a non-parametric machine
    learning model, such as a decision tree. This process is known as **functional
    gradient descent**.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 这个过程类似于梯度下降，但它在函数空间中操作，而不是在参数空间中，因为在每次迭代中，我们在假设空间 *H* 中移动到不同的函数，而不是在特定函数 *h*
    的参数空间中迈出一步。这允许 *h* 成为一个非参数的机器学习模型，例如决策树。这个过程被称为**函数梯度下降**。
- en: 'In functional gradient descent, our parameters are the values of *F*(**x**)
    at the points **x**₁, …, **x***ₙ*, and we seek to minimize *L*(*yᵢ*, *F*(**x***ᵢ*))
    at each individual **x***ᵢ*. The best steepest-descent direction of the loss function
    at every point **x***ᵢ* is its negative gradient:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在函数梯度下降中，我们的参数是 *F*(**x**) 在点 **x**₁, …, **x***ₙ* 处的值，我们试图最小化每个 **x***ᵢ* 处的
    *L*(*yᵢ*, *F*(**x***ᵢ*))。每个点 **x***ᵢ* 处损失函数的最佳最陡下降方向是其负梯度：
- en: '![](../Images/4859908d828725534ddb62898b098c3f.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4859908d828725534ddb62898b098c3f.png)'
- en: '*gₘ*(**x***ᵢ*) is the derivative of the loss with respect to its second parameter,
    evaluated at *Fₘ*₋₁(**x***ᵢ*).'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '*gₘ*(**x***ᵢ*) 是损失相对于其第二个参数的导数，在 *Fₘ*₋₁(**x***ᵢ*) 处进行评估。'
- en: Therefore, the vector
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，向量
- en: '![](../Images/30adf0bd11c6ee886b8ce8979fbc0450.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/30adf0bd11c6ee886b8ce8979fbc0450.png)'
- en: gives the best steepest-descent direction in the *n*-dimensional data space
    at *Fₘ*₋₁(**x***ᵢ*). However, this gradient is defined only at the data points
    **x**₁, …, **x***ₙ*, and cannot be generalized to other **x**-values.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 给出了在 *Fₘ*₋₁(**x***ᵢ*) 的 *n* 维数据空间中最佳的最陡下降方向。然而，这个梯度仅在数据点 **x**₁, …, **x***ₙ*
    上定义，不能推广到其他 **x** 值。
- en: In the continuous case, where *H* is the set of arbitrary differentiable functions
    on *R*, we could have simply chosen a function *hₘ* ∈ *H* where *hₘ*(**x***ᵢ*)
    = -*gₘ*(**x***ᵢ*).
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在连续情况下，其中 *H* 是 *R* 上任意可微分函数的集合，我们可以简单地选择一个函数 *hₘ* ∈ *H*，使得 *hₘ*(**x***ᵢ*) =
    -*gₘ*(**x***ᵢ*)。
- en: In the discrete case (i.e., when the set *H* is finite), we choose *hₘ* as a
    function in *H* that is closest to *gₘ*(**x***ᵢ*) at the data points **x***ᵢ*,
    i.e., *hₘ* that is most parallel to the vector -**g***ₘ* in *Rⁿ*. This function
    can be obtained by fitting a base learner *hₘ* to a training set {(**x***ᵢ*, *ỹᵢₘ*)},
    with the labels
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在离散情况下（即，当集合 *H* 是有限的时），我们选择 *hₘ* 作为 *H* 中最接近 *gₘ*(**x***ᵢ*) 在数据点 **x***ᵢ* 处的函数，即
    *hₘ* 最与向量 -**g***ₘ* 在 *Rⁿ* 中平行。这个函数可以通过将基学习器 *hₘ* 拟合到训练集 {(**x***ᵢ*, *ỹᵢₘ*)} 来获得，标签为
- en: '![](../Images/d94bbd5ac3da9e3b37f0102e19fac144.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d94bbd5ac3da9e3b37f0102e19fac144.png)'
- en: These labels are called **pseudo-residuals**. In other words, in every boosting
    iteration, we are fitting a base learner to predict the negative gradients of
    the loss function with respect to the ensemble’s predictions from the previous
    iteration. Note that this approach is heuristic, and does not necessarily yield
    an exact solution to the optimization problem.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 这些标签被称为**伪残差**。换句话说，在每次提升迭代中，我们都将一个基学习器拟合到预测损失函数相对于前一次迭代的集成预测的负梯度。注意，这种方法是启发式的，并不一定能给出优化问题的精确解。
- en: 'The complete pseudocode of the algorithm is shown below:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 算法的完整伪代码如下：
- en: '![](../Images/d67e219b24f67a0d5e59d650158df663.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d67e219b24f67a0d5e59d650158df663.png)'
- en: Gradient Tree Boosting
  id: totrans-83
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 梯度树提升
- en: Gradient tree boosting is a specialization of the gradient boosting algorithm
    to the case where the base learners *h*(**x**) are regression trees (see [this
    article](https://medium.com/@roiyeho/decision-trees-part-2-72adc626cca7) for more
    details on regression trees).
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度树提升是梯度提升算法的一种专门化，适用于基学习器 *h*(**x**) 是回归树的情况（有关回归树的更多细节，请参见[这篇文章](https://medium.com/@roiyeho/decision-trees-part-2-72adc626cca7)）。
- en: At each iteration *m*, a regression tree *hₘ*(**x**) is fit to the pseudo-residuals,
    i.e., we build a tree to predict the pseudo-residuals given our data points. The
    tree is built in a top-down greedy fashion using mean squared error as the splitting
    criterion.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在每次迭代*m*中，一个回归树*hₘ*(**x**)会拟合伪残差，即我们建立一棵树来预测伪残差，给定我们的数据点。树是使用均方误差作为分裂标准以自上而下的贪婪方式构建的。
- en: The question is which label the tree should assign to a given leaf node? Using
    the mean value of the samples in that leaf works well for regression problems,
    but not for other types of problems. Thus, we need to find a general way to assign
    an output value for each leaf node, which will minimize the expected loss of the
    model for any differentiable loss function.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 问题是树应该给定叶节点分配哪个标签？对回归问题，使用该叶子样本的均值效果很好，但对于其他类型的问题则不然。因此，我们需要找到一种通用的方法来为每个叶子节点分配输出值，以最小化模型对任何可微损失函数的预期损失。
- en: 'Let *Jₘ* be the number of the leaves in the tree. The tree partitions the input
    space into *Jₘ* disjoint regions: *R*₁*ₘ*, …, *Rⱼₘ*, and predicts a constant value'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 让*Jₘ*表示树中叶子的数量。树将输入空间划分为*Jₘ*个不相交的区域：*R*₁*ₘ*，…，*Rⱼₘ*，并预测一个常量值。
- en: '*γⱼₘ* in each one:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 每一个中的*γⱼₘ*：
- en: '![](../Images/ff242667cdcc9bf23a9efca071394d24.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ff242667cdcc9bf23a9efca071394d24.png)'
- en: where 1(⋅) is the indicator function, which has the value 1 if its argument
    is true, and 0 otherwise.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 其中1(⋅)是指示函数，其值为1（如果其参数为真），否则为0。
- en: 'Our goal is to find the coefficient *γⱼₘ* in each regionthat will minimize
    the total loss induced by the points that belong to that region:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的目标是在每个区域中找到系数*γⱼₘ*，以最小化由属于该区域的点引起的总损失：
- en: '![](../Images/b7071f1bb910bac9a4afb16ff5e966e3.png)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b7071f1bb910bac9a4afb16ff5e966e3.png)'
- en: i.e., we are trying to find the optimal constant update in each leaf node’s
    region to be added to the predictions of the previous ensemble *Fₘ*₋₁(**x**).
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 即，我们试图找到每个叶子节点区域中的最优常量更新，以便添加到之前的集成*Fₘ*₋₁(**x**)的预测中。
- en: 'For example, let’s find the optimal *γ* for the case of least-squares regression.
    In this case, our objective is to minimize the following sum of squared losses:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，让我们找到最小二乘回归情况下的最优*γ*。在这种情况下，我们的目标是最小化以下平方损失的总和：
- en: '![](../Images/78787d1de042f04d20fa9306aa87806f.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/78787d1de042f04d20fa9306aa87806f.png)'
- en: 'We now take the derivative of *E* with respect to *γ* and set it to 0:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在对*E*关于*γ*进行求导并设其为0：
- en: '![](../Images/7b3cc8d72f473e324d36445c6ab0baa6.png)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7b3cc8d72f473e324d36445c6ab0baa6.png)'
- en: 'That is, the optimal *γ* is simply the mean of the residuals in the *j*th leaf
    node at the *m*th iteration:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 即，最优的*γ*就是第*m*次迭代中*j*th叶节点的残差均值：
- en: '![](../Images/952e96b8746219e740d2d08c9def4872.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/952e96b8746219e740d2d08c9def4872.png)'
- en: The case of classification will be discussed later.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 分类的情况将在后续讨论。
- en: 'Once we find the optimal output values for the leaf nodes, the current approximation
    *Fₘ*₋₁(**x**) is separately updated in each corresponding region:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们找到叶节点的最优输出值，当前的近似*Fₘ*₋₁(**x**)将在每个相应区域中分别更新：
- en: '![](../Images/36244622093967fe59686439db3d9fea.png)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/36244622093967fe59686439db3d9fea.png)'
- en: Regularization
  id: totrans-103
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 正则化
- en: Compared to standard decision trees, gradient-boosted trees are fairly robust
    to overfitting. Nevertheless, there are several commonly used regularization techniques
    that help to control the complexity of gradient-boosted trees.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 与标准决策树相比，梯度提升树对过拟合相当稳健。尽管如此，仍有几种常用的正则化技术可以帮助控制梯度提升树的复杂性。
- en: First, we can use the same regularization techniques that we have in standard
    decision trees, such as limiting the depth of the tree, the number of leaves,
    or the minimum number of samples required to split a node. We can also use post-pruning
    techniques to remove branches from the tree that fail to reduce the loss by a
    predefined threshold.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们可以使用与标准决策树相同的正则化技术，例如限制树的深度、叶子数量或分裂节点所需的最小样本数量。我们还可以使用后剪枝技术来移除那些未能通过预定义阈值减少损失的树枝。
- en: Second, we can control the number of boosting iterations (i.e., the number of
    trees in the ensemble). Increasing the number of trees reduces the ensemble’s
    error on the training set, but may also lead to overfitting. The optimal number
    of trees is typically found by **early stopping**, i.e., the algorithm is terminated
    once the score on the validation set does not improve for a specified number of
    iterations.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，我们可以控制提升迭代的数量（即，集成中的树木数量）。增加树木数量减少了集成在训练集上的误差，但也可能导致过拟合。最佳的树木数量通常通过**早停法**来确定，即算法在验证集上的评分在指定的迭代次数内没有改善时终止。
- en: 'Lastly, Friedman [1, 2] has suggested the following regularization techniques,
    which are more specific to gradient-boosted trees:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，Friedman [1, 2] 提出了以下更具体于梯度提升树的正则化技术：
- en: Shrinkage
  id: totrans-108
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 缩减
- en: 'Shrinkage [1] scales the contribution of each base learner by a constant factor
    *ν*:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 缩减 [1] 通过一个常数因子*ν*来缩放每个基础学习器的贡献：
- en: '![](../Images/d71c451e5c83348dff4e840a915e1cd2.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d71c451e5c83348dff4e840a915e1cd2.png)'
- en: The parameter *ν* (0 < *ν* ≤ 1) is called the **learning rate**, as it controls
    the step size of the gradient descent procedure. Similar to a learning rate in
    stochastic optimization, shrinkage reduces the impact of each individual learner
    and leaves room for future learners to improve the model.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 参数*ν*（0 < *ν* ≤ 1）称为**学习率**，因为它控制梯度下降过程的步长。类似于随机优化中的学习率，缩减减少了每个个体学习器的影响，并为未来的学习者改进模型留出了空间。
- en: Empirically, it has been found that using small learning rates (e.g., *ν* ≤
    0.1) can significantly improve the model’s generalization ability. However, smaller
    learning rates also require more boosting iterations in order to maintain the
    same training error, thereby increasing the computational time during both training
    and prediction.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 实证研究发现，使用小学习率（例如，*ν* ≤ 0.1）可以显著提高模型的泛化能力。然而，更小的学习率也需要更多的提升迭代以保持相同的训练误差，从而增加了训练和预测的计算时间。
- en: Stochastic Gradient Boosting (Subsampling)
  id: totrans-113
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 随机梯度提升（子采样）
- en: In a follow-up paper [2], Friedman proposed stochastic gradient boosting, which
    combines gradient boosting with bagging.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 在随后的论文 [2] 中，Friedman 提出了随机梯度提升，它将梯度提升与装袋相结合。
- en: In each iteration, a base learner is trained only on a fraction (typically 0.5)
    of the training set, drawn at random without replacement. This subsampling procedure
    introduces randomness into the algorithm and helps prevent the model from overfitting.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 在每次迭代中，基础学习器仅在训练集的一个子集（通常为 0.5）上训练，该子集是随机抽取的且不替换。这种子采样过程为算法引入了随机性，并有助于防止模型过拟合。
- en: Like in bagging, subsampling also allows us to use the **out-of-bag samples**
    (samples that were not involved in building the next base learner) in order to
    evaluate the performance of the model, instead of having an independent validation
    data set. Out-of-bag estimates often underestimate the real performance of the
    model, thus they are used only if cross-validation takes too much time.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 与装袋方法类似，子采样也允许我们使用**袋外样本**（未参与构建下一个基础学习器的样本）来评估模型的性能，而不是使用独立的验证数据集。袋外估计通常低估了模型的实际性能，因此仅在交叉验证花费时间过长时使用。
- en: Another strategy to reduce the variance of the model is to randomly sample the
    features considered for split in each node of the tree (similar to [random forests](https://medium.com/@roiyeho/random-forests-98892261dc49)).
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种减少模型方差的策略是随机采样每个树节点中考虑进行拆分的特征（类似于 [随机森林](https://medium.com/@roiyeho/random-forests-98892261dc49)）。
- en: 'The pseudocode for gradient tree boosting with shrinkage is shown below:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 带有缩减的梯度树提升的伪代码如下所示：
- en: '![](../Images/88f1a968d909e91ea603e9bdabfe6557.png)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/88f1a968d909e91ea603e9bdabfe6557.png)'
- en: Gradient Tree Boosting for Classification
  id: totrans-120
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分类的梯度树提升
- en: The same algorithm of gradient tree boosting can also be used for classification
    tasks. However, since the sum of the trees *Fₘ*(**x**) can be any continuous value,
    it needs to be mapped into a probability (a value between 0 and 1). This mapping
    depends on the type of the classification problem (binary or multiclass).
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 相同的梯度树提升算法也可以用于分类任务。然而，由于树的和*Fₘ*(**x**)可以是任何连续值，因此需要将其映射为概率（介于 0 和 1 之间的值）。这种映射依赖于分类问题的类型（二分类或多分类）。
- en: Note that gradient-boosted trees are always regression trees, even when they
    are used for classification problems (since they are built to approximate the
    negative gradient of the loss function).
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，梯度提升树始终是回归树，即使它们被用于分类问题（因为它们是为了逼近损失函数的负梯度而构建的）。
- en: Binary Classification
  id: totrans-123
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 二分类
- en: In binary classification problems, we use the sigmoid function
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 在二分类问题中，我们使用的是 sigmoid 函数
- en: '*σ*(*x*) to model the probability that a samplebelongs to the positive class
    (similar to [logistic regression](/mastering-logistic-regression-3e502686f0ae)):'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '*σ*(*x*) 用于建模样本属于正类的概率（类似于 [逻辑回归](/mastering-logistic-regression-3e502686f0ae)）：'
- en: '![](../Images/39268c31dc0bde71291ea09129a8d3b9.png)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/39268c31dc0bde71291ea09129a8d3b9.png)'
- en: 'This means that *F*(**x**) represents the predicted log odds, i.e., the logarithm
    of the odds ratio:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着 *F*(**x**) 代表预测的对数几率，即对数几率比：
- en: '![](../Images/7a3729ce994a9b3229a21dda25a152af.png)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7a3729ce994a9b3229a21dda25a152af.png)'
- en: 'The initial model *F*₀(**x**) is given by the log odds of the positive class
    in the training set:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 初始模型 *F*₀(**x**) 由训练集中正类的对数几率给出：
- en: '![](../Images/a2bda3209feab1b1adf9494b6da77ff5.png)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a2bda3209feab1b1adf9494b6da77ff5.png)'
- en: 'As in logistic regression, we use the binary log loss as our loss function:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 与逻辑回归类似，我们使用二元对数损失作为我们的损失函数：
- en: '![](../Images/98132ed9ab141e05c3c69bee9cc0a714.png)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/98132ed9ab141e05c3c69bee9cc0a714.png)'
- en: where *pᵢ* = *σ*(*Fₘ*₋₁(**x***ᵢ*)) represents the predicted probability of the
    previous ensemble that sample **x***ᵢ* belongs to the positive class.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *pᵢ* = *σ*(*Fₘ*₋₁(**x***ᵢ*)) 代表之前集成模型预测的样本 **x***ᵢ* 属于正类的概率。
- en: 'We can compute the gradient of this loss with respect to *Fₘ*₋₁(**x***ᵢ*) using
    the chain rule:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用链式法则计算关于 *Fₘ*₋₁(**x***ᵢ*) 的损失梯度：
- en: '![](../Images/9561eca5d0715f13ed7ca6f5d20871c5.png)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9561eca5d0715f13ed7ca6f5d20871c5.png)'
- en: 'We used here the fact that the derivative of the sigmoid function is:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里使用了 sigmoid 函数的导数：
- en: '![](../Images/1411a40fa9e3a455aebcbd3d2f812eed.png)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1411a40fa9e3a455aebcbd3d2f812eed.png)'
- en: 'Therefore, the pseudo-residual at point **x***ᵢ* is:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，点 **x***ᵢ* 处的伪残差为：
- en: '![](../Images/525cbe9573acd7778eb95619a0264329.png)'
  id: totrans-139
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/525cbe9573acd7778eb95619a0264329.png)'
- en: i.e., the pseudo-residual is simply the actual label of **x***ᵢ* minus the predicted
    probability of the previous ensemble that this sample belongs to the positive
    class.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 即，伪残差就是**x***ᵢ*的实际标签减去之前集成模型预测的该样本属于正类的概率。
- en: 'With regression trees as base learners, we need to find the optimal output
    values *γⱼₘ* for each leaf node:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 以回归树作为基础学习器，我们需要为每个叶节点找到最优输出值 *γⱼₘ*：
- en: '![](../Images/b7071f1bb910bac9a4afb16ff5e966e3.png)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b7071f1bb910bac9a4afb16ff5e966e3.png)'
- en: There is no closed-form solution to this optimization problem (note that *γ*
    is added to the log-odds and not to the class probabilities directly). Therefore,
    we approximate the solution by performing a single [Newton-Raphson](https://en.wikipedia.org/wiki/Newton%27s_method)
    step.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 这个优化问题没有封闭形式的解（请注意 *γ* 被添加到对数几率中，而不是直接添加到类别概率中）。因此，我们通过执行单次 [牛顿-拉夫森](https://en.wikipedia.org/wiki/Newton%27s_method)
    步骤来逼近解。
- en: 'As a reminder, Newton’s method tries to find the minimum of a twice-differentiable
    function *f*: *R* → *R*, by building a sequence {*xₖ*} from an initial guess *x*₀
    ∈ *R* . This sequence is constructed from the second-order Taylor approximations
    of *f* around the elements *xₖ*.'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 作为提醒，牛顿方法试图通过从初始猜测*x*₀ ∈ *R*开始构建一个序列 {*xₖ*} 来寻找一个二次可微函数*f* 的最小值：*R* → *R*。这个序列是通过围绕元素*xₖ*的*f*
    的二阶泰勒近似构造的。
- en: 'The second-order Taylor expansion of *f* around *xₖ* is:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '*f* 在 *xₖ* 周围的二阶泰勒展开为：'
- en: '![](../Images/d2615913639b06f046cc532536d97653.png)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d2615913639b06f046cc532536d97653.png)'
- en: 'The next element in the sequence *xₖ*₊₁ is chosen as to minimize this quadratic
    expansion in *t*. The minimum can be found by setting the derivative of this expansion
    to 0:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 序列 *xₖ*₊₁ 中的下一个元素被选择为最小化 *t* 中的二次展开。最小值可以通过将该展开的导数设为 0 来找到：
- en: '![](../Images/0ceb6e19b1685d180a4b2e7a81440433.png)'
  id: totrans-148
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0ceb6e19b1685d180a4b2e7a81440433.png)'
- en: 'Thus, the minimum is achieved for:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，最小值可以通过以下公式得到：
- en: '![](../Images/52831f103e73813a29ea6a78a83a0328.png)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/52831f103e73813a29ea6a78a83a0328.png)'
- en: 'Therefore, Newton’s method performs the following iteration:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，牛顿方法执行以下迭代：
- en: '![](../Images/96d5000d99ad41012a8672d83183e139.png)'
  id: totrans-152
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/96d5000d99ad41012a8672d83183e139.png)'
- en: 'Similarly, we can write the second-order Taylor expansion of the loss function
    around the point *Fₘ*₋₁(**x**):'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，我们可以写出围绕点 *Fₘ*₋₁(**x**) 的损失函数的二阶泰勒展开：
- en: '![](../Images/d1104c9fbe6ccde1b3078c4976b9d14a.png)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d1104c9fbe6ccde1b3078c4976b9d14a.png)'
- en: 'Taking the derivative of this expression with respect to *γ* yields:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 对这个表达式对*γ*求导得到：
- en: '![](../Images/898f07d29f774881cdc93f3ae5839df8.png)'
  id: totrans-156
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/898f07d29f774881cdc93f3ae5839df8.png)'
- en: 'Therefore, the derivative of the total loss induced by the samples in region
    *Rⱼₘ* is:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，由*Rⱼₘ*区域中的样本引起的总损失的导数是：
- en: '![](../Images/26a5c2b6186b1b6da89af07159ba0f3f.png)'
  id: totrans-158
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/26a5c2b6186b1b6da89af07159ba0f3f.png)'
- en: 'Equating this derivative to 0 gives us the optimal output value for region
    *Rⱼₘ*:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 将这个导数等于0给出我们*Rⱼₘ*区域的最佳输出值：
- en: '![](../Images/05f74825042062c846dd2e23066a39ac.png)'
  id: totrans-160
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/05f74825042062c846dd2e23066a39ac.png)'
- en: The numerator of this expression is simply the sum of the derivatives of the
    loss function at the data points in region *Rⱼₘ* (their pseudo-residuals), while
    the denominator is the sum of the second derivatives of the loss functions at
    the same points.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 这个表达式的分子是*Rⱼₘ*区域内数据点（它们的伪残差）损失函数导数的总和，而分母是相同点损失函数二阶导数的总和。
- en: 'We have already found the first derivative of log loss:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经找到了对数损失的第一导数：
- en: '![](../Images/bdd69f086611167b53389e8e4a949b85.png)'
  id: totrans-163
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bdd69f086611167b53389e8e4a949b85.png)'
- en: 'We now need to find its second derivative. This derivative can be simply computed
    by taking the derivative of the first derivative of log loss:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在需要找到它的第二导数。这个导数可以通过对对数损失的第一导数进行求导来简单计算：
- en: '![](../Images/50fafdac041811747a968c7f925a4319.png)'
  id: totrans-165
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/50fafdac041811747a968c7f925a4319.png)'
- en: 'Therefore, we can write:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们可以写出：
- en: '![](../Images/c4a697ffaafd24438cffd86a915808a2.png)'
  id: totrans-167
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c4a697ffaafd24438cffd86a915808a2.png)'
- en: Example on a Toy Dataset
  id: totrans-168
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在一个玩具数据集上的示例
- en: 'Assume that we are given the following data set for a binary classification
    problem:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们得到以下数据集用于二分类问题：
- en: '![](../Images/3df833b109806ce842ad43e94ff9654c.png)'
  id: totrans-170
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3df833b109806ce842ad43e94ff9654c.png)'
- en: 'The goal is to predict whether a customer will buy a given product based on
    three attributes: the customer’s age, level of income (Low, Medium or High) and
    level of education (High School or College).'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 目标是根据三个属性预测客户是否会购买某个产品：客户的年龄、收入水平（低、中或高）和教育水平（高中或大学）。
- en: To solve this problem we are going to use an ensemble of gradient-boosted trees
    with a maximum depth of 2, and a learning rate of *ν* = 0.5 (we use a relatively
    large learning rate for illustration purposes). The trees will use only binary
    splits (as in the CART algorithm).
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这个问题，我们将使用一个最大深度为2的梯度提升树集成，学习率为*ν* = 0.5（我们使用相对较大的学习率以便于说明）。这些树将仅使用二元分裂（如CART算法）。
- en: 'First, we initialize the model with a constant value, which is the log odds
    of the positive class:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们用一个常数值初始化模型，该值是正类的对数几率：
- en: '![](../Images/cb0f4011e1d4fd5f1642eb2576aa5ea7.png)'
  id: totrans-174
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/cb0f4011e1d4fd5f1642eb2576aa5ea7.png)'
- en: 'Next, we calculate the pseudo-residuals. For the log loss function, these are
    simply the actual labels minus the predicted labels:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们计算伪残差。对于对数损失函数，这些只是实际标签减去预测标签：
- en: '![](../Images/0555ee74c12ec1f262d619314eb4596b.png)'
  id: totrans-176
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0555ee74c12ec1f262d619314eb4596b.png)'
- en: We now fit a regression tree to the pseudo-residuals. We start by finding the
    best split for the root node. In regression trees, at every node we select a split
    that yields the highest reduction in the variance of the values stored at that
    node (the pseudo-residuals in our case).
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将回归树拟合到伪残差上。我们首先找到根节点的最佳分裂。在回归树中，我们在每个节点选择一个分裂，使得节点上存储的值的方差（在我们这里是伪残差）降低最大。
- en: 'The mean of the residuals at the root node is:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 根节点的残差均值为：
- en: '![](../Images/5c5db46b3b7066cae94a3808c12593f0.png)'
  id: totrans-179
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5c5db46b3b7066cae94a3808c12593f0.png)'
- en: 'Therefore, the variance is just the average of their squared values:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，方差只是它们平方值的平均数：
- en: '![](../Images/e68ad71774b324dc455ade707396e320.png)'
  id: totrans-181
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e68ad71774b324dc455ade707396e320.png)'
- en: 'We will now compute the reduction in the variance that can be achieved by every
    possible split in every feature. We start from the two categorical attributes:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将计算每个特征中每个可能分裂所能实现的方差减少量。我们从两个类别属性开始：
- en: '![](../Images/3447126bfe32ffce2feceac9f925a3b5.png)'
  id: totrans-183
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3447126bfe32ffce2feceac9f925a3b5.png)'
- en: 'For the Age attribute, we sort the pseudo-residuals by age, and then consider
    each middle point between two consecutive ages as a candidate split point:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 对于年龄属性，我们按年龄对伪残差进行排序，然后考虑两个连续年龄之间的每个中点作为候选分裂点：
- en: '![](../Images/718d82d33035c763fb495a3b19303453.png)![](../Images/1be8c7031639b81454ff3b0294b1e305.png)'
  id: totrans-185
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/718d82d33035c763fb495a3b19303453.png)![](../Images/1be8c7031639b81454ff3b0294b1e305.png)'
- en: 'The split that provides the highest reduction in the variance is Age < 26.5\.
    Therefore, the first level of the tree looks as follows:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 提供最大方差减少的分裂点是 Age < 26.5。因此，树的第一层如下所示：
- en: '![](../Images/7daf3de86887a338c791c91d8578a0c9.png)'
  id: totrans-187
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7daf3de86887a338c791c91d8578a0c9.png)'
- en: The variance of the residuals in the left child node is 0, thus there is no
    need to split it anymore. We now need to find the best split for the right child
    node.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 左子节点的残差方差为0，因此无需再分裂。我们现在需要找到右子节点的最佳分裂点。
- en: 'First, let’s compute the variance of the residuals at the right child node:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们计算右子节点残差的方差：
- en: '![](../Images/258c8b005b4e6d0cfb84578fd8895911.png)'
  id: totrans-190
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/258c8b005b4e6d0cfb84578fd8895911.png)'
- en: 'We now consider all the possible splits for the four samples that belong to
    the right child node (samples 2, 4, 5, 6):'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们考虑属于右子节点的四个样本（样本2、4、5、6）的所有可能的分裂点：
- en: '![](../Images/b5f531f790353825965e50e52c51dff6.png)![](../Images/2b430ac37bd6e672942faec5fc82134c.png)'
  id: totrans-192
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b5f531f790353825965e50e52c51dff6.png)![](../Images/2b430ac37bd6e672942faec5fc82134c.png)'
- en: 'In this case we have multiple candidate splits that lead to the maximum reduction
    in the variance (0.0835). Let’s choose arbitrarily the split Income = Medium.
    The resulting regression tree is:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们有多个候选分裂点，这些分裂点能最大程度减少方差（0.0835）。我们可以随意选择分裂点 Income = Medium。得到的回归树是：
- en: '![](../Images/71b1ebf486276d2455c0ffe9f8b88956.png)'
  id: totrans-194
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/71b1ebf486276d2455c0ffe9f8b88956.png)'
- en: Next, we compute the optimal output values for the leaf nodes (the *γ* coefficients).
    Note that since this is our first tree, the most recent predicted probability
    for all the samples is *p* = *σ*(*F*₀(**x**)) = 0.667.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们计算叶节点的最佳输出值（*γ* 系数）。请注意，由于这是我们的第一棵树，所有样本的最新预测概率是 *p* = *σ*(*F*₀(**x**))
    = 0.667。
- en: 'The output value for the leftmost leaf is:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 最左边叶子的输出值是：
- en: '![](../Images/6258dffe26d55b3831cd1ab053eba23a.png)'
  id: totrans-197
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6258dffe26d55b3831cd1ab053eba23a.png)'
- en: 'Similarly, the output values for the other two leaves are:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，其他两个叶子的输出值是：
- en: '![](../Images/75487212d4914567c66e4a5b04da5181.png)'
  id: totrans-199
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/75487212d4914567c66e4a5b04da5181.png)'
- en: 'Thus, we get the following predictions from the first regression tree:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们从第一棵回归树中得到以下预测：
- en: '![](../Images/1c5b36ebb4529862dfd3d75dbcde5651.png)'
  id: totrans-201
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1c5b36ebb4529862dfd3d75dbcde5651.png)'
- en: 'We now scale these predictions by the learning rate and add them to the predictions
    of the previous iteration. We then use the new predictions to calculate the pseudo-residuals
    for the next iteration:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将这些预测值按学习率进行缩放，并将其加到上一轮的预测值中。然后，我们使用新的预测值计算下一轮的伪残差：
- en: '![](../Images/35d5b827208151720ea9b95c5bad688a.png)'
  id: totrans-203
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/35d5b827208151720ea9b95c5bad688a.png)'
- en: 'We now fit another regression tree to the pseudo-residuals. Following the same
    process as in the previous iteration, we get the following tree (verify that this
    is indeed the resulting tree!):'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在对伪残差拟合另一棵回归树。按照与之前相同的过程，我们得到以下树（验证这确实是结果树！）：
- en: '![](../Images/f18406a47649634c2bff10e0170f3918.png)'
  id: totrans-205
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f18406a47649634c2bff10e0170f3918.png)'
- en: 'Next, we compute the output values for the leaf nodes. Note that this time
    the most recent predicted probabilities *p* = *σ*(*F*₁(**x**)) are not the same
    for all the samples. Going from the leftmost leaf node to the rightmost node we
    obtain:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们计算叶节点的输出值。请注意，这一次最新的预测概率 *p* = *σ*(*F*₁(**x**)) 对所有样本并不相同。从最左边的叶节点到最右边的节点，我们得到：
- en: '![](../Images/ea12814f97da1b337aedd758766b8e07.png)'
  id: totrans-207
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ea12814f97da1b337aedd758766b8e07.png)'
- en: 'Thus, we get the following predictions from the second tree:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们从第二棵树中得到以下预测：
- en: '![](../Images/4fbb691c185c3ac4c11a64de5a0e7c5f.png)'
  id: totrans-209
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4fbb691c185c3ac4c11a64de5a0e7c5f.png)'
- en: 'We now scale these predictions by the learning rate and add them to the predictions
    of the previous model:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将这些预测值按学习率进行缩放，并将其加到之前模型的预测值中：
- en: '![](../Images/de73779f8804c46f5729db19b0b621e7.png)'
  id: totrans-211
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/de73779f8804c46f5729db19b0b621e7.png)'
- en: We can see that after three iterations, our ensemble correctly classifies all
    the samples in the training set. Hurray!
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到经过三次迭代，我们的集成方法正确地分类了训练集中的所有样本。太棒了！
- en: 'Let’s see how we can use this ensemble to make predictions on new samples.
    Assume that we have a new customer with the following attributes: Age = 33, Income
    = Medium, Education = High School.'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何使用这个集成方法对新样本进行预测。假设我们有一个新客户，属性如下：年龄 = 33，收入 = 中等，教育水平 = 高中。
- en: We first need to find to which terminal region this sample belongs in each tree
    of the ensemble. In the first tree, the sample belongs to region *R*₂₁ (since
    [Age < 26.5] is false and [Income = Medium] is true), and in the second tree it
    belongs to region *R*₂₂ (since [Education = High School] is true and [Age < 30]
    is false).
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先需要找出样本在每棵树中的哪个终端区域。在第一棵树中，样本属于区域 *R*₂₁（因为 [年龄 < 26.5] 为假，[收入 = 中等] 为真），而在第二棵树中，它属于区域
    *R*₂₂（因为 [教育 = 高中] 为真，[年龄 < 30] 为假）。
- en: 'Therefore, the predicted log odds that this customer will buy the product is:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，该客户购买产品的预测对数几率为：
- en: '![](../Images/f9bc2cf6f1757f546668224a6f5cb03a.png)'
  id: totrans-216
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f9bc2cf6f1757f546668224a6f5cb03a.png)'
- en: 'And the predicted probability is:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 预测的概率为：
- en: '![](../Images/e29c0872690c2e8cd85aa8e0d2d7b976.png)'
  id: totrans-218
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e29c0872690c2e8cd85aa8e0d2d7b976.png)'
- en: Since *p* < 0.5, our prediction is that this customer will not buy the product.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 *p* < 0.5，我们的预测是该客户不会购买该产品。
- en: Multiclass Classification
  id: totrans-220
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 多类别分类
- en: 'In multiclass classification problems, *K* trees (for *K* classes) are built
    at each of the *M* iterations. The probability that **x***ᵢ* belongs to class
    *k* is modeled as the softmax of the *Fₘ,ₖ*(**x***ᵢ*) values:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 在多类别分类问题中，每次迭代的 *M* 轮中都会构建 *K* 棵树（对应 *K* 类）。**x***ᵢ* 属于类别 *k* 的概率被建模为 *Fₘ,ₖ*(**x***ᵢ*)
    值的 softmax：
- en: '![](../Images/064140faf34afefde187d62bee9ac2f5.png)'
  id: totrans-222
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/064140faf34afefde187d62bee9ac2f5.png)'
- en: The initial model in this case is given by the prior probability of each class,
    and the loss function is the cross-entropy loss. The derivation of the coefficients
    *γⱼₘ* in this case is left as an exercise to the reader.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，初始模型由每个类别的先验概率给出，损失函数是交叉熵损失。*γⱼₘ* 系数的推导留给读者作为练习。
- en: Final Notes
  id: totrans-224
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 最终说明
- en: 'You can find the code examples of this article on my github: [https://github.com/roiyeho/medium/tree/main/gradient_boosting](https://github.com/roiyeho/medium/tree/main/gradient_boosting)'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在我的 GitHub 上找到本文的代码示例：[https://github.com/roiyeho/medium/tree/main/gradient_boosting](https://github.com/roiyeho/medium/tree/main/gradient_boosting)
- en: The second part of this article can be found [here](/gradient-boosting-from-theory-to-practice-part-2-25c8b7ca566b).
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 本文的第二部分可以在 [这里](/gradient-boosting-from-theory-to-practice-part-2-25c8b7ca566b)
    找到。
- en: Thanks for reading!
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 感谢阅读！
- en: References
  id: totrans-228
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] Friedman, J.H. (2001). [Greedy function approximation: A gradient boosting
    machine](https://doi.org/10.1214/aos/1013203451). Annals of Statistics, 29, 1189–1232.'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] Friedman, J.H. (2001). [贪婪函数逼近：一种梯度提升机器](https://doi.org/10.1214/aos/1013203451).
    统计年鉴, 29, 1189–1232.'
- en: '[2] Friedman, J.H. (2002). [Stochastic gradient boosting](https://statweb.stanford.edu/~jhf/ftp/stobst.pdf).
    Computational Statistics & Data Analysis, 38, 367–378.'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] Friedman, J.H. (2002). [随机梯度提升](https://statweb.stanford.edu/~jhf/ftp/stobst.pdf).
    计算统计与数据分析, 38, 367–378.'
