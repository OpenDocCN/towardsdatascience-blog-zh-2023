- en: 'Emojis Aid Social Media Sentiment Analysis: Stop Cleaning Them Out!'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 表情符号有助于社交媒体情感分析：不要再清除它们了！
- en: 原文：[https://towardsdatascience.com/emojis-aid-social-media-sentiment-analysis-stop-cleaning-them-out-bb32a1e5fc8e?source=collection_archive---------5-----------------------#2023-01-31](https://towardsdatascience.com/emojis-aid-social-media-sentiment-analysis-stop-cleaning-them-out-bb32a1e5fc8e?source=collection_archive---------5-----------------------#2023-01-31)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/emojis-aid-social-media-sentiment-analysis-stop-cleaning-them-out-bb32a1e5fc8e?source=collection_archive---------5-----------------------#2023-01-31](https://towardsdatascience.com/emojis-aid-social-media-sentiment-analysis-stop-cleaning-them-out-bb32a1e5fc8e?source=collection_archive---------5-----------------------#2023-01-31)
- en: Leverage emojis in social media sentiment analysis to improve accuracy.
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在社交媒体情感分析中利用表情符号来提高准确性。
- en: '[](https://medium.com/@bc3088?source=post_page-----bb32a1e5fc8e--------------------------------)[![Bale
    Chen](../Images/de6276419f1f93de346387a5ea2cc5b8.png)](https://medium.com/@bc3088?source=post_page-----bb32a1e5fc8e--------------------------------)[](https://towardsdatascience.com/?source=post_page-----bb32a1e5fc8e--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----bb32a1e5fc8e--------------------------------)
    [Bale Chen](https://medium.com/@bc3088?source=post_page-----bb32a1e5fc8e--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@bc3088?source=post_page-----bb32a1e5fc8e--------------------------------)[![Bale
    Chen](../Images/de6276419f1f93de346387a5ea2cc5b8.png)](https://medium.com/@bc3088?source=post_page-----bb32a1e5fc8e--------------------------------)[](https://towardsdatascience.com/?source=post_page-----bb32a1e5fc8e--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----bb32a1e5fc8e--------------------------------)
    [Bale Chen](https://medium.com/@bc3088?source=post_page-----bb32a1e5fc8e--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F8695cd8317da&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Femojis-aid-social-media-sentiment-analysis-stop-cleaning-them-out-bb32a1e5fc8e&user=Bale+Chen&userId=8695cd8317da&source=post_page-8695cd8317da----bb32a1e5fc8e---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----bb32a1e5fc8e--------------------------------)
    ·14 min read·Jan 31, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fbb32a1e5fc8e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Femojis-aid-social-media-sentiment-analysis-stop-cleaning-them-out-bb32a1e5fc8e&user=Bale+Chen&userId=8695cd8317da&source=-----bb32a1e5fc8e---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F8695cd8317da&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Femojis-aid-social-media-sentiment-analysis-stop-cleaning-them-out-bb32a1e5fc8e&user=Bale+Chen&userId=8695cd8317da&source=post_page-8695cd8317da----bb32a1e5fc8e---------------------post_header-----------)
    发布于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----bb32a1e5fc8e--------------------------------)
    ·14 分钟阅读·2023年1月31日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fbb32a1e5fc8e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Femojis-aid-social-media-sentiment-analysis-stop-cleaning-them-out-bb32a1e5fc8e&user=Bale+Chen&userId=8695cd8317da&source=-----bb32a1e5fc8e---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fbb32a1e5fc8e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Femojis-aid-social-media-sentiment-analysis-stop-cleaning-them-out-bb32a1e5fc8e&source=-----bb32a1e5fc8e---------------------bookmark_footer-----------)![](../Images/37923fbf0be08bd4aef8544f5266e515.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fbb32a1e5fc8e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Femojis-aid-social-media-sentiment-analysis-stop-cleaning-them-out-bb32a1e5fc8e&source=-----bb32a1e5fc8e---------------------bookmark_footer-----------)![](../Images/37923fbf0be08bd4aef8544f5266e515.png)'
- en: Photo by [Denis Cherkashin](https://unsplash.com/@denic?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
    on [Unsplash](https://unsplash.com/s/photos/emojis?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由 [Denis Cherkashin](https://unsplash.com/@denic?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
    提供，来源于 [Unsplash](https://unsplash.com/s/photos/emojis?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
- en: '**TL;DR:**'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '**简明扼要：**'
- en: Including emojis in the social media sentiment analysis would robustly improve
    the sentiment classification accuracy no matter what model you use or how you
    incorporate emojis in the loop
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 无论你使用什么模型，或如何在分析中纳入表情符号，将表情符号包括在社交媒体情感分析中都能显著提高情感分类的准确性
- en: More than half of the popular BERT-based encoders don’t support emojis
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 超过一半的流行 BERT 基础编码器不支持表情符号
- en: Twitter-RoBERTa encoder performs the best in sentiment analysis and coordinates
    well with emojis
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Twitter-RoBERTa编码器在情感分析中表现最佳，并且与表情符号协调良好
- en: Instead of cleaning emojis out, converting them to their textual description
    can help boost sentiment classification accuracy and handle the out-of-vocabulary
    issue.
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与其清除表情符号，不如将它们转换为文本描述，这可以帮助提高情感分类的准确性并处理词汇表外的问题。
- en: As social media has become an essential part of people’s lives, the content
    that people share on the Internet is highly valuable to many parties. Many modern
    natural language processing (NLP) techniques were deployed to understand the general
    public’s social media posts. Sentiment Analysis is one of the most popular and
    critical NLP topics that focuses on analyzing opinions, sentiments, emotions,
    or attitudes toward entities in written texts computationally [[1](#1a95)]. Social
    media sentiment analysis (SMSA) is thus a field of understanding and learning
    representations for the sentiments expressed in short social media posts.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 随着社交媒体成为人们生活的重要组成部分，人们在互联网上分享的内容对许多方面都极具价值。许多现代自然语言处理（NLP）技术被应用于理解公众的社交媒体帖子。情感分析是最受欢迎和最关键的NLP话题之一，侧重于计算性地分析对书面文本中实体的意见、情感、情绪或态度[[1](#1a95)]。因此，社交媒体情感分析（SMSA）是一个理解和学习在短社交媒体帖子中表达情感的领域。
- en: Another important feature of this project is the cute little in-text graphics
    — emojis😄. These graphical symbols have increasingly gained ground in social media
    communications. According to [Emojipedia’s statistics](https://emojipedia.org/stats/)
    in 2021, a famous emoji reference site, over one-fifth of the tweets now contains
    emojis (21.54%), while over half of the comments on Instagram include emojis.
    Emojis are handy and concise ways to express emotions and convey meanings, which
    may explain their great popularity.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 这个项目的另一个重要特征是可爱的文内图形——表情符号😄。这些图形符号在社交媒体通讯中越来越受到欢迎。根据[Emojipedia的统计数据](https://emojipedia.org/stats/)（2021年），一个著名的表情符号参考网站，现在超过五分之一的推文包含表情符号（21.54%），而超过一半的Instagram评论中包含表情符号。表情符号是一种方便且简洁的表达情感和传达意义的方式，这可能解释了它们的巨大受欢迎程度。
- en: However ubiquitous emojis are in network communications, they are not favored
    by the field of NLP and SMSA. In the stage of preprocessing data, emojis are usually
    removed alongside other unstructured information like URLs, stop words, unique
    characters, and pictures [[2](#c7e8)]. While some researchers have started to
    study the potential of including emojis in SMSA in recent years, it remains a
    niche approach and awaits further research. This project aims to **examine the
    emoji-compatibility of trending BERT encoders** and **explore different methods
    of incorporating emojis in SMSA to improve accuracy**.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管表情符号在网络通信中无处不在，但它们在NLP和SMSA领域并不受青睐。在数据预处理阶段，表情符号通常会与其他非结构化信息如URL、停用词、特殊字符和图片一起被删除[[2](#c7e8)]。虽然一些研究人员近年来开始研究在SMSA中包含表情符号的潜力，但这仍然是一种小众方法，等待进一步研究。这个项目旨在**检验流行的BERT编码器对表情符号的兼容性**并**探索将表情符号融入SMSA以提高准确性的方法**。
- en: Table of Contents
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 目录
- en: '**1** [**Background Knowledge**](#776f)'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '**1** [**背景知识**](#776f)'
- en: 1.1 [What is SMSA exactly?](#4843)
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 1.1 [SMSA究竟是什么？](#4843)
- en: 1.2 [Development of Sentiment Analysis Methodologies](#d8db)
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 1.2 [情感分析方法的发展](#d8db)
- en: 2[**Experiment**](#cfe0)2.1 [Model design](#40f1)
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 2[**实验**](#cfe0)2.1 [模型设计](#40f1)
- en: 2.2 [Lesson learned in Data Preparation Stage (A Sad Story)](#2f88)
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 2.2 [数据准备阶段的经验教训（一个悲伤的故事）](#2f88)
- en: 2.3 [Emoji-compatibility Test of the BERT family](#666a)
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 2.3 [BERT家族的表情符号兼容性测试](#666a)
- en: 2.4 [Experimenting Methods to Preprocess Emojis](#bc42)
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 2.4 [实验方法来预处理表情符号](#bc42)
- en: '**3** [**Results Discussion**](#df89) **4** [**Conclusion**](#fa8d)[**Acknowledgments**](#5d30)[**Reference**](#e8ad)'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '**3** [**结果讨论**](#df89) **4** [**结论**](#fa8d)[**致谢**](#5d30)[**参考文献**](#e8ad)'
- en: 1 Background Knowledge
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 1 背景知识
- en: 1.1 What is SMSA exactly?
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.1 SMSA究竟是什么？
- en: 'Here is some background knowledge about SMSA you might want to know before
    looking into the actual experiment. No technical background/math is required so
    far. Let me first explain the intuition of the most typical SMSA task:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是一些关于SMSA的背景知识，你可能希望在深入实际实验之前了解。到目前为止，不需要技术背景/数学知识。让我首先解释最典型的SMSA任务的直觉：
- en: '![](../Images/08d91363c13fa841f5deb5f015f9ded1.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/08d91363c13fa841f5deb5f015f9ded1.png)'
- en: SMSA Example
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: SMSA示例
- en: As the picture above shows, given a social media post, the model (represented
    by the gray robot) will output the prediction of its sentiment label. In this
    example, the model responds that this post is 57.60% likely to express positive
    sentiment, 12.38% likely to be negative, and 30.02% likely to be neutral. Some
    studies classify posts in a binary way, i.e. positive/negative, but others consider
    “neutral” as an option as well. This project follows the latter.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 如上图所示，给定一个社交媒体帖子，模型（由灰色机器人表示）将输出其情感标签的预测。在这个例子中，模型回应该帖子 57.60% 可能表达正面情感，12.38%
    可能是负面，30.02% 可能是中立的。一些研究将帖子分类为二元方式，即正面/负面，但其他研究也考虑了“中立”作为选项。本项目遵循后者。
- en: 1.2 Development of Sentiment Analysis Methodologies
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.2 情感分析方法的发展
- en: To my best knowledge, the first quantitative approach to studying social media
    sentiment is using the **lexicon-based method**. The model has a predefined lexicon
    that maps each token to a sentiment score. So, given a sentence, the model consults
    the lexicon, aggregates the sentiment scores of each word, and outputs the overall
    sentiment score. Very intuitive, right?
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 据我了解，研究社交媒体情感的首个定量方法是使用**词汇表方法**。该模型有一个预定义的词汇表，将每个词元映射到情感分数。因此，给定一个句子，模型查询词汇表，汇总每个词的情感分数，并输出总体情感分数。非常直观，对吧？
- en: '![](../Images/a25d24f3eec39ee769107336e506031b.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a25d24f3eec39ee769107336e506031b.png)'
- en: Basic Diagram of a Lexicon-based Sentiment Analysis Model
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 基于词汇表的情感分析模型基本图
- en: '[SentiWordNet](https://github.com/aesuli/SentiWordNet) and [VADER](https://github.com/cjhutto/vaderSentiment)
    are the two paradigms of this kind that have been favored by both the industry
    and academia.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '[SentiWordNet](https://github.com/aesuli/SentiWordNet) 和 [VADER](https://github.com/cjhutto/vaderSentiment)
    是这种方法的两个范例，受到了业界和学术界的青睐。'
- en: With the development of **machine learning**, classifiers like SVM, Random Forests,
    Multi-layer Perceptron, etc., gained ground in sentiment analysis. However, textual
    input isn’t valid for those models, so those classifiers are compounded with **word
    embedding models** to perform sentiment analysis tasks. Word embedding models
    convert words into numerical vectors that machines could play with. [Google’s
    word2vec](https://arxiv.org/abs/1301.3781) embedding model was a great breakthrough
    in representation learning for textual data, followed by [GloVe by Pennington
    et al.](https://nlp.stanford.edu/projects/glove/) and [fasttext by Facebook](https://fasttext.cc/).
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 随着**机器学习**的发展，像 SVM、随机森林、多层感知器等分类器在情感分析中获得了立足之地。然而，这些模型不适用于文本输入，因此这些分类器与**词嵌入模型**结合以执行情感分析任务。词嵌入模型将词语转换为机器可以操作的数值向量。[Google
    的 word2vec](https://arxiv.org/abs/1301.3781) 嵌入模型是文本数据表示学习的重大突破，其后是[Pennington
    等人的 GloVe](https://nlp.stanford.edu/projects/glove/)和[Facebook 的 fasttext](https://fasttext.cc/)。
- en: Due to the sequential nature of natural language and the immense popularity
    of Deep Learning, **Recurrent Neural Network (RNN)** then becomes “the popular
    kid.” RNN decodes, or “reads”, the sequence of word embeddings in order, preserving
    the sequential structure in the loop, which lexicon-based models and traditional
    machine learning models didn’t achieve.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 由于自然语言的顺序特性和深度学习的极大普及，**递归神经网络（RNN）** 成为了“流行的小子”。RNN 按顺序解码或“读取”词嵌入序列，在循环中保留顺序结构，而词汇表模型和传统机器学习模型未能实现这一点。
- en: '![](../Images/c4ff03168aa444fe5c62fe3842d61634.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c4ff03168aa444fe5c62fe3842d61634.png)'
- en: A Typical Workflow of SMSA Nowadays
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 现代 SMSA 的典型工作流程
- en: The evolved workflow is explained in the diagram above. Word embeddings are
    passed into an RNN model that outputs the last hidden state(s) (If you don’t know
    what the last hidden state is, it’s intuitively the “summary” composed by the
    RNN after “reading” all the text.) Lastly, we use a feed-forward fully connected
    neural network to map the high-dimensional hidden state to a sentiment label.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 上图解释了演变后的工作流程。词嵌入被传入 RNN 模型，模型输出最后一个隐藏状态（如果你不清楚最后的隐藏状态是什么，直观上它是 RNN 在“读取”完所有文本后“总结”的内容）。最后，我们使用前馈全连接神经网络将高维隐藏状态映射到情感标签上。
- en: We are almost there! The last piece of the puzzle is the **Transformer models**.
    Even if you haven’t learned NLP, you still might have heard about “Attention is
    All You Need” [[3](#1c64)]. In this paper, they proposed the self-attention technique
    and developed the Transformer Model.These models are so powerful that it transcends
    the previous models in almost every subtask of NLP. If you are not familiar with
    Transformer models, I strongly recommend you read [this introductory article](/transformers-141e32e69591)
    by Giuliano Giacaglia.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 我们快完成了！最后一块拼图是**Transformer模型**。即使你还没有学习NLP，你可能也听说过“Attention is All You Need”[[3](#1c64)]。在这篇论文中，他们提出了自注意力技术，并开发了Transformer模型。这些模型非常强大，以至于在几乎所有NLP的子任务中超越了之前的模型。如果你不熟悉Transformer模型，我强烈推荐你阅读Giuliano
    Giacaglia的[这篇入门文章](/transformers-141e32e69591)。
- en: Both industry and academia have started to use the pretrained Transformer models
    on a large scale due to their unbeatable performance. Thanks to the [Hugging Face](http://huggingface.co/models)
    *transformer* package, developers can now easily import and deploy those large
    pretrained models. BERT, aka. Bidirectional Encoder Representations for Transformer,
    is the most famous transformer-based encoder model that learns excellent representations
    for text. Later on, RoBERTa, BERTweet, DeBERTa, etc., were developed based on
    BERT.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 由于预训练Transformer模型具有无与伦比的性能，业界和学术界已经开始大规模使用这些模型。多亏了[Hugging Face](http://huggingface.co/models)的*transformer*包，开发者现在可以轻松导入和部署这些大型预训练模型。BERT，即双向编码器表示的Transformer，是最著名的基于Transformer的编码器模型，它为文本学习出色的表示。之后，基于BERT开发了RoBERTa、BERTweet、DeBERTa等模型。
- en: 2 Experiments
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 2 实验
- en: 'With all those background knowledge, we can now dive into the experiments and
    programming parts! If you do feel not confident with the mechanism of NLP, I recommend
    you to skip the technical details or go read some introductory blogs about NLP
    on Towards Data Science. Let’s clarify our experiment objectives first. We want
    to know:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这些背景知识，我们现在可以深入到实验和编程部分了！如果你对NLP机制感到不自信，我建议你跳过技术细节或阅读一些关于NLP的入门博客，例如在Towards
    Data Science上。首先，我们要明确实验目标。我们想知道：
- en: how compatible the currently trending pretrained BERT-based models are with
    emoji data.
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 目前流行的基于预训练BERT模型的兼容性如何与emoji数据。
- en: how the performance would be influenced if we incorporate emojis in the SMSA
    process.
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果我们在SMSA过程中加入emoji，性能会受到什么影响。
- en: what exactly we should do in the data processing stage to include the emojis.
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在数据处理阶段，我们应该如何处理以包含emoji。
- en: 2.1 Model Design
  id: totrans-49
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.1 模型设计
- en: 'Our model follows the aforementioned neural network paradigm that consists
    of a pretrained BERT-based encoder, a Bi-LSTM layer, and a feedforward fully connected
    network. The diagram is shown below:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的模型遵循上述的神经网络范式，由预训练的基于BERT的编码器、Bi-LSTM层和前馈全连接网络组成。图示如下：
- en: '![](../Images/1fe39eea86b8886152593fa59c468784.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1fe39eea86b8886152593fa59c468784.png)'
- en: Model diagram
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 模型图
- en: To be clear, a preprocessed tweet is first passed through the pretrained encoder
    and becomes a sequence of representational vectors. Then, the representational
    vectors are passed through the Bi-LSTM layer. The two last hidden states of the
    two directions of LSTM will be processed by the feedforward layer to output the
    final prediction of the tweet’s sentiment.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 为了清楚，预处理的推文首先通过预训练编码器，变成一个表示向量序列。然后，表示向量通过Bi-LSTM层。LSTM的两个方向的两个最后隐藏状态将由前馈层处理，以输出推文情感的最终预测。
- en: We alter the encoder models and emoji preprocessing methods to observe the varying
    performance. The Bi-LSTM and feedforward layers are configured in the same way
    for all experiments in order to control variables. In the training process, we
    only train the Bi-LSTM and feed-forward layers. The parameters of pretrained encoder
    models are frozen.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 我们改变编码器模型和emoji预处理方法，以观察性能的变化。为了控制变量，Bi-LSTM和前馈层在所有实验中配置相同。在训练过程中，我们只训练Bi-LSTM和前馈层。预训练编码器模型的参数被冻结。
- en: The PyTorch implementation of this model and other technical details can be
    found in my [GitHub Repo](https://github.com/BaleChen/emoji-setiment-analysis).
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型的PyTorch实现及其他技术细节可以在我的[GitHub Repo](https://github.com/BaleChen/emoji-setiment-analysis)中找到。
- en: 2.2 Lesson Learnt in Data Preparation Stage
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.2 数据准备阶段的经验教训
- en: 'Data availability is every data science researcher’s pain in the neck. At first,
    I wanted to find a benchmark Twitter sentiment analysis dataset where I can compare
    the results with the previous models, but I encountered the following setbacks:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 数据的可用性是每个数据科学研究者的痛点。起初，我想找一个基准的Twitter情感分析数据集，以便将结果与之前的模型进行比较，但我遇到了以下障碍：
- en: Most datasets only have “tweet ID” as a query key to find the original content.
    To access the original tweet with the IDs, I need to have **Twitter API access**.
    My professor mentor and I both tried to apply for one, but neither of us was approved
    (We still don’t know why).
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 大多数数据集只有“推文ID”作为查询键来查找原始内容。要访问带有ID的原始推文，我需要**Twitter API访问权限**。我和我的教授导师都尝试申请过，但我们都没有获得批准（我们仍然不知道原因）。
- en: Well…another problem is that **a large portion of the tweets already perished**!
    This means either they were deleted by the author or by the Twitter server for
    some reason.
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 嗯……另一个问题是**大部分推文已经消失**！这意味着它们要么被作者删除，要么因为某种原因被Twitter服务器删除了。
- en: Even though there are few datasets that directly store tweet content, those
    stored in ***.csv or *.tsv formats are unable to preserve emojis**. Namely, the
    original tweets have emojis, but the compiled dataset that I downloaded from the
    web completely lost all emojis.
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 尽管有很少的数据集直接存储推文内容，但存储在***.csv或*.tsv格式的数据集无法保留表情符号**。也就是说，原始推文中有表情符号，但我从网上下载的编译数据集完全丢失了所有表情符号。
- en: So, whenever you want to conduct Twitter sentiment analysis, make sure you first
    validate the dataset if the dataset store tweets by their Tweet ID, which require
    you to spend extra effort to retrieve the original text. Tweets can easily perish
    if the dataset is from years ago. Also, don’t expect too much on applying for
    Twitter API. My mentor, who is an assistant professor at a prestigious American
    university, can’t even meet their requirement (for some unknown reason). Lastly,
    to preserve the emojis, don’t ever save them in csv or tsv format. Pickle, xlsx,
    or json can be your good choices.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，无论何时你想进行Twitter情感分析，确保首先验证数据集是否通过Tweet ID存储推文，这需要你额外的努力来检索原始文本。如果数据集是几年前的，推文很容易消失。此外，不要对申请Twitter
    API抱有太高的期望。我的导师是一位美国著名大学的助理教授，但也无法满足他们的要求（原因不明）。最后，为了保留表情符号，千万不要将其保存为csv或tsv格式。Pickle、xlsx或json可能是你的好选择。
- en: Anyways, to find a dataset that retains emojis, has sentiment labels, and is
    of desirable size was extremely hard for me. Eventually, I found this Novak et
    al’s [dataset](https://figshare.com/articles/dataset/Emoji_Sentiment_Ranking/1600931)
    satisfies all criteria.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 无论如何，找到一个保留表情符号、具有情感标签且大小合适的数据集对我来说非常困难。最终，我发现Novak等人的[数据集](https://figshare.com/articles/dataset/Emoji_Sentiment_Ranking/1600931)符合所有标准。
- en: 2.3 Emoji-compatibility Test of the BERT family
  id: totrans-63
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.3 BERT家族的表情符号兼容性测试
- en: Before implementing the BERT-based encoders, we need to know whether they are
    compatible with emojis, i.e. whether they can produce unique representations for
    emoji tokens. More specifically, before passing the tweet into an encoder, it
    will first be ***tokenized*** by a model tokenizer that is unique to the encoder
    (e.g. RoBERTa-base uses the RoBERTa-base tokenizer, while BERT-base uses the BERT-base
    tokenizer). What the tokenizer does is splitting the long strings of textual input
    into individual word tokens that are in the vocabulary (shown in the graph below).
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在实现基于BERT的编码器之前，我们需要了解它们是否与表情符号兼容，即是否能为表情符号标记生成独特的表示。更具体地说，在将推文传递给编码器之前，它会首先由模型分词器进行***分词***，这种分词器是特定于编码器的（例如，RoBERTa-base使用RoBERTa-base分词器，而BERT-base使用BERT-base分词器）。分词器的作用是将长字符串的文本输入拆分为词汇表中的单个词标记（如下图所示）。
- en: '![](../Images/46fd0a58138ad2eeca109b73da2a8f1b.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/46fd0a58138ad2eeca109b73da2a8f1b.png)'
- en: spaCy’s rule-based Tokenizer ([source](https://github.com/explosion/spaCy))
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: spaCy的基于规则的分词器 ([source](https://github.com/explosion/spaCy))
- en: In our case, if emojis are not in the tokenizer vocabulary, then they will all
    be tokenized into an unknown token (e.g. “<UNK>”). Encoder models will thus produce
    the same vector representation for all those unknown tokens, in which case cleaning
    or not cleaning out the emojis will technically not make any difference in the
    model performance.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的案例中，如果表情符号不在分词器词汇表中，它们将被全部标记为未知标记（例如“<UNK>”）。因此，编码器模型将对所有这些未知标记产生相同的向量表示，在这种情况下，清理或不清理表情符号在模型性能上技术上不会有任何区别。
- en: I chose the following list of common BERT-based encoders.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 我选择了以下常见的基于BERT的编码器列表。
- en: ALBERT-base-v2
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ALBERT-base-v2
- en: BERT-base, BERT-large
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: BERT-base, BERT-large
- en: BERTweet-base, BERTweet-large
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: BERTweet-base, BERTweet-large
- en: DeBERTa-base, DeBERTa-large
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DeBERTa-base, DeBERTa-large
- en: DistilBERT
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DistilBERT
- en: RoBERTa-base, RoBERTa-large
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RoBERTa-base, RoBERTa-large
- en: Twitter-RoBERTa
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Twitter-RoBERTa
- en: XLMRoBERTa-base, XLMRoBERTa-large
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: XLMRoBERTa-base, XLMRoBERTa-large
- en: 'The test can be easily done using the HuggingFace [*transformers*](https://pypi.org/project/transformers/)
    package and the [*emoji*](https://pypi.org/project/emoji/) package. We first import
    them:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用HuggingFace的[*transformers*](https://pypi.org/project/transformers/)包和[*emoji*](https://pypi.org/project/emoji/)包轻松完成测试。我们首先导入它们：
- en: '[PRE0]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: AutoTokenizer is a very useful function where you can use the name of the model
    to load the corresponding tokenizer, like the following one-line code where I
    import the BERT-base tokenizer.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: AutoTokenizer是一个非常有用的功能，你可以使用模型名称来加载相应的分词器，比如下面这行代码，其中我导入了BERT-base分词器。
- en: '[PRE1]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Then, we use the *emoji* package to obtain the full list of emojis and use the
    encode and decode function to detect compatibility.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们使用*emoji*包来获取完整的表情符号列表，并使用编码和解码功能来检测兼容性。
- en: '[PRE2]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 2.4 Experimenting Methods to Preprocess Emojis
  id: totrans-83
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.4 实验方法来预处理表情符号
- en: We came up with 5 ways of data preprocessing methods to make use of the emoji
    information as opposed to removing emojis (rm) from the original tweets.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提出了5种数据预处理方法，以利用表情符号信息，而不是从原始推文中删除表情符号 (rm)。
- en: '**Directly encode (dir)** Use the pretrained encoder models that support emojis
    to directly vectorize the emojis. In this way, emojis are treated as normal word
    tokens. This is the most straightforward method.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '**直接编码 (dir)** 使用支持表情符号的预训练编码器模型直接将表情符号向量化。这样，表情符号就会被视为普通的词令。这是最直接的方法。'
- en: '**Replacing emojis with descriptions (emoji2desc)** The pretrained encoders
    are not specifically trained to create representations for emojis. Rather, they
    are trained on a vast amount of text. We conjecture that encoders might have better
    representations for words than emojis, so converting emojis to their official
    description might help better extract the semantic information. For example, “I
    love animals 😍” will become “I love animals smiling face with heart-eyes.” The
    python realization is shown below (using the *emoji* package):'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '**用描述替换表情符号 (emoji2desc)** 预训练的编码器并没有专门针对表情符号创建表示。相反，它们是在大量文本上进行训练的。我们推测编码器可能对词语的表示要优于表情符号，因此将表情符号转换为其官方描述可能有助于更好地提取语义信息。例如，“我爱动物
    😍”将变成“我爱动物 笑脸带心眼”。下面的Python实现展示了如何使用*emoji*包：'
- en: '[PRE3]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '**Concatenate emojis (concat-emoji)** Essentially, we reposition the emojis
    to the end of the sentence and perform directly encode method. Since emojis don’t
    belong to the grammatical structure of the sentences, we want to know if repositioning
    them would help better distinguish the textual and emoji information. For example,
    “The cold weather is killing me🧊. Don’t wanna work any longer😡😭. ” becomes “The
    cold weather is killing me. Don’t wanna work any longer. 🧊😡😭”'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '**连接表情符号 (concat-emoji)** 本质上，我们将表情符号重新定位到句子的末尾，并执行直接编码方法。由于表情符号不属于句子的语法结构，我们想知道重新定位它们是否有助于更好地区分文本和表情符号信息。例如，“天气太冷了🧊。
    不想再工作了😡😭。” 变成 “天气太冷了。 不想再工作了。 🧊😡😭”'
- en: '[PRE4]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '**Concatenate description (concat-desc)** Besides, we also tested replacing
    those repositioned emojis with their textual descriptions.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '**连接描述 (concat-desc)** 此外，我们还测试了用文本描述替换那些重新定位的表情符号。'
- en: '[PRE5]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '**Meta-feature (meta)** Instead of treating emojis as part of the sentence,
    we can also regard them as high-level features. We use the Emoji Sentiment Ranking
    [[4](#de51)] lexicon to get the positivity, neutrality, negativity, and sentiment
    score features. Then, we concatenate those features with the emoji vector representations,
    which form the emoji meta-feature vector of the tweet. This vector harbors the
    emoji sentiment information of the tweet. Pure text will be as usual passed through
    the encoder and BiLSTM layer, then the meta-feature vector will be concatenated
    with the last hidden states from the BiLSTM layer to be the input of the feedforward
    layers. This process is essentially isolating the emojis from the sentence and
    treating them as meta-data of a tweet.'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '**元特征（meta）** 除了将表情符号视为句子的一部分外，我们还可以将它们视为高级特征。我们使用表情符号情感排名 [[4](#de51)] 词典来获取积极性、中立性、消极性和情感分数特征。然后，我们将这些特征与表情符号的向量表示连接起来，形成推文的表情符号元特征向量。这个向量包含了推文的表情符号情感信息。纯文本将照常通过编码器和BiLSTM层，然后将元特征向量与BiLSTM层的最后隐藏状态连接，作为前馈层的输入。这个过程本质上是将表情符号从句子中隔离开来，将它们视为推文的元数据。'
- en: '**3 Results Discussion**'
  id: totrans-93
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**结果讨论**'
- en: With all those technical designs, we finally arrive at the results part. First,
    let’s look at the emoji-compatibility of those commonBERT-based encoder models.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 经过所有这些技术设计，我们最终进入了结果部分。首先，让我们看看这些常见的BERT基础编码器模型的表情符号兼容性。
- en: '![](../Images/bae1c8a4f2a8a36d4375f2199ed2ccb7.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bae1c8a4f2a8a36d4375f2199ed2ccb7.png)'
- en: Emoji-compatibility of BERT-based emoji models
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: BERT基础的表情符号模型的表情符号兼容性
- en: '**More than half of those models can’t recognize all emojis! RoBERTa** (both
    base and large versions), **DeBERTa** (both base and large versions), **BERTweet-large**,
    and **Twitter-RoBERTa** support all emojis. However, common encoders like **BERT**
    (both base and large versions), **DistilBERT**, and **ALBERT** nearly do not support
    any emoji.'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '**超过一半的这些模型不能识别所有表情符号！** RoBERTa（基础版和大版）、**DeBERTa**（基础版和大版）、**BERTweet-large**
    和 **Twitter-RoBERTa** 支持所有表情符号。然而，像 **BERT**（基础版和大版）、**DistilBERT** 和 **ALBERT**
    这样的常见编码器几乎不支持任何表情符号。'
- en: Now, let’s compare the model performance with different emoji-compatible encoders
    and different methods to incorporate emojis. The percentage in the following graph
    indicates the sentiment classification accuracy. Each cell represents the accuracy
    of an encoder model with a certain preprocessing method.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们比较不同表情符号兼容编码器和不同方法下的模型性能。下图中的百分比表示情感分类的准确率。每个单元格代表某种预处理方法下的编码器模型的准确率。
- en: (Note that *emoji2vec* is a baseline model that is developed in 2015\. It’s
    not BERT-based but it’s a predefined emoji-embedding model that can also produce
    vector representation for emojis. It can be seen as an extension of Google’s Word2vec
    model)
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: （请注意 *emoji2vec* 是一个于2015年开发的基准模型。它不是BERT基础的，而是一个预定义的表情符号嵌入模型，也可以生成表情符号的向量表示。它可以看作是Google的Word2vec模型的扩展）
- en: '![](../Images/9603acca512c29461bcc59bc9041739b.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9603acca512c29461bcc59bc9041739b.png)'
- en: Sentiment Classification Accuracy
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 情感分类准确率
- en: To compare different methods to incorporate emojis into the SMSA process, we
    also show the accuracy across different methods with confidence intervals.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 为了比较将表情符号融入SMSA过程中的不同方法，我们还展示了不同方法下的准确率及置信区间。
- en: '![](../Images/564a0d218182070e34434676c064f680.png)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/564a0d218182070e34434676c064f680.png)'
- en: The average accuracy of each preprocessing method (with confidence interval)
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 每种预处理方法的平均准确率（带置信区间）
- en: One of the most significant insights is that **including emojis, no matter how
    you include them, enhances the performance of** SMSA **models.** Removing the
    emojis lowers the accuracy by 1.202% on average. For methods that include emojis,
    the overlapping confidence intervals indicate a relatively blurry distinction.
    There’s no “generally best” method to utilize emojis.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 一个重要的发现是，**无论如何包含表情符号，都能提升** SMSA **模型的性能。** 移除表情符号平均降低了1.202%的准确率。对于包含表情符号的方法，重叠的置信区间表示出相对模糊的区分。没有“通常最佳”的方法来利用表情符号。
- en: '![](../Images/9e8a528a73b2c060276b63a66e7b3861.png)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9e8a528a73b2c060276b63a66e7b3861.png)'
- en: The average accuracy of each encoder model (with confidence interval)
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 每个编码器模型的平均准确率（带置信区间）
- en: For comparison among all encoder models, the results are shown in the bar chart
    above. The confidence interval is also annotated on the top of the bar chart.
    Small confidence intervals imply **high statistical confidence in the ranking**.
    **Twitter-RoBERTa performed the best** across all models, which is very likely
    caused by the **training domain**. ***emoji2vec***, which was developed in 2015
    and prior to the boom of transformer models, holds **relatively poor representations
    of emojis** under the standards of this time.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 在所有编码模型中进行比较的结果如上方柱状图所示。柱状图顶部也标注了置信区间。小的置信区间表示**对排名的统计信心较高**。**Twitter-RoBERTa在所有模型中表现最佳**，这很可能是由**训练领域**造成的。***emoji2vec***，该模型开发于2015年并早于变压器模型的兴起，在当前标准下对表情符号的**表示相对较差**。
- en: Now that no “generally best” method is found, we probe into how different models
    would benefit differently from various preprocessing methods. The following graph
    depicts the percentage improvement of using a certain preprocessing method compared
    with removing emojis at the beginning.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 既然没有找到“普遍最佳”的方法，我们深入探讨不同模型如何从各种预处理方法中获得不同的收益。下图展示了使用某种预处理方法与一开始移除表情符号相比的百分比改进。
- en: '![](../Images/e41ec99904cff81302c9a289b4d89a1f.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e41ec99904cff81302c9a289b4d89a1f.png)'
- en: Percentage improvement heatmap
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 百分比改进热图
- en: Firstly, all the improvement indices are positive, which strongly justifies
    the usefulness of emojis in SMSA. Including emojis in the data would improve the
    SMSA model’s performance.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，所有的改进指标都是正数，这强有力地证明了表情符号在SMSA中的有用性。在数据中包含表情符号将提高SMSA模型的性能。
- en: '**Generally for BERT-based models, directly encoding emojis seems to be a sufficient
    and sometimes the best method.** Surprisingly, the most straightforward methods
    work just as well as the complicated ones, if not better.'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '**通常，对于基于BERT的模型，直接编码表情符号似乎是一种足够且有时是最好的方法。** 令人惊讶的是，最简单的方法效果与复杂的方法一样好，甚至更好。'
- en: '**Poor emoji representation learning models might benefit more from converting
    emojis to textual descriptions.** Maximal and minimal improvement both appear
    on the emoji2vec model. It’s likely that emoji2vec has relatively worse vector
    representations of emojis, but converting emojis to their textual descriptions
    would help capture the emotional meanings of a social media post.'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '**表情符号表示学习模型较差的情况可能会从将表情符号转换为文本描述中受益更多。** emoji2vec模型的改进既出现最大值也出现最小值。很可能emoji2vec对表情符号的向量表示较差，但将表情符号转换为文本描述有助于捕捉社交媒体帖子的情感意义。'
- en: '**RoBERTa-large displayed an unexpectedly small improvement regardless of preprocessing
    methods**, indicating that it doesn’t benefit as much from the emojis as other
    BERT-based models. This result might be explained by the fact that RoBERTa-large’s
    architecture might be more suitable for learning representations for pure text
    than for emojis, but it still awaits a more rigorous justification.'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '**RoBERTa-large无论预处理方法如何显示出意外的小改进**，这表明它不像其他基于BERT的模型那样受益于表情符号。这个结果可能是因为RoBERTa-large的架构可能更适合学习纯文本的表示而不是表情符号，但仍需要更严格的论证。'
- en: 4 Conclusion
  id: totrans-116
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 4 结论
- en: From this project, the key takeaway is that **including emojis in the loop of
    SMSA would improve the sentiment classification accuracy no matter what model
    or preprocessing method you use. So, THINK TWICE about cleaning them out when
    you face a social media sentiment analysis task!**
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 从这个项目中，关键的收获是**无论你使用什么模型或预处理方法，在SMSA过程中包含表情符号都会提高情感分类的准确性。因此，当你面对社交媒体情感分析任务时，*一定要仔细考虑*是否清除它们！**
- en: The best model to handle SMSA tasks and coordinate with emojis is the [Twitter-RoBERTa](https://huggingface.co/cardiffnlp/twitter-roberta-base-sentiment)
    encoder! Please use it if you are dealing with Twitter data and analyzing tweet
    sentiment.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 处理SMSA任务并协调表情符号的最佳模型是[Twitter-RoBERTa](https://huggingface.co/cardiffnlp/twitter-roberta-base-sentiment)编码器！如果你正在处理Twitter数据和分析推文情感，请使用它。
- en: Regarding how to incorporate the emojis specifically, the methods didn’t show
    a significant difference, so a straightforward way — directly treating the emojis
    as regular word tokens — would do the job perfectly. Yet, considering that half
    of the common BERT-based encoders in our study don’t support emojis, we recommend
    using the [emoji2desc](#ff68) method. That means converting emojis to their official
    textual description using [a simple line of code](#57b7) I mentioned before, which
    can easily handle the out-of-vocabulary emoji tokens.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 关于如何具体融入表情符号，方法之间并没有显著差异，因此一种直接的方法——将表情符号直接视为普通单词标记——可以完美解决问题。然而，考虑到我们研究中一半的常见BERT编码器不支持表情符号，我们建议使用[emoji2desc](#ff68)方法。也就是说，使用之前提到的[一行简单代码](#57b7)将表情符号转换为其官方文本描述，这可以轻松处理词汇表外的表情符号标记。
- en: If you are using traditional word embeddings like *word2vec* and you also don’t
    want to waste the cute emojis, consider using the [emoji2desc](#ff68) or [concat-emoji](#afb8)
    method instead of using *emoji2vec* model.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你使用传统的词嵌入如 *word2vec*，而且你也不想浪费可爱的表情符号，可以考虑使用[emoji2desc](#ff68)或[concat-emoji](#afb8)方法，而不是使用
    *emoji2vec* 模型。
- en: Hope our project can guide SMSA researchers and industry workers on how to include
    emojis in the process. More importantly, this project offers a new perspective
    on improving SMSA accuracy. Diving into the technical bits is not necessarily
    the only way to make progress, and for example, these simple but powerful emojis
    can help as well.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 希望我们的项目能够指导SMSA研究人员和行业工作者如何在过程中包含表情符号。更重要的是，这个项目提供了一种提高SMSA准确性的全新视角。深入研究技术细节并不是唯一的进展方式，例如，这些简单却强大的表情符号也可以提供帮助。
- en: Scripts, an academic report, and more can be found in my [GitHub Repo](https://github.com/BaleChen/emoji-setiment-analysis).
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 脚本、学术报告及更多内容可以在我的[GitHub Repo](https://github.com/BaleChen/emoji-setiment-analysis)中找到。
- en: Regarding images in the post, all unless otherwise noted are by the author.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 关于文章中的图片，除非另有说明，否则均为作者提供。
- en: '**Acknowledgments**'
  id: totrans-124
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**致谢**'
- en: I would like to extend my warmest gratitude to my research supervisor and mentor
    Professor [Mathieu Laurière](https://mlauriere.github.io/). He provides me with
    insightful advice and guides me through this summer research. It is my great honor
    and pleasure to finish this study with him and receive his email greeting on my
    birthday.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 我衷心感谢我的研究导师和指导教授[马修·劳里埃](https://mlauriere.github.io/)。他为我提供了深刻的建议，并在这个夏季研究过程中给予指导。能与他一起完成这项研究，并在我生日时收到他的邮件问候，我感到非常荣幸和高兴。
- en: This work was also supported in part through the NYU IT High Performance Computing
    resources, services, and staff expertise.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 本工作还部分得到了纽约大学IT高性能计算资源、服务和员工专业知识的支持。
- en: Besides, I genuinely appreciate NYU and NYU Shanghai for offering me the [DURF](https://shanghai.nyu.edu/academics/undergraduate-research)
    research opportunity.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我真诚地感谢纽约大学及纽约大学上海校区提供的[DURF](https://shanghai.nyu.edu/academics/undergraduate-research)研究机会。
- en: Thanks to all my friends and family who helped me throughout this summer. The
    research would not have been possible without any of you.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 感谢所有在这个夏天帮助我的朋友和家人。没有你们的支持，这项研究是不可能完成的。
- en: Reference
  id: totrans-129
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] Liu, B. [Sentiment Analysis: Mining Opinions, Sentiments, and Emotions.](https://doi.org/10.1017/CBO9781139084789)
    (2015), Cambridge University Press, Cambridge.'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] Liu, B. [Sentiment Analysis: Mining Opinions, Sentiments, and Emotions.](https://doi.org/10.1017/CBO9781139084789)
    (2015)，剑桥大学出版社，剑桥。'
- en: '[2] Chakriswaran, P., Vincent, D. R., Srinivasan, K., Sharma, V., Chang, C.-Y.,
    and Reina, D. G. [Emotion AI-Driven Sentiment Analysis: A Survey, Future Research
    Directions, and Open Issues.](https://doi.org/10.3390/app9245462) (2019), Applied
    Sciences.'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] Chakriswaran, P., Vincent, D. R., Srinivasan, K., Sharma, V., Chang, C.-Y.,
    和 Reina, D. G. [Emotion AI-Driven Sentiment Analysis: A Survey, Future Research
    Directions, and Open Issues.](https://doi.org/10.3390/app9245462) (2019)，应用科学。'
- en: '[3] Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez,
    A., Kaiser, u., & Polosukhin, I. [Attention is All you Need.](https://papers.nips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf)
    (2017), In *Advances in Neural Information Processing Systems*. Curran Associates,
    Inc.'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez,
    A., Kaiser, u., & Polosukhin, I. [Attention is All you Need.](https://papers.nips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf)
    (2017), 在 *Advances in Neural Information Processing Systems* 中。Curran Associates,
    Inc.'
- en: '[4] Kralj Novak, P., Smailović, J., Sluban, B., & Mozetič, I. [Sentiment of
    Emojis.](https://doi.org/10.1371/journal.pone.0144296) (2015), *PLOS ONE*, *10*(12),
    e0144296.'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] Kralj Novak, P., Smailović, J., Sluban, B., & Mozetič, I. [Sentiment of
    Emojis.](https://doi.org/10.1371/journal.pone.0144296) (2015)，*PLOS ONE*，*10*(12)，e0144296。'
