- en: A Deep Dive into K-means for the Less Technophile
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 对K-means的深度解析，适合不太懂技术的读者
- en: 原文：[https://towardsdatascience.com/a-deep-dive-into-k-means-for-the-less-technophile-eaea262bd51f?source=collection_archive---------16-----------------------#2023-01-24](https://towardsdatascience.com/a-deep-dive-into-k-means-for-the-less-technophile-eaea262bd51f?source=collection_archive---------16-----------------------#2023-01-24)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/a-deep-dive-into-k-means-for-the-less-technophile-eaea262bd51f?source=collection_archive---------16-----------------------#2023-01-24](https://towardsdatascience.com/a-deep-dive-into-k-means-for-the-less-technophile-eaea262bd51f?source=collection_archive---------16-----------------------#2023-01-24)
- en: 'From clustering to algorithm: a journey in five steps'
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从聚类到算法：五步之旅
- en: '[](https://medium.com/@alexandre.allouin?source=post_page-----eaea262bd51f--------------------------------)[![Alexandre
    Allouin](../Images/b393b888498e3dcfab488220968355cb.png)](https://medium.com/@alexandre.allouin?source=post_page-----eaea262bd51f--------------------------------)[](https://towardsdatascience.com/?source=post_page-----eaea262bd51f--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----eaea262bd51f--------------------------------)
    [Alexandre Allouin](https://medium.com/@alexandre.allouin?source=post_page-----eaea262bd51f--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@alexandre.allouin?source=post_page-----eaea262bd51f--------------------------------)[![Alexandre
    Allouin](../Images/b393b888498e3dcfab488220968355cb.png)](https://medium.com/@alexandre.allouin?source=post_page-----eaea262bd51f--------------------------------)[](https://towardsdatascience.com/?source=post_page-----eaea262bd51f--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----eaea262bd51f--------------------------------)
    [Alexandre Allouin](https://medium.com/@alexandre.allouin?source=post_page-----eaea262bd51f--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fc5380bd2d1fe&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-deep-dive-into-k-means-for-the-less-technophile-eaea262bd51f&user=Alexandre+Allouin&userId=c5380bd2d1fe&source=post_page-c5380bd2d1fe----eaea262bd51f---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----eaea262bd51f--------------------------------)
    ·11 min read·Jan 24, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Feaea262bd51f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-deep-dive-into-k-means-for-the-less-technophile-eaea262bd51f&user=Alexandre+Allouin&userId=c5380bd2d1fe&source=-----eaea262bd51f---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fc5380bd2d1fe&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-deep-dive-into-k-means-for-the-less-technophile-eaea262bd51f&user=Alexandre+Allouin&userId=c5380bd2d1fe&source=post_page-c5380bd2d1fe----eaea262bd51f---------------------post_header-----------)
    发布于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----eaea262bd51f--------------------------------)
    ·11分钟阅读·2023年1月24日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Feaea262bd51f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-deep-dive-into-k-means-for-the-less-technophile-eaea262bd51f&user=Alexandre+Allouin&userId=c5380bd2d1fe&source=-----eaea262bd51f---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Feaea262bd51f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-deep-dive-into-k-means-for-the-less-technophile-eaea262bd51f&source=-----eaea262bd51f---------------------bookmark_footer-----------)![](../Images/c9579d7432eb6e709f28e9fee324f966.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Feaea262bd51f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-deep-dive-into-k-means-for-the-less-technophile-eaea262bd51f&source=-----eaea262bd51f---------------------bookmark_footer-----------)![](../Images/c9579d7432eb6e709f28e9fee324f966.png)'
- en: Image by Pauline Allouin
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来自**Pauline Allouin**
- en: While this article is more technical than my previous ones – one of which was
    [how to successfully translate tech talk for management and users](https://medium.com/towards-data-science/pitching-your-data-strategy-translating-tech-talk-for-management-and-users-bbd044872ee4)
    — it aims to explain a popular algorithm used in data science in simple words.
    Recently, I had to explain what clustering is to a curious, non-technical senior
    manager. He understood the whole concept of clustering and wanted to know more
    about how it was translated into an algorithm.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这篇文章比我之前的文章更为技术化——其中一篇是[如何成功地将技术语言翻译为管理层和用户的语言](https://medium.com/towards-data-science/pitching-your-data-strategy-translating-tech-talk-for-management-and-users-bbd044872ee4)——但它旨在用简单的语言解释数据科学中常用的一种算法。最近，我需要向一位好奇的非技术背景的高级经理解释什么是聚类。他理解了聚类的整个概念，并想进一步了解它如何被转化为算法。
- en: The explanations in this post go way beyond the simple senior manager explanation,
    but I thought I would share that interesting experience; here I attempt to provide
    more details that we can usually find with an online search. Thus, it might provide
    answers to questions people have when implementing the K-means algorithm for the
    very first time. Also, I do not intend to repeat the many resources available
    on the internet but rather focus on its essence. At the end of each paragraph
    I will highlight the point that could be communicated to a less technical audience.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 本文的解释远远超出了简单的高级经理解释，但我认为我会分享这个有趣的经历；在这里，我尝试提供通常可以通过在线搜索找到的更多细节。因此，它可能为人们在首次实施K-means算法时提出的问题提供答案。此外，我不打算重复互联网上的许多资源，而是专注于其本质。在每段的末尾，我将突出可以传达给非技术观众的要点。
- en: The big picture
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 大致概况
- en: Clustering is a technique for dividing a set of data points into groups (or
    clusters) based on their similarities. This method has many applications such
    as behavioural segmentation, inventory categorization, identifying groups in health
    monitoring, grouping images, detecting anomalies, etc. It can be used to explore
    or expand knowledge of a specific domain, especially when the datasets are too
    large to be analysed with traditional tools.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类是一种将数据点集合按相似性分组（或簇）的技术。这种方法有许多应用，如行为分割、库存分类、健康监测中的群体识别、图像分组、异常检测等。它可以用来探索或扩展特定领域的知识，特别是当数据集过大而无法用传统工具分析时。
- en: 'A commonly used example is customer segmentation: based on different factors
    (age, gender, income, etc.), you want to identify key profiles/personas to better
    target your marketing campaigns and improve your service or revenues. This [post
    from Franco](https://francotisocco.medium.com/customer-segmentation-and-hypothesis-testing-b6dfedaaf5cd)
    gives a concrete and complete application of such an analysis.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 一个常见的例子是客户细分：基于不同因素（年龄、性别、收入等），你希望识别关键的档案/人物角色，以更好地定位你的营销活动，提高服务或收入。这篇[Franco的帖子](https://francotisocco.medium.com/customer-segmentation-and-hypothesis-testing-b6dfedaaf5cd)提供了这种分析的具体和完整的应用。
- en: K-means is one of such clustering algorithms (other examples are covered by
    this [post from Indraneel](https://link.medium.com/h3g8xiTzTwb)). It is a type
    of unsupervised machine learning algorithm. In other words, the algorithm discovers
    patterns in data without requiring human intervention.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: K-means就是这样的聚类算法之一（其他示例可以参考这篇[Indraneel的帖子](https://link.medium.com/h3g8xiTzTwb)）。它是一种无监督机器学习算法。换句话说，该算法在没有人工干预的情况下发现数据中的模式。
- en: K-means clustering is an unsupervised learning algorithm that groups similar
    data points into non-overlapping clusters. Although it has become popular with
    the rise of Machine Learning, the algorithm was actually proposed by Stuart Lloyd
    of Bell Labs in 1957.
  id: totrans-15
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: K-means聚类是一种无监督学习算法，它将相似的数据点分组为不重叠的簇。尽管随着机器学习的兴起，它变得非常流行，但该算法实际上是由Bell Labs的Stuart
    Lloyd于1957年提出的。
- en: ''
  id: totrans-16
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Most famous use cases of K-means are customer segmentation, anomaly detection,
    document classification, etc.
  id: totrans-17
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: K-means最著名的应用包括客户细分、异常检测、文档分类等。
- en: K-means algorithm overview
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: K-means算法概述
- en: 'STEP 1: Specify the number (K) of clusters to use.'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第一步：指定要使用的簇数（K）。
- en: 'STEP 2: Randomly initialize (K) centroids.'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第二步：随机初始化（K）个质心。
- en: 'STEP 3: For each data point, calculate its distance with each centroid and
    assign the data point to the closest centroid.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第三步：对于每个数据点，计算它与每个质心的距离，并将数据点分配给离它最近的质心。
- en: 'STEP 4: Compute the new centroid of each cluster (i.e. the mean of each variable).'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第四步：计算每个簇的新质心（即每个变量的均值）。
- en: 'STEP 5: Execute STEP 3 again until the position of the centroids do not change
    (convergence).'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第五步：重复第三步，直到质心的位置不再变化（收敛）。
- en: The centroid of several continuous variables can be seen as the means of those
    variables, i.e. the data point that represents the centre of the cluster. It is
    important to keep in mind that convergence of centroids is not guaranteed and
    this should be considered when implementing such an algorithm.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 几个连续变量的质心可以看作是这些变量的均值，即表示簇中心的数据点。需要注意的是，质心的收敛并不保证，这在实施这种算法时应予以考虑。
- en: 'When you discover the K-means algorithm for the first time, two questions may
    arise:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 当你第一次发现K-means算法时，可能会出现两个问题：
- en: How to choose the number of clusters (STEP 1)?
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何选择簇的数量（第一步）？
- en: Does the random initialization of centroids have an impact on the result (STEP
    2)?
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 质心的随机初始化是否对结果有影响（步骤2）？
- en: These two topics will be covered in the following sections.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个主题将在接下来的部分中讨论。
- en: 'K-means assumes that you know the number (K) of clusters you want to use to
    partition your dataset. The choice of K is therefore left to the person running
    the algorithm. Among the many techniques available, there are two main methods
    to help you choose the right value for K. Both are widely described on the web.
    To start, you might be interested by these two Medium articles:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: K-means假设你知道你想用来划分数据集的簇的数量（K）。因此，K的选择由运行算法的人决定。在众多可用的技术中，有两种主要的方法可以帮助你选择正确的K值。这两种方法在网上有广泛的描述。开始时，你可能对以下两篇Medium文章感兴趣：
- en: '[K-means Clustering: Algorithm, Applications, Evaluation Methods, and Drawbacks](/k-means-clustering-algorithm-applications-evaluation-methods-and-drawbacks-aa03e644b48a)'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[K-means聚类：算法、应用、评估方法及缺点](/k-means-clustering-algorithm-applications-evaluation-methods-and-drawbacks-aa03e644b48a)'
- en: '[How to determine the optimal K for K-Means?](https://medium.com/analytics-vidhya/how-to-determine-the-optimal-k-for-k-means-708505d204eb)'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[如何确定K-Means的最优K？](https://medium.com/analytics-vidhya/how-to-determine-the-optimal-k-for-k-means-708505d204eb)'
- en: Again, I won’t focus too much on the formulas or code but rather on the explanations
    of the concepts.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，我不会过多关注公式或代码，而是专注于概念的解释。
- en: The letter K in K-means represents the number of clusters that will be created
    by the algorithm. This number of clusters is defined by the analyst. K-means allocates
    data points to clusters by measuring distances between them and groups the closest
    together.
  id: totrans-33
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: K-means中的字母K代表算法将创建的簇的数量。这个簇的数量由分析师定义。K-means通过测量数据点之间的距离来分配数据点到簇，并将最近的点分组在一起。
- en: Principle of the Elbow method
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 肘部法的原理
- en: 'This is an empirical method: the main idea is to run the K-means algorithm
    for a possible range of K values (e.g. from 1 to 10 clusters) and evaluate, for
    each scenario (K=1, K=2, …), the distance between the data points and their associated
    centroids (i.e. for each cluster).'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一种经验方法：主要思想是对可能范围内的K值（例如，从1到10个簇）运行K-means算法，并评估每种情况（K=1, K=2, …）中数据点与其关联质心（即每个簇）之间的距离。
- en: For each run of the K-means algorithm, we actually compute the sum of the squared
    distance between each point and its closest centroid, this reminds us of the notion
    of variance. The sum of these quantities is called WCSS, Within-Cluster Sum of
    Square.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每次运行K-means算法，我们实际上计算了每个点与其最近中心的平方距离之和，这让我们想起了方差的概念。这些量的总和称为WCSS，即簇内平方和。
- en: '![](../Images/63c310d6756f7bd2b2e9c3504fddcc9b.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/63c310d6756f7bd2b2e9c3504fddcc9b.png)'
- en: Image by Alexandre Allouin
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来自亚历山大·阿卢安
- en: Intuitively, this quantity decreases when the number of clusters increases.
    Indeed, by increasing the number of centroids, we increase the chance of a data
    point to get closer to a centroid and therefore reduce its distance. The method
    aims to find the point (elbow) where adding more clusters does not bring value.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 直观上，当簇的数量增加时，这个量会减少。确实，通过增加质心的数量，我们增加了数据点靠近质心的机会，从而减少了距离。该方法旨在找到一个点（肘部），在这个点上添加更多簇不会带来额外价值。
- en: The challenge, as described in the many articles, is to determine when the number
    of K clusters is correct because we do not want to end up with too many clusters.
    Consequently, the best choice can be somewhat arbitrary.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 正如许多文章中所述的挑战是确定K簇的数量是否正确，因为我们不想得到太多的簇。因此，最佳选择可能有些任意。
- en: '**Terminology sometimes used:**'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '**有时使用的术语：**'
- en: 'Inertia: sum of squared distances of samples to their closest cluster centre
    (equivalent to WCSS).'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 惯性：样本到其最近簇中心的平方距离之和（等同于WCSS）。
- en: 'Distortion: average of the euclidean squared distance from the centroid of
    the respective clusters.'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 失真：从各自簇的质心到数据点的欧几里得平方距离的平均值。
- en: Silhouette analysis
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 轮廓分析
- en: The silhouette value is a measure of how similar an object is to its own cluster
    (cohesion) compared to other clusters (separation). [-Wikipedia](https://en.wikipedia.org/wiki/Silhouette_(clustering))
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 轮廓值是衡量一个对象与其自身簇（凝聚度）相比与其他簇（分离度）相似程度的指标。[-维基百科](https://en.wikipedia.org/wiki/Silhouette_(clustering))
- en: 'The principle is as follows: if the number of clusters is correct, then the
    average distance between each data point within a cluster (often called “average
    intra-cluster distance”) should be smaller than the average distance with other
    data points from other clusters (“average inter-cluster distance”).'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 原则如下：如果簇的数量正确，则簇内每个数据点之间的平均距离（通常称为“平均簇内距离”）应小于与其他簇的其他数据点之间的平均距离（“平均簇间距离”）。
- en: '![](../Images/039d429fb3a6b4bb069adaa41db86556.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/039d429fb3a6b4bb069adaa41db86556.png)'
- en: Image by Alexandre Allouin
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由 Alexandre Allouin 提供
- en: 'This method assumes that the data have been clustered. In other words, each
    data point is already assigned to a cluster. Let’s focus now on a given data point
    (in light orange on the image above) using an example of three clusters:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 该方法假设数据已经被聚类。换句话说，每个数据点已经被分配到一个簇。现在让我们关注一个给定的数据点（在上图中为浅橙色），以三个簇为例：
- en: We calculate the average distance of this point with all other data points in
    the same cluster. This is often noted as *a(i).*
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们计算该点与同一簇中所有其他数据点的平均距离。这通常记作 *a(i)*。
- en: We calculate the average distance of the same point with all data points from
    other clusters. Let’s start with the blue cluster.
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们计算相同点与其他簇所有数据点的平均距离。我们从蓝色簇开始。
- en: We do the same calculation as step 2 but with the green cluster.
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们对绿色簇进行与步骤 2 相同的计算。
- en: We compare the results obtained in step 2 and step 3 and keep the minimum value
    because we do not really care which cluster is far away, but rather which one
    is near. To put it another way, we are interested in the neighbourhood of our
    considered data point. We denote this value *b(i)*.
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们比较步骤 2 和步骤 3 中获得的结果，并保留最小值，因为我们真正关心的是哪个簇靠近，而不是哪个簇远离。换句话说，我们对我们考虑的数据点的邻域感兴趣。我们将这个值表示为
    *b(i)*。
- en: 'Therefore, the silhouette value *S* for a given data point *i* is obtained
    by subtracting *b(i)* to *a(i)* and divide this quantity by the maximum value
    between *a(i)* and *b(i)*. I tried to stay away from formulas, but sometimes it
    can be useful to see them:'
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 因此，给定数据点 *i* 的轮廓值 *S* 通过从 *a(i)* 中减去 *b(i)* 并将该数量除以 *a(i)* 和 *b(i)* 之间的最大值来获得。我尝试避免公式，但有时查看它们可能很有用：
- en: '![](../Images/16edd12ca497098387f8cff058d4f4f3.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/16edd12ca497098387f8cff058d4f4f3.png)'
- en: As a consequence, this value is always between -1 and 1.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，这个值总是介于 -1 和 1 之间。
- en: '***b(i)*** and ***a(i)*** are always positive (average of distance), so if
    the average intra-cluster distance *a(i)* is greater than the average inter-cluster
    distance *b(i)* then *S(i)* will be negative. The data point under consideration
    is closer to another cluster. In this case , it appears misclassified.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '***b(i)*** 和 ***a(i)*** 始终为正（距离的平均值），因此如果平均簇内距离 *a(i)* 大于平均簇间距离 *b(i)*，则 *S(i)*
    将为负值。所考虑的数据点更接近另一个簇。在这种情况下，它似乎被错误分类了。'
- en: 'If we extend this reasoning, when *b(i)* is very large compared to *a(i)*,
    then *S(i)* will be close to 1: the data point is well classified.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们扩展这种推理，当 *b(i)* 相对于 *a(i)* 非常大时，*S(i)* 将接近 1：数据点被很好地分类了。
- en: When *S(i)* is about zero, the average intra-cluster distance is approximately
    equal to the average inter-cluster distance, the choice of assigning the data
    point to one cluster or another is not clear.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 当 *S(i)* 约为零时，平均簇内距离大约等于平均簇间距离，将数据点分配到一个簇或另一个簇的选择并不明确。
- en: Selection of the best value for K
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: K 的最佳值选择
- en: The average of the *S(i)* for all data points in a given cluster is called the
    average silhouette width of that cluster *S(K)*. For each model (with 2, 3, 4,
    … K clusters) we can then calculate the average of *S(K)* — the cluster average
    silhouette width. This quantity is called the *Average Silhouette Score*. To select
    the best value of K, the score obtained must be as high as possible.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在给定簇中所有数据点的 *S(i)* 的平均值称为该簇的平均轮廓宽度 *S(K)*。对于每个模型（具有 2、3、4、…… K 个簇），我们可以计算 *S(K)*
    的平均值——簇的平均轮廓宽度。这个量称为 *平均轮廓分数*。为了选择 K 的最佳值，获得的分数必须尽可能高。
- en: 'The two methods, Elbow and Silhouette, provide information from a different
    perspective. They are often used in combination to reinforce the choice of K.
    Another ally can sometimes support decision-making: the domain knowledge of the
    person performing the analysis!'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 两种方法，肘部法则和轮廓法，从不同的角度提供信息。它们通常结合使用，以增强 K 的选择。另一个盟友有时也可以支持决策：执行分析的人的领域知识！
- en: While the decision to identify the right number of clusters is ultimately up
    to the analyst, different techniques exist to help the analyst to make informed
    decisions. The most popular are the Elbow method and the Silhouette analysis.
  id: totrans-63
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 虽然确定正确的聚类数量的决定最终取决于分析师，但存在不同的技术可以帮助分析师做出明智的决策。最受欢迎的方法是肘部法则和轮廓分析。
- en: Initializing the centroids
  id: totrans-64
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 初始化质心
- en: 'The second facet of the algorithm that may concern you is the random choice
    of the initial centroids. Indeed, K-means is nondeterministic: the algorithm can
    produce different results depending on its initialization. Below are two different
    runs of the algorithm on the same dataset illustrating this:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个可能引起你关注的算法方面是初始质心的随机选择。实际上，K-means算法是非确定性的：算法可以根据其初始化产生不同的结果。下面是对同一数据集的两次不同算法运行的示例：
- en: '![](../Images/ad14d4fa4a5a04424572a0f9a5eff16c.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ad14d4fa4a5a04424572a0f9a5eff16c.png)'
- en: Image by Alexandre Allouin
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：Alexandre Allouin
- en: The purpose of this example is only to illustrate the fact that K-means can
    produce different results. Indeed, in this particular case, the number of clusters
    may not be appropriate. When K-means was run several times on this dataset but
    with only three clusters, the results were always consistent and identical. Moreover,
    as a best practice, it is recommended to run K-means several times to see the
    stability of the proposed clustering.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 这个示例的目的是仅仅为了说明K-means算法可能产生不同的结果。实际上，在这个特定的情况下，聚类的数量可能不合适。当K-means算法在这个数据集上多次运行，但只设置三个聚类时，结果始终是一致的。此外，作为最佳实践，建议多次运行K-means以查看提出的聚类的稳定性。
- en: 'sklearn.cluster.KMeans allows you to select the way the first centroids are
    initialized. There are two options: random or K-means++. K-means++ is a technique
    of smart centroids initialization that speeds up the convergence of the algorithm.
    The main idea is to select initial centroids as far apart as possible.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: sklearn.cluster.KMeans 允许你选择初始化首个质心的方式。有两个选项：随机或K-means++。K-means++ 是一种智能质心初始化技术，可以加速算法的收敛。其主要思想是选择尽可能远离的初始质心。
- en: The K-means algorithm can lead to different results, this is particularly true
    when the number of clusters is incorrect or when the pattern in the data is not
    clear.
  id: totrans-70
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: K-means算法可能导致不同的结果，这在聚类数量不正确或数据模式不明确时尤其如此。
- en: Things to keep in mind
  id: totrans-71
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 需要记住的事项
- en: Importance of feature scaling
  id: totrans-72
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 特征缩放的重要性
- en: '[Feature scaling](https://en.wikipedia.org/wiki/Feature_scaling) is essential
    for K-means to produce quality results. Because the algorithm relies on distance
    estimation, especially for the calculation of centroids, the different variables
    must be in the same range of magnitude.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '[特征缩放](https://en.wikipedia.org/wiki/Feature_scaling) 对于K-means算法产生高质量结果至关重要。因为算法依赖于距离估计，特别是在计算质心时，不同的变量必须在相同的量级范围内。'
- en: Below is an example of a dataset consisting of two obvious clusters (Chart A).
    It’s obvious because autoscaling lets you see it, but if this dataset is visualized
    on a graph with axes having the same scale (Chart B), this is no longer visible
    for either you or the algorithm.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是一个包含两个明显聚类的数据集示例（图表 A）。之所以明显，是因为自动缩放让你看到了这一点，但如果这个数据集在具有相同刻度的图表上进行可视化（图表
    B），这对你或算法来说就不再可见了。
- en: 'It becomes even more interesting when you run K-means on this non-standardized
    dataset (Chart C and Chart D): two clusters are created containing half of each
    cloud of points. This is clearly the opposite of what you want to achieve!'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 当你在这个未标准化的数据集上运行K-means（图表 C 和图表 D）时，它变得更加有趣：创建了两个包含每个点云一半的聚类。这显然是你想要避免的结果！
- en: Finally, when the dataset is standardized (Chart E and Chart F), the difference
    in scale appears and K-means is able to identify the true clusters.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，当数据集被标准化时（图表 E 和图表 F），尺度差异出现，K-means能够识别出真实的聚类。
- en: '![](../Images/498ddd45d4f9a5289f8d1ff4d618a4f1.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/498ddd45d4f9a5289f8d1ff4d618a4f1.png)'
- en: Image by Alexandre Allouin
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：Alexandre Allouin
- en: 'To be more concrete, suppose you want to perform a customer segmentation based
    on age and annual income. You have a dataset of 500 people between the ages of
    20 and 40 shown in the figure below. Chart A represents this dataset (autoscaling)
    on which we can visually imagine two main clusters. Again, as explained above,
    they are visible because the two dimensions are not on the same scale otherwise,
    you would not be able to visualize it. When K-means is applied on the original
    data, the algorithm horizontally splits the data points in two groups. The reason
    for this is that when calculating distances, a 20-year change will compare the
    same as a change of $20 annual income. The euclidean distance used in K-means
    in STEP 3 of the algorithm between a possible data point A (23, 21000) and an
    hypothetic centroid K1 (25, 22500) will be:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 更具体地说，假设你想根据年龄和年收入进行客户细分。你有一个包含500人的数据集，年龄在20到40岁之间，如下图所示。图表A代表了这个数据集（自动缩放），我们可以直观地想象出两个主要的簇。正如上面解释的那样，这些簇是可见的，因为两个维度的尺度不同，否则你将无法可视化。当K-means应用于原始数据时，算法将数据点水平地分成两组。原因是，在计算距离时，20年的变化会与$20的年收入变化进行比较。K-means算法在步骤3中使用的欧几里得距离在一个可能的数据点A（23,
    21000）和一个假设的质心K1（25, 22500）之间将是：
- en: '![](../Images/54bb12d193a1b4523526dc9b10fd7388.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/54bb12d193a1b4523526dc9b10fd7388.png)'
- en: Thus, age does not weigh much in the calculation of the distance and, therefore,
    will not have a significant impact whether the person is 20 or 40\. Consequently,
    K-means splits the clusters approximately following the mean of the annual income
    only, since that is the only dimension that really matters.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，年龄在计算距离时权重不大，因此，无论一个人是20岁还是40岁，影响都不显著。因此，K-means主要根据年收入的均值来分割簇，因为这是唯一真正重要的维度。
- en: However, you want your algorithm to detect this age variation because it is
    an important factor in your study. Chart C and Chart D provide better clusters
    when the data is standardized before running K-means.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，你希望你的算法能够检测到这个年龄变异，因为它在你的研究中是一个重要因素。图表C和图表D在数据标准化后运行K-means时提供了更好的簇。
- en: '![](../Images/7378911407cf1097d1e20b19522e858e.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7378911407cf1097d1e20b19522e858e.png)'
- en: Image by Alexandre Allouin
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：Alexandre Allouin
- en: A final comment on this example is that you can see that some data points could
    be better categorized. This is due to the standardization effect but also because
    they may be outliers, which K-means is not good at dealing with.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 对这个例子的最后评论是，你可以看到一些数据点的分类可以更好一些。这是由于标准化效应，但也因为它们可能是离群点，而K-means在处理离群点时表现不佳。
- en: Sensitivity to outliers
  id: totrans-86
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 对离群点的敏感性
- en: Since the algorithm minimizes distances between data points and centroids, outliers
    affect the position of the centroids and can shift cluster boundaries. However,
    dealing with outliers before running K-means can be challenging, especially on
    big datasets. Many attempts have been made to address the challenge of the outliers
    with K-means, but there does not seem to be a real consensus on an effective solution.
    Most of the time, using other algorithms K-medians, K-medoids or DBSCAN are preferred.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 由于算法最小化数据点和质心之间的距离，离群点会影响质心的位置，并可能改变簇的边界。然而，在运行K-means之前处理离群点可能具有挑战性，尤其是在大数据集上。虽然对K-means处理离群点的挑战做出了许多尝试，但似乎没有达成有效解决方案的真正共识。大多数时候，使用其他算法，如K-medians、K-medoids或DBSCAN更为优先。
- en: Categorical data
  id: totrans-88
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 类别数据
- en: 'For the same reason as above, the fact that K-means minimizes the distance
    between data points and their nearest centroid, K-means is not relevant for categorical
    data. The distance between two categories, even though you encoded them as numbers,
    does not mean anything: there is no natural order between values. Other algorithms
    should be preferred.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 出于同样的原因，K-means最小化数据点与其最近质心之间的距离，这使得K-means对类别数据不相关。即使你将类别编码为数字，两个类别之间的距离也没有意义：值之间没有自然顺序。应该优先使用其他算法。
- en: Data pattern
  id: totrans-90
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据模式
- en: 'The heart of the algorithm is based on the notion of centroid, so by design
    K-means always tries to find spherical clusters. The dataset containing a particular
    pattern might be difficult to capture for this algorithm. Algorithms such as DBSCAN
    are more suitable for detecting clusters of special data shapes. An illustration
    can be found here: [DBSCAN clustering for data shapes k-means can’t handle well](/dbscan-clustering-for-data-shapes-k-means-cant-handle-well-in-python-6be89af4e6ea).'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 该算法的核心基于质心的概念，因此 K-means 在设计上总是尝试寻找球形簇。包含特定模式的数据集可能难以被该算法捕捉。像 DBSCAN 这样的算法更适合检测特殊数据形状的簇。一个示例可以在这里找到：[DBSCAN
    聚类处理 K-means 无法处理的数据形状](/dbscan-clustering-for-data-shapes-k-means-cant-handle-well-in-python-6be89af4e6ea)。
- en: 'Since K-means is all about distance measurement, the algorithm is sensitive
    to outliers: they shift the centres of the clusters and thus the allocation of
    the data points.'
  id: totrans-92
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 由于 K-means 完全依赖于距离测量，该算法对异常值非常敏感：它们会改变簇的中心，从而影响数据点的分配。
- en: Should managers be involved in technical considerations?
  id: totrans-93
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 管理者是否应该参与技术方面的考虑？
- en: Recently, in a workshop organized by [CERN IdeaSquare](https://www.linkedin.com/feed/update/urn:li:activity:7022250242473381888/)
    and [Geneva Innovation Movement Association](https://www.linkedin.com/posts/geneva-innovation-movement-association_geneva-activity-7021896676013428736-libf),
    we explored ways to reconcile the communication between high-level managers and
    technical experts while developing a product.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，在由[CERN IdeaSquare](https://www.linkedin.com/feed/update/urn:li:activity:7022250242473381888/)和[日内瓦创新运动协会](https://www.linkedin.com/posts/geneva-innovation-movement-association_geneva-activity-7021896676013428736-libf)组织的工作坊中，我们探索了在开发产品时如何调和高级经理与技术专家之间的沟通。
- en: How can we optimize this collaboration to deliver faster and better solutions?
    While the event was fun (which can also be a parameter to consider), getting both
    parties around the same table to work on a common goal seemed to help a lot. In
    the specific case of data science, developing a data culture among the various
    stakeholders is certainly the way to go.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何优化这种协作，以便更快、更好地交付解决方案？虽然活动很有趣（这也可以作为一个参数来考虑），但让双方坐到同一张桌子上共同工作似乎帮助很大。在数据科学的具体案例中，培养各利益相关者之间的数据文化无疑是正确的方向。
- en: As mentioned earlier, I tried this with interesting feedback from a senior manager.
    Are you game? Why not spend 10 minutes in a meeting with high-level managers to
    demonstrate the selected model and how it behaves when playing with certain parameters?
    A script might be too crude, but with low-code/no-code software (such as Knime)
    it could trigger unexpected but interesting results.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，我尝试过这种方法，并得到了来自一位高级经理的有趣反馈。你有兴趣吗？为什么不花 10 分钟与高级经理们开会，展示所选择的模型以及在调整某些参数时的表现呢？脚本可能过于粗糙，但使用低代码/无代码软件（如
    Knime）可能会引发意想不到但有趣的结果。
- en: Acknowledgements
  id: totrans-97
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 致谢
- en: Sincere thanks to [Sharon](https://www.linkedin.com/in/sharonroffey/) for her
    proofreading, suggestions and continuous support.
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 真诚感谢[Sharon](https://www.linkedin.com/in/sharonroffey/)的校对、建议和持续支持。
- en: Many thanks to [Mathilde](https://www.linkedin.com/in/mathilde-allouin-bb8203263/)
    for her helpful comments and to Pauline for her artistic contribution!
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 特别感谢[Mathilde](https://www.linkedin.com/in/mathilde-allouin-bb8203263/)的有益评论和
    Pauline 的艺术贡献！
