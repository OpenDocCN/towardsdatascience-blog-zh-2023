- en: 'Intro to PyTorch 2: Convolutional Neural Networks'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: PyTorch 2：卷积神经网络
- en: 原文：[https://towardsdatascience.com/intro-to-pytorch-2-convolutional-neural-networks-487d8a35139a?source=collection_archive---------1-----------------------#2023-02-13](https://towardsdatascience.com/intro-to-pytorch-2-convolutional-neural-networks-487d8a35139a?source=collection_archive---------1-----------------------#2023-02-13)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/intro-to-pytorch-2-convolutional-neural-networks-487d8a35139a?source=collection_archive---------1-----------------------#2023-02-13](https://towardsdatascience.com/intro-to-pytorch-2-convolutional-neural-networks-487d8a35139a?source=collection_archive---------1-----------------------#2023-02-13)
- en: '[](https://medium.com/@florestony5454?source=post_page-----487d8a35139a--------------------------------)[![Tony
    Flores](../Images/9eacdf91506321ae7ed7b19006a58053.png)](https://medium.com/@florestony5454?source=post_page-----487d8a35139a--------------------------------)[](https://towardsdatascience.com/?source=post_page-----487d8a35139a--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----487d8a35139a--------------------------------)
    [Tony Flores](https://medium.com/@florestony5454?source=post_page-----487d8a35139a--------------------------------)'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@florestony5454?source=post_page-----487d8a35139a--------------------------------)[![托尼·弗洛雷斯](../Images/9eacdf91506321ae7ed7b19006a58053.png)](https://medium.com/@florestony5454?source=post_page-----487d8a35139a--------------------------------)[](https://towardsdatascience.com/?source=post_page-----487d8a35139a--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----487d8a35139a--------------------------------)
    [托尼·弗洛雷斯](https://medium.com/@florestony5454?source=post_page-----487d8a35139a--------------------------------)'
- en: ·
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fce0f79ea6056&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintro-to-pytorch-2-convolutional-neural-networks-487d8a35139a&user=Tony+Flores&userId=ce0f79ea6056&source=post_page-ce0f79ea6056----487d8a35139a---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----487d8a35139a--------------------------------)
    ·16 min read·Feb 13, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F487d8a35139a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintro-to-pytorch-2-convolutional-neural-networks-487d8a35139a&user=Tony+Flores&userId=ce0f79ea6056&source=-----487d8a35139a---------------------clap_footer-----------)'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fce0f79ea6056&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintro-to-pytorch-2-convolutional-neural-networks-487d8a35139a&user=Tony+Flores&userId=ce0f79ea6056&source=post_page-ce0f79ea6056----487d8a35139a---------------------post_header-----------)
    发表在[Towards Data Science](https://towardsdatascience.com/?source=post_page-----487d8a35139a--------------------------------)
    ·16分钟阅读·2023年2月13日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F487d8a35139a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintro-to-pytorch-2-convolutional-neural-networks-487d8a35139a&user=Tony+Flores&userId=ce0f79ea6056&source=-----487d8a35139a---------------------clap_footer-----------)'
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F487d8a35139a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintro-to-pytorch-2-convolutional-neural-networks-487d8a35139a&source=-----487d8a35139a---------------------bookmark_footer-----------)![](../Images/6d7fc147708d94a3fd991370a06255a3.png)'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F487d8a35139a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintro-to-pytorch-2-convolutional-neural-networks-487d8a35139a&source=-----487d8a35139a---------------------bookmark_footer-----------)![](../Images/6d7fc147708d94a3fd991370a06255a3.png)'
- en: Image adapted from Adobe Stock
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源于Adobe Stock
- en: '**Intro**'
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**简介**'
- en: 'In the [previous iteration of this series](https://medium.com/towards-data-science/intro-to-pytorch-part-1-663574fb9675),
    we worked with the [CIFAR-10 dataset](https://www.cs.toronto.edu/~kriz/cifar.html)
    and introduced the basics of PyTorch:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在[本系列的前一部分](https://medium.com/towards-data-science/intro-to-pytorch-part-1-663574fb9675)，我们使用了[CIFAR-10数据集](https://www.cs.toronto.edu/~kriz/cifar.html)并介绍了PyTorch的基础知识：
- en: The Tensor and some associated operations
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 张量及其相关操作
- en: Datasets and the DataLoader
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据集和DataLoader
- en: Building a basic neural network
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建基本神经网络
- en: Basic model training and evaluation
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基本模型训练和评估
- en: The model we developed for classifying images in the CIFAR-10 dataset was only
    able to achieve a 53% accuracy on the validation set, and really struggled to
    correctly classify images of some classes, like birds and cats (~33–35%). This
    was expected, since we would normally use Convolutional Neural Networks for image
    classification. In this part of the tutorial series, we will focus on CNN’s and
    improving the performance of image classification on CIFAR-10.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 我们为CIFAR-10数据集开发的图像分类模型只能在验证集上达到53%的准确率，并且在正确分类一些类别的图像（如鸟类和猫类，大约33–35%）时表现非常挣扎。这是预期中的，因为我们通常会使用卷积神经网络进行图像分类。在本教程系列的这一部分，我们将专注于CNN及其在CIFAR-10上提高图像分类性能。
- en: '**CNN Basics**'
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**CNN基础**'
- en: Before we dive into the code, let’s discuss the basics of convolutional neural
    networks so we can have a better understanding of what our code is doing. If you’re
    comfortable with how CNN’s work, feel free to skip this section.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们深入代码之前，让我们讨论一下卷积神经网络的基础，以便更好地理解我们的代码在做什么。如果你对CNN的工作原理很熟悉，可以跳过这一部分。
- en: In comparison to feed-forward networks, like the one we developed in the previous
    part of the series, CNN’s have different architecture, and are composed of different
    types of layers. In the figure below, we can see the general architecture of a
    typical CNN, including the different types of layers it can contain.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 与前馈网络（如我们在系列前一部分中开发的那种）相比，CNN具有不同的架构，并由不同类型的层组成。在下图中，我们可以看到典型CNN的一般架构，包括它可以包含的不同层类型。
- en: '![](../Images/3a48cb9a2be2b96e532e8f01602b4643.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3a48cb9a2be2b96e532e8f01602b4643.png)'
- en: 'Image source: Author'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：作者
- en: 'The three types of layers usually present in a Convolutional Network are:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积网络中通常存在的三种层是：
- en: Convolutional Layers (red dashed outline)
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 卷积层（红色虚线轮廓）
- en: Pooling Layers (blue dashed outline)
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 池化层（蓝色虚线轮廓）
- en: Fully Connected Layers (Red and Purple solid outlines)
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 全连接层（红色和紫色实线轮廓）
- en: '**Convolutional Layer**'
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**卷积层**'
- en: 'The defining component, and first layer of a CNN is the convolutional layer,
    and it consists of the following:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: CNN的定义组件和第一层是卷积层，它由以下内容组成。
- en: Input data (in this case, in image)
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输入数据（在本例中为图像）
- en: Filters
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 过滤器
- en: Feature Maps
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 特征图
- en: What really differentiates a convolutional layer from a densely connected layer
    is the convolution operation. We wont get into the deep specifics on the definition
    of convolution, but if you are really interested and want to get into the meat
    of it, [this article](https://betterexplained.com/articles/intuitive-convolution/#Part_3_Mathematical_Properties_of_Convolution)
    does an excellent job of explaining the mathematical definition, as well as giving
    some really fine concrete examples. I highly recommend it if you’re interested!
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积层与密集连接层的真正区别在于卷积操作。我们不会深入探讨卷积的定义，但如果你真的感兴趣并想深入了解，[这篇文章](https://betterexplained.com/articles/intuitive-convolution/#Part_3_Mathematical_Properties_of_Convolution)很好地解释了数学定义，并提供了一些非常具体的例子。如果你感兴趣，我强烈推荐它！
- en: So why is convolution better than a densely/fully connected layer for image
    data? In essence, dense layers will learn global patterns in their inputs, while
    convolutional layers have the advantage of learning local and spatial patterns.
    That may sound kind of vague or abstract, so let’s check out an example of what
    this means.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，为什么卷积层比密集/完全连接层在图像数据中更好呢？本质上，密集层会学习输入中的全局模式，而卷积层具有学习局部和空间模式的优势。这听起来可能有点模糊或抽象，因此让我们看一个示例来说明这意味着什么。
- en: '![](../Images/d7dc0c57603023b4e03de913c56e36fc.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d7dc0c57603023b4e03de913c56e36fc.png)'
- en: 'Image Source: Author'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：作者
- en: On the left of the image we can see how a basic 2-D, black and white image of
    a 4 would be represented in a convolutional layer. The red square would be the
    filter/feature detector/kernel, convolving over the image. On the right is how
    the same image would be input into in a densely connected layer. You can see the
    same 9 image pixels that were framed by the kernel in red. Notice how on the left,
    pixels are grouped spatially, adjacent to other neighboring pixels. On the right,
    however, those same 9 pixels are no longer neighbors.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在图像的左侧，我们可以看到一个基本的二维黑白图像4在卷积层中的表示方式。红色方框是过滤器/特征检测器/卷积核，正在对图像进行卷积。在右侧是相同图像输入到密集连接层中的方式。你可以看到与红色卷积核框架内相同的9个图像像素。注意左侧像素如何在空间上被分组，邻近其他像素。然而在右侧，这9个像素不再是邻居。
- en: With this, we can see how the spatial/location-based information is lost when
    an image is flattened and represented in a fully-connected/linear layer. This
    is why convolutional neural networks are more powerful at working with image data.
    The spatial structure of the input data is maintained, and patterns (edges, textures,
    shapes, etc.) in the image can be learned.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这一点，我们可以看到当图像被展平并在全连接/线性层中表示时，空间/位置相关的信息是如何丢失的。这就是为什么卷积神经网络在处理图像数据时更强大的原因。输入数据的空间结构得以保持，图像中的模式（边缘、纹理、形状等）可以被学习。
- en: This is essentially the **why** for using CNN’s on images, but now let’s discuss
    the **how**. Let’s have a look at the structure of our input data, these things
    we keep talking about called ‘filters’, and what convolution looks like when we
    put it all together.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 这本质上是使用 CNN 处理图像的**原因**，但现在让我们讨论一下**方法**。我们来看一下输入数据的结构、我们一直讨论的‘滤波器’，以及将所有这些结合在一起时卷积的样子。
- en: '**Input Data**'
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**输入数据**'
- en: The CIFAR-10 dataset contains 60,000 32x32 color images, and each image is represented
    as a 3-D tensor. Each image will be a `(32,32,3)` tensor, where the dimensions
    are 32 (height) x 32 (weight) x 3 (R-G-B color channels). The figure below illustrates
    the 3 different color channels (RGB) separated out from the fully color image
    of a plane in the dataset.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: CIFAR-10 数据集包含 60,000 张 32x32 彩色图像，每张图像被表示为一个三维张量。每张图像将是一个 `(32,32,3)` 张量，其中维度为
    32（高度）x 32（宽度）x 3（R-G-B 颜色通道）。下图展示了数据集中飞机的全彩图像中分离出的三种不同颜色通道（RGB）。
- en: '![](../Images/edeed5a366c6bebb10ba8a10645daf24.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/edeed5a366c6bebb10ba8a10645daf24.png)'
- en: 'Image Source: Author'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 图像来源：作者
- en: Images are usually thought of as 2-dimensional, so it can be easy to forget
    that since they have 3 color channels, they will actually be represented in 3
    dimensions!
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 图像通常被认为是二维的，因此可能容易忽略因为它们有三个颜色通道，它们实际上是以三维表示的！
- en: '**Filters**'
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**滤波器**'
- en: The filter (also referred to as a kernel or feature detector) in a convolutional
    layer is an array of weights that essentially scans over the image in a sliding-window
    fashion, computing the dot product at each stop, and outputs this dot product
    into a new array called a feature map. The sliding-window scanning is called convolution.
    Let’s have a look at an illustration of this process to help make sense of what’s
    going on.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积层中的滤波器（也称为内核或特征检测器）是一个权重数组，实质上以滑动窗口的方式扫描图像，在每个停靠点计算点积，并将该点积输出到一个新的数组中，称为特征图。滑动窗口扫描称为卷积。让我们看看这个过程的插图，以帮助理解发生了什么。
- en: '![](../Images/b9c7c0564b5eab99ea816f70199a7a70.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b9c7c0564b5eab99ea816f70199a7a70.png)'
- en: '*Illustration of a 3x3 filter (blue) convolving over an input (red) to create
    a feature map (purple). Image Source: Author*'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '*3x3 滤波器（蓝色）在输入（红色）上卷积以创建特征图（紫色）的插图。图像来源：作者*'
- en: '![](../Images/6aeac259fc8695fcc08af484d6907974.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6aeac259fc8695fcc08af484d6907974.png)'
- en: '*Illustration of the dot product computation at every step of the convolution.
    Image Source: Author*'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '*每一步卷积中的点积计算插图。图像来源：作者*'
- en: It’s important to note that the weights of the filter remain the same through
    each step. Just like the weights in a fully connected layer, these values are
    learned during training, and adjusted after each training iteration through backpropagation.
    The illustrations don’t tell the whole picture though. When training a CNN, your
    model won’t just have 1 filter at a convolutional layer. It’s pretty common to
    have 32 or 64 filters in a single convolutional layer, and in fact, we will have
    up to 96 filters in a layer in the model we develop in this tutorial.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的是，滤波器的权重在每一步中保持不变。就像全连接层中的权重一样，这些值在训练过程中被学习，并通过反向传播在每次训练迭代后进行调整。不过，插图并没有展示全部情况。在训练
    CNN 时，你的模型在卷积层中不会只有一个滤波器。通常情况下，一个卷积层中有 32 或 64 个滤波器，事实上，在我们本教程中开发的模型中，一个层中将有多达
    96 个滤波器。
- en: 'Finally, though the weights of the filters are the main parameters that are
    trained, there are also hyper-parameters that can be tuned for CNNs:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，尽管滤波器的权重是训练的主要参数，还有可以调整的超参数：
- en: number of filters in a layer
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 层中的滤波器数量
- en: dimensions of filters
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 滤波器的维度
- en: stride (number of pixels a filter moves each step)
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 步幅（滤波器每一步移动的像素数）
- en: padding (how the filter handles boundaries of images)
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 填充（滤波器如何处理图像的边界）
- en: We won’t get into the details of these hyperparameters, since this isn’t intended
    to be a comprehensive CNN walk-through, but these are important factors to be
    aware of.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不会深入讨论这些超参数，因为这并不是一个全面的 CNN 讲解，但这些是需要了解的重要因素。
- en: '**Pooling Layer**'
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**池化层**'
- en: Pooling layers are similar to convolutional layers, in that a filter convolves
    over the input data (usually a feature map that was output from a convolutional
    layer). However, rather than feature detection, the function of pooling layers
    is dimensionality reduction or downsampling. The two most common types of pooling
    used are Max Pooling and Average Pooling. With Max Pooling, the filter slides
    across the input, and at each step will select the pixel with the largest value
    as the output. In Average Pooling, the filter will output the average value of
    the pixels that the filter is passing over.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 池化层与卷积层类似，都是通过一个过滤器对输入数据（通常是卷积层输出的特征图）进行卷积。然而，与特征检测不同，池化层的功能是降维或下采样。最常用的两种池化类型是最大池化和平均池化。最大池化中，过滤器在输入上滑动，每一步都会选择具有最大值的像素作为输出。在平均池化中，过滤器会输出过滤器所经过的像素的平均值。
- en: '**Fully Connected Layer**'
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**全连接层**'
- en: Finally, CNNs typically will have fully connected layers after convolutional
    and pooling layers, and these layers will perform the classification in image
    classification tasks such as the one in this tutorial.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，CNN 通常在卷积层和池化层之后会有全连接层，这些层将执行图像分类任务中的分类，例如本教程中的任务。
- en: Now that we’ve gotten to see how Convolutional Neural Nets are structured and
    how they operate, let’s get to the fun part and train our own CNN in PyTorch!
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了卷积神经网络的结构和操作方式，让我们进入有趣的部分，在 PyTorch 中训练我们自己的 CNN！
- en: '**Setup**'
  id: totrans-59
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**设置**'
- en: As with the first part of this tutorial, I recommend using Google Colab to follow
    along since you will have your Python environment set up already with PyTorch
    and other libraries installed, as well as a GPU to train your model.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 与本教程的第一部分一样，我建议使用 Google Colab 来跟随，因为你将已经设置好 Python 环境，并安装了 PyTorch 和其他库，以及一个用于训练模型的
    GPU。
- en: So, if you are using Colab, to make sure you are utilizing a GPU go to `Runtime`
    and click `Change runtime type`.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，如果你正在使用 Colab，为了确保你正在使用 GPU，请前往 `Runtime` 并点击 `Change runtime type`。
- en: '![](../Images/14d135ca33cd5d0d2ab3dc0ab90ab770.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/14d135ca33cd5d0d2ab3dc0ab90ab770.png)'
- en: 'Image Source: Author'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：作者
- en: In the dialog select `GPU` and save.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在对话框中选择 `GPU` 并保存。
- en: '![](../Images/a20a5461174b46d89088e1d8233f4db8.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a20a5461174b46d89088e1d8233f4db8.png)'
- en: 'Image Source: Author'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：作者
- en: 'Now you have GPU access in Colab, and we can verify your device with PyTorch.
    So first, let’s get our imports taken care of:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你在 Colab 中有 GPU 访问权限，我们可以用 PyTorch 验证你的设备。所以首先，让我们处理一下导入：
- en: 'If you want to check what GPU you have access to, type and execute `torch.cuda.get_device_name(0)`
    and you should see your device output. Colab has a few different GPU options available,
    so your output will vary depending on what you are given access to, but as long
    as you dont get `RuntimeError: No CUDA GPUs are available` when you run this code,
    you are using a GPU!'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '如果你想检查你可以访问的 GPU，输入并执行 `torch.cuda.get_device_name(0)`，你应该能看到你的设备输出。Colab 提供了几种不同的
    GPU 选项，因此你的输出会因获得的访问权限而有所不同，但只要运行此代码时不出现 `RuntimeError: No CUDA GPUs are available`，你就正在使用
    GPU！'
- en: We can set our GPU as `device` so as we develop our model, we can assign it
    to the GPU by referencing `device`, as well as use CPU if we don’t have a CUDA
    GPU device available.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将 GPU 设置为 `device`，这样在开发模型时，可以通过引用 `device` 将模型分配到 GPU 上，如果没有 CUDA GPU 设备，也可以使用
    CPU。
- en: Next, let’s set a random seed so that our results are reproducible as well as
    download our training data and set a `transform` to convert images to Tensors
    and Normalize the data.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们设置一个随机种子，以确保结果是可重复的，并下载我们的训练数据，同时设置 `transform` 以将图像转换为张量并归一化数据。
- en: 'Once that has finished downloading, let’s check out the classes in the dataset:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 下载完成后，让我们查看数据集中包含的类别：
- en: 'Finally, let’s setup our train and test dataloaders:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们设置训练和测试数据加载器：
- en: Now we’re ready to build our model!
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们准备好构建我们的模型了！
- en: '**Building the CNN**'
  id: totrans-74
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**构建 CNN**'
- en: 'In PyTorch, `nn.Conv2d` is the convolutional layer that is used on image input
    data. The first argument for `Conv2d` is the number of channels in the input,
    so for our first convolutional layer, we will use 3 since a color image will have
    3 color channels. After the first convolutional layer, this argument will depend
    on the number of channels output from the previous layer. The second argument
    is the number of channels that are output from the convolution operation in the
    layer. These channels are the feature maps that were discussed in the intro to
    the convolutional layer. Finally, the third argument will be the size of the kernel
    or filter. This can be an integer value like `3` for a `3x3` kernel, or a tuple
    such as `(3,3)`. So our convolutional layers will take the form of `nn.Conv2d(in_channels,
    out_channels, kernel_size)`. Additional optional parameters can be added, including
    (but not limited to): `stride`, `padding`, and `dilation`. We will use `stride=2`
    in our convolutional layer `conv4`.'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在PyTorch中，`nn.Conv2d`是用于图像输入数据的卷积层。`Conv2d`的第一个参数是输入的通道数，因此对于我们的第一个卷积层，我们将使用3，因为彩色图像将有3个颜色通道。在第一个卷积层之后，这个参数将取决于前一层输出的通道数。第二个参数是层中卷积操作输出的通道数。这些通道就是在卷积层介绍中讨论的特征图。最后，第三个参数是卷积核或过滤器的大小。这可以是像`3`这样的整数值表示`3x3`的卷积核，或者是像`(3,3)`这样的元组。因此，我们的卷积层将采用`nn.Conv2d(in_channels,
    out_channels, kernel_size)`的形式。可以添加其他可选参数，包括（但不限于）：`stride`、`padding`和`dilation`。我们将在卷积层`conv4`中使用`stride=2`。
- en: After our series of convolutional layers, we will want to use a flattening layer
    to flatten our feature maps to be able to feed into linear layers, and for that
    we will use `nn.Flatten()`. We can apply batch normalization with `nn.BatchNorm1d()`
    and will need to pass the number of features as an argument. Finally, our linear,
    fully-connected layers are built using `nn.Linear()`, which will also take the
    number of features as the first argument, as well as specifying the number of
    output features as the second argument.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在经过一系列卷积层之后，我们需要使用一个展平层将特征图展平成可以输入到线性层的数据，为此我们将使用`nn.Flatten()`。我们可以使用`nn.BatchNorm1d()`来应用批量归一化，并且需要传入特征数量作为参数。最后，我们的线性全连接层使用`nn.Linear()`构建，它也会将特征数量作为第一个参数，并且指定输出特征的数量作为第二个参数。
- en: So to begin defining the base architecture of our model, we will define a `ConvNet`
    class that inherits from the PyTorch `nn.Module` class. We can then define each
    of our layers as attributes for our class, and build them as we see fit. Once
    we’ve specified the layer architecture, we can define the flow of the model by
    creating a `forward()` method. We can wrap each layer with an activation function,
    and in our case we will be using `relu`. We can apply `dropout` between layers
    by passing the previous layer and `p` the probability of an element being dropped
    out (which defaults to 0.5). Finally, we create our model object and attach it
    to our `device` so that it can train on the GPU.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 为了开始定义我们模型的基本架构，我们将定义一个继承自PyTorch `nn.Module`类的`ConvNet`类。然后，我们可以将每一层定义为类的属性，并根据需要构建它们。一旦我们指定了层的架构，就可以通过创建一个`forward()`方法来定义模型的流动。我们可以用激活函数包裹每一层，在我们的案例中我们将使用`relu`。我们可以在层之间应用`dropout`，通过传入前一层和`p`的元素丢弃概率（默认为0.5）。最后，我们创建我们的模型对象并将其附加到我们的`device`上，以便在GPU上进行训练。
- en: '**Train and Test Functions**'
  id: totrans-78
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**训练和测试函数**'
- en: If you went through the first part of this tutorial, our train and test functions
    will be identical to what we created then, except that we will be returning the
    `loss` in our train method, and `loss` and number of `correct` in our test method
    to utilize when we are tuning hyperparameters.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你完成了本教程的第一部分，我们的训练和测试函数将与之前创建的完全相同，只是我们将在训练方法中返回`loss`，并在测试方法中返回`loss`和`correct`的数量，以便在调整超参数时使用。
- en: Finally, we define the loss function and optimizer before the base model training.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们在基础模型训练之前定义损失函数和优化器。
- en: Let’s train the model.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们训练模型。
- en: '![](../Images/f83abc3b8a4c65aa307718fb1bd0d321.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f83abc3b8a4c65aa307718fb1bd0d321.png)'
- en: 'Image source: Author'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：作者
- en: After only 10 epochs, 61.7% is much better performance than fully connected
    model we trained! It’s pretty clear that a CNN is much better suited for classifying
    images, but we can squeeze out even more performance by extending the training
    duration and tuning hyperparameters. Before we get to that, let’s take a quick
    peek under the hood and check out what the filters look like. Recall that the
    pixels of the filters are the trainable parameters in our model. This isn’t a
    necessary step for training a model for image classification, nor will we find
    much useful information, but it’s pretty neat to see what’s going on inside our
    model.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 仅仅经过10个周期，61.7%的表现远远好于我们训练的全连接模型！显然，CNN更适合图像分类，但通过延长训练时间和调整超参数，我们可以挤出更多的性能。在我们进行这些操作之前，让我们快速查看一下内部情况，看看滤波器是什么样的。回忆一下，滤波器的像素是我们模型中的可训练参数。这不是训练图像分类模型的必要步骤，我们也不会发现太多有用的信息，但看到模型内部发生了什么还是很有趣的。
- en: '**Visualizing Filters**'
  id: totrans-85
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**可视化滤波器**'
- en: We can write a function to plot the filters from a specified layer in the model.
    All we have to do is specify which layer we want to see and pass that into our
    function.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以编写一个函数来绘制模型中特定层的滤波器。我们只需指定要查看的层，并将其传递到我们的函数中。
- en: Let’s check out what the filters in the first convolutional layer (`conv1`)
    look like since these are applied directly to the images.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们查看一下第一卷积层（`conv1`）中的滤波器，因为这些滤波器直接应用于图像。
- en: Below is the output, containing the visualization of the 48 filters from our
    `conv1` convolutional layer. We can see that each filter is a 3x3 tensor of different
    values or colors.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是输出，包含了来自我们的`conv1`卷积层的48个滤波器的可视化。我们可以看到每个滤波器是一个包含不同值或颜色的3x3张量。
- en: '![](../Images/800417b0ffb0ab653c78bfaa9f771b74.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/800417b0ffb0ab653c78bfaa9f771b74.png)'
- en: 'Image Source: Author'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：作者
- en: 'If our filters were 5x5 instead, we would see this difference in the plot.
    Recall that with `nn.Conv2d` we can change the size of the filter with the third
    argument, so if we wanted a 5x5, `conv1` would look like this:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们的滤波器是5x5的，我们会在图中看到这种差异。回忆一下，使用`nn.Conv2d`我们可以通过第三个参数来改变滤波器的大小，所以如果我们想要一个5x5的，`conv1`将会是这样的：
- en: 'If we re-trained the model with the new 5x5 filters the output would now look
    like this:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们使用新的5x5滤波器重新训练模型，输出将会变成这样：
- en: '![](../Images/a44f00a162d9680f72a989f19dec70e5.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a44f00a162d9680f72a989f19dec70e5.png)'
- en: 'Image Source: Author'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：作者
- en: Like I mentioned before, not too much useful information, but interesting to
    see nonetheless.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 如我之前提到的，这里没有太多有用的信息，但 nonetheless 仍然很有趣。
- en: '**Hyperparameter Optimization**'
  id: totrans-96
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**超参数优化**'
- en: For this tutorial, the hyperparameters that we’ll be tuning are the number of
    filters in our convolutional layers, and the number of neurons in our linear layer.
    Right now these values are hard-coded into our model, so to make them tunable
    we will need to make our model configurable. We can use parameters (`c1`, `c2`,
    and `l1`) in our models `__init__` method, and create the model’s layers with
    these values, which will be passed dynamically during the tuning process.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 对于本教程，我们将调整的超参数是卷积层中的滤波器数量和线性层中的神经元数量。目前这些值被硬编码在我们的模型中，因此为了使其可调，我们需要使模型可配置。我们可以在模型的`__init__`方法中使用参数（`c1`、`c2`和`l1`），并用这些值创建模型的层，这些值将在调整过程中动态传递。
- en: We certainly aren’t limited to tuning only these hyperparameters. In fact, learning
    rate and batch size are commonly included in the list of hyperparameters to tune,
    but since we will be using a grid search, we’ll have to greatly reduce the number
    of tunable variables to keep the training time reasonable.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 我们肯定不只限于调整这些超参数。实际上，学习率和批量大小通常也在需要调整的超参数列表中，但由于我们将使用网格搜索，我们必须大幅减少可调变量的数量，以保持训练时间合理。
- en: Next let’s define a dictionary for our search space, as well as one to save
    the parameters that give us the best results. Since we’re using grid search for
    our optimization, every combination of each hyperparameter listed will be used.
    You can just as easily add more values to the lists for each hyperparameter, but
    each additional value will greatly increase the runtime, so it’s recommended to
    start with the following values to save time.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们定义一个用于搜索空间的字典，以及一个保存给我们最佳结果的参数的字典。由于我们使用网格搜索进行优化，列表中列出的每种超参数的每种组合都会被使用。你可以很容易地为每个超参数的列表添加更多值，但每增加一个值都会大幅增加运行时间，因此建议从以下值开始以节省时间。
- en: '**Early Stopping**'
  id: totrans-100
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**早停法**'
- en: One component that will be important in our optimization process is the usage
    of early stopping. Since we’ll have multiple training runs, each taking a significant
    amount of time to complete, we will want to cut a run short if training performance
    doesn’t show improvement. There’s no sense it continuing to train a model that
    isn’t improving.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的优化过程中，一个重要的组成部分是早停的使用。由于我们将进行多个训练，每次训练都需要花费相当长的时间，因此如果训练性能没有改进，我们希望提前结束训练。如果模型没有改善，继续训练没有意义。
- en: In essence, we will keep track of the lowest loss the model has produced after
    each epoch. We then define a `tolerance`, which specifies the number of epochs
    the model has to attain a better loss. If it doesn’t achieve a lower loss within
    the specified tolerance, training is terminated for that run, and we move on to
    the next combination of hyperparamters. If you’re like me, and you like to check
    in on the training process, we can log updates to the console and see when the
    early stopping counter increases by setting `self.verbose = True`. You can hard
    code that into the `EarlyStopping` class here, or you can change the `verbose`
    value when we instantiate an `EarlyStopping` object during our optimization process.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 实质上，我们将跟踪模型在每个训练周期后产生的最低损失。然后我们定义一个 `tolerance`（容忍度），它指定模型必须在多少个周期内达到更好的损失。如果在指定的容忍度内未能达到更低的损失，该次训练将被终止，我们将转到下一个超参数组合。如果你像我一样喜欢检查训练过程，我们可以将更新日志记录到控制台，并查看早停计数器何时增加，方法是将
    `self.verbose` 设置为 `True`。你可以将其硬编码到 `EarlyStopping` 类中，也可以在优化过程中实例化 `EarlyStopping`
    对象时更改 `verbose` 值。
- en: '**Image Augmentation**'
  id: totrans-103
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**图像增强**'
- en: 'We have one last thing to do before setting up our hyperparameter optimization
    method to squeeze out some extra performance, and curb overfitting on our training
    data. Image Augmentation is a technique which applies random transforms to images,
    essentially creating “new” artificial data. These transforms can be things like:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 在设置我们的超参数优化方法以挤出一些额外的性能并遏制训练数据上的过拟合之前，我们还有最后一件事要做。图像增强是一种技术，通过对图像应用随机变换，实质上创建了“新的”人工数据。这些变换可能包括：
- en: rotating an image a few degrees
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将图像旋转几度
- en: flipping an image horizontally/vertically
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 水平/垂直翻转图像
- en: cropping
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 裁剪
- en: slight brightness/hue shifts
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 轻微的亮度/色调变化
- en: random zooming
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随机缩放
- en: Including these random transforms will improve the model’s ability to generalize,
    since augmented images will be similar, but distinct to the original image. The
    contents and patterns will remain, but the array representation will be different.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 包括这些随机变换将提高模型的泛化能力，因为增强的图像将与原始图像相似，但有所不同。内容和模式将保持不变，但数组表示将有所不同。
- en: PyTorch makes image augmentation easy with the `torchvision.transforms` module.
    If we have several transforms we would like to apply, we can chain them together
    with `Compose`. One thing to keep in mind is that image augmentation requires
    a little bit of computation **per transform**, and this is applied to **every
    image** in the dataset. Applying a lot of different random transforms to our dataset
    will increase the time it takes to train. So for now, let’s limit the transforms
    so our training doesn’t take too long. If you would like to add a few more, check
    out [the PyTorch docs on transforming and augmenting images](https://pytorch.org/vision/stable/transforms.html),
    and just add those into the `Compose` list.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch 通过 `torchvision.transforms` 模块使图像增强变得简单。如果我们有几个想要应用的变换，我们可以用 `Compose`
    将它们串联在一起。需要记住的一件事是，图像增强每应用一个变换需要一点计算量，而且这一过程应用于数据集中的**每张图像**。对数据集应用大量不同的随机变换将增加训练所需的时间。因此，现在让我们限制变换，以免训练时间过长。如果你想添加更多的变换，可以查看[PyTorch
    文档关于图像变换和增强](https://pytorch.org/vision/stable/transforms.html)，并将它们添加到 `Compose`
    列表中。
- en: Once we have the augmentation transforms picked, we can apply them to the dataset
    just as we would apply Normalization and transforming the images to a tensor.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们选择了数据增强的变换，我们可以像应用归一化和将图像转换为张量一样，将它们应用到数据集上。
- en: Now that we have image augmentation set up on our training data, we’re ready
    to set up our hyperparameter optimization method.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经在训练数据上设置了图像增强，我们准备好设置我们的超参数优化方法了。
- en: '**Defining the Optimization Method**'
  id: totrans-114
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**定义优化方法**'
- en: We can create a class (`HyperSearch`) with attributes for the hyperparameter
    value configuration, verbose reporting setting, a report list so we can see how
    each configuration performed after optimization completes, and a variable to store
    the config with the best performance.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以创建一个类（`HyperSearch`），包含超参数值配置、详细报告设置、一个报告列表（以便在优化完成后查看每个配置的表现），以及一个用于存储最佳性能配置的变量。
- en: Next, we can create a method (still in our `HyperSearch` class) to perform the
    grid search and do a training run with each combination of hyperparameters. First
    we’ll configure `EarlyStopping` with `tolerance=3`, and set it to save the weights
    for each hyperparameter combination. If we have `self.verbose` set to `True` we
    can see which hyperparameter combination is currently training in the console.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们可以在`HyperSearch`类中创建一个方法来执行网格搜索，并用每种超参数组合进行训练。首先，我们将配置`EarlyStopping`，使其`容忍度=tolerance=3`，并设置它保存每个超参数组合的权重。如果`self.verbose`设置为`True`，我们可以在控制台看到当前正在训练的超参数组合。
- en: After that, we define our `model` with the `CoinfigNet` model we designed, and
    pass the `l1`, `c1`, and `c2` values, as well as picking the loss function and
    optimizer, and setting up our train and validation `DataLoaders`. We will keep
    the number of epochs low, because we don’t have the time, nor desire, to train
    every combination fully. The goal is to get an idea of which combination will
    work best at classifying the dataset, then we can take that model and train it
    fully to see how well it can perform from a full training cycle.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们用我们设计的`CoinfigNet`模型来定义`model`，并传入`l1`、`c1`和`c2`值，同时选择损失函数和优化器，并设置我们的训练和验证`DataLoaders`。我们将保持较低的训练轮数，因为我们没有时间，也不希望完全训练每一个组合。目标是了解哪个组合在分类数据集时表现最好，然后我们可以拿到那个模型，进行全面训练，以查看其在完整训练周期中的表现。
- en: Now, we define our training loop, mostly the same as before, except now we’ll
    save the loss of the `train` and `test` methods so that `early_stopping` can keep
    track of training progress (or lack thereof). Finally after each epoch, the results
    are saved to a report, and the value for the best loss is updated.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们定义训练循环，基本上与之前相同，只是现在我们将保存`train`和`test`方法的损失，以便`early_stopping`可以跟踪训练进度（或缺乏进度）。最后，在每个轮次后，结果将被保存到报告中，并更新最佳损失值。
- en: We can output the results of the entire hyperparameter optimization cycle in
    a nice table, where we’ll be able to see the hyperparameter configuration for
    each run, and the respective loss and accuracy.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将整个超参数优化周期的结果输出为一个漂亮的表格，其中我们将能看到每次运行的超参数配置以及相应的损失和准确率。
- en: 'So putting all of this code together, our `HyperSearch` class should look like
    this:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 将所有这些代码整合在一起，我们的`HyperSearch`类应该如下所示：
- en: '**Time to tune!**'
  id: totrans-121
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**调整时间到！**'
- en: Now we can tune our hyperparameters! By using `%%time`, at the completion of
    execution of the entire tuning process, we can see exactly how long it all took.
    Let’s keep our learning rate `lrate=0.001` and the batch size `batch_sz=512`,
    instantiate `HyperSearch` with the `search_space` we defined earlier, set `verbose`
    equal to `True` or `False` (whichever you prefer), and call the `optimize()` method
    to start.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以调整我们的超参数了！通过使用`%%time`，在整个调优过程完成后，我们可以精确地看到所用的时间。我们将学习率保持为`lrate=0.001`和批量大小为`batch_sz=512`，用之前定义的`search_space`实例化`HyperSearch`，将`verbose`设置为`True`或`False`（你可以选择），然后调用`optimize()`方法开始调优。
- en: '**Note:** This took about 50 minutes to complete on my machine with an NVIDIA
    RTX 3070, so expect this to take around that long to complete if you’re on Colab
    using the provided GPU.'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意：** 在我的机器上（NVIDIA RTX 3070），完成这个过程大约花费了50分钟，所以如果你在Colab上使用提供的GPU，预计也会花费差不多的时间。'
- en: 'Once the entire optimization cycle is complete, you should get a table like
    this:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦整个优化周期完成，你应该会得到如下表格：
- en: '![](../Images/ce5ec39125424fde755fa34dc2beb37c.png)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ce5ec39125424fde755fa34dc2beb37c.png)'
- en: 'Image Source: Author'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：作者
- en: '**Results**'
  id: totrans-127
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**结果**'
- en: Looking at the table, the best results came from Run 00 which had `c1=48`, `c2=96`,
    and `l1=256`. A loss of 0.84 and accuracy of 71.24% is a nice improvement, especially
    considering it was only 10 epochs!
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 从表格来看，最佳结果来自Run 00，其`c1=48`，`c2=96`，`l1=256`。0.84的损失和71.24%的准确率是一个不错的改进，特别是考虑到它仅仅训练了10个轮次！
- en: 'So, now that we have the hyperparameters with the best performance over 10
    epochs, let’s fine tune this model! We can train it over many more epochs, and
    lower the learning rate slightly to try and squeeze out a little more performance.
    So first, let’s define the model we’d like to use, and set the batch size and
    learning rate:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们已经找到了在10个epoch中表现最佳的超参数，接下来就对这个模型进行微调吧！我们可以训练更多的epoch，并稍微降低学习率以尝试挤出更多的性能。首先，让我们定义我们希望使用的模型，并设置批量大小和学习率：
- en: Finally, we can set `epochs` to 50, and change the path that we want to save
    the weights to. Let the training cycle run, and early stopping will terminate
    training if progress halts.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以将`epochs`设置为50，并更改保存权重的路径。让训练周期运行，如果进展停止，早停将终止训练。
- en: Early stopping should terminate training before hitting 50 epochs, and should
    achieve an accuracy of about 77%.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 早停应该在达到50个epoch之前终止训练，并且应该达到约77%的准确率。
- en: '![](../Images/2da8c39fd833b4ad2c65c9080965621a.png)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2da8c39fd833b4ad2c65c9080965621a.png)'
- en: 'Image source: Author'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：作者
- en: Now that we’ve tuned hyperparameters, found our best configuration, and fine-tuned
    that model, it’s time to evaluate the model’s performance a little more in-depth.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经调整了超参数，找到了最佳配置，并对模型进行了微调，是时候更深入地评估模型的表现了。
- en: '**Model Evaluation**'
  id: totrans-135
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**模型评估**'
- en: In this case, our test dataset is actually our validation data. We will be reusing
    our validation data to evaluate the model, but usually you will want to use your
    real test data for model evaluation after hyperparameter tuning. Let’s load in
    our optimized model, prepare the `test_dataloader` without any image augmentation
    applied, and run `test()` to evaluate.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们的测试数据集实际上是我们的验证数据。我们将重用验证数据来评估模型，但通常你会在超参数调整后使用真正的测试数据进行模型评估。让我们加载优化后的模型，准备没有应用任何图像增强的`test_dataloader`，并运行`test()`进行评估。
- en: 'This should output the accuracy and loss:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 这应该输出准确率和损失：
- en: '![](../Images/9e779c41b6c96d762f987e0686238b39.png)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9e779c41b6c96d762f987e0686238b39.png)'
- en: 'Image source: Author'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：作者
- en: 'The overall performance is nice, but the performance for each class will be
    more useful to us. The following code will output our model’s accuracy for each
    class in the dataset:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 整体表现不错，但每个类别的表现对我们更有用。以下代码将输出我们模型在数据集中每个类别的准确率：
- en: 'Executing this block will give us the following output:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 执行这个代码块将给出以下输出：
- en: '![](../Images/9541621aa24e8bc4717ad486068317a9.png)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9541621aa24e8bc4717ad486068317a9.png)'
- en: 'Image source: Author'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：作者
- en: Our model performed quite well on the airplane, automobile, frog, ship, and
    truck classes. Also interesting to note that the classes it struggled most with
    are dog and cat, which were also the toughest classes for the fully connected
    model in the previous part of this series.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的模型在飞机、汽车、青蛙、船和卡车类别上的表现相当不错。同样值得注意的是，它最难处理的类别是狗和猫，这也是在本系列前面部分中，完全连接模型表现最差的类别。
- en: '**Confusion Matrix**'
  id: totrans-145
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**混淆矩阵**'
- en: We can gain even more insight on performance with a confusion matrix. Let’s
    set one up, then get a nice visualization.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过混淆矩阵获得更多关于性能的洞察。让我们建立一个混淆矩阵，然后获取一个良好的可视化效果。
- en: With `confusion_matrix` defined, we can use the Seaborn library to help us visualize
    it.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 定义了`confusion_matrix`之后，我们可以使用Seaborn库帮助我们可视化它。
- en: '![](../Images/efe97e933ef296bd8dcb0efa04bd3dcf.png)'
  id: totrans-148
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/efe97e933ef296bd8dcb0efa04bd3dcf.png)'
- en: 'Image source: Author'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：作者
- en: The two dimensions of this table are the “actual” and “predicted” values. We
    want most of our data to align in that center diagonal, where actual and predicted
    are the same class. From the incorrect predictions we can see the model often
    confused `cats` and `dogs`, which were the two classes with the lowest accuracy.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 该表的两个维度是“实际”值和“预测”值。我们希望大部分数据对齐在中心对角线处，即实际和预测类别相同。从错误预测中，我们可以看到模型经常混淆`cats`和`dogs`，这两个类别的准确率最低。
- en: Totals are nice to see, but precision and recall for each class will give us
    much more meaningful data. Let’s have a look at the recall per class first.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 总体数据很好，但每个类别的精度和召回率将提供更有意义的数据。让我们首先查看每个类别的召回率。
- en: '**Recall per Class**'
  id: totrans-152
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**每个类别的召回率**'
- en: '![](../Images/4c129e182dc3e733ac99fcd076beb09a.png)'
  id: totrans-153
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4c129e182dc3e733ac99fcd076beb09a.png)'
- en: 'Image source: Author'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：作者
- en: '**Precision per Class**'
  id: totrans-155
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**每个类别的精度**'
- en: '![](../Images/e5c94e732f7fc046f3751a1e06d23095.png)'
  id: totrans-156
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e5c94e732f7fc046f3751a1e06d23095.png)'
- en: 'Image source: Author'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：作者
- en: '**Sample Model Predictions**'
  id: totrans-158
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**样本模型预测**'
- en: 'Finally, let’s feed our model a few images and check out the predictions it
    makes. Let’s make a function to get our image data ready to view:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们给模型喂入几张图像，查看其做出的预测。让我们创建一个函数来准备图像数据以供查看：
- en: Now, we can get our test data prepared, and make another function to get a sample
    of `n` predictions
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以准备测试数据，并创建另一个函数来获取`n`个预测样本。
- en: Call the function, passing the number of images you want to sample. The output
    will give us the ground truth and predicted class for each image starting from
    left to right.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 调用函数，传入你想要采样的图像数量。输出将给出每张图像从左到右的真实标签和预测类别。
- en: '![](../Images/6b5f8a5182cb47e4ac87f5d0fc911a2a.png)'
  id: totrans-162
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6b5f8a5182cb47e4ac87f5d0fc911a2a.png)'
- en: 'Image source: Author'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：作者
- en: '![](../Images/319869d0ee25f4e30176f20816cd778c.png)'
  id: totrans-164
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/319869d0ee25f4e30176f20816cd778c.png)'
- en: 'Image source: Author'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：作者
- en: Utilizing a convolutional network with hyperparameter tuning and image augmentation
    really helped improve the performance on the CIFAR-10 dataset! As always, thanks
    for reading, and I really hope you’ve learned a bit about PyTorch and CNN’s for
    image classification. The full Notebook with all of the code presented here is
    available on [GitHub](https://github.com/florestony54/intro-to-pytorch-2/blob/main/pytorch2_2.ipynb).
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 利用卷积网络进行超参数调优和图像增强确实有助于提高在CIFAR-10数据集上的表现！一如既往，感谢阅读，希望你对PyTorch和用于图像分类的CNN有了一些了解。包含所有代码的完整Notebook可以在[GitHub](https://github.com/florestony54/intro-to-pytorch-2/blob/main/pytorch2_2.ipynb)上找到。
