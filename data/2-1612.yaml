- en: 'Other ML Jargons: Sparse and Dense Representations of Texts for Machine Learning'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å…¶ä»–æœºå™¨å­¦ä¹ æœ¯è¯­ï¼šæ–‡æœ¬çš„ç¨€ç–å’Œå¯†é›†è¡¨ç¤º
- en: åŸæ–‡ï¼š[https://towardsdatascience.com/other-ml-jargons-sparse-and-dense-representations-of-texts-for-machine-learning-21fcd7a01410](https://towardsdatascience.com/other-ml-jargons-sparse-and-dense-representations-of-texts-for-machine-learning-21fcd7a01410)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡ï¼š[https://towardsdatascience.com/other-ml-jargons-sparse-and-dense-representations-of-texts-for-machine-learning-21fcd7a01410](https://towardsdatascience.com/other-ml-jargons-sparse-and-dense-representations-of-texts-for-machine-learning-21fcd7a01410)
- en: OTHER ML JARGONS
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å…¶ä»–æœºå™¨å­¦ä¹ æœ¯è¯­
- en: A Brief Introduction to Vectorization and its Importance in the Context of NLP
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å‘é‡åŒ–çš„ç®€è¦ä»‹ç»åŠå…¶åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ä¸­çš„é‡è¦æ€§
- en: '[](https://nroy0110.medium.com/?source=post_page-----21fcd7a01410--------------------------------)[![Nabanita
    Roy](../Images/83ab7766a28c79371ebf9517e1f273d2.png)](https://nroy0110.medium.com/?source=post_page-----21fcd7a01410--------------------------------)[](https://towardsdatascience.com/?source=post_page-----21fcd7a01410--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----21fcd7a01410--------------------------------)
    [Nabanita Roy](https://nroy0110.medium.com/?source=post_page-----21fcd7a01410--------------------------------)'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://nroy0110.medium.com/?source=post_page-----21fcd7a01410--------------------------------)[![Nabanita
    Roy](../Images/83ab7766a28c79371ebf9517e1f273d2.png)](https://nroy0110.medium.com/?source=post_page-----21fcd7a01410--------------------------------)[](https://towardsdatascience.com/?source=post_page-----21fcd7a01410--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----21fcd7a01410--------------------------------)
    [Nabanita Roy](https://nroy0110.medium.com/?source=post_page-----21fcd7a01410--------------------------------)'
- en: Â·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----21fcd7a01410--------------------------------)
    Â·9 min readÂ·Feb 15, 2023
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: Â·å‘è¡¨äº [Towards Data Science](https://towardsdatascience.com/?source=post_page-----21fcd7a01410--------------------------------)
    Â·9 åˆ†é’Ÿé˜…è¯»Â·2023å¹´2æœˆ15æ—¥
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/a2550ebb25c16994eb499f8e089b474a.png)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a2550ebb25c16994eb499f8e089b474a.png)'
- en: Photo by [Compare Fibre](https://unsplash.com/@comparefibre?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ç‰‡ç”± [Compare Fibre](https://unsplash.com/@comparefibre?utm_source=medium&utm_medium=referral)
    æä¾›ï¼Œæ¥æºäº [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: '**Introduction**'
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**ä»‹ç»**'
- en: Matrices and vectors are quantified information that Machine Learning(ML) algorithms
    require for learning patterns and making predictions. For applying these techniques
    to textual data as well, numeric representations of the texts are engineered to
    form matrices that hold the relevant information from those texts. The concepts
    of â€œSparsityâ€ and â€œDensityâ€ arrive at efficiently designing and constructing these
    matrices for all high-dimensional data processing use-cases in the world of Artificial
    Intelligence.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: çŸ©é˜µå’Œå‘é‡æ˜¯æœºå™¨å­¦ä¹ ï¼ˆMLï¼‰ç®—æ³•è¿›è¡Œæ¨¡å¼å­¦ä¹ å’Œé¢„æµ‹æ‰€éœ€çš„é‡åŒ–ä¿¡æ¯ã€‚å°†è¿™äº›æŠ€æœ¯åº”ç”¨äºæ–‡æœ¬æ•°æ®æ—¶ï¼Œæ–‡æœ¬çš„æ•°å€¼è¡¨ç¤ºè¢«å·¥ç¨‹åŒ–ä¸ºçŸ©é˜µï¼Œä»¥åŒ…å«æ–‡æœ¬ä¸­çš„ç›¸å…³ä¿¡æ¯ã€‚â€œç¨€ç–æ€§â€å’Œâ€œå¯†é›†æ€§â€æ¦‚å¿µæœ‰åŠ©äºé«˜æ•ˆè®¾è®¡å’Œæ„å»ºè¿™äº›çŸ©é˜µï¼Œç”¨äºäººå·¥æ™ºèƒ½é¢†åŸŸä¸­çš„æ‰€æœ‰é«˜ç»´æ•°æ®å¤„ç†ç”¨ä¾‹ã€‚
- en: Significance of Vector Representations for NLP
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å‘é‡è¡¨ç¤ºåœ¨è‡ªç„¶è¯­è¨€å¤„ç†ä¸­çš„é‡è¦æ€§
- en: 'Representing text data as vectors are necessary for applying Machine Learning
    techniques to make predictions, recommendations, or clusters. In NLP, the concept
    of â€œ*similar words occur in similar contextsâ€* is fundamental. Letâ€™s see how:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: å°†æ–‡æœ¬æ•°æ®è¡¨ç¤ºä¸ºå‘é‡æ˜¯åº”ç”¨æœºå™¨å­¦ä¹ æŠ€æœ¯è¿›è¡Œé¢„æµ‹ã€æ¨èæˆ–èšç±»çš„å¿…è¦æ¡ä»¶ã€‚åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰ä¸­ï¼Œâ€œ*ç›¸ä¼¼çš„å•è¯å‡ºç°åœ¨ç›¸ä¼¼çš„ä¸Šä¸‹æ–‡ä¸­*â€è¿™ä¸€æ¦‚å¿µæ˜¯åŸºç¡€çš„ã€‚è®©æˆ‘ä»¬æ¥çœ‹çœ‹ï¼š
- en: In **Text Classification** use-cases like categorizing support tickets, spam
    detection, fake news detection, and feedback sentiment analysis, texts having
    similar words are classified into a particular category.
  id: totrans-13
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: åœ¨**æ–‡æœ¬åˆ†ç±»**ç”¨ä¾‹ä¸­ï¼Œå¦‚æ”¯æŒç¥¨æ®çš„åˆ†ç±»ã€åƒåœ¾é‚®ä»¶æ£€æµ‹ã€è™šå‡æ–°é—»æ£€æµ‹å’Œåé¦ˆæƒ…æ„Ÿåˆ†æï¼Œå…·æœ‰ç›¸ä¼¼å•è¯çš„æ–‡æœ¬ä¼šè¢«åˆ†ç±»åˆ°ç‰¹å®šç±»åˆ«ä¸­ã€‚
- en: In R**ecommendation Systems**, people with similar profile details, browsing
    history, and past orders indicate similar choices or tastes in products. This
    information is used to make recommendations.
  id: totrans-14
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: åœ¨**æ¨èç³»ç»Ÿ**ä¸­ï¼Œå…·æœ‰ç›¸ä¼¼ä¸ªäººèµ„æ–™ä¿¡æ¯ã€æµè§ˆå†å²å’Œè¿‡å»è®¢å•çš„äººè¡¨ç¤ºå¯¹äº§å“æœ‰ç±»ä¼¼çš„é€‰æ‹©æˆ–å£å‘³ã€‚è¿™äº›ä¿¡æ¯ç”¨äºç”Ÿæˆæ¨èã€‚
- en: '**Unsupervised Clustering** looks for patterns and similar words in the texts
    to group documents and articles. Typical applications include segregating news
    articles, trend analysis, and customer segmentation.'
  id: totrans-15
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**æ— ç›‘ç£èšç±»**åœ¨æ–‡æœ¬ä¸­å¯»æ‰¾æ¨¡å¼å’Œç›¸ä¼¼çš„å•è¯ï¼Œä»¥ä¾¿å¯¹æ–‡æ¡£å’Œæ–‡ç« è¿›è¡Œåˆ†ç»„ã€‚å…¸å‹åº”ç”¨åŒ…æ‹¬æ–°é—»æ–‡ç« çš„åˆ†ç±»ã€è¶‹åŠ¿åˆ†æå’Œå®¢æˆ·ç»†åˆ†ã€‚'
- en: In **Information Retrieval** systems, indexed documents are matched with queries,
    sometimes in a â€œfuzzyâ€ way and the collection of matched documents is returned
    to the user. Besides, the measure of similarity is used to rank the search.
  id: totrans-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: åœ¨**ä¿¡æ¯æ£€ç´¢**ç³»ç»Ÿä¸­ï¼Œç´¢å¼•æ–‡æ¡£ä¼šä¸æŸ¥è¯¢è¿›è¡ŒåŒ¹é…ï¼Œæœ‰æ—¶ä»¥â€œæ¨¡ç³Šâ€çš„æ–¹å¼è¿›è¡Œï¼Œç„¶åå°†åŒ¹é…çš„æ–‡æ¡£é›†åˆè¿”å›ç»™ç”¨æˆ·ã€‚æ­¤å¤–ï¼Œç›¸ä¼¼åº¦åº¦é‡ç”¨äºå¯¹æœç´¢ç»“æœè¿›è¡Œæ’åºã€‚
- en: Hence, capturing similarities in these vectors is a primary research area in
    the NLP domain. These vectors are projected in an N-dimensional plane and then
    patterns in these vectors in the N-dimensional space are extracted to categorize
    the texts. Sometimes, dimensionality reduction techniques are applied, like PCA
    or t-SNE. The design of the vectors controls the overall performance of text-based
    ML models and is, hence, crucial.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: å› æ­¤ï¼Œæ•æ‰è¿™äº›å‘é‡ä¸­çš„ç›¸ä¼¼æ€§æ˜¯è‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸçš„ä¸€ä¸ªä¸»è¦ç ”ç©¶æ–¹å‘ã€‚è¿™äº›å‘é‡è¢«æŠ•å½±åˆ°ä¸€ä¸ªNç»´å¹³é¢ä¸­ï¼Œç„¶åä»Nç»´ç©ºé—´ä¸­çš„è¿™äº›å‘é‡ä¸­æå–æ¨¡å¼ä»¥å¯¹æ–‡æœ¬è¿›è¡Œåˆ†ç±»ã€‚æœ‰æ—¶ä¼šåº”ç”¨é™ç»´æŠ€æœ¯ï¼Œå¦‚PCAæˆ–t-SNEã€‚å‘é‡çš„è®¾è®¡æ§åˆ¶äº†åŸºäºæ–‡æœ¬çš„æœºå™¨å­¦ä¹ æ¨¡å‹çš„æ•´ä½“æ€§èƒ½ï¼Œå› æ­¤è‡³å…³é‡è¦ã€‚
- en: The vector designs are broadly classified as â€œSparseâ€ (meaning scarcely populated)
    and â€œDenseâ€ (meaning densely populated) vectors. In this article, I have recalled
    the concepts of matrices and vectors from a mathematical perspective, and then
    discussed these two classes of vectorization techniques â€” sparse vector representations
    and dense vector representations. including a demo using Scikit Learn and Gensim,
    respectively. I have also concluded this article with an overview of the applications
    and usability of these representations.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: å‘é‡è®¾è®¡å¤§è‡´åˆ†ä¸ºâ€œç¨€ç–â€ï¼ˆæ„æŒ‡å¾ˆå°‘å¡«å……ï¼‰å’Œâ€œå¯†é›†â€ï¼ˆæ„æŒ‡å¯†é›†å¡«å……ï¼‰ä¸¤ç±»ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»æ•°å­¦çš„è§’åº¦å›é¡¾äº†çŸ©é˜µå’Œå‘é‡çš„æ¦‚å¿µï¼Œç„¶åè®¨è®ºäº†è¿™ä¸¤ç§å‘é‡åŒ–æŠ€æœ¯â€”â€”ç¨€ç–å‘é‡è¡¨ç¤ºå’Œå¯†é›†å‘é‡è¡¨ç¤ºï¼ŒåŒ…æ‹¬åˆ†åˆ«ä½¿ç”¨
    Scikit Learn å’Œ Gensim çš„æ¼”ç¤ºã€‚æˆ‘è¿˜åœ¨æ–‡ç« æœ«å°¾æ€»ç»“äº†è¿™äº›è¡¨ç¤ºçš„åº”ç”¨å’Œå®ç”¨æ€§ã€‚
- en: A Primer to Matrices and Vectors
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: çŸ©é˜µå’Œå‘é‡çš„å…¥é—¨
- en: Mathematically, *a matrix is defined as a 2-dimensional rectangular array of
    numbers.* If the array has *m* rows and *n* columns, then it is a matrix of size
    *m Ã— n*.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ•°å­¦ä¸Šï¼Œ*çŸ©é˜µå®šä¹‰ä¸ºä¸€ä¸ªäºŒç»´çŸ©å½¢æ•°å­—æ•°ç»„ã€‚* å¦‚æœæ•°ç»„æœ‰ *m* è¡Œå’Œ *n* åˆ—ï¼Œé‚£ä¹ˆå®ƒæ˜¯ä¸€ä¸ª *m Ã— n* çš„çŸ©é˜µã€‚
- en: 'If a matrix has only one row OR only one column it is called a vector. A *1Ã—n*
    matrix or vector is a row vector (where there are *n* columns but only *1* row)
    and an *m Ã— 1* matrix or vector is a column vector (where there are *m* rows but
    only *1* column). Hereâ€™s an image that clearly demonstrates this:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœä¸€ä¸ªçŸ©é˜µåªæœ‰ä¸€è¡Œæˆ–åªæœ‰ä¸€åˆ—ï¼Œå®ƒè¢«ç§°ä¸ºå‘é‡ã€‚*1Ã—n* çŸ©é˜µæˆ–å‘é‡æ˜¯è¡Œå‘é‡ï¼ˆå…¶ä¸­æœ‰ *n* åˆ—ä½†åªæœ‰ *1* è¡Œï¼‰ï¼Œè€Œ *m Ã— 1* çŸ©é˜µæˆ–å‘é‡æ˜¯åˆ—å‘é‡ï¼ˆå…¶ä¸­æœ‰
    *m* è¡Œä½†åªæœ‰ *1* åˆ—ï¼‰ã€‚è¿™é‡Œæœ‰ä¸€å¼ æ¸…æ™°æ¼”ç¤ºè¿™ä¸€ç‚¹çš„å›¾ç‰‡ï¼š
- en: '![](../Images/a79c68118010432cfdd21fbc389d8c03.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a79c68118010432cfdd21fbc389d8c03.png)'
- en: 'Image source: [Linear Algebra for Data Science Ep1 â€” Introduction to Vectors
    and Matrices using Python](/introduction-to-vectors-and-matrices-using-python-for-data-science-e836e014eb12)'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ç‰‡æ¥æºï¼š[æ•°æ®ç§‘å­¦çº¿æ€§ä»£æ•° Ep1 â€” ä½¿ç”¨ Python ä»‹ç»å‘é‡å’ŒçŸ©é˜µ](/introduction-to-vectors-and-matrices-using-python-for-data-science-e836e014eb12)
- en: Hereâ€™s a [primer for scalars, vectors, and matrices](https://www.mathsisfun.com/algebra/scalar-vector-matrix.html)
    and an [Introduction to Vectors and Matrices using Python](/introduction-to-vectors-and-matrices-using-python-for-data-science-e836e014eb12)
    for Data Science.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯ä¸€ä¸ª [æ ‡é‡ã€å‘é‡å’ŒçŸ©é˜µçš„å…¥é—¨](https://www.mathsisfun.com/algebra/scalar-vector-matrix.html)
    å’Œ [æ•°æ®ç§‘å­¦ä¸­ä½¿ç”¨ Python ä»‹ç»å‘é‡å’ŒçŸ©é˜µ](/introduction-to-vectors-and-matrices-using-python-for-data-science-e836e014eb12)ã€‚
- en: Sparse Representations | Matrices | Vectors
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç¨€ç–è¡¨ç¤º | çŸ©é˜µ | å‘é‡
- en: '![](../Images/3b14d8e5f8e6f8b1c9e99ed0eceaadf1.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3b14d8e5f8e6f8b1c9e99ed0eceaadf1.png)'
- en: Photo by [Henning Witzel](https://unsplash.com/@henning?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ç‰‡ç”± [Henning Witzel](https://unsplash.com/@henning?utm_source=medium&utm_medium=referral)
    æä¾›ï¼Œæ¥è‡ª [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: In almost all real-world cases, the count-based quantified numeric representation
    of information is **sparse in nature**, in other words, ***the numeric representation*
    *contains only a fraction that is useful to you in an ocean of numbers.***
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å‡ ä¹æ‰€æœ‰å®é™…åº”ç”¨ä¸­ï¼ŒåŸºäºè®¡æ•°çš„é‡åŒ–æ•°å€¼è¡¨ç¤ºä¿¡æ¯é€šå¸¸æ˜¯**ç¨€ç–çš„**ï¼Œæ¢å¥è¯è¯´ï¼Œ***æ•°å€¼è¡¨ç¤º* *åªåŒ…å«åœ¨æµ·é‡æ•°å­—ä¸­å¯¹ä½ æœ‰ç”¨çš„ä¸€å°éƒ¨åˆ†ã€‚***
- en: It is because, intuitively, in a collection of documents, only words that are
    articles, prepositions, conjunctions, and pronouns are overtly used and therefore,
    have a higher frequency of occurrence. However, in a collection of sports news
    articles, the terms â€˜soccerâ€™ or â€˜basketballâ€™, occurrences of which would help
    us determine which sport is the article associated with, occurs only a few times
    but is not of a very high frequency.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯å› ä¸ºï¼Œä»ç›´è§‚ä¸Šçœ‹ï¼Œåœ¨æ–‡æ¡£é›†åˆä¸­ï¼Œåªæœ‰ä½œä¸ºæ–‡ç« ã€ä»‹è¯ã€è¿è¯å’Œä»£è¯çš„å•è¯è¢«æ˜æ˜¾ä½¿ç”¨ï¼Œå› æ­¤å…·æœ‰è¾ƒé«˜çš„å‡ºç°é¢‘ç‡ã€‚ç„¶è€Œï¼Œåœ¨ä¸€ç»„ä½“è‚²æ–°é—»æ–‡ç« ä¸­ï¼Œè¯¸å¦‚â€˜soccerâ€™æˆ–â€˜basketballâ€™çš„æœ¯è¯­ï¼Œå…¶å‡ºç°æ¬¡æ•°æœ‰åŠ©äºç¡®å®šæ–‡ç« å…³è”çš„è¿åŠ¨ç±»å‹ï¼Œå°½ç®¡å‡ºç°æ¬¡æ•°è¾ƒå°‘ï¼Œä½†é¢‘ç‡å¹¶ä¸å¾ˆé«˜ã€‚
- en: Now, if we construct a vector per new article, assuming there are 50 words per
    article, the word â€˜soccerâ€™ would occur about 5 times. Hence, 45 out of 50 times,
    the elements in the vector will be zero, which indicates the absence of the word
    we are focusing on. Therefore, 90% of the vector of length 50 is redundant. This
    is an example of a [**one-hot vector**](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html)generation.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ï¼Œå¦‚æœæˆ‘ä»¬ä¸ºæ¯ä¸ªæ–°æ–‡ç« æ„é€ ä¸€ä¸ªå‘é‡ï¼Œå‡è®¾æ¯ç¯‡æ–‡ç« æœ‰50ä¸ªå•è¯ï¼Œé‚£ä¹ˆâ€˜soccerâ€™ä¼šå‡ºç°çº¦5æ¬¡ã€‚å› æ­¤ï¼Œåœ¨50æ¬¡ä¸­ï¼Œ45æ¬¡å‘é‡å…ƒç´ å°†ä¸ºé›¶ï¼Œè¿™è¡¨ç¤ºæˆ‘ä»¬å…³æ³¨çš„å•è¯çš„ç¼ºå¤±ã€‚å› æ­¤ï¼Œé•¿åº¦ä¸º50çš„å‘é‡ä¸­90%æ˜¯å†—ä½™çš„ã€‚è¿™æ˜¯[**one-hotå‘é‡**](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html)ç”Ÿæˆçš„ä¸€ä¸ªä¾‹å­ã€‚
- en: Another typical example of sparse matrix generation is the [**Count Vectorizer**](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)which
    determines how many times a word has occurred in a document. It generates a matrix
    of â€œcount vectorsâ€ per document to constitute a matrix of size *d Ã— v* where d
    is the number of documents and v is the number of words or vocabulary in the collection
    of documents.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: ç¨€ç–çŸ©é˜µç”Ÿæˆçš„å¦ä¸€ä¸ªå…¸å‹ä¾‹å­æ˜¯[**Count Vectorizer**](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)ï¼Œå®ƒç¡®å®šä¸€ä¸ªå•è¯åœ¨æ–‡æ¡£ä¸­å‡ºç°çš„æ¬¡æ•°ã€‚å®ƒä¸ºæ¯ä¸ªæ–‡æ¡£ç”Ÿæˆä¸€ä¸ªâ€œè®¡æ•°å‘é‡â€çŸ©é˜µï¼Œä»è€Œæ„æˆä¸€ä¸ª*d
    Ã— v*çš„çŸ©é˜µï¼Œå…¶ä¸­dæ˜¯æ–‡æ¡£çš„æ•°é‡ï¼Œvæ˜¯æ–‡æ¡£é›†åˆä¸­çš„å•è¯æˆ–è¯æ±‡çš„æ•°é‡ã€‚
- en: '**Hereâ€™s a demonstration of how a Count Vectorizer works:**'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '**è¿™é‡Œæ¼”ç¤ºäº†Count Vectorizerçš„å·¥ä½œåŸç†ï¼š**'
- en: Below are [four different meanings of the word â€˜demoâ€™](https://www.google.com/search?q=demo&oq=demo&aqs=chrome..69i57j69i61.775j0j4&sourceid=chrome&ie=UTF-8),
    each of which represent one document ~
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: ä»¥ä¸‹æ˜¯[â€˜demoâ€™ä¸€è¯çš„å››ç§ä¸åŒå«ä¹‰](https://www.google.com/search?q=demo&oq=demo&aqs=chrome..69i57j69i61.775j0j4&sourceid=chrome&ie=UTF-8)ï¼Œæ¯ç§å«ä¹‰ä»£è¡¨ä¸€ä¸ªæ–‡æ¡£
    ~
- en: '**Document 1:** a demonstration of a product or technique'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '**æ–‡æ¡£1ï¼š**æ¼”ç¤ºä¸€ä¸ªäº§å“æˆ–æŠ€æœ¯'
- en: '**Document 2:** a public meeting or march protesting against something or expressing
    views on a political issue'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '**æ–‡æ¡£2ï¼š**å…¬å¼€ä¼šè®®æˆ–æ¸¸è¡Œï¼ŒæŠ—è®®æŸäº‹æˆ–è¡¨è¾¾å¯¹æ”¿æ²»é—®é¢˜çš„è§‚ç‚¹'
- en: '**Document 3:** record a song or piece of music to demonstrate the capabilities
    of a musical group or performer or as preparation for a full recording'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '**æ–‡æ¡£3ï¼š**å½•åˆ¶ä¸€é¦–æ­Œæ›²æˆ–éŸ³ä¹ä½œå“ï¼Œä»¥å±•ç¤ºéŸ³ä¹å›¢é˜Ÿæˆ–è¡¨æ¼”è€…çš„èƒ½åŠ›ï¼Œæˆ–ä½œä¸ºå®Œæ•´å½•éŸ³çš„å‡†å¤‡'
- en: '**Document 4:** demonstrate the capabilities of software or another product'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '**æ–‡æ¡£4ï¼š**æ¼”ç¤ºè½¯ä»¶æˆ–å…¶ä»–äº§å“çš„åŠŸèƒ½'
- en: I used [**Scikit Learnâ€™s CountVectorizer**](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)
    implementation to generate this sparse matrix for these four â€œdocumentsâ€. Below
    is the code I have used ğŸ‘©â€ğŸ’»
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä½¿ç”¨äº†[**Scikit Learnçš„CountVectorizer**](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)å®ç°æ¥ç”Ÿæˆè¿™å››ä¸ªâ€œæ–‡æ¡£â€çš„ç¨€ç–çŸ©é˜µã€‚ä»¥ä¸‹æ˜¯æˆ‘ä½¿ç”¨çš„ä»£ç ğŸ‘©â€ğŸ’»
- en: 'Image Source: Author'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ç‰‡æ¥æºï¼šä½œè€…
- en: 'The output of *_cv.toarray()* is of the numeric representation of the words
    in a **4** *Ã—* **34 array** (*converted using .toarray() from the vector*)as in
    the screenshot below:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '*_cv.toarray()*çš„è¾“å‡ºæ˜¯**4** *Ã—* **34æ•°ç»„**ä¸­å•è¯çš„æ•°å­—è¡¨ç¤ºï¼ˆ*ä½¿ç”¨.toarray()ä»å‘é‡è½¬æ¢*ï¼‰ï¼Œå¦‚ä¸‹å›¾æ‰€ç¤ºï¼š'
- en: '![](../Images/977ab67300d896810371527907ee6841.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/977ab67300d896810371527907ee6841.png)'
- en: 'Find this example in this Jupiter Notebook | Image Source: Author'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ä¸ªJupiter Notebookä¸­æ‰¾åˆ°è¿™ä¸ªä¾‹å­ | å›¾ç‰‡æ¥æºï¼šä½œè€…
- en: 'In the matrix:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨çŸ©é˜µä¸­ï¼š
- en: zero represents no occurrence at all (basically no information)
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: zeroè¡¨ç¤ºå®Œå…¨æ²¡æœ‰å‡ºç°ï¼ˆåŸºæœ¬ä¸Šæ²¡æœ‰ä¿¡æ¯ï¼‰
- en: anything more than zero is the number of times the word has occurred in the
    four documents (some useful and some redundant information).
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä»»ä½•å¤§äºé›¶çš„æ•°å€¼æ˜¯å•è¯åœ¨å››ä¸ªæ–‡æ¡£ä¸­å‡ºç°çš„æ¬¡æ•°ï¼ˆä¸€äº›æœ‰ç”¨çš„ä¿¡æ¯å’Œä¸€äº›å†—ä½™çš„ä¿¡æ¯ï¼‰ã€‚
- en: 34 is the size of the vocabulary (or the total number of unique words in the
    documents), hence the shape is 4 x 34.
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 34æ˜¯è¯æ±‡è¡¨çš„å¤§å°ï¼ˆæˆ–æ–‡æ¡£ä¸­å”¯ä¸€å•è¯çš„æ€»æ•°ï¼‰ï¼Œå› æ­¤å½¢çŠ¶ä¸º4 x 34ã€‚
- en: The simplest measure of sparsity is the fraction of the total number of zeroes
    over the total number of elements, in this case ~
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: ç¨€ç–æ€§çš„æœ€ç®€å•è¡¡é‡æ ‡å‡†æ˜¯é›¶å…ƒç´ å æ€»å…ƒç´ çš„æ¯”ä¾‹ï¼Œåœ¨è¿™ç§æƒ…å†µä¸‹çº¦ä¸º~
- en: Number of zeros = 93
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: é›¶çš„æ•°é‡ = 93
- en: Number of elements in the array = 4 *Ã—* 34 *=* 136
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ•°ç»„ä¸­çš„å…ƒç´ æ•°é‡ = 4 *Ã—* 34 *=* 136
- en: Therefore, more than 50% of the array has no information at all (93 out of 136),
    yet it is a high-dimensional matrix that needs more memory (increased [**Space
    complexity**](https://en.wikipedia.org/wiki/Space_complexity)) and computation
    time (increased [**Time complexity**](https://en.wikipedia.org/wiki/Time_complexity)).
    If a machine learning model is fed with this high-dimensional data, it will find
    it difficult to
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: å› æ­¤ï¼Œæ•°ç»„ä¸­è¶…è¿‡50%çš„å…ƒç´ å®Œå…¨æ²¡æœ‰ä¿¡æ¯ï¼ˆ93ä¸ªä¸­çš„136ä¸ªï¼‰ï¼Œä½†è¿™æ˜¯ä¸€ä¸ªé«˜ç»´çŸ©é˜µï¼Œéœ€è¦æ›´å¤šçš„å†…å­˜ï¼ˆå¢åŠ çš„ [**ç©ºé—´å¤æ‚åº¦**](https://en.wikipedia.org/wiki/Space_complexity)ï¼‰å’Œè®¡ç®—æ—¶é—´ï¼ˆå¢åŠ çš„
    [**æ—¶é—´å¤æ‚åº¦**](https://en.wikipedia.org/wiki/Time_complexity)ï¼‰ã€‚å¦‚æœæœºå™¨å­¦ä¹ æ¨¡å‹ä½¿ç”¨è¿™ä¸ªé«˜ç»´æ•°æ®ï¼Œå®ƒå°†å‘ç°å¾ˆéš¾å¤„ç†ã€‚
- en: find patterns
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¯»æ‰¾æ¨¡å¼
- en: will be far too expensive to tune weights for all the dimensions
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¸ºæ‰€æœ‰ç»´åº¦è°ƒæ•´æƒé‡å°†èŠ±è´¹è¿‡é«˜ã€‚
- en: it will eventually lead to latency issues during prediction time
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å®ƒæœ€ç»ˆä¼šå¯¼è‡´é¢„æµ‹æ—¶çš„å»¶è¿Ÿé—®é¢˜ã€‚
- en: Sparsity is analogous to the concept of the curse of dimensionality. Recommender
    systems and collaborative filtering techniques, count-based data representations
    including the famous TF-IDF are prone to issues related to the sparsity of textual
    data.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: ç¨€ç–æ€§ç±»ä¼¼äºç»´åº¦è¯…å’’çš„æ¦‚å¿µã€‚æ¨èç³»ç»Ÿå’ŒååŒè¿‡æ»¤æŠ€æœ¯ã€è®¡æ•°æ•°æ®è¡¨ç¤ºï¼ŒåŒ…æ‹¬è‘—åçš„TF-IDFï¼Œéƒ½å®¹æ˜“å—åˆ°æ–‡æœ¬æ•°æ®ç¨€ç–æ€§ç›¸å…³çš„é—®é¢˜ã€‚
- en: Dense Representations | Matrices | Vectors
  id: totrans-55
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç¨ å¯†è¡¨ç¤º | çŸ©é˜µ | å‘é‡
- en: '![](../Images/c64852af08fb0022efd2856c916bbdfc.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c64852af08fb0022efd2856c916bbdfc.png)'
- en: Photo by [Mike L](https://unsplash.com/@wheremikeat?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: ç”± [Mike L](https://unsplash.com/@wheremikeat?utm_source=medium&utm_medium=referral)
    åœ¨ [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral) æ‹æ‘„
- en: The way out? A representation that has more information and less redundancy,
    is mathematically defined as a matrix or a vector where most elements are non-zero.
    Such data representations are called dense matrices or dense vectors. They are
    also usually smaller in size than sparse matrices.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: å‡ºè·¯ï¼Ÿä¸€ç§åŒ…å«æ›´å¤šä¿¡æ¯ä¸”å†—ä½™æ›´å°‘çš„è¡¨ç¤ºï¼Œæ•°å­¦ä¸Šå®šä¹‰ä¸ºå¤§å¤šæ•°å…ƒç´ éé›¶çš„çŸ©é˜µæˆ–å‘é‡ã€‚è¿™ç§æ•°æ®è¡¨ç¤ºè¢«ç§°ä¸ºç¨ å¯†çŸ©é˜µæˆ–ç¨ å¯†å‘é‡ã€‚å®ƒä»¬é€šå¸¸æ¯”ç¨€ç–çŸ©é˜µçš„å°ºå¯¸æ›´å°ã€‚
- en: Advantages of using Dense Vectors for Machine Learning
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä½¿ç”¨ç¨ å¯†å‘é‡è¿›è¡Œæœºå™¨å­¦ä¹ çš„ä¼˜ç‚¹
- en: Smaller the dimension, the faster and easier to optimize weights for ML models
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ç»´åº¦è¶Šå°ï¼Œä¼˜åŒ–æœºå™¨å­¦ä¹ æ¨¡å‹çš„æƒé‡å°±è¶Šå¿«ã€è¶Šå®¹æ˜“ã€‚
- en: Even though dense vectors could have smaller dimensions than sparse matrices,
    they could also be large enough to challenge computational infrastructures (imagine
    Word2Vec or BERT-based representations), but still would contain rich and useful
    information like syntactical, semantical, or morphological relationships.
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å°½ç®¡ç¨ å¯†å‘é‡çš„ç»´åº¦å¯èƒ½æ¯”ç¨€ç–çŸ©é˜µå°ï¼Œä½†å®ƒä»¬ä¹Ÿå¯èƒ½è¶³å¤Ÿå¤§ä»¥æŒ‘æˆ˜è®¡ç®—åŸºç¡€è®¾æ–½ï¼ˆæƒ³è±¡ä¸€ä¸‹Word2Vecæˆ–åŸºäºBERTçš„è¡¨ç¤ºï¼‰ï¼Œä½†ä»ç„¶åŒ…å«ä¸°å¯Œä¸”æœ‰ç”¨çš„ä¿¡æ¯ï¼Œå¦‚å¥æ³•ã€è¯­ä¹‰æˆ–å½¢æ€å…³ç³»ã€‚
- en: They generalize relationships between textual elements better which is clearly
    demonstrated by the success of word2vec algorithms
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å®ƒä»¬å¯¹æ–‡æœ¬å…ƒç´ ä¹‹é—´å…³ç³»çš„æ¦‚æ‹¬èƒ½åŠ›æ›´å¼ºï¼Œè¿™ä¸€ç‚¹é€šè¿‡word2vecç®—æ³•çš„æˆåŠŸå¾—åˆ°äº†æ˜ç¡®è¯æ˜ã€‚
- en: 'Common NLP techniques for transforming sparse to dense representations:'
  id: totrans-63
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å°†ç¨€ç–è¡¨ç¤ºè½¬æ¢ä¸ºç¨ å¯†è¡¨ç¤ºçš„å¸¸è§è‡ªç„¶è¯­è¨€å¤„ç†æŠ€æœ¯ï¼š
- en: '[**Word2Vec**](https://radimrehurek.com/gensim/auto_examples/tutorials/run_word2vec.html):
    It is one of the most popular schools of algorithms that learns dense representation
    using shallow Neural Networks (NN) while trying to predict the probable word(s)
    and capture semantic relations.'
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[**Word2Vec**](https://radimrehurek.com/gensim/auto_examples/tutorials/run_word2vec.html):
    è¿™æ˜¯æœ€å—æ¬¢è¿çš„ç®—æ³•ä¹‹ä¸€ï¼Œä½¿ç”¨æµ…å±‚ç¥ç»ç½‘ç»œå­¦ä¹ ç¨ å¯†è¡¨ç¤ºï¼ŒåŒæ—¶å°è¯•é¢„æµ‹å¯èƒ½çš„è¯æ±‡å¹¶æ•æ‰è¯­ä¹‰å…³ç³»ã€‚'
- en: '[**FastText**](https://fasttext.cc/): Another algorithm with the same objective
    using shallow NNs, except that FastText character-level n-grams. However, Word2Vec
    has been noted to work best for English but FastText is better for morphologically
    rich languages like Arabic, German, and Russian. Besides, it captures syntactic
    features better than semantic ones.'
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[**FastText**](https://fasttext.cc/): å¦ä¸€ç§ä½¿ç”¨æµ…å±‚ç¥ç»ç½‘ç»œå®ç°ç›¸åŒç›®æ ‡çš„ç®—æ³•ï¼Œåªæ˜¯FastTextä½¿ç”¨äº†å­—ç¬¦çº§n-gramsã€‚ç„¶è€Œï¼ŒWord2Vecè¢«è®¤ä¸ºå¯¹è‹±è¯­æ•ˆæœæœ€å¥½ï¼Œä½†FastTextå¯¹å½¢æ€ä¸°å¯Œçš„è¯­è¨€å¦‚é˜¿æ‹‰ä¼¯è¯­ã€å¾·è¯­å’Œä¿„è¯­æ›´ä¸ºé€‚ç”¨ã€‚æ­¤å¤–ï¼Œå®ƒåœ¨æ•æ‰å¥æ³•ç‰¹å¾æ–¹é¢ä¼˜äºè¯­ä¹‰ç‰¹å¾ã€‚'
- en: '[**GloVe**](https://nlp.stanford.edu/projects/glove/): Again same! Learns dense
    representation but based on probability of co-occurrence.'
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[**GloVe**](https://nlp.stanford.edu/projects/glove/): åŒæ ·ï¼å­¦ä¹ å¯†é›†è¡¨ç¤ºï¼Œä½†åŸºäºå…±ç°çš„æ¦‚ç‡ã€‚'
- en: Here is a compact demo of how to obtain dense representations using [Gensimâ€™s
    word2vec](https://radimrehurek.com/gensim/auto_examples/tutorials/run_word2vec.html)
    algorithm.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯ä¸€ä¸ªç®€æ˜çš„æ¼”ç¤ºï¼Œå±•ç¤ºå¦‚ä½•ä½¿ç”¨[**Gensimçš„word2vec**](https://radimrehurek.com/gensim/auto_examples/tutorials/run_word2vec.html)ç®—æ³•è·å–å¯†é›†è¡¨ç¤ºã€‚
- en: Below is the dense representation of the word â€˜demonstrationâ€™ of length 10 (note
    the value of *vector_size* argument for *word2vec* model is 10). Note how there
    are no zeros in this numeric representation.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: ä»¥ä¸‹æ˜¯è¯æ±‡â€˜demonstrationâ€™çš„é•¿åº¦ä¸º10çš„å¯†é›†è¡¨ç¤ºï¼ˆè¯·æ³¨æ„*vector_size*å‚æ•°å¯¹äº*word2vec*æ¨¡å‹çš„å€¼æ˜¯10ï¼‰ã€‚æ³¨æ„æ­¤æ•°å€¼è¡¨ç¤ºä¸­æ²¡æœ‰é›¶ã€‚
- en: '![](../Images/ee568a02c0a6725fdbadd9a3e87f1a71.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ee568a02c0a6725fdbadd9a3e87f1a71.png)'
- en: 'Image Source: Author'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ç‰‡æ¥æºï¼šä½œè€…
- en: Usually, the higher the vector size, the better the knowledge captured, especially
    semantic information. These are extremely useful for assessing text similarities
    and in unsupervised techniques for text processing.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: é€šå¸¸ï¼Œå‘é‡çš„å°ºå¯¸è¶Šå¤§ï¼Œæ•æ‰çš„çŸ¥è¯†å°±è¶Šå¥½ï¼Œç‰¹åˆ«æ˜¯è¯­ä¹‰ä¿¡æ¯ã€‚è¿™äº›å¯¹è¯„ä¼°æ–‡æœ¬ç›¸ä¼¼æ€§å’Œåœ¨æ— ç›‘ç£çš„æ–‡æœ¬å¤„ç†æŠ€æœ¯ä¸­éå¸¸æœ‰ç”¨ã€‚
- en: 'Also, below is a snapshot of the dense vectors obtained for each word in our
    first document â€œ*a demonstration of a product or techniqueâ€* appended in a list,
    corresponding to the sequence of occurrence within the document*.* This time I
    did not clean and kept the texts as is, hence there are seven word-vectors in
    the list:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤å¤–ï¼Œä»¥ä¸‹æ˜¯æˆ‘ä»¬ç¬¬ä¸€ä¸ªæ–‡æ¡£â€œ*äº§å“æˆ–æŠ€æœ¯çš„æ¼”ç¤º*â€ä¸­æ¯ä¸ªå•è¯è·å¾—çš„å¯†é›†å‘é‡çš„å¿«ç…§ï¼Œè¿™äº›å‘é‡æŒ‰æ–‡æ¡£ä¸­å‡ºç°çš„é¡ºåºæ’åˆ—ã€‚è¿™æ¬¡æˆ‘æ²¡æœ‰æ¸…ç†æ–‡æœ¬ï¼Œè€Œæ˜¯ä¿æŒåŸæ ·ï¼Œå› æ­¤åˆ—è¡¨ä¸­æœ‰ä¸ƒä¸ªè¯å‘é‡ï¼š
- en: '![](../Images/89598c769571754a487e49dc67430f96.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/89598c769571754a487e49dc67430f96.png)'
- en: 'Dense vectors of length 10 were obtained for the document â€˜*a demonstration
    of a product or techniqueâ€™ showing that this numeric representation contains no
    non-zero elements |* Image Source: Author'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹æ–‡æ¡£â€˜*äº§å“æˆ–æŠ€æœ¯çš„æ¼”ç¤º*â€™è·å¾—çš„é•¿åº¦ä¸º10çš„å¯†é›†å‘é‡æ˜¾ç¤ºè¯¥æ•°å€¼è¡¨ç¤ºä¸­æ²¡æœ‰éé›¶å…ƒç´  |* å›¾ç‰‡æ¥æºï¼šä½œè€…
- en: '**Dense vectors like these could also be pre-trained** on a large corpus, usually
    made available to be accessed online. Imagine a dictionary with, as usual, an
    index of all the unique words but their lexical meanings are replaced by pre-trained
    word vectors containing their numeric representations, quite like a **â€œQuantified
    Dictionaryâ€**. Here are two popular â€œQuantified Dictionariesâ€ ready to be downloaded
    and applied to text processing tasks:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '**åƒè¿™æ ·çš„å¯†é›†å‘é‡ä¹Ÿå¯ä»¥åœ¨å¤§å‹è¯­æ–™åº“ä¸Šè¿›è¡Œé¢„è®­ç»ƒ**ï¼Œè¿™äº›è¯­æ–™åº“é€šå¸¸å¯ä»¥åœ¨çº¿è®¿é—®ã€‚æƒ³è±¡ä¸€ä¸‹ä¸€ä¸ªå­—å…¸ï¼Œåƒå¾€å¸¸ä¸€æ ·åˆ—å‡ºæ‰€æœ‰å”¯ä¸€çš„è¯æ±‡ï¼Œä½†å…¶è¯æ±‡æ„ä¹‰è¢«åŒ…å«å…¶æ•°å€¼è¡¨ç¤ºçš„é¢„è®­ç»ƒè¯å‘é‡æ›¿ä»£ï¼Œé¢‡ä¼¼ä¸€ä¸ª**â€œé‡åŒ–å­—å…¸â€**ã€‚è¿™é‡Œæœ‰ä¸¤ä¸ªæµè¡Œçš„â€œé‡åŒ–å­—å…¸â€å¯ä»¥ä¸‹è½½å¹¶åº”ç”¨äºæ–‡æœ¬å¤„ç†ä»»åŠ¡ï¼š'
- en: '[**Google**](https://code.google.com/archive/p/word2vec/)**: Trained using
    Google News dataset** containing about 100 billion words. The word vectors of
    size 300 are available to download from [here](https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit?usp=sharing).
    The vocabulary size is 3 million.'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**Google**](https://code.google.com/archive/p/word2vec/)**ï¼šä½¿ç”¨Googleæ–°é—»æ•°æ®é›†è®­ç»ƒ**ï¼ŒåŒ…å«çº¦1000äº¿ä¸ªè¯æ±‡ã€‚300ç»´çš„è¯å‘é‡å¯ä»¥ä»[è¿™é‡Œ](https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit?usp=sharing)ä¸‹è½½ã€‚è¯æ±‡è¡¨çš„å¤§å°ä¸º300ä¸‡ã€‚'
- en: '[**GloVe**](https://nlp.stanford.edu/projects/glove/)**: Trained using Wikipedia
    articles** and contains 50, 100, 300, and 500-dimensional word vectors, ready
    to download from [here](https://nlp.stanford.edu/data/glove.6B.zip). The vocabulary
    size is 400k.'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**GloVe**](https://nlp.stanford.edu/projects/glove/)**ï¼šä½¿ç”¨Wikipediaæ–‡ç« è®­ç»ƒ**ï¼ŒåŒ…å«50ã€100ã€300å’Œ500ç»´çš„è¯å‘é‡ï¼Œå¯ä»¥ä»[è¿™é‡Œ](https://nlp.stanford.edu/data/glove.6B.zip)ä¸‹è½½ã€‚è¯æ±‡è¡¨çš„å¤§å°æ˜¯400kã€‚'
- en: '**Conclusion**'
  id: totrans-78
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**ç»“è®º**'
- en: Traditionally, one-hot vectors, word-frequency matrices (or count vectors),
    and TF-IDF scores (Sparse representations) were used for text analytics. They
    do not preserve any information about semantics. However, the modern approach
    of obtaining word embeddings using Neural Networks(like Word2Vec) or more sophisticated
    statistical approaches with normalization techniques (like GloVe) does a better
    job of retaining the â€œmeaningsâ€ of words and enables us to cluster words occurring
    in similar contexts. But, they also come with complexities in terms of higher
    computation times which makes them expensive to scale (depending on the learning
    hyperparameters, especially with higher vector representation size). Besides,
    they are also harder to explain.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: ä¼ ç»Ÿä¸Šï¼Œä¸€ç»´çƒ­ç¼–ç å‘é‡ã€è¯é¢‘çŸ©é˜µï¼ˆæˆ–è®¡æ•°å‘é‡ï¼‰å’ŒTF-IDFè¯„åˆ†ï¼ˆç¨€ç–è¡¨ç¤ºï¼‰è¢«ç”¨äºæ–‡æœ¬åˆ†æã€‚å®ƒä»¬å¹¶ä¸ä¿ç•™ä»»ä½•è¯­ä¹‰ä¿¡æ¯ã€‚ç„¶è€Œï¼Œç°ä»£æ–¹æ³•é€šè¿‡ç¥ç»ç½‘ç»œï¼ˆå¦‚Word2Vecï¼‰æˆ–æ›´å¤æ‚çš„ç»Ÿè®¡æ–¹æ³•å’Œå½’ä¸€åŒ–æŠ€æœ¯ï¼ˆå¦‚GloVeï¼‰æ¥è·å–è¯åµŒå…¥ï¼Œæ›´å¥½åœ°ä¿ç•™äº†è¯çš„â€œæ„ä¹‰â€ï¼Œå¹¶ä½¿æˆ‘ä»¬èƒ½å¤Ÿå¯¹å‡ºç°äºç±»ä¼¼ä¸Šä¸‹æ–‡ä¸­çš„è¯è¿›è¡Œèšç±»ã€‚ä½†è¿™äº›æ–¹æ³•ä¹Ÿå¸¦æ¥äº†è®¡ç®—æ—¶é—´æ›´é•¿çš„å¤æ‚æ€§ï¼Œè¿™ä½¿å¾—å®ƒä»¬åœ¨æ‰©å±•æ—¶æˆæœ¬è¾ƒé«˜ï¼ˆå–å†³äºå­¦ä¹ è¶…å‚æ•°ï¼Œç‰¹åˆ«æ˜¯æ›´é«˜çš„å‘é‡è¡¨ç¤ºå¤§å°ï¼‰ã€‚æ­¤å¤–ï¼Œå®ƒä»¬ä¹Ÿæ›´éš¾ä»¥è§£é‡Šã€‚
- en: '![](../Images/481dda7eb5f0a946abc59bd48fe1787e.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/481dda7eb5f0a946abc59bd48fe1787e.png)'
- en: Photo by [MichaÅ‚ Parzuchowski](https://unsplash.com/@mparzuchowski?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: ç…§ç‰‡ç”±[MichaÅ‚ Parzuchowski](https://unsplash.com/@mparzuchowski?utm_source=medium&utm_medium=referral)æ‹æ‘„ï¼Œå‘å¸ƒåœ¨[Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)ã€‚
- en: In all real-world cases, both approaches are applicable. For example, in the
    famous [Spam Classification Dataset](https://www.kaggle.com/datasets/uciml/sms-spam-collection-dataset/code),
    sparse representations yield near-perfect model performances. In such cases, we
    do not need to compute dense vector embeddings to achieve better performance since
    we successfully achieve our objectives with simple and transparent approaches.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ‰€æœ‰ç°å®ä¸–ç•Œçš„æ¡ˆä¾‹ä¸­ï¼Œè¿™ä¸¤ç§æ–¹æ³•éƒ½æ˜¯é€‚ç”¨çš„ã€‚ä¾‹å¦‚ï¼Œåœ¨è‘—åçš„[åƒåœ¾é‚®ä»¶åˆ†ç±»æ•°æ®é›†](https://www.kaggle.com/datasets/uciml/sms-spam-collection-dataset/code)ä¸­ï¼Œç¨€ç–è¡¨ç¤ºæ–¹æ³•å¯ä»¥è·å¾—æ¥è¿‘å®Œç¾çš„æ¨¡å‹è¡¨ç°ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬æ— éœ€è®¡ç®—ç¨ å¯†å‘é‡åµŒå…¥å³å¯è·å¾—æ›´å¥½çš„æ€§èƒ½ï¼Œå› ä¸ºæˆ‘ä»¬ç”¨ç®€å•é€æ˜çš„æ–¹æ³•å°±èƒ½æˆåŠŸå®ç°ç›®æ ‡ã€‚
- en: Hence the recommended starting point for a text processing task is with [frequency-based
    Bag of Words](https://en.wikipedia.org/wiki/Bag-of-words_model) models which produce
    sparse vectors. For such pipelines, cleaning and wrangling the text data is pivotal.
    With the right feature and vocabulary engineering, they could be the most efficient
    in terms of speed and performance, especially when semantic information is not
    required.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: å› æ­¤ï¼Œæ¨èçš„æ–‡æœ¬å¤„ç†ä»»åŠ¡èµ·ç‚¹æ˜¯ä½¿ç”¨[åŸºäºé¢‘ç‡çš„è¯è¢‹æ¨¡å‹](https://en.wikipedia.org/wiki/Bag-of-words_model)ï¼Œè¯¥æ¨¡å‹ç”Ÿæˆç¨€ç–å‘é‡ã€‚å¯¹äºè¿™æ ·çš„ç®¡é“ï¼Œæ¸…ç†å’Œæ•´ç†æ–‡æœ¬æ•°æ®è‡³å…³é‡è¦ã€‚é€šè¿‡åˆé€‚çš„ç‰¹å¾å’Œè¯æ±‡å·¥ç¨‹ï¼Œå®ƒä»¬åœ¨é€Ÿåº¦å’Œæ€§èƒ½æ–¹é¢å¯èƒ½æ˜¯æœ€æœ‰æ•ˆçš„ï¼Œç‰¹åˆ«æ˜¯å½“ä¸éœ€è¦è¯­ä¹‰ä¿¡æ¯æ—¶ã€‚
- en: '[**Hereâ€™s the Jupyter Notebook**](https://github.com/royn5618/Medium_Blog_Codes/blob/master/Sparsity_and_Density.ipynb)
    **with the complete Pythonic demonstration of obtaining Sparse and Dense vectors
    (Word2Vec using Gensim) for the same example.**'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '[**è¿™æ˜¯ Jupyter Notebook**](https://github.com/royn5618/Medium_Blog_Codes/blob/master/Sparsity_and_Density.ipynb)
    **ï¼Œå…¶ä¸­åŒ…å«å®Œæ•´çš„ Python æ¼”ç¤ºï¼Œå±•ç¤ºäº†å¦‚ä½•ä¸ºç›¸åŒç¤ºä¾‹è·å–ç¨€ç–å’Œç¨ å¯†å‘é‡ï¼ˆä½¿ç”¨ Gensim çš„ Word2Vecï¼‰ã€‚**'
- en: '**ğŸ’¡ Want to know more about Matrix Designs for NLP? Here is an article for
    you to learn further:**'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '**ğŸ’¡ æƒ³äº†è§£æ›´å¤šå…³äºè‡ªç„¶è¯­è¨€å¤„ç†çš„çŸ©é˜µè®¾è®¡å—ï¼Ÿè¿™é‡Œæœ‰ä¸€ç¯‡æ–‡ç« å¯ä»¥è¿›ä¸€æ­¥å­¦ä¹ ï¼š**'
- en: '[](/matrix-design-for-vector-space-models-in-natural-language-processing-fbef22c10399?source=post_page-----21fcd7a01410--------------------------------)
    [## Matrix Design for Vector Space Models in Natural Language Processing'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/matrix-design-for-vector-space-models-in-natural-language-processing-fbef22c10399?source=post_page-----21fcd7a01410--------------------------------)
    [## è‡ªç„¶è¯­è¨€å¤„ç†ä¸­çš„å‘é‡ç©ºé—´æ¨¡å‹çŸ©é˜µè®¾è®¡'
- en: A brief philosophy of knowledge representation for NLP and a concise guide to
    matrix design for distributed wordâ€¦
  id: totrans-87
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: è‡ªç„¶è¯­è¨€å¤„ç†ä¸­çš„çŸ¥è¯†è¡¨ç¤ºç®€è¦å“²å­¦å’Œåˆ†å¸ƒå¼è¯çŸ©é˜µè®¾è®¡çš„ç®€æ˜æŒ‡å—â€¦â€¦
- en: towardsdatascience.com](/matrix-design-for-vector-space-models-in-natural-language-processing-fbef22c10399?source=post_page-----21fcd7a01410--------------------------------)
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/matrix-design-for-vector-space-models-in-natural-language-processing-fbef22c10399?source=post_page-----21fcd7a01410--------------------------------)
- en: '**ğŸ’¡Want to implement Word2Vec for Text Classification? Here is a hands-on tutorial
    on Multiclass Text Classification by learning dense representations using Kerasâ€™s
    Embedding Layer as well as Gensimâ€™s Word2Vec algorithm:**'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '**ğŸ’¡ æƒ³è¦å®ç° Word2Vec ç”¨äºæ–‡æœ¬åˆ†ç±»ï¼Ÿè¿™é‡Œæœ‰ä¸€ä¸ªå…³äºå¤šç±»æ–‡æœ¬åˆ†ç±»çš„å®ç”¨æ•™ç¨‹ï¼Œè®²è§£äº†å¦‚ä½•ä½¿ç”¨ Keras çš„åµŒå…¥å±‚ä»¥åŠ Gensim çš„ Word2Vec
    ç®—æ³•å­¦ä¹ ç¨ å¯†è¡¨ç¤ºï¼š**'
- en: '[](/multiclass-text-classification-using-keras-to-predict-emotions-a-comparison-with-and-without-word-5ef0a5eaa1a0?source=post_page-----21fcd7a01410--------------------------------)
    [## Multiclass Text Classification Using Keras to Predict Emotions: A Comparison
    with and without Wordâ€¦'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/multiclass-text-classification-using-keras-to-predict-emotions-a-comparison-with-and-without-word-5ef0a5eaa1a0?source=post_page-----21fcd7a01410--------------------------------)
    [## ä½¿ç”¨ Keras è¿›è¡Œå¤šç±»æ–‡æœ¬åˆ†ç±»ï¼šæœ‰è¯åµŒå…¥ä¸æ— è¯åµŒå…¥çš„æ¯”è¾ƒâ€¦'
- en: Do word embeddings add value to text classification models? Letâ€™s find out in
    this multiclass prediction task forâ€¦
  id: totrans-91
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: è¯åµŒå…¥æ˜¯å¦ä¸ºæ–‡æœ¬åˆ†ç±»æ¨¡å‹å¢åŠ ä»·å€¼ï¼Ÿè®©æˆ‘ä»¬åœ¨è¿™ä¸ªå¤šç±»é¢„æµ‹ä»»åŠ¡ä¸­æ¥æ¢è®¨ä¸€ä¸‹â€¦
- en: towardsdatascience.com](/multiclass-text-classification-using-keras-to-predict-emotions-a-comparison-with-and-without-word-5ef0a5eaa1a0?source=post_page-----21fcd7a01410--------------------------------)
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/multiclass-text-classification-using-keras-to-predict-emotions-a-comparison-with-and-without-word-5ef0a5eaa1a0?source=post_page-----21fcd7a01410--------------------------------)
- en: '**References:**'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '**å‚è€ƒèµ„æ–™ï¼š**'
- en: '[https://machinelearningmastery.com/sparse-matrices-for-machine-learning/](https://machinelearningmastery.com/sparse-matrices-for-machine-learning/)'
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[https://machinelearningmastery.com/sparse-matrices-for-machine-learning/](https://machinelearningmastery.com/sparse-matrices-for-machine-learning/)'
- en: '[https://kavita-ganesan.com/fasttext-vs-word2vec/#.Y-OP7XbP02w](https://kavita-ganesan.com/fasttext-vs-word2vec/#.Y-OP7XbP02w)'
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[https://kavita-ganesan.com/fasttext-vs-word2vec/#.Y-OP7XbP02w](https://kavita-ganesan.com/fasttext-vs-word2vec/#.Y-OP7XbP02w)'
- en: '*Thanks for visiting!*'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '*æ„Ÿè°¢è®¿é—®ï¼*'
- en: '**My Links:** [Medium](https://medium.com/@nroy0110) | [LinkedIn](https://www.linkedin.com/in/nabanita-roy/)
    | [GitHub](https://github.com/royn5618)'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '**æˆ‘çš„é“¾æ¥ï¼š** [Medium](https://medium.com/@nroy0110) | [LinkedIn](https://www.linkedin.com/in/nabanita-roy/)
    | [GitHub](https://github.com/royn5618)'
