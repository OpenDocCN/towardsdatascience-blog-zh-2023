- en: 'Machine Learning with Expert Models: A Primer'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 专家模型的机器学习：入门指南
- en: 原文：[https://towardsdatascience.com/machine-learning-with-expert-models-a-primer-6c74585f223f](https://towardsdatascience.com/machine-learning-with-expert-models-a-primer-6c74585f223f)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/machine-learning-with-expert-models-a-primer-6c74585f223f](https://towardsdatascience.com/machine-learning-with-expert-models-a-primer-6c74585f223f)
- en: How a decades-old idea enables training outrageously large neural networks today
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数十年前的理念如何使得今天训练极其庞大的神经网络成为可能
- en: '[](https://medium.com/@samuel.flender?source=post_page-----6c74585f223f--------------------------------)[![Samuel
    Flender](../Images/390d82a673de8a8bb11cef66978269b5.png)](https://medium.com/@samuel.flender?source=post_page-----6c74585f223f--------------------------------)[](https://towardsdatascience.com/?source=post_page-----6c74585f223f--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----6c74585f223f--------------------------------)
    [Samuel Flender](https://medium.com/@samuel.flender?source=post_page-----6c74585f223f--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@samuel.flender?source=post_page-----6c74585f223f--------------------------------)[![Samuel
    Flender](../Images/390d82a673de8a8bb11cef66978269b5.png)](https://medium.com/@samuel.flender?source=post_page-----6c74585f223f--------------------------------)[](https://towardsdatascience.com/?source=post_page-----6c74585f223f--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----6c74585f223f--------------------------------)
    [Samuel Flender](https://medium.com/@samuel.flender?source=post_page-----6c74585f223f--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----6c74585f223f--------------------------------)
    ·9 min read·Sep 5, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----6c74585f223f--------------------------------)
    ·9分钟阅读·2023年9月5日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/6589b3c96e9f917a19170e43a71a9d9a.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6589b3c96e9f917a19170e43a71a9d9a.png)'
- en: ([Pexels](https://www.pexels.com/photo/set-of-tool-wrench-162553/))
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: （[Pexels](https://www.pexels.com/photo/set-of-tool-wrench-162553/)）
- en: Expert models are one of the most useful inventions in Machine Learning, yet
    they hardly receive as much attention as they deserve. In fact, expert modeling
    does not only allow us to train neural networks that are “outrageously large”
    (more on that later), they also allow us to build models that learn more like
    the human brain, that is, different regions specialize in different types of input.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 专家模型是机器学习中最有用的发明之一，但它们往往未能获得应有的关注。事实上，专家建模不仅使我们能够训练出“极其庞大”的神经网络（稍后会详细讲述），它们还使我们能够构建出更像人脑的模型，即不同区域专门处理不同类型的输入。
- en: 'In this article, we’ll take a tour of the key innovations in expert modeling
    which ultimately lead to recent breakthroughs such as the Switch Transformer and
    the Expert Choice Routing algorithm. But let’s go back first to the paper that
    started it all: “Mixtures of Experts”.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我们将探讨专家建模的关键创新，这些创新最终导致了如Switch Transformer和Expert Choice Routing算法这样的最新突破。但首先让我们回顾一下所有这一切的起点：
    “专家混合模型”。
- en: Mixtures of Experts (1991)
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 专家混合模型（1991）
- en: '![](../Images/ee84508a45288fb4a059d3aeeee05161.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ee84508a45288fb4a059d3aeeee05161.png)'
- en: 'The original MoE model from 1991\. Image credit: [Jabocs et al 1991, Adaptive
    Mixtures of Local Experts](https://www.cs.toronto.edu/~hinton/absps/jjnh91.pdf).'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 1991年的原始MoE模型。图片来源：[Jabocs et al 1991, Adaptive Mixtures of Local Experts](https://www.cs.toronto.edu/~hinton/absps/jjnh91.pdf)。
- en: 'The idea of mixtures of experts (MoE) traces back more than 3 decades ago,
    to a 1991 [paper](https://www.cs.toronto.edu/~hinton/absps/jjnh91.pdf) co-authored
    by none other than the godfather of AI, Geoffrey Hinton. The key idea in MoE is
    to model an output “y” by combining a number of “experts” E, the weight of each
    is being controlled by a “gating network” G:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 专家混合模型（MoE）的理念可以追溯到三十多年前，源于1991年由人工智能奠基人**Geoffrey Hinton**共同撰写的[论文](https://www.cs.toronto.edu/~hinton/absps/jjnh91.pdf)。MoE的核心思想是通过结合多个“专家”E来建模输出“y”，每个专家的权重由“门控网络”G控制：
- en: '![](../Images/22bac53e59837ac67d6b84d8f758bb9f.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/22bac53e59837ac67d6b84d8f758bb9f.png)'
- en: An expert in this context can be any kind of model, but is usually chosen to
    be a multi-layered neural network, and the gating network is
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个上下文中，专家可以是任何类型的模型，但通常选择的是多层神经网络，门控网络是
- en: '![](../Images/f1d03c869186766e071ffdf2da9fe361.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f1d03c869186766e071ffdf2da9fe361.png)'
- en: 'where W is a learnable matrix that assigns training examples to experts. When
    training MoE models, the learning objective is therefore two-fold:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 其中W是一个可学习的矩阵，用于将训练样本分配给专家。因此，训练MoE模型的学习目标有两个方面：
- en: the experts will learn to process the output they’re given into the best possible
    output (i.e., a prediction), and
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 专家将学习处理他们所得到的输出，生成最佳输出（即预测），并且
- en: the gating network will learn to “route” the right training examples to the
    right experts, by jointly learning the routing matrix W.
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 门控网络将学习“路由”正确的训练样本到正确的专家，通过联合学习路由矩阵W。
- en: 'Why should one do this? And why does it work? At a high level, there are three
    main motivations for using such an approach:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么要这样做？为什么有效？从高层次来看，使用这种方法有三个主要动机：
- en: First, MoE allows scaling neural networks to very large sizes due to the sparsity
    of the resulting model, that is, even though the overall model is large, only
    a small amount of computation needs to be executed for any given training example
    because of the presence of highly specialized experts. This approach stands in
    contrast to the standard “dense” neural networks, in which every single neuron
    is needed for every single input example.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，MoE 由于模型的稀疏性允许将神经网络扩展到非常大的规模，即使整体模型很大，由于存在高度专业化的专家，对于任何给定的训练样本，只需执行少量计算。这种方法与标准的“密集型”神经网络形成对比，在标准模型中，每个输入样本都需要每一个神经元。
- en: Second, MoE allows for better model generalization with less risk of overfitting
    because each individual expert can be a relatively small neural network, yet we
    still achieve strong overall model performance by adding more experts. Much like
    boosting, it’s a way to combine a large number of relatively weak learners into
    a single, strong learner.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，MoE 允许更好的模型泛化，降低过拟合风险，因为每个单独的专家可以是一个相对较小的神经网络，但通过添加更多专家，我们仍然能实现强大的整体模型性能。类似于提升方法，这是一种将大量相对较弱的学习器结合成一个强大的学习器的方法。
- en: 'And third, MoE mimics more closely how our brains work: any given input only
    activates certain regions in our brains, with distinct regions for touch, vision,
    sound, smell, taste, orientation, and so on. All of these are, well, “experts”.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 第三，MoE 更接近我们大脑的工作方式：任何给定的输入只会激活我们大脑中的某些区域，不同区域处理触觉、视觉、听觉、嗅觉、味觉、方向感等。这些区域都可以看作是“专家”。
- en: In short, MoE frees us from the requirement that every single neuron needs to
    be activated for every single input. MoE models are sparse, highly flexible, and
    extremely powerful.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，MoE 使我们不再需要对每个输入激活每一个神经元。MoE 模型稀疏、高度灵活且极其强大。
- en: Outrageously large neural networks (2017)
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**极其庞大的神经网络**（2017）'
- en: 'Fast-forward 26 (!) years to the highly influential paper “[Outrageously large
    neural networks](https://arxiv.org/pdf/1701.06538.pdf)”, again from Hinton’s team
    (this time at Google Brain). In this work, the authors take MoE to its limits,
    presenting a 6 Billion parameter MoE model with thousands of experts. In order
    to build such an outrageously large MoE model, the authors introduce several modeling
    tricks, including:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 快进 26 年到影响深远的论文 “[极其庞大的神经网络](https://arxiv.org/pdf/1701.06538.pdf)”，同样来自 Hinton
    的团队（这次在 Google Brain）。在这项工作中，作者将 MoE 推向极限，展示了一个具有千名专家的 60 亿参数 MoE 模型。为了构建如此庞大的
    MoE 模型，作者引入了几种建模技巧，包括：
- en: '**Noisy top-k gating.** Top-k gating simply means that for each training example
    we only compute the expert output from the top k experts (which are determined
    by the gate), and ignore all of the other experts. The main motivation is saved
    computational cost: for example, if we have 20 experts and apply top-k gating
    with k=5, we’d cut down the total computational cost of training the model by
    a factor of 4!'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '**噪声 top-k 门控。** Top-k 门控意味着对于每个训练样本，我们只计算 top k 专家的输出（由门控确定），忽略其他所有专家。主要动机是节省计算成本：例如，如果我们有20个专家并应用
    k=5 的 top-k 门控，我们可以将模型训练的总计算成本减少一个数量级的4倍！'
- en: “Noisy” means that we’re adding tunable Gaussian random noise to the gating
    values. The authors find that this help with load balancing, that is, making sure
    an entire batch of data is not being sent to a single expert.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: “噪声”意味着我们在门控值中添加可调的高斯随机噪声。作者发现这有助于负载均衡，即确保整批数据不会全部发送到单一专家。
- en: '**Auxiliary losses.** The authors add two auxiliary (regularizing) loss terms
    to the model’s loss function, “load balancing loss” and “expert diversity loss”,
    each with its own tunable regularization parameter:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '**辅助损失。** 作者将两个辅助（正则化）损失项添加到模型的损失函数中，即“负载均衡损失”和“专家多样性损失”，每个都有其自身可调的正则化参数：'
- en: Load balancing loss is proportional to the coefficient of variation in the number
    of training examples received by each expert. Adding this loss improves computational
    performance because it prevents the introduction of “expert bottlenecks” where
    all training examples have to pass through one expert. (One subtlety is that the
    number of training examples per expert is not differentiable — so the authors
    use a smooth approximation of that number instead.)
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 负载均衡损失与每个专家接收的训练样本数量的变异系数成正比。增加这个损失可以提高计算性能，因为它防止了“专家瓶颈”的出现，即所有训练样本必须通过一个专家。（一个细节是每个专家的训练样本数量不可微分——因此作者使用了这个数量的平滑近似值。）
- en: Expert diversity loss is proportional to the coefficient of variation of expert
    importances, where expert importance is defined as the sum of the gate values
    for that expert. Adding this loss nudges the model to utilize all of the experts
    equally, instead of simply sending all training examples to a single expert that
    learns everything, which is a common problem and a local minimum in training MoE
    models.
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 专家多样性损失与专家重要性的变异系数成正比，其中专家重要性定义为该专家的门值之和。增加这个损失可以促使模型平等地利用所有专家，而不是简单地将所有训练样本发送给一个学习所有内容的专家，这在训练MoE模型中是一个常见问题，也是一个局部最小值。
- en: While these two loss terms are similar, the authors find the best overall performance
    when adding both load balancing loss and expert diversity loss, both with a regularization
    parameter of 0.1.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这两个损失项相似，作者发现当同时添加负载均衡损失和专家多样性损失，并且两个损失项的正则化参数为0.1时，整体性能最佳。
- en: '**Customized parallelism.** The authors show that large MoE models benefit
    from a customized combination of [data parallelism and model parallelism](/distributed-learning-a-primer-790812b817f1):
    in particular, we can allow for experts to be distributed across machines because
    we don’t require communication between them, while at the same time using data
    parallelism in order to increase the batch size. This form of parallelism allows
    us to massively scale up the MoE model: in their experiments, the authors scale
    it up to 6 Billion parameters with thousands of experts.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '**定制化并行性。** 作者展示了大型MoE模型如何从[数据并行和模型并行的定制组合](/distributed-learning-a-primer-790812b817f1)中受益：特别是，我们可以允许专家分布在不同的机器上，因为我们不需要它们之间的通信，同时使用数据并行性来增加批处理大小。这种并行形式使我们能够大幅扩展MoE模型：在他们的实验中，作者将模型扩展到60亿参数，配备数千个专家。'
- en: Using their massive MoE model, the authors establish a new SOTA on the Billion-words
    language modeling benchmark.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 使用他们的大规模MoE模型，作者在Billion-words语言建模基准上建立了新的SOTA（最先进技术）。
- en: Switch Transformers (2022)
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Switch Transformers (2022)
- en: '![](../Images/b3d1e3e8781ff387be9ab7914d8cb929.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b3d1e3e8781ff387be9ab7914d8cb929.png)'
- en: '7x faster training with the Switch Transformer. Image credit: [Fedus et al
    2022](https://arxiv.org/pdf/2101.03961.pdf)'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Switch Transformer进行7倍更快的训练。图片来源：[Fedus et al 2022](https://arxiv.org/pdf/2101.03961.pdf)
- en: While “Outrageously large neural networks” demonstrated the usefulness of top-k
    gating in MoE models, [Switch Transformers](https://arxiv.org/pdf/2101.03961.pdf),
    also by Google, brought it to its extreme conclusion by building MoE Transformer
    models with k=1, that is, each training example is only being sent to a single
    expert. The authors call this “hard routing”, in contrast to the “soft routing”
    in standard MoE models where the output from multiple experts is being pooled.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然“极大规模神经网络”展示了top-k门控在MoE模型中的有效性，[Switch Transformers](https://arxiv.org/pdf/2101.03961.pdf)同样由Google提出，通过构建k=1的MoE
    Transformer模型将其推向了极限，即每个训练样本仅被发送到一个专家。作者称之为“硬路由”，与标准MoE模型中的“软路由”形成对比，在标准MoE模型中，来自多个专家的输出被汇聚。
- en: 'Practically, hard routing (k=1) means that we can have N>1 experts, with *any*
    number N, and constant computational complexity: the model capacity scales as
    O(1)! The only trade-off is that we need a huge amount of memory to store all
    of the experts. However, since we require no communication between experts, that
    memory can be distributed across a large cluster of machines.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，硬路由（k=1）意味着我们可以拥有多个专家，数量N可以为*任何*数值，并且计算复杂度保持不变：模型容量按O(1)扩展！唯一的折衷是我们需要大量的内存来存储所有专家。然而，由于我们不要求专家之间的通信，这些内存可以分布在一个大型机器集群中。
- en: 'In addition, the authors introduce a number of practical modeling tricks, including:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，作者还介绍了许多实用的建模技巧，包括：
- en: '**precision casting**: this means that we send weights in between machines
    in BF16 (16-bit [brain float](https://cloud.google.com/blog/products/ai-machine-learning/bfloat16-the-secret-to-high-performance-on-cloud-tpus))
    but cast them to FP32 (32-bit floating point precision) when computing the gating
    values. This trick minimized communication overhead because we only need to communicate
    16 instead of 32 bits, while at the same time ensuring that the softmax computation
    is stable (which it isn’t with 16 bits).'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**精确投射**：这意味着我们在机器之间传送权重时使用BF16（16位[brain float](https://cloud.google.com/blog/products/ai-machine-learning/bfloat16-the-secret-to-high-performance-on-cloud-tpus)），但在计算门控值时将其转换为FP32（32位浮点精度）。这一技巧减少了通信开销，因为我们只需传输16位而非32位，同时确保了softmax计算的稳定性（16位下不稳定）。'
- en: '**aggressive expert dropout**: the authors find that they can improve the performance
    of the model by applying aggressive dropout of 0.4 within the expert modules,
    while keeping dropout in the rest of the model architecture at a smaller rate
    of 0.1\. The reasoning is that experts overfit more easily because they only see
    a fraction of the data, and therefore need to be regularized more heavily.'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**激进的专家丢弃**：作者发现，通过在专家模块中应用0.4的激进丢弃，同时在模型架构的其他部分保持0.1的丢弃率，可以提高模型性能。原因是专家容易过拟合，因为它们仅看到一部分数据，因此需要更强的正则化。'
- en: '**scaled expert initialization**: the authors find much better training stability
    when scaling the initialization of the expert layers down by a factor of 10.'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**缩放的专家初始化**：作者发现，当将专家层的初始化缩小10倍时，训练稳定性显著提高。'
- en: Ultimately, the authors build a Switch Transformer based off of Google’s T5
    language model and obtain a 7x increase in pre-training speed with the same computational
    resources, a powerful demonstration of the modeling improvements we can get by
    combining hard-routing based MoE with Transformer models.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，作者基于Google的T5语言模型构建了一个Switch Transformer，并在相同计算资源下实现了7倍的预训练速度提升，这强有力地展示了通过将基于硬路由的MoE与Transformer模型结合所能带来的建模改进。
- en: Expert Choice Routing (2022)
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 专家选择路由（2022）
- en: '![](../Images/45da8d3241e0d84578938532e41ce163.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/45da8d3241e0d84578938532e41ce163.png)'
- en: 'Standard token-choice routing (left) compared to the new expert-choice routing
    algorithm (right). Note that in expert choice routing, a single token can be routed
    to multiple experts at the same time. Image credit: [Zhou et al 2022](https://arxiv.org/abs/2202.09368)'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 标准的令牌选择路由（左）与新的专家选择路由算法（右）相比。请注意，在专家选择路由中，一个令牌可以同时路由到多个专家。图像来源：[Zhou et al 2022](https://arxiv.org/abs/2202.09368)
- en: One of the most recent breakthroughs in expert modeling is “[Expert choice routing](https://arxiv.org/abs/2202.09368)”,
    yet another innovation by Google.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 近期在专家建模领域的突破之一是“[专家选择路由](https://arxiv.org/abs/2202.09368)”，这是Google的又一创新。
- en: 'The problem with training MoE models is that experts often remain under-utilized
    because of the “winner-take-all” effect, where a single expert that randomly receives
    the first few training examples rapidly becomes much better than the other experts
    and ends up dominating the gate. So far, the standard practice has been to add
    auxiliary losses which nudge the model to utilize all of the experts equally (such
    as expert diversity loss in Hinton’s work). However, adding auxiliary losses introduces
    the problem of tuning their regularization parameters relative to the actual loss
    we want to minimize: too small, and there’s no change in the model behavior at
    all, too large, and we risk making all experts identical (another local minimum).'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 训练MoE模型的问题在于专家通常由于“赢家通吃”效应而未被充分利用，其中一个随机接收到前几个训练样本的专家迅速变得比其他专家更优秀并主导门控。到目前为止，标准做法是添加辅助损失，以促使模型平等利用所有专家（如Hinton工作中的专家多样性损失）。然而，添加辅助损失带来了调整其正则化参数相对于实际损失的难题：太小，则模型行为无变化；太大，则风险使所有专家相同（另一个局部最小值）。
- en: 'The key idea in “expert choice routing” is simple yet powerful: let expert
    choose their top k training examples within a batch, instead of having a training
    example choose its top k experts. This has several advantages:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: “专家选择路由”的关键思想简单却强大：让专家在批次中选择其最优的k个训练样本，而不是让训练样本选择其最优的k个专家。这有几个优点：
- en: it guarantees perfect load balancing with equal expert utilization (each expert
    *always* gets the same number of training examples in each batch),
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它保证了负载均衡和专家利用率的完美一致（每个专家*总是*在每个批次中接收相同数量的训练样本），
- en: 'not every training example ends up with the same number of experts, some of
    them may be routed to 1 experts, and some of them to all experts. This is a desired
    property: different training examples represent different degrees of difficulty
    for the model, and consequently may require a different number of experts,'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 并非每个训练样本都有相同数量的专家，有些可能被分配给1位专家，有些则分配给所有专家。这是一种期望的特性：不同的训练样本对模型而言难度不同，因此可能需要不同数量的专家。
- en: no need for adhoc auxiliary losses that need to be tuned.
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不需要调整的额外辅助损失。
- en: Mathematically, expert choice routing replaces the standard gating function
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 从数学上讲，专家选择路由替代了标准的门控函数。
- en: '![](../Images/629e1928f81ac1ab4fe6518224904ef0.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/629e1928f81ac1ab4fe6518224904ef0.png)'
- en: with
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 与
- en: '![](../Images/2f0640c537b330ab652b38037fe5b352.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2f0640c537b330ab652b38037fe5b352.png)'
- en: that is, instead of an e-dimensional vector, the gate G is now a matrix with
    dimensions e (the number of experts) times n (the number of tokens). Why tokens?
    The authors consider expert choice routing in the context of NLP problems, hence
    each training example is simply a sequence of n possible tokens.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 也就是说，与e维向量不同，门控G现在是一个维度为e（专家数量）乘以n（令牌数量）的矩阵。为什么是令牌？作者将专家选择路由考虑在NLP问题的背景下，因此每个训练样本只是n个可能令牌的序列。
- en: So far for the theory (which really boils down to transforming G from a vector
    into a matrix), but how well does it work in practice? The authors show that they
    can achieve the same performance as the Switch Transformer in half the training
    time, and for the same computational cost, they beat the Switch Transformer on
    11 tasks from the GLUE and SuperGLUE benchmarks.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，这些理论（实际上就是将G从向量转换为矩阵），但它在实践中表现如何？作者展示了他们可以在一半的训练时间内达到与**Switch Transformer**相同的性能，并且在相同的计算成本下，在GLUE和SuperGLUE基准测试的11项任务上超越了**Switch
    Transformer**。
- en: Expert Choice Routing beat the Switch Transformer, which proved that, at least
    for NLP problems, “expert choice” is superior to “token choice”.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '**专家选择路由**击败了**Switch Transformer**，这证明了至少对于NLP问题而言，“专家选择”优于“令牌选择”。'
- en: Summary
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: 'A quick recap:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 简要回顾：
- en: the key idea in MoE models is to model the output y as a weighted sum of experts,
    where each expert is simply a small neural network itself, and the weights are
    determined by G(x) = softmax(Wx), where W is a learnable matrix.
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MoE模型中的关键思想是将输出y建模为专家的加权和，其中每个专家本身就是一个小型神经网络，权重由G(x) = softmax(Wx)决定，其中W是一个可学习的矩阵。
- en: top-k gating means that we ignore all experts except for the top k. This saves
    considerable computational cost, and was an important breakthrough in expert modeling.
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: top-k门控意味着我们忽略所有专家，只关注前k个。这节省了大量计算成本，并且是专家建模中的一个重要突破。
- en: MoE models often get stuck in a local minimum where all training examples are
    simply being sent to a single expert. A remedy is to add “expert diversity loss”
    to the model’s loss function.
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MoE模型经常陷入局部最小值，即所有训练样本都被送往单个专家。解决方法是向模型的损失函数中添加“专家多样性损失”。
- en: Switch Transformers combined the idea of MoE with Transformer models, and showed
    that this combination allows training the T5 language model 7x faster. A key innovation
    here is that of “hard routing”, which is simply top-k routing with k=1.
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Switch Transformers**将MoE的概念与**Transformer**模型结合，并展示了这种组合可以将T5语言模型的训练速度提高7倍。这里的一个关键创新是“硬路由”，即k=1的top-k路由。'
- en: Expert Choice Routing replaced the idea of training examples choosing their
    experts with experts choosing their training examples. This allows for better
    training stability without the introduction of adhoc auxiliary losses.
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**专家选择路由**用专家选择训练样本的概念取代了训练样本选择专家的概念。这允许更好的训练稳定性而无需引入额外的辅助损失。'
- en: 'And this is just the tip of the iceberg. In fact, there’s a wealth of new papers
    on expert modeling that has been sparked by the success of Hinton’s “outrageously
    large” MoE model, the Switch Transformer, and the new expert choice routing algorithm:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 这仅仅是冰山一角。事实上，受到**Hinton**“极大规模”MoE模型、**Switch Transformer**和新专家选择路由算法成功的启发，专家建模领域涌现了大量新的论文。
- en: '![](../Images/6bc1c533d38fc68d148ffdc1efbeb8ed.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6bc1c533d38fc68d148ffdc1efbeb8ed.png)'
- en: 'Cluster of research papers on MoE models, with the “Expert Choice” paper highlighted
    in red, Source: [ConnectedPapers](https://www.connectedpapers.com/main/bbc57e1b3cf90e09b64377f13de455793bc81ad5/Mixture%20of%20Experts-with-Expert-Choice-Routing/graph).'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 关于MoE模型的研究论文集，其中“专家选择”论文用红色突出显示，来源：[ConnectedPapers](https://www.connectedpapers.com/main/bbc57e1b3cf90e09b64377f13de455793bc81ad5/Mixture%20of%20Experts-with-Expert-Choice-Routing/graph)。
- en: Expert modeling is an exciting domain that has been decades in the making, and
    we’re just starting to see its impact on modern Machine Learning applications.
    Watch this space — new breakthroughs are certainly on the horizon!
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 专家建模是一个令人兴奋的领域，经过数十年的发展，我们才刚刚开始看到它对现代机器学习应用的影响。请关注这个领域——新的突破无疑在即！
