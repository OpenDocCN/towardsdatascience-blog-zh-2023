- en: Four LLM trends since ChatGPT and their implications for AI builders
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自ChatGPT以来的四个LLM趋势及其对AI构建者的影响
- en: 原文：[https://towardsdatascience.com/four-llm-trends-since-chatgpt-and-their-implications-for-ai-builders-a140329fc0d2?source=collection_archive---------2-----------------------#2023-05-29](https://towardsdatascience.com/four-llm-trends-since-chatgpt-and-their-implications-for-ai-builders-a140329fc0d2?source=collection_archive---------2-----------------------#2023-05-29)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/four-llm-trends-since-chatgpt-and-their-implications-for-ai-builders-a140329fc0d2?source=collection_archive---------2-----------------------#2023-05-29](https://towardsdatascience.com/four-llm-trends-since-chatgpt-and-their-implications-for-ai-builders-a140329fc0d2?source=collection_archive---------2-----------------------#2023-05-29)
- en: '[](https://medium.com/@janna.lipenkova_52659?source=post_page-----a140329fc0d2--------------------------------)[![Dr.
    Janna Lipenkova](../Images/112fe9a8c5936869243f2a43fde6dfee.png)](https://medium.com/@janna.lipenkova_52659?source=post_page-----a140329fc0d2--------------------------------)[](https://towardsdatascience.com/?source=post_page-----a140329fc0d2--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----a140329fc0d2--------------------------------)
    [Dr. Janna Lipenkova](https://medium.com/@janna.lipenkova_52659?source=post_page-----a140329fc0d2--------------------------------)'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@janna.lipenkova_52659?source=post_page-----a140329fc0d2--------------------------------)[![Dr.
    Janna Lipenkova](../Images/112fe9a8c5936869243f2a43fde6dfee.png)](https://medium.com/@janna.lipenkova_52659?source=post_page-----a140329fc0d2--------------------------------)[](https://towardsdatascience.com/?source=post_page-----a140329fc0d2--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----a140329fc0d2--------------------------------)
    [Dr. Janna Lipenkova](https://medium.com/@janna.lipenkova_52659?source=post_page-----a140329fc0d2--------------------------------)'
- en: ·
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ff215f8e427a2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffour-llm-trends-since-chatgpt-and-their-implications-for-ai-builders-a140329fc0d2&user=Dr.+Janna+Lipenkova&userId=f215f8e427a2&source=post_page-f215f8e427a2----a140329fc0d2---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----a140329fc0d2--------------------------------)
    ·15 min read·May 29, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fa140329fc0d2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffour-llm-trends-since-chatgpt-and-their-implications-for-ai-builders-a140329fc0d2&user=Dr.+Janna+Lipenkova&userId=f215f8e427a2&source=-----a140329fc0d2---------------------clap_footer-----------)'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ff215f8e427a2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffour-llm-trends-since-chatgpt-and-their-implications-for-ai-builders-a140329fc0d2&user=Dr.+Janna+Lipenkova&userId=f215f8e427a2&source=post_page-f215f8e427a2----a140329fc0d2---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----a140329fc0d2--------------------------------)
    ·15分钟阅读·2023年5月29日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fa140329fc0d2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffour-llm-trends-since-chatgpt-and-their-implications-for-ai-builders-a140329fc0d2&user=Dr.+Janna+Lipenkova&userId=f215f8e427a2&source=-----a140329fc0d2---------------------clap_footer-----------)'
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fa140329fc0d2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffour-llm-trends-since-chatgpt-and-their-implications-for-ai-builders-a140329fc0d2&source=-----a140329fc0d2---------------------bookmark_footer-----------)![](../Images/50eaefd7c0e07824bb0e6a17a5a93792.png)'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fa140329fc0d2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffour-llm-trends-since-chatgpt-and-their-implications-for-ai-builders-a140329fc0d2&source=-----a140329fc0d2---------------------bookmark_footer-----------)![](../Images/50eaefd7c0e07824bb0e6a17a5a93792.png)'
- en: 'Table 1: Selection of popular LLMs (by mention quantity) as of May 2023\. The
    mentions, trend and suitability for downstream tasks are computed from a dataset
    of more than 500k AI-related online documents which include business media, general
    press, AI blogs and scientific publications. Task suitability was computed using
    semantic embeddings and the latent association strength between models and NLP
    tasks.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 表1：截至2023年5月的热门LLM（按提及数量排序）。提及数量、趋势和下游任务的适用性是从超过50万份与AI相关的在线文档中计算得出的，这些文档包括商业媒体、普通新闻、AI博客和科学出版物。任务适用性通过语义嵌入和模型与NLP任务之间的潜在关联强度进行计算。
- en: In October 2022, I published an [article on LLM selection for specific NLP use
    cases](/choosing-the-right-language-model-for-your-nlp-use-case-1288ef3c4929)
    , such as conversation, translation and summarisation. Since then, AI has made
    a huge step forward, and in this article, we will review some of the trends of
    the past months as well as their implications for AI builders. Specifically, we
    will cover the topics of task selection for autoregressive models, the evolving
    trade-offs between commercial and open-source LLMs, as well as LLM integration
    and the mitigation of failures in production.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 2022年10月，我发布了一篇[关于特定NLP用例的LLM选择的文章](/choosing-the-right-language-model-for-your-nlp-use-case-1288ef3c4929)，涉及对话、翻译和摘要等内容。从那时起，AI已经取得了巨大的进步，在这篇文章中，我们将回顾过去几个月的一些趋势以及它们对AI构建者的影响。具体而言，我们将涵盖自回归模型的任务选择、商业和开源LLMs之间不断变化的权衡，以及LLM的集成和生产中故障的缓解。
- en: 1\. Generative AI pushes autoregressive models, while autoencoding models are
    waiting for their moment.
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 1\. 生成AI推动了自回归模型，而自动编码模型则在等待它们的时机。
- en: 'For many AI companies, it seems like ChatGPT has turned into the ultimate competitor.
    When pitching my analytics startups in earlier days, I would frequently be challenged:
    “what will you do if Google (Facebook, Alibaba, Yandex…) comes around the corner
    and does the same?” Now, the question du jour is: “why can’t you use ChatGPT to
    do this?”'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 对于许多AI公司来说，ChatGPT似乎已成为**终极**竞争者。在早期推介我的分析初创公司时，我经常面临挑战：“如果谷歌（Facebook、阿里巴巴、Yandex等）突然出现并做同样的事情，你会怎么做？”现在，当前的问题是：“为什么你不能用ChatGPT来做这个？”
- en: 'The short answer is: ChatGPT is great for many things, but it does by far not
    cover the full spectrum of AI. The current hype happens explicitly around **generative**
    AI — not analytical AI, or its rather fresh branch of synthetic AI [1]. What does
    this mean for LLMs? As described in my [previous article](https://medium.com/towards-data-science/choosing-the-right-language-model-for-your-nlp-use-case-1288ef3c4929),
    LLMs can be pre-trained with three objectives — autoregression, autoencoding and
    sequence-to-sequence (cf. also Table 1, column “Pre-training objective”). Typically,
    a model is pre-trained with one of these objectives, but there are exceptions
    — for example, UniLM [2] was pre-trained on all three objectives. The fun generative
    tasks that have popularised AI in the past months are conversation, question answering
    and content generation — those tasks where the model indeed learns to “generate”
    the next token, sentence etc. These are best carried out by autoregressive models,
    which include the GPT family as well as most of the recent open-source models,
    like MPT-7B, OPT and Pythia. Autoencoding models, which are better suited for
    information extraction, distillation and other analytical tasks, are resting in
    the background — but let’s not forget that the initial LLM breakthrough in 2018
    happened with BERT, an autoencoding model. While this might feel like stone age
    for modern AI, autoencoding models are especially relevant for many B2B use cases
    where the focus is on distilling concise insights that address specific business
    tasks. We might indeed witness another wave around autoencoding and a new generation
    of LLMs that excel at extracting and synthesizing information for analytical purposes.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 简短的回答是：ChatGPT在很多方面表现出色，但远未覆盖AI的全部领域。目前的热潮主要集中在**生成**AI上——而不是分析型AI，或者其相对较新的分支——合成AI[1]。这对LLMs意味着什么？正如我在[上一篇文章](https://medium.com/towards-data-science/choosing-the-right-language-model-for-your-nlp-use-case-1288ef3c4929)中描述的，LLMs可以通过三种目标进行预训练——自回归、自动编码和序列到序列（参见表1，列“预训练目标”）。通常，一个模型会以这些目标中的一个进行预训练，但也有例外——例如，UniLM[2]在所有三种目标上进行了预训练。近几个月来使AI流行的有趣生成任务包括对话、问答和内容生成——这些任务中，模型确实学习“生成”下一个标记、句子等。这些任务最适合由自回归模型执行，包括GPT家族以及大多数近期的开源模型，如MPT-7B、OPT和Pythia。自动编码模型更适合信息提取、蒸馏和其他分析任务，虽然它们处于背景中——但不要忘记，初次突破LLM的2018年是通过BERT这款自动编码模型实现的。虽然这对现代AI来说可能感觉像是石器时代，但自动编码模型对于许多B2B用例特别相关，其中重点是提炼出针对特定业务任务的简明见解。我们确实可能会看到围绕自动编码的另一波浪潮，以及一代新的LLMs，在提取和综合信息以进行分析方面表现卓越。
- en: For builders, this means that popular autoregressive models can be used for
    everything that is content generation — and the longer the content, the better.
    However, for analytical tasks, you should carefully evaluate whether the autoregressive
    LLM you use will output a satisfying result, and consider autoencoding models
    or even more traditional NLP methods otherwise.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 对于构建者来说，这意味着流行的自回归模型可以用于所有内容生成的任务——内容越长，效果越好。然而，对于分析任务，你应该仔细评估所使用的自回归大型语言模型（LLM）是否能输出令人满意的结果，并考虑使用自动编码模型或更传统的自然语言处理方法。
- en: 2\. Open-source competes with for-profits, spurring innovation in LLM efficiency
    and scaling.
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 2\. 开源与商业公司的竞争，推动了LLM效率和扩展性的创新。
- en: 'In the past months, there has been a lot of debate about the uneasy relationship
    between open-source and commercial AI. In the short term, the open-source community
    cannot keep up in a race where winning entails a huge spend on data and/or compute.
    But with a long-term perspective in mind, even the big companies like Google and
    OpenAI feel threatened by open-source.[3] Spurred by this tension, both camps
    have continued building, and the resulting advances are eventually converging
    into fruitful synergies. The open-source community has a strong focus on frugality,
    i. e. increasing the efficiency of LLMs by doing more with less. This not only
    makes LLMs affordable to a broader user base — think AI democratisation — but
    also more sustainable from an environmental perspective. There are three principal
    dimensions along which LLMs can become more efficient:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去几个月中，关于开源与商业AI之间的复杂关系有很多讨论。短期内，开源社区无法在需要大量数据和/或计算资源的竞赛中跟上。然而，从长远来看，即使是像谷歌和OpenAI这样的公司也感受到开源的威胁。[3]
    这种紧张关系促使双方继续发展，最终取得的进展逐渐汇聚成富有成果的协同效应。开源社区注重节俭，即通过更少的资源提高LLM的效率。这不仅使LLM对更广泛的用户群体更具负担能力——即AI民主化——而且从环境角度来看也更具可持续性。LLM变得更高效的主要维度有三个：
- en: '**Less compute and memory**: for example, FlashAttention [4] allows to reduce
    number of reads and writes on GPU as compared to standard attention algorithms,
    thus leading to faster and memory-efficient fine-tuning.'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**更少的计算和内存**：例如，FlashAttention [4] 允许减少GPU上的读写次数，相比于标准的注意力算法，从而实现更快且内存高效的微调。'
- en: '**Less parameters**: in standard fine-tuning, all model weights are retrained
    — however, in most cases only a small subset of weights affect the performance
    of a model on the fine-tuning data. Parameter-efficient fine-tuning (PEFT) identifies
    this subset and “freezes” the other weights, which allows to heavily reduce resource
    usage while achieving a more stable performance of the model.'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**更少的参数**：在标准微调中，所有模型权重都会被重新训练——然而，在大多数情况下，只有一小部分权重影响模型在微调数据上的表现。参数高效微调（PEFT）识别这一子集并“冻结”其他权重，这可以大幅减少资源使用，同时实现更稳定的模型表现。'
- en: '**Less training data**: data quality scales better than data size [3] — the
    more focussed and curated your training data, the less of it is needed to optimise
    performance. One of the most successful approaches here is instruction fine-tuning.
    During training, the LLM is provided with task-specific instructions which reflect
    how it will eventually be prompted during inference. Narrowing down the training
    space enables faster learning from less data. Instruction fine-tuning has been
    practiced for a while already, for instance in T0, FLAN, InstructGPT — and ultimately,
    it is also the method that underlies ChatGPT.'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**更少的训练数据**：数据质量比数据规模更具可扩展性[3]——你的训练数据越集中和精心策划，优化性能所需的数据量就越少。其中一种最成功的方法是指令微调。在训练过程中，LLM会被提供任务特定的指令，这些指令反映了它在推理过程中最终会如何被提示。缩小训练范围可以从更少的数据中更快地学习。指令微调已经被应用了一段时间，例如在T0、FLAN、InstructGPT中——最终，它也是ChatGPT所基于的方法。'
- en: On the other extreme, for now, “generative AI control is in the hands of the
    few that can afford the dollars to train and deploy models at scale”.[5] The commercial
    offerings are exploding in size — be it model size, data size or the time spent
    on training — and clearly outcompete open-source models in terms of output quality.
    There is not much to report here technically — rather, the concerns are more on
    the side of governance and regulation. Thus, “one key risk is that powerful LLMs
    like GPT develop only in a direction that suits the commercial objectives of these
    companies.”[5]
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，目前，“生成性人工智能的控制权掌握在那些能够负担得起训练和大规模部署模型的少数人手中”。[5] 商业产品的规模正在迅速膨胀——无论是模型规模、数据规模还是训练时间——在输出质量方面明显优于开源模型。技术上这里没有太多可以报告的内容——而更多的担忧在于治理和监管。因此，“一个关键风险是像GPT这样的强大LLM可能只朝着符合这些公司商业目标的方向发展。”[5]
- en: How will these two ends meet — and will they meet at all? On the one hand, any
    tricks that allow to reduce resource consumption can eventually be scaled up again
    by throwing more resources at them. On the other hand, LLM training follows the
    power law, which means that the learning curve flattens out as model size, dataset
    size and training time increase.[6] You can think of this in terms of the human
    education analogy — over the lifetime of humanity, schooling times have increased,
    but did the intelligence and erudition of the average person follow suit?
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 这两端将如何交汇——它们会交汇吗？一方面，任何能够减少资源消耗的技巧最终都可以通过投入更多资源来扩大规模。另一方面，LLM的训练遵循幂律，这意味着随着模型规模、数据集规模和训练时间的增加，学习曲线会变得平缓。[6]
    你可以将其理解为人类教育的类比——在人类历史的长河中，学校教育时间有所增加，但普通人的智力和博学是否也随之增加？
- en: The positive thing about a flattening learning curve is the relief it brings
    amidst fears about AI growing “stronger and smarter” than humans. But brace yourself
    — the LLM world is full of surprises, and one of the most unpredictable ones is
    **emergence**.[7] Emergence is when quantitative changes in a system result in
    qualitative changes in behaviour — summarised with “quantity leads to quality”,
    or simply “more is different”.[8] At some point in their training, LLMs seem to
    acquire new, unexpected capabilities that were not in the original training scope.
    At present, these capabilities come in the form of new linguistic skills — for
    instance, instead of just generating text, models suddenly learn to summarise
    or translate. It is impossible to predict when this might happen and what the
    nature and scope of the new capabilities will be. Hence, the phenomenon of emergence,
    while fascinating for researchers and futurists, is still far away from providing
    robust value in a commercial context.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 学习曲线平缓的积极之处在于它在对人工智能“变得比人类更强大和聪明”的担忧中带来的缓解。但请做好准备——LLM领域充满了惊喜，其中最不可预测的之一是**涌现**。[7]
    涌现是指系统中的定量变化导致行为上的定性变化——总结为“量变引起质变”，或者简单地说“更多即不同”。[8] 在训练的某个阶段，LLM似乎会获得一些新的、意想不到的能力，这些能力不在原始训练范围内。目前，这些能力表现为新的语言技能——例如，模型突然学会了总结或翻译，而不仅仅是生成文本。无法预测何时会发生这种情况，以及这些新能力的性质和范围是什么。因此，虽然涌现现象对研究人员和未来学家来说很吸引人，但在商业背景下仍然远未提供稳健的价值。
- en: As more and more methods are developed that increase the efficiency of LLM finetuning
    and inference, the resource bottleneck around the physical operation of open-source
    LLMs seems to be loosening. Concerned with the high usage cost and restricted
    quota of commercial LLMs, more and more companies consider deploying their own
    LLMs. However, development and maintenance costs remain, and most of the described
    optimisations also require extended technical skills for manipulating both the
    models and the hardware on which they are deployed. The choice between open-source
    and commercial LLMs is a strategic one and should be done after a careful exploration
    of a range of trade-offs that include costs (incl. development, operating and
    usage costs), availability, flexibility and performance. A common line of advice
    is to get a head start with the big commercial LLMs to quickly validate the business
    value of your end product, and “switch” to open-source later down the road. But
    this transition can be tough and even unrealistic, since LLMs widely differ in
    the tasks they are good at. There is a risk that open-source models cannot satisfy
    the requirements of your already developed application, or that you need to do
    considerable modifications to mitigate the associated trade-offs. Finally, the
    most advanced setup for companies that build a variety of features on LLMs is
    a multi-LLM architecture that allows to leverage the advantages of different LLMs.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 随着越来越多的方法被开发出来以提高LLM微调和推理的效率，围绕开源LLM物理操作的资源瓶颈似乎正在放松。由于商业LLM的高使用成本和限制配额，越来越多的公司考虑部署自己的LLM。然而，开发和维护成本依然存在，大多数描述的优化也需要扩展的技术技能来操作模型及其部署的硬件。选择开源还是商业LLM是一个战略性决策，应在仔细探索包括成本（包括开发、运营和使用成本）、可用性、灵活性和性能在内的一系列权衡后做出。一个常见的建议是先使用大型商业LLM来快速验证最终产品的商业价值，然后再“切换”到开源LLM。然而，这种过渡可能会很艰难，甚至不现实，因为LLM在擅长的任务上有很大差异。存在着开源模型无法满足你已开发应用需求的风险，或者你需要做大量修改以缓解相关权衡。最后，对于那些在LLM上构建各种功能的公司，最先进的设置是多LLM架构，这允许利用不同LLM的优势。
- en: 3\. LLMs are getting operational with plugins, agents and frameworks.
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 3\. LLM正在通过插件、代理和框架实现操作。
- en: 'The big challenges of LLM training being roughly solved, another branch of
    work has focussed on the integration of LLMs into real-world products. Beyond
    providing ready-made components that enhance convenience for developers, these
    innovations also help overcome the existing limitations of LLMs and enrich them
    with additional capabilities such as reasoning and the use of non-linguistic data.[9]
    The basic idea is that, while LLMs are already great at mimicking human linguistic
    capacity, they still have to be placed into the context of a broader computational
    “cognition” to conduct more complex reasoning and execution. This cognition encompasses
    a number of different capacities such as reasoning, action and observation of
    the environment. Basis: At the moment, it is approximated using plugins and agents,
    which can be combined using modular LLM frameworks such as LangChain, LlamaIndex
    and AutoGPT.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: LLM训练中的重大挑战已基本解决，另一项工作重点是将LLM集成到现实世界的产品中。除了提供提高开发者便利性的现成组件外，这些创新还帮助克服现有LLM的限制，并通过推理和使用非语言数据等附加能力来丰富它们。[9]
    基本思想是，尽管LLM在模拟人类语言能力方面已经很出色，但它们仍需置于更广泛的计算“认知”背景中，以进行更复杂的推理和执行。这种认知包括推理、行动和观察环境等多种不同能力。目前，这通过插件和代理来近似实现，这些插件和代理可以通过LangChain、LlamaIndex和AutoGPT等模块化LLM框架进行组合。
- en: 3.1 Plugins offer access to external data and functionality
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.1 插件提供对外部数据和功能的访问
- en: 'Pre-trained LLMs have significant practical limitations when it comes to the
    data they leverage: on the one hand, the data quickly gets outdated — for instance,
    while GPT-4 was published in 2023, its data was cut off in 2021\. On the other
    hand, most real-world applications require some customisation of the knowledge
    in the LLM. Consider building an app that allows you to create personalised marketing
    content — the more information you can feed into the LLM about your product and
    specific users, the better the result. Plugins make this possible — your program
    can fetch data from an external source, like customer e-mails and call records,
    and insert these into the prompt for a personalised, controlled output.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 预训练的LLM在利用数据时存在显著的实际限制：一方面，数据很快就会过时——例如，尽管GPT-4于2023年发布，其数据却截止于2021年。另一方面，大多数现实世界的应用需要对LLM中的知识进行一些定制。考虑构建一个允许你创建个性化营销内容的应用——你可以向LLM提供关于你的产品和具体用户的信息，结果会更好。插件使这一点成为可能——你的程序可以从外部源（如客户电子邮件和通话记录）获取数据，并将这些数据插入到提示中，以生成个性化的、受控的输出。
- en: 3.2 Agents walk the talk
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.2 代理人言行一致
- en: 'Language is closely tied with actionability. Our communicative intents often
    circle around action, for example when we ask someone to do something or when
    we refuse to act in a certain way. The same goes for computer programs, which
    can be seen as collections of functions that execute specific actions, block them
    when certain conditions are not met etc. LLM-based agents bring these two worlds
    together. The instructions for these agents are not hard-coded in a programming
    language, but are freely generated by LLMs in the form of reasoning chains that
    lead to achieving a given goal. Each agent has a set of plugins at hand and can
    juggle them around as required by the reasoning chain — for example, he can combine
    a search engine for retrieving specific information and a calculator to subsequently
    execute computations on this information. The idea of agents has existed for a
    long time in reinforcement learning — however, as of today, reinforcement learning
    still happens in relatively closed and safe environments. Backed by the vast common
    knowledge of LLMs, agents can now not only venture into the “big world”, but also
    tap into an endless combinatorial potential: each agent can execute a multitude
    of tasks to reach their goals, and multiple agents can interact and collaborate
    with each other.[10] Moreover, agents learn from their interactions with the world
    and build up a memory that comes much closer to the multi-modal memory of humans
    than does the purely linguistic memory of LLMs.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 语言与可操作性紧密相关。我们的交流意图常常围绕行动展开，例如当我们要求别人做某事或拒绝以某种方式行动时。计算机程序也是如此，它们可以被视为执行特定操作的函数集合，当满足特定条件时会阻止这些操作等。基于LLM的代理人将这两个世界结合在一起。这些代理人的指令不是用编程语言硬编码的，而是由LLM以推理链的形式自由生成，这些推理链引导实现给定目标。每个代理都有一组插件，并可以根据推理链的需要进行调整——例如，它可以结合一个用于检索特定信息的搜索引擎和一个用于随后对这些信息进行计算的计算器。代理人的概念在强化学习中存在已久——然而，到今天为止，强化学习仍发生在相对封闭和安全的环境中。凭借LLM的广泛常识，代理人不仅可以进入“广阔的世界”，还可以发挥无尽的组合潜力：每个代理可以执行多种任务以实现其目标，多个代理可以互相互动和合作。[10]
    此外，代理人从与世界的互动中学习，建立的记忆比LLM的纯语言记忆更接近人类的多模态记忆。
- en: 3.3 Frameworks provide a handy interface for LLM integration
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.3 框架提供了LLM集成的便捷接口
- en: In the last months, we have seen a range of new LLM-based frameworks such as
    LangChain, AutoGPT and LlamaIndex. These frameworks allow to integrate plugins
    and agents into complex chains of generations and actions to implement complex
    processes that include multi-step reasoning and execution. Developers can now
    focus on efficient prompt engineering and quick app prototyping.[11] At the moment,
    a lot of hard-coding is still going on when you use these frameworks — but gradually,
    they might be evolving towards a more comprehensive and flexible system for modelling
    cognition and action, such as the JEPA architecture proposed by Yann LeCun.[12]
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去几个月里，我们见证了一系列基于LLM的新框架，如LangChain、AutoGPT和LlamaIndex。这些框架允许将插件和代理集成到复杂的生成和操作链中，以实现包括多步骤推理和执行的复杂过程。开发人员现在可以专注于高效的提示工程和快速应用原型设计。[11]
    目前，使用这些框架时仍然有很多硬编码的工作——但逐渐地，它们可能会向更全面和灵活的系统发展，比如Yann LeCun提出的JEPA架构。[12]
- en: 'What are the implications of these new components and frameworks for builders?
    On the one hand, they boost the potential of LLMs by enhancing them with external
    data and agency. Frameworks, in combination with convenient commercial LLMs, have
    turned app prototyping into a matter of days. But the rise of LLM frameworks also
    has implications for the LLM layer. It is now hidden behind an additional abstraction,
    and as any abstraction it requires higher awareness and discipline to be leveraged
    in a sustainable way. First, when developing for production, a structured process
    is still required to evaluate and select specific LLMs for the tasks at hand.
    At the moment, many companies skip this process under the assumption that the
    latest models provided by OpenAI are the most appropriate. Second, LLM selection
    should be coordinated with the desired agent behavior: the more complex and flexible
    the desired behavior, the better the LLM should perform to ensure that it picks
    the right actions in a wide space of options.[13] Finally, in operation, an MLOps
    pipeline should ensure that the model doesn’t drift away from changing data distributions
    and user preferences.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 这些新组件和框架对构建者有何影响？一方面，它们通过增强外部数据和能力提升了 LLM 的潜力。框架与方便的商业 LLM 结合，将应用原型制作缩短到了几天。但
    LLM 框架的兴起也对 LLM 层有影响。它现在隐藏在额外的抽象层之后，任何抽象层都需要更高的意识和纪律，以可持续的方式利用。首先，在生产开发时，仍需要一个结构化的过程来评估和选择适合当前任务的
    LLM。目前，许多公司在假设 OpenAI 提供的最新模型最合适的情况下跳过了这个过程。其次，LLM 的选择应该与期望的代理行为协调：期望的行为越复杂和灵活，LLM
    应该表现得越好，以确保它能在广泛的选项空间中选择正确的行动。[13] 最后，在操作中，MLOps 管道应该确保模型不会偏离变化的数据分布和用户偏好。
- en: 4\. The linguistic interface of LLMs introduces new challenges for human-machine
    interaction.
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 4\. LLM 的语言接口为人机交互带来了新的挑战。
- en: With the advance of prompting, using AI to do cool and creative things is becoming
    accessible for non-technical people. No need to be a programmer anymore — just
    use language, our natural communication medium, to tell the machine what to do.
    However, amidst all the buzz and excitement around quick prototyping and experimentation
    with LLMs, at some point, we still come to realize that “it’s easy to make something
    cool with LLMs, but very hard to make something production-ready with them.”[14]
    In production, LLMs hallucinate, are sensitive to imperfect prompt designs, and
    raise a number of issues for governance, safety, and alignment with desired outcomes.
    And the thing we love most about LLMs — its open-ended space of in- and outputs
    — also makes it all the harder to test for potential failures before deploying
    them to production.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 随着提示技术的进步，使用 AI 来做炫酷和创造性的事情对于非技术人员变得越来越可行。不再需要成为程序员——只需使用语言，我们的自然沟通媒介，来告诉机器该做什么。然而，在所有围绕快速原型制作和
    LLM 实验的热潮和兴奋中，我们仍然会发现“用 LLM 做些炫酷的东西很容易，但要让它们达到生产级的准备却非常困难。”[14] 在生产环境中，LLM 会出现幻觉，对不完美的提示设计很敏感，并且在治理、安全和与期望结果的一致性方面提出了许多问题。我们最喜欢
    LLM 的地方——它开放的输入和输出空间——也使得在部署到生产环境之前，更难以测试潜在的故障。
- en: 4.1 Hallucinations and silent failures
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.1 幻觉和沉默的失败
- en: 'If you have ever built an AI product, you will know that end users are often
    highly sensitive to AI failures. Users are prone to a “negativity bias”: even
    if your system achieves high overall accuracy, those occasional but unavoidable
    error cases will be scrutinized with a magnifying glass. With LLMs, the situation
    is different. Just as with any other complex AI system, LLMs do fail — but they
    do so in a silent way. Even if they don’t have a good response at hand, they will
    still generate something and present it in a highly confident way, tricking us
    into believing and accepting them and putting us in embarrassing situations further
    down the stream. Imagine a multi-step agent whose instructions are generated by
    an LLM — an error in the first generation will cascade to all subsequent tasks
    and corrupt the whole action sequence of the agent.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你曾经构建过 AI 产品，你会知道最终用户通常对 AI 故障非常敏感。用户倾向于“负面偏见”：即使你的系统整体准确性很高，那些偶尔但不可避免的错误情况也会被放大镜审视。对于
    LLM 来说，情况有所不同。就像任何其他复杂的 AI 系统一样，LLM 也会失败——但它们以沉默的方式失败。即使它们没有一个好的回应，它们仍然会生成某些东西，并以高度自信的方式展示出来，欺骗我们相信和接受它们，并让我们在后续过程中陷入尴尬的境地。想象一下，一个由
    LLM 生成指令的多步骤代理——第一步的错误将级联到所有后续任务中，并破坏代理的整个行动序列。
- en: One of the biggest quality issues of LLMs is hallucination, which refers to
    the generation of texts that are semantically or syntactically plausible but are
    factually incorrect. Already Noam Chomsky, with his famous sentence “[Colorless
    green ideas sleep furiously](https://en.wikipedia.org/wiki/Colorless_green_ideas_sleep_furiously)”,
    made the point that a sentence can be perfectly well-formed from the linguistic
    point of view but completely nonsensical for humans. Not so for LLMs, which lack
    the non-linguistic knowledge that humans possess and thus cannot ground language
    in the reality of the underlying world. And while we can immediately spot the
    issue in Chomsky’s sentence, fact-checking LLM outputs becomes quite cumbersome
    once we get into more specialized domains that are outside of our field of expertise.
    The risk of undetected hallucinations is especially high for long-form content
    as well as for interactions for which no ground truth exists, such as forecasts
    and open-ended scientific or philosophical questions.[15]
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）面临的最大质量问题之一是幻觉，指的是生成在语义上或语法上看似合理但事实上却不正确的文本。早在诺姆·乔姆斯基通过他著名的句子“[无色的绿色思想愤怒地睡觉](https://en.wikipedia.org/wiki/Colorless_green_ideas_sleep_furiously)”中，就指出了一个句子从语言学角度看可能完美无缺，但对人类而言却完全毫无意义的观点。但对于LLMs而言情况不同，它们缺乏人类所拥有的非语言知识，因此无法将语言与基础世界的现实相结合。虽然我们能立即发现乔姆斯基句子中的问题，但一旦进入我们专业领域之外的更专业领域，验证LLM输出的准确性会变得相当繁琐。对于长篇内容以及没有真实依据的交互，如预测和开放式的科学或哲学问题，未被发现的幻觉风险尤其高。
- en: There are multiple approaches to hallucination. From a statistical viewpoint,
    we can expect that hallucination decreases as language models learn more. But
    in a business context, the incrementality and uncertain timeline of this “solution”
    makes it rather unreliable. Another approach is rooted in neuro-symbolic AI. By
    combining the powers of statistical language generation and deterministic world
    knowledge, we may be able to reduce hallucinations and silent failures and finally
    make LLMs robust for large-scale production. For instance, ChatGPT makes this
    promise with the integration of Wolfram Alpha, a vast structured database of curated
    world knowledge.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 对于幻觉有多种处理方法。从统计学的角度来看，我们可以期待随着语言模型的学习越来越多，幻觉现象会减少。但在商业环境中，这种“解决方案”的增量性和不确定的时间线使得它相当不可靠。另一种方法基于神经符号AI。通过结合统计语言生成和确定性世界知识的力量，我们或许能够减少幻觉和隐性失败，最终使大规模生产中的大型语言模型（LLM）更加稳健。例如，ChatGPT通过集成Wolfram
    Alpha这一庞大的结构化知识数据库来兑现这一承诺。
- en: 4.2 The challenges of prompting
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.2 提示的挑战
- en: On the surface, the natural language interface offered by prompting seems to
    close the gap between AI experts and laypeople — after all, all of us know at
    least one language and use it for communication, so why not do the same with an
    LLM? But prompting is a fine craft. Successful prompting that goes beyond trivia
    requires not only strong linguistic intuitions but also knowledge about how LLMs
    learn and work. And then, the process of designing successful prompts is highly
    iterative and requires systematic experimentation. As shown in the paper [Why
    Johnny can’t prompt](https://dl.acm.org/doi/abs/10.1145/3544548.3581388), humans
    struggle to maintain this rigor. On the one hand, we often are primed by expectations
    that are rooted in our experience of human interaction. Talking to humans is different
    from talking to LLMs — when we interact with each other, our inputs are transmitted
    in a rich situational context, which allows us to neutralize the imprecisions
    and ambiguities of human language. An LLM only gets the linguistic information
    and thus is much less forgiving. On the other hand, it is difficult to adopt a
    systematic approach to prompt engineering, so we quickly end up with opportunistic
    trial-and-error, making it hard to construct a scalable and consistent system
    of prompts.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 表面上，提示提供的自然语言界面似乎缩小了 AI 专家和外行之间的差距——毕竟，我们都知道至少一种语言并用它进行沟通，那么为何不在 LLM 中做同样的事？但提示是一门精细的工艺。成功的提示不仅需要强大的语言直觉，还需要对
    LLM 的学习和工作原理有深入了解。而且，设计成功的提示过程是高度迭代的，需要系统的实验。如论文 [Why Johnny can’t prompt](https://dl.acm.org/doi/abs/10.1145/3544548.3581388)
    所示，人类很难保持这种严格性。一方面，我们常常被根植于人际互动经验中的期望所影响。与人交谈不同于与 LLM 交谈——当我们互相交流时，我们的输入是通过丰富的情境背景传递的，这使我们能够中和人类语言中的不精确性和模糊性。LLM
    只接收到语言信息，因此宽容度要小得多。另一方面，采用系统化的方法进行提示工程很困难，因此我们很快陷入机会主义的试错中，难以构建一个可扩展且一致的提示系统。
- en: To resolve these challenges, it is necessary to educate both prompt engineers
    and users about the learning process and the failure modes of LLMs, and to maintain
    an awareness of possible mistakes in the interface. It should be clear that an
    LLM output is always an uncertain thing. For instance, this can be achieved using
    confidence scores in the user interface which can be derived via model calibration.[15]
    For prompt engineering, we currently see the rise of LLMOps, a subcategory of
    MLOps that allows to manage the prompt lifecycle with prompt templating, versioning,
    optimisation etc. Finally, finetuning trumps few-shot learning in terms of consistency
    since it removes the variable “human factor” of ad-hoc prompting and enriches
    the inherent knowledge of the LLM. Whenever possible given your setup, you should
    consider switching from prompting to finetuning once you have accumulated enough
    training data.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 解决这些挑战需要教育提示工程师和用户了解学习过程和 LLM 的失败模式，并保持对界面中可能出现的错误的意识。应该清楚，LLM 的输出总是具有不确定性的。例如，可以通过模型校准得出的置信度分数在用户界面中实现这一点。[15]
    对于提示工程，我们目前看到 LLMOps 的兴起，这是 MLOps 的一个子类别，允许通过提示模板、版本控制、优化等管理提示生命周期。最后，在一致性方面，微调胜过少量学习，因为它消除了即兴提示的“人为因素”并丰富了
    LLM 的内在知识。只要在你的设置中有可能，你应该考虑在积累了足够的训练数据后从提示切换到微调。
- en: Conclusion
  id: totrans-40
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: 'With new models, performance hacks and integrations coming up every day, the
    LLM rabbit hole is deepening day by day. For companies, it is important to stay
    differentiated, keep an eye on the recent developments and new risks and favour
    hands-on experimentation over the buzz — many trade-offs and issues related to
    LLMs only become visible during real-world use. In this article, we took a look
    at the recent developments and how they affect building with LLMs:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 随着新模型、性能优化和集成每天都在出现，LLM 的深度正在不断加深。对公司而言，保持差异化、关注近期发展和新风险以及偏向实际操作实验而非热点是重要的——许多与
    LLM 相关的权衡和问题只有在实际使用中才会显现。本文探讨了近期的发展及其对 LLM 构建的影响：
- en: '**Most current LLMs are autoregressive and excel at generative tasks**. They
    might be unreliable for analytical tasks, in which case either autoencoding LLMs
    or alternative NLP techniques should be preferred.'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**大多数当前的 LLM 是自回归的，在生成任务中表现出色**。它们在分析任务中可能不可靠，此时应优先考虑自编码 LLM 或其他 NLP 技术。'
- en: '**There are considerable differences between open-source and commercial LLMs,
    and switching between LLMs might turn out to be harder than it seems**. Carefully
    consider the trade-offs, evaluate possible development paths (start with open-source
    and switch to commercial later) and consider a multi-LLM setup if different features
    of your product rely on LLMs.'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**开源LLM和商业LLM之间存在显著差异，切换LLM可能比预期的更困难**。仔细考虑权衡，评估可能的发展路径（从开源开始，随后切换到商业LLM），并考虑如果你产品的不同特性依赖于LLM时，是否考虑多LLM设置。'
- en: '**Frameworks provide a handy interface to build with LLMs, but don’t underestimate
    the importance of the LLM layer** — LLMs should undergo a process of experimentation
    and careful selection, after which they run through the full MLOps cycle to ensure
    a robust, continuously optimised operation and mitigate issues such as model shift.'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**框架提供了一个便捷的接口来构建LLM，但不要低估LLM层的重要性** — LLM应经过实验和细致挑选的过程，然后通过完整的MLOps周期，以确保稳健、持续优化的操作，并减轻诸如模型漂移等问题。'
- en: '**Builders should proactively manage the human factor**. LLMs have conquered
    language, a cognitive area that was originally only accessible to humans. As humans,
    we quickly forget that LLMs are still “machines”, and fail to operate them as
    such. For users and employees, consider how you can raise their awareness and
    educate them on the correct operation and usage of LLMs.'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**构建者应主动管理人类因素**。LLM已经征服了语言，这是一个最初仅人类能够接触的认知领域。作为人类，我们很快忘记LLM仍然是“机器”，并未能如对待机器一样操作它们。对于用户和员工，考虑如何提高他们的意识，并教育他们正确操作和使用LLM。'
- en: References
  id: totrans-46
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] Andreessen Horowitz. 2023\. [For B2B Generative AI Apps, Is Less More](https://a16z.com/2023/03/30/b2b-generative-ai-synthai/)?'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] 安德森·霍洛维茨。2023年。 [对于B2B生成AI应用程序，少即是多吗](https://a16z.com/2023/03/30/b2b-generative-ai-synthai/)？'
- en: '[2] Li Dong et al. 2019\. Unified language model pre-training for natural language
    understanding and generation. In Proceedings of the 33rd International Conference
    on Neural Information Processing Systems, pp. 13063–13075.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] 李栋等。2019年。统一语言模型预训练用于自然语言理解和生成。在第33届国际神经信息处理系统会议论文集中，第13063–13075页。'
- en: '[3] The Information. 2023\. [Google Researcher: Company Has ‘No Moat’ in AI.](https://www.theinformation.com/briefings/google-researcher-memo-company-has-no-moat-in-ai)'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] 信息日报。2023年。 [谷歌研究员：公司在AI领域没有“护城河”](https://www.theinformation.com/briefings/google-researcher-memo-company-has-no-moat-in-ai)。'
- en: '[4] Tri Dao et al. 2022\. [FlashAttention: Fast and Memory-Efficient Exact
    Attention with IO-Awareness](https://arxiv.org/abs/2205.14135).'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] 特里·道等。2022年。 [FlashAttention：具有IO感知的快速且内存高效的精确注意力](https://arxiv.org/abs/2205.14135)。'
- en: '[5] EE Times. 2023\. [Can Open-Source LLMs Solve AI’s Democratization Problem](https://www.eetimes.com/can-open-source-llms-solve-ais-democratization-problem/)?'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '[5] EE Times。2023年。 [开源LLM能解决AI民主化问题吗](https://www.eetimes.com/can-open-source-llms-solve-ais-democratization-problem/)？'
- en: '[6] Jared Kaplan et al. 2023\. [Scaling Laws for Neural Language Models](https://arxiv.org/abs/2001.08361).'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '[6] 贾雷德·卡普兰等。2023年。 [神经语言模型的扩展定律](https://arxiv.org/abs/2001.08361)。'
- en: '[7] Jason Wei et al. 2023\. [Emergent Abilities of Large Language Models](https://arxiv.org/abs/2206.07682).'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '[7] 杰森·魏等。2023年。 [大型语言模型的涌现能力](https://arxiv.org/abs/2206.07682)。'
- en: '[8] Philip Anderson. 1972\. More is Different. In Science, Vol 177, Issue 4047,
    pp. 393–396.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '[8] 菲利普·安德森。1972年。更多即不同。在《科学》，第177卷，第4047期，第393–396页。'
- en: '[9] Janna Lipenkova. 2023\. [Overcoming the Limitations of Large Language Models](https://medium.com/towards-data-science/overcoming-the-limitations-of-large-language-models-9d4e92ad9823).'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '[9] 贾娜·利朋科娃。2023年。 [克服大型语言模型的局限性](https://medium.com/towards-data-science/overcoming-the-limitations-of-large-language-models-9d4e92ad9823)。'
- en: '[10] Joon Sung Park et al. 2023\. [Generative Agents: Interactive Simulacra
    of Human Behavior.](https://arxiv.org/pdf/2304.03442.pdf)'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '[10] 俊尚·朴等。2023年。 [生成代理：人类行为的互动模拟](https://arxiv.org/pdf/2304.03442.pdf)'
- en: '[11] Harvard University. 2023\. GPT-4 — [How does it work, and how do I build
    apps with it? — CS50 Tech Talk](https://www.youtube.com/live/vw-KWfKwvTQ).'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '[11] 哈佛大学。2023年。GPT-4 — [它是如何工作的，我如何用它构建应用程序？ — CS50技术讲座](https://www.youtube.com/live/vw-KWfKwvTQ)。'
- en: '[12] Yann LeCun. 2022\. [A Path Towards Autonomous Machine Intelligence](https://openreview.net/pdf?id=BZ5a1r-kVsf).'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '[12] 扬·勒昆。2022年。 [走向自主机器智能的路径](https://openreview.net/pdf?id=BZ5a1r-kVsf)。'
- en: '[13] Jerry Liu. 2023\. [Dumber LLM Agents Need More Constraints and Better
    Tools](https://medium.com/llamaindex-blog/dumber-llm-agents-need-more-constraints-and-better-tools-17a524c59e12).'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '[13] 杰瑞·刘。2023年。 [更笨的LLM代理需要更多的约束和更好的工具](https://medium.com/llamaindex-blog/dumber-llm-agents-need-more-constraints-and-better-tools-17a524c59e12)。'
- en: '[14] Chip Huyen. 2023\. [Building LLM applications for production](https://huyenchip.com/2023/04/11/llm-engineering.html).'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '[14] Chip Huyen. 2023\. [构建生产环境中的LLM应用](https://huyenchip.com/2023/04/11/llm-engineering.html)。'
- en: '[15] Stephanie Lin et al. 2022\. [Teaching models to express their uncertainty
    in words](https://arxiv.org/abs/2205.14334).'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '[15] Stephanie Lin 等. 2022\. [教模型用语言表达它们的不确定性](https://arxiv.org/abs/2205.14334)。'
