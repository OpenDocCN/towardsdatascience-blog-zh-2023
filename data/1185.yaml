- en: How to Validate OpenAI GPT Model Performance with Text Summarization
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何验证 OpenAI GPT 模型的文本摘要性能
- en: 原文：[https://towardsdatascience.com/how-to-validate-openai-gpt-model-performance-with-text-summarization-298978fea764?source=collection_archive---------6-----------------------#2023-04-04](https://towardsdatascience.com/how-to-validate-openai-gpt-model-performance-with-text-summarization-298978fea764?source=collection_archive---------6-----------------------#2023-04-04)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/how-to-validate-openai-gpt-model-performance-with-text-summarization-298978fea764?source=collection_archive---------6-----------------------#2023-04-04](https://towardsdatascience.com/how-to-validate-openai-gpt-model-performance-with-text-summarization-298978fea764?source=collection_archive---------6-----------------------#2023-04-04)
- en: Part 1 of a study on generative AI usage and testing
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 生成性 AI 使用与测试研究的第一部分
- en: '[](https://markopolocheno.medium.com/?source=post_page-----298978fea764--------------------------------)[![Mark
    Chen](../Images/2d51d4e7ab451b55733a018a3d10a0a7.png)](https://markopolocheno.medium.com/?source=post_page-----298978fea764--------------------------------)[](https://towardsdatascience.com/?source=post_page-----298978fea764--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----298978fea764--------------------------------)
    [Mark Chen](https://markopolocheno.medium.com/?source=post_page-----298978fea764--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://markopolocheno.medium.com/?source=post_page-----298978fea764--------------------------------)[![Mark
    Chen](../Images/2d51d4e7ab451b55733a018a3d10a0a7.png)](https://markopolocheno.medium.com/?source=post_page-----298978fea764--------------------------------)[](https://towardsdatascience.com/?source=post_page-----298978fea764--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----298978fea764--------------------------------)
    [Mark Chen](https://markopolocheno.medium.com/?source=post_page-----298978fea764--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F377682c0f342&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-validate-openai-gpt-model-performance-with-text-summarization-298978fea764&user=Mark+Chen&userId=377682c0f342&source=post_page-377682c0f342----298978fea764---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----298978fea764--------------------------------)
    ·9 min read·Apr 4, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F298978fea764&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-validate-openai-gpt-model-performance-with-text-summarization-298978fea764&user=Mark+Chen&userId=377682c0f342&source=-----298978fea764---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F377682c0f342&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-validate-openai-gpt-model-performance-with-text-summarization-298978fea764&user=Mark+Chen&userId=377682c0f342&source=post_page-377682c0f342----298978fea764---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----298978fea764--------------------------------)
    ·9 分钟阅读·2023年4月4日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F298978fea764&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-validate-openai-gpt-model-performance-with-text-summarization-298978fea764&user=Mark+Chen&userId=377682c0f342&source=-----298978fea764---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F298978fea764&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-validate-openai-gpt-model-performance-with-text-summarization-298978fea764&source=-----298978fea764---------------------bookmark_footer-----------)![](../Images/3d19cbe8fc67655d0ad5525170d590da.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F298978fea764&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-validate-openai-gpt-model-performance-with-text-summarization-298978fea764&source=-----298978fea764---------------------bookmark_footer-----------)![](../Images/3d19cbe8fc67655d0ad5525170d590da.png)'
- en: Photo by [Patrick Tomasso](https://unsplash.com/@impatrickt?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 照片由 [Patrick Tomasso](https://unsplash.com/@impatrickt?utm_source=medium&utm_medium=referral)
    提供，发布在 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: Regardless of your occupation or age, you’ve heard about OpenAI’s generative
    pre-trained transformer (GPT) technology on LinkedIn, YouTube, or in the news.
    These powerful artificial intelligence models/chatbots can seemingly handle any
    task, from creating poems to solving leetcode problems to coherently summarizing
    long articles of text.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 无论你从事什么职业或年龄多大，你都可能在 LinkedIn、YouTube 或新闻中听说过 OpenAI 的生成性预训练变换器（GPT）技术。这些强大的人工智能模型/聊天机器人似乎能够处理任何任务，从创作诗歌到解决
    leetcode 问题，再到连贯地总结长篇文章。
- en: '![](../Images/30787bc6542582b06ffd10096457ba8c.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/30787bc6542582b06ffd10096457ba8c.png)'
- en: Screenshot of [OpenAI’s GPT Playground](https://platform.openai.com/playground)
    Summarizing Jupiter Notes, taken by Author
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '[OpenAI的GPT Playground](https://platform.openai.com/playground) 的截图，总结了Jupiter
    Notes，由作者拍摄'
- en: The promising applications of GPT models seem endless within the expanding NLP
    industry. But with ever-increasing model sizes, it is crucial for teams that are
    building large language models (LLMs) to **understand every model’s performance
    and behaviors**.Since AI, like GPT, is a growing subject in ethics, developers
    should ensure that their models are fair, accountable, and explainable. However,
    doing proper testing on artificial general intelligence across many different
    contexts is tedious, expensive, and time-consuming.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: GPT模型在不断扩展的NLP行业中似乎有无尽的应用前景。但是随着模型规模的不断增加，构建大型语言模型（LLM）的团队必须 **了解每个模型的性能和行为**。由于AI，如GPT，是一个日益增长的伦理问题，开发者应确保他们的模型是公平、负责任和可解释的。然而，在许多不同的环境中对人工通用智能进行适当的测试既繁琐又昂贵，并且耗时。
- en: From the perspective of a machine learning engineer at [Kolena](https://www.kolena.io/),
    this article offers an extensive guide to using GPT models and compares theirperformance
    for the [abstractive text summarization](https://paperswithcode.com/task/abstractive-text-summarization#:~:text=Abstractive%20Text%20Summarization%20is%20the,appear%20in%20the%20source%20text.)
    task. With this actively researched NLP problem, we will be able to **review model
    behavior, performance differences, ROI**, and so much more.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 从 [Kolena](https://www.kolena.io/) 的机器学习工程师的角度来看，本文提供了使用GPT模型的全面指南，并比较了它们在 [抽象文本摘要](https://paperswithcode.com/task/abstractive-text-summarization#:~:text=Abstractive%20Text%20Summarization%20is%20the,appear%20in%20the%20source%20text.)
    任务上的表现。通过这项积极研究的NLP问题，我们将能够 **审查模型行为、性能差异、投资回报率** 等等。
- en: By the end of this article, you will learn that GPT-3.5’s Turbo model gives
    a 22% higher BERT-F1 score with a 15% lower failure rate at 4.8x the cost and
    4.5x the average inference time in comparison to GPT-3’s Ada model for abstractive
    text summarization.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文结束时，你将了解到GPT-3.5的Turbo模型在4.8倍的成本和4.5倍的平均推理时间下，比GPT-3的Ada模型在抽象文本摘要任务上具有22%的更高BERT-F1得分和15%的更低失败率。
- en: Using GPT Effectively
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 有效使用GPT
- en: 'Suppose you want to use GPT for fast solutions in NLP applications, like translating
    text or explaining code. Where do you start? Fortunately, there are only three
    main steps in using GPT for any unique task:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你想在自然语言处理应用中使用GPT来快速解决问题，比如翻译文本或解释代码。你从哪里开始？幸运的是，使用GPT处理任何独特任务时，只有三个主要步骤：
- en: Picking the right model
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择合适的模型
- en: Creating an appropriate [prompt](https://platform.openai.com/docs/guides/completion/prompt-design)
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个适当的 [提示](https://platform.openai.com/docs/guides/completion/prompt-design)
- en: Using [GPT’s API](https://platform.openai.com/docs/api-reference/completions/create)
    for responses (our code is at the end of this article)
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 [GPT的API](https://platform.openai.com/docs/api-reference/completions/create)
    来获取响应（我们的代码在本文末尾）
- en: 'Prior to picking a model, we must first consider a few things: How well does
    each model work? Which one gives the best ROI? Which one generally performs the
    best? Which one performs the best on your data?'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在选择模型之前，我们必须先考虑几个问题：每个模型的表现如何？哪个模型提供了最佳的投资回报率？哪个模型整体表现最佳？哪个模型在你的数据上表现最好？
- en: 'To narrow down the logistics in choosing a GPT model, we use the [CNN-DailyMail](https://paperswithcode.com/dataset/cnn-daily-mail-1)
    text summarization dataset to benchmark and compare the performance of five [**GPT
    models**](https://platform.openai.com/docs/models/gpt-3)**: Ada, Babbage, Curie,
    Davinci, and Turbo**. The test split of the dataset contains 11,490 news articles
    and their respective summaries.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在选择GPT模型时缩小范围，我们使用了 [CNN-DailyMail](https://paperswithcode.com/dataset/cnn-daily-mail-1)
    文本摘要数据集来基准测试和比较五种 [**GPT模型**](https://platform.openai.com/docs/models/gpt-3)**：Ada、Babbage、Curie、Davinci
    和 Turbo** 的性能。数据集的测试部分包含11,490篇新闻文章及其相应摘要。
- en: 'For step two, we generate new summaries with each model using a consistent
    prompt in the following format:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 第二步，我们使用一致的提示生成每个模型的新摘要，格式如下：
- en: '*“Professionally summarize this news article like a reporter with about {word_count_limit}
    to {word_count_limit+50} words:\n {full_text}”*'
  id: totrans-23
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*“像记者一样专业地总结这篇新闻文章，字数约为 {word_count_limit} 到 {word_count_limit+50} 字：\n {full_text}”*'
- en: In practice, it takes some experimentation to refine a prompt that will give
    subjectively optimal results. By using the same prompt, we can accurately compare
    model behaviors with one less variable in how each model differs.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 实际操作中，需要一些实验来优化提示，从而获得主观上最优的结果。通过使用相同的提示，我们可以准确比较模型行为，减少模型差异的一个变量。
- en: '![](../Images/d77b3f0f4dacf41d0af9a3b69ae09c7a.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d77b3f0f4dacf41d0af9a3b69ae09c7a.png)'
- en: In this particular article, we focus on step one, which is picking the right
    model.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我们重点关注第一步，即选择合适的模型。
- en: Validating GPT Model Performance
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 验证 GPT 模型性能
- en: Let’s get acquainted with the GPT models of interest, which come from the [GPT-3](https://platform.openai.com/docs/models/gpt-3)
    and [GPT-3.5](https://platform.openai.com/docs/models/gpt-3-5) series. Each model
    has a token limit defining the maximum size of the combined input and output,
    so if, for example, your prompt for the Turbo model contains 2,000 tokens, the
    maximum output you will receive is 2,096 tokens. For English text, 75 words typically
    tokenizes into roughly 100 tokens.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们了解一下感兴趣的 GPT 模型，它们来自 [GPT-3](https://platform.openai.com/docs/models/gpt-3)
    和 [GPT-3.5](https://platform.openai.com/docs/models/gpt-3-5) 系列。每个模型都有一个令牌限制，定义了输入和输出的最大尺寸。因此，例如，如果你的
    Turbo 模型的提示包含 2,000 个令牌，你将收到的最大输出是 2,096 个令牌。对于英文文本，75 个单词通常会分成大约 100 个令牌。
- en: '![](../Images/e5bbc5007d1d6b97dc1487ec2767c673.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e5bbc5007d1d6b97dc1487ec2767c673.png)'
- en: We’re currently on the waitlist for [GPT-4](https://platform.openai.com/docs/models/gpt-4)
    access, so we’ll include those models in the future. For now, the main difference
    between GPT-4 and GPT-3.5 is not significant for basic tasks, but GPT-4 offers
    a much larger limit for tokens at a much higher price point compared to Davinci.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 我们目前在 [GPT-4](https://platform.openai.com/docs/models/gpt-4) 的等待名单上，所以将来会包含这些模型。现在，GPT-4
    和 GPT-3.5 之间的主要区别对基本任务并不显著，但 GPT-4 提供了比 Davinci 更大的令牌限制，但价格要高得多。
- en: Performance Metrics of Abstractive Text Summarization
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 抽象文本摘要的性能指标
- en: 'As we all know, metrics help us measure performance. The tables below highlight
    the standard and custom metrics we use to evaluate models on their text summarization
    performance:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 众所周知，指标帮助我们衡量性能。下面的表格突出了我们用来评估模型文本摘要性能的标准和自定义指标：
- en: '![](../Images/119111b34611f75076a75019912c94c0.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/119111b34611f75076a75019912c94c0.png)'
- en: '*We calculate BLEU scores with [SacreBLEU](https://huggingface.co/spaces/evaluate-metric/sacrebleu)
    and BERT scores with Microsoft’s [deberta-xlarge-mnli](https://huggingface.co/microsoft/deberta-xlarge-mnli)
    model.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '*我们使用 [SacreBLEU](https://huggingface.co/spaces/evaluate-metric/sacrebleu)
    计算 BLEU 分数，并用微软的 [deberta-xlarge-mnli](https://huggingface.co/microsoft/deberta-xlarge-mnli)
    模型计算 BERT 分数。*'
- en: '![](../Images/3171ffddf0fda1b31e10bfaa94d80d75.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3171ffddf0fda1b31e10bfaa94d80d75.png)'
- en: 'ROUGE and BLEU measure similarity with word matchings in the ground truths
    and inferences, while BERT scores consider semantic similarity. The higher the
    value, the closer the similarity:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: ROUGE 和 BLEU 通过对比地面真实数据和推断结果中的单词匹配来测量相似度，而 BERT 分数则考虑语义相似度。值越高，相似度越接近：
- en: '![](../Images/2e553be150410d278ef069348edc5459.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2e553be150410d278ef069348edc5459.png)'
- en: Results with Standard Metrics
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 标准指标结果
- en: After we generate new summaries (inferences) per article on each model, we can
    compare model performance across any type of metric with the ground truths. Let’s
    look into the summary comparisons and metric plots, ignoring Babbage for more
    readability.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在每个模型上生成新的摘要（推断）后，我们可以通过任何类型的指标将模型性能与地面真实数据进行比较。让我们查看摘要比较和指标图，忽略 Babbage 以提高可读性。
- en: '**ROUGE_L and BLEU**‍'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '**ROUGE_L 和 BLEU**‍'
- en: 'In the following example, the original 350-word news article has this summary:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下示例中，原始的350字新闻文章有以下总结：
- en: '*A new report from Suncorp Bank found Australians spent $20 billion on technology
    in the past year. Men spent twice as much as women on computers, digital accessories,
    mobile apps, and streaming services. Families with children at home spend 50 per
    cent more to stay digitally than singles, couples without children and empty nesters.
    One third of households don’t budget for technology or wildly underestimate how
    much they will spend.*'
  id: totrans-42
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*来自Suncorp银行的一份新报告发现，过去一年澳大利亚人花费了200亿澳元用于科技。男性在计算机、数字配件、移动应用程序和流媒体服务上的花费是女性的两倍。有孩子的家庭在数字化方面的支出比单身人士、没有孩子的情侣和空巢老人多出50%。三分之一的家庭没有为科技预算，或者大大低估了他们的支出。*'
- en: 'We get the following ROUGE_L, BLEU, and generated summaries with Davinci and
    Ada:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用 Davinci 和 Ada 得到了以下 ROUGE_L、BLEU 和生成的摘要：
- en: '![](../Images/c3504e652170776af801823853e54562.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c3504e652170776af801823853e54562.png)'
- en: You’ll notice that by reading the generated summaries, Davinci does a coherent
    job of summarizing the content of a larger text. Ada, however, does not provide
    a summary of the same quality, and the lower values of ROUGE_L and BLEU reflect
    that lower quality of output.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 你会发现，通过阅读生成的摘要，Davinci 能很好地总结较大文本的内容。然而，Ada 并没有提供相同质量的摘要，ROUGE_L 和 BLEU 较低的值反映了输出质量的差距。
- en: '![](../Images/45808dfcdc595a9f4deb97d5a7466913.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/45808dfcdc595a9f4deb97d5a7466913.png)'
- en: Distribution of ROUGE_L
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: ROUGE_L 的分布
- en: When we examine the distributions of ROUGE_L and BLEU for each model, we see
    that Ada has lower metric values, and Turbo has the highest metric values. Davinci
    falls just behind Turbo in terms of these metrics. As GPT models **increase in
    size**, we see a general **increase in ROUGE and BLEU scores,** too. The greater
    the value for these metrics, the greater the number of words from the ground truth
    summary exist in the generated texts. In addition, these **larger models produce
    a more informative summary with fewer grammatical issues**.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们检查每个模型的 ROUGE_L 和 BLEU 分布时，我们看到 Ada 的指标值较低，而 Turbo 的指标值最高。Davinci 在这些指标方面略逊于
    Turbo。随着 GPT 模型的**规模增加**，我们看到**ROUGE 和 BLEU 分数也有普遍的增加**。这些指标的值越大，生成文本中与真实摘要匹配的单词数量就越多。此外，这些**较大的模型生成的摘要信息更丰富，语法问题更少**。
- en: '![](../Images/e9fd69ab2b5deccbe2f2537027173e24.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e9fd69ab2b5deccbe2f2537027173e24.png)'
- en: Distribution of BLEU
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: BLEU 的分布
- en: ‍**BERT_F1**‍
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: ‍**BERT_F1**‍
- en: 'For BERT scores, the same trend is consistent: larger models have better performance
    in matching key words and semantic meaning from the provided summary. This is
    evident in how the distribution for larger models shifts to the right, in the
    direction of higher F1 scores.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 BERT 分数，趋势一致：较大的模型在匹配关键字和语义意义方面表现更好。这在较大模型的分布向右移动的趋势中可以明显看出，这表明 F1 分数更高。
- en: '![](../Images/3844e97e53995f55fc429e5f482a0166.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3844e97e53995f55fc429e5f482a0166.png)'
- en: Distribution of BERT_F1
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: BERT_F1 的分布
- en: '![](../Images/616b44804e4e036d6dd7aa0ea8eccc64.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/616b44804e4e036d6dd7aa0ea8eccc64.png)'
- en: BERT_F1 vs word_count
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: BERT_F1 与 word_count
- en: From the plot above, we see that bigger models maintain their performance better
    than smaller models as text size grows. The larger models remain consistently
    performant across a wide range of text lengths while the smaller models fluctuate
    in performance as texts grow longer.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 从上图中，我们看到较大的模型在文本大小增加时能更好地保持性能。较大的模型在各种文本长度范围内表现一致，而较小的模型随着文本变长，性能波动较大。
- en: Results with Custom Metrics
  id: totrans-58
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 自定义指标结果
- en: Let’s check our custom metrics to see if there’s any reason not to use Turbo
    or Davinci.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们检查一下自定义指标，看看是否有理由不使用 Turbo 或 Davinci。
- en: '![](../Images/0684aa06a3d1ddacd2e24130db3e5aaf.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0684aa06a3d1ddacd2e24130db3e5aaf.png)'
- en: Distribution of API Request Costs
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: API 请求成本分布
- en: From the models’ cost distributions, we learn that **Davinci is far more expensive**
    than any other model. Although Davinci and Turbo perform at similar levels, **Davinci
    costs around ten times the cost of Turbo**.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 从模型的成本分布中，我们了解到**Davinci 的成本远高于**其他任何模型。尽管 Davinci 和 Turbo 的性能相似，**Davinci 的成本大约是
    Turbo 的十倍**。
- en: '![](../Images/eea86402e480a9f7dff0afdd07bef8a1.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/eea86402e480a9f7dff0afdd07bef8a1.png)'
- en: Distribution of inf_to_gt_word_count
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: inf_to_gt_word_count 的分布
- en: In the figure above, there is a drastic difference in the number of words generated
    for the same ground truth. Turbo and Davinci consistently provide a summary that
    is twice the ground truth summary length, whereas **other models are very inconsistent**.
    Specifically, some generated summaries from the smaller models are much shorter
    and some are more than four times as long! Keep in mind that we prompted each
    model with the same request and word count targetper article, but **certain models
    adhered to that restriction** while others completely ignored it.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在上图中，对于相同真实内容生成的单词数量差异非常大。Turbo 和 Davinci 始终提供长度为真实摘要两倍的总结，而**其他模型非常不一致**。具体来说，某些较小模型生成的摘要要短得多，而有些则超过了真实摘要长度的四倍！请记住，我们对每个模型使用了相同的请求和每篇文章的字数目标，但**某些模型遵守了这一限制**，而其他模型则完全忽视了它。
- en: '![](../Images/5a1ca507ffc3155e4c62d565beac0d6d.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5a1ca507ffc3155e4c62d565beac0d6d.png)'
- en: The variance in summary length is a problem for users as this imbalance indicates
    potential issues with the model or poor performance. In the example above, Curie
    repeats “number of charitable causes in the past, most notably his work with St.
    Jude Children’s Research Hospital” at least twice. In comparison to Turbo, **Curie’s
    summary is redundant and suboptimal** while costing the **same price within a
    tenth of a cent**. Within that small difference, we should note that the cost
    in generating this particular summary with Curie is double the cost of Turbo since
    the number of tokens contained in the output was extremely high.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 总结长度的差异对用户来说是一个问题，因为这种不平衡表明模型可能存在问题或性能不佳。在上面的例子中，Curie至少重复了“两次过去的慈善事业，特别是他在圣犹大儿童研究医院的工作”。与Turbo相比，**Curie的总结冗余且不够优化**，同时成本**在十分之一美分的范围内相同**。在这个小差异中，我们应该注意到，使用Curie生成这个特定总结的成本是Turbo的两倍，因为输出中的token数量极高。
- en: Analysis of Results
  id: totrans-68
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结果分析
- en: After running model evaluations for an hour on Kolena, we can outline and summarize
    each model’s performance and characteristics as shown below.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 在Kolena上运行模型评估一个小时后，我们可以概述和总结每个模型的性能和特征，如下所示。
- en: '![](../Images/1fd40e6d32caa34c54642e8622a5f42c.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1fd40e6d32caa34c54642e8622a5f42c.png)'
- en: 'We now understand that the larger the model size:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在了解到，模型规模越大：
- en: The more semantically similar the provided and generated summaries are
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提供的总结与生成的总结在语义上越相似
- en: The more expensive it is to compute, with the exception of Turbo
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算成本越高，除了Turbo之外
- en: The lower the number of empty summaries
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 空总结的数量越少
- en: The slower it is to generate a summary
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生成总结的速度越慢
- en: The **more consistently the model behaves**
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型表现越一致**'
- en: Ultimately, the **Turbo model is the top-performing model** offered in the GPT-3/3.5
    series, providing the most consistent text similarity scores, all while also being
    very cost-effective.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，**Turbo模型是GPT-3/3.5系列中表现最佳的模型**，提供了最一致的文本相似性评分，同时也非常具有成本效益。
- en: Notes for Further Research
  id: totrans-78
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 进一步研究的注意事项
- en: 'Interestingly, given a text to summarize, **some models simply refuse to generate
    output**, even though the prompt is within the token limit. Turbo failed on none
    of the articles, which is a great achievement. However, this might be because
    Turbo is not as responsive in flagging sensitive content or puts less emphasis
    in making such considerations. Ada might be less performant, but we should ask
    OpenAI if it refuses to generate summaries out of ethical consideration or technical
    limitations. Below is a sample of the **top sixteen news articles by BERT_F1 where
    Ada failed** to provide any summary, but Turbo produced decent summaries. It does
    seem like Ada is less lenient in producing summaries with sensitive content:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是，给定一个需要总结的文本，**一些模型根本拒绝生成输出**，即使提示在token限制内。Turbo没有在任何文章上失败，这是一项了不起的成就。然而，这可能是因为Turbo在标记敏感内容方面不如其他模型那么响应，或者在考虑这些因素时关注较少。Ada可能表现较差，但我们应该询问OpenAI是否因为道德考虑或技术限制而拒绝生成总结。下面是**Ada未能提供任何总结的前十六篇新闻文章的样本，其中Turbo生成了不错的总结**。看起来Ada在生成涉及敏感内容的总结时更不宽容：
- en: '![](../Images/bc14c077db7059eb345df4c3adeb8071.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bc14c077db7059eb345df4c3adeb8071.png)'
- en: Articles Where Ada Fails While Turbo Performs Well — From Kolena
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: Ada失败而Turbo表现良好的文章 — 来自Kolena
- en: 'The ground truth summaries from the dataset are **not necessarily ideal in
    content or length**. However, we assume ground truth summaries are ideal for the
    purpose of straightforward performance computations, so model evaluation metrics
    might indicate that a great model is actually underperforming, even though it
    produces perfectly valid and detailed summaries. Perhaps **some generated summaries
    are even better than their ground truth counterpart**, as shown below:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集中的真实总结**在内容或长度上不一定理想**。然而，我们假设真实总结对于直接的性能计算是理想的，因此模型评估指标可能会表明一个出色的模型实际上表现不佳，即使它生成了完全有效且详细的总结。也许**一些生成的总结甚至比其真实总结更好**，如下所示：
- en: '![](../Images/ee7e65ec15f1cf870d4c9f76fdd00de7.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ee7e65ec15f1cf870d4c9f76fdd00de7.png)'
- en: Conclusion
  id: totrans-84
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: The world of NLP is rapidly advancing with the introduction of LLMs like GPT.
    As such models become larger, more complex, and more expensive, it is crucial
    for developers and users alike to understand their expected performance levels
    for specific use cases.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: NLP的世界正迅速发展，随着像GPT这样的LLM的引入。这些模型变得越来越大、复杂且昂贵，因此开发者和用户都必须了解它们在特定使用场景下的预期性能水平。
- en: ‍
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: ‍
- en: ‍**Different models may better fit your business requirements**, depending on
    your problem, expectations, and available resources. There is much to consider
    when picking a single GPT model for your NLP tasks. In the quickly advancing era
    of LLMs, hopefully the findings outlined in this article give a new perspective
    on the differences among OpenAI’s models.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: ‍**不同的模型可能更适合你的业务需求**，这取决于你的问题、期望和可用资源。在为你的NLP任务选择单一GPT模型时，有很多因素需要考虑。在LLM快速发展的时代，希望本文中的发现能为你提供有关OpenAI模型差异的新视角。
- en: ‍
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: ‍
- en: Stay tuned for [more posts](https://www.kolena.io/blog) in the future where
    we may cover prompt engineering, GPT-4 performance, or differences in model behavior
    by types of content as well!
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 敬请关注未来的[更多帖子](https://www.kolena.io/blog)，我们可能会涉及提示工程、GPT-4性能，或不同内容类型对模型行为的影响等话题！
- en: ‍
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: ‍
- en: As promised earlier in this article, our code for reference and all five models’
    summaries for every example within this article are all on this [page](https://docs.google.com/document/d/1FrbfHgEJL90Nnwr3mr3De4gJJtntnN2ZsB98f_9-mZg/edit?usp=sharing).
    You can learn more about OpenAI’s API or models in [OpenAI’s documentation](https://platform.openai.com/docs/introduction).
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 正如本文早前承诺的，我们提供了参考代码以及本文所有例子的五个模型的摘要，都在这个[页面](https://docs.google.com/document/d/1FrbfHgEJL90Nnwr3mr3De4gJJtntnN2ZsB98f_9-mZg/edit?usp=sharing)上。你可以在[OpenAI的文档](https://platform.openai.com/docs/introduction)中了解更多关于OpenAI
    API或模型的信息。
- en: All images of plots are screenshots taken from [Kolena](https://www.kolena.io/)
    unless otherwise indicated. Note that similar plots can be manually generated
    in common frameworks such as mathplotlib.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 所有图表的图片均为[Kolena](https://www.kolena.io/)的截图，除非另有说明。注意，相似的图表可以在如mathplotlib等常见框架中手动生成。
