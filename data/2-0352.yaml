- en: 'Back to Basics, Part Tres: Logistic Regression'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 回到基础，第三部分：逻辑回归
- en: 原文：[https://towardsdatascience.com/back-to-basics-part-tres-logistic-regression-e309de76bd66](https://towardsdatascience.com/back-to-basics-part-tres-logistic-regression-e309de76bd66)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/back-to-basics-part-tres-logistic-regression-e309de76bd66](https://towardsdatascience.com/back-to-basics-part-tres-logistic-regression-e309de76bd66)
- en: An illustrated guide on Logistic Regression, with code
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 一本关于逻辑回归的图示指南，包括代码
- en: '[](https://medium.com/@shreya.rao?source=post_page-----e309de76bd66--------------------------------)[![Shreya
    Rao](../Images/03f13be6f5f67783d32f0798f09a4f86.png)](https://medium.com/@shreya.rao?source=post_page-----e309de76bd66--------------------------------)[](https://towardsdatascience.com/?source=post_page-----e309de76bd66--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----e309de76bd66--------------------------------)
    [Shreya Rao](https://medium.com/@shreya.rao?source=post_page-----e309de76bd66--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@shreya.rao?source=post_page-----e309de76bd66--------------------------------)[![Shreya
    Rao](../Images/03f13be6f5f67783d32f0798f09a4f86.png)](https://medium.com/@shreya.rao?source=post_page-----e309de76bd66--------------------------------)[](https://towardsdatascience.com/?source=post_page-----e309de76bd66--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----e309de76bd66--------------------------------)
    [Shreya Rao](https://medium.com/@shreya.rao?source=post_page-----e309de76bd66--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----e309de76bd66--------------------------------)
    ·8 min read·Mar 2, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----e309de76bd66--------------------------------)
    ·阅读时间8分钟·2023年3月2日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: 'Welcome back to the final installment of our ***Back to Basics*** series, where
    we’ll delve into another fundamental machine learning algorithm: **Logistic Regression**.
    In the previous two articles, we helped our friend Mark determine the ideal selling
    price for his 2400 feet² house using [Linear Regression](https://medium.com/towards-data-science/back-to-basics-part-uno-linear-regression-cost-function-and-gradient-descent-590dcb3eee46)
    and [Gradient Descent](/back-to-basics-part-dos-linear-regression-cost-function-and-gradient-descent-e3d7d05c56fd).'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 欢迎回到我们***回到基础***系列的最终篇，在这一篇中，我们将深入探讨另一种基础机器学习算法：**逻辑回归**。在之前的两篇文章中，我们帮助我们的朋友Mark使用
    [线性回归](https://medium.com/towards-data-science/back-to-basics-part-uno-linear-regression-cost-function-and-gradient-descent-590dcb3eee46)
    和 [梯度下降](/back-to-basics-part-dos-linear-regression-cost-function-and-gradient-descent-e3d7d05c56fd)
    确定了他2400平方英尺房子的理想售价。
- en: Today, Mark comes back to us again for help. He lives in a fancy neighborhood
    where he thinks houses below a certain size don’t sell, and he is worried that
    his house might not sell either. He asked us to help him determine how ***likely***
    it is that his house will sell.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 今天，Mark再次向我们求助。他住在一个高档社区，他认为某些尺寸以下的房子不会售出，他担心自己的房子也可能售不出去。他请我们帮助他确定他的房子***是否可能***售出。
- en: This is where Logistic Regression comes into play.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 这是逻辑回归发挥作用的地方。
- en: 'Logistic Regression is a type of algorithm that predicts the probability of
    a binary outcome, such as whether a house will sell or not. Unlike Linear Regression,
    Logistic Regression predicts probabilities using a range of 0% to 100%. Note the
    difference between predictions a linear regression model and logistic regression
    model make:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑回归是一种预测二元结果概率的算法，例如预测房子是否会售出。与线性回归不同，逻辑回归使用0%到100%的范围来预测概率。请注意线性回归模型和逻辑回归模型之间预测的区别：
- en: '![](../Images/ae4ff646211b57f9cb04c3cfabc55b8f.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ae4ff646211b57f9cb04c3cfabc55b8f.png)'
- en: Let’s delve deeper into how logistic regression works by determining the probability
    of selling houses with varying sizes.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更深入地探讨逻辑回归如何通过确定不同大小的房子的销售概率来工作。
- en: We start our process again by collecting data about house sizes in Mark’s neighborhood
    and seeing if they sold or not.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 我们再次通过收集关于Mark所在社区的房屋大小的数据，并查看这些房屋是否售出，来开始我们的过程。
- en: '![](../Images/5e151a4188e3e07fe6225979f53ad35e.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5e151a4188e3e07fe6225979f53ad35e.png)'
- en: 'Now let’s plot these points:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们绘制这些点：
- en: '![](../Images/16baef721e5cb51e73af1a0c05df9177.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/16baef721e5cb51e73af1a0c05df9177.png)'
- en: Rather than representing the outcome of the plot as a binary output, it’ll be
    more informative to represent it using probabilities since that is the quantity
    we are trying to predict.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 与其将图表的结果表示为二元输出，不如使用概率来表示，因为这是我们试图预测的量。
- en: We represent 100% probability as 1 and 0% probability as 0
  id: totrans-17
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 我们将100%的概率表示为1，将0%的概率表示为0
- en: '![](../Images/84e5644b20b6729ddaa6c55381ec8afe.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/84e5644b20b6729ddaa6c55381ec8afe.png)'
- en: In our [previous article](/back-to-basics-part-uno-linear-regression-cost-function-and-gradient-descent-590dcb3eee46),
    we learned about linear regression and its ability to fit a line to our data.
    But can it work for our problem where the desired output is a probability? Let’s
    find out by attempting to fit a line using linear regression.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们[上一篇文章](/back-to-basics-part-uno-linear-regression-cost-function-and-gradient-descent-590dcb3eee46)中，我们学习了线性回归及其将直线拟合到数据的能力。但它能否用于我们的这个问题，其中期望的输出是一个概率？让我们通过尝试使用线性回归拟合一条直线来找出答案。
- en: 'We know that the formula for the best-fitting line is:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 我们知道最佳拟合直线的公式是：
- en: '![](../Images/d3b26ad9ff6630c464a8a179fe616311.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d3b26ad9ff6630c464a8a179fe616311.png)'
- en: 'By following the steps outlined in linear regression, we can obtain optimal
    values for β₀ and β₁, which will result in the best-fitting line. Assuming we
    have done so, let’s take a look at the line that we have obtained:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 通过按照线性回归中概述的步骤，我们可以获得β₀和β₁的最佳值，从而得到最佳拟合的直线。假设我们已经完成了这些步骤，让我们看看我们获得的直线：
- en: '![](../Images/f3d8e33858bc2079af667a28904ed58f.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f3d8e33858bc2079af667a28904ed58f.png)'
- en: 'Based on this line, we can see that a house with a size just below 2700 feet²
    is predicted to have a 100% probability of being sold:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 根据这条直线，我们可以看到一栋面积稍低于2700平方英尺的房子被预测有100%的概率出售：
- en: '![](../Images/57c245fe0af0ab78f00c01b587e308e5.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/57c245fe0af0ab78f00c01b587e308e5.png)'
- en: '…and a 2200 feet² house is predicted to have a 0% chance of being sold:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: ……而一栋2200平方英尺的房子被预测有0%的出售概率：
- en: '![](../Images/08a691a73ef23e6460863d15c9738d67.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/08a691a73ef23e6460863d15c9738d67.png)'
- en: '…and a 2300 feet² house is predicted to have about a 20% probability of being
    sold:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: ……而一栋2300平方英尺的房子被预测有大约20%的出售概率：
- en: '![](../Images/45bb577b7a475f937350adcf0ded0cdd.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/45bb577b7a475f937350adcf0ded0cdd.png)'
- en: Alright, so far so good. But what if we have a house that is 2800 feet² in size?
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 好了，到目前为止一切顺利。但如果我们有一栋面积为2800平方英尺的房子呢？
- en: '![](../Images/0b440d8b839079aa49bdc3ee90de3793.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0b440d8b839079aa49bdc3ee90de3793.png)'
- en: Uh.. what does a probability above 100% mean? Would a house of this size be
    predicted to sell with a probability of 150%??
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 嗯……概率超过100%是什么意思？这样的房子会被预测以150%的概率出售吗？
- en: Weird. What about a house that’s 2100 feet²?
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 奇怪。那一栋2100平方英尺的房子呢？
- en: '![](../Images/fa1e4447b865ed673ba46d6a5af76115.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fa1e4447b865ed673ba46d6a5af76115.png)'
- en: Okay, clearly we have run into a problem as the predicted probability for a
    house with a size of 2100 feet² appears to be negative. This definitely does not
    make sense, and it indicates an issue with using a standard linear regression
    line.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 好吧，很明显我们遇到了一个问题，因为一个2100平方英尺房子的预测概率似乎是负值。这显然是没有意义的，这表明使用标准线性回归直线存在问题。
- en: As we know, the range of probabilities is from 0 to 1, and we cannot exceed
    this range. So we need to find a way to constrain our predicted output to this
    range.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 我们知道，概率的范围是从0到1，我们不能超出这个范围。所以我们需要找到一种方法来将预测输出限制在这个范围内。
- en: To solve this issue, we can pass our linear regression equation through a super
    cool machine called a **sigmoid function**. This machine transforms our predicted
    values to fall between 0 and 1\. We input our z value (where z = β₀ + β₁size)
    into the machine…
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这个问题，我们可以通过一个非常酷的机器——**sigmoid函数**，来处理我们的线性回归方程。这个机器将我们的预测值转换到0和1之间。我们将我们的z值（其中z
    = β₀ + β₁size）输入到机器中……
- en: '![](../Images/a47a8ad759320703836158194e3e49e2.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a47a8ad759320703836158194e3e49e2.png)'
- en: …and out comes a fancy-looking new equation that will fit our probability constraints.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: ……然后出现了一个看起来很高级的新方程，它将符合我们的概率约束。
- en: 'NOTE: The **e** in the output is a constant value and is approximately equal
    to 2.718.'
  id: totrans-40
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 注意：输出中的**e**是一个常数值，大约等于2.718。
- en: A math-ier way of representing the `sigmoid function:`
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 一种更数学化的方式来表示`sigmoid函数`：
- en: '![](../Images/74cdf30f641d27bb80c161fe77e2ccd7.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/74cdf30f641d27bb80c161fe77e2ccd7.png)'
- en: If we plot this, we see that the sigmoid function squeezes the straight line
    into an s-shaped curve confined between 0 and 1.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们绘制这个公式，我们会看到sigmoid函数将直线挤压成一个S形曲线，限制在0和1之间。
- en: '![](../Images/b4fb44ee2d49cf45e2a1ec4d21be1e97.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b4fb44ee2d49cf45e2a1ec4d21be1e97.png)'
- en: 'Optional note for all my math-heads: You might be wondering why and how we
    used the sigmoid function to get our desired output. Let’s break it down.'
  id: totrans-45
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 对于所有数学爱好者的可选说明：你可能在想我们为什么以及如何使用sigmoid函数来获得期望的输出。让我们来详细解释一下。
- en: ''
  id: totrans-46
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: We started with the incorrect assumption that using the linear regression formula
    will give us our desired probability.
  id: totrans-47
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 我们开始时错误地假设使用线性回归公式会给我们所需的概率。
- en: '![](../Images/5f064f58093036f48cd276d15f4f5939.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5f064f58093036f48cd276d15f4f5939.png)'
- en: The issue with this assumption is that (β₀ + β₁size) has range (-∞,+∞) and p
    has a range of [0,1]. So we need to find a value that has a range that matches
    that of (β₀ + β₁size).
  id: totrans-49
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 这个假设的问题在于(β₀ + β₁size)的范围是(-∞,+∞)，而p的范围是[0,1]。所以我们需要找到一个范围与(β₀ + β₁size)相匹配的值。
- en: ''
  id: totrans-50
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: To overcome this issue, we can equate the line to“log odds” (watch [this video](https://www.youtube.com/watch?v=ARfXDSkQf1Y)
    to understand log odds better) because we know that the log odds has a range of
    (-∞,+∞).
  id: totrans-51
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 为了解决这个问题，我们可以将直线等同于“对数几率”（观看[这个视频](https://www.youtube.com/watch?v=ARfXDSkQf1Y)以更好地理解对数几率），因为我们知道对数几率的范围是(-∞,+∞)。
- en: '![](../Images/8f6692ca17ad1202fcc58b922b18844b.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8f6692ca17ad1202fcc58b922b18844b.png)'
- en: Now that we did that, it’s just a matter of rearranging this equation, so that
    we find what the **p** value should equal.
  id: totrans-53
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 既然我们完成了这个步骤，只需重新排列这个方程，以找到**p**值应该等于多少。
- en: '![](../Images/5fbbd4598163dbbe98a40bc732fed798.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5fbbd4598163dbbe98a40bc732fed798.png)'
- en: Now that we know how to modify the linear regression line so that it fits our
    output constraints, we can return to our original problem.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们知道如何修改线性回归线以适应我们的输出约束，我们可以回到最初的问题。
- en: We need to determine the optimal curve for our dataset. To achieve this, we
    need to identify the optimal values for β₀ and β₁ (because these are the only
    values in the predicted probability equation that will change the shape of the
    curve).
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要为我们的数据集确定最佳曲线。为此，我们需要确定β₀和β₁的最佳值（因为这些是预测概率方程中会改变曲线形状的唯一值）。
- en: Similar to linear regression, we will leverage a cost function and the gradient
    descent algorithm to obtain suitable values for these coefficients. The key distinction,
    however, is that we will not be employing the [**MSE** cost function used in linear
    regression](https://medium.com/towards-data-science/back-to-basics-part-uno-linear-regression-cost-function-and-gradient-descent-590dcb3eee46#e9d3).
    Instead, we will be using a different cost function called **Log Loss**, which
    we will explore in greater detail shortly.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于线性回归，我们将利用一个成本函数和梯度下降算法来获取这些系数的合适值。然而，关键区别在于我们不会使用[**均方误差**成本函数](https://medium.com/towards-data-science/back-to-basics-part-uno-linear-regression-cost-function-and-gradient-descent-590dcb3eee46#e9d3)，而是使用一个称为**对数损失**的不同成本函数，我们将在稍后深入探讨。
- en: 'Say we used gradient descent and the **Log Loss** cost ([using these steps](https://medium.com/towards-data-science/back-to-basics-part-dos-linear-regression-cost-function-and-gradient-descent-e3d7d05c56fd))
    to find that our optimal values are β₀ = -120.6 and β₁ = 0.051, then our predicted
    probability equation will be:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们使用梯度下降和**对数损失**成本（[使用这些步骤](https://medium.com/towards-data-science/back-to-basics-part-dos-linear-regression-cost-function-and-gradient-descent-e3d7d05c56fd)）发现我们的最佳值为β₀
    = -120.6和β₁ = 0.051，那么我们预测的概率方程将是：
- en: '![](../Images/232b8ea908ccd4ac1bae77d7410152d0.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/232b8ea908ccd4ac1bae77d7410152d0.png)'
- en: 'And the corresponding optimal curve is:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 对应的最佳曲线是：
- en: '![](../Images/58fa4bdd05234712432a9ffb7c9a1236.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/58fa4bdd05234712432a9ffb7c9a1236.png)'
- en: With this new curve, we can now tackle Mark’s problem. By looking at it, we
    can see that a house with a size of 2400 feet²…
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这个新曲线，我们现在可以解决马克的问题。通过观察，我们可以看到一个大小为2400平方英尺的房子……
- en: '![](../Images/ccae1eadc01887044c62c6383efe6f5d.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ccae1eadc01887044c62c6383efe6f5d.png)'
- en: …has a predicted probability of approximately 78%. Therefore, we can tell Mark
    not to worry because it looks like his house is pretty likely to sell.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: …的预测概率大约为78%。因此，我们可以告诉马克不必担心，因为他的房子很可能会卖出去。
- en: 'We can further enhance our approach by developing a **Classification Algorithm**.
    A classification algorithm is commonly used in machine learning to categorize
    data into categories. In our case, we have two categories: houses that will sell
    and houses that will not sell.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过开发一个**分类算法**来进一步提升我们的方法。分类算法通常用于机器学习中，将数据分类到不同类别。在我们的情况下，我们有两个类别：会卖出的房子和不会卖出的房子。
- en: To develop a classification algorithm, we need to define a threshold probability
    value. This threshold probability value separates the predicted probabilities
    into two categories, “yes, the house will sell” and “no, the house will not sell.”
    Typically, 50% (or 0.5) is used as the threshold value.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 要开发分类算法，我们需要定义一个阈值概率。这个阈值概率将预测的概率分为两个类别，“是的，房屋会出售”和“不是，房屋不会出售”。通常，50%（或0.5）被用作阈值。
- en: If the predicted probability for a house size is above 50%, it will be classified
    as “will sell,” and if it’s below 50%, it will be classified as “won’t sell.”
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 如果对房屋面积的预测概率超过50%，它将被分类为“会出售”，如果低于50%，则被分类为“不会出售”。
- en: '![](../Images/bc471fc2fa66dbd718d80da68d68ff7a.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bc471fc2fa66dbd718d80da68d68ff7a.png)'
- en: And that’s about it. That’s how we can use logistic regression to solve our
    problem. Now let’s understand the cost function we used to find optimal values
    for logistic regression.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 就是这样。这就是我们如何使用逻辑回归来解决问题。现在让我们理解我们用来找到逻辑回归最优值的成本函数。
- en: Cost Function
  id: totrans-70
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 成本函数
- en: In linear regression, the cost is based on how far off the line was from our
    data points. And, in logistic regression, the cost function depends on how far
    off our predictions are from our actual data, *given that we are dealing with
    probabilities*.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在线性回归中，成本是基于直线与数据点之间的偏差。而在逻辑回归中，成本函数取决于我们的预测与实际数据之间的偏差，*考虑到我们处理的是概率*。
- en: If we used the [**MSE** cost function](/back-to-basics-part-uno-linear-regression-cost-function-and-gradient-descent-590dcb3eee46#e9d3)
    (like we did in linear regression) in logistic regression, we would end up with
    a ***non-convex*** (fancy term for a *not-so-pretty-curve-that-can’t-be-used-effectively-in-gradient-decsent*)
    [cost function curve that can be difficult to optimize.](https://medium.com/towards-data-science/back-to-basics-part-dos-linear-regression-cost-function-and-gradient-descent-e3d7d05c56fd#6754)
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们在逻辑回归中使用了[**MSE**成本函数](/back-to-basics-part-uno-linear-regression-cost-function-and-gradient-descent-590dcb3eee46#e9d3)（就像我们在线性回归中所做的那样），我们将得到一个***非凸***（一种*不那么漂亮的曲线，不能有效用于梯度下降*）[难以优化的成本函数曲线。](https://medium.com/towards-data-science/back-to-basics-part-dos-linear-regression-cost-function-and-gradient-descent-e3d7d05c56fd#6754)
- en: '![](../Images/67ffabe81bd7f9aac79c1914332dfacf.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/67ffabe81bd7f9aac79c1914332dfacf.png)'
- en: And as you may recall from our discussion on gradient descent, it is much easier
    to optimize a ***convex*** (aka *a curve with a distinct minimum point*) curve
    like this than a non-convex curve.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你可能从我们的梯度下降讨论中记得的那样，优化一个***凸***（即*具有明显最小点的曲线*）曲线比优化一个非凸曲线要容易得多。
- en: '![](../Images/04170f51d7864ff48bece76cd841b004.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/04170f51d7864ff48bece76cd841b004.png)'
- en: To achieve a convex cost function curve, we use a cost function called **Log
    Loss**.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现一个凸的成本函数曲线，我们使用一个称为**Log Loss**的成本函数。
- en: To break down the **Log Loss** cost function, we need to define separate costs
    for when the house actually sold (y=1) and when it did not (y=0).
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 要详细解析**Log Loss**成本函数，我们需要为房屋实际出售（y=1）和未出售（y=0）分别定义成本。
- en: If `y = 1` and we predicted 1 (i.e., 100% probability it sold), there is no
    penalty. However, if we predicted 0 (i.e., 0% probability it didn’t sell), then
    we get penalized heavily.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 如果`y = 1`且我们预测为1（即，100%概率出售），则没有惩罚。然而，如果我们预测为0（即，0%概率未出售），则会受到严重惩罚。
- en: '![](../Images/af708abbd0e008d4369c95bef4cbeaa2.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/af708abbd0e008d4369c95bef4cbeaa2.png)'
- en: Similarly, if `y = 0` and we predicted a high probability of the house selling,
    we should be penalized heavily, and if we predicted a low probability of the house
    selling, there should be a lower penalty. The more off we are, the more it costs
    us.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 同样地，如果`y = 0`且我们预测房屋出售的概率很高，我们应受到严重惩罚；如果我们预测房屋出售的概率很低，则应受到较低的惩罚。偏差越大，成本越高。
- en: '![](../Images/00788ee7542440a2f13f22c6f10da771.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/00788ee7542440a2f13f22c6f10da771.png)'
- en: 'To compute the cost for all houses in our dataset, we can average the costs
    of all the individual predictions like this:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 要计算我们数据集中所有房屋的成本，我们可以像这样对所有单个预测的成本进行平均：
- en: '![](../Images/811f835e3c26340d6b3ddaffd6463b18.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/811f835e3c26340d6b3ddaffd6463b18.png)'
- en: By cleverly rewriting the two equations, we can combine them into one to give
    us our **Log Loss** cost function.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 通过巧妙地重写这两个方程，我们可以将它们合并成一个方程，以得到我们的**Log Loss**成本函数。
- en: '![](../Images/8bb50be56791d1f4c9c62e61b0be49f6.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8bb50be56791d1f4c9c62e61b0be49f6.png)'
- en: This works because one of those two will always be zero, so only the other one
    will be used.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 这是有效的，因为这两个值中总有一个为零，因此只使用另一个值。
- en: 'And the combined cost graph looks like this:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 结合的成本图看起来是这样的：
- en: '![](../Images/317d3fd6fb24d0d7ed7747b5c2bcee1d.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/317d3fd6fb24d0d7ed7747b5c2bcee1d.png)'
- en: Now that we have a good understanding of the math and intuition behind logistic
    regression, let’s see how Mark’s house size problem can be implemented in Python.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们已经很好地理解了逻辑回归背后的数学和直觉，我们来看看如何在 Python 中实现 Mark 的房屋面积问题。
- en: And we’re done! You have everything you need to tackle a logistic regression
    problem of your own now.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 完成了！现在你拥有了解决逻辑回归问题所需的一切。
- en: And as always, please feel free to reach out to me on [LinkedIn](https://www.linkedin.com/in/shreyarao24/)
    or shoot me an email at *shreya.statistics@gmail.com*.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 以及一如既往，如果有任何问题，请随时通过[LinkedIn](https://www.linkedin.com/in/shreyarao24/)联系我，或者发邮件到
    *shreya.statistics@gmail.com*。
