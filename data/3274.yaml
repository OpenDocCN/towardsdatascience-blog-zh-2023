- en: Cracking the Code LLMs
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 破解代码LLMs
- en: 原文：[https://towardsdatascience.com/cracking-the-code-llms-354505c53295?source=collection_archive---------4-----------------------#2023-11-03](https://towardsdatascience.com/cracking-the-code-llms-354505c53295?source=collection_archive---------4-----------------------#2023-11-03)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/cracking-the-code-llms-354505c53295?source=collection_archive---------4-----------------------#2023-11-03](https://towardsdatascience.com/cracking-the-code-llms-354505c53295?source=collection_archive---------4-----------------------#2023-11-03)
- en: How code LLMs progressed from RNNs to Transformers
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 代码LLM如何从RNN发展到Transformer
- en: '[](https://medium.com/@agarwal.shubham166?source=post_page-----354505c53295--------------------------------)[![Shubham
    Agarwal](../Images/4a41b8e70829943cd91d1104424f9ce5.png)](https://medium.com/@agarwal.shubham166?source=post_page-----354505c53295--------------------------------)[](https://towardsdatascience.com/?source=post_page-----354505c53295--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----354505c53295--------------------------------)
    [Shubham Agarwal](https://medium.com/@agarwal.shubham166?source=post_page-----354505c53295--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@agarwal.shubham166?source=post_page-----354505c53295--------------------------------)[![Shubham
    Agarwal](../Images/4a41b8e70829943cd91d1104424f9ce5.png)](https://medium.com/@agarwal.shubham166?source=post_page-----354505c53295--------------------------------)[](https://towardsdatascience.com/?source=post_page-----354505c53295--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----354505c53295--------------------------------)
    [Shubham Agarwal](https://medium.com/@agarwal.shubham166?source=post_page-----354505c53295--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fdd67f0fc7318&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcracking-the-code-llms-354505c53295&user=Shubham+Agarwal&userId=dd67f0fc7318&source=post_page-dd67f0fc7318----354505c53295---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----354505c53295--------------------------------)
    ·7 min read·Nov 3, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F354505c53295&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcracking-the-code-llms-354505c53295&user=Shubham+Agarwal&userId=dd67f0fc7318&source=-----354505c53295---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fdd67f0fc7318&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcracking-the-code-llms-354505c53295&user=Shubham+Agarwal&userId=dd67f0fc7318&source=post_page-dd67f0fc7318----354505c53295---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----354505c53295--------------------------------)
    ·7分钟阅读·2023年11月3日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F354505c53295&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcracking-the-code-llms-354505c53295&user=Shubham+Agarwal&userId=dd67f0fc7318&source=-----354505c53295---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F354505c53295&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcracking-the-code-llms-354505c53295&source=-----354505c53295---------------------bookmark_footer-----------)![](../Images/95d03e0d607d3c3e4f897b86c3482843.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F354505c53295&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcracking-the-code-llms-354505c53295&source=-----354505c53295---------------------bookmark_footer-----------)![](../Images/95d03e0d607d3c3e4f897b86c3482843.png)'
- en: Photo by [Markus Spiske](https://unsplash.com/@markusspiske?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 照片由 [Markus Spiske](https://unsplash.com/@markusspiske?utm_source=medium&utm_medium=referral)
    提供，发布在 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: Introduction
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: Recent years have seen remarkable evolution of language models with the introduction
    of Transformers, which has revolutionized the way we perform our daily tasks like
    writing emails, creating documentations, searching the web and even the way we
    code. With researchers applying Large Language Models in code intelligence tasks,
    a new field of **Neural Code Intelligence** has emerged. This domain aims at improving
    programming efficiency and minimizing human errors in the software industry by
    solving tasks like code summarization, generation and translation.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，语言模型经历了显著的演变，尤其是Transformer的引入，这一变革彻底改变了我们处理日常任务的方式，如写电子邮件、创建文档、搜索网络，甚至编程。随着研究人员将大型语言模型应用于代码智能任务，一个新的领域——**神经代码智能**应运而生。这个领域旨在通过解决代码总结、生成和翻译等任务，提高编程效率，减少软件行业中的人为错误。
- en: With the latest release of Code Llama, the state of art model by Meta AI for
    code generation and understanding, this article looks back at the evolution of
    Large Language Models (LLMs) for Code, from RNNs to Transformers.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: Code Llama 的最新发布，Meta AI 开发的用于代码生成和理解的先进模型，本文回顾了从 RNN 到 Transformers 的大型语言模型（LLMs）在代码领域的演变历程。
- en: '![](../Images/7cf34459d21a523b15464a4fd87d17ea.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7cf34459d21a523b15464a4fd87d17ea.png)'
- en: 'Fig-1: A timeline for Large Language Models For Code. Image by Author.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: Fig-1：大型语言模型（LLMs）在代码领域的时间线。图片由作者提供。
- en: Code2Vec, 2018
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Code2Vec，2018
- en: This was one of the first attempts for language models to understand code. [Code2Vec](https://arxiv.org/pdf/1803.09473.pdf)
    aimed at representing code snippets into embeddings. These embeddings capture
    semantic and structural information from the code, making them useful for various
    software engineering tasks such as code classification, retrieval, and understanding.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 这是语言模型首次尝试理解代码的一个尝试。[Code2Vec](https://arxiv.org/pdf/1803.09473.pdf)旨在将代码片段表示为嵌入。这些嵌入捕获了代码的语义和结构信息，使它们在代码分类、检索和理解等各种软件工程任务中非常有用。
- en: Model tries to predict the method name from the code-snippet, by encoding well-named
    tokens and AST (Abstract Syntax Tree) paths, and applying neural attention for
    aggregation into fixed length vector representation.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 模型尝试从代码片段中预测方法名称，通过编码良好命名的标记和抽象语法树（AST）路径，并应用神经注意力将它们聚合成固定长度的向量表示。
- en: '![](../Images/1df35ea57f0c7130e459d84b7c3dd9d5.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1df35ea57f0c7130e459d84b7c3dd9d5.png)'
- en: 'Fig-2: Code2Vec Model Architecture: Program is first decomposed into Bag of
    Context which includes tokens and AST paths, then through fully connected layer
    and attention layer to generate code-vector.Image inspired from the original paper
    by Uri Alon et. al from [Code2Vec](https://arxiv.org/pdf/1803.09473.pdf)'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: Fig-2：Code2Vec 模型架构：程序首先被分解为上下文袋，其中包括标记和AST路径，然后通过完全连接层和注意力层生成代码向量。图片灵感来自Uri
    Alon等人的原始论文[Code2Vec](https://arxiv.org/pdf/1803.09473.pdf)
- en: '**Training Set:** 14M Java Program Examples'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '**训练集**：1400万个 Java 程序示例'
- en: '**Model Architecture**: RNN + Feed-Forward Network'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '**模型架构**：RNN + 前馈网络'
- en: '**Novelty**:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '**创新**：'
- en: '**Path-based Attention Mode**l- The authors propose a novel neural network
    architecture that uses syntactic paths in the Abstract Syntax Tree (AST) of a
    code snippet as input features. The model learns to assign different attention
    weights to each path, and to aggregate them into a single code vector. The code
    vector can then be used to predict the label distribution for the snippet, or
    to measure similarity and analogy between snippets.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基于路径的注意力模型** - 作者提出了一种新颖的神经网络架构，使用代码片段的抽象语法树（AST）中的句法路径作为输入特征。模型学习为每个路径分配不同的注意力权重，并将它们聚合成单一的代码向量。代码向量可以用来预测片段的标签分布，或者衡量片段之间的相似性和类比。'
- en: You can play with the model [here](https://code2vec.org/)
  id: totrans-23
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 您可以在[这里](https://code2vec.org/)尝试该模型。
- en: CodeBERT, 2020
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: CodeBERT，2020
- en: '[CodeBERT](https://arxiv.org/pdf/2002.08155.pdf), developed by Microsoft Research
    team, represents a significant advancement in the realm of Large Language Models
    (LLMs) for code by introducing multimodal data pre-training, combining Natural
    Language and Programming Language (NL + PL) on the Transformer based [BERT model](/bert-explained-state-of-the-art-language-model-for-nlp-f8b21a9b6270).
    The model is trained on a diverse dataset comprising both bimodal data points
    pair and unimodal data points for [Masked Language Modeling (MLM)](/masked-language-modelling-with-bert-7d49793e5d2c)
    and [Replaced Token Detection (RTD)](https://arxiv.org/pdf/2003.10555.pdf) tasks.
    CodeBERT demonstrated exceptional performance in a variety of domains, excelling
    notably in natural language code search and code to documentation generation.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '[CodeBERT](https://arxiv.org/pdf/2002.08155.pdf)，由微软研究团队开发，代表了大型语言模型（LLMs）在代码领域的重大进展，引入了多模态数据预训练，结合了基于Transformer的BERT模型中的自然语言和编程语言（NL
    + PL）。该模型在包括双模数据点对和单模数据点的多样数据集上进行训练，用于掩码语言建模（MLM）和替换令牌检测（RTD）任务。CodeBERT 在多个领域展现出了卓越的性能，特别在自然语言代码搜索和代码到文档生成方面表现突出。'
- en: '![](../Images/f83b83f3dbdba19b780af10b88fa345a.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f83b83f3dbdba19b780af10b88fa345a.png)'
- en: 'Fig-3: CodeBERT model pre-training using Replace Token Detection (RTD) task.
    Natural Language Generation and Code Generator replacing tokens with a different
    token, and the CodeBERT model is trained to classify each token as replaced or
    original. Image from Feng et. al, [CodeBERT](https://arxiv.org/pdf/2002.08155.pdf)'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 图-3：CodeBERT模型预训练使用替换标记检测（RTD）任务。自然语言生成和代码生成器将标记替换为不同的标记，CodeBERT模型被训练以分类每个标记是被替换的还是原始的。图片来自Feng等人，[CodeBERT](https://arxiv.org/pdf/2002.08155.pdf)
- en: '**Training Dataset:** [Codesearch Net Dataset](https://paperswithcode.com/dataset/codesearchnet)-
    2.1M bimodal Data points (NL + PL), 6.4M Unimodal Data Points (6 languages — Python,
    Java, Javascript, PHP, Ruby, Go)'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '**训练数据集：** [Codesearch Net 数据集](https://paperswithcode.com/dataset/codesearchnet)
    - 210万双模态数据点（自然语言 + 编程语言），640万单模态数据点（6种语言 — Python、Java、Javascript、PHP、Ruby、Go）'
- en: '**Parameter Size:** 125M'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '**参数规模：** 125M'
- en: '**Model Architecture:** RoBERTa-base'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '**模型架构：** RoBERTa-base'
- en: '**Novelty:**'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '**新颖性：**'
- en: '**Bimodal Training**: CodeBERT introduces an innovative training approach that
    encompasses both Natural Language and Programming Language tokens. This bimodal
    training technique enhances the model’s ability to understand and generate code
    by considering the intricate interplay between human-readable descriptions and
    programming language elements.'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**双模态训练：** CodeBERT引入了一种创新的训练方法，涵盖了自然语言和编程语言标记。这种双模态训练技术通过考虑人类可读描述与编程语言元素之间的复杂互动，增强了模型理解和生成代码的能力。'
- en: '**Replace Token Detection (RTD) Task for code**: CodeBERT pre-training used
    Replace Token Detection (RTD) instead of Next Sentence Prediction(NSP) which showed
    superior performance.'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**代码的替换标记检测（RTD）任务：** CodeBERT预训练使用了替换标记检测（RTD），而不是下一句预测（NSP），这显示了更好的性能。'
- en: Codex, 2021
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Codex，2021
- en: '[Codex](https://arxiv.org/abs/2107.03374) was one of the first successful Code
    LLM to generate code from doc-string or Natural language prompts with high accuracy,
    and predecessor of widely used [Github Copilot](https://github.com/features/copilot).
    Developed by the OpenAI team, Codex uses [GPT3](/understanding-gpt-3-in-5-minutes-7fe35c3a1e52)
    architecture & tokenizer, and pre-trains on a large corpus of Github code. This
    Large Language model has 12B parameters, and was a state-of-art model in 2021,
    which showed best performance on [human-eval dataset](https://github.com/openai/human-eval)
    by solving 28.8% of the problems at first pass.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '[Codex](https://arxiv.org/abs/2107.03374)是首批成功的代码LLM之一，能够从文档字符串或自然语言提示中生成高准确度的代码，是广泛使用的[Github
    Copilot](https://github.com/features/copilot)的前身。由OpenAI团队开发，Codex使用了[GPT3](/understanding-gpt-3-in-5-minutes-7fe35c3a1e52)架构和分词器，并在大量的GitHub代码库上进行预训练。这个大型语言模型有12B参数，在2021年是最先进的模型，首次通过率在[human-eval数据集](https://github.com/openai/human-eval)上解决了28.8%的问题。'
- en: Further fine-tuning of the model on standalone python functions (rather than
    whole code which include configs, class implementations etc.), showed significant
    improvement, and was able to solve **37.7% of the human-eval dataset** problem.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在独立Python函数上的进一步微调（而不是包括配置、类实现等的完整代码），显示了显著的改进，并能够解决**37.7%的human-eval数据集**问题。
- en: '![](../Images/57281f6d8acb2cde891466393c1f6354.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/57281f6d8acb2cde891466393c1f6354.png)'
- en: 'Fig-4: A decoder only Transformer architecture used for Codex GPT model. Image
    inspired from original [Transformer paper](https://arxiv.org/abs/1706.03762) by
    Vaswani et. al.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 图-4：用于Codex GPT模型的仅解码器Transformer架构。图像灵感来自Vaswani等人原始的[Transformer论文](https://arxiv.org/abs/1706.03762)。
- en: '**Training Dataset:** 159GB of python files from 54M Github Repositories.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '**训练数据集：** 159GB来自54M GitHub代码库的Python文件。'
- en: '**Parameter Size:** 12B (Codex- 12B)'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '**参数规模：** 12B（Codex-12B）'
- en: '**Model Architecture:** GPT3'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '**模型架构：** GPT3'
- en: '**Novelty:**'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '**新颖性：**'
- en: One of the first successful models which excelled in code-writing capabilities
    from Natural language prompts. This trains GPT-3 models on a large corpus of Github
    repositories.
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这是首批成功的模型之一，擅长从自然语言提示中生成代码。该模型在大量的GitHub代码库上进行训练。
- en: Authors of this model also created a new dataset, **“**[**HumanEval**](https://github.com/openai/human-eval)**”**
    to benchmark models for code-generation tasks. This dataset consists of 164 hand-written
    programming problems with unit tests.
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 该模型的作者还创建了一个新的数据集，**“**[**HumanEval**](https://github.com/openai/human-eval)**”**，用于对代码生成任务进行基准测试。该数据集包含164个手写编程问题及单元测试。
- en: Try Codex Model at OpenAI Playground [here](https://platform.openai.com/playground)
  id: totrans-45
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 在OpenAI Playground [这里](https://platform.openai.com/playground)尝试Codex模型
- en: CodeT5, 2021
  id: totrans-46
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: CodeT5，2021
- en: '[Code-T5](https://arxiv.org/pdf/2109.00859.pdf) is an **encoder-decoder model**
    based on the T5 architecture, distinct from both CodeBERT (encoder-only) and Codex
    (decoder-only) models. It introduces a unique identifier-aware denoising pre-training
    task which helps the model distinguish and recover identifiers in code, enhancing
    its understanding of structure.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '[Code-T5](https://arxiv.org/pdf/2109.00859.pdf) 是一种基于 T5 架构的**编码器-解码器模型**，与
    CodeBERT（仅编码器）和 Codex（仅解码器）模型不同。它引入了一种独特的标识符感知去噪预训练任务，帮助模型区分和恢复代码中的标识符，增强对结构的理解。'
- en: Code-T5 excels in various tasks such as Code Defect Detection, Clone Detection,
    Code Translation, and Refinement, through multi-task learning, requiring less
    data for quicker fine-tuning. However, it uses CodeBleu scores for evaluation
    rather than benchmarking against the HumanEval dataset.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: Code-T5 通过多任务学习，在代码缺陷检测、克隆检测、代码翻译和优化等任务中表现出色，所需数据较少，可以更快进行微调。然而，它使用 CodeBleu
    分数进行评估，而不是与 HumanEval 数据集进行基准测试。
- en: '![](../Images/25c86d7d4052e417c982092de6b81ab7.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/25c86d7d4052e417c982092de6b81ab7.png)'
- en: 'Fig-5: Illustration to show how CodeT5 excels in various code understanding
    and generation tasks. Image taken from Paper by Wang et al, [CodeT5](https://arxiv.org/pdf/2109.00859.pdf)'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 图-5：展示 CodeT5 在各种代码理解和生成任务中的卓越表现。图像来源于 Wang 等人的论文，[CodeT5](https://arxiv.org/pdf/2109.00859.pdf)
- en: '**Training Dataset**: [Codesearch Net Dataset](https://paperswithcode.com/dataset/codesearchnet)
    (Same as CodeBERT)'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '**训练数据集**：[Codesearch Net 数据集](https://paperswithcode.com/dataset/codesearchnet)（与
    CodeBERT 相同）'
- en: '**Parameter Size**: 220M'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '**参数大小**：220M'
- en: '**Model Architecture**: [T5](https://jmlr.org/papers/volume21/20-074/20-074.pdf)
    (Encoder-Decoder Architecture)'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '**模型架构**：[T5](https://jmlr.org/papers/volume21/20-074/20-074.pdf)（编码器-解码器架构）'
- en: '**Novelty:**'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '**新颖性：**'
- en: '**Encoder-Decoder Mode**l: One of the first Encoder-Decoder Code LLM to support
    both code-understanding and code-generation tasks.'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**编码器-解码器模式**：首批支持代码理解和代码生成任务的编码器-解码器代码大模型之一。'
- en: Proposes a novel pre-training objective **identifier-aware denoising,** which
    learns token-type information and structure of the code. This approach trains
    models to differentiate between identifiers (variable names, function names) from
    PL keywords (like if, while etc.), and also recovers them when they are masked.
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提出了新颖的预训练目标**标识符感知去噪**，学习代码的标记类型信息和结构。这种方法训练模型区分标识符（变量名、函数名）和 PL 关键字（如 if、while
    等），并在它们被掩盖时恢复它们。
- en: '**Multi-Task Learning in Fine Tuning stage**: Fine-tunes on various Code related
    tasks simultaneously like Code Defect Detection, Clone Detection, Code Translation,
    Refinement etc.'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**微调阶段的多任务学习**：同时对各种代码相关任务进行微调，如代码缺陷检测、克隆检测、代码翻译、优化等。'
- en: PLBart, 2021
  id: totrans-58
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: PLBart, 2021
- en: '[PLBART, or Program and Language BART](https://arxiv.org/pdf/2103.06333.pdf),
    model leverages the BART model architecture to automate a range of software engineering
    tasks, encompassing code summarization, generation, and translation under the
    umbrella of PLUG (Program and Language Understanding and Generation).'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '[PLBART，即程序和语言 BART](https://arxiv.org/pdf/2103.06333.pdf) 模型利用 BART 模型架构来自动化一系列软件工程任务，包括代码摘要、生成和翻译，属于
    PLUG（程序和语言理解与生成）的范畴。'
- en: It introduces a denoising sequence-to-sequence modeling approach for enhanced
    Program and Language understanding, strategically combining the strengths of BERT
    and GPT models. This is achieved by combining a bidirectional encoder with an
    autoregressive decoder, allowing for a more comprehensive grasp of context and
    a versatile generation process. The model employs three denoising strategies,
    including **token masking, token deletion, and token infilling**, to train and
    fine-tune its capabilities effectively.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 它引入了一种去噪序列到序列建模方法，以增强程序和语言理解，战略性地结合了 BERT 和 GPT 模型的优势。这通过将双向编码器与自回归解码器相结合，实现了对上下文的更全面理解和多功能生成过程。该模型采用了三种去噪策略，包括**标记掩盖、标记删除和标记填充**，有效地训练和微调其能力。
- en: '![](../Images/7cf67533f432400c1dc4966c5d73d9a4.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7cf67533f432400c1dc4966c5d73d9a4.png)'
- en: 'Fig-6: Illustration to visualize the BART model (used in PLBART too) architecture
    which has bidirectional encoder and autoregressive decoder. Image from original
    [BART paper](https://arxiv.org/pdf/1910.13461.pdf) by Lewis et. al.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 图-6：可视化 BART 模型（PLBART 也使用）架构的插图，具有双向编码器和自回归解码器。图像来自 Lewis 等人的原始 [BART 论文](https://arxiv.org/pdf/1910.13461.pdf)。
- en: '**Training Dataset**: 2M Java and Python Functions and their Natural Language
    descriptions collected from Github, Stackoverflow ([code](https://huggingface.co/docs/transformers/model_doc/plbart)).'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '**训练数据集**：从Github、Stackoverflow收集的2M Java和Python函数及其自然语言描述（[代码](https://huggingface.co/docs/transformers/model_doc/plbart)）。'
- en: '**Parameter Size**: 140M (6 encoder layer + 6 decoder layer + additional norm
    layer on encoder and decoder)'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '**参数规模**：140M（6个编码器层 + 6个解码器层 + 编码器和解码器上的附加归一化层）'
- en: '**Model Architecture**: [BART](https://arxiv.org/abs/1910.13461)'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '**模型架构**： [BART](https://arxiv.org/abs/1910.13461)'
- en: '**Novelty**:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '**新颖性**：'
- en: '**Denoising Auto-encoder Approach**: Employs a denoising auto-encoder approach,
    which enhances code understanding and generation by effectively utilizing the
    bidirectional and auto-regressive properties of both the encoder and decoder,
    combining the strengths of BERT and GPT models.'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**去噪自编码器方法**：采用去噪自编码器方法，通过有效利用编码器和解码器的双向和自回归特性，结合BERT和GPT模型的优势，增强了代码理解和生成能力。'
- en: '**Diverse Noising Strategies**: Proposes multiple denoising strategies, such
    as token masking, token deletion, and token infilling. This diversity in noising
    techniques enhances the model’s robustness and effectiveness in learning from
    noisy data, contributing to improved code understanding and generation.'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**多样的去噪策略**：提出多种去噪策略，如标记掩蔽、标记删除和标记填充。这些去噪技术的多样性提高了模型在噪声数据中学习的鲁棒性和效果，促进了代码理解和生成的改进。'
- en: Not all models use the same benchmark for evaluating the performance. PLBART
    authors don’t evaluate model performance on HumanEval, dataset used by majority
    of other models for benchmarking.
  id: totrans-69
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 不是所有模型都使用相同的基准来评估性能。PLBART的作者没有在HumanEval上评估模型性能，而其他大多数模型则使用该数据集进行基准测试。
- en: Code Llama, 2023
  id: totrans-70
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Code Llama，2023
- en: '[Code Llama](https://arxiv.org/abs/2308.12950) is the latest Code LLM, released
    by Meta, which beats all the existing open-source models in several benchmark
    datasets. It scores 53% on [HumanEval Dataset](https://github.com/openai/human-eval)
    and 55% on MBPP dataset (only GPT-4 has better performance). These gains can be
    attributed to longer context length of 16K (4x of Llama2) and training pre-trained
    Llama 2 on extra 500B tokes from Program and Natural Language.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '[Code Llama](https://arxiv.org/abs/2308.12950)是Meta发布的最新Code LLM，在多个基准数据集上超过了所有现有的开源模型。它在[HumanEval数据集](https://github.com/openai/human-eval)上的得分为53%，在MBPP数据集上的得分为55%（只有GPT-4表现更好）。这些进步归因于16K的较长上下文长度（是Llama2的4倍）和在程序和自然语言上额外训练的500B标记的预训练Llama
    2。'
- en: This model is suited best for Code Generation and Infilling tasks, and can act
    as best copilot during IDE based Software Development. Code Llama models family
    has 3 types of models-
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型最适合代码生成和填充任务，并且在基于IDE的软件开发过程中可以作为最佳的副驾驶。Code Llama模型家族有3种类型的模型—
- en: Code Llama
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Code Llama
- en: Code Llama Python
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Code Llama Python
- en: Code Llama-Instruct
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Code Llama-Instruct
- en: each of them coming in 3 sizes — **7B, 13B and 34B**
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 每种都有3种尺寸——**7B、13B和34B**
- en: '![](../Images/939f1d241185f9018c4f390209bdd46a.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/939f1d241185f9018c4f390209bdd46a.png)'
- en: 'Fig-7: Code Llama training and fine-tuning pipeline taking pre-trained Llama-2
    model as input. Image from original [Code Llama paper](https://arxiv.org/abs/2308.12950)
    by Rozière et. al.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 图-7：Code Llama训练和微调流程，以预训练的Llama-2模型为输入。图像来源于原始[Code Llama论文](https://arxiv.org/abs/2308.12950)。
- en: '**Training Dataset**: 500B tokens + additional 100B tokens for Code llama Python
    on publicly available code'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '**训练数据集**：500B标记 + 额外100B标记用于Code Llama Python，基于公开代码'
- en: '**Model Architecture**: Llama 2'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '**模型架构**：Llama 2'
- en: '**Parameter Size:** Available in 3 sizes — 7B, 13B and 34B.'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '**参数规模**：有3种尺寸——7B、13B和34B。'
- en: '**Novelty**:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '**新颖性**：'
- en: Proposed a fine-tuning step to handle long sequences called **Long Context Fine-Tuning**,
    which increases context length to 16,384 (4x from Llama 2 context length i.e.
    4096)
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提出了一个处理长序列的微调步骤，称为**长上下文微调**，将上下文长度增加到16,384（是Llama 2上下文长度4096的4倍）
- en: '**Instruction Fine Tuning & Self-Instruct**: One of the few models that performs
    instruction fine-tuning, which uses explicit instruction or prompts during the
    fine-tuning process. Instead of creating human feedback data which is expensive,
    authors propose a novel execution feedback approach to construct a self-instruction
    dataset.'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**指令微调和自我指令**：少数几个执行指令微调的模型之一，使用明确的指令或提示进行微调。作者提出了一种新颖的执行反馈方法来构建自我指令数据集，而不是创建昂贵的人类反馈数据。'
- en: Conclusion
  id: totrans-85
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: Andrej Karapathy, one of the founders of Open AI, recently called Transformers
    [the best idea in AI](https://www.youtube.com/watch?v=9uw3F6rndnA). He added that
    the transformer is like a general purpose differentiable computer which is simultaneously
    — expressive, optimizable and efficient ([X post](https://twitter.com/karpathy/status/1582807367988654081)).
    As evident with the transformation it has brought in the last 3–4 years, the Transformer
    model has vast potential to further change the landscape of how we code as a software
    engineer, and I think this is just the beginning.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: Open AI 的创始人之一**Andrej Karapathy**最近称 Transformer 是[人工智能领域的最佳创意](https://www.youtube.com/watch?v=9uw3F6rndnA)。他补充说，Transformer
    像是一种通用的可微分计算机，它同时具有——表现力、可优化性和高效性（[X 帖子](https://twitter.com/karpathy/status/1582807367988654081)）。从过去
    3-4 年 Transformer 带来的变革可以看出，Transformer 模型具有巨大潜力，可以进一步改变我们作为软件工程师编程的方式，我认为这只是一个开始。
- en: Follow me more!
  id: totrans-87
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 更多关注我吧！
- en: I am a Staff ML Engineer @ LinkedIn. You can follow me at [LinkedIn](https://www.linkedin.com/in/shubham166/)
    or [Twitter](https://twitter.com/ShubhhamAgarwal). You can reach out to me for
    quick chat at [Topmate.io](https://topmate.io/shubham_agarwal166)
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 我是 LinkedIn 的 Staff ML Engineer。你可以通过[LinkedIn](https://www.linkedin.com/in/shubham166/)或[Twitter](https://twitter.com/ShubhhamAgarwal)关注我。你也可以通过[Topmate.io](https://topmate.io/shubham_agarwal166)与我快速聊天。
