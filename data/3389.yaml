- en: All you need to know to Develop using Large Language Models
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 开发大型语言模型所需了解的一切
- en: 原文：[https://towardsdatascience.com/all-you-need-to-know-to-develop-using-large-language-models-5c45708156bc?source=collection_archive---------0-----------------------#2023-11-15](https://towardsdatascience.com/all-you-need-to-know-to-develop-using-large-language-models-5c45708156bc?source=collection_archive---------0-----------------------#2023-11-15)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/all-you-need-to-know-to-develop-using-large-language-models-5c45708156bc?source=collection_archive---------0-----------------------#2023-11-15](https://towardsdatascience.com/all-you-need-to-know-to-develop-using-large-language-models-5c45708156bc?source=collection_archive---------0-----------------------#2023-11-15)
- en: '![](../Images/98c22ee1ed98e7ee21069160e94b530b.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/98c22ee1ed98e7ee21069160e94b530b.png)'
- en: Image generated by Stable Diffusion
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 图像由 Stable Diffusion 生成
- en: Explaining in simple terms the core technologies required to start developing
    LLM-based applications.
  id: totrans-4
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 用简单的术语解释启动开发 LLM 基础应用所需的核心技术。
- en: '[](https://slgero.medium.com/?source=post_page-----5c45708156bc--------------------------------)[![Sergei
    Savvov](../Images/a653eaeeec954f1a71e6341b424f009a.png)](https://slgero.medium.com/?source=post_page-----5c45708156bc--------------------------------)[](https://towardsdatascience.com/?source=post_page-----5c45708156bc--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----5c45708156bc--------------------------------)
    [Sergei Savvov](https://slgero.medium.com/?source=post_page-----5c45708156bc--------------------------------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://slgero.medium.com/?source=post_page-----5c45708156bc--------------------------------)[![Sergei
    Savvov](../Images/a653eaeeec954f1a71e6341b424f009a.png)](https://slgero.medium.com/?source=post_page-----5c45708156bc--------------------------------)[](https://towardsdatascience.com/?source=post_page-----5c45708156bc--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----5c45708156bc--------------------------------)
    [Sergei Savvov](https://slgero.medium.com/?source=post_page-----5c45708156bc--------------------------------)'
- en: ·
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F227c6aaec11a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fall-you-need-to-know-to-develop-using-large-language-models-5c45708156bc&user=Sergei+Savvov&userId=227c6aaec11a&source=post_page-227c6aaec11a----5c45708156bc---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----5c45708156bc--------------------------------)
    ·12 min read·Nov 15, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F5c45708156bc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fall-you-need-to-know-to-develop-using-large-language-models-5c45708156bc&user=Sergei+Savvov&userId=227c6aaec11a&source=-----5c45708156bc---------------------clap_footer-----------)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F227c6aaec11a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fall-you-need-to-know-to-develop-using-large-language-models-5c45708156bc&user=Sergei+Savvov&userId=227c6aaec11a&source=post_page-227c6aaec11a----5c45708156bc---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----5c45708156bc--------------------------------)
    · 12 min read · 2023年11月15日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F5c45708156bc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fall-you-need-to-know-to-develop-using-large-language-models-5c45708156bc&user=Sergei+Savvov&userId=227c6aaec11a&source=-----5c45708156bc---------------------clap_footer-----------)'
- en: --
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F5c45708156bc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fall-you-need-to-know-to-develop-using-large-language-models-5c45708156bc&source=-----5c45708156bc---------------------bookmark_footer-----------)'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F5c45708156bc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fall-you-need-to-know-to-develop-using-large-language-models-5c45708156bc&source=-----5c45708156bc---------------------bookmark_footer-----------)'
- en: The purpose of this article is to explain in simple terms the key technologies
    necessary to start developing LLM-based applications. It is intended for software
    developers, data scientists and AI enthusiasts who have a basic understanding
    of machine learning concepts and want to dive deeper. The article also provides
    numerous useful links for further study. It’s going to be interesting!
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 本文的目的是用简单的术语解释开始开发基于 LLM 的应用程序所需的关键技术。它面向对机器学习概念有基本了解并希望深入学习的软件开发人员、数据科学家和人工智能爱好者。文章还提供了许多有用的链接供进一步学习。内容会很有趣！
- en: 1\. Introduction to Large Language Models (LLMs)
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 1. 大型语言模型（LLM）简介
- en: 'I think you’ve already heard a thousand times about what an LLM is, so I won’t
    overload you with it. All we need to know is: a Large Language Model (LLM) is
    a **LARGE** neural network model that predicts the next token based on the previously
    predicted one. That’s all.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 我想你已经听了无数次关于 LLM 是什么，所以我不会再过多赘述。我们只需知道：大型语言模型（LLM）是一个**大型**神经网络模型，根据先前预测的 token
    来预测下一个 token。就是这样。
- en: '![](../Images/baef17b325f3a0a91efb69b5f000121a.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/baef17b325f3a0a91efb69b5f000121a.png)'
- en: Comparison of the number of parameters of models. **Just look at how big GPT-3
    is.** And nobody knows about GPT-4…
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 模型参数数量的比较。**只需看看 GPT-3 有多大。**而且没人知道 GPT-4 的情况……
- en: The popularity of LLMs is due to their versatility and effectiveness. They perfectly
    cope with such tasks as translation, summarization, analysis of meanings, etc.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: LLMs 的受欢迎程度归因于其多功能性和有效性。它们可以完美地完成翻译、摘要、意义分析等任务。
- en: '![](../Images/f067bc2b066b128c05a9b99209c1abb0.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f067bc2b066b128c05a9b99209c1abb0.png)'
- en: LLMs capabilities
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: LLMs 的能力
- en: 'Some examples of projects using LLMs:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 LLMs 的一些项目示例：
- en: '[**Notion AI**](https://www.notion.so/product/ai)— helps improve writing quality,
    generate content, correct spelling and grammar, edit voice and intonation, translate,
    and more.'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**Notion AI**](https://www.notion.so/product/ai)——帮助提高写作质量，生成内容，纠正拼写和语法，编辑语音和语调，翻译等。'
- en: '[**GitHub Copilot**](https://github.com/features/copilot) — improves you code
    by offering autocomplete-style suggestions.'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**GitHub Copilot**](https://github.com/features/copilot)——通过提供自动补全样式的建议来改进你的代码。'
- en: '[**Dropbox Dash**](https://blog.dropbox.com/topics/product/introducing-AI-powered-tools)—
    provides a natural-language search functionality, and also specifically cites
    which files the answer is derived from.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**Dropbox Dash**](https://blog.dropbox.com/topics/product/introducing-AI-powered-tools)——提供自然语言搜索功能，并特别指出答案来源于哪些文件。'
- en: If you want a detailed understanding of how LLMs work, I **highly recommend**
    reading the excellent article “[A Very Gentle Introduction to Large Language Models
    without the Hype](https://medium.com/@mark-riedl/a-very-gentle-introduction-to-large-language-models-without-the-hype-5f67941fa59e)”
    by [Mark Riedl](https://medium.com/u/7247bdeb9655?source=post_page-----5c45708156bc--------------------------------).
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想详细了解 LLMs 的工作原理，我**强烈推荐**阅读 [Mark Riedl](https://medium.com/u/7247bdeb9655?source=post_page-----5c45708156bc--------------------------------)
    的优秀文章“[一种非常温和的介绍大型语言模型，没有炒作](https://medium.com/@mark-riedl/a-very-gentle-introduction-to-large-language-models-without-the-hype-5f67941fa59e)”。
- en: 2\. Open Source vs Closed Source Models
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 2\. 开源与闭源模型
- en: 'While there are quite a few differences, I highlight the following as the main
    ones:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然有很多差异，我强调以下几点作为主要差异：
- en: '**Privacy** — one of the most important reasons why large companies choose
    self-hosted solutions.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**隐私**——大型公司选择自托管解决方案的最重要原因之一。'
- en: '**Fast prototyping** — great for small startups to quickly test their ideas
    without excessive expenditure.'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**快速原型制作**——非常适合小型初创公司快速测试他们的想法，而不需要过多开支。'
- en: '**Quality of generation** — either you fine-tune the model for your specific
    task or use a paid API.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**生成质量**——要么为你的特定任务对模型进行微调，要么使用付费 API。'
- en: 'There is no definitive answer to what is better or worse. I highlighted the
    following points:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 没有明确的答案说明什么更好或更差。我总结了以下几点：
- en: '![](../Images/93415feefe3e2027c695a8346773c5bd.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/93415feefe3e2027c695a8346773c5bd.png)'
- en: If you are interested in delving deeper into the details, I suggest you read
    my article “[You don’t need hosted LLMs, do you?](https://medium.com/better-programming/you-dont-need-hosted-llms-do-you-1160b2520526)”.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你对深入了解细节感兴趣，我建议你阅读我的文章“[你真的不需要托管的 LLMs，对吧？](https://medium.com/better-programming/you-dont-need-hosted-llms-do-you-1160b2520526)”。
- en: '**Popular Open Source models**'
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**流行的开源模型**'
- en: '[LLaMA-2](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) by **Meta**'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[LLaMA-2](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) 由**Meta**提供'
- en: '[Falcon](https://huggingface.co/tiiuae/falcon-7b) by **Technology Innovation
    Institute in Abu Dhabi**'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Falcon](https://huggingface.co/tiiuae/falcon-7b) 由**阿布扎比技术创新研究所**提供'
- en: '[Mistral](https://huggingface.co/mistralai/Mistral-7B-v0.1) by **Mistral AI**'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Mistral](https://huggingface.co/mistralai/Mistral-7B-v0.1) 由**Mistral AI**提供'
- en: '**Popular Closed Source models**'
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**流行的闭源模型**'
- en: '[GPT-4](https://openai.com/gpt-4) by **OpenAI**'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[GPT-4](https://openai.com/gpt-4) 由**OpenAI**提供'
- en: '[Bard](https://bard.google.com/) by **Google**'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Bard](https://bard.google.com/) 由**Google**提供'
- en: '[Claude](https://www.anthropic.com/) by **Anthropic**'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Claude](https://www.anthropic.com/) 由**Anthropic**提供'
- en: Explore the [LLM Collection](https://www.promptingguide.ai/models/collection)
    to view all models.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 探索 [LLM Collection](https://www.promptingguide.ai/models/collection) 以查看所有模型。
- en: 3\. The Art of Prompt Engineering
  id: totrans-40
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 3\. 提示工程的艺术
- en: I know, I know, many consider it a pseudo-science or just a temporary hype.
    But the truth is, we still don’t fully understand how LLMs work. Why do they sometimes
    provide high-quality responses and other times fabricate facts ([hallucinate](https://medium.com/better-programming/fixing-hallucinations-in-llms-9ff0fd438e33))?
    Or why does adding “let’s think step-by-step” to a prompt suddenly improve the
    quality?
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 我知道，很多人认为这是一门伪科学或只是暂时的炒作。但事实是，我们仍然没有完全理解LLM的工作原理。为什么它们有时能提供高质量的回答，而有时却编造事实（[幻觉](https://medium.com/better-programming/fixing-hallucinations-in-llms-9ff0fd438e33)）？或者为什么在提示中添加“让我们逐步思考”会突然提高质量？
- en: '![](../Images/a54da5dfb878ed04049f2bf61d77a0ee.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a54da5dfb878ed04049f2bf61d77a0ee.png)'
- en: Adding emotional coloring increases the quality on any models. [Source](https://arxiv.org/pdf/2307.11760.pdf)
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 添加情感色彩会提高任何模型的质量。[来源](https://arxiv.org/pdf/2307.11760.pdf)
- en: Due to all this, scientists and enthusiasts can only experiment with different
    prompts, trying to make models perform better.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 正因为如此，科学家和爱好者只能尝试不同的提示，试图使模型表现得更好。
- en: '![](../Images/78d2c86b54dc16eac939b1047e3f5927.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/78d2c86b54dc16eac939b1047e3f5927.png)'
- en: Schematic illustrating various approaches to problem-solving with LLMs
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 示意图，展示了用LLM解决问题的各种方法
- en: 'I won’t bore you with complex prompt chains; instead, I’ll just give a few
    examples that will instantly improve performance:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 我不会用复杂的提示链来让你感到无聊，而是给你一些能立即提高性能的示例：
- en: '[***“Let’s think step by step”***](https://arxiv.org/pdf/2205.11916.pdf) —
    works great for reasoning or logical tasks..'
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[***“让我们逐步思考”***](https://arxiv.org/pdf/2205.11916.pdf) — 对推理或逻辑任务效果很好。'
- en: '[***“Take a deep breath and work on this problem step-by-step“***](https://arxiv.org/pdf/2309.03409.pdf)—
    an improved version of the previous point. It can add a few more percent of quality.'
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[***“深呼吸，逐步解决这个问题”***](https://arxiv.org/pdf/2309.03409.pdf) — 是前一点的改进版。它可以再提高几个百分点的质量。'
- en: '[***“This is very important to my career”***](https://arxiv.org/pdf/2307.11760.pdf)—
    just add it to the end of your prompt and you’ll notice a 5–20% improvement in
    quality.'
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[***“这对我的职业生涯非常重要”***](https://arxiv.org/pdf/2307.11760.pdf) — 只需将其添加到提示的末尾，你会注意到质量提高5–20%。'
- en: 'Also, I’ll share a useful prompt template right away:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 另外，我会立即分享一个有用的提示模板：
- en: Let’s combine our **X** command and clear thinking to quickly and accurately
    decipher the answer in the step-by-step approach. Provide details and include
    sources in the answer. This is very important to my career.
  id: totrans-52
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 让我们结合我们的**X**命令和清晰的思维，以逐步的方法快速准确地解读答案。提供详细信息并在答案中包括来源。这对我的职业生涯非常重要。
- en: ''
  id: totrans-53
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Where **X** is the industry of the task you are solving, for example, programming.
  id: totrans-54
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 其中**X**是你解决任务的行业，例如编程。
- en: I highly recommend spending a few evenings exploring prompt engineering techniques.
    This will not only allow you to better control the model’s behavior but will also
    help improve quality and reduce hallucinations. For this, I recommend reading
    the [**Prompt Engineering Guide.**](https://www.promptingguide.ai/introduction/basics)
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 我强烈建议你花几晚时间探索提示工程技术。这不仅能让你更好地控制模型的行为，还能帮助提高质量并减少幻觉。为此，我推荐阅读[**提示工程指南。**](https://www.promptingguide.ai/introduction/basics)
- en: 'Useful Links:'
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 有用链接：
- en: '[prompttools](https://github.com/hegelai/prompttools) — prompt testing and
    experimentation, with support for both LLMs (e.g. OpenAI, LLaMA).'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[prompttools](https://github.com/hegelai/prompttools) — 提示测试和实验，支持LLM（例如OpenAI，LLaMA）。'
- en: '[promptfoo](https://github.com/promptfoo/promptfoo) — testing and evaluating
    LLM output quality.'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[promptfoo](https://github.com/promptfoo/promptfoo) — 测试和评估LLM输出质量。'
- en: '[Awesome ChatGPT Prompts](https://github.com/f/awesome-chatgpt-prompts) — A
    collection of prompt examples to be used with the ChatGPT model.'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Awesome ChatGPT Prompts](https://github.com/f/awesome-chatgpt-prompts) — 一系列用于ChatGPT模型的提示示例集合。'
- en: '4\. Incorporating New Data: Retrieval Augmented Generation (RAG)'
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 4\. 融入新数据：检索增强生成（RAG）
- en: RAG is a technique that combines the LLM with external knowledge bases. This
    allows the model to add relevant information or specific data not included in
    the original training set to the model.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: RAG是一种将LLM与外部知识库结合的技术。这使得模型能够添加原始训练集中未包含的相关信息或特定数据。
- en: 'Despite the intimidating name (sometimes we add the word “reranker” to it),
    it’s actually a pretty old and surprisingly simple technique:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管名字听起来吓人（有时我们会在其前面加上“reranker”一词），但它实际上是一种相当古老且出乎意料简单的技术：
- en: '![](../Images/efe26cbc8b7fd9a77f9e9b7959339d57.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/efe26cbc8b7fd9a77f9e9b7959339d57.png)'
- en: Schematic illustration of how RAG works
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: RAG工作原理的示意图
- en: You convert documents into numbers, we call them [**embeddings**](/neural-network-embeddings-explained-4d028e6f0526).
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您将文档转换为数字，我们称之为[**嵌入**](/neural-network-embeddings-explained-4d028e6f0526)。
- en: Then, you also convert the user’s search query into embeddings using the same
    model.
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，您还使用同一模型将用户的搜索查询转换为嵌入。
- en: Find the top K closest documents, usually based on [cosine similarity](https://en.wikipedia.org/wiki/Cosine_similarity).
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 找到前K个最接近的文档，通常基于[余弦相似度](https://en.wikipedia.org/wiki/Cosine_similarity)。
- en: Ask the LLM to generate a response based on these documents.
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要求LLM基于这些文档生成响应。
- en: When to Use
  id: totrans-69
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 适合使用时
- en: '**Need for Current Information:** When the application requires information
    that is constantly updating, like news articles.'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**需要当前信息时：** 当应用程序需要不断更新的信息，例如新闻文章时。'
- en: '**Domain-Specific Applications:** For applications that require specialized
    knowledge outside the LLM’s training data. For example, internal company documents.'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**领域特定应用程序：** 对于需要LLM训练数据之外的专业知识的应用程序，例如内部公司文档。'
- en: When NOT to Use
  id: totrans-72
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 不适合使用时
- en: '**General Conversational Applications:** Where the information needs to be
    general and doesn’t require additional data.'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**通用对话应用程序：** 信息需要是通用的，不需要额外的数据。'
- en: '**Limited Resource Scenarios:** The retrieval component of RAG involves searching
    through large knowledge bases, which can be computationally expensive and slow
    — though still faster and less expensive than fine-tuning.'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**有限资源场景：** RAG的检索组件涉及搜索大型知识库，这可能计算成本高且速度慢 — 尽管仍然比微调更快且更便宜。'
- en: Building an Application with RAG
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用RAG构建应用程序
- en: 'A great starting point is using the [LlamaIndex library](https://github.com/run-llama/llama_index).
    It allows you to quickly connect your data to LLMs. For this you only need a few
    lines of code:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 一个很好的起点是使用[LlamaIndex库](https://github.com/run-llama/llama_index)。它允许您快速将数据连接到LLM。为此，您只需要几行代码：
- en: '[PRE0]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: In real-world applications, things are noticeably more complex. Like in any
    development, you’ll encounter many nuances. For example, the retrieved documents
    might not always be relevant to the question or there might be issues with speed.
    However, even at this stage, you can significantly improve the quality of your
    search system.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在实际应用中，事情显然更加复杂。与任何开发一样，您会遇到许多细微差别。例如，检索的文档可能并不总是与问题相关，或者可能存在速度问题。然而，即使在这个阶段，您也可以显著提高搜索系统的质量。
- en: What to Read & Useful Links
  id: totrans-79
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 读什么 & 有用的链接
- en: '[Building RAG-based LLM Applications for Production](https://www.anyscale.com/blog/a-comprehensive-guide-for-building-rag-based-llm-applications-part-1)
    — an excellent detailed article about the main components of RAG.'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[为生产环境构建基于RAG的LLM应用程序](https://www.anyscale.com/blog/a-comprehensive-guide-for-building-rag-based-llm-applications-part-1)
    — 一篇关于RAG主要组成部分的优秀详细文章。'
- en: '[Why Your RAG Is Not Reliable in a Production Environment](/why-your-rag-is-not-reliable-in-a-production-environment-9e6a73b3eddb)
    — a great article by [Ahmed Besbes](https://medium.com/u/adc8ea174c69?source=post_page-----5c45708156bc--------------------------------)
    that explains in clear language the difficulties that can arise when using RAG.'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[为何在生产环境中您的RAG不可靠](/why-your-rag-is-not-reliable-in-a-production-environment-9e6a73b3eddb)
    — 由[Ahmed Besbes](https://medium.com/u/adc8ea174c69?source=post_page-----5c45708156bc--------------------------------)撰写的一篇很棒的文章，清楚地解释了在使用RAG时可能出现的困难。'
- en: '[7 Query Strategies for Navigating Knowledge Graphs With LlamaIndex](https://betterprogramming.pub/7-query-strategies-for-navigating-knowledge-graphs-with-llamaindex-ed551863d416)
    — an informative article from [Wenqi Glantz](https://medium.com/u/ce7cd5b8b74a?source=post_page-----5c45708156bc--------------------------------)
    that takes a detailed and nuanced look at building a RAG pipeline using LlamaIndex.'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用LlamaIndex导航知识图谱的7种查询策略](https://betterprogramming.pub/7-query-strategies-for-navigating-knowledge-graphs-with-llamaindex-ed551863d416)
    — 来自[Wenqi Glantz](https://medium.com/u/ce7cd5b8b74a?source=post_page-----5c45708156bc--------------------------------)的信息丰富的文章，详细而微妙地探讨了使用LlamaIndex构建RAG管道。'
- en: '[OpenAI Retrieval tool](https://platform.openai.com/docs/assistants/tools/knowledge-retrieval)
    — use OpenAI’s RAG if you want a minimum of effort.'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[OpenAI 检索工具](https://platform.openai.com/docs/assistants/tools/knowledge-retrieval)
    — 如果你想要最小的工作量，可以使用 OpenAI 的 RAG。'
- en: 5\. Fine-Tuning Your LLM
  id: totrans-84
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 5\. 微调您的LLM
- en: 'Fine-tuning is the process of continuing the training of a pre-trained LLM
    on a specific dataset. You might ask why we need to train the model further if
    we can already add data using RAG. The simple answer is that only fine-tuning
    can tailor your model to understand a specific domain or define its style. For
    instance, I [created a copy of myself by fine-tuning on personal correspondences](https://medium.com/better-programming/unleash-your-digital-twin-how-fine-tuning-llm-can-create-your-perfect-doppelganger-b5913e7dda2e):'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 微调是继续在特定数据集上训练预训练大语言模型（LLM）的过程。你可能会问，如果我们已经可以通过RAG添加数据，为什么还需要进一步训练模型。简单的回答是，只有微调才能使你的模型适应特定领域或定义其风格。例如，我[通过在个人通信上进行微调创建了自己的副本](https://medium.com/better-programming/unleash-your-digital-twin-how-fine-tuning-llm-can-create-your-perfect-doppelganger-b5913e7dda2e)：
- en: '![](../Images/c3a7b21c134390c8c947c4ee28812d55.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c3a7b21c134390c8c947c4ee28812d55.png)'
- en: Demo of the fine-tuned model on the author’s correspondences
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 微调模型在作者通信上的演示
- en: 'Okay, if I’ve convinced you of its importance, let’s see how it works (**spoiler**
    — it’s not so difficult):'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 好吧，如果我已经说服了你它的重要性，让我们来看看它是如何工作的（**剧透** — 其实并不难）：
- en: '![](../Images/c65f03ad9d92f925b08f8adcde668485.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c65f03ad9d92f925b08f8adcde668485.png)'
- en: '*Classical approach of fine-tuning on domain specific data (all icons from*
    [*flaticon*](http://flaticon.com/)*)*'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '*经典的微调领域特定数据的方法（所有图标来自* [*flaticon*](http://flaticon.com/)*)*'
- en: Take a trained LLM, sometimes called Base LLM. You can download them from [HuggingFace](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard).
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择一个经过训练的大语言模型，有时称为基础LLM。你可以从[HuggingFace](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard)下载它们。
- en: Prepare your training data. You only need to compile instructions and responses.
    [Here’s an example](https://huggingface.co/datasets/databricks/databricks-dolly-15k)
    of such a dataset. You can also [generate synthetic data](https://www.promptingguide.ai/applications/generating)
    using GPT-4.
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 准备你的训练数据。你只需要编译指令和响应。[这里有一个例子](https://huggingface.co/datasets/databricks/databricks-dolly-15k)的这样的数据集。你也可以[使用GPT-4生成合成数据](https://www.promptingguide.ai/applications/generating)。
- en: Choose a suitable fine-tuning method. [LoRA](https://github.com/microsoft/LoRA)
    and [QLoRA](https://github.com/artidoro/qlora) are currently popular.
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择合适的微调方法。[LoRA](https://github.com/microsoft/LoRA)和[QLoRA](https://github.com/artidoro/qlora)目前很受欢迎。
- en: Fine-tune the model on new data.
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在新数据上微调模型。
- en: When to Use
  id: totrans-95
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 何时使用
- en: '**Niche Applications:** When the application deals with specialized or unconventional
    topics. For example, legal document applications that need to understand and handle
    legal jargon.'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**小众应用：** 当应用涉及专业或非常规话题时。例如，需要理解和处理法律术语的法律文件应用。'
- en: '**Custom Language Styles:** For applications requiring a specific tone or style.
    For example, creating an [AI character](https://beta.character.ai/) whether it’s
    a celebrity or a character from a book.'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**自定义语言风格：** 对于需要特定语调或风格的应用。例如，创建一个[AI角色](https://beta.character.ai/)，无论是名人还是书中的角色。'
- en: When NOT to Use
  id: totrans-98
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 何时不使用
- en: '**Broad Applications:** Where the scope of the application is general and doesn’t
    require specialized knowledge.'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**广泛应用：** 当应用的范围较广，不需要专业知识时。'
- en: '**Limited Data:** Fine-tuning requires a significant amount of relevant data.
    However, you can always [generate them with another LLM](https://www.confident-ai.com/blog/how-to-generate-synthetic-data-using-llms-part-1).
    For example, the [Alpaca dataset](https://github.com/gururise/AlpacaDataCleaned)
    of 52k LLM-generated instruction-response pairs was used to create the first finetuning
    [Llama v1](https://arxiv.org/abs/2302.13971) model earlier this year.'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**有限的数据：** 微调需要大量相关的数据。然而，你始终可以[使用另一个LLM生成数据](https://www.confident-ai.com/blog/how-to-generate-synthetic-data-using-llms-part-1)。例如，[Alpaca数据集](https://github.com/gururise/AlpacaDataCleaned)包含52k个LLM生成的指令-响应对，用于创建今年早些时候的第一个微调[Llama
    v1](https://arxiv.org/abs/2302.13971)模型。'
- en: Fine-tune your LLM
  id: totrans-101
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 微调你的LLM
- en: 'You can find a vast number of articles dedicated to model fine-tuning. Just
    on Medium alone, there are thousands. Therefore, I don’t want to delve too deeply
    into this topic and will show you a high-level library, [Lit-GPT](https://github.com/Lightning-AI/lit-gpt),
    which hides all the magic inside. Yes, it doesn’t allow for much customization
    of the training process, but you can quickly conduct experiments and get initial
    results. You’ll need just a few lines of code:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以找到大量关于模型微调的文章。仅在 Medium 上，就有成千上万篇。因此，我不想过于深入这个话题，会向你展示一个高级库，[Lit-GPT](https://github.com/Lightning-AI/lit-gpt)，它隐藏了所有的魔法。是的，它不允许对训练过程进行太多自定义，但你可以快速进行实验并获得初步结果。你只需要几行代码：
- en: '[PRE1]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'And that’s it! Your training process will start:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 就这样！你的训练过程将开始：
- en: '![](../Images/b919aa7721ea04f6df5c5783dd882fd0.png)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b919aa7721ea04f6df5c5783dd882fd0.png)'
- en: Be aware that the process can take a long time. It takes approximately **10
    hours** and **30 GB** memory to fine-tune Falcon-7B on a single A100 GPU.
  id: totrans-106
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 请注意，这个过程可能需要很长时间。在单个 A100 GPU 上微调 Falcon-7B 大约需要**10小时**和**30 GB**内存。
- en: Of course, I’ve slightly oversimplified, and we’ve only scratched the surface.
    In reality, the fine-tuning process is much more complex and to get better results,
    you’ll need to understand various adapters, their parameters, and much more. However,
    even after such a simple iteration, you will have a new model that follows your
    instructions.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，我稍微简化了一下，我们只触及了表面。实际上，微调过程要复杂得多，要获得更好的结果，你需要了解各种适配器、它们的参数以及更多内容。不过，即使经过如此简单的一轮，你也将拥有一个按照你的指示运行的新模型。
- en: What to Read & Useful Links
  id: totrans-108
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 阅读内容 & 有用链接
- en: '[Create a Clone of Yourself With a Fine-tuned LLM](https://medium.com/better-programming/unleash-your-digital-twin-how-fine-tuning-llm-can-create-your-perfect-doppelganger-b5913e7dda2e)
    — my article where I wrote about collecting datasets, used parameters, and gave
    useful tips on fine-tuning.'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[用微调的 LLM 创建一个你自己的克隆](https://medium.com/better-programming/unleash-your-digital-twin-how-fine-tuning-llm-can-create-your-perfect-doppelganger-b5913e7dda2e)
    — 我的文章，讲述了数据集的收集、使用的参数以及提供了关于微调的有用建议。'
- en: '[Understanding Parameter-Efficient Fine-tuning of Large Language Models](https://lightning.ai/pages/community/article/understanding-llama-adapters/)
    — an excellent tutorial if you want to get into the details of the concept of
    fine-tuning and popular parameter-efficient alternatives.'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[理解大型语言模型的参数高效微调](https://lightning.ai/pages/community/article/understanding-llama-adapters/)
    — 如果你想深入了解微调概念和流行的参数高效替代方案，这是一份极好的教程。'
- en: '[Fine-tuning LLMs with LoRA and QLoRA: Insights from Hundreds of Experiments](https://lightning.ai/pages/community/lora-insights/)
    — one of my favorite articles for understanding the capabilities of LoRA.'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用 LoRA 和 QLoRA 微调 LLM：来自数百次实验的见解](https://lightning.ai/pages/community/lora-insights/)
    — 我最喜欢的文章之一，用于了解 LoRA 的能力。'
- en: '[OpenAI Fine-tuning](https://platform.openai.com/docs/guides/fine-tuning) —
    if you want to fine-tune GPT-3.5 with minimal effort.'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[OpenAI 微调](https://platform.openai.com/docs/guides/fine-tuning) — 如果你想以最小的努力微调
    GPT-3.5。'
- en: 6\. Deploying Your LLM Application
  id: totrans-113
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 6\. 部署你的 LLM 应用程序
- en: Sometimes, all we want is to simply push a “deploy” button…
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 有时候，我们只希望简单地按下“部署”按钮……
- en: '![](../Images/ccafea187bb4ef867065b18a1781fc9e.png)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ccafea187bb4ef867065b18a1781fc9e.png)'
- en: Fortunately, this is quite feasible. There are a huge number of frameworks that
    specialize in deploying large language models. What makes them so good?
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，这相当可行。有大量专注于部署大型语言模型的框架。是什么让它们如此优秀？
- en: Lots of pre-built wrappers and integrations.
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 许多预构建的包装器和集成。
- en: A vast selection of available models.
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 丰富的模型选择。
- en: A multitude of internal optimizations.
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 众多的内部优化。
- en: Rapid prototyping.
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 快速原型开发。
- en: Choosing the Right Framework
  id: totrans-121
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 选择合适的框架
- en: 'The choice of framework for deploying an LLM application depends on various
    factors, including the size of the model, the scalability requirements of the
    application, and the deployment environment. Currently, there isn’t a vast diversity
    of frameworks, so it shouldn’t be too difficult to understand their differences.
    Below, I’ve prepared a cheat sheet for you that will help you quickly get started:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 部署 LLM 应用程序的框架选择取决于多种因素，包括模型的大小、应用程序的可扩展性要求和部署环境。目前，框架的种类并不多，因此理解它们的差异应该不会太难。下面，我为你准备了一份速查表，帮助你快速入门：
- en: '![](../Images/67ac80e1155c047a0175a5523a329036.png)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/67ac80e1155c047a0175a5523a329036.png)'
- en: Also, in my article “[7 Frameworks for Serving LLMs](https://medium.com/better-programming/frameworks-for-serving-llms-60b7f7b23407)”
    I provide a more detailed overview of the existing solutions. I recommend checking
    it out if you’re planning to deploy your model.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，在我的文章“[7种用于服务LLM的框架](https://medium.com/better-programming/frameworks-for-serving-llms-60b7f7b23407)”中，我提供了对现有解决方案的更详细概述。如果你打算部署你的模型，我建议查看一下。
- en: '![](../Images/2b73a99b6ce220522fcf5cb5db8d26d4.png)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2b73a99b6ce220522fcf5cb5db8d26d4.png)'
- en: Comparison of frameworks for LLMs inference
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: LLM推理框架的比较
- en: Example Code for Deployment
  id: totrans-127
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 部署示例代码
- en: 'Let’s move from theory to practice and try to deploy LLaMA-2 using [Text Generation
    Inference](https://github.com/huggingface/text-generation-inference). And, as
    you might have guessed, you’ll need just a few lines of code:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从理论转向实践，尝试使用[文本生成推理](https://github.com/huggingface/text-generation-inference)部署LLaMA-2。正如你可能猜到的，你只需几行代码：
- en: '[PRE2]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: That’s it! You’ve set up a RestAPI service with built-in logging, Prometheus
    endpoint for monitoring, token streaming, and your model is fully optimized. Isn’t
    this magical?
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 就这些！你已经设置了一个内置日志记录的RestAPI服务，带有用于监控的Prometheus端点、令牌流式传输，并且你的模型已完全优化。难道这不是很神奇吗？
- en: '![](../Images/89f89d8b815ae7b78448c9441b533a16.png)'
  id: totrans-131
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/89f89d8b815ae7b78448c9441b533a16.png)'
- en: API Documentation
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: API文档
- en: What to Read & Useful Links
  id: totrans-133
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 该读什么 & 有用的链接
- en: '[7 Frameworks for Serving LLMs](https://medium.com/better-programming/frameworks-for-serving-llms-60b7f7b23407)
    — comprehensive guide into LLMs inference and serving with detailed comparison.'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7种用于服务LLM的框架](https://medium.com/better-programming/frameworks-for-serving-llms-60b7f7b23407)
    —— 一份详尽的指南，介绍了LLMs推理和服务的详细比较。'
- en: '[Inference Endpoints](https://huggingface.co/inference-endpoints) — a product
    from HuggingFace that will allow you to deploy any LLMs in a few clicks. A good
    choice when you need rapid prototyping.'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[推理端点](https://huggingface.co/inference-endpoints) —— HuggingFace的一个产品，可以让你通过几次点击部署任何LLMs。当你需要快速原型时，这是一个不错的选择。'
- en: 7\. What Remains Behind the Scenes
  id: totrans-136
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 7. 什么在幕后仍然存在
- en: 'Even though we’ve covered the main concepts needed for developing LLM-based
    applications, there are still some aspects you’ll likely encounter in the future.
    So, I’d like to leave a few useful links:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们已经涵盖了开发基于LLM的应用所需的主要概念，但仍然有一些方面你可能会在未来遇到。因此，我想留下一些有用的链接：
- en: Optimization
  id: totrans-138
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 优化
- en: When you launch your first model, you inevitably find it’s not as fast as you’d
    like and consumes a lot of resources. If this is your case, you need to understand
    how it can be optimized.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 当你启动第一个模型时，你不可避免地会发现它没有你期望的那么快，而且消耗了大量资源。如果是这种情况，你需要了解如何优化它。
- en: '[7 Ways To Speed Up Inference of Your Hosted LLMs](https://medium.com/better-programming/speed-up-llm-inference-83653aa24c47)
    — techniques to speed up inference of LLMs to increase token generation speed
    and reduce memory consumption.'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7种加速托管LLM推理的方法](https://medium.com/better-programming/speed-up-llm-inference-83653aa24c47)
    —— 加速LLMs推理的技术，以提高令牌生成速度和减少内存消耗。'
- en: '[Optimizing Memory Usage for Training LLMs in PyTorch](https://lightning.ai/pages/community/tutorial/pytorch-memory-vit-llm/)
    — article provides a series of techniques that can reduce memory consumption in
    PyTorch by approximately 20x without sacrificing modeling performance and prediction
    accuracy.'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[在PyTorch中优化LLM的内存使用](https://lightning.ai/pages/community/tutorial/pytorch-memory-vit-llm/)
    —— 文章提供了一系列技术，可以在不牺牲建模性能和预测准确性的情况下，将PyTorch中的内存消耗减少约20倍。'
- en: Evaluating
  id: totrans-142
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 评估
- en: Suppose you have a fine-tuned model. But how can you be sure that its quality
    has improved? What metrics should we use?
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你有一个微调后的模型。但你如何确保其质量有所提高？我们应该使用什么指标？
- en: '[All about evaluating Large language models](https://explodinggradients.com/all-about-evaluating-large-language-models)
    — a good overview article about benchmarks and metrics.'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[关于评估大语言模型的一切](https://explodinggradients.com/all-about-evaluating-large-language-models)
    —— 一篇关于基准和指标的好概述文章。'
- en: '[evals](https://github.com/openai/evals) — the most popular framework for evaluating
    LLMs and LLM systems.'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[evals](https://github.com/openai/evals) —— 最受欢迎的评估LLMs和LLM系统的框架。'
- en: Vector Databases
  id: totrans-146
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 向量数据库
- en: If you work with RAG, at some point, you’ll move from storing vectors in memory
    to a database. For this, it’s important to understand what’s currently on the
    market and its limitations.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你使用RAG，某个时候你会从将向量存储在内存中转到数据库中。为此，了解市场上的当前产品及其局限性非常重要。
- en: '[All You Need to Know about Vector Databases](/all-you-need-to-know-about-vector-databases-and-how-to-use-them-to-augment-your-llm-apps-596f39adfedb)
    — a step-by-step guide by [Dominik Polzer](https://medium.com/u/3ab8d3143e32?source=post_page-----5c45708156bc--------------------------------)
    to discover and harness the power of vector databases.'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[你需要知道的关于向量数据库的所有信息](/all-you-need-to-know-about-vector-databases-and-how-to-use-them-to-augment-your-llm-apps-596f39adfedb)
    — [Dominik Polzer](https://medium.com/u/3ab8d3143e32?source=post_page-----5c45708156bc--------------------------------)
    提供的逐步指南，帮助你发现并利用向量数据库的力量。'
- en: '[Picking a vector database: a comparison and guide for 2023](https://benchmark.vectorview.ai/vectordbs.html)
    — comparison of Pinecone, Weviate, Milvus, Qdrant, Chroma, Elasticsearch and PGvector
    databases.'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[选择向量数据库：2023年的比较和指南](https://benchmark.vectorview.ai/vectordbs.html) — 对Pinecone、Weviate、Milvus、Qdrant、Chroma、Elasticsearch和PGvector数据库的比较。'
- en: LLM Agents
  id: totrans-150
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: LLM代理
- en: In my opinion, the most promising development in LLMs. If you want multiple
    models to work together, I recommend exploring the following links.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 在我看来，这是LLMs中最有前景的发展。如果你想让多个模型协同工作，建议你探索以下链接。
- en: '[A Survey on LLM-based Autonomous Agents](https://github.com/paitesanshi/llm-agent-survey#-more-comprehensive-summarization)
    — this is probably the most comprehensive overview of LLM based agents.'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[基于LLM的自主代理调查](https://github.com/paitesanshi/llm-agent-survey#-more-comprehensive-summarization)
    — 这可能是关于LLM代理的最全面的概述。'
- en: '[autogen](https://github.com/microsoft/autogen) — is a framework that enables
    the development of LLM applications using multiple agents that can converse with
    each other to solve tasks.'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[autogen](https://github.com/microsoft/autogen) — 一个框架，使得开发使用多个能够相互对话以解决任务的LLM应用程序成为可能。'
- en: '[OpenAgents](https://github.com/xlang-ai/OpenAgents) — an open platform for
    using and hosting language agents in the wild.'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[OpenAgents](https://github.com/xlang-ai/OpenAgents) — 一个开放平台，用于在实际环境中使用和托管语言代理。'
- en: Reinforcement Learning from Human Feedback (RLHF)
  id: totrans-155
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 来自人类反馈的强化学习（RLHF）
- en: 'As soon as you allow users access to your model, you start taking responsibility.
    What if it responds rudely? Or reveals bomb-making ingredients? To avoid this,
    check out these articles:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你允许用户访问你的模型，你就开始承担责任。如果它回应粗鲁？或者泄露炸弹制作成分？为避免这种情况，请查看这些文章：
- en: '[Illustrating Reinforcement Learning from Human Feedback (RLHF)](https://huggingface.co/blog/rlhf)
    — an overview article that details the RLHF technology.'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[说明来自人类反馈的强化学习（RLHF）](https://huggingface.co/blog/rlhf) — 详细介绍RLHF技术的概述文章。'
- en: '[RL4LMs](https://github.com/allenai/RL4LMs) — a modular RL library to fine-tune
    language models to human preferences.'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[RL4LMs](https://github.com/allenai/RL4LMs) — 一个模块化的RL库，用于将语言模型微调到人类偏好。'
- en: '[TRL](https://github.com/huggingface/trl) — a set of tools to train transformer
    language models with Reinforcement Learning, from the Supervised Fine-tuning step
    (SFT), Reward Modeling step (RM) to the Proximal Policy Optimization (PPO) step.'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[TRL](https://github.com/huggingface/trl) — 一套用于训练变换器语言模型的工具，从监督微调步骤（SFT）、奖励建模步骤（RM）到近端策略优化（PPO）步骤。'
- en: Conclusion
  id: totrans-160
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: Despite the hype, which we’re all a bit tired of, LLMs will be with us for a
    long time, and the ability to understand their stack and write simple applications
    can give you a significant boost. I hope I’ve managed to immerse you a bit in
    this area and show you that there is nothing complicated or scary about it.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管有些炒作让我们感到有些厌倦，LLMs将长期存在，而理解其堆栈并编写简单应用程序的能力可以为你带来显著的提升。我希望我能让你在这个领域中稍作沉浸，并展示它并不复杂或令人恐惧。
- en: Thank you for your attention, stay tuned for new articles!
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 感谢你的关注，请继续关注新文章！
- en: '**Disclaimer**: The information in the article is current as of November 2023,
    but please be aware that changes may occur thereafter.'
  id: totrans-163
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**免责声明**：本文中的信息截至2023年11月，但请注意之后可能会有所变动。'
- en: '*Unless otherwise noted, all images are by the author.*'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: '*除非另有说明，所有图片均由作者提供。*'
- en: If you have any questions or suggestions, feel free to connect on [LinkedIn](https://www.linkedin.com/in/sergey-savvov/).
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你有任何问题或建议，请随时通过[LinkedIn](https://www.linkedin.com/in/sergey-savvov/)联系我。
