- en: Recommender Systems From Implicit Feedback Using TensorFlow Recommenders
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用TensorFlow推荐系统的隐式反馈推荐系统
- en: 原文：[https://towardsdatascience.com/recommender-systems-from-implicit-feedback-using-tensorflow-recommenders-8ba36a976c57](https://towardsdatascience.com/recommender-systems-from-implicit-feedback-using-tensorflow-recommenders-8ba36a976c57)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/recommender-systems-from-implicit-feedback-using-tensorflow-recommenders-8ba36a976c57](https://towardsdatascience.com/recommender-systems-from-implicit-feedback-using-tensorflow-recommenders-8ba36a976c57)
- en: '[RECOMMENDATION SYSTEM](https://medium.com/tag/recommendation-system)'
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '[推荐系统](https://medium.com/tag/recommendation-system)'
- en: When customers don’t explicitly tell you what they want
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 当客户没有明确告诉你他们想要什么时
- en: '[](https://dr-robert-kuebler.medium.com/?source=post_page-----8ba36a976c57--------------------------------)[![Dr.
    Robert Kübler](../Images/3b8d8b88f76c0c43d9c305e3885e7ab9.png)](https://dr-robert-kuebler.medium.com/?source=post_page-----8ba36a976c57--------------------------------)[](https://towardsdatascience.com/?source=post_page-----8ba36a976c57--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----8ba36a976c57--------------------------------)
    [Dr. Robert Kübler](https://dr-robert-kuebler.medium.com/?source=post_page-----8ba36a976c57--------------------------------)'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://dr-robert-kuebler.medium.com/?source=post_page-----8ba36a976c57--------------------------------)[![Dr.
    Robert Kübler](../Images/3b8d8b88f76c0c43d9c305e3885e7ab9.png)](https://dr-robert-kuebler.medium.com/?source=post_page-----8ba36a976c57--------------------------------)[](https://towardsdatascience.com/?source=post_page-----8ba36a976c57--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----8ba36a976c57--------------------------------)
    [Dr. Robert Kübler](https://dr-robert-kuebler.medium.com/?source=post_page-----8ba36a976c57--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----8ba36a976c57--------------------------------)
    ·11 min read·Nov 5, 2023
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----8ba36a976c57--------------------------------)
    ·11分钟阅读·2023年11月5日
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/085198f24d7254f94814a1ebdec4619b.png)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/085198f24d7254f94814a1ebdec4619b.png)'
- en: Photo by [Noom Peerapong](https://unsplash.com/@imnoom?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由[Noom Peerapong](https://unsplash.com/@imnoom?utm_source=medium&utm_medium=referral)提供，来源于[Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: Making recommendations is actually not that hard. You just have to check how
    your customers rated your products, for example using 1 to 5 stars, and then train
    a **regression** model on top of it. Right?
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 提供推荐实际上并没有那么难。你只需要检查你的客户如何评分你的产品，例如使用1到5颗星，然后在其基础上训练一个**回归**模型，对吗？
- en: '![](../Images/ae0ec03fd0b1941c5147d59abe3a80c0.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ae0ec03fd0b1941c5147d59abe3a80c0.png)'
- en: A typical dataset that you would like to have. Image by the author.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 一个典型的数据集，你会希望拥有的。图片由作者提供。
- en: 'Okay, we might have to deal with embeddings if we don’t have any numerical
    user or movie features, but we have seen how to do that in my earlier article:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 好吧，如果我们没有任何数值的用户或电影特征，我们可能需要处理嵌入，但我们已经在我之前的文章中见过如何做到这一点。
- en: '[](/introduction-to-embedding-based-recommender-systems-956faceb1919?source=post_page-----8ba36a976c57--------------------------------)
    [## Introduction to Embedding-Based Recommender Systems'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/introduction-to-embedding-based-recommender-systems-956faceb1919?source=post_page-----8ba36a976c57--------------------------------)
    [## 嵌入式推荐系统简介'
- en: Learn to build a simple matrix factorization recommender in TensorFlow
  id: totrans-14
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 学习在TensorFlow中构建一个简单的矩阵分解推荐系统
- en: towardsdatascience.com](/introduction-to-embedding-based-recommender-systems-956faceb1919?source=post_page-----8ba36a976c57--------------------------------)
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/introduction-to-embedding-based-recommender-systems-956faceb1919?source=post_page-----8ba36a976c57--------------------------------)
- en: We will also need embeddings in this article, so I suggest reading the article
    above before you continue.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中我们也需要嵌入，因此我建议在继续之前阅读上面的文章。
- en: Implicit Feedback
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 隐式反馈
- en: However, sometimes we are not in the lucky position of having **explicit** user
    feedback, such as stars, thumbs up or down, or similar. This happens quite a lot
    in retail, where we know which customer bought which item, but not if they actually
    liked it. The only things we get from the customers are **implicit signals about
    their interest** in this product.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，有时候我们没有**显式**的用户反馈，例如星级、点赞或点踩，或类似的反馈。这在零售中很常见，我们知道哪个客户购买了哪个商品，但不知道他们是否真的喜欢它。我们从客户那里得到的唯一信息是关于他们对该产品**兴趣的隐式信号**。
- en: If they bought (watched, consumed, …) the product, they have showed interest
    in it. If not, they were **maybe** not interested, but maybe just didn’t know
    about it yet. We cannot tell.
  id: totrans-19
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 如果他们购买了（观看、消费……）这个产品，他们就表现出兴趣。如果没有，他们**可能**不感兴趣，但也可能只是还不知道。我们无法确定。
- en: This sounds like we can treat a classification problem. Interested = 1, not
    interested = 0\. However, this is the small problem that we cannot be sure if
    a 0 (not interested) is **really** a zero. It can also be that the customer just
    never had the chance to buy it, but would actually like to.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 这听起来像是我们可以将其视为分类问题。感兴趣 = 1，不感兴趣 = 0。然而，这里有一个小问题，我们不能确定0（不感兴趣）是否**真的**是零。也可能是客户只是从未有机会购买，但实际上是想要的。
- en: Let us go back to the movies and assume that we don’t have any ratings. We only
    know which user watched which movie.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回到电影，假设我们没有任何评分。我们只知道哪个用户观看了哪个电影。
- en: '![](../Images/2987a72fdbf65a3e5923851a43666eb0.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2987a72fdbf65a3e5923851a43666eb0.png)'
- en: Alice watched Gaußzilla and The Markov Chainsaw Massacre, for example. Image
    by the author.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 比如说，Alice 观看了 Gaußzilla 和 The Markov Chainsaw Massacre。图片由作者提供。
- en: There are at least two ways we can proceed from here.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 从这里我们可以有至少两种方法继续。
- en: Just treat all missing values as zero and then train a binary classifier.
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 只需将所有缺失值视为零，然后训练一个二分类器。
- en: Use a pairwise loss function to ensure that the similarity between a user and
    a movie they watched is higher than the similarity between the same user and a
    movie they did **not** watch.
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用成对损失函数来确保用户与他们观看过的电影之间的相似性高于同一用户与他们没有观看过的电影之间的相似性。
- en: Treat all missing values as zero
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将所有缺失值视为零
- en: 'This is the simplest solution. From the incomplete table above, you would create
    the following dataset:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 这是最简单的解决方案。从上面的不完整表格，你可以创建以下数据集：
- en: '***Note: A*** *= Alice,* ***B*** *= Bob,* ***C*** *= Charlie,* ***G*** *= Gauß,*
    ***E*** *= Euler,* ***M*** *= Markov*'
  id: totrans-29
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '***注意: A*** *= Alice,* ***B*** *= Bob,* ***C*** *= Charlie,* ***G*** *= Gauß,*
    ***E*** *= Euler,* ***M*** *= Markov*'
- en: '![](../Images/dc8285c01783fc90b2c4f4bcc62fd864.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/dc8285c01783fc90b2c4f4bcc62fd864.png)'
- en: Image by the author.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供。
- en: You can interpret the **Watched** column as a label for whether the user is
    **interested in the movie or not**. From this table, you would deduce that, for
    example, user **A** does like movie **G**, but not movie **E**,which is a bold
    statement given the data. Maybe **A** does not know about **E** yet. Or even worse,
    it’s actually on **A**’s watchlist, but did not have time to watch it.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以将**观看**列解释为用户**是否对电影感兴趣**的标签。从这个表格中，你可以推断出，例如，用户**A**喜欢电影**G**，但不喜欢电影**E**，这在数据上是一个大胆的声明。也许**A**还不知道**E**。或者更糟的是，它实际上在**A**的观看列表中，但没有时间观看。
- en: A problem on the technical side with this approach is that the model learns
    to say 0 to almost any (user, movie) input because most **Watched** values are
    typically zero. Imagine that you have a dataset of 1,000,000 users and 100,000
    movies. How many different movies does the average user watch? Maybe 1000? Then
    you **1%** of all Watched labels being 1\. So you have a heavily imbalanced dataset,
    which is nothing bad per se. However, since we **artificially create the zeros**,
    it can lead to bad performance.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 这个方法在技术方面的问题是，模型学会对几乎任何（用户，电影）输入都返回0，因为大多数**观看**值通常为零。想象一下你有一个包含1,000,000个用户和100,000部电影的数据集。平均每个用户观看多少部不同的电影？也许是1000部？那你**1%**的观看标签是1。因此，你有一个严重不平衡的数据集，这本身并不是坏事。然而，由于我们**人工创建这些零值**，可能会导致性能较差。
- en: A computational problem is that this dataset gets huge. 1,000,000 users times
    100,000 movies means you have a dataset with 100,000,000,000 rows. And often,
    you have more movies and items in your database. In this case, you don’t put all
    the zero target rows into your dataset, but you **subsample, also known as negative
    sampling**. For example, if you have 1,000,000,000 rows with target 1 in your
    dataset (= transactions that happened), you could subsample 1,000,000,000 negative
    samples (= transactions that never happened) as well. Then you have a nice dataset
    you can train on.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 一个计算问题是这个数据集变得非常庞大。1,000,000个用户乘以100,000部电影意味着你有一个包含100,000,000,000行的数据集。并且通常，你的数据库中还有更多的电影和项目。在这种情况下，你不会将所有的零目标行都放入数据集中，而是**进行子采样，也称为负采样**。例如，如果你有1,000,000,000行目标为1的数据（=
    发生的交易），你也可以子采样1,000,000,000个负样本（= 从未发生的交易）。这样你就有了一个可以训练的良好数据集。
- en: This works, but often not optimally since you make the problem harder than it
    has to be. You don’t have to predict a Watched label perfectly. You only want
    to rank movies for each user, i.e., you want to be able to say “User **A** likes
    movie **G** more than movie **E**”. The second approach gives us just that.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法有效，但通常不是最优的，因为你把问题变得比实际需要的更复杂。你不需要完美预测观看标签。你只想对每个用户对电影进行排序，即你想能够说“用户**A**更喜欢电影**G**而不是电影**E**”。第二种方法正好满足这个需求。
- en: Use a pairwise loss function
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用成对损失函数
- en: 'In this approach, we do not tell the model that a user does or does not like
    a specific movie. We phrase it more carefully:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种方法中，我们不会告诉模型用户是否喜欢某个特定的电影。我们更谨慎地表述：
- en: If a user A watched a movie G, but did not watch another movie E, we only say
    that A is more interested in G than in E.
  id: totrans-38
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 如果用户A观看了电影G，但没有观看电影E，我们仅仅说A对G的兴趣大于对E的兴趣。
- en: This allows us to tackle an easier target. Now, let us start with some formulas,
    so we can better understand how this intuition translates into an algorithm.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 这使我们能够解决一个更简单的目标。现在，让我们开始一些公式，以便更好地理解这种直觉如何转化为算法。
- en: We will train a model that works with **embeddings** again. Let us assume that
    we have embeddings for user **A**, for movie **G,** and movie **E**. If **A**
    watched **G**, but not **E,** we simply want that
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将再次训练一个处理**嵌入**的模型。假设我们有用户**A**、电影**G**和电影**E**的嵌入。如果**A**观看了**G**，但没有观看**E**，我们仅希望得到
- en: '![](../Images/33995c21ff7c4dc7fa4deb3b801e798c.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/33995c21ff7c4dc7fa4deb3b801e798c.png)'
- en: Image by the author.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供。
- en: where the *e*’s are the embeddings and · is the dot product. This implies that
    for user **A**, movie **G** is somehow **better** than movie **E**. But it is
    less drastic than saying “**A** likes **G** but does not like **E**” as in the
    binary classification case.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 其中的*e*是嵌入，·是点积。这意味着对于用户**A**来说，电影**G**在某种程度上是**更好**的，优于电影**E**。但这比说“**A**喜欢**G**但不喜欢**E**”要温和，因为后者是二元分类的情况。
- en: Training something like this sounds way more complicated than training a binary
    classifier, but several libraries get us covered. I will show you how to do it
    with [**TensorFlow Recommenders**](https://www.tensorflow.org/recommenders) since
    this is the most flexible library that I know. Another library worth mentioning
    that is easy to use, but inflexible is [implicit](https://benfred.github.io/implicit/).
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 训练这样的模型听起来比训练二元分类器复杂得多，但有几个库可以帮助我们。我将展示如何使用[**TensorFlow Recommenders**](https://www.tensorflow.org/recommenders)，因为这是我知道的最灵活的库。另一个值得一提的是[implicit](https://benfred.github.io/implicit/)，它易于使用但不够灵活。
- en: Training With TensorFlow Recommenders
  id: totrans-45
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 TensorFlow Recommenders 训练
- en: We will now see how easy it is to put this logic described before into code.
    Just to play with open cards,I’m following the [guide from the official TFRS website](https://www.tensorflow.org/recommenders/examples/basic_retrieval).
    I just tried to make it more concise.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将看到将之前描述的逻辑放入代码中是多么简单。为了开诚相见，我遵循了[官方 TFRS 网站的指南](https://www.tensorflow.org/recommenders/examples/basic_retrieval)。我只是试图让它更简洁。
- en: '*Preparations and data generation*'
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '*准备和数据生成*'
- en: First, let us do a
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们做一个
- en: '[PRE0]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: and then we can load some data via
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们可以通过以下方式加载一些数据
- en: '[PRE1]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '`ratings` is a TensorFlow dataset that is always a bit tedious to handle. For
    memory-efficient training of huge datasets, you have to use it, though. But for
    our small example, I try to stay in the friendly dataframe world as much as possible,
    hence I convert the dataset to the dataframe `ratings_df` . The data looks like
    this:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '`ratings` 是一个 TensorFlow 数据集，处理起来总是有点繁琐。然而，对于大数据集的内存高效训练，你必须使用它。但对于我们的小示例，我尽量停留在友好的数据框世界中，因此将数据集转换为数据框`ratings_df`。数据如下：'
- en: '![](../Images/42a19cafd1bd357457c9ce228fd02f5b.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/42a19cafd1bd357457c9ce228fd02f5b.png)'
- en: Image by the author.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供。
- en: Model definition
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型定义
- en: 'We will build a model that has two parts:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将构建一个有两个部分的模型：
- en: a user model
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个用户模型
- en: a movie model
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个电影模型
- en: These models should take a user or movie respectively, and turn it into an embedding,
    i.e., a bunch of floats.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 这些模型应分别接收用户或电影，并将其转换为嵌入，即一组浮点数。
- en: '[PRE2]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Using these two components, we can define the complete model like this:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这两个组件，我们可以这样定义完整的模型：
- en: '[PRE3]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: We can see how the model consists of `user_model` and `movie_model` . We will
    set this `task` attribute to a **retrieval task**, which is implementing exactly
    what we want. There is also another type of task, called a **ranking task** that
    you can use when you have explicit feedback such as ratings. We will not look
    into this further in this article.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到模型由`user_model`和`movie_model`组成。我们将这个`task`属性设置为**检索任务**，这正是我们所需要的实现。还有另一种任务类型，叫做**排名任务**，当你有明确的反馈如评分时可以使用。本文不会进一步探讨这个。
- en: 'You can also see that some loss is computed. The input is a dictionary named
    `features` that is supposed to look like this:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 你还可以看到一些损失值被计算出来。输入是一个名为`features`的字典，它应该看起来像这样：
- en: '[PRE4]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: It contains a bunch of user IDs as well as a bunch of movie titles. In this
    example, user **A** watched **G**, user **B** watched **E**, and user **C** watched
    **M**. We only have **positive examples** here, i.e., movie sessions that happened
    in the past.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 它包含了一些用户 ID 以及一系列电影标题。在这个例子中，用户**A**观看了**G**，用户**B**观看了**E**，用户**C**观看了**M**。我们这里只有**正例**，即过去发生的电影会话。
- en: The users and movies are turned into embeddings and then some loss is computed.
    I will go into detail later, but be assured that it is doing what we want it to
    do.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 用户和电影被转化为嵌入，然后计算一些损失。我稍后会详细说明，但请放心，它正在做我们希望它做的事。
- en: 'The architecture of the model is like in my other article:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 模型的架构如同我在其他文章中描述的那样：
- en: '![](../Images/432cf2b8372b38b95ab77cca25a82110.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/432cf2b8372b38b95ab77cca25a82110.png)'
- en: Image by the author.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供。
- en: Fitting the model
  id: totrans-71
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练模型
- en: We can use a nice TFRS prediction class in the end, but for it to work, we need
    a unique movie list as a TensorFlow dataset.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以使用一个很好的TFRS预测类，但为了使其正常工作，我们需要一个独特的电影列表作为TensorFlow数据集。
- en: '[PRE5]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Using this dataset, we can define the task that I talked about before:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这个数据集，我们可以定义之前提到的任务：
- en: '[PRE6]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: We can now fit the model!
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以训练模型了！
- en: '[PRE7]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'You will see something like this in the end:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 你最终会看到类似这样的结果：
- en: '![](../Images/4fa10014f6516a019104171a83351b54.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4fa10014f6516a019104171a83351b54.png)'
- en: Image by the author.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供。
- en: It is hard to give meaning to the `loss` , but the smaller the better. I will
    go into a bit more detail soon, but let us use our model first to predict some
    movies!
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 很难给`loss`赋予具体的意义，但越小越好。我会进一步详细说明，但让我们先用我们的模型预测一些电影！
- en: Prediction time
  id: totrans-82
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 预测时间
- en: First, you have to define something called an **index**. You can then use this
    index to get predictions.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，你需要定义一个叫做**索引**的东西。然后你可以使用这个索引来获取预测结果。
- en: '[PRE8]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '**Note:** You don’t use `model` anymore. You only used it to adjust the parameters
    for `user_model` and `movie_model` , and now you use these two submodels directly.
    You can basically throw the TFRS model `model` away at this point.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意：** 你不再使用`model`了。你只是用它来调整`user_model`和`movie_model`的参数，现在直接使用这两个子模型。此时你基本上可以抛弃TFRS模型`model`。'
- en: Now, you can pass a user to `index` . The index will
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你可以将用户传递给`index`。索引将会
- en: turn this user ID into an embedding — this is possible because you passed it
    `user_model` — and then
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将这个用户 ID 转换为嵌入 —— 这是因为你传递了它给`user_model` —— 然后
- en: compute the embedding for each movie — this is possible because of executing
    the `index_from_dataset` function — and then
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算每部电影的嵌入 —— 这是通过执行`index_from_dataset`函数实现的 —— 然后
- en: output the movie titles whose embeddings are closest to the user embedding.
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 输出与用户嵌入最接近的电影标题。
- en: '**Note:** It is doing an exact nearest neighbor search under the hood, which
    can be slow. It also supports an approximate nearest neighbor search using [ScaNN](https://github.com/google-research/google-research/tree/master/scann).
    You can use it by typing `ScaNN` instead of `BruteForce` .'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意：** 它在后台进行的是精确的最近邻搜索，这可能会很慢。它还支持使用[ScaNN](https://github.com/google-research/google-research/tree/master/scann)进行近似最近邻搜索。你可以通过输入`ScaNN`代替`BruteForce`来使用它。'
- en: 'It works like this:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 它的工作原理如下：
- en: '[PRE9]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Nice! You are now ready to use the model.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 很好！你现在已经准备好使用这个模型了。
- en: Loose ends
  id: totrans-94
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 收尾工作
- en: 'There is still something I promised I would get into: this `task` and the loss
    it outputs. It is not very well documented (yet) what is happening inside, but
    I looked at the source code to see what is going on. [You can find the source
    code I’m referring to here](https://github.com/tensorflow/recommenders/blob/7caed557b9d5194202d8323f2d4795231a5d0b1d/tensorflow_recommenders/tasks/retrieval.py#L119).'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 还有一些我承诺要深入探讨的内容：这个`task`和它输出的损失。内部发生的事情文档（尚未）非常好，但我查看了源代码以了解发生了什么。[你可以在这里找到我提到的源代码](https://github.com/tensorflow/recommenders/blob/7caed557b9d5194202d8323f2d4795231a5d0b1d/tensorflow_recommenders/tasks/retrieval.py#L119)。
- en: I will explain it to you using a small example batch.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 我将通过一个小例子批次来解释。
- en: '![](../Images/e99f92bf4583acaeb2821ea5567cd9f4.png)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e99f92bf4583acaeb2821ea5567cd9f4.png)'
- en: The batch going into the model. Image by the author.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 进入模型的批量。图片由作者提供。
- en: It goes into the model, and then embeddings for each user and movie are created
    using `user_model` and `movie_model` . These embeddings go into the retrieval
    task object.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 它进入模型，然后使用`user_model`和`movie_model`创建每个用户和电影的嵌入。这些嵌入进入检索任务对象。
- en: In the `task` , all user embeddings are multiplied (dot product) with all movie
    embeddings. This can be done by a simple matrix multiplication, where the movie
    matrix is transposed first. Let us assume that we use two-dimensional embeddings
    to save some space.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 在`task`中，所有用户嵌入与所有电影嵌入进行相乘（点积）。这可以通过简单的矩阵乘法完成，其中电影矩阵首先转置。假设我们使用二维嵌入来节省空间。
- en: '![](../Images/e6949c37f37826b9474b23e3b47d341f.png)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e6949c37f37826b9474b23e3b47d341f.png)'
- en: Image by the author.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供。
- en: The matrix product is
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 矩阵乘积是
- en: '![](../Images/ee3e4eb45f9fa7a327bd476daa3cdc25.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ee3e4eb45f9fa7a327bd476daa3cdc25.png)'
- en: Image by the author.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供。
- en: 'Now, the reasoning is as follows: From the data, we know that'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，推理过程如下：从数据中我们知道
- en: Alice watched Gauß,
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 爱丽丝观看了高斯，
- en: Bob watched Euler, and
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 鲍勃观看了欧拉，并且
- en: Charlie watched Markov.
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 查理观看了马尔可夫。
- en: That’s why we would like the corresponding numbers in the cells (**A**, **G**),
    (**B**, **E**), (**C**, **M**) — this is the main diagonal — to have the highest
    numbers. In this small example, we are way off.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是为什么我们希望在单元格中的对应数字（**A**，**G**），（**B**，**E**），（**C**，**M**）——这就是主对角线——具有最高的数字。在这个小例子中，我们差得很远。
- en: 'To quantify this, they do another step: the authors of TFRS do a row-wise softmax.'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 为了量化这一点，他们进行另一步：TFRS的作者进行逐行softmax。
- en: '![](../Images/78ec8f81d6c217d9cd20982559a039b2.png)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/78ec8f81d6c217d9cd20982559a039b2.png)'
- en: After a row-wise softmax. Note that the sum of each row is 1\. Image by the
    author.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 经过逐行softmax处理。注意每行的和是1。图片由作者提供。
- en: 'Now, the observation is: if the elements on the main diagonal in the previous
    matrix are way higher than the other numbers, then the “softmaxed” matrix is close
    to the **identity matrix**.'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，观察到的是：如果前一个矩阵中主对角线上的元素远高于其他数字，那么“softmax处理过”的矩阵接近于**单位矩阵**。
- en: '![](../Images/604191623a481dc49c87dd0ceca69bce.png)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/604191623a481dc49c87dd0ceca69bce.png)'
- en: The optimal identity matrix. Image by the author.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 最优的单位矩阵。图片由作者提供。
- en: 'This is because if you take the softmax of an array if one number is way higher
    than the others, this number will be close to 1\. So the other numbers must be
    close to zero. Just try it out:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 这是因为如果你对一个数组进行softmax处理，如果一个数字远高于其他数字，这个数字将接近1。所以其他数字必须接近零。可以尝试一下：
- en: '[PRE10]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: So, the loss then comes from comparing the matrix from above with the identity
    matrix. To be precise, the categorical cross-entropy loss is used. But not the
    mean across the rows, but the **sum**, [see here](https://github.com/tensorflow/recommenders/blob/7caed557b9d5194202d8323f2d4795231a5d0b1d/tensorflow_recommenders/tasks/retrieval.py#L84).
    That’s why the loss numbers are always so high. The larger the batches are, the
    larger the losses will be. So don’t be confused if the losses are suddenly super
    low, just because you changed the batch size from 10,000 to 1,000 or something
    similar.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，损失来自于将上面的矩阵与单位矩阵进行比较。具体来说，使用的是分类交叉熵损失。但是不是平均值，而是**总和**，[见此处](https://github.com/tensorflow/recommenders/blob/7caed557b9d5194202d8323f2d4795231a5d0b1d/tensorflow_recommenders/tasks/retrieval.py#L84)。这就是为什么损失值总是这么高的原因。批量越大，损失会越大。所以不要对损失突然变得非常低感到困惑，仅仅因为你把批量大小从10,000改成1,000或类似的数值。
- en: Conclusion
  id: totrans-120
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: 'In this article, we have learned how to utilize implicit feedback data to build
    a recommender system. To do this, we used TensorFlow Recommenders since it scales
    well, and is very expressive: you can take any submodel — as long as it outputs
    an embedding — and stick them together to jointly train them using the `tfrs.Model`class.'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我们学习了如何利用隐式反馈数据构建推荐系统。为此，我们使用了 TensorFlow Recommenders，因为它扩展性强且非常表达清晰：你可以使用任何子模型——只要它输出一个嵌入——然后将它们组合在一起，使用
    `tfrs.Model` 类进行联合训练。
- en: After training, you can use a convenient class to make actual predictions. If
    you use ScaNN, this should be quite fast, but if you need a search on steroids,
    you can use dedicated vector databases like [Qdrant](https://qdrant.tech/). You
    give it the user and movie embeddings from the trained models, and it does the
    search for you.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 经过训练后，你可以使用一个方便的类来进行实际的预测。如果你使用 ScaNN，这应该会非常快，但如果你需要更强大的搜索功能，你可以使用像 [Qdrant](https://qdrant.tech/)
    这样的专用向量数据库。你将训练模型中的用户和电影嵌入提供给它，它会为你进行搜索。
- en: We have also taken a glimpse into the internals of the library to understand
    where the to-be-minimized loss is coming from, so this library is no pure magic
    anymore.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还查看了库的内部结构，以了解要最小化的损失来自哪里，因此这个库不再是纯粹的魔法。
- en: 'If you want to learn how to assess the quality of an implicit feedback recommender,
    please refer to my other article:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想了解如何评估隐式反馈推荐系统的质量，请参考我的另一篇文章：
- en: '[](/the-guide-to-recommender-metrics-c5d72193ea2b?source=post_page-----8ba36a976c57--------------------------------)
    [## The Guide to Recommender Metrics'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/the-guide-to-recommender-metrics-c5d72193ea2b?source=post_page-----8ba36a976c57--------------------------------)
    [## 推荐系统度量指南'
- en: Evaluating a recommender system offline can be tricky
  id: totrans-126
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 离线评估推荐系统可能会很棘手
- en: towardsdatascience.com](/the-guide-to-recommender-metrics-c5d72193ea2b?source=post_page-----8ba36a976c57--------------------------------)
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/the-guide-to-recommender-metrics-c5d72193ea2b?source=post_page-----8ba36a976c57--------------------------------)
- en: I hope that you learned something new, interesting, and valuable today. Thanks
    for reading!
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 我希望你今天学到了一些新的、有趣的和有价值的东西。感谢你的阅读！
- en: '*If you have any questions, write me on* [*LinkedIn*](https://www.linkedin.com/in/dr-robert-k%C3%BCbler-983859150/)*!*'
  id: totrans-129
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*如果你有任何问题，可以在* [*LinkedIn*](https://www.linkedin.com/in/dr-robert-k%C3%BCbler-983859150/)*上写信给我*！'
- en: And if you want to dive deeper into the world of algorithms, give my new publication
    **All About Algorithms** a try! I’m still searching for writers!
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想更深入地了解算法的世界，可以尝试我的新出版物**《全面了解算法》**！我还在寻找作者！
- en: '[](https://medium.com/all-about-algorithms?source=post_page-----8ba36a976c57--------------------------------)
    [## All About Algorithms'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/all-about-algorithms?source=post_page-----8ba36a976c57--------------------------------)
    [## 全面了解算法'
- en: From intuitive explanations to in-depth analysis, algorithms come to life with
    examples, code, and awesome…
  id: totrans-132
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 从直观的解释到深入的分析，算法通过示例、代码和精彩的内容变得生动起来…
- en: medium.com](https://medium.com/all-about-algorithms?source=post_page-----8ba36a976c57--------------------------------)
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: medium.com](https://medium.com/all-about-algorithms?source=post_page-----8ba36a976c57--------------------------------)
