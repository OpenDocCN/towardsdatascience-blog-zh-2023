- en: Challenges in Stop Generation within Llama 2
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Llama 2中的停止生成挑战
- en: 原文：[https://towardsdatascience.com/challenges-in-stop-generation-within-llama-2-25f5fea8dea2?source=collection_archive---------1-----------------------#2023-09-10](https://towardsdatascience.com/challenges-in-stop-generation-within-llama-2-25f5fea8dea2?source=collection_archive---------1-----------------------#2023-09-10)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/challenges-in-stop-generation-within-llama-2-25f5fea8dea2?source=collection_archive---------1-----------------------#2023-09-10](https://towardsdatascience.com/challenges-in-stop-generation-within-llama-2-25f5fea8dea2?source=collection_archive---------1-----------------------#2023-09-10)
- en: An Exploration with Potential Solutions
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 潜在解决方案的探索
- en: '[](https://medium.com/@vanillaxiangshuyang?source=post_page-----25f5fea8dea2--------------------------------)[![Shuyang
    Xiang](../Images/36a5fd18fd9b7b88cb41094f09b83882.png)](https://medium.com/@vanillaxiangshuyang?source=post_page-----25f5fea8dea2--------------------------------)[](https://towardsdatascience.com/?source=post_page-----25f5fea8dea2--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----25f5fea8dea2--------------------------------)
    [Shuyang Xiang](https://medium.com/@vanillaxiangshuyang?source=post_page-----25f5fea8dea2--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@vanillaxiangshuyang?source=post_page-----25f5fea8dea2--------------------------------)[![Shuyang
    Xiang](../Images/36a5fd18fd9b7b88cb41094f09b83882.png)](https://medium.com/@vanillaxiangshuyang?source=post_page-----25f5fea8dea2--------------------------------)[](https://towardsdatascience.com/?source=post_page-----25f5fea8dea2--------------------------------)[![数据科学前沿](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----25f5fea8dea2--------------------------------)
    [Shuyang Xiang](https://medium.com/@vanillaxiangshuyang?source=post_page-----25f5fea8dea2--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F9b74bc8c860d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fchallenges-in-stop-generation-within-llama-2-25f5fea8dea2&user=Shuyang+Xiang&userId=9b74bc8c860d&source=post_page-9b74bc8c860d----25f5fea8dea2---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----25f5fea8dea2--------------------------------)
    ·9 min read·Sep 10, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F25f5fea8dea2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fchallenges-in-stop-generation-within-llama-2-25f5fea8dea2&user=Shuyang+Xiang&userId=9b74bc8c860d&source=-----25f5fea8dea2---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F9b74bc8c860d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fchallenges-in-stop-generation-within-llama-2-25f5fea8dea2&user=Shuyang+Xiang&userId=9b74bc8c860d&source=post_page-9b74bc8c860d----25f5fea8dea2---------------------post_header-----------)
    发表在 [数据科学前沿](https://towardsdatascience.com/?source=post_page-----25f5fea8dea2--------------------------------)
    · 9 分钟阅读 · 2023年9月10日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F25f5fea8dea2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fchallenges-in-stop-generation-within-llama-2-25f5fea8dea2&user=Shuyang+Xiang&userId=9b74bc8c860d&source=-----25f5fea8dea2---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F25f5fea8dea2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fchallenges-in-stop-generation-within-llama-2-25f5fea8dea2&source=-----25f5fea8dea2---------------------bookmark_footer-----------)![](../Images/919c3755f4c37f74e7796e7097458f19.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F25f5fea8dea2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fchallenges-in-stop-generation-within-llama-2-25f5fea8dea2&source=-----25f5fea8dea2---------------------bookmark_footer-----------)![](../Images/919c3755f4c37f74e7796e7097458f19.png)'
- en: 'Llama: Photo by [Liudmila Shuvalova](https://unsplash.com/@liudmila19)'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 'Llama: 图片由[柳德米拉·舒瓦洛娃](https://unsplash.com/@liudmila19)提供'
- en: The launch of Llama 2 by Meta has ignited excitement within the community, marking
    the dawn of an era for well performed large language models that were previously
    only accessible through company-specific APIs.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: Meta发布的Llama 2在社区中引发了兴奋，标志着一个新的时代的开始，此前大型语言模型的良好表现只能通过公司特定的API访问。
- en: However, it is important to acknowledge some imperfections inherent in these
    models. Among them, the stop generation issue stands out prominently. My personal
    experiences have shown that these models often struggle to determine the appropriate
    ‘stop’ point, leaving them uncertain about when to end a text generation.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，重要的是要承认这些模型固有的一些缺陷。其中，生成停止问题尤为突出。我的个人经历表明，这些模型往往难以确定合适的‘停止’点，使它们在何时结束文本生成方面感到不确定。
- en: In this blog post, I will delve into the issue of stop generation failures in
    the smallest Llama 2 model, the Llama 2–7b model, and discuss several potential
    remedies. The implementation in the coming sections can be found in this GoogleGolab
    [notebook](https://colab.research.google.com/drive/12R6HXUYMbhGh6dhMUH6FiOWWUIrJCaz6?authuser=1#scrollTo=VjJCzLmYmWEo)
    with the runtime type T4.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇博客文章中，我将深入探讨最小的Llama 2模型——Llama 2–7b模型中停止生成失败的问题，并讨论几种潜在的解决方案。接下来的实现可以在这个GoogleGolab
    [notebook](https://colab.research.google.com/drive/12R6HXUYMbhGh6dhMUH6FiOWWUIrJCaz6?authuser=1#scrollTo=VjJCzLmYmWEo)中找到，运行时类型为T4。
- en: Stop generation failure
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 停止生成失败
- en: In this section, we will harness the power of a Llama 2–7b model using a T4
    GPU equipped with ample high RAM resources in Google Colab (2.21 credits/hour).
    It is essential to bear in mind that the T4 GPU comes with a VRAM capacity of
    16 GB, precisely enough to house Llama 2–7b’s weights (7b × 2 bytes = 14 GB in
    FP16).
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将利用Google Colab中配备充足高RAM资源的T4 GPU来操作Llama 2–7b模型（2.21积分/小时）。需要记住的是，T4
    GPU具有16 GB的VRAM容量，刚好可以容纳Llama 2–7b的权重（7b × 2 字节 = 14 GB的FP16）。
- en: To efficiently manage VRAM usage, we will employ a technique called quantization.
    Quantization is an approach that focuses on minimizing both computational and
    memory requirements during inference by representing weights and activations using
    low-precision data types.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 为了有效管理VRAM的使用，我们将采用一种叫做量化的技术。量化是一种在推理过程中通过使用低精度数据类型表示权重和激活值来最小化计算和内存需求的方法。
- en: Let’s now delve into the following code snippet. Here, we’ll demonstrate how
    to load the “meta-llama/Llama-2–7b-chat-hf” model with a Bite and Byte configuration
    and set up a text generation pipeline based on this loaded model.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们深入研究以下代码片段。在这里，我们将演示如何加载带有Bite和Byte配置的“meta-llama/Llama-2–7b-chat-hf”模型，并基于该加载模型设置文本生成管道。
- en: '[PRE0]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'This configuration enables us to generate text for a given prompt in under
    one minute. Let’s put it to the test with a straightforward question: “What can
    I do in Paris?” Below, you’ll find the answer (Please keep in mind that your results
    might vary due to temperature settings).'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 这个配置使我们能够在一分钟以内为给定提示生成文本。让我们用一个简单的问题进行测试：“我在巴黎可以做些什么？”下面是答案（请注意，由于温度设置不同，您的结果可能会有所不同）。
- en: '[PRE1]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: It’s apparent that the model struggles to produce a satisfactory response; it
    appears to have difficulty knowing when to conclude its output. Upon tokenizing
    the generated text, it becomes evident that the final token is not a 2, which
    represents the eos (end-of-sequence) token in the model’s tokenizer.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 很明显，这个模型难以生成令人满意的响应；它似乎在确定何时结束输出方面存在困难。通过对生成的文本进行分词处理，可以明显看出最终的标记不是2，而2代表了模型分词器中的eos（序列结束）标记。
- en: Upon closer examination of the token scores (probabilities) provided by the
    model, I noticed that the **token_id 2 (eso_token_id) has a score of “-inf.”**
    This implies that it has no possibility of being generated.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 通过仔细检查模型提供的标记分数（概率），我注意到**token_id 2 (eso_token_id) 的分数为“-inf。”**这意味着它不可能被生成。
- en: Attempts of problem resolution
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题解决尝试
- en: In this section, we will explore several potential solutions aimed at addressing
    the issue at hand. It’s essential to keep in mind that the solutions discussed
    herein represent proactive efforts, but they may not always provide resolutions
    to the problems in question.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将探讨几种旨在解决当前问题的潜在解决方案。需要记住的是，这里讨论的解决方案代表了积极的努力，但它们可能并不总是能解决所面临的问题。
- en: Logits Processor
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Logits处理器
- en: 'A language model like Llama 2 processes a sequence of text tokens as input
    and produces a sequence of conditional probabilities for the next token, based
    on the context from the initial token to the current one. In light of this, it’s
    worth considering manual adjustments to these probabilities as we approach the
    maximum token limit, with the goal of increasing the likelihood of encountering
    the eos token. We do it by defining our customized LogitsProcessor called “EosTokenRewardLogitsProcessor”
    swith two initial inputs eos_token_id and max_length where the latter represents
    the max length at which the model should generate a eos token:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 像Llama 2这样的语言模型处理一系列文本标记作为输入，并根据从初始标记到当前标记的上下文生成下一标记的条件概率。鉴于此，值得考虑在接近最大标记限制时手动调整这些概率，以提高遇到eos标记的可能性。我们通过定义一个名为“EosTokenRewardLogitsProcessor”的自定义Logits处理器来实现这一点，该处理器具有两个初始输入eos_token_id和max_length，其中后者表示模型应生成eos标记的最大长度：
- en: '[PRE2]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: In the “__call__” method of the class, we enhance the probability (score) of
    the eos_token based on the sequence’s length. When the length approaches 80% of
    the specified maximum length, we set the eos_token_id’s score to 1e2 multiplied
    by a length ratio and adjust the scores of other tokens downward accordingly.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在类的“__call__”方法中，我们根据序列的长度增强eos_token的概率（得分）。当长度接近指定最大长度的80%时，我们将eos_token_id的得分设置为1e2乘以长度比例，并相应地调整其他令牌的得分。
- en: 'Now declare the logits processor in the pipeline’s definition:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 现在在管道的定义中声明logits处理器：
- en: '[PRE3]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Run the pipeline again with same prompt “What Can I do in Paris” and we get:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 使用相同的提示“What Can I do in Paris”再次运行管道，我们得到：
- en: '[PRE4]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: It works well! We have got a complete answer even if it might look short.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 它运行得很好！我们即使得到的答案可能看起来很简短，但它是完整的。
- en: Fine-Tuning
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 微调
- en: If the model fails to generate the EOS token, why not consider instructing it
    to do so? The concept of enhancing the model’s performance by fine-tuning it with
    a dataset that includes answers concluding with the EOS token is certainly a promising
    avenue to explore.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 如果模型未能生成EOS令牌，为什么不考虑指示它这样做呢？通过使用包括以EOS令牌结尾的答案的数据集来微调模型，以提高模型性能的概念无疑是一个值得探索的有前景的途径。
- en: 'In this section, I will use shamelessly the groundwork laid out in this blog
    post that employed a parameter-efficient fine-tuning (PEFT) method, such as QLoRA,
    to fine-tune the Llama 2–7b model. Much like its predecessor, LoRA, QLoRA utilizes
    a small set of trainable parameters (adapters) while keeping the core model parameters
    unchanged. It introduces two noteworthy innovations: 4-bit NormalFloat (NF4),
    an information-theoretically optimal data quantization method for normal data,
    and Double Quantization. For a more in-depth understanding, please consult the
    [original paper](https://arxiv.org/abs/2305.14314), should you have any further
    interest in this topic.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我将毫不掩饰地使用这篇博客文章中奠定的基础，这篇文章采用了参数高效的微调（PEFT）方法，如QLoRA，来微调Llama 2–7b模型。与其前身LoRA类似，QLoRA利用一小组可训练的参数（适配器），同时保持核心模型参数不变。它引入了两个值得注意的创新：4-bit
    NormalFloat (NF4)，一种对正常数据信息理论上最优的数据量化方法，以及双重量化。欲了解更多深入信息，请参考[原始论文](https://arxiv.org/abs/2305.14314)，如果您对该主题有进一步的兴趣。
- en: Let us train the model on a dataset called ‘timdettmers/openassistant-guanaco’
    where you can find on hugging face database. This dataset has the following format
    where the human and assistant’s conversation is separated by “###”.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在一个名为‘timdettmers/openassistant-guanaco’的数据集上训练模型，您可以在hugging face数据库中找到这个数据集。该数据集的格式如下，其中人类和助手的对话由“###”分隔。
- en: '![](../Images/a593fefb022a90da04e27e82ecb7e454.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a593fefb022a90da04e27e82ecb7e454.png)'
- en: 'Image author: “timdettmers/openassistant-guanaco’/ dataset'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 图片作者：“timdettmers/openassistant-guanaco”的数据集
- en: 'Before training, we have to transform the data into the Llama 2 prompt template:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练之前，我们需要将数据转换为Llama 2提示模板：
- en: '[PRE5]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'I will skip the detail of the dataset transformation here. Now let us take
    a look of the main part of training given by the following code:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 我将在这里跳过数据集转换的细节。现在让我们看看以下代码给出的训练的主要部分：
- en: '[PRE6]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: In the context of a dataset comprising instructions and responses, our approach
    involved the use of a Supervised Trainer (SFTainer) in conjunction with the QLoRA
    method to fine-tune the weight parameters within the Language Model (LLM). Our
    primary objective was to minimize the discrepancies between the generated answers
    and the ground-truth responses, which served as our reference labels.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在一个包含指令和响应的数据集中，我们的方法涉及使用监督训练器（SFTainer）与QLoRA方法相结合，以微调语言模型（LLM）中的权重参数。我们的主要目标是最小化生成的答案与真实响应之间的差异，真实响应作为我们的参考标签。
- en: A significant parameter within this configuration is “lora r,” representing
    a relatively small value pertaining to both the second and first dimensions of
    the pairs of rank-decomposition weight matrices. Training occurred exclusively
    on these two matrices, complementing the existing weights.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个配置中，一个重要的参数是“lora r”，它代表了一个相对较小的值，涉及到秩分解权重矩阵的第二维和第一维。训练仅在这两个矩阵上进行，补充了现有的权重。
- en: 'We train the model for 250 steps with training loss given in the plot below:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 我们训练模型250步，训练损失如下图所示：
- en: '![](../Images/21bae74012046266638a79cc1aba0c4c.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/21bae74012046266638a79cc1aba0c4c.png)'
- en: 'Image by author: training loss of llama 2 for 250 steps'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：Llama 2训练250步的损失
- en: 'Now let us run the pipeline with the fine-tuned model. This time, we get:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们使用微调后的模型运行管道。这一次，我们得到了：
- en: '[PRE7]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: This is rather a beautiful answer!
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个相当美丽的答案！
- en: 'Bonus: give a different prompt'
  id: totrans-50
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附加：给出不同的提示
- en: 'Ultimately, there’s a straightforward yet effective trick at our disposal,
    especially when the model’s verbosity is not a concern. We can explicitly mention
    in the prompt that we require a concise response. For instance, when I ask the
    model, “What can I do in Paris? Respond in five sentences or fewer,” it provides:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，我们手头有一个简单而有效的技巧，特别是当模型的冗长不是问题时。我们可以在提示中明确说明我们需要一个简洁的回答。例如，当我问模型：“在巴黎我可以做什么？请用五句话或更少的句子回答，”它会提供：
- en: '[PRE8]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: It is a short but clean and complete answer.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个简短但干净且完整的回答。
- en: 'Stopping Criteria: an unsucessful attempt'
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 停止标准：一个失败的尝试
- en: 'For those who are interested, Hugging Face has introduced another API called
    StoppingCriteria, intended for establishing specific conditions that compel a
    sequence to halt. However, when it comes to defining a customized criterion that
    stops the model upon encountering certain tokens (e.g., ‘\n’), it may not provide
    a comprehensive solution to the issue. As an example, I attempted to create a
    StopOnTokens class:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 对于感兴趣的用户，Hugging Face推出了另一个名为StoppingCriteria的API，旨在建立特定条件以强制序列停止。然而，当涉及到定义一个在遇到某些标记（例如‘\n’）时停止模型的自定义标准时，它可能无法提供一个全面的解决方案。例如，我尝试创建一个StopOnTokens类：
- en: '[PRE9]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: However, the model still fails to give a complete answer.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，模型仍然无法给出完整的回答。
- en: Conclusion
  id: totrans-58
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: In this blog post, I highlighted the issue of generation stop in Llama 2 and
    introduced several interim solutions. Again, I skip lots of details of implementations
    and I recommend you to have a deeper look of my notebook.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇博客文章中，我强调了Llama 2中生成停止的问题，并介绍了几种临时解决方案。再次，我跳过了很多实施细节，我建议你深入查看我的笔记本。
- en: '![](../Images/d6efeaff2e28e01d03f2e75eca9f4d09.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d6efeaff2e28e01d03f2e75eca9f4d09.png)'
- en: Image by [Jose Aragones](https://unsplash.com/@jodaarba)
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由[Jose Aragones](https://unsplash.com/@jodaarba)提供
- en: However, it’s important to note that these solutions are meant to enhance the
    user-friendliness of the responses in the short term, but we are eagerly anticipating
    a permanent fix to address this matter.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，需要注意的是，这些解决方案旨在在短期内提高响应的用户友好性，但我们迫切期待一个永久的解决方案来解决这个问题。
