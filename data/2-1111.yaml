- en: How to Avoid Five Common Mistakes in Google BigQuery / SQL
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何避免 Google BigQuery / SQL 中的五个常见错误
- en: 原文：[https://towardsdatascience.com/how-to-avoid-five-common-mistakes-in-google-bigquery-sql-6fafab396d88](https://towardsdatascience.com/how-to-avoid-five-common-mistakes-in-google-bigquery-sql-6fafab396d88)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/how-to-avoid-five-common-mistakes-in-google-bigquery-sql-6fafab396d88](https://towardsdatascience.com/how-to-avoid-five-common-mistakes-in-google-bigquery-sql-6fafab396d88)
- en: While working with BigQuery for years, I observed 5 issues that are commonly
    made, even by experienced Data Scientists
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在使用 BigQuery 多年的过程中，我观察到即使是经验丰富的数据科学家也常常犯的 5 个问题
- en: '[](https://medium.com/@benjamin.thuerer?source=post_page-----6fafab396d88--------------------------------)[![Benjamin
    Thürer](../Images/b4c49698c7270c592bf992fc47f75765.png)](https://medium.com/@benjamin.thuerer?source=post_page-----6fafab396d88--------------------------------)[](https://towardsdatascience.com/?source=post_page-----6fafab396d88--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----6fafab396d88--------------------------------)
    [Benjamin Thürer](https://medium.com/@benjamin.thuerer?source=post_page-----6fafab396d88--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@benjamin.thuerer?source=post_page-----6fafab396d88--------------------------------)[![本杰明·图赫尔](../Images/b4c49698c7270c592bf992fc47f75765.png)](https://medium.com/@benjamin.thuerer?source=post_page-----6fafab396d88--------------------------------)[](https://towardsdatascience.com/?source=post_page-----6fafab396d88--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----6fafab396d88--------------------------------)
    [本杰明·图赫尔](https://medium.com/@benjamin.thuerer?source=post_page-----6fafab396d88--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----6fafab396d88--------------------------------)
    ·8 min read·Oct 25, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----6fafab396d88--------------------------------)
    ·阅读时间 8 分钟·2023 年 10 月 25 日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/6b6e9ac6ded4356c801ecb5e2396fdfb.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6b6e9ac6ded4356c801ecb5e2396fdfb.png)'
- en: Photo by [James Harrison](https://unsplash.com/@jstrippa?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[詹姆斯·哈里森](https://unsplash.com/@jstrippa?utm_source=medium&utm_medium=referral)
    拍摄于 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)'
- en: Google BigQuery is popular for many reasons. It is incredibly fast, easy to
    work with, provides the full GCP suite, takes care of your data, and ensures to
    catch mistakes early on. On top of that, you can use standard SQL and some very
    nice built-in functions. Put short, it is almost the full package!
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: Google BigQuery 受欢迎有很多原因。它速度极快，易于使用，提供完整的 GCP 套件，照顾好你的数据，并确保早期发现错误。更重要的是，你可以使用标准
    SQL 和一些非常不错的内置函数。简而言之，它几乎是一个完整的套餐！
- en: Always assume bugs and duplicates, always!
  id: totrans-9
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 始终假设有错误和重复项，始终如此！
- en: However, similar to other web services and programming languages, when working
    with BigQuery there are a few things one needs to know to avoid falling into a
    trap. Over the years, I made a lot of mistakes on my own and realized that almost
    everyone I knew, at some point, encountered the same issues. A handful of these
    issues I want to call out here because I discovered those fairly late in my career
    and also see other very experienced data scientists encountering the same issues.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，与其他网络服务和编程语言类似，在使用 BigQuery 时，有一些事项需要了解，以避免陷入陷阱。多年来，我自己犯了很多错误，并意识到几乎所有我认识的人在某些时候都会遇到相同的问题。我在这里想指出其中的一些问题，因为我在职业生涯中发现这些问题较晚，并且也看到其他经验丰富的数据科学家遇到相同的问题。
- en: 'Therefore, I will provide you with my top 5 list of potential mistakes almost
    everyone makes in BigQuery at some point and which one might not even know about.
    So make sure to avoid these because each point can have severe consequences and
    keep in mind the right attitude when working with data: Always assume bugs and
    duplicates, always!'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我将提供我认为几乎所有人都会在某个时候在 BigQuery 中犯的潜在错误的前 5 名列表，并且有些错误可能甚至不知道。务必避免这些错误，因为每一点都可能导致严重后果，并记住在处理数据时要有正确的态度：始终假设有错误和重复项，始终如此！
- en: 1\. Be Careful When Using “NOT IN”
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 1\. 使用 “NOT IN” 时要小心
- en: It happens so fast. You are in a hurry and want to quickly check two tables
    and see if a certain item mentioned in one of the tables also exists inside of
    a second table. Why not then go with a `NOT IN` statement, since it sounds so
    intuitive?
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 这发生得很快。你很匆忙，想快速检查两个表，看看一个表中提到的某个项目是否也存在于第二个表中。既然 `NOT IN` 语句听起来如此直观，为什么不使用它呢？
- en: The problem is that `NOT IN` doesn’t work as intended when you have `NULL` values
    in your table. If so, you will not get the results you desire!
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 问题是当表中有`NULL`值时，`NOT IN`不会按预期工作。如果是这样，你将得不到想要的结果！
- en: 'See for yourself and check out this code example in which I am just trying
    to find the categories from *input_2* that are **not** inside of *input_1*:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 看看这个代码示例，我只是尝试找出*input_2*中**不**在*input_1*中的类别：
- en: '[PRE0]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Pay attention to the *input_1* cell that does not have a *c* category, yet,
    the result of this query will be `There is no data to display`.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 注意*input_1*单元格没有*c*类别，但这个查询的结果将是`没有数据可显示`。
- en: 'To overcome this, make sure you use a `LEFT JOIN`or remove all `NULL` values
    as shown in this slightly adjusted code (pay attention to the very last line):'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 为了克服这个问题，请确保使用`LEFT JOIN`或删除所有`NULL`值，如稍微调整的代码所示（注意最后一行）：
- en: '[PRE1]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 2\. You Might Lose Rows When “UNNEST” Data
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 2\. 使用“UNNEST”数据时你可能会丢失行
- en: When you work with BigData, nested columns are a must-have. They will save money
    and keep things organized. Also, sometimes nesting your data is a convenient step
    to limit your data based on a specific order for a partition.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 当你处理BigData时，嵌套列是必备的。它们将节省费用并保持数据组织有序。此外，有时将数据嵌套是根据分区的特定顺序限制数据的一个方便步骤。
- en: However, when unnesting your data, be aware that you might lose data! When using
    `UNNEST` in a `CROSS JOIN`, you will lose all rows for which there is no data
    inside the nested column.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在展开数据时，要注意你可能会丢失数据！当在`CROSS JOIN`中使用`UNNEST`时，你将丢失所有嵌套列中没有数据的行。
- en: 'Check out this example where we have 2 different IDs defined but only one has
    data in the nested column:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 查看这个例子，我们定义了2个不同的ID，但只有一个在嵌套列中有数据：
- en: '[PRE2]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: When running this code, you will see that due to the `CROSS JOIN` ID number
    2 will be lost. That can have a severe impact on your data since you are essentially
    filtering out that ID from your subsequent analysis.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 运行此代码时，你将看到由于`CROSS JOIN`，ID号2将丢失。这可能对你的数据产生严重影响，因为你实际上是将该ID从后续分析中过滤掉了。
- en: 'One way to improve is to use a `LEFT JOIN` instead of the `CROSS JOIN` as shown
    in the code below (please pay attention to the last row):'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 一种改进方法是使用`LEFT JOIN`代替下面代码中的`CROSS JOIN`（请注意最后一行）：
- en: '[PRE3]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 3\. Costs in BigQuery ML are Often Higher Than Estimated
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 3\. BigQuery ML中的成本往往高于估算
- en: Estimating costs is important. When using BigQuery, it is likely you are working
    with big data (because that is where BigQuery really shines). One of the intuitive
    things with BigQuery is that query costs are automatically estimated and BigQuery
    provides you with the maximum costs based on the estimated partition pruning.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 估算成本很重要。当使用BigQuery时，你可能在处理大数据（因为BigQuery的强项就在于此）。BigQuery的一个直观特点是查询成本会自动估算，并且根据估算的分区修剪提供最大成本。
- en: For instance, when the cost estimate is 1 TB, you can expect the query to cost
    6.25 dollars or less (because 6.25$ / TB is the on-demand pricing model; when
    using clustering, you can reduce the amount of data used even further).
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，当成本估算为1 TB时，你可以预期查询费用为6.25美元或更少（因为6.25美元/ TB是按需定价模式；使用聚类时，你甚至可以进一步减少使用的数据量）。
- en: '![](../Images/ad8a1010305241929e0515296cffb7bb.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ad8a1010305241929e0515296cffb7bb.png)'
- en: 'Figure 1: Estimate for the model training based on the Chicago taxi trips dataset.
    When training the model, it will actually bill 51.6 GB. (Image by author)'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：基于芝加哥出租车旅行数据集的模型训练估算。当训练模型时，它实际上将计费51.6 GB。（作者提供的图像）
- en: 'However, BigQuery ML is different! Two things I like to point out here:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，BigQuery ML 是不同的！我想在这里指出两点：
- en: It has a different pricing model where for instance linear regression is fairly
    expensive (312.5$ / TB).
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它有不同的定价模式，例如线性回归相当昂贵（312.5美元/ TB）。
- en: The processing costs are dependent on the underlying Vertex AI training costs.
    Unfortunately, BigQuery can’t tell you before running the query how large the
    passthrough Vertex AI costs are going to be. So you might pay much more than what
    has been estimated.
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 处理成本取决于底层的Vertex AI训练成本。不幸的是，在运行查询之前，BigQuery无法告诉你透传的Vertex AI成本将是多少。所以你可能会支付比估计的多得多的费用。
- en: 'Let’s take an example to highlight point 2\. Here, I try to predict trip length
    based on the day and the hour using the publicly available Chicago taxi trips
    dataset:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过一个例子来突出第2点。在这里，我尝试基于天和小时来预测旅行长度，使用的是公开的芝加哥出租车旅行数据集：
- en: '[PRE4]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: When estimating the processing costs, it estimates 3.12 GB. But pay attention
    to the `(ML)` in the estimate (see Figure 1). Because that is the clue telling
    you that it might get more expensive. And as a matter of fact, it did. Training
    this model resulted in 51.63 GB billed and, thus, has been about 17x more expensive
    than the original estimate.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在估算处理成本时，它估计为 3.12 GB。但请注意估算中的 `(ML)`（见图 1）。因为这提示你可能会变得更昂贵。事实上，它确实如此。训练此模型的结果是账单为
    51.63 GB，因此比原始估算贵了大约 17 倍。
- en: 4\. Avoid MERGE Statements When Parallelising Jobs in Production
  id: totrans-39
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 4. 在生产环境中并行化作业时避免使用 MERGE 语句
- en: Updating an existing table is a common situation when you have production pipelines.
    That is because you continuously get new data you want to append but you also
    sometimes have historical changes which you like to be reflected in your table.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 更新现有表是生产流水线中的常见情况。这是因为你不断获取新数据需要追加，但有时你也希望历史变化能够反映在你的表中。
- en: That being said, it is important to have a scalable setup for updating tables.
    One popular choice is via `MERGE` statements. Those allow you to update, or insert
    data based on specific conditions. What makes them really powerful is that they
    will prevent you from introducing duplicates because `MERGE` statements allow
    you only to update one matching row based on the defined conditions.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 也就是说，拥有一个可扩展的更新表的设置很重要。一个流行的选择是使用 `MERGE` 语句。它们允许你根据特定条件更新或插入数据。它们真正强大的地方在于，因为
    `MERGE` 语句允许你根据定义的条件只更新一行匹配的数据，从而防止引入重复数据。
- en: However, there is a problem. `MERGE` statements do not allow very much for parallelization.
    Here is an example query that tries to merge data into an existing table. When
    you run the `MERGE` statement several times in parallel, you will encounter some
    problems.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，有一个问题。`MERGE` 语句不允许很好的并行化。以下是一个示例查询，尝试将数据合并到现有表中。当你并行运行 `MERGE` 语句时，你会遇到一些问题。
- en: 'First, we create a dummy table which we want to merge into:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们创建一个虚拟表，我们希望将数据合并到其中：
- en: '[PRE5]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Then we run this query several times in parallel which just inserts or updates
    new dates into the dummy table:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们并行运行这个查询，它只向虚拟表中插入或更新新的日期：
- en: '[PRE6]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'When running this in parallel, you will observe two issues:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 当并行运行时，你会观察到两个问题：
- en: 'You will encounter an error message when an `UPDATE` statement is used on the
    same partition. The error message will look like this: *Could not serialize access
    to table your_project.your_dataset.merge_test due to concurrent update.*'
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当在同一分区上使用 `UPDATE` 语句时，你会遇到一个错误消息。错误消息如下：*由于并发更新，无法序列化对表 your_project.your_dataset.merge_test
    的访问。*
- en: You will also observe that the queries that were triggered a bit later are for
    a long time (several seconds) in a *pending* mode and not *running*.
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你还会观察到，稍后触发的查询在 *挂起* 状态中停留了很长时间（几秒钟），而不是 *运行* 状态。
- en: Even though `MERGE` statements are very useful and powerful, keep in mind when
    you have a production environment that is designed to parallelize data ingestions
    into the same table. The query will either crash due to partition conflicts in
    the update or, when having more than 2 tasks running in parallel, queries will
    be in the *pending* mode and you lose the benefits of parallelization.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管 `MERGE` 语句非常有用且强大，但请记住，在一个设计用于将数据并行化到同一表中的生产环境中时，查询要么由于更新中的分区冲突而崩溃，要么当并行运行超过
    2 个任务时，查询将处于 *挂起* 状态，从而失去了并行化的好处。
- en: One way to avoid that is to use `DELETE / INSERT` statements or creating temporary
    tables that are merged together in a batch at a later point.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 避免这种情况的一种方法是使用 `DELETE / INSERT` 语句或创建临时表，稍后将它们批量合并在一起。
- en: 5\. Partition and Cluster Even Temp Tables
  id: totrans-52
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 5. 分区和集群临时表
- en: Temporary (or `TEMP`) tables are very useful when you are constructing a complex
    query with several subqueries. They also come in handy when you are working with
    big data where you want to load the data once and then use it several times in
    subsequent parts of the query. So when you are running into a potential timeout
    issue that your query runs too long, they might be what you are looking for.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 临时（或 `TEMP`）表在构建包含多个子查询的复杂查询时非常有用。当你处理大数据并希望一次加载数据然后在查询的后续部分多次使用时，它们也很方便。所以，当你遇到可能的超时问题时，查询运行时间过长，它们可能是你所寻找的解决方案。
- en: However, often users see the benefits only in the possibility of creating a
    temporary table that is materialized while the whole query is running. But that
    is not the full picture as, similar to every other materialized table, `TEMP`
    tables can be *partitioned* and *clustered*. This provides additional advantages
    with which you can reduce your query costs and also improve your runtime (when
    working with really big data).
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，用户通常只看到创建一个在整个查询运行时会被物化的临时表的好处。但这并不是全部，因为类似于其他所有物化表，`TEMP` 表也可以进行*分区*和*聚类*。这提供了额外的优势，可以减少查询成本，并改善运行时（特别是在处理非常大的数据时）。
- en: 'The attached query is not very useful but it shows the concept. The query creates
    two `TEMP` tables based on the publicly available Chicago taxi trips dataset where
    one is using the full dataset and the other only a subset of that. In the last
    query, those two tables are joined together to filter down to the subset. Since
    the `TEMP` tables are partitioned and clustered, the actual query that joins those
    two temp tables together bills only 1GB while without partitioning it would bill
    36GB:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 附带的查询并不是非常有用，但它展示了这个概念。该查询基于公开的芝加哥出租车行程数据集创建了两个`TEMP` 表，其中一个使用完整的数据集，另一个仅使用该数据集的一部分。在最后的查询中，这两个表被连接在一起，以筛选出子集。由于`TEMP`
    表是分区和聚类的，实际将这两个临时表连接在一起的查询只会计费1GB，而不进行分区时则会计费36GB：
- en: '[PRE7]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: In addition, when working with big datasets and performing spatial indexing,
    performing the joins on the clustered and partitioned `TEMP` tables will improve
    your runtime as well.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，在处理大型数据集并进行空间索引时，在聚类和分区的`TEMP` 表上执行连接操作也会改善运行时间。
- en: Summary
  id: totrans-58
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: 'We all make mistakes and that is a good thing, as long as we learn from them.
    Those 5 issues mentioned in this story are for me the most important to mention
    because they are not very well known and beginners encounter them rarely. Few
    people think about them but when getting more and more experienced, at some point,
    those 5 issues might impact the data or production environment in a negative way.
    So pay attention to those and also remember the attitude when working with data
    in general: *Always assume bugs and duplicates, always!*'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 我们都会犯错，这是一件好事，只要我们从中学习。这篇故事中提到的5个问题对我来说是最重要的，因为它们并不是很知名，而且初学者遇到的机会很少。很少有人考虑这些问题，但随着经验的增长，这5个问题中的某些可能会以负面方式影响数据或生产环境。所以请注意这些问题，并记住在处理数据时的态度：*总是假设存在错误和重复，永远如此！*
