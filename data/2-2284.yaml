- en: V-Net, U-Net’s big brother in Image Segmentation
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: V-Net，U-Net 在图像分割中的“大哥”
- en: 原文：[https://towardsdatascience.com/v-net-u-nets-big-brother-in-image-segmentation-906e393968f7](https://towardsdatascience.com/v-net-u-nets-big-brother-in-image-segmentation-906e393968f7)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/v-net-u-nets-big-brother-in-image-segmentation-906e393968f7](https://towardsdatascience.com/v-net-u-nets-big-brother-in-image-segmentation-906e393968f7)
- en: Welcome to this guide about the V-Net, the cousin of the well known U-Net, for
    3D images segmentations. You will know it inside out!
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 欢迎阅读这本关于 V-Net 的指南，它是著名的 U-Net 的“亲戚”，专用于 3D 图像分割。你将对它了如指掌！
- en: '[](https://medium.com/@francoisporcher?source=post_page-----906e393968f7--------------------------------)[![François
    Porcher](../Images/9ddb233f8cadbd69026bd79e2bd62dea.png)](https://medium.com/@francoisporcher?source=post_page-----906e393968f7--------------------------------)[](https://towardsdatascience.com/?source=post_page-----906e393968f7--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----906e393968f7--------------------------------)
    [François Porcher](https://medium.com/@francoisporcher?source=post_page-----906e393968f7--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@francoisporcher?source=post_page-----906e393968f7--------------------------------)[![François
    Porcher](../Images/9ddb233f8cadbd69026bd79e2bd62dea.png)](https://medium.com/@francoisporcher?source=post_page-----906e393968f7--------------------------------)[](https://towardsdatascience.com/?source=post_page-----906e393968f7--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----906e393968f7--------------------------------)
    [François Porcher](https://medium.com/@francoisporcher?source=post_page-----906e393968f7--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----906e393968f7--------------------------------)
    ·8 min read·Jul 28, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表在[Towards Data Science](https://towardsdatascience.com/?source=post_page-----906e393968f7--------------------------------)
    ·阅读时间 8 分钟·2023 年 7 月 28 日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: Welcome to an exciting journey through the world of deep learning architectures!
    You may already be familiar with [U-Net](https://medium.com/@foporcher/cooking-your-first-u-net-for-image-segmentation-e812e37e9cd0),
    a game-changer in computer vision that has significantly reshaped the landscape
    of image segmentation.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 欢迎踏上探索深度学习架构的激动人心的旅程！你可能已经对[U-Net](https://medium.com/@foporcher/cooking-your-first-u-net-for-image-segmentation-e812e37e9cd0)有所了解，它是计算机视觉领域的一个重大突破，显著重塑了图像分割的格局。
- en: Today, let’s turn the spotlight onto U-Net’s big brother, the V-Net.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 今天，让我们将焦点转向 U-Net 的“大哥”——V-Net。
- en: 'Published by researchers Fausto Milletari, Nassir Navab, and Seyed-Ahmad Ahmadi,
    the paper [“VNet: Fully Convolutional Neural Networks for Volumetric Medical Image
    Segmentation”](https://arxiv.org/abs/1606.04797) introduced a breakthrough methodology
    for 3D image analysis.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '由研究人员 Fausto Milletari、Nassir Navab 和 Seyed-Ahmad Ahmadi 发表的论文[“VNet: Fully
    Convolutional Neural Networks for Volumetric Medical Image Segmentation”](https://arxiv.org/abs/1606.04797)介绍了一种突破性的
    3D 图像分析方法。'
- en: This article will take you on a tour of this groundbreaking paper, shedding
    light on its unique contributions and architectural advancements. Whether you’re
    a seasoned data scientist, a budding AI enthusiast, or just someone interested
    in the latest tech trends, there’s something here for you!
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 这篇文章将带你了解这篇开创性的论文，阐明其独特的贡献和架构进展。无论你是经验丰富的数据科学家、正在成长的 AI 爱好者，还是对最新科技趋势感兴趣的人，这里都有你需要的内容！
- en: A short reminder about U-Net
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 关于 U-Net 的简要提醒
- en: Before diving into the heart of V-Net, let’s take a moment to appreciate its
    architectural inspiration — U-Net. Don’t worry if this is your first introduction
    to U-Net; I’ve got you covered with a [quick and easy tutorial on the U-Net architecture](https://medium.com/@foporcher/cooking-your-first-u-net-for-image-segmentation-e812e37e9cd0).
    It’s so concise that you’ll grasp the concept in no more than five minutes!
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入 V-Net 的核心之前，让我们先欣赏一下它的架构灵感——U-Net。如果这是你第一次接触 U-Net，别担心，我有一个[关于 U-Net 架构的快速简单教程](https://medium.com/@foporcher/cooking-your-first-u-net-for-image-segmentation-e812e37e9cd0)来帮助你。它如此简洁，你最多五分钟就能掌握这个概念！
- en: 'Here’s a brief overview of U-Net for a refresher:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是对 U-Net 的简要回顾：
- en: '![](../Images/8b3d8c1b14e2b6d0b67b6eb9abc066de.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8b3d8c1b14e2b6d0b67b6eb9abc066de.png)'
- en: U Net Architecture, from [U Net article](https://arxiv.org/abs/1505.04597)
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: U Net 架构，见于[U Net 文章](https://arxiv.org/abs/1505.04597)
- en: 'U-Net is famed for its symmetrical structure, taking the form of a ‘U’. This
    architecture is composed of two distinct pathways:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: U-Net 以其对称结构而闻名，呈现“U”字形。这种架构由两个不同的路径组成：
- en: '**Contracting Pathway (Left):** Here, we progressively decrease the resolution
    of the image while increasing the number of filters.'
  id: totrans-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**收缩路径（左）：** 在这里，我们逐渐减少图像的分辨率，同时增加过滤器的数量。'
- en: '**Expanding Pathway (Right):** This pathway acts as the mirror image of the
    contracting pathway. We gradually decrease the number of filters while increase
    the resolution until it aligns with the original image size.'
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**扩展路径（右）：** 这个路径作为收缩路径的镜像。我们逐渐减少过滤器的数量，同时增加分辨率，直到与原始图像大小对齐。'
- en: The beauty of U-Net lies in its innovative use of **‘residual connections’**
    or **‘skip connections**’. These connect corresponding layers in the contracting
    and expanding paths, allowing the network to retain high-resolution details that
    are usually lost in the contracting process.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: U-Net的美在于其创新使用的**“残差连接”**或**“跳跃连接”**。这些连接收缩路径和扩展路径中的对应层，使网络能够保留在收缩过程中通常丢失的高分辨率细节。
- en: '![](../Images/eafd0fc04e3573f57b206526b2431a5c.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/eafd0fc04e3573f57b206526b2431a5c.png)'
- en: Residual Connection, from [U Net paper](https://arxiv.org/abs/1505.04597)
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 残差连接，来自 [U Net 论文](https://arxiv.org/abs/1505.04597)
- en: 'Why does this matter? Because it eases the gradient flow during backpropagation,
    particularly in the early layers. In essence, we circumvent the risk of vanishing
    gradients — a common problem where gradients approach zero, hindering the learning
    process:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么这很重要？因为它简化了反向传播过程中的梯度流动，特别是在早期层中。实质上，我们绕过了梯度消失的风险——一个常见的问题，梯度接近零，阻碍了学习过程：
- en: '![](../Images/8ee0d62e5c4e2e5c95970aa5c22290d7.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8ee0d62e5c4e2e5c95970aa5c22290d7.png)'
- en: Image from Author
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：作者
- en: Now, bearing this understanding of U-Net in mind, let’s transition into the
    world of V-Net. At its core, V-Net shares a similar encoder-decoder philosophy.
    But as you’ll soon discover, it comes with its own set of unique traits that set
    it apart from its sibling, U-Net.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，带着对U-Net的理解，让我们进入V-Net的世界。V-Net的核心与U-Net相似，采用类似的编码器-解码器理念。但正如你很快会发现的，它具有一套独特的特征，使其与其兄弟U-Net有所区别。
- en: '![](../Images/6c3382c5442ce9a2a60b468174e704bd.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6c3382c5442ce9a2a60b468174e704bd.png)'
- en: V-Net Architecture, from [VNet paper](https://arxiv.org/abs/1606.04797)
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: V-Net架构，来自 [VNet 论文](https://arxiv.org/abs/1606.04797)
- en: What sets V-Net apart from the U-Net?
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: V-Net与U-Net有何不同？
- en: Let’s dive in!
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们深入探讨吧！
- en: 'Difference 1: 3D Convolutions instead of 2D convolutions'
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 区别 1：3D卷积代替2D卷积
- en: The first difference is as clear as day. While U-Net was tailored for 2D image
    segmentation, medical images often require a 3D perspective (think of volumetric
    brain scans, CT scans, etc.).
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个区别显而易见。虽然U-Net是为2D图像分割量身定制的，但医学图像通常需要3D视角（例如体积脑扫描、CT扫描等）。
- en: This is where V-Net comes into play. The ‘V’ in V-Net stands for ‘Volumetric,’
    and this dimensionality shift requires the replacement of 2D convolutions with
    3D convolutions.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是V-Net发挥作用的地方。V-Net中的“V”代表“体积”，这种维度的变化要求将2D卷积替换为3D卷积。
- en: 'Difference 2: Activation Functions, PreLU instead of ReLU'
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 区别 2：激活函数，PreLU代替ReLU
- en: The realm of deep learning has fallen in love with the ReLU function, owing
    to its simplicity and computational efficiency. Compared to other functions like
    sigmoid or tanh, ReLU is “non-saturating,” meaning it reduces the issue of vanishing
    gradients.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习领域已经爱上了ReLU函数，因为它的简单性和计算效率。与sigmoid或tanh等其他函数相比，ReLU是“非饱和”的，这意味着它减少了梯度消失的问题。
- en: '![](../Images/5e731b05538d4cdfa61eac7c46fac7a0.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5e731b05538d4cdfa61eac7c46fac7a0.png)'
- en: (Left) ReLU, (Middle) LeakyReLU and (Last) PReLU, , from [PReLU paper](https://arxiv.org/abs/1502.01852v1)
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: （左）ReLU，（中）LeakyReLU和（右）PReLU，来自 [PReLU 论文](https://arxiv.org/abs/1502.01852v1)
- en: But ReLU isn’t perfect. It’s notorious for a phenomenon known as the ‘Dying
    ReLU problem,’ where many neurons always output zero, becoming ‘dead neurons.’
    To counter this, LeakyReLU was introduced, which has a small but nonzero slope
    on the left side of zero.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 但ReLU并不完美。它因一种被称为“Dying ReLU问题”的现象而臭名昭著，其中许多神经元总是输出零，成为“死神经元”。为了解决这个问题，引入了LeakyReLU，它在零的左侧有一个小但非零的斜率。
- en: Pushing the reasoning even further, V-Net leverages the Parametric ReLU (PReLU).
    Instead of hardcoding the slope of LeakyReLU, why not let the network learn it?
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 更进一步推理，V-Net利用了参数化ReLU（PReLU）。与其硬编码LeakyReLU的斜率，不如让网络来学习它？
- en: After all this is a core philosophy of Deep Learning, we want to put as little
    inductive bias as possible and let the model learn everything by itself, assuming
    we have enough data.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 毕竟，这是深度学习的核心哲学，我们希望尽可能少地引入归纳偏差，让模型自己学习一切，前提是我们有足够的数据。
- en: 'Difference 3: Different loss function based on the Dice Score'
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 区别 3：基于 Dice 分数的不同损失函数
- en: Now, we arrive at perhaps the most impactful contribution of V-Net — a shift
    in the loss function. Unlike U-Net’s cross entropy loss function, V-Net uses the
    Dice loss function.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们来到 V-Net 可能最具影响力的贡献——损失函数的改变。与 U-Net 的交叉熵损失函数不同，V-Net 使用 Dice 损失函数。
- en: '![](../Images/511b1b0a9a5e5ac72c65ac3a0294a130.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/511b1b0a9a5e5ac72c65ac3a0294a130.png)'
- en: Cross Entropy Function, Image from Author
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 交叉熵函数，来自作者
- en: But the main issue with this function is that it does not handle well unbalanced
    classes. And this issue is very frequent in medical images because most of the
    time the background is much more present than zone of interest.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 但这个函数的主要问题是它对不平衡的类别处理不好。而这个问题在医学图像中非常常见，因为大多数时候背景的存在远多于感兴趣区域。
- en: 'For example consider this picture:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 例如考虑这张图片：
- en: '![](../Images/5fc6e6c4ee2532b20604eb50a6bd93ca.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5fc6e6c4ee2532b20604eb50a6bd93ca.png)'
- en: Background is omnipresent, Image from Author
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 背景无处不在，来自作者
- en: As a result some models can get “lazy” and predict background everywhere because
    they will still get a small loss.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 结果是，一些模型可能变得“懒惰”，在任何地方都预测背景，因为它们仍然会得到一个较小的损失。
- en: 'So the V-Net uses a loss function that is much more effective for this matter:
    the Dice coefficient.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，V-Net 使用一种对这个问题更有效的损失函数：Dice 系数。
- en: The reason why it is better is that it measures the overlap between the predicted
    zone and the ground truth as a **proportion**, so the size of the class is taken
    into account.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 它更好的原因在于它将预测区域和真实值之间的重叠度量为**比例**，因此考虑了类别的大小。
- en: Even though the background is almost everywhere, the Dice score measures the
    overlap between the prediction and the ground truth, so we still get a number
    between 0 and 1 even though the class is preponderant.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管背景几乎无处不在，Dice 分数测量预测与真实值之间的重叠，因此即使类别占据主导地位，我们仍然可以得到一个介于 0 和 1 之间的数字。
- en: '![](../Images/18b6cc1f26d27f943c4e57d07a0cb8fc.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/18b6cc1f26d27f943c4e57d07a0cb8fc.png)'
- en: Dice Coefficient, from [VNet paper](https://arxiv.org/abs/1606.04797)
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: Dice 系数，来自 [VNet 论文](https://arxiv.org/abs/1606.04797)
- en: I am saying that this is maybe the main contribution of the article because
    going from 2D to 3D Convolutions is a very natural idea to handle 3D images. However
    this loss function has been very widely adopted in the Image Segmentation tasks.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 我认为这可能是本文的主要贡献，因为从 2D 卷积到 3D 卷积是处理 3D 图像的一个非常自然的想法。然而，这种损失函数已在图像分割任务中被广泛采用。
- en: In practice, a hybrid approach often proves effective, combining the Cross Entropy
    Loss and Dice Loss to leverage the strengths of both.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，混合方法通常被证明是有效的，结合交叉熵损失和 Dice 损失，以利用两者的优点。
- en: The Performance of the V-Net
  id: totrans-55
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: V-Net 的性能
- en: So, we’ve journeyed through the unique aspects of V-Net, but you’re probably
    thinking, “All this theory is great, but does V-Net really deliver in practice?”
    Well, let’s put V-Net to the test!
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们已经探讨了 V-Net 的独特方面，但你可能在想，“所有这些理论很棒，但 V-Net 实际上能否有效？”好吧，让我们对 V-Net 进行测试！
- en: The authors evaluated the V-Net performance on the PROMISE12 dataset.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 作者在 PROMISE12 数据集上评估了 V-Net 的性能。
- en: The **PROMISE12** dataset was made available for the MICCAI 2012 prostate segmentation
    challenge.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '**PROMISE12** 数据集是为 MICCAI 2012 前列腺分割挑战赛提供的。'
- en: The V-Net was trained on 50 Magnetic Resonance (MR) images, this is not a lot!
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: V-Net 在 50 张磁共振（MR）图像上进行训练，这个数量并不多！
- en: '![](../Images/1a294d06408f469afb9697f4d6c800e7.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1a294d06408f469afb9697f4d6c800e7.png)'
- en: Segmentation of VNet on the PROMISE 2012 challenge dataset, [from VNet paper](https://arxiv.org/abs/1606.04797)
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: VNet 在 PROMISE 2012 挑战数据集上的分割，[来自 VNet 论文](https://arxiv.org/abs/1606.04797)
- en: '![](../Images/fa5f0d97b7dab293d4751117aa31ed88.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fa5f0d97b7dab293d4751117aa31ed88.png)'
- en: Quantitative Metrics on the PROMISE 2012 Challenge dataset, from [VNet paper](https://arxiv.org/abs/1606.04797)
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: PROMISE 2012 挑战数据集上的定量指标，来自 [VNet 论文](https://arxiv.org/abs/1606.04797)
- en: As we can see, even with few labels, the V-Net is able to produce good qualitative
    segmentations and obtain a very good Dice Score.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所见，即使标签很少，V-Net 也能够生成良好的定性分割，并获得非常好的 Dice 分数。
- en: Main Limitations of the V-Net
  id: totrans-65
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: V-Net 的主要限制
- en: 'Indeed, V-Net has set a new benchmark in the realm of image segmentation, particularly
    in medical imaging. However, every innovation has room for growth. Here, we’ll
    discuss some of the prominent areas where V-Net could improve:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 确实，V-Net 在图像分割领域，特别是医学成像中设立了新的基准。然而，每项创新都有成长空间。在这里，我们将讨论 V-Net 可以改进的一些显著领域：
- en: 'Limitation 1: Size of the model'
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 局限性 1：模型大小
- en: 'Transitioning from 2D to 3D brings with it a significant increase in memory
    consumption. The ripple effects of this increase are multifold:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 从 2D 转到 3D 带来了显著的内存消耗增加。这种增加的连锁反应是多方面的：
- en: The model demands substantial memory space.
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 该模型需要大量的内存空间。
- en: It severely restricts the batch size (as loading multiple 3D tensors into GPU
    memory becomes challenging).
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它严重限制了批量大小（因为将多个 3D 张量加载到 GPU 内存中变得具有挑战性）。
- en: Medical imaging data is sparse and expensive to label, making it harder to fit
    a model with so many parameters.
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 医学成像数据稀缺且标注成本高，使得拥有如此多参数的模型更难以拟合。
- en: 'Limitation 2: Does not use unsupervised learning or Self supervised learning'
  id: totrans-72
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 局限性 2：不使用无监督学习或自监督学习
- en: V-Net operates purely in a supervised learning context, neglecting the potential
    of unsupervised learning. In a field where unlabelled scans significantly outnumber
    the annotated ones, incorporating unsupervised learning could be a game-changer.
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: V-Net 完全在监督学习的背景下运作，忽视了无监督学习的潜力。在未标注扫描大大超过标注扫描的领域中，融入无监督学习可能会带来突破性的改变。
- en: 'Limitation 3: No uncertainty estimation'
  id: totrans-74
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 局限性 3：没有不确定性估计
- en: V-Net doesn’t estimate uncertainties, meaning it cannot assess its own confidence
    in its predictions. This is an area where Bayesian Deep Learning shines. (Refer
    to this post for a [Gentle Introduction to Bayesian Deep Learning](https://medium.com/ai-mind-labs/a-gentle-introduction-to-bayesian-deep-learning-d298c7243fd6)).
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: V-Net 不估计不确定性，这意味着它无法评估自身预测的信心。这是贝叶斯深度学习闪耀的领域。（请参阅这篇[贝叶斯深度学习简明介绍](https://medium.com/ai-mind-labs/a-gentle-introduction-to-bayesian-deep-learning-d298c7243fd6)）。
- en: 'Limitation 4: Lack of Robustness'
  id: totrans-76
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 局限性 4：缺乏鲁棒性
- en: Convolutional Neural Networks (CNNs) traditionally struggle with generalization.
    They are not robust against variations like contrast change, multimodal distributions,
    or different resolutions. This is another area where V-Net could improve.
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 卷积神经网络（CNNs）传统上在泛化方面表现不佳。它们对于对比度变化、多模态分布或不同分辨率等变化不够鲁棒。这是 V-Net 还可以改进的另一个领域。
- en: Conclusion
  id: totrans-78
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: V-Net, the lesser-known yet powerful counterpart to U-Net, has revolutionized
    computer vision, especially image segmentation. Its transition from 2D to 3D images
    and the introduction of the Dice Coefficient, now a ubiquitous tool, set new standards
    in the field.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: V-Net，作为 U-Net 的较少知名但强大的对手，已经彻底改变了计算机视觉，尤其是图像分割。它从 2D 转向 3D 图像，并引入了现在广泛使用的 Dice
    系数，为该领域设立了新的标准。
- en: Despite its limitations, V-Net should be the go-to model for anyone embarking
    on a 3D image segmentation task. For further improvement, exploring unsupervised
    learning and integrating attention mechanisms seems like promising avenues.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管存在局限性，V-Net 应该是开始进行 3D 图像分割任务的首选模型。为了进一步改进，探索无监督学习和整合注意力机制似乎是有前景的方向。
- en: 'Thanks for reading! Before you go:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 感谢阅读！在你离开之前：
- en: Check my [compilation of AI tutorials](https://github.com/FrancoisPorcher/awesome-ai-tutorials)
    on Github
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 查看我在 Github 上的[AI 教程汇编](https://github.com/FrancoisPorcher/awesome-ai-tutorials)
- en: '[](https://github.com/FrancoisPorcher/awesome-ai-tutorials?source=post_page-----906e393968f7--------------------------------)
    [## GitHub - FrancoisPorcher/awesome-ai-tutorials: The best collection of AI tutorials
    to make you a…'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://github.com/FrancoisPorcher/awesome-ai-tutorials?source=post_page-----906e393968f7--------------------------------)
    [## GitHub - FrancoisPorcher/awesome-ai-tutorials: 最佳 AI 教程汇编，助你成为…'
- en: The best collection of AI tutorials to make you a boss of Data Science! - GitHub
    …
  id: totrans-84
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 最佳 AI 教程汇编，助你成为数据科学大师！- GitHub …
- en: github.com](https://github.com/FrancoisPorcher/awesome-ai-tutorials?source=post_page-----906e393968f7--------------------------------)
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: github.com](https://github.com/FrancoisPorcher/awesome-ai-tutorials?source=post_page-----906e393968f7--------------------------------)
- en: Y*ou should get my articles in your inbox.* [***Subscribe here.***](https://medium.com/@francoisporcher/subscribe)
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '*你应该在你的收件箱中获取我的文章。* [***点击此处订阅。***](https://medium.com/@francoisporcher/subscribe)'
- en: '*If you want to have access to premium articles on Medium, you only need a
    membership for $5 a month. If you sign up* [***with my link***](https://medium.com/@francoisporcher/membership)*,
    you support me with a part of your fee without additional costs.*'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '*如果你想访问Medium上的优质文章，只需每月$5的会员订阅。如果你通过* [***我的链接***](https://medium.com/@francoisporcher/membership)*进行注册，你可以在没有额外费用的情况下用你的一部分费用支持我。*'
- en: If you found this article insightful and beneficial, please consider following
    me and leaving a clap for more in-depth content! Your support helps me continue
    producing content that aids our collective understanding.
  id: totrans-88
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 如果你觉得这篇文章有见解且有帮助，请考虑关注我并点赞，以获取更多深入内容！你的支持帮助我继续制作有助于我们集体理解的内容。
- en: References
  id: totrans-89
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[Cooking your first U-Net for Image Segmentation](https://medium.com/@foporcher/cooking-your-first-u-net-for-image-segmentation-e812e37e9cd0)'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[为图像分割制作你的第一个U-Net](https://medium.com/@foporcher/cooking-your-first-u-net-for-image-segmentation-e812e37e9cd0)'
- en: '[A Gentle Introduction to Bayesian Deep Learning](https://medium.com/ai-mind-labs/a-gentle-introduction-to-bayesian-deep-learning-d298c7243fd6)'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[贝叶斯深度学习的温和介绍](https://medium.com/ai-mind-labs/a-gentle-introduction-to-bayesian-deep-learning-d298c7243fd6)'
- en: 'Milletari, F., Navab, N., & Ahmadi, S. A. (2016). [V-Net: Fully Convolutional
    Neural Networks for Volumetric Medical Image Segmentation](https://arxiv.org/abs/1606.04797).
    In 3D Vision (3DV), 2016 Fourth International Conference on (pp. 565–571). IEEE.'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Milletari, F., Navab, N., & Ahmadi, S. A. (2016). [V-Net：用于体积医学图像分割的全卷积神经网络](https://arxiv.org/abs/1606.04797)。在2016年第四届国际3D视觉会议（3DV）上（第565–571页）。IEEE。
- en: 'Ronneberger, O., Fischer, P., & Brox, T. (2015). [U-Net: Convolutional Networks
    for Biomedical Image Segmentation](https://arxiv.org/abs/1505.04597). In International
    Conference on Medical image computing and computer-assisted intervention (pp.
    234–241). Springer, Cham.'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ronneberger, O., Fischer, P., & Brox, T. (2015). [U-Net：用于生物医学图像分割的卷积网络](https://arxiv.org/abs/1505.04597)。在国际医学图像计算与计算机辅助干预会议（第234–241页）。Springer,
    Cham。
