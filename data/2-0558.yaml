- en: Controllable Medical Image Generation with ControlNets
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å¯æ§åŒ»å­¦å›¾åƒç”Ÿæˆä¸ControlNets
- en: åŸæ–‡ï¼š[https://towardsdatascience.com/controllable-medical-image-generation-with-controlnets-48ef33dde652](https://towardsdatascience.com/controllable-medical-image-generation-with-controlnets-48ef33dde652)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡ï¼š[https://towardsdatascience.com/controllable-medical-image-generation-with-controlnets-48ef33dde652](https://towardsdatascience.com/controllable-medical-image-generation-with-controlnets-48ef33dde652)
- en: Guide on using ControlNets to control the generation process of Latent Diffusion
    Models
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä½¿ç”¨ControlNetsæ§åˆ¶æ½œåœ¨æ‰©æ•£æ¨¡å‹ç”Ÿæˆè¿‡ç¨‹çš„æŒ‡å—
- en: '[](https://medium.com/@walhugolp?source=post_page-----48ef33dde652--------------------------------)[![Walter
    Hugo Lopez Pinaya](../Images/0c132d0d1321790b0cea880800d231e0.png)](https://medium.com/@walhugolp?source=post_page-----48ef33dde652--------------------------------)[](https://towardsdatascience.com/?source=post_page-----48ef33dde652--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----48ef33dde652--------------------------------)
    [Walter Hugo Lopez Pinaya](https://medium.com/@walhugolp?source=post_page-----48ef33dde652--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@walhugolp?source=post_page-----48ef33dde652--------------------------------)[![Walter
    Hugo Lopez Pinaya](../Images/0c132d0d1321790b0cea880800d231e0.png)](https://medium.com/@walhugolp?source=post_page-----48ef33dde652--------------------------------)[](https://towardsdatascience.com/?source=post_page-----48ef33dde652--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----48ef33dde652--------------------------------)
    [Walter Hugo Lopez Pinaya](https://medium.com/@walhugolp?source=post_page-----48ef33dde652--------------------------------)'
- en: Â·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----48ef33dde652--------------------------------)
    Â·9 min readÂ·Jun 13, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: Â·å‘è¡¨äº [Towards Data Science](https://towardsdatascience.com/?source=post_page-----48ef33dde652--------------------------------)
    Â·é˜…è¯»æ—¶é—´9åˆ†é’ŸÂ·2023å¹´6æœˆ13æ—¥
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: In this post, we will present a guide on training a **ControlNet** to empower
    users with precise control over the generation process of a **Latent Diffusion
    Model** **(like Stable Diffusion!)**. Our aim is to showcase the remarkable capabilities
    of these models in translating brain images across various contrasts. To achieve
    this, we will leverage the power of the recently introduced **open-source extension
    for MONAI,** [**MONAI Generative Models**](https://github.com/Project-MONAI/GenerativeModels)**!**
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘ä»¬å°†å±•ç¤ºå¦‚ä½•è®­ç»ƒä¸€ä¸ª**ControlNet**ï¼Œä»¥ä½¿ç”¨æˆ·èƒ½å¤Ÿç²¾å‡†æ§åˆ¶**æ½œåœ¨æ‰©æ•£æ¨¡å‹**çš„ç”Ÿæˆè¿‡ç¨‹ï¼ˆ**å¦‚ Stable Diffusion!**ï¼‰ã€‚æˆ‘ä»¬çš„ç›®æ ‡æ˜¯å±•ç¤ºè¿™äº›æ¨¡å‹åœ¨ä¸åŒå¯¹æ¯”åº¦ä¸‹è½¬æ¢è„‘éƒ¨å›¾åƒçš„å“è¶Šèƒ½åŠ›ã€‚ä¸ºå®ç°è¿™ä¸€ç›®æ ‡ï¼Œæˆ‘ä»¬å°†åˆ©ç”¨æœ€è¿‘æ¨å‡ºçš„**MONAIå¼€æºæ‰©å±•**ï¼Œ[**MONAI
    Generative Models**](https://github.com/Project-MONAI/GenerativeModels)**!**
- en: '![](../Images/802d4dc3c60ef6342ff0aa9478c0bad4.png)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/802d4dc3c60ef6342ff0aa9478c0bad4.png)'
- en: Generating T1-weighted brain images (right) from FLAIR images (left) using ControlNet
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨ControlNetä»FLAIRå›¾åƒï¼ˆå·¦ï¼‰ç”ŸæˆT1åŠ æƒè„‘éƒ¨å›¾åƒï¼ˆå³ï¼‰
- en: '***Our project code is available in this public repository*** [***https://github.com/Warvito/generative_brain_controlnet***](https://github.com/Warvito/generative_brain_controlnet)'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '***æˆ‘ä»¬çš„é¡¹ç›®ä»£ç å¯ä»¥åœ¨è¿™ä¸ªå…¬å¼€çš„ä»£ç åº“ä¸­æ‰¾åˆ°*** [***https://github.com/Warvito/generative_brain_controlnet***](https://github.com/Warvito/generative_brain_controlnet)'
- en: Introduction
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ä»‹ç»
- en: In recent years, text-to-image diffusion models have witnessed remarkable advancements,
    enabling the generation of highly realistic images based on open-domain text descriptions.
    These generated images have rich details, well-defined outlines, coherent structures,
    and meaningful contextual representation. However, despite the significant achievements
    of diffusion models, there remains a challenge in achieving precise control over
    the generative process. **Even with lengthy and intricate text descriptions, accurately
    capturing the userâ€™s desired ideas can be a struggle.**
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: è¿‘å¹´æ¥ï¼Œæ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½¿å¾—åŸºäºå¼€æ”¾é¢†åŸŸçš„æ–‡æœ¬æè¿°ç”Ÿæˆé«˜åº¦çœŸå®çš„å›¾åƒæˆä¸ºå¯èƒ½ã€‚è¿™äº›ç”Ÿæˆçš„å›¾åƒå…·æœ‰ä¸°å¯Œçš„ç»†èŠ‚ã€æ¸…æ™°çš„è½®å»“ã€ä¸€è‡´çš„ç»“æ„å’Œæœ‰æ„ä¹‰çš„ä¸Šä¸‹æ–‡è¡¨ç¤ºã€‚ç„¶è€Œï¼Œå°½ç®¡æ‰©æ•£æ¨¡å‹å–å¾—äº†é‡è¦æˆå°±ï¼Œä½†åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­å®ç°ç²¾ç¡®æ§åˆ¶ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚**å³ä½¿æœ‰å†—é•¿è€Œå¤æ‚çš„æ–‡æœ¬æè¿°ï¼Œå‡†ç¡®æ•æ‰ç”¨æˆ·æƒ³è¦çš„æƒ³æ³•ä»ç„¶å¯èƒ½æ˜¯ä¸€é¡¹å›°éš¾çš„ä»»åŠ¡ã€‚**
- en: The introduction of **ControlNets**, as proposed by Lvmin Zhang and Maneesh
    Agrawala in their groundbreaking paper â€œ[*Adding Conditional Control to Text-to-Image
    Diffusion Models*](https://arxiv.org/abs/2302.05543)â€ (2023), has significantly
    enhanced the controllability and customization of diffusion models. These neural
    networks act as lightweight adapters, enabling precise control and customization
    while preserving the original generation capability of diffusion models. By fine-tuning
    these adapters while keeping the original diffusion model frozen, text-to-image
    models can be efficiently augmented for a diverse range of image-to-image applications.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '**ControlNets**çš„å¼•å…¥ï¼Œå¦‚Lvmin Zhangå’ŒManeesh Agrawalaåœ¨å…¶å¼€åˆ›æ€§è®ºæ–‡â€œ[*å°†æ¡ä»¶æ§åˆ¶æ·»åŠ åˆ°æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹*](https://arxiv.org/abs/2302.05543)â€ï¼ˆ2023ï¼‰ä¸­æå‡ºçš„ï¼Œæ˜¾è‘—å¢å¼ºäº†æ‰©æ•£æ¨¡å‹çš„å¯æ§æ€§å’Œå®šåˆ¶æ€§ã€‚è¿™äº›ç¥ç»ç½‘ç»œä½œä¸ºè½»é‡çº§é€‚é…å™¨ï¼Œèƒ½å¤Ÿå®ç°ç²¾ç¡®çš„æ§åˆ¶å’Œå®šåˆ¶ï¼ŒåŒæ—¶ä¿ç•™æ‰©æ•£æ¨¡å‹çš„åŸå§‹ç”Ÿæˆèƒ½åŠ›ã€‚é€šè¿‡å¾®è°ƒè¿™äº›é€‚é…å™¨ï¼ŒåŒæ—¶ä¿æŒåŸå§‹æ‰©æ•£æ¨¡å‹çš„å†»ç»“ï¼Œæ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹å¯ä»¥é«˜æ•ˆåœ°æ‰©å±•åˆ°å„ç§å›¾åƒåˆ°å›¾åƒåº”ç”¨ä¸­ã€‚'
- en: What sets ControlNet apart is its solution to the challenge of spatial consistency.
    In contrast to previous methods, ControlNet allows for explicit control over spatial,
    structural, and geometric aspects of generated structures, while retaining semantic
    control derived from textual captions. The original study introduced various models
    that enable conditional generation based on edges, pose, semantic masks, and depth
    maps, paving the way for exciting advancements in the computer vision field.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: ControlNet çš„ç‹¬ç‰¹ä¹‹å¤„åœ¨äºå…¶è§£å†³äº†ç©ºé—´ä¸€è‡´æ€§çš„é—®é¢˜ã€‚ä¸ä¹‹å‰çš„æ–¹æ³•ä¸åŒï¼ŒControlNet å…è®¸å¯¹ç”Ÿæˆç»“æ„çš„ç©ºé—´ã€ç»“æ„å’Œå‡ ä½•æ–¹é¢è¿›è¡Œæ˜¾å¼æ§åˆ¶ï¼ŒåŒæ—¶ä¿ç•™äº†æ¥è‡ªæ–‡æœ¬è¯´æ˜çš„è¯­ä¹‰æ§åˆ¶ã€‚åŸå§‹ç ”ç©¶ä»‹ç»äº†å„ç§æ¨¡å‹ï¼Œè¿™äº›æ¨¡å‹åŸºäºè¾¹ç¼˜ã€å§¿åŠ¿ã€è¯­ä¹‰æ©ç å’Œæ·±åº¦å›¾å®ç°æ¡ä»¶ç”Ÿæˆï¼Œä¸ºè®¡ç®—æœºè§†è§‰é¢†åŸŸçš„æ¿€åŠ¨äººå¿ƒçš„è¿›å±•é“ºå¹³äº†é“è·¯ã€‚
- en: 'In the field of medical imaging, numerous image-to-image applications hold
    significant importance. Among these applications, one notable task involves translating
    images between different domains, such as converting computational tomography
    (CT) scans to magnetic resonance imaging (MRI) or transforming images between
    distinct contrasts, for instance, from T1-weighted to T2-weighted MRI images.
    In this post, we will focus on a specific case: using **2D slices of brain images
    obtained from a FLAIR image to generate the correspondent T1-weighted image**.
    Our objective is to demonstrate how our new extension for MONAI ([MONAI Generative
    Models](https://github.com/Project-MONAI/GenerativeModels)) and ControlNets can
    be effectively utilized to train and evaluate generative models on medical data.
    By delving into this example, we aim to provide insights into the practical application
    of these technologies in the medical imaging domain.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨åŒ»å­¦å½±åƒé¢†åŸŸï¼Œè®¸å¤šå›¾åƒåˆ°å›¾åƒåº”ç”¨å…·æœ‰é‡è¦æ„ä¹‰ã€‚åœ¨è¿™äº›åº”ç”¨ä¸­ï¼Œä¸€ä¸ªæ˜¾è‘—çš„ä»»åŠ¡æ˜¯è·¨é¢†åŸŸç¿»è¯‘å›¾åƒï¼Œä¾‹å¦‚å°†è®¡ç®—æœºæ–­å±‚æ‰«æï¼ˆCTï¼‰è½¬æ¢ä¸ºç£å…±æŒ¯æˆåƒï¼ˆMRIï¼‰ï¼Œæˆ–åœ¨ä¸åŒå¯¹æ¯”åº¦ä¹‹é—´è½¬æ¢å›¾åƒï¼Œä¾‹å¦‚ä»
    T1 åŠ æƒå›¾åƒåˆ° T2 åŠ æƒå›¾åƒã€‚åœ¨è¿™ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘ä»¬å°†é‡ç‚¹å…³æ³¨ä¸€ä¸ªç‰¹å®šæ¡ˆä¾‹ï¼šä½¿ç”¨ä» FLAIR å›¾åƒè·å¾—çš„**2D è„‘å›¾åƒåˆ‡ç‰‡ç”Ÿæˆå¯¹åº”çš„ T1 åŠ æƒå›¾åƒ**ã€‚æˆ‘ä»¬çš„ç›®æ ‡æ˜¯å±•ç¤ºå¦‚ä½•æœ‰æ•ˆåœ°åˆ©ç”¨æˆ‘ä»¬çš„
    MONAI æ‰©å±•ï¼ˆ[MONAI Generative Models](https://github.com/Project-MONAI/GenerativeModels)ï¼‰å’Œ
    ControlNets æ¥è®­ç»ƒå’Œè¯„ä¼°åŒ»å­¦æ•°æ®ä¸Šçš„ç”Ÿæˆæ¨¡å‹ã€‚é€šè¿‡æ·±å…¥æ¢è®¨è¿™ä¸ªä¾‹å­ï¼Œæˆ‘ä»¬æ—¨åœ¨æä¾›è¿™äº›æŠ€æœ¯åœ¨åŒ»å­¦å½±åƒé¢†åŸŸå®é™…åº”ç”¨çš„è§è§£ã€‚
- en: '![](../Images/c375ecdab449c61cfdde33f301c92d7f.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c375ecdab449c61cfdde33f301c92d7f.png)'
- en: FLAIR to T1w Translation
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: FLAIR åˆ° T1w ç¿»è¯‘
- en: Latent Diffusion Model Training
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ½œåœ¨æ‰©æ•£æ¨¡å‹è®­ç»ƒ
- en: '![](../Images/43544d640b73ae5bff77412a659a6e63.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/43544d640b73ae5bff77412a659a6e63.png)'
- en: Latent Diffusion Model Architecture
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: æ½œåœ¨æ‰©æ•£æ¨¡å‹æ¶æ„
- en: To generate T1-weighted (T1w) images from FLAIR images, **the initial step involves
    training a diffusion model capable of generating T1w images**. In our example,
    we utilize 2D slices extracted from brain MRI images sourced from the [UK Biobank
    dataset](https://www.ukbiobank.ac.uk/) (available under this [data agreement](https://www.ukbiobank.ac.uk/media/p3zffurf/biobank-mta.pdf)[).](https://www.ukbiobank.ac.uk/media/p3zffurf/biobank-mta.pdf).)
    After having the original 3D brains registered to an MNI space using your favourite
    method (for example, [ANTs](http://stnava.github.io/ANTs/) or [UniRes](https://github.com/brudfors/UniRes)),
    we extract five 2D slices from the central part of the brain. We chose this region
    since it presents various tissues, making it easier to evaluate the image translation
    we are performing. Using this [script](https://github.com/Warvito/generative_brain_controlnet/blob/main/src/python/preprocessing/create_png_dataset.py),
    we ended with about **190,000 slices** with a spatial dimension of **224 Ã— 160
    pixels**. Next, we divide our image into the train (~180,000 slices), validation
    (~5,000 slices), and test (~5,000 slices) sets using this script. With our dataset
    prepared, we can start to train our Latent Diffusion Model!
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: è¦ä»FLAIRå›¾åƒç”ŸæˆT1åŠ æƒï¼ˆT1wï¼‰å›¾åƒï¼Œ**ç¬¬ä¸€æ­¥æ¶‰åŠè®­ç»ƒä¸€ä¸ªèƒ½å¤Ÿç”ŸæˆT1wå›¾åƒçš„æ‰©æ•£æ¨¡å‹**ã€‚åœ¨æˆ‘ä»¬çš„ç¤ºä¾‹ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨ä»[UK Biobankæ•°æ®é›†](https://www.ukbiobank.ac.uk/)ï¼ˆæ ¹æ®æ­¤[æ•°æ®åè®®](https://www.ukbiobank.ac.uk/media/p3zffurf/biobank-mta.pdf)æä¾›ï¼‰æå–çš„è„‘éƒ¨MRIå›¾åƒçš„2Dåˆ‡ç‰‡ã€‚æˆ‘ä»¬å°†åŸå§‹çš„3Dè„‘éƒ¨å›¾åƒé€šè¿‡ä½ å–œæ¬¢çš„æ–¹æ³•ï¼ˆä¾‹å¦‚ï¼Œ[ANTs](http://stnava.github.io/ANTs/)æˆ–[UniRes](https://github.com/brudfors/UniRes)ï¼‰æ³¨å†Œåˆ°MNIç©ºé—´ã€‚ç„¶åï¼Œæˆ‘ä»¬ä»å¤§è„‘çš„ä¸­å¿ƒéƒ¨åˆ†æå–äº”ä¸ª2Dåˆ‡ç‰‡ã€‚æˆ‘ä»¬é€‰æ‹©è¿™ä¸ªåŒºåŸŸæ˜¯å› ä¸ºå®ƒå±•ç¤ºäº†å¤šç§ç»„ç»‡ï¼Œä½¿æˆ‘ä»¬æ›´å®¹æ˜“è¯„ä¼°æ‰€è¿›è¡Œçš„å›¾åƒè½¬æ¢ã€‚ä½¿ç”¨æ­¤[è„šæœ¬](https://github.com/Warvito/generative_brain_controlnet/blob/main/src/python/preprocessing/create_png_dataset.py)ï¼Œæˆ‘ä»¬æœ€ç»ˆè·å¾—äº†å¤§çº¦**190,000ä¸ªåˆ‡ç‰‡**ï¼Œç©ºé—´ç»´åº¦ä¸º**224
    Ã— 160åƒç´ **ã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬ä½¿ç”¨è¯¥è„šæœ¬å°†å›¾åƒåˆ’åˆ†ä¸ºè®­ç»ƒé›†ï¼ˆçº¦180,000ä¸ªåˆ‡ç‰‡ï¼‰ã€éªŒè¯é›†ï¼ˆçº¦5,000ä¸ªåˆ‡ç‰‡ï¼‰å’Œæµ‹è¯•é›†ï¼ˆçº¦5,000ä¸ªåˆ‡ç‰‡ï¼‰ã€‚æ•°æ®é›†å‡†å¤‡å¥½åï¼Œæˆ‘ä»¬å¯ä»¥å¼€å§‹è®­ç»ƒæˆ‘ä»¬çš„æ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼
- en: To optimize computational resources, the latent diffusion model employs an **encoder**
    to transform input image x into a lower-dimensional latent space z, which can
    then be reconstructed by a **decoder**. This approach enables training diffusion
    models even with limited computational capacity, while still preserving their
    original quality and flexibility. Similar to what we did in our previous post
    **(**[***Generating Medical Images with MONAI***](https://medium.com/towards-data-science/generating-medical-images-with-monai-e03310aa35e6)**)**,
    we use the [**Autoencoder with KL-regularization model**](https://github.com/Project-MONAI/GenerativeModels/blob/main/generative/networks/nets/autoencoderkl.py#L579)from
    MONAI Generative models to create our compression model. By using [this configuration](https://github.com/Warvito/generative_brain_controlnet/blob/main/configs/stage1/aekl_v0.yaml)
    and the L1 loss together with the KL-regularisation, [perceptual loss](https://github.com/Project-MONAI/GenerativeModels/blob/c1ec4ed4381de90ef18061c98624fa931c42e9b6/generative/losses/perceptual.py#L21)
    and [adversarial loss](https://github.com/Project-MONAI/GenerativeModels/blob/c1ec4ed4381de90ef18061c98624fa931c42e9b6/generative/losses/adversarial_loss.py#L29),
    **we created an autoencoder capable of encoding and decoding brain images with
    high fidelity** [(with this script)](https://github.com/Warvito/generative_brain_controlnet/blob/main/src/python/training/train_aekl.py).
    The quality of the autoencoderâ€™s reconstruction is crucial for the performance
    of the Latent Diffusion Model since it defines the ceiling of the quality of our
    generated images. If the autoencoderâ€™s decoder produces blurry or low-quality
    images, our generative model will not be able to generate higher-quality ones.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†ä¼˜åŒ–è®¡ç®—èµ„æºï¼Œæ½œåœ¨æ‰©æ•£æ¨¡å‹ä½¿ç”¨**ç¼–ç å™¨**å°†è¾“å…¥å›¾åƒxè½¬æ¢ä¸ºä½ç»´æ½œåœ¨ç©ºé—´zï¼Œç„¶åé€šè¿‡**è§£ç å™¨**é‡å»ºã€‚è¿™ç§æ–¹æ³•ä½¿å¾—å³ä½¿åœ¨è®¡ç®—èƒ½åŠ›æœ‰é™çš„æƒ…å†µä¸‹ä¹Ÿèƒ½è®­ç»ƒæ‰©æ•£æ¨¡å‹ï¼ŒåŒæ—¶ä¿ç•™å…¶åŸå§‹è´¨é‡å’Œçµæ´»æ€§ã€‚ç±»ä¼¼äºæˆ‘ä»¬åœ¨ä¸Šä¸€ç¯‡æ–‡ç« **ï¼ˆ[***ä½¿ç”¨MONAIç”ŸæˆåŒ»å­¦å›¾åƒ***](https://medium.com/towards-data-science/generating-medical-images-with-monai-e03310aa35e6)ï¼‰**ä¸­åšçš„ï¼Œæˆ‘ä»¬ä½¿ç”¨æ¥è‡ªMONAI
    Generativeæ¨¡å‹çš„[**å¸¦æœ‰KLæ­£åˆ™åŒ–çš„è‡ªç¼–ç å™¨æ¨¡å‹**](https://github.com/Project-MONAI/GenerativeModels/blob/main/generative/networks/nets/autoencoderkl.py#L579)æ¥åˆ›å»ºæˆ‘ä»¬çš„å‹ç¼©æ¨¡å‹ã€‚é€šè¿‡ä½¿ç”¨[è¿™ä¸ªé…ç½®](https://github.com/Warvito/generative_brain_controlnet/blob/main/configs/stage1/aekl_v0.yaml)å’ŒL1æŸå¤±ä»¥åŠKLæ­£åˆ™åŒ–ï¼Œ[æ„ŸçŸ¥æŸå¤±](https://github.com/Project-MONAI/GenerativeModels/blob/c1ec4ed4381de90ef18061c98624fa931c42e9b6/generative/losses/perceptual.py#L21)å’Œ[å¯¹æŠ—æ€§æŸå¤±](https://github.com/Project-MONAI/GenerativeModels/blob/c1ec4ed4381de90ef18061c98624fa931c42e9b6/generative/losses/adversarial_loss.py#L29)ï¼Œ**æˆ‘ä»¬åˆ›å»ºäº†ä¸€ä¸ªèƒ½å¤Ÿé«˜ä¿çœŸç¼–ç å’Œè§£ç è„‘éƒ¨å›¾åƒçš„è‡ªç¼–ç å™¨**
    [ï¼ˆä½¿ç”¨è¿™ä¸ªè„šæœ¬ï¼‰](https://github.com/Warvito/generative_brain_controlnet/blob/main/src/python/training/train_aekl.py)ã€‚è‡ªç¼–ç å™¨çš„é‡å»ºè´¨é‡å¯¹äºæ½œåœ¨æ‰©æ•£æ¨¡å‹çš„æ€§èƒ½è‡³å…³é‡è¦ï¼Œå› ä¸ºå®ƒå®šä¹‰äº†æˆ‘ä»¬ç”Ÿæˆå›¾åƒçš„è´¨é‡ä¸Šé™ã€‚å¦‚æœè‡ªç¼–ç å™¨çš„è§£ç å™¨ç”Ÿæˆæ¨¡ç³Šæˆ–ä½è´¨é‡çš„å›¾åƒï¼Œæˆ‘ä»¬çš„ç”Ÿæˆæ¨¡å‹å°†æ— æ³•ç”Ÿæˆæ›´é«˜è´¨é‡çš„å›¾åƒã€‚
- en: '![](../Images/ed59d3f19e84b64518ba86a2b5b4d483.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ed59d3f19e84b64518ba86a2b5b4d483.png)'
- en: Using this [script](https://github.com/Warvito/generative_brain_controlnet/blob/main/src/python/testing/compute_msssim_reconstruction.py),
    we can quantify the fidelity of the autoencoder by using the **Multi-scale Structural
    Similarity Index Measurement (MS-SSIM)** between the original images and their
    reconstructions. In this example, we obtain a high performance with an MS-SSIM
    metric equal to 0.9876.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨è¿™ä¸ª[è„šæœ¬](https://github.com/Warvito/generative_brain_controlnet/blob/main/src/python/testing/compute_msssim_reconstruction.py)ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡ä½¿ç”¨åŸå§‹å›¾åƒä¸å…¶é‡å»ºä¹‹é—´çš„**å¤šå°ºåº¦ç»“æ„ç›¸ä¼¼æ€§æŒ‡æ•°æµ‹é‡ï¼ˆMS-SSIMï¼‰**æ¥é‡åŒ–è‡ªç¼–ç å™¨çš„ä¿çœŸåº¦ã€‚åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œæˆ‘ä»¬è·å¾—äº†ä¸€ä¸ªMS-SSIMæŒ‡æ ‡ä¸º0.9876çš„é«˜æ€§èƒ½ã€‚
- en: After we train the autoencoder, we will train the [**diffusion model**](https://github.com/Project-MONAI/GenerativeModels/blob/c1ec4ed4381de90ef18061c98624fa931c42e9b6/generative/networks/nets/diffusion_model_unet.py#L1632)
    **on the latent space z**. The diffusion model it is a model that is able to generate
    images from a pure noise image by iteratively denoising it over a series of timesteps.
    It usually uses an **U-Net architecture** (that has an encoder-decoder format),
    where we have layers of the encoder skip connected with layers in the decoder
    part (via long **skip connections**), enabling feature reusability and stabilize
    training and convergence.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: è®­ç»ƒå®Œè‡ªç¼–ç å™¨åï¼Œæˆ‘ä»¬å°†è®­ç»ƒ[**æ‰©æ•£æ¨¡å‹**](https://github.com/Project-MONAI/GenerativeModels/blob/c1ec4ed4381de90ef18061c98624fa931c42e9b6/generative/networks/nets/diffusion_model_unet.py#L1632)
    **åœ¨æ½œåœ¨ç©ºé—´zä¸Š**ã€‚æ‰©æ•£æ¨¡å‹æ˜¯ä¸€ç§èƒ½å¤Ÿé€šè¿‡åœ¨ä¸€ç³»åˆ—æ—¶é—´æ­¥ä¸Šé€æ­¥å»å™ªï¼Œä»çº¯å™ªå£°å›¾åƒä¸­ç”Ÿæˆå›¾åƒçš„æ¨¡å‹ã€‚å®ƒé€šå¸¸ä½¿ç”¨**U-Netæ¶æ„**ï¼ˆå…·æœ‰ç¼–ç å™¨-è§£ç å™¨æ ¼å¼ï¼‰ï¼Œå…¶ä¸­ç¼–ç å™¨çš„å±‚é€šè¿‡é•¿**è·³è·ƒè¿æ¥**ä¸è§£ç å™¨éƒ¨åˆ†çš„å±‚ç›¸è¿ï¼Œä»è€Œå®ç°ç‰¹å¾é‡ç”¨ï¼Œç¨³å®šè®­ç»ƒå’Œæ”¶æ•›ã€‚
- en: '![](../Images/1a9a40bef9b19b542c683181e50c34bb.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1a9a40bef9b19b542c683181e50c34bb.png)'
- en: Diffusion Modelâ€™s U-Net architecture with skip connections between the encoder
    and decoder.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰©æ•£æ¨¡å‹çš„U-Netæ¶æ„ï¼Œå…·æœ‰ç¼–ç å™¨å’Œè§£ç å™¨ä¹‹é—´çš„è·³è·ƒè¿æ¥ã€‚
- en: During the training, the Latent Diffusion Model learns a conditional noise prediction
    given these prompts. Again, we are using MONAI to create and train this network.
    In this [script](https://github.com/Warvito/generative_brain_controlnet/blob/main/src/python/training/train_ldm.py),
    we are instantiating the model with this [configuration](https://github.com/Warvito/generative_brain_controlnet/blob/main/configs/ldm/ldm_v0.yaml),
    where the training and evaluation are performed in this [part of the code](https://github.com/Warvito/generative_brain_controlnet/blob/bb47f9c359e2d280b23bda84b5c14b65dd5b7af3/src/python/training/training_functions.py#L408).
    Since we are not too interested in the textual prompts in this tutorial, we are
    using the same one for all the image (a sentence saying [â€œ*T1-weighted image of
    a brain.*â€](https://github.com/Warvito/generative_brain_controlnet/blob/bb47f9c359e2d280b23bda84b5c14b65dd5b7af3/src/python/training/util.py#L38)).
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæ½œåœ¨æ‰©æ•£æ¨¡å‹æ ¹æ®è¿™äº›æç¤ºå­¦ä¹ æ¡ä»¶å™ªå£°é¢„æµ‹ã€‚æˆ‘ä»¬å†æ¬¡ä½¿ç”¨MONAIæ¥åˆ›å»ºå’Œè®­ç»ƒè¿™ä¸ªç½‘ç»œã€‚åœ¨è¿™ä¸ª[è„šæœ¬](https://github.com/Warvito/generative_brain_controlnet/blob/main/src/python/training/train_ldm.py)ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨è¿™ä¸ª[é…ç½®](https://github.com/Warvito/generative_brain_controlnet/blob/main/configs/ldm/ldm_v0.yaml)æ¥å®ä¾‹åŒ–æ¨¡å‹ï¼Œå…¶ä¸­è®­ç»ƒå’Œè¯„ä¼°åœ¨[ä»£ç çš„è¿™ä¸€éƒ¨åˆ†](https://github.com/Warvito/generative_brain_controlnet/blob/bb47f9c359e2d280b23bda84b5c14b65dd5b7af3/src/python/training/training_functions.py#L408)è¿›è¡Œã€‚ç”±äºåœ¨æœ¬æ•™ç¨‹ä¸­æˆ‘ä»¬å¯¹æ–‡æœ¬æç¤ºä¸å¤ªæ„Ÿå…´è¶£ï¼Œæˆ‘ä»¬å¯¹æ‰€æœ‰å›¾åƒä½¿ç”¨ç›¸åŒçš„æç¤ºï¼ˆå¥å­ä¸º[â€œ*è„‘éƒ¨T1åŠ æƒå›¾åƒ*â€](https://github.com/Warvito/generative_brain_controlnet/blob/bb47f9c359e2d280b23bda84b5c14b65dd5b7af3/src/python/training/util.py#L38)ï¼‰ã€‚
- en: '![](../Images/7f66db9ca3d59810a16ef620b751e791.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7f66db9ca3d59810a16ef620b751e791.png)'
- en: Synthetic brain images generated with our Latent Diffusion Model
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨æˆ‘ä»¬çš„æ½œåœ¨æ‰©æ•£æ¨¡å‹ç”Ÿæˆçš„åˆæˆè„‘éƒ¨å›¾åƒ
- en: Again, we can quantify the performance of our trained generative model, this
    time we evaluate the quality of the samples (using the **FrÃ©chet inception distance
    (FID)**) and the diversity of the model (computing the MS-SSIM between all pair
    of samples of a group of 1,000 samples). Using these couple of scripts ([1](https://github.com/Warvito/generative_brain_controlnet/blob/main/src/python/testing/compute_fid.py)
    and [2](https://github.com/Warvito/generative_brain_controlnet/blob/main/src/python/testing/compute_msssim_sample.py)),
    we obtained an FID = 2.1986 and an MS-SSIM Diversity = 0.5368.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥é‡åŒ–æˆ‘ä»¬è®­ç»ƒè¿‡çš„ç”Ÿæˆæ¨¡å‹çš„æ€§èƒ½ï¼Œè¿™æ¬¡æˆ‘ä»¬è¯„ä¼°äº†æ ·æœ¬çš„è´¨é‡ï¼ˆä½¿ç”¨**FrÃ©chet inception distance (FID)**ï¼‰å’Œæ¨¡å‹çš„å¤šæ ·æ€§ï¼ˆè®¡ç®—1000ä¸ªæ ·æœ¬ç»„ä¸­æ‰€æœ‰æ ·æœ¬å¯¹çš„MS-SSIMï¼‰ã€‚ä½¿ç”¨è¿™ä¸¤ä¸ªè„šæœ¬ï¼ˆ[1](https://github.com/Warvito/generative_brain_controlnet/blob/main/src/python/testing/compute_fid.py)
    å’Œ [2](https://github.com/Warvito/generative_brain_controlnet/blob/main/src/python/testing/compute_msssim_sample.py)ï¼‰ï¼Œæˆ‘ä»¬å¾—åˆ°äº†FID
    = 2.1986å’ŒMS-SSIMå¤šæ ·æ€§ = 0.5368ã€‚
- en: As you can see in the previous images and results, we now have a model that
    can generate high-resolution images with great quality. However, we do not have
    any spatial control over how the images look like. For this, we will use a ControlNet
    to guide the generation of our Latent Diffusion Model.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: æ­£å¦‚ä½ åœ¨ä¹‹å‰çš„å›¾åƒå’Œç»“æœä¸­çœ‹åˆ°çš„ï¼Œæˆ‘ä»¬ç°åœ¨æ‹¥æœ‰ä¸€ä¸ªå¯ä»¥ç”Ÿæˆé«˜åˆ†è¾¨ç‡ã€é«˜è´¨é‡å›¾åƒçš„æ¨¡å‹ã€‚ç„¶è€Œï¼Œæˆ‘ä»¬å¯¹å›¾åƒçš„å¤–è§‚æ²¡æœ‰ä»»ä½•ç©ºé—´æ§åˆ¶ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ControlNetæ¥æŒ‡å¯¼æˆ‘ä»¬æ½œåœ¨æ‰©æ•£æ¨¡å‹çš„ç”Ÿæˆã€‚
- en: ControlNet Training
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ControlNet è®­ç»ƒ
- en: '![](../Images/e66262faa4ed439dc9139cb1bbe85de4.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e66262faa4ed439dc9139cb1bbe85de4.png)'
- en: ControlNet Architecture
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: ControlNet æ¶æ„
- en: 'The [ControlNet architecture](https://github.com/Project-MONAI/GenerativeModels/blob/c1ec4ed4381de90ef18061c98624fa931c42e9b6/generative/networks/nets/controlnet.py#L125)
    comprises two main components: a **trainable version** of the encoder from the
    U-Net model, including the middle blocks, and a **pre-trained â€œlockedâ€ version**
    of the diffusion model. Here, the locked copy preserves the generative capability,
    while the trainable copy is trained on specific image-to-image datasets to learn
    conditional control. These two components are interconnected using a **â€œzero convolutionâ€
    layer** â€” a 1Ã—1 convolution layer with initialized weights and biases set to zeros.
    The convolution weights gradually transition from zeros to optimized parameters,
    ensuring that during the initial training steps, the outputs of both the trainable
    and locked copies remain consistent with what they would be if the ControlNet
    were absent. In other words, when a ControlNet is applied to certain neural network
    blocks prior to any optimization, it does not introduce any additional influence
    or noise to the deep neural features.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '[ControlNetæ¶æ„](https://github.com/Project-MONAI/GenerativeModels/blob/c1ec4ed4381de90ef18061c98624fa931c42e9b6/generative/networks/nets/controlnet.py#L125)åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦ç»„ä»¶ï¼šä¸€ä¸ª**å¯è®­ç»ƒçš„**æ¥è‡ªU-Netæ¨¡å‹çš„ç¼–ç å™¨ç‰ˆæœ¬ï¼ŒåŒ…æ‹¬ä¸­é—´å—ï¼Œä»¥åŠä¸€ä¸ª**é¢„è®­ç»ƒçš„â€œé”å®šâ€ç‰ˆæœ¬**çš„æ‰©æ•£æ¨¡å‹ã€‚è¿™é‡Œï¼Œé”å®šå‰¯æœ¬ä¿ç•™äº†ç”Ÿæˆèƒ½åŠ›ï¼Œè€Œå¯è®­ç»ƒå‰¯æœ¬åˆ™åœ¨ç‰¹å®šçš„å›¾åƒå¯¹å›¾åƒæ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒï¼Œä»¥å­¦ä¹ æ¡ä»¶æ§åˆ¶ã€‚è¿™ä¸¤ä¸ªç»„ä»¶é€šè¿‡**â€œé›¶å·ç§¯â€å±‚**äº’è”â€”â€”ä¸€ä¸ª1Ã—1çš„å·ç§¯å±‚ï¼Œå…¶åˆå§‹åŒ–æƒé‡å’Œåç½®è¢«è®¾ç½®ä¸ºé›¶ã€‚å·ç§¯æƒé‡é€æ¸ä»é›¶è¿‡æ¸¡åˆ°ä¼˜åŒ–å‚æ•°ï¼Œç¡®ä¿åœ¨åˆå§‹è®­ç»ƒæ­¥éª¤ä¸­ï¼Œå¯è®­ç»ƒå’Œé”å®šå‰¯æœ¬çš„è¾“å‡ºä¸ControlNetä¸å­˜åœ¨æ—¶çš„è¾“å‡ºä¿æŒä¸€è‡´ã€‚æ¢å¥è¯è¯´ï¼Œå½“ControlNetåº”ç”¨äºæŸäº›ç¥ç»ç½‘ç»œå—ä¹‹å‰ï¼Œæ²¡æœ‰å¼•å…¥ä»»ä½•é¢å¤–çš„å½±å“æˆ–å™ªå£°åˆ°æ·±å±‚ç¥ç»ç‰¹å¾ä¸­ã€‚'
- en: By integrating these two components, the ControlNet enables us to govern the
    behaviour of each level in the Diffusion Modelâ€™s U-Net.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: é€šè¿‡æ•´åˆè¿™ä¸¤ä¸ªç»„ä»¶ï¼ŒControlNetä½¿æˆ‘ä»¬èƒ½å¤Ÿæ§åˆ¶Diffusion Modelçš„U-Netä¸­æ¯ä¸ªçº§åˆ«çš„è¡Œä¸ºã€‚
- en: In our example, we instantiate the ControlNet in [this script](https://github.com/Warvito/generative_brain_controlnet/blob/main/src/python/training/train_controlnet.py),
    using the following equivalent snippet.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æˆ‘ä»¬çš„ç¤ºä¾‹ä¸­ï¼Œæˆ‘ä»¬åœ¨[è¿™ä¸ªè„šæœ¬](https://github.com/Warvito/generative_brain_controlnet/blob/main/src/python/training/train_controlnet.py)ä¸­å®ä¾‹åŒ–ControlNetï¼Œä½¿ç”¨ä»¥ä¸‹ç­‰æ•ˆä»£ç ç‰‡æ®µã€‚
- en: '[PRE0]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Since we are using a Latent Diffusion Model, this requires ControlNets to convert
    image-based conditions to the same latent space to match the convolution size.
    For that, we use a [convolutional network](https://github.com/Project-MONAI/GenerativeModels/blob/c1ec4ed4381de90ef18061c98624fa931c42e9b6/generative/networks/nets/controlnet.py#L45)
    trained jointly with the full model. In our case, we have three downsampling levels
    (similar to the autoencoder KL) defined in *â€œconditioning_embedding_num_channels=[64,
    128, 128, 256]â€*. Since our conditional image is a FLAIR image with one channel,
    we also need to specify its input number of channels in *â€œconditioning_embedding_in_channels=1â€*.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: ç”±äºæˆ‘ä»¬ä½¿ç”¨çš„æ˜¯æ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼Œè¿™è¦æ±‚ControlNetså°†åŸºäºå›¾åƒçš„æ¡ä»¶è½¬æ¢ä¸ºç›¸åŒçš„æ½œåœ¨ç©ºé—´ä»¥åŒ¹é…å·ç§¯å¤§å°ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬ä½¿ç”¨ä¸€ä¸ªä¸å®Œæ•´æ¨¡å‹å…±åŒè®­ç»ƒçš„[å·ç§¯ç½‘ç»œ](https://github.com/Project-MONAI/GenerativeModels/blob/c1ec4ed4381de90ef18061c98624fa931c42e9b6/generative/networks/nets/controlnet.py#L45)ã€‚åœ¨æˆ‘ä»¬çš„æ¡ˆä¾‹ä¸­ï¼Œæˆ‘ä»¬æœ‰ä¸‰ä¸ªä¸‹é‡‡æ ·çº§åˆ«ï¼ˆç±»ä¼¼äºè‡ªåŠ¨ç¼–ç å™¨KLï¼‰ï¼Œåœ¨*â€œconditioning_embedding_num_channels=[64,
    128, 128, 256]â€*ä¸­å®šä¹‰ã€‚ç”±äºæˆ‘ä»¬çš„æ¡ä»¶å›¾åƒæ˜¯ä¸€ä¸ªå…·æœ‰å•é€šé“çš„FLAIRå›¾åƒï¼Œæˆ‘ä»¬è¿˜éœ€è¦åœ¨*â€œconditioning_embedding_in_channels=1â€*ä¸­æŒ‡å®šå…¶è¾“å…¥é€šé“æ•°ã€‚
- en: After initialising our network, we train it similarly to a diffusion model.
    In the following snippet ([and in this part of the code](https://github.com/Warvito/generative_brain_controlnet/blob/bb47f9c359e2d280b23bda84b5c14b65dd5b7af3/src/python/training/training_functions.py#L623)),
    we can see that first we pass our conditional FLAIR image to the trainable network
    and obtain the outputs from its skip connections. Then, these values are inputted
    into the diffusion model when computing the predicted noise. Internally, the diffusion
    model sums the skip connection from the ControlNets with its own ones before feeding
    the decoder part ([code](https://github.com/Project-MONAI/GenerativeModels/blob/c1ec4ed4381de90ef18061c98624fa931c42e9b6/generative/networks/nets/diffusion_model_unet.py#L1901)).
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨åˆå§‹åŒ–æˆ‘ä»¬çš„ç½‘ç»œåï¼Œæˆ‘ä»¬åƒè®­ç»ƒæ‰©æ•£æ¨¡å‹ä¸€æ ·è®­ç»ƒå®ƒã€‚åœ¨ä»¥ä¸‹ä»£ç ç‰‡æ®µä¸­ï¼ˆ[ä»¥åŠä»£ç çš„è¿™ä¸€éƒ¨åˆ†](https://github.com/Warvito/generative_brain_controlnet/blob/bb47f9c359e2d280b23bda84b5c14b65dd5b7af3/src/python/training/training_functions.py#L623)ï¼‰ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œé¦–å…ˆæˆ‘ä»¬å°†æ¡ä»¶FLAIRå›¾åƒä¼ é€’åˆ°å¯è®­ç»ƒçš„ç½‘ç»œä¸­ï¼Œå¹¶ä»å…¶è·³è¿‡è¿æ¥ä¸­è·å¾—è¾“å‡ºã€‚ç„¶åï¼Œè¿™äº›å€¼åœ¨è®¡ç®—é¢„æµ‹å™ªå£°æ—¶è¾“å…¥åˆ°æ‰©æ•£æ¨¡å‹ä¸­ã€‚åœ¨å†…éƒ¨ï¼Œæ‰©æ•£æ¨¡å‹å°†ControlNetsçš„è·³è¿‡è¿æ¥ä¸è‡ªèº«çš„è·³è¿‡è¿æ¥ç›¸åŠ ï¼Œç„¶åå°†å…¶è¾“å…¥åˆ°è§£ç å™¨éƒ¨åˆ†ä¸­ï¼ˆ[ä»£ç ](https://github.com/Project-MONAI/GenerativeModels/blob/c1ec4ed4381de90ef18061c98624fa931c42e9b6/generative/networks/nets/diffusion_model_unet.py#L1901)ï¼‰ã€‚
- en: '[PRE1]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: ControlNet Sampling and Evaluation
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ControlNeté‡‡æ ·ä¸è¯„ä¼°
- en: After training our models, we can sample and evaluate them. [Here](https://github.com/Warvito/generative_brain_controlnet/blob/main/src/python/testing/sample_flair_to_t1w.py),
    we are using the FLAIR images from the test set to generate conditioned T1w images.
    Similar to our training, the the sampling process is very close to the one used
    with the diffusion model, the only difference is that we pass the condition image
    to the trained ControlNet, and we use its output to feed the diffusion model in
    each sampling timesteps. As we can observe from the figure below, **our generated
    images follow with high spatial fidelity the original conditioning**, with the
    cortex gyri following similar shapes and the images preserving the boundary between
    different tissues.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è®­ç»ƒæ¨¡å‹åï¼Œæˆ‘ä»¬å¯ä»¥å¯¹å…¶è¿›è¡Œé‡‡æ ·å’Œè¯„ä¼°ã€‚[è¿™é‡Œ](https://github.com/Warvito/generative_brain_controlnet/blob/main/src/python/testing/sample_flair_to_t1w.py)ï¼Œæˆ‘ä»¬ä½¿ç”¨æµ‹è¯•é›†ä¸­çš„
    FLAIR å›¾åƒç”Ÿæˆæ¡ä»¶ T1w å›¾åƒã€‚ç±»ä¼¼äºæˆ‘ä»¬çš„è®­ç»ƒï¼Œé‡‡æ ·è¿‡ç¨‹ä¸æ‰©æ•£æ¨¡å‹ä½¿ç”¨çš„éå¸¸æ¥è¿‘ï¼Œå”¯ä¸€çš„åŒºåˆ«æ˜¯æˆ‘ä»¬å°†æ¡ä»¶å›¾åƒä¼ é€’ç»™è®­ç»ƒå¥½çš„ ControlNetï¼Œå¹¶ä½¿ç”¨å…¶è¾“å‡ºåœ¨æ¯ä¸ªé‡‡æ ·æ—¶é—´æ­¥ä¸­é¦ˆé€ç»™æ‰©æ•£æ¨¡å‹ã€‚æ­£å¦‚æˆ‘ä»¬ä»ä¸‹å›¾ä¸­è§‚å¯Ÿåˆ°çš„é‚£æ ·ï¼Œ**æˆ‘ä»¬ç”Ÿæˆçš„å›¾åƒåœ¨ç©ºé—´ä¸Šé«˜åº¦å¿ å®äºåŸå§‹æ¡ä»¶**ï¼Œå¤§è„‘çš®å±‚å›æ—‹éµå¾ªç±»ä¼¼çš„å½¢çŠ¶ï¼Œå¹¶ä¸”å›¾åƒä¿æŒäº†ä¸åŒç»„ç»‡ä¹‹é—´çš„è¾¹ç•Œã€‚
- en: '![](../Images/b2ce5162a27dae654d7bd4739ffca9f7.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b2ce5162a27dae654d7bd4739ffca9f7.png)'
- en: Examples from the test set of the original FLAIR image used as input to the
    ControlNet (left), the generated T1-weighted image (middle), and the original
    T1-weighted image, a.k.a. the expected output (right)
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: æµ‹è¯•é›†ä¸­ç”¨äºè¾“å…¥åˆ° ControlNet çš„åŸå§‹ FLAIR å›¾åƒï¼ˆå·¦ï¼‰ã€ç”Ÿæˆçš„ T1 åŠ æƒå›¾åƒï¼ˆä¸­ï¼‰å’ŒåŸå§‹ T1 åŠ æƒå›¾åƒï¼Œå³æœŸæœ›è¾“å‡ºï¼ˆå³ï¼‰çš„ç¤ºä¾‹ã€‚
- en: After we sample the images of our models, we can quantify the performance of
    our ControlNet when translating the images between different contrasts. Since
    we have the expected T1w images from the test set, we can also check their differences
    and compute the distance between the real and synthetic images using the **mean
    absolute error (MAE)**, **peak** **signal-to-noise ratio (PSNR)**, and **MS-SSIM**.
    In our test set, we got a PSNR= 26.2458+-1.0092, MAE=0.02632+-0.0036 and MSSIM=0.9526+-0.0111
    when executing this [script](https://github.com/Warvito/generative_brain_controlnet/blob/main/src/python/testing/compute_controlnet_performance.py).
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æˆ‘ä»¬å¯¹æ¨¡å‹çš„å›¾åƒè¿›è¡Œé‡‡æ ·åï¼Œæˆ‘ä»¬å¯ä»¥é‡åŒ–æˆ‘ä»¬ ControlNet åœ¨ä¸åŒå¯¹æ¯”åº¦ä¹‹é—´ç¿»è¯‘å›¾åƒçš„æ€§èƒ½ã€‚ç”±äºæˆ‘ä»¬æ‹¥æœ‰æµ‹è¯•é›†ä¸­çš„æœŸæœ› T1w å›¾åƒï¼Œæˆ‘ä»¬è¿˜å¯ä»¥æ£€æŸ¥å®ƒä»¬ä¹‹é—´çš„å·®å¼‚ï¼Œå¹¶ä½¿ç”¨**å‡æ–¹ç»å¯¹è¯¯å·®
    (MAE)**ã€**å³°å€¼** **ä¿¡å™ªæ¯” (PSNR)** å’Œ**MS-SSIM**è®¡ç®—çœŸå®å›¾åƒä¸åˆæˆå›¾åƒä¹‹é—´çš„è·ç¦»ã€‚åœ¨æˆ‘ä»¬çš„æµ‹è¯•é›†ä¸­ï¼Œå½“æ‰§è¡Œè¿™ä¸ª [è„šæœ¬](https://github.com/Warvito/generative_brain_controlnet/blob/main/src/python/testing/compute_controlnet_performance.py)
    æ—¶ï¼Œæˆ‘ä»¬å¾—åˆ°äº† PSNR=26.2458+-1.0092ï¼ŒMAE=0.02632+-0.0036 å’Œ MSSIM=0.9526+-0.0111ã€‚
- en: And that is it! ControlNet offers incredible control over our diffusion models
    and recent approaches have extended its method to combine different trained ControlNets
    ([Multi-ControlNet](https://github.com/Mikubill/sd-webui-controlnet#multi-controlnet)),
    work with different types of conditioning in the same model ([T2I adapters](https://arxiv.org/abs/2302.08453)),
    and even condition the model on styles (using methods like ControlNet 1.1 â€” [reference
    only](https://github.com/Mikubill/sd-webui-controlnet/discussions/1236)). If these
    methods sound interesting, do not forget to follow me for more guides like this
    one! ğŸ˜
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: å°±è¿™äº›ï¼ControlNet æä¾›äº†å¯¹æˆ‘ä»¬æ‰©æ•£æ¨¡å‹çš„ä¸å¯æ€è®®çš„æ§åˆ¶ï¼Œè¿‘æœŸçš„æ–¹æ³•å·²ç»æ‰©å±•äº†å…¶æ–¹æ³•ï¼Œä»¥ç»“åˆä¸åŒè®­ç»ƒçš„ ControlNets ([Multi-ControlNet](https://github.com/Mikubill/sd-webui-controlnet#multi-controlnet))ï¼Œåœ¨åŒä¸€æ¨¡å‹ä¸­å¤„ç†ä¸åŒç±»å‹çš„æ¡ä»¶
    ([T2I adapters](https://arxiv.org/abs/2302.08453))ï¼Œç”šè‡³åŸºäºæ ·å¼è°ƒæ•´æ¨¡å‹ï¼ˆä½¿ç”¨åƒ ControlNet 1.1
    è¿™æ ·çš„æŠ€æœ¯ â€” [ä»…ä¾›å‚è€ƒ](https://github.com/Mikubill/sd-webui-controlnet/discussions/1236)ï¼‰ã€‚å¦‚æœè¿™äº›æ–¹æ³•å¬èµ·æ¥å¾ˆæœ‰è¶£ï¼Œä¸è¦å¿˜è®°å…³æ³¨æˆ‘ï¼Œä»¥è·å–æ›´å¤šç±»ä¼¼çš„æŒ‡å—ï¼ğŸ˜
- en: For more MONAI Generative Modelâ€™s tutorials and to learn more about our features,
    check out our [Tutorial page](https://github.com/Project-MONAI/GenerativeModels/tree/main/tutorials)!
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: æƒ³äº†è§£æ›´å¤š MONAI Generative Model çš„æ•™ç¨‹ä»¥åŠæˆ‘ä»¬çš„åŠŸèƒ½ï¼Œè¯·æŸ¥çœ‹æˆ‘ä»¬çš„ [æ•™ç¨‹é¡µé¢](https://github.com/Project-MONAI/GenerativeModels/tree/main/tutorials)ï¼
- en: '*Note: All images unless otherwise noted are by the author*'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '*æ³¨æ„ï¼šé™¤éå¦æœ‰è¯´æ˜ï¼Œæ‰€æœ‰å›¾åƒå‡ç”±ä½œè€…æä¾›*'
