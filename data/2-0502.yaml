- en: 'Mastering ChatGPT: Effective Summarization with LLMs'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 掌握ChatGPT：使用LLM进行有效的摘要生成
- en: 原文：[https://towardsdatascience.com/chatgpt-summarization-llms-chatgpt3-chatgpt4-artificial-intelligence-16cf0e3625ce](https://towardsdatascience.com/chatgpt-summarization-llms-chatgpt3-chatgpt4-artificial-intelligence-16cf0e3625ce)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/chatgpt-summarization-llms-chatgpt3-chatgpt4-artificial-intelligence-16cf0e3625ce](https://towardsdatascience.com/chatgpt-summarization-llms-chatgpt3-chatgpt4-artificial-intelligence-16cf0e3625ce)
- en: How to Prompt ChatGPT to get High-Quality Summaries
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何提示ChatGPT以获得高质量的摘要
- en: '[](https://medium.com/@andvalenzuela?source=post_page-----16cf0e3625ce--------------------------------)[![Andrea
    Valenzuela](../Images/ddfc1534af92413fd91076f826cc49b6.png)](https://medium.com/@andvalenzuela?source=post_page-----16cf0e3625ce--------------------------------)[](https://towardsdatascience.com/?source=post_page-----16cf0e3625ce--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----16cf0e3625ce--------------------------------)
    [Andrea Valenzuela](https://medium.com/@andvalenzuela?source=post_page-----16cf0e3625ce--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@andvalenzuela?source=post_page-----16cf0e3625ce--------------------------------)[![Andrea
    Valenzuela](../Images/ddfc1534af92413fd91076f826cc49b6.png)](https://medium.com/@andvalenzuela?source=post_page-----16cf0e3625ce--------------------------------)[](https://towardsdatascience.com/?source=post_page-----16cf0e3625ce--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----16cf0e3625ce--------------------------------)
    [Andrea Valenzuela](https://medium.com/@andvalenzuela?source=post_page-----16cf0e3625ce--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----16cf0e3625ce--------------------------------)
    ·10 min read·May 22, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page-----16cf0e3625ce--------------------------------)
    ·10分钟阅读·2023年5月22日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/6c875157182b9485516e809510ba0514.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6c875157182b9485516e809510ba0514.png)'
- en: Self-made gif.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 自制gif。
- en: Are you part of the population that leaves reviews in Google maps everytime
    you visit to a new restaurant?
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 你是否是那种每次去新餐厅时都会在Google Maps上留下评论的人？
- en: Or perhaps you are the type to share your opinion on Amazon purchases, especially
    when you get triggered by a low-quality product?
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 或许你是那种在Amazon购买时会分享意见的人，特别是当你对低质量产品感到愤怒时？
- en: '*Don’t worry, I won’t blame you — we all have our moments!*'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '*别担心，我不会责怪你——我们都有这样的时刻！*'
- en: In today’s data world, we all contribute to the data deluge in multiple ways.
    One data type that I find particularly interesting due to its diversity and difficulty
    of interpretation is textual data, such as the countless reviews that are posted
    over the Internet every day. *Have you ever stopped to consider the importance
    of standardizing and condensing textual data?* **Welcome to the world of summarization
    agents!**
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在当今的数据世界中，我们以多种方式贡献于数据泛滥。我觉得特别有趣的一种数据类型是文本数据，如每天在互联网上发布的大量评论。*你是否曾经停下来考虑过标准化和压缩文本数据的重要性？*
    **欢迎来到摘要生成代理的世界！**
- en: '![](../Images/b1300eb548d9ed867e48cea796690892.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b1300eb548d9ed867e48cea796690892.png)'
- en: Summarization agents imagined by the AI image generation tool Dall-E.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: AI图像生成工具Dall-E想象中的摘要生成代理。
- en: Summarization agents have seamlessly integrated into our daily lives condensing
    information and providing quick access to relevant content across a multitude
    of applications and platforms.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 摘要生成代理无缝地融入了我们的日常生活，压缩信息并在各种应用程序和平台中提供快速访问相关内容。
- en: In this article, we will explore the utilization of ChatGPT as a powerful summarization
    agent for our custom applications. Thanks to the ability of Large Language Models
    (LLM) to process and understand texts, **they can assist in reading texts and
    generating accurate summaries or standarizing information**. However, it is important
    to know how to extract their potential in doing such a task, as well as to acknowledge
    their limitations.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们将探讨如何利用ChatGPT作为强大的摘要生成代理，服务于我们的自定义应用程序。由于大型语言模型（LLM）处理和理解文本的能力，**它们可以帮助阅读文本并生成准确的摘要或标准化信息**。然而，了解如何挖掘它们在这项任务中的潜力，以及认识到它们的局限性，是非常重要的。
- en: '*The biggest limitation for summarization?*'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '*摘要生成的最大限制是什么？*'
- en: '**LLMs often fall short when it comes to adhering to specific character or
    word limitations** in their summaries.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '**大型语言模型在遵守特定字符或单词限制方面经常显得不足**。'
- en: '**Let’s explore the best practices for generating summaries with ChatGPT**
    for our custom application, as well as the reasons behind its limitations and
    how to overcome them!'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '**让我们探索使用 ChatGPT 生成总结的最佳实践**，以及其局限性的原因和如何克服这些限制！'
- en: Effective Summarization with ChatGPT
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 ChatGPT 进行有效总结
- en: Summarization agents are used all over the Internet. For instance, websites
    use summarization agents to offer concise summaries of articles, enabling users
    to gain a quick overview of the news without diving into the entire content. Social
    media platforms and seach engines do this too.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 总结代理在互联网上广泛使用。例如，网站使用总结代理提供文章的简洁总结，使用户能够快速了解新闻而无需深入阅读整个内容。社交媒体平台和搜索引擎也会这样做。
- en: '**From news aggregators and social media platforms to e-commerce websites,
    summarization agents have become an integral part of our digital landscape**.
    And with the raise of LLMs, some of these agents are now using AI for more effective
    summarization results.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '**从新闻聚合器和社交媒体平台到电子商务网站，总结代理已成为我们数字环境的重要组成部分**。随着大型语言模型（LLMs）的兴起，这些代理中的一些现在使用人工智能来获得更有效的总结结果。'
- en: ChatGPT can be a good ally when building an application using summarization
    agents for speeding up reading tasks and classifying texts. For example, imagine
    we have an e-commerce and we are interested in processing all our costumer reviews.
    **ChatGPT could help us in summarizing any given review in a few sentences, standarizing
    it to a generic format, determine the sentiment of the review and classify it
    accordingly**.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: ChatGPT 在使用总结代理构建应用程序以加速阅读任务和分类文本时可以成为一个好的助手。例如，假设我们有一个电子商务网站，我们希望处理所有客户评论。**ChatGPT
    可以帮助我们将任何给定的评论总结成几句话，将其标准化为通用格式，确定评论的情感，并相应地分类**。
- en: While it is true that we could simply feed the review to ChatGPT, there are
    a list of best practices *— and things to avoid —* to leverage the power of ChatGPT
    in this concrete task.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们确实可以简单地将评论提供给 ChatGPT，但为了发挥 ChatGPT 在这个具体任务中的威力，有一系列的最佳实践*— 以及需要避免的事项 —*。
- en: '**Let’s explore the options by bring this example to life!**'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '**让我们通过这个示例来探索一下选项吧！**'
- en: 'Example: E-commerce Reviews'
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 示例：电子商务评论
- en: '![](../Images/086702ae2103f86e931e36ff8ec4a39f.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/086702ae2103f86e931e36ff8ec4a39f.png)'
- en: Self-made gif.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 自制 gif。
- en: 'Consider the example above in which we are interested in processing all the
    reviews for a given product on our e-commerce website. We would be interested
    in processing reviews such as the following one about our star product: **a first
    computer for children!**'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑上述示例，我们希望处理电子商务网站上给定产品的所有评论。我们会对处理以下关于我们明星产品的评论感兴趣：**孩子们的第一台电脑！**
- en: 'In this case, we would like that ChatGPT to:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们希望 ChatGPT：
- en: Classify the review into positive or negative.
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将评论分类为积极或消极。
- en: Provide a summary of the review of 20 words.
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提供一个20个字的评论总结。
- en: Output the response with a concrete structure to standardize all the reviews
    into one single format.
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 以具体结构输出响应，以将所有评论标准化为一种格式。
- en: Implementation Notes
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实现说明
- en: Here is the basic code structure we colud use to prompt ChatGPT from our custom
    application. I also provide a link to a [Jupyter Notebook](https://github.com/for-code-sake/chatgpt/blob/main/summarization-with-llms/summarization-examples.ipynb)
    with all the examples used in this article.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们可以用来从自定义应用程序提示 ChatGPT 的基本代码结构。我还提供了一个链接到 [Jupyter Notebook](https://github.com/for-code-sake/chatgpt/blob/main/summarization-with-llms/summarization-examples.ipynb)
    的链接，其中包含了本文中使用的所有示例。
- en: The function `get_completion()` calls the ChatGPT API with a given *prompt*.
    If the prompt contains additional *user text*, such as the review itself in our
    case, it is separated from the rest of the code by triple quotes.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 函数 `get_completion()` 调用 ChatGPT API，并带有给定的*提示*。如果提示包含额外的*用户文本*，例如在我们的案例中是评论本身，它会通过三引号与其余代码分开。
- en: '**Let’s use the** `**get_completion()**` **function to prompt ChatGPT !**'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '**让我们使用** `**get_completion()**` **函数来提示 ChatGPT！**'
- en: 'Here is a prompt fulfilling the requirements described above:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个满足上述要求的提示：
- en: ⚠️ The prompting guidelines used in this example such as using delimiters to
    separate the input text from the rest of the prompt and asking for a structured
    output are fully explained at [**What I Learned from OpenAI’s Course on Prompt
    Engineering — Prompting Guidelines**](https://medium.com/geekculture/prompt-engineering-prompting-guidelines-chatgpt-chatgpt3-chatgpt4-artificial-intelligence-6b74f35d2695)**.**
  id: totrans-38
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ⚠️ 在这个例子中使用的提示指南，例如使用分隔符将输入文本与其余提示分开，以及请求结构化输出，在[**我从 OpenAI 的提示工程课程中学到的 — 提示指南**](https://medium.com/geekculture/prompt-engineering-prompting-guidelines-chatgpt-chatgpt3-chatgpt4-artificial-intelligence-6b74f35d2695)**中有详细解释**。
- en: 'Here is ChatGPT’s answer:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 这是 ChatGPT 的回答：
- en: As we can observe from the output, the review is accurate and well structured,
    although **it misses some information we could be interested on as the owners
    of the e-commerce**, such as information about the delivery of the product.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 从输出结果中可以观察到，评论准确且结构良好，尽管**它遗漏了一些我们作为电子商务所有者可能感兴趣的信息**，例如关于产品交付的信息。
- en: Summarize with a Focus on <Shipping and Delivery>
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 聚焦于 <运输和交付> 的总结
- en: '**We can iteratively improve our prompt asking to ChatGPT some focus to include
    in the summary**. In this case, we are interested in any details given about the
    shipping and delivery:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '**我们可以通过迭代改进我们的提示，要求 ChatGPT 在总结中包含一些焦点**。在这种情况下，我们对关于运输和交付的任何细节感兴趣：'
- en: 'This time, ChatGPT’s answer is the following one:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 这一次，ChatGPT 的回答如下：
- en: Now the review is much more complete. **Giving details on the important focus
    of the original review is crucial to avoid ChatGPT skipping some information that
    might be valuable for our use case**.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 现在评论更加完整。**提供关于原始评论的重要焦点的细节对于避免 ChatGPT 跳过可能对我们使用案例有价值的信息至关重要**。
- en: '*Have you noticed that although this second trial includes information on the
    delivery, it skipped the only negative aspect of the original review?*'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '*你是否注意到，尽管这第二次尝试包含了关于交付的信息，但它跳过了原始评论中唯一的负面方面？*'
- en: '**Let’s fix that!**'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '**让我们解决这个问题！**'
- en: “Extract” instead of “Summarize”
  id: totrans-47
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: “提取”而不是“总结”
- en: By investigating summarization tasks, I found out that that **summarization
    can be a tricky task for LLMs if the user prompt is not accurate enough**.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 通过调查总结任务，我发现**如果用户提示不够准确，总结可能是 LLM 的一个棘手任务**。
- en: When asking ChatGPT to provide a summary of a given text, it can skip information
    that might be relevant for us *— as we have recently experienced —* , or it will
    give the same importance to all the topics on the text, only providing an overview
    of the main points.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 当请求 ChatGPT 提供给定文本的总结时，它可能会跳过对我们*— 正如我们最近经历过的 —* 可能相关的信息，或者它会对文本中的所有主题给予相同的重要性，仅提供主要点的概述。
- en: Experts in LLMs use the term *extract* and additional information on their focuses
    instead of *summarize* when doing such tasks assisted by these type of models.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用 LLM 进行此类任务时，专家们使用*提取*和附加关注点的信息，而不是*总结*。
- en: '**While summarization aims to provide a concise overview of the text’s main
    points including topics non-related to the topic of focus, information extraction
    focuses on retrieving specific details** and can give us what we are exactly looking
    for. Let’s try then with extraction!'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '**总结旨在提供文本主要点的简明概述，包括与焦点主题无关的主题，而信息提取则专注于检索具体细节**，可以为我们提供我们确切寻找的内容。让我们尝试一下提取吧！'
- en: 'In this case, by using extraction, we only get information about our topic
    of focus: `Shipping: Arrived a day earlier than expected.`'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '在这种情况下，通过提取，我们只获取关于我们关注主题的信息：`Shipping: Arrived a day earlier than expected.`'
- en: Automatization
  id: totrans-53
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自动化
- en: This system works for one single review. Neverthtless, when designing a prompt
    for a concrete application, **it is important to test it in a batch of examples
    so that we can catch any outliers or misbehavior in the model**.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 这个系统适用于单一评论。然而，在为具体应用设计提示时，**重要的是在一批示例中测试它，以便我们可以发现模型中的任何异常或不良行为**。
- en: In case of processing multiple reviews, here is a sample Python code structure
    that can help.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 如果处理多个评论，这里有一个示例 Python 代码结构可以帮助你。
- en: 'Here is the summaries of our batch of reviews:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们评论批次的总结：
- en: ⚠️ Note that although the word restriction of our summaries was clear enough
    in our prompts, we can easily see that this word limitation is not accomplished
    in any of the iterations.
  id: totrans-57
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ⚠️ 请注意，尽管我们提示中的字数限制足够明确，但我们可以很容易地看到这个字数限制在任何迭代中都没有得到遵守。
- en: 'This mismatch in the word counting happens because **LLMs do not have a precise
    understanding of word or character count**. The reason behind relies on one of
    the main important components of their architecture: **the tokenizer**.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 这种词数不匹配的现象发生是因为**LLM对词或字符数没有精确的理解**。其原因与其架构中的一个重要组件有关：**分词器**。
- en: Tokenizer
  id: totrans-59
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分词器
- en: LLMs like ChatGPT are designed to generate text based on statistical patterns
    learned from vast amounts of language data. **While they are highly effective
    at generating fluent and coherent text, they lack precise control over the word
    count**.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 像ChatGPT这样的LLM（大语言模型）旨在基于从大量语言数据中学到的统计模式生成文本。**虽然它们在生成流畅且连贯的文本方面非常有效，但它们缺乏对词数的精确控制**。
- en: In the examples above, when we have given instructions about a very precise
    word count, **ChatGPT has struggle to meet that requirements**. Instead, it has
    generated text that is actually shorter than the specified word count.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述例子中，当我们对词数提出非常精确的要求时，**ChatGPT往往难以满足这些要求**。相反，它生成的文本实际上比指定的词数要短。
- en: In other cases, it may generate longer texts or simply text that is overly verbose
    or lacking in detail. Additionally, **ChatGPT may prioritize other factors such
    as coherence and relevance, over strict adherence to the word count**. This can
    result in text that is high-quality in terms of its content and coherence, but
    which does not precisely match the word count requirement.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在其他情况下，它可能生成更长的文本，或者文本可能过于冗长或缺乏细节。此外，**ChatGPT可能会优先考虑连贯性和相关性等因素，而不是严格遵循词数要求**。这可能导致生成的文本在内容和连贯性方面质量很高，但不完全符合词数要求。
- en: '**The tokenizer is the key element in the architecture of ChatGPT that clearly
    influences the number of words of the generated output**.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '**分词器是ChatGPT架构中的关键元素，明显影响生成输出的单词数量**。'
- en: '![](../Images/eab3dee577752c4e18c4dc06019c4b5c.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/eab3dee577752c4e18c4dc06019c4b5c.png)'
- en: Self-made gif.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 自制gif。
- en: Tokenizer Architecture
  id: totrans-66
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分词器架构
- en: The tokenizer is the first step in the process of text generation. **It is responsible
    for breaking down the piece of text that we input to ChatGPT into individual elements
    *— tokens —*** , which are then processed by the language model to generate new
    text.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 分词器是文本生成过程中的第一步。**它负责将我们输入给ChatGPT的文本分解成单独的元素 *— 词元 —***，这些词元随后被语言模型处理以生成新文本。
- en: When the tokenizer breaks down a piece of text into tokens, it does so based
    on a set of rules that are designed to identify the meaningful units of the target
    language. However, these rules are not always perfect, and **there can be cases
    where the tokenizer splits or merges tokens in a way that affects the overall
    word count of the text**.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 当分词器将一段文本拆分成词元时，它是基于一套旨在识别目标语言中有意义的单位的规则。然而，这些规则并不总是完美的，**可能会出现分词器以影响文本总体词数的方式拆分或合并词元的情况**。
- en: 'For example, consider the following sentence: *“I want to eat a peanut butter
    sandwich”.* If the tokenizer is configured to split tokens based on spaces and
    punctuation, it may break this sentence down into the following tokens with a
    total word count of 8, equal to the token count.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，考虑以下句子：*“我想吃一份花生酱三明治”。* 如果分词器被配置为根据空格和标点符号来拆分词元，它可能会将这个句子拆分成以下词元，总词数为8，与词元数相等。
- en: '![](../Images/b3c808ab0be23847a489e30ef868ef03.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b3c808ab0be23847a489e30ef868ef03.png)'
- en: Self-made image.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 自制图像。
- en: However, if the tokenizer is configured to treat *“peanut butter”* as a compound
    word, it may break the sentence down into the following tokens, **with a total
    word count of 8, but a token count of 7**.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，如果分词器被配置为将*“花生酱”*视为一个复合词，它可能会将句子拆分成以下词元，**总词数为8，但词元数为7**。
- en: '![](../Images/10d9eac60a0ece12a78885329a5adc1a.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/10d9eac60a0ece12a78885329a5adc1a.png)'
- en: '**Thus, the way the tokenizer is configured can affect the overall word count
    of the text**, and this can impact the ability of the LLM to follow instructions
    about precise word counts. While some tokenizers offer options to customize how
    text is tokenized, this is not always sufficient to ensure precise adherence to
    word count requirements. **For ChatGPT in this case, we cannot control this part
    of its architecture**.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '**因此，分词器的配置方式会影响文本的总体词数**，这可能会影响LLM按照精确词数要求执行指令的能力。虽然一些分词器提供了自定义文本分词方式的选项，但这并不总是足以确保精确遵循词数要求。**对于ChatGPT而言，我们无法控制其架构的这一部分**。'
- en: This makes ChatGPT not good accomplishing character or word limitations, **but
    one can try with sentences instead since the tokenizer do not affect on the number
    of sentences, but their length**.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 这使得 ChatGPT 在完成字符或字数限制时表现不佳，**但可以尝试使用句子，因为分词器不会影响句子的数量，而是句子的长度**。
- en: Being aware of this restriction can help you to build the best suitable prompt
    for your application in mind. *Having this knowledge about how word count works
    on ChatGPT, let’s do a final iteration with our prompt for the e-commerce application!*
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 了解这一限制可以帮助你为你的应用程序构建最合适的提示。*了解 ChatGPT 如何处理字数，让我们对电子商务应用程序的提示做最后一次迭代！*
- en: 'Wrapping up: E-commerce Reviews'
  id: totrans-77
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结：电子商务评论
- en: 'Let’s combine our learnings from this article into a final prompt! In this
    case, we will be asking for the results in `HTML` format for a nicer output:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将本文的学习成果结合起来形成一个最终提示！在这种情况下，我们将要求结果以`HTML`格式输出，以获得更好的效果：
- en: 'And here is the final output from ChatGPT:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 这是 ChatGPT 的最终输出：
- en: '![](../Images/6f0192580ead353510a25e251dc3cdfe.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6f0192580ead353510a25e251dc3cdfe.png)'
- en: Self-made screenshot from the [Jupyter Notebook](https://github.com/for-code-sake/chatgpt/blob/main/summarization-with-llms/summarization-examples.ipynb)
    with the examples used in this article.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 自制截图来自于[**Jupyter Notebook**](https://github.com/for-code-sake/chatgpt/blob/main/summarization-with-llms/summarization-examples.ipynb)，其中包含本文使用的示例。
- en: Summary
  id: totrans-82
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this article, **we have discussed the best practices for using ChatGPT as
    a summarization agent for our custom application**.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，**我们讨论了将 ChatGPT 作为总结代理用于自定义应用程序的最佳实践**。
- en: We have seen that when building an application, it is extremely difficult to
    come up with the perfect prompt that matches your application requirements in
    the first trial. I think a nice take-home message is to **think about prompting
    as an iterative process** where you refine and model your prompt until you get
    exactly the desired output.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到，在构建应用程序时，第一次尝试时很难提出完全符合应用程序需求的完美提示。我认为一个很好的结论是**将提示视为一个迭代过程**，在这个过程中你不断完善和建模提示，直到获得完全期望的输出。
- en: By iteratively refining your prompt and applying it to a batch of examples before
    deploying it into production, you can make sure **the output is consistent across
    multiple examples and cover outlier responses**. In our example, it could happen
    that someone provides a random text instead of a review. **We can instruct ChatGPT
    to also have an standardize output to exclude this outlier responses**.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 通过迭代地完善你的提示并在投入生产之前将其应用于一批示例，你可以确保**输出在多个示例中保持一致并涵盖异常响应**。在我们的例子中，可能会有人提供随机文本而不是评论。**我们可以指示
    ChatGPT 也提供标准化的输出，以排除这些异常响应**。
- en: In addition, when using ChatGPT for a specific task, it is also a good practice
    to learn about the pros and cons of using LLMs for our target task. That is how
    we came across with the fact that ***extraction* tasks are more effective than
    *summarization* when we want a common human-like summary** of an input text. We
    have also learn that **providing the focus of the summary can be game-changer**
    regarding the generated content.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，在使用 ChatGPT 执行特定任务时，了解使用 LLMs 进行目标任务的优缺点也是一个好习惯。这就是我们了解到***提取*任务在需要常见的人类类似总结时比*总结*任务更有效**。我们还了解到**提供总结的重点可以改变生成的内容**。
- en: Finally, while LLMs can be highly effective at generating text, **they are not
    ideal for following precise instructions about word count or other specific formatting
    requirements**. To achieve these goals, it may be necessary to stick to sentence
    counting or use other tools or methods, such as manual editing or more specialized
    software.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，虽然大型语言模型（LLMs）在生成文本方面可以非常有效，但**它们并不适合精确遵循字数或其他特定格式要求的指示**。为了实现这些目标，可能需要坚持句子计数或使用其他工具或方法，例如手动编辑或更专业的软件。
- en: And that is all! Many thanks for reading!
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 就这样！非常感谢阅读！
- en: I hope this article helps **when building custom applications with ChatGPT!**
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 我希望这篇文章能帮助**在构建自定义应用程序时使用 ChatGPT！**
- en: 'You can also subscribe to my [**Newsletter**](https://medium.com/@andvalenzuela/subscribe)
    to stay tuned for new content. **Especially**, **if you are interested in articles
    about ChatGPT**:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 你还可以订阅我的[**新闻通讯**](https://medium.com/@andvalenzuela/subscribe)以关注新内容。**特别是**，**如果你对关于
    ChatGPT 的文章感兴趣**：
- en: '[](/chatgpt-text-to-speech-artificial-intelligence-python-data-science-52456f51fad6?source=post_page-----16cf0e3625ce--------------------------------)
    [## Unlocking a New Dimension of ChatGPT: Text-to-Speech Integration'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '[解锁ChatGPT的新维度：文本到语音集成](https://medium.com/geekculture/prompt-engineering-prompting-guidelines-chatgpt-chatgpt3-chatgpt4-artificial-intelligence-6b74f35d2695?source=post_page-----16cf0e3625ce--------------------------------)'
- en: Enhancing User Experience in ChatGPT Interactions
  id: totrans-92
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 增强ChatGPT互动中的用户体验
- en: towardsdatascience.com](/chatgpt-text-to-speech-artificial-intelligence-python-data-science-52456f51fad6?source=post_page-----16cf0e3625ce--------------------------------)
    [](https://medium.com/geekculture/prompt-engineering-prompting-guidelines-chatgpt-chatgpt3-chatgpt4-artificial-intelligence-6b74f35d2695?source=post_page-----16cf0e3625ce--------------------------------)
    [## What I Learned from OpenAI’s Course on Prompt Engineering — Prompting Guidelines
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '[我从OpenAI的提示工程课程中学到的—提示指南](https://medium.com/geekculture/prompt-engineering-prompting-guidelines-chatgpt-chatgpt3-chatgpt4-artificial-intelligence-6b74f35d2695?source=post_page-----16cf0e3625ce--------------------------------) '
- en: Get to know OpenAI’s Guidelines for Better Prompting
  id: totrans-94
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 了解OpenAI关于更好提示的指南
- en: 'medium.com](https://medium.com/geekculture/prompt-engineering-prompting-guidelines-chatgpt-chatgpt3-chatgpt4-artificial-intelligence-6b74f35d2695?source=post_page-----16cf0e3625ce--------------------------------)
    [](/what-chatgpt-knows-about-you-openai-towards-data-privacy-science-ai-b0fa2376a5f6?source=post_page-----16cf0e3625ce--------------------------------)
    [## What ChatGPT Knows about You: OpenAI’s Journey Towards Data Privacy'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '[ChatGPT对你的了解：OpenAI在数据隐私方面的历程](https://medium.com/geekculture/prompt-engineering-prompting-guidelines-chatgpt-chatgpt3-chatgpt4-artificial-intelligence-6b74f35d2695?source=post_page-----16cf0e3625ce--------------------------------) '
- en: New ways to manage personal data in ChatGPT
  id: totrans-96
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 管理ChatGPT中个人数据的新方法
- en: towardsdatascience.com](/what-chatgpt-knows-about-you-openai-towards-data-privacy-science-ai-b0fa2376a5f6?source=post_page-----16cf0e3625ce--------------------------------)
    [](https://levelup.gitconnected.com/improve-chatgpt-performance-prompt-engineering-data-science-artificial-intelligence-6fa3953bc5b6?source=post_page-----16cf0e3625ce--------------------------------)
    [## Improve ChatGPT Performance with Prompt Engineering
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '[通过提示工程提升ChatGPT性能](https://medium.com/geekculture/openai-fine-tuning-custom-chatgpt-transfer-learning-prompt-data-science-machine-learning-chatgpt3-chatgpt4-2aad7148438a?source=post_page-----16cf0e3625ce--------------------------------) '
- en: How to Ask Questions to ChatGPT to Maximize the Chances of a Successful Answer
  id: totrans-98
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 如何向ChatGPT提问以最大化成功回答的机会
- en: levelup.gitconnected.com](https://levelup.gitconnected.com/improve-chatgpt-performance-prompt-engineering-data-science-artificial-intelligence-6fa3953bc5b6?source=post_page-----16cf0e3625ce--------------------------------)
    [](https://medium.com/geekculture/openai-fine-tuning-custom-chatgpt-transfer-learning-prompt-data-science-machine-learning-chatgpt3-chatgpt4-2aad7148438a?source=post_page-----16cf0e3625ce--------------------------------)
    [## Create Your Custom ChatGPT with Transfer Learning
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '[创建自定义ChatGPT的迁移学习方法](https://levelup.gitconnected.com/improve-chatgpt-performance-prompt-engineering-data-science-artificial-intelligence-6fa3953bc5b6?source=post_page-----16cf0e3625ce--------------------------------) '
- en: Enhance ChatGPT Capabilities Fine-tuning Your Own Model
  id: totrans-100
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提升ChatGPT能力，微调你自己的模型
- en: medium.com](https://medium.com/geekculture/openai-fine-tuning-custom-chatgpt-transfer-learning-prompt-data-science-machine-learning-chatgpt3-chatgpt4-2aad7148438a?source=post_page-----16cf0e3625ce--------------------------------)
    [](https://pub.towardsai.net/chatgpt-vs-artificial-intelligence-detectors-chatgpt4-chatgpt3-openai-technology-2c46e6acf6b?source=post_page-----16cf0e3625ce--------------------------------)
    [## ChatGPT vs AI Detectors — Place your Bets!
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '[ChatGPT与AI检测器——你会押注哪一方！](https://pub.towardsai.net/chatgpt-vs-artificial-intelligence-detectors-chatgpt4-chatgpt3-openai-technology-2c46e6acf6b?source=post_page-----16cf0e3625ce--------------------------------) '
- en: Testing the Most Popular AI Detectors on the Internet
  id: totrans-102
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 测试互联网中最受欢迎的AI检测器
- en: pub.towardsai.net](https://pub.towardsai.net/chatgpt-vs-artificial-intelligence-detectors-chatgpt4-chatgpt3-openai-technology-2c46e6acf6b?source=post_page-----16cf0e3625ce--------------------------------)
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '[pub.towardsai.net](https://pub.towardsai.net/chatgpt-vs-artificial-intelligence-detectors-chatgpt4-chatgpt3-openai-technology-2c46e6acf6b?source=post_page-----16cf0e3625ce--------------------------------)'
- en: '**Feel free to forward any questions** you may have to *forcodesake.hello@gmail.com*
    :)'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你有任何问题，**随时可以转发**到 *forcodesake.hello@gmail.com* :)
