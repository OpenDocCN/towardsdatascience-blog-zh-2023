- en: Unlock the Secret to Efficient Batch Prediction Pipelines Using Python, a Feature
    Store and GCS
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 解锁使用 Python、特征存储和 GCS 的高效批量预测管道的秘密
- en: 原文：[https://towardsdatascience.com/unlock-the-secret-to-efficient-batch-prediction-pipelines-using-python-a-feature-store-and-gcs-17a1462ca489](https://towardsdatascience.com/unlock-the-secret-to-efficient-batch-prediction-pipelines-using-python-a-feature-store-and-gcs-17a1462ca489)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/unlock-the-secret-to-efficient-batch-prediction-pipelines-using-python-a-feature-store-and-gcs-17a1462ca489](https://towardsdatascience.com/unlock-the-secret-to-efficient-batch-prediction-pipelines-using-python-a-feature-store-and-gcs-17a1462ca489)
- en: '[THE FULL STACK 7-STEPS MLOPS FRAMEWORK](https://towardsdatascience.com/tagged/full-stack-mlops)'
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '[完整堆栈 7 步 MLOps 框架](https://towardsdatascience.com/tagged/full-stack-mlops)'
- en: 'Lesson 3: Batch Prediction Pipeline. Package Python Modules with Poetry'
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 课程 3：批量预测管道。使用 Poetry 打包 Python 模块
- en: '[](https://pauliusztin.medium.com/?source=post_page-----17a1462ca489--------------------------------)[![Paul
    Iusztin](../Images/d07551a78fa87940220b49d9358f3166.png)](https://pauliusztin.medium.com/?source=post_page-----17a1462ca489--------------------------------)[](https://towardsdatascience.com/?source=post_page-----17a1462ca489--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----17a1462ca489--------------------------------)
    [Paul Iusztin](https://pauliusztin.medium.com/?source=post_page-----17a1462ca489--------------------------------)'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://pauliusztin.medium.com/?source=post_page-----17a1462ca489--------------------------------)[![Paul
    Iusztin](../Images/d07551a78fa87940220b49d9358f3166.png)](https://pauliusztin.medium.com/?source=post_page-----17a1462ca489--------------------------------)[](https://towardsdatascience.com/?source=post_page-----17a1462ca489--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----17a1462ca489--------------------------------)
    [Paul Iusztin](https://pauliusztin.medium.com/?source=post_page-----17a1462ca489--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----17a1462ca489--------------------------------)
    ·15 min read·May 12, 2023
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----17a1462ca489--------------------------------)
    ·阅读时间 15 分钟·2023年5月12日
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/d8383467c77df456d69215c8e1509ca6.png)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d8383467c77df456d69215c8e1509ca6.png)'
- en: Photo by [Hassan Pasha](https://unsplash.com/@hpzworkz?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 由 [Hassan Pasha](https://unsplash.com/@hpzworkz?utm_source=medium&utm_medium=referral)
    拍摄，来自 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: This tutorial represents **lesson 3 out of a 7-lesson course** that will walk
    you step-by-step through how to **design, implement, and deploy an ML system**
    using **MLOps good practices**. During the course, you will build a production-ready
    model to forecast energy consumption levels for the next 24 hours across multiple
    consumer types from Denmark.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 本教程代表了**一个包含 7 节课程中的第 3 节**，将逐步指导你如何**设计、实施和部署 ML 系统**，并运用**MLOps 的良好实践**。在课程中，你将构建一个生产就绪的模型，以预测来自丹麦的不同消费者类型在接下来的
    24 小时内的能源消耗水平。
- en: '*By the end of this course, you will understand all the fundamentals of designing,
    coding and deploying an ML system using a batch-serving architecture.*'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '*在本课程结束时，你将理解使用批量服务架构设计、编码和部署 ML 系统的所有基础知识。*'
- en: This course *targets mid/advanced machine learning engineers* who want to level
    up their skills by building their own end-to-end projects.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 本课程*面向中级/高级机器学习工程师*，他们希望通过构建自己的端到端项目来提升技能。
- en: Nowadays, certificates are everywhere. Building advanced end-to-end projects
    that you can later show off is the best way to get recognition as a professional
    engineer.
  id: totrans-12
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 现在，证书无处不在。构建高级端到端项目，并在之后展示出来，是获得专业工程师认可的最佳方式。
- en: 'Table of Contents:'
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 目录：
- en: Course Introduction
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 课程介绍
- en: Course Lessons
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 课程内容
- en: Data Source
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据来源
- en: 'Lesson 3: Batch Prediction Pipeline. Package Python Modules with Poetry.'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 课程 3：批量预测管道。使用 Poetry 打包 Python 模块。
- en: 'Lesson 3: Code'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 课程 3：代码
- en: Conclusion
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 结论
- en: References
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 参考资料
- en: Course Introduction
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 课程介绍
- en: '***At the end of this 7 lessons course, you will know how to:***'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '***在这 7 节课程结束时，你将学会：***'
- en: design a batch-serving architecture
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设计一个批量服务架构
- en: use Hopsworks as a feature store
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Hopsworks 作为特征存储
- en: design a feature engineering pipeline that reads data from an API
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设计一个从 API 读取数据的特征工程管道
- en: build a training pipeline with hyper-parameter tunning
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建一个带有超参数调优的训练管道
- en: use W&B as an ML Platform to track your experiments, models, and metadata
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 W&B 作为 ML 平台来跟踪你的实验、模型和元数据
- en: implement a batch prediction pipeline
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现一个批量预测管道
- en: use Poetry to build your own Python packages
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Poetry 构建你自己的 Python 包
- en: deploy your own private PyPi server
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 部署你自己的私有 PyPi 服务器
- en: orchestrate everything with Airflow
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Airflow 协调一切
- en: use the predictions to code a web app using FastAPI and Streamlit
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用预测来编写一个使用 FastAPI 和 Streamlit 的 Web 应用
- en: use Docker to containerize your code
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Docker 对代码进行容器化
- en: use Great Expectations to ensure data validation and integrity
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Great Expectations 确保数据验证和完整性
- en: monitor the performance of the predictions over time
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随时间监控预测的性能
- en: deploy everything to GCP
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将所有内容部署到 GCP
- en: build a CI/CD pipeline using GitHub Actions
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 GitHub Actions 构建 CI/CD 管道
- en: If that sounds like a lot, don't worry. After you cover this course, you will
    understand everything I said before. Most importantly, you will know WHY I used
    all these tools and how they work together as a system.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 如果这些听起来很多，不用担心。完成这门课程后，你会理解我之前说的所有内容。最重要的是，你将知道**为什么**我使用了这些工具以及它们如何作为一个系统协同工作。
- en: '**If you want to get the most out of this course,** [**I suggest you access
    the GitHub repository**](https://github.com/iusztinpaul/energy-forecasting) **containing
    all the lessons'' code. This course is designed to read and replicate the code
    along the articles quickly.**'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '**如果你想从这门课程中获得最大收益，** [**我建议你访问包含所有课程代码的 GitHub 仓库**](https://github.com/iusztinpaul/energy-forecasting)
    **。这门课程的设计目的是快速阅读和复现文章中的代码。**'
- en: By the end of the course, you will know how to implement the diagram below.
    Don't worry if something doesn't make sense to you. I will explain everything
    in detail.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在课程结束时，你将知道如何实现下面的图示。不要担心如果有些地方不太明白。我会详细解释所有内容。
- en: '![](../Images/4b5c3b0b8e2162ea8fd268ca745199ec.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4b5c3b0b8e2162ea8fd268ca745199ec.png)'
- en: Diagram of the architecture you will build during the course [Image by the Author].
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 你将在课程中构建的架构图 [作者提供的图片]。
- en: By the **end of Lesson 3**, you will know how to implement and integrate the
    **batch prediction pipeline** and **package all the Python modules using Poetry**.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在**第 3 课结束时**，你将学会如何实现和集成**批量预测管道**以及**使用 Poetry 打包所有 Python 模块**。
- en: 'Course Lessons:'
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 课程内容：
- en: '[Batch Serving. Feature Stores. Feature Engineering Pipelines.](https://medium.com/towards-data-science/a-framework-for-building-a-production-ready-feature-engineering-pipeline-f0b29609b20f)'
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[批量服务。特征存储。特征工程管道。](https://medium.com/towards-data-science/a-framework-for-building-a-production-ready-feature-engineering-pipeline-f0b29609b20f)'
- en: '[Training Pipelines. ML Platforms. Hyperparameter Tuning.](https://medium.com/towards-data-science/a-guide-to-building-effective-training-pipelines-for-maximum-results-6fdaef594cee)'
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[训练管道。ML 平台。超参数调整。](https://medium.com/towards-data-science/a-guide-to-building-effective-training-pipelines-for-maximum-results-6fdaef594cee)'
- en: '**Batch Prediction Pipeline. Package Python Modules with Poetry.**'
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**批量预测管道。使用 Poetry 打包 Python 模块。**'
- en: '[Private PyPi Server. Orchestrate Everything with Airflow.](https://medium.com/towards-data-science/unlocking-mlops-using-airflow-a-comprehensive-guide-to-ml-system-orchestration-880aa9be8cff)'
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[私有 PyPi 服务器。使用 Airflow 协调一切。](https://medium.com/towards-data-science/unlocking-mlops-using-airflow-a-comprehensive-guide-to-ml-system-orchestration-880aa9be8cff)'
- en: '[Data Validation for Quality and Integrity using GE. Model Performance Continuous
    Monitoring.](/ensuring-trustworthy-ml-systems-with-data-validation-and-real-time-monitoring-89ab079f4360)'
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[使用 GE 进行数据验证以确保质量和完整性。模型性能持续监控。](/ensuring-trustworthy-ml-systems-with-data-validation-and-real-time-monitoring-89ab079f4360)'
- en: '[Consume and Visualize your Model’s Predictions using FastAPI and Streamlit.
    Dockerize Everything.](https://medium.com/towards-data-science/fastapi-and-streamlit-the-python-duo-you-must-know-about-72825def1243)'
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[使用 FastAPI 和 Streamlit 消费和可视化模型预测。对一切进行 Docker 化。](https://medium.com/towards-data-science/fastapi-and-streamlit-the-python-duo-you-must-know-about-72825def1243)'
- en: '[Deploy All the ML Components to GCP. Build a CI/CD Pipeline Using Github Actions.](https://medium.com/towards-data-science/seamless-ci-cd-pipelines-with-github-actions-on-gcp-your-tools-for-effective-mlops-96f676f72012)'
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[将所有 ML 组件部署到 GCP。使用 GitHub Actions 构建 CI/CD 管道。](https://medium.com/towards-data-science/seamless-ci-cd-pipelines-with-github-actions-on-gcp-your-tools-for-effective-mlops-96f676f72012)'
- en: '[[Bonus] Behind the Scenes of an ‘Imperfect’ ML Project — Lessons and Insights](https://medium.com/towards-data-science/imperfections-unveiled-the-intriguing-reality-behind-our-mlops-course-creation-6ff7d52ecb7e)'
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[[额外内容] ‘不完美’ ML 项目的幕后 — 教训与见解](https://medium.com/towards-data-science/imperfections-unveiled-the-intriguing-reality-behind-our-mlops-course-creation-6ff7d52ecb7e)'
- en: 'If you want to grasp this lesson fully, we recommend you check out our [previous
    lesson](https://medium.com/towards-data-science/a-guide-to-building-effective-training-pipelines-for-maximum-results-6fdaef594cee),
    which talks about designing a training pipeline that uses a feature store and
    an ML platform:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想全面掌握本节内容，我们建议你查看我们的[上一篇课程](https://medium.com/towards-data-science/a-guide-to-building-effective-training-pipelines-for-maximum-results-6fdaef594cee)，其中讲述了设计一个使用特征存储和ML平台的训练管道：
- en: '[](/a-guide-to-building-effective-training-pipelines-for-maximum-results-6fdaef594cee?source=post_page-----17a1462ca489--------------------------------)
    [## A Guide to Building Effective Training Pipelines for Maximum Results'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/a-guide-to-building-effective-training-pipelines-for-maximum-results-6fdaef594cee?source=post_page-----17a1462ca489--------------------------------)
    [## 建立有效训练管道以获得最佳结果指南'
- en: 'Lesson 2: Training Pipelines. ML Platforms. Hyperparameter Tuning.'
  id: totrans-55
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 第2节：训练管道。ML平台。超参数调整。
- en: towardsdatascience.com](/a-guide-to-building-effective-training-pipelines-for-maximum-results-6fdaef594cee?source=post_page-----17a1462ca489--------------------------------)
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/a-guide-to-building-effective-training-pipelines-for-maximum-results-6fdaef594cee?source=post_page-----17a1462ca489--------------------------------)
- en: Data Source
  id: totrans-57
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据来源
- en: We used a free & open API that provides hourly energy consumption values for
    all the energy consumer types within Denmark [1].
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用了一个免费的开放API，提供丹麦所有能源消费者类型的每小时能源消耗值[1]。
- en: They provide an intuitive interface where you can easily query and visualize
    the data. [You can access the data here](https://www.energidataservice.dk/tso-electricity/ConsumptionDE35Hour)
    [1].
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 它们提供了一个直观的界面，您可以轻松查询和可视化数据。[您可以在这里访问数据](https://www.energidataservice.dk/tso-electricity/ConsumptionDE35Hour)
    [1]。
- en: 'The data has 4 main attributes:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 数据有4个主要属性：
- en: '**Hour UTC:** the UTC datetime when the data point was observed.'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**小时UTC：** 观察到数据点的UTC日期时间。'
- en: '**Price Area:** Denmark is divided into two price areas: DK1 and DK2 — divided
    by the Great Belt. DK1 is west of the Great Belt, and DK2 is east of the Great
    Belt.'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**价格区域：** 丹麦被分为两个价格区域：DK1和DK2——由大贝尔特分隔。DK1位于大贝尔特以西，DK2位于大贝尔特以东。'
- en: '**Consumer Type:** The consumer type is the Industry Code DE35, owned and maintained
    by Danish Energy.'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**消费者类型：** 消费者类型是工业代码DE35，由丹麦能源公司拥有和维护。'
- en: '**Total Consumption:** Total electricity consumption in kWh'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**总消耗：** 总电力消耗（以千瓦时为单位）'
- en: '**Note:** The observations have a lag of 15 days! But for our demo use case,
    that is not a problem, as we can simulate the same steps as it would in real-time.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意：** 观察数据有15天的延迟！但对于我们的演示用例，这不是问题，因为我们可以模拟实时中的相同步骤。'
- en: '![](../Images/e0bc098121320b6b981889d8d712952d.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e0bc098121320b6b981889d8d712952d.png)'
- en: A screenshot from our web app showing how we forecasted the energy consumption
    for area = 1 and consumer_type = 212 [Image by the Author].
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的网页应用程序的截图，显示了我们如何预测区域=1和消费者类型=212的能源消耗[作者提供的图片]。
- en: 'The data points have an hourly resolution. For example: "2023–04–15 21:00Z",
    "2023–04–15 20:00Z", "2023–04–15 19:00Z", etc.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 数据点具有每小时分辨率。例如：“2023–04–15 21:00Z”，“2023–04–15 20:00Z”，“2023–04–15 19:00Z”等。
- en: We will model the data as multiple time series. Each unique **price area** and
    **consumer type tuple represents its** unique time series.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将数据建模为多个时间序列。每个独特的**价格区域**和**消费者类型元组代表其**独特的时间序列。
- en: Thus, we will build a model that independently forecasts the energy consumption
    for the next 24 hours for every time series.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们将构建一个模型，独立预测每个时间序列的未来24小时能源消耗。
- en: '*Check out the video below to better understand what the data looks like* 👇'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '*查看下面的视频，以更好地了解数据的样子* 👇'
- en: Course & data source overview [Video by the Author].
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 课程和数据来源概览[作者提供的视频]。
- en: '**Lesson 3: Batch Prediction Pipeline. Package Python Modules with Poetry.**'
  id: totrans-73
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**第3节：批量预测管道。使用Poetry打包Python模块。**'
- en: The Goal of Lesson 3
  id: totrans-74
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第3节的目标
- en: This lesson will teach you how to build the batch prediction pipeline. Also,
    it will show you how to package into Python PyPi modules, using Poetry, all the
    code from the pipelines we have done so far in Lessons 1, 2, and 3\. 👇
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 本课程将教你如何构建批量预测管道。此外，它还将展示如何使用Poetry将我们在第1、第2和第3节中完成的所有管道代码打包成Python PyPi模块。👇
- en: '**Note:** In the next lesson, we will upload these Python modules into our
    own private PyPi server and install them from Airflow.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意：** 在下一节中，我们将把这些Python模块上传到我们自己的私有PyPi服务器，并从Airflow中安装它们。'
- en: '![](../Images/7d77eb4a5b81e825ecb286bea9157466.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7d77eb4a5b81e825ecb286bea9157466.png)'
- en: Diagram of the final architecture with the Lesson 3 components highlighted in
    blue [Image by the Author].
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 最终架构图，其中第 3 课的组件以蓝色突出显示 [作者提供的图片]。
- en: 'If you recall from Lesson 1, a model can be deployed in the following ways:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你记得第 1 课，模型可以通过以下方式进行部署：
- en: batch mode
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 批处理模式
- en: request-response (e.g., RESTful API or gRPC)
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 请求-响应（例如，RESTful API 或 gRPC）
- en: streaming mode
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 流式模式
- en: embedded
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 嵌入式
- en: This course will *deploy the model in batch mode*.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 本课程将*以批处理模式部署模型*。
- en: We will discuss strategies to transition from batch to other methods when building
    the web app. You will see how natural it is.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将讨论在构建 web 应用程序时如何从批处理过渡到其他方法。你会发现这非常自然。
- en: But, if you are eager to compare the batch mode with a request-response serving
    mode, [check out my 5-minute article that explains how to serve a model using
    the request-response methodology](https://faun.pub/key-concepts-for-model-serving-38ccbb2de372Q).
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，如果你渴望比较批处理模式与请求-响应服务模式，[可以查看我写的 5 分钟文章，解释如何使用请求-响应方法服务模型](https://faun.pub/key-concepts-for-model-serving-38ccbb2de372Q)。
- en: '**What are the main steps of deploying a model in batch mode, aka building
    a batch prediction pipeline?**'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '**批处理模式下部署模型的主要步骤是什么，也就是构建批量预测管道的步骤？**'
- en: '**Step 1:** You will load the features from the feature store in batch mode.'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤 1：** 你将从特征库中以批处理模式加载特征。'
- en: '**Step 2:** You will load the trained model from the model registry (in our
    case, we use Hopsworks as a model registry).'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤 2：** 你将从模型注册表中加载训练好的模型（在我们的例子中，我们使用 Hopsworks 作为模型注册表）。'
- en: '**Step 3:** You will forecast the energy consumption levels for the next 24
    hours.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤 3：** 你将预测未来 24 小时的能源消耗水平。'
- en: '**Step 4:** You will save the predictions in a GCP bucket.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤 4：** 你将把预测结果保存到 GCP 存储桶中。'
- en: After, various consumers will read the predictions from the GCP bucket and use
    them accordingly. In our case, we implemented a dashboard using FastAPI and Streamlit.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 之后，各种消费者将从 GCP 存储桶中读取预测并相应使用它们。在我们的案例中，我们使用 FastAPI 和 Streamlit 实现了一个仪表板。
- en: '*Often, your initial deployment strategy will be in batch mode.*'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '*通常，你的初始部署策略将是批处理模式。*'
- en: '**Why?**'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '**为什么？**'
- en: Because doing so, you don't have to focus on restrictions such as latency and
    throughput. By saving your predictions into some storage, you can quickly make
    your model online.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 因为这样做，你不必关注延迟和吞吐量等限制。通过将预测保存到某些存储中，你可以快速使模型上线。
- en: Thus, batch mode is the easiest and fastest way of deploying your model while
    preserving a good experience for the end user of the applications.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，批处理模式是部署模型的最简单和最快的方式，同时保持应用程序最终用户的良好体验。
- en: A model is online when an application can access the predictions in real-time.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 当应用程序可以实时访问预测时，模型是在线的。
- en: Note that the predictions are not made in real-time, only accessed in real-time
    (e.g., read from the storage).
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，预测不是实时生成的，仅在实时访问（例如，从存储读取）中进行。
- en: '*The biggest downside* of using this method is that your predictions will have
    a degree of lag. For example, in our use case, you make and save the predictions
    for the next 24 hours. Let’s assume that 2 hours pass without any new predictions.
    Now, you have predictions only for the next 22 hours.'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 使用此方法的*最大缺点*是你的预测将有一定的滞后。例如，在我们的用例中，你生成并保存未来 24 小时的预测。假设经过 2 小时没有新的预测，现在你只有未来
    22 小时的预测。
- en: Where the number of predictions that you have to store is reasonable, you can
    bypass this issue by making the predictions often. In our example, we will make
    the predictions hourly — our data has a resolution of 1 hour. Thus, we solved
    the lag issue by constantly making and storing new predictions.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 当你必须存储的预测数量是合理的时，你可以通过频繁生成预测来绕过这个问题。在我们的示例中，我们将每小时生成预测——我们的数据分辨率为 1 小时。因此，我们通过不断生成和存储新预测来解决延迟问题。
- en: '*But here comes the second problem with the batch prediction strategy*. Suppose
    the set of predictions is large. For example, you want to predict the recommendations
    for 1 million users with a database of 100 million items. Then, computing the
    predictions very often will be highly costly.'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '*但这里出现了批量预测策略的第二个问题*。假设预测集合很大。例如，你想预测 100 万用户的推荐，而数据库中有 1 亿个项目。那么，频繁计算预测将是非常昂贵的。'
- en: Then you have to consider using other serving methods strongly.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，你必须强烈考虑使用其他服务方法。
- en: '***But here is the catch.***'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '***但这里有一个陷阱。***'
- en: Your application probably won't start with a database of 1 million users and
    100 million items. That means you can safely begin using a batch mode architecture
    and gradually shift to other methodologies when it makes sense.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 你的应用程序可能不会一开始就有 100 万用户和 1 亿条数据。这意味着你可以安全地从批处理模式架构开始，并在有必要时逐步转向其他方法。
- en: That is what most people do!
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是大多数人做的事情！
- en: To get an intuition on how to shift to other methods, [check out this article](https://medium.com/mlearning-ai/this-is-what-you-need-to-know-to-build-an-mlops-end-to-end-architecture-c0be1deaa3ce)
    to learn about a *standardized ML architecture* *suggested by* *Google Cloud.*
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解如何转向其他方法，[请查看这篇文章](https://medium.com/mlearning-ai/this-is-what-you-need-to-know-to-build-an-mlops-end-to-end-architecture-c0be1deaa3ce)，了解*Google
    Cloud* *建议的* *标准化 ML 架构*。
- en: Theoretical Concepts & Tools
  id: totrans-107
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理论概念与工具
- en: '**GCS:** GCS stands for Google Cloud Storage, which is Google''s storage solution
    within GCP. It is similar to AWS S3 if you are more familiar with it.'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '**GCS：** GCS 代表 Google Cloud Storage，是 Google 在 GCP 中的存储解决方案。如果你更熟悉 AWS S3，它类似于此。'
- en: You can write to GCS any file. In our course, we will write Pandas DataFrames
    as parquet files.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以向 GCS 写入任何文件。在我们的课程中，我们将 Pandas DataFrames 写入 parquet 文件。
- en: '**GCS vs. Redis:** We choose to write our predictions in GCS because of 4 main
    reasons:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '**GCS 与 Redis：** 我们选择将预测结果写入 GCS 主要有 4 个原因：'
- en: Easy to setup
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 易于设置
- en: No maintenance
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 无需维护
- en: Access to the free tier
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 访问免费层
- en: We will also use GCP to deploy the code.
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们还将使用 GCP 来部署代码。
- en: Redis is a popular choice for caching your predictions to be later accessed
    by various clients.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: Redis 是缓存预测结果以便后续由各种客户端访问的热门选择。
- en: '*Why?*'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '*为什么？*'
- en: Because you can access the data at low latency, improving the users' experience.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 由于你可以以低延迟访问数据，从而改善用户体验。
- en: It would have been a good choice, but we wanted to simplify things.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 本来这是一个不错的选择，但我们想要简化事情。
- en: Also, it is good practice to write the predictions on GCS for long-term storage
    and cache them in Redis for real-time access.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，将预测结果写入 GCS 以便长期存储，并在 Redis 中缓存以供实时访问也是一种良好的做法。
- en: '**Poetry:** Poetry is my favorite Python virtual environment manager. It is
    similar to Conda, venv, and Pipenv. In my opinion, it is superior because:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '**Poetry：** Poetry 是我最喜欢的 Python 虚拟环境管理工具。它类似于 Conda、venv 和 Pipenv。依我看，它更优，因为：'
- en: It offers you a **.lock** file that reflects the versions of all your sub-dependencies.
    Thus, replicating code is extremely easy and safe.
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它提供了一个**.lock**文件，反映了所有子依赖项的版本。因此，复制代码非常简单和安全。
- en: You can quickly build your module directly using Poetry. No other setup is required.
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你可以直接使用 Poetry 快速构建你的模块。无需其他设置。
- en: You can quickly deploy your module to a PiPy server using Poetry. No other setup
    is required, and more…
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你可以使用 Poetry 快速将模块部署到 PiPy 服务器。无需其他设置，还有更多……
- en: 'Lesson 3: Code'
  id: totrans-124
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第 3 课：代码
- en: '[You can access the GitHub repository here.](https://github.com/iusztinpaul/energy-forecasting)'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '[你可以在这里访问 GitHub 仓库。](https://github.com/iusztinpaul/energy-forecasting)'
- en: '**Note:** All the installation instructions are in the READMEs of the repository.
    Here we will jump straight to the code.'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意：** 所有安装说明都在仓库的 README 文件中。这里我们将直接跳到代码。'
- en: '*All the code within Lesson 3 is located under the* [***batch-prediction-pipeline***](https://github.com/iusztinpaul/energy-forecasting/tree/main/batch-prediction-pipeline)*folder.*'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '*第 3 课中的所有代码都位于* [***batch-prediction-pipeline***](https://github.com/iusztinpaul/energy-forecasting/tree/main/batch-prediction-pipeline)*文件夹中。*'
- en: 'The files under the [**batch-prediction-pipeline**](https://github.com/iusztinpaul/energy-forecasting/tree/main/batch-prediction-pipeline)folderare
    structured as follows:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '[**batch-prediction-pipeline**](https://github.com/iusztinpaul/energy-forecasting/tree/main/batch-prediction-pipeline)
    文件夹下的文件结构如下：'
- en: '![](../Images/c47f99f9476d03de8ade95a30b583daa.png)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c47f99f9476d03de8ade95a30b583daa.png)'
- en: A screenshot that shows the structure of the batch-prediction-pipeline folder
    [Image by the Author].
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 显示批量预测管道文件夹结构的屏幕截图 [作者提供的图片]。
- en: All the code is located under the [**batch_prediction_pipeline**](https://github.com/iusztinpaul/energy-forecasting/tree/main/batch-prediction-pipeline/batch_prediction_pipeline)directory
    (note the "_" instead of "-")**.**
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 所有代码都位于 [**batch_prediction_pipeline**](https://github.com/iusztinpaul/energy-forecasting/tree/main/batch-prediction-pipeline/batch_prediction_pipeline)
    目录下（注意“_”而不是“-”）。
- en: Directly storing credentials in your git repository is a huge security risk.
    That is why you will inject sensitive information using a **.env** file.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 直接在你的 git 仓库中存储凭据是一个巨大的安全风险。这就是为什么你将使用**.env**文件来注入敏感信息。
- en: The **.env.default** is an example of all the variables you must configure.
    It is also helpful to store default values for attributes that are not sensitive
    (e.g., project name).
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '**.env.default**是你必须配置的所有变量的示例。它还可以帮助存储那些不敏感的属性的默认值（例如，项目名称）。'
- en: '![](../Images/1b4f40ca19a12ac8ff070610a8530d46.png)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1b4f40ca19a12ac8ff070610a8530d46.png)'
- en: A screenshot of the .env.default file [Image by the Author].
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: .env.default文件的截图[图片由作者提供]。
- en: Prepare Credentials
  id: totrans-136
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备凭据
- en: First of all, you have to create a **.env** filewhere you will add all our credentials.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，你需要创建一个**.env**文件，在其中添加我们所有的凭据。
- en: I already showed you in [Lesson 1](/a-framework-for-building-a-production-ready-feature-engineering-pipeline-f0b29609b20f)
    how to set up your **.env** file. Also, I explained in [Lesson 1](/a-framework-for-building-a-production-ready-feature-engineering-pipeline-f0b29609b20f)
    how the variables from the **.env** file are loaded from your **ML_PIPELINE_ROOT_DIR**
    directory into a **SETTINGS** Python dictionary to be used throughout your code.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 我已经在[第1课](/a-framework-for-building-a-production-ready-feature-engineering-pipeline-f0b29609b20f)中向你展示了如何设置你的**.env**文件。同时，我在[第1课](/a-framework-for-building-a-production-ready-feature-engineering-pipeline-f0b29609b20f)中解释了如何将**.env**文件中的变量从**ML_PIPELINE_ROOT_DIR**目录加载到**SETTINGS**
    Python字典中，以便在代码中使用。
- en: Thus, if you want to replicate what I have done, I strongly recommend checking
    out [Lesson 1](/a-framework-for-building-a-production-ready-feature-engineering-pipeline-f0b29609b20f).
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，如果你想复制我所做的，我强烈建议查看[第1课](/a-framework-for-building-a-production-ready-feature-engineering-pipeline-f0b29609b20f)。
- en: '*If you only want a light read, you can completely skip the "****Prepare Credentials****"
    step.*'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '*如果你只是想轻松阅读，可以完全跳过“****准备凭据****”步骤。*'
- en: 'In Lesson 3, you will use two services:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 在第3课中，你将使用两个服务：
- en: '[Hopsworks](https://www.hopsworks.ai/)'
  id: totrans-142
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[Hopsworks](https://www.hopsworks.ai/)'
- en: '[GCP — Cloud Storage](https://cloud.google.com/storage)'
  id: totrans-143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[GCP — Cloud Storage](https://cloud.google.com/storage)'
- en: '[***Hopsworks***](https://www.hopsworks.ai/) ***(free)***'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '[***Hopsworks***](https://www.hopsworks.ai/) ***(免费)***'
- en: We already showed you in [Lesson 1](https://medium.com/towards-data-science/a-framework-for-building-a-production-ready-feature-engineering-pipeline-f0b29609b20f)
    how to set up the credentials for **Hopsworks**. Please visit the ["Prepare Credentials"
    section from Lesson 1](https://medium.com/towards-data-science/a-framework-for-building-a-production-ready-feature-engineering-pipeline-f0b29609b20f),
    where we showed you in detail how to set up the API KEY for Hopsworks.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经在[第1课](https://medium.com/towards-data-science/a-framework-for-building-a-production-ready-feature-engineering-pipeline-f0b29609b20f)中向你展示了如何设置**Hopsworks**的凭据。请访问[第1课中的“准备凭据”部分](https://medium.com/towards-data-science/a-framework-for-building-a-production-ready-feature-engineering-pipeline-f0b29609b20f)，在那里我们详细展示了如何设置Hopsworks的API
    KEY。
- en: '[***GCP — Cloud Storage***](https://cloud.google.com/storage) ***(free)***'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '[***GCP — Cloud Storage***](https://cloud.google.com/storage) ***(免费)***'
- en: While replicating this course, you will stick to the *GCP — Cloud Storage free
    tier*. You can store up to 5GB for free in GCP — Cloud Storage, which is far more
    than enough for our use case.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 在复制本课程时，你将坚持使用*GCP — Cloud Storage 免费层*。你可以在GCP — Cloud Storage中免费存储最多5GB，这对我们的使用情况绰绰有余。
- en: This configuration step will be longer, but I promise that it is not complicated.
    By the way, you will learn the basics of using a cloud vendor such as GCP.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 这个配置步骤会稍长一些，但我保证它并不复杂。顺便说一下，你将学习使用像GCP这样的云服务提供商的基础知识。
- en: First, go to GCP and create a project called "**energy_consumption"** (or any
    other name)**.** Afterward, go to your GCP project's "Cloud Storage" section and
    create a **non-public bucket** called "**hourly-batch-predictions**"**.** Pick
    any region, but just be aware of it—[official docs about creating a bucket on
    GCP](https://cloud.google.com/storage/docs/creating-buckets) [2].
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，访问GCP并创建一个名为“**energy_consumption**”（或其他任意名称）的项目**。** 随后，前往GCP项目的“Cloud Storage”部分，创建一个名为“**hourly-batch-predictions**”的**非公开存储桶**。**
    选择任何区域，但请注意这一点—[创建GCP存储桶的官方文档](https://cloud.google.com/storage/docs/creating-buckets)
    [2]。
- en: '**NOTE:** *You might need to pick different names due to constant changes to
    the platform’s rules*. That is not an issue, just call them as you wish and change
    them in the **.env** file: *GOOGLE_CLOUD_PROJECT* (ours “energy_consumption”)
    and *GOOGLE_CLOUD_BUCKET_NAME (*ours “hourly-batch-predictions”*)*.'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意：** *由于平台规则的不断变化，你可能需要选择不同的名称*。这不是问题，只需根据你的需求命名，并在**.env**文件中进行更改：*GOOGLE_CLOUD_PROJECT*（我们的“energy_consumption”）和*GOOGLE_CLOUD_BUCKET_NAME*（我们的“hourly-batch-predictions”）。'
- en: '![](../Images/f0cd1bf7dff7a64180918b371d064e98.png)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f0cd1bf7dff7a64180918b371d064e98.png)'
- en: Screenshot of the GCP — Cloud Storage view, where you must create your bucket
    [Image by the Author].
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: GCP — Cloud Storage 视图的截图，你需要在其中创建你的桶 [图片由作者提供]。
- en: Now you finished creating all your GCP resources. The last step is to create
    a way to have read & write access to the GCP bucket directly from your Python
    code.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经完成了所有 GCP 资源的创建。最后一步是创建一种方式，以便通过你的 Python 代码直接访问 GCP 桶的读写权限。
- en: You can easily do this using GCP *service accounts.* I don't want to hijack
    the whole article with GCP configurations. Thus, [this GCP official doc shows
    you how to create a service account](https://cloud.google.com/iam/docs/service-accounts-create)
    [3].
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过 GCP *服务账户*轻松做到这一点。我不想把整篇文章都挤占于 GCP 配置。因此，[这份 GCP 官方文档展示了如何创建服务账户](https://cloud.google.com/iam/docs/service-accounts-create)
    [3]。
- en: '*When creating the service account, be aware of one thing!*'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '*创建服务账户时，请注意一件事！*'
- en: Service accounts have attached different roles. A role is a way to configure
    your service account with various permissions.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 服务账户具有不同的角色。角色是一种配置服务账户权限的方式。
- en: Thus, you need to configure your service account to have read & write access
    the your "**hourly-batch-predictions**" bucket.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，你需要配置你的服务账户以拥有对你的“**hourly-batch-predictions**”桶的读写访问权限。
- en: You can easily do that by choosing the "**Storage Object Admin**" role when
    creating your service account.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过在创建服务账户时选择“**Storage Object Admin**”角色来轻松做到这一点。
- en: The final step is to find a way to authenticate with your newly created service
    account in your Python code.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一步是找到一种方法，在你的 Python 代码中使用新创建的服务账户进行身份验证。
- en: You can easily do that by going to your service account and creating a JSON
    key. Again, [here are the official GCP docs that will show you how to create a
    JSON key for your service account](https://cloud.google.com/iam/docs/keys-create-delete)
    [4].
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过访问你的服务账户并创建一个 JSON 密钥来轻松做到这一点。再次，[这里是官方 GCP 文档，它会告诉你如何为你的服务账户创建 JSON 密钥](https://cloud.google.com/iam/docs/keys-create-delete)
    [4]。
- en: '*Again, keep in mind one thing!*'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: '*再次，请记住一件事！*'
- en: When creating the JSON key, you will download a JSON file.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 创建 JSON 密钥时，你将下载一个 JSON 文件。
- en: After you download your JSON file, put it in a safe place and go to your **.env**
    file. There, change the value of *GOOGLE_CLOUD_SERVICE_ACCOUNT_JSON_PATH*with
    your absolute path to the JSON file.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 下载 JSON 文件后，将其放在安全的地方，并转到你的**.env**文件。在那里，将*GOOGLE_CLOUD_SERVICE_ACCOUNT_JSON_PATH*的值更改为
    JSON 文件的绝对路径。
- en: '![](../Images/1b4f40ca19a12ac8ff070610a8530d46.png)'
  id: totrans-164
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1b4f40ca19a12ac8ff070610a8530d46.png)'
- en: A screenshot of the .env.default file [Image by the Author].
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: '**.env.default** 文件的截图 [图片由作者提供]。'
- en: '**NOTE:** Remember to change the *GOOGLE_CLOUD_PROJECT* and *GOOGLE_CLOUD_BUCKET_NAME*
    variables with your names.'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意：** 记得将*GOOGLE_CLOUD_PROJECT*和*GOOGLE_CLOUD_BUCKET_NAME*变量更改为你的名称。'
- en: '*Congratulations! You are done configuring GCS — Cloud Storage.*'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: '*恭喜！你已经完成了 GCS — Cloud Storage 的配置。*'
- en: Now you have created a GCP project and bucket. Also, you have read & write access
    using your Python code through your service account. You log in with your service
    account with the help of the JSON file.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经创建了一个 GCP 项目和桶。此外，你可以通过服务账户使用你的 Python 代码进行读写访问。你使用 JSON 文件的帮助登录到服务账户。
- en: If something isn't working, let me know in the comments below or directly on
    [LinkedIn](https://www.linkedin.com/feed/).
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 如果有什么问题，请在下面的评论中告诉我，或直接在[LinkedIn](https://www.linkedin.com/feed/)上联系我。
- en: Batch Prediction Pipeline — Main Function
  id: totrans-170
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 批量预测管道 — 主功能
- en: 'As you can see, the main function follows the 4 steps of a batch prediction
    pipeline:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，主功能遵循批量预测管道的四个步骤：
- en: Loads data from the Feature Store in batch mode.
  id: totrans-172
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从特征存储中以批处理模式加载数据。
- en: Loads the model from the model registry.
  id: totrans-173
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从模型注册表中加载模型。
- en: Makes the predictions.
  id: totrans-174
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 进行预测。
- en: It saves the predictions to the GCS bucket.
  id: totrans-175
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将预测结果保存到 GCS 桶中。
- en: Most of the function is log lines 😆
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 大部分功能都是日志行 😆
- en: Along these 4 main steps, you must load all the parameters from the metadata
    generated by previous steps, such as the **feature_view_version** and **model_version.**
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 在这四个主要步骤中，你必须从之前步骤生成的元数据中加载所有参数，例如**feature_view_version**和**model_version**。
- en: Also, you have to get a reference to the Hopsworks Feature store.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 另外，你需要获得对 Hopsworks 特征存储的引用。
- en: After, you go straight to the 4 main steps that we will detail later in the
    tutorial 👇
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，你直接进入我们将在后面的教程中详细介绍的四个主要步骤 👇
- en: 'Step 1: Loading Data From the Feature Store In Batch Mode'
  id: totrans-180
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 步骤 1：以批处理模式从特征存储加载数据
- en: This step is similar to what we have done in [Lesson 2](/a-guide-to-building-effective-training-pipelines-for-maximum-results-6fdaef594cee)
    when loading data for training.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 这一步类似于我们在[第2课](/a-guide-to-building-effective-training-pipelines-for-maximum-results-6fdaef594cee)中加载训练数据时所做的。
- en: But this time, instead of downloading the data from a training dataset, we directly
    ask for a batch of data between a datetime range, using the **get_batch_data()**
    method.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 但这一次，我们不是从训练数据集中下载数据，而是直接请求一个日期时间范围内的数据批次，使用**get_batch_data()**方法。
- en: Doing so allows us to time travel to our desired datetime range and ask for
    the features we need. This method makes batch inference extremely easy.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 这样做允许我们时间旅行到所需的日期时间范围，并请求所需的特征。这种方法使批量推理变得非常简单。
- en: The last step is to prepare the indexes of the DataFrame as expected by **sktime**
    and to split it between X and y.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一步是按照**sktime**的预期准备DataFrame的索引，并将其分割为X和y。
- en: '**Note:** This is an autoregressive process: we learn from past values of y
    to predict future values of y ( y = energy consumption levels). Thus, we will
    use only X as input to the model. We will use y only for visualization purposes.'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意：** 这是一个自回归过程：我们从过去的y值中学习以预测未来的y值（y = 能源消耗水平）。因此，我们将只使用X作为模型的输入。我们将仅将y用于可视化目的。'
- en: 'Step 2: Loading the Model From the Model Registry'
  id: totrans-186
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第二步：从模型注册表加载模型
- en: Loading a model from the Hopsworks model registry is extremely easy.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 从Hopsworks模型注册表加载模型非常简单。
- en: The function below has as a parameter a reference to the Hopsworks project and
    the version of the model we want to download.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的函数有一个参数是对Hopsworks项目的引用和我们要下载的模型版本。
- en: Using these two variables, you get a reference to the model registry. Afterward,
    you get a reference to the model itself using its name. In this case, it is **best_model**.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这两个变量，你可以获得对模型注册表的引用。之后，通过模型的名称，你可以获得对模型本身的引用。在这种情况下，它是**best_model**。
- en: Finally, you download the artifact/model and load it into memory.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，你下载工件/模型并将其加载到内存中。
- en: The trick here is that your model is versioned. Thus, you always know what model
    you are using.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的诀窍是你的模型是有版本控制的。因此，你总是知道你使用的是哪个模型。
- en: '**Note:** We uploaded the **best_model** in the model registry using the training
    pipeline explained in [Lesson 2](https://medium.com/towards-data-science/a-guide-to-building-effective-training-pipelines-for-maximum-results-6fdaef594cee).
    The training pipeline also provides us with a metadata dictionary that contains
    the latest model_version.'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意：** 我们使用在[第2课](https://medium.com/towards-data-science/a-guide-to-building-effective-training-pipelines-for-maximum-results-6fdaef594cee)中解释的训练管道上传了**best_model**到模型注册表。训练管道还提供了一个包含最新model_version的元数据字典。'
- en: 'Step 3: Forecast Energy Consumption Levels for the Next 24 Hours'
  id: totrans-193
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第三步：预测未来24小时的能源消耗水平
- en: '**Sktime** makes forecasting extremely easy. The key line from the snippet
    below is "**predictions = model.predict(X=X_forecast)"**, which forecasts the
    energy consumption values for the next 24 hours.'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: '**Sktime**使预测变得极其简单。下面代码片段中的关键行是“**predictions = model.predict(X=X_forecast)**”，它预测了未来24小时的能源消耗值。'
- en: The forecasting horizon of 24 hours was given when the model was trained. Thus,
    it already knows how many data points into the future to forecast.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 模型训练时设置了24小时的预测时间范围。因此，它已经知道未来要预测多少个数据点。
- en: Also, you have to prepare the exogenous variable **X_forecast**. In time series
    forecasting, an exogenous variable is a feature that you already know it will
    happen in the future. For example, a holiday. Thus, based on your training data
    X which contains all the area and consumer types IDs, you can generate the **X_forecast**
    variable by mapping the datetime range into the forecasting range.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 另外，你需要准备外生变量**X_forecast**。在时间序列预测中，外生变量是你已经知道未来会发生的特征。例如，节假日。因此，基于你的训练数据X，其中包含所有区域和消费者类型ID，你可以通过将日期时间范围映射到预测范围来生成**X_forecast**变量。
- en: 'Step 4: Save the Predictions to the Bucket'
  id: totrans-197
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第四步：将预测结果保存到存储桶
- en: The last component is the function that saves everything to the GCP bucket.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一个组件是将所有内容保存到GCP存储桶的函数。
- en: This step is relatively straightforward, and the hard part was to configure
    your bucket and access credentials.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 这一步相对简单，难点在于配置你的存储桶和访问凭证。
- en: We get a reference to the bucket, iterate through X, y & predictions and write
    them to the bucket as a blob.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 我们获取对存储桶的引用，遍历X、y和预测，将它们作为blob写入存储桶。
- en: '**Note:** Besides the predictions, we also save X and y to have everything
    in one place to quickly access everything we need and nicely render them in the
    web app.'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意：** 除了预测结果外，我们还保存X和y，以便将所有内容集中在一个地方，方便快速访问所有所需内容，并在网页应用中漂亮地呈现它们。'
- en: To get a reference to the bucket, you have to access the settings you configured
    at the beginning of the tutorial.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 要获取桶的引用，你必须访问在教程开始时配置的设置。
- en: As you can see, you create a GCS client with the project name and the JSON credentials
    file path. Afterward, you can quickly get a reference to your given bucket.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，你创建了一个GCS客户端，并指定了项目名称和JSON凭据文件路径。之后，你可以快速获取你指定桶的引用。
- en: Writing a blob to a bucket is highly similar to writing a regular file.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 将blob写入桶与写入普通文件非常相似。
- en: You get a reference to the blob you want to write and open the resource with
    "**with blob.open("wb") as f**".
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 你获取你要写入的blob的引用，并使用"**with blob.open("wb") as f**"打开资源。
- en: Note that you opened the blob in binary format.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 注意你以二进制格式打开了blob。
- en: You are writing the data in parquet format, as it is an excellent trade-off
    between storage size and writing & reading performance.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 你将数据以parquet格式写入，因为它在存储大小和写入&读取性能之间是一个很好的折衷。
- en: Package Python Modules with Poetry
  id: totrans-208
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用Poetry打包Python模块
- en: '[Poetry](https://python-poetry.org/) makes the building process extremely easy.'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: '[Poetry](https://python-poetry.org/)使构建过程变得极其简单。'
- en: The first obvious step is to use Poetry as your virtual environment manager.
    That means you already have the "**pyproject.toml"** and "**poetry.lock**" files
    — we already provided these files for you.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步显而易见，就是使用Poetry作为你的虚拟环境管理器。这意味着你已经拥有了"**pyproject.toml**"和"**poetry.lock**"文件——我们已经为你提供了这些文件。
- en: 'Now, all you have to do is to go to your project at the same level as your
    Poetry files (the ones mentioned above — for example, go to your [batch-prediction-pipeline](https://github.com/iusztinpaul/energy-forecasting/tree/main/batch-prediction-pipeline)
    directory) and run:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你只需进入与你的Poetry文件（上述文件）同一层级的项目目录（例如，进入你的[batch-prediction-pipeline](https://github.com/iusztinpaul/energy-forecasting/tree/main/batch-prediction-pipeline)目录）并运行：
- en: '[PRE0]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: This will create a **dist** folder containing your package as a **wheel.** Now
    you can directly install your package using the wheel file or deploy it to a PyPi
    server.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 这将创建一个包含你的包的**dist**文件夹作为**wheel**。现在你可以直接使用wheel文件安装你的包，或将其部署到PyPi服务器上。
- en: 'To deploy it, configure your PyPi server credentials with the following:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 要进行部署，请使用以下配置你的PyPi服务器凭据：
- en: '[PRE1]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Finally, deploy it using the following:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，使用以下命令进行部署：
- en: '[PRE2]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: And that was it. I was amazed at how easy Poetry can make this process.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 就这样。我对Poetry如何简化这个过程感到惊讶。
- en: Otherwise, building and deploying your Python package is a tedious and lengthy
    process.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 否则，构建和部署你的Python包是一个繁琐且漫长的过程。
- en: In [Lesson 4](/unlocking-mlops-using-airflow-a-comprehensive-guide-to-ml-system-orchestration-880aa9be8cff),
    you will deploy your private PyPi server and deploy all the code you have written
    until this point using the commands I showed you above.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第4课](/unlocking-mlops-using-airflow-a-comprehensive-guide-to-ml-system-orchestration-880aa9be8cff)中，你将部署你的私人PyPi服务器，并使用我在上面展示的命令部署到目前为止你编写的所有代码。
- en: Conclusion
  id: totrans-221
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: Congratulations! You finished the **third lesson** from the **Full Stack 7-Steps
    MLOps Framework** course.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜你！你完成了**Full Stack 7-Steps MLOps Framework**课程的**第三课**。
- en: 'If you have reached this far, you know how to:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你已经走到这一步，你知道如何：
- en: choose the right architecture
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择正确的架构
- en: access data from the feature store in batch mode
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 以批量模式从特征存储中访问数据
- en: download your model from the model registry
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从模型注册中心下载你的模型
- en: build an inference pipeline
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建推理管道
- en: save your predictions to GCS
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将你的预测结果保存到GCS
- en: Now that you understand the power of using and implementing a batch prediction
    architecture, you can quickly serve models in real-time while paving your way
    for other fancier serving methods.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你了解了使用和实现批量预测架构的强大功能，你可以快速实时提供模型，同时为其他更高级的服务方法铺平道路。
- en: Check out [Lesson 4](/unlocking-mlops-using-airflow-a-comprehensive-guide-to-ml-system-orchestration-880aa9be8cff)
    to learn about hosting your own private PyPi server and orchestrating all the
    pipelines using Airflow.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 查看[第4课](/unlocking-mlops-using-airflow-a-comprehensive-guide-to-ml-system-orchestration-880aa9be8cff)，了解如何托管你自己的私人PyPi服务器以及如何使用Airflow编排所有管道。
- en: '**Also,** [**you can access the GitHub repository here**](https://github.com/iusztinpaul/energy-forecasting)**.**'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: '**另外，** [**你可以在这里访问GitHub仓库**](https://github.com/iusztinpaul/energy-forecasting)**。**'
- en: 💡 My goal is to help machine learning engineers level up in designing and productionizing
    ML systems. Follow me on [LinkedIn](https://www.linkedin.com/in/pauliusztin/)
    or subscribe to my [weekly newsletter](https://pauliusztin.substack.com/) for
    more insights!
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 💡 我的目标是帮助机器学习工程师在设计和生产化机器学习系统方面提升水平。关注我在 [LinkedIn](https://www.linkedin.com/in/pauliusztin/)
    或订阅我的 [每周通讯](https://pauliusztin.substack.com/)以获取更多见解！
- en: 🔥 If you enjoy reading articles like this and wish to support my writing, consider
    [becoming a Medium member](https://pauliusztin.medium.com/membership). By using
    [my referral link](https://pauliusztin.medium.com/membership), you can support
    me without any extra cost while enjoying limitless access to Medium’s rich collection
    of stories.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 🔥 如果你喜欢阅读这样的文章并希望支持我的写作，可以考虑 [成为 Medium 会员](https://pauliusztin.medium.com/membership)。通过使用
    [我的推荐链接](https://pauliusztin.medium.com/membership)，你可以在没有额外成本的情况下支持我，同时享受 Medium
    丰富故事的无限制访问权限。
- en: '[](https://pauliusztin.medium.com/membership?source=post_page-----17a1462ca489--------------------------------)
    [## Join Medium with my referral link - Paul Iusztin'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://pauliusztin.medium.com/membership?source=post_page-----17a1462ca489--------------------------------)
    [## 使用我的推荐链接加入 Medium - Paul Iusztin]'
- en: 🤖 Join to get exclusive content about designing and building production-ready
    ML systems 🚀 Unlock full access to…
  id: totrans-235
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 🤖 加入以获取有关设计和构建生产就绪的机器学习系统的独家内容 🚀 解锁完整访问权限…
- en: pauliusztin.medium.com](https://pauliusztin.medium.com/membership?source=post_page-----17a1462ca489--------------------------------)
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: pauliusztin.medium.com](https://pauliusztin.medium.com/membership?source=post_page-----17a1462ca489--------------------------------)
- en: References
  id: totrans-237
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考资料
- en: '[1] [Energy Consumption per DE35 Industry Code from Denmark API](https://www.energidataservice.dk/tso-electricity/ConsumptionDE35Hour),
    [Denmark Energy Data Service](https://www.energidataservice.dk/about/)'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] [丹麦 API 的 DE35 行业代码能源消耗](https://www.energidataservice.dk/tso-electricity/ConsumptionDE35Hour)，[丹麦能源数据服务](https://www.energidataservice.dk/about/)'
- en: '[2] [Create buckets](https://cloud.google.com/storage/docs/creating-buckets),
    GCP Cloud Storage Docs'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] [创建存储桶](https://cloud.google.com/storage/docs/creating-buckets)，GCP Cloud
    Storage 文档'
- en: '[3] [Create service accounts](https://cloud.google.com/iam/docs/service-accounts-create),
    GCP IAM Docs'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] [创建服务账户](https://cloud.google.com/iam/docs/service-accounts-create)，GCP
    IAM 文档'
- en: '[4] [Create and delete service account keys](https://cloud.google.com/iam/docs/keys-create-delete),
    GCP IAM Docs'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] [创建和删除服务账户密钥](https://cloud.google.com/iam/docs/keys-create-delete)，GCP
    IAM 文档'
