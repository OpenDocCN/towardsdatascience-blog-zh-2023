- en: Metal Programming in Julia
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Julia中的Metal编程
- en: 原文：[https://towardsdatascience.com/metal-programming-in-julia-2db5fe8ee32c](https://towardsdatascience.com/metal-programming-in-julia-2db5fe8ee32c)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/metal-programming-in-julia-2db5fe8ee32c](https://towardsdatascience.com/metal-programming-in-julia-2db5fe8ee32c)
- en: '![](../Images/1b0fa479f9906bf1570e41eed0082af9.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1b0fa479f9906bf1570e41eed0082af9.png)'
- en: Little Heavy | Image by Author
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: Little Heavy | 作者提供的图片
- en: Leveraging the power of macOS GPUs with the Metal.jl Framework.
  id: totrans-4
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 利用macOS GPU的强大功能，通过Metal.jl框架。
- en: '[](https://lausena.medium.com/?source=post_page-----2db5fe8ee32c--------------------------------)[![Gabriel
    Sena](../Images/713235a9a7f276a72862c38293d7ac89.png)](https://lausena.medium.com/?source=post_page-----2db5fe8ee32c--------------------------------)[](https://towardsdatascience.com/?source=post_page-----2db5fe8ee32c--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----2db5fe8ee32c--------------------------------)
    [Gabriel Sena](https://lausena.medium.com/?source=post_page-----2db5fe8ee32c--------------------------------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://lausena.medium.com/?source=post_page-----2db5fe8ee32c--------------------------------)[![Gabriel
    Sena](../Images/713235a9a7f276a72862c38293d7ac89.png)](https://lausena.medium.com/?source=post_page-----2db5fe8ee32c--------------------------------)[](https://towardsdatascience.com/?source=post_page-----2db5fe8ee32c--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----2db5fe8ee32c--------------------------------)
    [Gabriel Sena](https://lausena.medium.com/?source=post_page-----2db5fe8ee32c--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----2db5fe8ee32c--------------------------------)
    ·11 min read·Dec 4, 2023
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page-----2db5fe8ee32c--------------------------------)
    ·阅读时间11分钟·2023年12月4日
- en: --
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: Introduction
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 介绍
- en: Just last year, we were [introduced](https://www.youtube.com/watch?v=IARikXzRU7s&ab_channel=TheJuliaProgrammingLanguage)
    to the [Metal.jl](https://github.com/JuliaGPU/Metal.jl) Framework, a GPU backend
    for Apple Hardware. This is exciting news for [Julia](https://julialang.org/)
    practitioners looking to leverage the full potential of their macOS M-series chips.
    In particular, data scientists and ML engineers can speed up their computational
    workflows by tapping into the parallel processing power of [GPUs](/what-is-a-gpu-and-do-you-need-one-in-deep-learning-718b9597aa0d),
    resulting in faster training and inference times. The integration of Metal.jl
    into the Julia ecosystem signifies an important push towards aligning the language’s
    capabilities with the continually evolving landscape of scientific computing and
    machine learning on Apple platforms.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 就在去年，我们[了解到了](https://www.youtube.com/watch?v=IARikXzRU7s&ab_channel=TheJuliaProgrammingLanguage)
    [Metal.jl](https://github.com/JuliaGPU/Metal.jl)框架，这是一个用于苹果硬件的GPU后端。这对希望利用其macOS
    M系列芯片全部潜力的[Julia](https://julialang.org/)从业者来说是令人兴奋的消息。特别是，数据科学家和机器学习工程师可以通过利用[GPU](/what-is-a-gpu-and-do-you-need-one-in-deep-learning-718b9597aa0d)的并行处理能力来加速他们的计算工作流程，从而实现更快的训练和推断时间。Metal.jl的引入标志着对将语言的能力与苹果平台上不断发展的科学计算和机器学习领域对接的重要推动。
- en: In [2020](https://www.apple.com/newsroom/2020/06/apple-announces-mac-transition-to-apple-silicon/),
    Apple began transitioning its Mac lineup from Intel-based processors to Apple
    Silicon, starting with the M1 chip. While this has been a historic and impressive
    achievement from Apple it did come with its fair share of criticisms and issues.
    Since picking up my 32-core Mac Studio M1-chip, I have been looking to fully leverage
    the GPU and tinker with new applications. I must say, it hasn’t been all fun and
    games. From [ARM](https://en.wikipedia.org/wiki/ARM_architecture_family) architecture
    compatibility issues to unsupported machine learning libraries — it has been a
    challenge at times to get a working environment. This is expected with any huge
    major transition and way of operating. I remain hopeful and have seen major improvements
    across the board for stability and features.
  id: totrans-10
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 在[2020年](https://www.apple.com/newsroom/2020/06/apple-announces-mac-transition-to-apple-silicon/)时，苹果公司开始将其Mac系列从基于Intel的处理器过渡到Apple
    Silicon，从M1芯片开始。尽管这是苹果公司历史性且令人印象深刻的成就，但也伴随着不少批评和问题。自从我拿到32核的Mac Studio M1芯片后，我一直希望充分利用GPU并尝试新的应用程序。我必须说，这并非全是轻松愉快的经历。从[ARM](https://en.wikipedia.org/wiki/ARM_architecture_family)架构兼容性问题到不支持的机器学习库——有时要获得一个工作环境确实是一种挑战。这是任何重大过渡和操作方式中都会遇到的预期问题。我仍然保持乐观，并看到在稳定性和功能方面的全面重大改进。
- en: '**In this article, we will preview the Metal.jl Framework in order to understand
    its capabilities. We will also demonstrate a practical example using** [**Flux**](https://fluxml.ai/)**,
    a library for machine learning in Julia, with the Metal backend.**'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '**在本文中，我们将预览 Metal.jl 框架以了解其能力。我们还将展示一个使用** [**Flux**](https://fluxml.ai/)**的实际示例，它是
    Julia 中的一个机器学习库，并配合 Metal 后端。**'
- en: '*Here is the outline for the topics covered:*'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '*以下是涵盖主题的纲要：*'
- en: '**I.** [**Project Setup**](#35fc)'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '**I.** [**项目设置**](#35fc)'
- en: i. [Julia environment setup](#39c6)
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: i. [Julia 环境设置](#39c6)
- en: ii. [Dependency overview](#8e23)
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: ii. [依赖概述](#8e23)
- en: '**II.** [**Leveraging the Metal API**](#6b3d)'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '**II.** [**利用 Metal API**](#6b3d)'
- en: i. [Kernel Functions](#0501)
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: i. [内核函数](#0501)
- en: ii. [Benchmarking](#0ab7)
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: ii. [基准测试](#0ab7)
- en: iii. [Profiling](#9a1c)
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: iii. [性能分析](#9a1c)
- en: '**III.** [**Working with Flux and Metal Backend**](#4dd6)'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '**III.** [**使用 Flux 和 Metal Backend**](#4dd6)'
- en: i. [Dataset overview](#29ea)
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: i. [数据集概述](#29ea)
- en: ii. [Simple neural network](#76ed)
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: ii. [简单神经网络](#76ed)
- en: iii. [Model evaluation](#3777)
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: iii. [模型评估](#3777)
- en: 'Readers who wish to follow along should have:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 希望跟随的读者应具备：
- en: Basic knowledge of the [Julia programming language](https://pub.aimind.so/getting-started-with-the-julia-programming-language-0c0d8521a381).
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对 [Julia 编程语言](https://pub.aimind.so/getting-started-with-the-julia-programming-language-0c0d8521a381)
    的基本知识。
- en: High-level understanding of Machine Learning concepts.
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对机器学习概念的高级理解。
- en: '*Let’s dive in!*'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '*让我们深入了解吧！*'
- en: '**PROJECT SETUP**'
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**项目设置**'
- en: i. Julia Environment Setup
  id: totrans-29
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: i. Julia 环境设置
- en: It is considered good practice to set up an environment unique to your project.
    By doing so you will isolate exact versions of packages required for a project
    and allow for an easily reproducible environment for yourself and team members.
    This is easily done in [Julia](https://pub.aimind.so/getting-started-with-the-julia-programming-language-0c0d8521a381)
    as seen below.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 设置一个特定于项目的环境被视为良好实践。这样可以隔离项目所需的确切版本的包，并为自己和团队成员提供一个容易复现的环境。这在 [Julia](https://pub.aimind.so/getting-started-with-the-julia-programming-language-0c0d8521a381)
    中很容易做到，如下所示。
- en: ii. Dependency overview
  id: totrans-31
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ii. 依赖概述
- en: '[**Metal**](https://github.com/JuliaGPU/Metal.jl)**:** This is the framework
    that makes it possible to program GPUs on macOS. As mentioned by the contributors,
    the package is a **work-in-progress** and there are bugs, functionality missing,
    and performance that hasn’t been fully optimized yet. **Please ensure you have
    also met the following system requirements:**'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '[**Metal**](https://github.com/JuliaGPU/Metal.jl)**:** 这是一个使在 macOS 上编程 GPU
    成为可能的框架。如贡献者所述，该包是**正在开发中**，存在错误、功能缺失和性能尚未完全优化的情况。**请确保您还满足以下系统要求：**'
- en: ✔ Mac device with M-series chip️️️
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: ✔ 配备 M 系列芯片的 Mac 设备️️️
- en: ✔ Julia 1.8–1.10
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: ✔ Julia 1.8–1.10
- en: ✔ macOS 13 (Ventura) or 14 (Sonoma)
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: ✔ macOS 13 (Ventura) 或 14 (Sonoma)
- en: '[**BenchmarkTools**](https://github.com/JuliaGPU/Metal.jl)**:** We will be
    using this library to execute benchmarks for some of the operations we send to
    our GPU via the Metal APIs. This package makes it easy to configure, execute,
    and analyze results.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '[**BenchmarkTools**](https://github.com/JuliaGPU/Metal.jl)**:** 我们将使用这个库来执行一些操作的基准测试，这些操作通过
    Metal APIs 发送到我们的 GPU。这个包使配置、执行和分析结果变得容易。'
- en: '[**Flux**](https://fluxml.ai/Flux.jl/stable/)**:** Flux is an intuitive machine
    learning library for Julia; it is designed to provide a high-level and user-friendly
    interface for building and training neural networks. We will be using this library
    for the example and leveraging the Metal Backend to leverage our GPUs.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '[**Flux**](https://fluxml.ai/Flux.jl/stable/)**:** Flux 是一个直观的 Julia 机器学习库；它旨在提供一个高级且用户友好的界面来构建和训练神经网络。我们将使用这个库作为示例，并利用
    Metal Backend 来利用我们的 GPU。'
- en: Below are the versions for the dependencies at the time of this article.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是本文发布时的依赖版本。
- en: '[PRE0]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: With our environment configured and a high-level understanding of the libraries
    in use, let’s explore the Metal API.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 配置好我们的环境并对所使用的库有了高级理解后，让我们探索 Metal API。
- en: LEVERAGING THE METAL API
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 利用 Metal API
- en: Metal.jl interfaces with Apple’s Metal Graphics API — a low-level API developed
    by Apple for their various platforms (macOS, iPhone, watch,…).
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: Metal.jl 与 Apple 的 Metal 图形 API 进行接口——这是 Apple 为其各种平台（macOS、iPhone、手表等）开发的低级
    API。
- en: This gives users direct control over the [GPU (Graphics Processing Unit)](https://en.wikipedia.org/wiki/Graphics_processing_unit)
    for tasks such as rendering graphics and parallel computations. Let’s look at
    a basic example.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 这使用户可以直接控制 [GPU (图形处理单元)](https://en.wikipedia.org/wiki/Graphics_processing_unit)，用于渲染图形和并行计算等任务。让我们看一个基本的示例。
- en: i. Kernel Functions
  id: totrans-44
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: i. 内核函数
- en: In the context of the Apple Metal framework, a **Kernel Function** is a special
    type of function that is executed on the GPU. These functions are written in a
    shading language; in our case, the [Metal Shading Language](https://developer.apple.com/metal/Metal-Shading-Language-Specification.pdf)
    (MSL).
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在Apple Metal框架的背景下，**内核函数**是一种在GPU上执行的特殊类型的函数。这些函数是用着色语言编写的；在我们的案例中是[Metal Shading
    Language](https://developer.apple.com/metal/Metal-Shading-Language-Specification.pdf)
    (MSL)。
- en: “MSL allows users to write a *shader program,* which is graphics and data-parallel
    compute code that runs on the GPU. Shader programs run on different programmable
    units of the GPU. MSL is a single, unified language that allows tighter integration
    between the graphics and compute programs.” ³
  id: totrans-46
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “MSL允许用户编写一个*shader程序*，这是一种在GPU上运行的图形和数据并行计算代码。Shader程序在GPU的不同可编程单元上运行。MSL是一个统一的语言，允许图形和计算程序之间有更紧密的集成。”
    ³
- en: 'Prior to getting started, let’s ensure we can monitor our GPU’s load. Apple
    has built-in GPU history. In the Activity Monitor app on your Mac, choose Window
    > GPU History. You should see something similar to mine:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始之前，让我们确保能够监控GPU的负载。Apple具有内置的GPU历史记录。在Mac上的活动监视器应用中，选择窗口 > GPU历史记录。你应该能看到类似我的画面：
- en: '![](../Images/046fd1e1d9cfc2121cbcc352daec59af.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/046fd1e1d9cfc2121cbcc352daec59af.png)'
- en: GPU History. Spikes to the top indicate max GPU usage | Image by Author
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: GPU历史记录。顶部的尖峰表示最大GPU使用率 | 图片来源：作者
- en: For this example, we will create a *matrix multiplication kernel*. To make use
    of the GPU, we will intentionally create compute complexity by iterating *N =
    1 million* times over the matrix operation.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们将创建一个*矩阵乘法内核*。为了利用GPU，我们将故意通过在矩阵操作上迭代*N = 100万*次来创建计算复杂性。
- en: We will define **A** as an *m x n* matrix, **B** an *n x p* matrix, and **C**
    as the resulting matrix product defined by *m x p.* **C=AB.** The resulting Kernel
    Function is shown below; other than a few tweaks to the Kernel Function and the
    Matrix variables defined at the bottom, the code is quite similar to what you
    would expect in Julia.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将定义**A**为一个*m x n*矩阵，**B**为一个*n x p*矩阵，**C**为由*m x p*定义的结果矩阵。**C=AB。** 结果的内核函数如下所示；除了对内核函数和底部定义的矩阵变量进行一些小的调整外，代码与你在Julia中期望的相当相似。
- en: 'The subtle differences, or additions, are as follows: The **thread_position_in_grid_1d()**
    function returns the index of the current thread within its one-dimensional grid.
    Essentially, it will know which data on each thread the GPU should operate. This
    fine-grained approach in controlling thread assignment gives the user the power
    to maximize the computational power of their system.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 微妙的差异或附加内容如下：**thread_position_in_grid_1d()**函数返回当前线程在其一维网格中的索引。基本上，它会知道GPU应该在每个线程上操作哪些数据。这种精细化的线程分配控制方法使用户能够最大限度地发挥系统的计算能力。
- en: 'When we initialize our matrices **A**, **B**, and **C** we want to ensure we
    are initializing values on the GPU using Metal; this ensures we are making it
    suitable for GPU-accelerated computations. In addition, the **storage=Shared**
    parameter indicates that the matrix data should be stored in [shared memory](https://developer.apple.com/documentation/metal/mtlstoragemode/mtlstoragemodeshared?language=objc),
    giving access to both CPU and GPU. By doing so, we need to ensure we synchronize
    prior to accessing that resource:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们初始化矩阵**A**、**B**和**C**时，我们希望确保在GPU上使用Metal初始化值；这确保我们使其适合GPU加速计算。此外，**storage=Shared**参数指示矩阵数据应存储在[共享内存](https://developer.apple.com/documentation/metal/mtlstoragemode/mtlstoragemodeshared?language=objc)中，从而使CPU和GPU都能访问。这样，我们需要确保在访问该资源之前进行同步：
- en: “Ensure that all changes you schedule on either the CPU or GPU for a resource
    that uses shared memory complete before accessing that resource on the other processor.”
    ²
  id: totrans-54
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “确保你在CPU或GPU上为使用共享内存的资源安排的所有更改在访问该资源的另一个处理器之前完成。” ²
- en: Last thing to mention is the `C_cpu` variable at the end. The `unsafe_wrap`
    function is used to create a CPU array, `C_cpu`, that shares the same underlying
    memory as the GPU array `C`. This enables us to transfer data between the CPU
    and GPU in order to perform computations later on. In the example below, I will
    show that we can modify the resulting matrix C by doing a inverse on it after
    all GPU operations have completed (*inv(C_cpu)).*
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 最后要提到的是末尾的`C_cpu`变量。`unsafe_wrap`函数用于创建一个与GPU数组`C`共享相同底层内存的CPU数组`C_cpu`。这使我们能够在CPU和GPU之间传输数据，以便后续进行计算。在下面的示例中，我将展示在所有GPU操作完成后，我们可以通过对其进行逆操作来修改结果矩阵C（*inv(C_cpu))。*
- en: Great! Now that we have our kernel ready, let’s move on to do the computation
    and benchmarking.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 太棒了！现在我们已经准备好了内核，接下来进行计算和基准测试吧。
- en: ii. Benchmarking
  id: totrans-57
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ii. 基准测试
- en: The **benchmark** macro (*macros start with “@”*) is used to measure the performance
    of the GPU kernel **matrix_multiplication_kernel.**
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '**benchmark**宏（*宏以“@”开头*）用于测量GPU内核**matrix_multiplication_kernel**的性能。'
- en: Prior to executing our `begin...end` block we will use the `Metal.@sync` to
    ensure synchronization between the CPU and GPU occurs and that all GPU commands
    have completed prior to moving onto the CPU code.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在执行我们的`begin...end`块之前，我们将使用`Metal.@sync`确保CPU和GPU之间的同步，并且所有GPU命令在切换到CPU代码之前已经完成。
- en: Within the **metal** macro, we specify both the number of `threads`and `groups`.
    Each thread is responsible for performing a computation. This number also determines
    how many operations occur in parallel. For instance, if we specify 256 threads
    then each thread will be responsible for a portion of the computation. Threads
    are organized in groups; a thread group is a collection of threads that can work
    together and coordinate the parallel execution of threads.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在**metal**宏中，我们指定了`threads`和`groups`的数量。每个线程负责执行计算。这个数量还决定了多少操作可以并行进行。例如，如果我们指定256个线程，则每个线程将负责一部分计算。线程被组织成组；一个线程组是可以一起工作并协调线程并行执行的线程集合。
- en: In total, the M1 GPU contains up to 128 [Execution units](https://en.wikipedia.org/wiki/Execution_unit)
    or 1024 ALUs,⁴ which Apple says can execute up to 24,576 threads simultaneously
    and which have a maximum floating point (FP32) performance of 2.6 [TFLOPs](https://en.wikipedia.org/wiki/TFLOPS).⁵
    ⁶
  id: totrans-61
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 总的来说，M1 GPU包含最多128个[执行单元](https://en.wikipedia.org/wiki/Execution_unit)或1024个ALU，⁴
    苹果表示这些执行单元可以同时执行最多24,576个线程，并且其最大浮点（FP32）性能为2.6 [TFLOPs](https://en.wikipedia.org/wiki/TFLOPS)。⁵
    ⁶
- en: Here are the results after playing around with the ***N (100)***, ***threads
    (256)***, and ***groups (256)*** parameters.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是对***N (100)***、***threads (256)***和***groups (256)***参数进行调整后的结果。
- en: '![](../Images/8aa87b1f71d7a814246f48f7a8e3ae48.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8aa87b1f71d7a814246f48f7a8e3ae48.png)'
- en: Benchmark Results | Image by Author
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 基准测试结果 | 作者提供的图片
- en: iii. Profiling
  id: totrans-65
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: iii. 性能分析
- en: '*In order to Profile and view the results you must have* [*XCode*](https://developer.apple.com/xcode/)
    *installed.*'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '*要进行性能分析并查看结果，您必须安装* [*XCode*](https://developer.apple.com/xcode/)。'
- en: Profiling code, an often overlooked discipline, is a crucial aspect of software
    development and performance optimization. Profiling involves measuring various
    aspects of a program’s execute. This can be things such as time taken by different
    functions, frequency of function calls, memory and usage or leaks. Profiling is
    also useful for ensuring any code changes made are quantified by means of performance.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 性能分析代码是一个常被忽视的学科，它是软件开发和性能优化的关键方面。性能分析涉及测量程序执行的各种方面。这可以包括不同函数所需的时间、函数调用的频率、内存使用或泄漏。性能分析还有助于确保对代码所做的任何更改通过性能量化。
- en: If you’ve experienced a scenario where code deployed to production unexpectedly
    causes a significant slowdown in system performance, only to discover later that
    a junior programmer inadvertently introduced a nested loop with quadratic time
    complexity O(n²), you’re not alone. While nested for-loops may seem acceptable
    in some situations the implications become increasingly problematic with a large
    number of values — leading to considerable performance challenges. **Catch this
    early, catch this with profiling!**
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你曾经历过代码部署到生产环境后意外导致系统性能显著下降，后来发现是因为初级程序员无意中引入了一个时间复杂度为O(n²)的嵌套循环，你并不孤单。虽然嵌套的for循环在某些情况下看似可以接受，但随着值的数量增加，其影响变得越来越麻烦，导致显著的性能挑战。**及早发现，借助性能分析来捕捉！**
- en: 'Profiling is done trivially with two steps. First, we need to specify the `Metal.@profile`
    macro in front of our `metal` macro. Next, ensure you have the following environment
    variable set: **ENV[“METAL_CAPTURE_ENABLED”] = 1**'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 性能分析可以通过两个步骤简单完成。首先，我们需要在`metal`宏前面指定`Metal.@profile`宏。接下来，确保你设置了以下环境变量：**ENV[“METAL_CAPTURE_ENABLED”]
    = 1**
- en: 'Now you can execute the following line:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你可以执行以下代码：
- en: '`Metal.@profile @metal threads=n_threads groups=n_groups matrix_multiplication_kernel(A,
    B, C)`'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '`Metal.@profile @metal threads=n_threads groups=n_groups matrix_multiplication_kernel(A,
    B, C)`'
- en: From there, we get a resulting **julia_capture_*N*.gputrace** stored in the
    same directory as the project. To interact with this, open it in XCode and replay
    the trace. We are presented with various useful metrics that we can dig into.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 从那里，我们获得一个**julia_capture_*N*.gputrace**文件，存储在与项目相同的目录中。要与之交互，请在XCode中打开它并重放跟踪。我们可以看到各种有用的指标，可以进一步挖掘。
- en: '![](../Images/679ccf8c1699e5581792249c64d55f66.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/679ccf8c1699e5581792249c64d55f66.png)'
- en: Xcode | Image by Author
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: Xcode | 作者提供的图片
- en: At this point, we have covered how to interact with Apple’s Metal Graphics API
    through Metal.jl — giving us direct GPU control for parallel computations. We
    have introduced Kernel Functions in the Metal Shading Language with a matrix multiplication
    kernel that intentionally increases GPU workloads. Additionally, we have introduced
    benchmarking tools and profiling capabilities through Metal’s macros.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 到此为止，我们已经介绍了如何通过Metal.jl与Apple的Metal图形API交互——这使我们能够直接控制GPU以进行并行计算。我们介绍了Metal着色语言中的内核函数，并使用了一个故意增加GPU工作负载的矩阵乘法内核。此外，我们通过Metal的宏介绍了基准测试工具和性能分析能力。
- en: '*Let’s move on to a practical scenario!*'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '*让我们进入一个实际的场景！*'
- en: WORKING WITH FLUX AND METAL BACKEND
  id: totrans-77
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用Flux和Metal后端
- en: i. Dataset overview
  id: totrans-78
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: i. 数据集概述
- en: We will be using a Breast Cancer database obtained from the University of Wisconsin
    Hospitals, Madison from Dr. William H. Wolberg.¹⁰ ¹¹ The ***Class*** feature will
    be the target where there are two possible values (2 for benign and 4 for malignant).
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用从威斯康星大学医院麦迪逊分院的威廉·H·沃尔伯格博士那里获得的乳腺癌数据库。¹⁰ ¹¹ ***Class***特征将作为目标，其中有两个可能的值（2表示良性，4表示恶性）。
- en: 'For brevity, I will not demonstrate the preprocessing required for the dataset,
    instead I will refer the reader to [Appendix I: The Julia source code](#e585).'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简洁起见，我将不会演示数据集所需的预处理，而是将读者参考到[附录I：Julia源代码](#e585)。
- en: ii. Simple neural network
  id: totrans-81
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ii. 简单的神经网络
- en: We will be building a simple neural network with [Flux](https://fluxml.ai/Flux.jl/stable/)
    that leverages the [Metal](https://developer.apple.com/documentation/metal) backend.
    Starting with v0.14, Flux doesn’t force a specific GPU backend and the corresponding
    package dependencies on the users.⁷ Although we are using Metal as the backend,
    Flux supports other [backends](https://fluxml.ai/Flux.jl/stable/gpu/) such as
    AMDGPU and CUDA.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用[Flux](https://fluxml.ai/Flux.jl/stable/)构建一个简单的神经网络，该网络利用了[Metal](https://developer.apple.com/documentation/metal)后端。从v0.14开始，Flux不强制用户使用特定的GPU后端和相应的软件包依赖。⁷
    尽管我们使用Metal作为后端，Flux也支持其他[后端](https://fluxml.ai/Flux.jl/stable/gpu/)，如AMDGPU和CUDA。
- en: Let’s do a quick sanity check to ensure our environment is setup.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们做一个快速的合理性检查，以确保我们的环境设置正确。
- en: '![](../Images/afda8bc58b7407e04fdf3adcdc2dd4d8.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/afda8bc58b7407e04fdf3adcdc2dd4d8.png)'
- en: Testing Metal with Flux | Image by Author
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Flux测试Metal | 作者提供的图片
- en: Now that we have the `device` variable we will be using that to move data and
    model to the GPU. **Be careful here as you can perform all business logic in Flux
    without ever exporting the model to the GPU — meaning, you’re still using the
    CPU (yikes)!**
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了`device`变量，我们将使用它将数据和模型移到GPU上。**在这里要小心，你可以在Flux中执行所有业务逻辑，而不需要将模型导出到GPU——这意味着你仍在使用CPU（哎呀）！**
- en: '![](../Images/4611a46eaa8be2e2c617e2a9deba28b0.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4611a46eaa8be2e2c617e2a9deba28b0.png)'
- en: Model Definition (This will be a Logistic Regression model for our 2-class problem)
    | Image by Author
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 模型定义（对于我们的二分类问题，这将是一个逻辑回归模型） | 作者提供的图片
- en: To prepare our data for the run, we will be leveraging [Flux.DataLoader](https://fluxml.ai/Flux.jl/previews/PR1786/data/dataloader/).
    This module handles iterations over mini-batches of data. I’ve keep it simple
    for this demo with a `batchsize=1` . This means that if my data contains 800 rows
    each batch is associated to a row. In a more efficient scenario you may want to
    split that up so you can group the data you are processing.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 为了准备我们的数据运行，我们将利用 [Flux.DataLoader](https://fluxml.ai/Flux.jl/previews/PR1786/data/dataloader/)。这个模块处理对小批量数据的迭代。为了演示，我保持简单，设置了`batchsize=1`。这意味着如果我的数据包含800行，每个批次与一行数据相关。在更高效的场景中，你可能希望将数据分开，以便将处理的数据进行分组。
- en: Before jumping into the next section, it is important to note that I faced issues
    passing my DataFrame directly to the DataLoader, so here are a few workarounds
    I had to implement.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 在跳到下一部分之前，重要的是要注意我在将 DataFrame 直接传递给 DataLoader 时遇到了一些问题，所以这里有一些我必须实现的解决方法。
- en: '![](../Images/515646d0995f178f0a87d144b469105c.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/515646d0995f178f0a87d144b469105c.png)'
- en: Data Prep | Image by Author
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 数据准备 | 图片来源：作者
- en: iii. Model evaluation
  id: totrans-93
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: iii. 模型评估
- en: The following code illustrates the model training and evaluation process. It
    is crucial to utilize the GPU when the `x_cpu` and `y_cpu` variables (our rows
    and labels) are returned from the [DataLoader](https://fluxml.ai/Flux.jl/previews/PR1786/data/dataloader/)
    — a failure to do so will crash due to compatibility issues as our model is expecting
    data on the GPU to work with.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码演示了模型训练和评估过程。当 `x_cpu` 和 `y_cpu` 变量（我们的行和标签）从 [DataLoader](https://fluxml.ai/Flux.jl/previews/PR1786/data/dataloader/)
    返回时，利用 GPU 是至关重要的——否则，由于兼容性问题，模型期望在 GPU 上的数据将导致崩溃。
- en: '![](../Images/716e15b34c3f867abfe2b083e390b83d.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/716e15b34c3f867abfe2b083e390b83d.png)'
- en: Demo of project running and leveraging M1-Max GPU! | Image by Author
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 项目运行演示及利用 M1-Max GPU！ | 图片来源：作者
- en: 'In this brief look into the Metal.jl Framework, we have covered the basics
    of GPU programming on Apple’s hardware. By exploring three core concepts — *Kernel
    Functions, Benchmarking, and Profiling* — the reader now has a high-level understanding
    of how to get started and is encouraged to dive deeper into the API’s capabilities.
    Additionally, we have demonstrated a practical example that leverages Flux to
    build a simple neural network application that leverages the [Metal backend](https://fluxml.ai/Flux.jl/stable/gpu/).
    If you’re a data scientist or ML engineer looking to tap into the power of your
    Apple M-series GPU, I hope this article has served as a starting point for accelerating
    your computational workflows. From here, I will leave the reader with a few more
    resources to check out:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在对 Metal.jl 框架的简要介绍中，我们涵盖了在 Apple 硬件上进行 GPU 编程的基础知识。通过探索三个核心概念——*内核函数、基准测试和性能分析*——读者现在对如何入门有了较高层次的理解，并被鼓励深入探索
    API 的功能。此外，我们还演示了一个实际示例，利用 Flux 构建了一个简单的神经网络应用程序，使用了 [Metal 后端](https://fluxml.ai/Flux.jl/stable/gpu/)。如果你是数据科学家或
    ML 工程师，想要利用 Apple M 系列 GPU 的强大功能，希望这篇文章能作为加速你的计算工作流的起点。从这里，我将留下更多资源供读者查看：
- en: '[](https://fluxml.ai/Flux.jl/stable/?source=post_page-----2db5fe8ee32c--------------------------------)
    [## Welcome · Flux'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://fluxml.ai/Flux.jl/stable/?source=post_page-----2db5fe8ee32c--------------------------------)
    [## 欢迎 · Flux'
- en: Flux is a library for machine learning. It comes "batteries-included" with many
    useful tools built in, but also lets…
  id: totrans-99
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Flux 是一个机器学习库。它包含许多有用的工具，但也允许…
- en: fluxml.ai](https://fluxml.ai/Flux.jl/stable/?source=post_page-----2db5fe8ee32c--------------------------------)
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: fluxml.ai](https://fluxml.ai/Flux.jl/stable/?source=post_page-----2db5fe8ee32c--------------------------------)
- en: '*I hope you enjoyed this article, thank you for reading!*'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '*希望你喜欢这篇文章，谢谢阅读！*'
- en: 👏
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 👏
- en: '**Lastly, I want to give a big shoutout to the creators of the Metal.jl project.
    I am looking forward to the continued success of the project. For anyone looking
    to contribute, please check out their Github page** [**here**](https://github.com/JuliaGPU/Metal.jl)**.**'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '**最后，我要大力感谢 Metal.jl 项目的创作者。我期待该项目继续成功。对于任何希望贡献的人，请查看他们的 Github 页面** [**这里**](https://github.com/JuliaGPU/Metal.jl)**。**'
- en: 'Appendix I: Julia Source Code'
  id: totrans-104
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 I：Julia 源代码
- en: Prior to running the source code found in the Github, I highly recommend matching
    the versions found in this article to ensure a successful compilation. Furthermore,
    the Metal.jl library is a work in progress and may lack or contains different
    features depending on the version specified.
  id: totrans-105
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 在运行Github上找到的源代码之前，我强烈建议匹配本文中找到的版本，以确保成功编译。此外，Metal.jl库仍在开发中，可能会根据指定的版本缺少或包含不同的功能。
- en: 🔗 [https://github.com/lausena/JuliaExperiments/tree/main](https://github.com/lausena/JuliaExperiments/tree/main)
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 🔗 [https://github.com/lausena/JuliaExperiments/tree/main](https://github.com/lausena/JuliaExperiments/tree/main)
- en: '*A supplement for preprocessing can be found in this* [*post*](https://medium.com/ai-mind-labs/practical-julia-logistic-regression-4e97a3cc3df8)*.*'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '*预处理的补充内容可以在此* [*帖子*](https://medium.com/ai-mind-labs/practical-julia-logistic-regression-4e97a3cc3df8)*中找到*。'
- en: '[](https://medium.com/subscribe/@lausena?source=post_page-----2db5fe8ee32c--------------------------------)
    [## Get an email whenever Gabriel Sena publishes.'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/subscribe/@lausena?source=post_page-----2db5fe8ee32c--------------------------------)
    [## 每当Gabriel Sena发布新内容时，你将会收到电子邮件。'
- en: Get an email whenever Gabriel Sena publishes. By signing up, you will create
    a Medium account if you don't already have…
  id: totrans-109
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 每当Gabriel Sena发布新内容时，你将会收到电子邮件。通过注册，如果你还没有Medium账户，将会创建一个…
- en: medium.com](https://medium.com/subscribe/@lausena?source=post_page-----2db5fe8ee32c--------------------------------)
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: medium.com](https://medium.com/subscribe/@lausena?source=post_page-----2db5fe8ee32c--------------------------------)
- en: '***References***'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '***参考文献***'
- en: '[1] [https://www.youtube.com/watch?v=IARikXzRU7s&ab_channel=TheJuliaProgrammingLanguage](https://www.youtube.com/watch?v=IARikXzRU7s&ab_channel=TheJuliaProgrammingLanguage)'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] [https://www.youtube.com/watch?v=IARikXzRU7s&ab_channel=TheJuliaProgrammingLanguage](https://www.youtube.com/watch?v=IARikXzRU7s&ab_channel=TheJuliaProgrammingLanguage)'
- en: '[2] [https://juliagpu.org/post/2022-06-24-metal/index.html](https://juliagpu.org/post/2022-06-24-metal/index.html)'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] [https://juliagpu.org/post/2022-06-24-metal/index.html](https://juliagpu.org/post/2022-06-24-metal/index.html)'
- en: '[3] [https://developer.apple.com/metal/Metal-Shading-Language-Specification.pdf](https://developer.apple.com/metal/Metal-Shading-Language-Specification.pdf)'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] [https://developer.apple.com/metal/Metal-Shading-Language-Specification.pdf](https://developer.apple.com/metal/Metal-Shading-Language-Specification.pdf)'
- en: '[4] Frumusanu, Andrei. [“The 2020 Mac Mini Unleashed: Putting Apple Silicon
    M1 To The Test”](https://www.anandtech.com/show/16252/mac-mini-apple-m1-tested).
    [*www.anandtech.com*.](http://www.anandtech.com.) [Archived](https://web.archive.org/web/20210201183558/https://www.anandtech.com/show/16252/mac-mini-apple-m1-tested)
    from the original on 2021–02–01\. Retrieved 2021–01–30.'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] Frumusanu, Andrei。 [“2020 Mac Mini大揭秘：测试Apple Silicon M1”](https://www.anandtech.com/show/16252/mac-mini-apple-m1-tested)。
    [*www.anandtech.com*](http://www.anandtech.com)。 [存档](https://web.archive.org/web/20210201183558/https://www.anandtech.com/show/16252/mac-mini-apple-m1-tested)
    自原始页面存档于2021年2月1日。检索日期为2021年1月30日。'
- en: '[5] [“Apple M1 Chip”](https://www.apple.com/mac/m1/). *Apple.com*. Apple. [Archived](https://web.archive.org/web/20201110184757/https://www.apple.com/mac/m1/)
    from the original on 10 November 2020\. Retrieved 11 November 2020.'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '[5] [“Apple M1芯片”](https://www.apple.com/mac/m1/)。 *Apple.com*。Apple。 [存档](https://web.archive.org/web/20201110184757/https://www.apple.com/mac/m1/)
    自原始页面存档于2020年11月10日。检索日期为2020年11月11日。'
- en: '[6] Kingsley-Hughes, Adrian (10 Nov 2020). [“Apple Silicon M1 chip: Here’s
    what we know”](https://www.zdnet.com/article/apple-silicon-m1-chip-heres-what-we-know/).
    *ZDnet*. Red Ventures. [Archived](https://web.archive.org/web/20210917094527/https://www.zdnet.com/article/apple-silicon-m1-chip-heres-what-we-know/)
    from the original on 17 September 2021\. Retrieved 1 July 2021.'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '[6] Kingsley-Hughes, Adrian (2020年11月10日)。 [“Apple Silicon M1芯片：我们知道什么”](https://www.zdnet.com/article/apple-silicon-m1-chip-heres-what-we-know/)。
    *ZDnet*。Red Ventures。 [存档](https://web.archive.org/web/20210917094527/https://www.zdnet.com/article/apple-silicon-m1-chip-heres-what-we-know/)
    自原始页面存档于2021年9月17日。检索日期为2021年7月1日。'
- en: '[7] [https://fluxml.ai/Flux.jl/stable/gpu/](https://fluxml.ai/Flux.jl/stable/gpu/)'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '[7] [https://fluxml.ai/Flux.jl/stable/gpu/](https://fluxml.ai/Flux.jl/stable/gpu/)'
- en: '[8] [https://juliagpu.org/post/2023-03-03-metal_0.2/](https://juliagpu.org/post/2023-03-03-metal_0.2/)'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '[8] [https://juliagpu.org/post/2023-03-03-metal_0.2/](https://juliagpu.org/post/2023-03-03-metal_0.2/)'
- en: '[9] [https://fluxml.ai/Flux.jl/stable/tutorials/logistic_regression/](https://fluxml.ai/Flux.jl/stable/tutorials/logistic_regression/)'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '[9] [https://fluxml.ai/Flux.jl/stable/tutorials/logistic_regression/](https://fluxml.ai/Flux.jl/stable/tutorials/logistic_regression/)'
- en: '[10] William H. Wolberg and O.L. Mangasarian: “Multisurface method of'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '[10] William H. Wolberg和O.L. Mangasarian: “用于医疗诊断的模式分离，应用于乳腺细胞学”。'
- en: pattern separation for medical diagnosis applied to breast cytology”,
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: “用于医疗诊断的模式分离，应用于乳腺细胞学”，
- en: Proceedings of the National Academy of Sciences, U.S.A., Volume 87,
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 《美国国家科学院院刊》，第87卷，
- en: December 1990, pp 9193–9196\.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 1990年12月，第9193–9196页。
- en: '[11] This dataset is licensed under a [Creative Commons Attribution 4.0 International](https://creativecommons.org/licenses/by/4.0/legalcode)
    (CC BY 4.0) license. [https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/breast-cancer-wisconsin.names](https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/breast-cancer-wisconsin.names)'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '[11] 该数据集遵循 [知识共享署名 4.0 国际](https://creativecommons.org/licenses/by/4.0/legalcode)
    (CC BY 4.0) 许可协议。 [https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/breast-cancer-wisconsin.names](https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/breast-cancer-wisconsin.names)'
