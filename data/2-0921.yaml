- en: Forward Pass & Backpropagation In Neural Networks
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 神经网络中的前向传播与反向传播
- en: 原文：[https://towardsdatascience.com/forward-pass-backpropagation-neural-networks-101-3a75996ada3b](https://towardsdatascience.com/forward-pass-backpropagation-neural-networks-101-3a75996ada3b)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/forward-pass-backpropagation-neural-networks-101-3a75996ada3b](https://towardsdatascience.com/forward-pass-backpropagation-neural-networks-101-3a75996ada3b)
- en: Explaining how neural networks “train” and “learn” patterns in data by hand
    and in code using PyTorch
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解释神经网络如何通过手动和代码中的 PyTorch 进行“训练”和“学习”数据模式
- en: '[](https://medium.com/@egorhowell?source=post_page-----3a75996ada3b--------------------------------)[![Egor
    Howell](../Images/1f796e828f1625440467d01dcc3e40cd.png)](https://medium.com/@egorhowell?source=post_page-----3a75996ada3b--------------------------------)[](https://towardsdatascience.com/?source=post_page-----3a75996ada3b--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----3a75996ada3b--------------------------------)
    [Egor Howell](https://medium.com/@egorhowell?source=post_page-----3a75996ada3b--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@egorhowell?source=post_page-----3a75996ada3b--------------------------------)[![Egor
    Howell](../Images/1f796e828f1625440467d01dcc3e40cd.png)](https://medium.com/@egorhowell?source=post_page-----3a75996ada3b--------------------------------)[](https://towardsdatascience.com/?source=post_page-----3a75996ada3b--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----3a75996ada3b--------------------------------)
    [Egor Howell](https://medium.com/@egorhowell?source=post_page-----3a75996ada3b--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----3a75996ada3b--------------------------------)
    ·10 min read·Nov 4, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----3a75996ada3b--------------------------------)
    ·10分钟阅读·2023年11月4日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/a894bd9e34501e8413c8bf96a918f1e5.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a894bd9e34501e8413c8bf96a918f1e5.png)'
- en: Neural network icons created by juicy_fish — Flaticon. [https://www.flaticon.com/free-icons/neural-network](https://www.flaticon.com/free-icons/neural-network).
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络图标由 juicy_fish 创建 — Flaticon. [https://www.flaticon.com/free-icons/neural-network](https://www.flaticon.com/free-icons/neural-network).
- en: Background
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 背景
- en: 'In my past two articles, we dived into the origins of the neural network from
    a single [***perceptron***](https://en.wikipedia.org/wiki/Perceptron)to a large
    interconnected ([***multi-layer perceptron***](https://en.wikipedia.org/wiki/Multilayer_perceptron)
    ***(MLP)***) non-linear optimisation engine. I highly recommend you check my previous
    posts if you are unfamiliar with the perceptron, MLP, and activation functions
    as we will discuss quite a bit in this article:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在我过去的两篇文章中，我们探讨了神经网络从单一的 [***感知机***](https://en.wikipedia.org/wiki/Perceptron)到大型互联的
    ([***多层感知机***](https://en.wikipedia.org/wiki/Multilayer_perceptron) ***(MLP)***
    ) 非线性优化引擎的起源。如果你对感知机、MLP 和激活函数不熟悉，我强烈建议你查看我之前的帖子，因为我们在这篇文章中将讨论很多：
- en: '[](https://levelup.gitconnected.com/intro-perceptron-architecture-neural-networks-101-2a487062810c?source=post_page-----3a75996ada3b--------------------------------)
    [## Intro, Perceptron & Architecture: Neural Networks 101'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://levelup.gitconnected.com/intro-perceptron-architecture-neural-networks-101-2a487062810c?source=post_page-----3a75996ada3b--------------------------------)
    [## 介绍，感知机与架构：神经网络 101'
- en: An introduction to Neural Networks and their building blocks
  id: totrans-11
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 神经网络及其构建块的介绍
- en: 'levelup.gitconnected.com](https://levelup.gitconnected.com/intro-perceptron-architecture-neural-networks-101-2a487062810c?source=post_page-----3a75996ada3b--------------------------------)
    [](/activation-functions-non-linearity-neural-networks-101-ab0036a2e701?source=post_page-----3a75996ada3b--------------------------------)
    [## Activation Functions & Non-Linearity: Neural Networks 101'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: levelup.gitconnected.com](https://levelup.gitconnected.com/intro-perceptron-architecture-neural-networks-101-2a487062810c?source=post_page-----3a75996ada3b--------------------------------)
    [](/activation-functions-non-linearity-neural-networks-101-ab0036a2e701?source=post_page-----3a75996ada3b--------------------------------)
    [## 激活函数与非线性：神经网络 101
- en: Explaining why neural networks can learn (nearly) anything and everything
  id: totrans-13
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 解释为什么神经网络可以学习（几乎）任何事物
- en: towardsdatascience.com](/activation-functions-non-linearity-neural-networks-101-ab0036a2e701?source=post_page-----3a75996ada3b--------------------------------)
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/activation-functions-non-linearity-neural-networks-101-ab0036a2e701?source=post_page-----3a75996ada3b--------------------------------)
- en: 'Now it’s time to understand how these neural networks get “trained” and “learn”
    the patterns in the data that you pass into it. There are two key components:
    [***forward pass***](https://theneuralblog.com/forward-pass-backpropagation-example/)
    and [***backpropagation***](https://en.wikipedia.org/wiki/Backpropagation). Let’s
    get into it!'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是时候理解这些神经网络如何“训练”和“学习”你传递给它的数据中的模式了。有两个关键组件：[***前向传播***](https://theneuralblog.com/forward-pass-backpropagation-example/)
    和 [***反向传播***](https://en.wikipedia.org/wiki/Backpropagation)。让我们深入了解吧！
- en: Architecture
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 架构
- en: 'Let’s quickly recap the general structure of a neural network:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们快速回顾一下神经网络的一般结构：
- en: '![](../Images/94cdac9897a5e852d9cd6edd425fcba0.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/94cdac9897a5e852d9cd6edd425fcba0.png)'
- en: A basic two-hidden multi-layer perceptron. Diagram by author.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 一个基本的两层隐藏的多层感知器。图示由作者提供。
- en: 'Where each hidden neuron is carrying out the following process:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 每个隐藏神经元执行以下过程：
- en: '![](../Images/57f8921c8f7e374d7f075b2f4ae32e6b.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/57f8921c8f7e374d7f075b2f4ae32e6b.png)'
- en: The process carried out inside each neuron. Diagram by author.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 每个神经元内部执行的过程。图示由作者提供。
- en: '***Inputs:*** *These are the features of our data.*'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***输入:*** *这些是我们数据的特征。*'
- en: '***Weights:*** *Some coefficients we multiply the inputs by. The goal of the
    algorithm is to find the most optimal weights.*'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***权重:*** *我们乘以输入的一些系数。算法的目标是找到最优的权重。*'
- en: '***Linear Weighted Sum:*** *Sum up the products of the inputs and weights and
    add a bias/offset term,* ***b****.*'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***线性加权和:*** *将输入和权重的乘积加起来，并添加一个偏置/偏移项，* ***b****。'
- en: '***Hidden Layer:*** *This is where the multiple neurons are stored to learn
    patterns in the data. The superscript refers to the layer and the subscript to
    the neuron/perceptron in that layer.*'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***隐藏层:*** *这是存储多个神经元以学习数据模式的地方。上标指的是层，下标指的是该层中的神经元/感知器。*'
- en: '***Edges/Arrows:*** *These are the weights for the network from the corresponding
    inputs, whether it be the features or the hidden layer outputs. I have omitted
    them for a cleaner plot.*'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***边缘/箭头:*** *这些是网络的权重，从相应的输入中获取，无论是特征还是隐藏层输出。我已经省略它们以使图表更简洁。*'
- en: '[***ReLU Activation Function***](https://en.wikipedia.org/wiki/Heaviside_step_function)***:***
    *The most popular* [***activation function***](https://en.wikipedia.org/wiki/Activation_function)
    *as it is computationally efficient and intuitive.*'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[***ReLU 激活函数***](https://en.wikipedia.org/wiki/Heaviside_step_function)***:***
    *最流行的* [***激活函数***](https://en.wikipedia.org/wiki/Activation_function) *因为它计算效率高且直观。*'
- en: Forward Pass
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 前向传播
- en: Overview
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 概述
- en: The first part of training a neural network is getting it to generate a prediction.
    This is called a **forward pass** and is where the data is traversed through all
    the neurons from the first to the last layer (also known as the output layer).
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 训练神经网络的第一部分是让它生成预测。这被称为 **前向传播**，即数据从第一层经过所有神经元到最后一层（也称为输出层）。
- en: For this article, we will do the forward pass by hand. In reality, we would
    use a package such as [***PyTorch***](https://pytorch.org/) or [***TensorFlow***](https://www.tensorflow.org/)***.***
    But this will give us a great intuition behind the process.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 对于本文，我们将手动进行前向传播。实际上，我们会使用像 [***PyTorch***](https://pytorch.org/) 或 [***TensorFlow***](https://www.tensorflow.org/)***
    的包。但这将帮助我们更好地理解过程。
- en: Example Neural Network
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 示例神经网络
- en: 'The neural network that we will perform the forward pass is shown below:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将执行前向传播的神经网络如下所示：
- en: '![](../Images/07e85418b0df7415133c2c8500b7e803.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/07e85418b0df7415133c2c8500b7e803.png)'
- en: Simple neural network. Diagram by author.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 简单的神经网络。图示由作者提供。
- en: As you can see, it is quite simple using two inputs, two neurons in the hidden
    layer (with a *ReLU* activation function), and a single prediction (output layer).
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，使用两个输入、隐藏层中的两个神经元（具有 *ReLU* 激活函数）和一个预测（输出层）非常简单。
- en: Weights & Biases
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 权重和偏置
- en: 'Let’s now create some arbitrary weights, biases, and inputs for this simple
    network:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们为这个简单的网络创建一些任意的权重、偏置和输入：
- en: '**Input values:** *[0.9, 1.0]*'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '**输入值:** *[0.9, 1.0]*'
- en: '**Target/True value:** *[2.0]*'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '**目标/真实值:** *[2.0]*'
- en: '**Input to hidden layer weights, *W_1:***'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '**输入到隐藏层的权重，*W_1:***'
- en: '*Neuron 1: W_{1,1} = [0.2, 0.3]*'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*神经元 1: W_{1,1} = [0.2, 0.3]*'
- en: '*Neuron 2: W_{1,2} = [0.4, 0.5]*'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*神经元 2: W_{1,2} = [0.4, 0.5]*'
- en: '**Hidden layer biases, *b_1*:** *[0.1, 0.2]*'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '**隐藏层偏置，*b_1*:** *[0.1, 0.2]*'
- en: '**Hidden to output layer weights,** ***W_2***: *[0.5, 0.6]*'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '**隐藏到输出层的权重，** ***W_2***: *[0.5, 0.6]*'
- en: '**Output layer bias, *b_2***: *[0.4]*'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '**输出层偏置，*b_2***: *[0.4]*'
- en: 'It’s important to note that I have randomly generated the initial weights and
    biases in this case. This is not a bad thing and you can achieve good results
    with a completely random initialization. However, there are more sophisticated
    methods:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的是，在这种情况下我随机生成了初始权重和偏置。这并不是坏事，你可以通过完全随机的初始化获得不错的结果。然而，还有更复杂的方法：
- en: '[**Xavier**](https://365datascience.com/tutorials/machine-learning-tutorials/what-is-xavier-initialization/#h_75113286374001686829810816):
    *Suitable for* [***sigmoid***](https://en.wikipedia.org/wiki/Sigmoid_function)
    *and* [***tanh***](https://en.wikipedia.org/wiki/Hyperbolic_functions) *activation
    functions. It generates random weights from a uniform distribution that utilizes
    the number of inputs to that node to set the distribution range.*'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**Xavier**](https://365datascience.com/tutorials/machine-learning-tutorials/what-is-xavier-initialization/#h_75113286374001686829810816):
    *适用于* [***sigmoid***](https://en.wikipedia.org/wiki/Sigmoid_function) *和* [***tanh***](https://en.wikipedia.org/wiki/Hyperbolic_functions)
    *激活函数。它从均匀分布中生成随机权重，利用该节点的输入数量来设置分布范围。*'
- en: '[**He**](https://machinelearningmastery.com/weight-initialization-for-deep-learning-neural-networks/):
    *Suitable for the ReLU activation function. It generates random weights from a
    normal distribution that utilizes the number of inputs to that node to set the
    standard deviation.*'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**He**](https://machinelearningmastery.com/weight-initialization-for-deep-learning-neural-networks/):
    *适用于 ReLU 激活函数。它从正态分布中生成随机权重，利用该节点的输入数量来设置标准差。*'
- en: If you want to learn more about these initialization methods, then check out
    the links tagged in the list above.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想了解更多关于这些初始化方法的信息，请查看上面列表中的链接。
- en: One last important note about weight initialization is to ensure they are different
    so we ‘[**break symmetry.**](/neural-network-breaking-the-symmetry-e04f963395dd)’
    If the neurons in the same layer start with the same weights, then they will most
    likely be updated exactly equally. Therefore, the network will not converge and
    fail to learn anything.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 关于权重初始化还有一个重要的注意事项是确保它们不同，以便我们‘[**打破对称性。**](/neural-network-breaking-the-symmetry-e04f963395dd)’
    如果同一层的神经元以相同的权重开始，它们很可能会被完全相同地更新。因此，网络将不会收敛，无法学习任何东西。
- en: First Forward Pass
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第一次前向传播
- en: 'Using the data listed above, we are now in a position to carry out our first
    forward pass! This looks like as follows:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 使用上面列出的数据，我们现在可以进行第一次前向传播！结果如下：
- en: '**The linear weighted sum, *z¹_{1}* and *z¹_{2},* from the inputs to the hidden
    layers is:**'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '**从输入到隐藏层的线性加权和，*z¹_{1}* 和 *z¹_{2}*，是：**'
- en: '![](../Images/adbe828addffb5cfa63bd36204d1787f.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/adbe828addffb5cfa63bd36204d1787f.png)'
- en: Equation generated by author in LaTeX.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 由作者在 LaTeX 中生成的方程。
- en: Remember the superscript denotes the layer and subscript the neuron in that
    layer. The first layer is considered to be the one straight after the input layer,
    this is because the input layer doesn’t do any calculations. So, in our case,
    we have a 2-layer network.
  id: totrans-58
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 记住，上标表示层，下标表示该层中的神经元。第一层被认为是紧接着输入层的那一层，因为输入层不进行任何计算。因此，在我们的情况下，我们有一个2层的网络。
- en: Here, we have utilized the [***dot product***](https://en.wikipedia.org/wiki/Dot_product)
    to simplify and condense the calculation. **Now, we can perform the ReLU activation
    function, *a¹_{1,1}* and *a¹_{1,2},* in the hidden layer:**
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们利用了 [***点积***](https://en.wikipedia.org/wiki/Dot_product) 来简化和压缩计算。 **现在，我们可以在隐藏层中执行
    ReLU 激活函数，*a¹_{1,1}* 和 *a¹_{1,2}*：**
- en: '![](../Images/90e70a103b4f3eba781e350a935ae01f.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/90e70a103b4f3eba781e350a935ae01f.png)'
- en: Equation generated by author in LaTeX.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 由作者在 LaTeX 中生成的方程。
- en: '**And the final bit we need to do is generate the output,** *z²_{1*}**, this
    doesn’t have an activation function associated with it:**'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '**我们需要做的最后一步是生成输出，** *z²_{1*}**，这没有关联的激活函数：**'
- en: '![](../Images/e78c0ef07eced4faaa31e59a9eb6501a.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e78c0ef07eced4faaa31e59a9eb6501a.png)'
- en: Equation generated by author in LaTeX.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 由作者在 LaTeX 中生成的方程。
- en: Voila, we have just done our first forward pass!
  id: totrans-65
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 哇，我们刚刚完成了第一次前向传播！
- en: 'Note: I have condensed the expression by writing the weights as a vector/matrix.
    This is how it’s frequently done in literature to make the workflow neater.'
  id: totrans-66
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 注意：我通过将权重写为向量/矩阵来简化表达。这是文献中常见的做法，以使工作流程更加整洁。
- en: 'The forward pass is also visualized below:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 前向传播也如下图所示：
- en: '![](../Images/72afd49009b5268e207f6ab02e502288.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/72afd49009b5268e207f6ab02e502288.png)'
- en: Simple neural network with its weights, biases and outputs. Diagram by author.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 简单神经网络及其权重、偏置和输出。图由作者提供。
- en: Backpropagation Algorithm
  id: totrans-70
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 反向传播算法
- en: Overview
  id: totrans-71
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 概述
- en: After doing our forward pass, we are now in a position to start updating our
    weights and biases to minimise the error of the prediction from the network. The
    way the weights and biases are updated is through the *backpropagation* algorithm.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 完成正向传播后，我们现在可以开始更新权重和偏置，以最小化网络预测的误差。权重和偏置的更新是通过 *反向传播* 算法实现的。
- en: Compute Graph, Chain Rule & Gradient Descent
  id: totrans-73
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 计算图、链式法则和梯度下降
- en: Let’s now gain some intuition behind this algorithm. Backpropagation aims to
    get the partial derivative of every weight and bias with respect to the error
    (loss). Then update each parameter using [***gradient descent***](/why-gradient-descent-is-so-common-in-data-science-def3e6515c5c),
    so that we minimise the error (loss) caused by each parameter.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们理解一下这个算法的直观意义。反向传播旨在获得每个权重和偏置对误差（损失）的偏导数。然后使用 [***梯度下降***](/why-gradient-descent-is-so-common-in-data-science-def3e6515c5c)
    更新每个参数，以最小化由每个参数引起的误差（损失）。
- en: Right, this may make sense on paper, but I appreciate it can still seem a bit
    arbitrary. I want to go through a really “simple” example using [***compute graphs***](https://www.tutorialspoint.com/python_deep_learning/python_deep_learning_computational_graphs.htm).
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 对，这在纸面上可能有意义，但我理解它仍然可能显得有些任意。我想通过一个真正的“简单”示例来深入了解 [***计算图***](https://www.tutorialspoint.com/python_deep_learning/python_deep_learning_computational_graphs.htm)。
- en: 'Consider the following function:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑以下函数：
- en: '![](../Images/f8cb287c88da22f545f8c2a619bdff87.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f8cb287c88da22f545f8c2a619bdff87.png)'
- en: Equation generated by author in LaTeX.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 由作者在 LaTeX 中生成的方程。
- en: 'We can plot it as a compute graph, which is just another way of visualising
    the calculation:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将其绘制为计算图，这只是另一种可视化计算的方法：
- en: '![](../Images/4047cf68f641963e1b2e1eeb0d893a1e.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4047cf68f641963e1b2e1eeb0d893a1e.png)'
- en: Compute graph example. Diagram by author.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 计算图示例。图示由作者提供。
- en: 'It’s basically a flowchart on how we calculate ***f(x,y,z).*** I have also
    expressed ***p=x-y***. Let’s now plug in some numbers:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 基本上这是一个计算 ***f(x,y,z)*** 的流程图。我也表达了 ***p=x-y***。现在我们来插入一些数字：
- en: '![](../Images/deb6bcaa4c35868c02d2abd5978aa53e.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/deb6bcaa4c35868c02d2abd5978aa53e.png)'
- en: Compute graph example with numbers. Diagram by author.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 带有数字的计算图示例。图示由作者提供。
- en: '*This all looks pretty good and intuitive so far!*'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '*这些看起来到目前为止都很好且直观！*'
- en: The way we calculate the minimum of ***f(x,y,z)*** is using calculus. Particularly,
    we need to know the partial derivative of ***f(x,y,z)*** with respect to all three
    of its variables ***x, y,*** and ***z***.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 我们计算 ***f(x,y,z)*** 最小值的方法是使用微积分。特别是，我们需要知道 ***f(x,y,z)*** 对其所有三个变量 ***x, y,***
    和 ***z*** 的偏导数。
- en: 'We can start by calculating the partial derivatives for ***p=x-y*** and ***f=pz***:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以开始计算 ***p=x-y*** 和 ***f=pz*** 的偏导数：
- en: '![](../Images/faa5838ed3085ddda4d76e6f939804bb.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/faa5838ed3085ddda4d76e6f939804bb.png)'
- en: Equation generated by author in LaTeX.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 由作者在 LaTeX 中生成的方程。
- en: '*But, how do we go about getting?*'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '*但是，我们该如何进行呢？*'
- en: '![](../Images/52687bd4c8162d292dd4b26310773380.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/52687bd4c8162d292dd4b26310773380.png)'
- en: Equation generated by author in LaTeX.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 由作者在 LaTeX 中生成的方程。
- en: 'Well, we use the[***chain rule!***](https://en.wikipedia.org/wiki/Chain_rule)This
    is an example for ***x***:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 好吧，我们使用 [***链式法则！***](https://en.wikipedia.org/wiki/Chain_rule) 这是一个 ***x***
    的示例：
- en: '![](../Images/95137bfc8b2891a2d09180e6b82b98d2.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/95137bfc8b2891a2d09180e6b82b98d2.png)'
- en: Equation generated by author in LaTeX.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 由作者在 LaTeX 中生成的方程。
- en: 'By combining different partial derivatives, we can get our desired expression.
    So, for the example above:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 通过组合不同的偏导数，我们可以得到我们期望的表达式。因此，对于上述示例：
- en: '![](../Images/7568e02dcb2b420c7e448156b669dda2.png)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7568e02dcb2b420c7e448156b669dda2.png)'
- en: Equation generated by author in LaTeX.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 由作者在 LaTeX 中生成的方程。
- en: The gradient of the output, ***f***, with respect to ***x*** is ***z***. This
    makes sense as ***z*** is the only value we multiply ***x.***
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 输出的梯度 ***f*** 对 ***x*** 的梯度是 ***z***。这很有意义，因为 ***z*** 是我们唯一乘以 ***x*** 的值。
- en: 'Repeating for ***y*** and ***z***:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 对 ***y*** 和 ***z*** 进行重复：
- en: '![](../Images/f56f6e7e20f83bf96d3cd44d9cbb159b.png)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f56f6e7e20f83bf96d3cd44d9cbb159b.png)'
- en: Equation generated by author in LaTeX.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 由作者在 LaTeX 中生成的方程。
- en: 'Now, we can write these gradients and their corresponding values on the compute
    graph:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以在计算图上写出这些梯度及其对应的值：
- en: '![](../Images/578b82e9ed62175c510b6eada2f180ae.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/578b82e9ed62175c510b6eada2f180ae.png)'
- en: Compute graph example with numbers and gradients. Diagram by author.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 带有数字和梯度的计算图示例。图示由作者提供。
- en: Gradient descent works by updating the values (***x,y,z***) by a small amount
    in the opposite direction of the gradient. The goal of gradient descent is to
    try and minimise the output function. For example, for ***x:***
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度下降通过在梯度的相反方向上小幅度更新值（***x, y, z***）来工作。梯度下降的目标是尽量减少输出函数。例如，对于 ***x:***
- en: '![](../Images/5866f8f21dd278964bc6c1743a9eeed7.png)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5866f8f21dd278964bc6c1743a9eeed7.png)'
- en: Equation generated by author in LaTeX.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 作者生成的 LaTeX 方程式。
- en: Where ***h*** is called the [***learning rate***](https://deepchecks.com/glossary/learning-rate-in-machine-learning/#:~:text=The%20learning%20rate%2C%20denoted%20by,network%20concerning%20the%20loss%20gradient%3E.)
    and decides how much the parameter will get updated. For this case, let’s define
    ***h=0.1***, so ***x=3.7***. What is the output now?
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 ***h*** 称为 [***学习率***](https://deepchecks.com/glossary/learning-rate-in-machine-learning/#:~:text=The%20learning%20rate%2C%20denoted%20by,network%20concerning%20the%20loss%20gradient%3E.)，决定了参数更新的幅度。在这个例子中，让我们定义
    ***h=0.1***，所以 ***x=3.7***。现在的输出是什么？
- en: '![](../Images/95a57e60032255539a1cbb03d242367b.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/95a57e60032255539a1cbb03d242367b.png)'
- en: Compute graph example with numbers and gradients after performing gradient descent.
    Diagram by author.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 执行梯度下降后的计算图示例，包含数字和梯度。图示由作者提供。
- en: The output got smaller, in other words, it’s getting minimised!
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 输出变小了，换句话说，它正在被最小化！
- en: This example was inspired by [Andrej Karpathy](https://www.youtube.com/@andrejkarpathy4906)
    [video on YouTube](https://www.youtube.com/watch?v=i94OvYb6noo&t=358s). I highly
    recommend you check it out to get a thorough understanding of this process.
  id: totrans-113
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 这个例子灵感来源于 [Andrej Karpathy](https://www.youtube.com/@andrejkarpathy4906) 在 [YouTube](https://www.youtube.com/watch?v=i94OvYb6noo&t=358s)
    上的视频。我强烈推荐你查看，以深入了解这一过程。
- en: Applying Backpropagation
  id: totrans-114
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 应用反向传播
- en: '*How does all that stuff above link into training neural networks?*'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '*以上所有内容如何与神经网络的训练相关？*'
- en: Well, a neural network is just a compute graph! The inputs are analogous to
    ***x, y***, and ***z*** above and the operations gates are analogous to the activation
    functions inside the neurons.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 好了，神经网络其实就是一个计算图！输入类似于上面的 ***x, y*** 和 ***z***，而操作门类似于神经元中的激活函数。
- en: 'Let’s now apply this process to the simple example neural network we had above.
    Remember, our prediction was ***1.326,*** and let’s say the target is ***2.0***.
    The loss function (error) in this prediction could be the [***mean squared error***](https://en.wikipedia.org/wiki/Mean_squared_error):'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们将这一过程应用于我们之前的简单示例神经网络。记住，我们的预测是 ***1.326***，假设目标是 ***2.0***。这个预测中的损失函数（误差）可以是
    [***均方误差***](https://en.wikipedia.org/wiki/Mean_squared_error)：
- en: '![](../Images/5ad5582ee7f5a12428e564dfcdc0e90b.png)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5ad5582ee7f5a12428e564dfcdc0e90b.png)'
- en: Equation generated by author in LaTeX.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 作者生成的 LaTeX 方程式。
- en: Remember the 2 attached to z is the layer number, it's not a power term!
  id: totrans-120
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 记住附加在 z 上的 2 是层数，而不是幂次项！
- en: 'The next step is to calculate the gradient of the loss with respect to the
    prediction/output:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是计算相对于预测/输出的损失梯度：
- en: '![](../Images/ae7e2e12dc0717c6c9301f931e3ed3b7.png)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ae7e2e12dc0717c6c9301f931e3ed3b7.png)'
- en: Equation generated by author in LaTeX.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 作者生成的 LaTeX 方程式。
- en: 'Now, we need to calculate the gradient of the loss with respect to the output
    layers’ bias and weights: ***W_2*** and ***b_2:***'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们需要计算相对于输出层偏置和权重的损失梯度：***W_2*** 和 ***b_2:***
- en: '![](../Images/02746a72459aeb1efc366008137b9b49.png)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/02746a72459aeb1efc366008137b9b49.png)'
- en: Equation generated by author in LaTeX.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 作者生成的 LaTeX 方程式。
- en: Don’t worry if the above expressions seem scary. All we have done is partially
    differentiated and applied the chain rule several times.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 如果上述表达式看起来让你感到害怕，不必担心。我们所做的只是进行了偏导数计算，并多次应用了链式法则。
- en: '![](../Images/ab1122a7115e3527d82642c02ee69d5a.png)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ab1122a7115e3527d82642c02ee69d5a.png)'
- en: Equation generated by author in LaTeX.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 作者生成的 LaTeX 方程式。
- en: 'The last step is to update the parameters using gradient descent, where I have
    set the learning rate, ***h=0.1***:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一步是使用梯度下降更新参数，其中我设置了学习率 ***h=0.1***：
- en: '![](../Images/99bebba4589568a8cea2d113f6dbea10.png)'
  id: totrans-131
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/99bebba4589568a8cea2d113f6dbea10.png)'
- en: Equation generated by author in LaTeX.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 作者生成的 LaTeX 方程式。
- en: Voila, we have updated the output layers’ weights and bias!
  id: totrans-133
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: Voilà，我们已经更新了输出层的权重和偏置！
- en: 'The next thing is to replicate this process for the hidden layers’ weights
    and biases. To start, we need to find the loss with respect to the activation
    function outputs, ***a¹_{1,1}* and *a¹_{1,2}***:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来是对隐藏层的权重和偏置重复这一过程。首先，我们需要找到相对于激活函数输出的损失，***a¹_{1,1}* 和 *a¹_{1,2}***：
- en: '![](../Images/1e00c4f35f203e86ec6f6eee5487b668.png)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1e00c4f35f203e86ec6f6eee5487b668.png)'
- en: Equation generated by author in LaTeX.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 由作者在 LaTeX 中生成的方程。
- en: Now, we find the derivatives for **z*¹_{1}* and z*¹_{2}*:**
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们找到 **z*¹_{1}* 和 z*¹_{2}* 的导数：**
- en: '![](../Images/cb7bdd6f67b656c6e9762ea7dda02a5a.png)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/cb7bdd6f67b656c6e9762ea7dda02a5a.png)'
- en: Equation generated by author in LaTeX.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 由作者在 LaTeX 中生成的方程。
- en: 'For some context:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 一些背景信息：
- en: '![](../Images/8f36e173b6bc3c4622c411947510c85e.png)'
  id: totrans-141
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8f36e173b6bc3c4622c411947510c85e.png)'
- en: Equation generated by author in LaTeX.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 由作者在 LaTeX 中生成的方程。
- en: 'The next bit is finding the loss with respect to ***W_{1,1}***and ***W_{1,2}.***
    Before that, I want to redefine some notation just to make it clear which weight
    we are working with:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是找到相对于 ***W_{1,1}*** 和 ***W_{1,2}*** 的损失。在此之前，我想重新定义一些符号，以便清楚我们在处理哪个权重：
- en: '![](../Images/6289f16900a84c4fed7ebcbe97d64af5.png)'
  id: totrans-144
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6289f16900a84c4fed7ebcbe97d64af5.png)'
- en: Equation generated by author in LaTeX.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 由作者在 LaTeX 中生成的方程。
- en: 'Therefore:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 因此：
- en: '![](../Images/80cbe028c9930838a828747967d7f811.png)'
  id: totrans-147
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/80cbe028c9930838a828747967d7f811.png)'
- en: Equation generated by author in LaTeX.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 由作者在 LaTeX 中生成的方程。
- en: 'And so, the partial derivative of the loss with respect to the weights and
    biases are:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，相对于权重和偏置的损失的偏导数是：
- en: '![](../Images/9b9cfe0f15de03a04cd2da7ce0ed431d.png)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9b9cfe0f15de03a04cd2da7ce0ed431d.png)'
- en: Equation generated by author in LaTeX.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 由作者在 LaTeX 中生成的方程。
- en: 'Finally, the last step is to update these values using gradient descent:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一步是使用梯度下降来更新这些值：
- en: '![](../Images/dd22b10d72a82642a99a80a0cfce5973.png)'
  id: totrans-153
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/dd22b10d72a82642a99a80a0cfce5973.png)'
- en: Equation generated by author in LaTeX.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 由作者在 LaTeX 中生成的方程。
- en: WOW. We have just together completed one whole iteration of backpropagation!
  id: totrans-155
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 哇，我们刚刚完成了整个反向传播的迭代！
- en: One cycle of a forward pass and backward pass (backpropagation) is known as
    an [**epoch**](https://www.baeldung.com/cs/epoch-neural-networks).
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 一个前向传播和反向传播（反向传播）的周期被称为 [**epoch**](https://www.baeldung.com/cs/epoch-neural-networks)。
- en: The next step would be to carry out another forward pass using the updated weights
    and biases. The result of this forward pass with the new weights and biases would
    be **1.618296**. This is closer to the target value of **2**, so the network has
    ‘learned’ better weights and biases. This is ‘machine learning’ in action. Truly
    amazing!
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是使用更新后的权重和偏置进行另一次前向传播。这个前向传播的结果将是 **1.618296**。这接近目标值 **2**，因此网络已经“学习”到了更好的权重和偏置。这就是“机器学习”在发挥作用。真的很惊人！
- en: In practice, this algorithm would run for 100 or 1000s of epochs. Luckily, there
    exist packages for this process, as doing it by hand would be very tedious!
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，这个算法会运行 100 或 1000 次 epoch。幸运的是，已经存在处理此过程的包，因为手动完成会非常繁琐！
- en: Extra Detail
  id: totrans-159
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 额外细节
- en: One can see why this process is called backpropagation as we are propagating
    the error (derivatives) backward at each layer of the network. Once you get the
    hang of it, it’s pretty simple to understand. So, I highly recommend you go through
    this process slowly, ideally by hand yourself and I promise it will just click!
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 可以看出，为什么这个过程被称为反向传播，因为我们在网络的每一层向后传播误差（导数）。一旦你掌握了它，理解起来就很简单。因此，我强烈建议你慢慢地进行这个过程，最好自己动手，我保证你会很快理解！
- en: PyTorch Example
  id: totrans-161
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: PyTorch 示例
- en: 'Doing by hand, as we did above, is quite exhaustive and this was for a tiny
    network. However, we can leverage PyTorch (a deep learning library) to do all
    this heavy lifting for us:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 手动完成，如上所述，确实很耗费精力，而且这只是一个小网络。然而，我们可以利用 PyTorch（一个深度学习库）来为我们完成所有这些繁重的工作：
- en: GitHub Gist by author.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: GitHub Gist 由作者提供。
- en: So, the whole forward pass and backpropagation process that we wrote out by
    hand can be done in ~50 lines of code! Power of Python!
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们手动编写的整个前向传播和反向传播过程可以在 ~50 行代码中完成！Python 的威力！
- en: However, due to randomization, floating point precision, and other computer
    factors the weights, biases, and predictions may not match exactly to the hand
    calculations. This is not a problem, but it’s important to be aware of it.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，由于随机化、浮点精度和其他计算机因素，权重、偏置和预测可能与手动计算的结果不完全匹配。这不是问题，但重要的是要意识到这一点。
- en: If you want to learn more about PyTorch, check out the [tutorials on their site](https://pytorch.org/tutorials/).
  id: totrans-166
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 如果你想了解更多关于 PyTorch 的内容，可以查看他们网站上的 [教程](https://pytorch.org/tutorials/)。
- en: Summary & Further Thoughts
  id: totrans-167
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结与进一步思考
- en: In this article, we went over how neural networks generate predictions and learn
    from their errors. The process revolves around updating the network’s parameter
    using partial differential against the loss error. The algorithm is backpropagation
    as it propagates the error backward through each layer using the chain rule. Backpropagation
    is quite intuitive once you gain the hang of it, but very tedious particularly
    for large neural networks. This is where we use deep learning libraries such as
    PyTorch that do most of the heavy lifting for us.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们探讨了神经网络如何生成预测并从错误中学习。这个过程围绕使用部分微分更新网络的参数，针对损失误差。算法是反向传播，因为它通过链式法则将误差反向传播通过每一层。一旦你掌握了它，反向传播相当直观，但对于大型神经网络来说非常繁琐。这就是我们使用像
    PyTorch 这样的深度学习库，它为我们做了大部分繁重的工作。
- en: 'The full code used in this article is available at my GitHub here:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 本文中使用的完整代码可以在我的 GitHub 上找到：
- en: '[](https://github.com/egorhowell/Medium-Articles/blob/main/Neural%20Networks/basic_foward_backward_pass.py?source=post_page-----3a75996ada3b--------------------------------)
    [## Medium-Articles/Neural Networks/basic_foward_backward_pass.py at main · egorhowell/Medium-Articles'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://github.com/egorhowell/Medium-Articles/blob/main/Neural%20Networks/basic_foward_backward_pass.py?source=post_page-----3a75996ada3b--------------------------------)
    [## Medium-Articles/Neural Networks/basic_foward_backward_pass.py 在主分支 · egorhowell/Medium-Articles'
- en: Code I use in my medium blog/articles. Contribute to egorhowell/Medium-Articles
    development by creating an account on…
  id: totrans-171
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 我在我的中等博客/文章中使用的代码。通过在…上创建帐户来贡献 egorhowell/Medium-Articles 的开发。
- en: github.com](https://github.com/egorhowell/Medium-Articles/blob/main/Neural%20Networks/basic_foward_backward_pass.py?source=post_page-----3a75996ada3b--------------------------------)
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: github.com](https://github.com/egorhowell/Medium-Articles/blob/main/Neural%20Networks/basic_foward_backward_pass.py?source=post_page-----3a75996ada3b--------------------------------)
- en: References & Further Reading
  id: totrans-173
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考资料与进一步阅读
- en: '[*Andrej Karpathy Neural Network Course*](https://www.youtube.com/watch?v=i94OvYb6noo)'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*Andrej Karpathy 神经网络课程*](https://www.youtube.com/watch?v=i94OvYb6noo)'
- en: '[*PyTorch site*](https://pytorch.org/)'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*PyTorch网站*](https://pytorch.org/)'
- en: '[*Another example of training a neural network by hand*](/training-a-neural-network-by-hand-1bcac4d82a6e)'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*另一个手动训练神经网络的例子*](/training-a-neural-network-by-hand-1bcac4d82a6e)'
- en: Another Thing!
  id: totrans-177
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 另一件事！
- en: I have a free newsletter, [**Dishing the Data**](https://dishingthedata.substack.com/),
    where I share weekly tips for becoming a better Data Scientist. There is no “fluff”
    or “clickbait,” just pure actionable insights from a practicing Data Scientist.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 我有一个免费的新闻通讯，[**数据解析**](https://dishingthedata.substack.com/)，在其中我分享成为更优秀数据科学家的每周技巧。没有“虚华”或“点击诱饵”，只有来自实践数据科学家的纯粹可操作的见解。
- en: '[](https://newsletter.egorhowell.com/?source=post_page-----3a75996ada3b--------------------------------)
    [## Dishing The Data | Egor Howell | Substack'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://newsletter.egorhowell.com/?source=post_page-----3a75996ada3b--------------------------------)
    [## 数据解析 | Egor Howell | Substack'
- en: How To Become A Better Data Scientist. Click to read Dishing The Data, by Egor
    Howell, a Substack publication with…
  id: totrans-180
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 如何成为更好的数据科学家。点击阅读《数据解析》，由 Egor Howell 撰写，这是一个 Substack 出版物，内容包括…
- en: newsletter.egorhowell.com](https://newsletter.egorhowell.com/?source=post_page-----3a75996ada3b--------------------------------)
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: newsletter.egorhowell.com](https://newsletter.egorhowell.com/?source=post_page-----3a75996ada3b--------------------------------)
- en: Connect With Me!
  id: totrans-182
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 与我联系！
- en: '[**YouTube**](https://www.youtube.com/@egorhowell)'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**YouTube**](https://www.youtube.com/@egorhowell)'
- en: '[**LinkedIn**](https://www.linkedin.com/in/egor-howell-092a721b3/)'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**LinkedIn**](https://www.linkedin.com/in/egor-howell-092a721b3/)'
- en: '[**Twitter**](https://twitter.com/EgorHowell)'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**Twitter**](https://twitter.com/EgorHowell)'
- en: '[**GitHub**](https://github.com/egorhowell)'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**GitHub**](https://github.com/egorhowell)'
