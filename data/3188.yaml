- en: 'Falcon: The Pinnacle of Open-Source LLMs'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 'Falcon: 开源LLMs的巅峰'
- en: 原文：[https://towardsdatascience.com/falcon-the-pinnacle-of-open-source-llms-600de69c333c?source=collection_archive---------1-----------------------#2023-10-24](https://towardsdatascience.com/falcon-the-pinnacle-of-open-source-llms-600de69c333c?source=collection_archive---------1-----------------------#2023-10-24)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/falcon-the-pinnacle-of-open-source-llms-600de69c333c?source=collection_archive---------1-----------------------#2023-10-24](https://towardsdatascience.com/falcon-the-pinnacle-of-open-source-llms-600de69c333c?source=collection_archive---------1-----------------------#2023-10-24)
- en: The gap between open-source and proprietary LLMs continues to shrink…
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 开源LLMs与专有LLMs之间的差距持续缩小……
- en: '[](https://wolfecameron.medium.com/?source=post_page-----600de69c333c--------------------------------)[![Cameron
    R. Wolfe, Ph.D.](../Images/52bb88d7cf1105501be2fae5ccbe7a03.png)](https://wolfecameron.medium.com/?source=post_page-----600de69c333c--------------------------------)[](https://towardsdatascience.com/?source=post_page-----600de69c333c--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----600de69c333c--------------------------------)
    [Cameron R. Wolfe, Ph.D.](https://wolfecameron.medium.com/?source=post_page-----600de69c333c--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://wolfecameron.medium.com/?source=post_page-----600de69c333c--------------------------------)[![Cameron
    R. Wolfe, Ph.D.](../Images/52bb88d7cf1105501be2fae5ccbe7a03.png)](https://wolfecameron.medium.com/?source=post_page-----600de69c333c--------------------------------)[](https://towardsdatascience.com/?source=post_page-----600de69c333c--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----600de69c333c--------------------------------)
    [Cameron R. Wolfe, Ph.D.](https://wolfecameron.medium.com/?source=post_page-----600de69c333c--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F28aa6026c553&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffalcon-the-pinnacle-of-open-source-llms-600de69c333c&user=Cameron+R.+Wolfe%2C+Ph.D.&userId=28aa6026c553&source=post_page-28aa6026c553----600de69c333c---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----600de69c333c--------------------------------)
    ·14 min read·Oct 24, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F600de69c333c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffalcon-the-pinnacle-of-open-source-llms-600de69c333c&user=Cameron+R.+Wolfe%2C+Ph.D.&userId=28aa6026c553&source=-----600de69c333c---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F28aa6026c553&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffalcon-the-pinnacle-of-open-source-llms-600de69c333c&user=Cameron+R.+Wolfe%2C+Ph.D.&userId=28aa6026c553&source=post_page-28aa6026c553----600de69c333c---------------------post_header-----------)
    发表在[Towards Data Science](https://towardsdatascience.com/?source=post_page-----600de69c333c--------------------------------)
    ·14 min read·2023年10月24日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F600de69c333c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffalcon-the-pinnacle-of-open-source-llms-600de69c333c&user=Cameron+R.+Wolfe%2C+Ph.D.&userId=28aa6026c553&source=-----600de69c333c---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F600de69c333c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffalcon-the-pinnacle-of-open-source-llms-600de69c333c&source=-----600de69c333c---------------------bookmark_footer-----------)![](../Images/4a5506c6e87e01bb8145fcb5b5d29b13.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F600de69c333c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffalcon-the-pinnacle-of-open-source-llms-600de69c333c&source=-----600de69c333c---------------------bookmark_footer-----------)![](../Images/4a5506c6e87e01bb8145fcb5b5d29b13.png)'
- en: (Photo by [Alan Mersom](https://unsplash.com/@merse?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)
    on [Unsplash](https://unsplash.com/photos/aerial-photography-of-bird-NzkjR1pw0Lc?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash))
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: （图片由[艾伦·梅尔索姆](https://unsplash.com/@merse?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)拍摄，发布在[Unsplash](https://unsplash.com/photos/aerial-photography-of-bird-NzkjR1pw0Lc?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)）
- en: 'Recent research in open-source large language models (LLMs) has mostly focused
    upon two areas: imitation learning and pre-training open-source base models. Though
    both approaches are viable, the creation of high-quality, open-source base models
    is especially enticing, as these models can be further fine-tuned (at a lower
    cost) and used in a variety of different downstream applications. Initial attempts
    at creating these models failed. Although later models (e.g., LLaMA and MPT-7B)
    perform much better, these models have struggled to match the quality of their
    proprietary counterparts (e.g., GPT-3.5 or GPT-4) until recently.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 最近对开源大型语言模型（LLMs）的研究主要集中在两个领域：模仿学习和预训练开源基础模型。虽然这两种方法都很可行，但创建高质量的开源基础模型尤为诱人，因为这些模型可以在更低的成本下进行进一步微调，并用于各种不同的下游应用。最初创建这些模型的尝试失败了。尽管后来的模型（例如LLaMA和MPT-7B）表现得更好，但这些模型直到最近才在质量上与它们的专有对手（例如GPT-3.5或GPT-4）相匹敌。
- en: With the release of the Falcon-7B and Falcon-40B LLMs [1], we see — *for the
    first time* — open-source base LLMs that begin to rival the quality of the most
    popular paid models. Trained over a massive textual corpus obtained via a novel
    data pipeline, these models achieve (by a decent margin) new state-of-the-art
    performance among open-source LLMs and are free to use in commercial applications.
    To make things better, the Falcon models adopt several modifications to their
    underlying transformer architecture that significantly accelerate inference and
    can even improve the efficiency of pre-training.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 随着Falcon-7B和Falcon-40B LLMs的发布[1]，我们首次看到——*开源基础LLMs*开始在质量上与最受欢迎的付费模型相抗衡。通过一种新颖的数据管道训练于大量文本语料库，这些模型在开源LLMs中达到了（大幅度超越）新的最先进性能，并且可以在商业应用中免费使用。更好的是，Falcon模型采用了对其基础变压器架构的若干修改，这些修改显著加快了推理速度，甚至可以提高预训练的效率。
