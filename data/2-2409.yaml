- en: 'XGBoost: The Definitive Guide (Part 2)'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: XGBoost：权威指南（第二部分）
- en: 原文：[https://towardsdatascience.com/xgboost-the-definitive-guide-part-2-c38ef02f74d0](https://towardsdatascience.com/xgboost-the-definitive-guide-part-2-c38ef02f74d0)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/xgboost-the-definitive-guide-part-2-c38ef02f74d0](https://towardsdatascience.com/xgboost-the-definitive-guide-part-2-c38ef02f74d0)
- en: Implementation of the XGBoost algorithm in Python from scratch
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从头开始用 Python 实现 XGBoost 算法
- en: '[](https://medium.com/@roiyeho?source=post_page-----c38ef02f74d0--------------------------------)[![Dr.
    Roi Yehoshua](../Images/905a512ffc8879069403a87dbcbeb4db.png)](https://medium.com/@roiyeho?source=post_page-----c38ef02f74d0--------------------------------)[](https://towardsdatascience.com/?source=post_page-----c38ef02f74d0--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----c38ef02f74d0--------------------------------)
    [Dr. Roi Yehoshua](https://medium.com/@roiyeho?source=post_page-----c38ef02f74d0--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@roiyeho?source=post_page-----c38ef02f74d0--------------------------------)[![Roi
    Yehoshua 博士](../Images/905a512ffc8879069403a87dbcbeb4db.png)](https://medium.com/@roiyeho?source=post_page-----c38ef02f74d0--------------------------------)[](https://towardsdatascience.com/?source=post_page-----c38ef02f74d0--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----c38ef02f74d0--------------------------------)
    [Roi Yehoshua 博士](https://medium.com/@roiyeho?source=post_page-----c38ef02f74d0--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----c38ef02f74d0--------------------------------)
    ·14 min read·Aug 15, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----c38ef02f74d0--------------------------------)
    ·阅读时间 14 分钟 ·2023年8月15日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/798f47ae83cd1fe10d15404d5b3ca636.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/798f47ae83cd1fe10d15404d5b3ca636.png)'
- en: Image by [StockSnap](https://pixabay.com/users/stocksnap-894430/?utm_source=link-attribution&utm_medium=referral&utm_campaign=image&utm_content=2557468)
    from [Pixabay](https://pixabay.com//?utm_source=link-attribution&utm_medium=referral&utm_campaign=image&utm_content=2557468)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由 [StockSnap](https://pixabay.com/users/stocksnap-894430/?utm_source=link-attribution&utm_medium=referral&utm_campaign=image&utm_content=2557468)
    提供，来自 [Pixabay](https://pixabay.com//?utm_source=link-attribution&utm_medium=referral&utm_campaign=image&utm_content=2557468)
- en: In the [previous article](https://medium.com/towards-data-science/xgboost-the-definitive-guide-part-1-cc24d2dcd87a)
    we discussed the XGBoost algorithm and showed its implementation in pseudocode.
    In this article we are going to implement the algorithm in Python from scratch.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [上一篇文章](https://medium.com/towards-data-science/xgboost-the-definitive-guide-part-1-cc24d2dcd87a)
    中，我们讨论了 XGBoost 算法并展示了其伪代码实现。在这篇文章中，我们将从头开始用 Python 实现该算法。
- en: 'The provided code is a concise and lightweight implementation of the XGBoost
    algorithm (with only about 300 lines of code), intended to demonstrate its core
    functionality. As such, it is not optimized for speed or memory usage, and does
    not include the full spectrum of options provided by the XGBoost library (see
    [https://xgboost.readthedocs.io/](https://xgboost.readthedocs.io/) for more details
    on the features of the library). More specifically:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 提供的代码是 XGBoost 算法的简洁轻量实现（仅约 300 行代码），旨在演示其核心功能。因此，它没有针对速度或内存使用进行优化，也没有包括 XGBoost
    库提供的所有选项（有关库功能的更多细节，请参见 [https://xgboost.readthedocs.io/](https://xgboost.readthedocs.io/)）。更具体地说：
- en: The code is written in pure Python, whereas the core of the XGBoost library
    is written in C++ (its Python classes are only thin wrappers over the C++ implementation).
  id: totrans-10
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 代码是用纯 Python 编写的，而 XGBoost 库的核心是用 C++ 编写的（其 Python 类只是对 C++ 实现的薄包装）。
- en: It does not include various optimizations that allow XGBoost to deal with huge
    amounts of data, such as weighted quantile sketch, out-of-core tree learning,
    and parallel and distributed processing of the data. These optimizations will
    be discussed in more detail in the next article in the series.
  id: totrans-11
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它不包括允许 XGBoost 处理大量数据的各种优化，例如加权分位数草图、外部树学习以及数据的并行和分布式处理。这些优化将在系列文章的下一篇中详细讨论。
- en: The implementation currently supports only regression and binary classification
    tasks, whereas the XGBoost library also supports multi-class classification and
    ranking problems.
  id: totrans-12
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 目前的实现仅支持回归和二分类任务，而 XGBoost 库还支持多分类和排序问题。
- en: 'The implementation provides only a small subset of the hyperparameters that
    exist in the XGBoost library. Specifically, it supports the following hyperparameters:'
  id: totrans-13
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实现仅提供了 XGBoost 库中存在的超参数的一个小子集。具体而言，它支持以下超参数：
- en: '*n_estimators* (default = 100): the number of regression trees in the ensemble
    (which is also the number of boosting iterations).'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*n_estimators*（默认值 = 100）：集合中的回归树数量（也就是提升迭代的数量）。'
- en: '*max_depth* (default = 6): the maximum depth (number of levels) of each tree.'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*max_depth*（默认值 = 6）：每棵树的最大深度（级别数）。'
- en: '*learning_rate* (default = 0.3): the step size shrinkage applied to the trees.'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*learning_rate*（默认值 = 0.3）：应用于树的步长收缩。'
- en: '*reg_lambda* (default = 1): L2 regularization term applied to the weights of
    the leaves.'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*reg_lambda*（默认值 = 1）：应用于叶子权重的 L2 正则化项。'
- en: '*gamma* (default = 0): minimum loss reduction required to split a given node.'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*gamma*（默认值 = 0）：分割给定节点所需的最小损失减少。'
- en: For consistency, I have kept the same names and default values of these hyperparameters
    as they are defined in the XGBoost library.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 为了保持一致性，我保持了这些超参数在 XGBoost 库中定义的名称和默认值。
- en: Despite the above limitations, the results obtained by the provided implementation
    are comparable to those obtained by the XGBoost library on a number of benchmark
    data sets (as will be demonstrated later in the article).
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管存在上述限制，但提供的实现所获得的结果与 XGBoost 库在多个基准数据集上获得的结果可比（稍后将在文章中演示）。
- en: The Code Structure
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 代码结构
- en: 'The source code consists of five classes:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 源代码由五个类组成：
- en: XGBNode represents a single node in the regression tree.
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: XGBNode 表示回归树中的一个单独节点。
- en: XGBTree represents a regression tree in the ensemble.
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: XGBTree 表示集合中的回归树。
- en: XGBBaseModel is a base estimator for the XGBoost models.
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: XGBBaseModel 是 XGBoost 模型的基础估计器。
- en: XGBRegressor is a subclass of XGBBaseModel that provides an estimator for regression
    tasks.
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: XGBRegressor 是 XGBBaseModel 的一个子类，提供回归任务的估计器。
- en: XGBClassifier is a subclass of XGBBaseModel that provides an estimator for binary
    classification tasks.
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: XGBClassifier 是 XGBBaseModel 的一个子类，提供二分类任务的估计器。
- en: For better readability and easy maintenance, each class is written in its own
    module (.py) file.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好的可读性和易于维护，每个类都写在自己的模块（.py）文件中。
- en: 'The complete source code can be found on my github: [https://github.com/roiyeho/medium/tree/main/xgboost/xgboost_from_scratch](https://github.com/roiyeho/medium/tree/main/xgboost/xgboost_from_scratch)'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 完整的源代码可以在我的 github 上找到：[https://github.com/roiyeho/medium/tree/main/xgboost/xgboost_from_scratch](https://github.com/roiyeho/medium/tree/main/xgboost/xgboost_from_scratch)
- en: We will now go through each piece of the code step-by-step.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将逐步讲解代码的每一部分。
- en: The XGBNode Class
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: XGBNode 类
- en: This class represents a node in the regression tree. It implements the core
    functionality of the algorithm for finding the best split at a given node.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 这个类表示回归树中的一个节点。它实现了算法中在给定节点找到最佳分割的核心功能。
- en: 'In the constructor of the class, we initialize the node to be a non-leaf node,
    and set the pointers to its left and right child nodes to be None:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在类的构造函数中，我们将节点初始化为非叶子节点，并将其左、右子节点的指针设置为 None：
- en: '[PRE0]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The *build*() method recursively builds the node until one of the following
    stopping criteria is reached:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '*build*() 方法递归地构建节点，直到达到以下停止标准之一：'
- en: There is only one sample left at the node.
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 节点上只剩下一个样本。
- en: The current level of the tree has reached the maximum depth.
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当前树的层级已达到最大深度。
- en: The best gain (reduction in the loss) we can obtain from splitting the node
    is less than *gamma*.
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从分割节点中我们可以获得的最佳增益（损失减少）小于 *gamma*。
- en: '[PRE1]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'If one of the stopping criteria has been reached, we turn the current node
    into a leaf node and then calculate its weight (score) by calling the helper method
    *calc_leaf_weight*():'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 如果已达到某个停止标准，我们将当前节点转换为叶子节点，然后通过调用辅助方法 *calc_leaf_weight*() 计算其权重（分数）：
- en: '[PRE2]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'On the other hand, if the node can be split, we call the helper method *find_best_split*(),
    which iterates over every possible split and returns the split with the highest
    gain:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，如果节点可以被分割，我们调用辅助方法 *find_best_split*()，该方法遍历所有可能的分割并返回增益最高的分割：
- en: '[PRE3]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The *find_best_split*() function uses the following helper function to compute
    the gain obtained by a given split:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '*find_best_split*() 函数使用以下辅助函数来计算给定分割获得的增益：'
- en: '[PRE4]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Lastly, the *predict*() method returns the score of a given sample according
    to the weight of the leaf to which it is mapped to by the tree. It uses the best
    split thresholds saved in the tree nodes in order to determine which path in the
    tree to follow.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，*predict*() 方法根据样本被映射到的叶子节点的权重返回给定样本的分数。它使用树节点中保存的最佳分割阈值来确定沿着树的哪个路径进行。
- en: '[PRE5]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The XGBTree Class
  id: totrans-48
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: XGBTree 类
- en: 'This class represents a single regression tree. In the constructor of the class
    we initialize the root node of the tree to None:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 这个类表示一个单独的回归树。在类的构造函数中，我们将树的根节点初始化为 None：
- en: '[PRE6]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'In the *build*() method, we simply call the *build*() method of the root node
    and pass to it the hyperparameters of the training algorithm:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在 *build*() 方法中，我们简单地调用根节点的 *build*() 方法，并将训练算法的超参数传递给它：
- en: '[PRE7]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Lastly, the *predict*() method returns the score of a given sample by delegating
    the call to the *predict*() method of the root node:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，*predict*() 方法通过委托调用根节点的 *predict*() 方法来返回给定样本的评分：
- en: '[PRE8]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The XGBBaseModel Class
  id: totrans-55
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: XGBBaseModel 类
- en: We now build the base class for the XGBoost estimators. To make this class compatible
    with the Scikit-Learn estimator API, we make it a subclass of BaseEstimator (from
    sklearn.base) and also implement the *fit*() and *predict*() methods. This will
    allow us to integrate this estimator with other Scikit-Learn mechanisms such as
    pipelines and grid search.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们构建 XGBoost 估计器的基类。为了使这个类兼容 Scikit-Learn 估计器 API，我们将其作为 BaseEstimator（来自
    sklearn.base）的子类，并实现 *fit*() 和 *predict*() 方法。这将允许我们将这个估计器与其他 Scikit-Learn 机制（如管道和网格搜索）集成。
- en: 'We first import the required libraries:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先导入所需的库：
- en: '[PRE9]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'In the constructor of the class, we initialize the hyperparameters of the model:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在类的构造函数中，我们初始化模型的超参数：
- en: '[PRE10]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The *fit*() method builds an ensemble of XGBoost trees for the given data set:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '*fit*() 方法为给定的数据集构建一个 XGBoost 树的集成：'
- en: '[PRE11]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: The *fit*() method uses a helper method called *get_output_values*() to get
    the predictions of the current ensemble for the given data set. Note that the
    output values are not necessarily the same as the final predicted labels of the
    model. For example, in classification tasks the output values of the ensemble
    are the predicted log odds and not the the class labels.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '*fit*() 方法使用一个叫做 *get_output_values*() 的辅助方法来获取当前集成对给定数据集的预测值。请注意，输出值不一定与模型的最终预测标签相同。例如，在分类任务中，集成的输出值是预测的对数几率，而不是类标签。'
- en: '[PRE12]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Finally, we define several abstract methods that need to be implemented by
    the concrete estimators:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们定义了几个需要具体估计器实现的抽象方法：
- en: '[PRE13]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: The XGBRegressor Class
  id: totrans-67
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: XGBRegressor 类
- en: 'This is a subclass of XGBBaseModel that is used to solve regression tasks.
    To implement this class, we first import the required libraries and functions:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 这是 XGBBaseModel 的一个子类，用于解决回归任务。要实现这个类，我们首先导入所需的库和函数：
- en: '[PRE14]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The constructor of the class simply passes the hyperparameters to the constructor
    of the base estimator:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 类的构造函数只是将超参数传递给基估计器的构造函数：
- en: '[PRE15]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The base prediction of the regressor is simply the mean of the target values:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 回归器的基预测只是目标值的均值：
- en: '[PRE16]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'The gradients and Hessians of the square loss are computed using the equations
    given in the [previous article](https://medium.com/towards-data-science/xgboost-the-definitive-guide-part-1-cc24d2dcd87a)
    (see the section “XGBoost for Regression”):'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 平方损失的梯度和赫西矩阵是使用 [前一篇文章](https://medium.com/towards-data-science/xgboost-the-definitive-guide-part-1-cc24d2dcd87a)（参见“XGBoost
    for Regression”部分）中给出的方程计算的：
- en: '[PRE17]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The *predict*() method returns the output values from the ensemble (in regression
    tasks the predicted labels are the same as the output values from the ensemble):'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '*predict*() 方法返回来自集成的输出值（在回归任务中，预测标签与集成的输出值相同）：'
- en: '[PRE18]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Lastly, we add a *score*() method to return the *R*² score of the model on
    a given data set:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们添加一个 *score*() 方法来返回模型在给定数据集上的 *R*² 分数：
- en: '[PRE19]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Regression Example
  id: totrans-80
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 回归示例
- en: After all this hard work, we are now ready to test our XGBoostRegressor!
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 经过这些艰苦的工作，我们现在准备测试我们的 XGBoostRegressor！
- en: First, we will use the [make_regression](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html)()
    function from Scikit-Learn to generate a synthetic data set. The default settings
    of this function generate a random linear regression data with 100 samples and
    100 features, out of which only 10 are informative (i.e., correlated with the
    target).
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将使用 [make_regression](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html)()
    函数从 Scikit-Learn 生成一个合成数据集。该函数的默认设置生成具有 100 个样本和 100 个特征的随机线性回归数据，其中只有 10 个特征是有信息的（即，与目标相关）。
- en: '[PRE20]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Note that tree-based models such as XGBoost do not require scaling or normalization
    of the features.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，像 XGBoost 这样的基于树的模型不需要对特征进行缩放或归一化。
- en: 'Next, we split the data set into 80% training and 20% test:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将数据集拆分为 80% 的训练集和 20% 的测试集：
- en: '[PRE21]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'For evaluating different models on this data set, let’s write a general utility
    function to train a given model, measure its training time and evaluate its performance
    both on the training and test sets:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在这个数据集上评估不同的模型，让我们编写一个通用的工具函数来训练给定模型，测量其训练时间，并评估其在训练集和测试集上的表现：
- en: '[PRE22]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Let’s start by evaluating our implementation of XGBRegressor on the data set.
    We will use the default hyperparameter settings defined in the class.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始在数据集上评估我们对XGBRegressor的实现。我们将使用类中定义的默认超参数设置。
- en: '[PRE23]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'The results we get are:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到的结果是：
- en: '[PRE24]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Our model obtains a perfect *R*² score on the training set but a relatively
    low score on the test set.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的模型在训练集上获得了完美的*R*²分数，但在测试集上得分相对较低。
- en: 'Let’s compare its performance to the XGBClassifier from the xgboost library.
    First, make sure that you have the library installed by running the following
    command:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将其性能与xgboost库中的XGBClassifier进行比较。首先，通过运行以下命令确保你已安装了该库：
- en: '[PRE25]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'We can now create an instance from the XGBRegressor class defined in this package.
    Since it has the same name as our class, we will use its fully qualified name
    (including the module name):'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以从此包中定义的XGBRegressor类创建一个实例。由于它的名称与我们的类相同，我们将使用其完全限定名称（包括模块名）：
- en: '[PRE26]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'The results we get are:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到的结果是：
- en: '[PRE27]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Surprisingly, our model achieves a slightly better *R*² test score than the
    original XGBoost! However, its training time is much slower (about 14 times slower).
    This is due to the fact that we have not implemented any runtime optimizations
    (and our code is written in Python rather than C++).
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 令人惊讶的是，我们的模型在*R*²测试分数上略优于原始的XGBoost！然而，它的训练时间慢得多（大约慢14倍）。这是因为我们没有实现任何运行时优化（而且我们的代码是用Python编写的，而不是C++）。
- en: 'For completeness, let’s also compare these models to the classical gradient
    boosting and histogram-based gradient boosting provided by Scikit-Learn:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 为了完整性，让我们将这些模型与Scikit-Learn提供的经典梯度提升和基于直方图的梯度提升进行比较：
- en: '[PRE28]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '[PRE29]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: In this case, the best test score is obtained by the classical gradient boosting
    algorithm. Note that all models have been used with their default hyperparameters,
    thus these results may change after hyperparameter tuning.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，最佳测试分数是由经典梯度提升算法获得的。请注意，所有模型都使用了其默认超参数，因此这些结果可能会在超参数调整后发生变化。
- en: 'Let’s also evaluate our implementation on a real-world data set, namely the
    [California housing data set](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_california_housing.html),
    available from Scikit-Learn. This time we will write the evaluation code a bit
    more succinctly by defining all the models in a list and then calling the evaluation
    function inside a loop:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们也在一个真实世界的数据集上评估我们的实现，即[加州住房数据集](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_california_housing.html)，它来自Scikit-Learn。这次我们将通过将所有模型定义在一个列表中，然后在循环中调用评估函数，来使评估代码更加简洁：
- en: '[PRE30]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'We get the following results:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到以下结果：
- en: '[PRE31]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: The test *R*² score of our model is slightly worse than the score of the XGBoost
    library (0.825 compared to 0.836), but is significantly higher than the score
    of the classical gradient boosting algorithm (0.777). As expected, the training
    time of our model is much longer compared to the other models.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 我们模型的*R*²测试分数略低于XGBoost库的分数（0.825对比0.836），但显著高于经典梯度提升算法的分数（0.777）。正如预期的那样，我们模型的训练时间相比其他模型要长得多。
- en: The XGBClassifier Class
  id: totrans-110
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: XGBClassifier 类
- en: 'This is also a subclass of XGBBaseModel, which is used to solve classification
    tasks (only binary classification is currently supported). To implement this class,
    we first import the required libraries and functions:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 这也是XGBBaseModel的一个子类，用于解决分类任务（目前只支持二分类）。要实现此类，我们首先导入所需的库和函数：
- en: '[PRE32]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'The constructor of the class simply passes the hyperparameters to the constructor
    of the base estimator:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 类的构造函数简单地将超参数传递给基础估计器的构造函数：
- en: '[PRE33]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'The base prediction of the classifier is the log odds of the positive class:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 分类器的基础预测是正类的对数几率：
- en: '[PRE34]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'To compute the gradients of log loss we will need the sigmoid function, so
    let’s implement it first:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 为了计算对数损失的梯度，我们需要sigmoid函数，因此首先实现它：
- en: '[PRE35]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'The gradients and Hessians of the log loss are computed using the equations
    given in the [previous article](https://medium.com/towards-data-science/xgboost-the-definitive-guide-part-1-cc24d2dcd87a)
    (see the section “XGBoost for Classification”):'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 对数损失的梯度和Hessian是使用[前一篇文章](https://medium.com/towards-data-science/xgboost-the-definitive-guide-part-1-cc24d2dcd87a)（见“XGBoost分类”部分）中给出的方程计算的：
- en: '[PRE36]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: Similar to other probabilistic classifiers in Scikit-Learn (such as LogisticRegression),
    our classifier will provide both *predict_proba*() and *predict*() methods.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于Scikit-Learn中的其他概率分类器（如LogisticRegression），我们的分类器将提供*predict_proba*() 和 *predict*()
    方法。
- en: 'The method *predict_proba*() returns the predicted probabilities of the positive
    class for the given samples. These probabilities are computed as the sigmoid of
    the log odds returned from the ensemble:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 方法 *predict_proba*() 返回给定样本的正类预测概率。这些概率是通过对集成返回的对数赔率进行sigmoid计算得到的：
- en: '[PRE37]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'The method *predict*() returns class labels for the samples in the given data
    set, using 0.5 as the threshold for assigning a sample to the positive class:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 方法 *predict*() 返回给定数据集中样本的类别标签，使用0.5作为将样本分配给正类的阈值：
- en: '[PRE38]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Lastly, we add a *score*() method that returns the accuracy of the model on
    a given data set:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们添加了一个*score*() 方法，返回模型在给定数据集上的准确率：
- en: '[PRE39]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: Classification Example
  id: totrans-128
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分类示例
- en: For evaluating our XGBoostClassifier, we will use the [make_classification](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_classification.html)()
    function from Scikit-Learn. This function creates clusters of normally distributed
    points (100 points by default) around vertices of an *n*-dimensional hypercube
    (*n* = 2 by default) with sides of length defined by a hyperparameter called *class_sep*
    (equal to 1.0 by default).
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 为了评估我们的XGBoostClassifier，我们将使用 [make_classification](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_classification.html)()
    函数来自Scikit-Learn。该函数在* n *维超立方体的顶点周围创建正态分布点的簇（默认100个点），边长由名为*class_sep*的超参数定义（默认1.0）。
- en: 'First, we will use the function to generate 1,000 sample points with a class
    separation of 0.1:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将使用该函数生成1,000个样本点，类分离度为0.1：
- en: '[PRE40]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Similar to the regression example, we will write a general utility function
    to evaluate a given model and measure its training time:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于回归示例，我们将编写一个通用实用函数来评估给定模型并测量其训练时间：
- en: '[PRE41]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'We will compare the following algorithms:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将比较以下算法：
- en: '[PRE42]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'The results we get are:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到的结果是：
- en: '[PRE43]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: Our model achieves the highest accuracy on the test set! On the down side, it
    is significantly slower than the other models (about 36 times slower than the
    XGBoost library).
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的模型在测试集上达到了最高的准确率！但它的速度比其他模型显著慢（比XGBoost库慢约36倍）。
- en: Exercises
  id: totrans-139
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 练习
- en: 'The provided implementation can be improved and extended in many ways. For
    example, you can try to add the following features to the codebase:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 提供的实现可以在许多方面进行改进和扩展。例如，您可以尝试向代码库中添加以下功能：
- en: Add support for multi-class classification.
  id: totrans-141
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 添加对多类别分类的支持。
- en: Add support for categorical features.
  id: totrans-142
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 添加对分类特征的支持。
- en: Implement early stopping, i.e., stop the boosting once the score on the validation
    set does not improve for a specified number of rounds.
  id: totrans-143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实现早期停止，即当验证集上的分数在指定的轮数内没有改善时停止提升。
- en: Add support for subsampling, i.e., train each tree only on a random subset of
    the training data.
  id: totrans-144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 添加对子采样的支持，即仅在训练数据的随机子集上训练每棵树。
- en: Add additional hyperparameters, such as *min_class_weight* that defines the
    minimum sum of instance weight (Hessian) needed in a child node.
  id: totrans-145
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 添加额外的超参数，例如*min_class_weight*，定义在子节点中所需的最小实例权重（Hessian）的总和。
- en: In the next article we will discuss the various hardware and software optimizations
    implemented by the XGBoost library that allow it to run so fast.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一篇文章中，我们将讨论XGBoost库实现的各种硬件和软件优化，使其运行如此迅速。
- en: Thanks for reading!
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 感谢阅读！
- en: References
  id: totrans-148
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] Chen, T., & Guestrin, C. (2016). XGBoost: A scalable tree boosting system.
    *Proceedings of the 22nd acm sigkdd international conference on knowledge discovery
    and data mining*, 785–794.'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] 陈天石，& 顾斯特林，C.（2016）。XGBoost: 一个可扩展的树提升系统。*第22届ACM SIGKDD国际知识发现与数据挖掘大会论文集*，785–794。'
- en: 'California housing data set info:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 加州住房数据集信息：
- en: '**Citation**: Pace, R. Kelley and Ronald Barry (1997), Sparse Spatial Autoregressions,
    Statistics and Probability Letters, 33, 291–297.'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '**引用**：Pace, R. Kelley 和 Ronald Barry（1997），稀疏空间自回归，统计与概率通讯，33，291–297。'
- en: '**License**: Creative Commons CC0: Public Domain.'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '**许可证**：创作共用 CC0：公有领域。'
