- en: '4 Ways to Write Data To Parquet With Python: A Comparison'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Python 将数据写入 Parquet 的 4 种方法：比较
- en: 原文：[https://towardsdatascience.com/4-ways-to-write-data-to-parquet-with-python-a-comparison-3c4f54ee5fec](https://towardsdatascience.com/4-ways-to-write-data-to-parquet-with-python-a-comparison-3c4f54ee5fec)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/4-ways-to-write-data-to-parquet-with-python-a-comparison-3c4f54ee5fec](https://towardsdatascience.com/4-ways-to-write-data-to-parquet-with-python-a-comparison-3c4f54ee5fec)
- en: Learn How To Efficiently Write Data To Parquet Format Using Pandas, FastParquet,
    PyArrow or PySpark.
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 学习如何高效地使用 Pandas、FastParquet、PyArrow 或 PySpark 将数据写入 Parquet 格式。
- en: '[](https://anbento4.medium.com/?source=post_page-----3c4f54ee5fec--------------------------------)[![Antonello
    Benedetto](../Images/bf802bb46dce03dd3bd4e17c1dffe5b7.png)](https://anbento4.medium.com/?source=post_page-----3c4f54ee5fec--------------------------------)[](https://towardsdatascience.com/?source=post_page-----3c4f54ee5fec--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----3c4f54ee5fec--------------------------------)
    [Antonello Benedetto](https://anbento4.medium.com/?source=post_page-----3c4f54ee5fec--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://anbento4.medium.com/?source=post_page-----3c4f54ee5fec--------------------------------)[![Antonello
    Benedetto](../Images/bf802bb46dce03dd3bd4e17c1dffe5b7.png)](https://anbento4.medium.com/?source=post_page-----3c4f54ee5fec--------------------------------)[](https://towardsdatascience.com/?source=post_page-----3c4f54ee5fec--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----3c4f54ee5fec--------------------------------)
    [Antonello Benedetto](https://anbento4.medium.com/?source=post_page-----3c4f54ee5fec--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----3c4f54ee5fec--------------------------------)
    ·7 min read·Mar 13, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----3c4f54ee5fec--------------------------------)
    ·7 分钟阅读·2023年3月13日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/39041e5a101311da2fe0002dd298f589.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/39041e5a101311da2fe0002dd298f589.png)'
- en: '[Photo by Dominika Roseclay](https://www.pexels.com/photo/background-of-pile-of-firewood-with-rough-surface-4318196/)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[照片来源：Dominika Roseclay](https://www.pexels.com/photo/background-of-pile-of-firewood-with-rough-surface-4318196/)'
- en: On-Demand Courses | Recommended
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 按需课程 | 推荐
- en: '*A few of my readers have contacted me asking for on-demand courses to learn
    more about* ***Data Engineering with Python & PySpark****. These are 3 great resources
    I would recommend:*'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '*一些读者联系我，询问关于* ***使用 Python 和 PySpark 进行数据工程*** *的按需课程。这些是我推荐的 3 个很好的资源：*'
- en: '[**Data Streaming With Apache Kafka & Apache Spark Nano-Degree (UDACITY)**](https://imp.i115008.net/zaX10r)'
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**使用 Apache Kafka 和 Apache Spark 的数据流处理 Nano-Degree（UDACITY）**](https://imp.i115008.net/zaX10r)'
- en: '[**Data Engineering Nano-Degree (UDACITY)**](https://imp.i115008.net/zaX10r)'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**数据工程 Nano-Degree（UDACITY）**](https://imp.i115008.net/zaX10r)'
- en: '[**Spark And Python For Big Data With PySpark (UDEMY)**](https://click.linksynergy.com/deeplink?id=533LxfDBSaM&mid=39197&murl=https%3A%2F%2Fwww.udemy.com%2Fcourse%2Fspark-and-python-for-big-data-with-pyspark%2F)'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**使用 PySpark 进行大数据处理的 Spark 和 Python（UDEMY）**](https://click.linksynergy.com/deeplink?id=533LxfDBSaM&mid=39197&murl=https%3A%2F%2Fwww.udemy.com%2Fcourse%2Fspark-and-python-for-big-data-with-pyspark%2F)'
- en: '*Not a Medium member yet? Consider signing up with my* [*referral link*](https://anbento4.medium.com/membership)
    *to gain access to everything Medium has to offer for as little as $5 a month!*'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '*还不是 Medium 会员？考虑通过我的* [*推荐链接*](https://anbento4.medium.com/membership) *注册，以仅需每月
    $5 即可访问 Medium 提供的所有内容！*'
- en: Introduction
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: In today’s data-driven world, efficient storage and processing of large datasets
    is a crucial requirement for many businesses and organisations. This is where
    the Parquet file format comes into play.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在今天的数据驱动世界中，高效存储和处理大规模数据集是许多企业和组织的关键需求。这就是 Parquet 文件格式发挥作用的地方。
- en: '[Parquet is a **columnar storage format** that is designed to optimise data
    processing and querying performance while minimising storage space.](https://www.databricks.com/glossary/what-is-parquet)'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '[Parquet 是一种**列式存储格式**，旨在优化数据处理和查询性能，同时最小化存储空间。](https://www.databricks.com/glossary/what-is-parquet)'
- en: It is particularly well-suited for use cases where data needs to be analysed
    quickly and efficiently, such as in data warehousing, big data analytics, and
    machine learning applications.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 它特别适合需要快速高效分析数据的用例，如数据仓库、大数据分析和机器学习应用。
- en: 'In this article, I will demonstrate how to write data to Parquet files in Python
    using four different libraries: [Pandas](https://pandas.pydata.org/docs/), [FastParquet](https://fastparquet.readthedocs.io/en/latest/),
    [PyArrow,](https://arrow.apache.org/docs/python/index.html) and [PySpark](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/index.html).'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我将演示如何使用四个不同的库将数据写入 Parquet 文件：[Pandas](https://pandas.pydata.org/docs/)、[FastParquet](https://fastparquet.readthedocs.io/en/latest/)、[PyArrow](https://arrow.apache.org/docs/python/index.html)
    和 [PySpark](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/index.html)。
- en: 'In particular, you will learn how to:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 特别是，你将学习如何：
- en: '**retrieve data from a database, convert it to a DataFrame, and use each one
    of these libraries to write records to a Parquet file.**'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**从数据库中检索数据，将其转换为 DataFrame，并使用这些库中的每一个将记录写入 Parquet 文件。**'
- en: '**write data to Parquet files in batches, to optimise performance and memory
    usage.**'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**以批量形式将数据写入 Parquet 文件，以优化性能和内存使用。**'
- en: By the end of this article, you’ll have a thorough understanding of how to use
    Python to write Parquet files and unlock the full power of this efficient storage
    format.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 到本文结束时，你将对如何使用 Python 写入 Parquet 文件有一个全面的理解，并解锁这种高效存储格式的全部潜力。
- en: The Dataset
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据集
- en: The dataset used as part of this tutorial, includes mock data about daily account
    balances in different currencies and for different companies.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 作为本教程的一部分使用的数据集包括关于不同货币和不同公司每日账户余额的模拟数据。
- en: Data has been generated using a Python recursive function and then inserted
    into a SnowFlake DB table.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 数据是通过 Python 递归函数生成的，然后插入到 SnowFlake 数据库表中。
- en: Then, a connection to the DB has been established using either the Python `snowflake.connector`
    or the native PySpark connectivity tools (*paired with jars*), to retrieve the
    dataset and convert it to DF format.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，使用 Python 的 `snowflake.connector` 或本地 PySpark 连接工具（*配合 jars*）建立与数据库的连接，以检索数据集并将其转换为
    DF 格式。
- en: 'The code to achieve the steps above is available [here](https://gist.github.com/anbento0490/918b9319109b1860b5b7c3878ba08896),
    whereas the first few rows of the DF, are displayed below:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 实现上述步骤的代码可在 [这里](https://gist.github.com/anbento0490/918b9319109b1860b5b7c3878ba08896)
    获得，而 DF 的前几行如下所示：
- en: '![](../Images/d5074981abd7092017a3dccf3ffeb6ed.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d5074981abd7092017a3dccf3ffeb6ed.png)'
- en: Mock data generated by the author, fetched and converted to DF.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 作者生成的模拟数据，已获取并转换为 DF。
- en: Let’s now describe four different strategies to write this dataset to parquet
    format using Python.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们描述四种使用 Python 将数据集写入 Parquet 格式的不同策略。
- en: Exploring 4 Methods To Write To Parquet Files
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索 4 种写入 Parquet 文件的方法
- en: As you can see, the common starting point to all methods, is having data already
    converted to either a Pandas or PySpark DF.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，所有方法的共同起点是将数据已转换为 Pandas 或 PySpark DF。
- en: This will make the code much more concise, readable and your life easier.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 这将使代码更加简洁、可读，并让你的生活更轻松。
- en: 'Method # 1: Using Plain Pandas'
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '方法 # 1：使用普通 Pandas'
- en: 'Probably the simplest way to write dataset to parquet files, is by using the
    `to_parquet()` method in the `pandas` module:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 将数据集写入 Parquet 文件的最简单方法可能是使用 `pandas` 模块中的 `to_parquet()` 方法：
- en: '[PRE0]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: In this case, `engine = 'pyarrow'` has been used as [PyArrow](https://arrow.apache.org/docs/python/index.html)
    is a high-performance Python library for working with Apache Arrow data. It provides
    a fast and memory-efficient implementation of the Parquet file format, which can
    improve the write performance of parquet files.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，使用了 `engine = 'pyarrow'`，因为 [PyArrow](https://arrow.apache.org/docs/python/index.html)
    是一个高性能的 Python 库，用于处理 Apache Arrow 数据。它提供了 Parquet 文件格式的快速和内存高效的实现，可以提高 Parquet
    文件的写入性能。
- en: In addition, PyArrow supports a range of compression algorithms, including `gzip`,
    `snappy` and `LZ4`.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，PyArrow 支持多种压缩算法，包括 `gzip`、`snappy` 和 `LZ4`。
- en: Throughout this tutorial, let’s pretend that the goal was to achieve high compression
    ratios (*that deliver smaller Parquet files in size*), even at the cost of slower
    compression and decompression speeds. This is why `compression = 'gzip'` has been
    used.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在本教程中，让我们假装目标是实现高压缩比（*即生成更小的 Parquet 文件*），即使这意味着压缩和解压速度较慢。这就是为什么使用了 `compression
    = 'gzip'`。
- en: '**Method # 2: Using Pandas & FastParquet**'
  id: totrans-40
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**方法 # 2：使用 Pandas 和 FastParquet**'
- en: 'Another popular way to write datasets to parquet files is by using the `fastparquet`
    package. In the simplest form, its `write()` method accepts a `pandas` DF as an
    input dataset and can compress it using variety of algorithms:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种流行的将数据集写入`parquet`文件的方法是使用`fastparquet`包。在最简单的形式下，它的`write()`方法接受一个`pandas`数据框作为输入数据集，并可以使用多种算法进行压缩：
- en: '[PRE1]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'But then, if `fastparquet` works with `pandas` DFs anyway, why shouldn’t **Method
    #1** be used instead?'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '但如果`fastparquet`可以处理`pandas`数据框，那么为什么不使用**方法 #1**呢？'
- en: 'Well, there are at least a couple of reasons to use `fastparquet` package:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，使用`fastparquet`包至少有几个理由：
- en: '*it is designed to be a lightweight package, known for its fast write performance
    and efficient memory usage.*'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*它被设计成一个轻量级包，以其快速的写入性能和高效的内存使用而闻名。*'
- en: '*it provides a pure Python implementation of the parquet format and then offers
    much more flexibility and options to developers.*'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*它提供了`parquet`格式的纯Python实现，并为开发人员提供了更多的灵活性和选项。*'
- en: For instance, by using the `fp.write()` method, you can specify the option `append
    = True` , something that is not yet possible through the `to_parquet()` method.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，通过使用`fp.write()`方法，你可以指定选项`append = True`，这是`to_parquet()`方法尚不支持的。
- en: This option becomes particularly handy when the source dataset is too large
    to be written in memory in one go, so that, to avoid `OOM` errors you decide to
    write it to the `parquet` files in batches.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 当源数据集太大，无法一次写入内存时，这个选项特别方便，因此为了避免`OOM`错误，你决定将其分批写入`parquet`文件中。
- en: 'The code below, is a valid example of how to write large datasets to a `parquet`
    file, in batches, by combining the power of `pandas` and `fastparquet`:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的代码是一个有效的示例，展示了如何利用`pandas`和`fastparquet`的强大功能，将大型数据集分批写入`parquet`文件：
- en: '[PRE2]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The strategy used in the code above is to:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码使用的策略是：
- en: 'Define a `batch_size`: in this case of this tutorial it has been set to only
    `250` rows but in production it can easily be increased to *several million rows*,
    depending on memory available on the worker that will perform the job.'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 定义一个`batch_size`：在本教程中，它设置为仅`250`行，但在生产中可以根据执行作业的工作机器的内存，轻松增加到*数百万行*。
- en: Fetch the very first batch of rows from the database, using `fetchmany()` instead
    of `fetchall()` and use this dataset to create a `pandas` DF. Write such DF to
    a parquet file.
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从数据库中提取第一批行，使用`fetchmany()`而不是`fetchall()`，并使用此数据集创建一个`pandas`数据框。将此数据框写入`parquet`文件。
- en: Instantiate a `WHILE loop` that will keep fetching data from the DB in batches,
    converting it to `pandans` DFs and eventually append it to the initial `parquet`
    file.
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实例化一个`WHILE loop`，它将不断从数据库中分批提取数据，将其转换为`pandas`数据框，最终附加到初始的`parquet`文件中。
- en: The `WHILE loop` will keep running until the last row has been written to the
    parquet file. Since the `batch_size` can be a variable updated depending on the
    use case, this should be considered a much more *“controlled”* and memory efficient
    method to write to a files with python.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '`WHILE loop`将持续运行，直到最后一行被写入`parquet`文件。由于`batch_size`可以根据使用情况进行更新，因此这应该被视为一种更加*“可控”*和内存高效的方法来用Python写入文件。'
- en: 'Method # 3: Using Pandas & PyArrow'
  id: totrans-56
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '方法 # 3：使用Pandas & PyArrow'
- en: Earlier in the tutorial, it has been mentioned that `pyarrow` is an high performance
    Python library that also provides a fast and memory efficient implementation of
    the `parquet` format.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在教程早期提到过，`pyarrow`是一个高性能的Python库，也提供了`parquet`格式的快速和内存高效实现。
- en: 'Its power can be used indirectly (by setting `engine = ''pyarrow''` like in
    **Method #1**) or directly by using some of its native methods.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '它的强大功能可以通过间接使用（设置`engine = ''pyarrow''`，如**方法 #1**所示）或直接使用其一些本地方法。'
- en: 'For example, you could easily replicate the code that writes to a `parquet`
    file in batches (**Method # 2.2**) , by using the `ParquetWriter()` method:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '例如，你可以很容易地复制**方法 # 2.2**中写入`parquet`文件的代码，使用`ParquetWriter()`方法：'
- en: '[PRE3]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Note that, since behind the scenes `pyarrow` takes advantage of the Apache Arrow
    format, the `ParquetWriter` requires a `pyarrow` schema as an argument (*which
    datatypes are fairly intuitive and somewhat similar to their* `pandas` *counterpart*).
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，由于`pyarrow`在幕后利用了Apache Arrow格式，因此`ParquetWriter`需要一个`pyarrow`模式作为参数（*这些数据类型相当直观，并且与其*
    `pandas` *对应类型有些类似*）。
- en: Moreover, while using this package, you are not allowed to write `pandas` DF
    directly, but those should be converted to a `pyarrow.Table` first (using the
    `from_pandas()` method), before the preferred dataset can be written to a file
    with the `write_table()` method.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，在使用此包时，你不能直接写入 `pandas` 数据框，但需要先将其转换为 `pyarrow.Table`（使用 `from_pandas()`
    方法），然后可以使用 `write_table()` 方法将所需数据集写入文件。
- en: 'Despite **Method #3** is a bit more verbose compared to the others, the Apache
    Arrow format is particularly recommended if declaring a schema and the availability
    of columns statistics is paramount for your use-case.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '尽管**方法 #3**相比其他方法更为详细，但如果在声明模式和列统计信息的可用性对你的用例至关重要，Apache Arrow 格式尤其推荐。'
- en: 'Method #4: Using PySpark'
  id: totrans-64
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '方法 #4：使用 PySpark'
- en: The last and probably most flexible way to write to a `parquet` file, is by
    using a `pyspark` native `df.write.parquet()` method.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 写入 `parquet` 文件的最后一种且可能是最灵活的方法是使用 `pyspark` 原生的 `df.write.parquet()` 方法。
- en: Of course the script below, assumes that you are connected to a DB and managed
    to load data into a DF, as shown [here](https://gist.github.com/anbento0490/918b9319109b1860b5b7c3878ba08896).
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，以下脚本假设你已连接到数据库并成功加载数据到数据框中，如 [这里](https://gist.github.com/anbento0490/918b9319109b1860b5b7c3878ba08896)
    所示。
- en: 'Note that in this case `mode(''overwrite'')` has been used, but you could easily
    switch it to `mode(''append'')` in case you wished to write data in batches. Also
    `pyspark` allows you to specify a large number of options among which the preferred
    `''compression''` algorithm:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，在这种情况下使用了 `mode('overwrite')`，但你可以轻松切换到 `mode('append')`，如果你希望批量写入数据。此外，`pyspark`
    允许你指定大量选项，其中包括首选的 `'compression'` 算法：
- en: '[PRE4]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '**Conclusion**'
  id: totrans-69
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**结论**'
- en: In this article, we have explored four different Python libraries that allow
    you to write data to Parquet files, including *Pandas*, *FastParquet*, *PyArrow,*
    and *PySpark*.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我们探索了四种不同的 Python 库，允许你将数据写入 Parquet 文件，包括 *Pandas*、*FastParquet*、*PyArrow*
    和 *PySpark*。
- en: Indeed, the Parquet file format is an essential tool for businesses and organisations
    that need to process and analyse large datasets quickly and efficiently.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，Parquet 文件格式是需要快速高效处理和分析大型数据集的企业和组织的关键工具。
- en: You have also learned how to write data in batches, which can further optimise
    performance and memory usage. By mastering these skills, you will be able to leverage
    the full power of Parquet and take your data processing and analysis to the next
    level.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 你还学习了如何批量写入数据，这可以进一步优化性能和内存使用。通过掌握这些技能，你将能够充分利用 Parquet 的强大功能，将数据处理和分析提升到一个新的水平。
- en: '**Sources**'
  id: totrans-73
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**来源**'
- en: '[What Is Parquet? | Databricks Official Documentation](https://www.databricks.com/glossary/what-is-parquet)'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[什么是 Parquet？| Databricks 官方文档](https://www.databricks.com/glossary/what-is-parquet)'
- en: '[Apache Arrow Official Documentation](https://arrow.apache.org/#:~:text=Format,data%20access%20without%20serialization%20overhead.)'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Apache Arrow 官方文档](https://arrow.apache.org/#:~:text=Format,data%20access%20without%20serialization%20overhead.)'
- en: '[File is too big? Make it chunks — By BlueBirz](https://www.bluebirz.net/en/make-it-chunks/)'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[文件太大？分块处理 — 由 BlueBirz 提供](https://www.bluebirz.net/en/make-it-chunks/)'
