- en: Thinking about fine-tuning a LLM? Here’s 3 considerations before you get started
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 考虑微调LLM？在你开始之前，这里有3个考虑因素
- en: 原文：[https://towardsdatascience.com/thinking-about-fine-tuning-an-llm-heres-3-considerations-before-you-get-started-c1f483f293](https://towardsdatascience.com/thinking-about-fine-tuning-an-llm-heres-3-considerations-before-you-get-started-c1f483f293)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/thinking-about-fine-tuning-an-llm-heres-3-considerations-before-you-get-started-c1f483f293](https://towardsdatascience.com/thinking-about-fine-tuning-an-llm-heres-3-considerations-before-you-get-started-c1f483f293)
- en: '[](https://medium.com/@smsmith714?source=post_page-----c1f483f293--------------------------------)[![Sean
    Smith](../Images/611395d113b10ec4bbfaf781301139c7.png)](https://medium.com/@smsmith714?source=post_page-----c1f483f293--------------------------------)[](https://towardsdatascience.com/?source=post_page-----c1f483f293--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----c1f483f293--------------------------------)
    [Sean Smith](https://medium.com/@smsmith714?source=post_page-----c1f483f293--------------------------------)'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@smsmith714?source=post_page-----c1f483f293--------------------------------)[![Sean
    Smith](../Images/611395d113b10ec4bbfaf781301139c7.png)](https://medium.com/@smsmith714?source=post_page-----c1f483f293--------------------------------)[](https://towardsdatascience.com/?source=post_page-----c1f483f293--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----c1f483f293--------------------------------)
    [Sean Smith](https://medium.com/@smsmith714?source=post_page-----c1f483f293--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----c1f483f293--------------------------------)
    ·11 min read·Jun 15, 2023
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page-----c1f483f293--------------------------------)
    ·11分钟阅读·2023年6月15日
- en: --
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/fabd8f334f6e560c8750dadc546fdd8e.png)'
  id: totrans-5
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fabd8f334f6e560c8750dadc546fdd8e.png)'
- en: Photo by [Brett Jordan](https://unsplash.com/@brett_jordan?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 照片由[Brett Jordan](https://unsplash.com/@brett_jordan?utm_source=medium&utm_medium=referral)拍摄，来自[Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: LLMs (Large Language Models) and generative AI are all the rage right now. A
    staggering statistic from [IBM](https://www.ibm.com/thought-leadership/institute-business-value/report/generative-ai-data-story)
    reveals that nearly 2 in 3 C-Suite executives feel pressure from investors to
    accelerate their adoption of generative AI. Naturally, this pressure is trickling
    down to Data Science and Machine Learning teams, who are responsible for navigating
    the hype and creating winning implementations.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 目前LLM（大型语言模型）和生成AI正当红。来自[IBM](https://www.ibm.com/thought-leadership/institute-business-value/report/generative-ai-data-story)的惊人统计数据显示，近2/3的C-Suite高管感受到来自投资者的压力，要求加快生成AI的采纳。这种压力自然会传递给数据科学和机器学习团队，他们负责应对炒作并创建成功的实施方案。
- en: 'As the landscape evolves, the ecosystem for LLMs has diverged between open
    source and industry models, with a [quickly filling moat](https://www.semianalysis.com/p/google-we-have-no-moat-and-neither).
    This emerging scene has prompted many teams to consider the following question:
    How can we make a LLM more specific for our use case?'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 随着环境的发展，LLM的生态系统在开源模型和行业模型之间发生了分化，形成了一个[迅速填补的护城河](https://www.semianalysis.com/p/google-we-have-no-moat-and-neither)。这一新兴场景促使许多团队考虑以下问题：我们如何使LLM更具体地适用于我们的用例？
- en: In this article we explore some key considerations that should be top of mind
    when contemplating the investment of time and engineering cycles to build a niche
    LLM. On this journey, it is crucial to be aware of some of the recent research
    surrounding potential limitations and best practices for building fine-tuned language
    models. After reading this article, you’ll be equipped with a few more ideas to
    lead your organization to the correct decision to train or not to train and how
    to train.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我们探讨了在考虑投入时间和工程周期来构建一个细分LLM时需要重点关注的一些关键因素。在这段旅程中，了解一些关于潜在限制和最佳实践的最新研究至关重要，这些研究涉及如何构建微调的语言模型。阅读完这篇文章后，你将会有更多的想法来指导你的组织做出正确的决定——是训练还是不训练，以及如何训练。
- en: You might not be able to mimic GPT with an open source model
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 你可能无法用开源模型模拟GPT
- en: 'It’s no secret to anyone that OpenAI is leading the LLM charge with it’s latest
    iterations of GPT. For that reason many stakeholders may ask for a development
    team do deploy a model that imitates the results of the more robust model for
    various reasons (rate limits, data privacy, costs, etc.) This naturally leads
    developers to wonder: Can we generate outputs from GPT and utilize them to fine-tune
    a model?'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 任何人都知道，OpenAI 正在通过其最新的 GPT 版本引领 LLM 的发展。因此，许多利益相关者可能会要求开发团队部署一个模型，以模仿更强大模型的结果，原因可能包括（速率限制、数据隐私、成本等）。这自然引发了开发者的疑问：我们能否从
    GPT 生成输出，并利用这些输出来微调一个模型？
- en: The answer to this question remains uncertain, as it seems to depend on several
    factors. This particular task, known as imitation learning, involves training
    a new language model through fine-tuning using target observations from a more
    advanced model such as GPT. While this seems like a great way to get good performance
    out of a downstream model, it does come with its share of potential issues.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 这个问题的答案仍然不确定，因为它似乎取决于几个因素。这个特定任务，称为模仿学习，涉及通过使用来自更先进模型（如 GPT）的目标观察来微调训练一个新的语言模型。虽然这似乎是从下游模型中获得良好性能的一个好方法，但也存在一些潜在的问题。
- en: '![](../Images/76cb4e5b4240d03930df9e5d59d3dce0.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/76cb4e5b4240d03930df9e5d59d3dce0.png)'
- en: Figure taken from reference Gudibande et al. [1].
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 图源自参考文献 Gudibande 等人 [1]。
- en: A recent paper titled “The False Promise of Imitating Proprietary LLMs” [1]
    sheds some light on potential pitfalls you may encounter with this approach. The
    authors present some experiments demonstrating that adding more imitation data
    can potentially lead to a degradation in model performance. Looking at the figure
    above, we can see that in the center graph that the accuracy on the benchmark
    task does decrease as the number of tokens increase. But why is that the case?
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 一篇题为“模仿专有 LLM 的虚假承诺” [1] 的最近论文对这种方法可能遇到的陷阱提供了一些见解。作者展示了一些实验，表明增加更多模仿数据可能会导致模型性能的下降。查看上图，我们可以看到在中心图表中，基准任务上的准确性随着令牌数量的增加而下降。那么这是为什么呢？
- en: 'The authors suggest the reason this happens is that imitation models learn
    the style of the model they are mimicking, rather than learning and understanding
    the content of the model. Taking a look in the left pane of the figure above,
    the human reviewers favored the results of the imitation model to those of ChatGPT.
    After exploring it was clear that the reviewers enjoyed the style of the imitation
    model, but did not closely examine the content. It was noted that the content
    produced by the imitation model tended to have weak factuality, leading the authors
    to summarize “imitation models actually embody some of the worst aspects of AI
    assistants: their answers sound confident but are less factual than ChatGPT.”'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 作者建议出现这种情况的原因是，模仿模型学习的是它们模仿的模型的风格，而不是学习和理解模型的内容。查看上图左侧的窗格，人工评审员更喜欢模仿模型的结果而非
    ChatGPT 的结果。经过探索，明显看到评审员喜欢模仿模型的风格，但并未仔细审查内容。注意到模仿模型产生的内容通常缺乏事实准确性，作者总结道：“模仿模型实际上体现了
    AI 助手的一些最糟糕的方面：它们的回答听起来很自信，但比 ChatGPT 的回答更不准确。”
- en: It’s important to note that there are some scenarios where imitation models
    can achieve great performance. The authors point out that the imitation models
    can achieve good performance on local tasks, or tasks that replicate a very specific
    behavior of the teacher model. On a task created for the study called NQ-Synthetic,
    the authors task the language model with generating 10 questions and answers related
    to a given context. Remarkably, the imitation model achieved a score close to
    that of GPT. This suggests that more specific models could achieve favorable outcomes
    when attempting to imitate behaviors from a teacher model.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是要注意，有些情况下模仿模型可以取得出色的表现。作者指出，模仿模型可以在本地任务或复制教师模型非常特定行为的任务上表现良好。在为研究创建的任务 NQ-Synthetic
    中，作者要求语言模型生成与给定背景相关的 10 个问题和答案。值得注意的是，模仿模型的得分接近 GPT。这表明，更具体的模型在尝试模仿教师模型的行为时可能会取得有利的结果。
- en: A fascinating corollary from the paper is that fine-tuning a model using a teacher
    model could actually help reduce the toxicity score of the imitation model. This
    could be extremely useful for companies that want to expose an open source LLM
    quickly without undergoing the laborious task of building filters surrounding
    the outputs. Instead of manually trying to build filters, companies could instead
    train on outputs from a carefully curated set of data from a teacher model to
    get a solid starting point.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 论文中的一个有趣的推论是，使用教师模型对模型进行微调实际上可以帮助减少模仿模型的毒性评分。这对于希望快速发布开源大语言模型的公司非常有用，而无需进行繁琐的过滤器构建任务。公司可以选择训练一个从精心挑选的教师模型中获得的输出，以获得一个良好的起点，而不是手动尝试构建过滤器。
- en: It is worth mentioning the recent release of Orca, a model developed by Microsoft
    Research, which incorporates signals from GPT as part of the training data. The
    difference here is in the size of the training data used for the model. Orca is
    fine-tuned on 5 million examples whereas the imitation model for broad coverage
    was tuned on approximately 151 thousand observations. Since I presume most of
    my audience will not be spending $16,000 to train an LLM as a casual experiment,
    I am inclined to make statements that more closely refer to the imitation modeling
    paper than Orca. That being said, we will have to wait for more research as to
    what the minimum number of examples required for imitation learning to emerge
    as a viable option for broader tasks.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 值得一提的是最近发布的Orca，这是一款由微软研究院开发的模型，它在训练数据中融入了GPT的信号。不同之处在于用于模型训练的数据量。Orca在500万示例上进行了微调，而广覆盖的模仿模型则在大约15.1万条观察数据上进行了调整。由于我认为大多数受众不会花费16000美元来训练大语言模型作为随意实验，我倾向于发表更接近于模仿建模论文的声明，而非Orca。话虽如此，我们还需要等待更多研究，以确定模仿学习在更广泛任务中成为可行选项所需的最小示例数。
- en: '**Takeaway: Depending on the complexity of your task, attempting to imitate
    the outputs of GPT or any sophisticated model with a weaker model may result in
    poor model performance.**'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '**结论：根据任务的复杂性，尝试用较弱的模型模仿GPT或任何复杂模型的输出可能会导致模型性能较差。**'
- en: Is In-Context Learning All You Need?
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 上下文学习是你所需的一切吗？
- en: In-Context Learning, or Few Shot Learning, is the process of including task-specific
    examples in the prompt. This approach is specific for sophisticated language models,
    since open source models have yet to achieve the desired flexibility to be able
    to handle In-Context Learning. Usually it is possible to achieve great results
    from this approach, but have you ever wondered why this is the case?
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 上下文学习（In-Context Learning），或称为少量样本学习（Few Shot Learning），是将任务特定的示例包含在提示中的过程。这种方法对于复杂的语言模型特别适用，因为开源模型尚未具备处理上下文学习所需的灵活性。通常，这种方法可以取得很好的结果，但你是否曾经想过为什么会这样？
- en: The answer to this question is explored in a paper by Dai et al. [3], where
    they explored the mathematical connections between loading examples in the prompt
    and fine-tuning using the same examples. The authors demonstrate that the prompt
    examples produce meta-gradients that are reflected during forward propagation
    at inference time. In the case of fine-tuning, the examples actually produce real
    gradients that are used to update the weights. Therefore, it appears that in-context
    learning achieves similar results to fine-tuning. For a more in-depth understanding
    of these finding I would encourage reading the paper, which spares no detail in
    the mathematical connections.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: Dai等人[3]的论文探讨了在提示中加载示例与使用相同示例进行微调之间的数学联系。作者展示了提示示例产生的元梯度在推理时的前向传播中得到体现。对于微调，示例实际产生的是真实梯度，用于更新权重。因此，上下文学习似乎达到了与微调类似的结果。为了更深入理解这些发现，我建议阅读该论文，其中详细阐述了数学联系。
- en: 'Although the approach of In-Context Learning is great, there does exist a limitation
    that is not evident in fine-tuning. In the case we have a large corpus of training
    data, a fine-tuned model will make use of all of that data by updating the model
    with real gradients during training. During In-Context Learning we can only provide
    a limited number of observations. So here a question arises: Given a substantial
    training corpus, how can we make use of the most relevant examples given our input
    to achieve the best results?'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管上下文学习的方法很棒，但存在一个在精细调整中不明显的限制。如果我们有大量的训练数据，精细调整的模型将通过在训练过程中更新模型的真实梯度来利用所有这些数据。在上下文学习中，我们只能提供有限数量的观察。因此，出现了一个问题：鉴于大量的训练语料库，我们如何利用最相关的示例来实现最佳结果？
- en: One approach to tackle this issue is by selecting examples using a heuristic,
    and fortunately, LangChain provides support for this. LangChain is a Python module
    that essentially houses pre-built prompts that simplifies working with language
    models. The tool from LangChain which we will concern ourselves with right now
    is the [ExampleSelector](https://python.langchain.com/docs/modules/model_io/prompts/example_selectors/).
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 解决这个问题的一种方法是使用启发式方法选择示例，幸运的是，LangChain 提供了支持。LangChain 是一个 Python 模块，基本上包含了简化与语言模型工作相关的预构建提示。我们现在关注的
    LangChain 工具是 [ExampleSelector](https://python.langchain.com/docs/modules/model_io/prompts/example_selectors/)。
- en: '[PRE0]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: ExampleSelectors are a type of prompt manipulator that allows us to dynamically
    change which examples are used during inference. There are many heuristics that
    can be used. Above I created some pseudo code of how a selector from LangChain
    essentially works. I used jaccard similarity between the input sequence and example
    sequence above. In LangChain there are many more options, so check them out [here](https://python.langchain.com/docs/modules/model_io/prompts/example_selectors/).
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: ExampleSelectors 是一种提示操控器，允许我们在推理过程中动态改变使用的示例。有很多启发式方法可以使用。上面我创建了一些 LangChain
    的选择器如何工作的伪代码。我使用了输入序列和示例序列之间的 jaccard 相似度。在 LangChain 中还有更多选项，请查看 [这里](https://python.langchain.com/docs/modules/model_io/prompts/example_selectors/)。
- en: There are two primary benefits to having an approach like this. The first is
    that you allow your LLM to be data efficient, by selectively choosing the most
    relevant examples for the given input. This is opposed to having a few examples
    statically loaded for all observations. The second benefit comes from cost savings,
    if tuning through a managed service. As of writing, to use a fine-tuned base Davinci
    model costs $0.12 per 1,000 tokens. In contrast, using instruct Davinci costs
    $0.02, that’s a 500% increase in price! These prices also doesn’t include the
    cost of training.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 采用这种方法有两个主要好处。首先，你可以使你的 LLM 数据高效，通过选择最相关的示例来处理给定的输入。这与为所有观察加载少量静态示例相对。第二个好处是节省成本，如果通过托管服务进行调整的话。写作时，使用精细调整的基础
    Davinci 模型的费用为每 1,000 个 tokens $0.12。相比之下，使用 instruct Davinci 的费用为 $0.02，这是一种
    500% 的价格增长！这些价格也不包括培训的费用。
- en: It’s important to note that these prices are subject to change as OpenAI is
    not yet using LoRa or Adapters, as revealed in a now-deleted [blog post](https://web.archive.org/web/20230531203946/https://humanloop.com/blog/openai-plans)
    [5]. Nevertheless, the fine-tuned models are still likely to be more expensive
    due to the necessity of maintaining custom weights for individual users. This
    also doesn’t account for cost of examples in context. Your team will need to evaluate
    if ICL or fine-tuning makes more sense for your task from cost and accuracy standpoints.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的是，这些价格可能会发生变化，因为 OpenAI 目前尚未使用 LoRa 或适配器，这在一篇现已删除的 [博客文章](https://web.archive.org/web/20230531203946/https://humanloop.com/blog/openai-plans)
    [5] 中有透露。然而，由于需要为每个用户维护自定义权重，精细调整的模型仍然可能更昂贵。这也不包括上下文中示例的费用。你的团队需要评估从成本和准确性的角度来看，ICL
    还是精细调整更适合你的任务。
- en: '**Takeaway: In-Context Learning with dynamic example loading may achieve the
    same results as fine tuning without substantial additional costs that would come
    from a managed service.**'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '**要点：使用动态示例加载的上下文学习可能会在没有来自托管服务的额外成本的情况下实现与精细调整相同的结果。**'
- en: Does your task benefit from one or more intermediate steps before final inference?
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 你的任务是否从最终推理之前的一个或多个中间步骤中受益？
- en: 'Let’s say you’re trying to answer complex questions over long documents. This
    task fundamentally requires the language model to have a good mastery of language
    and understanding. This leads us to a question: What if we assist the language
    model in breaking down the reasoning process into subtasks, similar to how a human
    would analyze a document and sequentially execute tasks?'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你试图回答关于长文档的复杂问题。这项任务从根本上要求语言模型具备良好的语言掌握和理解能力。这引出了一个问题：如果我们帮助语言模型将推理过程分解为子任务，类似于人类如何分析文档并按顺序执行任务呢？
- en: '![](../Images/01c095f6e54a62ae6b8ac5a97e01078e.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/01c095f6e54a62ae6b8ac5a97e01078e.png)'
- en: Figure taken from Sun et al. [4].
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 来自 Sun 等 [4] 的图示。
- en: 'This is exactly what researchers from Microsoft set out to accomplish and their
    answer to this problem is PEARL [4]. PEARL stands for Planning and Executing Actions
    for Reasoning over Long documents. The general framework is broken down into three
    steps:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 这正是微软研究人员设定的目标，他们的解决方案是 PEARL [4]。PEARL 代表规划和执行行动以推理长文档。整体框架分为三个步骤：
- en: '**Action Mining:** The language model is first prompted to read the documents
    and extract possible actions that could be used to answer questions that are domain
    specific. To extract these actions, the language model is given a few example
    actions. I included an example of what an action could look like below.'
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**行动挖掘：** 语言模型首先被提示阅读文档，并提取可能用于回答领域特定问题的行动。为了提取这些行动，语言模型会提供几个示例行动。以下是一个行动可能的示例。'
- en: '**Plan Generation:** After generating a set of task-specific actions, the LLM
    is now asked to generate a subsequent list of actions to execute in order given
    a question and context. The LLM is provided some examples of plans for other tasks
    which aids in construction of a quality plan. More details about the technicalities
    can be found in the paper.'
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**计划生成：** 在生成一组任务特定行动后，LLM 现在被要求根据问题和上下文生成一个后续的行动列表。LLM 会提供其他任务的计划示例，这有助于构建高质量的计划。有关技术细节的更多信息，请参阅论文。'
- en: '**Plan Execution:** The model now has the plan. We now provide the inputs to
    the model and execute the plan.'
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**计划执行：** 现在模型已经有了计划。我们现在将输入提供给模型并执行计划。'
- en: '![](../Images/2ef172c199bd6de8133e08c945179820.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2ef172c199bd6de8133e08c945179820.png)'
- en: Example Action taken from Sun et al. [4].
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 来自 Sun 等 [4] 的示例行动。
- en: There are some intermediary steps that are used to ensure quality between stages.
    The authors include a self-correction step which ensures the plan conforms to
    the required format. There is also a self-refinement step that determines if the
    plan can be used later as a few-shot example.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在各个阶段之间使用了一些中间步骤以确保质量。作者包括一个自我校正步骤，确保计划符合所需格式。还有一个自我精炼步骤，确定计划是否可以在以后作为少量示例使用。
- en: '![](../Images/5787834bfb64a1c7a69c673d9e4c6ce5.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5787834bfb64a1c7a69c673d9e4c6ce5.png)'
- en: Table taken from Sun et al. [4].
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 来自 Sun 等 [4] 的表格。
- en: In evaluation PEARL demonstrated notable improvements over other GPT models,
    specifically when long documents were included. The key takeaway from this process
    is that in certain cases having multiple steps can significantly assist the model.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在评估中，PEARL 展示了相较于其他 GPT 模型的显著改进，尤其是在处理长文档时。这个过程的关键收获是，在某些情况下，拥有多个步骤可以显著帮助模型。
- en: Another scenario when having intermediate steps proves beneficial is when the
    number of documents to be included in your context exceeds what is supported by
    the language model. As it currently stands, the attention mechanism used by OpenAI
    scales at O(n²) and there is no solution to overcome this yet [5]. This creates
    considerable interest in reducing the context to the most minimal form possible.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个中间步骤证明有益的场景是当要包含在上下文中的文档数量超过语言模型支持的范围时。当前，OpenAI 使用的注意力机制的规模为 O(n²)，尚无解决方案来克服这一点
    [5]。这引起了对将上下文缩减到最小形式的强烈兴趣。
- en: Depending on your task there are ways to handle this. For instance, if your
    task entirely revolves around entities there is an opportunity to extract the
    relevant entities and their related properties. You can think of this approach
    as a lossy compression that allows you to feed more context into the LLM. Another
    benefit of this intermediate step is that you converted unstructured data to a
    structured format, which allows you to create informed decision making without
    the LLM. An example of this task is shown below in the figure from Fei et al.
    [6].
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 根据你的任务，有处理此问题的方法。例如，如果你的任务完全围绕实体展开，可以提取相关实体及其相关属性。你可以将这种方法视为一种有损压缩，允许你向LLM提供更多上下文。这个中间步骤的另一个好处是，你将非结构化数据转换为结构化格式，这使得你能够在没有LLM的情况下做出明智的决策。下面的图示来自Fei等人[6]展示了这一任务的一个示例。
- en: '![](../Images/65d4b19ef247c9172802e564af8a99fb.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/65d4b19ef247c9172802e564af8a99fb.png)'
- en: Figure taken from Fei et al. [6]
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 图示摘自Fei等人[6]
- en: '**Takeaway: Breaking a task into smaller subsequent problems can help simplify
    a larger problem into more manageable pieces. You can also use these smaller tasks
    to solve bottlenecks related to model limitations.**'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '**总结：将任务分解为较小的后续问题可以帮助将较大问题简化为更易管理的部分。你还可以利用这些较小的任务来解决与模型限制相关的瓶颈。**'
- en: Concluding Thoughts
  id: totrans-50
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结思考
- en: These are some general ideas regarding what researchers are exploring in the
    new frontiers of LLM performance and efficiency. This is not an exhaustive list
    of all things to be considered when fine-tuning a model, but it’s a good place
    to start when considering the journey.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是关于研究人员在LLM性能和效率的新前沿中探索的一些一般想法。这不是微调模型时需要考虑的所有事项的详尽列表，但在考虑这段旅程时是一个很好的起点。
- en: For further reading, this [post](https://huggingface.co/blog/trl-peft) from
    Hugging Face regarding training LLMs is quite interesting, and would be a great
    place to start when exploring imitation models on a local problem. Getting a concrete
    understanding of [LangChain](https://python.langchain.com/en/latest/index.html)
    is also supremely helpful. While most of the library could be rewritten for your
    use case, the main benefit is that it’s easier to keep up with research if other
    people are writing the code for you!
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 进一步阅读，Hugging Face关于训练LLMs的[这篇文章](https://huggingface.co/blog/trl-peft)非常有趣，是探索本地问题模仿模型的一个好起点。对[LangChain](https://python.langchain.com/en/latest/index.html)有一个具体的理解也是极其有帮助的。尽管大部分库可以为你的用例重写，但主要好处在于，如果其他人为你编写代码，你可以更容易跟上研究进展！
- en: 'Here are the takeaways again:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 再次总结要点：
- en: '**Depending on the complexity of your task, attempting to imitate the outputs
    of GPT or any sophisticated model with a weaker model may result in poor model
    performance.**'
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**根据任务的复杂性，试图用较弱的模型模仿GPT或任何复杂模型的输出可能会导致模型性能差。**'
- en: '**In-Context Learning with dynamic example loading may achieve the same results
    as fine tuning without substantial additional costs that would come from a managed
    service.**'
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**通过动态示例加载进行上下文学习可能会达到与微调相同的效果，而不会带来额外的管理服务成本。**'
- en: '**Breaking a task into smaller subsequent problems can help simplify a larger
    problem into more manageable pieces. You can also use these smaller tasks to solve
    bottlenecks related to model limitations.**'
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**将任务分解为较小的后续问题可以帮助将较大问题简化为更易管理的部分。你还可以利用这些较小的任务来解决与模型限制相关的瓶颈。**'
- en: '*Thanks for reading the article! My key areas of interests are in creating
    personalized interactions for users. Please consider interacting with the article
    and following me on* [*Medium*](https://medium.com/@smsmith714)*! If you want
    to contact me regarding any technical errors in this article or to stay in touch,
    please reach out on* [*LinkedIn*](https://www.linkedin.com/in/sms714/)*.*'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '*感谢阅读这篇文章！我的主要兴趣领域是为用户创建个性化互动。请考虑互动文章并关注我在* [*Medium*](https://medium.com/@smsmith714)*！如果你想联系我以讨论文章中的技术错误或保持联系，请通过*
    [*LinkedIn*](https://www.linkedin.com/in/sms714/)*联系我。*'
- en: References
  id: totrans-58
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] Arnav Gudibande, Eric Wallace, Charlie Snell, Xinyang Geng, Hao Liu, Pieter
    Abbeel, Sergey Levine, & Dawn Song. (2023). The False Promise of Imitating Proprietary
    LLMs.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] Arnav Gudibande, Eric Wallace, Charlie Snell, Xinyang Geng, Hao Liu, Pieter
    Abbeel, Sergey Levine, & Dawn Song. (2023). 模仿专有LLMs的虚假承诺。'
- en: '[2] Mukherjee, S., Mitra, A., Jawahar, G., Agarwal, S., Palangi, H., & Awadallah,
    A. (2023). Orca: Progressive Learning from Complex Explanation Traces of GPT-4*.
    arXiv: Computation and Language*.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] Mukherjee, S., Mitra, A., Jawahar, G., Agarwal, S., Palangi, H., & Awadallah,
    A.（2023）。Orca：从 GPT-4* 复杂解释轨迹中逐步学习。arXiv: 计算与语言*。'
- en: '[3] Damai Dai, Yutao Sun, Li Dong, Yaru Hao, Shuming Ma, Zhifang Sui, & Furu
    Wei. (2023). Why Can GPT Learn In-Context? Language Models Implicitly Perform
    Gradient Descent as Meta-Optimizers.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] Damai Dai, Yutao Sun, Li Dong, Yaru Hao, Shuming Ma, Zhifang Sui, & Furu
    Wei.（2023）。为什么 GPT 能在上下文中学习？语言模型隐式地作为元优化器执行梯度下降。'
- en: '[4]Simeng Sun, Yang Liu, Shuohang Wang, Chenguang Zhu, & Mohit Iyyer. (2023).
    PEARL: Prompting Large Language Models to Plan and Execute Actions Over Long Documents.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] Simeng Sun, Yang Liu, Shuohang Wang, Chenguang Zhu, & Mohit Iyyer.（2023）。PEARL：引导大型语言模型规划和执行长文档上的动作。'
- en: '[5] Habib, R.. (2023). OpenAI’s plans according to Sam Altman.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '[5] Habib, R..（2023）。根据 Sam Altman 的说法，OpenAI 的计划。'
- en: '[6] Hao Fei , Fei Li , Chenliang Li , Shengqiong Wu , Jingye Li and Donghong
    Ji, (2022). Inheriting the Wisdom of Predecessors: A Multiplex Cascade Framework
    for Unifed Aspect-based Sentiment Analysis'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '[6] Hao Fei, Fei Li, Chenliang Li, Shengqiong Wu, Jingye Li 和 Donghong Ji，（2022）。继承前人的智慧：一个用于统一基于方面的情感分析的多层级级联框架'
