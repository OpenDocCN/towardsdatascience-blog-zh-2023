- en: A Gentle Introduction to GPT Models
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: GPT模型简介
- en: 原文：[https://towardsdatascience.com/a-gentle-introduction-to-gpt-models-e02b093a495b?source=collection_archive---------1-----------------------#2023-04-12](https://towardsdatascience.com/a-gentle-introduction-to-gpt-models-e02b093a495b?source=collection_archive---------1-----------------------#2023-04-12)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/a-gentle-introduction-to-gpt-models-e02b093a495b?source=collection_archive---------1-----------------------#2023-04-12](https://towardsdatascience.com/a-gentle-introduction-to-gpt-models-e02b093a495b?source=collection_archive---------1-----------------------#2023-04-12)
- en: Welcome to the new world of token generators
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 欢迎来到新的令牌生成器的世界
- en: '[](https://medium.com/@bnjmn_marie?source=post_page-----e02b093a495b--------------------------------)[![Benjamin
    Marie](../Images/3ea1ad230cb1e67610418a8e36a5e5dd.png)](https://medium.com/@bnjmn_marie?source=post_page-----e02b093a495b--------------------------------)[](https://towardsdatascience.com/?source=post_page-----e02b093a495b--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----e02b093a495b--------------------------------)
    [Benjamin Marie](https://medium.com/@bnjmn_marie?source=post_page-----e02b093a495b--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@bnjmn_marie?source=post_page-----e02b093a495b--------------------------------)[![本杰明·玛丽](../Images/3ea1ad230cb1e67610418a8e36a5e5dd.png)](https://medium.com/@bnjmn_marie?source=post_page-----e02b093a495b--------------------------------)[](https://towardsdatascience.com/?source=post_page-----e02b093a495b--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----e02b093a495b--------------------------------)
    [本杰明·玛丽](https://medium.com/@bnjmn_marie?source=post_page-----e02b093a495b--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fad2a414578b3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-gentle-introduction-to-gpt-models-e02b093a495b&user=Benjamin+Marie&userId=ad2a414578b3&source=post_page-ad2a414578b3----e02b093a495b---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----e02b093a495b--------------------------------)
    ·9 min read·Apr 12, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fe02b093a495b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-gentle-introduction-to-gpt-models-e02b093a495b&user=Benjamin+Marie&userId=ad2a414578b3&source=-----e02b093a495b---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fad2a414578b3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-gentle-introduction-to-gpt-models-e02b093a495b&user=Benjamin+Marie&userId=ad2a414578b3&source=post_page-ad2a414578b3----e02b093a495b---------------------post_header-----------)
    发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----e02b093a495b--------------------------------)
    ·9分钟阅读·2023年4月12日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fe02b093a495b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-gentle-introduction-to-gpt-models-e02b093a495b&user=Benjamin+Marie&userId=ad2a414578b3&source=-----e02b093a495b---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fe02b093a495b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-gentle-introduction-to-gpt-models-e02b093a495b&source=-----e02b093a495b---------------------bookmark_footer-----------)![](../Images/db0b10a3b127c9e3fba8e78be613555e.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fe02b093a495b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-gentle-introduction-to-gpt-models-e02b093a495b&source=-----e02b093a495b---------------------bookmark_footer-----------)![](../Images/db0b10a3b127c9e3fba8e78be613555e.png)'
- en: Image from [Pixabay](https://pixabay.com/illustrations/dinosaur-t-rex-animal-dino-5631999/)
    — Modified by author
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源于 [Pixabay](https://pixabay.com/illustrations/dinosaur-t-rex-animal-dino-5631999/)
    — 作者修改
- en: With the recent releases of ChatGPT and GPT-4, GPT models have drawn a lot of
    interest from the scientific community. These new versions of OpenAI’s GPT models
    are so powerful and versatile that it may take a lot of time before we can exploit
    their full potential.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 随着ChatGPT和GPT-4的最近发布，GPT模型引起了科学界的极大关注。这些OpenAI的GPT模型新版本如此强大和多才多艺，以至于我们可能需要很长时间才能充分挖掘它们的潜力。
- en: Even though they are impressive, what you may not know is that the main ideas
    and algorithms behind GPT models are far from new.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管它们令人印象深刻，但你可能不知道，GPT模型背后的主要思想和算法远非新颖。
- en: Whether you are a seasoned data scientist or just someone curious about GPT,
    knowing how GPT models evolved is particularly insightful on the impact of data
    and what to expect for the coming years.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 无论你是经验丰富的数据科学家还是仅仅对GPT感到好奇的人，了解GPT模型的演变对数据的影响以及未来几年的预期都是特别有启发性的。
- en: In this article, I explain how GPT models became what they are today. I’ll mainly
    focus on how OpenAI scaled GPT models over the years. I’ll also give some pointers
    if you want to get started using GPT models.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我解释了 GPT 模型如何发展到今天的状态。我将主要关注 OpenAI 如何在这些年里扩展 GPT 模型。如果你想开始使用 GPT 模型，我也会给出一些指引。
- en: Generative pre-trained language models
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 生成预训练语言模型
- en: GPT models are language models.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: GPT 模型是语言模型。
- en: Language models have existed for [more than 50 years](https://web.stanford.edu/~jurafsky/slp3/3.pdf).
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 语言模型已经存在了 [超过 50 年](https://web.stanford.edu/~jurafsky/slp3/3.pdf)。
- en: The first generation of language models was “*n*-gram based”. They modeled the
    probability of a word given some previous words.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 第一代语言模型是“*n*-gram 基础”的。它们对给定一些前置词的情况下，预测一个词的概率。
- en: 'For instance, if you have the sentence:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果你有以下句子：
- en: '*The cat sleeps in the kitchen.*'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '*猫在厨房里睡觉*。'
- en: With *n*=3, you can get from a 3-gram language model the probability of having
    “*in*” following “*cat sleeps*”.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 *n*=3，你可以从一个 3-gram 语言模型中获得“*in*”跟在“*cat sleeps*”后面的概率。
- en: '*n*-gram models remained useful in many natural language and speech processing
    tasks until the beginning of the 2010s.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '*n*-gram 模型在许多自然语言和语音处理任务中仍然很有用，直到2010年代初。'
- en: They suffer several limitations. The computational complexity dramatically increases
    with a higher *n*. So these models were often limited to *n*=5 or lower.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 这些模型存在几个限制。计算复杂性随着 *n* 的增加而急剧增加。因此，这些模型通常限制在 *n*=5 或更低。
- en: Then, thanks to neural networks and the use of more powerful machines, this
    main limitation was alleviated and it became possible to compute the probability
    for much longer n-grams, for instance for *n*=20 or higher.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，得益于神经网络和更强大的机器，这一主要限制得到了缓解，可以计算更长 *n*-gram 的概率，例如 *n*=20 或更高。
- en: Generating text with these models was also possible but their outputs were of
    a so poor quality that they were rarely used for this purpose.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这些模型生成文本也是可能的，但它们的输出质量很差，因此很少用于这个目的。
- en: Then, in 2018, [OpenAI proposed the first GPT model](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf).
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，在2018年，[OpenAI 提出了第一个 GPT 模型](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf)。
- en: GPT stands for “generative pre-trained”. “Pre-trained” means that the model
    was simply trained on a large amount of text to model probabilities without any
    other purpose than language modeling. GPT models can then be fine-tuned, i.e.,
    further trained, to perform more specific tasks.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: GPT 代表“生成预训练”。“预训练”意味着模型只是基于大量文本进行训练，以建模概率，除了语言建模没有其他目的。GPT 模型可以进一步微调，即进一步训练，以执行更具体的任务。
- en: For instance, you can use a small dataset of news summaries to obtain a GPT
    model very good at news summarization. Or fine-tune it on French-English translations
    to obtain a machine translation system capable of translating from French to English.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，你可以使用一个小的数据集来获得一个在新闻摘要方面表现非常好的 GPT 模型，或者在法英翻译上进行微调，以获得一个能够将法语翻译成英语的机器翻译系统。
- en: '*Note: The term “pre-training” suggests that the models are not fully trained
    and that another step is needed. With recent models, the need for fine-tuning
    tends to disappear. The pre-trained models are now directly used in applications.*'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '*注意：术语“预训练”暗示模型尚未完全训练，还需要另一个步骤。随着最近模型的出现，微调的需求趋于消失。预训练模型现在可以直接在应用中使用。*'
- en: 'GPT models are now very good in almost all natural language processing tasks.
    I particularly studied their ability to do machine translation, as you can read
    in the following article:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: GPT 模型现在在几乎所有自然语言处理任务中表现都很优秀。我特别研究了它们在机器翻译方面的能力，你可以在以下文章中阅读：
- en: '[](/translate-with-gpt-3-9903c4a6f385?source=post_page-----e02b093a495b--------------------------------)
    [## Translate with GPT-3'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/translate-with-gpt-3-9903c4a6f385?source=post_page-----e02b093a495b--------------------------------)
    [## 使用 GPT-3 进行翻译'
- en: Machine translation but without a machine translation system
  id: totrans-30
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 机器翻译，但没有机器翻译系统
- en: towardsdatascience.com](/translate-with-gpt-3-9903c4a6f385?source=post_page-----e02b093a495b--------------------------------)
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/translate-with-gpt-3-9903c4a6f385?source=post_page-----e02b093a495b--------------------------------)
- en: The scale of the training, and the Transformer neural network architecture that
    they exploit, are the main reasons why they can generate fluent text.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 训练的规模和它们利用的 Transformer 神经网络架构是它们能够生成流畅文本的主要原因。
- en: Since 2018 and the first GPT, several versions and subversions of GPT followed.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 自2018年首次发布GPT以来，出现了多个版本和子版本的GPT。
- en: 4 versions and many more subversions
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 4个版本和更多子版本
- en: GPT and GPT-2
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: GPT和GPT-2
- en: '[GPT-2 came out only a few months after the first GPT](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)
    was announced. *Note: The term “GPT” was never mentioned in the scientific paper
    describing the first GPT. Arguably, we could say that “GPT-1” never existed. To
    the best of my knowledge, it was also never released.*'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '[GPT-2在首次发布GPT后的几个月内推出](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)。*注：在描述首次GPT的科学论文中从未提到“GPT”这个术语。可以说，“GPT-1”实际上并不存在。据我所知，它也从未发布。*'
- en: '*What is the difference between GPT and GPT-2?*'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '*GPT和GPT-2有什么区别？*'
- en: The scale. GPT-2 is much larger than GPT.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 规模。GPT-2比GPT大得多。
- en: GPT was trained on the BookCorpus which contains 7,000 books. The model has
    120 million parameters.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: GPT是在包含7000本书的BookCorpus上进行训练的。该模型有1.2亿个参数。
- en: What’s a parameter?
  id: totrans-40
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 什么是参数？
- en: ''
  id: totrans-41
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: A parameter is a variable learned during the model training. Typically, a model
    with more parameters is bigger and better.
  id: totrans-42
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 参数是模型训练过程中学习到的变量。通常，参数更多的模型更大，表现更好。
- en: 120 million was a huge number in 2018.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 120百万在2018年是一个巨大的数字。
- en: With GPT-2, OpenAI proposed an even bigger model containing 1.5 billion parameters.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 借助GPT-2，OpenAI提出了一个包含15亿参数的更大模型。
- en: It was trained on an undisclosed corpus called WebText. This corpus is 10 times
    larger than BookCorpus ([according to the paper describing GPT-2](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)).
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 它是在一个未公开的语料库WebText上进行训练的。这个语料库比BookCorpus大10倍（[根据描述GPT-2的论文](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)）。
- en: 'OpenAI gradually released 4 versions of GPT-2:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI逐步发布了4个版本的GPT-2：
- en: 'small: 124 million parameters'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'small: 124百万参数'
- en: 'medium: 355 million parameters'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'medium: 355百万参数'
- en: 'large: 774 million parameters'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'large: 774百万参数'
- en: 'xl: 1.5 billion parameters'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'xl: 15亿参数'
- en: They are [all publicly available](https://openai.com/research/gpt-2-1-5b-release)
    and can be used in commercial products.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 它们[都是公开的](https://openai.com/research/gpt-2-1-5b-release)，可以用于商业产品中。
- en: While GPT-2-XL excels at generating fluent text in the wild, i.e., without any
    particular instructions or fine-tuning, it remains far less powerful than more
    recent GPT models for specific tasks.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然GPT-2-XL在生成自然流畅的文本方面表现出色，即没有任何特定的指令或微调，但在特定任务上仍然远不如更新的GPT模型强大。
- en: The release of GPT-2-XL was the last open release of a GPT model by OpenAI.
    GPT-3 and GPT-4 can only be used through OpenAI’s API.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-2-XL的发布是OpenAI最后一次公开发布的GPT模型。GPT-3和GPT-4只能通过OpenAI的API使用。
- en: GPT-3
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: GPT-3
- en: GPT-3 was announced in 2020\. With its 175 billion parameters, it was an even
    bigger jump from GPT-2 than GPT-2 from the first GPT.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-3于2020年发布。其拥有1750亿参数，比GPT-2的跳跃更大。
- en: '[This is also from GPT-3 that OpenAI stopped to disclose precise training information
    about GPT models.](https://arxiv.org/pdf/2005.14165.pdf)'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '[这也是因为OpenAI停止公开GPT模型的精确训练信息。](https://arxiv.org/pdf/2005.14165.pdf)'
- en: Today, there are 7 GPT-3 models available through OpenAI’s API but we only know
    little about them.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 今天，通过OpenAI的API提供了7个GPT-3模型，但我们对它们了解甚少。
- en: With GPT-3, OpenAI demonstrated that GPT models can be extremely good for specific
    language generation tasks if the users provide a few examples of the task they
    want the model to achieve.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 借助GPT-3，OpenAI展示了如果用户提供一些他们希望模型完成的任务示例，GPT模型可以在特定的语言生成任务中表现得极为出色。
- en: GPT-3.5
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: GPT-3.5
- en: With the GPT-3 models running in the API and attracting more and more users,
    OpenAI could collect a very large dataset of user inputs.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 随着GPT-3模型在API中运行并吸引越来越多的用户，OpenAI能够收集到一个非常大的用户输入数据集。
- en: They exploited these inputs to further improve their models.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 他们利用这些输入进一步改进了他们的模型。
- en: They used a technique called [reinforcement learning from human feedback (RLHF)](https://openai.com/blog/deep-reinforcement-learning-from-human-preferences/).
    I won’t explain the details here but you can find them in [a blog post](https://openai.com/research/instruction-following)
    published by OpenAI.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 他们使用了一种叫做[人类反馈强化学习（RLHF）](https://openai.com/blog/deep-reinforcement-learning-from-human-preferences/)的技术。我不会在这里详细解释，但你可以在OpenAI发布的[一篇博客文章](https://openai.com/research/instruction-following)中找到这些细节。
- en: In a nutshell, thanks to RLHF, GPT-3.5 is much better at following user instructions
    than GPT-3\. OpenAI denotes this class of GPT models as “instructGPT”.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，得益于 RLHF，GPT-3.5 在遵循用户指令方面比 GPT-3 好得多。OpenAI 将这类 GPT 模型称为“instructGPT”。
- en: With GPT-3.5, you can “prompt” the model to perform a specific task without
    the need to give it examples of the task. You just have to write the “right” prompt
    to get the best result. This is where “prompt engineering” becomes important and
    why skilled [prompt engineers are receiving incredible job offers](https://www.businessinsider.com/ai-prompt-engineer-jobs-pay-salary-requirements-no-tech-background-2023-3).
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 GPT-3.5，你可以“提示”模型执行特定任务，而无需给它任务的示例。你只需写出“正确”的提示以获得最佳结果。这就是“提示工程”变得重要的地方，也是为什么熟练的[提示工程师正在获得令人难以置信的工作机会](https://www.businessinsider.com/ai-prompt-engineer-jobs-pay-salary-requirements-no-tech-background-2023-3)。
- en: GPT-3.5 is the current model used to power ChatGPT.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-3.5 是当前用于驱动 ChatGPT 的模型。
- en: GPT-4
  id: totrans-66
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: GPT-4
- en: GPT-4 has been released in March 2023.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-4 于 2023 年 3 月发布。
- en: We know almost nothing about its training.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 我们几乎对其训练过程一无所知。
- en: 'The main difference with GPT-3/GPT-3.5 is that GPT-4 is bimodal: It can take
    as input images and text.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 与 GPT-3/GPT-3.5 的主要区别在于 GPT-4 是双模态的：它可以接收图像和文本作为输入。
- en: 'It can generate text but won’t directly generate images. *Note: GPT-4 can generate
    the code that can generate an image, or retrieve one from the Web.*'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 它可以生成文本，但不会直接生成图像。*注意：GPT-4 可以生成生成图像的代码，或者从网络上检索图像。*
- en: At the time of writing these lines, GPT-4 is still in a “limited beta”.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在撰写这些文字时，GPT-4 仍处于“有限 beta”阶段。
- en: ChatGPT
  id: totrans-72
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ChatGPT
- en: ChatGPT is just a user interface with chat functionalities. When you write something
    with ChatGPT, it’s a GPT-3.5 model that generates the answer.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: ChatGPT 只是一个具有聊天功能的用户界面。当你在 ChatGPT 中写东西时，生成答案的是一个 GPT-3.5 模型。
- en: 'A particularity of ChatGPT is that it’s not just taking as input the current
    query of the user as an out-of-the-box GPT model would do. To properly work as
    a chat engine, ChatGPT must keep track of the conversation: What has been said,
    what is the user goal, etc.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: ChatGPT 的一个特点是，它不仅仅是像开箱即用的 GPT 模型那样接收用户的当前查询。为了作为聊天引擎正常工作，ChatGPT 必须跟踪对话：已经说了什么，用户的目标是什么，等等。
- en: OpenAI didn’t disclose how it does that. Given that GPT models can only accept
    a prompt of a limited length (I’ll explain this later), ChatGPT can’t just concatenate
    all the dialogue turns together to put them in the same prompt. This kind of prompt
    could be way too large to be handled by GPT-3.5.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI 并没有透露它是如何做到这一点的。鉴于 GPT 模型只能接受有限长度的提示（我稍后会解释这一点），ChatGPT 不能简单地将所有对话回合串联在一起放在同一个提示中。这种提示可能会过大，GPT-3.5
    无法处理。
- en: How to use GPT models?
  id: totrans-76
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何使用 GPT 模型？
- en: 'You can easily get GPT-2 models online and use them on your computer. If you
    want to run large language models on your machine, you may be interested in reading
    my tutorial:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以轻松地在网上获取 GPT-2 模型并在计算机上使用它们。如果你想在你的机器上运行大型语言模型，你可能会对阅读我的教程感兴趣：
- en: '[](https://pub.towardsai.net/run-very-large-language-models-on-your-computer-390dd33838bb?source=post_page-----e02b093a495b--------------------------------)
    [## Run Very Large Language Models on Your Computer'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://pub.towardsai.net/run-very-large-language-models-on-your-computer-390dd33838bb?source=post_page-----e02b093a495b--------------------------------)
    [## 在你的计算机上运行非常大的语言模型'
- en: With PyTorch and Hugging Face’s device_map
  id: totrans-79
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用 PyTorch 和 Hugging Face 的 device_map
- en: pub.towardsai.net](https://pub.towardsai.net/run-very-large-language-models-on-your-computer-390dd33838bb?source=post_page-----e02b093a495b--------------------------------)
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: pub.towardsai.net](https://pub.towardsai.net/run-very-large-language-models-on-your-computer-390dd33838bb?source=post_page-----e02b093a495b--------------------------------)
- en: For GPT-3 and GPT-3.5, we have no other choice than to use OpenAI’s API. You
    will first need to create an OpenAI account on [their website](https://openai.com/).
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 GPT-3 和 GPT-3.5，我们别无选择，只能使用 OpenAI 的 API。你首先需要在[他们的网站](https://openai.com/)上创建一个
    OpenAI 账户。
- en: Once you have an account, you can start playing with the models inside the “playground”
    which is a sandbox that OpenAI proposes to experiment with the models. You can
    access it only when you are logged in.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你有了账户，你可以开始在“playground”（这是 OpenAI 提供的实验模型的沙盒）中玩耍。只有在登录后，你才能访问它。
- en: If you want to directly use the models in your application, OpenAI and the open-source
    community offer libraries in many languages, such as [Python, Node.js, and PHP,](https://platform.openai.com/docs/libraries)
    to call the models using OpenAI API.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想在你的应用程序中直接使用这些模型，OpenAI 和开源社区提供了许多语言的库，如[Python、Node.js 和 PHP](https://platform.openai.com/docs/libraries)，以通过
    OpenAI API 调用模型。
- en: 'You can create and get your OpenAI API key in your OpenAI account. *Note: Keep
    this key secret. Anyone who has it can consume your OpenAI credits.*'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在你的 OpenAI 账户中创建并获取你的 OpenAI API 密钥。*注意：请保密此密钥。任何拥有它的人都可以消耗你的 OpenAI 额度。*
- en: Each model has different settings that you can adjust. Be aware that GPT models
    are *non-deterministic.* If you prompt a model twice with the same prompt there
    is a high chance that you will have two close but different answers.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 每个模型有不同的设置，你可以进行调整。请注意，GPT 模型是*非确定性的*。如果你用相同的提示两次调用模型，很有可能会得到两个相似但不同的回答。
- en: '*Note: If you want to reduce the variations between answers given the same
    prompt, you can set to 0 the “temperature” parameter of the model. As a side effect,
    it will also significantly decrease the diversity of the answers, in other words,
    the generated text may be more redundant.*'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '*注意：如果你想减少相同提示下回答的变异性，可以将模型的“温度”参数设置为0。副作用是，它也会显著减少答案的多样性，换句话说，生成的文本可能会更冗余。*'
- en: You will also have to care about the “maximum content length”. This is the length
    of your prompt in addition to the length of the answer generated by GPT. For instance,
    GPT-3.5-turbo has a “maximum content length” of 4,096 *tokens*.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 你还需要注意“最大内容长度”。这是你的提示长度加上 GPT 生成的回答长度。例如，GPT-3.5-turbo 的“最大内容长度”是 4,096 *令牌*。
- en: '**A token is not a word.**'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '**一个令牌不是一个单词。**'
- en: A token is the minimal unit of text used by the GPT models to generate text.
    Yes, GPT models are not exactly word generators but rather token generators. A
    token can be a character, a piece of word, a word, or even a sequence of words
    for some languages.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 令牌是 GPT 模型用于生成文本的最小单位。是的，GPT 模型并非真正的单词生成器，而是令牌生成器。令牌可以是一个字符、一个词的一部分、一个单词，甚至是某些语言中的词组。
- en: OpenAI gives an example in the [API documentation](https://platform.openai.com/docs/guides/chat/introduction).
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI 在[API 文档](https://platform.openai.com/docs/guides/chat/introduction)中给出了一个示例。
- en: '`*"ChatGPT is great!"*` *is encoded into six tokens:* `*["Chat", "G", "PT",
    " is", " great", "!"]*`*.*'
  id: totrans-91
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`*"ChatGPT is great!"*` *被编码成六个令牌：* `*["Chat", "G", "PT", " is", " great",
    "!"]*`*。*'
- en: As a rule of thumb, count that 750 English words yield 1,000 tokens.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 一般而言，750 个英语单词约等于 1,000 个令牌。
- en: In my opinion, managing the “maximum content length” is the most tedious part
    of working with the OpenAI API. First, there is no straightforward way to know
    how many tokens your prompt contains. Then, you can’t know in advance how many
    tokens will be in the answer of the model.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 我认为，管理“最大内容长度”是使用 OpenAI API 中最繁琐的部分。首先，没有简单的方法来知道你的提示包含多少个令牌。然后，你不能提前知道模型的回答将包含多少个令牌。
- en: You have to guess. And you can only guess right if you have some experience
    with the models. I recommend experimenting a lot with them to better gauge how
    long can be the answers given your prompts.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 你需要猜测。只有当你有一定的模型经验时，才能猜对。我建议你多进行实验，以更好地评估在给定提示的情况下，回答可能有多长。
- en: If your prompt is too long, the answer will be cut-off.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你的提示太长，回答将会被截断。
- en: I won’t give more details about the API here as it can become quite technical.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 我不会在这里提供关于 API 的更多细节，因为它可能变得相当技术性。
- en: Limitations of GPT models
  id: totrans-97
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: GPT 模型的局限性
- en: GPT models are only token generators trained on the Web. They are biased by
    the content they were trained on and thus cannot be considered fully safe.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: GPT 模型仅仅是基于网络训练的令牌生成器。它们受限于训练数据的内容，因此不能被认为是完全安全的。
- en: Since GPT-3.5, OpenAI has trained its model to avoid answering harmful content.
    To achieve this, they used machine learning techniques and consequently this “self-moderation”
    of the model can’t be 100% trusted.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 自 GPT-3.5 起，OpenAI 已训练其模型以避免回答有害内容。为实现这一目标，他们使用了机器学习技术，因此这种“自我调节”无法100%被信任。
- en: This self-moderation may work for a given prompt, but may then completely fail
    after just changing one word in this prompt.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 这种自我调节可能对某个特定提示有效，但在仅仅改变一个词后，可能会完全失效。
- en: I also recommend reading the [Terms of Use](https://openai.com/policies/terms-of-use)
    of OpenAI products. In this document, the limitations of GPT models appear more
    clearly in my opinion.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 我还建议阅读 OpenAI 产品的[使用条款](https://openai.com/policies/terms-of-use)。在这份文档中，我认为
    GPT 模型的局限性更为清晰。
- en: 'If you plan to build your application with the API, you should particularly
    pay attention to this point:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你计划使用 API 构建应用程序，你应该特别注意这一点：
- en: You must be at least 13 years old to use the Services. If you are under 18 you
    must have your parent or legal guardian’s permission to use the Services. If you
    use the Services on behalf of another person or entity, you must have the authority
    to accept the Terms on their behalf. You must provide accurate and complete information
    to register for an account. You may not make your access credentials or account
    available to others outside your organization, and you are responsible for all
    activities that occur using your credentials.
  id: totrans-103
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 使用这些服务必须年满 13 岁。如果你未满 18 岁，必须获得父母或法定监护人的许可才能使用这些服务。如果你代表其他人或实体使用这些服务，你必须有权接受其条款。你必须提供准确和完整的信息来注册账户。你不得将访问凭证或账户提供给你组织以外的其他人，你对使用你的凭证发生的所有活动负责。
- en: Italy temporarily banned ChatGPT because it may generate inappropriate answers
    for people under 18, among other reasons.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 意大利暂时禁止使用 ChatGPT，因为它可能会生成不适合 18 岁以下人群的回答，以及其他原因。
- en: '[](https://medium.com/@bnjmn_marie/italy-bans-chatgpt-europe-may-follow-c7222112f97e?source=post_page-----e02b093a495b--------------------------------)
    [## Italy Bans ChatGPT, Europe May Follow'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '[## 意大利禁止 ChatGPT，欧洲可能跟随](https://medium.com/@bnjmn_marie/italy-bans-chatgpt-europe-may-follow-c7222112f97e?source=post_page-----e02b093a495b--------------------------------)'
- en: Towards a boom of ChatGPT wrappers “Ready for Italy”
  id: totrans-106
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 面向 ChatGPT 包装器的繁荣“准备好迎接意大利”
- en: medium.com](https://medium.com/@bnjmn_marie/italy-bans-chatgpt-europe-may-follow-c7222112f97e?source=post_page-----e02b093a495b--------------------------------)
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '[medium.com](https://medium.com/@bnjmn_marie/italy-bans-chatgpt-europe-may-follow-c7222112f97e?source=post_page-----e02b093a495b--------------------------------)'
- en: If you are a developer building an application on top of OpenAI API, you must
    check the age of your users.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你是一个开发者，并在 OpenAI API 的基础上构建应用程序，你必须检查用户的年龄。
- en: OpenAI also published a list of [usage policies pointing out all the prohibited
    usage of the models](https://openai.com/policies/usage-policies).
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI 还发布了一份[使用政策清单，指出了所有禁止使用模型的情况](https://openai.com/policies/usage-policies)。
- en: Conclusion
  id: totrans-110
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: GPT models are very simple models and their architecture didn’t evolve much
    since 2018\. But when you train a simple model at a large scale on the right data
    and with the right hyperparameters, you can get extremely powerful AI models such
    as GPT-3 and GPT-4.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: GPT 模型非常简单，其架构自 2018 年以来没有发生太大变化。但当你在大规模合适数据上训练一个简单模型，并使用合适的超参数时，你可以获得像 GPT-3
    和 GPT-4 这样的极其强大的 AI 模型。
- en: They are so powerful that we have not nearly explored all their potential.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 它们如此强大，以至于我们几乎没有完全探索其所有潜力。
- en: While recent GPT models are not open-source, they remain easy to use with OpenAI’s
    API. You can also play with them through [ChatGPT](https://chat.openai.com/chat).
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管最近的 GPT 模型不是开源的，但通过 OpenAI 的 API 使用它们仍然很容易。你也可以通过[ChatGPT](https://chat.openai.com/chat)来体验这些模型。
