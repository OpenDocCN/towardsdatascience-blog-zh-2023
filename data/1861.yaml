- en: Utilizing PyArrow to Improve pandas and Dask Workflows
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 利用 PyArrow 改进 pandas 和 Dask 工作流
- en: 原文：[https://towardsdatascience.com/utilizing-pyarrow-to-improve-pandas-and-dask-workflows-2891d3d96d2b?source=collection_archive---------4-----------------------#2023-06-06](https://towardsdatascience.com/utilizing-pyarrow-to-improve-pandas-and-dask-workflows-2891d3d96d2b?source=collection_archive---------4-----------------------#2023-06-06)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/utilizing-pyarrow-to-improve-pandas-and-dask-workflows-2891d3d96d2b?source=collection_archive---------4-----------------------#2023-06-06](https://towardsdatascience.com/utilizing-pyarrow-to-improve-pandas-and-dask-workflows-2891d3d96d2b?source=collection_archive---------4-----------------------#2023-06-06)
- en: '*Get the most out of PyArrow support in pandas and Dask right now*'
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '*现在最大限度地利用 PyArrow 在 pandas 和 Dask 中的支持*'
- en: '[](https://medium.com/@patrick_hoefler?source=post_page-----2891d3d96d2b--------------------------------)[![Patrick
    Hoefler](../Images/35ca9ef1100d8c93dbadd374f0569fe1.png)](https://medium.com/@patrick_hoefler?source=post_page-----2891d3d96d2b--------------------------------)[](https://towardsdatascience.com/?source=post_page-----2891d3d96d2b--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----2891d3d96d2b--------------------------------)
    [Patrick Hoefler](https://medium.com/@patrick_hoefler?source=post_page-----2891d3d96d2b--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@patrick_hoefler?source=post_page-----2891d3d96d2b--------------------------------)[![Patrick
    Hoefler](../Images/35ca9ef1100d8c93dbadd374f0569fe1.png)](https://medium.com/@patrick_hoefler?source=post_page-----2891d3d96d2b--------------------------------)[](https://towardsdatascience.com/?source=post_page-----2891d3d96d2b--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----2891d3d96d2b--------------------------------)
    [Patrick Hoefler](https://medium.com/@patrick_hoefler?source=post_page-----2891d3d96d2b--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F103b3417e0f5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Futilizing-pyarrow-to-improve-pandas-and-dask-workflows-2891d3d96d2b&user=Patrick+Hoefler&userId=103b3417e0f5&source=post_page-103b3417e0f5----2891d3d96d2b---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----2891d3d96d2b--------------------------------)
    ·13 min read·Jun 6, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F2891d3d96d2b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Futilizing-pyarrow-to-improve-pandas-and-dask-workflows-2891d3d96d2b&user=Patrick+Hoefler&userId=103b3417e0f5&source=-----2891d3d96d2b---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F103b3417e0f5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Futilizing-pyarrow-to-improve-pandas-and-dask-workflows-2891d3d96d2b&user=Patrick+Hoefler&userId=103b3417e0f5&source=post_page-103b3417e0f5----2891d3d96d2b---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----2891d3d96d2b--------------------------------)
    · 13 min 阅读 · 2023年6月6日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F2891d3d96d2b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Futilizing-pyarrow-to-improve-pandas-and-dask-workflows-2891d3d96d2b&user=Patrick+Hoefler&userId=103b3417e0f5&source=-----2891d3d96d2b---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F2891d3d96d2b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Futilizing-pyarrow-to-improve-pandas-and-dask-workflows-2891d3d96d2b&source=-----2891d3d96d2b---------------------bookmark_footer-----------)![](../Images/1fd9277102c552e00b0d50732a3c12cd.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F2891d3d96d2b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Futilizing-pyarrow-to-improve-pandas-and-dask-workflows-2891d3d96d2b&source=-----2891d3d96d2b---------------------bookmark_footer-----------)![](../Images/1fd9277102c552e00b0d50732a3c12cd.png)'
- en: All images were created by the author
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 所有图片均由作者创建
- en: Introduction
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 介绍
- en: This post investigates where we can use PyArrow to improve our pandas and Dask
    workflows right now. General support for PyArrow dtypes was added with pandas
    2.0 to [pandas](https://pandas.pydata.org) and [Dask](https://www.dask.org/?utm_source=tds&utm_medium=pyarrow-in-pandas-and-dask).
    This solves a bunch of long-standing pains for users of both libraries. pandas
    users often complain to me that pandas does not support missing values in arbitrary
    dtypes or that non-standard dtypes are not very well supported. A particularly
    annoying problem for Dask users is running out of memory with large datasets.
    PyArrow backed string columns consume up to 70% less memory compared to NumPy
    object columns and thus have the potential to mitigate this problem as well as
    providing a huge performance improvement.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 本文探讨了我们现在可以在何处使用 PyArrow 来改进我们的 pandas 和 Dask 工作流。对 PyArrow 数据类型的总体支持在 pandas
    2.0 中添加到 [pandas](https://pandas.pydata.org) 和 [Dask](https://www.dask.org/?utm_source=tds&utm_medium=pyarrow-in-pandas-and-dask)
    中。这解决了两个库用户长期以来的一些痛点。pandas 用户常常向我抱怨 pandas 不支持任意数据类型中的缺失值，或非标准数据类型支持不够好。Dask
    用户特别烦恼的问题是处理大数据集时内存不足。与 NumPy 对象列相比，PyArrow 支持的字符串列可以减少多达 70% 的内存消耗，因此有潜力缓解这个问题，并提供巨大的性能提升。
- en: Support for PyArrow dtypes in pandas, and by extension Dask, is still relatively
    new. I would recommend caution when opting into the PyArrow `dtype_backend` until
    at least pandas 2.1 is released. Not every part of both APIs is optimized yet.
    You should be able to get a big improvement in certain workflows though. This
    post will go over a couple of examples where I’d recommend switching to PyArrow
    right away, because it already provides huge benefits.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: pandas 和 Dask 对 PyArrow 数据类型的支持仍然相对较新。建议在至少 pandas 2.1 发布之前，谨慎选择 PyArrow 的 `dtype_backend`。并非所有
    API 部分都已优化。然而，在某些工作流中，你应该能获得显著的改进。本文将介绍几个我建议立即切换到 PyArrow 的示例，因为它已经提供了巨大的好处。
- en: Dask itself can benefit in various ways from PyArrow dtypes. We will investigate
    how PyArrow backed strings can easily mitigate the pain point of running out of
    memory on Dask clusters and how we can improve performance through utilizing PyArrow.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: Dask 本身可以通过 PyArrow 数据类型在多方面受益。我们将探讨 PyArrow 支持的字符串如何轻松缓解 Dask 集群内存不足的问题，以及我们如何通过利用
    PyArrow 改善性能。
- en: I am part of the pandas core team and was heavily involved in implementing and
    improving PyArrow support in pandas. I’ve recently joined [Coiled](https://www.coiled.io/?utm_source=tds&utm_medium=pyarrow-in-pandas-and-dask)
    where I am working on Dask. One of my tasks is improving the PyArrow integration
    in Dask.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 我是 pandas 核心团队的一员，并且在实现和改进 pandas 中的 PyArrow 支持方面参与了大量工作。我最近加入了 [Coiled](https://www.coiled.io/?utm_source=tds&utm_medium=pyarrow-in-pandas-and-dask)，在那里我负责
    Dask。我的任务之一是改进 Dask 中的 PyArrow 集成。
- en: General overview of PyArrow support
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: PyArrow 支持的总体概述
- en: PyArrow dtypes were initially introduced in pandas 1.5\. The implementation
    was experimental and I wouldn’t recommend using it on pandas 1.5.x. Support for
    them is still relatively new. pandas 2.0 provides a huge improvement, including
    making opting into PyArrow backed DataFrames easy. We are still working on supporting
    them properly everywhere, and thus they should be used with caution until at least
    pandas 2.1 is released. Both projects work continuously to improve support throughout
    Dask and pandas.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: PyArrow 数据类型最初是在 pandas 1.5 中引入的。该实现是实验性的，我不建议在 pandas 1.5.x 上使用它。对这些数据类型的支持仍然相对较新。pandas
    2.0 提供了巨大的改进，包括使选择 PyArrow 支持的 DataFrames 变得容易。我们仍在努力在各处提供适当的支持，因此在至少 pandas 2.1
    发布之前应谨慎使用。这两个项目都在不断努力改进对 Dask 和 pandas 的支持。
- en: We encourage users to try them out! This will help us to get a better idea of
    what is still lacking support or is not fast enough. Giving feedback helps us
    improve support and will drastically reduce the time that is necessary to create
    a smooth user experience.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 我们鼓励用户尝试使用它们！这将帮助我们更好地了解仍然缺乏支持或不够快的地方。反馈有助于我们改进支持，并将显著减少创建流畅用户体验所需的时间。
- en: Dataset
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据集
- en: 'We will use the taxi dataset from New York City that contains all Uber and
    Lyft rides. It has some interesting attributes like price, tips, driver pay and
    many more. The dataset can be found [here](https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page)
    (see [terms of service](https://www.nyc.gov/home/terms-of-use.page)) and is stored
    in parquet files. When analyzing Dask queries, we will use a publicly available
    S3 bucket to simplify our queries: `s3://coiled-datasets/uber-lyft-tlc/`. We will
    use the dataset from December 2022 for our pandas queries, since this is the maximum
    that fits comfortably into memory on my machine (24GB of RAM). We have to avoid
    stressing our RAM usage, since this might introduce side effects when analyzing
    performance.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用包含所有 Uber 和 Lyft 乘车记录的纽约市出租车数据集。它具有一些有趣的属性，如价格、小费、司机薪酬等。数据集可以在[这里](https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page)找到（参见
    [服务条款](https://www.nyc.gov/home/terms-of-use.page)），并存储在 parquet 文件中。在分析 Dask
    查询时，我们将使用一个公开的 S3 存储桶以简化我们的查询：`s3://coiled-datasets/uber-lyft-tlc/`。我们将使用 2022
    年 12 月的数据集进行 pandas 查询，因为这是在我的机器（24GB 内存）中舒适地适配的最大数据集。我们必须避免过度使用 RAM，因为这可能会在分析性能时引入副作用。
- en: We will also investigate the performance of `read_csv`. We will use the *Crimes
    in Chicago* dataset that can be found [here](https://www.kaggle.com/datasets/utkarshx27/crimes-2001-to-present).
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还将研究 `read_csv` 的性能。我们将使用可以在[这里](https://www.kaggle.com/datasets/utkarshx27/crimes-2001-to-present)找到的
    *芝加哥犯罪* 数据集。
- en: Dask cluster
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Dask 集群
- en: 'There are various different options to set up a Dask cluster, see the [Dask
    documentation](https://docs.dask.org/en/stable/deploying.html?utm_source=tds&utm_medium=pyarrow-in-pandas-and-dask)
    for a non-exhaustive list of deployment options. I will use [Coiled](https://docs.coiled.io/user_guide/index.html?utm_source=tds&utm_medium=pyarrow-in-pandas-and-dask)
    to create a cluster on AWS with 30 machines through:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 设置 Dask 集群有各种不同的选项，参见 [Dask 文档](https://docs.dask.org/en/stable/deploying.html?utm_source=tds&utm_medium=pyarrow-in-pandas-and-dask)
    获取非详尽的部署选项列表。我将使用 [Coiled](https://docs.coiled.io/user_guide/index.html?utm_source=tds&utm_medium=pyarrow-in-pandas-and-dask)
    在 AWS 上通过 30 台机器创建一个集群。
- en: '[PRE0]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Coiled is connected to my AWS account. It creates the cluster within my account
    and manages all resources for me. 30 machines are enough to operate on our dataset
    comfortably. We will investigate how we can reduce the required number of workers
    to 15 through some small modifications.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: Coiled 已连接到我的 AWS 账户。它在我的账户中创建集群并为我管理所有资源。30 台机器足以舒适地操作我们的数据集。我们将研究如何通过一些小的修改将所需的工作节点数减少到
    15 台。
- en: pandas `StringDtype` backed by PyArrow
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: pandas `StringDtype` 由 PyArrow 支持
- en: We begin with a feature that was originally introduced over 3 years ago in pandas
    1.0\. Setting the dtype in pandas or Dask to `string` returns an object with `StringDtype`.
    This feature is relatively mature and should provide a smooth user experience.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从 pandas 1.0 中最初引入的一个功能开始。将 pandas 或 Dask 中的 dtype 设置为 `string` 会返回一个具有 `StringDtype`
    的对象。这个功能相对成熟，应该能提供平滑的用户体验。
- en: Historically, pandas represented string data through NumPy arrays with dtype
    `object`. NumPy object data is stored as an array of pointers pointing to the
    actual data in memory. This makes iterating over an array containing strings very
    slow. pandas 1.0 initially introduced said `StringDtype` that allowed easier and
    consistent operations on strings. This dtype was still backed by Python strings
    and thus, wasn’t very performant either. Rather, it provided a clear abstraction
    of string data.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 历史上，pandas 通过 dtype 为 `object` 的 NumPy 数组来表示字符串数据。NumPy 对象数据存储为指向内存中实际数据的指针数组。这使得遍历包含字符串的数组非常缓慢。pandas
    1.0 最初引入了所谓的 `StringDtype`，允许对字符串进行更简单和一致的操作。这个 dtype 仍然由 Python 字符串支持，因此性能也不佳。它提供了字符串数据的清晰抽象。
- en: 'pandas 1.3 finally introduced an enhancement to create an efficient string
    dtype. This datatype is backed by PyArrow arrays. [PyArrow](https://arrow.apache.org/docs/python/index.html)
    provides a data structure that enables performant and memory efficient string
    operations. Starting from that point on, users could use a string dtype that was
    contiguous in memory and thus very fast. This dtype can be requested through `string[pyarrow]`.
    Alternatively, we can request it by specifying `string` as the dtype and setting:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: pandas 1.3 最终引入了一种创建高效字符串 dtype 的增强功能。该数据类型由 PyArrow 数组支持。[PyArrow](https://arrow.apache.org/docs/python/index.html)
    提供了一种数据结构，能够进行高效且节省内存的字符串操作。从那时起，用户可以使用在内存中连续的字符串 dtype，因此非常快速。该 dtype 可以通过 `string[pyarrow]`
    请求。或者，我们可以通过指定 `string` 作为 dtype 并设置来请求它：
- en: '[PRE1]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Since Dask builds on top of pandas, this string dtype is available here as well.
    On top of that, Dask offers a convenient option that automatically converts all
    string-data to `string[pyarrow]`.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 Dask 构建在 pandas 之上，这里也可以使用这种字符串数据类型。除此之外，Dask 还提供了一个方便的选项，可以自动将所有字符串数据转换为
    `string[pyarrow]`。
- en: '[PRE2]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: This is a convenient way of avoiding NumPy object dtype for string columns.
    Additionally, it has the advantage that it creates PyArrow arrays natively for
    I/O methods that operate with Arrow objects. On top of providing huge performance
    improvements, PyArrow strings consume significantly less memory. An average Dask
    DataFrame with PyArrow strings consumes around 33–50% of the original memory compared
    to NumPy object. This solves the biggest pain point for Dask users that is running
    out of memory when operating on large datasets. The option enables global testing
    in Dask’s test suite. This ensures that PyArrow backed strings are mature enough
    to provide a smooth user experience.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 这是避免字符串列使用 NumPy 对象数据类型的方便方法。此外，它还具有创建原生 PyArrow 数组的优势，适用于处理 Arrow 对象的 I/O 方法。在提供显著性能提升的同时，PyArrow
    字符串消耗的内存显著减少。与 NumPy 对象相比，使用 PyArrow 字符串的平均 Dask DataFrame 的内存消耗约为原始内存的 33-50%。这解决了
    Dask 用户在处理大型数据集时内存不足的最大痛点。该选项启用了 Dask 测试套件中的全局测试。这确保了 PyArrow 支持的字符串足够成熟，以提供流畅的用户体验。
- en: Let’s look at a few operations that represent typical string operations. We
    will start with a couple of pandas examples before switching over to operations
    on our Dask cluster.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看几个典型的字符串操作。我们将从一些 pandas 示例开始，然后切换到在 Dask 集群上执行的操作。
- en: We will use `df.convert_dtypes` to convert our object columns to PyArrow string
    arrays. There are more efficient ways of getting PyArrow dtypes in pandas that
    we will explore later. We will use the Uber-Lyft dataset from December 2022, this
    file fits comfortably into memory on my machine.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用 `df.convert_dtypes` 将我们的对象列转换为 PyArrow 字符串数组。还有更高效的方法来获取 pandas 中的 PyArrow
    数据类型，我们会在后面探讨。我们将使用 2022 年 12 月的 Uber-Lyft 数据集，这个文件在我的机器上可以舒适地放入内存中。
- en: '[PRE3]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Our DataFrame has NumPy dtypes for all non-string columns in this example. Let’s
    start with filtering for all rides that were operated by Uber.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们的 DataFrame 对所有非字符串列都有 NumPy 数据类型。让我们开始筛选所有由 Uber 操作的乘车记录。
- en: '[PRE4]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: This operation creates a mask with True/False values that specify whether Uber
    operated a ride. This doesn’t utilize any special string methods, but the equality
    comparison dispatches to PyArrow. Next, we will use the String accessor that is
    implemented in pandas and gives you access to all kinds of string operations on
    a per-element basis. We want to find all rides that were dispatched from a base
    starting with `"B028"`.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 这个操作创建了一个 True/False 值的掩码，指定 Uber 是否进行了乘车。这不使用任何特殊的字符串方法，但等式比较会交给 PyArrow。接下来，我们将使用
    pandas 实现的 String 访问器，它允许你对每个元素执行各种字符串操作。我们想要找到所有从以 `"B028"` 开头的基地发出的乘车记录。
- en: '[PRE5]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '`startswith` iterates over our array and checks whether every string starts
    with the specified substring. The advantage of PyArrow is easy to see. The data
    are contiguous in memory, which means that we can efficiently iterate over them.
    Additionally, these arrays have a second array with pointers that point to the
    first memory address of every string, which makes computing the starting sequence
    even faster.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '`startswith` 会遍历我们的数组，检查每个字符串是否以指定的子字符串开头。PyArrow 的优势显而易见。数据在内存中是连续的，这意味着我们可以高效地进行遍历。此外，这些数组还具有一个第二个数组，其中包含指向每个字符串首个内存地址的指针，这使得计算起始序列更快。'
- en: Finally, we look at a `GroupBy` operation that groups over PyArrow string columns.
    The calculation of the groups can dispatch to PyArrow as well, which is more efficient
    than factorizing over a NumPy object array.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们查看一个 `GroupBy` 操作，它对 PyArrow 字符串列进行分组。分组的计算也可以交给 PyArrow，这比在 NumPy 对象数组上因子化要高效得多。
- en: '[PRE6]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Let’s look at how these operations stack up against DataFrames where string
    columns are represented by NumPy object dtype.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看这些操作与字符串列由 NumPy 对象数据类型表示的 DataFrames 的对比。
- en: '![](../Images/4194a8e1e1222a4c91881bdafb5fd7f2.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4194a8e1e1222a4c91881bdafb5fd7f2.png)'
- en: The results are more or less as we expected. The string based comparisons are
    significantly faster when performed on PyArrow strings. Most string accessors
    should provide a huge performance improvement. Another interesting observation
    is memory usage, it is reduced by roughly 50% compared to NumPy object dtype.
    We will take a closer look at this with Dask.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 结果或多或少符合我们的预期。基于字符串的比较在 PyArrow 字符串上执行时显著更快。大多数字符串访问器应该能提供巨大的性能提升。另一个有趣的观察是内存使用，与
    NumPy 对象 dtype 相比，减少了大约 50%。我们将进一步用 Dask 详细研究这一点。
- en: 'Dask mirrors the pandas API and dispatches to pandas for most operations. Consequently,
    we can use the same API to access PyArrow strings. A convenient option to request
    these globally is the option mentioned above, which is what we will use here:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: Dask 镜像了 pandas API，并将大多数操作委派给 pandas。因此，我们可以使用相同的 API 来访问 PyArrow 字符串。上述选项是全局请求这些选项的便利方式，我们将在这里使用：
- en: '[PRE7]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: One of the biggest benefits of this option during development is that it enables
    easy testing of PyArrow strings globally in Dask to make sure that everything
    works smoothly. We will utilize the Uber-Lyft dataset for our explorations. The
    dataset takes up around 240GB of memory on our cluster. Our initial cluster has
    30 machines, which is enough to perform our computations comfortably.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在开发过程中，这种选项的最大好处之一是能够轻松地在 Dask 中全局测试 PyArrow 字符串，以确保一切顺利。我们将利用 Uber-Lyft 数据集进行探索。该数据集在我们的集群上占用大约
    240GB 的内存。我们的初始集群有 30 台机器，足以舒适地进行计算。
- en: '[PRE8]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: We persist the data in memory so that I/O performance does not influence our
    performance measurements. Our data is now available in memory, which makes access
    fast. We will perform computations that are similar to our pandas computations.
    One of the main goals is to show that the benefits from pandas will translate
    to computations in a distributed environment with Dask.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将数据保存在内存中，以避免 I/O 性能对我们性能测量的影响。我们的数据现在已存储在内存中，这使得访问速度很快。我们将执行类似于 pandas 计算的操作。主要目标之一是展示
    pandas 的好处如何转化为在 Dask 分布式环境中的计算。
- en: One of the first observations is that the DataFrame with PyArrow backed string
    columns consumes only 130GB of memory, only half of what it consumed with NumPy
    object columns. We have only a few string columns in our DataFrame, which means
    that the memory savings for string columns are actually higher than around 50%
    when switching to PyArrow strings. Consequently, we will reduce the size of our
    cluster to 15 workers when performing our operations on PyArrow string columns.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个观察结果是，使用 PyArrow 支持的字符串列的数据框仅消耗 130GB 内存，仅为使用 NumPy 对象列时的一半。我们的数据框中只有几个字符串列，这意味着在切换到
    PyArrow 字符串时，字符串列的内存节省实际上比约 50% 更高。因此，我们将在对 PyArrow 字符串列执行操作时将集群大小减少到 15 个工作节点。
- en: '[PRE9]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: We measure the performance of the mask-operation and one of the String accessors
    together through subsequent filtering of the DataFrame.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过对数据框进行后续筛选来衡量掩码操作和一个字符串访问器的性能。
- en: '[PRE10]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: We see that we can use the same methods as in our previous example. This makes
    transitioning from pandas to Dask relatively easy.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，可以使用与之前示例中相同的方法。这使得从 pandas 迁移到 Dask 相对容易。
- en: Additionally, we will again compute a `GroupBy` operation on our data. This
    is significantly harder in a distributed environment, which makes the results
    more interesting. The previous operations parallelize relatively easy onto a large
    cluster, while this is harder with `GroupBy`.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们将再次对数据执行 `GroupBy` 操作。这在分布式环境中要困难得多，因此结果更具趣味性。之前的操作在大型集群上相对容易并行，而 `GroupBy`
    则更具挑战性。
- en: '[PRE11]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '![](../Images/a371858da38112b3610ddc9d4bf49499.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a371858da38112b3610ddc9d4bf49499.png)'
- en: We get nice improvements by factors of 2 and 3\. This is especially intriguing
    since we reduced the size of our cluster from 30 machines to 15, reducing the
    cost by 50%. Subsequently, we also reduced our computational resources by a factor
    of 2, which makes our performance improvement even more impressive. Thus, the
    performance improved by a factor of 4 and 6 respectively. We can perform the same
    computations on a smaller cluster, which saves money and is more efficient in
    general, and still get a performance boost out of it.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到了 2 和 3 倍的显著改进。这一点尤其引人注目，因为我们将集群的大小从 30 台机器减少到 15 台，成本降低了 50%。随后，我们还将计算资源减少了
    2 倍，这使得我们的性能提升更加令人印象深刻。因此，性能分别提高了 4 倍和 6 倍。我们可以在较小的集群上执行相同的计算，这样既节省了成本，也在总体上更高效，并且仍然能够获得性能提升。
- en: Summarizing, we saw that PyArrow string-columns are a huge improvement when
    comparing them to NumPy object columns in DataFrames. Switching to PyArrow strings
    is a relatively small change that might improve the performance and efficiency
    of an average workflow that depends on string data immensely. These improvements
    are equally visible in pandas and Dask!
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 总结一下，我们看到 PyArrow 字符串列在与 DataFrame 中的 NumPy 对象列比较时有了巨大的改进。切换到 PyArrow 字符串是一个相对较小的改变，但可能极大地提升依赖于字符串数据的平均工作流程的性能和效率。这些改进在
    pandas 和 Dask 中都同样明显！
- en: Engine keyword in I/O methods
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: I/O 方法中的 Engine 关键字
- en: We will now take a look at I/O functions in pandas and Dask. Some functions
    have custom implementations, like `read_csv`, while others dispatch to another
    library, like `read_excel` to `openpyxl`. Some of these functions gained a new
    `engine` keyword that enables us to dispatch to `PyArrow`. The PyArrow parsers
    are multithreaded by default and hence, can provide a significant performance
    improvement.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将看看 pandas 和 Dask 中的 I/O 函数。一些函数有自定义实现，如 `read_csv`，而其他函数则委派给另一个库，如 `read_excel`
    委派给 `openpyxl`。其中一些函数新增了 `engine` 关键字，使我们能够委派给 `PyArrow`。PyArrow 解析器默认是多线程的，因此可以提供显著的性能提升。
- en: '[PRE12]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: This configuration will return the same results as the other engines. The only
    difference is that PyArrow is used to read the data. The same option is available
    for `read_json`. The PyArrow-engines were added to provide a faster way of reading
    data. The improved speed is only one of the advantages. The PyArrow parsers return
    the data as a [PyArrow Table](https://arrow.apache.org/docs/python/generated/pyarrow.Table.html).
    A PyArrow Table provides built-in functionality to convert to a pandas `DataFrame`.
    Depending on the data, this might require a copy while casting to NumPy (string,
    integers with missing values, ...), which brings an unnecessary slowdown. This
    is where the PyArrow `dtype_backend` comes in. It is implemented as an `ArrowExtensionArray`
    class in pandas, which is backed by a [PyArrow ChunkedArray](https://arrow.apache.org/docs/python/generated/pyarrow.ChunkedArray.html).
    As a direct consequence, the conversion from a PyArrow Table to pandas is extremely
    cheap since it does not require any copies.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 这个配置将返回与其他引擎相同的结果。唯一的区别是使用了 PyArrow 来读取数据。`read_json` 也提供了相同的选项。添加 PyArrow-engines
    是为了提供更快的数据读取方式。改进的速度只是一个优势。PyArrow 解析器返回的数据是一个 [PyArrow Table](https://arrow.apache.org/docs/python/generated/pyarrow.Table.html)。PyArrow
    Table 提供了内置功能来转换为 pandas `DataFrame`。根据数据的不同，这可能需要在转换为 NumPy（字符串、带缺失值的整数等）时进行拷贝，从而带来不必要的减速。这就是
    PyArrow `dtype_backend` 的作用。它在 pandas 中实现为一个 `ArrowExtensionArray` 类，背后是一个 [PyArrow
    ChunkedArray](https://arrow.apache.org/docs/python/generated/pyarrow.ChunkedArray.html)。直接的结果是，从
    PyArrow Table 转换为 pandas 非常便宜，因为它不需要任何拷贝。
- en: '[PRE13]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'This returns a `DataFrame` that is backed by PyArrow arrays. pandas isn''t
    optimized everywhere yet, so this can give you a slowdown in follow-up operations.
    It might be worth it if the workload is particularly I/O heavy. Let''s look at
    a direct comparison:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 这将返回一个由 PyArrow 数组支持的 `DataFrame`。pandas 并未在所有方面都得到优化，因此这可能会在后续操作中造成减速。如果工作负载特别重
    I/O，这可能是值得的。让我们看一下直接比较：
- en: '![](../Images/131457fefa9bd06c86d134bcffb68187.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/131457fefa9bd06c86d134bcffb68187.png)'
- en: We can see that PyArrow-engine and PyArrow dtypes provide a 15x speedup compared
    to the C-engine.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，与 C-engine 相比，PyArrow-engine 和 PyArrow dtypes 提供了 15 倍的加速。
- en: The same advantages apply to Dask. Dask wraps the pandas csv reader and hence,
    gets the same features for free.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 相同的优势适用于 Dask。Dask 封装了 pandas 的 csv 读取器，因此，它免费获得了相同的功能。
- en: The comparison for Dask is a bit more complicated. Firstly, my example reads
    the data from my local machine while our Dask examples will read the data from
    a S3 bucket. Network speed will be a relevant component. Also, distributed computations
    have some overhead that we have to account for.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 对 Dask 的比较要复杂一些。首先，我的示例从本地机器读取数据，而我们的 Dask 示例将从 S3 存储桶中读取数据。网络速度将是一个相关因素。此外，分布式计算也有一些开销，我们需要考虑到这一点。
- en: We are purely looking for speed here, so we will read some timeseries data from
    a public S3 bucket.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 我们这里只关注速度，所以我们将从一个公共 S3 存储桶中读取一些时间序列数据。
- en: '[PRE14]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: We will execute this code-snippet for `engine="c"`, `engine="pyarrow"` and additionally
    `engine="pyarrow"` with `dtype_backend="pyarrow"`. Let's look at some performance
    comparisons. Both examples were executed with 30 machines on the cluster.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将为 `engine="c"`、`engine="pyarrow"` 以及额外的 `engine="pyarrow"` 配置 `dtype_backend="pyarrow"`
    执行这个代码片段。让我们来看看一些性能比较。两个示例都在集群上的 30 台机器上执行。
- en: '![](../Images/f558e50482cde8aa566482578a74e017.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f558e50482cde8aa566482578a74e017.png)'
- en: The PyArrow-engine runs around 2 times as fast as the C-engine. Both implementations
    used the same number of machines. The memory usage was reduced by 50% with the
    PyArrow `dtype_backend`. The same reduction is available if only object columns
    are converted to PyArrow strings, which gives a better experience in follow-up
    operations.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: PyArrow引擎运行速度约为C引擎的两倍。两种实现使用了相同数量的机器。使用PyArrow `dtype_backend`时内存使用量减少了50%。如果仅将对象列转换为PyArrow字符串，也可以实现相同的减少，这能在后续操作中提供更好的体验。
- en: We’ve seen that the Arrow-engines provide significant speedups over the custom
    C implementations. They don’t support all features of the custom implementations
    yet, but if your use-case is compatible with the supported options, you should
    get a significant speedup for free.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到Arrow引擎提供了显著的速度提升，超过了自定义C实现。它们尚未支持自定义实现的所有功能，但如果你的使用场景与支持的选项兼容，你应该能免费获得显著的速度提升。
- en: The case with the PyArrow `dtype_backend` is a bit more complicated. Not all
    areas of the API are optimized yet. If you spend a lot of time processing your
    data outside I/O functions, then this might not give you what you need. It will
    speed up your processing if your workflow spends a lot of time reading the data.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 使用PyArrow `dtype_backend`的情况稍微复杂一些。并非所有API区域都经过优化。如果你在I/O函数之外花费大量时间处理数据，那么这可能无法满足你的需求。如果你的工作流程在读取数据时花费大量时间，它会加速你的处理。
- en: dtype_backend in PyArrow-native I/O readers
  id: totrans-77
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: PyArrow原生I/O读取器中的dtype_backend
- en: Some other I/O methods have an engine keyword as well. `read_parquet` is the
    most popular example. The situation is a bit different here though. These I/O
    methods were already using the PyArrow engine by default. So the parsing is as
    efficient as possible. One other potential performance benefit is the usage of
    the `dtype_backend` keyword. Normally, PyArrow will return a PyArrow table which
    is then converted to a pandas DataFrame. The PyArrow dtypes are converted to their
    NumPy equivalent. Setting `dtype_backend="pyarrow"` avoids this conversion. This
    gives a decent performance improvement and saves a lot of memory.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 其他一些I/O方法也有一个engine关键字。`read_parquet`是最常见的例子。不过这里的情况稍有不同。这些I/O方法默认已经使用PyArrow引擎。因此解析尽可能高效。另一个潜在的性能提升是使用`dtype_backend`关键字。通常，PyArrow会返回一个PyArrow表，然后转换为pandas
    DataFrame。PyArrow的数据类型会转换为其NumPy等效类型。设置`dtype_backend="pyarrow"`可以避免这种转换。这能显著提高性能并节省大量内存。
- en: Let’s look at one pandas performance comparison. We read the Uber-Lyft taxi
    data from December 2022.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看pandas性能的比较。我们从2022年12月读取了Uber-Lyft出租车数据。
- en: '[PRE15]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: We read the data with and without `dtype_backend="pyarrow"`.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 我们读取了带有和不带有`dtype_backend="pyarrow"`的数据。
- en: '![](../Images/ce5a2300466ab2ccfef7c65e92ea5079.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ce5a2300466ab2ccfef7c65e92ea5079.png)'
- en: We can easily see that the most time is taken up by the conversion after the
    reading of the Parquet file was finished. The function runs 3 times as fast when
    avoiding the conversion to NumPy dtypes.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以很容易地看到，转换所花费的时间最多，读取Parquet文件后完成的转换。如果避免转换为NumPy数据类型，函数运行速度提高了三倍。
- en: Dask has a specialized implementation for `read_parquet` that has some advantages
    tailored to distributed workloads compared to the pandas implementation. The common
    denominator is that both functions dispatch to PyArrow to read the parquet file.
    Both have in common that the data are converted to NumPy dtypes after successfully
    reading the file. We are reading the whole Uber-Lyft dataset, which consumes around
    240GB of memory on our cluster.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: Dask为`read_parquet`提供了一个专门的实现，相比于pandas实现，具有一些针对分布式工作负载的优势。共同点是这两个函数都调度到PyArrow来读取parquet文件。两者都共同点是数据在成功读取文件后会转换为NumPy数据类型。我们正在读取整个Uber-Lyft数据集，这在我们的集群上消耗了约240GB的内存。
- en: '[PRE16]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'We read the dataset in 3 different configurations. First with the default NumPy
    dtypes, then with the PyArrow string option turned on:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在3种不同的配置下读取了数据集。首先是使用默认的NumPy数据类型，然后打开PyArrow字符串选项：
- en: '[PRE17]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'And lastly with `dtype_backend="pyarrow"`. Let''s look at what this means performance-wise:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 最后是`dtype_backend="pyarrow"`。让我们看看这在性能方面意味着什么：
- en: '![](../Images/3a880dae2241fbb7eb36e94977d32ffb.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3a880dae2241fbb7eb36e94977d32ffb.png)'
- en: Similar to our pandas example, we can see that converting to NumPy dtypes takes
    up a huge chunk of our runtime. The PyArrow dtypes give us a nice performance
    improvement. Both PyArrow configurations use half of the memory that the NumPy
    dtypes are using.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于我们的 pandas 示例，我们可以看到转换为 NumPy dtypes 会占用我们大量的运行时间。PyArrow dtypes 提供了显著的性能提升。两个
    PyArrow 配置所使用的内存是 NumPy dtypes 的一半。
- en: PyArrow-strings are a lot more mature than the general PyArrow `dtype_backend`.
    Based on the performance chart we got, we get roughly the same performance improvement
    when using PyArrow strings and NumPy dtypes for all other dtypes. If a workflow
    does not work well enough on PyArrow dtypes yet, I'd recommend enabling PyArrow
    strings only.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: PyArrow 字符串比一般的 PyArrow `dtype_backend` 更成熟。根据我们得到的性能图表，当使用 PyArrow 字符串和 NumPy
    dtypes 对于所有其他 dtypes 时，我们得到的性能提升大致相同。如果某个工作流程在 PyArrow dtypes 上的表现还不够好，我建议仅启用
    PyArrow 字符串。
- en: Conclusion
  id: totrans-92
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结论
- en: We have seen how we can leverage PyArrow in pandas in Dask right now. PyArrow
    backed string columns have the potential to impact most workflows in a positive
    way and provide a smooth user experience with pandas 2.0\. Dask has a convenient
    option to globally avoid NumPy object dtype when possible, which makes opting
    into PyArrow backed strings even easier. PyArrow also provides huge speedups in
    other areas where available. The PyArrow `dtype_backend` is still pretty new and
    has the potential to cut I/O times significantly right now. It is certainly worth
    exploring whether it can solve performance bottlenecks. There is a lot of work
    going on to improve support for general PyArrow dtypes with the potential to speed
    up an average workflow in the near future.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到如何在 pandas 和 Dask 中利用 PyArrow。PyArrow 支持的字符串列有潜力以积极的方式影响大多数工作流程，并为 pandas
    2.0 提供顺畅的用户体验。Dask 提供了一个方便的选项，可以在可能的情况下全局避免使用 NumPy 对象 dtype，这使得选择 PyArrow 支持的字符串变得更加容易。PyArrow
    在其他可用领域也提供了巨大的加速。PyArrow 的 `dtype_backend` 仍然相当新，但有潜力显著减少 I/O 时间。探索它是否能解决性能瓶颈是非常值得的。许多工作正在进行中，以改进对一般
    PyArrow dtypes 的支持，并有潜力在不久的将来加速平均工作流程。
- en: There is a current proposal in pandas to start inferring strings as PyArrow
    backed strings by default starting from pandas 3.0\. Additionally, it includes
    many more areas where leaning more onto PyArrow makes a lot of sense (e.g. Decimals,
    structured data, …). You can read up on the proposal [here](https://github.com/pandas-dev/pandas/pull/52711).
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: pandas 目前有一个提议，从 pandas 3.0 开始默认将字符串推断为 PyArrow 支持的字符串。此外，这还包括更多领域，其中更多地依赖 PyArrow
    是非常有意义的（例如：Decimals、结构化数据等）。您可以在[这里](https://github.com/pandas-dev/pandas/pull/52711)阅读提议的详细信息。
- en: Thank you for reading. Feel free to reach out in the comments to share your
    thoughts and feedback about PyArrow support in both libraries. I will write follow
    up posts focused on this topic and pandas in general. Follow me on Medium if you
    like to read more about pandas and Dask.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 感谢阅读。欢迎在评论中分享您对两个库中 PyArrow 支持的想法和反馈。我将会写后续的文章，专注于这个话题以及 pandas 一般内容。如果您喜欢阅读关于
    pandas 和 Dask 的更多内容，可以关注我在 Medium 上的更新。
