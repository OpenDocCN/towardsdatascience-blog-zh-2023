- en: 'Converting Texts to Numeric Form with TfidfVectorizer: A Step-by-Step Guide'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将文本转换为数值形式的 TFIDF 向量化器：逐步指南
- en: 原文：[https://towardsdatascience.com/converting-texts-to-numeric-form-with-tfidfvectorizer-a-step-by-step-guide-bb9330562ae3](https://towardsdatascience.com/converting-texts-to-numeric-form-with-tfidfvectorizer-a-step-by-step-guide-bb9330562ae3)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/converting-texts-to-numeric-form-with-tfidfvectorizer-a-step-by-step-guide-bb9330562ae3](https://towardsdatascience.com/converting-texts-to-numeric-form-with-tfidfvectorizer-a-step-by-step-guide-bb9330562ae3)
- en: '![](../Images/d2844dbee6bc63d8e734f8a44ae8298f.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d2844dbee6bc63d8e734f8a44ae8298f.png)'
- en: Photo by [Mohamed Nohassi](https://unsplash.com/@coopery?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由 [Mohamed Nohassi](https://unsplash.com/@coopery?utm_source=medium&utm_medium=referral)
    提供，来源于 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: How to calculate Tfidf values manually and using sklearn
  id: totrans-4
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何手动计算 Tfidf 值及使用 sklearn
- en: '[](https://rashida00.medium.com/?source=post_page-----bb9330562ae3--------------------------------)[![Rashida
    Nasrin Sucky](../Images/42bd057e8eca255907c43c29a498f2ca.png)](https://rashida00.medium.com/?source=post_page-----bb9330562ae3--------------------------------)[](https://towardsdatascience.com/?source=post_page-----bb9330562ae3--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----bb9330562ae3--------------------------------)
    [Rashida Nasrin Sucky](https://rashida00.medium.com/?source=post_page-----bb9330562ae3--------------------------------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://rashida00.medium.com/?source=post_page-----bb9330562ae3--------------------------------)[![Rashida
    Nasrin Sucky](../Images/42bd057e8eca255907c43c29a498f2ca.png)](https://rashida00.medium.com/?source=post_page-----bb9330562ae3--------------------------------)[](https://towardsdatascience.com/?source=post_page-----bb9330562ae3--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----bb9330562ae3--------------------------------)
    [Rashida Nasrin Sucky](https://rashida00.medium.com/?source=post_page-----bb9330562ae3--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----bb9330562ae3--------------------------------)
    ·6 min read·Oct 25, 2023
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----bb9330562ae3--------------------------------)
    ·阅读时间 6 分钟 ·2023年10月25日
- en: --
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: TFIDF is a method to convert texts to numeric form for machine learning or AI
    models. In other words, TFIDF is a method to extract features from texts. This
    is a more sophisticated method than the CountVectorizer() method I discussed in
    [my last article](/countvectorizer-to-extract-features-from-texts-in-python-in-detail-0e7147c10753).
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: TFIDF 是一种将文本转换为数值形式以用于机器学习或人工智能模型的方法。换句话说，TFIDF 是一种从文本中提取特征的方法。这比我在[上一篇文章](/countvectorizer-to-extract-features-from-texts-in-python-in-detail-0e7147c10753)中讨论的
    CountVectorizer() 方法更为复杂。
- en: The TFIDF method provides a score for each word that represents the usefulness
    of that word or the relevance of the word. It measures the usage of the word compared
    to the other words present in the document.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: TFIDF 方法为每个词提供一个分数，表示该词的有用性或相关性。它衡量了词语的使用情况，与文档中其他词汇相比。
- en: This article will calculate the TFIDF scores manually so that you understand
    the concept of TFIDF clearly. Toward the end, we will see how to use the TFIDF
    vectorizer from the sklearn library as well.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 本文将手动计算 TFIDF 分数，以便您清楚地了解 TFIDF 的概念。在最后，我们还将查看如何使用 sklearn 库中的 TFIDF 向量化器。
- en: 'There are two parts to it: TF and IDF. Let’s see how each part works.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 这包括两个部分：TF 和 IDF。让我们看看每个部分是如何工作的。
- en: '**TF**'
  id: totrans-12
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**TF**'
- en: 'TF is elaborated as ‘Term Frequency’. TF can be calculated as:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: TF 解释为“词频”。TF 可以计算为：
- en: 'TF = # of occurrence of a word in a Document'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: TF = 单词在文档中的出现次数
- en: OR
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 或者
- en: TF = (# of occurrence in a document) / (# of words in a document)
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: TF = （文档中出现的次数）/ （文档中的词数）
- en: 'Let’s work on an example. We will find the TF for each word for this document:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们做一个例子。我们将找到该文档中每个词的 TF：
- en: '**My name is Lilly**'
  id: totrans-18
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**我的名字是莉莉**'
- en: Let’s see an example for each of the formulas.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们分别看一下每个公式的例子。
- en: 'TF = # of occurrence of a word in a Document'
  id: totrans-20
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: TF = 单词在文档中的出现次数
- en: If we take the first formula here which is simply the number of occurrences
    of a word in a document, TF for the word ‘MY’ is 1 as it appeared only once.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们取第一个公式，即文档中单词出现的次数，则“MY”的 TF 是 1，因为它只出现了一次。
- en: In the same way, the TF for the word
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 以相同的方式，词语的 TF
- en: ‘name’ = 1, ‘is’ = 1, ‘Lilly’ = 1
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: ‘name’ = 1，‘is’ = 1，‘Lilly’ = 1
- en: Now, let’s use the second formula.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们使用第二个公式。
- en: TF = (# of occurrence in a document) / (# of words in a document)
  id: totrans-25
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: TF = （文档中出现的次数）/ （文档中的词数）
- en: If we take the second formula, the first part of the formula (# of occurrences
    in a document) is 1, and the second part (# of words in a document) is 4.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们使用第二个公式，公式的第一部分（文档中的出现次数）是1，第二部分（文档中的单词数量）是4。
- en: So, the TF for the word ‘MY’ is 1/4 or 0.25.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，单词‘MY’的TF为1/4或0.25。
- en: In the same way, the TF for the words
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，单词的TF为
- en: name = ¼ = 0.25, is = ¼ = 0.25, Lilly = ¼ = 0.25.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: name = ¼ = 0.25，is = ¼ = 0.25，Lilly = ¼ = 0.25。
- en: '**IDF**'
  id: totrans-30
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**IDF**'
- en: The elaboration of IDF is Inverse Document Frequency.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: IDF的详细解释是逆文档频率。
- en: Here is the formula,
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 这是公式，
- en: idf = 1 + LN[n/df(t)]
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: idf = 1 + LN[n/df(t)]
- en: or
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 或
- en: idf = LN[n/df(t)]
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: idf = LN[n/df(t)]
- en: Where, n = Number of documents available, and
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，n = 可用的文档数量，以及
- en: df = Number of documents where the term appears
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: df = 术语出现的文档数量
- en: As per sklearn library’s documentation
  id: totrans-38
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 根据sklearn库的文档
- en: idf = LN[(1+n) / (1+df(t))] + 1 (default setting)
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: idf = LN[(1+n) / (1+df(t))] + 1（默认设置）
- en: or
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 或
- en: idf = LN[n / df(t)] + 1 (when smooth_idf = True)
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: idf = LN[n / df(t)] + 1（当smooth_idf = True时）
- en: We won’t work on all four formulas here. Let’s just work on the 2 formulas.
    You will get the idea.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不会处理所有四个公式。我们只讨论两个公式。你将会明白的。
- en: 'To demonstrate the IDF, only one document is not enough. I will these three
    documents:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 为了演示IDF，单个文档是不够的。我将使用这三个文档：
- en: My name is Lilly
  id: totrans-44
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 我的名字是Lilly
- en: ''
  id: totrans-45
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Lilly is my mom’s favorite flower
  id: totrans-46
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: Lilly是我妈妈最喜欢的花
- en: ''
  id: totrans-47
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: My mom loves flowers
  id: totrans-48
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 我妈妈喜欢花
- en: 'Let’s use this formula to practice this time:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 这次我们使用这个公式进行练习：
- en: IDF = LN[n/df(t)]
  id: totrans-50
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: IDF = LN[n/df(t)]
- en: If we take the word ‘My’ first, n is 3 because we have 3 documents here, and
    df(t) is also 3 because ‘My’ appeared in all three documents.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们先考虑‘My’，n是3，因为这里有3个文档，而df(t)也是3，因为‘My’出现在所有三个文档中。
- en: IDF(MY) = LN(3/3) = 0 (As ln(1) is 0)
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: IDF(MY) = LN(3/3) = 0（因为ln(1)是0）
- en: We will work on one more word to understand it clearly. Take the word ‘name’.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将再处理一个单词以便清楚理解。以‘name’为例。
- en: For the word ‘name’ again the n will be the same as before because the number
    of documents is 3 but the df(t) will be 1\. Because the word ‘name’ is present
    in only one document.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 对于单词‘name’，n将和之前相同，因为文档数量是3，但df(t)将是1。因为单词‘name’只出现在一个文档中。
- en: IDF(name) = ln(3/1) = 1.1 (I used Excel’s LN function for this)
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: IDF(name) = ln(3/1) = 1.1（我使用了Excel的LN函数）
- en: '**How does the sklearn library calculate TFIDF?**'
  id: totrans-56
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**sklearn库如何计算TFIDF？**'
- en: 'the sklearn library uses these two formulas for TF and IDF:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: sklearn库使用这两个公式来计算TF和IDF：
- en: 'TF = # of occurrence of a word in a Document'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: TF = 单词在文档中的出现次数
- en: idf = LN[(1+n) / (1+df(t))] + 1
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: idf = LN[(1+n) / (1+df(t))] + 1
- en: 'If I use the same three documents, for the word ‘MY’:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我使用相同的三个文档，单词‘MY’的计算如下：
- en: TF(My) = 1
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: TF(My) = 1
- en: IDF(My) =LN((1+3)/(1+3)) + 1 = 1
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: IDF(My) = LN((1+3)/(1+3)) + 1 = 1
- en: 'The formula for TFIDF is :'
  id: totrans-63
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: TFIDF的公式是：
- en: TFIDF = TF*IDF
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: TFIDF = TF * IDF
- en: 'So, the TFIDF for ‘My’ is :'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，‘My’的TFIDF为：
- en: TFIDF(My) = 1* 1 = 1
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: TFIDF(My) = 1 * 1 = 1
- en: 'For the word ‘name’:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 对于单词‘name’：
- en: TF(name) = 1
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: TF(name) = 1
- en: IDF(name) = =LN((1+3)/(1+1)) +1 = 1.69
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: IDF(name) = LN((1+3)/(1+1)) + 1 = 1.69
- en: TFIDF(name) = 1* 1.69 = 1.69
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: TFIDF(name) = 1 * 1.69 = 1.69
- en: 'In the same way, the TFIDF for all the words are :'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，所有单词的TFIDF如下：
- en: '![](../Images/96e3862dd07f2a029b0440e5addadf72.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/96e3862dd07f2a029b0440e5addadf72.png)'
- en: Image By Author
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 作者图片
- en: 'The sklearn’s tfidf vectorizer normalizes the values to bring them in a 0 to
    1 scale. For that, we need to have SS(Sum of Squared) for the tfidfs of each document:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: sklearn的tfidf向量化器将值归一化到0到1的范围。为此，我们需要每个文档的tfidf的平方和：
- en: '![](../Images/ee26e23ced9ed95c449e6b3f93f79d53.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ee26e23ced9ed95c449e6b3f93f79d53.png)'
- en: 'The normalized tfidf is:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 归一化的tfidf是：
- en: The tfidf value for the word/ the SS of the document
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 单词的tfidf值/文档的平方和
- en: 'If we take the word My. normalized tfidf for ‘My’ in the document-1 is:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们考虑单词My。文档-1中‘My’的归一化tfidf是：
- en: tfidf_normalized(My) = 1.00 / 7.871 = 0.356
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: tfidf_normalized(My) = 1.00 / 7.871 = 0.356
- en: 'tfidf for the word ‘mom’ in document-3 is:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 单词‘mom’在文档-3中的tfidf是：
- en: tfidf_normalized(name) = 1.42 / 9.005 = 0.472
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: tfidf_normalized(name) = 1.42 / 9.005 = 0.472
- en: 'Again, tfidf for the word ‘mom’ in document-2 is:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，单词‘mom’在文档-2中的tfidf是：
- en: tfidf_normalized(name) = 1.42 / 13.009 = 0.392
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: tfidf_normalized(name) = 1.42 / 13.009 = 0.392
- en: Looks like the word ‘mom’ has a bit more relevance in the document 2 than in
    the document 3
  id: totrans-84
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 看起来‘mom’这个词在文档2中的相关性比在文档3中高一些
- en: 'The normalized tfidf for all the words are here:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 所有单词的归一化tfidf如下：
- en: '![](../Images/7ef138e24d6aee2e9abca6f9cf268a2d.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7ef138e24d6aee2e9abca6f9cf268a2d.png)'
- en: Image By Author
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 作者图片
- en: Now, we should check how the [tfidf vectorizer in the sklearn library](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html)
    work.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们应该检查[sklearn库中的tfidf向量化器](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html)的工作方式。
- en: 'First, import the Tfidf vectorizer from sklearn library and define the text
    to be used for feature extraction:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，从sklearn库中导入Tfidf向量化器，并定义用于特征提取的文本：
- en: '[PRE0]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: In the next code block,
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一个代码块中，
- en: the first line calls the TfidfVectorizer method and saves it in a variable named
    vectorizer.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 第一行调用TfidfVectorizer方法，并将其保存到名为vectorizer的变量中。
- en: the second line fit_transform the text into the vectorizer
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 第二行将文本转换为向量化器
- en: the third line converts that to an array to display
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 第三行将其转换为数组以进行显示
- en: '[PRE1]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Output:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：
- en: '[PRE2]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: It will be helpful to convert this array into a DataFrame and use the words
    as the column names.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 将此数组转换为DataFrame并使用单词作为列名将会很有帮助。
- en: '[PRE3]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '![](../Images/4031e7040dc6397a8528106f6699ffb6.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4031e7040dc6397a8528106f6699ffb6.png)'
- en: Image By Author
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 作者图片
- en: You can use the same parameters as I explained in [my tutorial on CountVectorizer](/countvectorizer-to-extract-features-from-texts-in-python-in-detail-0e7147c10753)
    to refine or limit the number of features. Please feel free to check that.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用我在[关于CountVectorizer的教程](/countvectorizer-to-extract-features-from-texts-in-python-in-detail-0e7147c10753)中解释的相同参数来细化或限制特征数量。请随时检查。
- en: Conclusion
  id: totrans-103
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结论
- en: This tutorial explained in detail how Tfidf Vectorizer works. Though it is very
    simple to use the Tfidf Vectorizer from sklearn library, it is important to understand
    the concept behind it. When you know how a vectorizer works, it becomes easier
    to make the decision on what kind of vectorizer is suitable.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 本教程详细解释了Tfidf Vectorizer的工作原理。尽管使用sklearn库中的Tfidf Vectorizer非常简单，但理解其背后的概念也很重要。当你了解一个向量化器是如何工作的时，就更容易决定哪种向量化器是合适的。
- en: Feel free to follow me on [Twitter](https://twitter.com/rashida048) and like
    my [Facebook](https://www.facebook.com/rashida.smith.161) page.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 随时在[Twitter](https://twitter.com/rashida048)上关注我，并点赞我的[Facebook](https://www.facebook.com/rashida.smith.161)页面。
- en: '**The video version of this tutorial:**'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '**本教程的视频版本：**'
- en: More Reading
  id: totrans-107
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 更多阅读
- en: '[](/a-complete-step-by-step-tutorial-on-sentiment-analysis-in-keras-and-tensorflow-ea420cc8913f?source=post_page-----bb9330562ae3--------------------------------)
    [## A Complete Step by Step Tutorial on Sentiment Analysis in Keras and Tensorflow'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '[## Keras和Tensorflow中的完整步骤教程](https://a-complete-step-by-step-tutorial-on-sentiment-analysis-in-keras-and-tensorflow-ea420cc8913f?source=post_page-----bb9330562ae3--------------------------------)'
- en: Complete Working Code for Data Preparation, Deep Learning Model Development,
    and Training the network
  id: totrans-109
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据准备、深度学习模型开发和网络训练的完整工作代码
- en: towardsdatascience.com](/a-complete-step-by-step-tutorial-on-sentiment-analysis-in-keras-and-tensorflow-ea420cc8913f?source=post_page-----bb9330562ae3--------------------------------)
    [](https://pub.towardsai.net/a-complete-exploratory-data-analysis-in-python-a2148daac072?source=post_page-----bb9330562ae3--------------------------------)
    [## A Complete Exploratory Data Analysis in Python
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '[## Python中的完整探索性数据分析](https://pub.towardsai.net/a-complete-exploratory-data-analysis-in-python-a2148daac072?source=post_page-----bb9330562ae3--------------------------------)'
- en: Data Cleaning, Analysis, Visualization, Feature Selection, Predictive Modeling
  id: totrans-111
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据清理、分析、可视化、特征选择、预测建模
- en: pub.towardsai.net](https://pub.towardsai.net/a-complete-exploratory-data-analysis-in-python-a2148daac072?source=post_page-----bb9330562ae3--------------------------------)
    [](/30-very-useful-pandas-functions-for-everyday-data-analysis-tasks-f1eae16409af?source=post_page-----bb9330562ae3--------------------------------)
    [## 30 Very Useful Pandas Functions for Everyday Data Analysis Tasks
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '[## 30个非常有用的Pandas函数用于日常数据分析任务](https://pub.towardsai.net/a-complete-exploratory-data-analysis-in-python-a2148daac072?source=post_page-----bb9330562ae3--------------------------------)'
- en: Pandas Cheatsheet
  id: totrans-113
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Pandas备忘单
- en: towardsdatascience.com](/30-very-useful-pandas-functions-for-everyday-data-analysis-tasks-f1eae16409af?source=post_page-----bb9330562ae3--------------------------------)
    [](/how-to-define-custom-layer-activation-function-and-loss-function-in-tensorflow-bdd7e78eb67?source=post_page-----bb9330562ae3--------------------------------)
    [## How to Define Custom Layer, Activation Function, and Loss Function in TensorFlow
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/30-very-useful-pandas-functions-for-everyday-data-analysis-tasks-f1eae16409af?source=post_page-----bb9330562ae3--------------------------------)
    [](/how-to-define-custom-layer-activation-function-and-loss-function-in-tensorflow-bdd7e78eb67?source=post_page-----bb9330562ae3--------------------------------)
    [## 如何在TensorFlow中定义自定义层、激活函数和损失函数
- en: Step-by-step explanation and examples with complete code
  id: totrans-115
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 逐步解释和包含完整代码的示例
- en: towardsdatascience.com](/how-to-define-custom-layer-activation-function-and-loss-function-in-tensorflow-bdd7e78eb67?source=post_page-----bb9330562ae3--------------------------------)
    [](/morphological-operations-for-image-preprocessing-in-opencv-in-detail-15fccd1e5745?source=post_page-----bb9330562ae3--------------------------------)
    [## Morphological Operations for Image Preprocessing in OpenCV, in Detail
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/how-to-define-custom-layer-activation-function-and-loss-function-in-tensorflow-bdd7e78eb67?source=post_page-----bb9330562ae3--------------------------------)
    [](/morphological-operations-for-image-preprocessing-in-opencv-in-detail-15fccd1e5745?source=post_page-----bb9330562ae3--------------------------------)
    [## OpenCV中用于图像预处理的形态学操作详解
- en: Erosion, dilation, opening, closing, morphological gradient, tophat / whitehat,
    and blackhat explained with examples
  id: totrans-117
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 腐蚀、膨胀、开运算、闭运算、形态学梯度、顶帽/白帽和黑帽，配有示例进行解释
- en: towardsdatascience.com](/morphological-operations-for-image-preprocessing-in-opencv-in-detail-15fccd1e5745?source=post_page-----bb9330562ae3--------------------------------)
    [](/anomaly-detection-in-tensorflow-and-keras-using-the-autoencoder-method-5600aca29c50?source=post_page-----bb9330562ae3--------------------------------)
    [## Anomaly Detection in TensorFlow and Keras Using the Autoencoder Method
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/morphological-operations-for-image-preprocessing-in-opencv-in-detail-15fccd1e5745?source=post_page-----bb9330562ae3--------------------------------)
    [](/anomaly-detection-in-tensorflow-and-keras-using-the-autoencoder-method-5600aca29c50?source=post_page-----bb9330562ae3--------------------------------)
    [## 使用自编码器方法进行TensorFlow和Keras中的异常检测
- en: A cutting-edge unsupervised method for noise removal, dimensionality reduction,
    anomaly detection, and more
  id: totrans-119
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 一种前沿的无监督方法，用于噪声去除、降维、异常检测等
- en: towardsdatascience.com](/anomaly-detection-in-tensorflow-and-keras-using-the-autoencoder-method-5600aca29c50?source=post_page-----bb9330562ae3--------------------------------)
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/anomaly-detection-in-tensorflow-and-keras-using-the-autoencoder-method-5600aca29c50?source=post_page-----bb9330562ae3--------------------------------)
