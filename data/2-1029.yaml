- en: 'Hands-on Generative AI with GANs using Python: Image Generation'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Python和GANs进行生成式AI实践：图像生成
- en: 原文：[https://towardsdatascience.com/hands-on-generative-ai-with-gans-using-python-image-generation-9a62e591c7c6](https://towardsdatascience.com/hands-on-generative-ai-with-gans-using-python-image-generation-9a62e591c7c6)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/hands-on-generative-ai-with-gans-using-python-image-generation-9a62e591c7c6](https://towardsdatascience.com/hands-on-generative-ai-with-gans-using-python-image-generation-9a62e591c7c6)
- en: '![](../Images/8bacad55716d720d37ccf3e4e734e940.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8bacad55716d720d37ccf3e4e734e940.png)'
- en: Image by Author
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图像
- en: Learn how to implement GANs with PyTorch to generate synthetic images
  id: totrans-4
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 学习如何使用PyTorch实现GANs以生成合成图像
- en: '[](https://medium.com/@marcellopoliti?source=post_page-----9a62e591c7c6--------------------------------)[![Marcello
    Politi](../Images/484e44571bd2e75acfe5fef3146ab3c2.png)](https://medium.com/@marcellopoliti?source=post_page-----9a62e591c7c6--------------------------------)[](https://towardsdatascience.com/?source=post_page-----9a62e591c7c6--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----9a62e591c7c6--------------------------------)
    [Marcello Politi](https://medium.com/@marcellopoliti?source=post_page-----9a62e591c7c6--------------------------------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@marcellopoliti?source=post_page-----9a62e591c7c6--------------------------------)[![Marcello
    Politi](../Images/484e44571bd2e75acfe5fef3146ab3c2.png)](https://medium.com/@marcellopoliti?source=post_page-----9a62e591c7c6--------------------------------)[](https://towardsdatascience.com/?source=post_page-----9a62e591c7c6--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----9a62e591c7c6--------------------------------)
    [Marcello Politi](https://medium.com/@marcellopoliti?source=post_page-----9a62e591c7c6--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----9a62e591c7c6--------------------------------)
    ·7 min read·Mar 27, 2023
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----9a62e591c7c6--------------------------------)
    ·7分钟阅读·2023年3月27日
- en: --
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '**Introduction**'
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**介绍**'
- en: In my [previous article](https://medium.com/towards-data-science/hands-on-generative-ai-with-gans-using-python-autoencoders-c77232b402fc),
    we learned about Autoencoders, now let’s continue to talk about Generative AI.
    By now everyone is talking about it and everyone is excited about the practical
    applications that have been developed. But we continue to see the foundations
    of these AIs step by step.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在我的 [上一篇文章](https://medium.com/towards-data-science/hands-on-generative-ai-with-gans-using-python-autoencoders-c77232b402fc)中，我们了解了自编码器，现在让我们继续讨论生成式AI。目前每个人都在谈论它，并且对已经开发出的实际应用感到兴奋。但我们将一步步地继续探讨这些AI的基础。
- en: There are several Machine Learning models that allow us to build generative
    AI, to name a few we have Variational Autoencoders (VAE), autoregressive models
    and even normalizing flow models. In this article, however, we will focus on GANs.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 有几个机器学习模型可以构建生成式AI，例如变分自编码器（VAE）、自回归模型甚至正则化流模型。然而，本文将重点讨论GANs。
- en: Autoencoders and GANs
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 自编码器与GANs
- en: In the previous article, we dealt with autoencoders and saw their architecture,
    their use and implementation in PyTorch.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一篇文章中，我们讨论了自编码器，并了解了它们的架构、用途和在PyTorch中的实现。
- en: In short, Autoencoders receive an input x, compress it into a vector of smaller
    size z, called the latent vector, and finally from z reconstruct x in a more or
    less approximate way.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，自编码器接收输入x，将其压缩为一个较小的向量z，称为潜在向量，然后从z以或多或少的近似方式重构x。
- en: In Autoencoder we have no data generation, but simply an approximate reconstruction
    of the input. Now imagine that we break the Autoencoder in two and consider only
    the second part, the part where from the latent vector z the image is reconstructed.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在自编码器中，我们没有数据生成，而只是对输入的近似重构。现在假设我们将自编码器分成两部分，并仅考虑第二部分，即从潜在向量z重构图像的部分。
- en: '![](../Images/674c788436be31599035404c61b840cc.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/674c788436be31599035404c61b840cc.png)'
- en: Output Generation (Image By Author)
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 输出生成（作者提供的图像）
- en: In this case, we can say that the architecture is generative. In fact, given
    a vector of numbers as input this creates an image! Essentially this is what a
    generative AI does. The main difference though with respect to autoencoders is
    that we know well the probability distribution from which we take the latent vector
    z. For example, a *Gaussian(0,1)*.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们可以说架构是生成式的。实际上，给定一个数字向量作为输入，这会创建一幅图像！本质上，这就是生成式AI的作用。与自编码器的主要区别在于，我们清楚地知道我们从中获取潜在向量z的概率分布。例如，一个*Gaussian(0,1)*。
- en: So we thus have a way to generate images from random numbers taken from a Gaussian
    distribution, changing these random numbers will change the images we have in
    the output.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们有了一种从高斯分布中的随机数生成图像的方法，改变这些随机数将改变我们输出的图像。
- en: '![](../Images/01c55550204051a7c6d10b3d6487f4a3.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/01c55550204051a7c6d10b3d6487f4a3.png)'
- en: Generative Model (Image By Author)
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 生成模型（图片由作者提供）
- en: GANs Architecture
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: GANs架构
- en: The orange network shown in the previous image can be defined as a G function
    that given the input z generates the synthetic output *x_cap*, so *x_cap = G(z)*.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 前一张图片中显示的橙色网络可以定义为一个G函数，给定输入z生成合成输出 *x_cap*，因此 *x_cap = G(z)*。
- en: The network will be initialized with random weights, so it will not initially
    be able to generate output that looks real, but only images that will contain
    noise. So we need to do some training to improve the performance of our network.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 网络将以随机权重初始化，因此最初它无法生成看起来真实的输出，只能生成包含噪声的图像。因此，我们需要进行一些训练来提高网络的性能。
- en: So let’s imagine that we have a human annotator telling us each time whether
    the output is good or not, whether it looks real or not.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 所以让我们设想一下，每当我们得到输出时，有一个人工标注员告诉我们这些输出是否良好，是否看起来真实。
- en: '![](../Images/0386f4e179933422ffda2952ed945011.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0386f4e179933422ffda2952ed945011.png)'
- en: Towards GANs (Image By Author)
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 朝向GANs（图片由作者提供）
- en: Obviously, we cannot do network training expecting a person to make continuous
    judgments about the output. But then what can we do?
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，我们不能进行网络训练，期望一个人对输出进行持续判断。那么我们可以做什么呢？
- en: If you think about it what the annotator does, in this case, is binary classification!
    And we in Machine Learning are great at developing classifiers. So we can simply
    train a classifier that we’ll call Discriminator, and we’ll denote with the function
    D(), which has to be trained to recognize synthetic (fake) images versus real
    images. So we will feed it both fake images and real images.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你考虑一下标注员所做的工作，在这种情况下就是二元分类！而我们在机器学习中非常擅长开发分类器。因此，我们可以简单地训练一个分类器，我们称之为鉴别器，并用函数D()表示，它必须被训练来识别合成（虚假）图像与真实图像。因此我们将同时输入虚假图像和真实图像。
- en: So this is how our architecture changes.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 所以这就是我们的架构如何变化的。
- en: '![](../Images/8bacad55716d720d37ccf3e4e734e940.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8bacad55716d720d37ccf3e4e734e940.png)'
- en: GANs Architecture (Image By Author)
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: GANs架构（图片由作者提供）
- en: In short, the architecture is not too complex. The difficulty comes at the time
    of having to train these two networks G and D.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，架构并不复杂。困难在于训练这两个网络G和D时。
- en: It is clear that if in training, the two networks have to improve together,
    they need to find some kind of balance. Because if, for example, D gets too good
    at distinguishing fake images from real ones before G gets good at generating
    them, it is quite natural that G will never get better and we will never have
    our generator ready to be used.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 很明显，如果在训练过程中，这两个网络必须一起改进，它们需要找到某种平衡。因为例如，如果D在区分虚假图像与真实图像方面变得过于优秀，而G在生成图像方面尚未提升，那么G永远不会变得更好，我们的生成器也永远无法准备好使用。
- en: So the two networks are said to play an adversarial game in which G must fool
    D, and D must not be fooled by G.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 所以这两个网络被称为在玩一个对抗游戏，其中G必须欺骗D，而D不能被G欺骗。
- en: GANs Objective Function
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: GANs目标函数
- en: If we want to be a bit more precise, we can say that D and G have two complementary
    goals. Let’s suppose we want to generate images.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想更精确一点，可以说D和G有两个互补的目标。假设我们想生成图像。
- en: We define by D(x) the probability that x is a real image. Obviously, the discriminator
    wants to maximize its probability of recognizing real inputs from fake inputs.
    So we want to maximize D(x) when x is drawn from our distribution of real images.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 我们用D(x)定义x是真实图像的概率。显然，鉴别器想要最大化其识别真实输入与虚假输入的概率。因此，当x从我们的真实图像分布中抽取时，我们想要最大化D(x)。
- en: In contrast, the purpose of the generator G is to fool the discriminator. So
    if *G(z)* is the fake image generated by G, *D(G(z))* is the probability that
    D will recognize a fake image as real. Then *1-D(G(z))* is the probability that
    D correctly recognizes a fake image as fake. So G’s goal is to minimize *1-D(G(z))*,
    since he does want to fool D.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，生成器G的目的是欺骗鉴别器。因此，如果 *G(z)* 是由G生成的虚假图像， *D(G(z))* 是D将虚假图像识别为真实图像的概率。那么 *1-D(G(z))*
    是D正确识别虚假图像为虚假的概率。因此G的目标是最小化 *1-D(G(z))*，因为它确实想要欺骗D。
- en: 'So in the end we can sum up this game of maximization and minimization in the
    formula we find in the original paper (the formula looks a bit more concept but
    we have seen the concept):'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 所以最终我们可以将这种最大化和最小化的游戏总结到原始论文中的公式里（公式看起来更具概念性，但我们已经看过这个概念）：
- en: '![](../Images/a8adc55a900fd1a625d8c924e8c3a0e8.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a8adc55a900fd1a625d8c924e8c3a0e8.png)'
- en: 'Objective Function (src: [https://arxiv.org/pdf/1406.2661.pdf](https://arxiv.org/pdf/1406.2661.pdf))'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 目标函数（来源：[https://arxiv.org/pdf/1406.2661.pdf](https://arxiv.org/pdf/1406.2661.pdf)）
- en: GANs Implementation
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: GANs 实现
- en: We now implement a GAN capable of generating MNIST images automatically.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在实现一个能够自动生成MNIST图像的GAN。
- en: As usual, I will run my code a cloud-based environment Deepnote but you can
    use Google Colab as well, so even those who don’t have a GPU on their laptop can
    run this code.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 和往常一样，我将我的代码运行在基于云的环境Deepnote中，但你也可以使用Google Colab，这样即使没有GPU的用户也可以运行这段代码。
- en: We start by going to check whether indeed our hardware has a GPU.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先检查一下我们的硬件是否确实有GPU。
- en: Now if you’re using Colab you can connect to Google Drive.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你在使用Colab，你可以连接到Google Drive。
- en: '[PRE0]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Let’s import the needed libraries.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们导入所需的库。
- en: Now we need to create the functions that will define our networks, generator
    and discriminator.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们需要创建定义我们网络的函数，即生成器和判别器。
- en: The MNIST images have 784 pixels (since the images are 28x28). So the generator
    given as input a random z vector of length 20 will have to output a vector of
    784 which will be our fake image.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: MNIST图像有784个像素（因为图像是28x28）。因此，生成器输入一个长度为20的随机z向量，将输出一个784的向量，这就是我们的伪造图像。
- en: Instead, the discriminator will receive as input a 28x28 = 784-pixel image,
    it will have a single neuron in output that will classify the image as true or
    fake.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，判别器将接收一个28x28 = 784像素的图像作为输入，它将有一个输出神经元来将图像分类为真实或伪造。
- en: '![](../Images/f42e0cc6024372aafabd130d24076065.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f42e0cc6024372aafabd130d24076065.png)'
- en: Generator (Image By Author)
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 生成器（图片由作者提供）
- en: This function is used to instantiate the generator. Each layer will use a LeakyReLU
    (a variation of the ReLU, that works best in GANs) as its activation function,
    except the output is followed by a Hyperbolic Tangent (Tanh) function that results
    in output a number in the range [-1,1].
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 这个函数用于实例化生成器。每一层将使用LeakyReLU（ReLU的一种变体，在GANs中表现最佳）作为其激活函数，除了输出层后接一个双曲正切（Tanh）函数，使得输出范围为[-1,1]。
- en: '![](../Images/6928d3027580731763156ede1b4577f6.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6928d3027580731763156ede1b4577f6.png)'
- en: Discriminator (Image By Author)
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 判别器（图片由作者提供）
- en: Instead, this function defines the discriminator network, which has the special
    feature of using dropout after hidden layers (in the base case only one hidden
    layer). The output goes through a sigmoid function since it must give us the probability
    of being a real or fake image.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，这个函数定义了判别器网络，其特殊功能是在隐藏层之后使用dropout（在基本情况下只有一个隐藏层）。输出通过一个sigmoid函数，因为它必须给出图像是真实的还是伪造的概率。
- en: Now we also download the MNIST dataset that we are going to use. The MNIST dataset
    is in a range [0,255], but we want it in the range [-1,1] so that it will be similar
    to the data generated by the Generator network. So we also apply some preprocessing
    to do this.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们还下载了我们要使用的MNIST数据集。MNIST数据集的范围是[0,255]，但我们希望它在范围[-1,1]，这样它会类似于生成器网络生成的数据。因此，我们还对数据进行了预处理。
- en: Now we come to the most important part. We need to create the functions that
    define the training of our network. We have already said that we should pull the
    discriminator separately from the generator, so we will have 2 functions.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来到最重要的部分。我们需要创建定义我们网络训练的函数。我们已经提到过，我们应该将判别器与生成器分开，因此我们将有两个函数。
- en: The discriminator will be trained both on the fake data and on real data. When
    we train it on real data the labels will always be *“real” = 1*. So we create
    a vector of 1 with *d_labels_real = torch.ones(batch_size, 1, device = device)*.
    Then we feed the input x to the model and calculate the loss using *Binary Cross
    Entropy*.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 判别器将同时在假数据和真实数据上进行训练。当我们在真实数据上训练时，标签将始终是*“real” = 1*。因此，我们创建一个全为1的向量，即*d_labels_real
    = torch.ones(batch_size, 1, device = device)*。然后，我们将输入x送入模型，并使用*Binary Cross Entropy*计算损失。
- en: We do the same thing by feeding fake data. Here the labels will all be zero,
    *d_labels_fake = torch.zeros(batch_size, 1, device = device)*. The input instead
    will be the fake data, that is, the output of the generator *g_output = gen_model(input_z)*.
    And we calculate the loss in the same way.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过输入伪数据做同样的事情。这里的标签将全部为零，*d_labels_fake = torch.zeros(batch_size, 1, device
    = device)*。输入则是伪数据，即生成器的输出 *g_output = gen_model(input_z)*。我们以相同的方式计算损失。
- en: The final loss will be the sum of the two losses.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 最终损失将是两个损失的总和。
- en: As for the generator train function, the implementation is slightly different.
    The generator takes as input the output of the discriminator since it has to see
    if D has figured out whether it is a fake or real image. And based on that it
    calculates its loss.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 对于生成器训练函数，实施略有不同。生成器的输入是判别器的输出，因为它需要查看D是否已识别出图像是真实还是伪造的。基于此，它计算其损失。
- en: Now we can initialize our two networks.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以初始化我们的两个网络了。
- en: Let’s define a function to create network-generated samples, so as we go along
    we can see how the fake images improve as the training epochs increase.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们定义一个函数来创建网络生成的样本，这样随着训练周期的增加，我们可以看到伪图像的改进。
- en: Now we can finally train the net! We save the losses each time in a list so
    we can plot them later.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们终于可以训练网络了！我们每次将损失保存到一个列表中，以便后续绘图。
- en: The training should take about an hour, depending on the hardware cha you used
    certainly. But in the end, you can print out your fake data and have something
    like this.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 训练应该大约需要一个小时，具体取决于你使用的硬件。但最后，你可以打印出你的伪数据，得到类似这样的结果。
- en: In my case, I trained for a few epochs so the results are not great, but you
    are beginning to get a glimpse that the network was learning to generate MNIST-like
    images.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 在我的情况下，我训练了几个周期，所以结果并不理想，但你开始可以看到网络正在学习生成类似MNIST的图像。
- en: '![](../Images/8edea14b537654ada61c0f347ca2fe77.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8edea14b537654ada61c0f347ca2fe77.png)'
- en: Fake Data (Image By Author)
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 伪数据（图片由作者提供）
- en: Final Thoughts
  id: totrans-71
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 最后的想法
- en: In this article, we looked at how the architecture of GANs in more detail. We
    studied their objective function and were able to implement a network capable
    of generating images from the MNIST dataset! The operation of these networks is
    not too complicated but their training certainly is. Since we need to find that
    balance that allows both networks to learn. If you enjoyed this article follow
    me to read the next one on DCGANs.[😉](https://emojipedia.org/it/apple/ios-15.4/faccina-che-fa-l-occhiolino/)
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我们更详细地探讨了GAN的架构。我们研究了它们的目标函数，并实现了一个能够生成MNIST数据集图像的网络！这些网络的操作并不复杂，但它们的训练确实很困难。因为我们需要找到一个平衡点，让两个网络都能学习。如果你喜欢这篇文章，请关注我，阅读下一篇关于DCGAN的文章。[😉](https://emojipedia.org/it/apple/ios-15.4/faccina-che-fa-l-occhiolino/)
- en: The End
  id: totrans-73
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结束
- en: '*Marcello Politi*'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '*Marcello Politi*'
- en: '[Linkedin](https://www.linkedin.com/in/marcello-politi/), [Twitter](https://twitter.com/_March08_),
    [Website](https://marcello-politi.super.site/)'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '[Linkedin](https://www.linkedin.com/in/marcello-politi/)，[Twitter](https://twitter.com/_March08_)，[Website](https://marcello-politi.super.site/)'
