- en: 'Hands-on Generative AI with GANs using Python: Image Generation'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ä½¿ç”¨Pythonå’ŒGANsè¿›è¡Œç”Ÿæˆå¼AIå®è·µï¼šå›¾åƒç”Ÿæˆ
- en: åŸæ–‡ï¼š[https://towardsdatascience.com/hands-on-generative-ai-with-gans-using-python-image-generation-9a62e591c7c6](https://towardsdatascience.com/hands-on-generative-ai-with-gans-using-python-image-generation-9a62e591c7c6)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡ï¼š[https://towardsdatascience.com/hands-on-generative-ai-with-gans-using-python-image-generation-9a62e591c7c6](https://towardsdatascience.com/hands-on-generative-ai-with-gans-using-python-image-generation-9a62e591c7c6)
- en: '![](../Images/8bacad55716d720d37ccf3e4e734e940.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8bacad55716d720d37ccf3e4e734e940.png)'
- en: Image by Author
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: ä½œè€…æä¾›çš„å›¾åƒ
- en: Learn how to implement GANs with PyTorch to generate synthetic images
  id: totrans-4
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å­¦ä¹ å¦‚ä½•ä½¿ç”¨PyTorchå®ç°GANsä»¥ç”Ÿæˆåˆæˆå›¾åƒ
- en: '[](https://medium.com/@marcellopoliti?source=post_page-----9a62e591c7c6--------------------------------)[![Marcello
    Politi](../Images/484e44571bd2e75acfe5fef3146ab3c2.png)](https://medium.com/@marcellopoliti?source=post_page-----9a62e591c7c6--------------------------------)[](https://towardsdatascience.com/?source=post_page-----9a62e591c7c6--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----9a62e591c7c6--------------------------------)
    [Marcello Politi](https://medium.com/@marcellopoliti?source=post_page-----9a62e591c7c6--------------------------------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@marcellopoliti?source=post_page-----9a62e591c7c6--------------------------------)[![Marcello
    Politi](../Images/484e44571bd2e75acfe5fef3146ab3c2.png)](https://medium.com/@marcellopoliti?source=post_page-----9a62e591c7c6--------------------------------)[](https://towardsdatascience.com/?source=post_page-----9a62e591c7c6--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----9a62e591c7c6--------------------------------)
    [Marcello Politi](https://medium.com/@marcellopoliti?source=post_page-----9a62e591c7c6--------------------------------)'
- en: Â·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----9a62e591c7c6--------------------------------)
    Â·7 min readÂ·Mar 27, 2023
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Â·å‘è¡¨äº [Towards Data Science](https://towardsdatascience.com/?source=post_page-----9a62e591c7c6--------------------------------)
    Â·7åˆ†é’Ÿé˜…è¯»Â·2023å¹´3æœˆ27æ—¥
- en: --
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '**Introduction**'
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**ä»‹ç»**'
- en: In my [previous article](https://medium.com/towards-data-science/hands-on-generative-ai-with-gans-using-python-autoencoders-c77232b402fc),
    we learned about Autoencoders, now letâ€™s continue to talk about Generative AI.
    By now everyone is talking about it and everyone is excited about the practical
    applications that have been developed. But we continue to see the foundations
    of these AIs step by step.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æˆ‘çš„ [ä¸Šä¸€ç¯‡æ–‡ç« ](https://medium.com/towards-data-science/hands-on-generative-ai-with-gans-using-python-autoencoders-c77232b402fc)ä¸­ï¼Œæˆ‘ä»¬äº†è§£äº†è‡ªç¼–ç å™¨ï¼Œç°åœ¨è®©æˆ‘ä»¬ç»§ç»­è®¨è®ºç”Ÿæˆå¼AIã€‚ç›®å‰æ¯ä¸ªäººéƒ½åœ¨è°ˆè®ºå®ƒï¼Œå¹¶ä¸”å¯¹å·²ç»å¼€å‘å‡ºçš„å®é™…åº”ç”¨æ„Ÿåˆ°å…´å¥‹ã€‚ä½†æˆ‘ä»¬å°†ä¸€æ­¥æ­¥åœ°ç»§ç»­æ¢è®¨è¿™äº›AIçš„åŸºç¡€ã€‚
- en: There are several Machine Learning models that allow us to build generative
    AI, to name a few we have Variational Autoencoders (VAE), autoregressive models
    and even normalizing flow models. In this article, however, we will focus on GANs.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: æœ‰å‡ ä¸ªæœºå™¨å­¦ä¹ æ¨¡å‹å¯ä»¥æ„å»ºç”Ÿæˆå¼AIï¼Œä¾‹å¦‚å˜åˆ†è‡ªç¼–ç å™¨ï¼ˆVAEï¼‰ã€è‡ªå›å½’æ¨¡å‹ç”šè‡³æ­£åˆ™åŒ–æµæ¨¡å‹ã€‚ç„¶è€Œï¼Œæœ¬æ–‡å°†é‡ç‚¹è®¨è®ºGANsã€‚
- en: Autoencoders and GANs
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è‡ªç¼–ç å™¨ä¸GANs
- en: In the previous article, we dealt with autoencoders and saw their architecture,
    their use and implementation in PyTorch.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä¸Šä¸€ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘ä»¬è®¨è®ºäº†è‡ªç¼–ç å™¨ï¼Œå¹¶äº†è§£äº†å®ƒä»¬çš„æ¶æ„ã€ç”¨é€”å’Œåœ¨PyTorchä¸­çš„å®ç°ã€‚
- en: In short, Autoencoders receive an input x, compress it into a vector of smaller
    size z, called the latent vector, and finally from z reconstruct x in a more or
    less approximate way.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: ç®€è€Œè¨€ä¹‹ï¼Œè‡ªç¼–ç å™¨æ¥æ”¶è¾“å…¥xï¼Œå°†å…¶å‹ç¼©ä¸ºä¸€ä¸ªè¾ƒå°çš„å‘é‡zï¼Œç§°ä¸ºæ½œåœ¨å‘é‡ï¼Œç„¶åä»zä»¥æˆ–å¤šæˆ–å°‘çš„è¿‘ä¼¼æ–¹å¼é‡æ„xã€‚
- en: In Autoencoder we have no data generation, but simply an approximate reconstruction
    of the input. Now imagine that we break the Autoencoder in two and consider only
    the second part, the part where from the latent vector z the image is reconstructed.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è‡ªç¼–ç å™¨ä¸­ï¼Œæˆ‘ä»¬æ²¡æœ‰æ•°æ®ç”Ÿæˆï¼Œè€Œåªæ˜¯å¯¹è¾“å…¥çš„è¿‘ä¼¼é‡æ„ã€‚ç°åœ¨å‡è®¾æˆ‘ä»¬å°†è‡ªç¼–ç å™¨åˆ†æˆä¸¤éƒ¨åˆ†ï¼Œå¹¶ä»…è€ƒè™‘ç¬¬äºŒéƒ¨åˆ†ï¼Œå³ä»æ½œåœ¨å‘é‡zé‡æ„å›¾åƒçš„éƒ¨åˆ†ã€‚
- en: '![](../Images/674c788436be31599035404c61b840cc.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/674c788436be31599035404c61b840cc.png)'
- en: Output Generation (Image By Author)
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: è¾“å‡ºç”Ÿæˆï¼ˆä½œè€…æä¾›çš„å›¾åƒï¼‰
- en: In this case, we can say that the architecture is generative. In fact, given
    a vector of numbers as input this creates an image! Essentially this is what a
    generative AI does. The main difference though with respect to autoencoders is
    that we know well the probability distribution from which we take the latent vector
    z. For example, a *Gaussian(0,1)*.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬å¯ä»¥è¯´æ¶æ„æ˜¯ç”Ÿæˆå¼çš„ã€‚å®é™…ä¸Šï¼Œç»™å®šä¸€ä¸ªæ•°å­—å‘é‡ä½œä¸ºè¾“å…¥ï¼Œè¿™ä¼šåˆ›å»ºä¸€å¹…å›¾åƒï¼æœ¬è´¨ä¸Šï¼Œè¿™å°±æ˜¯ç”Ÿæˆå¼AIçš„ä½œç”¨ã€‚ä¸è‡ªç¼–ç å™¨çš„ä¸»è¦åŒºåˆ«åœ¨äºï¼Œæˆ‘ä»¬æ¸…æ¥šåœ°çŸ¥é“æˆ‘ä»¬ä»ä¸­è·å–æ½œåœ¨å‘é‡zçš„æ¦‚ç‡åˆ†å¸ƒã€‚ä¾‹å¦‚ï¼Œä¸€ä¸ª*Gaussian(0,1)*ã€‚
- en: So we thus have a way to generate images from random numbers taken from a Gaussian
    distribution, changing these random numbers will change the images we have in
    the output.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: å› æ­¤ï¼Œæˆ‘ä»¬æœ‰äº†ä¸€ç§ä»é«˜æ–¯åˆ†å¸ƒä¸­çš„éšæœºæ•°ç”Ÿæˆå›¾åƒçš„æ–¹æ³•ï¼Œæ”¹å˜è¿™äº›éšæœºæ•°å°†æ”¹å˜æˆ‘ä»¬è¾“å‡ºçš„å›¾åƒã€‚
- en: '![](../Images/01c55550204051a7c6d10b3d6487f4a3.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/01c55550204051a7c6d10b3d6487f4a3.png)'
- en: Generative Model (Image By Author)
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: ç”Ÿæˆæ¨¡å‹ï¼ˆå›¾ç‰‡ç”±ä½œè€…æä¾›ï¼‰
- en: GANs Architecture
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: GANsæ¶æ„
- en: The orange network shown in the previous image can be defined as a G function
    that given the input z generates the synthetic output *x_cap*, so *x_cap = G(z)*.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: å‰ä¸€å¼ å›¾ç‰‡ä¸­æ˜¾ç¤ºçš„æ©™è‰²ç½‘ç»œå¯ä»¥å®šä¹‰ä¸ºä¸€ä¸ªGå‡½æ•°ï¼Œç»™å®šè¾“å…¥zç”Ÿæˆåˆæˆè¾“å‡º *x_cap*ï¼Œå› æ­¤ *x_cap = G(z)*ã€‚
- en: The network will be initialized with random weights, so it will not initially
    be able to generate output that looks real, but only images that will contain
    noise. So we need to do some training to improve the performance of our network.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: ç½‘ç»œå°†ä»¥éšæœºæƒé‡åˆå§‹åŒ–ï¼Œå› æ­¤æœ€åˆå®ƒæ— æ³•ç”Ÿæˆçœ‹èµ·æ¥çœŸå®çš„è¾“å‡ºï¼Œåªèƒ½ç”ŸæˆåŒ…å«å™ªå£°çš„å›¾åƒã€‚å› æ­¤ï¼Œæˆ‘ä»¬éœ€è¦è¿›è¡Œä¸€äº›è®­ç»ƒæ¥æé«˜ç½‘ç»œçš„æ€§èƒ½ã€‚
- en: So letâ€™s imagine that we have a human annotator telling us each time whether
    the output is good or not, whether it looks real or not.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥è®©æˆ‘ä»¬è®¾æƒ³ä¸€ä¸‹ï¼Œæ¯å½“æˆ‘ä»¬å¾—åˆ°è¾“å‡ºæ—¶ï¼Œæœ‰ä¸€ä¸ªäººå·¥æ ‡æ³¨å‘˜å‘Šè¯‰æˆ‘ä»¬è¿™äº›è¾“å‡ºæ˜¯å¦è‰¯å¥½ï¼Œæ˜¯å¦çœ‹èµ·æ¥çœŸå®ã€‚
- en: '![](../Images/0386f4e179933422ffda2952ed945011.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0386f4e179933422ffda2952ed945011.png)'
- en: Towards GANs (Image By Author)
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: æœå‘GANsï¼ˆå›¾ç‰‡ç”±ä½œè€…æä¾›ï¼‰
- en: Obviously, we cannot do network training expecting a person to make continuous
    judgments about the output. But then what can we do?
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: æ˜¾ç„¶ï¼Œæˆ‘ä»¬ä¸èƒ½è¿›è¡Œç½‘ç»œè®­ç»ƒï¼ŒæœŸæœ›ä¸€ä¸ªäººå¯¹è¾“å‡ºè¿›è¡ŒæŒç»­åˆ¤æ–­ã€‚é‚£ä¹ˆæˆ‘ä»¬å¯ä»¥åšä»€ä¹ˆå‘¢ï¼Ÿ
- en: If you think about it what the annotator does, in this case, is binary classification!
    And we in Machine Learning are great at developing classifiers. So we can simply
    train a classifier that weâ€™ll call Discriminator, and weâ€™ll denote with the function
    D(), which has to be trained to recognize synthetic (fake) images versus real
    images. So we will feed it both fake images and real images.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœä½ è€ƒè™‘ä¸€ä¸‹æ ‡æ³¨å‘˜æ‰€åšçš„å·¥ä½œï¼Œåœ¨è¿™ç§æƒ…å†µä¸‹å°±æ˜¯äºŒå…ƒåˆ†ç±»ï¼è€Œæˆ‘ä»¬åœ¨æœºå™¨å­¦ä¹ ä¸­éå¸¸æ“…é•¿å¼€å‘åˆ†ç±»å™¨ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å¯ä»¥ç®€å•åœ°è®­ç»ƒä¸€ä¸ªåˆ†ç±»å™¨ï¼Œæˆ‘ä»¬ç§°ä¹‹ä¸ºé‰´åˆ«å™¨ï¼Œå¹¶ç”¨å‡½æ•°D()è¡¨ç¤ºï¼Œå®ƒå¿…é¡»è¢«è®­ç»ƒæ¥è¯†åˆ«åˆæˆï¼ˆè™šå‡ï¼‰å›¾åƒä¸çœŸå®å›¾åƒã€‚å› æ­¤æˆ‘ä»¬å°†åŒæ—¶è¾“å…¥è™šå‡å›¾åƒå’ŒçœŸå®å›¾åƒã€‚
- en: So this is how our architecture changes.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥è¿™å°±æ˜¯æˆ‘ä»¬çš„æ¶æ„å¦‚ä½•å˜åŒ–çš„ã€‚
- en: '![](../Images/8bacad55716d720d37ccf3e4e734e940.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8bacad55716d720d37ccf3e4e734e940.png)'
- en: GANs Architecture (Image By Author)
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: GANsæ¶æ„ï¼ˆå›¾ç‰‡ç”±ä½œè€…æä¾›ï¼‰
- en: In short, the architecture is not too complex. The difficulty comes at the time
    of having to train these two networks G and D.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: ç®€è€Œè¨€ä¹‹ï¼Œæ¶æ„å¹¶ä¸å¤æ‚ã€‚å›°éš¾åœ¨äºè®­ç»ƒè¿™ä¸¤ä¸ªç½‘ç»œGå’ŒDæ—¶ã€‚
- en: It is clear that if in training, the two networks have to improve together,
    they need to find some kind of balance. Because if, for example, D gets too good
    at distinguishing fake images from real ones before G gets good at generating
    them, it is quite natural that G will never get better and we will never have
    our generator ready to be used.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: å¾ˆæ˜æ˜¾ï¼Œå¦‚æœåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œè¿™ä¸¤ä¸ªç½‘ç»œå¿…é¡»ä¸€èµ·æ”¹è¿›ï¼Œå®ƒä»¬éœ€è¦æ‰¾åˆ°æŸç§å¹³è¡¡ã€‚å› ä¸ºä¾‹å¦‚ï¼Œå¦‚æœDåœ¨åŒºåˆ†è™šå‡å›¾åƒä¸çœŸå®å›¾åƒæ–¹é¢å˜å¾—è¿‡äºä¼˜ç§€ï¼Œè€ŒGåœ¨ç”Ÿæˆå›¾åƒæ–¹é¢å°šæœªæå‡ï¼Œé‚£ä¹ˆGæ°¸è¿œä¸ä¼šå˜å¾—æ›´å¥½ï¼Œæˆ‘ä»¬çš„ç”Ÿæˆå™¨ä¹Ÿæ°¸è¿œæ— æ³•å‡†å¤‡å¥½ä½¿ç”¨ã€‚
- en: So the two networks are said to play an adversarial game in which G must fool
    D, and D must not be fooled by G.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥è¿™ä¸¤ä¸ªç½‘ç»œè¢«ç§°ä¸ºåœ¨ç©ä¸€ä¸ªå¯¹æŠ—æ¸¸æˆï¼Œå…¶ä¸­Gå¿…é¡»æ¬ºéª—Dï¼Œè€ŒDä¸èƒ½è¢«Gæ¬ºéª—ã€‚
- en: GANs Objective Function
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: GANsç›®æ ‡å‡½æ•°
- en: If we want to be a bit more precise, we can say that D and G have two complementary
    goals. Letâ€™s suppose we want to generate images.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæˆ‘ä»¬æƒ³æ›´ç²¾ç¡®ä¸€ç‚¹ï¼Œå¯ä»¥è¯´Då’ŒGæœ‰ä¸¤ä¸ªäº’è¡¥çš„ç›®æ ‡ã€‚å‡è®¾æˆ‘ä»¬æƒ³ç”Ÿæˆå›¾åƒã€‚
- en: We define by D(x) the probability that x is a real image. Obviously, the discriminator
    wants to maximize its probability of recognizing real inputs from fake inputs.
    So we want to maximize D(x) when x is drawn from our distribution of real images.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ç”¨D(x)å®šä¹‰xæ˜¯çœŸå®å›¾åƒçš„æ¦‚ç‡ã€‚æ˜¾ç„¶ï¼Œé‰´åˆ«å™¨æƒ³è¦æœ€å¤§åŒ–å…¶è¯†åˆ«çœŸå®è¾“å…¥ä¸è™šå‡è¾“å…¥çš„æ¦‚ç‡ã€‚å› æ­¤ï¼Œå½“xä»æˆ‘ä»¬çš„çœŸå®å›¾åƒåˆ†å¸ƒä¸­æŠ½å–æ—¶ï¼Œæˆ‘ä»¬æƒ³è¦æœ€å¤§åŒ–D(x)ã€‚
- en: In contrast, the purpose of the generator G is to fool the discriminator. So
    if *G(z)* is the fake image generated by G, *D(G(z))* is the probability that
    D will recognize a fake image as real. Then *1-D(G(z))* is the probability that
    D correctly recognizes a fake image as fake. So Gâ€™s goal is to minimize *1-D(G(z))*,
    since he does want to fool D.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: ç›¸æ¯”ä¹‹ä¸‹ï¼Œç”Ÿæˆå™¨Gçš„ç›®çš„æ˜¯æ¬ºéª—é‰´åˆ«å™¨ã€‚å› æ­¤ï¼Œå¦‚æœ *G(z)* æ˜¯ç”±Gç”Ÿæˆçš„è™šå‡å›¾åƒï¼Œ *D(G(z))* æ˜¯Då°†è™šå‡å›¾åƒè¯†åˆ«ä¸ºçœŸå®å›¾åƒçš„æ¦‚ç‡ã€‚é‚£ä¹ˆ *1-D(G(z))*
    æ˜¯Dæ­£ç¡®è¯†åˆ«è™šå‡å›¾åƒä¸ºè™šå‡çš„æ¦‚ç‡ã€‚å› æ­¤Gçš„ç›®æ ‡æ˜¯æœ€å°åŒ– *1-D(G(z))*ï¼Œå› ä¸ºå®ƒç¡®å®æƒ³è¦æ¬ºéª—Dã€‚
- en: 'So in the end we can sum up this game of maximization and minimization in the
    formula we find in the original paper (the formula looks a bit more concept but
    we have seen the concept):'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥æœ€ç»ˆæˆ‘ä»¬å¯ä»¥å°†è¿™ç§æœ€å¤§åŒ–å’Œæœ€å°åŒ–çš„æ¸¸æˆæ€»ç»“åˆ°åŸå§‹è®ºæ–‡ä¸­çš„å…¬å¼é‡Œï¼ˆå…¬å¼çœ‹èµ·æ¥æ›´å…·æ¦‚å¿µæ€§ï¼Œä½†æˆ‘ä»¬å·²ç»çœ‹è¿‡è¿™ä¸ªæ¦‚å¿µï¼‰ï¼š
- en: '![](../Images/a8adc55a900fd1a625d8c924e8c3a0e8.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a8adc55a900fd1a625d8c924e8c3a0e8.png)'
- en: 'Objective Function (src: [https://arxiv.org/pdf/1406.2661.pdf](https://arxiv.org/pdf/1406.2661.pdf))'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: ç›®æ ‡å‡½æ•°ï¼ˆæ¥æºï¼š[https://arxiv.org/pdf/1406.2661.pdf](https://arxiv.org/pdf/1406.2661.pdf)ï¼‰
- en: GANs Implementation
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: GANs å®ç°
- en: We now implement a GAN capable of generating MNIST images automatically.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ç°åœ¨å®ç°ä¸€ä¸ªèƒ½å¤Ÿè‡ªåŠ¨ç”ŸæˆMNISTå›¾åƒçš„GANã€‚
- en: As usual, I will run my code a cloud-based environment Deepnote but you can
    use Google Colab as well, so even those who donâ€™t have a GPU on their laptop can
    run this code.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: å’Œå¾€å¸¸ä¸€æ ·ï¼Œæˆ‘å°†æˆ‘çš„ä»£ç è¿è¡Œåœ¨åŸºäºäº‘çš„ç¯å¢ƒDeepnoteä¸­ï¼Œä½†ä½ ä¹Ÿå¯ä»¥ä½¿ç”¨Google Colabï¼Œè¿™æ ·å³ä½¿æ²¡æœ‰GPUçš„ç”¨æˆ·ä¹Ÿå¯ä»¥è¿è¡Œè¿™æ®µä»£ç ã€‚
- en: We start by going to check whether indeed our hardware has a GPU.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬é¦–å…ˆæ£€æŸ¥ä¸€ä¸‹æˆ‘ä»¬çš„ç¡¬ä»¶æ˜¯å¦ç¡®å®æœ‰GPUã€‚
- en: Now if youâ€™re using Colab you can connect to Google Drive.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœä½ åœ¨ä½¿ç”¨Colabï¼Œä½ å¯ä»¥è¿æ¥åˆ°Google Driveã€‚
- en: '[PRE0]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Letâ€™s import the needed libraries.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬å¯¼å…¥æ‰€éœ€çš„åº“ã€‚
- en: Now we need to create the functions that will define our networks, generator
    and discriminator.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬éœ€è¦åˆ›å»ºå®šä¹‰æˆ‘ä»¬ç½‘ç»œçš„å‡½æ•°ï¼Œå³ç”Ÿæˆå™¨å’Œåˆ¤åˆ«å™¨ã€‚
- en: The MNIST images have 784 pixels (since the images are 28x28). So the generator
    given as input a random z vector of length 20 will have to output a vector of
    784 which will be our fake image.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: MNISTå›¾åƒæœ‰784ä¸ªåƒç´ ï¼ˆå› ä¸ºå›¾åƒæ˜¯28x28ï¼‰ã€‚å› æ­¤ï¼Œç”Ÿæˆå™¨è¾“å…¥ä¸€ä¸ªé•¿åº¦ä¸º20çš„éšæœºzå‘é‡ï¼Œå°†è¾“å‡ºä¸€ä¸ª784çš„å‘é‡ï¼Œè¿™å°±æ˜¯æˆ‘ä»¬çš„ä¼ªé€ å›¾åƒã€‚
- en: Instead, the discriminator will receive as input a 28x28 = 784-pixel image,
    it will have a single neuron in output that will classify the image as true or
    fake.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: ç›¸åï¼Œåˆ¤åˆ«å™¨å°†æ¥æ”¶ä¸€ä¸ª28x28 = 784åƒç´ çš„å›¾åƒä½œä¸ºè¾“å…¥ï¼Œå®ƒå°†æœ‰ä¸€ä¸ªè¾“å‡ºç¥ç»å…ƒæ¥å°†å›¾åƒåˆ†ç±»ä¸ºçœŸå®æˆ–ä¼ªé€ ã€‚
- en: '![](../Images/f42e0cc6024372aafabd130d24076065.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f42e0cc6024372aafabd130d24076065.png)'
- en: Generator (Image By Author)
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: ç”Ÿæˆå™¨ï¼ˆå›¾ç‰‡ç”±ä½œè€…æä¾›ï¼‰
- en: This function is used to instantiate the generator. Each layer will use a LeakyReLU
    (a variation of the ReLU, that works best in GANs) as its activation function,
    except the output is followed by a Hyperbolic Tangent (Tanh) function that results
    in output a number in the range [-1,1].
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªå‡½æ•°ç”¨äºå®ä¾‹åŒ–ç”Ÿæˆå™¨ã€‚æ¯ä¸€å±‚å°†ä½¿ç”¨LeakyReLUï¼ˆReLUçš„ä¸€ç§å˜ä½“ï¼Œåœ¨GANsä¸­è¡¨ç°æœ€ä½³ï¼‰ä½œä¸ºå…¶æ¿€æ´»å‡½æ•°ï¼Œé™¤äº†è¾“å‡ºå±‚åæ¥ä¸€ä¸ªåŒæ›²æ­£åˆ‡ï¼ˆTanhï¼‰å‡½æ•°ï¼Œä½¿å¾—è¾“å‡ºèŒƒå›´ä¸º[-1,1]ã€‚
- en: '![](../Images/6928d3027580731763156ede1b4577f6.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6928d3027580731763156ede1b4577f6.png)'
- en: Discriminator (Image By Author)
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: åˆ¤åˆ«å™¨ï¼ˆå›¾ç‰‡ç”±ä½œè€…æä¾›ï¼‰
- en: Instead, this function defines the discriminator network, which has the special
    feature of using dropout after hidden layers (in the base case only one hidden
    layer). The output goes through a sigmoid function since it must give us the probability
    of being a real or fake image.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: ç›¸åï¼Œè¿™ä¸ªå‡½æ•°å®šä¹‰äº†åˆ¤åˆ«å™¨ç½‘ç»œï¼Œå…¶ç‰¹æ®ŠåŠŸèƒ½æ˜¯åœ¨éšè—å±‚ä¹‹åä½¿ç”¨dropoutï¼ˆåœ¨åŸºæœ¬æƒ…å†µä¸‹åªæœ‰ä¸€ä¸ªéšè—å±‚ï¼‰ã€‚è¾“å‡ºé€šè¿‡ä¸€ä¸ªsigmoidå‡½æ•°ï¼Œå› ä¸ºå®ƒå¿…é¡»ç»™å‡ºå›¾åƒæ˜¯çœŸå®çš„è¿˜æ˜¯ä¼ªé€ çš„æ¦‚ç‡ã€‚
- en: Now we also download the MNIST dataset that we are going to use. The MNIST dataset
    is in a range [0,255], but we want it in the range [-1,1] so that it will be similar
    to the data generated by the Generator network. So we also apply some preprocessing
    to do this.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬è¿˜ä¸‹è½½äº†æˆ‘ä»¬è¦ä½¿ç”¨çš„MNISTæ•°æ®é›†ã€‚MNISTæ•°æ®é›†çš„èŒƒå›´æ˜¯[0,255]ï¼Œä½†æˆ‘ä»¬å¸Œæœ›å®ƒåœ¨èŒƒå›´[-1,1]ï¼Œè¿™æ ·å®ƒä¼šç±»ä¼¼äºç”Ÿæˆå™¨ç½‘ç»œç”Ÿæˆçš„æ•°æ®ã€‚å› æ­¤ï¼Œæˆ‘ä»¬è¿˜å¯¹æ•°æ®è¿›è¡Œäº†é¢„å¤„ç†ã€‚
- en: Now we come to the most important part. We need to create the functions that
    define the training of our network. We have already said that we should pull the
    discriminator separately from the generator, so we will have 2 functions.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬æ¥åˆ°æœ€é‡è¦çš„éƒ¨åˆ†ã€‚æˆ‘ä»¬éœ€è¦åˆ›å»ºå®šä¹‰æˆ‘ä»¬ç½‘ç»œè®­ç»ƒçš„å‡½æ•°ã€‚æˆ‘ä»¬å·²ç»æåˆ°è¿‡ï¼Œæˆ‘ä»¬åº”è¯¥å°†åˆ¤åˆ«å™¨ä¸ç”Ÿæˆå™¨åˆ†å¼€ï¼Œå› æ­¤æˆ‘ä»¬å°†æœ‰ä¸¤ä¸ªå‡½æ•°ã€‚
- en: The discriminator will be trained both on the fake data and on real data. When
    we train it on real data the labels will always be *â€œrealâ€ = 1*. So we create
    a vector of 1 with *d_labels_real = torch.ones(batch_size, 1, device = device)*.
    Then we feed the input x to the model and calculate the loss using *Binary Cross
    Entropy*.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: åˆ¤åˆ«å™¨å°†åŒæ—¶åœ¨å‡æ•°æ®å’ŒçœŸå®æ•°æ®ä¸Šè¿›è¡Œè®­ç»ƒã€‚å½“æˆ‘ä»¬åœ¨çœŸå®æ•°æ®ä¸Šè®­ç»ƒæ—¶ï¼Œæ ‡ç­¾å°†å§‹ç»ˆæ˜¯*â€œrealâ€ = 1*ã€‚å› æ­¤ï¼Œæˆ‘ä»¬åˆ›å»ºä¸€ä¸ªå…¨ä¸º1çš„å‘é‡ï¼Œå³*d_labels_real
    = torch.ones(batch_size, 1, device = device)*ã€‚ç„¶åï¼Œæˆ‘ä»¬å°†è¾“å…¥xé€å…¥æ¨¡å‹ï¼Œå¹¶ä½¿ç”¨*Binary Cross Entropy*è®¡ç®—æŸå¤±ã€‚
- en: We do the same thing by feeding fake data. Here the labels will all be zero,
    *d_labels_fake = torch.zeros(batch_size, 1, device = device)*. The input instead
    will be the fake data, that is, the output of the generator *g_output = gen_model(input_z)*.
    And we calculate the loss in the same way.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬é€šè¿‡è¾“å…¥ä¼ªæ•°æ®åšåŒæ ·çš„äº‹æƒ…ã€‚è¿™é‡Œçš„æ ‡ç­¾å°†å…¨éƒ¨ä¸ºé›¶ï¼Œ*d_labels_fake = torch.zeros(batch_size, 1, device
    = device)*ã€‚è¾“å…¥åˆ™æ˜¯ä¼ªæ•°æ®ï¼Œå³ç”Ÿæˆå™¨çš„è¾“å‡º *g_output = gen_model(input_z)*ã€‚æˆ‘ä»¬ä»¥ç›¸åŒçš„æ–¹å¼è®¡ç®—æŸå¤±ã€‚
- en: The final loss will be the sum of the two losses.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€ç»ˆæŸå¤±å°†æ˜¯ä¸¤ä¸ªæŸå¤±çš„æ€»å’Œã€‚
- en: As for the generator train function, the implementation is slightly different.
    The generator takes as input the output of the discriminator since it has to see
    if D has figured out whether it is a fake or real image. And based on that it
    calculates its loss.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºç”Ÿæˆå™¨è®­ç»ƒå‡½æ•°ï¼Œå®æ–½ç•¥æœ‰ä¸åŒã€‚ç”Ÿæˆå™¨çš„è¾“å…¥æ˜¯åˆ¤åˆ«å™¨çš„è¾“å‡ºï¼Œå› ä¸ºå®ƒéœ€è¦æŸ¥çœ‹Dæ˜¯å¦å·²è¯†åˆ«å‡ºå›¾åƒæ˜¯çœŸå®è¿˜æ˜¯ä¼ªé€ çš„ã€‚åŸºäºæ­¤ï¼Œå®ƒè®¡ç®—å…¶æŸå¤±ã€‚
- en: Now we can initialize our two networks.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬å¯ä»¥åˆå§‹åŒ–æˆ‘ä»¬çš„ä¸¤ä¸ªç½‘ç»œäº†ã€‚
- en: Letâ€™s define a function to create network-generated samples, so as we go along
    we can see how the fake images improve as the training epochs increase.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬å®šä¹‰ä¸€ä¸ªå‡½æ•°æ¥åˆ›å»ºç½‘ç»œç”Ÿæˆçš„æ ·æœ¬ï¼Œè¿™æ ·éšç€è®­ç»ƒå‘¨æœŸçš„å¢åŠ ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°ä¼ªå›¾åƒçš„æ”¹è¿›ã€‚
- en: Now we can finally train the net! We save the losses each time in a list so
    we can plot them later.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬ç»ˆäºå¯ä»¥è®­ç»ƒç½‘ç»œäº†ï¼æˆ‘ä»¬æ¯æ¬¡å°†æŸå¤±ä¿å­˜åˆ°ä¸€ä¸ªåˆ—è¡¨ä¸­ï¼Œä»¥ä¾¿åç»­ç»˜å›¾ã€‚
- en: The training should take about an hour, depending on the hardware cha you used
    certainly. But in the end, you can print out your fake data and have something
    like this.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: è®­ç»ƒåº”è¯¥å¤§çº¦éœ€è¦ä¸€ä¸ªå°æ—¶ï¼Œå…·ä½“å–å†³äºä½ ä½¿ç”¨çš„ç¡¬ä»¶ã€‚ä½†æœ€åï¼Œä½ å¯ä»¥æ‰“å°å‡ºä½ çš„ä¼ªæ•°æ®ï¼Œå¾—åˆ°ç±»ä¼¼è¿™æ ·çš„ç»“æœã€‚
- en: In my case, I trained for a few epochs so the results are not great, but you
    are beginning to get a glimpse that the network was learning to generate MNIST-like
    images.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æˆ‘çš„æƒ…å†µä¸‹ï¼Œæˆ‘è®­ç»ƒäº†å‡ ä¸ªå‘¨æœŸï¼Œæ‰€ä»¥ç»“æœå¹¶ä¸ç†æƒ³ï¼Œä½†ä½ å¼€å§‹å¯ä»¥çœ‹åˆ°ç½‘ç»œæ­£åœ¨å­¦ä¹ ç”Ÿæˆç±»ä¼¼MNISTçš„å›¾åƒã€‚
- en: '![](../Images/8edea14b537654ada61c0f347ca2fe77.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8edea14b537654ada61c0f347ca2fe77.png)'
- en: Fake Data (Image By Author)
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: ä¼ªæ•°æ®ï¼ˆå›¾ç‰‡ç”±ä½œè€…æä¾›ï¼‰
- en: Final Thoughts
  id: totrans-71
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æœ€åçš„æƒ³æ³•
- en: In this article, we looked at how the architecture of GANs in more detail. We
    studied their objective function and were able to implement a network capable
    of generating images from the MNIST dataset! The operation of these networks is
    not too complicated but their training certainly is. Since we need to find that
    balance that allows both networks to learn. If you enjoyed this article follow
    me to read the next one on DCGANs.[ğŸ˜‰](https://emojipedia.org/it/apple/ios-15.4/faccina-che-fa-l-occhiolino/)
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘ä»¬æ›´è¯¦ç»†åœ°æ¢è®¨äº†GANçš„æ¶æ„ã€‚æˆ‘ä»¬ç ”ç©¶äº†å®ƒä»¬çš„ç›®æ ‡å‡½æ•°ï¼Œå¹¶å®ç°äº†ä¸€ä¸ªèƒ½å¤Ÿç”ŸæˆMNISTæ•°æ®é›†å›¾åƒçš„ç½‘ç»œï¼è¿™äº›ç½‘ç»œçš„æ“ä½œå¹¶ä¸å¤æ‚ï¼Œä½†å®ƒä»¬çš„è®­ç»ƒç¡®å®å¾ˆå›°éš¾ã€‚å› ä¸ºæˆ‘ä»¬éœ€è¦æ‰¾åˆ°ä¸€ä¸ªå¹³è¡¡ç‚¹ï¼Œè®©ä¸¤ä¸ªç½‘ç»œéƒ½èƒ½å­¦ä¹ ã€‚å¦‚æœä½ å–œæ¬¢è¿™ç¯‡æ–‡ç« ï¼Œè¯·å…³æ³¨æˆ‘ï¼Œé˜…è¯»ä¸‹ä¸€ç¯‡å…³äºDCGANçš„æ–‡ç« ã€‚[ğŸ˜‰](https://emojipedia.org/it/apple/ios-15.4/faccina-che-fa-l-occhiolino/)
- en: The End
  id: totrans-73
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç»“æŸ
- en: '*Marcello Politi*'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '*Marcello Politi*'
- en: '[Linkedin](https://www.linkedin.com/in/marcello-politi/), [Twitter](https://twitter.com/_March08_),
    [Website](https://marcello-politi.super.site/)'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '[Linkedin](https://www.linkedin.com/in/marcello-politi/)ï¼Œ[Twitter](https://twitter.com/_March08_)ï¼Œ[Website](https://marcello-politi.super.site/)'
