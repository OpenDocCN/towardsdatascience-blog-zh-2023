- en: DINO â€” A Foundation Model for Computer Vision
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: DINO â€” ä¸€ç§è®¡ç®—æœºè§†è§‰çš„åŸºç¡€æ¨¡å‹
- en: åŸæ–‡ï¼š[https://towardsdatascience.com/dino-a-foundation-model-for-computer-vision-4cb08e821b18?source=collection_archive---------0-----------------------#2023-09-27](https://towardsdatascience.com/dino-a-foundation-model-for-computer-vision-4cb08e821b18?source=collection_archive---------0-----------------------#2023-09-27)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡ï¼š[https://towardsdatascience.com/dino-a-foundation-model-for-computer-vision-4cb08e821b18?source=collection_archive---------0-----------------------#2023-09-27](https://towardsdatascience.com/dino-a-foundation-model-for-computer-vision-4cb08e821b18?source=collection_archive---------0-----------------------#2023-09-27)
- en: '[ğŸš€Saschaâ€™s Paper Club](https://towardsdatascience.com/tagged/saschas-paper-club)'
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '[ğŸš€Saschaçš„è®ºæ–‡ä¿±ä¹éƒ¨](https://towardsdatascience.com/tagged/saschas-paper-club)'
- en: Emerging Properties in Self-Supervised Vision Transformers by M. Caron et. al.
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ç”± M. Caron ç­‰äººæ’°å†™çš„ã€Šè‡ªç›‘ç£è§†è§‰å˜æ¢å™¨ä¸­çš„æ–°å…´ç‰¹æ€§ã€‹
- en: '[](https://medium.com/@SaschaKirch?source=post_page-----4cb08e821b18--------------------------------)[![Sascha
    Kirch](../Images/a0d45da9dc9c602075b2810786c660c9.png)](https://medium.com/@SaschaKirch?source=post_page-----4cb08e821b18--------------------------------)[](https://towardsdatascience.com/?source=post_page-----4cb08e821b18--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----4cb08e821b18--------------------------------)
    [Sascha Kirch](https://medium.com/@SaschaKirch?source=post_page-----4cb08e821b18--------------------------------)'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@SaschaKirch?source=post_page-----4cb08e821b18--------------------------------)[![Sascha
    Kirch](../Images/a0d45da9dc9c602075b2810786c660c9.png)](https://medium.com/@SaschaKirch?source=post_page-----4cb08e821b18--------------------------------)[](https://towardsdatascience.com/?source=post_page-----4cb08e821b18--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----4cb08e821b18--------------------------------)
    [Sascha Kirch](https://medium.com/@SaschaKirch?source=post_page-----4cb08e821b18--------------------------------)'
- en: Â·
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: Â·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F5c38dace9d5e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdino-a-foundation-model-for-computer-vision-4cb08e821b18&user=Sascha+Kirch&userId=5c38dace9d5e&source=post_page-5c38dace9d5e----4cb08e821b18---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----4cb08e821b18--------------------------------)
    Â·13 min readÂ·Sep 27, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F4cb08e821b18&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdino-a-foundation-model-for-computer-vision-4cb08e821b18&user=Sascha+Kirch&userId=5c38dace9d5e&source=-----4cb08e821b18---------------------clap_footer-----------)'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '[å…³æ³¨](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F5c38dace9d5e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdino-a-foundation-model-for-computer-vision-4cb08e821b18&user=Sascha+Kirch&userId=5c38dace9d5e&source=post_page-5c38dace9d5e----4cb08e821b18---------------------post_header-----------)
    å‘è¡¨åœ¨ [Towards Data Science](https://towardsdatascience.com/?source=post_page-----4cb08e821b18--------------------------------)
    Â· 13åˆ†é’Ÿé˜…è¯» Â· 2023å¹´9æœˆ27æ—¥ [](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F4cb08e821b18&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdino-a-foundation-model-for-computer-vision-4cb08e821b18&user=Sascha+Kirch&userId=5c38dace9d5e&source=-----4cb08e821b18---------------------clap_footer-----------)'
- en: --
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4cb08e821b18&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdino-a-foundation-model-for-computer-vision-4cb08e821b18&source=-----4cb08e821b18---------------------bookmark_footer-----------)'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4cb08e821b18&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdino-a-foundation-model-for-computer-vision-4cb08e821b18&source=-----4cb08e821b18---------------------bookmark_footer-----------)'
- en: It is an exciting decade for computer vision. Great successes from the natural
    language domain are transferred to the vision domain including the introduction
    of the ViT (vision transformer) and lately large-scale self-supervised pre-training
    techniques have made headlines under the name of foundation models.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: è®¡ç®—æœºè§†è§‰çš„è¿™ä¸€åå¹´ä»¤äººå…´å¥‹ã€‚æ¥è‡ªè‡ªç„¶è¯­è¨€é¢†åŸŸçš„å·¨å¤§æˆåŠŸè¢«è½¬ç§»åˆ°è§†è§‰é¢†åŸŸï¼ŒåŒ…æ‹¬ ViTï¼ˆè§†è§‰å˜æ¢å™¨ï¼‰çš„å¼•å…¥ï¼Œä»¥åŠæœ€è¿‘çš„å¤§è§„æ¨¡è‡ªç›‘ç£é¢„è®­ç»ƒæŠ€æœ¯åœ¨åŸºç¡€æ¨¡å‹çš„åä¸‹æˆä¸ºå¤´æ¡æ–°é—»ã€‚
- en: 'Today we are looking into a framework called DINO (self **DI**stillation, **N**O
    labels), a visual foundation model built on interesting properties of ViTs. It
    is also the predecessor of one of todayâ€™s best performing foundation models: [DINOv2](https://arxiv.org/abs/2304.07193).'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: ä»Šå¤©æˆ‘ä»¬å°†æ·±å…¥äº†è§£ä¸€ä¸ªåä¸º DINOï¼ˆè‡ª **DI**è’¸é¦ï¼Œ**N**O æ ‡ç­¾ï¼‰çš„æ¡†æ¶ï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäº ViTï¼ˆè§†è§‰å˜æ¢å™¨ï¼‰æœ‰è¶£ç‰¹æ€§çš„è§†è§‰åŸºç¡€æ¨¡å‹ã€‚å®ƒä¹Ÿæ˜¯ä»Šå¤©è¡¨ç°æœ€ä½³çš„åŸºç¡€æ¨¡å‹ä¹‹ä¸€çš„å‰èº«ï¼š[DINOv2](https://arxiv.org/abs/2304.07193)ã€‚
- en: '![](../Images/826ce8cc3feae5497593efe2c4e9631c.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/826ce8cc3feae5497593efe2c4e9631c.png)'
- en: Image created from [publication](https://arxiv.org/abs/2104.14294) by [Sascha
    Kirch](https://medium.com/@SaschaKirch)
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ç‰‡æ¥æºäº [å‡ºç‰ˆç‰©](https://arxiv.org/abs/2104.14294)ï¼Œä½œè€… [Sascha Kirch](https://medium.com/@SaschaKirch)
- en: '**Paper:** [Emerging Properties in Self-Supervised Vision Transformers](https://arxiv.org/abs/2104.14294),
    by [Mathilde Caron](https://arxiv.org/search/cs?searchtype=author&query=Caron%2C+M)
    et.al., 29\. Apr. 2021'
  id: totrans-13
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**è®ºæ–‡ï¼š** [è‡ªç›‘ç£è§†è§‰å˜æ¢å™¨ä¸­çš„æ–°å…´ç‰¹æ€§](https://arxiv.org/abs/2104.14294)ï¼Œä½œè€… [Mathilde Caron](https://arxiv.org/search/cs?searchtype=author&query=Caron%2C+M)
    ç­‰ï¼Œ2021 å¹´ 4 æœˆ 29 æ—¥'
- en: ''
  id: totrans-14
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**Resources:** [GitHub](https://github.com/facebookresearch/dino) â€” [Blog Post](https://ai.meta.com/blog/dino-paws-computer-vision-with-self-supervised-transformers-and-10x-more-efficient-training/)'
  id: totrans-15
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**èµ„æºï¼š** [GitHub](https://github.com/facebookresearch/dino) â€” [åšå®¢æ–‡ç« ](https://ai.meta.com/blog/dino-paws-computer-vision-with-self-supervised-transformers-and-10x-more-efficient-training/)'
- en: ''
  id: totrans-16
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**Category:** foundation model, computer vision, vision transformer, knowledge
    distillation, similarity learning, self-supervised learning'
  id: totrans-17
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**ç±»åˆ«ï¼š** åŸºç¡€æ¨¡å‹ã€è®¡ç®—æœºè§†è§‰ã€è§†è§‰å˜æ¢å™¨ã€çŸ¥è¯†è’¸é¦ã€ç›¸ä¼¼æ€§å­¦ä¹ ã€è‡ªç›‘ç£å­¦ä¹ '
- en: ''
  id: totrans-18
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[**Other Walkthroughs**](https://medium.com/@SaschaKirch/list/paper-walkthroughs-by-sascha-kirch-89c7847da8e2)**:**'
  id: totrans-19
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '[**å…¶ä»–è®²è§£**](https://medium.com/@SaschaKirch/list/paper-walkthroughs-by-sascha-kirch-89c7847da8e2)**ï¼š**'
- en: '[[BYOL](/byol-the-alternative-to-contrastive-self-supervised-learning-5d0a26983d7c?sk=fc5a3b3a556088181d8726226862252c)]
    â€” [[CLIP](/the-clip-foundation-model-7770858b487d?sk=a7b10ba1d0c3a20ecd4adb8200a48500)]
    â€” [[GLIP](/glip-introducing-language-image-pre-training-to-object-detection-5ddb601873aa?sk=4f0acb404a38d342b7669f861c013a05)]
    â€” [[Segment Anything](/segment-anything-promptable-segmentation-of-arbitrary-objects-f28958c5612d?sk=bd1311a6d8b1e0e6d3369d536dba0700)]
    â€” [[DINO](/segment-anything-promptable-segmentation-of-arbitrary-objects-f28958c5612d?sk=bd1311a6d8b1e0e6d3369d536dba0700)]
    â€” [[Depth Anything](https://medium.com/towards-data-science/depth-anything-a-foundation-model-for-monocular-depth-estimation-8a7920b5c9cc?sk=fc6197edd68e6137c3396c83e50f65cb)]
    â€” [[DDPM](/the-rise-of-diffusion-models-a-new-era-of-generative-deep-learning-3ef4779f6e1b?sk=8c178422a977c6f49ec24b13502be4fd)]'
  id: totrans-20
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '[[BYOL](/byol-the-alternative-to-contrastive-self-supervised-learning-5d0a26983d7c?sk=fc5a3b3a556088181d8726226862252c)]
    â€” [[CLIP](/the-clip-foundation-model-7770858b487d?sk=a7b10ba1d0c3a20ecd4adb8200a48500)]
    â€” [[GLIP](/glip-introducing-language-image-pre-training-to-object-detection-5ddb601873aa?sk=4f0acb404a38d342b7669f861c013a05)]
    â€” [[Segment Anything](/segment-anything-promptable-segmentation-of-arbitrary-objects-f28958c5612d?sk=bd1311a6d8b1e0e6d3369d536dba0700)]
    â€” [[DINO](/segment-anything-promptable-segmentation-of-arbitrary-objects-f28958c5612d?sk=bd1311a6d8b1e0e6d3369d536dba0700)]
    â€” [[Depth Anything](https://medium.com/towards-data-science/depth-anything-a-foundation-model-for-monocular-depth-estimation-8a7920b5c9cc?sk=fc6197edd68e6137c3396c83e50f65cb)]
    â€” [[DDPM](/the-rise-of-diffusion-models-a-new-era-of-generative-deep-learning-3ef4779f6e1b?sk=8c178422a977c6f49ec24b13502be4fd)]'
- en: Outline
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å¤§çº²
- en: Context & Background
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: èƒŒæ™¯ä¸èƒŒæ™¯
- en: Method
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æ–¹æ³•
