- en: Vector Representations for Machine Learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习中的向量表示
- en: 原文：[https://towardsdatascience.com/vector-representations-for-machine-learning-5047c50aaeff](https://towardsdatascience.com/vector-representations-for-machine-learning-5047c50aaeff)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/vector-representations-for-machine-learning-5047c50aaeff](https://towardsdatascience.com/vector-representations-for-machine-learning-5047c50aaeff)
- en: '*How data scientists convert real-world objects in numerical representation
    for the development of machine learning models*'
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '*数据科学家如何将现实世界的对象转换为数值表示以开发机器学习模型*'
- en: '[](https://medium.com/@theDrewDag?source=post_page-----5047c50aaeff--------------------------------)[![Andrea
    D''Agostino](../Images/58c7c218815f25278aae59cea44d8771.png)](https://medium.com/@theDrewDag?source=post_page-----5047c50aaeff--------------------------------)[](https://towardsdatascience.com/?source=post_page-----5047c50aaeff--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----5047c50aaeff--------------------------------)
    [Andrea D''Agostino](https://medium.com/@theDrewDag?source=post_page-----5047c50aaeff--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@theDrewDag?source=post_page-----5047c50aaeff--------------------------------)[![安德烈亚·达戈斯提诺](../Images/58c7c218815f25278aae59cea44d8771.png)](https://medium.com/@theDrewDag?source=post_page-----5047c50aaeff--------------------------------)[](https://towardsdatascience.com/?source=post_page-----5047c50aaeff--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----5047c50aaeff--------------------------------)
    [安德烈亚·达戈斯提诺](https://medium.com/@theDrewDag?source=post_page-----5047c50aaeff--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----5047c50aaeff--------------------------------)
    ·8 min read·Apr 25, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----5047c50aaeff--------------------------------)
    ·8分钟阅读·2023年4月25日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/f867bba38db6c28d77fe559a6a5983f2.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f867bba38db6c28d77fe559a6a5983f2.png)'
- en: Photo by [Sigmund](https://unsplash.com/@sigmund?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 照片由 [Sigmund](https://unsplash.com/@sigmund?utm_source=medium&utm_medium=referral)
    提供，来自 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: Machine learning engineers leverage numerical representations of the world to
    build and train predictive algorithms.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习工程师利用世界的数值表示来构建和训练预测算法。
- en: In the context of supervised learning, these representations allow the computer
    to learn the relationship between them and the target variable.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在监督学习的背景下，这些表示方式使计算机能够学习它们与目标变量之间的关系。
- en: Let’s imagine a vector to just be a list of numbers
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 假设一个向量仅仅是一个数字列表。
- en: '[PRE0]'
  id: totrans-11
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: This list is related to the target variable `y`
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 这个列表与目标变量 `y` 相关。
- en: '[PRE1]'
  id: totrans-13
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The machine learning model learns the relationship between the features and
    targets and outputs a prediction — in this case a classification in which one
    of the classes is identified with the number 1.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习模型学习特征与目标之间的关系，并输出预测——在这种情况下，是一种将一个类别标识为数字1的分类。
- en: In this post, **I will write about how vectors can be used to represent complex
    concepts in a number format.**
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，**我将写关于如何使用向量以数值格式表示复杂概念。**
- en: The rationale is that a machine learning model cannot learn from observations
    that are not provided to it in numerical format.
  id: totrans-16
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 其理论基础是机器学习模型无法从以非数值格式提供的观察数据中学习。
- en: Text, images, sounds and other input observations must first be transformed
    into a numerical format suitable for learning.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 文本、图像、声音和其他输入观察数据必须首先转化为适合学习的数值格式。
- en: There are various techniques for transforming a phenomenon into vectors, and
    these depend on the type of data we are working with
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 有多种技术可以将现象转化为向量，这取决于我们处理的数据类型。
- en: We will start by introducing the concept of **One-Hot Encoding**, a technique
    used to represent words as numerical vectors
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们将从介绍**One-Hot 编码**的概念开始，这是一种用于将单词表示为数值向量的技术。
- en: Next, we will discuss the limitations of this technique and introduce the concept
    of **embeddings**, a technique that allows words, images, sounds, and more to
    be represented as smaller numerical vectors than the thousands of categories required
    with One-Hot Encoding
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 接下来，我们将探讨这种技术的局限性，并介绍**嵌入**的概念，这是一种可以将单词、图像、声音等表示为比 One-Hot 编码所需的成千上万个类别更小的数值向量的技术。
- en: We will also mention the **TF-IDF and bag of words models**, which are fundamental
    in text vectorization
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们还将提到**TF-IDF 和词袋模型**，它们在文本向量化中至关重要。
- en: How do we encode a phenomenon into a vector?
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 我们如何将现象编码为向量？
- en: We will use text as an example to carry the conversation forward. The example
    is quite obvious because as we can guess, machine learning models cannot directly
    use text for their learning. We need to turn each character or word into a number
    first.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将以文本为例继续讨论。这个例子非常明显，因为正如我们所猜测的那样，机器学习模型不能直接使用文本进行学习。我们需要首先将每个字符或词汇转换为数字。
- en: Let’s say we want to create a numerical representation of words
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们想创建词汇的数值表示
- en: King
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 国王
- en: Queen
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 皇后
- en: Prince
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 王子
- en: Princess
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 公主
- en: The simplest way to encode these words would be to assign each of them a number,
    sequentially.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 对这些词汇进行编码的最简单方法是依次为每个词汇分配一个数字。
- en: '![](../Images/e3fbf3f3e3dc07a2c6601ceb12ad199c.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e3fbf3f3e3dc07a2c6601ceb12ad199c.png)'
- en: Image by author
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片
- en: The words have been correctly transformed into numerical format, following the
    mapping
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 词汇已正确转换为数值格式，按照映射关系
- en: '[PRE2]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: But there’s a problem. If we fed this data to any predictive model, **it would
    assign a higher mathematical value to the prince and princess, making them more
    important than the king and queen.**
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 但有一个问题。如果我们将这些数据提供给任何预测模型，**它会为王子和公主分配更高的数学值，使它们比国王和皇后更重要。**
- en: Obviously this would provide wrong information to the model, which would learn
    wrong relationships. We need to make our numerical representation more precise.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，这将为模型提供错误的信息，导致模型学习错误的关系。我们需要使我们的数值表示更为精确。
- en: One-Hot Encoding
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: One-Hot Encoding
- en: To solve the numeric representation problem described above, the **One-Hot Encoding**
    technique can be used.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决上述数值表示问题，可以使用**One-Hot Encoding**技术。
- en: In this case, each word would be represented by a numerical vector with a size
    equal to the total number of words to be represented. **The vector would have
    all values equal to zero, except one, which represents the specific word.**
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，每个词将由一个数值向量表示，向量的大小等于需要表示的词总数。**该向量的所有值都为零，只有一个值表示特定的词。**
- en: For example, in the case of the four words “King”, “Queen”, “Prince” and “Princess”,
    each word would be represented by an array of four elements, with the value “1”
    in the position corresponding to the word and “0” in all other positions.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，对于“国王”、“皇后”、“王子”和“公主”这四个词，每个词将由一个包含四个元素的数组表示，其中值“1”在对应于该词的位置，所有其他位置的值为“0”。
- en: This technique solves the problem of assigning a higher mathematical value to
    words that are no more important than others in number representation.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 该技术解决了在数值表示中为词汇分配更高数学值的问题，这些词汇在数量上并不比其他词汇更重要。
- en: '![](../Images/14d80dcb7daf76310e6533078e704018.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/14d80dcb7daf76310e6533078e704018.png)'
- en: Image by author
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片
- en: Now our model has a “balanced” vectorial representation for each word belonging
    to the dataset (which in this case consists of only 4 words).
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们的模型为数据集中每个词（在此案例中仅包含 4 个词）具有“平衡”的向量表示。
- en: But… **what if our vocabulary is made up of thousands or even millions of words**?
    Considering that there are about 270,000 words in the Italian dictionary, applying
    one-hot encoding would be problematic to say the least.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 但是……**如果我们的词汇表由成千上万甚至数百万个词汇组成呢**？考虑到意大利词典中大约有 270,000 个词汇，应用 One-Hot Encoding
    至少会有问题。
- en: 'The computational resources to carry out this coding would be considerable
    and the final representation would be “only” balanced: *there is no information
    about the relationships between the words.*'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 执行这种编码所需的计算资源将是相当可观的，最终的表示将是“仅仅”平衡的：*没有关于词汇之间关系的信息。*
- en: Embeddings
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 嵌入
- en: To overcome the limitations of One-Hot Encoding, the technique called **embedding**
    can be used. This allows words to be represented as numerical vectors of controllable
    size compared to the thousands of categories needed with One-Hot Encoding.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 为了克服 One-Hot Encoding 的局限性，可以使用称为**嵌入**的技术。这允许将词汇表示为可控大小的数值向量，相较于 One-Hot Encoding
    所需的成千上万的类别。
- en: The idea is to create a numerical representation of the words that takes into
    account **the semantic relationships between the words themselves.**
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 其想法是创建一个数值表示词汇的方式，这种表示方式考虑了**词汇之间的语义关系。**
- en: In practice, each word is represented as a vector of real numbers, where each
    dimension represents a different aspect of the meaning of the word.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，每个词汇都表示为一个实数向量，其中每个维度表示词义的不同方面。
- en: 'Understanding embeddings is simple: related words should appear close together
    in vector space, while unrelated words should appear distant.'
  id: totrans-50
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 理解嵌入很简单：相关的单词应该在向量空间中靠得很近，而不相关的单词则应该相距较远。
- en: Let’s try to create a graph where we capture some of the characteristics of
    the words mentioned before.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试创建一个图表，捕捉前面提到的一些单词特征。
- en: '![](../Images/bc64232e79d3eca25fc0f33f84e1d29a.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bc64232e79d3eca25fc0f33f84e1d29a.png)'
- en: Image by author
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图像
- en: We see how close the words *prince* and *princess* are to each other, just like
    *king* and *queen*.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到*王子*和*公主*的词语彼此多么接近，就像*国王*和*女王*一样。
- en: Assuming that the gender variable can assume only two values, M and F (we use
    0 and 1), and that the age variable can assume only three values [Young, Middle-aged,
    Elderly] (we use 0, 1, 2), we see how embeddings can represent these relationships
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 假设性别变量只能取两个值，M和F（我们使用0和1），而年龄变量只能取三个值[年轻、中年、老年]（我们使用0、1、2），我们可以看到嵌入如何表示这些关系。
- en: '![](../Images/1bb9a55ed4d22c41bee362f007bcd25d.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1bb9a55ed4d22c41bee362f007bcd25d.png)'
- en: Image by author
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图像
- en: This representation manages to capture the noble status of an individual by
    using the dimensions of gender and age.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 这种表示通过使用性别和年龄维度，成功捕捉了个体的贵族地位。
- en: 'Moving on the X axis we can observe how the two nobles are equidistant from
    a dimension that represents the gender difference (0: male, 1: female). Moving
    on the Y axis, however, we can observe how age is represented by the distance
    of the embedding from the Y axis.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在X轴上移动，我们可以观察到这两个贵族如何在表示性别差异的维度上等距（0：男性，1：女性）。而在Y轴上移动，我们可以观察到年龄是如何通过嵌入距离Y轴的方式来表示的。
- en: In this way, **word embeddings can be used as input to machine learning models,
    allowing complex concepts to be more accurately represented in a numerical format**.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 这样，**词嵌入可以作为机器学习模型的输入，使得复杂的概念能以数字格式更准确地表示**。
- en: In this example we have only two dimensions. In fact, neural networks are trained
    with the specific task of finding these representations on several dimensions.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中我们只有两个维度。实际上，神经网络是通过特定任务来训练的，以在多个维度上找到这些表示。
- en: To put that into perspective, **models like GPT-3 use more than 12,000 dimensions.**
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地理解，**像GPT-3这样的模型使用了超过12,000维**。
- en: A milestone in the industry
  id: totrans-63
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 行业的一个里程碑
- en: Embeddings can be used not only for words, but also to represent images, sounds,
    and more.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 嵌入表示不仅可以用于单词，还可以用于表示图像、声音等。
- en: The use of vector representations is critical in today’s machine learning. The
    various innovations and technologies in the field of deep learning cascade from
    the concept of vectorization.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 向量表示的使用在当今的机器学习中至关重要。深度学习领域的各种创新和技术源于向量化的概念。
- en: Models like GPT-3.5 are born by crossing vector representations, well-studied
    optimization algorithms and large amounts of computational resources.
  id: totrans-66
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 像GPT-3.5这样的模型通过结合向量表示、经过充分研究的优化算法和大量计算资源而产生。
- en: There is theoretically no limit to this approach.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 理论上，这种方法没有限制。
- en: '**More data → Higher quality vectors → Models that will use those vectors for
    better training.**'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '**更多的数据 → 更高质量的向量 → 使用这些向量的模型进行更好的训练。**'
- en: Limits of embeddings
  id: totrans-69
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 嵌入的局限性
- en: Although embeddings are a very useful technique for representing complex concepts
    in numerical format, they also have limitations.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管嵌入是一种非常有用的技术，用于以数字格式表示复杂概念，但它们也有局限性。
- en: In particular, it is important to underline that the embeddings are built starting
    from the training data, **and therefore can be influenced by any bias present
    in the data.**
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 特别重要的是要强调，嵌入是从训练数据中构建的，**因此可能会受到数据中任何偏见的影响**。
- en: As mentioned, the quality of the embeddings depends on the quality of the training
    data. If the training data is not representative of the domain in which the model
    will be used, the embeddings may not be able to capture all semantic relationships
    between concepts.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，嵌入的质量取决于训练数据的质量。如果训练数据不代表模型将使用的领域，嵌入可能无法捕捉概念之间所有的语义关系。
- en: Also, embeddings can require a lot of memory to store, especially if the number
    of dimensions is large. This can be especially problematic for machine learning
    models that need to run on resource-constrained devices, such as mobile devices.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，嵌入可能需要大量的内存来存储，特别是当维度数量很大时。这对于需要在资源受限的设备上运行的机器学习模型，尤其是移动设备，可能特别有问题。
- en: Other ways of representing text
  id: totrans-74
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 其他文本表示方式
- en: Since text is the most common data format around us (just think of the huge
    amount of textual data on the internet), some text vectorization techniques are
    common and well known.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 由于文本是我们周围最常见的数据格式（只需想到互联网上大量的文本数据），一些文本向量化技术是常见且众所周知的。
- en: One of these is the **TF-IDF transformation** which is a text vectorization
    technique that assigns a weight to each word based on its frequency within a document
    and its overall frequency within the corpus.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 其中之一是**TF-IDF 转换**，这是一种文本向量化技术，根据词在文档中的频率及其在语料库中的总体频率，为每个词分配一个权重。
- en: This way, words that appear frequently within a document but rarely within the
    corpus will have more weight than those that appear frequently everywhere. This
    technique is widely used in the field of Natural Language Processing for text
    analysis.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 这样，文档中出现频率较高但在语料库中出现频率较低的词汇将比那些在各处频繁出现的词汇具有更高的权重。这种技术在自然语言处理领域用于文本分析中被广泛使用。
- en: I invite the interested reader to learn more about the TF-IDF model by reading
    the following article
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 我邀请有兴趣的读者通过阅读以下文章来了解更多关于TF-IDF模型的知识
- en: '[](https://medium.com/mlearning-ai/text-clustering-with-tf-idf-in-python-c94cd26a31e7?source=post_page-----5047c50aaeff--------------------------------)
    [## Text Clustering with TF-IDF in Python'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '[## Text Clustering with TF-IDF in Python](https://medium.com/mlearning-ai/text-clustering-with-tf-idf-in-python-c94cd26a31e7?source=post_page-----5047c50aaeff--------------------------------)'
- en: Explanation of a simple pipeline for text clustering. Full example and code
  id: totrans-80
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 文本聚类的简单管道解释。完整示例和代码
- en: medium.com](https://medium.com/mlearning-ai/text-clustering-with-tf-idf-in-python-c94cd26a31e7?source=post_page-----5047c50aaeff--------------------------------)
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '[medium.com](https://medium.com/mlearning-ai/text-clustering-with-tf-idf-in-python-c94cd26a31e7?source=post_page-----5047c50aaeff--------------------------------)'
- en: TF-IDF is based on the **bag of words model which represents a document as an
    unordered set of words, ignoring sentence structure and word order.**
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: TF-IDF 基于**词袋模型，该模型将文档表示为一个无序的词汇集合，忽略句子结构和词序。**
- en: In this way, the bag of words can be used to represent any document as an array
    of numeric values, where each value represents the frequency of a word within
    the document. Of course, there is no adequate representation of the relationship
    between words, which is provided by embeddings.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这种方式，词袋模型可以用来表示任何文档为一组数值，其中每个数值代表一个词在文档中的频率。当然，这并不能充分表示词汇之间的关系，这一点由词嵌入（embeddings）提供。
- en: Conclusion
  id: totrans-84
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: In this post we have seen how vectors can be used to represent complex concepts
    in a numeric format.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我们已经看到如何使用向量以数字格式表示复杂的概念。
- en: It is important for a data scientist to think in terms of vectorization. Questions
    like
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 对数据科学家来说，以向量化的方式思考是重要的。像这样的疑问
- en: '*how can i convert this stimulus into a number?*'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*我如何将这个刺激转换成一个数字？*'
- en: '*how is this data interpreted by the neural network?*'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*神经网络如何解释这些数据？*'
- en: '*How can I improve this representation?*'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*我如何改善这个表示？*'
- en: are critical, and the team that can adequately answer these questions will create
    better systems.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 这些问题至关重要，能够充分回答这些问题的团队将创造出更好的系统。
- en: Data scientists see the world in terms of vectors.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 数据科学家从向量的角度看待世界。
- en: '**If you want to support my content creation activity, feel free to follow
    my referral link below and join Medium’s membership program**. I will receive
    a portion of your investment and you’ll be able to access Medium’s plethora of
    articles on data science and more in a seamless way.'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '**如果你想支持我的内容创作活动，请随意通过下面的推荐链接加入Medium的会员计划**。我将获得你投资的一部分，你将能够无缝访问Medium上丰富的数据科学及更多领域的文章。'
- en: '[](https://medium.com/@theDrewDag/membership?source=post_page-----5047c50aaeff--------------------------------)
    [## Join Medium with my referral link - Andrea D''Agostino'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '[## Join Medium with my referral link - Andrea D''Agostino](https://medium.com/@theDrewDag/membership?source=post_page-----5047c50aaeff--------------------------------)'
- en: Read every story from Andrea D'Agostino (and thousands of other writers on Medium).
    Your membership fee directly…
  id: totrans-94
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 阅读Andrea D'Agostino（以及Medium上的其他数千位作家的）每个故事。你的会员费直接……
- en: medium.com](https://medium.com/@theDrewDag/membership?source=post_page-----5047c50aaeff--------------------------------)
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '[medium.com](https://medium.com/@theDrewDag/membership?source=post_page-----5047c50aaeff--------------------------------)'
- en: Recommended Reads
  id: totrans-96
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 推荐阅读
- en: For the interested, here are a list of books that I recommended for each ML-related
    topic. There are ESSENTIAL books in my opinion and have greatly impacted my professional
    career.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 对于有兴趣的人，这里是我推荐的每个机器学习相关主题的书单。这些书籍在我看来是**必读**的，并且对我的职业生涯产生了深远的影响。
- en: '*Disclaimer: these are Amazon affiliate links. I will receive a small commission
    from Amazon for referring you these items. Your experience won’t change and you
    won’t be charged more, but it will help me scale my business and produce even
    more content around AI.*'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '*免责声明：这些是亚马逊的附属链接。我将从亚马逊那里获得少量佣金作为推荐费。您的体验不会改变，您不会被收取额外费用，但这将帮助我扩大业务并制作更多有关人工智能的内容。*'
- en: '**Intro to ML:** [*Confident Data Skills: Master the Fundamentals of Working
    with Data and Supercharge Your Career*](https://amzn.to/3ZzKTz6)by Kirill Eremenko'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**机器学习简介：** [*自信的数据技能：掌握数据处理的基本知识，提升你的职业生涯*](https://amzn.to/3ZzKTz6) 由 Kirill
    Eremenko 编著'
- en: '**Sklearn / PyTorch:** [*Machine Learning with PyTorch and Scikit-Learn: Develop
    machine learning and deep learning models with Python*](https://amzn.to/3Gcavve)
    by Sebastian Raschka'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Sklearn / PyTorch：** [*用 PyTorch 和 Scikit-Learn 进行机器学习：使用 Python 开发机器学习和深度学习模型*](https://amzn.to/3Gcavve)
    由 Sebastian Raschka 编著'
- en: '**Sklearn / TensorFlow:** [*Hands-On Machine Learning with Scikit-Learn, Keras,
    and TensorFlow*](https://amzn.to/433F4Nm) by Aurelien Géron'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Sklearn / TensorFlow：** [*动手实践机器学习：使用 Scikit-Learn、Keras 和 TensorFlow*](https://amzn.to/433F4Nm)
    由 Aurelien Géron 编著'
- en: '**NLP:** [*Text as Data: A New Framework for Machine Learning and the Social
    Sciences*](https://amzn.to/3zvH43j)by Justin Grimmer'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**自然语言处理：** [*文本作为数据：机器学习与社会科学的新框架*](https://amzn.to/3zvH43j) 由 Justin Grimmer
    编著'
- en: '**Data Viz:** [*Storytelling with Data: A Data Visualization Guide for Business
    Professionals*](https://amzn.to/3HUtGtB) by Cole Knaflic'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据可视化：** [*用数据讲故事：商务专业人士的数据可视化指南*](https://amzn.to/3HUtGtB) 由 Cole Knaflic
    编著'
- en: Useful Links (written by me)
  id: totrans-104
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 有用的链接（由我编写）
- en: '**Learn how to perform a top-tier Exploratory Data Analysis in Python:** [*Exploratory
    Data Analysis in Python — A Step-by-Step Process*](/exploratory-data-analysis-in-python-a-step-by-step-process-d0dfa6bf94ee)'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**学习如何在 Python 中进行顶级探索性数据分析：** [*Python 中的探索性数据分析 — 分步过程*](/exploratory-data-analysis-in-python-a-step-by-step-process-d0dfa6bf94ee)'
- en: '**Learn the basics of PyTorch:** [*Introduction to PyTorch: from training loop
    to prediction*](/introduction-to-pytorch-from-training-loop-to-prediction-a70372764432)'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**学习 PyTorch 的基础知识：** [*PyTorch 入门：从训练循环到预测*](/introduction-to-pytorch-from-training-loop-to-prediction-a70372764432)'
- en: '**Learn the basics of TensorFlow:** [*Get started with TensorFlow 2.0 — Introduction
    to deep learning*](https://medium.com/towards-data-science/a-comprehensive-introduction-to-tensorflows-sequential-api-and-model-for-deep-learning-c5e31aee49fa)'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**学习 TensorFlow 的基础知识：** [*入门 TensorFlow 2.0 — 深度学习简介*](https://medium.com/towards-data-science/a-comprehensive-introduction-to-tensorflows-sequential-api-and-model-for-deep-learning-c5e31aee49fa)'
- en: '**Perform text clustering with TF-IDF in Python:** [*Text Clustering with TF-IDF
    in Python*](https://medium.com/mlearning-ai/text-clustering-with-tf-idf-in-python-c94cd26a31e7)'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**使用 TF-IDF 在 Python 中进行文本聚类：** [*使用 TF-IDF 在 Python 中进行文本聚类*](https://medium.com/mlearning-ai/text-clustering-with-tf-idf-in-python-c94cd26a31e7)'
