- en: 'Backpropagation: Step-By-Step Derivation'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 反向传播：逐步推导
- en: 原文：[https://towardsdatascience.com/backpropagation-step-by-step-derivation-99ac8fbdcc28](https://towardsdatascience.com/backpropagation-step-by-step-derivation-99ac8fbdcc28)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/backpropagation-step-by-step-derivation-99ac8fbdcc28](https://towardsdatascience.com/backpropagation-step-by-step-derivation-99ac8fbdcc28)
- en: A complete guide to the algorithm used to train neural networks
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 完整的神经网络训练算法指南
- en: '[](https://medium.com/@roiyeho?source=post_page-----99ac8fbdcc28--------------------------------)[![Dr.
    Roi Yehoshua](../Images/905a512ffc8879069403a87dbcbeb4db.png)](https://medium.com/@roiyeho?source=post_page-----99ac8fbdcc28--------------------------------)[](https://towardsdatascience.com/?source=post_page-----99ac8fbdcc28--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----99ac8fbdcc28--------------------------------)
    [Dr. Roi Yehoshua](https://medium.com/@roiyeho?source=post_page-----99ac8fbdcc28--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@roiyeho?source=post_page-----99ac8fbdcc28--------------------------------)[![Roi
    Yehoshua 博士](../Images/905a512ffc8879069403a87dbcbeb4db.png)](https://medium.com/@roiyeho?source=post_page-----99ac8fbdcc28--------------------------------)[](https://towardsdatascience.com/?source=post_page-----99ac8fbdcc28--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----99ac8fbdcc28--------------------------------)
    [Roi Yehoshua 博士](https://medium.com/@roiyeho?source=post_page-----99ac8fbdcc28--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----99ac8fbdcc28--------------------------------)
    ·11 min read·Apr 10, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----99ac8fbdcc28--------------------------------)
    ·阅读时间11分钟·2023年4月10日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/7e828233e39b8fb6f6f3bf155056673e.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7e828233e39b8fb6f6f3bf155056673e.png)'
- en: Photo by [DeepMind](https://unsplash.com/@deepmind?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
    on [Unsplash](https://unsplash.com/photos/8heReYC6Zt0?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：[DeepMind](https://unsplash.com/@deepmind?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
    在 [Unsplash](https://unsplash.com/photos/8heReYC6Zt0?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
- en: In the [previous article](https://medium.com/@roiyeho/multi-layer-perceptrons-8d76972afa2b)
    we talked about multi-layer perceptrons (MLPs) as the first neural network model
    that could solve non-linear and complex problems.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [上一篇文章](https://medium.com/@roiyeho/multi-layer-perceptrons-8d76972afa2b)
    中，我们讨论了多层感知机（MLPs）作为第一个可以解决非线性和复杂问题的神经网络模型。
- en: For a long time it was not clear how to train these networks on a given data
    set. While single-layer perceptrons had a simple learning rule that was guaranteed
    to converge to a solution, it could not be extended to networks with more than
    one layer. The AI community has struggled with this problem for more than 30 years
    (in a period known as the “AI winter”), when eventually in 1986 Rumelhart et al.
    introduced the **backpropagation algorithm** in their groundbreaking paper [1].
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 很长一段时间内，如何在给定数据集上训练这些网络并不明确。虽然单层感知机具有保证收敛的简单学习规则，但这一规则无法扩展到多层网络。人工智能社区在这个问题上挣扎了30多年（在一个被称为“人工智能寒冬”的时期），直到1986年Rumelhart等人首次在其开创性论文中提出了**反向传播算法**。
- en: In this article we will discuss the backpropagation algorithm in detail and
    derive its mathematical formulation step-by-step. Since this is the main algorithm
    used to train neural networks of all kinds (including the deep networks we have
    today), I believe it would be beneficial to anyone working with neural networks
    to know the details of this algorithm.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们将详细讨论反向传播算法，并逐步推导其数学公式。由于这是用于训练各种神经网络（包括我们今天拥有的深度网络）的主要算法，我相信了解该算法的详细信息对任何从事神经网络工作的人都是有益的。
- en: 'Although you can find descriptions of this algorithm in many textbooks and
    online sources, in writing this article I have tried to keep the following principles
    in mind:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管你可以在许多教科书和在线资源中找到关于该算法的描述，但在撰写本文时，我尽力牢记以下原则：
- en: Use clear and consistent notations.
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用清晰且一致的符号表示。
- en: Explain every step of the mathematical derivation.
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 解释数学推导的每一个步骤。
- en: Derive the algorithm for the most general case, i.e., for networks with any
    number of layers and any activation or loss functions.
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 推导最一般情况的算法，即适用于任意层数的网络及任何激活函数或损失函数。
- en: After deriving the backpropagation equations, a complete pseudocode for the
    algorithm is given and then illustrated on a numerical example.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在推导了反向传播方程后，提供了完整的算法伪代码，并通过数值示例进行了说明。
- en: Before reading the article, I recommend that you refresh your calculus knowledge,
    specifically in the area of derivatives (including [partial derivatives](https://en.wikipedia.org/wiki/Partial_derivative)
    and the [chain rule of derivatives](https://en.wikipedia.org/wiki/Chain_rule)).
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在阅读本文之前，我建议你刷新一下微积分知识，特别是在导数领域（包括[偏导数](https://en.wikipedia.org/wiki/Partial_derivative)和[导数链式法则](https://en.wikipedia.org/wiki/Chain_rule)）。
- en: Now grab a cup of coffee and let’s dive in :)
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 现在拿杯咖啡，让我们深入探讨 :)
- en: The Three Phases of the Algorithm
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 算法的三个阶段
- en: 'The backpropagation algorithm consists of three phases:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 反向传播算法包括三个阶段：
- en: '**Forward pass**. In this phase we feed the inputs through the network, make
    a prediction and measure its error with respect to the true label.'
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**向前传播**。在这个阶段，我们通过网络输入数据，做出预测并根据真实标签测量其误差。'
- en: '**Backward pass.** We propagate the gradients of the error with respect to
    each one of the weights backward from the output layer to the input layer.'
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**反向传播**。我们将与每个权重相关的错误梯度从输出层向输入层反向传播。'
- en: '**Gradient descent step**. We slightly tweak the connection weights in the
    network by taking a step in the opposite direction of the error gradients.'
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**梯度下降步骤**。我们通过在错误梯度的反方向上迈出一步来轻微调整网络中的连接权重。'
- en: We will now go over each one of these phases in more detail.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将详细讨论这些阶段。
- en: Forward Pass
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 向前传播
- en: 'In the forward pass, we propagate the inputs in a forward direction, layer-by-layer,
    until the output is generated. The activation of neuron *i* in layer *l* is computed
    using the following equation:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在向前传播中，我们逐层前向传播输入，直到生成输出。层 *l* 中神经元 *i* 的激活通过以下方程计算：
- en: '![](../Images/2e047a8af35d57800a95e155af0189bf.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2e047a8af35d57800a95e155af0189bf.png)'
- en: The forward pass equation
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 向前传播方程
- en: where *f* is the activation function, *zᵢˡ* is the net input of neuron *i* in
    layer *l*, *wᵢⱼˡ* is the connection weight between neuron *j* in layer *l* — 1
    and neuron *i* in layer *l*, and *bᵢˡ* is the bias of neuron *i* in layer *l*.
    For more details on the notations and the derivation of this equation see my [previous
    article](https://medium.com/@roiyeho/multi-layer-perceptrons-8d76972afa2b).
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *f* 是激活函数，*zᵢˡ* 是层 *l* 中神经元 *i* 的净输入，*wᵢⱼˡ* 是层 *l* — 1 中神经元 *j* 和层 *l* 中神经元
    *i* 之间的连接权重，*bᵢˡ* 是层 *l* 中神经元 *i* 的偏差。有关符号和该方程推导的更多细节，请参见我的[上一篇文章](https://medium.com/@roiyeho/multi-layer-perceptrons-8d76972afa2b)。
- en: 'To simplify the derivation of the learning algorithm, we will treat the bias
    as if it were the weight *w*₀ of an input neuron *x*₀ that has a constant value
    of 1\. This enables us to write the above equation as follows:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简化学习算法的推导，我们将偏差视为具有常数值 1 的输入神经元 *x*₀ 的权重 *w*₀。这使我们能够将上述方程写成如下形式：
- en: '![](../Images/8d660784aa002a8d9e09340051323e73.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8d660784aa002a8d9e09340051323e73.png)'
- en: Backward Pass
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 反向传播
- en: In the backward pass we propagate the gradients of the error from the output
    layer back to the input layer.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在反向传播中，我们将错误的梯度从输出层反向传播到输入层。
- en: Definition of the Error and Loss Functions
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 错误和损失函数的定义
- en: We first define the error of the network on the training set with respect to
    its weights. Let’s denote by **w** the vector that contains all the weights of
    the network.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先定义网络在训练集上的错误，关于其权重。假设 **w** 是包含网络所有权重的向量。
- en: 'Assume that we have *n* training samples {(**x***ᵢ, yᵢ*)}, *i* = 1,…,*n*, and
    the output of the network on sample *i* is *oᵢ*. Then the error of the network
    with respect to **w** is:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有 *n* 个训练样本 {(**x***ᵢ, yᵢ*)}，*i* = 1,…,*n*，且网络在样本 *i* 上的输出为 *oᵢ*。那么网络相对于
    **w** 的错误是：
- en: '![](../Images/508f02875ea0f634d6db4a6580d48141.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/508f02875ea0f634d6db4a6580d48141.png)'
- en: The error function
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 错误函数
- en: 'where *J*(*y*, *o*) is the **loss function.** The specific loss function that
    we use depends on the task the network is trying to accomplish:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *J*(*y*, *o*) 是**损失函数**。我们使用的具体损失函数取决于网络要完成的任务：
- en: 'For regression problems, we use the **squared loss** function:'
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于回归问题，我们使用**平方损失**函数：
- en: '![](../Images/0e4f8824ade135f76abe47d4754fffe7.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0e4f8824ade135f76abe47d4754fffe7.png)'
- en: '2\. For binary classification problems, we use **log loss** (also known as
    the **binary cross-entropy loss**):'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 2\. 对于二分类问题，我们使用**对数损失**（也称为**二分类交叉熵损失**）：
- en: '![](../Images/6aeb6075d5e97275dac0b0b8ad1f83d2.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6aeb6075d5e97275dac0b0b8ad1f83d2.png)'
- en: '3\. For multi-class classification problems, we use the **cross-entropy loss**
    function:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 3\. 对于多分类问题，我们使用**交叉熵损失**函数：
- en: '![](../Images/1642774222b5fdf2522fd0b4e9423922.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1642774222b5fdf2522fd0b4e9423922.png)'
- en: where *k* is the number of classes.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *k* 是类别的数量。
- en: The reason why we use these specific loss functions is explained in detail in
    [this article](/loss-functions-in-machine-learning-9977e810ac02).
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用这些特定损失函数的原因在[这篇文章](/loss-functions-in-machine-learning-9977e810ac02)中有详细解释。
- en: 'Our goal is to find the weights **w** that minimize *E*(**w**). Unfortunately,
    this function is non-convex because of the non-linear activations of the hidden
    neurons. This means that it may have multiple local minima:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的目标是找到权重**w**，以使 *E*(**w**) 最小化。不幸的是，由于隐藏神经元的非线性激活，这个函数是非凸的。这意味着它可能有多个局部最小值：
- en: '![](../Images/aa359faad562fcdba72eb2d2f6ae8d8a.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/aa359faad562fcdba72eb2d2f6ae8d8a.png)'
- en: There are various techniques that can be used to prevent gradient descent from
    getting stuck in a local minimum, such as momentum. These techniques will be covered
    in future articles.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 有多种技术可以用来防止梯度下降陷入局部最小值，例如动量。这些技术将在未来的文章中讨论。
- en: Finding the Gradients of the Error
  id: totrans-50
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 寻找误差的梯度
- en: 'In order to use gradient descent, we need to compute the partial derivatives
    of *E*(**w**) with respect to each one of the weights in the network:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使用梯度下降，我们需要计算 *E*(**w**) 对网络中每个权重的偏导数。
- en: '![](../Images/9e4c8524d699d0015e1c58263d0e6abe.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9e4c8524d699d0015e1c58263d0e6abe.png)'
- en: The partial derivative of the error with respect to a given weight
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 误差对给定权重的偏导数
- en: 'To simplify the mathematical derivation, we will assume that we have only one
    training example and find the partial derivatives of the error with respect to
    that example:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简化数学推导，我们将假设只有一个训练示例，并找到该示例对误差的偏导数：
- en: '![](../Images/4c05fc19990455b4a2a3ab40ce562e57.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4c05fc19990455b4a2a3ab40ce562e57.png)'
- en: where *y* is the label of this example and *o* is the output of the network
    for that example. The extension to *n* training samples is straightforward, since
    the derivative of the sum of functions is just the sum of their derivatives.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *y* 是该示例的标签，*o* 是网络对该示例的输出。对 *n* 个训练样本的扩展是直接的，因为函数和的导数只是它们导数的和。
- en: The computation of the partial derivatives of the weights in the hidden layers
    is not trivial, since those weights don’t affect directly the output (and hence
    the error). To address this problem, we will use the [chain rule of derivatives](https://en.wikipedia.org/wiki/Chain_rule)
    to establish a relationship between the gradients of the error in a given layer
    and the gradients in the subsequent layer.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 计算隐藏层权重的偏导数并不是简单的，因为这些权重并不直接影响输出（因此也不直接影响误差）。为了解决这个问题，我们将使用[链式法则](https://en.wikipedia.org/wiki/Chain_rule)来建立给定层误差梯度与后续层梯度之间的关系。
- en: The Delta Terms
  id: totrans-58
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Delta 项
- en: 'We first note that *E* depends on the weight *wᵢⱼˡ* only via the net input
    *zᵢˡ* of neuron *i* in layer *l*. Therefore, we can apply the chain rule of derivatives
    to the gradient of *E* with respect to this weight:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先注意到 *E* 仅通过第 *l* 层神经元 *i* 的净输入 *zᵢˡ* 依赖于权重 *wᵢⱼˡ*。因此，我们可以将链式法则应用于 *E* 对该权重的梯度：
- en: '![](../Images/5b2ebf3062fc48256cf9d5146224dd12.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5b2ebf3062fc48256cf9d5146224dd12.png)'
- en: 'The second derivative on the right side of the equation is:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 方程右侧的二阶导数是：
- en: '![](../Images/6e8e99aca22b26d661e5640ff3dc844d.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6e8e99aca22b26d661e5640ff3dc844d.png)'
- en: 'Therefore, we can write:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们可以写：
- en: '![](../Images/68874ddc5d32f9fca152070c597203d3.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/68874ddc5d32f9fca152070c597203d3.png)'
- en: The variable *δᵢ* is called the **delta term** of neuron *i* or **delta** for
    short.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 变量 *δᵢ* 被称为神经元 *i* 的 **delta 项**，简称 **delta**。
- en: The Delta Rule
  id: totrans-66
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Delta 规则
- en: The **delta rule** establishes the relationship between the delta terms in layer
    *l* and the delta terms in layer *l* + 1.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '**delta 规则**建立了第 *l* 层的 delta 项与第 *l* + 1 层的 delta 项之间的关系。'
- en: 'To derive the delta rule, we again use the chain rule of derivatives. The loss
    function depends on the net input of neuron *i* only via the net inputs of all
    the neurons it is connected to in layer *l* + 1\. Therefore we can write:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 为了推导 delta 规则，我们再次使用链式法则。损失函数仅通过其在第 *l* + 1 层连接的所有神经元的净输入来依赖于神经元 *i* 的净输入。因此，我们可以写：
- en: '![](../Images/421c4f1e5008d314de094bae9af2e17a.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/421c4f1e5008d314de094bae9af2e17a.png)'
- en: where the index *j* in the sum goes over all the neurons in layer *l* + 1 that
    neuron *i* in layer *l* is connected to.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 其中求和中的索引 *j* 遍历第 *l* + 1 层中与第 *l* 层神经元 *i* 连接的所有神经元。
- en: 'Once again we use the chain rule to decompose the second partial derivative
    inside the brackets:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 再次使用链式法则分解括号内的第二个偏导数：
- en: '![](../Images/ebf1c7d004137a813191fd54d96e6961.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ebf1c7d004137a813191fd54d96e6961.png)'
- en: 'The first partial derivative inside the brackets is just the delta of neuron
    *j* in layer *l* + 1, therefore we can write:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 括号内的第一个偏导数就是层*l* + 1中神经元*j*的delta，因此我们可以写：
- en: '![](../Images/22b181fe642ab9ebc72be2739664c2b0.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/22b181fe642ab9ebc72be2739664c2b0.png)'
- en: 'The second partial derivative is easy to compute:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个偏导数很容易计算：
- en: '![](../Images/81f8b1fe1633ba453a4a8a76e8f8424a.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/81f8b1fe1633ba453a4a8a76e8f8424a.png)'
- en: 'Therefore we get:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 因此我们得到：
- en: '![](../Images/d4d3213dd5842cc476c077fb52e5a192.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d4d3213dd5842cc476c077fb52e5a192.png)'
- en: But *aᵢˡ* = *f*(*zᵢˡ*), where *f* is the activation function. Hence, the partial
    derivative outside the sum is just the derivative of the activation function *f*’(*x*)
    for *x* = *zᵢˡ*.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 但*aᵢˡ* = *f*(*zᵢˡ*)，其中*f*是激活函数。因此，和外的偏导数只是激活函数*f*’(*x*)在*x* = *zᵢˡ*处的导数。
- en: 'Therefore we can write:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 因此我们可以写：
- en: '![](../Images/07713282e31ac44c9ffb0e23b5d9ee93.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/07713282e31ac44c9ffb0e23b5d9ee93.png)'
- en: The delta rule
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: delta规则
- en: This equation, known as the **delta rule**,shows the relationship between the
    deltas in layer *l* and the deltas in layer *l* + 1\. More specifically, each
    delta in layer *l* is a linear combination of the deltas in layer *l* + 1, where
    the coefficients of the combination are the connection weights between these layers.
    The delta rule allows us to compute all the delta terms (and thus all the gradients
    of the error) recursively, starting from the deltas in the output layer and going
    back layer-by-layer until we reach the input layer.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 这个方程，被称为**delta规则**，显示了层*l*中的delta与层*l* + 1中的delta之间的关系。更具体地说，层*l*中的每个delta都是层*l*
    + 1中delta的线性组合，其中组合的系数是这些层之间的连接权重。delta规则允许我们递归地计算所有delta项（从而所有误差的梯度），从输出层中的delta开始，一层一层地向回计算，直到达到输入层。
- en: 'The following diagram illustrates the flow of the error information:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 下图说明了误差信息的流动：
- en: '![](../Images/a9e6b7b137ce99e203f2bed43ec8ac46.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a9e6b7b137ce99e203f2bed43ec8ac46.png)'
- en: The calculation of the delta of neuron *i* in layer *l* by backpropagation of
    the deltas from those neurons in layer *l*+1 to which it is connected. The black
    arrows indicate the direction of flow during forward propagation, and the red
    arrows indicate the backward propagation of the error.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 通过从层*l*+1中与神经元*i*连接的神经元向后传播的delta来计算层*l*中神经元*i*的delta。黑色箭头表示前向传播的方向，红色箭头表示误差的反向传播。
- en: 'For specific activation functions, we can derive more explicit equations for
    the delta rule. For example, if we use the sigmoid function then:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 对于特定的激活函数，我们可以推导出delta规则的更明确的方程。例如，如果我们使用sigmoid函数，则：
- en: '![](../Images/8b893ba60884b9ad140623c200ea35fb.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8b893ba60884b9ad140623c200ea35fb.png)'
- en: 'The derivative of the sigmoid function has a simple form:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: sigmoid函数的导数具有简单的形式：
- en: '![](../Images/a5d3aa3f91f489200ba6d0f4f967e6f4.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a5d3aa3f91f489200ba6d0f4f967e6f4.png)'
- en: 'Hence:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 因此：
- en: '![](../Images/aead0691125e71e5da4843d16e29c10e.png)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/aead0691125e71e5da4843d16e29c10e.png)'
- en: 'Then the delta rule for the sigmoid function gets the following form:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 那么sigmoid函数的delta规则变为以下形式：
- en: '![](../Images/cdfe6a79624599e9c7bce25decf19d8c.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/cdfe6a79624599e9c7bce25decf19d8c.png)'
- en: The delta rule for the sigmoid function
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 对于sigmoid函数的delta规则
- en: The Deltas in the Output Layer
  id: totrans-96
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 输出层中的Deltas
- en: The final piece of the puzzle are the delta terms in the output layer, which
    are the first ones that we need to compute.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 拼图的最终部分是输出层中的delta项，这些是我们需要计算的第一个项。
- en: 'The deltas in the output layer depend both on the loss function and the activation
    function used in the output neurons:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 输出层中的deltas依赖于输出神经元中使用的损失函数和激活函数：
- en: '![](../Images/778baeef0a413fde2d9b5241c73a89e1.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/778baeef0a413fde2d9b5241c73a89e1.png)'
- en: where *f* is the activation function used to compute the output.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 其中*f*是用于计算输出的激活函数。
- en: 'Let’s now derive more specific delta terms for each type of learning task:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们推导每种学习任务的更具体的delta项：
- en: 'In regression problems, the activation function we use in the output is the
    identity function *f*(*x*) = *x*, whose derivative is 1, and the loss function
    is the squared loss. Therefore the delta is:'
  id: totrans-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在回归问题中，我们在输出中使用的激活函数是恒等函数*f*(*x*) = *x*，其导数为1，损失函数是平方损失。因此，delta为：
- en: '![](../Images/0094fabb2ef5c498e15edab58a3e227d.png)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0094fabb2ef5c498e15edab58a3e227d.png)'
- en: '2\. In binary classification problems, the activation function we use is sigmoid
    and the loss function is log loss, therefore we get:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 2\. 在二分类问题中，我们使用的激活函数是 sigmoid，损失函数是对数损失，因此我们得到：
- en: '![](../Images/b6c2385c98268f5167973370f65c25b0.png)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b6c2385c98268f5167973370f65c25b0.png)'
- en: In other words, the delta is simply the difference between the network’s output
    and the label.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，delta 只是网络输出和标签之间的差值。
- en: '3\. In multiclass classification problems, we have *k* output neurons (where
    *k* is the number of classes) and we use softmax activation and the cross-entropy
    log loss. Similar to the previous case, the delta term of the *i*th output neuron
    is surprisingly simple:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 3\. 在多分类问题中，我们有 *k* 个输出神经元（其中 *k* 是类别的数量），我们使用 softmax 激活函数和交叉熵对数损失。与前面的情况类似，第
    *i* 个输出神经元的 delta 项非常简单：
- en: '![](../Images/2444f2275ef23dc9691796e68adba5e7.png)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2444f2275ef23dc9691796e68adba5e7.png)'
- en: where *oᵢ* is the *i*-th component of the network’s prediction and *yᵢ* is the
    *i*-th component of the label. The proof is somewhat longer, and you can find
    it in [this article](https://medium.com/towards-data-science/deep-dive-into-softmax-regression-62deea103cb8).
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *oᵢ* 是网络预测的第 *i* 个组件，*yᵢ* 是标签的第 *i* 个组件。证明过程略长，你可以在 [这篇文章](https://medium.com/towards-data-science/deep-dive-into-softmax-regression-62deea103cb8)
    中找到它。
- en: Gradient Descent
  id: totrans-110
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 梯度下降
- en: 'Once we finish computing all the delta terms, we can use gradient descent to
    update the weights. In gradient descent, we take small steps in the opposite direction
    of the gradient (i.e., in the direction of the steepest descent) in order to get
    closer to the minimum error:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们完成了所有 delta 项的计算，我们可以使用梯度下降来更新权重。在梯度下降中，我们沿梯度的相反方向（即沿着最陡下降的方向）采取小步骤，以接近最小误差：
- en: '![](../Images/4136bc6fed46169ab4b42672f6dd2f1e.png)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4136bc6fed46169ab4b42672f6dd2f1e.png)'
- en: Gradient descent
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度下降
- en: 'Remember that the partial derivative of the error function with respect to
    each weight is:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，误差函数关于每个权重的偏导数是：
- en: '![](../Images/0bfefb241373b5f35b9224adc6443737.png)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0bfefb241373b5f35b9224adc6443737.png)'
- en: 'Therefore, we can write the gradient descent update rule as follows:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们可以将梯度下降的更新规则写成如下：
- en: '![](../Images/cc43d67db8ae41c2bff5c1ab29fb4d29.png)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/cc43d67db8ae41c2bff5c1ab29fb4d29.png)'
- en: The gradient descent update rule
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度下降更新规则
- en: where *α* is a learning rate that controls the step size (0 < *α* < 1). In other
    words, we subtract from the weight between neuron *j* in layer *l* — 1 and neuron
    *i* in layer *l* the delta of neuron *i* multiplied by the activation of neuron
    *j* (scaled by the learning rate).
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *α* 是控制步长的学习率（0 < *α* < 1）。换句话说，我们从第 *l* 层的神经元 *j* 到第 *l* 层的神经元 *i* 的权重中减去神经元
    *i* 的 delta 乘以神经元 *j* 的激活（按学习率缩放）。
- en: 'Gradient descent can be applied in one of the following modes:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度下降可以应用于以下模式之一：
- en: '**Batch gradient descent** — the weights are updated after we compute the error
    on the entire training set.'
  id: totrans-121
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**批量梯度下降** — 在计算完整个训练集上的误差后更新权重。'
- en: '**Stochastic gradient descent (SGD)** — a gradient descent step is performed
    after every training example. Typically converges faster than batch gradient descent
    but is less stable.'
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**随机梯度下降（SGD）** — 在每个训练样本之后执行一次梯度下降步骤。通常比批量梯度下降收敛速度更快，但稳定性较差。'
- en: '**Mini-batch gradient descent** — a middle way between batch gradient descent
    and SGD. We use small batches of random training samples (normally between 10
    to 1,000 examples) for the gradient updates. This reduces the noise in SGD but
    is still more efficient than full-batch updates, and it is the most common form
    to train neural networks.'
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**小批量梯度下降** — 在批量梯度下降和 SGD 之间的一种折中方法。我们使用小批量的随机训练样本（通常在 10 到 1,000 个示例之间）来进行梯度更新。这减少了
    SGD 中的噪声，但仍然比全批次更新更高效，并且是训练神经网络的最常见形式。'
- en: 'Backpropagation: The Complete Algorithm'
  id: totrans-124
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 反向传播：完整算法
- en: 'We are now ready to present the entire algorithm in its full glory:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在准备以完整的形式展示整个算法：
- en: '![](../Images/3204b2cb18d638732a189438bc17be67.png)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3204b2cb18d638732a189438bc17be67.png)'
- en: As an exercise, try to implement this algorithm in Python (or your favorite
    programming language).
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 作为练习，尝试在 Python（或你喜欢的编程语言）中实现这个算法。
- en: Backpropagation Example
  id: totrans-128
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 反向传播示例
- en: 'Imagine that we have a binary classification problem with two binary inputs
    and a single binary output. Our neural network has two hidden layers with the
    following weights:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下，我们有一个二分类问题，具有两个二进制输入和一个二进制输出。我们的神经网络有两个隐藏层，权重如下：
- en: '![](../Images/81bd7b7f409c34c0265ce16269b3abe1.png)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/81bd7b7f409c34c0265ce16269b3abe1.png)'
- en: The activation function in the hidden layers and in the output unit is the sigmoid
    function, and the learning rate is *α* = 0.5*.*
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 隐藏层和输出单元的激活函数是 sigmoid 函数，学习率是 *α* = 0.5*。
- en: The network is presented with a training example with the inputs *x*₁ = 1 and
    *x*₂ = 0, and the target label is *y* = 1\. Let’s perform one iteration of the
    backpropagation algorithm to update the weights.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 网络呈现一个训练示例，输入是 *x*₁ = 1 和 *x*₂ = 0，目标标签是 *y* = 1。让我们执行一次反向传播算法的迭代以更新权重。
- en: 'We start with forward propagation of the inputs:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从输入的前向传播开始：
- en: '![](../Images/bb8913aebebd7590cac08e28fd9c0c1f.png)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bb8913aebebd7590cac08e28fd9c0c1f.png)'
- en: The forward pass
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 前向传播
- en: The output of the network is 0.6718 while the true label is 1, hence we need
    to update the weights in order to increase the network’s output and make it closer
    to the label.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 网络的输出是 0.6718，而真实标签是 1，因此我们需要更新权重，以提高网络的输出，使其更接近标签。
- en: We first compute the delta at the output node. Since this is a binary classification
    problem we use the log loss function, and the delta at the output is *o* — *y*.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先计算输出节点的 delta。由于这是一个二分类问题，我们使用对数损失函数，输出的 delta 是 *o* — *y*。
- en: '![](../Images/761010e4ff07308e08093f7ffb1b798e.png)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/761010e4ff07308e08093f7ffb1b798e.png)'
- en: The delta at the output neuron
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 输出神经元的 delta
- en: 'We now propagate the deltas from the output neuron back to the input layer
    using the delta rule:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在使用 delta 规则将 deltas 从输出神经元传播回输入层：
- en: '![](../Images/5110588ec02922b78970946605f32e93.png)'
  id: totrans-141
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5110588ec02922b78970946605f32e93.png)'
- en: The backward pass
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 反向传播
- en: Note how the deltas become increasingly smaller as we go back in the layers,
    causing the early layers in the network to train very slowly. This phenomenon,
    known as the **vanishing gradients**, was one of the main reasons why backpropagation
    was not successful in training deep networks and a main motivation for the emergence
    of deep learning.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，当我们向后回溯到各层时，deltas 变得越来越小，导致网络中的早期层训练非常缓慢。这种现象被称为 **梯度消失**，是反向传播在训练深层网络时未能成功的主要原因之一，也是深度学习兴起的主要动因。
- en: 'Finally, we perform one step of gradient descent:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们执行一次梯度下降步骤：
- en: '![](../Images/c8d0580a985d8dc5a4baed318a9a97ca.png)'
  id: totrans-145
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c8d0580a985d8dc5a4baed318a9a97ca.png)'
- en: Gradient descent step
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度下降步骤
- en: 'Let’s do another forward pass to see if the network’s output became closer
    to the target:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们再做一次前向传播，看看网络的输出是否更接近目标：
- en: '![](../Images/973181d89d535b17a94eeeeaa99f1c2d.png)'
  id: totrans-148
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/973181d89d535b17a94eeeeaa99f1c2d.png)'
- en: Another forward pass
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个前向传播
- en: Indeed, the output has increased from 0.6718 to 0.6981!
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 确实，输出已从 0.6718 增加到 0.6981！
- en: '**Final Notes**'
  id: totrans-151
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**最终说明**'
- en: All images unless otherwise noted are by the author.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 除非另有说明，所有图像均由作者提供。
- en: Thanks for reading!
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 感谢阅读！
- en: References
  id: totrans-154
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] Rumelhart, David E., Geoffrey E. Hinton, and Ronald J. Williams. “Learning
    representations by back-propagating errors.” *nature* 323.6088 (1986): 533–536.'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] Rumelhart, David E.，Geoffrey E. Hinton 和 Ronald J. Williams. “通过反向传播误差学习表示。”
    *自然* 323.6088 (1986): 533–536。'
