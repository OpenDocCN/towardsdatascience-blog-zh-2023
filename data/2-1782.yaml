- en: Reshaping the Model’s Memory without the Need for Retraining
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在无需重新训练的情况下重塑模型的记忆
- en: 原文：[https://towardsdatascience.com/reshaping-the-models-memory-without-the-need-for-retraining-9ade69f56296](https://towardsdatascience.com/reshaping-the-models-memory-without-the-need-for-retraining-9ade69f56296)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/reshaping-the-models-memory-without-the-need-for-retraining-9ade69f56296](https://towardsdatascience.com/reshaping-the-models-memory-without-the-need-for-retraining-9ade69f56296)
- en: '| AI | LARGE LANGUAGE MODELS| MACHINE UNLEARNING|'
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '| 人工智能 | 大型语言模型| 机器遗忘|'
- en: Erasing any echo of problematic content a large language model has learned
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 擦除大型语言模型学到的有问题内容的任何痕迹
- en: '[](https://salvatore-raieli.medium.com/?source=post_page-----9ade69f56296--------------------------------)[![Salvatore
    Raieli](../Images/6bb4520e2df40d20283e7283141b5e06.png)](https://salvatore-raieli.medium.com/?source=post_page-----9ade69f56296--------------------------------)[](https://towardsdatascience.com/?source=post_page-----9ade69f56296--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----9ade69f56296--------------------------------)
    [Salvatore Raieli](https://salvatore-raieli.medium.com/?source=post_page-----9ade69f56296--------------------------------)'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://salvatore-raieli.medium.com/?source=post_page-----9ade69f56296--------------------------------)[![Salvatore
    Raieli](../Images/6bb4520e2df40d20283e7283141b5e06.png)](https://salvatore-raieli.medium.com/?source=post_page-----9ade69f56296--------------------------------)[](https://towardsdatascience.com/?source=post_page-----9ade69f56296--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----9ade69f56296--------------------------------)
    [Salvatore Raieli](https://salvatore-raieli.medium.com/?source=post_page-----9ade69f56296--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----9ade69f56296--------------------------------)
    ·11 min read·Oct 20, 2023
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----9ade69f56296--------------------------------)
    ·11分钟阅读·2023年10月20日
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/4d98c180635de8d1ec4607a94fdfa029.png)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4d98c180635de8d1ec4607a94fdfa029.png)'
- en: Photo by [Drew Saurus](https://unsplash.com/@drew_saurus?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由 [Drew Saurus](https://unsplash.com/@drew_saurus?utm_source=medium&utm_medium=referral)
    提供，来源于 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: “To forgive is wisdom, to forget is genius. ”
  id: totrans-9
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “原谅是智慧，遗忘是天才。”
- en: ― **Joyce Cary**
  id: totrans-10
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ― **乔伊斯·凯里**
- en: '[Large language models](https://en.wikipedia.org/wiki/Large_language_model)
    (LLMs) have taken the world by storm. In less than a year they are ubiquitous
    and are now used by millions of users. These models are often trained with huge
    amounts of text (including problematic material and sensitive data). How do you
    make a model forget? The same that could store the entirety of human knowledge?'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '[大型语言模型](https://en.wikipedia.org/wiki/Large_language_model)（LLMs）席卷了世界。在不到一年的时间里，它们已经无处不在，并且现在被数百万用户使用。这些模型通常以大量文本（包括有问题的材料和敏感数据）进行训练。你如何让一个模型忘记？同样能够存储整个人类知识的模型？'
- en: To learn how to forget
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 学会如何遗忘
- en: '![](../Images/c7c7cf08a9381a67f4ce29c3f641e677.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c7c7cf08a9381a67f4ce29c3f641e677.png)'
- en: Photo by [Paul Pastourmatzis](https://unsplash.com/@pueblovista?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由 [Paul Pastourmatzis](https://unsplash.com/@pueblovista?utm_source=medium&utm_medium=referral)
    提供，来源于 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: LLMs stand as a testament to both our accomplishments and the challenges that
    lie ahead — [source](https://arxiv.org/pdf/2310.02238.pdf)
  id: totrans-15
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: LLMs 既是我们成就的见证，也是我们面临挑战的象征 — [来源](https://arxiv.org/pdf/2310.02238.pdf)
- en: LLMs have surprised both users and researchers with their ability to learn from
    huge amounts of text and identify language patterns and cultural nuances. While
    they could be the basis for a new application and scientific revolution, they
    have a dark side.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: LLMs 以其从大量文本中学习、识别语言模式和文化细微差别的能力令用户和研究人员感到惊讶。虽然它们可能成为新应用和科学革命的基础，但它们也有阴暗的一面。
- en: 'Huge [corpora](https://it.wikipedia.org/wiki/Corpus) must be used to train
    these patterns. While it is true that the greater the amount of data used the
    better the performance of an LLM, collecting this data is expensive. To limit
    costs, indiscriminate scraping of data from the Internet is often used. These
    corpora therefore also contain [extremely problematic data](https://hiddenlayer.com/research/the-dark-side-of-large-language-models/):
    copyrighted texts, toxic or malicious data, inaccurate or fake content, personal
    data, and more.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 训练这些模式必须使用大量的[语料库](https://it.wikipedia.org/wiki/Corpus)。虽然使用的数据量越大，LLM的性能越好，但收集这些数据是昂贵的。为了限制成本，通常会对互联网进行无差别的数据抓取。因此，这些语料库也包含[极其有问题的数据](https://hiddenlayer.com/research/the-dark-side-of-large-language-models/)：版权文本、有毒或恶意的数据、不准确或虚假的内容、个人数据等。
- en: '![](../Images/b63f4540244b687eff11ef74d8cfc459.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b63f4540244b687eff11ef74d8cfc459.png)'
- en: 'image source: [here](https://arxiv.org/pdf/2307.03941.pdf)'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：[这里](https://arxiv.org/pdf/2307.03941.pdf)
- en: '[](/machine-unlearning-the-duty-of-forgetting-3666e5b9f6e5?source=post_page-----9ade69f56296--------------------------------)
    [## Machine unlearning: The duty of forgetting'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/machine-unlearning-the-duty-of-forgetting-3666e5b9f6e5?source=post_page-----9ade69f56296--------------------------------)
    [## 机器遗忘：遗忘的责任'
- en: How and why it is important to erase data point information from an AI model
  id: totrans-21
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 如何以及为什么从AI模型中删除数据点信息是重要的
- en: towardsdatascience.com](/machine-unlearning-the-duty-of-forgetting-3666e5b9f6e5?source=post_page-----9ade69f56296--------------------------------)
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/machine-unlearning-the-duty-of-forgetting-3666e5b9f6e5?source=post_page-----9ade69f56296--------------------------------)
- en: LLMs have the ability to store all this information and [to leak them once they
    are queried](https://arxiv.org/abs/2307.10476). This opens up enormous ethical
    and even legal risks. In addition, this has led to lawsuits, public pressure,
    and the focus of legislative discussions.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: LLM能够存储所有这些信息，并且[在被查询时泄露这些信息](https://arxiv.org/abs/2307.10476)。这带来了巨大的伦理甚至法律风险。此外，这还导致了诉讼、公众压力和立法讨论的重点。
- en: To date, through [fine-tuning](https://en.wikipedia.org/wiki/Fine-tuning_(deep_learning)),
    we know that we can reinforce the specific knowledge of a model. However, if we
    wanted a model to forget specific information, we would have to retrain the model.
    The problem is that training an [LLM costs millions of dollars](https://www.forbes.com/sites/craigsmith/2023/09/08/what-large-models-cost-you--there-is-no-free-ai-lunch/)
    and is time-intensive.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 迄今为止，通过[微调](https://en.wikipedia.org/wiki/Fine-tuning_(deep_learning))，我们知道可以强化模型的特定知识。然而，如果我们希望模型忘记特定的信息，就必须重新训练模型。问题在于训练一个[LLM需要数百万美元](https://www.forbes.com/sites/craigsmith/2023/09/08/what-large-models-cost-you--there-is-no-free-ai-lunch/)且时间密集。
- en: '**How do you get an LLM to forget?**'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '**如何让LLM忘记？**'
- en: In general, [machine unlearning](https://arxiv.org/abs/1912.03817) is an active
    field of research. Most studies focus on classification tasks and only a few studies
    are on generative AI or LLMs. [LLMs are particularly problematic](https://arxiv.org/abs/2307.03941)
    because it is difficult to understand from where personal data (chat history or
    training data) were acquired and in what parameters they are stored. Removing
    data from a trained model is extremely complex, as model weights are a complex
    integration of the whole collection of [training data](https://en.wikipedia.org/wiki/Training,_validation,_and_test_data_sets).
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '[机器遗忘](https://arxiv.org/abs/1912.03817)通常是一个活跃的研究领域。大多数研究集中在分类任务上，只有少数研究涉及生成AI或LLMs。[LLMs特别有问题](https://arxiv.org/abs/2307.03941)，因为很难理解个人数据（聊天记录或训练数据）从何处获取，以及以何种参数存储。删除训练模型中的数据极为复杂，因为模型权重是整个[训练数据](https://en.wikipedia.org/wiki/Training,_validation,_and_test_data_sets)集合的复杂整合。'
- en: '[An interesting approach that has recently been proposed](https://arxiv.org/abs/2210.01504)
    is that we fine-tune the model with the text we want to forget. In this case,
    we negate the [loss function](https://en.wikipedia.org/wiki/Loss_function), in
    other words, we penalize the model when it predicts as the next word in the text
    what we want to forget.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '[一种最近提出的有趣方法](https://arxiv.org/abs/2210.01504)是通过我们想要忘记的文本来微调模型。在这种情况下，我们否定[损失函数](https://en.wikipedia.org/wiki/Loss_function)，换句话说，我们在模型预测文本中我们想忘记的下一个词时对其进行惩罚。'
- en: '![](../Images/a95a62e4153b7ca7be31ba53c4bd67d7.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a95a62e4153b7ca7be31ba53c4bd67d7.png)'
- en: 'image source: [here](https://arxiv.org/pdf/2210.01504.pdf)'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：[这里](https://arxiv.org/pdf/2210.01504.pdf)
- en: '**As simple and effective as this approach seems, it actually has limitations.**
    For example, if the text we want to forget is my bio: “*My name is Salvatore…*”
    the model will forget not only “*Salvatore*” but also “*my name is.*” In other
    words, this model forgets general knowledge about language.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '**虽然这种方法看起来简单而有效，但实际上存在一定的局限性。** 例如，如果我们想忘记的文本是我的个人简介：“*我的名字是萨尔瓦托雷…*”，模型不仅会忘记“*萨尔瓦托雷*”，还会忘记“*我的名字是。*”
    换句话说，这个模型会忘记关于语言的一般知识。'
- en: Thus, we are interested in looking for an approach that instead of penalizing
    some text, shifts the model from predicting personal data to giving a generic
    answer (as if it had never encountered) personal data.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们感兴趣的是寻找一种方法，而不是惩罚某些文本，而是将模型从预测个人数据转变为给出通用答案（就像它从未遇到过）个人数据。
- en: So we want a model that is able to effectively forget about the problem text,
    but at the same time retain its skills and the rest of its knowledge.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们希望有一个模型能够有效地忘记问题文本，同时保留其技能和其他知识。
- en: How to forget Harry Potter
  id: totrans-33
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何忘记哈利·波特
- en: '![](../Images/5023274902f825b5d9c14ca8a3d8e695.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5023274902f825b5d9c14ca8a3d8e695.png)'
- en: Photo by [Dollar Gill](https://unsplash.com/@dollargill?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '[照片来源于 Dollar Gill](https://unsplash.com/@dollargill?utm_source=medium&utm_medium=referral)
    在 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)'
- en: '**“It does not do to dwell on dreams and forget to live” — Albus Dumbledore
    in the** *Sorcerer’s Stone*'
  id: totrans-36
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**“沉迷于梦想而忘记生活是不行的”—— 阿不思·邓布利多在** *《魔法石》*'
- en: Recently an article dealt with how you can make a model forget an entire book
    without impacting LLM performance. The authors show how a model can forget the
    complex plot of [Harry Potter](https://en.wikipedia.org/wiki/Harry_Potter) and
    at the same time manage to maintain performance in benchmark datasets.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 最近有一篇文章讨论了如何让模型忘记整本书而不影响 LLM 的性能。作者展示了模型如何忘记 [哈利·波特](https://en.wikipedia.org/wiki/Harry_Potter)
    的复杂情节，同时能够在基准数据集上保持性能。
- en: '[](https://arxiv.org/abs/2310.02238?source=post_page-----9ade69f56296--------------------------------)
    [## Who''s Harry Potter? Approximate Unlearning in LLMs'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://arxiv.org/abs/2310.02238?source=post_page-----9ade69f56296--------------------------------)
    [## 谁是哈利·波特？LLMs中的近似遗忘'
- en: Large language models (LLMs) are trained on massive internet corpora that often
    contain copyrighted content. This poses…
  id: totrans-39
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）是在包含大量互联网语料库的基础上进行训练的，这些语料库通常包含受版权保护的内容。这会带来…
- en: arxiv.org](https://arxiv.org/abs/2310.02238?source=post_page-----9ade69f56296--------------------------------)
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '[arxiv.org](https://arxiv.org/abs/2310.02238?source=post_page-----9ade69f56296--------------------------------)'
- en: We can consider that an LLM is trained on text dataset X and we want it to forget
    text subset Y. Through fine tuning we can obtain a model that has enhanced knowledge
    of Y. This model will be an expert on subject Y. The traditional method would
    be to retrain the LLM on X-Y but this would require a lot of time and computational
    resources.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以认为一个大型语言模型（LLM）是基于文本数据集 X 进行训练的，而我们希望它忘记文本子集 Y。通过微调，我们可以获得一个在 Y 方面具有增强知识的模型。这个模型将成为
    Y 主题的专家。传统的方法是重新训练 LLM 于 X-Y，但这需要大量的时间和计算资源。
- en: We want a model that preserves its general knowledge and understanding of the
    language. Therefore, the authors decided to exploit the expert model to help an
    LLM to forget.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望有一个模型可以保留其一般知识和语言理解。因此，作者决定利用专家模型来帮助 LLM 遗忘。
- en: The first step is to understand what a generic prediction would be. For the
    authors of this paper, a generic prediction for a sentence such as “*He looks
    at the scar on his__*” is the difference between an expert model (which has a
    thorough understanding of what we want to forget) and the baseline model.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步是了解什么是通用预测。对本文的作者来说，对于句子如“*他看着他__上的伤疤*”，通用预测是具有全面理解我们想要忘记的内容的专家模型和基准模型之间的差异。
- en: 'In simple words, the authors took an LLM ([LLaMA-7B](https://ai.meta.com/blog/large-language-model-llama-meta-ai/))
    as a baseline and fine-tuned it to Harry Potter (expert model). After that, a
    prompt is given to the two models (“*He looks at the scar on his__*”), and a vector
    v of predictions (logit) is obtained for each, the generic prediction is:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，作者以 LLM（[LLaMA-7B](https://ai.meta.com/blog/large-language-model-llama-meta-ai/)）作为基准，并对其进行微调以适应哈利·波特（专家模型）。之后，给两个模型提供一个提示（“*他看着他__上的伤疤*”），并为每个模型获得一个预测向量
    v（logit），通用预测是：
- en: '[PRE0]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Using [ReLU](https://en.wikipedia.org/wiki/Rectifier_(neural_networks)) and
    a constant α allows us to extract only the predictions specific to the expert
    model. This is to prevent the model from forgetting “*He looks at the scar on
    his*” but only “*forehead*” (i.e., where Harry Potter has his scar).
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 [ReLU](https://en.wikipedia.org/wiki/Rectifier_(neural_networks)) 和常数α可以让我们仅提取专家模型特定的预测。这是为了防止模型忘记“*他看着额头上的伤疤*”而只记住“*额头*”（即哈利·波特的伤疤所在位置）。
- en: Is it enough?
  id: totrans-47
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 这足够吗？
- en: No, because forgetting a book is not just forgetting the name of a protagonist
    or a specific term (also because by varying the prompts one could still access
    this knowledge). The idea is that our model forgets in a deeper way. For authors,
    this can be achieved by destroying links between entities in the text.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 不，因为忘记一本书不仅仅是忘记一个主角的名字或特定术语（也因为通过改变提示仍然可以访问这些知识）。我们的模型旨在以更深层次的方式遗忘。对于作者来说，这可以通过破坏文本中实体之间的链接来实现。
- en: 'For this reason, the authors extracted the various entities in the book with
    [GPT-4](https://en.wikipedia.org/wiki/GPT-4) and translated them with names or
    entities that are idiosyncratic to the text. These are terms that are consistent
    but not specific to the book, as you can see from the example:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，作者使用 [GPT-4](https://en.wikipedia.org/wiki/GPT-4) 提取了书中的各种实体，并用文本中具有特性的名称或实体进行翻译。这些术语是一致的，但并非特定于该书，正如示例所示：
- en: '![](../Images/0eb755e98a512c0ad10f11b6c09cb4c1.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0eb755e98a512c0ad10f11b6c09cb4c1.png)'
- en: 'image source: [here](https://arxiv.org/pdf/2310.02238.pdf)'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：[这里](https://arxiv.org/pdf/2310.02238.pdf)
- en: This serves to steer the model away conceptually from predicting Harry Potter-related
    content toward more general texts that are at the same time consistent with the
    textual input.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 这有助于将模型在概念上从预测《哈利·波特》相关内容转向与文本输入一致的更一般性文本。
- en: 'Combining these two elements together, the process is divided into four steps:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 将这两个元素结合起来，该过程分为四个步骤：
- en: We need to create a dictionary where we map specific elements of the text to
    generic translations.
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们需要创建一个字典，将文本中的特定元素映射到通用翻译。
- en: We get blocks of text (depending on the [context length](https://www.linkedin.com/pulse/expanding-context-lengths-llms-towards-causalgpt-vs-bard-butvinik/)
    of the chosen LLM). We do the block mapping with our dictionary we get the prediction
    with the expert model for the original text and the model baseline prediction
    for the mapped text.
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们得到文本块（取决于所选择的 [上下文长度](https://www.linkedin.com/pulse/expanding-context-lengths-llms-towards-causalgpt-vs-bard-butvinik/)）。我们使用字典对块进行映射，得到原始文本的专家模型预测和映射文本的模型基线预测。
- en: We combine the predictions of these two models with the equation described above
    (eq .1) and thus obtain the generic predictions.
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们用上述方程（方程1）结合这两个模型的预测，从而获得通用预测。
- en: In the last step we conduct fine-tuning of the baseline model using the original
    test as input and the generic labels as target tokens.
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在最后一步，我们使用原始测试作为输入和通用标签作为目标标记，对基线模型进行微调。
- en: '![](../Images/c30df9ffab685b871f614590e2d73b69.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c30df9ffab685b871f614590e2d73b69.png)'
- en: 'image source: [here](https://arxiv.org/pdf/2310.02238.pdf)'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：[这里](https://arxiv.org/pdf/2310.02238.pdf)
- en: Did our model forget about magic?
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 我们的模型是否忘记了魔法？
- en: '![](../Images/c66142d8e57dc9420cf65370acb3e602.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c66142d8e57dc9420cf65370acb3e602.png)'
- en: Photo by [Artem Maltsev](https://unsplash.com/@art_maltsev?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 照片由 [Artem Maltsev](https://unsplash.com/@art_maltsev?utm_source=medium&utm_medium=referral)
    拍摄，来源于 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: “The trick to forgetting the big picture is to look at everything close-up.”
    — Chuck Palahniuk
  id: totrans-63
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “忘记大局的秘诀是仔细观察一切。” — 查克·帕拉尼克
- en: The authors chose LLaMA-2 in the 7B version as the model, both because it was
    open-source and because it showed excellent capabilities despite its limited size.
    The training of the original model (pretraining phase with a huge corpus of text)
    required 184K GPU hours, while the forgetting process proposed by the authors
    requires only 1 GPU hour (thus definitely inexpensive in terms of resources and
    affordable for anyone).
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 作者选择了7B版本的LLaMA-2作为模型，因为它是开源的，并且尽管规模有限，但表现出色。原始模型的训练（在大量文本语料库上进行预训练）需要184K GPU小时，而作者提出的遗忘过程只需1
    GPU小时（因此在资源上确实便宜，对任何人都能负担得起）。
- en: '[](https://levelup.gitconnected.com/meta-llama-2-0-the-most-disruptive-ainimal-d465ef187f2?source=post_page-----9ade69f56296--------------------------------)
    [## META LLaMA 2.0: the most disruptive AInimal'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://levelup.gitconnected.com/meta-llama-2-0-the-most-disruptive-ainimal-d465ef187f2?source=post_page-----9ade69f56296--------------------------------)
    [## META LLaMA 2.0: 最具破坏性的AInimal'
- en: Meta LLaMA can reshape the chatbot and LLM usage landscape
  id: totrans-66
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Meta LLaMA可以重塑聊天机器人和LLM的使用格局
- en: levelup.gitconnected.com](https://levelup.gitconnected.com/meta-llama-2-0-the-most-disruptive-ainimal-d465ef187f2?source=post_page-----9ade69f56296--------------------------------)
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '[levelup.gitconnected.com](https://levelup.gitconnected.com/meta-llama-2-0-the-most-disruptive-ainimal-d465ef187f2?source=post_page-----9ade69f56296--------------------------------)'
- en: 'The first step is to assess whether the model actually retained information
    about the Harry Potter book (e.g., “*When Harry returned to class, he observed
    his best friends__*”). To be sure of this, the authors created a series of [textual
    prompts](https://en.wikipedia.org/wiki/Prompt_engineering) that the model had
    to complete based on its internal knowledge. In addition, they created prompts
    to check whether the model was familiar with what was described in the books (e.g.,
    “*Draft a brief narrative in the style of Harry Potter. Short story:*”). As can
    be seen, the model that went through the forgetting process seems to no longer
    be able to recall elements from the book:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步是评估模型是否确实保留了关于哈利·波特书籍的信息（例如，“*当哈利回到课堂时，他观察到他最好的朋友__*”）。为了确保这一点，作者创建了一系列[文本提示](https://en.wikipedia.org/wiki/Prompt_engineering)，模型必须根据其内部知识完成这些提示。此外，他们还创建了提示以检查模型是否熟悉书中描述的内容（例如，“*以哈利·波特的风格编写一个简短的故事。短篇故事：*”）。如所见，经过遗忘过程的模型似乎不再能够回忆起书中的元素：
- en: '![](../Images/8866a1bf08d6d33850a11304d9cf20de.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8866a1bf08d6d33850a11304d9cf20de.png)'
- en: 'image source: [here](https://arxiv.org/pdf/2310.02238.pdf)'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：[这里](https://arxiv.org/pdf/2310.02238.pdf)
- en: The authors manually evaluated not only how the model had completed the sentences
    but also the probabilities associated with a given [token](https://en.wikipedia.org/wiki/Lexical_analysis#Token).
    For example, considering the sentence “*Harry Potter studies__”* the authors saw
    whether the words “*magic*” or “*wizardry*” were among the highest probability
    tokens.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 作者们不仅手动评估了模型完成句子的方式，还评估了与给定[token](https://en.wikipedia.org/wiki/Lexical_analysis#Token)相关的概率。例如，在考虑句子“*哈利·波特研究__*”时，作者观察到“*魔法*”或“*巫术*”是否在概率最高的token中。
- en: The results show that the probability of the next token decreases significantly
    with each fine-tuning step. The lower the probability of a token the less likely
    it will be selected, even by changing prompts. According to the authors, only
    120 [gradient descent](https://en.wikipedia.org/wiki/Gradient_descent) fine-tuning
    steps are needed for optimal results.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 结果表明，每次微调步骤后，下一个token的概率显著下降。token的概率越低，即使更改提示，它被选择的可能性也越小。根据作者的说法，仅需要120次[梯度下降](https://en.wikipedia.org/wiki/Gradient_descent)微调步骤即可获得最佳结果。
- en: '![](../Images/f6fffe8ff9c61a322787f303eb6a1896.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f6fffe8ff9c61a322787f303eb6a1896.png)'
- en: 'image source: [here](https://arxiv.org/pdf/2310.02238.pdf)'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：[这里](https://arxiv.org/pdf/2310.02238.pdf)
- en: 'The model seems to have forgotten the book and provided generic answers. The
    question remains: **has the forgetting process impacted the model’s general skills
    and knowledge?**'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 模型似乎忘记了书籍内容，给出了通用的答案。问题仍然存在：**遗忘过程是否影响了模型的整体技能和知识？**
- en: 'For this, the authors used three benchmark datasets:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 为此，作者使用了三个基准数据集：
- en: '[**WinoGrande**](https://arxiv.org/abs/1907.10641)is a benchmark for commonsense
    reasoning (273 expert-crafted resolution problems).'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**WinoGrande**](https://arxiv.org/abs/1907.10641)是一个用于常识推理的基准（273个专家精心设计的解决问题）。'
- en: '[**HellaSwag**](https://arxiv.org/abs/1905.07830) is a dataset of sentences
    to complete that are trivial to humans but not to computers.'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**HellaSwag**](https://arxiv.org/abs/1905.07830)是一个需要完成的句子数据集，这些句子对人类来说很简单，但对计算机却不然。'
- en: '[**PIQA**](https://arxiv.org/abs/1911.11641), a dataset for commonsense reasoning
    created to investigate the physical knowledge of existing LLMs.'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**PIQA**](https://arxiv.org/abs/1911.11641)是一个用于常识推理的数据集，旨在调查现有LLM的物理知识。'
- en: '[**BoolQ**](https://arxiv.org/abs/1905.10044) is a large question-answering
    dataset (yes/no) where the model is provided a question, and the context, and
    has to provide an answer.'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**BoolQ**](https://arxiv.org/abs/1905.10044)是一个大型问答数据集（是/否），模型需要根据提供的问题和上下文给出答案。'
- en: '[**OpenBookQA**](https://arxiv.org/abs/1809.02789), a question-answering dataset
    modeled after open book exams for assessing human understanding of a subject.'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**OpenBookQA**](https://arxiv.org/abs/1809.02789)，一个模拟开放书籍考试的问答数据集，用于评估人类对学科的理解。'
- en: '[**ARC**](https://arxiv.org/abs/1803.05457), multiple-choice question-answering
    dataset containing questions from science exams.'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**ARC**](https://arxiv.org/abs/1803.05457)，一个包含科学考试问题的多项选择问答数据集。'
- en: '![](../Images/7782eae7dfd36ac960f4663d2c36bf69.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7782eae7dfd36ac960f4663d2c36bf69.png)'
- en: example of questions in the datasets. adapted from the original articles ([here](https://arxiv.org/abs/1905.10044),
    [here](https://arxiv.org/abs/1809.02789), and [here](https://arxiv.org/abs/1905.07830))
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集中问题的示例。改编自原始文章（[here](https://arxiv.org/abs/1905.10044)，[here](https://arxiv.org/abs/1809.02789)，和
    [here](https://arxiv.org/abs/1905.07830)）
- en: The results show that show that performance is minimally impacted by the unlearning
    process. Obviously, a greater number of [gradient steps](https://en.wikipedia.org/wiki/Gradient_descent)
    decreases familiarity with the topic but also impacts performance more.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 结果显示，性能受解学习过程的影响最小。显然，更多的 [gradient steps](https://en.wikipedia.org/wiki/Gradient_descent)
    减少了对主题的熟悉度，但也更大程度地影响了性能。
- en: '![](../Images/715e169c5137ecb5a8293c67986f64c4.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/715e169c5137ecb5a8293c67986f64c4.png)'
- en: 'image source: [here](https://arxiv.org/pdf/2310.02238.pdf)'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：[here](https://arxiv.org/pdf/2310.02238.pdf)
- en: 'However, this study has limitations:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这项研究存在一些局限性：
- en: There are occasional leaks (if you ask the model for the names of magic schools
    it suggests Hogwarts). Since the authors used the books as text (but there are
    also movies and theme parks dedicated to the world of Harry Potter) this could
    simply mean Wikipedia-level knowledge rather than actual leaks.
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 存在偶尔的泄漏（如果你询问模型关于魔法学校的名称，它会建议霍格沃茨）。由于作者使用了书籍作为文本（但也有关于哈利·波特世界的电影和主题公园），这可能仅意味着维基百科级别的知识，而非实际泄漏。
- en: Second, more sophisticated prompting techniques could lead the model to reveal
    information. It should therefore be tested with adversarial attacks or other prompting
    techniques.
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 其次，更复杂的提示技术可能会导致模型揭示信息。因此，应该使用对抗攻击或其他提示技术进行测试。
- en: The method uses [GPT-4](https://openai.com/research/gpt-4) and thus its knowledge
    of Harry Potter, but in other cases, this is not possible.
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 该方法使用了 [GPT-4](https://openai.com/research/gpt-4)，因此它对哈利·波特的知识也涵盖在内，但在其他情况下，这是不可能的。
- en: The Harry Potter books have a universe rich in characters, peculiar expressions,
    and precise themes. While the method seems to work well with a fictional topic,
    other topics do not have such rich lexical content or are much more abstruse.
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 哈利·波特书籍的宇宙充满了丰富的角色、独特的表达和精准的主题。虽然这种方法在虚构主题上似乎效果很好，但其他主题并没有如此丰富的词汇内容，或更为晦涩。
- en: 'The authors aware of the limitations invite the community to try and test the
    model:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 作者意识到这些局限性，邀请社区尝试并测试模型：
- en: Recognizing the intrinsic limitations of automated benchmarks and internal evaluations,
    we believe that unlearning verification parallels endeavors like jailbreaking
    in adversarial nature. Therefore, we open-sourced the model, encouraging the broader
    community to challenge it, providing a more diverse and extensive set of tests
    to discern if any remnants of the targeted knowledge persist. (source)
  id: totrans-94
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 认识到自动化基准和内部评估的内在局限性，我们认为，解学习验证类似于对抗性质的破解工作。因此，我们开源了模型，鼓励更广泛的社区挑战它，提供更多样化和广泛的测试，以辨别是否存在任何目标知识的残余。（来源）
- en: 'The model is stored on HuggingFace and is available here:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型存储在 HuggingFace 上，并在此处提供：
- en: '[## microsoft/Llama2-7b-WhoIsHarryPotter · Hugging Face'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '[## microsoft/Llama2-7b-WhoIsHarryPotter · Hugging Face'
- en: We're on a journey to advance and democratize artificial intelligence through
    open source and open science.
  id: totrans-97
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 我们正在通过开源和开放科学推动和普及人工智能的进步。
- en: huggingface.co](https://huggingface.co/microsoft/Llama2-7b-WhoIsHarryPotter?source=post_page-----9ade69f56296--------------------------------)
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: huggingface.co](https://huggingface.co/microsoft/Llama2-7b-WhoIsHarryPotter?source=post_page-----9ade69f56296--------------------------------)
- en: Parting thoughts
  id: totrans-99
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结束语
- en: '![](../Images/95710cc7ff409c95285f071c4b038114.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/95710cc7ff409c95285f071c4b038114.png)'
- en: Photo by [Saif71.com](https://unsplash.com/@saif71?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 照片来自 [Saif71.com](https://unsplash.com/@saif71?utm_source=medium&utm_medium=referral)
    在 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: “The advantage of a bad memory is that one enjoys several times the same good
    things for the first time.”
  id: totrans-102
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “糟糕记忆的优点在于，人们可以多次第一次享受同样的美好事物。”
- en: ― **Friedrich Nietzsche**
  id: totrans-103
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ― **弗里德里希·尼采**
- en: Forgetting something intentionally is a difficult challenge even for humans.
    This is also difficult for LLMs. As the study of grokking showed, there is a difference
    between memorizing and learning.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 有意忘记某些东西对人类来说是一个困难的挑战。对于 LLM（大语言模型）也是如此。正如对深度理解的研究所示，记忆和学习之间存在差异。
- en: '[](https://levelup.gitconnected.com/grokking-learning-is-generalization-and-not-memorization-52c43c9025e4?source=post_page-----9ade69f56296--------------------------------)
    [## Grokking: Learning Is Generalization and Not Memorization'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://levelup.gitconnected.com/grokking-learning-is-generalization-and-not-memorization-52c43c9025e4?source=post_page-----9ade69f56296--------------------------------)
    [## 深入理解：学习是泛化而非记忆'
- en: Understanding how a neural network learns helps us to avoid that the model from
    forgetting what it learns
  id: totrans-106
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 理解神经网络如何学习有助于我们避免模型忘记它所学到的知识。
- en: levelup.gitconnected.com](https://levelup.gitconnected.com/grokking-learning-is-generalization-and-not-memorization-52c43c9025e4?source=post_page-----9ade69f56296--------------------------------)
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '[levelup.gitconnected.com](https://levelup.gitconnected.com/grokking-learning-is-generalization-and-not-memorization-52c43c9025e4?source=post_page-----9ade69f56296--------------------------------)'
- en: Initial studies tried to make the model forget by eliminating what it memorized.
    This impacted his general knowledge and his understanding of language itself.
    This new study shows how it is not enough to focus on the key terms of a concept
    (for example, the main characters in Harry Potter) but also on the concept itself
    (the plot for example).
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 初步研究试图通过消除模型记住的内容来使其忘记。这影响了模型的常识和对语言本身的理解。这项新的研究表明，仅仅关注一个概念的关键术语（例如《哈利·波特》的主要角色）还不够，还需要关注概念本身（例如情节）。
- en: The authors show how the model loses familiarity with Harry Potter and at the
    same time maintains its performance in reasoning benchmarks. Although this method
    is not perfect and has only been tested on a limited case, it opens up some very
    interesting perspectives. Indeed, pretraining datasets are full of toxic comments,
    stereotypes, biases, and hateful speech. This is the first step in being able
    to allow a model to unlearn this content without re-training.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 作者展示了模型如何在保持推理基准性能的同时失去对《哈利·波特》的熟悉程度。虽然这种方法并不完美，只在有限的案例上进行过测试，但它开辟了一些非常有趣的前景。实际上，预训练的数据集充满了有害评论、刻板印象、偏见和仇恨言论。这是使模型能够在不重新训练的情况下忘记这些内容的第一步。
- en: What do think? Let me know in the comments
  id: totrans-110
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 你觉得怎么样？在评论中告诉我。
- en: 'If you have found this interesting:'
  id: totrans-111
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如果你觉得这有趣：
- en: '*You can look for my other articles, you can also* [***subscribe***](https://salvatore-raieli.medium.com/subscribe)
    *to get notified when I publish articles, you can* [***become a Medium member***](https://medium.com/@salvatore-raieli/membership)
    *to access all its stories (affiliate links of the platform for which I get small
    revenues without cost to you) and you can also connect or reach me on*[***LinkedIn***](https://www.linkedin.com/in/salvatore-raieli/)***.***'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '*你可以查看我的其他文章，你也可以* [***订阅***](https://salvatore-raieli.medium.com/subscribe)
    *以在我发布文章时收到通知，你可以* [***成为 Medium 会员***](https://medium.com/@salvatore-raieli/membership)
    *来访问所有的故事（这是平台的联盟链接，我从中获得少量收入，对你没有费用），你也可以在*[***LinkedIn***](https://www.linkedin.com/in/salvatore-raieli/)***上联系我。***'
- en: '*Here is the link to my GitHub repository, where I am planning to collect code
    and many resources related to machine learning, artificial intelligence, and more.*'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '*这是我 GitHub 仓库的链接，我计划在其中收集与机器学习、人工智能等相关的代码和许多资源。*'
- en: '[](https://github.com/SalvatoreRa/tutorial?source=post_page-----9ade69f56296--------------------------------)
    [## GitHub - SalvatoreRa/tutorial: Tutorials on machine learning, artificial intelligence,
    data science…'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://github.com/SalvatoreRa/tutorial?source=post_page-----9ade69f56296--------------------------------)
    [## GitHub - SalvatoreRa/tutorial: 机器学习、人工智能、数据科学教程…'
- en: Tutorials on machine learning, artificial intelligence, data science with math
    explanation and reusable code (in python…
  id: totrans-115
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 机器学习、人工智能、数据科学的教程，带有数学解释和可重复使用的代码（用 Python 编写…）
- en: github.com](https://github.com/SalvatoreRa/tutorial?source=post_page-----9ade69f56296--------------------------------)
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '[github.com](https://github.com/SalvatoreRa/tutorial?source=post_page-----9ade69f56296--------------------------------)'
- en: '*or you may be interested in one of my recent articles:*'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '*或者你可能对我最近的一篇文章感兴趣：*'
- en: '[](https://levelup.gitconnected.com/scaling-data-scaling-bias-a-deep-dive-into-hateful-content-and-racial-bias-in-generative-ai-70d8aa27a631?source=post_page-----9ade69f56296--------------------------------)
    [## Scaling Data, Scaling Bias: A Deep Dive into Hateful Content and Racial Bias
    in Generative AI'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://levelup.gitconnected.com/scaling-data-scaling-bias-a-deep-dive-into-hateful-content-and-racial-bias-in-generative-ai-70d8aa27a631?source=post_page-----9ade69f56296--------------------------------)
    [## 数据扩展，偏见扩展：深入探讨生成式人工智能中的仇恨内容和种族偏见'
- en: 'scaling seems the solution for every issue in machine learning: but it is true?'
  id: totrans-119
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 扩展似乎是机器学习中每个问题的解决方案：但这真的正确吗？
- en: 'levelup.gitconnected.com](https://levelup.gitconnected.com/scaling-data-scaling-bias-a-deep-dive-into-hateful-content-and-racial-bias-in-generative-ai-70d8aa27a631?source=post_page-----9ade69f56296--------------------------------)
    [](https://levelup.gitconnected.com/tabula-rasa-why-do-tree-based-algorithms-outperform-neural-networks-db641862859b?source=post_page-----9ade69f56296--------------------------------)
    [## Tabula Rasa: Why Do Tree-Based Algorithms Outperform Neural Networks'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: levelup.gitconnected.com](https://levelup.gitconnected.com/scaling-data-scaling-bias-a-deep-dive-into-hateful-content-and-racial-bias-in-generative-ai-70d8aa27a631?source=post_page-----9ade69f56296--------------------------------)
    [](https://levelup.gitconnected.com/tabula-rasa-why-do-tree-based-algorithms-outperform-neural-networks-db641862859b?source=post_page-----9ade69f56296--------------------------------)
    [## Tabula Rasa：为什么树基算法优于神经网络
- en: 'Tree-based algorithms are the winner in tabular data: Why?'
  id: totrans-121
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 树基算法在表格数据中是赢家：为什么？
- en: levelup.gitconnected.com](https://levelup.gitconnected.com/tabula-rasa-why-do-tree-based-algorithms-outperform-neural-networks-db641862859b?source=post_page-----9ade69f56296--------------------------------)
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: levelup.gitconnected.com](https://levelup.gitconnected.com/tabula-rasa-why-do-tree-based-algorithms-outperform-neural-networks-db641862859b?source=post_page-----9ade69f56296--------------------------------)
