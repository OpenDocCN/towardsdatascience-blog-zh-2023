- en: Building a Streaming Data Pipeline with Redshift Serverless and Kinesis
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Redshift Serverless和Kinesis构建流数据管道
- en: 原文：[https://towardsdatascience.com/building-a-streaming-data-pipeline-with-redshift-serverless-and-kinesis-04e09d7e85b2](https://towardsdatascience.com/building-a-streaming-data-pipeline-with-redshift-serverless-and-kinesis-04e09d7e85b2)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/building-a-streaming-data-pipeline-with-redshift-serverless-and-kinesis-04e09d7e85b2](https://towardsdatascience.com/building-a-streaming-data-pipeline-with-redshift-serverless-and-kinesis-04e09d7e85b2)
- en: An End-To-End Tutorial for Beginners
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 面向初学者的完整教程
- en: '[](https://mshakhomirov.medium.com/?source=post_page-----04e09d7e85b2--------------------------------)[![💡Mike
    Shakhomirov](../Images/bc6895c7face3244d488feb97ba0f68e.png)](https://mshakhomirov.medium.com/?source=post_page-----04e09d7e85b2--------------------------------)[](https://towardsdatascience.com/?source=post_page-----04e09d7e85b2--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----04e09d7e85b2--------------------------------)
    [💡Mike Shakhomirov](https://mshakhomirov.medium.com/?source=post_page-----04e09d7e85b2--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://mshakhomirov.medium.com/?source=post_page-----04e09d7e85b2--------------------------------)[![💡Mike
    Shakhomirov](../Images/bc6895c7face3244d488feb97ba0f68e.png)](https://mshakhomirov.medium.com/?source=post_page-----04e09d7e85b2--------------------------------)[](https://towardsdatascience.com/?source=post_page-----04e09d7e85b2--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----04e09d7e85b2--------------------------------)
    [💡Mike Shakhomirov](https://mshakhomirov.medium.com/?source=post_page-----04e09d7e85b2--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----04e09d7e85b2--------------------------------)
    ·9 min read·Oct 6, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page-----04e09d7e85b2--------------------------------)
    ·阅读时间9分钟·2023年10月6日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/d1700c0485714244a17aec09305461e6.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d1700c0485714244a17aec09305461e6.png)'
- en: Photo by [Sebastian Pandelache](https://unsplash.com/@pandelache?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由[Sebastian Pandelache](https://unsplash.com/@pandelache?utm_source=medium&utm_medium=referral)拍摄，来源于[Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: In this article, I will talk about one of the most popular data pipeline design
    patterns — event streaming. Among other benefits, it enables lightning-fast data
    analytics and we can create reporting dashboards that update results in real-time.
    I will demonstrate how it can be achieved by building a streaming data pipeline
    with AWS Kinesis and Redshift which can be deployed with just a few clicks using
    infrastructure as code. We will use AWS CloudFormation to describe our data platform
    architecture and simplify deployment.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我将讨论最受欢迎的数据管道设计模式之一——事件流。除了其他好处，它还支持超快的数据分析，我们可以创建实时更新结果的报告仪表盘。我将演示如何通过构建一个使用AWS
    Kinesis和Redshift的流数据管道来实现这一点，并且可以通过几次点击使用基础设施即代码进行部署。我们将使用AWS CloudFormation来描述我们的数据平台架构并简化部署过程。
- en: Imagine that as a data engineer, you are tasked to create a data pipeline that
    connects server event streams with a data warehouse solution (Redshift) to transform
    the data and create an analytics dashboard.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下，作为数据工程师，你的任务是创建一个将服务器事件流与数据仓库解决方案（Redshift）连接起来的数据管道，以便转换数据并创建分析仪表盘。
- en: '![](../Images/19d9e86d0773049b03bcd3c4ea66b9ab.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/19d9e86d0773049b03bcd3c4ea66b9ab.png)'
- en: Pipeline Infrastructure. Image by author.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 管道基础设施。图片来源：作者。
- en: What is a data pipeline?
  id: totrans-12
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 什么是数据管道？
- en: It is a sequence of data processing steps. Due to ***logical data flow connections***
    between these stages, each stage generates an **output** that serves as an **input**
    for the following stage.
  id: totrans-13
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 它是一个数据处理步骤的序列。由于这些阶段之间的***逻辑数据流连接***，每个阶段生成一个**输出**，作为下一个阶段的**输入**。
- en: 'I previously wrote about it in this article:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 我之前在这篇文章中写过相关内容：
- en: '[](/data-pipeline-design-patterns-100afa4b93e3?source=post_page-----04e09d7e85b2--------------------------------)
    [## Data pipeline design patterns'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/data-pipeline-design-patterns-100afa4b93e3?source=post_page-----04e09d7e85b2--------------------------------)
    [## 数据管道设计模式'
- en: Choosing the right architecture with examples
  id: totrans-16
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 选择合适的架构及示例
- en: towardsdatascience.com](/data-pipeline-design-patterns-100afa4b93e3?source=post_page-----04e09d7e85b2--------------------------------)
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/data-pipeline-design-patterns-100afa4b93e3?source=post_page-----04e09d7e85b2--------------------------------)
- en: For example, event data can be created by a source at the back end, an event
    stream built with Kinesis Firehose or Kafka stream. It can then feed a number
    of various consumers or destinations.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，事件数据可以由后端的源创建，使用 Kinesis Firehose 或 Kafka 流构建事件流。然后它可以馈送到多个不同的消费者或目的地。
- en: Streaming is a “must-have” solution for enterprise data due to its streaming
    data processing capabilities. It enables real-time data analytics.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 流式处理是企业数据的“必备”解决方案，因其流数据处理能力。它能够实现实时数据分析。
- en: In our use-case scenario we can set up an **ELT streaming** data pipeline to
    AWS Redshift. AWS Firehose stream can offer this type of seamless integration
    when streaming data will be uploaded directly into the data warehouse table. Then
    data can be transformed to create reports with AWS Quicksight as a BI tool for
    example.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的用例场景中，我们可以设置一个**ELT 流式**数据管道到 AWS Redshift。AWS Firehose 流可以提供这种无缝集成，当数据流被直接上传到数据仓库表时。然后，数据可以被转换以使用
    AWS Quicksight 作为 BI 工具来创建报告，例如。
- en: '![](../Images/90a8cb4b073924f3113fce5df13ab20c.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/90a8cb4b073924f3113fce5df13ab20c.png)'
- en: Added BI component. Image by author.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 添加了 BI 组件。图片来源于作者。
- en: This tutorial assumes that learners are familiar with AWS CLI and have minimal
    Python knowledge.
  id: totrans-23
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 本教程假设学习者熟悉 AWS CLI 并且具有基本的 Python 知识。
- en: Workflow
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 工作流程
- en: 1\. Firstly, we will create Kinesis data stream using AWS CloudFormation
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 1\. 首先，我们将使用 AWS CloudFormation 创建 Kinesis 数据流。
- en: 2\. We will send sample data events to this event stream using AWS Lambda.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 2\. 我们将使用 AWS Lambda 向此事件流发送示例数据事件。
- en: 3\. Finally, we will provision AWS Redshift cluster and test our streaming pipeline.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 3\. 最后，我们将配置 AWS Redshift 集群并测试我们的流式管道。
- en: Create an AWS Kinesis Data Stream
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建 AWS Kinesis 数据流
- en: AWS Kinesis Data Streams is an Amazon Kinesis real-time data streaming solution.
    It offers great scalability and durability where data streams are available for
    any consumer.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: AWS Kinesis Data Streams 是一个 Amazon Kinesis 实时数据流解决方案。它提供了出色的可扩展性和耐用性，数据流可以被任何消费者访问。
- en: 'We can create it with CloudFormation template. The commandline script below
    will trigger AWS CLI command to deploy it:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用 CloudFormation 模板来创建它。下面的命令行脚本将触发 AWS CLI 命令进行部署：
- en: '[PRE0]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'And the template kinesis-data-stream.yaml will look as follows:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 并且模板 kinesis-data-stream.yaml 将如下所示：
- en: '[PRE1]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Very simple. If everything goes well we will see our Kinesis stream being deployed:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 非常简单。如果一切顺利，我们将看到我们的 Kinesis 流被部署：
- en: '![](../Images/0f33f2db4f4b45e501787735db96cb76.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0f33f2db4f4b45e501787735db96cb76.png)'
- en: Stream created. Image by author.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 流已创建。图片来源于作者。
- en: 2\. Create AWS Lambda function to simulate an event stream
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2\. 创建 AWS Lambda 函数以模拟事件流
- en: Now we would want to send some events to our Kinesis Data Stream. For this,
    we can create a serverless application, such as AWS Lambda. We will use `boto3`
    library (The AWS SDK for Python) to build a data connector with AWS Kinesis at
    source.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们希望将一些事件发送到我们的 Kinesis 数据流。为此，我们可以创建一个无服务器应用程序，例如 AWS Lambda。我们将使用`boto3`库（AWS
    的 Python SDK）来构建一个数据连接器与 AWS Kinesis 进行数据源连接。
- en: '![](../Images/611621ecd4f8ba97807a73dcbfff5e82.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/611621ecd4f8ba97807a73dcbfff5e82.png)'
- en: Running app locally to simulate event stream. Image by author.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 本地运行应用以模拟事件流。图片来源于作者。
- en: 'Our application folder structure can look like this:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的应用程序文件夹结构可以如下所示：
- en: '[PRE2]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Our `app.py` must be able to send events to Kinesis Data Stream:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的`app.py`必须能够向 Kinesis 数据流发送事件：
- en: '[PRE3]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'We would like to add a helper function to generate some random event data.
    For instance:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望添加一个帮助函数来生成一些随机事件数据。例如：
- en: '[PRE4]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'We can use `python-lambda-local` library to run and test AWS Lambda locally
    like so:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用`python-lambda-local`库本地运行和测试 AWS Lambda，方法如下：
- en: '[PRE5]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '`env.json` is just an event payload to run Lambda locally.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '`env.json` 只是一个事件负载，用于本地运行 Lambda。'
- en: '`config/staging.yaml` can contain any environment specific setting our application
    might require in future. For example:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '`config/staging.yaml` 可以包含我们应用程序未来可能需要的任何环境特定设置。例如：'
- en: '[PRE6]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'If you need to use `requirements.txt` it can look like this:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你需要使用`requirements.txt`，它可以如下所示：
- en: '[PRE7]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Run this in your command line:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在你的命令行中运行这个：
- en: '[PRE8]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'This approach is useful because we might want to deploy our serverless application
    in the cloud and schedule it. We can use CloudFormation template for this. I prevoiusly
    wrote about it here:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法很有用，因为我们可能希望将无服务器应用程序部署到云中并进行调度。我们可以使用 CloudFormation 模板来实现这一点。我之前在这里写过：
- en: '[](https://levelup.gitconnected.com/infrastructure-as-code-for-beginners-a4e36c805316?source=post_page-----04e09d7e85b2--------------------------------)
    [## Infrastructure as Code for Beginners'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '[## Infrastructure as Code for Beginners](https://levelup.gitconnected.com/infrastructure-as-code-for-beginners-a4e36c805316?source=post_page-----04e09d7e85b2--------------------------------)'
- en: Deploy Data Pipelines like a pro with these templates
  id: totrans-58
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用这些模板像专业人士一样部署数据管道
- en: levelup.gitconnected.com](https://levelup.gitconnected.com/infrastructure-as-code-for-beginners-a4e36c805316?source=post_page-----04e09d7e85b2--------------------------------)
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '[levelup.gitconnected.com](https://levelup.gitconnected.com/infrastructure-as-code-for-beginners-a4e36c805316?source=post_page-----04e09d7e85b2--------------------------------)'
- en: 'When we use a CloudFormation template the application can be deployed with
    a shell script like so:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们使用 CloudFormation 模板时，应用程序可以通过类似的 shell 脚本进行部署：
- en: '[PRE9]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: It is a flexible setup allowing use to create robust CI/CD pipelines. I remember
    I created one in this post below.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个灵活的设置，允许我们创建强大的 CI/CD 管道。我记得我在下面的帖子中创建了一个。
- en: '[](/continuous-integration-and-deployment-for-data-platforms-817bf1b6bed1?source=post_page-----04e09d7e85b2--------------------------------)
    [## Continuous Integration and Deployment for Data Platforms'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '[## Continuous Integration and Deployment for Data Platforms'
- en: CI/CD for data engineers and ML Ops
  id: totrans-64
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据工程师和 ML Ops 的 CI/CD
- en: towardsdatascience.com](/continuous-integration-and-deployment-for-data-platforms-817bf1b6bed1?source=post_page-----04e09d7e85b2--------------------------------)
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '[towardsdatascience.com](/continuous-integration-and-deployment-for-data-platforms-817bf1b6bed1?source=post_page-----04e09d7e85b2--------------------------------)'
- en: Create Redshift Serverless resources
  id: totrans-66
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建 Redshift Serverless 资源
- en: Now we need to create Redshift Serverless cluster for our streaming data pipeline.
    We can provision Redshift Workgroup, create a Namespace and other required resources
    either manually or with CloudFormation template.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们需要为我们的流数据管道创建 Redshift Serverless 集群。我们可以手动或使用 CloudFormation 模板配置 Redshift
    Workgroup、创建 Namespace 和其他所需资源。
- en: Redshift Serverless is just a data warehouse solution. It enables the execution
    of analytics workloads of any size without the need for data warehouse infrastructure
    management. Redshift is fast and generates insights from enormous volumes of data
    in seconds. It scales automatically to provide quick performance for even the
    most demanding applications.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: Redshift Serverless 仅仅是一个数据仓库解决方案。它可以执行任何规模的分析工作负载，无需数据仓库基础设施管理。Redshift 运行迅速，并能在几秒钟内从巨量数据中生成洞察。它会自动扩展，为即使是最苛刻的应用程序提供快速性能。
- en: '![](../Images/9e11cbe2d920bab18ad3e3c56a7a0a94.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9e11cbe2d920bab18ad3e3c56a7a0a94.png)'
- en: Example view with events from our application. Image by author.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 例子视图显示了我们应用程序的事件。图像来源于作者。
- en: In our case we can deploy the Redshift resources using CloudFormation template
    definitions.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的案例中，我们可以使用 CloudFormation 模板定义来部署 Redshift 资源。
- en: '[PRE10]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'So if we run this in the command line it will deploy this stack:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 所以如果我们在命令行中运行这段代码，它将部署这个堆栈：
- en: '[PRE11]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Typically we would want to deploy databases in a private subnet. However, in
    the early stages of a development, you might want to have a direct access to Redshift
    from dev machine.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，我们希望在私有子网中部署数据库。然而，在开发的早期阶段，你可能希望从开发机器直接访问 Redshift。
- en: This is not recommended for production environments, but in this development
    case, you can start off by putting Redshift into our `default` VPC subnet.
  id: totrans-76
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 这不推荐用于生产环境，但在这种开发情况下，你可以先将 Redshift 放入我们的 `default` VPC 子网。
- en: Now when all required pipeline resources were successfully provisioned we can
    connect our Kinesis stream and Redshift data warehouse.
  id: totrans-77
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 现在，当所有所需的管道资源成功配置后，我们可以连接我们的 Kinesis 流和 Redshift 数据仓库。
- en: 'Then we can use SQL statements to create `kinesis_data` schema in Redshift:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们可以使用 SQL 语句在 Redshift 中创建 `kinesis_data` 模式：
- en: '[PRE12]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: The first part of this SQL will set AWS Kinesis as source. The second one will
    create a view with event data from our application.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 这段 SQL 的第一部分将设置 AWS Kinesis 作为数据源。第二部分将创建一个包含我们应用程序事件数据的视图。
- en: Make sure to create AWS Redshift role with added `AmazonRedshiftAllCommandsFullAccess`
    AWS managed policy.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 确保创建一个具有 `AmazonRedshiftAllCommandsFullAccess` AWS 管理策略的 AWS Redshift 角色。
- en: '[PRE13]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'That’s it. Everything is ready to run the application to simulate the event
    data stream. These events will appear straight away in the Redshift view we have
    just created:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 就这样。一切准备好运行应用程序以模拟事件数据流。这些事件会立即出现在我们刚刚创建的 Redshift 视图中：
- en: '![](../Images/611621ecd4f8ba97807a73dcbfff5e82.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/611621ecd4f8ba97807a73dcbfff5e82.png)'
- en: Application running locally. Image by author.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 应用程序在本地运行。图像来源于作者。
- en: '![](../Images/9e11cbe2d920bab18ad3e3c56a7a0a94.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9e11cbe2d920bab18ad3e3c56a7a0a94.png)'
- en: Example view with events from our application. Image by author.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 示例视图显示了来自我们应用程序的事件。图片由作者提供。
- en: Conclusion
  id: totrans-88
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: We created a simple and reliable streaming data pipeline from a serverless application
    created with AWS Lambda to AWS Redshift data warehouse where data is transformed
    and ingested in real time. It enables capturing, processing, and storing data
    streams of any size with ease. It is great for any Machine learning (ML) pipeline
    where models are used to examine data and forecast inference endpoints as streams
    flow to their destination.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 我们创建了一个简单而可靠的流数据管道，从使用 AWS Lambda 创建的无服务器应用程序到 AWS Redshift 数据仓库，在那里数据实时转化和摄取。它能够轻松捕获、处理和存储任何规模的数据流。对于任何机器学习（ML）管道都非常适用，其中模型用于检查数据并预测推理端点，因为数据流向其目标。
- en: We used infrastructure as code to deploy data pipeline resources. This is a
    preferable approach to deploying resources in different data environments.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用基础设施即代码来部署数据管道资源。这是部署不同数据环境中资源的首选方法。
- en: Recommended read
  id: totrans-91
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 推荐阅读
- en: '[](/continuous-integration-and-deployment-for-data-platforms-817bf1b6bed1?source=post_page-----04e09d7e85b2--------------------------------)
    [## Continuous Integration and Deployment for Data Platforms'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '[/continuous-integration-and-deployment-for-data-platforms-817bf1b6bed1?source=post_page-----04e09d7e85b2--------------------------------)
    [## 数据平台的持续集成和部署'
- en: CI/CD for data engineers and ML Ops
  id: totrans-93
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据工程师和 ML 运维的 CI/CD
- en: towardsdatascience.com](/continuous-integration-and-deployment-for-data-platforms-817bf1b6bed1?source=post_page-----04e09d7e85b2--------------------------------)
    [](/data-platform-architecture-types-f255ac6e0b7?source=post_page-----04e09d7e85b2--------------------------------)
    [## Data Platform Architecture Types
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '[Towards Data Science](/continuous-integration-and-deployment-for-data-platforms-817bf1b6bed1?source=post_page-----04e09d7e85b2--------------------------------)
    [/data-platform-architecture-types-f255ac6e0b7?source=post_page-----04e09d7e85b2--------------------------------)
    [## 数据平台架构类型'
- en: How well does it answer your business needs? Dilemma of a choice.
  id: totrans-95
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 它能多大程度上满足你的业务需求？选择的困境。
- en: towardsdatascience.com](/data-platform-architecture-types-f255ac6e0b7?source=post_page-----04e09d7e85b2--------------------------------)
    [](https://levelup.gitconnected.com/infrastructure-as-code-for-beginners-a4e36c805316?source=post_page-----04e09d7e85b2--------------------------------)
    [## Infrastructure as Code for Beginners
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '[Towards Data Science](/data-platform-architecture-types-f255ac6e0b7?source=post_page-----04e09d7e85b2--------------------------------)
    [https://levelup.gitconnected.com/infrastructure-as-code-for-beginners-a4e36c805316?source=post_page-----04e09d7e85b2--------------------------------)
    [## 初学者的基础设施即代码'
- en: Deploy Data Pipelines like a pro with these templates
  id: totrans-97
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用这些模板像专业人士一样部署数据管道
- en: levelup.gitconnected.com](https://levelup.gitconnected.com/infrastructure-as-code-for-beginners-a4e36c805316?source=post_page-----04e09d7e85b2--------------------------------)
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '[LevelUp](https://levelup.gitconnected.com/infrastructure-as-code-for-beginners-a4e36c805316?source=post_page-----04e09d7e85b2--------------------------------)'
