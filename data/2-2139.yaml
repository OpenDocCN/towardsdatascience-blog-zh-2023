- en: Training Sentence Transformers with Softmax Loss
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Softmax Loss训练句子变换器
- en: 原文：[https://towardsdatascience.com/training-sentence-transformers-with-softmax-loss-c114e74e1583](https://towardsdatascience.com/training-sentence-transformers-with-softmax-loss-c114e74e1583)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/training-sentence-transformers-with-softmax-loss-c114e74e1583](https://towardsdatascience.com/training-sentence-transformers-with-softmax-loss-c114e74e1583)
- en: '[NLP For Semantic Search](https://jamescalam.medium.com/list/nlp-for-semantic-search-d3a4b96a52fe)'
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '[NLP For Semantic Search](https://jamescalam.medium.com/list/nlp-for-semantic-search-d3a4b96a52fe)'
- en: How the original sentence transformer (SBERT) was built
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 原始句子变换器（SBERT）的构建方式
- en: '[](https://jamescalam.medium.com/?source=post_page-----c114e74e1583--------------------------------)[![James
    Briggs](../Images/cb34b7011748e4d8607b7ff4a8510a93.png)](https://jamescalam.medium.com/?source=post_page-----c114e74e1583--------------------------------)[](https://towardsdatascience.com/?source=post_page-----c114e74e1583--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----c114e74e1583--------------------------------)
    [James Briggs](https://jamescalam.medium.com/?source=post_page-----c114e74e1583--------------------------------)'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://jamescalam.medium.com/?source=post_page-----c114e74e1583--------------------------------)[![James
    Briggs](../Images/cb34b7011748e4d8607b7ff4a8510a93.png)](https://jamescalam.medium.com/?source=post_page-----c114e74e1583--------------------------------)[](https://towardsdatascience.com/?source=post_page-----c114e74e1583--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----c114e74e1583--------------------------------)
    [James Briggs](https://jamescalam.medium.com/?source=post_page-----c114e74e1583--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----c114e74e1583--------------------------------)
    ·9 min read·Jan 12, 2023
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page-----c114e74e1583--------------------------------)
    ·阅读时间9分钟·2023年1月12日
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/e9e629a36ae10fd37e92fbd2fa3327d2.png)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e9e629a36ae10fd37e92fbd2fa3327d2.png)'
- en: Photo by [Possessed Photography](https://unsplash.com/@possessedphotography?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral). Originally
    posted in the [NLP for Semantic Search ebook](https://www.pinecone.io/learn/sentence-embeddings/)
    at Pinecone (where the author is employed).
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由[Possessed Photography](https://unsplash.com/@possessedphotography?utm_source=medium&utm_medium=referral)提供，来源于[Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)。最初发布于[Pinecone的《NLP
    for Semantic Search ebook》](https://www.pinecone.io/learn/sentence-embeddings/)（作者所在公司）。
- en: Search is entering a golden age. Thanks to *“sentence embeddings”* and specially
    trained models called*“sentence transformers”* we can now search for information
    using concepts rather than keyword matching. Unlocking a human-like information
    discovery process.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 搜索正在进入一个黄金时代。得益于*“句子嵌入”*和专门训练的模型*“句子变换器”*，我们现在可以通过概念而不是关键词匹配来搜索信息，从而解锁类似人类的信息发现过程。
- en: This article will explore the training process of the first sentence transformer,
    *sentence-BERT* — more commonly known as *SBERT*. We will explore the **N**atural
    **L**anguage **I**nference (NLI) training approach of *softmax loss* to fine-tune
    models for producing sentence embeddings.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 本文将探讨第一个句子变换器的训练过程，即*sentence-BERT* — 更常被称为*SBERT*。我们将探讨**自然语言推理**（NLI）训练方法以及*softmax
    loss*如何用于微调模型，以生成句子嵌入。
- en: Be aware that softmax loss is no longer the preferred approach to training sentence
    transformers and has been superseded by other methods such as MSE margin and [multiple
    negatives ranking loss](https://www.pinecone.io/learn/fine-tune-sentence-transformers-mnr/).
    But we’re covering this training method as an important milestone in the development
    of ever-improving sentence embeddings.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，softmax loss 已不再是训练句子变换器的首选方法，它已被其他方法如MSE margin和[多负样本排序损失](https://www.pinecone.io/learn/fine-tune-sentence-transformers-mnr/)所取代。但我们仍将介绍这种训练方法，因为它是句子嵌入不断发展的一个重要里程碑。
- en: This article also covers *two approaches* to fine-tuning. The first shows how
    NLI training with softmax loss works. The second uses the excellent training utilities
    provided by the `sentence-transformers` library — it’s more abstracted, making
    building good sentence transformer models *much easier*.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 本文还涵盖了*两种方法*的微调。第一种展示了如何使用softmax loss进行NLI训练。第二种利用了`sentence-transformers`库提供的优秀训练工具——这种方法更为抽象，使得构建好的句子变换器模型*容易得多*。
- en: NLI Training
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: NLI训练
- en: There are several ways of training sentence transformers. One of the most popular
    (and the approach we will cover) is using Natural Language Inference (NLI) datasets.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 训练句子变换器有几种方法。最流行的一种（也是我们将讨论的方法）是使用自然语言推断（NLI）数据集。
- en: NLI focus on identifying sentence pairs that *infer* or *do not infer* one another.
    We will use two of these datasets; the Stanford Natural Language Inference (SNLI)
    and Multi-Genre NLI (MNLI) corpora.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: NLI 专注于识别那些*推断*或*不推断*彼此的句子对。我们将使用两个数据集：斯坦福自然语言推断（SNLI）和多领域 NLI（MNLI）语料库。
- en: 'Merging these two corpora gives us 943K sentence pairs (550K from SNLI, 393K
    from MNLI). All pairs include a `premise` and a `hypothesis`, and each pair is
    assigned a `label`:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 合并这两个语料库后，我们得到 943K 句子对（SNLI 550K，MNLI 393K）。所有对都包括一个 `premise` 和一个 `hypothesis`，每对都分配一个
    `label`：
- en: '**0** — *entailment*, e.g., the `premise` suggests the `hypothesis`.'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**0** — *蕴含*，例如，`premise` 暗示了 `hypothesis`。'
- en: '**1** — *neutral*, the `premise` and `hypothesis` could both be true, but they
    are not necessarily related.'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**1** — *中立*，`premise` 和 `hypothesis` 都可以为真，但它们不一定相关。'
- en: '**2** — *contradiction*, the `premise` and `hypothesis` contradict each other.'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**2** — *矛盾*，`premise` 和 `hypothesis` 互相矛盾。'
- en: When training the model, we will be feeding sentence A (the `premise`) into
    BERT, followed by sentence B (the `hypothesis`) on the next step.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 训练模型时，我们将首先将句子 A（即 `premise`）输入 BERT，然后在下一步输入句子 B（即 `hypothesis`）。
- en: From there, the models are optimized using *softmax loss* using the `label`
    field. We will explain this in more depth soon.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 从那里，模型使用 `label` 字段通过 *softmax 损失* 进行优化。我们将很快更深入地解释这一点。
- en: 'For now, let’s download and merge the two datasets. We will use the `datasets`
    library from Hugging Face, which can be downloaded using `!pip install datasets`.
    To download and merge, we write:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们下载并合并这两个数据集。我们将使用 Hugging Face 的 `datasets` 库，可以通过 `!pip install datasets`
    下载。要下载和合并，我们写：
- en: Both datasets contain `-1` values in the `label` feature where no confident
    class could be assigned. We remove them using the `filter` method.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 两个数据集的 `label` 特征中包含 `-1` 值，表示无法分配自信的类别。我们使用 `filter` 方法将其移除。
- en: We must convert our human-readable sentences into transformer-readable tokens,
    so we go ahead and tokenize our sentences. Both `premise` and `hypothesis` features
    must be split into their own `input_ids` and `attention_mask` tensors.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 我们必须将人类可读的句子转换为变换器可读的令牌，因此我们继续对句子进行分词。`premise` 和 `hypothesis` 特征必须分成各自的 `input_ids`
    和 `attention_mask` 张量。
- en: Now, all we need to do is prepare the data to be read into the model. To do
    this, we first convert the `dataset` features into PyTorch tensors and then initialize
    a data loader which will feed data into our model during training.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们需要做的就是准备数据，以便输入到模型中。为此，我们首先将 `dataset` 特征转换为 PyTorch 张量，然后初始化一个数据加载器，该加载器将在训练期间将数据输入到我们的模型中。
- en: And we’re done with data preparation. Let’s move on to the training approach.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 数据准备完成了。让我们继续讨论训练方法。
- en: Softmax Loss
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Softmax 损失
- en: Optimizing with *softmax loss* was the primary method used by Reimers and Gurevych
    in the original SBERT paper [1].
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 *softmax 损失* 是 Reimers 和 Gurevych 在原始 SBERT 论文 [1] 中使用的主要方法。
- en: Although this was used to train the first sentence transformer model, it is
    no longer the go-to training approach. Instead, [the MNR loss approach](https://www.pinecone.io/learn/fine-tune-sentence-transformers-mnr/)
    is most common today. We will cover this method in another article.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这用于训练第一个句子变换器模型，但它不再是首选的训练方法。现在，[MNR 损失方法](https://www.pinecone.io/learn/fine-tune-sentence-transformers-mnr/)
    是最常见的。我们将在另一篇文章中介绍这种方法。
- en: However, we hope that explaining softmax loss will help demystify the different
    approaches applied to training sentence transformers. We included a comparison
    to MNR loss at the end of the article.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我们希望解释 softmax 损失有助于揭示训练句子变换器时应用的不同方法。我们在文章末尾包含了与 MNR 损失的比较。
- en: Model Preparation
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型准备
- en: When we train an SBERT model, we don’t need to start from scratch. We begin
    with an already pretrained BERT model (and tokenizer).
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们训练 SBERT 模型时，不需要从头开始。我们从一个已经预训练好的 BERT 模型（及其分词器）开始。
- en: We will be using what is called a *‘siamese’*-BERT architecture during training.
    All this means is that given a sentence pair, we feed *sentence A* into BERT first,
    then feed *sentence B* once BERT has finished processing the first.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练过程中，我们将使用一种称为*‘siamese’*-BERT 架构。这意味着，对于一个句子对，我们首先将*句子 A* 输入 BERT，然后在 BERT
    处理完第一个句子后，再输入*句子 B*。
- en: This has the effect of creating a *siamese*-like network where we can imagine
    two identical BERTs are being trained in parallel on sentence pairs. In reality,
    there is just a single model processing two sentences one after the other.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 这会产生一个 *类似 Siamese* 的网络，我们可以想象有两个相同的 BERT 正在并行处理句子对。实际上，只有一个模型在处理两个句子。
- en: '![](../Images/efe79a46a78a8721758ea20b8be06f21.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/efe79a46a78a8721758ea20b8be06f21.png)'
- en: Siamese-BERT processing a sentence pair and then pooling the large token embeddings
    tensor into a single dense vector.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: Siamese-BERT 处理一对句子，然后将大规模的令牌嵌入张量池化为一个单一的密集向量。
- en: BERT will output 512 768-dimensional embeddings. We will convert these into
    an *average* embedding using *mean-pooling*. This *pooled output* is our sentence
    embedding. We will have two per step — one for sentence A that we call `u`, and
    one for sentence B, called `v`.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: BERT 将输出 512 个 768 维的嵌入。我们将这些嵌入转换为 *平均* 嵌入，使用 *均值池化*。这个 *池化输出* 是我们的句子嵌入。每步我们会有两个
    —— 一个是句子 A，称为 `u`，另一个是句子 B，称为 `v`。
- en: To perform this mean pooling operation, we will define a function called `mean_pool`.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 要执行此均值池化操作，我们将定义一个名为 `mean_pool` 的函数。
- en: Here we take BERT’s token embeddings output (we’ll see this all in full soon)
    and the sentence’s `attention_mask` tensor. We then resize the `attention_mask`
    to align to the higher `768`-dimensionality of the token embeddings.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们获取 BERT 的令牌嵌入输出（我们很快会全面看到）和句子的 `attention_mask` 张量。然后我们调整 `attention_mask`
    的大小，以对齐到令牌嵌入的更高 `768` 维度。
- en: We apply this resized mask `in_mask` to those token embeddings to exclude padding
    tokens from the mean pooling operation. Our mean pooling takes the average activation
    of values across each dimension to produce a single value. This brings our tensor
    sizes from `(512*768)` to `(1*768)`.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将调整大小的掩码`in_mask`应用于这些令牌嵌入，以排除平均池化操作中的填充令牌。我们的平均池化操作在每个维度上取值的平均激活值，以产生一个单一值。这将我们的张量大小从
    `(512*768)` 变为 `(1*768)`。
- en: 'The next step is to concatenate these embeddings. Several different approaches
    to this were presented in the paper:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是连接这些嵌入。论文中提出了几种不同的方法：
- en: Concatenation methods for sentence embeddings `u` and `v` and their performance
    on STS benchmarks.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 句子嵌入 `u` 和 `v` 的连接方法及其在 STS 基准测试中的表现。
- en: Of these, the best performing is built by concatenating vectors `u`, `v`, and
    `|u-v|`. Concatenation of them all produces a vector three times the length of
    each original vector. We label this concatenated vector `(u, v, |u-v|)`. Where
    `|u-v|` is the element-wise difference between vectors `u` and `v`.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，表现最佳的是通过连接向量 `u`、`v` 和 `|u-v|` 构建的。将它们全部连接起来会生成一个长度是每个原始向量三倍的向量。我们将这个连接向量标记为
    `(u, v, |u-v|)`。其中 `|u-v|` 是向量 `u` 和 `v` 之间的逐元素差异。
- en: '![](../Images/e10b0acdcbb05e26e8860e333aa0b99e.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e10b0acdcbb05e26e8860e333aa0b99e.png)'
- en: We concatenate `(u, v, |u-v|)` to merge the sentence embeddings from sentence
    A and B.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将 `(u, v, |u-v|)` 连接起来，以合并句子 A 和 B 的句子嵌入。
- en: 'We will perform this concatenation operation using PyTorch. Once we have our
    mean-pooled sentence vectors `u` and `v` we concatenate with:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用 PyTorch 执行此连接操作。一旦我们获得了均值池化的句子向量 `u` 和 `v`，我们将其连接为：
- en: Vector `(u, v, |u-v|)` is fed into a feed-forward neural network (FFNN). The
    FFNN processes the vector and outputs three activation values. One for each of
    our `label` classes; *entailment*, *neutral*, and *contradiction*.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 向量 `(u, v, |u-v|)` 输入到前馈神经网络 (FFNN)。FFNN 处理这个向量并输出三个激活值。每个 `label` 类别的一个值：*蕴涵*、*中性*
    和 *矛盾*。
- en: As these activations and `label` classes are aligned, we now calculate the softmax
    loss between them.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这些激活和 `label` 类别对齐，我们现在计算它们之间的 softmax 损失。
- en: '![](../Images/ce06be3cfb369530c14702cc5e7e8765.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ce06be3cfb369530c14702cc5e7e8765.png)'
- en: The final steps of training. The concatenated `(u, v, |u-v|)` vector is fed
    through a feed-forward NN to produce three output activations. Then we calculate
    the softmax loss between these predictions and the true labels.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 训练的最后步骤。将连接后的 `(u, v, |u-v|)` 向量输入到前馈神经网络中以生成三个输出激活值。然后我们计算这些预测和真实标签之间的 softmax
    损失。
- en: Softmax loss is calculated by applying a softmax function across the three activation
    values (or nodes), producing a predicted label. We then use [cross-entropy loss](https://www.pinecone.io/learn/cross-entropy-loss/)
    to calculate the difference between our predicted label and true `label`.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: Softmax 损失通过对三个激活值（或节点）应用 softmax 函数来计算，从而生成预测标签。然后我们使用 [交叉熵损失](https://www.pinecone.io/learn/cross-entropy-loss/)来计算预测标签与真实`label`之间的差异。
- en: 'The model is then optimized using this loss. We use an Adam optimizer with
    a learning rate of `2e-5` and a linear warmup period of *10%* of the total training
    data for the optimization function. To set that up, we use the standard PyTorch
    `Adam` optimizer alongside a learning rate scheduler provided by HF transformers:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，使用这个损失对模型进行优化。我们使用学习率为`2e-5`的Adam优化器，并且优化函数的线性预热期为总训练数据的*10%*。为此，我们使用标准的
    PyTorch `Adam`优化器以及 HF transformers 提供的学习率调度器：
- en: Now let’s put all of that together in a PyTorch training loop.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们将所有内容结合到一个 PyTorch 训练循环中。
- en: We only train for a single epoch here. Realistically this should be enough (and
    mirrors what was described in the original SBERT paper). The last thing we need
    to do is save the model.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 我们这里只训练一个 epoch。现实中这应该足够（并且与原始 SBERT 论文中描述的情况相符）。最后，我们需要做的是保存模型。
- en: Now let’s compare everything we’ve done so far with `sentence-transformers`
    training utilities. We will compare this and other sentence transformer models
    at the end of the article.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们将到目前为止所做的所有工作与`sentence-transformers`训练工具进行比较。我们将在文章末尾对比这个和其他句子变换模型。
- en: Fine-Tuning With Sentence Transformers
  id: totrans-56
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Sentence Transformers 进行微调
- en: As we already mentioned, the `sentence-transformers` library has excellent support
    for those of us just wanting to train a model without worrying about the underlying
    training mechanisms.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前提到的，`sentence-transformers`库对那些希望训练模型而不必担心底层训练机制的人提供了很好的支持。
- en: We don’t need to do much beyond a little data preprocessing (but less than what
    we did above). So let’s go ahead and put together the same fine-tuning process,
    but using `sentence-transformers`.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 我们只需要做一些数据预处理（但比我们之前做的少）。所以让我们继续完成相同的微调过程，但使用`sentence-transformers`。
- en: Training Data
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练数据
- en: Again we’re using the same SNLI and MNLI corpora, but this time we will be transforming
    them into the format required by `sentence-transformers` using their `InputExample`
    class. Before that, we need to download and merge the two datasets just like before.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 我们再次使用相同的 SNLI 和 MNLI 语料库，但这次我们将它们转换为`sentence-transformers`所需的格式，使用它们的`InputExample`类。在此之前，我们需要像之前一样下载并合并这两个数据集。
- en: Now we’re ready to format our data for `sentence-transformers`. All we do is
    convert the current `premise`, `hypothesis`, and `label` format into an *almost*
    matching format with the `InputExample` class.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们准备好将数据格式化为`sentence-transformers`所需的格式。我们只需将当前的`premise`、`hypothesis`和`label`格式转换为与`InputExample`类*几乎*匹配的格式即可。
- en: We’ve also initialized a `DataLoader` just as we did before. From here, we want
    to begin setting up the model. In `sentence-transformers` we build models using
    different *modules*.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也像之前一样初始化了一个`DataLoader`。接下来，我们要开始设置模型。在`sentence-transformers`中，我们使用不同的*模块*来构建模型。
- en: All we need is the transformer model module, followed by a mean pooling module.
    The transformer models are loaded from HF, so we define `bert-base-uncased` as
    before.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 我们只需要 transformer 模型模块，然后是均值池化模块。transformer 模型从 HF 加载，因此我们像之前一样定义`bert-base-uncased`。
- en: We have our data, the model, and now we define how to optimize our model. Softmax
    loss is *very* easy to initialize.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有数据、模型，现在定义如何优化我们的模型。Softmax 损失*非常*容易初始化。
- en: Now we’re ready to train the model. We train for a single epoch and warm up
    for 10% of training as before.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们准备好训练模型了。我们训练一个 epoch，并像之前一样将训练数据的 10% 用于预热。
- en: With that, we’re done, the new model is saved to `./sbert_test_b`. We can load
    the model from that location using either the `SentenceTransformer` or HF’s `from_pretrained`
    methods! Let’s move on to comparing this to other SBERT models.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 完成后，新的模型保存到`./sbert_test_b`。我们可以使用`SentenceTransformer`或 HF 的`from_pretrained`方法从该位置加载模型！让我们继续将其与其他
    SBERT 模型进行比较。
- en: Compare SBERT Models
  id: totrans-67
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 比较 SBERT 模型
- en: We’re going to test the models on a set of random sentences. We will build our
    mean-pooled embeddings for each sentence using *four* models; *softmax-loss* SBERT,
    *multiple-negatives-ranking-loss* SBERT, the original SBERT `sentence-transformers/bert-base-nli-mean-tokens`,
    and BERT `bert-base-uncased`.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将对一组随机句子进行模型测试。我们将使用*四*个模型构建每个句子的均值池化嵌入；*softmax-loss* SBERT、*multiple-negatives-ranking-loss*
    SBERT、原始 SBERT `sentence-transformers/bert-base-nli-mean-tokens` 和 BERT `bert-base-uncased`。
- en: After producing sentence embeddings, we will calculate the cosine similarity
    between all possible sentence pairs, producing a simple but insightful semantic
    textual similarity (STS) test.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 生成句子嵌入后，我们将计算所有可能句子对之间的余弦相似度，进行一个简单但有见地的语义文本相似性（STS）测试。
- en: We define two new functions; `sts_process` to build the sentence embeddings
    and compare them with cosine similarity and `sim_matrix` to construct a similarity
    matrix from all possible pairs.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 我们定义了两个新函数；`sts_process`用于构建句子嵌入并用余弦相似度进行比较，以及`sim_matrix`用于从所有可能的对构建相似性矩阵。
- en: Then we just run each model through the `sim_matrix` function.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们只是通过`sim_matrix`函数运行每个模型。
- en: After processing all pairs, we visualize the results in heatmap visualizations.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 处理完所有对之后，我们在热图可视化中展示结果。
- en: '![](../Images/ed5615ebbabe4878af807caf5bdab6da.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ed5615ebbabe4878af807caf5bdab6da.png)'
- en: Similarity score heatmaps for four BERT/SBERT models.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 四种BERT/SBERT模型的相似性分数热图。
- en: In these heatmaps, we ideally want all dissimilar pairs to have very low scores
    (near white) and similar pairs to produce distinctly higher scores.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些热图中，我们理想情况下希望所有不相似的对具有非常低的分数（接近白色），而相似的对则产生明显更高的分数。
- en: Let’s talk through these results. The bottom-left and top-right models produce
    the correct top three pairs, whereas BERT and *softmax loss* SBERT return 2/3
    of the correct pairs.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来讨论这些结果。左下角和右上角的模型生成了正确的前三对，而BERT和*softmax损失* SBERT返回了2/3的正确对。
- en: If we focus on the standard BERT model, we see minimal variation in square color.
    This is because almost every pair produces a similarity score of between *0.6*
    to *0.7*. This lack of variation makes it challenging to distinguish between more-or-less
    similar pairs. However, this is to be expected as BERT has *not* been fine-tuned
    for semantic similarity.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们关注标准BERT模型，我们会看到方块颜色变化极小。这是因为几乎每一对的相似性分数在*0.6*到*0.7*之间。这种缺乏变化使得区分相似对和不相似对变得困难。然而，这也是可以预期的，因为BERT*没有*针对语义相似性进行微调。
- en: Our PyTorch *softmax loss* SBERT (top-left) misses the *9–1* sentence pair.
    Nonetheless, the pairs it produces are much more distinct from dissimilar pairs
    than the vanilla BERT model, so it’s an improvement. The `sentence-transformers`
    version is better still and did *not* miss the *9-1* pair.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的PyTorch *softmax损失* SBERT（左上角）漏掉了*9–1*句子对。尽管如此，它产生的对比不相似对要明显更具区分性，比原始BERT模型有所改进。`sentence-transformers`版本表现更好，并且*没有*漏掉*9-1*对。
- en: Next, we have the SBERT model trained by Reimers and Gurevych in the 2019 paper
    (bottom-left) [1]. It produces better performance than our SBERT models but still
    has little variation between similar and dissimilar pairs.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们有Reimers和Gurevych在2019年论文中训练的SBERT模型（左下角）[1]。它比我们的SBERT模型表现更好，但相似对和不相似对之间仍然差异不大。
- en: And finally, we have an SBERT model trained using *MNR loss*. This model is
    easily the highest performing. Most dissimilar pairs produce a score *very* close
    to *zero*. The highest non-pair returns *0.28* — roughly half of the true-pair
    scores.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们有一个使用*MNR损失*训练的SBERT模型。这个模型无疑是性能最优的。大多数不相似的对产生的分数*非常*接近*零*。最高的非对分数返回*0.28*——大约是实际对分数的一半。
- en: These results show that the SBERT MNR model is the highest-performing. Producing
    much higher activations (with respect to the average) for true pairs than any
    other model, making similarity much easier to identify. SBERT with softmax loss
    is clearly an improvement over BERT, but unlikely to offer any benefit over the
    SBERT with MNR loss model.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 这些结果表明SBERT MNR模型是性能最优的。它为真实对产生的激活值（相对于平均值）远高于其他模型，使得相似性更容易识别。SBERT使用softmax损失显然比BERT有所改进，但不太可能比使用MNR损失的SBERT模型提供更多益处。
- en: That’s it for this article on fine-tuning BERT for building sentence embeddings!
    We delved into the details of preprocessing SNLI and MNLI datasets for NLI training
    and how to fine-tune BERT using the *softmax loss* approach.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是关于微调BERT以构建句子嵌入的文章了！我们深入探讨了SNLI和MNLI数据集的预处理细节以及如何使用*softmax损失*方法微调BERT。
- en: Finally, we compared this *softmax-loss* SBERT against vanilla BERT, the original
    SBERT, and [an *MNR loss* SBERT](https://www.pinecone.io/learn/fine-tune-sentence-transformers-mnr/)
    using a simple STS task. We found that although fine-tuning with *softmax loss*
    does produce valuable sentence embeddings — it still lacks quality compared to
    more recent training approaches.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将这个*softmax损失* SBERT与原始BERT、原始SBERT以及[一个*MNR损失* SBERT](https://www.pinecone.io/learn/fine-tune-sentence-transformers-mnr/)进行了比较，使用了一个简单的STS任务。我们发现，尽管使用*softmax损失*进行微调确实能产生有价值的句子嵌入，但与较新的训练方法相比，其质量仍然欠佳。
- en: We hope this has been an insightful and exciting exploration of how transformers
    can be fine-tuned for building sentence embeddings.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望这次对变换器如何微调以构建句子嵌入的探索能够带来深入的见解和兴奋感。
- en: References
  id: totrans-85
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] N. Reimers, I. Gurevych, [Sentence-BERT: Sentence Embeddings using Siamese
    BERT-Networks](https://arxiv.org/abs/1908.10084) (2019), ACL'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] N. Reimers, I. Gurevych, [Sentence-BERT: 使用 Siamese BERT 网络的句子嵌入](https://arxiv.org/abs/1908.10084)
    (2019), ACL'
- en: '**All images are by the author except where stated otherwise*'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '**除非另有说明，否则所有图像均由作者提供*'
