- en: A Marriage of Machine Learning and Optimization Algorithms
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习与优化算法的结合
- en: 原文：[https://towardsdatascience.com/a-marriage-of-machine-learning-and-optimization-algorithms-e6c680454f06](https://towardsdatascience.com/a-marriage-of-machine-learning-and-optimization-algorithms-e6c680454f06)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/a-marriage-of-machine-learning-and-optimization-algorithms-e6c680454f06](https://towardsdatascience.com/a-marriage-of-machine-learning-and-optimization-algorithms-e6c680454f06)
- en: How pattern detection and pattern exploitation might elevate each other to a
    new level
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模式检测和模式利用如何将彼此提升到一个新的层次
- en: '[](https://wvheeswijk.medium.com/?source=post_page-----e6c680454f06--------------------------------)[![Wouter
    van Heeswijk, PhD](../Images/9c996bccd6fdfb6d9aa8b50b93338eb9.png)](https://wvheeswijk.medium.com/?source=post_page-----e6c680454f06--------------------------------)[](https://towardsdatascience.com/?source=post_page-----e6c680454f06--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----e6c680454f06--------------------------------)
    [Wouter van Heeswijk, PhD](https://wvheeswijk.medium.com/?source=post_page-----e6c680454f06--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://wvheeswijk.medium.com/?source=post_page-----e6c680454f06--------------------------------)[![Wouter
    van Heeswijk, PhD](../Images/9c996bccd6fdfb6d9aa8b50b93338eb9.png)](https://wvheeswijk.medium.com/?source=post_page-----e6c680454f06--------------------------------)[](https://towardsdatascience.com/?source=post_page-----e6c680454f06--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----e6c680454f06--------------------------------)
    [Wouter van Heeswijk, PhD](https://wvheeswijk.medium.com/?source=post_page-----e6c680454f06--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----e6c680454f06--------------------------------)
    ·12 min read·Dec 2, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----e6c680454f06--------------------------------)
    ·阅读时间 12 分钟·2023年12月2日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/a9b9c8c0940005f58cb100e812a31d72.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a9b9c8c0940005f58cb100e812a31d72.png)'
- en: Instead of benchmark optimization- and machine learning algorithms against each
    other, we should consider how they can strengthen each other [Photo by [Wedding
    Dreamz](https://unsplash.com/@wedding_dreamz?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)]
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不应将基准优化和机器学习算法相互比较，而应考虑它们如何相互加强 [图片由 [Wedding Dreamz](https://unsplash.com/@wedding_dreamz?utm_source=medium&utm_medium=referral)
    提供，在 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral) 上]
- en: Although most of us don’t see it, optimization algorithms (OAs) are at work
    everywhere. They plan shelve stocking for our grocery stores, create airport schedules,
    and give us the shortest route to our holiday destination. **Exact algorithms
    in particular do very well at exploiting known structures** — e.g., convex structures
    — finding solutions even in massive decision spaces with many constraints. Over
    the past decades, the combination of hardware- and algorithmic improvements yielded
    massive speed-ups in the order of millions. A planning task that might have taken
    a computer months to complete in the 90’s could just take a second today.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们大多数人看不到，但优化算法（OAs）无处不在。它们为我们的超市规划货架，为机场制定航班时间表，并为我们提供前往度假目的地的最短路线。**特别是精确算法在利用已知结构方面表现优异**
    — 例如，凸结构 — 即使在具有众多约束的大型决策空间中也能找到解决方案。在过去几十年中，硬件和算法的改进相结合，带来了数百万倍的速度提升。一个可能在90年代需要几个月才能完成的规划任务，如今可能只需一秒钟。
- en: Similarly, machine learning (ML) has taken an incredible flight in the last
    decade or so. MuZero showed the capability to learn superhuman game-playing policies
    without knowing the games’ rules, Graph Neural Networks learn complex relations
    unperceivable to the human eye, and Transformers gave rise to ChatGPT and its
    competitors. **The commonality is that these algorithms are all able to detect
    patterns from their environment**, be it text databases or video games. Novel
    and highly complicated architectures are introduced on a regular basis, often
    solving new problems and offering unparalleled performance. Despite all successes
    and breakthroughs, for many real-world problems, end-to-end ML struggles to achieve
    competitive results. Tailored OAs often still beat ML, but may require substantial
    computational time.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，机器学习（ML）在过去十年左右取得了惊人的进展。MuZero 展现了在不知道游戏规则的情况下学习超人类游戏策略的能力，图神经网络学习了人眼无法感知的复杂关系，而变换器（Transformers）催生了
    ChatGPT 及其竞争对手。**这些算法的共同点在于它们都能够从环境中检测模式**，无论是文本数据库还是视频游戏。新颖且极其复杂的架构不断被引入，通常解决新的问题并提供无与伦比的性能。尽管取得了诸多成功和突破，对于许多现实世界的问题，端到端的机器学习仍难以取得竞争性结果。定制的优化算法通常仍然优于机器学习，但可能需要大量的计算时间。
- en: There is no need for the two approaches to compete though. Interestingly, **optimization
    algorithms excel at *exploiting* patterns, whereas machine learning shines at
    *detecting* patterns.** Instead of pitting them against each other as benchmarks
    and see which one outperforms the other, wouldn’t it make sense to marry the two
    complementary halves instead?
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 这两种方法并不需要竞争。有趣的是，**优化算法擅长于*利用*模式，而机器学习则在*检测*模式方面表现出色。** 不如将它们作为补充的两个部分结合起来，而不是对比它们的优劣，是否更有意义？
- en: When merging optimization and machine learning, it often boils down to **statistical
    learning being used to improve optimization routines** in one form or another.
    This way, we can speed up the search by exploiting patterns that we learned. The
    development of such integrated solutions has become an emerging research field
    in recent years, with many exciting possibilities ahead.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在合并优化和机器学习时，通常归结为**统计学习被用来以某种形式改进优化例程**。这样，我们可以通过利用学到的模式来加速搜索。这种集成解决方案的开发近年来已成为一个新兴的研究领域，未来有许多令人兴奋的可能性。
- en: How can we integrate machine learning and optimization algorithms?
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 我们如何将机器学习和优化算法结合起来？
- en: 'We established that OA is good at exploiting structures and ML is good at detecting
    them, so there is a natural synergy. Still, what concretely would constitute a
    marriage between OA and ML? Broadly, we might classify in the following four categories:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经确定了优化算法擅长利用结构，而机器学习擅长检测结构，因此它们之间存在自然的协同效应。然而，具体来说，什么构成了优化算法与机器学习之间的结合？广义上，我们可以将其分类为以下四个类别：
- en: '**I.** **OA** **provides input to ML.** OA may offer a heuristic solution that
    can further be improved using ML, or it might perform a computationally intense
    preprocessing step in the algorithmic pipeline.'
  id: totrans-14
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**I.** **优化算法（OA）为机器学习（ML）提供输入。** 优化算法可能提供一个启发式解决方案，该方案可以通过机器学习进一步改进，或者它可能在算法管道中执行计算密集型的预处理步骤。'
- en: ''
  id: totrans-15
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**II. ML provides input to OA.** For instance, ML may suggest a starting solution
    for a warm start, or learn problem structures for OA to exploit.'
  id: totrans-16
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**II. 机器学习（ML）为优化算法（OA）提供输入。** 例如，机器学习可以建议一个用于热启动的起始解决方案，或者学习问题结构供优化算法利用。'
- en: ''
  id: totrans-17
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**III.** **ML is used to accelerate OA.** ML may be used to iteratively detect
    structures, which aid OAsolvers in finding solutions more quickly.'
  id: totrans-18
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**III.** **机器学习（ML）用于加速优化算法（OA）。** 机器学习可以用于迭代检测结构，帮助优化算法更快地找到解决方案。'
- en: ''
  id: totrans-19
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**IV.** **OA solves subroutines in ML**. Routines such as tree search or action
    space evaluation may be efficiently performed using OA.'
  id: totrans-20
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**IV.** **优化算法（OA）解决机器学习中的子例程。** 像树搜索或动作空间评估这样的例程可以通过优化算法高效执行。'
- en: Let’s offer a bit of practical interpretation to this. In part due to increasing
    logging and utilization of data, algorithmic pipelines tend to get increasingly
    complex. They often are composed of multiple tasks, some more suitable for OA,
    others for ML. Thus, it has become very natural for one paradigm providing input
    to the other.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们对此进行一些实际的解释。由于数据记录和利用的增加，算法管道变得越来越复杂。它们通常由多个任务组成，有些更适合优化算法，有些则更适合机器学习。因此，一个范式向另一个范式提供输入已经变得非常自然。
- en: Furthermore, companies often repeatedly solve variants of a specific problem.
    For example, a transport company might solve a vehicle routing problem daily,
    dealing with variable yet comparable instances in terms of customers, time windows
    and load sizes. If general-purpose solvers would take advantage of these similarities,
    the OA algorithms could run more efficiently.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，公司通常会重复解决特定问题的变体。例如，运输公司可能每天解决一次车辆路径问题，处理在客户、时间窗口和负载大小方面变量但可比的实例。如果通用求解器能够利用这些相似性，优化算法的运行效率将会提高。
- en: Finally, since ML as an end-to-end paradigm is often not yet competitive with
    OA, it often helps to optimize some subroutines. Typically, this speeds up the
    ML algorithms and enhances there competitiveness.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，由于机器学习作为端到端的范式通常尚未与优化算法相竞争，因此优化某些子程序往往会有所帮助。这通常会加速机器学习算法并增强其竞争力。
- en: 'In general, combining OA and ML makes sense when:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，当以下情况成立时，结合优化算法和机器学习是有意义的：
- en: OA (or human expertise) is too slow to offer solutions within reasonable time
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 优化算法（或人类专家）在合理的时间内提供解决方案的速度太慢。
- en: There is room to improve upon heuristic solutions, or it is unknown how good
    (or bad) they actually are
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 启发式解决方案仍有改进的空间，或者尚不清楚它们实际效果如何（好或差）。
- en: Good solutions still need to be identified
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 仍需识别良好的解决方案。
- en: There is need for a fast approximative solution
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 需要一种快速的近似解决方案。
- en: The algorithmic pipeline involves elements of both pattern detection and pattern
    exploration
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 算法流程涉及模式检测和模式探索的元素。
- en: We proceed to contextualize optimization algorithms and machine learning, before
    offering some illustrative examples.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将优化算法和机器学习进行背景化处理，然后提供一些说明性的例子。
- en: Optimization algorithms
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 优化算法
- en: A brief introduction to optimization algorithms is in order. To align with the
    ML integration, we crudely categorize in **exact algorithms (optimal solutions
    but slow)** and **heuristics (suboptimal solutions but faster)**.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 有必要简要介绍优化算法。为了与机器学习集成一致，我们粗略地将其分类为**精确算法（最优解但速度较慢）**和**启发式方法（次优解但更快）**。
- en: Exact algorithms
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 精确算法
- en: Exact algorithms are powerful techniques employed in optimization problems,
    capable of finding **provably optimal solutions** within a feasible solution space.
    Commonly, problems are formulated as **mathematical programs** (e.g., linear or
    quadratic programs), which can be solved using optimization software such as CPLEX,
    SCIP or Gurobi. These solvers systematically explore the solution space and guarantee
    that the best solution is identified.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 精确算法是用于优化问题的强大技术，能够在可行的解决方案空间内找到**可证明的最优解**。通常，问题被表述为**数学规划**（例如，线性或二次规划），可以使用如
    CPLEX、SCIP 或 Gurobi 等优化软件进行求解。这些求解器系统地探索解决方案空间，并保证找到最佳解。
- en: Although powerful out of the box, there is a limit to the solution spaces that
    can be handled. For very large problems, we often need to put substantial design
    effort into designing decompositions and **branch-, price- and/or cut schemes**
    to reduce search complexity and efficiently navigate through the solution space,
    while preserving optimality guarantees.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管现成的方案功能强大，但可以处理的解决方案空间是有限的。对于非常大的问题，我们通常需要投入大量设计工作来设计**分支、定价和/或裁剪方案**，以减少搜索复杂性并有效地导航解决方案空间，同时保留最优性保证。
- en: Heuristics
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 启发式方法
- en: Heuristics offer an alternative problem-solving approach, **sacrificing optimality
    guarantees for computational efficiency**. Heuristics are particularly useful
    for large-scale or complex problems, for which finding optimal solutions is impractical
    within a reasonable timeframe.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 启发式方法提供了一种替代问题解决方案，**以牺牲最优性保证换取计算效率**。启发式方法特别适用于大规模或复杂问题，这些问题在合理的时间框架内寻找最优解是不切实际的。
- en: Basic heuristics provide quick, **rule-of-thumb solutions**. They typically
    yield suboptimal results, but often return decent solutions in limited computational
    time. Examples include nearest-neighbor insertions or 2-opt swaps.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 基本启发式方法提供快速的**经验规则解决方案**。它们通常会产生次优结果，但通常会在有限的计算时间内返回不错的解决方案。例子包括最近邻插入或2-opt交换。
- en: Metaheuristics employ higher-level **strategies that guide the search for solutions**,
    often incorporating elements of randomness and adaptability. Examples include
    genetic algorithms, simulated annealing, and particle swarm optimization. Metaheuristics
    are more intense in terms of tuning effort and runtime, but tend to offer superior
    results compared to basic heuristics.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 元启发式方法采用更高层次的**指导搜索解决方案的策略**，通常包含随机性和适应性元素。例子包括遗传算法、模拟退火和粒子群优化。元启发式方法在调整工作和运行时间方面更为密集，但通常能提供比基本启发式方法更优的结果。
- en: Machine learning
  id: totrans-40
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习
- en: We discuss two paradigms of machine learning; demonstration learning and experience
    learning. Although **strongly related to existing classifications in terms of
    (un)supervised- and reinforcement learning**, we here aim to connect more to the
    optimization context.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 我们讨论了两种机器学习范式；演示学习和经验学习。尽管**与现有分类（无论是监督学习还是强化学习）密切相关**，我们在这里旨在更多地连接到优化背景。
- en: Demonstration learning
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 演示学习
- en: Demonstration learning minimizes the distance between a provided expert policy
    and predicted policy (supervised). It can be seen as a **teacher-student model**,
    in which the student (the ML model) aims to mimic the teacher (e.g., an exact
    algorithm, a tailored heuristic, or human-crafted solutions). Note that this form
    of learning therefore requires theoretical or empirical knowledge in some form.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 演示学习通过最小化提供的专家策略与预测策略之间的距离（监督学习）来进行。它可以被视为一个**师生模型**，其中学生（机器学习模型）旨在模仿老师（例如，精确算法、量身定制的启发式方法或人工制定的解决方案）。需要注意的是，这种学习形式因此需要某种形式的理论或经验知识。
- en: The differences between the solution predicted by the ML model and the known
    expert solution allows to compute a loss function, which the algorithm subsequently
    aims to minimize. If an ML algorithm is shown many **examples of high-quality
    solutions**, we hope it can ultimately replicate those.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习模型预测的解决方案与已知专家解决方案之间的差异允许计算损失函数，算法随后旨在最小化这个损失函数。如果机器学习算法展示了许多**高质量解决方案的例子**，我们希望它最终能够复制这些解决方案。
- en: The downside of demonstration learning is that it **cannot beat the expert policy**.
    If the expert policy is suboptimal, so is the predicted policy. Therefore, demonstration
    learning only makes sense if acquiring the original solutions takes too long,
    and one strives to get the same solutions in a fraction of the time. However,
    if generating example solutions takes prohibitively long, the idea fails as well.
    This puts demonstration learning in a bit of a precarious position.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 演示学习的缺点在于它**无法超越专家策略**。如果专家策略是次优的，那么预测的策略也是如此。因此，演示学习只有在获取原始解决方案花费时间过长且希望在较短时间内获得相同解决方案时才有意义。然而，如果生成示例解决方案需要过长时间，这个想法也会失败。这使得演示学习处于一种相对尴尬的境地。
- en: Demonstration learning is a form of **imitation learning**. We try to reproduce
    the same decision the expert offers, but faster.
  id: totrans-46
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 演示学习是一种**模仿学习**。我们尝试更快地再现专家提供的相同决策。
- en: '![](../Images/55f40c53f950c314c07bb614ff28d44a.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/55f40c53f950c314c07bb614ff28d44a.png)'
- en: In demonstration learning, the ML algorithm aims to mimic expert solutions as
    closely as possible [Photo by [Andre Mouton](https://unsplash.com/@andremouton?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)]
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在演示学习中，机器学习算法旨在尽可能接近专家解决方案[照片来源于[Andre Mouton](https://unsplash.com/@andremouton?utm_source=medium&utm_medium=referral)于[Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)]
- en: Experience learning
  id: totrans-49
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 经验学习
- en: In contrast to demonstration learning, experience learning utilizes observations
    yielded by deployed policies, with the aim of improving them. This form of learning
    is typically associated with reinforcement learning (RL), which **iteratively
    executes and improves policies** based on observed rewards gained from its environment.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 与演示学习相对，经验学习利用通过部署策略获得的观察数据，旨在改进这些策略。这种学习形式通常与强化学习（RL）相关联，**通过观察从环境中获得的奖励迭代执行和改进策略**。
- en: Compared to demonstration learning, **experience learning requires no prior
    knowledge** on solution quality or structure. The downside is that — in its purest
    form — it **leverages no information about the environment**, interacting with
    it strictly through state, action and reward. Common problems associated with
    experience learning are getting stuck in local optima, the need to appropriately
    define rewards, and extensively explore the solution space.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 与示范学习相比，**经验学习不需要关于解决方案质量或结构的先验知识**。缺点是——在其最纯粹的形式下——它**不利用有关环境的信息**，仅通过状态、动作和奖励与环境进行交互。与经验学习相关的常见问题包括陷入局部最优、需要适当地定义奖励以及广泛探索解决方案空间。
- en: Experience learning utilizes feedback signals from its environment in order
    to improve its decision-making policies over time.
  id: totrans-52
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 经验学习利用来自环境的反馈信号，以便随着时间的推移改进其决策政策。
- en: '![](../Images/c12e61460615fc47c93f3297103f50ad.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c12e61460615fc47c93f3297103f50ad.png)'
- en: In experience learning, an ML algorithm aims to improve its decision-making
    policies through interactions with its environment [Photo by [Alex Kondratiev](https://unsplash.com/@alexkondratiev?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)]
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在经验学习中，ML算法旨在通过与环境的交互来改进其决策政策 [照片由 [Alex Kondratiev](https://unsplash.com/@alexkondratiev?utm_source=medium&utm_medium=referral)
    提供，在 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral) 上]
- en: Note that both forms of learning have substantial drawbacks, which in part explains
    why **end-to-end machine learning still struggles** on real-world problems. Thus,
    the integration of OA and ML is indeed mutually beneficial.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，这两种学习形式都有显著的缺点，这在一定程度上解释了为什么**端到端机器学习在现实世界问题上仍然面临困难**。因此，OA和ML的集成确实是互利的。
- en: Of course it is possible — or perhaps even preferable — to apply both forms
    of learning in a single pipeline. An example would be to (i) generate an initial
    solution through demonstration learning, (ii) improve with an optimization solver,
    and (iii) use experience learning to guide the search for the optimal solution.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，可以在一个单一的流程中应用这两种学习形式，或者说这可能更为优越。一个例子是（i）通过示范学习生成初始解决方案，（ii）通过优化求解器进行改进，以及（iii）利用经验学习来指导对最优解决方案的搜索。
- en: Examples of MO-OA
  id: totrans-57
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: MO-OA 示例
- en: Time for some concrete examples. Below follow three representative examples
    based on academic studies.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来是一些具体的例子。以下是基于学术研究的三个代表性例子。
- en: I. Learning to branch
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: I. 学习分支
- en: 'Perhaps the best-studied example that integrates OA and ML is learning to branch,
    or more specifically, deploying ML to guide branch-and-bound algorithms. For those
    unfamiliar with the concept: **branch-and-bound solves integer problems by systematically
    exploring a very large search tree**, pruning proven suboptimal or unpromising
    branches based on established bounds. If the relaxation of a subproblem does not
    contain the optimal solution, neither will its adjacent integers.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 也许最为研究透彻的例子是集成了OA和ML的分支学习，或者更具体地说，是利用ML来指导分支限界算法。对于那些不熟悉这个概念的人：**分支限界通过系统地探索一个非常大的搜索树来解决整数问题**，根据既定的界限修剪那些已被证明是次优或不太有希望的分支。如果子问题的松弛解不包含最优解，那么它的相邻整数也不会包含。
- en: 'Although branch-and-bound is a renowned algorithm to solve large action spaces
    to optimality, underneath the hood some fairly **rudimentary heuristic choices**
    are made:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管分支限界是一个著名的算法，用于在大动作空间中寻找最优解，但在幕后仍然会做出一些相当**基本的启发式选择**：
- en: '**Variable selection**: *Which variable to branch on next.* A common decision
    rule is to simply branch on the most ‘fractional’ (closest to 0.5) variable.'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**变量选择**：*下一步要分支的变量*。一个常见的决策规则是简单地选择最“接近0.5”的变量进行分支。'
- en: '**Node selection**: *Which of the currently open nodes to process first.* Extremes
    are to always explore the most promising node (best-first) or to always fully
    explore a node’s subtree (depth-first).'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**节点选择**：*首先处理当前开放的节点中的哪个*。极端的做法是总是探索最有希望的节点（最优先）或总是完全探索一个节点的子树（深度优先）。'
- en: '![](../Images/be1c69afb096d40365c90a3a67b1b5af.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/be1c69afb096d40365c90a3a67b1b5af.png)'
- en: Example of branch-and-bound procedure. The choices which node to explore next
    and which variable to branch on are typically based on heuristic rules [image
    from [WikiMedia](https://commons.wikimedia.org/wiki/File:Microsoft_Edge_1_27_2020_1_35_41_AM_(2).png)]
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 分支限界过程的例子。选择下一个要探索的节点和要分支的变量通常基于启发式规则 [图片来自 [WikiMedia](https://commons.wikimedia.org/wiki/File:Microsoft_Edge_1_27_2020_1_35_41_AM_(2).png)]
- en: Truth be told, this simplification does not do justice to more sophisticated
    heuristics developed over the years. Nonetheless, despite the guarantee to ultimately
    find the optimal solution, the actual **search procedure is not as intelligent**
    as one might expect it to be, which is the price paid for solvers being general-purpose.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 说实话，这种简化并未能公正地反映多年来开发出的更复杂的启发式算法。然而，尽管有最终找到最优解决方案的保证，实际**搜索过程并没有像预期的那样智能**，这是通用求解器的代价。
- en: Given that the **tree search is sequential and highly dynamic**, it seems sensible
    to replace static heuristic rules with dynamic learning-based ones. For instance,
    dynamic features of nodes, branches and trees could be designed to predict suitable
    variables and nodes, with their values being learned by ML.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于**树搜索是顺序的且高度动态**，用动态学习基础的规则替代静态启发式规则似乎是合理的。例如，节点、分支和树的动态特征可以设计用来预测合适的变量和节点，其值由机器学习进行学习。
- en: A number of solutions have been proposed in the past years, ranging from offline
    demonstration learning to online experience learning. An intuitive example is
    that of [upper confidence bounds](https://medium.com/towards-data-science/seven-exploration-strategies-in-reinforcement-learning-you-should-know-8eca7dec503b),
    **balancing exploration and exploitation** based on the node’s perceived value
    and the number of time it has been visited.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 过去几年提出了许多解决方案，从离线示范学习到在线经验学习。一个直观的例子是[上置信界](https://medium.com/towards-data-science/seven-exploration-strategies-in-reinforcement-learning-you-should-know-8eca7dec503b)，**基于节点的感知价值和访问次数来平衡探索和利用**。
- en: The example of learning to branch illustrates that ML can be deployed in the
    context of exact algorithms, potentially **speeding up search procedures** **without
    compromising** **optimality guarantees**.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 学习分支的例子说明了机器学习可以在精确算法的背景下应用，可能会**加快搜索过程**，**而不牺牲** **最优性保证**。
- en: Example based on Lodi & Zarpellon (2017) and Balcan et al. (2018)
  id: totrans-70
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 基于 Lodi & Zarpellon (2017) 和 Balcan 等 (2018) 的示例
- en: II. Learning routing with GNN
  id: totrans-71
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: II. 使用 GNN 进行路由学习
- en: Instances for routing problems are commonly represented by graphs. Customer
    locations and travel distances are obvious properties reflected in such graphs,
    yet nodes and arcs can have all kinds of labels, such as time windows and traffic
    densities. Graph Neural Networks (GNNs) are a powerful technique to **embed and
    meaningfully represent such graphs** in the problem context.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 路由问题的实例通常通过图表示。客户位置和旅行距离是这些图中明显的属性，但节点和弧线可以有各种标签，例如时间窗口和交通密度。图神经网络（GNNs）是一种强大的技术，用于**嵌入并有意义地表示这些图**在问题背景中。
- en: '![](../Images/b21c261c272dd47e01255bda6b1390d5.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b21c261c272dd47e01255bda6b1390d5.png)'
- en: The vehicle routing problem is a classical combinatorial optimization problem,
    exploding exponentially in size with the number of nodes [image from [WikiMedia](https://en.wikipedia.org/wiki/Vehicle_routing_problem#/media/File:Figure_illustrating_the_vehicle_routing_problem.png)]
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 车辆路由问题是一个经典的组合优化问题，随着节点数量的增加，规模呈指数爆炸增长 [图片来自 [WikiMedia](https://en.wikipedia.org/wiki/Vehicle_routing_problem#/media/File:Figure_illustrating_the_vehicle_routing_problem.png)]
- en: Traditionally, routes are generated using exact solvers (e.g., the Concorde
    TSP Solver) or metaheuristics (e.g., Hybrid Genetic Search). However, these solvers
    require fairly **long runtimes**, with exact algorithms failing to offer solutions
    at all for large instances.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 传统上，路线是使用精确求解器（例如 Concorde TSP Solver）或元启发式算法（例如混合遗传搜索）生成的。然而，这些求解器需要相当**长的运行时间**，精确算法在面对大型实例时完全无法提供解决方案。
- en: Instead of running optimization algorithms for hours and hours every time we
    encounter a new problem instance, we might learn to replicate them by detecting
    the patterns in their solutions. This example is a form of **demonstration learning**,
    trying to minimize the performance gap between the computationally-intensive optimal
    solutions and the ML-generated solutions. Some quality loss is incurred in the
    process, but solutions can be generated in a much friendlier timeframe.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 与其每次遇到新的问题实例时运行优化算法数小时，不如通过检测其解决方案中的模式来学习复制它们。这个例子是一种**示范学习**，试图最小化计算密集型最优解决方案与机器学习生成的解决方案之间的性能差距。在这个过程中会有一定的质量损失，但解决方案可以在更友好的时间框架内生成。
- en: By running GNNs on the solutions, it is possible to estimate the **probabilities
    that edges will be included in the solutions**. Subsequently, a sparse heatmap
    is constructed that retains the most promising edges. Guided by this heatmap,
    a dynamic programming subroutine is run for stepwise construction of the routes.
    If a route is dominated by others, it can be pruned.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 通过对解决方案运行GNNs，可以估计**边被纳入解决方案的概率**。随后，构建一个稀疏的热图，保留最有前景的边。根据这个热图，运行一个动态规划子程序进行逐步构建路线。如果某条路线被其他路线主导，可以进行剪枝。
- en: Example based on Kool et al. (2019) and Kool et al. (2022)
  id: totrans-78
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 基于Kool等人（2019）和Kool等人（2022）的示例
- en: III. Mathematical programming for large action spaces
  id: totrans-79
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: III. 大动作空间的数学编程
- en: In this final case, mathematical programming is deployed as a subroutine in
    reinforcement learning, with the purpose of handling large action spaces.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个最终案例中，数学编程作为强化学习中的一个子程序被部署，目的是处理大动作空间。
- en: Combinatorial optimization problems have the tendency to very quickly **explode
    in size**, e.g., the number of sequences to visit a set of nodes is the factorial
    of the nodes. They often require handling **many constraints**, such as the capacity
    and maximum driving time of a truck or time windows at the customer nodes. Enumerating
    all actions and running feasibility checks becomes cumbersome very quickly.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 组合优化问题有迅速**规模爆炸**的趋势，例如，访问一组节点的序列数是节点的阶乘。它们通常需要处理**许多约束**，如卡车的容量和最大驾驶时间或客户节点的时间窗口。枚举所有动作并进行可行性检查变得非常繁琐。
- en: If the problem structure allows, the **one-step decision problem can be formulated
    as a linear program**. Such programs can be solved very efficiently, vastly scaling
    up the size of action spaces that vanilla RL algorithms — which rely on explicit
    enumeration — could handle.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 如果问题结构允许，**一阶决策问题可以被形式化为线性规划**。这种程序可以非常高效地解决，大大扩展了传统RL算法——依赖于明确枚举——所能处理的动作空间大小。
- en: '![](../Images/e86df850a3efd84b0ca5578b4ab7acb8.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e86df850a3efd84b0ca5578b4ab7acb8.png)'
- en: Linear programming can efficiently explore large action spaces with constraints
    [image via [WikiMedia](https://commons.wikimedia.org/wiki/File:Linear_optimization_in_a_2-dimensional_polytope.svg)]
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 线性规划可以高效地探索带约束的大动作空间 [图片来源 [WikiMedia](https://commons.wikimedia.org/wiki/File:Linear_optimization_in_a_2-dimensional_polytope.svg)]
- en: The integration between OA and ML does not end there. Next, consider the approximation
    of Q-values. In many modern RL applications, this is done through a[**Deep Q-network**](https://medium.com/towards-data-science/a-minimal-working-example-for-deep-q-learning-in-tensorflow-2-0-e0ca8a944d5e),
    which by definition perform a nonlinear transformation on its input. However,
    ReLU activation functions can be embedded in the objective function through piecewise
    linear functions, supported by a set of additional constraints.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: OA与ML的整合并未止步于此。接下来，考虑Q值的近似。在许多现代RL应用中，这通常通过[**深度Q网络**](https://medium.com/towards-data-science/a-minimal-working-example-for-deep-q-learning-in-tensorflow-2-0-e0ca8a944d5e)来完成，该网络通过定义在其输入上执行非线性变换。然而，ReLU激活函数可以通过分段线性函数嵌入到目标函数中，支持一组附加约束。
- en: Another OA layer? Instead of searching the full action space, we may impose
    domain restrictions and **local branching** to control the action space, exploring
    only the neighborhood in which we suspect (based on Q-values) the best action
    resides.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个OA层？我们可以施加领域限制和**局部分支**来控制动作空间，仅探索我们怀疑（基于Q值）最佳动作所在的邻域。
- en: Example based on Van Heeswijk, W.J.A. & La Poutré (2020) and Akkerman et al.
    (2023)
  id: totrans-87
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 基于Van Heeswijk, W.J.A. & La Poutré（2020）和Akkerman等人（2023）的示例
- en: As you see, the opportunities for integrating machine learning and optimization
    algorithms are vast and diverse. It has been an active research area for years,
    with lots of potential yet to be unlocked.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所见，机器学习与优化算法的整合机会广阔而多样。这已经是一个活跃的研究领域，潜力尚未完全释放。
- en: '**TL;DR**'
  id: totrans-89
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**总结**'
- en: Both machine learning and optimization algorithms have flaws in terms of computational
    times, performance guarantees, knowledge utilization, etc. Given the nature of
    both paradigms, they may naturally complement each other.
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器学习和优化算法在计算时间、性能保证、知识利用等方面都有缺陷。鉴于这两种范式的性质，它们可能自然互补。
- en: Machine learning specializes in **pattern detection**, optimization algorithms
    specialize in pattern **exploitation**. They can be combined to build powerful
    algorithmic pipelines.
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器学习专注于**模式检测**，优化算法专注于模式**利用**。它们可以结合起来构建强大的算法管道。
- en: In practice, companies often repeatedly solve a single problem with relatively
    limited variance. **Extracting patterns from solution data** (i.e., statistical
    learning)can boost existing exact methods or heuristics.
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在实践中，公司通常会重复解决单一问题，并且这些问题的变异性相对有限。**从解决方案数据中提取模式**（即统计学习）可以提升现有的精确方法或启发式方法。
- en: Machine learning can be used in the form of **demonstration learning** (approximating
    solutions of optimization algorithms in a fraction of the time) or **experience
    learning** (interacting with the environment to iteratively enhance policies).
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器学习可以以**示范学习**的形式使用（在极短时间内近似优化算法的解决方案）或**经验学习**的形式使用（与环境互动以迭代地增强策略）。
- en: '*You may also like the following articles:*'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '*你可能还会喜欢以下文章：*'
- en: '[](/using-linear-programming-to-boost-your-reinforcement-learning-algorithms-994977665902?source=post_page-----e6c680454f06--------------------------------)
    [## Using Linear Programming to Boost Your Reinforcement Learning Algorithms'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/using-linear-programming-to-boost-your-reinforcement-learning-algorithms-994977665902?source=post_page-----e6c680454f06--------------------------------)
    [## 使用线性规划来提升强化学习算法'
- en: Large and high-dimensional action spaces are often computational bottlenecks
    in Reinforcement Learning. Formulating…
  id: totrans-96
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 在强化学习中，大规模和高维的动作空间通常是计算瓶颈。制定……
- en: towardsdatascience.com](/using-linear-programming-to-boost-your-reinforcement-learning-algorithms-994977665902?source=post_page-----e6c680454f06--------------------------------)
    [](/five-ways-to-handle-large-action-spaces-in-reinforcement-learning-8ba6b6ca7472?source=post_page-----e6c680454f06--------------------------------)
    [## Five Ways To Handle Large Action Spaces in Reinforcement Learning
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/using-linear-programming-to-boost-your-reinforcement-learning-algorithms-994977665902?source=post_page-----e6c680454f06--------------------------------)
    [](/five-ways-to-handle-large-action-spaces-in-reinforcement-learning-8ba6b6ca7472?source=post_page-----e6c680454f06--------------------------------)
    [## 五种处理强化学习中大规模动作空间的方法
- en: Action spaces, particularly in combinatorial optimization problems, may grow
    unwieldy in size. This article discusses…
  id: totrans-98
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 动作空间，特别是在组合优化问题中，可能会变得庞大不便。本文讨论了……
- en: towardsdatascience.com](/five-ways-to-handle-large-action-spaces-in-reinforcement-learning-8ba6b6ca7472?source=post_page-----e6c680454f06--------------------------------)
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/five-ways-to-handle-large-action-spaces-in-reinforcement-learning-8ba6b6ca7472?source=post_page-----e6c680454f06--------------------------------)
- en: '*For those interested in machine learning for combinatorial optimization, I
    can warmly recommend the following keynote speech by Prof. Andrea Lodi:*'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '*对于那些对组合优化中的机器学习感兴趣的人，我热情推荐以下由Andrea Lodi教授做的主题演讲：*'
- en: References
  id: totrans-101
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: Akkerman, F., Luy, J., Van Heeswijk, W.J.A., & Schiffer, M. (2023). Handling
    Large Discrete Action Spaces via Dynamic Neighborhood Construction. *arXiv preprint
    arXiv:2305.19891*.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: Akkerman, F., Luy, J., Van Heeswijk, W.J.A., & Schiffer, M. (2023). 通过动态邻域构造处理大规模离散动作空间。*arXiv预印本arXiv:2305.19891*。
- en: Balcan, M. F., Dick, T., Sandholm, T., & Vitercik, E. (2018, July). Learning
    to branch. In *International Conference on Machine Learning* (pp. 344–353). PMLR.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: Balcan, M. F., Dick, T., Sandholm, T., & Vitercik, E. (2018年7月). 学习分支。见 *国际机器学习会议*
    （第344–353页）。PMLR.
- en: Khalil, E., Le Bodic, P., Song, L., Nemhauser, G., & Dilkina, B. (2016, February).
    Learning to branch in mixed integer programming. In *Proceedings of the AAAI Conference
    on Artificial Intelligence* (Vol. 30, №1).
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: Khalil, E., Le Bodic, P., Song, L., Nemhauser, G., & Dilkina, B. (2016年2月).
    在混合整数规划中学习分支。见 *人工智能协会会议论文集*（第30卷，第1号）。
- en: Kool, W., Van Hoof, H., & Welling, M. (2019). Attention, learn to solve routing
    problems!. International Conference on Learning Representations 2019.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: Kool, W., Van Hoof, H., & Welling, M. (2019). 注意，学习解决路线问题！国际学习表示大会2019。
- en: 'Kool, W., Van Hoof, H., Gromicho, J., & Welling, M. (2022, June). Deep policy
    dynamic programming for vehicle routing problems. In *International conference
    on integration of constraint programming, artificial intelligence, and operations
    research* (pp. 190–213). Cham: Springer International Publishing.'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 'Kool, W., Van Hoof, H., Gromicho, J., & Welling, M. (2022年6月). 针对车辆路线问题的深度策略动态规划。在
    *国际约束编程、人工智能和运筹学整合会议* （第190–213页）。Cham: Springer International Publishing.'
- en: 'Lodi, A., & Zarpellon, G. (2017). On learning and branching: a survey. *Top*,
    *25*, 207–236.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: Lodi, A., & Zarpellon, G. (2017). 关于学习和分支的综述。*Top*, *25*, 207–236.
- en: Parmentier, A., & T’kindt, V. (2023). Structured learning based heuristics to
    solve the single machine scheduling problem with release times and sum of completion
    times. *European Journal of Operational Research*, *305*(3), 1032–1041.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: Parmentier, A. 和 T’kindt, V.（2023）。基于结构学习的启发式方法解决具有释放时间和完成时间总和的单台机器调度问题。*欧洲运筹学期刊*，*305*(3)，1032–1041。
- en: 'Santana, Í., Lodi, A., & Vidal, T. (2023, May). Neural Networks for Local Search
    and Crossover in Vehicle Routing: A Possible Overkill?. In *International Conference
    on Integration of Constraint Programming, Artificial Intelligence, and Operations
    Research* (pp. 184–199). Cham: Springer Nature Switzerland.'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: Santana, Í., Lodi, A., 和 Vidal, T.（2023年5月）。车辆路径中的局部搜索和交叉的神经网络：可能的过度杀伤？在*约束编程、人工智能与运筹学集成国际会议*（第184–199页）。Cham：Springer
    Nature Switzerland。
- en: Van Heeswijk, W.J.A. van & La Poutré, H.L. (2020, December). Deep reinforcement
    learning in linear discrete action spaces. In *2020 Winter Simulation Conference
    (WSC)* (pp. 1063–1074). IEEE.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: Van Heeswijk, W.J.A. van 和 La Poutré, H.L.（2020年12月）。线性离散动作空间中的深度强化学习。在*2020年冬季模拟会议（WSC）*（第1063–1074页）。IEEE。
- en: Vazacopoulos, A. (2020). Combining machine learning and mathematical optimization
    integration using Python. Via [LinkedIn](https://www.linkedin.com/pulse/combining-machine-learning-mathematical-optimization-vazacopoulos/).
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: Vazacopoulos, A.（2020）。使用 Python 结合机器学习和数学优化集成。通过 [LinkedIn](https://www.linkedin.com/pulse/combining-machine-learning-mathematical-optimization-vazacopoulos/)。
