- en: What does Entropy Measure? An Intuitive Explanation
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 熵度量了什么？直观解释
- en: 原文：[https://towardsdatascience.com/what-does-entropy-measure-an-intuitive-explanation-a7f7e5d16421](https://towardsdatascience.com/what-does-entropy-measure-an-intuitive-explanation-a7f7e5d16421)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/what-does-entropy-measure-an-intuitive-explanation-a7f7e5d16421](https://towardsdatascience.com/what-does-entropy-measure-an-intuitive-explanation-a7f7e5d16421)
- en: '[](https://tim-lou.medium.com/?source=post_page-----a7f7e5d16421--------------------------------)[![Tim
    Lou, PhD](../Images/e4931bb6d59e27730529ceaf00a23822.png)](https://tim-lou.medium.com/?source=post_page-----a7f7e5d16421--------------------------------)[](https://towardsdatascience.com/?source=post_page-----a7f7e5d16421--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----a7f7e5d16421--------------------------------)
    [Tim Lou, PhD](https://tim-lou.medium.com/?source=post_page-----a7f7e5d16421--------------------------------)'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://tim-lou.medium.com/?source=post_page-----a7f7e5d16421--------------------------------)[![Tim
    Lou, PhD](../Images/e4931bb6d59e27730529ceaf00a23822.png)](https://tim-lou.medium.com/?source=post_page-----a7f7e5d16421--------------------------------)[](https://towardsdatascience.com/?source=post_page-----a7f7e5d16421--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----a7f7e5d16421--------------------------------)
    [Tim Lou, PhD](https://tim-lou.medium.com/?source=post_page-----a7f7e5d16421--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----a7f7e5d16421--------------------------------)
    ·11 min read·Jan 4, 2023
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----a7f7e5d16421--------------------------------)
    ·阅读时间 11 分钟·2023 年 1 月 4 日
- en: --
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: 'Entropy may seem abstract, but it has an intuitive side: as the probability
    of seeing certain patterns in data. Here’s how it works.'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 熵可能看起来很抽象，但它有一个直观的方面：即看到数据中某些模式的概率。它是如何工作的呢？
- en: '![](../Images/4daae7bdad8d3ebc360bb1ef70760dd8.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4daae7bdad8d3ebc360bb1ef70760dd8.png)'
- en: 'Background Credit: Joe Maldonado [@unsplash](https://unsplash.com/@joesracingteam)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 背景来源：Joe Maldonado [@unsplash](https://unsplash.com/@joesracingteam)
- en: 'In data science, there are many concepts linked to the notion of entropy. The
    most basic one is Shannon’s information entropy, defined for any distribution,
    *P*(*x*), through the formula:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据科学中，有许多与熵的概念相关的概念。最基本的是香农信息熵，通过公式定义在任意分布 *P*(*x*) 中：
- en: '![](../Images/a8588251ff381e4720e63406deb3205a.png)'
  id: totrans-9
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a8588251ff381e4720e63406deb3205a.png)'
- en: Where the sum is over all the possible categories in *C*.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 其中求和是对所有可能的类别 *C* 进行的。
- en: 'There are other related concepts that have similarly looking formulae:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 还有其他相关的概念，其公式类似：
- en: '[Kullback–Leibler divergence](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence):
    for comparing two distributions'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Kullback–Leibler 散度](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence)：用于比较两个分布'
- en: '[Mutual information](https://en.wikipedia.org/wiki/Mutual_information): for
    capturing general relationships between two variables'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[互信息](https://en.wikipedia.org/wiki/Mutual_information)：用于捕捉两个变量之间的一般关系'
- en: '[Cross entropy](https://en.wikipedia.org/wiki/Cross_entropy): for training
    classification models'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[交叉熵](https://en.wikipedia.org/wiki/Cross_entropy)：用于训练分类模型'
- en: 'Despite the ubiquity of entropy-like formulae, there are rarely discussions
    on the intuitions behind the formula: Why is the logarithm involved? Why are we
    multiplying *P*(*x*) and log *P*(*x*)? While many articles mention terms like
    “information”, “expected surprise”, the intuitions behind them are missing.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管熵类似的公式很普遍，但关于公式背后的直觉讨论却很少：为什么涉及对数？为什么我们要将 *P*(*x*) 和 log *P*(*x*) 相乘？虽然许多文章提到“信息”、“期望惊讶”等术语，但其背后的直觉却缺失。
- en: It turns out, just like probabilities, entropy can be understood through a counting
    exercise, and it can be linked to a sort of log-likelihood for distributions.
    Furthermore, this counting can be linked to the literal number of bytes in a computer.
    These interpretations will enable us to demystify the many facts about entropy.
    Curious? Let’s get started!
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 事实证明，正如概率一样，熵可以通过计数练习来理解，并且可以与分布的某种对数似然性联系起来。此外，这种计数还可以与计算机中的字节数字面上的数量相关联。这些解释将使我们能够揭开熵的许多事实。好奇吗？那就开始吧！
- en: Counting Entropy
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 计数熵
- en: '![](../Images/d167237368b07ac265e675b3f71352a8.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d167237368b07ac265e675b3f71352a8.png)'
- en: Probability’s counting definition is what makes it so intuitive (Photo by [Ibrahim
    Rifath](https://unsplash.com/@photoripey?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral))
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 概率的计数定义使得它变得如此直观（照片由 [Ibrahim Rifath](https://unsplash.com/@photoripey?utm_source=medium&utm_medium=referral)
    提供，来源于 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)）
- en: 'Probability can be defined operationally: When we say a coin has a 50% chance
    to land a head, it means that if we were to flip the coin a million times, the
    number of heads will get pretty close to half a million. This fraction will get
    closer to the 50% probability as the number of trials increase. This definition
    is what makes probabilities so intuitive.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 概率可以从操作上定义：当我们说一枚硬币有50%的机会落到正面时，这意味着如果我们将硬币抛掷一百万次，正面的次数将接近五十万。随着试验次数的增加，这一比例会越来越接近50%的概率。这一定义使得概率变得如此直观。
- en: 'Is there a similar interpretation for entropy? There is, although the counting
    is a little bit tricker: it will require some basic combinatorics.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 熵是否有类似的解释？有的，不过计数会稍微复杂一些：这将需要一些基础的组合数学。
- en: 'How many ways are there to arrange *N* different balls? There are *N* choices
    for the first one, *N* − 1 for the second one… etc. The answer is *N*!, or the
    factorial symbol:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 有多少种方法可以排列 *N* 个不同的球？第一个球有 *N* 种选择，第二个球有 *N* − 1 种选择……等等。答案是 *N*!，或者阶乘符号：
- en: '![](../Images/344ab819c57f5948cf5f1ea8dbec506d.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/344ab819c57f5948cf5f1ea8dbec506d.png)'
- en: 'Just like in the definition of probabilities, we’ll be working with very large
    numbers. So it is helpful to approximate this object through [Sterling’s approximation](https://en.wikipedia.org/wiki/Stirling%27s_approximation):'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 就像在概率的定义中，我们将处理非常大的数字。因此，通过 [斯特林近似](https://en.wikipedia.org/wiki/Stirling%27s_approximation)
    来近似这个对象是有帮助的：
- en: '![](../Images/3f6508c3690964cbfbdaba2108afa70a.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3f6508c3690964cbfbdaba2108afa70a.png)'
- en: Where log indicates the natural logarithm; analogous formulae also exist if
    we use alternative bases like log₂ and log₁₀ (this will determine the units in
    which we measure entropy). The big-O notation indicates the validity of the approximation
    as *N* gets large. The term *N* log *N* will be the origin of *p* log *p* in entropy’s
    definition.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 log 表示自然对数；如果我们使用其他基数如 log₂ 和 log₁₀，也存在类似的公式（这将决定我们测量熵的单位）。大O符号表明近似在 *N*
    很大时的有效性。术语 *N* log *N* 将成为熵定义中 *p* log *p* 的来源。
- en: We are now ready to derive what entropy is counting. Imagine there is a large
    number of distinguishable objects, or distinguishable data points. These *N* data
    points are grouped into say *c* categories, like in the figure below
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在准备推导熵所计数的内容。想象有大量可区分的对象或可区分的数据点。这些 *N* 数据点被分组为 *c* 个类别，如下图所示：
- en: '![](../Images/921883f6e44aed334b172acfe55f503c.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/921883f6e44aed334b172acfe55f503c.png)'
- en: 'What is the total number of ways this can be done? Keeping in mind that we
    don’t care about the ordering of our data in any category, the answer is captured
    by the classic multinomial coefficients:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 这种情况的总排列方式是多少？记住我们不关心任何类别中数据的排序，答案由经典的多项式系数给出：
- en: '![](../Images/620970d66f7f69c551ca919e9ef76f60.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/620970d66f7f69c551ca919e9ef76f60.png)'
- en: Where we have used the Ω symbol to denote the number of configurations.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 我们用 Ω 符号来表示配置的数量。
- en: 'Just like the case for probability, we are only interested in the large *N*
    behaviors. When dealing with such large numbers, it is helpful to take the logarithm,
    so we can use Sterling’s approximation to make things more manageable:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 就像概率的情况一样，我们只对大的 *N* 行为感兴趣。当处理如此大的数字时，取对数是有帮助的，这样我们可以使用斯特林近似来使事情更可控：
- en: '![](../Images/1206496dc3bce7cf30dd9693b6719e8a.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1206496dc3bce7cf30dd9693b6719e8a.png)'
- en: The formula can be simplified by utilizing the fact that the sum of all the
    *nᵢ* equals *N*,
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 公式可以通过利用所有 *nᵢ* 总和等于 *N* 的事实来简化，
- en: '![](../Images/da8eae2a138667b729bad6985415f186.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/da8eae2a138667b729bad6985415f186.png)'
- en: 'if we substitute *nᵢ*/*N* → *P*(*i*), we get exactly the entropy formula. Alternatively,
    we can write (for large *N*):'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们将 *nᵢ*/*N* → *P*(*i*) 代入，我们就得到了熵的公式。或者，对于大的 *N*，我们可以写成：
- en: '![](../Images/5736092920f8617b70665370c19d02b1.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5736092920f8617b70665370c19d02b1.png)'
- en: 'So we arrived at an operational definition of entropy:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们达到了熵的操作定义：
- en: 'Entropy counts the # of ways of categorizing large amounts of data that resemble
    a given probability distribution (in logarithmic units and per number of data
    point)'
  id: totrans-39
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 熵计算了将大量数据分类成给定概率分布的方式数（以对数单位和每个数据点计算）。
- en: This counting exercise lies at the heart of information theory, which we’ll
    turn to next.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 这个计数练习是信息理论的核心，我们接下来会讨论它。
- en: Entropy as Information
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 熵作为信息
- en: So how does our concept of entropy relate to literal bits of 0s and 1s in a
    computer?
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，我们的熵概念如何与计算机中的字面0和1比特相关呢？
- en: '![](../Images/eb3f9ddc43602f73e10cc1f8d7bc3c23.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/eb3f9ddc43602f73e10cc1f8d7bc3c23.png)'
- en: 'The concept of entropy can linked to information when we apply it to patterns
    of 1s and 0s (credit: [Gerd Altmann](https://pixabay.com/users/geralt-9301/))'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们将熵应用于1和0的模式时，熵的概念可以与信息相关（信用：[Gerd Altmann](https://pixabay.com/users/geralt-9301/)）。
- en: 'Imagine a binary sequence of some fix length *N*. Intuitively we know that
    it contains *N* bits of information: because it literally takes *N* bits to store
    the sequence in a hard-drive or memory.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一个固定长度为*N*的二进制序列。直观上我们知道它包含*N*比特的信息：因为实际上需要*N*比特来存储序列到硬盘或内存中。
- en: But what if the sequence has some interesting patterns like the following?
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 但如果序列有一些有趣的模式，例如下面的呢？
- en: '000000000000000000000000000'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '000000000000000000000000000'
- en: '010101010101010101010101010'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '010101010101010101010101010'
- en: '000000010000000000000000000'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '000000010000000000000000000'
- en: 'In these cases, the binary sequence representation would be very inefficient.
    We intuitively know that there are more efficient ways to store these sequence:
    we can specify the patterns as opposed to all the literal bits, and the amount
    of meaningful information in these sequences should be smaller.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些情况下，二进制序列表示会非常低效。我们直观地知道有更高效的存储方式：我们可以指定模式，而不是所有的字面比特，这些序列中的有效信息量应该更小。
- en: So if we ignore the subtle patterns of digit repetitions, and just look at the
    basic statistical properties of the digits (proportions of 0s and 1s), how much
    better can we do in terms of storing those sequences?
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，如果我们忽略数字重复的微妙模式，只看数字的基本统计属性（0和1的比例），在存储这些序列时我们能做得更好吗？
- en: 'This is where our entropy counting formula can help us: it can count the total
    number of sequence given some fixed proportions of 0s and 1s.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们的熵计数公式可以帮助我们的地方：它可以计算给定固定比例的0和1的总序列数。
- en: 'In the case where the proportions of 0s and 1s are 50/50, the total number
    of possibilities is (in the large *N* limit):'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在0和1的比例为50/50的情况下，总的可能性数量是（在大的*N*极限下）：
- en: '![](../Images/1937ff9ce3a6eafbcb7d21f4a9b01a56.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1937ff9ce3a6eafbcb7d21f4a9b01a56.png)'
- en: 'We see that this just approximately yields the number of all the possible binary
    sequences 2*ᴺ.* So the number of bits required to store the sequence is still
    *N*. This isn’t surprising, as we know that random sequences should be impossible
    to compress: it has the maximum *N* bits of information.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到这大致上产生了所有可能的二进制序列的数量2*ᴺ*。所以存储序列所需的比特数仍然是*N*。这并不令人惊讶，因为我们知道随机序列应该无法压缩：它具有最大*N*比特的信息。
- en: 'But what if the proportions are no longer 50/50? We should expect some potential
    savings. In this case, the total number of bits required to store one sequence
    would be:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 但如果比例不再是50/50呢？我们应该期望有一些潜在的节省。在这种情况下，存储一个序列所需的比特总数将是：
- en: '![](../Images/749c4409803ad01dca62ec7a2f948886.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/749c4409803ad01dca62ec7a2f948886.png)'
- en: 'Let’s sanity check the case when the number of 0s is much smaller than the
    number of 1s, say *n* ≪ *N*. In this case the *P*₁ term can be ignored, and the
    number of bits is given by:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们 sanity check 当0的数量远小于1的数量时的情况，比如*n* ≪ *N*。在这种情况下，可以忽略*P*₁项，所需的比特数为：
- en: '![](../Images/635d4229fdbbfed25583b61af4459693.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/635d4229fdbbfed25583b61af4459693.png)'
- en: So the amount of information is proportional to *n* instead of *N.* This is
    because we now only need to store the position of each 0 instead of the whole
    sequence.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，信息量与*n*成正比，而不是*N*。这是因为我们现在只需要存储每个0的位置，而不是整个序列。
- en: This illustrate the power of entropy as it relates to physical bits and bytes
    in a computer. In summary,
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 这说明了熵与计算机中物理比特和字节相关的力量。总之，
- en: The information entropy specifies the expected number of bit per length that
    is required to store a sequence generated by a given probability distribution.
  id: totrans-62
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 信息熵指定了每单位长度所需的比特数，用于存储由给定概率分布生成的序列。
- en: In other words, entropy is a sort of optimal compression ratio for a fixed proportion
    of characters in a sequence. This is the way that entropy is linked to information.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，熵是一种针对序列中固定比例字符的*最佳*压缩比。这是熵与信息相关联的方式。
- en: Beyond thinking of sequence as our subject of interest, we can turn our attention
    to the distributions themselves. This view point allows us to interpret entropy
    as a sort of probability (or log-likelihood).
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 除了将序列视为我们的研究对象，我们还可以将注意力转向分布本身。这种观点使我们能够将熵解释为一种概率（或对数似然）。
- en: Entropy as Log-Likelihood
  id: totrans-65
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 熵作为对数似然
- en: Entropy counts the number of possibilities. We want to convert this to a sort
    of probability. To do this we simply need to normalize the counts.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 熵计算的是可能性的数量。我们想将其转换为一种概率。为此，我们只需对计数进行归一化。
- en: 'What is the total number of ways to categorize *N* data points into *c* categories?
    The answer is simple, because each data point has *c* choices:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 将 *N* 个数据点分为 *c* 类别的总方法数是多少？答案很简单，因为每个数据点有 *c* 种选择：
- en: '![](../Images/05935d2b317cfd14412fca4652c4ceca.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/05935d2b317cfd14412fca4652c4ceca.png)'
- en: 'We can now divide the entropy’s count by the total to obtain a probability
    (substituting *nᵢ*/*N* → *P*(*i*)):'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以将熵的计数除以总数以获得概率（替换 *nᵢ*/*N* → *P*(*i*)）：
- en: '![](../Images/84eb6451eccba793aacbcb92ccba2105.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/84eb6451eccba793aacbcb92ccba2105.png)'
- en: 'This way, entropy becomes the probability (asymptotic because of large *N*)
    of observing a particular distribution from randomly categorizing data points:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 这样，熵变成了观察到特定分布的概率（由于 *N* 很大，趋近于最终值）：
- en: Entropy can be viewed as the log-likelihood of observing a given distribution
    (per data point)
  id: totrans-72
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 熵可以视为观察给定分布的对数似然（每个数据点）。
- en: There is a hidden assumption in our discussion though, as we are treating every
    configuration as equally likely in our calculations. What happens if some categories
    are more favored over others?
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我们的讨论中存在一个隐含的假设，即我们在计算中将每种配置视为同样可能。如果某些类别比其他类别更受青睐会发生什么？
- en: 'We can consider some reference distribution *Q*(*x*). If each data point has
    a chance *Q*(*x*) to be in a particular category *x*, the probability of observing
    *n*₁ in category 1, *n*₂ in category 2 and so on is given by the multinomial probability:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以考虑一些参考分布 *Q*(*x*)。如果每个数据点有 *Q*(*x*) 的机会进入特定类别 *x*，那么观察到类别1中的 *n*₁、类别2中的
    *n*₂ 等的概率由多项式概率给出：
- en: '![](../Images/ed03a064964e21cd2f066d52f36d2090.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ed03a064964e21cd2f066d52f36d2090.png)'
- en: Once again, we can go through Sterling’s approximation. The calculation is very
    similar to the previous ones, except we have a extra *Q*(*i*) in the end
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以再次使用斯特林近似。计算过程与之前类似，只是最后多了一个 *Q*(*i*)。
- en: '![](../Images/f838ff7e0bc302f2cb063fe56450c4c1.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f838ff7e0bc302f2cb063fe56450c4c1.png)'
- en: Substituting *nᵢ*/*N* → *P*(*i*), the term inside the exponential becomes the
    [Kullback–Leibler divergence](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence).
    So our equation can be summarized as
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 替换 *nᵢ*/*N* → *P*(*i*)，指数内的项变成了 [Kullback–Leibler 散度](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence)。因此，我们的方程可以总结为
- en: '![](../Images/be46304139b1eb70476e9d8888b08426.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/be46304139b1eb70476e9d8888b08426.png)'
- en: 'Where we have used the common notation of KL-divergence inside the exponential.
    The KL-divergence is a generalization of Shannon’s information entropy, and our
    equations make our interpretation even more precise:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们使用了指数内的 KL-散度的常见表示法。KL-散度是香农信息熵的推广，我们的方程使我们的解释更加精准：
- en: The Kullback-Leibler divergence of P on Q is the negative log-likelihood (per
    data point) of observing P when sampling data according to Q
  id: totrans-81
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: P 相对于 Q 的 Kullback-Leibler 散度是观察到 P 时按照 Q 采样的负对数似然（每个数据点）。
- en: Once again, all this is assuming *N* is very large.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 再次说明，所有这些都是假设 *N* 非常大。
- en: 'A few facts about KL-divergence now become obvious:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 关于 KL-散度的一些事实现在变得明显：
- en: 'KL-divergence is always non-negative: this is because a probability can never
    be larger than 1.'
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: KL-散度始终非负：这是因为概率永远不会大于 1。
- en: 'KL-divergence can be infinite: this happens when two distributions have no
    overlap, and so the counting exercise yields 0 = exp[–∞].'
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: KL-散度可以是无限的：这发生在两个分布没有重叠时，因此计数的结果为 0 = exp[–∞]。
- en: 'KL-divergence is zero if and only if *P* = *Q*: when we sample data according
    to *Q*, we expect the resulting distributions to look like *Q —* this expectation
    is exact at large *N*.'
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: KL-散度为零当且仅当 *P* = *Q*：当我们按照 *Q* 采样数据时，我们期望结果分布类似于 *Q*——这种期望在 *N* 很大时是准确的。
- en: Armed with this new understanding, we are now ready to reinterpret facts about
    various entropic concepts in data science!
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 凭借这种新的理解，我们现在准备重新解释数据科学中各种熵概念的事实！
- en: An Entropic Sampler
  id: totrans-88
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 熵采样器
- en: Below we’ll discuss the intuitions behind some common entropy-like variables
    in data science. We’ll once again remind the reader that the large *N* limit is
    implicitly assumed.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 下面我们将讨论数据科学中一些常见的类似熵的变量的直觉。我们将再次提醒读者，大* N*极限是隐含假设的。
- en: Cross Entropy
  id: totrans-90
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 交叉熵
- en: This is useful for training categorical variables. It is defined as
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 这对训练分类变量很有用。定义为
- en: '![](../Images/a8485c1c02e027374c126a6355c55ec3.png)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a8485c1c02e027374c126a6355c55ec3.png)'
- en: Note that we’ve rewritten the definition as a sum of the KL-divergence and Shannon’s
    information entropy. This may look a little unfamiliar, since when we train machine
    learning model, we only compute an estimate of it through our samples (say *S*)
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们已经将定义重写为KL散度和香农信息熵的和。这可能看起来有点不熟悉，因为当我们训练机器学习模型时，我们仅通过样本（比如*S*）计算其估计值
- en: '![](../Images/868e4a17287e8aae05b7ce6098e40899.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/868e4a17287e8aae05b7ce6098e40899.png)'
- en: Using our counting intuition, we conclude that
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 利用我们的计数直觉，我们得出结论
- en: Minimizing cross-entropy is equivalent to maximizing the log-likelihood of observing
    the same statistics as those from our sampled data, if we sample our data from
    a distribution Q that is being trained
  id: totrans-96
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 最小化交叉熵等同于最大化观察到与我们从分布Q中采样的数据统计相同的对数似然性，如果我们从训练中的分布Q中采样数据
- en: 'This puts the cross-entropy loss on a similar conceptual ground as the L2 loss
    in regressions: they are both some sort of log-likelihood functions.'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 这使得交叉熵损失与回归中的L2损失处于类似的概念基础上：它们都是某种对数似然函数。
- en: Mutual Information
  id: totrans-98
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 互信息
- en: Mutual information can be thought of as a generalized sort of correlation between
    two variables. Denoted by *I*, it is defined through the KL-divergence
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 互信息可以被视为两个变量之间的一种广义相关性。用*I*表示，它通过KL散度定义
- en: '![](../Images/53ddc49f96d4672862b76e741c435412.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/53ddc49f96d4672862b76e741c435412.png)'
- en: Where in the KL-divergence computation, we are comparing a distribution of two
    variables, against the distribution of considering each variable separately.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在KL散度计算中，我们比较两个变量的分布与考虑每个变量单独分布的分布。
- en: 'Our counting intuitions give us a very nice interpretation:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的计数直觉给出了一个很好的解释：
- en: Mutual information is the negative log-likelihood (per data point) of obtaining
    a given distribution on two variables, when we sample the two variables independently
    based on their marginalized distributions
  id: totrans-103
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 互信息是获得两个变量给定分布的负对数似然性（每个数据点），当我们根据其边际分布独立地采样这两个变量时
- en: This explains why mutual information is a powerful tool that can capture nonlinear
    correlations between variables.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 这解释了为什么互信息是一个强大的工具，可以捕捉变量之间的非线性相关性。
- en: The Inevitable Increase In Entropy?
  id: totrans-105
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 熵的不可避免增加？
- en: 'Finally, we are ready to address one of the most well-known facts about entropy:
    the laws of thermodynamics and the inevitable increase in entropy.'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们准备讨论关于熵的最著名事实之一：热力学定律和熵的不可避免增加。
- en: 'It is important to keep in mind though that there are two concepts of entropy
    at play here:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 但需要记住的是，这里存在两个熵的概念：
- en: Shannon’s Information Entropy in Data Science
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 数据科学中的香农信息熵
- en: Entropy in Thermal Physics
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 热物理学中的熵
- en: The increase in Entropy is a physical law that only applies in the second case.
    However, the entropy in physics can be viewed as a special case of Shannon’s entropy
    when applied to physical systems, so there is a connection there.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 熵的增加是一个仅适用于第二种情况的物理定律。然而，物理中的熵可以被视为应用于物理系统时香农熵的一个特例，所以两者之间有联系。
- en: What this means in terms of the counting exercise then, is that the number of
    possibilities will inevitably increase. This makes intuitive sense, because when
    a physical (chaotic) system is not constrained, it should eventually sample all
    the possibilities. It’s a bit similar to the famous Murphy’s law that stats “anything
    that can go wrong will go wrong”.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着在计数练习中，可能性数量将不可避免地增加。这很直观，因为当一个物理（混沌）系统不受约束时，它最终应该会采样所有可能性。这有点类似于著名的墨菲定律，它说“任何可能出错的事情都会出错”。
- en: 'From a data science perspective, if we believe that our data are results of
    some dynamical systems, then it might makes sense to maximize entropy: because
    if we believe all the variables have been taken into account, there is no reason
    to think that our data would not explore all the possibilities. In other words,
    we want to consider all the possibilities/combinations — even ones that are not
    present in our data. This is perhaps what grants entropic concepts their super
    powers in data science, that'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 从数据科学的角度来看，如果我们相信我们的数据是某些动力系统的结果，那么最大化熵可能是合理的：因为如果我们相信所有变量都已被考虑，就没有理由认为我们的数据不会探索所有可能性。换句话说，我们希望考虑所有可能性/组合——即使是那些在我们数据中不存在的。这或许赋予了熵的概念在数据科学中强大的超能力。
- en: By counting all the possibilities, entropy is a conservative measure of our
    ignorances
  id: totrans-113
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 通过计算所有可能性，熵是我们无知的保守测量
- en: This viewpoint was explored in another one of my articles on [entropy](https://medium.com/swlh/entropy-is-not-disorder-a-physicists-perspective-c0dccfea67f1).
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 这种观点在我另一篇关于[熵](https://medium.com/swlh/entropy-is-not-disorder-a-physicists-perspective-c0dccfea67f1)的文章中得到了探讨。
- en: Conclusion
  id: totrans-115
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: By interpreting the formula for entropy as counting possibilities, we are able
    to understand entropy’s role in information theory and think of entropy as a sort
    of probability. This interpretation is ultimately what makes various entropic
    concepts meaningful and useful.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将熵的公式解释为计算可能性，我们能够理解熵在信息理论中的作用，并将熵视为一种概率。这种解释最终使各种熵的概念变得有意义和有用。
- en: Please share your thoughts and feedback if you have any, happy reading! 👋
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你有任何想法和反馈，请分享，祝阅读愉快！👋
- en: 'If you enjoy this article, you might be interested in some of my other related
    articles:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你喜欢这篇文章，你可能对我的一些其他相关文章感兴趣：
- en: '[](https://medium.com/swlh/entropy-is-not-disorder-a-physicists-perspective-c0dccfea67f1?source=post_page-----a7f7e5d16421--------------------------------)
    [## Entropy Is Not Disorder: A Physicist’s Perspective'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/swlh/entropy-is-not-disorder-a-physicists-perspective-c0dccfea67f1?source=post_page-----a7f7e5d16421--------------------------------)
    [## 熵不是混乱：物理学家的视角'
- en: Entropy is often treated as synonymous to chaos. But what is it really? In this
    article, we explore how entropy is more…
  id: totrans-120
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 熵常被视为混乱的同义词。但它到底是什么呢？在本文中，我们探索了熵如何更多地……
- en: 'medium.com](https://medium.com/swlh/entropy-is-not-disorder-a-physicists-perspective-c0dccfea67f1?source=post_page-----a7f7e5d16421--------------------------------)
    [](/a-physicists-view-of-machine-learning-the-thermodynamics-of-machine-learning-6a3ab00e46f1?source=post_page-----a7f7e5d16421--------------------------------)
    [## A Physicist’s View of Machine Learning: The Thermodynamics of Machine Learning'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '[medium.com](https://medium.com/swlh/entropy-is-not-disorder-a-physicists-perspective-c0dccfea67f1?source=post_page-----a7f7e5d16421--------------------------------)
    [](/a-physicists-view-of-machine-learning-the-thermodynamics-of-machine-learning-6a3ab00e46f1?source=post_page-----a7f7e5d16421--------------------------------)
    [## 物理学家对机器学习的看法：机器学习的热力学'
- en: Complex systems in nature can be successfully studied using thermodynamics.
    What about Machine Learning?
  id: totrans-122
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 自然界中的复杂系统可以通过热力学成功研究。那么，机器学习呢？
- en: towardsdatascience.com](/a-physicists-view-of-machine-learning-the-thermodynamics-of-machine-learning-6a3ab00e46f1?source=post_page-----a7f7e5d16421--------------------------------)
    [](https://medium.com/physicist-musings/why-we-dont-live-in-a-simulation-a-physicist-s-perspective-1811d65f502d?source=post_page-----a7f7e5d16421--------------------------------)
    [## Why We Don’t Live in a Simulation
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '[towardsdatascience.com](/a-physicists-view-of-machine-learning-the-thermodynamics-of-machine-learning-6a3ab00e46f1?source=post_page-----a7f7e5d16421--------------------------------)
    [](https://medium.com/physicist-musings/why-we-dont-live-in-a-simulation-a-physicist-s-perspective-1811d65f502d?source=post_page-----a7f7e5d16421--------------------------------)
    [## 为什么我们不生活在模拟中'
- en: Describing reality as a simulation vastly understates the complexities of our
    world. Here’s why the simulation…
  id: totrans-124
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 将现实描述为模拟大大低估了我们世界的复杂性。以下是为什么模拟……
- en: medium.com](https://medium.com/physicist-musings/why-we-dont-live-in-a-simulation-a-physicist-s-perspective-1811d65f502d?source=post_page-----a7f7e5d16421--------------------------------)
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '[medium.com](https://medium.com/physicist-musings/why-we-dont-live-in-a-simulation-a-physicist-s-perspective-1811d65f502d?source=post_page-----a7f7e5d16421--------------------------------)'
