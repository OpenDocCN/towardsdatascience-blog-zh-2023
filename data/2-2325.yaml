- en: What does Entropy Measure? An Intuitive Explanation
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç†µåº¦é‡äº†ä»€ä¹ˆï¼Ÿç›´è§‚è§£é‡Š
- en: åŸæ–‡ï¼š[https://towardsdatascience.com/what-does-entropy-measure-an-intuitive-explanation-a7f7e5d16421](https://towardsdatascience.com/what-does-entropy-measure-an-intuitive-explanation-a7f7e5d16421)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡ï¼š[https://towardsdatascience.com/what-does-entropy-measure-an-intuitive-explanation-a7f7e5d16421](https://towardsdatascience.com/what-does-entropy-measure-an-intuitive-explanation-a7f7e5d16421)
- en: '[](https://tim-lou.medium.com/?source=post_page-----a7f7e5d16421--------------------------------)[![Tim
    Lou, PhD](../Images/e4931bb6d59e27730529ceaf00a23822.png)](https://tim-lou.medium.com/?source=post_page-----a7f7e5d16421--------------------------------)[](https://towardsdatascience.com/?source=post_page-----a7f7e5d16421--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----a7f7e5d16421--------------------------------)
    [Tim Lou, PhD](https://tim-lou.medium.com/?source=post_page-----a7f7e5d16421--------------------------------)'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://tim-lou.medium.com/?source=post_page-----a7f7e5d16421--------------------------------)[![Tim
    Lou, PhD](../Images/e4931bb6d59e27730529ceaf00a23822.png)](https://tim-lou.medium.com/?source=post_page-----a7f7e5d16421--------------------------------)[](https://towardsdatascience.com/?source=post_page-----a7f7e5d16421--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----a7f7e5d16421--------------------------------)
    [Tim Lou, PhD](https://tim-lou.medium.com/?source=post_page-----a7f7e5d16421--------------------------------)'
- en: Â·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----a7f7e5d16421--------------------------------)
    Â·11 min readÂ·Jan 4, 2023
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: Â·å‘è¡¨äº [Towards Data Science](https://towardsdatascience.com/?source=post_page-----a7f7e5d16421--------------------------------)
    Â·é˜…è¯»æ—¶é—´ 11 åˆ†é’ŸÂ·2023 å¹´ 1 æœˆ 4 æ—¥
- en: --
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: 'Entropy may seem abstract, but it has an intuitive side: as the probability
    of seeing certain patterns in data. Hereâ€™s how it works.'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: ç†µå¯èƒ½çœ‹èµ·æ¥å¾ˆæŠ½è±¡ï¼Œä½†å®ƒæœ‰ä¸€ä¸ªç›´è§‚çš„æ–¹é¢ï¼šå³çœ‹åˆ°æ•°æ®ä¸­æŸäº›æ¨¡å¼çš„æ¦‚ç‡ã€‚å®ƒæ˜¯å¦‚ä½•å·¥ä½œçš„å‘¢ï¼Ÿ
- en: '![](../Images/4daae7bdad8d3ebc360bb1ef70760dd8.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4daae7bdad8d3ebc360bb1ef70760dd8.png)'
- en: 'Background Credit: Joe Maldonado [@unsplash](https://unsplash.com/@joesracingteam)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: èƒŒæ™¯æ¥æºï¼šJoe Maldonado [@unsplash](https://unsplash.com/@joesracingteam)
- en: 'In data science, there are many concepts linked to the notion of entropy. The
    most basic one is Shannonâ€™s information entropy, defined for any distribution,
    *P*(*x*), through the formula:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ•°æ®ç§‘å­¦ä¸­ï¼Œæœ‰è®¸å¤šä¸ç†µçš„æ¦‚å¿µç›¸å…³çš„æ¦‚å¿µã€‚æœ€åŸºæœ¬çš„æ˜¯é¦™å†œä¿¡æ¯ç†µï¼Œé€šè¿‡å…¬å¼å®šä¹‰åœ¨ä»»æ„åˆ†å¸ƒ *P*(*x*) ä¸­ï¼š
- en: '![](../Images/a8588251ff381e4720e63406deb3205a.png)'
  id: totrans-9
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a8588251ff381e4720e63406deb3205a.png)'
- en: Where the sum is over all the possible categories in *C*.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­æ±‚å’Œæ˜¯å¯¹æ‰€æœ‰å¯èƒ½çš„ç±»åˆ« *C* è¿›è¡Œçš„ã€‚
- en: 'There are other related concepts that have similarly looking formulae:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: è¿˜æœ‰å…¶ä»–ç›¸å…³çš„æ¦‚å¿µï¼Œå…¶å…¬å¼ç±»ä¼¼ï¼š
- en: '[Kullbackâ€“Leibler divergence](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence):
    for comparing two distributions'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Kullbackâ€“Leibler æ•£åº¦](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence)ï¼šç”¨äºæ¯”è¾ƒä¸¤ä¸ªåˆ†å¸ƒ'
- en: '[Mutual information](https://en.wikipedia.org/wiki/Mutual_information): for
    capturing general relationships between two variables'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[äº’ä¿¡æ¯](https://en.wikipedia.org/wiki/Mutual_information)ï¼šç”¨äºæ•æ‰ä¸¤ä¸ªå˜é‡ä¹‹é—´çš„ä¸€èˆ¬å…³ç³»'
- en: '[Cross entropy](https://en.wikipedia.org/wiki/Cross_entropy): for training
    classification models'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[äº¤å‰ç†µ](https://en.wikipedia.org/wiki/Cross_entropy)ï¼šç”¨äºè®­ç»ƒåˆ†ç±»æ¨¡å‹'
- en: 'Despite the ubiquity of entropy-like formulae, there are rarely discussions
    on the intuitions behind the formula: Why is the logarithm involved? Why are we
    multiplying *P*(*x*) and log *P*(*x*)? While many articles mention terms like
    â€œinformationâ€, â€œexpected surpriseâ€, the intuitions behind them are missing.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: å°½ç®¡ç†µç±»ä¼¼çš„å…¬å¼å¾ˆæ™®éï¼Œä½†å…³äºå…¬å¼èƒŒåçš„ç›´è§‰è®¨è®ºå´å¾ˆå°‘ï¼šä¸ºä»€ä¹ˆæ¶‰åŠå¯¹æ•°ï¼Ÿä¸ºä»€ä¹ˆæˆ‘ä»¬è¦å°† *P*(*x*) å’Œ log *P*(*x*) ç›¸ä¹˜ï¼Ÿè™½ç„¶è®¸å¤šæ–‡ç« æåˆ°â€œä¿¡æ¯â€ã€â€œæœŸæœ›æƒŠè®¶â€ç­‰æœ¯è¯­ï¼Œä½†å…¶èƒŒåçš„ç›´è§‰å´ç¼ºå¤±ã€‚
- en: It turns out, just like probabilities, entropy can be understood through a counting
    exercise, and it can be linked to a sort of log-likelihood for distributions.
    Furthermore, this counting can be linked to the literal number of bytes in a computer.
    These interpretations will enable us to demystify the many facts about entropy.
    Curious? Letâ€™s get started!
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: äº‹å®è¯æ˜ï¼Œæ­£å¦‚æ¦‚ç‡ä¸€æ ·ï¼Œç†µå¯ä»¥é€šè¿‡è®¡æ•°ç»ƒä¹ æ¥ç†è§£ï¼Œå¹¶ä¸”å¯ä»¥ä¸åˆ†å¸ƒçš„æŸç§å¯¹æ•°ä¼¼ç„¶æ€§è”ç³»èµ·æ¥ã€‚æ­¤å¤–ï¼Œè¿™ç§è®¡æ•°è¿˜å¯ä»¥ä¸è®¡ç®—æœºä¸­çš„å­—èŠ‚æ•°å­—é¢ä¸Šçš„æ•°é‡ç›¸å…³è”ã€‚è¿™äº›è§£é‡Šå°†ä½¿æˆ‘ä»¬èƒ½å¤Ÿæ­å¼€ç†µçš„è®¸å¤šäº‹å®ã€‚å¥½å¥‡å—ï¼Ÿé‚£å°±å¼€å§‹å§ï¼
- en: Counting Entropy
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: è®¡æ•°ç†µ
- en: '![](../Images/d167237368b07ac265e675b3f71352a8.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d167237368b07ac265e675b3f71352a8.png)'
- en: Probabilityâ€™s counting definition is what makes it so intuitive (Photo by [Ibrahim
    Rifath](https://unsplash.com/@photoripey?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral))
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: æ¦‚ç‡çš„è®¡æ•°å®šä¹‰ä½¿å¾—å®ƒå˜å¾—å¦‚æ­¤ç›´è§‚ï¼ˆç…§ç‰‡ç”± [Ibrahim Rifath](https://unsplash.com/@photoripey?utm_source=medium&utm_medium=referral)
    æä¾›ï¼Œæ¥æºäº [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)ï¼‰
- en: 'Probability can be defined operationally: When we say a coin has a 50% chance
    to land a head, it means that if we were to flip the coin a million times, the
    number of heads will get pretty close to half a million. This fraction will get
    closer to the 50% probability as the number of trials increase. This definition
    is what makes probabilities so intuitive.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: æ¦‚ç‡å¯ä»¥ä»æ“ä½œä¸Šå®šä¹‰ï¼šå½“æˆ‘ä»¬è¯´ä¸€æšç¡¬å¸æœ‰50%çš„æœºä¼šè½åˆ°æ­£é¢æ—¶ï¼Œè¿™æ„å‘³ç€å¦‚æœæˆ‘ä»¬å°†ç¡¬å¸æŠ›æ·ä¸€ç™¾ä¸‡æ¬¡ï¼Œæ­£é¢çš„æ¬¡æ•°å°†æ¥è¿‘äº”åä¸‡ã€‚éšç€è¯•éªŒæ¬¡æ•°çš„å¢åŠ ï¼Œè¿™ä¸€æ¯”ä¾‹ä¼šè¶Šæ¥è¶Šæ¥è¿‘50%çš„æ¦‚ç‡ã€‚è¿™ä¸€å®šä¹‰ä½¿å¾—æ¦‚ç‡å˜å¾—å¦‚æ­¤ç›´è§‚ã€‚
- en: 'Is there a similar interpretation for entropy? There is, although the counting
    is a little bit tricker: it will require some basic combinatorics.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: ç†µæ˜¯å¦æœ‰ç±»ä¼¼çš„è§£é‡Šï¼Ÿæœ‰çš„ï¼Œä¸è¿‡è®¡æ•°ä¼šç¨å¾®å¤æ‚ä¸€äº›ï¼šè¿™å°†éœ€è¦ä¸€äº›åŸºç¡€çš„ç»„åˆæ•°å­¦ã€‚
- en: 'How many ways are there to arrange *N* different balls? There are *N* choices
    for the first one, *N* âˆ’ 1 for the second oneâ€¦ etc. The answer is *N*!, or the
    factorial symbol:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: æœ‰å¤šå°‘ç§æ–¹æ³•å¯ä»¥æ’åˆ— *N* ä¸ªä¸åŒçš„çƒï¼Ÿç¬¬ä¸€ä¸ªçƒæœ‰ *N* ç§é€‰æ‹©ï¼Œç¬¬äºŒä¸ªçƒæœ‰ *N* âˆ’ 1 ç§é€‰æ‹©â€¦â€¦ç­‰ç­‰ã€‚ç­”æ¡ˆæ˜¯ *N*!ï¼Œæˆ–è€…é˜¶ä¹˜ç¬¦å·ï¼š
- en: '![](../Images/344ab819c57f5948cf5f1ea8dbec506d.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/344ab819c57f5948cf5f1ea8dbec506d.png)'
- en: 'Just like in the definition of probabilities, weâ€™ll be working with very large
    numbers. So it is helpful to approximate this object through [Sterlingâ€™s approximation](https://en.wikipedia.org/wiki/Stirling%27s_approximation):'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: å°±åƒåœ¨æ¦‚ç‡çš„å®šä¹‰ä¸­ï¼Œæˆ‘ä»¬å°†å¤„ç†éå¸¸å¤§çš„æ•°å­—ã€‚å› æ­¤ï¼Œé€šè¿‡ [æ–¯ç‰¹æ—è¿‘ä¼¼](https://en.wikipedia.org/wiki/Stirling%27s_approximation)
    æ¥è¿‘ä¼¼è¿™ä¸ªå¯¹è±¡æ˜¯æœ‰å¸®åŠ©çš„ï¼š
- en: '![](../Images/3f6508c3690964cbfbdaba2108afa70a.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3f6508c3690964cbfbdaba2108afa70a.png)'
- en: Where log indicates the natural logarithm; analogous formulae also exist if
    we use alternative bases like logâ‚‚ and logâ‚â‚€ (this will determine the units in
    which we measure entropy). The big-O notation indicates the validity of the approximation
    as *N* gets large. The term *N* log *N* will be the origin of *p* log *p* in entropyâ€™s
    definition.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­ log è¡¨ç¤ºè‡ªç„¶å¯¹æ•°ï¼›å¦‚æœæˆ‘ä»¬ä½¿ç”¨å…¶ä»–åŸºæ•°å¦‚ logâ‚‚ å’Œ logâ‚â‚€ï¼Œä¹Ÿå­˜åœ¨ç±»ä¼¼çš„å…¬å¼ï¼ˆè¿™å°†å†³å®šæˆ‘ä»¬æµ‹é‡ç†µçš„å•ä½ï¼‰ã€‚å¤§Oç¬¦å·è¡¨æ˜è¿‘ä¼¼åœ¨ *N*
    å¾ˆå¤§æ—¶çš„æœ‰æ•ˆæ€§ã€‚æœ¯è¯­ *N* log *N* å°†æˆä¸ºç†µå®šä¹‰ä¸­ *p* log *p* çš„æ¥æºã€‚
- en: We are now ready to derive what entropy is counting. Imagine there is a large
    number of distinguishable objects, or distinguishable data points. These *N* data
    points are grouped into say *c* categories, like in the figure below
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ç°åœ¨å‡†å¤‡æ¨å¯¼ç†µæ‰€è®¡æ•°çš„å†…å®¹ã€‚æƒ³è±¡æœ‰å¤§é‡å¯åŒºåˆ†çš„å¯¹è±¡æˆ–å¯åŒºåˆ†çš„æ•°æ®ç‚¹ã€‚è¿™äº› *N* æ•°æ®ç‚¹è¢«åˆ†ç»„ä¸º *c* ä¸ªç±»åˆ«ï¼Œå¦‚ä¸‹å›¾æ‰€ç¤ºï¼š
- en: '![](../Images/921883f6e44aed334b172acfe55f503c.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/921883f6e44aed334b172acfe55f503c.png)'
- en: 'What is the total number of ways this can be done? Keeping in mind that we
    donâ€™t care about the ordering of our data in any category, the answer is captured
    by the classic multinomial coefficients:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ç§æƒ…å†µçš„æ€»æ’åˆ—æ–¹å¼æ˜¯å¤šå°‘ï¼Ÿè®°ä½æˆ‘ä»¬ä¸å…³å¿ƒä»»ä½•ç±»åˆ«ä¸­æ•°æ®çš„æ’åºï¼Œç­”æ¡ˆç”±ç»å…¸çš„å¤šé¡¹å¼ç³»æ•°ç»™å‡ºï¼š
- en: '![](../Images/620970d66f7f69c551ca919e9ef76f60.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/620970d66f7f69c551ca919e9ef76f60.png)'
- en: Where we have used the Î© symbol to denote the number of configurations.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ç”¨ Î© ç¬¦å·æ¥è¡¨ç¤ºé…ç½®çš„æ•°é‡ã€‚
- en: 'Just like the case for probability, we are only interested in the large *N*
    behaviors. When dealing with such large numbers, it is helpful to take the logarithm,
    so we can use Sterlingâ€™s approximation to make things more manageable:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: å°±åƒæ¦‚ç‡çš„æƒ…å†µä¸€æ ·ï¼Œæˆ‘ä»¬åªå¯¹å¤§çš„ *N* è¡Œä¸ºæ„Ÿå…´è¶£ã€‚å½“å¤„ç†å¦‚æ­¤å¤§çš„æ•°å­—æ—¶ï¼Œå–å¯¹æ•°æ˜¯æœ‰å¸®åŠ©çš„ï¼Œè¿™æ ·æˆ‘ä»¬å¯ä»¥ä½¿ç”¨æ–¯ç‰¹æ—è¿‘ä¼¼æ¥ä½¿äº‹æƒ…æ›´å¯æ§ï¼š
- en: '![](../Images/1206496dc3bce7cf30dd9693b6719e8a.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1206496dc3bce7cf30dd9693b6719e8a.png)'
- en: The formula can be simplified by utilizing the fact that the sum of all the
    *náµ¢* equals *N*,
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: å…¬å¼å¯ä»¥é€šè¿‡åˆ©ç”¨æ‰€æœ‰ *náµ¢* æ€»å’Œç­‰äº *N* çš„äº‹å®æ¥ç®€åŒ–ï¼Œ
- en: '![](../Images/da8eae2a138667b729bad6985415f186.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/da8eae2a138667b729bad6985415f186.png)'
- en: 'if we substitute *náµ¢*/*N* â†’ *P*(*i*), we get exactly the entropy formula. Alternatively,
    we can write (for large *N*):'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæˆ‘ä»¬å°† *náµ¢*/*N* â†’ *P*(*i*) ä»£å…¥ï¼Œæˆ‘ä»¬å°±å¾—åˆ°äº†ç†µçš„å…¬å¼ã€‚æˆ–è€…ï¼Œå¯¹äºå¤§çš„ *N*ï¼Œæˆ‘ä»¬å¯ä»¥å†™æˆï¼š
- en: '![](../Images/5736092920f8617b70665370c19d02b1.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5736092920f8617b70665370c19d02b1.png)'
- en: 'So we arrived at an operational definition of entropy:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥æˆ‘ä»¬è¾¾åˆ°äº†ç†µçš„æ“ä½œå®šä¹‰ï¼š
- en: 'Entropy counts the # of ways of categorizing large amounts of data that resemble
    a given probability distribution (in logarithmic units and per number of data
    point)'
  id: totrans-39
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ç†µè®¡ç®—äº†å°†å¤§é‡æ•°æ®åˆ†ç±»æˆç»™å®šæ¦‚ç‡åˆ†å¸ƒçš„æ–¹å¼æ•°ï¼ˆä»¥å¯¹æ•°å•ä½å’Œæ¯ä¸ªæ•°æ®ç‚¹è®¡ç®—ï¼‰ã€‚
- en: This counting exercise lies at the heart of information theory, which weâ€™ll
    turn to next.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªè®¡æ•°ç»ƒä¹ æ˜¯ä¿¡æ¯ç†è®ºçš„æ ¸å¿ƒï¼Œæˆ‘ä»¬æ¥ä¸‹æ¥ä¼šè®¨è®ºå®ƒã€‚
- en: Entropy as Information
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç†µä½œä¸ºä¿¡æ¯
- en: So how does our concept of entropy relate to literal bits of 0s and 1s in a
    computer?
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: é‚£ä¹ˆï¼Œæˆ‘ä»¬çš„ç†µæ¦‚å¿µå¦‚ä½•ä¸è®¡ç®—æœºä¸­çš„å­—é¢0å’Œ1æ¯”ç‰¹ç›¸å…³å‘¢ï¼Ÿ
- en: '![](../Images/eb3f9ddc43602f73e10cc1f8d7bc3c23.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/eb3f9ddc43602f73e10cc1f8d7bc3c23.png)'
- en: 'The concept of entropy can linked to information when we apply it to patterns
    of 1s and 0s (credit: [Gerd Altmann](https://pixabay.com/users/geralt-9301/))'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: å½“æˆ‘ä»¬å°†ç†µåº”ç”¨äº1å’Œ0çš„æ¨¡å¼æ—¶ï¼Œç†µçš„æ¦‚å¿µå¯ä»¥ä¸ä¿¡æ¯ç›¸å…³ï¼ˆä¿¡ç”¨ï¼š[Gerd Altmann](https://pixabay.com/users/geralt-9301/)ï¼‰ã€‚
- en: 'Imagine a binary sequence of some fix length *N*. Intuitively we know that
    it contains *N* bits of information: because it literally takes *N* bits to store
    the sequence in a hard-drive or memory.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: æƒ³è±¡ä¸€ä¸ªå›ºå®šé•¿åº¦ä¸º*N*çš„äºŒè¿›åˆ¶åºåˆ—ã€‚ç›´è§‚ä¸Šæˆ‘ä»¬çŸ¥é“å®ƒåŒ…å«*N*æ¯”ç‰¹çš„ä¿¡æ¯ï¼šå› ä¸ºå®é™…ä¸Šéœ€è¦*N*æ¯”ç‰¹æ¥å­˜å‚¨åºåˆ—åˆ°ç¡¬ç›˜æˆ–å†…å­˜ä¸­ã€‚
- en: But what if the sequence has some interesting patterns like the following?
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†å¦‚æœåºåˆ—æœ‰ä¸€äº›æœ‰è¶£çš„æ¨¡å¼ï¼Œä¾‹å¦‚ä¸‹é¢çš„å‘¢ï¼Ÿ
- en: '000000000000000000000000000'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '000000000000000000000000000'
- en: '010101010101010101010101010'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '010101010101010101010101010'
- en: '000000010000000000000000000'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '000000010000000000000000000'
- en: 'In these cases, the binary sequence representation would be very inefficient.
    We intuitively know that there are more efficient ways to store these sequence:
    we can specify the patterns as opposed to all the literal bits, and the amount
    of meaningful information in these sequences should be smaller.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™äº›æƒ…å†µä¸‹ï¼ŒäºŒè¿›åˆ¶åºåˆ—è¡¨ç¤ºä¼šéå¸¸ä½æ•ˆã€‚æˆ‘ä»¬ç›´è§‚åœ°çŸ¥é“æœ‰æ›´é«˜æ•ˆçš„å­˜å‚¨æ–¹å¼ï¼šæˆ‘ä»¬å¯ä»¥æŒ‡å®šæ¨¡å¼ï¼Œè€Œä¸æ˜¯æ‰€æœ‰çš„å­—é¢æ¯”ç‰¹ï¼Œè¿™äº›åºåˆ—ä¸­çš„æœ‰æ•ˆä¿¡æ¯é‡åº”è¯¥æ›´å°ã€‚
- en: So if we ignore the subtle patterns of digit repetitions, and just look at the
    basic statistical properties of the digits (proportions of 0s and 1s), how much
    better can we do in terms of storing those sequences?
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥ï¼Œå¦‚æœæˆ‘ä»¬å¿½ç•¥æ•°å­—é‡å¤çš„å¾®å¦™æ¨¡å¼ï¼Œåªçœ‹æ•°å­—çš„åŸºæœ¬ç»Ÿè®¡å±æ€§ï¼ˆ0å’Œ1çš„æ¯”ä¾‹ï¼‰ï¼Œåœ¨å­˜å‚¨è¿™äº›åºåˆ—æ—¶æˆ‘ä»¬èƒ½åšå¾—æ›´å¥½å—ï¼Ÿ
- en: 'This is where our entropy counting formula can help us: it can count the total
    number of sequence given some fixed proportions of 0s and 1s.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯æˆ‘ä»¬çš„ç†µè®¡æ•°å…¬å¼å¯ä»¥å¸®åŠ©æˆ‘ä»¬çš„åœ°æ–¹ï¼šå®ƒå¯ä»¥è®¡ç®—ç»™å®šå›ºå®šæ¯”ä¾‹çš„0å’Œ1çš„æ€»åºåˆ—æ•°ã€‚
- en: 'In the case where the proportions of 0s and 1s are 50/50, the total number
    of possibilities is (in the large *N* limit):'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨0å’Œ1çš„æ¯”ä¾‹ä¸º50/50çš„æƒ…å†µä¸‹ï¼Œæ€»çš„å¯èƒ½æ€§æ•°é‡æ˜¯ï¼ˆåœ¨å¤§çš„*N*æé™ä¸‹ï¼‰ï¼š
- en: '![](../Images/1937ff9ce3a6eafbcb7d21f4a9b01a56.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1937ff9ce3a6eafbcb7d21f4a9b01a56.png)'
- en: 'We see that this just approximately yields the number of all the possible binary
    sequences 2*á´º.* So the number of bits required to store the sequence is still
    *N*. This isnâ€™t surprising, as we know that random sequences should be impossible
    to compress: it has the maximum *N* bits of information.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬çœ‹åˆ°è¿™å¤§è‡´ä¸Šäº§ç”Ÿäº†æ‰€æœ‰å¯èƒ½çš„äºŒè¿›åˆ¶åºåˆ—çš„æ•°é‡2*á´º*ã€‚æ‰€ä»¥å­˜å‚¨åºåˆ—æ‰€éœ€çš„æ¯”ç‰¹æ•°ä»ç„¶æ˜¯*N*ã€‚è¿™å¹¶ä¸ä»¤äººæƒŠè®¶ï¼Œå› ä¸ºæˆ‘ä»¬çŸ¥é“éšæœºåºåˆ—åº”è¯¥æ— æ³•å‹ç¼©ï¼šå®ƒå…·æœ‰æœ€å¤§*N*æ¯”ç‰¹çš„ä¿¡æ¯ã€‚
- en: 'But what if the proportions are no longer 50/50? We should expect some potential
    savings. In this case, the total number of bits required to store one sequence
    would be:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†å¦‚æœæ¯”ä¾‹ä¸å†æ˜¯50/50å‘¢ï¼Ÿæˆ‘ä»¬åº”è¯¥æœŸæœ›æœ‰ä¸€äº›æ½œåœ¨çš„èŠ‚çœã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œå­˜å‚¨ä¸€ä¸ªåºåˆ—æ‰€éœ€çš„æ¯”ç‰¹æ€»æ•°å°†æ˜¯ï¼š
- en: '![](../Images/749c4409803ad01dca62ec7a2f948886.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/749c4409803ad01dca62ec7a2f948886.png)'
- en: 'Letâ€™s sanity check the case when the number of 0s is much smaller than the
    number of 1s, say *n* â‰ª *N*. In this case the *P*â‚ term can be ignored, and the
    number of bits is given by:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬ sanity check å½“0çš„æ•°é‡è¿œå°äº1çš„æ•°é‡æ—¶çš„æƒ…å†µï¼Œæ¯”å¦‚*n* â‰ª *N*ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œå¯ä»¥å¿½ç•¥*P*â‚é¡¹ï¼Œæ‰€éœ€çš„æ¯”ç‰¹æ•°ä¸ºï¼š
- en: '![](../Images/635d4229fdbbfed25583b61af4459693.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/635d4229fdbbfed25583b61af4459693.png)'
- en: So the amount of information is proportional to *n* instead of *N.* This is
    because we now only need to store the position of each 0 instead of the whole
    sequence.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: å› æ­¤ï¼Œä¿¡æ¯é‡ä¸*n*æˆæ­£æ¯”ï¼Œè€Œä¸æ˜¯*N*ã€‚è¿™æ˜¯å› ä¸ºæˆ‘ä»¬ç°åœ¨åªéœ€è¦å­˜å‚¨æ¯ä¸ª0çš„ä½ç½®ï¼Œè€Œä¸æ˜¯æ•´ä¸ªåºåˆ—ã€‚
- en: This illustrate the power of entropy as it relates to physical bits and bytes
    in a computer. In summary,
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™è¯´æ˜äº†ç†µä¸è®¡ç®—æœºä¸­ç‰©ç†æ¯”ç‰¹å’Œå­—èŠ‚ç›¸å…³çš„åŠ›é‡ã€‚æ€»ä¹‹ï¼Œ
- en: The information entropy specifies the expected number of bit per length that
    is required to store a sequence generated by a given probability distribution.
  id: totrans-62
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ä¿¡æ¯ç†µæŒ‡å®šäº†æ¯å•ä½é•¿åº¦æ‰€éœ€çš„æ¯”ç‰¹æ•°ï¼Œç”¨äºå­˜å‚¨ç”±ç»™å®šæ¦‚ç‡åˆ†å¸ƒç”Ÿæˆçš„åºåˆ—ã€‚
- en: In other words, entropy is a sort of optimal compression ratio for a fixed proportion
    of characters in a sequence. This is the way that entropy is linked to information.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: æ¢å¥è¯è¯´ï¼Œç†µæ˜¯ä¸€ç§é’ˆå¯¹åºåˆ—ä¸­å›ºå®šæ¯”ä¾‹å­—ç¬¦çš„*æœ€ä½³*å‹ç¼©æ¯”ã€‚è¿™æ˜¯ç†µä¸ä¿¡æ¯ç›¸å…³è”çš„æ–¹å¼ã€‚
- en: Beyond thinking of sequence as our subject of interest, we can turn our attention
    to the distributions themselves. This view point allows us to interpret entropy
    as a sort of probability (or log-likelihood).
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: é™¤äº†å°†åºåˆ—è§†ä¸ºæˆ‘ä»¬çš„ç ”ç©¶å¯¹è±¡ï¼Œæˆ‘ä»¬è¿˜å¯ä»¥å°†æ³¨æ„åŠ›è½¬å‘åˆ†å¸ƒæœ¬èº«ã€‚è¿™ç§è§‚ç‚¹ä½¿æˆ‘ä»¬èƒ½å¤Ÿå°†ç†µè§£é‡Šä¸ºä¸€ç§æ¦‚ç‡ï¼ˆæˆ–å¯¹æ•°ä¼¼ç„¶ï¼‰ã€‚
- en: Entropy as Log-Likelihood
  id: totrans-65
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç†µä½œä¸ºå¯¹æ•°ä¼¼ç„¶
- en: Entropy counts the number of possibilities. We want to convert this to a sort
    of probability. To do this we simply need to normalize the counts.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: ç†µè®¡ç®—çš„æ˜¯å¯èƒ½æ€§çš„æ•°é‡ã€‚æˆ‘ä»¬æƒ³å°†å…¶è½¬æ¢ä¸ºä¸€ç§æ¦‚ç‡ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬åªéœ€å¯¹è®¡æ•°è¿›è¡Œå½’ä¸€åŒ–ã€‚
- en: 'What is the total number of ways to categorize *N* data points into *c* categories?
    The answer is simple, because each data point has *c* choices:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: å°† *N* ä¸ªæ•°æ®ç‚¹åˆ†ä¸º *c* ç±»åˆ«çš„æ€»æ–¹æ³•æ•°æ˜¯å¤šå°‘ï¼Ÿç­”æ¡ˆå¾ˆç®€å•ï¼Œå› ä¸ºæ¯ä¸ªæ•°æ®ç‚¹æœ‰ *c* ç§é€‰æ‹©ï¼š
- en: '![](../Images/05935d2b317cfd14412fca4652c4ceca.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/05935d2b317cfd14412fca4652c4ceca.png)'
- en: 'We can now divide the entropyâ€™s count by the total to obtain a probability
    (substituting *náµ¢*/*N* â†’ *P*(*i*)):'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ç°åœ¨å¯ä»¥å°†ç†µçš„è®¡æ•°é™¤ä»¥æ€»æ•°ä»¥è·å¾—æ¦‚ç‡ï¼ˆæ›¿æ¢ *náµ¢*/*N* â†’ *P*(*i*)ï¼‰ï¼š
- en: '![](../Images/84eb6451eccba793aacbcb92ccba2105.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/84eb6451eccba793aacbcb92ccba2105.png)'
- en: 'This way, entropy becomes the probability (asymptotic because of large *N*)
    of observing a particular distribution from randomly categorizing data points:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ ·ï¼Œç†µå˜æˆäº†è§‚å¯Ÿåˆ°ç‰¹å®šåˆ†å¸ƒçš„æ¦‚ç‡ï¼ˆç”±äº *N* å¾ˆå¤§ï¼Œè¶‹è¿‘äºæœ€ç»ˆå€¼ï¼‰ï¼š
- en: Entropy can be viewed as the log-likelihood of observing a given distribution
    (per data point)
  id: totrans-72
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ç†µå¯ä»¥è§†ä¸ºè§‚å¯Ÿç»™å®šåˆ†å¸ƒçš„å¯¹æ•°ä¼¼ç„¶ï¼ˆæ¯ä¸ªæ•°æ®ç‚¹ï¼‰ã€‚
- en: There is a hidden assumption in our discussion though, as we are treating every
    configuration as equally likely in our calculations. What happens if some categories
    are more favored over others?
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶è€Œï¼Œæˆ‘ä»¬çš„è®¨è®ºä¸­å­˜åœ¨ä¸€ä¸ªéšå«çš„å‡è®¾ï¼Œå³æˆ‘ä»¬åœ¨è®¡ç®—ä¸­å°†æ¯ç§é…ç½®è§†ä¸ºåŒæ ·å¯èƒ½ã€‚å¦‚æœæŸäº›ç±»åˆ«æ¯”å…¶ä»–ç±»åˆ«æ›´å—é’çä¼šå‘ç”Ÿä»€ä¹ˆï¼Ÿ
- en: 'We can consider some reference distribution *Q*(*x*). If each data point has
    a chance *Q*(*x*) to be in a particular category *x*, the probability of observing
    *n*â‚ in category 1, *n*â‚‚ in category 2 and so on is given by the multinomial probability:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥è€ƒè™‘ä¸€äº›å‚è€ƒåˆ†å¸ƒ *Q*(*x*)ã€‚å¦‚æœæ¯ä¸ªæ•°æ®ç‚¹æœ‰ *Q*(*x*) çš„æœºä¼šè¿›å…¥ç‰¹å®šç±»åˆ« *x*ï¼Œé‚£ä¹ˆè§‚å¯Ÿåˆ°ç±»åˆ«1ä¸­çš„ *n*â‚ã€ç±»åˆ«2ä¸­çš„
    *n*â‚‚ ç­‰çš„æ¦‚ç‡ç”±å¤šé¡¹å¼æ¦‚ç‡ç»™å‡ºï¼š
- en: '![](../Images/ed03a064964e21cd2f066d52f36d2090.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ed03a064964e21cd2f066d52f36d2090.png)'
- en: Once again, we can go through Sterlingâ€™s approximation. The calculation is very
    similar to the previous ones, except we have a extra *Q*(*i*) in the end
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥å†æ¬¡ä½¿ç”¨æ–¯ç‰¹æ—è¿‘ä¼¼ã€‚è®¡ç®—è¿‡ç¨‹ä¸ä¹‹å‰ç±»ä¼¼ï¼Œåªæ˜¯æœ€åå¤šäº†ä¸€ä¸ª *Q*(*i*)ã€‚
- en: '![](../Images/f838ff7e0bc302f2cb063fe56450c4c1.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f838ff7e0bc302f2cb063fe56450c4c1.png)'
- en: Substituting *náµ¢*/*N* â†’ *P*(*i*), the term inside the exponential becomes the
    [Kullbackâ€“Leibler divergence](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence).
    So our equation can be summarized as
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: æ›¿æ¢ *náµ¢*/*N* â†’ *P*(*i*)ï¼ŒæŒ‡æ•°å†…çš„é¡¹å˜æˆäº† [Kullbackâ€“Leibler æ•£åº¦](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence)ã€‚å› æ­¤ï¼Œæˆ‘ä»¬çš„æ–¹ç¨‹å¯ä»¥æ€»ç»“ä¸º
- en: '![](../Images/be46304139b1eb70476e9d8888b08426.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/be46304139b1eb70476e9d8888b08426.png)'
- en: 'Where we have used the common notation of KL-divergence inside the exponential.
    The KL-divergence is a generalization of Shannonâ€™s information entropy, and our
    equations make our interpretation even more precise:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬ä½¿ç”¨äº†æŒ‡æ•°å†…çš„ KL-æ•£åº¦çš„å¸¸è§è¡¨ç¤ºæ³•ã€‚KL-æ•£åº¦æ˜¯é¦™å†œä¿¡æ¯ç†µçš„æ¨å¹¿ï¼Œæˆ‘ä»¬çš„æ–¹ç¨‹ä½¿æˆ‘ä»¬çš„è§£é‡Šæ›´åŠ ç²¾å‡†ï¼š
- en: The Kullback-Leibler divergence of P on Q is the negative log-likelihood (per
    data point) of observing P when sampling data according to Q
  id: totrans-81
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: P ç›¸å¯¹äº Q çš„ Kullback-Leibler æ•£åº¦æ˜¯è§‚å¯Ÿåˆ° P æ—¶æŒ‰ç…§ Q é‡‡æ ·çš„è´Ÿå¯¹æ•°ä¼¼ç„¶ï¼ˆæ¯ä¸ªæ•°æ®ç‚¹ï¼‰ã€‚
- en: Once again, all this is assuming *N* is very large.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: å†æ¬¡è¯´æ˜ï¼Œæ‰€æœ‰è¿™äº›éƒ½æ˜¯å‡è®¾ *N* éå¸¸å¤§ã€‚
- en: 'A few facts about KL-divergence now become obvious:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: å…³äº KL-æ•£åº¦çš„ä¸€äº›äº‹å®ç°åœ¨å˜å¾—æ˜æ˜¾ï¼š
- en: 'KL-divergence is always non-negative: this is because a probability can never
    be larger than 1.'
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: KL-æ•£åº¦å§‹ç»ˆéè´Ÿï¼šè¿™æ˜¯å› ä¸ºæ¦‚ç‡æ°¸è¿œä¸ä¼šå¤§äº 1ã€‚
- en: 'KL-divergence can be infinite: this happens when two distributions have no
    overlap, and so the counting exercise yields 0 = exp[â€“âˆ].'
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: KL-æ•£åº¦å¯ä»¥æ˜¯æ— é™çš„ï¼šè¿™å‘ç”Ÿåœ¨ä¸¤ä¸ªåˆ†å¸ƒæ²¡æœ‰é‡å æ—¶ï¼Œå› æ­¤è®¡æ•°çš„ç»“æœä¸º 0 = exp[â€“âˆ]ã€‚
- en: 'KL-divergence is zero if and only if *P* = *Q*: when we sample data according
    to *Q*, we expect the resulting distributions to look like *Q â€”* this expectation
    is exact at large *N*.'
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: KL-æ•£åº¦ä¸ºé›¶å½“ä¸”ä»…å½“ *P* = *Q*ï¼šå½“æˆ‘ä»¬æŒ‰ç…§ *Q* é‡‡æ ·æ•°æ®æ—¶ï¼Œæˆ‘ä»¬æœŸæœ›ç»“æœåˆ†å¸ƒç±»ä¼¼äº *Q*â€”â€”è¿™ç§æœŸæœ›åœ¨ *N* å¾ˆå¤§æ—¶æ˜¯å‡†ç¡®çš„ã€‚
- en: Armed with this new understanding, we are now ready to reinterpret facts about
    various entropic concepts in data science!
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: å‡­å€Ÿè¿™ç§æ–°çš„ç†è§£ï¼Œæˆ‘ä»¬ç°åœ¨å‡†å¤‡é‡æ–°è§£é‡Šæ•°æ®ç§‘å­¦ä¸­å„ç§ç†µæ¦‚å¿µçš„äº‹å®ï¼
- en: An Entropic Sampler
  id: totrans-88
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç†µé‡‡æ ·å™¨
- en: Below weâ€™ll discuss the intuitions behind some common entropy-like variables
    in data science. Weâ€™ll once again remind the reader that the large *N* limit is
    implicitly assumed.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸‹é¢æˆ‘ä»¬å°†è®¨è®ºæ•°æ®ç§‘å­¦ä¸­ä¸€äº›å¸¸è§çš„ç±»ä¼¼ç†µçš„å˜é‡çš„ç›´è§‰ã€‚æˆ‘ä»¬å°†å†æ¬¡æé†’è¯»è€…ï¼Œå¤§* N*æé™æ˜¯éšå«å‡è®¾çš„ã€‚
- en: Cross Entropy
  id: totrans-90
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: äº¤å‰ç†µ
- en: This is useful for training categorical variables. It is defined as
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å¯¹è®­ç»ƒåˆ†ç±»å˜é‡å¾ˆæœ‰ç”¨ã€‚å®šä¹‰ä¸º
- en: '![](../Images/a8485c1c02e027374c126a6355c55ec3.png)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a8485c1c02e027374c126a6355c55ec3.png)'
- en: Note that weâ€™ve rewritten the definition as a sum of the KL-divergence and Shannonâ€™s
    information entropy. This may look a little unfamiliar, since when we train machine
    learning model, we only compute an estimate of it through our samples (say *S*)
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: è¯·æ³¨æ„ï¼Œæˆ‘ä»¬å·²ç»å°†å®šä¹‰é‡å†™ä¸ºKLæ•£åº¦å’Œé¦™å†œä¿¡æ¯ç†µçš„å’Œã€‚è¿™å¯èƒ½çœ‹èµ·æ¥æœ‰ç‚¹ä¸ç†Ÿæ‚‰ï¼Œå› ä¸ºå½“æˆ‘ä»¬è®­ç»ƒæœºå™¨å­¦ä¹ æ¨¡å‹æ—¶ï¼Œæˆ‘ä»¬ä»…é€šè¿‡æ ·æœ¬ï¼ˆæ¯”å¦‚*S*ï¼‰è®¡ç®—å…¶ä¼°è®¡å€¼
- en: '![](../Images/868e4a17287e8aae05b7ce6098e40899.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/868e4a17287e8aae05b7ce6098e40899.png)'
- en: Using our counting intuition, we conclude that
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: åˆ©ç”¨æˆ‘ä»¬çš„è®¡æ•°ç›´è§‰ï¼Œæˆ‘ä»¬å¾—å‡ºç»“è®º
- en: Minimizing cross-entropy is equivalent to maximizing the log-likelihood of observing
    the same statistics as those from our sampled data, if we sample our data from
    a distribution Q that is being trained
  id: totrans-96
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: æœ€å°åŒ–äº¤å‰ç†µç­‰åŒäºæœ€å¤§åŒ–è§‚å¯Ÿåˆ°ä¸æˆ‘ä»¬ä»åˆ†å¸ƒQä¸­é‡‡æ ·çš„æ•°æ®ç»Ÿè®¡ç›¸åŒçš„å¯¹æ•°ä¼¼ç„¶æ€§ï¼Œå¦‚æœæˆ‘ä»¬ä»è®­ç»ƒä¸­çš„åˆ†å¸ƒQä¸­é‡‡æ ·æ•°æ®
- en: 'This puts the cross-entropy loss on a similar conceptual ground as the L2 loss
    in regressions: they are both some sort of log-likelihood functions.'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä½¿å¾—äº¤å‰ç†µæŸå¤±ä¸å›å½’ä¸­çš„L2æŸå¤±å¤„äºç±»ä¼¼çš„æ¦‚å¿µåŸºç¡€ä¸Šï¼šå®ƒä»¬éƒ½æ˜¯æŸç§å¯¹æ•°ä¼¼ç„¶å‡½æ•°ã€‚
- en: Mutual Information
  id: totrans-98
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: äº’ä¿¡æ¯
- en: Mutual information can be thought of as a generalized sort of correlation between
    two variables. Denoted by *I*, it is defined through the KL-divergence
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: äº’ä¿¡æ¯å¯ä»¥è¢«è§†ä¸ºä¸¤ä¸ªå˜é‡ä¹‹é—´çš„ä¸€ç§å¹¿ä¹‰ç›¸å…³æ€§ã€‚ç”¨*I*è¡¨ç¤ºï¼Œå®ƒé€šè¿‡KLæ•£åº¦å®šä¹‰
- en: '![](../Images/53ddc49f96d4672862b76e741c435412.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/53ddc49f96d4672862b76e741c435412.png)'
- en: Where in the KL-divergence computation, we are comparing a distribution of two
    variables, against the distribution of considering each variable separately.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨KLæ•£åº¦è®¡ç®—ä¸­ï¼Œæˆ‘ä»¬æ¯”è¾ƒä¸¤ä¸ªå˜é‡çš„åˆ†å¸ƒä¸è€ƒè™‘æ¯ä¸ªå˜é‡å•ç‹¬åˆ†å¸ƒçš„åˆ†å¸ƒã€‚
- en: 'Our counting intuitions give us a very nice interpretation:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬çš„è®¡æ•°ç›´è§‰ç»™å‡ºäº†ä¸€ä¸ªå¾ˆå¥½çš„è§£é‡Šï¼š
- en: Mutual information is the negative log-likelihood (per data point) of obtaining
    a given distribution on two variables, when we sample the two variables independently
    based on their marginalized distributions
  id: totrans-103
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: äº’ä¿¡æ¯æ˜¯è·å¾—ä¸¤ä¸ªå˜é‡ç»™å®šåˆ†å¸ƒçš„è´Ÿå¯¹æ•°ä¼¼ç„¶æ€§ï¼ˆæ¯ä¸ªæ•°æ®ç‚¹ï¼‰ï¼Œå½“æˆ‘ä»¬æ ¹æ®å…¶è¾¹é™…åˆ†å¸ƒç‹¬ç«‹åœ°é‡‡æ ·è¿™ä¸¤ä¸ªå˜é‡æ—¶
- en: This explains why mutual information is a powerful tool that can capture nonlinear
    correlations between variables.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™è§£é‡Šäº†ä¸ºä»€ä¹ˆäº’ä¿¡æ¯æ˜¯ä¸€ä¸ªå¼ºå¤§çš„å·¥å…·ï¼Œå¯ä»¥æ•æ‰å˜é‡ä¹‹é—´çš„éçº¿æ€§ç›¸å…³æ€§ã€‚
- en: The Inevitable Increase In Entropy?
  id: totrans-105
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ç†µçš„ä¸å¯é¿å…å¢åŠ ï¼Ÿ
- en: 'Finally, we are ready to address one of the most well-known facts about entropy:
    the laws of thermodynamics and the inevitable increase in entropy.'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åï¼Œæˆ‘ä»¬å‡†å¤‡è®¨è®ºå…³äºç†µçš„æœ€è‘—åäº‹å®ä¹‹ä¸€ï¼šçƒ­åŠ›å­¦å®šå¾‹å’Œç†µçš„ä¸å¯é¿å…å¢åŠ ã€‚
- en: 'It is important to keep in mind though that there are two concepts of entropy
    at play here:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†éœ€è¦è®°ä½çš„æ˜¯ï¼Œè¿™é‡Œå­˜åœ¨ä¸¤ä¸ªç†µçš„æ¦‚å¿µï¼š
- en: Shannonâ€™s Information Entropy in Data Science
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æ•°æ®ç§‘å­¦ä¸­çš„é¦™å†œä¿¡æ¯ç†µ
- en: Entropy in Thermal Physics
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: çƒ­ç‰©ç†å­¦ä¸­çš„ç†µ
- en: The increase in Entropy is a physical law that only applies in the second case.
    However, the entropy in physics can be viewed as a special case of Shannonâ€™s entropy
    when applied to physical systems, so there is a connection there.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: ç†µçš„å¢åŠ æ˜¯ä¸€ä¸ªä»…é€‚ç”¨äºç¬¬äºŒç§æƒ…å†µçš„ç‰©ç†å®šå¾‹ã€‚ç„¶è€Œï¼Œç‰©ç†ä¸­çš„ç†µå¯ä»¥è¢«è§†ä¸ºåº”ç”¨äºç‰©ç†ç³»ç»Ÿæ—¶é¦™å†œç†µçš„ä¸€ä¸ªç‰¹ä¾‹ï¼Œæ‰€ä»¥ä¸¤è€…ä¹‹é—´æœ‰è”ç³»ã€‚
- en: What this means in terms of the counting exercise then, is that the number of
    possibilities will inevitably increase. This makes intuitive sense, because when
    a physical (chaotic) system is not constrained, it should eventually sample all
    the possibilities. Itâ€™s a bit similar to the famous Murphyâ€™s law that stats â€œanything
    that can go wrong will go wrongâ€.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ„å‘³ç€åœ¨è®¡æ•°ç»ƒä¹ ä¸­ï¼Œå¯èƒ½æ€§æ•°é‡å°†ä¸å¯é¿å…åœ°å¢åŠ ã€‚è¿™å¾ˆç›´è§‚ï¼Œå› ä¸ºå½“ä¸€ä¸ªç‰©ç†ï¼ˆæ··æ²Œï¼‰ç³»ç»Ÿä¸å—çº¦æŸæ—¶ï¼Œå®ƒæœ€ç»ˆåº”è¯¥ä¼šé‡‡æ ·æ‰€æœ‰å¯èƒ½æ€§ã€‚è¿™æœ‰ç‚¹ç±»ä¼¼äºè‘—åçš„å¢¨è²å®šå¾‹ï¼Œå®ƒè¯´â€œä»»ä½•å¯èƒ½å‡ºé”™çš„äº‹æƒ…éƒ½ä¼šå‡ºé”™â€ã€‚
- en: 'From a data science perspective, if we believe that our data are results of
    some dynamical systems, then it might makes sense to maximize entropy: because
    if we believe all the variables have been taken into account, there is no reason
    to think that our data would not explore all the possibilities. In other words,
    we want to consider all the possibilities/combinations â€” even ones that are not
    present in our data. This is perhaps what grants entropic concepts their super
    powers in data science, that'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: ä»æ•°æ®ç§‘å­¦çš„è§’åº¦æ¥çœ‹ï¼Œå¦‚æœæˆ‘ä»¬ç›¸ä¿¡æˆ‘ä»¬çš„æ•°æ®æ˜¯æŸäº›åŠ¨åŠ›ç³»ç»Ÿçš„ç»“æœï¼Œé‚£ä¹ˆæœ€å¤§åŒ–ç†µå¯èƒ½æ˜¯åˆç†çš„ï¼šå› ä¸ºå¦‚æœæˆ‘ä»¬ç›¸ä¿¡æ‰€æœ‰å˜é‡éƒ½å·²è¢«è€ƒè™‘ï¼Œå°±æ²¡æœ‰ç†ç”±è®¤ä¸ºæˆ‘ä»¬çš„æ•°æ®ä¸ä¼šæ¢ç´¢æ‰€æœ‰å¯èƒ½æ€§ã€‚æ¢å¥è¯è¯´ï¼Œæˆ‘ä»¬å¸Œæœ›è€ƒè™‘æ‰€æœ‰å¯èƒ½æ€§/ç»„åˆâ€”â€”å³ä½¿æ˜¯é‚£äº›åœ¨æˆ‘ä»¬æ•°æ®ä¸­ä¸å­˜åœ¨çš„ã€‚è¿™æˆ–è®¸èµ‹äºˆäº†ç†µçš„æ¦‚å¿µåœ¨æ•°æ®ç§‘å­¦ä¸­å¼ºå¤§çš„è¶…èƒ½åŠ›ã€‚
- en: By counting all the possibilities, entropy is a conservative measure of our
    ignorances
  id: totrans-113
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: é€šè¿‡è®¡ç®—æ‰€æœ‰å¯èƒ½æ€§ï¼Œç†µæ˜¯æˆ‘ä»¬æ— çŸ¥çš„ä¿å®ˆæµ‹é‡
- en: This viewpoint was explored in another one of my articles on [entropy](https://medium.com/swlh/entropy-is-not-disorder-a-physicists-perspective-c0dccfea67f1).
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ç§è§‚ç‚¹åœ¨æˆ‘å¦ä¸€ç¯‡å…³äº[ç†µ](https://medium.com/swlh/entropy-is-not-disorder-a-physicists-perspective-c0dccfea67f1)çš„æ–‡ç« ä¸­å¾—åˆ°äº†æ¢è®¨ã€‚
- en: Conclusion
  id: totrans-115
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç»“è®º
- en: By interpreting the formula for entropy as counting possibilities, we are able
    to understand entropyâ€™s role in information theory and think of entropy as a sort
    of probability. This interpretation is ultimately what makes various entropic
    concepts meaningful and useful.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: é€šè¿‡å°†ç†µçš„å…¬å¼è§£é‡Šä¸ºè®¡ç®—å¯èƒ½æ€§ï¼Œæˆ‘ä»¬èƒ½å¤Ÿç†è§£ç†µåœ¨ä¿¡æ¯ç†è®ºä¸­çš„ä½œç”¨ï¼Œå¹¶å°†ç†µè§†ä¸ºä¸€ç§æ¦‚ç‡ã€‚è¿™ç§è§£é‡Šæœ€ç»ˆä½¿å„ç§ç†µçš„æ¦‚å¿µå˜å¾—æœ‰æ„ä¹‰å’Œæœ‰ç”¨ã€‚
- en: Please share your thoughts and feedback if you have any, happy reading! ğŸ‘‹
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœä½ æœ‰ä»»ä½•æƒ³æ³•å’Œåé¦ˆï¼Œè¯·åˆ†äº«ï¼Œç¥é˜…è¯»æ„‰å¿«ï¼ğŸ‘‹
- en: 'If you enjoy this article, you might be interested in some of my other related
    articles:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœä½ å–œæ¬¢è¿™ç¯‡æ–‡ç« ï¼Œä½ å¯èƒ½å¯¹æˆ‘çš„ä¸€äº›å…¶ä»–ç›¸å…³æ–‡ç« æ„Ÿå…´è¶£ï¼š
- en: '[](https://medium.com/swlh/entropy-is-not-disorder-a-physicists-perspective-c0dccfea67f1?source=post_page-----a7f7e5d16421--------------------------------)
    [## Entropy Is Not Disorder: A Physicistâ€™s Perspective'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/swlh/entropy-is-not-disorder-a-physicists-perspective-c0dccfea67f1?source=post_page-----a7f7e5d16421--------------------------------)
    [## ç†µä¸æ˜¯æ··ä¹±ï¼šç‰©ç†å­¦å®¶çš„è§†è§’'
- en: Entropy is often treated as synonymous to chaos. But what is it really? In this
    article, we explore how entropy is moreâ€¦
  id: totrans-120
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ç†µå¸¸è¢«è§†ä¸ºæ··ä¹±çš„åŒä¹‰è¯ã€‚ä½†å®ƒåˆ°åº•æ˜¯ä»€ä¹ˆå‘¢ï¼Ÿåœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æ¢ç´¢äº†ç†µå¦‚ä½•æ›´å¤šåœ°â€¦â€¦
- en: 'medium.com](https://medium.com/swlh/entropy-is-not-disorder-a-physicists-perspective-c0dccfea67f1?source=post_page-----a7f7e5d16421--------------------------------)
    [](/a-physicists-view-of-machine-learning-the-thermodynamics-of-machine-learning-6a3ab00e46f1?source=post_page-----a7f7e5d16421--------------------------------)
    [## A Physicistâ€™s View of Machine Learning: The Thermodynamics of Machine Learning'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '[medium.com](https://medium.com/swlh/entropy-is-not-disorder-a-physicists-perspective-c0dccfea67f1?source=post_page-----a7f7e5d16421--------------------------------)
    [](/a-physicists-view-of-machine-learning-the-thermodynamics-of-machine-learning-6a3ab00e46f1?source=post_page-----a7f7e5d16421--------------------------------)
    [## ç‰©ç†å­¦å®¶å¯¹æœºå™¨å­¦ä¹ çš„çœ‹æ³•ï¼šæœºå™¨å­¦ä¹ çš„çƒ­åŠ›å­¦'
- en: Complex systems in nature can be successfully studied using thermodynamics.
    What about Machine Learning?
  id: totrans-122
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: è‡ªç„¶ç•Œä¸­çš„å¤æ‚ç³»ç»Ÿå¯ä»¥é€šè¿‡çƒ­åŠ›å­¦æˆåŠŸç ”ç©¶ã€‚é‚£ä¹ˆï¼Œæœºå™¨å­¦ä¹ å‘¢ï¼Ÿ
- en: towardsdatascience.com](/a-physicists-view-of-machine-learning-the-thermodynamics-of-machine-learning-6a3ab00e46f1?source=post_page-----a7f7e5d16421--------------------------------)
    [](https://medium.com/physicist-musings/why-we-dont-live-in-a-simulation-a-physicist-s-perspective-1811d65f502d?source=post_page-----a7f7e5d16421--------------------------------)
    [## Why We Donâ€™t Live in a Simulation
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '[towardsdatascience.com](/a-physicists-view-of-machine-learning-the-thermodynamics-of-machine-learning-6a3ab00e46f1?source=post_page-----a7f7e5d16421--------------------------------)
    [](https://medium.com/physicist-musings/why-we-dont-live-in-a-simulation-a-physicist-s-perspective-1811d65f502d?source=post_page-----a7f7e5d16421--------------------------------)
    [## ä¸ºä»€ä¹ˆæˆ‘ä»¬ä¸ç”Ÿæ´»åœ¨æ¨¡æ‹Ÿä¸­'
- en: Describing reality as a simulation vastly understates the complexities of our
    world. Hereâ€™s why the simulationâ€¦
  id: totrans-124
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: å°†ç°å®æè¿°ä¸ºæ¨¡æ‹Ÿå¤§å¤§ä½ä¼°äº†æˆ‘ä»¬ä¸–ç•Œçš„å¤æ‚æ€§ã€‚ä»¥ä¸‹æ˜¯ä¸ºä»€ä¹ˆæ¨¡æ‹Ÿâ€¦â€¦
- en: medium.com](https://medium.com/physicist-musings/why-we-dont-live-in-a-simulation-a-physicist-s-perspective-1811d65f502d?source=post_page-----a7f7e5d16421--------------------------------)
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '[medium.com](https://medium.com/physicist-musings/why-we-dont-live-in-a-simulation-a-physicist-s-perspective-1811d65f502d?source=post_page-----a7f7e5d16421--------------------------------)'
