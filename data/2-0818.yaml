- en: Introduction to Entropy and Gini Index
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 熵和基尼指数简介
- en: 原文：[https://towardsdatascience.com/entropy-and-gini-index-c04b7452efbe](https://towardsdatascience.com/entropy-and-gini-index-c04b7452efbe)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/entropy-and-gini-index-c04b7452efbe](https://towardsdatascience.com/entropy-and-gini-index-c04b7452efbe)
- en: Understanding how these measures help us quantify uncertainty in a dataset
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解这些度量如何帮助我们量化数据集中的不确定性
- en: '[](https://medium.com/@gurjinderkaur95?source=post_page-----c04b7452efbe--------------------------------)[![Gurjinder
    Kaur](../Images/d5c6746466025dad06077b1a89a789d1.png)](https://medium.com/@gurjinderkaur95?source=post_page-----c04b7452efbe--------------------------------)[](https://towardsdatascience.com/?source=post_page-----c04b7452efbe--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----c04b7452efbe--------------------------------)
    [Gurjinder Kaur](https://medium.com/@gurjinderkaur95?source=post_page-----c04b7452efbe--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@gurjinderkaur95?source=post_page-----c04b7452efbe--------------------------------)[![Gurjinder
    Kaur](../Images/d5c6746466025dad06077b1a89a789d1.png)](https://medium.com/@gurjinderkaur95?source=post_page-----c04b7452efbe--------------------------------)[](https://towardsdatascience.com/?source=post_page-----c04b7452efbe--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----c04b7452efbe--------------------------------)
    [Gurjinder Kaur](https://medium.com/@gurjinderkaur95?source=post_page-----c04b7452efbe--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----c04b7452efbe--------------------------------)
    ·7 min read·Nov 5, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----c04b7452efbe--------------------------------)
    ·阅读时间7分钟·2023年11月5日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/7471e1f27896be8a7c34dd14a251bf09.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7471e1f27896be8a7c34dd14a251bf09.png)'
- en: 'Can you tell which are the purest and impurest carts? (Source: Image by Author)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 你能告诉哪些是最纯净和最不纯净的购物车吗？（来源：作者提供的图像）
- en: E**ntropy** and **Gini Index** are important machine learning concepts particularly
    helpful in decision tree algorithms to determine the quality of a split. Both
    of these metrics are calculated differently but ultimately used to quantify the
    same thing i.e. uncertainty (or impurity) within a dataset.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '**熵**和**基尼指数**是重要的机器学习概念，特别有助于决策树算法来确定分裂的质量。这两种度量虽然计算方式不同，但最终用于量化数据集中相同的事物，即不确定性（或不纯度）。'
- en: The higher the Entropy (or Gini Index), the more random (mixed) the data is.
  id: totrans-9
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 熵（或基尼指数）越高，数据越随机（混合）。
- en: Let’s have an intuitive picture of impurity in a dataset and understand how
    these metrics can help measure it. (Impurity, uncertainty, randomness, heterogeneity
    — all can be used interchangeably in our context and the goal is to ultimately
    reduce them to have better clarity).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们直观地理解数据集中的不纯度，并了解这些度量如何帮助测量它。（不纯度、不确定性、随机性、异质性——这些都可以在我们的背景下互换使用，目标是最终减少它们以获得更好的清晰度）。
- en: '**What is impurity — explained with an example**'
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**什么是不纯度——用例子解释**'
- en: 'Imagine that you go to a supermarket with your friends — *Alice* and *Bob*
    to buy fruit. Each of you grabs a shopping cart because none of you likes sharing
    your fruits. Let’s find out what you guys got (looks like you love apples!!):'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下你和你的朋友——*爱丽丝*和*鲍勃*一起去超市买水果。你们每个人都拿了一个购物车，因为你们都不喜欢分享水果。让我们看看你们都买了些什么（看起来你们很喜欢苹果！！）：
- en: '![](../Images/28238e1afb78168cb4d68c8d864abbbf.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/28238e1afb78168cb4d68c8d864abbbf.png)'
- en: Image by the Author
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图像
- en: These three carts can be seen as three different data distributions. If we assumed
    that there are two classes (apples and bananas) initially, then the interpretations
    that follow would be incorrect. Rather, think of each cart as a different distribution
    — so the first cart is a data distribution where all data points belong to a single
    class, and the second & third carts are the data distributions with two classes.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 这三辆购物车可以看作是三种不同的数据分布。如果我们假设最初有两个类别（苹果和香蕉），那么以下的解释将是不正确的。相反，应该将每辆购物车看作是不同的数据分布——第一个购物车的数据分布中所有数据点都属于同一类别，而第二和第三个购物车的数据分布中包含两个类别。
- en: Looking at the example above, it is easy to identify the carts with the most
    pure or impure data distributions (*class distributions* to be precise). But in
    order to have a mathematical quantification of purity in a dataset so that it
    can be used by an algorithm to make decisions, entropy and Gini Index come to
    rescue.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 看看上面的例子，很容易识别出数据分布最纯净或最不纯净的购物车（*类别分布*）。但是，为了在数据集中进行数学化的纯度量化，以便算法可以利用它来做出决策，熵和基尼指数就显得尤为重要。
- en: 'Both of these measures look at the probability of occurrence (or presence)
    of each class in a dataset. In our example, we have a total of 8 data points (fruits)
    in each case, so we can compute our class probabilities for each of the carts
    as follows:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 这两种度量都考虑了数据集中每个类别出现（或存在）的概率。在我们的例子中，每种情况都有8个数据点（水果），因此我们可以计算每个购物车的类别概率如下：
- en: '![](../Images/df8a3e940e80a9ef2aa0ce78f08cbcff.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/df8a3e940e80a9ef2aa0ce78f08cbcff.png)'
- en: Image by the Author
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供
- en: Now we are equipped with everything we need to dive into formal definitions
    of Entropy and Gini Index!
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经掌握了所有需要的知识，可以深入探讨熵和基尼指数的正式定义了！
- en: As already discussed, both entropy and gini index are a measure of the degree
    of uncertainty or randomness in data. While they aim to quantify the same fundamental
    concept, each has its own mathematical formulation and interpretation to achieve
    that.
  id: totrans-21
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 如前所述，熵和基尼指数都是衡量数据中不确定性或随机性的度量。虽然它们旨在量化相同的基本概念，但各自有自己的数学公式和解释方法来实现这一点。
- en: Entropy
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 熵
- en: Given a labeled dataset where each label comes from a set of *n* classes, we
    can compute entropy as follows. Here pi is the probability of randomly picking
    up an element from class i.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个标签数据集，其中每个标签来自*n*个类别，我们可以按如下方式计算熵。这里，pi 是从类别 i 中随机选择一个元素的概率。
- en: '![](../Images/1863a937a67ddccd5046ba694b87a7ba.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1863a937a67ddccd5046ba694b87a7ba.png)'
- en: To determine the best split in a decision tree, entropy is used to compute information
    gain, and the feature contributing to the maximum information gain is selected
    at a node.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确定决策树中的最佳划分，熵用于计算信息增益，并选择在节点处贡献最大信息增益的特征。
- en: Gini Index
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 基尼指数
- en: Gini Index attempts to quantify randomness in a dataset by finding an answer
    to this question — *What is the probability of incorrectly labeling an element
    picked randomly from the given data?*
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 基尼指数试图通过回答这个问题来量化数据集中的随机性——*从给定的数据中随机选择一个元素的错误标记概率是多少？*
- en: Given a labeled dataset where each label comes from a set of *n* classes, the
    formula to calculate gini index is given below. Here, pi is the probability of
    randomly picking up an element from class i.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个标签数据集，其中每个标签来自*n*个类别，计算基尼指数的公式如下。这里，pi 是从类别 i 中随机选择一个元素的概率。
- en: '![](../Images/473c30a64b7d8898db7faf894adff777.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/473c30a64b7d8898db7faf894adff777.png)'
- en: 'This formula is often reframed as follows as well:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 这个公式通常也可以重新表述为：
- en: '![](../Images/b20fd9267e7abec313e8dcc3b181110e.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b20fd9267e7abec313e8dcc3b181110e.png)'
- en: '(Note: The sum of all class probabilities is 1).'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: （注意：所有类别概率的总和为1）。
- en: Gini index is an alternative to information gain that can be used in decision
    tree to determine the quality of split. At a given node, it compares the *difference*
    between the **gini index of the data before split** and **weighted sum of gini
    indices of both branches after split** and chooses the one with the highest difference
    (or *gini gain).* If this is unclear, don’t worry about it for now since it needs
    more context, and the goal of this article is to just have a basic intuition behind
    the meaning of these metrics.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 基尼指数是信息增益的一种替代方法，可以用于决策树中确定划分的质量。在给定节点上，它比较**划分前数据的基尼指数**与**划分后两个分支基尼指数的加权和**之间的*差异*，并选择差异最大（或*基尼增益*）的那个。如果这还不清楚，暂时不用担心，因为这需要更多的背景信息，本文的目标是让你对这些度量的基本含义有一个初步的了解。
- en: Going back to our example
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 回到我们的例子
- en: 'To make things easier to understand, refer to our shopping cart example, we
    have three datasets — C1, C2, and C3, each of which has 8 records with labels
    coming from two classes — [Apple, Banana]. Using the probabilities calculated
    in the table above, let’s unroll both of these computations for Alice’s cart:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更容易理解，以我们的购物车例子为参考，我们有三个数据集——C1、C2 和 C3，每个数据集有8条记录，标签来自两个类别——[苹果，香蕉]。使用上表中计算的概率，我们来逐步计算
    Alice 的购物车的两个指标：
- en: '![](../Images/ced21c7fff8d8226381fd708d53dcd14.png)![](../Images/3a0c704604f32360454680bf57b8465c.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ced21c7fff8d8226381fd708d53dcd14.png)![](../Images/3a0c704604f32360454680bf57b8465c.png)'
- en: 'Similarly, we can compute these metrics for C1 and C3 as well and will get
    the following results:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，我们也可以计算 C1 和 C3 的这些指标，并得到以下结果：
- en: '![](../Images/121c8d170df1437f5649ed49acebade5.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/121c8d170df1437f5649ed49acebade5.png)'
- en: Image by the Author
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供
- en: From the above table, we can have some interesting takeaways about the range
    of values both entropy and gini index can have. Let’s call the lowest possible
    value as the *lower bound* and the maximum possible value as the *upper bound*.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 从上表中，我们可以得出一些有趣的结论关于熵和基尼指数可能的值范围。我们将最低可能值称为 *下界*，将最大可能值称为 *上界*。
- en: '**Lower Bound**'
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**下界**'
- en: The lower bound of both entropy and gini index is 0 when our data is purely
    homogeneous. Take a look at cart C1 for reference.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们的数据完全同质时，熵和基尼指数的下界都是 0。可以参考 cart C1。
- en: Upper Bound
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 上界
- en: Entropy and Gini Index are 1 and 0.5 respectively when data has the highest
    uncertainty (take a look at cart C2 for reference as C2 represents an example
    having the highest possible randomness).
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 当数据具有最高的不确定性时，熵和基尼指数分别为 1 和 0.5（可以参考 cart C2，C2 代表了具有最高可能随机性的例子）。
- en: One thing to note here is that these values for the upper bound will only hold
    in case of binary classification (because that’s what our two-class apple-banana
    example represents). In the case of **n** classes which are equally likely each
    having probability **(1/n)**, the upper bound will be a function of **n,** as
    shown below.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的一点是，这些上界值仅适用于二分类（因为这就是我们的两类苹果-香蕉示例所代表的情况）。在 **n** 个类别且每个类别的概率相等的情况下，上界将是
    **n** 的一个函数，如下所示。
- en: '**Upper bound for entropy:**'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '**熵的上界：**'
- en: '**For binary classification**, the upper bound of Entropy is 1.'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**对于二分类**，熵的上界是 1。'
- en: '**For multi-class classification with each class having same probability**,
    the upper bound of Entropy will be:'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**对于每个类别具有相同概率的多分类**，熵的上界将是：'
- en: '![](../Images/28f6103ab099b939b7147e8d8234eb7d.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/28f6103ab099b939b7147e8d8234eb7d.png)'
- en: '**Upper bound for Gini Index:**'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '**基尼指数的上界：**'
- en: '**For binary classification,** the upper bound of Gini Index does not usually
    exceed 0.5'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**对于二分类，** 基尼指数的上界通常不超过 0.5'
- en: '**For multi-class classification with each class having same probability**,
    the upper bound of Gini Index will be:'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**对于每个类别具有相同概率的多分类**，基尼指数的上界将是：'
- en: '![](../Images/51970f0c49892dbbafb1e8e4313bd928.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/51970f0c49892dbbafb1e8e4313bd928.png)'
- en: Recap
  id: totrans-54
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 回顾
- en: Entropy and gini index are used to quantify randomness in a dataset and are
    important to determine the quality of split in a decision tree. We can use the
    terms randomness, uncertainty, impurity, and heterogeneity interchangeably here.
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 熵和基尼指数用于量化数据集中的随机性，并且对于确定决策树中分裂的质量非常重要。我们可以在这里交替使用随机性、不确定性、杂质和异质性这些术语。
- en: High values of entropy and gini index mean high randomness in the data.
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 熵和基尼指数的高值意味着数据中的随机性较高。
- en: '**Entropy**'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '**熵**'
- en: Entropy aims to quantify how unpredictable a dataset is.
  id: totrans-58
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 熵旨在量化数据集的不可预测性。
- en: The formula to calculate entropy is given below. Here pi is the probability
    of choosing an element from a class labeled as *i,* given *n* total classes.
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算熵的公式如下。这里 pi 是从标记为 *i* 的类别中选择一个元素的概率，给定 *n* 个总类别。
- en: '![](../Images/72a6fda8cbdbfbc7478e0a1808cfbf51.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/72a6fda8cbdbfbc7478e0a1808cfbf51.png)'
- en: If the data consists of elements belonging to a single class, it becomes highly
    predictable, so the entropy will be the minimum. And minimum value of entropy
    is 0.
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果数据由属于单一类别的元素组成，它变得高度可预测，因此熵将是最小的。而熵的最小值是 0。
- en: When the data consists of elements belonging to n classes which are equally
    likely, each having probability = 1/n, entropy will be the maximum.
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当数据由属于 n 个类别的元素组成且每个类别的概率相等，即每个类别的概率 = 1/n 时，熵将达到最大值。
- en: For binary classification (i.e., data with two classes), the value of entropy
    will never exceed 1.
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于二分类（即，具有两个类别的数据），熵的值不会超过 1。
- en: For multi-class classification, the maximum value of entropy can be generalized
    as log(n). (Here log is to the base of 2.)
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于多分类，熵的最大值可以概括为 log(n)。 (这里的 log 以 2 为底。)
- en: '**Gini Index**'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '**基尼指数**'
- en: Gini index aims to quantify the probability of incorrectly labeling an element
    chosen randomly from the data.
  id: totrans-66
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 基尼指数旨在量化从数据中随机选择一个元素并错误标记的概率。
- en: 'The formula is given below:'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 公式如下：
- en: '![](../Images/b20fd9267e7abec313e8dcc3b181110e.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b20fd9267e7abec313e8dcc3b181110e.png)'
- en: If the data consists of elements belonging to a single class, the probability
    of incorrectly labeling a randomly chosen element will be zero, hence the gini
    index will be the minimum in this case. So, the minimum possible value of gini
    index is also 0.
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果数据由属于单一类别的元素组成，则随机选择一个元素被错误标记的概率将为零，因此在这种情况下基尼指数将是最小的。所以，基尼指数的最小可能值也是0。
- en: When the data consists of elements belonging to n classes with balanced distribution
    i.e. each class has equal probability 1/n, gini index will be the maximum.
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当数据由属于n个类别且分布均衡的元素组成，即每个类别的概率为1/n时，基尼指数将达到最大值。
- en: For binary classification (i.e., data with two distinct classes), the maximum
    value of gini index will never exceed 0.5
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于二分类（即数据包含两个不同的类别），基尼指数的最大值永远不会超过0.5。
- en: For multi-class classification, the maximum value of gini index can be generalized
    as 1-(1/n).
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于多分类，基尼指数的最大值可以概括为1-(1/n)。
