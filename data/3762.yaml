- en: 'Newton’s Laws of Motion: The Original Gradient Descent'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 牛顿运动定律：最初的梯度下降
- en: 原文：[https://towardsdatascience.com/newtons-laws-of-motion-the-original-gradient-descent-a2860037c76f?source=collection_archive---------4-----------------------#2023-12-27](https://towardsdatascience.com/newtons-laws-of-motion-the-original-gradient-descent-a2860037c76f?source=collection_archive---------4-----------------------#2023-12-27)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/newtons-laws-of-motion-the-original-gradient-descent-a2860037c76f?source=collection_archive---------4-----------------------#2023-12-27](https://towardsdatascience.com/newtons-laws-of-motion-the-original-gradient-descent-a2860037c76f?source=collection_archive---------4-----------------------#2023-12-27)
- en: Exploring the shared language of gradient descent and Newton's motion equations
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 探索梯度下降和牛顿运动方程的共享语言
- en: '[](https://medium.com/@rodrigopesilva?source=post_page-----a2860037c76f--------------------------------)[![Rodrigo
    Silva](../Images/d260f05ed9887c5072e0590db1481be2.png)](https://medium.com/@rodrigopesilva?source=post_page-----a2860037c76f--------------------------------)[](https://towardsdatascience.com/?source=post_page-----a2860037c76f--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----a2860037c76f--------------------------------)
    [Rodrigo Silva](https://medium.com/@rodrigopesilva?source=post_page-----a2860037c76f--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@rodrigopesilva?source=post_page-----a2860037c76f--------------------------------)[![Rodrigo
    Silva](../Images/d260f05ed9887c5072e0590db1481be2.png)](https://medium.com/@rodrigopesilva?source=post_page-----a2860037c76f--------------------------------)[](https://towardsdatascience.com/?source=post_page-----a2860037c76f--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----a2860037c76f--------------------------------)
    [Rodrigo Silva](https://medium.com/@rodrigopesilva?source=post_page-----a2860037c76f--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F222e82adf972&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnewtons-laws-of-motion-the-original-gradient-descent-a2860037c76f&user=Rodrigo+Silva&userId=222e82adf972&source=post_page-222e82adf972----a2860037c76f---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----a2860037c76f--------------------------------)
    ·7 min read·Dec 27, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fa2860037c76f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnewtons-laws-of-motion-the-original-gradient-descent-a2860037c76f&user=Rodrigo+Silva&userId=222e82adf972&source=-----a2860037c76f---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F222e82adf972&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnewtons-laws-of-motion-the-original-gradient-descent-a2860037c76f&user=Rodrigo+Silva&userId=222e82adf972&source=post_page-222e82adf972----a2860037c76f---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----a2860037c76f--------------------------------)
    ·7 min read·2023年12月27日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fa2860037c76f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnewtons-laws-of-motion-the-original-gradient-descent-a2860037c76f&user=Rodrigo+Silva&userId=222e82adf972&source=-----a2860037c76f---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fa2860037c76f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnewtons-laws-of-motion-the-original-gradient-descent-a2860037c76f&source=-----a2860037c76f---------------------bookmark_footer-----------)![](../Images/0510803a4ad720b3e2ff6ee725c64c85.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fa2860037c76f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnewtons-laws-of-motion-the-original-gradient-descent-a2860037c76f&source=-----a2860037c76f---------------------bookmark_footer-----------)![](../Images/0510803a4ad720b3e2ff6ee725c64c85.png)'
- en: Photo by [Luddmyla .](https://unsplash.com/@luddmyla?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)
    on [Unsplash](https://unsplash.com/photos/man-playing-skateboard-while-making-tricks-pKSLMEwRpqI?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 照片由 [Luddmyla .](https://unsplash.com/@luddmyla?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)
    提供，来自 [Unsplash](https://unsplash.com/photos/man-playing-skateboard-while-making-tricks-pKSLMEwRpqI?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)
- en: 'I remember the first course on Machine Learning I took during undergrad as
    a physics student in the school of engineering. In other words, I was an outsider.
    While the professor explained the backpropagation algorithm via gradient descent,
    I had this somewhat vague question in my head: "Is gradient descent a random algorithm?"
    Before raising my hand to ask the professor, the non-familiar environment made
    me think twice; I shrunk a little bit. Suddenly, the answer struck me.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 我记得在工程学院读本科时，作为物理学学生，我上了第一门机器学习课程。换句话说，我是个局外人。当教授通过梯度下降解释反向传播算法时，我脑海里有个模糊的问题：“梯度下降是一个随机算法吗？”在举手向教授提问之前，陌生的环境让我犹豫了一下；我稍微缩了回来。突然，答案闪现在我脑海里。
- en: Here's what I thought.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我想到的。
- en: Gradient Descent
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 梯度下降
- en: To say what gradient descent is, first we need to define the problem of training
    a neural network, and we can do this with an overview of how machines learn.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 要说清楚什么是梯度下降，我们首先需要定义训练神经网络的问题，我们可以通过概述机器如何学习来做到这一点。
- en: Overview of a neural network training
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 神经网络训练概述
- en: In all supervised neural network tasks, we have a prediction and the true value.
    The larger the difference between the prediction and the true value, the worse
    our neural network is when it comes to predicting the values. Hence, we create
    a function called the *loss function,* usually denoted as *L*, that quantifies
    how much difference there is between the actual value and the predicted value.
    The task of training the neural network is to update the weights and biases (for
    short, the parameters) to minimize the loss function. That's the big picture of
    training a neural network, and "learning" is simply updating the parameters to
    fit actual data best, i.e., minimizing the loss function.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在所有监督学习的神经网络任务中，我们有一个预测值和真实值。预测值与真实值之间的差异越大，说明我们的神经网络在预测值方面的表现越差。因此，我们创建了一个称为*损失函数*的函数，通常表示为*L*，它量化了实际值与预测值之间的差异。训练神经网络的任务就是更新权重和偏差（简称参数）以最小化损失函数。这就是训练神经网络的大致情况，而“学习”只是更新参数以最佳适应实际数据，即最小化损失函数。
- en: Optimizing via gradient descent
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 通过梯度下降优化
- en: Gradient descent is one of the optimization techniques used to calculate these
    new parameters. Since our task is to choose the parameters to minimize the loss
    function, we need a criterion for such a choice. The loss function that we are
    trying to minimize is a function of the neural network output, so mathematically
    we express it as *L = L*(*y_nn, y*). But the neural network output *y_nn* also
    depends on its parameters, so *y_nn* = *y_nn*(**θ**), where **θ** is a vector
    containing all the parameters of our neural network. In other words, the loss
    function itself is a function of the neural networks' parameters.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度下降是一种用于计算这些新参数的优化技术。由于我们的任务是选择参数以最小化损失函数，我们需要一个选择标准。我们试图最小化的损失函数是神经网络输出的函数，因此在数学上我们将其表示为*L
    = L*(*y_nn, y*)。但神经网络输出*y_nn*也依赖于其参数，所以*y_nn* = *y_nn*(**θ**)，其中**θ**是一个包含我们神经网络所有参数的向量。换句话说，损失函数本身是神经网络参数的一个函数。
- en: Borrowing some concepts from vector calculus, we know that to minimize a function,
    you need to go against its *gradient*, since the gradient points in the direction
    of the fastest increase of the function. To gain some intuition, let's take a
    look at what L(**θ**) might look like in Fig. 1.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 借鉴了一些向量微积分的概念，我们知道要最小化一个函数，你需要逆着它的*梯度*方向前进，因为梯度指向函数最快增长的方向。为了获得一些直觉，我们来看一下图1中L(**θ**)可能的样子。
- en: '![](../Images/1139deaa99d1bfa730554f63b8062840.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1139deaa99d1bfa730554f63b8062840.png)'
- en: 'Figure 1: Surface showing *L(w1,w2) as a function of w1 and w2\. Image by author.*'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：显示*L(w1,w2)作为w1和w2函数的表面。图像由作者提供。*
- en: 'Here, we have a clear intuition of what is desirable and what is not when training
    a neural network: we want the values of the loss function to be smaller, so if
    we start with parameters w1 and w2 that result in a loss function in the yellow/orange
    region, we want to slide down to the surface in the direction of the purple region.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们对训练神经网络时什么是期望的、什么是不期望的有了清晰的直觉：我们希望损失函数的值更小，所以如果我们从使损失函数落在黄色/橙色区域的参数w1和w2开始，我们希望沿着紫色区域的方向滑动下降到表面。
- en: This "sliding down" motion is achieved through the gradient descent method.
    If we are placed at the brightest region on the surface, the gradient will continue
    to point up, since it's the direction of maximally fast increase. Then, going
    in the opposite direction (hence, gradient *descent*) creates a motion onto the
    region of maximally fast decrease.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 这种“滑下”运动是通过梯度下降方法实现的。如果我们处在表面上最亮的区域，梯度将继续指向上方，因为这是增加最快的方向。然后，沿相反方向（因此是梯度*下降*）会产生一个向着最大减少区域的运动。
- en: 'To see this, we can plot the gradient descent vector, as displayed in Fig.
    2\. In this figure, we have a contour plot that shows the same region and the
    same function displayed in Fig. 1, but the values of the loss function are now
    encoded into the color: the brighter, the larger.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 为了看到这一点，我们可以绘制梯度下降向量，如图2所示。在这个图中，我们有一个等高线图，显示了与图1相同的区域和函数，但损失函数的值现在编码为颜色：越亮，值越大。
- en: '![](../Images/3fea759395a7eec55f659093c188b1dd.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3fea759395a7eec55f659093c188b1dd.png)'
- en: 'Figure 2: Contour plot showing the vector pointing in the direction of gradient
    descent. Image by author.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：显示指向梯度下降方向的向量的等高线图。图片由作者提供。
- en: We can see that if we pick a point in the yellow/orange region, the gradient
    descent vector points in the direction that arrives the fastest in the purple
    region.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，如果我们选择一个位于黄色/橙色区域的点，梯度下降向量会指向最快到达紫色区域的方向。
- en: A nice disclaimer is that usually a neural network may contain an arbitrarily
    large number of parameters (GPT 3 has over 100 billion parameters!), which means
    that these nice visualizations are completely unpractical in real-life applications,
    and parameter optimization in neural networks is usually a very high-dimensional
    problem.
  id: totrans-26
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 一个很好的免责声明是，通常一个神经网络可能包含任意多的参数（GPT-3有超过1000亿个参数！），这意味着这些漂亮的可视化在实际应用中完全不实用，神经网络中的参数优化通常是一个非常高维的问题。
- en: Mathematically, the gradient descent algorithm is then given by
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 从数学上讲，梯度下降算法可以表示为
- en: '![](../Images/b58f4f03cc362ecb6bedab4bc1bc52ce.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b58f4f03cc362ecb6bedab4bc1bc52ce.png)'
- en: Here, **θ**_(n+1) are the updated parameters (the result of sliding down the
    surface of Fig. 1); **θ**_(n) are the parameters that we started with; ρ is called
    the learning rate (how big is the step towards the direction where the gradient
    descent is pointing); and ∇L is the gradient of the loss function calculated at
    the initial point **θ**_(n). What gives the name *descent* here is the minus sign
    in front of it.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，**θ**_(n+1) 是更新后的参数（即图1的表面滑下来的结果）；**θ**_(n) 是我们开始时的参数；ρ 被称为学习率（梯度下降指向的方向上的步长）；∇L
    是在初始点 **θ**_(n) 计算的损失函数的梯度。使这里的名字为*下降*的是前面的负号。
- en: Mathematics is key here because we'll see that Newton's Second Law of Motion
    has the same mathematical formulation as the gradient descent equation.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 数学在这里非常关键，因为我们会看到牛顿的运动第二定律与梯度下降方程具有相同的数学公式。
- en: Newton's Second Law
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 牛顿第二定律
- en: 'Newton''s second law of motion is probably one of the most important concepts
    in classical mechanics since it tells how force, mass, and acceleration are tied
    together. Everybody knows the high school formulation of Newton''s second law:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 牛顿的运动第二定律可能是经典力学中最重要的概念之一，因为它说明了力、质量和加速度是如何联系在一起的。每个人都知道牛顿第二定律的高中公式：
- en: '![](../Images/c327ee2b0739daab2e55cac453b94ff7.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c327ee2b0739daab2e55cac453b94ff7.png)'
- en: 'where F is the force, m is the mass, and a the acceleration. However, Newton''s
    original formulation was in terms of a deeper quantity: *momentum*. Momentum is
    the product between the mass and the velocity of a body:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，F 是力，m 是质量，a 是加速度。然而，牛顿原始的公式是基于一个更深层的量：*动量*。动量是物体的质量与速度的乘积：
- en: '![](../Images/86c23dd0fcfa9576402ad0d87154229e.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/86c23dd0fcfa9576402ad0d87154229e.png)'
- en: and can be interpreted as the *quantity of movement* of a body. The idea behind
    Newton's second law is that to change the momentum of a body, you need to disturb
    it somehow, and this disturbance is called a *force*. Hence, a neat formulation
    of Newton's second law is
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 并且可以解释为物体的*运动量*。牛顿第二定律背后的思想是，要改变一个物体的动量，你需要以某种方式干扰它，这种干扰被称为*力*。因此，牛顿第二定律的简洁公式是
- en: '![](../Images/f9989f6a13514f256555efe79f7a782f.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f9989f6a13514f256555efe79f7a782f.png)'
- en: This formulation works for every force you can think of, but we would like a
    little more structure in our discussion and, to gain structure, we need to constrain
    our domain of possibilities. Let's talk about conservative forces and potentials.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 这种公式适用于你能想到的每一种力，但我们希望在讨论中有更多的结构，为了获得结构，我们需要限制我们的可能性范围。让我们讨论保守力和势能。
- en: Conservative forces and potentials
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 保守力和势能
- en: A conservative force is a force that does not dissipate energy. It means that,
    when we are in a system with only conservative forces involved, the total energy
    is a constant. This must sound very restrictive, but in reality, the most fundamental
    forces in nature are conservative, such as gravity and electric force.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 保守力是一种不耗散能量的力。这意味着，当我们处于仅涉及保守力的系统中时，总能量是常量。这听起来很严格，但实际上，自然界中最基本的力量都是保守的，如重力和电力。
- en: For each conservative force, we associate something called a *potential*. This
    potential is related to the force via the equation
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个保守力，我们关联一个称为*势能*的量。这个势能通过公式与力相关联。
- en: '![](../Images/be34c31af4e79f240bf4dd5ea17c8609.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/be34c31af4e79f240bf4dd5ea17c8609.png)'
- en: 'in one dimension. If we take a closer look at the last two equations presented,
    we arrive at the second law of motion for conservative fields:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在一维中。如果我们仔细查看最后两个公式，就会得到保守场的第二运动定律：
- en: '![](../Images/75d544970a15fd52099e4d73e83d767a.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/75d544970a15fd52099e4d73e83d767a.png)'
- en: 'Since derivatives are kind of complicated to deal with, and in computer sciences
    we approximate derivatives as finite differences anyway, let''s replace d''s with
    Δ''s:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 由于导数处理起来有点复杂，并且在计算机科学中我们反正将导数近似为有限差分，因此让我们用Δ替换d：
- en: '![](../Images/c567d46cecf9295ec5d559c1857105e6.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c567d46cecf9295ec5d559c1857105e6.png)'
- en: We know that Δ means "take the updated value and subtract by the current value".
    Hence, we can re-write the formula above as
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 我们知道Δ意味着“取更新值并减去当前值”。因此，我们可以将上述公式重新写成
- en: '![](../Images/31faf4864f2ad24e1dcab43280c27134.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/31faf4864f2ad24e1dcab43280c27134.png)'
- en: 'This already looks pretty similar to the gradient descent equation shown in
    some lines above. To make it even more similar, we just have to look at it in
    three dimensions, where the gradient arises naturally:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 这已经看起来很像上面几行中的梯度下降公式。为了使其更类似，我们只需在三维中查看它，梯度自然会出现：
- en: '![](../Images/061d25617148bb0cda8e33dc704ee48e.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/061d25617148bb0cda8e33dc704ee48e.png)'
- en: We see a clear correspondence between the gradient descent and the formulation
    shown above, which completely derives from Newtonian physics. The momentum of
    a body (and you can read this as velocity if you prefer) will always point toward
    the direction where the potential decreases the fastest, with a step size given
    by Δt.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以清楚地看到梯度下降与上述公式之间的对应关系，这完全源于牛顿物理学。一个物体的动量（如果你愿意，也可以理解为速度）总是指向势能减少最快的方向，步长由Δt给出。
- en: Final words and takeaways
  id: totrans-52
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结束语和要点总结
- en: So we can relate the potential, within the Newtonian formulation, to the loss
    function in machine learning. The momentum vector is similar to the parameter
    vector, which we are trying to optimize, and the time step constant is the learning
    rate, i.e., how fast we are moving towards the minimum of the loss function. Hence,
    the similar mathematical formulation shows that these concepts are tied together
    and present a nice, unified way of looking at them.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们可以将牛顿公式中的势能与机器学习中的损失函数相关联。动量向量类似于我们试图优化的参数向量，时间步长常数即学习率，即我们朝着损失函数最小值移动的速度。因此，类似的数学公式表明这些概念是联系在一起的，并且提供了一种很好的统一视角。
- en: 'If you are wondering, the answer to my question at the beginning is "no". There
    is no randomness in the gradient descent algorithm since it replicates what nature
    does every day: the physical trajectory of a particle always tries to find a way
    to rest in the lowest possible potential around it. If you let a ball fall from
    a certain hight, it will always have the same trajectory, no randomness. When
    you see someone on a skateboard sliding down a steep ramp, remember: that''s literally
    nature applying the gradient descent algorithm.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想知道，我一开始的问题的答案是“不是”。梯度下降算法中没有随机性，因为它复制了自然每天所做的事情：粒子的物理轨迹总是试图在周围找到最低可能的势能。如果你让一个球从某个高度掉落，它总会有相同的轨迹，没有随机性。当你看到有人在滑板上滑下陡峭的坡道时，请记住：那实际上是自然在应用梯度下降算法。
- en: The way we see a problem may influence its solution. In this article, I have
    not shown you anything new in terms of computer science or physics (indeed, the
    physics presented here is ~400 years old), but shifting the perspective and tying
    (apparently) non-related concepts together may create new links and intuitions
    about a topic.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看待问题的方式可能会影响其解决方案。在这篇文章中，我没有展示任何关于计算机科学或物理的新内容（实际上，这里的物理知识已有约400年历史），但改变视角和将（表面上）不相关的概念结合在一起，可能会创造出新的联系和对某一主题的直觉。
- en: References
  id: totrans-56
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] Robert Kwiatkowski, [Gradient Descent Algorithm — a deep dive](/gradient-descent-algorithm-a-deep-dive-cf04e8115f21),
    2021.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] Robert Kwiatkowski, [梯度下降算法——深度探讨](/gradient-descent-algorithm-a-deep-dive-cf04e8115f21)，2021年。'
- en: '[2] Nivaldo A. Lemos, Analytical Mechanics, Cambridge University Press, 2018.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] Nivaldo A. Lemos, 《解析力学》，剑桥大学出版社，2018年。'
