- en: 'L1 vs L2 Regularization in Machine Learning: Differences, Advantages and How
    to Apply Them in Python'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: L1ä¸L2æ­£åˆ™åŒ–åœ¨æœºå™¨å­¦ä¹ ä¸­çš„æ¯”è¾ƒï¼šåŒºåˆ«ã€ä¼˜åŠ¿åŠå¦‚ä½•åœ¨Pythonä¸­åº”ç”¨
- en: åŸæ–‡ï¼š[https://towardsdatascience.com/l1-vs-l2-regularization-in-machine-learning-differences-advantages-and-how-to-apply-them-in-72eb12f102b5](https://towardsdatascience.com/l1-vs-l2-regularization-in-machine-learning-differences-advantages-and-how-to-apply-them-in-72eb12f102b5)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡ï¼š[https://towardsdatascience.com/l1-vs-l2-regularization-in-machine-learning-differences-advantages-and-how-to-apply-them-in-72eb12f102b5](https://towardsdatascience.com/l1-vs-l2-regularization-in-machine-learning-differences-advantages-and-how-to-apply-them-in-72eb12f102b5)
- en: '*Delving into L1 and L2 regularization techniques in Machine Learning to explain
    why they are important to prevent model overfitting*'
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '*æ·±å…¥æ¢è®¨L1å’ŒL2æ­£åˆ™åŒ–æŠ€æœ¯ï¼Œè§£é‡Šå®ƒä»¬ä¸ºä½•å¯¹é˜²æ­¢æ¨¡å‹è¿‡æ‹Ÿåˆè‡³å…³é‡è¦*'
- en: '[](https://medium.com/@theDrewDag?source=post_page-----72eb12f102b5--------------------------------)[![Andrea
    D''Agostino](../Images/58c7c218815f25278aae59cea44d8771.png)](https://medium.com/@theDrewDag?source=post_page-----72eb12f102b5--------------------------------)[](https://towardsdatascience.com/?source=post_page-----72eb12f102b5--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----72eb12f102b5--------------------------------)
    [Andrea D''Agostino](https://medium.com/@theDrewDag?source=post_page-----72eb12f102b5--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@theDrewDag?source=post_page-----72eb12f102b5--------------------------------)[![Andrea
    D''Agostino](../Images/58c7c218815f25278aae59cea44d8771.png)](https://medium.com/@theDrewDag?source=post_page-----72eb12f102b5--------------------------------)[](https://towardsdatascience.com/?source=post_page-----72eb12f102b5--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----72eb12f102b5--------------------------------)
    [Andrea D''Agostino](https://medium.com/@theDrewDag?source=post_page-----72eb12f102b5--------------------------------)'
- en: Â·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----72eb12f102b5--------------------------------)
    Â·8 min readÂ·Feb 23, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: Â·å‘è¡¨åœ¨ [Towards Data Science](https://towardsdatascience.com/?source=post_page-----72eb12f102b5--------------------------------)
    Â·é˜…è¯»æ—¶é—´8åˆ†é’ŸÂ·2023å¹´2æœˆ23æ—¥
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/423a619cb1e90d37b508328d6453e1d2.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/423a619cb1e90d37b508328d6453e1d2.png)'
- en: Image by author.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ç‰‡ç”±ä½œè€…æä¾›ã€‚
- en: Machine Learning is a discipline that is experiencing enormous development in
    the technological and industrial fields.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: æœºå™¨å­¦ä¹ æ˜¯ä¸€ä¸ªåœ¨æŠ€æœ¯å’Œå·¥ä¸šé¢†åŸŸç»å†å·¨å¤§å‘å±•çš„å­¦ç§‘ã€‚
- en: Thanks to its algorithms and modeling techniques, it is possible to build models
    capable of learning from past data, generalizing and making predictions on new
    data.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: å¾—ç›Šäºå…¶ç®—æ³•å’Œå»ºæ¨¡æŠ€æœ¯ï¼Œæˆ‘ä»¬å¯ä»¥æ„å»ºèƒ½å¤Ÿä»è¿‡å»æ•°æ®ä¸­å­¦ä¹ ã€æ³›åŒ–å¹¶å¯¹æ–°æ•°æ®è¿›è¡Œé¢„æµ‹çš„æ¨¡å‹ã€‚
- en: However, in some cases, **models can overfit the training data and lose their
    ability to generalize**. This phenomenon is called *overfitting*.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶è€Œï¼Œåœ¨æŸäº›æƒ…å†µä¸‹ï¼Œ**æ¨¡å‹å¯èƒ½ä¼šè¿‡æ‹Ÿåˆè®­ç»ƒæ•°æ®ï¼Œä»è€Œå¤±å»æ³›åŒ–èƒ½åŠ›**ã€‚è¿™ç§ç°è±¡ç§°ä¸º*è¿‡æ‹Ÿåˆ*ã€‚
- en: It is quite important for analysts to understand what overfitting is and why
    it represents one of the main obstacles in machine learning when it comes to creating
    a predictive model.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹åˆ†æå¸ˆæ¥è¯´ï¼Œç†è§£è¿‡æ‹Ÿåˆæ˜¯ä»€ä¹ˆä»¥åŠä¸ºä»€ä¹ˆå®ƒæ˜¯åˆ›å»ºé¢„æµ‹æ¨¡å‹æ—¶çš„ä¸»è¦éšœç¢ä¹‹ä¸€æ˜¯ç›¸å½“é‡è¦çš„ã€‚
- en: A general idea of overfitting is this one
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: è¿‡æ‹Ÿåˆçš„ä¸€ä¸ªå¤§è‡´æ¦‚å¿µæ˜¯è¿™æ ·çš„
- en: When a model is too complex or fits too well with the training data, it can
    become very accurate for that specific data, but will generalize poorly on data
    it has never seen before. **This means that the model will be ineffective when
    applied to new data in real life**.
  id: totrans-13
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: å½“ä¸€ä¸ªæ¨¡å‹è¿‡äºå¤æ‚æˆ–å¯¹è®­ç»ƒæ•°æ®æ‹Ÿåˆå¾—è¿‡å¥½æ—¶ï¼Œå®ƒå¯èƒ½å¯¹è¿™äº›ç‰¹å®šæ•°æ®éå¸¸å‡†ç¡®ï¼Œä½†å¯¹ä»æœªè§è¿‡çš„æ•°æ®çš„æ³›åŒ–èƒ½åŠ›å·®ã€‚**è¿™æ„å‘³ç€æ¨¡å‹åœ¨ç°å®ç”Ÿæ´»ä¸­åº”ç”¨äºæ–°æ•°æ®æ—¶å°†æ— æ•ˆ**ã€‚
- en: ''
  id: totrans-14
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Want to learn more about overfitting? Read the article titled [*Overcome the
    biggest obstacle in machine learning: Overfitting*](https://medium.com/towards-data-science/overcome-the-biggest-obstacle-in-machine-learning-overfitting-cca026873970)published
    on TDS'
  id: totrans-15
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: æƒ³äº†è§£æ›´å¤šå…³äºè¿‡æ‹Ÿåˆçš„ä¿¡æ¯ï¼Ÿé˜…è¯»æ ‡é¢˜ä¸º [*å…‹æœæœºå™¨å­¦ä¹ ä¸­çš„æœ€å¤§éšœç¢ï¼šè¿‡æ‹Ÿåˆ*](https://medium.com/towards-data-science/overcome-the-biggest-obstacle-in-machine-learning-overfitting-cca026873970)
    çš„æ–‡ç« ï¼Œè¯¥æ–‡ç« å‘è¡¨åœ¨TDSä¸Šã€‚
- en: '**Regularization techniques can be used to prevent overfitting.**'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '**æ­£åˆ™åŒ–æŠ€æœ¯å¯ä»¥ç”¨äºé˜²æ­¢è¿‡æ‹Ÿåˆã€‚**'
- en: The term *regularization* encompasses a set of techniques that **tend to simplify
    a predictive model**. In this article, we will focus on two regularization techniques,
    **L1 and L2**, explain their differences and show how to apply them in Python.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '*æ­£åˆ™åŒ–*ä¸€è¯æ¶µç›–äº†ä¸€ç»„**ç®€åŒ–é¢„æµ‹æ¨¡å‹çš„æŠ€æœ¯**ã€‚åœ¨è¿™ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘ä»¬å°†é‡ç‚¹ä»‹ç»ä¸¤ç§æ­£åˆ™åŒ–æŠ€æœ¯ï¼Œ**L1å’ŒL2**ï¼Œè§£é‡Šå®ƒä»¬çš„åŒºåˆ«ï¼Œå¹¶å±•ç¤ºå¦‚ä½•åœ¨Pythonä¸­åº”ç”¨å®ƒä»¬ã€‚'
- en: What is regularization and why is it important?
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ä»€ä¹ˆæ˜¯æ­£åˆ™åŒ–ï¼Œä¸ºä»€ä¹ˆå®ƒå¾ˆé‡è¦ï¼Ÿ
- en: In simple terms, regularizing a model means changing its learning behavior during
    the training phase.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: ç®€å•æ¥è¯´ï¼Œæ­£åˆ™åŒ–æ¨¡å‹æ„å‘³ç€åœ¨è®­ç»ƒé˜¶æ®µæ”¹å˜å…¶å­¦ä¹ è¡Œä¸ºã€‚
- en: Regularization helps prevent overfitting by **adding a penalty on model complexity**
    â€” if a model is too complex, it will be penalized during training, which helps
    maintain a good balance between the modelâ€™s complexity and its ability to generalize
    about data he has never seen before.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: æ­£åˆ™åŒ–é€šè¿‡ **å¯¹æ¨¡å‹å¤æ‚åº¦æ–½åŠ æƒ©ç½š** æ¥å¸®åŠ©é˜²æ­¢è¿‡æ‹Ÿåˆâ€”â€”å¦‚æœæ¨¡å‹è¿‡äºå¤æ‚ï¼Œåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å°†å—åˆ°æƒ©ç½šï¼Œè¿™æœ‰åŠ©äºä¿æŒæ¨¡å‹å¤æ‚æ€§å’Œå¯¹æœªè§æ•°æ®çš„æ³›åŒ–èƒ½åŠ›ä¹‹é—´çš„è‰¯å¥½å¹³è¡¡ã€‚
- en: To add an L1 or L2 regularization, **we are going to alter the loss function
    of the model**. This is the function that the learning algorithm tries to optimize
    during the training phase.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: è¦æ·»åŠ  L1 æˆ– L2 æ­£åˆ™åŒ–ï¼Œ**æˆ‘ä»¬å°†ä¿®æ”¹æ¨¡å‹çš„æŸå¤±å‡½æ•°**ã€‚è¿™æ˜¯å­¦ä¹ ç®—æ³•åœ¨è®­ç»ƒé˜¶æ®µè¯•å›¾ä¼˜åŒ–çš„å‡½æ•°ã€‚
- en: Regularization occurs by assigning a penalty that increases based on how complex
    the model becomes.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: æ­£åˆ™åŒ–é€šè¿‡åˆ†é…ä¸€ä¸ªåŸºäºæ¨¡å‹å¤æ‚åº¦å¢åŠ çš„æƒ©ç½šæ¥å‘ç”Ÿã€‚
- en: If we take linear regression as an example, MSE (mean squared error â€” read more
    about the evaluation metrics of a regression model here) is the typical loss function
    and can be expressed as
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: ä»¥çº¿æ€§å›å½’ä¸ºä¾‹ï¼ŒMSEï¼ˆå‡æ–¹è¯¯å·®â€”â€”æœ‰å…³å›å½’æ¨¡å‹è¯„ä¼°æŒ‡æ ‡çš„æ›´å¤šä¿¡æ¯è¯·ç‚¹å‡»è¿™é‡Œï¼‰æ˜¯å…¸å‹çš„æŸå¤±å‡½æ•°ï¼Œå¯ä»¥è¡¨ç¤ºä¸º
- en: '![](../Images/76b22fdf87b529c31efc3660b8bd724d.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/76b22fdf87b529c31efc3660b8bd724d.png)'
- en: Where the goal of the algorithm is to minimize the difference between the prediction
    *f(x)* and the observed *y*.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­ç®—æ³•çš„ç›®æ ‡æ˜¯æœ€å°åŒ–é¢„æµ‹å€¼ *f(x)* ä¸è§‚å¯Ÿå€¼ *y* ä¹‹é—´çš„å·®å¼‚ã€‚
- en: In the equation, *f(x)* is the regression line, and this will have be equal
    to
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ–¹ç¨‹ä¸­ï¼Œ*f(x)* æ˜¯å›å½’çº¿ï¼Œè¿™å°†ç­‰äº
- en: '![](../Images/c521b27104c9033d345afdac196b95ad.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c521b27104c9033d345afdac196b95ad.png)'
- en: The algorithm will therefore have to find the values of the parameters *w* and
    *b* from the training set by minimizing MSE.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: å› æ­¤ï¼Œç®—æ³•å°†éœ€è¦é€šè¿‡æœ€å°åŒ– MSE ä»è®­ç»ƒé›†æ‰¾å‡ºå‚æ•° *w* å’Œ *b* çš„å€¼ã€‚
- en: A model is considered less complex if some parameters *w* are close to or equal
    to zero.
  id: totrans-29
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: å¦‚æœä¸€äº›å‚æ•° *w* æ¥è¿‘æˆ–ç­‰äºé›¶ï¼Œåˆ™æ¨¡å‹è¢«è®¤ä¸ºè¾ƒå°‘å¤æ‚ã€‚
- en: L1 vs L2 regularization
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: L1 å’Œ L2 æ­£åˆ™åŒ–
- en: Now letâ€™s see the differences between L1 and L2 regularization.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨è®©æˆ‘ä»¬çœ‹çœ‹ L1 å’Œ L2 æ­£åˆ™åŒ–ä¹‹é—´çš„åŒºåˆ«ã€‚
- en: L1 regularization
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: L1 æ­£åˆ™åŒ–
- en: L1 regularization, **also known as â€œLassoâ€**, adds a penalty on the sum of the
    absolute values of the model weights.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: L1 æ­£åˆ™åŒ–ï¼Œ**ä¹Ÿç§°ä¸ºâ€œå¥—ç´¢â€**ï¼Œå¯¹æ¨¡å‹æƒé‡çš„ç»å¯¹å€¼æ€»å’Œæ–½åŠ æƒ©ç½šã€‚
- en: '**This means that weights that do not contribute much to the model will be
    zeroed**, which can lead to automatic feature selection (as weights corresponding
    to less important features will in fact be zeroed).'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '**è¿™æ„å‘³ç€å¯¹æ¨¡å‹è´¡çŒ®ä¸å¤§çš„æƒé‡å°†è¢«å½’é›¶**ï¼Œè¿™å¯èƒ½å¯¼è‡´è‡ªåŠ¨ç‰¹å¾é€‰æ‹©ï¼ˆå› ä¸ºå¯¹åº”äºä¸é‡è¦ç‰¹å¾çš„æƒé‡å®é™…ä¸Šä¼šè¢«å½’é›¶ï¼‰ã€‚'
- en: This makes L1 particularly useful for feature selection problems and sparse
    models.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä½¿å¾— L1 åœ¨ç‰¹å¾é€‰æ‹©é—®é¢˜å’Œç¨€ç–æ¨¡å‹ä¸­å°¤å…¶æœ‰ç”¨ã€‚
- en: Taking the MSE formula above as an example, an L1 regularization would look
    like this
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: ä»¥ä¸Šè¿° MSE å…¬å¼ä¸ºä¾‹ï¼ŒL1 æ­£åˆ™åŒ–çœ‹èµ·æ¥å¦‚ä¸‹
- en: '![](../Images/5946f1f3ce1469a686f87f68e1975f7d.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5946f1f3ce1469a686f87f68e1975f7d.png)'
- en: where *C* is a **model hyperparameter that controls the intensity of the regularization.**
    The higher the value of *C*, the more our weights will tend towards zero.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­ *C* æ˜¯ä¸€ä¸ª **æ§åˆ¶æ­£åˆ™åŒ–å¼ºåº¦çš„æ¨¡å‹è¶…å‚æ•°**ã€‚*C* çš„å€¼è¶Šé«˜ï¼Œæˆ‘ä»¬çš„æƒé‡è¶Šä¼šè¶‹å‘äºé›¶ã€‚
- en: In jargon, this is would be called **a sparse model**, where most of the parameters
    have the value of zero.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: è¡Œè¯ä¸­ï¼Œè¿™è¢«ç§°ä¸º **ç¨€ç–æ¨¡å‹**ï¼Œå…¶ä¸­å¤§å¤šæ•°å‚æ•°çš„å€¼ä¸ºé›¶ã€‚
- en: The risk here is that **a very high value of *C* will cause the model to underfit**,
    which is the opposite of overfitting â€” i.e. it wonâ€™t capture the patterns in our
    data.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é‡Œçš„é£é™©æ˜¯ **éå¸¸é«˜çš„ *C* å€¼ä¼šå¯¼è‡´æ¨¡å‹æ¬ æ‹Ÿåˆ**ï¼Œè¿™ä¸è¿‡æ‹Ÿåˆç›¸åâ€”â€”å³å®ƒä¸ä¼šæ•æ‰åˆ°æˆ‘ä»¬æ•°æ®ä¸­çš„æ¨¡å¼ã€‚
- en: L2 regularization
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: L2 æ­£åˆ™åŒ–
- en: On the other hand, L2 regularization, also called **Ridge regularization**,
    adds the square of the weights to the regularization term.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: å¦ä¸€æ–¹é¢ï¼ŒL2 æ­£åˆ™åŒ–ï¼Œä¹Ÿç§°ä¸º **å²­å›å½’æ­£åˆ™åŒ–**ï¼Œå°†æƒé‡çš„å¹³æ–¹æ·»åŠ åˆ°æ­£åˆ™åŒ–é¡¹ä¸­ã€‚
- en: '**This means that larger weights are reduced but not zeroed**, which leads
    to models with fewer variables than L1 regularization but with more distributed
    weights.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '**è¿™æ„å‘³ç€è¾ƒå¤§çš„æƒé‡ä¼šè¢«å‡å°‘ä½†ä¸ä¼šè¢«å½’é›¶**ï¼Œè¿™å¯¼è‡´ä¸ L1 æ­£åˆ™åŒ–ç›¸æ¯”ï¼Œæ¨¡å‹çš„å˜é‡è¾ƒå°‘ä½†æƒé‡æ›´åˆ†æ•£ã€‚'
- en: L2 regularization is especially useful when you have many highly correlated
    variables, as it tends to â€œspreadâ€ the weight across all the variables instead
    of focusing on just a few of them.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: L2 æ­£åˆ™åŒ–åœ¨ä½ æœ‰å¾ˆå¤šé«˜åº¦ç›¸å…³çš„å˜é‡æ—¶å°¤å…¶æœ‰ç”¨ï¼Œå› ä¸ºå®ƒå€¾å‘äºâ€œåˆ†æ•£â€æƒé‡åˆ°æ‰€æœ‰å˜é‡ä¸­ï¼Œè€Œä¸æ˜¯åªå…³æ³¨å…¶ä¸­çš„ä¸€äº›ã€‚
- en: As before, letâ€™s see how the initial equation changes to integrate L2
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚å‰æ‰€è¿°ï¼Œè®©æˆ‘ä»¬çœ‹çœ‹åˆå§‹æ–¹ç¨‹å¦‚ä½•å˜åŒ–ä»¥é›†æˆ L2ã€‚
- en: '![](../Images/a0e14dbcb4f39719b2ff763aa445b6b2.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a0e14dbcb4f39719b2ff763aa445b6b2.png)'
- en: L2 regularization can improve model stability when training data is noisy or
    incomplete, by reducing the impact of outliers or noise on variables.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: L2 æ­£åˆ™åŒ–å¯ä»¥æé«˜æ¨¡å‹çš„ç¨³å®šæ€§ï¼Œå°¤å…¶åœ¨è®­ç»ƒæ•°æ®å™ªå£°æˆ–ä¸å®Œæ•´æ—¶ï¼Œé€šè¿‡å‡å°‘å¼‚å¸¸å€¼æˆ–å™ªå£°å¯¹å˜é‡çš„å½±å“ã€‚
- en: How to apply regularization in Sklearn and Python
  id: totrans-48
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å¦‚ä½•åœ¨ Sklearn å’Œ Python ä¸­åº”ç”¨æ­£åˆ™åŒ–
- en: In this example we will see how to applyregularization to a logistic regression
    model for a classification problem.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œæˆ‘ä»¬å°†çœ‹åˆ°å¦‚ä½•å°†æ­£åˆ™åŒ–åº”ç”¨äºé€»è¾‘å›å½’æ¨¡å‹ä»¥è§£å†³åˆ†ç±»é—®é¢˜ã€‚
- en: '**We will see how the performance changes for different values of C** and compare
    how accurate the model is in modeling the input data.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '**æˆ‘ä»¬å°†çœ‹åˆ°æ€§èƒ½å¦‚ä½•éš C å€¼çš„å˜åŒ–è€Œå˜åŒ–**ï¼Œå¹¶æ¯”è¾ƒæ¨¡å‹å¯¹è¾“å…¥æ•°æ®çš„æ‹Ÿåˆå‡†ç¡®æ€§ã€‚'
- en: We will use the famous *breast cancer dataset* from Sklearn. Letâ€™s start by
    seeing how to import it, along with all the libraries.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†ä½¿ç”¨ Sklearn ä¸­è‘—åçš„ *ä¹³è…ºç™Œæ•°æ®é›†*ã€‚è®©æˆ‘ä»¬é¦–å…ˆçœ‹çœ‹å¦‚ä½•å¯¼å…¥å®ƒä»¥åŠæ‰€æœ‰çš„åº“ã€‚
- en: '[PRE0]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Being a classification problem, we will use accuracy to measure the performance
    of the model. Read one of my articles on [how to measure the performance of binary
    classification models](https://medium.com/towards-data-science/the-explanation-you-need-on-binary-classification-metrics-321d280b590f)
    if you are interested in learning more.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: ä½œä¸ºåˆ†ç±»é—®é¢˜ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨å‡†ç¡®ç‡æ¥è¡¡é‡æ¨¡å‹çš„æ€§èƒ½ã€‚å¦‚æœä½ æœ‰å…´è¶£äº†è§£æ›´å¤šï¼Œè¯·é˜…è¯»æˆ‘å…³äº[å¦‚ä½•è¡¡é‡äºŒåˆ†ç±»æ¨¡å‹æ€§èƒ½](https://medium.com/towards-data-science/the-explanation-you-need-on-binary-classification-metrics-321d280b590f)çš„æ–‡ç« ã€‚
- en: Now letâ€™s create a function to apply the comparison between L1 and L2 regularization
    on the dataframe.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨è®©æˆ‘ä»¬åˆ›å»ºä¸€ä¸ªå‡½æ•°æ¥å¯¹ dataframe ä¸Šçš„ L1 å’Œ L2 æ­£åˆ™åŒ–è¿›è¡Œæ¯”è¾ƒã€‚
- en: '[PRE1]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: We apply this logic by looking at the L1 regularization
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬é€šè¿‡è§‚å¯Ÿ L1 æ­£åˆ™åŒ–æ¥åº”ç”¨è¿™ä¸€é€»è¾‘ã€‚
- en: '[PRE2]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '![](../Images/3a79f0cff34cf347a844335ebcbc87b0.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3a79f0cff34cf347a844335ebcbc87b0.png)'
- en: How L1 regularization impacts model performance. Image by author.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: L1 æ­£åˆ™åŒ–å¦‚ä½•å½±å“æ¨¡å‹æ€§èƒ½ã€‚å›¾åƒç”±ä½œè€…æä¾›ã€‚
- en: We see how the L1 regularization flattens the model coefficients close to zero
    for many levels of C. **The coefficients with the highest values are, according
    to the model, the most important features for the prediction.**
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬çœ‹åˆ° L1 æ­£åˆ™åŒ–å¦‚ä½•ä½¿æ¨¡å‹ç³»æ•°åœ¨è®¸å¤š C å€¼ä¸‹æ¥è¿‘äºé›¶ã€‚**æ ¹æ®æ¨¡å‹ï¼Œç³»æ•°å€¼æœ€é«˜çš„æ˜¯é¢„æµ‹ä¸­æœ€é‡è¦çš„ç‰¹å¾**ã€‚
- en: We also see the onset of overfitting â€” at *C=100*, the performance of the training
    set increases while that in the test set decreases.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ä¹Ÿçœ‹åˆ°è¿‡æ‹Ÿåˆçš„å¼€å§‹â€”â€”åœ¨*C=100*æ—¶ï¼Œè®­ç»ƒé›†çš„æ€§èƒ½æé«˜ï¼Œè€Œæµ‹è¯•é›†çš„æ€§èƒ½ä¸‹é™ã€‚
- en: We now apply the same function to evaluate the effects of L2.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ç°åœ¨åº”ç”¨ç›¸åŒçš„å‡½æ•°æ¥è¯„ä¼° L2 çš„æ•ˆæœã€‚
- en: '[PRE3]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '![](../Images/213b3269b4e3e6e88891fe5e904ae8ad.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/213b3269b4e3e6e88891fe5e904ae8ad.png)'
- en: How L2 regularization impacts model performance. Image by author.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: L2 æ­£åˆ™åŒ–å¦‚ä½•å½±å“æ¨¡å‹æ€§èƒ½ã€‚å›¾åƒç”±ä½œè€…æä¾›ã€‚
- en: The coefficients are always above zero, **thus creating a distribution of gradually
    increasing weights for the most relevant features**. We note a very slight overfitting
    after the value of *C=100.*
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: ç³»æ•°æ€»æ˜¯å¤§äºé›¶ï¼Œ**ä»è€Œä¸ºæœ€ç›¸å…³çš„ç‰¹å¾åˆ›å»ºäº†é€æ¸å¢åŠ çš„æƒé‡åˆ†å¸ƒ**ã€‚æˆ‘ä»¬æ³¨æ„åˆ°åœ¨*C=100*æ—¶æœ‰éå¸¸è½»å¾®çš„è¿‡æ‹Ÿåˆã€‚
- en: Other regularization techniques
  id: totrans-67
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å…¶ä»–æ­£åˆ™åŒ–æŠ€æœ¯
- en: In addition to L1 and L2 regularizations, there are other regularization techniques
    that can be used in machine learning models. Among these techniques we find **dropout
    and early stopping.**
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: é™¤äº† L1 å’Œ L2 æ­£åˆ™åŒ–ï¼Œè¿˜æœ‰å…¶ä»–æ­£åˆ™åŒ–æŠ€æœ¯å¯ä»¥åº”ç”¨äºæœºå™¨å­¦ä¹ æ¨¡å‹ã€‚**å…¶ä¸­åŒ…æ‹¬ dropout å’Œ early stopping**ã€‚
- en: Dropout
  id: totrans-69
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Dropout
- en: Dropout is a technique used in neural networks to reduce overfitting. **Dropout
    works by randomly shutting down some neurons during the training phase**, forcing
    the neural network to find alternative ways to represent the data.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: Dropout æ˜¯ä¸€ç§åœ¨ç¥ç»ç½‘ç»œä¸­å‡å°‘è¿‡æ‹Ÿåˆçš„æŠ€æœ¯ã€‚**Dropout é€šè¿‡åœ¨è®­ç»ƒé˜¶æ®µéšæœºå…³é—­ä¸€äº›ç¥ç»å…ƒæ¥å·¥ä½œ**ï¼Œè¿«ä½¿ç¥ç»ç½‘ç»œæ‰¾åˆ°å…¶ä»–æ–¹æ³•æ¥è¡¨ç¤ºæ•°æ®ã€‚
- en: Early stopping
  id: totrans-71
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Early stopping
- en: Early stopping is another technique used to avoid overfitting in machine learning
    models. **This technique consists of stopping model training when the performance
    on the validation set starts to deteriorate**. This prevents the model from overlearning
    the training data and not generalizing well on data not seen before.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: Early stopping æ˜¯å¦ä¸€ç§ç”¨äºé¿å…æœºå™¨å­¦ä¹ æ¨¡å‹è¿‡æ‹Ÿåˆçš„æŠ€æœ¯ã€‚**è¿™ç§æŠ€æœ¯åŒ…æ‹¬åœ¨éªŒè¯é›†ä¸Šçš„æ€§èƒ½å¼€å§‹æ¶åŒ–æ—¶åœæ­¢æ¨¡å‹è®­ç»ƒ**ã€‚è¿™å¯ä»¥é˜²æ­¢æ¨¡å‹è¿‡åº¦å­¦ä¹ è®­ç»ƒæ•°æ®è€Œåœ¨æœªè§è¿‡çš„æ•°æ®ä¸Šæ³›åŒ–æ•ˆæœä¸å¥½ã€‚
- en: Want to learn more about early stopping? Read the article titled [*Early Stopping
    in TensorFlow â€” prevent overfitting of a neural network*](/control-the-training-of-your-neural-network-in-tensorflow-with-callbacks-ba2cc0c2fbe8)published
    on TDS
  id: totrans-73
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: æƒ³äº†è§£æ›´å¤šå…³äºæ—©åœçš„ä¿¡æ¯ï¼Ÿé˜…è¯»æ ‡é¢˜ä¸º[*TensorFlow ä¸­çš„æ—©åœ - é˜²æ­¢ç¥ç»ç½‘ç»œè¿‡æ‹Ÿåˆ*](/control-the-training-of-your-neural-network-in-tensorflow-with-callbacks-ba2cc0c2fbe8)çš„æ–‡ç« ï¼Œå‘è¡¨äº
    TDS
- en: In general, overfitting can be avoided by using a combination of regularization
    techniques. However, the choice of the most appropriate techniques will depend
    on the characteristics of the dataset and the machine learning model used.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€èˆ¬æ¥è¯´ï¼Œé€šè¿‡ä½¿ç”¨æ­£åˆ™åŒ–æŠ€æœ¯çš„ç»„åˆå¯ä»¥é¿å…è¿‡æ‹Ÿåˆã€‚ç„¶è€Œï¼Œæœ€åˆé€‚çš„æŠ€æœ¯é€‰æ‹©å°†å–å†³äºæ•°æ®é›†å’Œæ‰€ç”¨æœºå™¨å­¦ä¹ æ¨¡å‹çš„ç‰¹æ€§ã€‚
- en: Conclusions
  id: totrans-75
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ€»ç»“
- en: In conclusion, regularization is an important machine learning technique that
    helps improve model performance **by avoiding overfitting on training data.**
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: æ€»ä¹‹ï¼Œæ­£åˆ™åŒ–æ˜¯ä¸€ç§é‡è¦çš„æœºå™¨å­¦ä¹ æŠ€æœ¯ï¼Œæœ‰åŠ©äºæé«˜æ¨¡å‹æ€§èƒ½**é€šè¿‡é¿å…åœ¨è®­ç»ƒæ•°æ®ä¸Šè¿‡æ‹Ÿåˆã€‚**
- en: L1 and L2 regularizations are the most used techniques for this purpose, but
    there are others too that can be useful depending on context. For example, dropout
    is almost always seen in the context of deep learning, that is with neural networks.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: L1 å’Œ L2 æ­£åˆ™åŒ–æ˜¯æœ€å¸¸ç”¨çš„æŠ€æœ¯ï¼Œä½†æ ¹æ®ä¸Šä¸‹æ–‡ï¼Œè¿˜æœ‰å…¶ä»–å¯èƒ½æœ‰ç”¨çš„æŠ€æœ¯ã€‚ä¾‹å¦‚ï¼Œdropout å‡ ä¹æ€»æ˜¯å‡ºç°åœ¨æ·±åº¦å­¦ä¹ ä¸­ï¼Œå³ç¥ç»ç½‘ç»œä¸­ã€‚
- en: In our example we have seen how regularization affects the performance of the
    logistic regression model and how the value of C affects the regularization itself.
    We also examined how model coefficients change as the value of C changes, and
    how L1 and L2 regularization affect model coefficients differently.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æˆ‘ä»¬çš„ä¾‹å­ä¸­ï¼Œæˆ‘ä»¬çœ‹åˆ°äº†æ­£åˆ™åŒ–å¦‚ä½•å½±å“é€»è¾‘å›å½’æ¨¡å‹çš„æ€§èƒ½ï¼Œä»¥åŠ C çš„å€¼å¦‚ä½•å½±å“æ­£åˆ™åŒ–æœ¬èº«ã€‚æˆ‘ä»¬è¿˜æ£€æŸ¥äº† C å€¼å˜åŒ–æ—¶æ¨¡å‹ç³»æ•°çš„å˜åŒ–ï¼Œä»¥åŠ L1 å’Œ
    L2 æ­£åˆ™åŒ–å¦‚ä½•ä»¥ä¸åŒçš„æ–¹å¼å½±å“æ¨¡å‹ç³»æ•°ã€‚
- en: Thank you for taking the time to read my article! ğŸ˜Š
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: æ„Ÿè°¢æ‚¨æŠ½å‡ºæ—¶é—´é˜…è¯»æˆ‘çš„æ–‡ç« ï¼ğŸ˜Š
- en: Till next time!
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸‹æ¬¡è§ï¼
- en: '**If you want to support my content creation activity, feel free to follow
    my referral link below and join Mediumâ€™s membership program**. I will receive
    a portion of your investment and youâ€™ll be able to access Mediumâ€™s plethora of
    articles on data science and more in a seamless way.'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '**å¦‚æœæ‚¨æƒ³æ”¯æŒæˆ‘çš„å†…å®¹åˆ›ä½œæ´»åŠ¨ï¼Œè¯·éšæ—¶é€šè¿‡ä»¥ä¸‹æ¨èé“¾æ¥åŠ å…¥ Medium çš„ä¼šå‘˜è®¡åˆ’**ã€‚æˆ‘å°†è·å¾—æ‚¨æŠ•èµ„çš„ä¸€éƒ¨åˆ†ï¼Œæ‚¨å°†èƒ½å¤Ÿæ— ç¼è®¿é—® Medium ä¸Šçš„æ•°æ®ç§‘å­¦ç­‰ä¼—å¤šæ–‡ç« ã€‚'
- en: '[](https://medium.com/@theDrewDag/membership?source=post_page-----72eb12f102b5--------------------------------)
    [## Join Medium with my referral link - Andrea D''Agostino'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@theDrewDag/membership?source=post_page-----72eb12f102b5--------------------------------)
    [## é€šè¿‡æˆ‘çš„æ¨èé“¾æ¥åŠ å…¥ Medium - Andrea D''Agostino'
- en: Read every story from Andrea D'Agostino (and thousands of other writers on Medium).
    Your membership fee directlyâ€¦
  id: totrans-83
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: é˜…è¯» Andrea D'Agostino çš„æ¯ä¸ªæ•…äº‹ï¼ˆä»¥åŠ Medium ä¸Šå…¶ä»–æ•°åƒåä½œè€…çš„æ•…äº‹ï¼‰ã€‚æ‚¨çš„ä¼šå‘˜è´¹ç›´æ¥â€¦
- en: medium.com](https://medium.com/@theDrewDag/membership?source=post_page-----72eb12f102b5--------------------------------)
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: medium.com](https://medium.com/@theDrewDag/membership?source=post_page-----72eb12f102b5--------------------------------)
- en: Recommended Reads
  id: totrans-85
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ¨èé˜…è¯»
- en: For the interested, here are a list of books that I recommended for each ML-related
    topic. There are ESSENTIAL books in my opinion and have greatly impacted my professional
    career.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹æ„Ÿå…´è¶£çš„è¯»è€…ï¼Œè¿™é‡Œæœ‰æˆ‘æ¨èçš„æ¯ä¸ªä¸æœºå™¨å­¦ä¹ ç›¸å…³ä¸»é¢˜çš„ä¹¦å•ã€‚è¿™äº›ä¹¦åœ¨æˆ‘çœ‹æ¥æ˜¯**å¿…è¯»çš„**ï¼Œå¯¹æˆ‘çš„èŒä¸šç”Ÿæ¶¯äº§ç”Ÿäº†é‡å¤§å½±å“ã€‚
- en: '*Disclaimer: these are Amazon affiliate links. I will receive a small commission
    from Amazon for referring you these items. Your experience wonâ€™t change and you
    wonâ€™t be charged more, but it will help me scale my business and produce even
    more content around AI.*'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '*å…è´£å£°æ˜ï¼šè¿™äº›æ˜¯äºšé©¬é€Šé™„å±é“¾æ¥ã€‚æˆ‘å°†å› æ¨èè¿™äº›å•†å“è€Œä»äºšé©¬é€Šè·å¾—å°‘é‡ä½£é‡‘ã€‚æ‚¨çš„ä½“éªŒä¸ä¼šæ”¹å˜ï¼Œä¹Ÿä¸ä¼šé¢å¤–æ”¶è´¹ï¼Œä½†è¿™å°†å¸®åŠ©æˆ‘æ‰©å¤§ä¸šåŠ¡ï¼Œå¹¶åˆ¶ä½œæ›´å¤šå…³äº AI
    çš„å†…å®¹ã€‚*'
- en: '**Intro to ML**: [*Confident Data Skills: Master the Fundamentals of Working
    with Data and Supercharge Your Career*](https://amzn.to/3WZ51cE)by Kirill Eremenko'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**æœºå™¨å­¦ä¹ ç®€ä»‹**: [*Confident Data Skills: Master the Fundamentals of Working with
    Data and Supercharge Your Career*](https://amzn.to/3WZ51cE)ç”± Kirill Eremenko è‘—'
- en: '**Sklearn / TensorFlow**: [*Hands-On Machine Learning with Scikit-Learn, Keras,
    and TensorFlow*](https://amzn.to/3jseVGb) by Aurelien GÃ©ron'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Sklearn / TensorFlow**: [*Hands-On Machine Learning with Scikit-Learn, Keras,
    and TensorFlow*](https://amzn.to/3jseVGb) ç”± Aurelien GÃ©ron è‘—'
- en: '**NLP**: [*Text as Data: A New Framework for Machine Learning and the Social
    Sciences*](https://amzn.to/3l9FO22)by Justin Grimmer'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**è‡ªç„¶è¯­è¨€å¤„ç†**: [*Text as Data: A New Framework for Machine Learning and the Social
    Sciences*](https://amzn.to/3l9FO22)ç”± Justin Grimmer è‘—'
- en: '**Sklearn / PyTorch**: [*Machine Learning with PyTorch and Scikit-Learn: Develop
    machine learning and deep learning models with Python*](https://amzn.to/3wYZf0e)
    by Sebastian Raschka'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Sklearn / PyTorch**: [*ä½¿ç”¨ PyTorch å’Œ Scikit-Learn è¿›è¡Œæœºå™¨å­¦ä¹ ï¼šç”¨ Python å¼€å‘æœºå™¨å­¦ä¹ å’Œæ·±åº¦å­¦ä¹ æ¨¡å‹*](https://amzn.to/3wYZf0e)
    ä½œè€… Sebastian Raschka'
- en: '**Data Viz**: [*Storytelling with Data: A Data Visualization Guide for Business
    Professionals*](https://amzn.to/3HUtGtB) by Cole Knaflic'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**æ•°æ®å¯è§†åŒ–**: [*ç”¨æ•°æ®è®²æ•…äº‹ï¼šå•†ä¸šä¸“ä¸šäººå£«çš„æ•°æ®å¯è§†åŒ–æŒ‡å—*](https://amzn.to/3HUtGtB) ä½œè€… Cole Knaflic'
- en: Useful Links (written by me)
  id: totrans-93
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æœ‰ç”¨çš„é“¾æ¥ï¼ˆç”±æˆ‘ç¼–å†™ï¼‰
- en: '**Learn how to perform a top-tier Exploratory Data Analysis in Python**: [*Exploratory
    Data Analysis in Python â€” A Step-by-Step Process*](/exploratory-data-analysis-in-python-a-step-by-step-process-d0dfa6bf94ee)'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**äº†è§£å¦‚ä½•åœ¨ Python ä¸­æ‰§è¡Œé¡¶çº§æ¢ç´¢æ€§æ•°æ®åˆ†æ**: [*Python ä¸­çš„æ¢ç´¢æ€§æ•°æ®åˆ†æ â€” æ­¥éª¤æŒ‡å—*](/exploratory-data-analysis-in-python-a-step-by-step-process-d0dfa6bf94ee)'
- en: '**Learn the basics of TensorFlow**: [*Get started with TensorFlow 2.0 â€” Introduction
    to deep learning*](https://medium.com/towards-data-science/a-comprehensive-introduction-to-tensorflows-sequential-api-and-model-for-deep-learning-c5e31aee49fa)'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**å­¦ä¹  TensorFlow çš„åŸºç¡€çŸ¥è¯†**: [*å¼€å§‹ä½¿ç”¨ TensorFlow 2.0 â€” æ·±åº¦å­¦ä¹ ç®€ä»‹*](https://medium.com/towards-data-science/a-comprehensive-introduction-to-tensorflows-sequential-api-and-model-for-deep-learning-c5e31aee49fa)'
- en: '**Perform text clustering with TF-IDF in Python**: [*Text Clustering with TF-IDF
    in Python*](https://medium.com/mlearning-ai/text-clustering-with-tf-idf-in-python-c94cd26a31e7)'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**åœ¨ Python ä¸­ä½¿ç”¨ TF-IDF æ‰§è¡Œæ–‡æœ¬èšç±»**: [*ä½¿ç”¨ TF-IDF åœ¨ Python ä¸­è¿›è¡Œæ–‡æœ¬èšç±»*](https://medium.com/mlearning-ai/text-clustering-with-tf-idf-in-python-c94cd26a31e7)'
