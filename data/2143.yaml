- en: 'Domain Adaptation: Fine-Tune Pre-Trained NLP Models'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 领域适应：微调预训练的 NLP 模型
- en: 原文：[https://towardsdatascience.com/domain-adaption-fine-tune-pre-trained-nlp-models-a06659ca6668?source=collection_archive---------1-----------------------#2023-07-04](https://towardsdatascience.com/domain-adaption-fine-tune-pre-trained-nlp-models-a06659ca6668?source=collection_archive---------1-----------------------#2023-07-04)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/domain-adaption-fine-tune-pre-trained-nlp-models-a06659ca6668?source=collection_archive---------1-----------------------#2023-07-04](https://towardsdatascience.com/domain-adaption-fine-tune-pre-trained-nlp-models-a06659ca6668?source=collection_archive---------1-----------------------#2023-07-04)
- en: '![](../Images/60b46d488ea28b302a949054bded404e.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/60b46d488ea28b302a949054bded404e.png)'
- en: Photo by [Pietro Jeng](https://unsplash.com/@pietrozj?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由 [Pietro Jeng](https://unsplash.com/@pietrozj?utm_source=medium&utm_medium=referral)
    提供，来源于 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: '[Hands-on Tutorials](https://towardsdatascience.com/tagged/hands-on-tutorials)'
  id: totrans-4
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '[实用教程](https://towardsdatascience.com/tagged/hands-on-tutorials)'
- en: A step-by-step guide to fine-tuning pre-trained NLP models for any domain
  id: totrans-5
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 关于为任何领域微调预训练 NLP 模型的逐步指南
- en: '[](https://medium.com/@shashank.kapadia?source=post_page-----a06659ca6668--------------------------------)[![Shashank
    Kapadia](../Images/347e4cb92a7d27f032c5761e4526f2fa.png)](https://medium.com/@shashank.kapadia?source=post_page-----a06659ca6668--------------------------------)[](https://towardsdatascience.com/?source=post_page-----a06659ca6668--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----a06659ca6668--------------------------------)
    [Shashank Kapadia](https://medium.com/@shashank.kapadia?source=post_page-----a06659ca6668--------------------------------)'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@shashank.kapadia?source=post_page-----a06659ca6668--------------------------------)[![Shashank
    Kapadia](../Images/347e4cb92a7d27f032c5761e4526f2fa.png)](https://medium.com/@shashank.kapadia?source=post_page-----a06659ca6668--------------------------------)[](https://towardsdatascience.com/?source=post_page-----a06659ca6668--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----a06659ca6668--------------------------------)
    [Shashank Kapadia](https://medium.com/@shashank.kapadia?source=post_page-----a06659ca6668--------------------------------)'
- en: ·
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fcc7314ace45c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdomain-adaption-fine-tune-pre-trained-nlp-models-a06659ca6668&user=Shashank+Kapadia&userId=cc7314ace45c&source=post_page-cc7314ace45c----a06659ca6668---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----a06659ca6668--------------------------------)
    ·9 min read·Jul 4, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fa06659ca6668&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdomain-adaption-fine-tune-pre-trained-nlp-models-a06659ca6668&user=Shashank+Kapadia&userId=cc7314ace45c&source=-----a06659ca6668---------------------clap_footer-----------)'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fcc7314ace45c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdomain-adaption-fine-tune-pre-trained-nlp-models-a06659ca6668&user=Shashank+Kapadia&userId=cc7314ace45c&source=post_page-cc7314ace45c----a06659ca6668---------------------post_header-----------)
    发布于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----a06659ca6668--------------------------------)
    ·9 min read·Jul 4, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fa06659ca6668&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdomain-adaption-fine-tune-pre-trained-nlp-models-a06659ca6668&user=Shashank+Kapadia&userId=cc7314ace45c&source=-----a06659ca6668---------------------clap_footer-----------)'
- en: --
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fa06659ca6668&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdomain-adaption-fine-tune-pre-trained-nlp-models-a06659ca6668&source=-----a06659ca6668---------------------bookmark_footer-----------)'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fa06659ca6668&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdomain-adaption-fine-tune-pre-trained-nlp-models-a06659ca6668&source=-----a06659ca6668---------------------bookmark_footer-----------)'
- en: '*Preface: This article presents a summary of information about the given topic.
    It should not be considered original research. The information and code included
    in this article have may be influenced by things I have read or seen in the past
    from various online articles, research papers, books, and open-source code.*'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '*前言：本文总结了有关给定主题的信息。这不应被视为原创研究。本文中包含的信息和代码可能受到我过去阅读或看到的各种在线文章、研究论文、书籍和开源代码的影响。*'
- en: '*Co-Author:* [*Billy Hines*](https://medium.com/u/f6bf67256839?source=post_page-----a06659ca6668--------------------------------)'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '*合著者：* [*比利·海因斯*](https://medium.com/u/f6bf67256839?source=post_page-----a06659ca6668--------------------------------)'
- en: Table of Content
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 内容表
- en: Introduction
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 引言
- en: Theoretical Framework
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理论框架
- en: Data Overview
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据概览
- en: 'Starting Point: The Baseline Model'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 起点：基础模型
- en: Fine-Tuning the Model
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 微调模型
- en: Evaluating the Results
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 结果评估
- en: Closing Thoughts
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 结束语
- en: Introduction
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 引言
- en: In today’s world, the availability of pre-trained NLP models has greatly simplified
    the interpretation of textual data using deep learning techniques. However, while
    these models excel in general tasks, they often lack adaptability to specific
    domains. This comprehensive guide aims to walk you through the process of fine-tuning
    pre-trained NLP models to achieve improved performance in a particular domain.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在当今世界，预训练NLP模型的可用性大大简化了使用深度学习技术解释文本数据的过程。然而，虽然这些模型在一般任务中表现优异，但它们在特定领域的适应性往往不足。本指南旨在带您了解如何微调预训练NLP模型，以在特定领域获得更好的性能。
- en: Motivation
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 动机
- en: Although pre-trained NLP models like BERT and the Universal Sentence Encoder
    (USE) are effective in capturing linguistic intricacies, their performance in
    domain-specific applications can be limited due to the diverse range of datasets
    they are trained on. This limitation becomes evident when analyzing relationships
    within a specific domain.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管像BERT和通用句子编码器（USE）这样的预训练NLP模型在捕捉语言细节方面很有效，但由于其训练所用数据集的多样性，它们在特定领域应用中的表现可能受到限制。这种限制在分析特定领域内的关系时尤为明显。
- en: For example, when working with employment data, we expect the model to recognize
    the closer proximity between the roles of ‘Data Scientist’ and ‘Machine Learning
    Engineer’, or the stronger association between ‘Python’ and ‘TensorFlow’. Unfortunately,
    general-purpose models often miss these nuanced relationships.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在处理就业数据时，我们希望模型能识别出“数据科学家”和“机器学习工程师”角色之间的更紧密关系，或“Python”和“TensorFlow”之间的更强关联。不幸的是，通用模型往往会遗漏这些细微的关系。
- en: 'The table below demonstrates the discrepancies in the similarity obtained from
    a base multilingual USE model:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 下表演示了从基础多语言USE模型中获得的相似度差异：
- en: '![](../Images/fd70480995a62829117960a1a4fa1d88.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fd70480995a62829117960a1a4fa1d88.png)'
- en: Fig 1\. Similarity score between two text vectors from base [MultiLingual Universal
    Sentence Encoder Model](https://tfhub.dev/google/universal-sentence-encoder-multilingual/3)
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：基础[多语言通用句子编码器模型](https://tfhub.dev/google/universal-sentence-encoder-multilingual/3)中两个文本向量之间的相似度得分
- en: To address this issue, we can fine-tune pre-trained models with high-quality,
    domain-specific datasets. This adaptation process significantly enhances the model’s
    performance and precision, fully unlocking the potential of the NLP model.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这个问题，我们可以通过高质量的领域特定数据集微调预训练模型。这一适应过程显著提升了模型的性能和精准度，充分释放了NLP模型的潜力。
- en: When dealing with large pre-trained NLP models, it is advisable to initially
    deploy the base model and consider fine-tuning only if its performance falls short
    for the specific problem at hand.
  id: totrans-30
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 处理大型预训练NLP模型时，建议先部署基础模型，只有在其性能对具体问题不满足要求时才考虑微调。
- en: This tutorial focuses on fine-tuning the Universal Sentence Encoder (USE) model
    using easily accessible open-source data.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 本教程专注于使用易于获取的开源数据微调通用句子编码器（USE）模型。
- en: Theoretical Overview
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理论概述
- en: Fine-tuning an ML model can be achieved through various strategies, such as
    supervised learning and reinforcement learning. In this tutorial, we will concentrate
    on a one(few)-shot learning approach combined with a siamese architecture for
    the fine-tuning process.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 微调ML模型可以通过多种策略实现，例如监督学习和强化学习。在本教程中，我们将集中于结合了孪生网络架构的一次（少次）学习方法。
- en: Methodology
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 方法论
- en: In this tutorial, we utilize a siamese neural network, which is a specific type
    of Artificial Neural Network. This network leverages shared weights while simultaneously
    processing two distinct input vectors to compute comparable output vectors. Inspired
    by one-shot learning, this approach has proven to be particularly effective in
    capturing semantic similarity, although it may require longer training times and
    lack probabilistic output.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在本教程中，我们使用了一种 Siamese 神经网络，这是一种特定类型的人工神经网络。该网络利用共享权重，同时处理两个不同的输入向量，以计算可比较的输出向量。受一次性学习的启发，这种方法在捕捉语义相似性方面特别有效，尽管可能需要较长的训练时间，并且缺乏概率输出。
- en: A Siamese Neural Network creates an ‘embedding space’ where related concepts
    are positioned closely, enabling the model to better discern semantic relations.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: Siamese 神经网络创建了一个‘嵌入空间’，在该空间中，相关的概念被紧密地放置，使模型能够更好地辨别语义关系。
- en: '![](../Images/e04c290c089ac82b1e99511c2e8c6df0.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e04c290c089ac82b1e99511c2e8c6df0.png)'
- en: Fig 2\. Siamese architecture to fine-tune pre-trained NLP model
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2\. Siamese 架构用于微调预训练 NLP 模型
- en: '**Twin Branches and Shared Weights**: The architecture consists of two identical
    branches, each containing an embedding layer with shared weights. These dual branches
    handle two inputs simultaneously, either similar or dissimilar.'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**双分支与共享权重**：该架构由两个相同的分支组成，每个分支包含一个具有共享权重的嵌入层。这两个分支同时处理两个输入，无论是相似的还是不相似的。'
- en: '**Similarity and Transformation**: The inputs are transformed into vector embeddings
    using the pre-trained NLP model. The architecture then calculates the similarity
    between the vectors. The similarity score, ranging between -1 and 1, quantifies
    the angular distance between the two vectors, serving as a metric for their semantic
    similarity.'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**相似性与转换**：输入通过预训练的 NLP 模型转换为向量嵌入。然后，架构计算这些向量之间的相似性。相似性分数在 -1 和 1 之间，量化两个向量之间的角距离，作为它们语义相似性的度量。'
- en: '**Contrastive Loss and Learning**: The model’s learning is guided by the “Contrastive
    Loss,” which is the difference between the expected output (similarity score from
    the training data) and the computed similarity. This loss guides the adjustment
    of the model’s weights to minimize the loss and enhance the quality of the learned
    embeddings.'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**对比损失与学习**：模型的学习由“对比损失”引导，该损失是期望输出（来自训练数据的相似性分数）与计算出的相似性之间的差异。这个损失指导模型权重的调整，以最小化损失并提高学习嵌入的质量。'
- en: 'To learn more about one(few)-shot learning, siamese architecture, and contrastive
    loss, refer to the following resources:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解更多关于一次（少量）学习、Siamese 架构和对比损失的信息，请参阅以下资源：
- en: '[](https://www.projectpro.io/article/siamese-neural-networks/718?source=post_page-----a06659ca6668--------------------------------)
    [## A Gentle Introduction to Siamese Neural Networks Architecture'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://www.projectpro.io/article/siamese-neural-networks/718?source=post_page-----a06659ca6668--------------------------------)
    [## Siamese 神经网络架构简介'
- en: 'Siamese Neural Networks Architecture : An Overview and Key Concepts Explained
    with Examples | ProjectPro'
  id: totrans-44
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: “Siamese 神经网络架构：概述和关键概念解释与示例 | ProjectPro
- en: www.projectpro.io](https://www.projectpro.io/article/siamese-neural-networks/718?source=post_page-----a06659ca6668--------------------------------)
    [](https://bdtechtalks.com/2020/08/12/what-is-one-shot-learning/?source=post_page-----a06659ca6668--------------------------------)
    [## What is one-shot learning? — TechTalks
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '[www.projectpro.io](https://www.projectpro.io/article/siamese-neural-networks/718?source=post_page-----a06659ca6668--------------------------------)
    [## 什么是一次性学习？ — TechTalks'
- en: One-shot learning allows deep learning algorithms to measure the similarity
    and difference between two images.
  id: totrans-46
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 一次性学习允许深度学习算法测量两个图像之间的相似性和差异。
- en: bdtechtalks.com](https://bdtechtalks.com/2020/08/12/what-is-one-shot-learning/?source=post_page-----a06659ca6668--------------------------------)
    [](/contrastive-loss-explaned-159f2d4a87ec?source=post_page-----a06659ca6668--------------------------------)
    [## Contrastive Loss Explained
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '[对比损失解释](https://bdtechtalks.com/2020/08/12/what-is-one-shot-learning/?source=post_page-----a06659ca6668--------------------------------)
    [## 对比损失解释'
- en: Contrastive loss has been used recently in a number of papers showing state
    of the art results with unsupervised…
  id: totrans-48
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 对比损失最近在许多论文中得到应用，展示了最先进的无监督结果…
- en: towardsdatascience.com](/contrastive-loss-explaned-159f2d4a87ec?source=post_page-----a06659ca6668--------------------------------)
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '[towardsdatascience.com](/contrastive-loss-explaned-159f2d4a87ec?source=post_page-----a06659ca6668--------------------------------)'
- en: The complete code is available as a [Jupyter Notebook on GitHub](https://github.com/kapadias/medium-articles/blob/master/natural-language-processing/embedding-models/domain_adaption_fine_tune_nlp_model.ipynb)
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 完整代码可以在 [GitHub上的Jupyter Notebook](https://github.com/kapadias/medium-articles/blob/master/natural-language-processing/embedding-models/domain_adaption_fine_tune_nlp_model.ipynb)
    中找到
- en: Data Overview
  id: totrans-51
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据概述
- en: For the fine-tuning of pre-trained NLP models using this method, the training
    data should consist of pairs of text strings accompanied by similarity scores
    between them.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 对于使用这种方法对预训练 NLP 模型进行微调，训练数据应包含文本字符串对及其间的相似度评分。
- en: 'The training data follows the format shown below:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 训练数据遵循以下格式：
- en: '![](../Images/002a9075851dd4c80026dc760b2c3255.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/002a9075851dd4c80026dc760b2c3255.png)'
- en: Fig 3\. Sample Format for Training Data
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 图3\. 训练数据的示例格式
- en: In this tutorial, we use a dataset sourced from the [ESCO classification dataset](https://esco.ec.europa.eu/en),
    which has been transformed to generate similarity scores based on the relationships
    between different data elements.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在本教程中，我们使用来自 [ESCO 分类数据集](https://esco.ec.europa.eu/en) 的数据集，这些数据集已被转换为根据不同数据元素之间的关系生成相似度评分。
- en: Preparing the training data is a crucial step in the fine-tuning process. It
    is assumed that you have access to the required data and a method to transform
    it into the specified format. Since the focus of this article is to demonstrate
    the fine-tuning process, we will omit the details of how the data was generated
    using the ESCO dataset.
  id: totrans-57
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 准备训练数据是微调过程中的关键步骤。假设你可以访问所需的数据并有方法将其转换成指定格式。由于本文重点展示微调过程，我们将省略如何使用 ESCO 数据集生成数据的细节。
- en: ''
  id: totrans-58
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The ESCO dataset is available for developers to freely utilize as a foundation
    for various applications that offer services like autocomplete, suggestion systems,
    job search algorithms, and job matching algorithms. The dataset used in this tutorial
    has been transformed and provided as a sample, allowing unrestricted usage for
    any purpose.
  id: totrans-59
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ESCO 数据集可供开发者自由使用，作为提供自动补全、建议系统、职位搜索算法和职位匹配算法等服务的各种应用的基础。本教程使用的数据集已被转换并作为示例提供，允许无限制地用于任何目的。
- en: 'Let’s start by examining the training data:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始检查训练数据：
- en: '[PRE0]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '![](../Images/f63c7e896d9f23fa4d83168d3cf29e1f.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f63c7e896d9f23fa4d83168d3cf29e1f.png)'
- en: Fig 4\. Sample data used for fine-tuning the model
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 图4\. 用于微调模型的示例数据
- en: 'Starting Point: The Baseline Model'
  id: totrans-64
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 起点：基线模型
- en: To begin, we establish the [multilingual universal sentence encoder](https://tfhub.dev/google/universal-sentence-encoder-multilingual/3)
    as our baseline model. It is essential to set this baseline before proceeding
    with the fine-tuning process.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们建立 [多语言通用句子编码器](https://tfhub.dev/google/universal-sentence-encoder-multilingual/3)
    作为我们的基线模型。在进行微调过程之前，设定这个基线模型是至关重要的。
- en: For this tutorial, we will use the STS benchmark and a sample similarity visualization
    as metrics to evaluate the changes and improvements achieved through the fine-tuning
    process.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在本教程中，我们将使用 STS 基准和示例相似度可视化作为度量指标，以评估微调过程中的变化和改进。
- en: The STS Benchmark dataset consists of English sentence pairs, each associated
    with a similarity score. During the model training process, we evaluate the model’s
    performance on this benchmark set. The persisted scores for each training run
    are the Pearson correlation between the predicted similarity scores and the actual
    similarity scores in the dataset.
  id: totrans-67
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: STS Benchmark 数据集由英文句子对组成，每对句子都关联有一个相似度评分。在模型训练过程中，我们会在这个基准集上评估模型的性能。每次训练的持久化评分是预测相似度评分与数据集中实际相似度评分之间的皮尔逊相关系数。
- en: ''
  id: totrans-68
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: These scores ensure that as the model is fine-tuned with our context-specific
    training data, it maintains some level of generalizability.
  id: totrans-69
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 这些评分确保当模型使用我们特定上下文的训练数据进行微调时，它仍然保持一定程度的泛化能力。
- en: '[PRE1]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '![](../Images/ddf3d02dfb50c6ebf6c98aba0e9a055a.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ddf3d02dfb50c6ebf6c98aba0e9a055a.png)'
- en: Fig 5\. Similarity visualtions across test words
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 图5\. 测试词汇的相似度可视化
- en: 'STS Benchmark (dev): 0.8325'
  id: totrans-73
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: STS Benchmark (dev)：0.8325
- en: Fine Tuning the Model
  id: totrans-74
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 微调模型
- en: The next step involves constructing the siamese model architecture using the
    baseline model and fine-tuning it with our domain-specific data.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步涉及使用基线模型构建孪生网络模型架构，并用我们特定领域的数据对其进行微调。
- en: '[PRE2]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '![](../Images/efd1cd25e0ddc3ef8fe842e8185ea96d.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/efd1cd25e0ddc3ef8fe842e8185ea96d.png)'
- en: Fig 6\. Model architecture for fine-tuning
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 图6\. 微调的模型架构
- en: Fit the model
  id: totrans-79
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 适应模型
- en: '[PRE3]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Evaluation
  id: totrans-81
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 评估
- en: Now that we have the fine-tuned model, let’s re-evaluate it and compare the
    results to those of the base model.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经有了微调后的模型，让我们重新评估它，并将结果与基础模型进行比较。
- en: '[PRE4]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '![](../Images/c882c7db246a72c1d21308cd8dc462a9.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c882c7db246a72c1d21308cd8dc462a9.png)'
- en: 'STS Benchmark (dev): 0.8349'
  id: totrans-85
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: STS 基准（开发集）：0.8349
- en: Based on fine-tuning the model on the relatively small dataset, the STS benchmark
    score is comparable to that of the baseline model, indicating that the tuned model
    still exhibits generalizability. However, the similarity visualization demonstrates
    strengthened similarity scores between similar titles and a reduction in scores
    for dissimilar ones.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 基于在相对较小的数据集上对模型进行微调，STS基准得分与基线模型相当，这表明调优后的模型仍具有一定的泛化能力。然而，相似性可视化展示了相似标题之间的相似性得分得到了增强，而不相似标题的得分则减少了。
- en: Closing Thoughts
  id: totrans-87
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结束语
- en: Fine-tuning pre-trained NLP models for domain adaptation is a powerful technique
    to improve their performance and precision in specific contexts. By utilizing
    quality, domain-specific datasets and leveraging siamese neural networks, we can
    enhance the model’s ability to capture semantic similarity.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 对预训练自然语言处理（NLP）模型进行领域适配的微调是一种强大的技术，可以提高其在特定上下文中的表现和精度。通过利用高质量的领域特定数据集和利用孪生神经网络，我们可以增强模型捕捉语义相似性的能力。
- en: This tutorial provided a step-by-step guide to the fine-tuning process, using
    the Universal Sentence Encoder (USE) model as an example. We explored the theoretical
    framework, data preparation, baseline model evaluation, and the actual fine-tuning
    process. The results demonstrated the effectiveness of fine-tuning in strengthening
    similarity scores within a domain.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 本教程提供了微调过程的逐步指南，以Universal Sentence Encoder（USE）模型为例。我们探讨了理论框架、数据准备、基线模型评估以及实际的微调过程。结果展示了在特定领域内微调的有效性，增强了相似性得分。
- en: By following this approach and adapting it to your specific domain, you can
    unlock the full potential of pre-trained NLP models and achieve better results
    in your natural language processing tasks
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 通过遵循这种方法并将其适应到你的特定领域，你可以充分发挥预训练NLP模型的潜力，并在自然语言处理任务中取得更好的结果。
- en: Thanks for reading. *If you have any feedback, please feel to reach out by commenting
    on this post, messaging me on* [*LinkedIn*](https://www.linkedin.com/in/shashankkapadia/)*,
    or shooting me an email (smhkapadia[at]gmail.com)*
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 感谢阅读。*如果你有任何反馈，请随时通过评论这篇文章、在* [*LinkedIn*](https://www.linkedin.com/in/shashankkapadia/)
    *上给我发消息，或者发邮件到（smhkapadia[at]gmail.com）与我联系*
- en: '*If you enjoyed this article, visit my other articles*'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '*如果你喜欢这篇文章，请访问我的其他文章*'
- en: '[](/evaluate-topic-model-in-python-latent-dirichlet-allocation-lda-7d57484bb5d0?source=post_page-----a06659ca6668--------------------------------)
    [## Evaluate Topic Models: Latent Dirichlet Allocation (LDA)'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '[evaluate-topic-model-in-python-latent-dirichlet-allocation-lda-7d57484bb5d0?source=post_page-----a06659ca6668--------------------------------](https://towardsdatascience.com/evaluate-topic-model-in-python-latent-dirichlet-allocation-lda-7d57484bb5d0?source=post_page-----a06659ca6668--------------------------------)
    [## 评估主题模型：潜在狄利克雷分配（LDA）'
- en: A step-by-step guide to building interpretable topic models
  id: totrans-94
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 构建可解释的主题模型的逐步指南
- en: towardsdatascience.com](/evaluate-topic-model-in-python-latent-dirichlet-allocation-lda-7d57484bb5d0?source=post_page-----a06659ca6668--------------------------------)
    [](https://medium.com/aimonks/the-evolution-of-natural-language-processing-56ce27916e10?source=post_page-----a06659ca6668--------------------------------)
    [## The Evolution of Natural Language Processing
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '[evaluate-topic-model-in-python-latent-dirichlet-allocation-lda-7d57484bb5d0?source=post_page-----a06659ca6668--------------------------------](https://towardsdatascience.com/evaluate-topic-model-in-python-latent-dirichlet-allocation-lda-7d57484bb5d0?source=post_page-----a06659ca6668--------------------------------)
    [](https://medium.com/aimonks/the-evolution-of-natural-language-processing-56ce27916e10?source=post_page-----a06659ca6668--------------------------------)
    [## 自然语言处理的演变'
- en: A Historical Perspective on the Development of Language Models
  id: totrans-96
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 语言模型发展的历史视角
- en: 'medium.com](https://medium.com/aimonks/the-evolution-of-natural-language-processing-56ce27916e10?source=post_page-----a06659ca6668--------------------------------)
    [](/recommendation-system-in-python-lightfm-61c85010ce17?source=post_page-----a06659ca6668--------------------------------)
    [## Recommendation System in Python: LightFM'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '[medium.com](https://medium.com/aimonks/the-evolution-of-natural-language-processing-56ce27916e10?source=post_page-----a06659ca6668--------------------------------)
    [](/recommendation-system-in-python-lightfm-61c85010ce17?source=post_page-----a06659ca6668--------------------------------)
    [## Python中的推荐系统：LightFM'
- en: A Step-by-Step guide to building a recommender system in Python using LightFM
  id: totrans-98
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 在Python中使用LightFM构建推荐系统的逐步指南
- en: towardsdatascience.com](/recommendation-system-in-python-lightfm-61c85010ce17?source=post_page-----a06659ca6668--------------------------------)
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '[towardsdatascience.com](/recommendation-system-in-python-lightfm-61c85010ce17?source=post_page-----a06659ca6668--------------------------------)'
