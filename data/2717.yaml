- en: Randomizing Very Large Datasets
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 随机化非常大的数据集
- en: 原文：[https://towardsdatascience.com/randomizing-very-large-datasets-e2b14e507725?source=collection_archive---------7-----------------------#2023-08-26](https://towardsdatascience.com/randomizing-very-large-datasets-e2b14e507725?source=collection_archive---------7-----------------------#2023-08-26)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/randomizing-very-large-datasets-e2b14e507725?source=collection_archive---------7-----------------------#2023-08-26](https://towardsdatascience.com/randomizing-very-large-datasets-e2b14e507725?source=collection_archive---------7-----------------------#2023-08-26)
- en: Consider the problem of randomizing a dataset that is so large, it doesn’t even
    fit into memory. This article describes how you can do it easily and (relatively)
    quickly in Python.
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 考虑这样一个问题：如何随机化一个大到连内存都容纳不下的数据集。本文描述了如何在 Python 中轻松且（相对）快速地完成这一操作。
- en: '[](https://medium.com/@doug.blank?source=post_page-----e2b14e507725--------------------------------)[![Douglas
    Blank, PhD](../Images/b2fa86b9fe63a8bcb4f218ef5a6791e9.png)](https://medium.com/@doug.blank?source=post_page-----e2b14e507725--------------------------------)[](https://towardsdatascience.com/?source=post_page-----e2b14e507725--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----e2b14e507725--------------------------------)
    [Douglas Blank, PhD](https://medium.com/@doug.blank?source=post_page-----e2b14e507725--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@doug.blank?source=post_page-----e2b14e507725--------------------------------)[![Douglas
    Blank, PhD](../Images/b2fa86b9fe63a8bcb4f218ef5a6791e9.png)](https://medium.com/@doug.blank?source=post_page-----e2b14e507725--------------------------------)[](https://towardsdatascience.com/?source=post_page-----e2b14e507725--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----e2b14e507725--------------------------------)
    [Douglas Blank, PhD](https://medium.com/@doug.blank?source=post_page-----e2b14e507725--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F66e2bac7e7d8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frandomizing-very-large-datasets-e2b14e507725&user=Douglas+Blank%2C+PhD&userId=66e2bac7e7d8&source=post_page-66e2bac7e7d8----e2b14e507725---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----e2b14e507725--------------------------------)
    ·6 min read·Aug 26, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fe2b14e507725&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frandomizing-very-large-datasets-e2b14e507725&user=Douglas+Blank%2C+PhD&userId=66e2bac7e7d8&source=-----e2b14e507725---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F66e2bac7e7d8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frandomizing-very-large-datasets-e2b14e507725&user=Douglas+Blank%2C+PhD&userId=66e2bac7e7d8&source=post_page-66e2bac7e7d8----e2b14e507725---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----e2b14e507725--------------------------------)
    ·6 min read·2023年8月26日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fe2b14e507725&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frandomizing-very-large-datasets-e2b14e507725&user=Douglas+Blank%2C+PhD&userId=66e2bac7e7d8&source=-----e2b14e507725---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fe2b14e507725&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frandomizing-very-large-datasets-e2b14e507725&source=-----e2b14e507725---------------------bookmark_footer-----------)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fe2b14e507725&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frandomizing-very-large-datasets-e2b14e507725&source=-----e2b14e507725---------------------bookmark_footer-----------)'
- en: These days it is not at all uncommon to find datasets that are measured in Gigabytes,
    or even Terabytes, in size. That much data can help tremendously in the training
    process to create robust Machine Learning models. But how can you randomize such
    large datasets?
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 如今，发现以 Gigabytes 甚至 Terabytes 计量的数据集并不罕见。如此大量的数据可以极大地帮助训练过程，创建出强大的机器学习模型。但如何随机化如此庞大的数据集呢？
- en: '![](../Images/85fddbf8a9cde6f37e48e2320879b217.png)'
  id: totrans-9
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/85fddbf8a9cde6f37e48e2320879b217.png)'
- en: Photo by [Jess Bailey](https://unsplash.com/@jessbaileydesigns?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由 [Jess Bailey](https://unsplash.com/@jessbaileydesigns?utm_source=medium&utm_medium=referral)
    提供，发布在 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: Imagine that you have a very large dataset with one item per line in a file.
    The details of the data are irrelevant for our goals here. The dataset could be
    lines in a CSV (comma-separate values) or TSV (tab-separated values) file, or
    each line could be a JSON object, or a list of X, Y, Z values of a point in a
    large point cloud. All we need is that the dataset is formatted with one item
    per line.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下你有一个非常大的数据集，每行一个项目在一个文件中。数据的具体细节对于我们的目标无关紧要。数据集可以是 CSV（逗号分隔值）或 TSV（制表符分隔值）文件中的行，或者每行是一个
    JSON 对象，或者是大点云中某一点的 X、Y、Z 值。我们只需要的是数据集按每行一个项目进行格式化。
- en: 'For files containing smaller datasets, one could randomize the file (called
    “shuffling”) in memory using a simple Python function like this:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 对于包含较小数据集的文件，可以使用像这样的简单 Python 函数在内存中对文件进行随机化（称为“洗牌”）：
- en: '[PRE0]'
  id: totrans-13
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The **shuffle_in_memory()** function takes an input filename and an output filename,
    shuffles the lines in memory using the builtin **random.shuffle(),** and writes
    the randomized data out. Like its name implies, this function requires that all
    of the lines of the file be loaded into memory at once.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '**shuffle_in_memory()**函数接收一个输入文件名和一个输出文件名，在内存中使用内置的**random.shuffle()**函数洗牌，并将随机化的数据写出。顾名思义，该函数要求文件的所有行一次性加载到内存中。'
- en: To test this function out, let’s make some test files. The function **make_file()**
    takes the number lines that you would like in the test file. The function will
    create the file and return the filename.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 要测试这个函数，让我们制作一些测试文件。函数**make_file()**接收你希望在测试文件中包含的行数。该函数将创建文件并返回文件名。
- en: '[PRE1]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'For example, to make a file named “test-1000.txt” with 100 lines in it looks
    like this:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，要创建一个名为“test-1000.txt”的文件，其中包含100行，如下所示：
- en: '[PRE2]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'After running this function, you should find in your current directory you
    should find a file named “test-1000.txt” with 1,000 lines of text, like:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 运行此函数后，你应该在当前目录中找到一个名为“test-1000.txt”的文件，包含1,000行文本，如下所示：
- en: '[PRE3]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'To test out our **shuffle_in_memory()** function, we’ll name an output file,
    save the string in the variable **filename_out**, and call the function:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 要测试我们的**shuffle_in_memory()**函数，我们将命名一个输出文件，将字符串保存在变量**filename_out**中，并调用该函数：
- en: '[PRE4]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Now, there should be a second file in your directory named “test-randomized-1000.txt”.
    It should be exactly the same size as “test-1000.txt” with exactly the same lines,
    but in a randomized order:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你的目录中应该有一个第二个文件，名为“test-randomized-1000.txt”。它的大小应与“test-1000.txt”完全相同，行数也完全相同，但顺序是随机的：
- en: '[PRE5]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Okay, now for the big question: what to do if we have a very large file? Let’s
    make a medium-sized one with, say, 10 million lines. (For most computers this
    is still small enough to randomize in memory, but it is of sufficiently large
    enough size to practice with.) As before, we make the input file with a call to
    **make_file()**:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，现在大问题来了：如果我们有一个非常大的文件怎么办？让我们创建一个中等大小的文件，比如说1000万行。（对于大多数计算机来说，这仍然足够小，可以在内存中随机化，但大小足够大，可以进行练习。）如前所述，我们通过调用**make_file()**来创建输入文件：
- en: '[PRE6]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: This will take a few seconds. Afterwards you should have a file named “test-10000000.txt”
    in your directory. It should begin as before, but will have 10 million lines.
    The file is about 128 MB.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 这将花费几秒钟。之后，你应该在目录中有一个名为“test-10000000.txt”的文件。它应该与之前的文件一样开始，但将包含1000万行。文件大小约为128
    MB。
- en: How to randomize it? If we don’t want to use all of our RAM, or we don’t have
    enough RAM, we can use the hard disk instead. Here is a recursive algorithm based
    on a similar problem, sorting. The following function **shuffle()** is based on
    the merge sort algorithm.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 如何进行随机化？如果我们不想使用所有的 RAM，或者 RAM 不够，我们可以改用硬盘。这里有一个基于类似问题的递归算法，排序。以下函数**shuffle()**是基于归并排序算法。
- en: First, it checks to see if a file is small enough to be shuffled in memory (the
    base case in recursive function parlance). The parameter **memory_limit** is given
    in bytes. If a file size is smaller than **memory_limit**, then it will be shuffled
    in memory. If it is too big, then it is randomly split into a number of smaller
    files, and each of those is recursively shuffled. Finally, the contents of the
    smaller shuffled files are merged back together.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，它检查一个文件是否足够小，可以在内存中进行洗牌（递归函数术语中的基本情况）。参数**memory_limit**以字节为单位。如果文件大小小于**memory_limit**，那么它将在内存中进行洗牌。如果太大，则会随机分割成多个较小的文件，每个文件递归地进行洗牌。最后，将较小的洗牌文件的内容合并回一起。
- en: 'Here is the function:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 这是函数：
- en: '[PRE7]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'If this was a sorting algorithm, we would merge the files back together in
    a careful manner to create a sorted ordering. However, for shuffling, we don’t
    care about merging them in a particular order as we want them randomized. The
    **merge_files()** function therefore looks like:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 如果这是一个排序算法，我们将以一种小心的方式将文件合并在一起，以创建一个排序顺序。然而，对于洗牌，我们不关心以特定顺序合并它们，因为我们希望它们是随机的。因此，**merge_files()**函数看起来像这样：
- en: '[PRE8]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Note that we are careful to not read all of the lines of the files into memory
    all at once. Let’s test this out by giving the limit for memory shuffling to be
    exactly the size of the file. Since the file size is not smaller than 128,888,890
    it will be broken into a number of smaller files. For this example, let’s break
    the big file into 2, each of which will be small enough to shuffle in memory:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们会小心地避免一次性将所有文件的行读入内存。我们通过将内存洗牌的限制设置为文件的大小来测试这一点。由于文件大小不小于128,888,890，它将被分割成若干个较小的文件。对于这个例子，我们将大文件分成两个，每个文件都足够小，可以在内存中进行洗牌：
- en: '[PRE9]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'This call results in the following output:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 这个调用的结果如下：
- en: '[PRE10]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'And the contents of the resulting “test-randomized-10000000.txt” file should
    have 10 million lines, all randomized. A better test would be to reduce the memory
    needed to be much smaller than the file to randomize, and to break the too-big
    files into more than 2\. Let’s say we only want to use about 1 MB of RAM, and
    break the files into 20 smaller files:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 结果文件“test-randomized-10000000.txt”的内容应包含1000万行，所有行都是随机的。更好的测试方法是将所需内存缩小到远小于文件大小，并将过大的文件分割成超过2个。假设我们只想使用约1
    MB的RAM，并将文件分割成20个较小的文件：
- en: '[PRE11]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: This example will use no more than 1 MB of RAM, and recursively decompose the
    subfiles that are bigger than that, 20 at a time.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 这个例子将使用不超过1 MB的RAM，并递归地处理大于此大小的子文件，每次处理20个。
- en: This algorithm will work on files of any size (well, you need enough disk space!).
    The more memory you can allocate for the **shuffle_in_memory()** the faster it
    will run. If the number of smaller files is too large, then you’ll spend too much
    time opening and closing files. You can try different numbers for **memory_limit**,
    but I’ve had good luck with between 20 and 200\. The larger the initial file,
    you’ll probably want more subfiles.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 这个算法可以处理任何大小的文件（当然，你需要足够的磁盘空间！）。你为**shuffle_in_memory()**分配的内存越多，运行速度就会越快。如果较小的文件数量过多，你将花费太多时间打开和关闭文件。你可以尝试不同的**memory_limit**值，但我发现20到200之间的值效果很好。初始文件越大，你可能需要更多的子文件。
- en: There are other algorithms that you can also use. I had high hopes for writing
    all of the lines into a SQLite database, SELECTing them in a random order, but
    it was no faster that the above code.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 你还可以使用其他算法。我曾对将所有行写入SQLite数据库、以随机顺序SELECT它们抱有很高的期望，但它的速度并没有比上面的代码更快。
- en: '[PRE12]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '[PRE13]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Can you beat the recursive shuffle algorithm, staying purely in Python? If so,
    I’d love to hear about it!
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 你能在纯Python中击败递归洗牌算法吗？如果能，我很想听听你的方法！
- en: '***Interested in Artificial Intelligence, Machine Learning, and Data Science?
    Consider a clap and a follow. Let me know what you are interested in!***'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '***对人工智能、机器学习和数据科学感兴趣吗？请点赞并关注。告诉我你感兴趣的内容！***'
