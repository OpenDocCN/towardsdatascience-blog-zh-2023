- en: How GPT Models Work
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: GPT 模型如何运作
- en: 原文：[https://towardsdatascience.com/how-gpt-models-work-b5f4517d5b5?source=collection_archive---------0-----------------------#2023-05-20](https://towardsdatascience.com/how-gpt-models-work-b5f4517d5b5?source=collection_archive---------0-----------------------#2023-05-20)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/how-gpt-models-work-b5f4517d5b5?source=collection_archive---------0-----------------------#2023-05-20](https://towardsdatascience.com/how-gpt-models-work-b5f4517d5b5?source=collection_archive---------0-----------------------#2023-05-20)
- en: Learn the core concepts behind OpenAI’s GPT models
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 学习 OpenAI GPT 模型背后的核心概念
- en: '[](https://medium.com/@bea_684?source=post_page-----b5f4517d5b5--------------------------------)[![Beatriz
    Stollnitz](../Images/63a2a7daeca6d93e26b3ac0556c42aa1.png)](https://medium.com/@bea_684?source=post_page-----b5f4517d5b5--------------------------------)[](https://towardsdatascience.com/?source=post_page-----b5f4517d5b5--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----b5f4517d5b5--------------------------------)
    [Beatriz Stollnitz](https://medium.com/@bea_684?source=post_page-----b5f4517d5b5--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@bea_684?source=post_page-----b5f4517d5b5--------------------------------)[![Beatriz
    Stollnitz](../Images/63a2a7daeca6d93e26b3ac0556c42aa1.png)](https://medium.com/@bea_684?source=post_page-----b5f4517d5b5--------------------------------)[](https://towardsdatascience.com/?source=post_page-----b5f4517d5b5--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----b5f4517d5b5--------------------------------)
    [Beatriz Stollnitz](https://medium.com/@bea_684?source=post_page-----b5f4517d5b5--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F1c8863892480&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-gpt-models-work-b5f4517d5b5&user=Beatriz+Stollnitz&userId=1c8863892480&source=post_page-1c8863892480----b5f4517d5b5---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----b5f4517d5b5--------------------------------)
    ·14 min read·May 20, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fb5f4517d5b5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-gpt-models-work-b5f4517d5b5&user=Beatriz+Stollnitz&userId=1c8863892480&source=-----b5f4517d5b5---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F1c8863892480&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-gpt-models-work-b5f4517d5b5&user=Beatriz+Stollnitz&userId=1c8863892480&source=post_page-1c8863892480----b5f4517d5b5---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----b5f4517d5b5--------------------------------)
    · 14 分钟阅读 · 2023年5月20日 [](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fb5f4517d5b5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-gpt-models-work-b5f4517d5b5&user=Beatriz+Stollnitz&userId=1c8863892480&source=-----b5f4517d5b5---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fb5f4517d5b5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-gpt-models-work-b5f4517d5b5&source=-----b5f4517d5b5---------------------bookmark_footer-----------)![](../Images/3aacc19848583f3211780a4e9883415e.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fb5f4517d5b5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-gpt-models-work-b5f4517d5b5&source=-----b5f4517d5b5---------------------bookmark_footer-----------)![](../Images/3aacc19848583f3211780a4e9883415e.png)'
- en: Photo by [KOMMERS](https://unsplash.com/@kommers?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由 [KOMMERS](https://unsplash.com/@kommers?utm_source=medium&utm_medium=referral)
    提供，来源于 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: '**Introduction**'
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**介绍**'
- en: It was 2021 when I wrote my first few lines of code using a GPT model, and that
    was the moment I realized that text generation had reached an inflection point.
    Prior to that, I had written language models from scratch in grad school, and
    I had experience working with other text generation systems, so I knew just how
    difficult it was to get them to produce useful results. I was fortunate to get
    early access to GPT-3 as part of my work on the announcement of its release within
    the Azure OpenAI Service, and I tried it out in preparation for its launch. I
    asked GPT-3 to summarize a long document and experimented with few-shot prompts.
    I could see that the results were far more advanced than those of prior models,
    making me excited about the technology and eager to learn how it’s implemented.
    And now that the follow-on GPT-3.5, ChatGPT, and GPT-4 models are rapidly gaining
    wide adoption, more people in the field are also curious about how they work.
    While the details of their inner workings are proprietary and complex, all the
    GPT models share some fundamental ideas that aren’t too hard to understand. My
    goal for this post is to explain the core concepts of language models in general
    and GPT models in particular, with the explanations geared toward data scientists
    and machine learning…
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 我在2021年首次使用GPT模型编写了几行代码，那一刻我意识到文本生成技术已达到一个**拐点**。在此之前，我在研究生期间从零开始编写过语言模型，并且有使用其他文本生成系统的经验，因此我知道让它们产生有用结果是多么困难。我很幸运能在Azure
    OpenAI Service发布的公告中获得GPT-3的早期访问权限，并在其发布前进行试用。我让GPT-3总结了一份长文档，并尝试了少量示例提示。我看到结果比以前的模型要先进得多，这让我对这项技术感到兴奋，并渴望了解其实现方式。现在，后续的GPT-3.5、ChatGPT和GPT-4模型正在迅速获得广泛采用，领域内更多的人也对它们的工作原理感到好奇。虽然它们的内部工作细节是专有且复杂的，但所有GPT模型共享一些基础的理念，这些理念并不难以理解。我为这篇文章设定的目标是解释语言模型的核心概念，尤其是GPT模型的核心概念，解释内容针对数据科学家和机器学习…
