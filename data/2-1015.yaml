- en: 'Green AI: Methods and Solutions to Improve AI Sustainability'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 绿色AI：改进AI可持续性的方法和解决方案
- en: 原文：[https://towardsdatascience.com/green-ai-methods-and-solutions-to-improve-ai-sustainability-861d69dec658](https://towardsdatascience.com/green-ai-methods-and-solutions-to-improve-ai-sustainability-861d69dec658)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/green-ai-methods-and-solutions-to-improve-ai-sustainability-861d69dec658](https://towardsdatascience.com/green-ai-methods-and-solutions-to-improve-ai-sustainability-861d69dec658)
- en: A technical look at a long overdue topic
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 对一个长期被忽视的话题进行技术性的审视
- en: '[](https://pecciaf.medium.com/?source=post_page-----861d69dec658--------------------------------)[![Federico
    Peccia](../Images/48ad8401c28e87717718f58336cc64cf.png)](https://pecciaf.medium.com/?source=post_page-----861d69dec658--------------------------------)[](https://towardsdatascience.com/?source=post_page-----861d69dec658--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----861d69dec658--------------------------------)
    [Federico Peccia](https://pecciaf.medium.com/?source=post_page-----861d69dec658--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://pecciaf.medium.com/?source=post_page-----861d69dec658--------------------------------)[![Federico
    Peccia](../Images/48ad8401c28e87717718f58336cc64cf.png)](https://pecciaf.medium.com/?source=post_page-----861d69dec658--------------------------------)[](https://towardsdatascience.com/?source=post_page-----861d69dec658--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----861d69dec658--------------------------------)
    [Federico Peccia](https://pecciaf.medium.com/?source=post_page-----861d69dec658--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----861d69dec658--------------------------------)
    ·9 min read·Jun 26, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----861d69dec658--------------------------------)
    ·阅读时间9分钟·2023年6月26日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/e26e571fe94f4badd859d8df3d7613de.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e26e571fe94f4badd859d8df3d7613de.png)'
- en: Photo by [Benjamin Davies](https://unsplash.com/@bendavisual?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 照片由 [Benjamin Davies](https://unsplash.com/@bendavisual?utm_source=medium&utm_medium=referral)
    提供，来源于 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: Introduction
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: 'If you opened this article, you have probably heard about the current controversy
    regarding the safety and trustability of current Large Language Models (LLMs).
    The open letter signed by well-known names in the computer science world like
    Steve Wozniak, Gary Marcus and Stuart Russel presented their concerns on this
    matter and asked for a 6-month stop in the training of LLMs. But there is another
    topic which is slowly gaining a lot of attention, which will perhaps motivate
    another open letter in the near future: energy consumption and carbon footprint
    of the training and inference of AI models.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你打开了这篇文章，你可能已经听说过关于当前大型语言模型（LLMs）安全性和可信度的争议。由计算机科学界知名人物如**史蒂夫·沃兹尼亚克**、**加里·马库斯**和**斯图尔特·拉塞尔**签署的公开信表达了他们对这一问题的担忧，并要求暂停LLMs的训练6个月。但还有另一个话题正在慢慢引起大量关注，这可能会在不久的将来促使另一封公开信的出现：AI模型训练和推理的能源消耗和碳足迹。
- en: It is estimated that *only* the training of the popular GPT-3 model, a 175-billion
    parameter LLM, emitted approximately 502 tonnes of CO2 [1]. There are even [online
    calculators](https://mlco2.github.io/impact/) available to estimate the emissions
    of training a particular model. But the training step is not the only one consuming
    energy. After training, during the inference phase, an AI model is executed thousands
    or millions of times per day. Even if each execution consumes a small amount of
    energy, the accumulated consumption over weeks, months, and years can become a
    huge problem.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 估计仅**训练**流行的GPT-3模型，这一1750亿参数的LLM，便排放了大约502吨二氧化碳[1]。甚至有[在线计算器](https://mlco2.github.io/impact/)可以估算训练某个模型的排放量。但训练步骤并不是唯一消耗能源的环节。训练后，在推理阶段，AI模型每天会被执行数千次或数百万次。即便每次执行消耗的能源较少，经过数周、数月甚至数年的累计消耗，也可能成为一个巨大的问题。
- en: This is why the concept of Green AI is becoming increasingly popular. Its main
    focus is to find solutions and develop techniques to improve the sustainability
    of AI, by reducing its energy consumption and carbon footprint. In this article,
    I aim to present an overview of some techniques and methodologies that are being
    actively researched, that can be used to improve this concept, and that are not
    usually discussed in an accessible manner. At the end of this article, you will
    find resources and references related to the topics discussed.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是为什么绿色人工智能（Green AI）这一概念越来越受欢迎的原因。它的主要焦点是寻找解决方案和开发技术，以提高人工智能的可持续性，减少其能源消耗和碳足迹。在本文中，我旨在介绍一些正在积极研究的技术和方法，它们可以用来改进这一概念，并且这些内容通常不会以易于理解的方式讨论。本文末尾，你将找到与所讨论主题相关的资源和参考文献。
- en: Although this article is focused on the technical methodologies enabling energy
    savings when deploying AI algorithms, it is important to have a general grasp
    of them even if you are *not* a researcher. Are you the person responsible for
    training the AI algorithm of your company? Well, perhaps you can keep some optimizations
    in mind during the training that will improve the energy consumption of the algorithm
    once deployed. Are you perhaps the person responsible for selecting the hardware
    on which your algorithm will be deployed? Then keep an eye open for the concepts
    mentioned in this article, as they can be a sign of cutting-edge, optimized hardware.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管本文重点讨论了在部署人工智能算法时实现节能的技术方法，但即使你*不是*研究人员，也需要对这些方法有一个大致了解。你是否负责训练公司人工智能算法？那么，也许你可以在训练过程中考虑一些优化，以改善算法部署后的能源消耗。你是否负责选择将要部署算法的硬件？那么请留意本文提到的概念，因为它们可能是前沿优化硬件的标志。
- en: Computer architecture basics
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 计算机架构基础
- en: In order to understand this article it is essential to have a basic understanding
    of computer architecture, and how the software and the hardware interact with
    each other. This is a very complex topic, but I will try to provide a quick summary
    before entering the main part of the article.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解本文，基本了解计算机架构以及软件和硬件如何相互作用是至关重要的。这是一个非常复杂的话题，但在进入本文的主要部分之前，我将尝试提供一个简要的总结。
- en: 'You have probably heard about the bit, the most simple information unit inside
    any computer, and the reason the digital world exists. Bits can only take two
    states: 0 or 1\. A group of 8 bits is called a byte. For the purposes of this
    article, we can think about any computer architecture in terms of 2 hardware components
    which manipulate and store these bytes: the computation units and the memory.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能听说过比特（bit），它是任何计算机中最简单的信息单位，也是数字世界存在的原因。比特只能取两种状态：0 或 1。8 个比特组成一个字节（byte）。为了便于讨论，我们可以将任何计算机架构视为
    2 个硬件组件，它们操作和存储这些字节：计算单元和内存。
- en: The computation units are the ones responsible for taking a number of bytes
    as input and generating another group of bytes as output. For example, if we want
    to multiply 7 x 6, we would insert the bytes representing 7 into one of the inputs
    of a multiplier and the bytes representing 6 in the other input. The output of
    the multiplier would give us the bytes representing the number 42, the result
    of the multiplication. This multiplication takes a certain amount of time and
    energy until the result is available at the output of the multiplier.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 计算单元负责将一组字节作为输入，并生成另一组字节作为输出。例如，如果我们想要计算 7 x 6，我们会将表示 7 的字节插入到乘法器的一个输入端，将表示
    6 的字节插入到另一个输入端。乘法器的输出将给我们表示数字 42 的字节，即乘法的结果。这种乘法操作需要一定的时间和能源，直到结果在乘法器的输出端可用。
- en: The memory is where the bytes are stored for future use. Reading and writing
    bytes from memory (also called “accessing” the memory) takes time *and* energy.
    In computer architecture, there are usually multiple “levels” in the memory hierarchy,
    with the ones closer to the computations units having the fastest access times
    and the less energy consumption per byte read, and the ones further away being
    the slowest and more energy-demanding memories. The main idea behind this hierarchical
    organization of memory is data reuse. Data used very often is brought from the
    last memory level into the closest one and reused as many times as possible. This
    concept is called “caching”, and these faster and closest memories are called
    L1 and L2 caches.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 内存是存储未来使用字节的地方。从内存中读取和写入字节（也称为“访问”内存）需要时间*和*能量。在计算机架构中，内存层次结构通常有多个“层级”，接近计算单元的层级访问时间最快，每字节读取的能量消耗也较少，而离得较远的层级则是最慢且能量消耗最大的。内存的这种层次组织的主要思想是数据重用。使用非常频繁的数据从最后一级内存传输到最接近的一级，并尽可能多次重用。这一概念称为“缓存”，而这些更快且最接近的内存称为L1和L2缓存。
- en: The software is responsible for orchestrating the movement of data from the
    memory into the computation units, and then storing the results into the memory.
    As such, software decisions can really affect the energy consumption of a system.
    For example, if the software requests data that is not available in the L1 cache,
    the hardware first needs to fetch it from the L2 level or even from the last level,
    incurring time delays and more energy consumption.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 软件负责协调数据从内存到计算单元的移动，然后将结果存储到内存中。因此，软件的决策可以真正影响系统的能量消耗。例如，如果软件请求的数据在L1缓存中不可用，硬件首先需要从L2级别甚至从最后一级中获取数据，从而带来时间延迟和更多的能量消耗。
- en: Techniques for Green AI
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 绿色人工智能的技术
- en: 'Now that the computer architecture basics are established, we can focus on
    the specific techniques and methodologies used in Green AI. These are grouped
    into two distinct categories:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 现在计算机架构的基础已经建立，我们可以专注于绿色人工智能中使用的具体技术和方法。这些技术分为两个不同的类别：
- en: Hardware optimizations, like voltage/frequency scaling or approximate computing.
    These techniques work on the actual physical design and properties of the electronic
    circuits.
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 硬件优化，例如电压/频率调整或近似计算。这些技术作用于电子电路的实际物理设计和特性。
- en: Software optimizations, like pruning, quantization, fine-tuning, and others.
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 软件优化，例如剪枝、量化、微调等。
- en: 'DVFS: Dynamic voltage and frequency scaling'
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: DVFS：动态电压和频率调整
- en: The power consumption of standard silicon-based electronic circuits is directly
    related to the voltage used in the circuit and its working frequency. Under the
    same operating conditions, if any of these parameters is reduced, the same happens
    to the power consumption. Could we exploit this behaviour to make the execution
    of AI algorithms *greener*?
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 标准硅基电子电路的功耗直接与电路中使用的电压和工作频率相关。在相同的操作条件下，如果降低这些参数中的任何一个，功耗也会随之减少。我们是否可以利用这种行为使人工智能算法的执行更*绿色*？
- en: Of course! Imagine we have a small embedded device connected to a battery, receiving
    multiple requests (each one with its own criticality and constraints), processing
    them with an AI algorithm, and then sending the results back. We want the processing
    of the AI algorithm to consume as less energy as possible so that we can keep
    the battery running as long as possible, right? Could we actually dynamically
    change the voltage and the operating frequency of the device when less critical
    tasks arrive, and then return them to normal operating conditions when critical
    tasks need to be processed?
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 当然！设想我们有一个小型嵌入式设备连接到电池上，接收多个请求（每个请求有其自身的重要性和约束），用人工智能算法处理这些请求，然后将结果发送回去。我们希望人工智能算法的处理消耗尽可能少的能量，以便让电池尽可能长时间运行，对吧？我们是否可以在处理不那么重要的任务时动态调整设备的电压和工作频率，然后在需要处理关键任务时恢复到正常工作状态？
- en: 'Depending on the device executing the AI algorithm, this is a completely valid
    option! In fact, this is an active research field. If you are interested in reading
    more about it, I recommend you to take a look at “AutoScale: Energy Efficiency
    Optimization for Stochastic Edge Inference Using Reinforcement Learning” by Kim
    [2] or “Multi-Agent Collaborative Inference Via DNN Decoupling: Intermediate Feature
    Compression and Edge Learning” by Hao [3], which provide good examples of how
    this technique can be used to reduce the energy consumption of AI algorithms.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 根据执行AI算法的设备，这确实是一个完全有效的选项！实际上，这是一个活跃的研究领域。如果你对进一步阅读感兴趣，我建议你查看Kim [2] 的《AutoScale：使用强化学习进行随机边缘推理的能效优化》或Hao
    [3] 的《通过DNN解耦的多智能体协作推理：中间特征压缩与边缘学习》，这些论文提供了如何利用该技术减少AI算法能耗的良好示例。
- en: Approximate computing
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 近似计算
- en: When executing a mathematical operation on a CPU or a GPU, we are usually used
    to expecting exact results for the requested calculation, right? This is normally
    the case when using consumer-type hardware. Given that multiplication is one of
    the most used mathematical operations in an AI algorithm, we expect to obtain
    an exact result when multiplying two integer numbers, and a really good approximation
    when multiplying two floating point numbers (this approximation is usually so
    precise, it is not a problem for basic user programs). Why should we even *consider*
    the possibility of inserting two integer numbers and NOT obtaining the correct
    mathematical result?
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在CPU或GPU上执行数学运算时，我们通常期待得到所请求计算的精确结果，对吧？这在使用消费级硬件时通常是这样的。由于乘法是AI算法中最常用的数学操作之一，我们希望在乘以两个整数时得到精确结果，而在乘以两个浮点数时得到一个非常好的近似（这个近似通常如此精确，以至于对基础用户程序没有问题）。我们为什么还要*考虑*插入两个整数而未获得正确的数学结果的可能性呢？
- en: 'But a new approach is being actively researched in the last few years. The
    question is simple: is there a way to design simpler multipliers, which consume
    less physical area and less energy, by sacrificing accuracy in the multiplication
    result? But more importantly, can these new multipliers be used in real applications,
    without significantly hurting their performance? The answer to both of these questions
    is actually yes. This is the computing paradigm known as approximate computing.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 但在过去几年里，一种新的方法正在积极研究。问题很简单：是否可以通过牺牲乘法结果的准确性，设计出更简单的乘法器，从而减少物理面积和能量消耗？更重要的是，这些新乘法器是否可以在实际应用中使用，而不会显著影响性能？这两个问题的答案实际上都是肯定的。这就是被称为近似计算的计算范式。
- en: 'This is absolutely fascinating! There are already works presenting approximate
    multipliers that are able to provide exact results for the multiplication of two
    integers which only provide incorrect results for a reduced number of input combinations,
    but are able to provide energy reductions in the order of 20% for the execution
    of entire models. If you are interested in this incredible technique, I encourage
    you to take a look at “Approximate Computing for ML: State-of-the-art, Challenges
    and Visions” by Zervakis [4], which provides a nice overview of the specific works
    focused on this topic.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 这真是太吸引人了！目前已经有研究展示了一些近似乘法器，这些乘法器能够提供两个整数相乘的精确结果，且仅对少数输入组合提供不正确的结果，但能在执行整个模型时实现大约20%的能量减少。如果你对这种令人难以置信的技术感兴趣，我鼓励你查看Zervakis
    [4] 的《用于机器学习的近似计算：最新进展、挑战与展望》，这本书提供了关于这一主题的具体工作的良好概述。
- en: Pruning and quantization
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 剪枝与量化
- en: For people familiar with the training of AI algorithms, especially Neural Networks,
    these two techniques should sound familiar. For those not familiar with those
    terms, the concepts are really worth reading about.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 对于熟悉AI算法训练，尤其是神经网络的人，这两种技术应该很熟悉。对于那些不熟悉这些术语的人，这些概念非常值得阅读。
- en: Pruning is a method based on the idea that there is a lot of redundancy in the
    parameters of a Neural Network, which are the ones containing the knowledge of
    the network. This is why a lot of them can be removed without actually hurting
    the prediction of the network.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 剪枝是一种基于这样一个观点的方法：神经网络中的参数存在大量冗余，这些参数包含了网络的知识。因此，许多参数可以被移除，而不会实际影响网络的预测。
- en: Quantization means to represent the parameters of a network using fewer bytes.
    Remember how we said that computers represent numbers using a certain amount of
    bytes? Well, usually networks are trained using a representation called “floating
    point”, where each number can be 4 or 8 bytes long. But there are techniques to
    actually represent these parameters using only 1 byte (the “integer” representation)
    and still have a similar or sometimes even equal prediction quality.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 量化意味着使用更少的字节表示网络的参数。记得我们说过计算机使用一定数量的字节表示数字吗？通常，网络使用一种称为“浮点”的表示方式进行训练，其中每个数字可以占用4或8个字节。但实际上有技术可以只使用1个字节（“整数”表示法）来表示这些参数，并且仍然能保持类似甚至相同的预测质量。
- en: I am sure you are already imagining how these two techniques help reduce the
    energy consumption of a Neural Network. For pruning, if fewer parameters are needed
    to process one input, two things happen that improve the energy consumption of
    the algorithm. First, fewer computations need to be executed in the computation
    units. Second, because there are fewer computations to make, less data is read
    from memory. For quantization, multiplying two numbers represented as integers
    using only one byte requires a much smaller and simple hardware multiplier, which
    in turn requires less energy to do the actual multiplication. Finally, if the
    size of each parameter is reduced from 8 bytes to 1 byte, this means that the
    amount of data that needs to be read from memory is also 8 times smaller, thus
    greatly reducing the energy consumption needed to process one input.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 我相信你已经在想象这两种技术如何帮助减少神经网络的能源消耗了。对于剪枝，如果处理一个输入所需的参数更少，会发生两件事来改善算法的能源消耗。首先，需要在计算单元中执行的计算更少。其次，由于计算较少，从内存中读取的数据也更少。对于量化，使用仅一个字节表示的整数来乘以两个数字需要一个更小、更简单的硬件乘法器，这反过来需要更少的能量来进行实际的乘法运算。最后，如果每个参数的大小从8个字节减少到1个字节，这意味着需要从内存中读取的数据量也减少了8倍，从而大大减少了处理一个输入所需的能源消耗。
- en: 'Do you want to read more about it? Take a look at “Lightweight Parameter Pruning
    for Energy-Efficient Deep Learning: A Binarized Gating Module Approach” by Zhi
    [5] or “Pruning for Power: Optimizing Energy Efficiency in IoT with Neural Network
    Pruning” by Widmann [6] for examples of current work on the topic.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 想了解更多吗？看看 Zhi [5] 的“轻量级参数剪枝以实现节能深度学习：一种二值化门控模块方法”或 Widmann [6] 的“为电力优化：通过神经网络剪枝优化物联网的能源效率”以获取当前该主题的工作示例。
- en: Fine-tuning
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 微调
- en: Given the closed nature of a lot of the latest LLMs, a significant amount of
    computer power is being used just to *replicate* the results of these models.
    If these models were opened to the public, the technique known as fine-tuning
    could be applied to them. This is a method by which only some of the parameters
    of a pre-trained model are modified during a fine-tuning training procedure, to
    specialize the network for a particular task. This process usually requires fewer
    training iterations and thus consumes less energy than retraining a whole network
    from scratch.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 由于许多最新的LLM具有封闭的特性，通常需要大量的计算能力来*复制*这些模型的结果。如果这些模型向公众开放，可以应用被称为微调的技术。这是一种在微调训练过程中只修改预训练模型的一部分参数的方法，以将网络专门化为特定任务。这个过程通常需要较少的训练迭代，因此比从头开始重新训练整个网络消耗更少的能源。
- en: This is why opening these models to the public would help not only the people
    trying to build products with them but also the researchers that are retraining
    them from scratch and thus consuming a lot of energy that could be saved.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是为什么向公众开放这些模型不仅会帮助那些试图利用它们构建产品的人，还会帮助那些从头开始重新训练这些模型的研究人员，从而节省大量可以节省的能源。
- en: Conclusion
  id: totrans-40
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: I hope you found these techniques and methods as fascinating as I found them.
    It is reassuring and comforting to know that there are people actively researching
    these techniques and trying to improve as much as possible on a topic as important
    as energy savings and carbon footprint.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 我希望你觉得这些技术和方法与我一样令人着迷。了解到有人积极研究这些技术并尽可能改进在能源节省和碳足迹这样重要的主题上，感到安心和慰藉。
- en: But we cannot sit down and relax, offloading the responsibility of finding optimized
    solutions to the researchers working on these topics. Are you starting a new project?
    Check first if you can fine-tune a pre-trained model. Is your hardware optimized
    to run pruned algorithms, but you don't have the expertise to efficiently apply
    this technique? Go out there, spend some time learning it or find someone who
    already has the skill. In the long run, it will be worth it, not only for you
    and your company but for our planet Earth as a whole.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 但我们不能坐下来放松，把寻找优化解决方案的责任完全交给从事这些课题的研究人员。你是否正在启动一个新项目？首先检查是否可以微调一个预训练模型。你的硬件是否优化以运行剪枝算法，但你没有有效应用此技术的专业知识？去学习它，或者找一个已经具备技能的人。从长远来看，这将是值得的，不仅对你和你的公司，而且对我们地球整体。
- en: Feel free to follow me on [Twitter](https://twitter.com/PecciaF) or [LinkedIn](https://www.linkedin.com/in/fpecc/)
    and let me know what you think of this article, or [buy me a coffee](https://www.buymeacoffee.com/pecciaf)
    if you really liked it!
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 欢迎在[Twitter](https://twitter.com/PecciaF)或[LinkedIn](https://www.linkedin.com/in/fpecc/)上关注我，并告诉我你对这篇文章的看法，或者如果你真的喜欢这篇文章，可以[请我喝杯咖啡](https://www.buymeacoffee.com/pecciaf)！
- en: Thanks for reading!
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 感谢阅读！
- en: References
  id: totrans-45
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] [“Estimating the carbon footprint of BLOOM, a 176B parameter language model”](https://arxiv.org/pdf/2211.02001.pdf)'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] [“估算BLOOM的碳足迹，一个1760亿参数的语言模型”](https://arxiv.org/pdf/2211.02001.pdf)'
- en: '[2] [“AutoScale: Energy Efficiency Optimization for Stochastic Edge Inference
    Using Reinforcement Learning”](https://microarch.org/micro53/papers/738300b082.pdf)'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] [“AutoScale：使用强化学习优化随机边缘推断的能效”](https://microarch.org/micro53/papers/738300b082.pdf)'
- en: '[3] [“Multi-Agent Collaborative Inference Via DNN Decoupling: Intermediate
    Feature Compression and Edge Learning”](https://arxiv.org/abs/2205.11854)'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] [“通过DNN解耦的多智能体协作推断：中间特征压缩和边缘学习”](https://arxiv.org/abs/2205.11854)'
- en: '[4] [“Approximate Computing for ML: State-of-the-art, Challenges and Visions”](http://slam.ece.utexas.edu/pubs/aspdac21.AxC.pdf)'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] [“机器学习的近似计算：最新进展、挑战与愿景”](http://slam.ece.utexas.edu/pubs/aspdac21.AxC.pdf)'
- en: '[[5] “Lightweight Parameter Pruning for Energy-Efficient Deep Learning: A Binarized
    Gating Module Approach”](https://arxiv.org/abs/2302.10798)'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '[[5] “轻量级参数剪枝以实现能效深度学习：二值化门控模块方法”](https://arxiv.org/abs/2302.10798)'
- en: '[[6] “Pruning for Power: Optimizing Energy Efficiency in IoT with Neural Network
    Pruning”](https://link.springer.com/chapter/10.1007/978-3-031-34204-2_22)'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '[[6] “优化能效：通过神经网络剪枝提高物联网能效”](https://link.springer.com/chapter/10.1007/978-3-031-34204-2_22)'
