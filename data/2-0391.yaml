- en: Big Data File Formats, Explained
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 大数据文件格式解释
- en: 原文：[https://towardsdatascience.com/big-data-file-formats-explained-275876dc1fc9](https://towardsdatascience.com/big-data-file-formats-explained-275876dc1fc9)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/big-data-file-formats-explained-275876dc1fc9](https://towardsdatascience.com/big-data-file-formats-explained-275876dc1fc9)
- en: Parquet vs ORC vs AVRO vs JSON. Which one to choose and how to use them?
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Parquet 与 ORC 与 AVRO 与 JSON。该选择哪一个，如何使用？
- en: '[](https://mshakhomirov.medium.com/?source=post_page-----275876dc1fc9--------------------------------)[![💡Mike
    Shakhomirov](../Images/bc6895c7face3244d488feb97ba0f68e.png)](https://mshakhomirov.medium.com/?source=post_page-----275876dc1fc9--------------------------------)[](https://towardsdatascience.com/?source=post_page-----275876dc1fc9--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----275876dc1fc9--------------------------------)
    [💡Mike Shakhomirov](https://mshakhomirov.medium.com/?source=post_page-----275876dc1fc9--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://mshakhomirov.medium.com/?source=post_page-----275876dc1fc9--------------------------------)[![💡Mike
    Shakhomirov](../Images/bc6895c7face3244d488feb97ba0f68e.png)](https://mshakhomirov.medium.com/?source=post_page-----275876dc1fc9--------------------------------)[](https://towardsdatascience.com/?source=post_page-----275876dc1fc9--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----275876dc1fc9--------------------------------)
    [💡Mike Shakhomirov](https://mshakhomirov.medium.com/?source=post_page-----275876dc1fc9--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----275876dc1fc9--------------------------------)
    ·9 min read·Feb 28, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----275876dc1fc9--------------------------------)
    ·阅读时间 9 分钟·2023年2月28日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/ce5abf8987056d730cd51c36eda44998.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ce5abf8987056d730cd51c36eda44998.png)'
- en: Photo by [James Lee](https://unsplash.com/@picsbyjameslee?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 照片由 [James Lee](https://unsplash.com/@picsbyjameslee?utm_source=medium&utm_medium=referral)
    提供，来源于 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: I’m a big fan of data warehouse (DWH) solutions with ELT-designed (Extract-Load-Transform)
    data pipelines. However, at some point, I faced the requirement to *process* raw
    event data in ***Cloud Storage*** and had **to choose the file format** for data
    files.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 我是采用 ELT（Extract-Load-Transform）设计的数据仓库（DWH）解决方案的忠实粉丝。然而，在某些时候，我面临了*处理*原始事件数据的要求，并且必须**选择文件格式**用于数据文件。
- en: '*This is a typical scenario when machine learning engineers are tasked to create
    behavior datasets to train models and to generate better product recommendations
    or predict customer churn.*'
  id: totrans-9
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*这是一种典型的场景，当机器学习工程师被要求创建行为数据集以训练模型并生成更好的产品推荐或预测客户流失时。*'
- en: Choosing the right file format for our machine learning pipelines was crucial
    as it might have changed data I/O times significantly and would have definitely
    had a wider impact on our model trainer performance.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 为我们的机器学习管道选择正确的文件格式至关重要，因为这可能显著改变数据的 I/O 时间，并且肯定会对我们的模型训练性能产生更广泛的影响。
- en: '[](https://pub.towardsai.net/supercharge-your-data-engineering-skills-with-this-machine-learning-pipeline-b69d159780b7?source=post_page-----275876dc1fc9--------------------------------)
    [## Supercharge Your Data Engineering Skills with This Machine Learning Pipeline'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://pub.towardsai.net/supercharge-your-data-engineering-skills-with-this-machine-learning-pipeline-b69d159780b7?source=post_page-----275876dc1fc9--------------------------------)
    [## 利用这个机器学习管道提升你的数据工程技能'
- en: Data modeling, Python, DAGs, Big Data file formats, costs… It covers everything
  id: totrans-12
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据建模、Python、DAGs、大数据文件格式、成本……应有尽有。
- en: pub.towardsai.net](https://pub.towardsai.net/supercharge-your-data-engineering-skills-with-this-machine-learning-pipeline-b69d159780b7?source=post_page-----275876dc1fc9--------------------------------)
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '[pub.towardsai.net](https://pub.towardsai.net/supercharge-your-data-engineering-skills-with-this-machine-learning-pipeline-b69d159780b7?source=post_page-----275876dc1fc9--------------------------------)'
- en: Another thing to consider was the size of the data as we were paying already
    too much for the file storage.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个需要考虑的因素是数据的大小，因为我们已经为文件存储支付了过多费用。
- en: This story aims to consider these important questions and other options to find
    the optimal big data file format for the data pipelines.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 本文旨在探讨这些重要问题及其他选项，以找到数据管道的最佳大数据文件格式。
- en: '*The requirement was simple, emphasized the idea of using the datalake and
    reducing the costs related to data storage in the data warehouse.*'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '*需求很简单，强调了使用数据湖的想法，并减少与数据仓库中数据存储相关的成本。*'
- en: I created an **archive** bucket that would be the cheapest storage class and
    prepared to *extract* the data from some really big tables I had in my DWH. I
    have to say, those tables were heavy and had a lot of raw events with user engagement
    data. Therefore, they were the most expensive storage part of the whole data platform.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 我创建了一个**归档**存储桶，这是最便宜的存储类别，并准备*提取*我在 DWH 中的一些非常大的表的数据。我不得不说，那些表非常庞大，包含了大量的用户参与数据原始事件。因此，它们是整个数据平台中最昂贵的存储部分。
- en: And then I had to stop at some point to make a decision on which file format
    to use because storage size wasn’t the only consideration.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我不得不停下来决定使用哪种文件格式，因为存储大小并不是唯一的考虑因素。
- en: '*Indeed, in some scenarios read/write time or better schema support might be
    more important*'
  id: totrans-19
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*确实，在某些场景下，读写时间或更好的模式支持可能更重要*'
- en: Parquet vs ORC vs AVRO vs JSON
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Parquet vs ORC vs AVRO vs JSON
- en: With the rise of Data Mesh and a considerable number of data processing tools
    available in the **Hadoop** eco-system, it might be more effective **to process
    raw event data in the data lake**.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 随着数据网格的兴起以及**Hadoop** 生态系统中大量数据处理工具的出现，**在数据湖中处理原始事件数据**可能更有效。
- en: '*Modern data warehouse solutions are great but some* ***pricing models*** *are
    more expensive than the others and definitely more expensive than datalake tools*'
  id: totrans-22
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*现代数据仓库解决方案非常棒，但有些* ***定价模型*** *比其他模型更贵，而且肯定比数据湖工具更贵*'
- en: One of the **greatest benefits** of big data **file formats** is that they carry
    schema information on board. Therefore, it is much easier to load data in, split
    the data to process it more efficiently, etc. JSON can’t offer that feature and
    we would want to define the schema each time we read, load or validate the data.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 大数据 **文件格式** 的 **最大优势** 之一是它们携带了模式信息。因此，加载数据、拆分数据以更高效地处理等操作要容易得多。JSON 无法提供此功能，我们需要在每次读取、加载或验证数据时定义模式。
- en: Columnar ORC and Parquet
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 列式 ORC 和 Parquet
- en: '**Parquet** is a ***column-oriented*** data storage format designed for the
    *Apache Hadoop* ecosystem (backed by Cloudera, in collaboration with Twitter).
    It is very popular among data scientists and data engineers working with **Spark**.
    When working with huge amounts of data, you start to notice the major advantage
    of columnar data, which is achieved when a table has many more rows than columns.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '**Parquet** 是一种 ***面向列*** 的数据存储格式，专为 *Apache Hadoop* 生态系统设计（由 Cloudera 支持，与
    Twitter 合作）。它在使用**Spark**的数据显示科学家和数据工程师中非常受欢迎。当处理大量数据时，你会开始注意到列式数据的主要优势，这是在表中行数远多于列数时实现的。'
- en: '![](../Images/45fc6ec974c1bc982924ea6d8d9a1976.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/45fc6ec974c1bc982924ea6d8d9a1976.png)'
- en: 'Parquet File Layout. Source: [apache.org](http://apache.org) (Apache 2.0 license)'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: Parquet 文件布局。来源：[apache.org](http://apache.org)（Apache 2.0 许可证）
- en: '*Spark scales well and that’s why everybody likes it*'
  id: totrans-28
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*Spark 扩展性很好，这也是为什么大家喜欢它*'
- en: In fact, Parquet is a default data file format for Spark. **Parquet** performs
    beautifully while querying and working with analytical workloads.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，Parquet 是 Spark 的默认数据文件格式。**Parquet** 在查询和处理分析工作负载时表现出色。
- en: '*Columnar formats are more suitable for OLAP analytical queries. Specifically,
    we would want to retrieve only the column we need to perform an analytical query.
    That definitely has certain implications on memory and therefore, on performance
    and speed.*'
  id: totrans-30
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*列式格式更适合 OLAP 分析查询。具体来说，我们希望仅检索执行分析查询所需的列。这对内存有一定的影响，因此对性能和速度也有影响。*'
- en: '**ORC (Optimised Row Columnar)** is also a column-oriented data storage format
    similar to Parquet which carries a schema on board. it means that like Parquet
    it is **self-describing** and we can use it to load data into different disks
    or nodes.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '**ORC（优化行列式）** 也是一种列式数据存储格式，与 Parquet 类似，并且携带了模式信息。这意味着像 Parquet 一样，它是**自描述**的，我们可以用它将数据加载到不同的磁盘或节点中。'
- en: '![](../Images/b7301da75334cf83797ce798485045a3.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b7301da75334cf83797ce798485045a3.png)'
- en: 'ORC file layout. Source: [apache.org](http://apache.org) (Apache 2.0 license)'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: ORC 文件布局。来源：[apache.org](http://apache.org)（Apache 2.0 许可证）
- en: I did a little test and it seems that both **Parquet and ORC** offer similar
    compression ratios. However, there is an opinion that ORC is more compression
    efficient.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 我做了一个小测试，结果显示 **Parquet 和 ORC** 提供了类似的压缩比。然而，有一种观点认为 ORC 的压缩效率更高。
- en: '*10 Mb compressed with* ***SNAPPY*** *algorithm will turn into 2.4Mb in* ***Parquet***'
  id: totrans-35
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*10 Mb 使用* ***SNAPPY*** *算法压缩后会变成 2.4Mb 的* ***Parquet***'
- en: However, I have a feeling that **ORC** is supported by a smaller number of Hadoop
    projects than Parquet, i.e. Hive and Pig.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 不过，我感觉**ORC**的支持项目数量少于 Parquet，例如 Hive 和 Pig。
- en: So if we want a wider range of tools to run our OLAP analytics in the data lake
    I would recommend using **Parquet**. It has wider project support and especially
    **Spark**.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们希望在数据湖中运行更广泛的 OLAP 分析工具，我建议使用**Parquet**。它有更广泛的项目支持，尤其是**Spark**。
- en: '*In reality, PARQUET and ORC have somewhat different columnar architecture.*'
  id: totrans-38
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*实际上，PARQUET 和 ORC 的列式架构有所不同。*'
- en: Both work really well saving you a great deal of disk space.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 两者都非常有效地节省了大量的磁盘空间。
- en: However, data in **ORC** files are organized into independent *stripes* of data.
    Each has its own separate index, row data, and footer. It enables large, efficient
    reads from HDFS, making this format.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 不过，**ORC** 文件中的数据组织成独立的*条带*。每个条带都有自己独立的索引、行数据和页脚。这使得从 HDFS 进行大规模高效读取成为可能。
- en: The data is stored as pages in **Parquet**, and each page includes header information,
    definition level information, and repetition level information.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 数据以页面形式存储在**Parquet**中，每页包含头部信息、定义级别信息和重复级别信息。
- en: It is very effective when it comes to supporting a complicated **nested** data
    structure and seems to be more efficient at performing IO-type operations on data.
    The great advantage here is that **read time** can be significantly decreased
    if we choose only the columns we need.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 当支持复杂的**嵌套**数据结构时，它非常有效，并且在执行 IO 类型的数据操作时似乎更高效。这里的一个巨大优势是，如果我们只选择所需的列，**读取时间**可以显著减少。
- en: My personal experience suggests that selecting just a few columns can… read
    the data up to 30 times (!) faster than reading the same file with the complete
    schema.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 我的个人经验表明，只选择几列可以…将数据读取速度提高至 30 倍（！），比读取包含完整模式的相同文件快。
- en: Read the data 30 times faster (!)
  id: totrans-44
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 读取数据速度快 30 倍（！）
- en: Not bad, hah? Other tests performed by data engineers confirmed that too. I’ll
    add that link at the bottom.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 不错吧？数据工程师进行的其他测试也证实了这一点。我会在底部添加那个链接。
- en: '*Long story short, choose ORC if you work on HIVE. It is better optimized for
    it. And Choose Parquet if you work with Spark as that would be the default storage
    format for it.*'
  id: totrans-46
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*长话短说，如果你在 HIVE 上工作，选择 ORC 更佳。它对 HIVE 更优化。而选择 Parquet 如果你与 Spark 工作，因为这将是 Spark
    的默认存储格式。*'
- en: In everything else, both ORC and Parquet share similar architecture and KPIs
    look roughly the same.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在其他方面，ORC 和 Parquet 的架构相似，KPIs 看起来大致相同。
- en: AVRO
  id: totrans-48
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: AVRO
- en: '**AVRO** is a **row-based** storage format where data is indexed to improve
    query performance.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '**AVRO** 是一种**基于行**的存储格式，数据经过索引以提高查询性能。'
- en: It defines data types and schemas using JSON data and stores the data in a binary
    format (condensed) that help with disk space.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 它使用 JSON 数据定义数据类型和模式，并以二进制格式（压缩）存储数据，以帮助节省磁盘空间。
- en: '![](../Images/6535c558a34c0ed7ca5047e18c4ffd04.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6535c558a34c0ed7ca5047e18c4ffd04.png)'
- en: 'avro file structure. Source: [apache.org](http://apache.org) (Apache 2.0 license)'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: avro 文件结构。来源：[apache.org](http://apache.org)（Apache 2.0 许可证）
- en: Compared to Parquet and ORC it seems that it offers less efficient compression
    but faster **write** speeds.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 与 Parquet 和 ORC 相比，AVRO 似乎提供了较低效的压缩但更快的**写入**速度。
- en: '*10 Mb* ***Parquet*** *compressed with* ***SNAPPY*** *algorithm will turn into
    2.4Mb*'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*10 Mb* ***Parquet*** *用* ***SNAPPY*** *算法压缩后将变为 2.4Mb*'
- en: '*10 Mb* ***AVRO*** *compressed with* ***SNAPPY*** *algorithm will turn into
    6.2Mb*'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*10 Mb* ***AVRO*** *用* ***SNAPPY*** *算法压缩后将变为 6.2Mb*'
- en: '*The same 10 Mb* ***AVRO*** *compressed with* ***DEFLATE*** *algorithm will
    require 4Mb of storage*'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*相同的 10 Mb* ***AVRO*** *用* ***DEFLATE*** *算法压缩后将需要 4Mb 存储空间*'
- en: The main advantage of AVRO is a **schema-evolution support**. In other words,
    as data schemas evolve over time, ***AVRO*** enables those changes by tolerating
    fields that have been added, removed, or altered.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: AVRO 的主要优势是**模式演进支持**。换句话说，随着数据模式的演变，***AVRO*** 通过容忍添加、删除或更改的字段来支持这些变化。
- en: '*AVRO works really well with things like added fields and changed fields, missing
    fields.*'
  id: totrans-58
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*AVRO 在处理如新增字段、变更字段和缺失字段时表现非常好。*'
- en: AVRO is more efficient while working with ***write-intensive***, big-data operations.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: AVRO 在处理***写入密集型***大数据操作时更高效。
- en: It is a great candidate to store data in `source` layer or a `landing` area
    of our data platform because these *f****iles are usually being read as a whole***
    for further transformation depending on the data pipeline. Any schema evolution
    changes are handled with ease.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 它是存储数据于数据平台的`source`层或`landing`区域的一个优秀选择，因为这些*文件通常会被作为整体读取*以进行进一步转换，具体取决于数据管道。任何模式演变更改都能轻松处理。
- en: '[](/data-pipeline-design-patterns-100afa4b93e3?source=post_page-----275876dc1fc9--------------------------------)
    [## Data pipeline design patterns'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/data-pipeline-design-patterns-100afa4b93e3?source=post_page-----275876dc1fc9--------------------------------)
    [## 数据管道设计模式'
- en: Choosing the right architecture with examples
  id: totrans-62
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 选择合适的架构并附带示例
- en: towardsdatascience.com](/data-pipeline-design-patterns-100afa4b93e3?source=post_page-----275876dc1fc9--------------------------------)
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/data-pipeline-design-patterns-100afa4b93e3?source=post_page-----275876dc1fc9--------------------------------)
- en: Even applications written in different languages can share data saved using
    AVRO. Data exchange services might require a **coding generator** to decipher
    data specifications and generate code to access data. AVRO is an ideal contender
    for scripting languages because it [doesn’t require this](https://avro.apache.org/docs/1.11.1/getting-started-python/#serializing-and-deserializing-without-code-generation).
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 即使是用不同语言编写的应用程序也可以共享使用AVRO保存的数据。数据交换服务可能需要一个**编码生成器**来解读数据规格并生成访问数据的代码。AVRO是脚本语言的理想选择，因为它[不需要这个](https://avro.apache.org/docs/1.11.1/getting-started-python/#serializing-and-deserializing-without-code-generation)。
- en: JSON
  id: totrans-65
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: JSON
- en: For online applications, structured data is frequently serialized and sent using
    the JavaScript Object Notation (JSON) format.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 对于在线应用程序，结构化数据通常以JavaScript对象表示法（JSON）格式进行序列化和发送。
- en: This implies that a significant portion of the big data is gathered and kept
    in a JSON format.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着大部分大数据以JSON格式收集和存储。
- en: However, because JSON is not highly typed nor schema-enriched, dealing with
    JSON files in big data technologies like Hadoop may be sluggish.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，由于JSON既不高度类型化也没有丰富的模式，处理大数据技术如Hadoop中的JSON文件可能会很缓慢。
- en: Basically, it’s a no-go for Big Data processing frameworks. This is the main
    reason Parquet and ORC formats were created.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 基本上，它对大数据处理框架来说是不可行的。这也是Parquet和ORC格式被创建的主要原因。
- en: Compression
  id: totrans-70
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 压缩
- en: '*How does it work?*'
  id: totrans-71
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*它是如何工作的？*'
- en: A quick answer is to choose **splittable** compression types if our primary
    objective is to gain data processing performance, and non-splittable like **GZIP,
    DEFLATE** if storage costs deduction is our main objective.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们的主要目标是提高数据处理性能，那么选择**可拆分**压缩类型是快速答案；如果存储成本减少是我们的主要目标，那么选择非可拆分的如**GZIP**和**DEFLATE**。
- en: '*What is splittable?*'
  id: totrans-73
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*什么是可拆分的？*'
- en: '***Splittable*** means that Hadoop Mapper can split or divide a large file
    and process it in parallel. Compressing codecs apply compression in blocks and
    when it is applied at the block level mappers can read a single file concurrently
    even if it is a very large file.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '***可拆分***意味着Hadoop Mapper可以拆分或划分大型文件并并行处理它。压缩编解码器在块级别应用压缩，当它在块级别应用时，即使是非常大的文件，映射器也可以同时读取单个文件。'
- en: Having said that, *splittable* compression types are **LZO**, **LZ4**, **BZIP2**
    and **SNAPPY** whereas **GZIP** isn’t. These are very fast compressors and very
    fast decompressors. So if we can tolerate bigger storage then it is a better choice
    than **GZIP**.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 话虽如此，*可拆分的*压缩类型有**LZO**、**LZ4**、**BZIP2**和**SNAPPY**，而**GZIP**不是。这些压缩器和解压器非常快。因此，如果我们可以容忍较大的存储，那么它比**GZIP**更好的选择。
- en: '**GZIP** on average has a 30% better compression rate and would be a good choice
    for data that doesn’t require frequent access (cold storage).'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '**GZIP**平均具有30%的更好压缩率，对于不需要频繁访问（冷存储）的数据来说是一个不错的选择。'
- en: External data in lakes and Data warehouse
  id: totrans-77
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 外部数据在数据湖和数据仓库中
- en: '*Can we query external data storage using a data warehouse?*'
  id: totrans-78
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*我们可以使用数据仓库查询外部数据存储吗？*'
- en: Typically, all modern data warehouses offer externally partitioned table features
    so we can run analytics on data lake data.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，所有现代数据仓库都提供外部分区表功能，因此我们可以对数据湖中的数据进行分析。
- en: There is a number of limitation of course but in general, it is enough to connect
    two worlds in order to understand how it works.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 当然有一些限制，但一般来说，它足够将两个世界连接起来，以便理解它的工作原理。
- en: 'For example, in BigQuery we can create an external table in our data warehouse
    like so:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在BigQuery中，我们可以这样在数据仓库中创建一个外部表：
- en: '[PRE0]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: You will notice that external tables are much slower in any DWH and are limited
    in many ways, i.e. limited concurrent queries, no DML and wildcard support and
    inconsistency if underlying data was changed during the query run.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 你会注意到，在任何数据仓库中，外部表的速度都较慢，并且在许多方面受限，例如并发查询数量有限、不支持数据操作语言（DML）和通配符支持，并且如果在查询运行期间底层数据发生变化，会出现不一致的问题。
- en: 'So we might want to *load* it back like so:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们可能想要像这样*加载*它：
- en: '[PRE1]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Native tables are faster.
  id: totrans-86
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 本地表更快。
- en: If there is another data processing tool, then we might want to use a Hive partition
    layout. This is usually required to enable external partitioning. i.e.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 如果有其他数据处理工具，我们可能需要使用 Hive 分区布局。这通常是为了启用外部分区。例如：
- en: '[PRE2]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Here partition keys have to be always in the same order. And for partitioning,
    we will use `key = value` pairs which will be storage folders at the same time.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，分区键必须始终保持相同的顺序。对于分区，我们将使用 `key = value` 对，这些对也将作为存储文件夹。
- en: We can use it in any other data processing framework that supports *HIVE* layout.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在任何其他支持 *HIVE* 布局的数据处理框架中使用它。
- en: '[](https://mydataschool.com/blog/hive-partitioning-layout/?source=post_page-----275876dc1fc9--------------------------------)
    [## Hive Partitioning Layout'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://mydataschool.com/blog/hive-partitioning-layout/?source=post_page-----275876dc1fc9--------------------------------)
    [## Hive 分区布局'
- en: Lakehouse design is one of my favorites because it gives us the best of two
    worlds. In a data warehouse, we can manage…
  id: totrans-92
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Lakehouse 设计是我最喜欢的设计之一，因为它融合了两者的优点。在数据仓库中，我们可以管理…
- en: mydataschool.com](https://mydataschool.com/blog/hive-partitioning-layout/?source=post_page-----275876dc1fc9--------------------------------)
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: mydataschool.com](https://mydataschool.com/blog/hive-partitioning-layout/?source=post_page-----275876dc1fc9--------------------------------)
- en: Conclusion
  id: totrans-94
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: If we need support for Spark and a flat columnar format with efficient querying
    of complex nested data structures then we should use Parquet. It is highly integrated
    with Apache Spark and is, actually, a default file format for this framework.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们需要对复杂嵌套数据结构进行高效查询的平面列式格式支持，那么我们应该使用 Parquet。它与 Apache Spark 高度集成，并且实际上是该框架的默认文件格式。
- en: If our data platform architecture relies on data pipelines built with Hive or
    Pig then ORC data format is the better choice. Having all the advantages of columnar
    format it performs beautifully in analytical data lake queries but is better integrated
    with Hive and Pig. Therefore, the performance might be better compared to Parquet.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们的数据平台架构依赖于使用 Hive 或 Pig 构建的数据管道，那么 ORC 数据格式是更好的选择。它具备列式格式的所有优点，在分析数据湖查询中表现出色，但与
    Hive 和 Pig 更好地集成。因此，与 Parquet 相比，它的性能可能更好。
- en: When we need a higher compression ratio and faster read times then Parquet or
    ORC would suit better. Compared to AVRO read times might be up to 3 times faster.
    It can be improved even further if we reduce the number of selected columns to
    read.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们需要更高的压缩比和更快的读取时间时，Parquet 或 ORC 会更合适。与 AVRO 相比，读取时间可能快多达 3 倍。如果我们减少选择读取的列数，还可以进一步提高性能。
- en: However, we don’t worry to much about the speed of input operations AVRO might
    be a better choice offering a reasonable compression with splittable SNAPPY, schema
    evolution support and way faster write speed (it is row-based remember?). AVRO
    is an absolute leader in *write-intensive*, big-data operations. Therefore, it
    suits better for storing the data in the source layer of our data platform.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我们不太担心输入操作的速度，AVRO 可能是更好的选择，它提供了合理的压缩（使用可拆分的SNAPPY）、模式演变支持，并且写入速度比其他格式快得多（记住，它是基于行的）。AVRO
    在*写入密集型*的大数据操作中绝对领先。因此，它更适合用于我们数据平台源层的数据存储。
- en: '`GZIP`, `DEFLATE` and other non-splittable compression types would work best
    for cold storage where frequent access to data is not required.'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '`GZIP`、`DEFLATE` 和其他不可拆分的压缩类型在冷存储中效果最好，这里不需要频繁访问数据。'
- en: Recommended read
  id: totrans-100
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 推荐阅读
- en: '[1\. https://cwiki.apache.org/confluence/display/hive/languagemanual+orc](https://cwiki.apache.org/confluence/display/hive/languagemanual+orc)'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '[1\. https://cwiki.apache.org/confluence/display/hive/languagemanual+orc](https://cwiki.apache.org/confluence/display/hive/languagemanual+orc)'
- en: '[2\. https://parquet.apache.org/](https://parquet.apache.org/)'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '[2\. https://parquet.apache.org/](https://parquet.apache.org/)'
- en: '[3\. https://avro.apache.org/](https://avro.apache.org/)'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '[3\. https://avro.apache.org/](https://avro.apache.org/)'
- en: '[4\. https://avro.apache.org/docs/1.11.1/getting-started-python/#serializing-and-deserializing-without-code-generation](https://avro.apache.org/docs/1.11.1/getting-started-python/#serializing-and-deserializing-without-code-generation)'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '[4\. https://avro.apache.org/docs/1.11.1/getting-started-python/#serializing-and-deserializing-without-code-generation](https://avro.apache.org/docs/1.11.1/getting-started-python/#serializing-and-deserializing-without-code-generation)'
- en: '[5\. https://avro.apache.org/docs/1.11.1/specification/](https://avro.apache.org/docs/1.11.1/specification/)'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '[5\. https://avro.apache.org/docs/1.11.1/specification/](https://avro.apache.org/docs/1.11.1/specification/)'
- en: '[6\. https://stackoverflow.com/questions/14820450/best-splittable-compression-for-hadoop-input-bz2](https://stackoverflow.com/questions/14820450/best-splittable-compression-for-hadoop-input-bz2)'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '[6\. https://stackoverflow.com/questions/14820450/best-splittable-compression-for-hadoop-input-bz2](https://stackoverflow.com/questions/14820450/best-splittable-compression-for-hadoop-input-bz2)'
- en: '[7\. https://stackoverflow.com/questions/35789412/spark-sql-difference-between-gzip-vs-snappy-vs-lzo-compression-formats](https://stackoverflow.com/questions/35789412/spark-sql-difference-between-gzip-vs-snappy-vs-lzo-compression-formats)'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '[7\. https://stackoverflow.com/questions/35789412/spark-sql-difference-between-gzip-vs-snappy-vs-lzo-compression-formats](https://stackoverflow.com/questions/35789412/spark-sql-difference-between-gzip-vs-snappy-vs-lzo-compression-formats)'
- en: 8\. [https://medium.com/ssense-tech/csv-vs-parquet-vs-avro-choosing-the-right-tool-for-the-right-job-79c9f56914a8](https://medium.com/ssense-tech/csv-vs-parquet-vs-avro-choosing-the-right-tool-for-the-right-job-79c9f56914a8)
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://medium.com/ssense-tech/csv-vs-parquet-vs-avro-choosing-the-right-tool-for-the-right-job-79c9f56914a8](https://medium.com/ssense-tech/csv-vs-parquet-vs-avro-choosing-the-right-tool-for-the-right-job-79c9f56914a8)'
