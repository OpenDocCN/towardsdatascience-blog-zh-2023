- en: Creating a YouTube Data Pipeline with AWS and Apache Airflow
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用AWS和Apache Airflow创建YouTube数据管道
- en: 原文：[https://towardsdatascience.com/creating-a-youtube-data-pipeline-with-aws-and-apache-airflow-e5d3b11de9c2](https://towardsdatascience.com/creating-a-youtube-data-pipeline-with-aws-and-apache-airflow-e5d3b11de9c2)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/creating-a-youtube-data-pipeline-with-aws-and-apache-airflow-e5d3b11de9c2](https://towardsdatascience.com/creating-a-youtube-data-pipeline-with-aws-and-apache-airflow-e5d3b11de9c2)
- en: A solution for effectively managing YouTube data with cloud services and job
    schedulers
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 一种利用云服务和作业调度程序有效管理YouTube数据的解决方案
- en: '[](https://medium.com/@aashishnair?source=post_page-----e5d3b11de9c2--------------------------------)[![Aashish
    Nair](../Images/23f4b3839e464419332b690a4098d824.png)](https://medium.com/@aashishnair?source=post_page-----e5d3b11de9c2--------------------------------)[](https://towardsdatascience.com/?source=post_page-----e5d3b11de9c2--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----e5d3b11de9c2--------------------------------)
    [Aashish Nair](https://medium.com/@aashishnair?source=post_page-----e5d3b11de9c2--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@aashishnair?source=post_page-----e5d3b11de9c2--------------------------------)[![Aashish
    Nair](../Images/23f4b3839e464419332b690a4098d824.png)](https://medium.com/@aashishnair?source=post_page-----e5d3b11de9c2--------------------------------)[](https://towardsdatascience.com/?source=post_page-----e5d3b11de9c2--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----e5d3b11de9c2--------------------------------)
    [Aashish Nair](https://medium.com/@aashishnair?source=post_page-----e5d3b11de9c2--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----e5d3b11de9c2--------------------------------)
    ·12 min read·Apr 20, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----e5d3b11de9c2--------------------------------)
    ·阅读时间12分钟·2023年4月20日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/e59f2354c5c004a43224decca7d5b946.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e59f2354c5c004a43224decca7d5b946.png)'
- en: 'Photo by Artem Podrez: [https://www.pexels.com/photo/thoughtful-woman-with-earbuds-using-laptop-4492161/](https://www.pexels.com/photo/thoughtful-woman-with-earbuds-using-laptop-4492161/)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由Artem Podrez提供：[https://www.pexels.com/photo/thoughtful-woman-with-earbuds-using-laptop-4492161/](https://www.pexels.com/photo/thoughtful-woman-with-earbuds-using-laptop-4492161/)
- en: ∘ [Introduction](#d514)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: ∘ [引言](#d514)
- en: ∘ [Problem Statement](#615d)
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: ∘ [问题陈述](#615d)
- en: ∘ [Understanding the Data Pipeline](#c674)
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: ∘ [理解数据管道](#c674)
- en: ∘ [Understanding the Data](#c793)
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: ∘ [理解数据](#c793)
- en: ∘ [Phase 1 — Setting Up The AWS Environment](#d201)
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: ∘ [第1阶段 — 设置AWS环境](#d201)
- en: ∘ [Phase 2— Facilitating AWS Operations with Boto3](#bd0e)
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: ∘ [第2阶段— 使用Boto3简化AWS操作](#bd0e)
- en: ∘ [Phase 3— Setting Up the Airflow Pipeline](#0497)
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: ∘ [第3阶段— 设置Airflow管道](#0497)
- en: ∘ [Phase 4— Running the Pipeline](#2a4f)
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: ∘ [第4阶段— 运行管道](#2a4f)
- en: ∘ [Examining the Results](#66c0)
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: ∘ [结果分析](#66c0)
- en: ∘ [Advantages of This Data Pipeline](#1a83)
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: ∘ [该数据管道的优点](#1a83)
- en: ∘ [Disadvantages of This Data Pipeline](#b332)
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: ∘ [该数据管道的缺点](#b332)
- en: ∘ [Conclusion](#62b5)
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: ∘ [结论](#62b5)
- en: Introduction
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 引言
- en: YouTube has become a major medium of exchange for information, thoughts, and
    ideas, with an average of 3 million videos being uploaded each day. The video
    streaming platform always has a new topic of conversation prepared for its audience
    with its diverse content, ranging from somber news stories to upbeat music videos.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: YouTube已经成为信息、思想和观点交流的主要媒介，每天平均上传300万个视频。这个视频流媒体平台始终准备好以其多样化的内容为观众提供新的话题，从严肃的新闻故事到轻松的音乐视频。
- en: That being said, with a constant influx of video content, it’s difficult to
    gauge what types of content attract the attention of the fickle YouTube audience
    the most.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管如此，由于视频内容的持续涌入，很难判断哪些类型的内容最能吸引变化莫测的YouTube观众的注意。
- en: In general, what types of videos get the most likes and comments? Do YouTube
    users from different countries favor different types of content? Does interest
    in specific content categories fluctuate throughout time?
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，哪些类型的视频会获得最多的点赞和评论？来自不同国家的YouTube用户是否偏爱不同类型的内容？对特定内容类别的兴趣是否会随着时间而波动？
- en: Answering these types of questions requires a systematic approach toward collecting,
    processing, and storing YouTube data for subsequent analysis.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 回答这些问题需要一种系统的方法来收集、处理和存储YouTube数据，以便后续分析。
- en: Problem Statement
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题陈述
- en: 'The goal of the project is to create a data pipeline that collects YouTube
    data for the 5 countries with the most YouTube users: Brazil, India, Indonesia,
    Mexico, and the United States. This pipeline should collect data on a daily basis
    in order to keep the information up to date.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 项目的目标是创建一个数据管道，收集 5 个 YouTube 用户最多的国家（巴西、印度、印尼、墨西哥和美国）的 YouTube 数据。该管道应每日收集数据，以保持信息的最新。
- en: 'The data collected with the pipeline will be used to answer the following questions:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 通过管道收集的数据将用于回答以下问题：
- en: What are the most popular video categories in each country?
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 各个国家最受欢迎的视频类别是什么？
- en: Does the most popular video category change over time? If so, how?
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最受欢迎的视频类别会随时间变化吗？如果会，如何变化？
- en: 'Developing and running the pipeline will require a large number of steps. Thus,
    the project will be split into 4 phases:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 开发和运行管道将需要大量步骤。因此，项目将分为 4 个阶段：
- en: '**1\. Setting up the AWS environment**'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '**1\. 设置 AWS 环境**'
- en: '**2\. Facilitating AWS operations with boto3**'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '**2\. 利用 boto3 促进 AWS 操作**'
- en: '**3\. Setting up the Airflow pipeline**'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '**3\. 设置 Airflow 管道**'
- en: '**4\. Running the Airflow pipeline**'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '**4\. 运行 Airflow 管道**'
- en: Understanding the Data Pipeline
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解数据管道
- en: 'Before beginning the project, it’s worth discussing the setup of the pipeline,
    which can be represented by the following diagram:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在项目开始之前，值得讨论管道的设置，其可以通过以下图示表示：
- en: '![](../Images/3c88925bd94aafe74b9e91f50643c461.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3c88925bd94aafe74b9e91f50643c461.png)'
- en: Data Pipeline (Created by Author)
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 数据管道（由作者创建）
- en: There’s a lot going down, so let’s break it down.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 有很多内容，所以让我们来逐一解析。
- en: Amazon Web Services (AWS) will provide all of the compute and storage services
    needed for this project, whereas Apache Airflow will schedule the relevant processes
    to run daily.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 亚马逊网络服务（AWS）将提供项目所需的所有计算和存储服务，而 Apache Airflow 将调度相关过程以便每天运行。
- en: All of the processes carried out within the AWS environment will be facilitated
    by Python scripts that leverage boto3, the AWS software development kit (SDK)
    for Python. These scripts are to be scheduled with an Apache Airflow Directed
    Acyclic Graph (DAG), which is deployed into an EC2 instance.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 所有在 AWS 环境中进行的过程将由利用 boto3 的 Python 脚本来实现，boto3 是 AWS 的 Python 软件开发工具包（SDK）。这些脚本将通过
    Apache Airflow 的有向无环图（DAG）进行调度，并部署到 EC2 实例中。
- en: The DAG stored in the EC2 instance will pull data using the YouTube API and
    store it in an S3 bucket as csv files. The program will then use Amazon Athena
    to query the data in that bucket. The results of the query will be stored within
    the same S3 bucket.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 存储在 EC2 实例中的 DAG 将使用 YouTube API 拉取数据，并将其存储在 S3 桶中作为 csv 文件。程序随后将使用 Amazon Athena
    查询该桶中的数据。查询结果将存储在同一 S3 桶中。
- en: Understanding the Data
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解数据
- en: Next, understanding the YouTube data provided by the API will provide some insight
    into what processes it needs to be subject to in the pipeline.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，了解 API 提供的 YouTube 数据将对其在管道中需要经过的过程提供一些见解。
- en: 'The response of the API call contains information on up to 50 of the most popular
    videos in a given country at a given time. The raw data comes in a JSON format:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: API 调用的响应包含了在给定时间内某个国家中最多 50 个最受欢迎的视频的信息。原始数据以 JSON 格式呈现：
- en: '[PRE0]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: For more information, feel free to visit the YouTube [documentation](https://developers.google.com/youtube/v3/docs/videos/list).
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 欲了解更多信息，请随时访问 YouTube [文档](https://developers.google.com/youtube/v3/docs/videos/list)。
- en: In order to query this with SQL later on, this raw data will be transformed
    into csv format before being stored in AWS. The processed data will also omit
    features that are not relevant to this use case.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 为了方便后续使用 SQL 查询，这些原始数据将在存储到 AWS 之前转换为 csv 格式。处理后的数据还将省略与此用例无关的特征。
- en: 'This transformation from JSON data to tabular data is summarized by the following:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: JSON 数据到表格数据的转换总结如下：
- en: '![](../Images/d28600b0c0cdc7dbd4630a5b30a4fbbb.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d28600b0c0cdc7dbd4630a5b30a4fbbb.png)'
- en: Transforming JSON Data (Created By Author)
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: JSON 数据转换（由作者创建）
- en: Moreover, an API call only reveals information on a single country. In other
    words, obtaining data on popular videos in N countries will require N API calls.
    Since the goal is to extract YouTube data for 5 countries, each iteration will
    require making 5 API calls.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，API 调用只能显示单一国家的信息。换句话说，获取 N 个国家的热门视频数据将需要 N 次 API 调用。由于目标是提取 5 个国家的 YouTube
    数据，每次迭代将需要进行 5 次 API 调用。
- en: After storing the data in AWS S3 as a csv file, the data will be queried to
    determine the most popular video category for each country and date.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在 AWS S3 中以 csv 文件格式存储数据后，将对数据进行查询，以确定每个国家和日期的最受欢迎的视频类别。
- en: 'The query output will contain the following fields: date, country, video category,
    and number of videos.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 查询输出将包含以下字段：日期、国家、视频类别和视频数量。
- en: '![](../Images/bc183ae775a6faa00cf5b14604081f55.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bc183ae775a6faa00cf5b14604081f55.png)'
- en: Format of Query Output (Created By Author)
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 查询输出格式（作者创建）
- en: Phase 1 — Setting Up the AWS Environment
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 阶段 1 — 设置 AWS 环境
- en: '![](../Images/974440ed29f614c75ce1aa666dacbd3c.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/974440ed29f614c75ce1aa666dacbd3c.png)'
- en: AWS Environment (Created By Author)
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: AWS 环境（作者创建）
- en: Let’s first create/deploy the AWS resources that will be required to store and
    query the YouTube data.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 首先创建/部署存储和查询 YouTube 数据所需的 AWS 资源。
- en: '**1\. An IAM User**'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '**1\. 一个 IAM 用户**'
- en: An IAM user will be created just for this project. This user will gain permission
    to use the Amazon EC2, Amazon S3, and Amazon Athena.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 将为这个项目创建一个 IAM 用户。这个用户将获得使用 Amazon EC2、Amazon S3 和 Amazon Athena 的权限。
- en: The user account will also be provided with access keys, which will be needed
    to use these resources with Python scripts.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 用户帐户还将提供访问密钥，这些密钥将用于通过 Python 脚本使用这些资源。
- en: '**2\. EC2 instance**'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '**2\. EC2 实例**'
- en: We will set up a t2-small instance with Ubuntu AMI. This is where the Airflow
    pipeline will be deployed.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将设置一个带有 Ubuntu AMI 的 t2-small 实例。这是 Airflow 流水线将被部署的地方。
- en: The instance has a key pair file named “youtube_kp.pem”, which will be needed
    to access the EC2 instance with secure shell (SSH) and copy files to the instance
    with secure copy protocol (SCP).
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 实例有一个名为“youtube_kp.pem”的密钥对文件，该文件将用于通过安全外壳（SSH）访问 EC2 实例，并通过安全复制协议（SCP）将文件复制到实例中。
- en: In addition, this instance will have an inbound rule added to enable the user
    to view the Airflow webserver (the user interface).
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，将为实例添加一个入站规则，以便用户可以查看 Airflow Web 服务器（用户界面）。
- en: 'After connecting to the instance with SSH, the following installations will
    be made:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 连接到实例后，将进行以下安装：
- en: '[PRE1]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Next, a virtual environment named `youtube`, where the project will be run in,
    is set up.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，将设置一个名为 `youtube` 的虚拟环境，其中将运行该项目。
- en: '[PRE2]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: After that, a directory named `airflow` will be added to the instance. This
    is where all of the required files including the DAG will be stored.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 之后，将在实例中添加一个名为 `airflow` 的目录。这里将存储包括 DAG 在内的所有所需文件。
- en: '[PRE3]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '**2\. S3 Bucket**'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '**2\. S3 存储桶**'
- en: The S3 bucket will store the data pulled with the YouTube API as well as the
    outputs of the queries that are executed. The bucket will be named `youtube-data-storage`.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: S3 存储桶将存储通过 YouTube API 提取的数据以及执行查询的输出。存储桶将命名为 `youtube-data-storage`。
- en: '![](../Images/3f4a255ea70df150697896210a396cb1.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3f4a255ea70df150697896210a396cb1.png)'
- en: S3 Bucket (Created By Author)
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: S3 存储桶（作者创建）
- en: There is very little that needs to be done in terms of configuration. The bucket
    will simply contain two folders, named `data` and `query-output`, which will contain
    the pulled data and the query outputs, respectively.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在配置方面需要做的事情很少。存储桶将仅包含两个文件夹，分别名为 `data` 和 `query-output`，这些文件夹将分别包含提取的数据和查询输出。
- en: '![](../Images/b00eaa13cf2343548e53485a0a62a749.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b00eaa13cf2343548e53485a0a62a749.png)'
- en: S3 Bucket Folders (Created By Author)
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: S3 存储桶文件夹（作者创建）
- en: '**3\. AWS Athena**'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '**3\. AWS Athena**'
- en: Next, AWS Athena will be configured to query the data stored in the `data` folder
    and store the output in the `query-output` folder. By default, it leaves us an
    empty database with no tables.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，将配置 AWS Athena 以查询存储在 `data` 文件夹中的数据，并将输出存储在 `query-output` 文件夹中。默认情况下，它会留下一个没有表的空数据库。
- en: '![](../Images/b00f9b3a6cb4c80ee5170c47fd13af3f.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b00f9b3a6cb4c80ee5170c47fd13af3f.png)'
- en: Empty Database (Created by Author)
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 空数据库（作者创建）
- en: First, a new database named `youtube` is created in the query editor.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，在查询编辑器中创建一个名为 `youtube` 的新数据库。
- en: '[PRE4]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Next, an iceberg table named `youtube_videos` is created. The table’s fields
    should match those of the csv files that will be loaded into the S3 bucket. It
    should also specify the location of the csv files that will be queried.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，创建一个名为 `youtube_videos` 的冰山表。该表的字段应与将加载到 S3 存储桶中的 csv 文件的字段匹配。还应指定将被查询的
    csv 文件的位置。
- en: '[PRE5]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Finally, the query result location will be set to the `query-output` subdirectory
    in the `youtube-data-storage` bucket in the settings.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，查询结果位置将设置为设置中的 `youtube-data-storage` 存储桶中的 `query-output` 子目录。
- en: '![](../Images/c214d161ed7405645fbdfcfc4bd676cc.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c214d161ed7405645fbdfcfc4bd676cc.png)'
- en: Query Result Location (Created by Author)
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 查询结果位置（由作者创建）
- en: Phase 2— Facilitating AWS Operations with Boto3
  id: totrans-92
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第 2 阶段——利用 Boto3 促进 AWS 操作
- en: '![](../Images/3b13627766f4a17b6b238247c72b90e6.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3b13627766f4a17b6b238247c72b90e6.png)'
- en: AWS SDK (Created by Author)
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: AWS SDK（由作者创建）
- en: With the AWS environment set up, we can now focus on developing the scripts
    that will facilitate the transfer and processing of data.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 设置好 AWS 环境后，我们现在可以专注于开发将促进数据传输和处理的脚本。
- en: First, we pull the data from the YouTube API, convert it to a tabular format,
    and store it in the S3 bucket as a csv file with the following function named
    `pull_youtube_data.py`.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们从 YouTube API 拉取数据，将其转换为表格格式，并使用名为`pull_youtube_data.py`的函数将其存储在 S3 存储桶中，格式为
    csv 文件。
- en: 'Note: It might be worth remembering the names of these Python functions, as
    it will make it easier to follow along when the Airflow DAG is configured.'
  id: totrans-97
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 注意：记住这些 Python 函数的名称可能会很有帮助，因为这将使在配置 Airflow DAG 时更容易跟进。
- en: The only parameter of this function is `region_code` , which is the 2-digit
    code that denotes the country of interest.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 此函数的唯一参数是 `region_code`，这是表示感兴趣国家的两位数字代码。
- en: For instance, `pull_youtube_data('US')` will return data on the most popular
    videos in the United States. To obtain information on the most popular videos
    in the 5 countries, this function will need to be run 5 times.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，`pull_youtube_data('US')` 将返回美国最受欢迎的视频数据。为了获取 5 个国家最受欢迎的视频信息，这个函数需要运行 5 次。
- en: Next, we write a function that uses boto3 to analyze the data in the S3 bucket
    with Amazon Athena. This entails writing an SQL query on the `data` folder and
    storing the result in the `query-output` folder.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们编写一个使用 boto3 分析 S3 存储桶中数据的函数，使用 Amazon Athena。这涉及在 `data` 文件夹中编写 SQL 查询，并将结果存储在
    `query-output` 文件夹中。
- en: These steps are carried out in the function named `run_query`.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 这些步骤在名为 `run_query` 的函数中执行。
- en: Phase 3— Setting Up the Airflow Pipeline
  id: totrans-102
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第 3 阶段——设置 Airflow 流水线
- en: '![](../Images/c7905b4fceae68ae0338127f832e7f98.png)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c7905b4fceae68ae0338127f832e7f98.png)'
- en: Scheduling Python Code with Apache Airflow (Created by Author)
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 Apache Airflow 调度 Python 代码（由作者创建）
- en: After creating the Python functions, we can build the Airflow pipeline and schedule
    the functions to run on a daily basis. There are a number of steps in this phase,
    so brace yourself.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 在创建了 Python 函数之后，我们可以构建 Airflow 流水线，并安排函数每天运行。这一阶段有很多步骤，请做好准备。
- en: 1\. First, we write the Python script named `youtube_dag.py` that instantiates
    the DAG, defines the tasks, and establishes the dependencies between the tasks.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 1\. 首先，我们编写名为 `youtube_dag.py` 的 Python 脚本，该脚本实例化 DAG，定义任务，并建立任务之间的依赖关系。
- en: In short, the DAG is configured to execute the `pull_data` function 5 times
    (once for each country) before running the `run_query` function. The DAG runs
    are carried out on a daily basis.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，DAG 被配置为在运行 `run_query` 函数之前执行 `pull_data` 函数 5 次（每个国家一次）。DAG 运行是每天进行的。
- en: Now, we can deploy the Airflow DAG in the created EC2 instance.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以在创建的 EC2 实例中部署 Airflow DAG。
- en: '**2\. Access the EC2 instance with SSH**'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '**2\. 使用 SSH 访问 EC2 实例**'
- en: '[PRE6]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '**3\. Enter the created virtual environment**'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '**3\. 进入创建的虚拟环境**'
- en: '[PRE7]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '**4\. Install the required dependencies (including Apache Airflow)**'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '**4\. 安装所需的依赖项（包括 Apache Airflow）**'
- en: '[PRE8]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '**5\. Specify the location of the airflow home directory**'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '**5\. 指定 airflow 主目录的位置**'
- en: 'On Ubuntu, this can be done with the following command:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Ubuntu 上，可以使用以下命令完成：
- en: '[PRE9]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'To set the Airflow home directory to the created airflow folder, add the following
    line to the text editor:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 为了将 Airflow 主目录设置为创建的 airflow 文件夹，请将以下行添加到文本编辑器中：
- en: '[PRE10]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '**6\. Initialize an Airflow database**'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '**6\. 初始化 Airflow 数据库**'
- en: '[PRE11]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '**7\. Store the YouTube DAG (and other dependencies) in the required location**'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '**7\. 将 YouTube DAG（及其他依赖项）存储在所需位置**'
- en: The airflow.cfg file will indicate the directory that the DAG has to be in to
    be detected.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: airflow.cfg 文件将指示 DAG 所需的目录以便被检测到。
- en: '![](../Images/900d4a51361baf061cfb511212b86733.png)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/900d4a51361baf061cfb511212b86733.png)'
- en: Location of DAG File (Created By Author)
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: DAG 文件的位置（由作者创建）
- en: The `youtube_dag.py`, along with all other Python files are copied into this
    directory from the local machine using secure copy (SCP). The command for executing
    the copy has the following format.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '`youtube_dag.py` 和其他所有 Python 文件都使用安全拷贝（SCP）从本地机器复制到此目录。执行复制的命令格式如下。'
- en: '[PRE12]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: After executing the SCP commands, the `dags` directory should have all the necessary
    files.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 执行 SCP 命令后，`dags` 目录应包含所有必要的文件。
- en: '![](../Images/460998b73755aa4afd933dc7a8d062d8.png)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/460998b73755aa4afd933dc7a8d062d8.png)'
- en: Files in Dags Directory (Created by Author)
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: Dags 目录中的文件（由作者创建）
- en: 'Finally, Airflow should have access to the created DAG. To confirm this, we
    can use the command:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，Airflow 应该可以访问创建的 DAG。为确认这一点，我们可以使用以下命令：
- en: '[PRE13]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '![](../Images/7c6642d4a983f5ef860d020e44734fc4.png)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7c6642d4a983f5ef860d020e44734fc4.png)'
- en: Code Output (Created By Author)
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 代码输出（由作者创建）
- en: Phase 4— Running the Pipeline
  id: totrans-135
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第四阶段——运行管道
- en: Finally, we have set up the AWS resources, the Python scripts, and the Airflow
    DAG! The data pipeline is now ready to run!
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，我们已经设置了 AWS 资源、Python 脚本和 Airflow DAG！数据管道现在可以运行了！
- en: We can look at the workflow using the Airflow webserver.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用 Airflow web 服务器查看工作流。
- en: '[PRE14]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Although the DAG is scheduled to run automatically, we can run trigger it manually
    for the first iteration by running the following in the command line interface:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管 DAG 被设置为自动运行，但我们可以通过在命令行界面中运行以下命令手动触发第一次迭代：
- en: '[PRE15]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Alternatively, it can be triggered to run by clicking the “play” button in the
    webserver.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 另外，也可以通过点击 web 服务器中的“播放”按钮来触发运行。
- en: '![](../Images/c7992e8186cfccbd77ff21444e22bae4.png)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c7992e8186cfccbd77ff21444e22bae4.png)'
- en: Running a DAG Manually (Created by Author)
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 手动运行 DAG（由作者创建）
- en: 'For a better grasp of the constituents of the workflow, we can view the DAG’s
    graph:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地理解工作流的组成部分，我们可以查看 DAG 的图形：
- en: '![](../Images/fb1ff7cb4f7bbdf9ad2a2fdaba6c850d.png)'
  id: totrans-145
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fb1ff7cb4f7bbdf9ad2a2fdaba6c850d.png)'
- en: DAG Graph (Created by Author)
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: DAG 图形（由作者创建）
- en: As shown in the graph, the YouTube data containing the most popular videos for
    Brazil, Indonesia, India, Mexico, and the United States are first pulled with
    the YouTube API and stored in the S3 bucket. After the data for all countries
    is ingested, AWS Athena runs the pre-defined query to determine the most popular
    categories for each country on each given day.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 如图所示，包含巴西、印度尼西亚、印度、墨西哥和美国最受欢迎视频的 YouTube 数据首先通过 YouTube API 提取并存储在 S3 桶中。在所有国家的数据被摄取后，AWS
    Athena 运行预定义的查询，以确定每个国家在每个给定日期的最受欢迎类别。
- en: Furthermore, the green edges around all of the tasks signify that the tasks
    have been run successfully.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，所有任务周围的绿色边缘表示任务已成功运行。
- en: 'Note: There is no need to manually trigger the DAG every time; it will run
    on the pre-defined schedule, which is daily in this case.'
  id: totrans-149
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 注意：无需每次都手动触发 DAG；它会按照预定义的计划运行，在此情况下是每天运行一次。
- en: Examining the Results
  id: totrans-150
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 检查结果
- en: Let's see what the stored CSV files and the query output looks like after the
    pipeline is run twice (i.e., two dates for each country).
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看在管道运行两次后（即每个国家两个日期），存储的 CSV 文件和查询输出是什么样的。
- en: '![](../Images/1392f32d3609e0186903fc7aa62e670d.png)'
  id: totrans-152
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1392f32d3609e0186903fc7aa62e670d.png)'
- en: Data Directory in S3 Bucket (Created By Author)
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: S3 桶中的数据目录（由作者创建）
- en: We can see 10 csv files (1 for each country and date) in the data folder in
    the `youtube-data-storage` bucket.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在 `youtube-data-storage` 桶的数据文件夹中看到 10 个 csv 文件（每个国家和日期一个）。
- en: '![](../Images/fd6d373d83c2fd0ec6f2dcb0599561ec.png)'
  id: totrans-155
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fd6d373d83c2fd0ec6f2dcb0599561ec.png)'
- en: query-output directory in S3 Bucket (Created By Author)
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: S3 桶中的 query-output 目录（由作者创建）
- en: 'The `query-output` directory has a single csv file that contains the query
    output. The contents of the file are in the following:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '`query-output` 目录中有一个包含查询输出的 csv 文件。文件内容如下：'
- en: '![](../Images/5dc7ccdf8758f04c76fc42768a6a5625.png)'
  id: totrans-158
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5dc7ccdf8758f04c76fc42768a6a5625.png)'
- en: SQL Query Output (Created By Author)
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: SQL 查询输出（由作者创建）
- en: 'While there are many video categories within the YouTube platform, it seems
    that the most popular videos in Brazil, Indonesia, India, Mexico, and the United
    States fall into only 4 categories: “Music”, “Entertainment”, “People & Blogs”,
    and “Sports”.'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管 YouTube 平台上有许多视频类别，但似乎巴西、印度尼西亚、印度、墨西哥和美国的最受欢迎视频仅归入 4 类：**“音乐”**、**“娱乐”**、**“人物与博客”**
    和 **“体育”**。
- en: While this is an interesting finding, there is very little that can be inferred
    from it since it only comprises 2 days' worth of information. However, after running
    the Airflow DAG repeatedly, the information will adequately capture how the audiences’
    preference in videos for each country changes over time.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这是一个有趣的发现，但由于信息仅包括 2 天的内容，因此几乎无法从中得出任何结论。然而，在反复运行 Airflow DAG 后，这些信息将充分捕捉到每个国家的视频观众偏好随时间变化的情况。
- en: Advantages of This Data Pipeline
  id: totrans-162
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 本数据管道的优势
- en: Collecting YouTube data in such a pipeline is beneficial for a few reasons.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种管道中收集 YouTube 数据有几个好处。
- en: '**1\. Harnessing cloud technologies:** AWS provides us with the means to securely
    store our data and respond to any sudden changes in demand, all at an inexpensive
    rate.'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: '**1\. 利用云技术：** AWS 为我们提供了安全存储数据的手段，并能够以低廉的费用应对任何突发需求。'
- en: '**2\. Automation:** With the deployed Airflow pipeline, YouTube data will be
    pulled, transformed, stored, and queried without any manual effort.'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: '**2\. 自动化：** 使用部署的 Airflow 管道，YouTube 数据将被提取、转换、存储和查询，而无需人工干预。'
- en: '**3.** **Logging:** Airflow’s webserver allows us to access the log and examine
    all of the DAG runs as well as the current status of all tasks.'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: '**3.** **日志记录：** Airflow 的 Web 服务器使我们能够访问日志，并检查所有 DAG 运行及所有任务的当前状态。'
- en: '**4.** **Modifiable:** Elements can be added to or removed from the AWS environment
    without hampering the operations in the current pipeline. Users can easily add
    new data sources, write new queries, or launch other AWS resources if needed.'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: '**4.** **可修改：** 可以在 AWS 环境中添加或移除元素，而不会影响当前管道的操作。如果需要，用户可以轻松添加新的数据源、编写新的查询或启动其他
    AWS 资源。'
- en: Disadvantages of This Data Pipeline
  id: totrans-168
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 该数据管道的缺点
- en: That being said, the constructed pipeline does have limitations.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管如此，构建的管道确实存在一些局限性。
- en: '**1\. Lack of notifications:** Currently, the data pipeline doesn’t incorporate
    a notification service that alerts users if a task in the Airflow DAG doesn’t
    run as expected. This can delay response time.'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: '**1\. 通知缺乏：** 目前，数据管道没有集成通知服务来警告用户如果 Airflow DAG 中的任务没有按预期运行。这可能会延迟响应时间。'
- en: '**2\. Lack of a data warehouse:** The pipeline currently generates reports
    by directly querying the S3 bucket, which serves as the data lake. This is feasible
    with the current circumstances, but if other data sources were to be added, the
    pipeline would lack the tools needed to efficiently perform complex joins or aggregation
    needed for subsequent analyses.'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: '**2\. 缺乏数据仓库：** 该管道目前通过直接查询 S3 存储桶生成报告，该存储桶充当数据湖。这在当前情况下是可行的，但如果要添加其他数据源，管道将缺乏高效执行复杂连接或聚合所需的工具，以便进行后续分析。'
- en: Conclusion
  id: totrans-172
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结论
- en: '![](../Images/28ea94bb56e5a9eb7be2b9b6b65fd6f0.png)'
  id: totrans-173
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/28ea94bb56e5a9eb7be2b9b6b65fd6f0.png)'
- en: Photo by [Prateek Katyal](https://unsplash.com/it/@prateekkatyal?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 照片由 [Prateek Katyal](https://unsplash.com/it/@prateekkatyal?utm_source=medium&utm_medium=referral)
    提供，来自 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: Ultimately, tracking the most popular YouTube content in different regions over
    time is a massive endeavor, which requires a continuous collection of data that
    is suited for analysis. Although the operations needed to obtain this data can
    be executed manually with on-premise infrastructure, such an approach becomes
    more inefficient and infeasible as the volume of data increases and as the workflow
    gets more complex.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: '*最终*，追踪不同地区随时间变化的最受欢迎的 YouTube 内容是一项巨大的工作，这需要持续收集适合分析的数据。虽然获取这些数据的操作可以在本地基础设施上手动执行，但随着数据量的增加和工作流的复杂化，这种方法变得更加低效和不可行。'
- en: Thus, for this use case, it’s worth leveraging a cloud platform and a job scheduler
    to ensure that the ingestion and processing of data can be automated.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，对于这种用例，利用云平台和作业调度器是值得的，以确保数据的摄取和处理可以自动化。
- en: With AWS, we can create a scalable and cost-effective solution that collects,
    transforms, stores, and processes YouTube data. With Apache Airflow, we have the
    means to monitor complex workflows and debug them when necessary.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 AWS，我们可以创建一个可扩展且具有成本效益的解决方案，来收集、转换、存储和处理 YouTube 数据。利用 Apache Airflow，我们能够监控复杂的工作流，并在必要时进行调试。
- en: 'For the source code used in this project, please visit the GitHub repository:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 要查看本项目使用的源代码，请访问 GitHub 仓库：
- en: '[](https://github.com/anair123/Building-a-Youtube-Data-Pipeline-With-AWS-and-Airflow?source=post_page-----e5d3b11de9c2--------------------------------)
    [## GitHub - anair123/Building-a-Youtube-Data-Pipeline-With-AWS-and-Airflow'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://github.com/anair123/Building-a-Youtube-Data-Pipeline-With-AWS-and-Airflow?source=post_page-----e5d3b11de9c2--------------------------------)
    [## GitHub - anair123/Building-a-Youtube-Data-Pipeline-With-AWS-and-Airflow]'
- en: You can't perform that action at this time. You signed in with another tab or
    window. You signed out in another tab or…
  id: totrans-180
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 目前，您无法执行该操作。您在另一个标签页或窗口中登录了。您在另一个标签页或窗口中注销了……
- en: github.com](https://github.com/anair123/Building-a-Youtube-Data-Pipeline-With-AWS-and-Airflow?source=post_page-----e5d3b11de9c2--------------------------------)
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: github.com](https://github.com/anair123/Building-a-Youtube-Data-Pipeline-With-AWS-and-Airflow?source=post_page-----e5d3b11de9c2--------------------------------)
- en: Thank you for reading!
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 感谢阅读！
