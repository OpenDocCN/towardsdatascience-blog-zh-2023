- en: Deep Learning Training on AWS Inferentia
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在 AWS Inferentia 上进行深度学习训练
- en: 原文：[https://towardsdatascience.com/dl-training-on-aws-inferentia-53e103597a03?source=collection_archive---------7-----------------------#2023-08-30](https://towardsdatascience.com/dl-training-on-aws-inferentia-53e103597a03?source=collection_archive---------7-----------------------#2023-08-30)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/dl-training-on-aws-inferentia-53e103597a03?source=collection_archive---------7-----------------------#2023-08-30](https://towardsdatascience.com/dl-training-on-aws-inferentia-53e103597a03?source=collection_archive---------7-----------------------#2023-08-30)
- en: Yet another money-saving AI-model training hack
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 又一个节省开支的 AI 模型训练技巧
- en: '[](https://chaimrand.medium.com/?source=post_page-----53e103597a03--------------------------------)[![Chaim
    Rand](../Images/c52659c389f167ad5d6dc139940e7955.png)](https://chaimrand.medium.com/?source=post_page-----53e103597a03--------------------------------)[](https://towardsdatascience.com/?source=post_page-----53e103597a03--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----53e103597a03--------------------------------)
    [Chaim Rand](https://chaimrand.medium.com/?source=post_page-----53e103597a03--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://chaimrand.medium.com/?source=post_page-----53e103597a03--------------------------------)[![Chaim
    Rand](../Images/c52659c389f167ad5d6dc139940e7955.png)](https://chaimrand.medium.com/?source=post_page-----53e103597a03--------------------------------)[](https://towardsdatascience.com/?source=post_page-----53e103597a03--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----53e103597a03--------------------------------)
    [Chaim Rand](https://chaimrand.medium.com/?source=post_page-----53e103597a03--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F9440b37e27fe&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdl-training-on-aws-inferentia-53e103597a03&user=Chaim+Rand&userId=9440b37e27fe&source=post_page-9440b37e27fe----53e103597a03---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----53e103597a03--------------------------------)
    ·5 min read·Aug 30, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F53e103597a03&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdl-training-on-aws-inferentia-53e103597a03&user=Chaim+Rand&userId=9440b37e27fe&source=-----53e103597a03---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F9440b37e27fe&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdl-training-on-aws-inferentia-53e103597a03&user=Chaim+Rand&userId=9440b37e27fe&source=post_page-9440b37e27fe----53e103597a03---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----53e103597a03--------------------------------)
    · 5分钟阅读 · 2023年8月30日'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F53e103597a03&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdl-training-on-aws-inferentia-53e103597a03&source=-----53e103597a03---------------------bookmark_footer-----------)![](../Images/7a5f4d5ad988b426fd86cf773160ec04.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F53e103597a03&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdl-training-on-aws-inferentia-53e103597a03&source=-----53e103597a03---------------------bookmark_footer-----------)![](../Images/7a5f4d5ad988b426fd86cf773160ec04.png)'
- en: Photo by [Lisheng Chang](https://unsplash.com/@changlisheng?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：[李生昌](https://unsplash.com/@changlisheng?utm_source=medium&utm_medium=referral)
    通过 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: 'The topic of this post is AWS’s home-grown AI chip, [AWS Inferentia](https://aws.amazon.com/machine-learning/inferentia/)
    — more specifically, the second-generation AWS Inferentia2\. This is a sequel
    to [our post](/a-first-look-at-aws-trainium-1e0605071970) from last year on [AWS
    Trainium](https://aws.amazon.com/machine-learning/trainium/) and joins a series
    of posts on the topic of dedicated AI accelerators. Contrary to the chips we have
    explored in our previous posts in the series, AWS Inferentia was designed for
    AI model **inference** and is targeted *specifically* for deep-learning inference
    applications. However, the fact that AWS Inferentia2 and AWS Trainium both share
    the same underlying [NeuronCore-v2](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/arch/neuron-hardware/neuron-core-v2.html#neuroncores-v2-arch)
    architecture and the same software stack (the [AWS Neuron SDK](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/index.html)),
    begs the question: **Can AWS Inferentia be used for AI training workloads, as
    well?**'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 本文的主题是 AWS 自家研发的 AI 芯片，[AWS Inferentia](https://aws.amazon.com/machine-learning/inferentia/)——更具体地说，是第二代
    AWS Inferentia2。这是对我们去年关于 [AWS Trainium](https://aws.amazon.com/machine-learning/trainium/)
    的 [文章](/a-first-look-at-aws-trainium-1e0605071970) 的续集，并且是关于专用 AI 加速器系列文章的一部分。与我们之前在系列文章中探讨的芯片不同，AWS
    Inferentia 是为 AI 模型的 **推理** 设计的，*专门* 针对深度学习推理应用。然而，AWS Inferentia2 和 AWS Trainium
    共享相同的底层 [NeuronCore-v2](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/arch/neuron-hardware/neuron-core-v2.html#neuroncores-v2-arch)
    架构以及相同的软件栈（[AWS Neuron SDK](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/index.html)），这就引发了一个问题：**AWS
    Inferentia 是否也可以用于 AI 训练工作负载？**
- en: Granted, there are some elements of the [Amazon EC2 Inf2 instance](https://aws.amazon.com/ec2/instance-types/inf2/)
    family specifications (which are powered by AWS Inferentia accelerators) that
    might make them less appropriate for some training workloads when compared to
    the [Amazon EC2 Trn1 instance](https://aws.amazon.com/ec2/instance-types/trn1/)
    family. For example, although both Inf2 and support high-bandwidth and low-latency
    NeuronLink-v2 device-to-device interconnect, the Trainium devices are connected
    in a [2D Torus topology](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/arch/neuron-features/collective-communication.html#trn1-32xlarge-topology)
    rather than a [ring topology](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/arch/neuron-features/collective-communication.html#inf2-48xlarge-topology)
    which can potentially impact the performance of Collective Communication operators
    (see [here](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/arch/neuron-features/collective-communication.html)
    for more details). However, some training workloads may not require the unique
    features of the Trn1 architecture and may perform equally well on the Inf1 and
    Inf2 architectures.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，与 [Amazon EC2 Trn1 实例](https://aws.amazon.com/ec2/instance-types/trn1/) 系列相比，[Amazon
    EC2 Inf2 实例](https://aws.amazon.com/ec2/instance-types/inf2/) 系列的某些规格（由 AWS Inferentia
    加速器提供支持）可能会使其在一些训练工作负载上不太适用。例如，尽管 Inf2 和 Trainium 都支持高带宽和低延迟的 NeuronLink-v2 设备间互连，但
    Trainium 设备是以 [2D 环形拓扑](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/arch/neuron-features/collective-communication.html#trn1-32xlarge-topology)
    连接的，而不是 [环形拓扑](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/arch/neuron-features/collective-communication.html#inf2-48xlarge-topology)，这可能会影响
    Collective Communication 操作符的性能（更多细节请参见 [这里](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/arch/neuron-features/collective-communication.html)）。然而，一些训练工作负载可能不需要
    Trn1 架构的独特功能，并且在 Inf1 和 Inf2 架构上表现同样出色。
- en: In fact, the ability to train on both Trainium *and* Inferentia accelerators
    would greatly increase the variety of training instances at our disposal and our
    ability to tune the choice of training instance to the specific needs of each
    DL project. In our recent post, [Instance Selection for Deep Learning](/instance-selection-for-deep-learning-7463d774cff0),
    we elaborated on the value of having a wide variety of diverse instance types
    for DL training. While the Trn1 family includes just two instance types, enabling
    training on Inf2 would add four additional instance types. Including Inf1 in the
    mix would add four more.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，能够在 Trainium *和* Inferentia 加速器上进行训练，将极大增加我们可用的训练实例种类，并提升我们根据每个深度学习项目的具体需求来调整训练实例选择的能力。在我们最近的文章
    [深度学习的实例选择](/instance-selection-for-deep-learning-7463d774cff0) 中，我们详细阐述了拥有多样化实例类型对于深度学习训练的价值。虽然
    Trn1 系列仅包含两个实例类型，但启用 Inf2 训练将增加四种额外的实例类型。若将 Inf1 纳入其中，则会增加四种更多的实例类型。
- en: Our intention in this post is to demonstrate the opportunity of training on
    AWS Inferentia. We will define a toy vision model and compare the performance
    of training it on the Amazon EC2 Trn1 and Amazon EC2 Inf2 instance families. Many
    thanks to [Ohad Klein](https://www.linkedin.com/in/ohad-klein-947aaa187/?originalSubdomain=il)
    and [Yitzhak Levi](https://www.linkedin.com/in/yitzhak-levi-49a217201/) for their
    contributions to this post.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这篇文章中的目的是展示在 AWS Inferentia 上训练的机会。我们将定义一个玩具视觉模型，并比较在 Amazon EC2 Trn1 和 Amazon
    EC2 Inf2 实例系列上的训练性能。特别感谢[Ohad Klein](https://www.linkedin.com/in/ohad-klein-947aaa187/?originalSubdomain=il)和[Yitzhak
    Levi](https://www.linkedin.com/in/yitzhak-levi-49a217201/)对本帖的贡献。
- en: Disclaimers
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 免责声明
- en: Note that, as of the time of this writing, there are some DL model architectures
    that remain unsupported by the Neuron SDK. For example, while model inference
    of CNN models is supported, training CNN models is [still unsupported](https://github.com/orgs/aws-neuron/projects/1/views/1?filterQuery=cnn&pane=issue&itemId=12024107).
    The SDK documentation includes a [model support matrix](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/arch/model-architecture-fit.html#aws-trainium-neuroncore-v2)
    detailing the supported features per model architecture, training framework (e.g.,
    TensorFlow and PyTorch), and Neuron architecture version.
  id: totrans-14
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 请注意，截止到本文撰写时，有些深度学习模型架构尚不受 Neuron SDK 支持。例如，虽然 CNN 模型的推理是支持的，但训练 CNN 模型[仍不支持](https://github.com/orgs/aws-neuron/projects/1/views/1?filterQuery=cnn&pane=issue&itemId=12024107)。SDK
    文档包括一个[模型支持矩阵](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/arch/model-architecture-fit.html#aws-trainium-neuroncore-v2)，详细列出了每种模型架构、训练框架（例如
    TensorFlow 和 PyTorch）和 Neuron 架构版本的支持功能。
- en: The experiments that we will describe were run on Amazon EC2 with the most recent
    version of the [Deep Learning AMI for Neuron](https://aws.amazon.com/releasenotes/aws-deep-learning-ami-neuron-pytorch-1-13-ubuntu-20-04/)
    available at the time of this writing, “Deep Learning AMI Neuron PyTorch 1.13
    (Ubuntu 20.04) 20230720”, which includes version 2.8 of the Neuron SDK. Being
    that the Neuron SDK remains under active development, it is likely that the comparative
    results that we achieved will change over time. It is highly recommended that
    you reassess the findings of this post with the most up-to-date versions of the
    underlying libraries.
  id: totrans-15
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将描述的实验是在 Amazon EC2 上运行的，当时最新版本的[Deep Learning AMI for Neuron](https://aws.amazon.com/releasenotes/aws-deep-learning-ami-neuron-pytorch-1-13-ubuntu-20-04/)为“Deep
    Learning AMI Neuron PyTorch 1.13（Ubuntu 20.04）20230720”，该版本包括 Neuron SDK 的 2.8
    版本。鉴于 Neuron SDK 仍在积极开发中，可能会随着时间的推移，我们获得的比较结果会有所变化。强烈建议您使用最新版本的底层库重新评估本文的发现。
- en: Our intention in this post is to demonstrate the potential of training on AWS
    Inferentia powered instances. Please do **not** view this post as an endorsement
    for the use of these instances or any of the other products we might mention.
    There are many variables that factor into how to choose a training environment
    which may vary greatly based on the particulars of your project. In particular,
    different models might exhibit wholly different relative price-performance results
    when running on two different instance types.
  id: totrans-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们在这篇文章中的目的是展示 AWS Inferentia 计算实例的潜力。请**不要**将本文视为对这些实例或我们可能提及的其他产品的推荐。选择训练环境时有很多变量，这些变量可能会根据项目的具体情况而大相径庭。特别是，不同模型在两种不同实例类型上的相对价格性能结果可能完全不同。
- en: Toy Model
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 玩具模型
- en: Similar to the experiments we described in [our previous post](/a-first-look-at-aws-trainium-1e0605071970),
    we define a simple [Vision Transformer](https://en.wikipedia.org/wiki/Vision_transformer)
    (ViT)-backed classification model (using the [timm](https://pypi.org/project/timm/)
    Python package version 0.9.5) along with a randomly generated dataset.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于我们在[之前的文章](/a-first-look-at-aws-trainium-1e0605071970)中描述的实验，我们定义了一个简单的[视觉变换器](https://en.wikipedia.org/wiki/Vision_transformer)（ViT）支持的分类模型（使用[timm](https://pypi.org/project/timm/)
    Python 包版本 0.9.5），以及一个随机生成的数据集。
- en: '[PRE0]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Results
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结果
- en: In the table below we compare the speed and price performance of various Amazon
    EC2 Trn1 and Amazon EC2 Inf2 instance types.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 下表中我们比较了各种 Amazon EC2 Trn1 和 Amazon EC2 Inf2 实例类型的速度和价格性能。
- en: '![](../Images/b9834da39bbe4c3cee701baced5dff67.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b9834da39bbe4c3cee701baced5dff67.png)'
- en: Performance comparison of ViT-based classification model (By Author)
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: ViT基础的分类模型性能比较（作者）
- en: While it is clear that the Trainium-powered instance types support better absolute
    performance (i.e., increased training speeds), training on the Inferentia-powered
    instances resulted in ~39% better **price** performance (for the two-core instance
    types) and higher (for the larger instance types).
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然明显看到Trainium驱动的实例类型支持更好的绝对性能（即，提高的训练速度），但在Inferentia驱动的实例上训练的**价格**性能提高了约39%（对于双核实例类型）并且对于更大的实例类型则更高。
- en: Once again, we caution against making any design decisions based solely on these
    results. Some model architectures might run successfully on Trn1 instances but
    break down on Inf2\. Others might succeed on both but exhibit very different comparative
    performance results than the ones shown here.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 再次提醒，不要仅根据这些结果做出设计决策。一些模型架构可能在Trn1实例上成功运行，但在Inf2上会出现故障。其他模型可能在两者上都能成功，但表现出的比较性能结果可能与这里展示的结果非常不同。
- en: Note that we have omitted the time required for compiling the DL model. Although
    this is only required the first time the model is run, compilation times can be
    quite high (e.g., upward of ten minutes for our toy model). Two ways to reduce
    the overhead of model compilation are [parallel compilation](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/frameworks/torch/torch-neuronx/api-reference-guide/training/pytorch-neuron-parallel-compile.html#pytorch-neuronx-parallel-compile-cli)
    and [offline compilation](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/compiler/neuronx-cc/faq.html#where-can-i-compile-to-neuron).
    Importantly, make sure that your script does not include operations (or graph
    changes) that trigger frequent recompilations. See the [Neuron SDK documentation](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/compiler/index.html)
    for more details.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们已省略了编译DL模型所需的时间。虽然这仅在第一次运行模型时需要，但编译时间可能非常长（例如，我们的玩具模型可能超过十分钟）。减少模型编译开销的两种方法是[并行编译](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/frameworks/torch/torch-neuronx/api-reference-guide/training/pytorch-neuron-parallel-compile.html#pytorch-neuronx-parallel-compile-cli)和[离线编译](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/compiler/neuronx-cc/faq.html#where-can-i-compile-to-neuron)。重要的是，确保你的脚本不包含会触发频繁重新编译的操作（或图形更改）。有关更多细节，请参阅[Neuron
    SDK文档](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/compiler/index.html)。
- en: Summary
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: Although marketed as an AI **inference** chip, it appears that AWS Inferentia
    offers yet another option for **training** deep learning models. In our [previous
    post](/a-first-look-at-aws-trainium-1e0605071970) on AWS Trainium we highlighted
    some of the challenges that you might encounter when adapting your models to train
    on a new AI ASIC. The possibility of training the same models on AWS Inferentia-powered
    instance types, as well, could increase the potential reward of your efforts.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然被市场宣传为AI **推理**芯片，但看来AWS Inferentia还提供了另一个选项来进行**训练**深度学习模型。在我们关于AWS Trainium的[上一篇文章](/a-first-look-at-aws-trainium-1e0605071970)中，我们强调了在将模型适配到新的AI
    ASIC时可能遇到的一些挑战。在AWS Inferentia支持的实例类型上训练相同模型的可能性，也许会提高你努力的潜在回报。
