- en: From the Perceptron to Adaline
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从感知机到Adaline
- en: 原文：[https://towardsdatascience.com/from-the-perceptron-to-adaline-1730e33d41c5?source=collection_archive---------6-----------------------#2023-11-28](https://towardsdatascience.com/from-the-perceptron-to-adaline-1730e33d41c5?source=collection_archive---------6-----------------------#2023-11-28)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/from-the-perceptron-to-adaline-1730e33d41c5?source=collection_archive---------6-----------------------#2023-11-28](https://towardsdatascience.com/from-the-perceptron-to-adaline-1730e33d41c5?source=collection_archive---------6-----------------------#2023-11-28)
- en: Setting the foundations right
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 奠定基础
- en: '[](https://medium.com/@cretanpan?source=post_page-----1730e33d41c5--------------------------------)[![Pan
    Cretan](../Images/8b3fbab70c0e61f7ca516d2f54b646e5.png)](https://medium.com/@cretanpan?source=post_page-----1730e33d41c5--------------------------------)[](https://towardsdatascience.com/?source=post_page-----1730e33d41c5--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----1730e33d41c5--------------------------------)
    [Pan Cretan](https://medium.com/@cretanpan?source=post_page-----1730e33d41c5--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@cretanpan?source=post_page-----1730e33d41c5--------------------------------)[![Pan
    Cretan](../Images/8b3fbab70c0e61f7ca516d2f54b646e5.png)](https://medium.com/@cretanpan?source=post_page-----1730e33d41c5--------------------------------)[](https://towardsdatascience.com/?source=post_page-----1730e33d41c5--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----1730e33d41c5--------------------------------)
    [Pan Cretan](https://medium.com/@cretanpan?source=post_page-----1730e33d41c5--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fff990ba57425&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffrom-the-perceptron-to-adaline-1730e33d41c5&user=Pan+Cretan&userId=ff990ba57425&source=post_page-ff990ba57425----1730e33d41c5---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----1730e33d41c5--------------------------------)
    ·11 min read·Nov 28, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F1730e33d41c5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffrom-the-perceptron-to-adaline-1730e33d41c5&user=Pan+Cretan&userId=ff990ba57425&source=-----1730e33d41c5---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fff990ba57425&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffrom-the-perceptron-to-adaline-1730e33d41c5&user=Pan+Cretan&userId=ff990ba57425&source=post_page-ff990ba57425----1730e33d41c5---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----1730e33d41c5--------------------------------)
    ·11 min read·Nov 28, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F1730e33d41c5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffrom-the-perceptron-to-adaline-1730e33d41c5&user=Pan+Cretan&userId=ff990ba57425&source=-----1730e33d41c5---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F1730e33d41c5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffrom-the-perceptron-to-adaline-1730e33d41c5&source=-----1730e33d41c5---------------------bookmark_footer-----------)![](../Images/8ac80c2c95ab7e4f13888daeeb72fccc.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F1730e33d41c5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffrom-the-perceptron-to-adaline-1730e33d41c5&source=-----1730e33d41c5---------------------bookmark_footer-----------)![](../Images/8ac80c2c95ab7e4f13888daeeb72fccc.png)'
- en: Photo by [Einar Storsul](https://unsplash.com/@einarstorsul?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 照片由 [Einar Storsul](https://unsplash.com/@einarstorsul?utm_source=medium&utm_medium=referral)
    提供，来源于 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: Introduction
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 介绍
- en: In a previous article I tried to explain the most basic binary classifier that
    has likely ever existed, [Rosenblatt’s perceptron](https://medium.com/towards-data-science/classification-with-rosenblatts-perceptron-e7f49e3af562).
    Understanding this algorithm has educational value and can serve as a good introduction
    in elementary machine learning courses. It is an algorithm that can be coded from
    scratch in a single afternoon and can spark interest, a sense of achievement and
    motivation to delve into more complex topics. Still, as an algorithm it leaves
    much to be desired because convergence is only guaranteed when the classes are
    linearly separable that is often not the case.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一篇文章中，我尝试解释了可能存在的最基本的二元分类器，[Rosenblatt的感知机](https://medium.com/towards-data-science/classification-with-rosenblatts-perceptron-e7f49e3af562)。理解这个算法具有教育价值，可以作为初级机器学习课程的良好介绍。这是一个可以在一个下午从零开始编写的算法，并且能够激发兴趣、成就感和深入研究更复杂主题的动力。然而，作为一个算法，它仍然有很多不足，因为只有在类是线性可分的情况下才能保证收敛，这种情况往往不会发生。
- en: In this article we will continue the journey on mastering classification concepts.
    A natural evolution from the Rosenblatt’s perceptron is the **ada**ptive **li**near
    **ne**uron classifier, or adaline as it is colloquially known. Moving from the
    perceptron to adaline is not a big leap. We simply need to change the step activation
    function to a linear one. This small change leads to a continuous loss function
    that can be robustly minimised. This allows us to introduce many useful concepts
    in machine learning, such as vectorisation and optimisation methods.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们将继续掌握分类概念的旅程。从Rosenblatt的感知机自然演变而来的就是**自适应** **线性** **神经**分类器，俗称adaline。从感知机到adaline的过渡并不是一个大的飞跃。我们只需将阶跃激活函数更改为线性激活函数。这一小的变化导致了一个连续的损失函数，可以被稳健地最小化。这使我们能够引入机器学习中的许多有用概念，如向量化和优化方法。
- en: In future articles we will also cover further subtle changes of the activation
    and loss functions that will take us from adaline to logistic regression, that
    is already a useful algorithm in daily practice. All of the above algorithms are
    essentially single layer neural networks and can be readily extended to multilayer
    ones. In this sense, this article takes the reader a step further through this
    evolution and builds the foundations to tackle more advanced concepts.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在未来的文章中，我们还将讨论激活函数和损失函数的进一步细微变化，这将使我们从adaline过渡到逻辑回归，这已经是日常实践中的有用算法。上述所有算法本质上都是单层神经网络，并且可以很容易地扩展到多层神经网络。在这方面，本文将读者带到了这次进化的下一步，并为处理更高级的概念奠定基础。
- en: We will need some formulas. I used the online [LaTeX equation editor](https://latexeditor.lagrida.com/)
    to develop the LaTeX code for the equation and then the chrome plugin [Maths Equations
    Anywhere](https://chromewebstore.google.com/detail/math-equations-anywhere/fkioioejambaepmmpepneigdadjpfamh)
    to render the equation into an image. The only downside of this approach is that
    the LaTeX code is not stored in case you need to render it again. For this purpose
    I provide the list of equations at the end of this article. If you are not familiar
    with LaTex this may have its own educational value. Getting the notation right
    is part of the journey in machine learning.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要一些公式。我使用了在线的[LaTeX方程编辑器](https://latexeditor.lagrida.com/)来开发方程的LaTeX代码，然后使用Chrome插件[数学方程随处可用](https://chromewebstore.google.com/detail/math-equations-anywhere/fkioioejambaepmmpepneigdadjpfamh)将方程渲染成图片。这种方法的唯一缺点是LaTeX代码不会被存储，以备将来需要再次渲染时使用。为此，我在本文末尾提供了方程列表。如果你对LaTex不熟悉，这可能有其自身的教育价值。正确的符号表示是机器学习旅程的一部分。
- en: Adaptive linear neuron classifier (adaline)
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 自适应线性神经分类器（adaline）
- en: 'So what is the adaline algorithm? Adaline is a binary classifier as the perceptron.
    A prediction is made by using a set of input values for the features [x₁, .. ,
    xₘ] where m is the number of features. The input values are multiplied with the
    weights [w₁, .. , wₘ] and the bias is added to obtain the net input z = w₁x₁ +
    .. + wₘxₘ + b. The net input is passed to the linear activation function σ(z)
    that is then used to make a prediction using a step function as with the perceptron:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 那么adaline算法是什么呢？Adaline是一个像感知机一样的二元分类器。通过使用一组特征[x₁, .. , xₘ]的输入值来进行预测，其中m是特征的数量。输入值与权重[w₁,
    .. , wₘ]相乘，并加上偏置，以得到净输入z = w₁x₁ + .. + wₘxₘ + b。净输入通过线性激活函数σ(z)，然后使用阶跃函数进行预测，与感知机类似：
- en: '![](../Images/4de8a2c1883431481a490f75dae9cbc9.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4de8a2c1883431481a490f75dae9cbc9.png)'
- en: A key difference with the perceptron is that the linear activation function
    is used for learning the weights, whilst the step function is only used for making
    the prediction at the end. This sounds like a small thing, but it is of significant
    importance. The linear activation function is differentiable whilst the step function
    is not! The threshold 0.5 above is not written in stone. By adjusting the threshold
    we can regulate the precision and recall according to our use case, i.e. based
    on what is the cost of false positives and false negatives.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 与感知器的一个关键区别在于，线性激活函数用于学习权重，而阶跃函数仅用于最终的预测。这听起来是件小事，但它具有重要意义。线性激活函数是可微分的，而阶跃函数则不是！上面的阈值0.5并非一成不变。通过调整阈值，我们可以根据我们的使用案例调整精确度和召回率，即根据假阳性和假阴性的成本。
- en: In the case of adaline the linear activation function is simply the identity,
    i.e. σ(z) = z. The objective function (also known as loss function) that needs
    to be minimised in the training process is
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在 adaline 的情况下，线性激活函数就是恒等函数，即 σ(z) = z。需要在训练过程中最小化的目标函数（也称为损失函数）是
- en: '![](../Images/951717d16569b60b106264072ed64894.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/951717d16569b60b106264072ed64894.png)'
- en: where **w** are the weights
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 **w** 是权重。
- en: '![](../Images/950c38d1374475f49563667d652a7fab.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/950c38d1374475f49563667d652a7fab.png)'
- en: and b is the bias. The summation is over all of the examples in the training
    set. In some implementations the loss function also includes a 1/2 coefficient
    for convenience. This cancels out once we take the gradients of the loss function
    with respect to the weights and bias and, as we will see below, has no effect
    other than scaling the learning rate by a factor of 2\. In this article we do
    not use the 1/2 coefficient.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 并且 b 是偏置。求和是在训练集中的所有示例上进行的。在一些实现中，损失函数还包括 1/2 系数以便于计算。这个系数在我们对损失函数关于权重和偏置的梯度进行求导时会被抵消，并且正如我们下面将看到的，它除了将学习率缩放两倍外没有其他影响。在本文中，我们不使用
    1/2 系数。
- en: For each example, we compute the square difference between the calculated outcome
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个示例，我们计算计算结果与真实类标签之间的平方差。
- en: '![](../Images/0428f21b171a5009335f18e58547e0d9.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0428f21b171a5009335f18e58547e0d9.png)'
- en: and the true class label. Note that the input vector is understood to be a matrix
    with shape (1, m), i.e. as we will see later is one row of our feature matrix
    **x** with shape (n, m).
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 以及真实类标签。注意，输入向量被理解为形状为 (1, m) 的矩阵，即正如我们稍后将看到的，是我们的特征矩阵**x**的一行，其形状为 (n, m)。
- en: The training is nothing else than an optimisation problem. We need to adjust
    the weights and bias so that the loss function is minimised. As with any minimisation
    problem we need to compute the gradients of the objective function with respect
    to the independent variables that in our case will be the weights and the bias.
    The partial derivative of the loss function with regard to the weight wⱼ is
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 训练不过是一个优化问题。我们需要调整权重和偏置，使损失函数最小化。与任何最小化问题一样，我们需要计算目标函数对独立变量的梯度，在我们的情况下，这些变量将是权重和偏置。损失函数关于权重
    wⱼ 的偏导数是
- en: '![](../Images/36280ce22e0f3219e9f107deaafaac5d.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/36280ce22e0f3219e9f107deaafaac5d.png)'
- en: The last row introduces important matrix notation. The feature matrix **x**
    has shape (n, m) and we take the transpose of its column j, i.e. a matrix with
    shape (1, n). The true class labels **y** is a matrix with shape (n, 1). The net
    output of all samples **z** is also a matrix with shape (n, 1), that does not
    change after the activation that is understood to apply to each of its elements.
    The final result of the above formula is a scalar. Can you guess how we could
    express the gradients with respect to all weights using the matrix notation?
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一行引入了重要的矩阵符号。特征矩阵 **x** 的形状为 (n, m)，我们对其第 j 列取转置，即形状为 (1, n) 的矩阵。真实类标签 **y**
    是一个形状为 (n, 1) 的矩阵。所有样本的净输出 **z** 也是一个形状为 (n, 1) 的矩阵，激活应用于其每个元素后不会改变。上述公式的最终结果是一个标量。你能猜出我们如何使用矩阵符号表示所有权重的梯度吗？
- en: '![](../Images/b8f325ce6ba59bbc67f22bce6525a850.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b8f325ce6ba59bbc67f22bce6525a850.png)'
- en: where the transpose of the feature matrix has shape (m, n). The end result of
    this operation is a matrix with shape (m, 1). This notation is important. Instead
    of using loops, we will be using exactly this matrix multiplication using [numpy](https://numpy.org/).
    In the era of neural networks and GPUs, the ability to apply vectorization is
    essential!
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 特征矩阵的转置形状为 (m, n)。此操作的最终结果是一个形状为 (m, 1) 的矩阵。这种表示法非常重要。我们将使用 [numpy](https://numpy.org/)
    进行这种矩阵乘法，而不是使用循环。在神经网络和 GPU 时代，应用向量化的能力至关重要！
- en: What about the gradient of the loss function with respect to the bias?
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 那么损失函数关于偏差的梯度如何呢？
- en: '![](../Images/b9670bba511407e32efb41e492f9c4e8.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b9670bba511407e32efb41e492f9c4e8.png)'
- en: where the overbar denotes the mean of the vector under it. Once more, computing
    the mean with numpy is a vectorised operation, i.e. summation does not need to
    be implemented using a loop.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 其中横线表示向量的均值。再次强调，使用 numpy 计算均值是一个向量化操作，即求和不需要使用循环实现。
- en: Once we have the gradients we can employ the gradient descent optimisation method
    to minimise the loss. The weights and bias terms are iteratively updated using
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们得到梯度，就可以使用梯度下降优化方法来最小化损失。权重和偏差项会使用以下公式迭代更新
- en: '![](../Images/769ddba3bcab91dcc4cc72ee5b19191d.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/769ddba3bcab91dcc4cc72ee5b19191d.png)'
- en: where η is a suitable chosen learning rate. Too small values can delay convergence,
    whilst too high values can prevent convergence altogether. Some experimentation
    is needed, as is generally the case with the parameters of machine learning algorithms.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 η 是合适选择的学习率。值过小可能会延迟收敛，而值过高则可能完全阻止收敛。需要进行一些实验，这通常是机器学习算法参数调整的一部分。
- en: In the above implementation we assume that the weights and bias are updated
    based on all examples at once. This is known as full batch gradient descent and
    is one extreme. The other extreme is to update the weights and bias after each
    training example, that is known as stochastic gradient descent (SGD). In reality
    there is also some middle ground, known as mini batch gradient descent, where
    the weights and bias are updated based on a subset of the examples. Convergence
    is typically reached faster in this way, i.e. we do not need to run as many iterations
    over the whole training set, whilst vectorisation is still (at least partially)
    possible. If the training set is very large (or the model is very complex as is
    nowadays the case with the transformers in NLP) full batch gradient descent may
    simply be not an option.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述实现中，我们假设权重和偏差是基于所有样本一起更新的。这被称为全批量梯度下降，是一种极端情况。另一种极端情况是每处理一个训练样本后更新权重和偏差，这被称为随机梯度下降（SGD）。实际上，还有一些中间情况，称为小批量梯度下降，其中权重和偏差是基于样本的子集进行更新的。通常，这种方法收敛速度更快，即我们不需要对整个训练集进行过多的迭代，同时向量化仍然（至少部分地）是可能的。如果训练集非常大（或者模型非常复杂，如现在
    NLP 中的变换器），全批量梯度下降可能根本不可行。
- en: Alternative formulation and closed form solution
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 替代的公式化和闭式解
- en: Before we proceed with the implementation of adaline in Python, we will make
    a quick digression. We could absorb the bias b in the weight vector as
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们继续用 Python 实现 adaline 之前，我们将稍作岔开。我们可以将偏差 b 吸收到权重向量中，如下所示
- en: '![](../Images/6ff96cb3272df542dfd4908785bed347.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6ff96cb3272df542dfd4908785bed347.png)'
- en: in which case the net output for all samples in the training set becomes
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，训练集中所有样本的净输出变为
- en: '![](../Images/074d30b29d129705bfdffc2f75f2605e.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/074d30b29d129705bfdffc2f75f2605e.png)'
- en: meaning that the feature matrix has been prepended with a column filled with
    1, leading to a shape (n, m+1). The gradient with regard to the combined weights
    set becomes
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着特征矩阵前面加了一列全是1，从而导致形状为 (n, m+1)。关于组合权重集的梯度变为
- en: '![](../Images/514ef3e1d6948525ce50b24bcc970cb7.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/514ef3e1d6948525ce50b24bcc970cb7.png)'
- en: In principle we could derive a closed form solution given that at the minimum
    all gradients will be zero
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 原则上，我们可以推导出一个闭式解，因为在最小值时所有梯度都将为零
- en: '![](../Images/04f9d0536e724cc5365846649870a4cc.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/04f9d0536e724cc5365846649870a4cc.png)'
- en: In reality the inverse of the matrix in the above equation may not exist because
    of singularities or it cannot be computed sufficiently accurately. Hence, such
    closed form solution is not used in practice neither in machine learning nor in
    numerical methods in general. Still, it is useful to appreciate that adaline resembles
    linear regression and as such it has a closed form solution.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，上述方程中的矩阵的逆可能不存在，可能是因为奇异性，或者无法足够准确地计算。因此，这种封闭形式的解决方案在实践中不被使用，无论是在机器学习中还是在数值方法中。然而，了解adeline类似于线性回归，并且具有封闭形式的解决方案仍然很有用。
- en: Implementing adaline in Python
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在Python中实现adeline
- en: Our implementation will use mini batch gradient descent. However, the implementation
    is flexible and allows optimising the loss function using both stochastic gradient
    descent and full batch gradient descent as the two extremes. We will examine the
    convergence behaviour by varying the batch size.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的实现将使用小批量梯度下降。然而，该实现具有灵活性，可以使用随机梯度下降和全批量梯度下降这两个极端来优化损失函数。我们将通过改变批量大小来检查收敛行为。
- en: We implement adaline using a class that exposes a fit and a predict function
    in the usual [scikit-learn](https://scikit-learn.org/stable/) API style.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用一个类来实现adeline，该类在通常的[scikit-learn](https://scikit-learn.org/stable/) API样式中暴露了fit和predict函数。
- en: Upon initialisation the adaline classifier sets the batch size for the mini
    batch gradient descent. If batch size is set to None, the whole training set is
    used (full batch gradient descent), otherwise the training set is used in batches
    (mini batch gradient descent). If the batch size is one we essentially revert
    to stochastic gradient descent. The training set is shuffled before each pass
    through the training set to avoid repetitive cycles, but this only has an effect
    if mini batch gradient descent is used. The essence of the algorithm is in the
    `_update_weights_bias` function that carries out a full pass through the training
    set and returns the corresponding loss. This function applies the gradient descent
    with the analytically computed gradients using the derivations as in the previous
    section. Note the use of the numpy `matmul` and `dot` functions that avoid the
    use of explicit loops. If the batch_size is set to None then there are no loops
    whatsoever and the implementation is fully vectorised.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在初始化时，adeline分类器设置了小批量梯度下降的批量大小。如果批量大小设置为None，则使用整个训练集（全批量梯度下降）；否则，训练集将分批使用（小批量梯度下降）。如果批量大小为1，我们实际上回到了随机梯度下降。在每次训练集遍历之前，训练集会被打乱，以避免重复周期，但这仅在使用小批量梯度下降时才会产生效果。算法的核心在于`_update_weights_bias`函数，该函数完成对训练集的完整遍历，并返回相应的损失。此函数应用了梯度下降法，使用先前章节中的导数计算的梯度。注意使用numpy的`matmul`和`dot`函数，避免了显式循环。如果批量大小设置为None，则完全没有循环，且实现完全矢量化。
- en: Using adaline in practice
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实践中使用adeline
- en: We make the necessary imports and create a synthetic dataset as in the earlier
    perceptron [article](/classification-with-rosenblatts-perceptron-e7f49e3af562)
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 我们进行必要的导入，并创建一个合成数据集，如之前感知器的[文章](/classification-with-rosenblatts-perceptron-e7f49e3af562)中所示。
- en: that produces
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 这产生了
- en: '![](../Images/5dee3d70e7c16dba4c0a25bf0a52e432.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5dee3d70e7c16dba4c0a25bf0a52e432.png)'
- en: Scatterplot of the two classes in the synthetic dataset. Image by the Author.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 合成数据集中两个类别的散点图。作者提供的图像。
- en: The only difference with the earlier article is that we tweaked the gaussian
    means and covariances so that the classes are not linearly separable as we would
    expect adaline to overcome this. Moreover, the two independent variables have
    on purpose different scales to discuss the importance of feature scaling.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 与早期文章的唯一区别是我们调整了高斯均值和协方差，使得类别不是线性可分的，因为我们希望adeline能够克服这一点。此外，两个独立变量故意具有不同的尺度，以讨论特征缩放的重要性。
- en: Let’s try to fit a first model and visualise convergence. Prior to fitting we
    normalise the features so that they both have zero mean and unit standard deviation
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试拟合第一个模型并可视化收敛情况。在拟合之前，我们对特征进行归一化，使其均值为零且标准差为单位。
- en: This produces the convergence plot
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 这产生了收敛图。
- en: '![](../Images/98546345ebcf7fad86898a6205443025.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/98546345ebcf7fad86898a6205443025.png)'
- en: Adaline convergence. Image by the Author.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: Adaline收敛。作者提供的图像。
- en: Adaline slowly converges, but the loss function does not become zero. In order
    to verify the successful training we visualise the decision boundary using the
    same approach as in the earlier article
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: Adaline缓慢收敛，但损失函数不会变为零。为了验证训练的成功，我们使用与早期文章相同的方法可视化决策边界。
- en: that produces
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 产生
- en: '![](../Images/c30747fdfbb6ca87ec43952aaff19d3b.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c30747fdfbb6ca87ec43952aaff19d3b.png)'
- en: Decision boundary of the fitted adaline model. Image by the Author.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 拟合的Adaline模型的决策边界。图片由作者提供。
- en: There are some misclassified points given that the two classes in the training
    set were not linearly separable and we used a linear decision boundary. Still,
    the algorithm converged nicely. The solution is deterministic. With sufficient
    number of passes through the training set we obtain numerically equal weights
    and bias, regardless of their initial values.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 由于训练集中的两个类别不可线性分开且我们使用了线性决策边界，因此存在一些分类错误的点。尽管如此，算法还是良好地收敛了。该解决方案是确定性的。通过足够多的训练集轮次，我们可以获得数值上相等的权重和偏置，无论它们的初始值如何。
- en: '**Mini batch vs. full batch gradient descent**'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '**迷你批量与全批量梯度下降**'
- en: The above numerical experiment used full batch gradient descent that partially
    explains the slow convergence. We will use the same dataset and random state as
    before, but this time we will fit the adaline classifier using different batch
    sizes, ranging from 20 to 400 that is the number of examples in our training set.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 上述数值实验使用了全批量梯度下降，这部分解释了缓慢收敛的原因。我们将使用与之前相同的数据集和随机状态，但这次我们将使用不同的批量大小来拟合Adaline分类器，范围从20到400，即我们训练集中的样本数量。
- en: that produces
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 产生
- en: '![](../Images/f400fc967309960fb862c808c1c04073.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f400fc967309960fb862c808c1c04073.png)'
- en: Effect of batch size on convergence (using a 0.001 learning rate). Image by
    the Author.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 批量大小对收敛的影响（使用0.001学习率）。图片由作者提供。
- en: We can clearly see that the smaller the batch size the faster the convergence,
    but there are also some oscillations. These oscillation may destabilise the convergence
    with larger learning rates. If we double the learning rate to 0.002 this becomes
    evident
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以清楚地看到，批量大小越小，收敛越快，但也会有一些震荡。这些震荡可能会在较大的学习率下使收敛不稳定。如果将学习率加倍到0.002，这一点就会变得明显。
- en: '![](../Images/44a8a9c76b5e33ccf0783f8963a9265c.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/44a8a9c76b5e33ccf0783f8963a9265c.png)'
- en: Effect of batch size on convergence (using a 0.002 learning rate). Image by
    the Author.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 批量大小对收敛的影响（使用0.002学习率）。图片由作者提供。
- en: Increasing the learning rate further will eventually prevent convergence with
    the smaller batch sizes. With even larger learning rates even the full batch gradient
    descent will fail to converge as we would overshoot the global minimum.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 进一步增加学习率最终会阻止较小批量大小的收敛。即使使用更大的学习率，即使是全批量梯度下降也会因为超越全局最小值而无法收敛。
- en: Conclusions
  id: totrans-76
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结论
- en: Adaline is a significant improvement over the perceptron. The weights and bias
    are obtained via the minimisation of a continuous loss function that in addition
    is convex (and hence does not have local minima). With a sufficient small learning
    rate the algorithm converges even if the classes are not linearly separable. When
    using gradient descent in any of its variants the convergence rate is affected
    by the scaling of the features. In this article we used simple standardisation
    that shifts the mean of every feature to become zero, whilst the spread is adjusted
    to unit variance. In this way it is possible to select a learning rate that works
    well for all weights and bias, meaning that the global minimum can be obtained
    in fewer epochs.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: Adaline相比于感知机有了显著的改进。权重和偏置通过最小化一个连续的损失函数来获得，该函数是凸的（因此没有局部极小值）。使用足够小的学习率，即使类别不能线性分开，算法也会收敛。当使用梯度下降及其任何变体时，特征的缩放会影响收敛速度。在这篇文章中，我们使用了简单的标准化，将每个特征的均值调整为零，同时将方差调整为单位方差。通过这种方式，可以选择一个对所有权重和偏置都适用的学习率，这意味着可以在更少的轮次中获得全局最小值。
- en: Obtaining a good understanding on how to build a binary classifier using vectorisation
    is key before delving into more complex topics, such as support vector machines
    and multilayer neural networks. In daily practice, one would use [scikit-learn](https://scikit-learn.org/stable/)
    that offers advanced classification algorithms that allow for nonlinear decision
    boundaries, whilst supporting efficient and systematic hyper parameter tuning
    and cross validation. However, building simple binary classifiers from scratch
    offers a deep understanding, increases confidence and gives a sense of ownership.
    Although building everything from scratch is of course not realistic, deeply understanding
    the simpler algorithms provides the necessary skills and insights so that more
    advanced algorithms included in off-the-shelf libraries feel less opaque.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入研究更复杂的主题，如支持向量机和多层神经网络之前，理解如何利用向量化构建二分类器是关键。在日常实践中，可以使用 [scikit-learn](https://scikit-learn.org/stable/)，它提供了先进的分类算法，允许非线性决策边界，并支持高效和系统的超参数调整及交叉验证。然而，从零开始构建简单的二分类器能够提供深入的理解、增强自信并带来掌控感。虽然从头开始构建所有内容显然是不现实的，但深入理解简单算法能提供必要的技能和洞察力，使得从现成库中包含的更高级算法显得不那么神秘。
- en: LaTeX code of equations used in the article
  id: totrans-79
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 文章中使用的方程的 LaTeX 代码
- en: The equations used in the article can be found in the gist below, in case you
    would like to render them again.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 文章中使用的方程可以在下面的 gist 中找到，以便你可以再次渲染它们。
