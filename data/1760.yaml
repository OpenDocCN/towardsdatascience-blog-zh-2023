- en: Visualizations of Embeddings
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 嵌入的可视化
- en: 原文：[https://towardsdatascience.com/visualizations-of-embeddings-2910580df7f4?source=collection_archive---------0-----------------------#2023-05-27](https://towardsdatascience.com/visualizations-of-embeddings-2910580df7f4?source=collection_archive---------0-----------------------#2023-05-27)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/visualizations-of-embeddings-2910580df7f4?source=collection_archive---------0-----------------------#2023-05-27](https://towardsdatascience.com/visualizations-of-embeddings-2910580df7f4?source=collection_archive---------0-----------------------#2023-05-27)
- en: There is more than one way to visualize high-dimensional data. Here, we go back
    in the history of AI to explore the evolution of these visualizations.
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 可视化高维数据的方式不止一种。在这里，我们回顾了人工智能的发展历史，以探索这些可视化的演变。
- en: '[](https://medium.com/@doug.blank?source=post_page-----2910580df7f4--------------------------------)[![Douglas
    Blank, PhD](../Images/b2fa86b9fe63a8bcb4f218ef5a6791e9.png)](https://medium.com/@doug.blank?source=post_page-----2910580df7f4--------------------------------)[](https://towardsdatascience.com/?source=post_page-----2910580df7f4--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----2910580df7f4--------------------------------)
    [Douglas Blank, PhD](https://medium.com/@doug.blank?source=post_page-----2910580df7f4--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@doug.blank?source=post_page-----2910580df7f4--------------------------------)[![Douglas
    Blank, PhD](../Images/b2fa86b9fe63a8bcb4f218ef5a6791e9.png)](https://medium.com/@doug.blank?source=post_page-----2910580df7f4--------------------------------)[](https://towardsdatascience.com/?source=post_page-----2910580df7f4--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----2910580df7f4--------------------------------)
    [Douglas Blank, PhD](https://medium.com/@doug.blank?source=post_page-----2910580df7f4--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F66e2bac7e7d8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fvisualizations-of-embeddings-2910580df7f4&user=Douglas+Blank%2C+PhD&userId=66e2bac7e7d8&source=post_page-66e2bac7e7d8----2910580df7f4---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----2910580df7f4--------------------------------)
    ·7 min read·May 27, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F2910580df7f4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fvisualizations-of-embeddings-2910580df7f4&user=Douglas+Blank%2C+PhD&userId=66e2bac7e7d8&source=-----2910580df7f4---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F66e2bac7e7d8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fvisualizations-of-embeddings-2910580df7f4&user=Douglas+Blank%2C+PhD&userId=66e2bac7e7d8&source=post_page-66e2bac7e7d8----2910580df7f4---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----2910580df7f4--------------------------------)
    ·7分钟阅读·2023年5月27日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F2910580df7f4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fvisualizations-of-embeddings-2910580df7f4&user=Douglas+Blank%2C+PhD&userId=66e2bac7e7d8&source=-----2910580df7f4---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F2910580df7f4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fvisualizations-of-embeddings-2910580df7f4&source=-----2910580df7f4---------------------bookmark_footer-----------)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F2910580df7f4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fvisualizations-of-embeddings-2910580df7f4&source=-----2910580df7f4---------------------bookmark_footer-----------)'
- en: I submitted my first paper on AI in 1990 to a small, local conference — the
    “Midwest Artificial Intelligence and Cognitive Science Society.” In those days,
    the AI field was entirely defined by research into “symbols.” This approach was
    known as “Good, Old-Fashion AI” or GOFAI (pronounced “go fi” as in “wifi”). Those
    of us working in what is now known as “Deep Learning” had to really argue that
    what we were researching *should even be considered* as AI.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 我在1990年将我的第一篇关于人工智能的论文提交到一个小型地方会议——“中西部人工智能与认知科学学会”。在那些日子里，人工智能领域完全被“符号”研究所定义。这种方法被称为“古老的人工智能”或GOFAI（发音为“go
    fi”，类似“wifi”）。我们这些现在被称为“深度学习”的研究者不得不真的争论我们所研究的内容*是否应被视为*人工智能。
- en: Being excluded from AI was a double-edged sword. On the one hand, I didn’t agree
    with most of the basic tenets of what was defined as AI at the time. The basic
    assumption was that “symbols” and “symbol processing” must be the foundation of
    all AI. So, I was happy to be working in an area that wasn’t even considered to
    be AI. On the other hand, it was difficult to find people willing to listen to
    your ideas if you didn’t package it as at least related to AI.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 被排除在人工智能之外是把双刃剑。一方面，我不同意当时定义的人工智能的基本信条。基本假设是“符号”和“符号处理”必须是所有人工智能的基础。因此，我很高兴能在一个甚至不被认为是人工智能的领域工作。另一方面，如果你不将自己的想法包装成至少与人工智能相关的内容，就很难找到愿意倾听你意见的人。
- en: This little conference accepted papers on “AI” and “Cognitive Science” — which
    I saw as an invitation for ideas outside of just “symbolic processing.” So I submitted
    my first paper, and it was accepted! The paper featured a neural network approach
    to handling natural language. Many of us in this area called this type of neural
    network research “connectionism,” but now days this type of research, as mentioned,
    would be labeled “Deep Learning” (DL) — although my initial research wasn’t very
    deep… only three layers! Modern DL systems can be composed of hundreds of layers.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 这个小型会议接受了关于“人工智能”和“认知科学”的论文——我视此为对“符号处理”之外的想法的邀请。所以我提交了我的第一篇论文，并且被接受了！论文展示了一种处理自然语言的神经网络方法。我们这个领域的许多人称这种神经网络研究为“连接主义”，但现在这种研究，如前所述，会被标记为“深度学习”（DL）——尽管我的初期研究并不深……只有三层！现代DL系统可以由数百层组成。
- en: 'My paper was accepted at the conference, and I presented it in Carbondale,
    Illinois in 1990\. Later, the organizer of the conference, John Dinsmore, invited
    me to submit a version of the paper for a book that he was putting together. I
    didn’t think I could get a paper together by myself, so I asked two of my graduate
    school friends (Lisa Meeden and Jim Marshall) to join me. They did, and we ended
    up with a chapter in the book. The book was titled “[The Symbolic and Connectionist
    Paradigms: Closing the Gap](https://www.google.com/books/edition/The_Symbolic_and_Connectionist_Paradigms/ET2YAgAAQBAJ).”
    Our paper fit in nicely with the theme of the book. We titled our paper “[Exploring
    the symbolic/subsymbolic continuum: A case study of RAAM](https://repository.brynmawr.edu/compsci_pubs/32/).”
    To my delight, the book focused on this split between these two approaches to
    AI. I think the field is still wrestling with this divide to this day.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 我的论文在会议上被接受了，我于1990年在伊利诺伊州的卡本代尔进行了展示。后来，会议组织者John Dinsmore邀请我提交论文的一个版本用于他正在编辑的一本书。我觉得自己无法独立完成论文，于是邀请了两位研究生朋友（Lisa
    Meeden 和 Jim Marshall）加入我。他们同意了，我们最终在书中完成了一章。这本书的标题是“[符号与连接主义范式：弥合差距](https://www.google.com/books/edition/The_Symbolic_and_Connectionist_Paradigms/ET2YAgAAQBAJ)”。我们的论文很契合书的主题。我们将论文标题为“[探索符号/亚符号连续体：RAAM的案例研究](https://repository.brynmawr.edu/compsci_pubs/32/)”。令我高兴的是，这本书聚焦于这两种人工智能方法之间的分裂。我认为这个领域至今仍在挣扎于这种分歧。
- en: 'I’ll say more about that initial research of mine later. For now I want to
    talk about how the field was dealing with how to visualization “embeddings.” First,
    we didn’t call these vectors “embeddings” at the time. Most research used a phrase
    such as “hidden-layer representations.” That included any internal representation
    that a connectionist system had learned in order to solve a problem. As we defined
    them back then, there were three types of layers: “input” (where you plugged in
    the dataset), “output” (where you put the desired outputs, or “targets”), and
    everything else — the “hidden” layers. The hidden layers are where the activations
    of the network flow between the input and the output. The hidden-layer activations
    are often high-dimensional, and are the representations of the “concepts” learned
    by the network.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 关于我初期研究的更多内容，我会稍后再说。现在我想谈谈这个领域如何处理“嵌入”的可视化。首先，我们当时并没有称这些向量为“嵌入”。大多数研究使用了“隐层表示”这样的术语。这包括了连接主义系统为解决问题而学习的任何内部表示。按照我们当时的定义，有三种层：“输入”（在这里你插入数据集）、“输出”（在这里你放置期望的输出或“目标”）以及其他所有层——即“隐层”。隐层是网络激活在输入和输出之间流动的地方。隐层激活通常是高维的，并且是网络学习到的“概念”的表示。
- en: 'Like today, visualizing these high-dimension vectors was seen to help give
    insight into understanding how these systems work, and oftentimes fail. In our
    chapter in the book, we used three types of visualizations:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 像今天一样，视觉化这些高维向量被认为有助于深入理解这些系统的工作原理及其常见故障。在我们书中的章节中，我们使用了三种类型的可视化：
- en: So-called “Hinton Diagrams”
  id: totrans-14
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 所谓的“Hinton 图”
- en: Cluster Diagrams, or Dendograms
  id: totrans-15
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 聚类图，或树状图
- en: Projection into 2D space
  id: totrans-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 投影到二维空间
- en: 'The first method was a newly-created idea used by [Hinton and Shallice in 1991](https://www.cs.toronto.edu/~hinton/absps/lesioning.pdf).
    (That is the same Geoffrey Hinton that we know today. More on him in a future
    article). This diagram is a simple idea with limited utility. The basic idea is
    that activations, weights, or any type of numeric data, can be represented by
    boxes: white boxes (typically representing positive numbers), and black boxes
    (typically representing negative numbers). In addition, the size of the box represents
    a value’s magnitude in relation to the maximum and minimum values in the simulated
    neuron.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 第一种方法是 [Hinton 和 Shallice 在 1991 年](https://www.cs.toronto.edu/~hinton/absps/lesioning.pdf)
    使用的一个新创意。（这就是我们今天所知道的 Geoffrey Hinton。更多内容将在未来的文章中介绍）。这个图表是一个简单的想法，实用性有限。基本想法是，激活、权重或任何类型的数值数据可以用盒子表示：白色盒子（通常表示正数）和黑色盒子（通常表示负数）。此外，盒子的大小表示相对于模拟神经元中的最大值和最小值的值的大小。
- en: 'Here is the representation from our paper showing the average “embeddings”
    at the hidden layer of the network as a representation of words were presented
    to the network:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们论文中的表示，显示了网络隐藏层中的平均“嵌入”作为对网络呈现的单词的表示：
- en: '![](../Images/7a1446f630be2b9be84de250d362b552.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7a1446f630be2b9be84de250d362b552.png)'
- en: Figure 10 from our paper.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 我们论文中的图 10。
- en: The Hinton diagram does help to visualize patterns in the data. But they don’t
    really help in understanding the relationships between the representations, nor
    does it help when the number of dimensions gets much larger. Modern embeddings
    can have many thousands of dimensions.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: Hinton 图确实有助于可视化数据中的模式。但它们并不真正有助于理解表示之间的关系，也无助于在维度数目大大增加时的理解。现代嵌入可以有数千维。
- en: 'To help with those issues, we turn to the second method: cluster diagrams or
    *dendograms*. These are diagrams that show the distance (however defined) between
    any two patterns as a hierarchical tree. Here is an example from our paper using
    euclidean distance:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 为了帮助解决这些问题，我们转向第二种方法：聚类图或 *树状图*。这些图表显示了任意两个模式之间的距离（无论如何定义）作为一个层次树。以下是我们论文中使用欧几里得距离的一个示例：
- en: '![](../Images/4f2030cf6041372e95cc0bfa7c4abbc1.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4f2030cf6041372e95cc0bfa7c4abbc1.png)'
- en: Figure 9 from our paper.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 我们论文中的图 9。
- en: 'This is the same kind of information shown in the Hinton Diagram, but in a
    much more useful format. Here we can see the internal relationships between individual
    patterns, and overall patterns. Note that the vertical ordering is irrelevant:
    the horizontal position of the branch points is the meaningful aspect of the diagram.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 这与 Hinton 图中显示的信息相同，但格式更加实用。在这里，我们可以看到个别模式之间以及整体模式之间的内部关系。请注意，垂直排序无关紧要：分支点的水平位置是图表的有意义方面。
- en: 'In the above dendogram, we constructed the overall image by hand, given the
    tree cluster computed by a program. Today, there are methods for constructing
    such a tree and image automatically. However, the diagram can become hard to be
    meaningful when the number of patterns is much more than a few dozen. Here is
    an example made by matplotlib today. You can read more about the API here: [matplotlib
    dendogram](https://docs.scipy.org/doc/scipy/reference/generated/scipy.cluster.hierarchy.dendrogram.html).'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在上面的树状图中，我们手动构建了整体图像，基于程序计算出的树簇。今天，有自动构建这种树和图像的方法。然而，当模式的数量远超过几十个时，图表可能变得难以理解。这是今天由
    matplotlib 制作的一个示例。你可以在这里了解更多关于 API 的信息：[matplotlib 树状图](https://docs.scipy.org/doc/scipy/reference/generated/scipy.cluster.hierarchy.dendrogram.html)。
- en: '![](../Images/99d9ce842f37d1520cb20824eeb8295b.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/99d9ce842f37d1520cb20824eeb8295b.png)'
- en: Modern dendogram with a large number of patterns. Image made by the author.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 现代树状图，包含大量模式。图像由作者制作。
- en: 'Finally, we come to the last method, and the one that is used predominantly
    today: the Projection method. This methods uses an algorithm to find a method
    of reducing the number of dimensions of the embedding into a number that can more
    easily be understood by humans (e.g., 2 or 3 dimensions) and plotting as a scatter
    plot.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们来到了最后一种方法，也就是今天主要使用的方法：投影方法。这种方法使用算法找到一种将嵌入的维度数减少到更容易被人类理解的数量（例如，2或3维）并绘制为散点图的方法。
- en: At the time in 1990, the main method for projecting high-dimensional data into
    a smaller set of dimensions was [Principal Component Analysis](https://en.wikipedia.org/wiki/Principal_component_analysis)
    (or PCA for short). Dimensional reduction is an active research area, with new
    methods still being developed.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在1990年，投影高维数据到较小维度集的主要方法是[主成分分析](https://en.wikipedia.org/wiki/Principal_component_analysis)（简称PCA）。维度降维是一个活跃的研究领域，目前仍在不断开发新的方法。
- en: 'Perhaps the most-used algorithms of dimension reduction today are:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 目前最常用的降维算法包括：
- en: PCA
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: PCA
- en: t-SNE
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: t-SNE
- en: UMAP
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: UMAP
- en: Which is the best? It really depends of the details of the data, and on your
    goals for creating the reduction in dimensions.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 哪种方法最好？这实际上取决于数据的细节以及你创建维度降维的目标。
- en: PCA is probably the best method overall, as it is deterministic and allows you
    to create a mapping from the high-dimensional space to the reduced space. That
    is useful for training on one dataset, and then examining where a test dataset
    is projected into the learned space. However, PCA can be influenced by unscaled
    data, and can lead to a “ball of points” giving little insight into structural
    patterns.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: PCA可能是总体上最好的方法，因为它是确定性的，并且允许你从高维空间创建到降维空间的映射。这对于在一个数据集上训练，然后检查测试数据集在学习空间中的投影非常有用。然而，PCA可能会受到未缩放数据的影响，并可能导致一个“点球”，这对结构模式的洞察较少。
- en: t-SNE, which stands for t-distributed Stochastic Neighbor Embedding, was created
    by Roweis and Hinton (yes, that Hinton) in 2002\. This is a learned projection,
    and can exploit unscaled data. However, one downside to t-SNE is that it does
    not create a mapping, but is merely a learning method itself to find a clustering.
    That is, unlike other algorithms that have **Projection.fit()** and **Projection.transform()**
    methods, t-SNE can only perform a fit. (There are some implementations, such as
    [openTSNE](https://pypi.org/project/openTSNE/), that provide a transform mapping.
    However, openTSNE appears to be very different than other algorithms, is slow,
    and is less supported than other forms.)
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: t-SNE，即t-分布随机邻域嵌入，由Roweis和Hinton（是的，就是那个Hinton）于2002年创建。这是一种学习型投影，可以利用未缩放的数据。然而，t-SNE的一个缺点是它不会创建映射，而只是一个用于寻找聚类的学习方法。也就是说，与具有**Projection.fit()**和**Projection.transform()**方法的其他算法不同，t-SNE只能进行拟合。（有些实现，如[openTSNE](https://pypi.org/project/openTSNE/)，提供了转换映射。然而，openTSNE似乎与其他算法非常不同，速度较慢，支持度也低于其他形式。）
- en: Finally, there is UMAP, Uniform Manifold Approximation and Projection. This
    method was created in [2018 by McInnes and Healy](https://arxiv.org/abs/1802.03426v1).
    This may be the best compromise for many high-dimensional spaces as it fairly
    computationally inexpensive, and yet is capable of preserving important representational
    structures in the reduced dimensions.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，还有UMAP，即均匀流形近似与投影。该方法由[McInnes和Healy于2018年创建](https://arxiv.org/abs/1802.03426v1)。对于许多高维空间，这可能是最佳的折中方案，因为它计算开销相对较小，但能够在降维中保留重要的表示结构。
- en: 'Here is an example of the dimension reduction algorithms applied to the unscaled
    Breast Cancer data available in sklearn:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个将维度降维算法应用于sklearn中可用的未缩放乳腺癌数据的示例：
- en: '![](../Images/a9bb5c044b83a9fbd64de77d9db9dfb7.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a9bb5c044b83a9fbd64de77d9db9dfb7.png)'
- en: Example dimensional reductions between three projection methods, PCA, t-SNE,
    and UMAP. Image made by the author.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 三种投影方法的降维示例，包括PCA、t-SNE和UMAP。图像由作者制作。
- en: You can test out the dimension reduction algorithms yourself in order to find
    the best for your use-case, and create images like the above, using [Kangas DataGrid](https://github.com/comet-ml/kangas/wiki/Embedding).
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以自己测试维度降维算法，以找到适合你用例的最佳方法，并使用[Kangas DataGrid](https://github.com/comet-ml/kangas/wiki/Embedding)创建如上所示的图像。
- en: 'As mentioned, dimensional reduction is still an active research area. I fully
    expect to see continued improvements in this area, including visualizing the flow
    of information as it moves throughout a Deep Learning network. Here is a final
    example from our book chapter showing how activations flow in the representational
    space of our model:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，维度缩减仍然是一个活跃的研究领域。我完全期待在这一领域看到持续的改进，包括可视化信息在深度学习网络中的流动。以下是我们书籍章节中的一个最终示例，展示了激活在我们模型的表示空间中的流动：
- en: '![](../Images/9862b5b4aa7c98611c81c035b341cd35.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9862b5b4aa7c98611c81c035b341cd35.png)'
- en: Figure 7 from our paper. Hidden layer activations over single steps in the decoding
    section of the neural network.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 图7来自我们的论文。神经网络解码部分中单步的隐藏层激活。
- en: '***Interested in where ideas in Artificial Intelligence, Machine Learning,
    and Data Science come from? Consider a clap and a subscribe. Let me know what
    you are interested in!***'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '***对人工智能、机器学习和数据科学中的想法来源感兴趣吗？请考虑点赞和订阅。告诉我你感兴趣的内容！***'
