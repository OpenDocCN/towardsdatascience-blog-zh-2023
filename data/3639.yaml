- en: Implementing LoRA from Scratch
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从零实现 LoRA
- en: 原文：[https://towardsdatascience.com/implementing-lora-from-scratch-20f838b046f1?source=collection_archive---------0-----------------------#2023-12-12](https://towardsdatascience.com/implementing-lora-from-scratch-20f838b046f1?source=collection_archive---------0-----------------------#2023-12-12)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/implementing-lora-from-scratch-20f838b046f1?source=collection_archive---------0-----------------------#2023-12-12](https://towardsdatascience.com/implementing-lora-from-scratch-20f838b046f1?source=collection_archive---------0-----------------------#2023-12-12)
- en: How to implement LoRA from scratch and some practical tips
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何从零实现 LoRA 以及一些实用技巧
- en: '[](https://medium.com/@martin.p.dittgen?source=post_page-----20f838b046f1--------------------------------)[![Martin
    Dittgen](../Images/b469995c47e0cc4859225d225ab373db.png)](https://medium.com/@martin.p.dittgen?source=post_page-----20f838b046f1--------------------------------)[](https://towardsdatascience.com/?source=post_page-----20f838b046f1--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----20f838b046f1--------------------------------)
    [Martin Dittgen](https://medium.com/@martin.p.dittgen?source=post_page-----20f838b046f1--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@martin.p.dittgen?source=post_page-----20f838b046f1--------------------------------)[![马丁·迪特根](../Images/b469995c47e0cc4859225d225ab373db.png)](https://medium.com/@martin.p.dittgen?source=post_page-----20f838b046f1--------------------------------)[](https://towardsdatascience.com/?source=post_page-----20f838b046f1--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----20f838b046f1--------------------------------)
    [马丁·迪特根](https://medium.com/@martin.p.dittgen?source=post_page-----20f838b046f1--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F733d27d88c18&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fimplementing-lora-from-scratch-20f838b046f1&user=Martin+Dittgen&userId=733d27d88c18&source=post_page-733d27d88c18----20f838b046f1---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----20f838b046f1--------------------------------)
    ·17 min read·Dec 12, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F20f838b046f1&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fimplementing-lora-from-scratch-20f838b046f1&user=Martin+Dittgen&userId=733d27d88c18&source=-----20f838b046f1---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F733d27d88c18&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fimplementing-lora-from-scratch-20f838b046f1&user=Martin+Dittgen&userId=733d27d88c18&source=post_page-733d27d88c18----20f838b046f1---------------------post_header-----------)
    发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----20f838b046f1--------------------------------)
    ·17分钟阅读·2023年12月12日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F20f838b046f1&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fimplementing-lora-from-scratch-20f838b046f1&user=Martin+Dittgen&userId=733d27d88c18&source=-----20f838b046f1---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F20f838b046f1&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fimplementing-lora-from-scratch-20f838b046f1&source=-----20f838b046f1---------------------bookmark_footer-----------)![](../Images/503b0cc8bc85127a81c5479c847024b8.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F20f838b046f1&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fimplementing-lora-from-scratch-20f838b046f1&source=-----20f838b046f1---------------------bookmark_footer-----------)![](../Images/503b0cc8bc85127a81c5479c847024b8.png)'
- en: '*Abstract artistic representation of LoRA, created by DALLE*'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '*由 DALLE 创建的 LoRA 抽象艺术表现*'
- en: In this blog post, I will show you how to implement LoRA from scratch.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇博客文章中，我将向你展示如何从零开始实现 LoRA。
- en: LoRA, an acronym for *Low-Rank Adaptation* or *Low-Rank Adaptors*, offers an
    efficient and lightweight method for fine-tuning pre-existing language models.
    This includes masked language models like *BERT* and *RoBERTa*, as well as causal
    (or chatbot) models such as *GPT*, *Llama*, and *Mistral*.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: LoRA，即*低秩适配*（*Low-Rank Adaptation*）或*低秩适配器*（*Low-Rank Adaptors*），提供了一种高效且轻量级的方法来微调现有的语言模型。这包括像*BERT*和*RoBERTa*这样的掩码语言模型，以及像*GPT*、*Llama*和*Mistral*这样的因果（或聊天机器人）模型。
- en: One of the main advantages of low-rank adaptors is their efficiency. By utilizing
    fewer parameters, LoRAs significantly lower computational complexity and memory
    usage. This allows us to train large models on consumer-grade GPUs and effortlessly
    distribute our compact (in terms of megabytes) LoRAs to others.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 低秩适配器的主要优点之一是其高效性。通过使用更少的参数，LoRA 显著降低了计算复杂性和内存使用。这使我们能够在消费级 GPU 上训练大型模型，并轻松将我们紧凑（以兆字节为单位）的
    LoRA 分发给他人。
- en: Additionally, LoRAs can improve generalization performance. By constraining
    the model complexity, they help prevent overfitting, especially in scenarios with
    limited training data. This results in more resilient models that excel with new,
    unseen data, or at the very least, retain the knowledge from their initial training
    tasks.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，LoRA 可以提高泛化性能。通过限制模型复杂性，它们有助于防止过拟合，特别是在训练数据有限的情况下。这导致模型更能适应新的、未见过的数据，或者至少保留其初始训练任务的知识。
- en: Furthermore, low-rank adaptors can be seamlessly integrated into existing neural
    network architectures. This integration allows for fine-tuning and adaptation
    of pre-trained models with minimal additional training cost, making them highly
    suitable for transfer learning applications.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，低秩适配器可以无缝集成到现有的神经网络架构中。这种集成允许在最小的额外训练成本下进行预训练模型的微调和适应，使其非常适用于迁移学习应用。
- en: We’ll start by delving into how LoRA functions, then I’ll demonstrate how you
    can develop it from scratch for a *RoBERTa* model, followed by benchmarking our
    implementation using the *GLUE* and *SQuAD* benchmarks along with a discussion
    on general tips and improvements.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先深入探讨 LoRA 的功能，然后我将演示如何为 *RoBERTa* 模型从头开始开发它，并使用 *GLUE* 和 *SQuAD* 基准测试我们的实现，并讨论一般的技巧和改进。
- en: How LoRA works
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LoRA 的工作原理
- en: The basic idea of LoRA is to keep the pre-trained matrices (i.e. parameters
    of the original model) frozen (i.e. in a fixed state) and only add a small delta
    to the original matrix, which has fewer parameters than the original matrix.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: LoRA 的基本理念是保持预训练的矩阵（即原始模型的参数）冻结（即保持固定状态），只向原始矩阵添加一个小的 delta，其参数比原始矩阵少。
- en: 'For example consider the matrix *W*, which could either be the parameters of
    a fully connected layer or one of the matrices from the self-attention mechanism
    of a transformer:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 例如考虑矩阵 *W*，它可以是完全连接层的参数，或者是 transformer 的自注意机制中的一个矩阵之一：
- en: '![](../Images/1acfe2d8a621aeb34bb51fe08b784d9c.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1acfe2d8a621aeb34bb51fe08b784d9c.png)'
- en: Obviously, if ***W****-orig* had dimensions *n×m* and we would just initialize
    a new delta matrix with the same dimensions to fine-tune on we would have gained
    nothing; quite to the contrary we would have doubled the parameters.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，如果***W****-orig* 的尺寸为 *n×m*，我们只需初始化一个具有相同尺寸的新的 delta 矩阵进行微调，我们将毫无收获；相反，我们将会增加参数的数量。
- en: The trick is to make **ΔW** less *“dimensional”* than the original matrix, by
    constructing it via matrix multiplication from lower dimensional matrices *B*
    and *A*.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 这个技巧在于通过从较低维度矩阵 *B* 和 *A* 的矩阵乘法构建 **ΔW** 比原始矩阵更少*“维度化”*。
- en: '![](../Images/9ad4155288d76a9fbc316fef34fc12bf.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9ad4155288d76a9fbc316fef34fc12bf.png)'
- en: Where we first define a rank *r*, to be significantly smaller than the base
    matrix dimensions *r≪n* and *r≪m.* Then matrix ***B*** is *n×r* and matrix ***A***
    is *r×m*. Multiplying them yields a matrix with the same dimensions of ***W***,
    but constructed from a much lower parameter count.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先定义一个秩 *r*，要显著小于基本矩阵的维度 *r≪n* 和 *r≪m*。然后矩阵 ***B*** 是 *n×r*，矩阵 ***A*** 是 *r×m*。将它们相乘得到一个具有相同尺寸的
    ***W*** 矩阵，但是是由较低参数数量构建而成的。
- en: Obviously we want our delta to be zero at the start of the training, such that
    the fine-tuning starts just like the original model. Therefore ***B*** is often
    initialized as all zeros and ***A*** is initialized as random (usually normally
    distributed) values.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，我们希望在训练开始时我们的 delta 是零，这样微调才能像原始模型一样开始。因此，***B*** 通常初始化为全零，***A*** 初始化为随机（通常是正态分布）值。
- en: 'For example, this might look like this:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，这可能看起来像这样：
- en: '![](../Images/ea8acfaa08916bdc36979d8989788b40.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ea8acfaa08916bdc36979d8989788b40.png)'
- en: '*A figure with an example of how LoRA might look for an actual matrix*'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '*一个 LoRA 可能在实际矩阵中如何看的示例*'
- en: 'Imagine a situation where our base-dimensionality is 1024 and we chose a LoRA
    rank *r* of 4 then:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一种情况，我们的基础维度是 1024，并且我们选择了 LoRA 的秩 *r* 为 4，则：
- en: '***W*** has 1024 * 1024 ≈ 1 Million parameters'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***W*** 具有 1024 * 1024 ≈ 100 万个参数'
- en: '***A*** & ***B*** have r * 1024 = 4 * 1024 ≈ 4k parameters each, yielding 8k
    in total'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***A*** 和 ***B*** 每个都有 r * 1024 = 4 * 1024 ≈ 4k 个参数，总共是 8k'
- en: Thus we only have to train 0.8% of the parameters to update our matrix with
    LoRA
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 因此，我们只需要训练 0.8% 的参数来使用 LoRA 更新我们的矩阵
- en: 'A little aside, in the LoRA paper they weigh the delta matrix with an alpha
    parameter:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 顺便说一句，在 LoRA 论文中，他们使用 alpha 参数对 delta 矩阵进行加权：
- en: '![](../Images/2c10bf0da1628cfad229a9daefd7501a.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2c10bf0da1628cfad229a9daefd7501a.png)'
- en: If you just set your *α* to the first *r* you experiment with and fine-tune
    the learning rate you can generally change the *r* parameter later without having
    to fine-tune the learning rate again (at least approximately). While we can overlook
    this detail in our implementation, it’s a common feature in many other LoRA libraries,
    such as Hugging Face’s PEFT.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你只是将*α*设置为你实验的第一个*r*并微调学习率，通常可以在以后改变*r*参数，而无需再次微调学习率（至少大致如此）。虽然在我们的实现中可以忽略这一细节，但这是许多其他LoRA库（如Hugging
    Face的PEFT）的常见特性。
- en: Implementing LoRA
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现LoRA
- en: For our implementation we want to stick closely to the original LoRA paper.
    There they tested which matrices of a transformer you actually have to replace.
    They found that, when comparing different strategies on a GPT-3 fine-tune task,
    it was sufficient to only adapt the self-attention mechanism’s query and value
    vectors.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的实现，我们希望紧密跟随原始的LoRA论文。在那里，他们测试了变换器中实际需要替换哪些矩阵。他们发现，在对GPT-3微调任务进行不同策略比较时，仅适配自注意力机制的查询和数值向量就足够了。
- en: Note that many people ignore this assessment nowadays and allow each matrix
    to be fine-tuned, no matter the task or model (see QLoRA paper).
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 注意到现在很多人忽视了这种评估，并允许每个矩阵进行微调，无论任务或模型如何（参见QLoRA论文）。
- en: Our implementation here will be done in PyTorch, but should be easily adaptable
    to different frameworks.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的实现将在PyTorch中完成，但应该很容易适配到不同的框架中。
- en: 'For this blogpost, I simplified the code a bit, such that it should be easier
    to read, while still showing the essential elements. The full code and some trained
    LoRA weights can be found here: [https://github.com/Montinger/Transformer-Workbench](https://github.com/Montinger/Transformer-Workbench).'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这篇博客文章，我简化了一些代码，以便更容易阅读，同时仍展示了核心要素。完整代码和一些训练好的LoRA权重可以在这里找到：[https://github.com/Montinger/Transformer-Workbench](https://github.com/Montinger/Transformer-Workbench)。
- en: Reimplementing the self-attention model
  id: totrans-39
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 重新实现自注意力模型
- en: The model we wish to adapt is the RoBERTa model from Huggingface. The most straightforward
    way is to just re-wrap the original self-attention mechanism `RobertaSelfAttention`.
    The new class `LoraRobertaSelfAttention` will then initialize the LoRA matrices.
    All the B matrices will be initialized with zeros and all the A matrices with
    random numbers from a normal distribution.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望适配的模型是来自Huggingface的RoBERTa模型。最直接的方法是重新包装原始的自注意力机制`RobertaSelfAttention`。新类`LoraRobertaSelfAttention`将初始化LoRA矩阵。所有的B矩阵将初始化为零，所有的A矩阵将用正态分布的随机数初始化。
- en: '[PRE0]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Given these matrices, we now define new class methods `lora_query` and `lora_value`.
    These calculate the ***ΔW*** matrix, i.e. ***BA,*** and add it to the original
    matrix, which we call from the original methods `query` and `value`.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 给定这些矩阵，我们现在定义新的类方法`lora_query`和`lora_value`。这些方法计算***ΔW***矩阵，即***BA***，并将其添加到原始矩阵中，这些原始矩阵由原始方法`query`和`value`调用。
- en: '[PRE1]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Now the ugly part: To use the methods we have to overwrite the original forward
    function of the `RobertaSelfAttention`. Though this is a bit hard-coded (see the
    discussion on improvements later), it is quite simple. First, we copy the original
    forward code from [https://github.com/huggingface/transformers/blob/main/src/transformers/models/roberta/modeling_roberta.py](https://github.com/huggingface/transformers/blob/main/src/transformers/models/roberta/modeling_roberta.py).
    Second we replace each call to `query` by `lora_query` and each call to `value`
    to `lora_value`. The function then looks like this:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是难看的部分：为了使用这些方法，我们必须重写`RobertaSelfAttention`的原始前向函数。虽然这有点硬编码（参见后续改进讨论），但其实很简单。首先，我们从[https://github.com/huggingface/transformers/blob/main/src/transformers/models/roberta/modeling_roberta.py](https://github.com/huggingface/transformers/blob/main/src/transformers/models/roberta/modeling_roberta.py)复制原始前向代码。然后我们将每个`query`调用替换为`lora_query`，每个`value`调用替换为`lora_value`。函数看起来如下：
- en: '[PRE2]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Tada, there we have it: Our implementation of our LoRA-self-attention. Now
    the only task that remains is to swap out the attention modules in the original
    RoBERTa model.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 轰隆隆，我们完成了：我们的LoRA自注意力实现。现在唯一剩下的任务是将原始RoBERTa模型中的注意力模块替换出来。
- en: Replacing the modules
  id: totrans-47
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 替换模块
- en: Ok great, we have replaced the self-attention with our own implementation; but
    how do we get this new class into the old RoBERTa model? Essentially we have to
    loop over each named component of the RoBERTa model, check whether it is of the
    class `RobertaSelfAttention`, and if yes replace it by `LoraRobertaSelfAttention`,
    while making sure that the original weight matrices are retained.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 很好，我们已经用我们自己的实现替换了自注意力；但是我们如何将这个新类加入旧的RoBERTa模型中呢？实质上，我们必须遍历RoBERTa模型的每个命名组件，检查它是否是`RobertaSelfAttention`类，如果是，则替换为`LoraRobertaSelfAttention`，同时确保保留原始的权重矩阵。
- en: In order to achieve this we will write a new wrapper function that can do this
    replacement. Additionally, we also want to add the functionality for fine-tuning
    the RoBERTa model on some actual tasks later
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现这一点，我们将编写一个新的包装函数来进行此替换。此外，我们还希望稍后在一些实际任务上对RoBERTa模型进行微调。
- en: '[PRE3]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'As you can see we call two helper methods in the initialization:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您所见，我们在初始化中调用了两个辅助方法：
- en: '`self.replace_multihead_attention`: This replaces the attention of all neural
    network parts by our previously written `LoraRobertaSelfAttention`'
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`self.replace_multihead_attention`：这将使用我们之前编写的`LoraRobertaSelfAttention`替换所有神经网络部分的注意力。'
- en: '`self.freeze_parameters_except_lora_and_bias`: This will freeze all of the
    main parameters for the training, such that the gradients and optimizer steps
    are only applied to the LoRA parameters and the other bias and layer norm parameters
    we want to keep trainable.'
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`self.freeze_parameters_except_lora_and_bias`：这将冻结所有主要参数，以便在训练中仅应用于LoRA参数以及我们希望保持可训练的其他偏置和层归一化参数。'
- en: '[PRE4]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: We have to loop recursively through all the model parts, as in PyTorch parts
    of the network can (and in fact are for RoBERTa) packed into a separate PyTorch
    module.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 我们必须递归循环遍历所有模型部分，在PyTorch中，这些部分（实际上是RoBERTa的一部分）可以打包到一个单独的PyTorch模块中。
- en: 'Now we have to freeze all the parameters we don’t want to train any longer:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们必须冻结所有不想再训练的参数：
- en: '[PRE5]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Additionally, we have to implement the forward methods to account for the tasks
    we will fine-tune on as well as two methods to save and load the LoRA weights,
    such that we can load the adapters of a previously trained model.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们还必须实现前向方法，以考虑我们将在其上进行微调的任务，以及两种保存和加载LoRA权重的方法，以便我们可以加载先前训练模型的适配器。
- en: 'Cliffhanger: There is a way, that would have made the code much nicer and easy
    to generalize to other network architectures (as ours is pretty hard coded to
    the RoBERTa model). Can you think what this might be? You have time to ponder
    this question until we discuss it in the *Possible Improvements* section below.
    But until then: Let''s test on some benchmarks if our implementation actually
    works.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 悬念：有一种方法，可以让代码变得更加简洁，并且更容易推广到其他网络架构（因为我们的代码相对于RoBERTa模型而言相当硬编码）。你能想到这可能是什么吗？在下面的*可能的改进*部分讨论之前，你有时间思考这个问题。但在此之前：让我们测试一些基准，看看我们的实现是否真的有效。
- en: Benchmarking the results with GLUE and SQuAD
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用GLUE和SQuAD进行基准测试结果
- en: Our implementation is now ready to be evaluated using the GLUE (General Language
    Understanding Evaluation) and SQuAD (*Stanford Question Answering* Dataset) benchmarks.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的实现现在已准备好使用GLUE（通用语言理解评估）和SQuAD（斯坦福问答数据集）基准进行评估。
- en: The GLUE benchmark, a suite of eight diverse NLP tasks, gauges a language model’s
    comprehensive understanding abilities. It includes challenges like sentiment analysis,
    textual entailment, and sentence similarity, offering a robust measure of a model’s
    linguistic adaptability and proficiency.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: GLUE基准测试是一套八项多样化的NLP任务，评估语言模型的全面理解能力。它包括情感分析、文本蕴涵和句子相似性等挑战，提供了模型语言适应能力和熟练度的强有力衡量。
- en: SQuAD, on the other hand, focuses on assessing question-answering models. It
    involves extracting answers from Wikipedia passages, where the model identifies
    the relevant text span. SQuAD v2, a more advanced version, introduces unanswerable
    questions, adding complexity and mirroring real-life situations where models must
    recognize when text lacks an answer.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，SQuAD侧重于评估问答模型。它涉及从维基百科段落中提取答案，模型识别相关的文本片段。更高级的版本SQuAD v2引入了无法回答的问题，增加了复杂性，模拟了现实中模型必须识别文本缺失答案的情况。
- en: Note that for the following benchmark, I did not tune any hyperparameters, did
    not do multiple runes (especially the smaller GLUE datasets are prone to stochastic
    noise), did not do any early stopping, and did not start from a fine-tune on a
    previous GLUE task (as is often done to decrease the variability of the small
    dataset noise and prevent overfitting).
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，对于以下基准测试，我没有调整任何超参数，没有进行多次运行（特别是较小的GLUE数据集容易受到随机噪声的影响），没有进行任何早停策略，并且没有从前一个GLUE任务的精细调整开始（通常用于减少小数据集噪声的可变性和防止过拟合）。
- en: 'All runs:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 所有运行：
- en: Started from a freshly initialized LoRA injection with rank 8 into the RoBERTa-base
    model
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从RoBERTa-base模型中刚初始化的LoRA注入开始，其秩为8
- en: The training is done for exactly 6 epochs for each task, without any early stopping.
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个任务确切地进行了6个epoch的训练，没有任何早停策略。
- en: During the first 2 epochs the learning rate was linearly scaled up to the maximum
    value, and then linearly decayed towards zero over the remaining 4 epochs.
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在前2个epoch期间，学习率线性增加到最大值，然后在剩余的4个epoch期间线性衰减至零。
- en: The maximum learning rate for all tasks was 5e-4.
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所有任务的最大学习率为5e-4。
- en: The batch size for all tasks was 16
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所有任务的批处理大小为16
- en: The RoBERTa-base model has 124.6 million parameters. With the LoRA parameters,
    the biases, and layer norms we only have 420 thousand unfrozen parameters to train.
    This means we essentially train on only 0.34% of the original parameters.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: RoBERTa-base模型有1.246亿个参数。包括LoRA参数、偏差和层规范化，我们只有42万个未冻结参数需要训练。这意味着我们实际上只对原始参数的0.34%进行了训练。
- en: The number of parameters introduced by LoRA for these specific tasks is remarkably
    minimal, amounting to just 1.7 MB of actual disk size. You can find the trained
    LoRAs in the Git repo in the *Output* folder.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: LoRA为这些特定任务引入的参数数量非常少，实际磁盘大小仅为1.7 MB。您可以在Git仓库的*Output*文件夹中找到训练过的LoRA。
- en: 'Post-training, we reloaded the LoRA parameters, reapplied them, and tested
    performance on each task’s validation set. Below are the results:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 训练后，我们重新加载了LoRA参数，重新应用它们，并在每个任务的验证集上测试性能。以下是结果：
- en: '![](../Images/e92c5face32556fe13d7a5f16513bf11.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e92c5face32556fe13d7a5f16513bf11.png)'
- en: Performance on GLUE Benchmarks using LoRA
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 使用LoRA在GLUE基准测试中的性能
- en: '![](../Images/a5ccaa0cbca95310a416bd7cb67f45e4.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a5ccaa0cbca95310a416bd7cb67f45e4.png)'
- en: Performance on SQuAD Datasets using LoRA
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 使用LoRA在SQuAD数据集上的性能
- en: Likely these results could be greatly improved with some hyperparameter fine-tuning.
    Nevertheless, it clearly proves that our LoRA implementation is working and our
    injected low-rank matrices are learning.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 很可能这些结果可以通过一些超参数的微调大大改善。尽管如此，这清楚地证明了我们的LoRA实现是有效的，我们注入的低秩矩阵正在学习中。
- en: Possible Improvements
  id: totrans-79
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 可能的改进
- en: 'Reflecting on our implementation, one might wonder: “Could there have been
    a more efficient, generalizable (i.e. transferable to other network architectures)
    approach than recoding the self-attention class and performing complex replacements?”'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 回顾我们的实现，人们可能会想：“是否存在比重新编码自注意力类和执行复杂替换更有效、更可推广（即适用于其他网络架构）的方法？”
- en: 'Indeed we could have simply implemented a wrapper around the pytorch `nn.Linear`
    function and be more specific on which layers we want to replace with it, via
    checking their names. Similarly, you could write wrappers around most base pytorch
    layers and be able to quickly adapt LoRA to new network architectures. To give
    a quick sketch of how this could be done:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，我们可以简单地在pytorch的`nn.Linear`函数周围实现一个包装器，并具体说明我们想要替换的层的名称。同样地，您可以编写包装器来适应大多数基本的pytorch层，并能够快速调整LoRA以适应新的网络架构。以下是如何快速实现这一点的简要草图：
- en: '[PRE6]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: This is actually (close to) the way the huggingface PEFT (Parameter-Efficient
    Fine-Tuning) library implements LoRA. For any practical application, where you
    are not trying to learn, I strongly recommend using it, instead of coding your
    own.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，这接近了huggingface PEFT（参数高效微调）库实现LoRA的方式。对于任何实际应用场景，如果您不打算学习，我强烈建议使用它，而不是编写自己的代码。
- en: Also it became a rather common practice to inject LoRA into all linear layers
    as well (i.e. all matrices of the self-attention and the two linear layers for
    the fully connected forward network). It is usually a good idea to keep the biases
    and layer-norms trainable, in addition to the LoRA parameters. As they already
    are small you won’t need a low-rank injection for them.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，将 LoRA 注入所有线性层（即自注意力的所有矩阵以及全连接前向网络的两个线性层）也已成为一种相当常见的做法。通常，除了 LoRA 参数外，保持偏置和层归一化可训练也是个好主意。由于它们已经很小，你不需要对它们进行低秩注入。
- en: Quantizing the original matrix weights to conserve GPU VRAM is also advisable,
    facilitating the training of larger models on a given GPU. This can be efficiently
    done using the bits-and-bytes library, now fully integrated with Hugging Face
    (see references).
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 量化原始矩阵权重以节省 GPU VRAM 也是明智的，这样可以在给定 GPU 上训练更大的模型。这可以通过使用 bits-and-bytes 库有效完成，该库现在已完全与
    Hugging Face 集成（见参考文献）。
- en: 'Summarizing, here are the Five Commandments of Low-Rank Adaptation in a serious
    setting:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 总结一下，这里是在严肃环境中低秩适配的五大法则：
- en: '![](../Images/6222aa9676676f2a24a822f2250a2bd2.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6222aa9676676f2a24a822f2250a2bd2.png)'
- en: '*The Five Commandments of Low-Rank Adaptation*'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '*低秩适配的五大法则*'
- en: 'If you find the inscribed stone tablet hard to read, here they are again in
    plain text:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你觉得刻有法则的石碑难以阅读，这里以纯文本重新呈现：
- en: The Five Commandments of Low-Rank Adaptation
  id: totrans-90
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 低秩适配的五大法则
- en: ''
  id: totrans-91
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 1\. Utilize LoRA for efficient model fine-tuning, focusing on keeping parameter
    sizes minimal.
  id: totrans-92
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 1\. 利用 LoRA 高效地对模型进行微调，重点保持参数规模最小。
- en: 2\. Employ the PEFT library for LoRA implementation, avoiding the need for complex
    coding.
  id: totrans-93
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 2\. 使用 PEFT 库进行 LoRA 实现，避免复杂的编码工作。
- en: 3\. Extend LoRA adaptations to all linear layers, enhancing overall model capabilities.
  id: totrans-94
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 3\. 将 LoRA 适配扩展到所有线性层，增强整体模型能力。
- en: 4\. Keep biases and layer norms trainable, as they are critical for model adaptability
    and don’t require low-rank adaptations.
  id: totrans-95
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 4\. 保持偏置和层归一化可训练，因为它们对模型适应性至关重要，不需要低秩适配。
- en: 5\. Apply Quantized-LoRA — QLoRA — to preserve GPU VRAM and train your model,
    enabling the training of larger models.
  id: totrans-96
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 5\. 应用量化 LoRA — QLoRA — 以保护 GPU VRAM 并训练模型，使训练更大模型成为可能。
- en: Remember, training with QLoRA may be a bit slower than LoRA, as it involves
    de-quantizing matrices during each multiplication. For instance, when fine-tuning
    something massive like Llama-7B, QLoRA requires about 75% less VRAM but is roughly
    40% slower compared to standard LoRA. For more insights, check out the blogposts
    I linked in the references.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，使用 QLoRA 训练可能比 LoRA 慢一些，因为它涉及在每次乘法期间对矩阵进行反量化。例如，在微调像 Llama-7B 这样的大型模型时，QLoRA
    需要约 75% 更少的 VRAM，但比标准 LoRA 慢大约 40%。更多见解，请查看我在参考文献中链接的博客文章。
- en: A Step-by-Step Guide to PEFT Implementation
  id: totrans-98
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: PEFT 实现的逐步指南
- en: Let's look at how to actually obey our commandments and implement a better version
    via PEFT.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何真正遵守我们的法则，并通过 PEFT 实现更好的版本。
- en: First off, let’s load our model in a quantized manner. Thanks to the bitsandbytes
    integration with the Huggingface transformers library (introduced in May 2023),
    this is a breeze.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们以量化的方式加载模型。得益于 bitsandbytes 与 Huggingface transformers 库的集成（于 2023 年 5
    月推出），这变得非常简单。
- en: 'We have to specify a configuration file and then load the model directly from
    huggingface with this quantization. Generally, it is best to use the *AutoModel*
    objects from transformers. It is difficult to load a quantized model as a submodule
    of a larger, newly defined, `nn.module` object. You should generally work with
    the raw models from huggingface and thus import directly an `AutoModelForSequenceClassification`
    for the GLUE tasks and `AutoModelForQuestionAnswering` for the SQuAD benchmarks.
    In the configuration we can also specify which parameters not to quantize: Here
    we have to register the classification or qa-output heads, as we want to train
    these in full, i.e. without LoRA, as these were newly initialized for the fine-tuning
    and were never part of the pre-trained base model.'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 我们必须指定一个配置文件，然后直接从 huggingface 加载模型以进行量化。一般来说，最好使用 transformers 中的*AutoModel*对象。将量化模型作为较大、新定义的`nn.module`对象的子模块加载是困难的。你通常应该使用
    huggingface 的原始模型，因此直接导入 GLUE 任务的`AutoModelForSequenceClassification`和 SQuAD 基准的`AutoModelForQuestionAnswering`。在配置中，我们还可以指定不进行量化的参数：在这里，我们必须注册分类或
    qa 输出头，因为我们希望对这些头进行完整的训练，即不使用 LoRA，因为这些头是为微调而新初始化的，且从未成为预训练基础模型的一部分。
- en: '[PRE7]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'You can verify the 4-bit loading by inspecting the model’s modules and parameter
    data types:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过检查模型的模块和参数数据类型来验证4位加载：
- en: '[PRE8]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Now on to inject the LoRA parameters with PEFT. Note that the PEFT library is
    much more flexible, also when working with custom models or other convoluted structures,
    so as long as you are only doing LoRA instead of QLoRA (quantization is usually
    the tricky part).
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 现在开始使用PEFT注入LoRA参数。请注意，PEFT库在处理自定义模型或其他复杂结构时更加灵活，因此只要您只进行LoRA而不是QLoRA（量化通常是棘手的部分）。
- en: The PEFT library targets the modules to replace via their names; thus we have
    to take a look at the models `model.named_parameters()`. Here is how this looks
    for the non-quantized roberta-base model.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: PEFT库通过它们的名称来定位要替换的模块；因此，我们必须查看模型的`model.named_parameters()`。这是在非量化roberta-base模型中的样子。
- en: '[PRE9]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: We can then specify the LoRA targets to select for these strings. The check
    is if it contains the specified substring in its full name. Thus writing `query`
    and `value` is equivalent to our from-scratch implementation above. For the dense
    layers we have to be a bit more careful as the classifier also has a dense output.
    If we wish to fine-tune the other dense layers we have to be more specific via
    `intermediate.dense` and `output.dense`.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以指定LoRA目标以选择这些字符串。检查的方法是，如果其完整名称中包含指定的子字符串，则为真。因此，写`query`和`value`等效于我们的从头开始实现上述内容。对于密集层，我们必须更加小心，因为分类器还具有密集输出。如果我们希望微调其他密集层，我们必须通过`intermediate.dense`和`output.dense`更为具体。
- en: All parameters that were not injected with LoRA parameters are automatically
    frozen, i.e. will not receive any gradient updates. If there are any layers we
    want to train in their original form we can specify them by passing a list to
    the `modules_to_save` parameters of the Lora-Config. In our case, we want to add
    the `LayerNorm` here and the fine-tune heads for GLUE and SQuAD. Note that not
    each element of the lists has to match something. We can simply add the `classifier`
    and `qa_outputs` to this list and then have a single configuration file that will
    work correctly for both tasks.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 所有未注入LoRA参数的参数都会自动冻结，即不会接收任何梯度更新。如果有任何我们希望以其原始形式训练的层，我们可以通过将列表传递给Lora-Config的`modules_to_save`参数来指定它们。在我们的情况下，我们想在这里添加`LayerNorm`和GLUE以及SQuAD的微调头。请注意，列表的每个元素不必匹配某个内容。我们可以简单地将`classifier`和`qa_outputs`添加到此列表中，然后拥有一个可以正确工作于两个任务的单个配置文件。
- en: For the bias parameters you can use the convenient configuration parameter `bias`.
    You can specify either *all* to retrain all biases of all modules, *lora_only*
    to only train the injected ones, or *none* to keep all biases constant during
    training.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 对于偏置参数，你可以使用方便的配置参数`bias`。你可以指定*all*以重新训练所有模块的所有偏置，*lora_only*以仅训练注入的偏置，或者*none*在训练期间保持所有偏置不变。
- en: The following example injects a LoRA with rank 2\. We specify the alpha parameters
    with the 8 above, as this was the rank we tried first and should allow us to keep
    the original learning rate from our from-scratch example.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 以下示例注入了一个秩为2的LoRA。我们用上面的8指定alpha参数，因为这是我们首先尝试的秩，并且应该允许我们保持从头开始示例的原始学习率。
- en: '[PRE10]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Remember, specifying more modules for LoRA injections might increase VRAM requirements.
    If you encounter VRAM limitations, consider reducing the number of target modules
    or the LoRA rank.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，为LoRA注入指定更多模块可能会增加VRAM要求。如果遇到VRAM限制，请考虑减少目标模块的数量或LoRA秩。
- en: 'For training, especially with QLoRA, choose an optimizer that’s compatible
    with quantized matrices. Replace your standard torch optimizer with a bitsandbytes
    variant like so:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 对于训练，特别是使用QLoRA时，选择与量化矩阵兼容的优化器。用bitsandbytes变体替换你的标准torch优化器，如下所示：
- en: '[PRE11]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: You can then train this model like before, without having to explicitly worry
    about QLoRA during training.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，您可以像以前一样训练此模型，而无需在训练过程中明确担心QLoRA。
- en: Once training is complete, the process for saving and reloading your model is
    straightforward. Use `model.save_pretrained` to save your model, specifying the
    desired filename. The PEFT library will automatically create a directory at this
    location, where it stores the model weights and a configuration file. This file
    includes essential details like the base model and LoRA configuration parameters.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 训练完成后，保存和重新加载模型的过程非常简单。使用`model.save_pretrained`保存您的模型，并指定所需的文件名。PEFT库将在此位置自动创建一个目录，其中存储模型权重和配置文件。此文件包括基础模型和LoRA配置参数等重要细节。
- en: To reload the model, utilize `peft.AutoPeftModel.from_pretrained`, passing the
    directory path as an argument. A crucial point to remember is that the LoRA configuration
    currently does not retain the number of classes for which `AutoModelForSequenceClassification`
    was initialized. When using `from_pretrained`, you need to manually input this
    class number as an additional parameter. Failing to do so will result in an error.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 要重新加载模型，请使用 `peft.AutoPeftModel.from_pretrained`，并将目录路径作为参数传递。一个关键点是，LoRA 配置当前不保留
    `AutoModelForSequenceClassification` 初始化时的类别数量。在使用 `from_pretrained` 时，你需要手动输入这个类别数量作为附加参数。如果不这样做，将会导致错误。
- en: The reloaded model will comprise the original base model with the LoRA adapters
    applied. Should you decide to integrate the LoRA adapters permanently into the
    base model matrices, simply execute `model.merge_and_unload()`.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 重新加载的模型将包括应用了 LoRA 适配器的原始基础模型。如果你决定将 LoRA 适配器永久集成到基础模型矩阵中，只需执行 `model.merge_and_unload()`。
- en: For a more hands-on understanding and detailed instructions, have a look at
    the GitHub repository. There, you’ll find two notebooks titled *Train-QLoRA-with-PEFT.ipynb*
    and *Load-LoRA-Weights-PEFT.ipynb*, providing a step-by-step example for training
    and loading models with PEFT.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 要获得更为实操的理解和详细的说明，请查看 GitHub 仓库。在那里，你会找到两个名为*Train-QLoRA-with-PEFT.ipynb*和*Load-LoRA-Weights-PEFT.ipynb*的笔记本，提供了使用
    PEFT 训练和加载模型的逐步示例。
- en: Conclusion
  id: totrans-121
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: '*“We shall not cease from exploration, and the end of all our exploring will
    be to arrive where we started and know the place for the first time.”*'
  id: totrans-122
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*“我们不会停止探索，我们所有的探索最终将是到达我们开始的地方，并第一次了解这个地方。”*'
- en: ''
  id: totrans-123
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*— from “Little Gidding” by T.S. Eliot*'
  id: totrans-124
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*—— 摘自 T.S. 艾略特的《小吉丁》*'
- en: This journey has taken us from a straightforward, albeit hard-coded, LoRA implementation
    to a deeper understanding of low-rank adaptors, their practical implementation,
    and benchmark testing.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 这段旅程带领我们从简单的、尽管是硬编码的 LoRA 实现，深入了解了低秩适配器、它们的实际应用以及基准测试。
- en: We explored an alternative, more efficient implementation strategy and delved
    into the elegance of existing libraries like PEFT for LoRA integration.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 我们探讨了一种更高效的实现策略，并深入了解了像 PEFT 这样的现有库在 LoRA 集成中的优雅。
- en: Our adventure concludes with practical guidelines for employing LoRA, encapsulated
    in the ‘Five Commandments,’ ensuring efficient and effective use of this technique
    in real-world applications and a step-by-step guide on how to implement them in
    practice.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的冒险以实际的 LoRA 使用指南结束，这些指南被概括为“**五项戒律**”，确保在实际应用中有效且高效地使用这一技术，并提供了逐步实施的指南。
- en: References
  id: totrans-128
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考资料
- en: '*All images, unless otherwise noted, are by the author.*'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '*所有图片，除非另有说明，均由作者提供。*'
- en: 'Original LoRA paper: [https://arxiv.org/pdf/2106.09685.pdf](https://arxiv.org/pdf/2106.09685.pdf)'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '原始 LoRA 论文: [https://arxiv.org/pdf/2106.09685.pdf](https://arxiv.org/pdf/2106.09685.pdf)'
- en: 'QLoRA paper: [https://arxiv.org/abs/2305.14314](https://arxiv.org/abs/2305.14314)'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'QLoRA 论文: [https://arxiv.org/abs/2305.14314](https://arxiv.org/abs/2305.14314)'
- en: 'Sentdex Guide on QLoRA finetuning: [https://www.youtube.com/watch?v=J_3hDqSvpmg](https://www.youtube.com/watch?v=J_3hDqSvpmg)'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Sentdex 关于 QLoRA 微调的指南: [https://www.youtube.com/watch?v=J_3hDqSvpmg](https://www.youtube.com/watch?v=J_3hDqSvpmg)'
- en: 'Blogpost about LoRA fine-tuning on Llama: [https://www.anyscale.com/blog/fine-tuning-llms-lora-or-full-parameter-an-in-depth-analysis-with-llama-2](https://www.anyscale.com/blog/fine-tuning-llms-lora-or-full-parameter-an-in-depth-analysis-with-llama-2)'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '关于 Llama 上 LoRA 微调的博客文章: [https://www.anyscale.com/blog/fine-tuning-llms-lora-or-full-parameter-an-in-depth-analysis-with-llama-2](https://www.anyscale.com/blog/fine-tuning-llms-lora-or-full-parameter-an-in-depth-analysis-with-llama-2)'
- en: 'bitsandbytes Hugging Face integration: [https://huggingface.co/blog/4bit-transformers-bitsandbytes](https://huggingface.co/blog/4bit-transformers-bitsandbytes)'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'bitsandbytes Hugging Face 集成: [https://huggingface.co/blog/4bit-transformers-bitsandbytes](https://huggingface.co/blog/4bit-transformers-bitsandbytes)'
- en: 'LoRA training insights: [https://lightning.ai/pages/community/lora-insights/](https://lightning.ai/pages/community/lora-insights/)'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'LoRA 训练见解: [https://lightning.ai/pages/community/lora-insights/](https://lightning.ai/pages/community/lora-insights/)'
- en: 'Expected VRAM savings LoRA vs QLoRA when fine-tuning a Llama model: [https://cloud.google.com/vertex-ai/docs/model-garden/lora-qlora](https://cloud.google.com/vertex-ai/docs/model-garden/lora-qlora)'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '细化 Llama 模型时 LoRA 与 QLoRA 的预期 VRAM 节省: [https://cloud.google.com/vertex-ai/docs/model-garden/lora-qlora](https://cloud.google.com/vertex-ai/docs/model-garden/lora-qlora)'
- en: 'The font I used for the stone slab text, in case you want to create your own:
    [https://www.fontspace.com/sharp-objects-nbp-font-f14469](https://www.fontspace.com/sharp-objects-nbp-font-f14469)'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我用于石板文本的字体，以防你想自己制作：[https://www.fontspace.com/sharp-objects-nbp-font-f14469](https://www.fontspace.com/sharp-objects-nbp-font-f14469)
