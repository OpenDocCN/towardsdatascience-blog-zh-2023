- en: Topological Generalisation with Advective Diffusion Transformers
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于对流扩散变换器的拓扑泛化
- en: 原文：[https://towardsdatascience.com/topological-generalisation-with-advective-diffusion-transformers-70f263a5fec7](https://towardsdatascience.com/topological-generalisation-with-advective-diffusion-transformers-70f263a5fec7)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/topological-generalisation-with-advective-diffusion-transformers-70f263a5fec7](https://towardsdatascience.com/topological-generalisation-with-advective-diffusion-transformers-70f263a5fec7)
- en: Topological Generalisation in GNNs
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: GNNs中的拓扑泛化
- en: One of the key open questions in the study of graph neural networks (GNNs) is
    their generalisation capabilities, in particular, under changes in the topology
    of the graph. In this post, we study this problem from the perspective of graph
    diffusion equations, which are intimately related to GNNs and have been used in
    the past as a framework for analysing GNN dynamics, expressive power, and justifying
    architectural choices. We describe a new architecture based on advective diffusion
    that combines the computational structure of message-passing neural networks (MPNNs)
    and Transformers and shows superior topological generalisation capabilities.
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在图神经网络（GNNs）研究中，一个关键的未解问题是它们的泛化能力，尤其是在图的拓扑发生变化时。在这篇文章中，我们从图扩散方程的角度来研究这个问题，这些方程与GNNs密切相关，过去曾作为分析GNN动态、表达能力和证明架构选择的框架。我们描述了一种基于对流扩散的新架构，该架构结合了消息传递神经网络（MPNNs）和Transformers的计算结构，并显示出卓越的拓扑泛化能力。
- en: '[](https://michael-bronstein.medium.com/?source=post_page-----70f263a5fec7--------------------------------)[![Michael
    Bronstein](../Images/1aa876fce70bb07bef159fecb74e85bf.png)](https://michael-bronstein.medium.com/?source=post_page-----70f263a5fec7--------------------------------)[](https://towardsdatascience.com/?source=post_page-----70f263a5fec7--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----70f263a5fec7--------------------------------)
    [Michael Bronstein](https://michael-bronstein.medium.com/?source=post_page-----70f263a5fec7--------------------------------)'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://michael-bronstein.medium.com/?source=post_page-----70f263a5fec7--------------------------------)[![Michael
    Bronstein](../Images/1aa876fce70bb07bef159fecb74e85bf.png)](https://michael-bronstein.medium.com/?source=post_page-----70f263a5fec7--------------------------------)[](https://towardsdatascience.com/?source=post_page-----70f263a5fec7--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----70f263a5fec7--------------------------------)
    [Michael Bronstein](https://michael-bronstein.medium.com/?source=post_page-----70f263a5fec7--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----70f263a5fec7--------------------------------)
    ·9 min read·Oct 19, 2023
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----70f263a5fec7--------------------------------)
    ·9分钟阅读·2023年10月19日
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/4454e0295635120bca008b7c8f313e5a.png)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4454e0295635120bca008b7c8f313e5a.png)'
- en: 'Image: Unsplash'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：Unsplash
- en: '*This post was co-authored with* [*Qitian Wu*](https://qitianwu.github.io/)
    *and* [*Chenxiao Yang*](https://chr26195.github.io/) *and is based on the paper
    by Q. Wu et al.,* [*Advective Diffusion Transformer for Topological Generalization
    in Graph Learning*](https://arxiv.org/abs/2310.06417) *(2023) arXiv:2310.06417.*'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '*这篇文章与* [*Qitian Wu*](https://qitianwu.github.io/) *和* [*Chenxiao Yang*](https://chr26195.github.io/)
    *共同作者，并基于Q. Wu等人的论文* [*Advective Diffusion Transformer for Topological Generalization
    in Graph Learning*](https://arxiv.org/abs/2310.06417) *(2023) arXiv:2310.06417。*'
- en: Graph Neural Networks (GNNs) have emerged in the last decade as a popular architecture
    for machine learning on graph-structured data, with a wide variety of applications
    ranging from social networks and [life sciences](https://arxiv.org/abs/2307.08423)
    to [drug](https://medium.com/towards-data-science/geometric-ml-becomes-real-in-fundamental-sciences-3b0d109883b5)
    and [food design](https://medium.com/towards-data-science/hyperfoods-9582e5d9a8e4).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 图神经网络（GNNs）在过去十年中作为一种流行的图结构数据机器学习架构出现，应用范围广泛，从社交网络和 [生命科学](https://arxiv.org/abs/2307.08423)
    到 [药物](https://medium.com/towards-data-science/geometric-ml-becomes-real-in-fundamental-sciences-3b0d109883b5)
    和 [食品设计](https://medium.com/towards-data-science/hyperfoods-9582e5d9a8e4)。
- en: Two key theoretical questions regarding GNNs are their *expressive* and *generalisation*
    capabilities. The former question has been addressed extensively in the literature
    by resorting to [variants of the graph isomorphism test](https://medium.com/towards-data-science/expressive-power-of-graph-neural-networks-and-the-weisefeiler-lehman-test-b883db3c7c49)
    [1], and more recently, by formulating GNNs as [discretised diffusion-type equations](https://medium.com/towards-data-science/neural-sheaf-diffusion-for-deep-learning-on-graphs-bfa200e6afa6)
    [2]. However, the second question, despite multiple recent approaches [3–4], is
    still largely open.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 关于 GNN 的两个关键理论问题是它们的 *表达能力* 和 *泛化能力*。前一个问题在文献中已有广泛探讨，通过借助 [图同构测试的变体](https://medium.com/towards-data-science/expressive-power-of-graph-neural-networks-and-the-weisefeiler-lehman-test-b883db3c7c49)
    [1]，以及最近将 GNN 形式化为 [离散化扩散型方程](https://medium.com/towards-data-science/neural-sheaf-diffusion-for-deep-learning-on-graphs-bfa200e6afa6)
    [2]。然而，尽管有多个最近的研究方法 [3–4]，第二个问题仍然大部分悬而未决。
- en: Empirically, GNNs are often reported to show poor performance [5–7] when the
    training and test data are generated from different distributions (so-called *“distribution
    shift”*), especially when the topology of the graph changes (*“topological shift”*).
    This is a major concern in applications such as chemistry, where ML models are
    typically trained on a limited set of molecular graphs and are expected to learn
    some rules that generalise to previously unseen molecules with different structures.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 从经验上看，GNNs 在训练数据和测试数据来源于不同分布（即所谓的*“分布转移”*）时，通常表现较差，尤其是当图的拓扑发生变化（*“拓扑转移”*）时。这在化学等应用中尤为重要，其中
    ML 模型通常在有限的分子图集上训练，并期望学习一些规则，这些规则能够推广到结构不同的未见过的分子。
- en: '![](../Images/e690e59c2c82b6b5ec96935e3b77f35a.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e690e59c2c82b6b5ec96935e3b77f35a.png)'
- en: 'Example of topological shifts in graph-based molecular modeling: three molecules
    on the left have high drug-likeness (QED), whereas the molecules on the right
    do not.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 图基分子建模中的拓扑转变示例：左侧的三个分子具有较高的药物相似性（QED），而右侧的分子则没有。
- en: '**Neural Graph Diffusion**'
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**神经图扩散**'
- en: In a recent collaboration between Oxford and Shanghai Jiao Tong University [8],
    we used the neural diffusion PDE perspective to study the generalisation capabilities
    of GNNs under topological shifts. [Graph neural diffusion](https://medium.com/towards-data-science/graph-neural-networks-as-neural-diffusion-pdes-8571b8c0c774)
    approaches [9] replace discrete GNN layers with a continuous-time differential
    equation of the form
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在最近牛津大学与上海交通大学的合作中 [8]，我们利用神经扩散 PDE 视角研究了 GNN 在拓扑转变下的泛化能力。[图神经扩散](https://medium.com/towards-data-science/graph-neural-networks-as-neural-diffusion-pdes-8571b8c0c774)
    方法 [9] 用连续时间微分方程替换离散 GNN 层，形式为
- en: '**Ẋ**(*t*) = div(**S**(t)∇**X**(t)),'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '**Ẋ**(*t*) = div(**S**(t)∇**X**(t)),'
- en: known as the *graph diffusion equation*. It is initialised with **X**(0) = ENC(**X**ᵢ)
    and run for some time *t*≤*T*, in order to produce the output **X**ₒ = DEC(**X**(*T*))
    [10]. Here **X**(*t*) is an *n*×*d* matrix of *d*-dimensional node features at
    time *t*, **S** is the matrix-valued diffusivity function [11], and ENC/DEC is
    a feature encoder/decoder pair.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 被称为*图扩散方程*。它以**X**(0) = ENC(**X**ᵢ) 初始化，并运行一段时间 *t*≤*T*，以产生输出 **X**ₒ = DEC(**X**(*T*))
    [10]。这里的 **X**(*t*) 是时间 *t* 时的 *n*×*d* 维节点特征矩阵，**S** 是矩阵值的扩散函数 [11]，而 ENC/DEC
    是特征编码器/解码器对。
- en: When **S** depends only on the *structure* of the graph (through its adjacency
    matrix **A**, i.e., **S** = **S**(**A**)), the graph diffusion equation is *linear*
    and referred to as *homogeneous* (in the sense that diffusivity properties are
    “the same” everywhere on the domain). When **S** also depends on the *features*
    (i.e., **S** = **S**(**X**(*t*),**A**) and is thus time-dependent), the equation
    is *nonlinear* (*non-homogeneous*).
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 当 **S** 仅依赖于图的 *结构*（通过其邻接矩阵 **A**，即 **S** = **S**(**A**））时，图扩散方程是 *线性的* 并称为
    *均匀的*（在于扩散特性在整个域上“相同”）。当 **S** 还依赖于 *特征*（即 **S** = **S**(**X**(*t*),**A**），因此是时间依赖的），方程则是
    *非线性的*（*非均匀的*）。
- en: In the graph learning setting, **S** is implemented as a parametric function
    whose parameters are learned using backpropagation based on the downstream task.
    Many standard message-passing graph neural networks (MPNNs) can be recovered as
    special settings of the graph diffusion equation by an appropriate choice of diffusivity
    and a time discretisation scheme [12]. The linear setting corresponds to GNNs
    of the *convolutional* type and the nonlinear setting to the *attentional* one.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在图学习环境中，**S** 被实现为一个参数函数，其参数通过基于下游任务的反向传播学习得到。许多标准的消息传递图神经网络（MPNNs）可以通过适当选择扩散性和时间离散化方案[12]作为图扩散方程的特殊设置进行恢复。线性设置对应于*卷积型*
    GNN，而非线性设置对应于*注意力型* GNN。
- en: '**Generalisation capabilities** of the neural graph diffusion model are related
    to the sensitivity of the solution **X**(*T*) = *f*(**X**(0),**A**) to a perturbation
    of the adjacency matrix **Ã** = **A** + δ**A**. A model is expected to show good
    topological generalisation if *f*(**X**(0),**A**) ≈ *f*(**X**(0),**Ã**) for a
    small δ**A**. This is unfortunately not the case for graph diffusion: we show
    [13] that in both linear and nonlinear cases, the change in **X**(*T*) can be
    very large, of 𝒪(exp(‖δ**A**‖*T*)).'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 神经图扩散模型的**泛化能力**与解**X**(*T*) = *f*(**X**(0),**A**)对邻接矩阵**Ã** = **A** + δ**A**的扰动的敏感性有关。如果*
    f *(**X**(0),**A**) ≈ * f *(**X**(0),**Ã**) 对于一个小的 δ**A**，模型预期能表现出良好的拓扑泛化。然而，对于图扩散来说，这种情况并不成立：我们展示了[13]，在线性和非线性情况下，**X**(*T*)的变化可能非常大，达到
    𝒪(exp(‖δ**A**‖*T*))。
- en: '**Graph Transformers** attempt to overcome this problem by allowing for *non-local
    diffusion*, where exchange of information can occur between any pair of nodes
    rather than only those connected by an edge. This effectively decouples the input
    graph from the computational one (which is now fully-connected and learned through
    the attention mechanism) and makes the model agnostic to the input graph. We prove
    that such a non-local diffusion model is capable of topological generalisation
    [14] under a certain data generation model [15] with an additional assumption
    that the node labels and graph topology are statistically independent.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '**图变换器**尝试通过允许*非局部扩散*来克服这个问题，其中信息交换可以发生在任意一对节点之间，而不仅仅是那些通过边相连的节点之间。这有效地将输入图与计算图解耦（计算图现在是全连接的，并通过注意力机制进行学习），使模型对输入图不敏感。我们证明了这样一个非局部扩散模型在某些数据生成模型[15]下具备拓扑泛化能力[14]，额外假设是节点标签和图拓扑在统计上是独立的。'
- en: This independence assumption, however, is often far from reality, since node
    labels typically correlate with the graph structure. This implies that non-local
    diffusion alone, discarding any observed structural information, is insufficient
    for topological generalisation.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这一独立性假设通常与现实相去甚远，因为节点标签通常与图结构相关。这意味着仅依赖于非局部扩散，忽视任何观察到的结构信息，对于拓扑泛化是不够的。
- en: '**Advective Graph Diffusion**'
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**平流图扩散**'
- en: We consider a more general type of diffusion equations containing an additional
    advective term,
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 我们考虑包含附加平流项的更一般类型的扩散方程，
- en: '**Ẋ**(*t*) = div(**S**(*t*)∇**X**(*t*)) + *β*div(**V**(*t*)**X**(*t*)).'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '**Ẋ**(*t*) = div(**S**(*t*)∇**X**(*t*)) + *β*div(**V**(*t*)**X**(*t*))。'
- en: Equations of this type are known as *advective diffusion* and arise in real-world
    physical systems such as saline solutions. The first *diffusive term* is lead
    by the concentration gradient and describes the evolution of salinity due to spatial
    differences in the salt chemical concentration (**S** represents the molecular
    diffusivity in water). The second *advective term* is related to the movement
    of the water (**V** characterises the flow direction).
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 这种类型的方程被称为*平流扩散*，在实际物理系统中，例如盐水溶液中出现。第一个*扩散项*由浓度梯度主导，描述了由于盐化学浓度的空间差异而导致的盐度演变（**S**表示水中的分子扩散率）。第二个*平流项*与水的运动有关（**V**特征化流动方向）。
- en: '**Advective Diffusion Transformer.** In our setting, the diffusion process
    led by the feature gradient acts as an *internal* driving force of the evolution,
    in which the diffusivity remains invariant across environments. This corresponds
    to environment-invariant latent interactions among nodes, determined by the underlying
    data manifold, that induce all-pair information flow over a complete graph. Architecturally,
    we implement the diffusive term as a *Transformer* with precomputed global attention
    between the input features of all pairs of nodes, **S** = **S**(**X**(0),**A**).'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '**平滑扩散Transformer**。在我们的设置中，由特征梯度引导的扩散过程作为*内部*驱动力，其中扩散性在各种环境中保持不变。这对应于节点之间的环境不变潜在交互，这些交互由基础数据流形决定，导致在完全图上的所有配对信息流。在架构上，我们将扩散项实现为具有预计算全局注意力的*Transformer*，其中考虑了所有节点对的输入特征，**S**
    = **S**(**X**(0),**A**)。'
- en: The advection process driven by the directional movement (where we use **V**=**A**)
    is an *external* force, where the velocity depends on the context. This is analogous
    to environment-sensitive graph topology that is informative for prediction in
    specific environments. Architecturally, the advective term is implemented as message-passing
    on the input graph.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 由方向性运动（我们使用**V**=**A**）驱动的对流过程是一种*外部*力量，其中速度依赖于上下文。这类似于环境敏感的图拓扑，对特定环境中的预测具有信息量。在架构上，对流项实现为在输入图上的消息传递。
- en: '![](../Images/55094d1fdb20966e20a75a7315c30469.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/55094d1fdb20966e20a75a7315c30469.png)'
- en: '*Advective Diffusion Transformer architecture for generalisable learning on
    graph data. The model comprises three modules: a node-wise encoder (ENC), a diffusion
    module, and a node-wise decoder (DEC). The diffusion module is implemented with
    the graph advective diffusion equation, where the diffusion term is instantiated
    as global attention (Transformer-like) and the advection term as local message
    passing.*'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '*平滑扩散Transformer架构用于图数据的可泛化学习。模型包括三个模块：节点编码器（ENC），扩散模块和节点解码器（DEC）。扩散模块通过图平滑扩散方程实现，其中扩散项被实例化为全局注意力（类似Transformer），而对流项则作为局部消息传递。*'
- en: We refer to this architecture combining MPNNs and graph Transformers as the
    *Advective Diffusion Transformer* *(ADiT)*. It requires only learnable weights
    in the feature encoder and decoder neural networks (ENC and DEC) and the attention
    used for the pre-computed diffusivity and is highly parameter-efficient. The use
    of constant diffusivity also yields a closed-form solution of the advective diffusion
    equation, **X**(*t*) = exp(−(**I**−**S**−*β***A**)*t*)**X**(0), which can be numerically
    approximated using Padé-Chebyshev rational series [16] with linear complexity
    in the number of nodes *n*.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将这种结合了MPNN和图Transformer的架构称为*平滑扩散Transformer* *(ADiT)*。它仅需要在特征编码器和解码器神经网络（ENC和DEC）中学习的权重以及用于预计算扩散性的注意力，并且参数效率极高。常量扩散性的使用还提供了平滑扩散方程的闭式解，**X**(*t*)
    = exp(−(**I**−**S**−*β***A**)*t*)**X**(0)，该解可以使用具有线性复杂度的Padé-Chebyshev有理级数[16]进行数值近似。
- en: '**Topological generalisation of Advective Diffusion.** We show [17] that the
    change in **X**(*T*) as a result of graph adjacency perturbation is reduced from
    exponential to polynomial, 𝒪(Poly(‖δ**A**‖*T*)). This suggests that the advective
    diffusion model incorporating observed structural information is capable of controlling
    the impact of topology variation on node representations to arbitrary rates. Furthermore,
    we show that under our graph generation model [15], this architecture has provable
    topological generalisation capability.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '**平滑扩散的拓扑泛化**。我们展示了[17]，在图邻接扰动的结果中，**X**(*T*)的变化从指数级降到了多项式级，𝒪(Poly(‖δ**A**‖*T*))。这表明，结合观测结构信息的平滑扩散模型能够控制拓扑变化对节点表示的影响到任意的速率。此外，我们展示了在我们的图生成模型[15]下，这种架构具有可证明的拓扑泛化能力。'
- en: '**Experimental Validation**'
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**实验验证**'
- en: We provide an extensive experimental validation of the Advective Diffusion Transformer
    on a variety of node-, edge-, graph-level tasks across datasets ranging from molecules
    to social networks. The main focus of the evaluation is a comparison with state-of-the-art
    MPNN and graph Transformers on challenging training/testing splits in order to
    stress-test the generalisation capabilities.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对平滑扩散Transformer进行了广泛的实验验证，涵盖了从分子到社交网络的各种节点、边、图级任务。评估的主要重点是与最先进的MPNN和图Transformer在具有挑战性的训练/测试分割上的比较，以测试其泛化能力。
- en: '![](../Images/f932ffd746cfabeaf9cc3c1af92cca5d.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f932ffd746cfabeaf9cc3c1af92cca5d.png)'
- en: 'Results on node classification datasets: citation network Arxiv and social
    network Twitch. The train/validation/test data are split with the publication
    years of papers and geographic domains of users on Arxiv and Twitch, respectively,
    which introduces distribution shifts. OOM means out-of-memory error.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 节点分类数据集上的结果：引用网络 Arxiv 和社交网络 Twitch。训练/验证/测试数据根据 Arxiv 和 Twitch 上的论文出版年份和用户地理域进行拆分，这引入了分布变化。OOM
    表示内存不足错误。
- en: '![](../Images/8e9322ad8f8f17ea08bfd7a61cb6315b.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8e9322ad8f8f17ea08bfd7a61cb6315b.png)'
- en: 'Dynamic Graph Dataset DPPIN [19] of biological protein interactions where we
    consider three different tasks: node regression, edge regression, and link prediction.
    The performance is measured by RMSE and ROC-AUC, respectively. We consider dataset-level
    data splits for train/valid/test, where different datasets are obtained from distinct
    protein identification methods and have disparate topological patterns.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 动态图数据集 DPPIN [19] 涉及生物蛋白质交互，我们考虑了三个不同的任务：节点回归、边回归和链接预测。性能通过 RMSE 和 ROC-AUC 分别衡量。我们考虑数据集级的数据拆分用于训练/验证/测试，其中不同数据集来自不同的蛋白质识别方法，并具有不同的拓扑模式。
- en: '![](../Images/78c4481446bb4c48f15b1c598ce974cc.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/78c4481446bb4c48f15b1c598ce974cc.png)'
- en: Generating molecular coarse-grained mapping operators where the task is to find
    a representation of how atoms are grouped in a molecule that can be modeled as
    a graph segmentation problem [20]. It boils down to predicting the edges for subgraph
    partition (indicated by colors) resembling the expert annotations (ground truth).
    The presented scores for each method are averaged testing accuracy.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 生成分子粗粒度映射操作符，其中任务是找到如何在分子中对原子进行分组的表示，这可以建模为图分割问题 [20]。这归结为预测子图分区的边（由颜色指示）以类似专家注释（真实值）。每种方法的评分为测试准确度的平均值。
- en: Overall, our experimental results demonstrate the promising power and wide applicability
    of our model in the challenging generalisation tasks over graph data. More broadly,
    our work points to the possibility of leveraging established physical PDEs for
    understanding the generalisation capabilities of GNNs and designing novel generalisable
    architectures in graph learning.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，我们的实验结果展示了我们模型在图数据的挑战性泛化任务中的良好表现和广泛适用性。更广泛地说，我们的工作指出了利用既有物理偏微分方程（PDEs）来理解图神经网络（GNNs）的泛化能力，并设计新颖的可泛化架构的可能性。
- en: '[1] K. Xu et al., How powerful are graph neural networks? (2019) *ICLR*, and
    C. Morris et al., Weisfeiler and Leman go neural: Higher-order graph neural networks
    (2019) *AAAI* established the equivalence between message passing and the graph
    isomorphism test described in the classical paper of B. Weisfeiler and A. Lehman,
    The reduction of a graph to canonical form and the algebra which appears therein
    (1968) *Nauchno-Technicheskaya Informatsia* 2(9):12–16\. See our [previous blog
    post](https://medium.com/towards-data-science/expressive-power-of-graph-neural-networks-and-the-weisefeiler-lehman-test-b883db3c7c49)
    on this topic.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] K. Xu 等人，《图神经网络的能力有多强？》（2019） *ICLR*，以及 C. Morris 等人，《Weisfeiler 和 Leman
    走向神经网络：高阶图神经网络》（2019） *AAAI* 确立了消息传递与 B. Weisfeiler 和 A. Lehman 经典论文中描述的图同构测试之间的等价性，《图的规范形式简化及其所涉及的代数》（1968）
    *Nauchno-Technicheskaya Informatsia* 2(9):12–16。请参阅我们的 [之前的博客文章](https://medium.com/towards-data-science/expressive-power-of-graph-neural-networks-and-the-weisefeiler-lehman-test-b883db3c7c49)。'
- en: '[2] C. Bodnar et al., Neural Sheaf Diffusion: A Topological Perspective on
    Heterophily and Oversmoothing in GNNs (2022) *NeurIPS* (see also an [accompanying
    blog post](https://medium.com/towards-data-science/neural-sheaf-diffusion-for-deep-learning-on-graphs-bfa200e6afa6))
    showed how the ability of the diffusion equation on a cellular sheaf constructed
    over a graph to separate a certain number of node classes depends on the choice
    of the sheaf. F. Di Giovanni et al., Understanding convolution on graphs via energies
    (2023) *TMLR* (see an [accompanying blog post](https://medium.com/towards-data-science/graph-neural-networks-as-gradient-flows-4dae41fb2e8a)
    and a [talk](https://www.youtube.com/watch?v=sgTTtmwOMgE)) described the conditions
    under which diffusion with channel mixing (discretised as a convolutional-type
    GNN) can cope with heterophilic graphs.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] C. Bodnar 等人，《神经层扩散：GNN中的异质性和过平滑的拓扑视角》（2022） *NeurIPS*（另见 [附带的博客文章](https://medium.com/towards-data-science/neural-sheaf-diffusion-for-deep-learning-on-graphs-bfa200e6afa6)）展示了图上构造的细胞层扩散方程分离特定数量节点类别的能力如何取决于层的选择。F.
    Di Giovanni 等人，《通过能量理解图上的卷积》（2023） *TMLR*（另见 [附带的博客文章](https://medium.com/towards-data-science/graph-neural-networks-as-gradient-flows-4dae41fb2e8a)
    和 [讲座](https://www.youtube.com/watch?v=sgTTtmwOMgE)）描述了具有通道混合的扩散（离散化为卷积型GNN）在异质图上的适用条件。'
- en: '[3] V. Garg et al., Generalization and Representational Limits of Graph Neural
    Networks (2020) *ICML*.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] V. Garg 等人，《图神经网络的泛化和表示极限》（2020） *ICML*。'
- en: '[4] H. Tang and Y. Liu, Towards Understanding Generalization of Graph Neural
    Networks (2023) *ICML*.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] H. Tang 和 Y. Liu，《理解图神经网络的泛化》（2023） *ICML*。'
- en: '[5] P. W. Koh et al., WILDS: A benchmark of in-the-wild distribution shifts
    (2021) *ICML*.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '[5] P. W. Koh 等人，《WILDS：现实环境中分布变化的基准》（2021） *ICML*。'
- en: '[6] W. Hu et al., OGB-LSC: A large-scale challenge for machine learning on
    graphs (2021) *arXiv*:2103.09430.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '[6] W. Hu 等人，《OGB-LSC：图上机器学习的大规模挑战》（2021） *arXiv*:2103.09430。'
- en: '[7] G. Bazhenov and D. Kuznedelev, Evaluating robustness and uncertainty of
    graph models under structural distributional shifts (2023) *arXiv*:2302.13875.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '[7] G. Bazhenov 和 D. Kuznedelev，《在结构分布变化下评估图模型的鲁棒性和不确定性》（2023） *arXiv*:2302.13875。'
- en: '[8] Q. Wu et al., Advective Diffusion Transformer for Topological Generalization
    in Graph Learning (2023) *arXiv*:2310.06417.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '[8] Q. Wu 等人，《用于图学习的自流扩散变换器（Advective Diffusion Transformer）以实现拓扑泛化》（2023）
    *arXiv*:2310.06417。'
- en: '[9] There exist multiple diffusion-based GNN models. See, e.g., B. Chamberlain
    et al., GRAND: graph neural diffusion (2021) *ICML,* an [accompanying blog post](https://medium.com/towards-data-science/graph-neural-networks-as-neural-diffusion-pdes-8571b8c0c774),
    [talk](https://youtu.be/9SMbH18nMUg?si=3aNK6gFEjTcDcCHN),as well as our more general
    blog post on [physics-inspired GNNs](https://medium.com/towards-data-science/graph-neural-networks-beyond-weisfeiler-lehman-and-vanilla-message-passing-bc8605fa59a).'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '[9] 存在多种基于扩散的GNN模型。例如，B. Chamberlain 等人，《 GRAND: 图神经扩散》（2021） *ICML*，[附带的博客文章](https://medium.com/towards-data-science/graph-neural-networks-as-neural-diffusion-pdes-8571b8c0c774)、[讲座](https://youtu.be/9SMbH18nMUg?si=3aNK6gFEjTcDcCHN)，以及我们关于
    [物理启发的GNN](https://medium.com/towards-data-science/graph-neural-networks-beyond-weisfeiler-lehman-and-vanilla-message-passing-bc8605fa59a)
    的更一般性博客文章。'
- en: '[10] Additional boundary conditions can be imposed e.g. to model missing features,
    see E. Rossi et al., On the Unreasonable Effectiveness of Feature propagation
    in Learning on Graphs with Missing Node Features (2022) *LoG* (see also an [accompanying
    blog post](https://medium.com/towards-data-science/learning-on-graphs-with-missing-features-dd34be61b06)
    and a [talk](https://youtu.be/xe5A-xQTBdM?si=ftcUtxZHSHv0wXwq)).'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '[10] 可以施加额外的边界条件，例如，来模拟缺失的特征，请参见 E. Rossi 等人，《缺失节点特征的图学习中特征传播的非理性有效性》（2022）
    *LoG*（另见 [附带的博客文章](https://medium.com/towards-data-science/learning-on-graphs-with-missing-features-dd34be61b06)
    和 [讲座](https://youtu.be/xe5A-xQTBdM?si=ftcUtxZHSHv0wXwq)）。'
- en: '[11] The graph *G* is assumed to have *n* nodes and *m* edges, **W** is the
    *n*×*n* adjacency matrix with *wᵤᵥ*=1 if *u*~*v* and zero otherwise. The gradient
    is an operator assigning to each edge *u*~*v* the difference of the respective
    node feature vectors, (∇**X**)*ᵤᵥ*=**x***ᵥ*−x*ᵤ* and the divergence at node *u*
    sums the features of edges emanating from it, (div(**X**))*ᵤ*= ∑*ᵥ* *wᵤᵥ* **x***ᵤᵥ*.
    Given *d*-dimensional node features arranged into an *n*×*d* matrix **X**, the
    gradient ∇**X** can be represented as a matrix of size *m*×*d*. Similarly, given
    edge features matrix **Y** of size *m*×*d*, the divergence div(**Y**) is an *n*×*d*
    matrix. The two operators are adjoint w.r.t. the appropriate inner products, ⟨∇**X**,**Y**⟩=⟨**X**,div(**Y**)⟩.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '[11] 图 *G* 假定有 *n* 个节点和 *m* 条边，**W** 是 *n*×*n* 的邻接矩阵，其中 *wᵤᵥ*=1 如果 *u*~*v*，否则为零。梯度是一个操作符，将每条边
    *u*~*v* 分配给各自节点特征向量的差异，（∇**X**）*ᵤᵥ*=**x***ᵥ*−x*ᵤ*，节点 *u* 的散度将来自该节点的边的特征相加，（div(**X**））*ᵤ*=
    ∑*ᵥ* *wᵤᵥ* **x***ᵤᵥ*。给定 *d* 维的节点特征排列成 *n*×*d* 矩阵 **X**，梯度 ∇**X** 可以表示为 *m*×*d*
    的矩阵。类似地，给定大小为 *m*×*d* 的边特征矩阵 **Y**，散度 div(**Y**) 是一个 *n*×*d* 的矩阵。这两个操作符在适当的内积下是伴随的，⟨∇**X**，**Y**⟩=⟨**X**，div(**Y**)⟩。'
- en: '[12] For example, **S** can be defined as attention on the connected node features,
    as in P. Veličković et al., Graph Attention Networks (2018) *ICLR*.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '[12] 例如，**S** 可以定义为对连接节点特征的注意力，如 P. Veličković 等人所述，图注意力网络（2018年）*ICLR*。'
- en: '[13] Propositions 1 and 2 in our paper [8].'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '[13] 我们论文中的命题 1 和 2 [8]。'
- en: '[14] Proposition 3 in our paper [8].'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '[14] 我们论文中的命题 3 [8]。'
- en: '[15] The data generation hypothesis in our paper [8] is an extension of the
    graphon (continuous graph) model described in Section 3.1\. The general setting
    does not assume the independence between node labels and graph topology.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '[15] 我们论文中的数据生成假设 [8] 是第 3.1 节中描述的图谱（连续图）模型的扩展。一般情况下，不假设节点标签与图拓扑之间的独立性。'
- en: '[16] See E. Gallopoulos and Y. Saad, Efficient solution of parabolic equations
    by Krylov approximation methods (1992) *SIAM J. Scientific and Statistical Computing*
    13(5):1236–1264\. This method has previously been successfully used in geometry
    processing, e.g., G. Patané, Laplacian spectral distances and kernels on 3D shapes
    (2014) *Pattern Recognition Letters* 47:102–110.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '[16] 见 E. Gallopoulos 和 Y. Saad，利用 Krylov 近似方法有效解决抛物方程（1992年）*SIAM J. Scientific
    and Statistical Computing* 13(5):1236–1264。该方法以前在几何处理方面取得了成功，例如，G. Patané，3D 形状上的拉普拉斯谱距离和核（2014年）*Pattern
    Recognition Letters* 47:102–110。'
- en: '[17] Theorem 1 in our paper [8].'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '[17] 我们论文中的定理 1 [8]。'
- en: '[18] Theorem 2 in our paper [8].'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '[18] 我们论文中的定理 2 [8]。'
- en: '[19] D. Fu and J. He, DPPIN: A biological repository of dynamic protein-protein
    interaction network data (2022) *Big Data*.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '[19] D. Fu 和 J. He，DPPIN: 动态蛋白质-蛋白质相互作用网络数据的生物库（2022年）*Big Data*。'
- en: '[20] Z. Li et al., Graph neural network based coarse-grained mapping prediction
    (2020) *Chemical Science* 11(35):9524–9531.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '[20] Z. Li 等，基于图神经网络的粗粒度映射预测（2020年）*Chemical Science* 11(35):9524–9531。'
- en: '*We are grateful to Yonatan Gideoni and Fan Nie for proofreading this post.
    For additional articles about deep learning on graphs, see Michael’s* [*other
    posts*](https://towardsdatascience.com/graph-deep-learning/home) *in Towards Data
    Science,* [*subscribe*](https://michael-bronstein.medium.com/subscribe) *to his
    posts and* [*YouTube channel*](https://www.youtube.com/c/MichaelBronsteinGDL)*,
    get* [*Medium membership*](https://michael-bronstein.medium.com/membership)*,
    or follow* [*Michael*](https://twitter.com/mmbronstein), [*Qitian*](https://twitter.com/qitianwu_),
    *and* [*Chenxiao*](https://twitter.com/Chenxia58917359) *on Twitter.*'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '*我们感谢 Yonatan Gideoni 和 Fan Nie 对这篇文章的校对。有关图上的深度学习的更多文章，请参见 Michael 的* [*其他文章*](https://towardsdatascience.com/graph-deep-learning/home)
    *在 Towards Data Science 上，* [*订阅*](https://michael-bronstein.medium.com/subscribe)
    *他的文章以及* [*YouTube 频道*](https://www.youtube.com/c/MichaelBronsteinGDL)*，获取* [*Medium
    会员资格*](https://michael-bronstein.medium.com/membership)*，或者关注* [*Michael*](https://twitter.com/mmbronstein)，[*Qitian*](https://twitter.com/qitianwu_)
    *和* [*Chenxiao*](https://twitter.com/Chenxia58917359) *在 Twitter 上。*'
