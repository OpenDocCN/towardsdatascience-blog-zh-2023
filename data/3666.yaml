- en: 'Develop Your First AI Agent: Deep Q-Learning'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 开发你的第一个AI代理：深度Q学习
- en: 原文：[https://towardsdatascience.com/develop-your-first-ai-agent-deep-q-learning-375876ee2472?source=collection_archive---------0-----------------------#2023-12-15](https://towardsdatascience.com/develop-your-first-ai-agent-deep-q-learning-375876ee2472?source=collection_archive---------0-----------------------#2023-12-15)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/develop-your-first-ai-agent-deep-q-learning-375876ee2472?source=collection_archive---------0-----------------------#2023-12-15](https://towardsdatascience.com/develop-your-first-ai-agent-deep-q-learning-375876ee2472?source=collection_archive---------0-----------------------#2023-12-15)
- en: Dive into the world of artificial intelligence — build a deep reinforcement
    learning gym from scratch.
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 深入人工智能世界——从零开始构建深度强化学习环境。
- en: '[](https://medium.com/@heston.cv?source=post_page-----375876ee2472--------------------------------)[![Heston
    Vaughan](../Images/f480ce32e594e4d10258f5929f6bb83c.png)](https://medium.com/@heston.cv?source=post_page-----375876ee2472--------------------------------)[](https://towardsdatascience.com/?source=post_page-----375876ee2472--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----375876ee2472--------------------------------)
    [Heston Vaughan](https://medium.com/@heston.cv?source=post_page-----375876ee2472--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@heston.cv?source=post_page-----375876ee2472--------------------------------)[![赫斯顿·沃恩](../Images/f480ce32e594e4d10258f5929f6bb83c.png)](https://medium.com/@heston.cv?source=post_page-----375876ee2472--------------------------------)[](https://towardsdatascience.com/?source=post_page-----375876ee2472--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----375876ee2472--------------------------------)
    [赫斯顿·沃恩](https://medium.com/@heston.cv?source=post_page-----375876ee2472--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F72a0dba7a030&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdevelop-your-first-ai-agent-deep-q-learning-375876ee2472&user=Heston+Vaughan&userId=72a0dba7a030&source=post_page-72a0dba7a030----375876ee2472---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----375876ee2472--------------------------------)
    ·61 min read·Dec 15, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F375876ee2472&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdevelop-your-first-ai-agent-deep-q-learning-375876ee2472&user=Heston+Vaughan&userId=72a0dba7a030&source=-----375876ee2472---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F72a0dba7a030&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdevelop-your-first-ai-agent-deep-q-learning-375876ee2472&user=Heston+Vaughan&userId=72a0dba7a030&source=post_page-72a0dba7a030----375876ee2472---------------------post_header-----------)
    发布于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----375876ee2472--------------------------------)
    ·61分钟阅读·2023年12月15日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F375876ee2472&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdevelop-your-first-ai-agent-deep-q-learning-375876ee2472&user=Heston+Vaughan&userId=72a0dba7a030&source=-----375876ee2472---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F375876ee2472&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdevelop-your-first-ai-agent-deep-q-learning-375876ee2472&source=-----375876ee2472---------------------bookmark_footer-----------)![](../Images/1dd9d105bd98aeb6b77070c124fa0391.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F375876ee2472&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdevelop-your-first-ai-agent-deep-q-learning-375876ee2472&source=-----375876ee2472---------------------bookmark_footer-----------)![](../Images/1dd9d105bd98aeb6b77070c124fa0391.png)'
- en: Construct your own Deep Reinforcement Learning Gym — Image by author
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 构建你自己的深度强化学习环境——图片作者
- en: '**Table of Contents**'
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**目录**'
- en: '*If you already have a grasp of Reinforcement and Deep Q-Learning concepts,
    feel free to jump directly to the step-by-step tutorial. There you’ll have all
    the resources and code necessary to build a Deep Reinforcement Learning gym from
    the ground up, including the environment, agent, and training protocol.*'
  id: totrans-10
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*如果你已经掌握了强化学习和深度Q学习的概念，可以直接跳到逐步教程。在那里，你将获得所有构建深度强化学习环境所需的资源和代码，包括环境、代理和训练协议。*'
- en: '**Intro**'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '**简介**'
- en: '[Why Reinforcement Learning?](#c87e)'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '[为什么选择强化学习？](#c87e)'
- en: '[What you will gain](#3eed)'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '[你将获得的内容](#3eed)'
- en: '[What is Reinforcement Learning](#9838)?'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '[什么是强化学习](#9838)?'
- en: '[Deep Q-Learning](#31ad)'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '[深度Q学习](#31ad)'
- en: '**Step-by-Step Tutorial**'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '**逐步教程**'
- en: '[1\. Initial Setup](#c0d5)'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '[1\. 初步设置](#c0d5)'
- en: '[2\. The Big Picture](#16af)'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '[2\. 大致概况](#16af)'
- en: '[3\. The Environment: Initial Foundations](#f415)'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '[3\. 环境：初步基础](#f415)'
- en: '[4\. Implement The Agent: Neural Architecture and Policy](#b396)'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '[4\. 实现代理：神经架构和策略](#b396)'
- en: '[5\. Affect The Environment: Finishing Up](#d420)'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '[5\. 影响环境：完成](#d420)'
- en: '[6\. Learn From Experiences: Experience Replay](#05a1)'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '[6\. 从经验中学习：经验重放](#05a1)'
- en: '[7\. Define The Agent’s Learning Process: Fitting The NN](#fa62)'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '[7\. 定义代理的学习过程：调整神经网络](#fa62)'
- en: '[8\. Execute The Training Loop: Putting It All Together](#660f)'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '[8\. 执行训练循环：将一切整合](#660f)'
- en: '[9\. Wrapping It Up](#21fd)'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '[9\. 总结](#21fd)'
- en: '[10\. Bonus: Optimize State Representation](#d8f9)'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '[10\. 附录：优化状态表示](#d8f9)'
- en: Why Reinforcement Learning?
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 为什么选择强化学习？
- en: The recent widespread adoption of advanced AI systems, such as ChatGPT, Bard,
    Midjourney, Stable Diffusion, and many others, has sparked an interest in the
    field of artificial intelligence, machine learning, and neural networks that is
    often left unsatisfied because of the technical nature of implementing such systems.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，像ChatGPT、Bard、Midjourney、Stable Diffusion等先进AI系统的广泛采用，引发了对人工智能、机器学习和神经网络领域的兴趣，但由于实施这些系统的技术性特质，这种兴趣往往未能得到满足。
- en: For those looking to begin their journey into AI (or continue the one they are
    on), building a reinforcement learning gym using Deep Q-Learning is a great start,
    as it does not require advanced knowledge to implement, can be easily expanded
    to solve complex problems, and can give immediate, tangible insight into how artificial
    intelligence becomes “intelligent”.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 对于那些希望开始人工智能之旅（或继续当前进程）的人来说，使用深度Q学习构建一个强化学习gym是一个很好的起点，因为它不需要高级知识来实现，可以轻松扩展以解决复杂问题，并且可以立即直观地理解人工智能如何变得“智能”。
- en: What you will gain
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 你将获得的知识
- en: Assuming you have a basic understanding of Python, by the end of this introduction
    to deep reinforcement learning, without using high-level reinforcement learning
    frameworks, you will have developed your own gym to train an agent to solve a
    simple problem — move itself from its starting point to the goal!
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你对Python有基本了解，在这次深度强化学习的介绍结束时，不使用高级强化学习框架，你将开发自己的gym，以训练代理解决一个简单问题——从起点移动到目标！
- en: It’s not very glamorous, but you will have hands-on experience with topics like
    constructing an environment, defining reward structures and basic neural architecture,
    tweaking environmental parameters to observe different learning behaviors, and
    finding a balance between exploration and exploitation in decision-making.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然不太光鲜，但你将亲身体验到构建环境、定义奖励结构和基本神经架构、调整环境参数以观察不同学习行为，以及在决策中找到探索与利用之间平衡等主题。
- en: You will then have all of the tools you need to implement your own, more complex
    environments and systems, and be well poised to dive deeper into topics like neural
    networks and advanced optimization strategies in reinforcement learning.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 然后你将拥有所有必要的工具来实现自己更复杂的环境和系统，并为深入探讨神经网络和强化学习中的高级优化策略做好充分准备。
- en: '![](../Images/15debfa447043ce3cc589eb460a4be37.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/15debfa447043ce3cc589eb460a4be37.png)'
- en: Image by author using [Gymnasium’s](https://gymnasium.farama.org/) LunarLander-v2
    environment
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者使用[Gymnasium](https://gymnasium.farama.org/)的LunarLander-v2环境制作
- en: You will also gain the confidence and understanding needed to effectively utilize
    pre-built tools like the [OpenAI Gym](https://www.gymlibrary.dev/), as each component
    of the system is implemented from scratch and demystified. This allows you to
    seamlessly integrate these powerful resources into your own AI projects.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 你还将获得有效利用预构建工具如[OpenAI Gym](https://www.gymlibrary.dev/)的信心和理解，因为系统的每个组件都是从头开始实现并解密的。这使得你能够将这些强大的资源无缝集成到自己的AI项目中。
- en: What is Reinforcement Learning?
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 什么是强化学习？
- en: Reinforcement Learning (RL) is a sub-field of Machine Learning (ML) that is
    specifically focused on how agents (the entities making decisions) take actions
    in an environment to complete a goal.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习（RL）是机器学习（ML）的一个子领域，专注于代理（做出决策的实体）如何在环境中采取行动以完成目标。
- en: 'Its implementations include:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 其实现包括：
- en: Games
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 游戏
- en: Autonomous Vehicles
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自动驾驶车辆
- en: Robotics
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器人技术
- en: Finance (algorithmic trading)
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 金融（算法交易）
- en: Natural Language Processing
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自然语言处理
- en: and much more..
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 以及更多内容..
- en: The idea of RL is based on the fundamental principles of behavioral psychology
    where an animal or person learns from the consequences of their actions. If an
    action leads to a good outcome, then the agent is rewarded; if it does not, then
    it is punished or no reward is given.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习的理念基于行为心理学的基本原则，其中动物或人类从其行为的结果中学习。如果某个行动导致了良好的结果，则代理会获得奖励；如果没有，则会受到惩罚或不给予奖励。
- en: 'Before moving on, it is important to understand some commonly used terms:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在继续之前，了解一些常用术语非常重要：
- en: '**Environment**: This is the world — the place where the agent operates. It
    sets the rules, boundaries, and rewards that the agent must navigate.'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**环境**：这是世界——代理操作的地方。它设定了代理必须遵循的规则、边界和奖励。'
- en: '**Agent**: The decision-maker within the environment. The agent takes actions
    based on its understanding of the state it’s in.'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**代理**：环境中的决策者。代理根据对所处状态的理解来采取行动。'
- en: '**State**: A detailed snapshot of the agent’s current situation in the environment,
    including relevant metrics or sensory information used for decision-making.'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**状态**：代理在环境中的当前情况的详细快照，包括用于决策的相关度量或感官信息。'
- en: '**Action**: The specific measure the agent takes to interact with the environment,
    such as moving, collecting an item, or initiating an interaction.'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**行动**：代理与环境交互的具体措施，如移动、收集物品或发起互动。'
- en: '**Reward**: The feedback given from the environment as a result of the agent’s
    actions, which can be positive, negative, or neutral, guiding the learning process.'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**奖励**：环境根据代理的行为给予的反馈，可以是正面的、负面的或中性的，引导学习过程。'
- en: '**State/Action-Space:** The combination of all possible states the agent can
    encounter and all actions it can take in the environment. This defines the scope
    of decisions and situations the agent must learn to navigate.'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**状态/行动空间**：代理可能遇到的所有可能状态和它在环境中可以采取的所有行动的组合。这定义了代理必须学习导航的决策和情况的范围。'
- en: Essentially, in each step (turn) of the program the agent receives a state from
    the environment, chooses an action, receives a reward or punishment, and the environment
    is updated or the episode is complete. Information received after each step is
    saved as an “experience” for later training.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 本质上，在程序的每一步（回合），代理从环境中接收一个状态，选择一个行动，获得奖励或惩罚，环境被更新或回合结束。每一步后收到的信息会被保存为“经验”以供后续训练使用。
- en: For a more concrete example, imagine you are playing chess. The board is the
    ***environment*** and you are the ***agent***. At each step (or turn) you view
    the ***state*** of the board and choose from the action-space, which is the set
    of all possible moves you could make, and pick the ***action*** with the highest
    possible future ***reward***. After the move is made you evaluate whether it was
    a good action or not, and learn to perform better next time.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 举个更具体的例子，假设你在下棋。棋盘是***环境***，你是***代理***。每一步（或回合）你查看棋盘的***状态***，并从行动空间中选择，即所有可能的移动，然后挑选未来***奖励***最高的***行动***。完成移动后，你评估这个行动是否良好，并学习以便下次表现得更好。
- en: It may seem like a lot of information at first, but as you build this out yourself
    these terms will come to feel quite natural.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 这可能一开始看起来信息量很大，但随着你自己逐步建立，这些术语会变得非常自然。
- en: Deep Q-Learning
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 深度 Q 学习
- en: Q-Learning is an algorithm used in ML where the ‘Q’ stands for “Quality”, as
    in the value of actions an agent can take. It works by creating a table of Q-values,
    actions and their associated quality, that estimate the expected future reward
    for taking an action in a given state.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: Q 学习是一种用于机器学习的算法，其中“Q”代表“质量”，即代理可以采取的行动的价值。它通过创建一个 Q 值表来工作，该表包含行动及其相关的质量，用于估算在给定状态下采取某个行动的预期未来奖励。
- en: The agent is given the state of the environment, checks the table to see if
    it has encountered it before, and then chooses the action with the highest reward
    value.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 代理会获得环境的状态，检查表格以查看是否以前遇到过，然后选择奖励值最高的行动。
- en: '![](../Images/da21ab2772af2e4805ae824b776978f0.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/da21ab2772af2e4805ae824b776978f0.png)'
- en: 'Sequential flow of Q-Learning: from state evaluation to reward and Q-Table
    update. — Image by author'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: Q 学习的顺序流程：从状态评估到奖励和 Q 表更新。—— 作者提供的图像
- en: However, Q-Learning has a few drawbacks. Each state and action pair must be
    explored to achieve good results. If the state and action spaces (the set of all
    possible states and actions) are too large, then it is not feasible to store them
    all in a table.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，Q-Learning 有一些缺点。每个状态和动作对必须被探索才能获得良好的结果。如果状态和动作空间（所有可能状态和动作的集合）过大，那么将它们全部存储在表中是不现实的。
- en: This is where Deep Q-Learning (DQL), an evolution of Q-Learning, comes in. DQL
    utilizes a deep Neural Network (NN) to approximate a Q-value function rather than
    saving them in a table. This allows for handling environments that have high-dimensional
    state-spaces, like image inputs from a camera, which would not be practical for
    traditional Q-Learning.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是深度 Q-Learning（DQL）的作用，它是 Q-Learning 的一种进化形式。DQL 利用深度神经网络（NN）来近似 Q 值函数，而不是将其保存到表中。这使得处理具有高维状态空间的环境成为可能，比如来自相机的图像输入，这对于传统的
    Q-Learning 来说是不切实际的。
- en: '![](../Images/ff305fc6ef163adf06221b2a862850f0.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ff305fc6ef163adf06221b2a862850f0.png)'
- en: Deep Q-Learning is the intersection of Q-Learning and Deep Neural Networks —
    Image by author
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 深度 Q-Learning 是 Q-Learning 和深度神经网络的交集 — 作者提供的图像
- en: The neural network can then generalize over similar states and actions, choosing
    a desirable move even if it has not been trained on the exact situation, eliminating
    the need for a large table.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络可以在类似的状态和动作上进行泛化，即使它没有在具体情况上进行过训练，也能选择出合适的动作，从而消除对大型表格的需求。
- en: '*How the neural network does this is beyond the scope of this tutorial. Thankfully,
    a deep understanding is not needed to implement Deep Q-Learning effectively.*'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '*神经网络如何做到这一点超出了本教程的范围。幸运的是，实施深度 Q-Learning 并不需要深刻的理解。*'
- en: Constructing The Reinforcement Learning Gym
  id: totrans-68
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建强化学习 Gym
- en: 1\. Initial Setup
  id: totrans-69
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1\. 初始设置
- en: Before we start coding our AI agent, it is recommended that you have a solid
    understanding of Object Oriented Programming (OOP) principles in Python.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始编写我们的 AI 代理之前，建议您对 Python 中的面向对象编程（OOP）原则有扎实的理解。
- en: If you do not have Python installed already, below is a simple tutorial by [Bhargav
    Bachina](https://medium.com/@bhargavbachina) to get you started. The version I
    will be using is 3.11.6.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您尚未安装 Python，以下是 [Bhargav Bachina](https://medium.com/@bhargavbachina) 提供的简单教程，可以帮助您入门。我将使用的版本是
    3.11.6。
- en: '[](https://medium.com/bb-tutorials-and-thoughts/how-to-install-and-getting-started-with-python-acf369e4cf80?source=post_page-----375876ee2472--------------------------------)
    [## How to Install and Getting Started With Python'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/bb-tutorials-and-thoughts/how-to-install-and-getting-started-with-python-acf369e4cf80?source=post_page-----375876ee2472--------------------------------)
    [## 如何安装和开始使用 Python'
- en: A Beginner’s Guide and for anyone who wants to start learning Python
  id: totrans-73
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 初学者指南，适合任何想要开始学习 Python 的人
- en: medium.com](https://medium.com/bb-tutorials-and-thoughts/how-to-install-and-getting-started-with-python-acf369e4cf80?source=post_page-----375876ee2472--------------------------------)
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: medium.com](https://medium.com/bb-tutorials-and-thoughts/how-to-install-and-getting-started-with-python-acf369e4cf80?source=post_page-----375876ee2472--------------------------------)
- en: The only dependency you will need is [TensorFlow](https://www.tensorflow.org/),
    an open-source machine learning library by Google that we’ll use to build and
    train our neural network. This can be installed through pip in the terminal. My
    version is 2.14.0.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 您唯一需要的依赖是 [TensorFlow](https://www.tensorflow.org/)，这是 Google 提供的开源机器学习库，我们将用来构建和训练我们的神经网络。可以通过终端中的
    pip 安装。我的版本是 2.14.0。
- en: '[PRE0]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Or if that doesn’t work:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 或者如果这样做不行：
- en: '[PRE1]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: You will also need the package [NumPy](https://numpy.org/), but this should
    be included with TensorFlow. If you run into issues there, `pip install numpy`.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 您还需要 [NumPy](https://numpy.org/) 包，但这应该已经包含在 TensorFlow 中。如果遇到问题，可以使用 `pip install
    numpy`。
- en: It is also recommended that you create a new file for each class, (e.g., environment.py).
    This will keep you from being overwhelmed and ease troubleshooting any errors
    you may run into.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 还建议您为每个类创建一个新文件（例如，environment.py）。这样可以避免被信息量淹没，并简化故障排除。
- en: 'For your reference, here is the GitHub repository with the completed code:
    [https://github.com/HestonCV/rl-gym-from-scratch](https://github.com/HestonCV/rl-gym-from-scratch).
    Feel free to clone, explore, and use it as a reference point!'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 供您参考，这里是包含完整代码的 GitHub 仓库：[https://github.com/HestonCV/rl-gym-from-scratch](https://github.com/HestonCV/rl-gym-from-scratch)。请随意克隆、浏览，并将其作为参考！
- en: 2\. The Big Picture
  id: totrans-82
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2\. 全局视角
- en: To really understand the concepts rather than just copying code, it’s crucial
    to get a handle on the different parts we’re going to build and how they fit together.
    This way, each piece will have a place in the bigger picture.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 为了真正理解这些概念，而不仅仅是复制代码，了解我们将要构建的不同部分及其如何结合起来至关重要。这样，每个部分都能在更大的图景中找到位置。
- en: Below is the code for one training loop with 5000 episodes. An episode is essentially
    one complete round of interaction between the agent and the environment, from
    start to finish.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一个包含5000个回合的训练循环的代码。一个回合本质上是代理与环境之间的一个完整的互动过程，从开始到结束。
- en: '*This should not be implemented or fully understood at this point. As we build
    out each part, if you want to see how a specific class or method will be used,
    refer back to this.*'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '*这一点在目前不需要实现或完全理解。当我们构建每一部分时，如果你想了解特定类或方法的使用方式，请回到这里。*'
- en: '[PRE2]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Each inner loop is considered one step.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 每个内循环被视为一步。
- en: '![](../Images/5daaa2788171250a1b86d4aa4857b438.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5daaa2788171250a1b86d4aa4857b438.png)'
- en: Training process through Agent-Environment interaction — Image by author
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 通过代理-环境互动进行的训练过程——图片由作者提供
- en: 'In each step:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 在每一步：
- en: The state is retrieved from the environment.
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 状态从环境中获取。
- en: The agent chooses an action based on this state.
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 代理根据这个状态选择一个动作。
- en: Environment is acted on, returning the reward, resulting state after taking
    the action, and whether the episode is done.
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 环境受到操作，返回奖励，采取动作后的结果状态，以及回合是否结束。
- en: The initial `state`, `action`, `reward`, `next_state`, and `done` are then saved
    into `experience_replay` as a sort of long-term memory (experience).
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 初始的`state`、`action`、`reward`、`next_state`和`done`随后被保存到`experience_replay`中，作为一种长期记忆（经验）。
- en: The agent is then trained on a random sample of these experiences.
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然后，代理在这些经验的随机样本上进行训练。
- en: At the end of each episode, or however often you would like, the model weights
    are saved to the models folder. These can later be preloaded to keep from training
    from scratch each time. The environment is then reset at the start of the next
    episode.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在每个回合结束时，或者按你的需要，模型权重会被保存到模型文件夹中。这些权重可以在后续加载，以避免每次都从头训练。然后，环境在下一个回合开始时被重置。
- en: This basic structure is pretty much all it takes to create an intelligent agent
    to solve a large variety of problems!
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 这个基本结构几乎足以创建一个智能代理来解决各种问题！
- en: 'As stated in the introduction, our problem for the agent is quite simple: get
    from its initial position in a grid to the designated goal position.'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 正如引言中所述，我们对代理的问题相当简单：从网格中的初始位置到达指定的目标位置。
- en: '3\. The Environment: Initial Foundations'
  id: totrans-99
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3\. 环境：初步基础
- en: The most obvious place to start in developing this system is the environment.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 开发这个系统的最明显起点是环境。
- en: 'To have a functioning RL gym, the environment needs to do a few things:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 要拥有一个功能齐全的RL训练环境，环境需要做几件事：
- en: Maintain the current state of the world.
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 维护世界的当前状态。
- en: Keep track of the goal and agent.
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 跟踪目标和代理。
- en: Allow the agent to make changes to the world.
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 允许代理对世界进行修改。
- en: Return the state in a form the model can understand.
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 返回模型可以理解的状态形式。
- en: Render it in a way we can understand to observe the agent.
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 以我们能够理解的方式进行渲染，以观察代理。
- en: This will be the place the agent spends its entire life. We will define the
    environment as a simple square matrix/2D array, or a list of lists in Python.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 这里将是代理度过其整个生命周期的地方。我们将环境定义为一个简单的方阵/二维数组，或在Python中的列表列表。
- en: This environment will have a discrete state-space, meaning that the possible
    states the agent can encounter are distinct and countable. Each state is a separate,
    specific condition or scenario in the environment, unlike a continuous state space
    where the states can vary in an infinite, fluid manner — think of chess versus
    controlling a car.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 该环境将具有离散的状态空间，这意味着代理可能遇到的状态是不同且可计数的。每个状态都是环境中的一个单独、特定的条件或场景，不同于连续状态空间，其中状态可以以无限、流动的方式变化——想象一下国际象棋与控制汽车。
- en: '*DQL is specifically designed for discrete action-spaces (a finite number of
    actions)— this is what we will be focusing on. Other methods are used for continuous
    action-spaces.*'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '*DQL专门设计用于离散动作空间（有限数量的动作）——这将是我们关注的重点。其他方法用于连续动作空间。*'
- en: In the grid, empty space will be represented by 0s, the agent will be represented
    by a 1, and the goal will be represented by a -1\. The size of the environment
    can be whatever you would like, but as the environment grows larger, the set of
    all possible states (state-space) grows exponentially. This can slow training
    time significantly.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 在网格中，空白区域将由 0 表示，智能体将由 1 表示，目标将由 -1 表示。环境的大小可以是您希望的任何大小，但随着环境的增大，所有可能状态的集合（状态空间）会呈指数增长。这可能会显著延长训练时间。
- en: 'The grid will look something like this when rendered:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 渲染后的网格将类似于以下内容：
- en: '[PRE3]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '**Constructing the** `**Environment**` **class and** `**reset**` **method**
    We will begin by implementing the `Environment` class and a way to initialize
    the environment. For now, it will take an integer, `grid_size`, but we will expand
    on this shortly.'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '**构造** `**Environment**` **类和** `**reset**` **方法** 我们将首先实现 `Environment` 类以及初始化环境的方法。目前，它将接受一个整数`grid_size`，但我们很快会扩展这一点。'
- en: '[PRE4]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: When a new instance is created, `Environment` saves `grid_size` and initializes
    an empty grid.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 当创建一个新实例时，`Environment` 会保存 `grid_size` 并初始化一个空网格。
- en: The `reset` method populates the grid using `np.zeros((self.grid_size, self.grid_size))`
    , which takes a tuple, shape, and outputs a 2D NumPy array of that shape consisting
    only of zeros.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '`reset` 方法使用 `np.zeros((self.grid_size, self.grid_size))` 填充网格，该方法接受一个形状的元组，并输出一个由零组成的二维
    NumPy 数组。'
- en: A NumPy array is a grid-like data structure that behaves similar to a list in
    Python, except that it enables us to efficiently store and manipulate numerical
    data. It allows for vectorized operations, meaning that operations are automatically
    applied to all elements in the array without the need for explicit loops.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: NumPy 数组是一种类似网格的数据结构，行为类似于 Python 中的列表，但它使我们能够高效地存储和操作数值数据。它允许矢量化操作，这意味着操作会自动应用于数组中的所有元素，而无需显式循环。
- en: This makes computations on large datasets much faster and more efficient compared
    to standard Python lists. Not only that, but it is the data structure that our
    agent’s neural network architecture will expect!
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 这使得对大型数据集的计算比标准的 Python 列表要快得多且更高效。不仅如此，它还是我们的智能体神经网络架构所期望的数据结构！
- en: Why the name reset? Well, this method will be called to reset the environment
    and will eventually return the initial state of the grid.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么叫做 reset？嗯，这个方法将被调用以重置环境，并最终返回网格的初始状态。
- en: '**Adding the agent and goal**'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '**添加智能体和目标**'
- en: Next, we will construct the methods for adding the agent and the goal to the
    grid.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将构造将智能体和目标添加到网格中的方法。
- en: '[PRE5]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The locations for the agent and the goal will be represented by a tuple (x,
    y). Both methods select random values within the boundaries of the grid and return
    the location. The main difference is that `add_goal` ensures it does not select
    a location already occupied by the agent.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 智能体和目标的位置将由元组 (x, y) 表示。这两个方法都会在网格边界内选择随机值并返回位置。主要区别在于，`add_goal` 确保不会选择已被智能体占据的位置。
- en: We place the agent and goal at random starting locations to introduce variability
    in each episode, which helps the agent learn to navigate the environment from
    different starting points, rather than memorizing one route.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将智能体和目标放置在随机起始位置，以在每个回合中引入变化，这有助于智能体从不同的起点学习如何在环境中导航，而不是记住一条路径。
- en: Finally, we will add a method to render the world in the console to enable us
    to see the interactions between the agent and environment.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将添加一个方法来在控制台中渲染世界，以便我们能够看到智能体与环境之间的互动。
- en: '[PRE6]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '`render` does three things: casts the elements of `self.grid` to type int,
    converts it into a Python list, and prints each row.'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '`render` 做三件事：将 `self.grid` 的元素转换为整数类型，将其转换为 Python 列表，并打印每一行。'
- en: The only reason we don’t print each row from the NumPy array directly is simply
    that it just doesn’t look as nice.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不直接打印 NumPy 数组的每一行的唯一原因就是这样做的效果不够美观。
- en: '**Tying it all together..**'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '**把一切结合起来..**'
- en: '[PRE7]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '[PRE8]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '*When looking at the locations it may seem there was some error, but they should
    be read as (row, column) from the top left to the bottom right. Also, remember
    that the coordinates are zero indexed.*'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '*在查看位置时，可能会感觉有些错误，但它们应该从左上角到右下角读取为（行，列）。另外，记住坐标是从零开始索引的。*'
- en: Okay, so the environment is defined. What next?
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，那么环境已经定义好了。接下来是什么呢？
- en: '**Expanding on** `**reset**`'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '**扩展** `**reset**`'
- en: Let’s edit the reset method to handle placing the agent and goal for us. While
    we are at it, let’s automate render as well.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们编辑`reset`方法以处理代理和目标的放置。顺便说一下，也让我们自动化渲染。
- en: '[PRE9]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Now, when `reset` is called, the agent and goal are added to the grid, their
    initial locations are saved, and if `render_on` is set to true it will render
    the grid.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，当调用`reset`时，代理和目标会被添加到网格中，它们的初始位置会被保存，如果`render_on`设置为true，它将渲染网格。
- en: '[PRE10]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '[PRE11]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '**Defining the state of the environment**'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '**定义环境的状态**'
- en: The last method we will implement for now is `get_state`. At first glance it
    seems the state might simply be the grid itself, but the problem with this approach
    is it is not what the neural network will expect.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将实现的最后一个方法是`get_state`。乍一看，状态可能仅仅是网格本身，但这种方法的问题在于这并不是神经网络所期望的。
- en: Neural networks typically need one-dimensional input, not the two-dimensional
    shape that grid currently is represented by. We can fix this by flattening the
    grid using NumPy’s built-in `flatten` method. This will place each row into the
    same array.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络通常需要一维输入，而不是当前网格所表示的二维形状。我们可以通过使用NumPy的内置`flatten`方法将网格展平来解决这个问题。这将把每一行放入同一个数组中。
- en: '[PRE12]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'This will transform:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 这将转换为：
- en: '[PRE13]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Into:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 转换为：
- en: '[PRE14]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: As you can see, it’s not immediately obvious which cells are which, but this
    will be no problem for a deep neural network.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所看到的，哪一个单元格是哪个并不是一目了然，但这对深度神经网络来说不会是问题。
- en: Now we can update `reset` to return the state right after `grid` is populated.
    Nothing else will change.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以更新`reset`以在`grid`填充之后返回状态。其他内容将保持不变。
- en: '[PRE15]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '**Full code up to this point..**'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '**到目前为止的完整代码..**'
- en: '[PRE16]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: You have now successfully implemented the foundation for the environment! Although,
    if you haven’t noticed, we can’t interact with it yet. The agent is stuck in place.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 你现在已经成功实现了环境的基础！虽然，如果你没有注意到，我们还不能与其互动。代理被卡在了原地。
- en: We will return to this problem later after the `Agent` class has been coded
    to provide better context.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在`Agent`类编写完成后回到这个问题，以提供更好的上下文。
- en: 4\. Implement The Agent Neural Architecture and Policy
  id: totrans-155
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4. 实现代理神经网络架构和策略
- en: As stated previously, the agent is the entity that is given the state of its
    environment, in this case a flattened version of the world grid, and makes a decision
    on what action to take from the action-space.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，代理是接收其环境状态的实体，在这种情况下是世界网格的平面版本，并根据动作空间做出采取何种动作的决定。
- en: '*Just to reiterate, the action-space is the set of all possible actions, in
    this scenario the agent can move up, down, left, and right, so the size of the
    action space is 4.*'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '*需要重申的是，动作空间是所有可能动作的集合，在这种情况下，代理可以向上、向下、向左和向右移动，因此动作空间的大小为4。*'
- en: '*The state-space is the set of all possible states. This can be a massive number
    depending on the environment and perspective of the agent. In our case, if the
    world is a 5x5 grid there are 600 possible states, but if the world is a 25x25
    grid there are 390,000, wildly increasing the training time.*'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: '*状态空间是所有可能状态的集合。根据环境和代理的视角，这可能是一个巨大的数字。在我们的例子中，如果世界是一个5x5的网格，则有600个可能的状态；但如果世界是一个25x25的网格，则有390,000个状态，这会大大增加训练时间。*'
- en: 'For an agent to effectively learn to complete a goal it needs a few things:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 为了让代理有效地学习完成目标，它需要一些条件：
- en: Neural network to approximate the Q-values (estimated total amount of future
    reward for an action) in the case of DQL.
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 神经网络用于在DQL的情况下近似Q值（对一个动作的未来奖励的估计总量）。
- en: Policy or a strategy that the agent follows to choose an action.
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 策略或策略是代理选择动作时遵循的规则。
- en: Reward signals from the environment to tell an agent how well it is doing.
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 环境中的奖励信号告诉代理它的表现如何。
- en: Ability to train on past experiences.
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 能够基于过去的经验进行训练。
- en: 'There are two different policies one can implement:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 可以实现两种不同的策略：
- en: '**Greedy Policy**: Choose the action with the highest Q-value in the current
    state.'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**贪婪策略**：选择当前状态下Q值最高的动作。'
- en: '**Epsilon-Greedy Policy**: Choose the action with the highest Q-value in the
    current state, but there is a small chance, epsilon (commonly denoted as ϵ), to
    choose a random action. If epsilon = 0.02 then there is a 2% chance that the action
    will be random.'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Epsilon-Greedy策略**：选择当前状态下Q值最高的动作，但有一个小的概率，即epsilon（通常表示为ϵ），选择一个随机动作。如果epsilon
    = 0.02，那么这个动作有2%的概率是随机的。'
- en: What we will implement is the **Epsilon-Greedy Policy**.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将实现**Epsilon-Greedy策略**。
- en: Why would random actions help the agent learn? Exploration.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么随机动作有助于代理学习？探索。
- en: When the agent begins, it may learn a suboptimal path to the goal and continue
    to make this choice without ever changing or learning a new route.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 当代理开始时，它可能学习到一条次优路径，并继续选择这条路径而不改变或学习新路径。
- en: Beginning with a large epsilon value and slowly decreasing it allows the agent
    to thoroughly *explore* the environment as it updates its Q-values before *exploiting*
    the learned strategies. The amount we decrease epsilon by over time is called
    epsilon decay, which will make more sense soon.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 从一个较大的 epsilon 值开始，并逐渐减少它，可以让代理在更新 Q 值之前彻底 *探索* 环境，然后再 *利用* 学到的策略。我们随着时间减少 epsilon
    的量称为 epsilon 衰减，稍后会更清楚。
- en: Like we did with the environment, we will represent the agent with a class.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 就像我们对环境做的那样，我们将用一个类来表示代理。
- en: Now, before we implement the policy, we need a way to get Q-values. This is
    where our agent’s brain — or neural network — comes in.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，在实现策略之前，我们需要一种获取 Q 值的方法。这时我们代理的大脑——或神经网络——就派上用场了。
- en: '**The neural network**'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: '**神经网络**'
- en: Without getting too off track here, a neural network is simply a massive function.
    The values go in, get passed to each layer and transformed, and some different
    values pop out at the end. Nothing more than that. The magic comes in when training
    begins.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里不扯太远，神经网络只是一个巨大的函数。值进入后，传递到每一层并进行转换，最后输出一些不同的值。仅此而已。真正的魔力在于训练开始时。
- en: The idea is to give the NN large amounts of labeled data like, “here is an input,
    and here is what you should output”. It slowly adjusts the values between neurons
    with each training step, attempting to get as close as possible to the given outputs,
    finding patterns within the data, and hopefully helping us predict for inputs
    the network has never seen.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 这个想法是给神经网络大量标记的数据，比如，“这是一个输入，应该输出什么”。它在每一步训练中慢慢调整神经元之间的值，试图尽可能接近给定的输出，发现数据中的模式，并希望帮助我们预测网络从未见过的输入。
- en: '![](../Images/b85cd6191a53eb5638e64adcf5c74f34.png)'
  id: totrans-176
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b85cd6191a53eb5638e64adcf5c74f34.png)'
- en: Transformation of State to Q-Values through a neural network — Image by author
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 状态通过神经网络转化为 Q 值 — 作者图片
- en: '**The Agent class and defining the neural architecture** For now we will define
    the neural architecture using TensorFlow and focus on the “forward pass” of the
    data.'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: '**代理类和定义神经网络结构** 目前我们将使用 TensorFlow 定义神经网络结构，并专注于数据的“前向传播”。'
- en: '[PRE17]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '*Again, if you are unfamiliar with neural networks, don’t get too caught up
    on this section. While we use activations like ‘relu’ and ‘linear’ in our model,
    a detailed exploration of activation functions is beyond the scope of this article.*'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: '*再说一次，如果你对神经网络不太熟悉，不要被这一部分困扰。虽然我们在模型中使用了 ‘relu’ 和 ‘linear’ 等激活函数，但对激活函数的详细探讨超出了本文的范围。*'
- en: '*All you really need to know is the model takes in state as input, the values
    are transformed at each layer in the model, and the four Q-values corresponding
    to each action are output.*'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: '*你需要知道的只是模型将状态作为输入，值在模型的每一层中被转换，四个对应于每个动作的 Q 值被输出。*'
- en: In building our agent’s neural network, we start with an input layer that processes
    the state of the grid, represented as a one-dimensional array of size `grid_size²`.
    This is because we’ve flattened the grid to simplify the input. This layer is
    our input itself and does not need to be defined in our architecture because it
    takes no input.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 在构建代理的神经网络时，我们从一个输入层开始，该层处理网格的状态，以 `grid_size²` 大小的一维数组表示。这是因为我们已经将网格展平以简化输入。该层本身就是我们的输入，因此在架构中无需定义，因为它不接受任何输入。
- en: 'Next, we have two hidden layers. These are values we don’t see, but as our
    model learns, they are important for getting a closer approximation of the Q-value
    function:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们有两个隐藏层。这些是我们看不到的值，但随着模型的学习，它们对于更接近 Q 值函数的近似非常重要：
- en: The first hidden layer has 128 neurons, `Dense(128, activation='relu')`, and
    takes the flattened grid as its input.
  id: totrans-184
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 第一个隐藏层有 128 个神经元，`Dense(128, activation='relu')`，并以展平的网格作为输入。
- en: The second hidden layer consists of 64 neurons, `Dense(64, activation='relu')`,
    and further processes the information.
  id: totrans-185
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 第二个隐藏层包含 64 个神经元，`Dense(64, activation='relu')`，进一步处理信息。
- en: Finally, the output layer, `Dense(4, activation='linear')`, comprises 4 neurons,
    corresponding to the four possible actions (up, down, left, right). This layer
    outputs the Q-values — estimates for the future reward of each action.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，输出层 `Dense(4, activation='linear')` 包含 4 个神经元，对应于四种可能的动作（上、下、左、右）。该层输出 Q
    值——每个动作未来奖励的估计。
- en: Typically the more complex problems you have to solve, the more hidden layers
    and neurons you will need. Two hidden layers should be plenty for our simple use-case.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，你需要解决的问题越复杂，你需要的隐藏层和神经元就越多。对于我们的简单用例，两个隐藏层应该足够了。
- en: Neurons and layers can and should be experimented with to find a balance between
    speed and results — each adding to the network’s ability to capture and learn
    from the nuances of the data. Like the state-space, the larger the neural network,
    the slower training will be.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 神经元和层可以并且应该进行实验，以找到速度和结果之间的平衡——每一层都增加了网络捕捉和学习数据细微差别的能力。像状态空间一样，神经网络越大，训练就越慢。
- en: '**Greedy Policy** Using this neural network, we are now able to get a Q-value
    prediction, albeit not a very good one yet, and make a decision.'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: '**贪婪策略** 使用这个神经网络，我们现在可以得到一个 Q 值预测，虽然还不是很理想，但已经可以做出决策了。'
- en: '[PRE18]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: The TensorFlow neural network architecture requires input, the state, to be
    in batches. This is very useful for when you have a large number of inputs and
    you want a full batch of outputs, but it can be a little confusing when you only
    have one input to predict for.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow 神经网络架构要求输入状态为批量数据。这在你有大量输入并希望获得完整批次的输出时非常有用，但当你只有一个输入需要预测时可能会有些混淆。
- en: '[PRE19]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'We can fix this by using NumPy’s `expand_dims` method, specifying `axis=0`.
    What this does is simply make it a batch of one input. For example the state of
    a grid of size 5x5:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过使用 NumPy 的 `expand_dims` 方法并指定 `axis=0` 来解决这个问题。这会简单地将其转换为一个单一输入的批量。例如，一个
    5x5 网格的状态：
- en: '[PRE20]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Becomes:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 变为：
- en: '[PRE21]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'When training the model you will typically use batches of size 32 or more.
    It will look something like this:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练模型时，你通常会使用 32 或更多大小的批量。它看起来像这样：
- en: '[PRE22]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Now that we have prepared the input for the model in the correct format, we
    can predict the Q-values for each action and choose the highest one.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经以正确的格式准备好了模型的输入，我们可以预测每个动作的 Q 值并选择最高的一个。
- en: '[PRE23]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: We simply give the model the state and it outputs a batch of predictions. Remember,
    because we are feeding the network a batch of one, it will return a batch of one.
    Additionally, `verbose=0` ensures that the console remains clear of routine debug
    messages every time the predict function is called.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 我们只需将状态传递给模型，它就会输出一批预测。记住，因为我们提供给网络的是一个批量的单一数据，它将返回一个批量的单一数据。此外，`verbose=0`
    确保在每次调用 predict 函数时控制台不会出现常规调试消息。
- en: Finally, we choose and return the index of the action with the highest value
    using `np.argmax` on the first and only entry in the batch.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们使用 `np.argmax` 在批量中的第一个且唯一的条目上选择并返回具有最高值的动作的索引。
- en: In our case, the indices 0, 1, 2, and 3 will be mapped to up, down, left, and
    right respectively.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的例子中，索引 0、1、2 和 3 将分别映射到上、下、左和右。
- en: The Greedy-Policy always picks the action that has the highest reward according
    to the current Q-values, which may not always lead to the best long-term outcomes.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 贪婪策略总是选择根据当前 Q 值具有最高奖励的动作，但这可能不会总是导致最佳的长期结果。
- en: '**Epsilon-Greedy Policy** We have implemented the Greedy-Policy, but what we
    want to have is the Epsilon-Greedy policy. This introduces randomness into the
    agent’s choice to allow for *exploration* of the state-space.'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: '**Epsilon-贪婪策略** 我们已经实现了贪婪策略，但我们想要的是 Epsilon-贪婪策略。这将随机性引入代理的选择中，以便 *探索* 状态空间。'
- en: Just to recap, epsilon is the probability that a random action will be chosen.
    We also want some way to decrease this over time as the agent learns, allowing
    *exploitation*ofits learned policy. As briefly mentioned before, this is called
    epsilon decay.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 重申一下，epsilon 是选择随机动作的概率。我们还希望有一种方法随着代理的学习逐渐降低这一概率，以便 *利用* 所学策略。如前所述，这称为 epsilon
    衰减。
- en: The epsilon decay value should be set to a decimal number less than 1, which
    is used to progressively reduce the epsilon value after each step the agent takes.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: epsilon 衰减值应设置为小于 1 的十进制数，用于在代理每一步之后逐渐减少 epsilon 值。
- en: Typically epsilon will start at 1, and epsilon decay will be some value very
    close to 1, like 0.998\. After each step in the training process you multiply
    epsilon by the epsilon decay.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，epsilon 会从 1 开始，而 epsilon 衰减值将接近 1，比如 0.998。在训练过程中的每一步，你将 epsilon 乘以 epsilon
    衰减值。
- en: To illustrate this, below is how epsilon will change over the training process.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明这一点，下面是 epsilon 在训练过程中的变化情况。
- en: '[PRE24]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: As you can see epsilon slowly approaches zero with each step. By step 1000,
    there is a 13.5% chance that a random action will be chosen. Epsilon decay is
    a value that will need to be tweaked based on the state-space. With a large state-space,
    more exploration may be necessary, or a higher epsilon decay.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所看到的，epsilon随着每一步慢慢接近零。到第1000步时，随机动作被选择的概率为13.5%。epsilon衰减是一个需要根据状态空间进行调整的值。状态空间较大时，可能需要更多探索或更高的epsilon衰减。
- en: '![](../Images/c6ba59771bc11de889e39a57b19285bb.png)'
  id: totrans-212
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c6ba59771bc11de889e39a57b19285bb.png)'
- en: Decay of epsilon over steps — Image by author
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: epsilon在步骤中的衰减 — 图片由作者提供
- en: Even when the agent is trained well, it is beneficial to keep a small epsilon
    value. We should define a stopping point where epsilon does not get any lower,
    epsilon end. This can be 0.1, 0.01, or even 0.001 depending on the use-case and
    complexity of the task.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 即使代理已经训练得很好，保持一个较小的epsilon值也是有益的。我们应该定义一个停止点，在该点epsilon不再降低，即epsilon结束。根据用例和任务的复杂性，这可以是0.1、0.01，甚至0.001。
- en: In the figure above, you’ll notice epsilon stops decreasing at 0.1, the pre-defined
    epsilon end.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 在上图中，你会注意到epsilon在0.1时停止减少，这是预定义的epsilon结束值。
- en: Let’s update our Agent class to incorporate epsilon.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更新我们的Agent类以包含epsilon。
- en: '[PRE25]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: We’ve given `epsilon`, `epsilon_decay`, and `epsilon_end` default values of
    1, 0.998, and 0.01, respectively.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将`epsilon`、`epsilon_decay`和`epsilon_end`的默认值分别设为1、0.998和0.01。
- en: '*Remember epsilon, and its associated values, are hyper-parameters, parameters
    used to control the learning process. They can and should be experimented with
    to achieve the best result.*'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: '*记住epsilon及其相关值是超参数，用于控制学习过程。它们可以并且应该被实验以达到最佳结果。*'
- en: The method, `get_action`, has been updated to incorporate epsilon. If the random
    value given by `np.random.rand` is less than or equal to `epsilon`, a random action
    is chosen. Otherwise, the process is the same as before.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 方法`get_action`已更新以包含epsilon。如果`np.random.rand`生成的随机值小于或等于epsilon，则选择一个随机动作。否则，过程与之前相同。
- en: Finally, if `epsilon` has not reached `epsilon_end`, we update it by multiplying
    by `epsilon_decay` like so — `self.epsilon *= self.epsilon_decay`.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，如果`epsilon`没有达到`epsilon_end`，我们通过将其乘以`epsilon_decay`来更新它，如`self.epsilon *=
    self.epsilon_decay`。
- en: '`**Agent**` **up to this point:**'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: '`**代理**` **到目前为止：**'
- en: '[PRE26]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: We have effectively implemented the Epsilon-Greedy Policy, and we are almost
    ready to enable the agent to learn!
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经有效地实现了Epsilon-Greedy策略，我们几乎准备好让代理开始学习了！
- en: '5\. Affect The Environment: Finishing Up'
  id: totrans-225
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5\. 影响环境：完成
- en: '`Environment` currently has methods for reseting the grid, adding the agent
    and goal, providing the current state, and printing the grid to the console.'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: '`环境`目前有重置网格、添加代理和目标、提供当前状态以及将网格打印到控制台的方法。'
- en: For the environment to be complete we need to be able to not only allow the
    agent to affect it, but also provide feedback in the form of rewards.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使环境完整，我们需要不仅允许代理影响环境，还需要以奖励的形式提供反馈。
- en: '**Defining the reward structure** Coming up with a good reward structure is
    the main challenge of reinforcement learning. Your problem could be perfectly
    within the capabilities of the model, but if the reward structure is not set up
    correctly it may never learn.'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: '**定义奖励结构**：制定一个好的奖励结构是强化学习的主要挑战。你的问题可能完全在模型的能力范围内，但如果奖励结构设置不正确，模型可能永远无法学习。'
- en: The goal of the rewards is to encourage specific behavior. In our case we want
    to guide the agent towards the goal cell, defined by -1.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 奖励的目标是鼓励特定的行为。在我们的例子中，我们希望引导代理到达由-1定义的目标单元。
- en: Similar to the layers and neurons in the network, and epsilon and its associated
    values, there can be many right (and many wrong) ways to define the reward structure.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于网络中的层和神经元，以及epsilon及其相关值，定义奖励结构也有许多正确（和错误）的方法。
- en: 'The two main types of reward structures:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 奖励结构的两种主要类型：
- en: '**Sparse**: When rewards are only given in a handful of states.'
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**稀疏**：当奖励仅在少数状态中给予时。'
- en: '**Dense**: When rewards are common throughout the state-space.'
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**密集**：当奖励在状态空间中很常见时。'
- en: With sparse rewards the agent has very little feedback to lead it. This would
    be like simply giving a set penalty for each step, and if the agent reaches the
    goal you provide one large reward.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 对于稀疏奖励，代理几乎没有反馈来指导它。这就像是每一步都给一个固定的惩罚，如果代理到达目标则提供一个大奖励。
- en: The agent can certainly learn to reach the goal, but depending on the size of
    the state-space it can take much longer and may get stuck on a suboptimal strategy.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 代理确实可以学习达到目标，但根据状态空间的大小，这可能需要更长的时间，并且可能会陷入次优策略。
- en: This is in contrast with dense reward structures, which allow the agent to train
    quicker and behave more predictably.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 这与稠密奖励结构相对，稠密奖励结构允许代理更快地训练并表现得更可预测。
- en: Dense reward structures either
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 稠密奖励结构要么
- en: have more than one goal.
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有多个目标。
- en: give hints throughout an episode.
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在整个过程中提供提示。
- en: The agent then has more opportunities to learn desired behavior.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 代理有更多的机会学习期望的行为。
- en: For instance, pretend you’re training an agent to use a body to walk, and the
    only reward you give it is if it reaches a goal. The agent may learn to get there
    by simply inching or rolling along the ground, or not even learn at all.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设你在训练一个代理使用身体行走，而你给予的唯一奖励是它达到一个目标。代理可能会通过缓慢移动或在地面上滚动来学习如何到达那里，或者甚至根本没有学习到。
- en: Instead, if you reward the agent for heading towards the goal, staying on its
    feet, putting one foot in front of the other, and standing up straight, you will
    get a much more natural and interesting gait while also improving learning.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，如果你奖励代理朝目标前进、保持站立、迈出一步并保持直立，你将获得更自然和有趣的步态，同时改善学习效果。
- en: '**Allowing the agent to impact the environment** To even have rewards, you
    must allow the agent to interact with its world. Let’s revisit the `Environment`
    class to define this interaction.'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: '**允许代理对环境产生影响** 为了获得奖励，你必须允许代理与其环境互动。让我们重新审视一下`Environment`类，以定义这种互动。'
- en: '[PRE27]'
  id: totrans-244
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: The above code first defines the change in coordinates associated with each
    action value. If the action 0 is chosen, then the coordinates change by (-1, 0).
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码首先定义了与每个动作值相关的坐标变化。如果选择动作0，则坐标变化为（-1, 0）。
- en: '*Remember, in this scenario the coordinates are interpreted as (row, column).
    If row lowers by one, the agent moves up one cell, and if column lowers by one,
    the agent moves left one cell.*'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: '*记住，在这种情况下，坐标被解释为（行，列）。如果行减少1，则代理上移一个单元格；如果列减少1，则代理左移一个单元格。*'
- en: It then calculates the new location based on the move. If the new location is
    valid, `agent_location` is updated. Otherwise, the `agent_location` is left the
    same.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 然后根据移动计算新位置。如果新位置有效，则更新`agent_location`。否则，`agent_location`保持不变。
- en: Also, `is_valid_location` simply checks if the new location is within the grid
    boundaries.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，`is_valid_location` 只是检查新位置是否在网格边界内。
- en: That is fairly straight forward, but what are we missing? Feedback!
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 这相当简单，但我们还缺少什么？反馈！
- en: '**Providing feedback** The environment needs to provide an appropriate reward
    and whether the episode is complete or not.'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: '**提供反馈** 环境需要提供适当的奖励，并确定一集是否完成。'
- en: Let’s incorporate the `done` flag first to indicate that an episode is finished.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们首先加入`done`标志以指示一集是否结束。
- en: '[PRE28]'
  id: totrans-252
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: We’ve set `done` to false by default. If the new `agent_location` is the same
    as `goal_location` then `done` is set to true. Finally, we return this value.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将`done`默认设置为false。如果新的`agent_location`与`goal_location`相同，则将`done`设置为true。最后，我们返回这个值。
- en: We are ready for our reward structure. First, I will show the implementation
    for the sparse reward structure. This would be satisfactory for a grid of around
    5x5, but we will update it to allow for a larger environment.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经为奖励结构做好了准备。首先，我将展示稀疏奖励结构的实现。这对于大约5x5的网格是足够的，但我们将更新它以适应更大的环境。
- en: '**Sparse rewards** Implementing sparse rewards is quite simple. We primarily
    need to give a reward for landing on the goal.'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: '**稀疏奖励** 实现稀疏奖励非常简单。我们主要需要在到达目标时给予奖励。'
- en: Let’s also give a small negative reward for each step that doesn’t land on the
    goal and a larger one for hitting the boundary. This will encourage our agent
    to prioritize the shortest path.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以为每一步未到达目标的情况给予小的负奖励，并为撞击边界的情况给予更大的奖励。这将鼓励我们的代理优先选择最短路径。
- en: '[PRE29]'
  id: totrans-257
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Make sure to initialize `reward` so that it can be accessed after the if blocks.
    Also, check carefully for each case: valid move and achieved goal, valid move
    and did not achieve goal, and invalid move.'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 确保初始化`reward`以便在if块之后可以访问。此外，仔细检查每种情况：有效移动和达成目标、有效移动和未达成目标、以及无效移动。
- en: '**Dense rewards** Putting our dense reward system into practice is still quite
    simple, it just involves providing feedback more often.'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: '**稠密奖励** 实施稠密奖励系统仍然相当简单，只是需要更频繁地提供反馈。'
- en: What would be a good way to reward the agent to move towards the goal more incrementally?
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 让代理逐步朝目标移动的好方法是什么？
- en: 'The first way is to return the negative of the Manhattan distance. The Manhattan
    distance is the distance in the row direction, plus the distance in the column
    direction, rather than as the crow flies. Here is what that looks like in code:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个方法是返回曼哈顿距离的负值。曼哈顿距离是行方向的距离加上列方向的距离，而不是直线距离。以下是代码示例：
- en: '[PRE30]'
  id: totrans-262
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: So, the number of steps in the row direction plus the number of steps in the
    column direction, negated.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，行方向的步数加上列方向的步数，并取其负值。
- en: 'The other way we can do this is provide a reward based on the direction the
    agent moves: if it moves away from the goal provide a negative reward and if it
    moves toward it provide a positive reward.'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种方法是根据代理移动的方向提供奖励：如果它远离目标，则提供负奖励；如果它朝目标移动，则提供正奖励。
- en: We can calculate this by subtracting the new Manhattan distance from the previous
    Manhattan distance. It will either be 1 or -1 because the agent can only move
    one cell per step.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过将新的曼哈顿距离从之前的曼哈顿距离中减去来计算。这将是1或-1，因为代理每步只能移动一个单元格。
- en: In our case it would make most sense to choose the second option. This should
    provide better results because it gives immediate feedback based on that step
    rather than a more general reward.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的情况下，选择第二个选项最为合适。这应该提供更好的结果，因为它基于每一步提供即时反馈，而不是更一般的奖励。
- en: 'The code for this option:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 这个选项的代码：
- en: '[PRE31]'
  id: totrans-268
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: As you can see, if the agent did not get the goal, we calculate `previous_distance`,
    `new_distance`, and then define `reward` as the difference of these.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，如果代理没有达到目标，我们计算`previous_distance`、`new_distance`，然后将`reward`定义为这两者的差值。
- en: Depending on the performance it may be appropriate to scale it, or any reward
    in the system. You can do this by simply multiplying by a number (e.g., 0.01,
    2, 100) if it needs to be higher. Their proportions need to effectively guide
    the agent to the goal. For instance, a reward of 1 for moving closer to the goal
    and a reward of 0.1 for the goal itself would not make much sense.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 根据表现情况，可能需要对其进行缩放，或对系统中的任何奖励进行缩放。如果需要更高，可以通过简单地乘以一个数字（例如0.01、2、100）来实现。它们的比例需要有效地引导代理到目标。例如，为接近目标提供1的奖励，为目标本身提供0.1的奖励是不太合理的。
- en: Rewards are proportional. If you scale each positive and negative reward by
    the same factor it should not generally effect training, aside from very large
    or very small values.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 奖励是成比例的。如果你以相同的因子缩放每个正奖励和负奖励，通常不会对训练产生影响，除非是非常大或非常小的值。
- en: In summary, if the agent is 10 steps away from the goal, and it moves to a space
    11 steps away, then `reward` will be -1.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，如果代理离目标还有10步，而它移动到一个离目标11步的地方，则`reward`将是-1。
- en: '**Here is the updated** `**move_agent**`**.**'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: '**这是更新后的** `**move_agent**`**。**'
- en: '[PRE32]'
  id: totrans-274
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: The reward for achieving the goal and attempting an invalid move should remain
    the same with this structure.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 实现目标和尝试无效移动的奖励应保持一致。
- en: '**Step penalty** There is just one thing we are missing.'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤惩罚** 还有一件事我们遗漏了。'
- en: The agent is currently not penalized for how long it takes to reach the goal.
    Our implemented reward structure has many net neutral loops. It could go back
    and forth between two locations forever, and accumulate no penalty. We can fix
    this by subtracting a small value each step, causing the penalty of moving away
    to be greater than the reward for moving closer. This illustration should make
    it much clearer.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 代理当前没有因达到目标所需时间而受到惩罚。我们实现的奖励结构有许多净中性循环。它可能在两个位置之间来回移动而不积累任何惩罚。我们可以通过每步扣除一个小值来解决这个问题，使得远离目标的惩罚大于接近目标的奖励。这个说明应该会让情况更清楚。
- en: '![](../Images/3a66e1dec8556ff6db6d5b751268180b.png)'
  id: totrans-278
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3a66e1dec8556ff6db6d5b751268180b.png)'
- en: Reward paths with and without a step penalty — Image by author
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 奖励路径有和没有步骤惩罚 — 作者插图
- en: Imagine the agent is starting at the left most node and must make a decision.
    Without a step penalty, it could choose to go forward, then back as many times
    as it wants and its total reward would be 1 before finally moving to the goal.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 想象代理从最左边的节点开始，并必须做出决策。如果没有步骤惩罚，它可以选择前进，然后返回任意次数，其总奖励将在最终移动到目标之前为1。
- en: So mathematically, looping 1000 times and then moving to the goal is just as
    valid as moving straight there.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 所以从数学上讲，循环1000次然后再到达目标和直接到达目标是一样有效的。
- en: Try to imagine looping in either case and see how penalty is accumulated (or
    not accumulated).
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 试着想象在两种情况下循环，看惩罚是如何累积的（或者没有累积）。
- en: Let’s implement this.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来实现它。
- en: '[PRE33]'
  id: totrans-284
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: That’s it. The agent should now be incentivized to take the shortest path, preventing
    looping behavior.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 就这样。代理现在应该受到激励去选择最短路径，防止循环行为。
- en: '**Okay, but what is the point?** At this point you may be thinking it is a
    waste of time to define a reward system and train an agent for a task that could
    be completed with much simpler algorithms.'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: '**好的，但重点是什么？** 此时你可能会认为定义奖励系统并训练一个任务可以用更简单的算法完成是浪费时间。'
- en: And you would be correct.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 你说得对。
- en: The reason we are doing this is to learn how to think about guiding your agent
    to its goal. In this case it may seem trivial, but what if the agent’s environment
    included items to pick up, enemies to battle, obstacles to go through, and more?
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 我们这样做的原因是为了学习如何指导代理实现其目标。在这种情况下可能看起来很简单，但如果代理的环境中包含要拾取的物品、要战斗的敌人、要穿越的障碍物等等呢？
- en: Or a robot in the real world with dozens of sensors and motors that it needs
    to coordinate in sequence to navigate complex and varied environments?
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 或者一个在现实世界中需要协调数十个传感器和电机以导航复杂和多变环境的机器人？
- en: Designing a system to do these things using traditional programming would be
    quite difficult and most certainly would not behave near as organic or general
    as using RL and a good reward structure to encourage an agent to learn optimal
    strategies.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 使用传统编程设计一个系统来完成这些任务将会非常困难，并且肯定不会像使用RL和良好的奖励结构那样自然或通用，以鼓励代理学习最佳策略。
- en: Reinforcement learning is most useful in applications where defining the exact
    sequence of steps required to complete the task is difficult or impossible due
    to the complexity and variability of the environment. The only thing you need
    for RL to work is to be able to define what is useful behavior and what behavior
    should be discouraged.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习在定义完成任务所需的精确步骤序列由于环境的复杂性和可变性而困难或不可能的应用中最为有用。你需要RL工作的唯一条件是能够定义什么是有用的行为，以及应该避免什么行为。
- en: '**The final Environment method —** `**step**`**.** With the each component
    of `Environment` in place we can now define the heart of the interaction between
    the agent and the environment.'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: '**最终的环境方法——**`**step**`**。** 现在我们可以定义代理和环境之间交互的核心，因为`Environment`的每个组件都到位了。'
- en: Thankfully, it is quite simple.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，这非常简单。
- en: '[PRE34]'
  id: totrans-294
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: '`step` first moves the agent in the environment and records `reward` and `done`.
    Then it gets the state immediately following this interaction, `next_state`. Then
    if `render_on` is set to true the grid is rendered.'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: '`step`首先在环境中移动代理并记录`reward`和`done`。然后它获取此交互之后的状态，`next_state`。然后如果`render_on`设置为true，则会渲染网格。'
- en: Finally, `step` returns the recorded values, `reward`, `next_state` and `done`.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，`step`返回记录的值，`reward`、`next_state`和`done`。
- en: These will be essential to building the experiences our agent will learn from.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 这些将是构建我们代理将从中学习的经验的重要组成部分。
- en: Congratulations! You have officially completed the construction of the environment
    for your DRL gym.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜！你已经正式完成了你的DRL健身环境的构建。
- en: '**Below is the completed** `**Environment**` **class.**'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: '**下面是完成的**`**Environment**`**类。**'
- en: '[PRE35]'
  id: totrans-300
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: We have gone through a lot at this point. It may be beneficial to return to
    [the big picture](#16af) at the beginning and reevaluate how each part interacts
    using your new knowledge before moving on.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止我们已经讨论了很多内容。返回到[全局视图](#16af)并使用你的新知识重新评估每部分的互动可能会很有益，然后再继续前进。
- en: '6\. Learn From Experiences: Experience Replay'
  id: totrans-302
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6. 从经验中学习：经验回放
- en: The agent’s model and policy, along with the environment’s reward structure
    and mechanism for taking steps have all been completed, but we need some way to
    remember the past so that the agent can learn from it.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 代理的模型和策略，以及环境的奖励结构和采取步骤的机制都已经完成，但我们需要某种方式来记住过去，以便代理能够从中学习。
- en: This can be done by saving the experiences.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 这可以通过保存经验来实现。
- en: 'Each experience consists of a few things:'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 每个经验都包括几项内容：
- en: '**State**: The state before an action is taken.'
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**状态**：在采取行动之前的状态。'
- en: '**Action**: What action was taken in this state.'
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**行动**：在这个状态下采取了什么行动。'
- en: '**Reward**: Positive or negative feedback the agent received from the environment
    based on its action.'
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**奖励**：代理根据其行动从环境中获得的正面或负面反馈。'
- en: '**Next State**: The state immediately following the action, allowing the agent
    to act, not just based on the consequences of the current state, but many states
    in advance.'
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**下一状态**：紧跟动作之后的状态，使代理能够不仅仅基于当前状态的结果行动，而是基于多个状态的提前信息。'
- en: '**Done**: Indicates the end of an experience, letting the agent know if the
    task has been completed or not. It can be either true or false at each step.'
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**完成**：表示一个经验的结束，让代理知道任务是否已完成。它在每一步可以是 true 或 false。'
- en: '*These terms should not be new to you, but it never hurts to see them again!*'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: '*这些术语你应该不陌生，但再看一遍也无妨！*'
- en: Each experience is associated with exactly one step from the agent. This will
    provide all of the context needed to train it.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 每个经验都与代理的一个步骤相关联。这将提供训练所需的全部上下文。
- en: '**The** `**ExperienceReplay**` **class**'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: '**`ExperienceReplay` 类**'
- en: To keep track of and serve these experiences when needed, we will define one
    last class, `ExperienceReplay`.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 为了跟踪并在需要时提供这些经验，我们将定义最后一个类，`ExperienceReplay`。
- en: '[PRE36]'
  id: totrans-315
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: This class will take `capacity`, an integer value that defines the maximum number
    of experiences we will save at a time, and `batch_size`, an integer value that
    determines how many experiences we sample at a time for training.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 该类将接受 `capacity`，一个定义我们一次保存的最大经验数量的整数值，以及 `batch_size`，一个决定我们每次为训练采样多少经验的整数值。
- en: '**Batching the experiences** If you remember, the neural network in the `Agent`
    class takes batches of input. While we only used a batch of size one to predict,
    this would be incredibly inefficient for training. Typically, batches of size
    32 or higher are more common.'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: '**批处理经验** 如果你还记得，`Agent` 类中的神经网络接受输入批次。虽然我们只用一个大小为一的批次进行预测，但这对于训练来说效率极低。通常，批次大小为
    32 或更大的情况更为常见。'
- en: 'Batching the input for training does two things:'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 批处理输入进行训练有两个作用：
- en: Increases efficiency because it allows for parallel processing of multiple data
    points, reducing computational overhead and making better use of GPU or CPU resources.
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提高了效率，因为它允许并行处理多个数据点，减少计算开销，并更好地利用 GPU 或 CPU 资源。
- en: Helps the model learn more consistently, as it’s learning from a variety of
    examples at once, which can make it better at handling new, unseen data.
  id: totrans-320
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 帮助模型更一致地学习，因为它一次学习来自多种示例的内容，这可以提高其处理新数据的能力。
- en: '**Memory** The `memory` will be a deque (short for double-ended queue). This
    allows us to add new experiences to the front, and as the max length defined by
    `capacity` is reached, the deque will remove them without having to shift each
    element as you would with a Python list. This can greatly improve speed when `capacity`
    is set to 10,000 or more.'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: '**内存** `memory` 将是一个双端队列（deque）。这允许我们将新经验添加到前面，并且当达到由 `capacity` 定义的最大长度时，双端队列将删除它们，而不需要像
    Python 列表那样移动每个元素。这在 `capacity` 设置为 10,000 或更多时可以大大提高速度。'
- en: '**Experience** Each experience will be defined as a `namedtuple`. Although,
    many other data structures would work, this will improve readability as we extract
    each part as needed in training.'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: '**经验** 每个经验将被定义为一个 `namedtuple`。虽然许多其他数据结构也可以，但这将提高可读性，因为我们在训练时按需提取每一部分。'
- en: '`**add_experience**` **and** `**sample_batch**` **implementation** Adding a
    new experience and sampling a batch are rather straightforward.'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: '`**add_experience**` **和** `**sample_batch**` **实现** 添加新经验和采样批次是相当直接的。'
- en: '[PRE37]'
  id: totrans-324
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: The method `add_experience` creates a `namedtuple` with each part of an experience,
    `state`, `action`, `reward`, `next_state`, and `done`, and appends it to `memory`.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 方法 `add_experience` 创建一个 `namedtuple`，包含经验的每一部分：`state`、`action`、`reward`、`next_state`
    和 `done`，并将其附加到 `memory` 中。
- en: '`sample_batch` is just as simple. It gets and returns a random sample from
    `memory` of size `batch_size`.'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: '`sample_batch` 同样简单。它从 `memory` 中获取并返回一个大小为 `batch_size` 的随机样本。'
- en: '![](../Images/36c64b8dcf5ffa085576f0b2b77e98e5.png)'
  id: totrans-327
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/36c64b8dcf5ffa085576f0b2b77e98e5.png)'
- en: Experience Replay storing experiences for Agent to batch and learn from — Image
    by author
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 经验回放用于存储代理的经验以便批量处理和学习 — 图像来源于作者
- en: '**The last method needed —** `**can_provide_sample**` Finally, it would be
    useful to be able to check if `memory` contains enough experiences to provide
    us with a full sample before attempting to get a batch for training.'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: '**最后一个方法** — `**can_provide_sample**` 最终，能够检查 `memory` 是否包含足够的经验以提供完整的样本，将在尝试获取训练批次之前非常有用。'
- en: '[PRE38]'
  id: totrans-330
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: '**Completed** `**ExperienceReplay**` **class…**'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: '**完成** `**ExperienceReplay**` **类…**'
- en: '[PRE39]'
  id: totrans-332
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: With the mechanism for saving each experience and sampling from them in place,
    we can return to the `Agent` class to finally enable learning.
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 在保存每个经验和从中抽样的机制到位后，我们可以返回到`Agent`类，以最终启用学习。
- en: '7\. Define The Agent’s Learning Process: Fitting The NN'
  id: totrans-334
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7\. 定义代理的学习过程：调整神经网络
- en: The goal, when training the neural network, is to get the Q-values it produces
    to accurately represent the future reward each choice will provide.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 训练神经网络的目标是使其产生的Q值准确地代表每个选择将提供的未来奖励。
- en: Essentially, we want the network to learn to predict how valuable each decision
    is, considering not just the immediate reward, but also the rewards it could lead
    to in the future.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 本质上，我们希望网络学习预测每个决策的价值，不仅考虑即时奖励，还要考虑可能带来的未来奖励。
- en: '**Incorporating future rewards** To achieve this, we incorporate the Q-values
    of the subsequent state into the training process.'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: '**纳入未来奖励** 为实现这一点，我们将后续状态的Q值纳入训练过程。'
- en: When the agent takes an action and moves to a new state, we look at the Q-values
    in this new state to help inform the value of the previous action. In other words,
    the potential future rewards influence the perceived value of the current choices.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 当代理采取行动并移动到新状态时，我们查看这个新状态中的Q值，以帮助确定先前行动的价值。换句话说，潜在的未来奖励会影响当前选择的感知价值。
- en: '**The** `**learn**` **method**'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: '**`**learn**` 方法**'
- en: '[PRE40]'
  id: totrans-340
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: Using the provided batch, `experiences`, we will extract each part using list
    comprehension and the `namedtuple` values we defined earlier in `ExperienceReplay`.
    Then we convert each one into a NumPy array to improve efficiency and to align
    with what the model expects, as explained previously.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 使用提供的批量数据`experiences`，我们将通过列表推导和之前在`ExperienceReplay`中定义的`namedtuple`值提取每一部分。然后我们将每个部分转换为NumPy数组，以提高效率并与模型的预期一致，如前所述。
- en: Finally, we use the model to predict the Q-values of the current state the action
    was taken in and the state immediately following it.
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们使用模型预测在当前状态下采取行动的Q值以及紧接着的状态。
- en: Before continuing with the `learn` method, I need to explain something called
    the discount factor.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 在继续`learn`方法之前，我需要解释一下折扣因子的概念。
- en: '**Discounting future rewards — the role of gamma** Intuitively, we know that
    immediate rewards are generally prioritized when all else is equal. (Would you
    like your paycheck today or next week?)'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: '**折扣未来奖励——gamma的作用** 直观地说，我们知道在其他条件相同的情况下，立即奖励通常会被优先考虑。（你希望今天还是下周拿到工资？）'
- en: Representing this mathematically can seem much less intuitive. When considering
    the future, we don’t want it to be equally important (weighted) as the present.
    By how much we discount the future, or lower its effect on each decision, is defined
    by gamma (commonly denoted by the greek letter γ).
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 从数学上表示这一点可能显得不太直观。考虑到未来，我们不希望它与现在同等重要（加权）。折扣未来的程度，即每个决策的影响降低程度，由gamma（通常用希腊字母γ表示）定义。
- en: Gamma can be adjusted, with higher values encouraging planning and lower values
    encouraging more short sighted behavior. We will use a default value of 0.99.
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: Gamma可以进行调整，较高的值鼓励规划，较低的值则鼓励更短视的行为。我们将使用默认值0.99。
- en: '*The discount factor will pretty much always be between 0 and 1\. A discount
    factor greater than 1, prioritizing the future over the present, would introduce
    unstable behavior and has little to no practical applications.*'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: '*折扣因子通常在0和1之间。大于1的折扣因子会优先考虑未来而非现在，这会引入不稳定的行为，实际应用很少。*'
- en: '**Implementing gamma and defining the target Q-values** Recall that in the
    context of training a neural network, the process hinges on two key elements:
    the input data we provide and the corresponding outputs we want the network to
    learn to predict.'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: '**实现gamma和定义目标Q值** 记住，在训练神经网络的背景下，这一过程依赖于两个关键要素：我们提供的输入数据和我们希望网络学习预测的对应输出。'
- en: We will need to provide the network with some target Q-values that are updated
    based on the reward given by the environment at this specific state and action,
    plus the discounted (by gamma) predicted reward of the best action at the next
    state.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要向网络提供一些目标Q值，这些Q值是基于环境在特定状态和行动下给予的奖励，以及下一个状态中最佳行动的折扣（由gamma折扣）预测奖励更新的。
- en: I know that is a lot to take in, but it will be best explained through implementation
    and example.
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 我知道这可能很难理解，但通过实现和示例会更好地解释。
- en: '[PRE41]'
  id: totrans-351
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: We’ve defined the class attribute, `gamma`, with a default value of 0.99.
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经定义了类属性`gamma`，其默认值为0.99。
- en: Then, after getting the prediction for `state` and `next_state` that we implemented
    above, we initialize `target_q_values` to the current Q-values. These will be
    updated in the following loop.
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，在获取我们上面实现的`state`和`next_state`的预测后，我们将`target_q_values`初始化为当前的Q值。这些将在以下循环中更新。
- en: '**Updating** `**target_q_values**` We loop through each `experience` in the
    batch with two cases for updating the values:'
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: '**更新** `**target_q_values**` 我们遍历批次中的每个`experience`，有两种情况来更新这些值：'
- en: If the episode is `done`, the `target_q_value` for that action is simply the
    reward given because there is no relevant `next_q_value`.
  id: totrans-355
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果回合已`done`，则该动作的`target_q_value`仅仅是给定的奖励，因为没有相关的`next_q_value`。
- en: Otherwise, the episode is not `done`, and the `target_q_value` for that action
    becomes the reward given, plus the discounted Q-value of the predicted next action
    in `next_q_values`.
  id: totrans-356
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 否则，回合尚未`done`，该动作的`target_q_value`变为给定的奖励，加上`next_q_values`中预测的下一个动作的折扣Q值。
- en: 'Update if `done` is true:'
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 如果`done`为真，则更新：
- en: '[PRE42]'
  id: totrans-358
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Update if `done` is false:'
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 如果`done`为假，则更新：
- en: '[PRE43]'
  id: totrans-360
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: The syntax here, `target_q_values[i, actions[i]]`, can seem confusing but it’s
    essentially the Q-value of the i-th experience, for the action `actions[i]`.
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的语法`target_q_values[i, actions[i]]`可能看起来令人困惑，但它本质上是第i个经验的Q值，对于动作`actions[i]`。
- en: '[PRE44]'
  id: totrans-362
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: '*This is NumPy’s equivalent to* `*[i][actions[i]]*` *in Python lists. Remember
    each action is an index (0 to 3).*'
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: '*这相当于 NumPy 中的* `*[i][actions[i]]*` *在 Python 列表中。记住每个动作是一个索引（0 到 3）。*'
- en: '**How** `**target_q_values**` **is updated**'
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: '**如何** `**target_q_values**` **被更新**'
- en: Just to illustrate this more clearly I will show how `target_q_values` more
    closely aligns with the actual rewards given as we train. Remember that we are
    working with a batch. This will be a batch of three with example values for simplicity.
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更清楚地说明这一点，我将展示`target_q_values`如何更紧密地与实际奖励对齐，随着训练的进行。记住我们在处理一个批次。这将是一个简单的三个样本的批次。
- en: Also, ensure that you understand that the entries in `experiences` are independent.
    Meaning this is not a sequence of steps, but a random sample from a collection
    of individual experiences.
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 另外，确保你理解`experiences`中的条目是独立的。这意味着这不是一个步骤序列，而是从一组独立经验中随机抽取的样本。
- en: Pretend the values of `actions`, `rewards`, `dones`, `current_q_values`, and
    `next_q_values` are as follows.
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 假设`actions`、`rewards`、`dones`、`current_q_values`和`next_q_values`的值如下。
- en: '[PRE45]'
  id: totrans-368
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: We then copy `current_q_values` into `target_q_values` to be updated.
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们将`current_q_values`复制到`target_q_values`中进行更新。
- en: '[PRE46]'
  id: totrans-370
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: Then, for every experience in the batch we can show the associated values.
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，对于批次中的每个经验，我们可以展示相关的值。
- en: '*This is not code, but simply an example of the values at each stage. If you
    get lost, be sure to refer back to the initial values to see where each is coming
    from.*'
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: '*这不是代码，而只是每个阶段值的示例。如果你迷失了，确保回到初始值查看每个值的来源。*'
- en: '**Entry 1**'
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: '**条目 1**'
- en: '[PRE47]'
  id: totrans-374
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: Because `dones[i]` is false for this experience we need to consider the `next_q_values`
    and apply `gamma` (0.99).
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 因为这个经验的`dones[i]`为假，我们需要考虑`next_q_values`并应用`gamma`（0.99）。
- en: '[PRE48]'
  id: totrans-376
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: Why get the largest of `next_q_values[i]`? Because that would be the next action
    chosen and we want the estimated reward (Q-value).
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么获取`next_q_values[i]`的最大值？因为那将是下一个选择的动作，我们需要估计的奖励（Q值）。
- en: Then we update the i-th `target_q_values` at the index corresponding to `actions[i]`
    to the reward for this state/action pair plus the discounted reward for the next
    state/action pair.
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们在索引对应于`actions[i]`的`target_q_values`中，将其更新为该状态/动作对的奖励加上下一个状态/动作对的折扣奖励。
- en: Here are the target values in this experience after being updated.
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: 这是该经验在更新后的目标值。
- en: '[PRE49]'
  id: totrans-380
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: As you can see, for the current state, choosing 1 (down) is now even more desirable
    because the value is higher and this behavior has been reinforced.
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，对于当前状态，选择 1（向下）现在更具吸引力，因为值更高且这种行为已经被强化。
- en: '*It may help to calculate these yourself to really make it clear.*'
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: '*自己计算这些可能有助于真正弄清楚。*'
- en: '**Entry 2**'
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: '**条目 2**'
- en: '[PRE50]'
  id: totrans-384
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: '`dones[i]` is also false here, so we do need to consider the `next_q_values`.'
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: '`dones[i]`在这里也是假的，因此我们需要考虑`next_q_values`。'
- en: '[PRE51]'
  id: totrans-386
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: Again, updating the i-th experience’s `target_q_values` at the index `actions[i]`.
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，在索引`actions[i]`处更新第i个经验的`target_q_values`。
- en: '[PRE52]'
  id: totrans-388
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: Choosing 2 (left) is now less desirable because the Q-value is lower and this
    behavior is discouraged.
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: 选择 2（向左）现在不再那么理想，因为Q值较低且这种行为被抑制。
- en: '**Entry 3**'
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: '**条目 3**'
- en: Finally, the last entry in the batch.
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: 最后的条目在这一批中。
- en: '[PRE53]'
  id: totrans-392
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: '`dones[i]` for this entry is true, indicating that the episode is complete
    and there will be no further actions taken. This means we do not consider `next_q_values`
    in our update.'
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: 这个条目的`dones[i]`为真，表示这一轮已完成，不会再采取进一步的行动。这意味着我们在更新时不考虑`next_q_values`。
- en: '[PRE54]'
  id: totrans-394
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: Notice that we simply set `target_q_values[i, action[i]]` to the value of `rewards[i]`,
    because no more actions will be taken — there is no future to consider.
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: 注意我们只是将`target_q_values[i, action[i]]`设置为`rewards[i]`的值，因为不会再有更多的行动 — 没有未来需要考虑。
- en: '[PRE55]'
  id: totrans-396
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: Choosing 2 (left) in this and similar states will now be much more desirable.
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种及类似状态中选择2（左）现在会更具吸引力。
- en: This is the state where the goal was to the left of the agent, so when that
    action was chosen the full reward was given.
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: 这是目标在智能体左侧的状态，因此当选择那个行动时，给予了全部奖励。
- en: Although it can seem rather confusing, the idea is simply to make updated Q-values
    that accurately represent the rewards given by the environment to provide to the
    neural network. That is what the NN is supposed to approximate.
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管它可能看起来相当令人困惑，但这个想法只是为了制作准确表示环境给予的奖励的更新Q值，以便提供给神经网络。这就是神经网络需要近似的内容。
- en: Try to imagine it in reverse. Because the reward for reaching the goal is substantial,
    it will create a propagation effect throughout the states leading to the one where
    the agent achieves the goal. This is the power of gamma in considering the next
    state and its role in the rippling of reward values backward through the state-space.
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: 尝试反向思考。由于到达目标的奖励相当可观，它将在状态中创建传播效应，最终到达智能体实现目标的状态。这就是gamma在考虑下一个状态及其在状态空间中奖励值向后传播的作用的力量。
- en: '![](../Images/1e705737fe068437dec911c5df971487.png)'
  id: totrans-401
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1e705737fe068437dec911c5df971487.png)'
- en: Rippling effect of rewards across the state-space — Image by author
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: 奖励在状态空间中的波及效应 — 作者提供的图片
- en: Above is a simplified version of the Q-values and the effect of the discount
    factor, only considering the reward for the goal, not the incremental rewards
    or penalties.
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: 上面是Q值和折扣因子的简化版本，仅考虑目标的奖励，而不考虑增量奖励或惩罚。
- en: Pick any cell in the grid and move to the highest quality adjacent cell. You
    will see that it always provides an optimal path to the goal.
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: 选择网格中的任何一个单元格，并移动到质量最高的相邻单元格。你会发现它总是提供到达目标的最佳路径。
- en: '*This effect is not immediate. It requires the agent to explore the state and
    action-space to gradually learn and adjust its strategy, building an understanding
    of how different actions lead to varying rewards over time.*'
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: '*这一效果不是立竿见影的。它需要智能体探索状态和行动空间，逐渐学习和调整策略，建立对不同行动如何导致不同奖励的理解。*'
- en: If the reward structure was carefully crafted, this will slowly guide our agent
    towards taking more advantageous actions.
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: 如果奖励结构经过精心设计，这将慢慢引导我们的智能体采取更有利的行动。
- en: '**Fitting the neural network** For the `learn` method, the last thing there
    is to do is provide the agent’s neural network with `states` and their associated
    `target_q_values`. TensorFlow will then handle updating the weights to more closely
    predict these values on similar states.'
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: '**拟合神经网络** 对于`learn`方法，最后需要做的是将智能体的神经网络与`states`及其相关的`target_q_values`配对。TensorFlow将处理权重的更新，使其更准确地预测类似状态下的这些值。'
- en: '[PRE56]'
  id: totrans-408
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'The only new part is `self.model.fit(states, target_q_values, epochs=1, verbose=0)`.
    `fit` takes two main arguments: the input data and the target values we want.
    In this case, our input is a batch `states` and the target values are the updated
    Q-values for each state.'
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: 唯一的新部分是`self.model.fit(states, target_q_values, epochs=1, verbose=0)`。`fit`有两个主要参数：输入数据和我们想要的目标值。在这种情况下，我们的输入是一批`states`，目标值是每个状态的更新Q值。
- en: '`epochs=1` simply sets the number of times you want the network to try to fit
    to the data. One is enough because we want it to be able to generalize well, not
    to fit to this specific batch. `verbose=0` simply tells TensorFlow not to print
    debug messages like progress bars.'
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: '`epochs=1`只是设置你希望网络尝试拟合数据的次数。一个就足够了，因为我们希望它能够很好地泛化，而不是拟合到这个特定的批次。`verbose=0`只是告诉TensorFlow不要打印类似进度条的调试信息。'
- en: The `Agent` class is now equipped with the ability to learn from experiences
    but it needs two more simple methods — `save` and `load`.
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: '`Agent`类现在具备了从经验中学习的能力，但它还需要两个简单的方法 — `save`和`load`。'
- en: '**Saving and loading trained models** Saving and loading the model prevents
    us from having to completely retrain every time we need it. We can use the simple
    TensorFlow methods that only take one argument, `file_path`.'
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: '**保存和加载训练好的模型** 保存和加载模型可以防止我们每次需要时都进行完全的重训练。我们可以使用只需一个参数`file_path`的简单TensorFlow方法。'
- en: '[PRE57]'
  id: totrans-413
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: Make a directory called models, or whatever you like, and then you can save
    your trained model at set intervals. These files end in .h5\. So whenever you
    want to save your model you simply call `agent.save(‘models/model_name.h5’)`.
    The same goes for when you want to load one.
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个名为models的目录，或者其他你喜欢的名字，然后你可以在设定的间隔保存训练好的模型。这些文件以.h5结尾。所以每当你想要保存模型时，只需调用`agent.save('models/model_name.h5')`。加载模型时也是如此。
- en: '**Full** `**Agent**` **class**'
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: '**完整** `**Agent**` **类**'
- en: '[PRE58]'
  id: totrans-416
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: Each class of your deep reinforcement learning gym is now complete! You have
    successfully coded `Agent,` `Environment`, and `ExperienceReplay`. The only thing
    left is the main training loop.
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: 你的深度强化学习环境的每个类现在都完成了！你已经成功地编码了`Agent`、`Environment`和`ExperienceReplay`。剩下的唯一任务就是主训练循环。
- en: '8\. Executing The Training Loop: Putting It All Together'
  id: totrans-418
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8\. 执行训练循环：将所有部分整合在一起
- en: We are at the final stretch of the project! Every piece we have coded, `Agent`,
    `Environment`, and `ExperienceReplay`, needs some way to interact.
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已进入项目的最后阶段！我们编码的每一部分，`Agent`、`Environment`和`ExperienceReplay`，都需要某种交互方式。
- en: This will be the main program where each episode is run and where we define
    our hyper-parameters like `epsilon`.
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: 这将是主要程序，其中每个回合都会运行，并且我们定义像`epsilon`这样的超参数。
- en: Although it is fairly simple, I will break up each part as we code it to make
    it more clear.
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然它相当简单，但我会在编码时将每一部分拆开，以便更加清晰。
- en: '**Initialize each part** First, we set `grid_size` and use the classes we have
    made to initialize each instance.'
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: '**初始化每一部分** 首先，我们设置`grid_size`并使用我们创建的类来初始化每个实例。'
- en: '[PRE59]'
  id: totrans-423
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: Now we have each part we need for the main training loop.
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经准备好主训练循环所需的每一部分。
- en: '**Episode and step cap** Next, we will define the number of episodes we want
    the training to run, and the max number of steps allowed in each episode.'
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: '**回合数和步骤限制** 接下来，我们将定义训练中要运行的回合数和每个回合允许的最大步骤数。'
- en: Capping the number of steps helps ensure our agent doesn’t get stuck in a loop
    and encourages shorter paths. We will be fairly generous and for a 5x5 we will
    set the max to 200\. This will need to be increased for larger environments.
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: 限制步骤数有助于确保我们的代理不会陷入循环，并鼓励较短的路径。我们会相当慷慨地为5x5设置最大值为200。对于较大的环境，这个值需要增加。
- en: '[PRE60]'
  id: totrans-427
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: '**Episode loop** In each episode we will reset `environment` and save the initial
    `state`. Then we perform each step until either `done` is true or `max_steps`
    is reached. Finally, we save the model. The logic for each step has not been implemented
    quite yet.'
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: '**回合循环** 在每个回合中，我们将重置`environment`并保存初始`state`。然后，我们执行每一步，直到`done`为真或达到`max_steps`。最后，我们保存模型。每一步的逻辑尚未完全实现。'
- en: '[PRE61]'
  id: totrans-429
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: Notice we name the model using `grid_size` because the NN architecture will
    be different for each input size. Trying to load a 5x5 model into a 10x10 architecture
    will throw an error.
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们使用`grid_size`来命名模型，因为神经网络架构会因每个输入大小而异。尝试将5x5的模型加载到10x10的架构中将会导致错误。
- en: '**Step logic** Finally, inside of the step loop we will lay out the interaction
    between each piece as discussed before.'
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤逻辑** 最终，在步骤循环内部，我们将按照之前讨论的方式安排各部分之间的交互。'
- en: '[PRE62]'
  id: totrans-432
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: For every step of the episode, we start by printing the episode and step number
    to give us some information about where we are in training. Additionally, you
    can print `epsilon` to see what percentage of the agent’s actions are random.
    It also helps because if you want to stop for any reason you can restart the agent
    at the same `epsilon` value.
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个回合的每一步，我们首先打印回合数和步骤数，以便获得关于训练进度的信息。此外，你可以打印`epsilon`以查看代理动作的随机性百分比。这也有帮助，因为如果你想要停止，可以在相同的`epsilon`值下重新启动代理。
- en: After printing the information, we use the `agent` policy to get `action` from
    this `state` to take a step in `environment`, recording the returned values.
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
  zh: 在打印信息后，我们使用`agent`策略从这个`state`中获取`action`，在`environment`中执行一步，并记录返回的值。
- en: Then we save `state`, `action`, `reward`, `next_state`, and `done` as an experience.
    If `experience_replay` has enough memory we train `agent` on a random batch of
    `experiences`.
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们将`state`、`action`、`reward`、`next_state`和`done`保存为经验。如果`experience_replay`有足够的内存，我们将对`agent`进行随机经验批次训练。
- en: Finally, we set `state` to `next_state` and check if the episode is `done`.
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将`state`设置为`next_state`，并检查这一回合是否`done`。
- en: Once you’ve run at least one episode you’ll have a model saved you can load
    and either continue where you left off or evaluate the performance.
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你运行了至少一个回合，你将会有一个保存的模型，可以加载并继续之前的操作或评估性能。
- en: After you initialize `agent` simply use its load method similar to how we saved
    — `agent.load(f’models/model_{grid_size}.h5')`
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
  zh: 初始化`agent`后，只需使用它的加载方法，类似于我们保存时的操作 — `agent.load(f’models/model_{grid_size}.h5')`
- en: You can also add a slight delay at each step when you are evaluating the model
    using time — `time.sleep(0.5)`. This causes each step to pause for half a second.
    Make sure you include `import time`.
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
  zh: 你还可以在每一步中添加一个小的延迟，当你使用时间评估模型时 — `time.sleep(0.5)`。这会让每一步暂停半秒钟。确保包括`import time`。
- en: '**Completed training loop**'
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
  zh: '**完成训练循环**'
- en: '[PRE63]'
  id: totrans-441
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: When you need `time.sleep` or `agent.load` you can simply uncomment them.
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
  zh: 当你需要`time.sleep`或`agent.load`时，只需取消注释它们即可。
- en: '**Running the program** Give it a run! You should be able to successfully train
    the agent to complete the goal up to an 8x8 or so grid environment. Any grid size
    much larger than this and the training begins to struggle.'
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
  zh: '**运行程序** 试运行一下！你应该能够成功训练智能体完成一个大约8x8的网格环境。如果网格大小远大于此，训练会变得困难。'
- en: Try to see how large you can get the environment. You can do a few things such
    as adding layers and neurons to the neural network, changing `epsilon_decay`,
    or giving more time to train. Doing this can solidify your understanding of each
    part.
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
  zh: 尝试看看你可以让环境变得多大。你可以做一些事情，比如向神经网络添加层和神经元、更改`epsilon_decay`，或给予更多的训练时间。这样做可以巩固你对每个部分的理解。
- en: '*For instance, you may notice* `*epsilon*` *reaches* `*epsilon_end*` *rather
    fast. Don’t be afraid to change the* `*epsilon_decay*` *to values of 0.9998 or
    0.99998 if you would like.*'
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
  zh: '*例如，你可能会注意到* `*epsilon*` *很快就达到了* `*epsilon_end*` *。如果你愿意，可以将* `*epsilon_decay*`
    *更改为0.9998或0.99998。*'
- en: As the grid size grows, the state the network is fed gets exponentially larger.
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
  zh: 随着网格大小的增加，网络接收到的状态会呈指数增长。
- en: I’ve included a short bonus section at the end to fix this and to demonstrate
    that there are many ways you can represent the environment for the agent.
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
  zh: 我在最后添加了一个简短的附加部分，修复了这个问题，并演示了有许多方法可以为智能体表示环境。
- en: '**9\. Wrapping It Up**'
  id: totrans-448
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**9\. 总结**'
- en: Congratulations on completing this comprehensive journey through the world of
    Reinforcement and Deep Q-Learning!
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜你完成了对强化学习和深度Q学习世界的全面探索！
- en: Although there is always more to cover, you could walk away having acquired
    important insights and skills.
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管总有更多内容可以覆盖，你仍然可以获得重要的见解和技能。
- en: 'In this guide you:'
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
  zh: 在本指南中，你：
- en: Were introduced to the core concepts of reinforcement learning and why it’s
    a crucial area in AI.
  id: totrans-452
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 介绍了强化学习的核心概念以及为什么它在人工智能中至关重要。
- en: Built a simple environment, laying the groundwork for agent interaction and
    learning.
  id: totrans-453
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建了一个简单的环境，为智能体互动和学习奠定了基础。
- en: Defined the agent’s Neural Network architecture for use with Deep Q-Learning,
    enabling your agent to make decisions in more complex environments than traditional
    Q-Learning.
  id: totrans-454
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 定义了用于深度Q学习的智能体神经网络架构，使你的智能体能够在比传统Q学习更复杂的环境中做出决策。
- en: Understood why exploration is important before exploiting the learned strategy
    and implemented the Epsilon-Greedy policy.
  id: totrans-455
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解了探索在利用学习策略之前的重要性，并实现了Epsilon-Greedy策略。
- en: Implemented the reward system to guide the agent to the goal and learned the
    differences between sparse and dense rewards.
  id: totrans-456
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现了奖励系统以引导智能体达到目标，并学习了稀疏奖励和密集奖励之间的区别。
- en: Designed the experience replay mechanism, allowing the agent to learn from past
    experiences.
  id: totrans-457
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设计了经验回放机制，让智能体能够从过去的经验中学习。
- en: Gained hands-on experience in fitting the neural network, a critical process
    where the agent improves its performance based on feedback from the environment.
  id: totrans-458
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 获得了在拟合神经网络中的实际操作经验，这是一个关键过程，智能体根据环境反馈改进其性能。
- en: Put all these pieces together in a training loop, witnessing the agent’s learning
    process in action and tweaking it for optimal performance.
  id: totrans-459
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将所有这些部分结合在一个训练循环中，观察智能体的学习过程并进行调整，以获得最佳性能。
- en: By now, you should feel confident in your understanding of Reinforcement Learning
    and Deep Q-Learning. You’ve built a solid foundation, not just in theory but also
    in practical application, by constructing a DRL gym from scratch.
  id: totrans-460
  prefs: []
  type: TYPE_NORMAL
  zh: 到现在为止，你应该对强化学习和深度Q学习有了信心。通过从头构建一个DRL环境，你不仅在理论上建立了坚实的基础，而且在实际应用中也得到了锻炼。
- en: This knowledge equips you to tackle more complex RL problems and paves the way
    for further exploration in this exciting field of AI.
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
  zh: 这些知识使你能够处理更复杂的RL问题，并为进一步探索这个激动人心的AI领域铺平了道路。
- en: '![](../Images/782a7e8b27260d7f702d6be467400053.png)'
  id: totrans-462
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/782a7e8b27260d7f702d6be467400053.png)'
- en: Agar.io inspired game where agents are encouraged to eat one another to win
    — GIF by author
  id: totrans-463
  prefs: []
  type: TYPE_NORMAL
  zh: Agar.io风格的游戏，其中代理被鼓励相互吞噬以获胜——作者制作的GIF
- en: Above is a grid game inspired by Agar.io where agents are encouraged to grow
    in size, often from eating one another. At each step the environment was plotted
    on a graph using the Python library, [Matplotlib](https://matplotlib.org/). The
    boxes around the agents are their field of view. This is fed to them as their
    state from the environment as a flattened grid, similar to what we’ve done in
    our system.
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
  zh: 上面是一个受Agar.io启发的网格游戏，其中代理被鼓励通过相互吞噬来增大体积。在每一步，环境都会使用Python库[Matplotlib](https://matplotlib.org/)绘制在图上。围绕代理的框是它们的视野。这些作为环境中的状态以平铺网格的形式提供给它们，类似于我们在系统中所做的。
- en: Games like this, and a myriad of other uses, can be crafted with simple modifications
    to what you have made here.
  id: totrans-465
  prefs: []
  type: TYPE_NORMAL
  zh: 像这样的游戏以及其他许多应用，可以通过对你在这里制作的内容进行简单修改来实现。
- en: Remember though, Deep Q-Learning is only suitable for a discrete action-space
    — one that has a finite number of distinct actions. For a continuous action-space,
    like in a physics based environment, you will need to explore other methods in
    the world of DRL.
  id: totrans-466
  prefs: []
  type: TYPE_NORMAL
  zh: 但要记住，深度Q学习仅适用于离散的动作空间——即具有有限数量的不同动作的空间。对于连续的动作空间，如在基于物理的环境中，你需要探索DRL世界中的其他方法。
- en: '10\. Bonus: Optimize State Representation'
  id: totrans-467
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 10. 附加：优化状态表示
- en: Believe it or not, the way we have currently been representing state is not
    the most optimal for this use.
  id: totrans-468
  prefs: []
  type: TYPE_NORMAL
  zh: 不管你信不信，我们目前表示状态的方式并不是最优的。
- en: It is actually incredibly inefficient.
  id: totrans-469
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，这种方法非常低效。
- en: For a grid of 100x100 there are 99,990,000 possible states. Not only would the
    model need to be quite large considering the size of the input — 10,000 values,
    it would require a significant volume of training data. Depending on the computational
    resources one has available this could take days or weeks.
  id: totrans-470
  prefs: []
  type: TYPE_NORMAL
  zh: 对于100x100的网格，有99,990,000种可能的状态。考虑到输入的规模——10,000个值，模型不仅需要非常大，还需要大量的训练数据。根据可用的计算资源，这可能需要几天或几周。
- en: Another downfall is flexibility. The model currently is stuck at one grid size.
    If you want to use a different sized grid, you need to train another model completely
    from scratch.
  id: totrans-471
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个缺点是灵活性。模型目前被固定在一个网格大小。如果你想使用不同大小的网格，你需要从头训练另一个模型。
- en: We need a way to represent the state that significantly reduces the state-space
    and translates well to any grid size.
  id: totrans-472
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要一种表示状态的方法，这种方法能显著减少状态空间，并且适用于任何网格大小。
- en: '**The better way** While there are several ways to do this, the simplest, and
    probably most effective, is to use the relative distance from the goal.'
  id: totrans-473
  prefs: []
  type: TYPE_NORMAL
  zh: '**更好的方法** 尽管有几种方法可以做到这一点，最简单且可能最有效的方法是使用相对于目标的距离。'
- en: 'Rather than the state for a 5x5 grid looking like this:'
  id: totrans-474
  prefs: []
  type: TYPE_NORMAL
  zh: 而不是像这样表示5x5网格的状态：
- en: '[PRE64]'
  id: totrans-475
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'It can be represented with only two values:'
  id: totrans-476
  prefs: []
  type: TYPE_NORMAL
  zh: 它可以用两个值来表示：
- en: '[PRE65]'
  id: totrans-477
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: Using this method would lower the state-space of a 100x100 grid from 99,990,000
    to 39,601!
  id: totrans-478
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这种方法可以将100x100网格的状态空间从99,990,000减少到39,601！
- en: Not only that, but it can generalize much better. It simply has to learn that
    moving down is the right choice when the first value is negative, and moving right
    is appropriate when the second value is negative, with the opposite actions applying
    for positive values.
  id: totrans-479
  prefs: []
  type: TYPE_NORMAL
  zh: 不仅如此，它的泛化能力也更强。它只需学会当第一个值为负时，向下移动是正确的选择，而当第二个值为负时，向右移动是合适的选择，正值的情况则相反。
- en: This enables the model to only explore a fraction of the state-space.
  id: totrans-480
  prefs: []
  type: TYPE_NORMAL
  zh: 这使得模型只能探索状态空间的一部分。
- en: '![](../Images/ef1ad595d7674550d9fdd21f6dda6934.png)'
  id: totrans-481
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ef1ad595d7674550d9fdd21f6dda6934.png)'
- en: 25x25 heat-map of agent’s decisions at each cell with the goal in the center—GIF
    by author
  id: totrans-482
  prefs: []
  type: TYPE_NORMAL
  zh: 以目标为中心的25x25代理决策热图——作者制作的GIF
- en: Above is the progression of a model’s learning, trained on a 25x25 grid. It
    shows the agent’s choice color coded at each cell with the goal in the center.
  id: totrans-483
  prefs: []
  type: TYPE_NORMAL
  zh: 上图展示了在25x25网格上训练模型的学习进程。它展示了智能体在每个格子上的选择，颜色编码表示，目标位于中央。
- en: At first, during the exploration stage, the agent’s strategy is completely off.
    You can see that it chooses to go up when it is above the target, down when it
    is below, and so on.
  id: totrans-484
  prefs: []
  type: TYPE_NORMAL
  zh: 起初，在探索阶段，智能体的策略完全不对。你可以看到它在目标上方时选择向上移动，在目标下方时选择向下移动，等等。
- en: But in under 10 episodes it learns a strategy that allows it to reach the goal
    in the shortest number of steps from any cell.
  id: totrans-485
  prefs: []
  type: TYPE_NORMAL
  zh: 但在不到10集的情况下，它学会了一种策略，使其能够在最少的步骤内从任何格子到达目标。
- en: This also applies with the goal at any location.
  id: totrans-486
  prefs: []
  type: TYPE_NORMAL
  zh: 这同样适用于目标位于任何位置的情况。
- en: '![](../Images/0cb8f50fdea4e5950526f584e02abe93.png)'
  id: totrans-487
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0cb8f50fdea4e5950526f584e02abe93.png)'
- en: Four 25x25 heat-maps of the model applied to various goal locations — Image
    by author
  id: totrans-488
  prefs: []
  type: TYPE_NORMAL
  zh: 模型在不同目标位置应用的四个25x25热图 — 图片由作者提供
- en: And finally it generalizes its learning incredibly well.
  id: totrans-489
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，它的学习能力非常强。
- en: '![](../Images/30ea0bb6766421c6ed6a30186738e2fb.png)'
  id: totrans-490
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/30ea0bb6766421c6ed6a30186738e2fb.png)'
- en: 201x201 heat-map of the 25x25 model’s decisions, showing generalization — Image
    by author
  id: totrans-491
  prefs: []
  type: TYPE_NORMAL
  zh: 201x201热图展示了25x25模型的决策，显示了泛化能力 — 图片由作者提供
- en: This model has only ever seen a 25x25 grid, but it could use its strategy on
    a far larger environment — 201x201\. With an environment this size there are 1,632,200,400
    agent-goal permutations!
  id: totrans-492
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型只见过25x25的网格，但它可以在一个更大的环境中使用其策略——201x201。如此大的环境中有1,632,200,400种智能体与目标的排列组合！
- en: Let’s update our code with this radical improvement.
  id: totrans-493
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们用这种彻底的改进来更新我们的代码。
- en: '**Implementation** There really isn’t much we need to do to get this working,
    thankfully.'
  id: totrans-494
  prefs: []
  type: TYPE_NORMAL
  zh: '**实现** 幸好，我们需要做的事情并不多就能使其工作。'
- en: The first thing is update `get_state` in `Environment`.
  id: totrans-495
  prefs: []
  type: TYPE_NORMAL
  zh: 首先需要更新`Environment`中的`get_state`。
- en: '[PRE66]'
  id: totrans-496
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: Rather than a flattened version of the grid, we calculate the distance from
    the target and return it as a NumPy array. The `*` operator simply unpacks the
    tuple into individual components. It will have the same effect as doing this —
    `state = np.array([relative_distance[0], relative_distance[1])`.
  id: totrans-497
  prefs: []
  type: TYPE_NORMAL
  zh: 与网格的展平版本不同，我们计算目标的距离，并将其作为NumPy数组返回。`*`运算符仅仅是将元组解包成单独的组件。它的效果等同于这样做——`state
    = np.array([relative_distance[0], relative_distance[1])`。
- en: Also, in `move_agent` we can update the penalty for hitting the boundary to
    be the same as moving away from the target. This is so that when you change the
    grid size, the agent is not discouraged from moving outside where it was originally
    trained.
  id: totrans-498
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，在`move_agent`中，我们可以将撞击边界的惩罚更新为与远离目标的惩罚相同。这样，当你更改网格大小时，智能体不会因移到原本训练区域之外而受到挫折。
- en: '[PRE67]'
  id: totrans-499
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: '**Updating the neural architecture** Currently our TensorFlow model looks like
    this. I’ve excluded everything else for simplicity.'
  id: totrans-500
  prefs: []
  type: TYPE_NORMAL
  zh: '**更新神经网络架构** 目前我们的TensorFlow模型如下所示。为了简洁起见，我省略了其他所有内容。'
- en: '[PRE68]'
  id: totrans-501
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: If you remember, our model architecture needs to have a consistent input. In
    this case, the input size relied on `grid_size`.
  id: totrans-502
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你还记得，我们的模型架构需要有一致的输入。在这种情况下，输入大小依赖于`grid_size`。
- en: With our updated state representation, each state will only have two values
    no matter what `grid_size` is. We can update the model to expect this. Also, we
    can remove `self.grid_size` altogether because the `Agent` class no longer relies
    on it.
  id: totrans-503
  prefs: []
  type: TYPE_NORMAL
  zh: 使用我们更新的状态表示方式，无论`grid_size`是什么，每个状态只会有两个值。我们可以更新模型以适应这一点。同时，我们可以完全移除`self.grid_size`，因为`Agent`类不再依赖于它。
- en: '[PRE69]'
  id: totrans-504
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: The `input_shape` parameter expects a tuple representing the state of the input.
  id: totrans-505
  prefs: []
  type: TYPE_NORMAL
  zh: '`input_shape`参数期望一个表示输入状态的元组。'
- en: '`(2,)` specifies a one-dimensional array with two values. Looking something
    like this:'
  id: totrans-506
  prefs: []
  type: TYPE_NORMAL
  zh: '`(2,)`表示一个具有两个值的一维数组。看起来像这样：'
- en: '[PRE70]'
  id: totrans-507
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: 'While `(2,1)`, a two-dimensional array for example, specifies two rows and
    one column. Looking something like this:'
  id: totrans-508
  prefs: []
  type: TYPE_NORMAL
  zh: 而`(2,1)`，例如，一个二维数组，表示两行一列。看起来像这样：
- en: '[PRE71]'
  id: totrans-509
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: Finally, we’ve lowered the number of neurons in our hidden layers to 64 and
    32 respectively. With this simple state representation it’s still probably overkill,
    but should run plenty fast enough.
  id: totrans-510
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将隐藏层中的神经元数量分别降低到64和32。尽管这种简单的状态表示方式仍然可能有些过度，但运行速度应该足够快。
- en: '*When you start training, try to see how few neurons you need for the model
    to effectively learn. You can even try removing the second layer if you like.*'
  id: totrans-511
  prefs: []
  type: TYPE_NORMAL
  zh: '*当你开始训练时，尝试看看模型有效学习所需的最少神经元数量。如果愿意，你甚至可以尝试移除第二层。*'
- en: '**Fixing the main training loop** The training loop requires very few adjustments.
    Let’s update it to match our changes.'
  id: totrans-512
  prefs: []
  type: TYPE_NORMAL
  zh: '**修复主要训练循环** 训练循环需要很少的调整。让我们更新它以匹配我们的更改。'
- en: '[PRE72]'
  id: totrans-513
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: Because `agent` no longer needs the `grid_size`, we can remove it to prevent
    any errors.
  id: totrans-514
  prefs: []
  type: TYPE_NORMAL
  zh: 因为`agent`不再需要`grid_size`，我们可以移除它以防止任何错误。
- en: We also no longer have to give the model different names for each `grid_size`,
    since one model now works on any size.
  id: totrans-515
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也不再需要为每个`grid_size`给模型不同的名称，因为一个模型现在适用于任何大小。
- en: If you’re curious about `ExperienceReplay`, it will remain the same.
  id: totrans-516
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你对`ExperienceReplay`感兴趣，它将保持不变。
- en: '*Please note that there is no one-size-fits-all state representation. In some
    cases it may make sense to provide the full grid like we did, or a subsection
    of it like I’ve done with the multi-agent system in section 9\. The goal is to
    find a balance between simplifying the state-space and providing adequate information
    for the agent to learn.*'
  id: totrans-517
  prefs: []
  type: TYPE_NORMAL
  zh: '*请注意，没有一种适合所有情况的状态表示。在某些情况下，像我们这样提供完整的网格，或者像我在第9节中做的那样提供部分网格是有意义的。目标是找到简化状态空间和提供足够信息之间的平衡，以便代理能够学习。*'
- en: '**Hyper-parameters** Even a simple environment like ours requires adjustments
    of the hyper-parameters. Remember that these are the values we can change that
    effect training.'
  id: totrans-518
  prefs: []
  type: TYPE_NORMAL
  zh: '**超参数** 即使像我们这样简单的环境也需要调整超参数。记住，这些是我们可以更改的值，影响训练过程。'
- en: 'Each one we have discussed includes:'
  id: totrans-519
  prefs: []
  type: TYPE_NORMAL
  zh: 我们讨论的每一个都包括：
- en: '`epsilon`, `epsilon_decay`, `epsilon_end` (exploration/exploitation)'
  id: totrans-520
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`epsilon`, `epsilon_decay`, `epsilon_end`（探索/利用）'
- en: '`gamma` (discount factor)'
  id: totrans-521
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`gamma`（折扣因子）'
- en: number of neurons and layers
  id: totrans-522
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 神经元数量和层数
- en: '`batch_size`, `capacity` (experience replay)'
  id: totrans-523
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`batch_size`, `capacity`（经验回放）'
- en: '`max_steps`'
  id: totrans-524
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_steps`'
- en: There are plenty of others, but there is just one more we will discuss that
    will be critical for learning.
  id: totrans-525
  prefs: []
  type: TYPE_NORMAL
  zh: 还有很多其他的，但我们将讨论的还有一个对于学习至关重要。
- en: '**Learning rate** The Learning Rate (LR) is a hyper-parameter of the neural
    network model.'
  id: totrans-526
  prefs: []
  type: TYPE_NORMAL
  zh: '**学习率** 学习率（LR）是神经网络模型的一个超参数。'
- en: It basically tells the neural network how much to adjust its weights — values
    used for transformation of the input — each time it is fit to the data.
  id: totrans-527
  prefs: []
  type: TYPE_NORMAL
  zh: 它基本上告诉神经网络每次拟合数据时调整其权重——用于输入转换的值——的程度。
- en: The values of LR typically range from 1 down to 0.0000001, with the most common
    being values like 0.01, 0.001, and 0.0001.
  id: totrans-528
  prefs: []
  type: TYPE_NORMAL
  zh: 学习率的值通常范围从1到0.0000001，其中最常见的值是0.01、0.001和0.0001。
- en: '![](../Images/39e0d56351e0af4323dff78b2edb45c5.png)'
  id: totrans-529
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/39e0d56351e0af4323dff78b2edb45c5.png)'
- en: Sub-optimal learning rate that may never converge on an optimal strategy — Image
    by author
  id: totrans-530
  prefs: []
  type: TYPE_NORMAL
  zh: 次优学习率可能永远无法收敛到最优策略——作者提供的图像
- en: If the learning rate is too low, it might not update the Q-values quickly enough
    to learn an optimal strategy, a process known as convergence. If you notice that
    there seems to be a stagnation in learning, or none at all, this could be a sign
    that the learning rate is not high enough.
  id: totrans-531
  prefs: []
  type: TYPE_NORMAL
  zh: 如果学习率过低，可能无法足够快地更新Q值以学习最优策略，这个过程称为收敛。如果你注意到学习似乎停滞不前，或者完全没有，这可能是学习率不够高的一个迹象。
- en: '*While these diagrams on learning rate are greatly simplified, they should
    get the basic idea across.*'
  id: totrans-532
  prefs: []
  type: TYPE_NORMAL
  zh: '*虽然这些关于学习率的图示大大简化了，但它们应该传达了基本的概念。*'
- en: '![](../Images/a754d683568838fcd6a4d3d60c23945d.png)'
  id: totrans-533
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a754d683568838fcd6a4d3d60c23945d.png)'
- en: Sub-optimal learning rate that causes the Q-Values to continue to grow exponentially
    — Image by author
  id: totrans-534
  prefs: []
  type: TYPE_NORMAL
  zh: 次优学习率导致Q值持续指数增长——作者提供的图像
- en: One the other side, a learning rate that is too high can cause your values to
    “explode” or become increasingly large. The adjustments the model makes are too
    great, causing it to diverge — or get worse over time.
  id: totrans-535
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，学习率过高可能导致值“爆炸”或变得越来越大。模型的调整过大，导致它发散——或者随着时间推移变得更差。
- en: '**What is the perfect learning rate?** How long is a piece of string?'
  id: totrans-536
  prefs: []
  type: TYPE_NORMAL
  zh: '**什么是完美的学习率？** 一根绳子有多长？'
- en: In many cases you just have to use simple trial and error. A good way to determine
    if your learning rate is the issue is to check the output of the model.
  id: totrans-537
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多情况下，你只需使用简单的试错法。确定学习率是否是问题的好方法是检查模型的输出。
- en: This is exactly the issue I was facing when training this model. After switching
    to the simplified state representation, it refused to learn. The agent would actually
    continue to go to the bottom right of the grid after extensively testing each
    hyper-parameter.
  id: totrans-538
  prefs: []
  type: TYPE_NORMAL
  zh: 这正是我在训练这个模型时遇到的问题。切换到简化的状态表示后，它拒绝学习。代理实际上在广泛测试每个超参数后继续移动到网格的右下角。
- en: It did not make sense to me, so I decided to take a look at the Q-values output
    by the model in the `Agent` `get_action` method.
  id: totrans-539
  prefs: []
  type: TYPE_NORMAL
  zh: 这让我感到不解，所以我决定查看`Agent` `get_action`方法中模型输出的Q值。
- en: '[PRE73]'
  id: totrans-540
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: This is an example of exploding values.
  id: totrans-541
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个值爆炸的示例。
- en: In TensorFlow the optimizer we are using to adjust the weights, Adam, has a
    default learning rate of 0.001\. For this specific case it happened to be much
    too high.
  id: totrans-542
  prefs: []
  type: TYPE_NORMAL
  zh: 在TensorFlow中，我们用来调整权重的优化器Adam，其默认学习率为0.001。对于这种特定情况，这个值显然太高了。
- en: '![](../Images/34493346947c65275aac5e961813ab08.png)'
  id: totrans-543
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/34493346947c65275aac5e961813ab08.png)'
- en: Balanced learning rate, eventually converging to the Optimal Strategy — Image
    by author
  id: totrans-544
  prefs: []
  type: TYPE_NORMAL
  zh: 平衡学习率，最终收敛到最佳策略——作者提供的图像
- en: After testing various values, a sweet spot seems to be at 0.00001.
  id: totrans-545
  prefs: []
  type: TYPE_NORMAL
  zh: 在测试了各种值之后，最佳点似乎是0.00001。
- en: Let’s implement this.
  id: totrans-546
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来实现这个。
- en: '[PRE74]'
  id: totrans-547
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: Feel free to adjust this and observe how the Q-values are affected. Also, make
    sure to import Adam.
  id: totrans-548
  prefs: []
  type: TYPE_NORMAL
  zh: 随意调整这些设置，观察Q值的变化。同时，确保导入Adam。
- en: Finally, you can once again begin training!
  id: totrans-549
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，你可以再次开始训练！
- en: '**Heat-map code** Below is the code for plotting your own heat-map as shown
    previously if you are interested.'
  id: totrans-550
  prefs: []
  type: TYPE_NORMAL
  zh: '**热图代码** 如果你感兴趣，下面是绘制你自己热图的代码，正如之前所示。'
- en: '[PRE75]'
  id: totrans-551
  prefs: []
  type: TYPE_PRE
  zh: '[PRE75]'
- en: Simply import it into your training loop and run it however often you would
    like.
  id: totrans-552
  prefs: []
  type: TYPE_NORMAL
  zh: 只需将其导入到你的训练循环中，并根据需要运行。
- en: '**Next steps** Once you have effectively trained your model and experimented
    with the hyper-parameters, I encourage you to truly make it your own.'
  id: totrans-553
  prefs: []
  type: TYPE_NORMAL
  zh: '**下一步** 一旦你有效地训练了你的模型并尝试了各种超参数，我鼓励你真正把它做成你自己的。'
- en: 'Some ideas for expanding the system:'
  id: totrans-554
  prefs: []
  type: TYPE_NORMAL
  zh: 扩展系统的一些想法：
- en: Add obstacles between the agent and goal
  id: totrans-555
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在代理和目标之间添加障碍
- en: Create a more varied environment, possibly with randomly generated rooms and
    pathways
  id: totrans-556
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建一个更为多样的环境，可能包括随机生成的房间和通道
- en: Implement a multi-agent cooperation/competition system — hide and seek
  id: totrans-557
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现一个多代理合作/竞争系统——捉迷藏
- en: Create a Pong inspired game
  id: totrans-558
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建一个受乒乓球启发的游戏
- en: Implement resource management such as a hunger or energy system where the agent
    needs to collect food on the way to the goal
  id: totrans-559
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现资源管理，如饥饿或能量系统，其中代理需要在前往目标的途中收集食物
- en: 'Here is an example that goes beyond our simple grid system:'
  id: totrans-560
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个超越我们简单网格系统的示例：
- en: '![](../Images/0537b23b3da6d047d96e6e033df18e5a.png)'
  id: totrans-561
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0537b23b3da6d047d96e6e033df18e5a.png)'
- en: Flappy Bird inspired game where the agent must avoid the pipes to survive —
    GIF by author
  id: totrans-562
  prefs: []
  type: TYPE_NORMAL
  zh: Flappy Bird风格的游戏，代理必须避开管道才能生存——作者提供的GIF
- en: Using [Pygame](https://www.pygame.org/news), a popular Python library for making
    2d games, I constructed a Flappy Bird clone. Then I defined the interactions,
    constraints, and reward structure in our prebuilt `Environment` class.
  id: totrans-563
  prefs: []
  type: TYPE_NORMAL
  zh: 使用[Pygame](https://www.pygame.org/news)，一个流行的Python 2D游戏库，我构建了一个Flappy Bird克隆。然后，我在我们预构建的`Environment`类中定义了交互、约束和奖励结构。
- en: I represented the state as the current velocity and location of the agent, the
    distance to the closest pipe, and the location of the opening.
  id: totrans-564
  prefs: []
  type: TYPE_NORMAL
  zh: 我将状态表示为代理的当前速度和位置、与最近管道的距离，以及开口的位置。
- en: For the `Agent` class I simply updated the input size to `(4,)`, added more
    layers to the NN, and updated the network to only output two values — jump or
    not jump.
  id: totrans-565
  prefs: []
  type: TYPE_NORMAL
  zh: 对于`Agent`类，我只是将输入大小更新为`(4,)`，增加了神经网络的层数，并更新了网络以仅输出两个值——跳跃或不跳跃。
- en: You can find and run this in the `flappy_bird` directory on the GitHub [repo](https://github.com/HestonCV/rl-gym-from-scratch).
    Make sure to `pip install pygame`.
  id: totrans-566
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在GitHub [repo](https://github.com/HestonCV/rl-gym-from-scratch)的`flappy_bird`目录中找到并运行这些内容。确保`pip
    install pygame`。
- en: This shows that what you’ve built is applicable with a variety of environments.
    You can even have the agent explore a 3d environment or perform more abstract
    tasks like stock trading.
  id: totrans-567
  prefs: []
  type: TYPE_NORMAL
  zh: 这表明你所构建的系统适用于各种环境。你甚至可以让代理探索三维环境或执行更抽象的任务，如股票交易。
- en: While expanding your system don’t be afraid to get creative with your environment,
    state representation, and reward system. Like the agent, we learn best by exploration!
  id: totrans-568
  prefs: []
  type: TYPE_NORMAL
  zh: 在扩展你的系统时，不要害怕在环境、状态表示和奖励系统上进行创新。就像代理一样，我们也通过探索学习得最好！
- en: I hope building a DRL gym from scratch has opened your eyes to the beauty of
    AI and has inspired you to dive deeper.
  id: totrans-569
  prefs: []
  type: TYPE_NORMAL
  zh: 我希望从零开始构建DRL健身房让你领悟到了AI的美妙，并激励你深入探索。
- en: '*This article was inspired by the* [*Neural Networks From Scratch In Python
    Book*](https://nnfs.io/) *and* [*youtube series*](https://www.youtube.com/watch?v=Wo5dMEP_BbI&list=PLQVvvaa0QuDcjD5BAw2DxE6OF2tius3V3)
    *by Harrison Kinsley (sentdex) and Daniel Kukieł. The conversational style and
    from scratch code implementations really solidified my understanding of Neural
    Networks.*'
  id: totrans-570
  prefs: []
  type: TYPE_NORMAL
  zh: '*这篇文章的灵感来源于* [*《Python 从零开始的神经网络》*](https://nnfs.io/) *和* [*youtube 系列*](https://www.youtube.com/watch?v=Wo5dMEP_BbI&list=PLQVvvaa0QuDcjD5BAw2DxE6OF2tius3V3)
    *由 Harrison Kinsley (sentdex) 和 Daniel Kukieł 主讲。对话式风格和从零开始的代码实现真正巩固了我对神经网络的理解。*'
