- en: Train YOLOv8 Instance Segmentation on Your Data
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在您的数据上训练 YOLOv8 实例分割
- en: 原文：[https://towardsdatascience.com/trian-yolov8-instance-segmentation-on-your-data-6ffa04b2debd?source=collection_archive---------0-----------------------#2023-02-15](https://towardsdatascience.com/trian-yolov8-instance-segmentation-on-your-data-6ffa04b2debd?source=collection_archive---------0-----------------------#2023-02-15)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/trian-yolov8-instance-segmentation-on-your-data-6ffa04b2debd?source=collection_archive---------0-----------------------#2023-02-15](https://towardsdatascience.com/trian-yolov8-instance-segmentation-on-your-data-6ffa04b2debd?source=collection_archive---------0-----------------------#2023-02-15)
- en: How to train an instance segmentation model based on the state-of-the-art YOLOv8
    model on your data
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何基于最新的 YOLOv8 模型在您的数据上训练一个实例分割模型
- en: '[](https://alon-lek.medium.com/?source=post_page-----6ffa04b2debd--------------------------------)[![Alon
    Lekhtman](../Images/1451bacf9a127f7fe596cf32249035be.png)](https://alon-lek.medium.com/?source=post_page-----6ffa04b2debd--------------------------------)[](https://towardsdatascience.com/?source=post_page-----6ffa04b2debd--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----6ffa04b2debd--------------------------------)
    [Alon Lekhtman](https://alon-lek.medium.com/?source=post_page-----6ffa04b2debd--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://alon-lek.medium.com/?source=post_page-----6ffa04b2debd--------------------------------)[![Alon
    Lekhtman](../Images/1451bacf9a127f7fe596cf32249035be.png)](https://alon-lek.medium.com/?source=post_page-----6ffa04b2debd--------------------------------)[](https://towardsdatascience.com/?source=post_page-----6ffa04b2debd--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----6ffa04b2debd--------------------------------)
    [Alon Lekhtman](https://alon-lek.medium.com/?source=post_page-----6ffa04b2debd--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F931822b64e54&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftrian-yolov8-instance-segmentation-on-your-data-6ffa04b2debd&user=Alon+Lekhtman&userId=931822b64e54&source=post_page-931822b64e54----6ffa04b2debd---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----6ffa04b2debd--------------------------------)
    ·10 min read·Feb 15, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F6ffa04b2debd&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftrian-yolov8-instance-segmentation-on-your-data-6ffa04b2debd&user=Alon+Lekhtman&userId=931822b64e54&source=-----6ffa04b2debd---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F931822b64e54&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftrian-yolov8-instance-segmentation-on-your-data-6ffa04b2debd&user=Alon+Lekhtman&userId=931822b64e54&source=post_page-931822b64e54----6ffa04b2debd---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----6ffa04b2debd--------------------------------)
    ·10 分钟阅读·2023年2月15日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F6ffa04b2debd&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftrian-yolov8-instance-segmentation-on-your-data-6ffa04b2debd&user=Alon+Lekhtman&userId=931822b64e54&source=-----6ffa04b2debd---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F6ffa04b2debd&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftrian-yolov8-instance-segmentation-on-your-data-6ffa04b2debd&source=-----6ffa04b2debd---------------------bookmark_footer-----------)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F6ffa04b2debd&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftrian-yolov8-instance-segmentation-on-your-data-6ffa04b2debd&source=-----6ffa04b2debd---------------------bookmark_footer-----------)'
- en: YOLOv8 was launched on January 10th, 2023\. And as of this moment, this is the
    state-of-the-art model for classification, detection, and segmentation tasks in
    the computer vision world. The model outperforms all known models both in terms
    of accuracy and execution time.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: YOLOv8 于 2023 年 1 月 10 日发布。到目前为止，这是计算机视觉领域用于分类、检测和分割任务的最先进模型。该模型在准确性和执行时间方面均优于所有已知模型。
- en: '![](../Images/846d796782d2742599ba66f04d7a424c.png)'
  id: totrans-9
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/846d796782d2742599ba66f04d7a424c.png)'
- en: A comparison between YOLOv8 and other YOLO models (from [ultralytics](https://github.com/ultralytics/ultralytics))
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: YOLOv8 与其他 YOLO 模型的比较（来自 [ultralytics](https://github.com/ultralytics/ultralytics)）
- en: The ultralytics team did a really good job in making this model easier to use
    compared to all the previous YOLO models — you don’t even have to clone the git
    repository anymore!
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: ultralytics 团队在使这个模型比所有之前的 YOLO 模型更易于使用方面做得非常出色——你甚至不再需要克隆 git 仓库了！
- en: Creating the Image Dataset
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建图像数据集
- en: In this post, I created a very simple example of all you need to do to train
    YOLOv8 on your data, specifically for a segmentation task. The dataset is small
    and “easy to learn” for the model, on purpose, so that we would be able to get
    satisfying results after training for only a few seconds on a simple CPU.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我创建了一个非常简单的示例，展示了训练 YOLOv8 所需的所有步骤，特别是用于分割任务。数据集很小，对模型来说“易于学习”，故意如此，以便我们能够在简单的
    CPU 上训练几秒钟后获得令人满意的结果。
- en: We will create a dataset of white circles with a black background. The circles
    will be of varying sizes. We will train a model that segments the circles inside
    the image.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将创建一个白色圆圈与黑色背景的数据集。圆圈将有不同的尺寸。我们将训练一个模型，能够在图像中分割出这些圆圈。
- en: 'This is what the dataset looks like:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是数据集的样子：
- en: '![](../Images/7a79eee703fe9676934e9eef85f99867.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7a79eee703fe9676934e9eef85f99867.png)'
- en: 'The dataset was generated using the following code:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集是使用以下代码生成的：
- en: '[PRE0]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Creating Labels
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建标签
- en: 'Now that we have an image dataset, we need to create labels for the images.
    Usually, we would need to do some manual work for this, but because the dataset
    we created is very simple, it is pretty easy to create code that generates labels
    for us:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了图像数据集，我们需要为图像创建标签。通常，我们需要进行一些手动操作，但由于我们创建的数据集非常简单，因此很容易编写代码来生成标签：
- en: '[PRE1]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Here is an example of a label file content:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是标签文件内容的示例：
- en: '[PRE2]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The label corresponds to this image:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 标签对应于这张图像：
- en: '![](../Images/41ddab79fd8e2339d49bf12eec44bba4.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/41ddab79fd8e2339d49bf12eec44bba4.png)'
- en: an image that corresponds to the label example
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 对应于标签示例的图像
- en: The label content is only a single text line. We have only one object (circle)
    in each image, and each object is represented by a line in the file. If you have
    more than one object in each image, you should create a line for each labeled
    object.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 标签内容仅为一行文本。每张图像中只有一个物体（圆圈），每个物体由文件中的一行表示。如果每张图像中有多个物体，你应该为每个标记物体创建一行。
- en: The first 0 represents the class type of the label. Because we have only one
    class type (a circle) we always have 0\. If you have more than one class in your
    data, you should map each class to a number ( 0, 1, 2…) and use this number in
    the label file.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个 0 表示标签的类别类型。因为我们只有一个类别（圆圈），所以我们总是使用 0。如果你的数据中有多个类别，你应该将每个类别映射到一个数字（0, 1,
    2…），并在标签文件中使用这个数字。
- en: All the other numbers represent the coordinates of the bounding polygon of the
    labeled object. The format is <**x1 y1 x2 y2 x3 y3…>** and the coordinates are
    relative to the size of the image —you should normalize the coordinates to a 1x1
    image size. For example, if there is a point (15, 75) and the image size is 120x120
    the normalized point is (15/120, 75/120) = (0.125, 0.625).
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 所有其他数字表示标记物体的边界多边形的坐标。格式是 <**x1 y1 x2 y2 x3 y3…>**，这些坐标相对于图像的大小——你应该将坐标标准化为
    1x1 的图像大小。例如，如果有一个点 (15, 75) 而图像大小是 120x120，则标准化后的点是 (15/120, 75/120) = (0.125,
    0.625)。
- en: It is always confusing when dealing with image libraries to get the correct
    directionality of the coordinates. So to make this clear, for YOLO, the X coordinate
    goes from left to right, and the Y coordinate goes from top to bottom.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在处理图像库时，处理坐标的正确方向总是令人困惑。因此，为了使其清晰，对于 YOLO，X 坐标从左到右，而 Y 坐标从上到下。
- en: The YAML Configuration
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: YAML 配置
- en: 'We have the images and the labels. Now we need to create a YAML file with the
    dataset configuration:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经有了图像和标签。现在我们需要创建一个包含数据集配置的 YAML 文件：
- en: '[PRE3]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Pay attention that if you have more object class types, you need to add them
    here in the names array, in the same order you ordered them in the label files.
    The first is 0, the second is 1, etc…
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，如果你有更多的物体类别类型，你需要在名称数组中添加它们，顺序与标签文件中的顺序一致。第一个是 0，第二个是 1，依此类推…
- en: Dataset File Structure
  id: totrans-35
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据集文件结构
- en: 'Let''s see the file structure we created, using the Linux [**tree**](https://pimylifeup.com/tree-command-linux/)
    command:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看我们创建的文件结构，使用 Linux [**tree**](https://pimylifeup.com/tree-command-linux/)
    命令：
- en: '[PRE4]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '[PRE5]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Training The Model
  id: totrans-39
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练模型
- en: 'Now that we have the images and the labels, we can start training the model.
    So first of all let''s install the package:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经有了图像和标签，我们可以开始训练模型。因此首先安装包：
- en: '[PRE6]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The ultralytics library changes pretty fast and sometimes breaks the API, so
    I prefer to stick with one version. The below code depends on version 8.0.38 (the
    newest version at the time I write those words). If you upgrade to a newer version,
    maybe you will need to do some code adaptations to make it work.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: ultralytics 库更新非常快，有时会导致 API 中断，因此我更倾向于使用一个版本。以下代码依赖于版本 8.0.38（这是我写这些内容时的最新版本）。如果你升级到较新的版本，可能需要对代码进行一些调整以使其正常工作。
- en: 'And start the training:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 并开始训练：
- en: '[PRE7]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: For the simplicity of this post, I use the nano model (yolov8n-seg), I train
    it only on the CPU, with only 7 epochs. The training took just a few seconds on
    my laptop.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简化这篇文章，我使用了 nano 模型（yolov8n-seg），仅在 CPU 上训练，仅用了 7 个周期。训练在我的笔记本电脑上只花了几秒钟。
- en: For more information about the parameters that can be used to train the model,
    you can check [this](https://github.com/ultralytics/ultralytics/blob/main/ultralytics/yolo/engine/trainer.py#L42).
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 关于可用于训练模型的参数的更多信息，你可以查看[这个链接](https://github.com/ultralytics/ultralytics/blob/main/ultralytics/yolo/engine/trainer.py#L42)。
- en: Understanding the Results
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解结果
- en: 'After the training is done you will see a line, similar to this, at the end
    of the output:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 训练完成后，你会在输出的末尾看到一行，类似于这样：
- en: '[PRE8]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Let’s take a look at some of the results found here:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一下这里的一些结果：
- en: Validation labels
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 验证标签
- en: '[PRE9]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '![](../Images/567dec0b45f36610ab6970aaa544e672.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/567dec0b45f36610ab6970aaa544e672.png)'
- en: part of the validation set labels
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 部分验证集标签
- en: Here we can see the ground truth labels on part of the validation set. This
    should be almost perfectly aligned. In case you see those labels do not cover
    the objects well, it is highly likely that your labeling is incorrect.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里我们可以看到部分验证集的真实标签。这应该几乎完全对齐。如果你看到这些标签没有很好地覆盖对象，很可能是你的标签不正确。
- en: '**Predicted validation labels**'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '**预测验证标签**'
- en: '[PRE10]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '![](../Images/2192471bc68c59dcf54fcdecafb638d0.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2192471bc68c59dcf54fcdecafb638d0.png)'
- en: validation set predictions
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 验证集预测
- en: Here we can see the predictions the trained model did on part of the validation
    set (the same part we saw above). This can give you a feeling of how well the
    model performs. Pay attention that in order to create this image a confidence
    threshold should be chosen, the threshold used here is **0.5**, which is not always
    the optimal one (we will discuss it later).
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里我们可以看到训练模型在部分验证集上的预测（与上面看到的部分相同）。这可以让你感受模型的表现如何。请注意，为了创建此图像，应选择一个置信度阈值，这里使用的阈值是**0.5**，这并不总是最佳的（我们稍后会讨论）。
- en: Precision curve
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 精度曲线
- en: To understand this and the next charts you need to be familiar with precision
    and recall concepts. [Here](https://medium.com/@shrutisaxena0617/precision-vs-recall-386cf9f89488)
    is a good explanation of how they work.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 要理解这些以及接下来的图表，你需要对精度和召回率的概念有所了解。[这里](https://medium.com/@shrutisaxena0617/precision-vs-recall-386cf9f89488)对它们的工作原理做了很好的解释。
- en: '[PRE11]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '![](../Images/6d5e98b3d6235facea1866bfba43ad9b.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6d5e98b3d6235facea1866bfba43ad9b.png)'
- en: precision/confidence threshold curve
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 精度/置信度阈值曲线
- en: Every detected object by the model has some confidence, usually, if it is important
    to you to be as sure as possible when declaring “this is a circle” you will use
    only high confidence values (high confidence threshold). Off course, it comes
    with a tradeoff- you can miss some “circles”. On the other hand, if you want to
    “catch” as many “circles” as you can with a tradeoff that some of them are not
    really “circles” you would use both low and high confidence values (low confidence
    threshold).
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 模型检测到的每个对象都有一定的置信度，通常，如果你希望在声明“这是一个圆形”时尽可能确定，你会只使用高置信度值（高置信度阈值）。当然，这会有一个权衡——你可能会错过一些“圆形”。另一方面，如果你想“捕捉”尽可能多的“圆形”，即使有些并不真正是“圆形”，你会使用低置信度值和高置信度值（低置信度阈值）。
- en: The above chart (and the chart below) helps you decide which confidence threshold
    to use. In our case, we can see that for a threshold higher than **0.128**, we
    get **100%** precision, which means all objects are correctly predicted.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 上面的图表（以及下面的图表）帮助你决定使用哪个置信度阈值。在我们的例子中，我们可以看到对于高于**0.128**的阈值，我们获得了**100%**的精度，这意味着所有对象都被正确预测。
- en: Pay attention that because we actually doing a segmentation task, there is another
    important threshold we need to worry about — IoU ( intersection over union), if
    you are not familiar with it, you can read about it [here](https://pyimagesearch.com/2016/11/07/intersection-over-union-iou-for-object-detection/).
    For this chart, an IoU of **0.5** is used.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，由于我们实际上是在进行分割任务，还有一个重要的阈值需要关注——IoU（交并比），如果你不熟悉它，可以在[这里](https://pyimagesearch.com/2016/11/07/intersection-over-union-iou-for-object-detection/)阅读相关信息。对于这张图表，使用了**0.5**的IoU。
- en: '**Recall curve**'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '**召回率曲线**'
- en: '[PRE12]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '![](../Images/e9898f596c3f8de76b7f97ec15de9546.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e9898f596c3f8de76b7f97ec15de9546.png)'
- en: recall/confidence threshold curve
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 召回率/置信度阈值曲线
- en: Here you can see the recall chart, as the confidence threshold values go up,
    the recall goes down. Which means you “catch” fewer “circles”.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 这里你可以看到召回率图表，随着置信度阈值的提高，召回率下降。这意味着你“捕捉到”的“圆圈”变少了。
- en: Here you can see why using the **0.5** confidence threshold, in this case, is
    a bad idea. For a **0.5** threshold, you get about **90%** recall. However, in
    the precision curve, we saw that for a threshold above **0.128**, we get **100%**
    precision, so we don’t need to get to **0.5**, we can safely use a **0.128** threshold
    and get both **100%** precision and almost **100%** recall :)
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 这里你可以看到为什么在这种情况下使用**0.5**的置信度阈值是一个糟糕的主意。对于**0.5**的阈值，你得到大约**90%**的召回率。然而，在精度曲线中，我们看到对于高于**0.128**的阈值，我们可以获得**100%**的精度，因此我们不需要达到**0.5**，可以安全地使用**0.128**的阈值，获得**100%**的精度和接近**100%**的召回率
    :)
- en: Precision-Recall curve
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 精度-召回曲线
- en: '[Here](https://medium.com/@douglaspsteen/precision-recall-curves-d32e5b290248)
    is a good explanation of the precision-recall curve'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '[这里](https://medium.com/@douglaspsteen/precision-recall-curves-d32e5b290248)是对精度-召回曲线的很好解释'
- en: '[PRE13]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '![](../Images/ae03968630d242ece8e4631990d2805a.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ae03968630d242ece8e4631990d2805a.png)'
- en: precision-recall curve
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 精度-召回曲线
- en: We can see here clearly the conclusion we made before, for this model, we can
    get to almost **100%** precision and **100%** recall.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以清楚地看到我们之前得出的结论，对于这个模型，我们可以达到接近**100%**的精度和**100%**的召回率。
- en: The disadvantage of this chart is that we can’t see what threshold we should
    use, this is why we still need the charts above.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 这个图表的缺点是我们无法看到应该使用哪个阈值，这就是为什么我们仍然需要上述图表的原因。
- en: '**Loss over time**'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '**随时间变化的损失**'
- en: '[PRE14]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '![](../Images/94602d3dd0c79a4c321ba0f545bbd9d7.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/94602d3dd0c79a4c321ba0f545bbd9d7.png)'
- en: loss over time
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 随时间变化的损失
- en: Here you can see how the different losses change during the training, and how
    they behave on the validation set after each epoch.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 这里你可以看到不同损失在训练过程中的变化，以及每个时期之后它们在验证集上的表现。
- en: There is a lot to say about the losses, and conclusions you can make from those
    charts, however, it is out of the scope of this post. I just wanted to state that
    you can find it here :)
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 关于损失和从这些图表中得出的结论有很多可以说的，但这超出了本文的范围。我只是想说明你可以在这里找到相关信息 :)
- en: Using the trained model
  id: totrans-88
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用训练好的模型
- en: 'Another thing that can be found in the result directory is the model itself.
    Here’s how to use the model on new images:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 结果目录中还可以找到模型本身。这是如何在新图像上使用模型：
- en: '[PRE15]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: The results list may have multiple values, one for each detected object. Because
    in this example we have only one object in each image, we take the first list
    item.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 结果列表可能包含多个值，每个值对应一个检测到的对象。由于在这个例子中每张图片中只有一个对象，我们取第一个列表项。
- en: You can see I passed here the best confidence threshold value we found before
    (**0.128**).
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到我传递了我们之前找到的最佳置信度阈值（**0.128**）。
- en: There are two ways to get the actual placement of the detected object in the
    image. Choosing the right method depends on what you intend to do with the results.
    I will show both ways.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 有两种方法可以获取检测到的对象在图像中的实际位置。选择正确的方法取决于你打算如何使用结果。我将展示这两种方法。
- en: '[PRE16]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '[PRE17]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: This returns the bounding polygon of the object, similar to the format we passed
    the labeled data.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 这会返回对象的边界多边形，类似于我们传递标记数据的格式。
- en: 'And the second way:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 第二种方法是：
- en: '[PRE18]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '[PRE19]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: This returns a tensor with a shape (1, 128, 128) that represents all the pixels
    in the image. Pixels that are part of the object receive 1 and background pixels
    receive 0.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 这会返回一个形状为（1, 128, 128）的张量，表示图像中的所有像素。属于对象的像素接收1，背景像素接收0。
- en: 'Let’s see what the mask looks like:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看掩模的样子：
- en: '[PRE20]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '![](../Images/188f74a8e99b400d049abcf6fa0ffb6a.png)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/188f74a8e99b400d049abcf6fa0ffb6a.png)'
- en: predicted segmentation for image
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 预测图像的分割
- en: 'This was the original image:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是原始图像：
- en: '![](../Images/787981982a3eb511bfc09f1f297c9c6e.png)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/787981982a3eb511bfc09f1f297c9c6e.png)'
- en: original image
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 原始图像
- en: Not perfect, but good enough for many applications, and the IoU is definitely
    higher than **0.5**.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然不完美，但对于许多应用来说已经足够好，并且 IoU 确实高于**0.5**。
- en: 'In conclusion, the new ultralytics library is much easier to work with compared
    to the previous Yolo versions, especially for the segmentation task, which is
    now a first-class citizen. You can find Yolov5 also as part of the ultralytics
    new package, so if you don’t want to use the new Yolo version, which is still
    kind of new and experimental, you can just use the well-known yolov5:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，与之前的 Yolo 版本相比，新版的 ultralytics 库更容易使用，特别是在分割任务上，现在它已经成为一个一流的功能。你可以在 ultralytics
    新包中找到 Yolov5，因此如果你不想使用仍然有些新的和实验性的 Yolo 版本，你可以选择使用广为人知的 yolov5：
- en: '![](../Images/2d6bb0b746deda291cf6d79aa89155af.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2d6bb0b746deda291cf6d79aa89155af.png)'
- en: '[google trends comparison of yolov8 and yolov5](https://trends.google.com/trends/explore?date=today+5-y&q=yolov5%2Cyolov8)'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '[yolov8 和 yolov5 的 Google 趋势比较](https://trends.google.com/trends/explore?date=today+5-y&q=yolov5%2Cyolov8)'
- en: There were some topics I didn’t cover, like the different loss functions used
    for the model, the architecture changes that were made to create the yolov8, and
    more. Feel free to comment on this post if you want more information about those
    topics. If there will be interest, I will maybe write another post about it.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 有些话题我没有涵盖，比如用于模型的不同损失函数、创建 yolov8 时进行的架构更改等等。如果你想了解更多这些话题，欢迎在这篇文章下评论。如果有兴趣，我可能会写另一篇关于这些内容的文章。
- en: Thanks for taking the time to read this and I hope it helped you understand
    the Yolov8 model training process.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 感谢你抽时间阅读这篇文章，希望它能帮助你理解 Yolov8 模型的训练过程。
- en: '*All images, unless otherwise noted, were created by the author.*'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '*除非另有说明，所有图像均由作者创建。*'
