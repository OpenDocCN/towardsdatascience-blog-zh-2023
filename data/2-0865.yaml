- en: Extending Context Length in Large Language Models
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 扩展大规模语言模型中的上下文长度
- en: 原文：[https://towardsdatascience.com/extending-context-length-in-large-language-models-74e59201b51f](https://towardsdatascience.com/extending-context-length-in-large-language-models-74e59201b51f)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/extending-context-length-in-large-language-models-74e59201b51f](https://towardsdatascience.com/extending-context-length-in-large-language-models-74e59201b51f)
- en: How to turn your Llama into a Giraffe
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何将你的Llama变成长颈鹿
- en: '[](https://donatoriccio.medium.com/?source=post_page-----74e59201b51f--------------------------------)[![Donato
    Riccio](../Images/0af2a026e72a023db4635522cbca50eb.png)](https://donatoriccio.medium.com/?source=post_page-----74e59201b51f--------------------------------)[](https://towardsdatascience.com/?source=post_page-----74e59201b51f--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----74e59201b51f--------------------------------)
    [Donato Riccio](https://donatoriccio.medium.com/?source=post_page-----74e59201b51f--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://donatoriccio.medium.com/?source=post_page-----74e59201b51f--------------------------------)[![Donato
    Riccio](../Images/0af2a026e72a023db4635522cbca50eb.png)](https://donatoriccio.medium.com/?source=post_page-----74e59201b51f--------------------------------)[](https://towardsdatascience.com/?source=post_page-----74e59201b51f--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----74e59201b51f--------------------------------)
    [Donato Riccio](https://donatoriccio.medium.com/?source=post_page-----74e59201b51f--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----74e59201b51f--------------------------------)
    ·9 min read·Oct 15, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----74e59201b51f--------------------------------)
    ·9分钟阅读·2023年10月15日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/8bb0da11ed7b43b5f0cc752db52b0aad.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8bb0da11ed7b43b5f0cc752db52b0aad.png)'
- en: Image by the author. (AI generated Llamas)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供。（AI生成的Llamas）
- en: Context length refers to the maximum number of tokens the model can remember
    when generating text. A longer context window allows the model to understand long-range
    dependencies in text better. Models with longer contexts can build connections
    between ideas far apart in the text, generating more globally coherent outputs.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 上下文长度是指模型在生成文本时可以记住的最大标记数。更长的上下文窗口使模型能够更好地理解文本中的长距离依赖关系。具有更长上下文的模型可以在文本中构建相隔较远的思想之间的联系，生成更具全球一致性的输出。
- en: During training, the model processes the text data in chunks or fixed-length
    windows. Models need to be trained on lengthy texts to actually leverage long
    contexts. Training sequences must contain documents, books, articles, etc., with
    thousands of tokens.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练过程中，模型以块或固定长度窗口处理文本数据。模型需要在较长文本上进行训练，以真正利用长上下文。训练序列必须包含文档、书籍、文章等，拥有数千个标记。
- en: The length of training data sets a limit on usable context length.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 训练数据的长度限制了可用的上下文长度。
- en: So, why don’t we train models on longer sequences?
  id: totrans-11
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 那么，为什么我们不在更长的序列上训练模型呢？
- en: Not so fast.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 不要急于求成。
- en: Increasing context length increases the number of possible token combinations
    the model must learn to predict accurately.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 增加上下文长度会增加模型必须学习准确预测的可能标记组合的数量。
- en: This enables more robust long-range modeling but also require more memory and
    processing power, leading to higher training costs.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 这使得更强大的长距离建模成为可能，但也需要更多的内存和处理能力，从而导致更高的训练成本。
- en: Without any optimization, computation scales quadratically with context length
    — meaning that a 4096 token model will need 64 times more computation than a 512
    token model.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 没有任何优化的情况下，计算量与上下文长度的平方成正比——这意味着一个4096标记的模型需要比512标记模型多64倍的计算。
- en: You can use sparse or approximate attention methods to reduce the computation
    cost, but they may also affect the model’s accuracy.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用稀疏或近似注意力方法来降低计算成本，但它们也可能影响模型的准确性。
- en: 'Training and using large context language models presents three main challenges:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 训练和使用大上下文语言模型面临三个主要挑战：
- en: Fitting long contexts into the model.
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将长上下文适配到模型中。
- en: Accelerating inference and training so they don’t take forever.
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 加速推理和训练，以便它们不会拖延太久。
- en: Ensuring a high-quality inference that maintains awareness of the full context.
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 确保高质量推理，保持对全部上下文的意识。
- en: Attention is a complex operation
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 注意力是一个复杂的操作
- en: The attention mechanism is the core component of transformer models. It relates
    different positions of a sequence to compute its representation, allowing models
    to focus on relevant parts of the text and understand it better. Scaling transformers
    to longer sequences faces challenges due to the quadratic complexity of full attention.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 注意力机制是变换器模型的核心组件。它将序列的不同位置联系起来以计算其表示，使模型能够关注文本的相关部分并更好地理解它。将变换器扩展到更长的序列面临挑战，因为全注意力的复杂度是二次的。
- en: '![](../Images/5a3218089dd630220ceb86bf9b71802d.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5a3218089dd630220ceb86bf9b71802d.png)'
- en: There are two matrix multiplications involved in self-attention. Image based
    on the original paper. [1]
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 自注意力涉及两个矩阵乘法。图像来源于原始论文。[1]
- en: Stacked self-attention layers allow modeling long-range dependencies in text.
    The standard attention mechanism used in Transformers, which computes the attention
    weights for all possible pairs of input tokens, has a complexity of **O(n²).**
    It means that the computation and memory requirements grow quadratically with
    the input sequence length, limiting Transformers’ scalability and efficiency.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 堆叠自注意力层允许在文本中建模长距离依赖关系。Transformers中使用的标准注意力机制，计算所有可能的输入标记配对的注意力权重，其复杂度为**O(n²)**。这意味着计算和内存需求随着输入序列长度的平方增长，限制了Transformers的可扩展性和效率。
- en: When generating text, the model has to compute the attention matrix first. With
    a 100K context and quadratic attention, it can take minutes before the model starts
    generating text.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在生成文本时，模型必须首先计算注意力矩阵。对于100K上下文和二次注意力，模型可能需要几分钟才能开始生成文本。
- en: Let’s explore methods to improve attention efficiency, from approximations to
    hardware optimizations.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们深入探讨提高注意力效率的方法，从近似到硬件优化。
- en: Improving Attention Efficiency
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 提高注意力效率
- en: 'Reducing the quadratic cost has become an active research area. Proposed methods
    can be grouped into two main categories: **approximating attention** and **exact
    attention** using hardware-aware optimizations.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 减少二次成本已成为一个活跃的研究领域。提出的方法可以分为两大类：**近似注意力**和**通过硬件优化实现的精确注意力**。
- en: Approximation techniques constrain interactions between sequence positions.
    **Sparse attention** limits the number of non-zero attention weights per attention
    head, while **local attention** restricts interactions to a sliding window. These
    approximations reduce computational cost but may degrade accuracy on complex tasks.
    [2]
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 近似技术约束序列位置之间的交互。**稀疏注意力**限制每个注意力头的非零注意力权重数量，而**本地注意力**将交互限制在滑动窗口内。这些近似减少了计算成本，但可能会降低复杂任务的准确性。[2]
- en: Recent work has focused on optimizing attention to leverage GPU architectures.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 最近的工作集中于优化注意力以利用GPU架构。
- en: '**Sparse attention** approximates attention by only computing the attention
    weights for a subset of the input tokens instead of all possible pairs, thus saving
    time and memory. There are different ways to implement sparse attention, such
    as using fixed or static patterns (e.g., local, strided, or block attention) or
    dynamic or adaptive patterns that depend on the input sequence (e.g., entmax or
    dynamic sparse attention).'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '**稀疏注意力**通过仅计算输入标记子集的注意力权重来近似注意力，从而节省时间和内存，而不是计算所有可能的配对。有不同的稀疏注意力实现方式，如使用固定或静态模式（例如，本地、步幅或块注意力）或依赖于输入序列的动态或自适应模式（例如，entmax或动态稀疏注意力）。'
- en: '![](../Images/db8e90a3376384683f9f21dd41c2273e.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/db8e90a3376384683f9f21dd41c2273e.png)'
- en: Quadratic attention (left) computes every possible combination between input
    tokens. Sparse attention (right) limits the computation only to nearby tokens.
    [2]
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 二次注意力（左）计算输入标记之间的每一个可能组合。稀疏注意力（右）将计算限制在附近的标记上。[2]
- en: Sparse attention can improve the efficiency and scalability of Transformers,
    especially for long sequences, but it may also sacrifice some representation power
    and accuracy. Quadratic attention can achieve high performance and quality, but
    it may also be computationally expensive and impractical for large-scale applications.
    Therefore, there is a trade-off between sparsity and complexity in attention mechanisms.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 稀疏注意力可以提高Transformers的效率和可扩展性，尤其是在长序列中，但它也可能牺牲一些表示能力和准确性。二次注意力可以实现高性能和高质量，但它可能计算开销大，不适用于大规模应用。因此，注意力机制中的稀疏性和复杂性之间存在权衡。
- en: Flash Attention
  id: totrans-36
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Flash Attention
- en: The fundamental intuition is to avoid materializing the large N x N attention
    matrix, which requires quadratic reading/writing in the sequence length N.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 基本直觉是避免将大规模的N x N注意力矩阵物化，这需要在序列长度N上进行平方级的读写操作。
- en: '**FlashAttention** applies two techniques — tiling and recomputation. Tiling
    splits the input into blocks, loaded into fast GPU on-chip SRAM. Attention is
    computed block-by-block to avoid materializing the entire matrix. Recomputation
    stores just enough information to reconstruct the attention matrix on-chip during
    backpropagation, avoiding storing the large intermediate. [3]'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '**FlashAttention**应用了两种技术——分块和重新计算。分块将输入拆分成块，加载到快速GPU片上SRAM中。注意力机制按块计算，以避免物化整个矩阵。重新计算在反向传播过程中仅存储足够的信息来重建片上注意力矩阵，避免存储大型中间结果。[3]'
- en: The authors analyze the IO complexity, proving FlashAttention requires O(N²/M)
    memory accesses versus O(N²) for standard attention, where M is the SRAM size.
    This IO-awareness allows FlashAttention to run faster despite increased FLOPs
    from recomputation.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 作者分析了IO复杂性，证明FlashAttention需要O(N²/M)的内存访问，而标准注意力机制需要O(N²)，其中M是SRAM大小。这种IO意识使得FlashAttention在重新计算带来的FLOP增加的情况下运行得更快。
- en: Experiments validate the speedups — FlashAttention trains BERT 15% faster than
    the MLPerf record, GPT-2 3x faster, and Long Range Arena 2.4x faster.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 实验验证了加速效果——FlashAttention训练BERT比MLPerf记录快15%，GPT-2快3倍，Long Range Arena快2.4倍。
- en: '![](../Images/91d89ae3fb6519710fb048cc8cf01905.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/91d89ae3fb6519710fb048cc8cf01905.png)'
- en: Instead of computing the whole attention matrix on the slower HBM, FlashAttention
    copies blocks to the SRAM. [3]
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: FlashAttention不是在较慢的HBM上计算整个注意力矩阵，而是将块复制到SRAM中。[3]
- en: This idea was further developer in **FlashAttention-2**. The improvements focus
    on enhancing parallelism across sequence blocks and optimizing work partitioning
    between thread blocks and warps on GPUs. Key techniques include reducing non-matrix
    multiply operations, partitioning attention computation across threads to increase
    occupancy, and distributing work between warps to reduce shared memory traffic.
    Empirical validation shows FlashAttention-2 achieves around 2x speedup over FlashAttention,
    reaching up to 73% of theoretical peak FLOPs on A100 GPUs. When used to train
    GPT models end-to-end, training throughput reaches 225 TFLOPs/s per A100, translating
    to 1.3x faster training than FlashAttention. [4]
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 这个想法在**FlashAttention-2**中得到了进一步的发展。改进的重点是增强序列块之间的并行性，并优化线程块和GPU上的warp之间的工作分配。关键技术包括减少非矩阵乘法操作，将注意力计算分配到线程中以增加占用率，以及在warps之间分配工作以减少共享内存流量。实验证明，FlashAttention-2在FlashAttention上实现了大约2倍的加速，在A100
    GPU上达到了理论峰值FLOP的73%。用于端到端训练GPT模型时，训练吞吐量达到了每个A100 225 TFLOPs/s，比FlashAttention快1.3倍。[4]
- en: The improvements promise to enable training models on much longer sequences
    than before at a similar cost. Accelerating attention speeds up inference and
    training, but fitting text into the model while maintaining high output quality
    remains an issue.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 这些改进有望在相似成本下实现对比以往更长序列的模型训练。加速注意力机制可以加快推理和训练速度，但在保持高输出质量的同时将文本适配到模型中仍然是一个问题。
- en: Let’s see what to do about it.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何解决这个问题。
- en: Models are pre-trained on fixed-length sequences
  id: totrans-46
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型在固定长度的序列上进行预训练
- en: An efficient training and inference is not enough to have an high-quality model.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 高效的训练和推理还不足以获得高质量的模型。
- en: 'There are two main paradigms of context length extension: **fine-tuned extrapolation**,
    where the LLM further updates its weights on longer contexts, and **zero-shot
    extrapolation**, where the model is evaluated on long contexts with no change
    to weights from the short context training. [5]'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 上下文长度扩展主要有两种范式：**微调外推**，其中LLM在更长的上下文上进一步更新其权重，以及**零-shot外推**，其中模型在长上下文上进行评估，而短上下文训练的权重不发生变化。[5]
- en: To extend context, most approaches focus on modifying the positional encoding
    system used in the transformer attention mechanism to indicate where tokens are
    located in the input sequence. The idea is that representing longer input sequences
    in the positional encoding will allow the LLM to attend to those longer sequences.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 为了扩展上下文，大多数方法专注于修改变换器注意力机制中使用的位置编码系统，以指示标记在输入序列中的位置。其想法是，在位置编码中表示更长的输入序列将允许LLM关注这些更长的序列。
- en: '![](../Images/7272a89e1df22f57dac6474602c4425d.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7272a89e1df22f57dac6474602c4425d.png)'
- en: Positional encoding is used to make your model understand the order in the sentence.
    Image based on the original paper. [1]
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 位置编码用于使你的模型理解句子中的顺序。基于原始论文的图像。[1]
- en: Positional encoding is used to make your model understand the order in the sentence.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 位置编码用于使你的模型理解句子中的顺序。
- en: '**Positional embeddings** are added to the input token embeddings before feeding
    them into the model to enable the model to use the order of the sequence. They
    map the discrete positional IDs to continuous embedding vectors.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '**位置嵌入**在将输入标记嵌入送入模型之前被添加，以便模型能够使用序列的顺序。它们将离散的位置 ID 映射到连续的嵌入向量。'
- en: Usually, positional embeddings are defined algorithmically based on the position
    IDs. In the original Transformers paper, they used a trigonometric function, where
    each dimension of the positional embedding vector follows a sinusoidal pattern.
    [1]
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，位置嵌入是基于位置 ID 算法定义的。在原始 Transformers 论文中，他们使用了三角函数，其中位置嵌入向量的每个维度都遵循正弦波模式。[1]
- en: In LLaMa, **Rotary Position Embeddings (RoPE)** are used, where positional embeddings
    are computed on the fly using rotary embeddings. The token and positional dimensions
    are rotated together using trigonometric functions. The rotation amounts are determined
    by the position ID.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在 LLaMa 中，使用了**旋转位置嵌入（RoPE）**，其中位置嵌入通过旋转嵌入动态计算。标记和位置维度通过三角函数一起旋转。旋转量由位置 ID 决定。
- en: Regardless of how positional embeddings are generated, models struggle to generalize
    to sequences longer than what was seen during pretraining. *(context extrapolation)*
    Sinusoidal position embedding methods have limited extrapolation ability, only
    allowing for a few dozen more tokens during inference before performance degrades.
    [6]
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 无论位置嵌入如何生成，模型在处理比预训练时更长的序列时都很难进行泛化。*(上下文外推)* 正弦位置嵌入方法的外推能力有限，在推断期间只能再处理几十个标记，然后性能会下降。[6]
- en: Newer approaches like linear scaling and position interpolation have been introduced
    to address this limitation.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 新的方法如线性缩放和位置插值已经被引入以解决这一限制。
- en: Linear scaling
  id: totrans-58
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 线性缩放
- en: With **linear scaling**, the positional embeddings are rescaled to adapt the
    model to different sequence lengths. If the pre-trained model has embeddings up
    to length L, then for inference on a sequence of length N, each positional embedding
    vector is multiplied by N/L. This cheaply approximates embeddings for the new
    length while retaining the pre-trained embedding properties. Linear scaling improves
    performance on long sequences significantly. However, the model still underperforms
    on sequences much longer than the pretrained length. The linear scaling process
    destroys information by collapsing multiple position embeddings together.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 使用**线性缩放**，位置嵌入被重新缩放以适应不同的序列长度。如果预训练模型的嵌入长度为 L，则在对长度为 N 的序列进行推断时，每个位置嵌入向量会乘以
    N/L。这廉价地近似了新长度的嵌入，同时保留了预训练嵌入的特性。线性缩放显著提升了长序列的性能。然而，模型在比预训练长度长得多的序列上仍然表现不佳。线性缩放过程通过将多个位置嵌入合并在一起来破坏信息。
- en: '![](../Images/e82ac836770d64f13d2925c96376ca8b.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e82ac836770d64f13d2925c96376ca8b.png)'
- en: In position interpolation, the context range is not extended. Instead, there
    are more intermediate positions. [7]
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在位置插值中，上下文范围不会扩展。相反，有更多的中间位置。[7]
- en: '**Linear scaling/interpolation** performs best overall for extending context,
    with promise also shown in the truncated basis method. Further gains are achieved
    by using a longer scaling factor at evaluation time.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '**线性缩放/插值**在扩展上下文时整体表现最佳，同时在截断基方法中也显示出了一定的前景。通过在评估时使用更长的缩放因子，可以获得进一步的提升。'
- en: This method was concurrently researched by kaiokendev and Meta. [8]
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法由 kaiokendev 和 Meta 同时研究。[8]
- en: The first released YaRN, a model able to achieve 128k context length.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 首个发布的 YaRN 是一个能够实现 128k 上下文长度的模型。
- en: Later, three more fine-tuned models with 8k, 16k and 32k context were released,
    called Giraffe. [5]
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 后来，发布了三个额外微调的模型，分别具有 8k、16k 和 32k 的上下文，称为 Giraffe。[5]
- en: Attention with Linear Biases
  id: totrans-66
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 带有线性偏差的注意力
- en: '**ALiBi** introduces a simpler approach that eliminates positional embeddings.
    Instead, it negatively biases attention scores between queries and keys with a
    penalty proportional to their distance. This inductive bias towards recent contexts
    allows extrapolation at low computational cost. Experiments show a 1.3B parameter
    model trained on 1024 tokens with ALiBi achieving the same perplexity as a sinusoidal
    model trained on 2048 tokens when tested on 2048 tokens, training faster and using
    less memory. [6]'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '**ALiBi** 引入了一种更简单的方法，消除了位置嵌入。相反，它通过一个与查询和键之间距离成比例的惩罚来负向偏置注意力得分。这种对近期上下文的归纳偏置允许以低计算成本进行推断。实验表明，使用
    ALiBi 训练的 1.3B 参数模型在 2048 个标记上测试时，与在 2048 个标记上训练的正弦模型具有相同的困惑度，训练速度更快，内存使用更少。[6]'
- en: MosaicML’s **MPT-7B** model leverages the **ALiBi** architecture to enable extrapolation
    to extreme context lengths up to 65k tokens, far surpassing the limits of other
    open-source models. By replacing positional embeddings with ALiBi, the model gains
    the ability to handle inputs of arbitrary length during inference without being
    constrained to a fixed context window. This was demonstrated through fine-tuning
    MPT-7B into MPT-7B-StoryWriter-65k+ using 65k token excerpts from fiction books,
    allowing it to generate coherent continuations from the full text of The Great
    Gatsby at 68k tokens.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: MosaicML 的 **MPT-7B** 模型利用 **ALiBi** 架构，实现了对极长上下文长度（高达 65k 个标记）的外推，远远超过了其他开源模型的限制。通过用
    ALiBi 替代位置嵌入，该模型在推理过程中能够处理任意长度的输入，而不受限于固定的上下文窗口。这通过将 MPT-7B 微调为 MPT-7B-StoryWriter-65k+
    使用 65k 个标记的小说摘录来证明，使其能够在 68k 个标记的《了不起的盖茨比》完整文本中生成连贯的续篇。
- en: The choice of positional embedding approach depends on considerations like model
    size, expected sequence lengths, and how important generalization is for the problem.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 位置嵌入方法的选择取决于模型大小、预期序列长度以及问题对泛化的重要性等因素。
- en: There remains significant room for improvement, as all methods degrade in accuracy
    with increasing length, even when perplexity is reasonable.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 仍然有很大的改进空间，因为所有方法在长度增加时准确度都会下降，即使困惑度合理。
- en: 'Bonus model: RWKV'
  id: totrans-71
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 奖励模型：RWKV
- en: Another way to improve attention would be to *not use it*.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 改进注意力的另一种方法是*不使用它*。
- en: The **Receptance Weighted Key Value (RWKV)** model introduced by Peng et al.
    aims to reconcile the trade-off between computational efficiency and model performance
    in sequence processing tasks. RWKV combines aspects of both Transformers and RNNs
    into a novel architecture that achieves linear scaling. [9]
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '**Receptance Weighted Key Value (RWKV)** 模型由 Peng 等人提出，旨在调和序列处理任务中的计算效率和模型性能之间的权衡。RWKV
    将 Transformers 和 RNN 的某些方面结合成一种新颖的架构，实现了线性扩展。[9]'
- en: A key innovation is the reformulation of the attention mechanism to use scalar
    interactions rather than dot products, eliminating the quadratic bottleneck.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 一个关键的创新是将注意力机制重新表述为使用标量交互而不是点积，从而消除二次瓶颈。
- en: '![](../Images/b3e32afb0c0bb84770d2a397b5dd8267.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b3e32afb0c0bb84770d2a397b5dd8267.png)'
- en: RWKV architecture for language modeling. [9]
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 用于语言建模的 RWKV 架构。[9]
- en: '**RWKV** implements a variant of linear attention without approximation for
    improved efficiency. The model parallelizes computations in training like Transformers
    but behaves as an RNN decoder at inference, yielding constant speed and memory
    with unlimited context.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '**RWKV** 实现了一种线性注意力的变体，没有近似，以提高效率。该模型在训练中像 Transformers 一样并行计算，但在推理时表现为 RNN
    解码器，实现了速度和内存的恒定，能够处理无限上下文。'
- en: Experiments demonstrate RWKV is competitive with Transformers on language tasks
    while requiring lower computational cost.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 实验表明 RWKV 在语言任务中与 Transformers 具有竞争力，同时需要更低的计算成本。
- en: However, given its recurrent nature, it might be necessary to adapt the prompts
    carefully and it might struggle with maintaining detailed tracking over extremely
    long sequences.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，鉴于其递归特性，可能需要仔细调整提示，并且它可能在处理极长序列时难以保持详细跟踪。
- en: '[**Raven**](https://huggingface.co/RWKV/rwkv-raven-14b) is an example of an
    RWKV model. This model competes with the smallest Llama-based models. In my tests,
    Raven showed a good understanding of grammar and semantic meaning, but tended
    to hallucinate often.'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '[**Raven**](https://huggingface.co/RWKV/rwkv-raven-14b) 是一个 RWKV 模型的示例。这个模型与最小的基于
    Llama 的模型竞争。在我的测试中，Raven 显示出对语法和语义意义的良好理解，但倾向于经常产生幻觉。'
- en: '**Conclusion**'
  id: totrans-81
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**结论**'
- en: Language models benefit from longer contexts. However, longer contexts increase
    training costs quadratically due to the standard attention mechanism. Recent research
    focuses on approximating attention to improve efficiency. Methods like sparse
    attention and linear attention help. Optimizing hardware efficiency also works,
    as FlashAttention shows by leveraging GPU memory hierarchies.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 语言模型从更长的上下文中受益。然而，由于标准注意力机制的原因，更长的上下文会使训练成本呈二次方增加。最近的研究集中在通过近似注意力来提高效率。稀疏注意力和线性注意力等方法有所帮助。优化硬件效率也是一种有效的手段，正如
    FlashAttention 通过利用 GPU 内存层次结构所展示的那样。
- en: Pretrained models still struggle with contexts longer than those seen during
    training. Techniques like linear scaling of positional embeddings and ALiBi enable
    longer contexts. Fine-tuning on longer contexts further adapts models.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 预训练模型在处理比训练期间看到的更长的上下文时仍然会遇到困难。像位置嵌入的线性缩放和 ALiBi 等技术能够支持更长的上下文。对更长上下文的微调进一步适应了模型。
- en: State-of-the-art models push context lengths far beyond previous limits.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 目前最先进的模型将上下文长度大大超越了之前的限制。
- en: YaRN and Giraffe use position iterpolation. MPT-65k uses ALiBi for 65,000 token
    contexts. RWKV proposes linear-scaling attention, allowing unlimited context at
    inference.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: YaRN 和 Giraffe 使用位置插值。MPT-65k 使用 ALiBi 处理 65,000 个标记的上下文。RWKV 提出了线性缩放注意力机制，允许在推理时使用无限长度的上下文。
- en: Longer contexts empower models to process full documents and books, but challenges
    remain in maintaining output quality over extended lengths.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 更长的上下文使得模型能够处理完整的文档和书籍，但在维持输出质量方面仍面临挑战。
- en: '*If you enjoyed this article, join* [***Text Generation***](https://textgeneration.substack.com/)
    *— our newsletter has two weekly posts with the latest insights on Generative
    AI and Large Language Models.*'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '*如果你喜欢这篇文章，加入* [***文本生成***](https://textgeneration.substack.com/) *—— 我们的新闻通讯每周有两篇最新的生成
    AI 和大型语言模型的见解。*'
- en: '*Also, you can find me on* [***LinkedIn***](https://www.linkedin.com/in/driccio/)***.***'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '*此外，你还可以在* [***LinkedIn***](https://www.linkedin.com/in/driccio/)***上找到我。***'
- en: References
  id: totrans-89
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] [[1706.03762] Attention Is All You Need (arxiv.org)](https://arxiv.org/abs/1706.03762)'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] [[1706.03762] 注意力机制即一切 (arxiv.org)](https://arxiv.org/abs/1706.03762)'
- en: '[2] [[1904.10509] Generating Long Sequences with Sparse Transformers (arxiv.org)](https://arxiv.org/abs/1904.10509)'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] [[1904.10509] 使用稀疏变换器生成长序列 (arxiv.org)](https://arxiv.org/abs/1904.10509)'
- en: '[3] [[2205.14135] FlashAttention: Fast and Memory-Efficient Exact Attention
    with IO-Awareness (arxiv.org)](https://arxiv.org/abs/2205.14135)'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] [[2205.14135] FlashAttention: 快速且内存高效的精确注意力机制与 IO 关注 (arxiv.org)](https://arxiv.org/abs/2205.14135)'
- en: '[4] [[2307.08691] FlashAttention-2: Faster Attention with Better Parallelism
    and Work Partitioning (arxiv.org)](https://arxiv.org/abs/2307.08691)'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] [[2307.08691] FlashAttention-2: 更快的注意力机制，具备更好的并行性和工作划分 (arxiv.org)](https://arxiv.org/abs/2307.08691)'
- en: '[5] [[2308.10882] Giraffe: Adventures in Expanding Context Lengths in LLMs
    (arxiv.org)](https://arxiv.org/abs/2308.10882)'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '[5] [[2308.10882] Giraffe: 扩展 LLM 上下文长度的冒险 (arxiv.org)](https://arxiv.org/abs/2308.10882)'
- en: '[6] [[2108.12409] Train Short, Test Long: Attention with Linear Biases Enables
    Input Length Extrapolation (arxiv.org)](https://arxiv.org/abs/2108.12409)'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '[6] [[2108.12409] 训练短序列，测试长序列：具有线性偏置的注意力机制实现输入长度外推 (arxiv.org)](https://arxiv.org/abs/2108.12409)'
- en: '[7] [[2306.15595] Extending Context Window of Large Language Models via Positional
    Interpolation (arxiv.org)](https://arxiv.org/abs/2306.15595)'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '[7] [[2306.15595] 通过位置插值扩展大型语言模型的上下文窗口 (arxiv.org)](https://arxiv.org/abs/2306.15595)'
- en: '[8] [[2309.00071] YaRN: Efficient Context Window Extension of Large Language
    Models (arxiv.org)](https://arxiv.org/abs/2309.00071)'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '[8] [[2309.00071] YaRN: 大型语言模型的高效上下文窗口扩展 (arxiv.org)](https://arxiv.org/abs/2309.00071)'
- en: '[9] [[2305.13048] RWKV: Reinventing RNNs for the Transformer Era (arxiv.org)](https://arxiv.org/abs/2305.13048)'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '[9] [[2305.13048] RWKV: 为 Transformer 时代重新定义 RNN (arxiv.org)](https://arxiv.org/abs/2305.13048)'
