- en: Why OpenAI’s API Is More Expensive for Non-English Languages
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为什么 OpenAI 的 API 对非英语语言更昂贵
- en: 原文：[https://towardsdatascience.com/why-openais-api-is-more-expensive-for-non-english-languages-553da4a1eecc](https://towardsdatascience.com/why-openais-api-is-more-expensive-for-non-english-languages-553da4a1eecc)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/why-openais-api-is-more-expensive-for-non-english-languages-553da4a1eecc](https://towardsdatascience.com/why-openais-api-is-more-expensive-for-non-english-languages-553da4a1eecc)
- en: 'Beyond words: How byte pair encoding and Unicode encoding factor into pricing
    disparities'
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 超越文字：字节对编码和Unicode编码如何影响定价差异
- en: '[](https://medium.com/@iamleonie?source=post_page-----553da4a1eecc--------------------------------)[![Leonie
    Monigatti](../Images/4044b1685ada53a30160b03dc78f9626.png)](https://medium.com/@iamleonie?source=post_page-----553da4a1eecc--------------------------------)[](https://towardsdatascience.com/?source=post_page-----553da4a1eecc--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----553da4a1eecc--------------------------------)
    [Leonie Monigatti](https://medium.com/@iamleonie?source=post_page-----553da4a1eecc--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@iamleonie?source=post_page-----553da4a1eecc--------------------------------)[![Leonie
    Monigatti](../Images/4044b1685ada53a30160b03dc78f9626.png)](https://medium.com/@iamleonie?source=post_page-----553da4a1eecc--------------------------------)[](https://towardsdatascience.com/?source=post_page-----553da4a1eecc--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----553da4a1eecc--------------------------------)
    [Leonie Monigatti](https://medium.com/@iamleonie?source=post_page-----553da4a1eecc--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----553da4a1eecc--------------------------------)
    ·7 min read·Aug 16, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----553da4a1eecc--------------------------------)
    ·阅读时间 7 分钟·2023年8月16日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/7dbc785874f6574c212f692b9a203709.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7dbc785874f6574c212f692b9a203709.png)'
- en: How can it be that the phrase “Hello world” has two tokens in English and 12
    tokens in Hindi?
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 怎么会“Hello world”这个短语在英语中有两个令牌，而在印地语中却有12个令牌呢？
- en: After publishing [my recent article on how to estimate the cost for OpenAI’s
    API,](https://medium.com/towards-data-science/easily-estimate-your-openai-api-costs-with-tiktoken-c17caf6d015e)
    I received an interesting comment that someone had noticed that the OpenAI API
    is much more expensive in other languages, such as ones using Chinese, Japanese,
    or Korean (CJK) characters, than in English.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在发布了 [我最近关于如何估算 OpenAI API 成本的文章](https://medium.com/towards-data-science/easily-estimate-your-openai-api-costs-with-tiktoken-c17caf6d015e)
    后，我收到了一条有趣的评论，有人注意到 OpenAI API 在其他语言中的费用远高于英语，例如使用汉字、日语或韩文 (CJK) 字符的语言。
- en: '![](../Images/8e0c5de69c8cc1d758147b74a591882f.png)'
  id: totrans-9
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8e0c5de69c8cc1d758147b74a591882f.png)'
- en: Comment by a reader on [my recent article on how to estimate the cost for OpenAI’s
    API with the](https://medium.com/towards-data-science/easily-estimate-your-openai-api-costs-with-tiktoken-c17caf6d015e)
    `[tiktoken](https://medium.com/towards-data-science/easily-estimate-your-openai-api-costs-with-tiktoken-c17caf6d015e)`
    [library](https://medium.com/towards-data-science/easily-estimate-your-openai-api-costs-with-tiktoken-c17caf6d015e)
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 有读者在 [我最近关于如何估算 OpenAI API 成本的文章](https://medium.com/towards-data-science/easily-estimate-your-openai-api-costs-with-tiktoken-c17caf6d015e)
    上评论了 `[tiktoken](https://medium.com/towards-data-science/easily-estimate-your-openai-api-costs-with-tiktoken-c17caf6d015e)`
    [库](https://medium.com/towards-data-science/easily-estimate-your-openai-api-costs-with-tiktoken-c17caf6d015e)
- en: 'I wasn’t aware of this issue, but quickly realized that this is an active research
    field: At the beginning of this year, a paper called “Language Model Tokenizers
    Introduce Unfairness Between Languages” by Petrov et al. [2] showed that the “same
    text translated into different languages can have drastically different tokenization
    lengths, with differences up to 15 times in some cases.”'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 我之前并未意识到这个问题，但很快意识到这是一个活跃的研究领域：今年年初，Petrov 等人发表了一篇名为“语言模型分词器在语言之间引入不公平性”的论文[2]，该论文显示“同一文本翻译成不同语言后，其分词长度可能会有剧烈差异，在某些情况下差异可达15倍。”
- en: As a refresher, tokenization is the process of splitting a text into a list
    of tokens, which are common sequences of characters in a text.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 作为补充，分词是将文本拆分为一系列令牌的过程，这些令牌是文本中的常见字符序列。
- en: '![](../Images/ac4b4aa8e2cd8f729c9e7e3c10d1efb9.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ac4b4aa8e2cd8f729c9e7e3c10d1efb9.png)'
- en: An example for Tokenization
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 分词的一个示例
- en: The difference in tokenization lengths is an issue because [the OpenAI API is
    billed in units of 1,000 tokens](https://openai.com/pricing). Thus, if you have
    up to 15 times more tokens in a comparable text, this will result in 15 times
    the API costs.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 符号化长度的差异是一个问题，因为[OpenAI API按1,000个符号计费](https://openai.com/pricing)。因此，如果你在类似的文本中有多达15倍的符号，这将导致15倍的API费用。
- en: 'Experiment: Number of Tokens in Different Languages'
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实验：不同语言中的符号数量
- en: 'Let’s translate the phrase “Hello world” into Japanese (こんにちは世界) and transcribe
    it into Hindi (हैलो वर्ल्ड). When we tokenize the new phrases with the `cl100k_base`
    tokenizer used in OpenAI’s GPT models, we get the following results (you can find
    the [Code](#12fb) I used for these experiments at the end of this article):'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将短语“Hello world”翻译成日语（こんにちは世界）并转录成印地语（हैलो वर्ल्ड）。当我们用OpenAI的GPT模型中使用的`cl100k_base`分词器对这些新短语进行符号化时，得到以下结果（你可以在本文末尾找到我用于这些实验的[代码](#12fb)）：
- en: '![](../Images/2f059cf750b6225d0a8ef3e7f86a468e.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2f059cf750b6225d0a8ef3e7f86a468e.png)'
- en: Number of letters and tokens (`cl100k_base`) for the phrase “Hello world” in
    English, Japanese, and Hindi
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: “Hello world”在英语、日语和印地语中的字母和符号数量（`cl100k_base`）
- en: 'From the above graph, we can make two interesting observations:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 从上图中，我们可以得出两个有趣的观察：
- en: The number of letters for this phrase is highest in English and lowest in Hindi,
    but the number of resulting tokens is lowest in English but highest in Hindi.
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这个短语的字母数量在英语中最多，而在印地语中最少，但产生的符号数量在英语中最少，而在印地语中最多。
- en: In Hindi, there are more tokens than there are letters.
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在印地语中，符号的数量多于字母的数量。
- en: How can that happen?
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 这怎么可能发生？
- en: Fundamentals
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基础知识
- en: To understand why we end up with more tokens for the same phrase in languages
    other than English, we need to review two fundamental concepts of byte pair encoding
    and Unicode.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 要理解为什么在除英语之外的语言中，相同的短语会产生更多的符号，我们需要回顾字节对编码和Unicode的两个基本概念。
- en: Byte Pair Encoding
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 字节对编码
- en: The Byte Pair Encoding (BPE) algorithm was originally invented as a compression
    algorithm by Gage [1] in 1994.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 字节对编码（BPE）算法最初是由Gage [1]于1994年发明的，用作一种压缩算法。
- en: “The [BPE] algorithm compresses data by finding the most frequently occurring
    pairs of adjacent **bytes** in the data and replacing all instances of the pair
    with a byte that was not in the original data. The algorithm repeats this process
    until no further compression is possible, either because there are no more frequently
    occurring pairs or there are no more unused bytes to represent pairs.” [1]
  id: totrans-28
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “[BPE]算法通过查找数据中最频繁出现的相邻**字节**对来压缩数据，并用一个原始数据中不存在的字节替换所有出现的对。该算法重复此过程，直到无法进一步压缩，无论是因为没有更多频繁出现的对，还是没有更多的未使用字节来表示对。”[1]
- en: Let’s go through the example from the original paper [1]. Let’s say you have
    the smallest corpus of text consisting of the string “ABABCABCD”.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过原始论文中的例子[1]。假设你有一个最小的文本语料库，由字符串“ABABCABCD”组成。
- en: 1\. For every pair of bytes (in this example, characters), you will count its
    occurrences in the corpus as shown below.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 1\. 对于每一对字节（在这个例子中，即字符），你将统计它在语料库中的出现次数，如下所示。
- en: '[PRE0]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 2\. Take the pair of bytes with the highest number of occurrences and replace
    it with an unused character. In this case, we will replace the pair “AB” with
    “X”.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 2\. 取出现次数最多的一对字节，并用一个未使用的字符替换它。在这种情况下，我们将把“AB”这对字节替换为“X”。
- en: '[PRE1]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 3\. Repeat step 2 until no further compression is possible or no more unused
    bytes (in this example, characters) are available.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 3\. 重复步骤2，直到无法进一步压缩或没有更多的未使用字节（在这个例子中，即字符）为止。
- en: '[PRE2]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Unicode
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Unicode
- en: Unicode is an encoding standard that defines how different characters are represented
    in unique numbers called **code points**. In this article, we are not going to
    cover all details of Unicode. Here is an [excellent StackOverflow answer](https://stackoverflow.com/a/15128103)
    if you need a refresher.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: Unicode是一种编码标准，定义了不同字符如何用称为**码点**的唯一数字表示。在本文中，我们不会涵盖Unicode的所有细节。如果你需要复习，可以参考这个[优秀的StackOverflow回答](https://stackoverflow.com/a/15128103)。
- en: What you need to know for the following explanation is that if your text is
    encoded in UTF-8, characters of different languages will require different amounts
    of bytes.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 对于以下解释，你需要知道的是，如果你的文本是以UTF-8编码的，不同语言的字符将需要不同数量的字节。
- en: As you can see from the table below, letters of the English language can be
    represented with ASCII characters and only require 1 byte. But, e.g., Greek characters
    require 2 bytes, and Japanese characters require 3 bytes.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 从下面的表格中可以看出，英语字母可以用ASCII字符表示，只需要1个字节。但例如，希腊字符需要2个字节，而日文字符需要3个字节。
- en: '![](../Images/e989e7dfa70068946456ec6be4fa7f6f.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e989e7dfa70068946456ec6be4fa7f6f.png)'
- en: (Inspired by [Wikipedia article on UTF-8](https://en.wikipedia.org/wiki/UTF-8)
    and [StackOverflow answer](https://stackoverflow.com/a/15128103))
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: （灵感来自[维基百科关于UTF-8的文章](https://en.wikipedia.org/wiki/UTF-8)和[StackOverflow的回答](https://stackoverflow.com/a/15128103)）
- en: Looking Under The Hood
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索内部机制
- en: Now that we understand that characters of different languages require different
    amounts of bytes to be represented numerically and that the tokenizer used by
    OpenAI’s GPT models is a BPE algorithm, which tokenizes on the byte level, let’s
    have a deeper look at our opening experiment.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们理解了不同语言的字符在数值上需要不同的字节量，并且OpenAI的GPT模型使用的分词器是BPE算法，它在字节级别上进行分词，让我们深入探讨一下我们的开篇实验。
- en: English
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 英语
- en: 'First, let’s look at the vanilla example of tokenization in English:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们看看英文分词的普通例子：
- en: '![](../Images/8d8734f317677eb4a8e87496351c34e2.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8d8734f317677eb4a8e87496351c34e2.png)'
- en: Tokenizing the phrase “Hello world”
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 分词短语“Hello world”
- en: 'From the above visualization, we can make the following observations:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 从上面的可视化中，我们可以得出以下观察：
- en: One letter equals one code point
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个字母等于一个代码点
- en: One Unicode code point equals 1 byte
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个Unicode代码点等于1个字节
- en: The BPE tokenizes 5 bytes for “Hello” and 6 bytes for “ world” into two separate
    tokens
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: BPE将“Hello”分词为5个字节，将“ world”分词为6个字节，分成两个独立的标记
- en: 'This observation matches the statement on the [OpenAI’s tokenizer’s site](https://platform.openai.com/tokenizer):'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 这一观察结果与[OpenAI分词器网站](https://platform.openai.com/tokenizer)上的声明一致：
- en: “A helpful rule of thumb is that one token generally corresponds to ~4 characters
    of text for common English text.”
  id: totrans-53
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “一个有用的经验法则是，一个标记通常对应于~4个字符的常见英文文本。”
- en: Notice how it says “for common English text”? Let’s look at texts that are not
    English.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 注意到它说“对于常见的英文文本”？让我们看看非英文文本。
- en: Japanese
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 日语
- en: 'Now what happens in languages in which a letter does not correspond to one
    byte but multiple bytes? Let’s look at the phrase “Hello world” translated into
    Japanese, which uses CJK characters that are 3 bytes long in UTF-8 encoding:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 那么在那些一个字母不对应一个字节而是多个字节的语言中会发生什么呢？让我们看看翻译成日语的“Hello world”，它使用了在UTF-8编码中为3字节的CJK字符：
- en: '![](../Images/841162a2359180fcae0c32e3248f8102.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/841162a2359180fcae0c32e3248f8102.png)'
- en: Tokenizing the phrase “こんにちは世界”
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 分词短语“こんにちは世界”
- en: 'From the above visualization, we can make the following observations:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 从上面的可视化中，我们可以得出以下观察：
- en: One letter equals one code point
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个字母等于一个代码点
- en: One Unicode code point equals 3 bytes
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个Unicode代码点等于3个字节
- en: The BPE tokenizes 15 bytes for こんにちは (Japanese for “Hello”) into a single token
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: BPE将15个字节的こんにちは（“Hello”的日文）分词为一个单独的标记
- en: But the letter 界 is tokenized into a single token
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 但字母界被分词为一个单独的标记
- en: The letter 世 is tokenized into two tokens
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 字母世被分词为两个标记
- en: Hindi
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 印地语
- en: 'It becomes even crazier in languages where one letter doesn’t equal one code
    point but is made of multiple code points. Let’s look at the phrase “Hello world”
    transcribed into Hindi. The Devanāgarī alphabet used for Hindi has characters
    that have to be split into multiple code points with each code point requiring
    3 bytes:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在那些一个字母不等于一个代码点而是由多个代码点组成的语言中，情况变得更加复杂。让我们看看“Hello world”翻译成印地语的情况。用于印地语的天城文字符表中的字符必须拆分成多个代码点，每个代码点需要3个字节：
- en: '![](../Images/47dcfd843ce0d3de1a0bd7bf8b200432.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/47dcfd843ce0d3de1a0bd7bf8b200432.png)'
- en: Tokenizing the phrase “हैलो वर्ल्ड”
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 分词短语“हैलो वर्ल्ड”
- en: 'From the above visualization, we can make the following observations:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 从上面的可视化中，我们可以得出以下观察：
- en: One letter can be made up of multiple Unicode code points (e.g., the letter
    है is made from combining the code points ह and ै)
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个字母可以由多个Unicode代码点组成（例如，字母है由代码点ह和ै组合而成）
- en: One Unicode code point equals 3 bytes
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个Unicode代码点等于3个字节
- en: Similarly to the Japanese letter 世, one code point can be divided into two tokens
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与日文字符世类似，一个代码点可以被分为两个标记
- en: Some tokens span more than one but less than two letters (e.g., token id 31584)
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一些标记跨越一个以上但少于两个字母（例如，标记ID 31584）
- en: Summary
  id: totrans-74
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: This article explored how the same phrase “Hello world” translated to Japanese
    and transcribed to Hindi is tokenized. First, we learned that the tokenizer used
    in OpenAI’s GPt models tokenizes on the byte level. Additionally, we saw that
    Japanese and Devanāgarī characters require more than one byte to represent a character
    in contrast to English. Thus, we saw that the UFT-8 encoding and BPE tokenizer
    play a big role in the resulting number of tokens and impact the API costs.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 本文探讨了相同短语“Hello world”翻译成日语并转录成印地语后是如何进行分词的。首先，我们了解到 OpenAI 的 GPT 模型使用的分词器是在字节级别进行分词的。此外，我们还发现，日语和天城文字符号需要多个字节来表示一个字符，而不是像英语那样。因此，我们看到
    UTF-8 编码和 BPE 分词器在最终令牌数量中发挥了重要作用，并影响了 API 成本。
- en: Of course, different factors, such as the fact that GPT models are not trained
    equally on multilingual texts, influence tokenization. At the time of writing,
    this issue is an active research field, and I am curious to see different solutions.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，不同的因素，例如 GPT 模型在多语言文本上的训练不均等，会影响分词。在撰写时，这个问题是一个活跃的研究领域，我很期待看到不同的解决方案。
- en: Enjoyed This Story?
  id: totrans-77
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 享受这个故事吗？
- en: '[*Subscribe for free*](https://medium.com/subscribe/@iamleonie) *to get notified
    when I publish a new story.*'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '[*免费订阅*](https://medium.com/subscribe/@iamleonie) *以便在我发布新故事时收到通知。*'
- en: '[](https://medium.com/@iamleonie/subscribe?source=post_page-----553da4a1eecc--------------------------------)
    [## Get an email whenever Leonie Monigatti publishes.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@iamleonie/subscribe?source=post_page-----553da4a1eecc--------------------------------)
    [## 每当 Leonie Monigatti 发布新内容时，你会收到电子邮件。'
- en: Get an email whenever Leonie Monigatti publishes. By signing up, you will create
    a Medium account if you don’t already…
  id: totrans-80
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 每当 Leonie Monigatti 发布新内容时，你会收到电子邮件。通过注册，如果你还没有 Medium 帐户，你将创建一个…
- en: medium.com](https://medium.com/@iamleonie/subscribe?source=post_page-----553da4a1eecc--------------------------------)
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: medium.com](https://medium.com/@iamleonie/subscribe?source=post_page-----553da4a1eecc--------------------------------)
- en: '*Find me on* [*LinkedIn*](https://www.linkedin.com/in/804250ab/),[*Twitter*](https://twitter.com/helloiamleonie)*,
    and* [*Kaggle*](https://www.kaggle.com/iamleonie)*!*'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '*在* [*LinkedIn*](https://www.linkedin.com/in/804250ab/),[*Twitter*](https://twitter.com/helloiamleonie)*和*
    [*Kaggle*](https://www.kaggle.com/iamleonie)*上找到我！*'
- en: References
  id: totrans-83
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: Image References
  id: totrans-84
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 图片参考
- en: If not otherwise stated, all images are created by the author.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 如果没有另行说明，所有图片均由作者创作。
- en: Web & Literature
  id: totrans-86
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 网络与文学
- en: '[1] Gage, P. (1994). A new algorithm for data compression. *C Users Journal*,
    *12*(2), 23–38.'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] Gage, P. (1994). 一种新的数据压缩算法。*C 用户杂志*, *12*(2), 23–38.'
- en: '[2] Petrov, A., La Malfa, E., Torr, P. H., & Bibi, A. (2023). Language Model
    Tokenizers Introduce Unfairness Between Languages. [*arXiv preprint arXiv:2305.15425*](https://arxiv.org/abs/2305.15425).'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] Petrov, A., La Malfa, E., Torr, P. H., & Bibi, A. (2023). 语言模型分词器在语言间引入了不公平性。[*arXiv
    预印本 arXiv:2305.15425*](https://arxiv.org/abs/2305.15425)。'
- en: Code
  id: totrans-89
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 代码
- en: This is the code I used to calculate the number of tokens and decode the tokens
    for this article.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我用来计算和解码文章中令牌数量的代码。
- en: '[PRE3]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
