- en: AI Telephone — A Battle of Multimodal Models
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: AI 电话 — 多模态模型的对决
- en: 原文：[https://towardsdatascience.com/ai-telephone-a-battle-of-multimodal-models-282b01daf044?source=collection_archive---------5-----------------------#2023-06-15](https://towardsdatascience.com/ai-telephone-a-battle-of-multimodal-models-282b01daf044?source=collection_archive---------5-----------------------#2023-06-15)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/ai-telephone-a-battle-of-multimodal-models-282b01daf044?source=collection_archive---------5-----------------------#2023-06-15](https://towardsdatascience.com/ai-telephone-a-battle-of-multimodal-models-282b01daf044?source=collection_archive---------5-----------------------#2023-06-15)
- en: DALL-E2, Stable Diffusion, BLIP, and more!
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: DALL-E2、Stable Diffusion、BLIP 等！
- en: '[](https://medium.com/@jacob_marks?source=post_page-----282b01daf044--------------------------------)[![Jacob
    Marks, Ph.D.](../Images/94d9832b8706d1044e3195386613bfab.png)](https://medium.com/@jacob_marks?source=post_page-----282b01daf044--------------------------------)[](https://towardsdatascience.com/?source=post_page-----282b01daf044--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----282b01daf044--------------------------------)
    [Jacob Marks, Ph.D.](https://medium.com/@jacob_marks?source=post_page-----282b01daf044--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@jacob_marks?source=post_page-----282b01daf044--------------------------------)[![Jacob
    Marks, Ph.D.](../Images/94d9832b8706d1044e3195386613bfab.png)](https://medium.com/@jacob_marks?source=post_page-----282b01daf044--------------------------------)[](https://towardsdatascience.com/?source=post_page-----282b01daf044--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----282b01daf044--------------------------------)
    [Jacob Marks, Ph.D.](https://medium.com/@jacob_marks?source=post_page-----282b01daf044--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ff7dc0c0eae92&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fai-telephone-a-battle-of-multimodal-models-282b01daf044&user=Jacob+Marks%2C+Ph.D.&userId=f7dc0c0eae92&source=post_page-f7dc0c0eae92----282b01daf044---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----282b01daf044--------------------------------)
    ·14 min read·Jun 15, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F282b01daf044&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fai-telephone-a-battle-of-multimodal-models-282b01daf044&user=Jacob+Marks%2C+Ph.D.&userId=f7dc0c0eae92&source=-----282b01daf044---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ff7dc0c0eae92&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fai-telephone-a-battle-of-multimodal-models-282b01daf044&user=Jacob+Marks%2C+Ph.D.&userId=f7dc0c0eae92&source=post_page-f7dc0c0eae92----282b01daf044---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----282b01daf044--------------------------------)
    · 14 min 阅读 · 2023年6月15日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F282b01daf044&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fai-telephone-a-battle-of-multimodal-models-282b01daf044&user=Jacob+Marks%2C+Ph.D.&userId=f7dc0c0eae92&source=-----282b01daf044---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F282b01daf044&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fai-telephone-a-battle-of-multimodal-models-282b01daf044&source=-----282b01daf044---------------------bookmark_footer-----------)![](../Images/c69c273d766460fcee87fbfc328b6a01.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F282b01daf044&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fai-telephone-a-battle-of-multimodal-models-282b01daf044&source=-----282b01daf044---------------------bookmark_footer-----------)![](../Images/c69c273d766460fcee87fbfc328b6a01.png)'
- en: '*Artistic rendering of a game of AI Telephone. Image generated by the author
    using DALL-E2.*'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '*AI 电话游戏的艺术渲染图。图像由作者使用 DALL-E2 生成。*'
- en: Generative AI is on fire right now. The past few months especially have seen
    an explosion in multimodal machine learning — AI that connects concepts across
    different “modalities” such as text, images, and audio. As an example, [Midjourney](https://www.midjourney.com/)
    is a multimodal text-to-image model, because it takes in natural language, and
    outputs images. The magnum opus for this recent renaissance in multimodal synergy
    was Meta AI’s ImageBind, which can take inputs of 6(!) varieties and represent
    them in the same “space”.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 生成式 AI 现在正处于火热阶段。特别是过去几个月，多模态机器学习的爆炸性增长——AI 连接了不同“模态”如文本、图像和音频的概念。例如，[Midjourney](https://www.midjourney.com/)
    是一个多模态文本到图像模型，因为它接受自然语言输入，并输出图像。最近多模态协同的巅峰之作是 Meta AI 的 ImageBind，它可以接受 6 种(!)
    类型的输入，并将它们表示在相同的“空间”中。
- en: 'With all of this excitement, I wanted to put multimodal models to the test
    and see how good they *actually* are. In particular, I wanted to answer three
    questions:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于所有这些兴奋，我想测试多模态模型，看看它们 *实际上* 有多好。特别是，我想回答三个问题：
- en: Which text-to-image model is the best?
  id: totrans-11
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 哪种文本到图像的模型最好？
- en: Which image-to-text model is the best?
  id: totrans-12
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 哪种图像到文本的模型最好？
- en: What is more important — image-to-text, or text-to-image?
  id: totrans-13
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 什么更重要——图像到文本，还是文本到图像？
- en: '*Of course, each model brings its own biases to the table, from training data
    to model architecture, so there isn’t really ever one BEST model. But we can still
    put models to the test in a general context!*'
  id: totrans-14
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*当然，每个模型都有其自身的偏见，从训练数据到模型架构，所以实际上并没有一个最好的模型。但我们仍然可以在一般背景下测试模型！*'
- en: To answer these questions, I decided to play a game of AI Telephone, inspired
    by the board game [Telestrations](https://en.wikipedia.org/wiki/Telestrations),
    which my family and I love to play together.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 为了回答这些问题，我决定玩一个 AI 电话游戏，灵感来自于我们全家都喜欢一起玩的棋盘游戏 [Telestrations](https://en.wikipedia.org/wiki/Telestrations)。
- en: 'Telestrations is much like the [game of telephone](https://www.wikihow.com/Play-the-Telephone-Game):
    players go around in a circle, taking in communication from the person on one
    side, and in turn communicating their interpretation to the person on their other
    side. As the game ensues, the original message is invariably altered, if not lost
    entirely. Telestrations differs, however, by adding bimodal communication: players
    alternate between drawing (or *illustrating*) a description, and describing (in
    text) a description.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: Telestrations 非常类似于 [电话游戏](https://www.wikihow.com/Play-the-Telephone-Game)：玩家们围成一个圈，接收来自一侧的人的信息，并将他们的解释传达给另一侧的人。随着游戏的进行，原始信息通常会被改变，甚至完全丧失。然而，Telestrations
    的不同之处在于加入了双模态通信：玩家在绘制（或 *插图*）描述和用文本描述之间交替进行。
- en: Given that I was more interested in *comparing* models, I adapted the game to
    suit this purpose.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于我对 *比较* 模型更感兴趣，我调整了游戏以适应这一目的。
- en: 'Here’s how the game of AI Telephone works:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 这是 AI 电话游戏的工作方式：
- en: Each “game” will pair up an image-to-text (I2T) model with a text-to-image (T2I)
    model
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 每个“游戏”将配对一个图像到文本（I2T）模型和一个文本到图像（T2I）模型
- en: Given an initial prompt, we use the T2I model to generate an image.
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 给定一个初始提示，我们使用 T2I 模型生成一个图像。
- en: We then pass this image into the I2T model to generate a description.
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后我们将这个图像传入 I2T 模型以生成描述。
- en: We repeat steps 2 and 3 a fixed number of times `n` (in our case `n=10`).
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们重复步骤 2 和 3 固定次数 `n`（在我们的案例中 `n=10`）。
- en: Finally, we quantify the difference between the original prompt and the final
    description.
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们量化原始提示与最终描述之间的差异。
- en: In this post, I will walk you through this entire process, so that you can play
    AI Telephone too! At the end, I’ll answer the three motivating questions.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我将带你经历整个过程，这样你也可以玩 AI 电话游戏！最后，我会回答三个激励性的问题。
- en: '*Note: This game of AI Telephone is intimately connected with the notion of*
    [*cycle consistency*](https://openaccess.thecvf.com/content_ECCVW_2018/papers/11132/Cornia_Towards_Cycle-Consistent_Models_for_Text_and_Image_Retrieval_ECCVW_2018_paper.pdf)*.
    By incorporating a cycle consistency term in the loss function during training,
    models can be incentivized to, effectively, minimize degradation over a game of
    telephone. To my knowledge, none of the models considered in this experiment were
    trained with cycle consistency as a consideration.*'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '*注意：这个 AI 电话游戏与* [*循环一致性*](https://openaccess.thecvf.com/content_ECCVW_2018/papers/11132/Cornia_Towards_Cycle-Consistent_Models_for_Text_and_Image_Retrieval_ECCVW_2018_paper.pdf)*的概念密切相关。通过在训练过程中在损失函数中加入循环一致性项，可以有效地促使模型最小化电话游戏中的退化现象。据我所知，本实验中考虑的所有模型在训练时并未将循环一致性作为考虑因素。*'
- en: 'The post is structured as follows:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 文章的结构如下：
- en: '[Choosing the Multimodal Models](#6fb8)'
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[选择多模态模型](#6fb8)'
- en: '[Generating the Prompts](#cc8c)'
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[生成提示](#cc8c)'
- en: '[Creating Telephone Lines](#50e9)'
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[创建电话线路](#50e9)'
- en: '[Carrying out the Conversations](#5551)'
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[进行对话](#5551)'
- en: '[Visualizing and Analyzing the Results](#715e)'
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[可视化和分析结果](#715e)'
- en: All of the code to run this experiment and play AI Telephone can be found [here](https://github.com/voxel51/fiftyone-examples/blob/master/examples/ai_telephone.ipynb).
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 运行这个实验和玩 AI 电话游戏的所有代码可以在 [这里](https://github.com/voxel51/fiftyone-examples/blob/master/examples/ai_telephone.ipynb)
    找到。
- en: To run this code, you will need to install the [FiftyOne open source library](https://github.com/voxel51/fiftyone)
    for dataset curation, the [OpenAI Python Library](https://github.com/openai/openai-python),
    and the [Replicate Python client](https://replicate.com/docs/get-started/python).
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 要运行这段代码，你需要安装用于数据集策划的 [FiftyOne 开源库](https://github.com/voxel51/fiftyone)、[OpenAI
    Python 库](https://github.com/openai/openai-python) 和 [Replicate Python 客户端](https://replicate.com/docs/get-started/python)。
- en: '[PRE0]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Progression of images in a game of AI Telephone between DALL-E2 and BLIP.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: DALL-E2 和 BLIP 之间的 AI 电话游戏中的图像进展。
- en: Choosing the Competitors
  id: totrans-36
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 选择竞争者
- en: 'The space of multimodal models is massive: at the time of writing, Hugging
    Face alone has 4,425 T2I models and 155 I2T models. Playing AI Telephone with
    all of these models — or even a non-negligible fraction of them — would be completely
    infeasible. My first task was to pare down this space of potential candidates
    to a more manageable set of competitors.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 多模态模型的领域非常庞大：截至撰写时，仅 Hugging Face 就有 4,425 个 T2I 模型和 155 个 I2T 模型。用所有这些模型——甚至是其中一个不可忽视的部分——进行
    AI 电话游戏是完全不可行的。我的第一任务是将这些潜在候选者缩减到一个更可管理的竞争者集合。
- en: Opting for APIs
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 选择 API
- en: To start this project, I knew that I would be working with many models. Some
    of the prospective models were quite large, and many required their own environments,
    with a unique set of requirements. Given that I planned to pair up each T2I model
    with each I2T model, installing these models locally to play games of AI Telephone
    presented a potential dependency purgatory — especially because I work on a MacBook
    Pro M1!
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始这个项目时，我知道自己将使用许多模型。一些潜在的模型非常大，许多模型需要各自的环境和独特的要求。鉴于我计划将每个 T2I 模型与每个 I2T 模型配对，本地安装这些模型进行
    AI 电话游戏会带来潜在的依赖地狱——尤其是因为我使用的是 MacBook Pro M1！
- en: To circumvent this problem, I decided to stick to models that were accessible
    via APIs. In particular, I chose to primarily use [Replicate](https://replicate.com/),
    whose simple interface allowed me to work with T2I and I2T models in plug-and-play
    fashion. Almost every model that I used is open source, so if you are braver than
    I, you can run these models locally and avoid the charges. That being said, in
    total this experiment cost < $15 USD.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 为了绕过这个问题，我决定坚持使用可以通过 API 访问的模型。特别是，我选择了主要使用 [Replicate](https://replicate.com/)，其简单的接口允许我以即插即用的方式使用
    T2I 和 I2T 模型。我使用的几乎每个模型都是开源的，所以如果你比我更勇敢，可以在本地运行这些模型并避免费用。也就是说，总的来说，这个实验花费了不到 15
    美元。
- en: Text-to-Image Models
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 文本到图像模型
- en: When selecting T2I models, I chose from the models in Replicate’s [Text to image](https://replicate.com/collections/text-to-image)
    collection. My selection criteria were that the model needed to be cheap, fast,
    and relatively popular (judged by the number of “runs” of the model on Replicate).
    Additionally, the model needed to be *general purpose*, meaning that I wasn’t
    going to consider outpainting, logo generation, or anime styling models. You are
    more than welcome to try playing AI Telephone with these types of models if you’d
    like!
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在选择 T2I 模型时，我从 Replicate 的 [Text to image](https://replicate.com/collections/text-to-image)
    集合中进行挑选。我的选择标准是模型需要便宜、快速且相对受欢迎（通过 Replicate 上模型的“运行”次数来判断）。此外，模型需要是 *通用的*，意味着我不会考虑图像扩展、标志生成或动漫风格模型。如果你愿意，完全可以尝试使用这些类型的模型进行
    AI 电话游戏！
- en: Given these requirements, I chose [Stable Diffusion](https://replicate.com/stability-ai/stable-diffusion)
    and [Feed forward VQGAN CLIP](https://replicate.com/mehdidc/feed_forward_vqgan_clip).
    Initially, I also worked with [DALL-E Mini](https://replicate.com/kuprel/min-dalle),
    but in early tests I was disappointed by the model’s performance, so I swapped
    the model out for OpenAI’s DALL-E2, which I accessed through OpenAI’s [image generations
    endpoint](https://platform.openai.com/docs/guides/images/usage).
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 根据这些要求，我选择了 [Stable Diffusion](https://replicate.com/stability-ai/stable-diffusion)
    和 [Feed forward VQGAN CLIP](https://replicate.com/mehdidc/feed_forward_vqgan_clip)。最初，我也尝试了
    [DALL-E Mini](https://replicate.com/kuprel/min-dalle)，但在早期测试中对模型的表现感到失望，因此我将模型更换为
    OpenAI 的 DALL-E2，通过 OpenAI 的 [图像生成端点](https://platform.openai.com/docs/guides/images/usage)进行访问。
- en: As a side note, restricting my attention to API-accessible models meant that
    I did not consider [Midjourney](https://www.midjourney.com/). There is no official
    API, and I did not want to use an unofficial API, nor did I want to enter prompts
    into Discord one by one and download the generated images one at a time.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一个附带说明，限制我的关注范围到 API 可访问的模型意味着我没有考虑 [Midjourney](https://www.midjourney.com/)。没有官方
    API，我不想使用非官方 API，也不想逐个在 Discord 中输入提示并一一下载生成的图像。
- en: 'To make this process as plug-and-play as possible, I took an object oriented
    approach. I defined a base `Text2Image` class, which exposes a method `generate_image(text)`:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使这个过程尽可能即插即用，我采取了面向对象的方法。我定义了一个基本的 `Text2Image` 类，暴露了一个 `generate_image(text)`
    方法：
- en: '[PRE1]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'For Replicate models, all that is needed is then setting the `model_name` attribute,
    identifying the model on Replicate. For Stable Diffusion, for instance, the class
    definition looks like this:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 Replicate 模型，只需要设置 `model_name` 属性，以在 Replicate 上识别模型。例如，对于 Stable Diffusion，类定义如下：
- en: '[PRE2]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'For other models, such as DALL-E2, the `generate_image(text)` method can be
    overloaded:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 对于其他模型，如 DALL-E2，`generate_image(text)` 方法可以被重载：
- en: '[PRE3]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Each of these T2I models returns the URL of the generated image, which we can
    then pass directly to our I2T models.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 这些 T2I 模型中的每一个都会返回生成的图像的 URL，然后我们可以将其直接传递给我们的 I2T 模型。
- en: Image-to-Text Models
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 图像到文本模型
- en: 'I followed a similar process to determine the I2T competitors, evaluating candidates
    in Replicate’s [Image to text](https://replicate.com/collections/image-to-text)
    collection. After looking at the examples for all of the models in the collection,
    six models stood out: [BLIP](https://replicate.com/salesforce/blip), [BLIP-2](https://replicate.com/andreasjansson/blip-2),
    [CLIP prefix captioning](https://replicate.com/rmokady/clip_prefix_caption), [Fine-grained
    Image Captioning with CLIP Reward](https://replicate.com/j-min/clip-caption-reward),
    [mPLUG-Owl](https://replicate.com/joehoover/mplug-owl), and [MiniGPT-4](https://replicate.com/daanelson/minigpt-4).
    Other models were enticing, such as [CLIP Interrogator](https://replicate.com/pharmapsychotic/clip-interrogator),
    which tries to reverse engineer a prompt you can then use to generate a similar
    image. But this felt a bit like cheating as far as AI Telephone was concerned!'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 我遵循了类似的过程来确定 I2T 竞争者，评估了 Replicate 的 [Image to text](https://replicate.com/collections/image-to-text)
    收藏中的候选模型。在查看了收藏中所有模型的示例后，有六个模型脱颖而出：[BLIP](https://replicate.com/salesforce/blip)、[BLIP-2](https://replicate.com/andreasjansson/blip-2)、[CLIP
    prefix captioning](https://replicate.com/rmokady/clip_prefix_caption)、[Fine-grained
    Image Captioning with CLIP Reward](https://replicate.com/j-min/clip-caption-reward)、[mPLUG-Owl](https://replicate.com/joehoover/mplug-owl)
    和 [MiniGPT-4](https://replicate.com/daanelson/minigpt-4)。其他模型也很吸引人，比如 [CLIP Interrogator](https://replicate.com/pharmapsychotic/clip-interrogator)，它试图反向工程一个提示，然后你可以用来生成类似的图像。但就
    AI 电话而言，这感觉有点像作弊！
- en: 'Playing around with the six I2T candidates, I was able to quickly eliminate
    two models from contention: BLIP-2 generated responses that were consistently
    too short to be useful, and the CLIP Caption Reward model generated responses
    which were often incoherent.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在尝试这六个 I2T 候选模型时，我能够迅速淘汰两个模型：BLIP-2 生成的响应总是过短，不够实用，而 CLIP Caption Reward 模型生成的响应往往不连贯。
- en: 'In direct analogy with the T2I models, I defined a base class `Image2Text`
    class exposing a `generate_text(image_url)` method:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 与 T2I 模型直接类比，我定义了一个基本的 `Image2Text` 类，暴露了一个 `generate_text(image_url)` 方法：
- en: '[PRE4]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'I then created subclasses for each model. Here is what the BLIP subclass looks
    like:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我为每个模型创建了子类。下面是 BLIP 子类的样子：
- en: '[PRE5]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: All of the models are instantiated with the same task description — to “write
    a detailed description of this image”.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 所有模型都使用相同的任务描述进行实例化——“写一个关于这张图片的详细描述”。
- en: Progression of images in a game of AI Telephone between DALL-E2 and mPLUG-Owl.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在 DALL-E2 和 mPLUG-Owl 之间的 AI 电话游戏中的图像进展。
- en: Prompts
  id: totrans-61
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 提示
- en: 'To be as “scientific” as possible, I thought it best to not generate the initial
    prompts myself. Instead, (and just for fun) I outsourced the task to ChatGPT.
    I asked:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 为了尽可能“科学”，我认为最好不要自己生成初始提示。相反，（仅仅为了好玩）我将任务外包给了 ChatGPT。我问：
- en: '[PRE6]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: I’m playing a game of telephone using text-to-image and image-to-text AI models.
    I want to evaluate these models based on their ability to retain complex semantic
    information over the course of long conversations. Your job is to give me 10 text
    prompts that I can use to run these games of telephone. You must give me one 3
    easy, 3 medium, 3 hard, and 1 ultra-hard (“impossible”) prompt
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 我正在使用文本到图像和图像到文本的AI模型进行电话游戏。我希望根据这些模型在长时间对话中保持复杂语义信息的能力进行评估。你的任务是给我10个文本提示，我可以用来运行这些电话游戏。你必须给我3个简单的、3个中等的、3个困难的和1个超难（“不可能”）的提示。
- en: 'Here are some of the prompts ChatGPT generated:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是一些ChatGPT生成的提示：
- en: '[PRE7]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '*A more rigorous scientific approach would be far more intentional with the
    prompts used, as well as their categorization.*'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '*更严格的科学方法会对所使用的提示以及它们的分类更加用心。*'
- en: 'I then took the text prompts generated by ChatGPT and constructed `Prompt`
    objects, which contained the text for the prompt, and the “level” of difficulty
    assigned by ChatGPT:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 我将ChatGPT生成的文本提示转化为`Prompt`对象，其中包含提示的文本，以及ChatGPT分配的“难度”级别：
- en: '[PRE8]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Progression of images in a game of AI Telephone between VQGAN-CLIP and MiniGPT-4.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: AI电话游戏中VQGAN-CLIP与MiniGPT-4之间图像的进展。
- en: The Telephone Line
  id: totrans-71
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 电话线
- en: The last component to playing AI Telephone was the “telephone line” itself.
    I created a `TelephoneLine` class to encapsulate the connection between a T2I
    model and an I2T model. Given a single telephone line, a “game” of telephone is
    played by calling the `play(prompt, nturns=10)`, where the conversation evolves
    from `prompt`, and runs for `nturns` back-and-forth turns.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 玩AI电话游戏的最后一个组件是“电话线”本身。我创建了一个`TelephoneLine`类来封装T2I模型和I2T模型之间的连接。给定一条电话线，通过调用`play(prompt,
    nturns=10)`来进行“游戏”，对话从`prompt`开始，并进行`nturns`轮回合。
- en: '[PRE9]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: For each game played, the conversation is logged with a unique name, generated
    by hashing the T2I model name, I2T model name, and the prompt text (`get_conversation_name()`
    method).
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个进行的游戏，使用唯一名称记录对话，该名称由T2I模型名称、I2T模型名称和提示文本（`get_conversation_name()`方法）哈希生成。
- en: 'I also equipped the class with a `save_conversations_to_dataset()` method,
    which saves the images and descriptions from all games played on the telephone
    line to a FiftyOne `Dataset`:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 我还为该类配备了一个`save_conversations_to_dataset()`方法，该方法将所有游戏中的图像和描述保存到FiftyOne `Dataset`中：
- en: '[PRE10]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Progression of images in a game of AI Telephone between Stable Diffusion and
    CLIP Prefix Captioning.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: AI电话游戏中Stable Diffusion与CLIP Prefix Captioning之间图像的进展。
- en: Carrying out the Conversations
  id: totrans-78
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进行对话
- en: With all of the building blocks in place, playing AI Telephone is child’s play!
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 所有构建模块到位后，玩AI电话游戏简直是小菜一碟！
- en: 'We can instantiate T2I and I2T models:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以实例化T2I和I2T模型：
- en: '[PRE11]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'And then create a telephone line for each pair:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 然后为每对创建一条电话线：
- en: '[PRE12]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'We then load in our prompts:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们加载我们的提示：
- en: '[PRE13]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'And create a FiftyOne `Dataset` which we will use to store the generated images
    and all relevant information from the conversations:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 并创建一个FiftyOne `Dataset`，我们将用来存储生成的图像以及来自对话的所有相关信息：
- en: '[PRE14]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'We can then run all 120 games of telephone:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们可以运行所有120场电话游戏：
- en: '[PRE15]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: In the FiftyOne App, click on the splitting icon in the menu bar to group images
    by conversation, select `conversation_name` from the dropdown, then toggle the
    selector to `ordered` and select `step_number`.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 在FiftyOne应用程序中，点击菜单栏中的分割图标，以对话为单位对图像进行分组，从下拉菜单中选择`conversation_name`，然后将选择器切换到`ordered`并选择`step_number`。
- en: Results and Conclusions
  id: totrans-91
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结果和结论
- en: To assess the quality of a conversation — purely in terms of how closely the
    meaning of the final description approximated the meaning of the initial prompt,
    I decided to generate embeddings for the prompts and descriptions, and compute
    the [cosine distance](https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance.cosine.html)
    (in `[0, 2]`) between the two.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 为了评估对话的质量——纯粹是从最终描述的意义与初始提示的意义的接近程度来看，我决定生成提示和描述的嵌入，并计算两个之间的[余弦距离](https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance.cosine.html)（在`[0,
    2]`范围内）。
- en: '[PRE16]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'For an embedding model, I wanted a model that could embed both text and images,
    given the multimodal nature of the exercise. I ended up choosing to use ImageBind
    for three reasons:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 对于嵌入模型，我希望选择一个能够同时嵌入文本和图像的模型，以适应该任务的多模态特性。最终，我选择了使用ImageBind，原因有三：
- en: Other popular joint image-text embedding models like CLIP and BLIP are related
    to some of the models I used in the experiment (BLIP and CLIP prefix captioning),
    and I wanted to avoid any possible biases from using the same types of models
    for evaluation.
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 其他流行的联合图像-文本嵌入模型如 CLIP 和 BLIP 与我在实验中使用的一些模型（BLIP 和 CLIP prefix captioning）有关，我希望避免使用相同类型模型进行评估带来的任何潜在偏差。
- en: Many text embedding models have a small `max_token_count` — the maximum number
    of tokens allowed in a text to be embedded. CLIP, for instance, has `max_token_count=77`.
    Some of our descriptions are significantly longer than this. Fortunately, ImageBind
    has a much longer maximum token count.
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 许多文本嵌入模型有一个较小的 `max_token_count`——即允许在文本中嵌入的最大标记数。例如，CLIP 的 `max_token_count=77`。我们的一些描述明显比这长。幸运的是，ImageBind
    具有更长的最大标记数。
- en: I’d been meaning to try ImageBind, and this was a great opportunity!
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我一直想尝试 ImageBind，这正是一个绝佳的机会！
- en: 'I wrapped Replicate’s ImageBind API in a function `embed_text(text)`:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 我将 Replicate 的 ImageBind API 封装在一个函数 `embed_text(text)` 中：
- en: '[PRE17]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'To avoid redundant computations, I hashed the prompts and stored the prompt
    embeddings in a dictionary. This way, instead of embedding each prompt for each
    of the 12 telephone lines, we only need to embed each once:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 为了避免冗余计算，我对提示进行了哈希处理，并将提示嵌入存储在字典中。这样，我们只需为每个提示嵌入一次，而不是为每个 12 条电话线路的每个提示嵌入：
- en: '[PRE18]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'We can then group samples by conversation name, iterate through these groups,
    compute the text embedding for each step, and record the cosine distance (smaller
    is better!) between the text embedding and the initial prompt embedding:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们可以按对话名称对样本进行分组，迭代这些组，计算每一步的文本嵌入，并记录文本嵌入与初始提示嵌入之间的余弦距离（距离越小越好！）：
- en: '[PRE19]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: I then computed the average scores for each T2I-I2T pair across all prompts
    at a certain level of difficulty and plotted the results. In each of the videos,
    the I2T and T2I models are printed on the generated images, as well as the text
    used to generate that image (red), and the description generated from that image
    (green).
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我计算了每个 T2I-I2T 对在特定难度级别下的平均分数，并绘制了结果。在每个视频中，I2T 和 T2I 模型被打印在生成的图像上，以及用于生成该图像的文本（红色），以及从该图像生成的描述（绿色）。
- en: Easy
  id: totrans-105
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 简单
- en: '![](../Images/0f19f4db954511a26ab1c95cb13526c9.png)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0f19f4db954511a26ab1c95cb13526c9.png)'
- en: For easy prompts, performance tends to depend most strongly on the text-to-image
    model. DALL-E2 and Stable Diffusion dramatically outperform VQGAN-CLIP. MiniGPT-4
    is a member of both of the top-performing pairs.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 对于简单的提示，性能往往最依赖于文本到图像模型。DALL-E2 和 Stable Diffusion 显著优于 VQGAN-CLIP。MiniGPT-4
    是这两个顶级表现对中的成员。
- en: 'Here are some examples for the easy prompt introduced above:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是上面介绍的简单提示的一些示例：
- en: AI Telephone for an easy prompt, with pairs of text-to-image and image-to-text
    models.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 简单提示的 AI 电话，包括文本到图像和图像到文本模型对。
- en: In the games with MiniGPT-4 (and to a slightly lesser extent BLIP), the apple
    remains front and center, whereas for games involving CLIP Prefix, the apple gets
    phased out over time.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用 MiniGPT-4 的游戏中（以及在稍微小一点程度上的 BLIP），苹果始终保持在中心，而对于涉及 CLIP Prefix 的游戏，苹果随着时间的推移逐渐被淘汰。
- en: Medium
  id: totrans-111
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 中等
- en: '![](../Images/cc664b641e6ec5bbbc202e6fa1dbd1e7.png)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/cc664b641e6ec5bbbc202e6fa1dbd1e7.png)'
- en: When the prompts become a bit more difficult, the situation starts to change.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 当提示变得稍微困难时，情况开始发生变化。
- en: AI Telephone for a medium difficulty prompt, with pairs of text-to-image and
    image-to-text models.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 中等难度提示的 AI 电话，包括文本到图像和图像到文本模型对。
- en: For nearly all of the games, the subject changes somewhere around the fourth
    or fifth step. Early on, MiniGPT-4 holds an advantage. But by the end of the game,
    that advantage seems to have been entirely lost.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 对于几乎所有的游戏，主题在第四或第五步左右发生变化。早期，MiniGPT-4 具有优势。但到游戏结束时，这种优势似乎完全丧失了。
- en: Hard
  id: totrans-116
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 困难
- en: '![](../Images/9e02437c072b5f6388b7e36483df716b.png)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9e02437c072b5f6388b7e36483df716b.png)'
- en: 'By the time the prompts become challenging, we start to see something interesting:
    for early steps, the image-to-text model is most important (MiniGPT-4 is best,
    and CLIP Prefix is for the most part the worst). By later stages, however, the
    text-to-image model becomes most important. And to complicate the situation further,
    VQGAN-CLIP is best here!'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 当提示变得具有挑战性时，我们开始看到有趣的现象：在早期步骤中，图像到文本模型最为重要（MiniGPT-4 最佳，CLIP Prefix 通常最差）。但在后期阶段，文本到图像模型变得最为重要。更复杂的是，VQGAN-CLIP
    在这里表现最佳！
- en: One might worry that “better” could just mean that consistency is maintained,
    without accurately representing the original concept. However, when we look at
    examples, we can see that this is not the case.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 有人可能担心“更好”只是意味着保持一致性，而没有准确表示原始概念。然而，当我们查看例子时，可以看到情况并非如此。
- en: AI Telephone for a hard prompt, with pairs of text-to-image and image-to-text
    models.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: AI 电话对于一个困难的提示，涉及成对的文本到图像和图像到文本模型。
- en: Take the example highlighted in the video, where the initial prompt is the “hard”
    prompt introduced above concerning a “bustling marketplace”. While the images
    generated by VQGAN-CLIP are without a doubt grainy, the subject can still be made
    out, and matches the original prompt fairly closely.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 以视频中突出显示的例子为例，其中最初的提示是上述“繁忙市场”的“困难”提示。虽然 VQGAN-CLIP 生成的图像无疑是颗粒状的，但主题仍然可以辨认出来，并且与原始提示相当接近。
- en: Impossible
  id: totrans-122
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 不可能
- en: '![](../Images/00ae40dcc7622c335c9b871b46224e71.png)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/00ae40dcc7622c335c9b871b46224e71.png)'
- en: Unsurprisingly, none of our competitors do terribly well here. One might argue
    that VQGAN-CLIP is the winner. But for the most part, this is all just noise.
    In the video, even for games involving VQGAN-CLIP, the subject is effectively
    unrecognizable.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 不出所料，我们的竞争对手在这里表现得都不太好。有人可能会争辩说 VQGAN-CLIP 是赢家。但在大多数情况下，这一切只是噪音。在视频中，即使是涉及 VQGAN-CLIP
    的游戏，主题也基本上无法识别。
- en: AI Telephone for an “impossible” prompt, with pairs of text-to-image and image-to-text
    models.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: AI 电话对于一个“无法完成”的提示，涉及成对的文本到图像和图像到文本模型。
- en: Takeaways
  id: totrans-126
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 要点
- en: This exploration was far from scientific; I only looked at ten prompts, without
    true validation of their difficulty level. I only ran the conversations out to
    ten back-and-forth steps; and I only evaluated performance on one metric.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 这次探索远非科学；我只查看了十个提示，没有真正验证它们的难度。我仅将对话进行到十轮往返步骤；而且我只在一个指标上评估了性能。
- en: 'It is clear that which T2I and I2T models fare best depends in large part on
    the complexity of the prompt, and how long you want to keep the models talking.
    Nevertheless, it is worth noting a few key observations:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 很明显，哪种 T2I 和 I2T 模型表现最佳在很大程度上取决于提示的复杂性，以及你希望模型对话保持多长时间。然而，值得注意几个关键观察点：
- en: VQGAN-CLIP may fare better for more challenging prompts, but this doesn’t mean
    it is a *better* T2I model. The images produced by VQGAN-CLIP are often far less
    coherent and globally consistent than those produced by Stable Diffusion or DALL-E2.
  id: totrans-129
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: VQGAN-CLIP 对于更具挑战性的提示可能表现更好，但这并不意味着它是一个*更好的* T2I 模型。VQGAN-CLIP 生成的图像通常远不如 Stable
    Diffusion 或 DALL-E2 生成的图像那样连贯和全球一致。
- en: The analysis above is all about semantic similarity — it does not take *style*
    into account. The style of these images can change a ton over the course of a
    game of AI Telephone. Anecdotally, I found that the style is much more consistent
    for I2T models like mPLUG-Owl, which give long descriptions, than for models like
    BLIP, whose descriptions are more subject focused.
  id: totrans-130
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 上述分析全部关于语义相似性——它没有考虑*风格*。这些图像的风格在 AI 电话游戏过程中可能会变化很多。根据我的经验，I2T 模型如 mPLUG-Owl
    的风格更一致，因为它们提供了长描述，而像 BLIP 这样的模型风格则更多地集中在特定主题上。
- en: By around five or six iterations, the games had mostly converged to stable equilibria.
  id: totrans-131
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 到了大约五六轮迭代时，游戏大多数已经收敛到稳定的平衡状态。
- en: Even though the embedding model, ImageBind, was multimodal, the distance between
    consecutive image embeddings and text embeddings were far greater than the distance
    between consecutive images or consecutive descriptions. In general, they followed
    the same trends, but in less pronounced fashion, which is why I didn’t include
    these in the plots.
  id: totrans-132
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 尽管嵌入模型 ImageBind 是多模态的，但连续图像嵌入和文本嵌入之间的距离远大于连续图像或连续描述之间的距离。一般来说，它们遵循相同的趋势，但不那么明显，这也是我没有在图中包含这些的原因。
- en: I hope this inspires you to run your own experiments with generative AI — whether
    you’re playing AI Telephone, or doing something else entirely!
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 我希望这能激励你进行自己的生成 AI 实验——无论你是在玩 AI 电话，还是做其他完全不同的事情！
- en: If you try out a variation of this and get interesting results, comment on this
    post!
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你尝试这个变体并获得有趣的结果，请在这个帖子下评论！
