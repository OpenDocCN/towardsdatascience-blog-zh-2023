- en: How Google Used Fake Datasets to Train Generative Music AI
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 谷歌如何利用虚假数据集来训练生成音乐AI
- en: 原文：[https://towardsdatascience.com/how-google-used-fake-datasets-to-train-generative-music-ai-def6f3f71f19](https://towardsdatascience.com/how-google-used-fake-datasets-to-train-generative-music-ai-def6f3f71f19)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/how-google-used-fake-datasets-to-train-generative-music-ai-def6f3f71f19](https://towardsdatascience.com/how-google-used-fake-datasets-to-train-generative-music-ai-def6f3f71f19)
- en: '[](https://medium.com/@maxhilsdorf?source=post_page-----def6f3f71f19--------------------------------)[![Max
    Hilsdorf](../Images/01da76c553e43d5ed6b6849bdbfd00da.png)](https://medium.com/@maxhilsdorf?source=post_page-----def6f3f71f19--------------------------------)[](https://towardsdatascience.com/?source=post_page-----def6f3f71f19--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----def6f3f71f19--------------------------------)
    [Max Hilsdorf](https://medium.com/@maxhilsdorf?source=post_page-----def6f3f71f19--------------------------------)'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@maxhilsdorf?source=post_page-----def6f3f71f19--------------------------------)[![Max
    Hilsdorf](../Images/01da76c553e43d5ed6b6849bdbfd00da.png)](https://medium.com/@maxhilsdorf?source=post_page-----def6f3f71f19--------------------------------)[](https://towardsdatascience.com/?source=post_page-----def6f3f71f19--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----def6f3f71f19--------------------------------)
    [Max Hilsdorf](https://medium.com/@maxhilsdorf?source=post_page-----def6f3f71f19--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----def6f3f71f19--------------------------------)
    ·6 min read·May 28, 2023
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----def6f3f71f19--------------------------------)
    ·6分钟阅读·2023年5月28日
- en: --
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/ed2c5ce433310ba9f1ace3de4d277849.png)'
  id: totrans-5
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ed2c5ce433310ba9f1ace3de4d277849.png)'
- en: Photo by [James Stamler](https://unsplash.com/@jamesstamler?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由 [James Stamler](https://unsplash.com/@jamesstamler?utm_source=medium&utm_medium=referral)
    提供，来源于 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: In this post, we explore Google’s innovative approach to training their remarkable
    text-to-music models, including MusicLM and Noise2Music. We’ll delve into the
    concept of “fake” datasets and how they were utilized in these breakthrough models.
    If you’re curious about the inner workings of these techniques and their impact
    on advancing music AI, you’ve come to the right place.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我们探讨了谷歌训练其杰出的文本到音乐模型（包括MusicLM和Noise2Music）的创新方法。我们将深入了解“虚假”数据集的概念以及这些突破性模型如何利用它们。如果你对这些技术的内部运作及其对推动音乐AI的影响感到好奇，那么你来对地方了。
- en: '**The Lack of Labeled Music Data**'
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**标签化音乐数据的缺乏**'
- en: Large language models (LLMs) like ChatGPT or Bard are trained on huge amounts
    of unstructured text data. Although it can be computationally expensive to collect
    the content of millions of websites, there is an abundance of training data on
    the public web. In contrast, text-to-image models like DALL-E 2 require a totally
    different kind of dataset consisting of pairs of images with corresponding descriptions.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 像ChatGPT或Bard这样的语言模型（LLMs）在大量非结构化文本数据上进行训练。虽然收集数百万个网站的内容可能会耗费大量计算资源，但公共网络上有丰富的训练数据。相比之下，像DALL-E
    2这样的文本到图像模型需要完全不同类型的数据集，即配有相应描述的图像对。
- en: In the same way, text-to-music models rely on songs with descriptions of their
    musical content. However, unlike images, labeled music is really hard to find
    on the internet. Sometimes, metadata like instrumentation, genre, or mood, are
    available, but full-text in-depth descriptions are exceptionally hard to obtain.
    This poses a serious problem for researchers and companies trying to collect data
    to train generative music models.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 同样地，文本到音乐模型依赖于带有音乐内容描述的歌曲。然而，与图片不同，互联网上很难找到标签化的音乐。有时，像乐器、风格或情绪这样的元数据是可以获得的，但完整的深入描述则极其难以获取。这对研究人员和公司收集数据以训练生成音乐模型构成了严重问题。
- en: In early 2023, Google researchers created a lot of buzz around music AI with
    their breakthrough models, MusicLM and Noise2Music. However, among musicians,
    little is known about how the data for these models were collected. Let‘s dive
    into this topic together and learn about some of the tricks applied in Google’s
    music AI research.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 2023年初，谷歌研究人员通过突破性的模型MusicLM和Noise2Music引起了音乐AI领域的广泛关注。然而，在音乐家中，对于这些模型的数据如何收集知之甚少。让我们一起深入探讨这个话题，了解谷歌音乐AI研究中使用的一些技巧。
- en: '**How Google Overcame Data Scarcity**'
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**谷歌如何克服数据稀缺**'
- en: '**Weakly Associated Labels**'
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**弱关联标签**'
- en: For MusicLM and Noise2Music, Google relied on another one of their models called
    MuLan, which was trained to compute the similarity between any piece of music
    and any text description. To train MuLan, Google used what we call “weakly associated
    labels”. Instead of carefully curating a dataset of music with high-quality text
    descriptions, they purposefully took a different approach.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 MusicLM 和 Noise2Music，谷歌依赖于另一个叫做 MuLan 的模型，该模型经过训练可以计算任何音乐片段和任何文本描述之间的相似性。为了训练
    MuLan，谷歌使用了我们所说的“弱关联标签”。他们没有仔细策划一个高质量文本描述的音乐数据集，而是故意采取了不同的方法。
- en: 'First, they extracted a 30-second snippet from 44 million music videos available
    on YouTube, resulting in 370k hours of audio. The music was then labeled with
    various texts associated with the video: the video title and description, comments,
    the names of playlists featuring the video, and more. To reduce noise in this
    dataset, they employed a large language model to identify which associated text
    information had music-related content and discarded everything that did not.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，他们从 YouTube 上的 4400 万个音乐视频中提取了 30 秒的片段，得到 37 万小时的音频。然后，音乐被标记上与视频相关的各种文本：视频标题和描述、评论、播放列表的名称等。为了减少数据集中的噪音，他们使用了一个大型语言模型来识别哪些关联文本信息包含与音乐相关的内容，并丢弃所有不相关的内容。
- en: In my opinion, weakly associated labels can not be considered a “fake” dataset,
    yet, because the text information was still written by real humans and is undoubtedly
    associated with the music to some extent. However, this approach definitely prioritizes
    quantity over quality, which would have raised concerns among most machine learning
    researchers in the past. And Google was just getting started…
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在我看来，弱关联标签不能被视为“伪造”数据集，因为文本信息仍然是由真实的人撰写的，并且与音乐在某种程度上确实有关。然而，这种方法确实优先考虑了数量而非质量，这在过去曾引起大多数机器学习研究人员的关注。而谷歌才刚刚开始……
- en: '**Fake Labels**'
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**伪标签**'
- en: Noise2Music is a generative music AI based on diffusion technology, which was
    also used in image generation models like DALL-E or Midjourney.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: Noise2Music 是一个基于扩散技术的生成音乐 AI，这种技术也用于像 DALL-E 或 Midjourney 这样的图像生成模型。
- en: 'To train Noise2Music, Google took their previous approach to the extreme and
    transitioned from weakly associated labels to fully artificial labels. In what
    they refer to as “pseudo labeling”, the authors adopted a remarkable method to
    collect music description texts. They prompted a large language model (LaMDA)
    to write multiple descriptions for 150k popular songs, resulting in 4 million
    descriptions. Here is an example of such a description:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 为了训练 Noise2Music，谷歌将之前的方法推向极致，从弱关联标签过渡到完全人工标签。在他们称之为“伪标签”的方法中，作者采用了一种出色的方式来收集音乐描述文本。他们让一个大型语言模型（LaMDA）为
    15 万首热门歌曲写多个描述，结果得到了 400 万个描述。以下是一个描述的示例：
- en: '“Don’t Stop Me Now” by Queen : The energetic rock song builds on a piano, bass
    guitar, and drums. The singers are excited, ready to go, and uplifting.'
  id: totrans-20
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 皇后乐队的《Don’t Stop Me Now》：这首充满活力的摇滚歌曲由钢琴、贝斯吉他和鼓构成。歌手们充满激情，准备好出发，充满了振奋的感觉。
- en: Subsequently, the researchers removed the song and artist names to produce descriptions
    that could, in principle, apply to other songs, as well. However, even with these
    descriptions in hand, the researchers still needed to match them with suitable
    songs to obtain a large labeled dataset. Here is where MuLan, their model trained
    on weakly associated labels, proved to be useful.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 随后，研究人员移除了歌曲和艺术家的名字，生成的描述理论上可以适用于其他歌曲。然而，即使有了这些描述，研究人员仍然需要将它们与合适的歌曲匹配，以获得一个大规模标注的数据集。这时，MuLan，这个经过弱关联标签训练的模型，发挥了作用。
- en: The researchers collected a large dataset of unlabeled music, resulting in 340k
    hours of music. For each of these tracks, they utilized MuLan to identify the
    artificially generated song description that best matched it. Essentially, each
    piece of music is not mapped to a text describing the song itself, but to a description
    that encapsulates music similar to it.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 研究人员收集了一个大规模的未标记音乐数据集，总计 34 万小时的音乐。对于这些音乐曲目中的每一个，他们利用 MuLan 来识别最匹配的人工生成的歌曲描述。本质上，每段音乐并不是映射到描述歌曲本身的文本，而是映射到描述类似音乐的文本。
- en: '**Why Does This Work?**'
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**为什么这有效？**'
- en: The Issue
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题
- en: In traditional machine learning, the labels assigned to each observation (in
    this case, a piece of music) should ideally represent an objective truth. However,
    music descriptions inherently lack objectivity, presenting the first problem.
    Additionally, by utilizing audio-to-text mapping technology, the labels no longer
    reflect a “truthful” representation of what is happening in the song. They do
    not provide an accurate description of the music. Given these apparent flaws,
    one may wonder why this approach still yields useful results.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在传统的机器学习中，分配给每个观察（在这种情况下是音乐片段）的标签理想情况下应该代表客观的真相。然而，音乐描述本质上缺乏客观性，这提出了第一个问题。此外，通过使用音频到文本映射技术，标签不再反映对歌曲发生的事情的“真实”表示。它们没有提供对音乐的准确描述。鉴于这些明显的缺陷，人们可能会怀疑为什么这种方法仍然能够产生有用的结果。
- en: Bias vs. Noise
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 偏差 vs. 噪声
- en: 'When a dataset’s labels are not accurately assigned, there can be two main
    causes: bias and noise. Bias refers to a consistent tendency for the labels to
    be untruthful in a particular way. For instance, if the dataset frequently labels
    instrumental pieces as songs but never identifies songs as instrumental pieces,
    it demonstrates a bias toward predicting the presence of vocals.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 当数据集的标签分配不准确时，可能有两个主要原因：偏差和噪声。偏差指的是标签在某种特定方式上的一致性倾向不真实。例如，如果数据集经常将器乐曲标记为歌曲，但从不将歌曲标记为器乐曲，这就显示了对预测存在人声的偏向。
- en: On the other hand, noise indicates a general variability in the labels, regardless
    of the direction. For example, if every track is labeled as a “sad piano piece,”
    the dataset is heavily biased, as it consistently provides an inaccurate label
    for many songs. However, since it applies the same label to every track, there
    is no variability and therefore no noise present in the dataset.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，噪声表示标签的一般变异性，无论方向如何。例如，如果每一首曲目都标记为“悲伤的钢琴曲”，那么数据集就会严重偏向，因为它对许多歌曲提供了一致的不准确标签。然而，由于它对每个曲目应用相同的标签，因此数据集中没有变异性，也就没有噪声。
- en: By mapping tracks to descriptive texts written for other tracks, we introduce
    noise. This is because, for most tracks, it is unlikely that there exists a perfect
    description for it in the dataset. Consequently, most labels are a little bit
    off, i.e. untruthful, which results in noise. However, are the labels biased?
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将曲目映射到为其他曲目写的描述文本，我们引入了噪声。这是因为，对于大多数曲目而言，数据集中不太可能存在完美的描述。因此，大多数标签都稍有偏差，即不真实，这就产生了噪声。然而，这些标签是否有偏见？
- en: Since the available descriptions were generated for popular songs, it is reasonable
    to assume that the pool of descriptions is biased toward (western) popular music.
    Nevertheless, with 4 million descriptions based on 150k unique songs, one would
    expect a diverse range of descriptions to choose from. Additionally, most labeled
    music datasets exhibit the same bias, so this is not a unique disadvantage of
    this approach compared to others. What truly sets this approach apart is the introduction
    of added noise.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 由于现有的描述是为流行歌曲生成的，因此可以合理地假设这些描述池倾向于（西方）流行音乐。尽管如此，基于150k首独特歌曲的400万条描述，仍然可以期望有多样化的描述可供选择。此外，大多数标记的音乐数据集也表现出相同的偏向，因此这并不是这种方法相较于其他方法的独特劣势。真正使这种方法与众不同的是引入了额外的噪声。
- en: Why Noise can be O.K. in Machine Learning
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 噪声在机器学习中可以是可以接受的
- en: Training a machine learning model on a biased dataset is usually not a desirable
    approach because it would result in the model learning and replicating a biased
    understanding of the task at hand. However, training a machine learning model
    on unbiased but noisy data can still yield impressive results. Allow me to illustrate
    this with an example.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在偏向的数据集上训练机器学习模型通常不是一种理想的方法，因为这会导致模型学习和复制对任务的偏见理解。然而，在无偏但有噪声的数据上训练机器学习模型仍然可以获得令人印象深刻的结果。让我用一个例子来说明。
- en: Consider the figure below, which depicts two datasets consisting of orange and
    blue points. In the noise-free dataset, the blue and orange points are perfectly
    separable. However, in the noisy dataset, some orange points have shifted into
    the blue point cluster, and vice versa. Despite this added noise, if we examine
    the trained models, we observe that both models identify roughly the same patterns.
    This is because, even in the presence of noise, the AI learns to identify the
    optimal pattern that minimizes errors as much as possible.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 请考虑下图，其中展示了两个数据集，由橙色和蓝色点组成。在没有噪声的数据集中，蓝色和橙色点是完全可分的。然而，在有噪声的数据集中，一些橙色点已移动到蓝色点簇中，反之亦然。尽管有了这些附加噪声，如果我们检查经过训练的模型，会发现两者识别的模式大致相同。这是因为即使在存在噪声的情况下，AI也会学习识别最优模式，以尽可能减少错误。
- en: '![](../Images/e0233bcba3f0802135bda41d3976caa3.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e0233bcba3f0802135bda41d3976caa3.png)'
- en: An example for an AI model trained on noise-free and noisy data. Images generated
    with [Tensorflow Neural Network Playground](https://playground.tensorflow.org).
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 一个AI模型在无噪声和有噪声数据上训练的例子。图像生成使用了 [Tensorflow Neural Network Playground](https://playground.tensorflow.org)。
- en: This example demonstrates that an AI can indeed learn from noisy datasets, such
    as the one generated by Google. However, the main challenge lies in the fact that
    the noisier the dataset is, the larger amount of training data required to effectively
    train the model. This rationale is justified by the understanding that a noisy
    dataset inherently contains less valuable information compared to an equivalent
    noise-free dataset of the same size.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 这个例子表明，AI确实可以从噪声数据集中学习，例如谷歌生成的数据集。然而，主要的挑战在于数据集的噪声越大，所需的训练数据量就越大，以有效地训练模型。这一理由得到了支持，因为噪声数据集相对于大小相同的无噪声数据集，固有地包含较少有价值的信息。
- en: Conclusion
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: In conclusion, Google employed innovative techniques to address the challenge
    of limited labeled music data in training their generative music AI models. They
    utilized weakly associated labels for MuLan, leveraging text information from
    various sources related to music videos, and employed a language model to filter
    out irrelevant data. When developing Noise2Music, they introduced fake labels
    by generating multiple descriptions for popular songs and mapping them to suitable
    tracks using their pre-trained model.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，谷歌采用了创新技术来应对训练其生成音乐AI模型时有限的标记音乐数据的问题。他们为MuLan使用了弱相关标签，利用了来自各种音乐视频相关来源的文本信息，并使用语言模型来过滤掉无关数据。在开发Noise2Music时，他们通过为热门歌曲生成多个描述并使用预训练模型将其映射到合适的曲目，从而引入了伪标签。
- en: While these approaches may deviate from traditional labeling methods, they still
    proved effective. Despite introducing noise, the models were still able to learn
    and identify optimal patterns. Although the utilization of fake datasets may be
    considered unconventional, it highlights the immense potential of modern language
    models in creating large and valuable datasets for machine learning.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这些方法可能偏离了传统的标记方法，但它们仍然证明了有效性。尽管引入了噪声，模型仍然能够学习并识别最佳模式。尽管使用伪造数据集可能被认为是不寻常的，但它突显了现代语言模型在创建大规模和有价值的数据集方面的巨大潜力。
- en: 'I write a lot about music and AI. Here are three of my articles you may like:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 我写了很多关于音乐和AI的文章。以下是你可能喜欢的三篇文章：
- en: '[MusicLM — Has Google Solved AI Music Generation?](/musiclm-has-google-solved-ai-music-generation-c6859e76bc3c)'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[MusicLM — 谷歌解决了AI音乐生成的问题了吗？](/musiclm-has-google-solved-ai-music-generation-c6859e76bc3c)'
- en: '[AudioGPT — A Glimpse into the Future of Creating Music](/audiogpt-a-glimpse-into-the-future-of-creating-music-9e8e0c65069e)'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[AudioGPT — 未来音乐创作的瞥见](/audiogpt-a-glimpse-into-the-future-of-creating-music-9e8e0c65069e)'
- en: '[Zero-Shot Song Lyrics Transcription Using Whisper](https://medium.com/mlearning-ai/zero-shot-song-lyrics-transcription-using-whisper-3f360499bcfe)'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用 Whisper 的零样本歌曲歌词转录](https://medium.com/mlearning-ai/zero-shot-song-lyrics-transcription-using-whisper-3f360499bcfe)'
- en: References
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: '**[1]** Huang et al. (2022). Mulan: A Joint Embedding of Music Audio and Natural
    Language. [https://arxiv.org/abs/2208.12415](https://arxiv.org/abs/2208.12415)'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '**[1]** Huang 等人 (2022)。Mulan：音乐音频与自然语言的联合嵌入。 [https://arxiv.org/abs/2208.12415](https://arxiv.org/abs/2208.12415)'
- en: '**[2]** Agostinelli et al. (2023). MusicLM: Generating Music from Text. [https://arxiv.org/abs/2301.11325](https://arxiv.org/abs/2301.11325)'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '**[2]** Agostinelli 等人 (2023)。MusicLM：从文本生成音乐。 [https://arxiv.org/abs/2301.11325](https://arxiv.org/abs/2301.11325)'
