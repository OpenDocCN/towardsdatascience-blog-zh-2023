- en: 'ML Basics (Part-4): Decision Trees'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习基础（第4部分）：决策树
- en: 原文：[https://towardsdatascience.com/ml-basics-part-4-decision-trees-cc37d07137b2](https://towardsdatascience.com/ml-basics-part-4-decision-trees-cc37d07137b2)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/ml-basics-part-4-decision-trees-cc37d07137b2](https://towardsdatascience.com/ml-basics-part-4-decision-trees-cc37d07137b2)
- en: '**What are Decision Trees, How to Build and Apply Decision Trees for Different
    Classification Tasks**'
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**什么是决策树，如何构建和应用决策树于不同的分类任务**'
- en: '[](https://azad-wolf.medium.com/?source=post_page-----cc37d07137b2--------------------------------)[![J.
    Rafid Siddiqui, PhD](../Images/02280890ed87239c75cbcbfa7c5d686c.png)](https://azad-wolf.medium.com/?source=post_page-----cc37d07137b2--------------------------------)[](https://towardsdatascience.com/?source=post_page-----cc37d07137b2--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----cc37d07137b2--------------------------------)
    [J. Rafid Siddiqui, PhD](https://azad-wolf.medium.com/?source=post_page-----cc37d07137b2--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://azad-wolf.medium.com/?source=post_page-----cc37d07137b2--------------------------------)[![J.
    Rafid Siddiqui, PhD](../Images/02280890ed87239c75cbcbfa7c5d686c.png)](https://azad-wolf.medium.com/?source=post_page-----cc37d07137b2--------------------------------)[](https://towardsdatascience.com/?source=post_page-----cc37d07137b2--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----cc37d07137b2--------------------------------)
    [J. Rafid Siddiqui, PhD](https://azad-wolf.medium.com/?source=post_page-----cc37d07137b2--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----cc37d07137b2--------------------------------)
    ·8 min read·Jan 4, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----cc37d07137b2--------------------------------)
    ·8分钟阅读·2023年1月4日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/854204edb68a795f1f9cc7135d68d19a.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/854204edb68a795f1f9cc7135d68d19a.png)'
- en: 'Figure 1: Decision Tree Classifiers (Source: Author)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：决策树分类器（来源：作者）
- en: In the previous articles, we have explored the concepts of [*Regression*](https://azad-wolf.medium.com/ml-basics-part-1-regression-a-gateway-method-to-machine-learning-36d54d233907),
    [*Support Vector Machines*](https://azad-wolf.medium.com/ml-basics-part-2-support-vector-machines-ac4defba2615)*,
    and* [*Artificial Neural Networks*](https://azad-wolf.medium.com/ml-basics-part-3-artificial-neural-networks-879851bcd217)*.*
    In this article, we shall go through another Machine Learning concept called,
    *Decision Trees*. You can check-out the other methods from the aforementioned
    links.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的文章中，我们探讨了[*回归*](https://azad-wolf.medium.com/ml-basics-part-1-regression-a-gateway-method-to-machine-learning-36d54d233907)、[*支持向量机*](https://azad-wolf.medium.com/ml-basics-part-2-support-vector-machines-ac4defba2615)*以及*
    [*人工神经网络*](https://azad-wolf.medium.com/ml-basics-part-3-artificial-neural-networks-879851bcd217)*。在本文中，我们将介绍另一种机器学习概念——*决策树*。您可以通过上述链接查看其他方法。
- en: '**Introduction**'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '**介绍**'
- en: Decision Trees are perhaps one of the simplest and the most intuitive classification
    methods in a Machine Learning toolbox. The first occurrence of Decision Trees
    appeared in a publication by *William Belson* in 1959\. Earlier uses of Decision
    Trees were limited to Taxonomy for their natural semblance for that type of data.
    Later adaptations resulted in Decision Tree Classifiers which were not limited
    to a particular type of data (e.g., nominal) and were equally suited for other
    data types (e.g., numerical). Decision Trees are a simple yet effective tool for
    many classification problems and in some cases, can outperform other more complicated
    methods which might turn out to be overkill for a simple dataset.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树也许是机器学习工具箱中最简单和最直观的分类方法之一。决策树的首次出现是在*威廉·贝尔森*于1959年的一篇出版物中。早期对决策树的使用主要限于分类法，因为它们与这种数据类型的自然相似性。后来，决策树分类器的适应版本应运而生，它们不再局限于特定的数据类型（例如，名义型数据），也适用于其他数据类型（例如，数值型数据）。决策树是一种简单却有效的工具，用于许多分类问题，在某些情况下，它们可以超越其他更复杂的方法，这些复杂的方法对于简单的数据集可能显得过于繁琐。
- en: '**Decision Nodes, Constraints and Leaf Nodes**'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '**决策节点、约束和叶节点**'
- en: A decision tree — as the name suggests, is a tree data structure with a set
    of decision nodes and a set of edges connected in a directed graph. A Decision
    Node contains the decision criterion (i.e., a constraint) which determines the
    outgoing path from the node to the other node in the tree. Most commonly occurring
    decision trees are binary trees with only two edges at each decision node, however,
    there can be trees with more than two edges as well (e.g., trees built on nominal
    data). The constraint can be as simple as consisting of one variable (e.g., **X₁**
    **≤** C) or they can consist of a linear combination of multiple features (e.g.,
    **X₁** **≤ X** **₂ +** C). Similarly, constraints can also be non-linear. The
    leaf-nodes of the tree contain the classification labels. This means when we traverse
    a constructed tree from a trained dataset and reach a leaf node, we get the classification
    label for a test data point. In figure 2, we can see an example dataset and the
    respective learned decision tree from that data.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树——顾名思义，是一种树形数据结构，具有一组决策节点和一组在有向图中连接的边。决策节点包含决策标准（即约束），决定从节点到树中其他节点的出边路径。最常见的决策树是二叉树，每个决策节点只有两条边，但也可以有多于两条边的树（例如，基于名义数据构建的树）。约束可以简单到由一个变量组成（例如，**X₁**
    **≤** C），也可以由多个特征的线性组合组成（例如，**X₁** **≤ X** **₂ +** C）。同样，约束也可以是非线性的。树的叶节点包含分类标签。这意味着，当我们从训练数据集遍历构建的树并到达一个叶节点时，我们会得到测试数据点的分类标签。在图2中，我们可以看到一个示例数据集以及从该数据中学习到的决策树。
- en: '![](../Images/754897976473b07e3a965f21b4121f14.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/754897976473b07e3a965f21b4121f14.png)'
- en: 'Figure 2: Left: A random set of data points from two classes forming a set
    of clusters, Right: A decision tree diagram learned from the data — Numbers only
    used as markers for later reference. (Source: Author)'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：左侧：来自两个类别的一组随机数据点形成的簇，右侧：从数据中学习到的决策树图示——数字仅用于后续参考标记（来源：作者）
- en: '**Learning the Tree**'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '**学习树**'
- en: As it was mentioned in the previous section, a decision tree is a collection
    of decision constraints arranged in the form of a tree whereby the leaf nodes
    provide classification for a particular data instance. The most important step
    then is to find out the best constraints that we should arrange in the form of
    a tree. This we do by finding the best constraint at each node that provides an
    optimum data split for its child nodes. More specifically, at any given node,
    we pick a feature, and we generate a set of candidate constraints by iteratively
    going through all the variable values in the training-set for that feature. For
    example, we if we pick a variable **X₀** then the candidate constraints would
    be generated by using this variable against all the values of that variable (i.e.,
    **X₀ ≤ C).** Where C is the constant value of that variable in the training-set.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 正如前一节所提到的，决策树是一组以树形排列的决策约束，其中叶节点为特定数据实例提供分类。最重要的步骤是找出我们应该以树形排列的最佳约束。我们通过在每个节点找到最佳约束来做到这一点，这样可以为其子节点提供最佳的数据拆分。更具体地说，在任何给定节点，我们选择一个特征，然后通过迭代地遍历该特征在训练集中的所有变量值来生成一组候选约束。例如，如果我们选择一个变量**X₀**，那么候选约束将通过将该变量与该变量的所有值进行比较来生成（即，**X₀
    ≤ C**）。其中C是该变量在训练集中的常数值。
- en: '![](../Images/46b1c8f589e09f34d25e47e4bd0ee408.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/46b1c8f589e09f34d25e47e4bd0ee408.png)'
- en: 'Figure 3: Data Clusters, Decision Tree, and Visualization of Constraints with
    data splits (Source: Author)'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：数据簇、决策树以及数据拆分的约束可视化（来源：作者）
- en: 'The same process is repeated for all the feature variables and all the candidate
    constraints are obtained. In order to find out the optimum constraint, we apply
    the constraint on the data at that node and obtain two subsets of data: one for
    left branch, which satisfies the constraint and one for the right branch, which
    negates the constraint. We now need a measure to find out how well is the split.
    In other words, we want to know whether splitting the data improves our chances
    of finding the true data labels. For this we use an entropy measure.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 对所有特征变量重复相同的过程，获得所有候选约束。为了找出最佳约束，我们在该节点的数据上应用该约束，得到两个数据子集：一个是左分支，满足该约束，另一个是右分支，否定该约束。我们现在需要一个度量来评估拆分的效果。换句话说，我们想知道拆分数据是否提高了找到真实数据标签的机会。为此，我们使用熵度量。
- en: '![](../Images/ae3ad3326d1535abe356fb4745bd12ee.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ae3ad3326d1535abe356fb4745bd12ee.png)'
- en: 'Figure 4: Equations of Entropy (Source: Author)'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：熵的方程式（来源：作者）
- en: Entropy is the measure of uncertainty in the system. In information theory,
    entropy is the amount of information (surprise/uncertainty) present in the data.
    For example, a binary set representing the outcomes of a random variable **X ∈**
    [1,1,1,1,1,1] has zero entropy. Which means it has the least amount of information
    and therefore, we are certain about what would be the value for all the future
    occurrences in that set. However, for a set of values **X ∈** [1,1,1,0,0,0], the
    entropy would be *1.0* which means it has the highest amount of information, and
    we are not certain at all what would be the value of new events.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 熵是系统中不确定性的度量。在信息理论中，熵是数据中存在的信息量（惊讶/不确定性）。例如，一个表示随机变量结果的二元集**X ∈** [1,1,1,1,1,1]的熵为零。这意味着它包含最少量的信息，因此我们对该集合中所有未来发生的事件的值都很确定。然而，对于值集**X
    ∈** [1,1,1,0,0,0]，熵为*1.0*，这意味着它包含最多的信息，我们完全不确定新事件的值是什么。
- en: Now we measure the entropy of the classification labels in each data split and
    compare it against the entropy of the parent. More specifically, we compute a
    measure called *Information Gain* which measures the amount of information gained
    about a random variable by observing the outcome of another random variable. In
    the context of decision tree, this means the reduction in the amount of entropy
    of a child node in comparison to the parent node. Therefore, it can be computed
    by subtracting the entropy of the child node from the entropy of the parent node.
    In case of multiple child nodes, the weighted average of the child entropies is
    subtracted from the parent’s entropy. The weights can be computed by computing
    the probabilities of the left and right child with respect to the parent node.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们测量每个数据分割中分类标签的熵，并将其与父节点的熵进行比较。更具体地说，我们计算一个叫做*信息增益*的量度，它测量通过观察另一个随机变量的结果来获得关于一个随机变量的信息量。在决策树的背景下，这意味着相对于父节点，子节点的熵减少。因此，它可以通过从父节点的熵中减去子节点的熵来计算。如果有多个子节点，则从父节点的熵中减去子节点熵的加权平均值。权重可以通过计算左子节点和右子节点相对于父节点的概率来计算。
- en: '![](../Images/21919f25a9df3cc289c0cbb3e46328b6.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/21919f25a9df3cc289c0cbb3e46328b6.png)'
- en: 'Figure 5: Information Gain Computation (Source: Author)'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 图5：信息增益计算（来源：作者）
- en: We compute the *Information Gain* for each candidate constraint and choose the
    constraint which has the maximum value for *Information Gain.* Intuitively, we
    are trying to find those data splits in which there is the least amount of variation
    in terms of class labels. We do this until the leaf node where we want to end
    up with class labels of only one class. However, depending on the depth of the
    tree we build, we may not have single class labels at the leaf nodes, in which
    case, we shall take the class labels of the majority class of the data split as
    a final classification label. In Figure 3, you can see the tree building process
    with the optimum constraints, data splits and the classification labels at the
    leaf nodes.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 我们为每个候选约束计算*信息增益*，并选择具有最大*信息增益*值的约束。直观地说，我们试图找到那些在类别标签方面变化最小的数据分割。我们一直这样做，直到到达叶子节点，我们希望最终得到只有一个类别的分类标签。然而，根据我们构建的树的深度，叶子节点可能没有单一的类别标签，在这种情况下，我们将取数据分割中多数类别的类别标签作为最终分类标签。在图3中，你可以看到树的构建过程，包括最佳约束、数据分割和叶子节点的分类标签。
- en: '![](../Images/4f5ae8937f0bbae556a242a4dfa315dd.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4f5ae8937f0bbae556a242a4dfa315dd.png)'
- en: 'Figure 6: Algorithm for a Decision Tree (Source: Author)'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 图6：决策树算法（来源：作者）
- en: '**Decision Trees with Linear Constraints**'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '**带线性约束的决策树**'
- en: Using the process described in the previous section we can construct a decision
    tree in python using *numpy* and apply it on various types of data. We start by
    creating a random dataset and split it into training and test-set as shown in
    figure 7.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 使用前面描述的过程，我们可以使用*numpy*在Python中构建决策树，并将其应用于各种类型的数据。我们从创建一个随机数据集开始，并将其分割为训练集和测试集，如图7所示。
- en: '![](../Images/2123f86b9c1753899773e10652aca5e0.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2123f86b9c1753899773e10652aca5e0.png)'
- en: 'Figure 7: A set of random data points split into training and test sets (Source:
    Author)'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 图7：一组随机数据点分割为训练集和测试集（来源：作者）
- en: We can train a decision tree classifier on the training-set. The resulting decision
    tree built from the training-set is shown in figure 8.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在训练集上训练一个决策树分类器。从训练集构建的决策树如图8所示。
- en: '![](../Images/6fd311c87895323e1d499730f1efd97a.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6fd311c87895323e1d499730f1efd97a.png)'
- en: 'Figure 8: Single-Variable Decision Tree Classifier with an example prediction
    (Source: Author)'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '图 8: 单变量决策树分类器与示例预测（来源：作者）'
- en: Once a tree has been built, we can use it to predict a test point. For example,
    an instance of X=(0.5,-0.5) would be predicted as belonging to the ‘0’ class after
    traversing the tree. The prediction process only consists of a set of conditions
    applied on the data point. Training the tree is also a simple and intuitive process,
    however, it can become time consuming and cumbersome for large datasets.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦构建了树，我们可以使用它来预测测试点。例如，X=(0.5,-0.5) 的一个实例在遍历树后将被预测为属于‘0’类。预测过程仅包含一组应用于数据点的条件。训练树也是一个简单直观的过程，但对于大型数据集，它可能变得耗时且繁琐。
- en: '**Decision Trees with Non-Linear Constraints**'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '**具有非线性约束的决策树**'
- en: In the earlier section, we applied a Decision Tree classifier on linearly separable
    data. However, in most situations, data is not linearly separable. For example,
    the data in figure 9 is not linearly separable.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的部分，我们对线性可分数据应用了决策树分类器。然而，在大多数情况下，数据并不是线性可分的。例如，图 9 中的数据就不是线性可分的。
- en: '![](../Images/750a8fabca60be967e9eb2c25b31f985.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/750a8fabca60be967e9eb2c25b31f985.png)'
- en: 'Figure 9: An example of Non-Linear Data (Source: Author)'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '图 9: 非线性数据示例（来源：作者）'
- en: We can build the tree using a single variable as before, but it would have problems
    in accurately finding the decision boundary with only orthogonal constraints.
    We will therefore build constraints from a linear combination of variables. The
    learned Decision Tree from such constraints is shown in figure 10\. Note that
    in this way we model each constraint as a line and can find slanted decision boundaries.
    This would also require less depth of the tree compared to a decision tree built
    with single variable constraints for the accurate classification of same data.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以像以前一样使用单个变量来构建树，但仅使用正交约束会在准确找到决策边界时遇到问题。因此，我们将从变量的线性组合中构建约束。基于这些约束学习到的决策树如图
    10 所示。请注意，以这种方式，我们将每个约束建模为一条直线，并可以找到倾斜的决策边界。与使用单变量约束构建的决策树相比，这种方法对相同数据的准确分类所需的树深度也较少。
- en: '![](../Images/6721da8de1ca74c2b4a8b3d79cabd98e.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6721da8de1ca74c2b4a8b3d79cabd98e.png)'
- en: 'Figure 10: Multi-variable Decision Tree for Non-Linear Data (Source: Author)'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '图 10: 用于非线性数据的多变量决策树（来源：作者）'
- en: In most cases, a Decision Tree built with linear constraints would be sufficient.
    Because multiple linear constraints can accurately create a non-linear decision
    boundary. However, in certain cases, you might want to try a non-linear constraint
    for more precise classification. Such Decision Tree with non-linear constraints
    is shown in figure 11.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在大多数情况下，使用线性约束构建的决策树已经足够。因为多个线性约束可以准确地创建非线性决策边界。然而，在某些情况下，你可能希望尝试非线性约束以获得更精确的分类。具有非线性约束的决策树如图
    11 所示。
- en: '![](../Images/4eb482af4c11a806e712b1c4cc98e7ef.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4eb482af4c11a806e712b1c4cc98e7ef.png)'
- en: 'Figure 11: Decision Tree with Non-Linear Constraints (Source: Author)'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '图 11: 具有非线性约束的决策树（来源：作者）'
- en: The results of Decision Trees, both the multivariate linear as well as the non-linear
    constraints, can be seen in figure 12\. As you might notice, there is a very small
    difference between the two classification boundaries. This is because a Decision
    Tree classifier built with multivariate linear constraints is already a non-linear
    classifier. However, it might be worth using for a different dataset where the
    classification performance be significantly different.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树的结果，无论是多变量线性约束还是非线性约束，都可以在图 12 中看到。正如你可能注意到的，两种分类边界之间几乎没有差别。这是因为，使用多变量线性约束构建的决策树分类器本身就是一个非线性分类器。然而，对于分类性能可能有显著差异的其他数据集，使用这种分类器可能值得尝试。
- en: '![](../Images/c635f844af8ff483a8850478cc9112b2.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c635f844af8ff483a8850478cc9112b2.png)'
- en: 'Figure 12: Left: Classification with Multivariate Linear Constraints, Right:
    Classification with Non-Linear Constraints (Source: Author)'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '图 12: 左侧: 多变量线性约束的分类，右侧: 非线性约束的分类（来源：作者）'
- en: '**Conclusions**'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '**结论**'
- en: In this article, you have learned about the Decision Trees and how to build
    a Decision Tree. You have also learned how to apply a Decision Tree classifier
    for different data types. Decision Tree classifier is perhaps the simplest and
    most intuitive classifier in the Machine Learning Toolbox. However, in some cases,
    that is all what is needed to obtain accurate classification. You can find the
    code of Decision Tree implementation in python using *numpy* on the following
    github repository.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，你了解了决策树以及如何构建决策树。你还学会了如何对不同数据类型应用决策树分类器。决策树分类器可能是机器学习工具箱中最简单和最直观的分类器。然而，在某些情况下，这就是获得准确分类所需的一切。你可以在以下的
    GitHub 仓库中找到使用 *numpy* 实现决策树的代码。
- en: '**Code:**'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '**代码：**'
- en: '[https://www.github.com/azad-academy/MLBasics-DecisionTrees](https://www.github.com/azad-academy/MLBasics-DecisionTrees)'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://www.github.com/azad-academy/MLBasics-DecisionTrees](https://www.github.com/azad-academy/MLBasics-DecisionTrees)'
- en: '**Become a Patreon Supporter:**'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '**成为 Patreon 支持者：**'
- en: '[https://www.patreon.com/azadacademy](https://www.patreon.com/azadacademy)'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://www.patreon.com/azadacademy](https://www.patreon.com/azadacademy)'
- en: '**Find me on Substack:**'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '**在 Substack 上找到我：**'
- en: '[https://azadwolf.substack.com](https://azadwolf.substack.com)'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://azadwolf.substack.com](https://azadwolf.substack.com)'
- en: '**Follow Twitter for Updates:**'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '**关注 Twitter 以获取更新：**'
- en: '[https://twitter.com/azaditech](https://twitter.com/azaditech)'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://twitter.com/azaditech](https://twitter.com/azaditech)'
