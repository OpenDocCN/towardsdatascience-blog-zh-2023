- en: How to Do Cross-Validation Effectively
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何有效地进行交叉验证
- en: 原文：[https://towardsdatascience.com/how-to-do-cross-validation-effectively-1bbeb1d69ee8](https://towardsdatascience.com/how-to-do-cross-validation-effectively-1bbeb1d69ee8)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/how-to-do-cross-validation-effectively-1bbeb1d69ee8](https://towardsdatascience.com/how-to-do-cross-validation-effectively-1bbeb1d69ee8)
- en: 'A guide to cross-validation best practices: re-training and nesting'
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 交叉验证最佳实践指南：重新训练和嵌套
- en: '[](https://vcerq.medium.com/?source=post_page-----1bbeb1d69ee8--------------------------------)[![Vitor
    Cerqueira](../Images/9e52f462c6bc20453d3ea273eb52114b.png)](https://vcerq.medium.com/?source=post_page-----1bbeb1d69ee8--------------------------------)[](https://towardsdatascience.com/?source=post_page-----1bbeb1d69ee8--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----1bbeb1d69ee8--------------------------------)
    [Vitor Cerqueira](https://vcerq.medium.com/?source=post_page-----1bbeb1d69ee8--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://vcerq.medium.com/?source=post_page-----1bbeb1d69ee8--------------------------------)[![Vitor
    Cerqueira](../Images/9e52f462c6bc20453d3ea273eb52114b.png)](https://vcerq.medium.com/?source=post_page-----1bbeb1d69ee8--------------------------------)[](https://towardsdatascience.com/?source=post_page-----1bbeb1d69ee8--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----1bbeb1d69ee8--------------------------------)
    [Vitor Cerqueira](https://vcerq.medium.com/?source=post_page-----1bbeb1d69ee8--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----1bbeb1d69ee8--------------------------------)
    ·6 min read·Feb 6, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----1bbeb1d69ee8--------------------------------)
    ·6分钟阅读·2023年2月6日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/8ae7c75ea0734849abe1195cffecca97.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8ae7c75ea0734849abe1195cffecca97.png)'
- en: '[5-fold Monte Carlo cross-validation](https://medium.com/towards-data-science/monte-carlo-cross-validation-for-time-series-ed01c41e2995).
    Image by author.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[5折蒙特卡罗交叉验证](https://medium.com/towards-data-science/monte-carlo-cross-validation-for-time-series-ed01c41e2995)。图像来源于作者。'
- en: Cross-validation is a critical factor for building robust machine learning models.
    But, it is often not applied to its full potential.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 交叉验证是构建稳健机器学习模型的关键因素。然而，它往往未能发挥其全部潜力。
- en: 'In this article, we’ll explore two important practices to get the most out
    of cross-validation: re-training and nesting.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我们将探讨两种重要的实践，以充分利用交叉验证：重新训练和嵌套。
- en: Let’s get started!
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始吧！
- en: What is cross-validation?
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是交叉验证？
- en: Cross-validation is a technique for evaluating the performance of a model.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 交叉验证是一种评估模型性能的技术。
- en: This process usually involves testing several techniques. Or doing hyperparameter
    optimization of a particular method. In such cases, your goal is to check which
    alternative is best for the input data.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 这个过程通常涉及测试几种技术，或对特定方法进行超参数优化。在这种情况下，你的目标是检查哪种替代方案最适合输入数据。
- en: The idea is to select the approach that maximizes performance. This is the model
    that will be deployed into production. Besides, you also want to get a reliable
    estimate of that model’s performance.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 关键是选择能最大化性能的方法。这就是将被部署到生产中的模型。此外，你还希望对该模型的性能有一个可靠的估计。
- en: Re-training After Cross-Validation
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 交叉验证后的重新训练
- en: '![](../Images/0842a1b43d9c8f5bb44fee8cb06c0ff4.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0842a1b43d9c8f5bb44fee8cb06c0ff4.png)'
- en: After cross-validation, you should re-train the best model using all available
    data. Image by author.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 交叉验证后，你应该使用所有可用数据重新训练最佳模型。图像来源于作者。
- en: Suppose you do cross-validation to select a model. You test many alternatives
    using 5-fold cross-validation. Then, a linear regression comes out on top.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你进行交叉验证以选择一个模型。你使用5折交叉验证测试许多备选方案。然后，线性回归模型表现最佳。
- en: What should you do next?
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 那么接下来应该做什么呢？
- en: Should you re-train the linear regression using all available data? or should
    you use the models trained during cross-validation?
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 是否应该使用所有可用数据重新训练线性回归模型？还是应该使用交叉验证过程中训练的模型？
- en: This part creates some confusion among data scientists — not only among beginners
    but also among more seasoned professionals.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 这一部分在数据科学家中会引发一些困惑——不仅在初学者中，还有更有经验的专业人士中也会有。
- en: After cross-validation, you should re-train the best approach using all available
    data. Here’s a quote taken from the legendary book *Elements of Statistical Learning
    [1]*(parenthesis mine)*:*
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 交叉验证后，你应该使用所有可用数据重新训练最佳方法。以下是来自传奇书籍 *《统计学习的元素 [1]*（括号内为我所加）*：*
- en: Our final chosen model [after cross-validation] is f(x), which we then fit to
    all the data.
  id: totrans-23
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 我们在交叉验证后的最终选择模型是 f(x)，然后将其拟合到所有数据上。
- en: But, this idea is not consensual.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 但这一想法并不被普遍接受。
- en: Some practitioners keep the best models trained during cross-validation. Following
    the example above, you’d keep 5 linear regression models. Then, during the deployment
    stage, you’d average their predictions for each prediction.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 一些从业者在交叉验证期间保留了最佳模型。按照上述示例，你会保留5个线性回归模型。然后，在部署阶段，你会对每个预测进行预测平均。
- en: That’s not how cross-validation works.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 这不是交叉验证的工作方式。
- en: 'There are two problems with this approach:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法有两个问题：
- en: It uses fewer data for training;
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它使用较少的数据进行训练；
- en: It leads to increased costs due to having to maintain many models.
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它导致了成本增加，因为需要维护多个模型。
- en: Fewer data
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据较少
- en: By not re-training, you’re not using all available instances for creating a
    model.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 如果不重新训练，你将不会利用所有可用的实例来创建模型。
- en: This can lead to a sub-optimal model unless you have tons of data. Training
    with all available instances is likely to generalize better.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 如果没有大量数据，这可能会导致次优模型。使用所有可用实例进行训练更可能实现更好的泛化。
- en: Re-training is especially important in time series because the most recent observations
    are used for testing. By not re-training in these, the model might miss newly
    emerged patterns.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 重新训练在时间序列中特别重要，因为最新的观察结果用于测试。如果不在这些时间点重新训练，模型可能会遗漏新出现的模式。
- en: Increased costs
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 成本增加
- en: One can argue that combining the 5 models trained during cross-validation leads
    to better performance.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 有人认为，将交叉验证期间训练的5个模型结合起来可以带来更好的性能。
- en: Yet, it’s important to understand the implications. You’re no longer using a
    simple, interpretable, linear regression.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，理解其影响是很重要的。你不再使用简单、可解释的线性回归。
- en: Your model is an ensemble whose individual models are trained by random subsampling.
    Random subsampling is a way of [introducing diversity in ensembles](https://medium.com/towards-data-science/how-to-measure-and-improve-the-diversity-of-forecasting-ensembles-2ec899014d6).
    Ensembles often perform better than single models. But, they also lead to extra
    costs and lower transparency.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 你的模型是一个集成体，其中的个别模型通过随机子采样进行训练。随机子采样是一种[在集成模型中引入多样性](https://medium.com/towards-data-science/how-to-measure-and-improve-the-diversity-of-forecasting-ensembles-2ec899014d6)的方法。集成模型通常比单一模型表现更好。但它们也会导致额外的成本和较低的透明度。
- en: What if you just keep one, instead of combining all models?
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你只保留一个模型，而不是将所有模型结合起来，会怎么样？
- en: That would solve the problem of increased costs. Yet, it’s not clear which version
    of the model you should choose.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 这将解决成本增加的问题。然而，仍不清楚你应该选择哪个版本的模型。
- en: There are two reasons re-training can be skipped. If the data set is large or
    if re-training is too costly. These two issues are often linked.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 重新训练可以跳过的两个原因。如果数据集很大或者重新训练成本太高。这两个问题通常是相关的。
- en: Re-training — Practical example
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 重新训练——实际示例
- en: 'Here’s an example of how you can re-train the best model after cross-validation:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是如何在交叉验证后重新训练最佳模型的一个示例：
- en: '[PRE0]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The goal is to optimize the number of trees in a Random Forest. This is done
    with the *GridSearchCV* class from *scikit-learn.* You can set the parameter *refit=True,*
    and the best model is re-trained after cross-validation automatically.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 目标是优化随机森林中的树木数量。这是通过*scikit-learn*中的*GridSearchCV*类完成的。你可以设置参数*refit=True*，这样在交叉验证后，最佳模型将自动重新训练。
- en: 'You can do this explicitly by getting the best parameters from *GridSearchCV*
    to initialize a new model:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过从*GridSearchCV*获得最佳参数来显式地初始化新模型：
- en: '[PRE1]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Getting Reliable Performance Estimates
  id: totrans-47
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 获得可靠的性能估计
- en: 'When developing a model, you want to achieve three things:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在开发模型时，你希望达到三个目标：
- en: Select a model among many alternatives;
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从众多备选方案中选择一个模型；
- en: Train the selected model and deploy it;
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练所选模型并部署它；
- en: Get a reliable estimate of the performance of the selected model.
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 获得所选模型的可靠性能估计。
- en: Cross-validation and re-training cover the first two points, but not the third.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 交叉验证和重新训练涵盖了前两个要点，但不包括第三个。
- en: Why is that?
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么会这样？
- en: Cross-validation is often repeated several times before selecting a final model.
    You test different transformations and hyperparameters. So, you end up adjusting
    your method until you’re happy with the result.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 交叉验证通常会重复进行多次，以便选择最终模型。你会测试不同的转换和超参数。因此，你最终会调整方法，直到对结果感到满意为止。
- en: This can lead to overfitting because the details of the validation sets can
    leak into the model. Thus, the performance estimate you get from cross-validation
    can be too optimistic. You can read more about this in the article in reference
    [2].
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 这可能导致过拟合，因为验证集的细节可能泄漏到模型中。因此，你从交叉验证中得到的性能估计可能过于乐观。你可以在参考文献[2]中阅读更多相关内容。
- en: This is one of the reasons why Kaggle competitions have two leaderboards, one
    public and another private. This prevents competitors from overfitting the test
    set.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 这也是为什么Kaggle比赛有两个排行榜，一个是公共的，另一个是私人的。这可以防止竞争者对测试集进行过拟合。
- en: So, how do you solve this problem?
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，你该如何解决这个问题呢？
- en: 'You should make an extra evaluation step. After cross-validation, you evaluate
    the selected model in a held-out test set. The full workflow is like this:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该增加一个额外的评估步骤。在交叉验证后，你需要在一个保留的测试集上评估选择的模型。完整的工作流程如下：
- en: Split the available data into training and testing sets;
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将可用数据划分为训练集和测试集；
- en: Apply cross-validation with the training set to select a model;
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用训练集应用交叉验证来选择模型；
- en: Re-train the chosen model using the training data and evaluate it on the test
    set. This provides you with an unbiased performance estimate;
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用训练数据重新训练选择的模型，并在测试集上进行评估。这为你提供了一个无偏的性能估计；
- en: Re-train the chosen model using all available data and deploy it.
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用所有可用数据重新训练选择的模型并部署它。
- en: 'Here’s a visual description of this process:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 这是这个过程的可视化描述：
- en: '![](../Images/19bde3c87a9d8918ee34d28dd41448a4.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/19bde3c87a9d8918ee34d28dd41448a4.png)'
- en: Applying cross-validation with training data. After cross-validation, re-training
    the chosen model and evaluate it on the test set. Finally, re-train the chosen
    model and deploy it. Image by author.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 应用交叉验证和训练数据。在交叉验证之后，再训练选择的模型并在测试集上进行评估。最后，再次训练选择的模型并部署它。图片来源：作者。
- en: Practical example
  id: totrans-66
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实际例子
- en: Here’s a practical example of the complete process using *scikit-learn.* You
    can check the comments for more context.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 这是使用*scikit-learn*的完整过程的实际例子。你可以查看评论以获取更多背景信息。
- en: '[PRE2]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Nested cross-validation
  id: totrans-69
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 嵌套交叉验证
- en: The above is a simplified version of what’s called nested cross-validation.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 上述内容是所谓嵌套交叉验证的简化版本。
- en: In nested cross-validation, you carry out a full internal cross-validation process
    in each fold of an external cross-validation process. The goal of the internal
    process is to select the best model. Then, the external process provides unbiased
    performance estimates for this model.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在嵌套交叉验证中，你在外部交叉验证过程的每个折叠中执行完整的内部交叉验证过程。内部过程的目标是选择最佳模型。然后，外部过程为该模型提供无偏的性能估计。
- en: Nested cross-validation becomes inefficient quite quickly. It’s only practical
    on small data sets.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 嵌套交叉验证很快变得效率低下。它仅在小型数据集上是实际可行的。
- en: 'Most practitioners settle for the process exemplified above. If you have a
    large data set, you can also replace the cross-validation procedure with a single
    split. This way, you get three partitions: training, validation, and testing.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数从业者都采用上述过程。如果你有一个大型数据集，你也可以用单次划分来替代交叉验证程序。这样，你会得到三个部分：训练、验证和测试。
- en: Key Takeaways
  id: totrans-74
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 关键要点
- en: Nesting and re-training are two essential aspects of cross-validation.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 嵌套和再训练是交叉验证的两个重要方面。
- en: The performance estimates of the model selected by cross-validation can be too
    optimistic. So, you should make a three-way split to get reliable estimates. A
    three-way split is a form of nested cross-validation.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 交叉验证选择的模型的性能估计可能过于乐观。因此，你应该进行三重划分以获得可靠的估计。三重划分是一种嵌套交叉验证形式。
- en: After selecting a model, or estimating its performance, you should re-train
    it with all available data. This way, the model is likely to perform better in
    new observations.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在选择模型或估计其性能后，你应使用所有可用数据重新训练模型。这样，模型在新观察数据中的表现更可能更好。
- en: Thanks for reading, and see you in the next story!
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 感谢阅读，下次故事见！
- en: Related Articles
  id: totrans-79
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 相关文章
- en: '[4 Things to Do When Applying Cross-Validation with Time Series](/4-things-to-do-when-applying-cross-validation-with-time-series-c6a5674ebf3a)'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[应用时间序列交叉验证时需要做的4件事](/4-things-to-do-when-applying-cross-validation-with-time-series-c6a5674ebf3a)'
- en: '[Monte Carlo Cross-Validation for Time Series](/monte-carlo-cross-validation-for-time-series-ed01c41e2995)'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[时间序列的蒙特卡罗交叉验证](/monte-carlo-cross-validation-for-time-series-ed01c41e2995)'
- en: References
  id: totrans-82
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] [Hastie, Trevor, et al. *The elements of statistical learning: data mining,
    inference, and prediction*. Vol. 2\. New York: springer, 2009](https://hastie.su.domains/Papers/ESLII.pdf).'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] [Hastie, Trevor, 等. *统计学习的要素：数据挖掘、推断与预测*. 第2卷. 纽约: springer, 2009](https://hastie.su.domains/Papers/ESLII.pdf).'
- en: '[2] Cawley, Gavin C., and Nicola LC Talbot. “On over-fitting in model selection
    and subsequent selection bias in performance evaluation.” *The Journal of Machine
    Learning Research* 11 (2010): 2079–2107.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] Cawley, Gavin C., 和 Nicola LC Talbot. “模型选择中的过拟合及其对性能评估的选择偏差。” *机器学习研究期刊*
    11 (2010): 2079–2107.'
