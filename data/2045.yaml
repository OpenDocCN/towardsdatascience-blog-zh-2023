- en: 'GPT vs BERT: Which is Better?'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: GPT与BERT：哪个更好？
- en: 原文：[https://towardsdatascience.com/gpt-vs-bert-which-is-better-2f1cf92af21a?source=collection_archive---------0-----------------------#2023-06-23](https://towardsdatascience.com/gpt-vs-bert-which-is-better-2f1cf92af21a?source=collection_archive---------0-----------------------#2023-06-23)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/gpt-vs-bert-which-is-better-2f1cf92af21a?source=collection_archive---------0-----------------------#2023-06-23](https://towardsdatascience.com/gpt-vs-bert-which-is-better-2f1cf92af21a?source=collection_archive---------0-----------------------#2023-06-23)
- en: 'Comparing two large-language models: Approach and example'
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 比较两个大规模语言模型：方法和示例
- en: '[](https://pranay-dave9.medium.com/?source=post_page-----2f1cf92af21a--------------------------------)[![Pranay
    Dave](../Images/accecc418ea23d26862761bf470fcf04.png)](https://pranay-dave9.medium.com/?source=post_page-----2f1cf92af21a--------------------------------)[](https://towardsdatascience.com/?source=post_page-----2f1cf92af21a--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----2f1cf92af21a--------------------------------)
    [Pranay Dave](https://pranay-dave9.medium.com/?source=post_page-----2f1cf92af21a--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://pranay-dave9.medium.com/?source=post_page-----2f1cf92af21a--------------------------------)[![Pranay
    Dave](../Images/accecc418ea23d26862761bf470fcf04.png)](https://pranay-dave9.medium.com/?source=post_page-----2f1cf92af21a--------------------------------)[](https://towardsdatascience.com/?source=post_page-----2f1cf92af21a--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----2f1cf92af21a--------------------------------)
    [Pranay Dave](https://pranay-dave9.medium.com/?source=post_page-----2f1cf92af21a--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F89d4bb7ead78&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgpt-vs-bert-which-is-better-2f1cf92af21a&user=Pranay+Dave&userId=89d4bb7ead78&source=post_page-89d4bb7ead78----2f1cf92af21a---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----2f1cf92af21a--------------------------------)
    ·6 min read·Jun 23, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F2f1cf92af21a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgpt-vs-bert-which-is-better-2f1cf92af21a&user=Pranay+Dave&userId=89d4bb7ead78&source=-----2f1cf92af21a---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F89d4bb7ead78&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgpt-vs-bert-which-is-better-2f1cf92af21a&user=Pranay+Dave&userId=89d4bb7ead78&source=post_page-89d4bb7ead78----2f1cf92af21a---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----2f1cf92af21a--------------------------------)
    · 6分钟阅读·2023年6月23日 [](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F2f1cf92af21a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgpt-vs-bert-which-is-better-2f1cf92af21a&user=Pranay+Dave&userId=89d4bb7ead78&source=-----2f1cf92af21a---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F2f1cf92af21a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgpt-vs-bert-which-is-better-2f1cf92af21a&source=-----2f1cf92af21a---------------------bookmark_footer-----------)![](../Images/948d3d929fc0378773816d8614d45deb.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F2f1cf92af21a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgpt-vs-bert-which-is-better-2f1cf92af21a&source=-----2f1cf92af21a---------------------bookmark_footer-----------)![](../Images/948d3d929fc0378773816d8614d45deb.png)'
- en: Image created by DALLE and PPT by author ([https://labs.openai.com/s/qQgpAQbLi0srYlZHOHQjGKWh](https://labs.openai.com/s/qQgpAQbLi0srYlZHOHQjGKWh))
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 图像由DALLE创建，PPT由作者制作 ([https://labs.openai.com/s/qQgpAQbLi0srYlZHOHQjGKWh](https://labs.openai.com/s/qQgpAQbLi0srYlZHOHQjGKWh))
- en: 'The rise in popularity of generative AI has also led to an increase in the
    number of large language models. In this story, I will make a comparison between
    two of them: GPT and BERT. GPT (Generative Pre-trained Transformer) is developed
    by OpenAI and is based on decoder-only architecture. On the other hand, BERT (Bidirectional
    Encoder Representations from Transformers) is developed by Google and is an encoder-only
    pre-trained model'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 生成式AI的流行也导致了大规模语言模型数量的增加。在这个故事中，我将对比两种模型：GPT和BERT。GPT（生成预训练变换器）由OpenAI开发，基于仅解码器架构。而BERT（双向编码器表示变换器）由Google开发，是仅编码器的预训练模型。
- en: Both are technically different, but, they have a similar objective — to perform
    natural language processing tasks. Many articles compare the two from a technical
    point of view. However, in this story, I would compare them based on the quality
    of their objective, which is natural language processing.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 两者在技术上有所不同，但它们有一个相似的目标——执行自然语言处理任务。许多文章从技术角度对这两者进行比较。然而，在这个故事中，我将根据它们的目标质量进行比较，也就是自然语言处理。
- en: Comparison approach
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 比较方法
- en: How to compare two completely different technical architectures? GPT is decoder-only
    architecture and BERT is encoder-only architecture. So a technical comparison
    of a decoder-only vs encoder-only architecture is like comparing Ferrari vs Lamborgini
    — both are great but with completely different technology under the chassis.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 如何比较两种完全不同的技术架构？GPT 是仅解码器架构，而 BERT 是仅编码器架构。因此，对仅解码器与仅编码器架构的技术比较就像是在比较法拉利与兰博基尼——两者都很出色，但底盘下的技术完全不同。
- en: However, we can make a comparison based on the quality of a common natural language
    task that both can do — which is…
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我们可以基于两者都能执行的共同自然语言任务的质量进行比较——那就是……
