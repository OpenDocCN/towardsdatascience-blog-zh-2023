- en: Understanding Noisy Data and Uncertainty in Machine Learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解机器学习中的噪声数据和不确定性
- en: 原文：[https://towardsdatascience.com/understanding-noisy-data-and-uncertainty-in-machine-learning-4a2995a84198](https://towardsdatascience.com/understanding-noisy-data-and-uncertainty-in-machine-learning-4a2995a84198)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/understanding-noisy-data-and-uncertainty-in-machine-learning-4a2995a84198](https://towardsdatascience.com/understanding-noisy-data-and-uncertainty-in-machine-learning-4a2995a84198)
- en: The actual reason your machine learning model isn’t working
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 你的机器学习模型无法正常工作的真正原因
- en: '[](https://harrisonfhoffman.medium.com/?source=post_page-----4a2995a84198--------------------------------)[![Harrison
    Hoffman](../Images/5eaa3e2bd0507297eb6c4a7efcf06324.png)](https://harrisonfhoffman.medium.com/?source=post_page-----4a2995a84198--------------------------------)[](https://towardsdatascience.com/?source=post_page-----4a2995a84198--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----4a2995a84198--------------------------------)
    [Harrison Hoffman](https://harrisonfhoffman.medium.com/?source=post_page-----4a2995a84198--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://harrisonfhoffman.medium.com/?source=post_page-----4a2995a84198--------------------------------)[![Harrison
    Hoffman](../Images/5eaa3e2bd0507297eb6c4a7efcf06324.png)](https://harrisonfhoffman.medium.com/?source=post_page-----4a2995a84198--------------------------------)[](https://towardsdatascience.com/?source=post_page-----4a2995a84198--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----4a2995a84198--------------------------------)
    [Harrison Hoffman](https://harrisonfhoffman.medium.com/?source=post_page-----4a2995a84198--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----4a2995a84198--------------------------------)
    ·9 min read·Jan 23, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----4a2995a84198--------------------------------)
    ·阅读时间：9分钟·2023年1月23日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: The fields of Artificial Intelligence and Machine Learning are hotter than ever.
    With models like Chat GPT and Stable Diffusion taking the world by storm, AI and
    ML hype has made a resurgence and is catching the attention of [the masses](https://www.google.com/search?tbm=vid&sxsrf=AJOqlzXI5TC9wmd1MTAKaqmrRY_q6b0k3Q%3A1674266462280&q=chatgpt&spell=1&sa=X&ved=2ahUKEwiz-_jNyNf8AhUZk2oFHY1kCZoQBSgAegQIERAB&biw=1185&bih=714&dpr=2).
    With all of this hype, it’s important to remind ourselves who the governor of
    machine learning success is — high-quality data.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 人工智能和机器学习领域比以往任何时候都更火热。随着像Chat GPT和Stable Diffusion这样的模型席卷全球，人工智能和机器学习的炒作重新回归并吸引了[大众](https://www.google.com/search?tbm=vid&sxsrf=AJOqlzXI5TC9wmd1MTAKaqmrRY_q6b0k3Q%3A1674266462280&q=chatgpt&spell=1&sa=X&ved=2ahUKEwiz-_jNyNf8AhUZk2oFHY1kCZoQBSgAegQIERAB&biw=1185&bih=714&dpr=2)。面对这些炒作，我们必须提醒自己，机器学习成功的关键在于高质量数据。
- en: In the absence of quality training data, supervised machine learning models
    offer no utility. Unfortunately, most real-world data science projects fail because
    unrealistic expectations about model performance are made before the quality of
    the data source is fully understood. This article will attempt to provide intuition
    about noisy data and why machine learning models fail to perform. We will explore
    the nature of supervised learning and deterministic functions, different types
    of model uncertainty, and discuss methods for mitigating this uncertainty and
    managing expectations.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在缺乏优质训练数据的情况下，监督机器学习模型没有任何用处。不幸的是，大多数现实世界的数据科学项目失败是因为在数据源质量完全了解之前对模型性能有不切实际的期望。本文将尝试提供对噪声数据的直观理解以及为什么机器学习模型无法有效工作的原因。我们将探讨监督学习和确定性函数的本质、不同类型的模型不确定性，并讨论减少这种不确定性和管理期望的方法。
- en: Supervised Learning Formulation
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 监督学习表述
- en: 'At its core, supervised machine learning is all about function approximation.
    We present a model with some inputs (X) and targets (y), and expect the model
    to approximate (or learn) a function that maps X to y by optimizing an objective
    function. More formally, the formulation for supervised learning looks something
    like this:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 从根本上讲，监督机器学习就是函数近似。我们向模型提供一些输入（X）和目标（y），并期望模型通过优化目标函数来近似（或学习）将X映射到y的函数。更正式地说，监督学习的表述大致如下：
- en: '![](../Images/c488fe462867d83ca2ff67e74b9a8381.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c488fe462867d83ca2ff67e74b9a8381.png)'
- en: A Brief Formulation of Supervised Learning. Image by Author.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 监督学习的简要表述。图像来源：作者。
- en: Given a data set (X, y), we assume the relationship between X and y is at least
    partially deterministic. The variance of the random error term, epsilon, plays
    a crucial role in the level of determinism of the function f(X). Higher variance
    in epsilon leads to an increasingly random and less predictable relationship between
    X and y. Contrary to how supervised learning is often taught, it’s important to
    note that epsilon can be (and often is) a function of X. This means the amount
    of noise and corresponding model uncertainty varies across different regions of
    X.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个数据集（X，y），我们假设X和y之间的关系至少部分是确定性的。随机误差项epsilon的方差在函数f(X)的确定性水平中发挥了至关重要的作用。epsilon的方差越高，X和y之间的关系就越随机，越不容易预测。与监督学习的常见教学方式相反，值得注意的是，epsilon可以（并且通常是）X的函数。这意味着噪声量和相应的模型不确定性在X的不同区域中有所不同。
- en: To better understand the role of determinism and random errors, we will delve
    into the nature of deterministic functions, examining their characteristics and
    implications in machine learning.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地理解确定性和随机误差的作用，我们将深入研究确定性函数的性质，检查它们的特征以及在机器学习中的影响。
- en: Deterministic Functions
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 确定性函数
- en: 'Deterministic functions have outputs that are fully specified by the inputs.
    This means, for every input in the domain, the function outputs exactly one unique
    quantity. These are the kinds of functions most of us have seen since our first
    course in Algebra:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 确定性函数的输出完全由输入指定。这意味着对于定义域中的每个输入，函数都输出一个唯一的量。这些是我们从第一门代数课程开始就见过的大多数函数：
- en: '![](../Images/2801785c49ee9683e10e204841b0895f.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2801785c49ee9683e10e204841b0895f.png)'
- en: Examples of Deterministic Functions. Image by Author.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 确定性函数示例。图像由作者提供。
- en: In each example above, every input passed into the function produces a single
    output. Note that this does not mean the functions are defined for every possible
    number. For instance, function (4) is only defined for x1 > 0 and when the sum
    of x1 and x2 is not zero.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述每个示例中，传递到函数中的每个输入都产生一个单一的输出。注意，这并不意味着函数对每个可能的数字都定义。例如，函数（4）仅在x1 > 0且x1和x2的和不为零时定义。
- en: 'Visually, in one dimension, deterministic functions pass the [“vertical line
    test”](https://en.wikipedia.org/wiki/Vertical_line_test) — this means if we draw
    a vertical line at any point in the domain, it will intersect the function at
    most one time:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 从视觉上看，在一维中，确定性函数通过[“垂直线测试”](https://en.wikipedia.org/wiki/Vertical_line_test)——这意味着如果我们在定义域中的任何点画一条垂直线，它最多会与函数相交一次：
- en: '![](../Images/2e0e38ffc1ed7cd16e06a6dd9092d625.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2e0e38ffc1ed7cd16e06a6dd9092d625.png)'
- en: Vertical Line Test for a Deterministic Function. Image by Author.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 确定性函数的垂直线测试。图像由作者提供。
- en: Contrast this with a purely non-deterministic function. These are functions
    that can have multiple outputs for a single input.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 与此相比，纯非确定性函数则不同。这些函数对于单一输入可能有多个输出。
- en: '![](../Images/9c6835c688379d5f29ab91280a068fde.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9c6835c688379d5f29ab91280a068fde.png)'
- en: Vertical Line Test for a Non-Deterministic Function. Image by Author.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 非确定性函数的垂直线测试。图像由作者提供。
- en: In the plot above, the vertical line through x = 4 intersects the curve at both
    -2 and 2\. If someone asks us what f(4) is, we have to say it is either -2 or
    2\. In the most extreme case, a non-deterministic function could output infinitely
    many values for a single input.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在上图中，垂直线x = 4穿过曲线的-2和2处。如果有人问我们f(4)是什么，我们必须说是-2或2。在最极端的情况下，非确定性函数可能会对单一输入输出无限多个值。
- en: 'As simple as the above non-deterministic function is to visualize and quantify,
    a machine learning model would never be able to learn it. To see this, suppose
    we sample the function as follows:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管上述非确定性函数在可视化和量化上很简单，但机器学习模型永远无法学习它。为了解释这一点，假设我们按如下方式对函数进行采样：
- en: '![](../Images/fb15a376553ec351d57a50960b198a7f.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fb15a376553ec351d57a50960b198a7f.png)'
- en: Samples from a Non-Deterministic Function. Image by Author.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 非确定性函数的样本。图像由作者提供。
- en: 'We’ll use the above samples as the training set and fit a decision tree regressor:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用上述样本作为训练集，并拟合一个决策树回归器：
- en: '[PRE0]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'We can visualize how well the model performed on a test set:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以可视化模型在测试集上的表现：
- en: '![](../Images/6ec9ac9ac422b43c3aeddf910f5f5095.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6ec9ac9ac422b43c3aeddf910f5f5095.png)'
- en: Decision Tree Regressor Predictions Overlayed on a Test Set. Image by Author.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树回归器在测试集上的预测叠加。图像由作者提供。
- en: The decision tree is essentially guessing on the test set. Because each non-zero
    input in the domain maps to two different values, a machine learning model has
    no way of knowing which value to predict on a test set. Noise in the training
    data due to the nature of the underlying function will cause any machine learning
    model to make predictions with high uncertainty.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树在测试集上本质上是在猜测。由于领域中的每个非零输入映射到两个不同的值，机器学习模型无法知道在测试集上预测哪个值。由于基础函数的性质，训练数据中的噪声会导致任何机器学习模型做出具有高不确定性的预测。
- en: Sources of Uncertainty
  id: totrans-35
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 不确定性来源
- en: Now that we’ve seen the difference between deterministic and non-deterministic/random
    functions, let’s explore the types of uncertainty that can arise from trying to
    learn these functions.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了确定性和非确定性/随机函数之间的区别，让我们探讨从学习这些函数中可能产生的不确定性类型。
- en: Data (Aleatoric) Uncertainty
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据（随机）不确定性
- en: 'Data uncertainty, also known as aleatoric uncertainty, comes from the inherent
    complexity of the observed data set. In a classification setting, this often presents
    itself as overlapping classes. For instance, the following scatter depicts overlapping
    classes in two dimensions:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 数据的不确定性，也称为随机不确定性，源于观察数据集的固有复杂性。在分类设置中，这通常表现为重叠的类别。例如，以下散点图描绘了二维空间中重叠的类别：
- en: '![](../Images/67c3ccbe9285ac51d458ee174900e9b5.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/67c3ccbe9285ac51d458ee174900e9b5.png)'
- en: Classification Data with Overlapping Classes. Image by Author.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 带有重叠类别的分类数据。图片来源：作者。
- en: For most observations in a neighborhood of (x1, x2), there’s no obvious way
    to distinguish between the two classes. And while it is true that a sufficiently
    complex classifier could perfectly classify the examples in the data set above,
    that same classifier would likely not perform better than random guessing on a
    test set.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 (x1, x2) 邻域中的大多数观测值，没有明显的方法来区分两个类别。虽然确实存在一个足够复杂的分类器可以完美分类上述数据集中的例子，但同样的分类器在测试集上的表现可能不会比随机猜测更好。
- en: 'In a regression setting, data uncertainty often results from additive noise
    in the underlying data. Data uncertainty would arise from a data set that looks
    something like this:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在回归设置中，数据不确定性通常源于基础数据中的附加噪声。数据不确定性会出现在类似于以下数据集的情况中：
- en: '![](../Images/f3a9b60b6399131377a1f910dcde76c2.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f3a9b60b6399131377a1f910dcde76c2.png)'
- en: Noisy Regression Data. Image by Author.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 噪声回归数据。图片来源：作者。
- en: In this case, x is the only feature considered, and for any neighborhood of
    x, y takes on many possible values. For example, if we try to predict the value
    of y when x = 0 based on the data, no obvious answer stands out. In general, the
    higher the variance of the additive noise distribution, the more data uncertainty
    there will be.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，x 是唯一考虑的特征，对于 x 的任何邻域，y 都可能取很多不同的值。例如，如果我们基于数据来预测 x = 0 时 y 的值，没有明显的答案显现出来。一般来说，附加噪声分布的方差越大，数据不确定性就会越高。
- en: '![](../Images/22db0372fa8d3363f9eb6ab52683623a.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/22db0372fa8d3363f9eb6ab52683623a.png)'
- en: Samples from the Quadratic Function with Additive Noise. Image by Author.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 带有附加噪声的二次函数样本。图片来源：作者。
- en: 'As discussed earlier, it’s common to see noise levels and data uncertainty
    vary across regions of the domain. Data uncertainty is often higher towards the
    edges of the domain, as illustrated below:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，噪声水平和数据不确定性在领域的不同区域中常常有所不同。数据不确定性通常在领域的边缘区域较高，如下所示：
- en: '![](../Images/063963fff883b01411d5d63582d847d0.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/063963fff883b01411d5d63582d847d0.png)'
- en: Noise and Data Uncertainty Varying by x. The Variance of the Additive Noise
    Term is a Function of x. Image by Author.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 噪声和数据不确定性随 x 变化。附加噪声项的方差是 x 的函数。图片来源：作者。
- en: 'Data uncertainty usually manifests when critical features are left out of the
    model. To illustrate this, consider the following data set where we want to use
    x1 to predict y:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 数据的不确定性通常表现为模型中遗漏了关键特征。为了说明这一点，考虑以下数据集，我们希望使用 x1 来预测 y：
- en: '![](../Images/7dd902d27f046077e201211b4fc781c0.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7dd902d27f046077e201211b4fc781c0.png)'
- en: Noisy Data that Looks Like an Angry Frog. Image by Author.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 像愤怒的青蛙一样的噪声数据。图片来源：作者。
- en: 'If only x1 is used as a feature to predict y, no machine learning model would
    stand a chance of performing well. For instance, when x1 = -0.49, y takes on 14
    different values in this data set. In the absence of other features, predicting
    y looks hopeless. However, the above data set is actually a sample from the following
    function:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 如果仅使用 x1 作为特征来预测 y，没有任何机器学习模型能够表现良好。例如，当 x1 = -0.49 时，在这个数据集中 y 取14个不同的值。在没有其他特征的情况下，预测
    y 看起来无望。然而，上述数据集实际上是以下函数的一个样本：
- en: '![](../Images/6c249e5c5832ebc14302de9dbdb45727.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6c249e5c5832ebc14302de9dbdb45727.png)'
- en: An Equation. Image by Author.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 一个方程。图像由作者提供。
- en: 'If we take samples from x2, and visualize x1, x2, and y in three dimensions,
    a clear surface appears:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们从 x2 取样，并在三维中可视化 x1、x2 和 y，则会出现一个清晰的表面：
- en: '![](../Images/324d0e4aa4678f9e7eabd0437a28df66.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/324d0e4aa4678f9e7eabd0437a28df66.png)'
- en: A 3D Surface. Image by Author.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 一个三维表面。图像由作者提供。
- en: Interactive Surface. Image by Author.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 互动表面。图像由作者提供。
- en: The relationship between x1 and y is noisy and would result in high data uncertainty
    by any model that attempted to learn it. However, the relationship between (x1,
    x2) and y has no noise because it is fully determined by a closed-form equation.
    Given enough training data, a sufficiently non-linear model could learn this relationship
    with no data uncertainty.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: x1 和 y 之间的关系噪声较大，任何试图学习这种关系的模型都会导致高数据不确定性。然而，(x1, x2) 和 y 之间的关系没有噪声，因为它完全由一个封闭形式的方程决定。只要有足够的训练数据，足够非线性的模型可以学习这种关系，从而没有数据不确定性。
- en: Many financial applications encounter data uncertainty because many unobserved
    human behaviors affect markets. For instance, countless factors influence whether
    an individual will default on a loan. Even if two loan recipients have the same
    credit profile and look identical in feature space (most financial institutions
    have a large number of features to work with), there’s still a possibility that
    recipient A defaults because of a life emergency while recipient B doesn’t. Unless
    the default model is exposed to data that predicts life emergencies, there will
    always be an element of data uncertainty in these scenarios.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 许多金融应用面临数据不确定性，因为许多未观察到的人类行为会影响市场。例如，无数因素影响一个人是否会违约。即使两个贷款接收者有相同的信用档案并在特征空间中看起来相同（大多数金融机构有大量的特征），仍然有可能因为生活紧急情况而导致接收者A违约，而接收者B没有违约。除非违约模型接触到预测生活紧急情况的数据，否则这些情况下总会存在数据不确定性。
- en: Knowledge (Epistemic) Uncertainty
  id: totrans-63
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 知识（认知）不确定性
- en: 'Knowledge (epistemic) uncertainty results from data that is sparsely sampled
    in certain regions of the domain. Consider the following data set where we would
    like to model the relationship between x and y:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 知识（认知）不确定性来自于在领域的某些区域稀疏采样的数据。考虑以下数据集，我们希望建模 x 和 y 之间的关系：
- en: '![](../Images/3db41a53ee853f33ffceb2a53ed15660.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3db41a53ee853f33ffceb2a53ed15660.png)'
- en: High Knowledge Uncertainty. Image By Author.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 高知识不确定性。图像由作者提供。
- en: 'This data set presents no information about y when x takes on values between
    1 and 3 or between 4 and 7\. A machine learning model would have to make a blind
    guess when predicting the value of y in those regions. Fortunately, unlike data
    uncertainty, knowledge uncertainty can be reduced by sampling regions of missing
    data. If we fully sample the domain, the relationship between x and y becomes
    clear:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 当 x 在 1 到 3 之间或在 4 到 7 之间取值时，这个数据集没有关于 y 的信息。机器学习模型在预测这些区域的 y 值时不得不做出盲目猜测。幸运的是，与数据不确定性不同，知识不确定性可以通过采样缺失数据的区域来减少。如果我们对领域进行完全采样，x
    和 y 之间的关系将变得清晰：
- en: '![](../Images/76e7e04d7252283f971ae9776c9acaad.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/76e7e04d7252283f971ae9776c9acaad.png)'
- en: Knowledge Uncertainty Eliminated by Sampling. Image by Author.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 通过采样消除知识不确定性。图像由作者提供。
- en: Knowledge uncertainty is common in practice because most data sets are not sampled
    from uniform distributions, naturally resulting in sparse regions of the domain.
    In general, the more uniformly sampled the domain is, the less knowledge uncertainty
    there will be in the modeling process.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 知识不确定性在实际应用中很常见，因为大多数数据集不是从均匀分布中采样的，导致领域中的稀疏区域。一般来说，领域采样越均匀，建模过程中的知识不确定性就会越小。
- en: Methods for Handling Noisy Data and Uncertainty
  id: totrans-71
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 处理噪声数据和不确定性的方法
- en: Now that we’ve gained some intuition about the nature of noisy data and uncertainty,
    let's explore some practical actions that can be taken the combat the issue.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们对噪声数据和不确定性的性质有了一些直觉，让我们探讨一些可以采取的实际措施来应对这个问题。
- en: 1\. Stop Trying to Find Better Models
  id: totrans-73
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1\. 停止尝试寻找更好的模型
- en: Failing fast and knowing when to move on are invaluable data science skills.
    We live in a time where state-of-the-art, highly performant models exist for almost
    any machine learning problem. Any data scientist stuck in the model selection
    or hyperparameter tuning process with suboptimal results should look elsewhere
    for performance gains. In other words, the model probably doesn’t need to be improved,
    the data does.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 快速失败并知道何时转移是无价的数据科学技能。我们生活在一个几乎对任何机器学习问题都有最先进、高性能模型的时代。任何在模型选择或超参数调整过程中遇到次优结果的数据科学家应该寻求其他地方的性能提升。换句话说，模型可能并不需要改进，数据才需要。
- en: '**2\. Get More Data**'
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**2\. 获取更多数据**'
- en: As we saw earlier, poor-performing models are usually the result of incomplete
    data. Even a feature that appears unimportant in isolation can add significant
    predictive power when accompanied by more features. Because of this, data scientists
    must ensure they have a complete understanding of the problem and the data sources
    needed for successful modeling. Acquiring the right data and engineering good
    features is arguably the most difficult yet critical part of a data scientist’s
    job. A data scientist should be able to work with stakeholders on balancing the
    costs and benefits of getting more data, since this is ultimately where the most
    business value will come from.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前所见，表现不佳的模型通常是由于数据不完整。即使是一个看似不重要的特征，在与更多特征一起使用时也可以提供显著的预测能力。因此，数据科学家必须确保他们对问题及成功建模所需的数据源有全面的理解。获取正确的数据和工程化良好的特征可以说是数据科学家工作中最困难但最关键的部分。数据科学家应该能够与利益相关者一起权衡获取更多数据的成本和收益，因为这最终是带来最大商业价值的地方。
- en: 3\. Quantify Uncertainty in Model Predictions
  id: totrans-77
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3\. 量化模型预测的不确定性
- en: For high stakes machine learning problems where noisy data is unavoidable, it’s
    often valuable for a model to quantify the uncertainty in a prediction. Calibrated
    classifiers provide uncertainty estimates in their predicted probabilities that
    can be more useful than the predicted class. In regression, methods like [quantile
    regression](https://en.wikipedia.org/wiki/Quantile_regression), [conformal prediction](https://en.wikipedia.org/wiki/Conformal_prediction)
    , [bayesian neural networks](/bayesian-deep-learning-estimating-uncertainty-9907f5208cc0),
    and [natural gradient boosting](https://arxiv.org/pdf/1910.03225v1.pdf) can be
    leveraged to generate uncertainty estimates. In short, having a model capable
    of saying “I don’t know” when making a prediction can be incredibly valuable.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 对于那些噪声数据不可避免的高风险机器学习问题，模型量化预测的不确定性通常是有价值的。经过校准的分类器提供的预测概率的不确定性估计比预测类别可能更有用。在回归中，像[分位数回归](https://en.wikipedia.org/wiki/Quantile_regression)、[符合预测](https://en.wikipedia.org/wiki/Conformal_prediction)、[贝叶斯神经网络](/bayesian-deep-learning-estimating-uncertainty-9907f5208cc0)和[自然梯度提升](https://arxiv.org/pdf/1910.03225v1.pdf)等方法可以用于生成不确定性估计。简而言之，拥有一个在做出预测时能够说“我不知道”的模型可以非常有价值。
- en: 4\. Manage Expectations
  id: totrans-79
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4\. 管理期望
- en: As a data scientist, it is crucial to remember that machine learning is a tool
    to achieve a specific goal and should be used with a clear understanding of the
    underlying business problem. Having a comprehensive understanding of the problem,
    and the role of machine learning in solving it, allows for more accurate expectations
    to be set for the performance of the model, ultimately leading to more effective
    and efficient use of the tool. This will help in developing a machine learning
    model that is tailored to the specific needs of the business, delivering the best
    results. Machine learning is a means to an end. Having a perfect model isn’t usually
    required to solve a problem, but correctly utilizing imperfect models is.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 作为数据科学家，重要的是要记住机器学习是实现特定目标的工具，应该在清楚理解基本业务问题的情况下使用。全面理解问题以及机器学习在解决问题中的角色，可以更准确地设定模型性能的期望，从而最终更有效地使用这一工具。这将有助于开发出一个针对业务特定需求的机器学习模型，提供最佳结果。机器学习是一种手段。通常情况下，不需要一个完美的模型来解决问题，但正确利用不完美的模型则是必要的。
- en: '*Become a Member:* [*https://harrisonfhoffman.medium.com/membership*](https://harrisonfhoffman.medium.com/membership)'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '*成为会员：* [*https://harrisonfhoffman.medium.com/membership*](https://harrisonfhoffman.medium.com/membership)'
- en: '*Like my articles? Buy me a coffee:* [*https://www.buymeacoffee.com/HarrisonfhU*](https://www.buymeacoffee.com/HarrisonfhU)'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '*喜欢我的文章？请给我买杯咖啡：* [*https://www.buymeacoffee.com/HarrisonfhU*](https://www.buymeacoffee.com/HarrisonfhU)'
- en: References
  id: totrans-83
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: '*Estimating Uncertainty with Catboost —* [*https://catboost.ai/en/docs/references/uncertainty*](https://catboost.ai/en/docs/references/uncertainty)'
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*使用 Catboost 估计不确定性 —* [*https://catboost.ai/en/docs/references/uncertainty*](https://catboost.ai/en/docs/references/uncertainty)'
