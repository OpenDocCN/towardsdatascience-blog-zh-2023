- en: Top 10 Pre-Trained Models for Image Embedding every Data Scientist Should Know
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 每个数据科学家都应该了解的前10大预训练模型
- en: 原文：[https://towardsdatascience.com/top-10-pre-trained-models-for-image-embedding-every-data-scientist-should-know-88da0ef541cd](https://towardsdatascience.com/top-10-pre-trained-models-for-image-embedding-every-data-scientist-should-know-88da0ef541cd)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/top-10-pre-trained-models-for-image-embedding-every-data-scientist-should-know-88da0ef541cd](https://towardsdatascience.com/top-10-pre-trained-models-for-image-embedding-every-data-scientist-should-know-88da0ef541cd)
- en: Essential guide to transfer learning
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 转移学习的基础指南
- en: '[](https://satyam-kumar.medium.com/?source=post_page-----88da0ef541cd--------------------------------)[![Satyam
    Kumar](../Images/2360baa87ea7a20f41589c5f8d783288.png)](https://satyam-kumar.medium.com/?source=post_page-----88da0ef541cd--------------------------------)[](https://towardsdatascience.com/?source=post_page-----88da0ef541cd--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----88da0ef541cd--------------------------------)
    [Satyam Kumar](https://satyam-kumar.medium.com/?source=post_page-----88da0ef541cd--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://satyam-kumar.medium.com/?source=post_page-----88da0ef541cd--------------------------------)[![Satyam
    Kumar](../Images/2360baa87ea7a20f41589c5f8d783288.png)](https://satyam-kumar.medium.com/?source=post_page-----88da0ef541cd--------------------------------)[](https://towardsdatascience.com/?source=post_page-----88da0ef541cd--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----88da0ef541cd--------------------------------)
    [Satyam Kumar](https://satyam-kumar.medium.com/?source=post_page-----88da0ef541cd--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----88da0ef541cd--------------------------------)
    ·9 min read·Apr 19, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----88da0ef541cd--------------------------------)
    ·9分钟阅读·2023年4月19日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/bcf5c3734418b63acaacc768035b72e8.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bcf5c3734418b63acaacc768035b72e8.png)'
- en: Image by [Chen](https://pixabay.com/users/chenspec-7784448/?utm_source=link-attribution&amp%3Butm_medium=referral&amp%3Butm_campaign=image&amp%3Butm_content=5692896)
    from [Pixabay](https://pixabay.com//?utm_source=link-attribution&amp%3Butm_medium=referral&amp%3Butm_campaign=image&amp%3Butm_content=5692896)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图像由 [Chen](https://pixabay.com/users/chenspec-7784448/?utm_source=link-attribution&amp%3Butm_medium=referral&amp%3Butm_campaign=image&amp%3Butm_content=5692896)
    提供，来自 [Pixabay](https://pixabay.com//?utm_source=link-attribution&amp%3Butm_medium=referral&amp%3Butm_campaign=image&amp%3Butm_content=5692896)
- en: The rapid developments in Computer Vision — image classification use cases have
    been further accelerated by the advent of transfer learning. It takes a lot of
    computational resources and time to train a computer vision neural network model
    on a large dataset of images.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 计算机视觉的快速发展——图像分类用例已因转移学习的出现而进一步加速。在大规模图像数据集上训练计算机视觉神经网络模型需要大量计算资源和时间。
- en: Luckily, this time and resources can be shortened by using pre-trained models.
    The technique of leveraging feature representation from a pre-trained model is
    called transfer learning. The pre-trained are generally trained using high-end
    computational resources and on massive datasets.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，通过使用预训练模型，可以缩短时间和资源。利用预训练模型的特征表示的技术被称为转移学习。预训练模型通常使用高端计算资源和大规模数据集进行训练。
- en: 'The pre-trained models can be used in various ways:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 预训练模型可以以多种方式使用：
- en: Using the pre-trained weights and directly making predictions on the test data
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用预训练权重并直接对测试数据进行预测
- en: Using the pre-trained weights for initialization and training the model using
    the custom dataset
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用预训练的权重进行初始化，并使用自定义数据集训练模型
- en: Using only the architecture of the pre-trained network, and training it from
    scratch on the custom dataset
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 仅使用预训练网络的架构，并在自定义数据集上从头开始训练
- en: This article walks through the top 10 state-of-the-art pre-trained models to
    get image embedding. All these pre-trained models can be loaded as keras models
    using the [keras.application](https://keras.io/api/applications/) API.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 本文介绍了获取图像嵌入的十大最先进的预训练模型。所有这些预训练模型都可以通过 [keras.application](https://keras.io/api/applications/)
    API 以 Keras 模型的形式加载。
- en: '[PRE0]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '**License:** All the images used in this article are from [paperwithcode.com](https://paperswithcode.com/)
    which is licensed under CC BY-SA 4.0.'
  id: totrans-16
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**许可协议：** 本文中使用的所有图像均来自 [paperwithcode.com](https://paperswithcode.com/)，其许可协议为
    CC BY-SA 4.0。'
- en: '1) VGG:'
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '1) VGG:'
- en: The VGG-16/19 networks were introduced at the ILSVRC 2014 conference since it
    is one of the most popular pre-trained models. It was developed by the Visual
    Graphics Group at the University of Oxford.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: VGG-16/19网络在ILSVRC 2014大会上推出，因为它是最受欢迎的预训练模型之一。它由牛津大学的视觉图形组开发。
- en: 'There are two variations of the VGG model: 16 and 19 layers network, VGG-19
    (19-layer network) being an improvement of the VGG-16 (16-layer network) model.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: VGG模型有两种变体：16层和19层网络，VGG-19（19层网络）是VGG-16（16层网络）模型的改进。
- en: '**Architecture:**'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '**架构：**'
- en: '![](../Images/bf14e8e28334a24a8ae22c55a4e1de6d.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bf14e8e28334a24a8ae22c55a4e1de6d.png)'
- en: ([Source](https://paperswithcode.com/method/vgg), Free-to-use license under
    [CC BY-SA 4.0](https://creativecommons.org/licenses/by-sa/4.0/)), VGG-16 Network
    architecture
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: ([来源](https://paperswithcode.com/method/vgg)，使用[CC BY-SA 4.0](https://creativecommons.org/licenses/by-sa/4.0/)的免费许可证)，VGG-16网络架构
- en: The VGG network is simple and sequential in nature and uses a lot of filters.
    At each stage, small (3*3) filters are used to reduce the number of parameters.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: VGG网络结构简单且顺序，并且使用了大量的滤波器。在每个阶段，使用小的（3*3）滤波器来减少参数的数量。
- en: 'The VGG-16 network has the following:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: VGG-16网络具有以下特点：
- en: Convolutional Layers = 13
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 卷积层 = 13
- en: Pooling Layers = 5
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 池化层 = 5
- en: Fully Connected Dense Layers = 3
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 全连接密集层 = 3
- en: '**Input:** Image of dimensions (224, 224, 3)'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '**输入：** 维度为（224, 224, 3）的图像'
- en: '**Output:** Image embedding of 1000-dimension'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '**输出：** 1000维的图像嵌入'
- en: '**Other Details for VGG-16/19:**'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '**VGG-16/19的其他细节：**'
- en: 'Paper Link: [https://arxiv.org/pdf/1409.1556.pdf](https://arxiv.org/pdf/1409.1556.pdf)'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 论文链接：[https://arxiv.org/pdf/1409.1556.pdf](https://arxiv.org/pdf/1409.1556.pdf)
- en: 'GitHub: [VGG](https://paperswithcode.com/paper/very-deep-convolutional-networks-for-large#code)'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'GitHub: [VGG](https://paperswithcode.com/paper/very-deep-convolutional-networks-for-large#code)'
- en: 'Published On: April 2015'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 发布时间：2015年4月
- en: 'Performance on ImageNet Dataset: 71% (Top 1 Accuracy), 90% (Top 5 Accuracy)'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在ImageNet数据集上的性能：71%（Top 1准确率），90%（Top 5准确率）
- en: 'Number of Parameters: ~140M'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 参数数量：约140M
- en: 'Number of Layers: 16/19'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 层数：16/19
- en: 'Size on Disk: ~530MB'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 磁盘大小：约530MB
- en: '**Implementation:**'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '**实现：**'
- en: Call `[tf.keras.applications.vgg16.preprocess_input](https://www.tensorflow.org/api_docs/python/tf/keras/applications/vgg16/preprocess_input)`
    on your input data to convert input images to BGR with zero-center for each color
    channel.
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在您的输入数据上调用 `[tf.keras.applications.vgg16.preprocess_input](https://www.tensorflow.org/api_docs/python/tf/keras/applications/vgg16/preprocess_input)`
    将输入图像转换为BGR，并对每个颜色通道进行零中心化。
- en: 'Instantiate the VGG16 model using the below-mentioned code:'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用以下代码实例化VGG16模型：
- en: '[PRE1]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The above-mentioned code is for VGG-16 implementation, keras offers a similar
    API for VGG-19 implementation, for more details refer to [this documentation](https://keras.io/api/applications/vgg/#vgg16-function).
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码用于VGG-16的实现，keras提供了类似的API用于VGG-19的实现，更多细节请参阅[此文档](https://keras.io/api/applications/vgg/#vgg16-function)。
- en: '2) Xception:'
  id: totrans-43
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 2) Xception：
- en: Xception is a deep CNN architecture that involves depthwise separable convolutions.
    A depthwise separable convolution can be understood as an Inception model with
    a maximally large number of towers.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: Xception是一个深度CNN架构，涉及深度可分离卷积。深度可分离卷积可以理解为具有最大数量塔的Inception模型。
- en: '**Architecture:**'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '**架构：**'
- en: '![](../Images/cca16ef8eae292510d58bc3c94f14bb5.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/cca16ef8eae292510d58bc3c94f14bb5.png)'
- en: ([Source](https://paperswithcode.com/paper/xception-deep-learning-with-depthwise),
    Free-to-use license under [CC BY-SA 4.0](https://creativecommons.org/licenses/by-sa/4.0/)),
    Xception architecture
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: ([来源](https://paperswithcode.com/paper/xception-deep-learning-with-depthwise)，使用[CC
    BY-SA 4.0](https://creativecommons.org/licenses/by-sa/4.0/)的免费许可证)，Xception架构
- en: '**Input:** Image of dimensions (299, 299, 3)'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '**输入：** 维度为（299, 299, 3）的图像'
- en: '**Output:** Image embedding of 1000-dimension'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '**输出：** 1000维的图像嵌入'
- en: '**Other Details for Xception:**'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '**Xception的其他细节：**'
- en: 'Paper Link: [https://arxiv.org/pdf/1409.1556.pdf](https://arxiv.org/pdf/1610.02357.pdf)'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 论文链接：[https://arxiv.org/pdf/1409.1556.pdf](https://arxiv.org/pdf/1610.02357.pdf)
- en: 'GitHub: [Xception](https://paperswithcode.com/paper/xception-deep-learning-with-depthwise#code)'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'GitHub: [Xception](https://paperswithcode.com/paper/xception-deep-learning-with-depthwise#code)'
- en: 'Published On: April 2017'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 发布时间：2017年4月
- en: 'Performance on ImageNet Dataset: 79% (Top 1 Accuracy), 94.5% (Top 5 Accuracy)'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在ImageNet数据集上的性能：79%（Top 1准确率），94.5%（Top 5准确率）
- en: 'Number of Parameters: ~30M'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 参数数量：约30M
- en: 'Depth: 81'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深度：81
- en: 'Size on Disk: 88MB'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 磁盘大小：88MB
- en: '**Implementation:**'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '**实现：**'
- en: 'Instantiate the Xception model using the below-mentioned code:'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用以下代码实例化Xception模型：
- en: '[PRE2]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The above-mentioned code is for Xception implementation, for more details refer
    to [this documentation](https://keras.io/api/applications/xception/).
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码用于Xception实现，更多详细信息请参考[该文档](https://keras.io/api/applications/xception/)。
- en: '3) ResNet:'
  id: totrans-62
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '3) ResNet:'
- en: The previous CNN architectures were not designed to scale to many convolutional
    layers. It resulted in a vanishing gradient problem and limited performance upon
    adding new layers to the existing architecture.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的CNN架构未设计为扩展到许多卷积层。这导致了梯度消失问题，并且在向现有架构添加新层时性能受限。
- en: ResNets architecture offers to skip connections to solve the vanishing gradient
    problem.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: ResNet架构提供跳跃连接以解决梯度消失问题。
- en: '**Architecture:**'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '**架构：**'
- en: '![](../Images/ffc35dcf643069854d00c4f511e8ab1a.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ffc35dcf643069854d00c4f511e8ab1a.png)'
- en: ([Source](https://production-media.paperswithcode.com/methods/Screen_Shot_2020-09-25_at_10.26.40_AM_SAB79fQ.png),
    Free-to-use license under [CC BY-SA 4.0](https://creativecommons.org/licenses/by-sa/4.0/)),
    ResNet architecture
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: ([来源](https://production-media.paperswithcode.com/methods/Screen_Shot_2020-09-25_at_10.26.40_AM_SAB79fQ.png)，使用[CC
    BY-SA 4.0](https://creativecommons.org/licenses/by-sa/4.0/)的自由使用许可)，ResNet架构
- en: This ResNet model uses a 34-layer network architecture inspired by the VGG-19
    model to which the shortcut connections are added. These shortcut connections
    then convert the architecture into a residual network.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 这个ResNet模型使用了一个34层的网络架构，灵感来源于VGG-19模型，并在其上添加了捷径连接。这些捷径连接将架构转换为残差网络。
- en: 'There are several versions of ResNet architecture:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: ResNet架构有多个版本：
- en: ResNet50
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ResNet50
- en: ResNet50V2
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ResNet50V2
- en: ResNet101
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ResNet101
- en: ResNet101V2
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ResNet101V2
- en: ResNet152
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ResNet152
- en: ResNet152V2
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ResNet152V2
- en: '**Input:** Image of dimensions (224, 224, 3)'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '**输入：** 尺寸为(224, 224, 3)的图像'
- en: '**Output:** Image embedding of 1000-dimension'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '**输出：** 1000维的图像嵌入'
- en: '**Other Details for ResNet models:**'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '**ResNet模型的其他详细信息：**'
- en: 'Paper Link: [https://arxiv.org/pdf/1512.03385.pdf](https://arxiv.org/pdf/1512.03385.pdf)'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '论文链接: [https://arxiv.org/pdf/1512.03385.pdf](https://arxiv.org/pdf/1512.03385.pdf)'
- en: 'GitHub: [ResNet](https://paperswithcode.com/paper/deep-residual-learning-for-image-recognition#code)'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'GitHub: [ResNet](https://paperswithcode.com/paper/deep-residual-learning-for-image-recognition#code)'
- en: 'Published On: Dec 2015'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '发布日期: 2015年12月'
- en: 'Performance on ImageNet Dataset: 75–78% (Top 1 Accuracy), 92–93% (Top 5 Accuracy)'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '在ImageNet数据集上的性能: 75–78%（Top 1准确率），92–93%（Top 5准确率）'
- en: 'Number of Parameters: 25–60M'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '参数数量: 25–60M'
- en: 'Depth: 107–307'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '深度: 107–307'
- en: 'Size on Disk: ~100–230MB'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '磁盘大小: ~100–230MB'
- en: '**Implementation:**'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '**实现：**'
- en: 'Instantiate the ResNet50 model using the below-mentioned code:'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用下面的代码实例化ResNet50模型：
- en: '[PRE3]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The above-mentioned code is for ResNet50 implementation, keras offers a similar
    API to other ResNet architecture implementations, for more details refer to [this
    documentation](https://keras.io/api/applications/resnet/#resnet101-function).
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码用于ResNet50实现，keras提供了类似的API用于其他ResNet架构实现，更多详细信息请参考[该文档](https://keras.io/api/applications/resnet/#resnet101-function)。
- en: '4) Inception:'
  id: totrans-90
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '4) Inception:'
- en: 'Multiple deep layers of convolutions resulted in the overfitting of the data.
    To avoid overfitting, the inception model uses parallel layers or multiple filters
    of different sizes on the same level, to make the model wider rather than making
    it deeper. The Inception V1 model is made of 4 parallel layers with: (1*1), (3*3),
    (5*5) convolutions, and (3*3) max pooling.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 多层深度卷积导致了数据的过拟合。为避免过拟合，Inception模型使用了并行层或在同一层级上使用不同大小的多个滤波器，从而使模型变宽，而不是变深。Inception
    V1模型由4个并行层组成，分别为：(1*1)、(3*3)、(5*5)卷积和(3*3)最大池化。
- en: Inception (V1/V2/V3) is deep learning model-based CNN network developed by a
    team at Google. InceptionV3 is an advanced and optimized version of the InceptionV1
    and V2 models.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: Inception (V1/V2/V3) 是由谷歌团队开发的基于深度学习的CNN网络。InceptionV3是InceptionV1和V2模型的高级和优化版本。
- en: '**Architecture:**'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '**架构：**'
- en: 'The InceptionV3 model is made up of 42 layers. The architecture of InceptionV3
    is progressively step-by-step built as:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: InceptionV3模型由42层组成。InceptionV3的架构逐步构建如下：
- en: Factorized Convolutions
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 因式分解卷积
- en: Smaller Convolutions
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更小的卷积
- en: Asymmetric Convolutions
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 非对称卷积
- en: Auxilliary Convolutions
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 辅助卷积
- en: Grid Size Reduction
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 网格尺寸缩减
- en: 'All these concepts are consolidated into the final architecture mentioned below:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些概念都整合到了下面提到的最终架构中：
- en: '![](../Images/6c6221bb9bc94860e18662fde362d19d.png)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6c6221bb9bc94860e18662fde362d19d.png)'
- en: ([Source](https://paperswithcode.com/method/inception-v3), Free-to-use license
    under [CC BY-SA 4.0](https://creativecommons.org/licenses/by-sa/4.0/)), InceptionV3
    architecture
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: ([来源](https://paperswithcode.com/method/inception-v3)，使用 [CC BY-SA 4.0](https://creativecommons.org/licenses/by-sa/4.0/)
    许可证的免费使用)，InceptionV3 架构
- en: '**Input:** Image of dimensions (299, 299, 3)'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '**输入：** 尺寸为 (299, 299, 3) 的图像'
- en: '**Output:** Image embedding of 1000-dimension'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '**输出：** 1000维图像嵌入'
- en: '**Other Details for InceptionV3 models:**'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '**InceptionV3 模型的其他细节：**'
- en: 'Paper Link: [https://arxiv.org/pdf/1512.00567.pdf](https://arxiv.org/pdf/1512.00567.pdf)'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 论文链接： [https://arxiv.org/pdf/1512.00567.pdf](https://arxiv.org/pdf/1512.00567.pdf)
- en: 'GitHub: [InceptionV3](https://paperswithcode.com/paper/what-do-deep-networks-like-to-see#code)'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'GitHub: [InceptionV3](https://paperswithcode.com/paper/what-do-deep-networks-like-to-see#code)'
- en: 'Published On: Dec 2015'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 发布日期：2015年12月
- en: 'Performance on ImageNet Dataset: 78% (Top 1 Accuracy), 94% (Top 5 Accuracy)'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 ImageNet 数据集上的表现：78%（Top 1 精度），94%（Top 5 精度）
- en: 'Number of Parameters: 24M'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 参数数量：24M
- en: 'Depth: 189'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深度：189
- en: 'Size on Disk: 92MB'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 磁盘大小：92MB
- en: '**Implementation:**'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '**实现：**'
- en: 'Instantiate the InceptionV3 model using the below-mentioned code:'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用以下代码实例化 InceptionV3 模型：
- en: '[PRE4]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The above-mentioned code is for InceptionV3 implementation, for more details
    refer to [this documentation](https://keras.io/api/applications/inceptionv3/).
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码是用于 InceptionV3 实现的，更多细节请参考 [这份文档](https://keras.io/api/applications/inceptionv3/)。
- en: '5) InceptionResNet:'
  id: totrans-117
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '5) InceptionResNet:'
- en: InceptionResNet-v2 is a CNN model developed by researchers at Google. The target
    of this model was to reduce the complexity of InceptionV3 and explore the possibility
    of using residual networks on the Inception model.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: InceptionResNet-v2 是由 Google 的研究人员开发的 CNN 模型。该模型的目标是减少 InceptionV3 的复杂性，并探索在
    Inception 模型中使用残差网络的可能性。
- en: '**Architecture:**'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '**架构：**'
- en: '![](../Images/a49ccb18b6aae5e423a69f62a3f40e91.png)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a49ccb18b6aae5e423a69f62a3f40e91.png)'
- en: ([Source](https://paperswithcode.com/method/inception-resnet-v2), Free-to-use
    license under [CC BY-SA 4.0](https://creativecommons.org/licenses/by-sa/4.0/)),
    Inception-ResNet-V2 architecture
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: ([来源](https://paperswithcode.com/method/inception-resnet-v2)，使用 [CC BY-SA 4.0](https://creativecommons.org/licenses/by-sa/4.0/)
    许可证的免费使用)，Inception-ResNet-V2 架构
- en: '**Input:** Image of dimensions (299, 299, 3)'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '**输入：** 尺寸为 (299, 299, 3) 的图像'
- en: '**Output:** Image embedding of 1000-dimension'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '**输出：** 1000维图像嵌入'
- en: '**Other Details for Inception-ResNet-V2 models:**'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '**Inception-ResNet-V2 模型的其他细节：**'
- en: 'Paper Link: [https://arxiv.org/pdf/1602.07261.pdf](https://arxiv.org/pdf/1602.07261.pdf)'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 论文链接： [https://arxiv.org/pdf/1602.07261.pdf](https://arxiv.org/pdf/1602.07261.pdf)
- en: 'GitHub: [Inception-ResNet-V](https://paperswithcode.com/paper/inception-v4-inception-resnet-and-the-impact#code)2'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'GitHub: [Inception-ResNet-V](https://paperswithcode.com/paper/inception-v4-inception-resnet-and-the-impact#code)2'
- en: 'Published On: Aug 2016'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 发布日期：2016年8月
- en: 'Performance on ImageNet Dataset: 80% (Top 1 Accuracy), 95% (Top 5 Accuracy)'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 ImageNet 数据集上的表现：80%（Top 1 精度），95%（Top 5 精度）
- en: 'Number of Parameters: 56M'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 参数数量：56M
- en: 'Depth: 189'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深度：189
- en: 'Size on Disk: 215MB'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 磁盘大小：215MB
- en: '**Implementation:**'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '**实现：**'
- en: 'Instantiate the Inception-ResNet-V2 model using the below-mentioned code:'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用以下代码实例化 Inception-ResNet-V2 模型：
- en: '[PRE5]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The above-mentioned code is for Inception-ResNet-V2 implementation, for more
    details refer to [this documentation](https://keras.io/api/applications/inceptionresnetv2/).
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码是用于 Inception-ResNet-V2 实现的，更多细节请参考 [这份文档](https://keras.io/api/applications/inceptionresnetv2/)。
- en: '6) MobileNet:'
  id: totrans-136
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '6) MobileNet:'
- en: MobileNet is a streamlined architecture that uses depthwise separable convolutions
    to construct deep convolutional neural networks and provides an efficient model
    for mobile and embedded vision applications.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: MobileNet 是一种简化的架构，使用深度可分离卷积来构建深度卷积神经网络，并为移动和嵌入式视觉应用提供高效的模型。
- en: '**Architecture:**'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '**架构：**'
- en: '![](../Images/72ad57d5161cf1660e5501abfde689bc.png)'
  id: totrans-139
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/72ad57d5161cf1660e5501abfde689bc.png)'
- en: ([Source](https://www.hindawi.com/journals/misy/2020/7602384/), Free-to-use
    license under [CC BY-SA 4.0](https://creativecommons.org/licenses/by-sa/4.0/)),
    Mobile-Net architecture
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: ([来源](https://www.hindawi.com/journals/misy/2020/7602384/)，使用 [CC BY-SA 4.0](https://creativecommons.org/licenses/by-sa/4.0/)
    许可证的免费使用)，Mobile-Net 架构
- en: '**Input:** Image of dimensions (224, 224, 3)'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '**输入：** 尺寸为 (224, 224, 3) 的图像'
- en: '**Output:** Image embedding of 1000-dimension'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '**输出：** 1000维图像嵌入'
- en: '**Other Details for MobileNet models:**'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '**MobileNet 模型的其他细节：**'
- en: 'Paper Link: [https://arxiv.org/pdf/1602.07261.pdf](https://arxiv.org/pdf/1704.04861.pdf)'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 论文链接： [https://arxiv.org/pdf/1602.07261.pdf](https://arxiv.org/pdf/1704.04861.pdf)
- en: 'GitHub: [MobileNet-V3](https://paperswithcode.com/paper/searching-for-mobilenetv3#code),
    [MobileNet-V2](https://paperswithcode.com/paper/mobilenetv2-inverted-residuals-and-linear#code)'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GitHub：[MobileNet-V3](https://paperswithcode.com/paper/searching-for-mobilenetv3#code)，[MobileNet-V2](https://paperswithcode.com/paper/mobilenetv2-inverted-residuals-and-linear#code)
- en: 'Published On: Apr 2017'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 发布日期：2017年4月
- en: 'Performance on ImageNet Dataset: 71% (Top 1 Accuracy), 90% (Top 5 Accuracy)'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在ImageNet数据集上的表现：71%（Top 1准确率），90%（Top 5准确率）
- en: 'Number of Parameters: 3.5–4.3M'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 参数数量：3.5–4.3M
- en: 'Depth: 55–105'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深度：55–105
- en: 'Size on Disk: 14–16MB'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 磁盘上的大小：14–16MB
- en: '**Implementation:**'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '**实现：**'
- en: 'Instantiate the MobileNet model using the below-mentioned code:'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用下面提到的代码实例化MobileNet模型：
- en: '[PRE6]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The above-mentioned code is for MobileNet implementation, keras offers a similar
    API to other MobileNet architecture (MobileNet-V2, MobileNet-V3) implementation,
    for more details refer to [this documentation](https://keras.io/api/applications/mobilenet/).
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码用于MobileNet实现，keras提供了与其他MobileNet架构（MobileNet-V2，MobileNet-V3）实现类似的API，更多详细信息请参见[此文档](https://keras.io/api/applications/mobilenet/)。
- en: '7) DenseNet:'
  id: totrans-155
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 7) DenseNet：
- en: DenseNet is a CNN model developed to improve accuracy caused by the vanishing
    gradient in high-level neural networks due to the long distance between input
    and output layers and the information vanishes before reaching the destination.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: DenseNet是一个CNN模型，旨在改善由于输入层和输出层之间的长距离导致的梯度消失问题，从而提高高层神经网络的准确性。
- en: '**Architecture:**'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '**架构：**'
- en: A DenseNet architecture has 3 dense blocks. The layers between two adjacent
    blocks are referred to as transition layers and change feature-map sizes via convolution
    and pooling.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: DenseNet架构有3个密集块。两个相邻块之间的层称为过渡层，通过卷积和池化改变特征图的尺寸。
- en: '![](../Images/6ad573e101eebd6f3ecb20c10bcda321.png)'
  id: totrans-159
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6ad573e101eebd6f3ecb20c10bcda321.png)'
- en: ([Source](https://paperswithcode.com/method/densenet), Free-to-use license under
    [CC BY-SA 4.0](https://creativecommons.org/licenses/by-sa/4.0/)), DenseNet architecture
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: ([来源](https://paperswithcode.com/method/densenet)，在[CC BY-SA 4.0](https://creativecommons.org/licenses/by-sa/4.0/)下的免费使用许可)，DenseNet架构
- en: '**Input:** Image of dimensions (224, 224, 3)'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: '**输入：** 尺寸为（224, 224, 3）的图像'
- en: '**Output:** Image embedding of 1000-dimension'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '**输出：** 1000维的图像嵌入'
- en: '**Other Details for DenseNet models:**'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: '**DenseNet模型的其他细节：**'
- en: 'Paper Link: [https://arxiv.org/pdf/1608.06993.pdf](https://arxiv.org/pdf/1608.06993.pdf)'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 论文链接：[https://arxiv.org/pdf/1608.06993.pdf](https://arxiv.org/pdf/1608.06993.pdf)
- en: 'GitHub: [DenseNet-169](https://paperswithcode.com/paper/densely-connected-convolutional-networks#code),
    [DenseNet-201](https://paperswithcode.com/paper/densely-connected-convolutional-networks),
    [DenseNet-264](https://paperswithcode.com/paper/densely-connected-convolutional-networks#code)'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GitHub：[DenseNet-169](https://paperswithcode.com/paper/densely-connected-convolutional-networks#code)，[DenseNet-201](https://paperswithcode.com/paper/densely-connected-convolutional-networks)，[DenseNet-264](https://paperswithcode.com/paper/densely-connected-convolutional-networks#code)
- en: 'Published On: Jan 2018'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 发布日期：2018年1月
- en: 'Performance on ImageNet Dataset: 75–77% (Top 1 Accuracy), 92–94% (Top 5 Accuracy)'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在ImageNet数据集上的表现：75–77%（Top 1准确率），92–94%（Top 5准确率）
- en: 'Number of Parameters: 8–20M'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 参数数量：8–20M
- en: 'Depth: 240–400'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深度：240–400
- en: 'Size on Disk: 33–80MB'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 磁盘上的大小：33–80MB
- en: '**Implementation:**'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: '**实现：**'
- en: 'Instantiate the DenseNet121 model using the below-mentioned code:'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用下面提到的代码实例化DenseNet121模型：
- en: '[PRE7]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: The above-mentioned code is for DenseNet implementation, keras offers a similar
    API to other DenseNet architecture (DenseNet-169, DenseNet-201) implementation,
    for more details refer to [this documentation](https://keras.io/api/applications/mobilenet/).
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码用于DenseNet实现，keras提供了与其他DenseNet架构（DenseNet-169，DenseNet-201）实现类似的API，更多详细信息请参见[此文档](https://keras.io/api/applications/mobilenet/)。
- en: '8) NasNet:'
  id: totrans-175
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 8) NasNet：
- en: Google researchers designed a NasNet model that framed the problem to find the
    best CNN architecture as a Reinforcement Learning approach. The idea is to search
    for the best combination of parameters of the given search space of a number of
    layers, filter sizes, strides, output channels, etc.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: Google研究人员设计了一个NasNet模型，将寻找最佳CNN架构的问题框架化为一种强化学习方法。其思想是搜索给定搜索空间的最佳参数组合，包括层数、滤波器大小、步幅、输出通道等。
- en: '**Input:** Image of dimensions (331, 331, 3)'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: '**输入：** 尺寸为（331, 331, 3）的图像'
- en: '**Other Details for NasNet models:**'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: '**NasNet模型的其他细节：**'
- en: 'Paper Link: [https://arxiv.org/pdf/1608.06993.pdf](https://arxiv.org/pdf/1707.07012.pdf)'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 论文链接：[https://arxiv.org/pdf/1608.06993.pdf](https://arxiv.org/pdf/1707.07012.pdf)
- en: 'Published On: Apr 2018'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 发布日期：2018年4月
- en: 'Performance on ImageNet Dataset: 75–83% (Top 1 Accuracy), 92–96% (Top 5 Accuracy)'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在ImageNet数据集上的表现：75–83%（Top 1准确率），92–96%（Top 5准确率）
- en: 'Number of Parameters: 5–90M'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 参数数量：5–90M
- en: 'Depth: 389–533'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深度：389–533
- en: 'Size on Disk: 23–343MB'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 磁盘大小：23–343MB
- en: '**Implementation:**'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: '**实现：**'
- en: 'Instantiate the NesNetLarge model using the below-mentioned code:'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用以下代码实例化NesNetLarge模型：
- en: '[PRE8]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The above-mentioned code is for NesNet implementation, keras offers a similar
    API to other NasNet architecture (NasNetLarge, NasNetMobile) implementation, for
    more details refer to [this documentation](https://keras.io/api/applications/nasnet/#nasnetmobile-function).
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码用于NesNet实现，keras提供了类似的API用于其他NasNet架构（NasNetLarge，NasNetMobile）的实现，更多细节请参考[此文档](https://keras.io/api/applications/nasnet/#nasnetmobile-function)。
- en: '9) EfficientNet:'
  id: totrans-189
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 9) EfficientNet：
- en: EfficientNet is a CNN architecture from the researchers of Google, that can
    achieve better performance by a scaling method called compound scaling. This scaling
    method uniformly scales all dimensions of depth/width/resolution by a fixed amount
    (compound coefficient) uniformly.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: EfficientNet是谷歌研究人员提出的一种CNN架构，通过一种称为复合缩放的缩放方法实现更好的性能。该缩放方法通过固定的复合系数均匀缩放所有深度/宽度/分辨率维度。
- en: '**Architecture:**'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: '**架构：**'
- en: '![](../Images/c66322e34001b6b1958742fb0b98839d.png)'
  id: totrans-192
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c66322e34001b6b1958742fb0b98839d.png)'
- en: ([Source](https://paperswithcode.com/method/efficientnet), Free-to-use license
    under [CC BY-SA 4.0](https://creativecommons.org/licenses/by-sa/4.0/)), Efficient-B0
    architecture
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: ([来源](https://paperswithcode.com/method/efficientnet)，在[CC BY-SA 4.0](https://creativecommons.org/licenses/by-sa/4.0/)下免费使用许可证)，Efficient-B0架构
- en: '**Other Details for EfficientNet Models:**'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: '**EfficientNet模型的其他细节：**'
- en: 'Paper Link: [https://arxiv.org/pdf/1905.11946v5.pdf](https://arxiv.org/pdf/1905.11946v5.pdf)'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 论文链接：[https://arxiv.org/pdf/1905.11946v5.pdf](https://arxiv.org/pdf/1905.11946v5.pdf)
- en: 'GitHub: [EfficientNet](https://paperswithcode.com/paper/efficientnet-rethinking-model-scaling-for#code)'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GitHub：[EfficientNet](https://paperswithcode.com/paper/efficientnet-rethinking-model-scaling-for#code)
- en: 'Published On: Sep 2020'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 发布日期：2020年9月
- en: 'Performance on ImageNet Dataset: 77–84% (Top 1 Accuracy), 93–97% (Top 5 Accuracy)'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在ImageNet数据集上的表现：77–84%（Top 1准确率），93–97%（Top 5准确率）
- en: 'Number of Parameters: 5–67M'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 参数数量：5–67M
- en: 'Depth: 132–438'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深度：132–438
- en: 'Size on Disk: 29–256MB'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 磁盘大小：29–256MB
- en: '**Implementation:**'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: '**实现：**'
- en: 'Instantiate the EfficientNet-B0 model using the below-mentioned code:'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用以下代码实例化EfficientNet-B0模型：
- en: '[PRE9]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: The above-mentioned code is for EfficientNet-B0 implementation, keras offers
    a similar API for other EfficientNet architecture (EfficientNet-B0 to B7, EfficientNet-V2-B0
    to B3) implementation, for more details refer to [this documentation](https://keras.io/api/applications/efficientnet/#efficientnetb0-function),
    and [this documentation](https://keras.io/api/applications/efficientnet_v2/#efficientnetv2b0-function).
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码用于EfficientNet-B0实现，keras提供了类似的API用于其他EfficientNet架构（EfficientNet-B0到B7，EfficientNet-V2-B0到B3）的实现，更多细节请参考[此文档](https://keras.io/api/applications/efficientnet/#efficientnetb0-function)，以及[此文档](https://keras.io/api/applications/efficientnet_v2/#efficientnetv2b0-function)。
- en: '10) ConvNeXt:'
  id: totrans-206
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 10) ConvNeXt：
- en: The ConvNeXt CNN model was proposed as a pure convolutional model (ConvNet),
    inspired by the design of Vision Transformers, that claims to outperform them.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: ConvNeXt CNN模型被提出作为一种纯卷积模型（ConvNet），受Vision Transformers设计的启发，声称能够超越它们。
- en: '**Architecture:**'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: '**架构：**'
- en: '![](../Images/ec14809f89d123eb8425466d17393e20.png)'
  id: totrans-209
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ec14809f89d123eb8425466d17393e20.png)'
- en: ([Source](https://arxiv.org/pdf/2201.03545.pdf), Free-to-use license under [CC
    BY-SA 4.0](https://creativecommons.org/licenses/by-sa/4.0/)), ConvNeXt architecture
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: ([来源](https://arxiv.org/pdf/2201.03545.pdf)，在[CC BY-SA 4.0](https://creativecommons.org/licenses/by-sa/4.0/)下免费使用许可证)，ConvNeXt架构
- en: '**Other Details for ConvNeXt models:**'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: '**ConvNeXt模型的其他细节：**'
- en: 'Paper Link: [https://arxiv.org/pdf/1905.11946v5.pdf](https://arxiv.org/pdf/1905.11946v5.pdf)'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 论文链接：[https://arxiv.org/pdf/1905.11946v5.pdf](https://arxiv.org/pdf/1905.11946v5.pdf)
- en: 'GitHub: [ConvNeXt](https://paperswithcode.com/paper/a-convnet-for-the-2020s#code)'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GitHub：[ConvNeXt](https://paperswithcode.com/paper/a-convnet-for-the-2020s#code)
- en: 'Published On: March 2022'
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 发布日期：2022年3月
- en: 'Performance on ImageNet Dataset: 81–87% (Top 1 Accuracy)'
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在ImageNet数据集上的表现：81–87%（Top 1准确率）
- en: 'Number of Parameters: 29–350M'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 参数数量：29–350M
- en: 'Size on Disk: 110–1310MB'
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 磁盘大小：110–1310MB
- en: '**Implementation:**'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: '**实现：**'
- en: 'Instantiate the ConvNeXt-Tiny model using the below-mentioned code:'
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用以下代码实例化ConvNeXt-Tiny模型：
- en: '[PRE10]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: The above-mentioned code is for ConvNeXt-Tiny implementation, keras offers a
    similar API of the other EfficientNet architecture (ConvNeXt-Small, ConvNeXt-Base,
    ConvNeXt-Large, ConvNeXt-XLarge) implementation, for more details refer to [this
    documentation](https://keras.io/api/applications/convnext/#convnexttiny-function).
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码是用于 ConvNeXt-Tiny 实现的，keras 提供了类似的 API 用于其他 EfficientNet 架构（ConvNeXt-Small、ConvNeXt-Base、ConvNeXt-Large、ConvNeXt-XLarge）的实现，更多细节请参见[此文档](https://keras.io/api/applications/convnext/#convnexttiny-function)。
- en: 'Summary:'
  id: totrans-222
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结：
- en: I have discussed 10 popular CNN architectures that can generate embeddings using
    transfer learning. These pre-trained CNN models have outperformed the ImageNet
    dataset and proved the best. Keras library offers APIs to load the architecture
    and weights of the discussed pre-trained models. The image embeddings generated
    from these models can be used for various use cases.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 我讨论了 10 种流行的 CNN 架构，这些架构可以使用迁移学习生成嵌入。这些预训练的 CNN 模型在 ImageNet 数据集上表现优异，并且证明了它们是最好的。Keras
    库提供了 API 来加载这些讨论过的预训练模型的架构和权重。从这些模型生成的图像嵌入可以用于各种用例。
- en: However, this is a continuously growing domain and there is always a new CNN
    architecture to look forward to.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这是一个不断发展的领域，总是有新的 CNN 架构值得期待。
- en: 'References:'
  id: totrans-225
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献：
- en: '[1] Papers with code: [https://paperswithcode.com/sota/image-classification-on-imagenet](https://paperswithcode.com/sota/image-classification-on-imagenet)'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] Papers with code: [https://paperswithcode.com/sota/image-classification-on-imagenet](https://paperswithcode.com/sota/image-classification-on-imagenet)'
- en: '[2] Keras Documentation: [https://keras.io/api/applications/](https://keras.io/api/applications/)'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] Keras 文档: [https://keras.io/api/applications/](https://keras.io/api/applications/)'
- en: Thank You for Reading
  id: totrans-228
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 感谢阅读
