- en: Distributed Hyperparameter Tuning in Vertex AI Pipeline
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Vertex AI 流水线中的分布式超参数调优
- en: 原文：[https://towardsdatascience.com/distributed-hyperparameter-tuning-in-vertex-ai-pipeline-2f3278a1eb64?source=collection_archive---------11-----------------------#2023-03-29](https://towardsdatascience.com/distributed-hyperparameter-tuning-in-vertex-ai-pipeline-2f3278a1eb64?source=collection_archive---------11-----------------------#2023-03-29)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/distributed-hyperparameter-tuning-in-vertex-ai-pipeline-2f3278a1eb64?source=collection_archive---------11-----------------------#2023-03-29](https://towardsdatascience.com/distributed-hyperparameter-tuning-in-vertex-ai-pipeline-2f3278a1eb64?source=collection_archive---------11-----------------------#2023-03-29)
- en: A path to enable the distributed hyperparameter tuning in GCP Vertex AI pipeline
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 启用 GCP Vertex AI 流水线中的分布式超参数调优路径
- en: '[](https://medium.com/@hangyu_5199?source=post_page-----2f3278a1eb64--------------------------------)[![Hang
    Yu](../Images/feb12ff14af31f9875ea2ad121d5a41e.png)](https://medium.com/@hangyu_5199?source=post_page-----2f3278a1eb64--------------------------------)[](https://towardsdatascience.com/?source=post_page-----2f3278a1eb64--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----2f3278a1eb64--------------------------------)
    [Hang Yu](https://medium.com/@hangyu_5199?source=post_page-----2f3278a1eb64--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@hangyu_5199?source=post_page-----2f3278a1eb64--------------------------------)[![Hang
    Yu](../Images/feb12ff14af31f9875ea2ad121d5a41e.png)](https://medium.com/@hangyu_5199?source=post_page-----2f3278a1eb64--------------------------------)[](https://towardsdatascience.com/?source=post_page-----2f3278a1eb64--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----2f3278a1eb64--------------------------------)
    [Hang Yu](https://medium.com/@hangyu_5199?source=post_page-----2f3278a1eb64--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F2665192d75e3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdistributed-hyperparameter-tuning-in-vertex-ai-pipeline-2f3278a1eb64&user=Hang+Yu&userId=2665192d75e3&source=post_page-2665192d75e3----2f3278a1eb64---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----2f3278a1eb64--------------------------------)
    ·10 min read·Mar 29, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F2f3278a1eb64&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdistributed-hyperparameter-tuning-in-vertex-ai-pipeline-2f3278a1eb64&user=Hang+Yu&userId=2665192d75e3&source=-----2f3278a1eb64---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F2665192d75e3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdistributed-hyperparameter-tuning-in-vertex-ai-pipeline-2f3278a1eb64&user=Hang+Yu&userId=2665192d75e3&source=post_page-2665192d75e3----2f3278a1eb64---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----2f3278a1eb64--------------------------------)
    ·10 min read·2023年3月29日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F2f3278a1eb64&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdistributed-hyperparameter-tuning-in-vertex-ai-pipeline-2f3278a1eb64&user=Hang+Yu&userId=2665192d75e3&source=-----2f3278a1eb64---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F2f3278a1eb64&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdistributed-hyperparameter-tuning-in-vertex-ai-pipeline-2f3278a1eb64&source=-----2f3278a1eb64---------------------bookmark_footer-----------)![](../Images/8b0fddf4a86690a33d7255047da3c52b.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F2f3278a1eb64&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdistributed-hyperparameter-tuning-in-vertex-ai-pipeline-2f3278a1eb64&source=-----2f3278a1eb64---------------------bookmark_footer-----------)![](../Images/8b0fddf4a86690a33d7255047da3c52b.png)'
- en: Photo by [Marsha Reid](https://unsplash.com/@marsha_reid?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 照片由[Marsha Reid](https://unsplash.com/@marsha_reid?utm_source=medium&utm_medium=referral)提供，发布在[Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: Introduction
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: Vertex AI pipelines offer a handy way to implement end-to-end ML workflows from
    data collection to endpoint monitoring with extremely low effort. For new users,
    the easiness of development and deployment is largely thanks to the [**V**ertex
    AI pipeline example](https://github.com/GoogleCloudPlatform/professional-services/tree/main/examples/vertex_pipeline)
    offered by GCP.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: Vertex AI 流水线提供了一种便捷的方法来实现从数据收集到端点监控的端到端机器学习工作流，几乎无需额外努力。对于新用户来说，开发和部署的简便性很大程度上归功于
    GCP 提供的[**V**ertex AI 流水线示例](https://github.com/GoogleCloudPlatform/professional-services/tree/main/examples/vertex_pipeline)。
- en: '[](https://github.com/GoogleCloudPlatform/professional-services/tree/main/examples/vertex_pipeline?source=post_page-----2f3278a1eb64--------------------------------)
    [## professional-services/examples/vertex_pipeline at main · GoogleCloudPlatform/professional-services'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '[## professional-services/examples/vertex_pipeline 在 main · GoogleCloudPlatform/professional-services](https://github.com/GoogleCloudPlatform/professional-services/tree/main/examples/vertex_pipeline?source=post_page-----2f3278a1eb64--------------------------------)'
- en: This repository demonstrates end-to-end MLOps process using Vertex AI platform
    and Smart Analytics technology…
  id: totrans-12
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 这个仓库展示了如何使用 Vertex AI 平台和智能分析技术进行端到端的 MLOps 过程…
- en: github.com](https://github.com/GoogleCloudPlatform/professional-services/tree/main/examples/vertex_pipeline?source=post_page-----2f3278a1eb64--------------------------------)
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '[github.com](https://github.com/GoogleCloudPlatform/professional-services/tree/main/examples/vertex_pipeline?source=post_page-----2f3278a1eb64--------------------------------)'
- en: 'Despite the comprehensive demonstration of the essential components, the official
    example also exposes the feasibility for users to customize and enhance the pipeline
    based on their own needs. Amongst all, one of the most exciting components is
    the distributed Hyperparameter Tuning (HPT) that is capable of exploring a huge
    search space to identify the best hyperparameters in a short time. For the time
    being, GCP recommends leveraging **cloudml-hypertune** and **google_cloud_pipeline_components**
    for this purpose and presents the corresponding tutorials:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管示例全面展示了基本组件，官方示例仍然揭示了用户可以根据自身需求定制和增强管道的可行性。在所有组件中，最令人兴奋的之一是能够在短时间内探索大量搜索空间以识别最佳超参数的分布式超参数调整（HPT）。目前，GCP
    推荐使用**cloudml-hypertune**和**google_cloud_pipeline_components**来实现这一目的，并提供了相应的教程：
- en: '[GCP HPT task tutorial](https://codelabs.developers.google.com/vertex-training-200#0)'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '[GCP HPT 任务教程](https://codelabs.developers.google.com/vertex-training-200#0)'
- en: '[## Vertex AI: Distributed hyperparameter tuning | Google Codelabs'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '[## Vertex AI: 分布式超参数调整 | Google Codelabs](https://codelabs.developers.google.com/vertex-training-200?source=post_page-----2f3278a1eb64--------------------------------#0)'
- en: In this lab, you'll learn how to use Vertex AI for hyperparameter tuning and
    distributed training. While this lab uses…
  id: totrans-17
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 在这个实验中，你将学习如何使用 Vertex AI 进行超参数调整和分布式训练。虽然这个实验使用了…
- en: codelabs.developers.google.com](https://codelabs.developers.google.com/vertex-training-200?source=post_page-----2f3278a1eb64--------------------------------#0)
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '[codelabs.developers.google.com](https://codelabs.developers.google.com/vertex-training-200?source=post_page-----2f3278a1eb64--------------------------------#0)'
- en: '[HPT pipeline sample](https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/cad623ef84882f410fcc0dc39527be25a5e5f584/notebooks/community/ml_ops/stage3/get_started_with_hpt_pipeline_components.ipynb)'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '[HPT 管道示例](https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/cad623ef84882f410fcc0dc39527be25a5e5f584/notebooks/community/ml_ops/stage3/get_started_with_hpt_pipeline_components.ipynb)'
- en: '[](https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/cad623ef84882f410fcc0dc39527be25a5e5f584/notebooks/community/ml_ops/stage3/get_started_with_hpt_pipeline_components.ipynb?source=post_page-----2f3278a1eb64--------------------------------)
    [## vertex-ai-samples/get_started_with_hpt_pipeline_components.ipynb at…'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '[## vertex-ai-samples/get_started_with_hpt_pipeline_components.ipynb 在…](https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/cad623ef84882f410fcc0dc39527be25a5e5f584/notebooks/community/ml_ops/stage3/get_started_with_hpt_pipeline_components.ipynb?source=post_page-----2f3278a1eb64--------------------------------)'
- en: You can't perform that action at this time. You signed in with another tab or
    window. You signed out in another tab or…
  id: totrans-21
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 你目前无法执行该操作。你在另一个标签页或窗口中登录了。你在另一个标签页或窗口中登出了…
- en: github.com](https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/cad623ef84882f410fcc0dc39527be25a5e5f584/notebooks/community/ml_ops/stage3/get_started_with_hpt_pipeline_components.ipynb?source=post_page-----2f3278a1eb64--------------------------------)
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '[github.com](https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/cad623ef84882f410fcc0dc39527be25a5e5f584/notebooks/community/ml_ops/stage3/get_started_with_hpt_pipeline_components.ipynb?source=post_page-----2f3278a1eb64--------------------------------)'
- en: However, the limitation of the tutorials is that the distributed HPT is presented
    as a standalone HPT task/pipeline and it doesn’t explicitly present the approach
    to integrate into the existing Vertex AI pipeline shown in the [**V**ertex AI
    pipeline example](https://github.com/GoogleCloudPlatform/professional-services/tree/main/examples/vertex_pipeline),
    which motivates me to share my successful attempt that bridges the gap. I believe
    this will benefit many businesses who have built or will build their ML workflows
    based on Vertex AI pipeline.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这些教程的局限性在于分布式HPT被呈现为一个独立的HPT任务/管道，而没有明确展示如何将其集成到现有的Vertex AI管道中，如[**V**ertex
    AI管道示例](https://github.com/GoogleCloudPlatform/professional-services/tree/main/examples/vertex_pipeline)所示，这促使我分享我成功弥合这一差距的尝试。我相信这将有利于许多已经建立或将要建立基于Vertex
    AI管道的ML工作流的企业。
- en: '**The major contribution of this blog is the integration of the distributed
    HPT into the Vertex AI pipelines. Specifically, it demonstrates how to:**'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '**这个博客的主要贡献是将分布式HPT集成到Vertex AI管道中。具体来说，它演示了如何：**'
- en: Chain data collection and preprocessing of a Vertex AI pipeline to the distributed
    HPT. In comparison, the [GCP HPT task tutorial](https://codelabs.developers.google.com/vertex-training-200#0)
    and [HPT pipeline sample](https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/cad623ef84882f410fcc0dc39527be25a5e5f584/notebooks/community/ml_ops/stage3/get_started_with_hpt_pipeline_components.ipynb)
    simplify data collection and processing via loading a static tensorflow dataset
    in the training step.
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将数据收集和预处理链入Vertex AI管道中的分布式HPT。相比之下，[GCP HPT任务教程](https://codelabs.developers.google.com/vertex-training-200#0)和[HPT管道示例](https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/cad623ef84882f410fcc0dc39527be25a5e5f584/notebooks/community/ml_ops/stage3/get_started_with_hpt_pipeline_components.ipynb)通过在训练步骤中加载静态的tensorflow数据集简化了数据收集和处理。
- en: Optimize HPT results collection to avoid docker arg length limit. In the [HPT
    pipeline sample](https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/cad623ef84882f410fcc0dc39527be25a5e5f584/notebooks/community/ml_ops/stage3/get_started_with_hpt_pipeline_components.ipynb),
    the complete HPT results of all trials are encoded as a string that’s passed to
    a docker task as an input argument for further processing. However, the risk is
    that the string might violate the length limit of docker input argument in case
    of a large search space. Thus, a simple solution that combines those two components
    is explored in this article.
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 优化HPT结果收集以避免docker参数长度限制。在[HPT管道示例](https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/cad623ef84882f410fcc0dc39527be25a5e5f584/notebooks/community/ml_ops/stage3/get_started_with_hpt_pipeline_components.ipynb)中，所有试验的完整HPT结果被编码为一个字符串，该字符串作为输入参数传递给docker任务以进行进一步处理。然而，风险在于如果搜索空间较大，该字符串可能会违反docker输入参数的长度限制。因此，本文探讨了一种将这两个组件结合的简单解决方案。
- en: Save the best parameters in firestore. In the [HPT pipeline sample](https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/cad623ef84882f410fcc0dc39527be25a5e5f584/notebooks/community/ml_ops/stage3/get_started_with_hpt_pipeline_components.ipynb),
    HPT runs the trials, saves the models, and deploys the best one whereas it’s unclear
    how to visit the best parameters afterwards. This doesn’t suit the scenario that
    HPT and training is expected to be decoupled. Thus, the firestore option is explored
    to save the best hyperparameters for the later training jobs.
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将最佳参数保存在Firestore中。在[HPT管道示例](https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/cad623ef84882f410fcc0dc39527be25a5e5f584/notebooks/community/ml_ops/stage3/get_started_with_hpt_pipeline_components.ipynb)中，HPT运行试验、保存模型并部署最佳模型，但之后如何访问最佳参数尚不清楚。这不适合HPT和训练期望解耦的场景。因此，本文探讨了使用Firestore选项来保存最佳超参数以供后续训练作业使用。
- en: Chain the distributed HPT to the training component and train the model using
    the best parameters. Instead of saving the model of every HPT trial as shown in
    the [HPT pipeline sample](https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/cad623ef84882f410fcc0dc39527be25a5e5f584/notebooks/community/ml_ops/stage3/get_started_with_hpt_pipeline_components.ipynb),
    an alternative that re-trains and only saves the best model is explored though
    it’s open to debate if this approach offers a better storage-compute tradeoff
    depending on the specific scenario.
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将分布式HPT链到训练组件中，并使用最佳参数训练模型。与[HPT管道示例](https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/cad623ef84882f410fcc0dc39527be25a5e5f584/notebooks/community/ml_ops/stage3/get_started_with_hpt_pipeline_components.ipynb)中所示的每个HPT试验都保存模型不同，本文探讨了一种替代方案，即重新训练并仅保存最佳模型，尽管这种方法是否提供了更好的存储-计算权衡仍然存在争议，取决于具体场景。
- en: Integrating distributed HPT into Vertex AI pipeline
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将分布式 HPT 集成到 Vertex AI 管道中
- en: Now, let’s go through the major steps mentioned above. It is worth noting that
    the ML pipeline demonstrated here is largely based on the [**V**ertex AI pipeline
    example](https://github.com/GoogleCloudPlatform/professional-services/tree/main/examples/vertex_pipeline)
    and only the minimum changes are made to enable HPT. For the purpose of demonstration,
    only two hyperparameters are tuned via grid search as shown below.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们按照上面提到的主要步骤进行。值得注意的是，这里演示的 ML 管道主要基于[**Vertex AI 管道示例**](https://github.com/GoogleCloudPlatform/professional-services/tree/main/examples/vertex_pipeline)，仅对其进行了最小的更改以实现
    HPT。为了演示，只通过网格搜索调整了两个超参数，如下所示。
- en: '[PRE0]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The jupyter notebook of this work, which contains the end-to-end process to
    deploy the distributed HPT, is hosted in the [repo](https://github.com/simon19891101/professional-services/blob/distributed-hpt-demo/examples/vertex_pipeline/notebook/hpt_pipeline_development.ipynb)
    below.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 这个工作的 jupyter notebook 包含了部署分布式 HPT 的端到端过程，托管在以下的[仓库](https://github.com/simon19891101/professional-services/blob/distributed-hpt-demo/examples/vertex_pipeline/notebook/hpt_pipeline_development.ipynb)中。
- en: '[](https://github.com/simon19891101/professional-services/blob/distributed-hpt-demo/examples/vertex_pipeline/notebook/hpt_pipeline_development.ipynb?source=post_page-----2f3278a1eb64--------------------------------)
    [## professional-services/hpt_pipeline_development.ipynb at distributed-hpt-demo
    ·…'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://github.com/simon19891101/professional-services/blob/distributed-hpt-demo/examples/vertex_pipeline/notebook/hpt_pipeline_development.ipynb?source=post_page-----2f3278a1eb64--------------------------------)
    [## professional-services/hpt_pipeline_development.ipynb at distributed-hpt-demo
    ·…'
- en: Common solutions and tools developed by Google Cloud&#39;s Professional Services
    team …
  id: totrans-34
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Google Cloud 的专业服务团队开发的常见解决方案和工具...
- en: github.com](https://github.com/simon19891101/professional-services/blob/distributed-hpt-demo/examples/vertex_pipeline/notebook/hpt_pipeline_development.ipynb?source=post_page-----2f3278a1eb64--------------------------------)
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: github.com](https://github.com/simon19891101/professional-services/blob/distributed-hpt-demo/examples/vertex_pipeline/notebook/hpt_pipeline_development.ipynb?source=post_page-----2f3278a1eb64--------------------------------)
- en: 1\. Chain data preprocessing to HPT
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1. 将数据预处理链接到 HPT。
- en: The first challenge I encountered is that the [**vertex-ai-samples**](https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/cad623ef84882f410fcc0dc39527be25a5e5f584/notebooks/community/ml_ops/stage3/get_started_with_hpt_pipeline_components.ipynb)tutorialhard
    coded the data collection in the HPT container image that is called by the HyperparameterTuningJobRunOp
    class of google_cloud_pipeline_components.v1.hyperparameter_tuning_job whereas
    in practice we may want to use the data collection and processing pipeline component
    in the pipeline.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 我遇到的第一个挑战是 **vertex-ai-samples** 的教程在调用 google_cloud_pipeline_components.v1.hyperparameter_tuning_job
    的 HyperparameterTuningJobRunOp 类时，将数据收集硬编码在了 HPT 容器映像中，而实际上我们可能想要在管道中使用数据收集和处理组件。
- en: '[PRE1]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: However, currently the HyperparameterTuningJobRunOp doesn’t support input data
    as an argument, which motivates me to find an alternative way to pass the data
    source. Fortunately, it turns out that HyperparameterTuningJobRunOp consumes worker_pool_specs
    that contains the HPT container specification which supports the input args to
    the HPT container.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，目前 HyperparameterTuningJobRunOp 不支持将输入数据作为参数传递，这激发了我寻找一种替代方法来传递数据源的动机。幸运的是，HyperparameterTuningJobRunOp
    使用包含支持 HPT 容器输入参数的 HPT 容器规范的 worker_pool_specs。
- en: '[PRE2]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Intuitively, it implies the feasibility to pass the input data source as a part
    of the args of the container_spec and it’s validated to be a successful attempt.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 直观地说，它意味着将输入数据源作为容器规范的一部分传递是可行的，并且验证证明这是一次成功的尝试。
- en: The code example of such operation is shown below. Specifically, a new pipeline
    component called worker_pool_specs is created to receive the input_dataset from
    the data processing component and generate the worker_pool_specs that’s passed
    to HyperparameterTuningJobRunOp. In this way, data preprocessing and the core
    HPT module is associated as depicted in the screenshot below. It’s worth noting
    training_data_schema, label, and features are also passed as they are required
    by the training script of [**V**ertex AI pipeline example](https://github.com/GoogleCloudPlatform/professional-services/tree/main/examples/vertex_pipeline).
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 下面展示了这种操作的代码示例。具体来说，创建了一个名为 worker_pool_specs 的新管道组件，用于接收来自数据处理组件的 input_dataset，并生成传递给
    HyperparameterTuningJobRunOp 的 worker_pool_specs。这样，数据预处理和核心 HPT 模块就如下面的截图所示相关联。值得注意的是，training_data_schema、label
    和 features 也被传递，因为它们是 [**V**ertex AI 管道示例](https://github.com/GoogleCloudPlatform/professional-services/tree/main/examples/vertex_pipeline)
    的训练脚本所需的。
- en: '[PRE3]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '![](../Images/98738cee422ae463f6258bdf299f0792.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/98738cee422ae463f6258bdf299f0792.png)'
- en: Chain preprocess to HPT via worker-pool-specs component
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 通过 worker-pool-specs 组件将预处理链入 HPT
- en: 2\. Optimize HPT results collection
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2\. 优化 HPT 结果收集
- en: In the original [HPT pipeline sample](https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/cad623ef84882f410fcc0dc39527be25a5e5f584/notebooks/community/ml_ops/stage3/get_started_with_hpt_pipeline_components.ipynb),
    the output of the HPT module, which is the resource name of the HPT job, is passed
    to the GetTrialsOp module to retrieve all the hyperparameters and their scores
    so as to let the GetBestTrialOp module find the best, as shown below.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在原始的 [HPT 管道示例](https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/cad623ef84882f410fcc0dc39527be25a5e5f584/notebooks/community/ml_ops/stage3/get_started_with_hpt_pipeline_components.ipynb)
    中，HPT 模块的输出，即 HPT 作业的资源名称，被传递给 GetTrialsOp 模块以检索所有超参数及其得分，从而让 GetBestTrialOp 模块找到最佳的，如下所示。
- en: '[PRE4]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Currently, GetTrialsOp module encodes the results of all HPT trials into a string
    as presented below.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 当前，GetTrialsOp 模块将所有 HPT 试验的结果编码为一个字符串，如下所示。
- en: '![](../Images/ce5cb406081b5ed85f04d1b48f6af1db.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ce5cb406081b5ed85f04d1b48f6af1db.png)'
- en: Sample output of GetTrialsOp
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: GetTrialsOp 的示例输出
- en: When the search space is large, one risk observed in practice is that this long
    string may violate the input argument length of the following docker container
    that is GetBestTrialOp.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 当搜索空间很大时，实际观察到的一个风险是，这个长字符串可能会违反 GetBestTrialOp docker 容器的输入参数长度限制。
- en: 'job_spec.worker_pool_specs[0].container_spec.args; Message: Container args
    should contain less than 100k characters'
  id: totrans-53
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: job_spec.worker_pool_specs[0].container_spec.args; 消息：容器参数应包含少于 100k 字符
- en: To avoid this limitation, a hacky but effective approach is attempted though
    there may be some better options. Basically, the source code of GetTrialsOp (see
    [hyperparameter_tuning_job](https://github.com/kubeflow/pipelines/tree/master/components/google-cloud/google_cloud_pipeline_components/experimental/hyperparameter_tuning_job))
    is injected into that of GetBestTrialOp so that those two pipeline components
    merge into one to avoid passing the long string as a docker input.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 为了避免这个限制，尝试了一种有些绕但有效的方法，尽管可能还有更好的选择。基本上，将 GetTrialsOp 的源代码（见 [hyperparameter_tuning_job](https://github.com/kubeflow/pipelines/tree/master/components/google-cloud/google_cloud_pipeline_components/experimental/hyperparameter_tuning_job)）注入到
    GetBestTrialOp 的源代码中，使这两个管道组件合并为一个，以避免将长字符串作为 docker 输入传递。
- en: '[PRE5]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '![](../Images/256f15b58f9ea020e2e64d578f62aba7.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/256f15b58f9ea020e2e64d578f62aba7.png)'
- en: GetTrialsOp injected into GetBestTrialOp to be one component
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: GetTrialsOp 注入到 GetBestTrialOp 中成为一个组件
- en: 3\. Save the best parameters in firestore
  id: totrans-58
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3\. 将最佳参数保存到 firestore
- en: 'In the [HPT pipeline sample](https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/cad623ef84882f410fcc0dc39527be25a5e5f584/notebooks/community/ml_ops/stage3/get_started_with_hpt_pipeline_components.ipynb),
    each HPT trial saves its trained model and the best one gets deployed later on.
    However, this approach, which couples HPT and model training, exposes some limitations:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [HPT 管道示例](https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/cad623ef84882f410fcc0dc39527be25a5e5f584/notebooks/community/ml_ops/stage3/get_started_with_hpt_pipeline_components.ipynb)
    中，每个 HPT 试验保存其训练模型，并且最佳模型会在后续部署。然而，这种将 HPT 和模型训练耦合在一起的方法暴露了一些限制：
- en: The model deployed is trained during one of the HPT trials. However, not every
    training needs HPT in practice. One such example is the recommender system built
    using matrix factorization. The model needs to be trained frequently using the
    latest user-item interactions but HPT is not always needed. Therefore, the option
    that decouples training and HPT is expected.
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 部署的模型是在一次HPT试验期间训练的。然而，实际操作中并非所有训练都需要HPT。例如，使用矩阵分解构建的推荐系统。该模型需要频繁地使用最新的用户-项目交互数据进行训练，但HPT并不总是需要。因此，解耦训练和HPT的选项是必要的。
- en: Deploying the model of HPT directly could lead to a biased evaluation as HPT
    is based on the validation data.
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 直接部署HPT模型可能会导致偏倚评估，因为HPT是基于验证数据的。
- en: To this end, instead of saving the trained models, it is preferred to save the
    best HPT result to a database like firestore for later use. After storing the
    best hyperparameters, model training and HPT are decoupled. The best parameters
    can be re-used to train models until a new round of HPT is needed. Besides, model
    evaluation can be improved by adding a seperate test set when a model is trained.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 为此，与其保存训练后的模型，更倾向于将最佳HPT结果保存到Firestore等数据库以备后用。存储最佳超参数后，模型训练和HPT被解耦。最佳参数可以在需要新的HPT轮次之前重复用于训练模型。此外，通过在训练模型时添加单独的测试集，可以改进模型评估。
- en: The code below demonstrates how the best HPT result is saved to firestore. Specifically,
    a pipeline component called best_hpt_to_args is defined to consume the best hyperparameters
    found by the GetBestTrialOp step discussed previously. The storage structure of
    firestore is to be decided case by case. Here, the timestamp is used to label
    a HPT pipeline. Lastly, this function returns a string “true”, which is preferred
    by the pipeline condition, to kick off the conditional model training that’s discussed
    later on. The validation accuracy is also logged for the sake of observability
    but this is totally optional.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码演示了如何将最佳HPT结果保存到Firestore。具体来说，定义了一个名为best_hpt_to_args的管道组件，用于处理由之前讨论的GetBestTrialOp步骤找到的最佳超参数。Firestore的存储结构需要根据具体情况决定。在这里，时间戳用于标记HPT管道。最后，此函数返回字符串“true”，这是管道条件所偏好的，以启动稍后讨论的条件模型训练。为了可观察性，验证准确率也被记录，但这是完全可选的。
- en: '[PRE6]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '![](../Images/33f13156abf2552447f9663849fffefb.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/33f13156abf2552447f9663849fffefb.png)'
- en: Firestore example of the saved HPT result
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 保存HPT结果的Firestore示例
- en: '![](../Images/186df62776fe928e597cb8a84919dd0f.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/186df62776fe928e597cb8a84919dd0f.png)'
- en: Save best HPT result to firestore
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 将最佳HPT结果保存到Firestore
- en: 4\. Train models using the best hyperparameters
  id: totrans-69
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4\. 使用最佳超参数训练模型
- en: Finally, the HPT is finished. The last improvement I made is appending a conditional
    training task so that the latest HPT so that the best hyperparameters are utilized
    to update the model in production immediately. This step is totally optional and
    subject to the specific use case. It’s worth noting that this condition is receiving
    hpt_op.output that’s a function that wraps all HPT components from worker_pool_specs
    to best_hpt_to_args so its output equals that of best_hpt_to_args. Please see
    the details in the notebook.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，HPT完成了。我做的最后一个改进是添加了一个条件训练任务，以便利用最新的HPT最佳超参数立即更新生产中的模型。这一步是完全可选的，取决于具体的使用案例。值得注意的是，这个条件接收的是hpt_op.output，这是一个函数，封装了从worker_pool_specs到best_hpt_to_args的所有HPT组件，因此它的输出等于best_hpt_to_args的输出。详情请参见笔记本。
- en: '[PRE7]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '![](../Images/ba2fe1c2f095bb49182f915fe5ec6400.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ba2fe1c2f095bb49182f915fe5ec6400.png)'
- en: Conditional training
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 条件训练
- en: In the training script ([images/training/app.py](https://github.com/simon19891101/professional-services/blob/distributed-hpt-demo/examples/vertex_pipeline/images/training/app.py)),
    a function called get_best_param_values is implemented to collect the latest HPT
    result by querying firestore. Based on different ways to label the HPT pipelines,
    there might be different approaches to collect the HPT result of interest. The
    collected hyperparameters are in the form of a dictionary so they can easily get
    used to train the model.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练脚本([images/training/app.py](https://github.com/simon19891101/professional-services/blob/distributed-hpt-demo/examples/vertex_pipeline/images/training/app.py))中，实现了一个名为get_best_param_values的函数，用于通过查询Firestore收集最新的HPT结果。根据标记HPT管道的不同方式，可能会有不同的方法来收集感兴趣的HPT结果。收集的超参数形式为字典，因此可以轻松用于训练模型。
- en: '[PRE8]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '[PRE9]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Summary
  id: totrans-77
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: 'Vertex AI pipelines on GCP provides a great platform to productionize ML solutions
    with high performance and flexibility. However, the existing tutorials show a
    limited coverage regarding how distributed HPT can be achieved. To fill up the
    gap, this article demonstrates the successful attempt to integrate the distributed
    GCP HPT module into the existing Vertex AI pipeline. Specifically, four limitations
    that are overlooked by the existing tutorials have been addressed:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: Vertex AI管道在GCP上提供了一个出色的平台，用于以高性能和灵活性将ML解决方案生产化。然而，现有教程对于如何实现分布式HPT的覆盖面有限。为填补这一空白，本文展示了将分布式GCP
    HPT模块成功集成到现有Vertex AI管道中的尝试。具体而言，现有教程忽视的四个局限性已被解决：
- en: Data input. This would allow users to use the data preprocessed on the fly for
    the purpose of HPT.
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 数据输入。这将允许用户即时使用预处理的数据进行HPT。
- en: HPT results collection. The optimized result collection enables the capacity
    to explore a large search space.
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: HPT结果收集。优化的结果收集能够探索更大的搜索空间。
- en: HPT results storage. Saving HPT results in firestore means training and HPT
    could be decoupled.
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: HPT结果存储。将HPT结果保存在Firestore中意味着训练和HPT可以解耦。
- en: Model training using the best HPT result. Now we can train new models using
    the saved HPT result.
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用最佳HPT结果进行模型训练。现在我们可以使用保存的HPT结果来训练新模型。
- en: It’s believed that the improvements discussed above would largely benefit the
    industrial use cases of Vertex AI pipelines for the scenarios that need to involve
    a fully automated distributed HPT to optimize the predictive power of the ML solutions
    running in production. For the detailed end-to-end implementation, please visit
    the notebook hosted in the [repo](https://github.com/simon19891101/professional-services/blob/distributed-hpt-demo/examples/vertex_pipeline/notebook/hpt_pipeline_development.ipynb)
    and feel free to get in touch with me on [LinkedIn](https://www.linkedin.com/in/hang-yu-0242ac120002/).
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 以上讨论的改进预计将大大有利于Vertex AI管道在需要涉及完全自动化分布式HPT的工业应用案例中，以优化运行中的ML解决方案的预测能力。有关详细的端到端实现，请访问[笔记本](https://github.com/simon19891101/professional-services/blob/distributed-hpt-demo/examples/vertex_pipeline/notebook/hpt_pipeline_development.ipynb)，并随时通过[LinkedIn](https://www.linkedin.com/in/hang-yu-0242ac120002/)与我联系。
- en: Thank you for reading!
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 感谢您的阅读！
- en: '*All images unless otherwise noted are by the author.*'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '*除非另有说明，否则所有图片均为作者提供。*'
