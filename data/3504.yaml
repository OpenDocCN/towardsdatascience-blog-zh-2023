- en: 'Courage to Learn ML: Demystifying L1 & L2 Regularization (part 2)'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 学习机器学习的勇气：揭开L1和L2正则化的面纱（第2部分）
- en: 原文：[https://towardsdatascience.com/courage-to-learn-ml-unraveling-l1-l2-regularization-part-2-1bb171e43b35?source=collection_archive---------6-----------------------#2023-11-25](https://towardsdatascience.com/courage-to-learn-ml-unraveling-l1-l2-regularization-part-2-1bb171e43b35?source=collection_archive---------6-----------------------#2023-11-25)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/courage-to-learn-ml-unraveling-l1-l2-regularization-part-2-1bb171e43b35?source=collection_archive---------6-----------------------#2023-11-25](https://towardsdatascience.com/courage-to-learn-ml-unraveling-l1-l2-regularization-part-2-1bb171e43b35?source=collection_archive---------6-----------------------#2023-11-25)
- en: Unlocking the Intuition Behind L1 Sparsity with Lagrange multipliers
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解锁L1稀疏性的直觉与拉格朗日乘子
- en: '[](https://amyma101.medium.com/?source=post_page-----1bb171e43b35--------------------------------)[![Amy
    Ma](../Images/2edf55456a1f92724535a1441fa2bef5.png)](https://amyma101.medium.com/?source=post_page-----1bb171e43b35--------------------------------)[](https://towardsdatascience.com/?source=post_page-----1bb171e43b35--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----1bb171e43b35--------------------------------)
    [Amy Ma](https://amyma101.medium.com/?source=post_page-----1bb171e43b35--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://amyma101.medium.com/?source=post_page-----1bb171e43b35--------------------------------)[![Amy
    Ma](../Images/2edf55456a1f92724535a1441fa2bef5.png)](https://amyma101.medium.com/?source=post_page-----1bb171e43b35--------------------------------)[](https://towardsdatascience.com/?source=post_page-----1bb171e43b35--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----1bb171e43b35--------------------------------)
    [艾米·马](https://amyma101.medium.com/?source=post_page-----1bb171e43b35--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fd6d8df787b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcourage-to-learn-ml-unraveling-l1-l2-regularization-part-2-1bb171e43b35&user=Amy+Ma&userId=d6d8df787b&source=post_page-d6d8df787b----1bb171e43b35---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----1bb171e43b35--------------------------------)
    ·6 min read·Nov 25, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F1bb171e43b35&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcourage-to-learn-ml-unraveling-l1-l2-regularization-part-2-1bb171e43b35&user=Amy+Ma&userId=d6d8df787b&source=-----1bb171e43b35---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fd6d8df787b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcourage-to-learn-ml-unraveling-l1-l2-regularization-part-2-1bb171e43b35&user=Amy+Ma&userId=d6d8df787b&source=post_page-d6d8df787b----1bb171e43b35---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----1bb171e43b35--------------------------------)
    ·6分钟阅读·2023年11月25日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F1bb171e43b35&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcourage-to-learn-ml-unraveling-l1-l2-regularization-part-2-1bb171e43b35&user=Amy+Ma&userId=d6d8df787b&source=-----1bb171e43b35---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F1bb171e43b35&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcourage-to-learn-ml-unraveling-l1-l2-regularization-part-2-1bb171e43b35&source=-----1bb171e43b35---------------------bookmark_footer-----------)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F1bb171e43b35&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcourage-to-learn-ml-unraveling-l1-l2-regularization-part-2-1bb171e43b35&source=-----1bb171e43b35---------------------bookmark_footer-----------)'
- en: 'Welcome back to ‘[Courage to Learn ML](/towardsdatascience.com/tagged/courage-to-learn-ml):
    Demystifying L1 & L2 Regularization,’ Part Two. In [our previous discussion](https://medium.com/@yujing-ma45/understanding-l1-l2-regularization-part-1-9c7affe6f920),
    we explored the benefits of smaller coefficients and the means to attain them
    through weight penalization techniques. Now, in this follow-up, our mentor and
    learner will delve even deeper into the realm of L1 and L2 regularization.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '欢迎回到《[学习机器学习的勇气](/towardsdatascience.com/tagged/courage-to-learn-ml): 揭开L1和L2正则化的面纱》第二部分。在[我们之前的讨论](https://medium.com/@yujing-ma45/understanding-l1-l2-regularization-part-1-9c7affe6f920)中，我们探讨了较小系数的好处以及通过权重惩罚技术来实现这些系数的方法。在这次跟进中，我们的导师和学习者将**深入**探讨L1和L2正则化的领域。'
- en: 'If you’ve been pondering questions like these, you’re in the right place:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你曾在思考这些问题，那么你来对地方了：
- en: What’s the reason behind the names L1 and L2 regularization?
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: L1和L2正则化命名的原因是什么？
- en: How do we interpret the classic L1 and L2 regularization graph?
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们如何解读经典的L1和L2正则化图？
- en: What are Lagrange multipliers, and how can we understand them intuitively?
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lagrange 乘数是什么，我们如何直观理解它们？
- en: Applying Lagrange multipliers to comprehend L1 sparsity.
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应用 Lagrange 乘数理解 L1 稀疏性。
- en: Your engagement — likes, comments, and follows — does more than just boost morale;
    it powers our journey of discovery! So, let’s dive in.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 你们的参与 — 点赞、评论和关注 — 不仅仅是提升士气，更是推动我们探索之旅的动力！所以，让我们深入探讨吧。
- en: '![](../Images/eb38cadf6bb97d614c0c0bc8ddd9755a.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/eb38cadf6bb97d614c0c0bc8ddd9755a.png)'
- en: Photo by [Aarón Blanco Tejedor](https://unsplash.com/@the_meaning_of_love?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 照片由[Aarón Blanco Tejedor](https://unsplash.com/@the_meaning_of_love?utm_source=medium&utm_medium=referral)拍摄，来自[Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: Why they call L1, l2 regularization?
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为什么称之为 L1、L2 正则化？
- en: The name, L1 and L2 regularization, comes from the concept of Lp norms directly.
    Lp norms represent different ways to calculate distances from a point to the…
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 名称，L1 和 L2 正则化，直接来自 Lp 范数的概念。Lp 范数表示从一个点到另一点的距离的不同计算方式…
