- en: The ABCs of Differential Privacy
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 差分隐私的ABC
- en: 原文：[https://towardsdatascience.com/abcs-of-differential-privacy-8dc709a3a6b3](https://towardsdatascience.com/abcs-of-differential-privacy-8dc709a3a6b3)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/abcs-of-differential-privacy-8dc709a3a6b3](https://towardsdatascience.com/abcs-of-differential-privacy-8dc709a3a6b3)
- en: MASTERING BASICS
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 掌握基础知识
- en: A Guide to Understanding Fundamental Definitions and Key Principles
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解基本定义和关键原则的指南
- en: '[](https://medium.ealizadeh.com/?source=post_page-----8dc709a3a6b3--------------------------------)[![Essi
    Alizadeh](../Images/be2244231732f93bcadf09682ef8ca37.png)](https://medium.ealizadeh.com/?source=post_page-----8dc709a3a6b3--------------------------------)[](https://towardsdatascience.com/?source=post_page-----8dc709a3a6b3--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----8dc709a3a6b3--------------------------------)
    [Essi Alizadeh](https://medium.ealizadeh.com/?source=post_page-----8dc709a3a6b3--------------------------------)'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.ealizadeh.com/?source=post_page-----8dc709a3a6b3--------------------------------)[![Essi
    Alizadeh](../Images/be2244231732f93bcadf09682ef8ca37.png)](https://medium.ealizadeh.com/?source=post_page-----8dc709a3a6b3--------------------------------)[](https://towardsdatascience.com/?source=post_page-----8dc709a3a6b3--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----8dc709a3a6b3--------------------------------)
    [Essi Alizadeh](https://medium.ealizadeh.com/?source=post_page-----8dc709a3a6b3--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----8dc709a3a6b3--------------------------------)
    ·8 min read·Apr 27, 2023
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----8dc709a3a6b3--------------------------------)
    ·阅读时间8分钟·2023年4月27日
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/a9c9c860a5cbeead7b567f365584a0b4.png)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a9c9c860a5cbeead7b567f365584a0b4.png)'
- en: Image by Author
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图像
- en: Differential privacy (DP) is a rigorous mathematical framework that permits
    the analysis and manipulation of sensitive data while providing robust privacy
    guarantees.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 差分隐私（DP）是一个严谨的数学框架，允许对敏感数据进行分析和处理，同时提供强有力的隐私保障。
- en: DP is based on the premise that the inclusion or exclusion of a single individual
    should not significantly change the results of any analysis or query carried out
    on the dataset as a whole. In other words, the algorithm should come up with comparable
    findings when comparing these two sets of data, making it difficult to figure
    out anything distinctive about that individual. This safety keeps private information
    from getting out but still lets useful insights be drawn from the data.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: DP的基础是假设一个个体的加入或排除不应该显著改变对整个数据集进行的任何分析或查询的结果。换句话说，算法在比较这两组数据时应该得出相似的发现，从而使得难以识别该个体的任何独特信息。这种安全性可以防止私人信息泄露，但仍然可以从数据中得出有用的见解。
- en: Differential privacy initially appeared in the study “Differential Privacy”
    by Cynthia Dwork [1] while she was working at Microsoft Research.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 差分隐私最初出现在Cynthia Dwork在微软研究院工作期间的研究《Differential Privacy》中[1]。
- en: Let’s take a look at an example to better understand how differential privacy
    helps to protect data.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一个例子，更好地理解差分隐私如何帮助保护数据。
- en: Examples of How Differential Privacy Safeguards Data
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 差分隐私如何保护数据的示例
- en: Example 1
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 示例1
- en: In a study that looks at the link between social class and health results, researchers
    ask subjects for private information like where they live, how much money they
    have, and their medical background [2].
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在一项研究中，研究人员调查了社会阶层与健康结果之间的关系，要求受试者提供私人信息，如居住地、财富状况和医疗背景[2]。
- en: John, one of the participants, is worried that his personal information could
    get out and hurt his applications for life insurance or a mortgage. To make sure
    that John’s worries are taken care of, the researchers can use differential privacy.
    This makes sure that any data that is shared won’t reveal specific information
    about him. Different levels of privacy can be shown by John’s “opt-out” situation,
    in which his data is left out of the study. This protects his anonymity because
    the analysis's results are not tied to any of his personal details.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 参与者之一John担心他的个人信息可能会泄露，影响他申请人寿保险或抵押贷款。为了确保John的担忧得到解决，研究人员可以使用差分隐私。这确保了任何共享的数据不会透露关于他的具体信息。不同级别的隐私可以通过John的“退出”情况来展示，在这种情况下，他的数据被排除在研究之外。这保护了他的匿名性，因为分析的结果与他的个人细节无关。
- en: Differential privacy seeks to protect privacy in the real world as if the data
    were being looked at in an opt-out situation. Since John’s data is not part of
    the computation, the results regarding him can only be as accurate as the data
    available to everyone else.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 差分隐私旨在保护现实世界中的隐私，就好像数据是在选择退出的情况下查看的一样。由于John的数据不参与计算，因此关于他的结果只能与其他人可用的数据一样准确。
- en: A precise description of differential privacy requires formal mathematical language
    and technical concepts, but the basic concept is to protect the privacy of individuals
    by limiting the information that can be obtained about them from the released
    data, thereby ensuring that their sensitive information remains private.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 差分隐私的准确描述需要正式的数学语言和技术概念，但基本概念是通过限制从发布的数据中可以获取的个人信息量来保护个人隐私，从而确保其敏感信息保持私密。
- en: Example 2
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 示例2
- en: The U.S. Census Bureau used a differential privacy framework as a part of its
    disclosure avoidance strategy to strike a compromise between the data collection
    and reporting needs and the privacy concerns of the respondents. You can find
    more information about the confidentiality protection provided by the U.S. Census
    Bureau [here](https://www.census.gov/library/working-papers/2022/adrm/CED-WP-2022-003.html).
    Moreover, Garfinkel provides an explanation of how DP was utilized in the 2020
    US Census data [here](https://mit-serc.pubpub.org/pub/differential-privacy-2020-us-census).
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 美国普查局使用了差分隐私框架作为其披露避免策略的一部分，以在数据收集和报告需求与受访者隐私关切之间找到妥协。有关美国普查局提供的保密保护的更多信息，请访问[这里](https://www.census.gov/library/working-papers/2022/adrm/CED-WP-2022-003.html)。此外，Garfinkel提供了有关差分隐私在2020年美国人口普查数据中应用的解释，请见[这里](https://mit-serc.pubpub.org/pub/differential-privacy-2020-us-census)。
- en: Definition and key concepts
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 定义和关键概念
- en: The meaning of “differential” within the realm of DP
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: “差分隐私”在DP领域中的含义
- en: The term “differential” privacy refers to its emphasis on the dissimilarity
    between the results produced by a privacy-preserving algorithm on two datasets
    that differ by just one individual’s data.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: “差分”隐私这个术语指的是其对隐私保护算法在两个数据集上的结果差异的强调，这两个数据集之间仅有一个个体的数据不同。
- en: Mechanism M
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 机制 M
- en: A *mechanism* *M* is a mathematical method or process that is used on the data
    to make sure privacy is maintained while still giving useful information.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 一个*机制* *M* 是一种数学方法或过程，用于处理数据以确保在提供有用信息的同时维护隐私。
- en: Epsilon (ε)
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Epsilon (ε)
- en: ε is a privacy parameter that controls the level of privacy given by a differentially
    private mechanism. In other words, ε regulates how much the output of the mechanism
    can vary between two neighboring databases and measures how much privacy is lost
    when the mechanism is run on the database [3].
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: ε是一个隐私参数，它控制由差分隐私机制提供的隐私级别。换句话说，ε调节机制的输出在两个相邻数据库之间的变化程度，并衡量当机制在数据库上运行时隐私丧失的程度[3]。
- en: Stronger privacy guarantees are provided by a smaller ε, but the output may
    be less useful as a result [4]. εcontrols the amount of noise added to the data
    and shows how much the output probability distribution can change when the data
    of a single person is altered.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 较小的ε提供了更强的隐私保障，但结果可能因此变得不那么有用[4]。ε控制添加到数据中的噪声量，并显示当一个人的数据被更改时，输出概率分布可以发生的变化。
- en: Delta (𝛿)
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Delta (𝛿)
- en: 𝛿 is an extra privacy option that lets you set how likely it is that your privacy
    will be compromised. Hence, 𝛿 controls the probability of an extreme privacy breach,
    where the added noise (controlled by ε) does not provide sufficient protection.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 𝛿是一个额外的隐私选项，它允许你设定隐私被泄露的可能性。因此，𝛿控制了极端隐私泄露的概率，即添加的噪声（由ε控制）未能提供足够的保护。
- en: 𝛿 is a non-negative number that measures the chance of a data breach. It is
    usually very small and close to zero. This change makes it easier to do more complicated
    studies and machine learning models while still protecting privacy (see [4]).
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 𝛿是一个非负数，用于衡量数据泄露的可能性。它通常非常小且接近于零。这个变化使得在仍然保护隐私的情况下，进行更复杂的研究和机器学习模型变得更加容易（参见[4]）。
- en: If 𝛿 is low, there is less of a chance that someone’s privacy is going to get
    compromised. But this comes at a cost. If 𝛿 is too small, more noise might be
    introduced into the data, diminishing the quality of the end-result. 𝛿 is one
    parameter to consider, but it must be balanced with epsilon and the data’s practicality.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 如果𝛿很低，那么某人的隐私被泄露的机会较小。但这有代价。如果𝛿太小，可能会引入更多噪声，降低最终结果的质量。𝛿 是一个需要考虑的参数，但必须与 epsilon
    和数据的实用性进行平衡。
- en: Unveiling the Mathematics Behind Differential Privacy
  id: totrans-33
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 揭示差分隐私背后的数学
- en: Consider two databases, D and D', that differ by only one record.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑两个数据库 D 和 D'，它们只因一条记录不同。
- en: 'Formally, a mechanism M is ε-differentially private if, for any two adjacent
    datasets D and D’, and for any possible output O, the following holds:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 从形式上讲，如果对于任何两个相邻的数据集 D 和 D’以及任何可能的输出 O，以下条件成立，则机制 M 是 ε-差分隐私的：
- en: '![](../Images/2180335699dc30ba27c1a4213a95dfce.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2180335699dc30ba27c1a4213a95dfce.png)'
- en: 'However, we can reframe the above equation in terms of divergences, resulting
    in the following:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我们可以将上述方程重新表述为发散量，从而得到以下结果：
- en: '![](../Images/5494a4e9e80abb08a64a578400c1e365.png)![](../Images/7c92b5ab70004d237615fc4579c6ba6c.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5494a4e9e80abb08a64a578400c1e365.png)![](../Images/7c92b5ab70004d237615fc4579c6ba6c.png)'
- en: 'Figure 1: Differential privacy in the context of divergences (Image by Author).'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：差分隐私在发散量背景下的表现（图像由作者提供）。
- en: Here **div[⋅∣∣⋅]** denotes the Rényi divergence. See the paper [Renyi Differential
    Privacy](https://arxiv.org/abs/1702.07476) by Ilya Mironov for more information.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 这里**div[⋅∣∣⋅]**表示 Rényi 发散量。有关更多信息，请参见 Ilya Mironov 的论文 [Rényi Differential
    Privacy](https://arxiv.org/abs/1702.07476)。
- en: (ε, 𝛿)-DP Definition
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: (ε, 𝛿)-DP 定义
- en: A randomized *M* is considered (ε, 𝛿)-differentially private if the probability
    of a significant privacy breach (i.e., a breach that would not occur under ε-differential
    privacy) is no more than 𝛿. More formally, a mechanism M is (ε, 𝛿)-differentially
    private if
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 随机化的*M* 被认为是 (ε, 𝛿)-差分隐私的，如果显著隐私泄露（即，在 ε-差分隐私下不会发生的泄露）的概率不超过𝛿。更正式地说，机制 M 是 (ε,
    𝛿)-差分隐私的，如果
- en: '![](../Images/ecfb021e6c59b32d894209bac8c92e90.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ecfb021e6c59b32d894209bac8c92e90.png)'
- en: If 𝛿 = 0, then (ε, 𝛿)-DP is reduced to a ε-DP.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 如果𝛿 = 0，则 (ε, 𝛿)-DP 将简化为 ε-DP。
- en: (ε, 𝛿)-DP mechanism may be thought of informally as ε-DP with a probability
    of 1 — 𝛿.
  id: totrans-45
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: (ε, 𝛿)-DP 机制可以非正式地看作是带有 1 — 𝛿 概率的 ε-DP。
- en: Properties of Differential Privacy
  id: totrans-46
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 差分隐私的属性
- en: 1\. Post-processing immunity
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1\. 后处理免疫
- en: The differentially private output can be subjected to any function or analysis,
    and the outcome will continue to uphold the original privacy assurances. For instance,
    if you apply a differentially private mechanism to a dataset and then take the
    average age of the individuals in the dataset, the resulting average age will
    still be differentially private and will provide the same level of privacy assurances
    as the output it was originally designed to provide.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 不同ially 私密的输出可以受到任何函数或分析的影响，结果仍将保持原始的隐私保障。例如，如果你对数据集应用一个不同ially 私密的机制，然后计算数据集中个体的平均年龄，那么得到的平均年龄仍然是不同ially
    私密的，并将提供与其最初设计时相同水平的隐私保障。
- en: Thanks to the post-processing feature, we can use differentially private mechanisms
    in the same way as generic ones. Hence, it is possible to combine several differentially
    private mechanisms without sacrificing the integrity of differential privacy.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 多亏了后处理功能，我们可以像使用通用机制一样使用不同ially 私密的机制。因此，可以在不牺牲差分隐私完整性的情况下，结合多种不同ially 私密的机制。
- en: 2\. Composition
  id: totrans-50
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2\. 组合
- en: When multiple differentially private techniques are used on the same data or
    when queries are combined, composition is the property that ensures the privacy
    guarantees of differential privacy still apply. Composition can be either sequential
    or parallel. If you apply two mechanisms, *M1* with ε1-DP and *M2* with ε2-DP
    on a dataset, then the composition of *M1* and *M2* is at least (ε1 + ε2)-DP.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 当对相同的数据使用多种不同ially 私密技术或将查询组合时，组合是确保差分隐私的隐私保障仍然适用的属性。组合可以是顺序的或并行的。如果你在数据集上应用两个机制，*M1*
    具有 ε1-DP 和 *M2* 具有 ε2-DP，则 *M1* 和 *M2* 的组合至少是 (ε1 + ε2)-DP。
- en: 'WARNING: Despite composition’s ability to protect privacy, the composition
    theorem makes clear that there is a ceiling; as the value of ε rises, so does
    the amount of privacy lost whenever a new mechanism is employed. If ε becomes
    too large, then differential privacy guarantees are mostly meaningless [3].'
  id: totrans-52
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 警告：尽管组合具有保护隐私的能力，但组合定理明确表明存在上限；随着ε值的增加，每当使用新机制时，隐私损失的量也会增加。如果ε变得过大，那么差分隐私保证大多是无意义的[3]。
- en: '3\. Robustness to auxiliary information:'
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3\. 对辅助信息的鲁棒性：
- en: Differential privacy is resistant to auxiliary information attackers, which
    means that even if an attacker has access to other relevant data, they will not
    be able to learn anything about a person from a DP output. For instance, if a
    hospital were to share differentially private information regarding individuals’
    medical situations, an attacker with access to other medical records would not
    be able to greatly increase their knowledge of a given patient from the published
    numbers.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 差分隐私对辅助信息攻击者具有抵抗力，这意味着即使攻击者可以访问其他相关数据，他们也无法从差分隐私输出中得知有关某个人的信息。例如，如果医院分享有关个人医疗情况的差分隐私信息，攻击者即使拥有其他医疗记录，也无法通过发布的数字显著增加对某个患者的了解。
- en: Common Misunderstandings
  id: totrans-55
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 常见误解
- en: 'The notion of differential privacy has been misunderstood in several publications,
    especially during its early days. Dwork *et al.* wrote a short paper [5] to correct
    some widespread misunderstandings. Here are a few examples of common misunderstandings:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 差分隐私的概念在早期的几篇出版物中被误解了，特别是在其早期阶段。Dwork *et al.* 撰写了一篇简短的论文[5]来纠正一些广泛存在的误解。以下是一些常见误解的例子：
- en: DP is not an algorithm but rather a definition. DP is a mathematical guarantee
    that an algorithm must meet in order to disclose statistics about a dataset. Several
    distinct algorithms meet the criteria.
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 差分隐私不是一种算法，而是一种定义。差分隐私是一个数学保证，算法必须满足该保证才能公开数据集的统计信息。有多个不同的算法符合这一标准。
- en: Various algorithms can be differentialy private while still meeting various
    requirements. If someone claims that differential privacy, a specific requirement
    on ratios of probability distributions, is incompatible with any accuracy target,
    they must provide evidence for that claim. This means proving that there is no
    way a DP algorithm can perform to some specified standard. It’s challenging to
    come up with that proof, and our first guesses about what is and isn’t feasible
    are often off.
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 各种算法可以是差分隐私的，同时仍然满足不同的要求。如果有人声称差分隐私，即对概率分布的特定要求，与任何准确度目标不兼容，他们必须提供证据来支持这一主张。这意味着要证明没有任何差分隐私算法能够达到某个指定的标准。这种证明具有挑战性，我们对什么是可行的首个猜测往往不准确。
- en: There are no “good” or “bad” results for any given database. Generating the
    outputs in a way that preserves privacy (perfect or differential) is the key.
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对任何给定的数据库，没有“好”或“坏”的结果。以保护隐私的方式生成输出（无论是完美隐私还是差分隐私）是关键。
- en: Conclusion
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: DP has shown itself as a viable paradigm for the protection of data privacy,
    which is particularly important in this day and age, when machine learning and
    big data are becoming more widespread. Several key concepts were covered in this
    essay, including the various DP control settings like ε and *δ*. In addition,
    we provided several mathematical definitions of the DP. We also explained key
    features of the DP and addressed some of the most common misconceptions.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 差分隐私（DP）已被证明是一种可行的数据隐私保护范式，这在当今机器学习和大数据日益普及的时代尤为重要。本文涵盖了几个关键概念，包括各种差分隐私控制设置如ε和*δ*。此外，我们还提供了差分隐私的几个数学定义，解释了差分隐私的关键特性，并解决了一些最常见的误解。
- en: References
  id: totrans-62
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] Dwork, Cynthia (2006). “Differential Privacy.” In *Proceedings of the 33rd
    International Colloquium on Automata, Languages and Programming*, 1–12\. Berlin,
    Heidelberg: Springer Berlin Heidelberg. [https://doi.org/10.1007/11787006_1](https://doi.org/10.1007/11787006_1).'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] Dwork, Cynthia (2006). “Differential Privacy.” In *Proceedings of the 33rd
    International Colloquium on Automata, Languages and Programming*, 1–12\. Berlin,
    Heidelberg: Springer Berlin Heidelberg. [https://doi.org/10.1007/11787006_1](https://doi.org/10.1007/11787006_1).'
- en: '[2] Wood, Alexandra, Micah Altman, Aaron Bembenek, Mark Bun, Marco Gaboardi,
    James Honaker, Kobbi Nissim, David O’Brien, Thomas Steinke, and Salil Vadhan (2018).
    “Differential Privacy: A Primer for a Non-Technical Audience.” *Vand. J. Ent.
    & Tech. L.* 21 (1): 209–76\. [https://doi.org/10.2139/ssrn.3338027](https://doi.org/10.2139/ssrn.3338027).'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] Wood, Alexandra, Micah Altman, Aaron Bembenek, Mark Bun, Marco Gaboardi,
    James Honaker, Kobbi Nissim, David O’Brien, Thomas Steinke 和 Salil Vadhan (2018).
    “差分隐私: 为非技术受众准备的入门指南。” *Vand. J. Ent. & Tech. L.* 21 (1): 209–76。 [https://doi.org/10.2139/ssrn.3338027](https://doi.org/10.2139/ssrn.3338027)。'
- en: '[3] Brubaker, M., and S. Prince (2021). “Tutorial #12: Differential Privacy
    I: Introduction.” *Borealis AI*. [https://www.borealisai.com/research-blogs/tutorial-12-differential-privacy-i-introduction/](https://www.borealisai.com/research-blogs/tutorial-12-differential-privacy-i-introduction/).'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] Brubaker, M., 和 S. Prince (2021). “教程 #12: 差分隐私 I: 介绍。” *Borealis AI*.
    [https://www.borealisai.com/research-blogs/tutorial-12-differential-privacy-i-introduction/](https://www.borealisai.com/research-blogs/tutorial-12-differential-privacy-i-introduction/)。'
- en: '[4] Dwork, Cynthia, Aaron Roth, et al. (2014). “The Algorithmic Foundations
    of Differential Privacy.” *Foundations and Trends in Theoretical Computer Science*
    9 (3–4): 211–407.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] Dwork, Cynthia, Aaron Roth 等. (2014). “差分隐私的算法基础。” *理论计算机科学基础与趋势* 9 (3–4):
    211–407。'
- en: '[5] Dwork, Cynthia, Frank McSherry, Kobbi Nissim, and Adam Smith. 2011\. “Differential
    Privacy — A Primer for the Perplexed.” *Joint UNECE/Eurostat Work Session on Statistical
    Data Confidentiality* 11.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '[5] Dwork, Cynthia, Frank McSherry, Kobbi Nissim 和 Adam Smith. 2011. “差分隐私
    — 为困惑者准备的入门指南。” *联合国欧洲经济委员会/欧洲统计局统计数据保密工作会议* 11。'
- en: '*Originally published at* [*https://ealizadeh.com*](https://ealizadeh.com/blog/abc-of-differential-privacy/)
    *on April 27, 2023.*'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '*最初发布于* [*https://ealizadeh.com*](https://ealizadeh.com/blog/abc-of-differential-privacy/)
    *于2023年4月27日。*'
