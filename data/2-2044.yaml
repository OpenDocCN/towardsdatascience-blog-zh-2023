- en: The Only Guide You Need to Understand Regression Trees
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解回归树所需的唯一指南
- en: 原文：[https://towardsdatascience.com/the-only-guide-you-need-to-understand-regression-trees-4964992a07a8](https://towardsdatascience.com/the-only-guide-you-need-to-understand-regression-trees-4964992a07a8)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/the-only-guide-you-need-to-understand-regression-trees-4964992a07a8](https://towardsdatascience.com/the-only-guide-you-need-to-understand-regression-trees-4964992a07a8)
- en: A Complete Guide to Decision Trees with a Step-by-Step Implementation from Scratch
    and Hands-On Example Using Scikit-Learn
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 关于决策树的完整指南，包括从零开始的逐步实现和使用Scikit-Learn的动手示例
- en: '[](https://dmnkplzr.medium.com/?source=post_page-----4964992a07a8--------------------------------)[![Dominik
    Polzer](../Images/7e48cd15df31a0ab961391c0d57521de.png)](https://dmnkplzr.medium.com/?source=post_page-----4964992a07a8--------------------------------)[](https://towardsdatascience.com/?source=post_page-----4964992a07a8--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----4964992a07a8--------------------------------)
    [Dominik Polzer](https://dmnkplzr.medium.com/?source=post_page-----4964992a07a8--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://dmnkplzr.medium.com/?source=post_page-----4964992a07a8--------------------------------)[![多米尼克·波尔策](../Images/7e48cd15df31a0ab961391c0d57521de.png)](https://dmnkplzr.medium.com/?source=post_page-----4964992a07a8--------------------------------)[](https://towardsdatascience.com/?source=post_page-----4964992a07a8--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----4964992a07a8--------------------------------)
    [多米尼克·波尔策](https://dmnkplzr.medium.com/?source=post_page-----4964992a07a8--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----4964992a07a8--------------------------------)
    ·25 min read·Apr 4, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----4964992a07a8--------------------------------)
    ·阅读时长25分钟·2023年4月4日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/4c91b3953a804aad1f6ca79fb81508c9.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4c91b3953a804aad1f6ca79fb81508c9.png)'
- en: Build a tree - Image by the author
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 构建一棵树 - 作者提供的图像
- en: Table of Content
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 目录
- en: '[Introduction](#b3c0)'
  id: totrans-9
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[简介](#b3c0)'
- en: '[Decision trees for regression: the theory behind them](#8935)'
  id: totrans-10
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[回归决策树：背后的理论](#8935)'
- en: '[From theory to practice — Decision Trees from scratch](#6edb)'
  id: totrans-11
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[从理论到实践——从零开始的决策树](#6edb)'
- en: '[Hands-On Example — Implementation from scratch vs. Scikit-learn DecisionTree](#72e7)'
  id: totrans-12
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[动手示例——从零开始实现与Scikit-learn决策树对比](#72e7)'
- en: '[Summary](#0ce8)'
  id: totrans-13
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[总结](#0ce8)'
- en: '[References](#9542)'
  id: totrans-14
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[参考文献](#9542)'
- en: '[Appendix / Code](#e952)'
  id: totrans-15
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[附录 / 代码](#e952)'
- en: 1\. Introduction
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 1. 简介
- en: Decision Trees have been around since the 1960s. Despite being one of the simplest
    Machine Learning algorithms, they have proven to be highly effective in solving
    problems. One of their greatest advantages is their ease of interpretation, making
    them highly accessible to those without a technical background. In many industries,
    Data Scientists still have to build trust for Machine Learning use cases. Explainable
    baseline models like Decision Trees can help reduce the skepticism somewhat. If
    someone wanted to make the effort, they could even trace the branches of the learned
    tree and try to find patterns they already know about the problem.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树自1960年代以来一直存在。尽管它们是最简单的机器学习算法之一，但在解决问题时被证明非常有效。它们最大的优势之一是易于解释，使得那些没有技术背景的人也能轻松理解。在许多行业中，数据科学家仍需建立对机器学习用例的信任。像决策树这样的可解释基准模型可以帮助减少一些怀疑。如果有人愿意付出努力，他们甚至可以追踪学到的树的分支，并尝试找到他们已经知道的关于问题的模式。
- en: On the other hand, we quickly reach the limits of simple decision trees for
    complex problems. Theoretically, we can model any (complex) distribution of the
    data with appropriately sized trees, but the models often do not generalize well
    enough when applied to new data — they overfit the train dataset. Yet, decision
    trees have always played an important role in machine learning.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，我们很快就会遇到简单决策树在复杂问题上的极限。从理论上讲，我们可以用适当大小的树来建模任何（复杂的）数据分布，但这些模型在应用于新数据时往往无法很好地泛化——它们对训练数据集过拟合。然而，决策树在机器学习中始终发挥着重要作用。
- en: Some weaknesses of Decision Trees have been gradually solved or at least mitigated
    over time by the progress made with Tree Ensembles. In Tree Ensembles, we do not
    learn one decision tree, but a whole series of trees and finally combine them
    into an ensemble. Nowadays we distinguish between bagging and boosting algorithms.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树的一些弱点已经随着树集成技术的进步逐渐得到解决或至少得到缓解。在树集成中，我们不是学习一棵决策树，而是学习一系列树，并最终将它们组合成一个集成。如今我们区分了包装（bagging）和提升（boosting）算法。
- en: In Bagging, multiple decision trees are trained on different bootstrap samples
    (randomly selected subsets with replacement) of the training data. Each decision
    tree is trained independently, and the final prediction is made by averaging the
    predictions of all the individual trees. The **bagging** approach and in particular
    the **Random Forest** algorithm was developed by Leo Breiman.
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在袋装中，多个决策树在不同的自助样本（随机选择的带有替代的子集）上进行训练。每棵决策树是独立训练的，最终的预测是通过平均所有个体树的预测来得出的。**袋装**方法，特别是**随机森林**算法是由Leo
    Breiman开发的。
- en: In Boosting, decision trees are trained sequentially, where each tree is trained
    to correct the errors made by the previous tree. The training data is weighted,
    with more weight given to the misclassified samples from the previous tree.
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在提升中，决策树是按顺序训练的，每棵树都被训练以纠正前一棵树所犯的错误。训练数据被加权，对前一棵树错误分类的样本给予更高的权重。
- en: Even if random forest still plays an important role, today it is mostly **boosting**
    algorithms that perform best in data science competitions and often outperform
    bagging methods. Among the best known boosting algorithms are **AdaBoost, XGBoost,
    LightGBM and CatBoost**. Since 2016, their growth in popularity has continued
    relentlessly.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管随机森林仍然发挥着重要作用，但今天主要是**提升**算法在数据科学竞赛中表现最佳，且常常优于袋装方法。最知名的提升算法包括**AdaBoost、XGBoost、LightGBM和CatBoost**。自2016年以来，它们的受欢迎程度持续增长。
- en: '![](../Images/5faf6e506a5d37c156699d3171a81381.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5faf6e506a5d37c156699d3171a81381.png)'
- en: Type of tree-based algorithms — Image by the author
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 基于树的算法类型 — 图片由作者提供
- en: 'While the concept of decision trees has been known and actively applied for
    several decades, boosting approaches are relatively "new" and only gradually gained
    importance with the release of XGBoost in 2014:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然决策树的概念已经被认识并积极应用了几十年，但提升方法相对“新颖”，只有在2014年XGBoost发布后才逐渐获得重要性。
- en: '![](../Images/f255246a04156f5e320db518fb9d9b8c.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f255246a04156f5e320db518fb9d9b8c.png)'
- en: The evolution of tree based algorithms — Image by the author (inspired by (Chow,
    2021; Swalin, 2019))
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 基于树的算法的演变 — 图片由作者提供（灵感来源于（Chow, 2021; Swalin, 2019））
- en: Just a few months after the initial release of the concept behind XGBoost, the
    Higgs Boson Challenge on Kaggle was won with it. XGBoost is based on a number
    of concepts that add up to an extremely effective algorithm. The core of XGBoost
    is of course, the principle of gradient boosting, but XGBoost is much more. XGBoost
    includes various optimization techniques, which makes XGBoost extremely efficient
    and fast in the training process. Especially for small to medium sized structured
    datasets, gradient boosting framerworks like XGBoost, LightGBM and CatBoost continue
    to play a mayor role.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在XGBoost概念首次发布后的几个月，Higgs Boson Challenge 在Kaggle上用它赢得了比赛。XGBoost基于许多概念，这些概念汇聚成一个极其有效的算法。XGBoost的核心当然是梯度提升原理，但XGBoost远不止于此。XGBoost包括各种优化技术，使得XGBoost在训练过程中极其高效和快速。特别是对于小到中等规模的结构化数据集，像XGBoost、LightGBM和CatBoost这样的梯度提升框架继续发挥重要作用。
- en: It is not just my opinion. A good indicator are Kaggle competitions and their
    winning solutions.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 这不仅仅是我的观点。一个好的指标是Kaggle竞赛及其获胜解决方案。
- en: In the article "The State of Competitive Machine Learning", [](https://www.google.com/url?q=http%3A%2F%2Fmlcontests.com%2F&sa=D&ust=1680021204969306&usg=AOvVaw2gAknvaBr5xG8QHfDVyQIM)
    [mlcontests.com](https://mlcontests.com/) evaluated more than 200 data competitions
    in 2022 on Kaggle and other competition platforms. According to the report, gradient-boosted
    decision trees (GBDT) are still the go-to approach for tabular data use cases
    in 2022 and could manage to win most of the competitions in this area. (Carlens,
    n.d.)
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在文章《竞争性机器学习的现状》中， [](https://www.google.com/url?q=http%3A%2F%2Fmlcontests.com%2F&sa=D&ust=1680021204969306&usg=AOvVaw2gAknvaBr5xG8QHfDVyQIM)
    [mlcontests.com](https://mlcontests.com/) 评估了2022年在Kaggle和其他竞赛平台上的200多个数据竞赛。根据报告，梯度提升决策树（GBDT）仍然是2022年表格数据用例的首选方法，并且能够赢得大多数此类领域的竞赛。（Carlens,
    n.d.）
- en: Apart from the good performance that gradient boosting algorithms are showing
    again and again, the biggest advantage of decision trees or tree ensembles is
    speed. In general, gradient-boosting frameworks are faster in training compared
    to NNs, which can be an important factor in many real-world problems.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 除了梯度提升算法一再展示出的良好性能外，决策树或树集成的最大优势是速度。一般而言，梯度提升框架在训练速度上比神经网络更快，这在许多现实世界的问题中可能是一个重要因素。
- en: Often the data set is not clear at the beginning of an ML project. A major part
    of the work is the compilation of the dataset and extraction of relevant features.
    If we change the dataset, add a new column, or just slightly change the way we
    convert categorical values to numerical ones, we need a measure of whether we
    have improved or worsened the overall process by doing so. In this process, we
    may train models several hundred times. A faster training time can thus decisively
    influence the time for the entire development process of ML use cases.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 通常在机器学习项目开始时数据集并不明确。工作的重要部分是数据集的汇编和相关特征的提取。如果我们更改数据集、添加新列或仅稍微更改将分类值转换为数值的方式，我们需要衡量这样做是否改善了整体过程。在这个过程中，我们可能会训练几百次模型。因此，更快的训练时间可以决定性地影响整个机器学习用例的开发过程时间。
- en: '![](../Images/d29ccfc4e9cb40d8de9e9d3aeb81bc3c.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d29ccfc4e9cb40d8de9e9d3aeb81bc3c.png)'
- en: ML projects are iterative not linear — Image by the author
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习项目是迭代的，而不是线性的 — 图片来源：作者
- en: The following figure shows the individual steps along the ML pipeline. If we
    change one small thing in the process before we train the model, we have to re-evaluate
    the whole process and the resulting models.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 下图显示了机器学习管道中的各个步骤。如果我们在训练模型之前对过程中的某个小细节进行更改，我们必须重新评估整个过程和结果模型。
- en: '![](../Images/0ac161e3da9040b785648e2458ac3421.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0ac161e3da9040b785648e2458ac3421.png)'
- en: ML pipeline — Image by the author
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习管道 — 图片来源：作者
- en: '**Content of the article:**'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '**文章内容：**'
- en: This article is intended to lay the foundation to dive into various types of
    tree-based ensemble algorithms which are all based on Decision Trees. The concept
    of decision trees is very intuitive and easy to understand. At first glance somewhat
    more complex are XGBoost, CatBoostc and LightGBM. But if you take a closer look,
    XGBoost is just a combination of different concepts, which are again easy to understand
    each by itself.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 本文旨在奠定基础，深入探讨各种基于决策树的树型集成算法。决策树的概念非常直观，易于理解。乍一看，XGBoost、CatBoost 和 LightGBM
    看起来复杂一些。但如果你仔细观察，XGBoost 只是不同概念的组合，而这些概念本身又很容易理解。
- en: Once you understand the random forest and gradient boosting frameworks, you
    will be able to solve a wide range of data science problems. From classifications
    to regression to anomaly detection.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你理解了随机森林和梯度提升框架，你将能够解决各种数据科学问题。从分类到回归再到异常检测。
- en: It's kind of absurd that knowledge about Deep Learning frameworks like Pytorch,
    TensorFlow, and Co plays such a central role in almost every Data Science job
    posting. In many areas, you will spend most of your time collecting data, preparing
    data, and extracting features. If you have the right feature set, the model creation
    itself is often quite straightforward. If you deal mainly with tabular data, you
    will probably get quite far with bagging and boosting algorithms.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 知识关于深度学习框架如 Pytorch、TensorFlow 等在几乎所有数据科学职位中扮演如此核心的角色，这有些荒谬。在许多领域，你将花费大部分时间来收集数据、准备数据和提取特征。如果你有了合适的特征集，模型的创建本身通常是相当简单的。如果你主要处理表格数据，你可能会通过袋装和提升算法走得很远。
- en: If you want to download the code used in the article all at once and use it
    for reference, you can find the code snippets used on [github](https://github.com/polzerdo55862/decision-tree-from-scratch).
    You can also find the code for the decision tree algorithm that we will build
    in this article in the appendix, at the bottom of this article.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想一次性下载文章中使用的代码并作为参考，你可以在 [github](https://github.com/polzerdo55862/decision-tree-from-scratch)
    上找到使用的代码片段。你也可以在附录中找到我们将在本文中构建的决策树算法的代码，位于文章底部。
- en: '2\. Decision Trees for Regression: The theory behind it'
  id: totrans-43
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 2\. 回归的决策树：背后的理论
- en: Decision trees are among the simplest machine learning algorithms. The way they
    work is relatively easy to explain.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树是最简单的机器学习算法之一。它们的工作方式相对容易解释。
- en: We, as humans, try to solve complex problems by breaking them down into relatively
    simple yes or no decisions. When we want to buy a new car, we browse all the car
    websites we can find. After a while, we get a feeling for how much a certain car
    make and model should cost. We get a feeling for how big the cost difference is
    between luxury brands and cheaper manufacturers and how much more we have to pay
    for a 150 hp engine compared to a smaller 100 hp engine and so on.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 作为人类，我们尝试通过将复杂问题分解为相对简单的是或否决策来解决问题。当我们想买一辆新车时，我们浏览所有能找到的汽车网站。过一段时间后，我们对某款车的价格有了感觉。我们感觉到奢侈品牌和便宜制造商之间的成本差异有多大，以及与较小的100
    hp引擎相比，150 hp引擎的额外费用有多少，等等。
- en: Step by step, our brain memorizes certain ballpark values for certain combinations
    of features. When we then stop at a car dealer and go through the features of
    a car one by one, it is as if we are moving down a decision tree until we arrive
    at what we think is a fair price.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 一步步地，我们的大脑记住了某些特征组合的球park值。当我们停在汽车经销商那里，逐一查看汽车的特征时，就像是在决策树上向下移动，直到我们认为得到了一个公平的价格。
- en: '![](../Images/82efa6269742ac738614158a1bea8b91.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/82efa6269742ac738614158a1bea8b91.png)'
- en: A simple Regression Tree that predicts car prices — Image by the author
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 一个简单的回归树预测汽车价格 — 图像由作者提供
- en: '***Note:*** *Before we go into how Decision trees are built, it is important
    to mention that there are different Decision Tree algorithms. Some popular ones
    are ID3, C4.5, C5.0 and CART (Google Developers, 2017). Scikit-learns implementation
    is based on CART, which was first published in 1984 by Leo Breiman et al. Leo
    Breiman was an American statistician who shaped the approach of "bagging", developed
    the random forest and thus contributed significantly to the further development
    of tree-based algorithms.*'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '***注意：*** *在我们深入了解决策树的构建之前，需要提到有不同的决策树算法。一些流行的算法包括ID3、C4.5、C5.0和CART（Google
    Developers, 2017）。Scikit-learn的实现基于CART，该算法由Leo Breiman等人于1984年首次发布。Leo Breiman是一位美国统计学家，他塑造了“bagging”的方法，开发了随机森林，从而对基于树的算法的进一步发展做出了重要贡献。*'
- en: '**How do we start build the tree?**'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '**我们如何开始构建决策树？**'
- en: 'To start the Decision Tree building process, we need to answer three questions:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 要开始决策树构建过程，我们需要回答三个问题：
- en: '**Which feature do we start with?** - We split the data set at each node along
    one dimension. For the example, we use the feature x_1 for splitting. Since we
    don''t want to choose just random features, we search in advance for the feature
    where splitting the data set offers the greatest added value. (In this context
    we often speak about the so-called information gain. We can define the information
    gain in different ways, in regression we often use the *squared error*).'
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**我们从哪个特征开始？** - 我们在每个节点沿一个维度划分数据集。在这个例子中，我们使用特征x_1进行划分。由于我们不想仅仅选择随机特征，我们会提前搜索出划分数据集带来最大增益的特征。（在这种情况下，我们通常谈到所谓的信息增益。我们可以以不同的方式定义信息增益，在回归中我们通常使用*平方误差*）。'
- en: '**What is the best threshold to split the data?** - Similar to the first step,
    when we choose a feature, we still need to know the threshold we want to use to
    split the data set. So in other words, at what position along the dimension we
    want to split the data set.'
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**最佳的阈值来划分数据是什么？** - 类似于第一步，当我们选择一个特征时，我们仍然需要知道我们想用什么阈值来划分数据集。换句话说，就是在维度上的哪个位置我们想要划分数据集。'
- en: '**When do we stop splitting the data set?** - If we do not stop the splitting
    process at some point, the decision tree will continue until there is only one
    sample point in each leaf node. To avoid overfitting, we need some criteria to
    determine how far to split the data set and when to stop the splitting process
    so that the model does not become unnecessarily complex.'
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**我们什么时候停止划分数据集？** - 如果我们在某个点不停止划分过程，决策树将继续进行，直到每个叶子节点中只有一个样本点。为了避免过拟合，我们需要一些标准来决定划分数据集的深度以及何时停止划分过程，以避免模型变得不必要的复杂。'
- en: '![](../Images/065ee3908c081a52d5a95a63e112f58c.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/065ee3908c081a52d5a95a63e112f58c.png)'
- en: 3 questions we must answer — Image by the author
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 我们必须回答的三个问题 — 图像由作者提供
- en: Let's use the example of price prediction for cars again. First, we need to
    select one of the features and split the data set. We choose a feature and a threshold
    and split the dataset into a left and a right part and calculate the average price.
    That gives us a first node. If we stopped now, we would have a minimalistic decision
    tree with only one level - a so-called decision stump.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 我们再次使用汽车价格预测的例子。首先，我们需要选择一个特征并分割数据集。我们选择一个特征和一个阈值，将数据集分割成左侧和右侧部分，并计算平均价格。这给了我们第一个节点。如果我们现在停止，我们将拥有一个只有一个层级的简约决策树——所谓的决策桩。
- en: However, we do not want to start with a random split, but with the "best possible"
    split.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我们不想从随机分割开始，而是从“最佳可能”的分割开始。
- en: '**But how is the “best” split defined?**'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '**但是，如何定义“最佳”分割呢？**'
- en: We need to define a metrics, that helps us to evaluate how good a split performs.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要定义一个度量标准，帮助我们评估分割的效果。
- en: A often-used loss function in regression problems to assess how well a model
    performs is the **mean absolute error** or the **mean squared error**. Usually,
    we can choose between different metrics. To get an idea how scikit-learn is calculating
    the performance of each split, we can simply have a look into the documentation
    or directly in the source code.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在回归问题中，常用的损失函数有 **平均绝对误差** 或 **均方误差**。通常，我们可以在不同的度量标准之间进行选择。要了解 scikit-learn
    是如何计算每次分割的性能的，我们可以直接查看文档或源代码。
- en: The easiest way to access the source code is via the code editor.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 访问源代码的最简单方法是通过代码编辑器。
- en: 'If you have not yet installed scikit, you can do so with pip via:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你还没有安装 scikit，你可以通过以下 pip 命令进行安装：
- en: '[PRE0]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: I use Visual Studio Code for most of my projects. If I create a new notebook
    or Python file and import the corresponding modules, Visual Studio provides us
    a direct link to the source code behind it. In the picture on the left side, you
    can see how the whole thing looks like in Visual Studio Code.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 我在大多数项目中使用 Visual Studio Code。如果我创建一个新的笔记本或 Python 文件并导入相应的模块，Visual Studio
    会提供一个直接链接到其背后的源代码。在左侧的图片中，你可以看到 Visual Studio Code 中的整个情况。
- en: '![](../Images/bebe17511d12d3dee41c0768ed1f0356.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bebe17511d12d3dee41c0768ed1f0356.png)'
- en: Sceenshot by the author
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 作者截图
- en: Create a new file, in my case "**tree_algorithms.py**" and import the regression
    tree module "**sklearn.tree.DecisionTreeRegressor**".
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个新文件，在我的例子中是“**tree_algorithms.py**”，并导入回归树模块“**sklearn.tree.DecisionTreeRegressor**”。
- en: By pressing "**Ctrl**" and clicking on the according module you will be redirected
    directly to the corresponding part of the source code.
  id: totrans-69
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 按下“**Ctrl**”并点击相应模块，你将直接跳转到源代码的相应部分。
- en: Alternatively, you can find the source code in the documentation of scikit-learn.
    On the right you can see how the whole thing looks on *scikit-learn.org*. Each
    class and function has also a link to the source code on Github.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以在 scikit-learn 的文档中找到源代码。右侧可以看到在 *scikit-learn.org* 上的整个情况。每个类和函数还有一个指向
    Github 上源代码的链接。
- en: 'If we dive into the source code of DecisionTreeRegressor, we see that it is
    defined as a class that is initiated with the following values:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们深入到 DecisionTreeRegressor 的源代码中，我们会看到它被定义为一个用以下值初始化的类：
- en: '[PRE1]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: We will gradually go into what the individual hyperparameters do. What we are
    interested in for the start is the splitting criterion, i.e. how the decision
    tree determines how it splits the data set during building.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将逐渐深入了解每个超参数的作用。我们一开始感兴趣的是分割标准，即决策树在构建过程中如何确定如何分割数据集。
- en: '![](../Images/7cdac0f9233af5fa7f938241bb48d5eb.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7cdac0f9233af5fa7f938241bb48d5eb.png)'
- en: Sceenshot by the author
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 作者截图
- en: The code also contains a short description of which criterias we can select.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 代码中还包含了我们可以选择的标准的简短描述。
- en: 'Scikit-learn lets us choose between:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: Scikit-learn 让我们可以在以下选项中选择：
- en: squared_error
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: squared_error
- en: friedmann_mse
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: friedmann_mse
- en: aboslute_error
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 绝对误差
- en: poisson
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 泊松
- en: 'The default value is "squared_error". The documentation describes it as follows:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 默认值是“squared_error”。文档中将其描述如下：
- en: '*“squared_error” is equal to variance reduction and minimizes the L2 loss using
    the mean of each terminal node*'
  id: totrans-83
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*“squared_error” 等同于方差减少，并使用每个终端节点的均值来最小化 L2 损失*'
- en: So we are trying to minimize the *Mean Squared Error* in the terminal nodes.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们尝试在终端节点中最小化 *均方误差*。
- en: Let's imagine we have a simple two-dimensional dataset with only x_1 as the
    single input feature and y as the target variable. For this simple example, we
    don't need to decide which feature is best to split the dataset because we only
    have one in the dataset. So at the root node, we use x_1 to split the dataset
    in half.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有一个简单的二维数据集，只有 x_1 作为唯一的输入特征和 y 作为目标变量。对于这个简单的例子，我们不需要决定哪个特征最适合分割数据集，因为数据集中只有一个特征。因此，在根节点，我们使用
    x_1 将数据集分成两半。
- en: In the following figure, you can find a simple 2 dimensional dataset. The two
    halves of the dataset are our child nodes. At the moment we perform the first
    split, the two child nodes are the leaf nodes or terminal nodes (nodes that are
    not further split).
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 在下图中，您可以找到一个简单的二维数据集。数据集的两个部分是我们的子节点。在我们进行第一次分裂时，这两个子节点是叶节点或终端节点（不会进一步分裂的节点）。
- en: In the case shown, we divide the data set at x_1=2.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 在所示的情况下，我们在 x_1=2 处分割数据集。
- en: '**What value would the tree now predict if we used it to make prediction?**'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '**如果我们用它来进行预测，树现在会预测什么值？**'
- en: We have to define a value for each terminal node, which then represents the
    possible prediction values of the decision tree. We calculate this prediction
    value y_pred in the simplest way, we calculate the average of the data points
    in the left node (**here:** y_pred_left = 9) and the right node (**here:** y_pred_right
    = 5).
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 我们必须为每个终端节点定义一个值，这样就代表了决策树的可能预测值。我们以最简单的方式计算这个预测值 y_pred，我们计算左节点的平均值 (**这里：**
    y_pred_left = 9) 和右节点的平均值 (**这里：** y_pred_right = 5)。
- en: '![](../Images/5f44d0ed1c9312ea353eb85c46274d50.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5f44d0ed1c9312ea353eb85c46274d50.png)'
- en: Split the data set — Image by the author
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 分割数据集 — 图片由作者提供
- en: '**How do I find the best threshold for splitting the data set?**'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '**我如何找到最佳的分裂数据集的阈值？**'
- en: In the example shown, we have chosen x_1 = 2 as the threshold. But is this the
    optimal scenario?
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在所示的示例中，我们选择了 x_1 = 2 作为阈值。但这是否是最优情况？
- en: To evaluate the performance of the split, we calculate the residuals, i.e.,
    the difference between y_predict and the y for each sample. More precisely, we
    calculate the L2 loss function, the sum of squared residuals.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 为了评估分裂的性能，我们计算残差，即 y_predict 与每个样本的 y 之间的差异。更准确地说，我们计算 L2 损失函数，即残差的平方和。
- en: '![](../Images/e67e847ae54396e72a2a87b971520693.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e67e847ae54396e72a2a87b971520693.png)'
- en: How to calculate the prediction value for the left and right leaf — Image by
    the author
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 如何计算左叶子和右叶子的预测值 — 图片由作者提供
- en: To get a value for the performance of the stump, we calculate the deviations
    (l2 loss) for both sides separately and then calculate a weighted overall loss
    by including the number of samples in both halves.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 为了得到桩模型的性能值，我们分别计算两侧的偏差（l2 损失），然后通过包括两半样本的数量来计算加权总体损失。
- en: 'We do that over and over again, for different thresholds (see image). In our
    example, the weighted squared error gets minimal when we choose x_1 = 5 as the
    splitting threshold:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 我们反复进行这个过程，尝试不同的阈值（见图片）。在我们的例子中，当我们选择 x_1 = 5 作为分裂阈值时，加权平方误差最小：
- en: '![](../Images/28b23a80e1b1b4e7c2a14ebe07460772.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/28b23a80e1b1b4e7c2a14ebe07460772.png)'
- en: MSE calculation for parent and child nodes — Image by the author
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 父节点和子节点的 MSE 计算 — 图片由作者提供
- en: '**How does our algorithm find the smallest error?**'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '**我们的算法如何找到最小误差？**'
- en: The decision tree does this in a very simple way, it defines an iterative approach,
    that tries different values as thresholds. Therefore, we define a list of possible
    thresholds/splitting values and calculate the mean squared error for each of the
    possible thresholds in the list.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树以非常简单的方式执行这一操作，它定义了一种迭代方法，尝试不同的阈值。因此，我们定义了一个可能的阈值/分裂值列表，并计算列表中每个可能阈值的均方误差。
- en: '**Step 1 - Define a list with possible thresholds for the splitting:** We are
    defining all possible splitting values by sorting the values and calculating the
    rolling average. So if x_1 = 2 and x_2 = 3 then the first threshold in the list
    of possible thresholds is 2.5.'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**步骤 1 - 定义可能的分裂阈值列表：** 我们通过排序值并计算滚动平均值来定义所有可能的分裂值。因此，如果 x_1 = 2 和 x_2 = 3，那么可能阈值列表中的第一个阈值是
    2.5。'
- en: '**Step 2:** In the next step, we need to find the threshold that minimizes
    the squared error when building a node. We start iterating over all thresholds,
    splitting the data set, and calculating the MSE for the right and left nodes.'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**步骤 2：** 在下一步中，我们需要找到最小化平方误差的阈值来构建节点。我们开始迭代所有阈值，拆分数据集，并计算右节点和左节点的 MSE。'
- en: '![](../Images/b8acba7d78ed71a2b7d8400cbc68e7e5.png)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b8acba7d78ed71a2b7d8400cbc68e7e5.png)'
- en: Iterative calculation of Mean Squared Error for different thesholds — Image
    by the author
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 不同阈值下均方误差的迭代计算 — 图片由作者提供
- en: Lets try it with a real world data set.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们用实际数据集来尝试一下。
- en: Load a real world data set
  id: totrans-108
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 加载现实世界的数据集
- en: 'To demonstrate the steps just described on a real data set, we download the
    Automobile Data Set from UCIMachineLearning. The dataset includes a bunch of attributes
    like horsepower, dimensions, fuel type etc. which describe a car in detail. The
    target variable we are interested in is the price. [(](https://www.google.com/url?q=https%3A%2F%2Fwww.zotero.org%2Fgoogle-docs%2F%3FY7wEQt&sa=D&ust=1680021204980533&usg=AOvVaw3AthCMTcfOQu5mgN3xM1an)*UCI
    Machine Learning Repository: Automobile Data Set*, n.d.) *[License:* [*CC0: Public
    Domain*](https://creativecommons.org/publicdomain/zero/1.0/)*]*'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '为了在实际数据集上演示刚才描述的步骤，我们从UCIMachineLearning下载了汽车数据集。该数据集包含许多属性，如马力、尺寸、燃料类型等，用于详细描述汽车。我们感兴趣的目标变量是价格。[（](https://www.google.com/url?q=https%3A%2F%2Fwww.zotero.org%2Fgoogle-docs%2F%3FY7wEQt&sa=D&ust=1680021204980533&usg=AOvVaw3AthCMTcfOQu5mgN3xM1an)*UCI
    Machine Learning Repository: 汽车数据集*，无日期。）*[许可证：* [*CC0: 公共领域*](https://creativecommons.org/publicdomain/zero/1.0/)*]*'
- en: '[PRE2]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Afterwards, we perform exactly the steps just described. The following code
    snippet uses the selected feature **selected_feature** and the defined threshold
    **threshold** to split the data set (X_parent, y_parent).
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 之后，我们执行刚才描述的步骤。以下代码片段使用所选特征**selected_feature**和定义的阈值**threshold**来分割数据集（X_parent,
    y_parent）。
- en: The plot shows the samples of the left and right child nodes and the average
    of the observations. If we stopped now, the child nodes would be the leaf nodes
    of the tree and the predicted value of the tree would be represented by the calculated
    mean of the two halves.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 该图展示了左子节点和右子节点的样本及观察值的平均值。如果我们现在停止，子节点将成为树的叶子节点，树的预测值将由两个部分的计算均值表示。
- en: '[PRE3]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '![](../Images/a69dc5d600ff12c0e63645acb7ea1fdc.png)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a69dc5d600ff12c0e63645acb7ea1fdc.png)'
- en: Image by the author
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供
- en: Since we don't want to split the dataset anywhere, but at the "best" point,
    we do this iteratively as described above. We use the node plot class to calculate
    the residuals for a number of thresholds.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们不想在数据集中的任何地方进行分割，而是选择“最佳”点，因此我们按上述描述进行迭代。我们使用节点绘图类来计算多个阈值的残差。
- en: '[PRE4]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '![](../Images/ff243bc068186eb7b3324729231efc9e.png)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ff243bc068186eb7b3324729231efc9e.png)'
- en: Image by the author
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供
- en: We get the squared error of the parent and child nodes for each possible split.
    For decision trees, we are searching for the maximal **information gain**. Using
    the squared error as a splitting criterion, we calculate the information gain
    simply as the difference between the MSE of the parent node and the weighted MSE
    of the child nodes.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 我们计算每个可能分割的父节点和子节点的平方误差。对于决策树，我们寻求最大**信息增益**。使用平方误差作为分割标准，我们简单地将信息增益计算为父节点的MSE与子节点加权MSE之间的差异。
- en: In the chart, the Information Gain maximum lies at 120 hs.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 在图表中，信息增益的最大值出现在120小时。
- en: '![](../Images/6c9eb27909468b0d2a75e8db5f03ee65.png)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6c9eb27909468b0d2a75e8db5f03ee65.png)'
- en: Image by the author
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供
- en: In the Decision Tree, we perform the steps just described again and again.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 在决策树中，我们不断重复刚才描述的步骤。
- en: '*Note: The decision tree is counted among the greedy algorithms. Greedy algorithms
    gradually select the sequential state that promises the best result during selection.
    Previous and subsequent decisions are not taken into account. when making this
    decision.*'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '*注意：决策树被归类为贪心算法。贪心算法在选择过程中逐步选择承诺最佳结果的状态。做出决策时不考虑之前和之后的决策。*'
- en: '**When does the procedure end?**'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '**程序什么时候结束？**'
- en: Decision trees do not make many assumptions while training a model. Linear Regression,
    for example, is just the opposite, while the linear regression algorithm trains
    a model, it allows only one possible shape of the model, a straight line or a
    planar plane in space. Thus, when we use Linear Regression as a learning algorithm,
    we directly make the assumption that our problem follows a linear behavior.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树在训练模型时不会做很多假设。例如，线性回归恰恰相反，当线性回归算法训练模型时，它仅允许模型具有一种可能的形状，即空间中的直线或平面。因此，当我们使用线性回归作为学习算法时，我们直接假设我们的问题遵循线性行为。
- en: '![](../Images/f3ce75013d8d045de69aced1652fe9b5.png)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f3ce75013d8d045de69aced1652fe9b5.png)'
- en: Parametric (Linear Regression) vs. nonparametric model (Regression Tree) — Image
    by the author
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 参数模型（线性回归）与非参数模型（回归树）— 作者提供的图像
- en: Decision trees, on the other hand, are very flexible in their learning process.
    Such models are called "nonparametric models". Models are called non-parametric
    when their number of parameters is not determined in advance. Linear regression
    has a well-defined number of parameters, the slope and the offset. This significantly
    limits the degree of freedom in the training process. (Géron, 2022)
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树在学习过程中非常灵活。这些模型被称为“非参数模型”。当模型的参数数量没有提前确定时，就称为非参数模型。线性回归有一个明确定义的参数数量，即斜率和偏移量。这显著限制了训练过程中的自由度。（Géron，2022）
- en: Decision trees thus tend to overfit. To avoid that, we need to introduce hyperparameters
    that limit the freedom of the training process, so-called regularization hyperparameters.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，决策树往往容易过拟合。为了避免这种情况，我们需要引入限制训练过程自由度的超参数，即所谓的正则化超参数。
- en: 'A frequently used regularization parameter is **max_depth**, i.e. the maximum
    depth of the tree. Others are:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 一个常用的正则化参数是 **max_depth**，即树的最大深度。其他的包括：
- en: '**min_samples_split** (the minimum number of samples a node needs to be split)'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**min_samples_split**（节点需要的最小样本数，以便进行拆分）'
- en: '**min_samples_leaf** (the minimum number of samples each leaf node must have)'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**min_samples_leaf**（每个叶子节点必须具有的最小样本数）'
- en: '**min_weight_fraction_leaf** (similar to min_samples_leaf, but instead of a
    number we define a fraction of the whole dataset)'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**min_weight_fraction_leaf**（类似于 min_samples_leaf，但我们定义的是整个数据集的一个比例，而不是一个具体的数字）'
- en: '**max_leaf_nodes** (maximum number of leaf nodes)'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**max_leaf_nodes**（叶子节点的最大数量）'
- en: '**max_features** (the maximum number of features evaluated at each split).'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**max_features**（在每次分裂时评估的最大特征数量）。'
- en: After the actual model building, we can still prune the tree to avoid unnecessary
    complexity of the model.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 在实际模型构建后，我们仍然可以修剪树，以避免模型不必要的复杂性。
- en: '**What is pruning?**'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '**什么是剪枝？**'
- en: This technique involves growing the tree to its full size and then removing
    branches or subtrees that do not improve the accuracy of the tree on a validation
    dataset. This is done by calculating the change in error before and after pruning
    a subtree and comparing it to a threshold value. If the change in error is not
    significant, the subtree is pruned. I don’t want to go further into this for the
    moment, as I will leave it out for the following simple example.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 这项技术包括将树生长到其全部大小，然后删除那些对验证数据集的准确性没有提升的分支或子树。这是通过计算修剪子树前后的误差变化，并与一个阈值进行比较来完成的。如果误差变化不显著，则会修剪子树。由于我暂时不想深入探讨这个问题，我将在以下简单示例中省略它。
- en: In the following, I’ll show you how to build a basic version of a regression
    tree from scratch.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我将向你展示如何从头开始构建一个基本版本的回归树。
- en: 3\. From theory to practice - Decision Tree from Scratch
  id: totrans-142
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 3\. 从理论到实践 - 从头开始构建决策树
- en: To be able to use the regression tree in a flexible way, we put the code into
    a new module. We create a new Python file, where we put all the code concerning
    our algorithm and the learning process. In it, we define a new class called "RegressionTree"
    and initialize it with the hyperparameters that serve as constraints for the training
    process.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 为了灵活使用回归树，我们将代码放入一个新的模块中。我们创建一个新的 Python 文件，将所有与我们算法和学习过程相关的代码放入其中。在这个文件中，我们定义一个名为“RegressionTree”的新类，并用作为训练过程约束的超参数进行初始化。
- en: As mentioned earlier, one of the biggest challenges in working with decision
    trees is the risk of overfitting. To mitigate this risk and ensure that our model
    generalizes well to new data, we introduce regularisation parameters that guide
    and stop the learning process at a certain point.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，处理决策树的最大挑战之一是过拟合的风险。为了降低这一风险，并确保我们的模型能够很好地推广到新数据，我们引入了正则化参数，这些参数在某个点指导和停止学习过程。
- en: 'The regulation parameters (or stopping criteria) that we use in our simplified
    version of Decision Trees are the following two:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在简化版决策树中使用的正则化参数（或停止准则）有以下两个：
- en: '**min_samples_split**'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '**min_samples_split**'
- en: defines the maximum number of samples that a node needs in order to be split
    further. A suitable value depends on the type and size of the dataset. If chosen
    correctly, it can prevent overfitting. In scikit-learn, the default value is set
    to 2 samples.
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 定义一个节点需要的最大样本数量，以便进一步拆分。合适的值取决于数据集的类型和大小。如果选择得当，它可以防止过拟合。在 scikit-learn 中，默认值设置为
    2 个样本。
- en: '**max_depth**'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '**max_depth**'
- en: the maximum depth determines how many levels the tree can have at most. If another
    stopping criterion such as *min_sample_split* prevents further growth of the tree
    on all branches before this depth is reached, it may not even reach this tree
    size. Scikit-learn sets the default value to “None”, so the maximum depth is not
    limited by default.
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最大深度决定了树最多可以有多少层。如果其他停止准则如 *min_sample_split* 在达到此深度之前阻止了树的进一步生长，则可能无法达到此树的大小。Scikit-learn
    默认将值设置为“None”，因此默认情况下最大深度没有限制。
- en: Scikit-learn includes a few additional stopping parameters such as **min_samples_leaf,
    min_weighted_fraction, max_leaf_nodes, or max_features**, which are also not set
    by default and I will ignore for now.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: Scikit-learn 还包括一些额外的停止参数，如 **min_samples_leaf, min_weighted_fraction, max_leaf_nodes,
    or max_features**，这些参数默认为未设置，我暂时会忽略它们。
- en: A function that we need for every regressor is the fit function, which starts
    the training process. Input variables are a multi-dimensional array (X) with the
    input features. y is a one-dimensional array and describes the target variable.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 我们为每个回归器需要的一个函数是 fit 函数，它启动训练过程。输入变量是一个多维数组（X），包含输入特征。y 是一个一维数组，描述目标变量。
- en: In addition to our regressor (**RegressionTree**), we define a second class
    (**Node**) through which we set and store the parameters that each node of the
    tree has.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 除了我们的回归器（**RegressionTree**）外，我们还定义了一个第二个类（**Node**），通过它设置和存储树的每个节点所具有的参数。
- en: '[PRE5]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The fit function uses the helper function **_grow_tree(x, y)** to grow the tree
    piece by piece until one of the stopping criteria is reached.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: fit 函数使用辅助函数 **_grow_tree(x, y)** 一步步地生长树，直到达到停止准则中的一个。
- en: 'Before splitting the node, we check if one of the stopping criteria is met.
    In the simplified example, we only have two stopping criteria:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 在分割节点之前，我们检查是否满足任何一个停止准则。在简化的示例中，我们只有两个停止准则：
- en: '**(1) depth >= self.max_depth:** Is the maximum depth of the tree reached?'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: '**(1) depth >= self.max_depth:** 是否达到树的最大深度？'
- en: '**(2) n_samples < self.min_samples_split:** Is the number of samples in the
    node greater than **min_samples_split**?'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '**(2) n_samples < self.min_samples_split:** 节点中的样本数是否大于 **min_samples_split**？'
- en: If either condition is true, the node is a terminal node and the only thing
    we need to calculate is the mean (**np.mean(y)**).
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 如果任一条件为真，则该节点是终端节点，我们需要计算的唯一内容是均值（**np.mean(y)**）。
- en: If neither of the two conditions is true, we split the data set further. We
    first define which features we consider for the split. In our simplified case
    we do not limit the columns we use for the split. We use all features in X (**feat_idxs**).
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 如果两个条件都不为真，我们将进一步分割数据集。我们首先定义考虑哪些特征进行分割。在我们简化的情况下，我们不会限制用于分割的列。我们使用 X 中的所有特征（**feat_idxs**）。
- en: For the actual split, we define another helper function **_best_split** which
    we pass the x and y values of the node we are looking at.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 对于实际的分割，我们定义了另一个辅助函数 **_best_split**，我们将正在查看的节点的 x 和 y 值传递给它。
- en: I'll go into more detail about what **_best_split** does in a moment, but as
    the name implies **_best_split** returns us the "best" split, in the form of the
    selected feature for the split (**best_features**) and the threshold at which
    we split the dataset (**best_threshold**).
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 我会稍后详细介绍 **_best_split** 的作用，但正如名称所示，**_best_split** 会以所选特征（**best_features**）和我们分割数据集的阈值（**best_threshold**）的形式返回“最佳”分割。
- en: We use this information to actually split the dataset and store it as a node
    of our tree.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用这些信息来实际分割数据集并将其作为我们树的一个节点存储。
- en: Before we jump out of the function we call **_grow_tree** again for both halves
    of the branch.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们跳出函数之前，我们会再次调用**_grow_tree**来处理分支的两个部分。
- en: '[PRE6]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The only question that remains unanswered is how the algorithm figures out what
    the best split would be.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 唯一未解答的问题是算法如何确定最佳的分割。
- en: As mentioned before, we calculate a so-called information gain. In our case,
    we define the information gain as a reduction of the mean squared error. The errors
    or residuals for the node itself and the resulting child nodes is calculated as
    the difference between the average value of the target variable y in each node
    and the actual y values of the samples in the nodes.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，我们计算所谓的信息增益。在我们的例子中，我们将信息增益定义为均方误差的减少。节点本身及其结果子节点的误差或残差是通过计算每个节点中目标变量 y
    的平均值与节点中样本的实际 y 值之间的差异来得到的。
- en: The function goes through each feature one by one.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 该函数逐个遍历每个特征。
- en: We compute a set of possible thresholds for each feature as a moving average
    of all observations.
  id: totrans-168
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们为每个特征计算一组可能的阈值，作为所有观察值的移动平均值。
- en: Then we iterate over each threshold in the list, split the dataset and calculate
    a weighted mean squared error of the child nodes.
  id: totrans-169
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们遍历列表中的每个阈值，拆分数据集，并计算子节点的加权均方误差。
- en: Afterwards, we check if the calculated MSE is the smallest MSE calculated so
    far, if yes, we save the feature_idx and threshold as optimal. (in **best_feature_idxs**
    and **best_thresholds**)
  id: totrans-170
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们检查计算得到的MSE是否是迄今为止计算出的最小MSE，如果是的话，我们将feature_idx和threshold保存为最优（在**best_feature_idxs**和**best_thresholds**中）。
- en: '[PRE7]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Two functions which we have already used several times in the above sections
    are **_split** and **_squared_error**.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在上述几个部分中已经多次使用的两个函数是**_split**和**_squared_error**。
- en: '[PRE8]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The only thing we still need is a **predict()** function. For this we use **_traverse_tree**.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 唯一需要的就是**predict()**函数。为此，我们使用**_traverse_tree**。
- en: Using a loop function we go through the just built tree one by one. If we reach
    a leaf node, **_traverse_tree** returns the stored node value.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 使用循环函数，我们逐个遍历刚构建的树。如果到达叶子节点，**_traverse_tree**返回存储的节点值。
- en: '[PRE9]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'That''s it, the complete *decision tree regressor* is defined as:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 就这样，完整的*决策树回归器*定义为：
- en: '[PRE10]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 4\. Hands-On Example — Implementation from scratch vs. Scikit-learn DecisionTree
  id: totrans-179
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 4\. 实战示例 — 从头实现与Scikit-learn决策树
- en: '**Load the data set**'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: '**加载数据集**'
- en: 'For the test, we use the dataset already used as an example earlier, the automobile
    dataset. First, we load the dataset from uci.edu. Then we select a few attributes
    for the first simple test. For the following example, I choose:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 对于测试，我们使用之前作为示例的数据集，即汽车数据集。首先，我们从uci.edu加载数据集。然后，我们选择一些属性进行第一次简单测试。在以下示例中，我选择：
- en: Wheel Base
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 车轮基距
- en: Length
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 长度
- en: Width
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 宽度
- en: Hight
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 高度
- en: Make
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 制造商
- en: for the input vector X.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 对于输入向量X。
- en: Since the attribute “make” contains strings, we transform it into numeric features
    using OneHot Encoding.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 由于属性“make”包含字符串，我们使用OneHot Encoding将其转换为数值特征。
- en: '[PRE11]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '**Train the model**'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: '**训练模型**'
- en: After the input and output variables are defined, we try to run a first test
    with our algorithm to see if we can actually use it for predictions.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 在定义输入和输出变量后，我们尝试用我们的算法进行第一次测试，以查看我们是否可以实际用于预测。
- en: First, we split the dataset into a train and a test data set.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将数据集拆分为训练集和测试集。
- en: '[PRE12]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '![](../Images/ea6d2000d02b7cf85522ed1b8befb7aa.png)'
  id: totrans-194
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ea6d2000d02b7cf85522ed1b8befb7aa.png)'
- en: MSE of our “decision tree from scratch”— Screenshot by author
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的“从头开始的决策树”的MSE — 作者截图
- en: '**Compare it to the existing scikit-learn library**'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: '**与现有的scikit-learn库进行比较**'
- en: For comparison, we now try the same with the "DecisionTreeRegressor" library
    from scikit-learn. Our trained model performs exactly the same in this case. I
    won’t go into whether the result is good or bad here. If you want to know how
    to tune a regression model and find the model with the best performance, you can
    find a more detailed explanation of suitable methods in one of my previous articles
    ([here](https://medium.com/towards-data-science/7-of-the-most-commonly-used-regression-algorithms-and-how-to-choose-the-right-one-fc3c8890f9e3)
    or [here](https://medium.com/towards-data-science/a-step-by-step-introduction-to-bayesian-hyperparameter-optimization-94a623062fc)).
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 为了比较，我们现在尝试使用scikit-learn的"DecisionTreeRegressor"库。在这种情况下，我们训练的模型表现完全一样。我不会在这里探讨结果是好是坏。如果你想知道如何调整回归模型并找到表现最佳的模型，你可以在我以前的文章中找到更详细的适用方法解释（[这里](https://medium.com/towards-data-science/7-of-the-most-commonly-used-regression-algorithms-and-how-to-choose-the-right-one-fc3c8890f9e3)
    或 [这里](https://medium.com/towards-data-science/a-step-by-step-introduction-to-bayesian-hyperparameter-optimization-94a623062fc)）。
- en: '[PRE13]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '![](../Images/27891be9517a5e05166cee93702c5619.png)'
  id: totrans-199
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/27891be9517a5e05166cee93702c5619.png)'
- en: MSE of scikit-learn decision tree — Screenshot by author
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: scikit-learn决策树的MSE — 作者截图
- en: 5\. Summary
  id: totrans-201
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 5\. 总结
- en: The Decision Tree is the basis for a number of outstanding algorithms such as
    Random Forest, XGBoost, LightGBM and CatBoost. The concepts behind them are very
    intuitive and generally easy to understand, at least as long as you try to understand
    the individual subconcepts piece by piece. With this article, you have taken a
    good first step by understanding the core of every tree ensemble algorithm, the
    decision tree.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树是许多出色算法的基础，如随机森林、XGBoost、LightGBM和CatBoost。它们背后的概念非常直观，通常易于理解，至少当你尝试逐一理解各个子概念时。通过这篇文章，你已经通过理解每个树集成算法的核心——决策树，迈出了良好的一步。
- en: I plan to publish more articles about each concept that makes gradient-boosting
    frameworks so efficient.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 我计划发布更多关于使梯度提升框架如此高效的每个概念的文章。
- en: '**Enjoyed the story?**'
  id: totrans-204
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**喜欢这个故事吗？**'
- en: '*If you enjoyed reading and want to learn more about Machine Learning concepts,
    algorithms and applications, you can find a list with* [*all of my related articles.*](https://dmnkplzr.medium.com/list/e83997daeb7a)'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*如果你喜欢阅读并想了解更多关于机器学习的概念、算法和应用，你可以查看* [*我所有相关的文章列表。*](https://dmnkplzr.medium.com/list/e83997daeb7a)'
- en: '*If you don’t want to miss a new article, you can* [*subscribe for free*](https://dmnkplzr.medium.com/subscribe)
    *to get notified whenever I publish a new story.*'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*如果你不想错过新文章，你可以* [*免费订阅*](https://dmnkplzr.medium.com/subscribe) *以便在我发布新故事时收到通知。*'
- en: '*Become a Medium member to read more stories from other writers and me. You
    can support me by using my* [*referral link*](https://medium.com/@dmnkplzr/membership)
    *when you sign up. I’ll receive a commission at no extra cost to you.*'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*成为 Medium 会员，阅读更多来自其他作者和我的故事。你可以通过使用我的* [*推荐链接*](https://medium.com/@dmnkplzr/membership)
    *来支持我。你不会额外花费任何费用，我将获得佣金。*'
- en: Feel free to reach out to me on [LinkedIn](https://www.linkedin.com/in/polzerdo/)
    if you have any questions!
  id: totrans-208
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 如果你有任何问题，请随时通过 [LinkedIn](https://www.linkedin.com/in/polzerdo/) 联系我！
- en: 6\. References
  id: totrans-209
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 6\. 参考文献
- en: '[Carlens, H. (n.d.).](https://www.google.com/url?q=https%3A%2F%2Fwww.zotero.org%2Fgoogle-docs%2F%3FR398Mk&sa=D&ust=1680021205052647&usg=AOvVaw2Pb5R1CPby6AraUz7keuav)
    [*The State of Competitive Machine Learning*](https://www.google.com/url?q=https%3A%2F%2Fwww.zotero.org%2Fgoogle-docs%2F%3FR398Mk&sa=D&ust=1680021205053169&usg=AOvVaw02b_PEzqLVenZl73R1RWw-)[.
    ML Contests. Retrieved March 17, 2023, from https://mlcontests.com/state-of-competitive-machine-learning-2022/](https://www.google.com/url?q=https%3A%2F%2Fwww.zotero.org%2Fgoogle-docs%2F%3FR398Mk&sa=D&ust=1680021205053558&usg=AOvVaw070Krotnte0a5TdQv6FjZ8)[https://archive.ics.uci.edu/ml/datasets/automobile](https://www.google.com/url?q=https%3A%2F%2Farchive.ics.uci.edu%2Fml%2Fdatasets%2Fautomobile&sa=D&ust=1680021205053857&usg=AOvVaw2ehc0ELTUM0LQFHfPnPMNX)'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: '[Carlens, H. (无日期).](https://www.google.com/url?q=https%3A%2F%2Fwww.zotero.org%2Fgoogle-docs%2F%3FR398Mk&sa=D&ust=1680021205052647&usg=AOvVaw2Pb5R1CPby6AraUz7keuav)
    [*竞争性机器学习的现状*](https://www.google.com/url?q=https%3A%2F%2Fwww.zotero.org%2Fgoogle-docs%2F%3FR398Mk&sa=D&ust=1680021205053169&usg=AOvVaw02b_PEzqLVenZl73R1RWw-)[.
    ML 竞赛。检索日期2023年3月17日，来自 https://mlcontests.com/state-of-competitive-machine-learning-2022/](https://www.google.com/url?q=https%3A%2F%2Fwww.zotero.org%2Fgoogle-docs%2F%3FR398Mk&sa=D&ust=1680021205053558&usg=AOvVaw070Krotnte0a5TdQv6FjZ8)[https://archive.ics.uci.edu/ml/datasets/automobile](https://www.google.com/url?q=https%3A%2F%2Farchive.ics.uci.edu%2Fml%2Fdatasets%2Fautomobile&sa=D&ust=1680021205053857&usg=AOvVaw2ehc0ELTUM0LQFHfPnPMNX)'
- en: '[Chow, R. (2021, August 31).](https://www.google.com/url?q=https%3A%2F%2Fwww.zotero.org%2Fgoogle-docs%2F%3FR398Mk&sa=D&ust=1680021205054283&usg=AOvVaw3iOkiDt58NURkVrRBdafx7)
    [*Decision Tree and Random Forest Algorithms: Decision Drivers*](https://www.google.com/url?q=https%3A%2F%2Fwww.zotero.org%2Fgoogle-docs%2F%3FR398Mk&sa=D&ust=1680021205054645&usg=AOvVaw2CdIsxVMJJZXXrylsvekOy)[.
    History of Data Science. https://www.historyofdatascience.com/decision-tree-and-random-forest-algorithms-decision-drivers/](https://www.google.com/url?q=https%3A%2F%2Fwww.zotero.org%2Fgoogle-docs%2F%3FR398Mk&sa=D&ust=1680021205055068&usg=AOvVaw0I4ihT8MdGNEkPME96zZEH)'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: '[Chow, R. (2021年8月31日).](https://www.google.com/url?q=https%3A%2F%2Fwww.zotero.org%2Fgoogle-docs%2F%3FR398Mk&sa=D&ust=1680021205054283&usg=AOvVaw3iOkiDt58NURkVrRBdafx7)
    [*决策树与随机森林算法：决策驱动因素*](https://www.google.com/url?q=https%3A%2F%2Fwww.zotero.org%2Fgoogle-docs%2F%3FR398Mk&sa=D&ust=1680021205054645&usg=AOvVaw2CdIsxVMJJZXXrylsvekOy)[.
    数据科学史。 https://www.historyofdatascience.com/decision-tree-and-random-forest-algorithms-decision-drivers/](https://www.google.com/url?q=https%3A%2F%2Fwww.zotero.org%2Fgoogle-docs%2F%3FR398Mk&sa=D&ust=1680021205055068&usg=AOvVaw0I4ihT8MdGNEkPME96zZEH)'
- en: '[Géron, A. (2022).](https://www.google.com/url?q=https%3A%2F%2Fwww.zotero.org%2Fgoogle-docs%2F%3FR398Mk&sa=D&ust=1680021205055543&usg=AOvVaw2qodvWBxf0OfnVTygMXkQQ)
    [*HANDS-ON MACHINE LEARNING WITH SCIKIT-LEARN, KERAS, AND TENSORFLOW concepts,
    tools, and techniques to build intelligent systems*](https://www.google.com/url?q=https%3A%2F%2Fwww.zotero.org%2Fgoogle-docs%2F%3FR398Mk&sa=D&ust=1680021205055825&usg=AOvVaw0BB-492v7QeyrV6ajrJ35G)
    [(Third edition). O’Reilly Media, Inc.](https://www.google.com/url?q=https%3A%2F%2Fwww.zotero.org%2Fgoogle-docs%2F%3FR398Mk&sa=D&ust=1680021205056144&usg=AOvVaw21SJNw4GTfvgq7lPaxbny7)'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: '[Géron, A. (2022).](https://www.google.com/url?q=https%3A%2F%2Fwww.zotero.org%2Fgoogle-docs%2F%3FR398Mk&sa=D&ust=1680021205055543&usg=AOvVaw2qodvWBxf0OfnVTygMXkQQ)
    [*使用 Scikit-Learn、Keras 和 TensorFlow 的实践机器学习：构建智能系统的概念、工具和技术*](https://www.google.com/url?q=https%3A%2F%2Fwww.zotero.org%2Fgoogle-docs%2F%3FR398Mk&sa=D&ust=1680021205055825&usg=AOvVaw0BB-492v7QeyrV6ajrJ35G)
    [(第三版)。 O’Reilly Media, Inc.](https://www.google.com/url?q=https%3A%2F%2Fwww.zotero.org%2Fgoogle-docs%2F%3FR398Mk&sa=D&ust=1680021205056144&usg=AOvVaw21SJNw4GTfvgq7lPaxbny7)'
- en: '[Google Developers (Director). (2017, September 13).](https://www.google.com/url?q=https%3A%2F%2Fwww.zotero.org%2Fgoogle-docs%2F%3FR398Mk&sa=D&ust=1680021205056524&usg=AOvVaw0sn3PWrGmEkNvyLwwGzsVF)
    [*Let’s Write a Decision Tree Classifier from Scratch—Machine Learning Recipes
    #8*](https://www.google.com/url?q=https%3A%2F%2Fwww.zotero.org%2Fgoogle-docs%2F%3FR398Mk&sa=D&ust=1680021205056810&usg=AOvVaw1vfMxM9QW4YiXmrKThlU88)[.
    https://www.youtube.com/watch?v=LDRbO9a6XPU](https://www.google.com/url?q=https%3A%2F%2Fwww.zotero.org%2Fgoogle-docs%2F%3FR398Mk&sa=D&ust=1680021205057100&usg=AOvVaw3qIenpzMJ2DXQf4v8G4t5g)'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: '[Google Developers (导演). (2017年9月13日).](https://www.google.com/url?q=https%3A%2F%2Fwww.zotero.org%2Fgoogle-docs%2F%3FR398Mk&sa=D&ust=1680021205056524&usg=AOvVaw0sn3PWrGmEkNvyLwwGzsVF)
    [*从头开始编写决策树分类器——机器学习食谱第8集*](https://www.google.com/url?q=https%3A%2F%2Fwww.zotero.org%2Fgoogle-docs%2F%3FR398Mk&sa=D&ust=1680021205056810&usg=AOvVaw1vfMxM9QW4YiXmrKThlU88)[.
    https://www.youtube.com/watch?v=LDRbO9a6XPU](https://www.google.com/url?q=https%3A%2F%2Fwww.zotero.org%2Fgoogle-docs%2F%3FR398Mk&sa=D&ust=1680021205057100&usg=AOvVaw3qIenpzMJ2DXQf4v8G4t5g)'
- en: '[Swalin, A. (2019, June 11).](https://www.google.com/url?q=https%3A%2F%2Fwww.zotero.org%2Fgoogle-docs%2F%3FR398Mk&sa=D&ust=1680021205057661&usg=AOvVaw0Hx2RqIiROY9Gw9jSd-s6B)
    [*CatBoost vs. Light GBM vs. XGBoost*](https://www.google.com/url?q=https%3A%2F%2Fwww.zotero.org%2Fgoogle-docs%2F%3FR398Mk&sa=D&ust=1680021205057965&usg=AOvVaw16zH0uhPbzR4Fn-98cq3jH)[.
    Medium. https://towardsdatascience.com/catboost-vs-light-gbm-vs-xgboost-5f93620723db](https://www.google.com/url?q=https%3A%2F%2Fwww.zotero.org%2Fgoogle-docs%2F%3FR398Mk&sa=D&ust=1680021205058282&usg=AOvVaw3F9COP_67yKpUAwzl5-vV1)'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: '[Swalin, A. (2019年6月11日).](https://www.google.com/url?q=https%3A%2F%2Fwww.zotero.org%2Fgoogle-docs%2F%3FR398Mk&sa=D&ust=1680021205057661&usg=AOvVaw0Hx2RqIiROY9Gw9jSd-s6B)
    [*CatBoost vs. Light GBM vs. XGBoost*](https://www.google.com/url?q=https%3A%2F%2Fwww.zotero.org%2Fgoogle-docs%2F%3FR398Mk&sa=D&ust=1680021205057965&usg=AOvVaw16zH0uhPbzR4Fn-98cq3jH)[.
    Medium. https://towardsdatascience.com/catboost-vs-light-gbm-vs-xgboost-5f93620723db](https://www.google.com/url?q=https%3A%2F%2Fwww.zotero.org%2Fgoogle-docs%2F%3FR398Mk&sa=D&ust=1680021205058282&usg=AOvVaw3F9COP_67yKpUAwzl5-vV1)'
- en: '[*UCI Machine Learning Repository: Automobile Data Set*](https://www.google.com/url?q=https%3A%2F%2Fwww.zotero.org%2Fgoogle-docs%2F%3FR398Mk&sa=D&ust=1680021205058732&usg=AOvVaw0lCQ-tx1LhGwFzHMkPH8K7)[.
    (n.d.). Retrieved February 24, 2023, from https://archive.ics.uci.edu/ml/datasets/automobile](https://www.google.com/url?q=https%3A%2F%2Fwww.zotero.org%2Fgoogle-docs%2F%3FR398Mk&sa=D&ust=1680021205059072&usg=AOvVaw2NJyPeMqBT1B0qE6gA2aGI)'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: '[*UCI机器学习库：汽车数据集*](https://www.google.com/url?q=https%3A%2F%2Fwww.zotero.org%2Fgoogle-docs%2F%3FR398Mk&sa=D&ust=1680021205058732&usg=AOvVaw0lCQ-tx1LhGwFzHMkPH8K7)[.
    (无日期). 取自2023年2月24日, 来源 https://archive.ics.uci.edu/ml/datasets/automobile](https://www.google.com/url?q=https%3A%2F%2Fwww.zotero.org%2Fgoogle-docs%2F%3FR398Mk&sa=D&ust=1680021205059072&usg=AOvVaw2NJyPeMqBT1B0qE6gA2aGI)'
- en: 7\. Appendix
  id: totrans-216
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 7\. 附录
- en: Regression Tree from Scratch
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从头开始的回归树
- en: '[PRE14]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
