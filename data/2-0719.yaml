- en: Deploying PyTorch Models with Nvidia Triton Inference Server
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Nvidia Triton Inference Server部署PyTorch模型
- en: 原文：[https://towardsdatascience.com/deploying-pytorch-models-with-nvidia-triton-inference-server-bb139066a387](https://towardsdatascience.com/deploying-pytorch-models-with-nvidia-triton-inference-server-bb139066a387)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/deploying-pytorch-models-with-nvidia-triton-inference-server-bb139066a387](https://towardsdatascience.com/deploying-pytorch-models-with-nvidia-triton-inference-server-bb139066a387)
- en: A flexible high-performant model serving solution
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 一个灵活且高性能的模型服务解决方案
- en: '[](https://ram-vegiraju.medium.com/?source=post_page-----bb139066a387--------------------------------)[![Ram
    Vegiraju](../Images/07d9334e905f710d9f3c6187cf69a1a5.png)](https://ram-vegiraju.medium.com/?source=post_page-----bb139066a387--------------------------------)[](https://towardsdatascience.com/?source=post_page-----bb139066a387--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----bb139066a387--------------------------------)
    [Ram Vegiraju](https://ram-vegiraju.medium.com/?source=post_page-----bb139066a387--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://ram-vegiraju.medium.com/?source=post_page-----bb139066a387--------------------------------)[![Ram
    Vegiraju](../Images/07d9334e905f710d9f3c6187cf69a1a5.png)](https://ram-vegiraju.medium.com/?source=post_page-----bb139066a387--------------------------------)[](https://towardsdatascience.com/?source=post_page-----bb139066a387--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----bb139066a387--------------------------------)
    [Ram Vegiraju](https://ram-vegiraju.medium.com/?source=post_page-----bb139066a387--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----bb139066a387--------------------------------)
    ·7 min read·Sep 14, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----bb139066a387--------------------------------)
    ·7分钟阅读·2023年9月14日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/eb2afba9c81fb0c85c927e7f318bc8e1.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/eb2afba9c81fb0c85c927e7f318bc8e1.png)'
- en: Image from [Unsplash](https://unsplash.com/photos/8fVK5TFKaSE)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来自 [Unsplash](https://unsplash.com/photos/8fVK5TFKaSE)
- en: Machine Learning’s (ML) value is truly recognized in real-world applications
    when we arrive at [Model Hosting and Inference](https://cloud.google.com/bigquery/docs/inference-overview#:~:text=Machine%20learning%20inference%20is%20the,machine%20learning%20model%20into%20production.%22).
    It’s hard to productionize ML workloads if you don’t have a highly performant
    model-serving solution that will help your model scale up and down.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习（ML）的价值在实际应用中真正体现，当我们到达 [模型托管与推理](https://cloud.google.com/bigquery/docs/inference-overview#:~:text=Machine%20learning%20inference%20is%20the,machine%20learning%20model%20into%20production.%22)时。如果没有一个高性能的模型服务解决方案，帮助你的模型实现弹性扩展，将很难将ML工作负载投入生产。
- en: '**What is a model server/what is model serving?** Think of a model server as
    a bit of an equivalent to a web server in the ML world. It’s not enough to just
    throw large amounts of hardware behind the model, you need a communication layer
    that will help process your client’s requests while efficiently allocating the
    hardware needed to address the traffic your application is seeing. Model Servers
    are a tunable feature for users: we can squeeze performance from a latency perspective
    by controlling aspects such as gRPC vs REST, etc. Popular examples of model servers
    include the following:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '**什么是模型服务器/什么是模型服务？** 可以将模型服务器视为ML世界中与web服务器相当的东西。仅仅将大量硬件投入到模型中是不够的，你还需要一个通信层，帮助处理客户端的请求，同时有效分配硬件以应对你的应用程序所遇到的流量。模型服务器是一个可调节的功能：我们可以通过控制gRPC与REST等方面，从延迟角度挤压性能。流行的模型服务器示例如下：'
- en: '[TensorFlow Serving](https://www.tensorflow.org/tfx/guide/serving)'
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[TensorFlow Serving](https://www.tensorflow.org/tfx/guide/serving)'
- en: '[TorchServe](https://pytorch.org/serve/)'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[TorchServe](https://pytorch.org/serve/)'
- en: '[Multi-Model Server (MMS)](https://github.com/awslabs/multi-model-server)'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[多模型服务器 (MMS)](https://github.com/awslabs/multi-model-server)'
- en: '[Deep Java Library (DJL)](https://github.com/deepjavalibrary/djl-serving)'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[深度Java库 (DJL)](https://github.com/deepjavalibrary/djl-serving)'
- en: The one we explore today is [Nvidia Triton Inference Server](https://github.com/triton-inference-server/server),
    a highly flexible and performant model serving solution. Each model server requires
    for model artifacts and inference scripts to be presented in its own unique way
    that it can understand. In today’s article we take a sample PyTorch model and
    show how we can host it utilizing Triton Inference Server.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 今天我们探讨的是 [Nvidia Triton Inference Server](https://github.com/triton-inference-server/server)，这是一个高度灵活且高性能的模型服务解决方案。每个模型服务器要求以其特定的方式呈现模型工件和推理脚本，以便它可以理解。在今天的文章中，我们以一个示例PyTorch模型为例，展示如何利用Triton
    Inference Server进行托管。
- en: '**NOTE**: This article assumes basic understanding of Machine Learning and
    does not delve into any theory behind model building. A fluency of Python and
    a basic understanding of Docker containers is also assumed. We will also be working
    in a [SageMaker Classic Notebook Instance](https://docs.aws.amazon.com/sagemaker/latest/dg/nbi.html)
    for development, so please create an AWS account if needed (you can also run this
    sample elsewhere if you prefer).'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意**：本文假设你对机器学习有基本了解，并且不深入探讨模型构建的理论。还假设你对Python有一定的熟练度，并对Docker容器有基本了解。我们还将在[SageMaker经典Notebook实例](https://docs.aws.amazon.com/sagemaker/latest/dg/nbi.html)中进行开发，因此如有必要，请创建一个AWS账户（如果你愿意，也可以在其他地方运行此示例）。'
- en: '**DISCLAIMER**: I am a Machine Learning Architect at AWS and my opinions are
    my own.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '**免责声明**：我是AWS的机器学习架构师，我的观点仅代表我自己。'
- en: Why Triton Inference Server?
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 为什么选择Triton推理服务器？
- en: 'Triton Inference Server is an open source model serving solution that has a
    variety of benefits including the following:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: Triton推理服务器是一个开源的模型服务解决方案，具有以下多种好处：
- en: '**Framework Support**: Triton natively supports a multitude of frameworks such
    as PyTorch, TensorFlow, ONNX, and custom Python/C++ environments. Triton recognizes
    these different frameworks in its setup as a “backend”. In this example we use
    the PyTorch backend it provides for hosting our TorchScript model.'
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**框架支持**：Triton原生支持多种框架，如PyTorch、TensorFlow、ONNX，以及自定义的Python/C++环境。Triton在其设置中将这些不同的框架识别为“后端”。在这个例子中，我们使用它提供的PyTorch后端来托管我们的TorchScript模型。'
- en: '**Dynamic Batching**: Inference performance tuning is an iterative experiment.
    Dynamic Batching is one inference optimization technique where you can group together
    multiple requests into one. With Triton you can enable Dynamic Batching and control
    the Maximum Batch Size to try to optimize both throughput and latency.'
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**动态批处理**：推理性能调整是一个迭代实验。动态批处理是一种推理优化技术，通过这种技术，你可以将多个请求组合成一个。使用Triton，你可以启用动态批处理并控制最大批量大小，以优化吞吐量和延迟。'
- en: '**Ensembles/Model Pipelines**: Via ensembles and model pipelines you can stitch
    together multiple ML models along with any pre/post processing logic you have
    into a universal execution flow behind a single container.'
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**模型集成/管道**：通过集成和模型管道，你可以将多个机器学习模型及任何预处理/后处理逻辑组合成一个统一的执行流，放在一个容器后面。'
- en: '**Hardware Support**: Triton supports both CPU/GPU based workloads. This makes
    it easy to pair with a hosting platform such as [SageMaker Real-Time Inference](https://docs.aws.amazon.com/sagemaker/latest/dg/realtime-endpoints.html),
    where you can host thousands of models utilizing [Multi-Model Endpoints with GPUs](https://aws.amazon.com/blogs/machine-learning/run-multiple-deep-learning-models-on-gpu-with-amazon-sagemaker-multi-model-endpoints/)
    and Triton as the model serving stack.'
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**硬件支持**：Triton支持基于CPU/GPU的工作负载。这使得它可以轻松配对像[SageMaker实时推理](https://docs.aws.amazon.com/sagemaker/latest/dg/realtime-endpoints.html)这样的托管平台，在这些平台上，你可以托管数千个模型，利用[多模型端点与GPU](https://aws.amazon.com/blogs/machine-learning/run-multiple-deep-learning-models-on-gpu-with-amazon-sagemaker-multi-model-endpoints/)和Triton作为模型服务栈。'
- en: Triton Inference Server like other Model Servers has its pros and cons depending
    on the use-case, so based off of your model and hosting requirements, it’s essential
    you choose the right model server option.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: Triton推理服务器与其他模型服务器一样，具有各自的优缺点，具体取决于用例，因此根据你的模型和托管需求，选择合适的模型服务器选项至关重要。
- en: Local Model Setup
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 本地模型设置
- en: For this example, we will be working in a SageMaker Classic Notebook Instance
    on the PyTorch Kernel. The instance will be a g4dn.4xlarge, so we can run our
    Triton container on a GPU based instance.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们将使用SageMaker经典Notebook实例中的PyTorch内核。实例将是g4dn.4xlarge，因此我们可以在基于GPU的实例上运行我们的Triton容器。
- en: To get started with we will work with a sample PyTorch Linear Regression model
    that we will train on some dummy data generated by numpy.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 开始时，我们将使用一个样本PyTorch线性回归模型，并在由numpy生成的一些虚拟数据上进行训练。
- en: '[PRE0]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: We then save this model as a [TorchScript](https://pytorch.org/docs/stable/jit.html)
    model for our Triton PyTorch backend and run a sample inference so we can understand
    what a sample input for our model’s inference will look like.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们将此模型保存为[TorchScript](https://pytorch.org/docs/stable/jit.html)模型，以便在Triton
    PyTorch后端中使用，并运行一个样本推理，以便了解模型推理的样本输入将是什么样的。
- en: '[PRE1]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Now that we have our model.pt and understand how to run inference with this
    model, we can focus on setting up Triton to host this specific model.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了model.pt并了解了如何用此模型进行推理，我们可以专注于设置Triton以托管这个特定的模型。
- en: Triton Inference Server Hosting
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Triton推理服务器托管
- en: 'Before we can work on the getting the Triton Inference Server up we need to
    understand what artifacts it requires and the format it expects it to be presented
    in. We already have our model.pt, this is our model metadata file. Triton also
    expects a config.pbtxt file that essentially defines your serving properties.
    In this case we define a few fields that are necessary:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们可以开始设置Triton推理服务器之前，我们需要了解它需要哪些工件以及它期望的格式。我们已经有了model.pt，这是我们的模型元数据文件。Triton还需要一个config.pbtxt文件，基本上定义了你的服务属性。在这种情况下，我们定义了几个必要的字段：
- en: '**model_name**: This is the model name in our model repository, you can also
    version it incase there are multiple versions of the model.'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**model_name**：这是我们模型库中的模型名称，你还可以为其版本控制，以防有多个版本的模型。'
- en: '**platform**: This is the backend environment for the Server, in this case
    we define the backend as pytorch_libtorch to setup the environment for our TorchScript
    model.'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**平台**：这是服务器的后台环境，在这种情况下，我们将后台定义为pytorch_libtorch，以设置TorchScript模型的环境。'
- en: '**input/output**: For Triton we have to define our input/output data shapes
    and format (you can find out this info with numpy if you are unaware)'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**输入/输出**：对于Triton，我们必须定义我们的输入/输出数据形状和格式（如果你不知道，可以通过numpy找到这些信息）'
- en: 'Optionally you can also define more advanced parameters such as enabling dynamic
    batching, max batch size, and backend specific variables that you would like to
    tweak for performance testing. Our base config.pbtxt looks like the following:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 可选地，你还可以定义更高级的参数，如启用动态批处理、最大批大小，以及你希望为性能测试调整的后台特定变量。我们的基本config.pbtxt如下：
- en: '[PRE2]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Triton also expects this file and the model.pt to be presented in a specific
    format before we can kickstart the server. For the PyTorch backend the following
    model repository structure is expected for your artifacts:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: Triton还期望这个文件和model.pt以特定格式呈现，然后我们才能启动服务器。对于PyTorch后台，你的工件应该遵循以下模型库结构：
- en: '[PRE3]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'We then shift our artifacts into this existing structure using the following
    bash command:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们使用以下bash命令将工件移到这个现有结构中：
- en: '[PRE4]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: To start the Triton Inference Server, ensure you have [Docker](https://www.docker.com/)
    installed in your environment; this comes pre-installed in SageMaker Classic Notebook
    Instances (not SageMaker Studio at the time of this article).
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 要启动Triton推理服务器，请确保你的环境中已安装[Docker](https://www.docker.com/)；这在SageMaker Classic
    Notebook实例中预装（在本文撰写时，SageMaker Studio尚未预装）。
- en: 'To start out container we first pull the latest Triton image utilizing the
    following command:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 要启动容器，我们首先使用以下命令拉取最新的Triton镜像：
- en: '[PRE5]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'We can then utilize [Nvidia CLI](https://developer.nvidia.com/nvidia-system-management-interface)
    utility commands in conjunction with Docker to start our Triton Inference Server.
    Triton has three default ports it exposes for inference that we specify in our
    Docker run command:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们可以利用[Nvidia CLI](https://developer.nvidia.com/nvidia-system-management-interface)实用程序命令结合Docker来启动我们的Triton推理服务器。Triton有三个默认端口用于推理，我们在Docker运行命令中指定：
- en: '**8000**: HTTP REST API requests, we utilize this port in this example'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**8000**：HTTP REST API请求，我们在这个示例中使用此端口'
- en: '**8001**: [gRPC requests](https://aws.amazon.com/compare/the-difference-between-grpc-and-rest/#:~:text=In%20gRPC%2C%20one%20component%20(the,updates%20data%20on%20the%20server.),
    this is especially useful for optimizing Computer Vision workloads'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**8001**：[gRPC请求](https://aws.amazon.com/compare/the-difference-between-grpc-and-rest/#:~:text=In%20gRPC%2C%20one%20component%20(the,updates%20data%20on%20the%20server.)，这对于优化计算机视觉工作负载特别有用'
- en: '**8002**: Metrics and monitoring related to Triton Inference Server via Promotheus'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**8002**：通过Prometheus监控Triton推理服务器的指标'
- en: Make sure to also replace the path provided with the path to your model repository
    directory. In this instance it is ‘/home/ec2-user/SageMaker’, which you want to
    replace with the path to your model repository.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 确保将提供的路径替换为你的模型库目录路径。在这个实例中，它是‘/home/ec2-user/SageMaker’，你需要将其替换为你的模型库路径。
- en: '[PRE6]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Once we run this command you should see the Triton Inference Server has been
    started up.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们运行这个命令，你应该会看到Triton推理服务器已经启动。
- en: '![](../Images/9cad6fcc0e41b0f7e77a85c5ad882307.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9cad6fcc0e41b0f7e77a85c5ad882307.png)'
- en: Triton Server Started (Screenshot by Author)
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: Triton服务器已启动（截图由作者提供）
- en: 'We can now make requests to the model server, which we can conduct in two separate
    ways that we’ll explore:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以向模型服务器发出请求，我们可以通过两种不同的方式进行，这里将进行探索：
- en: '[Python Request Library](https://pypi.org/project/requests/): Here you can
    pass in the inference URL for the Triton Server address and specify your input
    parameters.'
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[Python Request Library](https://pypi.org/project/requests/): 在这里你可以传入 Triton
    Server 地址的推理 URL 并指定输入参数。'
- en: '[Triton Client Library](https://github.com/triton-inference-server/client/tree/main):
    A client provided by Triton that you can instantiate and use their built-in API
    calls. We use Python in this case, but you can also use Java and C++.'
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[Triton Client Library](https://github.com/triton-inference-server/client/tree/main):
    Triton 提供的客户端，你可以实例化并使用其内置的 API 调用。在这种情况下我们使用 Python，但你也可以使用 Java 和 C++。'
- en: 'For the requests library we pass in the appropriate URL with our model name
    and version like the following:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 requests 库，我们传入适当的 URL 和模型名称及版本，如下所示：
- en: '[PRE7]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: For the Triton Client Library we use the same values we specified in our config
    file and above, but utilize the HTTP client specific API calls.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 Triton 客户端库，我们使用在配置文件中指定的相同值，但利用 HTTP 客户端特定的 API 调用。
- en: '[PRE8]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Additional Resources & Conclusion
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 额外资源与总结
- en: '[](https://github.com/RamVegiraju/triton-inference-server-examples/blob/master/pytorch-backend/triton-pytorch-ann.ipynb?source=post_page-----bb139066a387--------------------------------)
    [## triton-inference-server-examples/pytorch-backend/triton-pytorch-ann.ipynb
    at master ·…'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://github.com/RamVegiraju/triton-inference-server-examples/blob/master/pytorch-backend/triton-pytorch-ann.ipynb?source=post_page-----bb139066a387--------------------------------)
    [## triton-inference-server-examples/pytorch-backend/triton-pytorch-ann.ipynb
    at master ·…'
- en: Triton Inference Server PyTorch Example. Contribute to RamVegiraju/triton-inference-server-examples
    development by…
  id: totrans-63
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Triton Inference Server PyTorch 示例。贡献者 RamVegiraju/triton-inference-server-examples
    开发中…
- en: github.com](https://github.com/RamVegiraju/triton-inference-server-examples/blob/master/pytorch-backend/triton-pytorch-ann.ipynb?source=post_page-----bb139066a387--------------------------------)
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '[github.com](https://github.com/RamVegiraju/triton-inference-server-examples/blob/master/pytorch-backend/triton-pytorch-ann.ipynb?source=post_page-----bb139066a387--------------------------------)'
- en: The entire code for the example can be found at the link above. Triton Inference
    Server is a dynamic model serving option that can be used for advanced ML model
    hosting. For more Triton specific examples please refer to the following [Github
    repository](https://github.com/triton-inference-server/tutorials). In coming articles
    we will continue to explore how we can harness different model servers to host
    various ML models.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 示例的完整代码可以在上面的链接中找到。Triton Inference Server 是一种动态模型服务选项，可用于高级 ML 模型托管。有关更多 Triton
    特定的示例，请参阅以下 [Github 仓库](https://github.com/triton-inference-server/tutorials)。在接下来的文章中，我们将继续探讨如何利用不同的模型服务器来托管各种
    ML 模型。
- en: As always thank you for reading and feel free to leave any feedback.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 一如既往，谢谢你的阅读，欢迎随时留下反馈。
- en: '*If you enjoyed this article feel free to connect with me on* [*LinkedIn*](https://www.linkedin.com/in/ram-vegiraju-81272b162/)
    *and subscribe to my Medium* [*Newsletter*](https://ram-vegiraju.medium.com/subscribe)*.
    If you’re new to Medium, sign up using my* [*Membership Referral*](https://ram-vegiraju.medium.com/membership)*.*'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '*如果你喜欢这篇文章，欢迎通过* [*LinkedIn*](https://www.linkedin.com/in/ram-vegiraju-81272b162/)
    *与我联系，并订阅我的 Medium* [*通讯*](https://ram-vegiraju.medium.com/subscribe)*。如果你是 Medium
    新用户，可以通过我的* [*会员推荐链接*](https://ram-vegiraju.medium.com/membership)*进行注册。*'
