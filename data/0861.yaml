- en: Using OpenCLIP for Image Search and Automatic Captioning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 OpenCLIP 进行图像搜索和自动标注
- en: 原文：[https://towardsdatascience.com/using-openclip-for-image-search-and-automatic-captioning-fa1cbbd48ce4?source=collection_archive---------7-----------------------#2023-03-07](https://towardsdatascience.com/using-openclip-for-image-search-and-automatic-captioning-fa1cbbd48ce4?source=collection_archive---------7-----------------------#2023-03-07)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/using-openclip-for-image-search-and-automatic-captioning-fa1cbbd48ce4?source=collection_archive---------7-----------------------#2023-03-07](https://towardsdatascience.com/using-openclip-for-image-search-and-automatic-captioning-fa1cbbd48ce4?source=collection_archive---------7-----------------------#2023-03-07)
- en: How LAION used more data and new ML training techniques to improve image and
    text embeddings for various applications
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何利用更多的数据和新的机器学习训练技术来改进图像和文本嵌入，以用于各种应用
- en: '[](https://robgon.medium.com/?source=post_page-----fa1cbbd48ce4--------------------------------)[![Robert
    A. Gonsalves](../Images/96b4da0f602a1cd9d1e1d2917868cbee.png)](https://robgon.medium.com/?source=post_page-----fa1cbbd48ce4--------------------------------)[](https://towardsdatascience.com/?source=post_page-----fa1cbbd48ce4--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----fa1cbbd48ce4--------------------------------)
    [Robert A. Gonsalves](https://robgon.medium.com/?source=post_page-----fa1cbbd48ce4--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://robgon.medium.com/?source=post_page-----fa1cbbd48ce4--------------------------------)[![Robert
    A. Gonsalves](../Images/96b4da0f602a1cd9d1e1d2917868cbee.png)](https://robgon.medium.com/?source=post_page-----fa1cbbd48ce4--------------------------------)[](https://towardsdatascience.com/?source=post_page-----fa1cbbd48ce4--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----fa1cbbd48ce4--------------------------------)
    [Robert A. Gonsalves](https://robgon.medium.com/?source=post_page-----fa1cbbd48ce4--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fc97e6c73c13c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fusing-openclip-for-image-search-and-automatic-captioning-fa1cbbd48ce4&user=Robert+A.+Gonsalves&userId=c97e6c73c13c&source=post_page-c97e6c73c13c----fa1cbbd48ce4---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----fa1cbbd48ce4--------------------------------)
    ·12 min read·Mar 7, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Ffa1cbbd48ce4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fusing-openclip-for-image-search-and-automatic-captioning-fa1cbbd48ce4&user=Robert+A.+Gonsalves&userId=c97e6c73c13c&source=-----fa1cbbd48ce4---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fc97e6c73c13c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fusing-openclip-for-image-search-and-automatic-captioning-fa1cbbd48ce4&user=Robert+A.+Gonsalves&userId=c97e6c73c13c&source=post_page-c97e6c73c13c----fa1cbbd48ce4---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----fa1cbbd48ce4--------------------------------)
    ·12 分钟阅读·2023年3月7日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Ffa1cbbd48ce4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fusing-openclip-for-image-search-and-automatic-captioning-fa1cbbd48ce4&user=Robert+A.+Gonsalves&userId=c97e6c73c13c&source=-----fa1cbbd48ce4---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ffa1cbbd48ce4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fusing-openclip-for-image-search-and-automatic-captioning-fa1cbbd48ce4&source=-----fa1cbbd48ce4---------------------bookmark_footer-----------)![](../Images/01279abccfb8391b5c22a466bcb77154.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ffa1cbbd48ce4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fusing-openclip-for-image-search-and-automatic-captioning-fa1cbbd48ce4&source=-----fa1cbbd48ce4---------------------bookmark_footer-----------)![](../Images/01279abccfb8391b5c22a466bcb77154.png)'
- en: “**high-tech computer system for finding pictures in a large library,”** I*mage
    created using an AI image creation program,* Midjourney, and edited by the author
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: “**高科技计算机系统用于在大型库中查找图片，**”我*使用 AI 图像生成程序 Midjourney 创建的图像*，并由作者编辑
- en: I have been using and writing about OpenAI’s CLIP system since it came out in
    2021 [1]. It consists of image and text encoding models that can be used for various
    forms of cross-modal comparison, like using a text query to find the best matching
    image in a library quickly.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 自从 2021 年 OpenAI 的 CLIP 系统发布以来，我一直在使用和撰写关于该系统的内容[1]。该系统包括图像和文本编码模型，可用于各种形式的跨模态比较，例如使用文本查询快速找到库中最匹配的图像。
- en: In December 2022, an independent group of researchers known as LAION released
    a paper called “Reproducible scaling laws for contrastive language-image learning”
    [2] that describes how they first reimplemented and trained a model similar to
    CLIP and then experimented with improving the system by training with a larger
    dataset and using new ML techniques. They call their new model OpenCLIP.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在2022年12月，一个独立的研究小组LAION发布了一篇名为“可重复的对比语言-图像学习扩展规律”[2]的论文，描述了他们如何首先重新实现和训练一个类似于CLIP的模型，然后通过训练更大的数据集和使用新的机器学习技术来改进系统。他们将他们的新模型称为OpenCLIP。
- en: In this article, I will provide some background info on the original CLIP, describe
    how LAION improved the model, and show some results from my experiments with the
    two systems using images from the [Library of Congress’s Flickr photostream](https://www.flickr.com/photos/library_of_congress/).
    I also implemented a cool technique called “embedding arithmetic” from Meta AI
    to search for photos using both images and text prompts [3].
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我将提供一些关于原始CLIP的背景信息，描述LAION如何改进模型，并展示我在使用[国会图书馆Flickr照片流](https://www.flickr.com/photos/library_of_congress/)的图像进行的两个系统实验的结果。我还实现了Meta
    AI的一个叫做“嵌入算术”的酷技术，以通过图像和文本提示[3]搜索照片。
