- en: Simplify Your Data Preparation with These Four Lesser-Known Scikit-Learn Classes
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 用这四个鲜为人知的Scikit-Learn类简化你的数据准备
- en: 原文：[https://towardsdatascience.com/simplify-your-data-preparation-with-these-4-lesser-known-scikit-learn-classes-70270c94569f](https://towardsdatascience.com/simplify-your-data-preparation-with-these-4-lesser-known-scikit-learn-classes-70270c94569f)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/simplify-your-data-preparation-with-these-4-lesser-known-scikit-learn-classes-70270c94569f](https://towardsdatascience.com/simplify-your-data-preparation-with-these-4-lesser-known-scikit-learn-classes-70270c94569f)
- en: 'Forget train_test_split: Pipeline, ColumnTransformer, FeatureUnion and FunctionTransformer
    are indispensable even if you use XGBoost or LGBM'
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 忘掉train_test_split：Pipeline、ColumnTransformer、FeatureUnion和FunctionTransformer即使在使用XGBoost或LGBM时也是不可或缺的
- en: '[](https://medium.com/@mattchapmanmsc?source=post_page-----70270c94569f--------------------------------)[![Matt
    Chapman](../Images/7511deb8d9ed408ece21031f6614c532.png)](https://medium.com/@mattchapmanmsc?source=post_page-----70270c94569f--------------------------------)[](https://towardsdatascience.com/?source=post_page-----70270c94569f--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----70270c94569f--------------------------------)
    [Matt Chapman](https://medium.com/@mattchapmanmsc?source=post_page-----70270c94569f--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@mattchapmanmsc?source=post_page-----70270c94569f--------------------------------)[![Matt
    Chapman](../Images/7511deb8d9ed408ece21031f6614c532.png)](https://medium.com/@mattchapmanmsc?source=post_page-----70270c94569f--------------------------------)[](https://towardsdatascience.com/?source=post_page-----70270c94569f--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----70270c94569f--------------------------------)
    [Matt Chapman](https://medium.com/@mattchapmanmsc?source=post_page-----70270c94569f--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----70270c94569f--------------------------------)
    ·10 min read·Jun 1, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于[Towards Data Science](https://towardsdatascience.com/?source=post_page-----70270c94569f--------------------------------)
    ·10分钟阅读·2023年6月1日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/4d616242f15fc7f0163616870c608e44.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4d616242f15fc7f0163616870c608e44.png)'
- en: Image by [Victor](https://unsplash.com/@victor_g) on [Unsplash](https://unsplash.com/photos/UoIiVYka3VY)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由[Victor](https://unsplash.com/@victor_g)提供，来源于[Unsplash](https://unsplash.com/photos/UoIiVYka3VY)
- en: Data preparation is famously the least-loved aspect of Data Science. If done
    right, however, it needn’t be such a headache.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 数据准备被公认为数据科学中最不受欢迎的部分。然而，如果做得正确，它其实不必那么令人头疼。
- en: While scikit-learn has fallen out of vogue as a *modelling* library in recent
    years given the meteoric rise of PyTorch, LightGBM, and XGBoost, it’s still easily
    one of the best *data preparation* libraries out there.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管近年来由于PyTorch、LightGBM和XGBoost的迅猛崛起，scikit-learn作为*建模*库的流行度有所下降，但它仍然是最佳的*数据准备*库之一。
- en: 'And I’m not just talking about that old chestnut: `train_test_split`. If you’re
    prepared to dig a little deeper, you’ll find a treasure trove of helpful tools
    for more advanced data preparation techniques, all of which are perfectly compatible
    with using other libraries like `lightgbm`, `xgboost` and `catboost` for subsequent
    modelling.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 我不仅仅是指那老掉牙的`train_test_split`。如果你愿意深入挖掘，你会发现一系列对高级数据准备技术非常有用的工具，这些工具与其他库如`lightgbm`、`xgboost`和`catboost`完美兼容，用于后续建模。
- en: In this article, I’ll walk through four scikit-learn classes which significantly
    speed up my data preparation workflows in my day-to-day job as a Data Scientist.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我将介绍四个scikit-learn类，这些类显著加快了我作为数据科学家日常工作中的数据准备流程。
- en: '1\. Pipeline: Seamlessly combine preprocessing steps'
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 1\. Pipeline：无缝结合预处理步骤
- en: 'Scikit-learn’s `Pipeline` class enables you to combine different preprocessors
    or models into a single, callable chunk of code:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: Scikit-learn的`Pipeline`类使你能够将不同的预处理器或模型组合成一个可调用的代码块：
- en: '![](../Images/6aa21afcbb7383b916da6ee40e39478f.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6aa21afcbb7383b916da6ee40e39478f.png)'
- en: Image by author
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片
- en: 'Pipelines can be composed of two different things:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 管道可以由两种不同的东西组成：
- en: '**Transformer**: any object with the `fit()` and `transform()` methods. You
    can think of a transformer as an object that’s used for processing your data,
    and you will commonly have multiple transformers in your data preparation workflow.
    E.g., you might use one transformer to impute missing values, and another one
    to scale features or one-hot encode your categorical variables. `MinMaxScaler()`,
    `SimpleImputer()` and `OneHotEncoder()` are all examples of transformers.'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**变换器**：任何具有`fit()`和`transform()`方法的对象。你可以把变换器看作是用于处理数据的对象，通常在数据准备工作流中会有多个变换器。例如，你可以使用一个变换器来填补缺失值，另一个来缩放特征或对分类变量进行独热编码。`MinMaxScaler()`、`SimpleImputer()`和`OneHotEncoder()`都是变换器的例子。'
- en: '**Estimator**: In scikit-learn lingo, an “estimator” usually means a machine
    learning model; i.e. an object with the `fit()` and `predict()` methods. `LinearRegression()`
    and `RandomForestClassifier()` are examples of estimators.'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**估计器**：在scikit-learn术语中，“估计器”通常指机器学习模型；即具有`fit()`和`predict()`方法的对象。`LinearRegression()`和`RandomForestClassifier()`是估计器的例子。'
- en: In a pipeline, you can chain together as many **transformers** as you like,
    enabling you to apply different data preprocessing steps sequentially. If you
    like, you can also add on an **estimator** (ML model) at the end in order to make
    predictions using the newly transformed data, but it’s not compulsory.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在一个管道中，你可以链式连接任意数量的**变换器**，使你能够顺序地应用不同的数据预处理步骤。如果愿意，你还可以在末尾添加一个**估计器**（ML模型），以便利用新变换的数据进行预测，但这不是强制性的。
- en: 'For example, you could build a pipeline that first imputes missing values with
    zeros and then one-hot encodes your variables:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，你可以构建一个管道，首先用零填补缺失值，然后对变量进行独热编码：
- en: '![](../Images/1755b225522bf4b8a22ed49b4b58cdcf.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1755b225522bf4b8a22ed49b4b58cdcf.png)'
- en: Image by author
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图像
- en: 'Or, if you wanted to directly include the modelling in the pipeline itself,
    you could build a pipeline that imputes missing values with the mean, scales the
    features and then makes predictions using a `RandomForestRegressor()`:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，如果你想直接在管道中包含建模步骤，你可以构建一个管道，该管道用均值填补缺失值、缩放特征，然后使用`RandomForestRegressor()`进行预测：
- en: '![](../Images/d2adfd6be2a9c50f3bbede182aadabd8.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d2adfd6be2a9c50f3bbede182aadabd8.png)'
- en: Image by author
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图像
- en: Building a pipeline with scikit-learn is remarkably simple.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 使用scikit-learn构建管道是非常简单的。
- en: To illustrate this, I’ll first load some data and split it into training and
    testing sets. In this example, I’ll use the [diabetes dataset](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_diabetes.html)
    provided by scikit-learn, which contains ten predictor variables (*age, sex, body
    mass index, average blood pressure, and six blood serum measurements*) for 442
    diabetes patients and a response variable representing the progression of each
    patient’s diabetes one year after these predictor variables were recorded.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明这一点，我将首先加载一些数据并将其分为训练集和测试集。在这个例子中，我将使用[糖尿病数据集](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_diabetes.html)，该数据集由scikit-learn提供，包含442名糖尿病患者的十个预测变量（*年龄、性别、体质指数、平均血压以及六个血清测量值*）和一个响应变量，表示这些预测变量记录一年后每位患者糖尿病的进展情况。
- en: '[PRE0]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '![](../Images/01d0df4d0bb03649f88d9ebe00313a71.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/01d0df4d0bb03649f88d9ebe00313a71.png)'
- en: Image by author. Diabetes dataset and scikit-learn are released under BSD-3
    license
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图像。糖尿病数据集和scikit-learn在BSD-3许可证下发布
- en: Next, we define our `Pipeline`. For now, I’ll just define a simple preprocessing
    `Pipeline` that includes two steps — impute missing values with the mean, and
    rescale all features — and I won’t include an estimator/model. The principles,
    however, are the same regardless of whether or not you include an estimator.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们定义我们的`Pipeline`。目前，我将定义一个简单的预处理`Pipeline`，包括两个步骤——用均值填补缺失值，并重新缩放所有特征——且不会包含估计器/模型。然而，无论是否包含估计器，原理都是一样的。
- en: '[PRE1]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Once we’ve defined our `Pipeline`, we “fit” it to our training dataset, and
    use it to transform both the training and testing datasets:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们定义了`Pipeline`，我们就“拟合”它到训练数据集，并用它来转换训练集和测试集：
- en: '[PRE2]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: This will give us two preprocessed datasets (`X_train_transformed` and `X_test_transformed`),
    ready for any subsequent steps like modelling or feature selection.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 这将给我们两个预处理的数据集（`X_train_transformed`和`X_test_transformed`），准备好进行建模或特征选择等后续步骤。
- en: 'The advantage of using a `Pipeline` to handle these preprocessing steps is
    twofold:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`Pipeline`来处理这些预处理步骤的优势有两个方面：
- en: '**Protect against leakage**: Because the preprocessor is fitted to the training
    dataset `X_train`, no information about the test set is “leaked” when imputing
    missing values or creating one-hot encoded features.'
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**防止信息泄露**：由于预处理器是拟合于训练数据集`X_train`的，因此在填补缺失值或创建独热编码特征时，测试集的信息不会“泄露”。'
- en: '**Avoid duplication**: If we didn’t use a `Pipeline` to handle these preprocessing
    steps, we’d end up transforming the `X_test` dataset multiple times (every time
    we wanted to apply a preprocessing step). At this small scale, the repetition
    might not seem too bad. But in complex ML workflows you can easily grow to 5,
    10, or even 20 preprocessing steps. Using a `Pipeline` makes this easy because
    we can add in as many steps as we like and still only have to transform `X_train`
    and `X_test` once:'
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**避免重复**：如果我们不使用`Pipeline`来处理这些预处理步骤，我们最终会多次转换`X_test`数据集（每次我们想应用一个预处理步骤时）。在这种小规模下，重复可能看起来不是太糟糕。但在复杂的机器学习工作流中，你很容易增加到5、10，甚至20个预处理步骤。使用`Pipeline`可以使这一过程变得简单，因为我们可以添加任意多的步骤，仍然只需对`X_train`和`X_test`进行一次转换：'
- en: '[PRE3]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '2\. ColumnTransformer: Apply separate transformers to different feature subsets'
  id: totrans-40
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 2\. ColumnTransformer：对不同特征子集应用不同的转换器
- en: In the previous example, we applied the same preprocessing steps to all features.
    But what if we had heterogenous datatypes and want to apply different preprocessors
    to different features? For example, if we only wanted to rescale the numerical
    features, or if we wanted to one-hot encode the categorical features?
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的示例中，我们对所有特征应用了相同的预处理步骤。但如果我们有异质的数据类型，并且希望对不同的特征应用不同的预处理器呢？例如，如果我们只想对数值特征进行缩放，或者如果我们想对分类特征进行独热编码？
- en: This is where `ColumnTransformer` steps in. A `ColumnTransformer` allows you
    to apply different transformers to different columns of an array or pandas DataFrame.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 这时`ColumnTransformer`派上用场了。`ColumnTransformer`允许你将不同的转换器应用于数组或 pandas DataFrame
    的不同列。
- en: In the code below, we start by defining the different groups of columns and,
    for each group, we use a `Pipeline` to build a preprocessor that will act on that
    specific group. Finally, we chain together all of the transformers in a single
    `ColumnTransformer`.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的代码中，我们首先定义了不同的列组，并且对于每个组，我们使用`Pipeline`来构建一个将作用于该特定组的预处理器。最后，我们将所有转换器链在一个`ColumnTransformer`中。
- en: '[PRE4]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '![](../Images/73a71294eda47d1fe395f59bfcae38dd.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/73a71294eda47d1fe395f59bfcae38dd.png)'
- en: Image by author
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 图片作者
- en: 'To apply the `ColumnTransformer` to our data, we use the same code as we did
    to apply our first `Pipeline`:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 要将`ColumnTransformer`应用于数据，我们使用与应用第一个`Pipeline`时相同的代码：
- en: '[PRE5]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '3\. FeatureUnion: Apply multiple transformers in parallel'
  id: totrans-49
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 3\. FeatureUnion：并行应用多个转换器
- en: '`Pipeline` and `ColumnTransformer` are awesome tools, but they have a significant
    limitation. Did you spot it?'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '`Pipeline`和`ColumnTransformer`是非常棒的工具，但它们有一个显著的限制。你发现了吗？'
- en: They can only apply transformers ***sequentially***.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 它们只能***顺序***应用转换器。
- en: In other words, when you transform a feature `Column1` using a `Pipeline`/`ColumnTransformer`,
    scikit-learn will first apply `transformer_1` to `Column1`, then apply `transformer_2`
    to the transformed version of `Column1`, and so on. This is fine for when we want
    to preprocess our data in a sequential manner (e.g. “first impute missing values,
    then one-hot encode”), but it’s not ideal in cases where we want to apply different
    preprocessing steps in parallel (e.g. “create two new features from the same underlying
    column at the same time”). In these cases, using a standard `Pipeline` or `ColumnTransformer`
    won’t suffice because the original “raw” values of `Column1` will be lost as soon
    as the first transformer in the sequence is applied.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，当你使用`Pipeline`/`ColumnTransformer`转换特征`Column1`时，scikit-learn 首先会将`transformer_1`应用于`Column1`，然后将`transformer_2`应用于`Column1`的转换版本，以此类推。这对于按顺序预处理数据是可以的（例如“首先填补缺失值，然后进行独热编码”），但在我们希望并行应用不同的预处理步骤时就不理想了（例如“同时从相同的基础列创建两个新特征”）。在这些情况下，使用标准的`Pipeline`或`ColumnTransformer`就不够用了，因为一旦序列中的第一个转换器应用，`Column1`的原始“原始”值就会丢失。
- en: 'If we want to apply multiple transformations to the same underlying features
    in parallel, we need to use another tool: `FeatureUnion`.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想对相同的基础特征应用多个变换并行，我们需要使用另一个工具：`FeatureUnion`。
- en: We can think of `FeatureUnion` as a tool that creates a “copy” of your underlying
    data, applies transformers to those copies in parallel, and then stitches the
    results together. Each transformer is passed the raw, underlying data, so we don’t
    experience the problem of sequential transformation.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将`FeatureUnion`视为一个工具，它创建了你底层数据的“副本”，并并行地对这些副本应用转换器，然后将结果拼接在一起。每个转换器都接收原始的底层数据，因此我们不会遇到顺序转换的问题。
- en: 'To use `FeatureUnion`, we just need to add a few lines of code:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用`FeatureUnion`，我们只需要添加几行代码：
- en: '[PRE6]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '![](../Images/e4dace85b9345490fdb7eea945042d0c.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e4dace85b9345490fdb7eea945042d0c.png)'
- en: Image by author
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片
- en: In this diagram, we can see that the `FeatureUnion` steps are applied in parallel,
    rather than sequentially. Just like before, we fit the `preprocessor` to our training
    data and then use it to transform any dataset we want to use for modelling/prediction.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个图示中，我们可以看到`FeatureUnion`步骤是并行应用的，而不是顺序应用的。就像之前一样，我们将`preprocessor`拟合到训练数据上，然后使用它来转换我们想用于建模/预测的任何数据集。
- en: '[PRE7]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '4\. FunctionTransformer: Seamlessly integrate feature engineering'
  id: totrans-61
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 4. `FunctionTransformer`：无缝集成特征工程
- en: All of the transformers and tools discussed above use pre-built classes in scikit-learn
    to apply standard transformations to your data (e.g., scaling, one-hot encoding,
    imputing, etc.).
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 上面讨论的所有转换器和工具都使用scikit-learn中预构建的类来对数据进行标准转换（例如，缩放、独热编码、插补等）。
- en: If you want to apply a custom function — for example during feature engineering
    — then you’ll want to use `FunctionTransformer`. Personally, I love this class
    - it makes it super easy to integrate custom functions into your `Pipeline` without
    having to write new transformer classes from scratch.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想应用自定义函数——例如在特征工程期间——那么你需要使用`FunctionTransformer`。就个人而言，我非常喜欢这个类——它使得将自定义函数集成到`Pipeline`中变得非常简单，而无需从头编写新的转换器类。
- en: 'Creating a `FunctionTransformer` is really simple. You start by defining your
    functions in the standard Pythonic style, and then create a pipeline. Here, I
    define two simple functions: one that adds together two columns, and another that
    subtracts two columns.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 创建`FunctionTransformer`非常简单。你可以按照标准的Python风格定义你的函数，然后创建一个管道。在这里，我定义了两个简单的函数：一个将两列相加，另一个将两列相减。
- en: '[PRE8]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'To simplify things even further, you could include multiple transformations
    within the same function:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进一步简化，你可以在同一个函数中包含多个转换：
- en: '[PRE9]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Finally, add the `feature_engineering` pipeline to the `preprocessing` pipeline
    we defined earlier:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，将`feature_engineering`管道添加到我们之前定义的`preprocessing`管道中：
- en: '[PRE10]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '![](../Images/509a78b53b47ac12a99a82738b0a2207.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/509a78b53b47ac12a99a82738b0a2207.png)'
- en: Image by author
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片
- en: 'And use this new pipeline to apply the same preprocessing/feature engineering
    steps to all your datasets:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 并使用这个新的管道对所有数据集应用相同的预处理/特征工程步骤：
- en: '[PRE11]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Bonus: Save your pipelines for truly reproducible workflows'
  id: totrans-74
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 附加提示：保存你的管道以实现真正可重复的工作流程
- en: In enterprise applications of machine learning, it’s very rare to only use a
    model or preprocessing workflow once. More often, you’ll be required to regularly
    rerun your model each week/month and generate new predictions for new data.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习的企业应用中，单次使用模型或预处理工作流的情况非常少见。更常见的是，你需要定期每周/月重新运行模型，并为新数据生成新的预测。
- en: 'In these situations, rather than writing a new preprocessing pipeline from
    scratch each time, you can use the same pipeline each time. To do this, once you’ve
    developed your pipeline use the `joblib` library, save the pipeline so that you
    can rerun the exact same transformations with future datasets:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些情况下，与其每次从头编写新的预处理管道，不如每次使用相同的管道。为此，在开发好管道后，使用`joblib`库保存管道，以便将来能够使用相同的转换来处理数据集：
- en: '[PRE12]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Conclusion
  id: totrans-78
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: 'To recap:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 总结：
- en: '`Pipeline` provides a quick way to sequentially apply different preprocessing
    transformers to your data'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Pipeline`提供了一种快速的方法来依次将不同的预处理转换器应用于你的数据'
- en: Using a `ColumnTransformer` is a fantastic way to sequentially apply separate
    preprocessing steps to different feature subsets
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用`ColumnTransformer`是一种绝佳的方法，可以将不同的预处理步骤依次应用于不同的特征子集
- en: '`FeatureUnion` enables you to apply different preprocessing transformations
    in parallel'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`FeatureUnion`使你能够并行应用不同的预处理转换'
- en: '`FunctionTransformer` provides a super-simple way to write custom feature engineering
    functions and integrate them within your pipelines'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`FunctionTransformer`提供了一种超级简单的方法来编写自定义特征工程函数，并将它们集成到你的管道中'
- en: If you use these tools, my promise to you is that they’ll help you write code
    that is more elegant, reproducible, and pythonic. Your Machine Learning Engineers
    will love you!
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你使用这些工具，我向你承诺它们会帮助你写出更优雅、可重复且符合 Python 风格的代码。你的机器学习工程师们会喜欢你的！
- en: If you liked this article, it would mean a lot if you followed me. If you’d
    like to get unlimited access to all of my stories (and the rest of Medium.com),
    you can sign up via my [referral link](https://medium.com/@mattchapmanmsc/membership)
    for $5 per month. It adds no extra cost to you vs. signing up via the general
    signup page, and helps to support my writing as I get a small commission.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你喜欢这篇文章，关注我将对我意义重大。如果你想无限制地访问我所有的故事（以及 Medium.com 的其他内容），你可以通过我的[推荐链接](https://medium.com/@mattchapmanmsc/membership)每月支付
    $5 注册。与通过普通注册页面注册相比，这不会增加你的额外费用，并且帮助支持我的写作，因为我会获得一小笔佣金。
- en: Thanks for reading!
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 感谢阅读！
