- en: 'Unraveling the Design Pattern of Physics-Informed Neural Networks: Part 07'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 揭示物理信息神经网络的设计模式：第 07 部分
- en: 原文：[https://towardsdatascience.com/unraveling-the-design-pattern-of-physics-informed-neural-networks-part-07-4ecb543b616a](https://towardsdatascience.com/unraveling-the-design-pattern-of-physics-informed-neural-networks-part-07-4ecb543b616a)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/unraveling-the-design-pattern-of-physics-informed-neural-networks-part-07-4ecb543b616a](https://towardsdatascience.com/unraveling-the-design-pattern-of-physics-informed-neural-networks-part-07-4ecb543b616a)
- en: Active learning for efficiently training parametric PINN
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 高效训练参数化 PINN 的主动学习
- en: '[](https://shuaiguo.medium.com/?source=post_page-----4ecb543b616a--------------------------------)[![Shuai
    Guo](../Images/d673c066f8006079be5bf92757e73a59.png)](https://shuaiguo.medium.com/?source=post_page-----4ecb543b616a--------------------------------)[](https://towardsdatascience.com/?source=post_page-----4ecb543b616a--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----4ecb543b616a--------------------------------)
    [Shuai Guo](https://shuaiguo.medium.com/?source=post_page-----4ecb543b616a--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://shuaiguo.medium.com/?source=post_page-----4ecb543b616a--------------------------------)[![Shuai
    Guo](../Images/d673c066f8006079be5bf92757e73a59.png)](https://shuaiguo.medium.com/?source=post_page-----4ecb543b616a--------------------------------)[](https://towardsdatascience.com/?source=post_page-----4ecb543b616a--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----4ecb543b616a--------------------------------)
    [Shuai Guo](https://shuaiguo.medium.com/?source=post_page-----4ecb543b616a--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----4ecb543b616a--------------------------------)
    ·8 min read·Jul 25, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----4ecb543b616a--------------------------------)
    ·阅读时长 8 分钟·2023 年 7 月 25 日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/65bd5d8fceb65306815ee058fe60f42d.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/65bd5d8fceb65306815ee058fe60f42d.png)'
- en: Photo by [Scott Graham](https://unsplash.com/@homajob?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源于 [Scott Graham](https://unsplash.com/@homajob?utm_source=medium&utm_medium=referral)
    在 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: Welcome to the 7th blog post of this series, where we continue our exciting
    journey of exploring ***design patterns*** of physics-informed neural networks
    (PINN)🙌
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 欢迎来到本系列的第 7 篇博客，我们将继续激动人心的探索***物理信息神经网络（PINN）设计模式***的旅程🙌
- en: 'In this blog, we will take a closer look at a paper that introduces **active
    learning** to PINN. As usual, we will examine the paper through the lens of design
    pattern: we will start with the target problem, followed by introducing the proposed
    method. After that, we will discuss the evaluation procedure and the advantages/disadvantages
    of the proposed method. Finally, we will conclude the blog by exploring future
    opportunities.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇博客中，我们将详细探讨一篇将**主动学习**引入 PINN 的论文。和以往一样，我们将从设计模式的角度分析这篇论文：首先，我们会介绍目标问题，然后引入提出的方法。接下来，我们将讨论评估过程以及该方法的优缺点。最后，我们将通过探索未来的机会来总结博客内容。
- en: 'As this series continues to expand, the collection of PINN design patterns
    grows even richer! Here’s a sneak peek at what awaits you:'
  id: totrans-10
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 随着系列的不断扩展，PINN 设计模式的集合也变得更加丰富！以下是一些即将呈现的内容预览：
- en: ''
  id: totrans-11
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[PINN design pattern 01: Optimizing the residual point distribution](/unraveling-the-design-pattern-of-physics-informed-neural-networks-series-01-8190df459527)'
  id: totrans-12
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '[PINN 设计模式 01: 优化残差点分布](/unraveling-the-design-pattern-of-physics-informed-neural-networks-series-01-8190df459527)'
- en: ''
  id: totrans-13
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[PINN design pattern 02: Dynamic solution interval expansion](/unraveling-the-design-pattern-of-physics-informed-neural-networks-part-02-2156516f2791)'
  id: totrans-14
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '[PINN 设计模式 02: 动态解决方案区间扩展](/unraveling-the-design-pattern-of-physics-informed-neural-networks-part-02-2156516f2791)'
- en: ''
  id: totrans-15
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[PINN design pattern 03: Training PINN with gradient boosting](https://medium.com/towards-data-science/unraveling-the-design-pattern-of-physics-informed-neural-networks-part-03-fe365ef480d9)'
  id: totrans-16
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '[PINN 设计模式 03: 使用梯度提升训练 PINN](https://medium.com/towards-data-science/unraveling-the-design-pattern-of-physics-informed-neural-networks-part-03-fe365ef480d9)'
- en: ''
  id: totrans-17
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[PINN design pattern 04: Gradient-enhanced PINN learning](https://medium.com/towards-data-science/unraveling-the-design-pattern-of-physics-informed-neural-networks-part-04-c778f4829dde)'
  id: totrans-18
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '[PINN 设计模式 04: 梯度增强 PINN 学习](https://medium.com/towards-data-science/unraveling-the-design-pattern-of-physics-informed-neural-networks-part-04-c778f4829dde)'
- en: ''
  id: totrans-19
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[PINN design pattern 05: Automated hyperparameter tuning](/unraveling-the-design-pattern-of-physics-informed-neural-networks-part-05-67a35a984b23)'
  id: totrans-20
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '[PINN 设计模式 05: 自动超参数调优](/unraveling-the-design-pattern-of-physics-informed-neural-networks-part-05-67a35a984b23)'
- en: ''
  id: totrans-21
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[PINN design pattern 06: Causal PINN training](https://medium.com/towards-data-science/unraveling-the-design-pattern-of-physics-informed-neural-networks-part-06-bcb3557199e2)'
  id: totrans-22
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '[PINN设计模式06：因果PINN训练](https://medium.com/towards-data-science/unraveling-the-design-pattern-of-physics-informed-neural-networks-part-06-bcb3557199e2)'
- en: Let’s dive in!
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们深入了解吧！
- en: 1\. Paper at a glance 🔍
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 1\. 论文概述 🔍
- en: '**Title**: Active training of physics-informed neural networks to aggregate
    and interpolate parametric solutions to the Navier-Stokes equations'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**标题**：主动训练物理信息神经网络以聚合和插值Navier-Stokes方程的参数化解'
- en: '**Authors**: C. A., Arthurs, A. P. King'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**作者**：C. A., Arthurs, A. P. King'
- en: '**Institutes**: King’s College London'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**机构**：伦敦国王学院'
- en: '**Link**: [Journal of Computational Physics](https://www.sciencedirect.com/science/article/pii/S002199912100259X)'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**链接**：[计算物理学期刊](https://www.sciencedirect.com/science/article/pii/S002199912100259X)'
- en: 2\. Design pattern 🎨
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 2\. 设计模式 🎨
- en: 2.1 Problem 🎯
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.1 问题 🎯
- en: One of the prime uses of PINNs is to ***surrogate*** high-fidelity, time-consuming
    numerical simulations (e.g., FEM simulations for structural dynamics). Thanks
    to the strong regularizations enforced by the known governing differential equations
    (represented as an extra loss term), PINNs’ training typically only requires minimal
    data gathered from just a handful of simulation runs.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: PINN的主要用途之一是***替代***高保真度、耗时的数值模拟（例如，结构动力学的FEM模拟）。得益于已知控制微分方程强有力的正则化（表现为额外的损失项），PINN的训练通常只需从少量模拟运行中收集的最少数据。
- en: However, this ideal scenario is limited to cases where the problem under investigation
    does not involve variable parameters. In practical applications, we frequently
    need to infer solutions for different domain geometries, model parameters (e.g.,
    material properties), and initial and/or boundary conditions. We certainly don’t
    want to retrain a new PINN for each distinct scenario, as that would be highly
    inefficient and computationally costly.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这种理想的情况仅限于研究的问题不涉及变量参数的情况。在实际应用中，我们经常需要推断不同领域几何形状、模型参数（例如，材料属性）以及初始和/或边界条件的解。我们肯定不希望为每种不同的情况重新训练一个新的PINN，因为这会非常低效且计算成本高。
- en: 'One strategy to address this challenge is to upgrade the vanilla PINNs to **parametric
    PINNs**: in essence, the variable parameters are treated as additional inputs
    to the PINN, as depicted in the figure below. Once trained, we can use parametric
    PINN to make instant predictions for any given conditions.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 解决这个挑战的一种策略是将普通PINN升级为**参数化PINN**：本质上，变量参数被视为PINN的额外输入，如下图所示。一旦训练完成，我们可以使用参数化PINN在任何给定条件下进行即时预测。
- en: '![](../Images/1049373831d730dae37e7ab666a1755d.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1049373831d730dae37e7ab666a1755d.png)'
- en: Figure 1\. Compared to the vanilla PINN, which only accepts spatial/temporal
    coordinates, parametric PINN also takes the variable parameters as inputs. (Image
    by this blog author)
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 图1\. 与仅接受空间/时间坐标的普通PINN相比，参数化PINN还将变量参数作为输入。（图片由本博客作者提供）
- en: 'As expected, however, more inputs calls for more training data. Since generating
    new training data implies running time-consuming simulations, a crucial question
    arises: how can we minimize the simulation runs (thus being more data-efficient)
    for PINN training, while reaching the desired prediction accuracy?'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 如预期的那样，更多的输入要求更多的训练数据。由于生成新训练数据意味着运行耗时的模拟，因此一个关键问题出现了：如何在达到期望的预测准确度的同时，最小化模拟运行次数（从而提高数据效率）以进行PINN训练？
- en: 2.2 Solution 💡
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.2 解决方案 💡
- en: The key to answering this question lies in the strategic selection of points
    in the parameter space (i.e., **θ** space) to run the simulations, and the solution
    brought forth by the paper is **active learning**.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 回答这个问题的关键在于战略性地选择参数空间（即**θ**空间）中的点来运行模拟，而论文提出的解决方案是**主动学习**。
- en: '![](../Images/6bcf8135bb99db1d52235180d1849411.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6bcf8135bb99db1d52235180d1849411.png)'
- en: Figure 2\. Active learning workflow. For PINN training, the “Label” step corresponds
    to running the numerical simulation at the selected sample locations in the parameter
    space. (Image by this blog author)
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 图2\. 主动学习工作流程。对于PINN训练，“标签”步骤对应于在参数空间中选择的样本位置运行数值模拟。（图片由本博客作者提供）
- en: 'Specific to the applications in addressing parametric PDEs, the proposed workflow
    can be illustrated in the figure below:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 针对解决参数化PDE的应用，所提出的工作流程可以在下图中说明：
- en: '![](../Images/3d34a29f4b245d6aef670b5543d36d42.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3d34a29f4b245d6aef670b5543d36d42.png)'
- en: Figure 3\. Illustration of the active learning pipeline proposed in the paper.
    Here, *θ*₁ and *θ*₂ denote variable parameters (in practice there can be many
    more such variable parameters), and u represents the physical quantity or quantities
    we are trying to simulate (e.g., in a fluid simulation, u can be velocity, pressure,
    etc.) (Image by this blog author)
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 图3\. 论文中提出的主动学习流程示意图。在这里，*θ*₁和*θ*₂表示变量参数（在实际应用中，可能有更多这样的变量参数），而u表示我们尝试模拟的物理量（例如，在流体仿真中，u可以是速度、压力等）（图片由本博客作者提供）
- en: The proposed active learning pipeline starts with randomly sampling the parameter
    space (in the illustration, *θ*₁ and *θ*₂ are the variable parameters) and running
    numerical simulations to gather the training data (i.e., *u*, the physical quantity
    being modeled, at various spatial/temporal locations).
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 提出的主动学习流程从随机采样参数空间开始（在图示中，*θ*₁和*θ*₂是变量参数），并进行数值仿真以收集训练数据（即在不同空间/时间位置的*u*，即所建模的物理量）。
- en: Then, we proceed with the usual PINN training, where the neural network predictions
    are required to not only match the collected simulation results *u* but also satisfy
    the governing differential equations.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们继续进行常规的PINN训练，其中神经网络的预测不仅需要与收集的模拟结果*u*匹配，还需要满足主导的微分方程。
- en: Once the training has converged, we would obtain a PINN model that can accurately
    predict *u* within the simulation spatial/temporal domain, for *θ*₁-*θ*₂ values
    included in the training dataset. However, up till now, there is no guarantee
    that for unseen *θ*₁-*θ*₂ value combinations, the PINN can also deliver accurate
    predictions.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦训练收敛，我们将获得一个能够在仿真空间/时间域内准确预测*u*的PINN模型，前提是*θ*₁-*θ*₂值包含在训练数据集中。然而，到目前为止，对于未见过的*θ*₁-*θ*₂值组合，没有保证PINN也能提供准确的预测。
- en: That’s why we kick off the active learning process. Our goal is to select the
    next *θ*₁-*θ*₂ sample to run the simulation. **Ideally, we want this new *θ*₁-*θ*₂
    sample to be the most informative and once its associated simulation results are
    appended to the training data, it can bring the most improvement in PINN’s accuracy.**
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是为什么我们启动主动学习过程的原因。我们的目标是选择下一个*θ*₁-*θ*₂样本来进行仿真。**理想情况下，我们希望这个新的*θ*₁-*θ*₂样本能够提供最多的信息，一旦将其相关的模拟结果附加到训练数据中，它可以带来PINN准确性的最大提升。**
- en: 'So how should we select the new *θ*₁-*θ*₂ sample? The paper proposed a simple
    criterion: it first uses the currently trained PINN to predict *u* for all candidate
    *θ*₁-*θ*₂ samples in the parameter space (those candidate samples can be pre-generated
    in large quantities to evenly fill the space), then it calculates the corresponding
    PDE residuals for different candidate *θ*₁-*θ*₂ samples. Finally, **the next *θ*₁-*θ*₂
    sample is chosen as the sample with the highest PDE residuals.**'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 那么我们应该如何选择新的*θ*₁-*θ*₂样本呢？论文提出了一个简单的标准：首先，使用当前训练好的PINN预测参数空间中所有候选*θ*₁-*θ*₂样本的*u*（这些候选样本可以预先大量生成以均匀填充空间），然后计算不同候选*θ*₁-*θ*₂样本的相应PDE残差。最后，**选择具有最高PDE残差的下一个*θ*₁-*θ*₂样本。**
- en: In other words, the paper proposed to use *PDE residuals* as the indicator for
    PINN prediction accuracy. Since the currently trained PINN has the most difficulty
    in accurately predicting *u* (that fulfills the known differential equation)at
    the selected *θ*₁-*θ*₂ location, it makes sense to then run the time-consuming
    simulation at the selected *θ*₁-*θ*₂ location and append the original training
    data with the simulated results. Once the training data is enriched, we start
    another round of PINN training, thus completing an iteration of active learning.
    The entire process terminates when the calculated maximum PDE residuals drop below
    a pre-defined threshold.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，论文建议使用*PDE残差*作为PINN预测准确性的指标。由于当前训练的PINN在选定的*θ*₁-*θ*₂位置最难准确预测满足已知微分方程的*u*，因此在选定的*θ*₁-*θ*₂位置运行耗时的仿真，并将模拟结果附加到原始训练数据中是有意义的。一旦训练数据得到丰富，我们就开始另一轮PINN训练，从而完成一次主动学习的迭代。整个过程在计算的最大PDE残差降到预定义的阈值以下时终止。
- en: 2.3 Why the solution might work 🛠️
  id: totrans-50
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.3 为什么解决方案可能有效 🛠️
- en: Active learning is a well-established technique for reducing model training
    costs. It is effective because it identifies the regions in the parameter space
    where the current PINN’s predictions are less satisfactory, and later chooses
    which points to simulate next accordingly such that the model improvement can
    be maximized. This way, active learning can drastically improve data efficiency
    while ensuring the desired prediction accuracy can be reached.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 主动学习是一种成熟的技术，用于减少模型训练成本。它之所以有效，是因为它能够识别当前PINN预测不够满意的参数空间区域，然后相应地选择下一步要模拟的点，以最大化模型的改进。通过这种方式，主动学习可以显著提高数据效率，同时确保达到所需的预测准确性。
- en: 2.4 Benchmark ⏱️
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.4 基准测试 ⏱️
- en: 'Unlike other PINN papers, the current paper solely focused on solving parametric
    steady, incompressible Navier-Stokes equations in a continuous set of 2D tubular
    domains:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 与其他PINN论文不同，当前论文专注于解决连续2D管道域中的参数稳态不可压Navier-Stokes方程：
- en: '![](../Images/12461ce9bd9f065de5590aa598adc368.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/12461ce9bd9f065de5590aa598adc368.png)'
- en: Two variable parameters are considered, including a flow boundary condition,
    i.e., the inflow rate, as well as a domain shape parameter.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑了两个变量参数，包括流动边界条件，即流入速率，以及域形状参数。
- en: The paper demonstrated that the PINN trained with the proposed active learning
    strategy can accurately and efficiently predict the flow field for any given values
    of inflow rate and domain shape within their considered variational ranges.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 论文展示了，通过提出的主动学习策略训练的PINN可以准确高效地预测给定流入速率和域形状值的流场，且这些值都在考虑的变动范围内。
- en: The paper also demonstrated that using the proposed active learning strategy
    requires much fewer simulation runs compared to random or uniform data selection
    within the considered parameter space, while achieving a far smaller error in
    flow field prediction.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 该论文还展示了，使用提出的主动学习策略相比于在所考虑的参数空间内进行随机或均匀数据选择，需要的模拟次数要少得多，同时在流场预测中的误差也要小得多。
- en: 'Finally, the paper showcased one practical usage of the trained parametric
    PINN: parameter sweeping. More specifically, the trained PINN can be used to search
    the parameter space and identify the values of the two parameters when the resulting
    flow field satisfies the given conditions.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，论文展示了训练后的参数化PINN的一个实际应用：参数扫描。更具体地说，训练后的PINN可以用于搜索参数空间，并识别当流场满足给定条件时两个参数的值。
- en: 2.5 Strengths and Weaknesses ⚡
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.5 优势与劣势 ⚡
- en: '**Strengths** 💪'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '**优点** 💪'
- en: Dramatically reduce the simulation data needed to train a *parametric* PINN
    model with desired prediction accuracy.
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 大幅减少训练具有所需预测准确性的*参数*PINN模型所需的模拟数据。
- en: The trained PINN model can provide extremely fast approximations to PDE solutions
    across a region of parameter space.
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练后的PINN模型能够在参数空间的区域内提供极快的PDE解决方案近似值。
- en: The trained PINN is highly flexible, as it can be adapted to a different parameter
    space without the need for comprehensive retraining. This is possible because
    active learning allows the model to be easily updated as new rounds of simulations
    are performed in the extended parameter space.
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练后的PINN具有高度灵活性，因为它可以在不需要全面重新训练的情况下适应不同的参数空间。这是可能的，因为主动学习允许模型随着新的模拟回合在扩展参数空间中被轻松更新。
- en: No need to store a large number of simulation results, as the trained parametric
    PINN can generate the flow field results on demand. Since a typical neural network
    only consumes a small amount of storage space, the potential data compression
    is significant.
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不需要存储大量模拟结果，因为训练后的参数化PINN可以按需生成流场结果。由于典型的神经网络只消耗少量存储空间，因此潜在的数据压缩是显著的。
- en: '**Weaknesses** 📉'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '**劣势** 📉'
- en: The effectiveness of the proposed active training pipeline may be dependent
    on the initial data. If the initial data set does not adequately represent the
    underlying behavior of the system, the model may struggle to identify areas of
    potential improvement, thus leading to a suboptimal selection of the following
    simulations.
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提出的主动训练流程的有效性可能取决于初始数据。如果初始数据集不能充分代表系统的基本行为，模型可能难以识别潜在的改进区域，从而导致后续模拟的选择不理想。
- en: Additionally, the proposed approach uses the PDE residuals as the indication
    for the model prediction accuracy. However, for cases when the model prediction
    accuracy does not correlate well with the PDE residuals, the effectiveness of
    the proposed approach will likely be discounted.
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 此外，所提的方法使用PDE残差作为模型预测精度的指示。然而，对于模型预测精度与PDE残差不相关的情况，所提方法的有效性可能会被打折扣。
- en: Currently, the paper only considered two variable parameters. However, active
    learning may become computationally expensive as the dimensionality of the problem
    increases. Therefore, the scalability of the proposed approach remains to be investigated.
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 目前，本文仅考虑了两个变量参数。然而，随着问题维度的增加，主动学习可能变得计算昂贵。因此，所提方法的可扩展性仍需进一步研究。
- en: 2.6 Alternatives 🔀
  id: totrans-69
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.6 替代方案 🔀
- en: Alternative to using an active learning scheme, the samples (which are used
    to run simulations to generate training data) can also be generated by adopting
    quasi-random sampling methods. Popular quasi-random sampling methods include Latin
    hypercube sampling (LHS), Sobol sequences, and Halton sequences. Compared to a
    simple random sampling scheme, the quasi-random sampling approaches have better
    *space-filling* properties, meaning that fewer samples are required to evenly
    cover the investigated parameter space.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 替代使用主动学习方案，样本（用于运行模拟以生成训练数据）也可以通过采用准随机采样方法生成。流行的准随机采样方法包括拉丁超立方体采样（LHS）、Sobol序列和Halton序列。与简单随机采样方案相比，准随机采样方法具有更好的*空间填充*特性，这意味着需要更少的样本来均匀覆盖所研究的参数空间。
- en: However, compared to the active learning approach proposed in the paper, the
    quasi-random sampling schemes would require the user to specify a sample number
    upfront, therefore they are less flexible in terms of enriching the training dataset
    when new simulation data is available. Nevertheless, active learning would introduce
    extra computational costs due to the necessity of estimating the model’s performance
    and determining the most informative points for performing the next simulation
    run.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，与论文中提出的主动学习方法相比，准随机采样方案需要用户预先指定样本数量，因此在新模拟数据可用时，在丰富训练数据集方面的灵活性较差。然而，主动学习会引入额外的计算成本，因为需要估计模型的性能并确定最具信息量的点以进行下一次模拟运行。
- en: 3 Potential Future Improvements 🌟
  id: totrans-72
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 3个潜在的未来改进 🌟
- en: 'There are several possibilities to further improve the proposed strategy:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 有几个可能的方向可以进一步改进所提的策略：
- en: Leveraging more sophisticated active learning algorithms to automate the selection
    of training data points. This may further improve the accuracy of the predictions
    and reduce the amount of time needed for training.
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 利用更复杂的主动学习算法来自动选择训练数据点。这可能进一步提高预测的准确性，并减少训练所需的时间。
- en: Developing strategies to allow sampling of multiple data points in each active
    learning iteration. As the corresponding simulations can be run in parallel, the
    overall training efficiency can be greatly improved.
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 开发策略以允许在每次主动学习迭代中采样多个数据点。由于相应的模拟可以并行运行，因此整体训练效率可以大大提高。
- en: Integrating other proven best practices of training PINN (collocation point
    sampling, casual training, ensemble learning, etc.) with the proposed active learning
    scheme to further improve the model performance.
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将其他经过验证的最佳实践（如协同点采样、因果训练、集成学习等）与所提的主动学习方案整合，以进一步提升模型性能。
- en: 4 Takeaways 📝
  id: totrans-77
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 4个要点 📝
- en: 'In this blog post, we looked at how to perform data-efficient, parametric PINN
    training with active learning. Here are the highlights of the design pattern proposed
    in the paper:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇博客中，我们探讨了如何使用主动学习进行数据高效的参数化PINN训练。以下是论文中提出的设计模式的亮点：
- en: '[Problem]: How to train parametric PINNs with as few data samples as possible.'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[问题]：如何用尽可能少的数据样本训练参数化PINN。'
- en: '[Solution]: **Training PINN with active learning**, where data are collected
    adaptively to maximize the improvement in PINN’s prediction accuracy.'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[解决方案]：**使用主动学习训练PINN**，在这种方法中，数据会被自适应地收集，以最大化PINN预测精度的提升。'
- en: '[Potential benefits]: 1\. Significantly reduced the training cost of parametric
    PINNs. 2\. Trained parametric PINN provides fast approximations to PDE solutions
    across a region of parameter space.'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[潜在好处]：1. 显著降低了参数化PINN的训练成本。2. 训练后的参数化PINN在参数空间的区域内提供了对PDE解的快速近似。'
- en: 'Here is the PINN design card to summarize the takeaways:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是总结要点的PINN设计卡：
- en: '![](../Images/a8c75b852264307afc8ab527e969f81a.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a8c75b852264307afc8ab527e969f81a.png)'
- en: PINN design pattern proposed in the paper. (Image by this blog author)
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 论文中提出的 PINN 设计模式。（图片由本博客作者提供）
- en: Reference 📑
  id: totrans-85
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献 📑
- en: '[1] Arthurs et al., Active training of physics-informed neural networks to
    aggregate and interpolate parametric solutions to the Navier-Stokes equations,
    [Journal of Computational Physics](https://www.sciencedirect.com/science/article/pii/S002199912100259X),
    2021.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] Arthurs 等人，《物理信息神经网络的主动训练以聚合和插值 Navier-Stokes 方程的参数解》，[计算物理学期刊](https://www.sciencedirect.com/science/article/pii/S002199912100259X)，2021年。'
