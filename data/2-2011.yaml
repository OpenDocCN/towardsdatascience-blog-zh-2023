- en: 'From Basic Gates to Deep Neural Networks: The Definitive Perceptron Tutorial'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从基础逻辑门到深度神经网络：权威感知机教程
- en: 原文：[https://towardsdatascience.com/the-definitive-perceptron-guide-fd384eb93382](https://towardsdatascience.com/the-definitive-perceptron-guide-fd384eb93382)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/the-definitive-perceptron-guide-fd384eb93382](https://towardsdatascience.com/the-definitive-perceptron-guide-fd384eb93382)
- en: Towards mastering AI
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 朝着掌握AI的方向前进
- en: Mathematics, binary classification, logic gates, and more
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数学、二分类、逻辑门等
- en: '[](https://jvision.medium.com/?source=post_page-----fd384eb93382--------------------------------)[![Joseph
    Robinson, Ph.D.](../Images/3117b65a4e10752724585d3457343695.png)](https://jvision.medium.com/?source=post_page-----fd384eb93382--------------------------------)[](https://towardsdatascience.com/?source=post_page-----fd384eb93382--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----fd384eb93382--------------------------------)
    [Joseph Robinson, Ph.D.](https://jvision.medium.com/?source=post_page-----fd384eb93382--------------------------------)'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://jvision.medium.com/?source=post_page-----fd384eb93382--------------------------------)[![Joseph
    Robinson, Ph.D.](../Images/3117b65a4e10752724585d3457343695.png)](https://jvision.medium.com/?source=post_page-----fd384eb93382--------------------------------)[](https://towardsdatascience.com/?source=post_page-----fd384eb93382--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----fd384eb93382--------------------------------)
    [Joseph Robinson, Ph.D.](https://jvision.medium.com/?source=post_page-----fd384eb93382--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----fd384eb93382--------------------------------)
    ·21 min read·Apr 28, 2023
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----fd384eb93382--------------------------------)
    ·阅读时间21分钟·2023年4月28日
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: TL;DR
  id: totrans-7
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: TL;DR
- en: The world of perceptrons is captivating, as these models are the foundation
    of modern artificial intelligence. In this blog post, we will provide a concise
    version of the story of the perceptron, from its neural network origins to its
    evolution into the multilayer perceptron and beyond. We will explore the basic
    mathematics that powers this model, allowing it to function as a binary classifier,
    simulated computer transistor, multiplier, and logic gate. Additionally, we will
    examine how the perceptron model laid the groundwork for more advanced classifiers,
    including logistic regression, SVM, and deep learning. We will provide sample
    code snippets and illustrations to enhance our comprehension. Furthermore, we
    will use practical use cases to understand when and where the perceptron model
    should be utilized.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 感知机的世界令人着迷，因为这些模型是现代人工智能的基础。在这篇博客文章中，我们将简明扼要地讲述感知机的故事，从它的神经网络起源到其演变为多层感知机及更高级的模型。我们将探讨驱动这个模型的基本数学，使其能够作为二分类器、模拟计算机晶体管、乘法器和逻辑门。此外，我们还将考察感知机模型如何为更高级的分类器奠定基础，包括逻辑回归、支持向量机和深度学习。我们将提供示例代码片段和插图以增强理解。此外，我们还将使用实际案例来了解何时以及如何使用感知机模型。
- en: This guide is a valuable resource for anyone interested in data science, regardless
    of their level of expertise. We will explore the perceptron model, which has existed
    since the early days of AI and continues to be relevant today. We will delve into
    its history, how it operates, and how it compares to other models. Additionally,
    we will build models and gates and provide insights into where the field may be
    headed. Whether you are a self-taught data scientist, an AI practitioner, or an
    experienced professional in machine learning, you'll find something of value in
    this comprehensive guide.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 本指南是任何对数据科学感兴趣的人的宝贵资源，无论其专业水平如何。我们将探讨感知机模型，这一模型自人工智能早期便存在，并且至今依然相关。我们将深入了解其历史、工作原理及与其他模型的比较。此外，我们还将构建模型和逻辑门，并提供对未来发展的见解。无论你是自学的数据科学家、AI从业者还是有经验的机器学习专业人士，你都会在这本全面的指南中找到有价值的内容。
- en: Table of Contents
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 目录
- en: '[1\. Introduction](#cded)'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '[1\. 介绍](#cded)'
- en: '[1.1 A Brief History of the Perceptron Model](#3a63)'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '[1.1 感知机模型的简史](#3a63)'
- en: '[1.2\. The Importance of the Perceptron Model in Machine Learning](#3213)'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '[1.2\. 感知机模型在机器学习中的重要性](#3213)'
- en: '[2\. The Mathematics Behind the Perceptron Model](#794d)'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '[2\. 感知机模型背后的数学](#794d)'
- en: '[2.1\. Linear Separability](#84d1)'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '[2.1\. 线性可分性](#84d1)'
- en: '[2.2\. The Perceptron Learning Algorithm](#12bc)'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '[2.2\. 感知机学习算法](#12bc)'
- en: '[2.3\. The Perceptron Convergence Theorem](#5e61)'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '[2.3\. 感知机收敛定理](#5e61)'
- en: '[3\. The Perceptron Model as a Binary Classifier](#424d)'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '[3\. 感知机模型作为二分类器](#424d)'
- en: '[3.1\. Linear Classification](#865e)'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '[3.1\. 线性分类](#865e)'
- en: '[3.2\. Limitations of the Perceptron Model](#f383)'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '[3.2\. 感知器模型的局限性](#f383)'
- en: '[3.3\. Multi-class Classification with the Perceptron Model](#d784)'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '[3.3\. 感知器模型的多类分类](#d784)'
- en: '[4\. Logic Gates and the Perceptron Model](#f223)'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '[4\. 逻辑门与感知器模型](#f223)'
- en: '[4.1\. How Perceptrons Can Be Used to Generate Logic Gates](#fe2d)'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '[4.1\. 感知器如何用于生成逻辑门](#fe2d)'
- en: '[4.2\. Example: Implementing a NAND Gate Using a Perceptron](#2465)'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '[4.2\. 示例：使用感知器实现NAND门](#2465)'
- en: '[4.3\. Extending to Other Logic Gates: AND, OR, XOR](#1428)'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '[4.3\. 扩展到其他逻辑门：AND、OR、XOR](#1428)'
- en: '[5\. Perceptrons for Multiplication and Transistor-like Functionality](#fdba)'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '[5\. 感知器用于乘法和晶体管类似功能](#fdba)'
- en: '[5.1\. Analogies Between Perceptrons and Transistors](#cc7c)'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '[5.1\. 感知器与晶体管的类比](#cc7c)'
- en: '[5.2\. Performing Multiplication with Perceptrons](#d860)'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '[5.2\. 使用感知器进行乘法](#d860)'
- en: '[5.3\. The Future of Perceptrons and Hardware Implementation](#a2e8)'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '[5.3\. 感知器的未来与硬件实现](#a2e8)'
- en: '[6\. Comparing the Perceptron Model to Logistic Regression](#e6c4)'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '[6\. 比较感知器模型与逻辑回归](#e6c4)'
- en: '[6.1\. Similarities Between Perceptron and Logistic Regression](#3b99)'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '[6.1\. 感知器与逻辑回归的相似性](#3b99)'
- en: '[6.2\. Differences Between Perceptron and Logistic Regression](#f1c3)'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '[6.2\. 感知器与逻辑回归的差异](#f1c3)'
- en: '[6.3\. Choosing Between Perceptron and Logistic Regression](#75d4)'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '[6.3\. 在感知器与逻辑回归之间的选择](#75d4)'
- en: '[7\. Creative and Unique Applications of the Perceptron Model](#6e58)'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '[7\. 感知器模型的创意与独特应用](#6e58)'
- en: '[7.1\. Optical Character Recognition (OCR)](#1fc1)'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '[7.1\. 光学字符识别（OCR）](#1fc1)'
- en: '[7.2\. Music Genre Classification](#79b2)'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '[7.2\. 音乐类型分类](#79b2)'
- en: '[7.3\. Intrusion Detection Systems](#64a0)'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '[7.3\. 入侵检测系统](#64a0)'
- en: '[7.4\. Sentiment Analysis](#518f)'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '[7.4\. 情感分析](#518f)'
- en: '[8\. The Evolution of the Perceptron Model and Legacy in Deep Learning](#e342)'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '[8\. 感知器模型的演变及其在深度学习中的遗产](#e342)'
- en: '[8.1\. The Evolution of Perceptrons to Multi-Layer Perceptrons (MLPs)](#5396)'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '[8.1\. 感知器到多层感知器（MLPs）的演变](#5396)'
- en: '[8.2\. Deep Learning and Perceptron’s Legacy](#077c)'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '[8.2\. 深度学习与感知器的遗产](#077c)'
- en: '[8.3\. The Future of Perceptrons and Deep Learning](#3762)'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '[8.3\. 感知器与深度学习的未来](#3762)'
- en: 9\. [Conclusion](#236b)
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 9\. [结论](#236b)
- en: · [References](#3a73)
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: · [参考文献](#3a73)
- en: · [Contact](#2117)
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: · [联系方式](#2117)
- en: 1\. Introduction
  id: totrans-46
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 1\. 介绍
- en: 1.1 A Brief History of the Perceptron Model
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.1 感知器模型的简史
- en: Warren McCulloch and Walter Pitts' work on artificial neurons in 1943 [1] inspired
    a psychologist named Frank Rosenblatt to make the perceptron model in 1957 [2].
    Rosenblatt's perceptron was the first neural network (NN) to be described with
    an algorithm, paving the way for modern techniques for machine learning (ML).
    Upon its discovery, the perceptron got much attention from scientists and the
    general public. Some saw this new technology as essential for intelligent machines—a
    model for learning and changing [3].
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: Warren McCulloch和Walter Pitts在1943年的人工神经元研究[1]启发了一位名叫Frank Rosenblatt的心理学家在1957年制造了感知器模型[2]。Rosenblatt的感知器是第一个用算法描述的神经网络（NN），为现代机器学习（ML）技术铺平了道路。发现后，感知器受到了科学家和公众的广泛关注。有些人认为这一新技术对智能机器至关重要——一个学习和改变的模型[3]。
- en: However, the perceptron’s popularity did not persist. Then, in 1969, Marvin
    Minsky and Seymour Papert published their book, “Perceptrons,” which highlighted
    the limitations of the perceptron model while revealing that it could not solve
    problems like the XOR classification [4] (Section 3). This work triggered a significant
    loss of interest in NNs, turning their attention to other methods. The early years
    of the perceptron are listed in **Fig. 1**.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，感知器的受欢迎程度并没有持续。然后，在1969年，Marvin Minsky和Seymour Papert出版了他们的书《感知器》，书中强调了感知器模型的局限性，同时揭示了它无法解决像XOR分类这样的难题[4]（第3节）。这项工作引发了对神经网络的重大兴趣丧失，转而关注其他方法。感知器的早期历程列于**图1**。
- en: '![](../Images/c2a6184cd79ca03b2ba4e583233adc83.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c2a6184cd79ca03b2ba4e583233adc83.png)'
- en: '**Fig 1\.** Significant milestones in the history of the perceptron (1943–1982).
    Figure created by the author.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '**图1\.** 感知器历史上的重要里程碑（1943–1982）。图由作者创作。'
- en: It took over a decade, but the 1980s saw interest in NNs rekindle. Many thanks,
    in part, for introducing multilayer NN training via the back-propagation algorithm
    by Rumelhart, Hinton, and Williams [5] (Section 5).
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然花费了十多年时间，但1980年代对神经网络的兴趣得以重新点燃。部分感谢于Rumelhart、Hinton和Williams通过反向传播算法引入的多层神经网络训练[5]（第5节）。
- en: In 2012, significant developments in computing power, big data, non-linear activations
    like RELU, and dropout techniques led to the creation of the most comprehensive
    convolutional neural networks. The large labeled dataset provided by ImageNet
    was instrumental in filling its capacity.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 2012年，计算能力、大数据、RELU等非线性激活以及dropout技术的重大进展促成了最全面的卷积神经网络的诞生。ImageNet提供的大型标注数据集在填充其容量方面发挥了重要作用。
- en: '![](../Images/6690b121315e96ace058902793d46ee0.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6690b121315e96ace058902793d46ee0.png)'
- en: '**Fig 2\.** Significant milestones in the history of the perceptron (1985–1997).
    Figure created by the author.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 2\.** 感知机历史上的重要里程碑（1985–1997）。图由作者绘制。'
- en: Out came the rise of today's frenzy for deep learning. Hence, the perceptron
    model plays a pivotal role in their foundation—**Fig. 2** and **3** list the remaining
    milestones (continuation of **Fig. 1**).
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 今天对深度学习的狂热由此兴起。因此，感知机模型在其基础中扮演了关键角色—**图 2**和**图 3**列出了剩余的里程碑（**图 1**的延续）。
- en: '![](../Images/864ec1ba2971d7a281ad9b4709fade39.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/864ec1ba2971d7a281ad9b4709fade39.png)'
- en: '**Fig 3\.** Significant milestones in the history of the perceptron (2006–2018).
    Figure created by the author.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 3\.** 感知机历史上的重要里程碑（2006–2018）。图由作者绘制。'
- en: 1.2\. The Importance of the Perceptron Model in Machine Learning
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.2\. 感知机模型在机器学习中的重要性
- en: Despite its limitations, the perceptron model remains an essential building
    block in ML. It is a fundamental part of artificial neural networks, which are
    now used in many different ways, from recognizing images to figuring out what
    people say.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管有其局限性，感知机模型仍然是机器学习中的一个重要构建块。它是人工神经网络的基础组成部分，现在这些网络被广泛用于各种应用，从图像识别到理解人类语言。
- en: The simplicity of the perceptron model makes it a great place to start for people
    new to machine learning. It makes linear classification and learning from data
    easy to understand. Also, the perceptron algorithm can be easily changed to create
    more complex models, such as multilayer perceptrons (MLP) and support vector machines
    (SVMs), which can be used in more situations and solve many problems with the
    original perceptron model.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 感知机模型的简洁性使其成为新手入门机器学习的绝佳起点。它使线性分类和从数据中学习变得易于理解。此外，感知机算法可以很容易地修改以创建更复杂的模型，例如多层感知机（MLP）和支持向量机（SVM），这些模型可以在更多情况下使用，并解决许多原始感知机模型无法解决的问题。
- en: In the following sections, we'll cover the math behind the perceptron model,
    how it can be used as a binary classifier and to make logic gates, and how it
    can be used to do multiplication tasks like a computer's transistors. We'll also
    talk about the differences between the perceptron model and logistic regression
    and show how the perceptron model can be used in new and exciting ways.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的部分，我们将介绍感知机模型背后的数学，如何将其用作二分类器和构建逻辑门，以及如何用于执行类似计算机晶体管的乘法任务。我们还将讨论感知机模型与逻辑回归之间的区别，并展示感知机模型如何以新颖和令人兴奋的方式使用。
- en: 2\. The Mathematics Behind the Perceptron Model
  id: totrans-63
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 2\. 感知机模型背后的数学
- en: 2.1\. Linear Separability
  id: totrans-64
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.1\. 线性可分性
- en: At its core, the perceptron model is a linear classifier. It aims to find a
    "hyperplane" (a line in two-dimensional space, a plane in three-dimensional space,
    or a higher-dimensional analog) separating two data classes. For a dataset to
    be linearly separable, a hyperplane must correctly sort all data points [6].
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 从本质上讲，感知机模型是一个线性分类器。它旨在找到一个“超平面”（二维空间中的一条线、三维空间中的一个平面，或更高维度的类似物）来分隔两个数据类别。为了使数据集具有线性可分性，超平面必须正确地分类所有数据点[6]。
- en: 'Mathematically, a perceptron model can be represented as follows:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 从数学上讲，感知机模型可以表示如下：
- en: '`y = f(w * x + b)`.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '`y = f(w * x + b)`。'
- en: '`x`is the input vector;`w` is the weight vector;`b` is the bias term; and `f`
    is the activation function. In the case of a perceptron, the activation function
    is a step function that maps the output to either 1 or 0, representing the two
    classes (**Fig. 4)**.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '`x`是输入向量；`w`是权重向量；`b`是偏置项；`f`是激活函数。在感知机的情况下，激活函数是一个阶跃函数，将输出映射为1或0，表示两个类别（**图
    4**）。'
- en: '![](../Images/61fcc753b8ee15617e40a4b5d8b4408f.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/61fcc753b8ee15617e40a4b5d8b4408f.png)'
- en: '**Fig. 4\.** Depiction of the unit step function, with the piece-wise conditions
    for mapping outputs to 0 or 1\. Figure created by the author.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 4\.** 单位阶跃函数的描述，包含将输出映射为0或1的分段条件。图由作者绘制。'
- en: 'A perceptron model can be extended to have multiple features in input`x`, which
    are defined as follows:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 感知机模型可以扩展到具有多个输入特征`x`，定义如下：
- en: '`y = f(w_1 * x_1 + w_1 * x_1 ... w_n * x_n + b)`.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '`y = f(w_1 * x_1 + w_1 * x_1 ... w_n * x_n + b)`。'
- en: The above equation, along with the step function for its output, is activated
    (i.e., turned off via 0 or on via 1), as depicted in the following figure, **Fig.
    5**.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 上述方程及其输出的阶跃函数被激活（即，通过0关闭或通过1打开），如下图所示，**图5**。
- en: '![](../Images/51274211ab39f1e65f7ffa1434fe5aa8.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/51274211ab39f1e65f7ffa1434fe5aa8.png)'
- en: '**Fig. 5\.** Multi-variant linear classification. Note that the weighted sum
    is passed through the activation, the above step function—source [link](https://pythoniseasytolearn.blogspot.com/2020/09/perceptron-mother-of-all-anns.html).'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '**图5.** 多变量线性分类。注意加权和通过激活函数，上述阶跃函数——来源 [link](https://pythoniseasytolearn.blogspot.com/2020/09/perceptron-mother-of-all-anns.html)。'
- en: 2.2\. The Perceptron Learning Algorithm
  id: totrans-76
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.2. 感知器学习算法
- en: 'The perceptron learning algorithm is a way to keep the weights and biases up-to-date
    to reduce classification errors [2]. The algorithm can be summarized as follows:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 感知器学习算法是一种保持权重和偏置最新以减少分类错误的方法[2]。该算法可以总结如下：
- en: Initialize the weights and the bias to small random values.
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将权重和偏置初始化为小的随机值。
- en: For each input-output pair`(x, d)`, compute the predicted output`y = f(w * x
    + b)`.
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于每对输入输出`(x, d)`，计算预测输出`y = f(w * x + b)`。
- en: 'Update the weights and bias based on the error`e = d - y`:'
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据误差`e = d - y`更新权重和偏置：
- en: '`w = w + η * e * x`'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '`w = w + η * e * x`'
- en: '`b = b + η * e`,'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '`b = b + η * e`,'
- en: where`η` is the learning rate, a small positive constant that controls the step
    size of the updates.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 其中`η`是学习率，一个小的正数，控制更新的步长。
- en: 4\. Repeat steps 2 and 3 for a fixed number of iterations or until the error
    converges.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 4. 对固定次数的迭代或直到误差收敛，重复步骤2和3。
- en: 'We can use Python and Sklearn to implement the steps above quickly:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用Python和Sklearn快速实现上述步骤：
- en: '[PRE0]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Then, using the fitted model, we can predict as follows:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，使用拟合的模型，我们可以进行如下预测：
- en: '[PRE1]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The perceptron learning algorithm guarantees convergence if the data is linearly
    separable [7].
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 如果数据是线性可分的，[7]中的感知器学习算法保证收敛。
- en: '![](../Images/97605757fef4dfe521e0c78e13507bc8.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/97605757fef4dfe521e0c78e13507bc8.png)'
- en: '**Fig. 6\.** Boolean classification, where the classes are linearly separable.
    Image created by the author.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '**图6.** 布尔分类，其中类别是线性可分的。图像由作者创建。'
- en: 2.3\. The Perceptron Convergence Theorem
  id: totrans-92
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.3. 感知器收敛定理
- en: Rosenblatt proved the perceptron convergence theorem in 1960\. It says that
    if a dataset can be separated linearly, the perceptron learning algorithm will
    find a solution in a finite number of steps [8]. The theorem says that, given
    enough time, the perceptron model will find the best weights and biases to classify
    all data points in a linearly separable dataset.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 1960年，Rosenblatt证明了感知器收敛定理。该定理指出，如果数据集可以线性分隔，感知器学习算法将在有限的步骤内找到解决方案[8]。该定理表明，只要时间足够，感知器模型将找到最佳的权重和偏置，以对所有数据点进行线性分隔的分类。
- en: But if the dataset isn't linearly separable, the perceptron learning algorithm
    might not find a suitable solution or converge. Because of this, researchers have
    developed more complex algorithms, like multilayer perceptrons and support vector
    machines, that can deal with data that doesn't separate in a straight line [9].
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 但如果数据集不是线性可分的，感知器学习算法可能找不到合适的解决方案或收敛。因此，研究人员开发了更复杂的算法，如多层感知器和支持向量机，这些算法可以处理不能直线分隔的数据[9]。
- en: 3\. The Perceptron Model as a Binary Classifier
  id: totrans-95
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 3. 感知器模型作为二分类器
- en: 3.1\. Linear Classification
  id: totrans-96
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.1. 线性分类
- en: As previously mentioned, the perceptron model is a linear classifier. It makes
    a decision boundary, a feature-space line separating the two classes [6]. When
    a new data point is added, the perceptron model sorts it based on where it falls
    on the decision boundary. The perceptron is fast and easy to use because it is
    simple but can only solve problems with data that can be separated linearly.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，感知器模型是一种线性分类器。它创建了一个决策边界，这是一个特征空间中的直线，用于分隔两个类别[6]。当添加新数据点时，感知器模型根据其在决策边界上的位置对其进行排序。感知器运行快速且易于使用，因为它简单，但只能解决数据可以线性分隔的问题。
- en: 3.2\. Limitations of the Perceptron Model
  id: totrans-98
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.2. 感知器模型的局限性
- en: One big problem with the perceptron model is that it can't deal with data that
    doesn't separate in a straight line. The XOR problem is an example of how some
    datasets are impossible to divide by a single hyperplane, which prevents the perceptron
    from finding a solution [4]. Researchers have developed more advanced methods
    to get around this problem, such as multilayer perceptrons, which have more than
    one layer of neurons and can learn to make decisions that don't follow a straight
    line [5].
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 感知器模型的一个大问题是它不能处理无法用直线分隔的数据。异或问题是一些数据集无法通过单个超平面分隔的例子，这使得感知器无法找到解决方案[4]。研究人员开发了更高级的方法来绕过这个问题，例如多层感知器，它们有多个神经网络层，能够学习进行不沿直线的决策[5]。
- en: The perceptron model is also sensitive to setting the learning rate and initial
    weights. For example, if the learning rate is too low, convergence might be slow,
    whereas a large learning rate may cause oscillations or divergence. In the same
    way, the choice of initial weights can affect how fast the solution converges
    and how it turns out [10].
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 感知器模型对学习率和初始权重的设置也很敏感。例如，如果学习率过低，收敛可能会很慢，而较大的学习率可能会导致振荡或发散。同样，初始权重的选择会影响解决方案的收敛速度以及最终效果[10]。
- en: 3.3\. Multi-class Classification with the Perceptron Model
  id: totrans-101
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.3\. 感知器模型的多类分类
- en: Even though the basic perceptron model is made for two-class problems, it can
    solve problems with more than two classes by training multiple perceptron classifiers,
    one for each category [11]. The most common approach is one-vs.-all (OvA), in
    which a separate perceptron is trained to distinguish classes. Then, when classifying
    a new data point, the perceptron with the highest output is chosen as the predicted
    class.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管基本的感知器模型是为两类问题设计的，但通过训练多个感知器分类器（每个类别一个），它可以解决多于两类的问题[11]。最常见的方法是“一对多（OvA）”，其中训练一个单独的感知器来区分各类。然后，在分类新数据点时，选择输出值最高的感知器作为预测类别。
- en: Another approach is the one-versus-one (OvO) method, in which a perceptron is
    trained for each pair of classes. The final classification decision is made using
    a voting scheme, where each perceptron casts a vote for its predicted class, and
    the type with the most votes is selected. While OvO requires training more classifiers
    than OvA, each perceptron only needs to handle a smaller subset of the data, which
    can benefit large datasets or problems with high computational complexity.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种方法是“一对一（OvO）”方法，其中对每对类别训练一个感知器。最终的分类决策是通过投票机制做出的，每个感知器对其预测的类别进行投票，票数最多的类别被选择。虽然OvO需要训练比OvA更多的分类器，但每个感知器只需要处理数据的一个较小子集，这对大型数据集或高计算复杂度的问题可能更有利。
- en: 4\. Logic Gates and the Perceptron Model
  id: totrans-104
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 4\. 逻辑门与感知器模型
- en: 4.1\. How Perceptrons Can Be Used to Generate Logic Gates
  id: totrans-105
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.1\. 感知器如何用于生成逻辑门
- en: Perceptron models can be used to represent logic gates, which are the most basic
    building blocks of digital circuits. By appropriately adjusting the weights and
    biases of a perceptron, it can be trained to perform logical operations such as
    AND, OR, and NOT [12]. This link between perceptrons and logic gates shows that
    neural networks can do computation and have the potential to simulate complex
    systems.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 感知器模型可以用来表示逻辑门，这些逻辑门是数字电路的最基本组成部分。通过适当调整感知器的权重和偏置，它可以被训练执行逻辑操作，如与（AND）、或（OR）和非（NOT）[12]。感知器与逻辑门之间的联系表明，神经网络不仅可以进行计算，还具有模拟复杂系统的潜力。
- en: '![](../Images/f6c507183d7f02a8bad4b768a95b44cd.png)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f6c507183d7f02a8bad4b768a95b44cd.png)'
- en: '**Fig. 6\.** Linearly separable logic gates: **AND** and **OR** (left and middle,
    respectively). On the other hand, **XOR** cannot be separated by a single linear
    classifier (right) but can be with a two-layer network (more on this later)—this
    figure was created by the author.'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 6\.** 线性可分逻辑门：**与（AND）** 和 **或（OR）**（分别为左侧和中间）。另一方面，**异或（XOR）** 不能通过单一线性分类器（右侧）进行分离，但可以通过两层网络进行分离（稍后会详细介绍）——该图由作者创建。'
- en: '4.2\. Example: Implementing a NAND Gate Using a Perceptron'
  id: totrans-109
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.2\. 示例：使用感知器实现 NAND 门
- en: 'A NAND gate is a fundamental logic gate that produces an output of 0 only when
    both inputs are 1, resulting in 1 in all other cases. The truth table for a NAND
    gate is as follows:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: NAND 门是一个基本的逻辑门，只有当两个输入都为 1 时，输出才为 0，在其他情况下输出为 1。NAND 门的真值表如下：
- en: '![](../Images/c92423d759617e15d8d93502f11b33ad.png)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c92423d759617e15d8d93502f11b33ad.png)'
- en: NAND Gate Truth Table. Table created by the author.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: NAND 门真值表。表格由作者创建。
- en: 'To implement a NAND gate using a perceptron, we can manually set the weights
    and biases or train the perceptron using the perceptron learning algorithm. Here’s
    a possible configuration of weights and bias:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用感知器实现 NAND 门，我们可以手动设置权重和偏置，或使用感知器学习算法来训练感知器。以下是可能的权重和偏置配置：
- en: '`w1 = -1`;'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '`w1 = -1`；'
- en: '`w2 = -1`;'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '`w2 = -1`；'
- en: '`b = 1.5`.'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '`b = 1.5`。'
- en: 'With these parameters, the perceptron can be represented as:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这些参数，感知器可以表示为：
- en: '`y = f((-1 * A) + (-1 * B) + 1.5)`.'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '`y = f((-1 * A) + (-1 * B) + 1.5)`。'
- en: '![](../Images/64348e4b15a4774b35b293f756e5225c.png)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/64348e4b15a4774b35b293f756e5225c.png)'
- en: '**Fig. 6\.** Training data, graphical depiction, and linear function for an
    AND gate. Figure created by the author.'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 6\.** 训练数据、图形表示以及 AND 门的线性函数。图由作者创建。'
- en: 'Here, `f` is the step function, and `A` and `B` are the inputs. If you test
    this setup with values from the truth table, you''ll get the right results from
    a NAND gate:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，`f` 是步进函数，`A` 和 `B` 是输入。如果使用真值表中的值测试此设置，你将从 NAND 门获得正确的结果：
- en: '![](../Images/eff8348e1a6f6841628032dd6544e48a.png)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/eff8348e1a6f6841628032dd6544e48a.png)'
- en: '**Fig. 7.** Truth table for the logic NAND, along with the output of the perceptron
    trained above and created by the author.'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 7.** 逻辑 NAND 的真值表，以及上述训练的感知器的输出和作者创建的结果。'
- en: 'In Python, NAND can be implemented as follows:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Python 中，NAND 可以如下实现：
- en: '[PRE2]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: As expected, reproducing the table summarizing the NAND gate above
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 正如预期的那样，重现上述总结 NAND 门的表格
- en: '[PRE3]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'A NAND gate can be used to build all other gates because it is functionally
    complete, meaning that any other logic function can be derived using just NAND
    gates. Here’s a brief explanation of how to create some of the basic gates using
    NAND gates:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: NAND 门可以用于构建所有其他门，因为它在功能上是完整的，这意味着任何其他逻辑函数都可以仅使用 NAND 门来推导。以下是如何使用 NAND 门创建一些基本门的简要说明：
- en: 'NOT gate: Connect both inputs of the NAND gate to the input value.'
  id: totrans-129
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: NOT 门：将 NAND 门的两个输入连接到输入值。
- en: 'AND gate: First, create a NAND gate and then pass the output through a NOT
    gate.'
  id: totrans-130
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: AND 门：首先创建一个 NAND 门，然后将输出通过一个 NOT 门。
- en: 'OR gate: Apply a NOT gate to each input before feeding them into a NAND gate.'
  id: totrans-131
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: OR 门：在将每个输入馈送到 NAND 门之前，对每个输入应用 NOT 门。
- en: 'To create a NAND gate that accepts an arbitrary number of inputs, you can use
    Python to define a function that takes a list of inputs and returns the NAND output.
    Here’s a code snippet demonstrating this:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 要创建一个接受任意数量输入的 NAND 门，可以使用 Python 定义一个函数，该函数接受一个输入列表并返回 NAND 输出。以下是演示此操作的代码片段：
- en: '[PRE4]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: This function uses a helper function (i.e., `and_gate`) to make a NAND gate
    with two or more inputs. The AND operation is then repeated on the given inputs.
    The final result is the output of the NAND gate, with an arbitrary number of input
    bits, which is the negated value of the AND gates.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 该函数使用一个辅助函数（即`and_gate`）来创建一个具有两个或更多输入的 NAND 门。然后在给定的输入上重复 AND 操作。最终结果是 NAND
    门的输出，具有任意数量的输入位，即 AND 门的取反值。
- en: '4.3\. Extending to Other Logic Gates: AND, OR, XOR'
  id: totrans-135
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.3\. 扩展到其他逻辑门：AND、OR、XOR
- en: Similarly, perceptrons can model other logic gates, such as AND, OR, and NOT.
    For example, a perceptron with weights `w1 = 1`, `w2 = 1`, and `b = -1.5` can
    represent an AND gate.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，感知器可以建模其他逻辑门，如 AND、OR 和 NOT。例如，具有权重 `w1 = 1`、`w2 = 1` 和 `b = -1.5` 的感知器可以表示一个
    AND 门。
- en: '[PRE5]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Again, outputs mimic those of the intended AND gate.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，输出模仿预期的 AND 门。
- en: '[PRE6]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: However, a single perceptron cannot model the XOR gate, which is not linearly
    separable. Instead, a multi-layer perceptron or a combination of perceptrons must
    be used to solve the XOR problem [5].
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，单个感知器无法建模 XOR 门，因为 XOR 门不是线性可分的。相反，必须使用多层感知器或感知器组合来解决 XOR 问题[5]。
- en: 5\. Perceptrons for Multiplication and Transistor-like Functionality
  id: totrans-141
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 5\. 用于乘法和类似晶体管功能的感知器
- en: 5.1\. Analogies Between Perceptrons and Transistors
  id: totrans-142
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.1\. 感知器与晶体管之间的类比
- en: Transistors are the basic building blocks of electronic devices. They are in
    charge of simple tasks like adding and multiplying. Interestingly, perceptrons
    can also be viewed as computational units that exhibit similar functionality.
    For example, perceptrons are used in machine learning and artificial neurons.
    Conversely, transistors are physical parts that change how electrical signals
    flow [13]. Still, as the last section showed, both systems can model and carry
    out logical operations.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 晶体管是电子设备的基本构建块。它们负责执行像加法和乘法这样的简单任务。有趣的是，感知器也可以被视为展示类似功能的计算单元。例如，感知器在机器学习和人工神经元中被使用。相比之下，晶体管是物理部件，改变电信号的流动
    [13]。尽管如此，正如上一节所示，这两种系统都可以建模和执行逻辑运算。
- en: 5.2\. Performing Multiplication with Perceptrons
  id: totrans-144
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.2\. 使用感知器进行乘法运算
- en: We can leverage their capabilities for binary operations to perform multiplication
    using perceptrons. For example, let’s consider the expansion of two binary digits
    (i.e., `A` and `B`), which can be represented as a simple AND gate. As demonstrated
    in Section 4, an AND gate can be modeled using a perceptron.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以利用感知器的二进制运算能力来执行乘法。例如，考虑两个二进制位（即`A`和`B`）的展开，可以将其表示为一个简单的AND门。正如第4节所示，AND门可以用感知器建模。
- en: But for more complicated multiplication tasks involving binary numbers with
    more than two bits, we need to add more parts, like half and full adders, which
    require a combination of logic gates [14]. Using perceptrons to build these parts
    makes making an artificial neural network that can perform binary multiplications
    possible.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，对于涉及多个二进制位的更复杂的乘法任务，我们需要添加更多的部件，如半加器和全加器，这些部件需要逻辑门的组合 [14]。使用感知器来构建这些部件使得构建可以执行二进制乘法的人工神经网络成为可能。
- en: 'For example, suppose we want to multiply two 2-bit binary numbers, `A1A0` and
    `B1B0`. Then, we can break down multiplication into a series of AND operations
    and additions:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设我们要乘以两个2位的二进制数，`A1A0` 和 `B1B0`。那么，我们可以将乘法分解为一系列的AND运算和加法：
- en: 'Compute the partial products: `P00 = A0 * B0`, `P01 = A0 * B1`, `P10 = A1 *
    B0`, and `P11 = A1 * B1`.'
  id: totrans-148
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算部分乘积：`P00 = A0 * B0`、`P01 = A0 * B1`、`P10 = A1 * B0` 和 `P11 = A1 * B1`。
- en: Add the partial products using half and full adders, resulting in a 4-bit binary
    product.
  id: totrans-149
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用半加器和全加器将部分乘积相加，得到一个4位的二进制乘积。
- en: Each AND operation and addition can be done with perceptrons or groups of perceptrons
    that represent the logic gates needed.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 每个AND运算和加法都可以通过感知器或表示所需逻辑门的感知器组来完成。
- en: 'Using the AND gate function we set up in the last section, we can do the following
    in Python to implement perceptron-based multiplication:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 使用上一节中设置的AND门函数，我们可以在Python中实现基于感知器的乘法：
- en: '[PRE7]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 5.3\. The Future of Perceptrons and Hardware Implementation
  id: totrans-153
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.3\. 感知器及其硬件实现的未来
- en: Even though perceptrons can act like transistors and perform basic math operations,
    their hardware implementation is less efficient than traditional transistors.
    However, recent improvements in neuromorphic computing have shown that it might
    be possible to make hardware that acts like neural networks, like perceptrons
    [15]. These neuromorphic chips could help machine learning tasks use less energy
    and open the door to new ways of thinking about computers.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管感知器可以像晶体管一样执行基本的数学运算，但其硬件实现的效率不如传统晶体管。然而，近期在神经形态计算方面的改进表明，可能有办法制造类似于神经网络的硬件，如感知器
    [15]。这些神经形态芯片可能有助于机器学习任务减少能源消耗，并开启对计算机新思维方式的探索。
- en: 6\. Comparing the Perceptron Model to Logistic Regression
  id: totrans-155
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 6\. 感知器模型与逻辑回归的比较
- en: 6.1\. Similarities Between Perceptron and Logistic Regression
  id: totrans-156
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.1\. 感知器与逻辑回归的相似性
- en: Both the perceptron model and logistic regression are linear classifiers that
    can be used to solve binary classification problems. They both rely on finding
    a decision boundary (a hyperplane) that separates the classes in the feature space
    [6]. Moreover, they can be extended to handle multi-class classification problems
    through techniques like one-vs-all and one-vs-one [11].
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 感知器模型和逻辑回归都是线性分类器，可以用来解决二分类问题。它们都依赖于找到一个决策边界（一个超平面），以在特征空间中分隔不同的类别 [6]。此外，它们还可以通过一对多和一对一等技术扩展以处理多分类问题
    [11]。
- en: 'Let’s take a look at the differences in Python implementation:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一下Python实现的区别：
- en: '[PRE8]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '[PRE9]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'This outputs:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 这将输出：
- en: '[PRE10]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 6.2\. Differences Between Perceptron and Logistic Regression
  id: totrans-163
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.2\. 感知器与逻辑回归的区别
- en: 'Even though the perceptron model and logistic regression have some similarities,
    there are some essential differences between the two:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管感知器模型和逻辑回归有一些相似之处，但两者之间存在一些本质区别：
- en: 'Activation function: The perceptron model uses a step function as its activation
    function, while logistic regression uses the logistic (sigmoid) function [10].
    This difference results in a perceptron having a binary output (`0` or `1`). At
    the same time, logistic regression produces a probability value (between 0 and
    1) representing the likelihood of an instance belonging to a particular class.'
  id: totrans-165
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 激活函数：感知器模型使用阶跃函数作为其激活函数，而逻辑回归使用逻辑（Sigmoid）函数[10]。这种差异导致感知器具有二元输出（`0` 或 `1`）。与此同时，逻辑回归生成一个概率值（介于0和1之间），表示实例属于特定类别的可能性。
- en: 'Loss function: The perceptron learning algorithm minimizes the misclassification
    errors, whereas logistic regression minimizes the log-likelihood or cross-entropy
    loss [16]. This distinction makes logistic regression more robust to noise and
    outliers in the dataset, as it considers the magnitude of the errors rather than
    just the number of misclassified instances.'
  id: totrans-166
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 损失函数：感知器学习算法最小化误分类错误，而逻辑回归最小化对数似然或交叉熵损失[16]。这种区别使逻辑回归对数据集中的噪声和异常值更具鲁棒性，因为它考虑了错误的幅度，而不仅仅是误分类实例的数量。
- en: 'Convergence: The perceptron learning algorithm can converge if the data is
    linearly separable but may fail to join otherwise [7]. Logistic regression, on
    the other hand, employs gradient-based optimization techniques like gradient descent
    or Newton-Raphson, which are guaranteed to reach a global optimum for convex loss
    functions like the log-likelihood [17].'
  id: totrans-167
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 收敛性：感知器学习算法在数据线性可分时可以收敛，但在其他情况下可能无法收敛[7]。另一方面，逻辑回归使用基于梯度的优化技术，如梯度下降或牛顿-拉夫森方法，这些方法能够保证在对数似然等凸损失函数中找到全局最优解[17]。
- en: 'Non-linearly separable data: While the perceptron model struggles with non-linearly
    separable data, logistic regression can be extended to handle non-linear decision
    boundaries by incorporating higher-order polynomial features or using kernel methods
    [18].'
  id: totrans-168
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 非线性可分数据：尽管感知器模型在处理非线性可分数据时会遇到困难，但逻辑回归可以通过引入高阶多项式特征或使用核方法来扩展处理非线性决策边界[18]。
- en: 6.3\. Choosing Between Perceptron and Logistic Regression
  id: totrans-169
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.3\. 选择感知器与逻辑回归
- en: The perceptron model and logistic regression choices depend on the problem and
    dataset. Logistic regression is more reliable and can deal with a broader range
    of problems because it is based on probabilities and can model non-linear decision
    boundaries. However, the perceptron model may be easier to use and use less computing
    power in some situations, especially when dealing with data that can be separated
    linearly.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 感知器模型和逻辑回归的选择取决于问题和数据集。逻辑回归更可靠，可以处理更广泛的问题，因为它基于概率并且可以建模非线性决策边界。然而，在某些情况下，特别是处理可以线性分离的数据时，感知器模型可能更易于使用且计算资源消耗更少。
- en: 7\. Creative and Unique Applications of the Perceptron Model
  id: totrans-171
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 7\. 感知器模型的创造性和独特应用
- en: 7.1\. Optical Character Recognition (OCR)
  id: totrans-172
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.1\. 光学字符识别（OCR）
- en: The perceptron model has been used in optical character recognition (OCR) tasks,
    where the goal is to recognize and turn printed or handwritten text into machine-encoded
    text [19]. A perceptron or other machine learning algorithm is often used for
    OCR tasks to preprocess the image that will be read, pull out features from it,
    and classify them. The perceptron model is a good choice for OCR tasks with characters
    that can be separated in a straight line because it is easy to use and works well
    with computers.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 感知器模型已被应用于光学字符识别（OCR）任务，其目标是识别并将打印或手写文本转换为机器编码文本[19]。感知器或其他机器学习算法通常用于OCR任务，以预处理将被读取的图像，从中提取特征并进行分类。感知器模型对于字符可以通过直线分离的OCR任务是一个不错的选择，因为它易于使用且与计算机配合良好。
- en: 7.2\. Music Genre Classification
  id: totrans-174
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.2\. 音乐类型分类
- en: Perceptrons can also be used for music genre classification, which involves
    identifying the genre of a given audio track. A perceptron model can be trained
    to classify audio into already-set genres [20]. This is done by taking relevant
    parts of audio signals, such as spectral or temporal features, and putting them
    together. Even though more advanced methods like deep learning and convolutional
    neural networks often give better results, the perceptron model can work well,
    especially when only a few genres or features can be separated linearly.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 感知机也可以用于音乐流派分类，这涉及识别给定音频轨迹的流派。可以训练感知机模型将音频分类为已设置的流派 [20]。这通过提取音频信号的相关部分，如频谱特征或时间特征，然后将其组合起来来完成。尽管深度学习和卷积神经网络等更先进的方法通常能提供更好的结果，但感知机模型仍能很好地工作，特别是在只有少数几个流派或特征可以线性分离的情况下。
- en: 7.3\. Intrusion Detection Systems
  id: totrans-176
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.3\. 入侵检测系统
- en: Intrusion detection systems, or IDS, are used in cybersecurity to look for bad
    behavior or unauthorized access to computer networks. IDS can use perceptrons
    as classifiers by looking at packet size, protocol type, and network traffic connection
    length to determine if the activity is regular or malicious [21]. Support vector
    machines and deep learning may better detect things, but the perceptron model
    can be used for simple IDS tasks or as a comparison point.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 入侵检测系统，或称为IDS，广泛用于网络安全中，以查找恶意行为或未经授权的访问计算机网络。IDS可以使用感知机作为分类器，通过查看数据包大小、协议类型和网络流量连接长度来确定活动是常规的还是恶意的
    [21]。支持向量机和深度学习可能更擅长检测，但感知机模型可以用于简单的IDS任务或作为比较点。
- en: 7.4\. Sentiment Analysis
  id: totrans-178
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.4\. 情感分析
- en: Perceptrons can be applied to sentiment analysis, a natural language processing
    task determining the sentiment (e.g., positive, negative, or neutral) expressed
    in text. By turning text into numerical feature vectors like term frequency-inverse
    document frequency (TF-IDF) representations [22], a perceptron model can be taught
    to classify text based on its tone. More advanced techniques like recurrent neural
    networks or transformers have since surpassed perceptrons in sentiment analysis
    performance. However, perceptrons can still be an introduction to text classification
    or a simpler alternative for specific use cases.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 感知机可以应用于情感分析，这是一个自然语言处理任务，旨在确定文本中表达的情感（例如，正面、负面或中性）。通过将文本转换为数值特征向量，如词频-逆文档频率（TF-IDF）表示
    [22]，可以训练感知机模型根据其语气分类文本。尽管更先进的技术如递归神经网络或变换器在情感分析性能上已经超越了感知机，但感知机仍然可以作为文本分类的入门方法或特定用例的简单替代方案。
- en: 8\. The Evolution of the Perceptron Model and Legacy in Deep Learning
  id: totrans-180
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 8\. 感知机模型的演变及其在深度学习中的遗产
- en: 8.1\. The Evolution of Perceptrons to Multi-Layer Perceptrons (MLPs)
  id: totrans-181
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.1\. 感知机到多层感知机（MLPs）的演变
- en: The perceptron model has been able to solve problems with clear decision lines,
    but it needs help with tasks that need clear decision lines. The introduction
    of multi-layer perceptrons (MLPs), consisting of multiple layers of perceptron-like
    units, marked a significant advancement in artificial neural networks [5]. MLPs
    can approximate any continuous function, given a sufficient number of hidden layers
    and neurons [23]. By employing the backpropagation algorithm, MLPs can be trained
    to solve more complex tasks, such as the XOR problem, which is not solvable by
    a single perceptron.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 感知机模型已经能够解决具有明确决策边界的问题，但在需要明确决策边界的任务中仍存在困难。多层感知机（MLPs）的引入，包含多个感知机样单位的层，标志着人工神经网络的显著进步
    [5]。MLPs可以逼近任何连续函数，只要具有足够数量的隐藏层和神经元 [23]。通过采用反向传播算法，MLPs可以训练以解决更复杂的任务，例如XOR问题，这是单个感知机无法解决的。
- en: 8.2\. Deep Learning and Perceptron’s Legacy
  id: totrans-183
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.2\. 深度学习与感知机的遗产
- en: The perceptron model laid the foundation for deep learning, a subfield of machine
    learning focused on neural networks with multiple layers (deep neural networks).
    The perceptron model was the basis for deep learning techniques like convolutional
    neural networks (CNNs) and recurrent neural networks (RNNs), which have reached
    state-of-the-art performance in tasks like image classification, natural language
    processing, and speech recognition [24].
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 感知机模型奠定了深度学习的基础，深度学习是机器学习的一个子领域，专注于具有多层（深度神经网络）的神经网络。感知机模型是卷积神经网络（CNNs）和递归神经网络（RNNs）等深度学习技术的基础，这些技术在图像分类、自然语言处理和语音识别等任务中达到了**最先进的**性能
    [24]。
- en: In CNNs, the idea of weighted input signals and activation functions from perceptrons
    is carried over to the convolutional layers. To learn about spatial hierarchies
    in the data, these layers apply filters to the input regions near them. In the
    same way, RNNs build on the perceptron model by adding recurrent connections.
    This lets the network learn temporal dependencies in sequential data [25].
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 在 CNNs 中，来自感知器的加权输入信号和激活函数的思想被传递到卷积层。这些层通过对输入区域应用滤波器来学习数据中的空间层次结构。同样，RNNs 通过添加递归连接来建立在感知器模型的基础上。这使得网络能够学习序列数据中的时间依赖关系
    [25]。
- en: '![](../Images/acbfdb72a70289f1aa6ad8468c4c25b1.png)'
  id: totrans-186
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/acbfdb72a70289f1aa6ad8468c4c25b1.png)'
- en: 'Deep learning versus other models: Google trend over time. Image created by
    the author following [Carrie Fowle](https://medium.com/u/3b0511e6a8d3?source=post_page-----fd384eb93382--------------------------------)’s
    TDS Medium blog ([link](/using-google-trends-at-scale-1c8b902b6bfa)).'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习与其他模型的对比：Google 趋势随时间变化。图片由作者根据 [Carrie Fowle](https://medium.com/u/3b0511e6a8d3?source=post_page-----fd384eb93382--------------------------------)’s
    TDS Medium 博客 ([link](/using-google-trends-at-scale-1c8b902b6bfa)) 制作。
- en: 8.3\. The Future of Perceptrons and Deep Learning
  id: totrans-188
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.3\. 感知器与深度学习的未来
- en: While fundamental, more sophisticated deep learning techniques have primarily
    eclipsed the perceptron model. However, it is still valuable for machine learning
    because it is a simple but effective way to teach the basics of neural networks
    and get ideas for making more complicated models. As deep learning keeps improving,
    the perceptron model's core ideas and principles will likely stay the same and
    influence the design of new architectures and algorithms.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管基础，更多复杂的深度学习技术已经主要取代了感知器模型。然而，它仍然对机器学习有价值，因为它是一种简单而有效的方式来教授神经网络的基础知识，并为构建更复杂的模型提供了思路。随着深度学习的不断进步，感知器模型的核心思想和原则可能仍会保持不变，并影响新架构和算法的设计。
- en: 9\. Conclusion
  id: totrans-190
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 9\. 结论
- en: This blog comprehensively explores the perceptron model, its mathematics, binary
    classification, and logic gate generation applications. By understanding these
    fundamentals, we have unlocked the potential to harness the perceptron’s power
    in various neat applications and even construct more advanced models like multi-layer
    perceptrons (MLPs) and convolutional neural networks (CNNs).
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 本博客全面探讨了感知器模型及其数学、二元分类和逻辑门生成应用。通过理解这些基础知识，我们已经解锁了在各种实际应用中利用感知器的潜力，甚至可以构建更高级的模型，如多层感知器（MLPs）和卷积神经网络（CNNs）。
- en: We also compared perceptrons and logistic regression, highlighting the differences
    and similarities by examining the role of a perceptron as a foundation for more
    advanced techniques in ML. We extended this upon setting perceptron’s role in
    artificial intelligence, historical significance, and ongoing influence.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还比较了感知器和逻辑回归，通过考察感知器作为机器学习中更高级技术的基础，突出了它们的异同。我们进一步探讨了感知器在人工智能中的作用、历史意义和持续影响。
- en: Let us remember that ‌perceptron is just one piece of the puzzle. Countless
    other models and techniques, either discovered or waiting to be, each with unique
    strengths and applications. Nonetheless, with a solid foundation provided by this
    tutorial, you are well-equipped to tackle the challenges and opportunities in
    your journey through artificial intelligence.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，感知器只是其中的一部分。还有无数其他模型和技术，无论是已被发现的还是待发现的，每个都有独特的优点和应用。尽管如此，借助本教程提供的坚实基础，你已准备好迎接人工智能领域中的挑战和机遇。
- en: I hope this blog is engaging, informative, and inspiring, and I encourage you
    to continue learning and experimenting with the perceptron model and beyond. Embrace
    your newfound knowledge, and let your creativity and curiosity guide you toward
    the exciting world of AI and machine learning. Please share your thoughts and
    comments below!
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 我希望本博客既引人入胜又富有启发性，鼓励你继续学习和尝试感知器模型及其他相关内容。拥抱你获得的新知识，让你的创造力和好奇心引导你探索人工智能和机器学习的精彩世界。请在下方分享你的想法和评论！
- en: References
  id: totrans-195
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] McCulloch, W.S., & Pitts, W. (1943). A logical calculus of the ideas immanent
    in nervous activity Bulletin of Mathematical Biophysics, 5, 115–133.'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] McCulloch, W.S., & Pitts, W. (1943). 神经活动中固有思想的逻辑演算。数学生物物理学公报，5，115–133。'
- en: '[2] Rosenblatt, F. (1958). The perceptron is a probabilistic model for information
    storage and organization in the brain. Psychological Review, 65(6), 386–408.'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] Rosenblatt, F. (1958). 感知器是脑中信息存储和组织的概率模型。心理学评论，65(6)，386–408。'
- en: '[3] The New York Times (1958, July 8). A New Navy Device Learns by Doing The
    New York Times'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] 纽约时报 (1958年7月8日). 一种新型海军设备通过实践学习. 纽约时报.'
- en: '[4] Minsky, M., & Papert, S. (1969). Perceptrons: An Introduction to Computational
    Geometry, MIT Press.'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] Minsky, M. 和 Papert, S. (1969). 感知器：计算几何入门, MIT Press.'
- en: '[5] Rumelhart, D. E., Hinton, G. E., & Williams, R. J. (1986). Learning representations
    by back-propagating errors Nature, 323 (6088), 533–536.'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: '[5] Rumelhart, D. E., Hinton, G. E., 和 Williams, R. J. (1986). 通过反向传播错误学习表示.
    Nature, 323 (6088), 533–536.'
- en: '[6] Duda, R. O., Hart, P. E., & Stork, D. G. (2001). Pattern Classification
    (2nd ed.). Wiley.'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: '[6] Duda, R. O., Hart, P. E., 和 Stork, D. G. (2001). 模式分类 (第2版). Wiley.'
- en: '[7] Novikoff, A. B. (1962), on convergence proofs for perceptrons Symposium
    on the Mathematical Theory of Automata, 12, 615–622.'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: '[7] Novikoff, A. B. (1962), 关于感知器收敛证明. 自动机数学理论研讨会, 12, 615–622.'
- en: '[8] Rosenblatt, F. (1960). The perceptron: A theory of statistical separability
    in cognitive systems (Project PARA Report 60–3777). Cornell Aeronautical Laboratory'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: '[8] Rosenblatt, F. (1960). 感知器：认知系统中统计可分离性的理论 (项目 PARA 报告 60–3777). 康奈尔航空实验室.'
- en: '[9] Cortes, C., & Vapnik, V. (1995). Support-vector networks. Machine Learning,
    20(3), 273–297.'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: '[9] Cortes, C. 和 Vapnik, V. (1995). 支持向量网络. 机器学习, 20(3), 273–297.'
- en: '[10] Bishop, C. M. (2006). Pattern Recognition and Machine Learning, Springer,'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: '[10] Bishop, C. M. (2006). 模式识别与机器学习, Springer.'
- en: '[11] Rifkin, R., & Klautau, A. (2004). In defense of the one-vs-all classification
    Journal of Machine Learning Research, 5, 101–141.'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: '[11] Rifkin, R. 和 Klautau, A. (2004). 为一对多分类辩护. 机器学习研究期刊, 5, 101–141.'
- en: '[12] Minsky, M. L. (1961). Steps toward artificial intelligence. Proceedings
    of the IRE, 49(1), 8–30.'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: '[12] Minsky, M. L. (1961). 迈向人工智能的步骤. IRE 会议录, 49(1), 8–30.'
- en: '[13] Horowitz, P., & Hill, W. (1989). The Art of Electronics (2nd ed.). Cambridge
    University Press'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: '[13] Horowitz, P. 和 Hill, W. (1989). 电子艺术 (第2版). 剑桥大学出版社.'
- en: '[14] Mano, M. M., & Ciletti, M. D. (2007). Digital Design (4th ed.). Prentice
    Hall.'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: '[14] Mano, M. M. 和 Ciletti, M. D. (2007). 数字设计 (第4版). Prentice Hall.'
- en: '[15] Merolla, P. A., Arthur, J. V., Alvarez-Icaza, R., Cassidy, A. S., Sawada,
    J., Akopyan, F.,... & Modha, D. S. (2014). A million spike-neuron integrated circuits
    with a scalable communication network and interface Science, 345 (6197), 668–673.'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: '[15] Merolla, P. A., Arthur, J. V., Alvarez-Icaza, R., Cassidy, A. S., Sawada,
    J., Akopyan, F.,... 和 Modha, D. S. (2014). 百万尖峰神经元集成电路与可扩展通信网络和接口. Science, 345
    (6197), 668–673.'
- en: '[16] Hastie, T., Tibshirani, R., & Friedman, J. (2009). The Elements of Statistical
    Learning: Data Mining, Inference, and Prediction (2nd ed.). Springer.'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: '[16] Hastie, T., Tibshirani, R., 和 Friedman, J. (2009). 统计学习的元素：数据挖掘、推断与预测
    (第2版). Springer.'
- en: '[17] Nocedal, J., & Wright, S. (2006). Numerical Optimization (2nd ed.). Springer.'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: '[17] Nocedal, J. 和 Wright, S. (2006). 数值优化 (第2版). Springer.'
- en: '[18] Schölkopf, B., & Smola, A. J. (2002). Learning with Kernels: Support Vector
    Machines, Regularization, Optimization, and Beyond. MIT Press,'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: '[18] Schölkopf, B. 和 Smola, A. J. (2002). 使用核的学习：支持向量机、正则化、优化及其他. MIT Press.'
- en: '[19] LeCun, Y., Boser, B., Denker, J. S., Henderson, D., Howard, R. E., Hubbard,
    W., & Jackel, L. D. (1989). Backpropagation was applied to handwritten zip code
    recognition. Neural Computation, 1(4), 541–551.'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: '[19] LeCun, Y., Boser, B., Denker, J. S., Henderson, D., Howard, R. E., Hubbard,
    W., 和 Jackel, L. D. (1989). 反向传播应用于手写邮政编码识别. 神经计算, 1(4), 541–551.'
- en: '[20] Tzanetakis, G., & Cook, P. (2002). Musical genre classification of audio
    signals IEEE Transactions on Speech and Audio Processing, 10(5), 293–302.'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: '[20] Tzanetakis, G. 和 Cook, P. (2002). 音频信号的音乐类型分类. IEEE 语音与音频处理汇刊, 10(5),
    293–302.'
- en: '[21] Garcia-Teodoro, P., Diaz-Verdejo, J., Maciá-Fernández, G., & Vázquez,
    E. (2009). Anomaly-based network intrusion detection: techniques, systems, and
    challenges Computers & Security, 28 (1–2), 18–28.'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: '[21] Garcia-Teodoro, P., Diaz-Verdejo, J., Maciá-Fernández, G., 和 Vázquez,
    E. (2009). 基于异常的网络入侵检测：技术、系统和挑战. 计算机与安全, 28 (1–2), 18–28.'
- en: '[22] Pang, B., Lee, L., & Vaithyanathan, S. (2002). Thumbs up? Sentiment classification
    using machine learning techniques Proceedings of the ACL-02 Conference on Empirical
    Methods in Natural Language Processing, 10, 79–86.'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: '[22] Pang, B., Lee, L., 和 Vaithyanathan, S. (2002). 竖起大拇指？使用机器学习技术进行情感分类. ACL-02
    自然语言处理经验方法会议论文集, 10, 79–86.'
- en: '[23] Hornik, K., Stinchcombe, M., & White, H. (1989). Multilayer feedforward
    networks are universal approximators. Neural Networks, 2(5), 359–366'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: '[23] Hornik, K., Stinchcombe, M., 和 White, H. (1989). 多层前馈网络是通用逼近器. 神经网络, 2(5),
    359–366.'
- en: '[24] LeCun, Y., Bengio, Y., & Hinton, G. (2015). Deep learning. Nature, 521
    (7553), 436–444.'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: '[24] LeCun, Y., Bengio, Y., 和 Hinton, G. (2015). 深度学习. Nature, 521 (7553),
    436–444.'
- en: '[25] Hochreiter, S., & Schmidhuber, J. (1997). long-term memory. Neural Computation,
    9(8), 1735–1780.'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: '[25] Hochreiter, S., & Schmidhuber, J. (1997). 长期记忆。神经计算，9(8), 1735–1780。'
- en: Contact
  id: totrans-221
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 联系方式
- en: Want to Connect? Follow Dr. Robinson on [LinkedIn](https://www.linkedin.com/in/jrobby/),
    [Twitter](https://twitter.com/jrobvision), [Facebook](https://www.facebook.com/joe.robinson.39750),
    and [Instagram](https://www.instagram.com/doctor__jjj/). Visit my homepage for
    papers, blogs, email signups, and more!
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 想要联系？请关注**罗宾逊博士**的[LinkedIn](https://www.linkedin.com/in/jrobby/)、[Twitter](https://twitter.com/jrobvision)、[Facebook](https://www.facebook.com/joe.robinson.39750)和[Instagram](https://www.instagram.com/doctor__jjj/)。访问我的主页获取论文、博客、邮箱注册等更多信息！
- en: '[](https://www.jrobs-vision.com/?source=post_page-----fd384eb93382--------------------------------)
    [## AI Research Engineer and Entrepreneur | Joseph P. Robinson'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://www.jrobs-vision.com/?source=post_page-----fd384eb93382--------------------------------)
    [## AI研究工程师与企业家 | 约瑟夫·P·罗宾逊'
- en: Researcher & Entrepreneur Greetings! As a researcher, Dr. Robinson proposed
    and employed advanced AI to understand…
  id: totrans-224
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 研究员与企业家，您好！作为一名研究员，**罗宾逊博士**提出并采用了先进的AI技术来理解…
- en: www.jrobs-vision.com.](https://www.jrobs-vision.com/?source=post_page-----fd384eb93382--------------------------------)
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: '[www.jrobs-vision.com](https://www.jrobs-vision.com/?source=post_page-----fd384eb93382--------------------------------)'
