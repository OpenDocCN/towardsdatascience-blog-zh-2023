- en: BEV Perception in Mass Production Autonomous Driving
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 大规模生产自主驾驶中的BEV感知
- en: 原文：[https://towardsdatascience.com/bev-perception-in-mass-production-autonomous-driving-c6e3f1e46ae0](https://towardsdatascience.com/bev-perception-in-mass-production-autonomous-driving-c6e3f1e46ae0)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/bev-perception-in-mass-production-autonomous-driving-c6e3f1e46ae0](https://towardsdatascience.com/bev-perception-in-mass-production-autonomous-driving-c6e3f1e46ae0)
- en: The Recipe of XNet from Xpeng Motors
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 小鹏汽车的XNet配方
- en: '[](https://medium.com/@patrickllgc?source=post_page-----c6e3f1e46ae0--------------------------------)[![Patrick
    Langechuan Liu](../Images/fecbf85146a9bde21e6b2251538ddd65.png)](https://medium.com/@patrickllgc?source=post_page-----c6e3f1e46ae0--------------------------------)[](https://towardsdatascience.com/?source=post_page-----c6e3f1e46ae0--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----c6e3f1e46ae0--------------------------------)
    [Patrick Langechuan Liu](https://medium.com/@patrickllgc?source=post_page-----c6e3f1e46ae0--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@patrickllgc?source=post_page-----c6e3f1e46ae0--------------------------------)[![Patrick
    Langechuan Liu](../Images/fecbf85146a9bde21e6b2251538ddd65.png)](https://medium.com/@patrickllgc?source=post_page-----c6e3f1e46ae0--------------------------------)[](https://towardsdatascience.com/?source=post_page-----c6e3f1e46ae0--------------------------------)[![数据科学前沿](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----c6e3f1e46ae0--------------------------------)
    [Patrick Langechuan Liu](https://medium.com/@patrickllgc?source=post_page-----c6e3f1e46ae0--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----c6e3f1e46ae0--------------------------------)
    ·9 min read·Jun 19, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于[数据科学前沿](https://towardsdatascience.com/?source=post_page-----c6e3f1e46ae0--------------------------------)
    ·阅读时间9分钟·2023年6月19日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: This blog post is based on the invited talk in the *End-to-end Autonomous Driving
    Workshop* at CVPR 2023 held in Vancouver, titled “The Practice of Mass Production
    Autonomous Driving in China”. The recording of the keynote can be found [here](https://www.youtube.com/watch?v=d6ucRgDDUWQ&t=162s).
  id: totrans-6
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 本博客文章基于在2023年CVPR于温哥华举行的*端到端自主驾驶研讨会*上的邀请演讲，演讲题为“中国大规模生产自主驾驶的实践”。主旨演讲的录音可以在[这里](https://www.youtube.com/watch?v=d6ucRgDDUWQ&t=162s)找到。
- en: BEV (bird’s-eye-view) perception has witnessed great progress over the past
    few years. It directly perceives the environment around autonomous driving vehicles.
    **BEV perception** can be seen as an **end-to-end perception** system, and an
    important step toward an end-to-end autonomous driving system. Here we define
    an end-to-end autonomous driving system as fully differentiable pipelines that
    take raw sensor data as input and produce a high-level driving plan or low-level
    control actions as output.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: BEV（鸟瞰视角）感知在过去几年中取得了巨大进展。它直接感知自主驾驶车辆周围的环境。**BEV感知**可以被看作是一个**端到端感知**系统，也是实现端到端自主驾驶系统的重要一步。我们将端到端自主驾驶系统定义为完全可微分的管道，这些管道以原始传感器数据作为输入，输出高层次的驾驶计划或低层次的控制动作。
- en: '[](https://opendrivelab.com/e2ead/cvpr23.html?source=post_page-----c6e3f1e46ae0--------------------------------)
    [## CVPR 2023 Autonomous Driving Workshop | OpenDriveLab'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://opendrivelab.com/e2ead/cvpr23.html?source=post_page-----c6e3f1e46ae0--------------------------------)
    [## CVPR 2023 自主驾驶研讨会 | OpenDriveLab'
- en: We are proud to announce four brand-new challenges this year, in collaboration
    with our partners -Vision-Centric…
  id: totrans-9
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 我们很自豪地宣布今年与我们的合作伙伴共同推出四个全新挑战 - 以视觉为中心…
- en: opendrivelab.com](https://opendrivelab.com/e2ead/cvpr23.html?source=post_page-----c6e3f1e46ae0--------------------------------)
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '[opendrivelab.com](https://opendrivelab.com/e2ead/cvpr23.html?source=post_page-----c6e3f1e46ae0--------------------------------)'
- en: The autonomous driving community has witnessed rapid growth in approaches that
    embrace an end-to-end algorithm framework. We will discuss the necessity of end-to-end
    approaches from the first principle. Then we will review the efforts to deploy
    the BEV perception algorithm onto mass production vehicles, taking the development
    of XNet, the BEV perception architecture from Xpeng as an example. Finally, we
    will brainstorm about the future of BEV perception toward fully end-to-end autonomous
    driving.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 自主驾驶社区见证了采用端到端算法框架的方法的快速增长。我们将从基本原理讨论端到端方法的必要性。然后，我们将回顾将BEV感知算法部署到大规模生产车辆的努力，以小鹏的BEV感知架构XNet为例。最后，我们将对BEV感知的未来进行头脑风暴，以实现完全端到端的自主驾驶。
- en: The Necessity of End-to-end Systems
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 端到端系统的必要性
- en: In solving any engineering problem, it is often necessary to use a divide-and-conquer
    approach to find practical solutions quickly. This strategy involves breaking
    down the big problem into smaller, relatively well-defined components that can
    be solved independently. While this approach assists in delivering a complete
    product quickly, it also increases the risk of being stuck at a local optimum
    solution. To reach the global optimum solution, all components must be optimized
    together in an end-to-end fashion.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在解决任何工程问题时，通常需要使用分而治之的方法来快速找到实际解决方案。这种策略涉及将大问题拆解成较小的、相对明确的组件，这些组件可以独立解决。虽然这种方法有助于快速交付完整产品，但也增加了被困在局部最优解的风险。为了达到全局最优解，所有组件必须以端到端的方式一起优化。
- en: '![](../Images/cd46416c815dae75d45c756ea2ca7334.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/cd46416c815dae75d45c756ea2ca7334.png)'
- en: The performance growth curve for Divide-and-conquer vs End-to-end (chart made
    by author)
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 分而治之与端到端的性能增长曲线（图表由作者制作）
- en: The 80–20 rule reinforces the concept that 80% of the desired performance can
    be achieved with only 20% of the total effort. The advantage of using a divide-and-conquer
    approach is that it allows developers to work quickly using minimal effort. However,
    the downside is that this method often leads to a performance ceiling at the 80%
    mark. To overcome the performance limit and get out of the local optimum, developers
    must optimize certain components together, which is the first step in developing
    an end-to-end solution. This process must be repeated several times, breaking
    performance ceilings over and over until a fully end-to-end solution is achieved.
    The resulting curve may take the form of a series of sigmoid curves until the
    global optimal solution is approximated. One example of an effort toward an end-to-end
    solution is the development of BEV perception algorithm.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 80-20法则强化了这样一个概念：80%的期望性能可以通过仅20%的总努力来实现。使用分而治之的方法的优势在于它允许开发者用最小的努力快速工作。然而，这种方法的缺点是它通常会在80%的标记处导致性能天花板。为了克服性能限制并摆脱局部最优，开发者必须将某些组件一起优化，这也是开发端到端解决方案的第一步。这个过程必须重复几次，不断打破性能天花板，直到实现一个完整的端到端解决方案。最终形成的曲线可能会呈现为一系列的S形曲线，直到接近全局最优解。一个朝着端到端解决方案努力的例子是BEV感知算法的开发。
- en: 'Perception 2.0: End-to-end Perception'
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 感知2.0：端到端感知
- en: In traditional autonomous driving stacks, 2D images are fed into the perception
    module to generate 2D results. Sensor fusion is then utilized to reason between
    2D results from multiple cameras and elevate these to 3D. The resulting 3D objects
    are subsequently sent to downstream components, such as prediction and planning.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在传统的自动驾驶堆栈中，2D图像被送入感知模块以生成2D结果。然后使用传感器融合来推理来自多个摄像头的2D结果，并将这些结果提升为3D。生成的3D对象随后被发送到下游组件，如预测和规划。
- en: '![](../Images/ef5c86c32ddd46d2be8ea1b5510384d8.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ef5c86c32ddd46d2be8ea1b5510384d8.png)'
- en: BEV perception is essentially end-to-end perception (image made by author)
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: BEV感知本质上是端到端感知（图像由作者制作）
- en: However, the sensor fusion step requires a lot of handcrafted rules to fuse
    the perception results from several camera streams. Each camera only perceives
    a portion of the object to be observed, so combining the obtained information
    necessitates careful adjustment of the fusion logic. We are essentially doing
    back-propagation through the engineers’ heads. Moreover, developing and maintaining
    these rules creates a set of complications, leading to numerous issues in complex
    urban environments.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，传感器融合步骤需要大量的手工规则来融合来自多个摄像头流的感知结果。每个摄像头只能感知要观察对象的一部分，因此结合获得的信息需要仔细调整融合逻辑。我们本质上是在通过工程师的头脑进行反向传播。此外，开发和维护这些规则会造成一系列复杂问题，导致在复杂的城市环境中出现许多问题。
- en: To overcome this challenge, we can apply the Bird’s Eye View (BEV) perception
    model, which allows us to perceive the environment directly in the BEV space.
    The BEV perception stack combines two separate components into a single solution,
    thereby eliminating the brittle human-crafted logic. BEV perception is essentially
    an **end-to-end perception** solution. This marks a critical step toward an end-to-end
    autonomous driving system.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 为了克服这个挑战，我们可以应用鸟瞰图（BEV）感知模型，它允许我们在BEV空间中直接感知环境。BEV感知堆栈将两个独立的组件结合成一个单一的解决方案，从而消除了脆弱的人为制定逻辑。BEV感知本质上是一个**端到端感知**解决方案。这标志着朝着端到端自动驾驶系统迈出的关键一步。
- en: 'XNet: BEV Perception Stack from Xpeng Motors'
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 'XNet: Xpeng Motors 的 BEV 感知堆栈'
- en: The BEV perception architecture from Xpeng is codenamed XNet. It was first publicly
    introduced in [Xpeng 1024 Tech Day in 2022](https://www.youtube.com/watch?v=0dEoctcK09Q).
    The visualization below depicts the onboard XNet perception architecture in action.
    The red vehicle in the middle represents the autonomous driving vehicle as it
    navigates a roundabout. The surrounding static environment is entirely detected
    by onboard perception, and no HD Map is used. We can observe that XNet accurately
    detects a wide range of dynamic and static objects around the vehicle.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: Xpeng 的 BEV 感知架构代号为 XNet。它首次公开介绍是在 [2022年 Xpeng 1024 科技日](https://www.youtube.com/watch?v=0dEoctcK09Q)
    上。下方的可视化图展示了车载 XNet 感知架构的实际运行情况。中间的红色车辆代表自动驾驶车辆在环形交叉路口导航。周围的静态环境完全由车载感知系统检测，无需使用高清地图。我们可以观察到
    XNet 精确地检测了车辆周围的各种动态和静态物体。
- en: The Xpeng AI team started experimenting with the XNet architecture over two
    years ago (early 2021), and it has since undergone several iterations before arriving
    at its current form. We exploit Convolutional Neural Network (CNN) backbones to
    generate image features, while the multicamera features are transposed into the
    BEV space through a transformer structure. Specifically, a cross-attention module
    was used. The BEV features from several past frames are then fused with the ego
    pose — both spatially and temporally — to decode the dynamic and static elements
    from the fused features.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: Xpeng AI 团队在两年前（2021年初）开始对 XNet 架构进行实验，并经过多次迭代后达到了现在的形式。我们利用卷积神经网络（CNN）主干生成图像特征，同时通过变换器结构将多摄像头特征转置到
    BEV 空间。具体而言，使用了交叉注意力模块。来自多个过去帧的 BEV 特征随后与自我姿态——在空间和时间上——融合，以解码融合特征中的动态和静态元素。
- en: '![](../Images/9b842cb903eed4b72c7a424e035210b9.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9b842cb903eed4b72c7a424e035210b9.png)'
- en: XNet results and architecture
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: XNet 结果与架构
- en: The vision-centric BEV perception architecture improves the cost-effectiveness
    for mass deployment of autonomous driving solutions, reducing the need for more
    expensive hardware components. The accurate 3D detections and velocity unfold
    a new dimension of redundancy and reduce the reliance on LiDARs and radars. Furthermore,
    the real-time 3D sensible environment perception lessens the dependency on HD
    maps. Both capabilities contribute significantly to a more reliable and cost-effective
    autonomous driving solution.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 以视觉为中心的 BEV 感知架构提高了大规模部署自动驾驶解决方案的性价比，减少了对更昂贵硬件组件的需求。准确的 3D 检测和速度展现了冗余的新维度，减少了对激光雷达和雷达的依赖。此外，实时
    3D 可感知环境减少了对高清地图的依赖。这两种能力都显著地有助于更可靠且具有成本效益的自动驾驶解决方案。
- en: '**The Challenges of XNet**'
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**XNet 的挑战**'
- en: Deploying such a neural network onto production vehicles presents several challenges.
    Firstly, millions of multicamera video clips are necessary to train XNet. These
    clips involve around one billion objects requiring annotation. Based on the current
    annotation efficiency, approximately **2,000 human-years** are needed for annotation.
    Unfortunately, this means that for the in-house annotation team of around 1000
    people at Xpeng, such a task would take around two years to accomplish, which
    is not acceptable. From a model training perspective, it would take **almost a
    year** to train such a network using a single machine. Furthermore, deploying
    such a network without any optimization on an NVIDIA Orin platform would take
    **122% computing power of one chip**.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 将这样的神经网络部署到生产车辆上存在多个挑战。首先，训练 XNet 需要数百万个多摄像头视频片段。这些片段涉及约十亿个需要标注的物体。根据当前的标注效率，大约需要**2,000
    人年**来完成标注。不幸的是，这意味着对于 Xpeng 约1000人的内部标注团队来说，这项任务需要大约两年才能完成，这是不可接受的。从模型训练的角度来看，使用一台机器训练这样的网络将需要**接近一年**。此外，在
    NVIDIA Orin 平台上未经优化地部署这样的网络将消耗**一个芯片的122%计算能力**。
- en: All of these issues present challenges that we have to address for successful
    training and deployment of such a complex and large model.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些问题都对成功训练和部署如此复杂且庞大的模型提出了挑战。
- en: Autolabel
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 自动标注
- en: To improve annotation efficiency, we have developed a highly effective autolabel
    system. This offline sensor fusion stack enhances efficiency by up to 45,000 times,
    enabling us to complete annotation tasks that would have required 200 human years
    in just 17 days.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 为了提高标注效率，我们开发了一个高效的自动标注系统。这个离线传感器融合堆栈将效率提升了多达45,000倍，使我们能够在17天内完成原本需要200人年才能完成的标注任务。
- en: '![](../Images/4450b96601971cbf724e35a9ab4663a0.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4450b96601971cbf724e35a9ab4663a0.png)'
- en: Autolabel system significantly boost annotation efficiency
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 自动标注系统显著提升标注效率
- en: Above is the lidar-based autolabel system, and we also developed a system that
    solely relies on vision sensors. This enables us to annotate clips obtained from
    the customer fleets that don not have lidars. This is a critical part of the data
    closed loop and enhancing the development of a self-evolving perception system.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 以上是基于激光雷达的自动标注系统，我们还开发了一个完全依赖视觉传感器的系统。这使我们能够标注没有激光雷达的客户车队获得的片段。这是数据闭环的关键部分，并增强了自我进化感知系统的发展。
- en: Large Scale Training
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 大规模训练
- en: We optimized the training pipeline for XNet from two perspectives. Firstly,
    we applied mixed precision training and operator optimization techniques to streamline
    the training process on a single node, which reduced the training time by a factor
    of 10\. Next, we partnered with Alicloud and built a GPU cluster with a computation
    power of 600 PFLOPS, allowing us to scale out the training from a single machine
    to multiple machines. This reduced the training time even further, although the
    process was not straightforward as we needed to carefully tweak the training procedure
    to achieve near-linear performance scaling. Overall, we reduced the training time
    for XNet from 276 days to a mere 11 hours. Note that as we add more data into
    the training process, the training time naturally increases, calling for additional
    optimization. Therefore, scaling out optimization remains a continuous and critical
    effort.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从两个角度优化了XNet的训练管道。首先，我们应用了混合精度训练和操作优化技术，以简化单节点上的训练过程，将训练时间缩短了10倍。接下来，我们与阿里云合作，建立了一个计算能力为600
    PFLOPS的GPU集群，使我们能够将训练从单机扩展到多机。虽然这一过程并不简单，因为我们需要仔细调整训练程序以实现接近线性的性能扩展，但它进一步减少了训练时间。总体而言，我们将XNet的训练时间从276天减少到仅11小时。请注意，随着我们将更多数据加入训练过程，训练时间自然会增加，这需要额外的优化。因此，扩展优化仍然是一个持续而关键的工作。
- en: '![](../Images/606d40ee55dd1bbd67a487868b4e68e6.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/606d40ee55dd1bbd67a487868b4e68e6.png)'
- en: Optimization of large scale parallel training pipeline for XNet
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: XNet大规模并行训练管道的优化
- en: Efficient Deployment on Orin
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在Orin上的高效部署
- en: We noted that without any optimization, running XNet on an Nvidia Orin chip
    would require 122% of the chip’s computation power. On analyzing the profiling
    chart displayed at the beginning, we observed that the transformer module consumed
    most of the runtime. This is understandable as the transformer module had not
    received much attention during the Orin chip’s initial design phase. As a result,
    we needed to redesign the transformer module and attention mechanism to support
    the Orin platform, allowing us to achieve a 3x speedup.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 我们注意到，如果没有任何优化，在Nvidia Orin芯片上运行XNet需要使用芯片计算能力的122%。通过分析开头展示的分析图，我们观察到变换器模块消耗了大部分的运行时间。这是可以理解的，因为在Orin芯片的初始设计阶段，变换器模块并没有得到足够的关注。因此，我们需要重新设计变换器模块和注意力机制以支持Orin平台，从而实现了3倍的加速。
- en: '![](../Images/0ba6e350f5231816c60474732c6b9cb1.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0ba6e350f5231816c60474732c6b9cb1.png)'
- en: Extreme optimization of transformer-based XNet on the Orin platform
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在Orin平台上对基于变换器的XNet进行极致优化
- en: Motivated to optimize further, we progressed to optimize the network through
    pruning, resulting in an additional 2.6x speedup. Lastly, employing workload balancing
    between GPU and DLA, we achieved a further 1.7x speedup.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进一步优化，我们通过剪枝优化了网络，结果实现了额外的2.6倍加速。最后，通过在GPU和DLA之间进行负载均衡，我们实现了进一步的1.7倍加速。
- en: With these various optimization techniques, we reduced XNet’s GPU utilization
    from 122% to just 9%. This freed us to explore new possibilities in architecture
    on the Orin platform.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这些优化技术，我们将XNet的GPU利用率从122%降低到仅9%。这使我们能够在Orin平台上探索新的架构可能性。
- en: Self-evolving Data Engine
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 自我进化数据引擎
- en: With the implementation of XNet architecture, we can now initiate data-driven
    iterations to boost the model’s performance. To accomplish this, we first identify
    corner cases on the car and then deploy configurable triggers to the customer
    fleet to collect relevant images. Subsequently, we retrieve images from collected
    data, based on a short description in natural language or an image itself. In
    doing so, we leverage recent advancements in large language models to increase
    the efficiency of dataset curation and annotation.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 通过实施XNet架构，我们现在可以启动数据驱动的迭代以提升模型性能。为此，我们首先在汽车上识别角落案例，然后向客户车队部署可配置触发器以收集相关图像。随后，我们根据自然语言的简短描述或图像本身从收集的数据中检索图像。在此过程中，我们利用大型语言模型的最新进展来提高数据集策划和注释的效率。
- en: '![](../Images/391d72a6b66e90910e8d16230089d311.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/391d72a6b66e90910e8d16230089d311.png)'
- en: Data engine helps improve XNet performance
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 数据引擎有助于提高XNet性能
- en: With the XNet architecture and data engine, we have created a scalable and self-evolving
    perception system.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 通过XNet架构和数据引擎，我们创建了一个可扩展且自我演化的感知系统。
- en: The Future
  id: totrans-52
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 未来展望
- en: The latest release of Xpeng Highway NGP 2.0 unifies highway and city pilot solutions,
    allowing users to drop a pin in a different city and have a smooth experience
    from start to finish. This unification is made possible by XNet, which provides
    a solid foundation for a unified stack across all scenarios. The ultimate goal
    is to enable point-to-point user experience with end-to-end autonomous driving.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 最新发布的Xpeng Highway NGP 2.0统一了高速公路和城市驾驶解决方案，让用户可以在不同城市放置一个定位点，从头到尾享受流畅的体验。这种统一是通过XNet实现的，XNet为所有场景的统一堆栈提供了坚实的基础。最终目标是实现点对点用户体验的端到端自动驾驶。
- en: In order to make the autonomous driving system end-to-end differentiable, another
    critical missing piece is a machine-learning based planning stack. Learning-based
    planning solutions can be largely divided into imitation learning or reinforcement
    learning approaches. Recent progress in large language models (LLMs) also spells
    great potential for the advancement of this important topic. The following [Github
    repo](https://github.com/OpenDriveLab/End-to-end-Autonomous-Driving) is a live
    collection of relevant work in the burgeoning field of end-to-end autonomous driving.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使自动驾驶系统具备端到端可微分性，另一个关键缺失的部分是基于机器学习的规划堆栈。基于学习的规划解决方案大致可以分为模仿学习或强化学习方法。大型语言模型（LLMs）的最新进展也为这一重要主题的发展带来了巨大潜力。以下的[Github
    repo](https://github.com/OpenDriveLab/End-to-end-Autonomous-Driving)是端到端自动驾驶新兴领域相关工作的实时集合。
- en: '[](https://github.com/OpenDriveLab/End-to-end-Autonomous-Driving?source=post_page-----c6e3f1e46ae0--------------------------------)
    [## GitHub - OpenDriveLab/End-to-end-Autonomous-Driving: All you need for End-to-end
    Autonomous Driving'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://github.com/OpenDriveLab/End-to-end-Autonomous-Driving?source=post_page-----c6e3f1e46ae0--------------------------------)
    [## GitHub - OpenDriveLab/End-to-end-Autonomous-Driving: 所有你需要的端到端自动驾驶资源'
- en: All you need for End-to-end Autonomous Driving. Contribute to OpenDriveLab/End-to-end-Autonomous-Driving
    development by…
  id: totrans-56
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 所有你需要的端到端自动驾驶资源。通过贡献于OpenDriveLab/End-to-end-Autonomous-Driving的发展…
- en: github.com](https://github.com/OpenDriveLab/End-to-end-Autonomous-Driving?source=post_page-----c6e3f1e46ae0--------------------------------)
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: github.com](https://github.com/OpenDriveLab/End-to-end-Autonomous-Driving?source=post_page-----c6e3f1e46ae0--------------------------------)
- en: Takeaways
  id: totrans-58
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 重点总结
- en: Divide and conquer reaches 80% of performance with 20% effort. End-to-end approaches
    aim to break the 80% performance ceiling, at potentially much greater cost.
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分而治之能够以20%的努力达到80%的性能。端到端方法旨在突破80%的性能上限，但可能需要更大的成本。
- en: XNet is an end-to-end perception system and one critical step toward an end-to-end
    full-stack solution. It requires significant engineering effort (80%) according
    to the 80–20 rule.
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: XNet是一个端到端的感知系统，是迈向端到端全栈解决方案的重要一步。根据80-20法则，它需要显著的工程投入（80%）。
- en: The large amount of annotation needed for XNet calls for automatic annotation,
    as human annotation is not feasible. The autolabel system can boost efficiency
    by 45000 times.
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: XNet所需的大量注释需要自动注释，因为人工注释不可行。自动标签系统可以提高45000倍的效率。
- en: Large scale training requires optimization of training on a single machine,
    and scaling out from one machine to multiple machines.
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 大规模训练需要优化单机训练，并从一台机器扩展到多台机器。
- en: XNet deployment on Nvidia Orin platform requires refactoring of transformer
    module.
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在Nvidia Orin平台上部署XNet需要重构变压器模块。
- en: References
  id: totrans-64
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考资料
- en: For the unique challenges in deploying mass-production autonomous driving in
    China, please refer to the following link. This was also part of the same invited
    talk at CVPR 2023.
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 针对在中国部署大规模生产自动驾驶所面临的独特挑战，请参阅以下链接。这也是在 CVPR 2023 上的邀请讲座的一部分。
- en: '[](https://medium.com/@patrickllgc/challenges-of-mass-production-autonomous-driving-in-china-407c7e2dc5d8?source=post_page-----c6e3f1e46ae0--------------------------------)
    [## Challenges of Mass Production Autonomous Driving in China'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@patrickllgc/challenges-of-mass-production-autonomous-driving-in-china-407c7e2dc5d8?source=post_page-----c6e3f1e46ae0--------------------------------)
    [## 中国大规模生产自动驾驶面临的挑战'
- en: And Xpeng’s Answer to Them
  id: totrans-67
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 以及 Xpeng 对这些问题的回答
- en: medium.com](https://medium.com/@patrickllgc/challenges-of-mass-production-autonomous-driving-in-china-407c7e2dc5d8?source=post_page-----c6e3f1e46ae0--------------------------------)
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: medium.com](https://medium.com/@patrickllgc/challenges-of-mass-production-autonomous-driving-in-china-407c7e2dc5d8?source=post_page-----c6e3f1e46ae0--------------------------------)
- en: A review of academic research on Transformer-based BEV perception algorithm.
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一项关于基于 Transformer 的 BEV 感知算法的学术研究回顾。
- en: '[https://medium.com/towards-data-science/monocular-bev-perception-with-transformers-in-autonomous-driving-c41e4a893944](https://medium.com/towards-data-science/monocular-bev-perception-with-transformers-in-autonomous-driving-c41e4a893944)'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://medium.com/towards-data-science/monocular-bev-perception-with-transformers-in-autonomous-driving-c41e4a893944](https://medium.com/towards-data-science/monocular-bev-perception-with-transformers-in-autonomous-driving-c41e4a893944)'
- en: 'Xpeng 1024 Tech Day 2022: [https://www.youtube.com/watch?v=0dEoctcK09Q](https://www.youtube.com/watch?v=0dEoctcK09Q)'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Xpeng 1024 科技日 2022: [https://www.youtube.com/watch?v=0dEoctcK09Q](https://www.youtube.com/watch?v=0dEoctcK09Q)'
