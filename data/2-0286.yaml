- en: An AI-Powered Analysis of our Postal Service Through Tweets
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通过推文对我们邮政服务的AI驱动分析
- en: 原文：[https://towardsdatascience.com/an-ai-powered-analysis-of-our-postal-service-through-tweets-fa1764409905](https://towardsdatascience.com/an-ai-powered-analysis-of-our-postal-service-through-tweets-fa1764409905)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/an-ai-powered-analysis-of-our-postal-service-through-tweets-fa1764409905](https://towardsdatascience.com/an-ai-powered-analysis-of-our-postal-service-through-tweets-fa1764409905)
- en: Deciphering Customer Voices with AI
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 用AI解码客户声音
- en: Delving into Machine Learning, Topic Modeling, and Sentiment Analysis to Uncover
    Valuable Customer Perspectives
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 深入探讨机器学习、主题建模和情感分析，以揭示有价值的客户观点
- en: '[](https://johnadeojo.medium.com/?source=post_page-----fa1764409905--------------------------------)[![John
    Adeojo](../Images/f6460fae462b055d36dce16fefcd142c.png)](https://johnadeojo.medium.com/?source=post_page-----fa1764409905--------------------------------)[](https://towardsdatascience.com/?source=post_page-----fa1764409905--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----fa1764409905--------------------------------)
    [John Adeojo](https://johnadeojo.medium.com/?source=post_page-----fa1764409905--------------------------------)'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://johnadeojo.medium.com/?source=post_page-----fa1764409905--------------------------------)[![John
    Adeojo](../Images/f6460fae462b055d36dce16fefcd142c.png)](https://johnadeojo.medium.com/?source=post_page-----fa1764409905--------------------------------)[](https://towardsdatascience.com/?source=post_page-----fa1764409905--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----fa1764409905--------------------------------)
    [John Adeojo](https://johnadeojo.medium.com/?source=post_page-----fa1764409905--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----fa1764409905--------------------------------)
    ·13 min read·Mar 22, 2023
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----fa1764409905--------------------------------)
    ·13分钟阅读·2023年3月22日
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/3ca21fa5a67996b27ed77e77c334b936.png)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3ca21fa5a67996b27ed77e77c334b936.png)'
- en: 'Image by Author: AI generated sentiment and topic for #royalmail'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '图片作者：AI生成的情感和主题分析 #royalmail'
- en: Introduction
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: My partner and I usually experience an excellent postal service. Most of the
    time letters arrive to our home un-opened and delivered in a timely fashion. That’s
    why when our post didn’t arrive for a few weeks we thought it was quite strange.
    After some diligent web searching, we discovered the most likely cause to this
    service disruption was strikes. As a data scientist this whole episode got me
    thinking…
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 我和我的搭档通常体验到优质的邮政服务。大多数时候，信件会及时到达我们家，并且没有被拆开。因此，当我们的邮件几周没到时，我们觉得非常奇怪。在经过一番认真的网络搜索后，我们发现这次服务中断最可能的原因是罢工。作为数据科学家，这整个事件让我开始思考……
- en: Is there a way to leverage online data to track these types of incidents?
  id: totrans-11
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 是否有办法利用在线数据追踪这些类型的事件？
- en: The answer to this question is yes, and I have already built a prototype which
    is available for you to play with. I recommend doing so before reading on as it
    will give you a feel for things before getting into the technical details.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 对这个问题的回答是肯定的，我已经构建了一个原型，你可以试玩一下。我建议在继续阅读之前先试用一下，这样可以让你在进入技术细节之前对事物有一个了解。
- en: 🌏 [Explore the m(app)](https://john-adeojo-royalmail-dash-scriptsstreamlitstreamlit-rm-o2f2wo.streamlit.app/)
    — This is best opened on a computer, although it will work on a mobile phone.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 🌏 [探索应用](https://john-adeojo-royalmail-dash-scriptsstreamlitstreamlit-rm-o2f2wo.streamlit.app/)
    — 最好在电脑上打开，尽管手机也能使用。
- en: I’ll spend the remainder of this write up walking you through how I went about
    answering this question. This is pretty much an end to end machine learning project
    exploring aspects of software engineering, social media data mining, topic modelling,
    transformers, custom loss functions, transfer learning, and data visualisation.
    If that sounds interesting to you at all grab a snack or a drink and get comfortable
    because this might be quite a long one but hopefully worth the read.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 我将在这篇文章的剩余部分带你了解我是如何回答这个问题的。这几乎是一个端到端的机器学习项目，涉及软件工程、社交媒体数据挖掘、主题建模、变压器、自定义损失函数、迁移学习和数据可视化等方面。如果这些对你有吸引力，拿个小吃或饮料，坐下来，因为这可能会比较长，但希望值得一读。
- en: '**Disclaimer***: This article is an independent analysis of tweets containing
    the #royalmail hashtag and is not affiliated with, endorsed, or sponsored by Royal
    Mail Group Ltd. The opinions and findings expressed within this article are solely
    those of the author and do not represent the views or official positions of Royal
    Mail Group Ltd or any of its subsidiaries.*'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '**免责声明**：本文是对包含 #royalmail 标签的推文的独立分析，与 Royal Mail Group Ltd 没有任何关联、认可或赞助。本文中表达的观点和发现仅代表作者个人观点，不代表
    Royal Mail Group Ltd 或其任何子公司的观点或官方立场。'
- en: Approach
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 方法
- en: When seeking to understand what people think, Twitter is always a good starting
    point. Much of what people post on Twitter is public and easily accessible through
    their API. It’s the kind of no holds barred verbal arena you would expect to find
    plenty of insights on customer service. I got curious and conducted a quick twitter
    search myself starting simply with ‘#royalmail’. And voila! a tonne of tweets.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在寻求了解人们的想法时，Twitter 总是一个很好的起点。人们在 Twitter 上发布的大部分内容是公开的，并且可以通过其 API 轻松访问。这是你可以找到大量客户服务见解的那种不受限制的言语领域。我感到好奇，自己进行了一个简单的
    Twitter 搜索，从‘#royalmail’开始。瞧！一堆推文。
- en: With my data source identified, the next thing I did was to figure out how I
    would “mine” issues raised from those tweets. Topic modelling came to mind immediately
    as something to try. I figured that using some kind of clustering on the tweets
    could reveal some latent topics. I’ll spend the remainder of the write up going
    into some technical details. This won’t be a step-by-step, but rather a peek over
    my shoulder and a window into my thought process in putting this project together.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 确定了数据源后，我做的下一步是找出如何“挖掘”那些推文中提出的问题。主题建模立即浮现在脑海中。我认为对推文进行某种类型的聚类可以揭示一些潜在的主题。我将花费剩下的篇幅深入一些技术细节。这不会是逐步的过程，而是窥视我的肩膀以及对我在这个项目中思考过程的窗口。
- en: Software Engineering
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 软件工程
- en: '**Development environment**: I do the majority of my ML projects in python
    so my preferred IDE is Jupyter labs. I find it useful to be able to quickly toggle
    between Jupyter notebooks, python scripts, and the terminal.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '**开发环境**：我大多数的机器学习项目都是用 python 完成的，所以我偏好使用 Jupyter labs。我发现能够快速切换 Jupyter notebooks、python
    脚本和终端非常有用。'
- en: '**File structure**: This is a rather complex project, if I do say so myself.
    There are several processes to consider here and therefore it’s not something
    that could just be done from the safety of a Jupyter notebook. Listing out all
    of these we have; data extraction, data processing, topic modeling, machine learning,
    and data visualisation. To help create some order I usually start by establishing
    an appropriate file structure. You can, and probably should leverage bash scripting
    to do this.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '**文件结构**：这是一个相当复杂的项目，如果我这么说的话。这里有几个过程需要考虑，因此这不是一个可以仅从 Jupyter notebook 的安全环境中完成的任务。列出所有这些过程，我们有：数据提取、数据处理、主题建模、机器学习和数据可视化。为了帮助创建一些秩序，我通常会先建立一个合适的文件结构。你可以，也应该利用
    bash 脚本来完成这个任务。'
- en: '[PRE0]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '**Modularisation**: I broke each process down into modules making it easy to
    re-use, adapt and tweak things for different use cases. Modules also help keep
    your code ‘clean’. Without the modular approach I would have ended up with a Jupyter
    notebook or python script thousands of lines long, very unappealing and difficult
    to de-bug.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '**模块化**：我将每个过程分解为模块，使其易于重用、调整和修改以适应不同的使用场景。模块还帮助保持代码的“干净”。如果没有模块化的方法，我会得到一个长达数千行的
    Jupyter notebook 或 python 脚本，非常不吸引人且难以调试。'
- en: '**Version control**: With complex projects, you do not want to lose your progress,
    overwrite something important, or mess up beyond repair. GitHub is really the
    perfect solution for this as it makes it hard to mess up badly. I get started
    by creating a remote repo and cloning it to my local machine allowing me to sleep
    easy knowing all my hard work is backed up. GitHub desk top allows me to carefully
    track any changes before committing them back to the remote repository.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '**版本控制**：对于复杂的项目，你不想丢失进度、覆盖重要内容或搞得一团糟。GitHub 是一个完美的解决方案，因为它使得搞砸的可能性很小。我首先创建一个远程仓库并将其克隆到本地机器上，这样我可以安心地知道我所有的辛勤工作都有备份。GitHub
    desktop 允许我在提交到远程仓库之前仔细跟踪任何更改。'
- en: '**Packages:** I leveraged a tonne of open source packages, I’ll list the key
    ones below and provide links.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '**包：** 我利用了大量的开源包，下面我将列出关键的包并提供链接。'
- en: '[Transformers](https://huggingface.co/docs/transformers/installation): API
    for hugging face large language model.'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Transformers](https://huggingface.co/docs/transformers/installation)：hugging
    face 大型语言模型的 API。'
- en: '[Pytorch](https://pytorch.org/get-started/locally/): Framework for building
    and customising transformers.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Pytorch](https://pytorch.org/get-started/locally/)：用于构建和定制变换器的框架。'
- en: '[Streamlit](https://streamlit.io/): For building web applications.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Streamlit](https://streamlit.io/)：用于构建 web 应用程序。'
- en: '[Scikit Learn](https://scikit-learn.org/stable/install.html): Framework for
    machine learning.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Scikit Learn](https://scikit-learn.org/stable/install.html)：机器学习框架。'
- en: '[UMAP](https://umap-learn.readthedocs.io/en/latest/): Open source implementation
    of the UMAP algorithm.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[UMAP](https://umap-learn.readthedocs.io/en/latest/)：UMAP 算法的开源实现。'
- en: '[HDBSCAN](https://hdbscan.readthedocs.io/en/latest/how_hdbscan_works.html):
    Open source implementation of the HDSCAN algorithm.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[HDBSCAN](https://hdbscan.readthedocs.io/en/latest/how_hdbscan_works.html)：HDSCAN
    算法的开源实现。'
- en: '[Folium](https://python-visualization.github.io/folium/#:~:text=folium%20makes%20it%20easy%20to,as%20markers%20on%20the%20map.):
    For geographic data visualisation.'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Folium](https://python-visualization.github.io/folium/#:~:text=folium%20makes%20it%20easy%20to,as%20markers%20on%20the%20map.)：用于地理数据可视化。'
- en: '[CUDA](https://docs.nvidia.com/cuda/cuda-installation-guide-microsoft-windows/):
    Parallel computing platform for leveraging the power of your GPU.'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[CUDA](https://docs.nvidia.com/cuda/cuda-installation-guide-microsoft-windows/)：利用
    GPU 功能的并行计算平台。'
- en: '[Seaborn](https://seaborn.pydata.org/): A library for data visualisation in
    python.'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Seaborn](https://seaborn.pydata.org/)：用于 Python 中数据可视化的库。'
- en: '[Pandas](https://pandas.pydata.org/): A library for handling structured data.'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Pandas](https://pandas.pydata.org/)：处理结构化数据的库。'
- en: '[Numpy](https://numpy.org/): A library for performing numeric operations in
    python.'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Numpy](https://numpy.org/)：在 Python 中执行数值操作的库。'
- en: '**Environment management**: Having access to a wealth of libraries on the internet
    is fantastic, but your environment can quickly run away with you. To manage this
    complexity I like to enforce upon myself a clean environment policy whenever I
    start a new project. It’s strictly one environment per project. I choose to use
    [Anaconda](https://www.anaconda.com/) as my choice of environment manager because
    of the flexibility it gives me.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '**环境管理**：虽然互联网提供了丰富的库，但环境管理很容易变得混乱。为了管理这种复杂性，我喜欢在开始新项目时执行干净环境政策。每个项目严格一个环境。我选择使用
    [Anaconda](https://www.anaconda.com/) 作为我的环境管理器，因为它提供了很大的灵活性。'
- en: '*note: for the purposes of this project I did create separate environments
    and GitHub repositories for the streamlet web application and the topic modeling.*'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '*注意：出于这个项目的目的，我确实为 streamlit web 应用程序和主题建模创建了单独的环境和 GitHub 仓库。*'
- en: Data
  id: totrans-39
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据
- en: 'I used the Twitter API to extract around 30k publicly available tweets searching
    #royalmail. I want to stress here that only data that is publicly available can
    be extracted with the Twitter API alleviating some of the data privacy concerns
    one may have.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '我使用 Twitter API 提取了大约 30k 条公开可用的推文，搜索 #royalmail。我想强调的是，只有公开的数据才能通过 Twitter
    API 提取，这减轻了一些可能存在的数据隐私问题。'
- en: Twitter data is incredibly messy and notoriously difficult to work with for
    any natural language processing (nlp) tasks. It’s social media data loaded with
    emoji’s, grammatical inconsistencies, special characters, expletives, URLS, and
    every other hurdle that comes with free form text. I wrote my own custom scripts
    to clean the data for this particular project. It was primarily getting rid of
    URLs and annoying stop words. I have given a snippet for the “lite” version, but
    I did also use a more heavy duty version during clustering.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: Twitter 数据非常混乱，对于任何自然语言处理（nlp）任务都极其困难。它是充满表情符号、语法不一致、特殊字符、脏话、网址以及所有其他自由文本带来的障碍的社交媒体数据。我为这个特定项目编写了自己的自定义脚本来清理数据。主要是去除网址和烦人的停用词。我提供了“简化”版本的代码片段，但在聚类过程中也使用了更高级的版本。
- en: Module for cleaning URLs from tweets
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 用于清理推文中的网址的模块
- en: '*Please note that this is within Twitters terms of service. They allow analysis,
    aggregation of publicly available data via* [*their API*](https://developer.twitter.com/en/developer-terms/agreement-and-policy)*.
    The data is permitted for both* [*non-commercial and commercial use*](https://developer.twitter.com/en/developer-terms/commercial-terms#:~:text=Know%20that%20if%20you%20are,access%20are%20available%20for%20free).&text=Your%20product%20or%20service%20is%20monetized%20if%20you%20earn%20money%20from%20it.)*.*'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '*请注意，这符合 Twitter 的服务条款。他们允许通过* [*他们的 API*](https://developer.twitter.com/en/developer-terms/agreement-and-policy)*对公开可用的数据进行分析和聚合。这些数据既允许用于*
    [*非商业和商业用途*](https://developer.twitter.com/en/developer-terms/commercial-terms#:~:text=Know%20that%20if%20you%20are,access%20are%20available%20for%20free).&text=Your%20product%20or%20service%20is%20monetized%20if%20you%20earn%20money%20from%20it.)*。*'
- en: Topic Modelling
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 主题建模
- en: The topic modelling approach I used draws inspiration from BERT topic¹. I had
    initially tried Latent Dirichlet Allocation , but struggled to get anything coherent.
    BERT topic was a great reference point, but I had noticed that it hadn’t explicitly
    been designed to extract topics from messy Twitter data. Following many of the
    same logical steps as BERT topic, I adapted the approach a little bit for the
    task.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 我使用的主题建模方法受到 BERT 主题¹ 的启发。我最初尝试了潜在狄利克雷分配（Latent Dirichlet Allocation），但难以得到连贯的结果。BERT
    主题是一个很好的参考点，但我注意到它并没有专门设计来从混乱的 Twitter 数据中提取主题。按照与 BERT 主题类似的逻辑步骤，我对方法进行了稍微的调整以适应这个任务。
- en: At a high level BERT topic uses the BERT model to generate embeddings, performs
    dimensionality reduction and clustering to reveal latent topics in documents.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 从高层次来看，BERT 主题使用 BERT 模型生成嵌入，进行降维和聚类以揭示文档中的潜在主题。
- en: My approach leveraged the twitter-xlm-roberta-base² model to generate embeddings.
    This transformer has been pretrained on twitter data and captures all the messy
    nuances, emojis and all. Embeddings, are simply a way to represent sentences in
    numeric form such that both syntactical and semantical information is preserved.
    Embeddings are learnt by transformers through self-attention. The amazing thing
    about all the recent innovation in the large language model space is that one
    can leverage state-of-the-art models to generate embeddings for one’s own purposes.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 我的做法利用了 twitter-xlm-roberta-base² 模型来生成嵌入。这个变换器已经在 Twitter 数据上进行过预训练，捕捉了所有复杂的细微差别，包括表情符号。嵌入只是以数字形式表示句子的一种方式，从而保留了语法和语义信息。嵌入是通过自注意力机制由变换器学习的。所有最近的大型语言模型领域的创新令人惊叹的是，人们可以利用最先进的模型为自己的目的生成嵌入。
- en: I used the UMAP algorithm to project the tweet embeddings into a two dimensional
    space and HDBSCAN to identify clusters. Treating each cluster as a document, I
    generated TF-IDF scores to extract a list of key words that roughly ‘define’ each
    cluster forming my initial topics.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 我使用 UMAP 算法将推文嵌入投影到二维空间中，并使用 HDBSCAN 识别簇。将每个簇视为一个文档，我生成了 TF-IDF 分数，以提取一个关键字列表，大致‘定义’每个簇，从而形成我的初步主题。
- en: '*TF-IDF is a handy way to measure a word’s significance in a cluster, considering
    how often it appears in that specific cluster and how rare it is in a larger group
    of clusters. It helps identify words that are unique and meaningful in each cluster.*'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '*TF-IDF 是一种便捷的方式来衡量一个词在某个簇中的重要性，考虑它在该簇中出现的频率以及它在更大簇组中的稀有程度。它有助于识别每个簇中独特且有意义的词。*'
- en: '*Some of these dimensionality reductions can be hard to make sense of at first.
    I found these resources useful for helping me get to grips with the algorithms.*'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '*这些降维方法有时一开始可能难以理解。我发现这些资源对帮助我掌握算法很有用。*'
- en: '[*Understanding UMAP*](https://pair-code.github.io/understanding-umap/) *—
    An excellent resource that helps you visualise and understand the impact of adjusting
    hyperparameters.*'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '[*理解 UMAP*](https://pair-code.github.io/understanding-umap/) *— 一个出色的资源，帮助你可视化和理解调整超参数的影响。*'
- en: '[*HDBSCAN Documentation*](https://hdbscan.readthedocs.io/en/latest/how_hdbscan_works.html)
    *— The most coherent explanation of HDBSCAN I could find was provided in the documentation
    itself.*'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '[*HDBSCAN 文档*](https://hdbscan.readthedocs.io/en/latest/how_hdbscan_works.html)
    *— 我能找到的对 HDBSCAN 最连贯的解释是文档本身提供的。*'
- en: Lastly, I tested the coherence of the topics generated by scoring the cosine
    similarity between the topics and the tweets themselves. This sounds rather formulaic
    on paper, but I can assure you this was no straight forward task. Unsupervised
    machine learning of this nature is just trial and error. It took me dozens of
    iterations and manual effort to find the right parameters to get coherent topics
    out of these tweets. So rather than going into the specifics of all the hyperparameters
    I used, I will just talk about the four critical ones that were really a make
    or break for this approach.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我通过对主题和推文本身之间的余弦相似度进行评分，测试生成主题的连贯性。这在纸面上看起来相当公式化，但我可以保证这并非简单的任务。这种无监督机器学习的性质就是反复试验。我花费了几十次迭代和人工努力来找到正确的参数，以从这些推文中提取连贯的主题。因此，与其详细讨论我使用的所有超参数，不如谈谈那些对这种方法至关重要的四个关键参数。
- en: '**Distance metrics**: for topic modelling the distance metric is really the
    difference between forming coherent topics and just generating a random list of
    words. For both UMAP and HDBSCAN I chose cosine distance. The choice here was
    a no-brainer considering my objective, to model topics. Topics are semantically
    similar groups of text, and the best way to measure semantic similarity is cosine
    distance.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '**距离度量**：在主题建模中，距离度量实际上是形成连贯主题与仅生成随机单词列表之间的区别。对于UMAP和HDBSCAN，我选择了余弦距离。考虑到我的目标是建模主题，这个选择毫无疑问是正确的。主题是语义上相似的文本群体，而衡量语义相似度的最佳方法是余弦距离。'
- en: '**Number of words**: after generating the clusters I wanted to understand the
    “contents” of those clusters through TF-IDF. The key metric of choice here is
    how many words to return for each cluster. This could range from one to the number
    of unique words in the whole corpus of text. Too many words, and your topics become
    incoherent, too few and you end up with poor coverage of your cluster. Selecting
    this was a matter of trial and error, after several iterations I landed on 4 words
    per topic.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '**词汇数量**：生成聚类后，我希望通过TF-IDF了解这些聚类的“内容”。这里选择的关键指标是每个聚类返回多少个单词。这可能从一个单词到整个文本语料库中唯一单词的数量不等。单词太多，你的主题会变得不连贯；单词太少，你的聚类覆盖率会很差。选择这个数量是一个反复试验的过程，经过几次迭代，我最终选择了每个主题4个单词。'
- en: '**Scoring**: Topic modelling isn’t an exact science, so some manual intervention
    is required to make sure topics made sense. I could do this for a few hundred
    or even a few thousand tweets, but tens of thousands? That’s not practically feasible.
    So I used a numeric “hack” by scoring the cosine similarity between the TFIDF
    topics generated and the tweets themselves. Again this was a lot of trial and
    error but after several iterations I found an appropriate cut off for cosine similarity
    to be around 0.9\. This left me with around 3k from the original 30k that were
    fairly well categorised. Most importantly, it was a large enough sample size to
    do some supervised machine learning.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '**评分**：主题建模不是一门精确的科学，因此需要一些人工干预以确保主题有意义。我可以对几百甚至几千条推文进行此操作，但十几万条呢？那就不切实际了。因此，我使用了一个数值上的“黑客”方法，通过对生成的TFIDF主题与推文本身之间的余弦相似度进行评分。虽然这涉及很多反复试验，但经过几次迭代，我发现余弦相似度的适当截止值大约为0.9。这使得原本30k条推文中大约有3k条被较好地分类。最重要的是，这提供了一个足够大的样本量来进行一些监督机器学习。'
- en: '**Topics in 2d:** UMAP provides a convenient way to visualise the topics. What
    we can see is that there is a mass of topics in the centre that have been clustered
    together with some smaller niche topics on the edge. It actually reminds me a
    bit of a galaxy. After doing some detective work (manual trawling through spreadsheets)
    I found this to make sense. The mass of topics in the centre are mainly around
    customer service, often complaints. What I thought was particularly fascinating
    was the model’s ability to actually isolate very niche areas. These included politics,
    economics, employment, and philately (which isn’t some minor celebrity, but the
    collection of stamps!). Of course, topics returned by TFIDF were no where near
    this coherent, but I was able to identify 6 well categorised topics from the analysis.
    My final 6 topics were customer service, politics, royal reply, jobs, financial
    news, and philately.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '**二维主题：** UMAP 提供了一种方便的方式来可视化主题。我们可以看到，中心有一大块主题被聚集在一起，而一些较小的利基主题则在边缘。这实际上让我想起了一个星系。经过一些侦查工作（手动浏览电子表格），我发现这确实有意义。中心的大块主题主要围绕客户服务，通常是投诉。我认为特别吸引人的是模型能够实际隔离出非常利基的领域。这些领域包括政治、经济、就业和集邮（这不是某个小明星，而是集邮！）。当然，TF-IDF
    返回的主题远没有这么一致，但我能够从分析中识别出 6 个良好分类的主题。我最终的 6 个主题是客户服务、政治、皇家回复、工作、财经新闻和集邮。'
- en: '**List of four words topics generated by TF-IDF on the clusters and taking
    the 0.9+ cosine similarity to tweets.**'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '**TF-IDF 在集群上生成的四个词汇主题列表，并考虑了与推文的 0.9+ 余弦相似度。**'
- en: 'apprenticeship, jinglejobs, job, label: **Jobs**'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学徒，广告工作，工作，标签：**工作**
- en: 'biggest, boss, revolt, year: **Politics**'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最大，老板，叛乱，年：**政治**
- en: 'birth, reply, royalletters, royalreply: **Royal Reply**'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 出生，回复，皇家信件，皇家回复：**皇家回复**
- en: 'collecting, pack, philatelist, philately: **Philately**'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 收集，包装，集邮爱好者，集邮：**集邮**
- en: 'declares, plc, position, short: **Financial News**'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 声明，股份公司，职位，简短：**财经新闻**
- en: 'definitive, philatelist, philately, presentation: **Philately**'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 确定的，集邮爱好者，集邮，演示：**集邮**
- en: 'driving, infoapply, job, office: **Jobs**'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 驾驶，信息申请，工作，办公室：**工作**
- en: 'driving, job, sm1jobs, suttonjobs: **Jobs**'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 驾驶，工作，sm1jobs，suttonjobs：**工作**
- en: 'ftse, rmg, share, stock: **Financial News**'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: FTSE，RMG，股票，股息：**财经新闻**
- en: 'germany, royal, royalletter, royalreply: **Royal Reply**'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 德国，皇家，皇家信件，皇家回复：**皇家回复**
- en: 'gradjobs, graduatescheme, jobsearch, listen: **Jobs**'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 毕业工作，毕业生计划，求职，收听：**工作**
- en: 'labour, libdems, tory, uk: **Politics**'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 劳工，自由民主党，保守党，英国：**政治**
- en: 'letter, mail, service, strike: **Customer Service**'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 信件，邮件，服务，罢工：**客户服务**
- en: 'luxembourg, royal, royalletter, royalreply: **Royal Reply**'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 卢森堡，皇家，皇家信件，皇家回复：**皇家回复**
- en: 'new, profit, shareholder, world: **Financial News**'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 新，利润，股东，世界：**财经新闻**
- en: 'plc, position, reduced, wace: **Financial News**'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 股份公司，职位，减少，WACE：**财经新闻**
- en: '![](../Images/8bc6976977c0bb1f8442f12aae195bf2.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8bc6976977c0bb1f8442f12aae195bf2.png)'
- en: 'Image by Author: A 2d representation of the embedding after applying UMAP'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 作者图片：应用 UMAP 后的嵌入二维表示
- en: '![](../Images/8bd6950f177d003c98cf88bfbe2c538b.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8bd6950f177d003c98cf88bfbe2c538b.png)'
- en: 'Image by Author: A view of the the topics after applying HDBSCAN. Yellow mass
    is customer service related'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 作者图片：应用 HDBSCAN 后的主题视图。黄色区域是与客户服务相关
- en: '*The topic modelling was fiddly and definitely not something you want to rely
    on continuously for generating insights. As far as I’m concerned it should be
    an exercise that you conduct once every few months or so (depending on the fidelity
    of your data), just in case anything new comes up.*'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '*主题建模繁琐，绝对不是你想持续依赖以生成洞见的东西。就我而言，这应该是你每隔几个月进行一次的练习（取决于数据的保真度），以防出现任何新情况。*'
- en: Transfer Learning
  id: totrans-80
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 转移学习
- en: Having performed the arduous task of topic modelling, I had some labels and
    a decent sized data set of just under 3k observations for training a model. Leveraging
    a pretrained transformer means not having to train from scratch, not having to
    build my own architecture and harnessing the power of the model’s existing knowledge.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在完成艰巨的主题建模任务后，我获得了一些标签和一个接近 3k 观察数据的合适数据集，用于训练模型。利用预训练的变换器意味着不必从头开始训练，也不需要构建自己的架构，能够利用模型现有知识的力量。
- en: Data Splitting
  id: totrans-82
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据拆分
- en: 'I proceeded with the standard Train, Validation, and Test splits with 80% of
    the observations being allocated to train. See script below:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 我使用标准的训练、验证和测试拆分，80% 的观察数据用于训练。请参见下面的脚本：
- en: Data splitting script
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 数据拆分脚本
- en: Implementing focal loss with a custom trainer
  id: totrans-85
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实现自定义训练器的焦点损失
- en: Model training turned out to be less straight forward than I had anticipated,
    and this wasn’t because of the hardware requirements but rather the data itself.
    What I was dealing with was a highly imbalanced multiclass classification problem.
    Customer service observations were at least ten times as prominent in the data
    set than the next most prominent class. This caused the model performance to be
    overwhelmed by the customer service class leading to low recall and precision
    for the less prominent classes.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 模型训练结果比我预期的要复杂，这并不是因为硬件要求，而是数据本身。我要处理的是一个高度不平衡的多类分类问题。客户服务观察在数据集中至少比下一个最突出类别多十倍。这导致模型性能被客户服务类别所压倒，从而导致其他不突出类别的召回率和精确度较低。
- en: I started with something simple initially applying class weights and cross entropy
    loss, but this didn’t do the trick. After a quick google search I discovered that
    the loss function focal loss has been used successfully to solve class imbalance.
    Focal loss reshapes the cross entropy loss to “down-weight” the loss assigned
    to well classified examples³.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 我最初从简单的类权重和交叉熵损失开始，但这没有奏效。经过快速的Google搜索，我发现焦点损失函数在解决类别不平衡问题上取得了成功。焦点损失重新塑造了交叉熵损失，以“降低”分配给分类良好示例的损失³。
- en: '*The original paper on focal loss focussed on computer vision tasks where images
    had shallow depth of field. The image below is an example of shallow depth of
    field, the foreground is prominent but the background very low res. This type
    of extreme imbalance between foreground and background is analogous to the imbalance
    I had to deal with to classify the tweets.*'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '*焦点损失的原始论文关注于计算机视觉任务，其中图像具有浅景深。下面的图像是浅景深的一个例子，前景突出但背景分辨率很低。这种极端的前景与背景之间的不平衡类似于我在分类推文时需要处理的不平衡。*'
- en: '![](../Images/00dea754e944d517bbed76b7eeea6859.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/00dea754e944d517bbed76b7eeea6859.png)'
- en: Photo by [Raphael Wild](https://unsplash.com/@veloradio?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 照片由[Raphael Wild](https://unsplash.com/@veloradio?utm_source=medium&utm_medium=referral)拍摄，发布于[Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: Below I have laid out my implementation of focal loss within a custom trainer
    object.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是我在自定义训练器对象中实现焦点损失的过程。
- en: '*note that the class weights (alpha) are hard coded. You will need to adjust
    these if you want to use this for you own purposes.*'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '*注意，类权重（alpha）是硬编码的。如果你想用于自己的目的，需要调整这些权重。*'
- en: Implementation of the focal loss. Custom trainer is just standard trainer with
    added focal loss
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 焦点损失的实现。自定义训练器只是标准训练器加上了焦点损失
- en: Model Training
  id: totrans-94
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型训练
- en: After a bit of customisation I was able to fit a model (and in under 7 minutes
    thanks to my GPU and CUDA). Focal loss vs. time gives us some evidence that the
    model was close to converging.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 在进行了一些自定义调整后，我成功地拟合了一个模型（得益于我的GPU和CUDA，时间不到7分钟）。Focal loss与时间的关系给我们提供了一些证据，表明模型接近收敛。
- en: '![](../Images/0a714d949b1f46e3251ba33e5b444f62.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0a714d949b1f46e3251ba33e5b444f62.png)'
- en: 'Image by Author: Focal loss vs time step'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图像：Focal loss与时间步长的关系
- en: Model training script. Notice the customer trainer is imported and implemented
    here.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 模型训练脚本。注意这里导入并实现了客户自定义的训练器。
- en: Model Performance
  id: totrans-99
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型性能
- en: The model was assessed on the test data set which included 525 randomly selected
    labelled examples. The performance appears impressive, with fairly high precision
    and recall across all classes. I would caveat that test performance is probably
    optimistic due to the small sample size and there is likely to be more variance
    in the nature of these tweets outside of our sample. However, we are dealing with
    a relatively narrow domain (#royalmail) so variance is likely to be narrower than
    it would be for something more general purpose.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 模型在包括525个随机选择的标记样本的测试数据集上进行了评估。性能表现令人印象深刻，各类的精确度和召回率都相当高。我需要强调的是，由于样本量较小，测试性能可能过于乐观，并且样本之外的推文性质可能有更多变化。然而，我们处理的是一个相对狭窄的领域（#royalmail），因此变化可能比更通用的领域要小。
- en: '![](../Images/b42ce43a03c1d467a4a40e164bf175a4.png)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b42ce43a03c1d467a4a40e164bf175a4.png)'
- en: 'Image by Author: confusion matrix (test dataset)'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图像：混淆矩阵（测试数据集）
- en: '![](../Images/387cd526d73841e894dd49cb85406f8f.png)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/387cd526d73841e894dd49cb85406f8f.png)'
- en: 'Image by Author: model performance metrics on Test'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图像：模型在测试集上的性能指标
- en: Geographic Visualisation
  id: totrans-105
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 地理可视化
- en: To effectively visualize the wealth of information I gathered, I decided to
    create a sentiment map. By utilizing my trained model, I generated topics for
    tweets posted between January and March 2023\. Additionally, I employed the pretrained
    `[twitter-roberta-base-sentiment](https://huggingface.co/cardiffnlp/twitter-roberta-base-sentiment-latest)`
    model from Cardiff NLP to assess the sentiment of each tweet. To build the final
    web application, I used Streamlit.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 为了有效地可视化我收集的大量信息，我决定创建一个情感地图。通过利用我训练的模型，我为2023年1月至3月之间的推文生成了话题。此外，我使用了来自Cardiff
    NLP的预训练 `[twitter-roberta-base-sentiment](https://huggingface.co/cardiffnlp/twitter-roberta-base-sentiment-latest)`
    模型来评估每条推文的情感。为了构建最终的网络应用程序，我使用了Streamlit。
- en: script for generating the Streamlit web application
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 生成Streamlit网络应用程序的脚本
- en: Business Applications
  id: totrans-108
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 商业应用
- en: 'The current app serves as a basic prototype, but it can be expanded to uncover
    more profound insights. I’ll briefly discuss a few potential extensions below:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 当前的应用程序作为一个基本原型存在，但可以扩展以揭示更深刻的见解。我将在下面简要讨论一些潜在的扩展：
- en: '**Temporal Filtering**: Incorporate a date range filter, allowing users to
    explore tweets within specific time periods. This can help identify trends and
    changes in sentiment over time.'
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**时间过滤**：加入日期范围过滤器，允许用户在特定时间段内浏览推文。这有助于识别趋势和情感随时间的变化。'
- en: '**Interactive Visualizations**: Implement interactive charts and visualizations
    that enable users to explore relationships between sentiment, topics, and other
    factors in the dataset.'
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**互动可视化**：实现互动图表和可视化工具，使用户能够探索数据集中情感、话题和其他因素之间的关系。'
- en: '**Real-time Data**: Connect the app to live Twitter data, enabling real-time
    analysis and visualization of sentiment and topics as they emerge.'
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**实时数据**：将应用程序连接到实时Twitter数据，启用实时分析和可视化情感及话题。'
- en: '**Advanced Filtering**: Provide more advanced filtering options, such as filtering
    by user, hashtag, or keyword, to allow for more targeted analysis of specific
    conversations and trends.'
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**高级过滤**：提供更多高级过滤选项，如按用户、话题标签或关键字过滤，以便对特定对话和趋势进行更有针对性的分析。'
- en: By extending the app with these features, you can provide users with a more
    powerful and insightful tool for exploring and understanding sentiment and topics
    in tweets.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 通过扩展应用程序这些功能，你可以为用户提供一个更强大、更有洞察力的工具，以探索和理解推文中的情感和话题。
- en: '[GitHub Repo](https://github.com/john-adeojo/twitter_issues_dashboard)'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '[GitHub 仓库](https://github.com/john-adeojo/twitter_issues_dashboard)'
- en: Thanks for reading!
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 感谢阅读！
- en: '[](https://medium.com/@johnadeojo/membership?source=post_page-----fa1764409905--------------------------------)
    [## Join Medium with my referral link - John Adeojo'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@johnadeojo/membership?source=post_page-----fa1764409905--------------------------------)
    [## 通过我的推荐链接加入 Medium - John Adeojo'
- en: I share data science projects, experiences, and expertise to assist you on your
    journey You can sign up to medium via…
  id: totrans-118
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 我分享数据科学项目、经验和专业知识，以帮助你在旅程中。你可以通过…注册Medium。
- en: medium.com](https://medium.com/@johnadeojo/membership?source=post_page-----fa1764409905--------------------------------)
    [](https://www.john-adeojo.com/?source=post_page-----fa1764409905--------------------------------)
    [## Home | John Adeojo
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '[medium.com](https://medium.com/@johnadeojo/membership?source=post_page-----fa1764409905--------------------------------)
    [](https://www.john-adeojo.com/?source=post_page-----fa1764409905--------------------------------)
    [## 主页 | John Adeojo'
- en: About Me An experienced data scientist and machine learning (ML) expert specialising
    in building bespoke ML powered…
  id: totrans-120
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 关于我 一名经验丰富的数据科学家和机器学习（ML）专家，专注于构建定制的ML解决方案…
- en: www.john-adeojo.com](https://www.john-adeojo.com/?source=post_page-----fa1764409905--------------------------------)
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '[www.john-adeojo.com](https://www.john-adeojo.com/?source=post_page-----fa1764409905--------------------------------)'
- en: Citations
  id: totrans-122
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 引用
- en: '[1]Grootendorst, M. (2022). *BERTopic: Neural topic modeling with a class-based
    TF-IDF procedure*. Paperswithcode.com. [https://paperswithcode.com/paper/bertopic-neural-topic-modeling-with-a-class](https://paperswithcode.com/paper/bertopic-neural-topic-modeling-with-a-class)'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '[1]Grootendorst, M. (2022). *BERTopic: 使用基于类的TF-IDF程序进行神经主题建模*. Paperswithcode.com.
    [https://paperswithcode.com/paper/bertopic-neural-topic-modeling-with-a-class](https://paperswithcode.com/paper/bertopic-neural-topic-modeling-with-a-class)'
- en: '[2]Barbieri, F., Anke, L. E., & Camacho-Collados, J. (2022). *XLM-T: Multilingual
    Language Models in Twitter for Sentiment Analysis and Beyond*. Paperswithcode.com.
    [https://arxiv.org/abs/2104.12250](https://arxiv.org/abs/2104.12250)'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '[2]Barbieri, F., Anke, L. E., & Camacho-Collados, J. (2022). *XLM-T: 多语言语言模型在Twitter上的情感分析及更多*.
    Paperswithcode.com. [https://arxiv.org/abs/2104.12250](https://arxiv.org/abs/2104.12250)'
- en: '[3]Lin, T.-Y., Goyal, P., Girshick, R., He, K. and Dollar, P. (2018). Focal
    Loss for Dense Object Detection. *Facebook AI Research (FAIR)*. [online] Available
    at: [https://arxiv.org/pdf/1708.02002.pdf](https://arxiv.org/pdf/1708.02002.pdf)
    [Accessed 21 Mar. 2023].'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '[3]Lin, T.-Y., Goyal, P., Girshick, R., He, K. 和 Dollar, P. (2018). Focal Loss
    for Dense Object Detection. *Facebook AI Research (FAIR)*. [在线] 可用网址: [https://arxiv.org/pdf/1708.02002.pdf](https://arxiv.org/pdf/1708.02002.pdf)
    [访问于 2023 年 3 月 21 日]。'
