- en: 'Stable Diffusion: Mastering the Art of Interior Design'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç¨³å®šæ‰©æ•£ï¼šæŒæ¡å®¤å†…è®¾è®¡çš„è‰ºæœ¯
- en: åŸæ–‡ï¼š[https://towardsdatascience.com/stable-diffusion-mastering-the-art-of-interior-design-9fb4214544b0](https://towardsdatascience.com/stable-diffusion-mastering-the-art-of-interior-design-9fb4214544b0)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡ï¼š[https://towardsdatascience.com/stable-diffusion-mastering-the-art-of-interior-design-9fb4214544b0](https://towardsdatascience.com/stable-diffusion-mastering-the-art-of-interior-design-9fb4214544b0)
- en: A deep dive into Stable Diffusion and its inpainting variant for interior design
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ·±å…¥æ¢ç´¢ç¨³å®šæ‰©æ•£åŠå…¶å®¤å†…è®¾è®¡çš„ä¿®å¤å˜ä½“
- en: '[](https://medium.com/@rjguedes?source=post_page-----9fb4214544b0--------------------------------)[![Rafael
    Guedes](../Images/b3d000b3bce0113d2b2727e84db04870.png)](https://medium.com/@rjguedes?source=post_page-----9fb4214544b0--------------------------------)[](https://towardsdatascience.com/?source=post_page-----9fb4214544b0--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----9fb4214544b0--------------------------------)
    [Rafael Guedes](https://medium.com/@rjguedes?source=post_page-----9fb4214544b0--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@rjguedes?source=post_page-----9fb4214544b0--------------------------------)[![Rafael
    Guedes](../Images/b3d000b3bce0113d2b2727e84db04870.png)](https://medium.com/@rjguedes?source=post_page-----9fb4214544b0--------------------------------)[](https://towardsdatascience.com/?source=post_page-----9fb4214544b0--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----9fb4214544b0--------------------------------)
    [Rafael Guedes](https://medium.com/@rjguedes?source=post_page-----9fb4214544b0--------------------------------)'
- en: Â·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----9fb4214544b0--------------------------------)
    Â·9 min readÂ·Dec 18, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: Â·å‘å¸ƒäº[Towards Data Science](https://towardsdatascience.com/?source=post_page-----9fb4214544b0--------------------------------)
    Â·é˜…è¯»æ—¶é—´9åˆ†é’ŸÂ·2023å¹´12æœˆ18æ—¥
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: In this fast-paced world that we live in and after the pandemic, many of us
    realised that having a pleasant environment like home to escape from reality is
    priceless and a goal to be pursued.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æˆ‘ä»¬ç”Ÿæ´»çš„è¿™ä¸ªå¿«èŠ‚å¥çš„ä¸–ç•Œä¸­ä»¥åŠç»å†äº†ç–«æƒ…åï¼Œæˆ‘ä»¬ä¸­çš„è®¸å¤šäººæ„è¯†åˆ°ï¼Œæ‹¥æœ‰ä¸€ä¸ªèˆ’é€‚çš„å®¶ä½œä¸ºé€ƒé¿ç°å®çš„é¿é£æ¸¯æ˜¯æ— ä»·çš„ï¼Œå€¼å¾—è¿½æ±‚çš„ç›®æ ‡ã€‚
- en: Whether you are looking for a Scandinavian, minimalist, or a glamorous style
    to decorate your home, it is not easy to imagine how every single object will
    fit in a space full of different pieces and colours. For that reason, we usually
    seek for professional help to create those amazing 3D images that help us understand
    how our future home will look like.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: æ— è®ºä½ æ˜¯å¯»æ‰¾æ–¯å ªçš„çº³ç»´äºšé£æ ¼ã€æç®€é£æ ¼ï¼Œè¿˜æ˜¯åä¸½é£æ ¼æ¥è£…é¥°ä½ çš„å®¶ï¼Œå¾ˆéš¾æƒ³è±¡æ¯ä»¶ç‰©å“å¦‚ä½•åœ¨å……æ»¡ä¸åŒä»¶æ•°å’Œé¢œè‰²çš„ç©ºé—´ä¸­ç›¸äº’é…åˆã€‚å› æ­¤ï¼Œæˆ‘ä»¬é€šå¸¸å¯»æ±‚ä¸“ä¸šå¸®åŠ©ï¼Œä»¥åˆ›å»ºé‚£äº›ä»¤äººæƒŠå¹çš„3Då›¾åƒï¼Œå¸®åŠ©æˆ‘ä»¬ç†è§£æœªæ¥çš„å®¶ä¼šæ˜¯ä»€ä¹ˆæ ·å­ã€‚
- en: However, these 3D images are expensive, and if our initial idea does not look
    as good as we thought, getting new images will take time and more money, things
    that are scarce nowadays.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶è€Œï¼Œè¿™äº›3Då›¾åƒè´¹ç”¨æ˜‚è´µï¼Œå¦‚æœæˆ‘ä»¬çš„åˆæ­¥æ„æ€ä¸å¦‚é¢„æœŸå¥½ï¼Œè·å–æ–°å›¾åƒå°†éœ€è¦æ—¶é—´å’Œæ›´å¤šçš„é‡‘é’±ï¼Œè¿™äº›åœ¨å½“ä»Šç¤¾ä¼šéƒ½å¾ˆç¨€ç¼ºã€‚
- en: In this article, I explore the Stable Diffusion model starting with a brief
    explanation of what it is, how it is trained and what is needed to adapt it for
    inpainting. Finally, I finish the article with its application on a 3D image from
    my future home where I change the kitchen island and cabinets to a different colour
    and material.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘æ¢è®¨äº†ç¨³å®šæ‰©æ•£æ¨¡å‹ï¼Œé¦–å…ˆç®€è¦è§£é‡Šäº†å®ƒæ˜¯ä»€ä¹ˆã€å¦‚ä½•è®­ç»ƒä»¥åŠéœ€è¦ä»€ä¹ˆæ¥é€‚åº”ä¿®å¤ã€‚æœ€åï¼Œæˆ‘ä»¥å¯¹æˆ‘æœªæ¥å®¶çš„3Då›¾åƒçš„åº”ç”¨ç»“æŸæ–‡ç« ï¼Œæˆ‘å°†å¨æˆ¿å²›å’Œæ©±æŸœçš„é¢œè‰²å’Œææ–™æ›´æ¢ä¸ºä¸åŒçš„ã€‚
- en: '![](../Images/2a2fbf68e14d0e4d69be676ac27fb9c3.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2a2fbf68e14d0e4d69be676ac27fb9c3.png)'
- en: 'Figure 1: Interior Design ([source](https://unsplash.com/photos/brown-wooden-framed-yellow-padded-chair-_HqHX3LBN18))'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾1ï¼šå®¤å†…è®¾è®¡ ([source](https://unsplash.com/photos/brown-wooden-framed-yellow-padded-chair-_HqHX3LBN18))
- en: As always, the code is available on [Github](https://github.com/rjguedes8/stable_diffusion).
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€å¦‚æ—¢å¾€ï¼Œä»£ç å¯åœ¨[Github](https://github.com/rjguedes8/stable_diffusion)ä¸Šæ‰¾åˆ°ã€‚
- en: Stable Diffusion
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç¨³å®šæ‰©æ•£
- en: What is it?
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è¿™æ˜¯ä»€ä¹ˆï¼Ÿ
- en: Stable Diffusion [1] is a generative AI model released in 2022 by CompVis Group
    that produces photorealistic images from text and image prompts. It was primarily
    designed to generate images influenced by text descriptions but it can be used
    for other tasks such as inpainting or video creation.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: ç¨³å®šæ‰©æ•£[1]æ˜¯CompVis Groupäº2022å¹´å‘å¸ƒçš„ç”Ÿæˆæ€§AIæ¨¡å‹ï¼Œå¯ä»¥æ ¹æ®æ–‡æœ¬å’Œå›¾åƒæç¤ºç”Ÿæˆé€¼çœŸçš„å›¾åƒã€‚å®ƒä¸»è¦è®¾è®¡ç”¨äºç”Ÿæˆå—æ–‡æœ¬æè¿°å½±å“çš„å›¾åƒï¼Œä½†ä¹Ÿå¯ä»¥ç”¨äºå…¶ä»–ä»»åŠ¡ï¼Œå¦‚ä¿®å¤æˆ–è§†é¢‘åˆ›å»ºã€‚
- en: Its success comes from the **Perceptual Image Compression** step that converts
    a high dimensional image into a smaller latent space. This compression enables
    the use of the model in low-resourced machines making it accessible to everyone,
    something that was not possible with the previous state-of-the-art models.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: å®ƒçš„æˆåŠŸæºäº **æ„ŸçŸ¥å›¾åƒå‹ç¼©** æ­¥éª¤ï¼Œè¯¥æ­¥éª¤å°†é«˜ç»´å›¾åƒè½¬æ¢ä¸ºæ›´å°çš„æ½œåœ¨ç©ºé—´ã€‚è¿™ç§å‹ç¼©ä½¿å¾—åœ¨èµ„æºæœ‰é™çš„æœºå™¨ä¸Šä½¿ç”¨è¯¥æ¨¡å‹æˆä¸ºå¯èƒ½ï¼Œä½¿æ¯ä¸ªäººéƒ½èƒ½ä½¿ç”¨ï¼Œè¿™åœ¨ä¹‹å‰çš„æœ€å…ˆè¿›æ¨¡å‹ä¸­æ˜¯ä¸å¯èƒ½çš„ã€‚
- en: '![](../Images/79fe033603707036fea343f9ec147dc1.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/79fe033603707036fea343f9ec147dc1.png)'
- en: 'Figure 2: Stable Diffusion architecture ([source](https://arxiv.org/pdf/2112.10752.pdf))'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 2ï¼šStable Diffusion æ¶æ„ ([source](https://arxiv.org/pdf/2112.10752.pdf))
- en: How does itÂ learn?
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å®ƒæ˜¯å¦‚ä½•å­¦ä¹ çš„ï¼Ÿ
- en: Stable Diffusion is a **Latent Diffusion Model** (LDM) with three main components
    (**variational autoencoder** (VAE) [2], **U-Net** [3] and an optional **text encoder**)
    that learns how to denoise images conditioned by a prompt (text or other image)
    in order to create a new image.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: Stable Diffusion æ˜¯ä¸€ä¸ª **æ½œåœ¨æ‰©æ•£æ¨¡å‹** (LDM)ï¼Œå…·æœ‰ä¸‰ä¸ªä¸»è¦ç»„ä»¶ (**å˜åˆ†è‡ªç¼–ç å™¨** (VAE) [2]ã€**U-Net**
    [3] å’Œä¸€ä¸ªå¯é€‰çš„ **æ–‡æœ¬ç¼–ç å™¨**)ï¼Œå®ƒå­¦ä¹ å¦‚ä½•åœ¨æç¤ºï¼ˆæ–‡æœ¬æˆ–å…¶ä»–å›¾åƒï¼‰çš„æ¡ä»¶ä¸‹å»å™ªå›¾åƒï¼Œä»¥åˆ›å»ºæ–°å›¾åƒã€‚
- en: 'The training process of Stable Diffusion has 5 main steps:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: Stable Diffusion çš„è®­ç»ƒè¿‡ç¨‹æœ‰ 5 ä¸ªä¸»è¦æ­¥éª¤ï¼š
- en: 1.The **Perceptual Image Compression** step consists in an **Encoder** that
    receives an image with a dimension of *512x512x3* and encodes it into a smaller
    latent space *Z* with a dimension of *64x64x4\.* To better preserve the details
    of an image (for example, the eyes in human face),the latent space *Z* is regularized
    using a low-weighted Kullback-Leibler-term to make it zero centered and to obtain
    a small variance.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 1. **æ„ŸçŸ¥å›¾åƒå‹ç¼©** æ­¥éª¤åŒ…æ‹¬ä¸€ä¸ª **ç¼–ç å™¨**ï¼Œå®ƒæ¥æ”¶ä¸€ä¸ªå°ºå¯¸ä¸º *512x512x3* çš„å›¾åƒï¼Œå¹¶å°†å…¶ç¼–ç ä¸ºä¸€ä¸ªå°ºå¯¸ä¸º *64x64x4*
    çš„æ›´å°çš„æ½œåœ¨ç©ºé—´ *Z*ã€‚ä¸ºäº†æ›´å¥½åœ°ä¿ç•™å›¾åƒçš„ç»†èŠ‚ï¼ˆä¾‹å¦‚ï¼Œäººè„¸ä¸­çš„çœ¼ç›ï¼‰ï¼Œæ½œåœ¨ç©ºé—´ *Z* ä½¿ç”¨ä½æƒé‡çš„ Kullback-Leibler é¡¹è¿›è¡Œæ­£åˆ™åŒ–ï¼Œä»¥ä½¿å…¶é›¶ä¸­å¿ƒå¹¶è·å¾—å°æ–¹å·®ã€‚
- en: '![](../Images/f30de379460e03b36399513a035cca84.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f30de379460e03b36399513a035cca84.png)'
- en: 'Figure 3: Perceptual Image Compression process where the Encoder converts a
    512x512x3 image to a latent space of 64x64x4 (image made by the author).'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 3ï¼šæ„ŸçŸ¥å›¾åƒå‹ç¼©è¿‡ç¨‹ï¼Œå…¶ä¸­ç¼–ç å™¨å°† 512x512x3 çš„å›¾åƒè½¬æ¢ä¸º 64x64x4 çš„æ½œåœ¨ç©ºé—´ï¼ˆå›¾åƒç”±ä½œè€…åˆ¶ä½œï¼‰ã€‚
- en: 2\. The **Diffusion Process** is responsible to progressively add Gaussian noise
    to the latent space *Z,* until all that remains is random noise, generating a
    new latent space ***Zt.*** ***t*** is the number of times the diffusion process
    occurred to achieve a full noisy latent space.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 2\. **æ‰©æ•£è¿‡ç¨‹** è´Ÿè´£é€æ­¥å‘æ½œåœ¨ç©ºé—´ *Z* æ·»åŠ é«˜æ–¯å™ªå£°ï¼Œç›´åˆ°æ‰€æœ‰å‰©ä¸‹çš„æ˜¯éšæœºå™ªå£°ï¼Œç”Ÿæˆä¸€ä¸ªæ–°çš„æ½œåœ¨ç©ºé—´ ***Zt.*** ***t***
    æ˜¯æ‰©æ•£è¿‡ç¨‹å‘ç”Ÿçš„æ¬¡æ•°ï¼Œä»¥å®ç°å®Œå…¨çš„å™ªå£°æ½œåœ¨ç©ºé—´ã€‚
- en: This step is important because Stable Diffusion has to learn how to go from
    noise to the original image as we will see in the next steps.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸€æ­¥éª¤å¾ˆé‡è¦ï¼Œå› ä¸º Stable Diffusion å¿…é¡»å­¦ä¹ å¦‚ä½•ä»å™ªå£°æ¢å¤åˆ°åŸå§‹å›¾åƒï¼Œæ­£å¦‚æˆ‘ä»¬å°†åœ¨ä¸‹ä¸€æ­¥ä¸­çœ‹åˆ°çš„é‚£æ ·ã€‚
- en: '![](../Images/08685adba6bacd6db70f947657a72950.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/08685adba6bacd6db70f947657a72950.png)'
- en: 'Figure 4: Diffusion Process where Gaussian noise is added gradually to the
    latent space (image made by the author)'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 4ï¼šæ‰©æ•£è¿‡ç¨‹ï¼Œå…¶ä¸­é«˜æ–¯å™ªå£°é€æ¸æ·»åŠ åˆ°æ½œåœ¨ç©ºé—´ä¸­ï¼ˆå›¾åƒç”±ä½œè€…åˆ¶ä½œï¼‰
- en: 3\. The **Denoising** **Process** trains a U-Net architecture to estimate the
    amount of noise in the latent space ***Zt*** in order to subtract it and restore
    *Z*. This process is able to recover the original latent space *Z* by gradually
    denoise *Zt,* basically, the inverse of the Diffusion Process.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 3\. **å»å™ª** **è¿‡ç¨‹** è®­ç»ƒäº†ä¸€ä¸ª U-Net æ¶æ„æ¥ä¼°è®¡æ½œåœ¨ç©ºé—´ ***Zt*** ä¸­çš„å™ªå£°é‡ï¼Œä»¥ä¾¿å‡å»å®ƒå¹¶æ¢å¤ *Z*ã€‚è¿™ä¸ªè¿‡ç¨‹é€šè¿‡é€æ¸å»å™ª
    *Zt* æ¥æ¢å¤åŸå§‹çš„æ½œåœ¨ç©ºé—´ *Z*ï¼ŒåŸºæœ¬ä¸Šæ˜¯æ‰©æ•£è¿‡ç¨‹çš„é€†è¿‡ç¨‹ã€‚
- en: '![](../Images/5778395079858775325368a96391d2aa.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5778395079858775325368a96391d2aa.png)'
- en: 'Figure 5: Denoising Process where U-Net predicts the noise in a latent space
    and removes it until it completely restores the original latent space (image made
    by the author)'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 5ï¼šå»å™ªè¿‡ç¨‹ï¼Œå…¶ä¸­ U-Net é¢„æµ‹æ½œåœ¨ç©ºé—´ä¸­çš„å™ªå£°å¹¶å°†å…¶å»é™¤ï¼Œç›´åˆ°å®Œå…¨æ¢å¤åŸå§‹æ½œåœ¨ç©ºé—´ï¼ˆå›¾åƒç”±ä½œè€…åˆ¶ä½œï¼‰
- en: 4\. During the **Denoising** **Process** a prompt, usually text and/or other
    image, can be concatenated to the latent space *Zt.* This concatenation will condition
    the Denoising Process which allows the creation of new images. The authors added
    cross-attention mechanisms in the backbone of U-Net to handle these prompts since
    they are effective for learning attention-based models of various inputs types.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 4\. åœ¨ **å»å™ª** **è¿‡ç¨‹** ä¸­ï¼Œå¯ä»¥å°†æç¤ºï¼Œé€šå¸¸æ˜¯æ–‡æœ¬å’Œ/æˆ–å…¶ä»–å›¾åƒï¼Œè¿æ¥åˆ°æ½œåœ¨ç©ºé—´ *Zt*ã€‚è¿™ç§è¿æ¥å°†æ¡ä»¶åŒ–å»å™ªè¿‡ç¨‹ï¼Œä»è€Œå…è®¸åˆ›å»ºæ–°å›¾åƒã€‚ä½œè€…åœ¨
    U-Net çš„éª¨å¹²ç½‘ç»œä¸­æ·»åŠ äº†äº¤å‰æ³¨æ„æœºåˆ¶æ¥å¤„ç†è¿™äº›æç¤ºï¼Œå› ä¸ºå®ƒä»¬å¯¹å­¦ä¹ å„ç§è¾“å…¥ç±»å‹çš„åŸºäºæ³¨æ„çš„æ¨¡å‹æ˜¯æœ‰æ•ˆçš„ã€‚
- en: When it comes to text, the model uses a trained text encoder **CLIP** [4] that
    encodes the prompt into a 768-dimensional vector which is then concatenated to
    *Zt* and received by U-Net as input.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: å½“æ¶‰åŠåˆ°æ–‡æœ¬æ—¶ï¼Œæ¨¡å‹ä½¿ç”¨è®­ç»ƒå¥½çš„æ–‡æœ¬ç¼–ç å™¨**CLIP** [4]ï¼Œè¯¥ç¼–ç å™¨å°†æç¤ºç¼–ç ä¸ºä¸€ä¸ª768ç»´çš„å‘é‡ï¼Œç„¶åå°†å…¶ä¸*Zt*è¿æ¥ï¼Œå¹¶ä½œä¸ºè¾“å…¥ä¼ é€’ç»™U-Netã€‚
- en: As we can see in Figure 6, we concatenated to *Zt* the text prompt *â€œremove
    the lampâ€,* which conditioned the Diffusion Process restoring a *Zt* without the
    lamp near the chair that the original *Zt* had.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: æ­£å¦‚æˆ‘ä»¬åœ¨å›¾6ä¸­æ‰€è§ï¼Œæˆ‘ä»¬å°†æ–‡æœ¬æç¤º*â€œç§»é™¤ç¯â€*ä¸*Zt*è¿›è¡Œè¿æ¥ï¼Œè¿™ä½¿å¾—Diffusion Processå¯¹*Zt*è¿›è¡Œè°ƒæ•´ï¼Œå»é™¤äº†åŸå§‹*Zt*ä¸­é è¿‘æ¤…å­çš„ç¯ã€‚
- en: '![](../Images/4b95fd7017e1a2147e6be92de15bf4f1.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4b95fd7017e1a2147e6be92de15bf4f1.png)'
- en: 'Figure 6: Condition the denoising process with a text prompt to remove the
    lamp in the original image (image made by the author)'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾6ï¼šä½¿ç”¨æ–‡æœ¬æç¤ºæ¥å»é™¤åŸå§‹å›¾åƒä¸­çš„ç¯å…‰çš„å»å™ªè¿‡ç¨‹ï¼ˆå›¾ç‰‡ç”±ä½œè€…åˆ¶ä½œï¼‰
- en: 5\. Finally, the **Decoder** receives the denoised latent space *Z* as input
    and it learns how to estimate the component-wise variance used to encode the image
    into a smaller latent space. After estimating the variance, the Decoder can generate
    a new image with the same dimension of the original one.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 5\. æœ€ç»ˆï¼Œ**è§£ç å™¨**æ¥æ”¶å»å™ªåçš„æ½œåœ¨ç©ºé—´*Z*ä½œä¸ºè¾“å…¥ï¼Œå¹¶å­¦ä¹ å¦‚ä½•ä¼°è®¡ç”¨äºå°†å›¾åƒç¼–ç åˆ°æ›´å°æ½œåœ¨ç©ºé—´ä¸­çš„æ¯ä¸ªç»„ä»¶çš„æ–¹å·®ã€‚åœ¨ä¼°è®¡æ–¹å·®åï¼Œè§£ç å™¨å¯ä»¥ç”Ÿæˆä¸åŸå§‹å›¾åƒç›¸åŒå°ºå¯¸çš„æ–°å›¾åƒã€‚
- en: '![](../Images/fa1d17e8e78ea64e86b60bc59ec9046e.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fa1d17e8e78ea64e86b60bc59ec9046e.png)'
- en: 'Figure 7: Decoder restores the original image without the lamp and with the
    original size of 512x512x3 (image made by the author)'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾7ï¼šè§£ç å™¨æ¢å¤äº†æ²¡æœ‰ç¯çš„åŸå§‹å›¾åƒï¼Œå°ºå¯¸ä¸º512x512x3ï¼ˆå›¾ç‰‡ç”±ä½œè€…åˆ¶ä½œï¼‰
- en: Inpainting Variant of Stable Diffusion
  id: totrans-40
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Stable Diffusionçš„ä¿®è¡¥å˜ä½“
- en: Inpainting is the task of filling masked regions of an image with new content
    either because we want to uncorrupt the image or because we want to replace some
    undesired content.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: ä¿®è¡¥ä»»åŠ¡æ˜¯ç”¨æ–°å†…å®¹å¡«è¡¥å›¾åƒä¸­è¢«é®è”½çš„åŒºåŸŸï¼Œæ— è®ºæ˜¯å› ä¸ºæˆ‘ä»¬æƒ³è¦ä¿®å¤å›¾åƒè¿˜æ˜¯å› ä¸ºæˆ‘ä»¬æƒ³è¦æ›¿æ¢ä¸€äº›ä¸å¸Œæœ›å‡ºç°çš„å†…å®¹ã€‚
- en: Stable Diffusion can be trained to generate new images based on an image, a
    text prompt and a mask. This type of model is already available in HuggingFace
    ğŸ¤— and its called *runwayml/stable-diffusion-inpainting*.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: Stable Diffusionå¯ä»¥è¢«è®­ç»ƒç”ŸæˆåŸºäºå›¾åƒã€æ–‡æœ¬æç¤ºå’Œæ©è†œçš„æ–°å›¾åƒã€‚è¿™ç§æ¨¡å‹å·²ç»åœ¨HuggingFace ğŸ¤— ä¸Šå¯ç”¨ï¼Œåç§°ä¸º*runwayml/stable-diffusion-inpainting*ã€‚
- en: To train Stable Diffusion to perform inpainting, we need to go through the same
    steps mentioned in the section above but with a slightly change on the input data.
    In this case apart from having the original image and text, we also have a mask
    (another image). For that, the U-Net needs to be adapted to receive an additional
    input channel for the mask.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†è®­ç»ƒStable Diffusionè¿›è¡Œä¿®è¡¥ï¼Œæˆ‘ä»¬éœ€è¦ç»å†ä¸Šè¿°éƒ¨åˆ†æåˆ°çš„ç›¸åŒæ­¥éª¤ï¼Œä½†è¾“å…¥æ•°æ®ç¨æœ‰ä¸åŒã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œé™¤äº†åŸå§‹å›¾åƒå’Œæ–‡æœ¬ï¼Œæˆ‘ä»¬è¿˜éœ€è¦ä¸€ä¸ªæ©è†œï¼ˆå¦ä¸€å¼ å›¾åƒï¼‰ã€‚ä¸ºæ­¤ï¼ŒU-Netéœ€è¦é€‚åº”ä»¥æ¥æ”¶é¢å¤–çš„æ©è†œè¾“å…¥é€šé“ã€‚
- en: During training, the area that is not under the mask remains untouched and it
    is only encoded to the latent space, while the masked area goes through the all
    process of encoding, diffusion and denoising. This way, the Stable Diffusion model
    knows which area should remain the same and which area should change (Figure 8
    illustrates this process).
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæ©è†œä¸‹çš„åŒºåŸŸä¿æŒä¸å˜ï¼Œåªå¯¹å…¶è¿›è¡Œç¼–ç åˆ°æ½œåœ¨ç©ºé—´ï¼Œè€Œæ©è†œåŒºåŸŸåˆ™ç»å†æ•´ä¸ªç¼–ç ã€æ‰©æ•£å’Œå»å™ªè¿‡ç¨‹ã€‚è¿™æ ·ï¼ŒStable Diffusionæ¨¡å‹çŸ¥é“å“ªäº›åŒºåŸŸåº”è¯¥ä¿æŒä¸å˜ï¼Œå“ªäº›åŒºåŸŸåº”è¯¥å‘ç”Ÿå˜åŒ–ï¼ˆå›¾8å±•ç¤ºäº†è¿™ä¸€è¿‡ç¨‹ï¼‰ã€‚
- en: '![](../Images/a22262eba205e0bec3179cef1acab740.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a22262eba205e0bec3179cef1acab740.png)'
- en: 'Figure 8: Training process of Inpainting Diffusion Stable model (image made
    by the author)'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾8ï¼šä¿®è¡¥æ‰©æ•£Stableæ¨¡å‹çš„è®­ç»ƒè¿‡ç¨‹ï¼ˆå›¾ç‰‡ç”±ä½œè€…åˆ¶ä½œï¼‰
- en: In Figure 9, we have an example of what is needed to perform inpaitining on
    our own uses cases. We give the original image together with a mask of what we
    want to change and a text prompt with the change we want to see and Stable Diffusion
    generates a new image. In the next section, we will see how to do it in practice.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å›¾9ä¸­ï¼Œæˆ‘ä»¬å±•ç¤ºäº†åœ¨è‡ªå·±çš„ä½¿ç”¨æ¡ˆä¾‹ä¸­è¿›è¡Œä¿®è¡¥æ‰€éœ€çš„ç¤ºä¾‹ã€‚æˆ‘ä»¬æä¾›åŸå§‹å›¾åƒä»¥åŠæˆ‘ä»¬å¸Œæœ›æ›´æ”¹çš„æ©è†œå’ŒåŒ…å«æˆ‘ä»¬æƒ³çœ‹åˆ°æ›´æ”¹çš„æ–‡æœ¬æç¤ºï¼ŒStable Diffusionç”Ÿæˆæ–°å›¾åƒã€‚åœ¨ä¸‹ä¸€èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†çœ‹åˆ°å¦‚ä½•å®é™…æ“ä½œã€‚
- en: '![](../Images/134c22d473a948035e00308ece16a734.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/134c22d473a948035e00308ece16a734.png)'
- en: 'Figure 9: Process to train Stable Diffusion for image inpainting (image made
    by the author)'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾9ï¼šè®­ç»ƒStable Diffusionè¿›è¡Œå›¾åƒä¿®è¡¥çš„è¿‡ç¨‹ï¼ˆå›¾ç‰‡ç”±ä½œè€…åˆ¶ä½œï¼‰
- en: Interior Design using Stable Diffusion
  id: totrans-50
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ä½¿ç”¨Stable Diffusionçš„å®¤å†…è®¾è®¡
- en: In this section I will cover how to use Stable Diffusion in an inpainting scenario
    for interior design.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘å°†è®²è§£å¦‚ä½•åœ¨å®¤å†…è®¾è®¡ä¸­ä½¿ç”¨Stable Diffusionè¿›è¡Œä¿®è¡¥åœºæ™¯çš„æ“ä½œã€‚
- en: When it comes to buy a new house or apartment that is still under construction,
    usually we have access to 3D images of how it will look. Based on those images
    we can request to change colours or materials to make it tailored to our taste.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: å½“æˆ‘ä»¬è´­ä¹°ä¸€æ ‹æ–°æˆ¿æˆ–ä»åœ¨å»ºè®¾ä¸­çš„å…¬å¯“æ—¶ï¼Œé€šå¸¸å¯ä»¥è·å–å®ƒçš„ 3D å›¾åƒã€‚åŸºäºè¿™äº›å›¾åƒï¼Œæˆ‘ä»¬å¯ä»¥è¦æ±‚æ›´æ”¹é¢œè‰²æˆ–ææ–™ï¼Œä»¥ä½¿å…¶ç¬¦åˆæˆ‘ä»¬çš„å£å‘³ã€‚
- en: However, it is not easy to imagine if the changes that we are requesting will
    fit the rest of the house or not, and asking for a new 3D might be expensive and
    time consuming. Therefore, we can use stable diffusion to quickly iterate and
    get a sense of how things will look if we apply the changes we want.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶è€Œï¼Œå¾ˆéš¾æƒ³è±¡æˆ‘ä»¬è¯·æ±‚çš„æ›´æ”¹æ˜¯å¦é€‚åˆæˆ¿å­çš„å…¶ä»–éƒ¨åˆ†ï¼Œè€Œè¯·æ±‚æ–°çš„ 3D å¯èƒ½ä¼šå¾ˆæ˜‚è´µä¸”è€—æ—¶ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ Stable Diffusion å¿«é€Ÿè¿­ä»£ï¼Œå¹¶äº†è§£å¦‚æœåº”ç”¨æˆ‘ä»¬æƒ³è¦çš„æ›´æ”¹ï¼Œäº‹ç‰©ä¼šæ˜¯ä»€ä¹ˆæ ·å­ã€‚
- en: For that we can use Python and HuggingFace ğŸ¤— to build our own Stable Diffusion
    Interior Designer!
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ Python å’Œ HuggingFace ğŸ¤— æ¥æ„å»ºæˆ‘ä»¬è‡ªå·±çš„ Stable Diffusion å®¤å†…è®¾è®¡å¸ˆï¼
- en: 'We start by importing the libraries:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬é¦–å…ˆå¯¼å…¥åº“ï¼š
- en: '[PRE0]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Then we load the Stable Diffusion Inpainting model available in HuggingFace
    ğŸ¤—:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åï¼Œæˆ‘ä»¬åŠ è½½ HuggingFace ğŸ¤— æä¾›çš„ Stable Diffusion Inpainting æ¨¡å‹ï¼š
- en: '[PRE1]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'With the model loaded, we load the original image and the mask of what we want
    to change:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨åŠ è½½äº†æ¨¡å‹åï¼Œæˆ‘ä»¬åŠ è½½åŸå§‹å›¾åƒå’Œæˆ‘ä»¬æƒ³è¦æ›´æ”¹çš„é®ç½©ï¼š
- en: The white part of the mask is what will change while the black part is what
    will remain untouched.
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: é®ç½©çš„ç™½è‰²éƒ¨åˆ†æ˜¯å°†è¦æ›´æ”¹çš„éƒ¨åˆ†ï¼Œè€Œé»‘è‰²éƒ¨åˆ†æ˜¯å°†ä¿æŒä¸å˜çš„éƒ¨åˆ†ã€‚
- en: The mask was manually created, but we can also use a segmentation model to create
    it.
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: é®ç½©æ˜¯æ‰‹åŠ¨åˆ›å»ºçš„ï¼Œä½†æˆ‘ä»¬ä¹Ÿå¯ä»¥ä½¿ç”¨åˆ†å‰²æ¨¡å‹æ¥åˆ›å»ºå®ƒã€‚
- en: '[PRE2]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '![](../Images/b3f7c5c57a1d1d2bb992b2409a31eb42.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b3f7c5c57a1d1d2bb992b2409a31eb42.png)'
- en: 'Figure 10: 3D image of a kitchen (purchased by the author) and a mask to change
    the island and the black wall (image made by the author)'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 10ï¼šå¨æˆ¿çš„ 3D å›¾åƒï¼ˆç”±ä½œè€…è´­ä¹°ï¼‰å’Œä¸€ä¸ªé®ç½©ï¼Œç”¨äºæ›´æ”¹å²›å°å’Œé»‘è‰²å¢™å£ï¼ˆå›¾åƒç”±ä½œè€…åˆ¶ä½œï¼‰
- en: With both image and mask loaded, it is time to create the prompt to condition
    the image generation to what we want and generate new images.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨åŠ è½½äº†å›¾åƒå’Œé®ç½©åï¼Œç°åœ¨æ˜¯åˆ›å»ºæç¤ºä»¥å°†å›¾åƒç”Ÿæˆæ¡ä»¶è®¾ç½®ä¸ºæˆ‘ä»¬æƒ³è¦çš„å†…å®¹å¹¶ç”Ÿæˆæ–°å›¾åƒçš„æ—¶å€™äº†ã€‚
- en: In this case, I want to replace the black island and the black wall to a marble
    island and a marble wall.
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘æƒ³å°†é»‘è‰²å²›å°å’Œé»‘è‰²å¢™å£æ›¿æ¢ä¸ºå¤§ç†çŸ³å²›å°å’Œå¤§ç†çŸ³å¢™å£ã€‚
- en: '[PRE3]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '![](../Images/09742ef89e90a9391e0f55615bcffa7d.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/09742ef89e90a9391e0f55615bcffa7d.png)'
- en: 'Figure 11: Comparison between original and my favourite generated image (they
    have different sizes because the model was trained with 512x512 images)'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 11ï¼šåŸå§‹å›¾åƒä¸æˆ‘æœ€å–œæ¬¢çš„ç”Ÿæˆå›¾åƒçš„æ¯”è¾ƒï¼ˆå®ƒä»¬çš„å°ºå¯¸ä¸åŒï¼Œå› ä¸ºæ¨¡å‹æ˜¯ç”¨ 512x512 å›¾åƒè®­ç»ƒçš„ï¼‰
- en: 'The result looks good, but I also want to replace the wooden kitchen cabinets
    with white ones, so letâ€™s redo the process:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: ç»“æœçœ‹èµ·æ¥ä¸é”™ï¼Œä½†æˆ‘è¿˜æƒ³å°†æœ¨åˆ¶å¨æˆ¿æ©±æŸœæ›¿æ¢ä¸ºç™½è‰²çš„ï¼Œæ‰€ä»¥è®©æˆ‘ä»¬é‡æ–°è¿›è¡Œè¿™ä¸ªè¿‡ç¨‹ï¼š
- en: 'Load the last image generated and a new mask:'
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: åŠ è½½æœ€åç”Ÿæˆçš„å›¾åƒå’Œä¸€ä¸ªæ–°çš„é®ç½©ï¼š
- en: '[PRE4]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '![](../Images/c99eebffd0a561b6c60a0e14d261b659.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c99eebffd0a561b6c60a0e14d261b659.png)'
- en: 'Figure 12: Previous generated image and new mask (image made by the author)'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 12ï¼šä¹‹å‰ç”Ÿæˆçš„å›¾åƒå’Œæ–°çš„é®ç½©ï¼ˆå›¾åƒç”±ä½œè€…åˆ¶ä½œï¼‰
- en: '2\. Create a new prompt and generate new images:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 2. åˆ›å»ºæ–°çš„æç¤ºå¹¶ç”Ÿæˆæ–°å›¾åƒï¼š
- en: '[PRE5]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '![](../Images/0b7e996ee6af90b86a437dc19e4ed3d0.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0b7e996ee6af90b86a437dc19e4ed3d0.png)'
- en: 'Figure 13: Generated image with white kitchen cabinets (image made by the author)'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 13ï¼šå¸¦æœ‰ç™½è‰²å¨æˆ¿æ©±æŸœçš„ç”Ÿæˆå›¾åƒï¼ˆå›¾åƒç”±ä½œè€…åˆ¶ä½œï¼‰
- en: The result looks really good and I would like you to notice the details that
    Stable Diffusion is able to reproduced such as the kitchen tap or the reflection
    of lights in the cabinets. Although the reflection on the left is not aligned
    is incredible how it managed to take the lights into consideration.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: ç»“æœçœ‹èµ·æ¥éå¸¸å¥½ï¼Œæˆ‘å¸Œæœ›ä½ æ³¨æ„åˆ° Stable Diffusion èƒ½å¤Ÿé‡ç°çš„ç»†èŠ‚ï¼Œä¾‹å¦‚å¨æˆ¿æ°´é¾™å¤´æˆ–æ©±æŸœä¸Šçš„å…‰çº¿åå°„ã€‚å°½ç®¡å·¦ä¾§çš„åå°„æ²¡æœ‰å¯¹é½ï¼Œä½†ä»¤äººæƒŠè®¶çš„æ˜¯å®ƒå¦‚ä½•è€ƒè™‘äº†å…‰çº¿ã€‚
- en: Conclusion
  id: totrans-80
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç»“è®º
- en: AI is not only useful for organisations with large amounts of data, it can be
    applied to anything we can think of!
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: AI ä¸ä»…å¯¹æ‹¥æœ‰å¤§é‡æ•°æ®çš„ç»„ç»‡æœ‰ç”¨ï¼Œå®ƒè¿˜å¯ä»¥åº”ç”¨äºæˆ‘ä»¬èƒ½æƒ³åˆ°çš„ä»»ä½•äº‹ç‰©ï¼
- en: In this article, we explored Stable Diffusion for a non-traditional use case
    but for a traditional job that exists for decades. With a few lines of code we
    managed to generate as many different images as we wanted for each prompt which
    gives us a lot of possibilities to choose from.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æ¢ç´¢äº† Stable Diffusion çš„ä¸€ç§éä¼ ç»Ÿç”¨é€”ï¼Œä½†è¿™æ˜¯ä¸€ä¸ªå­˜åœ¨äº†å‡ åå¹´çš„ä¼ ç»Ÿå·¥ä½œã€‚é€šè¿‡å‡ è¡Œä»£ç ï¼Œæˆ‘ä»¬èƒ½å¤Ÿä¸ºæ¯ä¸ªæç¤ºç”Ÿæˆå°½å¯èƒ½å¤šçš„ä¸åŒå›¾åƒï¼Œè¿™ç»™äº†æˆ‘ä»¬å¾ˆå¤šé€‰æ‹©çš„å¯èƒ½æ€§ã€‚
- en: However, like everything else, Stable Diffusion and, in particular the model
    we used, has its own limitations such as not achieving perfect photorealism as
    we saw in the light reflection or in Figure 14 where one of the chairs is inside
    of the kitchen island.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶è€Œï¼Œä¸å…¶ä»–äº‹ç‰©ä¸€æ ·ï¼Œç¨³å®šæ‰©æ•£æ¨¡å‹ï¼Œç‰¹åˆ«æ˜¯æˆ‘ä»¬ä½¿ç”¨çš„æ¨¡å‹ï¼Œä¹Ÿæœ‰å…¶è‡ªèº«çš„å±€é™æ€§ï¼Œä¾‹å¦‚åœ¨å…‰çº¿åå°„ä¸­æˆ–åœ¨å›¾ 14 ä¸­ï¼Œå…¶ä¸­ä¸€æŠŠæ¤…å­ä½äºå¨æˆ¿å²›å†…ï¼Œå¹¶æœªå®ç°å®Œç¾çš„çœŸå®æ„Ÿã€‚
- en: Nevertheless, the future of AI looks bright and some of this limitations can
    be overcome with a fine-tune in our own data and for our own use cases.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: å°½ç®¡å¦‚æ­¤ï¼Œäººå·¥æ™ºèƒ½çš„æœªæ¥ä»ç„¶å…‰æ˜ï¼Œå…¶ä¸­ä¸€äº›å±€é™æ€§å¯ä»¥é€šè¿‡åœ¨æˆ‘ä»¬è‡ªå·±çš„æ•°æ®å’Œä½¿ç”¨æ¡ˆä¾‹ä¸­è¿›è¡Œå¾®è°ƒæ¥å…‹æœã€‚
- en: '![](../Images/cc6bc6cb87942b845703843d69171247.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/cc6bc6cb87942b845703843d69171247.png)'
- en: 'Figure 14: Generated image with some quality problems (image made by the author)'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 14ï¼šç”Ÿæˆçš„å›¾åƒå­˜åœ¨ä¸€äº›è´¨é‡é—®é¢˜ï¼ˆå›¾åƒç”±ä½œè€…åˆ¶ä½œï¼‰
- en: '**Keep in touch:** [LinkedIn](https://www.linkedin.com/in/rafaelguedes97/),
    [Medium](https://medium.com/@rjguedes97)'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '**ä¿æŒè”ç³»ï¼š** [LinkedIn](https://www.linkedin.com/in/rafaelguedes97/)ï¼Œ [Medium](https://medium.com/@rjguedes97)'
- en: References
  id: totrans-88
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å‚è€ƒæ–‡çŒ®
- en: '[1] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, BjÃ¶rn
    Ommer. High-Resolution Image Synthesis with Latent Diffusion Models. arXiv:2001.08210,
    2022'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, BjÃ¶rn
    Ommer. é«˜åˆ†è¾¨ç‡å›¾åƒåˆæˆä¸æ½œåœ¨æ‰©æ•£æ¨¡å‹ã€‚arXiv:2001.08210, 2022'
- en: '[2] Diederik P. Kingma, Max Welling. An Introduction to Variational Autoencoders.
    arXiv:1906.02691, 2019'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] Diederik P. Kingma, Max Welling. å˜åˆ†è‡ªç¼–ç å™¨ç®€ä»‹ã€‚arXiv:1906.02691, 2019'
- en: '[3] Olaf Ronneberger, Philipp Fischer, Thomas Brox. U-Net: Convolutional Networks
    for Biomedical Image Segmentation. arXiv:1505.04597, 2015'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] Olaf Ronneberger, Philipp Fischer, Thomas Brox. U-Net: ç”¨äºç”Ÿç‰©åŒ»å­¦å›¾åƒåˆ†å‰²çš„å·ç§¯ç½‘ç»œã€‚arXiv:1505.04597,
    2015'
- en: '[4] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh,
    Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen
    Krueger, Ilya Sutskever. Learning Transferable Visual Models From Natural Language
    Supervision. arXiv:1911.02116, 2021'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh,
    Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen
    Krueger, Ilya Sutskever. ä»è‡ªç„¶è¯­è¨€ç›‘ç£ä¸­å­¦ä¹ å¯è½¬ç§»çš„è§†è§‰æ¨¡å‹ã€‚arXiv:1911.02116, 2021'
