- en: 'DL Notes: Advanced Gradient Descent'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: DL ç¬”è®°ï¼šé«˜çº§æ¢¯åº¦ä¸‹é™
- en: åŸæ–‡ï¼š[https://towardsdatascience.com/dl-notes-advanced-gradient-descent-4407d84c2515?source=collection_archive---------7-----------------------#2023-12-05](https://towardsdatascience.com/dl-notes-advanced-gradient-descent-4407d84c2515?source=collection_archive---------7-----------------------#2023-12-05)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡ï¼š[https://towardsdatascience.com/dl-notes-advanced-gradient-descent-4407d84c2515?source=collection_archive---------7-----------------------#2023-12-05](https://towardsdatascience.com/dl-notes-advanced-gradient-descent-4407d84c2515?source=collection_archive---------7-----------------------#2023-12-05)
- en: The main optimization algorithms used for training neural networks, explained
    and implemented from scratch in Python
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä¸»è¦çš„ä¼˜åŒ–ç®—æ³•ç”¨äºè®­ç»ƒç¥ç»ç½‘ç»œï¼Œä»å¤´å¼€å§‹åœ¨ Python ä¸­è§£é‡Šå’Œå®ç°ã€‚
- en: '[](https://medium.com/@luisdamed?source=post_page-----4407d84c2515--------------------------------)[![Luis
    Medina](../Images/d83d326290ae3272f0618d0bd28bd875.png)](https://medium.com/@luisdamed?source=post_page-----4407d84c2515--------------------------------)[](https://towardsdatascience.com/?source=post_page-----4407d84c2515--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----4407d84c2515--------------------------------)
    [Luis Medina](https://medium.com/@luisdamed?source=post_page-----4407d84c2515--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[![Luis Medina](../Images/d83d326290ae3272f0618d0bd28bd875.png)](https://medium.com/@luisdamed?source=post_page-----4407d84c2515--------------------------------)
    [![Towards Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----4407d84c2515--------------------------------)
    [Luis Medina](https://medium.com/@luisdamed?source=post_page-----4407d84c2515--------------------------------)'
- en: Â·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: Â·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F562a027a34f0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdl-notes-advanced-gradient-descent-4407d84c2515&user=Luis+Medina&userId=562a027a34f0&source=post_page-562a027a34f0----4407d84c2515---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----4407d84c2515--------------------------------)
    Â·17 min readÂ·Dec 5, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F4407d84c2515&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdl-notes-advanced-gradient-descent-4407d84c2515&user=Luis+Medina&userId=562a027a34f0&source=-----4407d84c2515---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[å…³æ³¨](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F562a027a34f0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdl-notes-advanced-gradient-descent-4407d84c2515&user=Luis+Medina&userId=562a027a34f0&source=post_page-562a027a34f0----4407d84c2515---------------------post_header-----------)
    å‘è¡¨åœ¨ [Towards Data Science](https://towardsdatascience.com/?source=post_page-----4407d84c2515--------------------------------)
    Â· 17 åˆ†é’Ÿé˜…è¯» Â· 2023å¹´12æœˆ5æ—¥'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4407d84c2515&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdl-notes-advanced-gradient-descent-4407d84c2515&source=-----4407d84c2515---------------------bookmark_footer-----------)![](../Images/5de63561379eb2ad31ce53a3d33a8cbb.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/5de63561379eb2ad31ce53a3d33a8cbb.png)'
- en: Photo by [Jack Anstey](https://unsplash.com/@jack_anstey?utm_source=ghost&utm_medium=referral&utm_campaign=api-credit)
    / [Unsplash](https://unsplash.com/?utm_source=ghost&utm_medium=referral&utm_campaign=api-credit)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ç‰‡æ¥æºï¼š[Jack Anstey](https://unsplash.com/@jack_anstey?utm_source=ghost&utm_medium=referral&utm_campaign=api-credit)
    / [Unsplash](https://unsplash.com/?utm_source=ghost&utm_medium=referral&utm_campaign=api-credit)
- en: In my [previous article about gradient descent](https://medium.com/@luisdamed/gradient-descent-f09f19eb35fb),
    I explained the basic concepts behind it and summarized the main challenges of
    this kind of optimization.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æˆ‘[ä¹‹å‰å…³äºæ¢¯åº¦ä¸‹é™çš„æ–‡ç« ](https://medium.com/@luisdamed/gradient-descent-f09f19eb35fb)ä¸­ï¼Œæˆ‘è§£é‡Šäº†å…¶åŸºæœ¬æ¦‚å¿µï¼Œå¹¶æ€»ç»“äº†è¿™ç§ä¼˜åŒ–æ–¹æ³•çš„ä¸»è¦æŒ‘æˆ˜ã€‚
- en: However, I only covered Stochastic Gradient Descent (SGD) and the â€œbatchâ€ and
    â€œmini-batchâ€ implementation of gradient descent.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶è€Œï¼Œæˆ‘åªæ¶‰åŠäº†éšæœºæ¢¯åº¦ä¸‹é™ï¼ˆSGDï¼‰ä»¥åŠâ€œæ‰¹å¤„ç†â€å’Œâ€œå°æ‰¹é‡â€æ¢¯åº¦ä¸‹é™çš„å®ç°ã€‚
- en: Other algorithms offer advantages in terms of convergence speed, robustness
    to â€œlandscapeâ€ features (the vanishing gradient problem), and less dependence
    on the choice of learning rate to achieve good performance.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä»–ç®—æ³•åœ¨æ”¶æ•›é€Ÿåº¦ã€å¯¹â€œæ™¯è§‚â€ç‰¹å¾ï¼ˆæ¢¯åº¦æ¶ˆå¤±é—®é¢˜ï¼‰çš„é²æ£’æ€§ï¼Œä»¥åŠå¯¹å­¦ä¹ ç‡é€‰æ‹©çš„ä¾èµ–ç¨‹åº¦ç­‰æ–¹é¢æä¾›äº†ä¼˜åŠ¿ã€‚
- en: So today Iâ€™ll write about more advanced optimization algorithms, implementing
    them from scratch in Python and comparing them through animated visualizations.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥ä»Šå¤©æˆ‘å°†è®¨è®ºæ›´é«˜çº§çš„ä¼˜åŒ–ç®—æ³•ï¼Œä»é›¶å¼€å§‹åœ¨ Python ä¸­å®ç°å®ƒä»¬ï¼Œå¹¶é€šè¿‡åŠ¨ç”»å¯è§†åŒ–è¿›è¡Œæ¯”è¾ƒã€‚
- en: Iâ€™ve also listed the resources I used for learning about these algorithms. They
    are great for diving deeper into formal concepts.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘è¿˜åˆ—å‡ºäº†ç”¨äºå­¦ä¹ è¿™äº›ç®—æ³•çš„èµ„æºã€‚è¿™äº›èµ„æºéå¸¸é€‚åˆæ·±å…¥æ¢è®¨æ­£å¼çš„æ¦‚å¿µã€‚
- en: Comparing the algorithms using a simple objective function
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ¯”è¾ƒä½¿ç”¨ç®€å•ç›®æ ‡å‡½æ•°çš„ç®—æ³•
- en: '![](../Images/214af3276a66868c0dd1d65362456436.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/214af3276a66868c0dd1d65362456436.png)'
- en: Throughout this article, Iâ€™ll show how I implemented the different algorithms
    in Python.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘å°†å±•ç¤ºå¦‚ä½•åœ¨ Python ä¸­å®ç°ä¸åŒçš„ç®—æ³•ã€‚
- en: Iâ€™ve created a Jupyter Notebook that you can [**access on GitHub**](https://github.com/luisdamed/Gradient_Descent_Visualizations/blob/main/Advanced_Gradient_Descent_Trajectories.ipynb?ref=makerluis.com)
    or directly [**on Google Colab**](https://colab.research.google.com/github/luisdamed/Gradient_Descent_Visualizations/blob/main/Advanced_Gradient_Descent_Trajectories.ipynb?ref=makerluis.com)to
    see all the code used to create the figures shown here.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘åˆ›å»ºäº†ä¸€ä¸ª Jupyter Notebookï¼Œä½ å¯ä»¥é€šè¿‡[**GitHub è®¿é—®**](https://github.com/luisdamed/Gradient_Descent_Visualizations/blob/main/Advanced_Gradient_Descent_Trajectories.ipynb?ref=makerluis.com)æˆ–ç›´æ¥[**åœ¨
    Google Colab ä¸ŠæŸ¥çœ‹**](https://colab.research.google.com/github/luisdamed/Gradient_Descent_Visualizations/blob/main/Advanced_Gradient_Descent_Trajectories.ipynb?ref=makerluis.com)ä»¥æŸ¥çœ‹ç”¨äºåˆ›å»ºæ­¤å¤„å±•ç¤ºçš„å›¾å½¢çš„æ‰€æœ‰ä»£ç ã€‚
- en: To generate the animations, I used the approach shown in my previous post about
    [creating an animated gradient descent figure in Python](/creating-a-gradient-descent-animation-in-python-3c4dcd20ca51?ref=makerluis.com).
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºç”ŸæˆåŠ¨ç”»ï¼Œæˆ‘ä½¿ç”¨äº†æˆ‘ä¹‹å‰çš„æ–‡ç« ä¸­å±•ç¤ºçš„[åœ¨ Python ä¸­åˆ›å»ºåŠ¨ç”»æ¢¯åº¦ä¸‹é™å›¾å½¢çš„æ–¹æ³•](/creating-a-gradient-descent-animation-in-python-3c4dcd20ca51?ref=makerluis.com)ã€‚
- en: The function definitions assume that the following code has been already included,
    as they use `numpy` classes and methods and call both the function `f` and its
    gradient, `grad_f`.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: å‡½æ•°å®šä¹‰å‡è®¾å·²ç»åŒ…å«äº†ä»¥ä¸‹ä»£ç ï¼Œå› ä¸ºå®ƒä»¬ä½¿ç”¨äº†`numpy`ç±»å’Œæ–¹æ³•ï¼Œå¹¶è°ƒç”¨äº†å‡½æ•°`f`åŠå…¶æ¢¯åº¦`grad_f`ã€‚
- en: '[PRE0]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Momentum
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: åŠ¨é‡
- en: '![](../Images/c2cf61a22635cb112694929bb532b4b0.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c2cf61a22635cb112694929bb532b4b0.png)'
- en: Photo by [Sharon Pittaway](https://unsplash.com/@sharonp?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ç‰‡ç”±[Sharon Pittaway](https://unsplash.com/@sharonp?utm_source=medium&utm_medium=referral)æä¾›ï¼Œæ‹æ‘„äº[Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: We could compare the optimization algorithm with a ball rolling downhill.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥å°†ä¼˜åŒ–ç®—æ³•ä¸çƒä½“æ»šä¸‹å¡é“è¿›è¡Œæ¯”è¾ƒã€‚
- en: If the â€œballâ€ had momentum like it has in real life, it would be less likely
    to remain stuck in a local minimum after accelerating down the hill at full speed.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœâ€œçƒä½“â€åƒç°å®ä¸­é‚£æ ·å…·æœ‰åŠ¨é‡ï¼Œå®ƒåœ¨ä»¥å…¨é€ŸåŠ é€Ÿä¸‹å¡åæ›´ä¸å®¹æ˜“åœç•™åœ¨å±€éƒ¨æœ€å°å€¼ä¸­ã€‚
- en: Thatâ€™s what people realized when dealing with the problem of gradient descent
    getting stuck in local minima.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å°±æ˜¯äººä»¬åœ¨å¤„ç†æ¢¯åº¦ä¸‹é™è¢«å›°åœ¨å±€éƒ¨æœ€å°å€¼çš„é—®é¢˜æ—¶æ„è¯†åˆ°çš„ã€‚
- en: 'From high school physics, we know that *translational* momentum is defined
    by the product of the mass of an object and its velocity:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: ä»é«˜ä¸­ç‰©ç†ä¸­æˆ‘ä»¬çŸ¥é“ï¼Œ*å¹³åŠ¨*åŠ¨é‡å®šä¹‰ä¸ºç‰©ä½“è´¨é‡ä¸å…¶é€Ÿåº¦çš„ä¹˜ç§¯ï¼š
- en: '![](../Images/3d3ebc4b9ecd19513eee3a78b3f19a25.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3d3ebc4b9ecd19513eee3a78b3f19a25.png)'
- en: Translational momentum.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: å¹³åŠ¨åŠ¨é‡ã€‚
- en: 'We also know that the gravitational potential energy of an object of mass ***m***
    is *proportional* to the height ***h*** at which it is placed:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬è¿˜çŸ¥é“ï¼Œè´¨é‡ä¸º***m***çš„ç‰©ä½“çš„é‡åŠ›åŠ¿èƒ½ä¸å…¶æ‰€å¤„çš„é«˜åº¦***h***æ˜¯*æˆæ­£æ¯”*çš„ï¼š
- en: '![](../Images/0403422b98dca25d394344ebd5a5dd03.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0403422b98dca25d394344ebd5a5dd03.png)'
- en: Gravitational potential energy.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: é‡åŠ›åŠ¿èƒ½ã€‚
- en: Moreover, there is a direct [relationship between the potential energy of an
    object and the force exerted on it](https://phys.libretexts.org/Under_Construction/Purgatory/2%3A_Applying_Models_to_Mechanical_Phenomena/2.5%3A_Force_and_Potential_Energy?ref=makerluis.com)
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤å¤–ï¼Œç‰©ä½“çš„åŠ¿èƒ½ä¸æ–½åŠ åœ¨å…¶ä¸Šçš„åŠ›ä¹‹é—´å­˜åœ¨ç›´æ¥çš„[å…³ç³»](https://phys.libretexts.org/Under_Construction/Purgatory/2%3A_Applying_Models_to_Mechanical_Phenomena/2.5%3A_Force_and_Potential_Energy?ref=makerluis.com)
- en: '![](../Images/eac99ef0f2de0d84a9b77378f042d61a.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/eac99ef0f2de0d84a9b77378f042d61a.png)'
- en: Force is equal to the negative gradient of potential energy.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: åŠ›ç­‰äºåŠ¿èƒ½çš„è´Ÿæ¢¯åº¦ã€‚
- en: 'The relationship between **p** and ***U*** can be derived from Newtonâ€™s second
    law of motion:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '**p**ä¸***U***ä¹‹é—´çš„å…³ç³»å¯ä»¥ä»ç‰›é¡¿ç¬¬äºŒå®šå¾‹æ¨å¯¼å‡ºæ¥ï¼š'
- en: The change of motion of an object is proportional to the force impressed; and
    is made in the direction of the straight line in which the force is impressed.
  id: totrans-37
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ç‰©ä½“çš„è¿åŠ¨å˜åŒ–ä¸æ–½åŠ çš„åŠ›æˆæ­£æ¯”ï¼Œå¹¶ä¸”æ²¿ç€åŠ›çš„æ–½åŠ æ–¹å‘å‘ç”Ÿã€‚
- en: '![](../Images/f58e180d6fece6b73243d282bba9189e.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f58e180d6fece6b73243d282bba9189e.png)'
- en: Newtonâ€™s second law of motion.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: ç‰›é¡¿ç¬¬äºŒå®šå¾‹ã€‚
- en: ğŸ’¡ In reality, this physics analogy is too simplified to cover all the advantages
    and disadvantages of adding momentum to a gradient descent optimization. To get
    the whole picture, I recommend you to check [**Why Momentum Really Works?**](https://distill.pub/2017/momentum/?ref=makerluis.com)
  id: totrans-40
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ğŸ’¡ å®é™…ä¸Šï¼Œè¿™ä¸ªç‰©ç†ç±»æ¯”è¿‡äºç®€åŒ–ï¼Œæ— æ³•æ¶µç›–å°†åŠ¨é‡æ·»åŠ åˆ°æ¢¯åº¦ä¸‹é™ä¼˜åŒ–ä¸­çš„æ‰€æœ‰ä¼˜ç¼ºç‚¹ã€‚è¦è·å–å…¨é¢çš„ä¿¡æ¯ï¼Œæ¨èæŸ¥çœ‹[**ä¸ºä»€ä¹ˆåŠ¨é‡çœŸçš„æœ‰æ•ˆï¼Ÿ**](https://distill.pub/2017/momentum/?ref=makerluis.com)
- en: How to add momentum?
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å¦‚ä½•æ·»åŠ åŠ¨é‡ï¼Ÿ
- en: When we initialize the optimization algorithm, we place the â€œballâ€ at a height
    ***h***, giving it potential energy ***U***.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: å½“æˆ‘ä»¬åˆå§‹åŒ–ä¼˜åŒ–ç®—æ³•æ—¶ï¼Œæˆ‘ä»¬æŠŠâ€œçƒâ€æ”¾åœ¨é«˜åº¦***h***å¤„ï¼Œèµ‹äºˆå®ƒåŠ¿èƒ½***U***ã€‚
- en: The force exerted on the ball is proportional to the gradient of such potential
    energy. Like the gradient of the function we are optimizing (the surface over
    which we are moving).
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: æ–½åŠ åœ¨çƒä¸Šçš„åŠ›ä¸è¿™ç§åŠ¿èƒ½çš„æ¢¯åº¦æˆæ­£æ¯”ï¼Œå°±åƒæˆ‘ä»¬ä¼˜åŒ–çš„å‡½æ•°çš„æ¢¯åº¦ä¸€æ ·ï¼ˆæˆ‘ä»¬æ­£åœ¨ç§»åŠ¨çš„è¡¨é¢ï¼‰ã€‚
- en: The way momentum works for the optimization is by using the gradient to change
    the â€œvelocityâ€ of the particle, which in turn changes its position.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: åŠ¨é‡åœ¨ä¼˜åŒ–ä¸­çš„ä½œç”¨æ˜¯åˆ©ç”¨æ¢¯åº¦æ¥æ”¹å˜ç²’å­çš„â€œé€Ÿåº¦â€ï¼Œè¿›è€Œæ”¹å˜å…¶ä½ç½®ã€‚
- en: '![](../Images/eeacef283ed7c5f063589f3754773bb5.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/eeacef283ed7c5f063589f3754773bb5.png)'
- en: Momentum update.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: åŠ¨é‡æ›´æ–°ã€‚
- en: Because of the velocity term, the â€œparticleâ€ builds up speed in any direction
    that has a consistent gradient.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: ç”±äºé€Ÿåº¦é¡¹ï¼Œâ€œç²’å­â€åœ¨å…·æœ‰ä¸€è‡´æ¢¯åº¦çš„ä»»ä½•æ–¹å‘ä¸ŠåŠ é€Ÿã€‚
- en: 'Iâ€™ve implemented this as a Python function as follows:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘å°†è¿™å®ç°ä¸ºä»¥ä¸‹Pythonå‡½æ•°ï¼š
- en: '[PRE1]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The Momentum update both accelerates the optimization in directions of low curvature
    and smooths out (dampening effect) oscillations caused by â€œlandscapeâ€ features
    or noisy data [3].
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: åŠ¨é‡æ›´æ–°æ—¢åŠ é€Ÿäº†åœ¨ä½æ›²ç‡æ–¹å‘ä¸Šçš„ä¼˜åŒ–ï¼Œåˆå¹³æ»‘äº†ï¼ˆé˜»å°¼æ•ˆåº”ï¼‰ç”±äºâ€œåœ°å½¢â€ç‰¹å¾æˆ–å™ªå£°æ•°æ®é€ æˆçš„æŒ¯è¡[3]ã€‚
- en: Some argue that the Momentum update is in reality more consistent with the physical
    effect of the friction coefficient because it reduces the kinetic energy of the
    system [2].
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€äº›äººè®¤ä¸ºåŠ¨é‡æ›´æ–°å®é™…ä¸Šä¸æ‘©æ“¦ç³»æ•°çš„ç‰©ç†æ•ˆæœæ›´ä¸€è‡´ï¼Œå› ä¸ºå®ƒå‡å°‘äº†ç³»ç»Ÿçš„åŠ¨èƒ½[2]ã€‚
- en: Another way to interpret it is that it gives â€œshort-termâ€ memory to the optimization
    process.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: å¦ä¸€ç§è§£é‡Šæ˜¯ï¼Œå®ƒä¸ºä¼˜åŒ–è¿‡ç¨‹æä¾›äº†â€œçŸ­æœŸâ€è®°å¿†ã€‚
- en: 'Being smaller than 1, the momentum parameter acts like an exponentially weighted
    sum of the previous gradients, and the velocity update can be rewritten as [3][5]:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: ç”±äºå°äº1ï¼ŒåŠ¨é‡å‚æ•°åƒæ˜¯ä¹‹å‰æ¢¯åº¦çš„æŒ‡æ•°åŠ æƒå’Œï¼Œé€Ÿåº¦æ›´æ–°å¯ä»¥é‡å†™ä¸º[3][5]ï¼š
- en: '![](../Images/da422a45d928db3a2feada48ebda9a0f.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/da422a45d928db3a2feada48ebda9a0f.png)'
- en: Velocity term rewritten as a weighted sum.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: é€Ÿåº¦é¡¹è¢«é‡å†™ä¸ºåŠ æƒå’Œã€‚
- en: where *g* is the instantaneous gradient, and *v* is the smoothed gradient estimator.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­*g*æ˜¯ç¬æ—¶æ¢¯åº¦ï¼Œ*v*æ˜¯å¹³æ»‘çš„æ¢¯åº¦ä¼°è®¡å™¨ã€‚
- en: The parameter **Î²** controls how much weight we give to new values of the instantaneous
    gradient over the previous ones.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°**Î²**æ§åˆ¶æˆ‘ä»¬å¯¹å½“å‰æ¢¯åº¦ä¸ä¹‹å‰æ¢¯åº¦ä¹‹é—´æ–°å€¼çš„æƒé‡åˆ†é…ã€‚
- en: It is usually equal to 0.9, but sometimes it is â€œscheduledâ€, i.e. increased
    from as low as 0.5 up to 0.99 as the iterations progress.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: é€šå¸¸å®ƒçš„å€¼ä¸º0.9ï¼Œä½†æœ‰æ—¶ä¼šâ€œè°ƒåº¦â€ï¼Œå³åœ¨è¿­ä»£è¿‡ç¨‹ä¸­ä»0.5é€æ­¥å¢åŠ åˆ°0.99ã€‚
- en: Nesterovâ€™s Accelerated Gradient (NAG)
  id: totrans-59
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: NesterovåŠ é€Ÿæ¢¯åº¦ï¼ˆNAGï¼‰
- en: Proposed by Nesterov, Y. in 1983.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: ç”±Nesteroväº1983å¹´æå‡ºã€‚
- en: The Nesterov update implements a â€œlookaheadâ€ feature to improve the stability
    and convergence speed of Momentum for convex functions.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: Nesterovæ›´æ–°å®ç°äº†â€œå‰ç»â€åŠŸèƒ½ï¼Œä»¥æé«˜åŠ¨é‡åœ¨å‡¸å‡½æ•°ä¸Šçš„ç¨³å®šæ€§å’Œæ”¶æ•›é€Ÿåº¦ã€‚
- en: '![](../Images/5499819332220dbcf8777354761a3983.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5499819332220dbcf8777354761a3983.png)'
- en: NAG update.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: NAGæ›´æ–°ã€‚
- en: While Momentum uses the current position to update the gradient, NAG performs
    a partial update of the current position first, knowing that
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: åŠ¨é‡ä½¿ç”¨å½“å‰ä½ç½®æ¥æ›´æ–°æ¢¯åº¦ï¼Œè€ŒNAGé¦–å…ˆå¯¹å½“å‰ä½ç½®è¿›è¡Œéƒ¨åˆ†æ›´æ–°ï¼ŒçŸ¥é“
- en: '![](../Images/c239638fff8b6cd73262d88f0b93ba0a.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c239638fff8b6cd73262d88f0b93ba0a.png)'
- en: Intuition for lookahead update.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹å‰ç»æ€§æ›´æ–°çš„ç›´è§‰ã€‚
- en: '![](../Images/15c24caf53126189dff5fd3fbae2b2c9.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/15c24caf53126189dff5fd3fbae2b2c9.png)'
- en: Vector representation of Momentum and NAG updates.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: åŠ¨é‡å’ŒNAGæ›´æ–°çš„çŸ¢é‡è¡¨ç¤ºã€‚
- en: 'To implement this as a Python function, I just made the following modification
    to the â€œvelocity updateâ€ in the code I showed before:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†å°†å…¶å®ç°ä¸ºPythonå‡½æ•°ï¼Œæˆ‘å¯¹ä¹‹å‰å±•ç¤ºçš„â€œé€Ÿåº¦æ›´æ–°â€ä»£ç è¿›è¡Œäº†ä»¥ä¸‹ä¿®æ”¹ï¼š
- en: '[PRE2]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: This partial update helps improve the accuracy of the optimization. In practical
    terms, it translates into fewer oscillations around a local minimum when compared
    to Momentum.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸€éƒ¨åˆ†æ›´æ–°æœ‰åŠ©äºæé«˜ä¼˜åŒ–çš„å‡†ç¡®æ€§ã€‚å®é™…ä¸Šï¼Œè¿™æ„å‘³ç€ä¸åŠ¨é‡æ³•ç›¸æ¯”ï¼Œå®ƒåœ¨å±€éƒ¨æœ€å°å€¼é™„è¿‘çš„æŒ¯è¡æ›´å°‘ã€‚
- en: The difference is evident in the next figure.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: å·®å¼‚åœ¨ä¸‹å›¾ä¸­éå¸¸æ˜æ˜¾ã€‚
- en: '![](../Images/06e151df4c4996aca7b296f530e03d3c.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/06e151df4c4996aca7b296f530e03d3c.png)'
- en: Comparing Momentum and NAG descent optimizations for a complex surface.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: æ¯”è¾ƒåŠ¨é‡æ³•å’ŒNAGåœ¨å¤æ‚è¡¨é¢ä¸Šçš„ä¸‹é™ä¼˜åŒ–ã€‚
- en: Both optimizers are initialized at the same coordinates, and using the same
    momentum parameter (0.95, fixed).
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸¤ä¸ªä¼˜åŒ–å™¨åœ¨ç›¸åŒåæ ‡ä¸Šåˆå§‹åŒ–ï¼Œå¹¶ä½¿ç”¨ç›¸åŒçš„åŠ¨é‡å‚æ•°ï¼ˆ0.95ï¼Œå›ºå®šï¼‰ã€‚
- en: The following animation also helps us see the intuition behind scheduling or
    â€œannealingâ€ the momentum parameter.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: ä»¥ä¸‹åŠ¨ç”»ä¹Ÿå¸®åŠ©æˆ‘ä»¬ç†è§£è°ƒåº¦æˆ–â€œé€€ç«â€åŠ¨é‡å‚æ•°çš„ç›´è§‚æ„Ÿå—ã€‚
- en: '![](../Images/659217b9fdbdccb3a83429be0bb9cbf8.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/659217b9fdbdccb3a83429be0bb9cbf8.png)'
- en: Comparison of different optimization algorithms getting past a vanishing gradient
    region. Momentum-based methods perform better in this case.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: æ¯”è¾ƒä¸åŒä¼˜åŒ–ç®—æ³•åœ¨ç©¿è¶Šæ¢¯åº¦æ¶ˆå¤±åŒºåŸŸæ—¶çš„è¡¨ç°ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼ŒåŸºäºåŠ¨é‡çš„æ–¹æ³•è¡¨ç°æ›´å¥½ã€‚
- en: In the beginning, a small amount of momentum can be beneficial to get past the
    vanishing gradient. When we approach the local minimum, having larger momentum
    values could dampen out the oscillations we see, improving the convergence speed.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€å¼€å§‹ï¼Œå°‘é‡åŠ¨é‡æœ‰åŠ©äºç©¿è¶Šæ¢¯åº¦æ¶ˆå¤±åŒºåŸŸã€‚å½“æˆ‘ä»¬æ¥è¿‘å±€éƒ¨æœ€å°å€¼æ—¶ï¼Œè¾ƒå¤§çš„åŠ¨é‡å€¼å¯èƒ½ä¼šå‡å°æˆ‘ä»¬è§‚å¯Ÿåˆ°çš„æŒ¯è¡ï¼Œä»è€Œæé«˜æ”¶æ•›é€Ÿåº¦ã€‚
- en: Adaptive methods
  id: totrans-80
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: è‡ªé€‚åº”æ–¹æ³•
- en: The other optimization algorithms shown in the animation above are adaptive
    methods, which Iâ€™ll describe in this section.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸Šé¢åŠ¨ç”»ä¸­å±•ç¤ºçš„å…¶ä»–ä¼˜åŒ–ç®—æ³•æ˜¯è‡ªé€‚åº”æ–¹æ³•ï¼Œæˆ‘å°†åœ¨æœ¬èŠ‚ä¸­æè¿°è¿™äº›æ–¹æ³•ã€‚
- en: Looking at this simple example with a local and a global minimum, Momentum and
    NAG may seem far superior to the other methods. However, adaptive algorithms are
    more robust. Iâ€™ll show this with a practical example in another article.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: é€šè¿‡è¿™ä¸ªç®€å•çš„ä¾‹å­ï¼ŒåŠ¨é‡æ³•å’ŒNAGä¼¼ä¹æ¯”å…¶ä»–æ–¹æ³•ä¼˜è¶Šã€‚ç„¶è€Œï¼Œè‡ªé€‚åº”ç®—æ³•æ›´å…·é²æ£’æ€§ã€‚æˆ‘å°†åœ¨å¦ä¸€ç¯‡æ–‡ç« ä¸­é€šè¿‡å®é™…ä¾‹å­æ¥å±•ç¤ºè¿™ä¸€ç‚¹ã€‚
- en: Adaptive Gradient Algorithm (AdaGrad)
  id: totrans-83
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è‡ªé€‚åº”æ¢¯åº¦ç®—æ³•ï¼ˆAdaGradï¼‰
- en: AdaGrad is a family of [subgradient](https://web.stanford.edu/class/ee392o/subgrad_method.pdf?ref=makerluis.com)
    algorithms for stochastic optimization, presented by [John Duchi, Elad Hazan and
    Yoram Singer in 2011.](https://jmlr.org/papers/v12/duchi11a.html?ref=makerluis.com)
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: AdaGrad æ˜¯ä¸€ç±»ç”¨äºéšæœºä¼˜åŒ–çš„[æ¬¡æ¢¯åº¦](https://web.stanford.edu/class/ee392o/subgrad_method.pdf?ref=makerluis.com)ç®—æ³•ï¼Œç”±[John
    Duchiã€Elad Hazan å’Œ Yoram Singeräº2011å¹´](https://jmlr.org/papers/v12/duchi11a.html?ref=makerluis.com)æå‡ºã€‚
- en: They proposed to improve gradient-based learning by incorporating the history
    of the gradients into each new update of the weights.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: ä»–ä»¬æå‡ºé€šè¿‡å°†æ¢¯åº¦çš„å†å²ä¿¡æ¯çº³å…¥æ¯æ¬¡æ–°çš„æƒé‡æ›´æ–°ä¸­æ¥æ”¹è¿›åŸºäºæ¢¯åº¦çš„å­¦ä¹ ã€‚
- en: Instead of biasing the gradient itself, as momentum does, AdaGrad modifies the
    learning rate dynamically and separately for each parameter of the objective function.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸åŠ¨é‡æ³•é€šè¿‡åç½®æ¢¯åº¦æœ¬èº«ä¸åŒï¼ŒAdaGrad åŠ¨æ€åœ°ä¸ºç›®æ ‡å‡½æ•°çš„æ¯ä¸ªå‚æ•°å•ç‹¬ä¿®æ”¹å­¦ä¹ ç‡ã€‚
- en: This means we have different learning rates for each model weight. They are
    adjusted based on the consistency of the gradients.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ„å‘³ç€æˆ‘ä»¬å¯¹æ¯ä¸ªæ¨¡å‹æƒé‡ä½¿ç”¨ä¸åŒçš„å­¦ä¹ ç‡ã€‚è¿™äº›å­¦ä¹ ç‡åŸºäºæ¢¯åº¦çš„ä¸€è‡´æ€§è¿›è¡Œè°ƒæ•´ã€‚
- en: 'To do this, the sequence of gradient estimates is stored as follows:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºæ­¤ï¼Œæ¢¯åº¦ä¼°è®¡çš„åºåˆ—å¦‚ä¸‹å­˜å‚¨ï¼š
- en: '![](../Images/018b04875576d906b9444d027b4adc3f.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/018b04875576d906b9444d027b4adc3f.png)'
- en: Sum of squared gradients or outer product of gradient history.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: æ¢¯åº¦çš„å¹³æ–¹å’Œæˆ–æ¢¯åº¦å†å²çš„å¤–ç§¯ã€‚
- en: If we are optimizing a function with *n* coordinates or parameters, g**â‚œ** will
    be a vector with *n* elements, and so will G**â‚œ**.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæˆ‘ä»¬ä¼˜åŒ–ä¸€ä¸ªå…·æœ‰*n*ä¸ªåæ ‡æˆ–å‚æ•°çš„å‡½æ•°ï¼Œg**â‚œ**å°†æ˜¯ä¸€ä¸ªå…·æœ‰*n*ä¸ªå…ƒç´ çš„å‘é‡ï¼ŒG**â‚œ**ä¹Ÿæ˜¯å¦‚æ­¤ã€‚
- en: 'Then, the update rule is given by:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åï¼Œæ›´æ–°è§„åˆ™å¦‚ä¸‹ï¼š
- en: '![](../Images/3bc65abe1897974a0676b8470522d9b0.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3bc65abe1897974a0676b8470522d9b0.png)'
- en: AdaGrad update.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: AdaGrad æ›´æ–°ã€‚
- en: The parameter **Îµ** is used to avoid a division by zero and is usually set to
    a small value, like 1e-08.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°**Îµ**ç”¨äºé¿å…é™¤é›¶é”™è¯¯ï¼Œé€šå¸¸è®¾ç½®ä¸ºä¸€ä¸ªå°å€¼ï¼Œå¦‚1e-08ã€‚
- en: Interestingly, the definition of G**â‚œ** is similar to the un-centered (zero-mean)
    [variance](https://en.wikipedia.org/wiki/Variance?ref=makerluis.com) of the distribution
    of gradients.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: æœ‰è¶£çš„æ˜¯ï¼ŒG**â‚œ**çš„å®šä¹‰ç±»ä¼¼äºæ¢¯åº¦åˆ†å¸ƒçš„éä¸­å¿ƒï¼ˆé›¶å‡å€¼ï¼‰[æ–¹å·®](https://en.wikipedia.org/wiki/Variance?ref=makerluis.com)ã€‚
- en: '![](../Images/77d7e86480587b0db5f7d8998e28bfd8.png)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/77d7e86480587b0db5f7d8998e28bfd8.png)'
- en: Variance definition.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: æ–¹å·®å®šä¹‰ã€‚
- en: The variance is a measure of the dispersion energy of a distribution.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: æ–¹å·®æ˜¯åˆ†å¸ƒçš„ç¦»æ•£èƒ½é‡çš„åº¦é‡ã€‚
- en: Hence, for each parameter **Î¸áµ¢**, the learning rate is adapted in proportion
    to the *inverse* of the gradientâ€™s variance for **Î¸áµ¢**.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: å› æ­¤ï¼Œå¯¹äºæ¯ä¸ªå‚æ•°**Î¸áµ¢**ï¼Œå­¦ä¹ ç‡æ˜¯æ ¹æ®**Î¸áµ¢**çš„æ¢¯åº¦æ–¹å·®çš„*å€’æ•°*æ¥è°ƒæ•´çš„ã€‚
- en: Considering this, we could say that parameters with more dispersion in their
    gradient distribution will scale down the learning rate by a larger factor, while
    those with more consistent gradients (lower variance) will have larger learning
    rates.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: è€ƒè™‘åˆ°è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬å¯ä»¥è¯´ï¼Œæ¢¯åº¦åˆ†å¸ƒä¸­ç¦»æ•£ç¨‹åº¦è¾ƒå¤§çš„å‚æ•°å°†æŒ‰è¾ƒå¤§çš„æ¯”ä¾‹ç¼©å°å­¦ä¹ ç‡ï¼Œè€Œæ¢¯åº¦è¾ƒä¸€è‡´ï¼ˆæ–¹å·®è¾ƒä½ï¼‰çš„å‚æ•°å°†å…·æœ‰è¾ƒå¤§çš„å­¦ä¹ ç‡ã€‚
- en: AdaGrad also implements learning rate decay automatically, based on the time
    (accumulation of previous gradients) and the curvature of the objective function
    (â€œareasâ€ with lower gradient variance will be assigned smaller step sizes). This
    improves the convergence rate of the algorithm.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: AdaGradè¿˜æ ¹æ®æ—¶é—´ï¼ˆä¹‹å‰æ¢¯åº¦çš„ç´¯ç§¯ï¼‰å’Œç›®æ ‡å‡½æ•°çš„æ›²ç‡ï¼ˆâ€œåŒºåŸŸâ€ä¸­çš„æ¢¯åº¦æ–¹å·®è¾ƒä½å°†åˆ†é…è¾ƒå°çš„æ­¥é•¿ï¼‰è‡ªåŠ¨å®ç°å­¦ä¹ ç‡è¡°å‡ã€‚è¿™æ”¹å–„äº†ç®—æ³•çš„æ”¶æ•›é€Ÿåº¦ã€‚
- en: 'Iâ€™ve implemented AdaGrad as a Python function as follows:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘å·²å°†AdaGradå®ç°ä¸ºä»¥ä¸‹Pythonå‡½æ•°ï¼š
- en: '[PRE3]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: One drawback of AdaGrad is that this learning rate decay during training may
    be too aggressive, causing the learning to stop early when training ANNs. Each
    parameter update is robust, but the rate at which the changes move toward the
    optimal point can decrease too much.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: AdaGradçš„ä¸€ä¸ªç¼ºç‚¹æ˜¯ï¼Œè¿™ç§è®­ç»ƒè¿‡ç¨‹ä¸­å­¦ä¹ ç‡çš„è¡°å‡å¯èƒ½è¿‡äºæ¿€è¿›ï¼Œå¯¼è‡´åœ¨è®­ç»ƒäººå·¥ç¥ç»ç½‘ç»œæ—¶å­¦ä¹ è¿‡æ—©åœæ­¢ã€‚æ¯æ¬¡å‚æ•°æ›´æ–°éƒ½å¾ˆç¨³å¥ï¼Œä½†å˜åŒ–æ¥è¿‘æœ€ä¼˜ç‚¹çš„é€Ÿåº¦å¯èƒ½ä¼šä¸‹é™å¾—å¤ªå¤šã€‚
- en: Another drawback is that, although the learning rates are self-adjusted during
    learning, AdaGrad can still be sensitive to the initial conditions. If the gradients
    are large at the start of the optimization, the learning rates will be low for
    the rest of the training.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: å¦ä¸€ä¸ªç¼ºç‚¹æ˜¯ï¼Œè™½ç„¶å­¦ä¹ ç‡åœ¨å­¦ä¹ è¿‡ç¨‹ä¸­ä¼šè‡ªæˆ‘è°ƒæ•´ï¼Œä½†AdaGradä»ç„¶å¯¹åˆå§‹æ¡ä»¶æ•æ„Ÿã€‚å¦‚æœä¼˜åŒ–å¼€å§‹æ—¶æ¢¯åº¦å¾ˆå¤§ï¼Œé‚£ä¹ˆè®­ç»ƒè¿‡ç¨‹ä¸­å­¦ä¹ ç‡ä¼šè¾ƒä½ã€‚
- en: We can see this in the animated figure. AdaGrad breaks off from the symmetry
    quickly, but the learning is very slow, compared to other algorithms.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥åœ¨åŠ¨ç”»å›¾ä¸­çœ‹åˆ°è¿™ä¸€ç‚¹ã€‚AdaGradå¾ˆå¿«æ‰“ç ´äº†å¯¹ç§°æ€§ï¼Œä½†å­¦ä¹ éå¸¸æ…¢ï¼Œä¸å…¶ä»–ç®—æ³•ç›¸æ¯”ã€‚
- en: To compensate for this, one may need to tune the learning rate to a higher value,
    which in part defeats the purpose of the self-adjusting characteristic.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†å¼¥è¡¥è¿™ä¸€ç‚¹ï¼Œå¯èƒ½éœ€è¦å°†å­¦ä¹ ç‡è°ƒæ•´åˆ°æ›´é«˜çš„å€¼ï¼Œè¿™åœ¨ä¸€å®šç¨‹åº¦ä¸Šå‰Šå¼±äº†è‡ªæˆ‘è°ƒæ•´ç‰¹æ€§çš„ç›®çš„ã€‚
- en: Root Mean Square Propagation (RMSprop)
  id: totrans-109
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å‡æ–¹æ ¹ä¼ æ’­ï¼ˆRMSpropï¼‰
- en: Unpublished method, but mentioned in the [slides for lecture 6](https://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf?ref=makerluis.com)
    of the course Neural Networks for Machine Learning, from Prof. [Geoffrey Hinton](https://en.wikipedia.org/wiki/Geoffrey_Hinton?ref=makerluis.com).
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: æœªå‘è¡¨çš„æ–¹æ³•ï¼Œä½†åœ¨è¯¾ç¨‹ã€Šç¥ç»ç½‘ç»œä¸æœºå™¨å­¦ä¹ ã€‹ç¬¬6è®²çš„[å¹»ç¯ç‰‡](https://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf?ref=makerluis.com)ä¸­æåˆ°ï¼Œç”±[Geoffrey
    Hinton](https://en.wikipedia.org/wiki/Geoffrey_Hinton?ref=makerluis.com)æ•™æˆæä¾›ã€‚
- en: The concept of this algorithm is similar to momentum. It also incorporates the
    short-term history of the magnitude of the gradients to perform the weights update.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªç®—æ³•çš„æ¦‚å¿µç±»ä¼¼äºåŠ¨é‡ã€‚å®ƒè¿˜ç»“åˆäº†æ¢¯åº¦å¹…åº¦çš„çŸ­æœŸå†å²æ¥æ‰§è¡Œæƒé‡æ›´æ–°ã€‚
- en: However, similarly to AdaGrad, RMSProp modifies the learning rate and not the
    gradient.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶è€Œï¼Œä¸AdaGradç±»ä¼¼ï¼ŒRMSPropä¿®æ”¹çš„æ˜¯å­¦ä¹ ç‡è€Œä¸æ˜¯æ¢¯åº¦ã€‚
- en: To do this, the learning rate is divided by a running average of the magnitudes
    of recent gradients.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºæ­¤ï¼Œå­¦ä¹ ç‡è¢«é™¤ä»¥æœ€è¿‘æ¢¯åº¦å¹…åº¦çš„æ»‘åŠ¨å¹³å‡ã€‚
- en: 'First, the algorithm computes a weighted sum of the squared cost values and
    the previous ones:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: é¦–å…ˆï¼Œç®—æ³•è®¡ç®—å¹³æ–¹æˆæœ¬å€¼åŠå…¶ä¹‹å‰å€¼çš„åŠ æƒå’Œã€‚
- en: '![](../Images/701f7cc431162487e3d89e088ba1bfc1.png)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/701f7cc431162487e3d89e088ba1bfc1.png)'
- en: Exponentially weighted sum of squared costs.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: å¹³æ–¹æˆæœ¬çš„æŒ‡æ•°åŠ æƒå’Œã€‚
- en: This is like a short-term mean, where the parameter **Î²** adjusts how much weight
    is given to more recent cost values over older ones.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å°±åƒæ˜¯ä¸€ä¸ªçŸ­æœŸå‡å€¼ï¼Œå…¶ä¸­å‚æ•°**Î²**è°ƒæ•´äº†ç»™æ›´è¿‘æœŸçš„æˆæœ¬å€¼ç›¸è¾ƒäºè¾ƒæ—§çš„æˆæœ¬å€¼çš„æƒé‡ã€‚
- en: It is analog to the re-written form of momentum that I mentioned before but
    applied to the squared costs, instead of the gradient.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: å®ƒç±»ä¼¼äºæˆ‘ä¹‹å‰æåˆ°çš„åŠ¨é‡çš„é‡å†™å½¢å¼ï¼Œä½†åº”ç”¨äºå¹³æ–¹æˆæœ¬ï¼Œè€Œä¸æ˜¯æ¢¯åº¦ã€‚
- en: The next step is dividing the learning rate by the square root of this moving
    average.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸‹ä¸€æ­¥æ˜¯å°†å­¦ä¹ ç‡é™¤ä»¥è¿™ä¸ªç§»åŠ¨å¹³å‡çš„å¹³æ–¹æ ¹ã€‚
- en: '![](../Images/6d74a625b221b9b4e476c5fb201a4368.png)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6d74a625b221b9b4e476c5fb201a4368.png)'
- en: RMSProp update rule.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: RMSProp æ›´æ–°è§„åˆ™ã€‚
- en: This way, the step size depends on the history of the gradient magnitudes (short-term
    memory).
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ ·ï¼Œæ­¥é•¿ä¾èµ–äºæ¢¯åº¦å¹…åº¦çš„å†å²ï¼ˆçŸ­æœŸè®°å¿†ï¼‰ã€‚
- en: Notice that computing the root of a weighted sum (or weighted average) of squared
    values is equivalent to computing the Root Mean Square (RMS) of those values.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: æ³¨æ„ï¼Œè®¡ç®—åŠ æƒå¹³æ–¹å’Œçš„å¹³æ–¹æ ¹ï¼ˆæˆ–åŠ æƒå¹³å‡ï¼‰ç­‰åŒäºè®¡ç®—è¿™äº›å€¼çš„å‡æ–¹æ ¹ï¼ˆRMSï¼‰ã€‚
- en: '![](../Images/752ab5645f800051560cda31b3e4e5da.png)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/752ab5645f800051560cda31b3e4e5da.png)'
- en: Definition of RMS.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: RMS çš„å®šä¹‰ã€‚
- en: The RMS of a signal is a representation of its *total* energy (as opposed to
    the variance, which represents its *dispersion* energy)[1].
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: ä¿¡å·çš„ RMS æ˜¯å…¶*æ€»*èƒ½é‡çš„è¡¨ç¤ºï¼ˆä¸æ–¹å·®ä¸åŒï¼Œæ–¹å·®è¡¨ç¤ºçš„æ˜¯*ç¦»æ•£*èƒ½é‡ï¼‰[1]ã€‚
- en: Therefore, with RMSProp, the learning rate is modulated according to the total
    energy of the gradient of the cost function and its previous values. This adjustment
    is done dynamically and for each direction or component of our loss function (each
    weight!).
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: å› æ­¤ï¼Œä½¿ç”¨ RMSProp æ—¶ï¼Œå­¦ä¹ ç‡ä¼šæ ¹æ®æˆæœ¬å‡½æ•°çš„æ¢¯åº¦åŠå…¶å…ˆå‰å€¼çš„æ€»èƒ½é‡è¿›è¡Œè°ƒèŠ‚ã€‚è¿™ç§è°ƒæ•´æ˜¯åŠ¨æ€çš„ï¼Œå¹¶ä¸”é’ˆå¯¹æŸå¤±å‡½æ•°çš„æ¯ä¸ªæ–¹å‘æˆ–ç»„ä»¶ï¼ˆæ¯ä¸ªæƒé‡ï¼ï¼‰ã€‚
- en: The goal is to reduce the volatility caused by large changes in the gradient
    by reducing the step size in those cases.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: ç›®æ ‡æ˜¯é€šè¿‡åœ¨è¿™äº›æƒ…å†µä¸‹å‡å°æ­¥é•¿æ¥å‡å°‘ç”±äºæ¢¯åº¦å¤§å¹…å˜åŒ–å¼•èµ·çš„æ³¢åŠ¨æ€§ã€‚
- en: This also helps with vanishing gradient problems because when thereâ€™s a trend
    of very small gradients we take larger steps.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¹Ÿæœ‰åŠ©äºè§£å†³æ¢¯åº¦æ¶ˆå¤±é—®é¢˜ï¼Œå› ä¸ºå½“æ¢¯åº¦éå¸¸å°æ—¶ï¼Œæˆ‘ä»¬ä¼šé‡‡å–æ›´å¤§çš„æ­¥ä¼ã€‚
- en: 'This is how I coded it as a Python function:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯æˆ‘ä½œä¸º Python å‡½æ•°ç¼–ç çš„æ–¹å¼ï¼š
- en: '[PRE4]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: RMSprop is robust to the initial choice of learning rate, and it also implements
    an automatic learning rate decay. However, since it is based on a short-term history
    of gradient values, the decay is less aggressive than AdaGrad.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: RMSprop å¯¹å­¦ä¹ ç‡çš„åˆå§‹é€‰æ‹©éå¸¸ç¨³å¥ï¼Œå¹¶ä¸”è¿˜å®ç°äº†è‡ªåŠ¨å­¦ä¹ ç‡è¡°å‡ã€‚ç„¶è€Œï¼Œç”±äºå®ƒåŸºäºæ¢¯åº¦å€¼çš„çŸ­æœŸå†å²ï¼Œå› æ­¤è¡°å‡æ¯” AdaGrad æ›´åŠ æ¸©å’Œã€‚
- en: '![](../Images/0c6c1edca99d72c5a323f8c2e692bcce.png)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0c6c1edca99d72c5a323f8c2e692bcce.png)'
- en: Photo by [Gonzalo Kaplanski](https://unsplash.com/@gonzakap?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ç‰‡ç”± [Gonzalo Kaplanski](https://unsplash.com/@gonzakap?utm_source=medium&utm_medium=referral)
    æ‹æ‘„ï¼Œæ¥æºäº [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: AdaDelta
  id: totrans-135
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: AdaDelta
- en: Proposed by [Matthew Zeiler in 2012](https://arxiv.org/abs/1212.5701?ref=makerluis.com).
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: ç”± [Matthew Zeiler äº 2012 å¹´æå‡º](https://arxiv.org/abs/1212.5701?ref=makerluis.com)ã€‚
- en: 'This method was developed to overcome the main limitations of AdaGrad: the
    continuous decay of the learning rate that causes early stopping and the need
    to tune the â€œglobalâ€ learning rate manually.'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ç§æ–¹æ³•æ˜¯ä¸ºäº†å…‹æœ AdaGrad çš„ä¸»è¦å±€é™æ€§è€Œå¼€å‘çš„ï¼šå­¦ä¹ ç‡çš„æŒç»­è¡°å‡å¯¼è‡´æå‰åœæ­¢ï¼Œå¹¶ä¸”éœ€è¦æ‰‹åŠ¨è°ƒæ•´â€œå…¨å±€â€å­¦ä¹ ç‡ã€‚
- en: To overcome the continuous learning rate decay, the algorithm accumulates the
    history of past gradients over a window or fixed size.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†å…‹æœæŒç»­çš„å­¦ä¹ ç‡è¡°å‡ï¼Œç®—æ³•åœ¨ä¸€ä¸ªçª—å£æˆ–å›ºå®šå¤§å°å†…ç´¯ç§¯è¿‡å»æ¢¯åº¦çš„å†å²ã€‚
- en: 'In practice, this involves dividing the learning rate by the RMS of previous
    gradients over a window of fixed size, just like RMSprop does:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: å®é™…ä¸Šï¼Œè¿™æ¶‰åŠåˆ°å°†å­¦ä¹ ç‡é™¤ä»¥å›ºå®šçª—å£å†…å…ˆå‰æ¢¯åº¦çš„ RMSï¼Œå°±åƒ RMSprop æ‰€åšçš„é‚£æ ·ï¼š
- en: '![](../Images/e3e257fde445b1c8167cd06aa71c2ba6.png)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e3e257fde445b1c8167cd06aa71c2ba6.png)'
- en: Learning rate scaling similar to RMSProp.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: å­¦ä¹ ç‡ç¼©æ”¾ç±»ä¼¼äº RMSPropã€‚
- en: The next modification from AdaGrad is the correction of the units of the optimization
    updates.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: AdaGrad çš„ä¸‹ä¸€ä¸ªæ”¹è¿›æ˜¯ä¼˜åŒ–æ›´æ–°å•å…ƒçš„ä¿®æ­£ã€‚
- en: 'In AdaGrad (and all other optimization algorithms Iâ€™ve described so far), the
    units of the optimization steps donâ€™t match the units of the parameters that we
    modify to optimize the cost function [9]:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ AdaGradï¼ˆä»¥åŠæˆ‘è¿„ä»Šä¸ºæ­¢æè¿°çš„æ‰€æœ‰å…¶ä»–ä¼˜åŒ–ç®—æ³•ï¼‰ä¸­ï¼Œä¼˜åŒ–æ­¥éª¤çš„å•ä½ä¸æˆ‘ä»¬ä¸ºä¼˜åŒ–æˆæœ¬å‡½æ•°è€Œä¿®æ”¹çš„å‚æ•°å•ä½ä¸åŒ¹é… [9]ï¼š
- en: '![](../Images/bfb23bf151501ad39ebfd3a3e9704540.png)'
  id: totrans-144
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bfb23bf151501ad39ebfd3a3e9704540.png)'
- en: We know from school that we canâ€™t add apples and oranges. But with these optimization
    algorithms, itâ€™s like weâ€™ve been adding â€œapplesâ€ (current parameter values, **Î¸â‚œ**
    and some unknown quantity (the optimization step **Î”Î¸** ) that mathematically
    can be added to them to obtain new apples (the updated parameters, **Î¸â‚œ â‚Šâ‚**.
    It just works, but doesnâ€™t make sense in real life.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬åœ¨å­¦æ ¡é‡ŒçŸ¥é“ä¸èƒ½å°†è‹¹æœå’Œæ©™å­ç›¸åŠ ã€‚ä½†ä½¿ç”¨è¿™äº›ä¼˜åŒ–ç®—æ³•æ—¶ï¼Œå°±åƒæˆ‘ä»¬åœ¨åŠ â€œè‹¹æœâ€ï¼ˆå½“å‰å‚æ•°å€¼ï¼Œ**Î¸â‚œ** å’Œä¸€äº›æœªçŸ¥é‡ï¼ˆä¼˜åŒ–æ­¥éª¤ **Î”Î¸**ï¼‰ï¼Œè¿™äº›åœ¨æ•°å­¦ä¸Šå¯ä»¥åŠ åˆ°ä¸€èµ·ä»¥è·å¾—æ–°çš„è‹¹æœï¼ˆæ›´æ–°åçš„å‚æ•°ï¼Œ**Î¸â‚œ
    â‚Šâ‚**ï¼‰ã€‚è¿™æœ‰æ•ˆï¼Œä½†åœ¨ç°å®ç”Ÿæ´»ä¸­æ²¡æœ‰æ„ä¹‰ã€‚
- en: 'Zeiler decided to correct the units, rearranging the update term from [Newtonâ€™s
    method](https://en.wikipedia.org/wiki/Newton%27s_method?ref=makerluis.com) and
    assuming the curvature of the loss function could be approximated by a diagonal
    [Hessian matrix](https://en.wikipedia.org/wiki/Hessian_matrix?ref=makerluis.com):'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: Zeiler å†³å®šçº æ­£å•ä½ï¼Œå°†æ›´æ–°é¡¹ä»[ç‰›é¡¿æ³•](https://en.wikipedia.org/wiki/Newton%27s_method?ref=makerluis.com)é‡æ–°æ’åˆ—ï¼Œå¹¶å‡è®¾æŸå¤±å‡½æ•°çš„æ›²ç‡å¯ä»¥ç”¨å¯¹è§’[æµ·æ£®çŸ©é˜µ](https://en.wikipedia.org/wiki/Hessian_matrix?ref=makerluis.com)æ¥è¿‘ä¼¼ï¼š
- en: '![](../Images/d4cb70742f35ebddcf11f40b98d708ac.png)'
  id: totrans-147
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d4cb70742f35ebddcf11f40b98d708ac.png)'
- en: Comparing this observation with the update rule similar to RMSProp, Zeiler determined
    the correct form of the update term to preserve the right units.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: å°†è¿™ä¸€è§‚å¯Ÿä¸ç±»ä¼¼äº RMSProp çš„æ›´æ–°è§„åˆ™è¿›è¡Œæ¯”è¾ƒï¼ŒZeiler ç¡®å®šäº†æ­£ç¡®çš„æ›´æ–°é¡¹å½¢å¼ï¼Œä»¥ä¿æŒæ­£ç¡®çš„å•ä½ã€‚
- en: 'The intuition is better explained in the original publication but in practice,
    it resulted in adding the square root of an exponentially-weighted average of
    the previous update values to the numerator of the update term:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: ç›´è§‚çš„è§£é‡Šåœ¨åŸå§‹å‡ºç‰ˆç‰©ä¸­æ›´ä¸ºè¯¦å°½ï¼Œä½†å®é™…ä¸Šï¼Œå®ƒå¯¼è‡´åœ¨æ›´æ–°é¡¹çš„åˆ†å­ä¸­æ·»åŠ äº†ä»¥å‰æ›´æ–°å€¼çš„æŒ‡æ•°åŠ æƒå¹³å‡çš„å¹³æ–¹æ ¹ï¼š
- en: '![](../Images/5d7d73b0fd9e37e9727fa810762f744a.png)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5d7d73b0fd9e37e9727fa810762f744a.png)'
- en: AdaDelta step for parameter update.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: AdaDelta å‚æ•°æ›´æ–°æ­¥éª¤ã€‚
- en: This is basically assuming that the loss function is smooth (low curvature)
    within a small window of size *w*, so that **Î”Î¸â‚œ** can be approximated by the
    exponential RMS of the previous values.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™åŸºæœ¬ä¸Šå‡è®¾æŸå¤±å‡½æ•°åœ¨å°çª—å£å¤§å° *w* å†…æ˜¯å¹³æ»‘çš„ï¼ˆæ›²ç‡ä½ï¼‰ï¼Œä»¥ä¾¿**Î”Î¸â‚œ** å¯ä»¥é€šè¿‡ä»¥å‰å€¼çš„æŒ‡æ•° RMS æ¥è¿‘ä¼¼ã€‚
- en: 'The algorithm looks like this if we implement it as a Python function:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæˆ‘ä»¬å°†å…¶å®ç°ä¸º Python å‡½æ•°ï¼Œç®—æ³•çœ‹èµ·æ¥æ˜¯è¿™æ ·çš„ï¼š
- en: '[PRE5]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: AdaDelta combines the advantages of the optimization methods it builds upon.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: AdaDelta ç»“åˆäº†å…¶æ‰€åŸºäºçš„ä¼˜åŒ–æ–¹æ³•çš„ä¼˜ç‚¹ã€‚
- en: For instance, the short-term memory of previous parameter updates in the numerator
    is similar to Momentum and has the effect of accelerating the gradient descent.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: ä¾‹å¦‚ï¼Œåˆ†å­ä¸­å‰æœŸå‚æ•°æ›´æ–°çš„çŸ­æœŸè®°å¿†ç±»ä¼¼äºåŠ¨é‡ï¼Œå¹¶å…·æœ‰åŠ é€Ÿæ¢¯åº¦ä¸‹é™çš„æ•ˆæœã€‚
- en: The denominator provides the per-dimension accuracy of AdaGrad but without the
    excessive learning rate decay (just like RMSProp).
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: åˆ†æ¯æä¾›äº† AdaGrad çš„æ¯ç»´åº¦å‡†ç¡®æ€§ï¼Œä½†æ²¡æœ‰è¿‡åº¦çš„å­¦ä¹ ç‡è¡°å‡ï¼ˆå°±åƒ RMSProp ä¸€æ ·ï¼‰ã€‚
- en: Additionally, AdaDelta is more robust to sudden gradient changes, and it is
    robust to the choice of initial learning rate (see a practical example in the
    last section of this article).
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤å¤–ï¼ŒAdaDelta å¯¹çªå‘çš„æ¢¯åº¦å˜åŒ–æ›´ä¸ºç¨³å¥ï¼Œå¯¹åˆå§‹å­¦ä¹ ç‡çš„é€‰æ‹©ä¹Ÿå¾ˆç¨³å¥ï¼ˆè¯·å‚è§æœ¬æ–‡æœ€åä¸€èŠ‚ä¸­çš„å®é™…ç¤ºä¾‹ï¼‰ã€‚
- en: Adam (Adaptive Momentum)
  id: totrans-159
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Adamï¼ˆè‡ªé€‚åº”åŠ¨é‡ï¼‰
- en: This is one of the most popular algorithms today.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯ä»Šå¤©æœ€æµè¡Œçš„ç®—æ³•ä¹‹ä¸€ã€‚
- en: It was introduced by [Diederik P. Kingma and Jimmy Lei Ba in 2014](https://arxiv.org/pdf/1412.6980.pdf?ref=makerluis.com),
    and has become very popular because of its computational efficiency and because
    it works very well for problems with large amounts of data and parameters.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: å®ƒç”±[è¿ªå¾·é‡Œå…‹Â·PÂ·é‡‘é©¬å’Œå‰ç±³Â·é›·Â·å·´äº 2014 å¹´æå‡º](https://arxiv.org/pdf/1412.6980.pdf?ref=makerluis.com)ï¼Œå¹¶å› å…¶è®¡ç®—æ•ˆç‡é«˜ä»¥åŠåœ¨å¤„ç†å¤§é‡æ•°æ®å’Œå‚æ•°çš„é—®é¢˜ä¸Šè¡¨ç°è‰¯å¥½è€Œå˜å¾—éå¸¸æµè¡Œã€‚
- en: Adam is like a combination of Momentum and RMSprop because it dynamically changes
    both the gradient of the loss function and the learning rates used to scale such
    gradient to update the weights.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: Adam ç±»ä¼¼äºåŠ¨é‡ï¼ˆMomentumï¼‰å’Œ RMSprop çš„ç»“åˆï¼Œå› ä¸ºå®ƒåŠ¨æ€åœ°æ”¹å˜äº†æŸå¤±å‡½æ•°çš„æ¢¯åº¦ä»¥åŠç”¨äºç¼©æ”¾è¿™äº›æ¢¯åº¦çš„å­¦ä¹ ç‡æ¥æ›´æ–°æƒé‡ã€‚
- en: To do this, the algorithm includes the computation of two terms that will be
    familiar from previous sections of this article.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºæ­¤ï¼Œç®—æ³•åŒ…æ‹¬è®¡ç®—ä¸¤ä¸ªåœ¨æœ¬æ–‡ä¹‹å‰éƒ¨åˆ†å·²ç»ç†Ÿæ‚‰çš„æœ¯è¯­ã€‚
- en: 'First, thereâ€™s a term from momentum, an exponentially weighted sum of the previous
    gradients of the cost function (this is like a weighted variance):'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: é¦–å…ˆï¼ŒåŠ¨é‡é¡¹æ˜¯æˆæœ¬å‡½æ•°å‰å‡ ä¸ªæ¢¯åº¦çš„æŒ‡æ•°åŠ æƒå’Œï¼ˆè¿™ç±»ä¼¼äºåŠ æƒæ–¹å·®ï¼‰ï¼š
- en: '![](../Images/4fcf8fc07a1a6b7e064c47aa116ae596.png)'
  id: totrans-165
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4fcf8fc07a1a6b7e064c47aa116ae596.png)'
- en: Exponentially weighted average of cost gradients.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: æˆæœ¬æ¢¯åº¦çš„æŒ‡æ•°åŠ æƒå¹³å‡ã€‚
- en: Then, thereâ€™s a term from RMSprop, an exponentially weighted moving average
    of the squared gradients.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åï¼Œæœ‰ä¸€ä¸ªæ¥è‡ª RMSprop çš„æœ¯è¯­ï¼Œæ˜¯å¹³æ–¹æ¢¯åº¦çš„æŒ‡æ•°åŠ æƒç§»åŠ¨å¹³å‡ã€‚
- en: '![](../Images/fb2aaf1105d8aebc521a41d67f9b5ed9.png)'
  id: totrans-168
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fb2aaf1105d8aebc521a41d67f9b5ed9.png)'
- en: Exponentially weighted average of squared cost gradients.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: æˆæœ¬æ¢¯åº¦çš„æŒ‡æ•°åŠ æƒå¹³å‡ã€‚
- en: Combining both terms with the SGD algorithm, the information from past gradients
    is included in the update step. Their total energy over a short window (RMS) is
    used to scale the learning rate, and their dispersion (variance) helps adjust
    the current gradient value used for updating the weights.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: å°†ä¸¤è€…ä¸ SGD ç®—æ³•ç»“åˆï¼Œè¿‡å»æ¢¯åº¦çš„ä¿¡æ¯è¢«çº³å…¥æ›´æ–°æ­¥éª¤ã€‚å®ƒä»¬åœ¨çŸ­çª—å£å†…çš„æ€»èƒ½é‡ï¼ˆRMSï¼‰ç”¨äºç¼©æ”¾å­¦ä¹ ç‡ï¼Œè€Œå®ƒä»¬çš„ç¦»æ•£åº¦ï¼ˆæ–¹å·®ï¼‰æœ‰åŠ©äºè°ƒæ•´ç”¨äºæ›´æ–°æƒé‡çš„å½“å‰æ¢¯åº¦å€¼ã€‚
- en: '![](../Images/e1ae4b44a6a1e7fb27694f23883149dc.png)'
  id: totrans-171
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e1ae4b44a6a1e7fb27694f23883149dc.png)'
- en: Adamâ€™s update rule.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: Adam çš„æ›´æ–°è§„åˆ™ã€‚
- en: 'The values with tilde (~) correspond to bias correction terms that are introduced
    to reduce the contribution from the initial values of m and v as learning progresses:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: å¸¦æœ‰æ³¢æµªå· (~) çš„å€¼å¯¹åº”äºå¼•å…¥çš„åå·®æ ¡æ­£é¡¹ï¼Œä»¥å‡å°‘å­¦ä¹ è¿‡ç¨‹ä¸­ m å’Œ v åˆå§‹å€¼çš„è´¡çŒ®ï¼š
- en: '![](../Images/d9d3e694982e67f5eb6551697682715d.png)'
  id: totrans-174
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d9d3e694982e67f5eb6551697682715d.png)'
- en: Initialization bias correction terms for Adam.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: Adam çš„åˆå§‹åŒ–åå·®æ ¡æ­£é¡¹ã€‚
- en: t = current training epoch.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: t = å½“å‰è®­ç»ƒè½®æ¬¡ã€‚
- en: Unlike AdaDelta, Adam does require tuning of some hyperparameters, but they
    are easy to interpret.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ AdaDelta ä¸åŒï¼ŒAdam ç¡®å®éœ€è¦è°ƒæ•´ä¸€äº›è¶…å‚æ•°ï¼Œä½†è¿™äº›å‚æ•°å¾ˆå®¹æ˜“è§£é‡Šã€‚
- en: The terms **Î²â‚** and **Î²â‚‚** are the decay rates of the [exponential moving averages](https://en.wikipedia.org/wiki/Exponential_smoothing)
    of the gradients and the squared gradients, respectively.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: '**Î²â‚** å’Œ **Î²â‚‚** æ˜¯æ¢¯åº¦å’Œå¹³æ–¹æ¢¯åº¦çš„ [æŒ‡æ•°ç§»åŠ¨å¹³å‡](https://en.wikipedia.org/wiki/Exponential_smoothing)
    çš„è¡°å‡ç‡ã€‚'
- en: Larger values will assign more weight to the previous gradients, giving a smoother
    behavior, and less reactive to recent changes. Values closer to zero give more
    weight to recent changes in the gradients. Typical values are **Î²â‚** = 0.9 and
    **Î²â‚‚** = 0.999.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: è¾ƒå¤§çš„å€¼ä¼šå¯¹å…ˆå‰çš„æ¢¯åº¦èµ‹äºˆæ›´å¤šæƒé‡ï¼Œè¡¨ç°æ›´å¹³æ»‘ï¼Œå¯¹æœ€è¿‘çš„å˜åŒ–ååº”è¾ƒå°‘ã€‚æ¥è¿‘é›¶çš„å€¼ä¼šèµ‹äºˆæœ€è¿‘æ¢¯åº¦å˜åŒ–æ›´å¤šæƒé‡ã€‚å…¸å‹å€¼ä¸º**Î²â‚** = 0.9 å’Œ**Î²â‚‚**
    = 0.999ã€‚
- en: '**Îµ** is, like in all previous cases, a constant added to avoid division by
    zero, usually set to 1e-8.'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: '**Îµ** ä¸æ‰€æœ‰å‰è¿°æƒ…å†µä¸€æ ·ï¼Œæ˜¯ä¸€ä¸ªå¸¸æ•°ï¼Œæ·»åŠ ä»¥é¿å…é™¤ä»¥é›¶ï¼Œé€šå¸¸è®¾ä¸º 1e-8ã€‚'
- en: 'Despite having a bunch of additional terms, and significant advantages, Adam
    is easy to implement:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: å°½ç®¡æœ‰è®¸å¤šé™„åŠ é¡¹å’Œæ˜¾è‘—çš„ä¼˜åŠ¿ï¼ŒAdam å®ç°èµ·æ¥éå¸¸ç®€å•ï¼š
- en: '[PRE6]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Interestingly, the authors of the paper point out that the term
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: æœ‰è¶£çš„æ˜¯ï¼Œè®ºæ–‡çš„ä½œè€…æŒ‡å‡ºäº†è¿™ä¸ªæœ¯è¯­
- en: '![](../Images/e4792c1a703ee073bf8177f34fe9d998.png)'
  id: totrans-184
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e4792c1a703ee073bf8177f34fe9d998.png)'
- en: Adamâ€™s learning rate scaling.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: Adam çš„å­¦ä¹ ç‡ç¼©æ”¾ã€‚
- en: '*resembles* the definition of the [Signal-to-Noise Ratio (SNR)](https://en.wikipedia.org/wiki/Signal-to-noise_ratio#Definition):'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: '*ç±»ä¼¼äº* [ä¿¡å™ªæ¯” (SNR)](https://en.wikipedia.org/wiki/Signal-to-noise_ratio#Definition)
    çš„å®šä¹‰ï¼š'
- en: '![](../Images/33d9ff2497e564d37438da78b5661d8e.png)'
  id: totrans-187
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/33d9ff2497e564d37438da78b5661d8e.png)'
- en: Signal-to-Noise ratio.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: ä¿¡å™ªæ¯”ã€‚
- en: Then, we could say that for smaller SNR values, the parameter updates will be
    close to zero. This means that we wonâ€™t perform large updates whenever thereâ€™s
    too much uncertainty about whether we are moving in the direction of the true
    gradient.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åï¼Œæˆ‘ä»¬å¯ä»¥è¯´ï¼Œå¯¹äºè¾ƒå°çš„ SNR å€¼ï¼Œå‚æ•°æ›´æ–°å°†æ¥è¿‘é›¶ã€‚è¿™æ„å‘³ç€å½“å¯¹æ˜¯å¦æœç€çœŸå®æ¢¯åº¦æ–¹å‘ç§»åŠ¨å­˜åœ¨å¤ªå¤šä¸ç¡®å®šæ€§æ—¶ï¼Œæˆ‘ä»¬ä¸ä¼šè¿›è¡Œå¤§å¹…æ›´æ–°ã€‚
- en: Adam and its variants typically outperform the other algorithms when training
    DL models, especially when there are very noisy and sparse gradients.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: Adam åŠå…¶å˜ä½“åœ¨è®­ç»ƒæ·±åº¦å­¦ä¹ æ¨¡å‹æ—¶é€šå¸¸ä¼˜äºå…¶ä»–ç®—æ³•ï¼Œç‰¹åˆ«æ˜¯åœ¨æ¢¯åº¦éå¸¸å˜ˆæ‚å’Œç¨€ç–çš„æƒ…å†µä¸‹ã€‚
- en: Performance for different learning rates
  id: totrans-191
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ä¸åŒå­¦ä¹ ç‡çš„æ€§èƒ½è¡¨ç°
- en: I decided to compare how the different optimizers perform when initialized with
    different â€œglobalâ€ learning rates.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘å†³å®šæ¯”è¾ƒä¸åŒä¼˜åŒ–å™¨åœ¨ä¸åŒâ€œå…¨å±€â€å­¦ä¹ ç‡ä¸‹çš„è¡¨ç°ã€‚
- en: This is a rather simple example, but it gives an idea of how these methods are
    affected by the choice of learning rate.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯ä¸€ä¸ªç›¸å½“ç®€å•çš„ä¾‹å­ï¼Œä½†å®ƒæä¾›äº†è¿™äº›æ–¹æ³•å¦‚ä½•å—åˆ°å­¦ä¹ ç‡é€‰æ‹©å½±å“çš„ä¸€ä¸ªæ¦‚å¿µã€‚
- en: '![](../Images/d5413170b1b8d1f0cde2cb54ba44551c.png)'
  id: totrans-194
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d5413170b1b8d1f0cde2cb54ba44551c.png)'
- en: Comparing the evolution of x and y coordinates during optimization with different
    algorithms. For Momentum and NAG, mu = 0.95\. For RMSProp and AdaDelta, decay
    parameter = 0.9.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: æ¯”è¾ƒä¸åŒç®—æ³•ä¼˜åŒ–è¿‡ç¨‹ä¸­ x å’Œ y åæ ‡çš„æ¼”å˜ã€‚å¯¹äº Momentum å’Œ NAGï¼Œmu = 0.95ã€‚å¯¹äº RMSProp å’Œ AdaDeltaï¼Œè¡°å‡å‚æ•°
    = 0.9ã€‚
- en: AdaDelta seems remarkably robust to the global learning rate setting, and it
    â€œdescendsâ€ at the same rate for all three cases. We can also see how AdaGrad requires
    larger learning rates to achieve a performance comparable to AdaDelta in this
    case.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: AdaDelta å¯¹å…¨å±€å­¦ä¹ ç‡è®¾ç½®ä¼¼ä¹éå¸¸é²æ£’ï¼Œåœ¨æ‰€æœ‰ä¸‰ç§æƒ…å†µä¸‹â€œä¸‹é™â€é€Ÿç‡ç›¸åŒã€‚æˆ‘ä»¬è¿˜å¯ä»¥çœ‹åˆ°ï¼Œåœ¨è¿™ç§æƒ…å†µä¸‹ï¼ŒAdaGrad éœ€è¦è¾ƒå¤§çš„å­¦ä¹ ç‡æ‰èƒ½å®ç°ä¸
    AdaDelta ç›¸å½“çš„æ€§èƒ½ã€‚
- en: For small learning rates, it is clear that Adam and RMSProp are similar, and
    superior to Momentum and SGD.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºè¾ƒå°çš„å­¦ä¹ ç‡ï¼ŒAdam å’Œ RMSProp æ˜¾ç„¶ç›¸ä¼¼ï¼Œå¹¶ä¸”ä¼˜äº Momentum å’Œ SGDã€‚
- en: However, for larger learning rates, RMSProp shows consistent oscillations around
    the optimum x value (x = 0), while Adam stabilizes after the initial transient,
    thanks to the dampening effect of the momentum term in the numerator.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶è€Œï¼Œå¯¹äºè¾ƒå¤§çš„å­¦ä¹ ç‡ï¼ŒRMSProp åœ¨æœ€ä¼˜ x å€¼ï¼ˆx = 0ï¼‰å‘¨å›´è¡¨ç°å‡ºä¸€è‡´çš„æŒ¯è¡ï¼Œè€Œ Adam åœ¨åˆå§‹ç¬æ€åç¨³å®šä¸‹æ¥ï¼Œè¿™å¾—ç›Šäºåˆ†å­ä¸­åŠ¨é‡é¡¹çš„é˜»å°¼æ•ˆåº”ã€‚
- en: The adaptive algorithms break off the symmetry earlier than SGD and Momentum
    methods, except for the case with a global learning rate of 0.1 for which Momentum
    and NAG outperform AdaDelta.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: è‡ªé€‚åº”ç®—æ³•æ¯” SGD å’Œ Momentum æ–¹æ³•æ›´æ—©æ‰“ç ´å¯¹ç§°æ€§ï¼Œé™¤äº†å…¨å±€å­¦ä¹ ç‡ä¸º 0.1 çš„æƒ…å†µï¼Œæ­¤æ—¶ Momentum å’Œ NAG ä¼˜äº AdaDeltaã€‚
- en: Again, these observations only apply to this particular scenario.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: å†æ¬¡å¼ºè°ƒï¼Œè¿™äº›è§‚å¯Ÿç»“æœä»…é€‚ç”¨äºç‰¹å®šçš„åœºæ™¯ã€‚
- en: Conclusions
  id: totrans-201
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç»“è®º
- en: The advantages of these optimization algorithms are not fully evident when we
    apply them to a simple function, like the saddle point example above.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: å½“æˆ‘ä»¬å°†è¿™äº›ä¼˜åŒ–ç®—æ³•åº”ç”¨äºç®€å•å‡½æ•°æ—¶ï¼Œå¦‚ä¸Šè¿°çš„éç‚¹ä¾‹å­ï¼Œå®ƒä»¬çš„ä¼˜ç‚¹å¹¶ä¸å®Œå…¨æ˜¾ç°ã€‚
- en: For other scenarios with small-scale models or datasets, even SGD may work better,
    so it is important to understand where each type of optimizer works best.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºå…¶ä»–å°è§„æ¨¡æ¨¡å‹æˆ–æ•°æ®é›†çš„åœºæ™¯ï¼Œå³ä½¿æ˜¯ SGD ä¹Ÿå¯èƒ½æ•ˆæœæ›´å¥½ï¼Œå› æ­¤ç†è§£æ¯ç§ä¼˜åŒ–å™¨æœ€ä½³å·¥ä½œæ¡ä»¶æ˜¯é‡è¦çš„ã€‚
- en: When training neural networks, we optimize the Loss function, and we donâ€™t have
    an exact value of its gradient at any point, just an estimation of it. This is
    why methods like Adam and AdaDelta, which are robust to noise and sparsity in
    the gradients have been used widely in the Data Science community.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è®­ç»ƒç¥ç»ç½‘ç»œæ—¶ï¼Œæˆ‘ä»¬ä¼˜åŒ–æŸå¤±å‡½æ•°ï¼Œè€Œåœ¨ä»»ä½•æ—¶åˆ»æˆ‘ä»¬æ²¡æœ‰å…¶æ¢¯åº¦çš„ç¡®åˆ‡å€¼ï¼Œä»…æœ‰å¯¹å…¶çš„ä¼°è®¡ã€‚è¿™å°±æ˜¯ä¸ºä»€ä¹ˆåƒ Adam å’Œ AdaDelta è¿™æ ·çš„å¯¹æ¢¯åº¦ä¸­çš„å™ªå£°å’Œç¨€ç–æ€§å…·æœ‰é²æ£’æ€§çš„ç®—æ³•åœ¨æ•°æ®ç§‘å­¦ç¤¾åŒºä¸­è¢«å¹¿æ³›ä½¿ç”¨ã€‚
- en: Also, we can deal with a large amount of model weights, instead of just x and
    y coordinates. In these scenarios, the ability to obtain per-parameter learning
    rates is beneficial.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤å¤–ï¼Œæˆ‘ä»¬å¯ä»¥å¤„ç†å¤§é‡çš„æ¨¡å‹æƒé‡ï¼Œè€Œä¸ä»…ä»…æ˜¯ x å’Œ y åæ ‡ã€‚åœ¨è¿™äº›æƒ…å†µä¸‹ï¼Œè·å–æ¯ä¸ªå‚æ•°çš„å­¦ä¹ ç‡çš„èƒ½åŠ›æ˜¯æœ‰ç›Šçš„ã€‚
- en: In a future post, Iâ€™ll show a more realistic comparison of these methods in
    another article, using an artificial neural network.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœªæ¥çš„æ–‡ç« ä¸­ï¼Œæˆ‘å°†å±•ç¤ºè¿™äº›æ–¹æ³•åœ¨å¦ä¸€ç¯‡æ–‡ç« ä¸­çš„æ›´ç°å®çš„æ¯”è¾ƒï¼Œä½¿ç”¨äººå·¥ç¥ç»ç½‘ç»œã€‚
- en: Further reading
  id: totrans-207
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: è¿›ä¸€æ­¥é˜…è¯»
- en: '[DL Notes: Gradient Descent](https://medium.com/@luisdamed/gradient-descent-f09f19eb35fb)'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[DL Notes: æ¢¯åº¦ä¸‹é™](https://medium.com/@luisdamed/gradient-descent-f09f19eb35fb)'
- en: '[DL Notes: Feed Forwards Artificial Neural Networks](https://medium.com/@luisdamed/feedforward-artificial-neural-networks-52bcf96d6ac3)'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[DL Notes: å‰é¦ˆäººå·¥ç¥ç»ç½‘ç»œ](https://medium.com/@luisdamed/feedforward-artificial-neural-networks-52bcf96d6ac3)'
- en: '[Creating a Gradient Descent Animation in Python (Towards Data Science)](/creating-a-gradient-descent-animation-in-python-3c4dcd20ca51?ref=makerluis.com)'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[åœ¨ Python ä¸­åˆ›å»ºæ¢¯åº¦ä¸‹é™åŠ¨ç”»ï¼ˆæ•°æ®ç§‘å­¦çš„å‰æ²¿ï¼‰](/creating-a-gradient-descent-animation-in-python-3c4dcd20ca51?ref=makerluis.com)'
- en: References
  id: totrans-211
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å‚è€ƒæ–‡çŒ®
- en: '**All figures, unless otherwise noted are created by the Author.**'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: '**æ‰€æœ‰å›¾ç¤ºï¼Œé™¤éå¦æœ‰è¯´æ˜ï¼Œå‡ç”±ä½œè€…åˆ›å»ºã€‚**'
- en: '[1] Online course [A Deep Understanding of Deep Learning](https://www.udemy.com/course/deeplearning_x/?ref=makerluis.com),
    by Mike X Cohen ( [sincxpress.com](https://sincxpress.com/?ref=makerluis.com))'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] åœ¨çº¿è¯¾ç¨‹ [æ·±å…¥ç†è§£æ·±åº¦å­¦ä¹ ](https://www.udemy.com/course/deeplearning_x/?ref=makerluis.com)ï¼Œç”±
    Mike X Cohen ä¸»è®²ï¼ˆ[sincxpress.com](https://sincxpress.com/?ref=makerluis.com)ï¼‰'
- en: '[2] [Standford Online: CS231 Convolutional Neural Networks for Visual Recognition](https://cs231n.github.io/neural-networks-3?ref=makerluis.com)'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] [æ–¯å¦ç¦åœ¨çº¿ï¼šCS231 å·ç§¯ç¥ç»ç½‘ç»œè¿›è¡Œè§†è§‰è¯†åˆ«](https://cs231n.github.io/neural-networks-3?ref=makerluis.com)'
- en: '[3] Goh. â€œWhy Momentum Really Worksâ€, Distill, 2017\. [http://doi.org/10.23915/distill.00006](http://doi.org/10.23915/distill.00006?ref=makerluis.com)'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] Goh. â€œä¸ºä»€ä¹ˆ Momentum çœŸæ­£æœ‰æ•ˆâ€ï¼ŒDistillï¼Œ2017\. [http://doi.org/10.23915/distill.00006](http://doi.org/10.23915/distill.00006?ref=makerluis.com)'
- en: '[4] Villalarga, D. â€œ [AdaGrad](https://optimization.cbe.cornell.edu/index.php?title=AdaGrad&ref=makerluis.com)
    â€œ. Published in Cornell University Computational Optimization Open Textbook â€”
    Optimization Wiki.'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] Villalarga, D. â€œ[AdaGrad](https://optimization.cbe.cornell.edu/index.php?title=AdaGrad&ref=makerluis.com)â€ã€‚å‘å¸ƒäºåº·å¥ˆå°”å¤§å­¦è®¡ç®—ä¼˜åŒ–å¼€æ”¾æ•™æ
    â€” ä¼˜åŒ–ç»´åŸºã€‚'
- en: '[5] Bengio, Yoshua. â€œPractical recommendations for gradient-based training
    of deep architectures.â€ *Neural Networks: Tricks of the Trade: Second Edition*.
    Berlin, Heidelberg: Springer Berlin Heidelberg, 437â€“478, 2012\. Online: [arXiv:1206.5533](https://arxiv.org/abs/1206.5533?ref=makerluis.com)
    [cs.LG]'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: '[5] Bengio, Yoshua. â€œæ·±åº¦æ¶æ„çš„æ¢¯åº¦è®­ç»ƒçš„å®é™…å»ºè®®ã€‚â€ *ç¥ç»ç½‘ç»œï¼šå®ç”¨æŠ€å·§ï¼šç¬¬äºŒç‰ˆ*ã€‚æŸæ—ï¼Œæµ·å¾·å ¡ï¼šSpringer Berlin
    Heidelberg, 437â€“478, 2012\. åœ¨çº¿: [arXiv:1206.5533](https://arxiv.org/abs/1206.5533?ref=makerluis.com)
    [cs.LG]'
- en: '[6] Sutskever, I., Martens, J., Dahl, G. & Hinton, G. â€œOn the importance of
    initialization and momentum in deep learningâ€. *Proceedings of Machine Learning
    Research,* 28(3):1139â€“1147, 2013\. Available from [https://proceedings.mlr.press/v28/sutskever13.html](https://proceedings.mlr.press/v28/sutskever13.html?ref=makerluis.com).'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: '[6] Sutskever, I., Martens, J., Dahl, G. & Hinton, G. â€œåœ¨æ·±åº¦å­¦ä¹ ä¸­åˆå§‹åŒ–å’ŒåŠ¨é‡çš„é‡è¦æ€§â€ã€‚*æœºå™¨å­¦ä¹ ç ”ç©¶ä¼šè®®è®ºæ–‡é›†,*
    28(3):1139â€“1147, 2013\. ç½‘å€: [https://proceedings.mlr.press/v28/sutskever13.html](https://proceedings.mlr.press/v28/sutskever13.html?ref=makerluis.com).'
- en: '[7] Duchi, J., Hazan, E., Singer, Y., â€œAdaptive Subgradient Methods for Online
    Learning and Stochastic Optimizationâ€. *Journal of Machine Learning Research,*
    12(61):2121âˆ’2159, 2011\. Available from: [https://jmlr.org/papers/v12/duchi11a.html](https://jmlr.org/papers/v12/duchi11a.html?ref=makerluis.com)'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: '[7] Duchi, J., Hazan, E., Singer, Y., â€œåœ¨çº¿å­¦ä¹ å’Œéšæœºä¼˜åŒ–çš„è‡ªé€‚åº”å­æ¢¯åº¦æ–¹æ³•â€ã€‚*æœºå™¨å­¦ä¹ ç ”ç©¶æ‚å¿—,* 12(61):2121âˆ’2159,
    2011\. ç½‘å€: [https://jmlr.org/papers/v12/duchi11a.html](https://jmlr.org/papers/v12/duchi11a.html?ref=makerluis.com)'
- en: '[8] Jason Brownlee, [Gradient Descent With AdaGrad From Scratch](https://machinelearningmastery.com/gradient-descent-with-adagrad-from-scratch/?ref=makerluis.com).
    2021'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: '[8] Jason Brownlee, [ä»é›¶å¼€å§‹çš„AdaGradæ¢¯åº¦ä¸‹é™](https://machinelearningmastery.com/gradient-descent-with-adagrad-from-scratch/?ref=makerluis.com)ã€‚2021'
- en: '[9] Zeiler, M. â€œADADELTA: AN ADAPTIVE LEARNING RATE METHODâ€, 2012\. [arXiv:1212.5701v1](https://arxiv.org/abs/1212.5701v1?ref=makerluis.com)
    [cs.LG]'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: '[9] Zeiler, M. â€œADADELTA: ä¸€ç§è‡ªé€‚åº”å­¦ä¹ ç‡æ–¹æ³•â€ï¼Œ2012\. [arXiv:1212.5701v1](https://arxiv.org/abs/1212.5701v1?ref=makerluis.com)
    [cs.LG]'
- en: '[10] Kingma, D., Ba, J. â€œAdam: A Method for Stochastic Optimizationâ€, 2014\.
    [arXiv:1412.6980](https://arxiv.org/abs/1412.6980?ref=makerluis.com) [cs.LG]'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: '[10] Kingma, D., Ba, J. â€œAdam: ä¸€ç§éšæœºä¼˜åŒ–æ–¹æ³•â€ï¼Œ2014\. [arXiv:1412.6980](https://arxiv.org/abs/1412.6980?ref=makerluis.com)
    [cs.LG]'
- en: '*Originally published at* [*https://www.makerluis.com*](https://www.makerluis.com/dl-notes-advanced-gradient-descent/)
    *on December 5, 2023.*'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: '*æœ€åˆå‘å¸ƒäº* [*https://www.makerluis.com*](https://www.makerluis.com/dl-notes-advanced-gradient-descent/)
    *2023å¹´12æœˆ5æ—¥ã€‚*'
