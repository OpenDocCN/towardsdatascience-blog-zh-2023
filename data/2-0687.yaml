- en: Decoding Strategies in Large Language Models
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 大型语言模型中的解码策略
- en: 原文：[https://towardsdatascience.com/decoding-strategies-in-large-language-models-9733a8f70539](https://towardsdatascience.com/decoding-strategies-in-large-language-models-9733a8f70539)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/decoding-strategies-in-large-language-models-9733a8f70539](https://towardsdatascience.com/decoding-strategies-in-large-language-models-9733a8f70539)
- en: A Guide to Text Generation From Beam Search to Nucleus Sampling
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从束搜索到核采样的文本生成指南
- en: '[](https://medium.com/@mlabonne?source=post_page-----9733a8f70539--------------------------------)[![Maxime
    Labonne](../Images/a7efdd305e3cc77d5509bbb1076d57d8.png)](https://medium.com/@mlabonne?source=post_page-----9733a8f70539--------------------------------)[](https://towardsdatascience.com/?source=post_page-----9733a8f70539--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----9733a8f70539--------------------------------)
    [Maxime Labonne](https://medium.com/@mlabonne?source=post_page-----9733a8f70539--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@mlabonne?source=post_page-----9733a8f70539--------------------------------)[![Maxime
    Labonne](../Images/a7efdd305e3cc77d5509bbb1076d57d8.png)](https://medium.com/@mlabonne?source=post_page-----9733a8f70539--------------------------------)[](https://towardsdatascience.com/?source=post_page-----9733a8f70539--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----9733a8f70539--------------------------------)
    [马克西姆·拉博讷](https://medium.com/@mlabonne?source=post_page-----9733a8f70539--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----9733a8f70539--------------------------------)
    ·15 min read·Jun 4, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----9733a8f70539--------------------------------)
    ·阅读时间15分钟·2023年6月4日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/8e3084bd4e009d7fb0c6ec5d7aef4aa6.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8e3084bd4e009d7fb0c6ec5d7aef4aa6.png)'
- en: Image by author.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供。
- en: In the fascinating world of large language models (LLMs), much attention is
    given to model architectures, data processing, and optimization. However, decoding
    strategies like beam search, which play a crucial role in text generation, are
    often overlooked. In this article, we will explore how LLMs generate text by delving
    into the mechanics of greedy search and beam search, as well as sampling techniques
    with top-k and nucleus sampling.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在大型语言模型（LLM）的迷人世界中，很多关注都集中在模型架构、数据处理和优化上。然而，像束搜索这样的解码策略，在文本生成中发挥着至关重要的作用，却常常被忽视。在本文中，我们将探讨LLM如何生成文本，通过深入了解贪婪搜索和束搜索的机制，以及使用top-k和nucleus采样的采样技术。
- en: By the conclusion of this article, you’ll not only understand these decoding
    strategies thoroughly but also be familiar with how to handle important hyperparameters
    like temperature, num_beams, top_k, and top_p.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 到本文结束时，你不仅会彻底理解这些解码策略，还会熟悉如何处理重要的超参数，如温度、num_beams、top_k和top_p。
- en: The code for this article can be found on [GitHub](https://github.com/mlabonne/llm-course/blob/main/Decoding_Strategies_in_Large_Language%C2%A0Models.ipynb)
    and [Google Colab](https://colab.research.google.com/drive/19CJlOS5lI29g-B3dziNn93Enez1yiHk2?usp=sharing)
    for reference and further exploration.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 本文的代码可以在 [GitHub](https://github.com/mlabonne/llm-course/blob/main/Decoding_Strategies_in_Large_Language%C2%A0Models.ipynb)
    和 [Google Colab](https://colab.research.google.com/drive/19CJlOS5lI29g-B3dziNn93Enez1yiHk2?usp=sharing)
    上找到，以供参考和进一步探索。
- en: 📚 Background
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 📚 背景
- en: To kick things off, let’s start with an example. We’ll feed the text “I have
    a dream” to a GPT-2 model and ask it to generate the next five tokens (words or
    subwords).
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 为了开始，让我们从一个例子开始。我们将“我有一个梦想”这个文本输入到GPT-2模型中，并要求它生成接下来的五个标记（单词或子单词）。
- en: '[PRE0]'
  id: totrans-13
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '[PRE1]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The sentence “I have a dream of being a doctor” appears to have been generated
    by GPT-2\. However, GPT-2 didn’t *exactly* produce this sentence.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 句子“我有一个成为医生的梦想”似乎是由GPT-2生成的。然而，GPT-2并没有*准确*生成这个句子。
- en: 'There’s a common misconception that LLMs like GPT-2 **directly produce text**.
    This isn’t the case. Instead, LLMs calculate logits, which are scores assigned
    to every possible token in their vocabulary. To simplify, here’s an illustrative
    breakdown of the process:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 有一种普遍的误解是，像GPT-2这样的LLM **直接生成文本**。事实并非如此。相反，LLM计算logits，即分配给其词汇表中每个可能标记的分数。为了简化，这里有一个说明性过程分解：
- en: '![](../Images/27050515483e3936d16fe97747ec1884.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/27050515483e3936d16fe97747ec1884.png)'
- en: Image by author.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供。
- en: The tokenizer, [Byte-Pair Encoding](https://en.wikipedia.org/wiki/Byte_pair_encoding)
    in this instance, translates each token in the input text into a corresponding
    token ID. Then, GPT-2 uses these token IDs as input and tries to predict the next
    most likely token. Finally, the model generates logits, which are converted into
    probabilities using a softmax function.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 分词器，[字节对编码](https://en.wikipedia.org/wiki/Byte_pair_encoding) 在这个实例中，将输入文本中的每个词翻译成对应的词
    ID。然后，GPT-2 使用这些词 ID 作为输入并尝试预测下一个最可能的词。最后，模型生成 logits，这些 logits 通过 softmax 函数转换为概率。
- en: For example, the model assigns a probability of 17% to the token for “of” being
    the next token after “I have a dream”. This output essentially represents a ranked
    list of potential next tokens in the sequence. More formally, we denote this probability
    as *P(of | I have a dream) = 17%*.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，该模型为“of”作为 “I have a dream” 之后的下一个词分配了 17% 的概率。这个输出本质上表示了潜在下一个词的排名列表。更正式地，我们将这个概率表示为
    *P(of | I have a dream) = 17%*。
- en: 'Autoregressive models like GPT predict the next token in a sequence based on
    the preceding tokens. Consider a sequence of tokens *w = (w*₁*, w*₂*, …, w*ₜ*)*.
    The joint probability of this sequence *P(w)* can be broken down as:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 自回归模型如 GPT 会基于前面的词预测序列中的下一个词。考虑一个词序列 *w = (w*₁*, w*₂*, …, w*ₜ*)*。这个序列的联合概率 *P(w)*
    可以被分解为：
- en: '![](../Images/4b65c8dd18074c1330b34b2d9bbb8250.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4b65c8dd18074c1330b34b2d9bbb8250.png)'
- en: For each token *wᵢ* in the sequence, *P(wᵢ | w₁, w₂, …, wᵢ₋₁)* represents the
    conditional probability of *wᵢ* given all the preceding tokens (*w₁, w₂, …, wᵢ₋₁*).
    GPT-2 calculates this conditional probability for each of the 50,257 tokens in
    its vocabulary.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 对于序列中的每个词 *wᵢ*，*P(wᵢ | w₁, w₂, …, wᵢ₋₁)* 表示在给定所有前面的词（*w₁, w₂, …, wᵢ₋₁*）的情况下
    *wᵢ* 的条件概率。GPT-2 为其词汇表中的每一个 50,257 个词计算这个条件概率。
- en: 'This leads to the question: how do we use these probabilities to generate text?
    This is where decoding strategies, such as greedy search and beam search, come
    into play.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 这就引出了一个问题：我们如何利用这些概率生成文本？这就是解码策略（如贪婪搜索和束搜索）发挥作用的地方。
- en: 🏃‍♂️ Greedy Search
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 🏃‍♂️ 贪婪搜索
- en: 'Greedy search is a decoding method that takes the most probable token at each
    step as the next token in the sequence. To put it simply, it only retains the
    most likely token at each stage, discarding all other potential options. Using
    our example:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 贪婪搜索是一种解码方法，它在每一步都选择最可能的词作为序列中的下一个词。简单来说，它只保留每个阶段中最可能的词，丢弃所有其他潜在的选项。以我们的例子为例：
- en: '**Step 1**: Input: “I have a dream” → Most likely token: “ of”'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**步骤 1**：输入：“I have a dream” → 最可能的词： “ of”'
- en: '**Step 2**: Input: “I have a dream of” → Most likely token: “ being”'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**步骤 2**：输入：“I have a dream of” → 最可能的词： “ being”'
- en: '**Step 3**: Input: “I have a dream of being” → Most likely token: “ a”'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**步骤 3**：输入：“I have a dream of being” → 最可能的词： “ a”'
- en: '**Step 4**: Input: “I have a dream of being a” → Most likely token: “ doctor”'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**步骤 4**：输入：“I have a dream of being a” → 最可能的词： “ doctor”'
- en: '**Step 5**: Input: “I have a dream of being a doctor” → Most likely token:
    “.”'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**步骤 5**：输入：“I have a dream of being a doctor” → 最可能的词： “.”'
- en: 'While this approach might sound intuitive, it’s important to note that the
    greedy search is short-sighted: it only considers the most probable token at each
    step without considering the overall effect on the sequence. This property makes
    it fast and efficient as it doesn’t need to keep track of multiple sequences,
    but it also means that it can miss out on better sequences that might have appeared
    with slightly less probable next tokens.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这种方法听起来直观，但需要注意的是，贪婪搜索具有短视性：它只考虑每一步中最可能的词，而不考虑对整个序列的总体影响。这一特性使得它快速且高效，因为它不需要跟踪多个序列，但也意味着它可能错过了那些通过稍微不那么可能的下一个词可能出现的更好序列。
- en: Next, let’s illustrate the greedy search implementation using graphviz and networkx.
    We select the ID with the highest score, compute its log probability (we take
    the log to simplify calculations), and add it to the tree. We’ll repeat this process
    for five tokens.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们使用 graphviz 和 networkx 来说明贪婪搜索的实现。我们选择得分最高的 ID，计算其对数概率（我们取对数以简化计算），并将其添加到树中。我们将为五个词重复这个过程。
- en: '[PRE2]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '[PRE3]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Our greedy search generates the same text as the one from the transformers
    library: “I have a dream of being a doctor.” Let’s visualize the tree we created.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的贪婪搜索生成的文本与 transformers 库中的文本相同：“I have a dream of being a doctor。”让我们可视化我们创建的树。
- en: '[PRE4]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '![](../Images/dcb58f405a250824dbd57e3f478bfd51.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/dcb58f405a250824dbd57e3f478bfd51.png)'
- en: Image by author.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片。
- en: In this graph, the top node stores the input token (thus with a 100% probability),
    while all other nodes represent generated tokens. Although each token in this
    sequence was the most likely at the time of prediction, “being” and “doctor” were
    assigned relatively low probabilities of 9.68% and 2.86%, respectively. This suggests
    that “of”, our first predicted token, may not have been the most suitable choice
    as it led to “being”, which is quite unlikely.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在此图中，顶级节点存储输入令牌（因此概率为100%），而所有其他节点表示生成的令牌。虽然这个序列中的每个令牌在预测时都是最可能的，但“being”和“doctor”被分配了相对较低的概率，分别为9.68%和2.86%。这表明“of”，我们的第一个预测令牌，可能不是最合适的选择，因为它导致了“being”，这是相当不可能的。
- en: In the following section, we’ll explore how beam search can address this problem.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的部分中，我们将探讨光束搜索如何解决这个问题。
- en: ⚖️ Beam Search
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ⚖️ Beam Search
- en: Unlike greedy search, which only considers the next most probable token, beam
    search takes into account the *n* most likely tokens, where *n* represents the
    number of beams. This procedure is repeated until a predefined maximum length
    is reached or an end-of-sequence token appears. At this point, the sequence (or
    “beam”) with the highest overall score is chosen as the output.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 与仅考虑下一个最可能令牌的贪婪搜索不同，光束搜索考虑了*n*个最可能的令牌，其中*n*代表光束的数量。这个过程会重复，直到达到预定义的最大长度或出现序列结束令牌。此时，具有最高总体分数的序列（或“光束”）被选为输出。
- en: We can adapt the previous function to consider the *n* most probable tokens
    instead of just one. Here, we’ll maintain the sequence score log *P(w)*, which
    is the cumulative sum of the log probability of every token in the beam. We normalize
    this score by the sequence length to prevent bias towards longer sequences (this
    factor can be adjusted). Once again, we’ll generate five additional tokens to
    complete the sentence “I have a dream.”
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以调整之前的函数，以考虑*n*个最可能的令牌，而不仅仅是一个。在这里，我们将保持序列分数日志*P(w)*，它是光束中每个令牌对数概率的累积和。我们通过序列长度来归一化此分数，以防止对较长序列的偏倚（这个因子可以调整）。我们将生成五个额外的令牌来完成句子“I
    have a dream.”
- en: '[PRE5]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The function computes the scores for 63 tokens and beams^length = 5² = 25 possible
    sequences. In our implementation, all the information is stored in the graph.
    Our next step is to extract the best sequence.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 该函数计算63个令牌的分数和beams^length = 5² = 25个可能序列。在我们的实现中，所有信息都存储在图表中。我们的下一步是提取最佳序列。
- en: 'First, we identify the leaf node with the highest sequence score. Next, we
    find the shortest path from the root to this leaf. Every node along this path
    contains a token from the optimal sequence. Here’s how we can implement it:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们确定具有最高序列分数的叶节点。接下来，我们找到从根到这个叶子节点的最短路径。沿着这条路径的每个节点都包含了最优序列中的一个令牌。以下是我们如何实现它：
- en: '[PRE6]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '[PRE7]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: The best sequence seems to be “I have a dream. I have a dream,” which is a common
    response from GPT-2, even though it may be surprising. To verify this, let’s plot
    the graph.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 最佳序列似乎是“I have a dream. I have a dream”，这是GPT-2的常见响应，尽管这可能令人惊讶。为了验证这一点，让我们绘制图表。
- en: In this visualization, we’ll display the sequence score for each node, which
    represents the score of the sequence up to that point. If the function get_best_sequence()
    is correct, the “dream” node in the sequence “I have a dream. I have a dream”
    should have the highest score among all the leaf nodes.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个可视化中，我们将显示每个节点的序列分数，这代表了到该点为止的序列分数。如果函数get_best_sequence()是正确的，则序列“I have
    a dream. I have a dream”中的“dream”节点应该在所有叶节点中具有最高分数。
- en: '[PRE8]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '![](../Images/64c8bcbca63a9847c6a66c247c0f25ce.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/64c8bcbca63a9847c6a66c247c0f25ce.png)'
- en: Indeed, the “dream” token has the **highest sequence score** with a value of
    -0.69\. Interestingly, we can see the score of the greedy sequence “I have a dream
    of being a doctor.” on the left with a value of -1.16.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 确实，“dream”令牌具有**最高序列分数**，值为-0.69。令人感兴趣的是，我们可以看到左侧贪婪序列“I have a dream of being
    a doctor.”的分数值为-1.16。
- en: 'As expected, the greedy search leads to suboptimal results. But, to be honest,
    our new outcome is not particularly compelling either. To generate more varied
    sequences, we’ll implement two sampling algorithms: top-k and nucleus.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 正如预期的那样，贪婪搜索导致了次优结果。但是，老实说，我们的新结果也没有特别引人注目。为了生成更多样化的序列，我们将实现两种采样算法：top-k和nucleus。
- en: 🎲 Top-k sampling
  id: totrans-56
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 🎲 Top-k采样
- en: Top-k sampling is a technique that leverages the probability distribution generated
    by the language model to **select a token randomly from the *k* most likely options**.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: Top-k采样是一种利用语言模型生成的概率分布来**从最可能的*k*个选项中随机选择一个令牌**的技术。
- en: 'To illustrate, suppose we have *k = 3* and four tokens: A, B, C, and D, with
    respective probabilities: *P(A) = 30%*, *P(B) = 15%*, *P(C) = 5%*, and *P(D) =
    1%*. In top-k sampling, token D is disregarded, and the algorithm will output
    A 60% of the time, B 30% of the time, and C 10% of the time. This approach ensures
    that we prioritize the most probable tokens while introducing an element of randomness
    in the selection process.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 举例来说，假设我们有*k = 3*和四个token：A、B、C和D，分别的概率为：*P(A) = 30%*、*P(B) = 15%*、*P(C) = 5%*和*P(D)
    = 1%*。在top-k采样中，token D 被忽略，算法会在60%的时间内输出A，在30%的时间内输出B，在10%的时间内输出C。这种方法确保我们优先考虑最可能的token，同时在选择过程中引入一定的随机性。
- en: 'Another way of introducing randomness is the concept of temperature. The temperature
    *T* is a parameter that ranges from 0 to 1, which affects the probabilities generated
    by the softmax function, making the most likely tokens more influential. In practice,
    it simply consists of dividing the input logits by a value we call temperature:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 引入随机性的另一种方式是温度的概念。温度*T*是一个范围从0到1的参数，它影响softmax函数生成的概率，使最可能的token更具影响力。在实际操作中，它简单地包括将输入logits除以我们称之为温度的值：
- en: '![](../Images/b3e030655ba992e2bc09d381d196c363.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b3e030655ba992e2bc09d381d196c363.png)'
- en: Here is a chart that demonstrates the impact of temperature on the probabilities
    generated for a given set of input logits [1.5, -1.8, 0.9, -3.2]. We’ve plotted
    three different temperature values to observe the differences.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是一个图表，展示了温度对给定输入logits [1.5, -1.8, 0.9, -3.2] 生成的概率的影响。我们绘制了三种不同的温度值来观察差异。
- en: '![](../Images/e34fa38ca09ebeaece4e1271c26964da.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e34fa38ca09ebeaece4e1271c26964da.png)'
- en: A temperature of 1.0 is equivalent to a default softmax with no temperature
    at all. On the other hand, a low temperature setting (0.1) significantly alters
    the probability distribution. This is commonly used in text generation to control
    the level of “creativity” in the generated output. By adjusting the temperature,
    we can influence the extent to which the model produces more diverse or predictable
    responses.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 温度为1.0相当于没有温度的默认softmax。另一方面，低温设置（0.1）会显著改变概率分布。这在文本生成中常用于控制生成输出的“创造性”水平。通过调整温度，我们可以影响模型生成更具多样性或更可预测的响应的程度。
- en: Let’s now implement the top k sampling algorithm. We’ll use it in the beam_search()
    function by providing the “top_k” argument. To illustrate how the algorithm works,
    we will also plot the probability distributions for top_k = 20.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们实现top-k采样算法。我们将在beam_search()函数中使用“top_k”参数。为了说明算法的工作原理，我们还将绘制top_k = 20的概率分布。
- en: '[PRE9]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '![](../Images/12300ff8d47224b762e26e6519b528f2.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/12300ff8d47224b762e26e6519b528f2.png)'
- en: Image by author.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供。
- en: These plots give a good intuition of how top-k sampling works, with all the
    potentially selected tokens on the left of the horizontal bar. While the most
    probable tokens are selected (in red) most of the time, it also allows less likely
    tokens to be chosen. This offers an interesting tradeoff that can steer a sequence
    towards a less predictable but more natural-sounding sentence. Now let’s print
    the text it generated.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 这些图给出了top-k采样如何工作的良好直观印象，所有可能选择的token在水平条的左侧。虽然最可能的token（红色）大多数时间被选择，但它也允许选择可能性较低的token。这提供了一个有趣的权衡，可以使序列趋向于一个更不可预测但更自然的句子。现在让我们打印它生成的文本。
- en: '[PRE10]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '[PRE11]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The top-k sampling found a new sequence: “I have a dream job and I want to”,
    which feels significantly more natural than “I have a dream. I have a dream”.
    We’re making progress!'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: top-k采样找到了一个新序列：“我有一个梦想工作，我想要”，这比“我有一个梦想。我有一个梦想”显得自然得多。我们在取得进展！
- en: Let’s see how this decision tree differs from the previous one.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看这个决策树与之前的有何不同。
- en: '[PRE12]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '![](../Images/29cff567ceb5d77709230d8c84e85d04.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/29cff567ceb5d77709230d8c84e85d04.png)'
- en: You can see how the nodes differ significantly from the previous iteration,
    making more diverse choices. Although the sequence score of this new outcome might
    not be the highest (-1.01 instead of -0.69 previously), it’s important to remember
    that higher scores do not always lead to more realistic or meaningful sequences.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到节点与之前的迭代相比有显著不同，做出了更多样的选择。虽然这一新结果的序列分数可能不是最高的（-1.01，而之前为-0.69），但要记住，更高的分数并不总是能导致更现实或有意义的序列。
- en: 'Now that we’ve introduced top-k sampling, we have to present the other most
    popular sampling technique: nucleus sampling.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们引入了top-k采样，我们必须介绍另一种最受欢迎的采样技术：核心采样。
- en: 🔬 Nucleus sampling
  id: totrans-77
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 🔬 核心采样
- en: Nucleus sampling, also known as top-p sampling, takes a different approach from
    top-k sampling. Rather than selecting the top *k* most probable tokens, nucleus
    sampling chooses a cutoff value *p* such that the **sum of the probabilities of
    the selected tokens exceeds *p***. This forms a “nucleus” of tokens from which
    to randomly choose the next token.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 核采样，也称为top-p采样，与top-k采样采取了不同的方法。核采样不是选择前*k*个最可能的标记，而是选择一个截止值*p*，使得**选择的标记的概率总和超过*p***。这形成了一个从中随机选择下一个标记的“核心”。
- en: In other words, the model examines its top probable tokens in descending order
    and keeps adding them to the list until the total probability surpasses the threshold
    *p*. Unlike top-k sampling, the number of tokens included in the nucleus can vary
    from step to step. This variability often results in a more diverse and creative
    output, making nucleus sampling popular for tasks such as text generation.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，模型按概率从高到低检查其最可能的标记，并不断将它们添加到列表中，直到总概率超过阈值*p*。与top-k采样不同，核中包含的标记数量可能在每一步变化。这种变化通常导致更具多样性和创造性的输出，使得核采样在文本生成等任务中颇受欢迎。
- en: To implement the nucleus sampling method, we can use the “nucleus” parameter
    in the beam_search() function. In this example, we’ll set the value of *p* to
    0.5\. To make it easier, we’ll include a minimum number of tokens equal to the
    number of beams. We’ll also consider tokens with cumulative probabilities lower
    than *p*, rather than higher. It’s worth noting that while the details may differ,
    the core idea of nucleus sampling remains the same.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现核采样方法，我们可以在`beam_search()`函数中使用“nucleus”参数。在这个示例中，我们将*p*的值设置为0.5。为了简化操作，我们将包括一个最小的标记数，等于光束的数量。我们还将考虑累计概率低于*p*的标记，而不是高于的标记。值得注意的是，虽然细节可能有所不同，但核采样的核心思想保持不变。
- en: '[PRE13]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '![](../Images/85778461ebdf2816a857f2d25e65eea7.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/85778461ebdf2816a857f2d25e65eea7.png)'
- en: Image by author.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片。
- en: In this plot, you can see that the number of tokens included in the nucleus
    (left of the vertical bar) fluctuates a lot. The generated probability distributions
    vary considerably, leading to the selection of tokens that are not always among
    the most probable ones. This opens the door to the generation of unique and varied
    sequences. Now, let’s observe the text it generated.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个图中，你可以看到核中的标记数量（垂直条的左侧）波动很大。生成的概率分布变化很大，导致选择的标记不总是最可能的。这为生成独特而多样化的序列打开了大门。现在，让我们观察它生成的文本。
- en: '[PRE14]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '[PRE15]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The nucleus sampling algorithm produces the sequence: “I have a dream. I’m
    going to”, which shows a notable enhancement in semantic coherence compared to
    greedy sampling.'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 核采样算法生成的序列是：“I have a dream. I’m going to”，与贪婪采样相比，显示了语义一致性的显著提升。
- en: To compare the decision paths, let’s visualize the new tree nucleus sampling
    generated.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 为了比较决策路径，让我们可视化新生成的树核采样。
- en: '[PRE16]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '![](../Images/23ad20ec7404c08df5cf975e2642a24b.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/23ad20ec7404c08df5cf975e2642a24b.png)'
- en: As with top-k sampling, this tree is very different from the one generated with
    greedy sampling, displaying more variety. Both top-k and nucleus sampling offer
    unique advantages when generating text, enhancing diversity, and introducing creativity
    into the output. Your choice between the two methods (or even greedy search) will
    depend on the specific requirements and constraints of your project.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 与top-k采样一样，这棵树与贪婪采样生成的树非常不同，展示了更多的多样性。top-k和核采样在生成文本时都提供了独特的优势，增强了多样性，并在输出中引入了创造力。你对这两种方法（甚至是贪婪搜索）的选择将取决于你项目的具体要求和限制。
- en: Conclusion
  id: totrans-92
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: In this article, we have delved deep into various decoding methods used by LLMs,
    specifically GPT-2\. We started with a simply **greedy search** and its immediate
    (yet often suboptimal) selection of the most probable next token. Next, we introduced
    the **beam search** technique, which considers several of the most likely tokens
    at each step. Although it offers more nuanced results, beam search can sometimes
    fall short in generating diverse and creative sequences.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我们深入探讨了LLM（尤其是GPT-2）使用的各种解码方法。我们从简单的**贪婪搜索**开始，这种方法会立即（但往往不是最佳的）选择最可能的下一个标记。接下来，我们介绍了**光束搜索**技术，这种方法在每一步考虑几个最可能的标记。尽管它提供了更为细致的结果，但光束搜索有时在生成多样化和富有创造性的序列方面表现不佳。
- en: To bring more variability into the process, we then moved on to **top-k sampling**
    and **nucleus sampling**. Top-k sampling diversifies the text generation by randomly
    selecting among the *k* most probable tokens, while nucleus sampling takes a different
    path by dynamically forming a nucleus of tokens based on cumulative probability.
    Each of these methods brings unique strengths and potential drawbacks to the table,
    and the specific requirements of your project will largely dictate the choice
    among them.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 为了引入更多的变异性，我们接着使用了**top-k 采样**和**nucleus 采样**。Top-k 采样通过在*最可能的k个*标记中随机选择来多样化文本生成，而nucleus
    采样则通过基于累计概率动态地形成一个标记的核心来采取不同的方法。这些方法各自具有独特的优点和潜在的缺点，具体的选择将主要取决于你项目的要求。
- en: Ultimately, understanding these techniques and their trade-offs will equip you
    to better guide the LLMs towards producing increasingly realistic, nuanced, and
    compelling textual output.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，理解这些技术及其权衡将帮助你更好地引导LLMs生成越来越真实、细致和引人入胜的文本输出。
- en: If you’re interested in more technical content around LLMs, you can follow me
    on Twitter [@maximelabonne](https://twitter.com/maximelabonne).
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你对更多关于LLMs的技术内容感兴趣，可以在Twitter上关注我 [@maximelabonne](https://twitter.com/maximelabonne)。
