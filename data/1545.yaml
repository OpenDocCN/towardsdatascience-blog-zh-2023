- en: 'The Art of Prompt Design: Prompt Boundaries and Token Healing'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 《提示设计的艺术：提示边界与标记修复》
- en: 原文：[https://towardsdatascience.com/the-art-of-prompt-design-prompt-boundaries-and-token-healing-3b2448b0be38?source=collection_archive---------2-----------------------#2023-05-08](https://towardsdatascience.com/the-art-of-prompt-design-prompt-boundaries-and-token-healing-3b2448b0be38?source=collection_archive---------2-----------------------#2023-05-08)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/the-art-of-prompt-design-prompt-boundaries-and-token-healing-3b2448b0be38?source=collection_archive---------2-----------------------#2023-05-08](https://towardsdatascience.com/the-art-of-prompt-design-prompt-boundaries-and-token-healing-3b2448b0be38?source=collection_archive---------2-----------------------#2023-05-08)
- en: '[](https://medium.com/@scottmlundberg?source=post_page-----3b2448b0be38--------------------------------)[![Scott
    Lundberg](../Images/99f1c984f0aaabfe4e348a92fa50a1ee.png)](https://medium.com/@scottmlundberg?source=post_page-----3b2448b0be38--------------------------------)[](https://towardsdatascience.com/?source=post_page-----3b2448b0be38--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----3b2448b0be38--------------------------------)
    [Scott Lundberg](https://medium.com/@scottmlundberg?source=post_page-----3b2448b0be38--------------------------------)'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@scottmlundberg?source=post_page-----3b2448b0be38--------------------------------)[![Scott
    Lundberg](../Images/99f1c984f0aaabfe4e348a92fa50a1ee.png)](https://medium.com/@scottmlundberg?source=post_page-----3b2448b0be38--------------------------------)[](https://towardsdatascience.com/?source=post_page-----3b2448b0be38--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----3b2448b0be38--------------------------------)
    [Scott Lundberg](https://medium.com/@scottmlundberg?source=post_page-----3b2448b0be38--------------------------------)'
- en: ·
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F3a739af9ef3a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-art-of-prompt-design-prompt-boundaries-and-token-healing-3b2448b0be38&user=Scott+Lundberg&userId=3a739af9ef3a&source=post_page-3a739af9ef3a----3b2448b0be38---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----3b2448b0be38--------------------------------)
    ·7 min read·May 8, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F3b2448b0be38&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-art-of-prompt-design-prompt-boundaries-and-token-healing-3b2448b0be38&user=Scott+Lundberg&userId=3a739af9ef3a&source=-----3b2448b0be38---------------------clap_footer-----------)'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F3a739af9ef3a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-art-of-prompt-design-prompt-boundaries-and-token-healing-3b2448b0be38&user=Scott+Lundberg&userId=3a739af9ef3a&source=post_page-3a739af9ef3a----3b2448b0be38---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----3b2448b0be38--------------------------------)
    ·7 分钟阅读·2023年5月8日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F3b2448b0be38&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-art-of-prompt-design-prompt-boundaries-and-token-healing-3b2448b0be38&user=Scott+Lundberg&userId=3a739af9ef3a&source=-----3b2448b0be38---------------------clap_footer-----------)'
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F3b2448b0be38&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-art-of-prompt-design-prompt-boundaries-and-token-healing-3b2448b0be38&source=-----3b2448b0be38---------------------bookmark_footer-----------)![](../Images/7da96052b147da9162ebeb9229095ca5.png)'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F3b2448b0be38&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-art-of-prompt-design-prompt-boundaries-and-token-healing-3b2448b0be38&source=-----3b2448b0be38---------------------bookmark_footer-----------)![](../Images/7da96052b147da9162ebeb9229095ca5.png)'
- en: All images are original creations.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 所有图像均为原创作品。
- en: This (written jointly with [Marco Tulio Ribeiro](https://medium.com/@marcotcr))
    is part 2 of a series on **the art of prompt design** (part 1 [here](https://medium.com/towards-data-science/the-art-of-prompt-design-use-clear-syntax-4fc846c1ebd5)),
    where we talk about controlling large language models (LLMs) with `[guidance](https://github.com/microsoft/guidance)`.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 这篇文章（与 [Marco Tulio Ribeiro](https://medium.com/@marcotcr) 共同撰写）是关于**提示设计艺术**系列的第二部分（第一部分
    [这里](https://medium.com/towards-data-science/the-art-of-prompt-design-use-clear-syntax-4fc846c1ebd5)），在其中我们讨论了如何通过
    `[guidance](https://github.com/microsoft/guidance)` 控制大型语言模型（LLMs）。
- en: In this post, we’ll discuss how the greedy/optimized tokenization methods used
    by language models can introduce a subtle and powerful bias into your prompts,
    leading to puzzling generations.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我们将讨论语言模型使用的贪婪/优化标记化方法如何在提示中引入微妙而强大的偏差，从而导致令人困惑的生成结果。
- en: Language models are not trained on raw text, but rather on tokens, which are
    chunks of text that often occur together, similar to words. This impacts how language
    models ‘see’ text, including prompts (since prompts are just sets of tokens).
    GPT-style models utilize tokenization methods like [Byte Pair Encoding](https://en.wikipedia.org/wiki/Byte_pair_encoding)
    (BPE), which map all input bytes to token ids in a greedy manner. This is fine
    for training, but it can lead to subtle issues during inference, as shown in the
    example below.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 语言模型不是在原始文本上进行训练，而是在标记（tokens）上进行训练，标记是经常一起出现的文本块，类似于单词。这会影响语言模型如何“看待”文本，包括提示（因为提示只是标记的集合）。GPT
    风格的模型使用像 [字节对编码](https://en.wikipedia.org/wiki/Byte_pair_encoding)（BPE）这样的标记化方法，它们以贪婪的方式将所有输入字节映射到标记
    ID。这在训练时是可以接受的，但在推理过程中可能会导致微妙的问题，如下例所示。
- en: '**An example of a prompt boundary problem**'
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**提示边界问题的一个例子**'
- en: 'Consider the following example, where we are trying to generate an HTTP URL
    string:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑以下示例，我们正在尝试生成 HTTP URL 字符串：
- en: '[PRE0]'
  id: totrans-13
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '![](../Images/9f08089bdd4af8fb7a57582e30d16f31.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9f08089bdd4af8fb7a57582e30d16f31.png)'
- en: Notebook output.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 笔记本输出。
- en: 'Note that the output generated by the LLM does not complete the url with the
    obvious next characters (two forward slashes). It instead creates an invalid URL
    string with a space in the middle. This is surprising, because the `//` completion
    is extremely obvious after `http:`. To understand why this happens, let’s change
    our prompt boundary so that our prompt does not include the colon character:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，LLM 生成的输出没有用明显的下一个字符（两个斜杠）完成 URL。它反而创建了一个中间带有空格的无效 URL 字符串。这令人惊讶，因为在 `http:`
    之后 `//` 的补全是非常明显的。要理解为什么会发生这种情况，让我们改变我们的提示边界，使提示不包括冒号字符：
- en: '[PRE1]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '![](../Images/9a83d183032d3ab835c0a1d65b13dac6.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9a83d183032d3ab835c0a1d65b13dac6.png)'
- en: 'Now the language model generates a valid url string like we expect. To understand
    why the `:` matters, we need to look at the tokenized representation of the prompts.
    Below is the tokenization of the prompt that ends in a colon (the prompt without
    the colon has the same tokenization, except for the last token):'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，语言模型生成了我们预期的有效 URL 字符串。要理解 `:` 的重要性，我们需要查看提示的标记化表示。下面是以冒号结尾的提示的标记化（不包含冒号的提示有相同的标记化，除了最后一个标记）：
- en: '[PRE2]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '![](../Images/06ebdcbaf90a7dc92b1e504e2829498b.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/06ebdcbaf90a7dc92b1e504e2829498b.png)'
- en: 'Now note what the tokenization of a valid URL looks like, paying careful attention
    to token `1358`, right after `http`:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 现在注意有效 URL 的标记化是什么样的，特别关注 `http` 后的标记 `1358`：
- en: '[PRE3]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '![](../Images/a3c2d8e91a5a4fdc91400781b851273d.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a3c2d8e91a5a4fdc91400781b851273d.png)'
- en: Most LLMs (including this one) use a greedy tokenization method, always preferring
    the longest possible token, i.e. `://` will always be preferred over `:` in full
    text (e.g. in training).
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数 LLM（包括这个）使用贪婪的标记化方法，总是偏向于选择最长的标记，即 `://` 在完整文本中（例如在训练中）总是优于 `:`。
- en: While URLs in training are encoded with token 1358 (`://`), our prompt makes
    the LLM see token `27` (`:`) instead, which throws off completion by artificially
    splitting `://`.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练中，URL 使用标记 1358（`://`）进行编码，而我们的提示使 LLM 看到标记 `27`（`:`），这会通过人为地将 `://` 拆分开来，从而影响完成。
- en: In fact, the model can be pretty sure that seeing token `27` (`:`) means what
    comes next is very unlikely to be anything that could have been encoded together
    with the colon using a “longer token” like `://`, since in the model’s training
    data those characters would have been encoded together with the colon (an exception
    to this that we will discuss later is subword regularization during training).
    The fact that seeing a token means both seeing the embedding of that token **and
    also** that whatever comes next wasn’t compressed by the greedy tokenizer is easy
    to forget, but it is important in prompt boundaries.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，模型可以非常确定看到标记 `27`（`:`）意味着接下来的内容极不可能是可以与冒号一起编码的“更长的标记”像 `://`，因为在模型的训练数据中，这些字符会与冒号一起编码（稍后我们将讨论的一个例外是训练期间的子词正则化）。看到一个标记意味着同时看到该标记的嵌入
    **以及** 之后的内容没有被贪婪的标记化器压缩，这一点容易被忘记，但在提示边界中很重要。
- en: 'Let’s search over the string representation of all the tokens in the model’s
    vocabulary, to see which ones start with a colon:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们搜索模型词汇表中所有标记的字符串表示，看看哪些标记以冒号开头：
- en: '[PRE4]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '![](../Images/bba5fa5bd4edeccb89dafdecbea690df.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bba5fa5bd4edeccb89dafdecbea690df.png)'
- en: 'Note that there are **34** different tokens starting with a colon, and thus
    ending a prompt with a colon means the model will likely not generate completions
    with any of these 34 token strings. *This subtle and powerful bias can have all
    kinds of unintended consequences.* And this applies to **any** string that could
    be potentially extended to make a longer single token (not just `:`). Even our
    “fixed” prompt ending with “http” has a built in bias as well, as it communicates
    to the model that what comes after “http” is likely not “s” (otherwise “http”
    would not have been encoded as a separate token):'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，有**34**种不同的令牌以冒号开头，因此以冒号结尾的提示可能不会生成这些**34**种令牌字符串中的任何一种。*这种微妙而强大的偏差可能会带来各种意想不到的后果。*
    这适用于**任何**可能被扩展为更长单一令牌的字符串（不仅仅是 `:`）。即使是我们以“http”结尾的“固定”提示也存在内建偏差，因为它向模型传达了“http”后面内容可能不是“s”（否则“http”不会被编码为单独的令牌）：
- en: '[PRE5]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '![](../Images/5722e1eed5f87ab98014368cd2e2dbce.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5722e1eed5f87ab98014368cd2e2dbce.png)'
- en: 'Lest you think this is an arcane problem that only touches URLs, remember that
    most tokenizers treat tokens differently depending on whether they start with
    a space, punctuation, quotes, etc, and thus **ending a prompt with any of these
    can lead to wrong token boundaries**, and break things:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 以为这是只影响URL的深奥问题而不值得担忧，请记住，大多数令牌化器会根据令牌是否以空格、标点、引号等开头来处理令牌，因此**以这些内容结尾的提示可能会导致错误的令牌边界**，从而破坏内容：
- en: '[PRE6]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '![](../Images/fab000c5aac694ca330aa3f9f98055a0.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fab000c5aac694ca330aa3f9f98055a0.png)'
- en: '[PRE7]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '![](../Images/c4b67b763845aed5ce9f75a575cefd3c.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c4b67b763845aed5ce9f75a575cefd3c.png)'
- en: 'Another example of this is the “[“ character. Consider the following prompt
    and completion:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个例子是“[”字符。考虑以下提示和补全：
- en: '[PRE8]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '![](../Images/2e193cde149e2a761afee84cf6cae3b6.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2e193cde149e2a761afee84cf6cae3b6.png)'
- en: 'Why is the second string not quoted? Because by ending our prompt with the
    “ `[`” token, we are telling the model that it should not generate completions
    that match the following 27 longer tokens (one of which adds the quote character,
    `15640`):'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么第二个字符串没有被引号括起来？因为通过以“`[`”令牌结尾，我们告诉模型不生成与以下27个更长令牌匹配的补全（其中一个增加了引号字符，`15640`）：
- en: '[PRE9]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '![](../Images/8d0086e91c6df84c0bcdd79736f6f867.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8d0086e91c6df84c0bcdd79736f6f867.png)'
- en: Token boundary bias happens everywhere. About *70% of the 10k most-common tokens
    for the StableLM model used above are prefixes of longer possible tokens, and
    so cause token boundary bias when they are the last token in a prompt.*
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 令牌边界偏差无处不在。上述StableLM模型的*70%*最常见令牌是较长可能令牌的前缀，因此在提示中的最后一个令牌时会造成令牌边界偏差。*
- en: '**Fixing unintended bias with “token healing”**'
  id: totrans-46
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**通过“令牌修复”修正无意的偏差**'
- en: What can we do to avoid these unintended biases? One option is to always end
    our prompts with tokens that cannot be extended into longer tokens (for example
    a role tag for chat-based models), but this is a severe limitation.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以做些什么来避免这些无意的偏差？一种选择是始终以不能扩展为更长令牌的令牌结束提示（例如用于基于聊天的模型的角色标签），但这是一种严重限制。
- en: Instead, `guidance` has a feature called “token healing”, which automatically
    backs up the generation process by one token before the end of the prompt, then
    constrains the first token generated to have a prefix that matches the last token
    in the prompt. In our URL example, this would mean removing the `:`, and forcing
    generation of the first token to have a `:` prefix. Token healing allows users
    to express prompts however they wish, without worrying about token boundaries.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，`guidance`具有一个叫做“令牌修复”的功能，它会在提示末尾之前自动将生成过程备份一个令牌，然后约束生成的第一个令牌具有与提示最后一个令牌匹配的前缀。在我们的URL示例中，这将意味着移除
    `:` 并强制生成的第一个令牌具有 `:` 前缀。令牌修复允许用户以任何他们希望的方式表达提示，而无需担心令牌边界。
- en: 'For example, let’s re-run some of the URL examples above with token healing
    turned on (it’s on by default for Transformer models, so we remove `token_healing=False`):'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，让我们重新运行一些上述URL示例，并开启令牌修复功能（对于Transformer模型，默认是开启的，因此我们去掉 `token_healing=False`）：
- en: '[PRE10]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '![](../Images/076ec3d9d9243b506a86a061a6e4e78f.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/076ec3d9d9243b506a86a061a6e4e78f.png)'
- en: '[PRE11]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '![](../Images/143c104e36c53354f3e659a68d58613f.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/143c104e36c53354f3e659a68d58613f.png)'
- en: 'Similarly, we don’t have to worry about extra spaces:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，我们无需担心额外的空格：
- en: '[PRE12]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '![](../Images/e217a0e6ec54dca35357504011974386.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e217a0e6ec54dca35357504011974386.png)'
- en: '[PRE13]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '![](../Images/0f2d372e350aab9997ee5e61dbd818f9.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0f2d372e350aab9997ee5e61dbd818f9.png)'
- en: 'And we now get quoted strings even when the prompt ends with a “ `[`” token:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 现在即使提示以“`[`”令牌结尾，我们也能获得引号括起来的字符串：
- en: '[PRE14]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '![](../Images/7aa6d716e7d702cae0a8567009775452.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7aa6d716e7d702cae0a8567009775452.png)'
- en: '**What about subword regularization?**'
  id: totrans-62
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**那么子词正则化呢？**'
- en: If you are familiar with how language models are trained, you may be wondering
    how [subword regularization](https://arxiv.org/abs/1804.10959) fits into all this.
    Subword regularization is a technique where during training sub-optimal tokenizations
    are randomly introduced to increase the model’s robustness. This means that the
    model does not always see the best greedy tokenization. Subword regularization
    is great at helping the model be more robust to token boundaries, but it does
    not altogether remove the bias that the model has towards the standard greedy/optimized
    tokenization. This means that while depending on the amount of subword regularization
    during training models may exhibit more or less token boundaries bias, all models
    still have this bias. And as shown above it can still have a powerful and unexpected
    impact on the model output.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你对语言模型的训练方式很熟悉，你可能会想知道[子词正则化](https://arxiv.org/abs/1804.10959)在这一切中如何发挥作用。子词正则化是一种技术，在训练过程中随机引入次优的分词，以提高模型的鲁棒性。这意味着模型并不总是看到最佳的贪婪分词。子词正则化有助于提高模型对标记边界的鲁棒性，但它并不能完全消除模型对标准贪婪/优化分词的偏见。这意味着虽然模型的子词正则化程度可能影响标记边界的偏见程度，但所有模型仍然存在这种偏见。如上所示，它仍然可以对模型输出产生强大而意外的影响。
- en: '**Conclusion**'
  id: totrans-64
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**结论**'
- en: When you write prompts, remember that greedy tokenization can have a significant
    impact on how language models interpret your prompts, particularly when the prompt
    ends with a token that could be extended into a longer token. This easy-to-miss
    source of bias can impact your results in surprising and unintended ways.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在编写提示时，请记住贪婪分词可能对语言模型如何解释你的提示产生重大影响，尤其是当提示以一个可能扩展为更长标记的标记结束时。这种容易被忽视的偏见来源可能以令人惊讶和意想不到的方式影响你的结果。
- en: To address to this, either end your prompt with a non-extendable token, or use
    something like `guidance`’s “token healing” feature so you can to express your
    prompts however you wish, without worrying about token boundary artifacts.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这个问题，可以在提示的末尾加上一个不可扩展的标记，或者使用类似`guidance`的“标记修复”功能，以便你可以根据自己的意愿表达提示，而不必担心标记边界的影响。
- en: '*To reproduce the results in this article yourself check out the* [*notebook*](https://github.com/microsoft/guidance/blob/main/notebooks/art_of_prompt_design/prompt_boundaries_and_token_healing.ipynb)
    *version.*'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '*要自己复现本文中的结果，请查看* [*notebook*](https://github.com/microsoft/guidance/blob/main/notebooks/art_of_prompt_design/prompt_boundaries_and_token_healing.ipynb)
    *版本。*'
