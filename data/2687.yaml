- en: Can We Stop LLMs from Hallucinating?
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 我们能否阻止LLMs产生幻觉？
- en: 原文：[https://towardsdatascience.com/can-we-stop-llms-from-hallucinating-17c4ebd652c6?source=collection_archive---------6-----------------------#2023-08-24](https://towardsdatascience.com/can-we-stop-llms-from-hallucinating-17c4ebd652c6?source=collection_archive---------6-----------------------#2023-08-24)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/can-we-stop-llms-from-hallucinating-17c4ebd652c6?source=collection_archive---------6-----------------------#2023-08-24](https://towardsdatascience.com/can-we-stop-llms-from-hallucinating-17c4ebd652c6?source=collection_archive---------6-----------------------#2023-08-24)
- en: Opinion
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 意见
- en: '*One of the greatest barriers to widespread LLM adoption may be inherently
    unsolvable.*'
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '*普及LLMs的最大障碍之一可能是本质上难以解决的。*'
- en: '[](https://medium.com/@juras.jursenas?source=post_page-----17c4ebd652c6--------------------------------)[![Juras
    Juršėnas](../Images/eb2ca720f2c8688dbf8079879c028d12.png)](https://medium.com/@juras.jursenas?source=post_page-----17c4ebd652c6--------------------------------)[](https://towardsdatascience.com/?source=post_page-----17c4ebd652c6--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----17c4ebd652c6--------------------------------)
    [Juras Juršėnas](https://medium.com/@juras.jursenas?source=post_page-----17c4ebd652c6--------------------------------)'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@juras.jursenas?source=post_page-----17c4ebd652c6--------------------------------)[![Juras
    Juršėnas](../Images/eb2ca720f2c8688dbf8079879c028d12.png)](https://medium.com/@juras.jursenas?source=post_page-----17c4ebd652c6--------------------------------)[](https://towardsdatascience.com/?source=post_page-----17c4ebd652c6--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----17c4ebd652c6--------------------------------)
    [Juras Juršėnas](https://medium.com/@juras.jursenas?source=post_page-----17c4ebd652c6--------------------------------)'
- en: ·
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F3041473d9e3c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcan-we-stop-llms-from-hallucinating-17c4ebd652c6&user=Juras+Jur%C5%A1%C4%97nas&userId=3041473d9e3c&source=post_page-3041473d9e3c----17c4ebd652c6---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----17c4ebd652c6--------------------------------)
    ·5 min read·Aug 24, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F17c4ebd652c6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcan-we-stop-llms-from-hallucinating-17c4ebd652c6&user=Juras+Jur%C5%A1%C4%97nas&userId=3041473d9e3c&source=-----17c4ebd652c6---------------------clap_footer-----------)'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F3041473d9e3c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcan-we-stop-llms-from-hallucinating-17c4ebd652c6&user=Juras+Jur%C5%A1%C4%97nas&userId=3041473d9e3c&source=post_page-3041473d9e3c----17c4ebd652c6---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----17c4ebd652c6--------------------------------)
    · 阅读时间5分钟·2023年8月24日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F17c4ebd652c6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcan-we-stop-llms-from-hallucinating-17c4ebd652c6&user=Juras+Jur%C5%A1%C4%97nas&userId=3041473d9e3c&source=-----17c4ebd652c6---------------------clap_footer-----------)'
- en: --
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F17c4ebd652c6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcan-we-stop-llms-from-hallucinating-17c4ebd652c6&source=-----17c4ebd652c6---------------------bookmark_footer-----------)![](../Images/09bc28622320bd28b5b369807b75f40d.png)'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F17c4ebd652c6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcan-we-stop-llms-from-hallucinating-17c4ebd652c6&source=-----17c4ebd652c6---------------------bookmark_footer-----------)![](../Images/09bc28622320bd28b5b369807b75f40d.png)'
- en: Photo by [Google DeepMind](https://unsplash.com/@googledeepmind?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由 [Google DeepMind](https://unsplash.com/@googledeepmind?utm_source=medium&utm_medium=referral)
    提供，来源于 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: While Large Language Models (LLMs) have captured the attention of nearly everyone,
    wide-scale deployment of such technology is slightly limited due to a rather annoying
    aspect of it — these models tend to hallucinate. In simple terms, they sometimes
    just make things up, and worst of all, it often looks highly convincing.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然大型语言模型（LLMs）已经引起了几乎所有人的关注，但由于它们存在一个相当恼人的方面——这些模型有时会“幻觉”，因此这种技术的大规模应用略显受限。简单来说，它们有时会编造信息，而且最糟的是，这些内容往往看起来非常令人信服。
- en: Hallucinations, frequent or not, bring with them two major issues. They can’t
    be directly implemented in many sensitive or brittle fields where a single mistake
    can be highly costly. In addition, it sows general distrust as users are expected
    to verify everything coming out of an LLM, which, at least in part, defeats the
    purpose of such technology.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 幻觉，无论频繁与否，都带来了两个主要问题。它们无法直接应用于许多敏感或脆弱的领域，其中一个错误可能会非常昂贵。此外，它还会造成普遍的不信任，因为用户被期望验证LLM输出的所有内容，这在一定程度上违背了这项技术的初衷。
- en: Academia seems to also think that hallucinations are a major problem, as there
    are dozens of research papers in 2023 discussing and attempting to solve the issue.
    I, however, [would tend to agree with Yann LeCun](https://www.youtube.com/watch?v=mViTAXCg1xQ&feature=youtu.be),
    Meta’s Chief AI Scientist, that the hallucinations are not resolvable at all.
    We would need a complete revamp of the technology to eliminate the issue.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 学术界似乎也认为幻觉是一个重大问题，因为2023年有很多研究论文讨论并试图解决这个问题。然而，我[倾向于同意Yann LeCun](https://www.youtube.com/watch?v=mViTAXCg1xQ&feature=youtu.be)，Meta的首席AI科学家，他认为幻觉根本无法解决。我们需要对技术进行彻底的重构以消除这个问题。
- en: Hallucinating false statements
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 幻觉虚假陈述
- en: There are two important aspects to any LLM which, I think, make hallucinations
    unsolvable. Starting with the rather obvious technological underpinning, LLMs,
    like any other machine learning model, are stochastic in nature. In simple terms,
    they make predictions.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 我认为，有两个重要方面使得幻觉问题无法解决。首先是相当明显的技术基础，LLM像其他机器学习模型一样，具有随机性。简单来说，它们做出预测。
- en: While they’re certainly much more advanced than “glorified autocomplete,” the
    underlying technology still uses statistical predictions about tokens. It’s both
    one of the strengths and weaknesses of LLMs.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然它们确实比“被吹捧的自动完成”要先进得多，但底层技术仍然使用关于令牌的统计预测。这既是LLM的优点也是缺点。
- en: On the strong part, we have seen how amazingly good they are at predicting what
    should come after an input (barring any intentional attempt to ruin an output).
    Users can make several types of mistakes, such as leaving in a typo, misunderstanding
    the meaning of a word, etc., and LLMs are still likely to get the output right.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在强项方面，我们已经看到它们在预测输入后的内容方面是多么出色（假设没有故意破坏输出的尝试）。用户可能会犯几种类型的错误，比如留有错别字、误解单词的意思等，而LLM仍然可能给出正确的输出。
- en: Back in the day, when the first text-based games were created, users were asked
    to input commands without any errors or room for interpretation. A command such
    as “move north” would error out if the user inputted “move morth”. An LLM, however,
    might be able to infer the meaning in both cases. In that sense, the technology
    is truly fascinating.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 当初，首批基于文本的游戏被创造时，用户需要准确输入命令，不能有任何错误或解释的空间。比如，“move north”命令如果用户输入为“move morth”就会出错。然而，LLM可能能够推测两者的意思。从这个角度来看，这项技术确实非常迷人。
- en: Yet, it also showcases a weakness. Any input has a wide potential decision tree
    for token choice. In simple terms, there’s always a huge range of ways a model
    can create an output. Out of that large range, a relatively small bit is the “correct”
    decision.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这也展示了一个弱点。任何输入都有一个广泛的令牌选择决策树。简单来说，模型生成输出的方式总是有很多种。在那种广泛的选择范围中，相对较小的部分是“正确”的决定。
- en: While there are numerous [optimization options available](https://github.com/hwchase17/langchain),
    the problem itself is not solvable. For example, if we increase the likelihood
    of providing one specific answer, the LLM becomes a lookup table, so we’d want
    to keep a balance. The underlying technology is simply based on stochastic predictions,
    and there has to be some room for a wider range of output tokens provided.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管有许多[优化选项](https://github.com/hwchase17/langchain)可供选择，但问题本身是不可解决的。例如，如果我们增加提供某个特定答案的可能性，LLM会变成一个查找表，所以我们希望保持平衡。底层技术仅基于随机预测，因此必须为更广泛的输出令牌提供一些空间。
- en: But there’s another problem that LLMs cannot solve, at least in their current
    state. It’s a bit more ephemeral and abstract as it relates to epistemology, the
    field of philosophy that studies the nature of knowledge. On the face of it, the
    problem is simple — how do we know which statements are true, and how do we gain
    such knowledge? After all, a hallucination is simply a set of false statements
    *post-hoc*, so if we could create a way for the model to verify that it has made
    a false statement and remove it, that would solve the problem.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 但LLMs在当前状态下不能解决另一个问题。这涉及到更为抽象和虚幻的认识论问题，即研究知识本质的哲学领域。表面上看，这个问题很简单——我们如何知道哪些陈述是真实的，我们如何获得这种知识？毕竟，幻觉只是一些虚假的陈述*事后*产生的，因此如果我们能为模型创建一种方法来验证其是否做出了虚假的陈述并将其删除，这将解决问题。
- en: Separating hallucinations from truthful statements
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分离幻觉和真实陈述
- en: Following in the footsteps of philosophy, we can separate two types of possible
    statements — analytic and synthetic. The former are statements that are true by
    virtue of definition (one of the most common examples is “a bachelor is an unmarried
    man”). In simple terms, we can find statements that are true by analyzing the
    language itself, and no external experience is required.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 借鉴哲学的思路，我们可以区分两种可能的陈述——分析性和综合性。前者是通过定义而真实的陈述（最常见的例子之一是“单身汉是未婚男子”）。简单来说，我们可以通过分析语言本身找到真实的陈述，而无需外部经验。
- en: Synthetic statements are any statements that are true by virtue of some form
    of experience, such as “there is an apple on the table in front of me.” There’s
    no way to know whether such a statement is true without referring to direct experience.
    Pure linguistic analysis does no good in determining whether it is true or false.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 综合性陈述是指通过某种形式的经验来判断真实的陈述，例如“桌子上有一个苹果”。在没有直接经验的情况下，无法知道这样的陈述是否真实。纯粹的语言分析对于判断其真假无济于事。
- en: I should note that the distinction between these statements has been hotly contested
    for hundreds of years, but the discussion is largely irrelevant for LLMs. As their
    name might state, they’re a highly advanced linguistic analysis and prediction
    machine.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 我应该指出，这些陈述之间的区别在几百年来一直备受争议，但这一讨论对于LLMs（大型语言模型）来说基本上不相关。正如它们的名字所示，它们是高度先进的语言分析和预测机器。
- en: Following the distinction between the two types, we can see that LLMs would
    have little to no issue with analytic statements (or at least as much as humans
    do). Yet, they have no access to experience or the world at large. There’s no
    way for them to know that some statements are true by virtue of an event.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 根据这两种类型的区别，我们可以看到LLMs在分析性陈述方面几乎不会有问题（至少不会比人类有更多问题）。然而，它们无法获取经验或大范围的世界知识。它们无法知道某些陈述是否因为事件的存在而真实。
- en: The major issue is that the number of analytic statements is significantly smaller
    than the set of all synthetic statements. Since an LLM has no way of verifying
    whether these statements are true, we, as humans, have to provide them with such
    information.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 主要问题在于，分析性陈述的数量远小于所有综合性陈述的集合。由于LLMs无法验证这些陈述是否真实，我们作为人类必须向它们提供这些信息。
- en: As such, LLMs run into a challenge. The set of all possible outputs will always
    have some number of synthetic statements, but to the model, all of them are truth-value
    agnostic. In simple terms, “Julius Caesar’s assassin was Brutus” (there were many,
    but for this case, it doesn’t matter) and “Julius Caesar’s assassin was Abraham
    Lincoln” are equivalent to a model.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，LLMs面临一个挑战。所有可能输出的集合中总会包含一些综合性陈述，但对于模型来说，它们都是不具备真值的。简单来说，“尤利乌斯·凯撒的刺客是布鲁图斯”（虽然有很多，但在此案例中无关紧要）和“尤利乌斯·凯撒的刺客是亚伯拉罕·林肯”对于模型来说是等同的。
- en: A counterargument might be that we have not had any direct experience about
    those events, either. We just read about them in books. But the discovery of the
    truthfulness of the statement is based on a reconstruction of surviving accounts
    and a wide range of other archaeological evidence.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 反驳意见可能是我们对这些事件也没有直接经验。我们只是从书籍中读到它们。但对陈述真实性的发现是基于对幸存记录的重建以及广泛的考古证据。
- en: A simpler example of an (albeit less relevant) statement would be “it is raining
    today.” Such statements are impossible to determine as true for an LLM at all
    as it needs access to real-world experience at the moment of query.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 一个简单的（虽然相关性较低）例子是“今天在下雨。” 对于大型语言模型来说，这样的陈述是无法确定其真实性的，因为它需要在查询时接触到现实世界的经验。
- en: In one sense, the epistemological problem is self-solving. Our literary corpus
    would make the output that “Julius Caesar’s assassin was Brutus” significantly
    more likely due to it being present more frequently. Yet, again, the problem is
    that such a self-solving solution relies on training an LLM on absolutely all
    available textual information, which, obviously, is impossible. Additionally,
    that would make other, less truthful outputs not entirely absent from the set
    of all possible outputs.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 从某种意义上说，认识论问题是自我解决的。我们的文学语料库会使“凯撒的刺客是布鲁图斯”这种输出变得显著更可能，因为它出现得更频繁。然而，再次强调的是，这种自我解决的方法依赖于在绝对所有可用文本信息上训练大型语言模型，这显然是不可能的。此外，这也会使其他不那么真实的输出仍然存在于所有可能输出的集合中。
- en: As such, data quality becomes an important factor, but that quality can only
    be judged by human observers. Even in cases where models are trained on enormous
    amounts of data, there’s a certain selection process that takes place, which means
    that the error rate for synthetic statements cannot be eliminated.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，数据质量变得非常重要，但这种质量只能由人类观察者来判断。即使模型在大量数据上进行训练，仍然会有一定的选择过程，这意味着合成陈述的错误率无法消除。
- en: Conclusion
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: I believe that the problem of stopping models from hallucinating is unsolvable.
    For one, the technology itself is based on a stochastic process, which inevitably,
    over a large number of outputs, will lead to erroneous predictions.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 我认为，阻止模型产生幻觉的问题是无法解决的。一方面，技术本身基于随机过程，这不可避免地会在大量输出中导致错误的预测。
- en: In addition to the technological hurdle, there’s the question of whether LLMs
    can make truth-value judgments about statements, which, again, I believe is impossible
    as they have no access to the real world. The issue is slightly attenuated by
    various search engine functions that are now available for many LLMs, according
    to which they may verify certain statements.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 除了技术难题，还有一个问题是大型语言模型是否可以对陈述做出真实性判断，我再次认为这是不可能的，因为它们无法接触到现实世界。这个问题在很多大型语言模型现在提供的各种搜索引擎功能中稍微得到了缓解，这些功能可以验证某些陈述。
- en: It might be possible, however, to collect a database against which statements
    can be tested, but that would require something beyond the technology itself,
    which leads us back to the initial problem.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，可能会有一种方法是收集一个可以测试陈述的数据库，但这需要超出技术本身的东西，这将我们带回到最初的问题。
