- en: Data is the Foundation of Language Models
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据是语言模型的基础
- en: 原文：[https://towardsdatascience.com/data-is-the-foundation-of-language-models-52e9f48c07f5](https://towardsdatascience.com/data-is-the-foundation-of-language-models-52e9f48c07f5)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/data-is-the-foundation-of-language-models-52e9f48c07f5](https://towardsdatascience.com/data-is-the-foundation-of-language-models-52e9f48c07f5)
- en: How high-quality data impacts every aspect of the LLM training pipeline…
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 高质量的数据如何影响LLM训练流程的每个方面…
- en: '[](https://wolfecameron.medium.com/?source=post_page-----52e9f48c07f5--------------------------------)[![Cameron
    R. Wolfe, Ph.D.](../Images/52bb88d7cf1105501be2fae5ccbe7a03.png)](https://wolfecameron.medium.com/?source=post_page-----52e9f48c07f5--------------------------------)[](https://towardsdatascience.com/?source=post_page-----52e9f48c07f5--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----52e9f48c07f5--------------------------------)
    [Cameron R. Wolfe, Ph.D.](https://wolfecameron.medium.com/?source=post_page-----52e9f48c07f5--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://wolfecameron.medium.com/?source=post_page-----52e9f48c07f5--------------------------------)[![Cameron
    R. Wolfe, Ph.D.](../Images/52bb88d7cf1105501be2fae5ccbe7a03.png)](https://wolfecameron.medium.com/?source=post_page-----52e9f48c07f5--------------------------------)[](https://towardsdatascience.com/?source=post_page-----52e9f48c07f5--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----52e9f48c07f5--------------------------------)
    [Cameron R. Wolfe, Ph.D.](https://wolfecameron.medium.com/?source=post_page-----52e9f48c07f5--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----52e9f48c07f5--------------------------------)
    ·16 min read·Oct 29, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----52e9f48c07f5--------------------------------)
    ·阅读时间16分钟·2023年10月29日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/0cdc29d0203d7225c4541733960f9b92.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0cdc29d0203d7225c4541733960f9b92.png)'
- en: (Photo by [Joshua Sortino](https://unsplash.com/@sortino?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)
    on [Unsplash](https://unsplash.com/photos/worms-eye-view-photography-of-ceiling-LqKhnDzSF-8?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash))
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: （照片由 [Joshua Sortino](https://unsplash.com/@sortino?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)
    提供，来源于 [Unsplash](https://unsplash.com/photos/worms-eye-view-photography-of-ceiling-LqKhnDzSF-8?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)）
- en: Large Language Models (LLMs) have been around for quite some time, but only
    recently has their impressive performance warranted significant attention from
    the broader AI community. With this in mind, we might begin to question the origin
    of the current LLM movement. *What was it that actually made recent models so
    impressive compared to their predecessors?* Although some may argue a variety
    of different factors, one especially impactful advancement was the ability to
    perform alignment. In other words, we figured out how to train LLMs to not just
    output the most likely next word, but to output text will satisfy the goals of
    a human, whether it be by following an instruction or retrieving important information.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）已经存在了一段时间，但直到最近，它们令人印象深刻的表现才引起了更广泛的人工智能社区的显著关注。考虑到这一点，我们可能开始质疑当前LLM运动的起源。*究竟是什么使得近期模型相比于前辈如此令人印象深刻？*
    尽管有些人可能会提出多种不同因素，但一个尤其重要的进步是执行对齐的能力。换句话说，我们找到了训练LLMs的方法，不仅输出最可能的下一个词，而且输出能满足人类目标的文本，无论是通过遵循指令还是检索重要信息。
- en: “We hypothesize that alignment can be a simple process where the model learns
    the style or format for interacting with users, to expose the knowledge and capabilities
    that were already acquired during pretraining” *— from [1]*
  id: totrans-9
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “我们假设对齐可以是一个简单的过程，其中模型学习与用户互动的风格或格式，以揭示在预训练期间已经获得的知识和能力”*— 来自 [1]*
- en: This overview will study the role and impact of alignment, as well as the interplay
    between alignment and pre-training. Interestingly, these ideas were explored by
    the recent LIMA model [1], which performs alignment by simply fine-tuning a pre-trained
    LLM over a semi-manually curated corpus of only 1,000 high-quality response examples.
    We will learn that the alignment process, although critical, primarily teaches
    an LLM steerability and correct behavior or style, while most knowledge is gained
    during pre-training. As such, alignment can be performed successfully even with
    minimal training data. However, we will see that the impact of data quality and
    diversity on both alignment and other avenues of LLM training (e.g., pre-training,
    fine-tuning, etc.) is absolutely massive.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 本概述将研究对齐的作用和影响，以及对齐与预训练之间的相互作用。有趣的是，这些观点由最近的LIMA模型 [1] 探讨，该模型通过仅对1,000个高质量响应示例的半手动策划语料库进行预训练的LLM的微调来执行对齐。我们将了解到，对齐过程虽然关键，但主要教会LLM可操控性和正确的行为或风格，而大多数知识是在预训练期间获得的。因此，即使数据训练量极少，对齐也可以成功执行。然而，我们将看到数据质量和多样性对对齐以及LLM训练的其他方面（例如，预训练、微调等）的影响是巨大的。
- en: The LLM Training Pipeline
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LLM训练流程
- en: '“LLMs are trained in two stages: (1) unsupervised pretraining from raw text,
    to learn general-purpose representations, and (2) large scale instruction tuning
    and reinforcement learning, to better align to end tasks and user preferences.”
    *— from [1]*'
  id: totrans-12
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “LLMs 的训练分为两个阶段：（1）从原始文本中进行无监督预训练，以学习通用表示，和（2）大规模的指令调整和强化学习，以更好地对齐最终任务和用户偏好。”
    *— 摘自 [1]*
- en: Although language models have been studied from a variety of different perspectives
    in recent months, the creation of these models tends to follow a standardized
    process with a few common components; see below.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管最近几个月从各种不同的角度研究了语言模型，但这些模型的创建往往遵循标准化的过程，并具有一些共同的组件；见下文。
- en: '![](../Images/00e8ba87fafbb39861ef94f0c9dc0de6.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/00e8ba87fafbb39861ef94f0c9dc0de6.png)'
- en: Multi-step process for creating and refining an LLM (from [11, 12])
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 创建和完善LLM的多步骤过程（摘自 [11, 12]）
- en: The first step in this process — pre-training — trains the model over a large
    corpus of unlabeled text using a language modeling objective and is typically
    the most expensive. After this, the model undergoes an alignment process, comprised
    of supervised fine-tuning (SFT) and/or reinforcement learning from human feedback
    (RLHF). After the model has been trained, it can be deployed to a downstream application,
    where further fine-tuning or in-context learning can be leveraged to improve performance.
    Within this section, we will overview each of these components to better understand
    their impact on an LLM’s behavior.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 这个过程的第一步——预训练——是通过语言建模目标在大规模的未标记文本语料库上训练模型，通常是最昂贵的步骤。之后，模型会经过一个对齐过程，包括监督微调（SFT）和/或来自人类反馈的强化学习（RLHF）。在模型训练完成后，可以将其部署到下游应用中，在那里可以利用进一步的微调或上下文学习来提高性能。在本节中，我们将概述这些组件，以更好地理解它们对大型语言模型（LLM）行为的影响。
- en: Language Model Pre-Training
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 语言模型预训练
- en: '![](../Images/d02148486a9a483ca5962319e8b7dc5e.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d02148486a9a483ca5962319e8b7dc5e.png)'
- en: (created by author)
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: （作者创建）
- en: The pre-training process — shown above — is the most computationally expensive
    step in the creation of an LLM. During this process, language models are exposed
    to a corpus of unlabeled textual data and trained using a standard language modeling
    objective. Put simply, this means that we train the model by *i)* sampling some
    text from the dataset and *ii)* training the model to predict the next word. This
    pre-training procedure is a form of self-supervised learning, as the correct “next”
    word can be determined by simply looking at the next word in the dataset. The
    pre-training process is extensive given that the dataset is large (e.g., ~0.5–2
    trillion tokens [13]) and the model must be trained from scratch.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 上述的预训练过程是创建大型语言模型（LLM）中计算最昂贵的一步。在此过程中，语言模型会接触到未标记的文本数据集，并使用标准的语言建模目标进行训练。简而言之，这意味着我们通过
    *i)* 从数据集中采样一些文本以及 *ii)* 训练模型以预测下一个单词来训练模型。这一预训练过程是一种自监督学习形式，因为正确的“下一个”单词可以通过简单地查看数据集中的下一个单词来确定。考虑到数据集庞大（例如，~0.5–2万亿标记
    [13]）且模型必须从头开始训练，预训练过程是广泛的。
- en: The Alignment Process
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 对齐过程
- en: '![](../Images/17760c0dcba8932e5693232ede3c2e10.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/17760c0dcba8932e5693232ede3c2e10.png)'
- en: (from [11])
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: （摘自 [11]）
- en: 'After pre-training is complete, we have a “base model”, or a generic LLM that
    does not yet possess any specialized abilities. To endow this model with the ability
    to conduct interesting conversations, follow instructions, and more, we must align
    this model, or train it to replicate behavior desired by a human user. In most
    cases, the alignment procedure is based upon two primary techniques:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在预训练完成后，我们拥有一个“基础模型”，即一个尚未具备任何专门能力的通用 LLM。为了赋予这个模型进行有趣对话、遵循指令等能力，我们必须对齐这个模型，或者训练它以复制人类用户期望的行为。在大多数情况下，对齐过程基于两种主要技术：
- en: Supervised Fine-Tuning (SFT)
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 监督微调（SFT）
- en: Reinforcement Learning from Human Feedback (RLHF)
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 人类反馈中的强化学习（RLHF）
- en: These techniques can either be used individually or combined together by performing
    one after the other, as was originally proposed by InstructGPT [11] (i.e., the
    predecessor to ChatGPT).
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 这些技术可以单独使用，也可以通过依次执行将它们组合在一起，正如 InstructGPT [11]（即 ChatGPT 的前身）最初提议的那样。
- en: '**SFT** is a simple alignment approach — we just obtain examples of desired
    behavior and directly fine-tune the LLM (using a language modeling objective)
    on this data. For example, if we want to teach a base LLM to follow directions,
    we can obtain many examples of accurate responses to instruction-based prompts,
    and train the base model over these examples. This technique, which we will focus
    upon in this overview, is both simple and powerful. As we will see, however, *getting
    good results with SFT is dependent upon curating a high-quality dataset*.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '**SFT** 是一种简单的对齐方法——我们只需获取期望行为的示例，并直接在这些数据上对 LLM 进行微调（使用语言建模目标）。例如，如果我们想教一个基础
    LLM 遵循指令，我们可以获取许多对基于指令的提示做出准确回应的示例，然后在这些示例上训练基础模型。我们将在本概述中重点讨论这一技术，它既简单又强大。然而，正如我们将看到的，*使用
    SFT 获得良好结果依赖于精心策划的数据集*。'
- en: '![](../Images/c7e939f59ef3fde30e8b2b94b1e531ff.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c7e939f59ef3fde30e8b2b94b1e531ff.png)'
- en: (from [11])
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: （来自 [11]）
- en: '**RLHF** provides us with the ability to optimize an LLM’s parameters based
    on feedback provided by humans. Starting with a set of prompts, we first use the
    LLM to generate several potential outputs for each prompt. Given these outputs,
    when can ask human annotators to rank the quality of these responses (i.e., which
    output is the “best”), then use these rankings to train a reward model — this
    is just a smaller LLM that predicts human preference given a model’s response[1](https://cameronrwolfe.substack.com/p/data-is-the-foundation-of-language#footnote-1-134561977);
    see above. We can use the reward model’s output as a scalar reward and optimize
    the LLM to maximize this reward via the PPO algorithm; see below.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '**RLHF** 使我们能够根据人类提供的反馈来优化 LLM 的参数。从一组提示开始，我们首先使用 LLM 为每个提示生成多个潜在输出。根据这些输出，我们可以要求人工标注员对这些回应的质量进行排名（即，哪个输出是“最佳”），然后使用这些排名来训练奖励模型——这只是一个较小的
    LLM，它根据模型的回应预测人类的偏好[1](https://cameronrwolfe.substack.com/p/data-is-the-foundation-of-language#footnote-1-134561977)；见上文。我们可以使用奖励模型的输出作为标量奖励，并通过
    PPO 算法优化 LLM 以最大化这一奖励；见下文。'
- en: '![](../Images/bd42a073e12ab62d8456c8757b891163.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bd42a073e12ab62d8456c8757b891163.png)'
- en: (from [11])
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: （来自 [11]）
- en: The beauty of the RLHF process described above is that we are directly optimizing
    the model based on human preference, but “preferences” can capture a variety of
    different properties! For example, maybe we want the model to follow instructions
    better, output more interesting content, or even stop hallucinating (i.e., making
    up false information). All of these behaviors can be optimized with the use of
    RLHF, making it a very robust alignment tool; see below.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 上述 RLHF 过程的美妙之处在于我们直接基于人类偏好来优化模型，但“偏好”可以捕捉各种不同的属性！例如，也许我们希望模型更好地遵循指令，输出更有趣的内容，甚至停止幻想（即，编造虚假信息）。所有这些行为都可以通过使用
    RLHF 进行优化，使其成为一种非常强大的对齐工具；见下文。
- en: '![](../Images/b02ed5ead26119e9219b860265e0b0be.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b02ed5ead26119e9219b860265e0b0be.png)'
- en: (from [11])
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: （来自 [11]）
- en: For those not familiar with reinforcement learning (RL), RLHF may be a difficult
    framework to understand without some additional background reading. To gain a
    better background in RL, I’d recommend checking out the resources below.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 对于那些不熟悉强化学习（RL）的人来说，没有额外背景阅读，RLHF 可能是一个难以理解的框架。为了获得更好的 RL 背景，我建议查看以下资源。
- en: Basics of Reinforcement Learning [[link](https://www.synopsys.com/ai/what-is-reinforcement-learning.html)]
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 强化学习基础 [[link](https://www.synopsys.com/ai/what-is-reinforcement-learning.html)]
- en: Proximal Policy Optimization [[link](https://openai.com/research/openai-baselines-ppo)]
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 近端策略优化 [[link](https://openai.com/research/openai-baselines-ppo)]
- en: Overview of RLHF [[link](https://huggingface.co/blog/rlhf)]
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RLHF 概述 [[link](https://huggingface.co/blog/rlhf)]
- en: “The model’s capabilities seem to come primarily from the pre-training process
    — RLHF does not improve exam performance (without active effort, it actually degrades
    it). But steering of the model comes from the post-training process — the base
    model requires prompt engineering to even know that it should answer the questions.”
    *— from GPT-4 blog*
  id: totrans-41
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “模型的能力主要来自预训练过程——RLHF 并不会提升考试表现（如果没有积极的努力，它实际上会降低表现）。但模型的引导来自于后训练过程——基础模型需要提示工程才能知道它应该回答问题。”
    *— 引自 GPT-4 博客*
- en: '**What is the purpose of alignment?** Alignment is an incredibly active area
    of research. Currently, there is a discussion within the research community around
    better understanding the role/purposes of alignment. In the analysis of GPT-4,
    we see that the role of alignment techniques like RLHF is to make the LLM more
    steerable and interesting, rather than to teach the model new information. In
    fact, most knowledge possessed by the model seems to come from pre-training. A
    similar argument is made in [1], where we see that high-quality alignment can
    be achieved via a small, curated dataset for SFT. Such a result is especially
    interesting given the [massive amount](https://openai.com/research/gpt-4) of human
    and computational resources that have been invested into aligning popular proprietary
    models like GPT-4.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '**对齐的目的是什么？** 对齐是一个极其活跃的研究领域。目前，研究社区正在讨论更好地理解对齐的角色/目的。在 GPT-4 的分析中，我们看到对齐技术如
    RLHF 的作用是使 LLM 更易于引导和有趣，而不是教给模型新的信息。事实上，模型所拥有的大部分知识似乎来自预训练。在 [1] 中也提出了类似的观点，我们看到通过一个小型的精心策划的数据集进行
    SFT 可以实现高质量的对齐。考虑到在对齐流行的专有模型如 GPT-4 上投入的大量人力和计算资源，这一结果尤其有趣。'
- en: Applying the LLM
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 应用 LLM
- en: '![](../Images/b2ac6d5fa56c8a815baab5a0648649c8.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b2ac6d5fa56c8a815baab5a0648649c8.png)'
- en: Different types of learning with an LLM (created by author)
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 与 LLM 的不同学习类型（由作者创建）
- en: Once an LLM has underwent pre-training and alignment, it’s (more or less) ready
    to be used in downstream applications. However, we must take some measures to
    ensure that the LLM solves a particular task accurately. Typically, this is done
    by either *i)* further fine-tuning the model or *ii)* using in-context learning;
    see above.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦一个 LLM 完成了预训练和对齐，它就（或多或少）准备好用于下游应用。然而，我们必须采取一些措施来确保 LLM 能够准确解决特定任务。通常，这可以通过
    *i)* 进一步微调模型或 *ii)* 使用上下文学习来完成；见上文。
- en: '**Domain specific fine-tuning.** If we are deploying an LLM into a specialized
    domain (e.g., medical, legal, software, etc.), then it might make sense to further
    fine-tune the model over the types of data that it will see in this domain. This
    process is quite simple. We just continue to train the model using a language
    modeling objective, but we use a domain-specific corpus (i.e., data that is similar
    to what will be seen in the desired application) instead of the pre-training dataset.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '**领域特定的微调。** 如果我们将 LLM 部署到一个专业领域（例如医疗、法律、软件等），那么进一步微调模型以适应该领域的数据类型可能是有意义的。这个过程非常简单。我们继续使用语言建模目标训练模型，但使用领域特定的语料库（即与所需应用中将看到的数据类似的数据）来代替预训练数据集。'
- en: '**In-context learning.** Once we are ready to deploy the model (with or without
    domain specific fine-tuning), we should leverage in-context learning, which uses
    textual prompts that instruct/guide the model towards desired behavior, to more
    accurately solve downstream tasks. These prompts may include examples of correct
    solutions (i.e., few-shot exemplars), but this data is only used by the model
    as context when generating output (i.e., we do not use it for training). We have
    explored prompting approaches extensively within prior overviews.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '**上下文学习。** 一旦我们准备好部署模型（无论是否经过领域特定的微调），我们应利用上下文学习，这种方法使用文本提示来指导/引导模型朝向期望的行为，以更准确地解决下游任务。这些提示可能包括正确解决方案的示例（即少量示例），但这些数据仅作为生成输出时的上下文使用（即，我们不用于训练）。我们在之前的综述中已经广泛探讨了提示方法。'
- en: Practical Prompt Engineering [[link](/practical-prompt-engineering-74e96130abc4)]
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实用提示工程 [[link](/practical-prompt-engineering-74e96130abc4)]
- en: Advanced Prompt Engineering [[link](/advanced-prompt-engineering-f07f9e55fe01)]
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 高级提示工程 [[link](/advanced-prompt-engineering-f07f9e55fe01)]
- en: 'LIMA: Less is More for Alignment'
  id: totrans-51
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 'LIMA: 对齐中的少即是多'
- en: In [1], authors study the relative importance of pre-training versus alignment
    by training LIMA, a derivative of LLaMA-65B [2] that undergoes SFT (without RLHF)
    over a curated alignment dataset. In particular, the fine-tuning set used by LIMA
    is a small set of 1,000 carefully curated prompt-and-response examples with a
    similar output style and diverse inputs; see below.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [1] 中，作者通过训练 LIMA（一种 LLaMA-65B [2] 的衍生模型），对比了预训练与对齐的相对重要性。LIMA 经过了针对策划对齐数据集的
    SFT（没有 RLHF）。特别是，LIMA 使用的微调数据集是由 1,000 个精心策划的提示和回答示例组成，这些示例具有相似的输出风格和多样化的输入；见下文。
- en: '![](../Images/792a78dfc3a503893647c0ed688f8e10.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/792a78dfc3a503893647c0ed688f8e10.png)'
- en: (from [1])
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: (来自 [1])
- en: When trained on these examples, we see that LIMA performs quite well and even
    approaches the performance of state-of-the-art proprietary models like GPT-4 and
    Claude. Such a result reveals that language models can be effectively aligned
    via a small number of carefully chosen examples, *thus emphasizing the role of
    data quality and diversity in training and aligning powerful language models*.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 当在这些示例上训练时，我们看到 LIMA 的表现非常好，甚至接近于最先进的专有模型，如 GPT-4 和 Claude。这样的结果表明，通过少量精心选择的示例，语言模型可以被有效对齐，*从而强调了数据质量和多样性在训练和对齐强大语言模型中的作用*。
- en: “A model’s knowledge and capabilities are learnt almost entirely during pretraining,
    while alignment teaches it which subdistribution of formats should be used when
    interacting with users.” *— from [1]*
  id: totrans-56
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “一个模型的知识和能力几乎完全是在预训练期间学到的，而对齐则教会它在与用户交互时应该使用哪些格式的子分布。” *— 来自 [1]*
- en: '**The Superficial Alignment Hypothesis.** Along these lines, authors in [1]
    propose the Superficial Alignment Hypothesis (SAH), which is summarized in the
    quote above. Most of an LLM’s core knowledge is learned during pre-training, while
    alignment searches for the proper format or style for surfacing this knowledge.
    The SAH simply states that alignment can be learned in a data efficient manner
    given a set of examples with sufficient quality and diversity.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '**肤浅对齐假说。** 在这方面，[1] 中的作者提出了肤浅对齐假说（SAH），其总结见上述引文。大多数大型语言模型（LLM）的核心知识是在预训练期间学习的，而对齐则寻找展示这些知识的适当格式或风格。SAH
    简单地表示，给定一组具有足够质量和多样性的示例，对齐可以以数据高效的方式学习。'
- en: Curating Data for Alignment
  id: totrans-58
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据策划以实现对齐
- en: The dataset used for alignment in [1] is constructed from a combination of community
    QA forums (e.g., StackExchange, wikiHow, and Reddit) and manually authored examples.
    Unlike recent work that attempts to automate the curation of data for SFT (e.g.,
    [Self-Instruct](https://cameronrwolfe.substack.com/i/125726849/the-self-instruct-framework)),
    we see in [1] that data from both of these sources is carefully (and oftentimes
    manually) filtered for both quality and diversity. Although manual curation takes
    time, it boosts the quality of the resulting dataset, which is found to be highly
    beneficial.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] 中用于对齐的数据集是由社区问答论坛（例如 StackExchange、wikiHow 和 Reddit）与手动编写的示例结合构建的。与近期试图自动化
    SFT 数据策划的工作不同（例如，[Self-Instruct](https://cameronrwolfe.substack.com/i/125726849/the-self-instruct-framework)），我们在
    [1] 中看到，这两种来源的数据都经过了仔细（且通常是手动）筛选，以确保质量和多样性。尽管手动策划需要时间，但它提升了最终数据集的质量，而这种质量被发现非常有益。'
- en: '![](../Images/8f3bc5d1845085e80d951c504ee14bfc.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8f3bc5d1845085e80d951c504ee14bfc.png)'
- en: (from [1])
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: (来自 [1])
- en: '**Sourcing the data.** The breakdown of LIMA’s training data is shown in the
    table above. In the training set, 750 examples are sourced from community QA forums.
    To ensure data quality, these examples are either filtered manually or via “up
    vote” metrics. The remaining 250 examples are manually written by the authors
    — 200 from scratch and 50 from [SuperNaturalInstructions](https://huggingface.co/datasets/andersonbcdefg/supernatural-instructions-2m).
    When manually creating data, the authors maximize diversity and uniformity by
    making sure sure that:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '**数据来源。** LIMA 的训练数据的详细信息见上表。在训练集中，750 个样本来自社区问答论坛。为了确保数据质量，这些样本要么经过人工筛选，要么通过“点赞”指标进行筛选。剩余的
    250 个样本由作者手动编写——其中 200 个是从零开始创建的，50 个则来自[SuperNaturalInstructions](https://huggingface.co/datasets/andersonbcdefg/supernatural-instructions-2m)。在手动创建数据时，作者通过确保以下几点来最大化数据的多样性和一致性：'
- en: Responses are stylistically aligned to the behavior of a helpful AI agent
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 回应的风格与有用的 AI 代理的行为保持一致
- en: Prompts are as diverse as possible
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提示尽可能多样化
- en: Put simply, we want to minimize the amount of noise in the alignment dataset
    (i.e., ensure a uniform style, tone, format, etc.), while making sure that the
    data observed by the LLM has as much diversity and coverage as possible. Notably,
    authors in [1] even include a few malevolent prompts in the alignment data to
    demonstrate how potentially harmful commands can be avoided.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，我们希望尽量减少对齐数据集中噪声的数量（即，确保风格、语气、格式等的一致性），同时确保LLM观察到的数据具有尽可能多的多样性和覆盖范围。值得注意的是，[1]中的作者甚至在对齐数据中包括了一些恶意提示，以展示如何避免潜在的有害命令。
- en: '**Can we automate this?** In recent work on imitation learning with open-source
    LLMs, we usually see that data used for fine-tuning is automatically curated.
    For example, data for SFT can be downloaded from online sources (e.g., [ShareGPT](https://sharegpt.com/))
    or obtained directly from LLM APIs for ChatGPT or GPT-4\. Such an approach is
    incredibly efficient compared to manual curation, and (in some cases) it even
    works well; e.g., Orca [3] is trained over a large number of dialogues obtained
    from ChatGPT/GPT-4 and performs quite well (even compared to top models). However,
    we see in [4] that models trained in this manner typically have limitations and
    perform poorly when subjected to extensive analysis.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '**我们能自动化吗？** 在关于开源LLM的模仿学习的近期工作中，我们通常会看到用于微调的数据是自动策划的。例如，SFT的数据可以从在线来源（例如，[ShareGPT](https://sharegpt.com/)）下载或直接从LLM
    API获取（如ChatGPT或GPT-4）。与手动策展相比，这种方法效率极高，并且（在某些情况下）表现良好；例如，Orca [3] 在从ChatGPT/GPT-4获得的大量对话中进行训练，表现相当好（甚至与顶级模型相比）。然而，在[4]中我们看到，以这种方式训练的模型通常存在局限性，并在经过广泛分析时表现较差。'
- en: “Manually creating diverse prompts and authoring rich responses in a uniform
    style is laborious. While some recent works avoid manual labor via distillation
    and other automatic means, optimizing for quantity over quality, this work explores
    the effects of investing in diversity and quality instead.” *— from [1]*
  id: totrans-67
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “手动创建多样化的提示并以一致的风格撰写丰富的回应是劳动密集型的。虽然一些近期工作通过蒸馏和其他自动化手段避免了手动劳动，优化数量而非质量，但这项工作探索了投资于多样性和质量的效果。”
    *— 来自[1]*
- en: In [1], we study an alternative approach to alignment that invests into the
    curation of high-quality data. Instead of automatically obtaining a large amount
    of data, we manually filter and select fewer examples. This smaller-scale (but
    labor-intensive) selection process allows the diversity and quality of data to
    be controlled, *which illustrates the impact of data scale and quality on LLM
    alignment*.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 在[1]中，我们研究了一种对齐的替代方法，即投入到高质量数据的策展中。与其自动获取大量数据，不如手动筛选和选择较少的示例。这种较小规模（但劳动密集型）的选择过程使数据的多样性和质量得以控制，*这说明了数据规模和质量对LLM对齐的影响*。
- en: Is alignment actually superficial?
  id: totrans-69
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 对齐实际上是肤浅的吗？
- en: '![](../Images/f41d3f2915b10e12838a57c348bfacb6.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f41d3f2915b10e12838a57c348bfacb6.png)'
- en: (from [1])
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: （来自[1]）
- en: LIMA’s performance is compared to that of a variety of different language models.
    In particular, we see LIMA’s performance compared to Alpaca-65B [6], DaVinci003
    (i.e., an RLHF-tuned version of GPT-3.5), Bard (i.e., based on PaLM [7]), Claude
    (i.e., 52B parameter LLM trained via AI feedback [8]), and GPT-4\. Evaluation
    is performed using both crowd workers and automated feedback from GPT-4, as shown
    in the Figure above. Interestingly, LIMA consistently outperforms Alpaca (despite
    being fine-tuned on `52X` less data!) and even matches or exceeds the quality
    of Claude and GPT-4 in a non-trivial number of cases. LIMA’s competitive performance
    is impressive given that these other models are trained over millions of user
    prompts with feedback.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: LIMA的表现与各种不同语言模型的表现进行了比较。特别地，我们将LIMA的表现与Alpaca-65B [6]、DaVinci003（即经过RLHF调优的GPT-3.5版本）、Bard（即基于PaLM
    [7]）、Claude（即通过AI反馈训练的52B参数LLM [8]）和GPT-4进行了比较。评估使用了众包工作者和来自GPT-4的自动反馈，如上图所示。有趣的是，LIMA的表现始终优于Alpaca（尽管其在`52X`少量数据上进行了微调！），甚至在相当多的情况下与Claude和GPT-4的质量相当或超越。考虑到这些其他模型在数百万用户提示和反馈下进行训练，LIMA的竞争力表现令人印象深刻。
- en: '**Absolute performance.** Beyond the model comparison trials conducted above,
    authors in [1] manually evaluated the quality of 50 random responses generated
    by LIMA. Interestingly, we see that LIMA fails to answer only six of the 50 prompts
    and produces an excellent response in 50% of the trials; see below.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '**绝对性能。** 除了上面进行的模型比较试验外，[1]中的作者还手动评估了LIMA生成的50个随机响应的质量。有趣的是，我们发现LIMA仅在50个提示中的六个中未能作答，并且在50%的试验中产生了优秀的响应；见下文。'
- en: '![](../Images/b41716f26cf08c98c6df7ad0da98afd9.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b41716f26cf08c98c6df7ad0da98afd9.png)'
- en: (from [1])
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: (来自 [1])
- en: When this manual evaluation is repeated on exclusively out-of-distribution prompts
    (i.e., those that are much different from examples included in the fine-tuning
    set), the results are not much different — 20% of responses fail, 35% pass, and
    45% are excellent. Such a result indicates that LIMA actually generalizes well
    and is not just memorizing or overfitting to the curated fine-tuning dataset[3](https://cameronrwolfe.substack.com/p/data-is-the-foundation-of-language#footnote-3-134561977).
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 当这种手动评估仅在分布外的提示上重复进行（即那些与微调集中包含的例子差异较大的提示），结果差异不大——20% 的响应失败，35% 的响应合格，45% 的响应优秀。这种结果表明
    LIMA 实际上具有较好的泛化能力，而不仅仅是记忆或过拟合于策划的微调数据集[3](https://cameronrwolfe.substack.com/p/data-is-the-foundation-of-language#footnote-3-134561977)。
- en: '![](../Images/d2be9a91780ffd4f56211777808618c0.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d2be9a91780ffd4f56211777808618c0.png)'
- en: (from [1])
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: (来自 [1])
- en: For example, LIMA can perform multi-turn dialogue relatively well (but not great)
    despite having no such examples in its fine-tuning dataset. When just 30 multi-turn
    dialogue examples are exposed to the model, we see that LIMA quickly learns how
    to maintain a dialogue from these examples; see above.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，LIMA 在多轮对话方面表现得相对较好（但不出色），尽管在其微调数据集中没有这样的例子。当模型仅接触到 30 个多轮对话例子时，我们看到 LIMA
    能够快速学习如何从这些例子中维持对话；见上文。
- en: '![](../Images/9d9f26bc77fb5dc989d22dd8e9a63f35.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9d9f26bc77fb5dc989d22dd8e9a63f35.png)'
- en: (from [1])
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: (来自 [1])
- en: '**Useful properties of data.** Beyond the main results outlined above, we see
    in the ablation experiments of [1] that the diversity and quality of examples
    used for alignment is incredibly important; see above. Notably, *just increasing
    the size of the fine-tuning dataset does not always improve the LLM’s performance*.
    As such, we learn from [1] that careful curation of high-quality data for alignment
    is valuable.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '**数据的有用属性。** 除了上述主要结果外，我们在 [1] 的消融实验中看到，用于对齐的样本的多样性和质量非常重要；见上文。值得注意的是，*仅仅增加微调数据集的大小并不总是能提高
    LLM 的性能*。因此，我们从 [1] 中了解到，高质量数据的精心策划对于对齐是非常有价值的。'
- en: The Bigger Picture
  id: totrans-83
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 更大的图景
- en: 'In recent work on open-source language models, we have seen a variety of different
    LLMs (e.g., Alpaca [6], Vicuna [9], Koala [10] and more) that have adopted an
    automatic approach for curating data for SFT. In particular, these model use an
    imitation approach that *i)* collects a large amount of dialogues from other LLMs
    and *ii)* performs supervised fine-tuning over this data. Although these models
    initially seemed to perform quite well, we see in more targeted evaluations that
    the quality of their alignment is poor. With this in mind, we might reasonably
    ask: *What makes LIMA’s approach more effective?*'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在最近的开源语言模型研究中，我们看到了一系列不同的 LLM（例如，Alpaca [6]、Vicuna [9]、Koala [10] 等）采用了自动化的数据策划方法进行
    SFT。特别是，这些模型使用了一种模仿方法，其中 *i)* 从其他 LLM 收集大量对话，*ii)* 对这些数据进行监督微调。虽然这些模型最初表现得相当不错，但我们在更有针对性的评估中看到它们的对齐质量较差。考虑到这一点，我们可以合理地问：*是什么使得
    LIMA 的方法更有效？*
- en: '![](../Images/8f654f01df91cf207cb31421d8111c2e.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8f654f01df91cf207cb31421d8111c2e.png)'
- en: (from [4])
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: (来自 [4])
- en: '**Quality > Quantity.** Even in studies on imitation models, we see that just
    increasing the amount of data in the fine-tuning set yields minimal impact on
    the underlying model’s performance; see above. We see in [1] that similar results
    are observed for LIMA. Given that increasing the amount of data alone yields no
    benefit, we a have a few different options for improving an LLM’s performance:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '**质量 > 数量。** 即使在模仿模型的研究中，我们也发现仅仅增加微调集中的数据量对模型性能的提升影响有限；见上文。我们在 [1] 中看到类似的结果对于
    LIMA 也是如此。鉴于仅增加数据量没有好处，我们有几个不同的选项来提升 LLM 的性能：'
- en: Create a more powerful base model
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个更强大的基础模型
- en: Improve the alignment dataset
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 改进对齐数据集
- en: While several works (e.g., MPT and Falcon) have explored the creation of better
    base models, LIMA studies how better alignment datasets can be created[4](https://cameronrwolfe.substack.com/p/data-is-the-foundation-of-language#footnote-4-134561977).
    Put simply, we see in [1] that creating an alignment dataset that is both diverse
    and high-quality (even if it is small!) is extremely effective. LLMs can accurately
    learn to emulate certain behaviors based on minimal data, which supports the SAH.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然一些工作（例如MPT和Falcon）已经探讨了更好的基础模型的创建，但LIMA研究了如何创建更好的对齐数据集[4](https://cameronrwolfe.substack.com/p/data-is-the-foundation-of-language#footnote-4-134561977)。简单来说，我们在
    [1] 中看到，创建一个既多样化又高质量的对齐数据集（即使它很小！）是极其有效的。LLMs可以根据最少的数据准确地学习模拟某些行为，这支持了SAH。
- en: “For the purpose of alignment, scaling up input diversity and output quality
    have measurable positive effects, while scaling up quantity alone might not.”
    *— from [1]*
  id: totrans-91
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “为了对齐的目的，扩大输入多样性和输出质量具有可测量的积极效果，而单独扩大数量可能没有。” *— 来自 [1]*
- en: '**But, these results aren’t perfect!** Prior imitation models were initially
    thought to perform incredibly well, even comparably to top proprietary models
    like ChatGPT. However, we later discovered that such conclusions were a product
    of human error. These models mimicked the style of proprietary LLMS, but lacked
    their factuality and tended to generalize poorly beyond their training sets, which
    is more difficult for humans to deduce when evaluating these models. Given that
    LIMA is also primarily evaluated with crowd workers, the results in [1] are subject
    to similar limitations. However, we see that LIMA tends to generalize well and
    oftentimes outperforms imitation models like Alpaca, which indicates that high-quality
    alignment data is still incredibly beneficial to LLM performance.'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '**但这些结果并不完美！** 之前的模仿模型最初被认为表现非常出色，甚至可以与像ChatGPT这样的顶级专有模型相媲美。然而，我们后来发现这些结论是由于人为错误所致。这些模型模仿了专有LLMS的风格，但缺乏其真实性，并且在训练集之外的泛化表现较差，这使得在评估这些模型时，人类更难以推断。鉴于LIMA也是主要通过众包工人进行评估的，[1]中的结果也存在类似的局限性。然而，我们看到LIMA往往具有良好的泛化能力，并且常常优于像Alpaca这样的模仿模型，这表明高质量的对齐数据仍然对LLM性能非常有益。'
- en: The Impact of Data Beyond Alignment
  id: totrans-93
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据对齐之外的影响
- en: Within [1], we see that the quality of data is incredibly important for effectively
    aligning a language model. However, the importance of data quality and diversity
    goes beyond alignment alone — *the type and quality of data being used impacts
    every aspect of the LLM training pipeline*. Let’s look at a few examples for reference.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [1] 中，我们看到数据的质量对于有效地对齐语言模型极其重要。然而，数据质量和多样性的重要性不仅仅局限于对齐 —— *数据的类型和质量会影响LLM训练流程的各个方面*。让我们来看几个参考例子。
- en: '**Pre-training.** Across a variety of different models, we have seen that the
    quality of data used for pre-training is incredibly important. For example, within
    Galactica [14], authors find that training on a smaller, heavily-curated dataset
    of high-quality scientific information yields the best possible performance. Similarly,
    the BioMedLM model is pre-trained over a smaller, curated corpus of technical
    content. Finally, Falcon-40B — *the current state-of-the-art for open-source language
    models* — places a notable emphasis on the quality of pre-training data, where
    we see that authors have invested significant effort into developing a novel and
    efficient pipeline for extracting high-quality pre-training data from the web.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '**预训练。** 在各种不同的模型中，我们看到用于预训练的数据质量至关重要。例如，在Galactica [14] 中，作者发现使用较小的、高度策划的高质量科学信息数据集进行训练可以获得最佳性能。类似地，BioMedLM模型是在较小的、策划过的技术内容语料库上进行预训练的。最后，Falcon-40B
    —— *目前开源语言模型的最先进水平* —— 给予了预训练数据质量显著的重视，我们看到作者投入了大量精力来开发一个新颖且高效的管道，从网络中提取高质量的预训练数据。'
- en: '**Alignment.** Beyond the approach explored in [1], the recently-proposed Orca
    model [3] heavily studies the role of data quality in solving the alignment problem.
    However, a slightly different approach is adopted. Namely, the authors train a
    model using an imitation approach but augment the data used for SFT (i.e., dialogue
    examples with other LLMs) with detailed information from the model about how each
    problem is solved. Including these extra details in the alignment dataset is found
    to produce imitation models that are much more robust compared to models like
    Alpaca [6] or Vicuna [9].'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '**对齐。** 除了[1]中探讨的方法外，最近提出的Orca模型[3]重点研究了数据质量在解决对齐问题中的作用。然而，采用了一种略有不同的方法。即，作者使用模仿方法训练模型，但将用于SFT的数据（即与其他LLM的对话示例）与模型关于每个问题如何解决的详细信息进行增强。包括这些额外细节在对齐数据集中被发现能够生成比像Alpaca[6]或Vicuna[9]这样的模型更为强健的模仿模型。'
- en: '**In-context learning.** Beyond training and fine-tuning the LLM, the data
    used for in-context/few-shot learning can massively impact performance. In particular,
    recent research on few-shot learning shows us that factors such as the ordering,
    distribution, or format of exemplars can impact a model’s performance. Going further,
    we see that the diversity of data is incredibly important, where models that are
    prompted with a diverse group few-shot exemplars tend to perform better.'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '**上下文学习。** 除了对LLM进行训练和微调外，用于上下文/少量示例学习的数据可以极大地影响性能。特别是，最近关于少量示例学习的研究表明，示例的排序、分布或格式等因素都可能影响模型的表现。进一步而言，我们看到数据的多样性是极其重要的，使用多样化的少量示例提示模型通常表现得更好。'
- en: Closing Remarks
  id: totrans-98
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结束语
- en: “These results strongly suggest that almost all knowledge in large language
    models is learned during pretraining, and only limited instruction tuning data
    is necessary to teach models to produce high quality output.” *— from [1]*
  id: totrans-99
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “这些结果强烈表明，大型语言模型中的几乎所有知识都在预训练期间学习，只有有限的指令调优数据才有必要教会模型生成高质量的输出。” *— 来自[1]*
- en: 'The major conclusions from work covered within this overview are twofold:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 本概述中涉及的主要结论有两点：
- en: '*The Superficial Alignment Hypothesis*: LLMs learn their knowledge during pre-training
    and alignment teaches them how to properly interact with users.'
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*表面对齐假设*：LLMs在预训练期间学习他们的知识，而对齐则教会它们如何与用户进行适当的互动。'
- en: The quality and diversity of data is incredibly important to the alignment process
    (much more so that data scale).
  id: totrans-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 数据的质量和多样性对对齐过程至关重要（比数据规模更为重要）。
- en: 'Within [1], we have observed these major conclusions in the creation of LIMA,
    where high-quality alignment can be performed using a smaller, curated corpus.
    Not much data is needed for alignment (with SFT) if the data is of sufficient
    quality, meaning that input prompts are diverse and responses have a standardized
    structure and tone. However, the positive impact of high-quality data spans far
    beyond alignment — *all aspects of LLM training are positively benefitted by the
    use of higher-quality data*. Whether it be during pre-training or in-context learning,
    language models are still fundamentally subject to the same basic rule as all
    other machine learning models: “garbage in, garbage out”.'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 在[1]中，我们在创建LIMA的过程中观察到了这些主要结论，其中高质量的对齐可以通过较小的、精心策划的语料库来实现。如果数据的质量足够高，即输入提示多样且响应具有标准化的结构和语气，则对齐（使用SFT）所需的数据量不多。然而，高质量数据的积极影响远远超出了对齐——*所有LLM训练的方面都从使用高质量数据中受益*。无论是在预训练还是上下文学习中，语言模型仍然基本遵循与其他所有机器学习模型相同的基本规则：“垃圾进，垃圾出”。
- en: Connect with me!
  id: totrans-104
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 与我联系！
- en: Thanks so much for reading this article. I am [Cameron R. Wolfe](https://cameronrwolfe.me/),
    Director of AI at [Rebuy](https://www.rebuyengine.com/). I study the empirical
    and theoretical foundations of deep learning. If you liked this overview, subscribe
    to my [Deep (Learning) Focus newsletter](https://cameronrwolfe.substack.com/),
    where I help readers understand AI research via overviews of relevant topics from
    the ground up. You can also follow me on [X](https://twitter.com/cwolferesearch)
    and [LinkedIn](https://www.linkedin.com/in/cameron-r-wolfe-ph-d-04744a238/), or
    check out my [other writings](https://medium.com/@wolfecameron) on medium!
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 非常感谢您阅读这篇文章。我是 [Cameron R. Wolfe](https://cameronrwolfe.me/)，[Rebuy](https://www.rebuyengine.com/)
    的 AI 总监。我研究深度学习的经验和理论基础。如果您喜欢这个概述，可以订阅我的 [Deep (Learning) Focus 新闻通讯](https://cameronrwolfe.substack.com/)，我会通过从基础开始的相关主题概述来帮助读者理解
    AI 研究。您还可以在 [X](https://twitter.com/cwolferesearch) 和 [LinkedIn](https://www.linkedin.com/in/cameron-r-wolfe-ph-d-04744a238/)
    上关注我，或查看我在 medium 上的 [其他著作](https://medium.com/@wolfecameron)！
- en: Bibliography
  id: totrans-106
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] Zhou, Chunting, et al. “Lima: Less is more for alignment.” *arXiv preprint
    arXiv:2305.11206* (2023).'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] Zhou, Chunting 等人. “Lima: 对齐问题中的简约主义。” *arXiv 预印本 arXiv:2305.11206* (2023)。'
- en: '[2] Touvron, Hugo, et al. “Llama: Open and efficient foundation language models.”
    *arXiv preprint arXiv:2302.13971* (2023).'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] Touvron, Hugo 等人. “Llama: 开放和高效的基础语言模型。” *arXiv 预印本 arXiv:2302.13971* (2023)。'
- en: '[3] Mukherjee, Subhabrata, et al. “Orca: Progressive Learning from Complex
    Explanation Traces of GPT-4.” *arXiv preprint arXiv:2306.02707* (2023).'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] Mukherjee, Subhabrata 等人. “Orca: 通过 GPT-4 的复杂解释轨迹进行渐进学习。” *arXiv 预印本 arXiv:2306.02707*
    (2023)。'
- en: '[4] Gudibande, Arnav, et al. “The false promise of imitating proprietary llms.”
    *arXiv preprint arXiv:2305.15717* (2023).'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] Gudibande, Arnav 等人. “模仿专有语言模型的虚假承诺。” *arXiv 预印本 arXiv:2305.15717* (2023)。'
- en: '[5] Wang, Yizhong, et al. “Super-naturalinstructions: Generalization via declarative
    instructions on 1600+ nlp tasks.” *arXiv preprint arXiv:2204.07705* (2022).'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '[5] Wang, Yizhong 等人. “Super-naturalinstructions: 通过声明性指令在 1600+ NLP 任务中进行泛化。”
    *arXiv 预印本 arXiv:2204.07705* (2022)。'
- en: '[6] Taori, Rohan et al. “Stanford Alpaca: An Instruction-following LLaMA model.”
    (2023).'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '[6] Taori, Rohan 等人. “Stanford Alpaca: 一个跟随指令的 LLaMA 模型。” (2023)。'
- en: '[7] Chowdhery, Aakanksha, et al. “Palm: Scaling language modeling with pathways.”
    *arXiv preprint arXiv:2204.02311* (2022).'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '[7] Chowdhery, Aakanksha 等人. “Palm: 通过路径扩展语言建模。” *arXiv 预印本 arXiv:2204.02311*
    (2022)。'
- en: '[8] Bai, Yuntao, et al. “Constitutional ai: Harmlessness from ai feedback.”
    *arXiv preprint arXiv:2212.08073* (2022).'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '[8] Bai, Yuntao 等人. “Constitutional ai: 从 AI 反馈中获得的无害性。” *arXiv 预印本 arXiv:2212.08073*
    (2022)。'
- en: '[9] Chiang, Wei-Lin et al. “Vicuna: An Open-Source Chatbot Impressing GPT-4
    with 90%* ChatGPT Quality.” (2023).'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '[9] Chiang, Wei-Lin 等人. “Vicuna: 一个开源聊天机器人，以 90%* ChatGPT 质量打动 GPT-4。” (2023)。'
- en: '[10] Geng, Xinyang et al. “Koala: A Dialogue Model for Academic Research.”
    (2023).'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '[10] Geng, Xinyang 等人. “Koala: 一个用于学术研究的对话模型。” (2023)。'
- en: '[11] Ouyang, Long, et al. “Training language models to follow instructions
    with human feedback.” *Advances in Neural Information Processing Systems* 35 (2022):
    27730–27744.'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '[11] Ouyang, Long 等人. “训练语言模型以通过人类反馈遵循指令。” *神经信息处理系统进展* 35 (2022): 27730–27744。'
- en: '[12] Glaese, Amelia, et al. “Improving alignment of dialogue agents via targeted
    human judgements.” *arXiv preprint arXiv:2209.14375* (2022).'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '[12] Glaese, Amelia 等人. “通过针对性的人类评判改善对话代理的对齐。” *arXiv 预印本 arXiv:2209.14375*
    (2022)。'
- en: '[13] Hoffmann, Jordan, et al. “Training compute-optimal large language models.”
    *arXiv preprint arXiv:2203.15556* (2022).'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '[13] Hoffmann, Jordan 等人. “训练计算最优的大型语言模型。” *arXiv 预印本 arXiv:2203.15556* (2022)。'
- en: '[14] Taylor, Ross, et al. “Galactica: A large language model for science.”
    *arXiv preprint arXiv:2211.09085* (2022).'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '[14] Taylor, Ross 等人. “Galactica: 一个用于科学的大型语言模型。” *arXiv 预印本 arXiv:2211.09085*
    (2022)。'
