- en: Using Large Language Models as Recommendation Systems
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用大型语言模型作为推荐系统
- en: 原文：[https://towardsdatascience.com/using-large-language-models-as-recommendation-systems-49e8aeeff29b?source=collection_archive---------0-----------------------#2023-04-10](https://towardsdatascience.com/using-large-language-models-as-recommendation-systems-49e8aeeff29b?source=collection_archive---------0-----------------------#2023-04-10)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/using-large-language-models-as-recommendation-systems-49e8aeeff29b?source=collection_archive---------0-----------------------#2023-04-10](https://towardsdatascience.com/using-large-language-models-as-recommendation-systems-49e8aeeff29b?source=collection_archive---------0-----------------------#2023-04-10)
- en: A review of recent research and a custom implementation
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 最近研究的综述及自定义实现
- en: '[](https://medium.com/@mohammadhia?source=post_page-----49e8aeeff29b--------------------------------)[![Mohamad
    Aboufoul](../Images/af1edb6a878499e693b398c3bda7a0c5.png)](https://medium.com/@mohammadhia?source=post_page-----49e8aeeff29b--------------------------------)[](https://towardsdatascience.com/?source=post_page-----49e8aeeff29b--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----49e8aeeff29b--------------------------------)
    [Mohamad Aboufoul](https://medium.com/@mohammadhia?source=post_page-----49e8aeeff29b--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@mohammadhia?source=post_page-----49e8aeeff29b--------------------------------)[![Mohamad
    Aboufoul](../Images/af1edb6a878499e693b398c3bda7a0c5.png)](https://medium.com/@mohammadhia?source=post_page-----49e8aeeff29b--------------------------------)[](https://towardsdatascience.com/?source=post_page-----49e8aeeff29b--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----49e8aeeff29b--------------------------------)
    [Mohamad Aboufoul](https://medium.com/@mohammadhia?source=post_page-----49e8aeeff29b--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F48bf72143c5f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fusing-large-language-models-as-recommendation-systems-49e8aeeff29b&user=Mohamad+Aboufoul&userId=48bf72143c5f&source=post_page-48bf72143c5f----49e8aeeff29b---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----49e8aeeff29b--------------------------------)
    ·8 min read·Apr 10, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F49e8aeeff29b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fusing-large-language-models-as-recommendation-systems-49e8aeeff29b&user=Mohamad+Aboufoul&userId=48bf72143c5f&source=-----49e8aeeff29b---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F48bf72143c5f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fusing-large-language-models-as-recommendation-systems-49e8aeeff29b&user=Mohamad+Aboufoul&userId=48bf72143c5f&source=post_page-48bf72143c5f----49e8aeeff29b---------------------post_header-----------)
    发布于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----49e8aeeff29b--------------------------------)
    · 8分钟阅读 · 2023年4月10日 [](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F49e8aeeff29b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fusing-large-language-models-as-recommendation-systems-49e8aeeff29b&user=Mohamad+Aboufoul&userId=48bf72143c5f&source=-----49e8aeeff29b---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F49e8aeeff29b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fusing-large-language-models-as-recommendation-systems-49e8aeeff29b&source=-----49e8aeeff29b---------------------bookmark_footer-----------)![](../Images/0777460e6a57934a4dced98e05c6b490.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F49e8aeeff29b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fusing-large-language-models-as-recommendation-systems-49e8aeeff29b&source=-----49e8aeeff29b---------------------bookmark_footer-----------)![](../Images/0777460e6a57934a4dced98e05c6b490.png)'
- en: Photo by [Luca Baggio](https://unsplash.com/@luca42?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：[卢卡·巴吉奥](https://unsplash.com/@luca42?utm_source=medium&utm_medium=referral)
    于 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: Large Language Models (LLMs) have taken the data science community and the news
    cycle by storm these past few months. Since the advent of the transformer architecture
    in 2017, we’ve seen exponential advancements in the complexity of natural language
    tasks that these models can tackle from classification, to intent & sentiment
    extraction, to generating text eerily similar to humans.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，大型语言模型（LLMs）在数据科学界和新闻界引起了极大的关注。自2017年变压器架构问世以来，我们见证了这些模型在自然语言任务复杂性方面的指数级进步，包括分类、意图与情感提取，以及生成与人类相似的文本。
- en: From an application standpoint, the possibilities seem endless when combining
    LLMs with various existing technologies, to cover their pitfalls (one of my favorite
    being the [GPT + Wolfram Alpha combo](https://writings.stephenwolfram.com/2023/03/chatgpt-gets-its-wolfram-superpowers/)
    to handle math and symbolic reasoning problems).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 从应用角度来看，将 LLMs 与各种现有技术结合使用，来弥补它们的缺陷（我最喜欢的之一是[GPT + Wolfram Alpha 组合](https://writings.stephenwolfram.com/2023/03/chatgpt-gets-its-wolfram-superpowers/)
    用于处理数学和符号推理问题），其可能性似乎无穷无尽。
- en: But what surprised me was that LLMs can also be used as recommendation systems,
    in and of themselves, without the need for any additional feature engineering
    or other manual processes that go into common rec systems. This capability is
    largely due to the nature of how LLMs are pre-trained and how they operate.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 但令我惊讶的是，LLMs 也可以作为推荐系统单独使用，而无需任何额外的特征工程或其他常见推荐系统中的手动过程。这种能力主要归功于 LLMs 的预训练方式及其操作方式。
- en: Table of Contents
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 目录
- en: '***A Recap of LLMs and How Transformers Work***'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***LLMs 和变换器如何工作的回顾***'
- en: '***LLMs as Recommendation Systems***'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***LLMs 作为推荐系统***'
- en: '***Implementing/Replicating P5 with Custom Data***'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***使用自定义数据实现/复制 P5***'
- en: '***Replication Attempt 2 — Arabic***'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***复制尝试 2 — 阿拉伯语***'
- en: '***Benefits and Pitfalls of LLMs as Recommendation Systems***'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***LLMs 作为推荐系统的优点和缺点***'
- en: '***Final Thoughts***'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***最终思考***'
- en: '***Code***'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***代码***'
- en: A Recap of LLMs and How Transformers Work
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LLMs 和变换器如何工作的回顾
- en: 'Language models are probabilistic models that try to map the probability of
    a sequence of tokens (words in a phrase, sentence, etc.) occurring. They’re trained
    on an array of texts, and derive probability distributions accordingly. For the
    various tasks that they can handle (summarization, question-answering, etc.) they
    are iteratively selecting the most probable token/word to continue a prompt, using
    conditional probability. See the example below:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 语言模型是概率模型，试图映射一系列标记（短语、句子等）的发生概率。它们在各种文本上进行训练，并相应地推导概率分布。对于它们可以处理的各种任务（总结、问答等），它们通过条件概率迭代地选择最可能的标记/词汇来继续提示。请参见下面的示例：
- en: '![](../Images/7b434459a3c104cfeaccb6a235198ce3.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7b434459a3c104cfeaccb6a235198ce3.png)'
- en: An example of probabilities of subsequent tokens based on context (Image by
    author)
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 基于上下文的后续标记的概率示例（作者提供的图片）
- en: LLMs are language models that have been trained on MASSIVE amounts of text with
    large architectures that utilize significant amounts of compute. These are commonly
    powered by the transformer architecture, which was introduced in the famous 2017
    paper “[Attention Is All You Need](https://arxiv.org/pdf/1706.03762.pdf)” by Google.
    This architecture utilizes a [“self-attention” mechanism](https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html),
    which allows the model to learn how different tokens relate to one another during
    the pre-training process.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: LLMs 是经过大量文本训练的语言模型，这些模型具有庞大的架构，并利用了大量计算资源。它们通常由变换器架构驱动，这种架构在 Google 的著名 2017
    年论文“[Attention Is All You Need](https://arxiv.org/pdf/1706.03762.pdf)”中被引入。这种架构利用了[“自注意力”机制](https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html)，使得模型能够在预训练过程中学习不同标记之间的关系。
- en: 'After pre-training across a large enough set of texts, similar words will have
    similar embeddings (ex: “King”, “Monarch”) and dissimilar words will have more
    different ones. Additionally, with these embeddings, we’ll see an algebraic mapping
    of words in relation to one another, allowing the model to more powerfully determine
    a correct next token for a sequence.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在对足够大的文本集合进行预训练后，相似的词汇将具有相似的嵌入（例如：“King”，“Monarch”），而不同的词汇将具有更大的差异。此外，利用这些嵌入，我们将看到词汇之间的代数映射，使模型能够更有效地确定序列的正确下一个标记。
- en: Like the classic example of [“King — Man + Woman = Queen”](https://kawine.github.io/blog/nlp/2019/06/21/word-analogies.html)
    in Word2Vec
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 就像经典的[“King — Man + Woman = Queen”](https://kawine.github.io/blog/nlp/2019/06/21/word-analogies.html)
    示例一样
- en: The added benefit of self-attention embeddings is that they will vary for a
    word depending on the words around it, making them more tailored to the meaning
    in that context.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 自注意力嵌入的附加好处是，它们会根据周围的词汇而有所变化，使其更贴合于特定上下文中的含义。
- en: Stanford’s Dr. Christopher Manning gives a great [high-level overview](https://www.youtube.com/watch?v=YfXc4OBDmnM&ab_channel=Databricks)
    of how LLMs work.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 斯坦福大学的克里斯托弗·曼宁博士提供了一个关于 LLMs 如何工作的[高层次概述](https://www.youtube.com/watch?v=YfXc4OBDmnM&ab_channel=Databricks)。
- en: LLMs as Recommendation Systems
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LLMs 作为推荐系统
- en: 'In 2022, researchers from Rutger’s University published the paper **“Recommendation
    as Language Processing (RLP): A Unified Pretrain, Personalized Prompt & Predict
    Paradigm (P5)”** (Geng et. al). In it they introduced a “flexible and unified
    text-to-text paradigm” which combined several recommendation tasks in a single
    system: P5\. This system is capable of performing the following via natural language
    sequences:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 2022年，Rutger’s University 的研究人员发布了论文**《推荐作为语言处理（RLP）：统一预训练、个性化提示与预测范式（P5）》**（Geng
    et. al）。论文中介绍了一种“灵活统一的文本到文本范式”，将多个推荐任务结合在一个系统中：P5。该系统能够通过自然语言序列执行以下操作：
- en: Sequential recommendation
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 顺序推荐
- en: Rating prediction
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 评分预测
- en: Explanation generation
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 解释生成
- en: Review summarization
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 评论总结
- en: Direct recommendation
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 直接推荐
- en: Let’s take a look at an example of the sequential recommendation task from the
    paper.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看看论文中的一个顺序推荐任务示例。
- en: '[PRE0]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The researchers assigned the user and each item a unique ID. Using a training
    set with thousands of users (and their purchase histories) and unique items, the
    LLM is able to learn that certain items are similar to one another and that certain
    users have inclinations towards certain items (due to the nature of the self-attention
    mechanism). During the pre-training process across all these purchase sequences,
    the model essentially goes through a form of [collaborative filtering](https://developers.google.com/machine-learning/recommendation/collaborative/basics).
    It sees what users have purchased the same items and what items tend to be purchased
    together. Combine that with an LLM’s ability to produce contextual embeddings,
    and we suddenly have a very powerful recommendation system.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 研究人员为用户和每个项目分配了唯一的 ID。使用包含数千名用户（及其购买历史）和唯一项目的训练集，LLM 能够学习到某些项目彼此相似以及某些用户对某些项目有倾向（由于自注意力机制的特性）。在所有这些购买序列的预训练过程中，模型本质上经历了一种[协同过滤](https://developers.google.com/machine-learning/recommendation/collaborative/basics)的形式。它查看用户购买了相同的项目以及哪些项目往往一起购买。结合
    LLM 生成上下文嵌入的能力，我们突然拥有了一个非常强大的推荐系统。
- en: In the example above, although we don’t know what item each ID corresponds to,
    we can infer that item “**1581**” was selected due to other users purchasing it
    along with any of the items that “**user_15466**” already purchased.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述示例中，尽管我们不知道每个 ID 对应的项目是什么，但我们可以推断出项“**1581**”被选择的原因是其他用户购买了它，并且与“**user_15466**”已经购买的任何项一起购买了。
- en: With regards to P5’s architecture, it **“utilizes the pretrained T5 checkpoints
    as backbone”** (Geng et. al).
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 关于 P5 的架构，它**“利用预训练的 T5 检查点作为骨干”**（Geng et. al）。
- en: '[T5 is another LLM](https://ai.googleblog.com/2020/02/exploring-transfer-learning-with-t5.html)
    that Google released a few years ago. It was designed to handle multiple types
    of sequence-to-sequence tasks, so it makes sense to use it as a starting point
    for this kind of system.'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[T5 是另一个 LLM](https://ai.googleblog.com/2020/02/exploring-transfer-learning-with-t5.html)，谷歌几年前发布的。它被设计来处理多种类型的序列到序列任务，因此作为这种系统的起点是合理的。'
- en: Implementing/Replicating P5 with Custom Data
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用自定义数据实现/复制 P5
- en: I was really impressed with this paper and wanted to see if I could replicate
    the sequential recommendation capability on a smaller scale. I decided to leverage
    an open-source T5 model from Hugging Face ([T5-large](https://huggingface.co/t5-large))
    and made my own custom dataset to fine-tune it to produce recommendations.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 我对这篇论文印象深刻，想看看是否可以在较小的规模上复制顺序推荐能力。我决定利用 Hugging Face 的开源 T5 模型（[T5-large](https://huggingface.co/t5-large)），并制作了自己的自定义数据集来微调它以产生推荐。
- en: 'The dataset I made consisted of over 100 examples of sports equipment purchases
    along with the next item to be purchased. For example:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 我制作的数据集包含了超过 100 个运动设备购买的示例以及下一个要购买的项目。例如：
- en: '[PRE1]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Of course, to make this more robust, I decided to use a more specific prompt.
    Which looked like this:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，为了使其更为强健，我决定使用更具体的提示。提示如下：
- en: '***Input:*** “ITEMS PURCHASED: {Soccer Goal Post, Soccer Ball, Soccer Cleats,
    Goalie Gloves} — CANDIDATES FOR RECOMMENDATION: {Soccer Jersey, Basketball Jersey,
    Football Jersey, Baseball Jersey, Tennis Shirt, Hockey Jersey, Basketball, Football,
    Baseball, Tennis Ball, Hockey Puck, Basketball Shoes, Football Cleats, Baseball
    Cleats, Tennis Shoes, Hockey Helmet, Basketball Arm Sleeve, Football Shoulder
    Pads, Baseball Cap, Tennis Racket, Hockey Skates, Basketball Hoop, Football Helmet,
    Baseball Bat, Hockey Stick, Soccer Cones, Basketball Shorts, Baseball Glove, Hockey
    Pads, Soccer Shin Guards, Soccer Shorts} — RECOMMENDATION: ”'
  id: totrans-47
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '***输入:*** “购买的物品: {足球门框, 足球, 足球鞋, 守门员手套} — 推荐候选: {足球球衣, 篮球球衣, 橄榄球球衣, 棒球球衣,
    网球衬衫, 曲棍球球衣, 篮球, 橄榄球, 棒球, 网球, 曲棍球, 篮球鞋, 橄榄球鞋, 棒球鞋, 网球鞋, 曲棍球头盔, 篮球臂套, 橄榄球护肩, 棒球帽,
    网球拍, 曲棍球滑冰鞋, 篮球架, 橄榄球头盔, 棒球棒, 曲棍球棍, 足球标志, 篮球短裤, 棒球手套, 曲棍球护具, 足球护腿板, 足球短裤} — 推荐:
    ”'
- en: ''
  id: totrans-48
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '***Target Output:*** “Soccer Jersey”'
  id: totrans-49
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '***目标输出:*** “足球球衣”'
- en: Above you can see the specification of items purchased by the user so far, followed
    by a list of candidates for recommendation that haven’t been purchased yet (this
    is the entire inventory).
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 上面你可以看到用户目前购买的物品规格，随后是尚未购买的推荐候选列表（这是整个库存）。
- en: 'After fine-tuning the T5 model using [Hugging Face’s Trainer API](https://huggingface.co/docs/transformers/main_classes/trainer)
    (*Seq2SeqTrainer* for ~10 epochs), I got some surprisingly good results! Some
    example evaluations:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用 [Hugging Face 的 Trainer API](https://huggingface.co/docs/transformers/main_classes/trainer)
    (*Seq2SeqTrainer* 训练了约 10 个周期) 对 T5 模型进行微调后，我获得了一些令人惊讶的好结果！一些示例评估：
- en: '***Input:*** “ITEMS PURCHASED: {Soccer Jersey, Soccer Goal Post, Soccer Cleats,
    Goalie Gloves} — CANDIDATES FOR RECOMMENDATION: {Basketball Jersey, Football Jersey,
    Baseball Jersey, Tennis Shirt, Hockey Jersey, Soccer Ball, Basketball, Football,
    Baseball, Tennis Ball, Hockey Puck, Basketball Shoes, Football Cleats, Baseball
    Cleats, Tennis Shoes, Hockey Helmet, Basketball Arm Sleeve, Football Shoulder
    Pads, Baseball Cap, Tennis Racket, Hockey Skates, Basketball Hoop, Football Helmet,
    Baseball Bat, Hockey Stick, Soccer Cones, Basketball Shorts, Baseball Glove, Hockey
    Pads, Soccer Shin Guards, Soccer Shorts} — RECOMMENDATION: ”'
  id: totrans-52
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '***输入:*** “购买的物品: {足球球衣, 足球门框, 足球鞋, 守门员手套} — 推荐候选: {篮球球衣, 橄榄球球衣, 棒球球衣, 网球衬衫,
    曲棍球球衣, 足球, 篮球, 橄榄球, 棒球, 网球, 曲棍球, 篮球鞋, 橄榄球鞋, 棒球鞋, 网球鞋, 曲棍球头盔, 篮球臂套, 橄榄球护肩, 棒球帽,
    网球拍, 曲棍球滑冰鞋, 篮球架, 橄榄球头盔, 棒球棒, 曲棍球棍, 足球标志, 篮球短裤, 棒球手套, 曲棍球护具, 足球护腿板, 足球短裤} — 推荐:
    ”'
- en: ''
  id: totrans-53
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '***Model Output:*** “​​Soccer Ball”'
  id: totrans-54
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '***模型输出:*** “足球”'
- en: '***Input:*** “ITEMS PURCHASED: {Basketball Jersey, Basketball, Basketball Arm
    Sleeve} — CANDIDATES FOR RECOMMENDATION: {Soccer Jersey, Football Jersey, Baseball
    Jersey, Tennis Shirt, Hockey Jersey, Soccer Ball, Football, Baseball, Tennis Ball,
    Hockey Puck, Soccer Cleats, Basketball Shoes, Football Cleats, Baseball Cleats,
    Tennis Shoes, Hockey Helmet, Goalie Gloves, Football Shoulder Pads, Baseball Cap,
    Tennis Racket, Hockey Skates, Soccer Goal Post, Basketball Hoop, Football Helmet,
    Baseball Bat, Hockey Stick, Soccer Cones, Basketball Shorts, Baseball Glove, Hockey
    Pads, Soccer Shin Guards, Soccer Shorts} — RECOMMENDATION: ”'
  id: totrans-55
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '***输入:*** “购买的物品: {篮球球衣, 篮球, 篮球臂套} — 推荐候选: {足球球衣, 橄榄球球衣, 棒球球衣, 网球衬衫, 曲棍球球衣,
    足球, 橄榄球, 棒球, 网球, 曲棍球, 足球鞋, 橄榄球鞋, 棒球鞋, 网球鞋, 曲棍球头盔, 守门员手套, 橄榄球护肩, 棒球帽, 网球拍, 曲棍球滑冰鞋,
    足球门框, 篮球架, 橄榄球头盔, 棒球棒, 曲棍球棍, 足球标志, 篮球短裤, 棒球手套, 曲棍球护具, 足球护腿板, 足球短裤} — 推荐: ”'
- en: ''
  id: totrans-56
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '***Model Output:*** “Basketball Shoes”'
  id: totrans-57
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '***模型输出:*** “篮球鞋”'
- en: This is of course subjective given that recommendations aren’t necessarily binary
    successes/failures, but the outputs being similar to the respective purchases
    so far is impressive.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 这当然是主观的，因为推荐不一定是二元的成功/失败，但输出与迄今为止的购买项相似，确实令人印象深刻。
- en: Replication Attempt 2 — Arabic
  id: totrans-59
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 复制尝试 2 — 阿拉伯语
- en: Next, I wanted to see if I could do this for Arabic, so I translated my dataset
    and looked for some publicly available T5 models that could handle Arabic text
    ([AraT5](https://huggingface.co/UBC-NLP/AraT5-base), [MT5](https://huggingface.co/google/mt5-large),
    etc.). After trying out a dozen or so variants I found on the [Hugging Face Hub](https://huggingface.co/models),
    I unfortunately couldn’t get it to produce acceptable results.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我想看看是否可以为阿拉伯语做这个，所以我翻译了我的数据集，并寻找了一些能够处理阿拉伯语文本的公开可用T5模型（[AraT5](https://huggingface.co/UBC-NLP/AraT5-base),
    [MT5](https://huggingface.co/google/mt5-large)等）。在尝试了十几个我在[Hugging Face Hub](https://huggingface.co/models)上找到的变体后，我遗憾地发现无法得到令人满意的结果。
- en: The model (after fine-tuning) would recommend the same 1 or 2 items, regardless
    of the purchase history — typically “كرة القدم”, a “soccer ball” (***but hey,
    maybe it knows that Arabic speakers love soccer and are always looking for a new
    soccer ball***). Even after trying larger versions of these models, like MT5-xl,
    I got the same result. This is likely due to the data paucity these LLMs have
    on languages other than English.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 模型（经过微调后）会推荐相同的1或2件商品，不管购买历史如何——通常是“كرة القدم”，即“soccer ball”（***不过，也许它知道阿拉伯语使用者喜欢足球，并且总是寻找新的足球***）。即使尝试了这些模型的更大版本，如MT5-xl，我也得到了相同的结果。这很可能是由于这些LLM在英语以外的语言上的数据稀缺。
- en: 'For my last attempt, I decided to try using the Google Translate API in conjunction
    with my English fine-tuned T5 model. The process was:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我的最后一次尝试，我决定尝试将Google Translate API与我的英语微调T5模型结合使用。过程如下：
- en: Take the Arabic input → Translate to English → Feed into the English fine-tuned
    model → Get model prediction in English → Translate back to Arabic
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将阿拉伯语输入 → 翻译成英语 → 输入到经过英语微调的模型中 → 获取模型的英语预测 → 翻译回阿拉伯语
- en: 'Unfortunately, this still didn’t help much since the translator would make
    some mistakes (ex: “كرة القدم”, which we’re using in place of “soccer” directly
    translates to “foot ball”), which threw the model off and resulted in the same
    1–2 items being recommended consistently.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，这仍然没能帮助多少，因为翻译器会犯一些错误（例如：“كرة القدم”，我们用来代替“soccer”的词直接翻译成“foot ball”），这让模型出错，导致始终推荐相同的1-2件商品。
- en: Benefits and Pitfalls of LLMs as Recommendation Systems
  id: totrans-65
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LLM作为推荐系统的优点和陷阱
- en: The most prominent benefits of this technique revolve around the ease of which
    it can be implemented as a stand-alone system. Because of the nature of LLMs and
    pre-training techniques discussed above, we can bypass the need for heavy, manual
    feature engineering — the model should be able to learn the representations and
    relationships naturally. Additionally, we can sidestep the cold start problem
    for new items to an extent — the name/description of the item can be extracted
    and naturally related to existing items that users have already purchased/selected.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 这种技术最突出的优点在于它作为独立系统实施的简便性。由于上述LLM和预训练技术的性质，我们可以绕过繁重的手动特征工程——模型应该能够自然地学习表示和关系。此外，我们可以在一定程度上绕过新商品的冷启动问题——商品的名称/描述可以被提取并自然地与用户已经购买/选择的现有商品相关联。
- en: There are, however, some pitfalls to this approach (don’t throw away your current
    rec systems just yet!) which primarily entail a lack of control on what’s recommended.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这种方法有一些陷阱（不要急着丢弃你当前的推荐系统！），主要是由于对推荐内容的控制缺乏。
- en: Because there’s no weighting for the different actions/events that a user takes
    to examine, then purchase an item, we’re solely dependent on what the LLM predicts
    is the most probable next token(s) for recommendation. We can’t take into consideration
    what the user bookmarked, looked at for a period of time, put into their cart,
    etc.
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 因为没有对用户在检查后购买商品的不同操作/事件进行加权，所以我们完全依赖LLM预测的最可能的下一个token(s)进行推荐。我们无法考虑用户书签、查看了一段时间、放入购物车等操作。
- en: Additionally, with these LLMs, we do run the risk that most of the recommendations
    are similarity based (i.e. items that are semantically similar to the items purchased
    so far), though I do think with extensive training data of user purchase history,
    we can ameliorate this issue via the “collaborative filtering” approach that this
    method could mimic.
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 此外，对于这些LLM，我们确实面临大多数推荐基于相似性的风险（即与目前购买的商品语义相似的商品），尽管我认为通过大量用户购买历史数据，我们可以通过这种方法模拟的“协同过滤”方法来改善这个问题。
- en: Finally, because LLMs can produce any text theoretically, the output could be
    a string that doesn’t exactly match an item in the inventory (though I think the
    likelihood of this occurring is low).
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，由于大型语言模型（LLMs）理论上可以生成任何文本，输出可能是一个与库存中的条目不完全匹配的字符串（尽管我认为这种情况发生的可能性较低）。
- en: Final Thoughts
  id: totrans-71
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 最终想法
- en: Based on the results of the P5 paper and my attempt to replicate this on a T5
    model with some fine-tuning and prompting, I would infer that this technique could
    be used across MANY language models. Using more powerful sequence-to-sequence
    models could help dramatically, especially if the fine-tuning data is large enough
    and the prompting technique is perfected.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 根据P5论文的结果以及我尝试在T5模型上进行一些微调和提示的结果，我推测这种技术可以应用于许多语言模型。使用更强大的序列到序列模型可能会有显著帮助，尤其是当微调数据足够大且提示技术得到完善时。
- en: However, I wouldn’t go as far as to recommend (no pun intended) that anyone
    use this method in and of itself. I would suggest that it be used in conjunction
    with other recommendation system techniques, so that we can avoid the pitfalls
    mentioned above while simultaneously reaping the rewards. How this can be done
    — I’m not certain, but I think with some creativity, this LLM technique can be
    integrated in a helpful manner (maybe with extracting embedding-based features
    to use in collaborative filtering, maybe in conjunction with the [“Two Tower”
    architecture](https://medium.com/nvidia-merlin/scale-faster-with-less-code-using-two-tower-with-merlin-c16f32aafa9f),
    the possibilities go on).
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我不会建议（没有双关的意思）任何人单独使用这种方法。我建议将其与其他推荐系统技术结合使用，这样我们可以避免上述提到的陷阱，同时获得好处。如何实现这一点——我不确定，但我认为通过一些创造力，这种LLM技术可以以有益的方式集成（也许通过提取基于嵌入的特征用于协同过滤，或者与[“双塔”架构](https://medium.com/nvidia-merlin/scale-faster-with-less-code-using-two-tower-with-merlin-c16f32aafa9f)结合使用，可能性是无限的）。
- en: Code
  id: totrans-74
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 代码
- en: '[My implementation of T5 as a recommendation system](https://github.com/Mohammadhia/t5_p5_recommendation_system)
    (Github Repository)'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[我实现的T5推荐系统](https://github.com/Mohammadhia/t5_p5_recommendation_system)（Github仓库）'
- en: '[My fine-tuned T5 model on Hugging Face](https://huggingface.co/mohammadhia/t5_recommendation_sports_equipment_english)
    (Hugging Face Hub)'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[我在Hugging Face上的微调T5模型](https://huggingface.co/mohammadhia/t5_recommendation_sports_equipment_english)（Hugging
    Face Hub）'
- en: References
  id: totrans-77
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] S. Geng, S. Liu, Z. Fu, Y. Ge, Y. Zhang, [Recommendation as Language Processing
    (RLP): A Unified Pretrain, Personalized Prompt & Predict Paradigm (P5)](https://arxiv.org/pdf/2203.13366.pdf)
    (2023), 16th ACM Conference on Recommender Systems'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] S. Geng, S. Liu, Z. Fu, Y. Ge, Y. Zhang, [推荐作为语言处理（RLP）：统一的预训练、个性化提示与预测范式（P5）](https://arxiv.org/pdf/2203.13366.pdf)（2023），第16届ACM推荐系统会议'
