- en: Custom Memory for ChatGPT API
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自定义 ChatGPT API 的记忆功能
- en: 原文：[https://towardsdatascience.com/custom-memory-for-chatgpt-api-artificial-intelligence-python-722d627d4d6d](https://towardsdatascience.com/custom-memory-for-chatgpt-api-artificial-intelligence-python-722d627d4d6d)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/custom-memory-for-chatgpt-api-artificial-intelligence-python-722d627d4d6d](https://towardsdatascience.com/custom-memory-for-chatgpt-api-artificial-intelligence-python-722d627d4d6d)
- en: A Gentle Introduction to LangChain Memory Types
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: LangChain 记忆类型的温和介绍
- en: '[](https://medium.com/@andvalenzuela?source=post_page-----722d627d4d6d--------------------------------)[![Andrea
    Valenzuela](../Images/ddfc1534af92413fd91076f826cc49b6.png)](https://medium.com/@andvalenzuela?source=post_page-----722d627d4d6d--------------------------------)[](https://towardsdatascience.com/?source=post_page-----722d627d4d6d--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----722d627d4d6d--------------------------------)
    [Andrea Valenzuela](https://medium.com/@andvalenzuela?source=post_page-----722d627d4d6d--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@andvalenzuela?source=post_page-----722d627d4d6d--------------------------------)[![安德烈亚·巴伦苏埃拉](../Images/ddfc1534af92413fd91076f826cc49b6.png)](https://medium.com/@andvalenzuela?source=post_page-----722d627d4d6d--------------------------------)[](https://towardsdatascience.com/?source=post_page-----722d627d4d6d--------------------------------)[![数据科学前沿](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----722d627d4d6d--------------------------------)
    [安德烈亚·巴伦苏埃拉](https://medium.com/@andvalenzuela?source=post_page-----722d627d4d6d--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----722d627d4d6d--------------------------------)
    ·8 min read·Aug 20, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于 [数据科学前沿](https://towardsdatascience.com/?source=post_page-----722d627d4d6d--------------------------------)
    ·8 分钟阅读·2023 年 8 月 20 日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/a89558351cd6f8d41f0994f05d4caebe.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a89558351cd6f8d41f0994f05d4caebe.png)'
- en: Self-made gif.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 自制 gif。
- en: If you have ever used the OpenAI API, I am sure you have noticed the catch.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你曾经使用过 OpenAI API，我相信你已经注意到这一点。
- en: '*Got it?*'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '*明白了吗？*'
- en: '*Right!* Every time you call the ChatGPT API, the model has no memory of the
    previous requests you have made. In other words: **each API call is a standalone
    interaction**.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '*正确!* 每次调用 ChatGPT API 时，模型不会记住你之前的请求。换句话说：**每次 API 调用都是独立的交互**。'
- en: And that is definitely annoying when you need to perform follow-up interactions
    with the model. A chatbot is the golden example where follow-up interactions are
    needed.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 当你需要与模型进行后续交互时，这确实很烦人。聊天机器人就是一个需要后续交互的经典例子。
- en: In this article, we will explore how to give memory to ChatGPT when using the
    OpenAI API, so that it remembers our previous interactions.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们将探讨如何在使用 OpenAI API 时为 ChatGPT 提供记忆，以便它记住我们之前的交互。
- en: Warm-Up!
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 热身！
- en: 'Let’s perform some interactions with the model so that we experience this default
    no-memory phenomenon:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们进行一些与模型的交互，以体验这种默认的无记忆现象：
- en: '[PRE0]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'But when asked a follow-up question:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 但是当被问到后续问题时：
- en: '[PRE1]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '*Right,* so in fact the model does not remember my name even though it was
    given on the first interaction.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '*对，* 实际上模型不会记住我的名字，即使它在第一次交互时被提到。'
- en: '**Note:** The method `chatgpt_call()` is just a wrapper around the OpenAI API.
    We already gave a shot on how easily call GPT models at [ChatGPT API Calls: A
    Gentle Introduction](https://medium.com/forcodesake/chatgpt-api-calls-introduction-chatgpt3-chatgpt4-ai-d19b79c49cc5)
    in case you want to check it out!'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意：** 方法 `chatgpt_call()` 只是 OpenAI API 的一个封装。如果你想查看如何轻松调用 GPT 模型，我们在 [ChatGPT
    API 调用：温和介绍](https://medium.com/forcodesake/chatgpt-api-calls-introduction-chatgpt3-chatgpt4-ai-d19b79c49cc5)
    中已经给出了示例！'
- en: Some people normally work around this memoryless situation by pre-feeding the
    previous conversation history to the model every time they do a new API call.
    Nevertheless, this practice is not cost-optimized and it has certainly a limit
    for long conversations.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 有些人通常通过在每次进行新的 API 调用时，提前将之前的对话历史传递给模型来绕过这种无记忆的情况。然而，这种做法并未优化成本，并且对于长对话有一定的限制。
- en: In order to create a memory for ChatGPT so that it is aware of the previous
    interactions, we will be using the popular `langchain` framework. This framework
    allows you to easily manage the ChatGPT conversation history and optimize it by
    choosing the right memory type for your application.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 为了为 ChatGPT 创建记忆，使其能够意识到之前的交互，我们将使用流行的 `langchain` 框架。该框架允许你轻松管理 ChatGPT 对话历史，并通过选择适合应用程序的记忆类型来优化它。
- en: '**LangChain Framework**'
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**LangChain 框架**'
- en: The `langchain` framework’s **purpose is to assist developers when building
    applications powered by Large Language Models (LLMs).**
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '`langchain`框架的**目的是帮助开发人员构建由大语言模型（LLMs）驱动的应用程序。**'
- en: '![](../Images/2db618c369cae1c8a41589c9bf47b91d.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2db618c369cae1c8a41589c9bf47b91d.png)'
- en: Self-made screenshot from the official LangChain [GitHub repository](https://github.com/hwchase17/langchain).
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 自制截图，来自官方LangChain [GitHub 仓库](https://github.com/hwchase17/langchain)。
- en: 'According to their [GitHub description](https://github.com/hwchase17/langchain):'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 根据他们的[GitHub 描述](https://github.com/hwchase17/langchain)：
- en: Large language models (LLMs) are emerging as a transformative technology, enabling
    developers to build applications that they previously could not. However, using
    these LLMs in isolation is often insufficient for creating a truly powerful app
    — the real power comes when you can combine them with other sources of computation
    or knowledge.
  id: totrans-27
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 大语言模型（LLMs）作为一种变革性技术正在出现，使开发人员能够构建以前无法构建的应用程序。然而，仅孤立使用这些LLMs通常不足以创建真正强大的应用程序——真正的力量在于将它们与其他计算或知识来源结合起来。
- en: ''
  id: totrans-28
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: This library aims to assist in the development of those types of applications.
  id: totrans-29
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 该库旨在协助开发这些类型的应用程序。
- en: They claim that building an application by only using LLMs may be insufficient.
    We found that too when doing follow-up interactions with the model by using the
    OpenAI API only.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 他们声称，仅使用LLMs（大语言模型）构建应用程序可能是不够的。我们在仅使用OpenAI API与模型进行后续交互时也发现了这一点。
- en: Framework Setup
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 框架设置
- en: 'Getting the `langchain` library up and running in Python is simple. As for
    any other Python library, we can install it with `pip`:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在Python中启动并运行`langchain`库很简单。像任何其他Python库一样，我们可以使用`pip`安装它：
- en: '[PRE2]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: LangChain calls the OpenAI API behind the scenes. Therefore, it is necessary
    to set your OpenAI API key as an environment variable called `OPENAI_API_KEY`.
    Check out [A Step-by-Step Guide to Getting Your API Key](https://medium.com/forcodesake/a-step-by-step-guide-to-getting-your-api-key-2f6ee1d3e197)
    if you need some guidance for getting your OpenAI key.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: LangChain在幕后调用OpenAI API。因此，需要将你的OpenAI API密钥设置为名为`OPENAI_API_KEY`的环境变量。如果需要获取OpenAI密钥的指导，请查看[获取API密钥的逐步指南](https://medium.com/forcodesake/a-step-by-step-guide-to-getting-your-api-key-2f6ee1d3e197)。
- en: 'LangChain: Basic Calls'
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: LangChain：基础调用
- en: Let’s start by setting up a basic API call to ChatGPT using LangChain.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们首先设置一个使用LangChain调用ChatGPT的基本API。
- en: 'This task is pretty straightforward since the module `langchain.llms` already
    provides an `OpenAI()` method for this purpose:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 这个任务相当简单，因为模块`langchain.llms`已经提供了一个`OpenAI()`方法用于此目的：
- en: '[PRE3]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Once the desired model is loaded, we need to start the so-called *conversation
    chain*. LangChain also provides a module for that purpose:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦加载了所需的模型，我们需要启动所谓的*对话链*。LangChain也提供了一个用于此目的的模块：
- en: '[PRE4]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Let’s define the conversation with `verbose=True` to observe the reasoning process
    of the model.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将对话定义为`verbose=True`，以观察模型的推理过程。
- en: Finally, `langchain` provides a `.predict()` method to send your desired prompt
    to ChatGPT and get its completion back. *Let’s try it!*
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，`langchain`提供了一个`.predict()`方法来将你想要的提示发送给ChatGPT，并获取其完成结果。*让我们试试吧！*
- en: '[PRE5]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '*Let’s do a follow-up interaction!*'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '*让我们进行一次后续交互！*'
- en: '[PRE6]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: We can see that **the model is capable of handling follow-up interactions without
    problems when using** `**langchain**`.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，使用`**langchain**`时，**模型能够顺利处理后续交互**。
- en: LangChain Memory Types
  id: totrans-47
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LangChain 内存类型
- en: As we have observed, LangChain conversation chains already keep track of the
    `.predict` calls for a declared `conversation`. However, **the default conversation
    chain stores each and every interaction we have had with the model**.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们观察到的，LangChain 对话链已经跟踪了声明的`conversation`的`.predict`调用。然而，**默认的对话链会存储我们与模型的每一个交互**。
- en: As we have briefly discussed at the beginning of the article, storing all the
    interactions with the model can **quickly escalate to a considerable amount of
    tokens to process every time we prompt the model**. It is essential to bear in
    mind that ChatGPT has a token limit per interaction.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在文章开头简要讨论的那样，存储与模型的所有交互可能会**迅速积累大量的令牌，以便每次提示模型时都需要处理**。需要记住的是，ChatGPT每次交互都有令牌限制。
- en: In addition, the **ChatGPT usage cost also depends on the number of tokens**.
    Processing all the conversation history in each new interaction is likely to be
    expensive over time.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，**ChatGPT 的使用费用也取决于令牌的数量**。处理每次新交互中的所有对话历史记录可能会随着时间的推移变得非常昂贵。
- en: To overcome these limitations, `langchain` implements different types of memories
    to use in your application.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 为了克服这些限制，`langchain`实现了不同类型的内存以供您在应用程序中使用。
- en: '*Let’s explore them!*'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '*让我们来探索它们吧！*'
- en: '#1\. Complete Interactions'
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '#1\. 完整的交互'
- en: 'Although the default behavior of LangChain is to store all the past interactions,
    this memory type can be explicitly declared. It is the so-called `ConversationBufferMemory`,
    and it simply fills a buffer with all our previous interactions:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管LangChain的默认行为是存储所有过去的交互，但这种内存类型可以被显式声明。这就是所谓的`ConversationBufferMemory`，它简单地用我们所有的先前交互填充一个缓冲区：
- en: '[PRE7]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Declaring the memory type allows us to have some additional control over the
    ChatGPT memory. For example, we can check the buffer content at any time with
    `memory.buffer` or `memory.load_memory_variables({})`.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 声明内存类型使我们能够对ChatGPT内存进行一些额外的控制。例如，我们可以随时使用`memory.buffer`或`memory.load_memory_variables({})`检查缓冲区内容。
- en: 'In addition, we can add extra information to the buffer without doing a real
    interaction with the model:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们可以在缓冲区中添加额外的信息，而无需与模型进行实际交互：
- en: '[PRE8]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: If you do not need to manipulate the buffer in your application, you might be
    good to go with the default memory and no explicit declaration. **Although I really
    recommend it for debugging purposes!**
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您不需要在应用程序中操作缓冲区，您可以使用默认内存而不需要显式声明。**尽管我确实推荐用于调试目的！**
- en: '#2\. Interactions within a window'
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '#2\. 窗口中的交互'
- en: One less costly alternative is storing only a certain amount of previous interactions
    (`k`) with the model. That is the so-called *window* of interaction.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 一种成本更低的替代方案是只存储与模型的某个数量的以前交互（`k`）。这就是所谓的*交互窗口*。
- en: When conversations grow big enough, it might be sufficient for your application
    that the model only remembers the most recent interactions. For those cases, the
    `ConversationBufferWindowMemory` module is available.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 当对话变得足够大时，对于您的应用程序来说，模型只记住最近的交互可能已经足够。在这些情况下，`ConversationBufferWindowMemory`模块是可用的。
- en: '*Let’s explore its behavior!*'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '*让我们来探索它的行为吧！*'
- en: 'Firstly, we need to load the `llm` model and the new type of `memory`. In this
    case, we are setting `k=1` which means that only the previous iteration will be
    kept in memory:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要加载`llm`模型和新的`memory`类型。在这种情况下，我们设置了`k=1`，这意味着只有前一次迭代会保存在内存中：
- en: '[PRE9]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '[PRE10]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Secondly, let’s add some context to our conversation as shown in the previous
    section:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，让我们像前面部分所示的那样为我们的对话添加一些上下文：
- en: '[PRE11]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Although we have stored two interactions in our conversation history, due to
    the fact that we have set `k=1`, the model will only remember the last interaction
    `{“input”: “Not much, just hanging”}, {“output”: “Cool”}`.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '尽管我们在对话历史中存储了两个交互，由于我们设置了`k=1`，模型只会记住最后一次交互`{“input”: “Not much, just hanging”},
    {“output”: “Cool”}`。'
- en: 'To prove it, let’s check what the model has in memory:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 为了证明这一点，让我们检查一下模型内存中有什么：
- en: '[PRE12]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'We can further prove it by asking a follow-up question setting `verbose=True`,
    so that we can observe the stored interactions:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过设置`verbose=True`来提出后续问题，从而进一步证明这一点，这样我们可以观察存储的交互记录：
- en: '[PRE13]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'And the verbose output is the following:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 详细输出如下：
- en: As we can observe, **the model only remembers the previous interaction**.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所观察到的，**模型只记住前一次交互**。
- en: '#3\. Summary of the interactions'
  id: totrans-76
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '#3\. 交互的总结'
- en: I am sure you are now thinking that **completely deleting old interactions with
    the model might be a bit risky for some applications**.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 我相信你现在正在考虑，**完全删除与模型的旧交互对于某些应用程序可能有点风险**。
- en: Let’s imagine a customer service chatbot that asks to the user its contract
    number in the first place. The model must not forget this information, no matter
    which interaction number it has.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们想象一个客户服务聊天机器人，它首先询问用户的合同编号。无论交互的编号是多少，模型都不能忘记这个信息。
- en: For that purpose, there is a memory type that uses the model itself to generate
    a summary of the previous interactions. Therefore, the model only stores a summary
    of the conversation in memory.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 为此，有一种内存类型使用模型本身生成之前交互的总结。因此，模型仅将对话的总结存储在内存中。
- en: This optimized memory type is the so-called `ConversationSummaryBufferMemory`.
    It also allows to store the complete most recent interactions up to a maximum
    number of tokens (given by `max_token_limit`) together with the summary of the
    previous ones.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 这种优化的内存类型被称为`ConversationSummaryBufferMemory`。它还允许存储完整的最近交互，最多到达一个最大令牌数（由`max_token_limit`给定），以及之前交互的总结。
- en: '*Let’s observe this memory behavior in practice!*'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '*让我们在实际操作中观察这种内存行为！*'
- en: '[PRE14]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Let’s create a conversation with quite some content so that we can explore
    the summary capabilities:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们创建一个内容丰富的对话，以便我们可以探索总结能力：
- en: '[PRE15]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Now, when checking the memory content with `memory.load_memory_variables({})`
    , we will see the actual summary of our interactions:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，当检查内存内容时使用`memory.load_memory_variables({})`，我们将看到我们互动的实际总结：
- en: '[PRE16]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '*The summary sounds nice, isn’t it?*'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '*总结听起来很不错，不是吗？*'
- en: Let’s perform a new interaction!
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们进行一次新的互动！
- en: '[PRE17]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'And the verbose output looks as follows:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 详细输出如下：
- en: As we can observe from the example, **this memory type allows the model to keep
    important information, while reducing the irrelevant information** and, therefore,
    the amount of used tokens in each new interaction.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 从示例中我们可以观察到，**这种内存类型允许模型保留重要信息，同时减少不相关的信息**，从而减少每次新互动中使用的 Token 数量。
- en: Summary
  id: totrans-92
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this article, we have seen **different ways to create a memory for our GPT-powered
    application** depending on our needs.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们已经看到**根据我们的需求创建 GPT 支持的应用程序内存的不同方式**。
- en: By **using the LangChain framework instead of bare API calls to the OpenAI API**,
    we get rid of simple problems such as making the model aware of the previous interactions.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 通过**使用 LangChain 框架而不是直接调用 OpenAI API**，我们摆脱了简单的问题，比如让模型了解之前的互动。
- en: Despite the fact that the default memory type of LangChain might be already
    enough for your application, I really encourage you to estimate the average length
    of your conversations. It is a nice exercise to compare the average number of
    tokes used — and therefore the cost! — with the usage of the summary memory. **You
    can get full model performance at a minimal cost!**
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管 LangChain 的默认内存类型可能已经足够满足你的应用需求，但我还是鼓励你估算一下对话的平均长度。这是一个很好的练习，可以将使用 summary
    内存的平均 Token 数量——因此也是成本——进行比较。**你可以以最低成本获得模型的最佳性能！**
- en: It seems to me that the LangChain framework has a lot to give us regarding GPT
    models. *Have you already discovered another handy functionality?*
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在我看来，LangChain 框架对 GPT 模型有很多贡献。*你是否已经发现了其他有用的功能？*
- en: That is all! Many thanks for reading!
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 就这些！非常感谢阅读！
- en: I hope this article helps you when **building ChatGPT applications!**
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 希望这篇文章能帮助你在**构建 ChatGPT 应用程序**时！
- en: 'You can also subscribe to my [**Newsletter**](https://medium.com/@andvalenzuela/subscribe)
    to stay tuned for new content. **Especially**, **if you are interested in articles
    about ChatGPT**:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以订阅我的 [**通讯**](https://medium.com/@andvalenzuela/subscribe) 以获取新内容更新。**特别是**，**如果你对关于
    ChatGPT 的文章感兴趣**：
- en: '[](/chatgpt-moderation-api-input-output-artificial-intelligence-chatgpt3-data-4754389ec9c8?source=post_page-----722d627d4d6d--------------------------------)
    [## ChatGPT Moderation API: Input/Output Control'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '[链接3](https://towardsdatascience.com/chatgpt-moderation-api-input-output-artificial-intelligence-chatgpt3-data-4754389ec9c8?source=post_page-----722d627d4d6d--------------------------------)
    [## ChatGPT 审核 API：输入/输出控制'
- en: Using the OpenAI’s Moderation Endpoint for Responsible AI
  id: totrans-101
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用 OpenAI 的审核端点实现负责任的 AI
- en: towardsdatascience.com](/chatgpt-moderation-api-input-output-artificial-intelligence-chatgpt3-data-4754389ec9c8?source=post_page-----722d627d4d6d--------------------------------)
    [](/chatgpt-tokenizer-chatgpt3-chatgpt4-artificial-intelligence-python-ai-27f78906ea54?source=post_page-----722d627d4d6d--------------------------------)
    [## Unleashing the ChatGPT Tokenizer
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '[链接4](https://towardsdatascience.com/chatgpt-tokenizer-chatgpt3-chatgpt4-artificial-intelligence-python-ai-27f78906ea54?source=post_page-----722d627d4d6d--------------------------------)
    [## 解锁 ChatGPT 分词器'
- en: Hands-On! How ChatGPT Manages Tokens?
  id: totrans-103
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 实操！ChatGPT 如何管理 Token？
- en: 'towardsdatascience.com](/chatgpt-tokenizer-chatgpt3-chatgpt4-artificial-intelligence-python-ai-27f78906ea54?source=post_page-----722d627d4d6d--------------------------------)
    [](/chatgpt-summarization-llms-chatgpt3-chatgpt4-artificial-intelligence-16cf0e3625ce?source=post_page-----722d627d4d6d--------------------------------)
    [## Mastering ChatGPT: Effective Summarization with LLMs'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '[链接1](https://towardsdatascience.com/chatgpt-tokenizer-chatgpt3-chatgpt4-artificial-intelligence-python-ai-27f78906ea54?source=post_page-----722d627d4d6d--------------------------------)
    [链接2](https://towardsdatascience.com/chatgpt-summarization-llms-chatgpt3-chatgpt4-artificial-intelligence-16cf0e3625ce?source=post_page-----722d627d4d6d--------------------------------)
    [## 精通 ChatGPT：利用 LLMs 进行有效总结'
- en: How to Prompt ChatGPT to get High-Quality Summaries
  id: totrans-105
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 如何提示 ChatGPT 以获取高质量的总结
- en: towardsdatascience.com](/chatgpt-summarization-llms-chatgpt3-chatgpt4-artificial-intelligence-16cf0e3625ce?source=post_page-----722d627d4d6d--------------------------------)
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '[链接5](https://towardsdatascience.com/chatgpt-summarization-llms-chatgpt3-chatgpt4-artificial-intelligence-16cf0e3625ce?source=post_page-----722d627d4d6d--------------------------------)'
- en: 'Also towards a **responsible AI**:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，朝着**负责任的 AI**迈进：
- en: '[](/what-chatgpt-knows-about-you-openai-towards-data-privacy-science-ai-b0fa2376a5f6?source=post_page-----722d627d4d6d--------------------------------)
    [## What ChatGPT Knows about You: OpenAI’s Journey Towards Data Privacy'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/what-chatgpt-knows-about-you-openai-towards-data-privacy-science-ai-b0fa2376a5f6?source=post_page-----722d627d4d6d--------------------------------)
    [## ChatGPT 对你的了解：OpenAI 数据隐私之旅'
- en: New ways to manage personal data in ChatGPT
  id: totrans-109
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 管理 ChatGPT 中个人数据的新方法
- en: towardsdatascience.com](/what-chatgpt-knows-about-you-openai-towards-data-privacy-science-ai-b0fa2376a5f6?source=post_page-----722d627d4d6d--------------------------------)
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/what-chatgpt-knows-about-you-openai-towards-data-privacy-science-ai-b0fa2376a5f6?source=post_page-----722d627d4d6d--------------------------------)
