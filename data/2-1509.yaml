- en: Meta AI’s Another Revolutionary Large Scale Model — DINOv2 for Image Feature
    Extraction
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Meta AI 的另一个革命性大规模模型——DINOv2 用于图像特征提取。
- en: 原文：[https://towardsdatascience.com/meta-ais-another-revolutionary-large-scale-model-dinov2-for-image-feature-extraction-1114b287eadd](https://towardsdatascience.com/meta-ais-another-revolutionary-large-scale-model-dinov2-for-image-feature-extraction-1114b287eadd)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/meta-ais-another-revolutionary-large-scale-model-dinov2-for-image-feature-extraction-1114b287eadd](https://towardsdatascience.com/meta-ais-another-revolutionary-large-scale-model-dinov2-for-image-feature-extraction-1114b287eadd)
- en: DINOv2 is one of the best self-supervised ViT-based DL models for image feature
    extraction
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: DINOv2 是一种出色的自监督 ViT 基于深度学习模型，用于图像特征提取。
- en: '[](https://medium.com/@gkeretchashvili?source=post_page-----1114b287eadd--------------------------------)[![Gurami
    Keretchashvili](../Images/4da78f113a0046c2deb8224e09dd9e3d.png)](https://medium.com/@gkeretchashvili?source=post_page-----1114b287eadd--------------------------------)[](https://towardsdatascience.com/?source=post_page-----1114b287eadd--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----1114b287eadd--------------------------------)
    [Gurami Keretchashvili](https://medium.com/@gkeretchashvili?source=post_page-----1114b287eadd--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@gkeretchashvili?source=post_page-----1114b287eadd--------------------------------)[![Gurami
    Keretchashvili](../Images/4da78f113a0046c2deb8224e09dd9e3d.png)](https://medium.com/@gkeretchashvili?source=post_page-----1114b287eadd--------------------------------)[](https://towardsdatascience.com/?source=post_page-----1114b287eadd--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----1114b287eadd--------------------------------)
    [Gurami Keretchashvili](https://medium.com/@gkeretchashvili?source=post_page-----1114b287eadd--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----1114b287eadd--------------------------------)
    ·8 min read·Jun 26, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----1114b287eadd--------------------------------)
    ·阅读时间 8 分钟·2023年6月26日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: Introduction
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: Mete AI introduces a new version of the image feature extraction model called
    DINOv2 which automatically extracts visual features from the image. This is another
    revolutionary step in the AI field especially in the computer vision domain in
    terms of data and model scaling.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Meta AI 介绍了图像特征提取模型的新版本 DINOv2，该模型自动从图像中提取视觉特征。这是 AI 领域，特别是在计算机视觉领域的数据和模型扩展方面的又一步革命性进展。
- en: '![](../Images/b5509b6f4b08541f8e6df946fd86e4c3.png)'
  id: totrans-8
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b5509b6f4b08541f8e6df946fd86e4c3.png)'
- en: Demo of DINOv2 by ai.facebook.com
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: DINOv2 的演示由 ai.facebook.com 提供
- en: Motivation — Why should we care?
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 动机——我们为什么要关注？
- en: 'DINOv2 is a self-supervised modelthatdoes not require fine-tuning and has a
    good performance. In addition, it can be used as a backbone for many different
    computer vision tasks such as:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: DINOv2 是一个自监督模型，无需微调且性能良好。此外，它还可以作为许多计算机视觉任务的主干网络，例如：
- en: Classification, fine-grained classification — e.g cat vs dog or e.g dog breed
    identifier
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分类，细粒度分类——例如猫与狗，或例如狗品种识别
- en: Image retrieval —e.g. find a bag that is similar to yours from a large amount
    of images on the internet.
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图像检索——例如，从互联网上大量图像中找到与您的包相似的图像。
- en: Semantic image Segmentation — Associates a label or category with every pixel
    in an image.
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 语义图像分割——为图像中的每一个像素关联标签或类别。
- en: Video understanding — automatically recognizes and interpret various aspects
    of videos, such as objects, actions, events, scenes, and even higher-level concepts.
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 视频理解——自动识别和解释视频的各种方面，如对象、动作、事件、场景，甚至更高级的概念。
- en: Monocular Depth Estimation — predict if an object is in the foreground or in
    the background in an image.
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 单目深度估计——预测图像中一个物体是位于前景还是背景。
- en: Image clustering —group images into clusters such that the images within the
    same clusters are similar to each other
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图像聚类——将图像分组到簇中，使得同一簇中的图像彼此相似
- en: Content-based recommendation system — recommend items to a user-based representation
    of an image.
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于内容的推荐系统——根据图像的表示向用户推荐项目。
- en: DINOv2 complements another recent computer vision research, including Segment
    Anything. Segment Anything is a promptable segmentation system focused on zero-shot
    generalization to a diverse set of segmentation tasks.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: DINOv2 补充了另一项近期计算机视觉研究，包括 Segment Anything。Segment Anything 是一个可提示的分割系统，专注于对多样化的分割任务进行零-shot
    泛化。
- en: You can have a look at Segment Anything Model (SAM) in my previous post.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以查看我之前的帖子中的 Segment Anything Model (SAM)。
- en: '[](/meta-ai-introduces-revolutionary-image-segmentation-model-trained-on-1-billion-masks-8f13c86a13a2?source=post_page-----1114b287eadd--------------------------------)
    [## Meta AI Introduces Revolutionary Image Segmentation Model Trained on 1 Billion
    Masks'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/meta-ai-introduces-revolutionary-image-segmentation-model-trained-on-1-billion-masks-8f13c86a13a2?source=post_page-----1114b287eadd--------------------------------)
    [## Meta AI 推出基于10亿个掩膜训练的革命性图像分割模型'
- en: Segment Anything Model (SAM) - Best DL Model for Image Segmentation
  id: totrans-22
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Segment Anything Model (SAM) - 最佳深度学习图像分割模型
- en: towardsdatascience.com](/meta-ai-introduces-revolutionary-image-segmentation-model-trained-on-1-billion-masks-8f13c86a13a2?source=post_page-----1114b287eadd--------------------------------)
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/meta-ai-introduces-revolutionary-image-segmentation-model-trained-on-1-billion-masks-8f13c86a13a2?source=post_page-----1114b287eadd--------------------------------)
- en: Methodology
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 方法论
- en: 'The main contribution of the [paper](https://arxiv.org/pdf/2304.07193.pdf)
    can be the following:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '[论文](https://arxiv.org/pdf/2304.07193.pdf)的主要贡献可以是以下几点：'
- en: Create a large and curated training dataset
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建一个大型整理的训练数据集
- en: Improve the training algorithm and implementation
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 改进训练算法和实现
- en: Design a functional distillation pipeline.
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设计一个功能性的蒸馏管道。
- en: Create a large and curated training dataset
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建一个大型整理的训练数据集
- en: Large deep learning models need large amounts of data for training. That is
    why, the authors have created an automatic pipeline shown in the figure below
    to get a curated training dataset.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 大型深度学习模型需要大量的数据进行训练。这就是为什么作者创建了一个自动化流程，如下图所示，以获取整理的训练数据集。
- en: '![](../Images/3df39205426167a23ecc1e2221efa155.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3df39205426167a23ecc1e2221efa155.png)'
- en: The workflow of curated image creation [1]
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 整理图像创建的工作流程 [1]
- en: They selected a set of seed images from a collection of about 25 third-party
    datasets and their goal was to augment this seed images. Here is how it works.
    They scraped a large amount of uncurated images about (1.2 Billion unique images)
    from the internet. After that they created image embeddings using a self-supervised
    ViT-H/16 network pre-trained on ImageNet-22k. Second, they used cosine-similarity
    as a distance measure between images to filter out duplicated images in order
    to reduce redundancy and increase diversity. As duplicated images was removed
    from large uncurated datasets, then they performed image retrieval stage, where
    images that was similar to curated images were extracted. In the end, this approach
    enabled them to create a 142 million curated images out of 1.2 billion uncurated
    database of images.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 他们从大约25个第三方数据集中选择了一组种子图像，并且他们的目标是增强这些种子图像。其工作原理如下。他们从互联网上抓取了大量未经整理的图像（约12亿张独特图像）。之后，他们使用在ImageNet-22k上预训练的自监督ViT-H/16网络创建了图像嵌入。其次，他们使用余弦相似度作为图像间的距离度量，以过滤重复图像，从而减少冗余并增加多样性。在从大规模未整理数据集中移除重复图像后，他们进行了图像检索阶段，其中提取了与整理图像相似的图像。最终，这种方法使他们能够从12亿张未整理的图像数据库中创建出1.42亿张整理图像。
- en: Improved training algorithm
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 改进的训练算法
- en: DINOv2 uses student-teacher mechanism that is a training technique where a smaller
    neural network, known as the student, learns to mimic the behavior of a larger
    or more complex neural network, known as the teacher. Studen and teacher are based
    on Vision Tranformer architecture (ViT) [2]. As for the loss they use cross-entropy
    loss for student-teacher feature similarities. In order to learn both local and
    global features on the image they used different level of learning. For the global
    learning, they used is image leveling learning — randomly crop data augmentation
    on the same image. As for the local feature learning, they used patch level learning
    — randomly applying Mask to the input patches to the student, but not to the teacher.
    In addition they performed different normalization techniques such as Sinkhorn-knop
    batch normalization etc. You can find more details in the paper.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: DINOv2 使用了学生-教师机制，这是一种训练技术，其中一个较小的神经网络（称为学生）学习模仿一个较大或更复杂的神经网络（称为教师）的行为。学生和教师基于视觉变换器架构（ViT）[2]。至于损失函数，他们使用交叉熵损失来计算学生-教师特征的相似性。为了学习图像的局部和全局特征，他们使用了不同层次的学习。对于全局学习，他们使用的是图像层次学习——对同一图像进行随机裁剪数据增强。至于局部特征学习，他们使用了补丁层次学习——随机对输入补丁应用遮罩，仅对学生应用，不对教师应用。此外，他们还进行了不同的归一化技术，如Sinkhorn-knop批归一化等。更多细节可以在论文中找到。
- en: Distillation pipeline.
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 蒸馏管道。
- en: Powerful hardware are required when making prediction (inference) using large
    models. To overcome this limitation, they also compressed large model into a smaller
    one. Knowledge distillation [5] aims at reproducing the output of a large model
    with a smaller model by minimizing some distance between both outputs for a set
    of given inputs. The training algorithm is based on self-distillation that makes
    it straightforward to compress our large models into smaller ones.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 使用大型模型进行预测（推理）时需要强大的硬件。为了克服这一限制，他们还将大型模型压缩为较小的模型。知识蒸馏 [5] 旨在通过最小化一组给定输入的两个输出之间的距离来用较小的模型重现大型模型的输出。训练算法基于自蒸馏，使得将大型模型压缩为较小模型变得简单。
- en: Result
  id: totrans-38
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结果
- en: They have evaluated the model performance on eight different computer vision
    tasks and compared to other methods.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 他们在八个不同的计算机视觉任务上评估了模型性能，并与其他方法进行了比较。
- en: In the graph bellow, the result from DINOv2 models are dark blue, other self-supervised
    methods are pale orange and weakly-supervised methods are highlighted as dark
    pink. the dashed horizontal line is best-performing weakly- supervised model.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的图表中，DINOv2 模型的结果为深蓝色，其他自监督方法为浅橙色，弱监督方法则用深粉色突出显示。虚线水平线表示表现最佳的弱监督模型。
- en: As the result shows, DINOv2 models drastically improves over the previous state
    of the art in self-supervised learning and reaches performance comparable with
    weakly-supervised features.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 结果表明，DINOv2 模型在自监督学习领域大幅超越了之前的最先进技术，达到了与弱监督特征相当的性能。
- en: '![](../Images/c5d15921f5e14d84d51bd56da95e158a.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c5d15921f5e14d84d51bd56da95e158a.png)'
- en: DINOv2 vs other SOTA models [1]
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: DINOv2 与其他 SOTA 模型 [1]
- en: Conclusion
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: In summary, DINOv2 is another revolutionary model from Meta AI team. It does
    not require fine-tuning and can be used as a backbone for many different computer
    vision models. DINOv2 uses self-supervision mechanism and it can learn from any
    collection of images.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，DINOv2 是 Meta AI 团队推出的另一个革命性模型。它无需微调，可以作为许多不同计算机视觉模型的基础。DINOv2 使用自监督机制，能够从任何图像集合中学习。
- en: DINOv2 Demo — Fine-grained Image classification
  id: totrans-46
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: DINOv2 演示 — 细粒度图像分类
- en: In this part, I will try to demonstrate how DINOv2 works in a real-case scenario.
    I will create fine-grained image classification task.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我将尝试展示 DINOv2 在实际场景中的工作原理。我将创建一个细粒度图像分类任务。
- en: 'Classification workflow:'
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分类工作流程：
- en: Download the Food101 dataset from PyTorch datasets.
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从 PyTorch 数据集中下载 Food101 数据集。
- en: Extract features from train and test datasets using small DINOv2
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用小型 DINOv2 从训练和测试数据集中提取特征
- en: Train ML classifier models (SVM, XGBoost and KNN) using extracted features from
    training dataset.
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用从训练数据集中提取的特征训练机器学习分类模型（SVM、XGBoost 和 KNN）。
- en: Make a prediction on extracted features from test dataset.
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对测试数据集提取的特征进行预测。
- en: Evaluate each ML model’s accuracy and F1score.
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 评估每个机器学习模型的准确率和 F1 分数。
- en: '**Data**: [Food 101](https://pytorch.org/vision/stable/generated/torchvision.datasets.Food101.html)
    is a challenging data set of 101 food categories with 101,000 images. For each
    class, 250 manually reviewed test images are provided as well as 750 training
    images.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '**数据**：[Food 101](https://pytorch.org/vision/stable/generated/torchvision.datasets.Food101.html)
    是一个具有挑战性的 101 个食品类别的数据集，共有 101,000 张图像。每个类别提供 250 张手动审核的测试图像以及 750 张训练图像。'
- en: '**Model**: small [DINOv2 model](https://github.com/facebookresearch/dinov2)
    (ViT-S/14 distilled)'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '**模型**：小型 [DINOv2 模型](https://github.com/facebookresearch/dinov2)（ViT-S/14
    蒸馏版）'
- en: '**ML models**: SVM, XGBoost, KNN.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '**机器学习模型**：SVM、XGBoost、KNN。'
- en: '**Step 1 — Set up (You can use** [**Google Colab**](http://colab.research.google.com)
    **to run the code and turn GPU on)**'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤 1 — 设置（你可以使用** [**Google Colab**](http://colab.research.google.com) **来运行代码并开启
    GPU）**'
- en: '[PRE0]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '**Step 2 — Create Transformation, download and create Food101 Pytorch datasets,
    create train and test dataloader objects.**'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤 2 — 创建转换，下载并创建 Food101 Pytorch 数据集，创建训练和测试数据加载器对象。**'
- en: '[PRE1]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '[out] 75750 25250'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '[输出] 75750 25250'
- en: '[out] 9469 3157'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '[输出] 9469 3157'
- en: '**Step 3 (Optional) — Visualize training dataloader batch**'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤 3（可选） — 可视化训练数据加载器批次**'
- en: '[PRE2]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '![](../Images/1857142179e4aec23c17f5c169af578a.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1857142179e4aec23c17f5c169af578a.png)'
- en: batch of images
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 批量图像
- en: '**Step 4 — load small DINOv2 model and extract features from training and test
    dataloaders.**'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤 4 — 加载小型 DINOv2 模型并从训练和测试数据加载器中提取特征。**'
- en: '[PRE3]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '[out] ((75750, 384), (75750,), (25250, 384), (25250,))'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '[输出] ((75750, 384), (75750,), (25250, 384), (25250,))'
- en: '**Step 5 — Build a function for SVM, XGBoost and KNN classifiers.**'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤 5 — 为 SVM、XGBoost 和 KNN 分类器构建一个函数。**'
- en: '[PRE4]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '**Results**'
  id: totrans-72
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**结果**'
- en: '![](../Images/ae8b127c16e2a83d9169141b17b323ba.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ae8b127c16e2a83d9169141b17b323ba.png)'
- en: Result of small DINOv2 + SVM/XGBoost/KNN (image by the author)
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 小型DINOv2 + SVM/XGBoost/KNN的结果（图片由作者提供）
- en: Wow, the results are great! As demonstrated, SVM model trained on small DINOv2
    extracted features outperformed other ML models and achieved almost 90% accuracy.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 哇，结果真棒！正如展示的那样，基于小型DINOv2提取特征训练的SVM模型优于其他机器学习模型，达到了近90%的准确率。
- en: Conclusion
  id: totrans-76
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: Even though we used small DINOv2 model to extract features, ML models (especially
    SVM) trained on extracted features demonstrated great performance on the fine
    grained classification task. The model can classify objects with almost 90% accuracy
    out of 101 different classes.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们使用了小型DINOv2模型提取特征，但在提取特征上训练的机器学习模型（尤其是SVM）在细粒度分类任务上表现出色。该模型能够在101个不同类别中以接近90%的准确率进行分类。
- en: The accuracy would improve if it was used big, large or giant DINOv2 models.
    You just need to change the *dinov2_vits14* in step 4 with *dinov2_vitb14*, *dinov2_vitl14
    or dinov2_vitg14*. You can have a try and feel free to share the accuracy result
    in the comment section :)
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 如果使用大型、超大或巨型DINOv2模型，准确率会提高。你只需在第4步将*dinov2_vits14*替换为*dinov2_vitb14*、*dinov2_vitl14*或*dinov2_vitg14*。你可以尝试一下，欢迎在评论区分享准确率结果
    :)
- en: I hope you enjoyed it. If you have any questions or would like to share your
    thoughts about this article, feel free to comment, I will be happy to answer.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 希望你喜欢这篇文章。如果你有任何问题或想分享对这篇文章的看法，请随时评论，我将很高兴回答。
- en: If you want to support my work directly and also get unlimited access on Medium
    articles, become a Medium member using my [referral link](https://medium.com/@gkeretchashvili/membership)
    here. Thank you a million times and have a nice day!
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想直接支持我的工作，并且获得Medium文章的无限访问权限，请使用我的[推荐链接](https://medium.com/@gkeretchashvili/membership)成为Medium会员。非常感谢，祝你有美好的一天！
- en: '[](https://medium.com/@gkeretchashvili/membership?source=post_page-----1114b287eadd--------------------------------)
    [## Join Medium with my referral link - Gurami Keretchashvili'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@gkeretchashvili/membership?source=post_page-----1114b287eadd--------------------------------)
    [## 使用我的推荐链接加入Medium - Gurami Keretchashvili'
- en: As a Medium member, a portion of your membership fee goes to writers you read,
    and you get full access to every story…
  id: totrans-82
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 作为Medium会员，你的一部分会员费用会分配给你阅读的作者，你可以完全访问每一篇故事……
- en: medium.com](https://medium.com/@gkeretchashvili/membership?source=post_page-----1114b287eadd--------------------------------)
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: medium.com](https://medium.com/@gkeretchashvili/membership?source=post_page-----1114b287eadd--------------------------------)
- en: References
  id: totrans-84
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] Oquab, M., Darcet, T., Moutakanni, T., Vo, H., Szafraniec, M., Khalidov,
    V., … & Bojanowski, P. (2023). Dinov2: Learning robust visual features without
    supervision. *arXiv preprint arXiv:2304.07193*.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] Oquab, M., Darcet, T., Moutakanni, T., Vo, H., Szafraniec, M., Khalidov,
    V., … & Bojanowski, P. (2023). Dinov2: 在没有监督的情况下学习稳健的视觉特征。*arXiv预印本 arXiv:2304.07193*。'
- en: '[2] Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X.,
    Unterthiner, T., … & Houlsby, N. (2020). An image is worth 16x16 words: Transformers
    for image recognition at scale. *arXiv preprint arXiv:2010.11929*.ISO 690'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X.,
    Unterthiner, T., … & Houlsby, N. (2020). 一张图片值16x16个词：用于大规模图像识别的Transformers。*arXiv预印本
    arXiv:2010.11929*。'
- en: '[3] The DINOv2 team, [DINOv2: State-of-the-art computer vision models with
    self-supervised learning](https://ai.facebook.com/blog/dino-v2-computer-vision-self-supervised-learning/)'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] DINOv2团队，[DINOv2: 具有自监督学习的最先进计算机视觉模型](https://ai.facebook.com/blog/dino-v2-computer-vision-self-supervised-learning/)'
- en: '[4] [DINOv2 Github](https://github.com/facebookresearch/dinov2)'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] [DINOv2 Github](https://github.com/facebookresearch/dinov2)'
- en: '[5] Hinton, G., Vinyals, O., & Dean, J. (2015). Distilling the knowledge in
    a neural network. *arXiv preprint arXiv:1503.02531*.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '[5] Hinton, G., Vinyals, O., & Dean, J. (2015). 提炼神经网络中的知识。*arXiv预印本 arXiv:1503.02531*。'
