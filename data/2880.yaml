- en: How to Build a Multi-GPU System for Deep Learning in 2023
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å¦‚ä½•åœ¨ 2023 å¹´æ„å»ºå¤š GPU ç³»ç»Ÿè¿›è¡Œæ·±åº¦å­¦ä¹ 
- en: åŸæ–‡ï¼š[https://towardsdatascience.com/how-to-build-a-multi-gpu-system-for-deep-learning-in-2023-e5bbb905d935?source=collection_archive---------0-----------------------#2023-09-16](https://towardsdatascience.com/how-to-build-a-multi-gpu-system-for-deep-learning-in-2023-e5bbb905d935?source=collection_archive---------0-----------------------#2023-09-16)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡ï¼š[https://towardsdatascience.com/how-to-build-a-multi-gpu-system-for-deep-learning-in-2023-e5bbb905d935?source=collection_archive---------0-----------------------#2023-09-16](https://towardsdatascience.com/how-to-build-a-multi-gpu-system-for-deep-learning-in-2023-e5bbb905d935?source=collection_archive---------0-----------------------#2023-09-16)
- en: '[](https://benuix.medium.com/?source=post_page-----e5bbb905d935--------------------------------)[![Antonis
    Makropoulos](../Images/5bdd3826eeb31dfb6d8e6fc393b24d8b.png)](https://benuix.medium.com/?source=post_page-----e5bbb905d935--------------------------------)[](https://towardsdatascience.com/?source=post_page-----e5bbb905d935--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----e5bbb905d935--------------------------------)
    [Antonis Makropoulos](https://benuix.medium.com/?source=post_page-----e5bbb905d935--------------------------------)'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://benuix.medium.com/?source=post_page-----e5bbb905d935--------------------------------)[![Antonis
    Makropoulos](../Images/5bdd3826eeb31dfb6d8e6fc393b24d8b.png)](https://benuix.medium.com/?source=post_page-----e5bbb905d935--------------------------------)[](https://towardsdatascience.com/?source=post_page-----e5bbb905d935--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----e5bbb905d935--------------------------------)
    [Antonis Makropoulos](https://benuix.medium.com/?source=post_page-----e5bbb905d935--------------------------------)'
- en: Â·
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: Â·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F866c99d649d0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-build-a-multi-gpu-system-for-deep-learning-in-2023-e5bbb905d935&user=Antonis+Makropoulos&userId=866c99d649d0&source=post_page-866c99d649d0----e5bbb905d935---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----e5bbb905d935--------------------------------)
    Â·10 min readÂ·Sep 16, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fe5bbb905d935&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-build-a-multi-gpu-system-for-deep-learning-in-2023-e5bbb905d935&user=Antonis+Makropoulos&userId=866c99d649d0&source=-----e5bbb905d935---------------------clap_footer-----------)'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '[å…³æ³¨](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F866c99d649d0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-build-a-multi-gpu-system-for-deep-learning-in-2023-e5bbb905d935&user=Antonis+Makropoulos&userId=866c99d649d0&source=post_page-866c99d649d0----e5bbb905d935---------------------post_header-----------)
    å‘å¸ƒäº [Towards Data Science](https://towardsdatascience.com/?source=post_page-----e5bbb905d935--------------------------------)
    Â·10 min readÂ·2023å¹´9æœˆ16æ—¥[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fe5bbb905d935&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-build-a-multi-gpu-system-for-deep-learning-in-2023-e5bbb905d935&user=Antonis+Makropoulos&userId=866c99d649d0&source=-----e5bbb905d935---------------------clap_footer-----------)'
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fe5bbb905d935&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-build-a-multi-gpu-system-for-deep-learning-in-2023-e5bbb905d935&source=-----e5bbb905d935---------------------bookmark_footer-----------)![](../Images/4bd3724494a1436a98b96b853132d183.png)'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fe5bbb905d935&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-build-a-multi-gpu-system-for-deep-learning-in-2023-e5bbb905d935&source=-----e5bbb905d935---------------------bookmark_footer-----------)![](../Images/4bd3724494a1436a98b96b853132d183.png)'
- en: My deep learning build â€” always work in progress :).
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘çš„æ·±åº¦å­¦ä¹ æ­å»ºâ€”â€”å§‹ç»ˆåœ¨è¿›è¡Œä¸­ :).
- en: This story provides a guide on how to build a multi-GPU system for deep learning
    and hopefully save you some research time and experimentation.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬æ–‡æä¾›äº†å¦‚ä½•æ„å»ºå¤š GPU ç³»ç»Ÿè¿›è¡Œæ·±åº¦å­¦ä¹ çš„æŒ‡å—ï¼Œå¸Œæœ›èƒ½èŠ‚çœä½ çš„ç ”ç©¶æ—¶é—´å’Œå®éªŒã€‚
- en: Target
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç›®æ ‡
- en: Build a multi-GPU system for training of computer vision and LLMs models without
    breaking the bank! *ğŸ¦*
  id: totrans-10
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: æ„å»ºä¸€ä¸ªå¤š GPU ç³»ç»Ÿï¼Œç”¨äºè®¡ç®—æœºè§†è§‰å’Œ LLMs æ¨¡å‹çš„è®­ç»ƒï¼Œè€Œä¸ä¼šç ´åé¢„ç®—ï¼ *ğŸ¦*
- en: Step 1\. GPUs
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç¬¬ä¸€æ­¥\. GPU
- en: Letâ€™s start with the fun (and expensive ğŸ’¸ğŸ’¸ğŸ’¸) part!
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬ä»æœ‰è¶£ï¼ˆå’Œæ˜‚è´µçš„ ğŸ’¸ğŸ’¸ğŸ’¸ï¼‰éƒ¨åˆ†å¼€å§‹å§ï¼
- en: '![](../Images/24f9a6334f3f16d591f251fdff672c5f.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/24f9a6334f3f16d591f251fdff672c5f.png)'
- en: The H100 beast! Image from [NVIDIA](https://developer.nvidia.com/blog/nvidia-hopper-architecture-in-depth/).
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: H100 æ€ªå…½ï¼å›¾ç‰‡æ¥è‡ª [NVIDIA](https://developer.nvidia.com/blog/nvidia-hopper-architecture-in-depth/)ã€‚
- en: 'The main considerations when buying a GPU are:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: è´­ä¹° GPU æ—¶çš„ä¸»è¦è€ƒè™‘å› ç´ æ˜¯ï¼š
- en: memory (VRAM)
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å†…å­˜ï¼ˆVRAMï¼‰
- en: performance (Tensor cores, clock speed)
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ€§èƒ½ï¼ˆå¼ é‡æ ¸å¿ƒï¼Œæ—¶é’Ÿé€Ÿåº¦ï¼‰
- en: slot width
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ’æ§½å®½åº¦
- en: power (TDP)
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åŠŸç‡ï¼ˆTDPï¼‰
- en: Memory
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å†…å­˜
- en: For deep learning tasks nowadays we need a loooot of memory. LLMs are huge even
    to fine-tune and computer vision tasks can get memory-intensive especially with
    3D networks. Naturally the most important aspect to look for is the GPU **VRAM**.
    For LLMs I recommend at least 24 GB memory and for computer vision tasks I wouldnâ€™t
    go below 12 GB.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æ·±åº¦å­¦ä¹ ä»»åŠ¡éœ€è¦å¤§é‡å†…å­˜ã€‚å³ä½¿æ˜¯å¾®è°ƒLLMsä¹Ÿå¾ˆåºå¤§ï¼Œè€Œè®¡ç®—æœºè§†è§‰ä»»åŠ¡ï¼Œç‰¹åˆ«æ˜¯3Dç½‘ç»œï¼Œä¹Ÿå¯èƒ½å˜å¾—å†…å­˜å¯†é›†ã€‚è‡ªç„¶ï¼Œæœ€é‡è¦çš„æ–¹é¢æ˜¯GPUçš„**VRAM**ã€‚å¯¹äºLLMsï¼Œæˆ‘å»ºè®®è‡³å°‘24
    GBå†…å­˜ï¼Œå¯¹äºè®¡ç®—æœºè§†è§‰ä»»åŠ¡ï¼Œæˆ‘å»ºè®®ä¸ä½äº12 GBã€‚
- en: Performance
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ€§èƒ½
- en: 'The second criterion is performance which can be estimated with FLOPS (Floating-point
    Operations per Second):'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: ç¬¬äºŒä¸ªæ ‡å‡†æ˜¯æ€§èƒ½ï¼Œå¯ä»¥é€šè¿‡FLOPSï¼ˆæ¯ç§’æµ®ç‚¹è¿ç®—ï¼‰æ¥ä¼°ç®—ï¼š
- en: '![](../Images/8ffc752fed94dc4a6083ed15312f4173.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8ffc752fed94dc4a6083ed15312f4173.png)'
- en: The crucial number in the past was the number of CUDA cores in the circuit.
    However, with the emergence of deep learning, NVIDIA has introduced specialized
    **tensor cores** that can perform many more FMA (Fused Multiply-Add) operations
    per clock. These are already supported by the main deep learning frameworks and
    are what you should look for in 2023.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: è¿‡å»çš„å…³é”®æ•°å­—æ˜¯ç”µè·¯ä¸­çš„CUDAæ ¸å¿ƒæ•°é‡ã€‚ç„¶è€Œï¼Œéšç€æ·±åº¦å­¦ä¹ çš„å…´èµ·ï¼ŒNVIDIAæ¨å‡ºäº†ä¸“é—¨çš„**å¼ é‡æ ¸å¿ƒ**ï¼Œæ¯æ—¶é’Ÿå‘¨æœŸå¯ä»¥æ‰§è¡Œæ›´å¤šçš„FMAï¼ˆèåˆåŠ æ³•ï¼‰æ“ä½œã€‚è¿™äº›å·²ç»è¢«ä¸»è¦æ·±åº¦å­¦ä¹ æ¡†æ¶æ”¯æŒï¼Œ2023å¹´æ‚¨åº”è¯¥å…³æ³¨è¿™äº›ã€‚
- en: 'Below you can find a chart of raw performance of GPUs grouped by memory that
    I compiled after quite some manual work:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: ä»¥ä¸‹æ˜¯æˆ‘åœ¨ç»è¿‡å¤§é‡æ‰‹åŠ¨å·¥ä½œåç¼–åˆ¶çš„æŒ‰å†…å­˜åˆ†ç»„çš„GPUåŸå§‹æ€§èƒ½å›¾è¡¨ï¼š
- en: '![](../Images/f43767df7f7dec9bff9c18fc1abdeb8b.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f43767df7f7dec9bff9c18fc1abdeb8b.png)'
- en: Raw performance of GPUs based on the CUDA and tensor cores (TFLOPs).
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: åŸºäºCUDAå’Œå¼ é‡æ ¸å¿ƒï¼ˆTFLOPsï¼‰çš„GPUåŸå§‹æ€§èƒ½ã€‚
- en: Note that you have to be *extra careful* when comparing performance of different
    GPUs. Tensor cores of different generations / architectures are not comparable.
    For instance, the A100 performs 256 FP16 FMA operations / clock while the V100
    â€œonlyâ€ 64\. Additionally, older architectures (Turing, Volta) do not support 32-bit
    tensor operations. What makes the comparison more difficult is that NVIDIA doesnâ€™t
    always report the FMA, not even in the whitepapers, and GPUs of the same architecture
    can have different FMAs. I kept banging my head with [this](https://forums.developer.nvidia.com/t/tf32-tflops-of-geforce-rtx-3090-vs-a40/265828)
    ğŸ˜µâ€ğŸ’«. Also note that NVIDIA often advertises the tensor FLOPS with sparsity which
    is a feature usable only at inference time.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: æ³¨æ„åœ¨æ¯”è¾ƒä¸åŒGPUçš„æ€§èƒ½æ—¶å¿…é¡»*æ ¼å¤–å°å¿ƒ*ã€‚ä¸åŒä»£/æ¶æ„çš„Tensoræ ¸å¿ƒä¸å¯æ¯”ã€‚ä¾‹å¦‚ï¼ŒA100æ¯æ—¶é’Ÿå‘¨æœŸæ‰§è¡Œ256 FP16 FMAæ“ä½œï¼Œè€ŒV100â€œä»…â€æ‰§è¡Œ64ä¸ªã€‚æ­¤å¤–ï¼Œè¾ƒæ—§çš„æ¶æ„ï¼ˆTuring,
    Voltaï¼‰ä¸æ”¯æŒ32ä½å¼ é‡æ“ä½œã€‚æ›´éš¾æ¯”è¾ƒçš„æ˜¯NVIDIAå¹¶ä¸æ€»æ˜¯æŠ¥å‘ŠFMAï¼Œç”šè‡³åœ¨ç™½çš®ä¹¦ä¸­ä¹Ÿæ²¡æœ‰ï¼Œä¸”ç›¸åŒæ¶æ„çš„GPUå¯èƒ½æœ‰ä¸åŒçš„FMAã€‚æˆ‘ä¸€ç›´åœ¨[è¿™ä¸ª](https://forums.developer.nvidia.com/t/tf32-tflops-of-geforce-rtx-3090-vs-a40/265828)ä¸Šç¢°å£ğŸ˜µâ€ğŸ’«ã€‚è¿˜è¦æ³¨æ„ï¼ŒNVIDIAé€šå¸¸ç”¨ç¨€ç–æ€§æ¥å®£ä¼ å¼ é‡FLOPSï¼Œè€Œè¿™ä¸€ç‰¹æ€§ä»…åœ¨æ¨ç†æ—¶å¯ç”¨ã€‚
- en: 'In order to identify the best GPU with respect to price, I collected the ebay
    prices using the ebay API and computed the relative performance per dollar (USD)
    for new cards:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†è¯†åˆ«æ€§ä»·æ¯”æœ€é«˜çš„GPUï¼Œæˆ‘ä½¿ç”¨eBay APIæ”¶é›†äº†eBayä»·æ ¼ï¼Œå¹¶è®¡ç®—äº†æ–°å¡çš„æ¯ç¾å…ƒç›¸å¯¹æ€§èƒ½ï¼š
- en: '![](../Images/f37be6f51d3986e0438c27fffaafd20f.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f37be6f51d3986e0438c27fffaafd20f.png)'
- en: Relative performance per USD of GPUs based on the CUDA and tensor cores (TFLOPs
    / USD). Prices are based on current ebay prices (September 2023).
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: åŸºäºCUDAå’Œå¼ é‡æ ¸å¿ƒï¼ˆTFLOPs / USDï¼‰çš„GPUç›¸å¯¹æ€§èƒ½ã€‚ä»·æ ¼åŸºäºå½“å‰eBayä»·æ ¼ï¼ˆ2023å¹´9æœˆï¼‰ã€‚
- en: I did the same for used cards but since the rankings donâ€™t change too much I
    omit the plot.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘å¯¹äºŒæ‰‹å¡åšäº†ç›¸åŒçš„å¤„ç†ï¼Œä½†ç”±äºæ’åå˜åŒ–ä¸å¤§ï¼Œæ‰€ä»¥çœç•¥äº†å›¾è¡¨ã€‚
- en: 'To select the best GPU for your budget, you can pick one of the top GPUs for
    the largest memory you can afford. My recommendation would be:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†é€‰æ‹©æœ€é€‚åˆæ‚¨é¢„ç®—çš„GPUï¼Œæ‚¨å¯ä»¥é€‰æ‹©æ‹¥æœ‰æœ€å¤§å†…å­˜çš„é¡¶çº§GPUã€‚æˆ‘çš„æ¨èæ˜¯ï¼š
- en: '![](../Images/97e05d42f4b815f70b2fe279adf077eb.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/97e05d42f4b815f70b2fe279adf077eb.png)'
- en: Recommendation of GPUs for different budgets based on current ebay prices (September
    2023).
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: æ ¹æ®å½“å‰eBayä»·æ ¼ï¼ˆ2023å¹´9æœˆï¼‰ï¼Œä¸åŒé¢„ç®—çš„GPUæ¨èã€‚
- en: If you want to dive into more technical aspects I advise to read Tim Dettmersâ€™
    excellent guide on [Which GPU(s) to Get for Deep Learning](https://timdettmers.com/2023/01/30/which-gpu-for-deep-learning/).
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæ‚¨æƒ³æ·±å…¥äº†è§£æ›´å¤šæŠ€æœ¯ç»†èŠ‚ï¼Œå»ºè®®é˜…è¯»Tim Dettmersçš„ä¼˜ç§€æŒ‡å—[é€‰æ‹©é€‚åˆæ·±åº¦å­¦ä¹ çš„GPU](https://timdettmers.com/2023/01/30/which-gpu-for-deep-learning/)ã€‚
- en: Slot width
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ’æ§½å®½åº¦
- en: 'When building a multi-GPU system, we need to plan how to physically fit the
    GPUs into a PC case. Since GPUs grow larger and larger, especially the gaming
    series, this becomes more of an issue. Consumer motherboards have up to 7 PCIe
    slots and PC cases are built around this setup. A 4090 can easily take up 4 slots
    depending on manufacturer, so you can see why this becomes an issue. Additionally
    we should leave at least 1 slot between GPUs that are not blower style or watercooled
    to avoid overheating. We have the following options:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ„å»ºå¤šGPUç³»ç»Ÿæ—¶ï¼Œæˆ‘ä»¬éœ€è¦è§„åˆ’å¦‚ä½•å°†GPUç‰©ç†å®‰è£…åˆ°PCæœºç®±ä¸­ã€‚ç”±äºGPUè¶Šæ¥è¶Šå¤§ï¼Œå°¤å…¶æ˜¯æ¸¸æˆç³»åˆ—ï¼Œè¿™æˆä¸ºäº†ä¸€ä¸ªé—®é¢˜ã€‚æ¶ˆè´¹çº§ä¸»æ¿æœ€å¤šæœ‰7ä¸ªPCIeæ’æ§½ï¼ŒPCæœºç®±ä¹Ÿå›´ç»•è¿™ä¸ªè®¾ç½®è¿›è¡Œè®¾è®¡ã€‚ä¸€å—4090æ ¹æ®åˆ¶é€ å•†çš„ä¸åŒï¼Œå¯èƒ½ä¼šå ç”¨4ä¸ªæ’æ§½ï¼Œå› æ­¤ä½ å¯ä»¥ç†è§£ä¸ºä»€ä¹ˆè¿™ä¼šæˆä¸ºé—®é¢˜ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬åº”è¯¥åœ¨éé£å†·æˆ–æ°´å†·çš„GPUä¹‹é—´è‡³å°‘ç•™å‡º1ä¸ªæ’æ§½ï¼Œä»¥é¿å…è¿‡çƒ­ã€‚æˆ‘ä»¬æœ‰ä»¥ä¸‹é€‰é¡¹ï¼š
- en: '**Watercooling**'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '**æ°´å†·**'
- en: Watercooled variants will take up to 2 slots but they are more expensive. You
    can alternatively convert an air-cooled GPU but this will void the warranty. If
    you donâ€™t get All-in-One (AIO) solutions you will need to build a custom watercooling
    loop. This is also true if you want to fit multiple watercooled GPUs since the
    AIO radiators may not fit in the case. Building your own loop is risky and I wouldnâ€™t
    personally do it with expensive cards. I would only buy AIO solutions straight
    from the manufactures (risk averse ğŸ™ˆ).
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: æ°´å†·å˜ä½“å°†å ç”¨æœ€å¤š2ä¸ªæ’æ§½ï¼Œä½†å®ƒä»¬ä»·æ ¼æ›´è´µã€‚ä½ ä¹Ÿå¯ä»¥å°†é£å†·GPUæ”¹è£…æˆæ°´å†·ï¼Œä½†è¿™ä¼šä½¿ä¿ä¿®å¤±æ•ˆã€‚å¦‚æœä½ ä¸é€‰æ‹©ä¸€ä½“å¼ï¼ˆAIOï¼‰è§£å†³æ–¹æ¡ˆï¼Œä½ å°†éœ€è¦æ„å»ºè‡ªå®šä¹‰æ°´å†·ç³»ç»Ÿã€‚å¦‚æœä½ æƒ³å®‰è£…å¤šä¸ªæ°´å†·GPUï¼Œè¿™ä¸€ç‚¹å°¤å…¶é‡è¦ï¼Œå› ä¸ºAIOæ•£çƒ­å™¨å¯èƒ½æ— æ³•é€‚é…æœºç®±ã€‚è‡ªè¡Œæ„å»ºç³»ç»Ÿæœ‰é£é™©ï¼Œæˆ‘ä¸ªäººä¸ä¼šåœ¨æ˜‚è´µçš„æ˜¾å¡ä¸Šå°è¯•ã€‚æˆ‘åªä¼šç›´æ¥ä»åˆ¶é€ å•†å¤„è´­ä¹°AIOè§£å†³æ–¹æ¡ˆï¼ˆé£é™©è§„é¿
    ğŸ™ˆï¼‰ã€‚
- en: '**Aircooled 2â€“3 slot cards and PCIe risers**'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '**é£å†·2-3æ§½æ˜¾å¡å’ŒPCIeæ‰©å±•å¡**'
- en: In this scenario you interleave cards on PCIe slots and cards connected with
    PCIe riser cables. The PCIe riser cards can be placed somewhere inside the PC
    case or in the open air. In either case you should make sure the GPUs are secured
    (see also the section about PC cases).
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œä½ å¯ä»¥å°†æ˜¾å¡äº¤é”™åœ°å®‰è£…åœ¨PCIeæ’æ§½ä¸Šï¼Œå¹¶é€šè¿‡PCIeæ‰©å±•å¡è¿æ¥æ˜¾å¡ã€‚PCIeæ‰©å±•å¡å¯ä»¥æ”¾ç½®åœ¨PCæœºç®±å†…éƒ¨çš„æŸä¸ªä½ç½®ï¼Œæˆ–æ”¾åœ¨å¼€æ”¾ç©ºæ°”ä¸­ã€‚åœ¨ä»»ä½•æƒ…å†µä¸‹ï¼Œä½ éƒ½åº”è¯¥ç¡®ä¿GPUå›ºå®šå¥½ï¼ˆå¦è§å…³äºPCæœºç®±çš„éƒ¨åˆ†ï¼‰ã€‚
- en: Power (TDP)
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: åŠŸç‡ï¼ˆTDPï¼‰
- en: Modern GPUs get more and more power hungry. For instance, A 4090 requires 450
    W while a H100 can get up to 700 W. Apart from the power bill, fitting three or
    more GPUs becomes an issue. This is especially true in the US that the power sockets
    can deliver up to around 1800w.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: ç°ä»£GPUçš„åŠŸè€—è¶Šæ¥è¶Šå¤§ã€‚ä¾‹å¦‚ï¼Œä¸€å—4090éœ€è¦450Wï¼Œè€ŒH100å¯ä»¥è¾¾åˆ°700Wã€‚é™¤äº†ç”µè´¹ï¼Œå®‰è£…ä¸‰å—æˆ–æ›´å¤šæ˜¾å¡ä¹Ÿæˆä¸ºäº†ä¸€ä¸ªé—®é¢˜ã€‚è¿™åœ¨ç¾å›½å°¤å…¶å¦‚æ­¤ï¼Œå› ä¸ºç”µæºæ’åº§çš„æœ€å¤§åŠŸç‡çº¦ä¸º1800wã€‚
- en: 'A solution to this problem if you are getting close to the max power you can
    draw from your PSU / power socket is power-limiting. All you need to reduce the
    max power a GPU can draw is:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœä½ æ¥è¿‘ç”µæº/ç”µæºæ’åº§çš„æœ€å¤§åŠŸç‡ï¼Œè§£å†³è¿™ä¸ªé—®é¢˜çš„ä¸€ä¸ªæ–¹æ¡ˆæ˜¯åŠŸç‡é™åˆ¶ã€‚è¦å‡å°‘GPUå¯ä»¥å¸å–çš„æœ€å¤§åŠŸç‡ï¼Œä½ åªéœ€ï¼š
- en: '[PRE0]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Power-limiting by 10-20% has been shown to reduce performance by less than 5%
    and keeps the cards cooler ([experiment by Puget Systems](https://www.pugetsystems.com/labs/hpc/NVIDIA-GPU-Power-Limit-vs-Performance-2296/?utm=)).
    Power-limiting four 3090s for instance by 20% will reduce their consumption to
    1120w and can easily fit in a 1600w PSU / 1800w socket (assuming 400w for the
    rest of the components).
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: é€šè¿‡å°†åŠŸç‡é™åˆ¶åœ¨10-20%ä¹‹é—´ï¼Œå·²è¢«è¯æ˜å¯ä»¥å°†æ€§èƒ½é™ä½ä¸åˆ°5%ï¼Œå¹¶ä¸”èƒ½ä½¿æ˜¾å¡ä¿æŒæ›´å‡‰çˆ½ï¼ˆ[Puget Systemsçš„å®éªŒ](https://www.pugetsystems.com/labs/hpc/NVIDIA-GPU-Power-Limit-vs-Performance-2296/?utm=)ï¼‰ã€‚ä¾‹å¦‚ï¼Œå°†å››å—3090æ˜¾å¡çš„åŠŸç‡é™åˆ¶ä¸º20%ä¼šå°†å…¶åŠŸè€—é™ä½åˆ°1120wï¼Œå¹¶ä¸”å¯ä»¥è½»æ¾é€‚é…1600wçš„ç”µæº/1800wçš„æ’åº§ï¼ˆå‡è®¾å…¶ä½™ç»„ä»¶æ¶ˆè€—400wï¼‰ã€‚
- en: Step 2\. Motherboard and CPU
  id: totrans-49
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ­¥éª¤2\. ä¸»æ¿å’ŒCPU
- en: The next step of the build is to pick a motherboard that allows multiple GPUs.
    Here the main consideration is the PCIe lanes. We need at **minimum PCIe 3.0 slots
    with x8 lanes** each for each of the cards (see [Tim Dettmersâ€™ post](https://timdettmers.com/2018/12/16/deep-learning-hardware-guide/#PCIe_Lanes_and_Multi-GPU_Parallelism)).
    PCIe 4.0 or 5.0 are rarer and not needed for most deep learning usecases.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: æ„å»ºçš„ä¸‹ä¸€æ­¥æ˜¯é€‰æ‹©ä¸€ä¸ªå…è®¸å¤šä¸ªGPUçš„ä¸»æ¿ã€‚åœ¨è¿™é‡Œä¸»è¦è€ƒè™‘çš„æ˜¯PCIeé€šé“ã€‚æˆ‘ä»¬éœ€è¦**æ¯å—æ˜¾å¡è‡³å°‘æœ‰PCIe 3.0 x8é€šé“**ï¼ˆè§[Tim Dettmersçš„å¸–å­](https://timdettmers.com/2018/12/16/deep-learning-hardware-guide/#PCIe_Lanes_and_Multi-GPU_Parallelism)ï¼‰ã€‚PCIe
    4.0æˆ–5.0æ›´ä¸ºç¨€æœ‰ï¼Œå¯¹äºå¤§å¤šæ•°æ·±åº¦å­¦ä¹ ç”¨é€”å¹¶ä¸å¿…è¦ã€‚
- en: Apart from the slot type, the spacing of the slots will determine where you
    can place the GPUs. Make sure you have checked the spacing and that your GPUs
    can actually go where you want them to. Note that most motherboards will use x8
    configuration for some x16 slots when you use multiple GPUs. The only real way
    to get this information is on the manual of the card.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: é™¤äº†æ’æ§½ç±»å‹å¤–ï¼Œæ’æ§½çš„é—´è·å°†å†³å®šä½ å¯ä»¥æ”¾ç½® GPU çš„ä½ç½®ã€‚ç¡®ä¿ä½ å·²ç»æ£€æŸ¥äº†é—´è·ï¼Œå¹¶ä¸”ä½ çš„ GPU ç¡®å®å¯ä»¥æ”¾åœ¨ä½ æƒ³è¦çš„ä½ç½®ã€‚è¯·æ³¨æ„ï¼Œå¤§å¤šæ•°ä¸»æ¿åœ¨ä½¿ç”¨å¤šä¸ª
    GPU æ—¶ä¼šå°†ä¸€äº› x16 æ’æ§½é…ç½®ä¸º x8ã€‚è·å–è¿™äº›ä¿¡æ¯çš„å”¯ä¸€å¯é é€”å¾„æ˜¯æŸ¥é˜…æ˜¾å¡çš„æ‰‹å†Œã€‚
- en: 'The easiest way to not spend hours of research and also future-proof your system
    is to pick a motherboard that has x16 slots everywhere. You can use PCPartPicker
    and filter motherboards that have [7+ PCIe x16 slots](https://pcpartpicker.com/products/motherboard/#h=7,8&xcx=0).
    This gives us 21 products to choose from. We then [reduce the list](https://pcpartpicker.com/products/motherboard/#h=7,8&xcx=0&D=137438953472,2199023255552&mt=ddr4,ddr5)
    by selecting the minimum amount of RAM we want (e.g. 128 GB) with DDR4 / DDR5
    type to bring it down to 10 products:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€ç®€å•çš„æ–¹æ³•æ˜¯é¿å…èŠ±è´¹æ•°å°æ—¶çš„ç ”ç©¶ï¼Œå¹¶ä½¿ä½ çš„ç³»ç»Ÿæœªæ¥-proofï¼Œæ˜¯é€‰æ‹©ä¸€ä¸ªåˆ°å¤„éƒ½æœ‰ x16 æ’æ§½çš„ä¸»æ¿ã€‚ä½ å¯ä»¥ä½¿ç”¨ PCPartPicker å¹¶ç­›é€‰å‡ºå…·æœ‰
    [7+ PCIe x16 æ’æ§½](https://pcpartpicker.com/products/motherboard/#h=7,8&xcx=0) çš„ä¸»æ¿ã€‚è¿™ç»™æˆ‘ä»¬æä¾›äº†
    21 ç§äº§å“é€‰æ‹©ã€‚ç„¶åæˆ‘ä»¬ [ç¼©å‡åˆ—è¡¨](https://pcpartpicker.com/products/motherboard/#h=7,8&xcx=0&D=137438953472,2199023255552&mt=ddr4,ddr5)
    é€‰æ‹©æˆ‘ä»¬æƒ³è¦çš„æœ€å° RAM æ•°é‡ï¼ˆä¾‹å¦‚ 128 GBï¼‰ï¼Œå¹¶é€‰æ‹© DDR4 / DDR5 ç±»å‹ï¼Œå°†äº§å“æ•°é‡å‡å°‘åˆ° 10 ç§ï¼š
- en: '![](../Images/69337ca759a3ca823c2e554bc1134b3a.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/69337ca759a3ca823c2e554bc1134b3a.png)'
- en: Motherboards with at least 7 PCIe x16 slots and 128 GB DDR4/DDR5 RAM based on
    [PCPartPicker](https://pcpartpicker.com/products/motherboard/#h=7,8&xcx=0&D=137438953472,2199023255552&mt=ddr4,ddr5).
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: åŸºäº [PCPartPicker](https://pcpartpicker.com/products/motherboard/#h=7,8&xcx=0&D=137438953472,2199023255552&mt=ddr4,ddr5)
    çš„ä¸»æ¿è‡³å°‘æœ‰ 7 ä¸ª PCIe x16 æ’æ§½å’Œ 128 GB DDR4/DDR5 RAMã€‚
- en: 'The supported CPU sockets of the above list are LGA2011â€“3 and LGA2066\. We
    then move to the CPU selection and select CPUs with the desired number of cores.
    These are mainly needed for data loading and batch preparation. Aim to have at
    least **2 cores / 4 threads per GPU**. For the CPU we should also check the PCIe
    lanes it supports. Any CPU of the last decade should support at least 40 lanes
    (covering 4 GPUs at x8 lanes) but better be safe than sorry. With a filtering
    of e.g. [16+ cores with the above sockets](https://pcpartpicker.com/products/cpu/#C=16,64&k=28,35&xcx=0)
    we get the following CPUs:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸Šè¿°åˆ—è¡¨ä¸­æ”¯æŒçš„ CPU æ’æ§½æ˜¯ LGA2011â€“3 å’Œ LGA2066ã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬è½¬åˆ° CPU é€‰æ‹©ï¼Œé€‰æ‹©å…·æœ‰æ‰€éœ€æ ¸å¿ƒæ•°é‡çš„ CPUã€‚è¿™äº›ä¸»è¦ç”¨äºæ•°æ®åŠ è½½å’Œæ‰¹å¤„ç†å‡†å¤‡ã€‚æ¯ä¸ª
    GPU è‡³å°‘åº”æœ‰ **2 æ ¸å¿ƒ / 4 çº¿ç¨‹**ã€‚å¯¹äº CPUï¼Œæˆ‘ä»¬è¿˜åº”æ£€æŸ¥å®ƒæ”¯æŒçš„ PCIe é€šé“ã€‚è¿‡å»åå¹´çš„ä»»ä½• CPU åº”è¯¥è‡³å°‘æ”¯æŒ 40 æ¡é€šé“ï¼ˆè¦†ç›–
    4 ä¸ª GPUï¼Œæ¯ä¸ª GPU x8 é€šé“ï¼‰ï¼Œä½†æœ€å¥½è¿˜æ˜¯è°¨æ…ä¸ºå¥½ã€‚é€šè¿‡ç­›é€‰ä¾‹å¦‚ [å…·æœ‰ä¸Šè¿°æ’æ§½çš„ 16+ æ ¸ CPU](https://pcpartpicker.com/products/cpu/#C=16,64&k=28,35&xcx=0)ï¼Œæˆ‘ä»¬å¾—åˆ°ä»¥ä¸‹
    CPUï¼š
- en: 'Intel Xeon E5 (LGA2011â€“3): 8 results'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Intel Xeon E5 (LGA2011â€“3)ï¼š8 ä¸ªç»“æœ
- en: 'Intel Core i9 (LGA2066): 9 results'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Intel Core i9 (LGA2066)ï¼š9 ä¸ªç»“æœ
- en: We then pick our favorite combination of motherboard and CPU based on the number
    of cores, availability and price.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åï¼Œæˆ‘ä»¬æ ¹æ®æ ¸å¿ƒæ•°é‡ã€å¯ç”¨æ€§å’Œä»·æ ¼é€‰æ‹©æˆ‘ä»¬å–œæ¬¢çš„ä¸»æ¿å’Œ CPU ç»„åˆã€‚
- en: Both LGA2011â€“3 and LGA2066 sockets are very old (2014 and 2017 respectively),
    and therefore you can find good deals on ebay for both the motherboard and CPU.
    An ASRock X99 WS-E motherboard and a 18-core Intel Xeon E5â€“2697 V4 can cost you
    less than 300$ in used condition. Donâ€™t buy the cheaper ES or QS versions for
    CPUs as these are engineering samples and may fail âš ï¸ï¸.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: LGA2011â€“3 å’Œ LGA2066 æ’æ§½éƒ½å·²ç»éå¸¸è€æ—§ï¼ˆåˆ†åˆ«æ˜¯ 2014 å¹´å’Œ 2017 å¹´ï¼‰ï¼Œå› æ­¤ä½ å¯ä»¥åœ¨ eBay ä¸Šæ‰¾åˆ°è¿™ä¸¤ä¸ªä¸»æ¿å’Œ CPU
    çš„å¥½äº¤æ˜“ã€‚ä¸€å— ASRock X99 WS-E ä¸»æ¿å’Œä¸€é¢— 18 æ ¸çš„ Intel Xeon E5â€“2697 V4 åœ¨äºŒæ‰‹çŠ¶æ€ä¸‹å¯èƒ½èŠ±è´¹ä¸åˆ° 300 ç¾å…ƒã€‚ä¸è¦è´­ä¹°ä¾¿å®œçš„
    ES æˆ– QS ç‰ˆæœ¬çš„ CPUï¼Œå› ä¸ºè¿™äº›æ˜¯å·¥ç¨‹æ ·å“ï¼Œå¯èƒ½ä¼šå‡ºç°æ•…éšœ âš ï¸ï¸ã€‚
- en: If you want to buy something more powerful and/or more recent and/or an AMD
    CPU you can look into motherboards with e.g. 4+ PCIe x16 slots but make sure you
    check the slot spacings.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœä½ æƒ³è´­ä¹°æ›´å¼ºå¤§å’Œ/æˆ–æ›´æ–°çš„ç»„ä»¶å’Œ/æˆ– AMD CPUï¼Œå¯ä»¥æŸ¥çœ‹ä¾‹å¦‚ 4+ PCIe x16 æ’æ§½çš„ä¸»æ¿ï¼Œä½†ç¡®ä¿æ£€æŸ¥æ’æ§½é—´è·ã€‚
- en: At this stage itâ€™s a good idea to start a [PCPartPicker build](https://pcpartpicker.com/list/).
    ğŸ› ï¸
  id: totrans-61
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åœ¨è¿™ä¸ªé˜¶æ®µï¼Œå¼€å§‹ä¸€ä¸ª [PCPartPicker æ„å»º](https://pcpartpicker.com/list/) æ˜¯ä¸ªå¥½ä¸»æ„ã€‚ ğŸ› ï¸
- en: PCPartPicker will check compatibilities between components for you and will
    make your life easier.
  id: totrans-62
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: PCPartPicker ä¼šä¸ºä½ æ£€æŸ¥ç»„ä»¶ä¹‹é—´çš„å…¼å®¹æ€§ï¼Œè®©ä½ çš„ç”Ÿæ´»æ›´è½»æ¾ã€‚
- en: Step 3\. RAM ğŸ
  id: totrans-63
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç¬¬ 3 æ­¥ã€‚RAM ğŸ
- en: 'Here the most important aspect is the amount of RAM. RAM is used in different
    places of the deep learning cycle: loading data from disk for batch creation,
    loading the model and of course prototyping. The amount needed depends a lot on
    your application (e.g. 3D image data will need much more additional RAM) but you
    should aim for 1xâ€“2x the total amount of VRAM of your GPUs. The type should be
    at least DDR4 but the RAM clock is not very important, so donâ€™t spend your money
    there ğŸ•³ï¸.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™é‡Œï¼Œæœ€é‡è¦çš„æ–¹é¢æ˜¯ RAM çš„æ•°é‡ã€‚RAM åœ¨æ·±åº¦å­¦ä¹ å¾ªç¯ä¸­çš„ä¸åŒåœ°æ–¹ä½¿ç”¨ï¼šä»ç¡¬ç›˜åŠ è½½æ•°æ®ä»¥åˆ›å»ºæ‰¹æ¬¡ã€åŠ è½½æ¨¡å‹ä»¥åŠå½“ç„¶æ˜¯åŸå‹è®¾è®¡ã€‚æ‰€éœ€çš„ RAM æ•°é‡å¾ˆå¤§ç¨‹åº¦ä¸Šå–å†³äºæ‚¨çš„åº”ç”¨ç¨‹åºï¼ˆä¾‹å¦‚ï¼Œ3D
    å›¾åƒæ•°æ®éœ€è¦æ›´å¤šçš„é¢å¤– RAMï¼‰ï¼Œä½†æ‚¨åº”è¯¥ä»¥ GPU VRAM æ€»é‡çš„ 1 å€åˆ° 2 å€ä¸ºç›®æ ‡ã€‚ç±»å‹è‡³å°‘åº”ä¸º DDR4ï¼Œä½† RAM æ—¶é’Ÿä¸æ˜¯éå¸¸é‡è¦ï¼Œæ‰€ä»¥ä¸è¦æŠŠé’±èŠ±åœ¨è¿™é‡Œ
    ğŸ•³ï¸ã€‚
- en: When buying RAM you should make sure that the form factor, type, number of modules
    and memory per module all agree with your motherboard specs (PCPartPicker is your
    friend!).
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è´­ä¹° RAM æ—¶ï¼Œæ‚¨åº”è¯¥ç¡®ä¿å…¶å½¢çŠ¶å› ç´ ã€ç±»å‹ã€æ¨¡å—æ•°é‡å’Œæ¯ä¸ªæ¨¡å—çš„å†…å­˜éƒ½ä¸æ‚¨çš„ä¸»æ¿è§„æ ¼ä¸€è‡´ï¼ˆPCPartPicker æ˜¯æ‚¨çš„å¥½å¸®æ‰‹ï¼ï¼‰ã€‚
- en: Step 4\. Disks
  id: totrans-66
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç¬¬ 4 æ­¥ã€‚ç¡¬ç›˜
- en: Another component that you can save on is the disks ğŸ˜Œ. Again the amount of disk
    space is important and depends on the application. You donâ€™t necessarily need
    ultra-fast disks or NVMEs as they wonâ€™t affect your deep learning performance.
    The data will be anyway loaded to RAM and in order to not create a bottleneck
    you can simply use more parallel CPU workers.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: å¦ä¸€ä¸ªå¯ä»¥èŠ‚çœå¼€æ”¯çš„ç»„ä»¶æ˜¯ç¡¬ç›˜ ğŸ˜Œã€‚ç¡¬ç›˜ç©ºé—´çš„å¤§å°å¾ˆé‡è¦ï¼Œå¹¶ä¸”å–å†³äºåº”ç”¨ç¨‹åºã€‚æ‚¨ä¸ä¸€å®šéœ€è¦è¶…é«˜é€Ÿç¡¬ç›˜æˆ– NVMEï¼Œå› ä¸ºå®ƒä»¬ä¸ä¼šå½±å“æ‚¨çš„æ·±åº¦å­¦ä¹ æ€§èƒ½ã€‚æ•°æ®æœ€ç»ˆä¼šåŠ è½½åˆ°
    RAM ä¸­ï¼Œä¸ºäº†é¿å…æˆä¸ºç“¶é¢ˆï¼Œæ‚¨å¯ä»¥ç®€å•åœ°ä½¿ç”¨æ›´å¤šçš„å¹¶è¡Œ CPU å·¥ä½œçº¿ç¨‹ã€‚
- en: Step 5\. Power supply (PSU) ğŸ”Œ
  id: totrans-68
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç¬¬ 5 æ­¥ã€‚ç”µæºä¾›åº”å™¨ (PSU) ğŸ”Œ
- en: As we saw GPUs are power-hungry components. When setting up a multi-GPU system,
    the selection of the PSU becomes an important consideration. The majority of PSUs
    can deliver up to 1600w â€” this is in line with the power limits of US sockets.
    There are a few PSUs that can deliver more than that but need some research and
    they target especially miners.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: æ­£å¦‚æˆ‘ä»¬æ‰€è§ï¼ŒGPU æ˜¯é«˜åŠŸè€—ç»„ä»¶ã€‚åœ¨è®¾ç½®å¤š GPU ç³»ç»Ÿæ—¶ï¼Œé€‰æ‹© PSU æˆä¸ºä¸€ä¸ªé‡è¦çš„è€ƒè™‘å› ç´ ã€‚å¤§å¤šæ•° PSU èƒ½æä¾›é«˜è¾¾ 1600w çš„åŠŸç‡ â€”â€”
    è¿™ç¬¦åˆç¾å›½æ’åº§çš„åŠŸç‡é™åˆ¶ã€‚æœ‰ä¸€äº› PSU å¯ä»¥æä¾›æ›´é«˜çš„åŠŸç‡ï¼Œä½†éœ€è¦ä¸€äº›ç ”ç©¶ï¼Œå¹¶ä¸”å®ƒä»¬ç‰¹åˆ«é’ˆå¯¹çŸ¿å·¥ã€‚
- en: '![](../Images/8fe334aa8016473809c613cf470581b1.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8fe334aa8016473809c613cf470581b1.png)'
- en: Estimated wattage provided by PCPartPicker for your builds.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: PCPartPicker ä¸ºæ‚¨çš„æ„å»ºæä¾›çš„ä¼°ç®—åŠŸç‡ã€‚
- en: To determine the wattage of your system, you can use again PCPartPicker that
    computes the total amount of your build. To this we need to add an extra 10%+
    for peace of mind since GPUs will have spikes of power more than what is on their
    specs.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: è¦ç¡®å®šç³»ç»Ÿçš„åŠŸç‡ï¼Œæ‚¨å¯ä»¥å†æ¬¡ä½¿ç”¨ PCPartPicker æ¥è®¡ç®—æ‚¨æ„å»ºçš„æ€»åŠŸç‡ã€‚ä¸ºäº†ç¡®ä¿å®‰å…¨ï¼Œæˆ‘ä»¬éœ€è¦é¢å¤–å¢åŠ 10%ä»¥ä¸Šçš„åŠŸç‡ï¼Œå› ä¸º GPU çš„å®é™…åŠŸè€—æœ‰æ—¶ä¼šè¶…è¿‡å…¶è§„æ ¼ã€‚
- en: An important criterion is the **PSU efficiency** that is marked with the 80
    PLUS rating. The supply will reach the wattage it advertises but will lose some
    power in the process. 80 PLUS Bronze supplies are rated with 82% efficiency vs
    e.g. a Gold that will reach 87% efficiency. If we have a system that draws 1600w
    and we use it 20% of the time, we would save 22$ per year with a GPU with Gold
    rating, assuming a cost of 0.16$ / KWh. When comparing prices take that into account
    in your calculations.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªé‡è¦çš„æ ‡å‡†æ˜¯**PSU æ•ˆç‡**ï¼Œè¿™ç”± 80 PLUS è¯„çº§æ ‡è®°ã€‚ç”µæºå°†è¾¾åˆ°å…¶å®£ä¼ çš„åŠŸç‡ï¼Œä½†åœ¨è¿‡ç¨‹ä¸­ä¼šæŸå¤±ä¸€äº›åŠŸç‡ã€‚80 PLUS é“œç‰Œç”µæºçš„æ•ˆç‡ä¸º
    82%ï¼Œè€Œä¾‹å¦‚é‡‘ç‰Œç”µæºçš„æ•ˆç‡ä¸º 87%ã€‚å¦‚æœæˆ‘ä»¬æœ‰ä¸€ä¸ªåŠŸç‡éœ€æ±‚ä¸º 1600w çš„ç³»ç»Ÿï¼Œå¹¶ä¸”æˆ‘ä»¬åœ¨ 20% çš„æ—¶é—´å†…ä½¿ç”¨å®ƒï¼Œæˆ‘ä»¬å°†èŠ‚çœ 22 ç¾å…ƒæ¯å¹´ï¼Œå‰ææ˜¯ç”µä»·ä¸º
    0.16 ç¾å…ƒ/åƒç“¦æ—¶ã€‚åœ¨æ¯”è¾ƒä»·æ ¼æ—¶ï¼Œè¯·å°†è¿™ä¸€ç‚¹çº³å…¥æ‚¨çš„è®¡ç®—ä¸­ã€‚
- en: '![](../Images/f2c585bf5a9b2f3468913cc0cb4d5159.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f2c585bf5a9b2f3468913cc0cb4d5159.png)'
- en: PSU efficiency ratings. Table from [techguided](https://techguided.com/80-plus-bronze-vs-gold-vs-platinum-vs-titanium-which-psu-rating-do-you-need/).
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: PSU æ•ˆç‡è¯„çº§ã€‚è¡¨æ ¼æ¥æºäº [techguided](https://techguided.com/80-plus-bronze-vs-gold-vs-platinum-vs-titanium-which-psu-rating-do-you-need/)ã€‚
- en: When running at full load some PSUs are more **noisy** than others since they
    use a fan at high RPMs. If you are working (or sleeping!) close to your case this
    can have some effect, so itâ€™s a good idea to check the decibels from the manual
    ğŸ˜µ.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ»¡è´Ÿè·è¿è¡Œæ—¶ï¼Œä¸€äº› PSU æ¯”å…¶ä»–çš„**å™ªéŸ³**æ›´å¤§ï¼Œå› ä¸ºå®ƒä»¬ä½¿ç”¨é«˜é€Ÿè¿è½¬çš„é£æ‰‡ã€‚å¦‚æœæ‚¨åœ¨é è¿‘æœºç®±çš„åœ°æ–¹å·¥ä½œï¼ˆæˆ–ç¡è§‰ï¼ï¼‰ï¼Œè¿™å¯èƒ½ä¼šæœ‰ä¸€äº›å½±å“ï¼Œå› æ­¤æŸ¥çœ‹æ‰‹å†Œä¸­çš„åˆ†è´æ•°æ˜¯ä¸ªå¥½ä¸»æ„
    ğŸ˜µã€‚
- en: When selecting a supply, we need to verify that it has enough connectors for
    all our parts. GPUs in particular use 8 (or 6+2) pin cables. One important note
    here is that for each power slot of the GPU we should use **a separate 8 pin cable**
    and not use multiple outputs of the same cable (daisy-chaining). 8 pin cables
    are generally rated to ~150w. When using a single cable for more than one power
    slot the GPU may not get enough power and throttle.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨é€‰æ‹©ç”µæºæ—¶ï¼Œæˆ‘ä»¬éœ€è¦ç¡®è®¤å®ƒæ˜¯å¦æœ‰è¶³å¤Ÿçš„è¿æ¥å™¨æ¥æ”¯æŒæ‰€æœ‰éƒ¨ä»¶ã€‚ç‰¹åˆ«æ˜¯ GPU ä½¿ç”¨ 8 é’ˆï¼ˆæˆ– 6+2ï¼‰ç”µç¼†ã€‚è¿™é‡Œæœ‰ä¸€ä¸ªé‡è¦çš„æç¤ºï¼šå¯¹äº GPU çš„æ¯ä¸ªç”µæºæ’æ§½ï¼Œæˆ‘ä»¬åº”è¯¥ä½¿ç”¨**å•ç‹¬çš„
    8 é’ˆç”µç¼†**ï¼Œè€Œä¸æ˜¯ä½¿ç”¨åŒä¸€æ ¹ç”µç¼†çš„å¤šä¸ªè¾“å‡ºï¼ˆä¸²è”è¿æ¥ï¼‰ã€‚8 é’ˆç”µç¼†é€šå¸¸é¢å®šåŠŸç‡çº¦ä¸º 150wã€‚å½“ä½¿ç”¨å•æ ¹ç”µç¼†ä¸ºå¤šä¸ªç”µæºæ’æ§½ä¾›ç”µæ—¶ï¼ŒGPU å¯èƒ½æ— æ³•è·å¾—è¶³å¤Ÿçš„ç”µåŠ›ï¼Œä»è€Œå¯¼è‡´é™é¢‘ã€‚
- en: Step 6\. PC case
  id: totrans-78
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ­¥éª¤ 6\. æœºç®±
- en: Last but not least, selecting a PC case is not trivial. GPUs can get humongous
    and some cases will not fit them. A 4090 for instance can reach 36 cm length ğŸ‘»!
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åä½†åŒæ ·é‡è¦çš„æ˜¯ï¼Œé€‰æ‹©ä¸€ä¸ª PC æœºç®±å¹¶ä¸ç®€å•ã€‚GPU å¯ä»¥éå¸¸åºå¤§ï¼Œä¸€äº›æœºç®±å¯èƒ½æ— æ³•å®¹çº³å®ƒä»¬ã€‚ä¾‹å¦‚ï¼Œ4090 çš„é•¿åº¦å¯ä»¥è¾¾åˆ° 36 å˜ç±³ ğŸ‘»ï¼
- en: On top of that, mounting GPUs with PCIe risers may require some hacks. There
    are some some newer cases that allow to mount an additional card, especially dual
    system cases like the Phanteks Enthoo 719\. Another option is the Lian-Li O11D
    EVO that can house a GPU in upright position with the Lian-Li Upright GPU Bracket.
    I donâ€™t have these cases so Iâ€™m not sure how well they would fit e.g. multiple
    3090 / 4090\. However you can still mount a GPU upright even if your PC case doesnâ€™t
    directly support it with the Lian-Li bracket. You will need to drill 2â€“3 holes
    to the case but is not crazy.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤å¤–ï¼Œä½¿ç”¨ PCIe å»¶é•¿æ¡å®‰è£… GPU å¯èƒ½éœ€è¦ä¸€äº›æŠ€å·§ã€‚æœ‰ä¸€äº›è¾ƒæ–°çš„æœºç®±å…è®¸å®‰è£…é¢å¤–çš„æ˜¾å¡ï¼Œç‰¹åˆ«æ˜¯åƒ Phanteks Enthoo 719 è¿™æ ·çš„åŒç³»ç»Ÿæœºç®±ã€‚å¦ä¸€ä¸ªé€‰æ‹©æ˜¯
    Lian-Li O11D EVOï¼Œå®ƒå¯ä»¥é€šè¿‡ Lian-Li Upright GPU Bracket ä»¥ç›´ç«‹ä½ç½®å®¹çº³ GPUã€‚æˆ‘æ²¡æœ‰è¿™äº›æœºç®±ï¼Œæ‰€ä»¥ä¸ç¡®å®šå®ƒä»¬å¦‚ä½•é€‚é…ï¼Œä¾‹å¦‚å¤šä¸ª
    3090 / 4090ã€‚ç„¶è€Œï¼Œå³ä½¿ä½ çš„ PC æœºç®±ä¸ç›´æ¥æ”¯æŒç›´ç«‹å®‰è£…ï¼Œä½ ä»ç„¶å¯ä»¥ä½¿ç”¨ Lian-Li æ”¯æ¶æ¥ç›´ç«‹å®‰è£… GPUã€‚ä½ éœ€è¦åœ¨æœºç®±ä¸Šé’» 2-3 ä¸ªå­”ï¼Œä½†å¹¶ä¸æ˜¯å¾ˆå¤æ‚ã€‚
- en: '![](../Images/f678cf20bc13c76aa0594d1d51c3c98a.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f678cf20bc13c76aa0594d1d51c3c98a.png)'
- en: Mounting a GPU in an upright position with the Lian Li upright bracket.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨ Lian Li ç›´ç«‹æ”¯æ¶å°† GPU å®‰è£…åœ¨ç›´ç«‹ä½ç½®ã€‚
- en: The end
  id: totrans-83
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç»“æŸ
- en: I hope you enjoyed reading this guide and that you found some useful tips. The
    guide is aimed to help in your research on building a multi-GPU system, and not
    replace it. Feel free to send me any questions or comments you may have. If I
    am wrong on anything in the above I would really appreciate a comment or DM to
    make it even better ğŸ™!
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: å¸Œæœ›ä½ å–œæ¬¢é˜…è¯»æœ¬æŒ‡å—ï¼Œå¹¶ä¸”å‘ç°äº†ä¸€äº›æœ‰ç”¨çš„æç¤ºã€‚æœ¬æŒ‡å—æ—¨åœ¨å¸®åŠ©ä½ ç ”ç©¶å¦‚ä½•æ„å»ºå¤š GPU ç³»ç»Ÿï¼Œè€Œä¸æ˜¯æ›¿ä»£ç ”ç©¶ã€‚å¦‚æœä½ æœ‰ä»»ä½•é—®é¢˜æˆ–æ„è§ï¼Œè¯·éšæ—¶å‘ç»™æˆ‘ã€‚å¦‚æœæˆ‘åœ¨ä¸Šè¿°å†…å®¹ä¸­æœ‰ä»»ä½•é”™è¯¯ï¼Œæˆ‘éå¸¸æ„Ÿè°¢ä½ ç•™ä¸‹è¯„è®ºæˆ–ç§ä¿¡ï¼Œä»¥ä¾¿è¿›ä¸€æ­¥æ”¹è¿›
    ğŸ™ï¼
- en: 'Note: Unless otherwise noted, all images are by the author.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: æ³¨æ„ï¼šé™¤éå¦æœ‰è¯´æ˜ï¼Œæ‰€æœ‰å›¾ç‰‡å‡ç”±ä½œè€…æä¾›ã€‚
