- en: A Foundation Model for Satellite Images
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 卫星图像基础模型
- en: 原文：[https://towardsdatascience.com/a-foundation-model-for-satellite-images-dbf356c746a9?source=collection_archive---------8-----------------------#2023-11-04](https://towardsdatascience.com/a-foundation-model-for-satellite-images-dbf356c746a9?source=collection_archive---------8-----------------------#2023-11-04)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/a-foundation-model-for-satellite-images-dbf356c746a9?source=collection_archive---------8-----------------------#2023-11-04](https://towardsdatascience.com/a-foundation-model-for-satellite-images-dbf356c746a9?source=collection_archive---------8-----------------------#2023-11-04)
- en: The Prithvi-100M IBM Geospatial AI Foundation Model for NASA Earth Observation
    Data
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Prithvi-100M IBM 地理空间 AI 基础模型用于 NASA 地球观测数据
- en: '[](https://medium.com/@caroline.arnold_63207?source=post_page-----dbf356c746a9--------------------------------)[![Caroline
    Arnold](../Images/2560e106ba9deda7889c7d253792d814.png)](https://medium.com/@caroline.arnold_63207?source=post_page-----dbf356c746a9--------------------------------)[](https://towardsdatascience.com/?source=post_page-----dbf356c746a9--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----dbf356c746a9--------------------------------)
    [Caroline Arnold](https://medium.com/@caroline.arnold_63207?source=post_page-----dbf356c746a9--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@caroline.arnold_63207?source=post_page-----dbf356c746a9--------------------------------)[![Caroline
    Arnold](../Images/2560e106ba9deda7889c7d253792d814.png)](https://medium.com/@caroline.arnold_63207?source=post_page-----dbf356c746a9--------------------------------)[](https://towardsdatascience.com/?source=post_page-----dbf356c746a9--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----dbf356c746a9--------------------------------)
    [Caroline Arnold](https://medium.com/@caroline.arnold_63207?source=post_page-----dbf356c746a9--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F9367198e7a3c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-foundation-model-for-satellite-images-dbf356c746a9&user=Caroline+Arnold&userId=9367198e7a3c&source=post_page-9367198e7a3c----dbf356c746a9---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----dbf356c746a9--------------------------------)
    ·7 min read·Nov 4, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fdbf356c746a9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-foundation-model-for-satellite-images-dbf356c746a9&user=Caroline+Arnold&userId=9367198e7a3c&source=-----dbf356c746a9---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F9367198e7a3c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-foundation-model-for-satellite-images-dbf356c746a9&user=Caroline+Arnold&userId=9367198e7a3c&source=post_page-9367198e7a3c----dbf356c746a9---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----dbf356c746a9--------------------------------)
    ·7分钟阅读·2023年11月4日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fdbf356c746a9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-foundation-model-for-satellite-images-dbf356c746a9&user=Caroline+Arnold&userId=9367198e7a3c&source=-----dbf356c746a9---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fdbf356c746a9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-foundation-model-for-satellite-images-dbf356c746a9&source=-----dbf356c746a9---------------------bookmark_footer-----------)![](../Images/b28106f03767604cb36e6ac0c33a59e3.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fdbf356c746a9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-foundation-model-for-satellite-images-dbf356c746a9&source=-----dbf356c746a9---------------------bookmark_footer-----------)![](../Images/b28106f03767604cb36e6ac0c33a59e3.png)'
- en: 'Satellite image of the Karavasta Lagoon in Albania, 2017\. Image credit: [https://www.esa.int/var/esa/storage/images/esa_multimedia/images/2017/03/karavasta_lagoon_albania/16854373-1-eng-GB/Karavasta_Lagoon_Albania.jpg](https://www.esa.int/var/esa/storage/images/esa_multimedia/images/2017/03/karavasta_lagoon_albania/16854373-1-eng-GB/Karavasta_Lagoon_Albania.jpg).
    Contains modified Copernicus Sentinel data.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 阿尔巴尼亚卡拉瓦斯塔泻湖的卫星图像，2017年。图像来源：[https://www.esa.int/var/esa/storage/images/esa_multimedia/images/2017/03/karavasta_lagoon_albania/16854373-1-eng-GB/Karavasta_Lagoon_Albania.jpg](https://www.esa.int/var/esa/storage/images/esa_multimedia/images/2017/03/karavasta_lagoon_albania/16854373-1-eng-GB/Karavasta_Lagoon_Albania.jpg)。包含修改后的
    Copernicus Sentinel 数据。
- en: Foundation models are flexible deep learning algorithms that are designed for
    general tasks, rather than being immediately focused on specific tasks. Trained
    on large amounts of unlabeled data, they can be applied to a variety of downstream
    tasks with minimal fine-tuning. Foundation models are well known from natural
    language processing ([BERT](https://huggingface.co/bert-base-uncased), GPT-x),
    and image processing ([DALL-E](https://huggingface.co/dalle-mini/dalle-mini)).
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 基础模型是灵活的深度学习算法，旨在处理通用任务，而不是立即专注于特定任务。在大量未标记数据上进行训练后，它们可以通过最少的微调应用于各种下游任务。基础模型在自然语言处理（[BERT](https://huggingface.co/bert-base-uncased)，GPT-x）和图像处理（[DALL-E](https://huggingface.co/dalle-mini/dalle-mini)）中都很有名。
- en: In August 2023, NASA and IBM released the Geospatial AI Foundation Model for
    NASA Earth Observation Data. The model is available open source on [Huggingface](https://huggingface.co/ibm-nasa-geospatial)
    under the name of Prithvi, the Hindu goddess of Mother Earth. It has been trained
    on NASA satellite data — [according to IBM](https://research.ibm.com/blog/nasa-hugging-face-ibm),
    *more than 250 Petabyte* of data are available.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 2023年8月，NASA和IBM发布了用于NASA地球观测数据的地理空间AI基础模型。该模型以Prithvi命名，开放源代码在[Huggingface](https://huggingface.co/ibm-nasa-geospatial)上，Prithvi是印度教的大地女神。它已在NASA卫星数据上进行训练——[根据IBM](https://research.ibm.com/blog/nasa-hugging-face-ibm)，*超过250
    PB*的数据可用。
- en: In this blog post, we discuss
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇博客文章中，我们讨论
- en: The NASA Harmonized Sentinel-2 Landsat dataset used for training,
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于训练的NASA协调Sentinel-2 Landsat数据集，
- en: The architecture of the Prithvi-100M Geospatial AI Foundation Model,
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Prithvi-100M地理空间AI基础模型的架构，
- en: The training process on IBM’s Vela supercomputer,
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在IBM的Vela超级计算机上的训练过程，
- en: 'Example applications: flooding and crop type identification.'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 示例应用：洪水和作物类型识别。
- en: Training data
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练数据
- en: The Geospatial AI Foundation Model has been trained on [NASA Harmonized LandSat
    Sentinel-2 data](https://docs.sentinel-hub.com/api/latest/data/hls/).
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 地理空间AI基础模型已在[NASA协调的LandSat Sentinel-2数据](https://docs.sentinel-hub.com/api/latest/data/hls/)上进行训练。
- en: '[Sentinel-2](https://en.wikipedia.org/wiki/Sentinel-2) is a satellite mission
    coordinated by the European Space Agency, with two satellites currently in orbit
    taking high-resolution images of the Earth. It focuses on land, coastal areas,
    and selected open waters. The Landsat satellites were launched by NASA to record
    surface reflectance. The harmonized data combine the input from both sensors,
    resulting in a spatial resolution of about 30 meters and an average revisit time
    of two to three days. This resolution is sufficient for agricultural monitoring,
    land use classification, and natural disaster detection.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '[Sentinel-2](https://en.wikipedia.org/wiki/Sentinel-2)是由欧洲航天局协调的卫星任务，目前有两颗卫星在轨道上拍摄地球的高分辨率图像。它专注于陆地、沿海地区和特定的开放水域。Landsat卫星由NASA发射，用于记录地表反射。协调数据结合了两个传感器的输入，
    resulting in a spatial resolution of about 30 meters and an average revisit time
    of two to three days. This resolution is sufficient for agricultural monitoring,
    land use classification, and natural disaster detection.'
- en: 'Standard photographs are made up of three colors: red, green, and blue. The
    Sentinel-2 data is available in a total of 13 “colors”, so-called *bands*, spanning
    the visible, near-infrared, and shortwave infrared range of the electromagnetic
    spectrum. Selected bands can be used to identify different things, e.g. the infrared
    bands contain information about the vegetation. For background, see [this post](https://gisgeography.com/sentinel-2-bands-combinations/)
    on Sentinel-2 band combinations.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 标准照片由红色、绿色和蓝色三种颜色组成。Sentinel-2数据总共提供13种“颜色”，即所谓的*波段*，涵盖可见光、近红外和短波红外电磁谱范围。选择的波段可以用于识别不同的事物，例如，红外波段包含有关植被的信息。有关背景，请参见[这篇文章](https://gisgeography.com/sentinel-2-bands-combinations/)关于Sentinel-2波段组合。
- en: '![](../Images/f16808f35a9ef0e6db6646f1f45e7a32.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f16808f35a9ef0e6db6646f1f45e7a32.png)'
- en: 'False-color infrared image of a Hawaiian airport. Image source: ESA sentinel
    satellite imagery, CC BY-SA 4.0 <[https://creativecommons.org/licenses/by-sa/4.0](https://creativecommons.org/licenses/by-sa/4.0)>,
    via Wikimedia Commons.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 夏威夷机场的伪彩色红外图像。图像来源：ESA sentinel卫星图像，CC BY-SA 4.0 <[https://creativecommons.org/licenses/by-sa/4.0](https://creativecommons.org/licenses/by-sa/4.0)>,
    通过维基媒体共享资源。
- en: Clouds block the view of Earth observation satellites. To counteract this effect,
    Sentinel-2 provides a band that can be used to identify cloud cover. The affected
    pixels are masked so as not to confuse the image processing algorithms.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 云层阻碍了地球观测卫星的视线。为应对这一影响，Sentinel-2提供了一个可用于识别云层覆盖的波段。受影响的像素被屏蔽，以免干扰图像处理算法。
- en: As such, Sentinel-2 and Landsat data are unlabeled. It requires significant
    human effort and expertise to provide a pixel-wise classification of land use
    categories. Foundation models are highly versatile and extract structure from
    data without requiring labeled data for the initial stage of the training procedure.
    Therefore, they seem very promising for Earth observation data.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，Sentinel-2和Landsat数据是未标记的。需要大量的人力和专业知识才能提供逐像素的土地使用类别分类。基础模型高度通用，并从数据中提取结构，而无需在训练过程的初始阶段提供标记数据。因此，它们在地球观测数据方面显得非常有前途。
- en: Model Architecture
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型架构
- en: 'The Prithvi-100M Geospatial AI Foundation Model builds on a temporal vision
    transformer and on a masked autoencoder. The model card is shown on Huggingface:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: Prithvi-100M地理空间AI基础模型基于时间序列视觉变换器和掩蔽自编码器。模型卡显示在Huggingface上：
- en: '![](../Images/e08769dab80b596aebf0969672e4b16a.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e08769dab80b596aebf0969672e4b16a.png)'
- en: 'Model card for Prithvi-100M on Huggingface. Image source: [https://huggingface.co/ibm-nasa-geospatial/Prithvi-100M/blob/main/GFM.png](https://huggingface.co/ibm-nasa-geospatial/Prithvi-100M/blob/main/GFM.png)'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: Huggingface上的Prithvi-100M模型卡。图像来源：[https://huggingface.co/ibm-nasa-geospatial/Prithvi-100M/blob/main/GFM.png](https://huggingface.co/ibm-nasa-geospatial/Prithvi-100M/blob/main/GFM.png)
- en: The model accepts Landsat images in video format as input. Images taken at the
    same location are loaded as a time series, while static images can be processed
    by setting the time series length to 1\. The bands correspond to the channels
    of the vision transformer.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型接受视频格式的Landsat图像作为输入。来自同一地点的图像被加载为时间序列，而静态图像可以通过将时间序列长度设置为1进行处理。波段对应于视觉变换器的通道。
- en: '**Vision Transformer**'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '**视觉变换器**'
- en: In 2020, a team from Google Research showed that transformers could be applied
    not only to natural language processing, but also to images [(Dosovitsky et al,
    ICLR 2020)](https://arxiv.org/pdf/2010.11929.pdf). Until then, convolutional neural
    networks had been the *de facto* standard for image processing.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在2020年，Google Research的团队展示了变换器不仅可以应用于自然语言处理，还可以应用于图像 [(Dosovitsky et al, ICLR
    2020)](https://arxiv.org/pdf/2010.11929.pdf)。在那之前，卷积神经网络一直是图像处理的*事实上的*标准。
- en: The vision transformer first cuts images into small patches, similar to the
    tokenization of sentences for a language processing transformer. Then, learnable
    embeddings and positional encodings are added. In the original paper, it was shown
    that with large amounts of training data, the vision transformer can outperform
    typical computer vision architectures such as ResNet.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 视觉变换器首先将图像切割成小块，类似于对语言处理变换器进行句子的标记化。然后，添加可学习的嵌入和位置编码。在原始论文中，展示了在大量训练数据下，视觉变换器可以超越典型的计算机视觉架构，如ResNet。
- en: '[](https://yurkovak.medium.com/vision-transformer-vit-under-the-magnifying-glass-part-1-70be8d6661a7?source=post_page-----dbf356c746a9--------------------------------)
    [## Vision Transformer (ViT) under the magnifying glass, Part 1'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://yurkovak.medium.com/vision-transformer-vit-under-the-magnifying-glass-part-1-70be8d6661a7?source=post_page-----dbf356c746a9--------------------------------)
    [## 视觉变换器（ViT）放大镜下，第一部分'
- en: Embeddings
  id: totrans-33
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 嵌入
- en: yurkovak.medium.com](https://yurkovak.medium.com/vision-transformer-vit-under-the-magnifying-glass-part-1-70be8d6661a7?source=post_page-----dbf356c746a9--------------------------------)
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: yurkovak.medium.com](https://yurkovak.medium.com/vision-transformer-vit-under-the-magnifying-glass-part-1-70be8d6661a7?source=post_page-----dbf356c746a9--------------------------------)
- en: '**Masked Autoencoder**'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '**掩蔽自编码器**'
- en: 'The Prithvi-100M masked autoencoder is based on the original implementation
    by He et al (2021), [https://arxiv.org/pdf/2111.06377.pdf](https://arxiv.org/pdf/2111.06377.pdf).
    The concept is straightforward:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: Prithvi-100M掩蔽自编码器基于He等人（2021）的原始实现，[https://arxiv.org/pdf/2111.06377.pdf](https://arxiv.org/pdf/2111.06377.pdf)。概念很简单：
- en: Random patches in an image are masked. The autoencoder learns to predict the
    missing pixels. This is similar to the training of large language models, where
    the model learns to predict missing words from a sentence.
  id: totrans-37
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 图像中的随机块被掩蔽。自编码器学习预测缺失的像素。这类似于大型语言模型的训练，其中模型学习预测句子中缺失的单词。
- en: In the original paper, 2D images with RGB (red, green, blue) color channels
    are considered. The difference between training on language data and image data
    is extensively discussed in the paper.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在原始论文中，考虑了带有RGB（红色、绿色、蓝色）颜色通道的2D图像。论文中广泛讨论了在语言数据和图像数据上进行训练的区别。
- en: The encoder works only on the unmasked patches, which saves computation time.
    The embedding is taken care of by providing a linear projection of the individual
    patches, containing learnable parameters.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 编码器仅在未被遮挡的图像块上工作，这样可以节省计算时间。嵌入由对单独图像块的线性投影来处理，该投影包含可学习参数。
- en: Position embedding is important so that the algorithm knows where a patch is
    in the original image. In the case of the Masked Autoencoder, position embedding
    is provided by a 2D sine-cosine function that is typically used in transformer
    models. It encodes the position of a patch in a 2D grid of the overall image.
    The positional embedding may contain learnable parameters, but this does not appear
    to be the case in the implementation in the [MAE repository](https://github.com/facebookresearch/mae/blob/efb2a8062c206524e35e47d04501ed4f544c0ae8/util/pos_embed.py#L38).
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 位置嵌入很重要，以便算法知道图像块在原始图像中的位置。在遮挡自编码器的情况下，位置嵌入通过2D正弦-余弦函数提供，这种函数通常用于变换器模型。它对图像中的2D网格位置进行编码。位置嵌入可能包含可学习的参数，但在[MAE库](https://github.com/facebookresearch/mae/blob/efb2a8062c206524e35e47d04501ed4f544c0ae8/util/pos_embed.py#L38)的实现中似乎并非如此。
- en: '![](../Images/3834b13d42f6bfe665feeea3305d63f8.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3834b13d42f6bfe665feeea3305d63f8.png)'
- en: 'Masked autoencoder in action. Left: Masked patches of the original image. Middle.
    Reconstruction. Right: Ground truth. Image source: [https://arxiv.org/pdf/2111.06377.pdf](https://arxiv.org/pdf/2111.06377.pdf)
    (Figure 2)'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 遮挡自编码器的应用。左侧：原始图像的遮挡图像块。中间：重建。右侧：真实情况。图像来源：[https://arxiv.org/pdf/2111.06377.pdf](https://arxiv.org/pdf/2111.06377.pdf)（图2）
- en: '**Changes to the MAE Architecture**'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '**MAE架构的变化**'
- en: In order to process time series of satellite data with more channels, the NASA
    and IBM team made several changes to the Masked Autoencoder architecture.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 为了处理具有更多通道的卫星数据时间序列，NASA和IBM团队对遮挡自编码器架构进行了若干修改。
- en: The 2D patch embedding was changed to a 3D patch embedding
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 2D图像块嵌入被更改为3D图像块嵌入。
- en: The 2D positional embedding was changed to a 3D positional embedding
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 2D位置嵌入被更改为3D位置嵌入。
- en: The patch creation takes into account the 3D nature of the data
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图像块创建考虑到数据的3D特性。
- en: Besides RGB colors, one near-infrared and two short-wave infrared bands were
    added
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 除了RGB颜色外，还增加了一个近红外和两个短波红外波段。
- en: '**Loss function**'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '**损失函数**'
- en: The mean squared error (MSE) loss is used for training by comparing the original
    and the reconstructed images pixel by pixel.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 均方误差（MSE）损失用于训练，通过逐像素比较原始图像和重建图像。
- en: Model Training
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型训练
- en: 'The model training procedure is described on the IBM blog: [https://research.ibm.com/blog/nasa-hugging-face-ibm](https://research.ibm.com/blog/nasa-hugging-face-ibm).
    Unfortunately, not a lot of detail is provided. IBM mentions, however, that they
    trained on their company’s AI supercomputer Vela. [Vela](https://research.ibm.com/blog/AI-supercomputer-Vela-GPU-cluster)
    is a fully cloud-based supercomputer that operates exclusively for IBM Research.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 模型训练过程描述在IBM博客中：[https://research.ibm.com/blog/nasa-hugging-face-ibm](https://research.ibm.com/blog/nasa-hugging-face-ibm)。遗憾的是，提供的细节不多。然而，IBM提到他们在公司AI超级计算机Vela上进行了训练。[Vela](https://research.ibm.com/blog/AI-supercomputer-Vela-GPU-cluster)是一个完全基于云的超级计算机，仅为IBM研究部门运营。
- en: The supercomputer consists of 200 nodes. Each node is equipped with 8 NVIDIA
    A100 GPUs, each with 80 GB of GPU memory. The node RAM is 1.5 TB, and four 3.2
    TB local hard drives are available. This accounts for the large datasets that
    need to be handled to train foundation models. The nodes are connected with a
    network that can transfer up to 100 GB/second.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 超级计算机由200个节点组成。每个节点配备了8个NVIDIA A100 GPU，每个GPU有80 GB的内存。节点RAM为1.5 TB，并且配备四个3.2
    TB的本地硬盘。这些配置能够处理训练基础模型所需的大数据集。节点之间通过一个能传输高达100 GB/秒的网络连接。
- en: Application
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 应用
- en: 'The Prithvi-100M Geospatial AI Foundation Model can be applied to a variety
    of downstream tasks. We focus on two tasks: Flooding and crop type identification.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: Prithvi-100M地理空间AI基础模型可以应用于多种下游任务。我们专注于两个任务：洪水和作物类型识别。
- en: '**Flooding**'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '**洪水**'
- en: Keeping the original encoder part of Prithvi-100M, the model is now adapted
    to predict the extension of flooding in a satellite image. Details are described
    on [Huggingface](https://huggingface.co/ibm-nasa-geospatial/Prithvi-100M-sen1floods11).
    The [Sen1Floods11](https://github.com/cloudtostreet/Sen1Floods11) dataset is used
    for finetuning, covering 11 flood events on six continents.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 保留 Prithvi-100M 的原始编码器部分，模型现在被调整为预测卫星图像中洪水的扩展。详细信息描述在 [Huggingface](https://huggingface.co/ibm-nasa-geospatial/Prithvi-100M-sen1floods11)。[Sen1Floods11](https://github.com/cloudtostreet/Sen1Floods11)
    数据集用于微调，涵盖了六大洲的 11 次洪水事件。
- en: '![](../Images/83cc4025a63d4e795321d2d1a3479a8a.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/83cc4025a63d4e795321d2d1a3479a8a.png)'
- en: 'Finetuning the Geospatial AI Foundation Model for flood detection. Image source:
    [https://huggingface.co/ibm-nasa-geospatial/Prithvi-100M-sen1floods11/blob/main/sen1floods11-finetuning.png](https://huggingface.co/ibm-nasa-geospatial/Prithvi-100M-sen1floods11/blob/main/sen1floods11-finetuning.png)'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 微调地理空间 AI 基础模型以进行洪水检测。图像来源：[https://huggingface.co/ibm-nasa-geospatial/Prithvi-100M-sen1floods11/blob/main/sen1floods11-finetuning.png](https://huggingface.co/ibm-nasa-geospatial/Prithvi-100M-sen1floods11/blob/main/sen1floods11-finetuning.png)
- en: In order to prepare Prithvi-100M for the downstream task, the embedding shape
    needs to be converted back to the original image shape. Then, a final 2D convolutional
    layer is added that applies the task-specific classification.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 为了将 Prithvi-100M 准备好以应对下游任务，需要将嵌入形状转换回原始图像形状。然后，添加一个最终的 2D 卷积层，应用特定任务的分类。
- en: Each pixel in the image is classified as being either water or non-water (land).
    Since this is a classification problem, the binary cross-entropy loss is used.
    Only one image is processed at a time, so the time series functionality of Prithvi-100M
    is not used here.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 图像中的每个像素被分类为水域或非水域（陆地）。由于这是一个分类问题，因此使用了二元交叉熵损失。一次只处理一张图像，因此未使用 Prithvi-100M
    的时间序列功能。
- en: The authors report a mean accuracy of 93% and mean intersection over union of
    86% for a holdout flood event in Bolivia.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 作者报告了在玻利维亚的一个保留洪水事件中，平均准确率为 93%，平均交并比为 86%。
- en: A demo page is provided where users can upload their own Sentinel-2 images and
    ask Prithvi-100M to identify flooding.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 提供了一个演示页面，用户可以上传自己的 Sentinel-2 图像，并要求 Prithvi-100M 识别洪水。
- en: '![](../Images/20093b5ca8b238b689f912a7d4da62e4.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/20093b5ca8b238b689f912a7d4da62e4.png)'
- en: 'Snapshot of the demo for flood identification. Black pixels correspond to land,
    white pixels to water. Image source: [https://huggingface.co/spaces/ibm-nasa-geospatial/Prithvi-100M-sen1floods11-demo](https://huggingface.co/spaces/ibm-nasa-geospatial/Prithvi-100M-sen1floods11-demo)
    (Using India_900498_S2Hand.tif)'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 洪水识别演示的快照。黑色像素对应陆地，白色像素对应水域。图像来源：[https://huggingface.co/spaces/ibm-nasa-geospatial/Prithvi-100M-sen1floods11-demo](https://huggingface.co/spaces/ibm-nasa-geospatial/Prithvi-100M-sen1floods11-demo)（使用
    India_900498_S2Hand.tif）
- en: '**Crop type identification**'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '**作物类型识别**'
- en: To leverage the time series functionality, the authors provide a demo for crop
    type identification. The crop type ground truth is given by labeled images. This
    is a multi-class classification problem, and cross-entropy loss is used for training.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 为了利用时间序列功能，作者提供了作物类型识别的演示。作物类型的实际情况由标记图像提供。这是一个多类分类问题，训练时使用了交叉熵损失。
- en: '![](../Images/790eec887281215a817d52b393442904.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/790eec887281215a817d52b393442904.png)'
- en: 'Multi-temporal crop type classification as a downstream task for Prithvi-100M.
    Image source: [https://huggingface.co/ibm-nasa-geospatial/Prithvi-100M-multi-temporal-crop-classification/blob/main/multi_temporal_crop_classification.png](https://huggingface.co/ibm-nasa-geospatial/Prithvi-100M-multi-temporal-crop-classification/blob/main/multi_temporal_crop_classification.png)'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 作为 Prithvi-100M 的下游任务，进行多时相作物类型分类。图像来源：[https://huggingface.co/ibm-nasa-geospatial/Prithvi-100M-multi-temporal-crop-classification/blob/main/multi_temporal_crop_classification.png](https://huggingface.co/ibm-nasa-geospatial/Prithvi-100M-multi-temporal-crop-classification/blob/main/multi_temporal_crop_classification.png)
- en: The authors report different accuracies for the different crop types. On average,
    the accuracy is 64%, and intersection over union is 46%. However, the authors
    note that the ground truth is noisy, and more accurate labels would help to improve
    this downstream task.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 作者报告了不同作物类型的不同准确率。平均准确率为 64%，交并比为 46%。然而，作者指出实际情况存在噪声，更准确的标签将有助于改进这一下游任务。
- en: '![](../Images/e53d8ac5ed3840af8ae01d0223a6c806.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e53d8ac5ed3840af8ae01d0223a6c806.png)'
- en: 'Crop type demo for Prithvi-100M. The three figures on the left show the time
    series of satellite images. The right figure shows the model prediction, with
    each pixel colored according to the crop type. Image source: [https://huggingface.co/spaces/ibm-nasa-geospatial/Prithvi-100M-multi-temporal-crop-classification-demo](https://huggingface.co/spaces/ibm-nasa-geospatial/Prithvi-100M-multi-temporal-crop-classification-demo)'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: Prithvi-100M 的作物类型演示。左侧三幅图显示卫星图像的时间序列。右侧图显示模型预测，每个像素根据作物类型着色。图片来源：[https://huggingface.co/spaces/ibm-nasa-geospatial/Prithvi-100M-multi-temporal-crop-classification-demo](https://huggingface.co/spaces/ibm-nasa-geospatial/Prithvi-100M-multi-temporal-crop-classification-demo)
- en: Summary
  id: totrans-73
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 总结
- en: We have covered the Geospatial AI Foundation Model, currently (2023) the largest
    geospatial model on Huggingface under the name Prithvi-100M. The model is developed
    by IBM Research and NASA and uses the Landsat dataset for training.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经介绍了地球空间 AI 基础模型，目前（2023 年）在 Huggingface 上以 Prithvi-100M 的名义是最大的地球空间模型。该模型由
    IBM Research 和 NASA 开发，使用 Landsat 数据集进行训练。
- en: We have covered the training data, architecture, and training procedure of the
    Geospatial AI Foundation Model. The model is available as open source and can
    be fine-tuned for more specific tasks. Flood detection and crop type identification
    applications have shown the great potential of the Geospatial AI Foundation Model.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经介绍了地球空间 AI 基础模型的训练数据、架构和训练过程。该模型开放源代码，可以进行更具体任务的微调。洪水检测和作物类型识别应用展示了地球空间
    AI 基础模型的巨大潜力。
- en: Since Sentinel-2 data is available for individual non-commercial use, it would
    be possible for interested users to create their own model tuned to a specific
    downstream task. In a future post, I will show how to fine-tune the Geospatial
    AI Foundation Model for vegetation identification and super-resolution.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 Sentinel-2 数据可用于个人非商业用途，有兴趣的用户可以创建适用于特定下游任务的自己的模型。在未来的帖子中，我将展示如何为植被识别和超分辨率微调地球空间
    AI 基础模型。
- en: Further Reading
  id: totrans-77
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: Jakubik et al, Prithvi-100M, [https://github.com/NASA-IMPACT/hls-foundation-os](https://github.com/NASA-IMPACT/hls-foundation-os),
    2023.
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jakubik 等人，Prithvi-100M，[https://github.com/NASA-IMPACT/hls-foundation-os](https://github.com/NASA-IMPACT/hls-foundation-os)，2023。
- en: 'Prithvi-100M on Huggingface: [https://huggingface.co/ibm-nasa-geospatial](https://huggingface.co/ibm-nasa-geospatial)'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Prithvi-100M 在 Huggingface 上：[https://huggingface.co/ibm-nasa-geospatial](https://huggingface.co/ibm-nasa-geospatial)
- en: 'Sentinel-2 satellite bands: [https://gisgeography.com/sentinel-2-bands-combinations/](https://gisgeography.com/sentinel-2-bands-combinations/)'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sentinel-2 卫星波段：[https://gisgeography.com/sentinel-2-bands-combinations/](https://gisgeography.com/sentinel-2-bands-combinations/)
- en: He et al (2021), “Masked Autoencoders are Scalable Vision Learners”, [https://arxiv.org/pdf/2111.06377.pdf](https://arxiv.org/pdf/2111.06377.pdf)
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: He 等人（2021），“Masked Autoencoders are Scalable Vision Learners”，[https://arxiv.org/pdf/2111.06377.pdf](https://arxiv.org/pdf/2111.06377.pdf)
- en: 'Dosovitskiy et al (2020), “An Image is Worth 16x16 Words: Transformers for
    Image Recognition at Scale”, ICLR 2020, [https://arxiv.org/abs/2010.11929](https://arxiv.org/abs/2010.11929)'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Dosovitskiy 等人（2020），“An Image is Worth 16x16 Words: Transformers for Image
    Recognition at Scale”，ICLR 2020，[https://arxiv.org/abs/2010.11929](https://arxiv.org/abs/2010.11929)'
- en: '[](/environmental-data-science-an-introduction-127b4b3422dc?source=post_page-----dbf356c746a9--------------------------------)
    [## Environmental Data Science: An Introduction'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/environmental-data-science-an-introduction-127b4b3422dc?source=post_page-----dbf356c746a9--------------------------------)
    [## 环境数据科学：简介'
- en: Examples, challenges, and perspectives for working with environmental data
  id: totrans-84
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 处理环境数据的示例、挑战和展望
- en: towardsdatascience.com](/environmental-data-science-an-introduction-127b4b3422dc?source=post_page-----dbf356c746a9--------------------------------)
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/environmental-data-science-an-introduction-127b4b3422dc?source=post_page-----dbf356c746a9--------------------------------)
