- en: Entropy-Regularized Reinforcement Learning Explained
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 熵正则化强化学习解释
- en: 原文：[https://towardsdatascience.com/entropy-regularized-reinforcement-learning-explained-2ba959c92aad](https://towardsdatascience.com/entropy-regularized-reinforcement-learning-explained-2ba959c92aad)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/entropy-regularized-reinforcement-learning-explained-2ba959c92aad](https://towardsdatascience.com/entropy-regularized-reinforcement-learning-explained-2ba959c92aad)
- en: Learn more reliable, robust, and transferable policies by adding entropy bonuses
    to your algorithm
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 通过为算法添加熵奖励来学习更可靠、稳健和可迁移的策略
- en: '[](https://wvheeswijk.medium.com/?source=post_page-----2ba959c92aad--------------------------------)[![Wouter
    van Heeswijk, PhD](../Images/9c996bccd6fdfb6d9aa8b50b93338eb9.png)](https://wvheeswijk.medium.com/?source=post_page-----2ba959c92aad--------------------------------)[](https://towardsdatascience.com/?source=post_page-----2ba959c92aad--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----2ba959c92aad--------------------------------)
    [Wouter van Heeswijk, PhD](https://wvheeswijk.medium.com/?source=post_page-----2ba959c92aad--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://wvheeswijk.medium.com/?source=post_page-----2ba959c92aad--------------------------------)[![Wouter
    van Heeswijk, PhD](../Images/9c996bccd6fdfb6d9aa8b50b93338eb9.png)](https://wvheeswijk.medium.com/?source=post_page-----2ba959c92aad--------------------------------)[](https://towardsdatascience.com/?source=post_page-----2ba959c92aad--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----2ba959c92aad--------------------------------)
    [Wouter van Heeswijk, PhD](https://wvheeswijk.medium.com/?source=post_page-----2ba959c92aad--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----2ba959c92aad--------------------------------)
    ·8 min read·Oct 26, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----2ba959c92aad--------------------------------)
    ·8 min read·2023年10月26日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/2fce76a87f64dcb27147a0868dc84b72.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2fce76a87f64dcb27147a0868dc84b72.png)'
- en: Photo by [Jeremy Thomas](https://unsplash.com/@jeremythomasphoto?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由 [Jeremy Thomas](https://unsplash.com/@jeremythomasphoto?utm_source=medium&utm_medium=referral)
    提供，来源于 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: '*Entropy* is a concept associated with a state of disorder, randomness, or
    uncertainty. It can be considered as a **measure of information for random variables**.
    Traditionally, it is associated with fields such as thermodynamics, but the term
    found its way to many other domains.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '*熵* 是与无序、随机或不确定状态相关的概念。它可以被视为**随机变量的信息度量**。传统上，它与热力学等领域相关，但这一术语也被引入了许多其他领域。'
- en: 'In 1948, Claude Shannon introduced the notion of entropy in information theory.
    In this context, an event is considered to offer more information if it has a
    lower probability of happening; the **information of an event is inversely correlated
    to its probability of occurrence**. Intuitively: we learn more from rare events.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 1948年，克劳德·香农在信息论中引入了熵的概念。在这种背景下，如果一个事件发生的概率较低，它被认为提供了更多的信息；**事件的信息与其发生的概率成反比**。直观地说：我们从稀有事件中学到的更多。
- en: 'The notion of entropy can be formalized as follows:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 熵的概念可以被形式化为以下内容：
- en: '![](../Images/e73e66ad287c526dbcbf6c48ef1d9c81.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e73e66ad287c526dbcbf6c48ef1d9c81.png)'
- en: Definition of entropy over a set of events x. The information of each event
    is inversely correlated to its probability of occurrence.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 对一组事件 x 的熵定义。每个事件的信息与其发生的概率成反比。
- en: In Reinforcement Learning (RL), the notion of entropy has been deployed as well,
    with the purpose of encouraging exploration. In this context, entropy is a **measure
    of predictability of actions** returned by a stochastic policy.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在强化学习（RL）中，熵的概念也被应用，目的是鼓励探索。在这种背景下，熵是**由随机策略返回的动作的可预测性度量**。
- en: Concretely, **RL takes the entropy of the policy (i.e., probability distribution
    of actions) as a bonus and embeds it as a reward component**. This article addresses
    the basic case, but entropy bonuses are an integral part of many state-of-the-art
    RL algorithms.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，**RL 将策略的熵（即，动作的概率分布）作为奖励的一个组成部分来加以奖励**。这篇文章讨论了基本情况，但熵奖励是许多最先进的 RL 算法的一个重要组成部分。
- en: What is entropy?
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是熵？
- en: First, let’s first build a bit of intuition for the concept of entropy. The
    figure below shows policies with low and high entropy, respectively. The low-entropy
    policy is nearly deterministic; we almost always select the same action. In the
    high-entropy policy, there is much more randomness in the action that we select.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们对熵的概念建立一些直觉。下图显示了低熵和高熵策略。低熵策略几乎是确定的；我们几乎总是选择相同的行动。在高熵策略中，我们选择的行动具有更多的随机性。
- en: '![](../Images/8249108e3727eaceb0769c8cdaf37b7d.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8249108e3727eaceb0769c8cdaf37b7d.png)'
- en: Example of low-entropy policy (left) and high-entropy policy (right). In the
    high-entropy policy, there is much more randomness in the action selection [image
    by author]
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 低熵策略（左）和高熵策略（右）的示例。在高熵策略中，行动选择中有更多的随机性。[作者提供的图片]
- en: Next, let’s consider the entropy of a coin flip.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们考虑硬币翻转的熵。
- en: Shannon’s entropy utilizes a log function with base 2 (np.log2 in Numpy) and
    the corresponding unit of measurement is called a bit. Other forms of entropy
    use different bases. These distinctions are not terribly important to grasp the
    main idea.
  id: totrans-20
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 香农的熵利用以2为底的对数函数（在Numpy中为np.log2），相应的测量单位称为比特。其他形式的熵使用不同的底数。这些区别对于掌握主要思想并不是特别重要。
- en: What happen if the coin is loaded? The figure below shows that **entropy would
    decrease, as there is more certainty whether a given outcome would occur**.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 如果硬币是加重的会发生什么？下图显示了**熵将会减少，因为更有可能确定某一结果是否会发生**。
- en: '![](../Images/a1b3060c05698700a76957ef49716bf9.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a1b3060c05698700a76957ef49716bf9.png)'
- en: Entropy of a coin flip with varying probabilities of head and tail, measured
    in bits. Entropy peaks when the outcome of the coin flip is most uncertain [image
    from [Wikipedia](https://en.wikipedia.org/wiki/Entropy_(information_theory)#/media/File:Binary_entropy_plot.svg)]
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 硬币翻转的熵，随着头和尾的概率变化，以比特为单位测量。当硬币翻转的结果最不确定时，熵达到峰值。[图片来自[维基百科](https://en.wikipedia.org/wiki/Entropy_(information_theory)#/media/File:Binary_entropy_plot.svg)]
- en: 'Now let’s compute the entropy of a fair die:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们计算一个公平骰子的熵：
- en: Note that the die has a higher entropy (2.58 bit) than the fair coin (1 bit).
    Although both have uniform outcome probabilities, the die displays lower individual
    probabilities.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，骰子的熵（2.58 bit）高于公平硬币（1 bit）。虽然两者都有均匀的结果概率，但骰子的个别概率较低。
- en: Now, let’s consider the probabilities of a loaded die, e.g., [3/12, 1/12, 2/12,
    2/12, 2/12, 2/12]. The corresponding entropy is 2.52, reflecting that outcomes
    are slightly more predictable now (we are more likely to see 1 eye and less likely
    to see 2 eyes). Finally, let’s consider a more heavily loaded die with probabilities
    [7/12, 1/12, 1/12, 1/12, 1/12, 1/12]? Now, we get an entropy of 1.95\. The predictability
    of the outcomes has further increased, as evidenced by the decreased entropy.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，考虑一个加重骰子的概率，例如，[3/12, 1/12, 2/12, 2/12, 2/12, 2/12]。相应的熵为2.52，这反映了结果现在略微更可预测（我们更可能看到1点而不太可能看到2点）。最后，考虑一个更重的骰子，概率为[7/12,
    1/12, 1/12, 1/12, 1/12, 1/12]。此时，我们得到的熵为1.95。结果的可预测性进一步提高，从熵的减少可以看出。
- en: Armed with an understanding of entropy, let’s see how we can utilize it in Reinforcement
    Learning.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 了解熵的概念后，让我们看看如何在强化学习中利用它。
- en: Entropy-Regularized Reinforcement Learning
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 熵正则化强化学习
- en: We define entropy in the context of RL, in which action probabilities derive
    from a stochastic policy π(⋅|s).
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在强化学习的背景下定义熵，其中行动概率源自随机策略π(⋅|s)。
- en: '![](../Images/17a5ae3a81c3bfe4fa2258dcfdff47d7.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/17a5ae3a81c3bfe4fa2258dcfdff47d7.png)'
- en: Entropy bonus in reinforcement learning, computed by summing the product of
    all action probabilities and their log
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在强化学习中的熵奖励，通过对所有行动概率及其对数的乘积求和来计算
- en: 'We use the entropy of the policy as an **entropy bonus**, adding it to our
    reward function. Note that we do this for every time step, implying that present
    actions also position us to maximize future entropy:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将策略的熵用作**熵奖励**，将其添加到我们的奖励函数中。请注意，我们对每一个时间步骤都这样做，这意味着当前的行动也为最大化未来的熵做好了准备：
- en: '![](../Images/a324b2256c1dbd631125d52652a26565.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a324b2256c1dbd631125d52652a26565.png)'
- en: Entropy-regularized reinforcement learning. The entropy bonus — weighted by
    α — is added to the reward we seek to maximize
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 熵正则化强化学习。熵奖励——由α加权——被添加到我们寻求最大化的奖励中
- en: This might seem counterintuitive at first. After all, RL aims to optimize decision-making,
    which involves routinely picking good decisions over bad ones. Why would we alter
    our reward signals in a way that encourages to *maximize* entropy? This would
    be a good point to introduce the **principle of maximum entropy:**
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 这乍看起来可能有些违反直觉。毕竟，强化学习旨在优化决策过程，这涉及到常规地选择好的决策而非坏的决策。为什么我们要以鼓励*最大化*熵的方式来改变我们的奖励信号呢？这是引入**最大熵原则**的一个好时机：
- en: “the probability distribution ***[policy]*** which best represents the current
    state of knowledge about a system ***[environment]*** is the one with largest
    entropy, in the context of precisely stated prior data ***[observed rewards]***”
    — [Wikipedia](https://en.wikipedia.org/wiki/Principle_of_maximum_entropy)
  id: totrans-36
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “***[策略]***最能代表关于系统***[环境]***当前知识状态的概率分布，是在精确陈述的先验数据***[观察奖励]***的背景下具有最大熵的分布”
    — [维基百科](https://en.wikipedia.org/wiki/Principle_of_maximum_entropy)
- en: If the entropy of the policy is large (e.g., shortly after initialization),
    we don’t know all that much about the impact of different actions. The high-entropy
    policy reflects that we haven’t yet sufficiently explored the environment and
    need to observe more rewards from a variety of actions still.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 如果策略的熵很大（例如，在初始化后的短时间内），我们对不同动作的影响了解得不多。高熵策略反映了我们尚未充分探索环境，还需要从各种动作中观察更多的奖励。
- en: Of course, ultimately we do want to take good actions rather than endlessly
    explore, meaning we have to decide the emphasis we put on the entropy bonus. This
    is done through the **entropy regularization coefficient** α, which is a tunable
    parameter. For practical purposes, the weighted entropy bonus *αH(π(⋅|s)* can
    simply be viewed as a reward component that encourages exploration.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，*最终*我们确实希望采取好的行动，而不是无休止地探索，这意味着我们必须决定对熵奖励的重视程度。这是通过**熵正则化系数**α来实现的，这是一个可调参数。为了实际应用，权重熵奖励*αH(π(⋅|s)*可以简单地视为一种鼓励探索的奖励组件。
- en: Note that the entropy bonus is always computed over the full action space, so
    we add the same bonus when evaluating each action. In a typical stochastic policy
    approach, **action probabilities are proportional to their expected rewards, including
    the entropy bonus** (e.g., by applying a softmax function on them). Thus, if entropy
    is very large relative to the rewards, action probabilities are more or less equal.
    If entropy is very small, the rewards are leading in defining action probabilities.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，熵奖励总是计算在整个动作空间上，因此在评估每个动作时，我们都会添加相同的奖励。在典型的随机策略方法中，**动作概率与它们的期望奖励成正比，包括熵奖励**（例如，通过对它们应用softmax函数）。因此，如果熵相对于奖励非常大，则动作概率或多或少相等。如果熵非常小，则奖励在定义动作概率上占主导地位。
- en: Python implementation
  id: totrans-40
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Python实现
- en: Time to implement entropy-regularized reinforcement learning. For the purpose
    of this article we use a basic discrete policy gradient algorithm in a multi-armed
    bandit context, but it can easily be extended to more sophisticated environments
    and algorithms.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 现在开始实现熵正则化的强化学习。为了本文的目的，我们在多臂老虎机背景下使用基本的离散策略梯度算法，但它可以很容易地扩展到更复杂的环境和算法。
- en: Remember that policy gradient algorithms have a built-in exploration mechanism
    — entropy bonuses applied on inherently deterministic policies (e.g., Q-learning)
    have more pronounced effects.
  id: totrans-42
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 记住，策略梯度算法具有内置的探索机制——对固有确定性策略（例如，Q-learning）应用的熵奖励具有更明显的效果。
- en: Incorporating entropy regularization is quite straightforward. We simply add
    the entropy bonus to the rewards — so it will be incorporated in the loss function
    — and proceed as usual. Depending on algorithm and problem setting, you might
    encounter a number of variants in literature and codebases, but the core idea
    remains the same. The code snippet below (a TensorFlow implementation of discrete
    policy gradient) illustrates how it works.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 引入熵正则化是相当直接的。我们只需将熵奖励添加到奖励中——因此它会被纳入损失函数——然后按常规步骤进行。根据算法和问题设置，您可能会在文献和代码库中遇到许多变体，但核心思想保持不变。下面的代码片段（离散策略梯度的TensorFlow实现）演示了它是如何工作的。
- en: Partial code for entropy-regularized reinforcement learning, in this case extending
    the discrete policy gradient algorithm.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 部分熵正则化强化学习的代码，在这种情况下扩展了离散策略梯度算法。
- en: 'Consider a set of slot machines with mean payoffs *μ=[1.00,0.80,0.90,0.98]*
    and st. dev. *σ=0.01* for all machines. Clearly, the optimal policy is to play
    Machine #1 with probability 1.0\. However, without sufficient exploration, it
    is easy to mistake Machine #4 for the best machine.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一组老虎机，其平均奖励为*μ=[1.00,0.80,0.90,0.98]*，所有机器的标准差*σ=0.01*。显然，最佳策略是以概率1.0玩机器#1。然而，如果没有足够的探索，很容易将机器#4误认为是最佳机器。
- en: 'To illustrate the idea, let’s first see the algorithmic behavior without entropy
    bonus (α=0). We plot the probability of playing Machine #1\. Although each machine
    starts with an equal probability, the algorithm fairly quickly identifies that
    Machine #1 yields the highest expected reward and starts playing it with increased
    probability.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明这个想法，让我们首先看看没有熵奖励（α=0）的算法行为。我们绘制了玩机器#1的概率。尽管每台机器开始时的概率相等，但算法相当快地识别出机器#1提供了最高的期望奖励，并开始以更高的概率玩它。
- en: '![](../Images/221a756e901548799aa9e66e4542a5fe.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/221a756e901548799aa9e66e4542a5fe.png)'
- en: 'Here, the algorithm without entropy regularization converges to playing the
    optimal Machine #1\. Left: probability per episode of playing Machine #1\. Right:
    probabilities per machine after 10k episodes [image by author]'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，没有熵正则化的算法收敛于主要玩最佳的机器#1。左图：每次试验玩机器#1的概率。右图：10k次试验后每台机器的概率[作者图片]
- en: 'However, that could have gone very differently... Below we see a run with the
    same algorithm, only this time it erroneously converges to the suboptimal Machine
    #4.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这可能会有很大的不同……下图显示了使用相同算法的一个运行，只是这次它错误地收敛到次优的机器#4。
- en: '![](../Images/53a0aa195796d2e588c02c59ea15d1d5.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/53a0aa195796d2e588c02c59ea15d1d5.png)'
- en: 'In this case, the algorithm without entropy regularization converges to predominantly
    playing the suboptimal Machine #4\. Left: probability per episode of playing Machine
    #1\. Right: probabilities per machine after 10k episodes [image by author]'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，没有熵正则化的算法收敛于主要玩次优的机器#4。左图：每次试验玩机器#1的概率。右图：10k次试验后每台机器的概率[作者图片]
- en: '*Now, we set α=1*. This yields an entropy bonus that is large relative to the
    rewards. Although the probability of playing Machine #1 gradually increases, the
    regularization component continues to encourage strong exploration even after
    10k iterations.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '*现在，我们设置α=1*。这产生了相对于奖励而言较大的熵奖励。尽管玩机器#1的概率逐渐增加，但正则化组件继续鼓励强烈的探索，即使在10k次迭代后。'
- en: '![](../Images/2454757681a09e04e3db97422e0323c9.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2454757681a09e04e3db97422e0323c9.png)'
- en: 'With entropy regularization, we see that the algorithm still explores a lot
    after 10k episodes, although slowly recognizing that Machine #1 offers superior
    rewards. Left: probability per episode of playing Machine #1\. Right: probabilities
    per machine after 10k episodes [image by author]'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 通过熵正则化，我们看到即使在10k次试验后，算法仍然进行了大量探索，虽然逐渐认识到机器#1提供了更好的奖励。左图：每次试验玩机器#1的概率。右图：10k次试验后每台机器的概率[作者图片]
- en: Evidently, in practice we don’t know the true best solution, nor the amount
    of exploration that is desirable. Typically, you’ll encounter values in the neighborhood
    of *α=0.001*, but you can imagine the ideal balance between exploration and exploitation
    strongly depends on the problem. Thus, it often requires some **trial-and-error
    to find an appropriate entropy bonus**. The coefficient *α* may also be dynamic,
    either through a predetermined decay scheme or being a learned weight in itself.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，在实际应用中，我们不知道真正的最佳解决方案，也不知道所需的探索量。通常，你会遇到*α=0.001*附近的值，但你可以想象，探索与利用之间的理想平衡强烈依赖于问题。因此，通常需要一些**试验和错误以找到合适的熵奖励**。系数*α*也可能是动态的，可能通过预定的衰减方案或者作为一个学习权重本身。
- en: Applications in Reinforcement Learning
  id: totrans-56
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 强化学习中的应用
- en: 'The principle of entropy regularization can be applied to just about any RL
    algorithm. For instance, you may add entropy bonuses to Q-values and transform
    the results into action probabilities through a softmax layer (soft Q-learning).
    State-of-the-art algorithms such as Proximal Policy Optimization (PPO) or soft-actor
    critic (SAC) typically include an entropy bonus, which is empirically shown to
    often enhance performance. Specifically, it offers the following three benefits:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 熵正则化的原则可以应用于几乎任何RL算法。例如，你可以将熵奖励添加到Q值中，并通过softmax层将结果转换为动作概率（soft Q-learning）。最先进的算法，如近端策略优化（PPO）或软演员评论家（SAC），通常包括熵奖励，实证研究表明这通常能提升性能。具体来说，它提供了以下三种好处：
- en: I. Better solution quality
  id: totrans-58
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: I. 更好的解决方案质量
- en: As elaborated earlier, the entropy bonus encourages exploration. Particularly
    when dealing with **sparse rewards**, this is very helpful, as we rarely receives
    feedback on action and might erroneously repeat sub-optimal actions for which
    it happens to overestimate rewards.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 正如前面详细说明的，熵奖金鼓励探索。特别是在处理**稀疏奖励**时，这非常有用，因为我们很少收到关于动作的反馈，并且可能错误地重复那些过高估计奖励的次优动作。
- en: II. Better robustness
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: II. 更好的鲁棒性
- en: Given that the entropy bonus encourages to explore more, we will also encounter
    rare or deviating state-action pairs more often. Because we have a encountered
    a **richer and more diverse set of experiences**, we learn a policy that is better
    equipped to handle a variety of situations. This added robustness enhances the
    quality of the policy.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 由于熵奖金鼓励更多的探索，我们也会更频繁地遇到稀有或偏离的状态-动作对。因为我们遇到了**更丰富和更多样化的经验**，我们学习到的策略能够更好地处理各种情况。这种增加的鲁棒性提升了策略的质量。
- en: III. Facilitate transfer learning
  id: totrans-62
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: III. 促进迁移学习
- en: Increased exploration also helps to adapt a learned policy to new tasks and
    environments. The more diverse experiences allows to better adapt to the new situation,
    because we already learned from comparable circumstances. As such, entropy regularization
    is often useful for transfer learning, making it easy to **retrain or update learned
    policies** when dealing with changing environments.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 增加探索也有助于将学习到的策略适应于新任务和环境。更多的多样化经验能够更好地适应新情况，因为我们已经从类似的情境中学到了东西。因此，熵正则化在迁移学习中通常很有用，它使得在应对变化环境时**重新训练或更新学习策略**变得容易。
- en: TL;DR
  id: totrans-64
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: TL;DR
- en: An entropy bonus encourages exploration of the action space, aiming to avoid
    premature convergence
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 熵奖金鼓励对动作空间的探索，旨在避免过早收敛
- en: The balance between reward (exploitation) and bonus (exploration) is governed
    through a coefficient that requires fine-tuning
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 奖励（利用）和奖金（探索）之间的平衡是通过一个需要微调的系数来控制的。
- en: Entropy bonuses are commonly used in modern RL algorithms such as PPO and SAC
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 熵奖金在现代 RL 算法中如 PPO 和 SAC 中被广泛使用。
- en: Exploration enhances quality, robustness and adaptability to new instance variants.
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索提高了质量、鲁棒性和对新实例变体的适应能力。
- en: Entropy regularization is particularly useful when dealing with sparse rewards,
    when robustness is important, and/or when the policy should be applicable to related
    problem settings.
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 熵正则化在处理稀疏奖励时特别有用，当鲁棒性很重要和/或策略应该适用于相关问题设置时。
- en: '*If you are interested in entropy regularization in RL, you might want to check
    out the following articles as well:*'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '*如果你对 RL 中的熵正则化感兴趣，你可能还想查看以下文章：*'
- en: '[](/a-minimal-working-example-for-discrete-policy-gradients-in-tensorflow-2-0-d6a0d6b1a6d7?source=post_page-----2ba959c92aad--------------------------------)
    [## A Minimal Working Example for Discrete Policy Gradients in TensorFlow 2.0'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/a-minimal-working-example-for-discrete-policy-gradients-in-tensorflow-2-0-d6a0d6b1a6d7?source=post_page-----2ba959c92aad--------------------------------)
    [## TensorFlow 2.0 中离散策略梯度的最小工作示例'
- en: A multi-armed bandit example for training discrete actor networks. With the
    aid of the GradientTape functionality, the…
  id: totrans-72
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 用于训练离散演员网络的多臂老虎机示例。借助 GradientTape 功能，…
- en: towardsdatascience.com](/a-minimal-working-example-for-discrete-policy-gradients-in-tensorflow-2-0-d6a0d6b1a6d7?source=post_page-----2ba959c92aad--------------------------------)
    [](/policy-gradients-in-reinforcement-learning-explained-ecec7df94245?source=post_page-----2ba959c92aad--------------------------------)
    [## Policy Gradients In Reinforcement Learning Explained
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/a-minimal-working-example-for-discrete-policy-gradients-in-tensorflow-2-0-d6a0d6b1a6d7?source=post_page-----2ba959c92aad--------------------------------)
    [](/policy-gradients-in-reinforcement-learning-explained-ecec7df94245?source=post_page-----2ba959c92aad--------------------------------)
    [## 强化学习中的策略梯度解释
- en: 'Learn all about policy gradient algorithms based on likelihood ratios (REINFORCE):
    the intuition, the derivation, the…'
  id: totrans-74
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 了解基于似然比的策略梯度算法（REINFORCE）：直觉、推导、…
- en: towardsdatascience.com](/policy-gradients-in-reinforcement-learning-explained-ecec7df94245?source=post_page-----2ba959c92aad--------------------------------)
    [](/proximal-policy-optimization-ppo-explained-abed1952457b?source=post_page-----2ba959c92aad--------------------------------)
    [## Proximal Policy Optimization (PPO) Explained
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/policy-gradients-in-reinforcement-learning-explained-ecec7df94245?source=post_page-----2ba959c92aad--------------------------------)
    [](/proximal-policy-optimization-ppo-explained-abed1952457b?source=post_page-----2ba959c92aad--------------------------------)
    [## 近端策略优化（PPO）解释
- en: The journey from REINFORCE to the go-to algorithm in continuous control
  id: totrans-76
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 从REINFORCE到连续控制中的首选算法的历程
- en: towardsdatascience.com](/proximal-policy-optimization-ppo-explained-abed1952457b?source=post_page-----2ba959c92aad--------------------------------)
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '[towardsdatascience.com](https://towardsdatascience.com/proximal-policy-optimization-ppo-explained-abed1952457b?source=post_page-----2ba959c92aad--------------------------------)'
- en: Further reading
  id: totrans-78
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: Ahmed, Z., Le Roux, N., Norouzi, M., & Schuurmans, D. (2019). [Understanding
    the impact of entropy on policy optimization.](https://proceedings.mlr.press/v97/ahmed19a.html)
    International Conference on Machine Learning.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: Ahmed, Z., Le Roux, N., Norouzi, M., & Schuurmans, D. (2019). [理解熵对策略优化的影响](https://proceedings.mlr.press/v97/ahmed19a.html)
    国际机器学习会议。
- en: Eysenbach, B. & Levine, S. (2022). [Maximum Entropy RL (Provably) Solves Some
    Robust RL Problems.](https://arxiv.org/pdf/2103.06257.pdf) International Conference
    on Learning Representations.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: Eysenbach, B. & Levine, S. (2022). [最大熵强化学习（可证明地）解决一些鲁棒强化学习问题](https://arxiv.org/pdf/2103.06257.pdf)
    国际学习表征会议。
- en: Haarnoja, T., Tang, H., Abbeel, P., & Levine, S. (2017). [Reinforcement learning
    with deep energy-based policies.](https://proceedings.mlr.press/v70/haarnoja17a/haarnoja17a.pdf)
    International Conference on Machine Learning.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: Haarnoja, T., Tang, H., Abbeel, P., & Levine, S. (2017). [使用深度能量基策略的强化学习](https://proceedings.mlr.press/v70/haarnoja17a/haarnoja17a.pdf)
    国际机器学习会议。
- en: Reddy, A. (2021). [How Maximum Entropy makes Reinforcement Learning Robust.](https://mlberkeley.substack.com/p/max-ent)
    Machine Learning at Berkeley.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: Reddy, A. (2021). [最大熵如何使强化学习更鲁棒](https://mlberkeley.substack.com/p/max-ent)
    伯克利机器学习。
- en: Schulman, J., Chen, X., & Abbeel, P. (2017). [Equivalence between policy gradients
    and soft q-learning.](https://arxiv.org/abs/1704.06440) *arXiv preprint arXiv:1704.06440*.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: Schulman, J., Chen, X., & Abbeel, P. (2017). [策略梯度与软Q学习的等价性](https://arxiv.org/abs/1704.06440)
    *arXiv预印本arXiv:1704.06440*。
- en: Tang, H. & Haarnoja, T. (2017). [Learning Diverse Skills via Maximum Entropy
    Deep Reinforcement Learning.](https://bair.berkeley.edu/blog/2017/10/06/soft-q-learning/)
    BAIR, Berkeley.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: Tang, H. & Haarnoja, T. (2017). [通过最大熵深度强化学习学习多样化技能](https://bair.berkeley.edu/blog/2017/10/06/soft-q-learning/)
    伯克利人工智能研究所。
- en: Yu, H., Zhang, H., & Xu, W. (2022). [Do You Need the Entropy Reward (in Practice)?](https://arxiv.org/abs/2201.12434)
    *arXiv preprint arXiv:2201.12434*.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: Yu, H., Zhang, H., & Xu, W. (2022). [你实际需要熵奖励吗？](https://arxiv.org/abs/2201.12434)
    *arXiv预印本arXiv:2201.12434*。
