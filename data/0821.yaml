- en: 'From Decision Trees to Transformers: Comparing Sentiment Analysis Models for
    Macedonian Restaurant Reviews'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从决策树到变换器：比较马其顿餐厅评论的情感分析模型
- en: 原文：[https://towardsdatascience.com/from-decision-trees-to-transformers-comparing-sentiment-analysis-models-for-macedonian-restaurant-4c2d931ec021?source=collection_archive---------4-----------------------#2023-03-03](https://towardsdatascience.com/from-decision-trees-to-transformers-comparing-sentiment-analysis-models-for-macedonian-restaurant-4c2d931ec021?source=collection_archive---------4-----------------------#2023-03-03)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/from-decision-trees-to-transformers-comparing-sentiment-analysis-models-for-macedonian-restaurant-4c2d931ec021?source=collection_archive---------4-----------------------#2023-03-03](https://towardsdatascience.com/from-decision-trees-to-transformers-comparing-sentiment-analysis-models-for-macedonian-restaurant-4c2d931ec021?source=collection_archive---------4-----------------------#2023-03-03)
- en: ML Techniques for Analysing Macedonian Restaurant Reviews
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分析马其顿餐厅评论的机器学习技术
- en: '[](https://medium.com/@danilo.najkov?source=post_page-----4c2d931ec021--------------------------------)[![Danilo
    Najkov](../Images/e0e8976f1f9f78ae58ba7efcc90a4f00.png)](https://medium.com/@danilo.najkov?source=post_page-----4c2d931ec021--------------------------------)[](https://towardsdatascience.com/?source=post_page-----4c2d931ec021--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----4c2d931ec021--------------------------------)
    [Danilo Najkov](https://medium.com/@danilo.najkov?source=post_page-----4c2d931ec021--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@danilo.najkov?source=post_page-----4c2d931ec021--------------------------------)[![Danilo
    Najkov](../Images/e0e8976f1f9f78ae58ba7efcc90a4f00.png)](https://medium.com/@danilo.najkov?source=post_page-----4c2d931ec021--------------------------------)[](https://towardsdatascience.com/?source=post_page-----4c2d931ec021--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----4c2d931ec021--------------------------------)
    [Danilo Najkov](https://medium.com/@danilo.najkov?source=post_page-----4c2d931ec021--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F19802d0e7d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffrom-decision-trees-to-transformers-comparing-sentiment-analysis-models-for-macedonian-restaurant-4c2d931ec021&user=Danilo+Najkov&userId=19802d0e7d&source=post_page-19802d0e7d----4c2d931ec021---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----4c2d931ec021--------------------------------)
    ·10 min read·Mar 3, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F4c2d931ec021&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffrom-decision-trees-to-transformers-comparing-sentiment-analysis-models-for-macedonian-restaurant-4c2d931ec021&user=Danilo+Najkov&userId=19802d0e7d&source=-----4c2d931ec021---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F19802d0e7d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffrom-decision-trees-to-transformers-comparing-sentiment-analysis-models-for-macedonian-restaurant-4c2d931ec021&user=Danilo+Najkov&userId=19802d0e7d&source=post_page-19802d0e7d----4c2d931ec021---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----4c2d931ec021--------------------------------)
    ·10分钟阅读·2023年3月3日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F4c2d931ec021&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffrom-decision-trees-to-transformers-comparing-sentiment-analysis-models-for-macedonian-restaurant-4c2d931ec021&user=Danilo+Najkov&userId=19802d0e7d&source=-----4c2d931ec021---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4c2d931ec021&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffrom-decision-trees-to-transformers-comparing-sentiment-analysis-models-for-macedonian-restaurant-4c2d931ec021&source=-----4c2d931ec021---------------------bookmark_footer-----------)![](../Images/af0e8f1d709eb059137751a9703ae73d.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4c2d931ec021&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffrom-decision-trees-to-transformers-comparing-sentiment-analysis-models-for-macedonian-restaurant-4c2d931ec021&source=-----4c2d931ec021---------------------bookmark_footer-----------)![](../Images/af0e8f1d709eb059137751a9703ae73d.png)'
- en: Graphic by author
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图形
- en: While machine learning models for natural language processing have traditionally
    focused on popular languages such as English and Spanish, less commonly spoken
    languages have seen much less development. However, with the recent rise in e-commerce
    due to the COVID-19 pandemic, even less commonly spoken languages like Macedonian
    are generating large amounts of data through online reviews. This has opened an
    opportunity to develop and train machine learning models for sentiment analysis
    on Macedonian restaurant reviews, which can help businesses better understand
    customer sentiment and improve their services. In this study, we tackle the challenges
    that arise from this problem and explore and compare various sentiment analysis
    models for analyzing sentiment in Macedonian restaurant reviews, ranging from
    the classical random forests to modern deep learning techniques and transformers.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '-   尽管自然语言处理的机器学习模型传统上集中于英语和西班牙语等流行语言，但较少使用的语言的发展则相对较少。然而，由于COVID-19大流行导致电子商务的兴起，即使是像马其顿语这样较少使用的语言也通过在线评论产生了大量数据。这为开发和训练马其顿语餐馆评论的情感分析机器学习模型提供了机会，这可以帮助企业更好地理解客户情感并改善服务。在这项研究中，我们解决了这一问题中出现的挑战，并探讨和比较了用于分析马其顿餐馆评论情感的各种情感分析模型，从经典的随机森林到现代的深度学习技术和变压器。'
- en: '**Contents**'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '**内容**'
- en: Challenges and preprocessing the data
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 挑战与数据预处理
- en: Creating vector embeddings
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建向量嵌入
- en: '- LASER embeddings'
  id: totrans-13
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '- LASER 嵌入'
- en: '- Multilingual universal text encoder'
  id: totrans-14
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '- 多语言通用文本编码器'
- en: '- OpenAI Ada v2'
  id: totrans-15
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '- OpenAI Ada v2'
- en: Machine learning models
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器学习模型
- en: '- Random forest'
  id: totrans-17
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '- 随机森林'
- en: '- XGBoost'
  id: totrans-18
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '- XGBoost'
- en: '- Support vector machines'
  id: totrans-19
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '- 支持向量机'
- en: '- Deep learning'
  id: totrans-20
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '- 深度学习'
- en: '- Transformers'
  id: totrans-21
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '- 变压器'
- en: Results and Discussion
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 结果与讨论
- en: Future work
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 未来工作
- en: Conclusion
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 结论
- en: Preprocessing the data
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据预处理
- en: Language is a uniquely human communication tool, and computers cannot interpret
    it without appropriate processing techniques. To allow machines to analyse and
    understand language, we need to represent the complex semantic and lexical information
    in a way that can be processed computationally. One popular method for achieving
    this is through the use of vector representations. In recent years, additional
    to language specific representation models, multilingual models emerged. These
    models can capture the semantic context of text on a large number of languages.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '-   语言是独特的人类交流工具，计算机在没有适当处理技术的情况下无法解释它。为了让机器分析和理解语言，我们需要以计算机可处理的方式表示复杂的语义和词汇信息。实现这一目标的一种流行方法是使用向量表示。近年来，除了特定语言的表示模型外，多语言模型也出现了。这些模型可以捕捉大量语言的文本语义上下文。'
- en: However, for languages with Cyrillic script, an additional challenge arises
    as users on the internet often express themselves using Latin script, resulting
    in mixed data consisting of both Latin and Cyrillic text. To address this challenge,
    I used a dataset from a local restaurant of approximately 500 reviews, containing
    both Latin and Cyrillic script. The dataset also includes a small subset of english
    reviews, which will help to assess the performance on mixed data. Additionally,
    online texts can contain symbols such as emojis that need to be removed. Therefore
    preprocessing is a crucial step before any text embedding can be performed.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '-   然而，对于使用西里尔字母的语言，额外的挑战是由于互联网用户经常使用拉丁字母表达自己，导致数据中混合了拉丁和西里尔文本。为了解决这一挑战，我使用了来自本地餐馆的大约500条评论的数据集，其中包含拉丁和西里尔字母。该数据集还包括一小部分英文评论，有助于评估在混合数据上的表现。此外，在线文本可能包含如表情符号等需要去除的符号。因此，数据预处理是进行任何文本嵌入之前的关键步骤。'
- en: '[PRE0]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The dataset contains positive and negative classes with nearly equal distribution.
    For removing emojis, I used the python library `emoji`, which can easily remove
    emojis and other symbols.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '-   数据集包含正类和负类，分布几乎相等。为了去除表情符号，我使用了Python库`emoji`，它可以轻松去除表情符号和其他符号。'
- en: '[PRE1]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: For the problem of Cyrillic and Latin text, i converted all texts into one or
    the other, so the machine learning models can be tested on both to compare the
    performance. I used the “cyrtranslit” library for this task. It supports most
    of the Cyrillic alphabets like Macedonian, Bulgarian, Ukrainian and others.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '-   对于西里尔和拉丁文本的问题，我将所有文本转换成其中一种，以便在两者上测试机器学习模型，以比较其性能。我使用了“cyrtranslit”库来完成这一任务。它支持大多数西里尔字母，如马其顿语、保加利亚语、乌克兰语等。'
- en: '[PRE2]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '![](../Images/df4ff53d87f8ed5066a9fc78eb489af6.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/df4ff53d87f8ed5066a9fc78eb489af6.png)'
- en: Figure 1\. Output from conversion
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 图1\. 转换后的输出
- en: For embedding models that I used, it’s generally not necessary to remove punctuation,
    stop-words and do other text cleaning. These models are designed to process natural
    language text, including punctuation marks, and are often able to capture the
    meaning of sentences more accurately when they are left intact. With that the
    preprocesing of the text is finished.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我使用的嵌入模型，通常不需要移除标点符号、停用词及其他文本清理。这些模型设计用来处理自然语言文本，包括标点符号，并且当文本保持完整时，通常能够更准确地捕捉句子的含义。这样文本的预处理就完成了。
- en: Vector embeddings
  id: totrans-36
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 向量嵌入
- en: Currently, there are no large-scale Macedonian representation models available.
    However, we can use multilingual models trained on Macedonian text. There are
    several such models available, but for this task, I have found that LASER and
    Multilingual Universal Sentence Encoder would be the most suitable options.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，没有大规模的马其顿语表示模型可用。然而，我们可以使用在马其顿语文本上训练的多语言模型。虽然有几个这样的模型可用，但对于这项任务，我发现LASER和Multilingual
    Universal Sentence Encoder是最合适的选择。
- en: LASER
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: LASER
- en: LASER (Language-Agnostic SEntence Representations) is a language-agnostic approach
    for generating high-quality multilingual sentence embeddings. The LASER model
    is based on a two-stage process, where the first stage is preprocessing the text,
    including tokenization, lowercasing, and applying sentencepiece. This part is
    language specific. The second stage involves mapping the preprocessed input text
    to a fixed-length embedding using a multi-layer bidirectional LSTM.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: LASER（语言无关句子表示）是一种生成高质量多语言句子嵌入的语言无关方法。LASER模型基于一个两阶段过程，其中第一阶段是预处理文本，包括分词、小写转换和应用sentencepiece。这部分是特定语言的。第二阶段涉及将预处理后的输入文本映射到固定长度的嵌入，使用的是多层双向LSTM。
- en: LASER has been shown to outperform other popular sentence embedding methods,
    such as fastText and InferSent, on a range of benchmark datasets. Additionally,
    the LASER model is open-source and freely available, making it easily accessible
    to everyone.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: LASER已被证明在一系列基准数据集上优于其他流行的句子嵌入方法，如fastText和InferSent。此外，LASER模型是开源的且免费提供，方便所有人使用。
- en: 'Creating the embeddings with LASER is a straightforward process:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 使用LASER创建嵌入是一个简单的过程：
- en: '[PRE3]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Multilingual Universal Sentence Encoder
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 多语言通用句子编码器
- en: Multilingual Universal Sentence Encoder (MUSE) is a pre-trained model for generating
    sentence embeddings, developed by Facebook. MUSE is designed to encode sentences
    in multiple languages into a common space.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 多语言通用句子编码器（MUSE）是一个预训练模型，用于生成句子嵌入，由Facebook开发。MUSE旨在将多种语言的句子编码到一个共同的空间中。
- en: The model is based on a deep neural network that uses an encoder-decoder architecture
    to learn a mapping between a sentence and its corresponding embedding vector in
    a high-dimensional space. MUSE is trained on a large-scale multilingual corpus,
    which includes texts from Wikipedia, news articles, and web pages.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型基于一个深度神经网络，使用编码器-解码器架构来学习句子与其对应的高维空间嵌入向量之间的映射。MUSE在一个大规模的多语言语料库上进行训练，该语料库包括维基百科、新闻文章和网页文本。
- en: '[PRE4]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: OpenAI Ada v2
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: OpenAI Ada v2
- en: Towards the end of 2022, OpenAI announced their brand new state-of-the-art embedding
    model [text-embedding-ada-002](https://openai.com/blog/new-and-improved-embedding-model/).
    As this model is built on GPT-3, it has multilingual processing capabilities.
    To compare the results between the Cyrillic and Latin reviews, I ran the model
    on both datasets
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在2022年底，OpenAI宣布了他们全新的最先进嵌入模型 [text-embedding-ada-002](https://openai.com/blog/new-and-improved-embedding-model/)。由于该模型建立在GPT-3之上，它具有多语言处理能力。为了比较斯拉夫字母和拉丁字母评论之间的结果，我在这两个数据集上运行了该模型。
- en: '[PRE5]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Machine learning models
  id: totrans-50
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习模型
- en: This section explores the various machine learning models utilized to predict
    sentiment in Macedonian restaurant reviews. From traditional machine learning
    models to deep learning techniques, we’ll look into the strengths and weaknesses
    of each model and compare their performance on the dataset.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 本节探讨了用于预测马其顿餐馆评论情感的各种机器学习模型。从传统机器学习模型到深度学习技术，我们将深入研究每种模型的优缺点，并比较它们在数据集上的表现。
- en: Before running any models, the data should be split for training and testing
    for every embedding type. This can be easily done with with the `sklearn` library.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在运行任何模型之前，应将数据按每种嵌入类型分为训练集和测试集。这可以通过`sklearn`库轻松完成。
- en: '[PRE6]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Random Forests
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 随机森林
- en: '![](../Images/f3c3e1b81c5fa1eb8e8f42e7fb391761.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f3c3e1b81c5fa1eb8e8f42e7fb391761.png)'
- en: Figure 2\. Simplified representation of the random forest classification. 100
    decision trees are constructed and the result is calculated as a majority vote
    between the results of each decision tree individually. (figure by author)
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 图2\. 随机森林分类的简化表示。构建了100棵决策树，结果通过对每棵决策树的结果进行多数投票来计算。（作者绘制）
- en: Random Forests are a widely-used machine learning algorithm that uses an ensemble
    of decision trees to classify data points. The algorithm works by training each
    decision tree on a subset of the full dataset and a random subset of the features.
    During inference, each decision tree generates a prediction of the sentiment,
    and the final output is obtained by taking a majority vote of all the trees. This
    approach helps to prevent overfitting and can lead to more robust and accurate
    predictions.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 随机森林是一种广泛使用的机器学习算法，它使用决策树的集成来分类数据点。该算法通过在完整数据集的子集和特征的随机子集上训练每棵决策树来工作。在推断过程中，每棵决策树生成情感的预测，最终输出通过对所有树的多数投票来获得。这种方法有助于防止过拟合，并能导致更稳健和准确的预测。
- en: '[PRE7]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: XGBoost
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: XGBoost
- en: '![](../Images/069088713baa3757823045c72cc6597c.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/069088713baa3757823045c72cc6597c.png)'
- en: Figure 3\. Sequential process of boosting based algorithms. Every next decision
    tree is trained on the residuals (errors) of the previous one. (figure by author)
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 图3\. 提升基算法的顺序过程。每棵后续决策树在前一棵树的残差（错误）上进行训练。（作者绘制）
- en: XGBoost (eXtreme Gradient Boosting) is a powerful ensemble method, mainly used
    in tabular data. Like Random Forest, XGBoost also uses decision trees to classify
    data points, but with a different approach. Instead of training all trees at once,
    XGBoost trains each tree in a sequential manner, learning from the errors made
    by the previous tree. This process is called boosting, which means combining weak
    models to form a stronger one. Although XGBoost primarly produces great results
    with tabular data, it would be interesting to test it with vector embeddings as
    well.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: XGBoost（极端梯度提升）是一种强大的集成方法，主要用于表格数据。与随机森林一样，XGBoost也使用决策树来分类数据点，但采用不同的方法。XGBoost不是一次训练所有树，而是以顺序方式训练每棵树，从前一棵树的错误中学习。这一过程称为提升，即将弱模型组合成一个更强的模型。尽管XGBoost主要在表格数据上产生出色的结果，但用向量嵌入测试它也很有趣。
- en: '[PRE8]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Support Vector Machines
  id: totrans-64
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 支持向量机
- en: '![](../Images/a7582c92959dbf18690e24b78a04b7ac.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a7582c92959dbf18690e24b78a04b7ac.png)'
- en: Figure 4\. Simplified representation of support vector classification. In the
    case of this sentiment analysis with 1024 input features, the hyperplane would
    be 1023 dimensional. (figure by author)
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 图4\. 支持向量分类的简化表示。在具有1024个输入特征的情感分析中，超平面将是1023维的。（作者绘制）
- en: Support Vector Machines (SVM) is a popular and powerful machine learning algorithm
    for classification and regression tasks. It works by finding the optimal hyperplane
    that separates the data into different classes, while maximizing the margin between
    the classes. SVM is particularly useful for high-dimensional data and can handle
    non-linear boundaries using kernel functions.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 支持向量机（SVM）是一种流行且强大的机器学习算法，用于分类和回归任务。它通过寻找将数据分成不同类别的最佳超平面，同时最大化类别之间的间隔来工作。SVM特别适用于高维数据，并可以使用核函数处理非线性边界。
- en: '[PRE9]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Deep Learning
  id: totrans-69
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 深度学习
- en: '![](../Images/2357db5eea9f7f39d928d7476ad3857d.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2357db5eea9f7f39d928d7476ad3857d.png)'
- en: Figure 5\. Simplified representation of the neural network used in this problem.
    (figure by author)
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 图5\. 该问题中使用的神经网络的简化表示。（作者绘制）
- en: Deep Learning is an advanced machine learning method that utilizes artificial
    neural networks consisting of multiple layers and neurons. Deep learning networks
    demonstrate great performance with text and image data. Implementing these networks
    is a straightforward process using the library Keras.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习是一种先进的机器学习方法，利用由多层神经元组成的人工神经网络。深度学习网络在文本和图像数据上表现出色。使用库Keras实现这些网络是一个简单的过程。
- en: '[PRE10]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Here, a neural network with two hidden layers and a rectified linear unit (ReLU)
    activation function was used. The output layer contains a single neuron with a
    sigmoid activation function, enabling the network to make binary predictions for
    positive or negative sentiment. The binary cross-entropy loss function is paired
    with the sigmoid activation to train the model. Additionally, Dropout was used
    to help prevent overfitting and improve the generalization of the model. I tested
    with various different hyper-parameters and found that this configuration works
    best for this problem.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，使用了具有两个隐藏层和一个整流线性单元（ReLU）激活函数的神经网络。输出层包含一个带有 sigmoid 激活函数的单一神经元，使得网络能够对正面或负面情感进行二分类预测。二元交叉熵损失函数与
    sigmoid 激活函数配对，用于训练模型。此外，还使用了 Dropout 来帮助防止过拟合，并提高模型的泛化能力。我测试了各种不同的超参数，发现这种配置最适合这个问题。
- en: With the following function we can visualize the training of the models.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 使用以下函数我们可以可视化模型的训练过程。
- en: '[PRE11]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '![](../Images/8848ddc6401ccfdb6b39638a2638dfc6.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8848ddc6401ccfdb6b39638a2638dfc6.png)'
- en: Figure 6\. Example training output
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6\. 示例训练输出
- en: Transformers
  id: totrans-79
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 变换器
- en: '![](../Images/4f481ee30e9e5081d647526efa18634f.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4f481ee30e9e5081d647526efa18634f.png)'
- en: 'Figure 7\. Pre-training and fine-tuning process of the BERT large language
    model. (source: [original BERT paper](https://arxiv.org/pdf/1810.04805v2.pdf))'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7\. BERT 大型语言模型的预训练和微调过程。（来源：[原始 BERT 论文](https://arxiv.org/pdf/1810.04805v2.pdf)）
- en: Fine-tuning transformers is a popular technique in natural language processing
    that involves adjusting pre-trained transformer models to suit specific tasks.
    Transformers, such as BERT, GPT-2, and RoBERTa, are pre-trained on large amounts
    of text data and are capable of learning complex patterns and relationships in
    language. However, in order to perform well on specific tasks, such as sentiment
    analysis or text classification, these models need to be fine-tuned on task-specific
    data.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 微调变换器是自然语言处理中的一种流行技术，涉及调整预训练的变换器模型以适应特定任务。变换器，如 BERT、GPT-2 和 RoBERTa，经过大量文本数据的预训练，能够学习语言中的复杂模式和关系。然而，为了在特定任务上表现良好，如情感分析或文本分类，这些模型需要在任务特定的数据上进行微调。
- en: For these types of models, the vector representations we created earlier are
    not needed, as they directly process the tokens (extracted directly from the text).
    For this task of sentiment analysis in Macedonian I worked with`bert-base-multilingual-uncased`
    which is the multilingual version of the BERT model.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这些类型的模型，我们之前创建的向量表示是不需要的，因为它们直接处理从文本中提取的标记。对于马其顿的情感分析任务，我使用了 `bert-base-multilingual-uncased`，这是
    BERT 模型的多语言版本。
- en: '[HuggingFace](https://huggingface.co/) has made fine-tuning the transformers
    a very simple task. Firstly the data needs to be loaded into a transformers dataset.
    Then the text is tokenized and finally the model is trained.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '[HuggingFace](https://huggingface.co/) 使得微调变换器变得非常简单。首先需要将数据加载到变换器数据集中。然后对文本进行标记化，最后训练模型。'
- en: '[PRE12]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: With that we have successfully fine-tuned BERT for sentiment analysis.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经成功地对 BERT 进行了情感分析的微调。
- en: Results and discussion
  id: totrans-87
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结果与讨论
- en: '![](../Images/9db813559915c7d0a87f0d2eadf8bfcf.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9db813559915c7d0a87f0d2eadf8bfcf.png)'
- en: Figure 8\. Results obtained with all the models
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8\. 所有模型的结果
- en: The results of the sentiment analysis on Macedonian restaurant reviews are promising,
    with several models achieving high accuracy and F1 scores. The experiments show
    that deep learning models and transformers, outperform traditional machine learning
    models like Random Forests and Support Vector Machines, although not by much.
    Transformers and deep neural networks using the new OpenAI embedding managed to
    break the 0.9 accuracy barrier.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 对马其顿餐馆评论的情感分析结果令人鼓舞，多个模型达到了高精度和 F1 分数。实验表明，深度学习模型和变换器在表现上优于传统的机器学习模型，如随机森林和支持向量机，尽管差距不大。使用新的
    OpenAI 嵌入的变换器和深度神经网络成功突破了 0.9 的准确率障碍。
- en: The OpenAI embedding model `textembedding-ada-002` managed to considerably boost
    the results obtained even from the classical ML models, especially on the Support
    Vector Machines. The best result in this study was achieved with this embedding
    on the Cyrillic text on a deep learning model.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI 嵌入模型 `textembedding-ada-002` 大幅提升了即使是经典机器学习模型的结果，尤其是在支持向量机上。在这项研究中，使用该嵌入在深度学习模型上对西里尔文本取得了最佳结果。
- en: Generally the Latin texts performed worse compared to the Cyrillic texts. Although
    I initially hypothesized that the performance of these models would be better,
    given the prevalence of similar words in Latin among other Slavic languages and
    the fact that the embedding models were trained on such data, the findings did
    not support this hypothesis.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 总的来说，拉丁文本的表现比西里尔文本差。虽然我最初假设这些模型的表现会更好，考虑到拉丁语中类似词汇的普遍性以及嵌入模型是基于这种数据训练的，但研究结果并未支持这一假设。
- en: Future work
  id: totrans-93
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 未来工作
- en: In future work, it would be valuable to collect more data to further train and
    test the models, especially with a larger diversity of review topics and sources.
    Additionally, trying to incorporate more features such as metadata (e.g., reviewer’s
    age, gender, location) or temporal information (e.g., time of review) into the
    models might improve their accuracy. Finally, it would be interesting to extend
    the analysis to other less commonly spoken languages and compare the performance
    of the models with these trained on the Macedonian reviews.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在未来的工作中，收集更多数据以进一步训练和测试模型将非常有价值，特别是涵盖更多样化的评论主题和来源。此外，尝试将更多特征（如元数据（例如，评论者的年龄、性别、位置）或时间信息（例如，评论时间））纳入模型中，可能会提高其准确性。最后，将分析扩展到其他不那么常见的语言，并比较这些模型与马其顿评论训练的模型的表现，将是很有趣的。
- en: Conclusion
  id: totrans-95
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: In conclusion, this post has demonstrated the effectiveness of various machine
    learning models and embedding techniques for sentiment analysis of Macedonian
    restaurant reviews. Several classic machine learning models are explored and compared,
    such as Random Forests and SVM, as well as modern deep learning techniques including
    neural networks and transformers. The results have shown that fine-tuned transformer
    models and deep learning models with the newest OpenAI embedding outperform other
    methods, with a validation accuracy of up to 90%.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，本文展示了多种机器学习模型和嵌入技术在马其顿餐馆评论情感分析中的有效性。探讨并比较了几种经典的机器学习模型，如随机森林和支持向量机，以及现代深度学习技术，包括神经网络和变压器。结果表明，经过微调的变压器模型和使用最新OpenAI嵌入的深度学习模型表现优于其他方法，验证准确率高达90%。
- en: Thanks for reading!
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 感谢阅读！
