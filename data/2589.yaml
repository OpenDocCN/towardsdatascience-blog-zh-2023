- en: 'Graph Convolutional Networks: Introduction to GNNs'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 图卷积网络：GNNs简介
- en: 原文：[https://towardsdatascience.com/graph-convolutional-networks-introduction-to-gnns-24b3f60d6c95?source=collection_archive---------0-----------------------#2023-08-14](https://towardsdatascience.com/graph-convolutional-networks-introduction-to-gnns-24b3f60d6c95?source=collection_archive---------0-----------------------#2023-08-14)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/graph-convolutional-networks-introduction-to-gnns-24b3f60d6c95?source=collection_archive---------0-----------------------#2023-08-14](https://towardsdatascience.com/graph-convolutional-networks-introduction-to-gnns-24b3f60d6c95?source=collection_archive---------0-----------------------#2023-08-14)
- en: A step-by-step guide using PyTorch Geometric
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用PyTorch Geometric的逐步指南
- en: '[](https://medium.com/@mlabonne?source=post_page-----24b3f60d6c95--------------------------------)[![Maxime
    Labonne](../Images/a7efdd305e3cc77d5509bbb1076d57d8.png)](https://medium.com/@mlabonne?source=post_page-----24b3f60d6c95--------------------------------)[](https://towardsdatascience.com/?source=post_page-----24b3f60d6c95--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----24b3f60d6c95--------------------------------)
    [Maxime Labonne](https://medium.com/@mlabonne?source=post_page-----24b3f60d6c95--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@mlabonne?source=post_page-----24b3f60d6c95--------------------------------)[![Maxime
    Labonne](../Images/a7efdd305e3cc77d5509bbb1076d57d8.png)](https://medium.com/@mlabonne?source=post_page-----24b3f60d6c95--------------------------------)[](https://towardsdatascience.com/?source=post_page-----24b3f60d6c95--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----24b3f60d6c95--------------------------------)
    [Maxime Labonne](https://medium.com/@mlabonne?source=post_page-----24b3f60d6c95--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fdc89da634938&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgraph-convolutional-networks-introduction-to-gnns-24b3f60d6c95&user=Maxime+Labonne&userId=dc89da634938&source=post_page-dc89da634938----24b3f60d6c95---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----24b3f60d6c95--------------------------------)
    ·16 min read·Aug 14, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F24b3f60d6c95&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgraph-convolutional-networks-introduction-to-gnns-24b3f60d6c95&user=Maxime+Labonne&userId=dc89da634938&source=-----24b3f60d6c95---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fdc89da634938&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgraph-convolutional-networks-introduction-to-gnns-24b3f60d6c95&user=Maxime+Labonne&userId=dc89da634938&source=post_page-dc89da634938----24b3f60d6c95---------------------post_header-----------)
    发表在[Towards Data Science](https://towardsdatascience.com/?source=post_page-----24b3f60d6c95--------------------------------)
    ·16分钟阅读·2023年8月14日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F24b3f60d6c95&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgraph-convolutional-networks-introduction-to-gnns-24b3f60d6c95&user=Maxime+Labonne&userId=dc89da634938&source=-----24b3f60d6c95---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F24b3f60d6c95&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgraph-convolutional-networks-introduction-to-gnns-24b3f60d6c95&source=-----24b3f60d6c95---------------------bookmark_footer-----------)![](../Images/53a44290154e9eb7c20b5a32cd4d5642.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F24b3f60d6c95&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgraph-convolutional-networks-introduction-to-gnns-24b3f60d6c95&source=-----24b3f60d6c95---------------------bookmark_footer-----------)![](../Images/53a44290154e9eb7c20b5a32cd4d5642.png)'
- en: Image by author
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图像
- en: '**Graph Neural Networks** (GNNs) represent one of the most captivating and
    rapidly evolving architectures within the deep learning landscape. As deep learning
    models designed to process data structured as graphs, GNNs bring remarkable versatility
    and powerful learning capabilities.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '**图神经网络**（GNNs）是深度学习领域中最吸引人和迅速发展的架构之一。作为处理图结构数据的深度学习模型，GNNs带来了显著的多样性和强大的学习能力。'
- en: Among the various types of GNNs, the **Graph Convolutional Networks** (GCNs)
    have emerged as the most [prevalent and broadly applied model](https://paperswithcode.com/methods/category/graph-models).
    GCNs are innovative due to their ability to leverage both the features of a node
    and its locality to make predictions, providing an effective way to handle graph-structured
    data.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在各种类型的GNN中，**图卷积网络**（GCNs）已经成为最[普遍且广泛应用的模型](https://paperswithcode.com/methods/category/graph-models)。GCNs因其能够利用节点的特征及其局部信息进行预测而具有创新性，提供了一种有效处理图结构数据的方法。
- en: In this article, we will delve into the mechanics of the GCN layer and explain
    its inner workings. Furthermore, we will explore its practical application for
    node classification tasks, using [PyTorch Geometric](https://pytorch-geometric.readthedocs.io/en/latest/index.html)
    as our tool of choice.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们将深入探讨GCN层的机制，并解释其内部工作原理。此外，我们还将探索其在节点分类任务中的实际应用，使用[PyTorch Geometric](https://pytorch-geometric.readthedocs.io/en/latest/index.html)作为我们的工具。
- en: PyTorch Geometric is a specialized extension of PyTorch that has been created
    specifically for the development and implementation of GNNs. It is an advanced,
    yet user-friendly library that provides a comprehensive suite of tools to facilitate
    graph-based machine learning. To commence our journey, the PyTorch Geometric installation
    will be required. If you are using Google Colab, [PyTorch](https://pytorch.org/get-started/locally/)
    should already be in place, so all we need to do is execute a few additional commands.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch Geometric是PyTorch的一个专门扩展，专为GNNs的开发和实现而创建。它是一个高级但用户友好的库，提供了一整套工具来促进基于图的机器学习。为了开始我们的旅程，我们需要安装PyTorch
    Geometric。如果你使用Google Colab，[PyTorch](https://pytorch.org/get-started/locally/)应该已经安装好了，因此我们只需要执行几个额外的命令。
- en: All the code is available on [Google Colab](https://colab.research.google.com/drive/1ZugveUjRrbSNwUbryeKJN2wyhGFRCw0q?usp=sharing)
    and [GitHub](https://github.com/mlabonne/graph-neural-network-course).
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 所有代码都可以在[Google Colab](https://colab.research.google.com/drive/1ZugveUjRrbSNwUbryeKJN2wyhGFRCw0q?usp=sharing)和[GitHub](https://github.com/mlabonne/graph-neural-network-course)上找到。
- en: '[PRE0]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '[PRE1]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Now that PyTorch Geometric is installed, let’s explore the dataset we will use
    in this tutorial.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 现在PyTorch Geometric已经安装好了，让我们探索一下本教程中将使用的数据集。
- en: 🌐 I. Graph data
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 🌐 I. 图数据
- en: '[Graphs](https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)) are an
    essential structure for representing relationships between objects. You can encounter
    graph data in a multitude of real-world scenarios, such as social and computer
    networks, chemical structures of molecules, natural language processing, and image
    recognition, to name a few.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '[图](https://en.wikipedia.org/wiki/Graph_(discrete_mathematics))是表示对象之间关系的重要结构。你可以在许多现实世界的场景中遇到图数据，例如社交和计算机网络、分子化学结构、自然语言处理和图像识别等。'
- en: In this article, we will study the infamous and much-used [Zachary’s karate
    club](https://en.wikipedia.org/wiki/Zachary%27s_karate_club) dataset.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我们将研究臭名昭著且广泛使用的[扎卡里的空手道俱乐部](https://en.wikipedia.org/wiki/Zachary%27s_karate_club)数据集。
- en: '![](../Images/00365b1de2d9e1641523993393467f7c.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/00365b1de2d9e1641523993393467f7c.png)'
- en: Image by author
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 图片作者提供
- en: The Zachary’s karate club dataset embodies the relationships formed within a
    karate club as observed by Wayne W. Zachary during the 1970s. It is a kind of
    social network, where each node represents a club member, and edges between nodes
    represent interactions that occurred outside the club environment.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 扎卡里的空手道俱乐部数据集体现了1970年代Wayne W. Zachary观察到的空手道俱乐部内部形成的关系。这是一种社交网络，其中每个节点代表一个俱乐部成员，节点之间的边代表发生在俱乐部环境之外的互动。
- en: In this particular scenario, the members of the club are split into four distinct
    groups. Our task is to **assign the correct group to each member** (node classification),
    based on the pattern of their interactions.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个特定的场景中，俱乐部成员被分为四个不同的组。我们的任务是**根据他们的互动模式给每个成员分配正确的组**（节点分类）。
- en: Let’s import the dataset with PyG’s built-in function and try to understand
    the `Datasets` object it uses.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用PyG的内置函数导入数据集，并尝试了解它使用的`Datasets`对象。
- en: '[PRE2]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '[PRE3]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '[PRE4]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: This dataset only has 1 graph, where each node has a feature vector of 34 dimensions
    and is part of one out of four classes (our four groups). Actually, the `Datasets`
    object can be seen as a collection of `Data` (graph) objects.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 该数据集仅包含1个图，其中每个节点具有34维的特征向量，并且属于四个类别中的一个（我们的四个组）。实际上，`Datasets`对象可以看作是`Data`（图）对象的集合。
- en: We can further inspect our unique graph to know more about it.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以进一步检查我们独特的图，以了解更多信息。
- en: '[PRE5]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '[PRE6]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The `[Data](https://pytorch-geometric.readthedocs.io/en/latest/modules/data.html)`
    object is particularly interesting. Printing it offers a good summary of the graph
    we''re studying:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '`[Data](https://pytorch-geometric.readthedocs.io/en/latest/modules/data.html)`对象特别有趣。打印它可以很好地总结我们正在研究的图：'
- en: '`x=[34, 34]` is the **node feature matrix** with shape (number of nodes, number
    of features). In our case, it means that we have 34 nodes (our 34 members), each
    node being associated to a 34-dim feature vector.'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`x=[34, 34]`是**节点特征矩阵**，其形状为（节点数，特征数）。在我们的例子中，这意味着我们有34个节点（我们的34个成员），每个节点都与一个34维特征向量相关联。'
- en: '`edge_index=[2, 156]` represents the **graph connectivity** (how the nodes
    are connected) with shape (2, number of directed edges).'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`edge_index=[2, 156]`表示**图的连通性**（节点如何连接），其形状为（2，定向边的数量）。'
- en: '`y=[34]` is the **node ground-truth labels**. In this problem, every node is
    assigned to one class (group), so we have one value for each node.'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`y=[34]`是**节点真实标签**。在这个问题中，每个节点被分配到一个类别（组），因此我们对每个节点有一个值。'
- en: '`train_mask=[34]` is an optional attribute that tells which nodes should be
    used for training with a list of `True` or `False` statements.'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`train_mask=[34]`是一个可选属性，用于指定哪些节点应用于训练，列表中包含`True`或`False`。'
- en: Let’s print each of these tensors to understand what they store. Let’s start
    with the node features.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们打印这些张量以了解它们存储了什么。我们从节点特征开始。
- en: '[PRE7]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '[PRE8]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '[PRE9]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Here, the node feature matrix `x` is an identity matrix: it **doesn''t contain
    any relevant information** about the nodes. It could contain information like
    age, skill level, etc. but this is not the case in this dataset. It means we''ll
    have to classify our nodes just by looking at their connections.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，节点特征矩阵`x`是一个单位矩阵：它**不包含任何相关信息**关于节点。它本可以包含诸如年龄、技能水平等信息，但在这个数据集中并非如此。这意味着我们只能通过查看节点之间的连接来对它们进行分类。
- en: Now, let’s print the edge index.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们打印边索引。
- en: '[PRE10]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '[PRE11]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: In graph theory and network analysis, connectivity between nodes is stored using
    a variety of data structures. The `edge_index` is one such data structure, where
    the graph's connections are stored in **two lists** (156 directed edges, which
    equate to 78 bidirectional edges). The reason for these two lists is that one
    list stores the source nodes, while the second one identifies the destination
    nodes.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在图论和网络分析中，节点之间的连通性通过多种数据结构进行存储。`edge_index`就是这种数据结构之一，其中图的连接存储在**两个列表**中（156条定向边，相当于78条双向边）。这两个列表的原因在于一个列表存储源节点，而第二个列表标识目标节点。
- en: This method is known as a **coordinate list** (COO) format, which is essentially
    a means to efficiently store a [sparse matrix](https://en.wikipedia.org/wiki/Sparse_matrix#Storing_a_sparse_matrix).
    Sparse matrices are data structures that efficiently store matrices with a majority
    of zero elements. In the COO format, only non-zero elements are stored, saving
    memory and computational resources.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法称为**坐标列表**（COO）格式，本质上是一种高效存储[稀疏矩阵](https://en.wikipedia.org/wiki/Sparse_matrix#Storing_a_sparse_matrix)的方式。稀疏矩阵是高效存储大部分为零元素的矩阵的数据结构。在COO格式中，仅存储非零元素，从而节省内存和计算资源。
- en: Contrarily, a more intuitive and straightforward way to represent graph connectivity
    is through an **adjacency matrix** *A*. This is a square matrix where each element
    *A*ᵢⱼ *s*pecifies the presence or absence of an edge from node *i* to node *j*
    in the graph. In other words, a non-zero element *A*ᵢⱼ implies a connection from
    node *i* to node *j*, and a zero indicates no direct connection.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，更直观和简洁的表示图连通性的方法是通过**邻接矩阵** *A*。这是一个方阵，其中每个元素*A*ᵢⱼ *s*指定图中从节点*i*到节点*j*的边的存在与否。换句话说，非零元素*A*ᵢⱼ
    表示从节点*i*到节点*j*的连接，而零表示没有直接连接。
- en: '![](../Images/7a85c8e90254bb8ed5f76a90cbc92443.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7a85c8e90254bb8ed5f76a90cbc92443.png)'
- en: Image by author
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片
- en: An adjacency matrix, however, is not as space-efficient as the COO format for
    sparse matrices or graphs with fewer edges. However, for clarity and easy interpretation,
    the adjacency matrix remains a popular choice for representing graph connectivity.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，邻接矩阵在稀疏矩阵或边较少的图中并不像COO格式那样节省空间。然而，为了清晰和易于解释，邻接矩阵仍然是表示图连通性的热门选择。
- en: The adjacency matrix can be inferred from the `edge_index` with a utility function
    `to_dense_adj()`.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 邻接矩阵可以通过`edge_index`和一个工具函数`to_dense_adj()`来推断。
- en: '[PRE12]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '[PRE13]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '[PRE14]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: With graph data, it is relatively uncommon for nodes to be densely interconnected.
    As you can see, our adjacency matrix *A* is **sparse** (filled with zeros).
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 对于图数据，节点之间密集互连的情况相对较少。正如你所见，我们的邻接矩阵*A*是**稀疏的**（填充了零）。
- en: In many real-world graphs, most nodes are connected to only a few other nodes,
    resulting in a large number of zeros in the adjacency matrix. Storing so many
    zeros is not efficient at all, which is why the COO format is adopted by PyG.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多现实世界的图中，大多数节点只与少数其他节点连接，导致邻接矩阵中有大量零。存储这么多零是完全不高效的，这就是为什么 PyG 采用了 COO 格式。
- en: On the contrary, ground-truth labels are easy to understand.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，真实标签易于理解。
- en: '[PRE15]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '[PRE16]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Our node ground-truth labels stored in `y` simply encode the group number (0,
    1, 2, 3) for each node, which is why we have 34 values.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 我们存储在 `y` 中的节点真实标签仅仅编码了每个节点的组号（0, 1, 2, 3），这就是为什么我们有34个值。
- en: Finally, let’s print the train mask.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们打印训练掩码。
- en: '[PRE17]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '[PRE18]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: The train mask shows which nodes are supposed to be used for training with `True`
    statements. These nodes represent the training set, while the others can be considered
    as the test set. This division helps in model evaluation by providing unseen data
    for testing.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 训练掩码显示了哪些节点应该用 `True` 语句进行训练。这些节点代表训练集，而其他节点可以视为测试集。这种划分有助于通过提供未见过的数据来进行模型评估。
- en: 'But we’re not done yet! The `[Data](https://pytorch-geometric.readthedocs.io/en/latest/modules/data.html)`
    object has a lot more to offer. It provides various utility functions that enable
    the investigation of several properties of the graph. For instance:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 但我们还没完成！`[Data](https://pytorch-geometric.readthedocs.io/en/latest/modules/data.html)`
    对象提供了更多功能。它提供了各种实用函数，使得可以调查图的多个属性。例如：
- en: '`is_directed()` tells you if the graph is **directed**. A directed graph signifies
    that the adjacency matrix is not symmetric, i.e., the direction of edges matters
    in the connections between nodes.'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`is_directed()` 告诉你图是否**有向**。有向图意味着邻接矩阵不是对称的，即边的方向在节点间的连接中是重要的。'
- en: '`isolated_nodes()` checks if some nodes are **not connected** to the rest of
    the graph. These nodes are likely to pose challenges in tasks like classification
    due to their lack of connections.'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`isolated_nodes()` 检查是否有一些节点**没有连接**到图的其余部分。这些节点可能在分类等任务中带来挑战，因为它们缺乏连接。'
- en: '`has_self_loops()` indicates if at least one node is **connected to itself**.
    This is distinct from the concept of [loops](https://en.wikipedia.org/wiki/Loop_(graph_theory)):
    a loop implies a path that starts and ends at the same node, traversing other
    nodes in between.'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`has_self_loops()` 表示是否至少有一个节点**自我连接**。这与[环](https://en.wikipedia.org/wiki/Loop_(graph_theory))的概念不同：环意味着一条路径开始和结束于同一个节点，在此过程中遍历其他节点。'
- en: In the context of the Zachary’s karate club dataset, all these properties return
    `False`. This implies that the graph is not directed, does not have any isolated
    nodes, and none of its nodes are connected to themselves.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 在扎卡里武术俱乐部数据集中，所有这些属性返回 `False`。这意味着图是无向的，没有孤立节点，并且没有节点与自身相连。
- en: '[PRE19]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '[PRE20]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Finally, we can convert a graph from PyTorch Geometric to the popular graph
    library [NetworkX](https://networkx.org/) using `[to_networkx](https://pytorch-geometric.readthedocs.io/en/latest/modules/utils.html?highlight=to_networkx#torch_geometric.utils.to_networkx)`.
    This is particularly useful to visualize a small graph with `networkx` and `matplotlib`.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以使用 `[to_networkx](https://pytorch-geometric.readthedocs.io/en/latest/modules/utils.html?highlight=to_networkx#torch_geometric.utils.to_networkx)`
    将 PyTorch Geometric 图转换为流行的图库 [NetworkX](https://networkx.org/)。这对于使用 `networkx`
    和 `matplotlib` 可视化小图特别有用。
- en: Let’s plot our dataset with a different color for each group.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们为每个组绘制不同颜色的数据集。
- en: '[PRE21]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '[PRE22]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '![](../Images/571cb9949785200ba0307b6172e0f3fb.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/571cb9949785200ba0307b6172e0f3fb.png)'
- en: This plot of Zachary’s karate club displays our 34 nodes, 78 (bidirectional)
    edges, and 4 labels with 4 different colors. Now that we’ve seen the essentials
    of loading and handling a dataset with PyTorch Geometric, we can introduce the
    **Graph Convolutional Network** architecture.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 这个扎卡里武术俱乐部的图显示了我们的34个节点、78条（双向）边和4个标签及4种不同颜色。现在我们已经了解了使用 PyTorch Geometric 加载和处理数据集的基本内容，我们可以介绍**图卷积网络**架构。
- en: ✉️ II. Graph Convolutional Network
  id: totrans-78
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ✉️ II. 图卷积网络
- en: This section aims to introduce and build the graph convolutional layer from
    the ground up.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 本节旨在从头开始介绍和构建图卷积层。
- en: 'In traditional neural networks, linear layers apply a **linear transformation**
    to the incoming data. This transformation converts input features *x* into hidden
    vectors *h* through the use of a weight matrix 𝐖. Ignoring biases for the time
    being, this can be expressed as:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在传统的神经网络中，线性层对传入的数据应用**线性变换**。这种变换通过使用权重矩阵 𝐖 将输入特征 *x* 转换为隐藏向量 *h*。暂时忽略偏差，这可以表示为：
- en: '![](../Images/d99b824e5c2dca4657e4b2eb18b4d9e7.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d99b824e5c2dca4657e4b2eb18b4d9e7.png)'
- en: With graph data, an additional layer of complexity is added through the **connections
    between nodes**. These connections matter because, typically, in networks, it’s
    assumed that similar nodes are more likely to be linked to each other than dissimilar
    ones, a phenomenon known as [network homophily](https://en.wikipedia.org/wiki/Network_homophily).
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在图数据中，通过**节点之间的连接**增加了额外的复杂性。这些连接很重要，因为在网络中，通常假设相似的节点比不相似的节点更可能互相链接，这种现象被称为[网络同质性](https://en.wikipedia.org/wiki/Network_homophily)。
- en: We can enrich our **node representation** by merging its features with those
    of its neighbors. This operation is called convolution, or neighborhood aggregation.
    Let’s represent the neighborhood of node *i* including itself as *Ñ*.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过将节点的特征与邻居的特征合并来丰富我们的**节点表示**。这个操作称为卷积或邻域聚合。让我们将节点 *i* 及其邻域表示为 *Ñ*。
- en: '![](../Images/c368d96059e7977d7ff141a4b92b8bd6.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c368d96059e7977d7ff141a4b92b8bd6.png)'
- en: 'Unlike filters in Convolutional Neural Networks (CNNs), our weight matrix 𝐖
    is unique and shared among every node. But there is another issue: nodes do not
    have a **fixed number of neighbors** like pixels do.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 与卷积神经网络（CNNs）中的滤波器不同，我们的权重矩阵 𝐖 是唯一的，并且在每个节点之间共享。但还有另一个问题：节点没有像像素那样的**固定邻居数量**。
- en: How do we address cases where one node has only one neighbor, and another has
    500? If we simply sum the feature vectors, the resulting embedding *h* would be
    much larger for the node with 500 neighbors. To ensure a **similar range** of
    values for all nodes and comparability between them, we can normalize the result
    based on the **degree** of nodes, where degree refers to the number of connections
    a node has.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何处理一个节点只有一个邻居，而另一个节点有500个邻居的情况？如果我们简单地将特征向量相加，那么对于拥有500个邻居的节点，得到的嵌入 *h* 将会大得多。为了确保所有节点的值具有**相似的范围**并便于比较，我们可以根据节点的**度**来归一化结果，其中度是指一个节点的连接数量。
- en: '![](../Images/99665492296bf842d6b89bd2c19fb899.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/99665492296bf842d6b89bd2c19fb899.png)'
- en: We’re almost there! Introduced by Kipf et al. (2016), the [graph convolutional
    layer](https://arxiv.org/abs/1609.02907) has one final improvement.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 我们快到了！由Kipf等人（2016）介绍的[图卷积层](https://arxiv.org/abs/1609.02907)还有一个最终的改进。
- en: 'The authors observed that features from nodes with numerous neighbors propagate
    much more easily than those from more isolated nodes. To offset this effect, they
    suggested assigning **bigger weights** to features from nodes with fewer neighbors,
    thus balancing the influence across all nodes. This operation is written as:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 作者观察到，具有大量邻居的节点的特征比那些较孤立节点的特征传播得更容易。为了抵消这种效应，他们建议为邻居较少的节点的特征分配**更大的权重**，从而平衡所有节点的影响。这个操作可以表示为：
- en: '![](../Images/04becb820335cbd89cebb547724d12ea.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/04becb820335cbd89cebb547724d12ea.png)'
- en: Note that when *i* and *j* have the same number of neighbors, it is equivalent
    to our own layer. Now, let’s see how to implement it in Python with PyTorch Geometric.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，当 *i* 和 *j* 拥有相同数量的邻居时，这等同于我们自己定义的层。现在，让我们看看如何在Python中使用PyTorch Geometric实现它。
- en: 🧠 III. Implementing a GCN
  id: totrans-92
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 🧠 III. 实现一个GCN
- en: PyTorch Geometric provides the `GCNConv` function, which directly implements
    the graph convolutional layer.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch Geometric提供了`GCNConv`函数，该函数直接实现了图卷积层。
- en: In this example, we’ll create a basic Graph Convolutional Network with a single
    GCN layer, a ReLU activation function, and a linear output layer. This output
    layer will yield **four values** corresponding to our four categories, with the
    highest value determining the class of each node.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们将创建一个基本的图卷积网络，包括一个GCN层、一个ReLU激活函数和一个线性输出层。这个输出层将产生**四个值**，对应我们的四个类别，最高值将决定每个节点的类别。
- en: In the following code block, we define the GCN layer with a 3-dimensional hidden
    layer.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下代码块中，我们定义了一个具有3维隐藏层的GCN层。
- en: '[PRE23]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '[PRE24]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '[PRE25]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: If we added a second GCN layer, our model would not only aggregate feature vectors
    from the neighbors of each node, but also from the neighbors of these neighbors.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们添加了第二个GCN层，我们的模型将不仅仅从每个节点的邻居处聚合特征向量，还会从这些邻居的邻居处聚合特征向量。
- en: 'We can **stack several graph layers** to aggregate more and more distant values,
    but there’s a catch: if we add too many layers, the aggregation becomes so intense
    that all the embeddings end up looking the same. This phenomenon is called **over-smoothing**
    and can be a real problem when you have too many layers.'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以**堆叠多个图层**以聚合更多的远离值，但有一个问题：如果我们添加太多图层，聚合变得如此强烈，以至于所有嵌入最终看起来都一样。这种现象被称为**过度平滑**，当图层过多时，可能会成为一个实际问题。
- en: Now that we’ve defined our GNN, let’s write a simple training loop with PyTorch.
    I chose a regular cross-entropy loss since it’s a multi-class classification task,
    with Adam as optimizer. In this article, we won’t implement a train/test split
    to keep things simple and focus on how GNNs learn instead.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经定义了 GNN，让我们用 PyTorch 编写一个简单的训练循环。我选择了常规的交叉熵损失，因为这是一个多类分类任务，优化器使用 Adam。在本文中，我们不会实现训练/测试拆分，以保持简单，专注于
    GNN 如何学习。
- en: 'The training loop is standard: we try to predict the correct labels, and we
    compare the GCN’s results to the values stored in `data.y`. The error is calculated
    by the cross-entropy loss and backpropagated with Adam to fine-tune our GNN''s
    weights and biases. Finally, we print metrics every 10 epochs.'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 训练循环是标准的：我们尝试预测正确的标签，并将 GCN 的结果与 `data.y` 中存储的值进行比较。通过交叉熵损失计算错误，并使用 Adam 进行反向传播，以微调
    GNN 的权重和偏差。最后，我们每 10 个 epochs 打印一次指标。
- en: '[PRE26]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '[PRE27]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: '[PRE28]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Great! Without much surprise, we reach 100% accuracy on the training set (full
    dataset). It means that our model learned to correctly assign every member of
    the karate club to its correct group.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 太好了！毫不奇怪，我们在训练集（完整数据集）上达到了 100% 的准确率。这意味着我们的模型学会了正确地将每个空手道俱乐部的成员分配到正确的组。
- en: We can produce a neat visualization by animating the graph and see the evolution
    of the GNN’s predictions during the training process.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过动画化图形来生成一个整洁的可视化效果，并观察 GNN 在训练过程中预测的演变。
- en: '[PRE29]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: '[PRE30]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: '![](../Images/04e6b675ab152228d57739474387a402.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/04e6b675ab152228d57739474387a402.png)'
- en: The first predictions are random, but the GCN perfectly labels every node after
    a while. Indeed, the final graph is the same as the one we plotted at the end
    of the first section. But what does the GCN really learn?
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 最初的预测是随机的，但经过一段时间，GCN 能够完美地标记每个节点。实际上，最终的图形与我们在第一部分末尾绘制的图形相同。但 GCN 到底学到了什么？
- en: By aggregating features from neighboring nodes, the GNN learns a vector representation
    (or **embedding**) of every node in the network. In our model, the final layer
    just learns how to use these representations to produce the best classifications.
    However, embeddings are the real products of GNNs.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 通过聚合邻近节点的特征，GNN 学习了网络中每个节点的向量表示（或**嵌入**）。在我们的模型中，最终层仅学习如何使用这些表示来产生最佳分类。然而，嵌入才是真正的
    GNN 产物。
- en: Let’s print the embeddings learned by our model.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们打印出模型学到的嵌入。
- en: '[PRE31]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: '[PRE32]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: As you can see, embeddings do not need to have the same dimensions as feature
    vectors. Here, I chose to reduce the number of dimensions from 34 (`dataset.num_features`)
    to three to get a nice visualization in 3D.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，嵌入不需要具有与特征向量相同的维度。在这里，我选择将维度从 34 (`dataset.num_features`) 降到三维，以获得更好的 3D
    可视化效果。
- en: Let’s plot these embeddings before any training happens, at epoch 0.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在训练开始之前，即第 0 轮，绘制这些嵌入。
- en: '[PRE33]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: '[PRE34]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: '![](../Images/a961a759d54dbe8911d59fb3ab66f824.png)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a961a759d54dbe8911d59fb3ab66f824.png)'
- en: We see every node from Zachary’s karate club with their true labels (and not
    the model’s predictions). For now, they’re all over the place since the GNN is
    not trained yet. But if we plot these embeddings at each step of the training
    loop, we’d be able to visualize what the GNN truly learns.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到 Zachary 空手道俱乐部中的每个节点及其真实标签（而不是模型的预测）。目前，它们还很分散，因为 GNN 尚未训练完成。但如果我们在训练循环的每一步绘制这些嵌入，我们将能够可视化
    GNN 实际上学到了什么。
- en: Let’s see how they evolve over time, as the GCN gets better and better at classifying
    nodes.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看它们随着时间的推移如何演变，随着 GCN 在分类节点方面变得越来越好。
- en: '[PRE35]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: '[PRE36]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: '![](../Images/b4af8ca00284bd6d1418674e7584cebe.png)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b4af8ca00284bd6d1418674e7584cebe.png)'
- en: Our Graph Convolutional Network (GCN) has effectively learned embeddings that
    group similar nodes into **distinct clusters**. This enables the final linear
    layer to distinguish them into separate classes with ease.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的图卷积网络（GCN）有效地学习了将相似节点分组到**不同的簇**中的嵌入。这使得最终的线性层能够轻松地区分它们为不同的类别。
- en: 'Embeddings are not unique to GNNs: they can be found everywhere in deep learning.
    They don’t have to be 3D either: actually, they rarely are. For instance, language
    models like [BERT](https://arxiv.org/abs/1810.04805) produce embeddings with 768
    or even 1024 dimensions.'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 嵌入并非GNN特有：它们在深度学习中无处不在。它们也不一定是三维的：实际上，它们很少是三维的。例如，像[BERT](https://arxiv.org/abs/1810.04805)这样的语言模型生成的嵌入维度通常是768甚至1024。
- en: Additional dimensions store more information about nodes, text, images, etc.
    but they also create bigger models that are more difficult to train. This is why
    keeping low-dimensional embeddings as long as possible is advantageous.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 额外的维度存储了关于节点、文本、图像等更多的信息，但它们也会创建更大的模型，这些模型更难以训练。这就是为什么尽可能保持低维嵌入是有利的原因。
- en: Conclusion
  id: totrans-129
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: Graph Convolutional Networks are an incredibly versatile architecture that can
    be applied in **many contexts**. In this article, we familiarized ourselves with
    the PyTorch Geometric library and objects like `Datasets` and `Data`. Then, we
    successfully reconstructed a graph convolutional layer from the ground up. Next,
    we put theory into practice by implementing a GCN, which gave us an understanding
    of practical aspects and how individual components interact. Finally, we visualized
    the training process and obtained a clear perspective of what it involves for
    such a network.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 图卷积网络是一种非常多功能的架构，可以应用于**许多背景**。在这篇文章中，我们熟悉了PyTorch Geometric库以及像`Datasets`和`Data`这样的对象。然后，我们成功地从头开始重建了一个图卷积层。接下来，我们通过实现一个GCN将理论付诸实践，这使我们理解了实际的方面以及各个组件如何相互作用。最后，我们可视化了训练过程，并清楚地了解了这种网络所涉及的内容。
- en: 'Zachary’s karate club is a simplistic dataset, but it is good enough to understand
    the most important concepts in graph data and GNNs. Although we only talked about
    node classification in this article, there are other tasks GNNs can accomplish:
    **link prediction** (e.g., to recommend a friend), **graph classification** (e.g.,
    to label molecules), **graph generation** (e.g., to create new molecules), and
    so on.'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: Zachary的空手道俱乐部是一个简单的数据集，但足够用来理解图数据和GNN中的最重要概念。尽管我们在这篇文章中仅讨论了节点分类，但GNN还可以完成其他任务：**链接预测**（例如，推荐朋友）、**图分类**（例如，标记分子）、**图生成**（例如，创建新分子）等。
- en: Beyond GCN, numerous GNN layers and architectures have been proposed by researchers.
    In the next article, we’ll introduce the [Graph Attention Network](https://mlabonne.github.io/blog/gat/)
    (GAT) architecture, which dynamically computes the GCN’s normalization factor
    and the importance of each connection with an attention mechanism.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 除了GCN之外，研究人员还提出了许多GNN层和架构。在下一篇文章中，我们将介绍[图注意力网络](https://mlabonne.github.io/blog/gat/)（GAT）架构，它通过注意机制动态计算GCN的归一化因子和每个连接的重要性。
- en: If you want to know more about graph neural networks, dive deeper into the world
    of GNNs with my book, [Hands-On Graph Neural Networks](https://mlabonne.github.io/blog/book.html).
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想了解更多关于图神经网络的信息，可以通过我的书籍[《动手实践图神经网络》](https://mlabonne.github.io/blog/book.html)深入探索GNN的世界。
- en: Next article
  id: totrans-134
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 下一篇文章
- en: '[](/graph-attention-networks-in-python-975736ac5c0c?source=post_page-----24b3f60d6c95--------------------------------)
    [## Chapter 2: Graph Attention Networks: Self-Attention Explained'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/graph-attention-networks-in-python-975736ac5c0c?source=post_page-----24b3f60d6c95--------------------------------)
    [## 第2章：图注意力网络：自注意力解析'
- en: A guide to GNNs with self-attention using PyTorch Geometric
  id: totrans-136
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用PyTorch Geometric的自注意力GNN指南
- en: towardsdatascience.com](/graph-attention-networks-in-python-975736ac5c0c?source=post_page-----24b3f60d6c95--------------------------------)
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '[towardsdatascience.com](/graph-attention-networks-in-python-975736ac5c0c?source=post_page-----24b3f60d6c95--------------------------------)'
- en: '*Learn more about machine learning and support my work with one click — become
    a Medium member here:*'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '*通过点击一下了解更多机器学习知识并支持我的工作 — 成为Medium会员，请点击这里：*'
- en: '[](https://medium.com/@mlabonne/membership?source=post_page-----24b3f60d6c95--------------------------------)
    [## Join Medium with my referral link — Maxime Labonne'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@mlabonne/membership?source=post_page-----24b3f60d6c95--------------------------------)
    [## 通过我的推荐链接加入Medium — Maxime Labonne'
- en: As a Medium member, a portion of your membership fee goes to writers you read,
    and you get full access to every story…
  id: totrans-140
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 作为Medium会员，你的会员费的一部分将用于支持你阅读的作者，你将获得对每个故事的完全访问权限……
- en: medium.com](https://medium.com/@mlabonne/membership?source=post_page-----24b3f60d6c95--------------------------------)
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '[medium.com](https://medium.com/@mlabonne/membership?source=post_page-----24b3f60d6c95--------------------------------)'
- en: '*If you’re already a member, you can* [*follow me on Medium*](https://medium.com/@mlabonne)*.*'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '*如果你已经是会员，你可以* [*在 Medium 上关注我*](https://medium.com/@mlabonne)*.*'
