- en: 'Graph Convolutional Networks: Introduction to GNNs'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å›¾å·ç§¯ç½‘ç»œï¼šGNNsç®€ä»‹
- en: åŸæ–‡ï¼š[https://towardsdatascience.com/graph-convolutional-networks-introduction-to-gnns-24b3f60d6c95?source=collection_archive---------0-----------------------#2023-08-14](https://towardsdatascience.com/graph-convolutional-networks-introduction-to-gnns-24b3f60d6c95?source=collection_archive---------0-----------------------#2023-08-14)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡ï¼š[https://towardsdatascience.com/graph-convolutional-networks-introduction-to-gnns-24b3f60d6c95?source=collection_archive---------0-----------------------#2023-08-14](https://towardsdatascience.com/graph-convolutional-networks-introduction-to-gnns-24b3f60d6c95?source=collection_archive---------0-----------------------#2023-08-14)
- en: A step-by-step guide using PyTorch Geometric
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä½¿ç”¨PyTorch Geometricçš„é€æ­¥æŒ‡å—
- en: '[](https://medium.com/@mlabonne?source=post_page-----24b3f60d6c95--------------------------------)[![Maxime
    Labonne](../Images/a7efdd305e3cc77d5509bbb1076d57d8.png)](https://medium.com/@mlabonne?source=post_page-----24b3f60d6c95--------------------------------)[](https://towardsdatascience.com/?source=post_page-----24b3f60d6c95--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----24b3f60d6c95--------------------------------)
    [Maxime Labonne](https://medium.com/@mlabonne?source=post_page-----24b3f60d6c95--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@mlabonne?source=post_page-----24b3f60d6c95--------------------------------)[![Maxime
    Labonne](../Images/a7efdd305e3cc77d5509bbb1076d57d8.png)](https://medium.com/@mlabonne?source=post_page-----24b3f60d6c95--------------------------------)[](https://towardsdatascience.com/?source=post_page-----24b3f60d6c95--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----24b3f60d6c95--------------------------------)
    [Maxime Labonne](https://medium.com/@mlabonne?source=post_page-----24b3f60d6c95--------------------------------)'
- en: Â·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: Â·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fdc89da634938&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgraph-convolutional-networks-introduction-to-gnns-24b3f60d6c95&user=Maxime+Labonne&userId=dc89da634938&source=post_page-dc89da634938----24b3f60d6c95---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----24b3f60d6c95--------------------------------)
    Â·16 min readÂ·Aug 14, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F24b3f60d6c95&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgraph-convolutional-networks-introduction-to-gnns-24b3f60d6c95&user=Maxime+Labonne&userId=dc89da634938&source=-----24b3f60d6c95---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[å…³æ³¨](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fdc89da634938&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgraph-convolutional-networks-introduction-to-gnns-24b3f60d6c95&user=Maxime+Labonne&userId=dc89da634938&source=post_page-dc89da634938----24b3f60d6c95---------------------post_header-----------)
    å‘è¡¨åœ¨[Towards Data Science](https://towardsdatascience.com/?source=post_page-----24b3f60d6c95--------------------------------)
    Â·16åˆ†é’Ÿé˜…è¯»Â·2023å¹´8æœˆ14æ—¥[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F24b3f60d6c95&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgraph-convolutional-networks-introduction-to-gnns-24b3f60d6c95&user=Maxime+Labonne&userId=dc89da634938&source=-----24b3f60d6c95---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F24b3f60d6c95&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgraph-convolutional-networks-introduction-to-gnns-24b3f60d6c95&source=-----24b3f60d6c95---------------------bookmark_footer-----------)![](../Images/53a44290154e9eb7c20b5a32cd4d5642.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F24b3f60d6c95&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgraph-convolutional-networks-introduction-to-gnns-24b3f60d6c95&source=-----24b3f60d6c95---------------------bookmark_footer-----------)![](../Images/53a44290154e9eb7c20b5a32cd4d5642.png)'
- en: Image by author
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: ä½œè€…æä¾›çš„å›¾åƒ
- en: '**Graph Neural Networks** (GNNs) represent one of the most captivating and
    rapidly evolving architectures within the deep learning landscape. As deep learning
    models designed to process data structured as graphs, GNNs bring remarkable versatility
    and powerful learning capabilities.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '**å›¾ç¥ç»ç½‘ç»œ**ï¼ˆGNNsï¼‰æ˜¯æ·±åº¦å­¦ä¹ é¢†åŸŸä¸­æœ€å¸å¼•äººå’Œè¿…é€Ÿå‘å±•çš„æ¶æ„ä¹‹ä¸€ã€‚ä½œä¸ºå¤„ç†å›¾ç»“æ„æ•°æ®çš„æ·±åº¦å­¦ä¹ æ¨¡å‹ï¼ŒGNNså¸¦æ¥äº†æ˜¾è‘—çš„å¤šæ ·æ€§å’Œå¼ºå¤§çš„å­¦ä¹ èƒ½åŠ›ã€‚'
- en: Among the various types of GNNs, the **Graph Convolutional Networks** (GCNs)
    have emerged as the most [prevalent and broadly applied model](https://paperswithcode.com/methods/category/graph-models).
    GCNs are innovative due to their ability to leverage both the features of a node
    and its locality to make predictions, providing an effective way to handle graph-structured
    data.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å„ç§ç±»å‹çš„GNNä¸­ï¼Œ**å›¾å·ç§¯ç½‘ç»œ**ï¼ˆGCNsï¼‰å·²ç»æˆä¸ºæœ€[æ™®éä¸”å¹¿æ³›åº”ç”¨çš„æ¨¡å‹](https://paperswithcode.com/methods/category/graph-models)ã€‚GCNså› å…¶èƒ½å¤Ÿåˆ©ç”¨èŠ‚ç‚¹çš„ç‰¹å¾åŠå…¶å±€éƒ¨ä¿¡æ¯è¿›è¡Œé¢„æµ‹è€Œå…·æœ‰åˆ›æ–°æ€§ï¼Œæä¾›äº†ä¸€ç§æœ‰æ•ˆå¤„ç†å›¾ç»“æ„æ•°æ®çš„æ–¹æ³•ã€‚
- en: In this article, we will delve into the mechanics of the GCN layer and explain
    its inner workings. Furthermore, we will explore its practical application for
    node classification tasks, using [PyTorch Geometric](https://pytorch-geometric.readthedocs.io/en/latest/index.html)
    as our tool of choice.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å°†æ·±å…¥æ¢è®¨GCNå±‚çš„æœºåˆ¶ï¼Œå¹¶è§£é‡Šå…¶å†…éƒ¨å·¥ä½œåŸç†ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å°†æ¢ç´¢å…¶åœ¨èŠ‚ç‚¹åˆ†ç±»ä»»åŠ¡ä¸­çš„å®é™…åº”ç”¨ï¼Œä½¿ç”¨[PyTorch Geometric](https://pytorch-geometric.readthedocs.io/en/latest/index.html)ä½œä¸ºæˆ‘ä»¬çš„å·¥å…·ã€‚
- en: PyTorch Geometric is a specialized extension of PyTorch that has been created
    specifically for the development and implementation of GNNs. It is an advanced,
    yet user-friendly library that provides a comprehensive suite of tools to facilitate
    graph-based machine learning. To commence our journey, the PyTorch Geometric installation
    will be required. If you are using Google Colab, [PyTorch](https://pytorch.org/get-started/locally/)
    should already be in place, so all we need to do is execute a few additional commands.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch Geometricæ˜¯PyTorchçš„ä¸€ä¸ªä¸“é—¨æ‰©å±•ï¼Œä¸“ä¸ºGNNsçš„å¼€å‘å’Œå®ç°è€Œåˆ›å»ºã€‚å®ƒæ˜¯ä¸€ä¸ªé«˜çº§ä½†ç”¨æˆ·å‹å¥½çš„åº“ï¼Œæä¾›äº†ä¸€æ•´å¥—å·¥å…·æ¥ä¿ƒè¿›åŸºäºå›¾çš„æœºå™¨å­¦ä¹ ã€‚ä¸ºäº†å¼€å§‹æˆ‘ä»¬çš„æ—…ç¨‹ï¼Œæˆ‘ä»¬éœ€è¦å®‰è£…PyTorch
    Geometricã€‚å¦‚æœä½ ä½¿ç”¨Google Colabï¼Œ[PyTorch](https://pytorch.org/get-started/locally/)åº”è¯¥å·²ç»å®‰è£…å¥½äº†ï¼Œå› æ­¤æˆ‘ä»¬åªéœ€è¦æ‰§è¡Œå‡ ä¸ªé¢å¤–çš„å‘½ä»¤ã€‚
- en: All the code is available on [Google Colab](https://colab.research.google.com/drive/1ZugveUjRrbSNwUbryeKJN2wyhGFRCw0q?usp=sharing)
    and [GitHub](https://github.com/mlabonne/graph-neural-network-course).
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€æœ‰ä»£ç éƒ½å¯ä»¥åœ¨[Google Colab](https://colab.research.google.com/drive/1ZugveUjRrbSNwUbryeKJN2wyhGFRCw0q?usp=sharing)å’Œ[GitHub](https://github.com/mlabonne/graph-neural-network-course)ä¸Šæ‰¾åˆ°ã€‚
- en: '[PRE0]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '[PRE1]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Now that PyTorch Geometric is installed, letâ€™s explore the dataset we will use
    in this tutorial.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨PyTorch Geometricå·²ç»å®‰è£…å¥½äº†ï¼Œè®©æˆ‘ä»¬æ¢ç´¢ä¸€ä¸‹æœ¬æ•™ç¨‹ä¸­å°†ä½¿ç”¨çš„æ•°æ®é›†ã€‚
- en: ğŸŒ I. Graph data
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ğŸŒ I. å›¾æ•°æ®
- en: '[Graphs](https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)) are an
    essential structure for representing relationships between objects. You can encounter
    graph data in a multitude of real-world scenarios, such as social and computer
    networks, chemical structures of molecules, natural language processing, and image
    recognition, to name a few.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '[å›¾](https://en.wikipedia.org/wiki/Graph_(discrete_mathematics))æ˜¯è¡¨ç¤ºå¯¹è±¡ä¹‹é—´å…³ç³»çš„é‡è¦ç»“æ„ã€‚ä½ å¯ä»¥åœ¨è®¸å¤šç°å®ä¸–ç•Œçš„åœºæ™¯ä¸­é‡åˆ°å›¾æ•°æ®ï¼Œä¾‹å¦‚ç¤¾äº¤å’Œè®¡ç®—æœºç½‘ç»œã€åˆ†å­åŒ–å­¦ç»“æ„ã€è‡ªç„¶è¯­è¨€å¤„ç†å’Œå›¾åƒè¯†åˆ«ç­‰ã€‚'
- en: In this article, we will study the infamous and much-used [Zacharyâ€™s karate
    club](https://en.wikipedia.org/wiki/Zachary%27s_karate_club) dataset.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘ä»¬å°†ç ”ç©¶è‡­åæ˜­è‘—ä¸”å¹¿æ³›ä½¿ç”¨çš„[æ‰å¡é‡Œçš„ç©ºæ‰‹é“ä¿±ä¹éƒ¨](https://en.wikipedia.org/wiki/Zachary%27s_karate_club)æ•°æ®é›†ã€‚
- en: '![](../Images/00365b1de2d9e1641523993393467f7c.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/00365b1de2d9e1641523993393467f7c.png)'
- en: Image by author
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ç‰‡ä½œè€…æä¾›
- en: The Zacharyâ€™s karate club dataset embodies the relationships formed within a
    karate club as observed by Wayne W. Zachary during the 1970s. It is a kind of
    social network, where each node represents a club member, and edges between nodes
    represent interactions that occurred outside the club environment.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰å¡é‡Œçš„ç©ºæ‰‹é“ä¿±ä¹éƒ¨æ•°æ®é›†ä½“ç°äº†1970å¹´ä»£Wayne W. Zacharyè§‚å¯Ÿåˆ°çš„ç©ºæ‰‹é“ä¿±ä¹éƒ¨å†…éƒ¨å½¢æˆçš„å…³ç³»ã€‚è¿™æ˜¯ä¸€ç§ç¤¾äº¤ç½‘ç»œï¼Œå…¶ä¸­æ¯ä¸ªèŠ‚ç‚¹ä»£è¡¨ä¸€ä¸ªä¿±ä¹éƒ¨æˆå‘˜ï¼ŒèŠ‚ç‚¹ä¹‹é—´çš„è¾¹ä»£è¡¨å‘ç”Ÿåœ¨ä¿±ä¹éƒ¨ç¯å¢ƒä¹‹å¤–çš„äº’åŠ¨ã€‚
- en: In this particular scenario, the members of the club are split into four distinct
    groups. Our task is to **assign the correct group to each member** (node classification),
    based on the pattern of their interactions.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ä¸ªç‰¹å®šçš„åœºæ™¯ä¸­ï¼Œä¿±ä¹éƒ¨æˆå‘˜è¢«åˆ†ä¸ºå››ä¸ªä¸åŒçš„ç»„ã€‚æˆ‘ä»¬çš„ä»»åŠ¡æ˜¯**æ ¹æ®ä»–ä»¬çš„äº’åŠ¨æ¨¡å¼ç»™æ¯ä¸ªæˆå‘˜åˆ†é…æ­£ç¡®çš„ç»„**ï¼ˆèŠ‚ç‚¹åˆ†ç±»ï¼‰ã€‚
- en: Letâ€™s import the dataset with PyGâ€™s built-in function and try to understand
    the `Datasets` object it uses.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬ä½¿ç”¨PyGçš„å†…ç½®å‡½æ•°å¯¼å…¥æ•°æ®é›†ï¼Œå¹¶å°è¯•äº†è§£å®ƒä½¿ç”¨çš„`Datasets`å¯¹è±¡ã€‚
- en: '[PRE2]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '[PRE3]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '[PRE4]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: This dataset only has 1 graph, where each node has a feature vector of 34 dimensions
    and is part of one out of four classes (our four groups). Actually, the `Datasets`
    object can be seen as a collection of `Data` (graph) objects.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: è¯¥æ•°æ®é›†ä»…åŒ…å«1ä¸ªå›¾ï¼Œå…¶ä¸­æ¯ä¸ªèŠ‚ç‚¹å…·æœ‰34ç»´çš„ç‰¹å¾å‘é‡ï¼Œå¹¶ä¸”å±äºå››ä¸ªç±»åˆ«ä¸­çš„ä¸€ä¸ªï¼ˆæˆ‘ä»¬çš„å››ä¸ªç»„ï¼‰ã€‚å®é™…ä¸Šï¼Œ`Datasets`å¯¹è±¡å¯ä»¥çœ‹ä½œæ˜¯`Data`ï¼ˆå›¾ï¼‰å¯¹è±¡çš„é›†åˆã€‚
- en: We can further inspect our unique graph to know more about it.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥è¿›ä¸€æ­¥æ£€æŸ¥æˆ‘ä»¬ç‹¬ç‰¹çš„å›¾ï¼Œä»¥äº†è§£æ›´å¤šä¿¡æ¯ã€‚
- en: '[PRE5]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '[PRE6]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The `[Data](https://pytorch-geometric.readthedocs.io/en/latest/modules/data.html)`
    object is particularly interesting. Printing it offers a good summary of the graph
    we''re studying:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '`[Data](https://pytorch-geometric.readthedocs.io/en/latest/modules/data.html)`å¯¹è±¡ç‰¹åˆ«æœ‰è¶£ã€‚æ‰“å°å®ƒå¯ä»¥å¾ˆå¥½åœ°æ€»ç»“æˆ‘ä»¬æ­£åœ¨ç ”ç©¶çš„å›¾ï¼š'
- en: '`x=[34, 34]` is the **node feature matrix** with shape (number of nodes, number
    of features). In our case, it means that we have 34 nodes (our 34 members), each
    node being associated to a 34-dim feature vector.'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`x=[34, 34]`æ˜¯**èŠ‚ç‚¹ç‰¹å¾çŸ©é˜µ**ï¼Œå…¶å½¢çŠ¶ä¸ºï¼ˆèŠ‚ç‚¹æ•°ï¼Œç‰¹å¾æ•°ï¼‰ã€‚åœ¨æˆ‘ä»¬çš„ä¾‹å­ä¸­ï¼Œè¿™æ„å‘³ç€æˆ‘ä»¬æœ‰34ä¸ªèŠ‚ç‚¹ï¼ˆæˆ‘ä»¬çš„34ä¸ªæˆå‘˜ï¼‰ï¼Œæ¯ä¸ªèŠ‚ç‚¹éƒ½ä¸ä¸€ä¸ª34ç»´ç‰¹å¾å‘é‡ç›¸å…³è”ã€‚'
- en: '`edge_index=[2, 156]` represents the **graph connectivity** (how the nodes
    are connected) with shape (2, number of directed edges).'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`edge_index=[2, 156]`è¡¨ç¤º**å›¾çš„è¿é€šæ€§**ï¼ˆèŠ‚ç‚¹å¦‚ä½•è¿æ¥ï¼‰ï¼Œå…¶å½¢çŠ¶ä¸ºï¼ˆ2ï¼Œå®šå‘è¾¹çš„æ•°é‡ï¼‰ã€‚'
- en: '`y=[34]` is the **node ground-truth labels**. In this problem, every node is
    assigned to one class (group), so we have one value for each node.'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`y=[34]`æ˜¯**èŠ‚ç‚¹çœŸå®æ ‡ç­¾**ã€‚åœ¨è¿™ä¸ªé—®é¢˜ä¸­ï¼Œæ¯ä¸ªèŠ‚ç‚¹è¢«åˆ†é…åˆ°ä¸€ä¸ªç±»åˆ«ï¼ˆç»„ï¼‰ï¼Œå› æ­¤æˆ‘ä»¬å¯¹æ¯ä¸ªèŠ‚ç‚¹æœ‰ä¸€ä¸ªå€¼ã€‚'
- en: '`train_mask=[34]` is an optional attribute that tells which nodes should be
    used for training with a list of `True` or `False` statements.'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`train_mask=[34]`æ˜¯ä¸€ä¸ªå¯é€‰å±æ€§ï¼Œç”¨äºæŒ‡å®šå“ªäº›èŠ‚ç‚¹åº”ç”¨äºè®­ç»ƒï¼Œåˆ—è¡¨ä¸­åŒ…å«`True`æˆ–`False`ã€‚'
- en: Letâ€™s print each of these tensors to understand what they store. Letâ€™s start
    with the node features.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬æ‰“å°è¿™äº›å¼ é‡ä»¥äº†è§£å®ƒä»¬å­˜å‚¨äº†ä»€ä¹ˆã€‚æˆ‘ä»¬ä»èŠ‚ç‚¹ç‰¹å¾å¼€å§‹ã€‚
- en: '[PRE7]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '[PRE8]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '[PRE9]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Here, the node feature matrix `x` is an identity matrix: it **doesn''t contain
    any relevant information** about the nodes. It could contain information like
    age, skill level, etc. but this is not the case in this dataset. It means we''ll
    have to classify our nodes just by looking at their connections.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™é‡Œï¼ŒèŠ‚ç‚¹ç‰¹å¾çŸ©é˜µ`x`æ˜¯ä¸€ä¸ªå•ä½çŸ©é˜µï¼šå®ƒ**ä¸åŒ…å«ä»»ä½•ç›¸å…³ä¿¡æ¯**å…³äºèŠ‚ç‚¹ã€‚å®ƒæœ¬å¯ä»¥åŒ…å«è¯¸å¦‚å¹´é¾„ã€æŠ€èƒ½æ°´å¹³ç­‰ä¿¡æ¯ï¼Œä½†åœ¨è¿™ä¸ªæ•°æ®é›†ä¸­å¹¶éå¦‚æ­¤ã€‚è¿™æ„å‘³ç€æˆ‘ä»¬åªèƒ½é€šè¿‡æŸ¥çœ‹èŠ‚ç‚¹ä¹‹é—´çš„è¿æ¥æ¥å¯¹å®ƒä»¬è¿›è¡Œåˆ†ç±»ã€‚
- en: Now, letâ€™s print the edge index.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ï¼Œè®©æˆ‘ä»¬æ‰“å°è¾¹ç´¢å¼•ã€‚
- en: '[PRE10]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '[PRE11]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: In graph theory and network analysis, connectivity between nodes is stored using
    a variety of data structures. The `edge_index` is one such data structure, where
    the graph's connections are stored in **two lists** (156 directed edges, which
    equate to 78 bidirectional edges). The reason for these two lists is that one
    list stores the source nodes, while the second one identifies the destination
    nodes.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å›¾è®ºå’Œç½‘ç»œåˆ†æä¸­ï¼ŒèŠ‚ç‚¹ä¹‹é—´çš„è¿é€šæ€§é€šè¿‡å¤šç§æ•°æ®ç»“æ„è¿›è¡Œå­˜å‚¨ã€‚`edge_index`å°±æ˜¯è¿™ç§æ•°æ®ç»“æ„ä¹‹ä¸€ï¼Œå…¶ä¸­å›¾çš„è¿æ¥å­˜å‚¨åœ¨**ä¸¤ä¸ªåˆ—è¡¨**ä¸­ï¼ˆ156æ¡å®šå‘è¾¹ï¼Œç›¸å½“äº78æ¡åŒå‘è¾¹ï¼‰ã€‚è¿™ä¸¤ä¸ªåˆ—è¡¨çš„åŸå› åœ¨äºä¸€ä¸ªåˆ—è¡¨å­˜å‚¨æºèŠ‚ç‚¹ï¼Œè€Œç¬¬äºŒä¸ªåˆ—è¡¨æ ‡è¯†ç›®æ ‡èŠ‚ç‚¹ã€‚
- en: This method is known as a **coordinate list** (COO) format, which is essentially
    a means to efficiently store a [sparse matrix](https://en.wikipedia.org/wiki/Sparse_matrix#Storing_a_sparse_matrix).
    Sparse matrices are data structures that efficiently store matrices with a majority
    of zero elements. In the COO format, only non-zero elements are stored, saving
    memory and computational resources.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ç§æ–¹æ³•ç§°ä¸º**åæ ‡åˆ—è¡¨**ï¼ˆCOOï¼‰æ ¼å¼ï¼Œæœ¬è´¨ä¸Šæ˜¯ä¸€ç§é«˜æ•ˆå­˜å‚¨[ç¨€ç–çŸ©é˜µ](https://en.wikipedia.org/wiki/Sparse_matrix#Storing_a_sparse_matrix)çš„æ–¹å¼ã€‚ç¨€ç–çŸ©é˜µæ˜¯é«˜æ•ˆå­˜å‚¨å¤§éƒ¨åˆ†ä¸ºé›¶å…ƒç´ çš„çŸ©é˜µçš„æ•°æ®ç»“æ„ã€‚åœ¨COOæ ¼å¼ä¸­ï¼Œä»…å­˜å‚¨éé›¶å…ƒç´ ï¼Œä»è€ŒèŠ‚çœå†…å­˜å’Œè®¡ç®—èµ„æºã€‚
- en: Contrarily, a more intuitive and straightforward way to represent graph connectivity
    is through an **adjacency matrix** *A*. This is a square matrix where each element
    *A*áµ¢â±¼ *s*pecifies the presence or absence of an edge from node *i* to node *j*
    in the graph. In other words, a non-zero element *A*áµ¢â±¼ implies a connection from
    node *i* to node *j*, and a zero indicates no direct connection.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: ç›¸åï¼Œæ›´ç›´è§‚å’Œç®€æ´çš„è¡¨ç¤ºå›¾è¿é€šæ€§çš„æ–¹æ³•æ˜¯é€šè¿‡**é‚»æ¥çŸ©é˜µ** *A*ã€‚è¿™æ˜¯ä¸€ä¸ªæ–¹é˜µï¼Œå…¶ä¸­æ¯ä¸ªå…ƒç´ *A*áµ¢â±¼ *s*æŒ‡å®šå›¾ä¸­ä»èŠ‚ç‚¹*i*åˆ°èŠ‚ç‚¹*j*çš„è¾¹çš„å­˜åœ¨ä¸å¦ã€‚æ¢å¥è¯è¯´ï¼Œéé›¶å…ƒç´ *A*áµ¢â±¼
    è¡¨ç¤ºä»èŠ‚ç‚¹*i*åˆ°èŠ‚ç‚¹*j*çš„è¿æ¥ï¼Œè€Œé›¶è¡¨ç¤ºæ²¡æœ‰ç›´æ¥è¿æ¥ã€‚
- en: '![](../Images/7a85c8e90254bb8ed5f76a90cbc92443.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7a85c8e90254bb8ed5f76a90cbc92443.png)'
- en: Image by author
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: ä½œè€…æä¾›çš„å›¾ç‰‡
- en: An adjacency matrix, however, is not as space-efficient as the COO format for
    sparse matrices or graphs with fewer edges. However, for clarity and easy interpretation,
    the adjacency matrix remains a popular choice for representing graph connectivity.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶è€Œï¼Œé‚»æ¥çŸ©é˜µåœ¨ç¨€ç–çŸ©é˜µæˆ–è¾¹è¾ƒå°‘çš„å›¾ä¸­å¹¶ä¸åƒCOOæ ¼å¼é‚£æ ·èŠ‚çœç©ºé—´ã€‚ç„¶è€Œï¼Œä¸ºäº†æ¸…æ™°å’Œæ˜“äºè§£é‡Šï¼Œé‚»æ¥çŸ©é˜µä»ç„¶æ˜¯è¡¨ç¤ºå›¾è¿é€šæ€§çš„çƒ­é—¨é€‰æ‹©ã€‚
- en: The adjacency matrix can be inferred from the `edge_index` with a utility function
    `to_dense_adj()`.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: é‚»æ¥çŸ©é˜µå¯ä»¥é€šè¿‡`edge_index`å’Œä¸€ä¸ªå·¥å…·å‡½æ•°`to_dense_adj()`æ¥æ¨æ–­ã€‚
- en: '[PRE12]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '[PRE13]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '[PRE14]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: With graph data, it is relatively uncommon for nodes to be densely interconnected.
    As you can see, our adjacency matrix *A* is **sparse** (filled with zeros).
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºå›¾æ•°æ®ï¼ŒèŠ‚ç‚¹ä¹‹é—´å¯†é›†äº’è¿çš„æƒ…å†µç›¸å¯¹è¾ƒå°‘ã€‚æ­£å¦‚ä½ æ‰€è§ï¼Œæˆ‘ä»¬çš„é‚»æ¥çŸ©é˜µ*A*æ˜¯**ç¨€ç–çš„**ï¼ˆå¡«å……äº†é›¶ï¼‰ã€‚
- en: In many real-world graphs, most nodes are connected to only a few other nodes,
    resulting in a large number of zeros in the adjacency matrix. Storing so many
    zeros is not efficient at all, which is why the COO format is adopted by PyG.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è®¸å¤šç°å®ä¸–ç•Œçš„å›¾ä¸­ï¼Œå¤§å¤šæ•°èŠ‚ç‚¹åªä¸å°‘æ•°å…¶ä»–èŠ‚ç‚¹è¿æ¥ï¼Œå¯¼è‡´é‚»æ¥çŸ©é˜µä¸­æœ‰å¤§é‡é›¶ã€‚å­˜å‚¨è¿™ä¹ˆå¤šé›¶æ˜¯å®Œå…¨ä¸é«˜æ•ˆçš„ï¼Œè¿™å°±æ˜¯ä¸ºä»€ä¹ˆ PyG é‡‡ç”¨äº† COO æ ¼å¼ã€‚
- en: On the contrary, ground-truth labels are easy to understand.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: ç›¸åï¼ŒçœŸå®æ ‡ç­¾æ˜“äºç†è§£ã€‚
- en: '[PRE15]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '[PRE16]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Our node ground-truth labels stored in `y` simply encode the group number (0,
    1, 2, 3) for each node, which is why we have 34 values.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å­˜å‚¨åœ¨ `y` ä¸­çš„èŠ‚ç‚¹çœŸå®æ ‡ç­¾ä»…ä»…ç¼–ç äº†æ¯ä¸ªèŠ‚ç‚¹çš„ç»„å·ï¼ˆ0, 1, 2, 3ï¼‰ï¼Œè¿™å°±æ˜¯ä¸ºä»€ä¹ˆæˆ‘ä»¬æœ‰34ä¸ªå€¼ã€‚
- en: Finally, letâ€™s print the train mask.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åï¼Œè®©æˆ‘ä»¬æ‰“å°è®­ç»ƒæ©ç ã€‚
- en: '[PRE17]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '[PRE18]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: The train mask shows which nodes are supposed to be used for training with `True`
    statements. These nodes represent the training set, while the others can be considered
    as the test set. This division helps in model evaluation by providing unseen data
    for testing.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: è®­ç»ƒæ©ç æ˜¾ç¤ºäº†å“ªäº›èŠ‚ç‚¹åº”è¯¥ç”¨ `True` è¯­å¥è¿›è¡Œè®­ç»ƒã€‚è¿™äº›èŠ‚ç‚¹ä»£è¡¨è®­ç»ƒé›†ï¼Œè€Œå…¶ä»–èŠ‚ç‚¹å¯ä»¥è§†ä¸ºæµ‹è¯•é›†ã€‚è¿™ç§åˆ’åˆ†æœ‰åŠ©äºé€šè¿‡æä¾›æœªè§è¿‡çš„æ•°æ®æ¥è¿›è¡Œæ¨¡å‹è¯„ä¼°ã€‚
- en: 'But weâ€™re not done yet! The `[Data](https://pytorch-geometric.readthedocs.io/en/latest/modules/data.html)`
    object has a lot more to offer. It provides various utility functions that enable
    the investigation of several properties of the graph. For instance:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†æˆ‘ä»¬è¿˜æ²¡å®Œæˆï¼`[Data](https://pytorch-geometric.readthedocs.io/en/latest/modules/data.html)`
    å¯¹è±¡æä¾›äº†æ›´å¤šåŠŸèƒ½ã€‚å®ƒæä¾›äº†å„ç§å®ç”¨å‡½æ•°ï¼Œä½¿å¾—å¯ä»¥è°ƒæŸ¥å›¾çš„å¤šä¸ªå±æ€§ã€‚ä¾‹å¦‚ï¼š
- en: '`is_directed()` tells you if the graph is **directed**. A directed graph signifies
    that the adjacency matrix is not symmetric, i.e., the direction of edges matters
    in the connections between nodes.'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`is_directed()` å‘Šè¯‰ä½ å›¾æ˜¯å¦**æœ‰å‘**ã€‚æœ‰å‘å›¾æ„å‘³ç€é‚»æ¥çŸ©é˜µä¸æ˜¯å¯¹ç§°çš„ï¼Œå³è¾¹çš„æ–¹å‘åœ¨èŠ‚ç‚¹é—´çš„è¿æ¥ä¸­æ˜¯é‡è¦çš„ã€‚'
- en: '`isolated_nodes()` checks if some nodes are **not connected** to the rest of
    the graph. These nodes are likely to pose challenges in tasks like classification
    due to their lack of connections.'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`isolated_nodes()` æ£€æŸ¥æ˜¯å¦æœ‰ä¸€äº›èŠ‚ç‚¹**æ²¡æœ‰è¿æ¥**åˆ°å›¾çš„å…¶ä½™éƒ¨åˆ†ã€‚è¿™äº›èŠ‚ç‚¹å¯èƒ½åœ¨åˆ†ç±»ç­‰ä»»åŠ¡ä¸­å¸¦æ¥æŒ‘æˆ˜ï¼Œå› ä¸ºå®ƒä»¬ç¼ºä¹è¿æ¥ã€‚'
- en: '`has_self_loops()` indicates if at least one node is **connected to itself**.
    This is distinct from the concept of [loops](https://en.wikipedia.org/wiki/Loop_(graph_theory)):
    a loop implies a path that starts and ends at the same node, traversing other
    nodes in between.'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`has_self_loops()` è¡¨ç¤ºæ˜¯å¦è‡³å°‘æœ‰ä¸€ä¸ªèŠ‚ç‚¹**è‡ªæˆ‘è¿æ¥**ã€‚è¿™ä¸[ç¯](https://en.wikipedia.org/wiki/Loop_(graph_theory))çš„æ¦‚å¿µä¸åŒï¼šç¯æ„å‘³ç€ä¸€æ¡è·¯å¾„å¼€å§‹å’Œç»“æŸäºåŒä¸€ä¸ªèŠ‚ç‚¹ï¼Œåœ¨æ­¤è¿‡ç¨‹ä¸­éå†å…¶ä»–èŠ‚ç‚¹ã€‚'
- en: In the context of the Zacharyâ€™s karate club dataset, all these properties return
    `False`. This implies that the graph is not directed, does not have any isolated
    nodes, and none of its nodes are connected to themselves.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ‰å¡é‡Œæ­¦æœ¯ä¿±ä¹éƒ¨æ•°æ®é›†ä¸­ï¼Œæ‰€æœ‰è¿™äº›å±æ€§è¿”å› `False`ã€‚è¿™æ„å‘³ç€å›¾æ˜¯æ— å‘çš„ï¼Œæ²¡æœ‰å­¤ç«‹èŠ‚ç‚¹ï¼Œå¹¶ä¸”æ²¡æœ‰èŠ‚ç‚¹ä¸è‡ªèº«ç›¸è¿ã€‚
- en: '[PRE19]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '[PRE20]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Finally, we can convert a graph from PyTorch Geometric to the popular graph
    library [NetworkX](https://networkx.org/) using `[to_networkx](https://pytorch-geometric.readthedocs.io/en/latest/modules/utils.html?highlight=to_networkx#torch_geometric.utils.to_networkx)`.
    This is particularly useful to visualize a small graph with `networkx` and `matplotlib`.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ `[to_networkx](https://pytorch-geometric.readthedocs.io/en/latest/modules/utils.html?highlight=to_networkx#torch_geometric.utils.to_networkx)`
    å°† PyTorch Geometric å›¾è½¬æ¢ä¸ºæµè¡Œçš„å›¾åº“ [NetworkX](https://networkx.org/)ã€‚è¿™å¯¹äºä½¿ç”¨ `networkx`
    å’Œ `matplotlib` å¯è§†åŒ–å°å›¾ç‰¹åˆ«æœ‰ç”¨ã€‚
- en: Letâ€™s plot our dataset with a different color for each group.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬ä¸ºæ¯ä¸ªç»„ç»˜åˆ¶ä¸åŒé¢œè‰²çš„æ•°æ®é›†ã€‚
- en: '[PRE21]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '[PRE22]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '![](../Images/571cb9949785200ba0307b6172e0f3fb.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/571cb9949785200ba0307b6172e0f3fb.png)'
- en: This plot of Zacharyâ€™s karate club displays our 34 nodes, 78 (bidirectional)
    edges, and 4 labels with 4 different colors. Now that weâ€™ve seen the essentials
    of loading and handling a dataset with PyTorch Geometric, we can introduce the
    **Graph Convolutional Network** architecture.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªæ‰å¡é‡Œæ­¦æœ¯ä¿±ä¹éƒ¨çš„å›¾æ˜¾ç¤ºäº†æˆ‘ä»¬çš„34ä¸ªèŠ‚ç‚¹ã€78æ¡ï¼ˆåŒå‘ï¼‰è¾¹å’Œ4ä¸ªæ ‡ç­¾åŠ4ç§ä¸åŒé¢œè‰²ã€‚ç°åœ¨æˆ‘ä»¬å·²ç»äº†è§£äº†ä½¿ç”¨ PyTorch Geometric åŠ è½½å’Œå¤„ç†æ•°æ®é›†çš„åŸºæœ¬å†…å®¹ï¼Œæˆ‘ä»¬å¯ä»¥ä»‹ç»**å›¾å·ç§¯ç½‘ç»œ**æ¶æ„ã€‚
- en: âœ‰ï¸ II. Graph Convolutional Network
  id: totrans-78
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: âœ‰ï¸ II. å›¾å·ç§¯ç½‘ç»œ
- en: This section aims to introduce and build the graph convolutional layer from
    the ground up.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬èŠ‚æ—¨åœ¨ä»å¤´å¼€å§‹ä»‹ç»å’Œæ„å»ºå›¾å·ç§¯å±‚ã€‚
- en: 'In traditional neural networks, linear layers apply a **linear transformation**
    to the incoming data. This transformation converts input features *x* into hidden
    vectors *h* through the use of a weight matrix ğ–. Ignoring biases for the time
    being, this can be expressed as:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä¼ ç»Ÿçš„ç¥ç»ç½‘ç»œä¸­ï¼Œçº¿æ€§å±‚å¯¹ä¼ å…¥çš„æ•°æ®åº”ç”¨**çº¿æ€§å˜æ¢**ã€‚è¿™ç§å˜æ¢é€šè¿‡ä½¿ç”¨æƒé‡çŸ©é˜µ ğ– å°†è¾“å…¥ç‰¹å¾ *x* è½¬æ¢ä¸ºéšè—å‘é‡ *h*ã€‚æš‚æ—¶å¿½ç•¥åå·®ï¼Œè¿™å¯ä»¥è¡¨ç¤ºä¸ºï¼š
- en: '![](../Images/d99b824e5c2dca4657e4b2eb18b4d9e7.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d99b824e5c2dca4657e4b2eb18b4d9e7.png)'
- en: With graph data, an additional layer of complexity is added through the **connections
    between nodes**. These connections matter because, typically, in networks, itâ€™s
    assumed that similar nodes are more likely to be linked to each other than dissimilar
    ones, a phenomenon known as [network homophily](https://en.wikipedia.org/wiki/Network_homophily).
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å›¾æ•°æ®ä¸­ï¼Œé€šè¿‡**èŠ‚ç‚¹ä¹‹é—´çš„è¿æ¥**å¢åŠ äº†é¢å¤–çš„å¤æ‚æ€§ã€‚è¿™äº›è¿æ¥å¾ˆé‡è¦ï¼Œå› ä¸ºåœ¨ç½‘ç»œä¸­ï¼Œé€šå¸¸å‡è®¾ç›¸ä¼¼çš„èŠ‚ç‚¹æ¯”ä¸ç›¸ä¼¼çš„èŠ‚ç‚¹æ›´å¯èƒ½äº’ç›¸é“¾æ¥ï¼Œè¿™ç§ç°è±¡è¢«ç§°ä¸º[ç½‘ç»œåŒè´¨æ€§](https://en.wikipedia.org/wiki/Network_homophily)ã€‚
- en: We can enrich our **node representation** by merging its features with those
    of its neighbors. This operation is called convolution, or neighborhood aggregation.
    Letâ€™s represent the neighborhood of node *i* including itself as *Ã‘*.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥é€šè¿‡å°†èŠ‚ç‚¹çš„ç‰¹å¾ä¸é‚»å±…çš„ç‰¹å¾åˆå¹¶æ¥ä¸°å¯Œæˆ‘ä»¬çš„**èŠ‚ç‚¹è¡¨ç¤º**ã€‚è¿™ä¸ªæ“ä½œç§°ä¸ºå·ç§¯æˆ–é‚»åŸŸèšåˆã€‚è®©æˆ‘ä»¬å°†èŠ‚ç‚¹ *i* åŠå…¶é‚»åŸŸè¡¨ç¤ºä¸º *Ã‘*ã€‚
- en: '![](../Images/c368d96059e7977d7ff141a4b92b8bd6.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c368d96059e7977d7ff141a4b92b8bd6.png)'
- en: 'Unlike filters in Convolutional Neural Networks (CNNs), our weight matrix ğ–
    is unique and shared among every node. But there is another issue: nodes do not
    have a **fixed number of neighbors** like pixels do.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNsï¼‰ä¸­çš„æ»¤æ³¢å™¨ä¸åŒï¼Œæˆ‘ä»¬çš„æƒé‡çŸ©é˜µ ğ– æ˜¯å”¯ä¸€çš„ï¼Œå¹¶ä¸”åœ¨æ¯ä¸ªèŠ‚ç‚¹ä¹‹é—´å…±äº«ã€‚ä½†è¿˜æœ‰å¦ä¸€ä¸ªé—®é¢˜ï¼šèŠ‚ç‚¹æ²¡æœ‰åƒåƒç´ é‚£æ ·çš„**å›ºå®šé‚»å±…æ•°é‡**ã€‚
- en: How do we address cases where one node has only one neighbor, and another has
    500? If we simply sum the feature vectors, the resulting embedding *h* would be
    much larger for the node with 500 neighbors. To ensure a **similar range** of
    values for all nodes and comparability between them, we can normalize the result
    based on the **degree** of nodes, where degree refers to the number of connections
    a node has.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¦‚ä½•å¤„ç†ä¸€ä¸ªèŠ‚ç‚¹åªæœ‰ä¸€ä¸ªé‚»å±…ï¼Œè€Œå¦ä¸€ä¸ªèŠ‚ç‚¹æœ‰500ä¸ªé‚»å±…çš„æƒ…å†µï¼Ÿå¦‚æœæˆ‘ä»¬ç®€å•åœ°å°†ç‰¹å¾å‘é‡ç›¸åŠ ï¼Œé‚£ä¹ˆå¯¹äºæ‹¥æœ‰500ä¸ªé‚»å±…çš„èŠ‚ç‚¹ï¼Œå¾—åˆ°çš„åµŒå…¥ *h* å°†ä¼šå¤§å¾—å¤šã€‚ä¸ºäº†ç¡®ä¿æ‰€æœ‰èŠ‚ç‚¹çš„å€¼å…·æœ‰**ç›¸ä¼¼çš„èŒƒå›´**å¹¶ä¾¿äºæ¯”è¾ƒï¼Œæˆ‘ä»¬å¯ä»¥æ ¹æ®èŠ‚ç‚¹çš„**åº¦**æ¥å½’ä¸€åŒ–ç»“æœï¼Œå…¶ä¸­åº¦æ˜¯æŒ‡ä¸€ä¸ªèŠ‚ç‚¹çš„è¿æ¥æ•°é‡ã€‚
- en: '![](../Images/99665492296bf842d6b89bd2c19fb899.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/99665492296bf842d6b89bd2c19fb899.png)'
- en: Weâ€™re almost there! Introduced by Kipf et al. (2016), the [graph convolutional
    layer](https://arxiv.org/abs/1609.02907) has one final improvement.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¿«åˆ°äº†ï¼ç”±Kipfç­‰äººï¼ˆ2016ï¼‰ä»‹ç»çš„[å›¾å·ç§¯å±‚](https://arxiv.org/abs/1609.02907)è¿˜æœ‰ä¸€ä¸ªæœ€ç»ˆçš„æ”¹è¿›ã€‚
- en: 'The authors observed that features from nodes with numerous neighbors propagate
    much more easily than those from more isolated nodes. To offset this effect, they
    suggested assigning **bigger weights** to features from nodes with fewer neighbors,
    thus balancing the influence across all nodes. This operation is written as:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: ä½œè€…è§‚å¯Ÿåˆ°ï¼Œå…·æœ‰å¤§é‡é‚»å±…çš„èŠ‚ç‚¹çš„ç‰¹å¾æ¯”é‚£äº›è¾ƒå­¤ç«‹èŠ‚ç‚¹çš„ç‰¹å¾ä¼ æ’­å¾—æ›´å®¹æ˜“ã€‚ä¸ºäº†æŠµæ¶ˆè¿™ç§æ•ˆåº”ï¼Œä»–ä»¬å»ºè®®ä¸ºé‚»å±…è¾ƒå°‘çš„èŠ‚ç‚¹çš„ç‰¹å¾åˆ†é…**æ›´å¤§çš„æƒé‡**ï¼Œä»è€Œå¹³è¡¡æ‰€æœ‰èŠ‚ç‚¹çš„å½±å“ã€‚è¿™ä¸ªæ“ä½œå¯ä»¥è¡¨ç¤ºä¸ºï¼š
- en: '![](../Images/04becb820335cbd89cebb547724d12ea.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/04becb820335cbd89cebb547724d12ea.png)'
- en: Note that when *i* and *j* have the same number of neighbors, it is equivalent
    to our own layer. Now, letâ€™s see how to implement it in Python with PyTorch Geometric.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: æ³¨æ„ï¼Œå½“ *i* å’Œ *j* æ‹¥æœ‰ç›¸åŒæ•°é‡çš„é‚»å±…æ—¶ï¼Œè¿™ç­‰åŒäºæˆ‘ä»¬è‡ªå·±å®šä¹‰çš„å±‚ã€‚ç°åœ¨ï¼Œè®©æˆ‘ä»¬çœ‹çœ‹å¦‚ä½•åœ¨Pythonä¸­ä½¿ç”¨PyTorch Geometricå®ç°å®ƒã€‚
- en: ğŸ§  III. Implementing a GCN
  id: totrans-92
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ğŸ§  III. å®ç°ä¸€ä¸ªGCN
- en: PyTorch Geometric provides the `GCNConv` function, which directly implements
    the graph convolutional layer.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch Geometricæä¾›äº†`GCNConv`å‡½æ•°ï¼Œè¯¥å‡½æ•°ç›´æ¥å®ç°äº†å›¾å·ç§¯å±‚ã€‚
- en: In this example, weâ€™ll create a basic Graph Convolutional Network with a single
    GCN layer, a ReLU activation function, and a linear output layer. This output
    layer will yield **four values** corresponding to our four categories, with the
    highest value determining the class of each node.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ä¸ªç¤ºä¾‹ä¸­ï¼Œæˆ‘ä»¬å°†åˆ›å»ºä¸€ä¸ªåŸºæœ¬çš„å›¾å·ç§¯ç½‘ç»œï¼ŒåŒ…æ‹¬ä¸€ä¸ªGCNå±‚ã€ä¸€ä¸ªReLUæ¿€æ´»å‡½æ•°å’Œä¸€ä¸ªçº¿æ€§è¾“å‡ºå±‚ã€‚è¿™ä¸ªè¾“å‡ºå±‚å°†äº§ç”Ÿ**å››ä¸ªå€¼**ï¼Œå¯¹åº”æˆ‘ä»¬çš„å››ä¸ªç±»åˆ«ï¼Œæœ€é«˜å€¼å°†å†³å®šæ¯ä¸ªèŠ‚ç‚¹çš„ç±»åˆ«ã€‚
- en: In the following code block, we define the GCN layer with a 3-dimensional hidden
    layer.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä»¥ä¸‹ä»£ç å—ä¸­ï¼Œæˆ‘ä»¬å®šä¹‰äº†ä¸€ä¸ªå…·æœ‰3ç»´éšè—å±‚çš„GCNå±‚ã€‚
- en: '[PRE23]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '[PRE24]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '[PRE25]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: If we added a second GCN layer, our model would not only aggregate feature vectors
    from the neighbors of each node, but also from the neighbors of these neighbors.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæˆ‘ä»¬æ·»åŠ äº†ç¬¬äºŒä¸ªGCNå±‚ï¼Œæˆ‘ä»¬çš„æ¨¡å‹å°†ä¸ä»…ä»…ä»æ¯ä¸ªèŠ‚ç‚¹çš„é‚»å±…å¤„èšåˆç‰¹å¾å‘é‡ï¼Œè¿˜ä¼šä»è¿™äº›é‚»å±…çš„é‚»å±…å¤„èšåˆç‰¹å¾å‘é‡ã€‚
- en: 'We can **stack several graph layers** to aggregate more and more distant values,
    but thereâ€™s a catch: if we add too many layers, the aggregation becomes so intense
    that all the embeddings end up looking the same. This phenomenon is called **over-smoothing**
    and can be a real problem when you have too many layers.'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥**å †å å¤šä¸ªå›¾å±‚**ä»¥èšåˆæ›´å¤šçš„è¿œç¦»å€¼ï¼Œä½†æœ‰ä¸€ä¸ªé—®é¢˜ï¼šå¦‚æœæˆ‘ä»¬æ·»åŠ å¤ªå¤šå›¾å±‚ï¼Œèšåˆå˜å¾—å¦‚æ­¤å¼ºçƒˆï¼Œä»¥è‡³äºæ‰€æœ‰åµŒå…¥æœ€ç»ˆçœ‹èµ·æ¥éƒ½ä¸€æ ·ã€‚è¿™ç§ç°è±¡è¢«ç§°ä¸º**è¿‡åº¦å¹³æ»‘**ï¼Œå½“å›¾å±‚è¿‡å¤šæ—¶ï¼Œå¯èƒ½ä¼šæˆä¸ºä¸€ä¸ªå®é™…é—®é¢˜ã€‚
- en: Now that weâ€™ve defined our GNN, letâ€™s write a simple training loop with PyTorch.
    I chose a regular cross-entropy loss since itâ€™s a multi-class classification task,
    with Adam as optimizer. In this article, we wonâ€™t implement a train/test split
    to keep things simple and focus on how GNNs learn instead.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬å·²ç»å®šä¹‰äº† GNNï¼Œè®©æˆ‘ä»¬ç”¨ PyTorch ç¼–å†™ä¸€ä¸ªç®€å•çš„è®­ç»ƒå¾ªç¯ã€‚æˆ‘é€‰æ‹©äº†å¸¸è§„çš„äº¤å‰ç†µæŸå¤±ï¼Œå› ä¸ºè¿™æ˜¯ä¸€ä¸ªå¤šç±»åˆ†ç±»ä»»åŠ¡ï¼Œä¼˜åŒ–å™¨ä½¿ç”¨ Adamã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä¸ä¼šå®ç°è®­ç»ƒ/æµ‹è¯•æ‹†åˆ†ï¼Œä»¥ä¿æŒç®€å•ï¼Œä¸“æ³¨äº
    GNN å¦‚ä½•å­¦ä¹ ã€‚
- en: 'The training loop is standard: we try to predict the correct labels, and we
    compare the GCNâ€™s results to the values stored in `data.y`. The error is calculated
    by the cross-entropy loss and backpropagated with Adam to fine-tune our GNN''s
    weights and biases. Finally, we print metrics every 10 epochs.'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: è®­ç»ƒå¾ªç¯æ˜¯æ ‡å‡†çš„ï¼šæˆ‘ä»¬å°è¯•é¢„æµ‹æ­£ç¡®çš„æ ‡ç­¾ï¼Œå¹¶å°† GCN çš„ç»“æœä¸ `data.y` ä¸­å­˜å‚¨çš„å€¼è¿›è¡Œæ¯”è¾ƒã€‚é€šè¿‡äº¤å‰ç†µæŸå¤±è®¡ç®—é”™è¯¯ï¼Œå¹¶ä½¿ç”¨ Adam è¿›è¡Œåå‘ä¼ æ’­ï¼Œä»¥å¾®è°ƒ
    GNN çš„æƒé‡å’Œåå·®ã€‚æœ€åï¼Œæˆ‘ä»¬æ¯ 10 ä¸ª epochs æ‰“å°ä¸€æ¬¡æŒ‡æ ‡ã€‚
- en: '[PRE26]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '[PRE27]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: '[PRE28]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Great! Without much surprise, we reach 100% accuracy on the training set (full
    dataset). It means that our model learned to correctly assign every member of
    the karate club to its correct group.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: å¤ªå¥½äº†ï¼æ¯«ä¸å¥‡æ€ªï¼Œæˆ‘ä»¬åœ¨è®­ç»ƒé›†ï¼ˆå®Œæ•´æ•°æ®é›†ï¼‰ä¸Šè¾¾åˆ°äº† 100% çš„å‡†ç¡®ç‡ã€‚è¿™æ„å‘³ç€æˆ‘ä»¬çš„æ¨¡å‹å­¦ä¼šäº†æ­£ç¡®åœ°å°†æ¯ä¸ªç©ºæ‰‹é“ä¿±ä¹éƒ¨çš„æˆå‘˜åˆ†é…åˆ°æ­£ç¡®çš„ç»„ã€‚
- en: We can produce a neat visualization by animating the graph and see the evolution
    of the GNNâ€™s predictions during the training process.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥é€šè¿‡åŠ¨ç”»åŒ–å›¾å½¢æ¥ç”Ÿæˆä¸€ä¸ªæ•´æ´çš„å¯è§†åŒ–æ•ˆæœï¼Œå¹¶è§‚å¯Ÿ GNN åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­é¢„æµ‹çš„æ¼”å˜ã€‚
- en: '[PRE29]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: '[PRE30]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: '![](../Images/04e6b675ab152228d57739474387a402.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/04e6b675ab152228d57739474387a402.png)'
- en: The first predictions are random, but the GCN perfectly labels every node after
    a while. Indeed, the final graph is the same as the one we plotted at the end
    of the first section. But what does the GCN really learn?
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åˆçš„é¢„æµ‹æ˜¯éšæœºçš„ï¼Œä½†ç»è¿‡ä¸€æ®µæ—¶é—´ï¼ŒGCN èƒ½å¤Ÿå®Œç¾åœ°æ ‡è®°æ¯ä¸ªèŠ‚ç‚¹ã€‚å®é™…ä¸Šï¼Œæœ€ç»ˆçš„å›¾å½¢ä¸æˆ‘ä»¬åœ¨ç¬¬ä¸€éƒ¨åˆ†æœ«å°¾ç»˜åˆ¶çš„å›¾å½¢ç›¸åŒã€‚ä½† GCN åˆ°åº•å­¦åˆ°äº†ä»€ä¹ˆï¼Ÿ
- en: By aggregating features from neighboring nodes, the GNN learns a vector representation
    (or **embedding**) of every node in the network. In our model, the final layer
    just learns how to use these representations to produce the best classifications.
    However, embeddings are the real products of GNNs.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: é€šè¿‡èšåˆé‚»è¿‘èŠ‚ç‚¹çš„ç‰¹å¾ï¼ŒGNN å­¦ä¹ äº†ç½‘ç»œä¸­æ¯ä¸ªèŠ‚ç‚¹çš„å‘é‡è¡¨ç¤ºï¼ˆæˆ–**åµŒå…¥**ï¼‰ã€‚åœ¨æˆ‘ä»¬çš„æ¨¡å‹ä¸­ï¼Œæœ€ç»ˆå±‚ä»…å­¦ä¹ å¦‚ä½•ä½¿ç”¨è¿™äº›è¡¨ç¤ºæ¥äº§ç”Ÿæœ€ä½³åˆ†ç±»ã€‚ç„¶è€Œï¼ŒåµŒå…¥æ‰æ˜¯çœŸæ­£çš„
    GNN äº§ç‰©ã€‚
- en: Letâ€™s print the embeddings learned by our model.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬æ‰“å°å‡ºæ¨¡å‹å­¦åˆ°çš„åµŒå…¥ã€‚
- en: '[PRE31]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: '[PRE32]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: As you can see, embeddings do not need to have the same dimensions as feature
    vectors. Here, I chose to reduce the number of dimensions from 34 (`dataset.num_features`)
    to three to get a nice visualization in 3D.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚ä½ æ‰€è§ï¼ŒåµŒå…¥ä¸éœ€è¦å…·æœ‰ä¸ç‰¹å¾å‘é‡ç›¸åŒçš„ç»´åº¦ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘é€‰æ‹©å°†ç»´åº¦ä» 34 (`dataset.num_features`) é™åˆ°ä¸‰ç»´ï¼Œä»¥è·å¾—æ›´å¥½çš„ 3D
    å¯è§†åŒ–æ•ˆæœã€‚
- en: Letâ€™s plot these embeddings before any training happens, at epoch 0.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬åœ¨è®­ç»ƒå¼€å§‹ä¹‹å‰ï¼Œå³ç¬¬ 0 è½®ï¼Œç»˜åˆ¶è¿™äº›åµŒå…¥ã€‚
- en: '[PRE33]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: '[PRE34]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: '![](../Images/a961a759d54dbe8911d59fb3ab66f824.png)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a961a759d54dbe8911d59fb3ab66f824.png)'
- en: We see every node from Zacharyâ€™s karate club with their true labels (and not
    the modelâ€™s predictions). For now, theyâ€™re all over the place since the GNN is
    not trained yet. But if we plot these embeddings at each step of the training
    loop, weâ€™d be able to visualize what the GNN truly learns.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬çœ‹åˆ° Zachary ç©ºæ‰‹é“ä¿±ä¹éƒ¨ä¸­çš„æ¯ä¸ªèŠ‚ç‚¹åŠå…¶çœŸå®æ ‡ç­¾ï¼ˆè€Œä¸æ˜¯æ¨¡å‹çš„é¢„æµ‹ï¼‰ã€‚ç›®å‰ï¼Œå®ƒä»¬è¿˜å¾ˆåˆ†æ•£ï¼Œå› ä¸º GNN å°šæœªè®­ç»ƒå®Œæˆã€‚ä½†å¦‚æœæˆ‘ä»¬åœ¨è®­ç»ƒå¾ªç¯çš„æ¯ä¸€æ­¥ç»˜åˆ¶è¿™äº›åµŒå…¥ï¼Œæˆ‘ä»¬å°†èƒ½å¤Ÿå¯è§†åŒ–
    GNN å®é™…ä¸Šå­¦åˆ°äº†ä»€ä¹ˆã€‚
- en: Letâ€™s see how they evolve over time, as the GCN gets better and better at classifying
    nodes.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬çœ‹çœ‹å®ƒä»¬éšç€æ—¶é—´çš„æ¨ç§»å¦‚ä½•æ¼”å˜ï¼Œéšç€ GCN åœ¨åˆ†ç±»èŠ‚ç‚¹æ–¹é¢å˜å¾—è¶Šæ¥è¶Šå¥½ã€‚
- en: '[PRE35]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: '[PRE36]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: '![](../Images/b4af8ca00284bd6d1418674e7584cebe.png)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b4af8ca00284bd6d1418674e7584cebe.png)'
- en: Our Graph Convolutional Network (GCN) has effectively learned embeddings that
    group similar nodes into **distinct clusters**. This enables the final linear
    layer to distinguish them into separate classes with ease.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬çš„å›¾å·ç§¯ç½‘ç»œï¼ˆGCNï¼‰æœ‰æ•ˆåœ°å­¦ä¹ äº†å°†ç›¸ä¼¼èŠ‚ç‚¹åˆ†ç»„åˆ°**ä¸åŒçš„ç°‡**ä¸­çš„åµŒå…¥ã€‚è¿™ä½¿å¾—æœ€ç»ˆçš„çº¿æ€§å±‚èƒ½å¤Ÿè½»æ¾åœ°åŒºåˆ†å®ƒä»¬ä¸ºä¸åŒçš„ç±»åˆ«ã€‚
- en: 'Embeddings are not unique to GNNs: they can be found everywhere in deep learning.
    They donâ€™t have to be 3D either: actually, they rarely are. For instance, language
    models like [BERT](https://arxiv.org/abs/1810.04805) produce embeddings with 768
    or even 1024 dimensions.'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: åµŒå…¥å¹¶éGNNç‰¹æœ‰ï¼šå®ƒä»¬åœ¨æ·±åº¦å­¦ä¹ ä¸­æ— å¤„ä¸åœ¨ã€‚å®ƒä»¬ä¹Ÿä¸ä¸€å®šæ˜¯ä¸‰ç»´çš„ï¼šå®é™…ä¸Šï¼Œå®ƒä»¬å¾ˆå°‘æ˜¯ä¸‰ç»´çš„ã€‚ä¾‹å¦‚ï¼Œåƒ[BERT](https://arxiv.org/abs/1810.04805)è¿™æ ·çš„è¯­è¨€æ¨¡å‹ç”Ÿæˆçš„åµŒå…¥ç»´åº¦é€šå¸¸æ˜¯768ç”šè‡³1024ã€‚
- en: Additional dimensions store more information about nodes, text, images, etc.
    but they also create bigger models that are more difficult to train. This is why
    keeping low-dimensional embeddings as long as possible is advantageous.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: é¢å¤–çš„ç»´åº¦å­˜å‚¨äº†å…³äºèŠ‚ç‚¹ã€æ–‡æœ¬ã€å›¾åƒç­‰æ›´å¤šçš„ä¿¡æ¯ï¼Œä½†å®ƒä»¬ä¹Ÿä¼šåˆ›å»ºæ›´å¤§çš„æ¨¡å‹ï¼Œè¿™äº›æ¨¡å‹æ›´éš¾ä»¥è®­ç»ƒã€‚è¿™å°±æ˜¯ä¸ºä»€ä¹ˆå°½å¯èƒ½ä¿æŒä½ç»´åµŒå…¥æ˜¯æœ‰åˆ©çš„åŸå› ã€‚
- en: Conclusion
  id: totrans-129
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç»“è®º
- en: Graph Convolutional Networks are an incredibly versatile architecture that can
    be applied in **many contexts**. In this article, we familiarized ourselves with
    the PyTorch Geometric library and objects like `Datasets` and `Data`. Then, we
    successfully reconstructed a graph convolutional layer from the ground up. Next,
    we put theory into practice by implementing a GCN, which gave us an understanding
    of practical aspects and how individual components interact. Finally, we visualized
    the training process and obtained a clear perspective of what it involves for
    such a network.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾å·ç§¯ç½‘ç»œæ˜¯ä¸€ç§éå¸¸å¤šåŠŸèƒ½çš„æ¶æ„ï¼Œå¯ä»¥åº”ç”¨äº**è®¸å¤šèƒŒæ™¯**ã€‚åœ¨è¿™ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘ä»¬ç†Ÿæ‚‰äº†PyTorch Geometricåº“ä»¥åŠåƒ`Datasets`å’Œ`Data`è¿™æ ·çš„å¯¹è±¡ã€‚ç„¶åï¼Œæˆ‘ä»¬æˆåŠŸåœ°ä»å¤´å¼€å§‹é‡å»ºäº†ä¸€ä¸ªå›¾å·ç§¯å±‚ã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬é€šè¿‡å®ç°ä¸€ä¸ªGCNå°†ç†è®ºä»˜è¯¸å®è·µï¼Œè¿™ä½¿æˆ‘ä»¬ç†è§£äº†å®é™…çš„æ–¹é¢ä»¥åŠå„ä¸ªç»„ä»¶å¦‚ä½•ç›¸äº’ä½œç”¨ã€‚æœ€åï¼Œæˆ‘ä»¬å¯è§†åŒ–äº†è®­ç»ƒè¿‡ç¨‹ï¼Œå¹¶æ¸…æ¥šåœ°äº†è§£äº†è¿™ç§ç½‘ç»œæ‰€æ¶‰åŠçš„å†…å®¹ã€‚
- en: 'Zacharyâ€™s karate club is a simplistic dataset, but it is good enough to understand
    the most important concepts in graph data and GNNs. Although we only talked about
    node classification in this article, there are other tasks GNNs can accomplish:
    **link prediction** (e.g., to recommend a friend), **graph classification** (e.g.,
    to label molecules), **graph generation** (e.g., to create new molecules), and
    so on.'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: Zacharyçš„ç©ºæ‰‹é“ä¿±ä¹éƒ¨æ˜¯ä¸€ä¸ªç®€å•çš„æ•°æ®é›†ï¼Œä½†è¶³å¤Ÿç”¨æ¥ç†è§£å›¾æ•°æ®å’ŒGNNä¸­çš„æœ€é‡è¦æ¦‚å¿µã€‚å°½ç®¡æˆ‘ä»¬åœ¨è¿™ç¯‡æ–‡ç« ä¸­ä»…è®¨è®ºäº†èŠ‚ç‚¹åˆ†ç±»ï¼Œä½†GNNè¿˜å¯ä»¥å®Œæˆå…¶ä»–ä»»åŠ¡ï¼š**é“¾æ¥é¢„æµ‹**ï¼ˆä¾‹å¦‚ï¼Œæ¨èæœ‹å‹ï¼‰ã€**å›¾åˆ†ç±»**ï¼ˆä¾‹å¦‚ï¼Œæ ‡è®°åˆ†å­ï¼‰ã€**å›¾ç”Ÿæˆ**ï¼ˆä¾‹å¦‚ï¼Œåˆ›å»ºæ–°åˆ†å­ï¼‰ç­‰ã€‚
- en: Beyond GCN, numerous GNN layers and architectures have been proposed by researchers.
    In the next article, weâ€™ll introduce the [Graph Attention Network](https://mlabonne.github.io/blog/gat/)
    (GAT) architecture, which dynamically computes the GCNâ€™s normalization factor
    and the importance of each connection with an attention mechanism.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: é™¤äº†GCNä¹‹å¤–ï¼Œç ”ç©¶äººå‘˜è¿˜æå‡ºäº†è®¸å¤šGNNå±‚å’Œæ¶æ„ã€‚åœ¨ä¸‹ä¸€ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘ä»¬å°†ä»‹ç»[å›¾æ³¨æ„åŠ›ç½‘ç»œ](https://mlabonne.github.io/blog/gat/)ï¼ˆGATï¼‰æ¶æ„ï¼Œå®ƒé€šè¿‡æ³¨æ„æœºåˆ¶åŠ¨æ€è®¡ç®—GCNçš„å½’ä¸€åŒ–å› å­å’Œæ¯ä¸ªè¿æ¥çš„é‡è¦æ€§ã€‚
- en: If you want to know more about graph neural networks, dive deeper into the world
    of GNNs with my book, [Hands-On Graph Neural Networks](https://mlabonne.github.io/blog/book.html).
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœä½ æƒ³äº†è§£æ›´å¤šå…³äºå›¾ç¥ç»ç½‘ç»œçš„ä¿¡æ¯ï¼Œå¯ä»¥é€šè¿‡æˆ‘çš„ä¹¦ç±[ã€ŠåŠ¨æ‰‹å®è·µå›¾ç¥ç»ç½‘ç»œã€‹](https://mlabonne.github.io/blog/book.html)æ·±å…¥æ¢ç´¢GNNçš„ä¸–ç•Œã€‚
- en: Next article
  id: totrans-134
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ä¸‹ä¸€ç¯‡æ–‡ç« 
- en: '[](/graph-attention-networks-in-python-975736ac5c0c?source=post_page-----24b3f60d6c95--------------------------------)
    [## Chapter 2: Graph Attention Networks: Self-Attention Explained'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/graph-attention-networks-in-python-975736ac5c0c?source=post_page-----24b3f60d6c95--------------------------------)
    [## ç¬¬2ç« ï¼šå›¾æ³¨æ„åŠ›ç½‘ç»œï¼šè‡ªæ³¨æ„åŠ›è§£æ'
- en: A guide to GNNs with self-attention using PyTorch Geometric
  id: totrans-136
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ä½¿ç”¨PyTorch Geometricçš„è‡ªæ³¨æ„åŠ›GNNæŒ‡å—
- en: towardsdatascience.com](/graph-attention-networks-in-python-975736ac5c0c?source=post_page-----24b3f60d6c95--------------------------------)
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '[towardsdatascience.com](/graph-attention-networks-in-python-975736ac5c0c?source=post_page-----24b3f60d6c95--------------------------------)'
- en: '*Learn more about machine learning and support my work with one click â€” become
    a Medium member here:*'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '*é€šè¿‡ç‚¹å‡»ä¸€ä¸‹äº†è§£æ›´å¤šæœºå™¨å­¦ä¹ çŸ¥è¯†å¹¶æ”¯æŒæˆ‘çš„å·¥ä½œ â€” æˆä¸ºMediumä¼šå‘˜ï¼Œè¯·ç‚¹å‡»è¿™é‡Œï¼š*'
- en: '[](https://medium.com/@mlabonne/membership?source=post_page-----24b3f60d6c95--------------------------------)
    [## Join Medium with my referral link â€” Maxime Labonne'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@mlabonne/membership?source=post_page-----24b3f60d6c95--------------------------------)
    [## é€šè¿‡æˆ‘çš„æ¨èé“¾æ¥åŠ å…¥Medium â€” Maxime Labonne'
- en: As a Medium member, a portion of your membership fee goes to writers you read,
    and you get full access to every storyâ€¦
  id: totrans-140
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ä½œä¸ºMediumä¼šå‘˜ï¼Œä½ çš„ä¼šå‘˜è´¹çš„ä¸€éƒ¨åˆ†å°†ç”¨äºæ”¯æŒä½ é˜…è¯»çš„ä½œè€…ï¼Œä½ å°†è·å¾—å¯¹æ¯ä¸ªæ•…äº‹çš„å®Œå…¨è®¿é—®æƒé™â€¦â€¦
- en: medium.com](https://medium.com/@mlabonne/membership?source=post_page-----24b3f60d6c95--------------------------------)
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '[medium.com](https://medium.com/@mlabonne/membership?source=post_page-----24b3f60d6c95--------------------------------)'
- en: '*If youâ€™re already a member, you can* [*follow me on Medium*](https://medium.com/@mlabonne)*.*'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '*å¦‚æœä½ å·²ç»æ˜¯ä¼šå‘˜ï¼Œä½ å¯ä»¥* [*åœ¨ Medium ä¸Šå…³æ³¨æˆ‘*](https://medium.com/@mlabonne)*.*'
