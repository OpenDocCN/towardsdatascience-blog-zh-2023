- en: Optimizing Neural Networks For Deep Learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度学习中的神经网络优化
- en: 原文：[https://towardsdatascience.com/optimisation-algorithms-neural-networks-101-256e16a88412](https://towardsdatascience.com/optimisation-algorithms-neural-networks-101-256e16a88412)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/optimisation-algorithms-neural-networks-101-256e16a88412](https://towardsdatascience.com/optimisation-algorithms-neural-networks-101-256e16a88412)
- en: How to improve training beyond the “vanilla” gradient descent algorithm
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何超越“普通”梯度下降算法改进训练
- en: '[](https://medium.com/@egorhowell?source=post_page-----256e16a88412--------------------------------)[![Egor
    Howell](../Images/1f796e828f1625440467d01dcc3e40cd.png)](https://medium.com/@egorhowell?source=post_page-----256e16a88412--------------------------------)[](https://towardsdatascience.com/?source=post_page-----256e16a88412--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----256e16a88412--------------------------------)
    [Egor Howell](https://medium.com/@egorhowell?source=post_page-----256e16a88412--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@egorhowell?source=post_page-----256e16a88412--------------------------------)[![Egor
    Howell](../Images/1f796e828f1625440467d01dcc3e40cd.png)](https://medium.com/@egorhowell?source=post_page-----256e16a88412--------------------------------)[](https://towardsdatascience.com/?source=post_page-----256e16a88412--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----256e16a88412--------------------------------)
    [Egor Howell](https://medium.com/@egorhowell?source=post_page-----256e16a88412--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----256e16a88412--------------------------------)
    ·8 min read·Nov 24, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----256e16a88412--------------------------------)
    ·8分钟阅读·2023年11月24日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/f4296e3c0bd744ca991919ad968f9e98.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f4296e3c0bd744ca991919ad968f9e98.png)'
- en: '[https://www.flaticon.com/free-icons/neural-network](https://www.flaticon.com/free-icons/neural-network).neural
    network icons. Neural network icons created by andinur — Flaticon.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://www.flaticon.com/free-icons/neural-network](https://www.flaticon.com/free-icons/neural-network).神经网络图标。神经网络图标由andinur创作
    — Flaticon.'
- en: Background
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 背景
- en: 'In my last post, we discussed how you can improve the performance of neural
    networks through [***hyperparameter tuning***](/optimise-your-hyperparameter-tuning-with-hyperopt-861573239eb5):'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在我上一篇文章中，我们讨论了如何通过[***超参数调整***](/optimise-your-hyperparameter-tuning-with-hyperopt-861573239eb5)来提高神经网络的性能：
- en: '[](/hyperparameter-tuning-neural-networks-101-ca1102891b27?source=post_page-----256e16a88412--------------------------------)
    [## Hyperparameter Tuning: Neural Networks 101'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/hyperparameter-tuning-neural-networks-101-ca1102891b27?source=post_page-----256e16a88412--------------------------------)
    [## 超参数调整：神经网络基础'
- en: How you can improve the “learning” and “training” of neural networks through
    tuning hyperparameters
  id: totrans-11
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 如何通过调整超参数来改善神经网络的“学习”和“训练”
- en: towardsdatascience.com](/hyperparameter-tuning-neural-networks-101-ca1102891b27?source=post_page-----256e16a88412--------------------------------)
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/hyperparameter-tuning-neural-networks-101-ca1102891b27?source=post_page-----256e16a88412--------------------------------)
- en: This is a process whereby the best hyperparameters such as learning rate and
    number of hidden layers are “tuned” to find the most optimal ones for our network
    to boost its performance.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个过程，通过对学习率和隐藏层数量等最佳超参数进行“调整”，以找到对我们的网络性能最优的参数。
- en: Unfortunately, this tuning process for large deep neural networks ([***deep
    learning***](https://en.wikipedia.org/wiki/Deep_learning)) is painstakingly slow.
    One way to improve upon this is to use *faster optimisers* than the traditional
    “vanilla” gradient descent method. In this post, we will dive into the most popular
    optimisers and variants of [***gradient descent***](/why-gradient-descent-is-so-common-in-data-science-def3e6515c5c)
    that can enhance the speed of training and also convergence and compare them in
    PyTorch!
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，对于大型深度神经网络（[***深度学习***](https://en.wikipedia.org/wiki/Deep_learning)），这种调整过程极其缓慢。改进的一种方法是使用比传统“普通”梯度下降方法更*快速的优化器*。在这篇文章中，我们将深入探讨最流行的优化器和[***梯度下降***](/why-gradient-descent-is-so-common-in-data-science-def3e6515c5c)的变体，这些可以提升训练速度以及收敛性，并在PyTorch中进行比较！
- en: 'Recap: Gradient Descent'
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 回顾：梯度下降
- en: Before diving in, let’s quickly brush up on our knowledge of gradient descent
    and the theory behind it.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入之前，让我们快速复习一下梯度下降及其背后的理论。
- en: The goal of gradient descent is to update the parameters of the model by subtracting
    the gradient (partial derivative) of the parameter with respect to the loss function.
    A learning rate, ***α***, serves to regulate this process to ensure updating of
    the parameters occurs on a reasonable scale and doesn’t over or undershoot the
    optimal value.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度下降的目标是通过减去参数关于损失函数的梯度（偏导数）来更新模型的参数。学习率 ***α*** 用于调节此过程，以确保参数的更新在合理范围内进行，避免过度或不足地达到最优值。
- en: '![](../Images/f2a7441d5107d106a25205788200a684.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f2a7441d5107d106a25205788200a684.png)'
- en: Gradient descent. Equation by author.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度下降。方程由作者提供。
- en: '***θ*** are the parameters of the model.'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***θ*** 是模型的参数。'
- en: '***J(θ)*** is the loss function.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***J(θ)*** 是损失函数。'
- en: '***∇J(θ)*** is the gradient of the loss function. ***∇*** is the gradient operator,
    also known as [***nabla***](https://en.wikipedia.org/wiki/Nabla_symbol).'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***∇J(θ)*** 是损失函数的梯度。***∇*** 是梯度算子，也称为 [***nabla***](https://en.wikipedia.org/wiki/Nabla_symbol)。'
- en: '***α*** is the learning rate.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***α*** 是学习率。'
- en: 'I wrote a previous article on gradient descent and how it works if you want
    to familiarise yourself a bit more about it:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 我曾写过一篇关于梯度下降及其工作原理的文章，如果你想对它有更多了解，可以参考一下：
- en: '[](/why-gradient-descent-is-so-common-in-data-science-def3e6515c5c?source=post_page-----256e16a88412--------------------------------)
    [## Linear Regression: Gradient Descent Vs Analytical Solution'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/why-gradient-descent-is-so-common-in-data-science-def3e6515c5c?source=post_page-----256e16a88412--------------------------------)
    [## 线性回归：梯度下降与解析解'
- en: An explanation of why Gradient Descent is frequently used in Data Science with
    an implementation in C
  id: totrans-26
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 解释了为什么梯度下降在数据科学中经常使用，并提供了 C 语言的实现
- en: towardsdatascience.com](/why-gradient-descent-is-so-common-in-data-science-def3e6515c5c?source=post_page-----256e16a88412--------------------------------)
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/why-gradient-descent-is-so-common-in-data-science-def3e6515c5c?source=post_page-----256e16a88412--------------------------------)
- en: Momentum
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 动量
- en: Gradient descent is often visualised as a ball rolling down a hill. When it
    reaches the bottom of the valley, it has converged to the minimum, which is the
    optimal value. A ball rolling downhill consistently will garner some *momentum,*
    however, regular gradient descent works on an individual iteration basis and does
    not know the previous updates.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度下降通常被可视化为一个球体在山坡上滚动。当它到达山谷底部时，它已经收敛到最小值，即最优值。一个持续向下滚动的球体会获得一定的 *动量*，然而，普通梯度下降是在每次迭代基础上进行的，并不了解之前的更新。
- en: By including momentum in the gradient descent update, it provides the algorithm
    information about previous gradients that it has computed.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 通过在梯度下降更新中包含动量，它为算法提供了关于先前计算的梯度的信息。
- en: 'Mathematically, what we have is:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 从数学上讲，我们得到的是：
- en: '![](../Images/19e655b5a915b2e471a1946a13502485.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/19e655b5a915b2e471a1946a13502485.png)'
- en: Momentum gradient descent. Equation by author.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 动量梯度下降。方程由作者提供。
- en: 'Where:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 其中：
- en: '***v_t***​ is the current velocity.'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***v_t*** 是当前的速度。'
- en: '***β*** is the momentum coefficient, a value between 0 and 1\. This is also
    sometimes interpreted as the ‘*friction*.’ You would need to find the best value
    of ***β,*** but often ***0.9*** is a good baseline***.***'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***β*** 是动量系数，值在 0 和 1 之间。这有时也被解释为“*摩擦力*”。你需要找到最佳的 ***β*** 值，但通常 ***0.9***
    是一个不错的基准***。***'
- en: '***t*** is the current time step or iteration number.'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***t*** 是当前的时间步长或迭代次数。'
- en: '***v_{t*−1}​** is the velocity from the previous step (the last calculated
    value).'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***v_{t*−1}​*** 是前一步的速度（上一次计算的值）。'
- en: The rest of the terms mean the same as declared earlier for vanilla gradient
    descent!
  id: totrans-39
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 其余术语与之前对普通梯度下降的定义相同！
- en: Notice that we are utilising information from the previous gradients to ‘accelerate’
    the current gradient in the direction of the previous ones. This increases the
    speed of convergence and dampens any oscillations that may occur with vanilla
    gradient descent.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们利用了先前梯度的信息来‘加速’当前梯度的方向。这提高了收敛速度，并减少了普通梯度下降中可能出现的任何振荡。
- en: 'Momentum is easily implemented in [***PyTorch***](https://pytorch.org/docs/stable/optim.html)
    as well:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 动量在 [***PyTorch***](https://pytorch.org/docs/stable/optim.html) 中也很容易实现。
- en: '[PRE0]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Nesterov Accelerated Gradient
  id: totrans-43
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Nesterov 加速梯度
- en: '[***Nesterov accelerated gradient***](https://golden.com/wiki/Nesterov_momentum-YX9WPE5)
    ***(NAG)***, or Nesterov momentum, is a slight modification to the momentum algorithm
    that often leads to better convergence.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '[***Nesterov 加速梯度***](https://golden.com/wiki/Nesterov_momentum-YX9WPE5) ***(NAG)***，或称为
    Nesterov 动量，是对动量算法的轻微修改，通常能导致更好的收敛效果。'
- en: NAG measures the gradient with respect to the loss function slightly ahead of
    ***θ.*** This improves convergence as the *momentum* value will generally be heading
    towards the optimal point. Therefore, allowing the algorithm to take a slight
    step ahead every time leads it to converge quicker.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: NAG 测量相对于损失函数的梯度时略微超前于 ***θ.*** 这改善了收敛性，因为 *动量* 值通常会朝着最优点前进。因此，每次允许算法略微前进一步可以使其更快收敛。
- en: '![](../Images/bbc5f2eeaadfb2811a54dba24e553b15.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bbc5f2eeaadfb2811a54dba24e553b15.png)'
- en: Nesterov accelerated gradient descent. Equation by author.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: Nesterov 加速梯度下降。方程由作者编写。
- en: Where ***∇J(θ+βv_{t−1}​)*** is the gradient of the loss function evaluated at
    a point slightly ahead of the current ***θ***.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 ***∇J(θ+βv_{t−1}​)*** 是在当前 ***θ*** 前稍微一点的损失函数的梯度。
- en: All the terms in the above equation are the same ones as for the previous optimisers,
    so I won’t list them out all again!
  id: totrans-49
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 上述方程中的所有术语与之前的优化器相同，因此我不会再次列出所有术语！
- en: The Nesterov accelerated gradient is also easily implemented in [***PyTorch***](https://pytorch.org/docs/stable/optim.html)***:***
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: Nesterov 加速梯度也可以在 [***PyTorch***](https://pytorch.org/docs/stable/optim.html)
    中轻松实现***：***
- en: '[PRE1]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: AdaGrad
  id: totrans-52
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: AdaGrad
- en: '[***Adaptive Gradient Algorithm (Adagrad)***](https://optimization.cbe.cornell.edu/index.php?title=AdaGrad)
    is a gradient descent algorithm that uses an adaptive learning rate that gets
    smaller if a feature/parameter is updated more frequently. In other words, it
    decays the learning rate a lot more for steeper gradients than shallow ones.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '[***自适应梯度算法（Adagrad）***](https://optimization.cbe.cornell.edu/index.php?title=AdaGrad)
    是一种梯度下降算法，它使用自适应学习率，如果特征/参数更新得更频繁，学习率会变得更小。换句话说，它对更陡峭的梯度比对较浅的梯度衰减更多。'
- en: '![](../Images/778ea9cbc67421eb5e62fce392b89d24.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/778ea9cbc67421eb5e62fce392b89d24.png)'
- en: Adagrad. Equation by author.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: Adagrad。方程由作者编写。
- en: 'Here:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 这里：
- en: '***G***​ is a diagonal matrix that accumulates the squares of all the gradients
    up to the time step for each parameter.'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***G***​ 是一个对角矩阵，积累了每个参数在时间步长内所有梯度的平方。'
- en: '***ϵ*** is a tiny smoothing term to avoid division by zero problems when ***G***
    is very small.'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***ϵ*** 是一个小的平滑项，用于避免当 ***G*** 非常小时的除零问题。'
- en: '***⊙*** denotes element-wise multiplication. This is the [***Hadamard product.***](https://en.wikipedia.org/wiki/Hadamard_product_%28matrices%29)'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***⊙*** 表示逐元素乘法。这是 [***Hadamard 乘积***](https://en.wikipedia.org/wiki/Hadamard_product_%28matrices%29)'
- en: The rest of the terms in the above equation are the same ones as for the previous
    optimisers, so I won’t list them out all again!
  id: totrans-60
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 上述方程中的其余术语与之前的优化器相同，因此我不会再次列出所有术语！
- en: 'An example of element-wise multiplication for matrices, assuming ***A*** and
    ***B*** are both ***2x2***:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 元素级矩阵乘法的一个例子，假设 ***A*** 和 ***B*** 都是 ***2x2***：
- en: '![](../Images/723511d56611bab9dcc6be64a8354860.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/723511d56611bab9dcc6be64a8354860.png)'
- en: An example of the Hadamard product. Equation by author in LaTeX.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: Hadamard 乘积的一个例子。方程由作者用 LaTeX 编写。
- en: As we can see, the larger the value of ***G***, the smaller the update will
    be to the parameter. It’s basically a moving average of the squared gradients.
    This ensures the learning slows down and doesn’t overshoot the optimum.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所看到的，***G*** 的值越大，对参数的更新就越小。它基本上是平方梯度的移动平均。这确保了学习过程变慢，不会超过最优点。
- en: One problem with Adagrad is that it sometimes decays the learning rate so much
    that neural networks stop learning too early on and plateau. Therefore, it’s not
    generally recommended to use Adagrad when training neural nets.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: Adagrad 的一个问题是，它有时会使学习率衰减得太多，导致神经网络过早停止学习并陷入停滞。因此，通常不推荐在训练神经网络时使用 Adagrad。
- en: '[PRE2]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: RMSProp
  id: totrans-67
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: RMSProp
- en: '[***RMSProp (Root Mean Squared Propagation)***](https://deepchecks.com/glossary/rmsprop/#:~:text=RMSprop%20is%20an%20innovative%20stochastic,and%20other%20Machine%20Learning%20techniques.)
    fixes the issue of Adagrad finishing training too early by only taking into account
    recent gradients. It does this by introducing another hyperparameter, ***β***,
    that scales down the impact of values inside the diagonal matrix ***G***:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '[***RMSProp（均方根传播）***](https://deepchecks.com/glossary/rmsprop/#:~:text=RMSprop%20is%20an%20innovative%20stochastic,and%20other%20Machine%20Learning%20techniques.)
    通过只考虑最近的梯度来解决 Adagrad 过早结束训练的问题。它通过引入另一个超参数 ***β*** 来做到这一点，从而缩小对对角矩阵 ***G*** 内部值的影响：'
- en: '![](../Images/dbfc2874b74d780204e2a5b77c565158.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/dbfc2874b74d780204e2a5b77c565158.png)'
- en: RMSProp. Equation by author in LaTeX.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: RMSProp。方程由作者用 LaTeX 编写。
- en: All the terms in the above equation are the same ones as for the previous optimisers,
    so I won’t list them out all again!
  id: totrans-71
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 上述方程中的所有项都与之前优化器的相同，所以我不会再次列出它们！
- en: 'Like the other optimisers, RMSProp is simple to implement in PyTorch:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 像其他优化器一样，RMSProp在PyTorch中实现起来很简单：
- en: '[PRE3]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Adam
  id: totrans-74
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Adam
- en: 'The final optimiser we will look at is [***Adaptive Moment Estimation***](https://arxiv.org/abs/1412.6980),
    better known as *Adam*. This algorithm is a combination of both momentum and RMSProp,
    so it’s kinda the best of both worlds. Although, it has a few more steps:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将要看的最终优化器是[***自适应矩估计***](https://arxiv.org/abs/1412.6980)，更为人知的是*Adam*。该算法结合了动量和RMSProp，因此可以说是两者的最佳结合。不过，它有几个额外的步骤：
- en: '![](../Images/f5e12b0431f48f8edf3066491d7211a5.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f5e12b0431f48f8edf3066491d7211a5.png)'
- en: Adam optimiser. Equation by author in LaTeX.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: Adam优化器。公式由作者以LaTeX呈现。
- en: The first two and last steps are pretty much the momentum and RMSProp algorithms
    we showed earlier. Steps three and four are correcting the bias of ***v_t*** and
    ***G_t*** as they are initialised to 0 at the start.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 前两个步骤和最后一步几乎是我们之前展示的动量和RMSProp算法。第三步和第四步是修正***v_t***和***G_t***的偏差，因为它们在开始时被初始化为0。
- en: Adam is an *adaptive learning rate* algorithm like RMSProp, so you don’t need
    to necessarily tune the learning rate when using this optimiser.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: Adam是*自适应学习率*算法，类似于RMSProp，因此使用此优化器时不一定需要调节学习率。
- en: The rest of the terms in the above equation are the same ones as for the previous
    optimisers, so I won’t list them out all again!
  id: totrans-80
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 上述方程中的其他项与之前优化器相同，所以我不会再次列出它们！
- en: 'Here’s how you apply Adam in PyTorch:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是如何在PyTorch中应用Adam：
- en: '[PRE4]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Other Optimisers
  id: totrans-83
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 其他优化器
- en: There are many other gradient descent optimisers out there, and the ones we
    have considered here are only *first-order derivatives*, which are called [***Jacobians***](https://en.wikipedia.org/wiki/Jacobian_matrix_and_determinant).
    A second-order derivative exists, called [***Hessians***](https://en.wikipedia.org/wiki/Hessian_matrix)***,***
    but their compute complexity is on the order of ***O²***, whereas first-orders
    is only ***O***.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有许多其他梯度下降优化器，我们考虑的只是*一阶导数*，这些被称为[***雅可比矩阵***](https://en.wikipedia.org/wiki/Jacobian_matrix_and_determinant)。还有一种二阶导数，称为[***赫西矩阵***](https://en.wikipedia.org/wiki/Hessian_matrix)***,***
    但其计算复杂度为***O²***，而一阶导数的计算复杂度仅为***O***。
- en: In practise, deep neural networks have tens of thousands to millions of rows
    of data, so Hessian gradient descent methods are rarely used. The gold standard
    is really Adam or Nestorov for most cases.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，深度神经网络有数万到数百万行数据，因此赫西梯度下降方法很少使用。大多数情况下，金标准确实是Adam或Nestorov。
- en: There is also batch, mini-batch, and stochastic gradient descent which affect
    the compute speed of the network. I have written about these algorithms in my
    [previous article linked here](https://medium.com/towards-data-science/hyperparameter-tuning-neural-networks-101-ca1102891b27).
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 还有批量、小批量和随机梯度下降，这些会影响网络的计算速度。我在[之前的文章中](https://medium.com/towards-data-science/hyperparameter-tuning-neural-networks-101-ca1102891b27)写过这些算法。
- en: 'Some other used optimisers are:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 其他一些常用优化器包括：
- en: '[*Adamax*](https://arxiv.org/abs/1412.6980)'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*Adamax*](https://arxiv.org/abs/1412.6980)'
- en: '[*Nadam*](https://openreview.net/pdf?id=OM0jvwB8jIp57ZJjtNEZ)'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*Nadam*](https://openreview.net/pdf?id=OM0jvwB8jIp57ZJjtNEZ)'
- en: '[*ADAMW*](https://openreview.net/forum?id=rk6qdGgCZ)'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*ADAMW*](https://openreview.net/forum?id=rk6qdGgCZ)'
- en: A full comprehensive list can be found [here](https://github.com/harsh306/awesome-nn-optimization).
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 完整的综合列表可以在[这里](https://github.com/harsh306/awesome-nn-optimization)找到。
- en: Performance Comparison
  id: totrans-92
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 性能比较
- en: The code below is a comparison of the different optimisers that we discussed
    above for the ***J(θ) = θ²*** loss function. The minimum is at ***θ = 0:***
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的代码是对我们之前讨论的不同优化器的比较，针对***J(θ) = θ²***损失函数。最小值在***θ = 0:***
- en: '[PRE5]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '![](../Images/50d96b2fc313b4cfcaa191a03adcda37.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/50d96b2fc313b4cfcaa191a03adcda37.png)'
- en: Optimiser comparison. Plot by author in Python.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 优化器比较。由作者使用Python绘制的图。
- en: 'This plot is quite interesting, some key things to point out:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 这个图很有趣，有几个关键点要指出：
- en: '*Both Momentum and Nestorov overshoot the optimal value of* ***θ.***'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*动量和Nestorov都超出了* ***θ.*** *的最优值。*'
- en: '*Adagrad is very slow. This is in line with what we discussed before regarding
    the training stopping too early as the learning rate decays rapidly and learning
    plateaus.*'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Adagrad非常慢。这与我们之前讨论的情况一致，即学习率迅速衰减，导致训练过早停止和学习停滞。*'
- en: '*Adam and RMSProp seem to be the best with RMSProp reaching the optimal value
    quicker.*'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Adam和RMSProp似乎是最好的，其中RMSProp更快地达到最优值。*'
- en: Of course, this is only a simple example and in real-life problems, the best
    optimiser will likely be different. So, it is often well worth trying a variety
    of different ones and picking the best-performing one.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，这只是一个简单的示例，在实际问题中，最佳的优化器可能会有所不同。因此，尝试各种不同的优化器并选择表现最佳的往往是非常值得的。
- en: 'This code is available at my GitHub here:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码可以在我的GitHub上找到：
- en: '[](https://github.com/egorhowell/Medium-Articles/blob/main/Neural%20Networks/optimisers.py?source=post_page-----256e16a88412--------------------------------)
    [## Medium-Articles/Neural Networks/optimisers.py at main · egorhowell/Medium-Articles'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://github.com/egorhowell/Medium-Articles/blob/main/Neural%20Networks/optimisers.py?source=post_page-----256e16a88412--------------------------------)
    [## Medium-Articles/Neural Networks/optimisers.py at main · egorhowell/Medium-Articles'
- en: Code I use in my medium blog/articles. Contribute to egorhowell/Medium-Articles
    development by creating an account on…
  id: totrans-104
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 在我的中等博客/文章中使用的代码。通过创建帐户来贡献开发…
- en: github.com](https://github.com/egorhowell/Medium-Articles/blob/main/Neural%20Networks/optimisers.py?source=post_page-----256e16a88412--------------------------------)
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: github.com](https://github.com/egorhowell/Medium-Articles/blob/main/Neural%20Networks/optimisers.py?source=post_page-----256e16a88412--------------------------------)
- en: Summary & Further Thoughts
  id: totrans-106
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要与进一步的思考
- en: In this post, we have seen several methods to speed up and improve the performance
    of the vanilla gradient descent. The two types of methods are momentum-based,
    using information from previous gradients, and adaptive-based, changing the learning
    rate regarding the computed gradients. In literature, the Adam optimiser is often
    the one recommended and used the most in research. However, it is always worth
    trying different optimisers, to determine which ones suit your model the most.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我们看到了几种加速和提高普通梯度下降性能的方法。这两种方法类型是基于动量的，使用来自先前梯度的信息，以及基于自适应的，依据计算出的梯度调整学习率。在文献中，Adam优化器通常是最推荐和最常用于研究的优化器。然而，尝试不同的优化器总是值得的，以确定哪种最适合你的模型。
- en: Another Thing!
  id: totrans-108
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 另一个话题！
- en: I have a free newsletter, [**Dishing the Data**](https://dishingthedata.substack.com/),
    where I share weekly tips for becoming a better Data Scientist. There is no “fluff”
    or “clickbait,” just pure actionable insights from a practicing Data Scientist.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 我有一个免费的新闻通讯，[**分析数据**](https://dishingthedata.substack.com/)，我每周分享成为更好的数据科学家的技巧。没有“虚
    fluff”或“点击诱饵”，只有来自实践数据科学家的纯粹可操作的见解。
- en: '[](https://newsletter.egorhowell.com/?source=post_page-----256e16a88412--------------------------------)
    [## Dishing The Data | Egor Howell | Substack'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://newsletter.egorhowell.com/?source=post_page-----256e16a88412--------------------------------)
    [## 分析数据 | Egor Howell | Substack'
- en: How To Become A Better Data Scientist. Click to read Dishing The Data, by Egor
    Howell, a Substack publication with…
  id: totrans-111
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 如何成为更好的数据科学家。点击阅读《分析数据》，由Egor Howell编写，Substack出版…
- en: newsletter.egorhowell.com](https://newsletter.egorhowell.com/?source=post_page-----256e16a88412--------------------------------)
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '[新闻通讯](https://newsletter.egorhowell.com/?source=post_page-----256e16a88412--------------------------------)'
- en: Connect With Me!
  id: totrans-113
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 与我联系！
- en: '[**YouTube**](https://www.youtube.com/@egorhowell)'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**YouTube**](https://www.youtube.com/@egorhowell)'
- en: '[**LinkedIn**](https://www.linkedin.com/in/egor-howell-092a721b3/)'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**LinkedIn**](https://www.linkedin.com/in/egor-howell-092a721b3/)'
- en: '[**Twitter**](https://twitter.com/EgorHowell)'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**Twitter**](https://twitter.com/EgorHowell)'
- en: '[**GitHub**](https://github.com/egorhowell)'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**GitHub**](https://github.com/egorhowell)'
- en: References & Further Reading
  id: totrans-118
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献与进一步阅读
- en: '[*Andrej Karpathy Neural Network Course*](https://www.youtube.com/watch?v=i94OvYb6noo)'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*Andrej Karpathy神经网络课程*](https://www.youtube.com/watch?v=i94OvYb6noo)'
- en: '[*PyTorch site*](https://pytorch.org/)'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*PyTorch官网*](https://pytorch.org/)'
- en: '[*Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow, 2nd Edition.
    Aurélien Géron. September 2019\. Publisher(s): O’Reilly Media, Inc. ISBN: 9781492032649*](https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/)*.*'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*动手机器学习：Scikit-Learn、Keras和TensorFlow，第2版。Aurélien Géron。2019年9月。出版社：O’Reilly
    Media, Inc. ISBN: 9781492032649*](https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/)*.*'
- en: '[*Great blog on optimising neural networks*](/neural-network-optimization-7ca72d4db3e0)'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*关于优化神经网络的优秀博客*](/neural-network-optimization-7ca72d4db3e0)'
- en: 'Some of my other blogs on neural networks that might be of interest:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我关于神经网络的其他一些可能感兴趣的博客：
- en: '[](/activation-functions-non-linearity-neural-networks-101-ab0036a2e701?source=post_page-----256e16a88412--------------------------------)
    [## Activation Functions & Non-Linearity: Neural Networks 101'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/activation-functions-non-linearity-neural-networks-101-ab0036a2e701?source=post_page-----256e16a88412--------------------------------)
    [## 激活函数与非线性：神经网络101'
- en: Explaining why neural networks can learn (nearly) anything and everything
  id: totrans-125
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 解释神经网络为何能学习（几乎）任何事物和一切
- en: 'towardsdatascience.com](/activation-functions-non-linearity-neural-networks-101-ab0036a2e701?source=post_page-----256e16a88412--------------------------------)
    [](/forward-pass-backpropagation-neural-networks-101-3a75996ada3b?source=post_page-----256e16a88412--------------------------------)
    [## Forward Pass & Backpropagation: Neural Networks 101'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '[towardsdatascience.com](/activation-functions-non-linearity-neural-networks-101-ab0036a2e701?source=post_page-----256e16a88412--------------------------------)
    [](/forward-pass-backpropagation-neural-networks-101-3a75996ada3b?source=post_page-----256e16a88412--------------------------------)
    [## 前向传播与反向传播：神经网络基础'
- en: Explaining how neural networks “train” and “learn” patterns in data by hand
    and in code using PyTorch
  id: totrans-127
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 通过手动和代码（使用 PyTorch）解释神经网络如何“训练”和“学习”数据中的模式
- en: towardsdatascience.com](/forward-pass-backpropagation-neural-networks-101-3a75996ada3b?source=post_page-----256e16a88412--------------------------------)
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '[towardsdatascience.com](/forward-pass-backpropagation-neural-networks-101-3a75996ada3b?source=post_page-----256e16a88412--------------------------------)'
