- en: Large Language Models, StructBERT — Incorporating Language Structures into Pretraining
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 大型语言模型，StructBERT — 将语言结构融入预训练
- en: 原文：[https://towardsdatascience.com/large-language-models-structbert-incorporating-language-structures-into-pretraining-be3058ab23b3?source=collection_archive---------3-----------------------#2023-11-22](https://towardsdatascience.com/large-language-models-structbert-incorporating-language-structures-into-pretraining-be3058ab23b3?source=collection_archive---------3-----------------------#2023-11-22)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/large-language-models-structbert-incorporating-language-structures-into-pretraining-be3058ab23b3?source=collection_archive---------3-----------------------#2023-11-22](https://towardsdatascience.com/large-language-models-structbert-incorporating-language-structures-into-pretraining-be3058ab23b3?source=collection_archive---------3-----------------------#2023-11-22)
- en: Making models smarter by incorporating better learning objectives
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 通过融入更好的学习目标来使模型更智能
- en: '[](https://medium.com/@slavahead?source=post_page-----be3058ab23b3--------------------------------)[![Vyacheslav
    Efimov](../Images/db4b02e75d257063e8e9d3f1f75d9d6d.png)](https://medium.com/@slavahead?source=post_page-----be3058ab23b3--------------------------------)[](https://towardsdatascience.com/?source=post_page-----be3058ab23b3--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----be3058ab23b3--------------------------------)
    [Vyacheslav Efimov](https://medium.com/@slavahead?source=post_page-----be3058ab23b3--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@slavahead?source=post_page-----be3058ab23b3--------------------------------)[![Vyacheslav
    Efimov](../Images/db4b02e75d257063e8e9d3f1f75d9d6d.png)](https://medium.com/@slavahead?source=post_page-----be3058ab23b3--------------------------------)[](https://towardsdatascience.com/?source=post_page-----be3058ab23b3--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----be3058ab23b3--------------------------------)
    [Vyacheslav Efimov](https://medium.com/@slavahead?source=post_page-----be3058ab23b3--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fc8a0ca9d85d8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flarge-language-models-structbert-incorporating-language-structures-into-pretraining-be3058ab23b3&user=Vyacheslav+Efimov&userId=c8a0ca9d85d8&source=post_page-c8a0ca9d85d8----be3058ab23b3---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----be3058ab23b3--------------------------------)
    ·5 min read·Nov 22, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fbe3058ab23b3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flarge-language-models-structbert-incorporating-language-structures-into-pretraining-be3058ab23b3&user=Vyacheslav+Efimov&userId=c8a0ca9d85d8&source=-----be3058ab23b3---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fc8a0ca9d85d8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flarge-language-models-structbert-incorporating-language-structures-into-pretraining-be3058ab23b3&user=Vyacheslav+Efimov&userId=c8a0ca9d85d8&source=post_page-c8a0ca9d85d8----be3058ab23b3---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----be3058ab23b3--------------------------------)
    ·5分钟阅读·2023年11月22日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fbe3058ab23b3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flarge-language-models-structbert-incorporating-language-structures-into-pretraining-be3058ab23b3&user=Vyacheslav+Efimov&userId=c8a0ca9d85d8&source=-----be3058ab23b3---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fbe3058ab23b3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flarge-language-models-structbert-incorporating-language-structures-into-pretraining-be3058ab23b3&source=-----be3058ab23b3---------------------bookmark_footer-----------)![](../Images/9950addc5beece240f634d42ca38618b.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fbe3058ab23b3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flarge-language-models-structbert-incorporating-language-structures-into-pretraining-be3058ab23b3&source=-----be3058ab23b3---------------------bookmark_footer-----------)![](../Images/9950addc5beece240f634d42ca38618b.png)'
- en: Introduction
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: After its first appearance, BERT has shown phenomenal results in a variety of
    NLP tasks including sentiment analysis, text similarity, question answering, etc.
    Since then, researchers notoriously tried to make BERT even more performant by
    either modifying its architecture, augmenting training data, increasing vocabulary
    size or changing the hidden size of layers.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 自BERT首次出现以来，它在各种NLP任务中显示了惊人的效果，包括情感分析、文本相似度、问答等。从那时起，研究人员就尝试通过修改架构、增加训练数据、扩充词汇量或改变层的隐藏大小等方式，使BERT变得更加高效。
- en: '[](/bert-3d1bf880386a?source=post_page-----be3058ab23b3--------------------------------)
    [## Large Language Models: BERT — Bidirectional Encoder Representations from Transformer'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/bert-3d1bf880386a?source=post_page-----be3058ab23b3--------------------------------)
    [## 大型语言模型：BERT — 双向编码器表示转换器'
- en: Understand how BERT constructs state-of-the-art embeddings
  id: totrans-11
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 了解 BERT 如何构建最先进的嵌入
- en: towardsdatascience.com](/bert-3d1bf880386a?source=post_page-----be3058ab23b3--------------------------------)
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '[towardsdatascience.com](/bert-3d1bf880386a?source=post_page-----be3058ab23b3--------------------------------)'
- en: Despite the creation of other powerful BERT-based models like RoBERTa, researchers
    found another efficient way to boost BERT’s performance which is going to be discussed
    in this article. This led to the development of a new model called **StructBERT**
    which confidently surpasses BERT on top benchmarks.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管创建了其他强大的基于 BERT 的模型如 RoBERTa，研究人员发现了另一种提升 BERT 性能的有效方法，这将在本文中讨论。这导致了新模型 **StructBERT**
    的发展，该模型在顶级基准上自信地超越了 BERT。
- en: The StructBERT idea is relatively simple and focuses on slightly modifying BERT’s
    pretraining objective.
  id: totrans-14
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: StructBERT 的想法相对简单，重点在于略微修改 BERT 的预训练目标。
- en: In this article, we will go through the main details of the StructBERT paper
    and understand the originally modified objectives.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们将深入探讨 StructBERT 论文的主要细节，并理解最初修改的目标。
- en: Pretraining
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 预训练
- en: For the most part, StructBERT has the same architectural principles as BERT.
    Nevertheless, StructBERT presents two new pretraining objectives to expand linguistic
    knowledge of BERT. The model is trained on this objective alongside masked language
    modeling. Let us look at these two objectives below.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在很大程度上，StructBERT 具有与 BERT 相同的架构原则。然而，StructBERT 提出了两个新的预训练目标，以扩展 BERT 的语言知识。该模型在此目标下进行训练，配合掩码语言建模。让我们看看下面的这两个目标。
- en: 1\. Word sentence objective
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 1\. 单词句子目标
- en: Experiments showed that masked language modeling (MSM) task plays a crucial
    role in the BERT setting to help it obtain vast linguistic knowledge. After pretraining,
    BERT can correctly guess masked words with high accuracy. Nevertheless, it is
    not capable of correctly reconstructing a sentence whose words are shuffled. To
    achieve this goal, the StructBERT developers modified the MSM objective by partially
    shuffling input tokens.
  id: totrans-19
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 实验表明，掩码语言建模（MSM）任务在 BERT 设置中发挥了关键作用，帮助其获得广泛的语言知识。预训练后，BERT 可以以高准确率正确猜测掩码词。然而，它无法正确重建单词被打乱的句子。为实现这一目标，StructBERT
    开发者通过部分打乱输入标记来修改 MSM 目标。
- en: As in the original BERT, an input sequence is tokenised, masked and then mapped
    to token, positional and segment embeddings. All of these embeddings are then
    summed up to produce combined embeddings which are fed to BERT.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 与原始 BERT 一样，输入序列会被标记化、掩码处理，然后映射到标记、位置和段嵌入。这些嵌入会被求和以产生组合嵌入，然后输入到 BERT。
- en: During masking, 15% of randomly chosen tokens are masked and then used for language
    modeling, as in BERT. But right after masking, StructBERT randomly selects 5%
    of K consecutive unmasked tokens and shuffles them within each subsequence. By
    default, StructBERT operates on trigrams (K = 3).
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在掩码处理过程中，与 BERT 一样，15% 的随机选择标记会被掩盖并用于语言建模。但在掩码处理后，StructBERT 随机选择 5% 的 K 个连续未掩盖标记，并在每个子序列内对其进行打乱。默认情况下，StructBERT
    对三元组（K = 3）进行操作。
- en: '![](../Images/ae1dfe76bcdacacb333330b5f75ccc0c.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ae1dfe76bcdacacb333330b5f75ccc0c.png)'
- en: Example of a trigram shuffling
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 三元组打乱示例
- en: When the last hidden layer is computed, output embeddings of masked and shuffled
    tokens are then used to predict original tokens taking into account their initial
    positions.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 当计算最后一层隐藏层时，掩码和打乱标记的输出嵌入被用于预测原始标记，同时考虑其初始位置。
- en: Ultimately, word sentence objective is combined with MLM objective with equal
    weights.
  id: totrans-25
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 最终，单词句子目标与 MLM 目标以相等的权重结合。
- en: 2\. Sentence structural objective
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 2\. 句子结构目标
- en: Next sentence prediction which is another BERT pretraining task is considered
    relatively simple. Mastering it does not lead to a significant boost to BERT performance
    on most downstream tasks. That is why StructBERT researchers increased the difficulty
    of this objective by making BERT predict the sentence order.
  id: totrans-27
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 下一句预测，作为 BERT 的另一个预训练任务，被认为相对简单。掌握它并不会显著提升 BERT 在大多数下游任务上的表现。因此，StructBERT 研究人员通过让
    BERT 预测句子顺序来增加这一目标的难度。
- en: 'By taking a pair of sequential sentences S₁ and S₂ in a document, StructBERT
    uses them to construct a training example in one of three possible ways. Each
    of these ways occurs with an equal probability of 1 / 3:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 通过在文档中取一对连续句子 S₁ 和 S₂，StructBERT 使用它们以三种可能的方式之一构建训练样本。每种方式发生的概率均为 1 / 3：
- en: S₂ is followed by S₁ *(label 1);*
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: S₂ 后跟 S₁ *(标签 1);*
- en: S₁ is followed by S₂ *(label 2);*
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: S₁ 后跟 S₂ *(标签 2);*
- en: Another sentence S₃ from a random document is sampled and is followed by S₁
    *(label 0).*
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从随机文档中抽取另一个句子 S₃，并且 S₁ *(标签 0)* 跟在其后。
- en: Each of these three procedures results in a ordered pair of sentences which
    are then concatenated. The token [CLS] is added before the beginning of the first
    sentence and [SEP] tokens are used to mark the end of each sentence. BERT takes
    this sequence as input and outputs a set of embeddings on the last hidden layer.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 这三种过程中的每一种都会生成一个有序的句子对，然后将它们连接起来。在第一个句子开始前添加了 [CLS] 标记，并使用 [SEP] 标记标记每个句子的结束。BERT
    将该序列作为输入，并输出最后隐藏层的嵌入集。
- en: The output of the [CLS] embedding which was originally used in BERT for next
    sentence prediction task, is now used in StructBERT to correctly identify one
    of three possible labels corresponding to the original way the input sequence
    was built.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '[CLS] 嵌入的输出，最初在 BERT 中用于下一句预测任务，现在在 StructBERT 中用于正确识别与输入序列构建方式相对应的三种可能标签之一。'
- en: '![](../Images/53c9a4372539ab5637e1fef508e2bc7d.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/53c9a4372539ab5637e1fef508e2bc7d.png)'
- en: Composition of training samples
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 训练样本的组成
- en: Final objective
  id: totrans-36
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 最终目标
- en: The final objective consists of a linear combination of word and sentence structural
    objectives.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 最终目标由词和句子结构目标的线性组合组成。
- en: '![](../Images/4d5d17bd852192d75c360c85a07f28c6.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4d5d17bd852192d75c360c85a07f28c6.png)'
- en: BERT pretraining including word and sentence structural objectives
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: BERT 预训练包括词和句子结构目标
- en: StructBERT settings
  id: totrans-40
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: StructBERT 设置
- en: 'All of the main pretraining details are the same in BERT and StructBERT:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: BERT 和 StructBERT 的所有主要预训练细节相同：
- en: 'StructBERT uses the same pretraining corpus as BERT: English Wikipedia (2500M
    words) and BookCorpus (800M words). Tokenization is done by WordPiece tokenizer.'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: StructBERT 使用与 BERT 相同的预训练语料库：英语维基百科（2500M 单词）和 BookCorpus（800M 单词）。分词由 WordPiece
    分词器完成。
- en: 'Optimisator: Adam (learning rate *l* = 1e-4, weight decay L₂ = 0.01, β₁ = 0.9,
    β₂ = 0.999).'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 优化器：Adam（学习率 *l* = 1e-4，权重衰减 L₂ = 0.01，β₁ = 0.9，β₂ = 0.999）。
- en: Learning rate warmup is performed over the first 10% of total steps and then
    reduced linearly.
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习率热身在总步骤的前 10% 中进行，然后线性减少。
- en: Dropout (α = 0.1) layer is used on all layers.
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在所有层上使用 Dropout（α = 0.1）。
- en: 'Activation function: GELU.'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 激活函数：GELU。
- en: The pretraining procedure is run for 40 epochs.
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预训练过程运行 40 个周期。
- en: StructBERT versions
  id: totrans-48
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: StructBERT 版本
- en: Like the original BERT, StructBERT comes up with base and large versions. All
    the main settings like the number of layers, attention heads, hidden size and
    the number parameters correspond exactly to base and large versions of BERT respectively.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 与原始 BERT 类似，StructBERT 提供了基础版和大型版。所有主要设置，如层数、注意力头、隐藏层大小和参数数量，都分别对应 BERT 的基础版和大型版。
- en: '![](../Images/554d81d8503b1480c90ba7c32d3130c4.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/554d81d8503b1480c90ba7c32d3130c4.png)'
- en: Comparison of StructBERT base and StructBERT large
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: StructBERT 基础版与 StructBERT 大型版的比较
- en: Conclusion
  id: totrans-52
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: By introducing a new pair of training objectives, StructBERT reaches new limits
    in NLP consistently outperforming BERT on various downstream tasks. It was demonstrated
    that both of the objectives play an indispensable role in the StructBERT setting.
    While the word structural objective mostly enhances the model’s performance on
    single-sentence problems making StructBERT able to reconstruct word order, the
    sentence structural objective improves the ability to understand inter-sentence
    relations which is particularly important for sentence-pair tasks.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 通过引入一对新的训练目标，StructBERT 在 NLP 中达到新的极限，在各种下游任务上始终超越 BERT。研究表明，这两个目标在 StructBERT
    设置中发挥了不可或缺的作用。词结构目标主要提高了模型在单句问题上的性能，使 StructBERT 能够重建词序，句子结构目标则改善了对句间关系的理解，这对句子对任务特别重要。
- en: Resources
  id: totrans-54
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 资源
- en: '[StructBERT: Incorporating Language Structures Into Pre-training for Deep Language
    Understanding](https://arxiv.org/pdf/1908.04577.pdf)'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[StructBERT：将语言结构纳入预训练以实现深度语言理解](https://arxiv.org/pdf/1908.04577.pdf)'
- en: '*All images unless otherwise noted are by the author*'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '*除非另有说明，否则所有图片均为作者提供*'
