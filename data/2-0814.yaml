- en: 'Ensemble of Classifiers: Voting Classifier'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分类器集成：Voting Classifier
- en: 原文：[https://towardsdatascience.com/ensemble-of-classifiers-voting-classifier-ef7f6a5b7795](https://towardsdatascience.com/ensemble-of-classifiers-voting-classifier-ef7f6a5b7795)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/ensemble-of-classifiers-voting-classifier-ef7f6a5b7795](https://towardsdatascience.com/ensemble-of-classifiers-voting-classifier-ef7f6a5b7795)
- en: Combine many different models for better Prediction
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结合多种不同模型以获得更好的预测
- en: '[](https://saptashwa.medium.com/?source=post_page-----ef7f6a5b7795--------------------------------)[![Saptashwa
    Bhattacharyya](../Images/b01238113a1f6b91cb6fb0fbfa50303a.png)](https://saptashwa.medium.com/?source=post_page-----ef7f6a5b7795--------------------------------)[](https://towardsdatascience.com/?source=post_page-----ef7f6a5b7795--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----ef7f6a5b7795--------------------------------)
    [Saptashwa Bhattacharyya](https://saptashwa.medium.com/?source=post_page-----ef7f6a5b7795--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://saptashwa.medium.com/?source=post_page-----ef7f6a5b7795--------------------------------)[![Saptashwa
    Bhattacharyya](../Images/b01238113a1f6b91cb6fb0fbfa50303a.png)](https://saptashwa.medium.com/?source=post_page-----ef7f6a5b7795--------------------------------)[](https://towardsdatascience.com/?source=post_page-----ef7f6a5b7795--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----ef7f6a5b7795--------------------------------)
    [Saptashwa Bhattacharyya](https://saptashwa.medium.com/?source=post_page-----ef7f6a5b7795--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----ef7f6a5b7795--------------------------------)
    ·7 min read·Aug 17, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----ef7f6a5b7795--------------------------------)
    ·阅读时间7分钟·2023年8月17日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/96aa1b7e6415940eeaa32ba8a06bad00.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/96aa1b7e6415940eeaa32ba8a06bad00.png)'
- en: 'Decision Boundaries with Voting Classifier (Image by Author: Codes in References)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Voting Classifier的决策边界（图片来源：作者；代码见参考文献）
- en: The word Ensemble in the context of ML refers to a collection of a finite number
    of ML models (may include ANN), trained for the same task. Usually, the models
    are trained independently and then their predictions are combined.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习的上下文中，Ensemble指的是一个有限数量的机器学习模型的集合（可能包括ANN），这些模型是为同一任务训练的。通常，这些模型是独立训练的，然后将它们的预测结果结合起来。
- en: When the predictions from different models differ, it is sometimes more useful
    to use the ensemble for classification than any individual classifier. Here, we
    would like to combine different classifiers and create an ensemble and then use
    the ensemble for the prediction task. What will be discussed in this post?
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 当不同模型的预测结果不一致时，有时使用集成模型进行分类比任何单个分类器更为有用。在这里，我们希望结合不同的分类器创建一个集成模型，然后使用这个集成模型进行预测任务。本文将讨论什么内容？
- en: Use Sklearn’s VotingClassifier to build an ensemble.
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Sklearn的VotingClassifier来构建一个集成模型。
- en: What is Hard and Soft Voting in VotingClassifier?
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 什么是VotingClassifier中的硬投票和软投票？
- en: Check individual model performance with VotingClassifier.
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用VotingClassifier检查单个模型的性能。
- en: Finally, use GridSearchCV + VotingClassifier to find the best model parameters
    for individual models.
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，使用GridSearchCV + VotingClassifier来寻找单个模型的最佳参数。
- en: Let’s begin!
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始吧！
- en: 'Data Preparation:'
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据准备：
- en: To see an example of VotingClassifier in action, I’m using the [Heart Failure
    Prediction dataset](https://www.kaggle.com/datasets/fedesoriano/heart-failure-prediction)
    (available under open database licensing). Here the task is the binary classification
    for predicting whether a patient with specific attributes may have heart disease
    or not. The dataset has 10 attributes including their age, sex, resting blood
    pressure etc., for data collected over 900 patients. Let’s check some distributions
    for different parameters. We check the ‘ClassLabel’ counts (1 represents heart
    disease, 0 represents healthy), i.e. healthy and ill population as a function
    of Sex.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 为了查看VotingClassifier的实际应用示例，我使用了[心脏病预测数据集](https://www.kaggle.com/datasets/fedesoriano/heart-failure-prediction)（开放数据库许可下提供）。这里的任务是对患者是否可能有心脏病进行二分类预测。数据集包含10个属性，包括年龄、性别、静息血压等，数据来源于900多名患者。让我们检查一些不同参数的分布情况。我们检查‘ClassLabel’的计数（1表示心脏病，0表示健康），即健康和患病人群随性别的变化情况。
- en: '![](../Images/7185779fad5fb131d2b0f07734b630af.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7185779fad5fb131d2b0f07734b630af.png)'
- en: 'Fig. 1: ClassLabel distribution as a function of the sex of the participants.
    (Image by Author; Codes in References).'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：参与者性别对ClassLabel分布的影响。（图片来源：作者；代码见参考文献）。
- en: In general, we see proportionately more Males are ill compared to Females. We
    can also check individual features such as Cholesterol and Resting BP distribution
    as below and we see that both the Cholesterol and Resting BP are higher for ill
    patients, especially for females.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 总的来说，我们看到男性的生病比例相较于女性要高。我们还可以检查个别特征，例如胆固醇和静息血压分布，见下文，我们发现生病患者的胆固醇和静息血压均较高，尤其是女性。
- en: '![](../Images/21752fae1a7b2642006128093b6e3b9d.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/21752fae1a7b2642006128093b6e3b9d.png)'
- en: 'Fig. 2: Cholesterol and Resting BP distributions for Males and Females for
    two different classes: Healthy and Ill (Image by Author).'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：两种不同类别（健康和生病）下男性和女性的胆固醇和静息血压分布（图片来源：作者）。
- en: For numerical features, one can also use Boxplots to see distributions where
    the box represents the range of the central 50% of the data.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 对于数值特征，也可以使用箱线图查看分布，其中箱体表示数据中心 50% 的范围。
- en: '![](../Images/e99db4bff049650e0662767e7bcf0661.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e99db4bff049650e0662767e7bcf0661.png)'
- en: 'Fig. 3: Some Numerical Parameter Distribution via Boxplot (Image by Author).'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：通过箱线图展示的一些数值参数分布（图片来源：作者）。
- en: We see some issues with the dataset here; There are quite a few data points
    with zero Cholesterols and RestingBP. Even though it is indeed possible to have
    [really low cholesterol](https://www.health.harvard.edu/blog/ldl-cholesterol-how-low-can-you-safely-go-2020012018638),
    to have zero for many data points (patients), is it a little too accidental? Anyway,
    as our main objective is to implement VotingClassifier so we leave those data
    points as they are and proceed to the next stage.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在数据集上发现了一些问题；有相当多的数据点胆固醇和静息血压为零。虽然确实有可能存在[非常低的胆固醇](https://www.health.harvard.edu/blog/ldl-cholesterol-how-low-can-you-safely-go-2020012018638)，但很多数据点（患者）都为零，这是否有些过于偶然？不过，由于我们的主要目标是实现
    VotingClassifier，因此我们将这些数据点保留原样，并继续进行下一阶段。
- en: 'Train-Test and Classification:'
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练-测试与分类：
- en: 'After some preliminary analysis, we prepare the dataset for training and testing
    different classifiers and compare their performances. First, we split the data
    into train and test sets, separately standardize the numerical columns (to prevent
    data leakage) and initialize 3 different classifiers- [Support Vector Machines](/understanding-support-vector-machine-part-1-lagrange-multipliers-5c24a52ffc5e),
    [Logistic Regression](/logit-of-logistic-regression-understanding-the-fundamentals-f384152a33d1)
    and AdaBoost, which itself is an Ensemble learning method specifically falling
    under the Boosting method (the other one being known as ‘Bagging’). Next is to
    train these classifiers using the training set and check predictions for the test
    set. We check 3 different performance metrics-precision, recall and F1 score.
    Below is the code block:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在一些初步分析后，我们准备数据集以训练和测试不同的分类器，并比较它们的性能。首先，我们将数据分成训练集和测试集，分别对数值列进行标准化（以防止数据泄漏），并初始化
    3 种不同的分类器——[支持向量机](/understanding-support-vector-machine-part-1-lagrange-multipliers-5c24a52ffc5e)、[逻辑回归](/logit-of-logistic-regression-understanding-the-fundamentals-f384152a33d1)
    和 AdaBoost，后者是一种集成学习方法，特别属于提升（Boosting）方法（另一种被称为‘袋装（Bagging）’）。接下来是使用训练集训练这些分类器，并检查测试集的预测结果。我们检查了
    3 种不同的性能指标——精度、召回率和 F1 分数。以下是代码块：
- en: Check the performances of individual classifiers on the Heart Disease dataset.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 检查心脏病数据集上各个分类器的性能。
- en: 'Below are the scores:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是得分：
- en: '[PRE0]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'We see that on average the F1 scores are around ~89%. One can also check the
    feature importance for the AdaBoost classifier and expectedly Cholesterol is one
    of the main important features. Below is the plot:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到，F1 分数的平均值约为 ~89%。还可以检查 AdaBoost 分类器的特征重要性，胆固醇显然是主要的重要特征之一。下面是图示：
- en: '![](../Images/993d1c247cfa2568eb54b3af171bdef7.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/993d1c247cfa2568eb54b3af171bdef7.png)'
- en: 'Fig. 4: Feature Importance Plot: Considering only AdaBoost classifier, Cholesterol
    is of the highest importance (Image by Author).'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：特征重要性图：仅考虑 AdaBoost 分类器，胆固醇的重要性最高（图片来源：作者）。
- en: 'The confusion matrix for the AdaBoost classifier separately looks as below:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: AdaBoost 分类器的混淆矩阵如下：
- en: '![](../Images/62c3bc6b635dbfd4613f60ac34459387.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/62c3bc6b635dbfd4613f60ac34459387.png)'
- en: 'Fig. 5: Confusion matrix for AdaBoost classifier.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：AdaBoost 分类器的混淆矩阵。
- en: 'VotingClassifier to Form an Ensemble:'
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: VotingClassifier 用于形成集成模型：
- en: VotingClassifier combines different machine learning classifiers and uses a
    voting rule (‘soft’ or ‘hard’) to predict the class labels.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: VotingClassifier 结合了不同的机器学习分类器，并使用投票规则（‘软’或‘硬’）来预测类别标签。
- en: It balances out the individual weaknesses of models when their performance is
    almost the same; here we will combine individual classifiers used before to build
    an ensemble. But what is soft and hard voting?
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 当模型的性能几乎相同时，它平衡了各个模型的个体弱点；在这里，我们将结合之前使用的单独分类器来构建集成模型。但什么是软投票和硬投票呢？
- en: '***Majority (‘Hard’) Voting:*** The predicted class label for a particular
    sample is the class label that represents the majority (mode) of the class labels
    predicted by each individual classifier. Let’s look at the table below for an
    example of a classification task with 3 classes'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '***多数 (‘硬’) 投票：*** 对于特定样本，预测的类别标签是代表每个单独分类器预测的类别标签的多数（众数）。下面的表格展示了一个具有 3 个类别的分类任务示例'
- en: '[PRE1]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Here, SVC and AdaBoost both predict Class3 as the output label and in the Hard
    Voting scenario this will be selected.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，SVC 和 AdaBoost 都预测 Class3 作为输出标签，在硬投票方案下，这将被选中。
- en: '***Soft Voting:*** Soft voting returns the class label as argmax of the sum
    of predicted probabilities; It’s also possible to assign a weight array for the
    classifiers involved in creating the ensemble. Below is an example code block'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '***软投票：*** 软投票返回预测概率之和的最大类别标签；也可以为参与创建集成的分类器分配一个权重数组。下面是一个示例代码块'
- en: '[PRE2]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Once again take a look at the table below for an example classification task
    with weights [1, 1, 1] i.e. all the classifiers are equally weighted
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 再次查看下面的表格，以权重 [1, 1, 1] 为例，即所有分类器的权重相等
- en: '[PRE3]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Here Class3 will be chosen under the ‘Soft’ voting scheme. For more on VotingClassifier
    you can also check the [official Sklearn guide](https://scikit-learn.org/stable/modules/ensemble.html#voting-classifier).
    Let’s now build a VotingClassifier and use this ensemble to train and perform
    predictions on the test set using both ‘Soft’ and ‘Hard’ schemes.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在‘软’投票方案下，这里将选择 Class3。有关 VotingClassifier 的更多信息，您还可以查看 [官方 Sklearn 指南](https://scikit-learn.org/stable/modules/ensemble.html#voting-classifier)。现在，让我们构建一个
    VotingClassifier，并使用这个集成模型在测试集上进行训练和预测，使用‘软’和‘硬’方案。
- en: Using VotingClassifier indeed we could see improvement in the performance metrics
    compared to the individual estimators. Similarly, we can also plot the Confusion
    Matrix to further verify the findings.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 VotingClassifier 确实可以看到相较于单个估算器，性能指标有所改善。同样，我们也可以绘制混淆矩阵以进一步验证结果。
- en: '![](../Images/71bc3a0ddf31de760905ed952f35af96.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/71bc3a0ddf31de760905ed952f35af96.png)'
- en: 'Fig. 6: Confusion Matrix obtained using VotingClassifier with ‘Hard’ voting
    scheme. (Image by Author).'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6：使用 VotingClassifier 和‘硬’投票方案获得的混淆矩阵。（图像由作者提供）。
- en: 'VotingClassifier and GridSearchCV:'
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: VotingClassifier 和 GridSearchCV：
- en: 'It’s also possible to tune the parameters (‘hyperparameters’) of individual
    estimators within VotingClassifier using GridSearchCV. GridSearch exhaustively
    considers all parameter combinations and uses Cross-Validation to find the best
    hyperparameters. We have gone through examples of building an analysis pipeline
    with an estimator like Support Vector Machine, Principal Component Analysis and
    GridSearchCV before [here](https://medium.com/towards-data-science/visualizing-support-vector-machine-decision-boundary-69e7591dacea).
    We show that it is fairly simple to obtain the best parameters of individual estimators
    within VotingClassifier and below is an example code block:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 也可以使用 GridSearchCV 调整 VotingClassifier 中各个估算器的参数（‘超参数’）。GridSearch 会穷尽所有参数组合，并使用交叉验证来找到最佳超参数。我们之前已经通过
    [这里](https://medium.com/towards-data-science/visualizing-support-vector-machine-decision-boundary-69e7591dacea)
    介绍了使用支持向量机、主成分分析和 GridSearchCV 构建分析管道的示例。我们展示了在 VotingClassifier 中获取最佳参数是相当简单的，下面是一个示例代码块：
- en: For parameter space (in line 18), we scour over SVM ‘C’ and ‘gamma’ parameters
    (with a radial basis function kernel) which we have discussed in detail [before](/visualizing-support-vector-machine-decision-boundary-69e7591dacea).
    For the logistic regression, we check over several values of the inverse of the
    regularization strength parameter; finally, for the AdaBoost classifier which
    uses decision trees as base estimators, we check over a few possible values of
    the number of estimators where boosting is terminated.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 对于参数空间（在第 18 行），我们仔细研究了 SVM 的 ‘C’ 和 ‘gamma’ 参数（使用径向基函数核），这些参数我们之前已经详细讨论过 [之前](/visualizing-support-vector-machine-decision-boundary-69e7591dacea)。对于逻辑回归，我们检查了正则化强度参数的多个逆值；最后，对于使用决策树作为基估算器的
    AdaBoost 分类器，我们检查了一些可能的估算器数量值，即 Boosting 终止的值。
- en: '**Conclusions:** In this post through the VotingClassifier, we have discussed
    one of the important concepts in machine learning, ensemble techniques. We build
    an ensemble using individual estimators whose predictions are then combined either
    via ‘Hard’ or ‘Soft’ voting scheme for an example classification task. It is generally
    argued that combining several different estimators may or may not beat the performance
    of the best estimator but the ensemble helps to reduce the risk of selection of
    a poorly performing classifier [1].'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '**结论：** 在这篇文章中，通过VotingClassifier，我们讨论了机器学习中的一个重要概念——集成技术。我们使用个体估计器构建一个集成，这些估计器的预测结果通过‘Hard’或‘Soft’投票方案结合，用于示例分类任务。通常认为，结合几个不同的估计器可能会胜过最佳估计器的表现，但集成有助于减少选择表现不佳的分类器的风险[1]。'
- en: Given the task at your hand, you can certainly consider the VotingClassifier
    ensemble technique than choosing the best classifier out of many.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 根据手头的任务，你可以考虑使用VotingClassifier集成技术，而不是从多个分类器中选择最佳的。
- en: '*Stay strong! Cheers!!*'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '*保持坚强！干杯！！*'
- en: 'References:'
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献：
- en: '[1] *“Ensemble based systems in decision making”*; R. Polikar, IEEExplore,
    DOI: [10.1109/MCAS.2006.1688199](https://doi.org/10.1109/MCAS.2006.1688199).'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] *“决策中的集成系统”*；R. Polikar，IEEExplore，DOI: [10.1109/MCAS.2006.1688199](https://doi.org/10.1109/MCAS.2006.1688199)。'
- en: '[2] Sklearn VotingClassifier: [User Guide](https://scikit-learn.org/stable/modules/ensemble.html#voting-classifier).'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] Sklearn VotingClassifier: [用户指南](https://scikit-learn.org/stable/modules/ensemble.html#voting-classifier)。'
- en: '[3] *“Ensemble-based Classifiers”*; L. Rokach, Artificial Intelligence Review
    33, 1–39 (2010). [Springer](https://link.springer.com/article/10.1007/s10462-009-9124-7).'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] *“基于集成的分类器”*；L. Rokach，人工智能评论 33，1–39 (2010)。 [Springer](https://link.springer.com/article/10.1007/s10462-009-9124-7)。'
- en: '[4] Codes and Notebooks: [GitHub](https://github.com/suvoooo/Machine_Learning/tree/master/VotingClassifier).'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] 代码和笔记本：[GitHub](https://github.com/suvoooo/Machine_Learning/tree/master/VotingClassifier)。'
- en: '[5] Heart Failure Prediction Data; Available Under Open Database License (ODbl);
    [Link](https://www.kaggle.com/datasets/fedesoriano/heart-failure-prediction).'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '[5] 心力衰竭预测数据；在开放数据库许可 (ODbl) 下提供；[链接](https://www.kaggle.com/datasets/fedesoriano/heart-failure-prediction)。'
