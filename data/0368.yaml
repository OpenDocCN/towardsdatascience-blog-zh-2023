- en: Can Reinforcement Learning Generalize Beyond Its Training?
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 强化学习能否超越训练泛化？
- en: 原文：[https://towardsdatascience.com/can-reinforcement-learning-generalize-beyond-its-training-3b9012d8e4cf?source=collection_archive---------20-----------------------#2023-01-24](https://towardsdatascience.com/can-reinforcement-learning-generalize-beyond-its-training-3b9012d8e4cf?source=collection_archive---------20-----------------------#2023-01-24)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/can-reinforcement-learning-generalize-beyond-its-training-3b9012d8e4cf?source=collection_archive---------20-----------------------#2023-01-24](https://towardsdatascience.com/can-reinforcement-learning-generalize-beyond-its-training-3b9012d8e4cf?source=collection_archive---------20-----------------------#2023-01-24)
- en: A case study in model generalization
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型泛化案例研究
- en: '[](https://medium.com/@john_morrow?source=post_page-----3b9012d8e4cf--------------------------------)[![John
    Morrow](../Images/4a8ce62a0b4e1eb1cf77ecaba6b7ddcc.png)](https://medium.com/@john_morrow?source=post_page-----3b9012d8e4cf--------------------------------)[](https://towardsdatascience.com/?source=post_page-----3b9012d8e4cf--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----3b9012d8e4cf--------------------------------)
    [John Morrow](https://medium.com/@john_morrow?source=post_page-----3b9012d8e4cf--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@john_morrow?source=post_page-----3b9012d8e4cf--------------------------------)[![John
    Morrow](../Images/4a8ce62a0b4e1eb1cf77ecaba6b7ddcc.png)](https://medium.com/@john_morrow?source=post_page-----3b9012d8e4cf--------------------------------)[](https://towardsdatascience.com/?source=post_page-----3b9012d8e4cf--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----3b9012d8e4cf--------------------------------)
    [John Morrow](https://medium.com/@john_morrow?source=post_page-----3b9012d8e4cf--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fb4bcd051bb38&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcan-reinforcement-learning-generalize-beyond-its-training-3b9012d8e4cf&user=John+Morrow&userId=b4bcd051bb38&source=post_page-b4bcd051bb38----3b9012d8e4cf---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----3b9012d8e4cf--------------------------------)
    ·6 min read·Jan 24, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F3b9012d8e4cf&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcan-reinforcement-learning-generalize-beyond-its-training-3b9012d8e4cf&user=John+Morrow&userId=b4bcd051bb38&source=-----3b9012d8e4cf---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fb4bcd051bb38&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcan-reinforcement-learning-generalize-beyond-its-training-3b9012d8e4cf&user=John+Morrow&userId=b4bcd051bb38&source=post_page-b4bcd051bb38----3b9012d8e4cf---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----3b9012d8e4cf--------------------------------)
    ·6 min read·2023年1月24日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F3b9012d8e4cf&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcan-reinforcement-learning-generalize-beyond-its-training-3b9012d8e4cf&user=John+Morrow&userId=b4bcd051bb38&source=-----3b9012d8e4cf---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F3b9012d8e4cf&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcan-reinforcement-learning-generalize-beyond-its-training-3b9012d8e4cf&source=-----3b9012d8e4cf---------------------bookmark_footer-----------)![](../Images/7c6eb049b8373330c9019cd75174f39e.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F3b9012d8e4cf&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcan-reinforcement-learning-generalize-beyond-its-training-3b9012d8e4cf&source=-----3b9012d8e4cf---------------------bookmark_footer-----------)![](../Images/7c6eb049b8373330c9019cd75174f39e.png)'
- en: Photo by [János Szüdi](https://unsplash.com/@szudi?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '[János Szüdi](https://unsplash.com/@szudi?utm_source=medium&utm_medium=referral)拍摄于[Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)'
- en: 'The project detailed in the paper, [Reinforcement Learning: A Case Study in
    Model Generalization](https://github.com/jmorrow1000/RL-generalize/blob/main/Reinforcement%20Learning%20-%20A%20Case%20Study%20in%20Model%20Generalization.pdf?raw=true),
    explores the ability of a model trained with reinforcement learning (RL) to generalize,
    i.e., produce acceptable results when presented with data it was not exposed to
    during training. The application in this study is an industrial process with multiple
    controls that determine the effect on a product as it transitions through the
    process. Determining optimal control settings in this environment can be challenging.
    For example, when there are interactions between the controls, adjusting one setting
    can require the readjustment of other settings. Also, a complex relationship between
    a control and its effect complicates finding an optimal solution. The results
    presented here show that a model trained by an RL process performs well in this
    environment and is able to generalize to conditions different from those used
    for training.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 论文中详细描述的项目，[强化学习：模型泛化案例研究](https://github.com/jmorrow1000/RL-generalize/blob/main/Reinforcement%20Learning%20-%20A%20Case%20Study%20in%20Model%20Generalization.pdf?raw=true)，探讨了一个使用强化学习（RL）训练的模型在泛化能力上的表现，即当遇到训练中未接触的数据时，能够产生可接受的结果。本研究中的应用是一个具有多个控制的工业过程，这些控制决定了产品在过程中的过渡效果。在这种环境中确定最佳控制设置可能具有挑战性。例如，当控制之间存在相互作用时，调整一个设置可能需要重新调整其他设置。此外，控制与其效果之间的复杂关系使得找到最佳解决方案变得更加困难。这里展示的结果表明，经过RL过程训练的模型在这种环境下表现良好，并能够泛化到不同于训练条件的情况下。
- en: The [paper](https://morrowconsultants.com/rl-model-generalization-paper) describes
    an RL model trained to find the optimal control settings for a reflow oven used
    for soldering electronic components to a circuit board (Figure 1). The oven’s
    moving belt transports the product (i.e., the circuit board) through multiple
    heating zones. This process heats the product according to a temperature-time
    target profile required to produce reliable solder connections.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '[论文](https://morrowconsultants.com/rl-model-generalization-paper)描述了一种RL模型，该模型被训练以寻找用于将电子组件焊接到电路板上的回流焊炉的最佳控制设置（图1）。烤箱的移动传送带将产品（即电路板）运输通过多个加热区域。此过程根据所需的温度-时间目标曲线加热产品，以生产可靠的焊接连接。'
- en: '![](../Images/449434f952710cedc7ac09afa5d05594.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/449434f952710cedc7ac09afa5d05594.png)'
- en: 'Figure 1: **Circuit boards on oven belt** (*Image via Adobe under license to
    John Morrow*)'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：**烤箱传送带上的电路板**（*图像来自Adobe，授权给John Morrow*）
- en: 'A human operator typically takes the following steps to determine the heater
    settings required to solder circuit boards successfully:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 人工操作员通常采取以下步骤来确定成功焊接电路板所需的加热器设置：
- en: • run one pass of the product through the oven
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: • 运行一次产品通过烤箱的过程
- en: • observe the resulting temperature-time profile from the sensor readings
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: • 观察传感器读数中的温度-时间曲线
- en: • adjust the heater settings to improve the profile toward the target profile
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: • 调整加热器设置以改善曲线，接近目标曲线
- en: • wait for the oven temperature to stabilize to the new settings
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: • 等待烤箱温度稳定到新的设置
- en: • repeat this procedure until the profile from the sensor readings is acceptably
    close to the target profile
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: • 重复此过程，直到传感器读数中的曲线足够接近目标曲线
- en: '**Learning the policy**'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '**学习策略**'
- en: An RL system replaces the operator steps with a two-stage process. In the first
    stage, an agent learns the dynamics of the oven and creates a policy for updating
    the heater settings under various oven conditions.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 一个RL系统用两阶段过程取代了操作员步骤。在第一阶段，代理学习烤箱的动态，并在各种烤箱条件下创建更新加热器设置的策略。
- en: Since considerable time is required to stabilize an oven’s temperature after
    changing the heater settings and passing the product through the oven, an oven
    simulator is used to speed up the learning process. The simulator emulates a single
    pass of the product through the heating profile in a few seconds instead of the
    many minutes required by a physical oven. ([Part 2](https://medium.com/@john_morrow/can-reinforcement-learning-generalize-beyond-its-training-part-2-79d7b864dc55)
    presents the details of the simulator.)
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 由于在改变加热器设置并将产品通过烤箱后，需要相当长的时间来稳定烤箱的温度，因此使用了烤箱模拟器以加速学习过程。模拟器在几秒钟内模拟了产品通过加热曲线的单次过程，而物理烤箱则需要许多分钟。（[第二部分](https://medium.com/@john_morrow/can-reinforcement-learning-generalize-beyond-its-training-part-2-79d7b864dc55)提供了模拟器的详细信息。）
- en: In each pass of the learning stage, the agent takes an action from its current
    state by sending the simulator new settings for the eight heaters. After the simulation
    run, the simulator reports back the product temperature readings (three hundred
    readings taken at 1-second intervals).
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在每一轮学习阶段中，代理从其当前状态采取行动，向模拟器发送八个加热器的新设置。模拟运行后，模拟器报告产品温度读数（每秒采集三百个读数）。
- en: The agent is rewarded for its action based on the difference between the returned
    readings and the target temperature-time profile. If the difference for the current
    run is less than the previous run, the reward is positive; otherwise, it is negative.
    A subset of the readings determines the new state of the system. The agent starts
    the next pass of the learning stage by taking action from the new state.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 代理的奖励基于返回读数与目标温度-时间曲线之间的差异。如果当前运行的差异小于之前的差异，则奖励为正；否则，奖励为负。一部分读数决定系统的新状态。代理通过从新状态中采取行动来开始学习阶段的下一轮。
- en: '**Planning with the policy**'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '**使用策略进行规划**'
- en: In the second stage, the agent follows the learned policy to find optimal heater
    settings. These settings will produce the closest match between the actual product
    profile and the target temperature-time profile. Figure 2 shows the result of
    the agent following the policy to find optimal settings. The blue trace is the
    target temperature-time profile, and the red trace is the actual profile produced
    by the optimal settings.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在第二阶段，代理按照学习到的策略寻找最佳加热器设置。这些设置将使实际产品曲线与目标温度-时间曲线之间的匹配最接近。图2展示了代理遵循策略找到最佳设置的结果。蓝色轨迹是目标温度-时间曲线，红色轨迹是由最佳设置产生的实际曲线。
- en: '![](../Images/ae5249872da7b2f455c158de0603140e.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ae5249872da7b2f455c158de0603140e.png)'
- en: 'Figure 2: **Example planning result Blue trace: target profile. Red trace:
    actual product profile.**'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：**示例规划结果 蓝色轨迹：目标曲线。红色轨迹：实际产品曲线。**
- en: '**Reinforcement learning system**'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '**强化学习系统**'
- en: 'As discussed above, an RL system comprises an agent taking action in an environment
    to learn a policy to reach the target goal. The environment responds to each action
    with a reward indicating whether the action was good or bad toward achieving the
    goal. The environment also returns the state of the agent in the environment.
    The agent consists of two neural networks: the model network and the target network.
    The agent’s goal is to find heater settings that will produce a product time-temperature
    profile very close to the target profile. The environment is the reflow oven simulator.
    Figure 3shows the components of the RL system, each of which is described in detail
    in the paper.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 如上所述，RL系统包括一个代理在环境中采取行动，以学习一个策略来实现目标。环境对每个行动作出回应，提供奖励以指示行动是否有助于实现目标。环境还会返回代理在环境中的状态。代理由两个神经网络组成：模型网络和目标网络。代理的目标是找到加热器设置，使得生产的产品时间-温度曲线与目标曲线非常接近。环境是回流炉模拟器。图3显示了RL系统的组成部分，文中对每个部分进行了详细描述。
- en: '![](../Images/67227dda344339d0894f8f6dfddb2cb8.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/67227dda344339d0894f8f6dfddb2cb8.png)'
- en: 'Figure 3: **Reinforcement learning system**'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：**强化学习系统**
- en: '**Generalization: state and reward definition**'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '**泛化：状态和奖励定义**'
- en: The state and reward definitions are critical to the RL model’s ability to generalize
    to new environments where the target profile and product parameters differ from
    those used during training. Specifically, both the state and reward are defined
    in terms of the relative difference between the product and target profile temperatures
    and normalized by the maximum range of allowed heater values.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 状态和奖励定义对于RL模型在目标曲线和产品参数与训练期间使用的参数不同的新环境中的泛化能力至关重要。具体来说，状态和奖励都是根据产品和目标曲线温度之间的相对差异定义的，并通过允许的加热器值的最大范围进行规范化。
- en: State parameters are defined at the centers of the eight heater zones. Each
    state parameter is defined as the normalized difference between the temperature
    at the center of the product and the temperature of the profile at the center
    of each heater zone.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 状态参数在八个加热器区域的中心定义。每个状态参数被定义为产品中心温度与每个加热器区域中心温度的规范化差异。
- en: When the agent performs an action, the environment returns a reward indicating
    the effectiveness of the action in achieving the agent’s goal. The reward is based
    on whether the action reduced the total temperature difference between the actual
    temperatures and target profile. The state and reward functions are described
    in greater detail in the paper.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 当代理执行一个动作时，环境会返回一个奖励，表明该动作在实现代理目标方面的有效性。奖励是基于该动作是否减少了实际温度与目标曲线之间的总温差。状态和奖励函数在论文中有更详细的描述。
- en: '**Results**'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '**结果**'
- en: 'Following are two of the paper’s test results from running the planning process
    on various configurations of product materials and temperature-time profiles.
    All of the tests were run with a model neural network trained with the following
    product and profile parameters:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是论文中在不同产品材料配置和温度-时间曲线下运行规划过程的两个测试结果。所有测试都使用了一个模型神经网络，该网络经过以下产品和曲线参数的训练：
- en: '![](../Images/65ee37070838ea349f597be3f2bedfad.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/65ee37070838ea349f597be3f2bedfad.png)'
- en: '**Test 1: Baseline**'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '**测试 1：基准测试**'
- en: 'Test 1 is a baseline for testing the model’s performance with the same parameters
    used to train the model. Following are the test 1 errors, the optimal heat zone
    settings, and the target profile vs. actual temperature-time plot:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 测试 1 作为测试模型性能的基准，使用与训练模型时相同的参数。以下是测试 1 的误差、最佳热区设置以及目标曲线与实际温度-时间图：
- en: '![](../Images/3e4804da2b61492040372400ab03a69e.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3e4804da2b61492040372400ab03a69e.png)'
- en: '**Test 1: Blue trace: target profile. Red trace: actual product profile.**'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '**测试 1：蓝色曲线：目标曲线。红色曲线：实际产品曲线。**'
- en: '**Test 6**'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '**测试 6**'
- en: 'Test 6 changes the product from FR4 to aluminum oxide (alumina 99%), changes
    the size of the product, and changes the profile. The oven parameter values are
    the same as used in the baseline of test 1, except that both the top and bottom
    heating elements are active. The following tables reflect the profile and product
    parameters used for this test (changes from the baseline training parameters are
    bold):'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 测试 6 将产品从 FR4 更改为氧化铝（铝土矿 99%），更改了产品的尺寸，并且更改了曲线。烤箱参数值与测试 1 的基准相同，只是顶部和底部加热元件都处于活动状态。以下表格反映了该测试所用的曲线和产品参数（相对于基准训练参数的更改为粗体）：
- en: '![](../Images/ed10da52ad97359a26fb5aee47904a36.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ed10da52ad97359a26fb5aee47904a36.png)'
- en: 'Following are the test 6 errors, the optimal heat zone settings, and the target
    profile vs. actual temperature/time plot:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是测试 6 的误差、最佳热区设置以及目标曲线与实际温度/时间图：
- en: '![](../Images/22a233e42efc39a0548e61a13f3038d3.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/22a233e42efc39a0548e61a13f3038d3.png)'
- en: '**Conclusion**'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '**结论**'
- en: This project demonstrates that a reinforcement learning system can provide solutions
    to control a complex industrial process. Specifically, a reinforcement learning
    system successfully learns the optimal control settings of a reflow oven used
    to solder electronic components to a circuit board. Further, once trained, the
    system can generalize to produce acceptable results in environments with different
    requirements from those used during training.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 本项目展示了强化学习系统如何为控制复杂的工业过程提供解决方案。具体来说，强化学习系统成功学习了用于将电子组件焊接到电路板上的回流焊炉的最佳控制设置。此外，一旦训练完成，系统可以泛化到在与训练时不同要求的环境中产生可接受的结果。
- en: 'Link to paper: [Reinforcement Learning: A Case Study in Model Generalization](https://github.com/jmorrow1000/RL-generalize/blob/main/Reinforcement%20Learning%20-%20A%20Case%20Study%20in%20Model%20Generalization.pdf?raw=true)'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 论文链接：[强化学习：模型泛化的案例研究](https://github.com/jmorrow1000/RL-generalize/blob/main/Reinforcement%20Learning%20-%20A%20Case%20Study%20in%20Model%20Generalization.pdf?raw=true)
- en: All images, unless otherwise noted, are by the author.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 除非另有说明，所有图片均为作者所用。
