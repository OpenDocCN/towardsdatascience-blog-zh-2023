- en: Why and How to Achieve Longer Context Windows for LLMs
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为什么以及如何实现更长的 LLM 上下文窗口
- en: 原文：[https://towardsdatascience.com/why-and-how-to-achieve-longer-context-windows-for-llms-5f76f8656ea9?source=collection_archive---------1-----------------------#2023-10-13](https://towardsdatascience.com/why-and-how-to-achieve-longer-context-windows-for-llms-5f76f8656ea9?source=collection_archive---------1-----------------------#2023-10-13)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/why-and-how-to-achieve-longer-context-windows-for-llms-5f76f8656ea9?source=collection_archive---------1-----------------------#2023-10-13](https://towardsdatascience.com/why-and-how-to-achieve-longer-context-windows-for-llms-5f76f8656ea9?source=collection_archive---------1-----------------------#2023-10-13)
- en: '[](https://medium.com/@ddxzzx?source=post_page-----5f76f8656ea9--------------------------------)[![Davide
    Ghilardi](../Images/1643cb58465144e914c545bbca4359b4.png)](https://medium.com/@ddxzzx?source=post_page-----5f76f8656ea9--------------------------------)[](https://towardsdatascience.com/?source=post_page-----5f76f8656ea9--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----5f76f8656ea9--------------------------------)
    [Davide Ghilardi](https://medium.com/@ddxzzx?source=post_page-----5f76f8656ea9--------------------------------)'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@ddxzzx?source=post_page-----5f76f8656ea9--------------------------------)[![Davide
    Ghilardi](../Images/1643cb58465144e914c545bbca4359b4.png)](https://medium.com/@ddxzzx?source=post_page-----5f76f8656ea9--------------------------------)[](https://towardsdatascience.com/?source=post_page-----5f76f8656ea9--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----5f76f8656ea9--------------------------------)
    [Davide Ghilardi](https://medium.com/@ddxzzx?source=post_page-----5f76f8656ea9--------------------------------)'
- en: ·
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F2907a3374fa5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhy-and-how-to-achieve-longer-context-windows-for-llms-5f76f8656ea9&user=Davide+Ghilardi&userId=2907a3374fa5&source=post_page-2907a3374fa5----5f76f8656ea9---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----5f76f8656ea9--------------------------------)
    ·7 min read·Oct 13, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F5f76f8656ea9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhy-and-how-to-achieve-longer-context-windows-for-llms-5f76f8656ea9&user=Davide+Ghilardi&userId=2907a3374fa5&source=-----5f76f8656ea9---------------------clap_footer-----------)'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F2907a3374fa5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhy-and-how-to-achieve-longer-context-windows-for-llms-5f76f8656ea9&user=Davide+Ghilardi&userId=2907a3374fa5&source=post_page-2907a3374fa5----5f76f8656ea9---------------------post_header-----------)
    发布于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----5f76f8656ea9--------------------------------)
    ·7 min read·2023年10月13日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F5f76f8656ea9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhy-and-how-to-achieve-longer-context-windows-for-llms-5f76f8656ea9&user=Davide+Ghilardi&userId=2907a3374fa5&source=-----5f76f8656ea9---------------------clap_footer-----------)'
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '-- '
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F5f76f8656ea9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhy-and-how-to-achieve-longer-context-windows-for-llms-5f76f8656ea9&source=-----5f76f8656ea9---------------------bookmark_footer-----------)'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F5f76f8656ea9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhy-and-how-to-achieve-longer-context-windows-for-llms-5f76f8656ea9&source=-----5f76f8656ea9---------------------bookmark_footer-----------)'
- en: Language models (LLMs) have revolutionized the field of natural language processing
    (NLP) over the last few years, achieving state-of-the-art results on a wide range
    of tasks. However, a key challenge in developing and improving these models lies
    in extending the length of their context. This is very important since it determines
    how much information is available to the model when generating an output.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 语言模型（LLMs）在过去几年中彻底改变了自然语言处理（NLP）领域，在各种任务中取得了最先进的成果。然而，开发和改进这些模型的一个关键挑战在于延长它们的上下文长度。这非常重要，因为它决定了模型在生成输出时可用的信息量。
- en: However, increasing the context window of a LLM isn’t so simple. In fact, it
    comes at the cost of increased computational complexity, since the attention matrix
    grows quadratically.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，增加 LLM 的上下文窗口并非易事。实际上，这会带来计算复杂度的增加，因为注意力矩阵呈二次方增长。
- en: One solution could be training the model on a large amount of data on a relatively
    small window (e.g. 4K tokens) and then fine-tuning it on a bigger one (e.g. 64K
    tokens). This operation isn’t straightforward, because even though the context
    length doesn't impact the number of model’s weights, it does affect how **positional
    information** of tokens is encoded by those weights in the tokens’ dense representation.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 一种解决方案可能是用相对较小的窗口（例如4K个令牌）在大量数据上训练模型，然后在更大的窗口（例如64K个令牌）上进行微调。这一操作并不简单，因为尽管上下文长度不会影响模型的权重数量，但它确实影响这些权重在令牌稠密表示中如何编码**位置的信息**。
- en: This reduces the model’s capacity to adapt to longer context windows even after
    fine-tuning, resulting in poor performance and thus requiring new techniques to
    encode positional information correctly and dynamically between training and fine-tuning.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 这减少了模型在长上下文窗口下适应的能力，即使在微调之后也会导致性能不佳，因此需要新的技术来正确和动态地编码位置的信息，在训练和微调之间。
- en: Absolute positional encoding
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 绝对位置编码
- en: In transformers, the information about the position of each token is encoded
    before the token is fed to the attention heads. It’s a crucial step since transformers,
    differently from RNNs, don’t keep track of the tokens’ position.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在transformers中，每个令牌的位置信息在令牌输入到注意力头之前被编码。这是一个关键步骤，因为与RNN不同，transformers不会跟踪令牌的位置。
- en: The original transformer architecture [[1](#d8de)] encodes positions as vectors
    of the same shape of tokens’ embeddings so that they can be **added** together.
    In particular, they used a combination of cos/sin waves with lengths increasing
    from low to higher order dimensions of the embedding.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 原始transformer架构 [[1](#d8de)] 将位置编码为与令牌嵌入相同形状的向量，以便它们可以**相加**在一起。特别是，他们使用了一个cos/sin波的组合，其长度从低维到高维嵌入的顺序递增。
- en: '![](../Images/ba7885dd0e0a7826e17a67e0dc250355.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ba7885dd0e0a7826e17a67e0dc250355.png)'
- en: Image by author
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供
- en: This method allows an efficient unique dense representation of all the tokens’
    positions. However, it doesn’t solve the challenge of extending context length
    since these representations can't efficiently encode relative positions of tokens.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法允许对所有令牌位置进行高效的唯一稠密表示。然而，它不能解决扩展上下文长度的挑战，因为这些表示不能有效地编码令牌的相对位置。
- en: 'To see why this happens, let''s focus on a single couple of two consecutive
    dimensions. If we plot them on a chart, we can represent token and position embeddings
    as 2D vectors, as shown in the following figure:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 为了了解为什么会发生这种情况，我们来关注一对连续的维度。如果我们在图表上绘制它们，我们可以将令牌和位置嵌入表示为二维向量，如下图所示：
- en: '![](../Images/c2c045db49e24a0fde1c83c690ef7775.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c2c045db49e24a0fde1c83c690ef7775.png)'
- en: '*Image by author*'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '*图片由作者提供*'
- en: 'The figure above includes two charts: on the left, we have the embedding space,
    and on the right, there’s the key/query space. Embedding vectors are the black
    ones, positional vectors are the blue ones, and their sum is coloured in green.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 上图包含两个图表：左侧是嵌入空间，右侧是键/查询空间。嵌入向量是黑色的，位置向量是蓝色的，它们的和用绿色表示。
- en: To shift from embeddings to keys (K) and queries (Q), the transformer applies
    some linear transformations defined by the two matrices **W**q and **W**k. Thanks
    to the linearity property, the transformation can be applied separately between
    embeddings and positional vectors. In the KQ-space the attention is computed as
    the dot product between keys and queries, and in the figure, it’s represented
    by the yellow area between the two green vectors.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 为了从嵌入转到键（K）和查询（Q），transformer应用了由两个矩阵**W**q和**W**k定义的线性变换。由于线性性质，该变换可以在嵌入和位置向量之间分别应用。在KQ空间中，注意力作为键和查询之间的点积计算，图中用黄色区域表示在两个绿色向量之间。
- en: The relative distance between tokens is not directly accessible to the model
    since rotations mix up differently based on the original orientations of the tokens’
    and positions’ embeddings, and on how they’re scaled in the transformations.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 由于旋转的混合方式不同，基于令牌和位置嵌入的原始方向，以及它们在变换中的缩放方式，模型无法直接访问令牌之间的相对距离。
- en: It’s also important to note that, in this case, the addition is applied before
    the transformation and not vice-versa since position embeddings have to be scaled
    by the linear layer.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 同样重要的是，在这种情况下，加法是在变换之前应用的，而不是反过来，因为位置嵌入需要通过线性层进行缩放。
- en: Relative positional encoding
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 相对位置编码
- en: To efficiently encode relative position information among tokens other methods
    have been proposed. We will focus on **RoPE** [[2](#d8de)], which stands for **Ro**tary
    **P**osition **E**mbedding, and for which extensions for longer context windows
    have been proposed.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 为了高效编码标记之间的相对位置信息，已经提出了其他方法。我们将重点关注**RoPE** [[2](#d8de)]，即**Ro**tary **P**osition
    **E**mbedding，已经为更长的上下文窗口提出了扩展方法。
- en: 'The two main innovations of RoPE are:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: RoPE 的两个主要创新是：
- en: With RoPE, we can make the dot product between keys and queries embeddings sensitive
    only to the **relative distance** between them.
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 RoPE，我们可以使键和值嵌入之间的点积仅对它们的**相对距离**敏感。
- en: Positional embeddings are **multiplied** by the tokens’ embeddings without the
    need for a fixed-size look-up table.
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 位置嵌入被**乘以**标记的嵌入，而无需固定大小的查找表。
- en: To achieve this goal, first, we map tokens’ embeddings from the real D-dimensional
    space to a complex D/2-dimensional one and apply the rotation in that space.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现这一目标，首先，我们将标记的嵌入从实际的 D 维空间映射到一个复数 D/2 维空间，并在该空间中应用旋转。
- en: 'Again, let’s consider a single couple of two consecutive dimensions. If we
    plot them on the complex plane (setting the first on the real axis and the second
    on the imaginary one), we can represent token embeddings as complex vectors, as
    shown in the following figure:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 再次考虑一对连续的维度。如果我们将它们绘制在复数平面上（将第一个设置在实轴上，第二个设置在虚轴上），我们可以将标记嵌入表示为复数向量，如下图所示：
- en: '![](../Images/0aee7722d74ac1e4feed129c77317423.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0aee7722d74ac1e4feed129c77317423.png)'
- en: '*Image by author*'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '*图片来源：作者*'
- en: Here positional encodings acquire a new meaning, in fact, they become rotations
    directly applied to the vectors. This change allows them to both uniquely encode
    the position of the token, and be preserved in the linear transformation.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，位置编码获得了新的含义，实际上，它们变成了直接应用于向量的旋转。这一变化使它们能够唯一地编码标记的位置，并在线性变换中得到保留。
- en: 'This last property is fundamental to incorporate the relative positions in
    the computation of attention. In fact, if we consider the attentions between the
    unrotated (in black) and rotated (in green) versions of **k**ₙ and **q**ₙ₊ₖ, represented
    respectively by the orange and yellow angles in the right chart, we can note something
    interesting:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 这一最后的特性对于在注意力计算中纳入相对位置至关重要。实际上，如果我们考虑未旋转（黑色）和旋转（绿色）版本的**k**ₙ 和 **q**ₙ₊ₖ 之间的注意力，如右侧图表中分别由橙色和黄色角度表示，我们可以注意到一些有趣的现象：
- en: '![](../Images/8fdcabc8b8b3a1a6dec25f60b52eb584.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8fdcabc8b8b3a1a6dec25f60b52eb584.png)'
- en: '*Image by author*'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '*图片来源：作者*'
- en: The attention between the rotated keys and queries differs from the unrotated
    version only by a factor proportional to the difference between their positions!
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 旋转的键和值之间的注意力仅因其位置差异成比例地不同于未旋转版本！
- en: This new property allows models trained with RoPE to show **rapid convergence**
    and **lower losses** and lies at the heart of some popular techniques to extend
    the context window beyond the training set.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 这一新特性使得使用 RoPE 训练的模型显示出**快速收敛**和**更低的损失**，并且是一些流行技术的核心，这些技术可以将上下文窗口扩展到训练集之外。
- en: Until now we have always kept embedding dimensions fixed, but to see the whole
    framework, it’s also important to observe what happens along that direction.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们始终保持嵌入维度固定，但为了看到整个框架，还需要观察沿该方向发生的情况。
- en: '![](../Images/757e569819126fd9ce4a772d5b7975dc.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/757e569819126fd9ce4a772d5b7975dc.png)'
- en: '*Image by author*'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '*图片来源：作者*'
- en: The RoPE formula defines the angle of rotation for the *d*-th dimension proportional
    to *exp(-d)* so, as we can see in the figure above, as *d* grows, the rotation
    applied to the embedding vector decreases exponentially (for fixed values of n
    and position). This feature of RoPE lets the model shift the type of information
    encoded in the embeddings from low-frequency (close tokens associations) to high-frequency
    (far tokens associations) by going from low to higher dimensions.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: RoPE 公式定义了第 *d* 维的旋转角度，与 *exp(-d)* 成正比。因此，正如我们在上图中看到的，随着 *d* 的增加，应用于嵌入向量的旋转呈指数下降（对于固定的
    n 和位置）。这一特性使模型能够通过从低维到高维，转变嵌入中编码的信息类型，从低频（近标记关联）到高频（远标记关联）。
- en: RoPE extensions
  id: totrans-43
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: RoPE 扩展
- en: Once we have efficiently incorporated relative position information inside our
    model, the most straightforward way to increase the context window L of our LLM
    is by fine-tuning with **position interpolation** (**PI**) [[3](#d8de)].
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们有效地将相对位置信息整合到模型中，增加我们LLM的上下文窗口L的最直接方法是通过**位置插值**（**PI**） [[3](#d8de)]进行微调。
- en: It is a simple technique that scales tokens' position to fit the new context
    length. So, for example, if we decide to double it, all positions will be divided
    in half. In particular, for any context length L* > L we want to achieve, we can
    define a scale factor *s* = L/ L* < 1.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一种简单的技术，将标记的位置缩放以适应新的上下文长度。例如，如果我们决定将其翻倍，所有位置将被缩小一半。特别地，对于我们想要实现的任何上下文长度L*
    > L，我们可以定义一个缩放因子*s* = L/ L* < 1。
- en: Although this technique has shown promising results by successfully extending
    the context of LLM with fine-tuning on a relatively small amount of tokens, it
    has its own drawbacks.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这种技术通过在相对较少的标记上进行微调成功地扩展了LLM的上下文，并显示了有希望的结果，但它也有其自身的缺点。
- en: One of them is that it slightly decreases performance (e.g. perplexity increases)
    for short context sizes after fine-tuning on larger ones. This issue happens because
    by scaling by *s <* 1 the position of tokens (and so also their relative distances),
    we reduce the rotation applied to the vectors, causing the loss of high-frequency
    information. Thus, the model is less able to recognize small rotations and so
    to figure out the positional order of close-by tokens.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 其中之一是，它在对较大上下文进行微调后，略微降低了短上下文大小的性能（例如，困惑度增加）。这个问题发生是因为通过缩放*s <* 1标记的位置（以及它们的相对距离），我们减少了对向量的旋转，导致高频信息的丧失。因此，模型较难识别小旋转，从而难以确定接近标记的位置信序。
- en: To solve this problem, we can apply a clever mechanism called **NTK-aware**
    [[4](#d8de)] positional interpolation that instead of scaling every dimension
    of RoPE equally by *s*, spreads out the interpolation pressure across multiple
    dimensions by scaling high frequencies less and low frequencies more.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这个问题，我们可以应用一种巧妙的机制，称为**NTK-aware** [[4](#d8de)]位置插值，它不是将每个RoPE维度均等地缩放*s*，而是通过对高频率缩放较少，对低频率缩放较多来分散插值压力。
- en: Other PI extensions exist such as the **NTK-by-parts** [[5](#d8de)] and **dynamic
    NTK** [[6](#d8de)]methods. The first imposes two thresholds to limit the scaling
    above and below certain dimensions; the second dynamically adjusts *s* during
    inference.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 其他PI扩展方法还包括**NTK-by-parts** [[5](#d8de)]和**dynamic NTK** [[6](#d8de)]方法。前者施加两个阈值，以限制在某些维度上的缩放；后者在推理过程中动态调整*s*。
- en: Finally, since it was observed that as the number of tokens increases, the attention
    softmax distribution becomes more and more “spikier” (the average entropy of attention
    softmax decreases), **YaRN** [[7](#d8de)] (**Y**et **a**nother **R**oPE extensio**N**
    method) is a method that inverts this process by multiplying the attention matrix
    by a temperature factor *t* before the application of the softmax.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，由于观察到随着标记数量的增加，注意力softmax分布变得越来越“尖锐”（注意力softmax的平均熵减少），**YaRN** [[7](#d8de)]（**Y**et
    **a**nother **R**oPE extensio**N** method）是一种通过在应用softmax之前将注意力矩阵乘以温度因子*t*来逆转这一过程的方法。
- en: Here’s a look at what these methods do from both position (numbers of rotations)
    and dimension (degree per single rotation) perspectives.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是这些方法在位置（旋转次数）和维度（每次旋转的度数）视角下的操作方式。
- en: '![](../Images/bf9c6d315ad5524bc56e483f555771e7.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bf9c6d315ad5524bc56e483f555771e7.png)'
- en: '*Image by author*'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '*作者提供的图像*'
- en: Other methods
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 其他方法
- en: 'Finally, as we said before, other context extension methods exist, here’s a
    brief description of the most popular ones and how they operate:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，如前所述，其他上下文扩展方法也存在，这里简要描述了最流行的一些方法及其操作方式：
- en: 'Alibi [[8](#d8de)]: It’s another method for positional encoding which penalizes
    the attention value that that query can assign to the key depending on how far
    away the key and query are.'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Alibi [[8](#d8de)]：这是一种位置编码的另一种方法，根据键和值的距离来惩罚查询可以分配给键的注意力值。
- en: 'XPos [[9](#d8de)]: Again another positional encoding method that generalizes
    RoPE to include a scaling factor.'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: XPos [[9](#d8de)]：另一种位置编码方法，它将RoPE推广到包括一个缩放因子。
- en: References
  id: totrans-58
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: Vaswani et al, 2017\. Attention Is All You Need. [link](https://arxiv.org/abs/1706.03762)
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Vaswani等人，2017年。《Attention Is All You Need》。 [link](https://arxiv.org/abs/1706.03762)
- en: 'Su et al, 2022\. RoFormer: Enhanced transformer with rotary position embedding.
    [link](https://arxiv.org/abs/2104.09864)'
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Su 等人, 2022\. RoFormer：具有旋转位置嵌入的增强型变压器。[link](https://arxiv.org/abs/2104.09864)
- en: Chen et al, 2023\. Extending context window of large language models via positional
    interpolation. [link](https://arxiv.org/abs/2306.15595)
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Chen 等人, 2023\. 通过位置插值扩展大型语言模型的上下文窗口。[link](https://arxiv.org/abs/2306.15595)
- en: bloc97, 2023\. NTK-Aware Scaled RoPE allows LLaMA models to have extended (8k+)
    context size without any fine-tuning and minimal perplexity degradation. [link](https://www.reddit.com/r/LocalLLaMA/comments/14lz7j5/ntkaware_scaled_rope_allows_llama_models_to_have/)
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: bloc97, 2023\. NTK-Aware Scaled RoPE 使 LLaMA 模型在无需任何微调且最小化困惑度下降的情况下，能够拥有扩展（8k+）的上下文大小。[link](https://www.reddit.com/r/LocalLLaMA/comments/14lz7j5/ntkaware_scaled_rope_allows_llama_models_to_have/)
- en: bloc97, 2023\. Add NTK-Aware interpolation “by parts” correction. [link](https://github.com/jquesnelle/yarn/pull/1)
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: bloc97, 2023\. 添加 NTK-Aware 插值“分段”修正。[link](https://github.com/jquesnelle/yarn/pull/1)
- en: emozilla, 2023\. Dynamically Scaled RoPE further increases performance of long
    context LLaMA with zero fine-tuning. [link](https://www.reddit.com/r/LocalLLaMA/comments/14mrgpr/dynamically_scaled_rope_further_increases/)
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: emozilla, 2023\. 动态缩放的 RoPE 进一步提升了长上下文 LLaMA 的性能，无需任何微调。[link](https://www.reddit.com/r/LocalLLaMA/comments/14mrgpr/dynamically_scaled_rope_further_increases/)
- en: 'Peng et al, 2023\. YaRN: Efficient Context Window Extension of Large Language
    Models. [link](https://arxiv.org/abs/2309.00071)'
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Peng 等人, 2023\. YaRN：大型语言模型的高效上下文窗口扩展。[link](https://arxiv.org/abs/2309.00071)
- en: 'Press et al, 2022\. Train Short, Test Long: Attention with linear biases enables
    input length extrapolation. [link](https://openreview.net/forum?id=R8sQPpGCv0)'
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Press 等人, 2022\. 训练短，测试长：具有线性偏差的注意力机制实现了输入长度的外推。[link](https://openreview.net/forum?id=R8sQPpGCv0)
- en: Sun et al, 2022\. A Length-Extrapolatable Transformer. [link](https://arxiv.org/abs/2212.10554)
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Sun 等人, 2022\. 一个长度可外推的变压器。[link](https://arxiv.org/abs/2212.10554)
