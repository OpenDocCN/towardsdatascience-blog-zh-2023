- en: The Matrix Algebra of Linear Regression
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 线性回归的矩阵代数
- en: 原文：[https://towardsdatascience.com/the-matrix-algebra-of-linear-regression-6fb433f522d5?source=collection_archive---------3-----------------------#2023-05-03](https://towardsdatascience.com/the-matrix-algebra-of-linear-regression-6fb433f522d5?source=collection_archive---------3-----------------------#2023-05-03)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/the-matrix-algebra-of-linear-regression-6fb433f522d5?source=collection_archive---------3-----------------------#2023-05-03](https://towardsdatascience.com/the-matrix-algebra-of-linear-regression-6fb433f522d5?source=collection_archive---------3-----------------------#2023-05-03)
- en: Looking under the hood at the matrix operations behind linear regression
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 查看线性回归背后的矩阵运算
- en: '[](https://medium.com/@dataforyou?source=post_page-----6fb433f522d5--------------------------------)[![Rob
    Taylor, PhD](../Images/5e4e86da7b77404ed42d00a60ea5eacf.png)](https://medium.com/@dataforyou?source=post_page-----6fb433f522d5--------------------------------)[](https://towardsdatascience.com/?source=post_page-----6fb433f522d5--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----6fb433f522d5--------------------------------)
    [Rob Taylor, PhD](https://medium.com/@dataforyou?source=post_page-----6fb433f522d5--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@dataforyou?source=post_page-----6fb433f522d5--------------------------------)[![Rob
    Taylor, PhD](../Images/5e4e86da7b77404ed42d00a60ea5eacf.png)](https://medium.com/@dataforyou?source=post_page-----6fb433f522d5--------------------------------)[](https://towardsdatascience.com/?source=post_page-----6fb433f522d5--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----6fb433f522d5--------------------------------)
    [Rob Taylor, PhD](https://medium.com/@dataforyou?source=post_page-----6fb433f522d5--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F98de080592fc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-matrix-algebra-of-linear-regression-6fb433f522d5&user=Rob+Taylor%2C+PhD&userId=98de080592fc&source=post_page-98de080592fc----6fb433f522d5---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----6fb433f522d5--------------------------------)
    ·11 min read·May 3, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F6fb433f522d5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-matrix-algebra-of-linear-regression-6fb433f522d5&user=Rob+Taylor%2C+PhD&userId=98de080592fc&source=-----6fb433f522d5---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F98de080592fc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-matrix-algebra-of-linear-regression-6fb433f522d5&user=Rob+Taylor%2C+PhD&userId=98de080592fc&source=post_page-98de080592fc----6fb433f522d5---------------------post_header-----------)
    发布于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----6fb433f522d5--------------------------------)
    ·11分钟阅读·2023年5月3日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F6fb433f522d5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-matrix-algebra-of-linear-regression-6fb433f522d5&user=Rob+Taylor%2C+PhD&userId=98de080592fc&source=-----6fb433f522d5---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F6fb433f522d5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-matrix-algebra-of-linear-regression-6fb433f522d5&source=-----6fb433f522d5---------------------bookmark_footer-----------)![](../Images/46a525be79cdfa66f016233756660562.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F6fb433f522d5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-matrix-algebra-of-linear-regression-6fb433f522d5&source=-----6fb433f522d5---------------------bookmark_footer-----------)![](../Images/46a525be79cdfa66f016233756660562.png)'
- en: Photo by [Mingwei Lim](https://unsplash.com/@cmzw?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由 [Mingwei Lim](https://unsplash.com/@cmzw?utm_source=medium&utm_medium=referral)
    提供，来源于 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: Introduction
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 介绍
- en: For most, simple linear regression is a common starting point for understanding
    model-based estimation and model comparison. Regardless of whether you’re taking
    an introductory statistics or data science course, you can bet your bottom dollar
    that linear regression will crop up at some point. And there’s a good reason for
    it.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 对大多数人来说，简单线性回归是理解基于模型的估计和模型比较的常见起点。无论你是在学习入门统计学还是数据科学课程，你都可以肯定线性回归会在某个时刻出现。这是有充分理由的。
- en: Simple linear regression provides a natural extension of simple descriptive
    statistics by modeling the response variable as a linear combination of only a
    single predictor. This simplicity not only facilitates the interpretation of model
    parameters but also makes estimation via ordinary least squares (OLS) easier to
    grasp.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 简单线性回归通过将响应变量建模为仅由单个预测变量的线性组合，自然地扩展了简单描述性统计。这种简单性不仅有助于解释模型参数，而且使得通过普通最小二乘法（OLS）进行估计更容易理解。
- en: While most textbook introductions will provide a detailed mathematical treatment
    in addition to the broader conceptual elements, when it comes to actually implementing
    the model it’s almost never through first principles. No matter which language
    you’re using there’ll almost certainly be a convenience function that fits the
    model for you. And why wouldn’t you use it? You don’t have to do all those tedious
    calculations by hand! That’s definitely a plus; though I’m a firm believer that
    taking some time to familiarise yourself with a model’s statistical machinery
    is an essential part of becoming an effective analyst and data scientist.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然大多数教科书介绍会提供详细的数学处理，除了更广泛的概念元素外，但在实际实现模型时，几乎从不会通过第一性原理来进行。不论你使用什么语言，几乎总会有一个便利函数来为你拟合模型。那为什么不使用呢？你不必手动进行所有那些繁琐的计算！这无疑是一个优点；不过，我坚信花一些时间熟悉模型的统计机制是成为一个有效的分析师和数据科学家的重要部分。
- en: In an earlier post, I provided a primer on linear algebra that touched on some
    fundamental principles and operations. In this post, we’re going to build on those
    concepts by looking under the hood and getting our hands dirty with the matrix
    operations that underpin linear regression.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的文章中，我提供了一个线性代数的入门，涉及了一些基本原理和操作。在这篇文章中，我们将基于这些概念，深入了解线性回归背后的矩阵操作。
- en: Simple Linear Regression in Matrix Form
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 矩阵形式的简单线性回归
- en: 'Most will be familiar with the standard regression formula that models a response
    variable *Y* as a linear combination of a single predictor *X*:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数人都对标准回归公式很熟悉，它将响应变量 *Y* 作为单个预测变量 *X* 的线性组合建模：
- en: '![](../Images/10fec3de1e6596b67bdfb8735cde4992.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/10fec3de1e6596b67bdfb8735cde4992.png)'
- en: The linear regression equation (image by author).
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 线性回归方程（作者提供的图片）。
- en: where here I’ve adopted the convention of assuming errors are normally distributed.
    From here we’ll build out the matrix representation by assigning the elements
    to vectors and matrices.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我采用了假设误差服从正态分布的惯例。从这里开始，我们将通过将元素分配给向量和矩阵，建立矩阵表示。
- en: 'First, we’ll put all the responses in an *n*-dimensional vector called the
    *response vector*:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将所有响应放在一个 *n* 维的向量中，这个向量称为 *响应向量*：
- en: '![](../Images/3e826f9d3b97b2f3360e9957ad25e81f.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3e826f9d3b97b2f3360e9957ad25e81f.png)'
- en: The response vector (image by author).
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 响应向量（作者提供的图片）。
- en: I’ve been quite explicit here by including the size of the vector — which is
    actually represented as a *column matrix —* just so we can keep track of things.
    However, it’s perfectly reasonable to use a lowercase boldface **y** if you wanted
    to.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 我在这里非常明确地包括了向量的大小 —— 实际上它表示为一个 *列矩阵* —— 这样我们可以跟踪情况。然而，如果你愿意，使用小写粗体 **y** 也是完全合理的。
- en: 'Next, the predictor *X* is placed into an *n* × *p* matrix called the design
    matrix:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，预测变量 *X* 被放置在一个 *n* × *p* 的矩阵中，称为设计矩阵：
- en: '![](../Images/be8751ab79cbbcdc314506630441a5e9.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/be8751ab79cbbcdc314506630441a5e9.png)'
- en: The design matrix (image by author).
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 设计矩阵（作者提供的图片）。
- en: where *p* denotes the number of columns and corresponds to the number of coefficients
    in the model. Note that the first column contains only ones — we’ll discuss this
    shortly — but this is to accommodate the intercept, which is a constant. Therefore,
    the number of columns in the design matrix is always one greater than the number
    of predictors you have. In the example above, we only have a single predictor
    meaning we need to estimate an intercept and a slope; so, *p* = 2 in this case.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *p* 表示列的数量，并对应于模型中的系数数量。请注意，第一列仅包含 1 —— 我们稍后会讨论这一点 —— 这是为了适应截距，这是一个常数。因此，设计矩阵中的列数总是比你拥有的预测变量数多一个。在上面的示例中，我们只有一个预测变量，这意味着我们需要估计一个截距和一个斜率；因此，在这种情况下
    *p* = 2。
- en: 'The regression coefficients are also placed into a *p* × 1 vector called the
    *parameter vector*:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 回归系数也被放置在一个 *p* × 1 的向量中，这个向量称为 *参数向量*：
- en: '![](../Images/7bc5dde3b39840066299f01844d16f71.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7bc5dde3b39840066299f01844d16f71.png)'
- en: The parameter vector (image by author).
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 参数向量（作者提供的图像）。
- en: Again, *p* denotes the number of parameters, though here *p* denotes the number
    of rows, unlike the design matrix where *p* is the column dimension. This arrangement
    is important because we will need to do some *matrix multiplication* with these
    two objects to compute the linear predictor.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 再次说明，*p* 表示参数的数量，不过这里的 *p* 表示行数，而设计矩阵中的 *p* 是列维度。这个安排很重要，因为我们需要对这两个对象进行一些*矩阵乘法*以计算线性预测器。
- en: 'However, before we do that, there is one last thing to do — place all the error
    terms into an *n* × 1 vector:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 不过，在我们进行这些操作之前，还有最后一件事要做——将所有的误差项放入一个 *n* × 1 向量中：
- en: '![](../Images/37954d9e73d10f2bfc8dfbae410aaf50.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/37954d9e73d10f2bfc8dfbae410aaf50.png)'
- en: The error vector (image by author).
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 误差向量（作者提供的图像）。
- en: 'With all of these in place, we can now express the simple linear regression
    model using matrix notation like so:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这些，我们现在可以使用矩阵符号来表示简单线性回归模型，如下所示：
- en: '![](../Images/e9cfd5d551172af9b4d6c7c13fe0eefb.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e9cfd5d551172af9b4d6c7c13fe0eefb.png)'
- en: The linear regression model in matrix form (image by author).
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 矩阵形式的线性回归模型（作者提供的图像）。
- en: The Linear Predictor
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 线性预测器
- en: 'In words, the matrix formulation of the linear regression model is the product
    of two matrices *X* and *β* plus an error vector. The product of *X* and *β* is
    an *n* × 1 matrix called the *linear predictor,* which I’ll denote here:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 用语言表述，线性回归模型的矩阵形式是两个矩阵 *X* 和 *β* 的乘积加上一个误差向量。*X* 和 *β* 的乘积是一个 *n* × 1 的矩阵，称为*线性预测器*，我在这里表示为：
- en: '![](../Images/99504be4d07eea0e14233832c9a8a358.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/99504be4d07eea0e14233832c9a8a358.png)'
- en: The linear predictor vector (image by author).
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 线性预测器向量（作者提供的图像）。
- en: Now, matrix multiplication works a little differently than you might expect.
    I covered this in my primer on linear algebra — so if you haven’t checked it out
    you can find it [here](https://medium.com/towards-data-science/a-primer-on-linear-algebra-414111d195ca)
    — but I’ll quickly run through the details now.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，矩阵乘法的工作方式与您预期的稍有不同。我在我的线性代数入门中讲解过这一点——如果您还没有查看，可以在[这里](https://medium.com/towards-data-science/a-primer-on-linear-algebra-414111d195ca)找到——但我现在会快速介绍一下细节。
- en: If we have some *m* × *q* matrix *A* and another *q* × *r* matrix *B*, then
    the product is an *m* × *r* matrix *C* (note how the *q* dimension drops out from
    the result). The reason for this change in size is because the element located
    at row *i* and column *j* in *C* is the *dot product* of the *i*th row in *A*
    and the *j*th column in *B:*
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们有一个 *m* × *q* 矩阵 *A* 和另一个 *q* × *r* 矩阵 *B*，那么它们的乘积是一个 *m* × *r* 矩阵 *C*（注意
    *q* 维度从结果中消失了）。这种大小变化的原因是，因为 *C* 中第 *i* 行第 *j* 列的元素是 *A* 中第 *i* 行和 *B* 中第 *j*
    列的 *点积*：
- en: '![](../Images/4b65503f8cd63f53a462cbe47d08e9bb.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4b65503f8cd63f53a462cbe47d08e9bb.png)'
- en: The dot product (image by author).
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 点积（作者提供的图像）。
- en: So, because the dot product takes the sum across the *q* dimension, this drops
    out from the resulting matrix.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，由于点积在 *q* 维度上取和，这一维度在结果矩阵中被省略了。
- en: 'For the simple linear regression case, the product is between an *n* × *p*
    matrix *X* and a *p* × 1 matrix *β,* which therefore results in an *n* × 1 matrix
    η. Following on from above, the (*i, j*) element in η is computed using the following
    dot product:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 对于简单线性回归情况，乘积是一个 *n* × *p* 矩阵 *X* 和一个 *p* × 1 矩阵 *β* 之间的乘积，因此结果是一个 *n* × 1 矩阵
    η。根据上述内容，η 中的（*i, j*）元素是使用以下点积计算的：
- en: '![](../Images/b4f0289122ae5c6f93db018f43db2b39.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b4f0289122ae5c6f93db018f43db2b39.png)'
- en: The linear predictor expanded (image by author).
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 扩展的线性预测器（作者提供的图像）。
- en: Here the sum is taken over *p*, which we know is the number of coefficients
    in the model, and so *p* = 2.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的和是对 *p* 进行的，我们知道 *p* 是模型中的系数数量，因此 *p* = 2。
- en: 'If we then substitute the dot product into the linear predictor vector and
    substitute in the values from the first column of the design matrix, we get the
    following (because *j* = 1 I’m going to drop that subscript to declutter the notation
    a little):'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们将点积代入线性预测器向量，并代入设计矩阵第一列的值，我们得到以下结果（因为 *j* = 1，我将省略那个下标，以简化符号）：
- en: '![](../Images/ccf3987424c41599043fc8a7dc727f59.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ccf3987424c41599043fc8a7dc727f59.png)'
- en: The linear predictor in full (image by author).
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 完整的线性预测器（作者提供的图像）。
- en: 'All of this reduces to something very familiar indeed: each element in η is
    just our linear regression equation applied to each value of *X*! Hopefully, you
    can see why the column of ones is included in the design matrix. This ensures
    that the intercept is added to each observation.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些都简化为非常熟悉的形式：η中的每个元素只是我们的线性回归方程应用于每个*X*值！希望你能理解为什么设计矩阵中包含了全1列。这确保了截距被加到每个观测值上。
- en: Model Errors
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型误差
- en: There are three critical assumptions relating to the error terms — or *perturbations
    —* around the linear predictor that fall out of the *Gauss-Markov theorem*. The
    first is that the expected conditional mean of the errors is zero, implying that
    the average of the errors should not depend on any particular value of *X*. This
    is called the *zero conditional mean assumption:*
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 与误差项相关的三个关键假设 — 或*扰动* — 源于*高斯-马尔科夫定理*。第一个假设是误差的期望条件均值为零，这意味着误差的平均值不应依赖于任何特定的*X*值。这被称为*零条件均值假设*：
- en: '![](../Images/61bcb0ecb338b1a81a25c1190bd445df.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/61bcb0ecb338b1a81a25c1190bd445df.png)'
- en: The zero conditional mean assumption (image by author).
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 零条件均值假设（图片由作者提供）。
- en: 'Related to this is the *homoscedasticity* *assumption* which states that the
    variance of the errors should not be affected by the values of the independent
    variables. That is, the distribution of the errors should be entirely independent
    of any information contained within the design matrix:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 与此相关的是*同方差性* *假设*，它表明误差的方差不应受到自变量值的影响。也就是说，误差的分布应完全独立于设计矩阵中的任何信息：
- en: '![](../Images/0ccadfdd56196652ea6342870ed3a276.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0ccadfdd56196652ea6342870ed3a276.png)'
- en: The *homoscedasticity* assumption (image by author).
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '*同方差性*假设（图片由作者提供）。'
- en: 'The final assumption is the *no autocorrelation assumption* which requires
    that error terms are uncorrelated. This implies that knowledge about one error
    term does not confer any information about another and therefore does not co-vary:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 最后的假设是*无自相关假设*，它要求误差项是无关的。这意味着对一个误差项的了解不会提供关于另一个误差项的信息，因此它们不会共变：
- en: '![](../Images/d035a65aa2873c1e03ca5b81515e4f96.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d035a65aa2873c1e03ca5b81515e4f96.png)'
- en: No autocorrelation assumption (image by author).
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 无自相关假设（图片由作者提供）。
- en: 'Under these assumptions, the covariance matrix for the error terms is a scalar
    matrix and the errors are considered to be *spherical*:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些假设下，误差项的协方差矩阵是一个标量矩阵，误差被认为是*球形的*：
- en: '![](../Images/b66335a14f27c56bcdaf148a2b4ed269.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b66335a14f27c56bcdaf148a2b4ed269.png)'
- en: The covariance matrix under the Gauss-Markov assumptions (image by author).
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 高斯-马尔科夫假设下的协方差矩阵（图片由作者提供）。
- en: Just a note before moving on, while I adopted the convention of normally distributed
    error, the Gauss-Markov theorem does not require the error terms to be normal,
    nor does it require that errors are independent and identically distributed; only
    that the error terms are homoscedastic and uncorrelated. This implies that a variable
    *can* have dependencies, but so long as those dependencies are not *linear* —
    which is what correlation measures — then parameter estimation can proceed safely.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在继续之前要说明的是，虽然我采用了正态分布误差的惯例，但高斯-马尔科夫定理并不要求误差项是正态分布的，也不要求误差是独立且同分布的；只要求误差项是同方差且无相关的。这意味着一个变量*可以*有依赖关系，但只要这些依赖关系不是*线性的*
    — 这是相关性所衡量的 — 那么参数估计可以安全进行。
- en: Parameter Estimation via Ordinary Least Squares
  id: totrans-68
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 通过普通最小二乘法进行参数估计
- en: When fitting a linear regression model to data the goal is to estimate the unknown
    model coefficients contained in *β*.The usual approach to finding the best values
    for these unknowns is to satisfy the least squares criterion, the goal of which
    is to minimize the total squared error between the linear predictor and the response.
    I’m going to step through how this is implemented for the simple linear regression
    model, though will move through this section fairly quickly.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 当将线性回归模型拟合到数据时，目标是估计包含在*β*中的未知模型系数。通常的做法是满足最小二乘准则，其目标是最小化线性预测器与响应之间的总平方误差。我将逐步介绍如何为简单线性回归模型实现这一点，尽管会快速通过这一部分。
- en: 'In matrix form, the vector of errors, or *residuals*, is defined as follows:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 以矩阵形式，误差向量或*残差*定义如下：
- en: '![](../Images/ceb410bad892b2979fe89861fe91ec98.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ceb410bad892b2979fe89861fe91ec98.png)'
- en: Model error, or residuals (image by author).
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 模型误差或残差（图片由作者提供）。
- en: 'where the hat on top of *β* denotes the estimated coefficients. The sum of
    the squared residuals can then be written as the dot product of the error vector
    with itself:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，*β* 上方的帽子表示估计系数。平方残差的和可以写成误差向量与自身的点积：
- en: '![](../Images/ec9eb1ab0d24bb7af6be47ff78ab0f82.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ec9eb1ab0d24bb7af6be47ff78ab0f82.png)'
- en: The sum squared error (image by author).
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 平方和误差（作者提供的图像）。
- en: 'where *T* denotes the transpose operator¹. To derive the least squares criterion
    it’s convenient to write out the errors in full as follows:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *T* 表示转置运算符¹。为了推导最小二乘准则，将误差完全展开是方便的，如下所示：
- en: '![](../Images/fcfe51eefc98430d52f0d8b096757fdd.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fcfe51eefc98430d52f0d8b096757fdd.png)'
- en: Expanding the dot product of the error vector (image by author).
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 误差向量的点积展开（作者提供的图像）。
- en: The idea, then, is to find the parameters that minimize this value. To do so,
    we need to take the derivative of the above with respect to the vector *β* and
    set this to zero*:*
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，想法是找到使该值最小化的参数。为此，我们需要对向量 *β* 取上面的导数并将其设为零*：*
- en: '![](../Images/bc22aa5e4258bdb72358dd656cd5336f.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bc22aa5e4258bdb72358dd656cd5336f.png)'
- en: The first derivative of the sum of squared error (image by author).
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 平方误差的第一导数（作者提供的图像）。
- en: 'from which the *normal equation* can be derived:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 从中可以推导出 *正规方程*：
- en: '![](../Images/b1d44c78834574b4e7515cae3829549b.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b1d44c78834574b4e7515cae3829549b.png)'
- en: The normal equation (image by author).
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 正规方程（作者提供的图像）。
- en: From here, we can find a solution for our unknown parameters by multiplying
    each side of the normal equation by the inverse of *X*ᵀ*X.* I cover matrix inversion
    in this [post](https://medium.com/p/eba53c564a90/edit), though if you haven’t
    read that, a matrix *A* is invertible if there exists a matrix *B* such that their
    product returns the identity matrix, *I*.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 从这里，我们可以通过将正规方程的两边乘以 *X*ᵀ*X* 的逆来找到我们未知的参数。我在这篇 [文章](https://medium.com/p/eba53c564a90/edit)
    中介绍了矩阵求逆的内容，虽然如果你还没有阅读，那些内容可以帮助你理解，矩阵 *A* 是可逆的，如果存在一个矩阵 *B* 使得它们的乘积返回单位矩阵 *I*。
- en: 'For a simple linear regression model, *X*ᵀ*X* is square with a size of 2 ×
    2, though more generally the matrix will be a *p* × *p* matrix*.* We then need
    to find another 2 × 2 matrix that is the multiplicative inverse of *X*ᵀ*X*. If
    such a matrix does not exist then the equation cannot be solved, though if *X*ᵀ*X*
    is indeed invertible, then we can obtain the parameter vector **b** as follows:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 对于简单的线性回归模型，*X*ᵀ*X* 是一个 2 × 2 的方阵，虽然更一般来说，矩阵将是一个 *p* × *p* 的矩阵*。* 我们接着需要找到另一个
    2 × 2 的矩阵，它是 *X*ᵀ*X* 的乘法逆。如果这样的矩阵不存在，则方程无法解决，但如果 *X*ᵀ*X* 确实是可逆的，则我们可以得到参数向量 **b**，如下所示：
- en: '![](../Images/a5feb27ce5e6986edffd826732751bc4.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a5feb27ce5e6986edffd826732751bc4.png)'
- en: The estimation equation for linear regression (image by author).
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 线性回归的估计方程（作者提供的图像）。
- en: Here you can see why it is necessary that the design matrix is not rank deficient.
    If there are indeed linear dependencies between columns then *X*ᵀ*X* cannot be
    inverted and a unique solution cannot be found². This is particularly true if
    you have *perfect multicollinearity*.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里你可以看到为什么设计矩阵必须不是秩亏的。如果列之间确实存在线性依赖，则 *X*ᵀ*X* 不能被逆，且无法找到唯一解²。如果存在 *完美多重共线性*，这尤其适用。
- en: A Closer Look at Parameter Estimation
  id: totrans-90
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参数估计的进一步观察
- en: It is interesting to further consider the elements contained in each matrix.
    First, let's look at the cross-product of the design matrix *X*ᵀ*X:*
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 进一步考虑每个矩阵中包含的元素是很有趣的。首先，让我们看一下设计矩阵 *X*ᵀ*X* 的叉积：
- en: '![](../Images/b1682048cb8a51d79ef59ef220e99985.png)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b1682048cb8a51d79ef59ef220e99985.png)'
- en: Taking the cross product of the design matrix (image by author).
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 设计矩阵的叉积（作者提供的图像）。
- en: 'From here we can see that the matrix contains products of each column in the
    design matrix. But what we need is the inverse of this matrix. I won’t go through
    how to derive the inverse, but it looks like this:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 从这里我们可以看到矩阵包含了设计矩阵中每一列的乘积。但我们需要的是这个矩阵的逆。我不会详细讲解如何推导逆矩阵，但它看起来是这样的：
- en: '![](../Images/936f0f5ae04f7aeb3071c1cb3b4ece9c.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/936f0f5ae04f7aeb3071c1cb3b4ece9c.png)'
- en: The inverse of the design matrix cross product (image by author).
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 设计矩阵叉积的逆（作者提供的图像）。
- en: 'Finally, we also need the cross-product of the design matrix and the response
    vector Y, which produces the following:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们还需要设计矩阵与响应向量 Y 的叉积，结果如下：
- en: '![](../Images/694d2c7c07ee8921b843d9a6cd10f55d.png)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/694d2c7c07ee8921b843d9a6cd10f55d.png)'
- en: The cross product of the design matrix with the response vector (image by author).
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 设计矩阵与响应向量的叉积（作者提供的图像）。
- en: 'With the matrices written out in full, we can substitute them into the estimation
    question and work through the calculation like so:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 将矩阵完全写出后，我们可以将其代入估计问题，并按如下方式进行计算：
- en: '![](../Images/ba7df46fbb63754e8a9c3e6c2e7a2d5e.png)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ba7df46fbb63754e8a9c3e6c2e7a2d5e.png)'
- en: Derivation of the parameter vector (image by author).
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 参数向量的推导（作者提供的图像）。
- en: 'To be fair, there is a bit going on in that derivation, but it’s really the
    last line that is most interesting. What this all boils down to is something quite
    convenient; we can estimate the slope coefficient using the sample covariance
    and variance like so:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 公平地说，这一推导过程有些复杂，但最有趣的实际上是最后一行。这一切归结为一个非常方便的东西；我们可以像这样使用样本协方差和方差来估计斜率系数：
- en: '![](../Images/34ab197a6dc7deb317ffcd1787e9a765.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/34ab197a6dc7deb317ffcd1787e9a765.png)'
- en: Estimate for the slope coefficient (image by author).
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 斜率系数的估计（作者提供的图像）。
- en: 'Once we have that, we can use this estimate, along with the mean of *y* and
    *x*, to derive the estimate for the intercept:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们有了这个估计值，我们可以利用这个估计值，以及*y*和*x*的均值，来推导截距的估计值：
- en: '![](../Images/80d70d35f595c980c8a328469c442ee2.png)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/80d70d35f595c980c8a328469c442ee2.png)'
- en: Estimate for the intercept coefficient (image by author).
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 截距系数的估计（作者提供的图像）。
- en: Fitted Values
  id: totrans-109
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 拟合值
- en: We have now used some linear algebra to find the best-fitting parameters for
    a simple linear regression model. Now that we have these in hand the next step
    is to see how well the fitted values correspond with the values contained in the
    response vector.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在使用了一些线性代数来找到简单线性回归模型的最佳拟合参数。既然我们已经掌握了这些参数，下一步是查看拟合值与响应向量中包含的值的对应程度。
- en: 'All we need to obtain the fitted values is the design matrix along with the
    parameter vector **b**. Multiplying together, the fitted values are computed like
    so:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要获得拟合值的唯一条件是设计矩阵和参数向量**b**。将它们相乘，拟合值的计算如下：
- en: '![](../Images/0c48389a0ad0af3b6fde2494f1b54920.png)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0c48389a0ad0af3b6fde2494f1b54920.png)'
- en: Vector of fitted values (image by author).
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 拟合值的向量（作者提供的图像）。
- en: Note that our fitted values have a hat placed on top of them to indicate that
    they’re estimated values. For this reason, an alternative way to express the fitted
    values is as a combination of the response vector and the *hat matrix,* which
    gets its name from the fact that it puts a hat on *Y*.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们的拟合值上面放有一个帽子，以表示它们是估计值。因此，表达拟合值的另一种方式是将其作为响应向量和*帽子矩阵*的组合，帽子矩阵之所以得名，是因为它给*Y*加了一个帽子。
- en: 'To see this, let’s write out the equation for the fitted values by substituting
    **b** with the full equation for the parameter vector:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 要查看这一点，我们将通过用参数向量的完整方程替代**b**来写出拟合值的方程：
- en: '![](../Images/8630753e77222a816dcd02bd2bf1e099.png)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8630753e77222a816dcd02bd2bf1e099.png)'
- en: Substituting in expanded version of parameter vector (image by author).
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 替代的参数向量扩展版（作者提供的图像）。
- en: 'Basically, every term that involves the design matrix *X* is lumped together
    to form the definition of the hat matrix:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 基本上，所有涉及设计矩阵*X*的项被归并在一起，形成了帽子矩阵的定义：
- en: '![](../Images/1491dcb685d8e42ca3c91af3b14d9a30.png)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1491dcb685d8e42ca3c91af3b14d9a30.png)'
- en: The ‘hat’ matrix (image by author).
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: ‘帽子’矩阵（作者提供的图像）。
- en: Final Remarks
  id: totrans-121
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 最后的说明
- en: If you’ve made it to this point, thanks for sticking at it. There was a lot
    to unpack, for sure. However, I hope you can see how the principles of matrix
    algebra are used to build simple linear regression models. Now, while I have focussed
    on simple linear regression throughout to keep the examples concise, everything
    discussed here works the same way for multiple regression, too. All that changes
    is that the dimensionality of the matrices and vectors increases.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你能读到这里，感谢你坚持下来。确实有很多内容需要解读。然而，我希望你能看到矩阵代数的原理是如何用于构建简单线性回归模型的。现在，虽然我在整个过程中专注于简单线性回归以保持示例简洁，但这里讨论的所有内容对多重回归同样适用。唯一变化的是矩阵和向量的维度增加。
- en: Footnotes
  id: totrans-123
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 脚注
- en: '[1] The notation I have used here is different from what I used in my primer
    article, but they mean exactly the same thing. The reason for the change is to
    keep in step with how the matrix formulations are typically described.'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] 我在这里使用的符号与我在入门文章中使用的不同，但它们的意思完全相同。之所以进行更改，是为了与矩阵公式的通常描述保持一致。'
- en: '[2] Strictly speaking, singular matrices *can* be inverted. The *generalised
    inverse* extends the idea of the inverse and can be applied to a broader class
    of matrices. The generalised inverse is particularly useful in finding least squares
    solutions in cases where no unique solution exists.'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] 严格来说，奇异矩阵*是可以*被逆的。*广义逆*扩展了逆的概念，并可以应用于更广泛的矩阵类别。广义逆在寻找最小二乘解时尤其有用，特别是在没有唯一解的情况下。'
- en: Related Articles
  id: totrans-126
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 相关文章
- en: '[A Primer on Linear Algebra](https://medium.com/towards-data-science/a-primer-on-linear-algebra-414111d195ca)'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[线性代数入门](https://medium.com/towards-data-science/a-primer-on-linear-algebra-414111d195ca)'
- en: '[Linear Digressions](https://medium.com/@dataforyou/linear-digressions-1427951c0257)'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[线性离题](https://medium.com/@dataforyou/linear-digressions-1427951c0257)'
- en: '[Multicollinearity: Problem, or Not?](https://medium.com/towards-data-science/multicollinearity-problem-or-not-d4bd7a9cfb91)'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[多重共线性：问题还是非问题？](https://medium.com/towards-data-science/multicollinearity-problem-or-not-d4bd7a9cfb91)'
- en: Thanks for reading!
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 感谢阅读！
- en: If you enjoyed this post and would like to stay up to date then please consider
    [following me on Medium.](https://medium.com/@dataforyou) This will ensure you
    don’t miss out on any new content.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你喜欢这篇文章并希望保持更新，请考虑[关注我在 Medium 的账号。](https://medium.com/@dataforyou) 这样可以确保你不会错过任何新内容。
- en: To get unlimited access to all content consider signing up for a [Medium subscription](https://medium.com/membership).
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 想要无限访问所有内容，可以考虑注册[Medium 订阅](https://medium.com/membership)。
- en: You can also follow me on [Twitter](https://twitter.com/dataforyounz), [LinkedIn](https://www.linkedin.com/in/dataforyou/),
    or check out my [GitHub](https://github.com/dataforyounz) if that’s more your
    thing.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以在[Twitter](https://twitter.com/dataforyounz)、[LinkedIn](https://www.linkedin.com/in/dataforyou/)上关注我，或者查看我的[GitHub](https://github.com/dataforyounz)。
