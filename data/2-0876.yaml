- en: Fast and Scalable Hyperparameter Tuning and Cross-validation in AWS SageMaker
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: AWS SageMaker ä¸­çš„å¿«é€Ÿå’Œå¯æ‰©å±•è¶…å‚æ•°è°ƒä¼˜ä¸äº¤å‰éªŒè¯
- en: åŸæ–‡ï¼š[https://towardsdatascience.com/fast-and-scalable-hyperparameter-tuning-and-cross-validation-in-aws-sagemaker-d2b4095412eb](https://towardsdatascience.com/fast-and-scalable-hyperparameter-tuning-and-cross-validation-in-aws-sagemaker-d2b4095412eb)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡ï¼š[https://towardsdatascience.com/fast-and-scalable-hyperparameter-tuning-and-cross-validation-in-aws-sagemaker-d2b4095412eb](https://towardsdatascience.com/fast-and-scalable-hyperparameter-tuning-and-cross-validation-in-aws-sagemaker-d2b4095412eb)
- en: Using SageMaker Managed Warm Pools
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä½¿ç”¨ SageMaker ç®¡ç†çš„ Warm Pools
- en: '[](https://medium.com/@joao.pereira.abt?source=post_page-----d2b4095412eb--------------------------------)[![JoÃ£o
    Pereira](../Images/2946b185eb134ddfaa71cf5af5e3cda6.png)](https://medium.com/@joao.pereira.abt?source=post_page-----d2b4095412eb--------------------------------)[](https://towardsdatascience.com/?source=post_page-----d2b4095412eb--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----d2b4095412eb--------------------------------)
    [JoÃ£o Pereira](https://medium.com/@joao.pereira.abt?source=post_page-----d2b4095412eb--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@joao.pereira.abt?source=post_page-----d2b4095412eb--------------------------------)[![JoÃ£o
    Pereira](../Images/2946b185eb134ddfaa71cf5af5e3cda6.png)](https://medium.com/@joao.pereira.abt?source=post_page-----d2b4095412eb--------------------------------)[](https://towardsdatascience.com/?source=post_page-----d2b4095412eb--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----d2b4095412eb--------------------------------)
    [JoÃ£o Pereira](https://medium.com/@joao.pereira.abt?source=post_page-----d2b4095412eb--------------------------------)'
- en: Â·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----d2b4095412eb--------------------------------)
    Â·8 min readÂ·Mar 3, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: Â·å‘è¡¨äº [Towards Data Science](https://towardsdatascience.com/?source=post_page-----d2b4095412eb--------------------------------)
    Â·é˜…è¯»æ—¶é—´ 8 åˆ†é’ŸÂ·2023å¹´3æœˆ3æ—¥
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/125320a8ffef1abf9fcab6a60572290d.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/125320a8ffef1abf9fcab6a60572290d.png)'
- en: Photo by [SpaceX](https://unsplash.com/@spacex?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral).
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ç‰‡ç”± [SpaceX](https://unsplash.com/@spacex?utm_source=medium&utm_medium=referral)
    æä¾›ï¼Œæ¥æºäº [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)ã€‚
- en: This article shares a recipe to **speeding up to 60%** yourhyperparameter tuning
    with cross-validation in SageMaker Pipelines leveraging SageMaker Managed Warm
    Pools. By using Warm Pools, the runtime of a Tuning step with 120 sequential jobs
    is reduced **from 10h to 4h**.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬æ–‡åˆ†äº«äº†ä¸€ç§é…æ–¹ï¼Œä»¥ **æé«˜ 60%** çš„é€Ÿåº¦ï¼Œé€šè¿‡ SageMaker ç®¡é“åˆ©ç”¨ SageMaker ç®¡ç†çš„ Warm Pools è¿›è¡Œè¶…å‚æ•°è°ƒä¼˜ä¸äº¤å‰éªŒè¯ã€‚é€šè¿‡ä½¿ç”¨
    Warm Poolsï¼Œä¸€ä¸ªåŒ…å« 120 ä¸ªé¡ºåºä½œä¸šçš„è°ƒä¼˜æ­¥éª¤çš„è¿è¡Œæ—¶é—´å‡å°‘äº† **ä» 10 å°æ—¶åˆ° 4 å°æ—¶**ã€‚
- en: Improving and evaluating the performance of a machine learning model often requires
    a variety of ingredients. Hyperparameter tuning and cross-validation are 2 such
    ingredients. The first finds the best version of a model, while the second estimates
    how a model will generalize to unseen data. These steps, combined, introduce computing
    challenges as they require training and validating a model multiple times, in
    parallel and/or in sequence.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: æå‡å’Œè¯„ä¼°æœºå™¨å­¦ä¹ æ¨¡å‹çš„æ€§èƒ½é€šå¸¸éœ€è¦å¤šç§å› ç´ ã€‚è¶…å‚æ•°è°ƒä¼˜å’Œäº¤å‰éªŒè¯å°±æ˜¯ä¸¤ä¸ªè¿™æ ·çš„å› ç´ ã€‚å‰è€…æ‰¾åˆ°æ¨¡å‹çš„æœ€ä½³ç‰ˆæœ¬ï¼Œè€Œåè€…ä¼°è®¡æ¨¡å‹å¦‚ä½•æ¨å¹¿åˆ°æœªè§æ•°æ®ã€‚è¿™äº›æ­¥éª¤ç»“åˆèµ·æ¥ï¼Œå¸¦æ¥äº†è®¡ç®—æŒ‘æˆ˜ï¼Œå› ä¸ºå®ƒä»¬éœ€è¦å¤šæ¬¡è®­ç»ƒå’ŒéªŒè¯æ¨¡å‹ï¼Œå¯èƒ½æ˜¯å¹¶è¡Œçš„å’Œ/æˆ–é¡ºåºçš„ã€‚
- en: '***What this article is aboutâ€¦***'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '***æœ¬æ–‡ä»‹ç»çš„å†…å®¹â€¦***'
- en: What are Warm Pools and how to leverage them to speed-up hyperparameter tuning
    with cross-validation.
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä»€ä¹ˆæ˜¯ Warm Pools ä»¥åŠå¦‚ä½•åˆ©ç”¨å®ƒä»¬åŠ é€Ÿè¶…å‚æ•°è°ƒä¼˜ä¸äº¤å‰éªŒè¯ã€‚
- en: How to design a production-grade SageMaker Pipeline that includes Processing,
    Tuning, Training, and Lambda steps.
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¦‚ä½•è®¾è®¡ä¸€ä¸ªåŒ…å«å¤„ç†ã€è°ƒä¼˜ã€è®­ç»ƒå’Œ Lambda æ­¥éª¤çš„ç”Ÿäº§çº§ SageMaker æµæ°´çº¿ã€‚
- en: 'We will consider [Bayesian optimization](/bayesian-optimization-for-hyperparameter-tuning-how-and-why-655b0ee0b399)
    for hyperparameter tuning that leverages the scores of the hyperparameter combinations
    already tested to choose the hyperparameter set to test in the next round. We
    will use [*k*-fold cross-validation](https://medium.com/@zstern/k-fold-cross-validation-explained-5aeba90ebb3)
    to score each combination of hyperparameters, in which the splits are as follows:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†è€ƒè™‘ç”¨äºè¶…å‚æ•°è°ƒä¼˜çš„[è´å¶æ–¯ä¼˜åŒ–](/bayesian-optimization-for-hyperparameter-tuning-how-and-why-655b0ee0b399)ï¼Œå®ƒåˆ©ç”¨å·²æµ‹è¯•çš„è¶…å‚æ•°ç»„åˆçš„è¯„åˆ†æ¥é€‰æ‹©ä¸‹ä¸€è½®æµ‹è¯•çš„è¶…å‚æ•°é›†ã€‚æˆ‘ä»¬å°†ä½¿ç”¨[*k*-æŠ˜äº¤å‰éªŒè¯](https://medium.com/@zstern/k-fold-cross-validation-explained-5aeba90ebb3)æ¥è¯„åˆ†æ¯ä¸ªè¶…å‚æ•°ç»„åˆï¼Œåˆ†å‰²å¦‚ä¸‹ï¼š
- en: '![](../Images/7f98ae8209ab60a0f673cbedde9fd943.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7f98ae8209ab60a0f673cbedde9fd943.png)'
- en: ğ‘˜-fold cross-validation strategy.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: ğ‘˜-fold äº¤å‰éªŒè¯ç­–ç•¥ã€‚
- en: The full dataset is partitioned into ğ‘˜ validation folds, the model trained on
    ğ‘˜-1 folds, and validated on its corresponding held-out fold. The overall score
    is the average over the individual validation scores obtained for each validation
    fold.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: å®Œæ•´æ•°æ®é›†è¢«åˆ’åˆ†ä¸ºğ‘˜ä¸ªéªŒè¯æŠ˜å ï¼Œæ¨¡å‹åœ¨ğ‘˜-1ä¸ªæŠ˜å ä¸Šè®­ç»ƒï¼Œå¹¶åœ¨ç›¸åº”çš„ä¿ç•™æŠ˜å ä¸ŠéªŒè¯ã€‚æ€»ä½“å¾—åˆ†æ˜¯æ¯ä¸ªéªŒè¯æŠ˜å å¾—åˆ°çš„ä¸ªåˆ«éªŒè¯å¾—åˆ†çš„å¹³å‡å€¼ã€‚
- en: '**Storyline*:***'
  id: totrans-17
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**æ•…äº‹æƒ…èŠ‚ï¼š**'
- en: ''
  id: totrans-18
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 1\. [What are Warm Pools?](#5fb3)
  id: totrans-19
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 1\. [ä»€ä¹ˆæ˜¯æ¸©æš–æ± ï¼Ÿ](#5fb3)
- en: ''
  id: totrans-20
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 2\. [End-to-end SageMaker Pipeline](#9b0f)
  id: totrans-21
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 2\. [ç«¯åˆ°ç«¯SageMakerç®¡é“](#9b0f)
- en: ''
  id: totrans-22
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 3\. [What happens inside the Tuning step?](#66e1)
  id: totrans-23
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 3\. [è°ƒä¼˜æ­¥éª¤å†…éƒ¨å‘ç”Ÿäº†ä»€ä¹ˆï¼Ÿ](#66e1)
- en: ''
  id: totrans-24
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 4\. [What do we get out of using Warm Pools?](#7360)
  id: totrans-25
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 4\. [ä½¿ç”¨æ¸©æš–æ± æœ‰ä»€ä¹ˆå¥½å¤„ï¼Ÿ](#7360)
- en: ''
  id: totrans-26
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 5\. [Summary](#2c90)
  id: totrans-27
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 5\. [æ€»ç»“](#2c90)
- en: 1\. What are Warm Pools?
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 1\. ä»€ä¹ˆæ˜¯æ¸©æš–æ± ï¼Ÿ
- en: Whenever a training job is launched in AWS, the provisioned instance takes roughly
    3min to bootstrap before the training script is executed. This startup time adds
    up when running multiple jobs sequentially, which is the case when performing
    hyperparameter tuning using a Bayesian optimization strategy. Here, dozens or
    even hundreds of jobs are run in sequence leading to a significant total time
    that can be on par with or even higher than the actual execution times of the
    scripts.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: æ¯å½“åœ¨AWSä¸­å¯åŠ¨è®­ç»ƒä»»åŠ¡æ—¶ï¼Œé¢„é…å®ä¾‹åœ¨æ‰§è¡Œè®­ç»ƒè„šæœ¬ä¹‹å‰å¤§çº¦éœ€è¦3åˆ†é’Ÿæ¥å¼•å¯¼ã€‚è¿™ç§å¯åŠ¨æ—¶é—´åœ¨é¡ºåºè¿è¡Œå¤šä¸ªä»»åŠ¡æ—¶ä¼šç´¯ç§¯ï¼Œè¿™åœ¨ä½¿ç”¨è´å¶æ–¯ä¼˜åŒ–ç­–ç•¥è¿›è¡Œè¶…å‚æ•°è°ƒä¼˜æ—¶å°¤å…¶æ˜æ˜¾ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæ•°åä¸ªç”šè‡³æ•°ç™¾ä¸ªä»»åŠ¡è¢«é¡ºåºè¿è¡Œï¼Œå¯¼è‡´æ€»æ—¶é—´æ˜¾è‘—å¢åŠ ï¼Œè¿™å¯èƒ½ä¸è„šæœ¬çš„å®é™…æ‰§è¡Œæ—¶é—´ç›¸å½“ï¼Œç”šè‡³æ›´é«˜ã€‚
- en: '[SageMaker Managed Warm Pools](https://aws.amazon.com/about-aws/whats-new/2022/09/reduce-ml-model-training-job-startup-time-8x-sagemaker-training-managed-warm-pools/)
    make it possible to retain training infrastructure after a job is completed for
    a desired number of seconds, enabling saving the instance startup time for every
    subsequent job.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '[SageMakeræ‰˜ç®¡æ¸©æš–æ± ](https://aws.amazon.com/about-aws/whats-new/2022/09/reduce-ml-model-training-job-startup-time-8x-sagemaker-training-managed-warm-pools/)ä½¿å¾—åœ¨ä»»åŠ¡å®Œæˆåä¿ç•™è®­ç»ƒåŸºç¡€è®¾æ–½æˆä¸ºå¯èƒ½ï¼Œä»è€Œä¸ºæ¯ä¸ªåç»­ä»»åŠ¡èŠ‚çœå®ä¾‹å¯åŠ¨æ—¶é—´ã€‚'
- en: 'Enabling Warm Pools is straightforward. You simply add an extra parameter (`keep_alive_period_in_seconds`)
    when creating a training job in SageMaker:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: å¯ç”¨æ¸©æš–æ± æ˜¯ç›´æ¥çš„ã€‚ä½ åªéœ€åœ¨åˆ›å»ºSageMakerè®­ç»ƒä»»åŠ¡æ—¶æ·»åŠ ä¸€ä¸ªé¢å¤–çš„å‚æ•°ï¼ˆ`keep_alive_period_in_seconds`ï¼‰ï¼š
- en: '[PRE0]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'If you want to learn more about SageMaker Managed Warm Pools, here is the documentation:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœä½ æƒ³äº†è§£æ›´å¤šå…³äºSageMakeræ‰˜ç®¡æ¸©æš–æ± çš„ä¿¡æ¯ï¼Œè¿™é‡Œæ˜¯æ–‡æ¡£ï¼š
- en: '[## Train Using SageMaker Managed Warm Pools'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '[## ä½¿ç”¨SageMakeræ‰˜ç®¡æ¸©æš–æ± è¿›è¡Œè®­ç»ƒ'
- en: SageMaker Managed Warm Pools let you retain and reuse provisioned infrastructure
    after the completion of a training jobâ€¦
  id: totrans-35
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: SageMakeræ‰˜ç®¡æ¸©æš–æ± ä½¿ä½ èƒ½å¤Ÿåœ¨è®­ç»ƒä»»åŠ¡å®Œæˆåä¿ç•™å’Œé‡ç”¨é¢„é…çš„åŸºç¡€è®¾æ–½â€¦
- en: docs.aws.amazon.com](https://docs.aws.amazon.com/sagemaker/latest/dg/train-warm-pools.html?source=post_page-----d2b4095412eb--------------------------------)
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '[docs.aws.amazon.com](https://docs.aws.amazon.com/sagemaker/latest/dg/train-warm-pools.html?source=post_page-----d2b4095412eb--------------------------------)'
- en: Now that we know what are Warm Pools, in [Section 2](#9b0f) we are going to
    dive deep into how to leverage them to speed-up the overall runtime of a SageMaker
    Pipeline that includes hyperparameter tuning with cross-validation.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: æ—¢ç„¶æˆ‘ä»¬äº†è§£äº†ä»€ä¹ˆæ˜¯æ¸©æš–æ± ï¼Œåœ¨[ç¬¬2èŠ‚](#9b0f)ä¸­ï¼Œæˆ‘ä»¬å°†æ·±å…¥æ¢è®¨å¦‚ä½•åˆ©ç”¨å®ƒä»¬æ¥åŠ é€ŸåŒ…å«äº¤å‰éªŒè¯çš„SageMakerç®¡é“çš„æ•´ä½“è¿è¡Œæ—¶é—´ã€‚
- en: 2\. End-to-end SageMaker Pipeline
  id: totrans-38
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 2\. ç«¯åˆ°ç«¯SageMakerç®¡é“
- en: The following figure depicts an end-to-end SageMaker Pipeline that performs
    hyperparameter tuning with cross-validation.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸‹å›¾å±•ç¤ºäº†ä¸€ä¸ªç«¯åˆ°ç«¯çš„SageMakerç®¡é“ï¼Œè¯¥ç®¡é“é€šè¿‡äº¤å‰éªŒè¯è¿›è¡Œè¶…å‚æ•°è°ƒä¼˜ã€‚
- en: '![](../Images/fb9e36cdf54d270bfa876e123c87e0b4.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fb9e36cdf54d270bfa876e123c87e0b4.png)'
- en: Architecture diagram of the end-to-end SageMaker Pipeline.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: ç«¯åˆ°ç«¯SageMakerç®¡é“çš„æ¶æ„å›¾ã€‚
- en: 'We will create the pipeline using the [SageMaker Python SDK](https://sagemaker.readthedocs.io/en/stable/),
    which is an open-source library that simplifies the process of training, tuning,
    and deploying machine learning models in AWS SageMaker. The pipeline steps in
    the diagram are summarized as follows:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†ä½¿ç”¨[SageMaker Python SDK](https://sagemaker.readthedocs.io/en/stable/)æ¥åˆ›å»ºç®¡é“ï¼Œè¿™æ˜¯ä¸€ä¸ªå¼€æºåº“ï¼Œç®€åŒ–äº†åœ¨AWS
    SageMakerä¸­è®­ç»ƒã€è°ƒä¼˜å’Œéƒ¨ç½²æœºå™¨å­¦ä¹ æ¨¡å‹çš„è¿‡ç¨‹ã€‚å›¾ä¸­çš„ç®¡é“æ­¥éª¤æ€»ç»“å¦‚ä¸‹ï¼š
- en: '**Data Preprocessing(**`ProcessingStep`**) â€”** Data is retrieved from the source,
    transformed, and split into *k* cross-validation folds. An additional full dataset
    is saved for final training.'
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**æ•°æ®é¢„å¤„ç†ï¼ˆ**`ProcessingStep`**ï¼‰â€”** æ•°æ®ä»æºä¸­æ£€ç´¢ï¼Œè½¬åŒ–ï¼Œå¹¶åˆ’åˆ†ä¸º *k* ä¸ªäº¤å‰éªŒè¯æŠ˜å ã€‚ä¸€ä¸ªé¢å¤–çš„å®Œæ•´æ•°æ®é›†è¢«ä¿å­˜ç”¨äºæœ€ç»ˆè®­ç»ƒã€‚'
- en: '**Hyperparameter Tuning With CV(**`TuningStep`**) â€”** This is the step that
    we will concentrate on. It finds the combination of hyperparameters that achieves
    the best average performance across validation folds.'
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**è¶…å‚æ•°è°ƒä¼˜ä¸äº¤å‰éªŒè¯ï¼ˆ**`TuningStep`**ï¼‰â€”** è¿™æ˜¯æˆ‘ä»¬å°†é‡ç‚¹å…³æ³¨çš„æ­¥éª¤ã€‚å®ƒæ‰¾åˆ°åœ¨éªŒè¯æŠ˜ä¸­å®ç°æœ€ä½³å¹³å‡æ€§èƒ½çš„è¶…å‚æ•°ç»„åˆã€‚'
- en: '**Optimal Hyperparameters Retrieval(**`LambdaStep`**) â€”** Fires a *Lambda*
    function that retrieves the optimal set of hyperparameters by accessing the results
    of the hyperparameter tuning job using *Boto3*.'
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**æœ€ä½³è¶…å‚æ•°æ£€ç´¢ï¼ˆ**`LambdaStep`**ï¼‰â€”** è§¦å‘ä¸€ä¸ª*Lambda*å‡½æ•°ï¼Œé€šè¿‡è®¿é—®è¶…å‚æ•°è°ƒä¼˜ä½œä¸šçš„ç»“æœæ¥æ£€ç´¢æœ€ä½³è¶…å‚æ•°é›†ï¼Œä½¿ç”¨ *Boto3*ã€‚'
- en: '**Final Training(**`TrainingStep`**) â€”** Trains the model on the full dataset
    `train_full.csv` with the optimal hyperparameters.'
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**æœ€ç»ˆè®­ç»ƒï¼ˆ**`TrainingStep`**ï¼‰â€”** ä½¿ç”¨æœ€ä½³è¶…å‚æ•°åœ¨å®Œæ•´æ•°æ®é›† `train_full.csv` ä¸Šè®­ç»ƒæ¨¡å‹ã€‚'
- en: '**Model Registration (**`ModelStep`**) â€”** Registers the final trained model
    in the SageMaker Model Registry.'
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**æ¨¡å‹æ³¨å†Œï¼ˆ**`ModelStep`**ï¼‰â€”** å°†æœ€ç»ˆè®­ç»ƒå¥½çš„æ¨¡å‹æ³¨å†Œåˆ° SageMaker æ¨¡å‹æ³¨å†Œè¡¨ä¸­ã€‚'
- en: '**Inference (**`TransformStep`**) â€”** Generates predictions using the registered
    model.'
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**æ¨ç†ï¼ˆ**`TransformStep`**ï¼‰â€”** ä½¿ç”¨æ³¨å†Œçš„æ¨¡å‹ç”Ÿæˆé¢„æµ‹ç»“æœã€‚'
- en: Please find detailed documentation on how to implement these steps on the [SageMaker
    Developer Guide](https://docs.aws.amazon.com/sagemaker/latest/dg/build-and-manage-steps.html).
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: è¯·åœ¨[SageMaker å¼€å‘è€…æŒ‡å—](https://docs.aws.amazon.com/sagemaker/latest/dg/build-and-manage-steps.html)ä¸­æŸ¥æ‰¾æœ‰å…³å¦‚ä½•å®ç°è¿™äº›æ­¥éª¤çš„è¯¦ç»†æ–‡æ¡£ã€‚
- en: 3\. What happens inside the Tuning step?
  id: totrans-50
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 3\. è°ƒä¼˜æ­¥éª¤å†…éƒ¨å‘ç”Ÿäº†ä»€ä¹ˆï¼Ÿ
- en: 'Let''s now dig deeper into the **pipeline step 2** that iteratively tries and
    cross-validates multiple hyperparameter combinations in parallel and in sequence.
    The solution is represented in the following diagram:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬æ¥æ·±å…¥æ¢è®¨**ç®¡é“æ­¥éª¤ 2**ï¼Œè¯¥æ­¥éª¤è¿­ä»£åœ°å¹¶è¡Œå’Œé¡ºåºåœ°å°è¯•å’Œäº¤å‰éªŒè¯å¤šä¸ªè¶…å‚æ•°ç»„åˆã€‚è¯¥è§£å†³æ–¹æ¡ˆåœ¨ä¸‹å›¾ä¸­è¡¨ç¤ºï¼š
- en: '![](../Images/82a6c6ada9427d721d1e193e628e4d1f.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/82a6c6ada9427d721d1e193e628e4d1f.png)'
- en: Architecture diagram of the hyperparameter tuning with cross-validation step.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: è¶…å‚æ•°è°ƒä¼˜ä¸äº¤å‰éªŒè¯æ­¥éª¤çš„æ¶æ„å›¾ã€‚
- en: The solution relies on SageMaker Automatic Model Tuning to create and orchestrate
    the training jobs that test multiple hyperparameter combinations. The Automatic
    Model Tuning job can be launched using the `HyperparameterTuner` available in
    the [SageMaker Python SDK](https://sagemaker.readthedocs.io/en/stable/). It creates
    *M*x*N* hyperparameter tuning training jobs, *M* of which are run in parallel
    over *N* sequential rounds that progressively search for the best hyperparameters.
    Each of these jobs launches and monitors a set of *K* cross-validation jobs. At
    each tuning round, *M*x*K* instances in a Warm Pool are **retained for the next
    round**. In the subsequent rounds there is no instance startup time.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: è¯¥è§£å†³æ–¹æ¡ˆä¾èµ–äº SageMaker è‡ªåŠ¨æ¨¡å‹è°ƒä¼˜æ¥åˆ›å»ºå’Œåè°ƒæµ‹è¯•å¤šä¸ªè¶…å‚æ•°ç»„åˆçš„è®­ç»ƒä½œä¸šã€‚å¯ä»¥ä½¿ç”¨[SageMaker Python SDK](https://sagemaker.readthedocs.io/en/stable/)ä¸­çš„`HyperparameterTuner`å¯åŠ¨è‡ªåŠ¨æ¨¡å‹è°ƒä¼˜ä½œä¸šã€‚å®ƒåˆ›å»ºäº†*M*x*N*ä¸ªè¶…å‚æ•°è°ƒä¼˜è®­ç»ƒä½œä¸šï¼Œå…¶ä¸­*M*ä¸ªä½œä¸šåœ¨*N*ä¸ªé¡ºåºè½®æ¬¡ä¸­å¹¶è¡Œè¿è¡Œï¼Œé€æ­¥æœç´¢æœ€ä½³è¶…å‚æ•°ã€‚æ¯ä¸ªä½œä¸šå¯åŠ¨å¹¶ç›‘æ§ä¸€ç»„*K*äº¤å‰éªŒè¯ä½œä¸šã€‚åœ¨æ¯ä¸ªè°ƒä¼˜è½®æ¬¡ä¸­ï¼Œ*M*x*K*ä¸ªå®ä¾‹ä¼š**ä¿ç•™åˆ°ä¸‹ä¸€è½®**ã€‚åœ¨éšåçš„è½®æ¬¡ä¸­ï¼Œæ²¡æœ‰å®ä¾‹å¯åŠ¨æ—¶é—´ã€‚
- en: SageMaker's `HyperparameterTuner` already makes use of Warm Pools as announced
    on the [AWS News Blog](https://aws.amazon.com/about-aws/whats-new/2022/08/amazon-sagemaker-automatic-model-tuning-reuses-sagemaker-training-instances-reduce-start-up-overheads/).
    However, the cross-validation training jobs that are created in each tuning job
    â€” that cross-validate a specific combination of hyperparameters â€” have to be **manually
    created and monitored**, **and the provisioned** **instances are not kept in a
    Warm Pool**. Each hyperparameter tuning training job will only finish when all
    the underlying cross-validation training jobs have completed.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: SageMaker çš„ `HyperparameterTuner` å·²ç»åˆ©ç”¨äº† Warm Poolsï¼Œæ­£å¦‚åœ¨[AWS æ–°é—»åšå®¢](https://aws.amazon.com/about-aws/whats-new/2022/08/amazon-sagemaker-automatic-model-tuning-reuses-sagemaker-training-instances-reduce-start-up-overheads/)ä¸­æ‰€å®£å¸ƒçš„é‚£æ ·ã€‚ç„¶è€Œï¼Œæ¯ä¸ªè°ƒä¼˜ä½œä¸šä¸­åˆ›å»ºçš„äº¤å‰éªŒè¯è®­ç»ƒä½œä¸š
    â€” äº¤å‰éªŒè¯ç‰¹å®šçš„è¶…å‚æ•°ç»„åˆ â€” éœ€è¦**æ‰‹åŠ¨åˆ›å»ºå’Œç›‘æ§**ï¼Œ**ä¸”é…ç½®çš„** **å®ä¾‹ä¸ä¼šä¿ç•™åœ¨ Warm Pool ä¸­**ã€‚æ¯ä¸ªè¶…å‚æ•°è°ƒä¼˜è®­ç»ƒä½œä¸šä»…åœ¨æ‰€æœ‰åŸºç¡€çš„äº¤å‰éªŒè¯è®­ç»ƒä½œä¸šå®Œæˆåæ‰ä¼šå®Œæˆã€‚
- en: 'To bring the architecture above to life and enable Warm Pools for **all** training
    jobs, we need to create three main scripts: `pipeline.py`, `cross_validation.py`,
    and `training.py`:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†ä½¿ä¸Šè¿°æ¶æ„ç”Ÿæ•ˆå¹¶ä¸º**æ‰€æœ‰**è®­ç»ƒä½œä¸šå¯ç”¨ Warm Poolsï¼Œæˆ‘ä»¬éœ€è¦åˆ›å»ºä¸‰ä¸ªä¸»è¦è„šæœ¬ï¼š`pipeline.py`ã€`cross_validation.py`å’Œ`training.py`ï¼š
- en: '`**pipeline.py**` **script â€”** Defines the SageMaker Pipeline steps described
    in [Section 2](#9b0f), which includes SageMaker''s `HyperparameterTuner`:'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`**pipeline.py**` **è„šæœ¬ â€”** å®šä¹‰äº†åœ¨ [ç¬¬ 2 èŠ‚](#9b0f) ä¸­æè¿°çš„ SageMaker Pipeline æ­¥éª¤ï¼ŒåŒ…æ‹¬
    SageMaker çš„ `HyperparameterTuner`ï¼š'
- en: '[PRE1]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '`**cross_validation.py**` **script** â€” Serves as entry point of SageMaker''s
    `HyperparameterTuner`. It launches multiple cross-validation training jobs. It
    is inside this script that the `keep_alive_period_in_seconds` parameter has to
    be specified, when calling the SageMaker Training Job API. The script computes
    and logs the average validation score across all validation folds. Logging the
    value enables easy reading of that metric using *Regex* by the `HyperparameterTuner`
    (as in the code snippet above). This metric is going to be tagged to each combination
    of hyperparameters.'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`**cross_validation.py**` **è„šæœ¬**â€”â€”ä½œä¸º SageMaker çš„ `HyperparameterTuner` çš„å…¥å£ç‚¹ã€‚å®ƒå¯åŠ¨å¤šä¸ªäº¤å‰éªŒè¯è®­ç»ƒä»»åŠ¡ã€‚åœ¨è°ƒç”¨
    SageMaker è®­ç»ƒä»»åŠ¡ API æ—¶ï¼Œå¿…é¡»åœ¨æ­¤è„šæœ¬ä¸­æŒ‡å®š `keep_alive_period_in_seconds` å‚æ•°ã€‚è¯¥è„šæœ¬è®¡ç®—å¹¶è®°å½•æ‰€æœ‰éªŒè¯æŠ˜çš„å¹³å‡éªŒè¯å¾—åˆ†ã€‚è®°å½•è¿™äº›å€¼ä½¿å¾—
    `HyperparameterTuner` å¯ä»¥é€šè¿‡ *Regex* è½»æ¾è¯»å–è¯¥æŒ‡æ ‡ï¼ˆå¦‚ä¸Šè¿°ä»£ç ç‰‡æ®µæ‰€ç¤ºï¼‰ã€‚è¯¥æŒ‡æ ‡å°†æ ‡è®°åˆ°æ¯ä¸ªè¶…å‚æ•°ç»„åˆä¸­ã€‚'
- en: '**Tip:** Add a small delay, i.e., a few seconds, between the calls to the SageMaker
    APIs that create and monitor the training jobs to prevent theâ€œRate Exceededâ€ error,
    as in the example:'
  id: totrans-60
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**æç¤ºï¼š** åœ¨è°ƒç”¨åˆ›å»ºå’Œç›‘æ§è®­ç»ƒä»»åŠ¡çš„ SageMaker API ä¹‹é—´æ·»åŠ å‡ ç§’é’Ÿçš„å°å»¶è¿Ÿï¼Œä»¥é˜²æ­¢â€œè¶…å‡ºé€Ÿç‡â€é”™è¯¯ï¼Œå¦‚ç¤ºä¾‹æ‰€ç¤ºï¼š'
- en: '[PRE2]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '**Tip:** [Disable the debugger profiler](https://docs.aws.amazon.com/sagemaker/latest/dg/debugger-turn-off.html)
    when launching your SageMaker training jobs. These profiler instances will be
    as many as the training instances and can make the overall cost increase significantly.
    You can do so by simply setting `disable_profiler=True`in the Estimator definition.'
  id: totrans-62
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**æç¤ºï¼š** [å¯åŠ¨ SageMaker è®­ç»ƒä»»åŠ¡æ—¶ç¦ç”¨è°ƒè¯•å™¨åˆ†æå™¨](https://docs.aws.amazon.com/sagemaker/latest/dg/debugger-turn-off.html)ã€‚è¿™äº›åˆ†æå™¨å®ä¾‹å°†ä¸è®­ç»ƒå®ä¾‹æ•°é‡ç›¸åŒï¼Œå¹¶ä¸”å¯èƒ½æ˜¾è‘—å¢åŠ æ€»ä½“æˆæœ¬ã€‚ä½ å¯ä»¥é€šè¿‡åœ¨
    Estimator å®šä¹‰ä¸­ç®€å•åœ°è®¾ç½® `disable_profiler=True` æ¥å®ç°ã€‚'
- en: '`**training.py**`**script** â€” Trains a model on a given input training set.
    The hyperparameters being cross-validated are passed as arguments of this script.'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`**training.py**`**è„šæœ¬**â€”â€”åœ¨ç»™å®šçš„è¾“å…¥è®­ç»ƒé›†ä¸Šè®­ç»ƒæ¨¡å‹ã€‚äº¤å‰éªŒè¯çš„è¶…å‚æ•°ä½œä¸ºæ­¤è„šæœ¬çš„å‚æ•°ä¼ é€’ã€‚'
- en: '**Tip:** Write a general-purpose `*training.py*`script and reuse it for training
    the model on cross-validation sets and for training the final model with the optimal
    hyperparameters on the full training set.'
  id: totrans-64
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**æç¤ºï¼š** ç¼–å†™ä¸€ä¸ªé€šç”¨çš„ `*training.py*` è„šæœ¬ï¼Œå¹¶åœ¨äº¤å‰éªŒè¯é›†ä¸Šè®­ç»ƒæ¨¡å‹ä»¥åŠåœ¨æ•´ä¸ªè®­ç»ƒé›†ä¸Šä½¿ç”¨æœ€ä½³è¶…å‚æ•°è®­ç»ƒæœ€ç»ˆæ¨¡å‹æ—¶é‡ç”¨å®ƒã€‚'
- en: To control each parallel cross-validation set of jobs, as well as to compute
    a final validation metric for each specific hyperparameter combination tested,
    there are several custom functions that have to be implemented inside the `cross_validation.py`
    script. [This example](https://github.com/aws-samples/sagemaker-cross-validation-pipeline)
    provides good inspiration, even though it does not enable Warm Pools or Lambda.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: è¦æ§åˆ¶æ¯ä¸ªå¹¶è¡Œäº¤å‰éªŒè¯ä»»åŠ¡é›†ï¼Œä»¥åŠä¸ºæ¯ä¸ªç‰¹å®šè¶…å‚æ•°ç»„åˆè®¡ç®—æœ€ç»ˆéªŒè¯æŒ‡æ ‡ï¼Œéœ€è¦åœ¨ `cross_validation.py` è„šæœ¬ä¸­å®ç°å‡ ä¸ªè‡ªå®šä¹‰å‡½æ•°ã€‚[è¿™ä¸ªç¤ºä¾‹](https://github.com/aws-samples/sagemaker-cross-validation-pipeline)
    æä¾›äº†å¾ˆå¥½çš„çµæ„Ÿï¼Œå°½ç®¡å®ƒæœªå¯ç”¨ Warm Pools æˆ– Lambdaã€‚
- en: How many jobs are created in total?
  id: totrans-66
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ€»å…±åˆ›å»ºäº†å¤šå°‘ä»»åŠ¡ï¼Ÿ
- en: '*M* x *N* x *(K+1)* jobs. Why?'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '*M* x *N* x *(K+1)* ä»»åŠ¡ã€‚ä¸ºä»€ä¹ˆï¼Ÿ'
- en: '*M* x *N* hyperparameter tuning training jobs â€” M in parallel and N in sequence
    â€” matches the number of hyperparameter combinations.'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*M* x *N* è¶…å‚æ•°è°ƒæ•´è®­ç»ƒä»»åŠ¡â€”â€”M ä¸ªå¹¶è¡Œå’Œ N ä¸ªä¸²è¡Œâ€”â€”åŒ¹é…è¶…å‚æ•°ç»„åˆçš„æ•°é‡ã€‚'
- en: '*K* parallel cross-validation jobs *per* hyperparameter tuning training job
    + 1 (the hyperparameter tuning training job itself).'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ¯ä¸ªè¶…å‚æ•°è°ƒæ•´è®­ç»ƒä»»åŠ¡çš„ *K* ä¸ªå¹¶è¡Œäº¤å‰éªŒè¯ä»»åŠ¡ + 1ï¼ˆè¶…å‚æ•°è°ƒæ•´è®­ç»ƒä»»åŠ¡æœ¬èº«ï¼‰ã€‚
- en: If we have **5** validation folds, run **4** hyperparameter tuning training
    jobs in parallel and **120** in sequence, then the **total number of jobs will
    be 2880**.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæˆ‘ä»¬æœ‰ **5** ä¸ªéªŒè¯æŠ˜ï¼Œè¿è¡Œ **4** ä¸ªè¶…å‚æ•°è°ƒæ•´è®­ç»ƒä»»åŠ¡å¹¶è¡Œå’Œ **120** ä¸ªä¸²è¡Œï¼Œé‚£ä¹ˆ **ä»»åŠ¡æ€»æ•°å°†æ˜¯ 2880**ã€‚
- en: '**Important:** Make sure that you have all the required service quotas in place
    for the instance types that you are using. Check the AWS guides to understand
    how to set these quotas for both [Warm Pools](https://docs.aws.amazon.com/sagemaker/latest/dg/train-warm-pools.html#train-warm-pools-resource-limits)
    and [Automatic Model Tuning](https://docs.aws.amazon.com/sagemaker/latest/dg/automatic-model-tuning-limits.html).'
  id: totrans-71
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**é‡è¦ï¼š** ç¡®ä¿ä½ æ‹¥æœ‰æ‰€ä½¿ç”¨çš„å®ä¾‹ç±»å‹æ‰€éœ€çš„æ‰€æœ‰æœåŠ¡é…é¢ã€‚æŸ¥çœ‹ AWS æŒ‡å—ä»¥äº†è§£å¦‚ä½•ä¸º [Warm Pools](https://docs.aws.amazon.com/sagemaker/latest/dg/train-warm-pools.html#train-warm-pools-resource-limits)
    å’Œ [è‡ªåŠ¨æ¨¡å‹è°ƒæ•´](https://docs.aws.amazon.com/sagemaker/latest/dg/automatic-model-tuning-limits.html)
    è®¾ç½®è¿™äº›é…é¢ã€‚'
- en: 4\. What do we get out of using Warm Pools?
  id: totrans-72
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 4\. æˆ‘ä»¬ä»ä½¿ç”¨ Warm Pools ä¸­å¾—åˆ°ä»€ä¹ˆï¼Ÿ
- en: 'Let''s say we want to run N=120 sequential training jobs and that the startup
    time of the instances is 3min and that training takes 2min to run (5min per job).
    This means that the total runtime is approximately:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: å‡è®¾æˆ‘ä»¬æƒ³è¦è¿è¡Œ N=120 ä¸ªé¡ºåºè®­ç»ƒä»»åŠ¡ï¼Œå¹¶ä¸”å®ä¾‹çš„å¯åŠ¨æ—¶é—´ä¸º 3 åˆ†é’Ÿï¼Œè®­ç»ƒæ—¶é—´ä¸º 2 åˆ†é’Ÿï¼ˆæ¯ä¸ªä»»åŠ¡ 5 åˆ†é’Ÿï¼‰ã€‚è¿™æ„å‘³ç€æ€»è¿è¡Œæ—¶é—´å¤§çº¦ä¸ºï¼š
- en: '*Without* Warm Pools: 5min x 120 jobs = **10h**'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*æ²¡æœ‰* Warm Poolsï¼š5åˆ†é’Ÿ x 120ä¸ªä»»åŠ¡ = **10å°æ—¶**'
- en: '*With* Warm Pools: 5min x 1 job + 2min x 119 jobs â‰ˆ **4h**'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*æœ‰* Warm Poolsï¼š5åˆ†é’Ÿ x 1ä¸ªä»»åŠ¡ + 2åˆ†é’Ÿ x 119ä¸ªä»»åŠ¡ â‰ˆ **4å°æ—¶**'
- en: '**This means that with Warm Pools the process takes 60% less time!**'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '**è¿™æ„å‘³ç€ä½¿ç”¨ Warm Pools è¿‡ç¨‹çš„æ—¶é—´å‡å°‘äº† 60%ï¼**'
- en: 5\. Summary
  id: totrans-77
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 5\. æ€»ç»“
- en: In this article, I showed how we can leverage Warm Pools to significantly speed-up
    hyperparameter tuning with cross-validation in SageMaker Pipelines. Warm Pools
    are a great feature of SageMaker that not only enables more efficient production
    pipelines, but also faster iterations in experiments. At the moment, SageMaker
    Managed Warm Pools have been integrated in SageMaker Training, but not in SageMaker
    Processing.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘å±•ç¤ºäº†å¦‚ä½•åˆ©ç”¨ Warm Pools æ˜¾è‘—åŠ å¿« SageMaker Pipelines ä¸­çš„è¶…å‚æ•°è°ƒä¼˜ã€‚Warm Pools æ˜¯ SageMaker
    çš„ä¸€ä¸ªå¾ˆæ£’çš„åŠŸèƒ½ï¼Œå®ƒä¸ä»…ä½¿ç”Ÿäº§æµæ°´çº¿æ›´åŠ é«˜æ•ˆï¼Œè¿˜åŠ å¿«äº†å®éªŒçš„è¿­ä»£ã€‚ç›®å‰ï¼ŒSageMaker ç®¡ç†çš„ Warm Pools å·²ç»é›†æˆåˆ° SageMaker Training
    ä¸­ï¼Œä½†å°šæœªé›†æˆåˆ° SageMaker Processingã€‚
- en: â€” JoÃ£o Pereira
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: â€” è‹¥æ˜‚Â·ä½©é›·æ‹‰
- en: '*Thank you for reading. Hope this article helps you scaling hyperparameter
    tuning in SageMaker. If you would like to read my future articles, please* [*follow
    me*](https://medium.com/@joao.pereira.abt/subscribe)*. Feedback is highly appreciated!
    Leave a comment below if you have any questions or reach out to me directly* [***by
    email***](mailto:mail@joao-pereira.pt) *or in* [***LinkedIn***](https://www.linkedin.com/in/jpcpereira/)*.*'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '*æ„Ÿè°¢é˜…è¯»ã€‚å¸Œæœ›è¿™ç¯‡æ–‡ç« èƒ½å¸®åŠ©ä½ åœ¨ SageMaker ä¸­æ‰©å±•è¶…å‚æ•°è°ƒä¼˜ã€‚å¦‚æœä½ æƒ³é˜…è¯»æˆ‘æœªæ¥çš„æ–‡ç« ï¼Œè¯·* [*å…³æ³¨æˆ‘*](https://medium.com/@joao.pereira.abt/subscribe)*ã€‚éå¸¸æ„Ÿè°¢åé¦ˆï¼å¦‚æœæœ‰ä»»ä½•é—®é¢˜ï¼Œè¯·åœ¨ä¸‹æ–¹ç•™è¨€æˆ–ç›´æ¥è”ç³»æˆ‘*
    [***é€šè¿‡ç”µå­é‚®ä»¶***](mailto:mail@joao-pereira.pt) *æˆ–åœ¨* [***LinkedIn***](https://www.linkedin.com/in/jpcpereira/)*ä¸Šè”ç³»æˆ‘ã€‚*'
- en: '*All images unless otherwise noted are by the author.*'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '*é™¤éå¦æœ‰è¯´æ˜ï¼Œæ‰€æœ‰å›¾ç‰‡å‡ç”±ä½œè€…æä¾›ã€‚*'
